<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 45]
- [cs.LG](#cs.LG) [Total: 76]
- [econ.EM](#econ.EM) [Total: 1]
- [eess.IV](#eess.IV) [Total: 2]
- [q-fin.RM](#q-fin.RM) [Total: 2]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [stat.ML](#stat.ML) [Total: 12]
- [math.OC](#math.OC) [Total: 14]
- [cs.AI](#cs.AI) [Total: 32]
- [cs.CY](#cs.CY) [Total: 3]
- [eess.SY](#eess.SY) [Total: 24]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Measuring Teaching with LLMs](https://arxiv.org/abs/2510.22968)
*Michael Hardy*

Main category: cs.CL

TL;DR: 使用基于句子嵌入的定制LLM来测量教学质量，在数据高效训练下达到人类水平甚至超人类表现，并与教师增值指标相关，为AI驱动的教学评估提供了可行方法。


<details>
  <summary>Details</summary>
Motivation: 教育中需要客观可扩展的教学质量测量方法，传统通用LLM在处理复杂课堂观察工具时表现不佳，需要更适合长文本解释性内容的架构。

Method: 使用基于句子级嵌入的定制LLM，系统评估五种不同句子嵌入，采用数据高效训练机制防止过拟合，分析标注上下文窗口。

Result: 专业模型达到人类水平表现（专家评分>0.65），超越平均人-人评分者相关性；高级模型将更多分数变异归因于课程级特征而非孤立话语；聚合模型分数与教师增值指标相关。

Conclusion: 建立了AI驱动教学测量的可行强大方法，为教育者发展提供可扩展、可靠有效的反馈路径，但模型尚未实现完全泛化。

Abstract: Objective and scalable measurement of teaching quality is a persistent
challenge in education. While Large Language Models (LLMs) offer potential,
general-purpose models have struggled to reliably apply complex, authentic
classroom observation instruments. This paper uses custom LLMs built on
sentence-level embeddings, an architecture better suited for the long-form,
interpretive nature of classroom transcripts than conventional subword
tokenization. We systematically evaluate five different sentence embeddings
under a data-efficient training regime designed to prevent overfitting. Our
results demonstrate that these specialized models can achieve human-level and
even super-human performance with expert human ratings above 0.65 and
surpassing the average human-human rater correlation. Further, through analysis
of annotation context windows, we find that more advanced models-those better
aligned with human judgments-attribute a larger share of score variation to
lesson-level features rather than isolated utterances, challenging the
sufficiency of single-turn annotation paradigms. Finally, to assess external
validity, we find that aggregate model scores align with teacher value-added
measures, indicating they are capturing features relevant to student learning.
However, this trend does not hold at the individual item level, suggesting that
while the models learn useful signals, they have not yet achieved full
generalization. This work establishes a viable and powerful new methodology for
AI-driven instructional measurement, offering a path toward providing scalable,
reliable, and valid feedback for educator development.

</details>


### [2] [Evaluating Machine Translation Datasets for Low-Web Data Languages: A Gendered Lens](https://arxiv.org/abs/2511.03880)
*Hellina Hailu Nigatu,Bethelhem Yemane Mamo,Bontu Fufa Balcha,Debora Taye Tesfaye,Elbethel Daniel Zewdie,Ikram Behiru Nesiru,Jitu Ewnetu Hailu,Senait Mengesha Yayo*

Main category: cs.CL

TL;DR: 该研究分析了三种低资源语言（阿法尔奥罗莫语、阿姆哈拉语和提格里尼亚语）机器翻译数据集中的性别偏见问题，发现数据集存在男性主导、有害内容以及领域分布不均等问题。


<details>
  <summary>Details</summary>
Motivation: 随着低资源语言被纳入NLP研究，大规模数据收集往往重数量轻质量，可能导致技术性能差和产生有害偏见内容。

Method: 研究分析了三种低资源语言的机器翻译数据集，重点关注性别表征，包括人名、动词语法性别和刻板印象描绘。

Result: 发现训练数据中政治和宗教领域文本占主导，基准数据集集中在新闻、健康、体育领域；存在严重的男性偏向，且数据量最大的语言中有害和毒性内容更突出。

Conclusion: 数量不能保证质量，需要更深入地调查低资源语言数据集并及早减轻有害内容。

Abstract: As low-resourced languages are increasingly incorporated into NLP research,
there is an emphasis on collecting large-scale datasets. But in prioritizing
quantity over quality, we risk 1) building language technologies that perform
poorly for these languages and 2) producing harmful content that perpetuates
societal biases. In this paper, we investigate the quality of Machine
Translation (MT) datasets for three low-resourced languages--Afan Oromo,
Amharic, and Tigrinya, with a focus on the gender representation in the
datasets. Our findings demonstrate that while training data has a large
representation of political and religious domain text, benchmark datasets are
focused on news, health, and sports. We also found a large skew towards the
male gender--in names of persons, the grammatical gender of verbs, and in
stereotypical depictions in the datasets. Further, we found harmful and toxic
depictions against women, which were more prominent for the language with the
largest amount of data, underscoring that quantity does not guarantee quality.
We hope that our work inspires further inquiry into the datasets collected for
low-resourced languages and prompts early mitigation of harmful content.
WARNING: This paper contains discussion of NSFW content that some may find
disturbing.

</details>


### [3] [Activation-Space Personality Steering: Hybrid Layer Selection for Stable Trait Control in LLMs](https://arxiv.org/abs/2511.03738)
*Pranav Bhandari,Nicolas Fay,Sanjeevan Selvaganapathy,Amitava Datta,Usman Naseem,Mehwish Nasim*

Main category: cs.CL

TL;DR: 提出了一种基于大五人格特质提取LLM隐藏状态激活的新方法，通过低秩子空间发现和动态层选择来精确控制LLM的人格特质表达。


<details>
  <summary>Details</summary>
Motivation: LLM在生成中表现出隐含的人格特质，但如何可靠控制或对齐这些特质以满足特定需求仍是一个开放挑战。需要有效的机制来在生成过程中操纵模型行为。

Method: 从transformer层提取隐藏状态激活，应用低秩子空间发现方法，识别特质特定的最优层，并通过动态层选择的灵活引导框架操作人格对齐方向。

Result: 发现人格特质占据低秩共享子空间，这些潜在结构可以通过精心扰动转化为有效的引导机制，而不影响流畅性、方差和一般能力。

Conclusion: 该方法有助于弥合心理学理论与实际模型对齐之间的差距，为LLM的人格特质控制提供了可行的技术路径。

Abstract: Large Language Models exhibit implicit personalities in their generation, but
reliably controlling or aligning these traits to meet specific needs remains an
open challenge. The need for effective mechanisms for behavioural manipulation
of the model during generation is a critical gap in the literature that needs
to be fulfilled. Personality-aware LLMs hold a promising direction towards this
objective. However, the relationship between these psychological constructs and
their representations within LLMs remains underexplored and requires further
investigation. Moreover, it is intriguing to understand and study the use of
these representations to steer the models' behaviour. We propose a novel
pipeline that extracts hidden state activations from transformer layers using
the Big Five Personality Traits (Openness, Conscientiousness, Extraversion,
Agreeableness and Neuroticism), which is a comprehensive and empirically
validated framework to model human personality applies low-rank subspace
discovery methods, and identifies trait-specific optimal layers across
different model architectures for robust injection. The resulting
personality-aligned directions are then operationalised through a flexible
steering framework with dynamic layer selection, enabling precise control of
trait expression in LLM outputs. Our findings reveal that personality traits
occupy a low-rank shared subspace, and that these latent structures can be
transformed into actionable mechanisms for effective steering through careful
perturbations without impacting the fluency, variance and general capabilities,
helping to bridge the gap between psychological theory and practical model
alignment.

</details>


### [4] [The Human Flourishing Geographic Index: A County-Level Dataset for the United States, 2013--2023](https://arxiv.org/abs/2511.03915)
*Stefano M. Iacus,Devika Jain,Andrea Nasuto,Giuseppe Porro,Marcello Carammia,Andrea Vezzulli*

Main category: cs.CL

TL;DR: 开发了人类繁荣地理指数(HFGI)，通过分析26亿条美国推文，使用微调的大语言模型对48个繁荣相关指标进行分类，提供县级和州级月度/年度数据，验证了测量准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的人类繁荣测量方法缺乏精细的空间和时间分辨率，需要超越经济指标来理解社会福祉。

Method: 分析2013-2023年约26亿条地理定位的美国推文，使用微调的大语言模型分类48个与哈佛全球繁荣研究框架一致的指标，包括移民态度和腐败感知。

Result: 创建了提供县级和州级月度/年度繁荣相关话语指标的数据集，验证显示测量准确代表基础构念，并与既定指标呈现预期相关性。

Conclusion: 该资源能以前所未有的分辨率进行福祉、不平等和社会变革的多学科分析，提供了过去十年美国社交媒体话语中反映的人类繁荣动态洞察。

Abstract: Quantifying human flourishing, a multidimensional construct including
happiness, health, purpose, virtue, relationships, and financial stability, is
critical for understanding societal well-being beyond economic indicators.
Existing measures often lack fine spatial and temporal resolution. Here we
introduce the Human Flourishing Geographic Index (HFGI), derived from analyzing
approximately 2.6 billion geolocated U.S. tweets (2013-2023) using fine-tuned
large language models to classify expressions across 48 indicators aligned with
Harvard's Global Flourishing Study framework plus attitudes towards migration
and perception of corruption. The dataset offers monthly and yearly county- and
state-level indicators of flourishing-related discourse, validated to confirm
that the measures accurately represent the underlying constructs and show
expected correlations with established indicators. This resource enables
multidisciplinary analyses of well-being, inequality, and social change at
unprecedented resolution, offering insights into the dynamics of human
flourishing as reflected in social media discourse across the United States
over the past decade.

</details>


### [5] [TextualVerifier: Verify TextGrad Step-by-Step](https://arxiv.org/abs/2511.03739)
*Eugenius Mario Situmorang,Adila Alfa Krisnadhi,Ari Wibisono*

Main category: cs.CL

TL;DR: TextualVerifier是首个为TextGrad设计的自验证框架，通过链式思维推理和多数投票机制，在不依赖数值梯度的情况下提升文本优化的可靠性。


<details>
  <summary>Details</summary>
Motivation: TextGrad缺乏自验证机制来确保文本决策中推理的有效性，这限制了其在复合AI系统中的可靠性。

Method: 采用四阶段工作流程：链式思维分解、变体生成、多数投票和共识聚合，集成到TextGrad的损失函数和优化结果验证阶段。

Result: 在PRM800K上推理步骤有效性提升29%；集成到TextGrad后在GPQA-Diamond、MMLU-ML和MMLU-CP基准上分别获得8.08、10.71和3.92个百分点的改进。

Conclusion: TextualVerifier通过基于LLM的技术为TextGrad提供了首个自验证框架，无需数值梯度即可实现更可靠的推理，为文本优化验证开辟了新方向。

Abstract: TextGrad is a novel approach to text-based automatic differentiation that
enables composite AI systems to perform optimization without explicit numerical
equations. However, it currently lacks self-verification mechanisms that ensure
reasoning validity in text-based decision making. This research introduces
TextualVerifier, a verification framework that leverages chain-of-thought
reasoning and majority voting with large language models to address this
verification gap. TextualVerifier implements a four-stage workflow:
chain-of-thought decomposition, variant generation, majority voting, and
consensus aggregation. It integrates non-invasively with TextGrad at both the
loss function and optimization result verification stages. Experimental
evaluation using the Gemini 1.5 Pro model is conducted in two phases: (1)
standalone evaluation on PRM800K, and (2) integrated evaluation with TextGrad
on GPQA-Diamond, MMLU-ML, and MMLU-CP benchmarks. Results show statistically
significant improvements (p < 0.001). In phase one, TextualVerifier improves
the validity of reasoning steps by 29 percent. In phase two, integration into
TextGrad loss function yields a 2.2 percentage point gain from 68.2 to 70.4
percent with a moderate overhead of 5.9 LLM calls on average. Further
evaluations of TextualVerifier versioning yield 8.08, 10.71, and 3.92
percentage point improvements on GPQA, MMLU-ML, and MMLU-CP respectively.
TextualVerifier thus presents the first self-verification framework for
TextGrad through LLM-based techniques without requiring numerical gradients,
enabling more reliable reasoning and opening new directions for verification in
text-based optimization.

</details>


### [6] [GRDD+: An Extended Greek Dialectal Dataset with Cross-Architecture Fine-tuning Evaluation](https://arxiv.org/abs/2511.03772)
*Stergios Chatzikyriakidis,Dimitris Papadakis,Sevasti-Ioanna Papaioannou,Erofili Psaltaki*

Main category: cs.CL

TL;DR: 扩展希腊方言数据集GRDD+，新增6种希腊方言变体，总规模达637万词，包含10种方言变体。在多个LLM上进行微调实验，比较不同模型架构与前沿模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有希腊方言数据集覆盖范围有限，需要更全面的方言变体数据来研究方言数据质量对LLM性能的影响。

Method: 构建扩展的希腊方言数据集GRDD+，包含10种方言变体。使用三种模型架构（Llama-3-8B、Llama-3.1-8B、Krikri-8B）进行微调，并与前沿模型（Claude-3.7-Sonnet、Gemini-2.5、ChatGPT-5）进行比较。

Result: 创建了迄今为止规模最大、变体最丰富的希腊方言数据集，总规模637万词。通过微调实验评估了高质量方言数据对LLM的影响。

Conclusion: GRDD+是首个具有如此规模和变体多样性的希腊方言数据集，为研究方言处理提供了重要资源，通过微调实验展示了方言数据质量对模型性能的重要性。

Abstract: We present an extended Greek Dialectal Dataset (GRDD+) 1that complements the
existing GRDD dataset with more data from Cretan, Cypriot, Pontic and Northern
Greek, while we add six new varieties: Greco-Corsican, Griko (Southern Italian
Greek), Maniot, Heptanesian, Tsakonian, and Katharevusa Greek. The result is a
dataset with total size 6,374,939 words and 10 varieties. This is the first
dataset with such variation and size to date. We conduct a number of
fine-tuning experiments to see the effect of good quality dialectal data on a
number of LLMs. We fine-tune three model architectures (Llama-3-8B,
Llama-3.1-8B, Krikri-8B) and compare the results to frontier models
(Claude-3.7-Sonnet, Gemini-2.5, ChatGPT-5).

</details>


### [7] [PLLuM: A Family of Polish Large Language Models](https://arxiv.org/abs/2511.03823)
*Jan Kocoń,Maciej Piasecki,Arkadiusz Janz,Teddy Ferdinan,Łukasz Radliński,Bartłomiej Koptyra,Marcin Oleksy,Stanisław Woźniak,Paweł Walkowiak,Konrad Wojtasik,Julia Moska,Tomasz Naskręt,Bartosz Walkowiak,Mateusz Gniewkowski,Kamil Szyc,Dawid Motyka,Dawid Banach,Jonatan Dalasiński,Ewa Rudnicka,Bartłomiej Alberski,Tomasz Walkowiak,Aleksander Szczęsny,Maciej Markiewicz,Tomasz Bernaś,Hubert Mazur,Kamil Żyta,Mateusz Tykierko,Grzegorz Chodak,Tomasz Kajdanowicz,Przemysław Kazienko,Agnieszka Karlińska,Karolina Seweryn,Anna Kołos,Maciej Chrabąszcz,Katarzyna Lorenc,Aleksandra Krasnodębska,Artur Wilczek,Katarzyna Dziewulska,Paula Betscher,Zofia Cieślińska,Katarzyna Kowol,Daria Mikoś,Maciej Trzciński,Dawid Krutul,Marek Kozłowski,Sławomir Dadas,Rafał Poświata,Michał Perełkiewicz,Małgorzata Grębowiec,Maciej Kazuła,Marcin Białas,Roman Roszko,Danuta Roszko,Jurgita Vaičenonienė,Andrius Utka,Paweł Levchuk,Paweł Kowalski,Irena Prawdzic-Jankowska,Maciej Ogrodniczuk,Monika Borys,Anna Bulińska,Wiktoria Gumienna,Witold Kieraś,Dorota Komosińska,Katarzyna Krasnowska-Kieraś,Łukasz Kobyliński,Martyna Lewandowska,Marek Łaziński,Mikołaj Łątkowski,Dawid Mastalerz,Beata Milewicz,Agnieszka Anna Mykowiecka,Angelika Peljak-Łapińska,Sandra Penno,Zuzanna Przybysz,Michał Rudolf,Piotr Rybak,Karolina Saputa,Aleksandra Tomaszewska,Aleksander Wawer,Marcin Woliński,Joanna Wołoszyn,Alina Wróblewska,Bartosz Żuk,Filip Żarnecki,Konrad Kaczyński,Anna Cichosz,Zuzanna Deckert,Monika Garnys,Izabela Grabarczyk,Wojciech Janowski,Sylwia Karasińska,Aleksandra Kujawiak,Piotr Misztela,Maria Szymańska,Karolina Walkusz,Igor Siek,Jakub Kwiatkowski,Piotr Pęzik*

Main category: cs.CL

TL;DR: PLLuM是专门为波兰语开发的最大开源基础模型家族，旨在解决英语中心化商业环境中高质量、透明和文化相关语言模型的不足。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的发展主要集中在英语上，对其他语言的支持有限。PLLuM旨在满足对高质量、透明和文化相关语言模型的需求，超越英语中心的商业格局。

Method: 构建了新的1400亿token波兰语文本语料库用于预训练，77k自定义指令数据集和100k偏好优化数据集。采用负责任的AI框架，包含严格的数据治理和混合模块用于输出校正和安全过滤。

Result: 开发了PLLuM模型家族，包括基础模型和指令调优变体，并在公共管理下游任务中展示了其实用性。

Conclusion: 通过公开发布这些模型，PLLuM旨在促进开放研究，加强波兰的主权AI技术。

Abstract: Large Language Models (LLMs) play a central role in modern artificial
intelligence, yet their development has been primarily focused on English,
resulting in limited support for other languages. We present PLLuM (Polish
Large Language Model), the largest open-source family of foundation models
tailored specifically for the Polish language. Developed by a consortium of
major Polish research institutions, PLLuM addresses the need for high-quality,
transparent, and culturally relevant language models beyond the English-centric
commercial landscape. We describe the development process, including the
construction of a new 140-billion-token Polish text corpus for pre-training, a
77k custom instructions dataset, and a 100k preference optimization dataset. A
key component is a Responsible AI framework that incorporates strict data
governance and a hybrid module for output correction and safety filtering. We
detail the models' architecture, training procedures, and alignment techniques
for both base and instruction-tuned variants, and demonstrate their utility in
a downstream task within public administration. By releasing these models
publicly, PLLuM aims to foster open research and strengthen sovereign AI
technologies in Poland.

</details>


### [8] [STARS: Segment-level Token Alignment with Rejection Sampling in Large Language Models](https://arxiv.org/abs/2511.03827)
*Mohammad Atif Quamar,Mohammad Areeb,Mikhail Kuznetsov,Muslum Ozgur Ozmen,Z. Berkay Celik*

Main category: cs.CL

TL;DR: STARS是一种解码时算法，通过迭代采样、评分和拒绝/接受固定长度的token片段来引导模型生成，显著提高计算效率和对齐质量。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型对齐方法（如微调）计算成本高且效果不佳，而推理时方法（如Best-of-N采样）需要不切实际的计算量才能达到最优对齐。

Method: STARS采用分段级token对齐与拒绝采样，在解码时迭代采样、评分和拒绝/接受固定长度的token片段，实现生成路径的早期修正。

Result: 在六个LLM上的实验表明，STARS在胜率上比监督微调高出14.9个百分点，比直接偏好优化高出4.3个百分点，同时与强大的Best-of-N基线保持高度竞争力。

Conclusion: STARS建立了细粒度、奖励引导的采样方法，作为传统微调和全序列排序方法的通用、鲁棒且高效替代方案，用于对齐LLM。

Abstract: Aligning large language models with human values is crucial for their safe
deployment; however, existing methods, such as fine-tuning, are computationally
expensive and suboptimal. In contrast, inference-time approaches like Best-of-N
sampling require practically infeasible computation to achieve optimal
alignment. We propose STARS: Segment-level Token Alignment with Rejection
Sampling, a decoding-time algorithm that steers model generation by iteratively
sampling, scoring, and rejecting/accepting short, fixed-size token segments.
This allows for early correction of the generation path, significantly
improving computational efficiency and boosting alignment quality. Across a
suite of six LLMs, we show that STARS outperforms Supervised Fine-Tuning (SFT)
by up to 14.9 percentage points and Direct Preference Optimization (DPO) by up
to 4.3 percentage points on win-rates, while remaining highly competitive with
strong Best-of-N baselines. Our work establishes granular, reward-guided
sampling as a generalizable, robust, and efficient alternative to traditional
fine-tuning and full-sequence ranking methods for aligning LLMs.

</details>


### [9] [Divide, Cache, Conquer: Dichotomic Prompting for Efficient Multi-Label LLM-Based Classification](https://arxiv.org/abs/2511.03830)
*Mikołaj Langner,Jan Eliasz,Ewa Rudnicka,Jan Kocoń*

Main category: cs.CL

TL;DR: 提出一种高效的多标签文本分类方法，将分类任务重构为一系列二分决策，结合前缀缓存机制实现短文本推理的效率提升，并通过LLM到SLM的蒸馏技术训练小型模型。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在多标签分类任务中效率低下的问题，特别是短文本推理场景，同时保持分类准确性。

Method: 将多标签分类分解为独立的二分查询，使用前缀缓存机制优化推理效率，通过DeepSeek-V3生成标注数据，蒸馏训练小型模型如HerBERT-Large、CLARIN-1B等。

Result: 微调后的模型在零样本基准上表现显著提升，特别是在训练中见过的维度上，实现了效率与准确性的平衡。

Conclusion: 二分查询分解结合蒸馏和缓存感知推理为基于LLM的分类提供了可扩展且有效的框架，该方法具有领域通用性。

Abstract: We introduce a method for efficient multi-label text classification with
large language models (LLMs), built on reformulating classification tasks as
sequences of dichotomic (yes/no) decisions. Instead of generating all labels in
a single structured response, each target dimension is queried independently,
which, combined with a prefix caching mechanism, yields substantial efficiency
gains for short-text inference without loss of accuracy. To demonstrate the
approach, we focus on affective text analysis, covering 24 dimensions including
emotions and sentiment. Using LLM-to-SLM distillation, a powerful annotator
model (DeepSeek-V3) provides multiple annotations per text, which are
aggregated to fine-tune smaller models (HerBERT-Large, CLARIN-1B, PLLuM-8B,
Gemma3-1B). The fine-tuned models show significant improvements over zero-shot
baselines, particularly on the dimensions seen during training. Our findings
suggest that decomposing multi-label classification into dichotomic queries,
combined with distillation and cache-aware inference, offers a scalable and
effective framework for LLM-based classification. While we validate the method
on affective states, the approach is general and applicable across domains.

</details>


### [10] [GRAD: Graph-Retrieved Adaptive Decoding for Hallucination Mitigation](https://arxiv.org/abs/2511.03900)
*Manh Nguyen,Sunil Gupta,Dai Do,Hung Le*

Main category: cs.CL

TL;DR: GRAD是一种解码时方法，通过构建稀疏令牌转移图来缓解LLM幻觉问题，无需重新训练模型，在多个问答基准测试中显著提升了准确性和事实性。


<details>
  <summary>Details</summary>
Motivation: 现有缓解LLM幻觉的方法依赖外部知识源，但基于提示的方法脆弱且领域敏感，符号知识集成则成本高昂。受知识图谱启发，开发无需重新训练即可在语料库证据基础上进行生成的方法。

Method: GRAD通过单次前向传播在小型检索语料库上累积下一个令牌对数概率来构建稀疏令牌转移图。解码时，图检索的对数概率经过最大归一化并与模型对数概率自适应融合，优先选择高证据延续同时保持流畅性。

Result: 在三个模型和多个问答基准测试中，GRAD始终优于基线方法：内在准确率提高9.7%，幻觉率降低8.6%，正确性提高6.9%，在所有方法中获得了最高的真实性-信息性乘积分数。

Conclusion: GRAD提供了一种轻量级、即插即用的替代方案，证明语料级令牌转移的统计证据可以有效引导生成更真实和可验证的输出。

Abstract: Hallucination mitigation remains a persistent challenge for large language
models (LLMs), even as model scales grow. Existing approaches often rely on
external knowledge sources, such as structured databases or knowledge graphs,
accessed through prompting or retrieval. However, prompt-based grounding is
fragile and domain-sensitive, while symbolic knowledge integration incurs heavy
retrieval and formatting costs. Motivated by knowledge graphs, we introduce
Graph-Retrieved Adaptive Decoding (GRAD), a decoding-time method that grounds
generation in corpus-derived evidence without retraining. GRAD constructs a
sparse token transition graph by accumulating next-token logits across a small
retrieved corpus in a single forward pass. During decoding, graph-retrieved
logits are max-normalized and adaptively fused with model logits to favor
high-evidence continuations while preserving fluency. Across three models and a
range of question-answering benchmarks spanning intrinsic, extrinsic
hallucination, and factuality tasks, GRAD consistently surpasses baselines,
achieving up to 9.7$\%$ higher intrinsic accuracy, 8.6$\%$ lower hallucination
rates, and 6.9$\%$ greater correctness compared to greedy decoding, while
attaining the highest truth--informativeness product score among all methods.
GRAD offers a lightweight, plug-and-play alternative to contrastive decoding
and knowledge graph augmentation, demonstrating that statistical evidence from
corpus-level token transitions can effectively steer generation toward more
truthful and verifiable outputs.

</details>


### [11] [Context informs pragmatic interpretation in vision-language models](https://arxiv.org/abs/2511.03908)
*Alvin Wei Ming Tan,Ben Prystawski,Veronica Boyce,Michael C. Frank*

Main category: cs.CL

TL;DR: 论文研究了迭代参考游戏中人类和视觉语言模型的性能差异，发现模型在缺乏相关上下文时表现远不如人类，但在有相关上下文时性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 测试智能体在多轮语言环境中进行上下文敏感语用推理的能力，特别是在迭代参考游戏中的表现。

Method: 在迭代参考游戏中测试人类和视觉语言模型，通过改变上下文的数量、顺序和相关性来评估性能。

Result: 没有相关上下文时，模型表现高于随机但远逊于人类；有相关上下文时，模型性能随试验次数显著提升。

Conclusion: 带有抽象指称物的少样本参考游戏对机器学习模型来说仍然是一个困难任务。

Abstract: Iterated reference games - in which players repeatedly pick out novel
referents using language - present a test case for agents' ability to perform
context-sensitive pragmatic reasoning in multi-turn linguistic environments. We
tested humans and vision-language models on trials from iterated reference
games, varying the given context in terms of amount, order, and relevance.
Without relevant context, models were above chance but substantially worse than
humans. However, with relevant context, model performance increased
dramatically over trials. Few-shot reference games with abstract referents
remain a difficult task for machine learning models.

</details>


### [12] [Direct Semantic Communication Between Large Language Models via Vector Translation](https://arxiv.org/abs/2511.03945)
*Fu-Chun Yang,Jason Eshraghian*

Main category: cs.CL

TL;DR: 该论文提出了一种通过向量翻译在多智能体系统中实现跨模型语义通信的方法，避免了传统基于token的消息传递方式，减少了计算开销并提高了信息传输效率。


<details>
  <summary>Details</summary>
Motivation: 在多智能体设置中，LLMs通常通过明文token传递消息，这丢弃了大部分潜在语义，限制了信息传输并增加了不必要的计算开销。

Method: 通过训练双编码器翻译器在Llama-2-7B和Mistral-7B-Instruct之间建立向量翻译映射，实现表示空间之间的直接语义交换。使用30%混合强度的翻译向量注入来引导目标模型的生成。

Result: 双编码器翻译器实现了平均余弦对齐度0.538，双向评估显示2.01:1的传输不对称性，表明通用模型比指令调优变体产生更可转移的表示。

Conclusion: 这种保守的注入方法保持了计算稳定性，同时证明了跨模型潜在通信是可行的，使得协作AI系统能够共享意义而非token。

Abstract: In multi-agent settings, such as debate, reflection, or tool-calling, large
language models (LLMs) pass messages as plain tokens, discarding most latent
semantics. This constrains information transfer and adds unnecessary
computational overhead. We form a latent bridge via vector translations, which
use learned mappings that enable direct semantic exchange between
representation spaces. A dual-encoder translator trained between Llama-2-7B and
Mistral-7B-Instruct attains an average cosine alignment of 0.538. Injecting the
translated vectors at 30 percent blending strength steers the target model's
generation without destabilizing logits. Bidirectional evaluation shows a
2.01:1 transfer asymmetry, indicating that general-purpose models yield more
transferable representations than instruction-tuned variants. This conservative
injection preserves computational stability while demonstrating that
cross-model latent communication is feasible, enabling collaborative AI systems
that share meaning rather than tokens.

</details>


### [13] [Abductive Inference in Retrieval-Augmented Language Models: Generating and Validating Missing Premises](https://arxiv.org/abs/2511.04020)
*Shiyin Lin*

Main category: cs.CL

TL;DR: 提出一个将溯因推理集成到检索增强LLMs中的框架，通过检测证据不足、生成候选缺失前提并进行验证来提升RAG系统的推理能力。


<details>
  <summary>Details</summary>
Motivation: RAG管道在检索证据不完整时经常失败，导致推理过程中出现空白，而溯因推理能够通过生成合理的缺失前提来弥补这些空白。

Method: 提出一个集成溯因推理的框架，包括检测证据不足、生成候选缺失前提，并通过一致性和合理性检查进行验证。

Result: 在溯因推理和多跳问答基准测试上的实验结果表明，该方法提高了答案准确性和推理忠实度。

Conclusion: 溯因推理是增强RAG系统鲁棒性和可解释性的一个有前景的方向。

Abstract: Large Language Models (LLMs) enhanced with retrieval -- commonly referred to
as Retrieval-Augmented Generation (RAG) -- have demonstrated strong performance
in knowledge-intensive tasks. However, RAG pipelines often fail when retrieved
evidence is incomplete, leaving gaps in the reasoning process. In such cases,
\emph{abductive inference} -- the process of generating plausible missing
premises to explain observations -- offers a principled approach to bridge
these gaps. In this paper, we propose a framework that integrates abductive
inference into retrieval-augmented LLMs. Our method detects insufficient
evidence, generates candidate missing premises, and validates them through
consistency and plausibility checks. Experimental results on abductive
reasoning and multi-hop QA benchmarks show that our approach improves both
answer accuracy and reasoning faithfulness. This work highlights abductive
inference as a promising direction for enhancing the robustness and
explainability of RAG systems.

</details>


### [14] [WST: Weakly Supervised Transducer for Automatic Speech Recognition](https://arxiv.org/abs/2511.04035)
*Dongji Gao,Chenda Liao,Changliang Liu,Matthew Wiesner,Leibny Paola Garcia,Daniel Povey,Sanjeev Khudanpur,Jian Wu*

Main category: cs.CL

TL;DR: 提出弱监督Transducer（WST）方法，通过灵活的训练图设计处理转录错误，无需额外置信度估计或预训练模型，在转录错误率高达70%时仍能保持性能。


<details>
  <summary>Details</summary>
Motivation: RNN-T在端到端语音识别中依赖大规模高质量标注数据，但这类数据获取成本高且困难。

Method: 设计弱监督Transducer（WST），使用灵活的训练图来鲁棒处理转录错误，无需置信度估计或辅助预训练模型。

Result: 在合成和工业数据集上的实验表明，WST在转录错误率高达70%时仍能有效保持性能，优于现有的基于CTC的弱监督方法（如BTC和OTC）。

Conclusion: WST在实际ASR场景中具有实用性和鲁棒性，代码将公开。

Abstract: The Recurrent Neural Network-Transducer (RNN-T) is widely adopted in
end-to-end (E2E) automatic speech recognition (ASR) tasks but depends heavily
on large-scale, high-quality annotated data, which are often costly and
difficult to obtain. To mitigate this reliance, we propose a Weakly Supervised
Transducer (WST), which integrates a flexible training graph designed to
robustly handle errors in the transcripts without requiring additional
confidence estimation or auxiliary pre-trained models. Empirical evaluations on
synthetic and industrial datasets reveal that WST effectively maintains
performance even with transcription error rates of up to 70%, consistently
outperforming existing Connectionist Temporal Classification (CTC)-based weakly
supervised approaches, such as Bypass Temporal Classification (BTC) and
Omni-Temporal Classification (OTC). These results demonstrate the practical
utility and robustness of WST in realistic ASR settings. The implementation
will be publicly available.

</details>


### [15] [T-FIX: Text-Based Explanations with Features Interpretable to eXperts](https://arxiv.org/abs/2511.04070)
*Shreya Havaldar,Helen Jin,Chaehyeon Kim,Anton Xue,Weiqiu You,Marco Gatti,Bhuvnesh Jain,Helen Qu,Daniel A Hashimoto,Amin Madani,Rajat Deo,Sameed Ahmed M. Khatana,Gary E. Weissman,Lyle Ungar,Eric Wong*

Main category: cs.CL

TL;DR: 提出了T-FIX基准测试，用于评估LLM在知识密集型领域生成解释时与专家直觉的对齐程度


<details>
  <summary>Details</summary>
Motivation: 当前LLM解释评估主要关注解释的合理性和内部一致性，但无法衡量解释内容是否真正符合专家直觉，特别是在知识密集型领域如医学、天文学等

Method: 与领域专家合作开发了T-FIX基准测试，涵盖七个知识密集型领域，并设计了新的指标来测量LLM解释与专家判断的对齐度

Result: 开发了一个系统性的评估框架来衡量LLM解释的专家对齐性

Conclusion: 专家对齐性是评估LLM在知识密集型领域生成解释质量的重要标准，T-FIX基准为此提供了有效的评估工具

Abstract: As LLMs are deployed in knowledge-intensive settings (e.g., surgery,
astronomy, therapy), users expect not just answers, but also meaningful
explanations for those answers. In these settings, users are often domain
experts (e.g., doctors, astrophysicists, psychologists) who require
explanations that reflect expert-level reasoning. However, current evaluation
schemes primarily emphasize plausibility or internal faithfulness of the
explanation, which fail to capture whether the content of the explanation truly
aligns with expert intuition. We formalize expert alignment as a criterion for
evaluating explanations with T-FIX, a benchmark spanning seven
knowledge-intensive domains. In collaboration with domain experts, we develop
novel metrics to measure the alignment of LLM explanations with expert
judgment.

</details>


### [16] [Plan of Knowledge: Retrieval-Augmented Large Language Models for Temporal Knowledge Graph Question Answering](https://arxiv.org/abs/2511.04072)
*Xinying Qian,Ying Zhang,Yu Zhao,Baohang Zhou,Xuhui Sui,Xiaojie Yuan*

Main category: cs.CL

TL;DR: 提出PoK框架，通过知识规划和对比时序检索器增强LLMs在时序知识图谱问答中的推理能力，显著提升检索精度和推理准确性


<details>
  <summary>Details</summary>
Motivation: 现有方法未能充分理解时间约束的复杂语义信息，而LLMs虽然语义理解能力强但时序推理能力有限且存在幻觉问题

Method: 设计知识规划模块将复杂时序问题分解为子目标序列，构建时序知识库和对比检索框架选择性检索语义和时间对齐的事实

Result: 在四个基准TKGQA数据集上的实验表明，PoK显著提升LLMs的检索精度和推理准确性，最多超越现有最佳方法56.0%

Conclusion: PoK通过结构化规划和时序知识检索的结合，有效增强时序推理的可解释性和事实一致性

Abstract: Temporal Knowledge Graph Question Answering (TKGQA) aims to answer
time-sensitive questions by leveraging factual information from Temporal
Knowledge Graphs (TKGs). While previous studies have employed pre-trained TKG
embeddings or graph neural networks to inject temporal knowledge, they fail to
fully understand the complex semantic information of time constraints.
Recently, Large Language Models (LLMs) have shown remarkable progress,
benefiting from their strong semantic understanding and reasoning
generalization capabilities. However, their temporal reasoning ability remains
limited. LLMs frequently suffer from hallucination and a lack of knowledge. To
address these limitations, we propose the Plan of Knowledge framework with a
contrastive temporal retriever, which is named PoK. Specifically, the proposed
Plan of Knowledge module decomposes a complex temporal question into a sequence
of sub-objectives from the pre-defined tools, serving as intermediate guidance
for reasoning exploration. In parallel, we construct a Temporal Knowledge Store
(TKS) with a contrastive retrieval framework, enabling the model to selectively
retrieve semantically and temporally aligned facts from TKGs. By combining
structured planning with temporal knowledge retrieval, PoK effectively enhances
the interpretability and factual consistency of temporal reasoning. Extensive
experiments on four benchmark TKGQA datasets demonstrate that PoK significantly
improves the retrieval precision and reasoning accuracy of LLMs, surpassing the
performance of the state-of-the-art TKGQA methods by 56.0% at most.

</details>


### [17] [The truth is no diaper: Human and AI-generated associations to emotional words](https://arxiv.org/abs/2511.04077)
*Špela Vintar,Jan Jona Javoršek*

Main category: cs.CL

TL;DR: 比较人类与大型语言模型在情感词汇联想行为上的差异，发现LLMs的联想更可预测、缺乏创造性，且会放大刺激词的情感负荷


<details>
  <summary>Details</summary>
Motivation: 探索人类与大型语言模型在词汇联想行为上的相似性，特别是对情感词汇的联想方式，以了解LLMs是否能模拟人类创造性的联想机制

Method: 通过比较人类参与者和大型语言模型对情感词汇的联想反应，分析联想重叠度、情感放大效应和创造性差异

Result: 人类与LLMs的联想重叠度中等，但LLMs的联想更可预测、缺乏创造性，且会放大刺激词的情感负荷

Conclusion: 大型语言模型在词汇联想行为上与人类存在显著差异，其联想机制更可预测但缺乏人类特有的创造性联想能力

Abstract: Human word associations are a well-known method of gaining insight into the
internal mental lexicon, but the responses spontaneously offered by human
participants to word cues are not always predictable as they may be influenced
by personal experience, emotions or individual cognitive styles. The ability to
form associative links between seemingly unrelated concepts can be the driving
mechanisms of creativity. We perform a comparison of the associative behaviour
of humans compared to large language models. More specifically, we explore
associations to emotionally loaded words and try to determine whether large
language models generate associations in a similar way to humans. We find that
the overlap between humans and LLMs is moderate, but also that the associations
of LLMs tend to amplify the underlying emotional load of the stimulus, and that
they tend to be more predictable and less creative than human ones.

</details>


### [18] [Improving the Performance of Radiology Report De-identification with Large-Scale Training and Benchmarking Against Cloud Vendor Methods](https://arxiv.org/abs/2511.04079)
*Eva Prakash,Maayane Attias,Pierre Chambon,Justin Xu,Steven Truong,Jean-Benoit Delbrouck,Tessa Cook,Curtis Langlotz*

Main category: cs.CL

TL;DR: 本文通过大规模多模态训练改进基于transformer的放射学报告去识别模型，在PHI检测方面超越了现有学术和商业系统，建立了临床文本安全处理的新基准。


<details>
  <summary>Details</summary>
Motivation: 提升放射学报告的自动去识别能力，通过扩展训练数据集和基准测试来改进受保护健康信息(PHI)检测，并与商业云供应商系统进行性能比较。

Method: 在现有最先进的transformer-based PHI去识别流水线基础上，使用斯坦福大学两个大型注释放射学语料库进行微调，引入额外的PHI类别(AGE)，并在斯坦福和宾夕法尼亚大学的测试集上评估token级PHI检测性能。

Result: 模型在宾夕法尼亚数据集上获得0.973的总体F1分数，在斯坦福数据集上获得0.996的总体F1分数，超越了之前的最高性能。在合成PHI评估中，50个独立去识别的宾夕法尼亚数据集显示一致的检测能力(总体F1:0.959)。模型在所有供应商系统中表现最佳。

Conclusion: 基于transformer的去识别模型在多样化放射学数据集上训练后，在PHI检测方面超越了先前的学术和商业系统，为安全的临床文本处理建立了新基准。

Abstract: Objective: To enhance automated de-identification of radiology reports by
scaling transformer-based models through extensive training datasets and
benchmarking performance against commercial cloud vendor systems for protected
health information (PHI) detection. Materials and Methods: In this
retrospective study, we built upon a state-of-the-art, transformer-based, PHI
de-identification pipeline by fine-tuning on two large annotated radiology
corpora from Stanford University, encompassing chest X-ray, chest CT,
abdomen/pelvis CT, and brain MR reports and introducing an additional PHI
category (AGE) into the architecture. Model performance was evaluated on test
sets from Stanford and the University of Pennsylvania (Penn) for token-level
PHI detection. We further assessed (1) the stability of synthetic PHI
generation using a "hide-in-plain-sight" method and (2) performance against
commercial systems. Precision, recall, and F1 scores were computed across all
PHI categories. Results: Our model achieved overall F1 scores of 0.973 on the
Penn dataset and 0.996 on the Stanford dataset, outperforming or maintaining
the previous state-of-the-art model performance. Synthetic PHI evaluation
showed consistent detectability (overall F1: 0.959 [0.958-0.960]) across 50
independently de-identified Penn datasets. Our model outperformed all vendor
systems on synthetic Penn reports (overall F1: 0.960 vs. 0.632-0.754).
Discussion: Large-scale, multimodal training improved cross-institutional
generalization and robustness. Synthetic PHI generation preserved data utility
while ensuring privacy. Conclusion: A transformer-based de-identification model
trained on diverse radiology datasets outperforms prior academic and commercial
systems in PHI detection and establishes a new benchmark for secure clinical
text processing.

</details>


### [19] [A Characterization of List Language Identification in the Limit](https://arxiv.org/abs/2511.04103)
*Moses Charikar,Chirag Pabbaraju,Ambuj Tewari*

Main category: cs.CL

TL;DR: 本文研究了k列表语言识别问题，给出了k列表识别的精确特征描述，并建立了统计设置下的识别速率界限。


<details>
  <summary>Details</summary>
Motivation: 经典的语言识别在极限中是不可能的，但最近在语言生成方面的积极结果促使重新审视这个问题，考虑学习者每次产生k个猜测的情况。

Method: 基于Angluin特征描述的递归版本，给出了k列表识别的精确特征描述，并将其分解为k个可识别语言集合的并集。

Result: 证明了语言集合可k列表识别的充要条件是它可以分解为k个可识别语言集合，并建立了指数识别速率的下界。

Conclusion: k列表识别提供了一个介于经典识别和生成之间的中间模型，具有理论上的优雅特征和实际应用价值。

Abstract: We study the problem of language identification in the limit, where given a
sequence of examples from a target language, the goal of the learner is to
output a sequence of guesses for the target language such that all the guesses
beyond some finite time are correct. Classical results of Gold showed that
language identification in the limit is impossible for essentially any
interesting collection of languages. Later, Angluin gave a precise
characterization of language collections for which this task is possible.
Motivated by recent positive results for the related problem of language
generation, we revisit the classic language identification problem in the
setting where the learner is given the additional power of producing a list of
$k$ guesses at each time step. The goal is to ensure that beyond some finite
time, one of the guesses is correct at each time step.
  We give an exact characterization of collections of languages that can be
$k$-list identified in the limit, based on a recursive version of Angluin's
characterization (for language identification with a list of size $1$). This
further leads to a conceptually appealing characterization: A language
collection can be $k$-list identified in the limit if and only if the
collection can be decomposed into $k$ collections of languages, each of which
can be identified in the limit (with a list of size $1$). We also use our
characterization to establish rates for list identification in the statistical
setting where the input is drawn as an i.i.d. stream from a distribution
supported on some language in the collection. Our results show that if a
collection is $k$-list identifiable in the limit, then the collection can be
$k$-list identified at an exponential rate, and this is best possible. On the
other hand, if a collection is not $k$-list identifiable in the limit, then it
cannot be $k$-list identified at any rate that goes to zero.

</details>


### [20] [Batch Prompting Suppresses Overthinking Reasoning Under Constraint: How Batch Prompting Suppresses Overthinking in Reasoning Models](https://arxiv.org/abs/2511.04108)
*Wenmo Qiu,Saurabh Srivastava*

Main category: cs.CL

TL;DR: 批处理提示不仅降低LLM推理成本，还能作为推理时正则化器，提高多步推理的准确性和效率，减少3-5倍的推理令牌使用，抑制过度思考，促进更果断的回答。


<details>
  <summary>Details</summary>
Motivation: 探索批处理在大型推理模型中的额外好处，超越传统的推理成本优化，研究其作为推理时正则化器的潜力。

Method: 在13个多样化基准测试上进行综合研究，通过详细的行为分析考察批处理对模型推理行为的影响。

Result: 批处理显著提高了准确性，大幅减少了推理令牌使用（通常3-5倍），抑制了过度思考，减少了犹豫语言，促进了更果断的回答，并观察到批次内集体效应。

Conclusion: 批处理不仅是吞吐量优化工具，更是强大的推理时正则化器，能够实现更高效可靠的LLM推理。

Abstract: Recent work has explored batch prompting as a strategy to amortize inference
cost in large language models (LLMs). In this paper, we show that batching
offers an additional, underappreciated benefit: it regularizes model behavior
during multi-step reasoning for Large Reasoning Models (LRMs). We conduct a
comprehensive study across 13 diverse benchmarks and observe that batching
improves accuracy while substantially reducing reasoning token usage, often by
3x-5x. Through detailed behavioral analysis, we find that batching suppresses
overthinking, reduces hedging language (e.g., repetitive self-corrections), and
encourages more decisive answers. Surprisingly, we also observe emergent
collective effects in batched inference: models often generalize patterns from
earlier examples to solve harder ones in the same batch. These findings
position batching not just as a throughput optimization, but as a powerful
inference-time regularizer for more efficient and reliable LLM reasoning.

</details>


### [21] [RIDE: Difficulty Evolving Perturbation with Item Response Theory for Mathematical Reasoning](https://arxiv.org/abs/2511.04120)
*Xinyuan Li,Murong Xu,Wenbiao Tao,Hanlun Zhu,Yike Zhao,Jipeng Zhang,Yunshi Lan*

Main category: cs.CL

TL;DR: RIDE是一个基于项目反应理论的对抗性问题重写框架，用于评估LLMs的真实数学推理能力，通过生成难度可控的变体问题来暴露模型的鲁棒性限制。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在数学推理上的高表现可能源于训练数据泄露或表面模式匹配，而非真正的推理能力，需要对抗性评估来测量真实能力。

Method: 使用项目反应理论测量问题难度，通过35个LLMs模拟学生构建难度排序器，在强化学习中使用该排序器指导问题重写模型生成难度可控的变体问题。

Result: 在竞赛级数学基准上应用RIDE，生成的扰动版本使先进LLMs性能平均下降21.73%，暴露了数学推理的鲁棒性限制。

Conclusion: RIDE框架有效验证了LLMs数学推理能力的局限性，为评估真实推理能力提供了有效方法。

Abstract: Large language models (LLMs) achieve high performance on mathematical
reasoning, but these results can be inflated by training data leakage or
superficial pattern matching rather than genuine reasoning. To this end, an
adversarial perturbation-based evaluation is needed to measure true
mathematical reasoning ability. Current rule-based perturbation methods often
generate ill-posed questions and impede the systematic evaluation of question
difficulty and the evolution of benchmarks. To bridge this gap, we propose
RIDE, a novel adversarial question-rewriting framework that leverages Item
Response Theory (IRT) to rigorously measure question difficulty and to generate
intrinsically more challenging, well-posed variations of mathematical problems.
We employ 35 LLMs to simulate students and build a difficulty ranker from their
responses. This ranker provides a reward signal during reinforcement learning
and guides a question-rewriting model to reformulate existing questions across
difficulty levels. Applying RIDE to competition-level mathematical benchmarks
yields perturbed versions that degrade advanced LLM performance, with
experiments showing an average 21.73% drop across 26 models, thereby exposing
limited robustness in mathematical reasoning and confirming the validity of our
evaluation approach.

</details>


### [22] [CantoASR: Prosody-Aware ASR-LALM Collaboration for Low-Resource Cantonese](https://arxiv.org/abs/2511.04139)
*Dazhong Chen,Yi-Cheng Lin,Yuchen Huang,Ziwei Gong,Di Jiang,Zeying Xie,Yi R.,Fung*

Main category: cs.CL

TL;DR: CantoASR是一个针对低资源粤语的协作式ASR-LALM错误校正框架，通过整合强制对齐、LoRA微调的Whisper和指令调优的Qwen-Audio，显著提升了粤语语音识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 粤语作为一种低资源语言，由于标注数据有限、六个声调、变调和口音变化，现有ASR模型（如Whisper）词错误率较高。需要结合声学和韵律特征来提升识别性能。

Method: 提出CantoASR框架：1）使用强制对齐提取声学特征；2）LoRA微调Whisper提升声调辨别能力；3）指令调优Qwen-Audio进行韵律感知校正。

Result: 在自发粤语数据上的评估显示，相比Whisper-Large-V3，CantoASR在字符错误率（CER）方面取得了显著提升。

Conclusion: 将声学线索与LALM推理相结合，为低资源声调语言和方言的ASR提供了一种可扩展的策略。

Abstract: Automatic speech recognition (ASR) is critical for language accessibility,
yet low-resource Cantonese remains challenging due to limited annotated data,
six lexical tones, tone sandhi, and accent variation. Existing ASR models, such
as Whisper, often suffer from high word error rates. Large audio-language
models (LALMs), in contrast, can leverage broader contextual reasoning but
still require explicit tonal and prosodic acoustic cues. We introduce CantoASR,
a collaborative ASR-LALM error correction framework that integrates forced
alignment for acoustic feature extraction, a LoRA-finetuned Whisper for
improved tone discrimination, and an instruction-tuned Qwen-Audio for
prosody-aware correction. Evaluations on spontaneous Cantonese data show
substantial CER gains over Whisper-Large-V3. These findings suggest that
integrating acoustic cues with LALM reasoning provides a scalable strategy for
low-resource tonal and dialectal ASR.

</details>


### [23] [BAPPA: Benchmarking Agents, Plans, and Pipelines for Automated Text-to-SQL Generation](https://arxiv.org/abs/2511.04153)
*Fahim Ahmed,Md Mubtasim Ahasan,Jahir Sadik Monon,Muntasir Wahed,M Ashraful Amin,A K M Mahbubur Rahman,Amin Ahsan Ali*

Main category: cs.CL

TL;DR: 本文探索了三种多智能体LLM流水线来提升Text-to-SQL性能，发现多智能体讨论可以显著提升小模型表现，其中LLM Reasoner-Coder流水线效果最佳，能将准确率从52.4%提升到56.4%。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在从自然语言生成SQL时面临模式规模大和复杂推理的挑战，先前工作主要关注复杂不实用的旗舰模型流水线，而忽视了更小更高效的模型。

Method: 提出了三种多智能体LLM流水线：(1)多智能体讨论流水线，智能体迭代批评和优化SQL查询，法官合成最终答案；(2)规划器-编码器流水线，思考模型规划器生成逐步SQL生成计划，编码器合成查询；(3)编码器-聚合器流水线，多个编码器独立生成SQL查询，推理智能体选择最佳查询。

Result: 在Bird-Bench Mini-Dev集上的实验显示，多智能体讨论可以提升小模型性能，Qwen2.5-7b-Instruct在执行准确率上经过三轮讨论后提升了10.6%。LLM Reasoner-Coder流水线效果最佳，DeepSeek-R1-32B和QwQ-32B规划器将Gemma 3 27B IT准确率从52.4%提升到最高的56.4%。

Conclusion: 多智能体方法可以有效提升Text-to-SQL系统的性能，特别是对小模型有显著改进，其中规划器-编码器流水线表现最优。

Abstract: Text-to-SQL systems provide a natural language interface that can enable even
laymen to access information stored in databases. However, existing Large
Language Models (LLM) struggle with SQL generation from natural instructions
due to large schema sizes and complex reasoning. Prior work often focuses on
complex, somewhat impractical pipelines using flagship models, while smaller,
efficient models remain overlooked. In this work, we explore three multi-agent
LLM pipelines, with systematic performance benchmarking across a range of small
to large open-source models: (1) Multi-agent discussion pipeline, where agents
iteratively critique and refine SQL queries, and a judge synthesizes the final
answer; (2) Planner-Coder pipeline, where a thinking model planner generates
stepwise SQL generation plans and a coder synthesizes queries; and (3)
Coder-Aggregator pipeline, where multiple coders independently generate SQL
queries, and a reasoning agent selects the best query. Experiments on the
Bird-Bench Mini-Dev set reveal that Multi-Agent discussion can improve small
model performance, with up to 10.6% increase in Execution Accuracy for
Qwen2.5-7b-Instruct seen after three rounds of discussion. Among the pipelines,
the LLM Reasoner-Coder pipeline yields the best results, with DeepSeek-R1-32B
and QwQ-32B planners boosting Gemma 3 27B IT accuracy from 52.4% to the highest
score of 56.4%. Codes are available at
https://github.com/treeDweller98/bappa-sql.

</details>


### [24] [Trustworthy LLM-Mediated Communication: Evaluating Information Fidelity in LLM as a Communicator (LAAC) Framework in Multiple Application Domains](https://arxiv.org/abs/2511.04184)
*Mohammed Musthafa Rafi,Adarsh Krishnamurthy,Aditya Balu*

Main category: cs.CL

TL;DR: LAAC提出将LLM作为智能通信中介，通过结构化对话捕获发送者意图，促进真实知识交换，避免AI生成内容的膨胀-压缩循环。


<details>
  <summary>Details</summary>
Motivation: 当前AI生成内容泛滥导致通信剧场化，发送者用LLM将简单想法膨胀为冗长内容，接收者用LLM压缩回摘要，双方都不接触真实内容。

Method: 采用多智能体架构，通过结构化对话捕获发送者意图，在学术论文、提案、专业邮件等场景中作为通信中介。

Result: 初步发现存在可测量的信任差距，包括信息捕获保真度、可重现性和查询响应完整性方面的问题。

Conclusion: 在LAAC可靠部署于高风险通信场景之前，必须解决这些信任维度上的差距。

Abstract: The proliferation of AI-generated content has created an absurd communication
theater where senders use LLMs to inflate simple ideas into verbose content,
recipients use LLMs to compress them back into summaries, and as a consequence
neither party engage with authentic content. LAAC (LLM as a Communicator)
proposes a paradigm shift - positioning LLMs as intelligent communication
intermediaries that capture the sender's intent through structured dialogue and
facilitate genuine knowledge exchange with recipients. Rather than perpetuating
cycles of AI-generated inflation and compression, LAAC enables authentic
communication across diverse contexts including academic papers, proposals,
professional emails, and cross-platform content generation. However, deploying
LLMs as trusted communication intermediaries raises critical questions about
information fidelity, consistency, and reliability. This position paper
systematically evaluates the trustworthiness requirements for LAAC's deployment
across multiple communication domains. We investigate three fundamental
dimensions: (1) Information Capture Fidelity - accuracy of intent extraction
during sender interviews across different communication types, (2)
Reproducibility - consistency of structured knowledge across multiple
interaction instances, and (3) Query Response Integrity - reliability of
recipient-facing responses without hallucination, source conflation, or
fabrication. Through controlled experiments spanning multiple LAAC use cases,
we assess these trust dimensions using LAAC's multi-agent architecture.
Preliminary findings reveal measurable trust gaps that must be addressed before
LAAC can be reliably deployed in high-stakes communication scenarios.

</details>


### [25] [Computational Turing Test Reveals Systematic Differences Between Human and AI Language](https://arxiv.org/abs/2511.04195)
*Nicolò Pagan,Petter Törnberg,Christopher A. Bail,Anikó Hannák,Christopher Barrie*

Main category: cs.CL

TL;DR: 本文提出了一个计算图灵测试框架来评估LLM生成文本的人类相似度，发现即使经过校准，LLM输出仍与人类文本有显著差异，特别是在情感表达方面。


<details>
  <summary>Details</summary>
Motivation: 当前社会科学中广泛使用LLM模拟人类行为，但缺乏对LLM生成文本真实性的可靠验证工具，现有评估方法依赖人类判断且不可靠。

Method: 引入计算图灵测试框架，结合BERT检测性、语义相似度等聚合指标与可解释语言特征；系统比较9个开源LLM在5种校准策略下的表现。

Result: LLM输出与人类文本明显可区分，指令调优模型表现不如基础模型，模型规模扩大不提升人类相似度，存在人类相似度与语义保真度的权衡。

Conclusion: 提供了可扩展的验证和校准框架，揭示了LLM在捕捉人类交流方面的当前局限性，对LLM模拟研究提出警示。

Abstract: Large language models (LLMs) are increasingly used in the social sciences to
simulate human behavior, based on the assumption that they can generate
realistic, human-like text. Yet this assumption remains largely untested.
Existing validation efforts rely heavily on human-judgment-based evaluations --
testing whether humans can distinguish AI from human output -- despite evidence
that such judgments are blunt and unreliable. As a result, the field lacks
robust tools for assessing the realism of LLM-generated text or for calibrating
models to real-world data. This paper makes two contributions. First, we
introduce a computational Turing test: a validation framework that integrates
aggregate metrics (BERT-based detectability and semantic similarity) with
interpretable linguistic features (stylistic markers and topical patterns) to
assess how closely LLMs approximate human language within a given dataset.
Second, we systematically compare nine open-weight LLMs across five calibration
strategies -- including fine-tuning, stylistic prompting, and context retrieval
-- benchmarking their ability to reproduce user interactions on X (formerly
Twitter), Bluesky, and Reddit. Our findings challenge core assumptions in the
literature. Even after calibration, LLM outputs remain clearly distinguishable
from human text, particularly in affective tone and emotional expression.
Instruction-tuned models underperform their base counterparts, and scaling up
model size does not enhance human-likeness. Crucially, we identify a trade-off:
optimizing for human-likeness often comes at the cost of semantic fidelity, and
vice versa. These results provide a much-needed scalable framework for
validation and calibration in LLM simulations -- and offer a cautionary note
about their current limitations in capturing human communication.

</details>


### [26] [LLM-as-a-Judge is Bad, Based on AI Attempting the Exam Qualifying for the Member of the Polish National Board of Appeal](https://arxiv.org/abs/2511.04205)
*Michał Karp,Anna Kubaszewska,Magdalena Król,Robert Król,Aleksander Smywiński-Pohl,Mateusz Szymański,Witold Wydmański*

Main category: cs.CL

TL;DR: 评估当前大语言模型能否通过波兰国家上诉庭的官方资格考试，测试了模型作为考生和自动评分者的表现，发现虽然知识测试成绩尚可，但实践写作部分均未通过，且自动评分与官方评判存在差异。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在法律专业资格考试中的实际表现，验证其是否具备替代人类法官或独立考官的能力，特别是在波兰公共采购裁决领域。

Method: 构建混合信息检索和提取管道，测试多个LLM（包括GPT-4.1、Claude 4 Sonnet和Bielik-11B-v2.6）在闭卷和检索增强生成设置下的表现，采用'LLM-as-a-judge'方法自动评估模型生成的答案。

Result: 模型在知识测试中取得满意分数，但在实践写作部分均未达到及格线，'LLM-as-a-judge'的评估结果与官方审查委员会的判断经常不一致。

Conclusion: 尽管技术进步迅速，当前的大语言模型尚无法在波兰公共采购裁决中替代人类法官或独立考官，存在幻觉、错误引用法律条款、逻辑论证薄弱等关键限制。

Abstract: This study provides an empirical assessment of whether current large language
models (LLMs) can pass the official qualifying examination for membership in
Poland's National Appeal Chamber (Krajowa Izba Odwo{\l}awcza). The authors
examine two related ideas: using LLM as actual exam candidates and applying the
'LLM-as-a-judge' approach, in which model-generated answers are automatically
evaluated by other models. The paper describes the structure of the exam, which
includes a multiple-choice knowledge test on public procurement law and a
written judgment, and presents the hybrid information recovery and extraction
pipeline built to support the models. Several LLMs (including GPT-4.1, Claude 4
Sonnet and Bielik-11B-v2.6) were tested in closed-book and various
Retrieval-Augmented Generation settings. The results show that although the
models achieved satisfactory scores in the knowledge test, none met the passing
threshold in the practical written part, and the evaluations of the
'LLM-as-a-judge' often diverged from the judgments of the official examining
committee. The authors highlight key limitations: susceptibility to
hallucinations, incorrect citation of legal provisions, weaknesses in logical
argumentation, and the need for close collaboration between legal experts and
technical teams. The findings indicate that, despite rapid technological
progress, current LLMs cannot yet replace human judges or independent examiners
in Polish public procurement adjudication.

</details>


### [27] [REMIND: Input Loss Landscapes Reveal Residual Memorization in Post-Unlearning LLMs](https://arxiv.org/abs/2511.04228)
*Liran Cohen,Yaniv Nemcovesky,Avi Mendelson*

Main category: cs.CL

TL;DR: REMIND是一种新的机器学习遗忘评估方法，通过分析模型在输入微小变化时的损失模式来检测未学习数据的残余影响，比现有单点评估方法更敏感可靠。


<details>
  <summary>Details</summary>
Motivation: 现有遗忘评估方法仅在单个输入层面评估，可能忽略语义相似示例中的残余影响，这会危及隐私并导致间接信息泄露。需要更敏感的评估方法来确保模型真正忘记目标数据。

Method: REMIND通过分析模型在输入微小变化时的损失动态来检测残余记忆。未学习数据会产生更平坦、不陡峭的损失景观，而保留或无关数据则表现出更尖锐、易变的模式。

Result: REMIND在仅需查询访问的条件下优于现有方法，在不同模型、数据集和转述输入中表现出鲁棒性，为现实世界部署提供了实用框架。

Conclusion: REMIND提供了更敏感和可解释的遗忘效果度量，为语言模型中的遗忘评估提供了可靠框架，并对记忆和遗忘提供了新的视角。

Abstract: Machine unlearning aims to remove the influence of specific training data
from a model without requiring full retraining. This capability is crucial for
ensuring privacy, safety, and regulatory compliance. Therefore, verifying
whether a model has truly forgotten target data is essential for maintaining
reliability and trustworthiness. However, existing evaluation methods often
assess forgetting at the level of individual inputs. This approach may overlook
residual influence present in semantically similar examples. Such influence can
compromise privacy and lead to indirect information leakage. We propose REMIND
(Residual Memorization In Neighborhood Dynamics), a novel evaluation method
aiming to detect the subtle remaining influence of unlearned data and classify
whether the data has been effectively forgotten. REMIND analyzes the model's
loss over small input variations and reveals patterns unnoticed by single-point
evaluations. We show that unlearned data yield flatter, less steep loss
landscapes, while retained or unrelated data exhibit sharper, more volatile
patterns. REMIND requires only query-based access, outperforms existing methods
under similar constraints, and demonstrates robustness across different models,
datasets, and paraphrased inputs, making it practical for real-world
deployment. By providing a more sensitive and interpretable measure of
unlearning effectiveness, REMIND provides a reliable framework to assess
unlearning in language models. As a result, REMIND offers a novel perspective
on memorization and unlearning.

</details>


### [28] [Reusing Pre-Training Data at Test Time is a Compute Multiplier](https://arxiv.org/abs/2511.04234)
*Alex Fang,Thomas Voice,Ruoming Pang,Ludwig Schmidt,Tom Gunter*

Main category: cs.CL

TL;DR: 该论文通过检索增强生成和测试时计算来量化预训练过程中数据集价值的损失情况，发现在MMLU、Math-500和SimpleQA等任务上，预训练后从标准开源数据集检索能显著提升准确率，表明当前预训练方法未能充分利用现有数据集的信息。


<details>
  <summary>Details</summary>
Motivation: 理解预训练装置从数据中提取知识和思想的效率，量化预训练过程中遗留的数据集价值，以及这种价值随模型规模的变化情况。

Method: 使用检索增强生成和测试时计算作为量化工具，在预训练后从标准开源数据集进行检索，并利用额外计算解析检索到的上下文。

Result: 在MMLU、Math-500和SimpleQA任务上观察到显著的准确率提升，且这些提升在去污染后仍然存在。在MMLU上，检索相当于预训练单独使用的约5倍计算乘数。通过额外测试时计算解析检索上下文，LLaMA 3.1 8B模型在MMLU上获得10个百分点的改进。

Conclusion: 当前的预训练方法未能充分利用现有预训练数据集中的信息，仍有显著的改进空间。

Abstract: Large language models learn from their vast pre-training corpora, gaining the
ability to solve an ever increasing variety of tasks; yet although researchers
work to improve these datasets, there is little effort to understand how
efficient the pre-training apparatus is at extracting ideas and knowledge from
the data. In this work, we use retrieval augmented generation along with
test-time compute as a way to quantify how much dataset value was left behind
by the process of pre-training, and how this changes across scale. We
demonstrate that pre-training then retrieving from standard and largely
open-sourced datasets results in significant accuracy gains in MMLU, Math-500,
and SimpleQA, which persist through decontamination. For MMLU we observe that
retrieval acts as a ~5x compute multiplier versus pre-training alone. We show
that these results can be further improved by leveraging additional compute at
test time to parse the retrieved context, demonstrating a 10 percentage point
improvement on MMLU for the public LLaMA 3.1 8B model. Overall, our results
suggest that today's pre-training methods do not make full use of the
information in existing pre-training datasets, leaving significant room for
progress.

</details>


### [29] [Efficient Topic Extraction via Graph-Based Labeling: A Lightweight Alternative to Deep Models](https://arxiv.org/abs/2511.04248)
*Salma Mekaoui,Hiba Sofyan,Imane Amaaz,Imane Benchrif,Arsalane Zarghili,Ilham Chaker,Nikola S. Nikolov*

Main category: cs.CL

TL;DR: 提出一种基于图的方法进行主题标注，相比传统基准方法在BERTScore和余弦相似度上表现更好，与ChatGPT-3.5效果相当但计算效率更高。


<details>
  <summary>Details</summary>
Motivation: 现有主题建模方法生成的主题词分布缺乏清晰可解释性，且大多数方法计算成本高昂，需要更高效的主题标注方法。

Method: 使用基于图的方法，通过丰富主题词的语义相关词汇并分析它们之间的关系来推导合适的主题标签。

Result: 在两个数据集上的比较研究表明，该方法在BERTScore和余弦相似度指标上优于传统基准方法，与ChatGPT-3.5效果相当但计算效率更高。

Conclusion: 图基方法为高效的主题标注提供了可行方案，未来可进一步探索提升可解释性和自动化的研究方向。

Abstract: Extracting topics from text has become an essential task, especially with the
rapid growth of unstructured textual data. Most existing works rely on highly
computational methods to address this challenge. In this paper, we argue that
probabilistic and statistical approaches, such as topic modeling (TM), can
offer effective alternatives that require fewer computational resources. TM is
a statistical method that automatically discovers topics in large collections
of unlabeled text; however, it produces topics as distributions of
representative words, which often lack clear interpretability. Our objective is
to perform topic labeling by assigning meaningful labels to these sets of
words. To achieve this without relying on computationally expensive models, we
propose a graph-based approach that not only enriches topic words with
semantically related terms but also explores the relationships among them. By
analyzing these connections within the graph, we derive suitable labels that
accurately capture each topic's meaning. We present a comparative study between
our proposed method and several benchmarks, including ChatGPT-3.5, across two
different datasets. Our method achieved consistently better results than
traditional benchmarks in terms of BERTScore and cosine similarity and produced
results comparable to ChatGPT-3.5, while remaining computationally efficient.
Finally, we discuss future directions for topic labeling and highlight
potential research avenues for enhancing interpretability and automation.

</details>


### [30] [SSPO: Subsentence-level Policy Optimization](https://arxiv.org/abs/2511.04256)
*Kun Yang,Zikang chen,Yanmeng Wang,Zhigen Li*

Main category: cs.CL

TL;DR: SSPO提出句子级别的重要性比率，在GRPO和GSPO之间取得平衡，避免训练崩溃和高方差，同时防止整个响应被裁剪机制丢弃，在五个数据集上平均得分46.57，超越GRPO和GSPO。


<details>
  <summary>Details</summary>
Motivation: 现有的RLVR算法如GRPO存在策略更新不稳定问题，GSPO存在采样数据利用率低的问题，需要一种平衡的解决方案。

Method: SSPO应用句子级别的重要性比率，并在PPO-CLIP中使用句子熵来稳定调整裁剪边界，鼓励高熵token探索，缩小低熵token的裁剪范围。

Result: SSPO在五个数据集上平均得分46.57，超过GRPO(43.01)和GSPO(44.42)，在三个数据集上达到最先进性能。

Conclusion: SSPO通过采用GSPO的优点但拒绝其缺点，有效利用了生成数据，在RLVR中取得了更好的性能。

Abstract: As a significant part of post-training of the Large Language Models (LLMs),
Reinforcement Learning from Verifiable Reward (RLVR) has greatly improved LLMs'
reasoning skills. However, some RLVR algorithms, such as GRPO (Group Relative
Policy Optimization) and GSPO (Group Sequence Policy Optimization), are
observed to suffer from unstable policy updates and low usage of sampling data,
respectively. The importance ratio of GRPO is calculated at the token level,
which focuses more on optimizing a single token. This will be easily affected
by outliers, leading to model training collapse. GSPO proposed the calculation
of the response level importance ratio, which solves the problem of high
variance and training noise accumulation in the calculation of the GRPO
importance ratio. However, since all the response tokens share a common
importance ratio, extreme values can easily raise or lower the overall mean,
leading to the entire response being mistakenly discarded, resulting in a
decrease in the utilization of sampled data. This paper introduces SSPO, which
applies sentence-level importance ratio, taking the balance between GRPO and
GSPO. SSPO not only avoids training collapse and high variance, but also
prevents the whole response tokens from being abandoned by the clipping
mechanism. Furthermore, we apply sentence entropy to PPO-CLIP to steadily
adjust the clipping bounds, encouraging high-entropy tokens to explore and
narrow the clipping range of low-entropy tokens. In particular, SSPO achieves
an average score of 46.57 across five datasets, surpassing GRPO (43.01) and
GSPO (44.42), and wins state-of-the-art performance on three datasets. These
results highlight SSPO's effectiveness in leveraging generated data by taking
the essence of GSPO but rejecting its shortcomings.

</details>


### [31] [Dynamic Jointly Batch Selection for Data Efficient Machine Translation Fine-Tuning](https://arxiv.org/abs/2511.04406)
*Mohammad Amin Ghanizadeh,Mohammad Javad Dousti*

Main category: cs.CL

TL;DR: 提出了一种用于机器翻译微调的数据选择方法，通过定义可学习性分数来评估数据点的训练效用，结合批量选择策略优化训练效率，在多个语言对上实现了5倍的数据效率提升。


<details>
  <summary>Details</summary>
Motivation: 数据质量及其有效选择是提高机器翻译模型性能的基础，对于构建稳健可靠的翻译系统至关重要。

Method: 利用学习器模型与预训练参考模型之间的协同作用，通过定义可学习性分数系统评估数据点的训练效用，并采用考虑数据点间相互依赖关系的批量选择策略。

Result: 在英波等语言对上的实验表明，该方法相比iid基线实现了5倍的数据效率提升，使用缓存嵌入时计算效率提高24%，并获得了优于随机选择方法的翻译性能。

Conclusion: 该方法能够有效提升机器翻译微调的数据效率和计算效率，同时改善模型的泛化能力。

Abstract: Data quality and its effective selection are fundamental to improving the
performance of machine translation models, serving as cornerstones for
achieving robust and reliable translation systems. This paper presents a data
selection methodology specifically designed for fine-tuning machine translation
systems, which leverages the synergy between a learner model and a pre-trained
reference model to enhance overall training effectiveness. By defining a
learnability score, our approach systematically evaluates the utility of data
points for training, ensuring that only the most relevant and impactful
examples contribute to the fine-tuning process. Furthermore, our method employs
a batch selection strategy which considers interdependencies among data points,
optimizing the efficiency of the training process while maintaining a focus on
data relevance. Experiments on English to Persian and several other language
pairs using an mBART model fine-tuned on the CCMatrix dataset demonstrate that
our method can achieve up to a fivefold improvement in data efficiency compared
to an iid baseline. Experimental results indicate that our approach improves
computational efficiency by 24 when utilizing cached embeddings, as it requires
fewer training data points. Additionally, it enhances generalization, resulting
in superior translation performance compared to random selection method.

</details>


### [32] [If I Could Turn Back Time: Temporal Reframing as a Historical Reasoning Task for LLMs](https://arxiv.org/abs/2511.04432)
*Lars Bungum,Charles Yijia Huang,Abeer Kashar*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在时间推理方面的能力，通过使用1940年挪威书籍中的琐事问题，让LLMs以1940年的视角回答问题，并比较了英语和挪威语提示的效果。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在时间推理方面的表现，特别是当要求模型以历史时间点的知识来回答问题时，以及不同语言提示对结果的影响。

Method: 使用1940年挪威书籍中的琐事问题，分别用英语和挪威语提示多个LLM模型（DeepSeek-R1、Gemma3、Qwen3、Llama3.1等），让模型以1940年的视角回答问题，采用LLM-as-judge方法进行评分。

Result: 英语提示始终比挪威语提示效果更好，这是一个意外结果；使用更大的LLM模型能改善结果；测试了多个模型家族，包括专门为挪威语设计的最大可用LLM。

Conclusion: LLMs在时间推理方面表现出一定的能力，但语言选择对结果有显著影响，英语提示优于挪威语提示，模型规模对性能有积极影响。

Abstract: In this study, we experiment with the ability of LLMs to do temporal
reasoning. Using a Norwegian book from 1940 containing trivia questions, we
prompt the LLMs to answer the questions as if it were 1940. We also pose the
questions in both English and Norwegian. Correct answers are often presented as
sentences, and grading is done by means of LLM-as-judge, with sampled checks by
a native speaker. Prompting in English consistently gave better results than in
Norwegian, an unexpected result. In contrast, using larger LLMs improved
results. We tested the DeepSeek-R1, Gemma3, Qwen3, and Llama3.1 model families,
and also the largest available LLM especially crafted for Norwegian.

</details>


### [33] [Probabilistic Textual Time Series Depression Detection](https://arxiv.org/abs/2511.04476)
*Fabian Schmidt,Seyedehmoniba Ravan,Vladimir Vlassov*

Main category: cs.CL

TL;DR: PTTSD是一个概率性文本时间序列抑郁症检测框架，通过临床访谈的逐句分析预测PHQ-8抑郁评分，同时建模时间不确定性。


<details>
  <summary>Details</summary>
Motivation: 现有抑郁症预测模型缺乏不确定性估计和时间建模能力，而准确的、可解释的预测对临床决策支持至关重要。

Method: 提出PTTSD框架，包含序列到序列和序列到单一两种变体，结合双向LSTM、自注意力机制、残差连接，使用高斯或学生t分布输出头，通过负对数似然训练。

Result: 在E-DAIC和DAIC-WOZ数据集上达到文本系统的最优性能（MAE分别为3.85和3.55），产生良好校准的预测区间。消融实验证实了注意力和概率建模的价值。

Conclusion: 该框架提供了可解释且具有临床相关性的不确定性感知预测，三部分校准分析和定性案例研究进一步突出了其临床实用性。

Abstract: Accurate and interpretable predictions of depression severity are essential
for clinical decision support, yet existing models often lack uncertainty
estimates and temporal modeling. We propose PTTSD, a Probabilistic Textual Time
Series Depression Detection framework that predicts PHQ-8 scores from
utterance-level clinical interviews while modeling uncertainty over time. PTTSD
includes sequence-to-sequence and sequence-to-one variants, both combining
bidirectional LSTMs, self-attention, and residual connections with Gaussian or
Student-t output heads trained via negative log-likelihood. Evaluated on E-DAIC
and DAIC-WOZ, PTTSD achieves state-of-the-art performance among text-only
systems (e.g., MAE = 3.85 on E-DAIC, 3.55 on DAIC) and produces well-calibrated
prediction intervals. Ablations confirm the value of attention and
probabilistic modeling, while comparisons with MentalBERT establish generality.
A three-part calibration analysis and qualitative case studies further
highlight the interpretability and clinical relevance of uncertainty-aware
forecasting.

</details>


### [34] [ThaiOCRBench: A Task-Diverse Benchmark for Vision-Language Understanding in Thai](https://arxiv.org/abs/2511.04479)
*Surapon Nonesung,Teetouch Jaknamon,Sirinya Chaiophat,Natapong Nitarach,Chanakan Wittayasakpan,Warit Sirichotedumrong,Adisai Na-Thalang,Kunat Pipatanakul*

Main category: cs.CL

TL;DR: ThaiOCRBench是首个针对泰语文档视觉理解的综合基准测试，包含2,808个样本和13个任务类别，评估了多种VLMs在零样本设置下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注高资源语言，泰语在多模态建模中代表性不足，特别是在需要文档结构理解的任务中。

Method: 构建了多样化的人工标注数据集，在零样本设置下评估了包括专有和开源系统在内的多种最先进VLMs。

Result: 专有模型（如Gemini 2.5 Pro）表现优于开源模型，特别是在细粒度文本识别和手写内容提取任务中，开源模型性能下降最明显。

Conclusion: ThaiOCRBench为评估低资源、复杂脚本环境下的VLMs提供了标准化框架，并识别了语言偏见、结构不匹配和幻觉内容等关键挑战。

Abstract: We present ThaiOCRBench, the first comprehensive benchmark for evaluating
vision-language models (VLMs) on Thai text-rich visual understanding tasks.
Despite recent progress in multimodal modeling, existing benchmarks
predominantly focus on high-resource languages, leaving Thai underrepresented,
especially in tasks requiring document structure understanding. ThaiOCRBench
addresses this gap by offering a diverse, human-annotated dataset comprising
2,808 samples across 13 task categories. We evaluate a wide range of
state-of-the-art VLMs in a zero-shot setting, spanning both proprietary and
open-source systems. Results show a significant performance gap, with
proprietary models (e.g., Gemini 2.5 Pro) outperforming open-source
counterparts. Notably, fine-grained text recognition and handwritten content
extraction exhibit the steepest performance drops among open-source models.
Through detailed error analysis, we identify key challenges such as language
bias, structural mismatch, and hallucinated content. ThaiOCRBench provides a
standardized framework for assessing VLMs in low-resource, script-complex
settings, and provides actionable insights for improving Thai-language document
understanding.

</details>


### [35] [RUST-BENCH: Benchmarking LLM Reasoning on Unstructured Text within Structured Tables](https://arxiv.org/abs/2511.04491)
*Nikhil Abhyankar,Purvi Chaurasia,Sanchit Kabra,Ananya Srivastava,Vivek Gupta,Chandan K. Reddy*

Main category: cs.CL

TL;DR: RUST-BENCH是一个新的表格推理基准测试，包含7966个问题，来自2031个真实世界表格，涵盖科学和体育两个领域，旨在评估LLM在复杂、异构表格上的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有表格推理基准测试主要使用小型、统一的表格，无法反映真实世界数据的复杂性，也无法全面评估LLM的推理能力。真实表格通常很长、异构且领域特定，需要跨数千个标记进行多跳推理。

Method: 引入RUST-BENCH基准测试，包含两个领域：RB-Science（NSF资助记录）和RB-Sports（NBA统计数据），共7966个问题来自2031个真实表格，评估LLM在规模、异构性、领域特定性和推理复杂性方面的表现。

Result: 实验表明，开源和专有LLM在处理异构模式和复杂多跳推理方面都存在困难，揭示了当前架构和提示策略的持续弱点。

Conclusion: RUST-BENCH为推进表格推理研究建立了一个具有挑战性的新测试平台。

Abstract: Existing tabular reasoning benchmarks mostly test models on small, uniform
tables, underrepresenting the complexity of real-world data and giving an
incomplete view of Large Language Models' (LLMs) reasoning abilities. Real
tables are long, heterogeneous, and domain-specific, mixing structured fields
with free text and requiring multi-hop reasoning across thousands of tokens. To
address this gap, we introduce RUST-BENCH, a benchmark of 7966 questions from
2031 real-world tables spanning two domains: i) RB-Science (NSF grant records)
and ii) RB-Sports (NBA statistics). Unlike prior work, RUST-BENCH evaluates
LLMs jointly across scale, heterogeneity, domain specificity, and reasoning
complexity. Experiments with open-source and proprietary models show that LLMs
struggle with heterogeneous schemas and complex multi-hop inference, revealing
persistent weaknesses in current architectures and prompting strategies.
RUST-BENCH establishes a challenging new testbed for advancing tabular
reasoning research.

</details>


### [36] [OUNLP at TSAR 2025 Shared Task: Multi-Round Text Simplifier via Code Generation](https://arxiv.org/abs/2511.04495)
*Cuong Huynh,Jie Cao*

Main category: cs.CL

TL;DR: 本文提出了基于LLM提示的多轮文本简化方法，通过分析源文本和目标文本的CEFR级别差距来优化简化效果，在TSAR-2025共享任务中排名第7。


<details>
  <summary>Details</summary>
Motivation: 研究发现文本简化性能与源文本和目标文本的CEFR级别差距高度相关，这启发了多轮简化方法的设计。

Method: 提出了两种多轮简化方法：基于规则的多轮简化(MRS-Rule)和联合规则与LLM的多轮简化(MRS-Joint)，均使用GPT-4o生成。

Result: 提交的系统在20个团队中排名第7，后续改进显示以LLM简化结果作为起点可以进一步提升多轮简化性能。

Conclusion: 多轮简化方法有效，特别是将LLM简化结果作为起点可以显著提升性能，证明了CEFR级别差距对文本简化的重要性。

Abstract: This paper describes the OUNLP system submitted to the TSAR-2025 Shared Task
(Alva-Manchego et al., 2025), designed for readability-controlled text
simplification using LLM-prompting-based generation. Based on the analysis of
prompt-based text simplification methods, we discovered an interesting finding
that text simplification performance is highly related to the gap between the
source CEFR (Arase et al., 2022) level and the target CEFR level. Inspired by
this finding, we propose two multi-round simplification methods and generate
them via GPT-4o: rule-based simplification (MRS-Rule) and jointly rule-based
LLM simplification (MRS-Joint). Our submitted systems ranked 7 out of 20 teams.
Later improvements with MRS-Joint show that taking the LLM simplified
candidates as the starting point could further boost the multi-round
simplification performance.

</details>


### [37] [Decoding Emergent Big Five Traits in Large Language Models: Temperature-Dependent Expression and Architectural Clustering](https://arxiv.org/abs/2511.04499)
*Christos-Nikolaos Zacharopoulos,Revekka Kyriakoglou*

Main category: cs.CL

TL;DR: 该论文使用BFI-2框架评估了6个大型语言模型在不同采样温度下的人格特征表达，发现神经质和外向性对温度调整敏感，并通过层次聚类识别出具有稳定人格特征的不同模型集群。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在人类中心应用中的重要性增加，理解其类似人格的行为对于负责任的发展和部署变得越来越重要。

Method: 应用Big Five Inventory-2 (BFI-2)框架系统评估六个LLM在不同采样温度下的人格特质表达，并使用层次聚类分析模型特征。

Result: 在五个人格维度中的四个维度上发现了显著差异，神经质和外向性对温度调整特别敏感。层次聚类揭示了不同的模型集群，表明架构特征可能使某些模型倾向于稳定的特质特征。

Conclusion: 这些结果为LLM中类似人格模式的出现提供了新见解，并为模型调优、选择以及AI系统的伦理治理提供了新视角。

Abstract: As Large Language Models (LLMs) become integral to human-centered
applications, understanding their personality-like behaviors is increasingly
important for responsible development and deployment. This paper systematically
evaluates six LLMs, applying the Big Five Inventory-2 (BFI-2) framework, to
assess trait expressions under varying sampling temperatures. We find
significant differences across four of the five personality dimensions, with
Neuroticism and Extraversion susceptible to temperature adjustments. Further,
hierarchical clustering reveals distinct model clusters, suggesting that
architectural features may predispose certain models toward stable trait
profiles. Taken together, these results offer new insights into the emergence
of personality-like patterns in LLMs and provide a new perspective on model
tuning, selection, and the ethical governance of AI systems. We share the data
and code for this analysis here:
https://osf.io/bsvzc/?view_only=6672219bede24b4e875097426dc3fac1

</details>


### [38] [RAGalyst: Automated Human-Aligned Agentic Evaluation for Domain-Specific RAG](https://arxiv.org/abs/2511.04502)
*Joshua Gao,Quoc Huy Pham,Subin Varghese,Silwal Saurav,Vedhus Hoskere*

Main category: cs.CL

TL;DR: RAGalyst是一个自动化、与人类对齐的代理框架，用于严格评估领域特定的RAG系统，通过生成高质量合成QA数据集和优化LLM-as-a-Judge指标来实现与人类标注的高度相关性。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统评估框架在专业安全关键领域存在不足，基于启发式的指标无法捕捉领域特定细微差别，而LLM-as-a-Judge方法缺乏与人类判断的有效对齐验证。

Method: RAGalyst采用代理管道从源文档生成高质量合成QA数据集，包含代理过滤步骤确保数据保真度，并通过提示优化改进答案正确性和可回答性两个关键LLM-as-a-Judge指标。

Result: 在三个不同领域（军事行动、网络安全、桥梁工程）评估各种RAG组件发现，性能高度依赖于上下文，没有单一嵌入模型、LLM或超参数配置普遍最优。

Conclusion: 研究强调了像RAGalyst这样的系统评估框架的必要性，使从业者能够发现领域特定的权衡，并为构建可靠有效的RAG系统做出明智的设计选择。

Abstract: Retrieval-Augmented Generation (RAG) is a critical technique for grounding
Large Language Models (LLMs) in factual evidence, yet evaluating RAG systems in
specialized, safety-critical domains remains a significant challenge. Existing
evaluation frameworks often rely on heuristic-based metrics that fail to
capture domain-specific nuances and other works utilize LLM-as-a-Judge
approaches that lack validated alignment with human judgment. This paper
introduces RAGalyst, an automated, human-aligned agentic framework designed for
the rigorous evaluation of domain-specific RAG systems. RAGalyst features an
agentic pipeline that generates high-quality, synthetic question-answering (QA)
datasets from source documents, incorporating an agentic filtering step to
ensure data fidelity. The framework refines two key LLM-as-a-Judge
metrics-Answer Correctness and Answerability-using prompt optimization to
achieve a strong correlation with human annotations. Applying this framework to
evaluate various RAG components across three distinct domains (military
operations, cybersecurity, and bridge engineering), we find that performance is
highly context-dependent. No single embedding model, LLM, or hyperparameter
configuration proves universally optimal. Additionally, we provide an analysis
on the most common low Answer Correctness reasons in RAG. These findings
highlight the necessity of a systematic evaluation framework like RAGalyst,
which empowers practitioners to uncover domain-specific trade-offs and make
informed design choices for building reliable and effective RAG systems.
RAGalyst is available on our Github.

</details>


### [39] [Modeling Clinical Uncertainty in Radiology Reports: from Explicit Uncertainty Markers to Implicit Reasoning Pathways](https://arxiv.org/abs/2511.04506)
*Paloma Rabaey,Jong Hak Moon,Jung-Oh Lee,Min Gwan Kim,Hangyul Yoon,Thomas Demeester,Edward Choi*

Main category: cs.CL

TL;DR: 提出了一个两阶段框架来处理放射学报告中的不确定性：量化显性不确定性（通过LLM验证的短语排名映射到概率值）和建模隐性不确定性（通过专家定义的诊断路径扩展子发现），并发布了Lunguage++基准。


<details>
  <summary>Details</summary>
Motivation: 放射学报告对临床决策至关重要，但包含两种不确定性：显性不确定性（通过模糊短语表达）和隐性不确定性（因推理过程省略而产生），现有规则系统无法充分处理这些不确定性。

Method: 1. 量化显性不确定性：创建专家验证的LLM参考排名，将常见模糊短语映射到概率值；2. 建模隐性不确定性：通过专家定义的14种常见诊断路径，系统性地添加特征性子发现。

Result: 发布了Lunguage++，这是Lunguage基准的扩展版本，具有不确定性感知能力，支持不确定性感知的图像分类、忠实诊断推理和诊断不确定性临床影响研究。

Conclusion: 该框架成功解决了放射学报告中的不确定性挑战，为不确定性感知的医学图像分析和诊断推理提供了有价值的资源。

Abstract: Radiology reports are invaluable for clinical decision-making and hold great
potential for automated analysis when structured into machine-readable formats.
These reports often contain uncertainty, which we categorize into two distinct
types: (i) Explicit uncertainty reflects doubt about the presence or absence of
findings, conveyed through hedging phrases. These vary in meaning depending on
the context, making rule-based systems insufficient to quantify the level of
uncertainty for specific findings; (ii) Implicit uncertainty arises when
radiologists omit parts of their reasoning, recording only key findings or
diagnoses. Here, it is often unclear whether omitted findings are truly absent
or simply unmentioned for brevity. We address these challenges with a two-part
framework. We quantify explicit uncertainty by creating an expert-validated,
LLM-based reference ranking of common hedging phrases, and mapping each finding
to a probability value based on this reference. In addition, we model implicit
uncertainty through an expansion framework that systematically adds
characteristic sub-findings derived from expert-defined diagnostic pathways for
14 common diagnoses. Using these methods, we release Lunguage++, an expanded,
uncertainty-aware version of the Lunguage benchmark of fine-grained structured
radiology reports. This enriched resource enables uncertainty-aware image
classification, faithful diagnostic reasoning, and new investigations into the
clinical impact of diagnostic uncertainty.

</details>


### [40] [Are language models aware of the road not taken? Token-level uncertainty and hidden state dynamics](https://arxiv.org/abs/2511.04527)
*Amir Zur,Atticus Geiger,Ekdeep Singh Lubana,Eric Bigelow*

Main category: cs.CL

TL;DR: 语言模型在生成文本时，隐藏激活可以预测和控制模型在思维链推理中的不确定性，表明模型隐式表示可能的推理路径。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型是否在生成过程中表示其可能采取的替代推理路径，以量化模型的不确定性。

Method: 使用隐藏激活来控制语言模型在思维链推理中的不确定性，并预测其未来结果分布。

Result: 发现模型在不同token上的不确定性与通过激活控制模型的可操纵性之间存在明显相关性，且隐藏激活可以预测模型的未来结果分布。

Conclusion: 激活干预在模型尚未确定最终答案时最有效，表明模型隐式表示可能的推理路径空间。

Abstract: When a language model generates text, the selection of individual tokens
might lead it down very different reasoning paths, making uncertainty difficult
to quantify. In this work, we consider whether reasoning language models
represent the alternate paths that they could take during generation. To test
this hypothesis, we use hidden activations to control and predict a language
model's uncertainty during chain-of-thought reasoning. In our experiments, we
find a clear correlation between how uncertain a model is at different tokens,
and how easily the model can be steered by controlling its activations. This
suggests that activation interventions are most effective when there are
alternate paths available to the model -- in other words, when it has not yet
committed to a particular final answer. We also find that hidden activations
can predict a model's future outcome distribution, demonstrating that models
implicitly represent the space of possible paths.

</details>


### [41] [IntelliProof: An Argumentation Network-based Conversational Helper for Organized Reflection](https://arxiv.org/abs/2511.04528)
*Kaveh Eskandari Miandoab,Katharine Kowalyshyn,Kabir Pamnani,Anesu Gavhera,Vasanth Sarathy,Matthias Scheutz*

Main category: cs.CL

TL;DR: IntelliProof是一个基于LLM的交互式议论文分析系统，将文章构建为论证图，通过可视化展示论点关系并提供分类解释和连贯性量化评估。


<details>
  <summary>Details</summary>
Motivation: 现有自动评分系统缺乏用户体验，需要开发既能自动分析又能保持人工监督的系统，弥合议论文结构语义与用户理解之间的差距。

Method: 将议论文构建为论证图（节点表示论点，边表示支持/攻击关系），使用LLM进行关系分类和评分，并提供可视化界面和自然语言工具。

Result: 开发了可交互的系统原型，提供实时论证质量探索、分类解释和连贯性量化指标，支持用户深入理解论证结构。

Conclusion: IntelliProof成功创建了结合LLM分析和用户体验的议论文分析系统，实现了结构语义与用户理解的桥梁，提供了可用的在线演示系统。

Abstract: We present IntelliProof, an interactive system for analyzing argumentative
essays through LLMs. IntelliProof structures an essay as an argumentation
graph, where claims are represented as nodes, supporting evidence is attached
as node properties, and edges encode supporting or attacking relations. Unlike
existing automated essay scoring systems, IntelliProof emphasizes the user
experience: each relation is initially classified and scored by an LLM, then
visualized for enhanced understanding. The system provides justifications for
classifications and produces quantitative measures for essay coherence. It
enables rapid exploration of argumentative quality while retaining human
oversight. In addition, IntelliProof provides a set of tools for a better
understanding of an argumentative essay and its corresponding graph in natural
language, bridging the gap between the structural semantics of argumentative
essays and the user's understanding of a given text. A live demo and the system
are available here to try: \textbf{https://intelliproof.vercel.app}

</details>


### [42] [From Model to Breach: Towards Actionable LLM-Generated Vulnerabilities Reporting](https://arxiv.org/abs/2511.04538)
*Cyril Vallez,Alexander Sternfeld,Andrei Kucharavy,Ljiljana Dolamic*

Main category: cs.CL

TL;DR: 最新开源大语言模型在代码生成中仍存在早期报道的安全漏洞，表明安全性与功能性权衡阻碍了有效修复。作者提出新的严重性度量Prompt Exposure和Model Exposure来评估LLM生成漏洞的风险。


<details>
  <summary>Details</summary>
Motivation: 随着基于LLM的编程助手在软件开发中作用日益重要，其生成的漏洞对网络安全的影响也越来越关键。现有安全基准和改进方法对主流编程LLM的实际影响尚不明确。

Method: 引入新的严重性度量Prompt Exposure(PE)，综合考虑漏洞严重程度、生成概率和诱导漏洞代码生成的提示表述。基于PE定义Model Exposure(ME)评分来评估模型生成漏洞的严重性和普遍性。

Result: 研究发现即使最新的开源模型在现实使用场景中仍然容易受到早期报道的漏洞攻击，安全性与功能性之间的权衡阻碍了漏洞的有效修复。

Conclusion: 需要新的严重性评估方法来鼓励缓解最严重和普遍的漏洞，提出的PE和ME指标有助于更准确地评估LLM生成代码的安全风险。

Abstract: As the role of Large Language Models (LLM)-based coding assistants in
software development becomes more critical, so does the role of the bugs they
generate in the overall cybersecurity landscape. While a number of LLM code
security benchmarks have been proposed alongside approaches to improve the
security of generated code, it remains unclear to what extent they have
impacted widely used coding LLMs. Here, we show that even the latest
open-weight models are vulnerable in the earliest reported vulnerability
scenarios in a realistic use setting, suggesting that the safety-functionality
trade-off has until now prevented effective patching of vulnerabilities. To
help address this issue, we introduce a new severity metric that reflects the
risk posed by an LLM-generated vulnerability, accounting for vulnerability
severity, generation chance, and the formulation of the prompt that induces
vulnerable code generation - Prompt Exposure (PE). To encourage the mitigation
of the most serious and prevalent vulnerabilities, we use PE to define the
Model Exposure (ME) score, which indicates the severity and prevalence of
vulnerabilities a model generates.

</details>


### [43] [BanglaMedQA and BanglaMMedBench: Evaluating Retrieval-Augmented Generation Strategies for Bangla Biomedical Question Answering](https://arxiv.org/abs/2511.04560)
*Sadia Sultana,Saiyma Sittul Muna,Mosammat Zannatul Samarukh,Ajwad Abrar,Tareque Mohmud Chowdhury*

Main category: cs.CL

TL;DR: 开发了首个孟加拉语生物医学问答数据集BanglaMedQA和BanglaMMedBench，并评估了多种检索增强生成(RAG)策略，其中Agentic RAG在GPT-4o-120b上达到最高89.54%的准确率。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言生物医学问答系统开发困难的问题，促进孟加拉语医学知识的公平获取。

Method: 结合教科书和网络检索，应用多种RAG策略（传统、零样本回退、Agentic、迭代反馈、聚合RAG），其中Agentic RAG通过动态选择检索和推理策略来提升事实准确性。

Result: Agentic RAG在GPT-4o-120b上获得89.54%的最高准确率，优于其他配置，并展现出更优的推理质量。

Conclusion: RAG方法能够显著提升孟加拉语医学问答的可靠性和可访问性，为多语言医学AI研究奠定基础。

Abstract: Developing accurate biomedical Question Answering (QA) systems in
low-resource languages remains a major challenge, limiting equitable access to
reliable medical knowledge. This paper introduces BanglaMedQA and
BanglaMMedBench, the first large-scale Bangla biomedical Multiple Choice
Question (MCQ) datasets designed to evaluate reasoning and retrieval in medical
artificial intelligence (AI). The study applies and benchmarks several
Retrieval-Augmented Generation (RAG) strategies, including Traditional,
Zero-Shot Fallback, Agentic, Iterative Feedback, and Aggregate RAG, combining
textbook-based and web retrieval with generative reasoning to improve factual
accuracy. A key novelty lies in integrating a Bangla medical textbook corpus
through Optical Character Recognition (OCR) and implementing an Agentic RAG
pipeline that dynamically selects between retrieval and reasoning strategies.
Experimental results show that the Agentic RAG achieved the highest accuracy
89.54% with openai/gpt-oss-120b, outperforming other configurations and
demonstrating superior rationale quality. These findings highlight the
potential of RAG-based methods to enhance the reliability and accessibility of
Bangla medical QA, establishing a foundation for future research in
multilingual medical artificial intelligence.

</details>


### [44] [When retrieval outperforms generation: Dense evidence retrieval for scalable fake news detection](https://arxiv.org/abs/2511.04643)
*Alamgir Munir Qazi,John P. McCrae,Jamal Abdul Nasir*

Main category: cs.CL

TL;DR: DeReC是一个轻量级事实验证框架，使用通用文本嵌入和密集检索替代计算密集型LLM方法，在保持高准确率的同时显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的事实验证系统存在计算成本高和幻觉风险的问题，需要开发更高效实用的解决方案。

Method: 结合密集检索和专门分类，使用通用文本嵌入替代自回归LLM生成解释性推理的方法。

Result: 在RAWFC数据集上F1得分65.58%，超越L-Defense的61.20%；运行时间减少95%（RAWFC）和92%（LIAR-RAW）。

Conclusion: 精心设计的基于检索的系统可以在专业任务中匹配或超越LLM性能，同时更适合实际部署。

Abstract: The proliferation of misinformation necessitates robust yet computationally
efficient fact verification systems. While current state-of-the-art approaches
leverage Large Language Models (LLMs) for generating explanatory rationales,
these methods face significant computational barriers and hallucination risks
in real-world deployments. We present DeReC (Dense Retrieval Classification), a
lightweight framework that demonstrates how general-purpose text embeddings can
effectively replace autoregressive LLM-based approaches in fact verification
tasks. By combining dense retrieval with specialized classification, our system
achieves better accuracy while being significantly more efficient. DeReC
outperforms explanation-generating LLMs in efficiency, reducing runtime by 95%
on RAWFC (23 minutes 36 seconds compared to 454 minutes 12 seconds) and by 92%
on LIAR-RAW (134 minutes 14 seconds compared to 1692 minutes 23 seconds),
showcasing its effectiveness across varying dataset sizes. On the RAWFC
dataset, DeReC achieves an F1 score of 65.58%, surpassing the state-of-the-art
method L-Defense (61.20%). Our results demonstrate that carefully engineered
retrieval-based systems can match or exceed LLM performance in specialized
tasks while being significantly more practical for real-world deployment.

</details>


### [45] [Logit-Entropy Adaptive Stopping Heuristic for Efficient Chain-of-Thought Reasoning](https://arxiv.org/abs/2511.04654)
*Mohammad Atif Quamar,Mohammad Areeb*

Main category: cs.CL

TL;DR: LEASH是一种无需训练的解码算法，通过监控token级熵的斜率和top-logit边际的改进来自适应停止推理生成，在保持准确性的同时显著减少token使用和延迟。


<details>
  <summary>Details</summary>
Motivation: Chain-of-Thought (CoT)提示虽然能实现复杂推理，但生成完整固定长度的推理过程在计算上是浪费的，增加了token使用和延迟。

Method: LEASH监控两个内在信号：token级熵的斜率和top-logit边际的改进，一旦两个信号都趋于平稳就终止生成，表明模型已达到稳定的推理状态。

Result: 在四个指令调优模型上，在GSM8K和AQuA-RAT基准测试中，LEASH平均减少30-35%的token生成和27%的延迟，相对于CoT仅造成10个百分点的准确率下降。

Conclusion: LEASH是模型无关的，无需额外训练或监督，为CoT解码提供了一个简单高效的替代方案。

Abstract: Chain-of-Thought (CoT) prompting is a key technique for enabling complex
reasoning in large language models. However, generating full, fixed-length
rationales is computationally wasteful, inflating both token usage and latency.
We introduce LEASH: Logit-Entropy Adaptive Stopping Heuristic, a training-free
decoding algorithm that adaptively halts rationale generation. LEASH monitors
two intrinsic signals: the slope of token-level entropy and the improvement in
the top-logit margin. It terminates the generation once both signals plateau,
indicating the model has reached a stable reasoning state. Across four
instruction-tuned models on the GSM8K and AQuA-RAT benchmarks, LEASH reduces
average token generation by 30--35% and latency by 27%, while incurring a 10
p.p. accuracy drop relative to CoT. LEASH is model-agnostic and requires no
additional training or supervision, offering a simple and efficient alternative
to CoT decoding.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [46] [Applying Time Series Deep Learning Models to Forecast the Growth of Perennial Ryegrass in Ireland](https://arxiv.org/abs/2511.03749)
*Oluwadurotimi Onibonoje,Vuong M. Ngo,Andrew McCarre,Elodie Ruelle,Bernadette O-Briend,Mark Roantree*

Main category: cs.LG

TL;DR: 该研究开发了基于深度学习的草地生长预测模型，特别是针对爱尔兰乳业中多年生黑麦草的生长预测，使用时间卷积网络在历史草高数据上取得了良好性能。


<details>
  <summary>Details</summary>
Motivation: 草地作为全球第二大碳汇对生物多样性和碳循环调节至关重要，爱尔兰乳业面临盈利性和可持续性挑战，现有机械模型不实用，需要更有效的预测方法。

Method: 提出针对单变量数据集的深度学习模型，特别是时间卷积网络(TCN)，利用历史草高数据进行多年生黑麦草生长预测。

Result: 时间卷积网络在科克地区的多年生黑麦草生长预测中表现优异，RMSE为2.74，MAE为3.46，基于34年1,757周的数据集验证了最优模型配置。

Conclusion: 该研究提升了模型行为理解，改进了草地生长预测的可靠性，有助于推进可持续乳业实践的发展。

Abstract: Grasslands, constituting the world's second-largest terrestrial carbon sink,
play a crucial role in biodiversity and the regulation of the carbon cycle.
Currently, the Irish dairy sector, a significant economic contributor, grapples
with challenges related to profitability and sustainability. Presently, grass
growth forecasting relies on impractical mechanistic models. In response, we
propose deep learning models tailored for univariate datasets, presenting
cost-effective alternatives. Notably, a temporal convolutional network designed
for forecasting Perennial Ryegrass growth in Cork exhibits high performance,
leveraging historical grass height data with RMSE of 2.74 and MAE of 3.46.
Validation across a comprehensive dataset spanning 1,757 weeks over 34 years
provides insights into optimal model configurations. This study enhances our
understanding of model behavior, thereby improving reliability in grass growth
forecasting and contributing to the advancement of sustainable dairy farming
practices.

</details>


### [47] [Federated Learning with Gramian Angular Fields for Privacy-Preserving ECG Classification on Heterogeneous IoT Devices](https://arxiv.org/abs/2511.03753)
*Youssef Elmir,Yassine Himeur,Abbes Amira*

Main category: cs.LG

TL;DR: 提出了一种基于联邦学习的隐私保护ECG分类框架，通过将1D ECG信号转换为2D GAF图像，使用CNN进行特征提取，在保护医疗数据隐私的同时实现高效分类。


<details>
  <summary>Details</summary>
Motivation: 解决物联网医疗环境中ECG分类的隐私保护问题，确保敏感医疗数据在本地设备上处理，避免数据集中存储带来的隐私风险。

Method: 将1D ECG信号转换为2D GAF图像，采用CNN进行特征提取，在联邦学习框架下实现多设备协同训练，数据不离开本地设备。

Result: 在包含服务器、笔记本电脑和资源受限的Raspberry Pi 4的多客户端设置中，FL-GAF模型达到95.18%的分类准确率，显著优于单客户端基线。

Conclusion: 该框架展示了轻量级、隐私保护的AI在物联网医疗监测中的潜力，支持智能健康系统中可扩展且安全的边缘部署。

Abstract: This study presents a federated learning (FL) framework for
privacy-preserving electrocardiogram (ECG) classification in Internet of Things
(IoT) healthcare environments. By transforming 1D ECG signals into 2D Gramian
Angular Field (GAF) images, the proposed approach enables efficient feature
extraction through Convolutional Neural Networks (CNNs) while ensuring that
sensitive medical data remain local to each device. This work is among the
first to experimentally validate GAF-based federated ECG classification across
heterogeneous IoT devices, quantifying both performance and communication
efficiency. To evaluate feasibility in realistic IoT settings, we deployed the
framework across a server, a laptop, and a resource-constrained Raspberry Pi 4,
reflecting edge-cloud integration in IoT ecosystems. Experimental results
demonstrate that the FL-GAF model achieves a high classification accuracy of
95.18% in a multi-client setup, significantly outperforming a single-client
baseline in both accuracy and training time. Despite the added computational
complexity of GAF transformations, the framework maintains efficient resource
utilization and communication overhead. These findings highlight the potential
of lightweight, privacy-preserving AI for IoT-based healthcare monitoring,
supporting scalable and secure edge deployments in smart health systems.

</details>


### [48] [Alternative Fairness and Accuracy Optimization in Criminal Justice](https://arxiv.org/abs/2511.04505)
*Shaolong Wu,James Blume,Geshi Yeung*

Main category: cs.LG

TL;DR: 本文提出了一种改进的群体公平性方法，通过最小化加权错误损失并控制假阴性率差异在容忍范围内，来解决算法公平性在刑事司法中的冲突问题。


<details>
  <summary>Details</summary>
Motivation: 算法公平性研究快速发展，但在刑事司法等关键领域核心概念仍存在争议，特别是不同公平性概念之间的冲突问题需要解决。

Method: 开发了一种改进的群体公平性方法：不要求受保护群体间的精确平等，而是在保持假阴性率差异在较小容忍范围内的同时，最小化加权错误损失。

Result: 该方法使解决方案更容易找到，可以提高预测准确性，并凸显错误成本的伦理选择。

Conclusion: 提出了基于三个支柱的实用部署框架：基于需求的决策、透明度和问责制、以及精确定义和解决方案，将技术设计与合法性联系起来，为使用风险评估工具的机构提供可操作指导。

Abstract: Algorithmic fairness has grown rapidly as a research area, yet key concepts
remain unsettled, especially in criminal justice. We review group, individual,
and process fairness and map the conditions under which they conflict. We then
develop a simple modification to standard group fairness. Rather than exact
parity across protected groups, we minimize a weighted error loss while keeping
differences in false negative rates within a small tolerance. This makes
solutions easier to find, can raise predictive accuracy, and surfaces the
ethical choice of error costs. We situate this proposal within three classes of
critique: biased and incomplete data, latent affirmative action, and the
explosion of subgroup constraints. Finally, we offer a practical framework for
deployment in public decision systems built on three pillars: need-based
decisions, Transparency and accountability, and narrowly tailored definitions
and solutions. Together, these elements link technical design to legitimacy and
provide actionable guidance for agencies that use risk assessment and related
tools.

</details>


### [49] [Laugh, Relate, Engage: Stylized Comment Generation for Short Videos](https://arxiv.org/abs/2511.03757)
*Xuan Ouyang,Senan Wang,Bouzhou Wang,Siyuan Xiahou,Jinrong Zhou,Yuekang Li*

Main category: cs.LG

TL;DR: LOLGORITHM是一个用于短视频评论生成的多代理系统，支持六种不同评论风格，通过多模态大语言模型处理视频输入，在抖音和YouTube上获得超过87%的用户偏好率。


<details>
  <summary>Details</summary>
Motivation: 短视频平台中评论对社区参与和内容再创作至关重要，但生成既符合平台规范又具有风格多样性和上下文感知的评论仍具挑战。

Method: 采用模块化多代理系统，集成视频分割、上下文和情感分析、风格感知提示构建，支持六种评论风格，使用多模态大语言模型直接处理视频输入。

Result: 在包含40个视频和105名参与者的大规模人类偏好研究中，LOLGORITHM显著优于基线模型，在抖音上偏好率超过90%，在YouTube上达87.55%。

Conclusion: 该工作为短视频平台上的风格化评论生成提供了一个可扩展和文化适应的框架，有望增强用户参与度和创意互动。

Abstract: Short-video platforms have become a central medium in the modern Internet
landscape, where efficient information delivery and strong interactivity are
reshaping user engagement and cultural dissemination. Among the various forms
of user interaction, comments play a vital role in fostering community
participation and enabling content re-creation. However, generating comments
that are both compliant with platform guidelines and capable of exhibiting
stylistic diversity and contextual awareness remains a significant challenge.
We introduce LOLGORITHM, a modular multi-agent system (MAS) designed for
controllable short-video comment generation. The system integrates video
segmentation, contextual and affective analysis, and style-aware prompt
construction. It supports six distinct comment styles: puns (homophones),
rhyming, meme application, sarcasm (irony), plain humor, and content
extraction. Powered by a multimodal large language model (MLLM), LOLGORITHM
directly processes video inputs and achieves fine-grained style control through
explicit prompt markers and few-shot examples. To support development and
evaluation, we construct a bilingual dataset using official APIs from Douyin
(Chinese) and YouTube (English), covering five popular video genres: comedy
skits, daily life jokes, funny animal clips, humorous commentary, and talk
shows. Evaluation combines automated metrics originality, relevance, and style
conformity with a large-scale human preference study involving 40 videos and
105 participants. Results show that LOLGORITHM significantly outperforms
baseline models, achieving preference rates of over 90% on Douyin and 87.55% on
YouTube. This work presents a scalable and culturally adaptive framework for
stylized comment generation on short-video platforms, offering a promising path
to enhance user engagement and creative interaction.

</details>


### [50] [What's in Common? Multimodal Models Hallucinate When Reasoning Across Scenes](https://arxiv.org/abs/2511.03768)
*Candace Ross,Florian Bordes,Adina Williams,Polina Kirichenko,Mark Ibrahim*

Main category: cs.LG

TL;DR: 提出了Common-O基准测试，评估多模态语言模型在真实场景中的推理能力，发现即使最佳模型在跨场景推理方面表现也很差（仅35%准确率），揭示模型在感知基准饱和与实际推理能力之间的差距。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型在感知基准上表现饱和，但在真实世界场景中仍存在严重幻觉问题，需要开发能更好评估跨场景推理能力的基准。

Method: 构建包含10.5k个全新图像的Common-O基准，避免训练数据污染，通过"共同点是什么"的问题评估模型在跨场景中的推理能力。

Result: 最佳模型在Common-O上仅达35%准确率，在更复杂的Common-O Complex上仅1%；模型在场景中存在相似物体时更容易产生幻觉。

Conclusion: 多图像训练可能提供改进，需要进一步研究解决跨场景推理中的幻觉问题，Common-O基准将公开以促进相关研究。

Abstract: Multimodal language models possess a remarkable ability to handle an
open-vocabulary's worth of objects. Yet the best models still suffer from
hallucinations when reasoning about scenes in the real world, revealing a gap
between their seemingly strong performance on existing perception benchmarks
that are saturating and their reasoning in the real world. To address this gap,
we build a novel benchmark of in-the-wild scenes that we call Common-O. With
more than 10.5k examples using exclusively new images not found in web training
data to avoid contamination, Common-O goes beyond just perception, inspired by
cognitive tests for humans, to probe reasoning across scenes by asking "what's
in common?". We evaluate leading multimodal language models, including models
specifically trained to perform chain-of-thought reasoning. We find that
perceiving objects in single images is tractable for most models, yet reasoning
across scenes is very challenging even for the best models, including reasoning
models. Despite saturating many leaderboards focusing on perception, the best
performing model only achieves 35% on Common-O -- and on Common-O Complex,
consisting of more complex scenes, the best model achieves only 1%. Curiously,
we find models are more prone to hallucinate when similar objects are present
in the scene, suggesting models may be relying on object co-occurrence seen
during training. Among the models we evaluated, we found scale can provide
modest improvements while models explicitly trained with multi-image inputs
show bigger improvements, suggesting scaled multi-image training may offer
promise. We make our benchmark publicly available to spur research into the
challenge of hallucination when reasoning across scenes.

</details>


### [51] [Contamination Detection for VLMs using Multi-Modal Semantic Perturbation](https://arxiv.org/abs/2511.03774)
*Jaden Park,Mu Cai,Feng Yao,Jingbo Shang,Soochahn Lee,Yong Jae Lee*

Main category: cs.LG

TL;DR: 本文提出了一种基于多模态语义扰动的检测方法，用于识别视觉语言模型在测试集泄露情况下的性能膨胀问题。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型使用互联网规模且通常专有的预训练语料库，导致测试集泄露问题，使得性能评估失真。现有的缓解策略主要针对LLMs，而针对VLMs的污染检测方法研究不足。

Method: 首先故意污染开源VLMs在流行基准测试上，然后提出基于多模态语义扰动的新检测方法，通过观察污染模型在受控扰动下泛化能力的失败来识别污染。

Result: 实验表明现有检测方法要么完全失败，要么表现不一致，而提出的多模态语义扰动方法在多种现实污染策略下都表现出鲁棒性和有效性。

Conclusion: 多模态语义扰动是一种简单而有效的检测方法，能够可靠地识别VLMs中的测试集泄露问题，为模型性能评估提供更准确的判断依据。

Abstract: Recent advances in Vision-Language Models (VLMs) have achieved
state-of-the-art performance on numerous benchmark tasks. However, the use of
internet-scale, often proprietary, pretraining corpora raises a critical
concern for both practitioners and users: inflated performance due to test-set
leakage. While prior works have proposed mitigation strategies such as
decontamination of pretraining data and benchmark redesign for LLMs, the
complementary direction of developing detection methods for contaminated VLMs
remains underexplored. To address this gap, we deliberately contaminate
open-source VLMs on popular benchmarks and show that existing detection
approaches either fail outright or exhibit inconsistent behavior. We then
propose a novel simple yet effective detection method based on multi-modal
semantic perturbation, demonstrating that contaminated models fail to
generalize under controlled perturbations. Finally, we validate our approach
across multiple realistic contamination strategies, confirming its robustness
and effectiveness. The code and perturbed dataset will be released publicly.

</details>


### [52] [FusionDP: Foundation Model-Assisted Differentially Private Learning for Partially Sensitive Features](https://arxiv.org/abs/2511.03806)
*Linghui Zeng,Ruixuan Liu,Atiquer Rahman Sarkar,Xiaoqian Jiang,Joyce C. Ho,Li Xiong*

Main category: cs.LG

TL;DR: FusionDP是一个特征级差分隐私框架，利用基础模型对敏感特征进行估算，在保护敏感特征隐私的同时显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统DP-SGD对所有特征统一施加隐私保护，导致噪声注入过多和模型性能下降。实际应用中，只有部分敏感特征需要隐私保护，非敏感特征可以无隐私约束使用。

Method: 1. 使用大型基础模型根据非敏感特征估算敏感特征，作为外部先验；2. 改进DP-SGD算法，在原始特征和估算特征上训练模型，同时严格保护原始敏感特征的隐私。

Result: 在PhysioNet败血症预测任务和MIMIC-III临床笔记分类任务上的实验表明，FusionDP相比隐私保护基线方法显著提升了模型性能，同时保持了严格的特征级隐私保护。

Conclusion: FusionDP证明了基础模型驱动的估算方法能够有效改善各种模态数据的隐私-效用权衡，为特征级差分隐私提供了实用解决方案。

Abstract: Ensuring the privacy of sensitive training data is crucial in
privacy-preserving machine learning. However, in practical scenarios, privacy
protection may be required for only a subset of features. For instance, in ICU
data, demographic attributes like age and gender pose higher privacy risks due
to their re-identification potential, whereas raw lab results are generally
less sensitive. Traditional DP-SGD enforces privacy protection on all features
in one sample, leading to excessive noise injection and significant utility
degradation. We propose FusionDP, a two-step framework that enhances model
utility under feature-level differential privacy. First, FusionDP leverages
large foundation models to impute sensitive features given non-sensitive
features, treating them as external priors that provide high-quality estimates
of sensitive attributes without accessing the true values during model
training. Second, we introduce a modified DP-SGD algorithm that trains models
on both original and imputed features while formally preserving the privacy of
the original sensitive features. We evaluate FusionDP on two modalities: a
sepsis prediction task on tabular data from PhysioNet and a clinical note
classification task from MIMIC-III. By comparing against privacy-preserving
baselines, our results show that FusionDP significantly improves model
performance while maintaining rigorous feature-level privacy, demonstrating the
potential of foundation model-driven imputation to enhance the privacy-utility
trade-off for various modalities.

</details>


### [53] [Fair and Explainable Credit-Scoring under Concept Drift: Adaptive Explanation Frameworks for Evolving Populations](https://arxiv.org/abs/2511.03807)
*Shivogo John*

Main category: cs.LG

TL;DR: 开发自适应解释框架来解决信用评分系统中概念漂移导致的解释不稳定和不公平问题，通过三种自适应SHAP变体显著提高时间稳定性和公平性。


<details>
  <summary>Details</summary>
Motivation: 传统可解释性技术（如SHAP）假设静态数据和固定背景分布，当概念漂移发生时，其解释会变得不稳定且可能不公平。

Method: 集成XGBoost预测建模与三种自适应SHAP变体：按切片重新加权解释、漂移感知SHAP重新基线化、在线代理校准。

Result: 自适应方法特别是重新基线化和基于代理的解释，显著提高了时间稳定性，减少了跨人口群体的差异影响，且不降低预测准确性。

Conclusion: 自适应可解释性是维持数据驱动信用系统透明度、问责制和伦理可靠性的实用机制，适用于任何决策模型随人口变化而演变的领域。

Abstract: Evolving borrower behaviors, shifting economic conditions, and changing
regulatory landscapes continuously reshape the data distributions underlying
modern credit-scoring systems. Conventional explainability techniques, such as
SHAP, assume static data and fixed background distributions, making their
explanations unstable and potentially unfair when concept drift occurs. This
study addresses that challenge by developing adaptive explanation frameworks
that recalibrate interpretability and fairness in dynamically evolving credit
models. Using a multi-year credit dataset, we integrate predictive modeling via
XGBoost with three adaptive SHAP variants: (A) per-slice explanation
reweighting that adjusts for feature distribution shifts, (B) drift-aware SHAP
rebaselining with sliding-window background samples, and (C) online surrogate
calibration using incremental Ridge regression. Each method is benchmarked
against static SHAP explanations using metrics of predictive performance (AUC,
F1), directional and rank stability (cosine, Kendall tau), and fairness
(demographic parity and recalibration). Results show that adaptive methods,
particularly rebaselined and surrogate-based explanations, substantially
improve temporal stability and reduce disparate impact across demographic
groups without degrading predictive accuracy. Robustness tests, including
counterfactual perturbations, background sensitivity analysis, and
proxy-variable detection, confirm the resilience of adaptive explanations under
real-world drift conditions. These findings establish adaptive explainability
as a practical mechanism for sustaining transparency, accountability, and
ethical reliability in data-driven credit systems, and more broadly, in any
domain where decision models evolve with population change.

</details>


### [54] [Optimizing Reasoning Efficiency through Prompt Difficulty Prediction](https://arxiv.org/abs/2511.03808)
*Bo Zhao,Berkcan Kapusuzoglu,Kartik Balasubramaniam,Sambit Sahu,Supriyo Chakraborty,Genta Indra Winata*

Main category: cs.LG

TL;DR: 提出一种路由方法，将问题分配给最可能解决的最小模型，在保持准确性的同时减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 推理语言模型在复杂任务上表现良好，但由于模型规模和长推理轨迹导致部署成本高昂。

Method: 使用s1.1-32B的中间表示训练轻量级预测器，预测问题难度或模型正确性，指导在推理模型池中的路由分配。

Result: 在多样化数学基准测试中，路由方法相比随机分配提高了效率，同时匹配s1.1-32B的性能但使用显著更少的计算资源。

Conclusion: 难度感知路由对于推理模型的成本高效部署是有效的。

Abstract: Reasoning language models perform well on complex tasks but are costly to
deploy due to their size and long reasoning traces. We propose a routing
approach that assigns each problem to the smallest model likely to solve it,
reducing compute without sacrificing accuracy. Using intermediate
representations from s1.1-32B, we train lightweight predictors of problem
difficulty or model correctness to guide routing across a pool of reasoning
models. On diverse math benchmarks, routing improves efficiency over random
assignment and matches s1.1-32B's performance while using significantly less
compute. Our results demonstrate that difficulty-aware routing is effective for
cost-efficient deployment of reasoning models.

</details>


### [55] [One Size Does Not Fit All: Architecture-Aware Adaptive Batch Scheduling with DEBA](https://arxiv.org/abs/2511.03809)
*François Belias,Naser Ezzati-Jivan,Foutse Khomh*

Main category: cs.LG

TL;DR: DEBA自适应批量调度器通过监控梯度方差、梯度范数变化和损失变化来调整批量大小，但研究发现架构类型决定了自适应方法的有效性，轻量级和中深架构受益最大，而深度残差网络和稳定架构收益有限。


<details>
  <summary>Details</summary>
Motivation: 现有自适应批量大小方法对所有架构采用相同策略，假设存在通用解决方案，但缺乏对不同架构适应性的系统评估。

Method: 提出DEBA自适应批量调度器，监控梯度方差、梯度范数变化和损失变化来指导批量大小调整，并在6种架构上进行系统评估。

Result: 轻量级和中深架构获得45-62%训练加速和1-7%精度提升；浅层残差网络精度提升2.4-4.0%，加速36-43%；深度残差网络表现不稳定；稳定架构仅6%加速。

Conclusion: 自适应方法不能跨架构通用，批量大小调整需要架构感知设计，梯度稳定性指标可预测哪些架构会受益。

Abstract: Adaptive batch size methods aim to accelerate neural network training, but
existing approaches apply identical adaptation strategies across all
architectures, assuming a one-size-fits-all solution. We introduce DEBA
(Dynamic Efficient Batch Adaptation), an adaptive batch scheduler that monitors
gradient variance, gradient norm variation and loss variation to guide batch
size adaptations. Through systematic evaluation across six architectures
(ResNet-18/50, DenseNet-121, EfficientNet-B0, MobileNet-V3, ViT-B16) on
CIFAR-10 and CIFAR-100, with five random seeds per configuration, we
demonstrate that the architecture fundamentally determines adaptation efficacy.
Our findings reveal that: (1) lightweight and medium-depth architectures
(MobileNet-V3, DenseNet-121, EfficientNet-B0) achieve a 45-62% training speedup
with simultaneous accuracy improvements of 1-7%; (2) shallow residual networks
(ResNet-18) show consistent gains of +2.4 - 4.0% in accuracy, 36 - 43% in
speedup, while deep residual networks (ResNet-50) exhibit high variance and
occasional degradation; (3) already-stable architectures (ViT-B16) show minimal
speedup (6%) despite maintaining accuracy, indicating that adaptation benefits
vary with baseline optimization characteristics. We introduce a baseline
characterization framework using gradient stability metrics (stability score,
gradient norm variation) that predicts which architectures will benefit from
adaptive scheduling. Our ablation studies reveal critical design choices often
overlooked in prior work: sliding window statistics (vs. full history) and
sufficient cooldown periods (5+ epochs) between adaptations are essential for
success. This work challenges the prevailing assumption that adaptive methods
generalize across architectures and provides the first systematic evidence that
batch size adaptation requires an architecture-aware design.

</details>


### [56] [Sketch-Augmented Features Improve Learning Long-Range Dependencies in Graph Neural Networks](https://arxiv.org/abs/2511.03824)
*Ryien Hosseini,Filippo Simini,Venkatram Vishwanath,Rebecca Willett,Henry Hoffmann*

Main category: cs.LG

TL;DR: 提出了一种名为"草图随机特征"的方法，通过向标准GNN注入随机全局节点特征嵌入，有效解决GNN的长距离信息压缩、节点表示过平滑和表达能力有限三大挑战。


<details>
  <summary>Details</summary>
Motivation: GNN通过局部消息传递聚合邻居信息，虽然具有强大的归纳偏置并能利用图稀疏性，但面临三个关键挑战：(i)长距离信息压缩，(ii)节点表示过平滑，(iii)表达能力有限。

Method: 向标准GNN注入随机全局节点特征嵌入（称为草图随机特征），这些嵌入具有唯一性、距离敏感性和拓扑无关性，能够高效捕获长距离依赖关系。

Result: 实验结果表明，该策略在真实世界图学习任务中持续优于基线GNN，既可以作为独立解决方案，也可以作为图位置编码等现有技术的补充增强。

Conclusion: 草图随机特征方法能有效缓解GNN的三大限制，提供了一种高效捕获长距离依赖关系的解决方案，代码已开源。

Abstract: Graph Neural Networks learn on graph-structured data by iteratively
aggregating local neighborhood information. While this local message passing
paradigm imparts a powerful inductive bias and exploits graph sparsity, it also
yields three key challenges: (i) oversquashing of long-range information, (ii)
oversmoothing of node representations, and (iii) limited expressive power. In
this work we inject randomized global embeddings of node features, which we
term \textit{Sketched Random Features}, into standard GNNs, enabling them to
efficiently capture long-range dependencies. The embeddings are unique,
distance-sensitive, and topology-agnostic -- properties which we analytically
and empirically show alleviate the aforementioned limitations when injected
into GNNs. Experimental results on real-world graph learning tasks confirm that
this strategy consistently improves performance over baseline GNNs, offering
both a standalone solution and a complementary enhancement to existing
techniques such as graph positional encodings. Our source code is available at
\href{https://github.com/ryienh/sketched-random-features}{https://github.com/ryienh/sketched-random-features}.

</details>


### [57] [From Static to Dynamic: Enhancing Offline-to-Online Reinforcement Learning via Energy-Guided Diffusion Stratification](https://arxiv.org/abs/2511.03828)
*Lipeng Zu,Hansong Zhou,Xiaonan Zhang*

Main category: cs.LG

TL;DR: 提出了一种名为StratDiff的创新方法，通过扩散模型学习离线数据先验知识，利用能量函数改进策略模仿，在在线微调期间生成离线类动作，并通过KL散度对训练批次进行分层处理，实现更平滑的离线到在线强化学习过渡。


<details>
  <summary>Details</summary>
Motivation: 解决离线到在线强化学习中的分布偏移问题，现有方法很少显式评估或利用离线数据本身的分布结构，存在适应不同样本类型的学习策略研究空白。

Method: 使用扩散模型学习离线数据先验知识，通过能量函数改进策略模仿，计算生成动作与采样动作的KL散度来分层训练批次（离线类和在线类），分别采用离线和在线学习策略更新。

Result: 在D4RL基准测试上的广泛实证评估表明，StratDiff显著优于现有方法，与Cal-QL和IQL集成后，在各种强化学习设置中实现了增强的适应性和更稳定的性能。

Conclusion: StratDiff方法通过显式利用离线数据的分布结构，有效解决了离线到在线强化学习的分布偏移问题，为平滑过渡提供了创新解决方案。

Abstract: Transitioning from offline to online reinforcement learning (RL) poses
critical challenges due to distributional shifts between the fixed behavior
policy in the offline dataset and the evolving policy during online learning.
Although this issue is widely recognized, few methods attempt to explicitly
assess or utilize the distributional structure of the offline data itself,
leaving a research gap in adapting learning strategies to different types of
samples. To address this challenge, we propose an innovative method,
Energy-Guided Diffusion Stratification (StratDiff), which facilitates smoother
transitions in offline-to-online RL. StratDiff deploys a diffusion model to
learn prior knowledge from the offline dataset. It then refines this knowledge
through energy-based functions to improve policy imitation and generate
offline-like actions during online fine-tuning. The KL divergence between the
generated action and the corresponding sampled action is computed for each
sample and used to stratify the training batch into offline-like and
online-like subsets. Offline-like samples are updated using offline objectives,
while online-like samples follow online learning strategies. We demonstrate the
effectiveness of StratDiff by integrating it with off-the-shelf methods Cal-QL
and IQL. Extensive empirical evaluations on D4RL benchmarks show that StratDiff
significantly outperforms existing methods, achieving enhanced adaptability and
more stable performance across diverse RL settings.

</details>


### [58] [Higher-Order Causal Structure Learning with Additive Models](https://arxiv.org/abs/2511.03831)
*James Enouen,Yujia Zheng,Ignavier Ng,Yan Liu,Kun Zhang*

Main category: cs.LG

TL;DR: 该论文将因果加性模型扩展到包含高阶交互作用的加性模型，引入了有向无环超图来表示这种更复杂的因果结构，并提供了相应的理论分析和学习算法。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的许多过程都表现出高阶机制，但现有的因果发现方法很少明确处理交互作用。本文旨在扩展因果加性模型以包含高阶交互作用。

Method: 扩展因果加性模型到包含高阶交互作用的加性模型，使用有向无环超图表示结构，开发了贪婪CAM算法的扩展版本来处理更复杂的超图搜索空间。

Result: 提供了超DAG的可识别性结果，扩展了典型的马尔可夫等价类，并通过合成实验证明了所提算法的经验有效性。

Conclusion: 学习更复杂的超图结构可能带来更好的经验结果，更严格的假设对应更易学习的超DAG和更好的有限样本复杂度。

Abstract: Causal structure learning has long been the central task of inferring causal
insights from data. Despite the abundance of real-world processes exhibiting
higher-order mechanisms, however, an explicit treatment of interactions in
causal discovery has received little attention. In this work, we focus on
extending the causal additive model (CAM) to additive models with higher-order
interactions. This second level of modularity we introduce to the structure
learning problem is most easily represented by a directed acyclic hypergraph
which extends the DAG. We introduce the necessary definitions and theoretical
tools to handle the novel structure we introduce and then provide
identifiability results for the hyper DAG, extending the typical Markov
equivalence classes. We next provide insights into why learning the more
complex hypergraph structure may actually lead to better empirical results. In
particular, more restrictive assumptions like CAM correspond to easier-to-learn
hyper DAGs and better finite sample complexity. We finally develop an extension
of the greedy CAM algorithm which can handle the more complex hyper DAG search
space and demonstrate its empirical usefulness in synthetic experiments.

</details>


### [59] [Conditional Score Learning for Quickest Change Detection in Markov Transition Kernels](https://arxiv.org/abs/2511.03953)
*Wuxia Chen,Taposh Banerjee,Vahid Tarokh*

Main category: cs.LG

TL;DR: 提出了一种基于条件分数的快速变化检测方法，用于处理未知转移核的马尔可夫过程，避免了显式似然评估，适用于高维数据。


<details>
  <summary>Details</summary>
Motivation: 解决马尔可夫过程中转移核未知时的快速变化检测问题，特别是在高维数据场景下，传统方法难以直接评估似然函数。

Method: 通过样本对直接学习条件分数∇_y log p(y|x)，开发基于分数的CUSUM程序，使用条件Hyvarinen分数差检测核变化，并提出了截断统计量确保有界增量。

Result: 证明了均匀遍历马尔可夫过程的指数级误报时间下界，以及检测延迟的渐近上界，为高维马尔可夫模型中的分数检测提供了理论保证。

Conclusion: 该方法为高维马尔可夫模型中的变化检测提供了理论保证和实际可行性，避免了显式似然评估的困难。

Abstract: We address the problem of quickest change detection in Markov processes with
unknown transition kernels. The key idea is to learn the conditional score
$\nabla_{\mathbf{y}} \log p(\mathbf{y}|\mathbf{x})$ directly from sample pairs
$( \mathbf{x},\mathbf{y})$, where both $\mathbf{x}$ and $\mathbf{y}$ are
high-dimensional data generated by the same transition kernel. In this way, we
avoid explicit likelihood evaluation and provide a practical way to learn the
transition dynamics. Based on this estimation, we develop a score-based CUSUM
procedure that uses conditional Hyvarinen score differences to detect changes
in the kernel. To ensure bounded increments, we propose a truncated version of
the statistic. With Hoeffding's inequality for uniformly ergodic Markov
processes, we prove exponential lower bounds on the mean time to false alarm.
We also prove asymptotic upper bounds on detection delay. These results give
both theoretical guarantees and practical feasibility for score-based detection
in high-dimensional Markov models.

</details>


### [60] [Enhancing Q-Value Updates in Deep Q-Learning via Successor-State Prediction](https://arxiv.org/abs/2511.03836)
*Lipeng Zu,Hansong Zhou,Xiaonan Zhang*

Main category: cs.LG

TL;DR: 提出SADQ方法，通过建模环境动态和整合后继状态分布来解决DQN中目标更新依赖过时策略状态的问题，提高训练稳定性和学习效率。


<details>
  <summary>Details</summary>
Motivation: DQN的目标更新依赖于从回放缓冲区采样的转换，这些转换中的下一个状态可能来自过去次优策略的动作，导致学习信号不具信息性，增加更新过程的方差。

Method: 提出Successor-state Aggregation Deep Q-Network (SADQ)，使用随机转移模型显式建模环境动态，将后继状态分布整合到Q值估计过程中，实现更稳定和策略对齐的值更新。

Result: 理论保证SADQ保持无偏值估计同时减少训练方差。在标准RL基准和实际向量控制任务上的实验表明，SADQ在稳定性和学习效率方面持续优于DQN变体。

Conclusion: SADQ通过显式建模环境动态和整合后继状态分布，有效解决了DQN中策略不匹配问题，提供了更稳定和高效的学习方法。

Abstract: Deep Q-Networks (DQNs) estimate future returns by learning from transitions
sampled from a replay buffer. However, the target updates in DQN often rely on
next states generated by actions from past, potentially suboptimal, policy. As
a result, these states may not provide informative learning signals, causing
high variance into the update process. This issue is exacerbated when the
sampled transitions are poorly aligned with the agent's current policy. To
address this limitation, we propose the Successor-state Aggregation Deep
Q-Network (SADQ), which explicitly models environment dynamics using a
stochastic transition model. SADQ integrates successor-state distributions into
the Q-value estimation process, enabling more stable and policy-aligned value
updates. Additionally, it explores a more efficient action selection strategy
with the modeled transition structure. We provide theoretical guarantees that
SADQ maintains unbiased value estimates while reducing training variance. Our
extensive empirical results across standard RL benchmarks and real-world
vector-based control tasks demonstrate that SADQ consistently outperforms DQN
variants in both stability and learning efficiency.

</details>


### [61] [Non-Asymptotic Optimization and Generalization Bounds for Stochastic Gauss-Newton in Overparameterized Models](https://arxiv.org/abs/2511.03972)
*Semih Cayci*

Main category: cs.LG

TL;DR: 本文分析了随机高斯牛顿方法在过参数化深度神经网络训练中的收敛性和泛化性能，建立了有限时间收敛界和泛化界，发现高斯牛顿矩阵最小特征值越大，泛化性能越好。


<details>
  <summary>Details</summary>
Motivation: 研究高阶优化方法如何影响深度学习中的泛化性能，特别关注随机高斯牛顿方法在过参数化深度神经网络中的作用。

Method: 使用带Levenberg-Marquardt阻尼和mini-batch采样的随机高斯牛顿方法，在回归设置中训练具有平滑激活函数的过参数化深度神经网络。

Result: 建立了参数空间中有限时间收敛界，推导了非渐近泛化界，发现高斯牛顿矩阵最小特征值越大，稳定性界越紧。

Conclusion: 识别了SGN的有利泛化机制，其中沿优化路径的高斯牛顿矩阵最小特征值较大可获得更紧的稳定性界。

Abstract: An important question in deep learning is how higher-order optimization
methods affect generalization. In this work, we analyze a stochastic
Gauss-Newton (SGN) method with Levenberg-Marquardt damping and mini-batch
sampling for training overparameterized deep neural networks with smooth
activations in a regression setting. Our theoretical contributions are twofold.
First, we establish finite-time convergence bounds via a variable-metric
analysis in parameter space, with explicit dependencies on the batch size,
network width and depth. Second, we derive non-asymptotic generalization bounds
for SGN using uniform stability in the overparameterized regime, characterizing
the impact of curvature, batch size, and overparameterization on generalization
performance. Our theoretical results identify a favorable generalization regime
for SGN in which a larger minimum eigenvalue of the Gauss-Newton matrix along
the optimization path yields tighter stability bounds.

</details>


### [62] [Benchmark Datasets for Lead-Lag Forecasting on Social Platforms](https://arxiv.org/abs/2511.03877)
*Kimia Kazemian,Zhenzhen Liu,Yangfanyu Yang,Katie Z Luo,Shuhan Gu,Audrey Du,Xinyu Yang,Jack Jansons,Kilian Q Weinberger,John Thickstun,Yian Yin,Sarah Dean*

Main category: cs.LG

TL;DR: 本文提出了Lead-Lag Forecasting（LLF）这一新的预测范式，并发布了arXiv和GitHub两个大规模基准数据集，用于研究早期使用行为（如浏览量）与后续高影响力结果（如引用量）之间的时间滞后预测问题。


<details>
  <summary>Details</summary>
Motivation: 社交和协作平台中存在大量多元时间序列数据，其中早期交互行为（如浏览、点赞）与后续高影响力结果（如引用、销售）之间存在时间滞后关系，但缺乏标准化数据集和统一的研究框架。

Method: 构建了两个高容量基准数据集：arXiv（230万篇论文的访问量->引用量）和GitHub（300万个仓库的推送/星标->分叉），并提出了参数化和非参数化回归基准模型进行测试。

Result: 通过统计和分类测试验证了数据集中存在lead-lag动态关系，为LLF研究提供了实证基础，数据集捕获了跨年度的长期动态，避免了生存偏差。

Conclusion: 本研究确立了LLF作为一个新的预测范式，为在社交和使用数据中系统探索lead-lag预测问题奠定了实证基础，并提供了可公开访问的数据门户。

Abstract: Social and collaborative platforms emit multivariate time-series traces in
which early interactions-such as views, likes, or downloads-are followed,
sometimes months or years later, by higher impact like citations, sales, or
reviews. We formalize this setting as Lead-Lag Forecasting (LLF): given an
early usage channel (the lead), predict a correlated but temporally shifted
outcome channel (the lag). Despite the ubiquity of such patterns, LLF has not
been treated as a unified forecasting problem within the time-series community,
largely due to the absence of standardized datasets. To anchor research in LLF,
here we present two high-volume benchmark datasets-arXiv (accesses -> citations
of 2.3M papers) and GitHub (pushes/stars -> forks of 3M repositories)-and
outline additional domains with analogous lead-lag dynamics, including
Wikipedia (page views -> edits), Spotify (streams -> concert attendance),
e-commerce (click-throughs -> purchases), and LinkedIn profile (views ->
messages). Our datasets provide ideal testbeds for lead-lag forecasting, by
capturing long-horizon dynamics across years, spanning the full spectrum of
outcomes, and avoiding survivorship bias in sampling. We documented all
technical details of data curation and cleaning, verified the presence of
lead-lag dynamics through statistical and classification tests, and benchmarked
parametric and non-parametric baselines for regression. Our study establishes
LLF as a novel forecasting paradigm and lays an empirical foundation for its
systematic exploration in social and usage data. Our data portal with downloads
and documentation is available at https://lead-lag-forecasting.github.io/.

</details>


### [63] [Towards Scalable Meta-Learning of near-optimal Interpretable Models via Synthetic Model Generations](https://arxiv.org/abs/2511.04000)
*Kyaw Hpone Myint,Zhe Wu,Alexandre G. R. Day,Giri Iyengar*

Main category: cs.LG

TL;DR: 提出了一种高效的合成预训练数据生成方法，用于决策树的元学习，通过采样近似最优决策树来创建大规模真实数据集，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 决策树在金融和医疗等高风险领域广泛应用，但传统方法需要大量真实数据或计算昂贵的优化决策树进行元学习，限制了其可扩展性。

Method: 使用MetaTree transformer架构，通过合成采样近似最优决策树来生成大规模预训练数据集，实现高效的元学习。

Result: 该方法在性能上可与使用真实世界数据或计算昂贵的优化决策树相媲美，同时显著降低了计算成本。

Conclusion: 该策略为可扩展且高效的元学习解释性决策树模型开辟了新途径，增强了数据生成的灵活性。

Abstract: Decision trees are widely used in high-stakes fields like finance and
healthcare due to their interpretability. This work introduces an efficient,
scalable method for generating synthetic pre-training data to enable
meta-learning of decision trees. Our approach samples near-optimal decision
trees synthetically, creating large-scale, realistic datasets. Using the
MetaTree transformer architecture, we demonstrate that this method achieves
performance comparable to pre-training on real-world data or with
computationally expensive optimal decision trees. This strategy significantly
reduces computational costs, enhances data generation flexibility, and paves
the way for scalable and efficient meta-learning of interpretable decision tree
models.

</details>


### [64] [DecoHD: Decomposed Hyperdimensional Classification under Extreme Memory Budgets](https://arxiv.org/abs/2511.03911)
*Sanggeon Yun,Hyunwoo Oh,Ryozo Masukawa,Mohsen Imani*

Main category: cs.LG

TL;DR: Decomposition-based compression method for hyperdimensional computing (HDC) that learns directly in decomposed parameterization, achieving extreme memory savings with minimal accuracy loss and significant energy/speed gains.


<details>
  <summary>Details</summary>
Motivation: Traditional HDC compression methods shrink the feature axis and erode concentration and robustness, while prior decompositions use fixed atomic hypervectors that are ill-suited for compressing learned class prototypes.

Method: Introduces DecoHD which learns in decomposed HDC parameterization using a small shared set of per-layer channels with multiplicative binding across layers and bundling at the end, compressing along class axis via lightweight bundling head while preserving native bind-bundle-score operations.

Result: Achieves extreme memory savings with only 0.1-0.15% accuracy degradation on average (worst case 5.7%), more robust to random bit-flip noise, reaches accuracy plateau with ~97% fewer trainable parameters, and delivers 277x/35x energy/speed gains over CPU, 13.5x/3.7x over GPU, and 2.0x/2.4x over baseline HDC ASIC.

Conclusion: Decomposition is an effective approach for compressing HDC networks while maintaining performance and enabling significant hardware efficiency improvements.

Abstract: Decomposition is a proven way to shrink deep networks without changing I/O.
We bring this idea to hyperdimensional computing (HDC), where footprint cuts
usually shrink the feature axis and erode concentration and robustness. Prior
HDC decompositions decode via fixed atomic hypervectors, which are ill-suited
for compressing learned class prototypes. We introduce DecoHD, which learns
directly in a decomposed HDC parameterization: a small, shared set of per-layer
channels with multiplicative binding across layers and bundling at the end,
yielding a large representational space from compact factors. DecoHD compresses
along the class axis via a lightweight bundling head while preserving native
bind-bundle-score; training is end-to-end, and inference remains pure HDC,
aligning with in/near-memory accelerators. In evaluation, DecoHD attains
extreme memory savings with only minor accuracy degradation under tight
deployment budgets. On average it stays within about 0.1-0.15% of a strong
non-reduced HDC baseline (worst case 5.7%), is more robust to random bit-flip
noise, reaches its accuracy plateau with up to ~97% fewer trainable parameters,
and -- in hardware -- delivers roughly 277x/35x energy/speed gains over a CPU
(AMD Ryzen 9 9950X), 13.5x/3.7x over a GPU (NVIDIA RTX 4090), and 2.0x/2.4x
over a baseline HDC ASIC.

</details>


### [65] [On Joint Regularization and Calibration in Deep Ensembles](https://arxiv.org/abs/2511.04160)
*Laurits Fredsgaard,Mikkel N. Schmidt*

Main category: cs.LG

TL;DR: 本文研究了联合调优深度集成中权重衰减、温度缩放和早停策略对预测性能和不确定性量化的影响，提出了部分重叠保留策略作为实用折中方案。


<details>
  <summary>Details</summary>
Motivation: 深度集成能提升模型性能和不确定性校准，虽然通常单独训练和调优模型，但证据表明联合调优集成可以获得更好性能。

Method: 提出部分重叠保留策略，在联合评估和最大化训练数据使用之间取得平衡，研究联合调优权重衰减、温度缩放和早停策略的效果。

Result: 联合调优集成通常能匹配或改善性能，在不同任务和指标上效果大小差异显著，部分重叠保留策略提供了实用的解决方案。

Conclusion: 研究结果为优化深度集成模型提供了有价值的见解和指导，强调了单独优化与联合优化之间的权衡。

Abstract: Deep ensembles are a powerful tool in machine learning, improving both model
performance and uncertainty calibration. While ensembles are typically formed
by training and tuning models individually, evidence suggests that jointly
tuning the ensemble can lead to better performance. This paper investigates the
impact of jointly tuning weight decay, temperature scaling, and early stopping
on both predictive performance and uncertainty quantification. Additionally, we
propose a partially overlapping holdout strategy as a practical compromise
between enabling joint evaluation and maximizing the use of data for training.
Our results demonstrate that jointly tuning the ensemble generally matches or
improves performance, with significant variation in effect size across
different tasks and metrics. We highlight the trade-offs between individual and
joint optimization in deep ensemble training, with the overlapping holdout
strategy offering an attractive practical solution. We believe our findings
provide valuable insights and guidance for practitioners looking to optimize
deep ensemble models. Code is available at:
https://github.com/lauritsf/ensemble-optimality-gap

</details>


### [66] [On Predicting Sociodemographics from Mobility Signals](https://arxiv.org/abs/2511.03924)
*Ekin Uğurel,Cynthia Chen,Brian H. Y. Lee,Filipe Rodrigues*

Main category: cs.LG

TL;DR: 该论文提出了一种从移动数据推断社会人口属性的方法，通过引入基于有向移动图的高阶移动描述符、开发评估模型置信度的诊断工具，以及使用多任务学习框架来提高预测准确性、可解释性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 从移动数据推断社会人口属性有助于交通规划者更好地利用被动收集的数据集，但由于移动模式与社会人口特征之间关系弱且不一致，以及跨情境泛化能力有限，这一任务仍然具有挑战性。

Method: 1) 引入基于有向移动图的行为驱动高阶移动描述符，捕捉出行序列、出行方式和社交同行的结构化模式；2) 开发评估模型置信度与准确性一致性的指标和可视化诊断工具；3) 构建多任务学习框架，从共享表示中联合预测多个社会人口属性。

Result: 该方法在预测年龄、性别、收入和家庭结构方面显著优于基线特征，多任务学习框架在训练数据有限或测试集分布与训练集不同时表现优于单任务模型。

Conclusion: 所提出的方法通过改进特征表示、量化不确定性和增强泛化能力，有效解决了从移动数据推断社会人口属性的关键挑战，为交通规划提供了更可靠的工具。

Abstract: Inferring sociodemographic attributes from mobility data could help
transportation planners better leverage passively collected datasets, but this
task remains difficult due to weak and inconsistent relationships between
mobility patterns and sociodemographic traits, as well as limited
generalization across contexts. We address these challenges from three angles.
First, to improve predictive accuracy while retaining interpretability, we
introduce a behaviorally grounded set of higher-order mobility descriptors
based on directed mobility graphs. These features capture structured patterns
in trip sequences, travel modes, and social co-travel, and significantly
improve prediction of age, gender, income, and household structure over
baselines features. Second, we introduce metrics and visual diagnostic tools
that encourage evenness between model confidence and accuracy, enabling
planners to quantify uncertainty. Third, to improve generalization and sample
efficiency, we develop a multitask learning framework that jointly predicts
multiple sociodemographic attributes from a shared representation. This
approach outperforms single-task models, particularly when training data are
limited or when applying models across different time periods (i.e., when the
test set distribution differs from the training set).

</details>


### [67] [TwIST: Rigging the Lottery in Transformers with Independent Subnetwork Training](https://arxiv.org/abs/2511.03983)
*Michael Menezes,Barbara Su,Xinze Feng,Yehya Farhat,Hamza Shili,Anastasios Kyrillidis*

Main category: cs.LG

TL;DR: TwIST是一个分布式训练框架，用于高效的大语言模型稀疏化，通过并行训练多个子网络、参数聚合和重采样来识别高质量子网络，实现零成本部署时剪枝。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型稀疏化方法通常需要训练后处理（如校准或基于Hessian的恢复），这增加了部署成本。TwIST旨在在训练过程中直接识别高质量稀疏子网络，避免这些额外步骤。

Method: TwIST并行训练多个子网络，定期聚合它们的参数，并在训练过程中重采样新的子网络。这种方法能够识别出无需训练后处理的"黄金票券"子网络。

Result: 在50%+的高稀疏度下，TwIST显著优于基线方法（如23.14 PPL vs 31.64 PPL）。与无结构剪枝不同，TwIST产生结构化的密集矩阵，在普通硬件上提供实际的推理加速和内存减少。

Conclusion: TwIST提供了一种高效的训练时路径，可在没有额外微调或恢复开销的情况下部署稀疏大语言模型。

Abstract: We introduce TwIST, a distributed training framework for efficient large
language model (LLM) sparsification. TwIST trains multiple subnetworks in
parallel, periodically aggregates their parameters, and resamples new
subnetworks during training. This process identifies high-quality subnetworks
("golden tickets") without requiring post-training procedures such as
calibration or Hessian-based recovery. As a result, TwIST enables zero-cost
pruning at deployment time while achieving perplexity competitive with
state-of-the-art post-training sparsification methods. The benefits are most
pronounced under aggressive sparsity (e.g., 50%+), where TwIST significantly
outperforms baseline methods; for example, reaching 23.14 PPL compared to 31.64
for the closest prior approach. Unlike unstructured pruning, TwIST produces
structured, dense matrices that offer practical inference speedups and memory
reductions on commodity hardware (e.g., CPUs) that do not support efficient
sparse computation. TwIST provides an efficient training-time path to
deployable sparse LLMs without additional fine-tuning or recovery overhead.

</details>


### [68] [ForecastGAN: A Decomposition-Based Adversarial Framework for Multi-Horizon Time Series Forecasting](https://arxiv.org/abs/2511.04445)
*Syeda Sitara Wishal Fatima,Afshin Rahimi*

Main category: cs.LG

TL;DR: ForecastGAN是一个基于分解的对抗性框架，用于多时间跨度预测，通过三个模块解决现有方法在短期预测和分类特征处理上的局限性，在11个基准数据集上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有transformer模型在长期时间序列预测中表现出色，但在短期预测中表现不佳，且通常忽略分类特征，需要一种更通用的方法来解决这些局限性。

Method: 包含三个集成模块：分解模块提取季节性和趋势成分；模型选择模块根据预测时间跨度选择最优神经网络配置；对抗训练模块通过条件生成对抗网络训练增强预测鲁棒性。

Result: 在11个多变量时间序列基准数据集上的结果显示，ForecastGAN在短期预测中持续优于最先进的transformer模型，在长期预测中保持竞争力。

Conclusion: 该研究建立了一种更通用的时间序列预测方法，能够适应特定上下文，在不同数据特征下保持强大性能，无需大量超参数调优。

Abstract: Time series forecasting is essential across domains from finance to supply
chain management. This paper introduces ForecastGAN, a novel decomposition
based adversarial framework addressing limitations in existing approaches for
multi-horizon predictions. Although transformer models excel in long-term
forecasting, they often underperform in short-term scenarios and typically
ignore categorical features. ForecastGAN operates through three integrated
modules: a Decomposition Module that extracts seasonality and trend components;
a Model Selection Module that identifies optimal neural network configurations
based on forecasting horizon; and an Adversarial Training Module that enhances
prediction robustness through Conditional Generative Adversarial Network
training. Unlike conventional approaches, ForecastGAN effectively integrates
both numerical and categorical features. We validate our framework on eleven
benchmark multivariate time series datasets that span various forecasting
horizons. The results show that ForecastGAN consistently outperforms
state-of-the-art transformer models for short-term forecasting while remaining
competitive for long-term horizons. This research establishes a more
generalizable approach to time series forecasting that adapts to specific
contexts while maintaining strong performance across diverse data
characteristics without extensive hyperparameter tuning.

</details>


### [69] [SynQuE: Estimating Synthetic Dataset Quality Without Annotations](https://arxiv.org/abs/2511.03928)
*Arthur Chen,Victor Zhong*

Main category: cs.LG

TL;DR: 提出了SynQuE问题：在只有少量未标注真实数据的情况下，评估合成数据集的质量并排序，以选择最佳数据集用于下游任务训练。


<details>
  <summary>Details</summary>
Motivation: 解决在数据收集成本高或隐私受限情况下，如何有效选择高质量合成数据集的实际挑战。

Method: 1) 建立首个SynQuE基准测试；2) 提出基于分布和多样性的代理指标；3) 针对复杂任务提出LENS方法，利用大语言模型推理能力。

Result: SynQuE代理指标与真实任务性能相关，在情感分析、Text2SQL、网页导航和图像分类等任务上表现良好。LENS在复杂任务上表现最佳，如在Text2SQL解析中，使用SynQuE选择的前3个数据集可将准确率从30.4%提升至38.4%。

Conclusion: SynQuE为真实数据稀缺下的合成数据选择提供了实用框架，并推动了基于基础模型的数据特征分析和细粒度数据选择的未来研究。

Abstract: We introduce and formalize the Synthetic Dataset Quality Estimation (SynQuE)
problem: ranking synthetic datasets by their expected real-world task
performance using only limited unannotated real data. This addresses a critical
and open challenge where data is scarce due to collection costs or privacy
constraints. We establish the first comprehensive benchmarks for this problem
by introducing and evaluating proxy metrics that choose synthetic data for
training to maximize task performance on real data. We introduce the first
proxy metrics for SynQuE by adapting distribution and diversity-based distance
measures to our context via embedding models. To address the shortcomings of
these metrics on complex planning tasks, we propose LENS, a novel proxy that
leverages large language model reasoning. Our results show that SynQuE proxies
correlate with real task performance across diverse tasks, including sentiment
analysis, Text2SQL, web navigation, and image classification, with LENS
consistently outperforming others on complex tasks by capturing nuanced
characteristics. For instance, on text-to-SQL parsing, training on the top-3
synthetic datasets selected via SynQuE proxies can raise accuracy from 30.4% to
38.4 (+8.1)% on average compared to selecting data indiscriminately. This work
establishes SynQuE as a practical framework for synthetic data selection under
real-data scarcity and motivates future research on foundation model-based data
characterization and fine-grained data selection.

</details>


### [70] [Q3R: Quadratic Reweighted Rank Regularizer for Effective Low-Rank Training](https://arxiv.org/abs/2511.04485)
*Ipsita Ghosh,Ethan Nguyen,Christian Kümmerle*

Main category: cs.LG

TL;DR: 提出Q3R方法，一种基于二次重加权秩正则化的低秩训练策略，能够在预训练和微调任务中有效保持低秩结构，实现参数高效压缩。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效训练方法在低秩预训练任务中难以保持低秩结构和目标函数，需要新的低秩诱导训练策略。

Method: 基于IRLS框架，提出二次重加权秩正则化器Q3R，通过二次正则项逼近平滑对数行列式作为秩代理目标，实现指定目标秩的训练。

Result: 在ViT-Tiny模型上能够截断60%和80%参数，CIFAR-10准确率仅下降约1.3%和4%；在图像和语言任务的Transformer模型上验证了有效性。

Conclusion: Q3R能够训练具有指定低目标秩的权重矩阵，实现与密集模型相当的预测性能，计算开销小，且与现有架构完全兼容。

Abstract: Parameter-efficient training, based on low-rank optimization, has become a
highly successful tool for fine-tuning large deep-learning models. However,
these methods fail at low-rank pre-training tasks where maintaining the
low-rank structure and the objective remains a challenging task. We propose the
Quadratic Reweighted Rank Regularizer dubbed Q3R, which leads to a novel
low-rank inducing training strategy inspired by the iteratively reweighted
least squares (IRLS) framework. Q3R is based on a quadratic regularizer term
which majorizes a smoothed log determinant serving as rank surrogate objective.
Unlike other low-rank training techniques, Q3R is able to train weight matrices
with prescribed, low target ranks of models that achieve comparable predictive
performance as dense models, with small computational overhead, while remaining
fully compatible with existing architectures. For example, we demonstrated one
experiment where we are able to truncate $60\%$ and $80\%$ of the parameters of
a ViT-Tiny model with $~1.3\%$ and $~4\%$ accuracy drop in CIFAR-10 performance
respectively. The efficacy of Q3R is confirmed on Transformers across both
image and language tasks, including for low-rank fine-tuning.

</details>


### [71] [Comparing EPGP Surrogates and Finite Elements Under Degree-of-Freedom Parity](https://arxiv.org/abs/2511.04518)
*Obed Amo,Samit Ghosh,Markus Lange-Hegermann,Bogdan Raiţă,Michael Pokojovy*

Main category: cs.LG

TL;DR: 比较边界约束Ehrenpreis-Palamodov高斯过程(B-EPGP)代理模型与经典有限元方法结合Crank-Nicolson时间步进(CN-FEM)在求解二维波动方程时的性能，B-EPGP在匹配自由度条件下精度提高约两个数量级。


<details>
  <summary>Details</summary>
Motivation: 评估新型B-EPGP代理模型与传统CN-FEM方法在求解二维波动方程时的相对性能，特别关注在公平比较条件下的精度表现。

Method: B-EPGP利用特征簇导出的指数多项式基精确执行PDE和边界条件，采用惩罚最小二乘法估计系数；CN-FEM使用经典有限元方法结合Crank-Nicolson时间步进；引入自由度匹配协议确保公平比较。

Result: 在匹配自由度条件下，B-EPGP始终获得比CN-FEM更低的时空L^2误差和空间最大时间L^2误差，精度提高约两个数量级。

Conclusion: B-EPGP方法在求解二维波动方程时显著优于传统CN-FEM方法，特别是在匹配自由度条件下表现出更高的精度和计算效率。

Abstract: We present a new benchmarking study comparing a boundary-constrained
Ehrenpreis--Palamodov Gaussian Process (B-EPGP) surrogate with a classical
finite element method combined with Crank--Nicolson time stepping (CN-FEM) for
solving the two-dimensional wave equation with homogeneous Dirichlet boundary
conditions. The B-EPGP construction leverages exponential-polynomial bases
derived from the characteristic variety to enforce the PDE and boundary
conditions exactly and employs penalized least squares to estimate the
coefficients. To ensure fairness across paradigms, we introduce a
degrees-of-freedom (DoF) matching protocol. Under matched DoF, B-EPGP
consistently attains lower space-time $L^2$-error and maximum-in-time
$L^{2}$-error in space than CN-FEM, improving accuracy by roughly two orders of
magnitude.

</details>


### [72] [NVIDIA Nemotron Nano V2 VL](https://arxiv.org/abs/2511.03929)
*NVIDIA,:,Amala Sanjay Deshmukh,Kateryna Chumachenko,Tuomas Rintamaki,Matthieu Le,Tyler Poon,Danial Mohseni Taheri,Ilia Karmanov,Guilin Liu,Jarno Seppanen,Guo Chen,Karan Sapra,Zhiding Yu,Adi Renduchintala,Charles Wang,Peter Jin,Arushi Goel,Mike Ranzinger,Lukas Voegtle,Philipp Fischer,Timo Roman,Wei Ping,Boxin Wang,Zhuolin Yang,Nayeon Lee,Shaokun Zhang,Fuxiao Liu,Zhiqi Li,Di Zhang,Greg Heinrich,Hongxu,Yin,Song Han,Pavlo Molchanov,Parth Mannan,Yao Xu,Jane Polak Scowcroft,Tom Balough,Subhashree Radhakrishnan,Paris Zhang,Sean Cha,Ratnesh Kumar,Zaid Pervaiz Bhat,Jian Zhang,Darragh Hanley,Pritam Biswas,Jesse Oliver,Kevin Vasques,Roger Waleffe,Duncan Riach,Oluwatobi Olabiyi,Ameya Sunil Mahabaleshwarkar,Bilal Kartal,Pritam Gundecha,Khanh Nguyen,Alexandre Milesi,Eugene Khvedchenia,Ran Zilberstein,Ofri Masad,Natan Bagrov,Nave Assaf,Tomer Asida,Daniel Afrimi,Amit Zuker,Netanel Haber,Zhiyu Cheng,Jingyu,Xin,Di,Wu,Nik Spirin,Maryam Moosaei,Roman Ageev,Vanshil Atul Shah,Yuting Wu,Daniel Korzekwa,Unnikrishnan Kizhakkemadam Sreekumar,Wanli Jiang,Padmavathy Subramanian,Alejandra Rico,Sandip Bhaskar,Saeid Motiian,Kedi Wu,Annie Surla,Chia-Chih Chen,Hayden Wolff,Matthew Feinberg,Melissa Corpuz,Marek Wawrzos,Eileen Long,Aastha Jhunjhunwala,Paul Hendricks,Farzan Memarian,Benika Hall,Xin-Yu Wang,David Mosallanezhad,Soumye Singhal,Luis Vega,Katherine Cheung,Krzysztof Pawelec,Michael Evans,Katherine Luna,Jie Lou,Erick Galinkin,Akshay Hazare,Kaustubh Purandare,Ann Guan,Anna Warno,Chen Cui,Yoshi Suhara,Shibani Likhite,Seph Mard,Meredith Price,Laya Sleiman,Saori Kaji,Udi Karpas,Kari Briski,Joey Conway,Michael Lightstone,Jan Kautz,Mohammad Shoeybi,Mostofa Patwary,Jonathen Cohen,Oleksii Kuchaiev,Andrew Tao,Bryan Catanzaro*

Main category: cs.LG

TL;DR: Nemotron Nano V2 VL是Nemotron视觉语言系列的最新模型，专为真实世界文档理解、长视频理解和推理任务设计，相比前代模型在所有视觉和文本领域都有显著改进。


<details>
  <summary>Details</summary>
Motivation: 开发一个在真实世界文档理解、长视频理解和推理任务中表现更强大的视觉语言模型，解决长文档和视频场景中的推理效率问题。

Method: 基于Nemotron Nano V2混合Mamba-Transformer LLM架构，采用创新的token缩减技术来提高长文档和视频场景中的推理吞吐量，并在模型架构、数据集和训练方法上进行重大改进。

Result: 相比前代模型Llama-3.1-Nemotron-Nano-VL-8B，在所有视觉和文本领域都实现了显著改进，在长文档和视频场景中实现了更高的推理吞吐量。

Conclusion: Nemotron Nano V2 VL是一个强大的视觉语言模型，在文档理解、视频理解和推理任务中表现出色，并发布了BF16、FP8和FP4格式的模型检查点以及大部分数据集、配方和训练代码。

Abstract: We introduce Nemotron Nano V2 VL, the latest model of the Nemotron
vision-language series designed for strong real-world document understanding,
long video comprehension, and reasoning tasks. Nemotron Nano V2 VL delivers
significant improvements over our previous model,
Llama-3.1-Nemotron-Nano-VL-8B, across all vision and text domains through major
enhancements in model architecture, datasets, and training recipes. Nemotron
Nano V2 VL builds on Nemotron Nano V2, a hybrid Mamba-Transformer LLM, and
innovative token reduction techniques to achieve higher inference throughput in
long document and video scenarios. We are releasing model checkpoints in BF16,
FP8, and FP4 formats and sharing large parts of our datasets, recipes and
training code.

</details>


### [73] [End-to-End Reinforcement Learning of Koopman Models for eNMPC of an Air Separation Unit](https://arxiv.org/abs/2511.04522)
*Daniel Mayfrank,Kayra Dernek,Laura Lang,Alexander Mitsos,Manuel Dahmen*

Main category: cs.LG

TL;DR: 基于强化学习的Koopman代理模型训练方法在大型空分装置需求响应案例中表现良好，相比系统辨识方法，在保持经济性能的同时避免了约束违反。


<details>
  <summary>Details</summary>
Motivation: 验证之前提出的基于强化学习的Koopman代理模型训练方法在大规模工业案例中的可扩展性，特别是在仅观测少量实际可测量变量的条件下。

Method: 使用强化学习方法训练Koopman代理模型，应用于大型单产品（氮气）空分装置的需求响应案例研究，仅假设观测少量实际可测量变量。

Result: 该方法在大型案例中表现良好，相比基于系统辨识的Koopman eNMPC，在保持相似经济性能的同时完全避免了约束违反。

Conclusion: 基于强化学习的Koopman代理模型训练方法具有良好的可扩展性，能够在大规模工业应用中实现约束满足的经济优化控制。

Abstract: With our recently proposed method based on reinforcement learning (Mayfrank
et al. (2024), Comput. Chem. Eng. 190), Koopman surrogate models can be trained
for optimal performance in specific (economic) nonlinear model predictive
control ((e)NMPC) applications. So far, our method has exclusively been
demonstrated on a small-scale case study. Herein, we show that our method
scales well to a more challenging demand response case study built on a
large-scale model of a single-product (nitrogen) air separation unit. Across
all numerical experiments, we assume observability of only a few realistically
measurable plant variables. Compared to a purely system identification-based
Koopman eNMPC, which generates small economic savings but frequently violates
constraints, our method delivers similar economic performance while avoiding
constraint violations.

</details>


### [74] [Forgetting is Everywhere](https://arxiv.org/abs/2511.04666)
*Ben Sanati,Thomas L. Lee,Trevor McInroe,Aidan Scannell,Nikolay Malkin,David Abel,Amos Storkey*

Main category: cs.LG

TL;DR: 该论文提出了一个算法和任务无关的理论，将遗忘定义为学习者在未来经验预测分布中缺乏自一致性，表现为预测信息的损失。该理论自然地产生了一个衡量算法遗忘倾向的通用指标。


<details>
  <summary>Details</summary>
Motivation: 开发通用学习算法的一个基本挑战是它们在适应新数据时倾向于忘记过去的知识。解决这个问题需要对遗忘有原则性的理解，但尽管经过几十年的研究，尚未出现统一的定义来揭示学习的基本动态。

Method: 提出了一个算法和任务无关的理论，将遗忘定义为预测分布中缺乏自一致性，并基于此设计了一个衡量遗忘倾向的通用指标。通过设计涵盖分类、回归、生成建模和强化学习的全面实验来验证理论。

Result: 实证研究表明遗忘存在于所有学习设置中，并在决定学习效率方面起着重要作用。验证了理论的有效性，并展示了遗忘在不同学习环境中的普遍性。

Conclusion: 这些结果建立了对遗忘的原则性理解，为分析和改进通用学习算法的信息保留能力奠定了基础。

Abstract: A fundamental challenge in developing general learning algorithms is their
tendency to forget past knowledge when adapting to new data. Addressing this
problem requires a principled understanding of forgetting; yet, despite decades
of study, no unified definition has emerged that provides insights into the
underlying dynamics of learning. We propose an algorithm- and task-agnostic
theory that characterises forgetting as a lack of self-consistency in a
learner's predictive distribution over future experiences, manifesting as a
loss of predictive information. Our theory naturally yields a general measure
of an algorithm's propensity to forget. To validate the theory, we design a
comprehensive set of experiments that span classification, regression,
generative modelling, and reinforcement learning. We empirically demonstrate
how forgetting is present across all learning settings and plays a significant
role in determining learning efficiency. Together, these results establish a
principled understanding of forgetting and lay the foundation for analysing and
improving the information retention capabilities of general learning
algorithms.

</details>


### [75] [LogHD: Robust Compression of Hyperdimensional Classifiers via Logarithmic Class-Axis Reduction](https://arxiv.org/abs/2511.03938)
*Sanggeon Yun,Hyunwoo Oh,Ryozo Masukawa,Pietro Mercati,Nathaniel D. Bastian,Mohsen Imani*

Main category: cs.LG

TL;DR: LogHD是一种超维计算压缩方法，通过对类别轴进行对数压缩，将内存需求从O(CD)降低到O(D log_k C)，同时保持维度D不变，在保持准确性的同时显著提升能效和抗干扰能力。


<details>
  <summary>Details</summary>
Motivation: 标准超维计算的"每类一个原型"设计需要O(CD)内存，限制了其在内存、能耗和可靠性受限系统中的应用。现有压缩方法主要减少特征维度D，但会削弱鲁棒性。

Method: 使用对数类别轴压缩，用n≈⌈log_k C⌉个捆绑超向量替代C个每类原型，在n维激活空间中进行解码，结合容量感知码本和基于配置文件的解码，可与特征轴稀疏化组合使用。

Result: 在各种数据集和注入比特翻转的情况下，LogHD在较小模型下达到竞争性准确度，在相同内存下比特征轴压缩方法维持目标准确度的比特翻转率高出2.5-3.0倍。ASIC实现相比AMD Ryzen 9 9950X实现498倍能效和62.6倍加速，相比NVIDIA RTX 4090实现24.3倍/6.58倍提升。

Conclusion: LogHD通过类别轴压缩有效解决了超维计算的内存瓶颈问题，在保持模型性能的同时显著提升了能效、速度和抗干扰能力。

Abstract: Hyperdimensional computing (HDC) suits memory, energy, and
reliability-constrained systems, yet the standard "one prototype per class"
design requires $O(CD)$ memory (with $C$ classes and dimensionality $D$). Prior
compaction reduces $D$ (feature axis), improving storage/compute but weakening
robustness. We introduce LogHD, a logarithmic class-axis reduction that
replaces the $C$ per-class prototypes with $n\!\approx\!\lceil\log_k C\rceil$
bundle hypervectors (alphabet size $k$) and decodes in an $n$-dimensional
activation space, cutting memory to $O(D\log_k C)$ while preserving $D$. LogHD
uses a capacity-aware codebook and profile-based decoding, and composes with
feature-axis sparsification. Across datasets and injected bit flips, LogHD
attains competitive accuracy with smaller models and higher resilience at
matched memory. Under equal memory, it sustains target accuracy at roughly
$2.5$-$3.0\times$ higher bit-flip rates than feature-axis compression; an ASIC
instantiation delivers $498\times$ energy efficiency and $62.6\times$ speedup
over an AMD Ryzen 9 9950X and $24.3\times$/$6.58\times$ over an NVIDIA RTX
4090, and is $4.06\times$ more energy-efficient and $2.19\times$ faster than a
feature-axis HDC ASIC baseline.

</details>


### [76] [RLHF: A comprehensive Survey for Cultural, Multimodal and Low Latency Alignment Methods](https://arxiv.org/abs/2511.03939)
*Raghav Sharma,Manan Mehta,Sai Tiger Raina*

Main category: cs.LG

TL;DR: 本调查论文综述了RLHF对齐研究的新前沿，重点关注多模态对齐、文化公平性和低延迟优化等关键领域，为构建更鲁棒、高效和公平的AI系统提供路线图。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型对齐研究从传统的文本方法扩展到更广泛的领域，需要系统性地总结多模态对齐、文化公平性和低延迟优化等关键方向的最新进展。

Method: 首先回顾基础算法（PPO、DPO、GRPO），然后详细分析最新创新技术，通过比较性综合来系统探索这些领域。

Result: 提供了对齐技术的比较性综合，识别了当前研究中的关键差距和挑战。

Conclusion: 本工作为研究人员构建更鲁棒、高效和公平的AI系统提供了重要的路线图，指明了未来研究的方向。

Abstract: Reinforcement Learning from Human Feedback (RLHF) is the standard for
aligning Large Language Models (LLMs), yet recent progress has moved beyond
canonical text-based methods. This survey synthesizes the new frontier of
alignment research by addressing critical gaps in multi-modal alignment,
cultural fairness, and low-latency optimization. To systematically explore
these domains, we first review foundational algo- rithms, including PPO, DPO,
and GRPO, before presenting a detailed analysis of the latest innovations. By
providing a comparative synthesis of these techniques and outlining open
challenges, this work serves as an essential roadmap for researchers building
more robust, efficient, and equitable AI systems.

</details>


### [77] [PrivacyCD: Hierarchical Unlearning for Protecting Student Privacy in Cognitive Diagnosis](https://arxiv.org/abs/2511.03966)
*Mingliang Hou,Yinuo Wang,Teng Guo,Zitao Liu,Wenzhou Dou,Jiaqi Zheng,Renqiang Luo,Mi Tian,Weiqi Luo*

Main category: cs.LG

TL;DR: 本文提出了首个针对认知诊断模型的数据遗忘问题系统研究，开发了分层重要性引导遗忘算法，有效平衡遗忘完整性、模型效用和效率。


<details>
  <summary>Details</summary>
Motivation: 随着用户对"被遗忘权"的日益重视，需要从认知诊断模型中移除特定学生数据，但现有模型缺乏有效的数据遗忘机制。

Method: 提出分层重要性引导遗忘算法，利用认知诊断模型参数重要性的分层特性，通过结合个体和层级重要性的创新平滑机制，精确识别与待遗忘数据相关的参数。

Result: 在三个真实数据集上的实验表明，HIF在关键指标上显著优于基线方法。

Conclusion: HIF为认知诊断模型提供了首个有效解决方案，能够响应用户数据移除请求，并部署高性能的隐私保护AI系统。

Abstract: The need to remove specific student data from cognitive diagnosis (CD) models
has become a pressing requirement, driven by users' growing assertion of their
"right to be forgotten". However, existing CD models are largely designed
without privacy considerations and lack effective data unlearning mechanisms.
Directly applying general purpose unlearning algorithms is suboptimal, as they
struggle to balance unlearning completeness, model utility, and efficiency when
confronted with the unique heterogeneous structure of CD models. To address
this, our paper presents the first systematic study of the data unlearning
problem for CD models, proposing a novel and efficient algorithm: hierarchical
importanceguided forgetting (HIF). Our key insight is that parameter importance
in CD models exhibits distinct layer wise characteristics. HIF leverages this
via an innovative smoothing mechanism that combines individual and layer, level
importance, enabling a more precise distinction of parameters associated with
the data to be unlearned. Experiments on three real world datasets show that
HIF significantly outperforms baselines on key metrics, offering the first
effective solution for CD models to respond to user data removal requests and
for deploying high-performance, privacy preserving AI systems

</details>


### [78] [PETRA: Pretrained Evolutionary Transformer for SARS-CoV-2 Mutation Prediction](https://arxiv.org/abs/2511.03976)
*Xu Zou*

Main category: cs.LG

TL;DR: PETRA是一种基于进化轨迹的预训练变换器模型，通过系统发育树而非原始RNA序列来预测SARS-CoV-2的未来突变，有效应对测序噪声和病毒进化的层次结构。


<details>
  <summary>Details</summary>
Motivation: SARS-CoV-2的快速进化轨迹和免疫逃逸变种的持续出现给公共卫生和疫苗开发带来挑战，而现有的大规模生成预训练变换器在处理嘈杂的病毒基因组序列时存在局限。

Method: 提出PETRA方法，基于系统发育树推导的进化轨迹而非原始RNA序列，采用加权训练框架解决全球序列数据的地理和时间不平衡问题。

Result: 在预测SARS-CoV-2未来突变方面表现优异，核苷酸突变的加权召回率@1达到9.45%，刺突蛋白氨基酸突变为17.10%，显著优于基线方法的0.49%和6.64%。

Conclusion: PETRA能够有效预测SARS-CoV-2的未来突变，包括主要支系如24F(XEC)和25A(LP.8.1)的实时突变预测，为病毒进化监测提供有力工具。

Abstract: Since its emergence, SARS-CoV-2 has demonstrated a rapid and unpredictable
evolutionary trajectory, characterized by the continual emergence of
immune-evasive variants. This poses persistent challenges to public health and
vaccine development.
  While large-scale generative pre-trained transformers (GPTs) have
revolutionized the modeling of sequential data, their direct applications to
noisy viral genomic sequences are limited. In this paper, we introduce
PETRA(Pretrained Evolutionary TRAnsformer), a novel transformer approach based
on evolutionary trajectories derived from phylogenetic trees rather than raw
RNA sequences. This method effectively mitigates sequencing noise and captures
the hierarchical structure of viral evolution.
  With a weighted training framework to address substantial geographical and
temporal imbalances in global sequence data, PETRA excels in predicting future
SARS-CoV-2 mutations, achieving a weighted recall@1 of 9.45% for nucleotide
mutations and 17.10\% for spike amino-acid mutations, compared to 0.49% and
6.64% respectively for the best baseline. PETRA also demonstrates its ability
to aid in the real-time mutation prediction of major clades like 24F(XEC) and
25A(LP.8.1). The code is open sourced on https://github.com/xz-keg/PETra

</details>


### [79] [Structural Priors and Modular Adapters in the Composable Fine-Tuning Algorithm of Large-Scale Models](https://arxiv.org/abs/2511.03981)
*Yuxiao Wang,Di Wu,Feng Liu,Zhimin Qiu,Chenrui Hu*

Main category: cs.LG

TL;DR: 提出了一种可组合的微调方法，通过图结构先验和模块化适配器解决大规模预训练模型在多任务适应中的高计算成本和结构不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 解决大规模预训练模型在多任务适应时面临的高计算成本和结构不稳定问题，提高参数效率和训练稳定性。

Method: 引入关系矩阵建模任务间依赖关系，将节点和路径相关性编码为图结构先验，通过低秩映射和可插拔机制将模块化适配器嵌入不同层，实现高效的跨任务组合和重用。

Result: 显著提升任务预测精度、适配器权重分配精度和整体计算效率，同时保持模型轻量化设计。

Conclusion: 图先验和模块化机制在可组合微调中具有协同优势，能够有效缓解多任务场景中的路径冲突和冗余计算问题。

Abstract: This paper proposes a composable fine-tuning method that integrates graph
structural priors with modular adapters to address the high computational cost
and structural instability faced by large-scale pre-trained models in
multi-task adaptation. The method introduces a relation matrix to model
dependencies among tasks, explicitly encoding correlations between nodes and
paths into graph structural priors, which provide unified structural
constraints for adapter weight allocation and path selection. Modular adapters
are embedded into different layers through low-rank mapping and a pluggable
mechanism, enabling efficient cross-task composition and reuse under prior
guidance. This mechanism not only improves parameter efficiency and training
stability but also alleviates path conflicts and redundant computation in
multi-task scenarios. Furthermore, experiments on hyperparameter sensitivity,
environmental sensitivity, and data sensitivity are conducted to systematically
analyze key factors such as routing temperature, gating thresholds, and
relation matrix regularization strength, verifying the consistency and superior
performance of the method under structural constraints. The results demonstrate
that the proposed framework significantly enhances task prediction accuracy,
adapter weight allocation precision, and overall computational efficiency while
maintaining model lightweight design, highlighting the synergistic advantages
of graph priors and modular mechanisms in composable fine-tuning.

</details>


### [80] [Use of Continuous Glucose Monitoring with Machine Learning to Identify Metabolic Subphenotypes and Inform Precision Lifestyle Changes](https://arxiv.org/abs/2511.03986)
*Ahmed A. Metwally,Heyjun Park,Yue Wu,Tracey McLaughlin,Michael P. Snyder*

Main category: cs.LG

TL;DR: 本文综述了如何利用连续血糖监测和可穿戴技术，结合机器学习模型，从静态血糖阈值转向动态代谢表型分析，实现糖尿病的精准预防和个性化干预。


<details>
  <summary>Details</summary>
Motivation: 传统的糖尿病和糖尿病前期分类基于静态血糖阈值，掩盖了由胰岛素抵抗、β细胞功能障碍和肠促胰岛素缺乏驱动的病理生理异质性。需要新的方法来识别不同的代谢亚型。

Method: 使用连续血糖监测和可穿戴技术收集高分辨率血糖数据，结合机器学习模型分析家庭口服葡萄糖耐量测试数据，预测肌肉胰岛素抵抗和β细胞功能。整合可穿戴数据（饮食、睡眠、体力活动模式）进行代谢亚型识别。

Result: 研究表明机器学习模型能够准确预测金标准测量的肌肉胰岛素抵抗和β细胞功能。个体对标准化餐食的餐后血糖反应可作为代谢亚型的生物标志物。饮食缓解剂对餐后血糖反应的衰减效果具有表型依赖性。

Conclusion: 连续血糖监测能够将早期血糖异常的复杂性分解为不同的、可操作的亚表型，超越简单的血糖控制，为针对个体核心代谢缺陷的精准营养、行为和药物策略开辟了新途径。

Abstract: The classification of diabetes and prediabetes by static glucose thresholds
obscures the pathophysiological dysglycemia heterogeneity, primarily driven by
insulin resistance (IR), beta-cell dysfunction, and incretin deficiency. This
review demonstrates that continuous glucose monitoring and wearable
technologies enable a paradigm shift towards non-invasive, dynamic metabolic
phenotyping. We show evidence that machine learning models can leverage
high-resolution glucose data from at-home, CGM-enabled oral glucose tolerance
tests to accurately predict gold-standard measures of muscle IR and beta-cell
function. This personalized characterization extends to real-world nutrition,
where an individual's unique postprandial glycemic response (PPGR) to
standardized meals, such as the relative glucose spike to potatoes versus
grapes, could serve as a biomarker for their metabolic subtype. Moreover,
integrating wearable data reveals that habitual diet, sleep, and physical
activity patterns, particularly their timing, are uniquely associated with
specific metabolic dysfunctions, informing precision lifestyle interventions.
The efficacy of dietary mitigators in attenuating PPGR is also shown to be
phenotype-dependent. Collectively, this evidence demonstrates that CGM can
deconstruct the complexity of early dysglycemia into distinct, actionable
subphenotypes. This approach moves beyond simple glycemic control, paving the
way for targeted nutritional, behavioral, and pharmacological strategies
tailored to an individual's core metabolic defects, thereby paving the way for
a new era of precision diabetes prevention.

</details>


### [81] [Multiscale Astrocyte Network Calcium Dynamics for Biologically Plausible Intelligence in Anomaly Detection](https://arxiv.org/abs/2511.03993)
*Berk Iskar,Michael Taynnan Barros*

Main category: cs.LG

TL;DR: 提出了一种受星形胶质细胞钙信号启发的Ca²⁺调制学习框架，用于网络异常检测，能够有效应对概念漂移和新威胁。


<details>
  <summary>Details</summary>
Motivation: 传统离线训练的异常检测系统容易受到概念漂移和新型威胁（如零日攻击、多态攻击）的影响，需要更适应性的解决方案。

Method: 将多细胞星形胶质细胞动力学模拟器与深度神经网络耦合，模拟IP₃介导的Ca²⁺释放、SERCA泵摄取和间隙连接扩散三个关键机制。

Result: 在CTU-13网络流量数据上，Ca²⁺门控模型优于基线DNN，准确率高达约98%，假阳性和假阴性显著减少，且运行时开销可忽略。

Conclusion: 该Ca²⁺调制学习框架为需要快速适应演化数据模式的流式检测任务提供了通用的生物启发解决方案。

Abstract: Network anomaly detection systems encounter several challenges with
traditional detectors trained offline. They become susceptible to concept drift
and new threats such as zero-day or polymorphic attacks. To address this
limitation, we propose a Ca$^{2+}$-modulated learning framework that draws
inspiration from astrocytic Ca$^{2+}$ signaling in the brain, where rapid,
context-sensitive adaptation enables robust information processing. Our
approach couples a multicellular astrocyte dynamics simulator with a deep
neural network (DNN). The simulator models astrocytic Ca$^{2+}$ dynamics
through three key mechanisms: IP$_3$-mediated Ca$^{2+}$ release, SERCA pump
uptake, and conductance-aware diffusion through gap junctions between cells.
Evaluation of our proposed network on CTU-13 (Neris) network traffic data
demonstrates the effectiveness of our biologically plausible approach. The
Ca$^{2+}$-gated model outperforms a matched baseline DNN, achieving up to
$\sim$98\% accuracy with reduced false positives and negatives across multiple
train/test splits. Importantly, this improved performance comes with negligible
runtime overhead once Ca$^{2+}$ trajectories are precomputed. While
demonstrated here for cybersecurity applications, this Ca$^{2+}$-modulated
learning framework offers a generic solution for streaming detection tasks that
require rapid, biologically grounded adaptation to evolving data patterns.

</details>


### [82] [Accelerating scientific discovery with the common task framework](https://arxiv.org/abs/2511.04001)
*J. Nathan Kutz,Peter Battaglia,Michael Brenner,Kevin Carlberg,Aric Hagberg,Shirley Ho,Stephan Hoyer,Henning Lange,Hod Lipson,Michael W. Mahoney,Frank Noe,Max Welling,Laure Zanna,Francis Zhu,Steven L. Brunton*

Main category: cs.LG

TL;DR: 提出了一个用于科学和工程的通用任务框架(CTF)，包含不断增长的挑战数据集，用于评估机器学习算法在预测、状态重建、泛化和控制等科学目标上的表现。


<details>
  <summary>Details</summary>
Motivation: 机器学习和人工智能算法正在改变工程、物理和生物科学中的动态系统表征和控制，需要比较指标来评估多样化的科学目标，同时考虑有限数据和噪声测量的情况。

Method: 引入通用任务框架(CTF)，该框架包含不断增长的挑战数据集，涵盖实际和常见的目标，类似于在语音识别、语言处理和计算机视觉领域成功应用的框架。

Result: CTF为快速发展的科学和工程领域提供了客观的评估指标，能够比较当前实践中正在开发和部署的多样化算法。

Conclusion: 通用任务框架是推动科学和工程领域机器学习算法发展的关键技术，填补了当前缺乏客观比较指标的空白。

Abstract: Machine learning (ML) and artificial intelligence (AI) algorithms are
transforming and empowering the characterization and control of dynamic systems
in the engineering, physical, and biological sciences. These emerging modeling
paradigms require comparative metrics to evaluate a diverse set of scientific
objectives, including forecasting, state reconstruction, generalization, and
control, while also considering limited data scenarios and noisy measurements.
We introduce a common task framework (CTF) for science and engineering, which
features a growing collection of challenge data sets with a diverse set of
practical and common objectives. The CTF is a critically enabling technology
that has contributed to the rapid advance of ML/AI algorithms in traditional
applications such as speech recognition, language processing, and computer
vision. There is a critical need for the objective metrics of a CTF to compare
the diverse algorithms being rapidly developed and deployed in practice today
across science and engineering.

</details>


### [83] [Memory- and Latency-Constrained Inference of Large Language Models via Adaptive Split Computing](https://arxiv.org/abs/2511.04002)
*Mingyu Sung,Vikas Palakonda,Suhwan Im,Sunghwan Moon,Il-Min Kim,Sangseok Yun,Jae-Mo Kang*

Main category: cs.LG

TL;DR: 提出了首个面向边缘设备LLM部署的自回归感知分割计算框架，通过混合精度量化、中间压缩和联合优化解决KV缓存和通信开销问题。


<details>
  <summary>Details</summary>
Motivation: LLM在资源受限的IoT设备上部署不实用，现有分割计算方法无法解决自回归推理的独特挑战，特别是迭代token生成和扩展KV缓存需求。

Method: 开发了三点关键贡献：1) 单点分割压缩的混合精度量化方案；2) 两阶段中间压缩管道；3) 联合优化框架，共同选择最优分割点、量化设置和序列长度。

Result: 在多样化LLM和硬件平台上评估显示，相比SmoothQuant、OmniQuant和Atom等先进量化方法，实现了1.49倍推理加速和显著通信开销减少，同时保持或提升模型精度。

Conclusion: 该框架成功解决了边缘设备上LLM部署的挑战，为资源受限环境下的高效LLM推理提供了可行方案。

Abstract: Large language models (LLMs) have achieved near-human performance across
diverse reasoning tasks, yet their deployment on resource-constrained
Internet-of-Things (IoT) devices remains impractical due to massive parameter
footprints and memory-intensive autoregressive decoding. While split computing
offers a promising solution by partitioning model execution between edge
devices and cloud servers, existing approaches fail to address the unique
challenges of autoregressive inference, particularly the iterative token
generation process and expanding key-value (KV) cache requirements. This work
introduces the first autoregressive-aware split computing framework designed
explicitly for LLM deployment on edge devices. Our approach makes three key
contributions. First, we develop one-point split compression (OPSC), a
mixed-precision quantization scheme that prevents out-of-memory failures by
strategically partitioning models into front-end and back-end segments with
different precision levels. Second, we propose a two-stage intermediate
compression pipeline that combines threshold splitting (TS) and token-wise
adaptive bit quantization (TAB-Q) to preserve accuracy-critical activations
while dramatically reducing communication overhead. Third, we formulate a
unified optimization framework that jointly selects optimal split points,
quantization settings, and sequence lengths to satisfy strict memory and
latency constraints. Extensive evaluations across diverse LLMs and hardware
platforms demonstrate superior performance compared to state-of-the-art
quantization methods, including SmoothQuant, OmniQuant, and Atom. The framework
achieves a 1.49 inference speedup and significant communication overhead
reduction while maintaining or improving model accuracy.

</details>


### [84] [Enhancing Multimodal Protein Function Prediction Through Dual-Branch Dynamic Selection with Reconstructive Pre-Training](https://arxiv.org/abs/2511.04040)
*Xiaoling Luo,Peng Chen,Chengliang Liu,Xiaopeng Jin,Jie Wen,Yumeng Liu,Junsong Wang*

Main category: cs.LG

TL;DR: DSRPGO是一种多模态蛋白质功能预测方法，通过动态选择和重构预训练机制，结合双向交互模块来提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 多模态蛋白质特征包含结构数据、序列特征、蛋白质属性和相互作用网络等多种信息，这些复杂关联难以解析，需要有效方法来整合这些特征进行功能预测。

Method: 采用重构预训练挖掘细粒度信息，设计双向交互模块促进多模态特征交互学习，使用动态选择模块选择最适合当前预测的特征表示。

Result: 在人类数据集上，DSRPGO在BPO、MFO和CCO三个指标上显著提升，优于其他基准模型。

Conclusion: DSRPGO通过多模态特征的有效整合和动态选择机制，成功提升了蛋白质功能预测的性能。

Abstract: Multimodal protein features play a crucial role in protein function
prediction. However, these features encompass a wide range of information,
ranging from structural data and sequence features to protein attributes and
interaction networks, making it challenging to decipher their complex
interconnections. In this work, we propose a multimodal protein function
prediction method (DSRPGO) by utilizing dynamic selection and reconstructive
pre-training mechanisms. To acquire complex protein information, we introduce
reconstructive pre-training to mine more fine-grained information with low
semantic levels. Moreover, we put forward the Bidirectional Interaction Module
(BInM) to facilitate interactive learning among multimodal features.
Additionally, to address the difficulty of hierarchical multi-label
classification in this task, a Dynamic Selection Module (DSM) is designed to
select the feature representation that is most conducive to current protein
function prediction. Our proposed DSRPGO model improves significantly in BPO,
MFO, and CCO on human datasets, thereby outperforming other benchmark models.

</details>


### [85] [DartQuant: Efficient Rotational Distribution Calibration for LLM Quantization](https://arxiv.org/abs/2511.04063)
*Yuantian Shao,Yuanteng Chen,Peisong Wang,Jianlin Yu,Jing Lin,Yiwu Yao,Zhihui Wei,Jian Cheng*

Main category: cs.LG

TL;DR: 提出DartQuant方法，通过分布感知的旋转校准来优化大模型量化，显著降低计算成本并避免过拟合，在70B模型上实现47倍加速和10倍内存节省。


<details>
  <summary>Details</summary>
Motivation: 旋转矩阵能改善量化性能但端到端微调计算成本高且易过拟合，需要更高效的旋转优化方法。

Method: 提出分布感知旋转校准方法，通过约束旋转后激活值的分布来降低旋转优化复杂度，并引入QR-Orth优化方案替代交替优化。

Result: 在70B模型上实现47倍加速和10倍内存节省，首次在单张3090 GPU上完成70B模型的旋转校准。

Conclusion: DartQuant使大语言模型量化在资源受限环境中变得可行，为大规模模型部署提供了高效解决方案。

Abstract: Quantization plays a crucial role in accelerating the inference of
large-scale models, and rotational matrices have been shown to effectively
improve quantization performance by smoothing outliers. However, end-to-end
fine-tuning of rotational optimization algorithms incurs high computational
costs and is prone to overfitting. To address this challenge, we propose an
efficient distribution-aware rotational calibration method, DartQuant, which
reduces the complexity of rotational optimization by constraining the
distribution of the activations after rotation. This approach also effectively
reduces reliance on task-specific losses, thereby mitigating the risk of
overfitting. Additionally, we introduce the QR-Orth optimization scheme, which
replaces expensive alternating optimization with a more efficient solution. In
a variety of model quantization experiments, DartQuant demonstrates superior
performance. Compared to existing methods, it achieves 47$\times$ acceleration
and 10$\times$ memory savings for rotational optimization on a 70B model.
Furthermore, it is the first to successfully complete rotational calibration
for a 70B model on a single 3090 GPU, making quantization of large language
models feasible in resource-constrained environments. Code is available at
https://github.com/CAS-CLab/DartQuant.git.

</details>


### [86] [Learning Filter-Aware Distance Metrics for Nearest Neighbor Search with Multiple Filters](https://arxiv.org/abs/2511.04073)
*Ananya Sutradhar,Suryansh Gupta,Ravishankar Krishnaswamy,Haiyang Xu,Aseem Rastogi,Gopal Srinivasa*

Main category: cs.LG

TL;DR: 提出了一种学习数据驱动权重的方法来解决过滤近似最近邻搜索问题，相比固定惩罚方法提高了5-10%的准确率


<details>
  <summary>Details</summary>
Motivation: 现有基于图的方法使用固定、数据无关的惩罚来处理过滤条件，无法适应不同数据集的标签和向量分布差异

Method: 将过滤ANN搜索建模为约束线性优化问题，从数据中学习向量距离和过滤匹配之间的最优权衡权重

Result: 实验表明，通过数据自适应距离函数，相比固定惩罚方法准确率提升5-10%

Conclusion: 学习最优权衡权重的方法为过滤ANN搜索问题提供了更灵活和可泛化的框架

Abstract: Filtered Approximate Nearest Neighbor (ANN) search retrieves the closest
vectors for a query vector from a dataset. It enforces that a specified set of
discrete labels $S$ for the query must be included in the labels of each
retrieved vector. Existing graph-based methods typically incorporate filter
awareness by assigning fixed penalties or prioritizing nodes based on filter
satisfaction. However, since these methods use fixed, data in- dependent
penalties, they often fail to generalize across datasets with diverse label and
vector distributions. In this work, we propose a principled alternative that
learns the optimal trade-off between vector distance and filter match directly
from the data, rather than relying on fixed penalties. We formulate this as a
constrained linear optimization problem, deriving weights that better reflect
the underlying filter distribution and more effectively address the filtered
ANN search problem. These learned weights guide both the search process and
index construction, leading to graph structures that more effectively capture
the underlying filter distribution and filter semantics. Our experiments
demonstrate that adapting the distance function to the data significantly im-
proves accuracy by 5-10% over fixed-penalty methods, providing a more flexible
and generalizable framework for the filtered ANN search problem.

</details>


### [87] [DeNoise: Learning Robust Graph Representations for Unsupervised Graph-Level Anomaly Detection](https://arxiv.org/abs/2511.04086)
*Qingfeng Chen,Haojin Zeng,Jingyi Jie,Shichao Zhang,Debo Cheng*

Main category: cs.LG

TL;DR: DeNoise是一个针对训练数据被污染的图级异常检测框架，通过联合优化图编码器、属性解码器和结构解码器，结合对抗目标和对比学习，在噪声环境下学习鲁棒的图表示。


<details>
  <summary>Details</summary>
Motivation: 现实世界中图数据的训练集通常包含异常样本，而大多数图神经网络方法假设训练集是干净的，这会导致表示学习失真和性能下降。

Method: 提出DeNoise框架：1）联合优化图编码器、属性解码器和结构解码器；2）引入编码器锚点对齐去噪机制；3）使用对比学习压缩正常图嵌入并排斥异常图嵌入。

Result: 在8个真实数据集上的实验表明，DeNoise在不同噪声强度下都能学习可靠的图级表示，显著优于现有最先进的图级异常检测方法。

Conclusion: DeNoise能够有效处理训练数据被污染的情况，为图级异常检测提供了鲁棒的解决方案。

Abstract: With the rapid growth of graph-structured data in critical domains,
unsupervised graph-level anomaly detection (UGAD) has become a pivotal task.
UGAD seeks to identify entire graphs that deviate from normal behavioral
patterns. However, most Graph Neural Network (GNN) approaches implicitly assume
that the training set is clean, containing only normal graphs, which is rarely
true in practice. Even modest contamination by anomalous graphs can distort
learned representations and sharply degrade performance. To address this
challenge, we propose DeNoise, a robust UGAD framework explicitly designed for
contaminated training data. It jointly optimizes a graph-level encoder, an
attribute decoder, and a structure decoder via an adversarial objective to
learn noise-resistant embeddings. Further, DeNoise introduces an encoder
anchor-alignment denoising mechanism that fuses high-information node
embeddings from normal graphs into all graph embeddings, improving
representation quality while suppressing anomaly interference. A contrastive
learning component then compacts normal graph embeddings and repels anomalous
ones in the latent space. Extensive experiments on eight real-world datasets
demonstrate that DeNoise consistently learns reliable graph-level
representations under varying noise intensities and significantly outperforms
state-of-the-art UGAD baselines.

</details>


### [88] [KoTaP: A Panel Dataset for Corporate Tax Avoidance, Performance, and Governance in Korea](https://arxiv.org/abs/2511.04094)
*Hyungjong Na,Wonho Song,Seungyong Han,Donghyeon Jo,Sejin Myung,Hyungjoon Kim*

Main category: cs.LG

TL;DR: 本研究介绍了韩国避税面板(KoTaP)，这是一个包含2011-2024年韩国KOSPI和KOSDAQ上市非金融企业的长期面板数据集，包含12,653个公司年度观测值，涵盖避税、盈余管理、盈利能力、稳定性、增长和公司治理等多个维度。


<details>
  <summary>Details</summary>
Motivation: 构建一个标准化的长期面板数据集，将企业避税作为预测变量，并将其与多个领域联系起来，同时反映韩国企业特有的制度特征。

Method: 使用现金有效税率(CETR)、GAAP有效税率(GETR)和账面税收差异指标(TSTA、TSDA)等互补指标来衡量避税，并构建平衡面板结构。

Result: 创建了包含12,653个公司年度观测值的标准化数据集，具有国际可比性同时反映韩国企业特征，如股权集中、外资持股高和流动性比率高等。

Conclusion: KoTaP数据集为会计、金融和跨学科研究提供了重要开放资源，支持计量经济学和深度学习模型基准测试、外部有效性检验、可解释AI分析以及政策评估和投资分析。

Abstract: This study introduces the Korean Tax Avoidance Panel (KoTaP), a long-term
panel dataset of non-financial firms listed on KOSPI and KOSDAQ between 2011
and 2024. After excluding financial firms, firms with non-December fiscal year
ends, capital impairment, and negative pre-tax income, the final dataset
consists of 12,653 firm-year observations from 1,754 firms. KoTaP is designed
to treat corporate tax avoidance as a predictor variable and link it to
multiple domains, including earnings management (accrual- and activity-based),
profitability (ROA, ROE, CFO, LOSS), stability (LEV, CUR, SIZE, PPE, AGE,
INVREC), growth (GRW, MB, TQ), and governance (BIG4, FORN, OWN). Tax avoidance
itself is measured using complementary indicators cash effective tax rate
(CETR), GAAP effective tax rate (GETR), and book-tax difference measures (TSTA,
TSDA) with adjustments to ensure interpretability. A key strength of KoTaP is
its balanced panel structure with standardized variables and its consistency
with international literature on the distribution and correlation of core
indicators. At the same time, it reflects distinctive institutional features of
Korean firms, such as concentrated ownership, high foreign shareholding, and
elevated liquidity ratios, providing both international comparability and
contextual uniqueness. KoTaP enables applications in benchmarking econometric
and deep learning models, external validity checks, and explainable AI
analyses. It further supports policy evaluation, audit planning, and investment
analysis, making it a critical open resource for accounting, finance, and
interdisciplinary research.

</details>


### [89] [Decomposable Neuro Symbolic Regression](https://arxiv.org/abs/2511.04124)
*Giorgio Morales,John W. Sheppard*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Symbolic regression (SR) models complex systems by discovering mathematical
expressions that capture underlying relationships in observed data. However,
most SR methods prioritize minimizing prediction error over identifying the
governing equations, often producing overly complex or inaccurate expressions.
To address this, we present a decomposable SR method that generates
interpretable multivariate expressions leveraging transformer models, genetic
algorithms (GAs), and genetic programming (GP). In particular, our explainable
SR method distills a trained ``opaque'' regression model into mathematical
expressions that serve as explanations of its computed function. Our method
employs a Multi-Set Transformer to generate multiple univariate symbolic
skeletons that characterize how each variable influences the opaque model's
response. We then evaluate the generated skeletons' performance using a
GA-based approach to select a subset of high-quality candidates before
incrementally merging them via a GP-based cascade procedure that preserves
their original skeleton structure. The final multivariate skeletons undergo
coefficient optimization via a GA. We evaluated our method on problems with
controlled and varying degrees of noise, demonstrating lower or comparable
interpolation and extrapolation errors compared to two GP-based methods, three
neural SR methods, and a hybrid approach. Unlike them, our approach
consistently learned expressions that matched the original mathematical
structure.

</details>


### [90] [Exploring the Feasibility of End-to-End Large Language Model as a Compiler](https://arxiv.org/abs/2511.04132)
*Hongbin Zhang,Shihao Gao,Yang Liu,Mingjie Xing,Yanjun Wu,Chen Zhao*

Main category: cs.LG

TL;DR: 本文探索了将大型语言模型作为端到端编译器的可行性，设计了CompilerEval数据集评估LLMs在源代码理解和汇编代码生成方面的能力，发现LLMs具备基本编译能力但成功率低，通过优化提示、模型扩展和推理方法可显著提升汇编代码质量。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs已在编译器开发和维护中发挥作用，但其作为端到端编译器的潜力尚未充分探索。本文旨在研究LLM作为编译器的可行性及其未来发展方向。

Method: 设计了CompilerEval数据集和评估框架，分析主流LLMs在源代码理解和汇编代码生成方面的能力，通过错误分析、多种改进方法探索以及跨平台编译能力评估来研究LLM编译性能。

Result: 实验结果显示LLMs展现出基本的编译器能力，但目前编译成功率较低。通过提示优化、模型扩展和推理方法集成，可以显著提升LLM生成的汇编代码质量。

Conclusion: 对LLM作为编译器持乐观态度，提出了实用的架构设计和未来研究方向。通过针对性训练、知识丰富的提示和专用基础设施，LaaC有潜力生成高质量汇编代码并推动编译领域的范式转变。

Abstract: In recent years, end-to-end Large Language Model (LLM) technology has shown
substantial advantages across various domains. As critical system software and
infrastructure, compilers are responsible for transforming source code into
target code. While LLMs have been leveraged to assist in compiler development
and maintenance, their potential as an end-to-end compiler remains largely
unexplored. This paper explores the feasibility of LLM as a Compiler (LaaC) and
its future directions. We designed the CompilerEval dataset and framework
specifically to evaluate the capabilities of mainstream LLMs in source code
comprehension and assembly code generation. In the evaluation, we analyzed
various errors, explored multiple methods to improve LLM-generated code, and
evaluated cross-platform compilation capabilities. Experimental results
demonstrate that LLMs exhibit basic capabilities as compilers but currently
achieve low compilation success rates. By optimizing prompts, scaling up the
model, and incorporating reasoning methods, the quality of assembly code
generated by LLMs can be significantly enhanced. Based on these findings, we
maintain an optimistic outlook for LaaC and propose practical architectural
designs and future research directions. We believe that with targeted training,
knowledge-rich prompts, and specialized infrastructure, LaaC has the potential
to generate high-quality assembly code and drive a paradigm shift in the field
of compilation.

</details>


### [91] [Exchange Policy Optimization Algorithm for Semi-Infinite Safe Reinforcement Learning](https://arxiv.org/abs/2511.04147)
*Jiaming Zhang,Yujie Yang,Haoning Wang,Liping Zhang,Shengbo Eben Li*

Main category: cs.LG

TL;DR: 提出了交换策略优化(EPO)框架，用于解决半无限安全强化学习问题，通过迭代求解有限约束集的安全RL子问题，并自适应调整活动约束集，确保策略性能最优且安全约束严格有界。


<details>
  <summary>Details</summary>
Motivation: 实际应用中安全强化学习常涉及无限约束条件（半无限安全RL），如需要在连续参数空间（如每个空间位置）强制执行安全条件，现有方法难以有效处理此类问题。

Method: EPO通过迭代求解有限约束集的安全RL子问题，采用约束扩展和删除机制自适应调整活动集：违反预定义容差的约束被添加，拉格朗日乘子为零的约束在策略更新后被移除。

Result: 理论分析表明，在温和假设下，通过EPO训练的策略实现与最优解相当的性能，且全局约束违反严格保持在预定界限内。

Conclusion: EPO为半无限安全RL问题提供了有效的算法框架，能够平衡策略性能与安全约束，防止工作集无控制增长，支持有效的策略训练。

Abstract: Safe reinforcement learning (safe RL) aims to respect safety requirements
while optimizing long-term performance. In many practical applications,
however, the problem involves an infinite number of constraints, known as
semi-infinite safe RL (SI-safe RL). Such constraints typically appear when
safety conditions must be enforced across an entire continuous parameter space,
such as ensuring adequate resource distribution at every spatial location. In
this paper, we propose exchange policy optimization (EPO), an algorithmic
framework that achieves optimal policy performance and deterministic bounded
safety. EPO works by iteratively solving safe RL subproblems with finite
constraint sets and adaptively adjusting the active set through constraint
expansion and deletion. At each iteration, constraints with violations
exceeding the predefined tolerance are added to refine the policy, while those
with zero Lagrange multipliers are removed after the policy update. This
exchange rule prevents uncontrolled growth of the working set and supports
effective policy training. Our theoretical analysis demonstrates that, under
mild assumptions, strategies trained via EPO achieve performance comparable to
optimal solutions with global constraint violations strictly remaining within a
prescribed bound.

</details>


### [92] [Learning to Land Anywhere: Transferable Generative Models for Aircraft Trajectories](https://arxiv.org/abs/2511.04155)
*Olav Finne Praesteng Larsen,Massimiliano Ruocco,Michail Spitieris,Abdulmajid Murad,Martina Ragosta*

Main category: cs.LG

TL;DR: 研究探索了使用迁移学习将数据丰富机场训练的生成模型高效适配到数据稀缺机场的方法，结果表明扩散模型仅需目标机场5%的数据就能达到竞争性性能，20%数据即可达到基线水平。


<details>
  <summary>Details</summary>
Motivation: 许多次级和区域机场面临严重的轨迹数据稀缺问题，这限制了机器学习方法的应用和大规模模拟分析的能力。

Method: 采用最先进的扩散和流匹配架构，在苏黎世机场预训练模型，然后在都柏林机场使用不同比例（0%-100%）的本地数据进行微调。

Result: 扩散模型仅需都柏林机场5%的数据就能达到竞争性性能，20%数据即可达到基线水平，始终优于从头训练的模型。流匹配模型泛化能力较弱。

Conclusion: 迁移学习能够显著降低ATM中轨迹生成的数据需求，即使在历史记录有限的环境中也能实现现实的合成数据生成。

Abstract: Access to trajectory data is a key requirement for developing and validating
Air Traffic Management (ATM) solutions, yet many secondary and regional
airports face severe data scarcity. This limits the applicability of machine
learning methods and the ability to perform large-scale simulations or
"what-if" analyses. In this paper, we investigate whether generative models
trained on data-rich airports can be efficiently adapted to data-scarce
airports using transfer learning. We adapt state-of-the-art diffusion- and
flow-matching-based architectures to the aviation domain and evaluate their
transferability between Zurich (source) and Dublin (target) landing trajectory
datasets. Models are pretrained on Zurich and fine-tuned on Dublin with varying
amounts of local data, ranging from 0% to 100%. Results show that
diffusion-based models achieve competitive performance with as little as 5% of
the Dublin data and reach baseline-level performance around 20%, consistently
outperforming models trained from scratch across metrics and visual
inspections. Latent flow matching and latent diffusion models also benefit from
pretraining, though with more variable gains, while flow matching models show
weaker generalization. Despite challenges in capturing rare trajectory
patterns, these findings demonstrate the potential of transfer learning to
substantially reduce data requirements for trajectory generation in ATM,
enabling realistic synthetic data generation even in environments with limited
historical records.

</details>


### [93] [Deep Learning Approach for Clinical Risk Identification Using Transformer Modeling of Heterogeneous EHR Data](https://arxiv.org/abs/2511.04158)
*Anzhuo Xie,Wei-Chen Chang*

Main category: cs.LG

TL;DR: 提出基于Transformer的纵向建模方法，解决电子健康记录中不规则时间模式、大模态差异和复杂语义结构的挑战，在临床风险分类中优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决异构电子健康记录数据在临床风险分类中的挑战，包括不规则时间模式、大模态差异和复杂语义结构。

Method: 使用特征嵌入层统一表示结构化和非结构化数据，引入可学习的时间编码机制捕捉动态演化，采用多头自注意力结构进行全局依赖建模，设计语义加权池化模块增强语义表示。

Result: 实验结果显示，该模型在准确性、召回率、精确度和F1分数上优于传统机器学习和时序深度学习模型。

Conclusion: 该方法在多源异构电子健康记录环境中实现了稳定精确的风险识别，为临床智能决策提供了高效可靠框架。

Abstract: This study proposes a Transformer-based longitudinal modeling method to
address challenges in clinical risk classification with heterogeneous
Electronic Health Record (EHR) data, including irregular temporal patterns,
large modality differences, and complex semantic structures. The method takes
multi-source medical features as input and employs a feature embedding layer to
achieve a unified representation of structured and unstructured data. A
learnable temporal encoding mechanism is introduced to capture dynamic
evolution under uneven sampling intervals. The core model adopts a multi-head
self-attention structure to perform global dependency modeling on longitudinal
sequences, enabling the aggregation of long-term trends and short-term
fluctuations across different temporal scales. To enhance semantic
representation, a semantic-weighted pooling module is designed to assign
adaptive importance to key medical events, improving the discriminative ability
of risk-related features. Finally, a linear mapping layer generates
individual-level risk scores. Experimental results show that the proposed model
outperforms traditional machine learning and temporal deep learning models in
accuracy, recall, precision, and F1-Score, achieving stable and precise risk
identification in multi-source heterogeneous EHR environments and providing an
efficient and reliable framework for clinical intelligent decision-making.

</details>


### [94] [ScaleDL: Towards Scalable and Efficient Runtime Prediction for Distributed Deep Learning Workloads](https://arxiv.org/abs/2511.04162)
*Xiaokai Wang,Shaoyuan Huang,Yuting Li,Xiaofei Wang*

Main category: cs.LG

TL;DR: ScaleDL是一个新颖的DNN运行时预测框架，通过结合非线性逐层建模和图神经网络，在准确性和数据收集成本之间取得平衡，相比基线模型显著提升了预测精度。


<details>
  <summary>Details</summary>
Motivation: 随着DNN模型规模和复杂度的增加，训练和推理任务对分布式计算资源的需求急剧增长，准确的运行时预测对优化开发和资源分配至关重要。传统方法精度有限，而图增强方法数据收集成本过高。

Method: 提出ScaleDL框架，结合非线性逐层建模和基于GNN的跨层交互机制，并使用D-optimal方法降低数据收集成本。

Result: 在五个流行DNN模型上的实验证明，ScaleDL相比基线模型实现了6倍更低的MRE和5倍更低的RMSE，显著提升了运行时预测准确性和泛化能力。

Conclusion: ScaleDL在DNN运行时预测方面实现了准确性、泛化性和数据收集成本之间的良好平衡，为优化AI服务开发提供了有效解决方案。

Abstract: Deep neural networks (DNNs) form the cornerstone of modern AI services,
supporting a wide range of applications, including autonomous driving,
chatbots, and recommendation systems. As models increase in size and
complexity, DNN workloads like training and inference tasks impose
unprecedented demands on distributed computing resources, making the accurate
prediction of runtime essential for optimizing development and resource
allocation. Traditional methods rely on additive computational unit models,
limiting their accuracy and generalizability. In contrast, graph-enhanced
modeling improves performance but significantly increases data collection
costs. Therefore, there is a critical need for a method that strikes a balance
between accuracy, generalizability, and the costs of data collection. To
address these challenges, we propose ScaleDL, a novel runtime prediction
framework that combines nonlinear layer-wise modeling with graph neural network
(GNN)-based cross-layer interaction mechanism, enabling accurate DNN runtime
prediction and hierarchical generalizability across different network
architectures. Additionally, we employ the D-optimal method to reduce data
collection costs. Experiments on the workloads of five popular DNN models prove
that ScaleDL enhances runtime prediction accuracy and generalizability,
achieving 6$\times$ lower MRE and 5$\times$ lower RMSE compared to baseline
models.

</details>


### [95] [Block Rotation is All You Need for MXFP4 Quantization](https://arxiv.org/abs/2511.04214)
*Yuantian Shao,Peisong Wang,Yuanteng Chen,Chang Xu,Zhihui Wei,Jian Cheng*

Main category: cs.LG

TL;DR: 本文建立了MXFP4格式下PTQ方法的全面基准测试，发现GPTQ等方法表现良好，而基于旋转的方法与MXFP4存在严重不兼容。通过分析发现根本原因是MXFP4的PoT块缩放与全局旋转的能量重分配不匹配，并提出了有效的块旋转策略来改进基于旋转的方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型规模快速增长带来了巨大的内存、计算和能源成本，后训练量化是高效部署的解决方案。MXFP4作为一种新的FP4格式得到多种硬件支持，但现有量化方法主要针对INT4格式，需要评估其在MXFP4格式下的适用性。

Method: 建立了MXFP4格式下PTQ方法的全面基准测试，系统评估了包括GPTQ和基于旋转的方法在内的多种量化技术。深入分析了基于旋转方法与MXFP4不兼容的根本原因，并提出了一种简单有效的块旋转策略来适配MXFP4格式。

Result: GPTQ等方法在MXFP4格式下表现稳定，而基于旋转的方法（当前SOTA方法普遍采用）与MXFP4存在严重不兼容。提出的块旋转策略显著提高了基于旋转方法在MXFP4格式下的准确率，在多种LLM上都取得了实质性改进。

Conclusion: 研究结果为从业者提供了明确的实践指导，并为在新型低精度格式下推进PTQ研究奠定了基础。MXFP4格式需要专门适配的量化方法，特别是对于基于旋转的量化技术需要重新设计以克服与PoT块缩放的根本冲突。

Abstract: Large language models (LLMs) have achieved remarkable success, but their
rapidly growing scale imposes prohibitive costs in memory, computation, and
energy. Post-training quantization (PTQ) is a promising solution for efficient
deployment, yet achieving accurate W4A4 quantization remains an open challenge.
While most existing methods are designed for INT4 formats, the emergence of
MXFP4 -- a new FP4 format with various hardware support (NVIDIA, AMD, Intel)--
raises questions about the applicability of current techniques. In this work,
we establish a comprehensive benchmark of PTQ methods under the MXFP4 format.
Through systematic evaluation, we find that methods like GPTQ consistently
deliver strong performance, whereas rotation-based approaches, which are almost
used by all state-of-the-art approaches, suffer from severe incompatibility
with MXFP4. We further provide the first in-depth analysis of this conflict,
tracing its root to a fundamental mismatch between MXFP4's PoT (power-of-two)
block scaling and the redistribution of outlier energy via global rotation.
Building on this insight, we propose a simple yet effective block rotation
strategy that adapts rotation-based methods to MXFP4, leading to substantial
accuracy improvements across diverse LLMs. Our findings not only offer clear
guidance for practitioners but also set a foundation for advancing PTQ research
under emerging low-precision formats.

</details>


### [96] [The Strong Lottery Ticket Hypothesis for Multi-Head Attention Mechanisms](https://arxiv.org/abs/2511.04217)
*Hikari Otsuka,Daiki Chijiwa,Yasuyuki Okoshi,Daichi Fujiki,Susumu Takeuchi,Masato Motomura*

Main category: cs.LG

TL;DR: 本文证明了多头注意力机制和Transformer架构中存在强彩票票证，即随机初始化的网络包含可逼近目标网络的子网络。


<details>
  <summary>Details</summary>
Motivation: 当前强彩票票证假说的理论研究尚未涵盖Transformer架构中的多头注意力机制，这是Transformer的核心组件。

Method: 理论分析证明：当随机初始化的多头注意力具有足够大的隐藏维度时，高概率包含能逼近任意目标多头注意力的子网络。

Result: 实证验证表明，源模型与目标模型之间的逼近误差随源模型隐藏维度的增加呈指数级下降。

Conclusion: 成功将强彩票票证假说扩展到Transformer架构，填补了多头注意力机制理论研究的空白。

Abstract: The strong lottery ticket hypothesis (SLTH) conjectures that high-performing
subnetworks, called strong lottery tickets (SLTs), are hidden in randomly
initialized neural networks. Although recent theoretical studies have
established the SLTH across various neural architectures, the SLTH for
transformer architectures still lacks theoretical understanding. In particular,
the current theory of the SLTH does not yet account for the multi-head
attention (MHA) mechanism, a core component of transformers. To address this
gap, we introduce a theoretical analysis of the existence of SLTs within MHAs.
We prove that, if a randomly initialized MHA of $H$ heads and input dimension
$d$ has the hidden dimension $O(d\log(Hd^{3/2}))$ for the key and value, it
contains an SLT that approximates an arbitrary MHA with the same input
dimension with high probability. Furthermore, by leveraging this theory for
MHAs, we extend the SLTH to transformers without normalization layers. We
empirically validate our theoretical findings, demonstrating that the
approximation error between the SLT within a source model (MHA and transformer)
and an approximate target counterpart decreases exponentially by increasing the
hidden dimension of the source model.

</details>


### [97] [seqme: a Python library for evaluating biological sequence design](https://arxiv.org/abs/2511.04239)
*Rasmus Møller-Larsen,Adam Izdebski,Jan Olszewski,Pankhil Gawade,Michal Kmicikiewicz,Wojciech Zarzecki,Ewa Szczurek*

Main category: cs.LG

TL;DR: seqme是一个用于评估生物序列设计计算方法的开源Python库，包含序列、嵌入和属性三类模型无关的指标，适用于多种生物序列类型。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏一个统一的软件库来实现评估生物序列设计方法性能的指标，这些指标需要衡量设计序列对目标分布的保真度和所需属性的实现程度。

Method: 开发了seqme这个模块化且高度可扩展的开源Python库，包含序列、嵌入和属性三类模型无关的评估指标，提供多种嵌入和属性模型，以及诊断和可视化功能。

Result: 成功创建了一个适用于小分子、DNA、ncRNA、mRNA、肽和蛋白质等多种生物序列的评估工具，支持一次性设计和迭代设计方法的评估。

Conclusion: seqme填补了生物序列设计评估工具的空缺，为研究人员提供了一个统一、灵活且功能丰富的评估平台。

Abstract: Recent advances in computational methods for designing biological sequences
have sparked the development of metrics to evaluate these methods performance
in terms of the fidelity of the designed sequences to a target distribution and
their attainment of desired properties. However, a single software library
implementing these metrics was lacking. In this work we introduce seqme, a
modular and highly extendable open-source Python library, containing
model-agnostic metrics for evaluating computational methods for biological
sequence design. seqme considers three groups of metrics: sequence-based,
embedding-based, and property-based, and is applicable to a wide range of
biological sequences: small molecules, DNA, ncRNA, mRNA, peptides and proteins.
The library offers a number of embedding and property models for biological
sequences, as well as diagnostics and visualization functions to inspect the
results. seqme can be used to evaluate both one-shot and iterative
computational design methods.

</details>


### [98] [Guided by Stars: Interpretable Concept Learning Over Time Series via Temporal Logic Semantics](https://arxiv.org/abs/2511.04244)
*Irene Ferfoglia,Simone Silvetti,Gaia Saveri,Laura Nenzi,Luca Bortolussi*

Main category: cs.LG

TL;DR: STELLE是一个神经符号框架，通过将时间序列嵌入到时序逻辑概念空间，统一分类和解释，提供可解释的时间序列分类。


<details>
  <summary>Details</summary>
Motivation: 时间序列分类在安全关键应用中很重要，但深度学习方法通常是黑盒的，难以理解其决策依据。

Method: 引入基于信号时序逻辑(STL)的新核函数，将原始时间序列映射到与预定义STL公式的对齐程度，联合优化准确性和可解释性。

Result: 实验证明STELLE在保持竞争力的准确性的同时，提供逻辑忠实的解释，在多样化真实世界基准上得到验证。

Conclusion: STELLE框架成功实现了时间序列分类与解释的统一，为黑盒深度学习模型提供了可解释的替代方案。

Abstract: Time series classification is a task of paramount importance, as this kind of
data often arises in safety-critical applications. However, it is typically
tackled with black-box deep learning methods, making it hard for humans to
understand the rationale behind their output. To take on this challenge, we
propose a novel approach, STELLE (Signal Temporal logic Embedding for
Logically-grounded Learning and Explanation), a neuro-symbolic framework that
unifies classification and explanation through direct embedding of trajectories
into a space of temporal logic concepts. By introducing a novel STL-inspired
kernel that maps raw time series to their alignment with predefined STL
formulae, our model jointly optimises accuracy and interpretability, as each
prediction is accompanied by the most relevant logical concepts that
characterise it. This yields (i) local explanations as human-readable STL
conditions justifying individual predictions, and (ii) global explanations as
class-characterising formulae. Experiments demonstrate that STELLE achieves
competitive accuracy while providing logically faithful explanations, validated
on diverse real-world benchmarks.

</details>


### [99] [Efficient Reinforcement Learning from Human Feedback via Bayesian Preference Inference](https://arxiv.org/abs/2511.04286)
*Matteo Cercola,Valeria Capretti,Simone Formentin*

Main category: cs.LG

TL;DR: 提出了一个混合框架，将RLHF的可扩展性与PBO的查询效率相结合，通过在RLHF流程中集成主动查询模块，实现样本高效的人类偏好学习。


<details>
  <summary>Details</summary>
Motivation: 从人类偏好中学习是使机器学习模型与主观人类判断对齐的关键，但收集偏好数据成本高且耗时，需要更高效的学习范式。

Method: 在RLHF流程中集成主动查询模块，结合RLHF的可扩展性和PBO的样本效率优势，实现主动且样本高效的偏好收集。

Result: 在两个代表性领域（高维偏好优化和LLM微调）的实验中，该方法在样本效率和整体性能上均表现出持续改进。

Conclusion: 所提出的混合框架成功地将RLHF的可扩展性与PBO的查询效率相结合，为人类偏好学习提供了更高效的解决方案。

Abstract: Learning from human preferences is a cornerstone of aligning machine learning
models with subjective human judgments. Yet, collecting such preference data is
often costly and time-consuming, motivating the need for more efficient
learning paradigms. Two established approaches offer complementary advantages:
RLHF scales effectively to high-dimensional tasks such as LLM fine-tuning,
while PBO achieves greater sample efficiency through active querying. We
propose a hybrid framework that unifies RLHF's scalability with PBO's query
efficiency by integrating an acquisition-driven module into the RLHF pipeline,
thereby enabling active and sample-efficient preference gathering. We validate
the proposed approach on two representative domains: (i) high-dimensional
preference optimization and (ii) LLM fine-tuning. Experimental results
demonstrate consistent improvements in both sample efficiency and overall
performance across these tasks.

</details>


### [100] [The Illusion of Certainty: Uncertainty quantification for LLMs fails under ambiguity](https://arxiv.org/abs/2511.04418)
*Tim Tomov,Dominik Fuchsgruber,Tom Wollschläger,Stephan Günnemann*

Main category: cs.LG

TL;DR: 现有不确定性量化方法在无歧义任务中表现良好，但在歧义数据上性能大幅下降，揭示了当前LLM不确定性量化方法的关键缺陷。


<details>
  <summary>Details</summary>
Motivation: 真实语言具有固有的歧义性，但现有不确定性量化方法通常只在无歧义任务上进行基准测试，无法有效处理歧义数据。

Method: 引入了MAQA*和AmbigQA*两个歧义问答数据集，包含基于事实共现估计的真实答案分布；评估了三种不确定性估计方法：预测分布、内部表示和模型集成。

Result: 当前不确定性估计器在歧义数据上性能下降到接近随机水平；预测分布和集成方法在歧义情况下存在根本性限制。

Conclusion: 研究揭示了当前LLM不确定性量化方法的关键不足，需要重新思考当前的建模范式。

Abstract: Accurate uncertainty quantification (UQ) in Large Language Models (LLMs) is
critical for trustworthy deployment. While real-world language is inherently
ambiguous, reflecting aleatoric uncertainty, existing UQ methods are typically
benchmarked against tasks with no ambiguity. In this work, we demonstrate that
while current uncertainty estimators perform well under the restrictive
assumption of no ambiguity, they degrade to close-to-random performance on
ambiguous data. To this end, we introduce MAQA* and AmbigQA*, the first
ambiguous question-answering (QA) datasets equipped with ground-truth answer
distributions estimated from factual co-occurrence. We find this performance
deterioration to be consistent across different estimation paradigms: using the
predictive distribution itself, internal representations throughout the model,
and an ensemble of models. We show that this phenomenon can be theoretically
explained, revealing that predictive-distribution and ensemble-based estimators
are fundamentally limited under ambiguity. Overall, our study reveals a key
shortcoming of current UQ methods for LLMs and motivates a rethinking of
current modeling paradigms.

</details>


### [101] [Differentially Private In-Context Learning with Nearest Neighbor Search](https://arxiv.org/abs/2511.04332)
*Antti Koskela,Tejas Kulkarni,Laith Zumot*

Main category: cs.LG

TL;DR: 提出了一种差分隐私上下文学习框架，通过隐私感知的最近邻搜索检索相关示例，在文本分类和文档问答任务中显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的差分隐私上下文学习方法忽略了现代大语言模型流水线中的关键组件——用于检索相关上下文数据的相似性搜索，存在隐私风险。

Method: 采用最近邻检索从上下文数据库中获取相关示例，结合隐私过滤器跟踪选定样本的累积隐私成本，确保符合中心差分隐私预算。

Result: 在所有评估基准上都显著优于现有基线方法，实现了更优的隐私-效用权衡。

Conclusion: 提出的差分隐私上下文学习框架通过集成隐私感知的最近邻搜索，有效提升了隐私保护效果和模型性能。

Abstract: Differentially private in-context learning (DP-ICL) has recently become an
active research topic due to the inherent privacy risks of in-context learning.
However, existing approaches overlook a critical component of modern large
language model (LLM) pipelines: the similarity search used to retrieve relevant
context data. In this work, we introduce a DP framework for in-context learning
that integrates nearest neighbor search of relevant examples in a privacy-aware
manner. Our method outperforms existing baselines by a substantial margin
across all evaluated benchmarks, achieving more favorable privacy-utility
trade-offs. To achieve this, we employ nearest neighbor retrieval from a
database of context data, combined with a privacy filter that tracks the
cumulative privacy cost of selected samples to ensure adherence to a central
differential privacy budget. Experimental results on text classification and
document question answering show a clear advantage of the proposed method over
existing baselines.

</details>


### [102] [Ground-Truth Subgraphs for Better Training and Evaluation of Knowledge Graph Augmented LLMs](https://arxiv.org/abs/2511.04473)
*Alberto Cattaneo,Carlo Luschi,Daniel Justus*

Main category: cs.LG

TL;DR: SynthKGQA是一个从知识图谱生成高质量合成问答数据集的框架，用于改进知识图谱检索器的评估和训练。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏具有真实答案的挑战性知识图谱问答数据集，难以比较不同的知识图谱检索方法。

Method: 开发了SynthKGQA框架，可以从任何知识图谱生成合成问答数据集，并提供完整的真实事实集来推理每个问题。

Result: 应用SynthKGQA到Wikidata生成了GTSQA数据集，用于测试知识图谱检索器在未见过的图结构和关系类型上的零样本泛化能力。

Conclusion: SynthKGQA不仅支持更有效的信息检索基准测试，还能训练更好的模型，为知识图谱增强的大语言模型提供了新的评估标准。

Abstract: Retrieval of information from graph-structured knowledge bases represents a
promising direction for improving the factuality of LLMs. While various
solutions have been proposed, a comparison of methods is difficult due to the
lack of challenging QA datasets with ground-truth targets for graph retrieval.
We present SynthKGQA, a framework for generating high-quality synthetic
Knowledge Graph Question Answering datasets from any Knowledge Graph, providing
the full set of ground-truth facts in the KG to reason over each question. We
show how, in addition to enabling more informative benchmarking of KG
retrievers, the data produced with SynthKGQA also allows us to train better
models. We apply SynthKGQA to Wikidata to generate GTSQA, a new dataset
designed to test zero-shot generalization abilities of KG retrievers with
respect to unseen graph structures and relation types, and benchmark popular
solutions for KG-augmented LLMs on it.

</details>


### [103] [LUME-DBN: Full Bayesian Learning of DBNs from Incomplete data in Intensive Care](https://arxiv.org/abs/2511.04333)
*Federico Pirola,Fabio Stella,Marco Grzegorczyk*

Main category: cs.LG

TL;DR: 提出一种基于吉布斯采样的动态贝叶斯网络学习方法，用于处理临床纵向数据中的缺失值问题，通过将缺失值视为高斯分布参数进行采样，实现原则性填补和不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要源自静态贝叶斯网络文献，未能充分考虑数据的时序特性，限制了在重症监护等关键场景下量化时间不确定性的能力。

Method: 使用吉布斯采样方法，将每个缺失值视为遵循高斯分布的未知参数，在每次迭代中从未观测值的完全条件分布中进行采样。

Result: 在模拟数据集和真实重症监护数据上的评估显示，相比MICE等标准方法，该贝叶斯方法具有更优的重建精度和收敛特性。

Conclusion: 该方法支持更安全、更明智的临床决策，特别是在缺失数据频繁且可能产生重大影响的场景中。

Abstract: Dynamic Bayesian networks (DBNs) are increasingly used in healthcare due to
their ability to model complex temporal relationships in patient data while
maintaining interpretability, an essential feature for clinical
decision-making. However, existing approaches to handling missing data in
longitudinal clinical datasets are largely derived from static Bayesian
networks literature, failing to properly account for the temporal nature of the
data. This gap limits the ability to quantify uncertainty over time, which is
particularly critical in settings such as intensive care, where understanding
the temporal dynamics is fundamental for model trustworthiness and
applicability across diverse patient groups. Despite the potential of DBNs, a
full Bayesian framework that integrates missing data handling remains
underdeveloped. In this work, we propose a novel Gibbs sampling-based method
for learning DBNs from incomplete data. Our method treats each missing value as
an unknown parameter following a Gaussian distribution. At each iteration, the
unobserved values are sampled from their full conditional distributions,
allowing for principled imputation and uncertainty estimation. We evaluate our
method on both simulated datasets and real-world intensive care data from
critically ill patients. Compared to standard model-agnostic techniques such as
MICE, our Bayesian approach demonstrates superior reconstruction accuracy and
convergence properties. These results highlight the clinical relevance of
incorporating full Bayesian inference in temporal models, providing more
reliable imputations and offering deeper insight into model behavior. Our
approach supports safer and more informed clinical decision-making,
particularly in settings where missing data are frequent and potentially
impactful.

</details>


### [104] [Spurious Correlation-Aware Embedding Regularization for Worst-Group Robustness](https://arxiv.org/abs/2511.04401)
*Subeen Park,Joowang Kim,Hakyung Lee,Sunjae Yoo,Kyungwoo Song*

Main category: cs.LG

TL;DR: 提出SCER方法，通过正则化特征表示来抑制虚假相关性，提升模型在最差群体上的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 深度学习模型依赖虚假相关性，在子群体分布偏移场景下表现不佳，现有方法缺乏连接嵌入空间表示与最差群体误差的理论框架

Method: SCER方法在嵌入层面对特征表示施加理论约束，鼓励模型关注核心特征而非虚假模式，基于跨域和跨类的群体均值嵌入差异识别核心与虚假方向

Result: 在多个视觉和语言任务上，SCER在最差群体准确率上优于现有最先进方法

Conclusion: SCER通过理论驱动的嵌入正则化有效提升了模型对分布偏移的鲁棒性，特别是在最差群体上的表现

Abstract: Deep learning models achieve strong performance across various domains but
often rely on spurious correlations, making them vulnerable to distribution
shifts. This issue is particularly severe in subpopulation shift scenarios,
where models struggle in underrepresented groups. While existing methods have
made progress in mitigating this issue, their performance gains are still
constrained. They lack a rigorous theoretical framework connecting the
embedding space representations with worst-group error. To address this
limitation, we propose Spurious Correlation-Aware Embedding Regularization for
Worst-Group Robustness (SCER), a novel approach that directly regularizes
feature representations to suppress spurious cues. We show theoretically that
worst-group error is influenced by how strongly the classifier relies on
spurious versus core directions, identified from differences in group-wise mean
embeddings across domains and classes. By imposing theoretical constraints at
the embedding level, SCER encourages models to focus on core features while
reducing sensitivity to spurious patterns. Through systematic evaluation on
multiple vision and language, we show that SCER outperforms prior
state-of-the-art studies in worst-group accuracy. Our code is available at
\href{https://github.com/MLAI-Yonsei/SCER}{https://github.com/MLAI-Yonsei/SCER}.

</details>


### [105] [On the Equivalence of Regression and Classification](https://arxiv.org/abs/2511.04422)
*Jayadeva,Naman Dwivedi,Hari Krishnan,N. M. Anoop Krishnan*

Main category: cs.LG

TL;DR: 本文建立了回归与分类之间的形式化等价关系，证明了M个样本的回归问题等价于2M个样本的线性可分分类任务，并基于此提出了新的回归公式和可回归性度量方法。


<details>
  <summary>Details</summary>
Motivation: 回归与分类之间的形式化联系一直很薄弱，支持向量回归中的边界最大化项通常只被解释为正则化器，缺乏严格的理论基础。

Method: 通过证明回归问题与线性可分分类任务的等价性，将回归转化为分类问题，并利用边界最大化推导出新的回归公式，同时提出了可回归性度量方法。

Result: 建立了回归与分类的严格等价关系，提出了基于分类边界最大化的新回归公式，并开发了无需先学习模型即可评估数据集回归难度的可回归性度量。

Conclusion: 该等价关系为回归问题提供了新的理论框架，使得可以利用分类理论来改进回归方法，并为评估回归任务的难度提供了实用工具。

Abstract: A formal link between regression and classification has been tenuous. Even
though the margin maximization term $\|w\|$ is used in support vector
regression, it has at best been justified as a regularizer. We show that a
regression problem with $M$ samples lying on a hyperplane has a one-to-one
equivalence with a linearly separable classification task with $2M$ samples. We
show that margin maximization on the equivalent classification task leads to a
different regression formulation than traditionally used. Using the
equivalence, we demonstrate a ``regressability'' measure, that can be used to
estimate the difficulty of regressing a dataset, without needing to first learn
a model for it. We use the equivalence to train neural networks to learn a
linearizing map, that transforms input variables into a space where a linear
regressor is adequate.

</details>


### [106] [Federated Stochastic Minimax Optimization under Heavy-Tailed Noises](https://arxiv.org/abs/2511.04456)
*Xinwen Zhang,Hongchang Gao*

Main category: cs.LG

TL;DR: 本文针对联邦学习中的非凸-PL极小极大优化问题，在重尾梯度噪声条件下提出了两种新算法：Fed-NSGDA-M（集成归一化梯度）和FedMuon-DA（利用Muon优化器进行局部更新），并建立了理论收敛保证。


<details>
  <summary>Details</summary>
Motivation: 重尾噪声在非凸随机优化中日益受到关注，因为大量实证研究表明它比标准有界方差假设更符合实际情况。本文旨在解决联邦学习环境下重尾梯度噪声对极小极大优化的影响。

Method: 提出了两种新算法：Fed-NSGDA-M通过归一化梯度处理重尾噪声，FedMuon-DA利用Muon优化器进行局部更新。两种算法都在较温和的条件下有效处理联邦极小极大优化中的重尾噪声。

Result: 理论上证明了两种算法都能达到$O({1}/{(TNp)^{rac{s-1}{2s}}})$的收敛速率。据作者所知，这是首个在重尾噪声条件下具有严格理论保证的联邦极小极大优化算法。

Conclusion: 提出的两种算法在理论和实验上都验证了在联邦学习环境下处理重尾梯度噪声的有效性，为相关研究提供了新的解决方案。

Abstract: Heavy-tailed noise has attracted growing attention in nonconvex stochastic
optimization, as numerous empirical studies suggest it offers a more realistic
assumption than standard bounded variance assumption. In this work, we
investigate nonconvex-PL minimax optimization under heavy-tailed gradient noise
in federated learning. We propose two novel algorithms: Fed-NSGDA-M, which
integrates normalized gradients, and FedMuon-DA, which leverages the Muon
optimizer for local updates. Both algorithms are designed to effectively
address heavy-tailed noise in federated minimax optimization, under a milder
condition. We theoretically establish that both algorithms achieve a
convergence rate of $O({1}/{(TNp)^{\frac{s-1}{2s}}})$. To the best of our
knowledge, these are the first federated minimax optimization algorithms with
rigorous theoretical guarantees under heavy-tailed noise. Extensive experiments
further validate their effectiveness.

</details>


### [107] [Towards Causal Market Simulators](https://arxiv.org/abs/2511.04469)
*Dennis Thumm,Luis Ontaneda Mijares*

Main category: cs.LG

TL;DR: 提出了TNCM-VAE模型，结合变分自编码器和结构因果模型，生成具有因果关系的反事实金融时间序列数据


<details>
  <summary>Details</summary>
Motivation: 现有市场生成器缺乏因果推理能力，无法进行反事实分析和风险评估

Method: 在解码器架构中通过有向无环图施加因果约束，并使用因果Wasserstein距离进行训练

Result: 在受Ornstein-Uhlenbeck过程启发的自回归模型上验证，反事实概率估计的L1距离低至0.03-0.10

Conclusion: 该模型能够生成尊重底层因果机制的反事实市场轨迹，支持金融压力测试、情景分析和改进的回测

Abstract: Market generators using deep generative models have shown promise for
synthetic financial data generation, but existing approaches lack causal
reasoning capabilities essential for counterfactual analysis and risk
assessment. We propose a Time-series Neural Causal Model VAE (TNCM-VAE) that
combines variational autoencoders with structural causal models to generate
counterfactual financial time series while preserving both temporal
dependencies and causal relationships. Our approach enforces causal constraints
through directed acyclic graphs in the decoder architecture and employs the
causal Wasserstein distance for training. We validate our method on synthetic
autoregressive models inspired by the Ornstein-Uhlenbeck process, demonstrating
superior performance in counterfactual probability estimation with L1 distances
as low as 0.03-0.10 compared to ground truth. The model enables financial
stress testing, scenario analysis, and enhanced backtesting by generating
plausible counterfactual market trajectories that respect underlying causal
mechanisms.

</details>


### [108] [Distribution-Aware Tensor Decomposition for Compression of Convolutional Neural Networks](https://arxiv.org/abs/2511.04494)
*Alper Kalle,Theo Rudkiewicz,Mohamed-Oumar Ouerfelli,Mohamed Tamaazousti*

Main category: cs.LG

TL;DR: 提出一种基于数据信息的神经网络压缩方法，通过张量分解和低秩表示来减少模型的计算和内存需求，直接在函数空间优化误差而非传统的权重空间。


<details>
  <summary>Details</summary>
Motivation: 神经网络在图像任务中计算需求大，传统压缩方法在权重空间最小化Frobenius范数，但这种方法不考虑数据分布，通常需要后压缩微调。

Method: 使用数据信息范数衡量层输出分布的变化，提出新的交替最小二乘算法用于Tucker-2和CPD张量分解，直接优化基于输入协方差的新范数。

Result: 在多个CNN架构和数据集上的实验表明，该方法无需后压缩微调即可达到竞争性精度，且协方差范数可在不同数据集间迁移。

Conclusion: 数据信息压缩方法优于传统方法，能够有效压缩神经网络而无需微调，具有实际应用价值。

Abstract: Neural networks are widely used for image-related tasks but typically demand
considerable computing power. Once a network has been trained, however, its
memory- and compute-footprint can be reduced by compression. In this work, we
focus on compression through tensorization and low-rank representations.
Whereas classical approaches search for a low-rank approximation by minimizing
an isotropic norm such as the Frobenius norm in weight-space, we use
data-informed norms that measure the error in function space. Concretely, we
minimize the change in the layer's output distribution, which can be expressed
as $\lVert (W - \widetilde{W}) \Sigma^{1/2}\rVert_F$ where $\Sigma^{1/2}$ is
the square root of the covariance matrix of the layer's input and $W$,
$\widetilde{W}$ are the original and compressed weights. We propose new
alternating least square algorithms for the two most common tensor
decompositions (Tucker-2 and CPD) that directly optimize the new norm. Unlike
conventional compression pipelines, which almost always require
post-compression fine-tuning, our data-informed approach often achieves
competitive accuracy without any fine-tuning. We further show that the same
covariance-based norm can be transferred from one dataset to another with only
a minor accuracy drop, enabling compression even when the original training
dataset is unavailable. Experiments on several CNN architectures (ResNet-18/50,
and GoogLeNet) and datasets (ImageNet, FGVC-Aircraft, Cifar10, and Cifar100)
confirm the advantages of the proposed method.

</details>


### [109] [Linear Mode Connectivity under Data Shifts for Deep Ensembles of Image Classifiers](https://arxiv.org/abs/2511.04514)
*C. Hepburn,T. Zielke,A. P. Raulf*

Main category: cs.LG

TL;DR: 本文研究线性模式连接(LMC)在数据偏移下的表现，发现通过小学习率和大批量可以减轻数据偏移的影响，LMC有助于在训练效率和多样化集成模型之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 研究LMC在数据偏移条件下的表现，探索如何减轻数据偏移对模型训练的影响，理解LMC与训练稳定性、局部最小值平滑度和泛化能力之间的关系。

Method: 将数据偏移解释为随机梯度噪声的额外来源，通过实验研究学习率和批量大小对LMC的影响，分析模型是否收敛到相同局部最小值或不同区域。

Result: 小学习率和大批量可以减少数据偏移的影响；通过LMC采样的模型倾向于做出更相似的错误预测；LMC在训练效率和多样化集成之间提供了平衡。

Conclusion: 数据偏移可被视为随机梯度噪声，通过适当的学习率和批量大小可以减轻其影响；LMC有助于在保持训练效率的同时获得集成模型的多样性优势。

Abstract: The phenomenon of linear mode connectivity (LMC) links several aspects of
deep learning, including training stability under noisy stochastic gradients,
the smoothness and generalization of local minima (basins), the similarity and
functional diversity of sampled models, and architectural effects on data
processing. In this work, we experimentally study LMC under data shifts and
identify conditions that mitigate their impact. We interpret data shifts as an
additional source of stochastic gradient noise, which can be reduced through
small learning rates and large batch sizes. These parameters influence whether
models converge to the same local minimum or to regions of the loss landscape
with varying smoothness and generalization. Although models sampled via LMC
tend to make similar errors more frequently than those converging to different
basins, the benefit of LMC lies in balancing training efficiency against the
gains achieved from larger, more diverse ensembles. Code and supplementary
materials will be made publicly available at https://github.com/DLR-KI/LMC in
due course.

</details>


### [110] [Uncertainty Quantification for Reduced-Order Surrogate Models Applied to Cloud Microphysics](https://arxiv.org/abs/2511.04534)
*Jonas E. Katona,Emily K. de Jong,Nipun Gunawardena*

Main category: cs.LG

TL;DR: 提出了一种后处理、模型无关的框架，用于潜在空间降阶模型中的预测不确定性量化，无需修改底层架构或训练过程。


<details>
  <summary>Details</summary>
Motivation: 现有降阶模型缺乏稳健的不确定性量化方法，且现有方法通常受限于特定架构或训练方式，限制了灵活性和泛化能力。

Method: 使用保形预测方法，在降阶模型管道的多个组件中估计统计预测区间：潜在动力学、重构和端到端预测。

Result: 在云微物理学的潜在空间动力学模型上验证了该方法，能够准确预测液滴尺寸分布的演化，并在整个降阶模型管道中量化不确定性。

Conclusion: 该方法为降阶模型提供了一种灵活且通用的不确定性量化解决方案，适用于各种应用场景。

Abstract: Reduced-order models (ROMs) can efficiently simulate high-dimensional
physical systems, but lack robust uncertainty quantification methods. Existing
approaches are frequently architecture- or training-specific, which limits
flexibility and generalization. We introduce a post hoc, model-agnostic
framework for predictive uncertainty quantification in latent space ROMs that
requires no modification to the underlying architecture or training procedure.
Using conformal prediction, our approach estimates statistical prediction
intervals for multiple components of the ROM pipeline: latent dynamics,
reconstruction, and end-to-end predictions. We demonstrate the method on a
latent space dynamical model for cloud microphysics, where it accurately
predicts the evolution of droplet-size distributions and quantifies uncertainty
across the ROM pipeline.

</details>


### [111] [Integrating Temporal and Structural Context in Graph Transformers for Relational Deep Learning](https://arxiv.org/abs/2511.04557)
*Divyansha Lachi,Mahmoud Mohammadi,Joe Meyer,Vinam Arora,Tom Palczewski,Eva L. Dyer*

Main category: cs.LG

TL;DR: 提出了一种用于关系深度学习的图变换器架构RGP，通过时间子图采样器和交叉注意力潜在瓶颈来整合时空依赖关系，支持多任务预测。


<details>
  <summary>Details</summary>
Motivation: 现有图模型主要关注空间结构，将时间信息仅作为过滤约束而非建模信号，且通常只支持单任务预测，无法满足医疗、金融等领域的复杂时空交互需求。

Method: 引入时间子图采样器捕获时间相关关系；提出RGP架构，使用交叉注意力潜在瓶颈整合结构和时间上下文信息；采用灵活交叉注意力解码器支持多任务联合学习。

Result: 在RelBench、SALT和CTU数据集上的实验表明，RGP实现了最先进的性能。

Conclusion: RGP为关系深度学习提供了一个通用且可扩展的解决方案，能够支持多样化的预测任务。

Abstract: In domains such as healthcare, finance, and e-commerce, the temporal dynamics
of relational data emerge from complex interactions-such as those between
patients and providers, or users and products across diverse categories. To be
broadly useful, models operating on these data must integrate long-range
spatial and temporal dependencies across diverse types of entities, while also
supporting multiple predictive tasks. However, existing graph models for
relational data primarily focus on spatial structure, treating temporal
information merely as a filtering constraint to exclude future events rather
than a modeling signal, and are typically designed for single-task prediction.
To address these gaps, we introduce a temporal subgraph sampler that enhances
global context by retrieving nodes beyond the immediate neighborhood to capture
temporally relevant relationships. In addition, we propose the Relational Graph
Perceiver (RGP), a graph transformer architecture for relational deep learning
that leverages a cross-attention-based latent bottleneck to efficiently
integrate information from both structural and temporal contexts. This latent
bottleneck integrates signals from different node and edge types into a common
latent space, enabling the model to build global context across the entire
relational system. RGP also incorporates a flexible cross-attention decoder
that supports joint learning across tasks with disjoint label spaces within a
single model. Experiments on RelBench, SALT, and CTU show that RGP delivers
state-of-the-art performance, offering a general and scalable solution for
relational deep learning with support for diverse predictive tasks.

</details>


### [112] [ARETE: an R package for Automated REtrieval from TExt with large language models](https://arxiv.org/abs/2511.04573)
*Vasco V. Branco,Jandó Benedek,Lidia Pivovarova,Luís Correia,Pedro Cardoso*

Main category: cs.LG

TL;DR: ARETE是一个基于大型语言模型的R包，用于自动从科学文献中提取物种出现数据，显著提高了数据提取效率并扩展了已知物种分布范围。


<details>
  <summary>Details</summary>
Motivation: 传统物种出现数据提取需要大量人工工作，且无法满足人类活动加速带来的快速数据收集需求。科学文献中的关键信息往往不是机器可读的。

Method: 开发ARETE R包，利用chatGPT API进行自动化数据提取，整合了从光学字符识别到异常值检测和表格输出的完整流程，并与人工标注进行比较验证。

Result: 对100种蜘蛛进行测试，新提取的数据使已知出现范围平均扩大了三个数量级，揭示了新的历史分布区域，对空间保护规划和灭绝风险评估有重要意义。

Conclusion: ARETE能够快速获取未开发的物种出现数据，是相关项目的潜在变革工具，使研究人员能更好地分配资源，在项目规划中预测可用文献数据。

Abstract: 1. A hard stop for the implementation of rigorous conservation initiatives is
our lack of key species data, especially occurrence data. Furthermore,
researchers have to contend with an accelerated speed at which new information
must be collected and processed due to anthropogenic activity. Publications
ranging from scientific papers to gray literature contain this crucial
information but their data are often not machine-readable, requiring extensive
human work to be retrieved. 2. We present the ARETE R package, an open-source
software aiming to automate data extraction of species occurrences powered by
large language models, namely using the chatGPT Application Programming
Interface. This R package integrates all steps of the data extraction and
validation process, from Optical Character Recognition to detection of outliers
and output in tabular format. Furthermore, we validate ARETE through systematic
comparison between what is modelled and the work of human annotators. 3. We
demonstrate the usefulness of the approach by comparing range maps produced
using GBIF data and with those automatically extracted for 100 species of
spiders. Newly extracted data allowed to expand the known Extent of Occurrence
by a mean three orders of magnitude, revealing new areas where the species were
found in the past, which mayhave important implications for spatial
conservation planning and extinction risk assessments. 4. ARETE allows faster
access to hitherto untapped occurrence data, a potential game changer in
projects requiring such data. Researchers will be able to better prioritize
resources, manually verifying selected species while maintaining automated
extraction for the majority. This workflow also allows predicting available
bibliographic data during project planning.

</details>


### [113] [Complexity as Advantage: A Regret-Based Perspective on Emergent Structure](https://arxiv.org/abs/2511.04590)
*Oshri Naparstek*

Main category: cs.LG

TL;DR: CAA框架将复杂性定义为系统相对于观察者家族的特性，通过评估系统对不同观察者建模时产生的预测遗憾来衡量复杂性。系统复杂意味着对某些观察者容易预测而对其他观察者困难，从而创造信息优势。


<details>
  <summary>Details</summary>
Motivation: 传统方法将复杂性视为系统固有属性，而CAA框架旨在从观察者依赖的角度重新定义复杂性，统一多种涌现行为概念，并为复杂性为何具有功能价值提供量化基础。

Method: 提出CAA框架，通过预测遗憾来衡量复杂性，展示该框架如何统一多尺度熵、预测信息和观察者依赖结构等概念，并通过简单动力学模型进行验证。

Result: CAA框架成功统一了多种涌现行为概念，表明"有趣"系统是那些能在不同观察者间创造差异化预测遗憾的系统，为复杂性的功能价值提供了量化依据。

Conclusion: CAA框架为理解复杂性提供了新的观察者依赖视角，表明复杂性源于系统对不同观察者预测能力的差异化影响，这对学习、进化和人工代理等领域具有重要启示。

Abstract: We introduce Complexity as Advantage (CAA), a framework that defines the
complexity of a system relative to a family of observers. Instead of measuring
complexity as an intrinsic property, we evaluate how much predictive regret a
system induces for different observers attempting to model it. A system is
complex when it is easy for some observers and hard for others, creating an
information advantage. We show that this formulation unifies several notions of
emergent behavior, including multiscale entropy, predictive information, and
observer-dependent structure. The framework suggests that "interesting" systems
are those positioned to create differentiated regret across observers,
providing a quantitative grounding for why complexity can be functionally
valuable. We demonstrate the idea through simple dynamical models and discuss
implications for learning, evolution, and artificial agents.

</details>


### [114] [Regret Lower Bounds for Decentralized Multi-Agent Stochastic Shortest Path Problems](https://arxiv.org/abs/2511.04594)
*Utkarsh U. Chavan,Prashant Trivedi,Nandyala Hemachandra*

Main category: cs.LG

TL;DR: 本文研究了去中心化多智能体随机最短路径问题，在线性函数逼近框架下建立了首个遗憾下界Ω(√K)，揭示了去中心化控制中的学习难度。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在群体机器人和交通路由等应用中至关重要，但去中心化多智能体随机最短路径问题的学习问题尚未得到充分研究，本文旨在填补这一空白。

Method: 应用基于对称性的新论证方法，在线性函数逼近框架下识别最优策略结构，并通过构建难以学习的实例来建立遗憾下界。

Result: 建立了去中心化多智能体随机最短路径问题的首个遗憾下界Ω(√K)，其中K为学习回合数，揭示了该问题的内在学习难度。

Conclusion: 研究结果阐明了去中心化控制的学习复杂性，可为多智能体系统中高效学习算法的设计提供指导。

Abstract: Multi-agent systems (MAS) are central to applications such as swarm robotics
and traffic routing, where agents must coordinate in a decentralized manner to
achieve a common objective. Stochastic Shortest Path (SSP) problems provide a
natural framework for modeling decentralized control in such settings. While
the problem of learning in SSP has been extensively studied in single-agent
settings, the decentralized multi-agent variant remains largely unexplored. In
this work, we take a step towards addressing that gap. We study decentralized
multi-agent SSPs (Dec-MASSPs) under linear function approximation, where the
transition dynamics and costs are represented using linear models. Applying
novel symmetry-based arguments, we identify the structure of optimal policies.
Our main contribution is the first regret lower bound for this setting based on
the construction of hard-to-learn instances for any number of agents, $n$. Our
regret lower bound of $\Omega(\sqrt{K})$, over $K$ episodes, highlights the
inherent learning difficulty in Dec-MASSPs. These insights clarify the learning
complexity of decentralized control and can further guide the design of
efficient learning algorithms in multi-agent systems.

</details>


### [115] [Environment Agnostic Goal-Conditioning, A Study of Reward-Free Autonomous Learning](https://arxiv.org/abs/2511.04598)
*Hampus Åström,Elin Anna Topp,Jacek Malec*

Main category: cs.LG

TL;DR: 将常规强化学习环境转换为目标条件环境，使智能体能够自主、无需奖励地学习解决任务


<details>
  <summary>Details</summary>
Motivation: 研究如何让智能体在无需外部奖励指导的情况下自主选择目标并学习解决任务

Method: 将常规RL环境转换为目标条件环境，让智能体自主选择目标进行学习，该方法与底层离策略学习算法无关

Result: 智能体能够在与外部指导RL相当的训练时间内学习解决任务，平均目标成功率提高并稳定，但单个目标的性能存在不稳定性

Conclusion: 该方法使智能体能够被指示寻找环境中的任何观测，为特定用例前的通用智能体训练提供了可能

Abstract: In this paper we study how transforming regular reinforcement learning
environments into goal-conditioned environments can let agents learn to solve
tasks autonomously and reward-free. We show that an agent can learn to solve
tasks by selecting its own goals in an environment-agnostic way, at training
times comparable to externally guided reinforcement learning. Our method is
independent of the underlying off-policy learning algorithm. Since our method
is environment-agnostic, the agent does not value any goals higher than others,
leading to instability in performance for individual goals. However, in our
experiments, we show that the average goal success rate improves and
stabilizes. An agent trained with this method can be instructed to seek any
observations made in the environment, enabling generic training of agents prior
to specific use cases.

</details>


### [116] [Addressing divergent representations from causal interventions on neural networks](https://arxiv.org/abs/2511.04638)
*Satchel Grant,Simon Jerome Han,Alexa Tartaglini,Christopher Potts*

Main category: cs.LG

TL;DR: 论文探讨了机制可解释性中因果干预技术可能产生分布外表示的问题，分析了无害和有害两种偏离类型，并提出了改进的CL损失来减轻有害偏离。


<details>
  <summary>Details</summary>
Motivation: 研究动机是验证因果干预是否会产生分布外表示，以及这是否会影响解释方法对目标模型自然状态的忠实性。

Method: 首先通过实证研究证明常见因果干预技术确实会导致内部表示偏离自然分布，然后理论分析两类偏离，最后改进CL损失来正则化干预。

Result: 实证结果显示因果干预经常导致表示偏离自然分布；理论分析区分了无害和有害偏离；改进的CL损失能有效减少有害偏离。

Conclusion: 这些结果为开发更可靠的可解释性方法指明了方向，强调了在保持解释力的同时控制干预偏离的重要性。

Abstract: A common approach to mechanistic interpretability is to causally manipulate
model representations via targeted interventions in order to understand what
those representations encode. Here we ask whether such interventions create
out-of-distribution (divergent) representations, and whether this raises
concerns about how faithful their resulting explanations are to the target
model in its natural state. First, we demonstrate empirically that common
causal intervention techniques often do shift internal representations away
from the natural distribution of the target model. Then, we provide a
theoretical analysis of two classes of such divergences: `harmless' divergences
that occur in the null-space of the weights and from covariance within
behavioral decision boundaries, and `pernicious' divergences that activate
hidden network pathways and cause dormant behavioral changes. Finally, in an
effort to mitigate the pernicious cases, we modify the Counterfactual Latent
(CL) loss from Grant (2025) that regularizes interventions to remain closer to
the natural distributions, reducing the likelihood of harmful divergences while
preserving the interpretive power of interventions. Together, these results
highlight a path towards more reliable interpretability methods.

</details>


### [117] [Efficient probabilistic surrogate modeling techniques for partially-observed large-scale dynamical systems](https://arxiv.org/abs/2511.04641)
*Hans Harder,Abhijeet Vishwasrao,Luca Guastoni,Ricardo Vinuesa,Sebastian Peitz*

Main category: cs.LG

TL;DR: 本文研究用于偏微分方程描述的动力系统概率预测技术，比较了多种减少采样步数的流匹配扩展方法，并在挑战性系统上进行实验，包括直接预测大规模3D模拟的2D切片。


<details>
  <summary>Details</summary>
Motivation: 开发高效的概率预测方法，用于偏微分方程描述的动力系统（如Navier-Stokes方程），减少流匹配方法的采样步骤，提高预测效率。

Method: 比较多种流匹配扩展方法：直接蒸馏、渐进蒸馏、对抗扩散蒸馏、Wasserstein GANs和整流流，并在挑战性系统上进行实验验证。

Result: 评估了不同方法在减少采样步骤方面的效果，并成功实现了直接预测大规模3D模拟的2D切片，为求解器的高效流入生成铺平了道路。

Conclusion: 多种流匹配扩展方法在减少采样步骤方面具有潜力，特别是直接预测3D模拟的2D切片技术为高效流入生成提供了可行途径。

Abstract: This paper is concerned with probabilistic techniques for forecasting
dynamical systems described by partial differential equations (such as, for
example, the Navier-Stokes equations). In particular, it is investigating and
comparing various extensions to the flow matching paradigm that reduce the
number of sampling steps. In this regard, it compares direct distillation,
progressive distillation, adversarial diffusion distillation, Wasserstein GANs
and rectified flows. Moreover, experiments are conducted on a set of
challenging systems. In particular, we also address the challenge of directly
predicting 2D slices of large-scale 3D simulations, paving the way for
efficient inflow generation for solvers.

</details>


### [118] [Optimal Inference Schedules for Masked Diffusion Models](https://arxiv.org/abs/2511.04647)
*Sitan Chen,Kevin Cong,Jerry Li*

Main category: cs.LG

TL;DR: 本文研究了扩散语言模型（特别是掩码扩散模型MDM）的并行采样能力，通过函数逼近理论精确刻画了采样分布与真实分布之间的期望差异，并给出了关于总相关性和对偶总相关性的新上下界。


<details>
  <summary>Details</summary>
Motivation: 标准自回归大语言模型的推理过程本质上是顺序的，导致推理时间长且成本高。扩散语言模型（如MDM）能够并行采样，但缺乏对其并行采样能力退化的严格理解。

Method: 通过函数逼近理论建立采样分布与真实分布差异的精确表征，分析不同解掩码调度方案，并基于总相关性和对偶总相关性提出新的采样调度方案。

Result: 证明了在自然设置下，可以在O(log n)步内采样而不损失性能，但若无先验分布知识，通常无法达到最优解掩码调度。

Conclusion: 扩散语言模型在特定条件下可实现高效并行采样，但最优调度需要分布的先验知识，基于信息论性质的新调度方案在自然设置下表现良好。

Abstract: A major bottleneck of standard auto-regressive large language models is that
their inference process is inherently sequential, resulting in very long and
costly inference times. To circumvent this, practitioners proposed a class of
language models called diffusion language models, of which the masked diffusion
model (MDM) is the most successful. The MDM is able to sample tokens
out-of-order and, ostensibly, many tokens at once and in parallel. However,
there is very limited rigorous understanding of how much parallel sampling
these models can perform without noticeable degradation in their sampling
performance. Prior work of Li and Cai obtained some preliminary bounds, but
these are not tight for many natural classes of distributions. In this work, we
give a new, exact characterization of the expected divergence between the true
distribution and the sampled distribution, for any distribution and any
unmasking schedule for the sampler, showing an elegant connection to the theory
of univariate function approximation.
  By leveraging this connection, we then attain a number of novel lower and
upper bounds for this problem. While the connection to function approximation
in principle gives the optimal unmasking schedule for any distribution, we show
that it is in general impossible to compete with it without strong a priori
knowledge of the distribution, even in seemingly benign settings. However, we
also demonstrate new upper bounds and new sampling schedules in terms of
well-studied information-theoretic properties of the base distribution, namely,
its total correlation and dual total correlation, which show that in some
natural settings, one can sample in $O(log n)$ steps without any visible loss
in performance, where $n$ is the total sequence length.

</details>


### [119] [TT-Prune: Joint Model Pruning and Resource Allocation for Communication-efficient Time-triggered Federated Learning](https://arxiv.org/abs/2511.04653)
*Xinlu Zhang,Yansha Deng,Toktam Mahmoodi*

Main category: cs.LG

TL;DR: 本文提出了一种将自适应模型剪枝应用于无线时间触发联邦学习系统的方法，通过联合优化剪枝率和带宽分配来最小化训练损失并确保低延迟。


<details>
  <summary>Details</summary>
Motivation: 联邦学习面临无线带宽有限导致的通信开销大和训练延迟问题，特别是在用户设备数量不断增长的场景下。

Method: 基于模型剪枝进行收敛分析，建立联合优化问题，利用KKT条件推导剪枝率和带宽分配的闭式解。

Result: 模拟结果显示模型剪枝可减少40%的通信成本，同时保持模型性能不变。

Conclusion: 自适应模型剪枝能有效解决无线联邦学习中的通信瓶颈问题，实现高效训练。

Abstract: Federated learning (FL) offers new opportunities in machine learning,
particularly in addressing data privacy concerns. In contrast to conventional
event-based federated learning, time-triggered federated learning (TT-Fed), as
a general form of both asynchronous and synchronous FL, clusters users into
different tiers based on fixed time intervals. However, the FL network consists
of a growing number of user devices with limited wireless bandwidth,
consequently magnifying issues such as stragglers and communication overhead.
In this paper, we introduce adaptive model pruning to wireless TT-Fed systems
and study the problem of jointly optimizing the pruning ratio and bandwidth
allocation to minimize the training loss while ensuring minimal learning
latency. To answer this question, we perform convergence analysis on the
gradient l_2 norm of the TT-Fed model based on model pruning. Based on the
obtained convergence upper bound, a joint optimization problem of pruning ratio
and wireless bandwidth is formulated to minimize the model training loss under
a given delay threshold. Then, we derive closed-form solutions for wireless
bandwidth and pruning ratio using Karush-Kuhn-Tucker(KKT) conditions. The
simulation results show that model pruning could reduce the communication cost
by 40% while maintaining the model performance at the same level.

</details>


### [120] [Nowcast3D: Reliable precipitation nowcasting via gray-box learning](https://arxiv.org/abs/2511.04659)
*Huaguan Chen,Wei Han,Haofei Sun,Ning Lin,Xingtao Song,Yunfan Yang,Jie Tian,Yang Liu,Ji-Rong Wen,Xiaoye Zhang,Xueshun Shen,Hao Sun*

Main category: cs.LG

TL;DR: 提出了一个灰盒三维临近预报框架，结合物理约束神经网络和数据驱动学习，直接处理体积雷达反射率数据，实现更准确的三小时极端降水预报。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在局限：数值天气预报及其深度学习模拟速度慢且分辨率低；外推和纯数据驱动模型存在误差累积和过度平滑问题；二维雷达方法丢弃了关键的垂直信息。

Method: 开发了全三维临近预报框架，学习垂直变化的3D平流场，参数化空间变化的扩散，引入布朗运动启发的随机项表示未解析运动，使用残差分支捕捉小尺度对流启动和微物理变化。

Result: 在降水机制中实现更准确的三小时预报，在160名气象学家的盲评中57%的情况下排名第一。

Conclusion: 通过恢复具有物理一致性的完整3D动力学，为极端降水的熟练可靠临近预报提供了可扩展且稳健的途径。

Abstract: Extreme precipitation nowcasting demands high spatiotemporal fidelity and
extended lead times, yet existing approaches remain limited. Numerical Weather
Prediction (NWP) and its deep-learning emulations are too slow and coarse for
rapidly evolving convection, while extrapolation and purely data-driven models
suffer from error accumulation and excessive smoothing. Hybrid 2D radar-based
methods discard crucial vertical information, preventing accurate
reconstruction of height-dependent dynamics. We introduce a gray-box, fully
three-dimensional nowcasting framework that directly processes volumetric radar
reflectivity and couples physically constrained neural operators with
datadriven learning. The model learns vertically varying 3D advection fields
under a conservative advection operator, parameterizes spatially varying
diffusion, and introduces a Brownian-motion--inspired stochastic term to
represent unresolved motions. A residual branch captures small-scale convective
initiation and microphysical variability, while a diffusion-based stochastic
module estimates uncertainty. The framework achieves more accurate forecasts up
to three-hour lead time across precipitation regimes and ranked first in 57\%
of cases in a blind evaluation by 160 meteorologists. By restoring full 3D
dynamics with physical consistency, it offers a scalable and robust pathway for
skillful and reliable nowcasting of extreme precipitation.

</details>


### [121] [Multi-Method Analysis of Mathematics Placement Assessments: Classical, Machine Learning, and Clustering Approaches](https://arxiv.org/abs/2511.04667)
*Julian D. Allagan,Dasia A. Singleton,Shanae N. Perry,Gabrielle C. Morgan,Essence A. Morgan*

Main category: cs.LG

TL;DR: 本研究使用经典测试理论、机器学习和无监督聚类的多方法框架评估了198名学生参加的40项数学分班考试。结果显示55%的题目具有优秀区分度，30%需要替换，第6题是最佳区分项。机器学习算法达到97.5%准确率，聚类分析发现42.5%的自然能力分界点，建议替换弱区分题目、实施两阶段评估并整合随机森林预测。


<details>
  <summary>Details</summary>
Motivation: 通过多方法整合为数学分班考试提供实证基础，优化分班决策的准确性和公平性。

Method: 结合经典测试理论、机器学习（随机森林、梯度提升）和无监督聚类（K-means）的多方法框架，分析40项数学分班考试数据。

Result: 55%的题目区分度优秀，30%需要替换；第6题是最佳区分项；机器学习算法准确率达97.5%；聚类发现42.5%的自然能力分界点，与机构设定的55%阈值存在差异。

Conclusion: 多方法整合为数学分班优化提供了稳健的实证基础，建议替换弱区分题目、实施两阶段评估并整合机器学习预测，以提高分班准确性和公平性。

Abstract: This study evaluates a 40-item mathematics placement examination administered
to 198 students using a multi-method framework combining Classical Test Theory,
machine learning, and unsupervised clustering. Classical Test Theory analysis
reveals that 55\% of items achieve excellent discrimination ($D \geq 0.40$)
while 30\% demonstrate poor discrimination ($D < 0.20$) requiring replacement.
Question 6 (Graph Interpretation) emerges as the examination's most powerful
discriminator, achieving perfect discrimination ($D = 1.000$), highest ANOVA
F-statistic ($F = 4609.1$), and maximum Random Forest feature importance
(0.206), accounting for 20.6\% of predictive power. Machine learning algorithms
demonstrate exceptional performance, with Random Forest and Gradient Boosting
achieving 97.5\% and 96.0\% cross-validation accuracy. K-means clustering
identifies a natural binary competency structure with a boundary at 42.5\%,
diverging from the institutional threshold of 55\% and suggesting potential
overclassification into remedial categories. The two-cluster solution exhibits
exceptional stability (bootstrap ARI = 0.855) with perfect lower-cluster
purity. Convergent evidence across methods supports specific refinements:
replace poorly discriminating items, implement a two-stage assessment, and
integrate Random Forest predictions with transparency mechanisms. These
findings demonstrate that multi-method integration provides a robust empirical
foundation for evidence-based mathematics placement optimization.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [122] [Specification tests for regression models with measurement errors](https://arxiv.org/abs/2511.04127)
*Xiaojun Song,Jichao Yuan*

Main category: econ.EM

TL;DR: 提出了针对存在测量误差的解释变量的回归模型的新规范检验方法，基于去卷积残差标记经验过程构建ICM型检验统计量，并首次在测量误差规范检验中引入乘数bootstrap方法。


<details>
  <summary>Details</summary>
Motivation: 在回归模型中，解释变量的测量误差会影响模型规范检验的有效性，需要开发能够处理测量误差的检验方法。

Method: 使用去卷积核估计器构建残差，基于去卷积残差标记经验过程构造ICM型检验统计量，采用正交投影消除参数估计效应，并通过乘数bootstrap模拟临界值。

Result: 蒙特卡洛模拟表明，无论测量误差分布已知还是未知，所提出的检验方法都具有良好的有限样本性能。

Conclusion: 该方法首次在测量误差规范检验中引入乘数bootstrap，为存在测量误差的回归模型提供了有效的规范检验工具。

Abstract: In this paper, we propose new specification tests for regression models with
measurement errors in the explanatory variables. Inspired by the integrated
conditional moment (ICM) approach, we use a deconvoluted residual-marked
empirical process and construct ICM-type test statistics based on it. The issue
of measurement errors is addressed by applying a deconvolution kernel estimator
in constructing the residuals. We demonstrate that employing an orthogonal
projection onto the tangent space of nuisance parameters not only eliminates
the parameter estimation effect but also facilitates the simulation of critical
values via a computationally simple multiplier bootstrap procedure. It is the
first time a multiplier bootstrap has been proposed in the literature of
specification testing with measurement errors. We also develop specification
tests and the multiplier bootstrap procedure when the measurement error
distribution is unknown. The finite-sample performance of the proposed tests
for both known and unknown measurement error distributions is evaluated through
Monte Carlo simulations, which demonstrate their efficacy.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [123] [Pediatric Appendicitis Detection from Ultrasound Images](https://arxiv.org/abs/2511.04069)
*Fatemeh Hosseinabadi,Seyedhassan Sharifi*

Main category: eess.IV

TL;DR: 开发基于预训练ResNet架构的深度学习模型，用于从超声图像中自动检测小儿阑尾炎，在Regensburg小儿阑尾炎数据集上达到93.44%的准确率。


<details>
  <summary>Details</summary>
Motivation: 小儿阑尾炎是儿童急性腹痛的常见原因，但由于症状重叠和影像质量不一，诊断仍然具有挑战性。

Method: 使用预训练的ResNet架构进行微调，通过图像归一化、调整大小和数据增强等预处理步骤，区分阑尾炎与非阑尾炎病例。

Result: 模型整体准确率为93.44%，精确率为91.53%，召回率为89.8%，在异质性超声图像中表现出强大的阑尾炎识别能力。

Conclusion: 该模型能有效学习区分性空间特征，克服了小儿影像中低对比度、斑点噪声和解剖变异带来的挑战。

Abstract: Pediatric appendicitis remains one of the most common causes of acute
abdominal pain in children, and its diagnosis continues to challenge clinicians
due to overlapping symptoms and variable imaging quality. This study aims to
develop and evaluate a deep learning model based on a pretrained ResNet
architecture for automated detection of appendicitis from ultrasound images. We
used the Regensburg Pediatric Appendicitis Dataset, which includes ultrasound
scans, laboratory data, and clinical scores from pediatric patients admitted
with abdominal pain to Children Hospital. Hedwig in Regensburg, Germany. Each
subject had 1 to 15 ultrasound views covering the right lower quadrant,
appendix, lymph nodes, and related structures. For the image based
classification task, ResNet was fine tuned to distinguish appendicitis from
non-appendicitis cases. Images were preprocessed by normalization, resizing,
and augmentation to enhance generalization. The proposed ResNet model achieved
an overall accuracy of 93.44, precision of 91.53, and recall of 89.8,
demonstrating strong performance in identifying appendicitis across
heterogeneous ultrasound views. The model effectively learned discriminative
spatial features, overcoming challenges posed by low contrast, speckle noise,
and anatomical variability in pediatric imaging.

</details>


### [124] [Left Atrial Segmentation with nnU-Net Using MRI](https://arxiv.org/abs/2511.04071)
*Fatemeh Hosseinabadi,Seyedhassan Sharifi*

Main category: eess.IV

TL;DR: 使用nnU-Net框架对心脏MRI左心房进行自动分割，在2013左心房分割挑战数据集上达到93.5的平均Dice分数，优于传统分割方法。


<details>
  <summary>Details</summary>
Motivation: 手动分割左心房耗时、主观性强，不适合大规模或时间敏感的临床工作流程，需要自动化的深度学习解决方案。

Method: 应用nnU-Net框架，这是一个自动配置的深度学习分割架构，能够根据MRI数据特征自动调整预处理、网络配置和训练流程。

Result: 在30个MRI扫描数据集上，模型与专家标注高度重叠，平均Dice分数达到93.5，在左心房形状、对比度和图像质量变化中表现稳健。

Conclusion: nnU-Net在左心房分割任务中表现出色，能够准确描绘心房体和近端肺静脉，为房颤消融和心脏生物物理模型构建提供了可靠的自动化工具。

Abstract: Accurate segmentation of the left atrium (LA) from cardiac MRI is critical
for guiding atrial fibrillation (AF) ablation and constructing biophysical
cardiac models. Manual delineation is time-consuming, observer-dependent, and
impractical for large-scale or time-sensitive clinical workflows. Deep learning
methods, particularly convolutional architectures, have recently demonstrated
superior performance in medical image segmentation tasks. In this study, we
applied the nnU-Net framework, an automated, self-configuring deep learning
segmentation architecture, to the Left Atrial Segmentation Challenge 2013
dataset. The dataset consists of thirty MRI scans with corresponding
expert-annotated masks. The nnU-Net model automatically adapted its
preprocessing, network configuration, and training pipeline to the
characteristics of the MRI data. Model performance was quantitatively evaluated
using the Dice similarity coefficient (DSC), and qualitative results were
compared against expert segmentations. The proposed nnUNet model achieved a
mean Dice score of 93.5, demonstrating high overlap with expert annotations and
outperforming several traditional segmentation approaches reported in previous
studies. The network exhibited robust generalization across variations in left
atrial shape, contrast, and image quality, accurately delineating both the
atrial body and proximal pulmonary veins.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [125] [Mean-field approximations in insurance](https://arxiv.org/abs/2511.04198)
*Philipp C. Hornung*

Main category: q-fin.RM

TL;DR: 使用平均场近似将高维线性前向积分-微分方程组简化为低维非线性方程组，以计算依赖个体群体的保险负债，并证明在大群体规模下收敛到平均场近似。


<details>
  <summary>Details</summary>
Motivation: 计算依赖个体群体的保险负债需要解决高维耦合线性前向积分-微分方程组，这在较大群体中不可行，因此需要简化方法。

Method: 采用平均场近似方法，将高维线性方程组替换为低维非线性前向积分-微分方程组。

Result: 在满足特定正则性条件下，保险负债作为基础跳跃过程泛函的条件期望会收敛到其平均场近似，当群体个体数趋于无穷时。

Conclusion: 平均场近似在寿险和非寿险领域具有重要实际应用价值，能够有效解决大规模群体保险负债计算问题。

Abstract: The calculation of the insurance liabilities of a cohort of dependent
individuals in general requires the solution of a high-dimensional system of
coupled linear forward integro-differential equations, which is infeasible for
a larger cohort. However, by using a mean-field approximation, the high
dimensional system of linear forward equations can be replaced by a
low-dimensional system of non-linear forward integro-differential equations. We
show that, subject to certain regularity conditions, the insurance liability
viewed as a (conditional) expectation of a functional of an underlying jump
process converges to its mean-field approximation, as the number of individuals
in the cohort goes to infinity. Examples from both life- and non-life insurance
illuminate the practical importance of mean-field approximations.

</details>


### [126] [On the Estimation of Own Funds for Life Insurers: A Study of Direct, Indirect, and Control Variate Methods in a Risk-Neutral Pricing Framework](https://arxiv.org/abs/2511.04412)
*Mark-Oliver Wolf*

Main category: q-fin.RM

TL;DR: 本文研究了Solvency II下直接和间接估计自有资金的方法，证明了两者的收敛性，提出了混合估计器家族，并开发了有效的方差缩减技术。


<details>
  <summary>Details</summary>
Motivation: Solvency II下的偿付能力资本要求计算计算量大，法规要求使用直接估计方法，但在特定假设下间接方法也能得到相同估计。研究比较这两种方法的特性并给出新见解。

Method: 1) 提供直接和间接估计器收敛性的直接证明；2) 引入包含直接和间接方法作为边缘情况的新混合估计器家族；3) 利用这些估计器开发方差缩减技术，包括单一控制变量和多控制变量框架。

Result: 在三个德国寿险公司的简化资产负债管理模型上评估，发现间接方法在更现实设置中始终优于直接方法。提出的控制变量技术在某些情况下能将方差减少到标准直接估计器的十分之一。

Conclusion: 直接和间接估计器都不是普遍最优的，间接方法在更现实场景中表现更好。提出的方差缩减技术潜力显著，但其效果具有模型依赖性。

Abstract: The Solvency Capital Requirement (SCR) calculation under Solvency II is
computationally intensive, relying on the estimation of own funds. Regulation
mandates the direct estimation method. It has been proven that under specific
assumptions, the indirect method results in the same estimate. We study their
comparative properties and give novel insights.
  First, we provide a straightforward proof that the direct and indirect
estimators for own funds converge to the same value. Second, we introduce a
novel family of mixed estimators that encompasses the direct and indirect
methods as its edge cases. Third, we leverage these estimators to develop
powerful variance reduction techniques, constructing a single control variate
from the direct and indirect estimators and a multi-control variate framework
using subsets of the mixed family. These techniques can be combined with
existing methods like Least-Squares Monte Carlo.
  We evaluate the estimators on three simplified asset-liability management
models of a German life insurer, Bauer's model MUST and IS case from Bauer et
al. (2006), and openIRM by Wolf et al. (2025). Our analysis confirms that
neither the direct nor indirect estimator is universally superior, though the
indirect method consistently outperforms the direct one in more realistic
settings. The proposed control variate techniques show significant potential,
in some cases reducing variance to one-tenth of that from the standard direct
estimator. However, we also identify scenarios where improvements are marginal,
highlighting the model-dependent nature of their efficacy.
  The source code is publicly available at
https://gitlab.cc-asp.fraunhofer.de/itwm-fm-lv-public/wolf-estimation-of-own-funds.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [127] [Causal Regime Detection in Energy Markets With Augmented Time Series Structural Causal Models](https://arxiv.org/abs/2511.04361)
*Dennis Thumm*

Main category: q-fin.CP

TL;DR: 提出了增强时间序列因果模型（ATSCM），用于能源市场的因果建模和反事实推理，能够学习时变因果图并回答反事实问题。


<details>
  <summary>Details</summary>
Motivation: 当前能源市场模型缺乏明确的因果解释和反事实推理能力，无法处理复杂的天气-发电-价格因果关系和连续的制度变化。

Method: 扩展反事实推理框架到多元时间序列数据，通过可解释因素（天气、发电组合、需求模式）建模能源系统，集成神经因果发现学习时变因果图。

Result: 应用于真实电价数据，ATSCM能够回答新颖的反事实查询，如在不同可再生能源发电情景下的价格变化。

Conclusion: ATSCM为能源市场提供了具有因果解释和反事实推理能力的建模框架，能够处理复杂的动态因果关系。

Abstract: Energy markets exhibit complex causal relationships between weather patterns,
generation technologies, and price formation, with regime changes occurring
continuously rather than at discrete break points. Current approaches model
electricity prices without explicit causal interpretation or counterfactual
reasoning capabilities. We introduce Augmented Time Series Causal Models
(ATSCM) for energy markets, extending counterfactual reasoning frameworks to
multivariate temporal data with learned causal structure. Our approach models
energy systems through interpretable factors (weather, generation mix, demand
patterns), rich grid dynamics, and observable market variables. We integrate
neural causal discovery to learn time-varying causal graphs without requiring
ground truth DAGs. Applied to real-world electricity price data, ATSCM enables
novel counterfactual queries such as "What would prices be under different
renewable generation scenarios?".

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [128] [Friction on Demand: A Generative Framework for the Inverse Design of Metainterfaces](https://arxiv.org/abs/2511.03735)
*Valentin Mouton,Adrien Mélot*

Main category: stat.ML

TL;DR: 提出使用变分自编码器(VAE)的生成建模框架，从目标摩擦定律推断表面形貌，解决摩擦界面设计的逆问题。


<details>
  <summary>Details</summary>
Motivation: 设计具有预定宏观行为的摩擦界面是一个具有挑战性的逆问题，传统方法依赖低维参数化的启发式搜索，限制了其在复杂或非线性摩擦定律中的适用性。

Method: 使用变分自编码器(VAE)进行生成建模，基于参数化接触力学模型构建的2亿个合成样本数据集进行训练，实现无仿真的候选形貌生成。

Result: 该方法能够高效、无仿真地生成候选表面形貌，在准确性、吞吐量和生成解多样性之间取得平衡。

Conclusion: 该方法为实现通过定制表面形貌进行摩擦行为的近实时控制铺平了道路，并突出了在平衡这些目标时的权衡和实际考虑。

Abstract: Designing frictional interfaces to exhibit prescribed macroscopic behavior is
a challenging inverse problem, made difficult by the non-uniqueness of
solutions and the computational cost of contact simulations. Traditional
approaches rely on heuristic search over low-dimensional parameterizations,
which limits their applicability to more complex or nonlinear friction laws. We
introduce a generative modeling framework using Variational Autoencoders (VAEs)
to infer surface topographies from target friction laws. Trained on a synthetic
dataset composed of 200 million samples constructed from a parameterized
contact mechanics model, the proposed method enables efficient, simulation-free
generation of candidate topographies. We examine the potential and limitations
of generative modeling for this inverse design task, focusing on balancing
accuracy, throughput, and diversity in the generated solutions. Our results
highlight trade-offs and outline practical considerations when balancing these
objectives. This approach paves the way for near-real-time control of
frictional behavior through tailored surface topographies.

</details>


### [129] [Riesz Regression As Direct Density Ratio Estimation](https://arxiv.org/abs/2511.04568)
*Masahiro Kato*

Main category: stat.ML

TL;DR: Riesz回归与直接密度比估计（DRE）在重要情况下密切相关，特别是平均处理效应（ATE）估计。Riesz回归的思想和目标与最小二乘重要性拟合（LSIF）在直接密度比估计中一致。这种等价性使得可以在特定情况下直接应用现有结果，反之亦然。


<details>
  <summary>Details</summary>
Motivation: 研究Riesz回归与直接密度比估计之间的关系，以促进两个领域之间的知识转移和方法应用扩展。

Method: 通过理论分析建立Riesz回归与直接密度比估计（特别是LSIF方法）之间的等价关系，并探讨这种关系带来的方法学启示。

Result: 证明了Riesz回归在重要情况下（包括ATE估计）与直接密度比估计等价，特别是在思想、目标和具体方法上与LSIF一致。

Conclusion: Riesz回归与直接密度比估计的等价关系为两个领域提供了相互借鉴的机会，扩展了各自方法的应用范围，并促进了相关理论结果的共享。

Abstract: Riesz regression has garnered attention as a tool in debiased machine
learning for causal and structural parameter estimation (Chernozhukov et al.,
2021). This study shows that Riesz regression is closely related to direct
density-ratio estimation (DRE) in important cases, including average treat-
ment effect (ATE) estimation. Specifically, the idea and objective in Riesz
regression coincide with the one in least-squares importance fitting (LSIF,
Kanamori et al., 2009) in direct density-ratio estimation. While Riesz
regression is general in the sense that it can be applied to Riesz representer
estimation in a wide class of problems, the equivalence with DRE allows us to
directly import exist- ing results in specific cases, including
convergence-rate analyses, the selection of loss functions via
Bregman-divergence minimization, and regularization techniques for flexible
models, such as neural networks. Conversely, insights about the Riesz
representer in debiased machine learning broaden the applications of direct
density-ratio estimation methods. This paper consolidates our prior results in
Kato (2025a) and Kato (2025b).

</details>


### [130] [Bifidelity Karhunen-Loève Expansion Surrogate with Active Learning for Random Fields](https://arxiv.org/abs/2511.03756)
*Aniket Jivani,Cosmin Safta,Beckett Y. Zhou,Xun Huan*

Main category: stat.ML

TL;DR: 提出了一种双保真度Karhunen-Loève展开（KLE）代理模型，用于处理不确定输入下的场值感兴趣量（QoIs）。该方法结合KLE的光谱效率和多项式混沌展开（PCEs），通过耦合低成本低保真度模拟和有限数量高保真度模拟，构建准确且计算负担小的代理模型，并采用主动学习策略进一步提高精度。


<details>
  <summary>Details</summary>
Motivation: 为场值QoIs构建计算高效的代理模型，在保持输入不确定性与输出场之间显式映射的同时，通过双保真度方法平衡计算成本与精度需求。

Method: 结合KLE的光谱效率和PCEs，耦合低保真度模拟（捕捉主要响应趋势）与有限高保真度模拟（校正系统偏差），并采用基于交叉验证和Gaussian过程回归的主动学习策略，自适应选择新的高保真度评估点。

Result: 在三个复杂度递增的案例中（一维分析基准、二维对流扩散系统、三维湍流射流模拟），该方法相比单保真度和随机采样方法，在预测精度和样本效率方面均取得一致改进。

Conclusion: BF-KLE-AL框架能够有效构建场值QoIs的准确代理模型，通过双保真度方法和主动学习策略显著提升预测性能，适用于复杂工程系统的不确定性量化。

Abstract: We present a bifidelity Karhunen-Lo\`eve expansion (KLE) surrogate model for
field-valued quantities of interest (QoIs) under uncertain inputs. The approach
combines the spectral efficiency of the KLE with polynomial chaos expansions
(PCEs) to preserve an explicit mapping between input uncertainties and output
fields. By coupling inexpensive low-fidelity (LF) simulations that capture
dominant response trends with a limited number of high-fidelity (HF)
simulations that correct for systematic bias, the proposed method enables
accurate and computationally affordable surrogate construction. To further
improve surrogate accuracy, we form an active learning strategy that adaptively
selects new HF evaluations based on the surrogate's generalization error,
estimated via cross-validation and modeled using Gaussian process regression.
New HF samples are then acquired by maximizing an expected improvement
criterion, targeting regions of high surrogate error. The resulting BF-KLE-AL
framework is demonstrated on three examples of increasing complexity: a
one-dimensional analytical benchmark, a two-dimensional convection-diffusion
system, and a three-dimensional turbulent round jet simulation based on
Reynolds-averaged Navier--Stokes (RANS) and enhanced delayed detached-eddy
simulations (EDDES). Across these cases, the method achieves consistent
improvements in predictive accuracy and sample efficiency relative to
single-fidelity and random-sampling approaches.

</details>


### [131] [Learning Paths for Dynamic Measure Transport: A Control Perspective](https://arxiv.org/abs/2511.03797)
*Aimee Maurais,Bamdad Hosseini,Youssef Marzouk*

Main category: stat.ML

TL;DR: 本文从控制论视角研究动态测度传输中的路径识别问题，提出基于均值场博弈的优化方法，通过高斯过程求解偏微分方程来获得更平滑高效的传输路径。


<details>
  <summary>Details</summary>
Motivation: 现有动态测度传输方法中常用的路径选择可能效果不佳，需要寻找更优的路径来提高采样效率。

Method: 提出基于均值场博弈的优化问题族，使用鼓励速度平滑性的目标项，采用高斯过程方法求解偏微分方程。

Result: 相比未倾斜参考路径，新方法能够恢复更高效和更平滑的传输模型。

Conclusion: 从控制论视角出发的路径优化方法能够显著改善动态测度传输的性能，平滑的传输路径对采样效率有重要影响。

Abstract: We bring a control perspective to the problem of identifying paths of
measures for sampling via dynamic measure transport (DMT). We highlight the
fact that commonly used paths may be poor choices for DMT and connect existing
methods for learning alternate paths to mean-field games. Based on these
connections we pose a flexible family of optimization problems for identifying
tilted paths of measures for DMT and advocate for the use of objective terms
which encourage smoothness of the corresponding velocities. We present a
numerical algorithm for solving these problems based on recent Gaussian process
methods for solution of partial differential equations and demonstrate the
ability of our method to recover more efficient and smooth transport models
compared to those which use an untilted reference path.

</details>


### [132] [A general technique for approximating high-dimensional empirical kernel matrices](https://arxiv.org/abs/2511.03892)
*Chiraag Kaushik,Justin Romberg,Vidya Muthukumar*

Main category: stat.ML

TL;DR: 本文提出了随机核矩阵期望算子范数的简单用户友好边界，使用U-统计量的解耦结果和非交换Khinchine不等式，得到仅依赖于核函数标量统计量和相关核矩阵的上下界。


<details>
  <summary>Details</summary>
Motivation: 为随机核矩阵的算子范数提供更简单、更通用的边界，避免传统方法中复杂的组合论证和矩方法。

Method: 使用U-统计量的解耦结果和非交换Khinchine不等式，构建仅依赖于核函数标量统计量和相关核矩阵的边界。

Result: 获得了更紧的边界，简化了现有结果的证明，并为各向异性高斯数据提供了新的近似结果。

Conclusion: 该方法为高维数据中的内积核矩阵提供了更紧的近似，并在各向异性高斯数据的核回归中展示了更紧的偏差下界。

Abstract: We present simple, user-friendly bounds for the expected operator norm of a
random kernel matrix under general conditions on the kernel function
$k(\cdot,\cdot)$. Our approach uses decoupling results for U-statistics and the
non-commutative Khintchine inequality to obtain upper and lower bounds
depending only on scalar statistics of the kernel function and a ``correlation
kernel'' matrix corresponding to $k(\cdot,\cdot)$. We then apply our method to
provide new, tighter approximations for inner-product kernel matrices on
general high-dimensional data, where the sample size and data dimension are
polynomially related. Our method obtains simplified proofs of existing results
that rely on the moment method and combinatorial arguments while also providing
novel approximation results for the case of anisotropic Gaussian data. Finally,
using similar techniques to our approximation result, we show a tighter lower
bound on the bias of kernel regression with anisotropic Gaussian data.

</details>


### [133] [High-dimensional limit theorems for SGD: Momentum and Adaptive Step-sizes](https://arxiv.org/abs/2511.03952)
*Aukosh Jagannath,Taj Jones-McCormick,Varnan Sarangian*

Main category: stat.ML

TL;DR: 本文开发了带Polyak动量的随机梯度下降(SGD-M)的高维缩放极限，并与在线SGD进行严格比较。研究发现，在适当时间重缩放和特定步长选择下，SGD-M与在线SGD的缩放极限一致，但若步长相同，SGD-M会放大高维效应。在尖峰张量PCA和单指标模型上的应用表明，基于归一化梯度的自适应步长算法能带来多个好处。


<details>
  <summary>Details</summary>
Motivation: 为严格比较在线SGD及其流行变体（特别是带Polyak动量的SGD）在高维环境下的性能差异，提供理论分析框架。

Method: 开发SGD-M的高维缩放极限理论框架，通过时间重缩放和步长调整进行比较分析，并在尖峰张量PCA和单指标模型上进行验证，同时考察基于归一化梯度的自适应步长算法。

Result: 在适当时间重缩放和特定步长下，SGD-M与在线SGD缩放极限一致；但若步长相同，SGD-M会放大高维效应。自适应步长算法能产生更接近总体最小值的固定点，并扩大收敛步长范围。

Conclusion: 早期预处理器可以在在线SGD失败的场景中稳定和改进动态性能，这为经验观察提供了严格的理论解释。

Abstract: We develop a high-dimensional scaling limit for Stochastic Gradient Descent
with Polyak Momentum (SGD-M) and adaptive step-sizes. This provides a framework
to rigourously compare online SGD with some of its popular variants. We show
that the scaling limits of SGD-M coincide with those of online SGD after an
appropriate time rescaling and a specific choice of step-size. However, if the
step-size is kept the same between the two algorithms, SGD-M will amplify
high-dimensional effects, potentially degrading performance relative to online
SGD. We demonstrate our framework on two popular learning problems: Spiked
Tensor PCA and Single Index Models. In both cases, we also examine online SGD
with an adaptive step-size based on normalized gradients. In the
high-dimensional regime, this algorithm yields multiple benefits: its dynamics
admit fixed points closer to the population minimum and widens the range of
admissible step-sizes for which the iterates converge to such solutions. These
examples provide a rigorous account, aligning with empirical motivation, of how
early preconditioners can stabilize and improve dynamics in settings where
online SGD fails.

</details>


### [134] [Robust inference using density-powered Stein operators](https://arxiv.org/abs/2511.03963)
*Shinto Eguchi*

Main category: stat.ML

TL;DR: 提出基于γ-散度的γ-Stein算子，构建针对非归一化概率模型的鲁棒推断方法，并应用于鲁棒拟合优度检验和贝叶斯后验近似。


<details>
  <summary>Details</summary>
Motivation: 现有Stein算子方法对异常值敏感，需要开发能够自动降低异常值影响的鲁棒推断方法。

Method: 通过模型密度的γ次方加权构建γ-Stein算子，推导出鲁棒的分数匹配方法，并扩展至核化Stein差异和Stein变分梯度下降。

Result: 在污染高斯和四次势能模型上的实验表明，该方法在鲁棒性和统计效率上显著优于基准方法。

Conclusion: γ-Stein算子提供了一种原理性的鲁棒推断框架，适用于非归一化模型且保持计算便利性。

Abstract: We introduce a density-power weighted variant for the Stein operator, called
the $\gamma$-Stein operator. This is a novel class of operators derived from
the $\gamma$-divergence, designed to build robust inference methods for
unnormalized probability models. The operator's construction (weighting by the
model density raised to a positive power $\gamma$ inherently down-weights the
influence of outliers, providing a principled mechanism for robustness.
Applying this operator yields a robust generalization of score matching that
retains the crucial property of being independent of the model's normalizing
constant. We extend this framework to develop two key applications: the
$\gamma$-kernelized Stein discrepancy for robust goodness-of-fit testing, and
$\gamma$-Stein variational gradient descent for robust Bayesian posterior
approximation. Empirical results on contaminated Gaussian and quartic potential
models show our methods significantly outperform standard baselines in both
robustness and statistical efficiency.

</details>


### [135] [Online Conformal Inference with Retrospective Adjustment for Faster Adaptation to Distribution Shift](https://arxiv.org/abs/2511.04275)
*Jungbin Jun,Ilsang Ohn*

Main category: stat.ML

TL;DR: 提出了一种具有回顾性调整的新型在线共形推理方法，通过回归方法和留一法更新公式来调整过去预测，以更快适应分布变化。


<details>
  <summary>Details</summary>
Motivation: 传统共形预测在在线环境中假设数据可交换性，但实际数据分布会随时间变化。现有方法仅向前更新预测，适应分布变化较慢。

Method: 使用回归方法和高效的留一法更新公式，在新数据到达时回顾性调整过去预测，使所有预测与最新数据分布对齐。

Result: 在合成和真实数据集上的广泛数值研究表明，该方法比现有在线共形预测方法实现了更快的覆盖率重新校准和更高的统计效率。

Conclusion: 所提出的回顾性调整方法能够更快速地适应分布变化，提高在线共形预测的性能。

Abstract: Conformal prediction has emerged as a powerful framework for constructing
distribution-free prediction sets with guaranteed coverage assuming only the
exchangeability assumption. However, this assumption is often violated in
online environments where data distributions evolve over time. Several recent
approaches have been proposed to address this limitation, but, typically, they
slowly adapt to distribution shifts because they update predictions only in a
forward manner, that is, they generate a prediction for a newly observed data
point while previously computed predictions are not updated. In this paper, we
propose a novel online conformal inference method with retrospective
adjustment, which is designed to achieve faster adaptation to distributional
shifts. Our method leverages regression approaches with efficient leave-one-out
update formulas to retroactively adjust past predictions when new data arrive,
thereby aligning the entire set of predictions with the most recent data
distribution. Through extensive numerical studies performed on both synthetic
and real-world data sets, we show that the proposed approach achieves faster
coverage recalibration and improved statistical efficiency compared to existing
online conformal prediction methods.

</details>


### [136] [Robustness of Minimum-Volume Nonnegative Matrix Factorization under an Expanded Sufficiently Scattered Condition](https://arxiv.org/abs/2511.04291)
*Giovanni Barbarino,Nicolas Gillis,Subhayan Saha*

Main category: stat.ML

TL;DR: 本文证明了最小体积非负矩阵分解在噪声条件下能够识别真实因子，前提是数据点满足扩展充分散射条件。


<details>
  <summary>Details</summary>
Motivation: 最小体积NMF已在多个领域成功应用，但其对噪声的鲁棒性一直是一个未解决的问题。

Method: 使用最小体积非负矩阵分解方法，在扩展充分散射条件下进行分析。

Result: 证明了在噪声存在的情况下，最小体积NMF能够识别出真实的基础因子。

Conclusion: 最小体积NMF在满足扩展充分散射条件时对噪声具有鲁棒性，能够准确恢复真实因子。

Abstract: Minimum-volume nonnegative matrix factorization (min-vol NMF) has been used
successfully in many applications, such as hyperspectral imaging, chemical
kinetics, spectroscopy, topic modeling, and audio source separation. However,
its robustness to noise has been a long-standing open problem. In this paper,
we prove that min-vol NMF identifies the groundtruth factors in the presence of
noise under a condition referred to as the expanded sufficiently scattered
condition which requires the data points to be sufficiently well scattered in
the latent simplex generated by the basis vectors.

</details>


### [137] [Simultaneous Optimization of Geodesics and Fréchet Means](https://arxiv.org/abs/2511.04301)
*Frederik Möbius Rygaard,Søren Hauberg,Steen Markvorsen*

Main category: stat.ML

TL;DR: 提出GEORCE-FM算法，在局部坐标系中同时计算Fréchet均值和黎曼距离，比现有方法更快，并扩展到Finsler流形和自适应扩展以处理大数据集。


<details>
  <summary>Details</summary>
Motivation: Fréchet均值是几何统计中的核心概念，但现有方法需要在每次迭代中求解嵌入式优化问题，计算效率低。

Method: 开发GEORCE-FM算法，在局部坐标系中同时优化Fréchet均值和距离计算，并扩展到Finsler流形和自适应扩展。

Result: 理论证明算法具有全局收敛性和局部二次收敛性，自适应扩展在期望上收敛到Fréchet均值。实证显示在准确性和运行时间上优于基线方法。

Conclusion: GEORCE-FM为计算Fréchet均值提供了更高效的方法，适用于黎曼和Finsler流形，并能扩展到大规模数据集。

Abstract: A central part of geometric statistics is to compute the Fr\'echet mean. This
is a well-known intrinsic mean on a Riemannian manifold that minimizes the sum
of squared Riemannian distances from the mean point to all other data points.
The Fr\'echet mean is simple to define and generalizes the Euclidean mean, but
for most manifolds even minimizing the Riemannian distance involves solving an
optimization problem. Therefore, numerical computations of the Fr\'echet mean
require solving an embedded optimization problem in each iteration. We
introduce the GEORCE-FM algorithm to simultaneously compute the Fr\'echet mean
and Riemannian distances in each iteration in a local chart, making it faster
than previous methods. We extend the algorithm to Finsler manifolds and
introduce an adaptive extension such that GEORCE-FM scales to a large number of
data points. Theoretically, we show that GEORCE-FM has global convergence and
local quadratic convergence and prove that the adaptive extension converges in
expectation to the Fr\'echet mean. We further empirically demonstrate that
GEORCE-FM outperforms existing baseline methods to estimate the Fr\'echet mean
in terms of both accuracy and runtime.

</details>


### [138] [Online Bayesian Experimental Design for Partially Observed Dynamical Systems](https://arxiv.org/abs/2511.04403)
*Sara Pérez-Vieites,Sahel Iqbal,Simo Särkkä,Dominik Baumann*

Main category: stat.ML

TL;DR: 本文提出了一个用于非线性状态空间模型中贝叶斯实验设计的新框架，通过推导EIG及其梯度的新估计器，结合嵌套粒子滤波器，解决了部分可观测动态系统中的在线优化问题。


<details>
  <summary>Details</summary>
Motivation: 现有的贝叶斯实验设计方法无法应用于具有部分可观测性的动态系统，这类系统只能获得噪声和不完整的观测数据，且似然函数难以处理，需要在线算法来更新后验分布并顺序选择设计。

Method: 推导了期望信息增益及其梯度的新估计器，显式地边缘化潜在状态，使用嵌套粒子滤波器进行高效的在线推断，具有收敛保证。

Result: 在现实模型（如SIR模型和移动源定位任务）上的应用表明，该框架成功处理了部分可观测性和在线计算问题。

Conclusion: 提出的框架为非线性状态空间模型中的贝叶斯实验设计提供了可扩展的随机优化方法，特别适用于部分可观测动态系统。

Abstract: Bayesian experimental design (BED) provides a principled framework for
optimizing data collection, but existing approaches do not apply to crucial
real-world settings such as dynamical systems with partial observability, where
only noisy and incomplete observations are available. These systems are
naturally modeled as state-space models (SSMs), where latent states mediate the
link between parameters and data, making the likelihood -- and thus
information-theoretic objectives like the expected information gain (EIG) --
intractable. In addition, the dynamical nature of the system requires online
algorithms that update posterior distributions and select designs sequentially
in a computationally efficient manner. We address these challenges by deriving
new estimators of the EIG and its gradient that explicitly marginalize latent
states, enabling scalable stochastic optimization in nonlinear SSMs. Our
approach leverages nested particle filters (NPFs) for efficient online
inference with convergence guarantees. Applications to realistic models, such
as the susceptible-infected-recovered (SIR) and a moving source location task,
show that our framework successfully handles both partial observability and
online computation.

</details>


### [139] [Physics-Informed Neural Networks and Neural Operators for Parametric PDEs: A Human-AI Collaborative Analysis](https://arxiv.org/abs/2511.04576)
*Zhuo Zhang,Xiong Xiong,Sen Zhang,Yuan Zhao,Xi Yang*

Main category: stat.ML

TL;DR: 该论文系统分析了参数化偏微分方程求解的两种机器学习范式：物理信息神经网络(PINNs)和神经算子，比较了它们在流体力学、固体力学等领域的性能，展示了神经算子相比传统求解器可获得10^3到10^5倍的计算加速。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法需要为每个参数重新求解PDE，参数空间探索成本过高。机器学习方法特别是PINNs和神经算子通过学**解算子实现了跨参数空间的泛化能力。

Method: 分析两种主要范式：(1) PINNs：将物理定律作为软约束嵌入，擅长稀疏数据的逆问题；(2) 神经算子：学**无限维函数空间之间的映射，实现前所未有的泛化能力。

Result: 在流体动力学、固体力学、热传导和电磁学等领域的比较显示，神经算子在多查询场景下相比传统求解器可获得10^3到10^5倍的计算加速，同时保持相当的精度。

Conclusion: 建立了通过算子学**理解参数化PDE求解器的统一框架，为这一快速发展的领域提供了全面的、可增量更新的资源，并指出了高维参数、复杂几何和分布外泛化等关键开放挑战。

Abstract: PDEs arise ubiquitously in science and engineering, where solutions depend on
parameters (physical properties, boundary conditions, geometry). Traditional
numerical methods require re-solving the PDE for each parameter, making
parameter space exploration prohibitively expensive. Recent machine learning
advances, particularly physics-informed neural networks (PINNs) and neural
operators, have revolutionized parametric PDE solving by learning solution
operators that generalize across parameter spaces. We critically analyze two
main paradigms: (1) PINNs, which embed physical laws as soft constraints and
excel at inverse problems with sparse data, and (2) neural operators (e.g.,
DeepONet, Fourier Neural Operator), which learn mappings between
infinite-dimensional function spaces and achieve unprecedented generalization.
Through comparisons across fluid dynamics, solid mechanics, heat transfer, and
electromagnetics, we show neural operators can achieve computational speedups
of $10^3$ to $10^5$ times faster than traditional solvers for multi-query
scenarios, while maintaining comparable accuracy. We provide practical guidance
for method selection, discuss theoretical foundations (universal approximation,
convergence), and identify critical open challenges: high-dimensional
parameters, complex geometries, and out-of-distribution generalization. This
work establishes a unified framework for understanding parametric PDE solvers
via operator learning, offering a comprehensive, incrementally updated resource
for this rapidly evolving field

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [140] [Robust mean-field control under common noise uncertainty](https://arxiv.org/abs/2511.04515)
*Mathieu Laurière,Ariel Neufeld,Kyunghyun Park*

Main category: math.OC

TL;DR: 该论文提出了一个离散时间鲁棒均值场控制框架，处理具有共同噪声不确定性的问题。通过将问题转化为概率测度空间上的鲁棒马尔可夫决策问题，证明了最优开环控制的存在性，并通过数值实验验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决在共同噪声不确定性下的多智能体系统控制问题。当大量合作智能体受到共同随机扰动影响，且该扰动的真实分布未知时，需要设计鲁棒的控制策略来最大化最坏情况下的期望回报。

Method: 方法包括：1）将鲁棒均值场控制问题建模为概率测度空间上的提升鲁棒马尔可夫决策问题；2）建立动态规划原理和Bellman-Isaac不动点定理；3）证明最优开环控制的存在性；4）通过传播混沌理论连接N-智能体鲁棒优化问题。

Result: 理论结果表明：1）鲁棒均值场控制问题是N-智能体鲁棒优化问题的渐近极限；2）存在最优开环控制；3）动态规划原理和Bellman-Isaac不动点定理成立。数值实验在分布规划和金融系统性风险中的应用验证了方法的优势。

Conclusion: 结论是提出的鲁棒均值场控制框架能够有效处理共同噪声不确定性，为大规模合作智能体系统提供了理论保证和实用方法，在考虑噪声不确定性的情况下具有明显优势。

Abstract: We propose and analyze a framework for discrete-time robust mean-field
control problems under common noise uncertainty. In this framework, the
mean-field interaction describes the collective behavior of infinitely many
cooperative agents' state and action, while the common noise -- a random
disturbance affecting all agents' state dynamics -- is uncertain. A social
planner optimizes over open-loop controls on an infinite horizon to maximize
the representative agent's worst-case expected reward, where worst-case
corresponds to the most adverse probability measure among all candidates
inducing the unknown true law of the common noise process. We refer to this
optimization as a robust mean-field control problem under common noise
uncertainty. We first show that this problem arises as the asymptotic limit of
a cooperative $N$-agent robust optimization problem, commonly known as
propagation of chaos. We then prove the existence of an optimal open-loop
control by linking the robust mean field control problem to a lifted robust
Markov decision problem on the space of probability measures and by
establishing the dynamic programming principle and Bellman--Isaac fixed point
theorem for the lifted robust Markov decision problem. Finally, we complement
our theoretical results with numerical experiments motivated by distribution
planning and systemic risk in finance, highlighting the advantages of
accounting for common noise uncertainty.

</details>


### [141] [Hidden Convexity in Queueing Models](https://arxiv.org/abs/2511.03955)
*Xin Chen,Linwei Xin,Minda Zhao*

Main category: math.OC

TL;DR: 该论文研究了排队系统中到达率和服务率的联合控制问题，揭示了目标函数虽然非凸但具有隐藏凸性，证明了梯度方法能收敛到全局最优解。


<details>
  <summary>Details</summary>
Motivation: 虽然排队系统控制的目标函数是非凸的，但一阶方法在实践中被观察到能收敛到全局最优解。本文旨在为这一经验现象提供理论解释。

Method: 通过变量变换将原问题转化为凸优化问题，建立了Polyak-Lojasiewicz-Kurdyka条件，证明了一阶方法的全局收敛性。

Result: 发现了排队系统控制问题的隐藏凸性，证明了在GI/GI/1排队模型（包括Gamma分布）中不存在伪局部最小值。

Conclusion: 排队系统控制问题具有隐藏凸性结构，这为一阶方法的全局收敛性提供了理论保证，适用于广泛的排队模型。

Abstract: We study the joint control of arrival and service rates in queueing systems
with the objective of minimizing long-run expected cost minus revenue. Although
the objective function is non-convex, first-order methods have been empirically
observed to converge to globally optimal solutions. This paper provides a
theoretical foundation for this empirical phenomenon by characterizing the
optimization landscape and identifying a hidden convexity: the problem admits a
convex reformulation after an appropriate change of variables. Leveraging this
hidden convexity, we establish the Polyak-Lojasiewicz-Kurdyka (PLK) condition
for the original control problem, which excludes spurious local minima and
ensures global convergence for first-order methods. Our analysis applies to a
broad class of $GI/GI/1$ queueing models, including those with
Gamma-distributed interarrival and service times. As a key ingredient in the
proof, we establish a new convexity property of the expected queue length under
a square-root transformation of the traffic intensity.

</details>


### [142] [Towards optimal control of ensembles of discrete-time systems](https://arxiv.org/abs/2511.04230)
*Christian Fiedler,Alessandro Scagliotti*

Main category: math.OC

TL;DR: 本文研究了离散时间系统集合的最优控制问题，旨在最小化集合上的平均有限时域成本。建立了在温和假设下最优解的存在性，并提供了Γ收敛结果以实现对复杂集合最优控制问题的近似求解。


<details>
  <summary>Details</summary>
Motivation: 控制动态系统集合是一个有趣且具有挑战性的问题，特别是在量子控制等领域。本文旨在研究离散时间系统集合的最优控制，为这一领域奠定理论基础。

Method: 针对非常一般的非线性控制系统和阶段/终端成本，在温和假设下建立了最优解的存在性。提供了Γ收敛结果，使得可以通过使用集合上的经验概率测度来一致地近似复杂的集合最优控制问题。

Result: 证明了离散时间系统集合最优控制问题最优解的存在性，并提供了能够实现一致近似的Γ收敛理论框架。

Conclusion: 研究结果为离散时间系统集合的最优控制奠定了坚实的理论基础，为未来的研究开辟了许多有趣的方向。

Abstract: The control of ensembles of dynamical systems is an intriguing and
challenging problem, arising for example in quantum control. We initiate the
investigation of optimal control of ensembles of discrete-time systems,
focusing on minimising the average finite horizon cost over the ensemble. For
very general nonlinear control systems and stage and terminal costs, we
establish existence of minimisers under mild assumptions. Furthermore, we
provide a $\Gamma$-convergence result which enables consistent approximation of
the challenging ensemble optimal control problem, for example, by using
empirical probability measures over the ensemble. Our results form a solid
foundation for discrete-time optimal control of ensembles, with many
interesting avenues for future research.

</details>


### [143] [An Efficient Algorithm for Learning-Based Visual Localization](https://arxiv.org/abs/2511.04232)
*Jindi Zhong,Ziyuan Guo,Hongxia Wang,Huanshui Zhang*

Main category: math.OC

TL;DR: 提出基于最优控制原理的新算法，在GPS受限环境中实现高效视觉定位，通过Hessian矩阵对角信息估计提升神经网络性能并加速优化收敛。


<details>
  <summary>Details</summary>
Motivation: 解决GPS受限环境下计算资源有限时的视觉定位问题，需要在有限计算资源下实现高效且鲁棒的定位性能。

Method: 基于最优控制原理，提出新算法，通过Hessian矩阵对角信息估计来训练高性能深度神经网络并加速优化收敛。

Result: 在公开数据集上的实验表明，最终模型实现了具有竞争力的定位精度，并展现出显著的泛化能力。

Conclusion: 该研究为开发高性能离线定位系统提供了新的思路和方法。

Abstract: This paper addresses the visual localization problem in Global Positioning
System (GPS)-denied environments, where computational resources are often
limited. To achieve efficient and robust performance under these constraints,
we propose a novel algorithm. The algorithm stems from the optimal control
principle (OCP). It incorporates diagonal information estimation of the Hessian
matrix, which results in training a higher-performance deep neural network and
accelerates optimization convergence. Experimental results on public datasets
demonstrate that the final model achieves competitive localization accuracy and
exhibits remarkable generalization capability. This study provides new insights
for developing high-performance offline positioning systems.

</details>


### [144] [Some obstacle problems for partially hinged plates and related optimization issues](https://arxiv.org/abs/2511.04287)
*Elvise Berchio,Filomena Feo,Antonio Giuseppe Grimaldi*

Main category: math.OC

TL;DR: 研究带障碍物的部分铰接矩形板的最优化问题，包括避免碰撞的密度分布优化和增强扭转稳定性的障碍物优化


<details>
  <summary>Details</summary>
Motivation: 研究桥梁路面等部分铰接矩形板在存在真实和人工障碍物情况下的优化问题，真实障碍物需要避免碰撞，人工障碍物用于增强稳定性

Method: 针对真实障碍物建立最坏情况优化问题，最小化振荡幅度与密度分布的关系；针对人工障碍物，最小化量化板长边位移差异的间隙函数最大值

Result: 为两个问题提供了存在性结果，并讨论了最优密度分布和障碍物的定性性质

Conclusion: 建立了部分铰接矩形板在障碍物存在情况下的优化框架，为桥梁等结构的稳定性设计提供了理论基础

Abstract: We study optimization problems for partially hinged rectangular plates,
modeling bridge roadways, in the presence of real and artificial obstacles.
Real obstacles represent structural constraints to avoid, while artificial ones
are introduced to enhance stability. For the former, aiming to prevent
collisions, we set up a worst-case optimization problem in which we minimize
the amplitude of oscillations with respect to the density distribution; for the
latter, aiming to improve the torsional stability, we minimize, with respect to
the obstacles, the maximum of a gap function quantifying the displacement
between the long edges of the plate. For both problems, existence results are
provided, along with a discussion about qualitative properties of optimal
density distributions and obstacles.

</details>


### [145] [Signature-Based Universal Bilinear Approximations for Nonlinear Systems and Model Order Reduction](https://arxiv.org/abs/2511.04303)
*Martin Redmann,Justus Werner*

Main category: math.OC

TL;DR: 本文提出了一种基于签名方法处理非Lipschitz非线性系统的技术，通过签名构建通用双线性系统来近似原系统，并结合模型降阶方法获得低维双线性模型。


<details>
  <summary>Details</summary>
Motivation: 处理非Lipschitz非线性系统的近似问题，传统线性化方法在大规模场景下不可行，需要一种维度仅随输入数量增长的高效近似方法。

Method: 使用粗糙路径理论中的签名方法构建通用双线性系统来近似非线性系统，然后针对不稳定的双线性系统开发专门的模型降阶方法，仅需数据即可学习签名线性映射。

Result: 该方法能够有效近似非线性系统状态或输出，在大规模设置下仍保持可行性，通过数值实验验证了在非线性系统模型降阶应用中的有效性。

Conclusion: 签名方法结合模型降阶为非线性系统提供了一种高效的近似和数据拟合技术，特别适用于仅需数据而不需要显式系统模型的情况。

Abstract: This paper deals with non-Lipschitz nonlinear systems. Such systems can be
approximated by a linear map of so-called signatures, which play a crucial role
in the theory of rough paths and can be interpreted as collections of iterated
integrals involving the control process. As a consequence, we identify a
universal bilinear system, solved by the signature, that can approximate the
state or output of the original nonlinear dynamics arbitrarily well. In
contrast to other (bi)linearization techniques, the signature approach remains
feasible in large-scale settings, as the dimension of the associated bilinear
system grows only with the number of inputs. However, the signature model is
typically of high order, requiring an optimization process based on model order
reduction (MOR). We derive an MOR method for unstable bilinear systems with
non-zero initial states and apply it to the signature, yielding a potentially
low-dimensional bilinear model. An advantage of our method is that the original
nonlinear system need not be known explicitly, since only data are required to
learn the linear map of the signature. The subsequent MOR procedure is
model-oriented and specifically designed for the signature process.
Consequently, this work has two main applications: (1) efficient modeling/data
fitting using small-scale bilinear systems, and (2) MOR for nonlinear systems.
We illustrate the effectiveness of our approach in the second application
through numerical experiments.

</details>


### [146] [On the relationship between MESP and 0/1 D-Opt and their upper bounds](https://arxiv.org/abs/2511.04350)
*Gabriel Ponte,Marcia Fampa,Jon Lee*

Main category: math.OC

TL;DR: 本文建立了最大熵采样和0/1 D最优性这两个实验设计中的基本非线性0/1优化问题之间的强连接，通过实例映射分析其行为，并传输上界方法，建立了新的支配结果和不等式。


<details>
  <summary>Details</summary>
Motivation: 探索最大熵采样和0/1 D最优性这两个实验设计中的基础优化问题之间的深层联系，以改进上界方法和分支定界策略。

Method: 通过实例映射建立两个问题间的连接，分析映射行为，传输上界方法，比较基于这些映射的不同分支定界方案。

Result: 建立了新的支配结果和不等式，发现了在直接应用于真实数据MESP实例时看似不理想的上界方法，在来自0/1 D最优性的MESP实例中变得有用。

Conclusion: 两个问题间的映射连接为改进上界方法和分支定界策略提供了新的途径，某些上界方法在特定问题转换后表现出更好的性能。

Abstract: We establish strong connections between two fundamental nonlinear 0/1
optimization problems coming from the area of experimental design, namely
maximum entropy sampling and 0/1 D-Optimality. The connections are based on
maps between instances, and we analyze the behavior of these maps. Using these
maps, we transport basic upper-bounding methods between these two problems, and
we are able to establish new domination results and other inequalities relating
various basic upper bounds. Further, we establish results relating how
different branch-and-bound schemes based on these maps compare. Additionally,
we observe some surprising numerical results, where bounding methods that did
not seem promising in their direct application to real-data MESP instances, are
now useful for MESP instances that come from 0/1 D-Optimality.

</details>


### [147] [Lower and Upper Bounds for Small Canonical and Ordered Ramsey Numbers](https://arxiv.org/abs/2511.04364)
*Daniel Brosch,Bernard Lidický,Sydney Miyasaki,Diane Puges*

Main category: math.OC

TL;DR: 本文研究了Ramsey数在三种组合设置下的扩展：有序Ramsey数、规范Ramsey数和无序规范Ramsey数，针对小图确定了相应的Ramsey数值。


<details>
  <summary>Details</summary>
Motivation: 将经典Ramsey理论扩展到其他组合设置，研究有序图、规范着色和无序规范着色中的Ramsey型问题。

Method: 使用禁忌搜索和整数规划获得下界，使用标志代数或整数规划建立上界。

Result: 确定了所有最多4个顶点图的有序Ramsey数（除K4-外）、所有P4排序的规范Ramsey数，以及精确值CR(6,3)=26和CR(3,5)=13。

Conclusion: 成功将Ramsey理论扩展到三种不同的组合设置，并为小图确定了相应的Ramsey数值。

Abstract: In this paper, we investigate three extensions of Ramsey numbers to other
combinatorial settings.
  We first consider ordered Ramsey numbers. Here, we ask for a monochromatic
copy of a linearly ordered graph $G$ in every $2$-edge-coloring of a linearly
ordered complete graph $K_n$. The smallest such $n$ is denoted by $\vec{R}(G)$.
  Next, we study canonical Ramsey numbers. A canonical coloring of a linearly
ordered graph $G$ is an edge-coloring in which $G$ is monochromatic, rainbow,
or min/max-lexicographic. In the latter case, each pair of edges receives the
same color if and only if they share the same first (respectively, second)
vertex. Erd\H{o}s and Rado showed that for every $p$ there exists $n$ such that
every edge-coloring of a linearly ordered $K_n$ contains a canonical copy of
$K_p$; the smallest such $n$ is denoted by $ER(G)$.
  Finally, we examine unordered canonical Ramsey numbers, introduced by Richer.
An edge-coloring of $G$ is orderable if there exists a linear ordering of its
vertices such that the color of each edge is determined by its first vertex.
Unlike lexicographic colorings, this notion also includes monochromatic
colorings. Richer proved that for all $s$ and $t$, there exists $n$ such that
every edge-coloring of $K_n$ contains an orderable copy of $K_s$ or a rainbow
$K_t$. The smallest such $n$ is denoted by $CR(s,t)$.
  In all three settings, we focus on determining the corresponding Ramsey
numbers for small graphs $G$. We use tabu search and integer programming to
obtain lower bounds, and flag algebras or integer programming to establish
upper bounds. Among other results, we determine $\vec{R}(G)$ for all graphs $G$
on up to four vertices except $K_4^-$, $ER(P_4)$ for all orderings of $P_4$,
and the exact values $CR(6,3)=26$ and $CR(3,5)=13$.

</details>


### [148] [On the feasibility of generalized inverse linear programs](https://arxiv.org/abs/2511.04549)
*Christoph Buchheim,Lowig T. Duer*

Main category: math.OC

TL;DR: 本文研究了广义逆线性规划的可行性问题，分析了不同参数设置和目标集结构下的计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 研究在参数化线性规划中，如何选择参数使得最优解满足特定目标集条件，这在优化理论和实际应用中具有重要意义。

Method: 通过分析目标集Y的结构、LP的形式、可调参数和场景类型，系统研究可行性决策问题的计算复杂度。

Result: 当Y为单点集时，标准形式的LP问题可解，但自然形式的LP问题是NP难的；当给定目标基B时，乐观情况下标准形式LP是NP完全的，悲观情况下仍可解；部分固定目标解的问题几乎立即NP难，但固定参数在非固定变量数量下可解。

Conclusion: 广义逆线性规划的可行性问题复杂度高度依赖于目标集结构和问题形式，为这类问题的算法设计提供了理论指导。

Abstract: We investigate the feasibility problem for generalized inverse linear
programs. Given an LP with affinely parametrized objective function and
right-hand side as well as a target set Y, the goal is to decide whether the
parameters can be chosen such that there exists an optimal solution that
belongs to Y (optimistic scenario) or such that all optimal solutions belong to
Y (pessimistic scenario). We study the complexity of this decision problem and
show how it depends on the structure of the set Y, the form of the LP, the
adjustable parameters, and the underlying scenario. For a target singleton Y =
{y}, we show that the problem is tractable if the given LP is in standard form,
but NP-hard if the LP is given in natural form. If instead we are given a
target basis B, the problem in standard form becomes NP-complete in the
optimistic case, while remaining tractable in the pessimistic case. For
partially fixed target solutions, the problem gets almost immediately NP-hard,
but we prove fixed-parameter tractability in the number of non-fixed variables.
Moreover, we give a rigorous proof of membership in NP for any polyhedral
target set, and discuss how this property can be extended to more general
target sets using an oracle-based approach.

</details>


### [149] [Unified Theory of Adaptive Variance Reduction](https://arxiv.org/abs/2511.04569)
*Aleksandr Shestakov,Valery Parfenov,Aleksandr Beznosikov*

Main category: math.OC

TL;DR: 本文提出了一种新的方差缩减方法，采用自适应步长且无需超参数调优，扩展了现有框架以包含有偏估计器，适用于有限和问题、分布式优化和坐标方法。


<details>
  <summary>Details</summary>
Motivation: 现有的方差缩减方法主要关注无偏估计器，但无偏性假设可能过于严格。本文旨在扩展方差缩减框架以包含有偏估计器，并提出无需超参数调优的自适应步长方法。

Method: 提出新的方差缩减方法，采用自适应步长调整机制，无需手动调参。方法扩展了现有框架，允许使用有偏估计器，并适用于有限和问题、分布式优化和坐标方法。

Result: 数值实验在多种任务中验证了所提方法的有效性，表明自适应步长方差缩减方法具有良好的性能表现。

Conclusion: 无偏性假设在方差缩减中并非必要，有偏估计器也可被有效纳入框架。提出的自适应步长方法无需超参数调优，在多种优化任务中表现出色。

Abstract: Variance reduction is a family of powerful mechanisms for stochastic
optimization that appears to be helpful in many machine learning tasks. It is
based on estimating the exact gradient with some recursive sequences.
Previously, many papers demonstrated that methods with unbiased
variance-reduction estimators can be described in a single framework. We
generalize this approach and show that the unbiasedness assumption is
excessive; hence, we include biased estimators in this analysis. But the main
contribution of our work is the proposition of new variance reduction methods
with adaptive step sizes that are adjusted throughout the algorithm iterations
and, moreover, do not need hyperparameter tuning. Our analysis covers finite-
sum problems, distributed optimization, and coordinate methods. Numerical
experiments in various tasks validate the effectiveness of our methods.

</details>


### [150] [Knothe-Rosenblatt maps via soft-constrained optimal transport](https://arxiv.org/abs/2511.04579)
*Ricardo Baptista,Franca Hoffmann,Minh Van Hoang Nguyen,Benjamin Zhang*

Main category: math.OC

TL;DR: 本文证明了Knothe-Rosenblatt重排可以通过带软约束的加权成本最优输运问题的松弛解序列的极限获得，并扩展到动态最优输运中三角速度场的构造。


<details>
  <summary>Details</summary>
Motivation: 扩展Knothe-Rosenblatt重排的构造方法，证明其可以通过松弛的最优输运问题序列极限获得，为实际估计KR映射提供理论基础。

Method: 使用带软约束的加权成本最优输运问题的松弛解序列，并扩展到动态最优输运框架中构造三角速度场。

Result: 成功证明了KR映射可以作为松弛最优输运问题解的极限获得，并建立了动态最优输运中三角速度场的最优性。

Conclusion: 该研究为估计KR映射的各种变分方法提供了理论依据，并为开发新的静态和动态最优输运估计器开辟了可能性。

Abstract: In the theory of optimal transport, the Knothe-Rosenblatt (KR) rearrangement
provides an explicit construction to map between two probability measures by
building one-dimensional transformations from the marginal conditionals of one
measure to the other. The KR map has shown to be useful in different realms of
mathematics and statistics, from proving functional inequalities to designing
methodologies for sampling conditional distributions. It is known that the KR
rearrangement can be obtained as the limit of a sequence of optimal transport
maps with a weighted quadratic cost. We extend these results in this work by
showing that one can obtain the KR map as a limit of maps that solve a
relaxation of the weighted-cost optimal transport problem with a
soft-constraint for the target distribution. In addition, we show that this
procedure also applies to the construction of triangular velocity fields via
dynamic optimal transport yielding optimal velocity fields. This justifies
various variational methodologies for estimating KR maps in practice by
minimizing a divergence between the target and pushforward measure through an
approximate map. Moreover, it opens the possibilities for novel static and
dynamic OT estimators for KR maps.

</details>


### [151] [Computational Modeling and Learning-Based Adaptive Control of Solid-Fuel Ramjets](https://arxiv.org/abs/2511.04580)
*Gohar T. Khokhar,Kyle Hanquist,Parham Oveissi,Alex Dorsey,Ankit Goel*

Main category: math.OC

TL;DR: 本文提出了一个结合计算流体动力学模型和学习型自适应控制方法的固体燃料冲压发动机控制框架，实现了在复杂非线性系统中的精确推力调节。


<details>
  <summary>Details</summary>
Motivation: 固体燃料冲压发动机具有紧凑、能量密度高的优点，适用于长距离高速飞行，但由于强烈的非线性、有限的执行权限以及燃料回归、燃烧和可压缩流之间的复杂多物理场耦合，推力调节面临重大挑战。

Method: 开发了包含热添加的计算流体动力学模型来表征推力响应，建立操作包线并识别进气道失速起始点。然后应用基于回顾成本自适应控制算法的自适应比例积分控制器进行在线推力调节。

Result: 闭环仿真表明，基于RCAC的控制器在静态和动态操作条件下都能实现精确的推力调节，同时对指令、超参数和进气道状态的变化具有鲁棒性。

Conclusion: RCAC特别适用于难以获得精确降阶模型的SFRJ控制，学习型自适应控制方法有望在未来吸气式推进应用中实现SFRJ的鲁棒可靠运行。

Abstract: Solid-fuel ramjets offer a compact, energy-dense propulsion option for
long-range, high-speed flight but pose significant challenges for thrust
regulation due to strong nonlinearities, limited actuation authority, and
complex multi-physics coupling between fuel regression, combustion, and
compressible flow. This paper presents a computational and control framework
that combines a computational fluid dynamics model of an SFRJ with a
learning-based adaptive control approach. A CFD model incorporating heat
addition was developed to characterize thrust response, establish the
operational envelope, and identify the onset of inlet unstart. An adaptive
proportional-integral controller, updated online using the retrospective cost
adaptive control (RCAC) algorithm, was then applied to regulate thrust.
Closed-loop simulations demonstrate that the RCAC-based controller achieves
accurate thrust regulation under both static and dynamic operating conditions,
while remaining robust to variations in commands, hyperparameters, and inlet
states. The results highlight the suitability of RCAC for SFRJ control, where
accurate reduced-order models are challenging to obtain, and underscore the
potential of learning-based adaptive control to enable robust and reliable
operation of SFRJs in future air-breathing propulsion applications.

</details>


### [152] [Closing the Gap: Efficient Algorithms for Discrete Wasserstein Barycenters](https://arxiv.org/abs/2511.04607)
*Jiaqi Wang,Weijun Xie*

Main category: math.OC

TL;DR: 本文针对离散Wasserstein重心问题开发了多项式时间近似方案(PTAS)，将已知的2-近似比改进为更紧的近似保证。


<details>
  <summary>Details</summary>
Motivation: 离散Wasserstein重心问题在机器学习和运筹学中经常出现，但已知是NP难问题，现有最佳算法只能达到2-近似比，需要开发更好的近似算法。

Method: 提出了多项式时间近似方案(PTAS)，对于等权重测度的特殊情况获得了更紧的近似保证，并通过数值实验验证计算效率。

Result: 新算法显著改进了现有2-近似比，在等权重情况下获得更严格的近似保证，数值实验显示算法计算高效且产生接近最优的重心解。

Conclusion: 成功开发了PTAS算法，填补了离散Wasserstein重心问题近似算法的空白，提供了理论上更优且实际计算高效的解决方案。

Abstract: The Wasserstein barycenter problem seeks a probability measure that minimizes
the weighted average of the Wasserstein distances to a given collection of
probability measures. We study the discrete setting, where each measure has
finite support-- a regime that frequently arises in machine learning and
operations research. The discrete Wasserstein barycenter problem is known to be
NP-hard, which motivates us to study approximation algorithms with provable
guarantees. The best-known algorithm to date achieves an approximation ratio of
two. We close this gap by developing a polynomial-time approximation scheme
(PTAS) for the discrete Wasserstein barycenter problem that generalizes and
improves upon the 2-approximation method. In addition, for the special case of
equally weighted measures, we obtain a strictly tighter approximation
guarantee. Numerical experiments show that the proposed algorithms are
computationally efficient and produce near-optimal barycenter solutions.

</details>


### [153] [ODE approximation for the Adam algorithm: General and overparametrized setting](https://arxiv.org/abs/2511.04622)
*Steffen Dereich,Arnulf Jentzen,Sebastian Kassing*

Main category: math.OC

TL;DR: 本文通过ODE方法分析Adam优化器，在快慢尺度机制下证明Adam是特定向量场的渐近伪轨迹，并建立了收敛性结果。在一般设置中，Adam收敛点必须是Adam向量场的零点而非目标函数临界点；但在过参数化经验风险最小化中，Adam能局部找到最小值集。


<details>
  <summary>Details</summary>
Motivation: Adam是目前深度学习中最流行的优化方法，但对其收敛行为的理论理解仍然有限。本文旨在通过ODE方法在快慢尺度机制下系统研究Adam优化器的收敛性质。

Method: 采用基于ODE的方法，在固定动量参数和消失步长的条件下，将Adam算法建模为特定向量场（Adam向量场）的渐近伪轨迹，并利用渐近伪轨迹的性质分析收敛行为。

Result: 在一般设置中，如果Adam收敛，其极限必须是Adam向量场的零点而非目标函数的临界点或局部极小值点；在过参数化经验风险最小化中，目标函数可作为Adam向量场诱导流的Lyapunov函数，使得Adam能在最小值邻域内收敛到全局最小值集。

Conclusion: Adam优化器的收敛行为在不同设置下具有显著差异：一般情形下收敛到向量场零点，过参数化情形下能有效找到全局最小值集，这为理解Adam的实际表现提供了理论依据。

Abstract: The Adam optimizer is currently presumably the most popular optimization
method in deep learning. In this article we develop an ODE based method to
study the Adam optimizer in a fast-slow scaling regime. For fixed momentum
parameters and vanishing step-sizes, we show that the Adam algorithm is an
asymptotic pseudo-trajectory of the flow of a particular vector field, which is
referred to as the Adam vector field. Leveraging properties of asymptotic
pseudo-trajectories, we establish convergence results for the Adam algorithm.
In particular, in a very general setting we show that if the Adam algorithm
converges, then the limit must be a zero of the Adam vector field, rather than
a local minimizer or critical point of the objective function.
  In contrast, in the overparametrized empirical risk minimization setting, the
Adam algorithm is able to locally find the set of minima. Specifically, we show
that in a neighborhood of the global minima, the objective function serves as a
Lyapunov function for the flow induced by the Adam vector field. As a
consequence, if the Adam algorithm enters a neighborhood of the global minima
infinitely often, it converges to the set of global minima.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [154] [Scaling Agent Learning via Experience Synthesis](https://arxiv.org/abs/2511.03773)
*Zhaorun Chen,Zhuokai Zhao,Kai Zhang,Bo Liu,Qi Qi,Yifan Wu,Tarun Kalluri,Sara Cao,Yuanhao Xiong,Haibo Tong,Huaxiu Yao,Hengduo Li,Jiacheng Zhu,Xian Li,Dawn Song,Bo Li,Jason Weston,Dat Huynh*

Main category: cs.AI

TL;DR: DreamGym是一个统一的框架，通过基于推理的经验模型合成多样化经验数据，解决RL训练中昂贵的真实环境交互问题，实现可扩展的自主智能体在线强化学习训练。


<details>
  <summary>Details</summary>
Motivation: 强化学习虽然能让大语言模型智能体通过交互实现自我改进，但实际应用面临成本高昂的交互、任务多样性有限、奖励信号不可靠和基础设施复杂等挑战，阻碍了可扩展经验数据的收集。

Method: DreamGym将环境动态提炼为基于推理的经验模型，通过逐步推理推导一致的状态转换和反馈信号；利用离线真实数据初始化经验回放缓冲区并持续丰富；自适应生成挑战当前策略的新任务以支持在线课程学习。

Result: 在多样环境和智能体骨干上的实验表明，DreamGym显著改善了RL训练。在非RL就绪任务如WebArena上，性能超过所有基线30%以上；在RL就绪但成本高昂的环境中，仅使用合成交互就能匹配GRPO和PPO性能；在将纯合成经验训练的策略迁移到真实环境RL时，DreamGym在需要更少真实世界交互的同时带来显著的额外性能提升。

Conclusion: DreamGym为通用强化学习提供了一个可扩展的预热启动策略，通过合成经验数据有效解决了RL训练中的可扩展性问题。

Abstract: While reinforcement learning (RL) can empower large language model (LLM)
agents by enabling self-improvement through interaction, its practical adoption
remains challenging due to costly rollouts, limited task diversity, unreliable
reward signals, and infrastructure complexity, all of which obstruct the
collection of scalable experience data. To address these challenges, we
introduce DreamGym, the first unified framework designed to synthesize diverse
experiences with scalability in mind to enable effective online RL training for
autonomous agents. Rather than relying on expensive real-environment rollouts,
DreamGym distills environment dynamics into a reasoning-based experience model
that derives consistent state transitions and feedback signals through
step-by-step reasoning, enabling scalable agent rollout collection for RL. To
improve the stability and quality of transitions, DreamGym leverages an
experience replay buffer initialized with offline real-world data and
continuously enriched with fresh interactions to actively support agent
training. To improve knowledge acquisition, DreamGym adaptively generates new
tasks that challenge the current agent policy, enabling more effective online
curriculum learning. Experiments across diverse environments and agent
backbones demonstrate that DreamGym substantially improves RL training, both in
fully synthetic settings and in sim-to-real transfer scenarios. On non-RL-ready
tasks like WebArena, DreamGym outperforms all baselines by over 30%. And in
RL-ready but costly settings, it matches GRPO and PPO performance using only
synthetic interactions. When transferring a policy trained purely on synthetic
experiences to real-environment RL, DreamGym yields significant additional
performance gains while requiring far fewer real-world interactions, providing
a scalable warm-start strategy for general-purpose RL.

</details>


### [155] [How Different Tokenization Algorithms Impact LLMs and Transformer Models for Binary Code Analysis](https://arxiv.org/abs/2511.03825)
*Ahmed Mostafa,Raisul Arefin Nahid,Samuel Mulder*

Main category: cs.AI

TL;DR: 本文评估了NLP分词模型在汇编代码分析中的内在特性，包括词汇量大小、语义覆盖等，并研究了其对下游任务（如函数签名预测）的影响。研究发现分词器选择显著影响下游性能，内在指标只能部分预测外在评估结果。


<details>
  <summary>Details</summary>
Motivation: 汇编代码分析中的分词问题研究不足，但分词对词汇量大小、语义覆盖和下游任务性能有重要影响。本研究旨在填补这一空白，探索适合汇编代码特性的分词方法和参数选择。

Method: 系统研究多种分词模型，通过内在评估比较分词效率、词汇压缩和表示保真度。使用Llama 3.2、BERT和BART等预训练模型，评估分词器在多个性能指标上的有效性。

Result: 初步发现表明分词器选择显著影响下游性能，内在指标只能部分预测外在评估结果。揭示了内在分词器特性与实际汇编代码任务效用之间的复杂权衡。

Conclusion: 本研究为优化低级代码分析的分词模型提供了宝贵见解，有助于提高基于自然语言模型的二进制分析工作流程的鲁棒性和可扩展性。

Abstract: Tokenization is fundamental in assembly code analysis, impacting intrinsic
characteristics like vocabulary size, semantic coverage, and extrinsic
performance in downstream tasks. Despite its significance, tokenization in the
context of assembly code remains an underexplored area. This study aims to
address this gap by evaluating the intrinsic properties of Natural Language
Processing (NLP) tokenization models and parameter choices, such as vocabulary
size. We explore preprocessing customization options and pre-tokenization rules
tailored to the unique characteristics of assembly code. Additionally, we
assess their impact on downstream tasks like function signature prediction -- a
critical problem in binary code analysis.
  To this end, we conduct a thorough study on various tokenization models,
systematically analyzing their efficiency in encoding assembly instructions and
capturing semantic nuances. Through intrinsic evaluations, we compare
tokenizers based on tokenization efficiency, vocabulary compression, and
representational fidelity for assembly code. Using state-of-the-art pre-trained
models such as the decoder-only Large Language Model (LLM) Llama 3.2, the
encoder-only transformer BERT, and the encoder-decoder model BART, we evaluate
the effectiveness of these tokenizers across multiple performance metrics.
Preliminary findings indicate that tokenizer choice significantly influences
downstream performance, with intrinsic metrics providing partial but incomplete
predictability of extrinsic evaluation outcomes. These results reveal complex
trade-offs between intrinsic tokenizer properties and their utility in
practical assembly code tasks. Ultimately, this study provides valuable
insights into optimizing tokenization models for low-level code analysis,
contributing to the robustness and scalability of Natural Language Model
(NLM)-based binary analysis workflows.

</details>


### [156] [To See or To Read: User Behavior Reasoning in Multimodal LLMs](https://arxiv.org/abs/2511.03845)
*Tianning Dong,Luyi Ma,Varun Vasudevan,Jason Cho,Sushant Kumar,Kannan Achan*

Main category: cs.AI

TL;DR: BehaviorLens框架系统评估了多模态大语言模型在用户行为推理中的模态权衡，发现图像表示相比文本表示能提升87.5%的预测准确率。


<details>
  <summary>Details</summary>
Motivation: 探索文本和图像表示用户行为数据哪种更有效，以最大化多模态大语言模型的性能。

Method: 使用BehaviorLens框架，在六个MLLMs上评估三种数据表示方式：文本段落、散点图和流程图，基于真实购买序列数据集。

Result: 当数据表示为图像时，MLLMs的下一次购买预测准确率相比等效文本表示提高了87.5%，且无需额外计算成本。

Conclusion: 图像表示在用户行为推理中显著优于文本表示，为MLLMs的应用提供了重要指导。

Abstract: Multimodal Large Language Models (MLLMs) are reshaping how modern agentic
systems reason over sequential user-behavior data. However, whether textual or
image representations of user behavior data are more effective for maximizing
MLLM performance remains underexplored. We present \texttt{BehaviorLens}, a
systematic benchmarking framework for assessing modality trade-offs in
user-behavior reasoning across six MLLMs by representing transaction data as
(1) a text paragraph, (2) a scatter plot, and (3) a flowchart. Using a
real-world purchase-sequence dataset, we find that when data is represented as
images, MLLMs next-purchase prediction accuracy is improved by 87.5% compared
with an equivalent textual representation without any additional computational
cost.

</details>


### [157] [Question the Questions: Auditing Representation in Online Deliberative Processes](https://arxiv.org/abs/2511.04588)
*Soham De,Lodewijk Gelauff,Ashish Goel,Smitha Milli,Ariel Procaccia,Alice Siu*

Main category: cs.AI

TL;DR: 本文提出了一个基于正当代表性（JR）的审计框架，用于衡量专家问答环节中问题选择的代表性，并开发了高效的审计算法。通过历史审议数据比较了主持人选择、整数线性规划选择和LLM生成问题三种方法的代表性。


<details>
  <summary>Details</summary>
Motivation: 在公民大会等审议过程中，参与者向专家提问的机会有限，如何选择最具代表性的问题子集是一个重要挑战。现有方法缺乏系统性的代表性衡量标准。

Method: 引入基于正当代表性（JR）的审计框架，开发了在通用效用设置下审计JR的算法，最有效算法的时间复杂度为O(mn log n)。将方法应用于历史审议数据，比较了主持人选择、整数线性规划和LLM生成问题的代表性。

Result: 研究结果显示了LLM在支持审议过程中的潜力和当前局限性。通过将方法整合到已在50多个国家使用的在线审议平台，使实践者能够轻松审计和改进代表性。

Conclusion: 提出的审计框架和算法为审议过程中的问题选择提供了系统性的代表性衡量方法，有助于提高审议过程的民主性和包容性。

Abstract: A central feature of many deliberative processes, such as citizens'
assemblies and deliberative polls, is the opportunity for participants to
engage directly with experts. While participants are typically invited to
propose questions for expert panels, only a limited number can be selected due
to time constraints. This raises the challenge of how to choose a small set of
questions that best represent the interests of all participants. We introduce
an auditing framework for measuring the level of representation provided by a
slate of questions, based on the social choice concept known as justified
representation (JR). We present the first algorithms for auditing JR in the
general utility setting, with our most efficient algorithm achieving a runtime
of $O(mn\log n)$, where $n$ is the number of participants and $m$ is the number
of proposed questions. We apply our auditing methods to historical
deliberations, comparing the representativeness of (a) the actual questions
posed to the expert panel (chosen by a moderator), (b) participants' questions
chosen via integer linear programming, (c) summary questions generated by large
language models (LLMs). Our results highlight both the promise and current
limitations of LLMs in supporting deliberative processes. By integrating our
methods into an online deliberation platform that has been used for over
hundreds of deliberations across more than 50 countries, we make it easy for
practitioners to audit and improve representation in future deliberations.

</details>


### [158] [KnowThyself: An Agentic Assistant for LLM Interpretability](https://arxiv.org/abs/2511.03878)
*Suraj Prasai,Mengnan Du,Ying Zhang,Fan Yang*

Main category: cs.AI

TL;DR: KnowThyself是一个基于聊天的LLM可解释性工具，通过整合现有功能、降低技术门槛，提供交互式可视化和指导性解释。


<details>
  <summary>Details</summary>
Motivation: 现有LLM可解释性工具分散且代码密集，需要更统一、易用的解决方案来降低技术门槛。

Method: 使用编排器LLM重新表述用户查询，通过代理路由器将查询定向到专门模块，最后将输出情境化为连贯解释。

Result: 开发了一个基于聊天的可扩展平台，提供交互式可视化，为LLM检查提供强大基础。

Conclusion: KnowThyself通过对话式工作流程为LLM可解释性提供了易于访问的解决方案，具有可扩展性。

Abstract: We develop KnowThyself, an agentic assistant that advances large language
model (LLM) interpretability. Existing tools provide useful insights but remain
fragmented and code-intensive. KnowThyself consolidates these capabilities into
a chat-based interface, where users can upload models, pose natural language
questions, and obtain interactive visualizations with guided explanations. At
its core, an orchestrator LLM first reformulates user queries, an agent router
further directs them to specialized modules, and the outputs are finally
contextualized into coherent explanations. This design lowers technical
barriers and provides an extensible platform for LLM inspection. By embedding
the whole process into a conversational workflow, KnowThyself offers a robust
foundation for accessible LLM interpretability.

</details>


### [159] [Extracting Causal Relations in Deep Knowledge Tracing](https://arxiv.org/abs/2511.03948)
*Kevin Hong,Kia Karbasi,Gregory Pottie*

Main category: cs.AI

TL;DR: 本文挑战了关于深度知识追踪(DKT)性能来源的普遍解释，证明DKT的优势在于其隐式建模先决关系作为因果结构的能力，而非双向关系。


<details>
  <summary>Details</summary>
Motivation: 长期以来，计算教育研究的目标是开发可解释的知识追踪模型。DKT被认为比传统方法有重大改进，但对其性能来源的解释存在争议。

Method: 通过将练习关系图修剪为有向无环图(DAGs)，并在Assistments数据集的因果子集上训练DKT，分析其预测能力与因果结构的一致性。

Result: 实验表明DKT的预测能力与因果结构高度一致，并提出了使用DKT学习表示提取练习关系DAGs的替代方法。

Conclusion: DKT的有效性主要源于其近似知识组件间因果依赖关系的能力，而非简单的关系映射。

Abstract: A longstanding goal in computational educational research is to develop
explainable knowledge tracing (KT) models. Deep Knowledge Tracing (DKT), which
leverages a Recurrent Neural Network (RNN) to predict student knowledge and
performance on exercises, has been proposed as a major advancement over
traditional KT methods. Several studies suggest that its performance gains stem
from its ability to model bidirectional relationships between different
knowledge components (KCs) within a course, enabling the inference of a
student's understanding of one KC from their performance on others. In this
paper, we challenge this prevailing explanation and demonstrate that DKT's
strength lies in its implicit ability to model prerequisite relationships as a
causal structure, rather than bidirectional relationships. By pruning exercise
relation graphs into Directed Acyclic Graphs (DAGs) and training DKT on causal
subsets of the Assistments dataset, we show that DKT's predictive capabilities
align strongly with these causal structures. Furthermore, we propose an
alternative method for extracting exercise relation DAGs using DKT's learned
representations and provide empirical evidence supporting our claim. Our
findings suggest that DKT's effectiveness is largely driven by its capacity to
approximate causal dependencies between KCs rather than simple relational
mappings.

</details>


### [160] [LLMs and Cultural Values: the Impact of Prompt Language and Explicit Cultural Framing](https://arxiv.org/abs/2511.03980)
*Bram Bulté,Ayla Rigouts Terryn*

Main category: cs.AI

TL;DR: LLMs对提示语言和文化框架敏感，但存在系统性偏见，偏向荷兰、德国、美国和日本等少数国家的价值观，无法充分代表文化多样性。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs是否能代表其广泛用户群体的文化多样性，以及提示语言和文化框架如何影响模型响应与不同国家人类价值观的匹配度。

Method: 使用霍夫斯泰德价值观调查模块和世界价值观调查中的63个项目，翻译成11种语言，以带或不带明确文化视角的提示形式测试10个LLM。

Result: 提示语言和文化视角都会导致LLM输出变化，但所有模型都表现出相似模式：在大多数话题上保持中立，在社交宽容等问题上采取选择性进步立场。明确文化视角比针对性提示语言更能改善与人类受访者文化价值观的匹配度。

Conclusion: LLMs处于尴尬的中间地带：对提示变化足够敏感以产生变化，但又过于固守特定文化默认值，无法充分代表文化多样性。

Abstract: Large Language Models (LLMs) are rapidly being adopted by users across the
globe, who interact with them in a diverse range of languages. At the same
time, there are well-documented imbalances in the training data and
optimisation objectives of this technology, raising doubts as to whether LLMs
can represent the cultural diversity of their broad user base. In this study,
we look at LLMs and cultural values and examine how prompt language and
cultural framing influence model responses and their alignment with human
values in different countries. We probe 10 LLMs with 63 items from the Hofstede
Values Survey Module and World Values Survey, translated into 11 languages, and
formulated as prompts with and without different explicit cultural
perspectives. Our study confirms that both prompt language and cultural
perspective produce variation in LLM outputs, but with an important caveat:
While targeted prompting can, to a certain extent, steer LLM responses in the
direction of the predominant values of the corresponding countries, it does not
overcome the models' systematic bias toward the values associated with a
restricted set of countries in our dataset: the Netherlands, Germany, the US,
and Japan. All tested models, regardless of their origin, exhibit remarkably
similar patterns: They produce fairly neutral responses on most topics, with
selective progressive stances on issues such as social tolerance. Alignment
with cultural values of human respondents is improved more with an explicit
cultural perspective than with a targeted prompt language. Unexpectedly,
combining both approaches is no more effective than cultural framing with an
English prompt. These findings reveal that LLMs occupy an uncomfortable middle
ground: They are responsive enough to changes in prompts to produce variation,
but too firmly anchored to specific cultural defaults to adequately represent
cultural diversity.

</details>


### [161] [ArchPilot: A Proxy-Guided Multi-Agent Approach for Machine Learning Engineering](https://arxiv.org/abs/2511.03985)
*Zhuowen Yuan,Tao Liu,Yang Yang,Yang Wang,Feng Qi,Kaushik Rangadurai,Bo Li,Shuang Yang*

Main category: cs.AI

TL;DR: ArchPilot是一个多代理系统，通过集成架构生成、代理评估和自适应搜索来解决LLM代理在自动ML工程中依赖重复完整训练的问题，显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM代理在自动ML工程中严重依赖重复的完整训练来评估候选方案，导致计算开销大、搜索空间扩展性差、迭代周期慢。

Method: ArchPilot包含三个专业代理：协调搜索过程的编排代理（使用MCTS启发算法和重启机制）、迭代生成改进架构的生成代理、执行代理训练和优化代理函数的评估代理。

Result: 在MLE-Bench上的实验表明，ArchPilot优于AIDE和ML-Master等SOTA基线，验证了多代理系统的有效性。

Conclusion: ArchPilot通过多代理协作，在有限预算下优先考虑高潜力候选方案，最小化对昂贵完整训练的依赖，实现高效的ML工程。

Abstract: Recent LLM-based agents have demonstrated strong capabilities in automated ML
engineering. However, they heavily rely on repeated full training runs to
evaluate candidate solutions, resulting in significant computational overhead,
limited scalability to large search spaces, and slow iteration cycles. To
address these challenges, we introduce ArchPilot, a multi-agent system that
integrates architecture generation, proxy-based evaluation, and adaptive search
into a unified framework. ArchPilot consists of three specialized agents: an
orchestration agent that coordinates the search process using a Monte Carlo
Tree Search (MCTS)-inspired novel algorithm with a restart mechanism and
manages memory of previous candidates; a generation agent that iteratively
generates, improves, and debugs candidate architectures; and an evaluation
agent that executes proxy training runs, generates and optimizes proxy
functions, and aggregates the proxy scores into a fidelity-aware performance
metric. This multi-agent collaboration allows ArchPilot to prioritize
high-potential candidates with minimal reliance on expensive full training
runs, facilitating efficient ML engineering under limited budgets. Experiments
on MLE-Bench demonstrate that ArchPilot outperforms SOTA baselines such as AIDE
and ML-Master, validating the effectiveness of our multi-agent system.

</details>


### [162] [Detecting Silent Failures in Multi-Agentic AI Trajectories](https://arxiv.org/abs/2511.04032)
*Divya Pathak,Harshit Kumar,Anuska Roy,Felix George,Mudit Verma,Pratibha Moogi*

Main category: cs.AI

TL;DR: 提出了多智能体AI系统中的异常检测任务，创建了两个包含4,275和894条轨迹的数据集，并展示了监督和半监督方法能达到98%和96%的准确率。


<details>
  <summary>Details</summary>
Motivation: 多智能体AI系统具有非确定性且容易发生难以检测的静默故障，如漂移、循环和输出细节缺失等问题。

Method: 开发了数据集构建流程来捕捉用户行为、智能体非确定性和LLM变化，并使用XGBoost和SVDD等异常检测方法进行基准测试。

Result: 监督方法（XGBoost）达到98%准确率，半监督方法（SVDD）达到96%准确率，两种方法表现相当。

Conclusion: 这是首个对多智能体AI系统异常检测的系统性研究，提供了数据集、基准和见解来指导未来研究。

Abstract: Multi-Agentic AI systems, powered by large language models (LLMs), are
inherently non-deterministic and prone to silent failures such as drift,
cycles, and missing details in outputs, which are difficult to detect. We
introduce the task of anomaly detection in agentic trajectories to identify
these failures and present a dataset curation pipeline that captures user
behavior, agent non-determinism, and LLM variation. Using this pipeline, we
curate and label two benchmark datasets comprising \textbf{4,275 and 894}
trajectories from Multi-Agentic AI systems. Benchmarking anomaly detection
methods on these datasets, we show that supervised (XGBoost) and
semi-supervised (SVDD) approaches perform comparably, achieving accuracies up
to 98% and 96%, respectively. This work provides the first systematic study of
anomaly detection in Multi-Agentic AI systems, offering datasets, benchmarks,
and insights to guide future research.

</details>


### [163] [Interpreting Multi-Attribute Confounding through Numerical Attributes in Large Language Models](https://arxiv.org/abs/2511.04053)
*Hirohane Takagi,Gouki Minegishi,Shota Kizawa,Issey Sukeda,Hitomi Yanaka*

Main category: cs.AI

TL;DR: LLMs在数值推理中存在系统错误，主要原因是数值属性在共享潜在子空间中编码，且会放大真实世界相关性，无关上下文会干扰表示并影响决策。


<details>
  <summary>Details</summary>
Motivation: 尽管行为研究发现LLMs存在数值推理错误，但其底层表示机制尚不清楚。研究旨在探索LLMs如何内部整合实体的多个数值属性，以及无关数值上下文如何干扰这些表示和输出。

Method: 结合线性探测与偏相关分析，以及基于提示的脆弱性测试，在不同规模的模型上进行实验。

Result: LLMs编码了真实世界的数值相关性但倾向于系统性地放大它们；无关上下文会引起数值表示的一致偏移，且下游影响随模型规模变化。

Conclusion: 这些发现揭示了LLM决策中的脆弱性，为在多属性纠缠下实现更公平、表示感知的控制奠定了基础。

Abstract: Although behavioral studies have documented numerical reasoning errors in
large language models (LLMs), the underlying representational mechanisms remain
unclear. We hypothesize that numerical attributes occupy shared latent
subspaces and investigate two questions:(1) How do LLMs internally integrate
multiple numerical attributes of a single entity? (2)How does irrelevant
numerical context perturb these representations and their downstream outputs?
To address these questions, we combine linear probing with partial correlation
analysis and prompt-based vulnerability tests across models of varying sizes.
Our results show that LLMs encode real-world numerical correlations but tend to
systematically amplify them. Moreover, irrelevant context induces consistent
shifts in magnitude representations, with downstream effects that vary by model
size. These findings reveal a vulnerability in LLM decision-making and lay the
groundwork for fairer, representation-aware control under multi-attribute
entanglement.

</details>


### [164] [Agentmandering: A Game-Theoretic Framework for Fair Redistricting via Large Language Model Agents](https://arxiv.org/abs/2511.04076)
*Hao Li,Haotian Chen,Ruoyuan Gong,Juanjuan Wang,Hao Jiang*

Main category: cs.AI

TL;DR: 本文提出了Agentmandering框架，将选区重划重新构想为两个代表对立政治利益的智能体之间的回合制谈判，通过LLM智能体将战略互动嵌入到选区重划过程中。


<details>
  <summary>Details</summary>
Motivation: 现有的计算方法主要生成大量合法选区重划方案，但忽视了选择过程中的战略动态，这为党派行为者挑选技术上合规但政治上有利的地图创造了机会。

Method: 采用基于游戏理论的"选择-冻结"协议，两个LLM智能体轮流从少量候选地图中选择和冻结选区，通过受限且可解释的选择逐步划分州。

Result: 在2020年美国人口普查数据上的评估显示，Agentmandering显著减少了党派偏见和不公平性，同时比标准基线方法降低了2到3个数量级的方差。

Conclusion: 该方法在摇摆州情景中尤其表现出公平性和稳定性，证明了将战略互动嵌入选区重划过程的有效性。

Abstract: Redistricting plays a central role in shaping how votes are translated into
political power. While existing computational methods primarily aim to generate
large ensembles of legally valid districting plans, they often neglect the
strategic dynamics involved in the selection process. This oversight creates
opportunities for partisan actors to cherry-pick maps that, while technically
compliant, are politically advantageous. Simply satisfying formal constraints
does not ensure fairness when the selection process itself can be manipulated.
We propose \textbf{Agentmandering}, a framework that reimagines redistricting
as a turn-based negotiation between two agents representing opposing political
interests. Drawing inspiration from game-theoretic ideas, particularly the
\textit{Choose-and-Freeze} protocol, our method embeds strategic interaction
into the redistricting process via large language model (LLM) agents. Agents
alternate between selecting and freezing districts from a small set of
candidate maps, gradually partitioning the state through constrained and
interpretable choices. Evaluation on post-2020 U.S. Census data across all
states shows that Agentmandering significantly reduces partisan bias and
unfairness, while achieving 2 to 3 orders of magnitude lower variance than
standard baselines. These results demonstrate both fairness and stability,
especially in swing-state scenarios. Our code is available at
https://github.com/Lihaogx/AgentMandering.

</details>


### [165] [KGFR: A Foundation Retriever for Generalized Knowledge Graph Question Answering](https://arxiv.org/abs/2511.04093)
*Yuanning Cui,Zequn Sun,Wei Hu,Zhangjie Fu*

Main category: cs.AI

TL;DR: 提出了LLM-KGFR协作框架，结合大语言模型和知识图谱基础检索器，解决LLM在知识密集型问题上的局限性，实现零样本泛化和高效处理大规模图谱。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在推理方面表现出色，但在处理知识密集型问题时受限于上下文长度和参数化知识。现有方法依赖微调或图神经网络检索器，存在数据集特定调优和在大型或未见图谱上可扩展性不足的问题。

Method: LLM-KGFR协作框架：KGFR使用LLM生成的关系描述编码关系，根据问题中的角色初始化实体，实现零样本泛化。采用非对称渐进传播（APP）高效处理大型图谱，通过节点、边和路径级别接口支持LLM迭代请求候选答案、支持事实和推理路径。

Result: 实验表明LLM-KGFR在保持可扩展性和泛化能力的同时实现了强大性能，为知识图谱增强推理提供了实用解决方案。

Conclusion: LLM-KGFR框架通过结合LLM和结构化检索器的优势，有效解决了知识密集型推理问题，在可扩展性、泛化能力和性能方面都表现出色。

Abstract: Large language models (LLMs) excel at reasoning but struggle with
knowledge-intensive questions due to limited context and parametric knowledge.
However, existing methods that rely on finetuned LLMs or GNN retrievers are
limited by dataset-specific tuning and scalability on large or unseen graphs.
We propose the LLM-KGFR collaborative framework, where an LLM works with a
structured retriever, the Knowledge Graph Foundation Retriever (KGFR). KGFR
encodes relations using LLM-generated descriptions and initializes entities
based on their roles in the question, enabling zero-shot generalization to
unseen KGs. To handle large graphs efficiently, it employs Asymmetric
Progressive Propagation (APP)- a stepwise expansion that selectively limits
high-degree nodes while retaining informative paths. Through node-, edge-, and
path-level interfaces, the LLM iteratively requests candidate answers,
supporting facts, and reasoning paths, forming a controllable reasoning loop.
Experiments demonstrate that LLM-KGFR achieves strong performance while
maintaining scalability and generalization, providing a practical solution for
KG-augmented reasoning.

</details>


### [166] [Testing the Testers: Human-Driven Quality Assessment of Voice AI Testing Platforms](https://arxiv.org/abs/2511.04133)
*Miguel E. Andres,Vadim Fedorov,Rida Sadek,Enric Spagnolo-Arrizabalaga,Nadescha Trudel*

Main category: cs.AI

TL;DR: 提出了首个系统化框架来评估语音AI测试质量，通过人类中心化基准测试，解决了测试平台生成真实对话和准确评估响应的双重挑战。


<details>
  <summary>Details</summary>
Motivation: 随着语音AI代理快速部署到生产环境，缺乏系统化的测试可靠性验证方法，组织无法客观评估测试方法的有效性，这在大规模部署时造成了关键的测量差距。

Method: 结合心理测量技术（成对比较产生Elo评分、自助置信区间和置换检验）与严格的统计验证，提供可重复的度量标准，适用于任何测试方法。

Result: 对三个领先商业平台进行实证评估，结果显示性能存在显著差异，最佳平台Evalion的评估质量F1分数达到0.92，模拟质量达到0.61，明显优于其他平台。

Conclusion: 该框架使研究人员和组织能够实证验证任何平台的测试能力，为大规模语音AI部署提供了必要的测量基础。

Abstract: Voice AI agents are rapidly transitioning to production deployments, yet
systematic methods for ensuring testing reliability remain underdeveloped.
Organizations cannot objectively assess whether their testing approaches
(internal tools or external platforms) actually work, creating a critical
measurement gap as voice AI scales to billions of daily interactions.
  We present the first systematic framework for evaluating voice AI testing
quality through human-centered benchmarking. Our methodology addresses the
fundamental dual challenge of testing platforms: generating realistic test
conversations (simulation quality) and accurately evaluating agent responses
(evaluation quality). The framework combines established psychometric
techniques (pairwise comparisons yielding Elo ratings, bootstrap confidence
intervals, and permutation tests) with rigorous statistical validation to
provide reproducible metrics applicable to any testing approach.
  To validate the framework and demonstrate its utility, we conducted
comprehensive empirical evaluation of three leading commercial platforms
focused on Voice AI Testing using 21,600 human judgments across 45 simulations
and ground truth validation on 60 conversations. Results reveal statistically
significant performance differences with the proposed framework, with the
top-performing platform, Evalion, achieving 0.92 evaluation quality measured as
f1-score versus 0.73 for others, and 0.61 simulation quality using a league
based scoring system (including ties) vs 0.43 for other platforms.
  This framework enables researchers and organizations to empirically validate
the testing capabilities of any platform, providing essential measurement
foundations for confident voice AI deployment at scale. Supporting materials
are made available to facilitate reproducibility and adoption.

</details>


### [167] [When Empowerment Disempowers](https://arxiv.org/abs/2511.04177)
*Claire Yang,Maya Cakmak,Max Kleiman-Weiner*

Main category: cs.AI

TL;DR: 本文介绍了Disempower-Grid测试套件，发现优化单人类赋能的辅助RL代理在多人类环境中可能导致其他人类的环境影响力和奖励显著降低，即"去赋能"现象。


<details>
  <summary>Details</summary>
Motivation: 赋能作为AI代理的通用目标无关目标已被提出，但先前工作假设代理在孤立环境中协助单个人类。多人类环境如家庭和医院是AI协助的有前景场景，需要研究赋能目标在这些环境中的表现。

Method: 开发了开源的多人网格世界测试套件Disempower-Grid，通过实验验证优化单人类赋能的辅助RL代理在多人类环境中的影响。

Result: 实证表明，优化单人类赋能的辅助代理会显著降低其他人类的环境影响力和奖励，即产生去赋能现象。联合赋能可以缓解去赋能，但会以用户奖励为代价。

Conclusion: AI对齐社区面临更广泛的挑战：在单智能体设置中看似对齐的目标无关目标，在多智能体环境中可能变得不对齐。

Abstract: Empowerment, a measure of an agent's ability to control its environment, has
been proposed as a universal goal-agnostic objective for motivating assistive
behavior in AI agents. While multi-human settings like homes and hospitals are
promising for AI assistance, prior work on empowerment-based assistance assumes
that the agent assists one human in isolation. We introduce an open source
multi-human gridworld test suite Disempower-Grid. Using Disempower-Grid, we
empirically show that assistive RL agents optimizing for one human's
empowerment can significantly reduce another human's environmental influence
and rewards - a phenomenon we formalize as disempowerment. We characterize when
disempowerment occurs in these environments and show that joint empowerment
mitigates disempowerment at the cost of the user's reward. Our work reveals a
broader challenge for the AI alignment community: goal-agnostic objectives that
seem aligned in single-agent settings can become misaligned in multi-agent
contexts.

</details>


### [168] [Opus: A Quantitative Framework for Workflow Evaluation](https://arxiv.org/abs/2511.04220)
*Alan Seroul,Théo Fagnoni,Inès Adnani,Dana O. Mohamed,Phillip Kingston*

Main category: cs.AI

TL;DR: Opus工作流评估框架：一个概率-规范性框架，用于量化工作流质量和效率，结合正确性、可靠性和成本，支持自动化评估、排名和优化。


<details>
  <summary>Details</summary>
Motivation: 需要一种数学方法来直接比较、评分和优化工作流，整合正确性、可靠性和成本等概念。

Method: 结合Opus工作流奖励（概率函数估计预期性能）和Opus工作流规范性惩罚（测量结构信息质量），支持自动化评估和强化学习优化。

Result: 提出了统一的工作流评估框架，能够形式化工作流成功概率，定义可测量的规范性惩罚，并提出联合奖励-惩罚权衡的优化公式。

Conclusion: 该框架为工作流质量评估和优化提供了数学基础，可集成到现代自动化系统中指导工作流发现和改进。

Abstract: This paper introduces the Opus Workflow Evaluation Framework, a
probabilistic-normative formulation for quantifying Workflow quality and
efficiency. It integrates notions of correctness, reliability, and cost into a
coherent mathematical model that enables direct comparison, scoring, and
optimization of Workflows. The framework combines the Opus Workflow Reward, a
probabilistic function estimating expected performance through success
likelihood, resource usage, and output gain, with the Opus Workflow Normative
Penalties, a set of measurable functions capturing structural and informational
quality across Cohesion, Coupling, Observability, and Information Hygiene. It
supports automated Workflow assessment, ranking, and optimization within modern
automation systems such as Opus and can be integrated into Reinforcement
Learning loops to guide Workflow discovery and refinement. In this paper, we
introduce the Opus Workflow Reward model that formalizes Workflow success as a
probabilistic expectation over costs and outcomes. We define measurable Opus
Workflow Normative Penalties capturing structural, semantic, and signal-related
properties of Workflows. Finally, we propose a unified optimization formulation
for identifying and ranking optimal Workflows under joint Reward-Penalty
trade-offs.

</details>


### [169] [Shared Spatial Memory Through Predictive Coding](https://arxiv.org/abs/2511.04235)
*Zhengru Fang,Yu Guo,Jingjing Wang,Yuang Zhang,Haonan An,Yinhai Wang,Yuguang Fang*

Main category: cs.AI

TL;DR: 提出多智能体预测编码框架，通过最小化相互不确定性来协调，在带宽受限条件下实现高效通信和社会空间表征学习。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体系统中部分可观测性和有限带宽导致的协调失败问题，需要开发带宽高效的通信机制和社会空间表征。

Method: 基于信息瓶颈目标的多智能体预测编码框架，结合网格细胞状度规作为内部空间编码，通过自监督运动预测自发涌现，并采用分层强化学习策略主动探索减少联合不确定性。

Result: 在Memory-Maze基准测试中，方法对带宽约束表现出优异鲁棒性：带宽从128位/步降至4位/步时，成功率从73.5%优雅下降至64.4%，而全广播基线从67.6%崩溃至28.6%。

Conclusion: 为复杂社会表征如何从统一的预测驱动中涌现提供了理论原则和生物学合理的基础，实现了社会集体智能。

Abstract: Sharing and reconstructing a consistent spatial memory is a critical
challenge in multi-agent systems, where partial observability and limited
bandwidth often lead to catastrophic failures in coordination. We introduce a
multi-agent predictive coding framework that formulate coordination as the
minimization of mutual uncertainty among agents. Instantiated as an information
bottleneck objective, it prompts agents to learn not only who and what to
communicate but also when. At the foundation of this framework lies a
grid-cell-like metric as internal spatial coding for self-localization,
emerging spontaneously from self-supervised motion prediction. Building upon
this internal spatial code, agents gradually develop a bandwidth-efficient
communication mechanism and specialized neural populations that encode
partners' locations: an artificial analogue of hippocampal social place cells
(SPCs). These social representations are further enacted by a hierarchical
reinforcement learning policy that actively explores to reduce joint
uncertainty. On the Memory-Maze benchmark, our approach shows exceptional
resilience to bandwidth constraints: success degrades gracefully from 73.5% to
64.4% as bandwidth shrinks from 128 to 4 bits/step, whereas a full-broadcast
baseline collapses from 67.6% to 28.6%. Our findings establish a theoretically
principled and biologically plausible basis for how complex social
representations emerge from a unified predictive drive, leading to social
collective intelligence.

</details>


### [170] [RLoop: An Self-Improving Framework for Reinforcement Learning with Iterative Policy Initialization](https://arxiv.org/abs/2511.04285)
*Zeng Zhiyuan,Jiashuo Liu,Zhangyue Yin,Ge Zhang,Wenhao Huang,Xipeng Qiu*

Main category: cs.AI

TL;DR: RLVR训练中存在RL过拟合问题，模型获得训练奖励但失去泛化能力。RLoop框架通过迭代策略初始化将训练过程转化为探索-利用循环，有效提升泛化性能。


<details>
  <summary>Details</summary>
Motivation: 解决RLVR训练中的RL过拟合问题，即模型在训练中获得奖励但泛化能力下降，这由策略过度专业化和训练期间产生的多样化解的灾难性遗忘驱动。

Method: 提出RLoop自改进框架，基于迭代策略初始化：首先使用RL从给定策略探索解空间，然后筛选成功轨迹创建专家数据集，通过拒绝采样微调(RFT)优化初始策略，为下一次迭代创建更好的起点。

Result: RLoop显著缓解遗忘并大幅提升泛化能力，相比原始RL平均准确率提高9%，pass@32指标提升超过15%。

Conclusion: RLoop通过探索-利用的迭代重新初始化循环，有效将瞬态策略变化转化为稳健的性能提升，解决了RLVR训练中的泛化问题。

Abstract: While Reinforcement Learning for Verifiable Rewards (RLVR) is powerful for
training large reasoning models, its training dynamics harbor a critical
challenge: RL overfitting, where models gain training rewards but lose
generalization. Our analysis reveals this is driven by policy
over-specialization and catastrophic forgetting of diverse solutions generated
during training. Standard optimization discards this valuable inter-step policy
diversity. To address this, we introduce RLoop, a self-improving framework
built on iterative policy initialization. RLoop transforms the standard
training process into a virtuous cycle: it first uses RL to explore the
solution space from a given policy, then filters the successful trajectories to
create an expert dataset. This dataset is used via Rejection-sampling
Fine-Tuning (RFT) to refine the initial policy, creating a superior starting
point for the next iteration. This loop of exploration and exploitation via
iterative re-initialization effectively converts transient policy variations
into robust performance gains. Our experiments show RLoop mitigates forgetting
and substantially improves generalization, boosting average accuracy by 9% and
pass@32 by over 15% compared to vanilla RL.

</details>


### [171] [GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents](https://arxiv.org/abs/2511.04307)
*Jian Mu,Chaoyun Zhang,Chiming Ni,Lu Wang,Bo Qiao,Kartik Mathur,Qianhui Wu,Yuhang Xie,Xiaojun Ma,Mengyu Zhou,Si Qin,Liqun Li,Yu Kang,Minghua Ma,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.AI

TL;DR: GUI-360°是一个大规模数据集和基准测试套件，旨在推进计算机使用代理(CUAs)的发展。它解决了真实CUA任务稀缺、多模态轨迹自动收集标注管道缺乏以及统一基准测试缺失的问题。


<details>
  <summary>Details</summary>
Motivation: 计算机使用代理面临三个关键挑战：真实世界CUA任务稀缺、缺乏多模态轨迹的自动收集和标注管道、缺少统一评估GUI定位、屏幕解析和动作预测的基准测试。

Method: 使用LLM增强的自动化管道进行查询来源、环境模板构建、任务实例化、批量执行和LLM驱动的质量过滤。包含120万+执行动作步骤、数千条轨迹、全分辨率截图、可访问性元数据等。

Result: 在GUI-360°上评估最先进的视觉语言模型显示，在定位和动作预测方面存在显著不足。监督微调和强化学习带来显著改进，但尚未达到人类水平的可靠性。

Conclusion: GUI-360°为可重复研究和稳健桌面CUAs的加速进展提供了支持，数据集已在HuggingFace上公开。

Abstract: We introduce GUI-360$^\circ$, a large-scale, comprehensive dataset and
benchmark suite designed to advance computer-using agents (CUAs). CUAs present
unique challenges and is constrained by three persistent gaps: a scarcity of
real-world CUA tasks, the lack of automated collection-and-annotation pipelines
for multi-modal trajectories, and the absence of a unified benchmark that
jointly evaluates GUI grounding, screen parsing, and action prediction.
  GUI-360$^\circ$ addresses these gaps with an LLM-augmented, largely automated
pipeline for query sourcing, environment-template construction, task
instantiation, batched execution, and LLM-driven quality filtering. The
released corpus contains over 1.2M executed action steps across thousands of
trajectories in popular Windows office applications, and includes
full-resolution screenshots, accessibility metadata when available,
instantiated goals, intermediate reasoning traces, and both successful and
failed action trajectories. The dataset supports three canonical tasks, GUI
grounding, screen parsing, and action prediction, and a hybrid GUI+API action
space that reflects modern agent designs. Benchmarking state-of-the-art
vision--language models on GUI-360$^\circ$ reveals substantial out-of-the-box
shortcomings in grounding and action prediction; supervised fine-tuning and
reinforcement learning yield significant gains but do not close the gap to
human-level reliability. We release GUI-360$^\circ$ and accompanying code to
facilitate reproducible research and accelerate progress on robust desktop
CUAs.
  The full dataset has been made public on
https://huggingface.co/datasets/vyokky/GUI-360.

</details>


### [172] [Probing the Probes: Methods and Metrics for Concept Alignment](https://arxiv.org/abs/2511.04312)
*Jacob Lysnæs-Larsen,Marte Eggen,Inga Strümke*

Main category: cs.AI

TL;DR: 本文揭示了在可解释AI中，仅凭概念激活向量(CAV)探测器的分类精度无法可靠评估概念对齐度，因为探测器可能捕捉虚假相关性而非目标概念。作者提出了基于空间线性归因的新概念定位方法，并引入三类定量评估概念对齐的指标。


<details>
  <summary>Details</summary>
Motivation: 当前在可解释AI领域，普遍假设高精度的概念探测器意味着CAV能忠实表示目标概念，但这种假设存在严重问题，探测器可能捕捉的是虚假相关性而非真正概念。

Method: 提出了基于空间线性归因的新概念定位方法，并与现有特征可视化技术进行比较。引入了三类定量评估指标：硬精度、分割分数和增强鲁棒性。

Result: 研究发现，具有平移不变性和空间对齐的探测器能持续提高概念对齐度。故意构造的错位探测器利用虚假相关性也能达到接近标准探测器的精度。

Conclusion: 需要基于对齐度的评估指标而非仅依赖探测器精度，并且需要根据模型架构和目标概念性质定制探测器。

Abstract: In explainable AI, Concept Activation Vectors (CAVs) are typically obtained
by training linear classifier probes to detect human-understandable concepts as
directions in the activation space of deep neural networks. It is widely
assumed that a high probe accuracy indicates a CAV faithfully representing its
target concept. However, we show that the probe's classification accuracy alone
is an unreliable measure of concept alignment, i.e., the degree to which a CAV
captures the intended concept. In fact, we argue that probes are more likely to
capture spurious correlations than they are to represent only the intended
concept. As part of our analysis, we demonstrate that deliberately misaligned
probes constructed to exploit spurious correlations, achieve an accuracy close
to that of standard probes. To address this severe problem, we introduce a
novel concept localization method based on spatial linear attribution, and
provide a comprehensive comparison of it to existing feature visualization
techniques for detecting and mitigating concept misalignment. We further
propose three classes of metrics for quantitatively assessing concept
alignment: hard accuracy, segmentation scores, and augmentation robustness. Our
analysis shows that probes with translation invariance and spatial alignment
consistently increase concept alignment. These findings highlight the need for
alignment-based evaluation metrics rather than probe accuracy, and the
importance of tailoring probes to both the model architecture and the nature of
the target concept.

</details>


### [173] [AdversariaLLM: A Unified and Modular Toolbox for LLM Robustness Research](https://arxiv.org/abs/2511.04316)
*Tim Beyer,Jonas Dornbusch,Jakob Steimle,Moritz Ladenburger,Leo Schwinn,Stephan Günnemann*

Main category: cs.AI

TL;DR: AdversariaLLM是一个用于LLM越狱鲁棒性研究的工具箱，旨在解决当前LLM安全研究生态系统碎片化、实现困难的问题，通过提供可复现、正确且可扩展的框架来促进LLM安全研究的透明性和可比性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM安全和鲁棒性研究生态系统存在碎片化、实现错误多的问题，导致研究结果难以复现和比较，阻碍了该领域的实质性进展。

Method: 设计了一个以可复现性、正确性和可扩展性为核心的工具箱，实现了12种对抗攻击算法，集成了7个基准数据集，支持通过Hugging Face访问多种开源LLM，并包含计算资源跟踪、确定性结果和分布评估等高级功能。

Result: 开发了AdversariaLLM工具箱，为LLM安全研究提供了强大的基础框架，能够支持透明、可比和可复现的研究。

Conclusion: AdversariaLLM通过建立标准化的研究框架，解决了LLM安全研究中的碎片化和复现性问题，为该领域的健康发展奠定了基础。

Abstract: The rapid expansion of research on Large Language Model (LLM) safety and
robustness has produced a fragmented and oftentimes buggy ecosystem of
implementations, datasets, and evaluation methods. This fragmentation makes
reproducibility and comparability across studies challenging, hindering
meaningful progress. To address these issues, we introduce AdversariaLLM, a
toolbox for conducting LLM jailbreak robustness research. Its design centers on
reproducibility, correctness, and extensibility. The framework implements
twelve adversarial attack algorithms, integrates seven benchmark datasets
spanning harmfulness, over-refusal, and utility evaluation, and provides access
to a wide range of open-weight LLMs via Hugging Face. The implementation
includes advanced features for comparability and reproducibility such as
compute-resource tracking, deterministic results, and distributional evaluation
techniques. \name also integrates judging through the companion package
JudgeZoo, which can also be used independently. Together, these components aim
to establish a robust foundation for transparent, comparable, and reproducible
research in LLM safety.

</details>


### [174] [RxSafeBench: Identifying Medication Safety Issues of Large Language Models in Simulated Consultation](https://arxiv.org/abs/2511.04328)
*Jiahao Zhao,Luxin Xu,Minghuan Tan,Lichao Zhang,Ahmadreza Argha,Hamid Alinejad-Rokny,Min Yang*

Main category: cs.AI

TL;DR: 提出了RxSafeBench框架，通过模拟临床咨询来系统评估LLMs的药物安全能力，包含6,725种禁忌症、28,781种药物相互作用和14,906个适应症-药物对的高质量数据集。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLMs的医疗系统在药物安全方面的研究有限，缺乏真实世界数据集，且在现实临床咨询环境中的评估不足。

Method: 构建包含药物风险信息的咨询对话，创建专门的药物安全数据库RxRisk DB，采用两阶段过滤策略确保临床真实性和专业性，最终形成2,443个高质量咨询场景的基准测试。

Result: 评估显示当前LLMs难以整合禁忌症和相互作用知识，特别是在风险暗示而非明确的情况下表现更差。

Conclusion: RxSafeBench提供了首个全面评估LLMs药物安全性的基准，为改进AI驱动的临床决策支持系统的可靠性提供了重要见解。

Abstract: Numerous medical systems powered by Large Language Models (LLMs) have
achieved remarkable progress in diverse healthcare tasks. However, research on
their medication safety remains limited due to the lack of real world datasets,
constrained by privacy and accessibility issues. Moreover, evaluation of LLMs
in realistic clinical consultation settings, particularly regarding medication
safety, is still underexplored. To address these gaps, we propose a framework
that simulates and evaluates clinical consultations to systematically assess
the medication safety capabilities of LLMs. Within this framework, we generate
inquiry diagnosis dialogues with embedded medication risks and construct a
dedicated medication safety database, RxRisk DB, containing 6,725
contraindications, 28,781 drug interactions, and 14,906 indication-drug pairs.
A two-stage filtering strategy ensures clinical realism and professional
quality, resulting in the benchmark RxSafeBench with 2,443 high-quality
consultation scenarios. We evaluate leading open-source and proprietary LLMs
using structured multiple choice questions that test their ability to recommend
safe medications under simulated patient contexts. Results show that current
LLMs struggle to integrate contraindication and interaction knowledge,
especially when risks are implied rather than explicit. Our findings highlight
key challenges in ensuring medication safety in LLM-based systems and provide
insights into improving reliability through better prompting and task-specific
tuning. RxSafeBench offers the first comprehensive benchmark for evaluating
medication safety in LLMs, advancing safer and more trustworthy AI-driven
clinical decision support.

</details>


### [175] [Monitor-Generate-Verify (MGV):Formalising Metacognitive Theory for Language Model Reasoning](https://arxiv.org/abs/2511.04341)
*Nick Oh,Fernand Gobet*

Main category: cs.AI

TL;DR: 论文提出了Monitor-Generate-Verify (MGV)框架，通过在Generate-Verify范式前添加显式监控机制来解决模型早期陷入次优推理路径的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时推理架构缺乏监控过程，导致模型容易陷入前缀主导陷阱——过早承诺次优推理路径且难以恢复，造成约20%的准确率损失。

Method: 将Flavell、Nelson和Narens的元认知理论形式化为计算规范，提出MGV框架。该框架在生成前添加显式监控，捕获元认知体验（从难度评估到置信度判断），并通过验证反馈来改进未来监控。

Result: 虽然未提供实证验证，但这是首次系统性地将基础元认知理论转化为计算规范。

Conclusion: 这项工作为理解推理系统失败提供了原则性词汇，并为未来测试时推理设计提出了具体的架构干预建议。

Abstract: Test-time reasoning architectures such as those following the Generate-Verify
paradigm -- where a model iteratively refines or verifies its own generated
outputs -- prioritise generation and verification but exclude the monitoring
processes that determine when and how reasoning should begin. This omission may
contribute to the prefix dominance trap, in which models commit early to
suboptimal reasoning paths and seldom recover, yielding roughly 20% accuracy
loss. We address this architectural gap by formalising Flavell's and Nelson and
Narens' metacognitive theories into computational specifications, proposing the
Monitor-Generate-Verify (MGV) framework. MGV extends the Generate-Verify
paradigm by adding explicit monitoring that captures metacognitive experiences
(from difficulty assessments to confidence judgements) before generation begins
and refines future monitoring through verification feedback. Though we present
no empirical validation, this work provides the first systematic computational
translation of foundational metacognitive theories, offering a principled
vocabulary for understanding reasoning system failures and suggesting specific
architectural interventions for future test-time reasoning designs.

</details>


### [176] [Post-Training LLMs as Better Decision-Making Agents: A Regret-Minimization Approach](https://arxiv.org/abs/2511.04393)
*Chanwoo Park,Ziyang Chen,Asuman Ozdaglar,Kaiqing Zhang*

Main category: cs.AI

TL;DR: 提出Iterative Regret-Minimization Fine-Tuning (Iterative RMFT)方法，通过迭代蒸馏低后悔决策轨迹来增强LLMs的决策能力。


<details>
  <summary>Details</summary>
Motivation: LLMs原本并非为决策设计，在基础在线决策问题中表现不佳，无法实现低后悔或有效的探索-利用权衡。

Method: 迭代后训练过程：模型生成多个决策轨迹，选择k个最低后悔的轨迹，并在这些轨迹上微调自身。利用后悔指标激发模型自身的决策能力和推理逻辑。

Result: Iterative RMFT提升了多种模型的决策性能，包括Transformer、开源LLMs和GPT-4o mini等闭源模型。在具有不同视野、动作空间、奖励过程和自然语言上下文的任务中展现出泛化能力。

Conclusion: Iterative RMFT提供了一个原则性的通用后训练框架，用于增强LLMs的决策能力，理论分析表明在简化设置下单层Transformer可作为无后悔学习器。

Abstract: Large language models (LLMs) are increasingly deployed as "agents" for
decision-making (DM) in interactive and dynamic environments. Yet, since they
were not originally designed for DM, recent studies show that LLMs can struggle
even in basic online DM problems, failing to achieve low regret or an effective
exploration-exploitation tradeoff. To address this, we introduce Iterative
Regret-Minimization Fine-Tuning (Iterative RMFT), a post-training procedure
that repeatedly distills low-regret decision trajectories back into the base
model. At each iteration, the model rolls out multiple decision trajectories,
selects the k-lowest regret ones, and fine-tunes itself on them. Unlike prior
methods that (a) distill action sequences from known DM algorithms or (b) rely
on manually crafted chain-of-thought templates, our approach leverages the
regret metric to elicit the model's own DM ability and reasoning rationales.
This reliance on model-generated reasoning avoids rigid output engineering and
provides more flexible, natural-language training signals. Empirical results
show that Iterative RMFT improves LLMs' DM performance across diverse models -
from Transformers with numerical input/output, to open-weight LLMs, and
advanced closed-weight models like GPT-4o mini. Its flexibility in output and
reasoning formats enables generalization across tasks with varying horizons,
action spaces, reward processes, and natural-language contexts. Finally, we
provide theoretical insight showing that a single-layer Transformer under this
paradigm can act as a no-regret learner in a simplified setting. Overall,
Iterative RMFT offers a principled and general post-training framework for
enhancing LLMs' decision-making capabilities.

</details>


### [177] [The Peril of Preference: Why GRPO fails on Ordinal Rewards](https://arxiv.org/abs/2511.04439)
*Anisha Garg,Ganesh Venkatesh*

Main category: cs.AI

TL;DR: CoRPO解决了GRPO在强化学习中处理序数奖励时的缺陷，通过自适应基准确保失败轨迹不被正向强化，并在达到质量阈值后切换到相对偏好模式以寻找最优解。


<details>
  <summary>Details</summary>
Motivation: GRPO的简单性使其在处理非二元反馈时存在缺陷，特别是使用序数奖励给予部分信用时，其组平均基准可能错误地正向强化失败轨迹。

Method: CoRPO采用自适应基准，强制执行最低质量阈值，确保失败解决方案永远不会被正向强化。一旦策略持续满足此阈值，基准自动切换到相对偏好模式。

Result: 在代码验证任务上的实证验证表明，CoRPO表现出更稳定的收敛性和更好的域外泛化能力。

Conclusion: 这项工作是在强化学习中使LLM学习真正新能力的关键步骤，通过从二元奖励扩展到序数奖励，为更密集的逐步监督奠定了基础。

Abstract: Group-relative Policy Optimization's (GRPO) simplicity makes it highly
desirable for adapting LLMs to become experts at specific tasks. But this
simplicity also makes it ill-specified as we seek to enhance RL training with
richer, non-binary feedback. When using ordinal rewards to give partial credit,
GRPO's simplicity starts to hurt, as its group-average baseline often assigns a
positive advantage to failed trajectories and reinforces incorrect behavior.
  We introduce Correctness Relative Policy Optimization (CoRPO), a new
formulation that solves this flaw. CoRPO uses an adaptive baseline that
enforces a minimum quality threshold, ensuring failed solutions are never
positively reinforced. Once the policy consistently meets this threshold, the
baseline automatically transitions to a relative preference mode, pushing the
model to find optimal solutions rather than just "acceptable" ones. We
empirically validate CoRPO on a code verification task, where it demonstrates
more stable convergence and better out-of-domain generalization.
  This work represents a critical step in our broader research program to
enable LLMs to learn genuinely new capabilities through reinforcement learning.
We achieve this by enabling LLMs to learn from rich, multi-dimensional feedback
- progressing from binary to ordinal rewards in this work, and onward to
denser, per-step supervision.

</details>


### [178] [Beyond Shortest Path: Agentic Vehicular Routing with Semantic Context](https://arxiv.org/abs/2511.04464)
*Carnot Braun,Rafael O. Jarczewski,Gabriel U. Talasso,Leandro A. Villas,Allan M. de Souza*

Main category: cs.AI

TL;DR: PAVe系统结合传统路径规划算法与LLM语义推理，通过多目标Dijkstra算法生成候选路线，再由LLM代理根据用户任务、偏好和规避规则进行上下文评估，实现个性化车辆路线规划。


<details>
  <summary>Details</summary>
Motivation: 传统车辆路线系统只能优化单一指标（如时间或距离），无法理解人类驾驶员的复杂语义和动态上下文（如多步骤任务、情境约束或紧急需求）。

Method: 使用多目标（时间、CO2）Dijkstra算法生成候选路线集，然后由LLM代理基于预处理的POI地理空间缓存，评估这些路线是否符合用户提供的任务、偏好和规避规则。

Result: 在真实城市场景基准测试中，PAVe成功将复杂用户意图转化为适当的路线修改，使用本地模型在初始路线选择中达到超过88%的准确率。

Conclusion: 将经典路由算法与基于LLM的语义推理层相结合，是创建个性化、自适应和可扩展的城市交通优化解决方案的稳健有效方法。

Abstract: Traditional vehicle routing systems efficiently optimize singular metrics
like time or distance, and when considering multiple metrics, they need more
processes to optimize . However, they lack the capability to interpret and
integrate the complex, semantic, and dynamic contexts of human drivers, such as
multi-step tasks, situational constraints, or urgent needs. This paper
introduces and evaluates PAVe (Personalized Agentic Vehicular Routing), a
hybrid agentic assistant designed to augment classical pathfinding algorithms
with contextual reasoning. Our approach employs a Large Language Model (LLM)
agent that operates on a candidate set of routes generated by a multi-objective
(time, CO2) Dijkstra algorithm. The agent evaluates these options against
user-provided tasks, preferences, and avoidance rules by leveraging a
pre-processed geospatial cache of urban Points of Interest (POIs). In a
benchmark of realistic urban scenarios, PAVe successfully used complex user
intent into appropriate route modifications, achieving over 88% accuracy in its
initial route selections with a local model. We conclude that combining
classical routing algorithms with an LLM-based semantic reasoning layer is a
robust and effective approach for creating personalized, adaptive, and scalable
solutions for urban mobility optimization.

</details>


### [179] [Promoting Sustainable Web Agents: Benchmarking and Estimating Energy Consumption through Empirical and Theoretical Analysis](https://arxiv.org/abs/2511.04481)
*Lars Krupp,Daniel Geißler,Vishal Banwari,Paul Lukowicz,Jakob Karolus*

Main category: cs.AI

TL;DR: 本文首次探索了网络代理的能源消耗和碳排放问题，通过理论和实证分析揭示了不同设计理念对能耗的显著影响，并呼吁在评估网络代理时考虑能效指标。


<details>
  <summary>Details</summary>
Motivation: 尽管网络代理研究蓬勃发展，但其引发的可持续性问题（如能源消耗和碳排放）尚未得到充分探索。本文旨在揭示这一紧迫问题，强调网络代理的环境影响。

Method: 采用理论和实证相结合的方法：理论层面通过估算分析，实证层面通过基准测试来评估网络代理的能源和二氧化碳成本。

Result: 研究发现不同网络代理设计理念会显著影响能耗，且更高能耗不一定带来更好结果。同时指出模型参数和过程缺乏透明度限制了能耗估算的准确性。

Conclusion: 需要改变网络代理的评估方式，在基准测试中引入专门的能耗衡量指标，以促进更可持续的网络代理发展。

Abstract: Web agents, like OpenAI's Operator and Google's Project Mariner, are powerful
agentic systems pushing the boundaries of Large Language Models (LLM). They can
autonomously interact with the internet at the user's behest, such as
navigating websites, filling search masks, and comparing price lists. Though
web agent research is thriving, induced sustainability issues remain largely
unexplored. To highlight the urgency of this issue, we provide an initial
exploration of the energy and $CO_2$ cost associated with web agents from both
a theoretical -via estimation- and an empirical perspective -by benchmarking.
Our results show how different philosophies in web agent creation can severely
impact the associated expended energy, and that more energy consumed does not
necessarily equate to better results. We highlight a lack of transparency
regarding disclosing model parameters and processes used for some web agents as
a limiting factor when estimating energy consumption. Our work contributes
towards a change in thinking of how we evaluate web agents, advocating for
dedicated metrics measuring energy consumption in benchmarks.

</details>


### [180] [Large language models replicate and predict human cooperation across experiments in game theory](https://arxiv.org/abs/2511.04500)
*Andrea Cera Palatsi,Samuel Martin-Gutierrez,Ana S. Cardenal,Max Pellert*

Main category: cs.AI

TL;DR: 本文通过开发博弈论实验的数字孪生和系统提示框架，发现LLaMA能高保真复制人类合作模式，而Qwen更接近纳什均衡预测，证明适当校准的LLM可以复制人类行为模式并探索新的实验空间。


<details>
  <summary>Details</summary>
Motivation: 理解LLM与人类决策的相似度至关重要，因为不匹配可能导致实际应用中的有害结果，而无法复制人类行为会使LLM在社会模拟中无效。

Method: 开发博弈论实验的数字孪生，引入系统提示和探测框架进行机器行为评估，测试三个开源模型（Llama、Mistral和Qwen）。

Result: Llama能高保真复制人类合作模式，捕捉人类偏离理性选择理论的行为；Qwen与纳什均衡预测高度一致；无需基于角色的提示即可实现群体级行为复制。

Conclusion: 适当校准的LLM可以复制人类行为模式，并能够系统探索未开发的实验空间，为社会科学研究提供补充方法，生成关于人类社交决策的新经验预测。

Abstract: Large language models (LLMs) are increasingly used both to make decisions in
domains such as health, education and law, and to simulate human behavior. Yet
how closely LLMs mirror actual human decision-making remains poorly understood.
This gap is critical: misalignment could produce harmful outcomes in practical
applications, while failure to replicate human behavior renders LLMs
ineffective for social simulations. Here, we address this gap by developing a
digital twin of game-theoretic experiments and introducing a systematic
prompting and probing framework for machine-behavioral evaluation. Testing
three open-source models (Llama, Mistral and Qwen), we find that Llama
reproduces human cooperation patterns with high fidelity, capturing human
deviations from rational choice theory, while Qwen aligns closely with Nash
equilibrium predictions. Notably, we achieved population-level behavioral
replication without persona-based prompting, simplifying the simulation
process. Extending beyond the original human-tested games, we generate and
preregister testable hypotheses for novel game configurations outside the
original parameter grid. Our findings demonstrate that appropriately calibrated
LLMs can replicate aggregate human behavioral patterns and enable systematic
exploration of unexplored experimental spaces, offering a complementary
approach to traditional research in the social and behavioral sciences that
generates new empirical predictions about human social decision-making.

</details>


### [181] [Optimizing Sensor Placement in Urban Storm Sewers: A Data-Driven Sparse Sensing Approach](https://arxiv.org/abs/2511.04556)
*Zihang Ding,Kun Zhang*

Main category: cs.AI

TL;DR: 提出了一种数据驱动的稀疏传感框架，结合EPA-SWMM模型，通过优化传感器布局来重建城市雨水系统中的峰值流量，在资源受限条件下实现高效洪水监测。


<details>
  <summary>Details</summary>
Motivation: 城市地表水洪水日益频繁，但高时空分辨率的洪水预测和监测在时间、预算和技术上存在实际限制，如何在资源受限条件下监测城市排水网络和预测流量状况是一个主要挑战。

Method: 使用SWMM模型生成训练数据集，应用数据驱动稀疏传感框架，利用奇异值分解进行降维和QR分解进行传感器分配，基于模拟训练数据集识别最优监测节点。

Result: 在77个节点中仅部署3个优化传感器即可实现满意的重建性能，纳什-萨克利夫效率值达到0.92-0.95，模型对测量不确定性具有良好的鲁棒性。

Conclusion: 该框架平衡了计算效率和物理可解释性，能够以最少的传感器实现高精度流量重建，可进一步与预测模型集成，在有限的传感和监测资源下实现洪水早期预警和实时控制。

Abstract: Urban surface water flooding, triggered by intense rainfall overwhelming
drainage systems, is increasingly frequent and widespread. While flood
prediction and monitoring in high spatial-temporal resolution are desired,
practical constraints in time, budget, and technology hinder its full
implementation. How to monitor urban drainage networks and predict flow
conditions under constrained resource is a major challenge. This study presents
a data-driven sparse sensing (DSS) framework, integrated with EPA-SWMM, to
optimize sensor placement and reconstruct peak flowrates in a stormwater
system, using the Woodland Avenue catchment in Duluth, Minnesota, as a case
study. We utilized a SWMM model to generate a training dataset of peak flowrate
profiles across the stormwater network. Furthermore, we applied DSS -
leveraging singular value decomposition for dimensionality reduction and QR
factorization for sensor allocation - to identify the optimal monitoring nodes
based on the simulated training dataset. We then validated the
representativeness of these identified monitoring nodes by comparing the
DSS-reconstructed peak flowrate profiles with those obtained from SWMM. Three
optimally placed sensors among 77 nodes achieved satisfactory reconstruction
performance with Nash-Sutcliffe Efficiency (NSE) values of 0.92-0.95 (25th to
75th percentiles). In addition, the model showed good robustness to uncertainty
in measurements. Its robustness to sensor failures is location-dependent and
improves with the number of sensors deployed. The framework balances
computational efficiency and physical interpretability, enabling high-accuracy
flow reconstruction with minimal sensors. This DSS framework can be further
integrated with predictive models to realize flood early warning and real-time
control under limited sensing and monitoring resource.

</details>


### [182] [Jr. AI Scientist and Its Risk Report: Autonomous Scientific Exploration from a Baseline Paper](https://arxiv.org/abs/2511.04583)
*Atsuyuki Miyai,Mashiro Toyooka,Takashi Otonari,Zaiying Zhao,Kiyoharu Aizawa*

Main category: cs.AI

TL;DR: 开发了Jr. AI Scientist系统，这是一个模拟学生研究流程的自主AI科学家系统，能够分析论文局限性、提出假设、实验验证并撰写论文，在评估中表现优于现有全自动系统，但仍有重要局限性。


<details>
  <summary>Details</summary>
Motivation: 理解AI科学家系统的当前能力和风险对于确保可信赖和可持续的AI驱动科学进步至关重要，同时需要保护学术生态系统的完整性。

Method: 开发Jr. AI Scientist系统，模拟新手学生研究者的核心研究流程：分析基线论文局限性、制定改进假设、通过严格实验验证、撰写结果论文。利用现代编码代理处理复杂的多文件实现。

Result: Jr. AI Scientist生成的论文在AI评审中获得比现有全自动系统更高的评分，但作者评估和Agents4Science评审都发现了重要局限性。

Conclusion: 当前AI科学家系统存在直接应用的风险和关键挑战，需要进一步研究。开发过程中识别了各种风险，这些见解有助于深化对AI科学家发展现状和风险的理解。

Abstract: Understanding the current capabilities and risks of AI Scientist systems is
essential for ensuring trustworthy and sustainable AI-driven scientific
progress while preserving the integrity of the academic ecosystem. To this end,
we develop Jr. AI Scientist, a state-of-the-art autonomous AI scientist system
that mimics the core research workflow of a novice student researcher: Given
the baseline paper from the human mentor, it analyzes its limitations,
formulates novel hypotheses for improvement, validates them through rigorous
experimentation, and writes a paper with the results. Unlike previous
approaches that assume full automation or operate on small-scale code, Jr. AI
Scientist follows a well-defined research workflow and leverages modern coding
agents to handle complex, multi-file implementations, leading to scientifically
valuable contributions. For evaluation, we conducted automated assessments
using AI Reviewers, author-led evaluations, and submissions to Agents4Science,
a venue dedicated to AI-driven scientific contributions. The findings
demonstrate that Jr. AI Scientist generates papers receiving higher review
scores than existing fully automated systems. Nevertheless, we identify
important limitations from both the author evaluation and the Agents4Science
reviews, indicating the potential risks of directly applying current AI
Scientist systems and key challenges for future research. Finally, we
comprehensively report various risks identified during development. We hope
these insights will deepen understanding of current progress and risks in AI
Scientist development.

</details>


### [183] [Are We Asking the Right Questions? On Ambiguity in Natural Language Queries for Tabular Data Analysis](https://arxiv.org/abs/2511.04584)
*Daniel Gomm,Cornelius Wolff,Madelon Hulsebos*

Main category: cs.AI

TL;DR: 该论文将自然语言查询中的歧义重新定义为合作交互的特征，开发了一个区分可解析合作查询与不可解析非合作查询的框架，并分析了15个流行数据集的查询类型。


<details>
  <summary>Details</summary>
Motivation: 自然语言表格数据接口需要处理查询中的固有歧义，作者认为不应将歧义视为缺陷，而应将其视为合作交互的特征，由用户和系统共同承担查询规范的责任。

Method: 开发了一个原则性框架来区分合作查询（可解析解释）和非合作查询（无法解析），并将该框架应用于15个流行数据集的表格问答和分析评估中。

Result: 分析发现这些数据集中的查询类型混合不当，既不适合评估系统执行准确性，也不适合评估解释能力。

Conclusion: 该框架和分析将视角从修复歧义转向在解析查询中拥抱合作，为表格数据自然语言接口的设计和评估提供了更明智的方法，并概述了未来研究的方向。

Abstract: Natural language interfaces to tabular data must handle ambiguities inherent
to queries. Instead of treating ambiguity as a deficiency, we reframe it as a
feature of cooperative interaction, where the responsibility of query
specification is shared among the user and the system. We develop a principled
framework distinguishing cooperative queries, i.e., queries that yield a
resolvable interpretation, from uncooperative queries that cannot be resolved.
Applying the framework to evaluations for tabular question answering and
analysis, we analyze the queries in 15 popular datasets, and observe an
uncontrolled mixing of query types neither adequate for evaluating a system's
execution accuracy nor for evaluating interpretation capabilities. Our
framework and analysis of queries shifts the perspective from fixing ambiguity
to embracing cooperation in resolving queries. This reflection enables more
informed design and evaluation for natural language interfaces for tabular
data, for which we outline implications and directions for future research.

</details>


### [184] [DR. WELL: Dynamic Reasoning and Learning with Symbolic World Model for Embodied LLM-Based Multi-Agent Collaboration](https://arxiv.org/abs/2511.04646)
*Narjes Nourzad,Hanqing Yang,Shiyu Chen,Carlee Joe-Wong*

Main category: cs.AI

TL;DR: DR.WELL是一个去中心化的神经符号框架，用于合作多智能体规划，通过两阶段协商协议和符号规划避免轨迹级协调的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 合作多智能体规划中，轨迹级协调往往因时序或运动的微小偏差而失败，需要更高层次的抽象来实现同步和集体进展。

Method: 采用两阶段协商协议：智能体先提出候选角色及理由，然后在共识和环境约束下承诺联合分配；承诺后各智能体独立生成和执行符号计划，通过共享世界模型进行执行结果的基础化。

Result: 在合作推块任务实验中，智能体能够跨情节适应，动态世界模型捕获可重用模式，提高了任务完成率和效率。

Conclusion: 通过符号规划而非原始轨迹推理，DR.WELL避免了脆弱的步级对齐，实现了可重用、可同步和可解释的高层操作，以时间开销换取演化出的更高效协作策略。

Abstract: Cooperative multi-agent planning requires agents to make joint decisions with
partial information and limited communication. Coordination at the trajectory
level often fails, as small deviations in timing or movement cascade into
conflicts. Symbolic planning mitigates this challenge by raising the level of
abstraction and providing a minimal vocabulary of actions that enable
synchronization and collective progress. We present DR. WELL, a decentralized
neurosymbolic framework for cooperative multi-agent planning. Cooperation
unfolds through a two-phase negotiation protocol: agents first propose
candidate roles with reasoning and then commit to a joint allocation under
consensus and environment constraints. After commitment, each agent
independently generates and executes a symbolic plan for its role without
revealing detailed trajectories. Plans are grounded in execution outcomes via a
shared world model that encodes the current state and is updated as agents act.
By reasoning over symbolic plans rather than raw trajectories, DR. WELL avoids
brittle step-level alignment and enables higher-level operations that are
reusable, synchronizable, and interpretable. Experiments on cooperative
block-push tasks show that agents adapt across episodes, with the dynamic world
model capturing reusable patterns and improving task completion rates and
efficiency. Experiments on cooperative block-push tasks show that our dynamic
world model improves task completion and efficiency through negotiation and
self-refinement, trading a time overhead for evolving, more efficient
collaboration strategies.

</details>


### [185] [VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical Consistency Checks](https://arxiv.org/abs/2511.04662)
*Yu Feng,Nathaniel Weir,Kaj Bostrom,Sam Bayless,Darion Cassel,Sapana Chaudhary,Benjamin Kiesl-Reiter,Huzefa Rangwala*

Main category: cs.AI

TL;DR: VeriCoT是一个神经符号方法，从CoT推理中提取并验证形式逻辑论证，通过一阶逻辑形式化推理步骤，使用自动求解器验证逻辑有效性，提高推理可靠性和准确性。


<details>
  <summary>Details</summary>
Motivation: LLMs通过CoT进行多步推理，但无法可靠验证自身逻辑，即使答案正确，底层推理可能存在缺陷，影响高风险场景中的可信度。

Method: VeriCoT将CoT推理步骤形式化为一阶逻辑，识别前提条件，使用符号表示和自动求解器验证逻辑有效性，支持人类和系统识别无根据或谬误推理步骤。

Result: 在ProofWriter、LegalBench和BioASQ数据集上的实验表明，VeriCoT能有效识别有缺陷的推理，并作为最终答案正确性的强预测指标。

Conclusion: VeriCoT的验证信号可用于推理时自我反思、监督微调和偏好微调，进一步提高了推理的有效性和准确性。

Abstract: LLMs can perform multi-step reasoning through Chain-of-Thought (CoT), but
they cannot reliably verify their own logic. Even when they reach correct
answers, the underlying reasoning may be flawed, undermining trust in
high-stakes scenarios. To mitigate this issue, we introduce VeriCoT, a
neuro-symbolic method that extracts and verifies formal logical arguments from
CoT reasoning. VeriCoT formalizes each CoT reasoning step into first-order
logic and identifies premises that ground the argument in source context,
commonsense knowledge, or prior reasoning steps. The symbolic representation
enables automated solvers to verify logical validity while the NL premises
allow humans and systems to identify ungrounded or fallacious reasoning steps.
Experiments on the ProofWriter, LegalBench, and BioASQ datasets show VeriCoT
effectively identifies flawed reasoning, and serves as a strong predictor of
final answer correctness. We also leverage VeriCoT's verification signal for
(1) inference-time self-reflection, (2) supervised fine-tuning (SFT) on
VeriCoT-distilled datasets and (3) preference fine-tuning (PFT) with direct
preference optimization (DPO) using verification-based pairwise rewards,
further improving reasoning validity and accuracy.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [186] [Levers of Power in the Field of AI](https://arxiv.org/abs/2511.03859)
*Tammy Mackenzie,Sukriti Punj,Natalie Perez,Sreyoshi Bhaduri,Branislav Radeljic*

Main category: cs.CY

TL;DR: 本研究通过虚构人物分析决策者在AI实施中如何运用权力杠杆，探讨个人能动性、组织逻辑和制度基础设施在AI治理中的交集。


<details>
  <summary>Details</summary>
Motivation: 探讨学术界、政府、企业和公民社会的决策者如何在人工智能实施中处理权力问题，了解他们如何体验和运用塑造制度对技术变革反应的社会机制。

Method: 使用基于新制度主义者工作的制度治理框架设计个性化问卷，收集决策者的制度管辖范围见解，并将匿名真实回应转化为12个虚构的高级决策者人物。

Result: 展示了北美和欧洲高级决策者的12个虚构人物，说明了个人能动性、组织逻辑和制度基础设施如何在AI治理中相互作用。

Conclusion: 为制度内的政策制定者和公民社会的同行提供了参与AI治理的个人手段，提出了权力杠杆动态表和五个可检验的假设。

Abstract: This paper examines how decision makers in academia, government, business,
and civil society navigate questions of power in implementations of artificial
intelligence. The study explores how individuals experience and exercise levers
of power, which are presented as social mechanisms that shape institutional
responses to technological change. The study reports on the responses of
personalized questionnaires designed to gather insight on a decision maker's
institutional purview, based on an institutional governance framework developed
from the work of Neo-institutionalists. Findings present the anonymized, real
responses and circumstances of respondents in the form of twelve fictional
personas of high-level decision makers from North America and Europe. These
personas illustrate how personal agency, organizational logics, and
institutional infrastructures may intersect in the governance of AI. The
decision makers' responses to the questionnaires then inform a discussion of
the field-level personal power of decision makers, methods of fostering
institutional stability in times of change, and methods of influencing
institutional change in the field of AI. The final section of the discussion
presents a table of the dynamics of the levers of power in the field of AI for
change makers and five testable hypotheses for institutional and social
movement researchers. In summary, this study provides insight on the means for
policymakers within institutions and their counterparts in civil society to
personally engage with AI governance.

</details>


### [187] [The Benefits of Data Storytelling in Accessible Teaching](https://arxiv.org/abs/2511.04024)
*Marina Buzzi,Barbara Leporini,Angelica Lo Duca*

Main category: cs.CY

TL;DR: 本文探讨了数据叙事作为实现无障碍教学策略的潜力，提出了基于ADA Title II的六个设计原则，并通过模拟场景展示了如何通过叙事驱动增强数据理解、参与度和教育公平。


<details>
  <summary>Details</summary>
Motivation: 当前无障碍教学在计算机科学领域已有广泛研究，但在数据素养等其他学科中的整合仍有限。本文旨在探索数据叙事作为使复杂信息对多样化学习者可访问的策略，以满足ADA Title II的要求。

Method: 提出了基于ADA Title II核心义务的六个设计原则，并通过模拟场景展示了这些原则在实际教育环境中的应用操作化过程。

Result: 研究表明，叙事驱动的数据呈现可以增强学习者的理解能力、参与度，并在不同教育环境中实现更公平的访问。

Conclusion: 数据叙事是一种有效的无障碍教学策略，能够帮助教育者在包容性学习环境中更好地应用数据可视化，促进教育公平和可访问性。

Abstract: Accessible teaching has been extensively investigated in computer science,
yet its integration into other disciplines, such as data literacy, remains
limited. This paper examines the potential of data storytelling, defined as the
integration of data, visualizations, and narrative, as a possible strategy for
making complex information accessible to diverse learners in compliance with
Title II of the Americans with Disabilities Act (ADA). We propose six design
principles, derived from Title II's core obligations, to guide educators in
applying data storytelling within inclusive learning environments. A simulated
scenario shows the operationalization of these principles, illustrating how
narrative-driven data presentation can enhance comprehension, engagement, and
equitable access across different educational contexts.

</details>


### [188] [The Psychogeography of Imaginary Places](https://arxiv.org/abs/2511.04105)
*Michael Heron,Pauline Belford,Klara Aune*

Main category: cs.CY

TL;DR: 将心理地理学从物理空间扩展到虚拟和虚构空间，探讨电子游戏世界中的漂移实践，将游戏视为心理地理学的新实验室和景观。


<details>
  <summary>Details</summary>
Motivation: 传统心理地理学主要关注物理环境对情感和行为的影响，但心理地理学的哲学核心是识别个人与环境之间的情感关系，这不一定需要物理空间的限制。

Method: 借鉴文学、情境主义和当代心理地理学传统，分析漂移实践如何在电子游戏世界的弹性空间和时间性中运作，探索数字环境如何促进新的意义建构和自我反思。

Result: 数字环境作为完全构建的空间，为心理地理学提供了新的实践场域，游戏成为研究情感与环境关系的实验室和景观。

Conclusion: 通过将心理地理学扩展到虚拟空间，游戏不仅成为研究街道和城市精神的工具，也成为探索代码、像素和游戏中幽灵般存在的新领域，为心理地理学注入了新的活力。

Abstract: Psychogeography -- the study of how environments shape emotion and behaviour
-- has long concerned itself with the emotional resonance of the physical,
often through the idea of the derive through the city. Its philosophical core,
however, is primarily concerned with identifying affective relationships
between the personal and the environmental, and this does not require the
constraint of concrete.
  This paper extends psychogeographical practice into the realm of the
imaginary, proposing a psychogeography of virtual and fictive spaces. Drawing
on literary, Situationist, and contemporary psychogeographical traditions, we
examine how the derive might operate within the elastic spatiality and
temporalities of video game worlds. We argue that digital environments, being
wholly constructed, invite new forms of meaning-making and self-reflection.
Through this reframing, games become both laboratory and landscape for a
revitalised psychogeography: one attuned not only to the spirits of streets and
cities, but also to the ghosts that haunt code, pixels, and play.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [189] [On excitation of control-affine systems and its use for data-driven Koopman approximants](https://arxiv.org/abs/2511.03734)
*Philipp Schmitz,Lea Bold,Friedrich M. Philipp,Mario Rosenfelder,Peter Eberhard,Henrik Ebel,Karl Worthmann*

Main category: eess.SY

TL;DR: 提出一个框架来提高控制仿射系统的数据拟合鲁棒性，通过基于子空间角的输入选择确保最小奇异值达到期望阈值，并推导了最大化最小奇异值的最优性条件。


<details>
  <summary>Details</summary>
Motivation: 控制仿射系统的扩展动态模态分解(EDMD)方法在生成双线性代理模型时需要大量数据，应用复杂，需要提高系统辨识问题的鲁棒性。

Method: 基于子空间角的输入选择框架，确保最小奇异值达到期望阈值，推导最大化最小奇异值的最优性条件。

Result: 提出的方法提高了双线性EDMD方案的可靠性，并在非完整机器人控制中验证了有效性。

Conclusion: 该框架为控制仿射系统的数据驱动建模提供了更可靠的辨识方法，通过优化输入选择提高了双线性EDMD的鲁棒性。

Abstract: The Koopman operator and extended dynamic mode decomposition (EDMD) as a
data-driven technique for its approximation have attracted considerable
attention as a key tool for modeling, analysis, and control of complex
dynamical systems. However, extensions towards control-affine systems resulting
in bilinear surrogate models are prone to demanding data requirements rendering
their applicability intricate. In this paper, we propose a framework for
data-fitting of control-affine mappings to increase the robustness margin in
the associated system identification problem and, thus, to provide more
reliable bilinear EDMD schemes. In particular, guidelines for input selection
based on subspace angles are deduced such that a desired threshold with respect
to the minimal singular value is ensured. Moreover, we derive necessary and
sufficient conditions of optimality for maximizing the minimal singular value.
Further, we demonstrate the usefulness of the proposed approach using bilinear
EDMD with control for non-holonomic robots.

</details>


### [190] [Hybrid ILM-NILM Smart Plug System](https://arxiv.org/abs/2511.03737)
*Dániel István Németh,Kálmán Tornai*

Main category: eess.SY

TL;DR: 提出了一种混合负载分类方法，通过智能插座连接多个负载来降低安装成本，同时保持一定的控制粒度。


<details>
  <summary>Details</summary>
Motivation: 传统侵入式和非侵入式负载分类方法各有优缺点，很少有研究尝试结合这两种方法。智能插座连接多个负载在家庭中很常见但文献中很少考虑。

Method: 扩展智能插座解决方案，使每个插座可以连接多个负载，通过混合负载分类方法处理这种情况。

Result: 能够降低系统安装成本，同时减少控制粒度，实现了成本与控制的平衡。

Conclusion: 提出的混合负载分类方案有效解决了智能插座连接多个负载的场景，在降低安装成本的同时保持了可接受的控制能力。

Abstract: Electrical load classification is generally divided into intrusive and
non-intrusive approaches, both having their limitations and advantages. With
the non-intrusive approach, controlling appliances is not possible, but the
installation cost of a single measurement device is cheap. In comparison,
intrusive, smart plug-based solutions offer individual appliance control, but
the installation cost is much higher. There have been very few approaches
aiming to combine these methods. In this paper we show that extending a smart
plug-based solution to multiple loads per plug can reduce control granularity
in favor of lowering the system's installation costs. Connecting various loads
to a Smart Plug through an extension cord is seldom considered in the
literature, even though it is common in households. This scenario is also
handled by the hybrid load classification solution presented in this paper.

</details>


### [191] [Kalman-Bucy Filtering with Randomized Sensing: Fundamental Limits and Sensor Network Design for Field Estimation](https://arxiv.org/abs/2511.03740)
*Xinyi Wang,Devansh R. Agrawal,Dimitra Panagou*

Main category: eess.SY

TL;DR: 本文在连续时间框架下分析了随机丢失测量时的卡尔曼滤波器稳定性，推导了估计协方差的上界，并将其应用于时空场估计，建立了与网格无关的清晰度下界，揭示了传感器网络设计的性能极限。


<details>
  <summary>Details</summary>
Motivation: 研究随机丢失测量情况下卡尔曼滤波器的稳定性分析问题，特别是在连续时间框架下，考虑测量矩阵和噪声协方差作为随机过程变化的情况，以捕捉传感位置的可变性。

Method: 在连续时间框架下推导卡尔曼滤波的期望估计协方差闭式上界，应用于时空场估计，使用清晰度（重标度的微分熵）建立空间平均期望清晰度的网格无关下界。

Result: 得到了一个复合传感参数，联合捕捉传感器数量、噪声水平和测量频率的影响，仿真验证了该边界对离散时间卡尔曼滤波器是紧的，在测量率降低时趋近该边界。

Conclusion: 推导的极限为传感器网络设计问题提供了原则性和高效的指导，可在部署前进行网络设计优化。

Abstract: Stability analysis of the Kalman filter under randomly lost measurements has
been widely studied. We revisit this problem in a general continuous-time
framework, where both the measurement matrix and noise covariance evolve as
random processes, capturing variability in sensing locations. Within this
setting, we derive a closed-form upper bound on the expected estimation
covariance for continuous-time Kalman filtering. We then apply this framework
to spatiotemporal field estimation, where the field is modeled as a Gaussian
process observed by randomly located, noisy sensors. Using clarity, introduced
in our earlier work as a rescaled form of the differential entropy of a random
variable, we establish a grid-independent lower bound on the spatially averaged
expected clarity. This result exposes fundamental performance limits through a
composite sensing parameter that jointly captures the effects of the number of
sensors, noise level, and measurement frequency. Simulations confirm that the
proposed bound is tight for the discrete-time Kalman filter, approaching it as
the measurement rate decreases, while avoiding the recursive computations
required in the discrete-time formulation. Most importantly, the derived limits
provide principled and efficient guidelines for sensor network design problem
prior to deployment.

</details>


### [192] [Electric Vehicle Charging Load Modeling: A Survey, Trends, Challenges and Opportunities](https://arxiv.org/abs/2511.03741)
*Xiachong Lin,Arian Prabowo,Imran Razzak,Hao Xue,Matthew Amos,Sam Behrens,Flora D. Salim*

Main category: eess.SY

TL;DR: 本文系统综述了过去五年电动汽车充电负荷模型，将建模方法分为统计、模拟和数据驱动三类，分析了信息融合的三个底层操作，并讨论了该领域的挑战与机遇。


<details>
  <summary>Details</summary>
Motivation: 电动汽车充电行为预测对基础设施规划至关重要，但现有充电负荷受不确定性和随机性影响，且缺乏对信息融合建模方法的系统分析。

Method: 对过去五年EV充电负荷模型进行全面综述，将建模方法分为统计方法、模拟方法和数据驱动方法三类，并分析信息融合的三个底层操作。

Result: 系统梳理了各类建模方法的优缺点，明确了信息融合在充电负荷建模中的关键作用，为未来研究提供了分类框架。

Conclusion: 总结了该领域面临的挑战和发展机遇，为未来研究提供了指导方向，推动对电动汽车充电行为的深入理解和实践探索。

Abstract: The evolution of electric vehicles (EVs) is reshaping the automotive
industry, advocating for more sustainable transportation practices. Accurately
predicting EV charging behavior is essential for effective infrastructure
planning and optimization. However, the charging load of EVs is significantly
influenced by uncertainties and randomness, posing challenges for accurate
estimation. Furthermore, existing literature reviews lack a systematic analysis
of modeling approaches focused on information fusion. This paper
comprehensively reviews EV charging load models from the past five years. We
categorize state-of-the-art modeling methods into statistical, simulated, and
data-driven approaches, examining the advantages and drawbacks of each.
Additionally, we analyze the three bottom-up level operations of information
fusion in existing models. We conclude by discussing the challenges and
opportunities in the field, offering guidance for future research endeavors to
advance our understanding and explore practical research directions.

</details>


### [193] [A Model-Based Approach to Automated Digital Twin Generation in Manufacturing](https://arxiv.org/abs/2511.03742)
*Angelos Alexopoulos,Agorakis Bompotas,Nikitas Rigas Kalogeropoulos,Panagiotis Kechagias,Athanasios P. Kalogeras,Christos Alexakos*

Main category: eess.SY

TL;DR: 提出一个自动化数字孪生生成和部署平台，使用AutomationML工厂规划，结合GAI驱动的仿真场景生成和自动物理产线重配置，提升制造效率和适应性。


<details>
  <summary>Details</summary>
Motivation: 现代制造需要高灵活性和可重构性来适应动态生产需求，基于模型的工程支持快速产线设计，但最终重配置需要仿真和验证。

Method: 开发一个自动化数字孪生平台，使用AutomationML工厂规划，集成GAI驱动的仿真场景生成器，实现自动物理产线重配置。

Result: 该平台通过实时监控、仿真和重配置简化了制造过程，提高了效率和适应性。

Conclusion: 提出的平台通过自动化数字孪生生成和部署，结合GAI仿真和自动重配置，显著提升了制造系统的灵活性和效率。

Abstract: Modern manufacturing demands high flexibility and reconfigurability to adapt
to dynamic production needs. Model-based Engineering (MBE) supports rapid
production line design, but final reconfiguration requires simulations and
validation. Digital Twins (DTs) streamline this process by enabling real-time
monitoring, simulation, and reconfiguration. This paper presents a novel
platform that automates DT generation and deployment using AutomationML-based
factory plans. The platform closes the loop with a GAI-powered simulation
scenario generator and automatic physical line reconfiguration, enhancing
efficiency and adaptability in manufacturing.

</details>


### [194] [A convolutional neural network deep learning method for model class selection](https://arxiv.org/abs/2511.03743)
*Marios Impraimakis*

Main category: eess.SY

TL;DR: 提出一种基于一维卷积神经网络的模型类别选择方法，仅使用系统响应信号即可识别动态系统的模型类别，无需输入信息或完整系统辨识。


<details>
  <summary>Details</summary>
Motivation: 传统模型类别选择方法通常需要系统输入信息或完整系统辨识，限制了在结构健康监测等实际应用中的使用。

Method: 使用单自由度响应信号及其类别信息训练一维卷积神经网络，可选地结合卡尔曼滤波融合加速度和位移数据。

Result: 该方法能够在线性和非线性动态系统以及3D建筑有限元模型中，准确识别由阻尼行为或迟滞行为引起的微小信号变化对应的模型类别。

Conclusion: 该方法为结构健康监测应用提供了一个强大的工具，能够仅基于响应信号实现模型类别选择。

Abstract: The response-only model class selection capability of a novel deep
convolutional neural network method is examined herein in a simple, yet
effective, manner. Specifically, the responses from a unique degree of freedom
along with their class information train and validate a one-dimensional
convolutional neural network. In doing so, the network selects the model class
of new and unlabeled signals without the need of the system input information,
or full system identification. An optional physics-based algorithm enhancement
is also examined using the Kalman filter to fuse the system response signals
using the kinematics constraints of the acceleration and displacement data.
Importantly, the method is shown to select the model class in slight signal
variations attributed to the damping behavior or hysteresis behavior on both
linear and nonlinear dynamic systems, as well as on a 3D building finite
element model, providing a powerful tool for structural health monitoring
applications.

</details>


### [195] [Predictive Compensation in Finite-Horizon LQ Games under Gauss-Markov Deviations](https://arxiv.org/abs/2511.03744)
*Navid Mojahed,Mahdis Rabbani,Shima Nazari*

Main category: eess.SY

TL;DR: 提出了一个预测补偿框架，用于处理存在高斯-马尔可夫偏差的有限时域离散时间线性二次动态博弈，其中一个玩家经历相关随机偏差，另一个玩家使用预测策略进行补偿。


<details>
  <summary>Details</summary>
Motivation: 在动态博弈中，玩家可能会偏离反馈纳什策略，这种偏差可能具有相关性，影响博弈性能。需要开发能够预测并补偿这种相关偏差的框架。

Method: 使用一阶自回归过程建模相关随机偏差，推导均值和协方差传播的闭式递推公式，分析预期成本敏感性来评估性能改进。

Result: 开发了预测补偿框架，能够有效处理相关偏差，通过闭式递推公式实现均值和协方差的传播计算。

Conclusion: 提出的预测补偿框架能够有效应对动态博弈中的相关偏差问题，通过预测未来相关性实现性能改进。

Abstract: This paper presents a predictive compensation framework for finite-horizon
discrete-time linear quadratic dynamic games in the presence of Gauss-Markov
deviations from feedback Nash strategies. One player experiences correlated
stochastic deviations, modeled via a first-order autoregressive process, while
the other compensates using a predictive strategy that anticipates the effect
of future correlation. Closed-form recursions for mean and covariance
propagation are derived, and the resulting performance improvement is analyzed
through the sensitivity of expected cost.

</details>


### [196] [InvSim algorithm for pre-computing airplane flight controls in limited-range autonomous missions, and demonstration via double-roll maneuver of Mirage III fighters](https://arxiv.org/abs/2511.03745)
*Osama A. Marzouk*

Main category: eess.SY

TL;DR: 该论文提出了一个用于固定翼飞机6自由度飞行动力学的逆仿真数学框架，通过指定目标飞行轨迹来预测实现该机动所需的飞行控制变量。


<details>
  <summary>Details</summary>
Motivation: 开发一个通用的逆仿真框架，能够根据给定的飞行轨迹预测必要的控制输入，这在飞行控制设计和轨迹规划中具有重要应用价值。

Method: 建立6自由度飞行动力学数学模型，包含多种坐标系和角度定义，然后推导逆仿真方程，结合符号数学、RK4数值积分和有限差分法来求解控制变量。

Result: 开发了一个数值程序，能够计算发动机推力、副翼、升降舵和方向舵的偏转角等四个控制变量，使飞机能够实现由惯性笛卡尔坐标和欧拉滚转角指定的期望飞行轨迹。

Conclusion: 提出的逆仿真方法能够有效预测实现特定飞行机动所需的控制输入，为飞行控制系统的设计和验证提供了有力工具。

Abstract: In this work, we start with a generic mathematical framework for the
equations of motion (EOM) in flight mechanics with six degrees of freedom
(6-DOF) for a general (not necessarily symmetric) fixed-wing aircraft. This
mathematical framework incorporates (1) body axes (fixed in the airplane at its
center of gravity), (2) inertial axes (fixed in the earth/ground at the
take-off point), wind axes (aligned with the flight path/course), (3) spherical
flight path angles (azimuth angle measured clockwise from the geographic north,
and elevation angle measured above the horizon plane), and (4) spherical flight
angles (angle of attack and sideslip angle). We then manipulate these equations
of motion to derive a customized version suitable for inverse simulation flight
mechanics, where a target flight trajectory is specified while a set of
corresponding necessary flight controls to achieve that maneuver are predicted.
We then present a numerical procedure for integrating the developed inverse
simulation (InvSim) system in time; utilizing (1) symbolic mathematics, (2)
explicit fourth-order Runge-Kutta (RK4) numerical integration technique, and
(3) expressions based on the finite difference method (FDM); such that the four
necessary control variables (engine thrust force, ailerons' deflection angle,
elevators' deflection angle, and rudder's deflection angle) are computed as
discrete values over the entire maneuver time, and these calculated control
values enable the airplane to achieve the desired flight trajectory, which is
specified by three inertial Cartesian coordinates of the airplane, in addition
to the Euler's roll angle. We finally demonstrate the proposed numerical
procedure of flight mechanics inverse simulation (InvSim).

</details>


### [197] [A Dynamic Recurrent Adjacency Memory Network for Mixed-Generation Power System Stability Forecasting](https://arxiv.org/abs/2511.03746)
*Guang An Ooi,Otavio Bertozzi,Mohd Asim Aftab,Charalambos Konstantinou,Shehab Ahmed*

Main category: eess.SY

TL;DR: 提出了一种动态循环邻接记忆网络(DRAMN)，结合物理信息分析和深度学习，用于实时电力系统稳定性预测，在多个测试系统中达到99%以上的准确率。


<details>
  <summary>Details</summary>
Motivation: 现代电力系统中基于逆变器的资源高渗透率带来了复杂的动态行为，传统稳定性评估方法在可扩展性和泛化性方面面临挑战。

Method: 使用滑动窗口动态模式分解构建时变多层邻接矩阵，将图卷积操作直接集成到循环门控机制中，同时建模演化动态和时间依赖性。

Result: 在改进的IEEE 9总线、39总线和多端HVDC网络上验证，分别达到99.85%、99.90%和99.69%的平均准确率，优于所有基准模型，特征维度减少82%且性能无损失。

Conclusion: DRAMN实现了最先进的准确性，同时为电力系统操作员提供了增强的可解释性，适合在现代控制中心实时部署。

Abstract: Modern power systems with high penetration of inverter-based resources
exhibit complex dynamic behaviors that challenge the scalability and
generalizability of traditional stability assessment methods. This paper
presents a dynamic recurrent adjacency memory network (DRAMN) that combines
physics-informed analysis with deep learning for real-time power system
stability forecasting. The framework employs sliding-window dynamic mode
decomposition to construct time-varying, multi-layer adjacency matrices from
phasor measurement unit and sensor data to capture system dynamics such as
modal participation factors, coupling strengths, phase relationships, and
spectral energy distributions. As opposed to processing spatial and temporal
dependencies separately, DRAMN integrates graph convolution operations directly
within recurrent gating mechanisms, enabling simultaneous modeling of evolving
dynamics and temporal dependencies. Extensive validations on modified IEEE
9-bus, 39-bus, and a multi-terminal HVDC network demonstrate high performance,
achieving 99.85\%, 99.90\%, and 99.69\% average accuracies, respectively,
surpassing all tested benchmarks, including classical machine learning
algorithms and recent graph-based models. The framework identifies optimal
combinations of measurements that reduce feature dimensionality by 82\% without
performance degradation. Correlation analysis between dominant measurements for
small-signal and transient stability events validates generalizability across
different stability phenomena. DRAMN achieves state-of-the-art accuracy while
providing enhanced interpretability for power system operators, making it
suitable for real-time deployment in modern control centers.

</details>


### [198] [Analytical modelling of a stop-less modular bus service with an application to charging strategies comparison](https://arxiv.org/abs/2511.03754)
*Haoran Zhao,Neema Nassir,Andres Fielbaum*

Main category: eess.SY

TL;DR: 本文开发了集成V2V充电技术的SLAM公交服务优化模型，分析了不同乘客需求下的运营阶段演进，包括从空闲容量到全小巴、全大巴，以及频率受限和能量受限等阶段。


<details>
  <summary>Details</summary>
Motivation: 传统公交服务存在效率低下问题，特别是停靠时间长增加车内乘客旅行时间。SLAM公交服务通过动态容量减少停靠时间，同时公交电动化带来充电需求的新约束。

Method: 开发了集成V2V充电技术的SLAM公交服务分析优化模型，比较了无充电情况和不同充电策略下的最优设计和可行性。

Result: 识别了随乘客量增长的运营阶段序列：低需求下的空闲容量、全小巴、全大巴、频率受限阶段。在移动充电策略下，还包括频率下降的能量受限阶段和最终高需求下的不可行性。

Conclusion: 这些发现使运营商能够提供更高效的服务，为SLAM公交服务的运营规划提供了理论指导。

Abstract: Buses are a vital component of metropolitan public transport, yet
conventional bus services often struggle with inefficiencies including extended
dwelling time, which increases in-vehicle travel time for non-alighting
passengers. A stop-less autonomous modular (SLAM) bus service has emerged as a
solution, enabling dynamic capacity to reduce dwelling time. Meanwhile, the
electrification of buses is advancing as a strategy to mitigate greenhouse gas
emissions and reduces operators' costs, but introduces new operational
constraints due to charging requirements. This study develops analytical
optimization models for SLAM bus service that integrates vehicle-to-vehicle
(V2V) charging technology. By comparing the optimal designs and their
feasibility across non-charging case and charging strategies, we identify a
sequence of operational stages as ridership grows: from idle capacity under low
demand, to full small buses, full large buses, and a proposed frequency-capped
regime where only bus capacity expands. Under the mobile charging strategy,
this progression further includes an energy-limited regime, in which frequency
declines, and ultimately infeasibility under high demand. These findings enable
operators to deliver more efficient services.

</details>


### [199] [Removing Time-Scale Separation in Feedback-Based Optimization via Estimators](https://arxiv.org/abs/2511.03903)
*Niloufar Yousefi,John W. Simpson-Porco*

Main category: eess.SY

TL;DR: 提出了一种基于估计器的反馈优化改进方法，利用动态系统模型信息消除传统反馈优化的时间尺度分离要求，显著提升闭环系统性能。


<details>
  <summary>Details</summary>
Motivation: 传统反馈优化需要控制器在比系统慢得多的时间尺度上运行以保证闭环稳定性，这严重限制了可实现的闭环性能。

Method: 基于估计器的反馈优化改进方法，利用动态系统模型信息来消除时间尺度分离要求。

Result: 改进后的方法使闭环系统收敛速率仅受限于开环系统的主导特征值，显著提升了性能。

Conclusion: 该方法成功消除了传统反馈优化的时间尺度分离限制，在电力系统频率控制等应用中表现出优越性能。

Abstract: Feedback-based optimization (FBO) provides a simple control framework for
regulating a stable dynamical system to the solution of a constrained
optimization problem in the presence of exogenous disturbances, and does so
without full knowledge of the plant dynamics. However, closed-loop stability
requires the controller to operate on a sufficiently slower timescale than the
plant, significantly constraining achievable closed-loop performance. Motivated
by this trade-off, we propose an estimator-based modification of FBO which
leverages dynamic plant model information to eliminate the time-scale
separation requirement of traditional FBO. Under this design, the convergence
rate of the closed-loop system is limited only by the dominant eigenvalue of
the open-loop system. We extend the approach to the case of design based on
only an approximate plant model when the original system is singularly
perturbed. The results are illustrated via an application to fast power system
frequency control using inverter-based resources.

</details>


### [200] [A Co-simulation Framework for Quadrotor Control System Design using ROS 2 and MATLAB/Simulink](https://arxiv.org/abs/2511.03969)
*Hangyu Teng*

Main category: eess.SY

TL;DR: 提出了一种集成ROS 2和MATLAB/Simulink的四旋翼无人机控制系统联合仿真框架，采用分层控制架构（LQR姿态控制+PID位置控制），实现了高效的算法原型设计和软件在环验证。


<details>
  <summary>Details</summary>
Motivation: 联合仿真是复杂信息物理系统设计和分析的关键方法，能够提高开发效率并降低成本。需要为四旋翼无人机控制系统提供一个高效的标准化解决方案。

Method: 1. 基于牛顿-欧拉方程建立四旋翼六自由度非线性动力学模型；2. 设计分层控制架构：LQR控制器用于姿态控制以获得最优调节性能，PID控制器用于位置控制以确保鲁棒性和实用性；3. 构建跨平台数据交换机制的框架架构。

Result: 仿真结果验证了该框架的有效性，证明其能够为无人机控制算法提供高效的快速原型设计和软件在环验证能力。

Conclusion: 该联合仿真框架成功集成了ROS 2和MATLAB/Simulink，为四旋翼无人机控制系统的设计和验证提供了一个高效、标准化的解决方案，特别适用于快速原型开发和软件在环验证。

Abstract: Co-simulation is a critical approach for the design and analysis of complex
cyber-physical systems. It will enhance development efficiency and reduce
costs. This paper presents a co-simulation framework integrating ROS 2 and
MATLAB/Simulink for quadrotor unmanned aerial vehicle (UAV) control system
design and verification. First, a six-degree-of-freedom nonlinear dynamic model
of the quadrotor is derived accurately that based on Newton-Euler equations.
Second, within the proposed framework, a hierarchical control architecture was
designed and implemented: LQR controller for attitude control to achieve
optimal regulation performance, and PID controller for position control to
ensure robustness and practical applicability. Third, elaborated the
architecture of the framework, including the implementation details of the
cross-platform data exchange mechanism. Simulation results demonstrate the
effectiveness of the framework, highlighting its capability to provide an
efficient and standardized solution for rapid prototyping and
Software-in-the-Loop (SIL) validation of UAV control algorithms.

</details>


### [201] [Necessary and Sufficient Conditions for the Optimization-Based Concurrent Execution of Learned Robotic Tasks](https://arxiv.org/abs/2511.04054)
*Sheikh A. Tahmid,Gennaro Notomista*

Main category: eess.SY

TL;DR: 本文提出了关于多任务强化学习价值函数并发执行的理论条件，扩展了基于优化的框架以支持带折扣因子的价值函数。


<details>
  <summary>Details</summary>
Motivation: 先前工作开发了多任务执行框架，但未解决何时可以并发执行学习到的价值函数这一基本问题。

Method: 使用定理形式给出了在状态空间子集中并发执行学习任务的必要和充分条件，基于先前提出的最小范数控制器。

Result: 提供了关于何时学习到的控制任务可以并发执行、何时已经固有地可并发执行、以及何时不可能并发执行的理论洞察。

Conclusion: 扩展了基于优化的多任务执行框架，使其与标准强化学习实践更兼容，特别是支持带折扣因子的价值函数。

Abstract: In this work, we consider the problem of executing multiple tasks encoded by
value functions, each learned through Reinforcement Learning, using an
optimization-based framework. Prior works develop such a framework, but left
unanswered a fundamental question of when learned value functions can be
concurrently executed. The main contribution of this work is to present
theorems which provide necessary and sufficient conditions to concurrently
execute sets of learned tasks within subsets of the state space, using a
previously proposed min-norm controller. These theorems provide insight into
when learned control tasks are possible to be made concurrently executable,
when they might already inherently be concurrently executable and when it is
not possible at all to make a set of learned tasks concurrently executable
using the previously proposed methods. Additional contributions of this work
include extending the optimization-based framework to execute multiple tasks
encoded by value functions to also account for value functions trained with a
discount factor, making the overall framework more compatible with standard RL
practices.

</details>


### [202] [Differential Flatness of Quasi-Static Slider-Pusher Models with Applications in Control](https://arxiv.org/abs/2511.04246)
*Sander De Witte,Tom Lefebvre,Thomas Neve,Andras Retzler,Guillaume Crevecoeur*

Main category: eess.SY

TL;DR: 本文研究平面滑块-推杆系统的动态特性，建立了准静态假设下的微分运动学模型，发现多边形滑块和圆形推杆系统具有微分平坦性，并提出了两种轨迹跟踪控制策略。


<details>
  <summary>Details</summary>
Motivation: 研究滑块-推杆系统作为机械臂操作任务中的运动基元，旨在开发有效的控制方法。

Method: 基于极限表面方法构建微分运动学模型，分析系统的微分平坦性，提出级联准静态反馈和动态反馈线性化两种控制策略。

Result: 通过闭环仿真和物理实验验证了控制策略的有效性，实验结果表明仿真增益在真实系统中具有适用性。

Conclusion: 所提出的方法在滑块-推杆系统控制中具有应用潜力，特别是利用微分平坦性简化了控制综合和规划问题。

Abstract: This paper investigates the dynamic properties of planar slider-pusher
systems as a motion primitive in manipulation tasks. To that end, we construct
a differential kinematic model deriving from the limit surface approach under
the quasi-static assumption and with negligible contact friction. The
quasi-static model applies to generic slider shapes and circular pusher
geometries, enabling a differential kinematic representation of the system.
From this model, we analyze differential flatness - a property advantageous for
control synthesis and planning - and find that slider-pusher systems with
polygon sliders and circular pushers exhibit flatness with the centre of mass
as a flat output. Leveraging this property, we propose two control strategies
for trajectory tracking: a cascaded quasi-static feedback strategy and a
dynamic feedback linearization approach. We validate these strategies through
closed-loop simulations incorporating perturbed models and input noise, as well
as experimental results using a physical setup with a finger-like pusher and
vision-based state detection. The real-world experiments confirm the
applicability of the simulation gains, highlighting the potential of the
proposed methods for

</details>


### [203] [ComEMS4Build: Comfort-Oriented Energy Management System for Residential Buildings using Hydrogen for Seasonal Storage](https://arxiv.org/abs/2511.04293)
*Jovana Kovačević,Felix Langner,Erfan Tajalli-Ardekani,Marvin Dorn,Simon Waczowicz,Ralf Mikut,Jörg Matthes,Hüseyin K. Çakmak,Veit Hagenmeyer*

Main category: eess.SY

TL;DR: 开发了基于模糊逻辑的舒适导向能源管理系统(ComEMS4Build)，用于集成光伏、电池和氢存储的住宅建筑，在冬季12周测试中表现出良好的热舒适性和成本效益。


<details>
  <summary>Details</summary>
Motivation: 将柔性负荷和存储系统集成到住宅领域有助于匹配波动性可再生能源发电与需求，氢存储系统可实现季节性能量转移，但初始成本高，通过耦合燃料电池和热泵可减小氢系统规模。

Method: 开发了基于模糊逻辑的ComEMS4Build系统，包含光伏、电池储能和氢存储，燃料电池和热泵作为互补技术。采用半合成建模方法在德国家庭住宅进行12周冬季评估，与基于规则的控制和模型预测控制进行比较。

Result: ComEMS4Build在12周中有10周未违反热舒适性，与MPC相当，而RBC的中位不适感为0.68Kh。ComEMS4Build周电费比MPC高12.06欧元，比RBC低18.08欧元。ComEMS4Build提高了混合储能系统利用率和与主网的能量交换，但RBC在燃料电池操作方面有优势。

Conclusion: ComEMS4Build在保持热舒适性的同时实现了合理的成本控制，在混合储能系统利用方面优于基于规则的控制，但在燃料电池操作优化方面还有改进空间。

Abstract: Integrating flexible loads and storage systems into the residential sector
contributes to the alignment of volatile renewable generation with demand.
Besides batteries serving as a short-term storage solution, residential
buildings can benefit from a Hydrogen (H2) storage system, allowing seasonal
shifting of renewable energy. However, as the initial costs of H2 systems are
high, coupling a Fuel Cell (FC) with a Heat Pump (HP) can contribute to the
size reduction of the H2 system. The present study develops a Comfort-Oriented
Energy Management System for Residential Buildings (ComEMS4Build) comprising
Photovoltaics (PV), Battery Energy Storage System (BESS), and H2 storage, where
FC and HP are envisioned as complementary technologies. The fuzzy-logic-based
ComEMS4Build is designed and evaluated over a period of 12 weeks in winter for
a family household building in Germany using a semi-synthetic modeling
approach. The Rule-Based Control (RBC), which serves as a lower benchmark, is a
scheduler designed to require minimal inputs for operation. The Model
Predictive Control (MPC) is intended as a cost-optimal benchmark with an ideal
forecast. The results show that ComEMS4Build, similar to MPC, does not violate
the thermal comfort of occupants in 10 out of 12 weeks, while RBC has a
slightly higher median discomfort of 0.68 Kh. The ComEMS4Build increases the
weekly electricity costs by 12.06 EUR compared to MPC, while RBC increases the
weekly costs by 30.14 EUR. The ComEMS4Build improves the Hybrid Energy Storage
System (HESS) utilization and energy exchange with the main grid compared to
the RBC. However, when it comes to the FC operation, the RBC has an advantage,
as it reduces the toggling counts by 3.48% and working hours by 7.59% compared
to MPC...

</details>


### [204] [Data-Driven Modeling of Photosynthesis Regulation Under Oscillating Light Condition - Part I: In-Silico Exploration](https://arxiv.org/abs/2511.04330)
*Christian Portilla,Arviandy G Aribowo,Ramachandran Anantharaman,César A Gómez-Pérez,Leyla Özkan*

Main category: eess.SY

TL;DR: 该论文应用频域数据驱动系统辨识技术，在振荡光照条件下建立简化的光合作用调控控制导向模型，通过Basic DREAM Model生成仿真数据，使用BLA方法估计二阶LTI传递函数，并构建基于光照强度DC值的LPV表示。


<details>
  <summary>Details</summary>
Motivation: 研究光合作用调控在振荡光照条件下的简化建模方法，为控制系统设计提供实用的控制导向模型。

Method: 使用Basic DREAM Model生成仿真数据，输入为包含DC和AC分量的光照强度信号，输出为叶绿素荧光；应用BLA方法估计二阶LTI传递函数；基于局部模型构建以光照强度DC值为调度参数的LPV状态空间表示。

Result: 成功建立了光合作用调控系统的简化LTI传递函数模型和LPV状态空间表示，能够描述不同操作条件下的系统动态特性。

Conclusion: 数据驱动的频域系统辨识方法可以有效建立光合作用调控的简化控制导向模型，LPV表示提供了紧凑的系统动态描述，为控制系统设计奠定了基础。

Abstract: This paper explores the application of data-driven system identification
techniques in the frequency domain to obtain simplified, control-oriented
models of photosynthesis regulation under oscillating light conditions.
In-silico datasets are generated using simulations of the physics-based Basic
DREAM Model (BDM) Funete et al.[2024], with light intensity signals --
comprising DC (static) and AC (modulated) components as input and chlorophyll
fluorescence (ChlF) as output. Using these data, the Best Linear Approximation
(BLA) method is employed to estimate second-order linear time-invariant (LTI)
transfer function models across different operating conditions defined by DC
levels and modulation frequencies of light intensity. Building on these local
models, a Linear Parameter-Varying (LPV) representation is constructed, in
which the scheduling parameter is defined by the DC values of the light
intensity, providing a compact state-space representation of the system
dynamics.

</details>


### [205] [Overview and Performance Evaluation of Supervisory Controller Synthesis with Eclipse ESCET v4.0](https://arxiv.org/abs/2511.04370)
*Dennis Hendriks,Michel Reniers,Wan Fokkink,Wytse Oortwijn*

Main category: eess.SY

TL;DR: 本文介绍了ESCET开源项目中CIF建模语言的符号化监督控制器综合算法，包括防止运行时错误、处理不同类型需求和输入变量等实用方面。提出了包含23个工业与学术模型的基准测试集，评估了从v0.8到v4.0版本的性能改进，并探讨了多级综合方法的性能增益。


<details>
  <summary>Details</summary>
Motivation: 监督控制器对确保信息物理系统的正确安全运行至关重要。基于综合的工程方法旨在自动化监督控制器的设计和实现过程，让工程师专注于系统需求而非实现细节。

Method: 使用CIF建模语言和符号化监督控制器综合算法，包含防止运行时错误、处理不同需求类型和支持输入变量等实用功能。建立了包含23个工业与学术模型的基准测试集，评估了ESCET工具从v0.8到v4.0版本的性能改进，并研究了多级综合方法。

Result: 通过基准测试展示了ESCET工具在监督控制器综合性能方面的显著提升，多级综合方法进一步改善了性能，但对于复杂模型的综合仍需更多性能改进。

Conclusion: ESCET项目为监督控制器综合提供了实用的工具和方法，性能持续改进但仍需进一步提升以处理更复杂的模型。多级综合是有前景的方向，但需要更多研究来充分发挥其潜力。

Abstract: Supervisory controllers control cyber-physical systems to ensure their
correct and safe operation. Synthesis-based engineering (SBE) is an approach to
largely automate their design and implementation. SBE combines model-based
engineering with computer-aided design, allowing engineers to focus on 'what'
the system should do (the requirements) rather than 'how' it should do it
(design and implementation). In the Eclipse Supervisory Control Engineering
Toolkit (ESCET) open-source project, a community of users, researchers, and
tool vendors jointly develop a toolkit to support the entire SBE process,
particularly through the CIF modeling language and tools. In this paper, we
first provide a description of CIF's symbolic supervisory controller synthesis
algorithm, and thereby include aspects that are often omitted in the
literature, but are of great practical relevance, such as the prevention of
runtime errors, handling different types of requirements, and supporting input
variables (to connect to external inputs). Secondly, we introduce and describe
CIF's benchmark models, a collection of 23 freely available industrial and
academic models of various sizes and complexities. Thirdly, we describe recent
improvements between ESCET versions v0.8 (December 2022) and v4.0 (June 2024)
that affect synthesis performance, evaluate them on our benchmark models, and
show the current practical synthesis performance of CIF. Fourthly, we briefly
look at multi-level synthesis, a non-monolithic synthesis approach, evaluate
its gains, and show that while it can help to further improve synthesis
performance, further performance improvements are still needed to synthesize
complex models.

</details>


### [206] [Deep Koopman Economic Model Predictive Control of a Pasteurisation Unit](https://arxiv.org/abs/2511.04437)
*Patrik Valábek,Michaela Horváthová,Martin Klaučo*

Main category: eess.SY

TL;DR: 提出基于深度Koopman的经济模型预测控制方法，用于实验室巴氏杀菌装置的高效运行，相比传统方法减少32%经济成本


<details>
  <summary>Details</summary>
Motivation: 解决巴氏杀菌装置这类复杂非线性系统的经济优化控制问题，传统方法难以在保持准确性的同时实现高效优化

Method: 使用Koopman算子理论将非线性系统转换为线性表示，结合神经网络学习线性动力学，并设计包含能源消耗、材料损失和执行器磨损的经济成本函数

Result: 深度Koopman EMPC相比传统N4SID方法在开环预测精度提升45%，总经济成本降低32%，稳态运行电能消耗减少10.2%

Conclusion: 深度Koopman表示与经济优化结合能够有效实现热密集型工厂的资源高效控制，具有显著实用优势

Abstract: This paper presents a deep Koopman-based Economic Model Predictive Control
(EMPC) for efficient operation of a laboratory-scale pasteurization unit (PU).
The method uses Koopman operator theory to transform the complex, nonlinear
system dynamics into a linear representation, enabling the application of
convex optimization while representing the complex PU accurately. The deep
Koopman model utilizes neural networks to learn the linear dynamics from
experimental data, achieving a 45% improvement in open-loop prediction accuracy
over conventional N4SID subspace identification. Both analyzed models were
employed in the EMPC formulation that includes interpretable economic costs,
such as energy consumption, material losses due to inadequate pasteurization,
and actuator wear. The feasibility of EMPC is ensured using slack variables.
The deep Koopman EMPC and N4SID EMPC are numerically validated on a nonlinear
model of multivariable PU under external disturbance. The disturbances include
feed pump fail-to-close scenario and the introduction of a cold batch to be
pastuerized. These results demonstrate that the deep Koopmand EMPC achieves a
32% reduction in total economic cost compared to the N4SID baseline. This
improvement is mainly due to the reductions in material losses and energy
consumption. Furthermore, the steady-state operation via Koopman-based EMPC
requires 10.2% less electrical energy. The results highlight the practical
advantages of integrating deep Koopman representations with economic
optimization to achieve resource-efficient control of thermal-intensive plants.

</details>


### [207] [Deep Dictionary-Free Method for Identifying Linear Model of Nonlinear System with Input Delay](https://arxiv.org/abs/2511.04451)
*Patrik Valábek,Marek Wadinger,Michal Kvasnica,Martin Klaučo*

Main category: eess.SY

TL;DR: 提出了一种使用LSTM增强的深度Koopman模型来近似Koopman算子的新方法，用于处理具有时间延迟的非线性动力系统，相比传统eDMD方法在预测精度上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 具有输入延迟的非线性动力系统在预测、估计和控制方面面临重大挑战，传统线性控制技术往往失效，需要创新方法来解决延迟对系统行为的影响。

Method: 采用LSTM增强的深度Koopman模型，通过LSTM层捕捉历史依赖关系，将时间延迟系统动态高效编码到潜在空间中，无需预定义字典。

Result: 在模拟系统上与扩展eDMD的定量比较显示，当真实非线性动态未知时，预测精度有显著性能提升；在已知系统动态的情况下，结果与eDMD相当。

Conclusion: LSTM增强的深度Koopman模型为处理具有时间延迟的非线性系统提供了一种有效的无字典方法，在未知系统动态时表现优于传统eDMD方法。

Abstract: Nonlinear dynamical systems with input delays pose significant challenges for
prediction, estimation, and control due to their inherent complexity and the
impact of delays on system behavior. Traditional linear control techniques
often fail in these contexts, necessitating innovative approaches. This paper
introduces a novel approach to approximate the Koopman operator using an
LSTM-enhanced Deep Koopman model, enabling linear representations of nonlinear
systems with time delays. By incorporating Long Short-Term Memory (LSTM)
layers, the proposed framework captures historical dependencies and efficiently
encodes time-delayed system dynamics into a latent space. Unlike traditional
extended Dynamic Mode Decomposition (eDMD) approaches that rely on predefined
dictionaries, the LSTM-enhanced Deep Koopman model is dictionary-free, which
mitigates the problems with the underlying dynamics being known and
incorporated into the dictionary. Quantitative comparisons with extended eDMD
on a simulated system demonstrate highly significant performance gains in
prediction accuracy in cases where the true nonlinear dynamics are unknown and
achieve comparable results to eDMD with known dynamics of a system.

</details>


### [208] [Data-driven uncertainty-aware seakeeping prediction of the Delft 372 catamaran using ensemble Hankel dynamic mode decomposition](https://arxiv.org/abs/2511.04461)
*Giorgio Palma,Andrea Serani,Matteo Diez*

Main category: eess.SY

TL;DR: 本文提出并验证了一种基于集成学习的Hankel动态模态分解控制方法(HDMDc)，用于高速双体船的不确定性感知耐波性预测，比较了贝叶斯和频率主义两种集成策略。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够提供不确定性量化的高效耐波性预测方法，以支持船舶设计和运营决策，解决传统确定性模型在预测精度和可靠性方面的不足。

Method: 使用HDMDc算法构建无方程线性降阶模型，通过状态和输入的时滞副本捕捉非线性和记忆效应。比较了贝叶斯HDMDc(采样超参数)和频率主义HDMDc(数据子集聚合)两种集成策略。

Result: 频率主义HDMDc相比确定性模型提高了预测精度，并提供了稳健的不确定性估计；而贝叶斯HDMDc在当前测试案例中相比确定性模型没有明显优势。FHDMDc得到的运动概率密度函数与实验数据和URANS结果高度吻合。

Conclusion: FHDMDc方法能够提供可靠且计算高效的耐波性预测，适用于船舶设计和运营支持，特别是在需要不确定性量化的情况下表现优异。

Abstract: In this study, we present and validate an ensemble-based Hankel Dynamic Mode
Decomposition with control (HDMDc) for uncertainty-aware seakeeping predictions
of a high-speed catamaran, namely the Delft 372 model. Experimental
measurements (time histories) of wave elevation at the longitudinal center of
gravity, heave, pitch, notional flight-deck velocity, notional bridge
acceleration, and total resistance were collected from irregular wave basin
tests on a 1:33.3 scale replica of the Delft 372 model under sea state 5
conditions at Fr = 0.425, and organized into training, validation, and test
sets. The HDMDc algorithm constructs an equation-free linear reduced-order
model of the seakeeping vessel by augmenting states and inputs with their
time-lagged copies to capture nonlinear and memory effects. Two ensembling
strategies, namely Bayesian HDMDc (BHDMDc), which samples hyperparameters
considered stochastic variables with prior distribution to produce posterior
mean forecasts with confidence intervals, and Frequentist HDMDc (FHDMDc), which
aggregates multiple model obtained over data subsets, are compared in providing
seakeeping prediction and uncertainty quantification. The FHDMDc approach is
found to improve the accuracy of the predictions compared to the deterministic
counterpart, also providing robust uncertainty estimation; whereas the
application of BHDMDc to the present test case is not found beneficial in
comparison to the deterministic model. FHDMDc-derived probability density
functions for the motions closely match both experimental data and URANS
results, demonstrating reliable and computationally efficient seakeeping
prediction for design and operational support.

</details>


### [209] [AI-Driven Phase-Shifted Carrier Optimization for Cascaded Bridge Converters, Modular Multilevel Converters, and Reconfigurable Batteries](https://arxiv.org/abs/2511.04470)
*Amin Hashemi-Zadeh,Nima Tashakor,Sandun Hettiarachchi,Stefan Goetz*

Main category: eess.SY

TL;DR: 提出一种神经网络方法替代传统优化器，用于优化级联桥式变换器中的相移角度，显著降低计算负担，实时减少电流纹波和总谐波失真达50%，比传统优化算法快10万到50万倍。


<details>
  <summary>Details</summary>
Motivation: 传统PSC-PWM算法在固定相移角下会导致模块间脉冲宽度不均匀，引起显著的电流纹波和输出电压失真。虽然电压均匀性需要优化各载波的相移，但计算负担超出了简单嵌入式控制器的能力。

Method: 使用神经网络模拟瞬时优化器的行为，通过一次训练即可适应不同系统规模。还包含简单的缩放策略，允许将训练好的神经网络通过模块分组和相移调整重用于更大系统，无需重新训练。

Result: 大规模评估、仿真和实验表明，该方法平均可实时减少电流纹波和加权总谐波失真达50%，比传统优化器（如遗传算法）快10万到50万倍，是唯一适用于在线应用的解决方案。

Conclusion: 所提出的神经网络方法能够以极低的计算负担实现相移角度的实时优化，解决了传统优化方法计算复杂、不适合在线应用的问题，具有良好的适应性和可扩展性。

Abstract: Phase-shifted carrier pulse-width modulation (PSC-PWM) is a widely adopted
scheduling algorithm in cascaded bridge converters, modular multilevel
converters, and reconfigurable batteries. However, non-uniformed pulse widths
for the modules with fixed phase shift angles lead to significant ripple
current and output-voltage distortion. Voltage uniformity instead would require
optimization of the phase shifts of the individual carriers. However, the
computational burden for such optimization is beyond the capabilities of any
simple embedded controller. This paper proposes a neural network that emulates
the behavior of an instantaneous optimizer with significantly reduced
computational burden. The proposed method has the advantages of stable
performance in predicting the optimum phase-shift angles under balanced battery
modules with non-identical modulation indices without requiring extensive
lookup tables, slow numerical optimization, or complex controller tuning. With
only one (re)training session for any specified number of modules, the proposed
method is readily adaptable to different system sizes. Furthermore, the
proposed framework also includes a simple scaling strategy that allows a neural
network trained for fewer modules to be reused for larger systems by grouping
modules and adjusting their phase shifts. The scaling strategy eliminates the
need for retraining. Large-scale assessment, simulations, and experiments
demonstrate that, on average, the proposed approach can reduce the current
ripple and the weighted total harmonic distortion by up to 50 % in real time
and is 100 to 500 thousand times faster than a conventional optimizer (e.g.,
genetic algorithms), making it the only solution for an online application.

</details>


### [210] [Synchronous Observer Design for Landmark-Inertial SLAM with Almost-Global Convergence](https://arxiv.org/abs/2511.04531)
*Arkadeep Saha,Pieter van Goor,Antonio Franchi,Ravi Banavar*

Main category: eess.SY

TL;DR: 提出了一种用于地标惯性SLAM问题的非线性观测器，在连续时间框架下分析，证明了误差动力学在基空间中的局部指数稳定性和几乎全局渐近稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决地标惯性同时定位与建图问题，通过地标位置测量和IMU测量来估计环境地标位置和机器人相对于这些地标的姿态。

Method: 在连续时间框架下提出非线性观测器，在编码所有可观测状态的基空间中进行分析。

Result: 证明了误差动力学在基空间中的局部指数稳定性和几乎全局渐近稳定性，并通过仿真验证了这些性质。

Conclusion: 所提出的非线性观测器能够有效解决LI-SLAM问题，具有理论保证的稳定性性能。

Abstract: Landmark Inertial Simultaneous Localisation and Mapping (LI-SLAM) is the
problem of estimating the locations of landmarks in the environment and the
robot's pose relative to those landmarks using landmark position measurements
and measurements from Inertial Measurement Unit (IMU). This paper proposes a
nonlinear observer for LI-SLAM posed in continuous time and analyses the
observer in a base space that encodes all the observable states of LI-SLAM. The
local exponential stability and almost-global asymptotic stability of the error
dynamics in base space is established in the proof section and validated using
simulations.

</details>


### [211] [Funnel-Based Online Recovery Control for Nonlinear Systems With Unknown Dynamics](https://arxiv.org/abs/2511.04626)
*Zihao Song,Shirantha Welikala,Panos J. Antsaklis,Hai Lin*

Main category: eess.SY

TL;DR: 提出了一种基于递归均衡网络（RENs）和漏斗控制的方法，用于非线性系统在遭受攻击或故障后的恢复控制，通过增量积分二次约束保证模型性质，并在DC微电网控制应用中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 解决非线性系统在攻击或故障后恢复控制的两个主要挑战：学习未知动态并提供形式化保证，以及找到状态不变集来确保与标称轨迹的允许偏差。

Method: 应用递归均衡网络（RENs）从实时系统状态数据中学习未知动态，使用增量积分二次约束（IQCs）保证模型输入输出性质；提出基于漏斗的控制方法实现系统恢复，推导标称轨迹稳定的充分条件和沿标称轨迹的不变漏斗。

Result: 开发了具有形式化保证的恢复控制框架，通过仿真验证了在DC微电网控制应用中的有效性。

Conclusion: 所提出的RENs和漏斗控制方法能够有效处理非线性系统在攻击或故障后的恢复问题，为系统安全运行提供了形式化保证。

Abstract: In this paper, we focus on recovery control of nonlinear systems from attacks
or failures. The main challenges of this problem lie in (1) learning the
unknown dynamics caused by attacks or failures with formal guarantees, and (2)
finding the invariant set of states to formally ensure the state deviations
allowed from the nominal trajectory. To solve this problem, we propose to apply
the Recurrent Equilibrium Networks (RENs) to learn the unknown dynamics using
the data from the real-time system states. The input-output property of this
REN model is guaranteed by incremental integral quadratic constraints (IQCs).
Then, we propose a funnel-based control method to achieve system recovery from
the deviated states. In particular, a sufficient condition for nominal
trajectory stabilization is derived together with the invariant funnels along
the nominal trajectory. Eventually, the effectiveness of our proposed control
method is illustrated by a simulation example of a DC microgrid control
application.

</details>


### [212] [Control Affine Hybrid Power Plant Subsystem Modeling for Supervisory Control Design](https://arxiv.org/abs/2511.04644)
*Stephen Ampleman,Himanshu Sharma,Sayak Mukherjee,Sonja Glavaski*

Main category: eess.SY

TL;DR: 本文提出了一个混合发电厂（风电场、太阳能电站和电池储能）的建模与控制设计框架，将各组件模型转化为适合控制设计的仿射形式，并使用非线性控制和控制屏障函数技术开发控制律。


<details>
  <summary>Details</summary>
Motivation: 混合发电厂结合多种发电机组和储能能力来支持发电不足和电网需求，需要统一的建模和控制框架来协调运行。

Method: 将风电场、太阳能电站和电池模型转化为控制仿射形式，使用非线性控制和控制屏障函数技术开发发电机扭矩和电池电流控制律，结合基于规则的监督控制。

Result: 通过测试案例验证了该框架的实用性，能够跟踪电网需求信号，同时保持安全稳定运行。

Conclusion: 该建模和控制框架为混合发电厂的协调控制提供了有效解决方案，能够应对变化的运行条件。

Abstract: Hybrid power plants (HPPs) combine multiple power generators
(conventional/variable) and energy storage capabilities to support generation
inadequacy and grid demands. This paper introduces a modeling and control
design framework for hybrid power plants (HPPs) consisting of a wind farm,
solar plant, and battery storage. Specifically, this work adapts established
modeling paradigms for wind farms, solar plants and battery models into a
control affine form suitable for control design at the supervisory level. In
the case of wind and battery models, generator torque and cell current control
laws are developed using nonlinear control and control barrier function
techniques to track a command from a supervisory control law while maintaining
safe and stable operation. The utility of this modeling and control framework
is illustrated through a test case using a utility demand signal for tracking,
time varying wind and irradiance data, and a rule-based supervisory control
law.

</details>
