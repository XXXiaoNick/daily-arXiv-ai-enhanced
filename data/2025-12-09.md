<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 74]
- [q-fin.CP](#q-fin.CP) [Total: 3]
- [eess.SY](#eess.SY) [Total: 37]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [math.OC](#math.OC) [Total: 30]
- [cs.AI](#cs.AI) [Total: 42]
- [cs.CY](#cs.CY) [Total: 14]
- [q-fin.MF](#q-fin.MF) [Total: 3]
- [econ.EM](#econ.EM) [Total: 6]
- [cs.LG](#cs.LG) [Total: 145]
- [q-fin.PR](#q-fin.PR) [Total: 1]
- [q-fin.RM](#q-fin.RM) [Total: 3]
- [stat.ML](#stat.ML) [Total: 16]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Empathy by Design: Aligning Large Language Models for Healthcare Dialogue](https://arxiv.org/abs/2512.06097)
*Emre Umucu,Guillermina Solis,Leon Garza,Emilia Rivas,Beatrice Lee,Anantaa Kotal,Aritran Piplai*

Main category: cs.CL

TL;DR: 提出基于直接偏好优化(DPO)的框架，用于提升医疗对话AI的事实准确性和同理心，解决现有LLM在医疗照护场景中的不可靠和缺乏情感支持问题。


<details>
  <summary>Details</summary>
Motivation: 通用大语言模型在医疗和照护应用中存在两个关键缺陷：事实不可靠性和缺乏同理心沟通。这些缺陷在敏感场景中带来显著风险，特别是当非专业照护者寻求医疗指导或情感支持时。

Method: 采用基于直接偏好优化(DPO)的对齐框架，使用成对偏好数据微调领域适应的LLM。偏好响应体现支持性和可访问的沟通风格，而被拒绝的响应代表指令性或过于技术化的语气。

Result: DPO调优模型在多个开源和专有LLM上表现出更高的语义对齐、改善的事实准确性以及更强的人本主义评估分数，优于基线和谷歌医疗对话系统等商业替代方案。

Conclusion: 基于偏好的对齐为开发可信赖、有同理心且具备临床知识的AI助手提供了可扩展且透明的途径，适用于照护者和医疗沟通场景。

Abstract: General-purpose large language models (LLMs) have demonstrated remarkable generative and reasoning capabilities but remain limited in healthcare and caregiving applications due to two key deficiencies: factual unreliability and a lack of empathetic communication. These shortcomings pose significant risks in sensitive contexts where users, particularly non-professionals and caregivers, seek medically relevant guidance or emotional reassurance. To address these challenges, we introduce a Direct Preference Optimization (DPO)-based alignment framework designed to improve factual correctness, semantic coherence, and human-centric qualities such as empathy, politeness, and simplicity in caregiver-patient dialogues. Our approach fine-tunes domain-adapted LLMs using pairwise preference data, where preferred responses reflect supportive and accessible communication styles while rejected ones represent prescriptive or overly technical tones. This direct optimization method aligns model outputs with human preferences more efficiently than traditional reinforcement-learning-based alignment. Empirical evaluations across multiple open and proprietary LLMs show that our DPO-tuned models achieve higher semantic alignment, improved factual accuracy, and stronger human-centric evaluation scores compared to baseline and commercial alternatives such as Google medical dialogue systems. These improvements demonstrate that preference-based alignment offers a scalable and transparent pathway toward developing trustworthy, empathetic, and clinically informed AI assistants for caregiver and healthcare communication. Our open-source code is available at: https://github.com/LeonG19/Empathy-by-Design

</details>


### [2] [Morphologically-Informed Tokenizers for Languages with Non-Concatenative Morphology: A case study of Yoloxóchtil Mixtec ASR](https://arxiv.org/abs/2512.06169)
*Chris Crawford*

Main category: cs.CL

TL;DR: 该研究探索了为Yoloxóchitl Mixtec语言设计形态感知的分词器，以改进音频语料的词间注释效率，提出了两种非线性分词方案，在ASR任务中与传统方法竞争。


<details>
  <summary>Details</summary>
Motivation: 传统分词方法在处理Yoloxóchitl Mixtec这种具有非连接性形态（特别是声调形态）的语言时效率不高，需要人工标注者大量工作。研究旨在通过设计专门针对该语言形态特征的分词器，提高音频语料词间注释的效率。

Method: 提出了两种新颖的非线性分词方案：1) Segment and Melody分词器：提取声调信息而不预测分词；2) Sequence of Processes分词器：预测词语的分词，使端到端ASR系统能一次性生成分词和未分词转录。使用ASR和文本序列到序列工具，与传统BPE和Unigram模型进行比较。

Result: 新型分词器与传统BPE和Unigram模型竞争性相当。Segment-and-Melody模型在词错误率上优于传统分词器，但在字符错误率上未达到同等水平。通过形态学和信息论指标分析，发现这些指标与下游性能存在预测性关联。

Conclusion: 针对语言非连接性形态专门设计的非线性分词器在ASR任务中与传统方法具有竞争力，表明形态感知分词器对处理复杂形态语言有价值。需要进一步研究这些分词器在下游处理任务中的适用性。

Abstract: This paper investigates the impact of using morphologically-informed tokenizers to aid and streamline the interlinear gloss annotation of an audio corpus of Yoloxóchitl Mixtec (YM) using a combination of ASR and text-based sequence-to-sequence tools, with the goal of improving efficiency while reducing the workload of a human annotator. We present two novel tokenization schemes that separate words in a nonlinear manner, preserving information about tonal morphology as much as possible. One of these approaches, a Segment and Melody tokenizer, simply extracts the tones without predicting segmentation. The other, a Sequence of Processes tokenizer, predicts segmentation for the words, which could allow an end-to-end ASR system to produce segmented and unsegmented transcriptions in a single pass. We find that these novel tokenizers are competitive with BPE and Unigram models, and the Segment-and-Melody model outperforms traditional tokenizers in terms of word error rate but does not reach the same character error rate. In addition, we analyze tokenizers on morphological and information-theoretic metrics to find predictive correlations with downstream performance. Our results suggest that nonlinear tokenizers designed specifically for the non-concatenative morphology of a language are competitive with conventional BPE and Unigram models for ASR. Further research will be necessary to determine the applicability of these tokenizers in downstream processing tasks.

</details>


### [3] [Do You Feel Comfortable? Detecting Hidden Conversational Escalation in AI Chatbots](https://arxiv.org/abs/2512.06193)
*Jihyung Park,Saleh Afroogh,Junfeng Jiao*

Main category: cs.CL

TL;DR: 提出GAUGE框架，用于实时检测LLM对话中的隐性情感升级危害，传统毒性过滤器难以发现这种渐进式的情感强化问题。


<details>
  <summary>Details</summary>
Motivation: LLM日益融入日常生活，不仅作为信息助手，也作为情感伴侣。即使没有明显毒性，重复的情感强化或情感漂移可能逐渐加剧用户痛苦，形成传统毒性过滤器无法检测的"隐性危害"。现有防护机制依赖外部分类器或临床标准，难以跟上对话发展的细微实时动态。

Method: 提出GAUGE（Guarding Affective Utterance Generation Escalation）框架，这是一个轻量级的基于logit的框架，用于实时检测隐藏的对话升级。GAUGE通过测量LLM输出如何概率性地改变对话的情感状态来实现检测。

Result: 论文提出了GAUGE框架，但摘要中未提供具体的实验结果数据。

Conclusion: GAUGE框架填补了现有防护机制的空白，能够实时检测LLM对话中的隐性情感升级危害，为LLM的安全交互提供了新的防护方法。

Abstract: Large Language Models (LLM) are increasingly integrated into everyday interactions, serving not only as information assistants but also as emotional companions. Even in the absence of explicit toxicity, repeated emotional reinforcement or affective drift can gradually escalate distress in a form of \textit{implicit harm} that traditional toxicity filters fail to detect. Existing guardrail mechanisms often rely on external classifiers or clinical rubrics that may lag behind the nuanced, real-time dynamics of a developing conversation. To address this gap, we propose GAUGE (Guarding Affective Utterance Generation Escalation), a lightweight, logit-based framework for the real-time detection of hidden conversational escalation. GAUGE measures how an LLM's output probabilistically shifts the affective state of a dialogue.

</details>


### [4] [Automated Data Enrichment using Confidence-Aware Fine-Grained Debate among Open-Source LLMs for Mental Health and Online Safety](https://arxiv.org/abs/2512.06227)
*Junyu Mao,Anthony Hills,Talia Tseriotou,Maria Liakata,Aya Shamir,Dan Sayda,Dana Atzil-Slonim,Natalie Djohari,Arpan Mandal,Silke Roth,Pamela Ugwudike,Mahesan Niranjan,Stuart E. Middleton*

Main category: cs.CL

TL;DR: 本文提出CFD框架，通过多LLM代理的细粒度辩论来增强现实世界指标的数据标注，在两个新数据集上验证了有效性，下游任务性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 现实世界指标（如心理健康事件、在线安全风险行为）对NLP任务很重要，但人工标注成本高且动态变化。需要开发有效的自动数据增强方法来降低标注成本。

Method: 提出置信感知细粒度辩论（CFD）框架：多个LLM代理模拟人类标注者，交换细粒度证据达成共识。比较了多种LLM数据增强方法，并在两个新专家标注数据集（心理健康Reddit数据集和在线安全Facebook分享风险数据集）上验证。

Result: CFD框架在数据增强方面表现最稳健，辩论记录作为增强特征带来最大性能提升：在线安全任务比非增强基线提升10.1%，下游任务性能持续改善。

Conclusion: CFD框架是有效的LLM数据增强方法，通过多代理辩论能生成高质量标注，显著提升下游NLP任务性能，为现实世界指标标注提供了成本效益高的解决方案。

Abstract: Real-world indicators are important for improving natural language processing (NLP) tasks such as life events for mental health analysis and risky behaviour for online safety, yet labelling such information in NLP training datasets is often costly and/or difficult given the dynamic nature of such events. This paper compares several LLM-based data enrichment methods and introduces a novel Confidence-Aware Fine-Grained Debate (CFD) framework in which multiple LLM agents simulate human annotators and exchange fine-grained evidence to reach consensus. We describe two new expert-annotated datasets, a mental health Reddit wellbeing dataset and an online safety Facebook sharenting risk dataset. Our CFD framework achieves the most robust data enrichment performance compared to a range of baselines and we show that this type of data enrichment consistently improves downstream tasks. Enriched features incorporated via debate transcripts yield the largest gains, outperforming the non-enriched baseline by 10.1% for the online safety task.

</details>


### [5] [Policy-based Sentence Simplification: Replacing Parallel Corpora with LLM-as-a-Judge](https://arxiv.org/abs/2512.06228)
*Xuanxin Wu,Yuki Arase,Masaaki Nagata*

Main category: cs.CL

TL;DR: 利用LLM-as-a-Judge自动构建策略对齐的训练数据，无需人工标注或平行语料，实现可适应不同简化策略的句子简化系统


<details>
  <summary>Details</summary>
Motivation: 句子简化需要根据不同应用场景采用不同的简化策略（如仅替换复杂词汇或整体重写），但实现这种策略驱动的控制仍然是一个开放挑战，现有方法需要昂贵的人工标注或平行语料

Method: 提出一种简单而强大的方法，利用LLM-as-a-Judge自动构建策略对齐的训练数据，完全消除对人工标注或平行语料的需求，从而构建能够适应不同简化策略的简化系统

Result: 即使是小规模开源LLM（如Phi-3-mini-3.8B）在词汇导向的简化任务上也能超越GPT-4o，在整体重写任务上达到可比性能，通过自动指标和人工评估验证，在不同模型家族和规模上均表现出稳健改进

Conclusion: 该方法通过LLM-as-a-Judge自动生成策略对齐数据，有效解决了句子简化中的策略控制问题，为构建适应多样化简化需求的系统提供了高效且可扩展的解决方案

Abstract: Sentence simplification aims to modify a sentence to make it easier to read and understand while preserving the meaning. Different applications require distinct simplification policies, such as replacing only complex words at the lexical level or rewriting the entire sentence while trading off details for simplicity. However, achieving such policy-driven control remains an open challenge. In this work, we introduce a simple yet powerful approach that leverages Large Language Model-as-a-Judge (LLM-as-a-Judge) to automatically construct policy-aligned training data, completely removing the need for costly human annotation or parallel corpora. Our method enables building simplification systems that adapt to diverse simplification policies. Remarkably, even small-scale open-source LLMs such as Phi-3-mini-3.8B surpass GPT-4o on lexical-oriented simplification, while achieving comparable performance on overall rewriting, as verified by both automatic metrics and human evaluations. The consistent improvements across model families and sizes demonstrate the robustness of our approach.

</details>


### [6] [LOCUS: A System and Method for Low-Cost Customization for Universal Specialization](https://arxiv.org/abs/2512.06239)
*Dhanasekar Sundararaman,Keying Li,Wayne Xiong,Aashna Garg*

Main category: cs.CL

TL;DR: LOCUS是一个低成本的NLP模型定制化流程，通过少量标注数据实现高效模型构建，结合检索、合成数据生成和参数高效调优，在NER和文本分类任务上超越GPT-4o等基线，同时大幅降低成本和模型大小。


<details>
  <summary>Details</summary>
Motivation: 传统NLP模型定制需要大量标注数据和计算资源，成本高昂。需要一种能够用少量数据高效构建专用模型的方法，同时保持高性能并大幅降低资源消耗。

Method: LOCUS采用三阶段流程：1) 从大型知识库中检索相关数据；2) 通过上下文数据生成合成额外训练样本；3) 使用全参数或LoRA等参数高效方法进行微调。针对NER和文本分类任务设计。

Result: 在多个基准测试中持续超越GPT-4o等强基线，内存优化模型保持99%全微调精度的同时仅使用5%内存占用，参数量不到GPT-4o的1%但性能更优，显著降低成本。

Conclusion: LOCUS证明了通过少量数据结合智能检索和合成数据生成，可以构建高效、低成本的专业化NLP模型，为实际应用提供了实用的模型定制解决方案。

Abstract: We present LOCUS (LOw-cost Customization for Universal Specialization), a pipeline that consumes few-shot data to streamline the construction and training of NLP models through targeted retrieval, synthetic data generation, and parameter-efficient tuning. With only a small number of labeled examples, LOCUS discovers pertinent data in a broad repository, synthesizes additional training samples via in-context data generation, and fine-tunes models using either full or low-rank (LoRA) parameter adaptation. Our approach targets named entity recognition (NER) and text classification (TC) benchmarks, consistently outperforming strong baselines (including GPT-4o) while substantially lowering costs and model sizes. Our resultant memory-optimized models retain 99% of fully fine-tuned accuracy while using barely 5% of the memory footprint, also beating GPT-4o on several benchmarks with less than 1% of its parameters.

</details>


### [7] [Convergence of Outputs When Two Large Language Models Interact in a Multi-Agentic Setup](https://arxiv.org/abs/2512.06256)
*Aniruddha Maiti,Satya Nimmagadda,Kartha Veerya Jammuladinne,Niladri Sengupta,Ananya Jana*

Main category: cs.CL

TL;DR: 两个大语言模型在无外部输入的多轮对话中会逐渐陷入重复循环，即使它们是大规模、独立训练的模型，最终会收敛到相似的输出。


<details>
  <summary>Details</summary>
Motivation: 研究两个大型语言模型在没有外部输入的情况下相互对话时会发生什么，探索多智能体设置中的对话动态和收敛行为。

Method: 使用Mistral Nemo Base 2407和Llama 2 13B hf模型，从一个短种子句子开始，让两个模型相互读取对方输出并生成回复，持续固定轮数。应用词汇和嵌入指标来测量对话从初始种子的漂移程度以及两个模型输出的相似性。

Result: 大多数对话开始时是连贯的，但后来会陷入重复。在许多运行中，会出现短短语并在多个回合中重复。一旦重复开始，两个模型倾向于产生相似的输出而不是引入新的对话方向，导致相同或相似文本的循环。这种收敛行为发生在即使模型是大型、独立训练且没有提示指令的情况下。

Conclusion: 两个大型语言模型在无外部输入的多轮对话中会表现出收敛行为，最终陷入重复循环，这表明即使是大规模模型在多智能体设置中也存在稳定性问题，需要通过指标来量化和理解这种对话动态。

Abstract: In this work, we report what happens when two large language models respond to each other for many turns without any outside input in a multi-agent setup. The setup begins with a short seed sentence. After that, each model reads the other's output and generates a response. This continues for a fixed number of steps. We used Mistral Nemo Base 2407 and Llama 2 13B hf. We observed that most conversations start coherently but later fall into repetition. In many runs, a short phrase appears and repeats across turns. Once repetition begins, both models tend to produce similar output rather than introducing a new direction in the conversation. This leads to a loop where the same or similar text is produced repeatedly. We describe this behavior as a form of convergence. It occurs even though the models are large, trained separately, and not given any prompt instructions. To study this behavior, we apply lexical and embedding-based metrics to measure how far the conversation drifts from the initial seed and how similar the outputs of the two models becomes as the conversation progresses.

</details>


### [8] [Nanbeige4-3B Technical Report: Exploring the Frontier of Small Language Models](https://arxiv.org/abs/2512.06266)
*Chen Yang,Guangyue Peng,Jiaying Zhu,Ran Le,Ruixiang Feng,Tao Zhang,Wei Ruan,Xiaoqi Liu,Xiaoxue Cheng,Xiyun Xu,Yang Song,Yanzipeng Gao,Yiming Jia,Yun Xing,Yuntao Wen,Zekai Wang,Zhenwei An,Zhicong Sun,Zongchao Chen*

Main category: cs.CL

TL;DR: Nanbeige4-3B是一个高性能的小规模语言模型家族，通过创新的训练策略在多个阶段优化模型性能，在3B参数规模下达到与更大模型相媲美的表现。


<details>
  <summary>Details</summary>
Motivation: 推动小规模语言模型的性能边界，证明通过精心设计的训练策略，小模型也能达到与更大模型相媲美的性能水平。

Method: 采用多阶段训练策略：1) 预训练阶段使用FG-WSD调度器优化数据混合；2) 指令微调阶段使用联合机制提升SFT数据质量；3) 使用DPD方法进行蒸馏；4) 多阶段强化学习增强推理和对齐能力。

Result: 在广泛基准测试中，Nanbeige4-3B不仅显著优于同参数规模的模型，还能与更大模型竞争，展示了小规模模型的高性能潜力。

Conclusion: 通过创新的训练策略组合，成功扩展了小规模语言模型的缩放定律边界，为高效的小模型开发提供了新思路。

Abstract: We present Nanbeige4-3B, a family of small-scale but high-performing language models. Pretrained on 23T high-quality tokens and finetuned on over 30 million diverse instructions, we extend the boundary of the scaling law for small language models. In pre-training, we design a Fine-Grained Warmup-Stable-Decay (FG-WSD) training scheduler, which progressively refines data mixtures across stages to boost model performance. In post-training, to improve the quality of the SFT data, we design a joint mechanism that integrates deliberative generation refinement and chain-of-thought reconstruction, yielding substantial gains on complex tasks. Following SFT, we employ our flagship reasoning model to distill Nanbeige4-3B through our proposed Dual Preference Distillation (DPD) method, which leads to further performance gains. Finally, a multi-stage reinforcement learning phase was applied, leveraging verifiable rewards and preference modeling to strengthen abilities on both reasoning and human alignment. Extensive evaluations show that Nanbeige4-3B not only significantly outperforms models of comparable parameter scale but also rivals much larger models across a wide range of benchmarks. The model checkpoints are available at https://huggingface.co/Nanbeige.

</details>


### [9] [Modeling Contextual Passage Utility for Multihop Question Answering](https://arxiv.org/abs/2512.06464)
*Akriti Jain,Aparna Garimella*

Main category: cs.CL

TL;DR: 提出轻量级上下文感知的篇章效用预测方法，通过建模篇章间依赖关系提升多跳问答性能


<details>
  <summary>Details</summary>
Motivation: 现有方法独立建模篇章效用，忽略了多跳推理中篇章效用的上下文依赖性——篇章效用受其与其他篇章关系的影响（提供补充信息或形成关键链接）

Method: 使用小型transformer模型预测篇章效用分数，利用先进推理模型的推理轨迹获取训练数据，建模篇章间依赖关系

Result: 基于效用的篇章评分相比基于相关性的重排序方法，在重排序和下游问答性能上都有提升

Conclusion: 建模上下文感知的篇章效用能有效提升多跳问答系统性能，通过考虑篇章间依赖关系可以更好地识别和利用关键信息

Abstract: Multihop Question Answering (QA) requires systems to identify and synthesize information from multiple text passages. While most prior retrieval methods assist in identifying relevant passages for QA, further assessing the utility of the passages can help in removing redundant ones, which may otherwise add to noise and inaccuracies in the generated answers. Existing utility prediction approaches model passage utility independently, overlooking a critical aspect of multihop reasoning: the utility of a passage can be context-dependent, influenced by its relation to other passages - whether it provides complementary information or forms a crucial link in conjunction with others. In this paper, we propose a lightweight approach to model contextual passage utility, accounting for inter-passage dependencies. We fine-tune a small transformer-based model to predict passage utility scores for multihop QA. We leverage the reasoning traces from an advanced reasoning model to capture the order in which passages are used to answer a question and obtain synthetic training data. Through comprehensive experiments, we demonstrate that our utility-based scoring of retrieved passages leads to improved reranking and downstream QA performance compared to relevance-based reranking methods.

</details>


### [10] [Knowing What's Missing: Assessing Information Sufficiency in Question Answering](https://arxiv.org/abs/2512.06476)
*Akriti Jain,Aparna Garimella*

Main category: cs.CL

TL;DR: 提出Identify-then-Verify框架，通过首先生成缺失信息假设并建立语义共识，然后验证信息是否真正缺失，来更可靠地评估上下文是否足以回答问题。


<details>
  <summary>Details</summary>
Motivation: 现有简单提示策略在处理事实性问题时表现良好，但在需要推理的推断性问题中经常失败。需要更可靠的方法来判断上下文是否包含足够信息来回答问题。

Method: 提出结构化Identify-then-Verify框架：1) 生成多个关于缺失信息的假设并建立语义共识；2) 执行关键验证步骤，强制模型重新检查源文本以确认信息是否真正缺失。

Result: 在多种多跳和事实性QA数据集上评估，结果显示该方法通过引导模型证明其关于缺失信息的判断，能产生更准确的充分性判断，同时清晰阐明信息差距。

Conclusion: 通过让模型首先推理缺失的具体信息，可以提供更可靠的隐式信号来评估整体充分性，从而构建更可靠的问答系统。

Abstract: Determining whether a provided context contains sufficient information to answer a question is a critical challenge for building reliable question-answering systems. While simple prompting strategies have shown success on factual questions, they frequently fail on inferential ones that require reasoning beyond direct text extraction. We hypothesize that asking a model to first reason about what specific information is missing provides a more reliable, implicit signal for assessing overall sufficiency. To this end, we propose a structured Identify-then-Verify framework for robust sufficiency modeling. Our method first generates multiple hypotheses about missing information and establishes a semantic consensus. It then performs a critical verification step, forcing the model to re-examine the source text to confirm whether this information is truly absent. We evaluate our method against established baselines across diverse multi-hop and factual QA datasets. The results demonstrate that by guiding the model to justify its claims about missing information, our framework produces more accurate sufficiency judgments while clearly articulating any information gaps.

</details>


### [11] [Classifying German Language Proficiency Levels Using Large Language Models](https://arxiv.org/abs/2512.06483)
*Elias-Leander Ahlers,Witold Brunsmann,Malte Schilling*

Main category: cs.CL

TL;DR: 本文研究使用大语言模型自动将德语文本按CEFR语言能力等级分类，通过构建多样化数据集并比较多种方法，实现了比现有方法更优的性能。


<details>
  <summary>Details</summary>
Motivation: 语言能力评估对教育至关重要，能够根据学习者需求提供定制化教学。本文旨在探索使用大语言模型自动将德语文本按CEFR标准分类，以实现可靠且可扩展的语言能力评估。

Method: 1. 构建多样化数据集：结合多个现有CEFR标注语料库与合成数据；2. 评估多种方法：提示工程策略、微调LLaMA-3-8B-Instruct模型、基于探测的方法（利用LLM内部神经状态进行分类）。

Result: 研究结果显示，所提出的方法在CEFR分类任务上实现了比先前方法更一致的性能提升，证明了LLMs在可靠且可扩展的CEFR分类方面的潜力。

Conclusion: 大语言模型在德语文本CEFR分类任务中表现出色，为可靠且可扩展的语言能力评估提供了有效解决方案，具有重要的教育应用价值。

Abstract: Assessing language proficiency is essential for education, as it enables instruction tailored to learners needs. This paper investigates the use of Large Language Models (LLMs) for automatically classifying German texts according to the Common European Framework of Reference for Languages (CEFR) into different proficiency levels. To support robust training and evaluation, we construct a diverse dataset by combining multiple existing CEFR-annotated corpora with synthetic data. We then evaluate prompt-engineering strategies, fine-tuning of a LLaMA-3-8B-Instruct model and a probing-based approach that utilizes the internal neural state of the LLM for classification. Our results show a consistent performance improvement over prior methods, highlighting the potential of LLMs for reliable and scalable CEFR classification.

</details>


### [12] [ProSocialAlign: Preference Conditioned Test Time Alignment in Language Models](https://arxiv.org/abs/2512.06515)
*Somnath Banerjee,Sayan Layek,Sayantan Adak,Mykola Pechenizkiy,Animesh Mukherjee,Rima Hazra*

Main category: cs.CL

TL;DR: ProSocialAlign：一个测试时参数高效框架，通过词典约束生成和方向性调节，在保持安全性的同时生成共情且价值对齐的响应，无需重新训练基础模型。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型安全范式在情感激烈或高风险场景中存在不足：仅拒绝可能疏远用户，而简单顺从又会放大风险。需要一种能在保持安全的同时生成共情且价值对齐响应的方法。

Method: 提出ProSocialAlign框架：1）形式化五个以人为本的目标；2）将安全视为词典约束生成：先应用硬约束消除有害延续，然后在安全集内优化亲社会质量；3）结合方向性调节（在参数空间减去学习到的"伤害向量"）和偏好感知自回归奖励建模（联合训练多属性并解决梯度冲突）。

Result: 在五个安全基准测试中达到最先进性能，显著减少不安全泄漏，提升与人类价值观的对齐度，在多个评估指标上均有强劲提升。

Conclusion: ProSocialAlign为在推理时生成上下文敏感、安全且人类对齐的响应提供了稳健且模块化的基础，实现了测试时参数高效的安全对齐。

Abstract: Current language model safety paradigms often fall short in emotionally charged or high-stakes settings, where refusal-only approaches may alienate users and naive compliance can amplify risk. We propose ProSocialAlign, a test-time, parameter-efficient framework that steers generation toward safe, empathetic, and value-aligned responses without retraining the base model. We formalize five human-centered objectives and cast safety as lexicographic constrained generation: first, applying hard constraints to eliminate harmful continuations; then optimizing for prosocial quality within the safe set. Our method combines (i) directional regulation, a harm-mitigation mechanism that subtracts a learned "harm vector" in parameter space, and (ii) preference-aware autoregressive reward modeling trained jointly across attributes with gradient conflict resolution, enabling fine-grained, user-controllable decoding. Empirical evaluations across five safety benchmarks demonstrate state-of-the-art performance, reducing unsafe leakage and boosting alignment to human values, with strong gains across multiple evaluation metrics. ProSocialAlign offers a robust and modular foundation for generating context-sensitive, safe, and human-aligned responses at inference time.

</details>


### [13] [Adapting AlignScore Mertic for Factual Consistency Evaluation of Text in Russian: A Student Abstract](https://arxiv.org/abs/2512.06586)
*Mikhail Zimin,Milyausha Shamsutdinova,Georgii Andriushchenko*

Main category: cs.CL

TL;DR: 将AlignScore指标适配到俄语，开发了AlignRuScore用于评估俄语文本的事实一致性


<details>
  <summary>Details</summary>
Motivation: 现有事实一致性评估工具主要针对英语语料，缺乏俄语文本的评估工具，需要填补这一空白

Method: 基于RuBERT的对齐模型，使用分类和回归头在俄语和翻译的英语数据集上进行微调

Result: 统一的对齐指标可以成功移植到俄语，为稳健的多语言事实一致性评估奠定基础

Conclusion: 发布了翻译语料库、模型检查点和代码，支持进一步研究，为俄语事实一致性评估提供了有效工具

Abstract: Ensuring factual consistency in generated text is crucial for reliable natural language processing applications. However, there is a lack of evaluation tools for factual consistency in Russian texts, as existing tools primarily focus on English corpora. To bridge this gap, we introduce AlignRuScore, a comprehensive adaptation of the AlignScore metric for Russian. To adapt the metric, we fine-tuned a RuBERT-based alignment model with task-specific classification and regression heads on Russian and translated English datasets. Our results demonstrate that a unified alignment metric can be successfully ported to Russian, laying the groundwork for robust multilingual factual consistency evaluation. We release the translated corpora, model checkpoints, and code to support further research.

</details>


### [14] [The Online Discourse of Virtual Reality and Anxiety](https://arxiv.org/abs/2512.06656)
*Kwabena Yamoah,Cass Dykeman*

Main category: cs.CL

TL;DR: 该研究使用语料库语言学方法分析在线讨论中虚拟现实（VR）与焦虑相关的话题，发现VR、Oculus和头显是最常被讨论的词汇，并揭示了VR系统设计与体验相关的语言模式。


<details>
  <summary>Details</summary>
Motivation: VR已被用于治疗广泛性焦虑障碍和社交焦虑等临床问题，为患者福祉和护理创造了新途径。了解用户对该技术的在线讨论可以进一步支持其疗效。本研究旨在通过语料库语言学方法揭示虚拟现实与焦虑在线讨论的语言特征。

Method: 采用语料库语言学方法，使用Sketch Engine软件分析英语趋势语料库中VR与焦虑子语料库的词汇使用频率和搭配模式，识别常用词汇及其网络关系。

Result: 研究发现VR、Oculus和头显是VR与焦虑子语料库中最常讨论的词汇，表明关注点集中在虚拟系统本身及其物理设备。此外，识别出"of virtual reality"、"in virtual reality"和"for virtual reality"等介词短语搭配，分别与设计、体验和开发相关。

Conclusion: 这些发现为理解VR与焦虑在一般话语中的讨论提供了新视角，并为通过技术开发和可及性来支持咨询需求提供了未来机会的路径。

Abstract: VR in the treatment of clinical concerns such as generalized anxiety disorder or social anxiety. VR has created additional pathways to support patient well-being and care. Understanding online discussion of what users think about this technology may further support its efficacy. The purpose of this study was to employ a corpus linguistic methodology to identify the words and word networks that shed light on the online discussion of virtual reality and anxiety. Using corpus linguistics, frequently used words in discussion along with collocation were identified by utilizing Sketch Engine software. The results of the study, based upon the English Trends corpus, identified VR, Oculus, and headset as the most frequently discussed within the VR and anxiety subcorpus. These results point to the development of the virtual system, along with the physical apparatus that makes viewing and engaging with the virtual environment possible. Additional results point to collocation of prepositional phrases such as of virtual reality, in virtual reality, and for virtual reality relating to the design, experience, and development, respectively. These findings offer new perspective on how VR and anxiety together are discussed in general discourse and offer pathways for future opportunities to support counseling needs through development and accessibility. Keywords: anxiety disorders, corpus linguistics, Sketch Engine, and virtual reality VR

</details>


### [15] [CMV-Fuse: Cross Modal-View Fusion of AMR, Syntax, and Knowledge Representations for Aspect Based Sentiment Analysis](https://arxiv.org/abs/2512.06679)
*Smitha Muthya Sudheendra,Mani Deep Cherukuri,Jaideep Srivastava*

Main category: cs.CL

TL;DR: CMV-Fuse是一个跨模态视图融合框架，通过结合多种语言视角（抽象意义表示、成分句法、依存句法和语义注意力）来模拟人类语言处理，提升方面级情感分析的性能。


<details>
  <summary>Details</summary>
Motivation: 当前方面级情感分析系统通常利用孤立的语言视角，忽视了人类自然利用的结构表征之间的复杂相互作用。自然语言理解本质上需要整合从表层句法到深层语义和世界知识的多个互补视角。

Method: 提出CMV-Fuse框架，系统整合四种语言视角：抽象意义表示、成分句法、依存句法和语义注意力，并增强外部知识集成。通过局部句法、中间语义和全局知识层次的分层门控注意力融合，捕捉细粒度结构模式和广泛上下文理解。采用新颖的结构感知多视图对比学习机制确保互补表征的一致性。

Result: 在标准基准测试中相比强基线有显著改进，分析揭示了每种语言视角如何为更鲁棒的情感分析做出贡献。

Conclusion: 通过模拟人类语言处理的多视角融合方法能够有效提升方面级情感分析的性能，证明了整合多种语言视角的重要性。

Abstract: Natural language understanding inherently depends on integrating multiple complementary perspectives spanning from surface syntax to deep semantics and world knowledge. However, current Aspect-Based Sentiment Analysis (ABSA) systems typically exploit isolated linguistic views, thereby overlooking the intricate interplay between structural representations that humans naturally leverage. We propose CMV-Fuse, a Cross-Modal View fusion framework that emulates human language processing by systematically combining multiple linguistic perspectives. Our approach systematically orchestrates four linguistic perspectives: Abstract Meaning Representations, constituency parsing, dependency syntax, and semantic attention, enhanced with external knowledge integration. Through hierarchical gated attention fusion across local syntactic, intermediate semantic, and global knowledge levels, CMV-Fuse captures both fine-grained structural patterns and broad contextual understanding. A novel structure aware multi-view contrastive learning mechanism ensures consistency across complementary representations while maintaining computational efficiency. Extensive experiments demonstrate substantial improvements over strong baselines on standard benchmarks, with analysis revealing how each linguistic view contributes to more robust sentiment analysis.

</details>


### [16] [Mechanistic Interpretability of GPT-2: Lexical and Contextual Layers in Sentiment Analysis](https://arxiv.org/abs/2512.06681)
*Amartya Hatua*

Main category: cs.CL

TL;DR: GPT-2的情感处理机制研究：早期层负责词汇情感检测，但上下文整合发生在晚期层而非中期层，且采用统一的非模块化机制


<details>
  <summary>Details</summary>
Motivation: 研究GPT-2中情感信息处理的机制，特别是验证假设的两阶段架构（早期词汇检测和中期上下文整合），以理解大型语言模型中情感计算的实际模式

Method: 使用系统性的激活修补技术，在GPT-2的所有12个transformer层上进行因果分析，测试三种上下文整合假设：中层集中、现象特异性和分布式处理

Result: 早期层（0-3）确实作为词汇情感检测器，编码稳定、位置特定的极性信号，基本独立于上下文。但所有三种上下文整合假设都被证伪，上下文现象（否定、讽刺、领域转移等）主要在晚期层（8-11）通过统一的非模块化机制整合

Conclusion: GPT-2的情感计算与预测的分层模式不同，上下文整合主要在晚期层进行，这突显了需要进一步实证表征大型语言模型中的上下文整合机制

Abstract: We present a mechanistic interpretability study of GPT-2 that causally examines how sentiment information is processed across its transformer layers. Using systematic activation patching across all 12 layers, we test the hypothesized two-stage sentiment architecture comprising early lexical detection and mid-layer contextual integration. Our experiments confirm that early layers (0-3) act as lexical sentiment detectors, encoding stable, position specific polarity signals that are largely independent of context. However, all three contextual integration hypotheses: Middle Layer Concentration, Phenomenon Specificity, and Distributed Processing are falsified. Instead of mid-layer specialization, we find that contextual phenomena such as negation, sarcasm, domain shifts etc. are integrated primarily in late layers (8-11) through a unified, non-modular mechanism. These experimental findings provide causal evidence that GPT-2's sentiment computation differs from the predicted hierarchical pattern, highlighting the need for further empirical characterization of contextual integration in large language models.

</details>


### [17] [Large Language Models and Forensic Linguistics: Navigating Opportunities and Threats in the Age of Generative AI](https://arxiv.org/abs/2512.06922)
*George Mikros*

Main category: cs.CL

TL;DR: 大型语言模型对法庭语言学构成双重挑战：既是强大的分析工具，又通过风格模仿、作者混淆和合成文本破坏语言特征的基本假设，需要方法学重构以保持科学可信度和法律可采性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在法庭语言学领域带来双重影响：一方面作为强大的分析工具支持大规模语料分析和基于嵌入的作者归属，另一方面通过风格模仿、作者混淆和合成文本的扩散，动摇了关于个人语言特征的基本假设。这种张力对法庭应用有重要影响，需要重新评估现有方法。

Method: 文章分析了当前AI文本检测技术的局限性，包括基于分类器的方法、风格计量学方法和水印技术。这些方法存在高误报率（特别是对非母语英语写作者）和对抗策略（如同形异义替换）的脆弱性。提出了方法学重构的建议。

Result: 研究发现LLM可以近似表面风格特征，但与人类写作者存在可检测的差异。当前AI文本检测技术面临重大限制，在法律可采性标准（特别是Daubert和Kumho Tire框架）下存在不确定性，需要方法学调整。

Conclusion: 法庭语言学需要进行方法学重构以保持科学可信度和法律可采性。建议的调整包括混合人机工作流程、超越二元分类的可解释检测范式，以及测量不同人群错误和偏见的验证机制。语言揭示其生产者信息的核心见解仍然有效，但必须适应日益复杂的人类和机器作者链。

Abstract: Large language models (LLMs) present a dual challenge for forensic linguistics. They serve as powerful analytical tools enabling scalable corpus analysis and embedding-based authorship attribution, while simultaneously destabilising foundational assumptions about idiolect through style mimicry, authorship obfuscation, and the proliferation of synthetic texts. Recent stylometric research indicates that LLMs can approximate surface stylistic features yet exhibit detectable differences from human writers, a tension with significant forensic implications. However, current AI-text detection techniques, whether classifier-based, stylometric, or watermarking approaches, face substantial limitations: high false positive rates for non-native English writers and vulnerability to adversarial strategies such as homoglyph substitution. These uncertainties raise concerns under legal admissibility standards, particularly the Daubert and Kumho Tire frameworks. The article concludes that forensic linguistics requires methodological reconfiguration to remain scientifically credible and legally admissible. Proposed adaptations include hybrid human-AI workflows, explainable detection paradigms beyond binary classification, and validation regimes measuring error and bias across diverse populations. The discipline's core insight, i.e., that language reveals information about its producer, remains valid but must accommodate increasingly complex chains of human and machine authorship.

</details>


### [18] [PersonaMem-v2: Towards Personalized Intelligence via Learning Implicit User Personas and Agentic Memory](https://arxiv.org/abs/2512.06688)
*Bowen Jiang,Yuan Yuan,Maohao Shen,Zhuoqun Hao,Zhangchen Xu,Zichen Chen,Ziyi Liu,Anvesh Rao Vijjini,Jiashu He,Hanchao Yu,Radha Poovendran,Gregory Wornell,Lyle Ungar,Dan Roth,Sihao Chen,Camillo Jose Taylor*

Main category: cs.CL

TL;DR: PersonaMem-v2是用于LLM个性化的大规模数据集，包含1000个用户-聊天机器人交互、300+场景、20000+用户偏好。研究发现前沿LLM在隐式个性化任务上准确率仅37-48%，通过强化微调Qwen3-4B可超越GPT-5达到53%准确率，而智能记忆框架仅用2k令牌记忆即可达到55%准确率，比完整32k对话历史节省16倍令牌。


<details>
  <summary>Details</summary>
Motivation: 个性化是AI能力和对齐的下一个里程碑。当前前沿LLM在隐式个性化任务上表现不佳，准确率仅37-48%，尽管支持长上下文窗口，但推理能力成为瓶颈。需要开发更好的数据集和训练方法来提升LLM的个性化能力。

Method: 1) 创建PersonaMem-v2数据集，模拟1000个真实用户-聊天机器人交互，包含300+场景、20000+用户偏好、128k令牌上下文窗口，大多数用户偏好是隐式揭示的。2) 使用强化微调训练模型提升长上下文推理能力。3) 开发智能记忆系统框架，维护单一、人类可读的记忆，随着用户交互而增长。

Result: 1) 前沿LLM在隐式个性化任务上准确率仅37-48%。2) 通过强化微调，Qwen3-4B超越GPT-5，达到53%准确率。3) 智能记忆框架达到55%准确率，仅使用2k令牌记忆，比完整32k对话历史节省16倍输入令牌。

Conclusion: PersonaMem-v2数据集对个性化研究有重要影响，智能记忆系统是实现可扩展、真实世界个性化智能的有效路径。即使模型支持长上下文，推理能力仍是隐式个性化任务的关键瓶颈，需要专门的训练方法提升。

Abstract: Personalization is one of the next milestones in advancing AI capability and alignment. We introduce PersonaMem-v2, the state-of-the-art dataset for LLM personalization that simulates 1,000 realistic user-chatbot interactions on 300+ scenarios, 20,000+ user preferences, and 128k-token context windows, where most user preferences are implicitly revealed to reflect real-world interactions. Using this data, we investigate how reinforcement fine-tuning enables a model to improve its long-context reasoning capabilities for user understanding and personalization. We also develop a framework for training an agentic memory system, which maintains a single, human-readable memory that grows with each user over time.
  In our experiments, frontier LLMs still struggle with implicit personalization, achieving only 37-48% accuracy. While they support long context windows, reasoning remains the bottleneck for implicit personalization tasks. Using reinforcement fine-tuning, we successfully train Qwen3-4B to outperforms GPT-5, reaching 53% accuracy in implicit personalization. Moreover, our agentic memory framework achieves state-of-the-art 55% accuracy while using 16x fewer input tokens, relying on a 2k-token memory instead of full 32k conversation histories. These results underscore the impact of our dataset and demonstrate agentic memory as a scalable path toward real-world personalized intelligence.

</details>


### [19] [Think-While-Generating: On-the-Fly Reasoning for Personalized Long-Form Generation](https://arxiv.org/abs/2512.06690)
*Chengbing Wang,Yang Zhang,Wenjie Wang,Xiaoyan Zhao,Fuli Feng,Xiangnan He,Tat-Seng Chua*

Main category: cs.CL

TL;DR: FlyThinker提出了一种"边思考边生成"的高效个性化长文本生成框架，通过并行推理模型生成潜在token级推理，动态指导响应生成，在保持训练和推理效率的同时实现更好的个性化生成。


<details>
  <summary>Details</summary>
Motivation: 当前偏好对齐方法主要优化群体级偏好，忽视个体用户。早期个性化方法（如提示定制或微调）难以推理隐含偏好，而最近的"先思考后生成"方法在长文本生成中面临挑战：静态一次性推理需要为完整响应生成捕获所有相关信息，学习困难且适应性有限。

Method: FlyThinker采用"边思考边生成"框架，使用独立的推理模型并行生成潜在token级推理，将其融合到生成模型中动态指导响应生成。推理模型仅依赖先前响应而非自身先前输出，保持训练并行性，所有推理token可在单次前向传递中生成。

Result: 在真实世界基准测试上的广泛实验表明，FlyThinker在保持训练和推理效率的同时，实现了更好的个性化生成效果。

Conclusion: FlyThinker通过创新的"边思考边生成"框架解决了个性化长文本生成的挑战，在效率和效果之间取得了良好平衡，为个性化语言模型提供了实用解决方案。

Abstract: Preference alignment has enabled large language models (LLMs) to better reflect human expectations, but current methods mostly optimize for population-level preferences, overlooking individual users. Personalization is essential, yet early approaches-such as prompt customization or fine-tuning-struggle to reason over implicit preferences, limiting real-world effectiveness. Recent "think-then-generate" methods address this by reasoning before response generation. However, they face challenges in long-form generation: their static one-shot reasoning must capture all relevant information for the full response generation, making learning difficult and limiting adaptability to evolving content. To address this issue, we propose FlyThinker, an efficient "think-while-generating" framework for personalized long-form generation. FlyThinker employs a separate reasoning model that generates latent token-level reasoning in parallel, which is fused into the generation model to dynamically guide response generation. This design enables reasoning and generation to run concurrently, ensuring inference efficiency. In addition, the reasoning model is designed to depend only on previous responses rather than its own prior outputs, which preserves training parallelism across different positions-allowing all reasoning tokens for training data to be produced in a single forward pass like standard LLM training, ensuring training efficiency. Extensive experiments on real-world benchmarks demonstrate that FlyThinker achieves better personalized generation while keeping training and inference efficiency.

</details>


### [20] [TopiCLEAR: Topic extraction by CLustering Embeddings with Adaptive dimensional Reduction](https://arxiv.org/abs/2512.06694)
*Aoi Fujita,Taichi Yamamoto,Yuri Nakayama,Ryota Kobayashi*

Main category: cs.CL

TL;DR: TopiCLEAR：一种针对社交媒体短文本的主题提取方法，通过聚类嵌入和自适应降维，在多个数据集上优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统主题模型是为较长、正式文档设计的，在处理社交媒体短文本时面临挑战：共现统计有限、语义碎片化、拼写不一致和非正式语言。需要专门针对短文本的主题提取方法。

Method: TopiCLEAR：使用Sentence-BERT嵌入文本，先用高斯混合模型进行初步聚类，然后通过基于线性判别分析的监督投影迭代优化聚类，直到收敛。无需预处理步骤如停用词移除。

Result: 在四个数据集（20News、AgNewsTitle、Reddit、TweetTopic）上评估，与7个基线方法相比，TopiCLEAR与人工标注主题的相似度最高，对社交媒体帖子和在线新闻文章均有显著改进。

Conclusion: TopiCLEAR方法在社交媒体短文本主题提取方面表现出色，产生更可解释的主题，在社交媒体数据和网络内容分析中具有应用潜力。

Abstract: Rapid expansion of social media platforms such as X (formerly Twitter), Facebook, and Reddit has enabled large-scale analysis of public perceptions on diverse topics, including social issues, politics, natural disasters, and consumer sentiment. Topic modeling is a widely used approach for uncovering latent themes in text data, typically framed as an unsupervised classification task. However, traditional models, originally designed for longer and more formal documents, struggle with short social media posts due to limited co-occurrence statistics, fragmented semantics, inconsistent spelling, and informal language. To address these challenges, we propose a new method, TopiCLEAR: Topic extraction by CLustering Embeddings with Adaptive dimensional Reduction. Specifically, each text is embedded using Sentence-BERT (SBERT) and provisionally clustered using Gaussian Mixture Models (GMM). The clusters are then refined iteratively using a supervised projection based on linear discriminant analysis, followed by GMM-based clustering until convergence. Notably, our method operates directly on raw text, eliminating the need for preprocessing steps such as stop word removal. We evaluate our approach on four diverse datasets, 20News, AgNewsTitle, Reddit, and TweetTopic, each containing human-labeled topic information. Compared with seven baseline methods, including a recent SBERT-based method and a zero-shot generative AI method, our approach achieves the highest similarity to human-annotated topics, with significant improvements for both social media posts and online news articles. Additionally, qualitative analysis shows that our method produces more interpretable topics, highlighting its potential for applications in social media data and web content analytics.

</details>


### [21] [MASim: Multilingual Agent-Based Simulation for Social Science](https://arxiv.org/abs/2512.07195)
*Xuan Zhang,Wenxuan Zhang,Anxu Wang,See-Kiong Ng,Yang Deng*

Main category: cs.CL

TL;DR: MASim是首个支持多语言智能体交互的模拟框架，用于研究跨语言社会行为，包含公共舆论建模和媒体影响分析，并通过MAPS基准验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有智能体角色扮演模拟大多是单语言的，无法模拟真实社会中跨语言交互这一重要特性，限制了计算社会科学研究。

Method: 开发了MASim多语言智能体模拟框架，支持具有不同社会语言特征的生成式智能体进行多轮交互，包含全球公共舆论建模和媒体影响分析两个核心功能，并构建了MAPS基准数据集进行验证。

Result: 实验表明MASim能够复现社会文化现象，校准、敏感性、一致性和文化案例研究验证了其有效性，突显了多语言模拟对于可扩展、可控计算社会科学的重要性。

Conclusion: MASim填补了多语言智能体模拟的空白，为研究跨语言社会行为提供了有效工具，推动了计算社会科学的发展。

Abstract: Multi-agent role-playing has recently shown promise for studying social behavior with language agents, but existing simulations are mostly monolingual and fail to model cross-lingual interaction, an essential property of real societies. We introduce MASim, the first multilingual agent-based simulation framework that supports multi-turn interaction among generative agents with diverse sociolinguistic profiles. MASim offers two key analyses: (i) global public opinion modeling, by simulating how attitudes toward open-domain hypotheses evolve across languages and cultures, and (ii) media influence and information diffusion, via autonomous news agents that dynamically generate content and shape user behavior. To instantiate simulations, we construct the MAPS benchmark, which combines survey questions and demographic personas drawn from global population distributions. Experiments on calibration, sensitivity, consistency, and cultural case studies show that MASim reproduces sociocultural phenomena and highlights the importance of multilingual simulation for scalable, controlled computational social science.

</details>


### [22] [Parameter-Efficient Fine-Tuning with Differential Privacy for Robust Instruction Adaptation in Large Language Models](https://arxiv.org/abs/2512.06711)
*Yulin Huang,Yaxuan Luan,Jinxu Guo,Xiangchen Song,Yuchen Liu*

Main category: cs.CL

TL;DR: 提出一种结合差分隐私噪声分配与梯度裁剪的参数高效微调方法，在保护隐私的同时提高大语言模型指令微调的效率


<details>
  <summary>Details</summary>
Motivation: 解决大规模语言模型指令微调中的隐私保护与效率问题，传统方法在隐私预算消耗和训练稳定性方面存在不足

Method: 冻结主干模型，通过低维投影子空间更新参数，在梯度计算中引入裁剪和自适应噪声分配，构建梯度约束、噪声分配和参数投影的统一框架

Result: 在准确性、隐私预算和参数效率方面优于基线模型，在多样化和不确定数据条件下保持稳定性能

Conclusion: 丰富了差分隐私与参数高效微调的理论整合，展示了在指令任务中的实际适应性，为复杂指令环境下的安全训练提供了可行解决方案

Abstract: This study addresses the issues of privacy protection and efficiency in instruction fine-tuning of large-scale language models by proposing a parameter-efficient method that integrates differential privacy noise allocation with gradient clipping in a collaborative optimization framework. The method keeps the backbone model frozen and updates parameters through a low-dimensional projection subspace, while introducing clipping and adaptive noise allocation during gradient computation. This design reduces privacy budget consumption and ensures training stability and robustness. The unified framework combines gradient constraints, noise allocation, and parameter projection, effectively mitigating performance fluctuations and privacy risks in multi-task instruction scenarios. Experiments are conducted across hyperparameter, environment, and data sensitivity dimensions. Results show that the method outperforms baseline models in accuracy, privacy budget, and parameter efficiency, and maintains stable performance under diverse and uncertain data conditions. The findings enrich the theoretical integration of differential privacy and parameter-efficient fine-tuning and demonstrate its practical adaptability in instruction tasks, providing a feasible solution for secure training in complex instruction environments.

</details>


### [23] ["The Dentist is an involved parent, the bartender is not": Revealing Implicit Biases in QA with Implicit BBQ](https://arxiv.org/abs/2512.06732)
*Aarushi Wagh,Saniya Srivastava*

Main category: cs.CL

TL;DR: ImplicitBBQ是一个新的基准测试，扩展了BBQ，通过隐式线索评估LLM中的偏见，发现GPT-4o在隐式提示下性能下降，揭示了现有显式基准无法检测的隐式偏见。


<details>
  <summary>Details</summary>
Motivation: 现有偏见基准主要依赖显式线索（直接声明受保护属性），但现实世界互动中偏见往往是隐式的（通过名字、文化线索等推断）。这种关键疏忽造成了公平性评估的盲点。

Method: 扩展Bias Benchmark for QA (BBQ)，创建ImplicitBBQ基准，包含6个类别的隐式线索受保护属性，用于评估LLM在隐式偏见下的表现。

Result: 评估GPT-4o在ImplicitBBQ上显示：与显式BBQ提示相比，性能显著下降，"性取向"子类别准确率下降达7%，其他大多数类别也出现一致下降，表明当前LLM存在显式基准无法检测的隐式偏见。

Conclusion: ImplicitBBQ为NLP领域的公平性评估提供了关键工具，能够更细致地检测LLM中的隐式偏见，弥补现有显式基准的不足。

Abstract: Existing benchmarks evaluating biases in large language models (LLMs) primarily rely on explicit cues, declaring protected attributes like religion, race, gender by name. However, real-world interactions often contain implicit biases, inferred subtly through names, cultural cues, or traits. This critical oversight creates a significant blind spot in fairness evaluation. We introduce ImplicitBBQ, a benchmark extending the Bias Benchmark for QA (BBQ) with implicitly cued protected attributes across 6 categories. Our evaluation of GPT-4o on ImplicitBBQ illustrates troubling performance disparity from explicit BBQ prompts, with accuracy declining up to 7% in the "sexual orientation" subcategory and consistent decline located across most other categories. This indicates that current LLMs contain implicit biases undetected by explicit benchmarks. ImplicitBBQ offers a crucial tool for nuanced fairness evaluation in NLP.

</details>


### [24] [A Patient-Doctor-NLP-System to contest inequality for less privileged](https://arxiv.org/abs/2512.06734)
*Subrit Dikshit,Ritu Tiwari,Priyank Jain*

Main category: cs.CL

TL;DR: PDFTEMRA：一种紧凑的Transformer架构，集成模型蒸馏、频域调制、集成学习和随机激活模式，用于低资源医疗NLP应用，特别是为印地语使用者和视障用户提供医疗协助。


<details>
  <summary>Details</summary>
Motivation: 尽管迁移学习加速了大语言模型的发展，但在资源受限的真实医疗场景中训练和部署这些大型模型仍然具有挑战性。本研究旨在解决视障用户和印地语等低资源语言使用者在农村环境中医疗协助需求有限的问题。

Method: 提出PDFTEMRA（Performant Distilled Frequency Transformer Ensemble Model with Random Activations），一种紧凑的Transformer架构，集成模型蒸馏、频域调制、集成学习和随机激活模式，以降低计算成本同时保持语言理解性能。模型在针对印地语和可访问性场景定制的医疗问答和咨询数据集上进行训练和评估。

Result: PDFTEMRA在显著降低计算需求的同时，实现了与标准NLP最先进模型基线相当的性能，表明其适用于可访问、包容性的低资源医疗NLP应用。

Conclusion: PDFTEMRA通过创新的架构设计，成功解决了在资源受限的医疗环境中部署高效NLP模型的挑战，特别适用于服务视障用户和低资源语言社区，为实现包容性医疗AI提供了可行方案。

Abstract: Transfer Learning (TL) has accelerated the rapid development and availability of large language models (LLMs) for mainstream natural language processing (NLP) use cases. However, training and deploying such gigantic LLMs in resource-constrained, real-world healthcare situations remains challenging. This study addresses the limited support available to visually impaired users and speakers of low-resource languages such as Hindi who require medical assistance in rural environments. We propose PDFTEMRA (Performant Distilled Frequency Transformer Ensemble Model with Random Activations), a compact transformer-based architecture that integrates model distillation, frequency-domain modulation, ensemble learning, and randomized activation patterns to reduce computational cost while preserving language understanding performance. The model is trained and evaluated on medical question-answering and consultation datasets tailored to Hindi and accessibility scenarios, and its performance is compared against standard NLP state-of-the-art model baselines. Results demonstrate that PDFTEMRA achieves comparable performance with substantially lower computational requirements, indicating its suitability for accessible, inclusive, low-resource medical NLP applications.

</details>


### [25] [One Word Is Not Enough: Simple Prompts Improve Word Embeddings](https://arxiv.org/abs/2512.06744)
*Rajeev Ranjan*

Main category: cs.CL

TL;DR: 在单词前添加语义提示（如"meaning: {word}"）能显著提升文本嵌入模型在单词相似度任务上的表现，无需训练即可超越传统静态嵌入方法


<details>
  <summary>Details</summary>
Motivation: 文本嵌入模型主要针对句子级应用进行评估，其在孤立单词上的表现较少被研究。研究者希望探索如何改进这些模型在单词相似度任务上的性能。

Method: 在7个文本嵌入模型（包括OpenAI、Cohere、Voyage AI等公司的模型）上，通过在单词前添加语义提示（如"meaning: {word}"或"Represent the semantic concept: {word}"），然后在3个标准基准（SimLex-999、WordSim-353、MEN-3000）上评估单词相似度相关性。

Result: 语义提示显著提升了所有模型的性能：在SimLex-999上相关性提升高达+0.29；某些在裸单词上完全失败的模型（相关性=0）通过提示恢复性能（提升+0.73）。最佳结果在SimLex-999上达到0.692相关性，超越了传统静态嵌入方法如Word2Vec（0.40）和LexVec（0.48）。

Conclusion: 简单的零样本语义提示技术能显著提升文本嵌入模型在单词相似度任务上的表现，无需训练即可超越传统静态嵌入方法，为纯嵌入方法建立了新的最先进水平。

Abstract: Text embedding models are designed for sentence-level applications like retrieval and semantic similarity, and are primarily evaluated on sentence-level benchmarks. Their behavior on isolated words is less understood. We show that simply prepending semantic prompts to words before embedding substantially improves word similarity correlations. Testing 7 text embedding models, including text-embedding-3-large (OpenAI), embed-english-v3.0 (Cohere), voyage-3(Voyage AI), all-mpnet-base-v2, and Qwen3-Embedding-8B, on 3 standard benchmarks (SimLex-999, WordSim-353, MEN-3000), we find that prompts like "meaning: {word}" or "Represent the semantic concept: {word}" improve Spearman correlations by up to +0.29 on SimLex-999. Some models fail completely on bare words (correlation = 0) but recover with prompts (+0.73 improvement). Our best results achieve correlation = 0.692 on SimLex-999 with embed-english-v3.0 (Cohere), correlation = 0.811 on WordSim-353, and correlation = 0.855 on MEN-3000 with text-embedding-3-large (OpenAI). These results outperform classic static embeddings like Word2Vec (correlation = 0.40) and even the best static method LexVec (correlation = 0.48) on SimLex-999, establishing a new state-of-the-art for pure embedding methods. This zero-shot technique requires no training and works with any text embedding model.

</details>


### [26] [Becoming Experienced Judges: Selective Test-Time Learning for Evaluators](https://arxiv.org/abs/2512.06751)
*Seungyeon Jwa,Daechul Ahn,Reokyoung Kim,Dongyeop Kang,Jonghyun Choi*

Main category: cs.CL

TL;DR: 提出LWE框架，让LLM评估器在推理时通过自生成反馈改进元提示，实现顺序学习；进一步提出Selective LWE，只在自不一致案例上更新元提示，提高成本效益。


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估器存在两个问题：1) 独立处理每个案例，无法积累经验；2) 使用固定提示，缺乏样本特定的评估标准。需要让评估器在推理时能够学习和改进。

Method: 提出Learning While Evaluating (LWE)框架：维护一个演化的元提示，为每个样本生成特定评估指令，并通过自生成反馈改进元提示。进一步提出Selective LWE，只在评估器自不一致的案例上更新元提示，聚焦计算资源。

Result: 在两个成对比较基准测试中，Selective LWE优于强基线方法，证明评估器可以通过简单的选择性更新在顺序测试中改进，从最困难的案例中学习最多。

Conclusion: 评估器可以在推理时通过顺序学习改进，选择性更新策略既保留了顺序学习的优势，又显著提高了成本效益，为LLM评估器的持续改进提供了有效框架。

Abstract: Automatic evaluation with large language models, commonly known as LLM-as-a-judge, is now standard across reasoning and alignment tasks. Despite evaluating many samples in deployment, these evaluators typically (i) treat each case independently, missing the opportunity to accumulate experience, and (ii) rely on a single fixed prompt for all cases, neglecting the need for sample-specific evaluation criteria. We introduce Learning While Evaluating (LWE), a framework that allows evaluators to improve sequentially at inference time without requiring training or validation sets. LWE maintains an evolving meta-prompt that (i) produces sample-specific evaluation instructions and (ii) refines itself through self-generated feedback. Furthermore, we propose Selective LWE, which updates the meta-prompt only on self-inconsistent cases, focusing computation where it matters most. This selective approach retains the benefits of sequential learning while being far more cost-effective. Across two pairwise comparison benchmarks, Selective LWE outperforms strong baselines, empirically demonstrating that evaluators can improve during sequential testing with a simple selective update, learning most from the cases they struggle with.

</details>


### [27] [From Next-Token to Next-Block: A Principled Adaptation Path for Diffusion LLMs](https://arxiv.org/abs/2512.06776)
*Yuchuan Tian,Yuchen Liang,Jiacheng Sun,Shuo Zhang,Guangwen Yang,Yingte Shu,Sibo Fang,Tianyu Guo,Kai Han,Chao Xu,Hanting Chen,Xinghao Chen,Yunhe Wang*

Main category: cs.CL

TL;DR: 提出NBDiff方法，将自回归语言模型高效适配到块扩散模型，避免从头训练扩散语言模型的高成本，在7B规模上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 自回归解码存在顺序生成的吞吐瓶颈，扩散语言模型支持并行生成但从头训练成本高昂。现有适配方法未能解决自回归因果性与块扩散双向性之间的根本不匹配问题。

Method: 将自回归视为块大小=1的块扩散，设计上下文因果注意力掩码、高效并行适配流程、辅助自回归损失以最大化数据利用，并逐步增加生成块大小。

Result: NBDiff-7B在通用知识、数学和代码基准测试中超越强基线，在7B级扩散语言模型中达到最先进性能，继承了长上下文建模和推理能力。

Conclusion: 原则性的自回归到块扩散适配是训练扩散语言模型的有效且计算高效的替代方案，避免了从头训练的高成本。

Abstract: Large language models (LLMs) excel at generation but dominant autoregressive (AR) decoding is inherently sequential, creating a throughput bottleneck. Diffusion Language Models (DLMs)--especially block-wise variants--enable parallel generation and intra-block bidirectional reasoning, yet training large DLMs from scratch is costly and wastes the knowledge in mature AR checkpoints. Prior "adaptation" attempts either modify logits or randomly grow attention masks to full-sequence diffusion, or simply transplant AR weights into a block-diffusion recipe, leaving a fundamental mismatch between AR causality and block-wise bidirectionality unaddressed. We reframe adaptation as a intra-paradigm path from AR to Block-Diffusion by viewing AR as Block-Diffusion with blocksize=1. Concretely, we design the pathway of adaptation as follows: we use a context-causal attention mask (causal in context, bidirectional only within the active block), an efficient parallel adaptation procedure, an auxiliary AR loss to maximize data utilization and retain pretrained knowledge, and gradual increment of the generation block size. The recipe integrates cleanly with masked block-diffusion and maintains train-inference consistency. Built on these components, NBDiff-7B (Base and Instruct) could inherit the long-context modeling and reasoning capabilities, and achieve state-of-the-art performance among the 7B-class DLMs, delivering strong gains on general-knowledge, math, and code benchmarks over strong baselines. These results demonstrate that principled AR-to-block-diffusion adaptation is an effective and compute-efficient alternative to training DLMs from scratch. Codes: https://github.com/YuchuanTian/NBDiff.

</details>


### [28] [LLM4SFC: Sequential Function Chart Generation via Large Language Models](https://arxiv.org/abs/2512.06787)
*Ofek Glick,Vladimir Tchuiev,Marah Ghoummaid,Michal Moshkovitz,Dotan Di-Castro*

Main category: cs.CL

TL;DR: LLM4SFC框架首次实现从自然语言描述生成可执行的顺序功能图(SFC)，解决了图形化PLC编程语言生成难题，在真实工业数据集上达到75%-94%的成功率。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型已用于生成文本化PLC编程语言（如结构化文本），但IEC 61131-3标准的图形化语言（如顺序功能图SFC）仍未被充分探索。生成SFC面临挑战，因为其图形化特性和嵌入的ST动作与标准生成技术不兼容，常导致生成不可执行的代码。

Method: LLM4SFC框架包含三个组件：(1) 简化的结构化表示，捕获拓扑结构和内联ST代码，减少文本冗余；(2) 微调和少样本检索增强生成(RAG)，使模型符合SFC编程规范；(3) 结构化生成方法，实时修剪非法标记，确保符合SFC文本格式。

Result: 在自动化制造项目的真实SFC数据集上评估，使用开源和专有大语言模型。结果显示LLM4SFC可靠地生成语法有效的SFC程序，有效桥接图形化和文本化PLC语言，生成成功率达到75%-94%。

Conclusion: LLM4SFC首次实现了从自然语言到可执行SFC的生成，为自动化工业编程铺平了道路，解决了图形化PLC编程语言生成的挑战。

Abstract: While Large Language Models (LLMs) are increasingly used for synthesizing textual PLC programming languages like Structured Text (ST) code, other IEC 61131-3 standard graphical languages like Sequential Function Charts (SFCs) remain underexplored. Generating SFCs is challenging due to graphical nature and ST actions embedded within, which are not directly compatible with standard generation techniques, often leading to non-executable code that is incompatible with industrial tool-chains In this work, we introduce LLM4SFC, the first framework to receive natural-language descriptions of industrial workflows and provide executable SFCs. LLM4SFC is based on three components: (i) A reduced structured representation that captures essential topology and in-line ST and reduced textual verbosity; (ii) Fine-tuning and few-shot retrieval-augmented generation (RAG) for alignment with SFC programming conventions; and (iii) A structured generation approach that prunes illegal tokens in real-time to ensure compliance with the textual format of SFCs. We evaluate LLM4SFC on a dataset of real-world SFCs from automated manufacturing projects, using both open-source and proprietary LLMs. The results show that LLM4SFC reliably generates syntactically valid SFC programs effectively bridging graphical and textual PLC languages, achieving a generation generation success of 75% - 94%, paving the way for automated industrial programming.

</details>


### [29] [Large Language Model-Based Generation of Discharge Summaries](https://arxiv.org/abs/2512.06812)
*Tiago Rodrigues,Carla Teixeira Lopes*

Main category: cs.CL

TL;DR: 研究探索使用五种大语言模型（包括开源和专有模型）自动生成出院小结，发现专有模型特别是Gemini在单样本提示下表现最佳，开源模型虽有潜力但存在幻觉和重复信息问题。


<details>
  <summary>Details</summary>
Motivation: 出院小结包含丰富的患者信息，对医疗护理至关重要。自动化生成出院小结可以显著减少医护人员的工作量，减少错误，并确保关键患者信息易于获取和可操作。

Method: 使用五种大语言模型（开源模型：Mistral、Llama 2；专有系统：GPT-3、GPT-4、Gemini 1.5 Pro），基于MIMIC-III数据集中的摘要和笔记。采用精确匹配、软重叠和无参考指标进行评估。

Result: 专有模型，特别是使用单样本提示的Gemini，表现最佳，生成的摘要与黄金标准摘要相似度最高。开源模型虽然表现出潜力（特别是微调后的Mistral），但性能落后，经常出现幻觉和重复信息问题。临床专家的人工评估证实了专有模型生成的摘要具有实际应用价值。

Conclusion: 尽管存在幻觉和缺失信息等挑战，研究结果表明大语言模型（特别是专有模型）是自动生成出院小结的有前途的候选方案，前提是确保数据隐私。

Abstract: Discharge Summaries are documents written by medical professionals that detail a patient's visit to a care facility. They contain a wealth of information crucial for patient care, and automating their generation could significantly reduce the effort required from healthcare professionals, minimize errors, and ensure that critical patient information is easily accessible and actionable. In this work, we explore the use of five Large Language Models on this task, from open-source models (Mistral, Llama 2) to proprietary systems (GPT-3, GPT-4, Gemini 1.5 Pro), leveraging MIMIC-III summaries and notes. We evaluate them using exact-match, soft-overlap, and reference-free metrics. Our results show that proprietary models, particularly Gemini with one-shot prompting, outperformed others, producing summaries with the highest similarity to the gold-standard ones. Open-source models, while promising, especially Mistral after fine-tuning, lagged in performance, often struggling with hallucinations and repeated information. Human evaluation by a clinical expert confirmed the practical utility of the summaries generated by proprietary models. Despite the challenges, such as hallucinations and missing information, the findings suggest that LLMs, especially proprietary models, are promising candidates for automatic discharge summary generation as long as data privacy is ensured.

</details>


### [30] [CAuSE: Decoding Multimodal Classifiers using Faithful Natural Language Explanation](https://arxiv.org/abs/2512.06814)
*Dibyanayan Bandyopadhyay,Soham Bhattacharjee,Mohammed Hasanuzzaman,Asif Ekbal*

Main category: cs.CL

TL;DR: CAuSE是一个新颖的框架，通过因果抽象和模拟解释为预训练的多模态分类器生成忠实于模型内部决策过程的自然语言解释。


<details>
  <summary>Details</summary>
Motivation: 多模态分类器通常被视为黑盒模型，现有的解释技术中很少有像自然语言解释那样直观易懂的。为了建立信任，这些解释必须忠实反映分类器的内部决策行为（忠实性）。

Method: 提出CAuSE框架，通过交换干预训练，形成底层分类器的因果抽象，为任何预训练的多模态分类器生成忠实的自然语言解释。

Result: CAuSE在跨数据集和模型的广泛实证评估中表现出良好的泛化能力，在重新设计的多模态因果忠实性度量上超越其他方法，并通过定性分析验证其优势。

Conclusion: CAuSE能够为多模态分类器生成忠实的自然语言解释，通过详细的错误分析指出了其失败案例，为可复现性提供了代码开源。

Abstract: Multimodal classifiers function as opaque black box models. While several techniques exist to interpret their predictions, very few of them are as intuitive and accessible as natural language explanations (NLEs). To build trust, such explanations must faithfully capture the classifier's internal decision making behavior, a property known as faithfulness. In this paper, we propose CAuSE (Causal Abstraction under Simulated Explanations), a novel framework to generate faithful NLEs for any pretrained multimodal classifier. We demonstrate that CAuSE generalizes across datasets and models through extensive empirical evaluations. Theoretically, we show that CAuSE, trained via interchange intervention, forms a causal abstraction of the underlying classifier. We further validate this through a redesigned metric for measuring causal faithfulness in multimodal settings. CAuSE surpasses other methods on this metric, with qualitative analysis reinforcing its advantages. We perform detailed error analysis to pinpoint the failure cases of CAuSE. For replicability, we make the codes available at https://github.com/newcodevelop/CAuSE

</details>


### [31] [AquaFusionNet: Lightweight VisionSensor Fusion Framework for Real-Time Pathogen Detection and Water Quality Anomaly Prediction on Edge Devices](https://arxiv.org/abs/2512.06848)
*Sepyan Purnama Kristanto,Lutfi Hakim,Hermansyah*

Main category: cs.CL

TL;DR: AquaFusionNet是一个轻量级跨模态框架，统一微生物显微图像和水质传感器数据，用于实时监测小型饮用水系统的微生物污染，在边缘设备上实现高效检测。


<details>
  <summary>Details</summary>
Motivation: 现有监测工具只能捕捉微生物污染的片段信息，显微图像和物理化学传感器数据需要分开解读，导致实时决策不可靠。在低收入和中等收入地区，小型饮用水系统的微生物污染波动迅速，需要更有效的监测方法。

Method: 提出AquaFusionNet框架，通过门控交叉注意力机制学习微生物外观与传感器动态之间的统计依赖关系。使用专门设计的低功耗硬件，基于新构建的AquaMicro12K数据集（12,846张饮用水环境标注显微图像）进行训练。

Result: 在印度尼西亚东爪哇7个设施部署6个月，处理184万帧图像，污染事件检测达到94.8% mAP@0.5，异常预测准确率96.3%，在Jetson Nano上功耗仅4.8W。相比其他轻量级检测器，在相同或更低功耗下提供更高准确性。

Conclusion: 跨模态耦合减少了单模态检测器的常见故障模式，特别是在污垢、浊度峰值和不一致光照条件下。所有模型、数据和硬件设计已开源，促进分散式水安全基础设施的复制和适应。

Abstract: Evidence from many low and middle income regions shows that microbial contamination in small scale drinking water systems often fluctuates rapidly, yet existing monitoring tools capture only fragments of this behaviour. Microscopic imaging provides organism level visibility, whereas physicochemical sensors reveal shortterm changes in water chemistry; in practice, operators must interpret these streams separately, making realtime decision-making unreliable. This study introduces AquaFusionNet, a lightweight cross-modal framework that unifies both information sources inside a single edge deployable model. Unlike prior work that treats microscopic detection and water quality prediction as independent tasks, AquaFusionNet learns the statistical dependencies between microbial appearance and concurrent sensor dynamics through a gated crossattention mechanism designed specifically for lowpower hardware. The framework is trained on AquaMicro12K, a new dataset comprising 12,846 annotated 1000 micrographs curated for drinking water contexts, an area where publicly accessible microscopic datasets are scarce. Deployed for six months across seven facilities in East Java, Indonesia, the system processed 1.84 million frames and consistently detected contamination events with 94.8% mAP@0.5 and 96.3% anomaly prediction accuracy, while operating at 4.8 W on a Jetson Nano. Comparative experiments against representative lightweight detectors show that AquaFusionNet provides higher accuracy at comparable or lower power, and field results indicate that cross-modal coupling reduces common failure modes of unimodal detectors, particularly under fouling, turbidity spikes, and inconsistent illumination. All models, data, and hardware designs are released openly to facilitate replication and adaptation in decentralized water safety infrastructures.

</details>


### [32] [Rhea: Role-aware Heuristic Episodic Attention for Conversational LLMs](https://arxiv.org/abs/2512.06869)
*Wanyang Hong,Zhaoning Zhang,Yi Chen,Libo Zhang,Baihui Liu,Linbo Qiao,Zhiliang Tian,Dongsheng Li*

Main category: cs.CL

TL;DR: Rhea框架通过将对话历史解耦为指令记忆和情景记忆两个模块，解决了LLM在多轮对话中性能衰减的问题，实现了16%的相对性能提升。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在单轮任务上表现出色，但在多轮对话中性能会逐渐衰减。作者将这种现象定义为累积上下文衰减，由注意力污染、稀释和漂移引起。

Method: 提出Rhea框架，将对话历史解耦为两个功能独立的记忆模块：指令记忆（持久存储全局约束）和情景记忆（动态管理用户-模型交互）。通过优先级注意力机制构建高信噪比上下文。

Result: 在多个多轮对话基准测试中，Rhea缓解了性能衰减，在10分制上提高了1.04分（相对基线提升16%），并在长时交互中保持接近完美的指令保真度（IAR > 8.1）。

Conclusion: Rhea为构建更精确、指令一致的对话式LLM提供了一个原则性且有效的框架，通过解耦记忆模块有效解决了多轮对话中的累积上下文衰减问题。

Abstract: Large Language Models (LLMs) have achieved remarkable performance on single-turn tasks, yet their effectiveness deteriorates in multi-turn conversations. We define this phenomenon as cumulative contextual decay - a progressive degradation of contextual integrity caused by attention pollution, dilution, and drift. To address this challenge, we propose Rhea (Role-aware Heuristic Episodic Attention), a novel framework that decouples conversation history into two functionally independent memory modules: (1) an Instructional Memory (IM) that persistently stores high-fidelity global constraints via a structural priority mechanism, and (2) an Episodic Memory (EM) that dynamically manages user-model interactions via asymmetric noise control and heuristic context retrieval. During inference, Rhea constructs a high signal-to-noise context by applying its priority attention: selectively integrating relevant episodic information while always prioritizing global instructions. To validate this approach, experiments on multiple multi-turn conversation benchmarks - including MT-Eval and Long-MT-Bench+ - show that Rhea mitigates performance decay and improves overall accuracy by 1.04 points on a 10-point scale (a 16% relative gain over strong baselines). Moreover, Rhea maintains near-perfect instruction fidelity (IAR > 8.1) across long-horizon interactions. These results demonstrate that Rhea provides a principled and effective framework for building more precise, instruction-consistent conversational LLMs.

</details>


### [33] [An Analysis of Large Language Models for Simulating User Responses in Surveys](https://arxiv.org/abs/2512.06874)
*Ziyun Yu,Yiru Zhou,Chen Zhao,Hongyi Wen*

Main category: cs.CL

TL;DR: LLMs在模拟用户观点时存在偏见，难以准确代表不同人口统计和文化背景的用户。本文提出的CLAIMSIM方法虽然能增加回答多样性，但仍无法准确模拟用户。


<details>
  <summary>Details</summary>
Motivation: LLMs（特别是经过RLHF训练的模型）倾向于偏向主流观点，这引发了对它们能否准确代表不同人口统计和文化背景用户的担忧。需要研究LLMs在跨领域调查问题中模拟人类响应的能力。

Method: 研究比较了直接提示和思维链提示两种方法，并提出了CLAIMSIM方法，该方法从LLM的参数知识中提取观点作为上下文输入。在调查问题回答任务上进行实验分析。

Result: 虽然CLAIMSIM能产生更多样化的回答，但两种方法都难以准确模拟用户。分析发现两个关键限制：1）LLMs倾向于在不同人口统计特征间保持固定观点，生成单一视角的主张；2）面对相互矛盾的主张时，LLMs难以推理人口统计特征间的细微差异，限制了它们针对特定用户画像调整响应的能力。

Conclusion: LLMs在模拟多样化用户观点方面存在显著局限性，特别是在处理人口统计特征的细微差异和适应特定用户画像方面。需要进一步研究改进LLMs在模拟用户观点方面的能力。

Abstract: Using Large Language Models (LLMs) to simulate user opinions has received growing attention. Yet LLMs, especially trained with reinforcement learning from human feedback (RLHF), are known to exhibit biases toward dominant viewpoints, raising concerns about their ability to represent users from diverse demographic and cultural backgrounds. In this work, we examine the extent to which LLMs can simulate human responses to cross-domain survey questions through direct prompting and chain-of-thought prompting. We further propose a claim diversification method CLAIMSIM, which elicits viewpoints from LLM parametric knowledge as contextual input. Experiments on the survey question answering task indicate that, while CLAIMSIM produces more diverse responses, both approaches struggle to accurately simulate users. Further analysis reveals two key limitations: (1) LLMs tend to maintain fixed viewpoints across varying demographic features, and generate single-perspective claims; and (2) when presented with conflicting claims, LLMs struggle to reason over nuanced differences among demographic features, limiting their ability to adapt responses to specific user profiles.

</details>


### [34] [Automated PRO-CTCAE Symptom Selection based on Prior Adverse Event Profiles](https://arxiv.org/abs/2512.06919)
*Francois Vandenhende,Anna Georgiou,Michalis Georgiou,Theodoros Psaras,Ellie Karekla*

Main category: cs.CL

TL;DR: 开发了基于历史安全数据和MedDRA语义的自动化方法，用于选择最小但全面的PRO-CTCAE症状子集，以平衡信号覆盖和患者负担


<details>
  <summary>Details</summary>
Motivation: PRO-CTCAE系统包含大量症状项目，传统选择方法依赖预期毒性谱，但选择过多会增加患者负担降低依从性，选择过少可能遗漏重要安全信号，需要更科学的方法来平衡覆盖范围和患者负担

Method: 将PRO-CTCAE症状映射到MedDRA首选术语，编码到Safeterm高维语义空间，结合相关性和发生率计算效用函数，应用谱分析识别正交医学概念集，按重要性排序并基于解释信息确定截断点

Result: 该方法已集成到Safeterm试验安全应用中，通过模拟和肿瘤学案例研究验证了性能，能够提供客观可重复的PRO-CTCAE设计方法

Conclusion: 自动化方法利用MedDRA语义和历史数据简化PRO-CTCAE设计，平衡信号覆盖与患者负担，为临床试验提供更科学的症状选择工具

Abstract: The PRO-CTCAE is an NCI-developed patient-reported outcome system for capturing symptomatic adverse events in oncology trials. It comprises a large library drawn from the CTCAE vocabulary, and item selection for a given trial is typically guided by expected toxicity profiles from prior data. Selecting too many PRO-CTCAE items can burden patients and reduce compliance, while too few may miss important safety signals. We present an automated method to select a minimal yet comprehensive PRO-CTCAE subset based on historical safety data. Each candidate PRO-CTCAE symptom term is first mapped to its corresponding MedDRA Preferred Terms (PTs), which are then encoded into Safeterm, a high-dimensional semantic space capturing clinical and contextual diversity in MedDRA terminology. We score each candidate PRO item for relevance to the historical list of adverse event PTs and combine relevance and incidence into a utility function. Spectral analysis is then applied to the combined utility and diversity matrix to identify an orthogonal set of medical concepts that balances relevance and diversity. Symptoms are rank-ordered by importance, and a cut-off is suggested based on the explained information. The tool is implemented as part of the Safeterm trial-safety app. We evaluate its performance using simulations and oncology case studies in which PRO-CTCAE was employed. This automated approach can streamline PRO-CTCAE design by leveraging MedDRA semantics and historical data, providing an objective and reproducible method to balance signal coverage against patient burden.

</details>


### [35] [XAM: Interactive Explainability for Authorship Attribution Models](https://arxiv.org/abs/2512.06924)
*Milad Alshomary,Anisha Bhatnagar,Peter Zeng,Smaranda Muresan,Owen Rambow,Kathleen McKeown*

Main category: cs.CL

TL;DR: IXAM是一个交互式可解释性框架，用于作者归属模型，允许用户探索嵌入空间并以多层次粒度构建模型预测的写作风格特征解释。


<details>
  <summary>Details</summary>
Motivation: 现有的作者归属模型缺乏交互式可解释性工具，用户无法深入理解模型如何基于写作风格做出预测决策。

Method: 开发了IXAM交互式框架，基于嵌入的作者归属模型，允许用户交互式探索嵌入空间，并以不同粒度构建写作风格特征的解释。

Result: 通过用户评估，证明该框架相比预定义的风格解释具有更高价值，能更好地帮助用户理解模型预测。

Conclusion: IXAM为作者归属模型提供了有效的交互式可解释性解决方案，增强了用户对模型决策过程的理解和信任。

Abstract: We present IXAM, an Interactive eXplainability framework for Authorship Attribution Models. Given an authorship attribution (AA) task and an embedding-based AA model, our tool enables users to interactively explore the model's embedding space and construct an explanation of the model's prediction as a set of writing style features at different levels of granularity. Through a user evaluation, we demonstrate the value of our framework compared to predefined stylistic explanations.

</details>


### [36] [Progress Ratio Embeddings: An Impatience Signal for Robust Length Control in Neural Text Generation](https://arxiv.org/abs/2512.06938)
*Ivanhoé Botcazou,Tassadit Amghar,Sylvain Lamprier,Frédéric Saubion*

Main category: cs.CL

TL;DR: 本文提出Progress Ratio Embeddings (PRE)方法，通过连续三角函数信号实现稳健的文本生成长度控制，解决了现有方法在超出训练分布时的不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 现代神经语言模型在文本生成方面取得高精度，但对生成长度的精确控制仍不成熟。现有基于反向位置嵌入(RPE)的方法在控制超出训练分布的长度时存在限制，特别是使用与绝对剩余标记数相关的离散倒计时信号会导致不稳定性。

Method: 提出Progress Ratio Embeddings (PRE)方法，使用连续嵌入与三角函数不耐烦信号相结合。PRE无缝集成到标准Transformer架构中，提供稳定的长度保真度，同时不影响标准评估指标下的文本准确性。

Result: 在两个广泛使用的新闻摘要基准测试中验证了PRE的有效性。实验表明PRE能够提供稳健的长度控制，在未见目标长度上具有良好的泛化能力，且不会降低文本生成质量。

Conclusion: PRE方法通过连续三角函数信号实现了比现有离散方法更稳健的文本生成长度控制，解决了超出训练分布时的不稳定问题，为精确控制生成长度提供了有效解决方案。

Abstract: Modern neural language models achieve high accuracy in text generation, yet precise control over generation length remains underdeveloped. In this paper, we first investigate a recent length control method based on Reverse Positional Embeddings (RPE) and show its limits when control is requested beyond the training distribution. In particular, using a discrete countdown signal tied to the absolute remaining token count leads to instability. To provide robust length control, we introduce Progress Ratio Embeddings (PRE), as continuous embeddings tied to a trigonometric impatience signal. PRE integrates seamlessly into standard Transformer architectures, providing stable length fidelity without degrading text accuracy under standard evaluation metrics. We further show that PRE generalizes well to unseen target lengths. Experiments on two widely used news-summarization benchmarks validate these findings.

</details>


### [37] [Prompting-in-a-Series: Psychology-Informed Contents and Embeddings for Personality Recognition With Decoder-Only Models](https://arxiv.org/abs/2512.06991)
*Jing Jie Tan,Ban-Hoe Kwan,Danny Wee-Kiat Ng,Yan-Chai Hum,Anissa Mokraoui,Shih-Yu Lo*

Main category: cs.CL

TL;DR: PICEPR是一种基于心理学内容嵌入的人格识别新算法，通过内容和嵌入两条流水线，利用模块化解码器LLM进行内容生成和人格特征提取，在人格识别任务上实现了5-15%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在各种NLP任务中表现出色，但在人格识别任务上仍有改进空间。研究者希望开发一种专门针对人格识别的新算法，利用心理学知识增强模型的人格特征提取能力。

Method: 提出PICEPR算法，包含两条流水线：(a)内容流水线：使用模块化解码器LLM生成或总结内容；(b)嵌入流水线：将生成的内容用于人格识别。算法结合心理学知识进行内容嵌入，作为人格特征提取器和人格丰富内容生成器。

Result: PICEPR在人格识别任务上实现了新的最先进性能，相比现有方法提升了5-15%。同时对比了GPT-4o、Gemini和Mistral等闭源和开源模型的内容生成质量。

Conclusion: PICEPR算法通过心理学知识增强的内容嵌入方法，显著提升了人格识别性能，为LLM在人格分析领域的应用提供了有效框架。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across various natural language processing tasks. This research introduces a novel "Prompting-in-a-Series" algorithm, termed PICEPR (Psychology-Informed Contents Embeddings for Personality Recognition), featuring two pipelines: (a) Contents and (b) Embeddings. The approach demonstrates how a modularised decoder-only LLM can summarize or generate content, which can aid in classifying or enhancing personality recognition functions as a personality feature extractor and a generator for personality-rich content. We conducted various experiments to provide evidence to justify the rationale behind the PICEPR algorithm. Meanwhile, we also explored closed-source models such as \textit{gpt4o} from OpenAI and \textit{gemini} from Google, along with open-source models like \textit{mistral} from Mistral AI, to compare the quality of the generated content. The PICEPR algorithm has achieved a new state-of-the-art performance for personality recognition by 5-15\% improvement. The work repository and models' weight can be found at https://research.jingjietan.com/?q=PICEPR.

</details>


### [38] [FVA-RAG: Falsification-Verification Alignment for Mitigating Sycophantic Hallucinations](https://arxiv.org/abs/2512.07015)
*Mayank Ravishankara*

Main category: cs.CL

TL;DR: FVA-RAG通过引入对抗性检索策略和双重验证机制，将检索范式从归纳验证转向演绎证伪，有效减少RAG系统中的检索奉承问题。


<details>
  <summary>Details</summary>
Motivation: 标准RAG系统存在检索奉承的严重漏洞：当查询基于错误前提或常见误解时，向量检索器倾向于获取符合用户偏见的文档，导致模型"带引用的幻觉"。

Method: 提出FVA-RAG框架，采用对抗性检索策略生成"致命查询"来寻找矛盾证据，并引入双重验证机制，将草稿答案与"反上下文"进行权衡。

Result: 在常见误解数据集上的初步实验表明，FVA-RAG相比标准RAG基线显著提高了对抗奉承幻觉的鲁棒性。

Conclusion: FVA-RAG通过将检索范式从寻求支持的归纳验证转向寻求证伪的演绎证伪，有效充当事实生成的推理时"红队"。

Abstract: Retrieval-Augmented Generation (RAG) systems have significantly reduced hallucinations in Large Language Models (LLMs) by grounding responses in external context. However, standard RAG architectures suffer from a critical vulnerability: Retrieval Sycophancy. When presented with a query based on a false premise or a common misconception, vector-based retrievers tend to fetch documents that align with the user's bias rather than objective truth, leading the model to "hallucinate with citations."
  In this work, we introduce Falsification-Verification Alignment RAG (FVA-RAG), a framework that shifts the retrieval paradigm from Inductive Verification (seeking support) to Deductive Falsification (seeking disproof). Unlike existing "Self-Correction" methods that rely on internal consistency, FVA-RAG deploys a distinct Adversarial Retrieval Policy that actively generates "Kill Queries"-targeted search terms designed to surface contradictory evidence. We introduce a dual-verification mechanism that explicitly weighs the draft answer against this "Anti-Context." Preliminary experiments on a dataset of common misconceptions demonstrate that FVA-RAG significantly improves robustness against sycophantic hallucinations compared to standard RAG baselines, effectively acting as an inference-time "Red Team" for factual generation.

</details>


### [39] [Replicating TEMPEST at Scale: Multi-Turn Adversarial Attacks Against Trillion-Parameter Frontier Models](https://arxiv.org/abs/2512.07059)
*Richard Young*

Main category: cs.CL

TL;DR: 研究发现当前大语言模型的安全对齐在多轮对抗攻击下存在根本性脆弱性，模型规模不预测鲁棒性，而思维模式可作为有效防御手段。


<details>
  <summary>Details</summary>
Motivation: 尽管在安全对齐方面投入了大量资源，但大语言模型对复杂多轮对抗攻击的脆弱性仍未得到充分表征，且模型规模或推理模式是否影响鲁棒性尚不清楚。

Method: 使用TEMPEST多轮攻击框架评估了来自8个供应商的10个前沿模型，涵盖1000种有害行为，通过独立安全分类器自动评估，生成了超过97,000个API查询。

Result: 结果显示脆弱性谱系：6个模型攻击成功率（ASR）达96%-100%，4个模型表现出有意义的抵抗（ASR 42%-78%）；在相同架构上启用扩展推理可将ASR从97%降至42%。

Conclusion: 当前对齐技术对自适应多轮攻击存在根本性脆弱性，模型规模不预测对抗鲁棒性，而审慎推理模式是有前景的防御方向。

Abstract: Despite substantial investment in safety alignment, the vulnerability of large language models to sophisticated multi-turn adversarial attacks remains poorly characterized, and whether model scale or inference mode affects robustness is unknown. This study employed the TEMPEST multi-turn attack framework to evaluate ten frontier models from eight vendors across 1,000 harmful behaviors, generating over 97,000 API queries across adversarial conversations with automated evaluation by independent safety classifiers. Results demonstrated a spectrum of vulnerability: six models achieved 96% to 100% attack success rate (ASR), while four showed meaningful resistance, with ASR ranging from 42% to 78%; enabling extended reasoning on identical architecture reduced ASR from 97% to 42%. These findings indicate that safety alignment quality varies substantially across vendors, that model scale does not predict adversarial robustness, and that thinking mode provides a deployable safety enhancement. Collectively, this work establishes that current alignment techniques remain fundamentally vulnerable to adaptive multi-turn attacks regardless of model scale, while identifying deliberative inference as a promising defense direction.

</details>


### [40] [SETUP: Sentence-level English-To-Uniform Meaning Representation Parser](https://arxiv.org/abs/2512.07068)
*Emma Markle,Javier Gutierrez Bach,Shira Wein*

Main category: cs.CL

TL;DR: 本文提出了两种英语文本到UMR解析的方法，其中SETUP模型在AnCast和SMATCH++评分上表现最佳，显著提升了UMR自动解析能力。


<details>
  <summary>Details</summary>
Motivation: UMR（统一意义表示）是一种基于图的语义表示方法，能够捕捉文本核心意义并适用于多种语言（包括低资源语言）。虽然UMR在语言记录、低资源语言技术改进和可解释性方面有潜力，但只有在能够自动大规模生成准确UMR图时，其下游应用才能充分探索。目前文本到UMR解析的研究还很有限。

Method: 提出了两种英语文本到UMR解析的方法：1）微调现有的抽象意义表示（AMR）解析器；2）利用从通用依存关系（Universal Dependencies）转换器的方法。以先前工作作为基线进行比较。

Result: 最佳性能模型SETUP在AnCast评分上达到84分，SMATCH++评分达到91分，表明在自动UMR解析方面取得了显著进展。

Conclusion: 本文提出的方法特别是SETUP模型，显著推进了英语文本到UMR自动解析的能力，为UMR在下游应用中的大规模使用奠定了基础。

Abstract: Uniform Meaning Representation (UMR) is a novel graph-based semantic representation which captures the core meaning of a text, with flexibility incorporated into the annotation schema such that the breadth of the world's languages can be annotated (including low-resource languages). While UMR shows promise in enabling language documentation, improving low-resource language technologies, and adding interpretability, the downstream applications of UMR can only be fully explored when text-to-UMR parsers enable the automatic large-scale production of accurate UMR graphs at test time. Prior work on text-to-UMR parsing is limited to date. In this paper, we introduce two methods for English text-to-UMR parsing, one of which fine-tunes existing parsers for Abstract Meaning Representation and the other, which leverages a converter from Universal Dependencies, using prior work as a baseline. Our best-performing model, which we call SETUP, achieves an AnCast score of 84 and a SMATCH++ score of 91, indicating substantial gains towards automatic UMR parsing.

</details>


### [41] [Do Large Language Models Truly Understand Cross-cultural Differences?](https://arxiv.org/abs/2512.07075)
*Shiwei Guo,Sihang Jiang,Qianxi He,Yanghua Xiao,Jiaqing Liang,Bi Yude,Minggui He,Shimin Tao,Li Zhang*

Main category: cs.CL

TL;DR: SAGE是一个基于场景的基准测试，通过跨文化核心概念对齐和生成式任务设计来评估大语言模型的跨文化理解和推理能力，揭示了模型在跨文化理解方面的系统性局限。


<details>
  <summary>Details</summary>
Motivation: 现有评估LLMs跨文化理解能力的基准存在三个关键限制：缺乏上下文场景、跨文化概念映射不足、深度文化推理能力有限。需要一个新的基准来更全面地评估LLMs是否真正具备跨文化理解能力。

Method: 基于文化理论将跨文化能力分为九个维度，筛选210个核心概念，在15个具体现实世界场景中构建4530个测试项目，涵盖四大类跨文化情境，遵循既定的项目设计原则。

Result: SAGE数据集支持持续扩展，实验证实其可迁移到其他语言。基准测试揭示了模型在各个维度和场景中的弱点，暴露了跨文化推理的系统性局限。

Conclusion: 虽然已有进展，但大语言模型距离真正细致的跨文化理解还有相当距离。SAGE为评估和提升LLMs的跨文化能力提供了重要工具。

Abstract: In recent years, large language models (LLMs) have demonstrated strong performance on multilingual tasks. Given its wide range of applications, cross-cultural understanding capability is a crucial competency. However, existing benchmarks for evaluating whether LLMs genuinely possess this capability suffer from three key limitations: a lack of contextual scenarios, insufficient cross-cultural concept mapping, and limited deep cultural reasoning capabilities. To address these gaps, we propose SAGE, a scenario-based benchmark built via cross-cultural core concept alignment and generative task design, to evaluate LLMs' cross-cultural understanding and reasoning. Grounded in cultural theory, we categorize cross-cultural capabilities into nine dimensions. Using this framework, we curated 210 core concepts and constructed 4530 test items across 15 specific real-world scenarios, organized under four broader categories of cross-cultural situations, following established item design principles. The SAGE dataset supports continuous expansion, and experiments confirm its transferability to other languages. It reveals model weaknesses across both dimensions and scenarios, exposing systematic limitations in cross-cultural reasoning. While progress has been made, LLMs are still some distance away from reaching a truly nuanced cross-cultural understanding. In compliance with the anonymity policy, we include data and code in the supplement materials. In future versions, we will make them publicly available online.

</details>


### [42] [Leveraging KV Similarity for Online Structured Pruning in LLMs](https://arxiv.org/abs/2512.07090)
*Jungmin Lee,Gwangeun Byeon,Yulhwa Kim,Seokin Hong*

Main category: cs.CL

TL;DR: Token Filtering：一种轻量级在线结构化剪枝技术，通过联合键值相似度在推理时直接做出剪枝决策，无需校准数据，减少计算成本同时保持关键信息。


<details>
  <summary>Details</summary>
Motivation: 现有LLM剪枝方法依赖离线校准数据，可能无法泛化到不同输入，导致不稳定。需要一种无需校准数据、在推理时直接做出剪枝决策的方法。

Method: 通过联合键值相似度测量token冗余度，跳过冗余注意力计算；设计方差感知融合策略，自适应加权不同头的键值相似度，确保在高剪枝率下保留信息丰富的token。

Result: 在LLaMA-2 (7B/13B)、LLaMA-3 (8B)和Mistral (7B)上实验表明，Token Filtering优于现有结构化剪枝方法，在常识推理基准上保持准确性，在MMLU等挑战性任务上即使剪枝50%仍保持强性能。

Conclusion: Token Filtering提供了一种无需校准数据的稳定在线剪枝方案，通过联合键值相似度和方差感知融合策略，有效减少LLM推理成本同时保持模型性能。

Abstract: Pruning has emerged as a promising direction for accelerating large language model (LLM) inference, yet existing approaches often suffer from instability because they rely on offline calibration data that may not generalize across inputs. In this work, we introduce Token Filtering, a lightweight online structured pruning technique that makes pruning decisions directly during inference without any calibration data. The key idea is to measure token redundancy via joint key-value similarity and skip redundant attention computations, thereby reducing inference cost while preserving critical information. To further enhance stability, we design a variance-aware fusion strategy that adaptively weights key and value similarity across heads, ensuring that informative tokens are retained even under high pruning ratios. This design introduces no additional memory overhead and provides a more reliable criterion for token importance. Extensive experiments on LLaMA-2 (7B/13B), LLaMA-3 (8B), and Mistral (7B) demonstrate that Token Filtering consistently outperforms prior structured pruning methods, preserving accuracy on commonsense reasoning benchmarks and maintaining strong performance on challenging tasks such as MMLU, even with 50% pruning.

</details>


### [43] [DART: Leveraging Multi-Agent Disagreement for Tool Recruitment in Multimodal Reasoning](https://arxiv.org/abs/2512.07132)
*Nithin Sivakumaran,Justin Chih-Yao Chen,David Wan,Yue Zhang,Jaehong Yoon,Elias Stengel-Eskin,Mohit Bansal*

Main category: cs.CL

TL;DR: DART是一个多智能体框架，通过视觉智能体之间的辩论分歧来识别有用的视觉工具，利用工具提供的新信息和专家对齐评分来促进讨论，最终通过聚合智能体选择最佳答案。


<details>
  <summary>Details</summary>
Motivation: 专业视觉工具可以为大语言模型或视觉语言模型提供专家知识（如接地、空间推理、医学知识等），但确定何时调用哪些工具具有挑战性。现有方法在工具选择和调用时机方面存在不足。

Method: DART采用多智能体辩论框架，通过智能体间的分歧识别需要调用的视觉工具（如目标检测、OCR、空间推理等）。工具提供新信息和工具对齐的同意分数，突出与专家工具一致的智能体。最后使用聚合智能体基于智能体输出和工具信息选择最佳答案。

Result: 在四个多样化基准测试中，DART优于多智能体辩论和单智能体工具调用框架，在A-OKVQA和MMMU上分别比次优基线（带评判模型的多智能体辩论）高出3.4%和2.4%。在M3D医学数据集上比其他基线提高1.3%。文本重叠分析显示DART讨论更丰富，工具调用分布显示多样化工具被可靠使用来解决分歧。

Conclusion: DART通过多智能体辩论分歧有效识别和调用视觉工具，显著提升视觉问答性能，并能良好适应新工具和应用领域，提供比现有方法更丰富的讨论过程。

Abstract: Specialized visual tools can augment large language models or vision language models with expert knowledge (e.g., grounding, spatial reasoning, medical knowledge, etc.), but knowing which tools to call (and when to call them) can be challenging. We introduce DART, a multi-agent framework that uses disagreements between multiple debating visual agents to identify useful visual tools (e.g., object detection, OCR, spatial reasoning, etc.) that can resolve inter-agent disagreement. These tools allow for fruitful multi-agent discussion by introducing new information, and by providing tool-aligned agreement scores that highlight agents in agreement with expert tools, thereby facilitating discussion. We utilize an aggregator agent to select the best answer by providing the agent outputs and tool information. We test DART on four diverse benchmarks and show that our approach improves over multi-agent debate as well as over single agent tool-calling frameworks, beating the next-strongest baseline (multi-agent debate with a judge model) by 3.4% and 2.4% on A-OKVQA and MMMU respectively. We also find that DART adapts well to new tools in applied domains, with a 1.3% improvement on the M3D medical dataset over other strong tool-calling, single agent, and multi-agent baselines. Additionally, we measure text overlap across rounds to highlight the rich discussion in DART compared to existing multi-agent methods. Finally, we study the tool call distribution, finding that diverse tools are reliably used to help resolve disagreement.

</details>


### [44] [GUMBridge: a Corpus for Varieties of Bridging Anaphora](https://arxiv.org/abs/2512.07134)
*Lauren Levine,Amir Zeldes*

Main category: cs.CL

TL;DR: GUMBridge是一个新的英语桥接指代资源，涵盖16种不同文体，提供桥接现象广泛覆盖和细粒度子类型分类标注，并评估了LLM在桥接解析任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有英语桥接指代资源大多规模小、覆盖有限、文体类型少，需要更全面、多样化的标注资源来支持桥接指代研究。

Method: 创建GUMBridge资源，包含16种不同文体的英语文本，提供桥接现象的广泛覆盖和细粒度子类型分类标注，并评估标注质量，使用开源和闭源LLM在三个基础任务上进行基准测试。

Result: GUMBridge提供了高质量的桥接指代标注资源，评估显示桥接解析和子类型分类在LLM时代仍然是困难的NLP任务，LLM在这些任务上的表现有限。

Conclusion: GUMBridge填补了桥接指代资源的空白，为桥接研究提供了全面多样的标注数据，同时揭示了桥接解析在LLM时代仍然具有挑战性，需要进一步研究。

Abstract: Bridging is an anaphoric phenomenon where the referent of an entity in a discourse is dependent on a previous, non-identical entity for interpretation, such as in "There is 'a house'. 'The door' is red," where the door is specifically understood to be the door of the aforementioned house. While there are several existing resources in English for bridging anaphora, most are small, provide limited coverage of the phenomenon, and/or provide limited genre coverage. In this paper, we introduce GUMBridge, a new resource for bridging, which includes 16 diverse genres of English, providing both broad coverage for the phenomenon and granular annotations for the subtype categorization of bridging varieties. We also present an evaluation of annotation quality and report on baseline performance using open and closed source contemporary LLMs on three tasks underlying our data, showing that bridging resolution and subtype classification remain difficult NLP tasks in the age of LLMs.

</details>


### [45] [NeSTR: A Neuro-Symbolic Abductive Framework for Temporal Reasoning in Large Language Models](https://arxiv.org/abs/2512.07218)
*Feng Liang,Weixin Zeng,Runhao Zhao,Xiang Zhao*

Main category: cs.CL

TL;DR: NeSTR是一个结合符号表示与反思推理的神经符号框架，用于提升大语言模型在复杂时间约束下的推理能力，无需微调即可在时间问答任务上取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在时间推理方面存在挑战，特别是复杂时间约束下的推理。现有方法要么符号方法未充分利用LLM的推理能力，要么反思方法缺乏结构化时间表示，导致不一致或幻觉推理。即使有正确的时间上下文，LLM仍可能误解或误用时间信息。

Method: 提出神经符号时间推理框架NeSTR，整合结构化符号表示与混合反思推理：1）通过符号编码保留显式时间关系；2）通过验证强制逻辑一致性；3）使用溯因反思纠正错误推理。

Result: 在多样化时间问答基准测试中，NeSTR实现了优异的零样本性能，无需任何微调就能持续改进时间推理，展示了神经符号集成在增强大语言模型时间理解方面的优势。

Conclusion: NeSTR框架通过整合符号表示和反思推理，有效提升了大语言模型的时间推理能力，证明了神经符号方法在增强时间理解方面的价值。

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing tasks. However, temporal reasoning, particularly under complex temporal constraints, remains a major challenge. To this end, existing approaches have explored symbolic methods, which encode temporal structure explicitly, and reflective mechanisms, which revise reasoning errors through multi-step inference. Nonetheless, symbolic approaches often underutilize the reasoning capabilities of LLMs, while reflective methods typically lack structured temporal representations, which can result in inconsistent or hallucinated reasoning. As a result, even when the correct temporal context is available, LLMs may still misinterpret or misapply time-related information, leading to incomplete or inaccurate answers. To address these limitations, in this work, we propose Neuro-Symbolic Temporal Reasoning (NeSTR), a novel framework that integrates structured symbolic representations with hybrid reflective reasoning to enhance the temporal sensitivity of LLM inference. NeSTR preserves explicit temporal relations through symbolic encoding, enforces logical consistency via verification, and corrects flawed inferences using abductive reflection. Extensive experiments on diverse temporal question answering benchmarks demonstrate that NeSTR achieves superior zero-shot performance and consistently improves temporal reasoning without any fine-tuning, showcasing the advantage of neuro-symbolic integration in enhancing temporal understanding in large language models.

</details>


### [46] [Ensembling LLM-Induced Decision Trees for Explainable and Robust Error Detection](https://arxiv.org/abs/2512.07246)
*Mengqi Wang,Jianwei Wang,Qing Liu,Xiwei Xu,Zhenchang Xing,Liming Zhu,Wenjie Zhang*

Main category: cs.CL

TL;DR: 提出LLM-as-an-inducer框架，通过LLM诱导决策树进行错误检测，提升可解释性和鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的错误检测方法存在两个主要问题：1) 黑盒决策过程缺乏可解释性；2) 对提示词敏感，输出不一致，缺乏鲁棒性

Method: 提出TreeED方法，使用LLM诱导决策树骨架，包含规则节点、GNN节点和叶子节点；进一步提出ForestED方法，通过不确定性采样构建多个决策树，使用EM算法估计树可靠性并优化共识预测

Result: 实验表明方法准确、可解释且鲁棒，平均F1分数比最佳基线提升16.1%

Conclusion: LLM-as-an-inducer框架能有效解决现有LLM错误检测方法的可解释性和鲁棒性问题，通过决策树提供透明决策过程

Abstract: Error detection (ED), which aims to identify incorrect or inconsistent cell values in tabular data, is important for ensuring data quality. Recent state-of-the-art ED methods leverage the pre-trained knowledge and semantic capability embedded in large language models (LLMs) to directly label whether a cell is erroneous. However, this LLM-as-a-labeler pipeline (1) relies on the black box, implicit decision process, thus failing to provide explainability for the detection results, and (2) is highly sensitive to prompts, yielding inconsistent outputs due to inherent model stochasticity, therefore lacking robustness. To address these limitations, we propose an LLM-as-an-inducer framework that adopts LLM to induce the decision tree for ED (termed TreeED) and further ensembles multiple such trees for consensus detection (termed ForestED), thereby improving explainability and robustness. Specifically, based on prompts derived from data context, decision tree specifications and output requirements, TreeED queries the LLM to induce the decision tree skeleton, whose root-to-leaf decision paths specify the stepwise procedure for evaluating a given sample. Each tree contains three types of nodes: (1) rule nodes that perform simple validation checks (e.g., format or range), (2) Graph Neural Network (GNN) nodes that capture complex patterns (e.g., functional dependencies), and (3) leaf nodes that output the final decision types (error or clean). Furthermore, ForestED employs uncertainty-based sampling to obtain multiple row subsets, constructing a decision tree for each subset using TreeED. It then leverages an Expectation-Maximization-based algorithm that jointly estimates tree reliability and optimizes the consensus ED prediction. Extensive xperiments demonstrate that our methods are accurate, explainable and robust, achieving an average F1-score improvement of 16.1% over the best baseline.

</details>


### [47] [TeluguST-46: A Benchmark Corpus and Comprehensive Evaluation for Telugu-English Speech Translation](https://arxiv.org/abs/2512.07265)
*Bhavana Akkiraju,Srihari Bandarupalli,Swathi Sambangi,Vasavi Ravuri,R Vijaya Saraswathi,Anil Kumar Vuppala*

Main category: cs.CL

TL;DR: 为泰卢固语-英语语音翻译建立高质量基准，比较级联与端到端架构，发现端到端系统在低资源场景下具有竞争力，并评估了多种自动评估指标。


<details>
  <summary>Details</summary>
Motivation: 泰卢固语有超过8000万使用者，但作为形态丰富的语言，其语音翻译研究严重不足。需要填补这一空白，为低资源语言建立可靠的翻译基准。

Method: 从CSTD语料库中创建46小时高质量泰卢固语-英语语音翻译基准（30h/8h/8h划分）。系统比较级联架构（IndicWhisper + IndicMT）与端到端架构（SeamlessM4T），并评估BLEU、METEOR、ChrF++、ROUGE-L、TER和BERTScore等自动评估指标。

Result: 级联架构（IndicWhisper + IndicMT）因使用大量泰卢固语特定训练数据而表现最佳，但微调的SeamlessM4T端到端模型使用显著更少的泰卢固语数据仍表现出色。传统评估指标比BERTScore在泰卢固语-英语翻译中提供更好的质量区分。

Conclusion: 通过仔细的超参数调整和足够的平行数据（可能少于100小时），端到端系统在低资源场景下可以达到与级联方法相当的性能。为形态复杂的语言对提供了实用的自动评估指导。

Abstract: Despite Telugu being spoken by over 80 million people, speech translation research for this morphologically rich language remains severely underexplored. We address this gap by developing a high-quality Telugu--English speech translation benchmark from 46 hours of manually verified CSTD corpus data (30h/8h/8h train/dev/test split). Our systematic comparison of cascaded versus end-to-end architectures shows that while IndicWhisper + IndicMT achieves the highest performance due to extensive Telugu-specific training data, finetuned SeamlessM4T models demonstrate remarkable competitiveness despite using significantly less Telugu-specific training data. This finding suggests that with careful hyperparameter tuning and sufficient parallel data (potentially less than 100 hours), end-to-end systems can achieve performance comparable to cascaded approaches in low-resource settings. Our metric reliability study evaluating BLEU, METEOR, ChrF++, ROUGE-L, TER, and BERTScore against human judgments reveals that traditional metrics provide better quality discrimination than BERTScore for Telugu--English translation. The work delivers three key contributions: a reproducible Telugu--English benchmark, empirical evidence of competitive end-to-end performance potential in low-resource scenarios, and practical guidance for automatic evaluation in morphologically complex language pairs.

</details>


### [48] [Efficient ASR for Low-Resource Languages: Leveraging Cross-Lingual Unlabeled Data](https://arxiv.org/abs/2512.07277)
*Srihari Bandarupalli,Bhavana Akkiraju,Charan Devarakonda,Vamsiraghusimha Narsinga,Anil Kumar Vuppala*

Main category: cs.CL

TL;DR: 该论文研究了低资源语言的自动语音识别，通过跨语言持续预训练方法，使用波斯-阿拉伯语系作为案例，证明数据相关性和战略预训练比模型规模更重要。


<details>
  <summary>Details</summary>
Motivation: 低资源语言的自动语音识别面临标注数据稀缺和计算资源不足的双重限制，现有大模型方法对这些语言不适用，需要寻找更高效的解决方案。

Method: 采用跨语言持续预训练策略，构建3000小时多语言语料库，结合形态感知分词技术，开发300M参数模型，利用无标注语音数据弥补资源差距。

Result: 300M参数模型在波斯语上超越Whisper Large v3（1.5B参数），在阿拉伯语和乌尔都语上取得竞争性结果，证明小模型通过战略预训练能达到大模型性能。

Conclusion: ASR质量主要取决于数据相关性和战略预训练而非模型规模，这为低资源语言提供了实用路径，无需依赖大规模计算基础设施或专有数据集。

Abstract: Automatic speech recognition for low-resource languages remains fundamentally constrained by the scarcity of labeled data and computational resources required by state-of-the-art models. We present a systematic investigation into cross-lingual continuous pretraining for low-resource languages, using Perso-Arabic languages (Persian, Arabic, and Urdu) as our primary case study. Our approach demonstrates that strategic utilization of unlabeled speech data can effectively bridge the resource gap without sacrificing recognition accuracy. We construct a 3,000-hour multilingual corpus through a scalable unlabeled data collection pipeline and employ targeted continual pretraining combined with morphologically-aware tokenization to develop a 300M parameter model that achieves performance comparable to systems 5 times larger. Our model outperforms Whisper Large v3 (1.5B parameters) on Persian and achieves competitive results on Arabic and Urdu despite using significantly fewer parameters and substantially less labeled data. These findings challenge the prevailing assumption that ASR quality scales primarily with model size, revealing instead that data relevance and strategic pretraining are more critical factors for low-resource scenarios. This work provides a practical pathway toward inclusive speech technology, enabling effective ASR for underrepresented languages without dependence on massive computational infrastructure or proprietary datasets.

</details>


### [49] [Investigating Training and Generalization in Faithful Self-Explanations of Large Language Models](https://arxiv.org/abs/2512.07288)
*Tomoki Doi,Masaru Isonuma,Hitomi Yanaka*

Main category: cs.CL

TL;DR: 通过特征归因方法构建伪忠实自解释，并用于持续学习，可提升大语言模型自解释的忠实度，且这种改进在不同解释风格和任务间具有泛化性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型能根据用户指令生成不同风格的自解释，但这些解释往往缺乏忠实性（faithfulness）。现有研究对如何提升忠实性探索不足，且不清楚不同解释风格间的改进是否具有泛化性。

Method: 使用特征归因方法构建单词约束的伪忠实自解释，将其用于指令调优模型的持续学习。在三个分类任务和三种解释风格上进行实验。

Result: 训练能提升所有分类任务和解释风格的自解释忠实度，且改进在多词设置和未见任务中显示出泛化迹象。三种风格间存在一致的跨风格泛化，表明训练可能促进忠实自解释能力的广泛提升。

Conclusion: 通过伪忠实自解释进行持续学习能有效提升大语言模型自解释的忠实度，且这种提升在不同解释风格和任务间具有泛化性，为改善模型自解释的可靠性提供了可行路径。

Abstract: Large language models have the potential to generate explanations for their own predictions in a variety of styles based on user instructions. Recent research has examined whether these self-explanations faithfully reflect the models' actual behavior and has found that they often lack faithfulness. However, the question of how to improve faithfulness remains underexplored. Moreover, because different explanation styles have superficially distinct characteristics, it is unclear whether improvements observed in one style also arise when using other styles. This study analyzes the effects of training for faithful self-explanations and the extent to which these effects generalize, using three classification tasks and three explanation styles. We construct one-word constrained explanations that are likely to be faithful using a feature attribution method, and use these pseudo-faithful self-explanations for continual learning on instruction-tuned models. Our experiments demonstrate that training can improve self-explanation faithfulness across all classification tasks and explanation styles, and that these improvements also show signs of generalization to the multi-word settings and to unseen tasks. Furthermore, we find consistent cross-style generalization among three styles, suggesting that training may contribute to a broader improvement in faithful self-explanation ability.

</details>


### [50] [Multilingual corpora for the study of new concepts in the social sciences and humanities:](https://arxiv.org/abs/2512.07367)
*Revekka Kyriakoglou,Anna Pappa*

Main category: cs.CL

TL;DR: 提出一种构建多语料库的混合方法，用于研究人文社科中的新兴概念，以"非技术创新"为例，结合公司网站文本和年度报告，创建适合机器学习的数据集。


<details>
  <summary>Details</summary>
Motivation: 需要系统性地研究人文社科领域的新兴概念，特别是"非技术创新"这类跨学科概念，传统方法难以捕捉其在不同语境中的词汇变异性。

Method: 混合方法：1) 从公司网站自动提取法英文本内容；2) 收集年度报告并按标准自动筛选。处理流程包括语言检测、内容过滤、相关片段提取和元数据增强。构建英文数据集时，为每个专业词汇提取包含前后两句的上下文块，并标注主题类别。

Result: 创建了一个可重复、可扩展的多语言语料库资源，既可用于分析新兴概念的词汇变异性，也可生成适合监督分类任务的NLP应用数据集。

Conclusion: 该方法为研究人文社科新兴概念提供了系统化的语料库构建框架，结合了传统文本分析和现代机器学习需求，具有实用性和扩展性。

Abstract: This article presents a hybrid methodology for building a multilingual corpus designed to support the study of emerging concepts in the humanities and social sciences (HSS), illustrated here through the case of ``non-technological innovation''. The corpus relies on two complementary sources: (1) textual content automatically extracted from company websites, cleaned for French and English, and (2) annual reports collected and automatically filtered according to documentary criteria (year, format, duplication). The processing pipeline includes automatic language detection, filtering of non-relevant content, extraction of relevant segments, and enrichment with structural metadata. From this initial corpus, a derived dataset in English is created for machine learning purposes. For each occurrence of a term from the expert lexicon, a contextual block of five sentences is extracted (two preceding and two following the sentence containing the term). Each occurrence is annotated with the thematic category associated with the term, enabling the construction of data suitable for supervised classification tasks. This approach results in a reproducible and extensible resource, suitable both for analyzing lexical variability around emerging concepts and for generating datasets dedicated to natural language processing applications.

</details>


### [51] [Training Language Models to Use Prolog as a Tool](https://arxiv.org/abs/2512.07407)
*Niklas Mellgren,Peter Schneider-Kamp,Lukas Galke Poech*

Main category: cs.CL

TL;DR: 通过强化学习微调语言模型使用Prolog作为可验证计算工具，提升推理可靠性和可审计性


<details>
  <summary>Details</summary>
Motivation: 语言模型经常产生看似合理但错误的推理结果，难以验证，这对安全关键型AI系统构成风险。需要确保工具使用的可靠性。

Method: 使用Group Relative Policy Optimization (GRPO)微调Qwen2.5-3B-Instruct模型，在GSM8K-Prolog-Prover数据集上实验不同提示结构、奖励组合（执行、语法、语义、结构）和推理协议（单次、best-of-N、两种代理模式）

Result: 强化学习方法优于监督微调，3B模型在零样本MMLU上达到与7B模型少样本相当的性能。最佳配置在GSM8K上实现最高准确率，代理推理在MMLU-Stem和MMLU-Pro上获得更好的零样本泛化能力

Conclusion: 将模型推理建立在形式化验证系统上能显著提高安全关键应用的可靠性和可审计性，联合优化提示、奖励和推理协议能有效塑造程序语法和逻辑

Abstract: Ensuring reliable tool use is critical for safe agentic AI systems. Language models frequently produce unreliable reasoning with plausible but incorrect solutions that are difficult to verify. To address this, we investigate fine-tuning models to use Prolog as an external tool for verifiable computation. Using Group Relative Policy Optimization (GRPO), we fine-tune Qwen2.5-3B-Instruct on a cleaned GSM8K-Prolog-Prover dataset while varying (i) prompt structure, (ii) reward composition (execution, syntax, semantics, structure), and (iii) inference protocol: single-shot, best-of-N, and two agentic modes where Prolog is invoked internally or independently. Our reinforcement learning approach outperforms supervised fine-tuning, with our 3B model achieving zero-shot MMLU performance comparable to 7B few-shot results. Our findings reveal that: 1) joint tuning of prompt, reward, and inference shapes program syntax and logic; 2) best-of-N with external Prolog verification maximizes accuracy on GSM8K; 3) agentic inference with internal repair yields superior zero-shot generalization on MMLU-Stem and MMLU-Pro. These results demonstrate that grounding model reasoning in formal verification systems substantially improves reliability and auditability for safety-critical applications. The source code for reproducing our experiments is available under https://github.com/niklasmellgren/grpo-prolog-inference

</details>


### [52] [Persian-Phi: Efficient Cross-Lingual Adaptation of Compact LLMs via Curriculum Learning](https://arxiv.org/abs/2512.07454)
*Amir Mohammad Akhlaghi,Amirhossein Shabani,Mostafa Abdolmaleki,Saeed Reza Kheradpisheh*

Main category: cs.CL

TL;DR: 本文提出Persian-Phi，一个3.8B参数的波斯语模型，通过创新的课程学习流程将英语模型Phi-3 Mini高效适配到波斯语，证明了小模型也能在多语言任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前AI民主化受到训练低资源语言大语言模型所需巨大计算成本的阻碍。本文旨在挑战"强大多语言能力需要大规模模型或多语言基线"的假设，为资源匮乏语言提供高效适配方案。

Method: 采用资源高效的课程学习流程：1) 使用双语叙事(Tiny Stories)进行"预热"阶段以对齐嵌入；2) 通过参数高效微调(PEFT)进行持续预训练和指令调优；3) 将英语模型Phi-3 Mini适配到波斯语。

Result: Persian-Phi在HuggingFace的Open Persian LLM Leaderboard上取得有竞争力的结果，证明了小模型在多语言任务中的有效性，为低资源语言提供了可扩展的框架。

Conclusion: 该研究提供了一个经过验证的可扩展框架，能够以最小硬件资源将最先进的LLM扩展到代表性不足的语言，促进了AI民主化进程。

Abstract: The democratization of AI is currently hindered by the immense computational costs required to train Large Language Models (LLMs) for low-resource languages. This paper presents Persian-Phi, a 3.8B parameter model that challenges the assumption that robust multilingual capabilities require massive model sizes or multilingual baselines. We demonstrate how Microsoft Phi-3 Mini -- originally a monolingual English model -- can be effectively adapted to Persian through a novel, resource-efficient curriculum learning pipeline. Our approach employs a unique "warm-up" stage using bilingual narratives (Tiny Stories) to align embeddings prior to heavy training, followed by continual pretraining and instruction tuning via Parameter-Efficient Fine-Tuning (PEFT). Despite its compact size, Persian-Phi achieves competitive results on Open Persian LLM Leaderboard in HuggingFace. Our findings provide a validated, scalable framework for extending the reach of state-of-the-art LLMs to underrepresented languages with minimal hardware resources. The Persian-Phi model is publicly available at https://huggingface.co/amirakhlaghiqqq/PersianPhi.

</details>


### [53] [Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning](https://arxiv.org/abs/2512.07461)
*Tong Wu,Yang Liu,Jun Bai,Zixia Jia,Shuyi Zhang,Ziyong Lin,Yanting Wang,Song-Chun Zhu,Zilong Zheng*

Main category: cs.CL

TL;DR: NPR是一个无教师框架，让大语言模型自我进化出真正的并行推理能力，通过自蒸馏训练、并行感知策略优化和NPR引擎三大创新，在8个推理基准上实现最高24.5%性能提升和4.6倍推理加速。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在推理时通常采用顺序模拟方式，缺乏真正的并行认知能力。先前基线方法经常回退到自回归解码，无法实现真正的并行执行。需要让LLMs从顺序模拟转变为原生并行认知。

Method: 1) 自蒸馏渐进训练范式：从"冷启动"格式发现过渡到严格拓扑约束，无需外部监督；2) 并行感知策略优化算法：在执行图中直接优化分支策略，通过试错学习自适应分解；3) NPR引擎：重构SGLang的内存管理和流程控制，实现稳定的大规模并行强化学习训练。

Result: 在8个推理基准测试中，基于Qwen3-4B训练的NPR实现了最高24.5%的性能提升和最高4.6倍的推理加速。与先前基线不同，NPR展示了100%真正的并行执行，而非回退到自回归解码。

Conclusion: NPR为自我进化、高效且可扩展的智能体推理建立了新标准，使大语言模型能够从顺序模拟转变为原生并行认知，实现真正的并行推理能力。

Abstract: We introduce Native Parallel Reasoner (NPR), a teacher-free framework that enables Large Language Models (LLMs) to self-evolve genuine parallel reasoning capabilities. NPR transforms the model from sequential emulation to native parallel cognition through three key innovations: 1) a self-distilled progressive training paradigm that transitions from ``cold-start'' format discovery to strict topological constraints without external supervision; 2) a novel Parallel-Aware Policy Optimization (PAPO) algorithm that optimizes branching policies directly within the execution graph, allowing the model to learn adaptive decomposition via trial and error; and 3) a robust NPR Engine that refactors memory management and flow control of SGLang to enable stable, large-scale parallel RL training. Across eight reasoning benchmarks, NPR trained on Qwen3-4B achieves performance gains of up to 24.5% and inference speedups up to 4.6x. Unlike prior baselines that often fall back to autoregressive decoding, NPR demonstrates 100% genuine parallel execution, establishing a new standard for self-evolving, efficient, and scalable agentic reasoning.

</details>


### [54] [Enhancing Agentic RL with Progressive Reward Shaping and Value-based Sampling Policy Optimization](https://arxiv.org/abs/2512.07478)
*Zhuoran Zhuang,Ye Chen,Jianghao Su,Chao Luo,Luhui Liu,Xia Zeng*

Main category: cs.CL

TL;DR: 论文提出PRS和VSPO两种技术，解决工具集成推理LLM在强化学习中的奖励稀疏和梯度退化问题，提升复杂推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前工具集成推理LLM在强化学习优化中存在两个关键挑战：1）稀疏的二元奖励信号对中间步骤指导有限，收敛缓慢；2）GRPO中相同奖励导致零优势，降低样本效率和训练稳定性。

Method: 提出两种互补技术：渐进奖励塑造（PRS）和基于价值的采样策略优化（VSPO）。PRS采用课程式奖励设计，提供密集的阶段反馈；VSPO是GRPO的增强变体，通过价值度量和平滑剪枝提升稳定性。

Result: 在多个短形式和长形式QA基准测试中，PRS始终优于传统二元奖励，VSPO相比PPO、GRPO、CISPO和SFT基线表现出更好的稳定性、更快收敛和更高最终性能。

Conclusion: PRS和VSPO相结合能够产生跨领域泛化能力更强的LLM工具集成推理智能体，有效解决了奖励稀疏和梯度退化问题。

Abstract: Large Language Models (LLMs) empowered with Tool-Integrated Reasoning (TIR) can iteratively plan, call external tools, and integrate returned information to solve complex, long-horizon reasoning tasks. Agentic Reinforcement Learning (Agentic RL) optimizes such models over full tool-interaction trajectories, but two key challenges hinder effectiveness: (1) Sparse, non-instructive rewards, such as binary 0-1 verifiable signals, provide limited guidance for intermediate steps and slow convergence; (2) Gradient degradation in Group Relative Policy Optimization (GRPO), where identical rewards within a rollout group yield zero advantage, reducing sample efficiency and destabilizing training. To address these challenges, we propose two complementary techniques: Progressive Reward Shaping (PRS) and Value-based Sampling Policy Optimization (VSPO). PRS is a curriculum-inspired reward design that introduces dense, stage-wise feedback - encouraging models to first master parseable and properly formatted tool calls, then optimize for factual correctness and answer quality. We instantiate PRS for short-form QA (with a length-aware BLEU to fairly score concise answers) and long-form QA (with LLM-as-a-Judge scoring to prevent reward hacking). VSPO is an enhanced GRPO variant that replaces low-value samples with prompts selected by a task-value metric balancing difficulty and uncertainty, and applies value-smoothing clipping to stabilize gradient updates. Experiments on multiple short-form and long-form QA benchmarks show that PRS consistently outperforms traditional binary rewards, and VSPO achieves superior stability, faster convergence, and higher final performance compared to PPO, GRPO, CISPO, and SFT-only baselines. Together, PRS and VSPO yield LLM-based TIR agents that generalize better across domains.

</details>


### [55] [SPAD: Seven-Source Token Probability Attribution with Syntactic Aggregation for Detecting Hallucinations in RAG](https://arxiv.org/abs/2512.07515)
*Pengqian Lu,Jie Lu,Anjin Liu,Guangquan Zhang*

Main category: cs.CL

TL;DR: SPAD提出了一种新的幻觉检测方法，通过将token概率分解为七个来源并基于词性标签聚合，有效识别RAG中的幻觉


<details>
  <summary>Details</summary>
Motivation: 现有方法将幻觉归因于内部知识（FFN）与检索上下文之间的二元冲突，但这种观点不完整，忽略了生成过程中其他组件的影响，如用户查询、先前生成的token、当前token本身和最终LayerNorm调整

Method: 首先将每个token的概率数学归因于七个不同来源：查询、RAG、过去token、当前token、FFN、最终LayerNorm和初始嵌入；然后基于词性标签聚合这些分数，量化不同组件如何驱动特定语言类别；通过识别异常（如名词依赖最终LayerNorm）来检测幻觉

Result: 大量实验表明SPAD达到了最先进的性能

Conclusion: SPAD通过全面分析生成过程中多个组件的贡献，提供了一种更完整的幻觉检测方法，超越了传统的二元冲突视角

Abstract: Detecting hallucinations in Retrieval-Augmented Generation (RAG) remains a challenge. Prior approaches attribute hallucinations to a binary conflict between internal knowledge (stored in FFNs) and retrieved context. However, this perspective is incomplete, failing to account for the impact of other components in the generative process, such as the user query, previously generated tokens, the current token itself, and the final LayerNorm adjustment. To address this, we introduce SPAD. First, we mathematically attribute each token's probability into seven distinct sources: Query, RAG, Past, Current Token, FFN, Final LayerNorm, and Initial Embedding. This attribution quantifies how each source contributes to the generation of the current token. Then, we aggregate these scores by POS tags to quantify how different components drive specific linguistic categories. By identifying anomalies, such as Nouns relying on Final LayerNorm, SPAD effectively detects hallucinations. Extensive experiments demonstrate that SPAD achieves state-of-the-art performance

</details>


### [56] [LIME: Making LLM Data More Efficient with Linguistic Metadata Embeddings](https://arxiv.org/abs/2512.07522)
*Sebastian Sztwiertnia,Felix Friedrich,Kristian Kersting,Patrick Schramowski,Björn Deiseroth*

Main category: cs.CL

TL;DR: LIME是一种通过将语法、语义和上下文属性的元数据嵌入到token表示中来提高语言模型预训练效率的方法，能够加速56%的适应速度，仅增加0.01%的参数，并显著提升语言建模和生成任务性能。


<details>
  <summary>Details</summary>
Motivation: 当前仅解码器语言模型的预训练依赖大量高质量数据，但这类数据的可用性正达到极限。虽然元数据通常用于创建和整理数据集，但其作为直接训练信号的潜力尚未充分探索。

Method: 提出LIME（语言元数据嵌入）方法，通过将捕捉语法、语义和上下文属性的元数据丰富到token嵌入中。还开发了LIME+1变体，使用偏移元数据来指导token生成。

Result: LIME显著提升预训练效率：适应训练数据分布的速度加快56%，仅增加0.01%参数且计算开销可忽略。改善分词效果，增强语言建模能力和生成任务性能。LIME+1变体在给定下一个token的元数据时，推理性能提升38%，算术准确率提升35%。

Conclusion: 元数据作为直接训练信号具有巨大潜力，LIME方法不仅能显著提高预训练效率，还能增强语言模型的核心能力，且这些优势在不同规模模型（5亿到20亿参数）中都能保持。

Abstract: Pre-training decoder-only language models relies on vast amounts of high-quality data, yet the availability of such data is increasingly reaching its limits. While metadata is commonly used to create and curate these datasets, its potential as a direct training signal remains under-explored. We challenge this status quo and propose LIME (Linguistic Metadata Embeddings), a method that enriches token embeddings with metadata capturing syntax, semantics, and contextual properties. LIME substantially improves pre-training efficiency. Specifically, it adapts up to 56% faster to the training data distribution, while introducing only 0.01% additional parameters at negligible compute overhead. Beyond efficiency, LIME improves tokenization, leading to remarkably stronger language modeling capabilities and generative task performance. These benefits persist across model scales (500M to 2B). In addition, we develop a variant with shifted metadata, LIME+1, that can guide token generation. Given prior metadata for the next token, LIME+1 improves reasoning performance by up to 38% and arithmetic accuracy by up to 35%.

</details>


### [57] [Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs](https://arxiv.org/abs/2512.07525)
*Xiaoran Liu,Yuerong Song,Zhigeng Liu,Zengfeng Huang,Qipeng Guo,Zhaoxiang Liu,Shiguo Lian,Ziwei He,Xipeng Qiu*

Main category: cs.CL

TL;DR: 该论文提出了一种改进的RoPE方法，通过重新引入被丢弃的虚部信息来增强长上下文依赖建模能力。


<details>
  <summary>Details</summary>
Motivation: 标准RoPE实现只使用复数点积的实部计算注意力分数，丢弃了包含重要相位信息的虚部，这可能导致长上下文依赖建模中关系细节的损失。

Method: 提出扩展方法，重新引入被丢弃的虚部信息，利用完整的复数表示创建双组件注意力分数，从而保留更多位置信息。

Result: 在长上下文语言建模基准测试中，该方法相比标准RoPE持续提升性能，且随着上下文长度增加，改进效果更加显著。

Conclusion: 通过重新利用RoPE中被丢弃的虚部信息，可以增强长上下文依赖建模能力，为位置编码方法提供了有价值的改进方向。

Abstract: Rotary Position Embeddings (RoPE) have become a standard for encoding sequence order in Large Language Models (LLMs) by applying rotations to query and key vectors in the complex plane. Standard implementations, however, utilize only the real component of the complex-valued dot product for attention score calculation. This simplification discards the imaginary component, which contains valuable phase information, leading to a potential loss of relational details crucial for modeling long-context dependencies. In this paper, we propose an extension that re-incorporates this discarded imaginary component. Our method leverages the full complex-valued representation to create a dual-component attention score. We theoretically and empirically demonstrate that this approach enhances the modeling of long-context dependencies by preserving more positional information. Furthermore, evaluations on a suite of long-context language modeling benchmarks show that our method consistently improves performance over the standard RoPE, with the benefits becoming more significant as context length increases. The code is available at https://github.com/OpenMOSS/rope_pp.

</details>


### [58] [SwissGov-RSD: A Human-annotated, Cross-lingual Benchmark for Token-level Recognition of Semantic Differences Between Related Documents](https://arxiv.org/abs/2512.07538)
*Michelle Wastl,Jannis Vamvas,Rico Sennrich*

Main category: cs.CL

TL;DR: 本文介绍了首个自然主义、文档级、跨语言的语义差异识别数据集SwissGov-RSD，并评估了多种LLM和编码器模型在该任务上的表现，发现现有方法在跨语言文档级语义差异识别上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 识别文档间的语义差异，尤其是跨语言文档，对于文本生成评估和多语言内容对齐至关重要，但作为独立任务却很少受到关注。

Method: 构建了SwissGov-RSD数据集，包含224个英-德、英-法、英-意多平行文档，带有人工标注的token级差异标记。评估了多种开源和闭源大语言模型以及编码器模型在不同微调设置下的表现。

Result: 当前自动方法在该新基准上表现不佳，远低于其在单语言、句子级和合成基准上的性能，揭示了LLM和编码器模型在该任务上的显著差距。

Conclusion: 跨语言文档级语义差异识别是一个具有挑战性的任务，现有模型表现不足，需要进一步研究。作者公开了代码和数据集以促进该领域发展。

Abstract: Recognizing semantic differences across documents, especially in different languages, is crucial for text generation evaluation and multilingual content alignment. However, as a standalone task it has received little attention. We address this by introducing SwissGov-RSD, the first naturalistic, document-level, cross-lingual dataset for semantic difference recognition. It encompasses a total of 224 multi-parallel documents in English-German, English-French, and English-Italian with token-level difference annotations by human annotators. We evaluate a variety of open-source and closed source large language models as well as encoder models across different fine-tuning settings on this new benchmark. Our results show that current automatic approaches perform poorly compared to their performance on monolingual, sentence-level, and synthetic benchmarks, revealing a considerable gap for both LLMs and encoder models. We make our code and datasets publicly available.

</details>


### [59] [Minimum Bayes Risk Decoding for Error Span Detection in Reference-Free Automatic Machine Translation Evaluation](https://arxiv.org/abs/2512.07540)
*Boxuan Lyu,Haiyue Song,Hidetaka Kamigaito,Chenchen Ding,Hideki Tanaka,Masao Utiyama,Kotaro Funakoshi,Manabu Okumura*

Main category: cs.CL

TL;DR: 该论文提出在生成式错误跨度检测（ESD）任务中使用最小贝叶斯风险（MBR）解码替代传统的最大后验（MAP）解码，以解决模型概率估计与人工标注相似度不匹配的问题，并通过MBR蒸馏降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有生成式ESD方法通常使用MAP解码，假设模型估计的概率与人工标注相似度完美相关。但作者观察到，与人工标注不相似的标注可能获得更高的模型似然度，这表明MAP解码存在局限性。

Method: 采用最小贝叶斯风险（MBR）解码，使用句子级和跨度级相似度指标作为效用函数，选择与人工标注近似相似度最高的候选假设。为降低MBR解码的计算成本，提出MBR蒸馏方法，使标准贪婪模型能够匹配MBR解码性能。

Result: 实验结果表明，MBR解码在系统级、句子级和跨度级均优于MAP基线。MBR蒸馏能够使标准贪婪模型达到与MBR解码相当的性能，有效消除了推理时的延迟瓶颈。

Conclusion: MBR解码能够有效解决生成式ESD模型中概率估计与人工标注相似度不匹配的问题，显著提升性能。通过MBR蒸馏可以降低计算成本，使该方法在实际应用中更加可行。

Abstract: Error Span Detection (ESD) is a subtask of automatic machine translation evaluation that localizes error spans in translations and labels their severity. State-of-the-art generative ESD methods typically decode using Maximum a Posteriori (MAP), assuming that model-estimated probabilities are perfectly correlated with similarity to human annotation. However, we observed that annotations dissimilar to the human annotation could achieve a higher model likelihood than the human annotation. We address this issue by applying Minimum Bayes Risk (MBR) decoding to generative ESD models. Specifically, we employ sentence- and span-level similarity metrics as utility functions to select candidate hypotheses based on their approximate similarity to the human annotation. Extensive experimental results show that our MBR decoding outperforms the MAP baseline at the system, sentence, and span-levels. Furthermore, to mitigate the computational cost of MBR decoding, we demonstrate that applying MBR distillation enables a standard greedy model to match MBR decoding performance, effectively eliminating the inference-time latency bottleneck.

</details>


### [60] [Most over-representation of phonological features in basic vocabulary disappears when controlling for spatial and phylogenetic effects](https://arxiv.org/abs/2512.07543)
*Frederic Blum*

Main category: cs.CL

TL;DR: 重新检验语音象征性在基本词汇中的统计显著性，使用更大样本和更严格的控制后，发现多数先前观察到的模式并不稳健，只有少数模式保持稳定。


<details>
  <summary>Details</summary>
Motivation: 先前关于语音特征在语言基本词汇中统计过表达的研究可能存在样本偏差、模型问题，且未充分控制语言间的谱系和地域依赖关系，需要检验这些结果的稳健性。

Method: 使用Lexibank的2864种语言数据（原研究为245种），修改原始模型，增加对语言间空间和谱系依赖关系的统计控制。

Result: 多数先前观察到的语音象征模式并不稳健，许多模式在加入谱系和地域控制后完全消失。只有少数模式在新的样本中保持高度稳定。

Conclusion: 语音象征性在基本词汇中的分布需要更严格的检验，只有少数模式具有跨语言的稳健性。研究强调了对语言普遍性主张进行多层次稳健性测试的必要性。

Abstract: The statistical over-representation of phonological features in the basic vocabulary of languages is often interpreted as reflecting potentially universal sound symbolic patterns. However, most of those results have not been tested explicitly for reproducibility and might be prone to biases in the study samples or models. Many studies on the topic do not adequately control for genealogical and areal dependencies between sampled languages, casting doubts on the robustness of the results. In this study, we test the robustness of a recent study on sound symbolism of basic vocabulary concepts which analyzed245 languages.The new sample includes data on 2864 languages from Lexibank. We modify the original model by adding statistical controls for spatial and phylogenetic dependencies between languages. The new results show that most of the previously observed patterns are not robust, and in fact many patterns disappear completely when adding the genealogical and areal controls. A small number of patterns, however, emerges as highly stable even with the new sample. Through the new analysis, we are able to assess the distribution of sound symbolism on a larger scale than previously. The study further highlights the need for testing all universal claims on language for robustness on various levels.

</details>


### [61] [MoCoRP: Modeling Consistent Relations between Persona and Response for Persona-based Dialogue](https://arxiv.org/abs/2512.07544)
*Kyungro Lee,Dongha Choi,Hyunju Lee*

Main category: cs.CL

TL;DR: MoCoRP是一个在基于角色的对话中显式建模角色句子与回复之间关系的框架，通过NLI专家提取关系，提升角色一致性和对话质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于角色的对话数据集缺乏角色句子与回复之间的显式关系，导致模型难以有效捕捉角色信息，影响对话的一致性和吸引力。

Method: 提出MoCoRP框架，利用NLI专家显式提取角色句子与回复之间的NLI关系（如蕴含、矛盾、中性），使模型能有效结合上下文中的角色信息生成回复。该框架应用于BART等预训练模型，并通过对齐调优扩展到现代大语言模型。

Result: 在ConvAI2和MPChat数据集上，MoCoRP超越了现有基线，在角色一致性和上下文感知对话生成方面表现优异。不仅在定量指标上领先，在定性评估方面也有显著提升。

Conclusion: 显式建模角色-回复关系能有效提升基于角色的对话系统的性能，MoCoRP框架为角色一致性对话生成提供了有效解决方案。

Abstract: As dialogue systems become increasingly important across various domains, a key challenge in persona-based dialogue is generating engaging and context-specific interactions while ensuring the model acts with a coherent personality. However, existing persona-based dialogue datasets lack explicit relations between persona sentences and responses, which makes it difficult for models to effectively capture persona information. To address these issues, we propose MoCoRP (Modeling Consistent Relations between Persona and Response), a framework that incorporates explicit relations into language models. MoCoRP leverages an NLI expert to explicitly extract the NLI relations between persona sentences and responses, enabling the model to effectively incorporate appropriate persona information from the context into its responses. We applied this framework to pre-trained models like BART and further extended it to modern large language models (LLMs) through alignment tuning. Experimental results on the public datasets ConvAI2 and MPChat demonstrate that MoCoRP outperforms existing baselines, achieving superior persona consistency and engaging, context-aware dialogue generation. Furthermore, our model not only excels in quantitative metrics but also shows significant improvements in qualitative aspects. These results highlight the effectiveness of explicitly modeling persona-response relations in persona-based dialogue. The source codes of MoCoRP are available at https://github.com/DMCB-GIST/MoCoRP.

</details>


### [62] [Performance of the SafeTerm AI-Based MedDRA Query System Against Standardised MedDRA Queries](https://arxiv.org/abs/2512.07552)
*Francois Vandenhende,Anna Georgiou,Michalis Georgiou,Theodoros Psaras,Ellie Karekla,Elena Hadjicosta*

Main category: cs.CL

TL;DR: SafeTerm AMQ系统使用AI自动检索MedDRA相关术语，在药物安全审查中表现良好，平衡了召回率和精确率


<details>
  <summary>Details</summary>
Motivation: 在药物上市前安全审查中，将相关不良事件术语分组到SMQs或OCMQs对信号检测至关重要，需要自动化工具来提高效率

Method: 开发了SafeTerm AMQ系统，将医学术语和MedDRA PTs嵌入多维向量空间，应用余弦相似度和极值聚类生成排名列表，通过相似度阈值验证性能

Result: 在110个SMQs验证中，系统在中等阈值下达到94%的高召回率，高阈值下精确率可达89%，最佳阈值0.70时召回率48%、精确率45%

Conclusion: SafeTerm AMQ在SMQs和OCMQs上表现满意，是自动化MedDRA查询生成的可行补充方法，建议使用合适术语并应用自动阈值优化召回率

Abstract: In pre-market drug safety review, grouping related adverse event terms into SMQs or OCMQs is critical for signal detection. We assess the performance of SafeTerm Automated Medical Query (AMQ) on MedDRA SMQs. The AMQ is a novel quantitative artificial intelligence system that understands and processes medical terminology and automatically retrieves relevant MedDRA Preferred Terms (PTs) for a given input query, ranking them by a relevance score (0-1) using multi-criteria statistical methods. The system (SafeTerm) embeds medical query terms and MedDRA PTs in a multidimensional vector space, then applies cosine similarity, and extreme-value clustering to generate a ranked list of PTs. Validation was conducted against tier-1 SMQs (110 queries, v28.1). Precision, recall and F1 were computed at multiple similarity-thresholds, defined either manually or using an automated method. High recall (94%)) is achieved at moderate similarity thresholds, indicative of good retrieval sensitivity. Higher thresholds filter out more terms, resulting in improved precision (up to 89%). The optimal threshold (0.70)) yielded an overall recall of (48%) and precision of (45%) across all 110 queries. Restricting to narrow-term PTs achieved slightly better performance at an increased (+0.05) similarity threshold, confirming increased relatedness of narrow versus broad terms. The automatic threshold (0.66) selection prioritizes recall (0.58) to precision (0.29). SafeTerm AMQ achieves comparable, satisfactory performance on SMQs and sanitized OCMQs. It is therefore a viable supplementary method for automated MedDRA query generation, balancing recall and precision. We recommend using suitable MedDRA PT terminology in query formulation and applying the automated threshold method to optimise recall. Increasing similarity scores allows refined, narrow terms selection.

</details>


### [63] [A Simple Method to Enhance Pre-trained Language Models with Speech Tokens for Classification](https://arxiv.org/abs/2512.07571)
*Nicolas Calbucura,Valentin Barriere*

Main category: cs.CL

TL;DR: 提出一种简单方法，通过特征选择将语音信息融入文本预训练大语言模型，用于特定分类任务，在论辩谬误检测任务上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统多模态融合中，音频序列长度远大于文本序列，且现有语音分词器输出长序列大词汇表token，难以低成本集成到大语言模型中。需要解决音频信息有效融入文本模型的问题。

Method: 使用基于lasso的特征选择方法，在多模态词袋表示中保留对任务最重要的音频token，然后通过自监督语言建模目标使语言模型适应这些token，最后在下游任务上微调。

Result: 相比单模态模型、更大的SpeechLM或学习表示集成音频的方法，本方法性能更好。在两个论辩谬误检测与分类任务上达到SOTA，即使随机选择音频token也能增强单模态模型。

Conclusion: 提出了一种简单有效的语音信息融入文本预训练大语言模型的方法，通过特征选择减少音频token数量，在需要音频信息的任务上显著提升性能，为多模态融合提供了新思路。

Abstract: This paper presents a simple method that allows to easily enhance textual pre-trained large language models with speech information, when fine-tuned for a specific classification task. A classical issue with the fusion of many embeddings from audio with text is the large length of the audio sequence compared to the text one. Our method benefits from an existing speech tokenizer trained for Audio Speech Recognition that output long sequences of tokens from a large vocabulary, making it difficult to integrate it at low cost in a large language model. By applying a simple lasso-based feature selection on multimodal Bag-of-Words representation, we retain only the most important audio tokens for the task, and adapt the language model to them with a self-supervised language modeling objective, before fine-tuning it on the downstream task. We show this helps to improve the performances compared to an unimodal model, to a bigger SpeechLM or to integrating audio via a learned representation. We show the effectiveness of our method on two recent Argumentative Fallacy Detection and Classification tasks where the use of audio was believed counterproductive, reaching state-of-the-art results. We also provide an in-depth analysis of the method, showing that even a random audio token selection helps enhancing the unimodal model. Our code is available [online](https://github.com/salocinc/EACL26SpeechTokFallacy/).

</details>


### [64] [Complementary Learning Approach for Text Classification using Large Language Models](https://arxiv.org/abs/2512.07583)
*Navid Asgari,Benjamin M. Cole*

Main category: cs.CL

TL;DR: 提出一种结构化方法，通过思维链和少样本学习提示，将LLMs以成本效益高的方式整合到定量研究中，实现人机协作优势互补


<details>
  <summary>Details</summary>
Motivation: 解决如何以经济高效的方式利用大型语言模型进行定量研究，同时结合学者和机器的各自优势，弥补各自的弱点

Method: 采用思维链和少样本学习提示技术，将定性研究中合著团队的最佳实践扩展到定量研究的人机团队中，允许人类使用溯因推理和自然语言来审查机器和人类的工作

Result: 开发了一种低成本技术来管理LLMs的固有弱点，并应用于分析1990-2017年间1,934份制药联盟新闻稿中的人机评分差异

Conclusion: 该方法展示了学者如何通过仔细的低成本技术有效利用LLMs进行定量研究，实现人机协作的优势互补

Abstract: In this study, we propose a structured methodology that utilizes large language models (LLMs) in a cost-efficient and parsimonious manner, integrating the strengths of scholars and machines while offsetting their respective weaknesses. Our methodology, facilitated through a chain of thought and few-shot learning prompting from computer science, extends best practices for co-author teams in qualitative research to human-machine teams in quantitative research. This allows humans to utilize abductive reasoning and natural language to interrogate not just what the machine has done but also what the human has done. Our method highlights how scholars can manage inherent weaknesses OF LLMs using careful, low-cost techniques. We demonstrate how to use the methodology to interrogate human-machine rating discrepancies for a sample of 1,934 press releases announcing pharmaceutical alliances (1990-2017).

</details>


### [65] [Metric-Fair Prompting: Treating Similar Samples Similarly](https://arxiv.org/abs/2512.07608)
*Jing Wang,Jie Shen,Xing Niu,Tong Zhang,Jeremy Weiss*

Main category: cs.CL

TL;DR: 提出Metric-Fair Prompting框架，通过度量公平性约束指导LLM在医学选择题中做出决策，提升公平性和准确性


<details>
  <summary>Details</summary>
Motivation: 在医学问答等高风险应用中，需要确保LLM决策的公平性，特别是要保证相似问题得到相似处理（个体公平性），避免不一致的决策

Method: 将每个(问题,选项)对视为二元实例，使用NLP嵌入计算问题相似度，在相似问题的联合对中解决项目而非孤立处理。提示框架强制全局决策协议：提取决定性临床特征，将每个(问题,选项)映射到置信度分数f(x)，并施加Lipschitz式约束确保相似输入获得相似分数和一致输出

Result: 在MedQA (US)基准测试中，Metric-Fair Prompting相比标准单项目提示方法提高了性能，表明公平性引导的置信度导向推理可以增强LLM在高风险临床选择题上的准确性

Conclusion: 提出的度量公平提示框架通过强制相似问题获得一致处理，不仅提升了公平性，还意外地提高了LLM在医学选择题上的准确性，为高风险应用中的LLM决策提供了新的公平性增强方法

Abstract: We introduce \emph{Metric-Fair Prompting}, a fairness-aware prompting framework that guides large language models (LLMs) to make decisions under metric-fairness constraints. In the application of multiple-choice medical question answering, each {(question, option)} pair is treated as a binary instance with label $+1$ (correct) or $-1$ (incorrect). To promote {individual fairness}~--~treating similar instances similarly~--~we compute question similarity using NLP embeddings and solve items in \emph{joint pairs of similar questions} rather than in isolation. The prompt enforces a global decision protocol: extract decisive clinical features, map each \((\text{question}, \text{option})\) to a score $f(x)$ that acts as confidence, and impose a Lipschitz-style constraint so that similar inputs receive similar scores and, hence, consistent outputs. Evaluated on the {MedQA (US)} benchmark, Metric-Fair Prompting is shown to improve performance over standard single-item prompting, demonstrating that fairness-guided, confidence-oriented reasoning can enhance LLM accuracy on high-stakes clinical multiple-choice questions.

</details>


### [66] [PCMind-2.1-Kaiyuan-2B Technical Report](https://arxiv.org/abs/2512.07612)
*Kairong Luo,Zhenbo Sun,Xinyu Shi,Shengqi Chen,Bowen Yu,Yunyi Chen,Chenyi Dang,Hengtao Tao,Hui Wang,Fangming Liu,Kaifeng Lyu,Wenguang Chen*

Main category: cs.CL

TL;DR: PCMind-2.1-Kaiyuan-2B是一个完全开源的20亿参数模型，通过创新的数据混合策略、选择性重复训练和多领域课程训练，在资源受限条件下实现了与最先进开源模型竞争的性能。


<details>
  <summary>Details</summary>
Motivation: 解决开源社区与产业界在大型语言模型知识上的差距，产业界依赖闭源的高质量数据和训练方案，而开源社区缺乏这些资源。作者旨在为资源受限的预训练提供实用且可扩展的解决方案。

Method: 1. 分位数数据基准测试方法：系统比较异构开源数据集，为数据混合策略提供见解；2. 多阶段范式中的战略性选择性重复方案：有效利用稀疏的高质量数据；3. 多领域课程训练策略：按质量排序样本。还包括高度优化的数据预处理流程和FP16稳定性的架构修改。

Result: Kaiyuan-2B实现了与最先进完全开源模型竞争的性能，所有资产（模型权重、数据和代码）都在Apache 2.0许可下发布。

Conclusion: 该工作展示了在资源受限条件下进行高效预训练的实用解决方案，通过创新的数据管理和训练策略，缩小了开源社区与产业界在LLM发展上的差距。

Abstract: The rapid advancement of Large Language Models (LLMs) has resulted in a significant knowledge gap between the open-source community and industry, primarily because the latter relies on closed-source, high-quality data and training recipes. To address this, we introduce PCMind-2.1-Kaiyuan-2B, a fully open-source 2-billion-parameter model focused on improving training efficiency and effectiveness under resource constraints. Our methodology includes three key innovations: a Quantile Data Benchmarking method for systematically comparing heterogeneous open-source datasets and providing insights on data mixing strategies; a Strategic Selective Repetition scheme within a multi-phase paradigm to effectively leverage sparse, high-quality data; and a Multi-Domain Curriculum Training policy that orders samples by quality. Supported by a highly optimized data preprocessing pipeline and architectural modifications for FP16 stability, Kaiyuan-2B achieves performance competitive with state-of-the-art fully open-source models, demonstrating practical and scalable solutions for resource-limited pretraining. We release all assets (including model weights, data, and code) under Apache 2.0 license at https://huggingface.co/thu-pacman/PCMind-2.1-Kaiyuan-2B.

</details>


### [67] [Bridging Code Graphs and Large Language Models for Better Code Understanding](https://arxiv.org/abs/2512.07666)
*Zeqi Chen,Zhaoyang Chu,Yi Gui,Feng Guo,Yao Wan,Chuan Shi*

Main category: cs.CL

TL;DR: CGBridge：一种即插即用方法，通过外部可训练的Bridge模块，将代码图信息注入冻结的大型语言模型，增强其对程序结构语义的理解能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码智能任务中表现优异，但依赖线性化token序列限制了其对程序结构语义的理解能力。现有方法要么受限于提示长度约束，要么需要特定任务架构修改，无法与大规模指令跟随LLMs兼容。

Method: 1) 在27万代码图数据集上自监督预训练代码图编码器学习结构语义；2) 训练外部模块通过跨模态注意力机制桥接代码、图和文本的语义鸿沟；3) 桥接模块生成结构感知提示注入冻结LLM，并针对下游任务微调。

Result: CGBridge显著优于原始模型和图增强提示方法：代码摘要任务上相对提升16.19%和9.12%，代码翻译任务上执行准确率相对提升9.84%和38.87%。推理速度比LoRA调优模型快4倍以上。

Conclusion: CGBridge通过外部可训练桥接模块有效注入代码结构信息，既保持了LLMs的通用性，又增强了结构感知能力，在代码智能任务中实现了效果与效率的双重提升。

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance in code intelligence tasks such as code generation, summarization, and translation. However, their reliance on linearized token sequences limits their ability to understand the structural semantics of programs. While prior studies have explored graphaugmented prompting and structure-aware pretraining, they either suffer from prompt length constraints or require task-specific architectural changes that are incompatible with large-scale instructionfollowing LLMs. To address these limitations, this paper proposes CGBridge, a novel plug-and-play method that enhances LLMs with Code Graph information through an external, trainable Bridge module. CGBridge first pre-trains a code graph encoder via selfsupervised learning on a large-scale dataset of 270K code graphs to learn structural code semantics. It then trains an external module to bridge the modality gap among code, graph, and text by aligning their semantics through cross-modal attention mechanisms. Finally, the bridge module generates structure-informed prompts, which are injected into a frozen LLM, and is fine-tuned for downstream code intelligence tasks. Experiments show that CGBridge achieves notable improvements over both the original model and the graphaugmented prompting method. Specifically, it yields a 16.19% and 9.12% relative gain in LLM-as-a-Judge on code summarization, and a 9.84% and 38.87% relative gain in Execution Accuracy on code translation. Moreover, CGBridge achieves over 4x faster inference than LoRA-tuned models, demonstrating both effectiveness and efficiency in structure-aware code understanding.

</details>


### [68] [When Large Language Models Do Not Work: Online Incivility Prediction through Graph Neural Networks](https://arxiv.org/abs/2512.07684)
*Zihan Chen,Lanyu Yu*

Main category: cs.CL

TL;DR: 提出基于图神经网络的在线不文明行为检测框架，通过结合文本内容和评论间关系结构，在维基百科社区中检测毒性、攻击性和人身攻击三类行为，性能优于12个大型语言模型且推理成本更低。


<details>
  <summary>Details</summary>
Motivation: 在线不文明行为已成为数字社区的普遍问题，给用户带来社会和心理负担。现有平台通过审核和自动检测来遏制不文明行为，但现有方法在准确性和效率方面仍有局限，需要更有效的解决方案。

Method: 提出图神经网络框架，将用户评论表示为节点，基于评论间文本相似性定义边，使网络能够同时学习语言内容和评论间关系结构。引入动态调整的注意力机制，在信息聚合过程中自适应平衡节点特征和拓扑特征。

Result: 实证评估表明，该架构在多个指标上优于12个最先进的大型语言模型，同时需要显著更低的推理成本。结果突显了结构上下文在检测在线不文明行为中的关键作用，并解决了纯文本LLM范式在行为预测中的局限性。

Conclusion: 该研究强调了结构上下文对于在线不文明行为检测的重要性，为解决纯文本LLM方法的局限性提供了有效方案。所有数据集和比较输出将公开提供，以支持进一步研究和可重复性。

Abstract: Online incivility has emerged as a widespread and persistent problem in digital communities, imposing substantial social and psychological burdens on users. Although many platforms attempt to curb incivility through moderation and automated detection, the performance of existing approaches often remains limited in both accuracy and efficiency. To address this challenge, we propose a Graph Neural Network (GNN) framework for detecting three types of uncivil behavior (i.e., toxicity, aggression, and personal attacks) within the English Wikipedia community. Our model represents each user comment as a node, with textual similarity between comments defining the edges, allowing the network to jointly learn from both linguistic content and relational structures among comments. We also introduce a dynamically adjusted attention mechanism that adaptively balances nodal and topological features during information aggregation. Empirical evaluations demonstrate that our proposed architecture outperforms 12 state-of-the-art Large Language Models (LLMs) across multiple metrics while requiring significantly lower inference cost. These findings highlight the crucial role of structural context in detecting online incivility and address the limitations of text-only LLM paradigms in behavioral prediction. All datasets and comparative outputs will be publicly available in our repository to support further research and reproducibility.

</details>


### [69] [HalluShift++: Bridging Language and Vision through Internal Representation Shifts for Hierarchical Hallucinations in MLLMs](https://arxiv.org/abs/2512.07687)
*Sujoy Nath,Arkaprabha Basu,Sharanya Dasgupta,Swagatam Das*

Main category: cs.CL

TL;DR: 提出HalluShift++方法，通过分析MLLM内部层动态的异常来检测幻觉，无需依赖外部LLM评估器


<details>
  <summary>Details</summary>
Motivation: MLLM虽然能生成语言连贯的输出，但经常产生与视觉内容事实不一致的幻觉，可能导致严重后果。现有方法依赖外部LLM评估器，但这些评估器本身也有幻觉问题且存在领域适应挑战。

Method: 提出假设：幻觉表现为MLLM内部层动态的可测量异常，而不仅仅是分布偏移。通过层间分析特定假设，将基于文本LLM的幻觉检测扩展到多模态场景。

Result: 开发了HalluShift++方法，能够有效检测MLLM中的幻觉，代码已开源。

Conclusion: 通过分析MLLM内部层动态异常来检测幻觉是可行的，HalluShift++为多模态场景的幻觉检测提供了新方法，减少了对易出错的外部评估器的依赖。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in vision-language understanding tasks. While these models often produce linguistically coherent output, they often suffer from hallucinations, generating descriptions that are factually inconsistent with the visual content, potentially leading to adverse consequences. Therefore, the assessment of hallucinations in MLLM has become increasingly crucial in the model development process. Contemporary methodologies predominantly depend on external LLM evaluators, which are themselves susceptible to hallucinations and may present challenges in terms of domain adaptation. In this study, we propose the hypothesis that hallucination manifests as measurable irregularities within the internal layer dynamics of MLLMs, not merely due to distributional shifts but also in the context of layer-wise analysis of specific assumptions. By incorporating such modifications, \textsc{\textsc{HalluShift++}} broadens the efficacy of hallucination detection from text-based large language models (LLMs) to encompass multimodal scenarios. Our codebase is available at https://github.com/C0mRD/HalluShift_Plus.

</details>


### [70] [Automated Generation of Custom MedDRA Queries Using SafeTerm Medical Map](https://arxiv.org/abs/2512.07694)
*Francois Vandenhende,Anna Georgiou,Michalis Georgiou,Theodoros Psaras,Ellie Karekla,Elena Hadjicosta*

Main category: cs.CL

TL;DR: SafeTerm是一个AI驱动的药物安全术语系统，通过向量嵌入和相似度计算自动从MedDRA术语库中检索相关不良反应术语，为药物安全监测提供自动化支持。


<details>
  <summary>Details</summary>
Motivation: 在药物上市前安全审查中，将相关不良事件术语分组到标准化MedDRA查询或FDA OCMQs中对于信号检测至关重要。传统方法需要人工操作，效率较低且可能存在主观偏差。

Method: 系统将医学术语查询和MedDRA首选术语嵌入多维向量空间，应用余弦相似度和极值聚类方法，生成按相关性分数排名的术语列表。验证使用FDA OCMQ v3.0的104个查询。

Result: 在中等阈值下召回率>95%，更高阈值下精确度可达86%。最佳阈值(~0.70-0.75)时召回率约50%，精确度约33%。窄术语子集表现类似但需要稍高相似度阈值。

Conclusion: SafeTerm AI系统为自动化MedDRA查询生成提供了可行的补充方法。建议初始使用约0.60的相似度阈值，精炼术语选择时可提高阈值。

Abstract: In pre-market drug safety review, grouping related adverse event terms into standardised MedDRA queries or the FDA Office of New Drugs Custom Medical Queries (OCMQs) is critical for signal detection. We present a novel quantitative artificial intelligence system that understands and processes medical terminology and automatically retrieves relevant MedDRA Preferred Terms (PTs) for a given input query, ranking them by a relevance score using multi-criteria statistical methods. The system (SafeTerm) embeds medical query terms and MedDRA PTs in a multidimensional vector space, then applies cosine similarity and extreme-value clustering to generate a ranked list of PTs. Validation was conducted against the FDA OCMQ v3.0 (104 queries), restricted to valid MedDRA PTs. Precision, recall and F1 were computed across similarity-thresholds. High recall (>95%) is achieved at moderate thresholds. Higher thresholds improve precision (up to 86%). The optimal threshold (~0.70 - 0.75) yielded recall ~50% and precision ~33%. Narrow-term PT subsets performed similarly but required slightly higher similarity thresholds. The SafeTerm AI-driven system provides a viable supplementary method for automated MedDRA query generation. A similarity threshold of ~0.60 is recommended initially, with increased thresholds for refined term selection.

</details>


### [71] [Mary, the Cheeseburger-Eating Vegetarian: Do LLMs Recognize Incoherence in Narratives?](https://arxiv.org/abs/2512.07777)
*Karin de Langis,Püren Öncel,Ryan Peters,Andrew Elfenbein,Laura Kristen Allen,Andreas Schramm,Dongyeop Kang*

Main category: cs.CL

TL;DR: LLMs能识别不连贯叙事，但无法可靠区分连贯与不连贯故事，尤其在角色特质不一致时表现更差，表明其对叙事连贯性理解不完整。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型是否能可靠区分连贯与不连贯的叙事，探索其在故事理解方面的能力局限。

Method: 使用配对叙事数据集，通过探测研究分析LLMs的内部表征和生成响应，测试多种提示变体和推理链方法。

Result: LLMs内部表征能识别不连贯叙事，但生成响应无法可靠区分；对设定违反比角色特质违反更敏感；推理链无法解决内部状态与行为差异。

Conclusion: LLMs对叙事连贯性的理解不完整，更依赖原型世界知识而非基于意义的叙事连贯性构建，存在内部表征与外部行为的不一致。

Abstract: Leveraging a dataset of paired narratives, we investigate the extent to which large language models (LLMs) can reliably separate incoherent and coherent stories. A probing study finds that LLMs' internal representations can reliably identify incoherent narratives. However, LLMs generate responses to rating questions that fail to satisfactorily separate the coherent and incoherent narratives across several prompt variations, hinting at a gap in LLM's understanding of storytelling. The reasoning LLMs tested do not eliminate these deficits, indicating that thought strings may not be able to fully address the discrepancy between model internal state and behavior. Additionally, we find that LLMs appear to be more sensitive to incoherence resulting from an event that violates the setting (e.g., a rainy day in the desert) than to incoherence arising from a character violating an established trait (e.g., Mary, a vegetarian, later orders a cheeseburger), suggesting that LLMs may rely more on prototypical world knowledge than building meaning-based narrative coherence. The consistent asymmetry found in our results suggests that LLMs do not have a complete grasp on narrative coherence.

</details>


### [72] [On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models](https://arxiv.org/abs/2512.07783)
*Charlie Zhang,Graham Neubig,Xiang Yue*

Main category: cs.CL

TL;DR: 该研究通过完全受控的实验框架，揭示了预训练、中期训练和RL后训练在语言模型推理能力发展中的因果作用，发现RL仅在预训练留有足够空间且针对模型能力边界时有效，中期训练在固定计算下比单纯RL更高效。


<details>
  <summary>Details</summary>
Motivation: 当前RL技术虽然提升了语言模型的推理能力，但难以确定这种提升是真正扩展了模型能力还是仅仅利用了预训练中获得的知识。由于现代训练流程缺乏控制（预训练语料不透明、中期训练研究不足、RL目标与未知先验知识复杂交互），需要建立受控实验框架来厘清各训练阶段的因果贡献。

Method: 开发了完全受控的实验框架，使用合成推理任务（具有明确的原子操作、可解析的逐步推理痕迹），系统操纵训练分布。从两个维度评估模型：1）外推泛化到更复杂的组合；2）跨表面上下文的语境泛化。通过该框架分析预训练、中期训练和RL后训练的相互作用。

Result: 1) RL仅在预训练留有足够空间且RL数据针对模型能力边界（困难但尚未超出能力范围的任务）时才能产生真正的能力提升；2) 语境泛化需要最小但足够的预训练暴露，之后RL可以可靠地迁移；3) 在固定计算下，中期训练相比单纯RL能显著提升性能；4) 过程级奖励减少了奖励黑客行为并提高了推理保真度。

Conclusion: 研究阐明了预训练、中期训练和RL在语言模型推理能力发展中的相互作用，为理解和改进推理语言模型训练策略提供了基础。中期训练在训练流程中扮演着核心但未被充分探索的角色，而RL的有效性取决于预训练留下的空间和针对模型能力边界的训练数据。

Abstract: Recent reinforcement learning (RL) techniques have yielded impressive reasoning improvements in language models, yet it remains unclear whether post-training truly extends a model's reasoning ability beyond what it acquires during pre-training. A central challenge is the lack of control in modern training pipelines: large-scale pre-training corpora are opaque, mid-training is often underexamined, and RL objectives interact with unknown prior knowledge in complex ways. To resolve this ambiguity, we develop a fully controlled experimental framework that isolates the causal contributions of pre-training, mid-training, and RL-based post-training. Our approach employs synthetic reasoning tasks with explicit atomic operations, parseable step-by-step reasoning traces, and systematic manipulation of training distributions. We evaluate models along two axes: extrapolative generalization to more complex compositions and contextual generalization across surface contexts. Using this framework, we reconcile competing views on RL's effectiveness. We show that: 1) RL produces true capability gains (pass@128) only when pre-training leaves sufficient headroom and when RL data target the model's edge of competence, tasks at the boundary that are difficult but not yet out of reach. 2) Contextual generalization requires minimal yet sufficient pre-training exposure, after which RL can reliably transfer. 3) Mid-training significantly enhances performance under fixed compute compared with RL only, demonstrating its central but underexplored role in training pipelines. 4) Process-level rewards reduce reward hacking and improve reasoning fidelity. Together, these results clarify the interplay between pre-training, mid-training, and RL, offering a foundation for understanding and improving reasoning LM training strategies.

</details>


### [73] [Collaborative Causal Sensemaking: Closing the Complementarity Gap in Human-AI Decision Support](https://arxiv.org/abs/2512.07801)
*Raunak Jain,Mudita Khurana*

Main category: cs.CL

TL;DR: 论文提出"协作因果感知(CCS)"作为决策支持代理的研究议程，旨在解决当前AI助手在复杂决策中无法与专家有效协作的问题，强调AI应作为认知伙伴参与共同构建和测试因果假设。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的代理在复杂高风险决策环境中表现不佳，人类-AI团队往往表现不如最佳个体，专家在验证循环和过度依赖之间摇摆，承诺的互补性未能实现。这不仅是准确性问题，更是AI辅助概念的根本缺陷。

Method: 提出协作因果感知(CCS)作为研究议程和组织框架，设计作为认知工作伙伴的系统，维护专家推理的演化模型，帮助表达和修订目标，共同构建和压力测试因果假设，并从联合决策结果中学习。

Result: 论文提出了CCS框架的具体挑战：使协作思维具有工具价值的训练生态、共同构建模型的表示和交互协议、以信任和互补性为中心的评估方法。

Conclusion: 这些方向可以将多智能体系统研究重新定位为参与协作感知的代理，使其成为与人类伙伴共同思考的AI队友，实现真正的人类-AI互补性。

Abstract: LLM-based agents are rapidly being plugged into expert decision-support, yet in messy, high-stakes settings they rarely make the team smarter: human-AI teams often underperform the best individual, experts oscillate between verification loops and over-reliance, and the promised complementarity does not materialise. We argue this is not just a matter of accuracy, but a fundamental gap in how we conceive AI assistance: expert decisions are made through collaborative cognitive processes where mental models, goals, and constraints are continually co-constructed, tested, and revised between human and AI. We propose Collaborative Causal Sensemaking (CCS) as a research agenda and organizing framework for decision-support agents: systems designed as partners in cognitive work, maintaining evolving models of how particular experts reason, helping articulate and revise goals, co-constructing and stress-testing causal hypotheses, and learning from the outcomes of joint decisions so that both human and agent improve over time. We sketch challenges around training ecologies that make collaborative thinking instrumentally valuable, representations and interaction protocols for co-authored models, and evaluation centred on trust and complementarity. These directions can reframe MAS research around agents that participate in collaborative sensemaking and act as AI teammates that think with their human partners.

</details>


### [74] [Do Generalisation Results Generalise?](https://arxiv.org/abs/2512.07832)
*Matteo Boglioni,Andrea Sgobbi,Gabriel Tavernini,Francesco Rita,Marius Mosbach,Tiago Pimentel*

Main category: cs.CL

TL;DR: 研究评估LLM在多个OOD测试集上的泛化性能相关性，发现不同模型间的相关性模式不一致


<details>
  <summary>Details</summary>
Motivation: 现有评估LLM泛化能力的方法通常只关注单个OOD数据集，无法准确反映模型部署时面临的多样化数据偏移，需要更全面的评估方法

Method: 在微调过程中评估模型在多个OOD测试集上的性能，通过回归控制域内性能后计算部分相关性，分析不同OOD测试集之间的泛化性能相关性

Result: 分析OLMo2和OPT模型发现，不同OOD测试集之间的泛化性能相关性没有统一趋势，正负相关性高度依赖于具体模型选择

Conclusion: LLM的OOD泛化结果不能简单推广，不同模型在不同OOD测试集上的表现相关性模式各异，需要更细致的评估方法

Abstract: A large language model's (LLM's) out-of-distribution (OOD) generalisation ability is crucial to its deployment. Previous work assessing LLMs' generalisation performance, however, typically focuses on a single out-of-distribution dataset. This approach may fail to precisely evaluate the capabilities of the model, as the data shifts encountered once a model is deployed are much more diverse. In this work, we investigate whether OOD generalisation results generalise. More specifically, we evaluate a model's performance across multiple OOD testsets throughout a finetuning run; we then evaluate the partial correlation of performances across these testsets, regressing out in-domain performance. This allows us to assess how correlated are generalisation performances once in-domain performance is controlled for. Analysing OLMo2 and OPT, we observe no overarching trend in generalisation results: the existence of a positive or negative correlation between any two OOD testsets depends strongly on the specific choice of model analysed.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [75] [Market Reactions and Information Spillovers in Bank Mergers: A Multi-Method Analysis of the Japanese Banking Sector](https://arxiv.org/abs/2512.06550)
*Haibo Wang,Takeshi Tsuyuguchi*

Main category: q-fin.CP

TL;DR: 本研究采用多方法分析日本银行业两次重大并购事件（2005年三菱UFJ金融集团成立和2018年理索纳控股合并），发现市场对并购有显著正面反应，并存在对其他银行的长期正面溢出效应。


<details>
  <summary>Details</summary>
Motivation: 主要银行并购重组会改变金融市场结构，但其估值效应和溢出效应仍存在疑问。本研究旨在探究日本银行业两次重大并购事件的市场反应和溢出效应。

Method: 采用多方法分析：1）事件研究法（市场模型、CAPM、Fama-French三因子模型）计算累积异常收益率；2）向量自回归模型检验格兰杰因果关系和脉冲响应函数分析动态效应；3）倾向得分匹配法估计平均处理效应。

Result: 分析发现并购事件引发了显著正向的市场反应，并且存在对其他银行的长期正面溢出效应，这可能表明日本银行业存在协同效应。

Conclusion: 多方法分析为日本银行业并购事件提供了独特视角，为关注市场效率和系统稳定性的投资者、管理者和监管者提供了有价值的见解。

Abstract: Major bank mergers and acquisitions (M&A) transform the financial market structure, but their valuation and spillover effects remain open to question. This study examines the market reaction to two M&A events: the 2005 creation of Mitsubishi UFJ Financial Group following the Financial Big Bang in Japan, and the 2018 merger involving Resona Holdings after the global financial crisis. The multi-method analysis in this research combines several distinct methods to explore these M&A events. An event study using the market model, the capital asset pricing model (CAPM), and the Fama-French three-factor model is implemented to estimate cumulative abnormal returns (CAR) for valuation purposes. Vector autoregression (VAR) models are used to test for Granger causality and map dynamic effects using impulse response functions (IRFs) to investigate spillovers. Propensity score matching (PSM) helps provide a causal estimate of the average treatment effect on the treated (ATT). The analysis detected a significant positive market reaction to the mergers. The findings also suggest the presence of prolonged positive spillovers to other banks, which may indicate a synergistic effect among Japanese banks. Combining these methods provides a unique perspective on M&A events in the Japanese banking sector, offering valuable insights for investors, managers, and regulators concerned with market efficiency and systemic stability

</details>


### [76] [Unveiling Hedge Funds: Topic Modeling and Sentiment Correlation with Fund Performance](https://arxiv.org/abs/2512.06620)
*Chang Liu*

Main category: q-fin.CP

TL;DR: 本研究首次将主题建模应用于对冲基金文档分析，比较LDA、Top2Vec和BERTopic三种方法，发现LDA在可解释性上最优，Top2Vec在分类性能上最佳。同时建立了文档情感与基金表现的量化框架，发现通用模型DistilBERT优于金融专用FinBERT，且DistilBERT+Top2Vec组合的情感分数与后续基金表现相关性最强。


<details>
  <summary>Details</summary>
Motivation: 对冲基金行业因不透明和披露有限给投资者带来重大挑战，传统上依赖专家解读的定性信息需要转化为系统化的投资信号。本研究旨在探索自动化文本分析在对冲基金文档处理中的应用价值。

Method: 使用包含1,125家对冲基金管理人的35,000多份文档数据集，比较LDA、Top2Vec和BERTopic三种主题建模方法。同时建立文档情感与基金表现的量化框架，比较通用模型DistilBERT和金融专用模型FinBERT在情感分析中的表现。

Result: LDA（20个主题）产生最可解释的结果且在不同主题数量下更稳健；Top2Vec在分类性能上最优。情感分析中，通用模型DistilBERT优于金融专用FinBERT，能更好适应对冲基金文档的多样化语言模式。DistilBERT+Top2Vec组合的情感分数与后续基金表现相关性最强。

Conclusion: 自动化主题建模和情感分析能有效处理对冲基金文档，为投资者提供新的数据驱动决策支持工具，将传统需要专家解读的定性信息转化为系统化的投资信号。

Abstract: The hedge fund industry presents significant challenges for investors due to its opacity and limited disclosure requirements. This pioneering study introduces two major innovations in financial text analysis. First, we apply topic modeling to hedge fund documents-an unexplored domain for automated text analysis-using a unique dataset of over 35,000 documents from 1,125 hedge fund managers. We compared three state-of-the-art methods: Latent Dirichlet Allocation (LDA), Top2Vec, and BERTopic. Our findings reveal that LDA with 20 topics produces the most interpretable results for human users and demonstrates higher robustness in topic assignments when the number of topics varies, while Top2Vec shows superior classification performance. Second, we establish a novel quantitative framework linking document sentiment to fund performance, transforming qualitative information traditionally requiring expert interpretation into systematic investment signals. In sentiment analysis, contrary to expectations, the general-purpose DistilBERT outperforms the finance-specific FinBERT in generating sentiment scores, demonstrating superior adaptability to diverse linguistic patterns found in hedge fund documents that extend beyond specialized financial news text. Furthermore, sentiment scores derived using DistilBERT in combination with Top2Vec show stronger correlations with subsequent fund performance compared to other model combinations. These results demonstrate that automated topic modeling and sentiment analysis can effectively process hedge fund documents, providing investors with new data-driven decision support tools.

</details>


### [77] [DeepSVM: Learning Stochastic Volatility Models with Physics-Informed Deep Operator Networks](https://arxiv.org/abs/2512.07162)
*Kieran A. Malandain,Selim Kalici,Hakob Chakhoyan*

Main category: q-fin.CP

TL;DR: DeepSVM使用物理信息深度算子网络学习Heston模型在整个参数空间上的解算子，无需标注数据，通过硬约束和自适应细化实现高精度期权定价。


<details>
  <summary>Details</summary>
Motivation: 随机波动率模型（SVMs）的实时校准计算瓶颈在于需要反复求解耦合偏微分方程，传统方法计算成本高。

Method: 提出DeepSVM，基于物理信息深度算子网络（PI-DeepONet），采用硬约束架构强制满足终端支付和静态无套利条件，使用残差自适应细化（RAR）稳定高梯度区域的训练。

Result: 训练损失达到10^{-5}，能够在典型市场动态范围内准确预测期权价格，但在平价区域希腊字母计算存在噪声。

Conclusion: DeepSVM实现了高效准确的期权定价，但希腊字母计算的噪声表明在物理信息算子学习中需要更高阶的正则化。

Abstract: Real-time calibration of stochastic volatility models (SVMs) is computationally bottlenecked by the need to repeatedly solve coupled partial differential equations (PDEs). In this work, we propose DeepSVM, a physics-informed Deep Operator Network (PI-DeepONet) designed to learn the solution operator of the Heston model across its entire parameter space. Unlike standard data-driven deep learning (DL) approaches, DeepSVM requires no labelled training data. Rather, we employ a hard-constrained ansatz that enforces terminal payoffs and static no-arbitrage conditions by design. Furthermore, we use Residual-based Adaptive Refinement (RAR) to stabilize training in difficult regions subject to high gradients. Overall, DeepSVM achieves a final training loss of $10^{-5}$ and predicts highly accurate option prices across a range of typical market dynamics. While pricing accuracy is high, we find that the model's derivatives (Greeks) exhibit noise in the at-the-money (ATM) regime, highlighting the specific need for higher-order regularization in physics-informed operator learning.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [78] [Variable L0 Guidance Strategy: Enlarged Operational Envelope and Path-Following](https://arxiv.org/abs/2512.06124)
*Amit Shivam,Manuel C. R. M. Fernandes,Fernando A. C. C. Fontes,Lorenzo Fagiano*

Main category: eess.SY

TL;DR: 提出一种指数变化的前视距离参数用于无人机路径跟踪制导，通过自适应调整前视距离避免转向率饱和，相比固定前视距离方法将非饱和操作区域扩大70%以上。


<details>
  <summary>Details</summary>
Motivation: 传统固定前视距离制导律在航向或横向跟踪误差较大时容易导致转向率饱和，限制了机动能力并增加了控制负担。

Method: 提出可变前视距离策略，通过指数变化的前视参数使制导指令能够适应变化的跟踪误差几何形状，重新塑造前视距离剖面。

Result: 该方法显著扩大了指令转向率保持非饱和的区域，非饱和操作范围相比固定前视距离方法增加70%以上，实现更平滑的轨迹、更早从饱和状态恢复以及更低的控制需求。

Conclusion: 可变前视距离策略在直线和椭圆路径上表现出控制高效且可靠的路径跟踪性能，为无人机制导提供了几何和理论上的改进方案。

Abstract: This paper presents a geometric and theoretical study of an exponentially varying look-ahead parameter for UAV path-following guidance. Conventional guidance laws with a fixed look-ahead distance often drive the vehicle into turn-rate saturation when the heading or cross-track error is large, leading to constrained maneuvers and higher control effort. The proposed variable L0 strategy reshapes the look-ahead profile so that the guidance command adapts to the evolving tracking error geometry. A detailed investigation shows that this adaptation significantly enlarges the region in which the commanded turn rate remains unsaturated, allowing the vehicle to operate smoothly over a broader range of error conditions. For representative settings, the unsaturated operational envelope increases by more than 70% relative to the constant L0 formulation. These geometric insights translate to smoother trajectories, earlier recovery from saturation, and reduced control demand. Simulation studies on straight-line and elliptical paths demonstrate the merits of the variable look-ahead strategy, highlighting its control-efficient and reliable path-following performance.

</details>


### [79] [A Nonlinear Observer for Air-Velocity and Attitude Estimation Using Pitot and Barometric Measurements](https://arxiv.org/abs/2512.06133)
*Melone Nyoba Tchonkeu,Soulaimane Berkane,Tarek Hamel*

Main category: eess.SY

TL;DR: 该论文提出了一种在GNSS拒止环境下使用最小机载传感器估计无人机空速和全姿态的非线性观测器设计方法。


<details>
  <summary>Details</summary>
Motivation: 解决无人机在GNSS拒止环境中仅使用最小机载传感器进行空速和全姿态估计的实际导航挑战。

Method: 1) 进行可观测性分析，建立均匀可观测条件；2) 在SO(3)×R³上设计非线性观测器，整合皮托管、气压高度计和磁力计作为输出，IMU数据作为输入。

Result: 仿真结果表明所提设计具有收敛性和鲁棒性，即使在最小激励轨迹下也能有效工作。

Conclusion: 该论文为GNSS拒止环境下的无人机导航提供了有效的空速和全姿态估计解决方案，具有实际应用价值。

Abstract: This paper addresses the problem of estimating air velocity and full attitude for unmanned aerial vehicles (UAVs) in GNSS-denied environments using minimal onboard sensing-an interesting and practically relevant challenge for UAV navigation. The contribution of the paper is twofold: (i) an observability analysis establishing the conditions for uniform observability, which are useful for trajectory planning and motion control of the UAV; and (ii) the design of a nonlinear observer on SO3R3R that incorporates pitot-tube, barometric altitude, and magnetometer measurements as outputs, with IMU data used as inputs, within a unified framework. Simulation results are presented to confirm the convergence and robustness of the proposed design, including under minimally excited trajectories.

</details>


### [80] [Sparse Neural Approximations for Bilevel Adversarial Problems in Power Grids](https://arxiv.org/abs/2512.06187)
*Young-ho Cho,Harsha Nagarajan,Deepjyoti Deka,Hao Zhu*

Main category: eess.SY

TL;DR: 提出一种基于神经网络代理和物理约束的单层优化方法，用于高效求解电力系统对抗性最坏情况减载问题，显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统双层混合整数规划方法在求解对抗性最坏情况减载问题时，由于拓扑组合爆炸和AC潮流约束的非凸性，计算成本极高，难以在线应用。

Method: 1) 开发单层最优值函数重构；2) 使用数据驱动的神经网络代理下层最优值；3) 嵌入物理约束神经网络确保物理可实现性；4) 通过谱聚类学习稀疏分区神经网络实现可扩展性。

Result: 在IEEE 14和118节点系统上验证，相比传统方法平均最优性差距为5.8%，计算时间控制在1分钟内，能推广到不同负载条件和未见拓扑。

Conclusion: 该方法通过神经网络代理和物理约束，实现了对抗性最坏情况减载问题的高效求解，具有可扩展性和在线快速重计算能力。

Abstract: The adversarial worst-case load shedding (AWLS) problem is pivotal for identifying critical contingencies under line outages. It is naturally cast as a bilevel program: the upper level simulates an attacker determining worst-case line failures, and the lower level corresponds to the defender's generator redispatch operations. Conventional techniques using optimality conditions render the bilevel, mixed-integer formulation computationally prohibitive due to the combinatorial number of topologies and the nonconvexity of AC power flow constraints. To address these challenges, we develop a novel single-level optimal value-function (OVF) reformulation and further leverage a data-driven neural network (NN) surrogate of the follower's optimal value. To ensure physical realizability, we embed the trained surrogate in a physics-constrained NN (PCNN) formulation that couples the OVF inequality with (relaxed) AC feasibility, yielding a mixed-integer convex model amenable to off-the-shelf solvers. To achieve scalability, we learn a sparse, area-partitioned NN via spectral clustering; the resulting block-sparse architecture scales essentially linearly with system size while preserving accuracy. Notably, our approach produces near-optimal worst-case failures and generalizes across loading conditions and unseen topologies, enabling rapid online recomputation. Numerical experiments on the IEEE 14- and 118-bus systems demonstrate the method's scalability and solution quality for large-scale contingency analysis, with an average optimality gap of 5.8% compared to conventional methods, while maintaining computation times under one minute.

</details>


### [81] [Explainable LP-MPC: Shadow Price Contributions Reveal MV-CV Pairings](https://arxiv.org/abs/2512.06194)
*Lim C. Siang,Daniel L. O'Connor*

Main category: eess.SY

TL;DR: 提出一种新颖的LP-MPC可解释性方法，通过将影子价格重新解释为MV-CV关系的归因机制，解决工业控制器异常行为难以解释的问题。


<details>
  <summary>Details</summary>
Motivation: 工业LP-MPC系统包含大量操纵变量和控制变量，虽然LP是白盒模型，但控制器异常行为往往难以合理解释，需要实用的可解释性工具。

Method: 将CV影子价格重新解释为MV在约束CV时贡献的成本敏感性聚合，然后通过线性分配问题解决方案分解为一对一的MV-CV配对。

Result: 提出的MV-CV配对框架可作为在线LP-MPC系统的实用可解释性工具，帮助诊断次优约束并验证控制器行为与原始设计意图的一致性。

Conclusion: 该方法通过重新解释影子价格的角色，为工业LP-MPC系统提供了有效的后验可解释性方法，有助于提升控制系统的透明度和可诊断性。

Abstract: Large industrial LP-MPC (Linear Program-Model Predictive Control) systems contain tens to hundreds of manipulated variables (MVs) and controlled variables (CVs) for honoring constraints while operating at plant-wide economic optima. The LP is a white-box model, yet for a number of reasons, abnormal behaviors in industrial controllers are often difficult to rationalize. We introduce a novel, post-hoc LP explainability method by recasting the role of shadow prices in the LP solution as an attribution mechanism for MV-CV relationships. The core idea is that CV shadow prices are not just intrinsic properties of the optimal solution, but an aggregate of the cost sensitivities contributed by MVs in enforcing CV constraints, which is then resolved into one-to-one MV-CV pairings using a linear sum assignment solution. The proposed MV-CV pairing framework serves as a practical explainability tool for online LP-MPC systems, enabling practitioners to diagnose suboptimal constraints and verify alignment of the controller's behavior with its original design intent and historical constraints.

</details>


### [82] [A Physics-Informed Fixed Skyroad Model for Continuous UAS Traffic Management (C-UTM)](https://arxiv.org/abs/2512.06268)
*Muhammad Junayed Hasan Zahed,Hossein Rastgoftar*

Main category: eess.SY

TL;DR: 提出一种连续无人机交通管理（C-UTM）框架，通过物理信息方法构建固定空中道路，优化分配时变容量空域，实现可扩展、安全高效的城市场景低空无人机交通管理。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体协调框架假设固定数量的智能体，而无人机交通管理需要支持无人机自由进出受限低空空域。空域中的无人机数量随时间变化，车辆在密集、有障碍物的环境中动态加入或离开，需要开发计算高效的管理系统。

Method: 1. 使用物理信息方法在多高度层城市空域中构建固定空中道路，每个空中道路的方向性设计保证完全可达性；2. 提出新颖的连续无人机交通管理（C-UTM）框架，在考虑空域时变容量的情况下，最优分配空中道路给无人机请求。

Result: 提出的模型通过提供可扩展、安全和高效的解决方案，解决了低空无人机交通管理的关键挑战，最大化空域可用性同时确保安全性和效率。

Conclusion: C-UTM框架为城市空域可用性提供了可扩展、安全、高效的解决方案，通过结构化空中道路和优化分配机制，有效管理时变无人机流量，解决了传统固定智能体框架的局限性。

Abstract: Unlike traditional multi-agent coordination frameworks, which assume a fixed number of agents, UAS traffic management (UTM) requires a platform that enables Uncrewed Aerial Systems (UAS) to freely enter or exit constrained low-altitude airspace. Consequently, the number of UAS operating in a given region is time-varying, with vehicles dynamically joining or leaving even in dense, obstacle-laden environments. The primary goal of this paper is to develop a computationally efficient management system that maximizes airspace usability while ensuring safety and efficiency. To achieve this, we first introduce physics-informed methods to structure fixed skyroads across multiple altitude layers of urban airspace, with the directionality of each skyroad designed to guarantee full reachability. We then present a novel Continuous UTM (C-UTM) framework that optimally allocates skyroads to UAS requests while accounting for the time-varying capacity of the airspace. Collectively, the proposed model addresses the key challenges of low-altitude UTM by providing a scalable, safe, and efficient solution for urban airspace usability.

</details>


### [83] [Scale-free weak output synchronization of multi-agent systems with adaptive protocols](https://arxiv.org/abs/2512.06278)
*Anton A. Stoorvogel,Ali Saberi,Zhenwei Liu,Qiaofeng Wen*

Main category: eess.SY

TL;DR: 研究多智能体系统的输出同步问题，设计不依赖网络知识的协议，在具有有向生成树时实现经典输出同步，否则实现弱同步。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体系统的输出同步问题，目标是设计仅依赖智能体动力学、不需要网络知识的协议。现有研究通常需要非线性协议，本文使用自适应协议来解决线性智能体的同步问题。

Method: 使用自适应协议，考虑两类协议：协作协议（协议间有额外通信）和非协作协议（完全分散式，无额外通信）。协议设计仅基于智能体动力学，不依赖网络拓扑信息。

Result: 当网络具有有向生成树时，协议实现经典输出同步；否则实现弱同步（通过网络稳定性实现，网络交换信号收敛到零）。详细解释了弱同步的概念。

Conclusion: 本文提出了不依赖网络知识的自适应协议，能够在不同网络条件下实现输出同步（经典同步或弱同步），同时考虑了协作和非协作两种协议类型。

Abstract: In this paper, we study output synchronization for multi-agent systems. The objective is to design a protocol which only depends on the agent dynamics and does not require any knowledge of the network. If the network has a directed spanning tree then the protocols designed in this paper achieve classical output synchronization. Otherwise, the protocol achieves weak synchronization which is induced by network stability in the sense that the signals exchanged over the network converge to zero. Weak sychronization is explained in detail in this paper. Even though we consider linear agents, it is known that this in general requires nonlinear protocols. In the paper we use adaptive protocols. In the literature, two classes of protocols are considered often called collaborative protocols (with additional communication between the protocols and non-collaborative protocols (sometimes referred to as fully decentralized where the additional communication is not present). This paper considers both of these cases.

</details>


### [84] [Optimal Platoon Formation and Stable Benefit Allocation in Mixed-Energy Truck Fleets under Size Limitations](https://arxiv.org/abs/2512.06283)
*Ting Bai,Xinfeng Ru,Andreas A. Malikopoulos*

Main category: eess.SY

TL;DR: 研究混合能源卡车车队（电动和燃油车）的协同编队形成和收益分配问题，解决编队规模约束下的最优编队结构和稳定收益分配方案。


<details>
  <summary>Details</summary>
Motivation: 混合能源卡车车队的协同编队面临编队规模约束的挑战，该约束限制了每个编队允许的卡车数量，并在寻找最优编队结构时引入了组合耦合问题。

Method: 将问题建模为具有有限联盟规模的联盟博弈，推导出最大化车队整体编队收益的最优联盟结构的闭式特征。在此基础上开发基于类型的最小核心收益分配方案，保证在联盟结构核心内的稳定性。

Result: 提出的框架在所有可行编队配置中始终实现最高的总编队收益，同时提供稳定的收益分配，优于现有基线方法。对于联盟结构核心为空的情况，计算最小核心半径来确定实现近似稳定性所需的最小松弛。

Conclusion: 该研究为混合能源卡车车队的协同编队形成和收益分配提供了一个有效的理论框架，能够处理编队规模约束，实现最优编队结构和稳定的收益分配。

Abstract: In this paper, we investigate cooperative platoon formation and benefit allocation in mixed-energy truck fleets composed of both electric and fuel-powered trucks. The central challenge arises from the platoon-size constraint, which limits the number of trucks permitted in each platoon and introduces combinatorial coupling into the search for optimal platoon formation structures. We formulate this problem as a coalitional game with bounded coalition sizes and derive a closed-form characterization of the optimal coalition structure that maximizes the fleet-wide platooning benefit. Building on this structure, we develop a type-based least-core payoff allocation scheme that guarantees stability within the coalition-structure core (CS-core). For cases in which the CS-core is empty, we compute the least-core radius to determine the minimal relaxation required to achieve approximate stability. Through numerical studies, we demonstrate that the proposed framework consistently achieves the highest total platooning benefit among all feasible formation configurations while providing stable benefit allocations that outperform existing baseline methods.

</details>


### [85] [Distributionally Robust Kalman Filter](https://arxiv.org/abs/2512.06286)
*Minhyuk Jang,Astghik Hakobyan,Insoon Yang*

Main category: eess.SY

TL;DR: 提出一种基于噪声中心化表述的分布鲁棒卡尔曼滤波器，通过将Wasserstein模糊集直接置于过程噪声和测量噪声分布上，在保持经典卡尔曼滤波器解析结构的同时，为所有可行协方差提供先验谱界。


<details>
  <summary>Details</summary>
Motivation: 针对离散时间线性随机系统中噪声统计特性不确定的问题，现有鲁棒滤波器计算复杂度高且缺乏理论保证。需要一种既能处理噪声不确定性，又能保持卡尔曼滤波器计算效率和理论性质的分布鲁棒方法。

Method: 提出分布鲁棒卡尔曼滤波器（DRKF），将Wasserstein模糊集直接应用于过程噪声和测量噪声分布。在时不变设置下，从单个稳态半定规划推导出稳态DRKF，得到恒定增益估计器，其每步计算复杂度与标准卡尔曼滤波器相同。

Result: 建立了稳态解存在性、唯一性和收敛性的条件，证明了该方法在最坏情况均方误差下的渐近极小极大最优性。数值实验验证了理论，表明DRKF在未知或不确定噪声模型下提高了估计精度，同时相比现有鲁棒和分布鲁棒滤波器具有计算优势。

Conclusion: 提出的DRKF方法在保持经典卡尔曼滤波器计算效率的同时，有效处理了噪声统计特性不确定性，为实际应用中噪声模型不确定的系统提供了理论保证强、计算效率高的鲁棒估计解决方案。

Abstract: In this work, we propose a noise-centric formulation of the distributionally robust Kalman filter (DRKF) for discrete-time linear stochastic systems with uncertain noise statistics. By placing Wasserstein ambiguity sets directly on the process and measurement noise distributions, the proposed DRKF preserves the analytical structure of the classical Kalman filter while providing a priori spectral bounds on all feasible covariances. In the time-invariant setting, we derive a steady-state DRKF from a single stationary semidefinite program, yielding a constant-gain estimator with the same per-step computational complexity as the standard Kalman filter. We establish conditions guaranteeing the existence, uniqueness, and convergence of this steady-state solution, and we prove its asymptotic minimax optimality with respect to the worst-case mean-square error. Numerical experiments validate the theory and demonstrate that the proposed DRKF improves estimation accuracy under unknown or uncertain noise models while offering computational advantages over existing robust and distributionally robust filters.

</details>


### [86] [A Hybrid Physics-Based and Reinforcement Learning Framework for Electric Vehicle Charging Time Prediction](https://arxiv.org/abs/2512.06287)
*Praharshitha Aryasomayajula,Ting Bai,Andreas A. Malikopoulos*

Main category: eess.SY

TL;DR: 提出混合预测框架，结合物理分析模型与强化学习，用于准确估计电动汽车充电时间，考虑电池健康状态退化，实现高精度预测和长期适应性。


<details>
  <summary>Details</summary>
Motivation: 准确预测电动汽车充电时间对行程规划、用户满意度和充电基础设施高效运营至关重要。现有方法在历史数据有限时预测精度不足，且缺乏对电池老化影响的长期适应性。

Method: 开发混合预测框架：1) 物理分析模型捕捉非线性恒流/恒压充电动态，显式建模健康状态相关的容量和功率衰减；2) 强化学习组件在运营数据积累过程中逐步优化充电时间预测，实现长期适应。两种模型都考虑了健康状态退化。

Result: 使用5,000个模拟充电会话和公开数据集评估：分析模型达到R²=98.5%，MAPE=2.1%；强化学习模型进一步提升至R²=99.2%，MAPE=1.6%，对应23%的精度提升和35%的抗老化鲁棒性改进。

Conclusion: 混合框架结合了物理模型的可靠性和强化学习的适应性，在历史数据有限和电池老化条件下都能提供准确的充电时间预测，为电动汽车充电管理提供了有效解决方案。

Abstract: In this paper, we develop a hybrid prediction framework for accurate electric vehicle (EV) charging time estimation, a capability that is critical for trip planning, user satisfaction, and efficient operation of charging infrastructure. We combine a physics-based analytical model with a reinforcement learning (RL) approach. The analytical component captures the nonlinear constant-current/constant-voltage (CC--CV) charging dynamics and explicitly models state-of-health (SoH)--dependent capacity and power fade, providing a reliable baseline when historical data are limited. Building on this foundation, we introduce an RL component that progressively refines charging-time predictions as operational data accumulate, enabling improved long-term adaptation. Both models incorporate SoH degradation to maintain predictive accuracy over the battery lifetime. We evaluate the framework using $5{,}000$ simulated charging sessions calibrated to manufacturer specifications and publicly available EV charging datasets. Our results show that the analytical model achieves $R^{2}=98.5\%$ and $\mathrm{MAPE}=2.1\%$, while the RL model further improves performance to $R^{2}=99.2\%$ and $\mathrm{MAPE}=1.6\%$, corresponding to a $23\%$ accuracy gain and $35\%$ improved robustness to battery aging.

</details>


### [87] [Control-Oriented System Identification: Classical, Learning, and Physics-Informed Approaches](https://arxiv.org/abs/2512.06315)
*S. Sivaranjani,Yuanyuan Shi,Nikolay Atanasov,Thai Duong,Jie Feng,Tim Martin,Yuezhu Xu,Vijay Gupta,Frank Allgöwer*

Main category: eess.SY

TL;DR: 本文综述了将系统辨识与控制相关属性和物理信息约束相结合的方法，旨在获得具有可证明保证的、适用于控制的动态系统模型。


<details>
  <summary>Details</summary>
Motivation: 机器学习方法虽然能从噪声、高维和复杂数据中进行系统辨识，但缺乏对控制相关属性的可证明保证。控制理论已识别出许多对分析和控制合成有用的属性（如耗散性、单调性、能量守恒等），将这些属性融入系统辨识可以提供有用的归纳偏置、增强可解释性、实现具有可证明保证的控制合成，并改善样本复杂度。

Method: 将系统辨识表述为优化问题，通过三种方式施加控制相关属性：直接参数化（约束模型结构以构造性地满足所需属性）、软约束（通过正则化或惩罚项鼓励控制相关属性）、硬约束（将控制相关属性作为优化问题的约束）。综述了从经典线性/非线性系统辨识到机器学习方法，再到通过数据驱动和行为表示的直接辨识方法。

Result: 提出了一个统一的框架来融合系统辨识与控制相关属性，提供了包含代码和教程的公开Github仓库作为示例，并识别了未来研究方向。

Conclusion: 将系统辨识与控制相关或物理信息属性相结合，能够提供有用的归纳偏置、增强可解释性、实现具有可证明保证的控制合成，并改善样本复杂度。未来研究方向包括网络化、切换和时变系统的辨识、实验设计，以及弥合数据驱动、学习型和面向控制方法之间的差距。

Abstract: We survey classical, machine learning, and data-driven system identification approaches to learn control-relevant and physics-informed models of dynamical systems. Recently, machine learning approaches have enabled system identification from noisy, high-dimensional, and complex data. However, their utility is limited by their ability to provide provable guarantees on control-relevant properties. Meanwhile, control theory has identified several properties that are useful in analysis and control synthesis, such as dissipativity, monotonicity, energy conservation, and symmetry-preserving structures. We posit that merging system identification with such control-relevant or physics-informed properties can provide useful inductive bias, enhance explainability, enable control synthesis with provable guarantees, and improve sample complexity. We formulate system identification as an optimization problem where control-relevant properties can be enforced through direct parameterization (constraining the model structure to satisfy a desired property by construction), soft constraints (encouraging control-relevant properties through regularization or penalty terms), and hard constraints (imposing control-relevant properties as constraints in the optimization problem). Through this lens, we survey methods to learn physics-informed and control-relevant models spanning classical linear and nonlinear system identification, machine learning approaches, and direct identification through data-driven and behavioral representations. We also provide several expository examples that are accompanied by code and brief tutorials on a public Github repository. We also describe challenging directions for future research, including identification in networked, switched, and time-varying systems, experiment design, and bridging the gaps between data-driven, learning-based, and control-oriented approaches.

</details>


### [88] [Data Generation for Stability Studies of Power Systems with High Penetration of Inverter-Based Resources](https://arxiv.org/abs/2512.06369)
*Francesca Rossi,Mauro Garcia Lorenzo,Eduardo Iraola de Acevedo,Elia Mateu Barriendos,Vinicius Albernaz Lacerda,Francesc Lordan-Gomis,Rosa Badia,Eduardo Prieto-Araujo*

Main category: eess.SY

TL;DR: 开发了一个开源高性能计算框架，用于系统生成电力系统稳定性评估所需的大规模数据集，特别针对高比例逆变器资源（IBR）系统。


<details>
  <summary>Details</summary>
Motivation: 随着逆变器资源（IBR）在电力系统中的渗透率不断增加，系统动态特性发生根本性变化，给稳定性评估带来新挑战。数据驱动方法（特别是机器学习模型）需要大量代表性数据集来捕捉系统稳定性在各种运行条件和控制设置下的变化。

Method: 提出了一个开源高性能计算框架：1）为大规模电力系统定义可扩展的运行空间；2）通过基于灵敏度分析指导的自适应采样策略探索该空间；3）执行小信号稳定性评估以填充高信息内容数据集。该框架有效针对稳定性边界附近的区域，同时保持对可行运行条件的广泛覆盖。工作流完全用Python实现，支持并行执行。

Result: 开发了一个能够创建高质量数据集的工具，该工具支持现代高IBR渗透率电力系统中的数据驱动稳定性研究。框架能够高效生成包含稳定性边界附近关键区域信息的数据集。

Conclusion: 该开源高性能计算框架为电力系统稳定性研究提供了系统化的数据集生成解决方案，特别适用于高IBR渗透率场景，支持数据驱动方法在电力系统稳定性分析中的应用。

Abstract: The increasing penetration of inverter-based resources (IBRs) is fundamentally reshaping power system dynamics and creating new challenges for stability assessment. Data-driven approaches, and in particular machine learning models, require large and representative datasets that capture how system stability varies across a wide range of operating conditions and control settings. This paper presents an open-source, high-performance computing framework for the systematic generation of such datasets. The proposed tool defines a scalable operating space for large-scale power systems, explores it through an adaptive sampling strategy guided by sensitivity analysis, and performs small-signal stability assessments to populate a high-information-content dataset. The framework efficiently targets regions near the stability margin while maintaining broad coverage of feasible operating conditions. The workflow is fully implemented in Python and designed for parallel execution. The resulting tool enables the creation of high-quality datasets that support data-driven stability studies in modern power systems with high IBR penetration.

</details>


### [89] [The E-Rocket: Low-cost Testbed for TVC Rocket GNC Validation](https://arxiv.org/abs/2512.06535)
*Pedro Santos,André Fonte,Pedro Martins,Paulo Oliveira*

Main category: eess.SY

TL;DR: E-Rocket：基于推力矢量控制的低成本电动火箭原型，用于验证GNC算法


<details>
  <summary>Details</summary>
Motivation: 需要一种低成本、可重复使用的测试平台来验证火箭的制导、导航与控制算法，特别是基于推力矢量控制的技术

Method: 使用商用组件和3D打印部件构建电动火箭，采用对转直流无刷电机和伺服驱动的万向节机构实现推力矢量控制，开发基于PX4自动驾驶仪和ROS 2的双计算机定制航电系统

Result: 在室内运动捕捉环境中验证了基于PID的轨迹跟踪控制器，展示了精确的轨迹跟踪性能，确认了E-Rocket作为火箭GNC算法测试平台的适用性

Conclusion: E-Rocket是一个多功能、低成本的火箭GNC算法测试平台，能够有效验证推力矢量控制等先进控制技术

Abstract: This paper presents the E-Rocket, an electric-powered, low-cost rocket prototype for validation of Guidance, Navigation & Control (GNC) algorithms based on Thrust Vector Control (TVC). Relying on commercially available components and 3D printed parts, a pair of contra-rotating DC brushless motors is assembled on a servo-actuated gimbal mechanism that provides thrust vectoring capability. A custom avionics hardware and software stack is developed considering a dual computer setup which leverages the capabilities of the PX4 autopilot and the modularity of ROS 2 to accommodate for tailored GNC algorithms. The platform is validated in an indoor motion-capture arena using a baseline PID-based trajectory tracking controller. Results demonstrate accurate trajectory tracking and confirm the suitability of the E-Rocket as a versatile testbed for rocket GNC algorithms.

</details>


### [90] [Deep Neural Network-Based Aerial Transport in the Presence of Cooperative and Uncooperative UAS](https://arxiv.org/abs/2512.06577)
*Muhammad Junayed Hasan Zahed,Hossein Rastgoftar*

Main category: eess.SY

TL;DR: 提出一个基于深度神经网络的弹性框架，用于无人机系统的去中心化运输和覆盖，该框架能处理不合作代理并保持稳定收敛。


<details>
  <summary>Details</summary>
Motivation: 现有的多无人机运输系统在面对不合作代理时缺乏弹性，需要一种能保持稳定性和收敛性的框架来处理现实中的故障场景。

Method: 使用DNN构建分层通信图，通过前向调度机制分配时变通信权重，动态修剪与不合作代理的边，保持合作代理间的凸前向指导，并通过稀疏线性关系计算全局目标点。

Result: 在完全合作情况下，所有代理快速收敛到目标区域（边界裕度10%）；在部分合作情况下，系统保持合作代理的高收敛性，性能下降仅局限于干扰附近，表现出优雅的弹性和可扩展性。

Conclusion: 前向权重调度、分层导师-学员协调和实时DNN重构相结合，能够在现实故障场景中实现鲁棒且可证明稳定的无人机运输。

Abstract: We present a resilient deep neural network (DNN) framework for decentralized transport and coverage using uncrewed aerial systems (UAS) operating in $\mathbb{R}^n$. The proposed DNN-based mass-transport architecture constructs a layered inter-UAS communication graph from an initial formation, assigns time-varying communication weights through a forward scheduling mechanism that guides the team from the initial to the final configuration, and ensures stability and convergence of the resulting multi-agent transport dynamics. The framework is explicitly designed to remain robust in the presence of uncooperative agents that deviate from or refuse to follow the prescribed protocol. Our method preserves a fixed feed-forward topology but dynamically prunes edges to uncooperative agents, maintains convex, feedforward mentoring among cooperative agents, and computes global desired set points through a sparse linear relation consistent with leader references. The target set is abstracted by $N$ points that become final desired positions, enabling coverage-optimal transport while keeping computation low and guarantees intact. Extensive simulations demonstrate that, under full cooperation, all agents converge rapidly to the target zone with a 10\% boundary margin and under partial cooperation with uncooperative agents, the system maintains high convergence among cooperative agents with performance degradation localized near the disruptions, evidencing graceful resilience and scalability. These results confirm that forward-weight scheduling, hierarchical mentor--mentee coordination, and on-the-fly DNN restructuring yield robust, provably stable UAS transport in realistic fault scenarios.

</details>


### [91] [Learning Reachability of Energy Storage Arbitrage](https://arxiv.org/abs/2512.06600)
*Tomás Tapia,Agustin Castellano,Enrique Mallada,Yury Dvorkin*

Main category: eess.SY

TL;DR: 提出一种结合停止时间奖励和SOC范围目标惩罚的在线优化方法，通过E2E学习框架联合训练价格预测器和控制策略，使储能系统在套利激励与系统可靠性之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 电力系统面临日益增加的天气驱动变异性，越来越依赖灵活但能量有限的储能资源。现有储能控制策略可能短视地过早放电或在关键时段未能保持储备，未能考虑存储能量的未来可靠性价值。

Method: 引入停止时间奖励和SOC范围目标惩罚，将问题建模为具有机会约束终端SOC的在线优化，并嵌入端到端学习框架，联合训练价格预测器和控制策略。

Result: 所提设计增强了目标SOC范围的可达性，在波动条件下提高了利润并降低了利润标准差。

Conclusion: 通过奖励储能在关键时段前保持足够SOC，提出的方法成功将套利激励与系统可靠性对齐，解决了储能控制中的短视行为问题。

Abstract: Power systems face increasing weather-driven variability and, therefore, increasingly rely on flexible but energy-limited storage resources. Energy storage can buffer this variability, but its value depends on intertemporal decisions under uncertain prices. Without accounting for the future reliability value of stored energy, batteries may act myopically, discharging too early or failing to preserve reserves during critical hours. This paper introduces a stopping-time reward that, together with a state-of-charge (SoC) range target penalty, aligns arbitrage incentives with system reliability by rewarding storage that maintains sufficient SoC before critical hours. We formulate the problem as an online optimization with a chance-constrained terminal SoC and embed it in an end-to-end (E2E) learning framework, jointly training the price predictor and control policy. The proposed design enhances reachability of target SoC ranges, improves profit under volatile conditions, and reduces its standard deviation.

</details>


### [92] [Sliding-Mode Control Strategies for PMSM: Benchmarking and Comparative Simulation Study](https://arxiv.org/abs/2512.06603)
*Mubarak Badamasi Aremu,Abdullah Ajasa,Ali Nasir*

Main category: eess.SY

TL;DR: 该论文提出了一个统一的仿真基准，用于比较六种滑模控制技术在水磁同步电机速度调节中的性能，包括传统、积分、终端、分数阶、自适应和超螺旋滑模控制。


<details>
  <summary>Details</summary>
Motivation: 水磁同步电机因其高效率和高功率密度而广泛应用于高性能驱动系统，但其非线性动态、参数不确定性和负载扰动使得控制变得复杂。滑模控制具有强鲁棒性，但存在多种变体且缺乏标准化的评估标准。

Method: 采用标准化的PMSM模型、扰动配置和调谐协议，确保所有方法的公平比较。通过时域响应、积分误差指标（ISE、IAE、ITSE、ITAE）和控制努力剖面评估性能，同时考察计算复杂度和实现可行性。

Result: 结果表明，自适应和高阶滑模控制，特别是超螺旋和自适应变体，在鲁棒性、平滑性和计算成本之间实现了最平衡的权衡。

Conclusion: 该研究提供了一个可复现的基准测试框架、参数选择指南和实用见解，适用于设计适合实时嵌入式实现的高效、低抖振的基于滑模控制的PMSM驱动器。

Abstract: Permanent Magnet Synchronous Motors (PMSMs) are widely employed in high-performance drive systems owing to their high efficiency and power density. However, nonlinear dynamics, parameter uncertainties, and load disturbances complicate their control. Sliding-Mode Control (SMC) offers strong robustness but exists in numerous variants with unstandardized evaluation criteria. This paper presents a unified simulation benchmark and comparative analysis of six representative SMC techniques for PMSM speed regulation: conventional, integral, terminal, fractional-order, adaptive, and super-twisting. A standardized PMSM model, disturbance profile, and tuning protocol are adopted to ensure fair comparison across all methods. Performance is assessed through time-domain responses, integral error indices (ISE, IAE, ITSE, ITAE), and control-effort profiles, while also examining computational complexity and implementation feasibility. Results demonstrate that adaptive and higher-order SMCs, particularly the super-twisting and adaptive variants, achieve the most balanced trade-off between robustness, smoothness, and computational cost. The study provides a reproducible benchmarking framework, parameter-selection guidelines, and practical insights for designing efficient, low-chatter SMC-based PMSM drives suitable for real-time embedded implementation.

</details>


### [93] [Data-driven functional state estimation of complex networks](https://arxiv.org/abs/2512.06614)
*Yuan Zhang,Ziyuan Luo,Wenxuan Xu,Jiayu Wu,Wenqi Cao,Ranbo Cheng,Tingting Qin,Yuanqing Xia,Mohamed Darouach,Aming Li,Tyrone Fernando*

Main category: eess.SY

TL;DR: 提出数据驱动的功能观测器框架，无需系统模型即可估计目标状态变量，适用于复杂网络系统


<details>
  <summary>Details</summary>
Motivation: 动态系统的内部状态通常隐藏且无法完全测量，这对实时监控和控制构成核心挑战。传统观测器设计依赖精确系统模型，但复杂网络系统往往无法获得精确模型

Method: 基于历史轨迹建立功能可观测性准则，开发使用输入-输出数据或部分状态数据构建观测器的方法，包含噪声抑制，可通过Koopman嵌入扩展到非线性网络

Result: 观测器性能达到或超过基于模型的对应方法，甚至适用于不可观测系统，在水网络传感器故障检测、电网负荷频率控制、非线性神经元系统目标估计等应用中展示了广泛实用性

Conclusion: 为模型不可用的复杂系统中的实时目标状态推断提供了实用途径

Abstract: The internal state of a dynamical system, a set of variables that defines its evolving configuration, is often hidden and cannot be fully measured, posing a central challenge for real-time monitoring and control. While observers are designed to estimate these latent states from sensor outputs, their classical designs rely on precise system models, which are often unattainable for complex network systems. Here, we introduce a data-driven framework for estimating a targeted set of state variables, known as functional observers, without identifying the model parameters. We establish a fundamental functional observability criterion based on historical trajectories that guarantees the existence of such observers. We then develop methods to construct observers using either input-output data or partial state data. These observers match or exceed the performance of model-based counterparts while remaining applicable even to unobservable systems. The framework incorporates noise mitigation and can be easily extended to nonlinear networks via Koopman embeddings. We demonstrate its broad utility through applications including sensor fault detection in water networks, load-frequency control in power grids, and target estimation in nonlinear neuronal systems. Our work provides a practical route for real-time target state inference in complex systems where models are unavailable.

</details>


### [94] [From Forecast to Action: A Deep Learning Model for Predicting Power Outages During Tropical Cyclones](https://arxiv.org/abs/2512.06644)
*Yongchuan Yang,Naiyu Wang,Zhenguo Wang,Min Ouyang,Can Wan*

Main category: eess.SY

TL;DR: STO-CAST是一个用于台风期间实时、区域尺度停电预测的深度学习模型，提供4km分辨率的小时级预测，支持6小时短时临近预报和60小时长期预报。


<details>
  <summary>Details</summary>
Motivation: 热带气旋导致的停电对电力系统和社区构成严重风险，需要准确、高分辨率的停电预测来支持主动减灾规划和实时应急响应。

Method: 使用门控循环单元和全连接层，整合静态环境/基础设施属性和动态气象/停电序列，采用Leave-One-Storm-Out交叉验证和保留网格实验来评估泛化能力。

Result: 模型能够生成4km分辨率的小时级停电预测，支持双模式预报：基于实时观测同化的6小时短时预报和基于气象预测的60小时长期预报。台风"梅花"案例研究验证了模型的操作有效性。

Conclusion: STO-CAST提供了一个可扩展的数据驱动解决方案，支持基于风险的应急响应，增强电力系统在日益加剧的热带气旋威胁下的韧性。

Abstract: Power outages caused by tropical cyclones (TCs) pose serious risks to electric power systems and the communities they serve. Accurate, high-resolution outage forecasting is essential for enabling both proactive mitigation planning and real-time emergency response. This study introduces the SpatioTemporal Outage ForeCAST (STO-CAST) model, a deep learning framework developed for real-time, regional-scale outage prediction during TC events with high-resolution outputs in both space and time. STO-CAST integrates static environmental and infrastructure attributes with dynamic meteorological and outage sequences using gated recurrent units (GRUs) and fully connected layers, and is trained via a Leave-One-Storm-Out (LOSO) cross-validation strategy along with holdout grid experiments to demonstrate its preliminary generalization capability to unseen storms and grids. The model produces hourly outage forecasts at a 4 km * 4 km resolution and supports dual forecasting modes: short-term nowcasting with a 6-hour lead time via assimilation of real-time observations, and long-term forecasting with a 60-hour lead time based on evolving meteorological projections. A case study on Typhoon Muifa (2022) demonstrates STO-CAST's operational effectiveness, including error decomposition across model design, meteorological uncertainty, and observation gaps, while highlighting the value of real-time data assimilation and the model's capacity to identify evolving outage hotspots. STO-CAST offers a scalable, data-driven solution to support risk-informed emergency response and enhance power system resilience under intensifying TC threats.

</details>


### [95] [Symmetry-Based Formation Control on Cycle Graphs Using Dihedral Point Groups](https://arxiv.org/abs/2512.06733)
*Zamir Martinez,Daniel Zelazo*

Main category: eess.SY

TL;DR: 提出基于对称性的循环图编队控制框架，利用二面体点群约束，通过n-1个通信链路实现所有C_nv对称配置


<details>
  <summary>Details</summary>
Motivation: 传统编队控制方法通常需要大量通信链路，本文旨在利用对称性约束减少通信需求，同时保证编队性能

Method: 基于二面体点群约束，强制智能体间反射对称性，并锚定指定智能体到其规定的镜像轴，采用矩阵加权拉普拉斯结构的控制律

Result: 仅需n-1个通信链路即可实现所有C_nv对称配置，控制律保证指数收敛到期望对称配置，并可扩展到时变参考轨迹的协调机动

Conclusion: 对称性约束为循环图编队控制提供了高效解决方案，显著减少通信需求，同时保证收敛性和可扩展性

Abstract: This work develops a symmetry-based framework for formation control on cycle graphs using Dihedral point-group constraints. We show that enforcing inter-agent reflection symmetries, together with anchoring a single designated agent to its prescribed mirror axis, is sufficient to realize every $\mathcal{C}_{nv}$-symmetric configuration using only $n-1$ communication links. The resulting control laws have a matrix-weighted Laplacian structure and guarantee exponential convergence to the desired symmetric configuration. Furthermore, we extend the method to enable coordinated maneuvers along a time-varying reference trajectory. Simulation results are provided to support the theoretical analysis.

</details>


### [96] [Distributed Traffic State Estimation in V2X-Enabled Connected Vehicle Networks](https://arxiv.org/abs/2512.06765)
*Vincent de Heij,M. Umar B. Niazi,Saeed Ahmed,Karl Henrik Johansson*

Main category: eess.SY

TL;DR: 提出基于V2X通信的分布式交通状态估计框架，结合基础设施传感器和网联车辆作为协同感知节点，通过分布式卡尔曼滤波和共识协议实现全局状态感知。


<details>
  <summary>Details</summary>
Motivation: 传统交通状态估计方法通常依赖密集的基础设施传感器，成本高且覆盖有限。需要利用网联车辆的感知能力，在稀疏传感器和间歇性通信条件下实现准确的交通状态估计。

Method: 1) 将基础设施传感器和网联车辆作为自主协同感知节点；2) 使用V2X通信共享局部交通估计；3) 采用针对二阶宏观交通流模型定制的分布式卡尔曼滤波；4) 应用共识协议融合异构时空估计；5) 通过显式投影步骤保持密度和流量估计的物理一致性。

Result: 1) 在高速公路瞬态拥堵的微观仿真中，分布式估计器能准确重建非线性冲击波动态；2) 即使在稀疏基础设施传感器和间歇性车辆网络连接条件下仍能保持准确估计；3) 统计分析显示不同网联车辆渗透率对估计精度有显著影响，揭示了网络可观测性的明显相变。

Conclusion: 提出的分布式交通状态估计框架能有效利用网联车辆和基础设施传感器的协同感知能力，在现实通信条件下实现准确的交通状态重建，为智能交通系统提供了可行的解决方案。

Abstract: This paper presents a distributed traffic state estimation framework in which infrastructure sensors and connected vehicles act as autonomous, cooperative sensing nodes. These nodes share local traffic estimates with nearby nodes using Vehicle-to-Everything (V2X) communication. The proposed estimation algorithm uses a distributed Kalman filter tailored to a second-order macroscopic traffic flow model. To achieve global state awareness, the algorithm employs a consensus protocol to fuse heterogeneous spatiotemporal estimates from V2X neighbors and applies explicit projection steps to maintain physical consistency in density and flow estimates. The algorithm's performance is validated through microscopic simulations of a highway segment experiencing transient congestion. Results demonstrate that the proposed distributed estimator accurately reconstructs nonlinear shockwave dynamics, even with sparse infrastructure sensors and intermittent vehicular network connectivity. Statistical analysis explores how different connected vehicle penetration rates affect estimation accuracy, revealing notable phase transitions in network observability.

</details>


### [97] [A Physics-Aware Attention LSTM Autoencoder for Early Fault Diagnosis of Battery Systems](https://arxiv.org/abs/2512.06809)
*Jiong Yang*

Main category: eess.SY

TL;DR: 提出PA-ALSTM-AE框架，通过将电池老化规律（里程）融入深度学习，显著提升电动汽车电池早期故障诊断性能


<details>
  <summary>Details</summary>
Motivation: 电动汽车电池安全至关重要，但早期故障诊断面临挑战：异常信号微弱、动态运行噪声干扰。现有数据驱动方法存在"物理盲区"，导致漏检或误报

Method: 提出物理感知注意力LSTM自编码器（PA-ALSTM-AE），通过多阶段融合机制将电池老化规律（里程）显式集成到深度学习流程中。包括自适应物理特征构建模块选择里程敏感特征，以及物理引导的潜在融合模块基于老化状态动态校准LSTM记忆单元

Result: 在大规模Vloong真实世界数据集上的实验表明，该方法显著优于现有最先进基准方法。特别地，在保持高精度的同时，将早期故障召回率提高了3倍以上

Conclusion: PA-ALSTM-AE为工业电池管理系统提供了稳健的解决方案，通过物理知识与深度学习的有效融合，解决了电池早期故障诊断的关键挑战

Abstract: Battery safety is paramount for electric vehicles. Early fault diagnosis remains a challenge due to the subtle nature of anomalies and the interference of dynamic operating noise. Existing data-driven methods often suffer from "physical blindness" leading to missed detections or false alarms. To address this, we propose a Physics-Aware Attention LSTM Autoencoder (PA-ALSTM-AE). This novel framework explicitly integrates battery aging laws (mileage) into the deep learning pipeline through a multi-stage fusion mechanism. Specifically, an adaptive physical feature construction module selects mileage-sensitive features, and a physics-guided latent fusion module dynamically calibrates the memory cells of the LSTM based on the aging state. Extensive experiments on the large-scale Vloong real-world dataset demonstrate that the proposed method significantly outperforms state-of-the-art baselines. Notably, it improves the recall rate of early faults by over 3 times while maintaining high precision, offering a robust solution for industrial battery management systems.

</details>


### [98] [Bridging Abstraction-Based Hierarchical Control and Moment Matching: A Conceptual Unification](https://arxiv.org/abs/2512.06875)
*Zirui Niu,Mohammad Fahim Shakib,Giordano Scarciotti*

Main category: eess.SY

TL;DR: 该论文在近似仿真分层控制(ASHC)和矩匹配技术之间建立了联系，为两个框架之间的思想交流提供了概念桥梁。


<details>
  <summary>Details</summary>
Motivation: 研究动机是建立近似仿真分层控制(ASHC)和矩匹配技术之间的理论联系，为这两个框架之间的思想交叉融合提供基础。

Method: 通过矩匹配的视角研究ASHC的两个关键要求：有界输出差异和M关系，在线性时不变情况下，通过特定的系统互联结构来解释这些要求。

Result: 证明了在线性时不变情况下，ASHC的两个关键要求都可以通过矩匹配视角来解释，建立了两个框架之间的概念桥梁。

Conclusion: 成功建立了ASHC和矩匹配技术之间的理论联系，为两个框架之间的思想交叉融合奠定了基础，促进了控制理论不同领域之间的交流。

Abstract: In this paper, we establish a relation between approximate-simulation-based hierarchical control (ASHC) and moment matching techniques, and build a conceptual bridge between these two frameworks. To this end, we study the two key requirements of the ASHC technique, namely the bounded output discrepancy and the $M$-relation, through the lens of moment matching. We show that, in the linear time-invariant case, both requirements can be interpreted in the moment matching perspective through certain system interconnection structures. Building this conceptual bridge provides a foundation for cross-pollination of ideas between these two frameworks.

</details>


### [99] [Joint Learning of Feasibility-Aware Signal Temporal Logic and BarrierNet for Robust and Correct Control](https://arxiv.org/abs/2512.06973)
*Shuo Liu,Wenliang Liu,Wei Xiao,Calin A. Belta*

Main category: eess.SY

TL;DR: 提出可行性感知学习框架，将可训练时变高阶控制屏障函数嵌入可微二次规划，自动调整参数以提升STL任务完成率并减少保守性。


<details>
  <summary>Details</summary>
Motivation: 现有CBF-STL方法依赖固定超参数和短视的逐时间步优化，导致过度保守行为、在严格输入限制下不可行，以及难以满足长时域STL任务。

Method: 提出可行性感知学习框架，将可训练的时变高阶控制屏障函数嵌入可微二次规划；引入统一鲁棒性度量；使用三个神经网络（InitNet、RefNet和扩展BarrierNet）协作生成参考输入并自适应调整约束参数。

Result: 仿真结果表明，该框架在严格输入限制下保持高STL鲁棒性，在复杂环境中显著优于固定参数和非自适应基线方法。

Conclusion: 该框架实现了STL满足与严格可行的可微二次规划，无需手动调参，能自动适应时间和初始条件变化，减少保守性同时最大化鲁棒性。

Abstract: Control Barrier Functions (CBFs) have emerged as a powerful tool for enforcing safety in optimization-based controllers, and their integration with Signal Temporal Logic (STL) has enabled the specification-driven synthesis of complex robotic behaviors. However, existing CBF-STL approaches typically rely on fixed hyperparameters and myopic, per-time step optimization, which can lead to overly conservative behavior, infeasibility near tight input limits, and difficulty satisfying long-horizon STL tasks. To address these limitations, we propose a feasibility-aware learning framework that embeds trainable, time-varying High Order Control Barrier Functions (HOCBFs) into a differentiable Quadratic Program (dQP). Our approach provides a systematic procedure for constructing time-varying HOCBF constraints for a broad fragment of STL and introduces a unified robustness measure that jointly captures STL satisfaction, QP feasibility, and control-bound compliance. Three neural networks-InitNet, RefNet, and an extended BarrierNet-collaborate to generate reference inputs and adapt constraint-related hyperparameters automatically over time and across initial conditions, reducing conservativeness while maximizing robustness. The resulting controller achieves STL satisfaction with strictly feasible dQPs and requires no manual tuning. Simulation results demonstrate that the proposed framework maintains high STL robustness under tight input bounds and significantly outperforms fixed-parameter and non-adaptive baselines in complex environments.

</details>


### [100] [Synergies between AI Computing and Power Systems: Metrics, Scheduling, and Resilience](https://arxiv.org/abs/2512.07001)
*Farzaneh Pourahmadi,Olivier Corradi,Pierre Pinson*

Main category: eess.SY

TL;DR: 论文提出将绿色AI与节俭AI概念区分，强调需要建立连接AI计算与电力系统的标准化碳度量，并开发了实时调度和长期规划的两种架构，同时支持从排放优先到稳定性优先的韧性切换。


<details>
  <summary>Details</summary>
Motivation: 当前绿色AI和节俭AI方法虽然互补，但缺乏将AI计算与电力系统背景连接的共享量化基础，需要建立标准化的碳度量来连接算法决策与其物理后果。

Method: 1. 澄清绿色AI与节俭AI概念；2. 开发标准化碳度量作为桥梁；3. 提出两种架构：(i) 实时操作的迭代信号-响应循环，(ii) 长期规划的集成优化框架，学习并编码灵活负载行为；4. 支持韧性切换，在压力事件中从排放优先转向稳定性优先。

Result: 建立了连接AI算法决策与电力系统物理后果的标准化碳度量框架，开发了支持实时操作和长期规划的两种协调架构，并展示了同一协调堆栈如何支持韧性，在压力事件中提供针对性缓解和更快恢复。

Conclusion: 通过标准化碳度量和协调架构，论文为AI计算与电力系统整合提供了系统化框架，不仅优化了能效和排放，还增强了系统韧性，实现了从排放优先到稳定性优先的动态切换能力。

Abstract: In this paper, we first clarify the concepts of green AI versus frugal AI, positioning frugality as efficiency by design and green AI as transparency and accountability. We then argue that these approaches, while complementary, are insufficient without a shared quantitative foundation that links AI computing to power system contexts. This motivates the development of standardized carbon metrics as a bridge between algorithmic decisions and their physical consequences. We next embed these signals into scheduling and planning frameworks, presenting two architectures: (i) an iterative signal-response loop for real-time operations, and (ii) an integrated optimization that learns and encodes flexible-load behavior for long-term planning. Finally, we show how the same coordination stack supports resilience, enabling signals to shift from emissions-first to stability-first during stress events, providing targeted relief and faster restoration.

</details>


### [101] [Green O-RAN Operation: a Modern ML-Driven Network Energy Consumption Optimisation](https://arxiv.org/abs/2512.07006)
*Xuanyu Liang,Ahmed Al-Tahmeesschi,Swarna Chetty,Hamed Ahmadi*

Main category: eess.SY

TL;DR: 该研究提出使用TD3强化学习算法优化O-RAN系统中的RU睡眠模式控制，相比传统DQN方法能实现更精确的连续控制，在密集大规模无线环境中实现超过50%的节能效果。


<details>
  <summary>Details</summary>
Motivation: 下一代移动网络（尤其是6G）的能源需求日益增长，基站RU组件在低流量时段仍保持活跃状态导致高能耗，需要智能控制策略来提高O-RAN系统的能源效率。

Method: 采用TD3（Twin Delayed Deep Deterministic Policy Gradient）强化学习算法，利用其连续动作空间特性来精确控制RU睡眠模式，避免传统离散动作方法（如DQN）在密集大规模无线环境中的动作空间指数增长问题。

Result: 仿真结果显示，该方法相比持续开启基线能实现超过50%的节能效果，TD3相比基于DQN的方法性能提升达6%，同时提供更好的稳定性和更快的收敛速度。

Conclusion: TD3算法在O-RAN系统中能有效优化RU睡眠模式控制，显著提高能源效率，为下一代移动网络的节能问题提供了有效的智能解决方案。

Abstract: The increasing energy demand of next-generation mobile networks, especially 6G, is becoming a major concern, particularly due to the high power usage of base station components RU, which often remain active even during low traffic periods. To tackle this challenge, our study focuses on improving energy efficiency in O-RAN systems using intelligent control strategies. TD3 leverages a continuous action space to overcome the limitations of traditional discrete-action methods like DQN. By avoiding exponential growth in action space, TD3 enables more precise control of RU sleep modes in dense and large radio environments. Simulation results show that our approach consistently achieves over 50% energy savings compared to the always-on baseline, with TD3 outperforming DQN-based methods by up to 6%, while also offering better stability and faster convergence.

</details>


### [102] [Momentum-Accelerated Online Feedback Optimization for Power System Flexibility](https://arxiv.org/abs/2512.07077)
*Florian Klein-Helmkamp,Matthis Berger,Irina Zettl,Andreas Ulbig*

Main category: eess.SY

TL;DR: 提出基于在线反馈优化的控制器框架，用于电力系统灵活性的实时协调，引入动量增强投影步以加速收敛并改善动态性能。


<details>
  <summary>Details</summary>
Motivation: 现代电力系统运行中灵活性日益重要，需要实时协调电力系统灵活性的控制器框架。

Method: 基于在线反馈优化的控制器框架，引入动量增强投影步来加速收敛和改善动态性能。

Result: 动量控制器在配电网在线拥堵管理和多层灵活性调度案例中实现更快收敛并保持约束满足。

Conclusion: 动量增强控制器在大规模电力系统中具有实时灵活性控制的潜力。

Abstract: Flexibility is increasingly gaining importance in modern power system operation. This paper presents a controller framework based on Online Feedback Optimization for real-time coordination of power system flexibility. The proposed approach introduces a momentum-augmented projection-step to accelerate convergence and improve dynamic performance. We derive the controller formulation, and evaluate its performance and stability in two representative case studies. The first examines online congestion management in distribution feeders, and the second addresses multi-layer flexibility dispatch across system interfaces. Numerical results demonstrate that the momentum-based controller achieves faster convergence and maintains constraint satisfaction, highlighting its potential for real-time flexibility control in large-scale power systems.

</details>


### [103] [A Structured Review of Fixed and Multimodal Sensing Techniques for Bat Monitoring](https://arxiv.org/abs/2512.07153)
*Maatla Sefawe,Sravya Ganti,Julianna Segalla,Erwei He,Isaac Tourner,Julia Gersey*

Main category: eess.SY

TL;DR: 本文综述了用于监测蝙蝠种群的固定传感技术，包括红外传感器、摄像头、雷达和声学探测器，分析了各种技术的覆盖范围、应用、准确性和局限性，为未来研究提供指导。


<details>
  <summary>Details</summary>
Motivation: 有效监测移动动物种群对生态研究、野生动物管理和农业应用至关重要。蝙蝠监测有助于了解疾病传播、迁徙模式、种群动态以及环境变化对蝙蝠群体的影响。

Method: 本文首先介绍蝙蝠生物学的背景信息，然后系统综述四种固定传感技术：红外传感器、摄像头、雷达和声学探测器，分析每种方法面临的独特挑战和贡献。

Result: 综述了各种传感技术的覆盖范围、应用场景、准确性和局限性，为研究人员提供了不同监测方法的全面比较和评估。

Conclusion: 通过综合最新进展，本文为蝙蝠监测领域提供了全面的技术概述，旨在指导未来研究方向和传感技术的发展。

Abstract: Effective monitoring of mobile animal populations is crucial for ecological research, wildlife management, and agricultural applications. Monitoring of bats specifically can help understand the spread of disease as well as shine light on bat migration patterns, population dynamics, and the impacts of environmental changes on bat colonies. Fixed sensing modalities, such as infrared sensors, cameras, radar, and acoustic detectors, play a pivotal role in tracking and understanding animal behavior. This survey goes over context-informing details about bat biology, and then reviews these fixed sensing modalities, discussing the unique challenges and contributions of each approach. We highlight the coverage, applications, accuracy, and limitations associated with each of these sensing modalities. By synthesizing recent advances, we provide a comprehensive overview to guide future research in this area.

</details>


### [104] [Off-grid solar energy storage system with hybrid lithium iron phosphate (LFP) and lead-acid batteries in high mountains: a case report of Jiujiu Cabins in Taiwan](https://arxiv.org/abs/2512.07353)
*Hsien-Ching Chung*

Main category: eess.SY

TL;DR: 台湾雪霸国家公园九九山庄的离网太阳能储能系统发生故障后，通过现场调查、系统重组修复，并引入磷酸铁锂电池形成混合储能系统，实现了更环保可靠的离网供电。


<details>
  <summary>Details</summary>
Motivation: 高山山屋的能源供应是一个重要问题，使用可再生能源是合适的解决方案。九九山庄作为台湾著名山屋，其离网太阳能储能系统发生严重故障导致断电，需要进行修复和系统改进。

Method: 进行详细的现场调查，实施系统重组和修复项目，同时用环保的磷酸铁锂电池部分替代铅酸电池，形成混合储能系统。

Result: 能源系统恢复正常运行，混合储能系统提供了更好、更环保的离网太阳能储能解决方案。

Conclusion: 通过系统重组、修复和引入磷酸铁锂电池，成功解决了高山山屋的能源供应问题，为类似离网太阳能储能系统提供了改进方案和未来规划。

Abstract: Mountain huts are buildings located at high altitude, offering a place for hikers and providing shelter. Energy supply on mountain huts is still an open issue. Using renewable energies could be an appropriate solution. Jiujiu Cabins, a famous mountain hut in Shei-Pa National Park, Taiwan, has operated an off-grid solar energy storage system (ESS) with lead-acid batteries. In 2021, a serious system failures took place, leading to no electricity. After an detailed on-site survey, a reorganization and repair project implemented, the energy system came back to operate normally. Meanwhile, a eco-friendly lithium iron phosphate battery (LFP battery) ESS replaces part of the lead-acid battery ESS, forming a hybrid ESS, making a better and green off-grid solar ESS. In this case report, the energy architecture, detailed descriptions, and historical status of the system are provided. An on-site survey of the failed energy system, a system improvement project, and future plan are listed.

</details>


### [105] [Safety-Critical Control on Lie Groups Using Energy-Augmented Zeroing Control Barrier Functions](https://arxiv.org/abs/2512.07395)
*Alessandro Letti,Riccardo Zanella,Alessandro Macchelli,Federico Califano*

Main category: eess.SY

TL;DR: 在完全驱动机械系统上，通过李群上的零化控制屏障函数实现安全临界控制，提出两类ZCBF：一类用于运动学约束（避障），另一类用于动能限制（安全物理交互）。


<details>
  <summary>Details</summary>
Motivation: 研究如何在完全驱动机械系统上实现安全临界控制，特别是在李群框架下，需要处理运动学约束（如避障）和动力学约束（如安全物理交互）的安全控制问题。

Method: 提出并理论验证两类零化控制屏障函数：第一类用于强制执行运动学约束，适用于避障算法；第二类用于在规定的惯性坐标系平移和旋转方向上限制动能，确保安全物理交互。

Result: 通过狭缝穿越和安全着陆场景的数值仿真验证了所提方法的有效性和通用性，展示了ZCBF在李群框架下处理安全临界控制问题的能力。

Conclusion: 基于李群的零化控制屏障函数为完全驱动机械系统提供了一种有效的安全临界控制框架，能够同时处理运动学约束和动力学约束，在复杂安全场景中表现出良好的性能。

Abstract: We study safety-critical control on fully actuated mechanical systems by means of Zeroing Control Barrier Functions (ZCBFs) defined on Lie Groups. Specifically, we introduce and theoretically validate two classes of ZCBFs. The first enforces kinematic constraints, suitable for implementing obstacle avoidance algorithms. The second enforces kinetic energy limits along prescribed inertial-frame translational and rotational directions, relevant for ensuring safe physical interaction. Numerical simulations involving slit traversal and safe landing scenarios are presented to validate the effectiveness and versatility of the proposed methodology.

</details>


### [106] [Scalable Formal Verification of Incremental Stability in Large-Scale Systems Using Graph Neural Networks](https://arxiv.org/abs/2512.07448)
*Ahan Basu,Mahathi Anand,Pushpak Jagtap*

Main category: eess.SY

TL;DR: 提出基于图神经网络的分布式框架，用于验证具有未知动态和已知互连结构的大规模系统的增量稳定性


<details>
  <summary>Details</summary>
Motivation: 大规模系统通常具有复杂的非线性动态和互连结构，传统稳定性分析方法难以处理未知动态的系统，需要数据驱动的方法来验证增量稳定性

Method: 1) 为子系统构建局部增量李雅普诺夫函数；2) 使用图神经网络以数据驱动方式合成这些函数；3) 通过组合局部函数获得互连系统的李雅普诺夫函数；4) 利用训练神经网络的Lipschitz边界保证形式正确性

Result: 方法在两个非线性案例研究中得到验证，证明其有效性

Conclusion: 提出的基于图神经网络的分布式框架能够有效验证具有未知动态的大规模互连系统的增量稳定性，为复杂系统分析提供了数据驱动的解决方案

Abstract: This work proposes a novel distributed framework for verifying the incremental stability of large-scale systems with unknown dynamics and known interconnection structures using graph neural networks. Our proposed approach relies on the construction of local incremental Lyapunov functions for subsystems, which are then composed together to obtain a suitable Lyapunov function for the interconnected system. Graph neural networks are used to synthesize these functions in a data-driven fashion. The formal correctness guarantee is then obtained by leveraging Lipschitz bounds of the trained neural networks. Finally, the effectiveness of our approach is validated through two nonlinear case studies.

</details>


### [107] [Control of Discrete-Time Linear Systems with Charge-Balanced Inputs](https://arxiv.org/abs/2512.07506)
*Yuzhen Qin,Zonglin Liu,Marcel van Gerven*

Main category: eess.SY

TL;DR: 研究电荷平衡刺激在离散时间线性系统中引导状态轨迹的能力，推导了周期性和非周期性电荷平衡输入的能达性和能控性条件


<details>
  <summary>Details</summary>
Motivation: 脑电刺激需要电荷平衡以确保安全，但电荷平衡刺激如何工作仍不清楚。本文旨在理解电荷平衡输入如何引导系统状态轨迹，为开环和自适应神经刺激协议提供理论基础

Method: 研究离散时间线性系统中两种电荷平衡输入结构：周期性（重复）电荷平衡输入和非重复电荷平衡输入。为每种情况推导能达性和能控性条件，并通过最小能量控制输入设计的数值演示进行验证

Result: 推导了电荷平衡输入的能达性和能控性条件，并通过数值演示验证了理论结果，展示了最小能量控制输入的设计

Conclusion: 电荷平衡输入能够有效引导离散时间线性系统的状态轨迹，为神经刺激的安全应用提供了理论框架和设计指导

Abstract: Electrical brain stimulation relies on externally applied currents to modulate neural activity, but safety constraints require each stimulation cycle to be charge-balanced, enforcing a zero net injected charge. However, how such charge-balanced stimulation works remains poorly understood. This paper investigates the ability of charge-balanced inputs to steer state trajectories in discrete-time linear systems. Motivated by both open-loop and adaptive neurostimulation protocols, we study two practically relevant input structures: periodic (repetitive) charge-balanced inputs and non-repetitive charge-balanced inputs. For each case, we derive novel reachability and controllability conditions. The theoretical results are further validated through numerical demonstrations of minimum-energy control input design.

</details>


### [108] [Data-Driven Robust Safety Verification for Markov Decision Processes](https://arxiv.org/abs/2512.07550)
*Abhijit Mazumdar,Manuela L. Bujorianu,Rafal Wisniewski*

Main category: eess.SY

TL;DR: 提出基于数据驱动的鲁棒安全验证框架，用于具有时变不确定转移概率的随机动态系统，通过Wasserstein距离构建统一模糊集，提供高置信度安全保证。


<details>
  <summary>Details</summary>
Motivation: 现实中的随机动态系统通常具有时变和不确定的转移概率，且无法获得精确的名义转移核，只能通过多个系统执行的样本数据进行学习，需要处理样本有限性和系统内在变化带来的不确定性。

Method: 1) 使用多个系统执行样本构建统一模糊集，同时捕捉转移动态的内在运行间变化和有限样本统计不确定性；2) 通过Wasserstein距离球围绕名义经验分布形式化模糊集；3) 将系统表示为区间马尔可夫决策过程；4) 引入鲁棒安全函数，在区间MDP表示下评估所有一致转移核的到达-避免型概率安全性。

Result: 为真实未知时变系统推导出高置信度安全保证，并通过数值示例验证了所提方法的适用性和有效性。

Conclusion: 该框架为具有时变不确定转移概率的随机系统提供了数据驱动的鲁棒安全验证方法，能够处理有限样本和系统内在变化，为实际工程应用中的安全验证提供了理论保证。

Abstract: In this paper, we propose a data-driven robust safety verification framework for stochastic dynamical systems modeled as Markov decision processes with time-varying and uncertain transition probabilities. Rather than assuming access to the exact nominal transition kernel, we consider the realistic setting where only samples from multiple system executions are available. These samples may correspond to different transition models inside an ambiguity set around the nominal transition kernel. Using these observations, we construct a unified ambiguity set that captures both inherent run-to-run variability in the transition dynamics and finite-sample statistical uncertainty. This ambiguity set is formalized through a Wasserstein-distance ball around a nominal empirical distribution and naturally induces an interval Markov decision process representation of the underlying system. Within this representation, we introduce a robust safety function that characterizes reach-avoid type probabilistic safety under all transition kernels consistent with the interval Markov decision process. We further derive high-confidence safety guarantees for the true, unknown time-varying system. A numerical example illustrates the applicability and effectiveness of the proposed approach.

</details>


### [109] [Optimal Pulse Patterns through a Hybrid Optimal Control Perspective](https://arxiv.org/abs/2512.07585)
*Jared Miller,Petros Karamanakos*

Main category: eess.SY

TL;DR: 将多电平变换器的最优脉冲模式合成问题建模为混合系统的最优控制问题，通过凸松弛方法转化为无限维锥规划，利用矩-SOS层次结构求解半定规划序列，获得谐波失真的下界。


<details>
  <summary>Details</summary>
Motivation: 最优脉冲模式（OPPs）是一种通过离线优化开关角度和电平来最小化负载电流谐波失真的调制方法。然而，该优化问题高度非凸，包含三角函数目标函数和约束，以及实数和整数优化变量，求解困难。

Method: 将多电平变换器的OPP合成问题建模为混合系统的最优控制问题，然后利用凸松弛方法将其转化为无限维锥规划（occupation measures）。通过矩-SOS层次结构求解一系列半定规划，这些半定规划的规模与允许的开关转换次数和变换器电压电平数呈线性关系。

Result: 获得了最小可达到谐波失真的下界。该方法能够处理高度非凸的优化问题，且计算复杂度与开关转换次数和电压电平数线性相关。

Conclusion: 通过将OPP合成问题转化为混合系统最优控制问题，并利用凸松弛和矩-SOS层次结构，可以有效地求解这一高度非凸的优化问题，为多电平变换器的谐波失真最小化提供了理论下界和实用计算方法。

Abstract: Optimal pulse patterns (OPPs) are a modulation method in which the switching angles and levels of a switching signal are computed via an offline optimization procedure to minimize a performance metric, typically the harmonic distortions of the load current. Additional constraints can be incorporated into the optimization problem to achieve secondary objectives, such as the limitation of specific harmonics or the reduction of power converter losses. The resulting optimization problem, however, is highly nonconvex, featuring a trigonometric objective function and constraints as well as both real- and integer-valued optimization variables. This work casts the task of OPP synthesis for a multilevel converter as an optimal control problem of a hybrid system. This problem is in turn lifted into a convex but infinite-dimensional conic program of occupation measures using established methods in convex relaxations of optimal control. Lower bounds on the minimum achievable harmonic distortion are acquired by solving a sequence of semidefinite programs via the moment-sum-of-squares hierarchy, where each semidefinite program scales in a jointly linear manner with the numbers of permitted switching transitions and converter voltage levels.

</details>


### [110] [Obstacle Avoidance of UAV in Dynamic Environments Using Direction and Velocity-Adaptive Artificial Potential Field](https://arxiv.org/abs/2512.07609)
*Nikita Vaibhav Pavle,Shrreya Rajneesh,Rakesh Kumar Sahoo,Manoranjan Sinha*

Main category: eess.SY

TL;DR: 提出一种基于方向和相对速度加权的APF方法，结合MPC框架，解决无人机在动态复杂环境中的自主避障问题，有效克服传统APF的局部最小值和运动障碍物处理限制。


<details>
  <summary>Details</summary>
Motivation: 传统人工势场法存在局部最小值问题，且无法考虑运动障碍物的运动学特性，这限制了无人机在动态复杂空域中的自主避障能力。

Method: 提出方向和相对速度加权人工势场法，引入有界加权函数ω(θ,ve)根据障碍物相对于无人机的方向和速度动态调整排斥势能，并将该APF集成到模型预测控制框架中生成满足运动学约束的无碰撞轨迹。

Result: 仿真结果表明，该方法有效解决了局部最小值问题，显著提高了安全性，实现了平滑的预测性避障机动，确保了优越的路径完整性和可靠性能。

Conclusion: 所提出的方法在复杂环境中具有可行的自主导航能力，为无人机在动态拥挤空域中的安全运行提供了有效解决方案。

Abstract: The conventional Artificial Potential Field (APF) is fundamentally limited by the local minima issue and its inability to account for the kinematics of moving obstacles. This paper addresses the critical challenge of autonomous collision avoidance for Unmanned Aerial Vehicles (UAVs) operating in dynamic and cluttered airspace by proposing a novel Direction and Relative Velocity Weighted Artificial Potential Field (APF). In this approach, a bounded weighting function, $ω(θ,v_{e})$, is introduced to dynamically scale the repulsive potential based on the direction and velocity of the obstacle relative to the UAV. This robust APF formulation is integrated within a Model Predictive Control (MPC) framework to generate collision-free trajectories while adhering to kinematic constraints. Simulation results demonstrate that the proposed method effectively resolves local minima and significantly enhances safety by enabling smooth, predictive avoidance maneuvers. The system ensures superior path integrity and reliable performance, confirming its viability for autonomous navigation in complex environments.

</details>


### [111] [Linear Quadratic Control with Non-Markovian and Non-Semimartingale Noise Models](https://arxiv.org/abs/2512.07699)
*Mostafa M. Shibl,Sharan Srinivasan,Harsha Honnappa,Vijay Gupta*

Main category: eess.SY

TL;DR: 本文解决了广义线性二次最优控制问题，其中噪声过程可以是非马尔可夫、非半鞅的，且具有低Hölder正则性。通过使用粗糙路径理论而非传统Itô微积分，推导出最优状态估计和控制策略。


<details>
  <summary>Details</summary>
Motivation: 传统的LQG框架假设布朗噪声过程并依赖Itô微积分工具，这限制了噪声模型的类型。实际应用中，噪声可能具有非马尔可夫、非半鞅特性，且样本路径具有低Hölder正则性，这些噪声模型无法使用标准Itô微积分处理。

Method: 采用粗糙路径理论来表述和解决问题，利用签名表示和受控粗糙路径的方法，推导最优状态估计和控制策略。

Result: 成功解决了广义线性二次最优控制问题，其中噪声过程可以是非马尔可夫、非半鞅的，且具有低Hölder正则性。推导出了适用于这类广义噪声模型的最优状态估计和控制策略。

Conclusion: 粗糙路径理论为处理具有低正则性样本路径的非马尔可夫、非半鞅噪声过程提供了有效的数学框架，扩展了传统LQG控制问题的适用范围。

Abstract: The standard linear quadratic Gaussian (LQG) framework assumes a Brownian noise process and relies on classical stochastic calculus tools, such as those based on Itô calculus. In this paper, we solve a generalized linear quadratic optimal control problem where the process and measurement noises can be non-Markovian and non-semimartingale stochastic processes with sample paths that have low Hölder regularity. Since these noise models do not, in general, permit the use of the standard Itô calculus, we employ rough path theory to formulate and solve the problem. By leveraging signature representations and controlled rough paths, we derive the optimal state estimation and control strategies.

</details>


### [112] [Research on a Monitoring System for High-Voltage Cables in a Coal Mine Based on Intelligent Sensing Technology](https://arxiv.org/abs/2512.07714)
*Z Gao,J Li,L Tao,B Meng*

Main category: eess.SY

TL;DR: 基于智能传感技术的高压电缆在线监测系统，采用高频电流传感器和分布式光纤温度传感器，结合边缘计算与云计算，实现对煤矿高压电缆绝缘缺陷和过热隐患的准确诊断，准确率超95%。


<details>
  <summary>Details</summary>
Motivation: 煤矿高压电缆运行状态监测对保障矿井供电可靠性至关重要，传统监测方法存在局限性，需要智能化的在线监测技术来及时发现绝缘缺陷和过热隐患。

Method: 设计三层架构监测系统，采用高频电流传感器采集局部放电信号，分布式光纤温度传感器获取温度分布数据，通过边缘计算与云计算相结合的方式进行数据分析和故障诊断。

Result: 系统能够准确识别电缆绝缘缺陷和潜在过热隐患，诊断准确率超过95%，显著提升了矿井供电的可靠性。

Conclusion: 智能传感技术在煤矿高压电缆在线监测中具有良好应用效果，该系统为煤矿电力系统安全运行提供了有效的技术保障。

Abstract: Given the importance of monitoring the operational status of high-voltage cables in coal mines, this study investigates the application of intelligent sensing technology to the online monitoring of such cables. Taking an actual coal mine as a case study, a three-layer architecture high-voltage cable monitoring system was designed. The system employs high-frequency current sensors and distributed optical fiber temperature sensors to achieve real-time acquisition of partial discharge signals and temperature distribution data. Data analysis and fault diagnosis are performed through a combined approach of edge computing and cloud computing. The research results demonstrate that the system can accurately identify cable insulation defects and potential overheating hazards, with a diagnostic accuracy exceeding 95%, thereby significantly enhancing the reliability of power supply in mines.

</details>


### [113] [The explicit game-theoretic linear quadratic regulator for constrained multi-agent systems](https://arxiv.org/abs/2512.07749)
*Emilio Benenati,Giuseppe Belgioioso*

Main category: eess.SY

TL;DR: 提出一种高效算法，用于计算受状态和输入约束的有限和无限时域动态博弈的显式开环解，将经典显式约束LQR和MPC框架扩展到多智能体非合作场景。


<details>
  <summary>Details</summary>
Motivation: 传统动态博弈求解方法在线计算成本高，难以满足高采样率需求，特别是在多智能体系统中存在状态和输入约束时，需要更高效的求解算法。

Method: 基于多参数仿射变分不等式刻画开环纳什均衡，将经典显式约束LQR和MPC框架扩展到多智能体非合作设置，实现离线计算和在线快速查询。

Result: 相比最先进的博弈论求解器，在线计算时间实现数量级改进，解精度显著提升，使线性二次博弈论MPC在中等规模多智能体系统中即使在高采样率下也切实可行。

Conclusion: 该方法为受约束动态博弈提供了高效的显式求解框架，显著提升了多智能体系统在线控制性能，为高采样率应用场景提供了可行的技术方案。

Abstract: We present an efficient algorithm to compute the explicit open-loop solution to both finite and infinite-horizon dynamic games subject to state and input constraints. Our approach relies on a multiparametric affine variational inequality characterization of the open-loop Nash equilibria and extends the classical explicit constrained LQR and MPC frameworks to multi-agent non-cooperative settings. A key practical implication is that linear-quadratic game-theoretic MPC becomes viable even at very high sampling rates for multi-agent systems of moderate size. Extensive numerical experiments demonstrate order-of-magnitude improvements in online computation time and solution accuracy compared with state-of-the-art game-theoretic solvers.

</details>


### [114] [Augmented Neural Ordinary Differential Equations for Power System Identification](https://arxiv.org/abs/2512.07757)
*Hannes M. H. Wolf,Christian A. Hans*

Main category: eess.SY

TL;DR: 提出基于增强神经ODE和TCN的新结构，用于电力系统辨识，无需相角测量信息


<details>
  <summary>Details</summary>
Motivation: 现代电力系统复杂，基于一阶原理建模困难；黑箱辨识技术可获取仿真和控制设计所需的动态模型；神经ODE需要系统状态初始值（如相角和频率），但相角通常无法测量

Method: 提出基于增强神经ODE的新结构，结合时间卷积网络（TCN）在历史观测数据上学习潜在相角表示，融合先进深度学习技术

Result: 该方法明显优于简单的增强技术

Conclusion: 提出的方法结合了增强神经ODE和TCN，避免了电力系统辨识中对相角信息的需求，为复杂电力系统建模提供了有效解决方案

Abstract: Due the complexity of modern power systems, modeling based on first-order principles becomes increasingly difficult. As an alternative, dynamical models for simulation and control design can be obtained by black-box identification techniques. One such technique for the identification of continuous-time systems are neural ordinary differential equations. For training and inference, they require initial values of system states, such as phase angles and frequencies. While frequencies can typically be measured, phase angle measurements are usually not available. To tackle this problem, we propose a novel structure based on augmented neural ordinary differential equations, learning latent phase angle representations on historic observations with temporal convolutional networks. Our approach combines state-of-the art deep learning techniques, avoiding the necessity of phase angle information for the power system identification. Results show, that our approach clearly outperforms simpler augmentation techniques.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [115] [Detrended cross-correlations and their random matrix limit: an example from the cryptocurrency market](https://arxiv.org/abs/2512.06473)
*Stanisław Drożdż,Paweł Jarosz,Jarosław Kwapień,Maria Skupień,Marcin Wątorek*

Main category: q-fin.ST

TL;DR: 该研究提出了一种基于多重分形去趋势互相关系数ρ_r的尺度与波动依赖相关矩阵方法，用于分析非平稳、长记忆、重尾系统中的相关性，并应用于加密货币市场。


<details>
  <summary>Details</summary>
Motivation: 复杂系统中的相关性常被非平稳性、长程记忆和重尾波动所掩盖，传统协方差分析在这些情况下效果有限，需要新的方法来区分真实相互依赖与噪声。

Method: 使用多重分形去趋势互相关系数ρ_r构建尺度与波动依赖的相关矩阵，分析其谱特性，并与合成高斯和q高斯信号的谱特性进行比较。将框架应用于2021-2024年140种主要加密货币的分钟收益率数据。

Result: 去趋势、重尾和波动阶参数r共同产生显著偏离随机情况的谱特性，即使在时间序列无互相关时也是如此。加密货币分析揭示了稳健的集体模式，包括主导市场因子和几个部门成分，其强度取决于分析尺度和波动阶。过滤市场模式后，经验特征值分布与随机去趋势互相关的极限高度一致，能清晰识别结构显著异常值。

Conclusion: 该研究为去趋势互相关提供了精细的谱基线，为在复杂、非平稳、重尾系统中区分真实相互依赖与噪声提供了有前景的工具。

Abstract: Correlations in complex systems are often obscured by nonstationarity, long-range memory, and heavy-tailed fluctuations, which limit the usefulness of traditional covariance-based analyses. To address these challenges, we construct scale and fluctuation-dependent correlation matrices using the multifractal detrended cross-correlation coefficient $ρ_r$ that selectively emphasizes fluctuations of different amplitudes. We examine the spectral properties of these detrended correlation matrices and compare them to the spectral properties of the matrices calculated in the same way from synthetic Gaussian and $q$Gaussian signals. Our results show that detrending, heavy tails, and the fluctuation-order parameter $r$ jointly produce spectra, which substantially depart from the random case even under absence of cross-correlations in time series. Applying this framework to one-minute returns of 140 major cryptocurrencies from 2021-2024 reveals robust collective modes, including a dominant market factor and several sectoral components whose strength depends on the analyzed scale and fluctuation order. After filtering out the market mode, the empirical eigenvalue bulk aligns closely with the limit of random detrended cross-correlations, enabling clear identification of structurally significant outliers. Overall, the study provides a refined spectral baseline for detrended cross-correlations and offers a promising tool for distinguishing genuine interdependencies from noise in complex, nonstationary, heavy-tailed systems.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [116] [Unifying Entropy Regularization in Optimal Control: From and Back to Classical Objectives via Iterated Soft Policies and Path Integral Solutions](https://arxiv.org/abs/2512.06109)
*Ajinkya Bhole,Mohammad Mahmoudi Filabadi,Guillaume Crevecoeur,Tom Lefebvre*

Main category: math.OC

TL;DR: 该论文通过Kullback-Leibler正则化提出了一个统一的随机最优控制框架，分离了策略和转移的KL惩罚，能够恢复经典SOC、风险敏感控制及其软策略变体，并发现了具有线性Bellman方程和路径积分解的特殊同步情况。


<details>
  <summary>Details</summary>
Motivation: 现有随机最优控制问题存在多种不同表述（如经典SOC、风险敏感控制等），缺乏统一的理论框架。作者希望通过KL正则化建立一个能够统一这些不同控制问题的生成性结构，同时探索正则化变体与原始问题之间的关系。

Method: 提出一个中心问题，将KL惩罚分别应用于策略和状态转移，赋予它们独立的权重。这个广义公式作为生成结构，能够恢复经典随机最优控制（SOC）、风险敏感最优控制（RSOC）及其策略级KL正则化对应物（软策略SOC和RSOC）。

Result: 1. 软策略公式是原始SOC和RSOC问题的上界，可以通过迭代正则化解来恢复原始解。2. 发现了风险寻求软策略RSOC的特殊同步情况（策略和转移KL正则化权重相同），该情况具有线性Bellman方程、路径积分解和组合性等强大性质。3. 将这些计算友好的性质扩展到更广泛的控制问题类别。

Conclusion: 通过KL正则化建立了一个统一的随机最优控制框架，不仅能够统一多种现有控制问题，还发现了具有优良计算性质的特殊同步情况，为随机最优控制提供了新的理论视角和实用工具。

Abstract: This paper develops a unified perspective on several stochastic optimal control formulations through the lens of Kullback-Leibler regularization. We propose a central problem that separates the KL penalties on policies and transitions, assigning them independent weights, thereby generalizing the standard trajectory-level KL-regularization commonly used in probabilistic and KL-regularized control. This generalized formulation acts as a generative structure allowing to recover various control problems. These include the classical Stochastic Optimal Control (SOC), Risk-Sensitive Optimal Control (RSOC), and their policy-based KL-regularized counterparts. The latter we refer to as soft-policy SOC and RSOC, facilitating alternative problems with tractable solutions. Beyond serving as regularized variants, we show that these soft-policy formulations majorize the original SOC and RSOC problem. This means that the regularized solution can be iterated to retrieve the original solution. Furthermore, we identify a structurally synchronized case of the risk-seeking soft-policy RSOC formulation, wherein the policy and transition KL-regularization weights coincide. Remarkably, this specific setting gives rise to several powerful properties such as a linear Bellman equation, path integral solution, and, compositionality, thereby extending these computationally favourable properties to a broad class of control problems.

</details>


### [117] [A note on Johnson's rule for minimizing makespan in the Two-Machine Flow Shop scheduling problem](https://arxiv.org/abs/2512.06119)
*Federico Della Croce,Quentin Schau*

Main category: math.OC

TL;DR: Johnson规则用于最小化两机流水车间调度的完工时间，虽然最坏情况复杂度为O(n log n)，但本文提出可在线性时间内检测何时能避免完全排序，并在线性时间内计算最优解。


<details>
  <summary>Details</summary>
Motivation: Johnson规则是解决两机流水车间调度问题的经典算法，但其最坏情况复杂度为O(n log n)，需要完全排序作业。本文旨在探索在实际应用中是否能够避免这种完全排序，从而降低计算复杂度。

Method: 提出一种线性时间检测方法，能够判断何时可以避免对作业进行完全排序。通过分析处理时间的分布特征，识别出可以直接在线性时间内计算最优解的情况。

Result: 在标准基准实例上，当处理时间服从均匀分布时，线性时间复杂度总是能够实现。这意味着在实际应用中，Johnson规则可以在线性时间内完成，而不需要O(n log n)的完全排序。

Conclusion: 虽然Johnson规则的最坏情况复杂度为O(n log n)，但在实际应用中，特别是处理时间服从均匀分布时，可以通过线性时间检测避免完全排序，从而在线性时间内获得最优调度方案。

Abstract: We consider Johnson's rule for minimizing the makespan in the two-machine flow shop scheduling problem. We show that although the worst-case complexity of Johnson's rule is O(n log n), since it requires a complete sorting of the jobs, it is possible to detect in linear time whenever a full sort can be avoided and the optimal solution can be computed in linear time. Computational testing indicates that the linear time complexity always occurs in practice on standard benchmark instances with uniform distribution of the processing times.

</details>


### [118] [Data-driven Synchronization for Network Systems with Noiseless Data](https://arxiv.org/abs/2512.06136)
*Yongzhang Li,M. Kanat Camlibel*

Main category: math.OC

TL;DR: 该论文研究了同质网络的数据驱动同步问题，当系统模型未知时，通过收集单个系统的输入-状态数据，提供基于数据的同步条件，并直接从数据中获得同步网络的反馈增益。


<details>
  <summary>Details</summary>
Motivation: 传统的基于模型的同步方法需要已知系统模型，但在实际应用中系统模型往往未知。因此需要研究数据驱动的同步方法，仅通过收集的实验数据来实现网络同步。

Method: 收集单个LTI系统的输入-状态数据，基于网络拓扑结构，提供基于数据的必要和充分同步条件。当条件满足时，直接从数据中获得同步网络的反馈增益，并提供了相应的设计方法。

Result: 提出了数据驱动的同步条件判断方法，能够直接从实验数据中获得同步反馈增益，并通过数值仿真验证了方法的有效性。

Conclusion: 该论文解决了同质网络在系统模型未知情况下的同步问题，提供了一种实用的数据驱动同步方法，仅需单个系统的实验数据和网络拓扑信息即可实现网络同步。

Abstract: For a collection of homogeneous LTI systems that is interconnected by a protocol, given the network topology and the system model, one may obtain a feedback gain to synchronize the network. However, the model-based methods cannot be applied in case the system model is unknown. Therefore, in this paper, we study the data-driven synchronization problem for homogeneous networks. In particular, given a collection of LTI systems, we collect the input-state data from one individual system. Then, given the network topology, we provide data-based necessary and sufficient conditions for synchronizability. Once the conditions are satisfied, one can also obtain a feedback gain directly from data to synchronize the network with the corresponding design method provided in this paper. Finally, we illustrate our results with a numerical simulation.

</details>


### [119] [A geometric view of formation control with application to directed sensing](https://arxiv.org/abs/2512.06195)
*Louis Theran,Daniel Zelazo,Jessica Sidman*

Main category: math.OC

TL;DR: 提出一种基于黎曼梯度下降最小范数提升的几何方法，用于距离编队控制，统一了经典梯度控制器及其有向变体


<details>
  <summary>Details</summary>
Motivation: 为距离编队控制提供统一的几何框架，特别关注有向图情况下的收敛性分析

Method: 基于黎曼梯度下降在边空间的最小范数提升到节点空间的几何方法，构建统一的控制器家族

Result: 提出了适用于任何有向图和目标的局部收敛简单数值测试，证明了持久性对于有向控制器局部收敛既非必要也非充分，并提出了更易检验的必要条件

Conclusion: 该几何方法为距离编队控制提供了统一框架，对有向图情况提出了改进的收敛性分析工具

Abstract: We propose a geometric approach to distance-based formation control modeled on a minimum-norm lifting of Riemannian gradient descent in edge-space to node-space. This yields a unified family of controllers, including the classical gradient controller and its directed variant. For the directed case, we give a simple numerical test for local convergence that applies to any directed graph and target. We show that persistence is neither necessary nor sufficient for local convergence of our directed controller and propose an alternative that is necessary and more easily checked.

</details>


### [120] [New Results on the Polyak Stepsize: Tight Convergence Analysis and Universal Function Classes](https://arxiv.org/abs/2512.06231)
*Chang He,Wenzhi Gao,Bo Jiang,Madeleine Udell,Shuzhong Zhang*

Main category: math.OC

TL;DR: 本文重新研究了梯度下降的Polyak步长策略，证明了其收敛速率的最坏情况紧性，并展示了该步长在不同函数类中的普适性自适应能力。


<details>
  <summary>Details</summary>
Motivation: 重新审视经典的Polyak步长策略，旨在深入理解其收敛行为，包括最坏情况分析和在不同函数类中的普适性表现。

Method: 通过构造最坏情况函数证明收敛速率的紧性，并分析Polyak步长在Hölder光滑性和Hölder增长条件下的收敛保证。

Result: 证明了Polyak步长在光滑强凸函数中的O((1-1/κ)^K)速率和光滑凸函数中的O(1/K)速率都是紧的；发现Polyak步长能自动利用浮点误差逃离最坏情况行为；证明了该步长在Hölder条件下具有普适自适应能力。

Conclusion: Polyak步长不仅在最坏情况下收敛速率是紧的，而且具有普适性，能够自动适应不同函数类而无需问题参数的先验知识。

Abstract: In this paper, we revisit a classical adaptive stepsize strategy for gradient descent: the Polyak stepsize (\texttt{PolyakGD}), originally proposed in \cite{polyak1969minimization}. We study the convergence behavior of \texttt{PolyakGD} from two perspectives: tight worst-case analysis and universality across function classes. As our first main result, we establish the tightness of the known convergence rates of \texttt{PolyakGD} by explicitly constructing worst-case functions. In particular, we show that the $\mathcal{O}((1-\frac{1}κ)^K)$ rate for smooth strongly convex functions and the $\mathcal{O}(1/K)$ rate for smooth convex functions are both tight. Moreover, we theoretically show that \texttt{PolyakGD} automatically exploits floating-point errors to escape the worst-case behavior. Our second main result provides new convergence guarantees for \texttt{PolyakGD} under both Hölder smoothness and Hölder growth conditions. These findings show that the Polyak stepsize is universal, automatically adapting to various function classes without requiring prior knowledge of problem parameters.

</details>


### [121] [Stabilizing Rate of Stochastic Control Systems with Multiplicative Noise](https://arxiv.org/abs/2512.06349)
*Hui Jia,Yuan-Hua Ni,Guangchen Wang*

Main category: math.OC

TL;DR: 提出了一个量化框架来分析具有乘性噪声的随机线性系统的均方指数镇定，重点关注最优镇定率，并开发了可验证的必要充分条件和计算算法。


<details>
  <summary>Details</summary>
Motivation: 研究随机线性系统在乘性噪声下的最优指数镇定问题，现有方法在确定最优镇定率及其可达性方面存在不足，需要开发可验证的计算框架。

Method: 1) 将确定性切换系统的范数技术扩展到随机设置，推导最优镇定率可达性的可验证条件；2) 将状态反馈下的最优镇定率问题重构为非线性成本的最优控制问题，推导Bellman型方程；3) 将Bellman方程转化为非线性矩阵特征值问题，引入正则化方案并开发RNVI算法。

Result: 1) 获得了最优镇定率可达性的可验证必要充分条件及可计算上下界；2) 开发了RNVI算法生成严格正定固定点，产生反馈控制器；3) 正则化解提供了最优镇定率的认证上下界，形成了构造性可验证框架。

Conclusion: 建立了一个完整的量化框架来解决随机线性系统在乘性噪声下的最优指数镇定问题，提供了理论条件、计算算法和可验证边界，为实际系统镇定设计提供了有效工具。

Abstract: This paper develops a quantitative framework for analyzing the mean-square exponential stabilization of stochastic linear systems with multiplicative noise, focusing specifically on the optimal stabilizing rate, which characterizes the fastest exponential stabilization achievable under admissible control policies. 
Our contributions are twofold. First, we extend norm-based techniques from deterministic switched systems to the stochastic setting, deriving a verifiable necessary and sufficient condition for the exact attainability of the optimal stabilizing rate, together with computable upper and lower bounds. Second, by restricting attention to state-feedback policies, we reformulate the optimal stabilizing rate problem as an optimal control problem with a nonlinear cost function and derive a Bellman-type equation. Since this Bellman-type equation is not directly tractable, we recast it as a nonlinear matrix eigenvalue problem whose valid solutions require strictly positive-definite matrices. To ensure the existence of such solutions, we introduce a regularization scheme and develop a Regularized Normalized Value Iteration (RNVI) algorithm, which in turn generates strictly positive-definite fixed points for a perturbed version of original nonlinear matrix eigenvalue problem while producing feedback controllers. Evaluating these regularized solutions further yields certified lower and upper bounds for the optimal stabilizing rate, resulting in a constructive and verifiable framework for determining the fastest achievable mean-square stabilization under multiplicative noise.

</details>


### [122] [A Low-rank Augmented Lagrangian Method for Polyhedral-SDP and Moment-SOS Relaxations of Polynomial Optimization](https://arxiv.org/abs/2512.06359)
*Di Hou,Tianyun Tang,Kim-Chuan Toh*

Main category: math.OC

TL;DR: 提出RiNNAL-POP方法，一种低秩增广拉格朗日算法，用于高效求解多项式优化问题的大规模多面体-SDP松弛


<details>
  <summary>Details</summary>
Motivation: 多项式优化问题(POPs)的多面体-SDP松弛虽然能提供紧的下界，但随着松弛阶数τ增加，变量和约束数量以Ω(n^{2τ})增长，计算代价高昂

Method: 提出RiNNAL-POP方法：1) 设计计算成本与变量数成线性关系的投影方案处理非负性和一致性约束；2) 识别多面体-SDP松弛中的隐藏面结构，通过限制矩阵变量到半定锥的暴露面对应的仿射子空间来消除大量线性约束；3) 在每次ALM迭代中增加对原始矩阵变量的单次投影梯度步，自动调整秩并逃离伪局部极小值

Result: 在各种基准问题上的大量数值实验表明，RiNNAL-POP在求解大规模多面体-SDP松弛时具有鲁棒性和高效性

Conclusion: RiNNAL-POP算法框架能有效解决大规模多项式优化问题的多面体-SDP松弛计算挑战，并可扩展到求解POPs的矩-SOS松弛

Abstract: Polynomial optimization problems (POPs) can be reformulated as geometric convex conic programs, as shown by Kim, Kojima, and Toh (SIOPT 30:1251-1273, 2020), though such formulations remain NP-hard. In this work, we prove that several well-known relaxations can be unified under a common polyhedral-SDP framework, which arises by approximating the intractable cone by tractable intersections of polyhedral cones with the positive semidefinite matrix cone. Although effective in providing tight lower bounds, these relaxations become computationally expensive as the number of variables and constraints grows at the rate of $Ω(n^{2τ})$ with the relaxation order $τ$. To address this challenge, we propose RiNNAL-POP, a low-rank augmented Lagrangian method (ALM) tailored to solve large-scale polyhedral-SDP relaxations of POPs. To efficiently handle the $Ω(n^{2τ})$ nonnegativity and consistency constraints, we design a tailored projection scheme whose computational cost scales linearly with the number of variables. In addition, we identify a hidden facial structure in the polyhedral-SDP relaxation, which enables us to eliminate a large number of linear constraints by restricting the matrix variable to affine subspaces corresponding to exposed faces of the semidefinite cone. The latter enables us to efficiently solve the factorized ALM subproblems over the affine subspaces. At each ALM iteration, we additionally carry out a single projected gradient step with respect to the original matrix variable to automatically adjust the rank and escape from spurious local minima when necessary. We also extend our RiNNAL-POP algorithmic framework to solve moment-SOS relaxations of POPs. Extensive numerical experiments on various benchmark problems demonstrate the robustness and efficiency of RiNNAL-POP in solving large-scale polyhedral-SDP relaxations.

</details>


### [123] [Compressed Momentum-based Single-Point Zero-Order Algorithm for Stochastic Distributed Nonconvex Optimization](https://arxiv.org/abs/2512.06366)
*Linjing Chen,Antai Xie,Xinlei Yi,Xiaoqiang Ren,Xiaofan Wang*

Main category: math.OC

TL;DR: 提出一种基于压缩动量的单点零阶算法，用于分布式随机非凸优化，通过压缩通信和零阶信息降低通信开销并处理梯度不可用问题。


<details>
  <summary>Details</summary>
Motivation: 分布式非凸优化中，通信开销大且显式梯度信息可能不可用，需要开发通信高效且仅需零阶信息的算法。

Method: 基于压缩动量的单点零阶算法：每个智能体仅访问本地目标函数的随机零阶信息，执行带动量的本地随机更新，并与邻居交换压缩后的更新。

Result: 理论证明：使用递减步长时可达到精确解；使用固定步长时可实现向平稳点邻域的子线性收敛率。数值实验验证了算法的有效性和通信效率。

Conclusion: 所提算法能有效降低通信开销，在仅有零阶信息的情况下解决分布式随机非凸优化问题，具有理论和实际价值。

Abstract: This paper studies a compressed momentum-based single-point zeroth-order algorithm for stochastic distributed nonconvex optimization, aiming to alleviate communication overhead and address the unavailability of explicit gradient information. In the developed framework, each agent has access only to stochastic zeroth-order information of its local objective function, performs local stochastic updates with momentum, and exchanges compressed updates with its neighbors. We theoretically prove that the proposed algorithm can achieve the exact solution with diminishing step sizes and can achieve a sublinear convergence rate towards a neighborhood of the stationary point with fixed step sizes. Numerical experiments validate the effectiveness and communication efficiency of the proposed algorithm.

</details>


### [124] [The Joint Range of Quadratic Mapping on Hilbert Space](https://arxiv.org/abs/2512.06437)
*Huu-Quang Nguyen*

Main category: math.OC

TL;DR: 提出一种基于基础数学原理分析希尔伯特空间上二次映射联合范围中隐藏凸结构的新方法


<details>
  <summary>Details</summary>
Motivation: 研究希尔伯特空间上二次映射联合范围中隐藏的凸结构，为相关数学分析提供新视角

Method: 基于基础数学原理，开发分析二次映射联合范围中凸结构的新技术方法

Result: 提出了一种完全依赖基础数学原理的新方法，能够有效分析希尔伯特空间上二次映射的隐藏凸结构

Conclusion: 该方法为分析二次映射的凸结构提供了新的技术途径，完全基于基础数学原理，具有理论简洁性

Abstract: We present a novel technical method for analyzing the hidden convex structure embedded in the joint range of a quadratic mapping defined on a Hilbert space. Our approach stands out by relying exclusively on elementary mathematical principles.

</details>


### [125] [An Approach to the Joint Rapid and Slow Transit Network Design Problem](https://arxiv.org/abs/2512.06540)
*Natividad González-Blanco,Antonio J. Lozano,Vladimir Marianov,Juan A. Mesa*

Main category: math.OC

TL;DR: 本文提出一个整合模型，同时设计快速交通网络并重新设计慢速网络，以最大化公共交通系统的需求覆盖率，采用改进的Benders分解算法求解。


<details>
  <summary>Details</summary>
Motivation: 城市交通拥堵、空气污染等环境问题促使公共交通发展。新建快速交通线路能吸引更多通勤者，但会部分替代现有慢速线路，导致需要取消或调整慢速线路。目前的顺序调整方式通常导致次优解。

Method: 提出一个整合的数学模型，同时设计快速交通网络并重新设计慢速网络。采用数学规划公式，通过特别改进的Benders分解算法求解，包括部分分解以加速计算。

Result: 基于西班牙塞维利亚市交通区域流动性调查的真实数据进行计算实验，验证了模型的有效性。

Conclusion: 同时设计快速和慢速交通网络的整合模型比顺序调整方式更优，能最大化公共交通需求覆盖率，改进的Benders分解算法能有效求解该问题。

Abstract: The increase in congestion in surface traffic, airborne pollution, and other environmental issues have motivated the transit authorities to promote public transit worldwide. In big cities and large metropolitan areas, adding new rapid transit lines attracts more commuters to the public system, as they frequently allow saving travel time as compared to the private mode (car) that faces high congestion. In addition, the travel time has less variability with respect to preset schedules, and rapid lines are more efficient than slow modes operated by buses. When a new rapid transit line is constructed, it partially replaces the traffic of existing slow transit lines. As a consequence, some of the slow-mode lines have to be either canceled or their routes modified to collaborate properly with the new rapid transit line. This process is usually carried out in a sequential way, thus leading to suboptimal solutions.
  In this paper, we consider an integrated model for simultaneously designing rapid and redesigning slow networks. The aim of the model is community-oriented, that is, to maximize the demand covered (or captured) by both modes. We present a mathematical programming formulation that is solved by using a specially improved Benders decomposition. For this purpose, we include a partial decomposition to speed up the computation. The computational experiments are done on a case study based on real data obtained from a survey of mobility among transportation zones in the city of Seville.

</details>


### [126] [Semidefinite hierarchies for diagonal unitary invariant bipartite quantum states](https://arxiv.org/abs/2512.06551)
*Jonas Britz,Monique Laurent*

Main category: math.OC

TL;DR: 论文研究了具有特殊结构性质（对角酉不变性和玻色对称性）的双粒子可分态锥SEP_n，改进了DPS层次结构的计算效率，并建立了与完全正锥CP_n及其对偶锥COP_n的联系。


<details>
  <summary>Details</summary>
Motivation: 量子纠缠检测是量子信息中的基本任务，但计算上很困难。研究具有特殊结构性质的双粒子态可以简化DPS层次结构的实现，提高纠缠检测的效率。

Method: 针对两种特殊结构：(i)对角酉不变态，通过块对角化DPS层次并结合矩重构实现高效计算；(ii)玻色对称态，用厄米复数多项式的平方和来表征对偶层次。

Result: 对于对角酉不变态，实现了DPS层次的高效块对角化实现；对于玻色对称态，给出了对偶层次的平方和表征；建立了与完全正锥CP_n、对偶锥COP_n及其平方和近似K_n^(t)的联系。

Conclusion: 通过利用双粒子态的特殊对称性，可以显著改进DPS层次结构的计算效率，为量子纠缠检测提供了更实用的工具，并揭示了可分态锥与完全正锥之间的深层联系。

Abstract: We investigate questions about the cone $\mathrm{SEP}_n$ of separable bipartite states, consisting of the Hermitian matrices acting on $\mathbb{C}^n\otimes\mathbb{C}^n$ that can be written as conic combinations of rank one matrices of the form $xx^*\otimes yy^*$ with $x,y\in\mathbb{C}^n$. Bipartite states that are not separable are said to be entangled. Detecting quantum entanglement is a fundamental task in quantum information and a hard computational problem. We explore the Doherty-Parrilo-Spedaglieri (DPS) hierarchy of semidefinite conic approximations for $\mathrm{SEP}_n$ when the bipartite states have some additional structural properties: first, (i) for states with diagonal unitary invariance, and second (ii) for states with Bose symmetry. In case (i) we show that the DPS hierarchy can be block diagonalized, which, combining with its moment reformulation, leads to a substantially more efficient implementation. In case (ii), we give a characterization of the dual hierarchy, in terms of sums of squares of Hermitian complex polynomials, extending a known result in the generic case. It turns out that the completely positive cone $\mathrm{CP}_n$, its dual cone $\mathrm{COP}_n$, and their sums-of-squares based conic approximations $\mathcal{K}^{(t)}_n$, play a central role in these two settings (i),(ii). We clarify these connections and test the block diagonal relaxations on classes of examples.

</details>


### [127] [Switched Linear Ensemble Systems and Structural Controllability](https://arxiv.org/abs/2512.06561)
*Haoyu Yin,Yi Li,Ouyang Du,Bruno Sinopoli,Xudong Chen*

Main category: math.OC

TL;DR: 本文研究切换线性系统集合的结构能控性问题，提出判断稀疏模式对给定(k,q)是否结构能控的充要条件，并给出多项式时间算法验证。


<details>
  <summary>Details</summary>
Motivation: 研究切换线性系统集合的能控性问题，旨在理解如何使用共同控制输入同时引导所有子系统，这在多智能体系统、网络控制等应用中具有重要意义。

Method: 通过分析稀疏模式的结构特性，建立结构能控性的充要条件，并将问题转化为最大流问题，设计多项式时间算法进行验证和计算。

Result: 推导出稀疏模式对给定(k,q)结构能控的充要条件，证明该条件比线性时不变系统集合的情况更弱，并给出O(n³)和O(n³ log n)时间复杂度的算法。

Conclusion: 成功解决了切换线性系统集合的结构能控性问题，建立了理论框架和高效算法，为实际应用中的多系统协同控制提供了理论基础。

Abstract: This paper introduces and solves a structural controllability problem for ensembles of switched linear systems. All individual subsystems in the ensemble are sparse, governed by the same sparsity pattern, and undergo switching at the same sequence of time instants. The controllability of an ensemble system describes the ability to use a common control input to simultaneously steer every individual system. A sparsity pattern is called structurally controllable for pair \((k,q)\) if it admits a controllable ensemble of \(q\) individual systems with at most \(k\) switches. We derive a necessary and sufficient condition for a sparsity pattern to be structurally controllable for a given \((k,q)\), and characterize when a sparsity pattern admits a finite \(k\) that guarantees structural controllability for \((k,q)\) for arbitrary $q$. Compared with the linear time-invariant ensemble case, this second condition is strictly weaker. We further show that these conditions have natural connections with maximum flow, and hence can be checked by polynomial algorithms. Specifically, the time complexity of deciding structural controllability is \(O(n^3)\) and the complexity of computing the smallest number of switches needed is \(O(n^3 \log n)\), with \(n\) the dimension of each individual subsystem.

</details>


### [128] [Optimal Preconditioning is a Geodesically Convex Optimization Problem](https://arxiv.org/abs/2512.06618)
*M. Levent Doğan,Alperen Ergür,Elias Tsigaridas*

Main category: math.OC

TL;DR: 提出统一框架计算线性与非线性方程组的近似最优预处理器，证明条件数最小化问题在酉不变范数下是测地凸的，并给出高效一阶算法及收敛保证。


<details>
  <summary>Details</summary>
Motivation: 传统预处理器设计缺乏理论保证，特别是对于非线性系统。本文旨在建立统一的理论框架，为线性和非线性系统提供具有理论保证的预处理器设计方法。

Method: 将条件数最小化问题转化为测地凸优化问题，利用对称李群作用在输入矩阵上，在黎曼商流形上分析对数条件数的凸性，推导显式梯度公式并设计一阶优化算法。

Result: 证明了对数条件数在酉不变范数下是测地凸函数，获得了线性系统最优Frobenius条件数的收敛率：一般双侧预处理器为$\widetilde{O}(1/\eps^2)$，强凸情况为$\widetilde{O}(κ_F^2 \log(1/\eps))$。首次为多项式系统提供了理论保证的预处理器算法。

Conclusion: 建立了预处理器设计的统一理论框架，证明了条件数最小化问题的测地凸性，为线性和非线性系统提供了高效且具有理论保证的预处理器算法。

Abstract: We introduce a unified framework for computing approximately-optimal preconditioners for solving linear and non-linear systems of equations. We demonstrate that the condition number minimization problem, under structured transformations such as diagonal and block-diagonal preconditioners, is geodesically convex with respect to unitarily invariant norms, including the Frobenius and Bombieri--Weyl norms. This allows us to introduce efficient first-order algorithms with precise convergence guarantees. 
For linear systems, we analyze the action of symmetric Lie subgroups $G \subseteq \GL_m(\CC) \times \GL_n(\CC)$ on the input matrix and prove that the logarithm of the condition number is a smooth geodesically convex function on the associated Riemannian quotient manifold. We obtain explicit gradient formulas, show Lipschitz continuity, and prove convergence rates for computing the optimal Frobenius condition number: $\widetilde{O}(1/\eps^2)$ iterations for general two-sided preconditioners and $\widetilde{O}(κ_F^2 \log(1/\eps))$ for strongly convex cases such as left preconditioning. We extend our framework to consider preconditioning of polynomial systems $\f(x) = 0$, where $\f$ is a system of multivariate polynomials. We analyze the local condition number $μ(\f, ξ)$, at a root $ξ$ and prove that it also admits a geodesically convex formulation under appropriate group actions. We deduce explicit formulas for the Riemannian gradients and present convergence bounds for the corresponding optimization algorithms. To the best of our knowledge, this is the first preconditioning algorithm with theoretical guarantees for polynomial systems.

</details>


### [129] [PG-Flow: Deterministic implicit policy gradients for geometric product-form queueing networks](https://arxiv.org/abs/2512.06633)
*Youssef Ait El Mahjoub*

Main category: math.OC

TL;DR: PG-Flow：一种确定性策略梯度框架，用于几何乘积形式排队网络的稳态优化，通过隐式微分和局部伴随系统提供精确梯度，避免轨迹采样。


<details>
  <summary>Details</summary>
Motivation: 乘积形式排队网络（PFQNs）具有可分解的稳态分布，但现有的确定性稳态优化方法有限。需要一种能够直接通过稳态流固定点方程进行微分的方法，以进行高效的优化。

Method: 提出PG-Flow框架，通过隐式微分对稳态流固定点方程进行微分，使用局部伴随系统计算精确梯度，避免轨迹采样和泊松方程。在结构假设（仿射流算子和凸局部成本）下建立全局收敛性，并展示无环网络具有线性时间计算复杂度。

Result: 在Jackson网络的路径控制和能量包网络的能量到达控制实验中，PG-Flow展示了原理性和计算高效性，为几何乘积形式网络的确定性稳态优化提供了有效方法。

Conclusion: PG-Flow为几何乘积形式排队网络的确定性稳态优化提供了一个原则性且计算高效的框架，通过隐式微分和局部伴随系统实现精确梯度计算，在无环网络中具有线性时间计算复杂度。

Abstract: Product-form queueing networks (PFQNs) admit steady-state distributions that factorize into local terms, and in many classical PFQNs including Jackson, BCMP, G-networks, and Energy Packet Networks, these marginals are geometric and parametrized by local flow variables satisfying balance equations. While this structure yields closed-form expressions for key performance metrics, its use for deterministic steady-state optimization remains limited. We introduce PG-Flow, a deterministic policy-gradient framework that differentiates through the steady-state flow fixed-point equations, providing exact gradients via implicit differentiation and a local adjoint system while avoiding trajectory sampling and Poisson equations. We establish global convergence under structural assumptions (affine flow operators and convex local costs), and show that acyclic networks admit linear-time computation of both flows and gradients. Numerical experiments on routing control in Jackson networks and energy-arrival control in Energy Packet Networks demonstrate that PG-Flow provides a principled and computationally efficient approach to deterministic steady-state optimization in geometric product-form networks.

</details>


### [130] [GPU-Accelerated Optimization Solver for Unit Commitment in Large-Scale Power Grids](https://arxiv.org/abs/2512.06715)
*Hussein Sharadga,Javad Mohammadi*

Main category: math.OC

TL;DR: 提出基于GPU加速的PDHG算法求解大规模电力系统机组组合问题，相比传统CPU方法显著减少计算时间


<details>
  <summary>Details</summary>
Motivation: 传统CPU方法求解大规模电力系统机组组合问题计算时间长，需要更高效的求解方法

Method: 使用GPU加速的原始-对偶混合梯度(PDHG)算法高效求解松弛线性子问题，改进边界估计和分支定界收敛

Result: 在4224、6049、6717节点大规模系统上验证，保持解质量的同时实现显著加速

Conclusion: GPU加速的PDHG算法能有效减少大规模机组组合问题的计算时间，具有实际应用价值

Abstract: This work presents a GPU-accelerated solver for the unit commitment (UC) problem in large-scale power grids. The solver uses the Primal-Dual Hybrid Gradient (PDHG) algorithm to efficiently solve the relaxed linear subproblem, achieving faster bound estimation and improved crossover and branch-and-bound convergence compared to conventional CPU-based methods. These improvements significantly reduce the total computation time for the mixed-integer linear UC problem. The proposed approach is validated on large-scale systems, including 4224-, 6049-, and 6717-bus networks with long control horizons and computationally intensive problems, demonstrating substantial speed-ups while maintaining solution quality.

</details>


### [131] [On the Monotonicity and Rate of Convergence of the Markovian Persuasion Value](https://arxiv.org/abs/2512.06794)
*Dimitry Shaiderman*

Main category: math.OC

TL;DR: 研究马尔可夫劝说模型，发现δ折现价值轨迹随δ单调递减，结合已有递增结果得到收敛速率上界，并将结果扩展到马尔可夫链博弈模型。


<details>
  <summary>Details</summary>
Motivation: 研究动态贝叶斯劝说模型中的马尔可夫劝说问题，其中接收者对马尔可夫链状态的信念通过发送者的信号控制，发送者目标是最大化期望折现收益。

Method: 分析马尔可夫劝说模型，证明从任何不变分布出发，δ折现价值轨迹随δ单调递减，结合Lehrer和S.(2025)的递增结果，推导出当马尔可夫链遍历时的收敛速率上界。

Result: 发现δ折现价值轨迹在马尔可夫劝说模型中单调递减，结合已有递增结果，得到了δ折现价值收敛速率的上界估计。

Conclusion: 马尔可夫劝说模型中δ折现价值轨迹具有单调性特征，这一结果可扩展到更一般的马尔可夫链博弈模型，为动态劝说问题的分析提供了理论工具。

Abstract: We study a dynamic Bayesian persuasion model called Markovian persuasion. In such a model, the belief of the receiver regarding the current state of a Markov chain $(X_n)_{n\geq 1}$, over a finite state space $K$, is controlled through signals she obtains from a sender, who observes $(X_n)_{n\geq 1}$ in real time. At each stage $n\geq 1$, the receiver takes an action based on his current belief, which together with the realized state of $X_n$, determines the $n$'th stage payoff of the sender. The sender's goal in a Markovian persuasion game is to find a signaling policy that maximizes her expected $δ$-discounted sum of stage payoffs for a discount factor $δ\in [0,1)$. We show that starting from any invariant distribution $(X_n)_{n\geq 1}$ the trajectory of the $δ$-discounted value is a monotone decreasing in $δ$. By combining this result with the opposite increasing monotone trajectories found in Lehrer and S.\ (2025, GEB), we are able to derive an upper bound on the rate of convergence of the $δ$-discounted values (as $δ\to 1^-$) in the case where $(X_n)_{n\geq 1}$ is ergodic. The results for the Markovian persuasion model are then extended to the Markov chain games model of Renault (2006, MOR).

</details>


### [132] [Optimal and Diffusion Transports in Machine Learning](https://arxiv.org/abs/2512.06797)
*Gabriel Peyré*

Main category: math.OC

TL;DR: 该论文综述了机器学习中时间演化概率分布的设计与分析，重点比较了扩散方法和最优传输两种互补方法，并展示了它们在采样、神经网络优化和大型语言模型中的应用。


<details>
  <summary>Details</summary>
Motivation: 机器学习中的许多问题（如扩散采样、神经网络权重优化、大语言模型token分布演化）都自然地表达为时间演化概率分布的设计与分析。这些应用虽然目标不同，但具有共同的数学结构，需要系统性地研究其理论框架和应用方法。

Method: 采用欧拉表示到拉格朗日表示的转换，通过向量场对流粒子来研究密度演化。重点介绍了两种互补方法：1）基于随机插值过程的扩散方法（支撑现代生成AI）；2）通过最小化位移成本定义插值的最优传输方法。

Result: 展示了这两种方法在多个应用场景中的具体表现：采样方法、神经网络优化、以及大语言模型transformer的动态建模。这些方法在规律性、稳定性和计算可处理性方面展现出有利特性。

Conclusion: 扩散方法和最优传输为机器学习中的时间演化概率分布问题提供了统一的数学框架和有效的解决方案，尽管拉格朗日向量场的非唯一性带来挑战，但也为设计具有良好性质的密度演化和流提供了机会。

Abstract: Several problems in machine learning are naturally expressed as the design and analysis of time-evolving probability distributions. This includes sampling via diffusion methods, optimizing the weights of neural networks, and analyzing the evolution of token distributions across layers of large language models. While the targeted applications differ (samples, weights, tokens), their mathematical descriptions share a common structure. A key idea is to switch from the Eulerian representation of densities to their Lagrangian counterpart through vector fields that advect particles. This dual view introduces challenges, notably the non-uniqueness of Lagrangian vector fields, but also opportunities to craft density evolutions and flows with favorable properties in terms of regularity, stability, and computational tractability. This survey presents an overview of these methods, with emphasis on two complementary approaches: diffusion methods, which rely on stochastic interpolation processes and underpin modern generative AI, and optimal transport, which defines interpolation by minimizing displacement cost. We illustrate how both approaches appear in applications ranging from sampling, neural network optimization, to modeling the dynamics of transformers for large language models.

</details>


### [133] [Urgent Samples in Clinical Laboratories: Stochastic Batching to Minimize Patient Turnaround Time](https://arxiv.org/abs/2512.06820)
*Antonin Novak,Andrzej Gnatowski,Premysl Sucha*

Main category: math.OC

TL;DR: 该论文提出了一种优化医院实验室离心机样本分批策略的方法，通过结合运输时间分布知识，可将危急样本的中位周转时间减少4.9分钟，95%分位数减少9.7分钟。


<details>
  <summary>Details</summary>
Motivation: 医院实验室面临不同优先级样本连续到达且运输时间不确定的问题，需要优化离心机分批策略以减少患者周转时间，特别是对生命威胁的危急样本，以提升患者护理质量。

Method: 提出了在线和离线方法，包括将随机混合整数二次规划模型集成到离散事件系统仿真中，利用真实医院数据进行案例研究，并与完美知识离线算法获得的上界进行比较。

Result: 将运输时间分布知识纳入决策策略后，危急样本的中位患者周转时间减少4.9分钟，95%分位数减少9.7分钟，但对低优先级样本无显著影响。与完美知识上界比较表明该结果接近最优。

Conclusion: 通过优化离心机分批策略并考虑运输时间不确定性，可显著改善危急样本的周转时间，提升患者护理时效性，该方法在实际医院环境中验证有效且接近最优。

Abstract: This paper addresses the problem of batching laboratory samples in hospital laboratories where samples of different priorities are received continuously with uncertain transportation times. The focus is on optimizing the control strategy for loading a centrifuge to minimize patient turnaround time (TAT). While focusing on samples of patients in life-threatening situations (i.e., vital samples), we propose several online and offline methods, including a stochastic mixed-integer quadratic programming model integrated within a discrete-event system simulation. This paper aims to enhance patient care by providing timely laboratory results through improved batching strategies. The case study, which uses real data from a university hospital, demonstrates that incorporating distributional knowledge of transport times into our decision policy can reduce the median patient TAT of vital samples by 4.9 minutes and the 0.95 quantile by 9.7 minutes, but has no significant effect on low-priority samples. In addition, we show that this is essentially an optimal result by comparison with the upper bound obtained by a perfect-knowledge offline algorithm.

</details>


### [134] [OEF (Proximal) Newton-type Method with Inexact Derivatives for Unconstrained Optimization](https://arxiv.org/abs/2512.06825)
*Hong Zhu*

Main category: math.OC

TL;DR: 提出了无需精确目标函数值的近端牛顿法和正则化牛顿法变体，使用不精确的梯度和Hessian评估，保持了与传统方法相同的收敛速率。


<details>
  <summary>Details</summary>
Motivation: 在非凸复合优化和无约束优化问题中，精确评估目标函数及其导数通常计算成本高昂或不可行。本文旨在开发无需精确目标函数评估的算法，仅使用不精确的梯度和Hessian信息。

Method: 提出了两种算法：1) 用于非凸复合优化的OEF近端牛顿法；2) 用于无约束优化的OEF正则化牛顿法。两种算法都使用不精确的梯度和Hessian评估。还提出了OEF正则化牛顿与负曲率算法来寻找近似二阶驻点。

Result: 理论分析表明，所提算法的全局/局部收敛速率与精确评估目标函数和导数时相同。OEF正则化牛顿与负曲率算法的最坏情况迭代/(样本)操作复杂度与文献中报道的最优结果匹配。

Conclusion: 成功开发了无需精确目标函数评估的优化算法，仅依赖不精确的导数信息，在保持收敛性能的同时降低了计算成本，为实际应用提供了更实用的解决方案。

Abstract: In this paper, we propose objective-evaluation-free (OEF) variants of the proximal Newton method for nonconvex composite optimization problems and the regularized Newton method for unconstrained optimization problems, respectively, using inexact evaluations of gradients and Hessians. Theoretical analysis demonstrates that the global/local convergence rates of the proposed algorithms are consistent with those achieved when both objective function and derivatives are evaluated exactly. Additionally, we present an OEF regularized Newton and negative curvature algorithm that uses inexact derivatives to find approximate second-order stationary points for unconstrained optimization problems. The worst-case iteration/(sample) operation complexity of the proposed algorithm matches the optimal results reported in the literature.

</details>


### [135] [Inverse problems for infinite-dimensional transport PDEs on Wasserstein space](https://arxiv.org/abs/2512.06871)
*Hongyu Liu,Jianliang Qian,Shen Zhang*

Main category: math.OC

TL;DR: 本文为Wasserstein空间上由演化偏微分方程控制的逆问题建立了一个基础框架，首次系统性地解决了从观测数据重建未知算子、成本函数或交互核的逆问题。


<details>
  <summary>Details</summary>
Motivation: 虽然Wasserstein空间上输运型偏微分方程的正问题已被广泛研究，但其逆问题——从观测的解数据重建未知算子、成本函数或交互核——在这一普遍性水平上基本未被探索。现有研究缺乏系统性的理论框架来处理无限维、非线性和非局部的输运PDE逆问题。

Method: 提出了一个系统性的方法论，包括：1）Wasserstein空间上的高阶微积分；2）渐进变分方案。该方法专门设计用于处理无限维、非线性和非局部输运PDE逆问题中的固有挑战。

Result: 通过两个典型例子展示了该理论的力量和通用性：平均场控制动态规划方程和平均场博弈主方程的逆问题。首次为从价值函数数据识别成本函数和交互核提供了统一的基础框架。

Conclusion: 这项工作为Wasserstein空间上演化PDE逆问题建立了新的基础理论框架，开辟了一个新的、富有成果的数学研究领域，对随机控制和平均场博弈的理论和应用都有重要意义。

Abstract: We develop a foundational framework for inverse problems governed by evolutionary partial differential equations (PDEs) on the Wasserstein space of probability measures. While the forward problems for such transport-type PDEs have been extensively and intensively studied, their corresponding inverse problems--which aim to reconstruct unknown operators, cost functions, or interaction kernels from observed solution data--remain largely unexplored at this level of generality.
  The cornerstone of our theory is a systematic approach featuring high-order calculus on the Wasserstein space and a progressive variational scheme. This methodology is specifically designed to address the challenges inherent in inverse problems for infinite-dimensional, nonlinear, and nonlocal transport PDEs.
  We demonstrate the power and versatility of our theory through two canonical examples: inverse problems for both the Mean Field Control (MFC) Dynamic Programming Equation and the Mean Field Game (MFG) Master Equation. Our work provides, for the first time, a unified foundation for identifying cost functions and interaction kernels from value function data. This establishes a new and fertile field of mathematical research with significant implications for both theory and applications in stochastic control and mean field games.

</details>


### [136] [Set-based Optimal, Robust, and Resilient Control with Applications to Autonomous Precision Landing](https://arxiv.org/abs/2512.07043)
*Abhinav G. Kamath,Abraham P. Vinod,Purnanand Elango,Stefano Di Cairano,Avishai Weiss*

Main category: math.OC

TL;DR: 提出基于集合的实时闭环预测控制框架，使用计算几何、动态规划和凸优化工具，通过离线预计算可控管实现全局最优控制，具备鲁棒性和弹性特性。


<details>
  <summary>Details</summary>
Motivation: 为自主系统开发实时闭环预测控制框架，解决传统方法在高维空间中计算复杂度高、难以保证全局最优性、缺乏鲁棒性和弹性等问题。

Method: 采用受限zonotopes表示集合，支持快速集合运算；离线预计算可控管（时间索引的可控集序列）；在线通过一系列单步最优控制问题获得全局最优控制轨迹；提出自由终端时间最优性算法、有界扰动集处理随机不确定性、瞬时可达集计算和最大决策延迟方法。

Result: 通过自主精确着陆案例研究，展示了全局最优自由终端时间制导、对导航和执行不确定性的鲁棒性、瞬时转向包线计算和最大决策延迟能力。

Conclusion: 提出的基于集合的框架能够实现实时闭环预测控制，在保证全局最优性的同时，具备处理随机不确定性和系统弹性的能力，适用于自主系统的高维控制问题。

Abstract: We present a real-time-capable set-based framework for closed-loop predictive control of autonomous systems using tools from computational geometry, dynamic programming, and convex optimization. The control architecture relies on the offline precomputation of the controllable tube, i.e, a time-indexed sequence of controllable sets. Sets are represented using constrained zonotopes (CZs), which are efficient encodings of convex polytopes that support fast set operations and enable tractable dynamic programming in high dimensions. Online, we obtain a globally optimal control profile by solving a series of one-step optimal control problems. Our key contributions are: (1) free-final-time optimality: we devise an optimal horizon computation algorithm to achieve global optimality; (2) robustness: we handle stochastic uncertainty in both the state and control, with probabilistic guarantees, by constructing bounded disturbance sets; (3) resilience: we develop (i) an optimization-free approach to computing the instantaneous reachable set, i.e., the reachable set from the current state, to enable, for example, large/maximal divert maneuvers, and (ii) an approach to achieving maximal decision-deferral, i.e., maintaining reachability/divert-feasibility to multiple targets for as long as possible. By means of an autonomous precision landing case study, we demonstrate globally optimal free-final-time guidance, robustness to navigation and actuation uncertainties, instantaneous divert envelope computation, and maximal decision-deferral.

</details>


### [137] [Minimizing Control Attention:The Linear Gauss-Markov paradigm](https://arxiv.org/abs/2512.07046)
*Ralph Sabbagh,Asmaa Eldesoukey,Mahmoud Abdelgalil,Tryphon T. Georgiou*

Main category: math.OC

TL;DR: 论文重新定义"注意力"作为量化基于可用数据校准控制动作所需努力的技术术语，分析线性马尔可夫动态系统下最小注意力控制方案的结构


<details>
  <summary>Details</summary>
Motivation: 重新审视"注意力"作为技术术语，量化控制动作基于可用数据的校准努力。Wiener在控制论中预见了优先处理任务相关信号的关键原则，但直到Brockett在1990年代末才提出相关的优化问题，这启发了后续和当前的研究

Method: 聚焦于线性马尔可夫动态系统和高斯状态不确定性，分析最小注意力控制方案的结构，这些方案在具有高斯不确定性分布的终端状态之间引导动态系统

Result: 定义了"注意力"作为量化控制律对时间和空间/状态坐标依赖性的技术术语。独立于时间和空间的控制律（如果满足规范）需要零注意力

Conclusion: 通过将注意力定义为控制律对时空坐标的依赖性，为量化控制系统的校准努力提供了理论框架，特别适用于线性马尔可夫系统的高斯不确定性场景

Abstract: We revisit the concept of `attention' as a technical term to quantify the effort in calibrating control action based on available data. While Wiener, in his work on Cybernetics, anticipated key principles on prioritizing task-relevant signals, it was not until the late 1990's when Brockett first formulated pertinent optimization problems that have inspired subsequent as well as the present work. `Attention,' as a technical term, is defined so as to quantify the dependence of the control law on the time and space/state coordinate; a control law that is independent of time and space, assuming it meets specifications, requires vanishing attention. In the present work we focus on Linear-Markovian dynamics with Gaussian state uncertainty so as to analyze the structure of minimal-attention control schemes that steer the dynamics between terminal states with Gaussian uncertainty profile.

</details>


### [138] [An Accelerated Primal Dual Algorithm with Backtracking for Decentralized Constrained Optimization](https://arxiv.org/abs/2512.07085)
*Qiushui Xu,Necdet Serhat Aybat,Mert Gürbüzbalaban*

Main category: math.OC

TL;DR: 提出分布式加速原始对偶回溯方法(D-APDB)，用于多智能体约束共识优化问题，无需Lipschitz常数先验知识，通过分布式回溯步长搜索自适应局部光滑性，达到最优收敛率。


<details>
  <summary>Details</summary>
Motivation: 现有分布式原始对偶方法需要Lipschitz常数知识，这在实践中难以获取。当节点具有私有约束（特别是非线性凸约束）且投影计算成本高时，需要无需先验知识的自适应方法。

Method: 提出D-APDB方法：分布式加速原始对偶方法结合回溯步长搜索。每个代理仅依赖自身目标函数和约束函数的一阶oracle，通过局部通信与邻居交换信息，无需Lipschitz常数先验知识。支持一跳简单信息交换（如LoRaWAN协议）。

Result: 在光滑性和通信图连通性标准假设下，建立了次优性、不可行性和共识违反的O(1/K)收敛保证。当节点具有私有约束（特别是非线性凸约束）时，D-APDB是首个实现约束复合凸优化问题最优收敛率的分布式回溯方法。

Conclusion: D-APDB是首个无需Lipschitz常数先验知识的分布式回溯方法，对具有私有约束的多智能体约束共识优化问题达到最优收敛率，数值实验在分布式QCQP问题上展示了性能优势。

Abstract: We propose a distributed accelerated primal-dual method with backtracking (D-APDB) for cooperative multi-agent constrained consensus optimization problems over an undirected network of agents, where only those agents connected by an edge can directly communicate to exchange large-volume data vectors using a high-speed, short-range communication protocol, e.g., WiFi, and we also assume that the network allows for one-hop simple information exchange beyond immediate neighbors as in LoRaWAN protocol. The objective is to minimize the sum of agent-specific composite convex functions over agent-specific private constraint sets. Unlike existing decentralized primal-dual methods that require knowledge of the Lipschitz constants, D-APDB automatically adapts to local smoothness by employing a distributed backtracking step-size search. Each agent relies only on first-order oracles associated with its own objective and constraint functions and on local communications with the neighboring agents, without any prior knowledge of Lipschitz constants. We establish $\mathcal{O}(1/K)$ convergence guarantees for sub-optimality, infeasibility and consensus violation, under standard assumptions on smoothness and on the connectivity of the communication graph. To our knowledge, when nodes have private constraints, especially when they are nonlinear convex constraints onto which projections are not cheap to compute, D-APDB is the first distributed method with backtracking that achieves the optimal convergence rate for the class of constrained composite convex optimization problems. We also provide numerical results for D-APDB on a distributed QCQP problem illustrating the potential performance gains that can be achieved by D-APDB.

</details>


### [139] [Iterative Switching Time Optimization for Mixed-integer Optimal Control Problems](https://arxiv.org/abs/2512.07213)
*Ramin Abbasi-Esfeden,Wim Van Roy,Jan Swevers*

Main category: math.OC

TL;DR: 提出一种迭代方法解决具有切换动力学的混合整数最优控制问题，通过切换时间优化和序列优化两个组件的迭代交互，避免传统松弛问题方法的缺陷。


<details>
  <summary>Details</summary>
Motivation: 混合整数最优控制问题在切换动力系统中常见，传统方法依赖松弛问题可能导致解偏离实际最优解，需要更有效的求解方法。

Method: 提出迭代切换时间优化方法，包含两个迭代交互组件：切换时间优化（STO）和序列优化，通过数值示例验证方法有效性。

Result: 数值示例显示所提算法效率高，能够解决传统松弛方法可能导致解偏离的问题，展示了方法的实际应用效果。

Conclusion: 该方法为混合整数最优控制问题提供了有效解决方案，讨论了方法的优缺点，并展望了未来研究方向。

Abstract: This paper proposes an iterative method to solve Mixed-Integer Optimal Control Problems arising from systems with switched dynamics. The so-called relaxed problem plays a central role within this context. Through a numerical example, it is shown why relying on the relaxed problem can lead the solution astray. As an alternative, an iterative Switching Time Optimization method is proposed. The method consists of two components that iteratively interact: a Switching Time Optimization (STO) problem and a sequence optimization. Each component is explained in detail, and the numerical example is resolved, the results of which shows the efficiency of the proposed algorithm. Finally, the advantages and disadvantages of the method are discussed and future lines of research are sketched.

</details>


### [140] [Nash Equilibrium of Bi-objective Optimal Control of Fractional Space-Time Parabolic PDE](https://arxiv.org/abs/2512.07327)
*Kedarnath Buda,B. V. Rathish Kumar,Anil Rathi*

Main category: math.OC

TL;DR: 研究分数时空抛物型偏微分方程控制的双目标最优控制问题中纳什均衡的存在性与唯一性，建立了理论结果并提出了数值求解算法。


<details>
  <summary>Details</summary>
Motivation: 研究由分数阶偏微分方程控制的竞争性最优控制问题中纳什均衡的存在性与唯一性，这类问题在多个控制主体追求不同目标时具有重要理论意义和应用价值。

Method: 采用Caputo时间分数阶导数和空间分数阶拉普拉斯算子建立数学模型，在凸性和强制性假设下证明纳什均衡的存在唯一性，使用共轭梯度算法迭代求解离散化最优控制问题。

Result: 在凸性和强制性条件下建立了纳什均衡的存在性与唯一性理论结果，数值实验验证了理论估计并证明了所提方案的高效性。

Conclusion: 成功解决了分数时空抛物型偏微分方程控制的双目标最优控制问题中纳什均衡的存在唯一性问题，提出的数值方法有效且与理论结果一致。

Abstract: This work investigates the existence and uniqueness of the Nash equilibrium (solutions to competitive problems in which individual controls aim at separate desired states) for a bi-objective optimal control problem governed by a fractional space-time parabolic partial differential equation. The governing equation involves a Caputo fractional derivative with respect to time of order $γ$ in (0,1) and a fractional Laplacian in the spatial variables of order $s$ in (0,1). The system is associated with two independent controls, each aiming at different targets. The problem is formulated as a distributed optimal control system with quadratic cost functionals. Existence and uniqueness of the Nash equilibrium are established under convexity and coercivity assumptions. The solution is computed using conjugate gradient algorithms applied iteratively to the discretized optimal control problems. The numerical experiments agree with the theoretical estimates and demonstrate the efficiency of the proposed scheme.

</details>


### [141] [Control and Reinforcement Learning through the Lens of Optimization: An Algorithmic Perspective](https://arxiv.org/abs/2512.07377)
*Tolga Ok,Arman Sharifi Kolarijani,Mohamad Amin Sharif Kolarijani,Peyman Mohajerin Esfahani*

Main category: math.OC

TL;DR: 该论文建立了一个统一框架，将马尔可夫决策过程的控制算法与优化算法进行系统性类比，实现两类算法间的相互转换。


<details>
  <summary>Details</summary>
Motivation: 自贝尔曼提出动态规划以来，控制算法与优化算法之间的联系一直被隐含或显式地利用。近年来，从成熟优化算法中汲取灵感开发新控制算法的研究备受关注，但现有研究较为分散，缺乏统一框架。

Method: 提出一个新颖的统一框架，在四个问题类别中建立控制算法与优化算法的明确类比关系。通过统一解决方案表征，实现算法从优化领域到控制领域的系统性转换。

Result: 识别了现有文献中已指出的等效优化和控制算法，解决了为新控制算法提供理论收敛保证的问题，并提供了简单有效的技术方案。

Conclusion: 该框架和技术为开发新的收敛控制算法提供了具体方法论，实现了控制与优化领域算法的系统性互转。

Abstract: The connection between control algorithms for Markov decision processes and optimization algorithms has been implicitly and explicitly exploited since the introduction of dynamic programming algorithm by Bellman in the 1950s. Recently, this connection has attracted a lot of attention for developing new control algorithms inspired by well-established optimization algorithms. In this paper, we make this analogy explicit across four problem classes with a unified solution characterization. This novel framework, in turn, allows for a systematic transformation of algorithms from one domain to the other. In particular, we identify equivalent optimization and control algorithms that have already been pointed out in the existing literature, but mostly in a scattered way. We also discuss the issues arising in providing theoretical convergence guarantees for these new control algorithms and provide simple yet effective techniques to solve them. The provided framework and techniques then lay out a concrete methodology for developing new convergent control algorithms.

</details>


### [142] [Open qubit parameter identification with bounded pulses](https://arxiv.org/abs/2512.07409)
*Ghaieth Aloui,Ivan Beschastnyi,Ludovic Sacchelli*

Main category: math.OC

TL;DR: 提出一种用于开放量子比特弛豫和退相干参数辨识的最小化实验方案，通过饱和控制脉冲生成特定配置，在理想脉冲下可解析重构参数，并分析有限脉冲下的估计误差界限。


<details>
  <summary>Details</summary>
Motivation: 解决开放量子比特弛豫和退相干参数辨识问题，旨在减少实验开销并提供可解释的参数识别方法。

Method: 选择最小化量子比特配置集合，通过饱和控制脉冲生成这些配置。在理想无限振幅脉冲下解析重构参数，将有限脉冲视为理想情况的扰动，提供估计误差界限。

Result: 在理想脉冲条件下可实现参数解析重构，有限脉冲下可量化估计误差，能够区分统计波动和建模误差来源。

Conclusion: 该方法为开放量子比特参数识别提供了系统框架，能够最小化实验开销并明确区分不同误差来源，具有实际应用价值。

Abstract: We address the problem of parameter identification for a single open qubit subjected to relaxation and dephasing. Our approach is based on selecting a minimal set of carefully chosen qubit configurations that can be reliably prepared and measured in order to provide an interpretable methodology of parameter identification while potentially minimizing experimental overhead. The protocol relies on saturating control pulses to generate these configurations. In an idealized regime of infinite-amplitude pulses, we demonstrate that the parameters can be reconstructed analytically from the measured observables. We then consider large but finite pulses as a perturbation of this ideal regime and provide bounds on the estimation error introduced by the practical implementation. This framework allows us to separate the sources of uncertainty in the estimation procedure, distinguishing between statistical fluctuations arising from repeated measurements and modeling errors due to deviations from the ideal pulse regime.

</details>


### [143] [Recovery of the optimal control value function in reproducing kernel Hilbert spaces from verification conditions](https://arxiv.org/abs/2512.07477)
*Tobias Ehring,Behzad Azmi,Bernard Haasdonk*

Main category: math.OC

TL;DR: 该论文提出了一种在再生核希尔伯特空间中重构未知目标函数的抽象最优恢复框架，用于逼近无限时域非线性自治最优控制问题的最优值函数v*，通过验证最优性条件（特别是HJB方程）将逼近问题转化为配点型问题。


<details>
  <summary>Details</summary>
Motivation: 逼近无限时域、非线性、自治最优控制问题的最优值函数v*对于合成实时最优反馈控制至关重要，但这一任务具有挑战性。现有方法需要更有效的框架来处理混合等式和不等式函数约束下的函数重构问题。

Method: 在再生核希尔伯特空间(RKHS)中开发抽象最优恢复框架，从混合等式和不等式函数约束重构未知目标函数。将v*的逼近问题转化为基于最优性验证条件（特别是Hamilton-Jacobi-Bellman方程）的配点型问题，该条件唯一地表征v*。

Result: 当配点集在域Ω中变得稠密时，证明了RKHS逼近函数收敛于v*：当v*解析时在Ω上全局收敛于RKHS范数；当v*被二次函数上下界限制时在原点邻域局部收敛。实际数值实现简化为经典策略迭代算法，数值实验验证了方法的有效性。

Conclusion: 提出的RKHS最优恢复框架为逼近最优控制问题的最优值函数提供了理论基础和实用算法，建立了收敛性保证，并通过数值实验验证了方法的有效性，为实时最优反馈控制合成提供了新途径。

Abstract: Approximating the optimal value function $v^*$ for infinite-horizon, nonlinear, autonomous optimal control problems is both challenging and essential for synthesizing real-time optimal feedback. We develop an abstract optimal recovery framework in reproducing kernel Hilbert spaces (RKHS) for reconstructing unknown target functions from mixed equality and inequality functional constraints. Within this framework, the approximation of $v^*$ is cast as a collocation-type problem derived from verification conditions for optimality -- most prominently, the Hamilton-Jacobi-Bellman (HJB) equation -- that uniquely characterizes $v^*$. As the set of collocation points becomes dense in the ambient domain $Ω$, we establish convergence of the RKHS approximants to $v^*$: globally on $Ω$ in the RKHS norm when $v^*$ is analytic, and locally (in a neighborhood of the origin) in the RKHS norm when $v^*$ is bounded from above and below by quadratic functions. Furthermore, we show that a practical numerical realization of the abstract scheme reduces to the classical policy iteration algorithm. Numerical experiments support the effectiveness of the proposed approach.

</details>


### [144] [Energy-Aware Aggregation of Input Data for the Optimisation of Heat Supply of Municipal Districts](https://arxiv.org/abs/2512.07646)
*Patrik Schönfeldt,Elif Turhan*

Main category: math.OC

TL;DR: 该研究提出了一种在市政供热规划中考虑建筑能源性能指标的分组方法，以平衡地理位置和能源特性，优化能源系统模型的复杂性。


<details>
  <summary>Details</summary>
Motivation: 市政供热规划涉及大量建筑，导致模型变量激增和计算复杂度指数增长。同时，热转型需要高分辨率映射长期系统寿命，特别是日间负荷峰值。现有方法通常基于地理分组，但未充分考虑能源性能差异。

Method: 提出结合地理位置、建筑几何和能源性能指标（年热耗和太阳能发电潜力）的建筑分组方法。开发从地理数据到技术-社会-经济帕累托最优供热方案的全流程工作方法，考虑能源量的时间分布。

Result: 基于Neu-Schwachhausen区真实数据的案例研究表明，在建筑分组时平衡地理位置和能源特性对能源系统模型有益。能源性能指标能改善分组效果，优化供热方案选择。

Conclusion: 在市政供热规划中，结合地理和能源性能的建筑分组方法能有效应对模型复杂性挑战，为热转型提供更优的决策支持框架。

Abstract: In the context of municipal heat planning, it is imperative to consider the numerous buildings, numbering in the hundreds or thousands, that are involved. This poses particular challenges for model-based energy system optimization, as the number of variables increases with the number of buildings under consideration. In the worst case, the computational complexity of the models experiences an exponential increase with the number of variables. Furthermore, within the context of heat transition, it is often necessary to map extended periods of time (i.e., the service life of systems) with high resolution (particularly in the case of load peaks that occur at the onset of the day). In response to these challenges, the aggregation of input data is a common practice. In general, building blocks or other geographical and urban formations, such as neighbourhoods, are combined. This article explores the potential of incorporating energy performance indicators into the grouping of buildings. The case study utilizes authentic data from the Neu-Schwachhausen district, grouped based on geographical location, building geometry, and energy performance indicators. The selection of energy indicators includes the annual heat consumption as well as the potential for solar energy generation. To this end, a methodology is hereby presented that considers not only the anticipated annual energy quantity, but also its progression over time. We present a full workflow from geodata to a set of techno-socio-economically Pareto-optimal heat supply options. Our findings suggest that it is beneficial to find a balance between geographical position and energy properties when grouping buildings for the use in energy system models.

</details>


### [145] [Optimal Control of a Higher-Order Cahn-Hilliard Equation Coupled with Brinkman Equation](https://arxiv.org/abs/2512.07682)
*Manika Bag*

Main category: math.OC

TL;DR: 研究Brinkman方程与六阶Cahn-Hilliard方程耦合系统的最优控制问题，目标函数包含L1范数项以产生稀疏控制


<details>
  <summary>Details</summary>
Motivation: 研究具有质量交换源项的Cahn-Hilliard方程与Brinkman方程耦合系统的控制问题，通过包含L1范数的目标函数实现稀疏控制，这在相分离和流体动力学应用中具有重要意义

Method: 使用Lagrange乘子法处理可微情况（κ=0），对于不可微情况（κ>0）采用Casas和Tröltzsch的方法推导一阶必要条件

Result: 建立了两种情况下的一阶必要最优性条件：可微情况通过Lagrange乘子法表征，不可微情况通过Casas和Tröltzsch方法处理L1范数项

Conclusion: 成功推导了Brinkman-Cahn-Hilliard耦合系统在稀疏控制下的最优性条件，为相分离和流体动力学中的控制问题提供了理论框架

Abstract: In this work, we investigate optimal control of a Brinkman equation couple with sixth-order Cahn-Hilliard equation. The Cahn-Hilliard equation is endowed with a source term accounting for mass exchange and the velocity equation contains a non divergence-free forcing term, which act as distributed control variable. We consider the aforementioned system with constant mobility, viscosity and nonlinearity of double-well shape is regular. The cost functional of the optimal control problem contains a nondifferentiable term like the $L^1$-norm with sparsity constant $κ$, which leads to sparsity of optimal controls. We study the first order necessary optimality condition for both the case $κ=0$ and $κ>0.$ When the cost functional is differentiable, first order necessary optimality conditions are characterized by Lagrange multiplier method and for nondifferentiable case we have used the idea of Casas and Tröltzsch from the paper (Math. control Relat. Fields, 10(3):527-546, 2020).

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [146] [Going All-In on LLM Accuracy: Fake Prediction Markets, Real Confidence Signals](https://arxiv.org/abs/2512.05998)
*Michael Todasco*

Main category: cs.AI

TL;DR: 通过将LLM评估任务设计为投注游戏（使用虚拟货币的预测市场），研究探索了这种框架是否能提高预测准确性并产生校准的置信度信号。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在评估其他模型时通常缺乏置信度表示，研究者希望探索通过投注游戏框架能否让LLM展现出更好的预测能力和置信度校准。

Method: 研究设计了100个数学和逻辑问题，让6个基线模型回答，然后让3个预测模型在两种条件下预测基线模型的回答正确性：控制条件（简单正确/错误预测）和激励条件（预测加上1-100,000虚拟货币的投注）。

Result: 激励条件下的预测准确率略高（81.5% vs. 79.1%），学习速度显著更快。最重要的是，投注金额与置信度相关：大额投注（40,000+虚拟币）正确率约99%，小额投注（<1,000虚拟币）正确率仅约74%。

Conclusion: 投注机制为LLM创建了可读的置信度信号，使内部信念变得可见可用，为LLM间预测市场和元评估系统提供了基础框架。

Abstract: Large language models are increasingly used to evaluate other models, yet these judgments typically lack any representation of confidence. This pilot study tests whether framing an evaluation task as a betting game (a fictional prediction market with its own LLM currency) improves forecasting accuracy and surfaces calibrated confidence signals. We generated 100 math and logic questions with verifiable answers. Six Baseline models (three current-generation, three prior-generation) answered all items. Three Predictor models then forecasted, for each question-baseline pair, if the baseline would answer correctly. Each predictor completed matched runs in two conditions: Control (simple correct/incorrect predictions) and Incentive (predictions plus wagers of 1-100,000 LLMCoin under even odds, starting from a 1,000,000 LLMCoin bankroll). Across 5,400 predictions per condition, Incentive runs showed modestly higher accuracy (81.5% vs. 79.1%, p = .089, d = 0.86) and significantly faster learning across rounds (12.0 vs. 2.9 percentage-point improvement from Round 1 to Round 4, p = .011). Most notably, stake size tracked confidence. "Whale" bets of 40,000+ coins were correct ~99% of the time, while small bets (<1,000 coins) showed only ~74% accuracy. The key finding is not that fictional money makes models smarter; accuracy gains were modest and did not reach statistical significance (p = .089) in this pilot. Rather, the betting mechanic created a legible confidence signal absent from binary yes/no outputs. This suggests that simple financial framing may help transform LLMs into risk-aware forecasters, making their internal beliefs visible and usable. The protocol offers a foundation for future work for meta-evaluation systems and what may become LLM-to-LLM prediction markets.

</details>


### [147] [Deep learning for autism detection using clinical notes: A comparison of transfer learning for a transparent and black-box approach](https://arxiv.org/abs/2512.06161)
*Gondy Leroy,Prakash Bisht,Sai Madhuri Kandula,Nell Maltman,Sydney Rice*

Main category: cs.AI

TL;DR: 使用BioBERT分析临床文本的透明机器学习方法，通过混合数据集训练实现97%敏感性和98%特异性的ASD诊断，优于黑盒模型。


<details>
  <summary>Details</summary>
Motivation: ASD诊断过程漫长且需求增加，现有机器学习模型多为黑盒且通常基于单一数据集训练，限制了其可泛化性和临床可信度。

Method: 采用BioBERT语言模型分析非结构化临床文本，训练模型标记行为描述并映射到诊断标准，然后分配最终标签（ASD或非ASD）。评估了顺序训练和混合训练两种策略，并与黑盒方法比较。

Result: 透明模型表现稳健，混合数据训练策略效果最佳（敏感性97%，特异性98%）。顺序训练导致性能略有下降。黑盒模型表现较差（敏感性90%，特异性96%）。透明方法整体优于黑盒方法。

Conclusion: 混合数据集训练可获得更好性能，应作为首选方法。该透明方法为神经发育诊断中更可信、可泛化和临床可操作的AI工具铺平了道路。

Abstract: Autism spectrum disorder (ASD) is a complex neurodevelopmental condition whose rising prevalence places increasing demands on a lengthy diagnostic process. Machine learning (ML) has shown promise in automating ASD diagnosis, but most existing models operate as black boxes and are typically trained on a single dataset, limiting their generalizability. In this study, we introduce a transparent and interpretable ML approach that leverages BioBERT, a state-of-the-art language model, to analyze unstructured clinical text. The model is trained to label descriptions of behaviors and map them to diagnostic criteria, which are then used to assign a final label (ASD or not). We evaluate transfer learning, the ability to transfer knowledge to new data, using two distinct real-world datasets. We trained on datasets sequentially and mixed together and compared the performance of the best models and their ability to transfer to new data. We also created a black-box approach and repeated this transfer process for comparison. Our transparent model demonstrated robust performance, with the mixed-data training strategy yielding the best results (97 % sensitivity, 98 % specificity). Sequential training across datasets led to a slight drop in performance, highlighting the importance of training data order. The black-box model performed worse (90 % sensitivity, 96 % specificity) when trained sequentially or with mixed data. Overall, our transparent approach outperformed the black-box approach. Mixing datasets during training resulted in slightly better performance and should be the preferred approach when practically possible. This work paves the way for more trustworthy, generalizable, and clinically actionable AI tools in neurodevelopmental diagnostics.

</details>


### [148] [ARCANE: A Multi-Agent Framework for Interpretable and Configurable Alignment](https://arxiv.org/abs/2512.06196)
*Charlie Masters,Marta Grześkiewicz,Stefano V. Albrecht*

Main category: cs.AI

TL;DR: ARCANE框架将AI对齐问题转化为多智能体协作，通过动态生成自然语言评分标准来代表利益相关者偏好，实现可解释、无需重新训练即可调整的奖励模型。


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型的智能体越来越多地部署到长期任务中，保持其与利益相关者偏好的一致性变得至关重要。需要可解释的奖励模型以便利益相关者理解和审核模型目标，并且能够在交互时引导智能体，无需重新训练即可纳入偏好变化。

Method: 引入ARCANE框架，将对齐问题构建为多智能体协作问题，动态生成自然语言评分标准（加权可验证标准）。采用正则化组序列策略优化（GSPO）进行评分标准学习，平衡可解释性、忠实度和计算效率。

Result: 使用GDPVal基准的219个标注评分标准进行评估，在需要多步推理和工具使用的挑战性任务上表现良好。学习的评分标准产生紧凑、易读的评估，无需重新训练即可实现可配置的权衡（如正确性与简洁性）。

Conclusion: 基于评分标准的奖励模型为复杂、长期AI系统提供了一条有前景的路径，实现了可解释、测试时自适应对齐。

Abstract: As agents based on large language models are increasingly deployed to long-horizon tasks, maintaining their alignment with stakeholder preferences becomes critical. Effective alignment in such settings requires reward models that are interpretable so that stakeholders can understand and audit model objectives. Moreover, reward models must be capable of steering agents at interaction time, allowing preference shifts to be incorporated without retraining. We introduce ARCANE, a framework that frames alignment as a multi-agent collaboration problem that dynamically represents stakeholder preferences as natural-language rubrics: weighted sets of verifiable criteria that can be generated on-the-fly from task context. Inspired by utility theory, we formulate rubric learning as a reconstruction problem and apply a regularized Group-Sequence Policy Optimization (GSPO) procedure that balances interpretability, faithfulness, and computational efficiency. Using a corpus of 219 labeled rubrics derived from the GDPVal benchmark, we evaluate ARCANE on challenging tasks requiring multi-step reasoning and tool use. The learned rubrics produce compact, legible evaluations and enable configurable trade-offs (e.g., correctness vs. conciseness) without retraining. Our results show that rubric-based reward models offer a promising path toward interpretable, test-time adaptive alignment for complex, long-horizon AI systems.

</details>


### [149] [On measuring grounding and generalizing grounding problems](https://arxiv.org/abs/2512.06205)
*Daniel Quigley,Eric Maynard*

Main category: cs.AI

TL;DR: 该论文将符号接地问题从二元判断重构为跨多个期望标准的审计框架，包括真实性、保持性、忠实性、鲁棒性和组合性，并将其应用于四种接地模式和三个案例研究。


<details>
  <summary>Details</summary>
Motivation: 传统符号接地问题关注符号如何获得意义，但缺乏系统评估框架。本文旨在将哲学上的表征问题操作化，为哲学家、计算机科学家、语言学家和数学家提供一个共同语言和技术框架，以系统研究接地和意义。

Method: 提出一个基于评估元组（上下文、意义类型、威胁模型、参考分布）的审计框架，包含五个核心期望标准：真实性、保持性、忠实性（相关性和病因性）、鲁棒性和组合性。将该框架应用于四种接地模式（符号、指称、向量、关系）和三个案例研究。

Result: 模型论语义学实现精确组合但缺乏病因性保证；大语言模型在语言任务上显示相关性拟合和局部鲁棒性，但在无接地交互的世界任务上缺乏成功选择；人类语言通过进化和发展习得满足强真实性期望。不同接地模式在不同期望标准上表现各异。

Conclusion: 通过将符号接地问题操作化为多维度审计框架，为跨学科研究提供了系统评估接地和意义的工具。该框架揭示了不同表征系统在满足接地期望标准方面的优势和局限，促进了哲学、计算机科学、语言学和数学之间的对话。

Abstract: The symbol grounding problem asks how tokens like cat can be about cats, as opposed to mere shapes manipulated in a calculus. We recast grounding from a binary judgment into an audit across desiderata, each indexed by an evaluation tuple (context, meaning type, threat model, reference distribution): authenticity (mechanisms reside inside the agent and, for strong claims, were acquired through learning or evolution); preservation (atomic meanings remain intact); faithfulness, both correlational (realized meanings match intended ones) and etiological (internal mechanisms causally contribute to success); robustness (graceful degradation under declared perturbations); compositionality (the whole is built systematically from the parts). We apply this framework to four grounding modes (symbolic; referential; vectorial; relational) and three case studies: model-theoretic semantics achieves exact composition but lacks etiological warrant; large language models show correlational fit and local robustness for linguistic tasks, yet lack selection-for-success on world tasks without grounded interaction; human language meets the desiderata under strong authenticity through evolutionary and developmental acquisition. By operationalizing a philosophical inquiry about representation, we equip philosophers of science, computer scientists, linguists, and mathematicians with a common language and technical framework for systematic investigation of grounding and meaning.

</details>


### [150] [AI Application in Anti-Money Laundering for Sustainable and Transparent Financial Systems](https://arxiv.org/abs/2512.06240)
*Chuanhao Nie,Yunbo Liu,Chao Wang*

Main category: cs.AI

TL;DR: 本文探讨AI如何革新反洗钱工作流，提出基于RAG-Graph的KYC应用，实验显示该架构在忠实度和相关性方面表现优异，提升KYC流程效率和透明度。


<details>
  <summary>Details</summary>
Motivation: 洗钱和金融欺诈每年造成数万亿美元损失，威胁全球金融稳定，传统监管方式面临挑战。需要利用AI技术改进反洗钱检测准确性、降低误报率、减轻人工调查负担，支持可持续发展。

Method: 提出AI驱动的KYC应用，整合基于图的检索增强生成（RAG Graph）与生成模型。该方法结合联邦学习、公平可解释AI、强化学习和人机协同可视化系统，构建下一代反洗钱架构。

Result: 实验结果显示，RAG-Graph架构在不同评估场景下展现出高忠实度和强答案相关性，显著提升了KYC客户尽职调查/增强尽职调查工作流的效率和透明度，有助于实现资源优化的合规实践。

Conclusion: AI技术能够有效现代化反洗钱工作流，RAG-Graph架构为KYC流程提供了高效透明的解决方案。未来研究方向包括隐私保护协作、公平可解释AI、自适应防御和人机协同系统，确保下一代反洗钱架构的透明性、问责性和鲁棒性。

Abstract: Money laundering and financial fraud remain major threats to global financial stability, costing trillions annually and challenging regulatory oversight. This paper reviews how artificial intelligence (AI) applications can modernize Anti-Money Laundering (AML) workflows by improving detection accuracy, lowering false-positive rates, and reducing the operational burden of manual investigations, thereby supporting more sustainable development. It further highlights future research directions including federated learning for privacy-preserving collaboration, fairness-aware and interpretable AI, reinforcement learning for adaptive defenses, and human-in-the-loop visualization systems to ensure that next-generation AML architectures remain transparent, accountable, and robust. In the final part, the paper proposes an AI-driven KYC application that integrates graph-based retrieval-augmented generation (RAG Graph) with generative models to enhance efficiency, transparency, and decision support in KYC processes related to money-laundering detection. Experimental results show that the RAG-Graph architecture delivers high faithfulness and strong answer relevancy across diverse evaluation settings, thereby enhancing the efficiency and transparency of KYC CDD/EDD workflows and contributing to more sustainable, resource-optimized compliance practices.

</details>


### [151] [How Sharp and Bias-Robust is a Model? Dual Evaluation Perspectives on Knowledge Graph Completion](https://arxiv.org/abs/2512.06296)
*Sooho Moon,Yunyong Ko*

Main category: cs.AI

TL;DR: 本文提出PROBE评估框架，解决知识图谱补全（KGC）现有评估指标忽视预测锐度（predictive sharpness）和流行度偏差鲁棒性（popularity-bias robustness）的问题。


<details>
  <summary>Details</summary>
Motivation: 现有KGC评估指标存在两个关键缺陷：1）忽视预测锐度——评估单个预测的严格程度；2）忽视流行度偏差鲁棒性——预测低流行度实体的能力。这导致现有指标可能高估或低估KGC模型的准确性。

Method: 提出PROBE评估框架，包含两个组件：1）排名转换器（RT）——根据所需预测锐度水平估计每个预测的得分；2）排名聚合器（RA）——以流行度感知的方式聚合所有得分。

Result: 在真实世界知识图谱上的实验表明，现有指标倾向于高估或低估KGC模型的准确性，而PROBE能够提供对KGC模型的全面理解和可靠的评估结果。

Conclusion: PROBE框架通过同时考虑预测锐度和流行度偏差鲁棒性，为KGC评估提供了更全面可靠的解决方案，有助于更准确地理解KGC模型的性能。

Abstract: Knowledge graph completion (KGC) aims to predict missing facts from the observed KG. While a number of KGC models have been studied, the evaluation of KGC still remain underexplored. In this paper, we observe that existing metrics overlook two key perspectives for KGC evaluation: (A1) predictive sharpness -- the degree of strictness in evaluating an individual prediction, and (A2) popularity-bias robustness -- the ability to predict low-popularity entities. Toward reflecting both perspectives, we propose a novel evaluation framework (PROBE), which consists of a rank transformer (RT) estimating the score of each prediction based on a required level of predictive sharpness and a rank aggregator (RA) aggregating all the scores in a popularity-aware manner. Experiments on real-world KGs reveal that existing metrics tend to over- or under-estimate the accuracy of KGC models, whereas PROBE yields a comprehensive understanding of KGC models and reliable evaluation results.

</details>


### [152] [DaGRPO: Rectifying Gradient Conflict in Reasoning via Distinctiveness-Aware Group Relative Policy Optimization](https://arxiv.org/abs/2512.06337)
*Xuan Xie,Xuan Wang,Wenjie Wang*

Main category: cs.AI

TL;DR: DaGRPO通过序列级梯度校正和离策略数据增强解决GRPO训练不稳定和样本效率低的问题，在数学推理和OOD泛化基准上取得SOTA性能


<details>
  <summary>Details</summary>
Motivation: GRPO虽然能有效激发LLM的长程推理能力，但存在训练不稳定和样本效率低的问题。研究发现根本原因是on-policy rollout中样本缺乏区分度：常规查询中高度同质化样本导致破坏性梯度冲突，而困难查询中有效正样本稀缺导致优化无效。

Method: 提出Distinctiveness-aware Group Relative Policy Optimization (DaGRPO)，包含两个核心机制：1) 序列级梯度校正：使用细粒度评分动态屏蔽低区分度的样本对，从源头消除梯度冲突；2) 离策略数据增强：引入高质量锚点样本，为困难任务恢复训练信号。

Result: 在9个数学推理和OOD泛化基准上的广泛实验表明，DaGRPO显著超越现有SFT、GRPO和混合基线，达到新的SOTA性能（例如在数学基准上平均准确率提升+4.7%）。深入分析证实DaGRPO有效缓解梯度爆炸并加速长链推理能力的涌现。

Conclusion: DaGRPO通过解决GRPO中样本区分度不足的根本问题，显著提升了训练稳定性和样本效率，为激发LLM的长程推理能力提供了更有效的后训练机制。

Abstract: The evolution of Large Language Models (LLMs) has catalyzed a paradigm shift from superficial instruction following to rigorous long-horizon reasoning. While Group Relative Policy Optimization (GRPO) has emerged as a pivotal mechanism for eliciting such post-training reasoning capabilities due to its exceptional performance, it remains plagued by significant training instability and poor sample efficiency. We theoretically identify the root cause of these issues as the lack of distinctiveness within on-policy rollouts: for routine queries, highly homogeneous samples induce destructive gradient conflicts; whereas for hard queries, the scarcity of valid positive samples results in ineffective optimization. To bridge this gap, we propose Distinctiveness-aware Group Relative Policy Optimization (DaGRPO). DaGRPO incorporates two core mechanisms: (1) Sequence-level Gradient Rectification, which utilizes fine-grained scoring to dynamically mask sample pairs with low distinctiveness, thereby eradicating gradient conflicts at the source; and (2) Off-policy Data Augmentation, which introduces high-quality anchors to recover training signals for challenging tasks. Extensive experiments across 9 mathematical reasoning and out-of-distribution (OOD) generalization benchmarks demonstrate that DaGRPO significantly surpasses existing SFT, GRPO, and hybrid baselines, achieving new state-of-the-art performance (e.g., a +4.7% average accuracy gain on math benchmarks). Furthermore, in-depth analysis confirms that DaGRPO effectively mitigates gradient explosion and accelerates the emergence of long-chain reasoning capabilities.

</details>


### [153] [Less Is More for Multi-Step Logical Reasoning of LLM Generalisation Under Rule Removal, Paraphrasing, and Compression](https://arxiv.org/abs/2512.06393)
*Qiming Bao,Xiaoxuan Fu*

Main category: cs.AI

TL;DR: LLMs在逻辑推理中表现出对语义保持变换的稳定性，但对缺失规则和矛盾证据极度脆弱


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs在许多自然语言任务上表现出色，但其在逻辑推理中对结构性扰动的泛化能力尚不清楚，需要系统评估其推理可靠性

Method: 提出受控评估框架，包含四种压力测试：1)规则删除（冗余vs必要）；2)矛盾证据注入；3)逻辑保持重写（使用六种等价律）；4)多定律等价堆叠（2-5个同时变换）

Result: 所有模型在基础任务上完美准确，对冗余规则删除和等价重写完全泛化，但在必要规则删除时准确率降至25%，面对矛盾证据时完全崩溃（0%准确率）

Conclusion: LLMs对语义保持的逻辑变换具有稳定不变性，但对缺失或冲突证据极度脆弱，框架为诊断推理失败模式提供了工具，揭示了当前LLMs在逻辑泛化能力上的持续差距

Abstract: Large language models (LLMs) excel across many natural language tasks, yet their generalisation to structural perturbations in logical contexts remains poorly understood. We introduce a controlled evaluation framework that probes reasoning reliability through four targeted stress tests: (1) rule deletion, removing either redundant or essential rules from a multi-step inference chain; (2) contradictory evidence injection; (3) logic-preserving rewrites generated through several families of equivalence laws (contrapositive, double negation, implication, De Morgan, identity, and commutativity); and (4) multi-law equivalence stacking that introduces 2-5 simultaneous logical transformations.
  Across three representative model families: BERT, Qwen2, and LLaMA-like models. Our experiments reveal a strikingly consistent pattern: all models achieve perfect accuracy on the base tasks and remain fully generalise to redundant rule deletion and all equivalence-based rewrites (single or multi-law), but fail sharply under essential rule deletion (dropping to 25% accuracy) and collapse completely in the presence of explicit contradictions (0% accuracy). These results demonstrate that LLMs possess stable invariance to semantic-preserving logical transformations, yet remain fundamentally brittle to missing or conflicting evidence. Our framework provides a clean diagnostic tool for isolating such reasoning failure modes and highlights persistent gaps in the logical generalisation abilities of current LLMs.

</details>


### [154] [GENIUS: An Agentic AI Framework for Autonomous Design and Execution of Simulation Protocols](https://arxiv.org/abs/2512.06404)
*Mohammad Soleymanibrojeni,Roland Aydin,Diego Guedes-Sobrinho,Alexandre C. Dias,Maurício J. Piotrowski,Wolfgang Wenzel,Celso Ricardo Caldeira Rêgo*

Main category: cs.AI

TL;DR: GENIUS是一个AI代理工作流，通过结合量子ESPRESSO知识图谱、分层大语言模型和有限状态错误恢复机，将自然语言提示自动转换为可运行的DFT模拟输入文件，成功率约80%，显著降低了材料计算的门槛。


<details>
  <summary>Details</summary>
Motivation: 尽管预测性原子模拟推动了材料发现，但常规设置和调试仍需要计算机专家，这种知识差距限制了集成计算材料工程（ICME）的发展。现有先进代码对非专家用户来说仍然繁琐，需要解决这一瓶颈。

Method: GENIUS融合了智能量子ESPRESSO知识图谱、由有限状态错误恢复机监督的分层大语言模型层次结构。该工作流将自由形式的人类生成提示转换为经过验证的输入文件。

Result: 在295个多样化基准测试中，约80%的案例成功运行到完成，其中76%是自主修复的。与纯LLM基线相比，GENIUS将推理成本减半，并几乎消除了幻觉。成功率呈指数衰减至7%的基线水平。

Conclusion: GENIUS框架通过智能自动化协议生成、验证和修复，民主化了电子结构DFT模拟，为大规模筛选和加速全球学术界和工业界的ICME设计循环打开了大门。

Abstract: Predictive atomistic simulations have propelled materials discovery, yet routine setup and debugging still demand computer specialists. This know-how gap limits Integrated Computational Materials Engineering (ICME), where state-of-the-art codes exist but remain cumbersome for non-experts. We address this bottleneck with GENIUS, an AI-agentic workflow that fuses a smart Quantum ESPRESSO knowledge graph with a tiered hierarchy of large language models supervised by a finite-state error-recovery machine. Here we show that GENIUS translates free-form human-generated prompts into validated input files that run to completion on $\approx$80% of 295 diverse benchmarks, where 76% are autonomously repaired, with success decaying exponentially to a 7% baseline. Compared with LLM-only baselines, GENIUS halves inference costs and virtually eliminates hallucinations. The framework democratizes electronic-structure DFT simulations by intelligently automating protocol generation, validation, and repair, opening large-scale screening and accelerating ICME design loops across academia and industry worldwide.

</details>


### [155] [UncertaintyZoo: A Unified Toolkit for Quantifying Predictive Uncertainty in Deep Learning Systems](https://arxiv.org/abs/2512.06406)
*Xianzong Wu,Xiaohong Li,Lili Quan,Qiang Hu*

Main category: cs.AI

TL;DR: UncertaintyZoo是一个统一的不确定性量化工具包，集成了29种方法，用于评估LLMs在代码漏洞检测等任务中的预测置信度。


<details>
  <summary>Details</summary>
Motivation: LLMs在安全关键场景中可能做出错误预测，现有不确定性量化方法缺乏统一工具，阻碍了实际应用和研究进展。

Method: 开发UncertaintyZoo工具包，标准化接口集成29种不确定性量化方法，涵盖五大类别，并在CodeBERT和ChatGLM3模型上进行代码漏洞检测评估。

Result: UncertaintyZoo能有效揭示模型预测的不确定性，工具已在GitHub开源，包含演示视频。

Conclusion: UncertaintyZoo填补了不确定性量化工具集的空白，为实际应用和未来研究提供了统一平台。

Abstract: Large language models(LLMs) are increasingly expanding their real-world applications across domains, e.g., question answering, autonomous driving, and automatic software development. Despite this achievement, LLMs, as data-driven systems, often make incorrect predictions, which can lead to potential losses in safety-critical scenarios. To address this issue and measure the confidence of model outputs, multiple uncertainty quantification(UQ) criteria have been proposed. However, even though important, there are limited tools to integrate these methods, hindering the practical usage of UQ methods and future research in this domain. To bridge this gap, in this paper, we introduce UncertaintyZoo, a unified toolkit that integrates 29 uncertainty quantification methods, covering five major categories under a standardized interface. Using UncertaintyZoo, we evaluate the usefulness of existing uncertainty quantification methods under the code vulnerability detection task on CodeBERT and ChatGLM3 models. The results demonstrate that UncertaintyZoo effectively reveals prediction uncertainty. The tool with a demonstration video is available on the project site https://github.com/Paddingbuta/UncertaintyZoo.

</details>


### [156] [Smart Spatial Planning in Egypt: An Algorithm-Driven Approach to Public Service Evaluation in Qena City](https://arxiv.org/abs/2512.06431)
*Mohamed Shamroukh,Mohamed Alkhuzamy Aziz*

Main category: cs.AI

TL;DR: 开发针对埃及基纳市的定制化公共服务规划模型，使用Voronoi图算法评估设施覆盖率，发现平均覆盖率为81.3%，市中心服务密度高而郊区低。


<details>
  <summary>Details</summary>
Motivation: 埃及的国家公共服务规划标准往往无法适应地方独特特征，需要开发针对特定城市的定制化规划模型来解决这一差距。

Method: 采用混合方法（描述性、分析性和实验性），使用Python编程开发基于Voronoi图的智能空间分析算法，生成城市特定的规划标准并评估现有公共设施覆盖情况。

Result: 模型应用显示平均服务覆盖率为81.3%，救护车站效率最高（99.8%），公园和开放空间覆盖率最低（10%）。市中心服务密度高（>45个/平方公里），郊区显著降低（<5个/平方公里）。Hajer基纳区未服务区域最多，第一区服务覆盖率最高。

Conclusion: 成功开发了本地化规划标准模型和自动化评估算法，为埃及城市提供了可复制的数据驱动城市规划框架。

Abstract: National planning standards for public services in Egypt often fail to align with unique local characteristics. Addressing this gap, this study develops a tailored planning model for Qena City. Using a hybrid methodology (descriptive, analytical, and experimental), the research utilizes Python programming to generate an intelligent spatial analysis algorithm based on Voronoi Diagrams. This approach creates city-specific planning criteria and evaluates the current coverage of public facilities. The primary contribution of this study is the successful derivation of a localized planning standards model and the deployment of an automated algorithm to assess service efficiency. Application of this model reveals a general service coverage average of 81.3%. Ambulance stations demonstrated the highest efficiency (99.8%) due to recent upgrades, while parks and open spaces recorded the lowest coverage (10%) caused by limited land availability. Spatial analysis indicates a high service density in midtown (>45 services/km^2), which diminishes significantly towards the outskirts (<5 services/km^2). Consequently, the Hajer Qena district contains the highest volume of unserved areas, while the First District (Qesm 1) exhibits the highest level of service coverage. This model offers a replicable framework for data-driven urban planning in Egyptian cities.

</details>


### [157] [The Effect of Belief Boxes and Open-mindedness on Persuasion](https://arxiv.org/abs/2512.06573)
*Onur Bilgin,Abdullah As Sami,Sriram Sai Vujjini,John Licato*

Main category: cs.AI

TL;DR: 研究探索了LLM智能体的信念箱（belief boxes）如何影响其行为、信念改变和说服力，特别是在多智能体辩论和同伴压力场景中。


<details>
  <summary>Details</summary>
Motivation: 随着多智能体系统在推理和决策应用中日益普及，需要让基于LLM的智能体拥有类似命题信念的能力。当前简单方法是在提示空间中包含信念陈述（信念箱），但需要研究这如何实际影响智能体行为和信念倾向。

Method: 通过一系列实验探索信念箱对智能体行为的影响，包括：1）信念陈述及其强度如何影响对对立观点的抵抗力和说服力；2）开放心态指令如何影响信念改变的倾向；3）在辩论中被对立观点包围（同伴压力场景）时的信念变化。

Result: 研究发现：1）开放心态指令确实影响智能体对信念改变的接受度；2）信念陈述及其强度影响智能体对对立观点的抵抗力和说服力；3）在同伴压力场景中（智能体在辩论中被对立观点包围），信念陈述显著影响信念改变的可能性。

Conclusion: 信念箱技术在推理和决策任务中是可行且有效的，能够显著影响智能体的信念倾向、说服力和对同伴压力的反应，为多智能体系统中的信念建模提供了实用方法。

Abstract: As multi-agent systems are increasingly utilized for reasoning and decision-making applications, there is a greater need for LLM-based agents to have something resembling propositional beliefs. One simple method for doing so is to include statements describing beliefs maintained in the prompt space (in what we'll call their belief boxes). But when agents have such statements in belief boxes, how does it actually affect their behaviors and dispositions towards those beliefs? And does it significantly affect agents' ability to be persuasive in multi-agent scenarios? Likewise, if the agents are given instructions to be open-minded, how does that affect their behaviors? We explore these and related questions in a series of experiments. Our findings confirm that instructing agents to be open-minded affects how amenable they are to belief change. We show that incorporating belief statements and their strengths influences an agent's resistance to (and persuasiveness against) opposing viewpoints. Furthermore, it affects the likelihood of belief change, particularly when the agent is outnumbered in a debate by opposing viewpoints, i.e., peer pressure scenarios. The results demonstrate the feasibility and validity of the belief box technique in reasoning and decision-making tasks.

</details>


### [158] [FlatFormer: A Flat Transformer Knowledge Tracing Model Based on Cognitive Bias Injection](https://arxiv.org/abs/2512.06629)
*Xiao-li Xia,Hou-biao Li*

Main category: cs.AI

TL;DR: FlatFormer：一种基于"信息注入而非结构堆叠"的扁平Transformer架构，解决了知识追踪中性能与复杂度的矛盾，在保持高性能的同时大幅降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 知识追踪模型面临"性能-复杂度陷阱"：捕捉复杂认知动态（如学习会话和记忆衰减）需要深度分层架构，但这会导致实时部署时计算成本过高。

Method: 提出FlatFormer，采用扁平Transformer架构，通过两种轻量级注入机制：1) 结合可学习会话标识符和固定正弦步长嵌入的混合输入编码；2) 将预计算幂律偏置直接集成到注意力对数中，显式建模遗忘曲线。

Result: 在四个大规模数据集（如EdNet、Junyi）上的实验表明，FlatFormer达到最先进性能。在EdNet数据集上，相比最强分层基线（HiTSKT），绝对AUC提升8.3%，参数使用量不到15%，推理速度约快3倍。

Conclusion: 高认知保真度不需要架构复杂性，通过信息注入而非结构堆叠的设计范式可以有效解决知识追踪中的性能-复杂度权衡问题。

Abstract: Knowledge Tracing (KT) models face a critical ``Performance-Complexity Trap'': capturing complex cognitive dynamics like learning sessions and memory decay typically requires deep hierarchical architectures, which incur prohibitive computational costs for real-time deployment. To resolve this, we propose FlatFormer, a streamlined architecture based on the novel design paradigm of ``Information Injection over Structural Stacking.'' Unlike parameter-heavy hierarchical models, FlatFormer leverages a standard flat Transformer augmented with two lightweight injection mechanisms: (i) a hybrid input encoding strategy combining learnable session identifiers with fixed sinusoidal step embeddings; and (ii) a pre-computed power-law bias integrated directly into attention logits to explicitly model the forgetting curve. Extensive experiments on four large-scale datasets (e.g., EdNet, Junyi) show that FlatFormer achieves state-of-the-art performance. For example, on the EdNet dataset, compared to the strongest hierarchical baseline (HiTSKT), its absolute AUC increased by 8.3%, while using less than 15% of parameters, and inference speed was about three times faster. These results validate that high cognitive fidelity does not necessitate architectural complexity.

</details>


### [159] [LightSearcher: Efficient DeepSearch via Experiential Memory](https://arxiv.org/abs/2512.06653)
*Hengzhi Lan,Yue Yu,Li Qian,Li Peng,Jie Wu,Wei Liu,Jian Luan,Ting Bai*

Main category: cs.AI

TL;DR: LightSearcher是一个高效的强化学习框架，通过文本经验记忆和自适应奖励机制，在DeepSearch范式中平衡准确性和效率，显著减少工具调用次数和计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有的RL驱动的DeepSearch系统存在准确性-效率的跷跷板权衡：频繁调用搜索工具可以提高事实准确性，但会导致不必要的计算开销和效率下降。需要解决这一挑战。

Method: 1. 通过对比学习推理轨迹来学习文本经验记忆，生成成功推理模式的可解释摘要；2. 采用自适应奖励塑造机制，仅在正确答案场景中惩罚冗余工具调用。

Result: 在四个多跳QA基准测试中，LightSearcher保持了与SOTA基线ReSearch相当的准确性，同时将搜索工具调用减少了39.6%，推理时间减少了48.6%，token消耗减少了21.2%。

Conclusion: LightSearcher通过创新的文本经验记忆和自适应奖励机制，有效解决了DeepSearch范式中准确性-效率的权衡问题，实现了更高效的推理系统。

Abstract: DeepSearch paradigms have become a core enabler for deep reasoning models, allowing them to invoke external search tools to access up-to-date, domain-specific knowledge beyond parametric boundaries, thereby enhancing the depth and factual reliability of reasoning. Building upon this foundation, recent advances in reinforcement learning (RL) have further empowered models to autonomously and strategically control search tool usage, optimizing when and how to query external knowledge sources. Yet, these RL-driven DeepSearch systems often reveal a see-saw trade-off between accuracy and efficiency-frequent tool invocations can improve factual correctness but lead to unnecessary computational overhead and diminished efficiency. To address this challenge, we propose LightSearcher, an efficient RL framework that incorporates textual experiential memory by learning contrastive reasoning trajectories to generate interpretable summaries of successful reasoning patterns. In addition, it employs an adaptive reward shaping mechanism that penalizes redundant tool calls only in correct-answer scenarios. This design effectively balances the inherent accuracy-efficiency trade-off in DeepSearch paradigms. Experiments on four multi-hop QA benchmarks show that LightSearcher maintains accuracy comparable to SOTA baseline ReSearch, while reducing search tool invocations by 39.6%, inference time by 48.6%, and token consumption by 21.2%, demonstrating its superior efficiency.

</details>


### [160] [Academic journals' AI policies fail to curb the surge in AI-assisted academic writing](https://arxiv.org/abs/2512.06705)
*Yongyuan He,Yi Bu*

Main category: cs.AI

TL;DR: 期刊AI政策虽普及但无效：70%期刊有AI使用披露政策，但研究者AI工具使用激增，政策有无无显著差异；仅0.1%论文实际披露AI使用，显示透明度严重不足。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在学术写作中的快速应用引发了期刊和出版社的政策响应，但这些政策的实际效果尚不明确。需要评估AI使用指南在现实世界中的影响。

Method: 分析5,114种期刊和520万篇论文，评估AI使用指南的实际影响。对164,000篇科学出版物进行全文分析，特别关注2023年以来的75,000篇论文。

Result: 1. 70%期刊采纳AI政策（主要要求披露），但研究者AI工具使用在各学科均大幅增长，有政策与无政策期刊间无显著差异；2. 非英语国家、物理科学和高开放获取期刊增长最快；3. 2023年以来发表的75,000篇论文中，仅76篇（0.1%）明确披露AI使用，存在巨大透明度差距。

Conclusion: 当前政策在促进透明度或限制AI采用方面基本失败。需要重新评估伦理框架，以促进科学中负责任的AI整合。

Abstract: The rapid integration of generative AI into academic writing has prompted widespread policy responses from journals and publishers. However, the effectiveness of these policies remains unclear. Here, we analyze 5,114 journals and over 5.2 million papers to evaluate the real-world impact of AI usage guidelines. We show that despite 70% of journals adopting AI policies (primarily requiring disclosure), researchers' use of AI writing tools has increased dramatically across disciplines, with no significant difference between journals with or without policies. Non-English-speaking countries, physical sciences, and high-OA journals exhibit the highest growth rates. Crucially, full-text analysis on 164k scientific publications reveals a striking transparency gap: Of the 75k papers published since 2023, only 76 (0.1%) explicitly disclosed AI use. Our findings suggest that current policies have largely failed to promote transparency or restrain AI adoption. We urge a re-evaluation of ethical frameworks to foster responsible AI integration in science.

</details>


### [161] [Stochasticity in Agentic Evaluations: Quantifying Inconsistency with Intraclass Correlation](https://arxiv.org/abs/2512.06710)
*Zairah Mustahsan,Abel Lim,Megna Anand,Saahil Jain,Bryan McCann*

Main category: cs.AI

TL;DR: 提出使用组内相关系数(ICC)来评估语言模型代理系统的可靠性，分解方差为任务难度和代理不一致性，提高评估可信度


<details>
  <summary>Details</summary>
Motivation: 当前评估实践仅报告单一准确率数字，掩盖了结果背后的方差，无法区分真实能力提升与幸运采样，导致不可靠的代理系统评估

Method: 采用测量科学中的组内相关系数(ICC)来分解观察方差，分为查询间方差(任务难度)和查询内方差(代理不一致性)，并在GAIA和FRAMES数据集上进行评估

Result: ICC随任务结构变化显著：推理和检索任务(FRAMES)ICC=0.4955-0.7118，代理任务(GAIA)ICC=0.304-0.774。结构化任务需要8-16次试验收敛，复杂推理需要≥32次

Conclusion: 建议将准确率与ICC和查询内方差一起作为标准报告实践，更新评估卡片以捕捉这些指标，将代理基准测试从不透明的排行榜竞争转变为可信的实验科学

Abstract: As large language models become components of larger agentic systems, evaluation reliability becomes critical: unreliable sub-agents introduce brittleness into downstream system behavior. Yet current evaluation practice, reporting a single accuracy number from a single run, obscures the variance underlying these results, making it impossible to distinguish genuine capability improvements from lucky sampling. We propose adopting Intraclass Correlation Coefficient (ICC), a metric from measurement science, to characterize this variance. ICC decomposes observed variance into between-query variance (task difficulty) and within-query variance (agent inconsistency), highlighting whether reported results reflect true capability or measurement noise. We evaluated on GAIA (Levels 1-3, measuring agentic capabilities across varying reasoning complexity) and FRAMES (measuring retrieval and factuality across multiple documents). We found that ICC varies dramatically with task structure, with reasoning and retrieval tasks (FRAMES) exhibit ICC=0.4955-0.7118 across models, and agentic tasks (GAIA) exhibiting ICC=0.304-0.774 across models. For sub-agent replacement decisions in agentic systems, accuracy improvements are only trustworthy if ICC also improves. We demonstrate that ICC converges by n=8-16 trials for structured tasks and n>=32 for complex reasoning, enabling practitioners to set evidence-based resampling budgets. We recommend reporting accuracy alongside ICC and within-query variance as standard practice, and propose updated Evaluation Cards capturing these metrics. By making evaluation stability visible, we aim to transform agentic benchmarking from opaque leaderboard competition to trustworthy experimental science. Our code is open-sourced at https://github.com/youdotcom-oss/stochastic-agent-evals.

</details>


### [162] [Cognitive Control Architecture (CCA): A Lifecycle Supervision Framework for Robustly Aligned AI Agents](https://arxiv.org/abs/2512.06716)
*Zhibo Liang,Tianze Hu,Zaiye Chen,Mingjie Tang*

Main category: cs.AI

TL;DR: 本文提出认知控制架构（CCA），一种针对LLM智能体间接提示注入攻击的全生命周期防御框架，通过意图图和分层裁决器实现安全、功能与效率的平衡。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体对间接提示注入攻击高度脆弱，现有防御机制存在安全与功能之间的根本性权衡冲突，且防御架构碎片化，无法在整个任务执行流水线中提供完整的完整性保障。

Method: 提出认知控制架构（CCA），包含两个协同支柱：1）通过预生成的"意图图"主动执行控制流和数据流完整性保护；2）创新的"分层裁决器"，在检测到异常时基于多维评分启动深度推理，专门应对复杂的条件攻击。

Result: 在AgentDojo基准测试中，CCA不仅能有效抵御挑战其他先进防御方法的复杂攻击，而且在不妥协安全性的前提下实现了显著的效率和鲁棒性，成功调和了多维度权衡。

Conclusion: CCA通过全生命周期认知监督，为LLM智能体提供了对抗间接提示注入攻击的有效防御框架，解决了现有防御机制在安全、功能和效率之间的根本性权衡问题。

Abstract: Autonomous Large Language Model (LLM) agents exhibit significant vulnerability to Indirect Prompt Injection (IPI) attacks. These attacks hijack agent behavior by polluting external information sources, exploiting fundamental trade-offs between security and functionality in existing defense mechanisms. This leads to malicious and unauthorized tool invocations, diverting agents from their original objectives. The success of complex IPIs reveals a deeper systemic fragility: while current defenses demonstrate some effectiveness, most defense architectures are inherently fragmented. Consequently, they fail to provide full integrity assurance across the entire task execution pipeline, forcing unacceptable multi-dimensional compromises among security, functionality, and efficiency. Our method is predicated on a core insight: no matter how subtle an IPI attack, its pursuit of a malicious objective will ultimately manifest as a detectable deviation in the action trajectory, distinct from the expected legitimate plan. Based on this, we propose the Cognitive Control Architecture (CCA), a holistic framework achieving full-lifecycle cognitive supervision. CCA constructs an efficient, dual-layered defense system through two synergistic pillars: (i) proactive and preemptive control-flow and data-flow integrity enforcement via a pre-generated "Intent Graph"; and (ii) an innovative "Tiered Adjudicator" that, upon deviation detection, initiates deep reasoning based on multi-dimensional scoring, specifically designed to counter complex conditional attacks. Experiments on the AgentDojo benchmark substantiate that CCA not only effectively withstands sophisticated attacks that challenge other advanced defense methods but also achieves uncompromised security with notable efficiency and robustness, thereby reconciling the aforementioned multi-dimensional trade-off.

</details>


### [163] [ProAgent: Harnessing On-Demand Sensory Contexts for Proactive LLM Agent Systems](https://arxiv.org/abs/2512.06721)
*Bufang Yang,Lilin Xu,Liekang Zeng,Yunqi Guo,Siyang Jiang,Wenrui Lu,Kaiwei Liu,Hancheng Xiang,Xiaofan Jiang,Guoliang Xing,Zhenyu Yan*

Main category: cs.AI

TL;DR: ProAgent：首个端到端主动式LLM代理系统，通过分层感知提取上下文，结合LLM推理提供主动协助，在AR眼镜上实现并显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理主要采用被动响应模式，需要用户明确指令才能启动服务，增加了用户的物理和认知负担。需要一种能够主动感知环境并提供协助的系统。

Method: 1. 主动导向的上下文提取：采用按需分层感知方法持续感知环境，提取包含感官和用户画像线索的层次化上下文；2. 上下文感知的主动推理器：将上下文映射到用户需求和工具调用，提供主动协助

Result: 在真实世界测试平台、公共数据集和用户研究中评估，ProAgent比最先进基线方法：主动预测准确率提高33.4%，工具调用F1分数提高16.8%，用户满意度显著提升

Conclusion: ProAgent是首个端到端主动代理系统，通过大规模感官上下文和LLM推理提供主动协助，在AR眼镜上实现，标志着向主动助手迈出了重要一步

Abstract: Large Language Model (LLM) agents are emerging to transform daily life. However, existing LLM agents primarily follow a reactive paradigm, relying on explicit user instructions to initiate services, which increases both physical and cognitive workload. In this paper, we propose ProAgent, the first end-to-end proactive agent system that harnesses massive sensory contexts and LLM reasoning to deliver proactive assistance. ProAgent first employs a proactive-oriented context extraction approach with on-demand tiered perception to continuously sense the environment and derive hierarchical contexts that incorporate both sensory and persona cues. ProAgent then adopts a context-aware proactive reasoner to map these contexts to user needs and tool calls, providing proactive assistance. We implement ProAgent on Augmented Reality (AR) glasses with an edge server and extensively evaluate it on a real-world testbed, a public dataset, and through a user study. Results show that ProAgent achieves up to 33.4% higher proactive prediction accuracy, 16.8% higher tool-calling F1 score, and notable improvements in user satisfaction over state-of-the-art baselines, marking a significant step toward proactive assistants. A video demonstration of ProAgent is available at https://youtu.be/pRXZuzvrcVs.

</details>


### [164] [PICKT: Practical Interlinked Concept Knowledge Tracing for Personalized Learning using Knowledge Map Concept Relations](https://arxiv.org/abs/2512.07179)
*Wonbeen Lee,Channyoung Lee,Junho Sohn,Hansam Cho*

Main category: cs.AI

TL;DR: 提出了PICKT模型，通过知识图谱处理多种输入数据格式，解决知识追踪中的冷启动问题，提升在真实ITS环境中的实用性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有知识追踪模型存在输入数据格式受限、新学生/新题目冷启动问题、以及真实服务环境中稳定性不足等局限性，需要开发更实用的模型来支持个性化智能教学系统。

Method: 提出PICKT模型，利用知识图谱结构化概念间关系，结合题目和概念文本信息，有效处理多种输入数据格式，特别针对冷启动场景进行优化。

Result: 实验表明PICKT在真实操作环境中表现优异，在两种核心冷启动挑战（新学生注册和新题目添加）上显著优于现有模型，验证了模型的稳定性和实用性。

Conclusion: PICKT模型为下一代智能教学系统的实际部署提供了关键的理论和技术基础，通过解决冷启动问题和提升模型实用性，增强了知识追踪在真实产品环境中的应用价值。

Abstract: With the recent surge in personalized learning, Intelligent Tutoring Systems (ITS) that can accurately track students' individual knowledge states and provide tailored learning paths based on this information are in demand as an essential task. This paper focuses on the core technology of Knowledge Tracing (KT) models that analyze students' sequences of interactions to predict their knowledge acquisition levels. However, existing KT models suffer from limitations such as restricted input data formats, cold start problems arising with new student enrollment or new question addition, and insufficient stability in real-world service environments. To overcome these limitations, a Practical Interlinked Concept Knowledge Tracing (PICKT) model that can effectively process multiple types of input data is proposed. Specifically, a knowledge map structures the relationships among concepts considering the question and concept text information, thereby enabling effective knowledge tracing even in cold start situations. Experiments reflecting real operational environments demonstrated the model's excellent performance and practicality. The main contributions of this research are as follows. First, a model architecture that effectively utilizes diverse data formats is presented. Second, significant performance improvements are achieved over existing models for two core cold start challenges: new student enrollment and new question addition. Third, the model's stability and practicality are validated through delicate experimental design, enhancing its applicability in real-world product environments. This provides a crucial theoretical and technical foundation for the practical implementation of next-generation ITS.

</details>


### [165] [DoVer: Intervention-Driven Auto Debugging for LLM Multi-Agent Systems](https://arxiv.org/abs/2512.06749)
*Ming Ma,Jue Zhang,Fangkai Yang,Yu Kang,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.AI

TL;DR: DoVer是一个基于干预的调试框架，用于LLM多智能体系统，通过主动验证而非仅日志分析来定位和修复故障


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的多智能体系统调试方法存在两个关键限制：1) 仅日志调试缺乏验证，产生未测试的假设；2) 单步或单智能体归因通常不准确，因为多个不同的干预措施都能独立修复故障任务

Method: DoVer引入干预驱动的调试框架，通过有针对性的干预（如编辑消息、改变计划）来增强假设生成与主动验证。采用结果导向的调试视角，衡量系统是否解决故障或向任务成功取得可量化进展

Result: 在Magnetic-One框架上，DoVer将18-28%的失败试验转为成功，达到16%的里程碑进展，验证或反驳30-60%的故障假设。在不同数据集和框架上也能恢复49%的失败试验

Conclusion: 干预是提高智能体系统可靠性的实用机制，为基于LLM的多智能体系统开启了更强大、可扩展的调试方法机会

Abstract: Large language model (LLM)-based multi-agent systems are challenging to debug because failures often arise from long, branching interaction traces. The prevailing practice is to leverage LLMs for log-based failure localization, attributing errors to a specific agent and step. However, this paradigm has two key limitations: (i) log-only debugging lacks validation, producing untested hypotheses, and (ii) single-step or single-agent attribution is often ill-posed, as we find that multiple distinct interventions can independently repair the failed task. To address the first limitation, we introduce DoVer, an intervention-driven debugging framework, which augments hypothesis generation with active verification through targeted interventions (e.g., editing messages, altering plans). For the second limitation, rather than evaluating on attribution accuracy, we focus on measuring whether the system resolves the failure or makes quantifiable progress toward task success, reflecting a more outcome-oriented view of debugging. Within the Magnetic-One agent framework, on the datasets derived from GAIA and AssistantBench, DoVer flips 18-28% of failed trials into successes, achieves up to 16% milestone progress, and validates or refutes 30-60% of failure hypotheses. DoVer also performs effectively on a different dataset (GSMPlus) and agent framework (AG2), where it recovers 49% of failed trials. These results highlight intervention as a practical mechanism for improving reliability in agentic systems and open opportunities for more robust, scalable debugging methods for LLM-based multi-agent systems. Project website and code will be available at https://aka.ms/DoVer.

</details>


### [166] [Decouple to Generalize: Context-First Self-Evolving Learning for Data-Scarce Vision-Language Reasoning](https://arxiv.org/abs/2512.06835)
*Tingyu Li,Zheng Sun,Jingxuan Wei,Siyuan Li,Conghui He,Lijun Wu,Cheng Tan*

Main category: cs.AI

TL;DR: DoGe提出双解耦框架，通过分离思考者和解决者组件，结合两阶段强化学习，解决视觉语言模型在专业领域训练中的奖励黑客和数据多样性问题。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在专业领域（如化学、地球科学、多模态数学）训练面临高质量多模态数据稀缺问题。现有合成数据和自奖励机制存在分布有限和对齐困难，导致奖励黑客现象——模型利用高奖励模式，导致策略熵崩溃和训练不稳定。

Method: DoGe采用双解耦框架：1) 将学习过程解耦为思考者和解决者两个组件，引导模型首先从上下文学习而非直接解决问题；2) 提出两阶段强化学习后训练方法，从自由探索上下文到实际解决任务；3) 构建演化课程学习管道，包括扩展的本体领域知识语料库和迭代演化的种子问题池。

Result: 实验表明，该方法在各种基准测试中持续优于基线，为实现自演化大型视觉语言模型提供了可扩展的路径。

Conclusion: DoGe通过双解耦框架和演化课程学习，有效解决了视觉语言模型在专业领域训练中的奖励黑客和数据多样性问题，为实现自演化大型视觉语言模型提供了可行方案。

Abstract: Recent vision-language models (VLMs) achieve remarkable reasoning through reinforcement learning (RL), which provides a feasible solution for realizing continuous self-evolving large vision-language models (LVLMs) in the era of experience. However, RL for VLMs requires abundant high-quality multimodal data, especially challenging in specialized domains like chemistry, earth sciences, and multimodal mathematics. Existing strategies such as synthetic data and self-rewarding mechanisms suffer from limited distributions and alignment difficulties, ultimately causing reward hacking: models exploit high-reward patterns, collapsing policy entropy and destabilizing training. We propose DoGe (Decouple to Generalize), a dual-decoupling framework that guides models to first learn from context rather than problem solving by refocusing on the problem context scenarios overlooked by synthetic data methods. By decoupling learning process into dual components (Thinker and Solver), we reasonably quantify the reward signals of this process and propose a two-stage RL post-training approach from freely exploring context to practically solving tasks. Second, to increase the diversity of training data, DoGe constructs an evolving curriculum learning pipeline: an expanded native domain knowledge corpus and an iteratively evolving seed problems pool. Experiments show that our method consistently outperforms the baseline across various benchmarks, providing a scalable pathway for realizing self-evolving LVLMs.

</details>


### [167] [JT-DA: Enhancing Data Analysis with Tool-Integrated Table Reasoning Large Language Models](https://arxiv.org/abs/2512.06859)
*Ce Chi,Xing Wang,Zhendong Wang,Xiaofan Liu,Ce Li,Zhiyan Song,Chen Zhao,Kexin Yang,Boshen Shi,Jingjing Yang,Chao Deng,Junlan Feng*

Main category: cs.AI

TL;DR: JT-DA-8B是一个专门用于复杂表格推理任务的8B参数大语言模型，通过构建包含34个表格推理任务的多样化训练语料，采用SFT和RL优化，并提出了四阶段表格推理工作流程来提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前在表格推理场景中缺乏高质量监督数据，需要专门针对复杂表格推理任务的大语言模型来处理多样化的真实世界场景。

Method: 1) 构建包含34个表格推理任务的多样化训练语料，整合29个公开表格QA数据集和300万张表格；2) 提出自动管道生成现实的多步分析任务；3) 基于开源的JT-Coder-8B模型，采用LLM评分和工作流对齐过滤来蒸馏高质量表格数据；4) 结合监督微调(SFT)和强化学习(RL)优化模型；5) 提出四阶段表格推理工作流程：表格预处理、表格感知、工具集成推理和提示工程。

Result: JT-DA-8B在各种表格推理任务中表现出色，证明了数据为中心的数据生成和工作流驱动的优化方法的有效性。

Conclusion: 该研究展示了通过构建高质量训练数据和系统化工作流程，可以开发出在复杂表格推理任务上表现优异的专门化大语言模型。

Abstract: In this work, we present JT-DA-8B (JiuTian Data Analyst 8B), a specialized large language model designed for complex table reasoning tasks across diverse real-world scenarios. To address the lack of high-quality supervision in tabular reasoning scenarios, we construct a comprehensive and diverse training corpus with 34 well-defined table reasoning tasks, by aggregating 29 public table QA datasets and 3 million tables. An automatic pipeline is proposed to generate realistic multi-step analytical tasks involving reasoning patterns. The model is trained upon open-source JT-Coder-8B model, an 8B-parameter decoder-only foundation model trained from scratch. In the training stage, we leverage LLM-based scoring and workflow-aligned filtering to distill high-quality, table-centric data. Both supervised fine-tuning (SFT) and Reinforcement learning (RL) are adopted to optimize our model. Afterwards, a four-stage table reasoning workflow is proposed, including table preprocessing, table sensing, tool-integrated reasoning, and prompt engineering, to improve model interpretability and execution accuracy. Experimental results show that JT-DA-8B achieves strong performance in various table reasoning tasks, demonstrating the effectiveness of data-centric generation and workflow-driven optimization.

</details>


### [168] [Do Persona-Infused LLMs Affect Performance in a Strategic Reasoning Game?](https://arxiv.org/abs/2512.06867)
*John Licato,Stephen Steinle,Brayden Hollis*

Main category: cs.AI

TL;DR: 研究探讨角色提示对LLM战略决策的影响，发现通过中介翻译过程将角色转化为启发式值能提升游戏表现，提出基于心理测量学原理的启发式生成方法。


<details>
  <summary>Details</summary>
Motivation: 虽然角色提示能触发LLM生成不同风格的文本，但尚不清楚这些差异是否能转化为可测量的行为差异，特别是在对抗性战略环境中如何影响决策制定。

Method: 在PERIL世界统治棋盘游戏中研究角色提示对战略表现的影响，引入结构化翻译过程（受探索性因子分析启发），将LLM生成的清单响应映射为启发式值，比较角色衍生的启发式策略与手动选择的策略。

Result: 某些与战略思维相关的角色能提升游戏表现，但仅当使用中介翻译过程时；该方法相比直接推断的启发式提高了启发式的可靠性和表面效度。

Conclusion: 研究推进了对角色提示如何影响LLM决策的理解，提出将心理测量学原理应用于LLM的启发式生成方法，为研究角色类型对决策的影响提供了更好工具。

Abstract: Although persona prompting in large language models appears to trigger different styles of generated text, it is unclear whether these translate into measurable behavioral differences, much less whether they affect decision-making in an adversarial strategic environment that we provide as open-source. We investigate the impact of persona prompting on strategic performance in PERIL, a world-domination board game. Specifically, we compare the effectiveness of persona-derived heuristic strategies to those chosen manually. Our findings reveal that certain personas associated with strategic thinking improve game performance, but only when a mediator is used to translate personas into heuristic values. We introduce this mediator as a structured translation process, inspired by exploratory factor analysis, that maps LLM-generated inventory responses into heuristics. Results indicate our method enhances heuristic reliability and face validity compared to directly inferred heuristics, allowing us to better study the effect of persona types on decision making. These insights advance our understanding of how persona prompting influences LLM-based decision-making and propose a heuristic generation method that applies psychometric principles to LLMs.

</details>


### [169] [On Memory: A comparison of memory mechanisms in world models](https://arxiv.org/abs/2512.06983)
*Eli J. Laird,Corey Clark*

Main category: cs.AI

TL;DR: 该论文研究了基于Transformer的世界模型的有效记忆跨度，分析了多种记忆增强机制，提出了记忆编码与记忆注入的分类框架，并通过状态回忆任务评估了不同机制的性能。


<details>
  <summary>Details</summary>
Motivation: 世界模型能够通过预测未来状态来帮助智能体在想象环境中进行规划，但其长期规划能力受到骨干架构有效记忆跨度的限制。这种限制导致长轨迹生成中的感知漂移，阻碍了模型在想象轨迹中完成循环闭合的能力。

Method: 作者研究了基于Transformer的世界模型的有效记忆跨度，分析了多种记忆增强机制。提出了一个分类法，区分记忆编码和记忆注入机制，并从残差流动力学的角度解释了它们扩展世界模型记忆的作用。使用状态回忆评估任务来测量每种机制的记忆回忆能力并分析各自的权衡。

Result: 研究发现记忆机制能够提高视觉Transformer的有效记忆跨度，并为在世界模型的想象中完成循环闭合提供了路径。

Conclusion: 记忆增强机制对于扩展世界模型的长期规划能力至关重要，特别是在处理长序列和实现循环闭合方面。该研究为改进世界模型的记忆系统提供了理论框架和实证评估方法。

Abstract: World models enable agents to plan within imagined environments by predicting future states conditioned on past observations and actions. However, their ability to plan over long horizons is limited by the effective memory span of the backbone architecture. This limitation leads to perceptual drift in long rollouts, hindering the model's capacity to perform loop closures within imagined trajectories. In this work, we investigate the effective memory span of transformer-based world models through an analysis of several memory augmentation mechanisms. We introduce a taxonomy that distinguishes between memory encoding and memory injection mechanisms, motivating their roles in extending the world model's memory through the lens of residual stream dynamics. Using a state recall evaluation task, we measure the memory recall of each mechanism and analyze its respective trade-offs. Our findings show that memory mechanisms improve the effective memory span in vision transformers and provide a path to completing loop closures within a world model's imagination.

</details>


### [170] [Utilizing Multi-Agent Reinforcement Learning with Encoder-Decoder Architecture Agents to Identify Optimal Resection Location in Glioblastoma Multiforme Patients](https://arxiv.org/abs/2512.06990)
*Krishna Arun,Moinak Bhattachrya,Paras Goel*

Main category: cs.AI

TL;DR: 开发了一个用于胶质母细胞瘤诊断和治疗的端到端AI系统，包含诊断阶段的4个分类模型和治疗阶段的3个生成模型，通过反馈循环优化治疗方案。


<details>
  <summary>Details</summary>
Motivation: 目前医学领域缺乏支持医生治疗异质性脑肿瘤（如胶质母细胞瘤）的AI系统。胶质母细胞瘤是最致命的人类癌症，五年生存率仅为5.1%，需要创新的AI解决方案来辅助诊断和治疗规划。

Method: 1. 诊断阶段：采用顺序决策框架，包含4个分类模型（卷积神经网络和支持向量机），逐步将患者大脑分类到更具体的类别，最终得出诊断结果。
2. 治疗规划阶段：使用强化学习系统，包含3个生成模型：扩散模型预测切除结果、时空视觉Transformer生成放疗后MRI、扩散模型生成化疗后MRI。
3. 反馈循环：使用生存率计算器（卷积神经网络）检查治疗后MRI的生存率是否在用户目标15%范围内，否则使用近端策略优化进行迭代优化。

Result: 1. 顺序决策框架将计算成本降低22.28倍
2. Transformer的回归能力将肿瘤进展推理时间减少113小时
3. 模拟真实情况的增强技术将整体DICE分数提高2.9%
4. 预计可将生存率提高0.9%，可能挽救约2250人的生命

Conclusion: 该研究开发了一个创新的端到端AI系统，能够有效辅助胶质母细胞瘤的诊断和治疗规划。通过顺序决策框架、生成模型和反馈循环的结合，系统在计算效率、推理速度和预测准确性方面都有显著提升，有望提高患者生存率。

Abstract: Currently, there is a noticeable lack of AI in the medical field to support doctors in treating heterogenous brain tumors such as Glioblastoma Multiforme (GBM), the deadliest human cancer in the world with a five-year survival rate of just 5.1%. This project develops an AI system offering the only end-to-end solution by aiding doctors with both diagnosis and treatment planning. In the diagnosis phase, a sequential decision-making framework consisting of 4 classification models (Convolutional Neural Networks and Support Vector Machine) are used. Each model progressively classifies the patient's brain into increasingly specific categories, with the final step being named diagnosis. For treatment planning, an RL system consisting of 3 generative models is used. First, the resection model (diffusion model) analyzes the diagnosed GBM MRI and predicts a possible resection outcome. Second, the radiotherapy model (Spatio-Temporal Vision Transformer) generates an MRI of the brain's progression after a user-defined number of weeks. Third, the chemotherapy model (Diffusion Model) produces the post-treatment MRI. A survival rate calculator (Convolutional Neural Network) then checks if the generated post treatment MRI has a survival rate within 15% of the user defined target. If not, a feedback loop using proximal policy optimization iterates over this system until an optimal resection location is identified. When compared to existing solutions, this project found 3 key findings: (1) Using a sequential decision-making framework consisting of 4 small diagnostic models reduced computing costs by 22.28x, (2) Transformers regression capabilities decreased tumor progression inference time by 113 hours, and (3) Applying Augmentations resembling Real-life situations improved overall DICE scores by 2.9%. These results project to increase survival rates by 0.9%, potentially saving approximately 2,250 lives.

</details>


### [171] [ClinNoteAgents: An LLM Multi-Agent System for Predicting and Interpreting Heart Failure 30-Day Readmission from Clinical Notes](https://arxiv.org/abs/2512.07081)
*Rongjia Zhou,Chengzhuo Li,Carl Yang,Jiaying Lu*

Main category: cs.AI

TL;DR: 提出ClinNoteAgents框架，利用LLM多智能体将临床自由文本笔记转化为结构化风险因素表示和临床风格抽象，用于心衰30天再入院预测，减少对结构化字段的依赖。


<details>
  <summary>Details</summary>
Motivation: 心衰是美国老年人再入院的主要原因之一。临床笔记包含丰富的患者信息，占电子健康记录的大部分，但在心衰再入院风险分析中未得到充分利用。传统模型依赖专家规则、医学术语表和本体来解释临床笔记，但这些笔记通常存在拼写错误、缩写和专业术语。

Method: ClinNoteAgents是一个基于LLM的多智能体框架，将自由文本临床笔记转化为：(1) 临床和社会风险因素的结构化表示用于关联分析；(2) 临床风格的抽象表示用于心衰30天再入院预测。该框架减少对结构化字段的依赖，最小化人工标注和模型训练。

Result: 在3,544份来自2,065名患者（再入院率=35.16%）的笔记上评估，展示了在从自由文本提取风险因素、识别关键贡献因素和预测再入院风险方面的强大性能。

Conclusion: ClinNoteAgents为数据有限的医疗系统提供了一种可扩展且可解释的基于笔记的心衰再入院风险建模方法，通过减少对结构化字段的依赖并最小化人工标注和模型训练。

Abstract: Heart failure (HF) is one of the leading causes of rehospitalization among older adults in the United States. Although clinical notes contain rich, detailed patient information and make up a large portion of electronic health records (EHRs), they remain underutilized for HF readmission risk analysis. Traditional computational models for HF readmission often rely on expert-crafted rules, medical thesauri, and ontologies to interpret clinical notes, which are typically written under time pressure and may contain misspellings, abbreviations, and domain-specific jargon. We present ClinNoteAgents, an LLM-based multi-agent framework that transforms free-text clinical notes into (1) structured representations of clinical and social risk factors for association analysis and (2) clinician-style abstractions for HF 30-day readmission prediction. We evaluate ClinNoteAgents on 3,544 notes from 2,065 patients (readmission rate=35.16%), demonstrating strong performance in extracting risk factors from free-text, identifying key contributing factors, and predicting readmission risk. By reducing reliance on structured fields and minimizing manual annotation and model training, ClinNoteAgents provides a scalable and interpretable approach to note-based HF readmission risk modeling in data-limited healthcare systems.

</details>


### [172] [VIGIL: A Reflective Runtime for Self-Healing Agents](https://arxiv.org/abs/2512.07094)
*Christopher Cruz*

Main category: cs.AI

TL;DR: VIGIL是一个可验证的检查和防护迭代学习运行时系统，通过监督兄弟代理、分析行为日志、维护情感银行，实现自主维护和自我修复，而非任务执行。


<details>
  <summary>Details</summary>
Motivation: 当前代理LLM框架存在脆弱性问题，缺乏运行时自省能力，无法诊断自身故障模式，且没有人类干预就无法改进。大多数代理系统退化为装饰性的LLM调用链，缺乏可靠性结构机制。

Method: VIGIL作为反射运行时监督兄弟代理，摄入行为日志，将事件评估为结构化情感表示，维护具有衰减和上下文策略的持久情感银行，生成RBT诊断（优势、机会、失败），并产生防护性提示更新和只读代码提案。采用状态门控管道，非法转换产生显式错误。

Result: 在提醒延迟案例研究中，VIGIL识别出延迟升高，提出提示和代码修复方案。当自身诊断工具因模式冲突失败时，能暴露内部错误、生成备用诊断并发出修复计划，展示了部署代理运行时的元级自我修复能力。

Conclusion: VIGIL实现了代理系统的自主维护和自我修复能力，通过结构化情感分析、状态门控管道和元级错误处理，解决了当前代理框架缺乏运行时自省和改进机制的问题。

Abstract: Agentic LLM frameworks promise autonomous behavior via task decomposition, tool use, and iterative planning, but most deployed systems remain brittle. They lack runtime introspection, cannot diagnose their own failure modes, and do not improve over time without human intervention. In practice, many agent stacks degrade into decorated chains of LLM calls with no structural mechanisms for reliability. We present VIGIL (Verifiable Inspection and Guarded Iterative Learning), a reflective runtime that supervises a sibling agent and performs autonomous maintenance rather than task execution. VIGIL ingests behavioral logs, appraises each event into a structured emotional representation, maintains a persistent EmoBank with decay and contextual policies, and derives an RBT diagnosis that sorts recent behavior into strengths, opportunities, and failures. From this analysis, VIGIL generates both guarded prompt updates that preserve core identity semantics and read only code proposals produced by a strategy engine that operates on log evidence and code hotspots. VIGIL functions as a state gated pipeline. Illegal transitions produce explicit errors rather than allowing the LLM to improvise. In a reminder latency case study, VIGIL identified elevated lag, proposed prompt and code repairs, and when its own diagnostic tool failed due to a schema conflict, it surfaced the internal error, produced a fallback diagnosis, and emitted a repair plan. This demonstrates meta level self repair in a deployed agent runtime.

</details>


### [173] [A Neural Affinity Framework for Abstract Reasoning: Diagnosing the Compositional Gap in Transformer Architectures via Procedural Task Taxonomy](https://arxiv.org/abs/2512.07109)
*Miguel Ingram,Arthur Joseph Merritt*

Main category: cs.AI

TL;DR: 提出了首个包含400个任务的9类别分类法，验证了Transformer在抽象推理任务中的神经亲和力天花板效应，揭示了组合性差距问题。


<details>
  <summary>Details</summary>
Motivation: 响应Hodel等人对任务相关性正式定义的需求，研究Transformer架构在抽象推理任务中的性能限制，探索神经亲和力对任务表现的影响。

Method: 1) 开发基于规则代码分析的9类别任务分类法；2) 使用原始网格像素训练CNN验证分类法的视觉一致性；3) 在302个任务上微调170万参数Transformer；4) 应用分类法分析ARC-AGI-2测试集和独立研究数据。

Result: 1) 分类法准确率达97.5%；2) CNN验证显示95.24%的S3准确率；3) 发现69.5%任务存在组合性差距（局部模式准确率高但全局合成准确率低）；4) 低亲和力任务表现显著低于高亲和力任务（51.9% vs 77.7%）；5) 揭示35.3%任务对Transformer具有低神经亲和力。

Conclusion: Transformer在抽象推理任务中存在神经亲和力天花板效应，性能受架构适宜性而非训练数据限制。未来进展需要开发具有亲和力对齐模块的混合架构。已发布验证的分类法供研究使用。

Abstract: Responding to Hodel et al.'s (2024) call for a formal definition of task relatedness in re-arc, we present the first 9-category taxonomy of all 400 tasks, validated at 97.5% accuracy via rule-based code analysis. We prove the taxonomy's visual coherence by training a CNN on raw grid pixels (95.24% accuracy on S3, 36.25% overall, 3.3x chance), then apply the taxonomy diagnostically to the original ARC-AGI-2 test set. Our curriculum analysis reveals 35.3% of tasks exhibit low neural affinity for Transformers--a distributional bias mirroring ARC-AGI-2. To probe this misalignment, we fine-tuned a 1.7M-parameter Transformer across 302 tasks, revealing a profound Compositional Gap: 210 of 302 tasks (69.5%) achieve >80% cell accuracy (local patterns) but <10% grid accuracy (global synthesis). This provides direct evidence for a Neural Affinity Ceiling Effect, where performance is bounded by architectural suitability, not curriculum. Applying our framework to Li et al.'s independent ViTARC study (400 specialists, 1M examples each) confirms its predictive power: Very Low affinity tasks achieve 51.9% versus 77.7% for High affinity (p<0.001), with a task at 0% despite massive data. The taxonomy enables precise diagnosis: low-affinity tasks (A2) hit hard ceilings, while high-affinity tasks (C1) reach 99.8%. These findings indicate that progress requires hybrid architectures with affinity-aligned modules. We release our validated taxonomy,

</details>


### [174] [ContextualSHAP : Enhancing SHAP Explanations Through Contextual Language Generation](https://arxiv.org/abs/2512.07178)
*Latifa Dwiyanti,Sergio Ryan Wibisono,Hidetaka Nambo*

Main category: cs.AI

TL;DR: 提出一个Python包，将SHAP与大型语言模型（GPT）结合，生成情境化的文本解释，提高非技术用户对机器学习模型解释的理解性。


<details>
  <summary>Details</summary>
Motivation: SHAP虽然能有效可视化特征重要性，但缺乏对非技术背景终端用户有意义的情境化解释。当前的可解释人工智能方法在提供用户友好的上下文解释方面存在不足。

Method: 开发一个Python包，将SHAP与OpenAI的GPT集成，通过用户定义的参数（如特征别名、描述和附加背景）来生成情境化的文本解释，使解释更贴合模型上下文和用户视角。

Result: 在医疗相关案例研究中应用该包，通过李克特量表调查和后续访谈进行用户评估。结果显示，与仅可视化输出相比，生成的解释被认为更容易理解和情境更合适。

Conclusion: 将可视化与情境化文本结合可以支持更用户友好和可信的模型解释。虽然结果是初步的，但表明这种集成方法有潜力提高可解释人工智能的实用性和可访问性。

Abstract: Explainable Artificial Intelligence (XAI) has become an increasingly important area of research, particularly as machine learning models are deployed in high-stakes domains. Among various XAI approaches, SHAP (SHapley Additive exPlanations) has gained prominence due to its ability to provide both global and local explanations across different machine learning models. While SHAP effectively visualizes feature importance, it often lacks contextual explanations that are meaningful for end-users, especially those without technical backgrounds. To address this gap, we propose a Python package that extends SHAP by integrating it with a large language model (LLM), specifically OpenAI's GPT, to generate contextualized textual explanations. This integration is guided by user-defined parameters (such as feature aliases, descriptions, and additional background) to tailor the explanation to both the model context and the user perspective. We hypothesize that this enhancement can improve the perceived understandability of SHAP explanations. To evaluate the effectiveness of the proposed package, we applied it in a healthcare-related case study and conducted user evaluations involving real end-users. The results, based on Likert-scale surveys and follow-up interviews, indicate that the generated explanations were perceived as more understandable and contextually appropriate compared to visual-only outputs. While the findings are preliminary, they suggest that combining visualization with contextualized text may support more user-friendly and trustworthy model explanations.

</details>


### [175] [Sample from What You See: Visuomotor Policy Learning via Diffusion Bridge with Observation-Embedded Stochastic Differential Equation](https://arxiv.org/abs/2512.07212)
*Zhaoyang Liu,Mokai Pan,Zhongyi Wang,Kaizhen Zhu,Haotao Lu,Jingya Wang,Ye Shi*

Main category: cs.AI

TL;DR: BridgePolicy是一种新的扩散桥生成视觉运动策略，通过将观测嵌入扩散过程的随机微分方程中，从信息丰富的先验而非随机噪声开始采样，显著提高了机器人控制的精度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的模仿学习方法通常将观测作为去噪网络的高层条件输入，而不是将其整合到扩散过程的随机动力学中。这导致采样必须从随机高斯噪声开始，削弱了感知与控制之间的耦合，通常产生次优性能。

Method: 提出BridgePolicy，通过扩散桥公式将观测显式嵌入随机微分方程中。设计了多模态融合模块和语义对齐器，统一视觉和状态输入，对齐观测和动作表示，使桥方法适用于异构机器人数据。

Result: 在三个基准测试的52个仿真任务和五个真实世界任务上的广泛实验表明，BridgePolicy始终优于最先进的生成策略。

Conclusion: BridgePolicy通过将观测整合到扩散过程的随机动力学中，从信息丰富的先验开始采样，显著提高了机器人控制的精度和可靠性，为异构机器人数据的生成策略提供了有效解决方案。

Abstract: Imitation learning with diffusion models has advanced robotic control by capturing multi-modal action distributions. However, existing approaches typically treat observations as high-level conditioning inputs to the denoising network, rather than integrating them into the stochastic dynamics of the diffusion process itself. As a result, sampling must begin from random Gaussian noise, weakening the coupling between perception and control and often yielding suboptimal performance. We introduce BridgePolicy, a generative visuomotor policy that explicitly embeds observations within the stochastic differential equation via a diffusion-bridge formulation. By constructing an observation-informed trajectory, BridgePolicy enables sampling to start from a rich, informative prior rather than random noise, substantially improving precision and reliability in control. A key challenge is that classical diffusion bridges connect distributions with matched dimensionality, whereas robotic observations are heterogeneous and multi-modal and do not naturally align with the action space. To address this, we design a multi-modal fusion module and a semantic aligner that unify visual and state inputs and align observation and action representations, making the bridge applicable to heterogeneous robot data. Extensive experiments across 52 simulation tasks on three benchmarks and five real-world tasks demonstrate that BridgePolicy consistently outperforms state-of-the-art generative policies.

</details>


### [176] [Cross-platform Product Matching Based on Entity Alignment of Knowledge Graph with RAEA model](https://arxiv.org/abs/2512.07232)
*Wenlong Liu,Jiahua Pan,Xingyu Zhang,Xinxin Gong,Yang Ye,Xujin Zhao,Xin Wang,Kent Wu,Hua Xiang,Houmin Yan,Qingpeng Zhang*

Main category: cs.AI

TL;DR: 提出RAEA框架用于实体对齐，通过两阶段过滤匹配eBay和亚马逊产品，利用属性和关系三元组的交互提升对齐效果


<details>
  <summary>Details</summary>
Motivation: 现有实体对齐方法未能充分利用属性三元组和关系三元组及其交互，特别是在产品匹配场景中，需要更有效地整合这两种信息源

Method: 采用两阶段管道：粗过滤和精细过滤。精细过滤使用RAEA框架，包含属性感知实体编码器和关系感知图注意力网络，聚合属性和关系的对齐信号

Result: 在跨语言数据集DBP15K上比12个基线平均提升6.59% Hits@1，在单语言数据集DWY100K上取得竞争性结果

Conclusion: RAEA框架通过有效利用属性和关系三元组的交互，显著提升了实体对齐性能，适用于产品匹配等实际应用场景

Abstract: Product matching aims to identify identical or similar products sold on different platforms. By building knowledge graphs (KGs), the product matching problem can be converted to the Entity Alignment (EA) task, which aims to discover the equivalent entities from diverse KGs. The existing EA methods inadequately utilize both attribute triples and relation triples simultaneously, especially the interactions between them. This paper introduces a two-stage pipeline consisting of rough filter and fine filter to match products from eBay and Amazon. For fine filtering, a new framework for Entity Alignment, Relation-aware and Attribute-aware Graph Attention Networks for Entity Alignment (RAEA), is employed. RAEA focuses on the interactions between attribute triples and relation triples, where the entity representation aggregates the alignment signals from attributes and relations with Attribute-aware Entity Encoder and Relation-aware Graph Attention Networks. The experimental results indicate that the RAEA model achieves significant improvements over 12 baselines on EA task in the cross-lingual dataset DBP15K (6.59% on average Hits@1) and delivers competitive results in the monolingual dataset DWY100K. The source code for experiments on DBP15K and DWY100K is available at github (https://github.com/Mockingjay-liu/RAEA-model-for-Entity-Alignment).

</details>


### [177] [M-STAR: Multi-Scale Spatiotemporal Autoregression for Human Mobility Modeling](https://arxiv.org/abs/2512.07314)
*Yuxiao Luo,Songming Zhang,Sijie Ruan,Siran Chen,Kang Liu,Yang Xu,Yu Zheng,Ling Yin*

Main category: cs.AI

TL;DR: M-STAR是一个多尺度时空自回归框架，用于高效生成长期人类移动轨迹，通过分层编码和粗到细预测提升生成质量和速度。


<details>
  <summary>Details</summary>
Motivation: 现有基于自回归和扩散模型的方法在生成长期轨迹（如周轨迹）时效率低下，且缺乏显式的时空多尺度建模能力。

Method: 提出M-STAR框架，包含多尺度时空标记器编码分层移动模式，以及基于Transformer的解码器进行下一尺度自回归预测，实现从粗到细的时空预测过程。

Result: 在两个真实数据集上的实验表明，M-STAR在保真度上优于现有方法，并显著提高了生成速度。

Conclusion: M-STAR通过多尺度时空建模有效解决了长期轨迹生成的效率和保真度问题，为人类移动建模提供了新方法。

Abstract: Modeling human mobility is vital for extensive applications such as transportation planning and epidemic modeling. With the rise of the Artificial Intelligence Generated Content (AIGC) paradigm, recent works explore synthetic trajectory generation using autoregressive and diffusion models. While these methods show promise for generating single-day trajectories, they remain limited by inefficiencies in long-term generation (e.g., weekly trajectories) and a lack of explicit spatiotemporal multi-scale modeling. This study proposes Multi-Scale Spatio-Temporal AutoRegression (M-STAR), a new framework that generates long-term trajectories through a coarse-to-fine spatiotemporal prediction process. M-STAR combines a Multi-scale Spatiotemporal Tokenizer that encodes hierarchical mobility patterns with a Transformer-based decoder for next-scale autoregressive prediction. Experiments on two real-world datasets show that M-STAR outperforms existing methods in fidelity and significantly improves generation speed. The data and codes are available at https://github.com/YuxiaoLuo0013/M-STAR.

</details>


### [178] [A Geometric Unification of Concept Learning with Concept Cones](https://arxiv.org/abs/2512.07355)
*Alexandre Rocchi--Henry,Thomas Fel,Gianni Franchi*

Main category: cs.AI

TL;DR: 该论文提出了一个统一几何框架，将监督式概念瓶颈模型（CBMs）和无监督稀疏自编码器（SAEs）联系起来，两者都学习激活空间中的线性方向，其非负组合形成概念锥。作者建立了CBMs和SAEs之间的操作桥梁，并提出了量化指标来评估SAEs学习的概念与人类定义概念的匹配程度。


<details>
  <summary>Details</summary>
Motivation: 概念可解释性领域存在两个并行发展但很少交流的传统：CBMs（规定概念应该是什么）和SAEs（发现涌现的概念）。CBMs使用监督学习将激活与人类标注的概念对齐，而SAEs依赖稀疏编码发现涌现概念。作者旨在统一这两种范式，为概念发现提供更系统的评估框架。

Method: 作者提出两种方法都实例化相同的几何结构：学习激活空间中的一组线性方向，其非负组合形成概念锥。基于这一观点，他们建立了CBMs和SAEs之间的操作桥梁：CBMs提供人类定义的参考几何，而SAEs可以通过其学习的概念锥与CBM概念锥的近似或包含程度来评估。这产生了量化指标，将SAE类型、稀疏度、扩展比等归纳偏置与合理概念的涌现联系起来。

Result: 研究发现，在稀疏度和扩展因子方面存在一个"最佳点"，能最大化与CBM概念的几何和语义对齐。通过提出的包含框架，作者能够定量评估SAEs学习的概念与人类定义概念的匹配程度，为衡量SAE进展提供了原则性指标。

Conclusion: 该工作通过共享的几何框架统一了监督和无监督概念发现，为衡量SAE进展和评估发现的概念与合理人类概念的对齐程度提供了原则性指标。这为概念可解释性研究建立了更系统的评估基础。

Abstract: Two traditions of interpretability have evolved side by side but seldom spoken to each other: Concept Bottleneck Models (CBMs), which prescribe what a concept should be, and Sparse Autoencoders (SAEs), which discover what concepts emerge. While CBMs use supervision to align activations with human-labeled concepts, SAEs rely on sparse coding to uncover emergent ones. We show that both paradigms instantiate the same geometric structure: each learns a set of linear directions in activation space whose nonnegative combinations form a concept cone. Supervised and unsupervised methods thus differ not in kind but in how they select this cone. Building on this view, we propose an operational bridge between the two paradigms. CBMs provide human-defined reference geometries, while SAEs can be evaluated by how well their learned cones approximate or contain those of CBMs. This containment framework yields quantitative metrics linking inductive biases -- such as SAE type, sparsity, or expansion ratio -- to emergence of plausible\footnote{We adopt the terminology of \citet{jacovi2020towards}, who distinguish between faithful explanations (accurately reflecting model computations) and plausible explanations (aligning with human intuition and domain knowledge). CBM concepts are plausible by construction -- selected or annotated by humans -- though not necessarily faithful to the true latent factors that organise the data manifold.} concepts. Using these metrics, we uncover a ``sweet spot'' in both sparsity and expansion factor that maximizes both geometric and semantic alignment with CBM concepts. Overall, our work unifies supervised and unsupervised concept discovery through a shared geometric framework, providing principled metrics to measure SAE progress and assess how well discovered concept align with plausible human concepts.

</details>


### [179] [LocalSearchBench: Benchmarking Agentic Search in Real-World Local Life Services](https://arxiv.org/abs/2512.07436)
*Hang He,Chuhuai Yue,Chengqi Dong,Mingxue Tian,Zhenfeng Liu,Jiajun Chai,Xiaohan Wang,Yufei Zhang,Qun Liao,Guojun Yin,Wei Lin,Chengcheng Wan,Haiying Sun,Ting Su*

Main category: cs.AI

TL;DR: LocalSearchBench：首个针对本地生活服务的智能搜索基准，包含30万高质量条目和300个多跳QA任务，揭示当前大模型在该领域表现不佳（最佳模型正确率仅34.34%）


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型研究多关注通用信息检索，很少探索具有独特挑战的垂直领域。本地生活服务领域的查询往往模糊且需要跨商家和产品的多跳推理，现有方法难以充分解决

Method: 构建LocalSearchBench基准，包含来自不同城市和业务类型的15万高质量条目；基于真实用户查询构建300个多跳QA任务；开发LocalPlayground统一环境，集成多种工具供智能体交互

Result: 即使最先进的大模型在LocalSearchBench上也表现不佳：最佳模型（DeepSeek-V3.1）正确率仅34.34%；大多数模型在完整性（平均77.33%）和忠实性（平均61.99%）方面存在问题

Conclusion: 本地生活服务领域需要专门的基准测试和领域特定的智能体训练，当前大模型在该领域仍有很大提升空间

Abstract: Recent advances in large reasoning models (LRMs) have enabled agentic search systems to perform complex multi-step reasoning across multiple sources. However, most studies focus on general information retrieval and rarely explores vertical domains with unique challenges. In this work, we focus on local life services and introduce LocalSearchBench, which encompass diverse and complex business scenarios. Real-world queries in this domain are often ambiguous and require multi-hop reasoning across merchants and products, remaining challenging and not fully addressed. As the first comprehensive benchmark for agentic search in local life services, LocalSearchBench includes over 150,000 high-quality entries from various cities and business types. We construct 300 multi-hop QA tasks based on real user queries, challenging agents to understand questions and retrieve information in multiple steps. We also developed LocalPlayground, a unified environment integrating multiple tools for agent interaction. Experiments show that even state-of-the-art LRMs struggle on LocalSearchBench: the best model (DeepSeek-V3.1) achieves only 34.34% correctness, and most models have issues with completeness (average 77.33%) and faithfulness (average 61.99%). This highlights the need for specialized benchmarks and domain-specific agent training in local life services. Code, Benchmark, and Leaderboard are available at localsearchbench.github.io.

</details>


### [180] [How Do LLMs Fail In Agentic Scenarios? A Qualitative Analysis of Success and Failure Scenarios of Various LLMs in Agentic Simulations](https://arxiv.org/abs/2512.07497)
*JV Roig*

Main category: cs.AI

TL;DR: 研究大型语言模型作为自主工具使用代理时的失败模式，通过KAMI基准测试分析三个代表性模型，发现模型规模并非代理可靠性的唯一决定因素，识别出四种常见失败模式。


<details>
  <summary>Details</summary>
Motivation: 当前对大型语言模型作为自主代理的研究多关注聚合性能指标，缺乏对具体失败模式的细粒度分析。本研究旨在深入理解LLM在工具使用场景中的失败机制，为提升代理可靠性提供洞见。

Method: 使用KAMI v0.1基准测试，分析900个执行轨迹，涵盖三个代表性模型（Granite 4 Small、Llama 4 Maverick、DeepSeek V3.1）在文件系统、文本提取、CSV分析和SQL场景中的表现。采用细粒度、逐试验的行为分析方法，而非关注聚合分数。

Result: 1. 模型规模并非代理鲁棒性的唯一预测因素：400B的Llama 4 Maverick在不确定性任务中仅比32B的Granite 4 Small略好；2. DeepSeek V3.1的优越可靠性主要源于后训练强化学习而非架构或规模；3. 识别出四种常见失败模式：缺乏基础的过早行动、替代缺失实体的过度帮助、干扰诱导的上下文污染、负载下的脆弱执行。

Conclusion: 可靠的代理部署不仅需要更强的模型，更需要强调交互基础、恢复行为和环境感知适应的评估方法，以及强化验证、约束发现和遵循真实数据源的有意训练和设计选择。

Abstract: We investigate how large language models (LLMs) fail when operating as autonomous agents with tool-use capabilities. Using the Kamiwaza Agentic Merit Index (KAMI) v0.1 benchmark, we analyze 900 execution traces from three representative models - Granite 4 Small, Llama 4 Maverick, and DeepSeek V3.1 - across filesystem, text extraction, CSV analysis, and SQL scenarios. Rather than focusing on aggregate scores, we perform fine-grained, per-trial behavioral analysis to surface the strategies that enable successful multi-step tool execution and the recurrent failure modes that undermine reliability. Our findings show that model scale alone does not predict agentic robustness: Llama 4 Maverick (400B) performs only marginally better than Granite 4 Small (32B) in some uncertainty-driven tasks, while DeepSeek V3.1's superior reliability derives primarily from post-training reinforcement learning rather than architecture or size. Across models, we identify four recurring failure archetypes: premature action without grounding, over-helpfulness that substitutes missing entities, vulnerability to distractor-induced context pollution, and fragile execution under load. These patterns highlight the need for agentic evaluation methods that emphasize interactive grounding, recovery behavior, and environment-aware adaptation, suggesting that reliable enterprise deployment requires not just stronger models but deliberate training and design choices that reinforce verification, constraint discovery, and adherence to source-of-truth data.

</details>


### [181] [Comparative Analysis and Parametric Tuning of PPO, GRPO, and DAPO for LLM Reasoning Enhancement](https://arxiv.org/abs/2512.07611)
*Yongsheng Lian*

Main category: cs.AI

TL;DR: 系统比较了三种强化学习算法（PPO、GRPO、DAPO）在提升大语言模型复杂推理能力上的效果，通过控制性迁移学习评估发现RL训练模型在所有任务上都优于基础模型，但改进程度因基准而异。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索不同强化学习算法如何有效提升大语言模型的复杂推理能力，为RL-based LLM训练提供实用的指导原则。

Method: 采用控制性迁移学习评估方法：首先在专门的Countdown Game上对模型进行微调，然后在通用推理基准测试套件上进行评估。比较了PPO、GRPO和DAPO三种RL算法，并分析了组大小、KL惩罚系数等参数的影响。

Result: 所有RL训练模型在各项任务上都优于对应的基础模型，但改进程度因基准而异。增加GRPO和DAPO中的组大小能带来更稳定的训练动态和更高准确率，KL惩罚系数的影响是非单调的。DAPO中的动态采样组件并未提升性能，禁用DS时DAPO取得最佳整体结果。

Conclusion: 强化学习能有效提升大语言模型的推理能力，但算法选择和参数设置对性能有重要影响。GRPO和DAPO中的组大小是关键参数，而DAPO的动态采样组件在实际应用中可能不是必要的。

Abstract: This study presents a systematic comparison of three Reinforcement Learning (RL) algorithms (PPO, GRPO, and DAPO) for improving complex reasoning in large language models (LLMs). Our main contribution is a controlled transfer-learning evaluation: models are first fine-tuned on the specialized Countdown Game and then assessed on a suite of general-purpose reasoning benchmarks. Across all tasks, RL-trained models outperform their corresponding base models, although the degree of improvement differs by benchmark.
  Our parametric analysis offers practical guidance for RL-based LLM training. Increasing the group size in GRPO and DAPO leads to more stable training dynamics and higher accuracy, while the impact of the KL-penalty coefficient is non-monotonic. Additionally, we find that the Dynamic Sampling (DS) component in DAPO does not improve performance; in fact, the best overall results are achieved with DAPO when DS is disabled.

</details>


### [182] [The Agent Capability Problem: Predicting Solvability Through Information-Theoretic Bounds](https://arxiv.org/abs/2512.07631)
*Shahar Lutati*

Main category: cs.AI

TL;DR: 提出Agent Capability Problem (ACP)框架，通过信息获取视角预测智能体在资源约束下能否解决问题，用信息量度量问题难度，推导出有效成本公式来预测资源需求。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖经验启发式，缺乏理论指导。需要一种理论框架来预测智能体在有限资源下能否完成任务，帮助智能体做出更好的资源分配决策。

Method: 将问题解决视为信息获取过程：智能体需要I_total比特信息来识别解决方案，每步行动获得I_step比特信息，成本为C_step。推导出有效成本C_eff = (I_total/I_step) * C_step来预测资源需求。提供概率上下界证明。

Result: 实验验证显示ACP预测与实际智能体性能高度一致，能有效约束搜索努力，相比贪婪和随机策略提高效率。框架适用于LLM和智能体工作流。

Conclusion: ACP提供了一个统一的信息论视角，连接主动学习、贝叶斯优化和强化学习等原理，为智能体资源分配提供理论指导。

Abstract: When should an autonomous agent commit resources to a task? We introduce the Agent Capability Problem (ACP), a framework for predicting whether an agent can solve a problem under resource constraints. Rather than relying on empirical heuristics, ACP frames problem-solving as information acquisition: an agent requires $\Itotal$ bits to identify a solution and gains $\Istep$ bits per action at cost $\Cstep$, yielding an effective cost $\Ceff = (\Itotal/\Istep), \Cstep$ that predicts resource requirements before search. We prove that $\Ceff$ lower-bounds expected cost and provide tight probabilistic upper bounds. Experimental validation shows that ACP predictions closely track actual agent performance, consistently bounding search effort while improving efficiency over greedy and random strategies. The framework generalizes across LLM-based and agentic workflows, linking principles from active learning, Bayesian optimization, and reinforcement learning through a unified information-theoretic lens. \

</details>


### [183] [Each Prompt Matters: Scaling Reinforcement Learning Without Wasting Rollouts on Hundred-Billion-Scale MoE](https://arxiv.org/abs/2512.07710)
*Anxiang Zeng,Haibo Zhang,Hailing Zhang,Kaixiang Mo,Liang Yao,Ling Hu,Long Zhang,Shuman Liu,Shuyi Xie,Yanshi Li,Yizhang Chen,Yuepeng Sheng,Yuwei Huang,Zhaochen Xu,Zhiqiang Zhou,Ziqin Liew*

Main category: cs.AI

TL;DR: CompassMax-V3-Thinking是一个千亿规模的MoE推理模型，采用新的RL框架训练，核心原则是"每个提示都必须有意义"。通过多项技术创新解决了大规模RL训练中的效率问题，实现了稳定高效的训练。


<details>
  <summary>Details</summary>
Motivation: 将RL扩展到千亿规模时暴露出关键效率问题：零方差提示浪费rollout资源、长视野重要性采样不稳定、标准奖励模型导致优势反转、rollout处理存在系统性瓶颈。需要解决这些挑战以实现大规模MoE模型的稳定高效RL训练。

Method: 提出四项统一创新：1) 多阶段零方差消除，过滤非信息性提示并稳定基于组的策略优化；2) ESPO熵自适应优化方法，平衡token级和序列级重要性采样；3) 路由器重放策略，对齐训练时MoE路由器决策与推理时行为；4) 高吞吐RL系统，采用FP8精度rollout、重叠奖励计算和长度感知调度。

Result: 构建了一个使千亿规模MoE模型RL训练稳定高效的完整流水线。得到的模型在内部和公共评估中都表现出色。

Conclusion: 通过系统性地解决大规模RL训练中的效率瓶颈，成功实现了千亿规模MoE推理模型的稳定高效训练，为大规模语言模型的RL优化提供了可行的技术方案。

Abstract: We present CompassMax-V3-Thinking, a hundred-billion-scale MoE reasoning model trained with a new RL framework built on one principle: each prompt must matter. Scaling RL to this size exposes critical inefficiencies-zero-variance prompts that waste rollouts, unstable importance sampling over long horizons, advantage inversion from standard reward models, and systemic bottlenecks in rollout processing. To overcome these challenges, we introduce several unified innovations: (1) Multi-Stage Zero-Variance Elimination, which filters out non-informative prompts and stabilizes group-based policy optimization (e.g. GRPO) by removing wasted rollouts; (2) ESPO, an entropy-adaptive optimization method that balances token-level and sequence-level importance sampling to maintain stable learning dynamics; (3) a Router Replay strategy that aligns training-time MoE router decisions with inference-time behavior to mitigate train-infer discrepancies, coupled with a reward model adjustment to prevent advantage inversion; (4) a high-throughput RL system with FP8-precision rollouts, overlapped reward computation, and length-aware scheduling to eliminate performance bottlenecks. Together, these contributions form a cohesive pipeline that makes RL on hundred-billion-scale MoE models stable and efficient. The resulting model delivers strong performance across both internal and public evaluations.

</details>


### [184] [RL-MTJail: Reinforcement Learning for Automated Black-Box Multi-Turn Jailbreaking of Large Language Models](https://arxiv.org/abs/2512.07761)
*Xiqiao Xiong,Ouxiang Li,Zhuo Liu,Moxin Li,Wentao Shi,Fuli Feng,Xiangnan He*

Main category: cs.AI

TL;DR: 该论文提出了一种基于强化学习的多轮越狱攻击方法，通过优化最终输出的有害性作为结果奖励，并引入两个启发式过程奖励来提升攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型容易受到越狱攻击，威胁其在现实应用中的安全部署。现有方法通常依赖单轮优化，不足以学习长期攻击策略。

Method: 将问题建模为多轮强化学习任务，直接优化最终轮输出的有害性作为结果奖励。引入两个启发式过程奖励：1) 控制中间输出的有害性以避免触发黑盒模型的拒绝机制；2) 保持中间输出的语义相关性以避免内容漂移。

Result: 在多个基准测试上的实验结果表明，该方法在多个模型上持续提高了攻击成功率，证明了方法的有效性。

Conclusion: 提出的强化学习方法能有效提升多轮越狱攻击的成功率，为黑盒模型的安全部署提供了重要参考。

Abstract: Large language models are vulnerable to jailbreak attacks, threatening their safe deployment in real-world applications. This paper studies black-box multi-turn jailbreaks, aiming to train attacker LLMs to elicit harmful content from black-box models through a sequence of prompt-output interactions. Existing approaches typically rely on single turn optimization, which is insufficient for learning long-term attack strategies. To bridge this gap, we formulate the problem as a multi-turn reinforcement learning task, directly optimizing the harmfulness of the final-turn output as the outcome reward. To mitigate sparse supervision and promote long-term attack strategies, we propose two heuristic process rewards: (1) controlling the harmfulness of intermediate outputs to prevent triggering the black-box model's rejection mechanisms, and (2) maintaining the semantic relevance of intermediate outputs to avoid drifting into irrelevant content. Experimental results on multiple benchmarks show consistently improved attack success rates across multiple models, highlighting the effectiveness of our approach. The code is available at https://github.com/xxiqiao/RL-MTJail. Warning: This paper contains examples of harmful content.

</details>


### [185] [ReasonBENCH: Benchmarking the (In)Stability of LLM Reasoning](https://arxiv.org/abs/2512.07795)
*Nearchos Potamitis,Lars Klein,Akhil Arora*

Main category: cs.AI

TL;DR: ReasonBENCH：首个量化LLM推理不稳定性的基准，通过多轮评估协议提供统计可靠的质量和成本指标，揭示当前推理方法普遍存在高不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理评估主要报告单次运行准确率，忽略了随机解码带来的内在不确定性，导致无法可靠评估方法的稳定性、可重复性和成本一致性，存在评估盲点。

Method: 提出ReasonBENCH基准，包含：(1)标准化推理框架、模型和任务的模块化评估库；(2)报告统计可靠质量和成本指标的多轮评估协议；(3)鼓励方差感知报告的公开排行榜。

Result: 跨领域任务分析发现，绝大多数推理策略和模型表现出高不稳定性。即使平均性能相似的策略，置信区间宽度差异可达4倍，且性能最佳的方法通常成本更高且更不稳定。

Conclusion: 推理不稳定性损害了跨运行的可重复性，进而影响报告性能的可靠性。可重复性是可靠LLM推理的关键维度，ReasonBENCH为未来推理方法和不确定性量化技术奠定了基础。

Abstract: Large language models (LLMs) are increasingly deployed in settings where reasoning, such as multi-step problem solving and chain-of-thought, is essential. Yet, current evaluation practices overwhelmingly report single-run accuracy while ignoring the intrinsic uncertainty that naturally arises from stochastic decoding. This omission creates a blind spot because practitioners cannot reliably assess whether a method's reported performance is stable, reproducible, or cost-consistent. We introduce ReasonBENCH, the first benchmark designed to quantify the underlying instability in LLM reasoning. ReasonBENCH provides (i) a modular evaluation library that standardizes reasoning frameworks, models, and tasks, (ii) a multi-run protocol that reports statistically reliable metrics for both quality and cost, and (iii) a public leaderboard to encourage variance-aware reporting. Across tasks from different domains, we find that the vast majority of reasoning strategies and models exhibit high instability. Notably, even strategies with similar average performance can display confidence intervals up to four times wider, and the top-performing methods often incur higher and less stable costs. Such instability compromises reproducibility across runs and, consequently, the reliability of reported performance. To better understand these dynamics, we further analyze the impact of prompts, model families, and scale on the trade-off between solve rate and stability. Our results highlight reproducibility as a critical dimension for reliable LLM reasoning and provide a foundation for future reasoning methods and uncertainty quantification techniques. ReasonBENCH is publicly available at https://github.com/au-clan/ReasonBench .

</details>


### [186] [Large Causal Models from Large Language Models](https://arxiv.org/abs/2512.07796)
*Sridhar Mahadevan*

Main category: cs.AI

TL;DR: DEMOCRITUS是一个利用大语言模型构建大规模因果模型的新范式，通过提取、组织和可视化跨领域因果知识，将分散的因果陈述整合为统一的因果图。


<details>
  <summary>Details</summary>
Motivation: 传统因果推断方法局限于特定领域和假设驱动，依赖实验产生的数值数据。本文旨在利用LLMs的巨大潜力，构建跨越不同领域的大规模因果模型，从文本中提取和组织广泛的因果知识。

Method: 使用高质量LLM提出主题、生成因果问题、从多领域提取因果陈述，然后将这些分散的因果主张转化为关系三元组，嵌入大规模因果模型。系统包含六个模块，采用新的范畴机器学习方法整合碎片化因果知识。

Result: DEMOCRITUS在考古学、生物学、气候变化、经济学、医学和技术等多个领域成功应用，能够构建跨领域的大规模因果模型。分析了系统的计算成本特征，确定了当前扩展系统的瓶颈。

Conclusion: DEMOCRITUS展示了利用LLMs构建大规模因果模型的可行性，为跨领域因果知识整合提供了新方法。讨论了当前系统的局限性，并提出了扩展能力的未来方向。

Abstract: We introduce a new paradigm for building large causal models (LCMs) that exploits the enormous potential latent in today's large language models (LLMs). We describe our ongoing experiments with an implemented system called DEMOCRITUS (Decentralized Extraction of Manifold Ontologies of Causal Relations Integrating Topos Universal Slices) aimed at building, organizing, and visualizing LCMs that span disparate domains extracted from carefully targeted textual queries to LLMs. DEMOCRITUS is methodologically distinct from traditional narrow domain and hypothesis centered causal inference that builds causal models from experiments that produce numerical data. A high-quality LLM is used to propose topics, generate causal questions, and extract plausible causal statements from a diverse range of domains. The technical challenge is then to take these isolated, fragmented, potentially ambiguous and possibly conflicting causal claims, and weave them into a coherent whole, converting them into relational causal triples and embedding them into a LCM. Addressing this technical challenge required inventing new categorical machine learning methods, which we can only briefly summarize in this paper, as it is focused more on the systems side of building DEMOCRITUS. We describe the implementation pipeline for DEMOCRITUS comprising of six modules, examine its computational cost profile to determine where the current bottlenecks in scaling the system to larger models. We describe the results of using DEMOCRITUS over a wide range of domains, spanning archaeology, biology, climate change, economics, medicine and technology. We discuss the limitations of the current DEMOCRITUS system, and outline directions for extending its capabilities.

</details>


### [187] [Auditing Games for Sandbagging](https://arxiv.org/abs/2512.07810)
*Jordan Taylor,Sid Black,Dillon Bowen,Thomas Read,Satvik Golechha,Alex Zelenka-Martin,Oliver Makins,Connor Kissane,Kola Ayonrinde,Jacob Merizian,Samuel Marks,Chris Cundy,Joseph Bloom*

Main category: cs.AI

TL;DR: 论文通过审计游戏测试了检测AI系统"沙袋化"（隐藏能力）的方法，发现现有检测技术不可靠，但基于训练的激发方法能有效引出隐藏能力，但存在误报风险。


<details>
  <summary>Details</summary>
Motivation: 未来AI系统可能在评估中隐藏能力（沙袋化），误导开发者和审计者，需要测试现有检测技术的有效性。

Method: 使用审计游戏：红队微调五个模型（部分有条件地表现不佳作为沙袋化代理），蓝队使用黑盒、模型内部或基于训练的方法识别沙袋化模型。

Result: 蓝队无法可靠区分沙袋化模型和良性模型。黑盒方法被有效模仿较弱模型的行为击败。线性探测（模型内部方法）更有希望但易受红队植入行为影响。基于训练的激发方法能持续引出沙袋化模型的全部性能，但也会提升良性模型性能，导致误报。

Conclusion: 短期建议开发者使用分布内训练移除潜在沙袋化；长期需要进一步研究确保基于训练的激发方法有效性，并开发鲁棒的沙袋化检测方法。

Abstract: Future AI systems could conceal their capabilities ('sandbagging') during evaluations, potentially misleading developers and auditors. We stress-tested sandbagging detection techniques using an auditing game. First, a red team fine-tuned five models, some of which conditionally underperformed, as a proxy for sandbagging. Second, a blue team used black-box, model-internals, or training-based approaches to identify sandbagging models. We found that the blue team could not reliably discriminate sandbaggers from benign models. Black-box approaches were defeated by effective imitation of a weaker model. Linear probes, a model-internals approach, showed more promise but their naive application was vulnerable to behaviours instilled by the red team. We also explored capability elicitation as a strategy for detecting sandbagging. Although Prompt-based elicitation was not reliable, training-based elicitation consistently elicited full performance from the sandbagging models, using only a single correct demonstration of the evaluation task. However the performance of benign models was sometimes also raised, so relying on elicitation as a detection strategy was prone to false-positives. In the short-term, we recommend developers remove potential sandbagging using on-distribution training for elicitation. In the longer-term, further research is needed to ensure the efficacy of training-based elicitation, and develop robust methods for sandbagging detection. We open source our model organisms at https://github.com/AI-Safety-Institute/sandbagging_auditing_games and select transcripts and results at https://huggingface.co/datasets/sandbagging-games/evaluation_logs . A demo illustrating the game can be played at https://sandbagging-demo.far.ai/ .

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [188] [The Tragedy of Productivity: A Unified Framework for Diagnosing Coordination Failures in Labor Markets and AI Governance](https://arxiv.org/abs/2512.05995)
*Ali Dasdan*

Main category: cs.CY

TL;DR: 论文提出了"结构性悲剧"框架，分析为何生产率提升未缩短工时、AI发展不顾风险警告，揭示两者具有相同博弈结构，并量化AI治理比气候变化更难协调。


<details>
  <summary>Details</summary>
Motivation: 尽管生产率自凯恩斯时代已提升8倍，但全球工时仍约为其预测的2倍；同时AI发展不顾存在风险警告加速推进。作者发现这两个看似不同的问题具有相同的博弈论结构，需要建立统一框架来分析此类"结构性悲剧"。

Method: 提出结构性悲剧的五个必要充分条件：N参与者结构、具有负外部性的二元选择、背叛占优策略、帕累托无效性（合作优于相互背叛）、以及结构性障碍导致的执行困难。通过条件强度分析扩展框架，引入悲剧指数量化协调难度，并应用于生产率竞争和AI治理案例验证。

Result: 验证框架适用于经典案例，发现AI治理的协调难度比气候变化或核武器高出数量级。证明企业面临协调失败，生产率提升无法转化为工人福利（欧洲证据支持）。俄乌无人机战争验证了AI治理的结构性悲剧：双方在两年内从零激增至每月数千架无人机，尽管先前有治理对话。

Conclusion: 分析是诊断性的而非处方性的，识别协调的结构性障碍而非提出解决方案。揭示了生产率-福利脱钩和AI治理困境共享相同的博弈结构，但AI治理在八个维度上强度更大，协调难度极高。

Abstract: Despite productivity increasing eightfold since Keynes's 1930
  prediction of 15-hour workweeks, workers globally still work roughly
  double these hours. Separately, AI development accelerates despite
  existential risk warnings from leading researchers. We demonstrate
  these failures share identical game-theoretic structure.
  We synthesize five necessary and sufficient conditions
  characterizing structural tragedies: N-player structure,
  binary choices with negative externalities, dominance where
  defection yields higher payoffs, Pareto-inefficiency where
  cooperation dominates mutual defection, and enforcement difficulty
  from structural barriers. We validate this framework across canonical
  cases and extend it through condition intensities, introducing a
  Tragedy Index revealing AI governance faces orders-of-magnitude
  greater coordination difficulty than climate change or nuclear
  weapons.
  Applied to productivity competition, we prove firms face
  coordination failure preventing productivity gains from translating
  to worker welfare. European evidence shows that even under favorable
  conditions, productivity-welfare decoupling persists. Applied to AI
  governance, we demonstrate development faces the same structure but
  with amplified intensity across eight dimensions compared to
  successful arms control. The Russia-Ukraine drone war validates this:
  both sides escalated from zero to thousands of drones monthly within
  two years despite prior governance dialogue.
  The analysis is diagnostic rather than prescriptive, identifying
  structural barriers to coordination rather than proposing solutions.

</details>


### [189] [Uncovering Students' Inquiry Patterns in GenAI-Supported Clinical Practice: An Integration of Epistemic Network Analysis and Sequential Pattern Mining](https://arxiv.org/abs/2512.06018)
*Jiameng Wei,Dinh Dang,Kaixun Yang,Emily Stokes,Amna Mazeh,Angelina Lim,David Wei Dai,Joel Moore,Yizhou Fan,Danijela Gasevic,Dragan Gasevic,Guanliang Chen*

Main category: cs.CY

TL;DR: 本研究应用学习分析技术，分析药学学生在GenAI虚拟病人训练中的临床沟通能力发展模式，发现高表现者具有策略性信息识别行为，而低表现者陷入常规问答循环。


<details>
  <summary>Details</summary>
Motivation: 传统药物史采集评估依赖人工观察，难以规模化且缺乏详细性能数据。GenAI平台虽能收集大量数据，学习分析也能分析教育痕迹，但这些方法在药学临床培训中应用不足。本研究旨在填补这一空白，特别是在学生群体多样化、语言背景各异、传统培训个性化反馈机会有限的情况下。

Method: 分析澳大利亚和马来西亚323名学生的交互日志，包含50,871条编码话语和1,487个学生-GenAI对话。结合认知网络分析（ENA）建模询问共现模式，以及序列模式挖掘（SPM）捕捉时间序列，研究学生临床沟通能力发展。

Result: 高表现者展现出策略性的信息识别行为，其询问围绕临床相关信息识别展开，并整合关系建立和结构组织；低表现者则陷入常规问题验证循环。人口因素（母语背景、药房工作经验、机构背景）也塑造了不同的询问模式。

Conclusion: 研究揭示了GenAI辅助环境下可能指示临床推理发展的询问模式，为卫生专业教育评估提供了方法论见解，并为支持多样化学习路径的自适应GenAI系统设计提供了信息。

Abstract: Assessment of medication history-taking has traditionally relied on human observation, limiting scalability and detailed performance data. While Generative AI (GenAI) platforms enable extensive data collection and learning analytics provide powerful methods for analyzing educational traces, these approaches remain largely underexplored in pharmacy clinical training. This study addresses this gap by applying learning analytics to understand how students develop clinical communication competencies with GenAI-powered virtual patients -- a crucial endeavor given the diversity of student cohorts, varying language backgrounds, and the limited opportunities for individualized feedback in traditional training settings. We analyzed 323 students' interaction logs across Australian and Malaysian institutions, comprising 50,871 coded utterances from 1,487 student-GenAI dialogues. Combining Epistemic Network Analysis to model inquiry co-occurrences with Sequential Pattern Mining to capture temporal sequences, we found that high performers demonstrated strategic deployment of information recognition behaviors. Specifically, high performers centered inquiry on recognizing clinically relevant information, integrating rapport-building and structural organization, while low performers remained in routine question-verification loops. Demographic factors including first-language background, prior pharmacy work experience, and institutional context, also shaped distinct inquiry patterns. These findings reveal inquiry patterns that may indicate clinical reasoning development in GenAI-assisted contexts, providing methodological insights for health professions education assessment and informing adaptive GenAI system design that supports diverse learning pathways.

</details>


### [190] [Protocol Futuring: Speculating Second-Order Dynamics of Protocols in Sociotechnical Infrastructural Futures](https://arxiv.org/abs/2512.06108)
*Botao 'Amber' Hu,Samuel Chua,Helena Rong*

Main category: cs.CY

TL;DR: 论文提出Protocol Futuring方法框架，通过关注协议规则在长期时间尺度上的演变来探索未来社会技术系统。


<details>
  <summary>Details</summary>
Motivation: 现有设计未来方法主要关注离散的未来物品，缺乏对协议规则长期演变及其二阶效应的系统性分析。需要一种方法来研究基础设施政治和长期影响。

Method: 提出Protocol Futuring方法框架，以协议（规则、标准、协调机制）为主要分析材料。通过Knowledge Futurama案例研究，采用多团队参与式工作坊和接力格式，让团队继承和重新解释部分形成的设计。

Result: 工作坊揭示了协议在跨社区和时代传播时的演变机制：模糊交接、对抗性重新解释、文化规范变化和危机动态。该方法使基础设施政治和长期后果变得分析可见。

Conclusion: Protocol Futuring为研究长期时间尺度上的新兴社会技术系统提供了有效方法，能够揭示协议规则在长期演变中的政治影响和二阶效应。

Abstract: Drawing on infrastructure studies in HCI and CSCW, this paper introduces Protocol Futuring, a methodological framework that extends design futuring by foregrounding protocols-rules, standards, and coordination mechanisms-as the primary material of speculative inquiry. Rather than imagining discrete future artifacts, Protocol Futuring examines how protocol rules accumulate drift, jam, and other second-order effects over long temporal horizons. We demonstrate the method through a case study of Knowledge Futurama, a multi-team participatory workshop exploring millennial-scale knowledge preservation. Using a relay format in which teams inherited and reinterpreted partially formed designs, the workshop revealed how ambiguous handovers, adversarial reinterpretations, shifting cultural norms, and crisis dynamics transform protocols as they move across communities and epochs. The case shows how Protocol Futuring makes infrastructural politics and long-run consequences analytically visible. We discuss the method's strengths, limitations, and implications for researchers seeking to investigate emergent sociotechnical systems whose impacts unfold over extended timescales.

</details>


### [191] [The Role of Smart Cities in Ethical Design Framework](https://arxiv.org/abs/2512.06336)
*Yijun Chen*

Main category: cs.CY

TL;DR: 本文探讨智慧城市建设中的伦理挑战，包括数据隐私、公平性、包容性和透明度等问题，并提出通过监管沙盒、参与式治理等方案促进伦理城市发展。


<details>
  <summary>Details</summary>
Motivation: 随着数字技术融入城市规划形成"智慧城市"，旨在提升生活质量和运营效率，但技术实施带来了数据隐私、公平性、包容性和透明度等伦理挑战，需要系统分析并解决。

Method: 采用Beard和Longstaff分析框架，结合理论分析和案例研究，聚焦自决、公平、可及性和目的性原则，考察治理模式、利益相关者角色和智慧城市中的伦理困境。

Result: 研究识别了智慧城市实施中的关键伦理挑战，并提出了具体建议：采用监管沙盒、促进参与式治理、弥合数字鸿沟，确保智慧城市符合社会价值观。

Conclusion: 智慧城市发展必须与伦理原则保持一致，通过适当的治理框架和包容性策略，才能实现真正的城市发展目标，促进包容性和伦理城市发展。

Abstract: The integration of digital technologies into urban planning has given rise to "smart cities," aiming to enhance quality of life and operational efficiency. However, the implementation of such technologies introduces ethical challenges, including data privacy, equity, inclusion, and transparency. This article employs the Beard and Longstaff framework to discuss these challenges through a combination of theoretical analysis and case studies. Focusing on principles of self-determination, fairness, accessibility, and purpose, the study examines governance models, stakeholder roles, and ethical dilemmas inherent in smart city initiatives. Recommendations include adopting regulatory sandboxes, fostering participatory governance, and bridging digital divides to ensure that smart cities align with societal values, promoting inclusivity and ethical urban development.

</details>


### [192] [Why They Disagree: Decoding Differences in Opinions about AI Risk on the Lex Fridman Podcast](https://arxiv.org/abs/2512.06350)
*Nghi Truong,Phanish Puranam,Özgecan Koçak*

Main category: cs.CY

TL;DR: 该论文分析AI风险辩论中的"末日论者"与"繁荣论者"观点分歧，发现分歧主要源于对复杂系统设计vs涌现、以及过去理论适用性vs革命性的不同因果前提假设，而非道德价值观差异。


<details>
  <summary>Details</summary>
Motivation: AI等变革性技术引发了深刻的社会分歧，尽管各方都希望AI造福人类并避免灾难性后果，但分歧依然存在。论文旨在系统分析AI风险辩论中的核心分歧点，理解为何共享目标下仍存在深刻对立。

Method: 将"末日论者"与"繁荣论者"的观点分歧分解为定义性、事实性、因果性和道德性前提；使用LLM集成方法大规模分析推理链条，解析文本数据以识别关键争议点。

Result: 发现关于存在性风险(X-risk)的分歧源于对复杂系统中设计vs涌现的不同因果前提；关于就业风险(E-risks)的分歧源于对过去理论(进化)适用性vs不适用性(革命)的不同因果前提。两种分歧都不涉及重大道德价值观差异，且都可描述为对人类理性有限性的不同看法。

Conclusion: 论文提出的使用LLM集成分析推理链条的方法可应用于任何领域的公共风险辩论，帮助识别关键争议点。AI风险辩论的分歧更多是认知框架差异而非价值观冲突，这为更建设性的对话提供了基础。

Abstract: The emergence of transformative technologies often surfaces deep societal divisions, nowhere more evident than in contemporary debates about artificial intelligence (AI). A striking feature of these divisions is that they persist despite shared interests in ensuring that AI benefits humanity and avoiding catastrophic outcomes. This paper analyzes contemporary debates about AI risk, parsing the differences between the "doomer" and "boomer" perspectives into definitional, factual, causal, and moral premises to identify key points of contention. We find that differences in perspectives about existential risk ("X-risk") arise fundamentally from differences in causal premises about design vs. emergence in complex systems, while differences in perspectives about employment risks ("E-risks") pertain to different causal premises about the applicability of past theories (evolution) vs their inapplicability (revolution). Disagreements about these two forms of AI risk appear to share two properties: neither involves significant disagreements on moral values and both can be described in terms of differing views on the extent of boundedness of human rationality. Our approach to analyzing reasoning chains at scale, using an ensemble of LLMs to parse textual data, can be applied to identify key points of contention in debates about risk to the public in any arena.

</details>


### [193] [The Missing Variable: Socio-Technical Alignment in Risk Evaluation](https://arxiv.org/abs/2512.06354)
*Niclas Flehmig,Mary Ann Lundteigen,Shen Yin*

Main category: cs.CY

TL;DR: 该论文提出了一种新的社会技术对齐变量STA，用于增强AI安全关键系统的风险评估，弥补现有方法忽视人机组织复杂交互的缺陷。


<details>
  <summary>Details</summary>
Motivation: 当前AI安全关键系统的风险评估方法存在重大缺陷，它们未能充分考虑人、技术和组织要素之间的复杂交互作用，而这些系统本质上是复杂的社会技术系统。

Method: 通过社会技术系统和AI系统属性的比较分析，以及对现有风险评估方法的审查，确认了标准风险表达式中社会技术考虑的缺失。为此提出了社会技术对齐变量STA，可集成到基础风险方程中，用于估计AI系统、人类操作者和组织流程之间的和谐交互程度。

Result: 在AI驱动的液氢加注系统案例研究中，通过比较原始设计和安全防护设计，证明了STA增强表达式能够捕捉传统风险评估忽视的社会技术安全影响，为风险评估提供了更全面的基础。

Conclusion: STA变量的引入填补了AI安全关键系统风险评估的重要空白，通过量化社会技术对齐程度，使风险评估能够更全面地考虑人机组织交互的复杂性，从而提高安全关键系统的整体安全性评估。

Abstract: This paper addresses a critical gap in the risk assessment of AI-enabled safety-critical systems. While these systems, where AI systems assists human operators, function as complex socio-technical systems, existing risk evaluation methods fail to account for the associated complex interaction between human, technical, and organizational elements. Through a comparative analysis of system attributes from both socio-technical and AI-enabled systems and a review of current risk evaluation methods, we confirm the absence of socio-technical considerations in standard risk expressions. To bridge this gap, we introduce a novel socio-technical alignment $STA$ variable designed to be integrated into the foundational risk equation. This variable estimates the degree of harmonious interaction between the AI systems, human operators, and organizational processes. A case study on an AI-enabled liquid hydrogen bunkering system demonstrates the variable's relevance. By comparing a naive and a safeguarded system design, we illustrate how the $STA$-augmented expression captures socio-technical safety implications that traditional risk evaluation overlooks, providing a more holistic basis for risk evaluation.

</details>


### [194] [Code vs. Context: STEM Students' Resistance to Non-STEM Coursework](https://arxiv.org/abs/2512.06529)
*Md Abdullah Al Kafi,Raka Moni,Sumit Kumar Banshal*

Main category: cs.CY

TL;DR: 研究发现工程学生对非技术课程的抵触主要源于角色模糊（专业身份冲突），而非认知转换成本或工作负荷；减少角色模糊可提升参与意愿和技能长期应用。


<details>
  <summary>Details</summary>
Motivation: STEM项目要求学生修读非技术课程以培养软技能，但工程学生常对此抵触。先前研究多归因于课业负担，但对认知和身份相关机制了解不足。本研究旨在填补这一知识空白。

Method: 收集212名计算机科学与工程本科生的调查数据，使用顺序OLS回归测试认知转换成本、工作负荷、角色模糊对情感抵触的影响，以及后续对参与意愿和技能长期应用的影响。

Result: 角色模糊是情感抵触的最强预测因子（β=0.47），超过工作负荷（β=0.20）和认知转换成本（β=0.14）。情感抵触显著降低参与意愿（β=-0.25），而参与意愿强烈预测技能长期应用（β=0.55）。

Conclusion: 学生抵触主要源于非技术内容与专业身份的不一致，而非认知努力或工作负荷。课程设计应通过将人文社科材料置于明确的工程情境中来减少角色模糊，从而改善学习效果。

Abstract: Many STEM programs now require students to take non-technical courses to develop the soft skills necessary for professional practice, yet engineering students frequently resist this requirement. While prior research often attributes this resistance to heavy workloads, little is known about its cognitive and identity-related mechanisms. This study fills this knowledge gap by examining the effects of Cognitive Switching Costs, Work Overload, and Role Ambiguity on students' Affective Resistance to non-STEM coursework, as well as the subsequent impact on their Willingness to Engage and Long-Term Adoption of skills. We collected survey data from 212 undergraduate Computer Science and Engineering students and tested directional relationships using sequential OLS regression. Role Ambiguity emerged as the strongest predictor of Affective Resistance (beta of 0.47, p less than 0.001), exceeding the effects of Work Overload (beta of 0.20, p equals 0.007) and Cognitive Switching Cost (beta of 0.14, p equals 0.038). In turn, Affective Resistance significantly reduced Willingness to Engage (beta of -0.25, p less than 0.001), while Willingness to Engage served as a strong predictor of Long-Term Adoption (beta of 0.55, p less than 0.001). These results indicate that student resistance is driven primarily by the incongruence between non-technical content and students' emergent professional identities, rather than by cognitive effort or workload alone. To improve outcomes, curricula should focus on reducing role ambiguity by placing humanities and social science material in clear engineering contexts.

</details>


### [195] [Generic visuality of war? How image-generative AI models (mis)represent Russia's war against Ukraine](https://arxiv.org/abs/2512.06570)
*Mykola Makhortykh,Miglė Bareikytė*

Main category: cs.CY

TL;DR: 该研究审计了美国Midjourney和俄罗斯Kandinsky两个图像生成AI模型在描绘俄乌战争时的表现差异，发现上下文因素导致战争表征存在变异性，但也存在可能导致战争美学同质化的模式。


<details>
  <summary>Details</summary>
Motivation: 生成式AI可能改变社会现实的表征方式，包括现代战争的描绘。虽然已有研究关注AI的军事应用，但生成式AI如何影响战争的呈现、记忆和解释尚不清楚。现有研究主要关注生成式AI可能扭曲大规模暴力表征的风险，特别是通过净化和同质化手段，但不同模型在不同暴力事件中的表征差异尚未被充分探索。

Method: 以俄罗斯对乌克兰的侵略为案例研究，审计美国Midjourney和俄罗斯Kandinsky两个图像生成模型。分析模型对战争相关提示的响应性，以及生成图像的美学和内容特征。比较模型在虚构和事实战争场景中的表现。

Result: 研究发现上下文因素导致战争表征存在变异性，既体现在不同模型之间，也体现在同一模型的输出内部。然而，存在一些一致的表征模式，这些模式可能促进战争美学的同质化。西方和非西方模型在战争表征上存在差异。

Conclusion: 生成式AI对战争的表征受到多种因素影响，包括模型来源国的文化背景。虽然存在变异性，但某些一致模式可能导致战争美学的同质化，这可能影响公众对战争的理解和记忆。需要进一步研究生成式AI如何塑造战争叙事。

Abstract: The rise of generative AI (genAI) can transform the representation of different aspects of social reality, including modern wars. While scholarship has largely focused on the military applications of AI, the growing adoption of genAI technologies may have major implications for how wars are portrayed, remembered, and interpreted. A few initial scholarly inquiries highlight the risks of genAI in this context, specifically regarding its potential to distort the representation of mass violence, particularly by sanitising and homogenising it. However, little is known about how genAI representation practices vary between different episodes of violence portrayed by Western and non-Western genAI models. Using the Russian aggression against Ukraine as a case study, we audit how two image-generative models, the US-based Midjourney and the Russia-based Kandinsky, represent both fictional and factual episodes of the war. We then analyse the models' responsiveness to the war-related prompts, together with the aesthetic and content-based aspects of the resulting images. Our findings highlight that contextual factors lead to variation in the representation of war, both between models and within the outputs of the same model. However, there are some consistent patterns of representation that may contribute to the homogenization of war aesthetics.

</details>


### [196] [When Does Regulation by Insurance Work? The Case of Frontier AI](https://arxiv.org/abs/2512.06597)
*Cristian Trout*

Main category: cs.CY

TL;DR: 本文提出了一个评估保险对风险监管效果的分析框架，认为在某些条件下保险可以产生净监管效应而非道德风险，并以AI行业为例展示了该框架的应用。


<details>
  <summary>Details</summary>
Motivation: 关于"监管性保险"（regulation by insurance）的争论持续存在，支持者和反对者都记录了保险成功或失败产生净监管效应的案例。本文旨在收集这些案例并基于经济学文献，建立一个原则性框架来评估保险在特定情境下的监管效果。

Method: 收集现有案例并借鉴广泛的经济学文献，开发一个分析框架。该框架首先识别可能产生净监管效应的扭曲因素（如责任限制、竞争动态、行为偏差），然后分析政策持有人类型、风险类型、保险公司类型和保险市场结构如何影响监管潜力的实现。

Result: 分析表明，对于灾难性的非产品事故，当市场机制提供不足的约束且心理偏差最强时，监管性保险可能特别有效。将该框架应用于前沿AI行业，显示出显著的净监管效应潜力，但也需要政策干预来实现这一潜力。

Conclusion: 监管性保险在某些条件下可以有效改善福利。一个可行的政策选择是精心设计的强制保险要求，鼓励形成专门的保险公司或互助组织，专注于灾难性而非常规风险，并禁止纯粹的专属保险公司。

Abstract: No one doubts the utility of insurance for its ability to spread risk or streamline claims management; much debated is when and how insurance uptake can improve welfare by reducing harm, despite moral hazard. Proponents and dissenters of "regulation by insurance" have now documented a number of cases of insurers succeeding or failing to have such a net regulatory effect (in contrast with a net hazard effect). Collecting these examples together and drawing on an extensive economics literature, this Article develops a principled framework for evaluating insurance uptake's effect in a given context. The presence of certain distortions - including judgment-proofness, competitive dynamics, and behavioral biases - creates potential for a net regulatory effect. How much of that potential gets realized then depends on the type of policyholder, type of risk, type of insurer, and the structure of the insurance market. The analysis suggests regulation by insurance can be particularly effective for catastrophic non-product accidents where market mechanisms provide insufficient discipline and psychological biases are strongest. As a demonstration, the framework is applied to the frontier AI industry, revealing significant potential for a net regulatory effect but also the need for policy intervention to realize that potential. One option is a carefully designed mandate that encourages forming a specialized insurer or mutual, focuses on catastrophic rather than routine risks, and bars pure captives.

</details>


### [197] [IyaCare: An Integrated AI-IoT-Blockchain Platform for Maternal Health in Resource-Constrained Settings](https://arxiv.org/abs/2512.07333)
*Oche D. Ankeli,Marvin M. Ogore*

Main category: cs.CY

TL;DR: IyaCare是一个集成AI、物联网和区块链的数字健康平台，专门为资源有限的撒哈拉以南非洲地区设计，用于改善孕产妇健康管理。


<details>
  <summary>Details</summary>
Motivation: 撒哈拉以南非洲地区孕产妇死亡率极高（占全球70%），而现有数字健康干预措施通常孤立部署AI、物联网和区块链技术，错失了协同增效的机会。

Method: 开发了基于Next.js前端、Firebase后端、以太坊区块链架构和XGBoost AI模型的集成平台，结合预测风险评估、连续生命体征监测和安全健康记录管理，特别设计了离线优先功能和基于SMS的社区医疗工作者通信。

Result: 可行性研究显示高风险妊娠预测准确率达到85.2%，验证了区块链数据完整性，证明了集成数字健康解决方案的技术可行性。

Conclusion: 该研究为资源有限环境中的集成孕产妇健康平台提供了一个可复制的架构模型，有助于推进实现可持续发展目标3.1的进展。

Abstract: Maternal mortality in Sub-Saharan Africa remains critically high, accounting for 70% of global deaths despite representing only 17% of the world population. Current digital health interventions typically deploy artificial intelligence (AI), Internet of Things (IoT), and blockchain technologies in isolation, missing synergistic opportunities for transformative healthcare delivery. This paper presents IyaCare, a proof-of-concept integrated platform that combines predictive risk assessment, continuous vital sign monitoring, and secure health records management specifically designed for resource-constrained settings. We developed a web-based system with Next.js frontend, Firebase backend, Ethereum blockchain architecture, and XGBoost AI models trained on maternal health datasets. Our feasibility study demonstrates 85.2% accuracy in high-risk pregnancy prediction and validates blockchain data integrity, with key innovations including offline-first functionality and SMS-based communication for community health workers. While limitations include reliance on synthetic validation data and simulated healthcare environments, results confirm the technical feasibility and potential impact of converged digital health solutions. This work contributes a replicable architectural model for integrated maternal health platforms in low-resource settings, advancing progress toward SDG 3.1 targets.

</details>


### [198] [Artificial Intelligence and Nuclear Weapons Proliferation: The Technological Arms Race for (In)visibility](https://arxiv.org/abs/2512.07487)
*David M. Allison,Stephen Herzog*

Main category: cs.CY

TL;DR: 论文分析了新兴技术如何重塑核扩散风险格局，重点关注核（不）可见性技术竞赛，开发了量化PETs与DETs相对优势的模型，指出AI加速了PET发展并挑战传统监测方法，呼吁制定灵活的政策应对技术变革。


<details>
  <summary>Details</summary>
Motivation: 现有核不扩散机制虽限制了核武器扩散，但新兴颠覆性技术正在改变核风险格局。决策者面临关键转折点，需要理解技术驱动的核（不）可见性竞赛如何影响战略扩散模式，特别是人工智能加速扩散使能技术发展并挑战传统监测方法。

Method: 开发了基于相对优势指数（RAI）的形式化模型，量化扩散使能技术（PETs）与检测增强技术（DETs）之间的平衡变化。通过可复现的情景模拟，分析不对称技术进步（特别是AI驱动的PETs对数增长与DETs逐步改进）如何扩大扩散可检测性的不确定性范围，评估不同PETs增长率和DETs投资策略对累积核突破风险的影响。

Result: 模型显示AI驱动的PETs快速增长与DETs逐步改进之间的不对称技术进步扩大了扩散可检测性的不确定性范围。模拟表明，如果不采取更广泛的PETs治理措施，仅靠检测可能不再足够。技术竞赛正在重塑核扩散战略格局。

Conclusion: 决策者面临战略分叉：仅靠检测已不足以应对技术驱动的核扩散风险。政府和国际组织需要投资于足够灵活的政策和工具，以跟上未来技术发展的步伐，特别是需要建立更广泛的PETs治理框架。

Abstract: A robust nonproliferation regime has contained the spread of nuclear weapons to just nine states. Yet, emerging and disruptive technologies are reshaping the landscape of nuclear risks, presenting a critical juncture for decision makers. This article lays out the contours of an overlooked but intensifying technological arms race for nuclear (in)visibility, driven by the interplay between proliferation-enabling technologies (PETs) and detection-enhancing technologies (DETs). We argue that the strategic pattern of proliferation will be increasingly shaped by the innovation pace in these domains. Artificial intelligence (AI) introduces unprecedented complexity to this equation, as its rapid scaling and knowledge substitution capabilities accelerate PET development and challenge traditional monitoring and verification methods. To analyze this dynamic, we develop a formal model centered on a Relative Advantage Index (RAI), quantifying the shifting balance between PETs and DETs. Our model explores how asymmetric technological advancement, particularly logistic AI-driven PET growth versus stepwise DET improvements, expands the band of uncertainty surrounding proliferation detectability. Through replicable scenario-based simulations, we evaluate the impact of varying PET growth rates and DET investment strategies on cumulative nuclear breakout risk. We identify a strategic fork ahead, where detection may no longer suffice without broader PET governance. Governments and international organizations should accordingly invest in policies and tools agile enough to keep pace with tomorrow's technology.

</details>


### [199] [A Framework for Data Valuation and Monetisation](https://arxiv.org/abs/2512.07664)
*Eduardo Vyhmeister,Bastien Pietropaoli,UdoBub,Rob Schneider,Andrea Visentin*

Main category: cs.CY

TL;DR: 提出统一的数据估值框架，整合经济、治理和战略视角，通过混合模型系统化评估数据价值，连接组织战略与数据资产。


<details>
  <summary>Details</summary>
Motivation: 组织将数据视为战略资源，但现有估值方法分散，缺乏整合经济、治理和战略视角的操作机制，难以将信息资产转化为可衡量的商业价值。

Method: 基于设计科学研究方法，结合工业案例研究，开发混合估值模型：整合定性评分、成本/效用估算、相关性/质量指数和多标准加权，使用ANP工具确定相对重要性，并基于平衡计分卡将指标与组织战略对齐。

Result: 框架在分析用例中展现出灵活性、透明度和减少估值随意性，为组织提供了将数据资产与战略和经济结果连接的结构化基础，支持跨DaaS、IaaS和AaaS路径的货币化潜力评估。

Conclusion: 该统一估值框架通过整合多视角和系统化方法，帮助组织透明、系统地定义数据价值，将数据资产与战略目标对齐，为数据驱动决策提供支持。

Abstract: As organisations increasingly recognise data as a strategic resource, they face the challenge of translating informational assets into measurable business value. Existing valuation approaches remain fragmented, often separating economic, governance, and strategic perspectives and lacking operational mechanisms suitable for real settings. This paper introduces a unified valuation framework that integrates these perspectives into a coherent decision-support model. Building on two artefacts from the Horizon Europe DATAMITE project, a taxonomy of data-quality and performance metrics, and an Analytic Network Process (ANP) tool for deriving relative importance, we develop a hybrid valuation model. The model combines qualitative scoring, cost- and utility-based estimation, relevance/quality indexing, and multi-criteria weighting to define data value transparently and systematically. Anchored in the Balanced Scorecard (BSC), the framework aligns indicators and valuation outcomes with organisational strategy, enabling firms to assess monetisation potential across Data-as-a-Service, Information-as-a-Service, and Answers-as-a-Service pathways. Methodologically, the study follows a Design Science approach complemented by embedded case studies with industrial partners, which informed continual refinement of the model. Because the evaluation is connected to a high-level taxonomy, the approach also reveals how valuation considerations map to BSC perspectives. Across the analysed use cases, the framework demonstrated flexibility, transparency, and reduced arbitrariness in valuation, offering organisations a structured basis for linking data assets to strategic and economic outcomes.

</details>


### [200] [Reliable agent engineering should integrate machine-compatible organizational principles](https://arxiv.org/abs/2512.07665)
*R. Patrick Xian,Garry A. Gabison,Ahmed Alaa,Christoph Riedl,Grigorios G. Chrysos*

Main category: cs.CY

TL;DR: 论文探讨如何借鉴组织科学理论来提升LLM智能体的可靠性，提出了三个组织原则来平衡智能体设计、扩展和管理中的关键因素。


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型的AI智能体在社会中日益普及，协调、控制、委托和问责等问题与其可靠性密切相关。需要借鉴人类组织应对类似平衡问题的经验，提升智能体系统的可靠性。

Method: 通过分析LLM智能体与组织科学框架的相似性，研究组织设计、扩展和管理原则对智能体系统的启示。提出了三个组织原则：平衡智能体设计中的自主性与能力、平衡扩展中的资源约束与性能收益、平衡管理中的内部与外部机制。

Result: 提出了初步的组织原则框架，用于指导AI智能体工程实现可靠性和有效性，扩展了AI系统与社会系统在操作和治理原则方面的交流。

Conclusion: 组织科学为LLM智能体可靠性设计提供了有价值的理论框架，通过借鉴人类组织的平衡策略，可以更好地设计、扩展和管理AI智能体系统，促进系统集成。

Abstract: As AI agents built on large language models (LLMs) become increasingly embedded in society, issues of coordination, control, delegation, and accountability are entangled with concerns over their reliability. To design and implement LLM agents around reliable operations, we should consider the task complexity in the application settings and reduce their limitations while striving to minimize agent failures and optimize resource efficiency. High-functioning human organizations have faced similar balancing issues, which led to evidence-based theories that seek to understand their functioning strategies. We examine the parallels between LLM agents and the compatible frameworks in organization science, focusing on what the design, scaling, and management of organizations can inform agentic systems towards improving reliability. We offer three preliminary accounts of organizational principles for AI agent engineering to attain reliability and effectiveness, through balancing agency and capabilities in agent design, resource constraints and performance benefits in agent scaling, and internal and external mechanisms in agent management. Our work extends the growing exchanges between the operational and governance principles of AI systems and social systems to facilitate system integration.

</details>


### [201] [LLM Use for Mental Health: Crowdsourcing Users' Sentiment-based Perspectives and Values from Social Discussions](https://arxiv.org/abs/2512.07797)
*Lingyao Li,Xiaoshan Huang,Renkai Ma,Ben Zefeng Zhang,Haolun Wu,Fan Yang,Chen Chen*

Main category: cs.CY

TL;DR: 用户通过社交媒体分析发现，LLM聊天机器人在心理健康支持中的使用效果因具体精神健康状况而异，神经多样性群体体验积极，高风险障碍群体则更多负面情绪，需要转向针对具体状况、价值敏感的设计。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM聊天机器人越来越多地用于心理健康支持，提供可访问的治疗支持，但也存在错误信息、过度依赖和高风险情境下的担忧。研究旨在通过社交媒体数据分析用户与LLM聊天机器人互动的实际体验，了解不同心理健康状况下的使用差异。

Method: 从六个主要社交媒体平台众包大规模用户帖子，采用基于价值敏感设计(VSD)的LLM辅助分析流程，映射用户报告的情绪、心理健康状况、观点和价值观之间的关系。

Result: LLM聊天机器人的使用具有条件特异性：神经多样性状况(如ADHD、ASD)用户报告强烈积极情绪和工具性或评价性支持，而高风险障碍(如精神分裂症、双相情感障碍)则显示更多负面情绪。用户观点与身份认同、自主权、隐私等潜在价值观共同出现。

Conclusion: 需要从"一刀切"的聊天机器人设计转向针对具体状况、价值敏感的LLM设计，考虑不同心理健康状况用户的独特需求和价值观，以提供更有效、安全的支持。

Abstract: Large language models (LLMs) chatbots like ChatGPT are increasingly used for mental health support. They offer accessible, therapeutic support but also raise concerns about misinformation, over-reliance, and risks in high-stakes contexts of mental health. We crowdsource large-scale users' posts from six major social media platforms to examine how people discuss their interactions with LLM chatbots across different mental health conditions. Through an LLM-assisted pipeline grounded in Value-Sensitive Design (VSD), we mapped the relationships across user-reported sentiments, mental health conditions, perspectives, and values. Our results reveal that the use of LLM chatbots is condition-specific. Users with neurodivergent conditions (e.g., ADHD, ASD) report strong positive sentiments and instrumental or appraisal support, whereas higher-risk disorders (e.g., schizophrenia, bipolar disorder) show more negative sentiments. We further uncover how user perspectives co-occur with underlying values, such as identity, autonomy, and privacy. Finally, we discuss shifting from "one-size-fits-all" chatbot design toward condition-specific, value-sensitive LLM design.

</details>


<div id='q-fin.MF'></div>

# q-fin.MF [[Back]](#toc)

### [202] [Market Reactions to Material Cybersecurity Incident Disclosures](https://arxiv.org/abs/2512.06144)
*Maxwell Block*

Main category: q-fin.MF

TL;DR: 研究显示，根据Form 8-K第1.05条披露的重大网络安全事件会导致短期股价下跌，小公司跌幅更明显


<details>
  <summary>Details</summary>
Motivation: 研究旨在了解公开市场如何正式报告网络安全风险时的反应，特别是通过Form 8-K第1.05条披露重大网络安全事件后的市场反应

Method: 使用2023-2025年间Form 8-K第1.05条披露的样本，评估每个申报文件周围标准化事件窗口内的每日股价变动

Result: 平均而言，公司在披露重大网络安全事件后出现负面价格反应；小公司跌幅更明显，而行业和贝塔系数差异不明显

Conclusion: 公开市场对正式报告的网络安全风险有负面反应，公司规模会影响对此类事件的敏感程度

Abstract: This study examines short-term market responses to material cybersecurity incidents disclosed under Item 1.05 of Form 8-K. Drawing on a sample of disclosures made between 2023 and 2025, daily stock price movements were evaluated over a standardized event window surrounding each filing. On average, companies experienced negative price reactions following the disclosure of a material cybersecurity incident. Comparisons across company characteristics indicate that smaller companies tended to incur more pronounced declines, while differences by sector and beta were not evident. These findings offer empirical insight into how public markets interpret cybersecurity risks when they are formally reported and suggest that firm size may influence the degree of sensitivity to such events.

</details>


### [203] [Asian option valuation under price impact](https://arxiv.org/abs/2512.07154)
*Priyanshu Tiwari,Sourav Majumdar*

Main category: q-fin.MF

TL;DR: 在具有永久价格影响的二项市场中研究亚式期权定价，扩展了Cox-Ross-Rubinstein框架，获得了几何亚式期权的精确路径表示和算术亚式期权的双边边界。


<details>
  <summary>Details</summary>
Motivation: 研究在存在永久价格影响的市场中如何对亚式期权进行定价，扩展传统的CRR二项模型框架，考虑价格冲击对期权估值的影响。

Method: 使用修改后的风险中性概率，在具有永久价格影响的二项市场中扩展Cox-Ross-Rubinstein框架，获得几何亚式期权的精确路径表示，并推导算术亚式期权的双边边界。

Result: 获得了几何亚式期权的精确路径表示和算术亚式期权的双边边界，识别了套利区域与对冲量的关系，发现永久价格影响会系统性提高亚式期权价格。

Conclusion: 永久价格影响对亚式期权定价有显著影响，会系统性提高期权价格，数值示例展示了影响参数和对冲量对最终价格的影响。

Abstract: We study the valuation of Asian options in a binomial market with permanent price impact, extending the Cox-Ross-Rubinstein framework under a modified risk-neutral probability. We obtain an exact pathwise representation for geometric Asian options and derive two-sided bounds for arithmetic Asian options. Our analysis identifies the no-arbitrage region in terms of hedging volumes and shows that permanent price impact systematically raises Asian option prices. Numerical examples illustrate the effect of the impact parameter and hedging volumes on the resulting prices.

</details>


### [204] [On the structure of increasing profits in a 1D general diffusion market with interest rates](https://arxiv.org/abs/2512.07555)
*Alexis Anagnostakis,David Criens,Mikhail Urusov*

Main category: q-fin.MF

TL;DR: 该论文研究了一般扩散模型下的金融市场，通过尺度函数和速度测度参数化，探讨了递增利润这种强套利机会的存在条件、特征及其与随机过程理论的联系。


<details>
  <summary>Details</summary>
Motivation: 研究一般扩散模型下的金融市场，该模型允许反射边界、偏斜效应、粘滞点和分形集上的减速等特征，旨在深入理解递增利润这种强套利机会的结构特征。

Method: 使用尺度函数、速度测度和常数利率参数化的一般扩散模型，通过构建辅助确定性符号测度ν和规范交易策略θ来表征递增利润的存在性，并建立与随机过程理论的联系。

Result: 1) 递增利润存在当且仅当ν非平凡，且等价于θ本身产生递增利润；2) 完整描述了递增利润集合及其价值过程；3) 建立了无套利理论与一般随机过程理论的新联系，特别是将扩散表示性质的失效与特定类型递增利润联系起来。

Conclusion: 论文为一般扩散模型下的金融市场提供了递增利润的完整理论框架，揭示了套利机会与模型参数特征的深刻联系，并建立了无套利理论与随机过程理论之间的新桥梁。

Abstract: In this paper, we investigate a financial market model consisting of a risky asset, modeled as a general diffusion parameterized by a scale function and a speed measure, and a bank account process with a constant interest rate. This flexible class of financial market models allows for features such as reflecting boundaries, skewness effects, sticky points, and slowdowns on fractal sets. For this market model, we study the structure of a strong form of arbitrage opportunity called increasing profits. Our main contributions are threefold. First, we characterize the existence of increasing profits in terms of an auxiliary deterministic signed measure $ν$ and a canonical trading strategy $θ$, both of which depend only on the deterministic parametric characteristics of our model, namely the scale function, the speed measure, and the interest rate. More precisely, we show that an increasing profit exists if and only if $ν$ is nontrivial, and that this is equivalent to $θ$ itself generating an increasing profit. Second, we provide a precise characterization of the entire set of increasing profits in terms of $ν$ and $θ$, and moreover characterize the value processes associated with increasing profits. Finally, we establish novel connections between no-arbitrage theory and the general theory of stochastic processes. Specifically, we relate the failure of the representation property for general diffusions to the existence of certain types of increasing profits whose value processes are dominated by the quadratic variation measure of a space-transformed version of the asset price process.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [205] [Making Event Study Plots Honest: A Functional Data Approach to Causal Inference](https://arxiv.org/abs/2512.06804)
*Chencheng Fang,Dominik Liebl*

Main category: econ.EM

TL;DR: 提出一种基于函数数据分析的DiD方法，通过事件研究图实现诚实的因果推断，即使平行趋势和/或无预期假设不成立时也能提供可靠的因果推断。


<details>
  <summary>Details</summary>
Motivation: 传统的事件研究图方法在平行趋势和/或无预期假设失败时无法提供诚实的因果推断，需要一种新方法来解决这一局限性。

Method: 引入函数数据分析方法到DiD中，估计量在连续函数Banach空间中收敛到高斯过程，从而能够构建快速且强大的同时置信带。通过等价性检验验证前预期期的诚实参考带，通过相关性检验测试后处理期的诚实因果效应。

Result: 该方法在模拟和两个案例研究中表现出良好性能，能够将事件研究图转化为严谨的诚实因果推断工具。

Conclusion: 提出的函数数据DiD方法为事件研究图提供了理论基础，使其成为诚实的因果推断工具，即使在传统假设不成立时也能提供可靠的因果推断。

Abstract: Event study plots are the centerpiece of Difference-in-Differences (DiD) analysis, but current plotting methods cannot provide honest causal inference when the parallel trends and/or no-anticipation assumptions fail. We introduce a novel functional data approach to DiD that directly enables honest causal inference via event study plots. Our DiD estimator converges to a Gaussian process in the Banach space of continuous functions, enabling fast and powerful simultaneous confidence bands. This theoretical contribution allows us to turn an event study plot into a rigorous honest causal inference tool through equivalence and relevance testing: Honest reference bands can be validated using equivalence testing in the pre-anticipation period, and honest causal effects can be tested using relevance testing in the post-treatment period. We demonstrate the performance of the method in simulations and two case studies.

</details>


### [206] [Estimating Duration Dependence in Job Search: the Within-Estimation Duration Bias](https://arxiv.org/abs/2512.06928)
*Jeremy Zuchuat*

Main category: econ.EM

TL;DR: 固定效应模型在分析纵向数据时可能产生严重偏差，特别是在失业退出等特定时间点存在特定结果值的情况下


<details>
  <summary>Details</summary>
Motivation: 许多研究使用个体纵向数据来分析求职行为，固定效应模型被认为可以解决动态选择问题并识别时间结构效应。然而，当求职结果在失业退出时具有特定值时，固定效应可能导致显著的估计偏差

Method: 推导固定效应估计器提供有效持续时间依赖关系估计的条件，并通过蒙特卡洛模拟展示偏差的大小

Result: 偏差可能极其巨大，研究结果不仅限于求职背景，可扩展到任何使用纵向数据测量时间结构效应的框架

Conclusion: 固定效应模型在特定条件下才能提供有效的持续时间依赖关系估计，研究者需要警惕在失业退出等特定时间点存在特定结果值时可能产生的机械相关性偏差

Abstract: Many recent studies use individual longitudinal data to analyze job search behaviors. Such data allow the use of fixed-effects models, which supposedly address the issue of dynamic selection and make it possible to identify the structural effect of time. However, using fixed effects can induce a sizable within-estimation bias if job search outcomes take specific values at the time job seekers exit unemployment. This pattern creates an undesirable mechanical correlation between the error term and the time regressor. This paper derives the conditions under which the fixed-effects estimator provides valid estimates of structural duration-dependence relationships. Using Monte Carlo simulations, we show that the magnitude of the bias can be extremely large. Our results are not limited to the job search context but naturally extend to any framework in which longitudinal data are used to measure the structural effect of time.

</details>


### [207] [Testing the Significance of the Difference-in-Differences Coefficient via Doubly Randomised Inference](https://arxiv.org/abs/2512.06946)
*Stanisław Marek Sergiusz Halkiewicz,Andrzej Kałuża*

Main category: econ.EM

TL;DR: 本文提出了一种基于双重随机化推断的DID估计量显著性检验方法，通过同时置换处理和时间指标来生成DID系数的经验零分布，相比传统方法具有更大的随机化空间和更好的小样本稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统的DID显著性检验方法（如t检验或单边置换程序）存在随机化空间有限的问题，特别是在小样本或非规则分配结构下表现不稳定。需要一种更稳健的非参数方法来评估DID估计的统计显著性。

Method: 提出双重随机化推断方法：同时置换处理指标和时间指标来生成DID系数的经验零分布。该方法将可容许重新标记的数量增加了$\binom{n}{n_T}$倍，创建了指数级更丰富的置换空间，通过信息论（熵）解释进一步证明了其合理性。

Result: 在多个实证数据集（印尼学校建设计划、品牌搜索数据、最低工资改革、希腊市级难民流入）上的测试表明，双重随机化推断与标准方法表现相当，同时由于扩大的置换空间，在小样本稳定性和临界区域锐度方面表现更优。

Conclusion: 双重随机化推断为评估DID估计的统计显著性提供了一个稳健的非参数替代方案，特别适用于组规模有限或分配结构不规则的实验设计，具有更好的小样本性能和更精确的推断能力。

Abstract: This article develops a significance test for the Difference-in-Differences (DiD) estimator based on doubly randomised inference, in which both the treatment and time indicators are permuted to generate an empirical null distribution of the DiD coefficient. Unlike classical $t$-tests or single-margin permutation procedures, the proposed method exploits a substantially enlarged randomization space. We formally characterise this expansion and show that dual randomization increases the number of admissible relabelings by a factor of $\binom{n}{n_T}$, yielding an exponentially richer permutation universe. This combinatorial gain implies a denser and more stable approximation of the null distribution, a result further justified through an information-theoretic (entropy) interpretation. The validity and finite-sample behaviour of the test are examined using multiple empirical datasets commonly analysed in applied economics, including the Indonesian school construction program (INPRES), brand search data, minimum wage reforms, and municipality-level refugee inflows in Greece. Across all settings, doubly randomised inference performs comparably to standard approaches while offering superior small-sample stability and sharper critical regions due to the enlarged permutation space. The proposed procedure therefore provides a robust, nonparametric alternative for assessing the statistical significance of DiD estimates, particularly in designs with limited group sizes or irregular assignment structures.

</details>


### [208] [Limitations of Randomization Tests in Finite Samples](https://arxiv.org/abs/2512.07099)
*Deniz Dutz,Xinyi Zhang*

Main category: econ.EM

TL;DR: 随机化检验在某些原假设下无法构建有效的有限样本检验，特别是均值零假设不存在随机化检验，这解释了实践中需要更强条件的原因。


<details>
  <summary>Details</summary>
Motivation: 随机化检验在满足随机化假设时能提供精确的有限样本第一类错误控制，但实践中常需要比感兴趣的原假设更强的条件。例如，均值零的符号变化检验需要对称性，对于非对称的均值零分布无法控制有限样本错误。作者想探究这种限制是特定检验选择的问题，还是某些假设根本不可能构建有效随机化检验的根本性限制。

Method: 开发了一个框架，提供了原假设何时允许随机化检验的简单必要和充分条件。将该框架应用于单样本检验，对有限和连续支持的情况，描述了哪些原假设满足这个条件。特别证明了某些原假设（包括均值零）不允许随机化检验。进一步表明，基于线性群作用的随机化检验对应的原假设只能是对称或正态分布的子集。

Result: 证明了某些原假设（包括均值零）根本不允许随机化检验，这解释了为什么实践中需要更强的条件。基于线性群作用的随机化检验只能对应对称或正态分布的子集。这些发现确认了实践者在使用现有检验时没有无意中增加第一类错误，并进一步支持关注随机化检验的渐近有效性。

Conclusion: 随机化检验在某些原假设下存在根本性限制，特别是均值零假设无法构建有效的有限样本随机化检验。这解释了为什么实践中需要比感兴趣的原假设更强的条件。研究结果确认了现有检验的合理性，并强调了关注渐近有效性的重要性。

Abstract: Randomization tests yield exact finite-sample Type 1 error control when the null satisfies the randomization hypothesis. However, achieving these guarantees in practice often requires stronger conditions than the null hypothesis of primary interest. For instance, sign-change tests for mean zero require symmetry and fail to control finite-sample error for non-symmetric mean-zero distributions. We investigate whether such limitations stem from specific test choices or reflect a fundamental inability to construct valid randomization tests for certain hypotheses. We develop a framework providing a simple necessary and sufficient condition for when null hypotheses admit randomization tests. Applying this framework to one-sample tests, we provide characterizations of which nulls satisfy this condition for both finite and continuous supports. In doing so, we prove that certain null hypotheses -- including mean zero -- do not admit randomization tests. We further show that nulls that admit randomization tests based on linear group actions correspond only to subsets of symmetric or normal distributions. Overall, our findings affirm that practitioners are not inadvertently incurring additional Type 1 error when using existing tests and further motivate focusing on the asymptotic validity of randomization tests.

</details>


### [209] [Variational Regularized Bilevel Estimation for Exponential Random Graph Models](https://arxiv.org/abs/2512.07176)
*Yoon Choi*

Main category: econ.EM

TL;DR: 提出一种用于指数随机图模型（ERGM）的估计算法，通过变分平均场方法解决归一化常数难处理和模型退化问题，引入ℓ₂正则化确保唯一解，在蒙特卡洛模拟中显著提高三角形参数符号恢复率。


<details>
  <summary>Details</summary>
Motivation: 现有ERGM估计方法对三角形参数（衡量共同朋友连接倾向的关键网络结构）的估计不可靠，可能导致不可信的政策建议。需要解决ERGM估计中的两个主要困难：归一化常数的难处理性和模型退化问题。

Method: 采用变分平均场方法处理ERGM估计问题，引入ℓ₂正则化确保平均场近似问题的唯一解。提供非渐近优化收敛速率分析，并通过蒙特卡洛模拟验证算法性能。

Result: 在蒙特卡洛模拟中，该方法在小型和中等规模网络上对三角形参数实现了100%的符号恢复率，而现有算法只有50%。提供了ERGM参数估计对超参数选择的敏感性分析。

Conclusion: 提出的算法有效解决了ERGM估计中的关键问题，显著提高了三角形参数的估计可靠性，为网络形成策略的经济和金融应用提供了更可信的估计工具。

Abstract: I propose an estimation algorithm for Exponential Random Graph Models (ERGM), a popular statistical network model for estimating the structural parameters of strategic network formation in economics and finance. Existing methods often produce unreliable estimates of parameters for the triangle, a key network structure that captures the tendency of two individuals with friends in common to connect. Such unreliable estimates may lead to untrustworthy policy recommendations for networks with triangles. Through a variational mean-field approach, my algorithm addresses the two well-known difficulties when estimating the ERGM, the intractability of its normalizing constant and model degeneracy. In addition, I introduce $\ell_2$ regularization that ensures a unique solution to the mean-field approximation problem under suitable conditions. I provide a non-asymptotic optimization convergence rate analysis for my proposed algorithm under mild regularity conditions. Through Monte Carlo simulations, I demonstrate that my method achieves a perfect sign recovery rate for triangle parameters for small and mid-sized networks under perturbed initialization, compared to a 50% rate for existing algorithms. I provide the sensitivity analysis of estimates of ERGM parameters to hyperparameter choices, offering practical insights for implementation.

</details>


### [210] [Bounds on inequality with incomplete data](https://arxiv.org/abs/2512.07709)
*James Banks,Thomas Glinnan,Tatiana Komarova*

Main category: econ.EM

TL;DR: 提出非参数框架，用于在收入或财富仅被粗粒度观测（如分组表格或区间报告）时，对不平等指数进行尖锐的部分识别和推断，支持包含线性约束（如已知均值或子组总和）。


<details>
  <summary>Details</summary>
Motivation: 现实经济数据中，收入或财富往往只能通过分组表格、区间报告等粗粒度方式观测，这给不平等指数的精确估计带来挑战。现有方法难以在有限信息下获得尖锐的识别边界，且计算复杂。

Method: 1) 对一类Schur-凸不等式度量，刻画极值分配并证明尖锐边界可通过具有简单有限支撑的分布实现；2) 对可线性分数表示的指数（如基尼系数、分位数比、胡佛指数），将边界问题转化为线性或二次规划；3) 使用均匀方向delta方法和bootstrap程序进行√n推断。

Result: 在ELSA财富数据中，获得液体储蓄的基尼系数尖锐边界为0.714-0.792，广义储蓄度量为0.686-0.767；历史美国收入表格在分组信息下为基尼系数、分位数比和胡佛指数提供了时间序列边界。

Conclusion: 该框架为粗粒度观测数据下的不平等指数识别提供了统一、计算高效且统计有效的解决方案，能够处理现实数据中的信息限制并产生尖锐的识别边界。

Abstract: We develop a unified, nonparametric framework for sharp partial identification and inference on inequality indices when income or wealth are only coarsely observed -- for example via grouped tables or individual interval reports -- possibly together with linear restrictions such as known means or subgroup totals. First, for a broad class of Schur-convex inequality measures, we characterize extremal allocations and show that sharp bounds are attained by distributions with simple, finite support, reducing the underlying infinite-dimensional problem to finite-dimensional optimization. Second, for indices that admit linear-fractional representations after suitable ordering of the data (including the Gini coefficient, quantile ratios, and the Hoover index), we recast the bound problems as linear or quadratic programs, yielding fast computation of numerically sharp bounds. Third, we establish $\sqrt{n}$ inference for bound endpoints using a uniform directional delta method and a bootstrap procedure for standard errors. In ELSA wealth data with mixed point and interval observations, we obtain sharp Gini bounds of 0.714--0.792 for liquid savings and 0.686--0.767 for a broad savings measure; historical U.S. income tables deliver time-series bounds for the Gini, quantile ratios, and Hoover index under grouped information.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [211] [A self-driving lab for solution-processed electrochromic thin films](https://arxiv.org/abs/2512.05989)
*Selma Dahms,Luca Torresi,Shahbaz Tareq Bandesha,Jan Hansmann,Holger Röhm,Alexander Colsmann,Marco Schott,Pascal Friederich*

Main category: cs.LG

TL;DR: 利用自驱动实验室加速电致变色涂层开发，结合自动化和机器学习优化旋涂工艺参数


<details>
  <summary>Details</summary>
Motivation: 溶液处理电致变色材料在节能智能窗和显示器中潜力巨大，但旋涂工艺参数优化复杂，传统方法开发缓慢

Method: 构建自驱动实验室系统，结合自动化数据采集、图像处理、光谱分析和贝叶斯优化，高效探索处理参数

Result: 该方法不仅提高了通量，还能有针对性地搜索最优处理参数，加速电致变色涂层开发

Conclusion: 自驱动实验室方法可应用于各种溶液处理材料，在材料发现和工艺优化方面具有巨大潜力

Abstract: Solution-processed electrochromic materials offer high potential for energy-efficient smart windows and displays. Their performance varies with material choice and processing conditions. Electrochromic thin film electrodes require a smooth, defect-free coating for optimal contrast between bleached and colored states. The complexity of optimizing the spin-coated electrochromic thin layer poses challenges for rapid development. This study demonstrates the use of self-driving laboratories to accelerate the development of electrochromic coatings by coupling automation with machine learning. Our system combines automated data acquisition, image processing, spectral analysis, and Bayesian optimization to explore processing parameters efficiently. This approach not only increases throughput but also enables a pointed search for optimal processing parameters. The approach can be applied to various solution-processed materials, highlighting the potential of self-driving labs in enhancing materials discovery and process optimization.

</details>


### [212] [The Adoption and Usage of AI Agents: Early Evidence from Perplexity](https://arxiv.org/abs/2512.07828)
*Jeremy Yang,Noah Yonack,Kate Zyskowski,Denis Yarats,Johnny Ho,Jerry Ma*

Main category: cs.LG

TL;DR: 首次对开放网络环境中通用AI代理的大规模实地研究，分析Perplexity的Comet浏览器及其AI助手的使用情况，揭示用户采用、使用强度和使用案例的异质性。


<details>
  <summary>Details</summary>
Motivation: 了解通用AI代理在开放网络环境中的实际采用情况、使用强度和具体用途，填补这一新兴AI能力类别的研究空白。

Method: 基于Perplexity的Comet浏览器及其Comet Assistant代理的数亿匿名用户交互数据，引入分层代理分类法（主题、子主题、任务三个层次）系统分析使用案例。

Result: 早期采用者、高GDP国家用户、数字/知识密集型行业从业者更可能采用AI代理；生产力与工作流、学习与研究占57%查询；个人用途占55%；使用案例短期有粘性，长期向认知导向转变。

Conclusion: AI代理的扩散对研究者、企业、政策制定者和教育者具有重要意义，需要进一步研究这一快速发展的AI能力类别。

Abstract: This paper presents the first large-scale field study of the adoption, usage intensity, and use cases of general-purpose AI agents operating in open-world web environments. Our analysis centers on Comet, an AI-powered browser developed by Perplexity, and its integrated agent, Comet Assistant. Drawing on hundreds of millions of anonymized user interactions, we address three fundamental questions: Who is using AI agents? How intensively are they using them? And what are they using them for? Our findings reveal substantial heterogeneity in adoption and usage across user segments. Earlier adopters, users in countries with higher GDP per capita and educational attainment, and individuals working in digital or knowledge-intensive sectors -- such as digital technology, academia, finance, marketing, and entrepreneurship -- are more likely to adopt or actively use the agent. To systematically characterize the substance of agent usage, we introduce a hierarchical agentic taxonomy that organizes use cases across three levels: topic, subtopic, and task. The two largest topics, Productivity & Workflow and Learning & Research, account for 57% of all agentic queries, while the two largest subtopics, Courses and Shopping for Goods, make up 22%. The top 10 out of 90 tasks represent 55% of queries. Personal use constitutes 55% of queries, while professional and educational contexts comprise 30% and 16%, respectively. In the short term, use cases exhibit strong stickiness, but over time users tend to shift toward more cognitively oriented topics. The diffusion of increasingly capable AI agents carries important implications for researchers, businesses, policymakers, and educators, inviting new lines of inquiry into this rapidly emerging class of AI capabilities.

</details>


### [213] [Memory-Amortized Inference: A Topological Unification of Search, Closure, and Structure](https://arxiv.org/abs/2512.05990)
*Xin Li*

Main category: cs.LG

TL;DR: 提出MAI框架，用代数拓扑统一学习和记忆，基于同调奇偶性原理区分内容与语境，通过拓扑循环闭包将高复杂度搜索转化为低复杂度查找。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习将参数静态结构与推理动态流程分离，缺乏生物认知的样本效率和热力学经济性，需要统一学习与记忆的理论框架。

Method: 基于代数拓扑的Memory-Amortized Inference框架，引入同调奇偶性原理区分内容与语境，通过拓扑三位一体变换（搜索→闭包→结构）实现认知过程。

Result: 建立了拓扑循环闭包机制，将NPSPACE复杂度搜索转化为P复杂度查找，提出拓扑版Wake-Sleep算法协调推理与学习。

Conclusion: MAI框架为快思考（直觉）从慢思考（推理）中涌现提供严格解释，为后图灵架构提供基于拓扑共振的计算蓝图。

Abstract: Contemporary ML separates the static structure of parameters from the dynamic flow of inference, yielding systems that lack the sample efficiency and thermodynamic frugality of biological cognition. In this theoretical work, we propose \textbf{Memory-Amortized Inference (MAI)}, a formal framework rooted in algebraic topology that unifies learning and memory as phase transitions of a single geometric substrate. Central to our theory is the \textbf{Homological Parity Principle}, which posits a fundamental dichotomy: even-dimensional homology ($H_{even}$) physically instantiates stable \textbf{Content} (stable scaffolds or ``what''), while odd-dimensional homology ($H_{odd}$) instantiates dynamic \textbf{Context} (dynamic flows or ``where''). We derive the logical flow of MAI as a topological trinity transformation: \textbf{Search $\to$ Closure $\to$ Structure}. Specifically, we demonstrate that cognition operates by converting high-complexity recursive search (modeled by \textit{Savitch's Theorem} in NPSPACE) into low-complexity lookup (modeled by \textit{Dynamic Programming} in P) via the mechanism of \textbf{Topological Cycle Closure}. We further show that this consolidation process is governed by a topological generalization of the Wake-Sleep algorithm, functioning as a coordinate descent that alternates between optimizing the $H_{odd}$ flow (inference/wake) and condensing persistent cycles into the $H_{even}$ scaffold (learning/sleep). This framework offers a rigorous explanation for the emergence of fast-thinking (intuition) from slow-thinking (reasoning) and provides a blueprint for post-Turing architectures that compute via topological resonance.

</details>


### [214] [Deep learning recognition and analysis of Volatile Organic Compounds based on experimental and synthetic infrared absorption spectra](https://arxiv.org/abs/2512.06059)
*Andrea Della Valle,Annalisa D'Arco,Tiziana Mancini,Rosanna Mosetti,Maria Chiara Paolozzi,Stefano Lupi,Sebastiano Pilati,Andrea Perali*

Main category: cs.LG

TL;DR: 该论文开发了一种基于红外光谱和神经网络的VOC检测方法，通过实验数据增强和条件生成网络提高识别精度，实现九种VOCs的实时识别和浓度预测。


<details>
  <summary>Details</summary>
Motivation: 挥发性有机化合物(VOCs)对人类健康构成重大风险，需要准确检测。虽然红外光谱能够超灵敏检测低浓度VOCs，但光谱复杂性限制了实时识别和定量分析的可能性。深度神经网络需要大量训练数据，而实验数据集通常有限。

Method: 1. 创建九种不同类别化合物的实验VOC数据集，包含不同浓度的红外吸收光谱；2. 使用条件生成神经网络生成合成光谱，增加光谱数量和浓度多样性；3. 训练稳健的判别神经网络，用于VOC识别和浓度预测。

Result: 成功训练出能够可靠识别九种VOCs并精确预测其浓度的神经网络。该方法适用于集成到VOC识别和分析的传感设备中，实现实时监测。

Conclusion: 通过结合实验数据和条件生成神经网络增强的数据集，能够训练出有效的VOC识别和定量分析模型，为实时环境监测提供了可行的技术方案。

Abstract: Volatile Organic Compounds (VOCs) are organic molecules that have low boiling points and therefore easily evaporate into the air. They pose significant risks to human health, making their accurate detection the crux of efforts to monitor and minimize exposure. Infrared (IR) spectroscopy enables the ultrasensitive detection at low-concentrations of VOCs in the atmosphere by measuring their IR absorption spectra. However, the complexity of the IR spectra limits the possibility to implement VOC recognition and quantification in real-time. While deep neural networks (NNs) are increasingly used for the recognition of complex data structures, they typically require massive datasets for the training phase. Here, we create an experimental VOC dataset for nine different classes of compounds at various concentrations, using their IR absorption spectra. To further increase the amount of spectra and their diversity in term of VOC concentration, we augment the experimental dataset with synthetic spectra created via conditional generative NNs. This allows us to train robust discriminative NNs, able to reliably identify the nine VOCs, as well as to precisely predict their concentrations. The trained NN is suitable to be incorporated into sensing devices for VOCs recognition and analysis.

</details>


### [215] [When Privacy Isn't Synthetic: Hidden Data Leakage in Generative AI Models](https://arxiv.org/abs/2512.06062)
*S. M. Mustaqim,Anantaa Kotal,Paul H. Yi*

Main category: cs.LG

TL;DR: 提出一种基于聚类和质心的黑盒成员推理攻击，利用合成数据与真实数据在流形结构上的重叠来推断训练样本成员信息，即使生成器采用差分隐私等机制仍存在泄露风险。


<details>
  <summary>Details</summary>
Motivation: 尽管生成模型被广泛用于生成保护隐私的合成数据，但现有方法主要关注样本级别的记忆化，忽略了合成数据与真实数据在分布结构上的重叠可能导致的隐私泄露风险。

Method: 提出黑盒成员推理攻击：1）反复查询生成模型获取大量合成样本；2）进行无监督聚类识别合成分布的密集区域；3）分析聚类质心和邻域，这些区域对应原始训练数据的高密度区域；4）利用这些邻域作为训练样本的代理来推断成员信息或重建近似记录。

Result: 在医疗、金融等敏感领域的实验表明，即使生成器采用差分隐私或其他噪声机制，真实数据与合成数据之间的聚类重叠仍会导致可测量的成员信息泄露。

Conclusion: 合成数据生成管道存在未充分探索的攻击面，需要更强的隐私保证机制，不仅要考虑样本级别的记忆化，还要考虑分布邻域推理，这对隐私保护数据发布具有重要意义。

Abstract: Generative models are increasingly used to produce privacy-preserving synthetic data as a safe alternative to sharing sensitive training datasets. However, we demonstrate that such synthetic releases can still leak information about the underlying training samples through structural overlap in the data manifold. We propose a black-box membership inference attack that exploits this vulnerability without requiring access to model internals or real data. The attacker repeatedly queries the generative model to obtain large numbers of synthetic samples, performs unsupervised clustering to identify dense regions of the synthetic distribution, and then analyzes cluster medoids and neighborhoods that correspond to high-density regions in the original training data. These neighborhoods act as proxies for training samples, enabling the adversary to infer membership or reconstruct approximate records. Our experiments across healthcare, finance, and other sensitive domains show that cluster overlap between real and synthetic data leads to measurable membership leakage-even when the generator is trained with differential privacy or other noise mechanisms. The results highlight an under-explored attack surface in synthetic data generation pipelines and call for stronger privacy guarantees that account for distributional neighborhood inference rather than sample-level memorization alone, underscoring its role in privacy-preserving data publishing. Implementation and evaluation code are publicly available at:github.com/Cluster-Medoid-Leakage-Attack.

</details>


### [216] [JaxWildfire: A GPU-Accelerated Wildfire Simulator for Reinforcement Learning](https://arxiv.org/abs/2512.06102)
*Ufuk Çakır,Victor-Alexandru Darvariu,Bruno Lacerda,Nick Hawes*

Main category: cs.LG

TL;DR: JaxWildfire是一个基于JAX实现的高性能野火模拟器，使用概率细胞自动机模型，通过GPU向量化加速模拟，比现有软件快6-35倍，支持参数梯度优化和强化学习训练。


<details>
  <summary>Details</summary>
Motivation: 强化学习在野火等自然灾害管理中具有潜力，但现有模拟器速度慢限制了RL代理的训练，因为训练需要大量环境交互。

Method: 开发JaxWildfire模拟器，基于概率细胞自动机的火灾传播模型，使用JAX实现，通过vmap实现向量化模拟，支持GPU加速。

Result: JaxWildfire比现有软件快6-35倍，支持模拟器参数的梯度优化，并能用于训练RL代理学习野火抑制策略。

Conclusion: JaxWildfire是推进强化学习技术在自然灾害管理应用中的重要一步，解决了模拟速度瓶颈问题。

Abstract: Artificial intelligence methods are increasingly being explored for managing wildfires and other natural hazards. In particular, reinforcement learning (RL) is a promising path towards improving outcomes in such uncertain decision-making scenarios and moving beyond reactive strategies. However, training RL agents requires many environment interactions, and the speed of existing wildfire simulators is a severely limiting factor. We introduce $\texttt{JaxWildfire}$, a simulator underpinned by a principled probabilistic fire spread model based on cellular automata. It is implemented in JAX and enables vectorized simulations using $\texttt{vmap}$, allowing high throughput of simulations on GPUs. We demonstrate that $\texttt{JaxWildfire}$ achieves 6-35x speedup over existing software and enables gradient-based optimization of simulator parameters. Furthermore, we show that $\texttt{JaxWildfire}$ can be used to train RL agents to learn wildfire suppression policies. Our work is an important step towards enabling the advancement of RL techniques for managing natural hazards.

</details>


### [217] [ARC-AGI Without Pretraining](https://arxiv.org/abs/2512.06104)
*Isaac Liao,Albert Gu*

Main category: cs.LG

TL;DR: 76K参数模型CompressARC无需预训练，通过最小描述长度(MDL)在推理时解决20%的ARC-AGI视觉谜题，展示了MDL作为智能替代路径的潜力


<details>
  <summary>Details</summary>
Motivation: 挑战传统观点：反驳LLM时代认为解决ARC-AGI视觉谜题需要大规模预训练的假设，探索在极有限数据条件下实现智能的替代方法

Method: 提出CompressARC模型（仅76K参数），不使用任何预训练，在推理时仅基于目标谜题本身（移除最终解信息）进行训练，通过最小描述长度(MDL)原则解决视觉谜题

Result: 在极有限数据条件下（仅使用目标谜题本身训练，不使用ARC-AGI训练集），模型解决了20%的评估谜题，展现出深度学习罕见的极端泛化能力

Conclusion: 最小描述长度(MDL)是传统预训练之外可行的智能产生方式，为在数据稀缺条件下实现智能提供了新思路

Abstract: Conventional wisdom in the age of LLMs dictates that solving IQ-test-like visual puzzles from the ARC-AGI-1 benchmark requires capabilities derived from massive pretraining. To counter this, we introduce CompressARC, a 76K parameter model without any pretraining that solves 20% of evaluation puzzles by minimizing the description length (MDL) of the target puzzle purely during inference time. The MDL endows CompressARC with extreme generalization abilities typically unheard of in deep learning. To our knowledge, CompressARC is the only deep learning method for ARC-AGI where training happens only on a single sample: the target inference puzzle itself, with the final solution information removed. Moreover, CompressARC does not train on the pre-provided ARC-AGI "training set". Under these extremely data-limited conditions, we do not ordinarily expect any puzzles to be solvable at all. Yet CompressARC still solves a diverse distribution of creative ARC-AGI puzzles, suggesting MDL to be an alternative feasible way to produce intelligence, besides conventional pretraining.

</details>


### [218] [A Prescriptive Framework for Determining Optimal Days for Short-Term Traffic Counts](https://arxiv.org/abs/2512.06111)
*Arthur Mukwaya,Nancy Kasamala,Nana Kankam Gyimah,Judith Mwakalonge,Gurcan Comert,Saidi Siuhi,Denis Ruganuza,Mark Ngotonie*

Main category: cs.LG

TL;DR: 提出机器学习框架选择最佳代表日进行短期交通计数，以提高年度平均日交通量预测精度


<details>
  <summary>Details</summary>
Motivation: 美国各州交通部门难以获取准确的年度平均日交通量数据，特别是未监测道路。连续计数站成本高难以广泛部署，目前依赖短期计数但准确性不足。

Method: 使用机器学习框架，通过迭代选择对AADT估计最具信息量的最优代表日。利用德州2022-2023年交通数据，比较"最优日"方法和"无最优日"基线。采用留一法技术生成无偏的代表性日交通特征。

Result: 最优日方法在Top5天内均优于基线，最佳日（第186天）的RMSE：7,871.15，MAE：3,645.09，MAPE：11.95%，R²：0.9756，显著优于基线（RMSE：11,185.00，MAE：5,118.57，MAPE：14.42%，R²：0.9499）。

Conclusion: 该研究为交通部门提供了替代传统短期计数实践的方法，能提高AADT估计精度，支持公路性能监测系统合规性，并降低全州交通数据收集的运营成本。

Abstract: The Federal Highway Administration (FHWA) mandates that state Departments of Transportation (DOTs) collect reliable Annual Average Daily Traffic (AADT) data. However, many U.S. DOTs struggle to obtain accurate AADT, especially for unmonitored roads. While continuous count (CC) stations offer accurate traffic volume data, their implementation is expensive and difficult to deploy widely, compelling agencies to rely on short-duration traffic counts. This study proposes a machine learning framework, the first to our knowledge, to identify optimal representative days for conducting short count (SC) data collection to improve AADT prediction accuracy. Using 2022 and 2023 traffic volume data from the state of Texas, we compare two scenarios: an 'optimal day' approach that iteratively selects the most informative days for AADT estimation and a 'no optimal day' baseline reflecting current practice by most DOTs. To align with Texas DOT's traffic monitoring program, continuous count data were utilized to simulate the 24 hour short counts. The actual field short counts were used to enhance feature engineering through using a leave-one-out (LOO) technique to generate unbiased representative daily traffic features across similar road segments. Our proposed methodology outperforms the baseline across the top five days, with the best day (Day 186) achieving lower errors (RMSE: 7,871.15, MAE: 3,645.09, MAPE: 11.95%) and higher R^2 (0.9756) than the baseline (RMSE: 11,185.00, MAE: 5,118.57, MAPE: 14.42%, R^2: 0.9499). This research offers DOTs an alternative to conventional short-duration count practices, improving AADT estimation, supporting Highway Performance Monitoring System compliance, and reducing the operational costs of statewide traffic data collection.

</details>


### [219] [Physics-Informed Neural Koopman Machine for Interpretable Longitudinal Personalized Alzheimer's Disease Forecasting](https://arxiv.org/abs/2512.06134)
*Georgi Hrusanov,Duy-Thanh Vu,Duy-Cat Can,Sophie Tascedda,Margaret Ryan,Julien Bodelet,Katarzyna Koscielska,Carsten Magnus,Oliver Y. Chén*

Main category: cs.LG

TL;DR: NKM是一种新型机器学习架构，结合动态系统和注意力机制，利用多模态数据同时预测多个认知评分，在阿尔茨海默病认知衰退预测中优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病早期认知衰退预测对疾病评估和管理至关重要，但现有方法难以在保持可解释性的同时整合多模态数据进行纵向个性化预测。

Method: 提出神经库普曼机器（NKM），结合动态系统和注意力机制，通过融合分组感知分层注意力机制，将复杂非线性轨迹转换为可解释的线性表示，同时预测多个认知评分。

Result: 在ADNI数据集上，NKM在预测认知衰退轨迹方面始终优于传统机器学习和深度学习模型，能同时预测多个认知评分变化，量化生物标志物贡献，识别最预测认知恶化的脑区。

Conclusion: NKM通过可解释的显式系统推进了基于多模态数据的个性化认知衰退预测，揭示了阿尔茨海默病进展的多模态生物学基础。

Abstract: Early forecasting of individual cognitive decline in Alzheimer's disease (AD) is central to disease evaluation and management. Despite advances, it is as of yet challenging for existing methodological frameworks to integrate multimodal data for longitudinal personalized forecasting while maintaining interpretability. To address this gap, we present the Neural Koopman Machine (NKM), a new machine learning architecture inspired by dynamical systems and attention mechanisms, designed to forecast multiple cognitive scores simultaneously using multimodal genetic, neuroimaging, proteomic, and demographic data. NKM integrates analytical ($α$) and biological ($β$) knowledge to guide feature grouping and control the hierarchical attention mechanisms to extract relevant patterns. By implementing Fusion Group-Aware Hierarchical Attention within the Koopman operator framework, NKM transforms complex nonlinear trajectories into interpretable linear representations. To demonstrate NKM's efficacy, we applied it to study the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset. Our results suggest that NKM consistently outperforms both traditional machine learning methods and deep learning models in forecasting trajectories of cognitive decline. Specifically, NKM (1) forecasts changes of multiple cognitive scores simultaneously, (2) quantifies differential biomarker contributions to predicting distinctive cognitive scores, and (3) identifies brain regions most predictive of cognitive deterioration. Together, NKM advances personalized, interpretable forecasting of future cognitive decline in AD using past multimodal data through an explainable, explicit system and reveals potential multimodal biological underpinnings of AD progression.

</details>


### [220] [gp2Scale: A Class of Compactly-Supported Non-Stationary Kernels and Distributed Computing for Exact Gaussian Processes on 10 Million Data Points](https://arxiv.org/abs/2512.06143)
*Marcus M. Noack,Mark D. Risser,Hengrui Luo,Vardaan Tekriwal,Ronald J. Pandolfi*

Main category: cs.LG

TL;DR: 提出gp2Scale方法，在不依赖诱导点、核插值或邻域近似的情况下，将精确高斯过程扩展到1000万+数据点，利用灵活核设计实现自然稀疏结构


<details>
  <summary>Details</summary>
Motivation: 现有高斯过程扩展方法在计算速度、预测精度和不确定性量化之间存在顽固权衡，大多依赖近似方法降低了精度并限制了核和噪声模型设计的灵活性，而当前表达性非平稳核在许多领域兴起，需要更灵活的解决方案

Method: 提出gp2Scale方法，利用高度灵活、紧支撑和非平稳核设计识别协方差矩阵中自然出现的稀疏结构，然后利用这种稀疏性进行线性系统求解和对数行列式计算以进行训练

Result: 方法在多个真实世界数据集上展示功能，与最先进近似算法相比，在许多情况下表现出优越的近似性能，能够扩展到超过1000万个数据点

Conclusion: 该方法的核心优势在于对任意GP定制（核心核设计、噪声和均值函数）以及输入空间类型的不可知性，使其特别适合现代高斯过程应用

Abstract: Despite a large corpus of recent work on scaling up Gaussian processes, a stubborn trade-off between computational speed, prediction and uncertainty quantification accuracy, and customizability persists. This is because the vast majority of existing methodologies exploit various levels of approximations that lower accuracy and limit the flexibility of kernel and noise-model designs -- an unacceptable drawback at a time when expressive non-stationary kernels are on the rise in many fields. Here, we propose a methodology we term \emph{gp2Scale} that scales exact Gaussian processes to more than 10 million data points without relying on inducing points, kernel interpolation, or neighborhood-based approximations, and instead leveraging the existing capabilities of a GP: its kernel design. Highly flexible, compactly supported, and non-stationary kernels lead to the identification of naturally occurring sparse structure in the covariance matrix, which is then exploited for the calculations of the linear system solution and the log-determinant for training. We demonstrate our method's functionality on several real-world datasets and compare it with state-of-the-art approximation algorithms. Although we show superior approximation performance in many cases, the method's real power lies in its agnosticism toward arbitrary GP customizations -- core kernel design, noise, and mean functions -- and the type of input space, making it optimally suited for modern Gaussian process applications.

</details>


### [221] [Learning Invariant Graph Representations Through Redundant Information](https://arxiv.org/abs/2512.06154)
*Barproda Halder,Pasan Dissanayake,Sanghamitra Dutta*

Main category: cs.LG

TL;DR: 该论文提出了一种基于部分信息分解(PID)的冗余信息引导的图不变学习框架(RIG)，用于解决图数据分布外泛化问题，通过最大化冗余信息并分离虚假和因果子图来实现更好的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有的图不变表示学习方法通常依赖经典信息论度量，但学习到的表示往往仍包含虚假成分，导致分布外泛化能力不足。需要更精确地关注虚假子图和不变子图之间关于目标Y的冗余信息。

Method: 提出RIG框架：1) 使用部分信息分解(PID)识别虚假子图Gs和不变子图Gc之间的冗余信息；2) 采用多级优化框架，交替估计冗余信息下界并最大化该下界；3) 同时分离虚假和因果子图以实现分布外泛化。

Result: 在合成和真实世界图数据集上的实验表明，RIG框架在各种分布偏移下具有优越的泛化能力，验证了基于PID的冗余信息引导方法的有效性。

Conclusion: 部分信息分解(PID)为图不变学习提供了超越经典信息论度量的新工具，通过精确关注冗余信息并设计相应的优化框架，能够有效提升图模型在分布外场景下的泛化性能。

Abstract: Learning invariant graph representations for out-of-distribution (OOD) generalization remains challenging because the learned representations often retain spurious components. To address this challenge, this work introduces a new tool from information theory called Partial Information Decomposition (PID) that goes beyond classical information-theoretic measures. We identify limitations in existing approaches for invariant representation learning that solely rely on classical information-theoretic measures, motivating the need to precisely focus on redundant information about the target $Y$ shared between spurious subgraphs $G_s$ and invariant subgraphs $G_c$ obtained via PID. Next, we propose a new multi-level optimization framework that we call -- Redundancy-guided Invariant Graph learning (RIG) -- that maximizes redundant information while isolating spurious and causal subgraphs, enabling OOD generalization under diverse distribution shifts. Our approach relies on alternating between estimating a lower bound of redundant information (which itself requires an optimization) and maximizing it along with additional objectives. Experiments on both synthetic and real-world graph datasets demonstrate the generalization capabilities of our proposed RIG framework.

</details>


### [222] [PMA-Diffusion: A Physics-guided Mask-Aware Diffusion Framework for TSE from Sparse Observations](https://arxiv.org/abs/2512.06183)
*Lindong Liu,Zhixiong Jin,Seongjin Choi*

Main category: cs.LG

TL;DR: PMA-Diffusion：基于物理引导的掩码感知扩散框架，用于从稀疏观测中重建高速公路速度场，在仅有5%可见度时仍优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 智能交通系统需要高分辨率交通状态信息，但现有的环路检测器和探测车数据通常过于稀疏和嘈杂，无法捕捉交通流的详细动态。

Method: 提出PMA-Diffusion框架，包含两个掩码感知训练策略（单掩码和双掩码），在推理阶段采用物理引导的后验采样器，交替进行反向扩散更新、观测投影和基于自适应各向异性平滑的物理引导投影。

Result: 在I-24 MOTION数据集上测试，即使在仅有5%可见度的严重稀疏情况下，PMA-Diffusion在三个重建误差指标上均优于其他基线方法。使用稀疏观测训练的模型性能接近基于完整观测训练的基线模型。

Conclusion: 将掩码感知扩散先验与物理引导后验采样器相结合，为实际传感稀疏条件下的交通状态估计提供了可靠且灵活的解决方案。

Abstract: High-resolution highway traffic state information is essential for Intelligent Transportation Systems, but typical traffic data acquired from loop detectors and probe vehicles are often too sparse and noisy to capture the detailed dynamics of traffic flow. We propose PMA-Diffusion, a physics-guided mask-aware diffusion framework that reconstructs unobserved highway speed fields from sparse, incomplete observations. Our approach trains a diffusion prior directly on sparsely observed speed fields using two mask-aware training strategies: Single-Mask and Double-Mask. At the inference phase, the physics-guided posterior sampler alternates reverse-diffusion updates, observation projection, and physics-guided projection based on adaptive anisotropic smoothing to reconstruct the missing speed fields. The proposed framework is tested on the I-24 MOTION dataset with varying visibility ratios. Even under severe sparsity, with only 5% visibility, PMA-Diffusion outperforms other baselines across three reconstruction error metrics. Furthermore, PMA-diffusion trained with sparse observation nearly matches the performance of the baseline model trained on fully observed speed fields. The results indicate that combining mask-aware diffusion priors with a physics-guided posterior sampler provides a reliable and flexible solution for traffic state estimation under realistic sensing sparsity.

</details>


### [223] [How Should We Evaluate Data Deletion in Graph-Based ANN Indexes?](https://arxiv.org/abs/2512.06200)
*Tomohiro Yamashita,Daichi Amagata,Yusuke Matsui*

Main category: cs.LG

TL;DR: 本文提出了一个用于评估近似最近邻搜索（ANNS）索引数据删除效率的实验框架和综合评估指标，将图基ANNS的数据删除方法分为三类并进行数学形式化，最后应用于HNSW方法并提出动态选择删除方法的Deletion Control方法。


<details>
  <summary>Details</summary>
Motivation: 随着检索增强生成等应用的发展，动态数据上的ANNS算法需求日益增长，但目前缺乏针对ANNS数据删除的全面评估方法。需要建立系统化的评估框架来衡量不同删除方法的效率。

Method: 1) 提出实验框架和综合评估指标；2) 将图基ANNS的数据删除方法分为三类并进行数学形式化；3) 在准确率、查询速度等指标上评估性能；4) 将框架应用于HNSW方法；5) 提出Deletion Control方法，根据所需搜索精度动态选择适当的删除方法。

Result: 建立了ANNS数据删除的评估框架，将删除方法系统分类，并在HNSW上验证了评估方法的有效性。提出的Deletion Control方法能够根据精度要求动态优化删除策略。

Conclusion: 本文填补了ANNS数据删除评估方法的空白，提出的框架和Deletion Control方法为动态ANNS系统的设计和优化提供了重要工具，有助于在实际应用中平衡删除效率和搜索性能。

Abstract: Approximate Nearest Neighbor Search (ANNS) has recently gained significant attention due to its many applications, such as Retrieval-Augmented Generation. Such applications require ANNS algorithms that support dynamic data, so the ANNS problem on dynamic data has attracted considerable interest. However, a comprehensive evaluation methodology for data deletion in ANNS has yet to be established. This study proposes an experimental framework and comprehensive evaluation metrics to assess the efficiency of data deletion for ANNS indexes under practical use cases. Specifically, we categorize data deletion methods in graph-based ANNS into three approaches and formalize them mathematically. The performance is assessed in terms of accuracy, query speed, and other relevant metrics. Finally, we apply the proposed evaluation framework to Hierarchical Navigable Small World, one of the state-of-the-art ANNS methods, to analyze the effects of data deletion, and propose Deletion Control, a method which dynamically selects the appropriate deletion method under a required search accuracy.

</details>


### [224] [K2-V2: A 360-Open, Reasoning-Enhanced LLM](https://arxiv.org/abs/2512.06201)
*K2 Team,Zhengzhong Liu,Liping Tang,Linghao Jin,Haonan Li,Nikhil Ranjan,Desai Fan,Shaurya Rohatgi,Richard Fan,Omkar Pangarkar,Huijuan Wang,Zhoujun Cheng,Suqi Sun,Seungwook Han,Bowen Tan,Gurpreet Gosal,Xudong Han,Varad Pimpalkhute,Shibo Hao,Ming Shan Hee,Joel Hestness,Haolong Jia,Liqun Ma,Aaryamonvikram Singh,Daria Soboleva,Natalia Vassilieva,Renxi Wang,Yingquan Wu,Yuekai Sun,Taylor Killian,Alexander Moreno,John Maggs,Hector Ren,Guowei He,Hongyi Wang,Xuezhe Ma,Yuqi Wang,Mikhail Yurochkin,Eric P. Xing*

Main category: cs.LG

TL;DR: K2-V2是一个从头开始构建的360度开放LLM，专注于推理能力，在72B规模中表现优异，接近Qwen3-235B水平，并公开完整训练历史和数据


<details>
  <summary>Details</summary>
Motivation: 构建一个专门为推理任务优化的开放基础模型，超越传统LLM在对话和知识检索方面的功能，为复杂推理任务提供更好的基础

Method: 从头开始训练，在训练过程中主动注入领域知识、推理能力、长上下文和工具使用能力，使用简单的监督微调建立强基线

Result: 成为最强的完全开放模型，在相同规模中表现优异，超越Qwen2.5-72B，接近Qwen3-235B的性能水平

Conclusion: K2-V2为推理任务提供了强大的基础模型，通过公开完整训练历史和数据，支持社区的持续训练和研究，展示了高级对齐的显著提升空间

Abstract: We introduce K2-V2, a 360-open LLM built from scratch as a superior base for reasoning adaptation, in addition to functions such as conversation and knowledge retrieval from general LLMs. It stands as the strongest fully open model, rivals open-weight leaders in its size class, outperforms Qwen2.5-72B and approaches the performance of Qwen3-235B. We actively infuse domain knowledge, reasoning, long-context, and tool use throughout the training process. This explicitly prepares the model for complex reasoning tasks. We demonstrate this potential using simple supervised fine-tuning, establishing a strong baseline that indicates significant headroom for advanced alignment. By releasing the full training history and data composition, we maximize the effectiveness of continuous training, a key open source production scenario. We release the model weights and signature LLM360 artifacts, such as complete training data, to empower the community with a capable, reasoning-centric foundation.

</details>


### [225] [Quantifying Memory Use in Reinforcement Learning with Temporal Range](https://arxiv.org/abs/2512.06204)
*Rodney Lafuente-Mercado,Daniela Rus,T. Konstantin Rusch*

Main category: cs.LG

TL;DR: 提出Temporal Range度量，用于量化RL策略对历史观测的依赖程度，通过计算输出对输入序列的敏感性加权平均滞后时间


<details>
  <summary>Details</summary>
Motivation: 量化训练好的RL策略实际使用过去观测的程度，为比较不同智能体和环境提供实用的记忆依赖度量

Method: 提出Temporal Range度量，通过反向自动微分计算Jacobian块∂y_s/∂x_t，得到时间影响剖面，用幅度加权平均滞后时间总结

Result: 在诊断和控制任务中验证：完全观测控制中值小；Copy-k任务中与真实滞后时间匹配；与达到近最优回报所需的最小历史窗口一致

Conclusion: Temporal Range为比较智能体和环境、选择最短足够上下文提供了实用的每序列记忆依赖度量

Abstract: How much does a trained RL policy actually use its past observations? We propose \emph{Temporal Range}, a model-agnostic metric that treats first-order sensitivities of multiple vector outputs across a temporal window to the input sequence as a temporal influence profile and summarizes it by the magnitude-weighted average lag. Temporal Range is computed via reverse-mode automatic differentiation from the Jacobian blocks $\partial y_s/\partial x_t\in\mathbb{R}^{c\times d}$ averaged over final timesteps $s\in\{t+1,\dots,T\}$ and is well-characterized in the linear setting by a small set of natural axioms. Across diagnostic and control tasks (POPGym; flicker/occlusion; Copy-$k$) and architectures (MLPs, RNNs, SSMs), Temporal Range (i) remains small in fully observed control, (ii) scales with the task's ground-truth lag in Copy-$k$, and (iii) aligns with the minimum history window required for near-optimal return as confirmed by window ablations. We also report Temporal Range for a compact Long Expressive Memory (LEM) policy trained on the task, using it as a proxy readout of task-level memory. Our axiomatic treatment draws on recent work on range measures, specialized here to temporal lag and extended to vector-valued outputs in the RL setting. Temporal Range thus offers a practical per-sequence readout of memory dependence for comparing agents and environments and for selecting the shortest sufficient context.

</details>


### [226] [Average-reward reinforcement learning in semi-Markov decision processes via relative value iteration](https://arxiv.org/abs/2512.06218)
*Huizhen Yu,Yi Wan,Richard S. Sutton*

Main category: cs.LG

TL;DR: 将异步随机逼近理论应用于平均奖励半马尔可夫决策过程的强化学习，建立了RVI Q-learning算法的收敛性


<details>
  <summary>Details</summary>
Motivation: 将作者最近在Borkar-Meyn框架下的异步随机逼近结果应用于平均奖励SMDPs的强化学习，扩展Schweitzer经典相对值迭代算法到异步随机环境

Method: 应用异步随机逼近理论到RVI Q-learning算法，引入新的单调性条件来估计最优奖励率，对弱通信SMDPs建立收敛性分析

Result: 算法几乎必然收敛到平均奖励最优方程解的紧致连通子集，在额外步长和异步条件下收敛到唯一的样本路径依赖解

Conclusion: 通过引入新的单调性条件，显著扩展了RVI Q-learning的算法框架，为平均奖励SMDPs提供了理论保证

Abstract: This paper applies the authors' recent results on asynchronous stochastic approximation (SA) in the Borkar-Meyn framework to reinforcement learning in average-reward semi-Markov decision processes (SMDPs). We establish the convergence of an asynchronous SA analogue of Schweitzer's classical relative value iteration algorithm, RVI Q-learning, for finite-space, weakly communicating SMDPs. In particular, we show that the algorithm converges almost surely to a compact, connected subset of solutions to the average-reward optimality equation, with convergence to a unique, sample path-dependent solution under additional stepsize and asynchrony conditions. Moreover, to make full use of the SA framework, we introduce new monotonicity conditions for estimating the optimal reward rate in RVI Q-learning. These conditions substantially expand the previously considered algorithmic framework and are addressed through novel arguments in the stability and convergence analysis of RVI Q-learning.

</details>


### [227] [Estimating Black Carbon Concentration from Urban Traffic Using Vision-Based Machine Learning](https://arxiv.org/abs/2512.06649)
*Camellia Zakaria,Aryan Sadeghi,Weaam Jaafar,Junshi Xu,Alex Mariakakis,Marianne Hatzopoulou*

Main category: cs.LG

TL;DR: 利用交通监控视频和天气数据，通过机器学习模型估计街道级黑碳浓度，填补交通与环境影响之间的数据鸿沟。


<details>
  <summary>Details</summary>
Motivation: 城市黑碳排放主要来自交通，但监测成本高导致数据缺乏，而交通监控系统广泛部署，存在交通状况与环境影响之间的信息不平衡。

Method: 从交通视频中提取车辆行为和状况的视觉信息，结合天气数据，构建机器学习模型估计街道级黑碳浓度。

Result: 模型达到R平方值0.72，RMSE为129.42 ng/m³，能够有效估计街道级黑碳浓度。

Conclusion: 利用现有城市基础设施和建模技术生成交通排放相关信息，为污染减排、城市规划、公共卫生和环境正义提供可操作见解。

Abstract: Black carbon (BC) emissions in urban areas are primarily driven by traffic, with hotspots near major roads disproportionately affecting marginalized communities. Because BC monitoring is typically performed using costly and specialized instruments. there is little to no available data on BC from local traffic sources that could help inform policy interventions targeting local factors. By contrast, traffic monitoring systems are widely deployed in cities around the world, highlighting the imbalance between what we know about traffic conditions and what do not know about their environmental consequences. To bridge this gap, we propose a machine learning-driven system that extracts visual information from traffic video to capture vehicles behaviors and conditions. Combining these features with weather data, our model estimates BC at street level, achieving an R-squared value of 0.72 and RMSE of 129.42 ng/m3 (nanogram per cubic meter). From a sustainability perspective, this work leverages resources already supported by urban infrastructure and established modeling techniques to generate information relevant to traffic emission. Obtaining BC concentration data provides actionable insights to support pollution reduction, urban planning, public health, and environmental justice at the local municipal level.

</details>


### [228] [Back to Author Console Empowering GNNs for Domain Adaptation via Denoising Target Graph](https://arxiv.org/abs/2512.06236)
*Haiyang Yu,Meng-Chieh Lee,Xiang song,Qi Zhu,Christos Faloutsos*

Main category: cs.LG

TL;DR: 提出GraphDeT框架，通过引入辅助边去噪任务来提升图神经网络在域适应节点分类中的泛化能力


<details>
  <summary>Details</summary>
Motivation: 图结构域偏移（如不同时间或区域收集的图数据）导致GNN在目标图上性能下降，需要有效的域适应方法

Method: 提出GraphDeT框架，在GNN训练中集成辅助边去噪任务，通过理论分析连接该任务与图泛化边界

Result: 实验显示GraphDeT在处理时间和区域域图偏移时优于现有基线方法

Conclusion: 简单的辅助边去噪任务能有效提升GNN在域适应中的泛化性能，GraphDeT框架在理论和实验上均验证了其有效性

Abstract: We explore the node classification task in the context of graph domain adaptation, which uses both source and target graph structures along with source labels to enhance the generalization capabilities of Graph Neural Networks (GNNs) on target graphs. Structure domain shifts frequently occur, especially when graph data are collected at different times or from varying areas, resulting in poor performance of GNNs on target graphs. Surprisingly, we find that simply incorporating an auxiliary loss function for denoising graph edges on target graphs can be extremely effective in enhancing GNN performance on target graphs. Based on this insight, we propose our framework, GraphDeT, a framework that integrates this auxiliary edge task into GNN training for node classification under domain adaptation. Our theoretical analysis connects this auxiliary edge task to the graph generalization bound with -distance, demonstrating such auxiliary task can imposes a constraint which tightens the bound and thereby improves generalization. The experimental results demonstrate superior performance compared to the existing baselines in handling both time and regional domain graph shifts.

</details>


### [229] [Entropic Confinement and Mode Connectivity in Overparameterized Neural Networks](https://arxiv.org/abs/2512.06297)
*Luca Di Carlo,Chase Goddard,David J. Schwab*

Main category: cs.LG

TL;DR: 神经网络损失景观中吸引盆之间存在低损失路径连接，但优化动态通常局限于单个凸盆地，很少探索中间点。研究发现这是由于曲率变化与优化噪声相互作用产生的熵势垒所致。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络优化中的一个矛盾现象：虽然损失景观中的吸引盆之间存在低损失路径连接，但优化动态通常局限于单个凸盆地，很少探索中间点。研究者希望理解这种连通性与实际优化行为之间的差异。

Method: 通过分析曲率变化与优化噪声的相互作用来识别熵势垒。实证研究发现，沿着这些路径的曲率会系统性地远离最小值而增加，产生有效的力将噪声动态偏转回端点，即使损失保持近乎平坦。

Result: 曲率诱导的熵势垒比能量势垒持续更长时间，塑造了参数空间中解的晚期定位。这些势垒解释了为什么优化动态通常局限于单个吸引盆，尽管存在低损失路径连接不同盆地。

Conclusion: 曲率诱导的熵力在深度学习的损失景观中同时控制着连通性和约束性，解释了优化动态的局部化行为与景观连通性之间的明显矛盾。

Abstract: Modern neural networks exhibit a striking property: basins of attraction in the loss landscape are often connected by low-loss paths, yet optimization dynamics generally remain confined to a single convex basin and rarely explore intermediate points. We resolve this paradox by identifying entropic barriers arising from the interplay between curvature variations along these paths and noise in optimization dynamics. Empirically, we find that curvature systematically rises away from minima, producing effective forces that bias noisy dynamics back toward the endpoints - even when the loss remains nearly flat. These barriers persist longer than energetic barriers, shaping the late-time localization of solutions in parameter space. Our results highlight the role of curvature-induced entropic forces in governing both connectivity and confinement in deep learning landscapes.

</details>


### [230] [Quantization Blindspots: How Model Compression Breaks Backdoor Defenses](https://arxiv.org/abs/2512.06243)
*Rohan Pandey,Eric Ye*

Main category: cs.LG

TL;DR: 量化（INT8/INT4）使现有后门防御完全失效，而攻击成功率仍保持90%以上，暴露了防御评估与实际部署间的严重脱节。


<details>
  <summary>Details</summary>
Motivation: 现实部署中模型通常被量化为INT8或更低精度以减少内存和延迟，但现有后门防御主要在FP32模型上评估，这种评估与实际部署的脱节可能导致防御在实际应用中失效。

Method: 对五种代表性后门防御在三种精度设置（FP32、INT8动态量化、INT4模拟）和两个标准视觉基准（GTSRB、CIFAR-10）上进行系统实证研究，使用经典的BadNet攻击。

Result: INT8量化将所有防御的检测率降至0%，而攻击成功率仍高于99%；INT4量化显示数据集依赖性：Neural Cleanse在GTSRB上有效但在CIFAR-10上失败，而攻击成功率仍高于90%。

Conclusion: 量化鲁棒性应成为未来后门防御评估和设计的必要维度，需要解决防御评估（FP32）与实际部署（量化模型）之间的不匹配问题。

Abstract: Backdoor attacks embed input-dependent malicious behavior into neural networks while preserving high clean accuracy, making them a persistent threat for deployed ML systems. At the same time, real-world deployments almost never serve full-precision models: post-training quantization to INT8 or lower precision is now standard practice for reducing memory and latency. This work asks a simple question: how do existing backdoor defenses behave under standard quantization pipelines? We conduct a systematic empirical study of five representative defenses across three precision settings (FP32, INT8 dynamic, INT4 simulated) and two standard vision benchmarks using a canonical BadNet attack. We observe that INT8 quantization reduces the detection rate of all evaluated defenses to 0% while leaving attack success rates above 99%. For INT4, we find a pronounced dataset dependence: Neural Cleanse remains effective on GTSRB but fails on CIFAR-10, even though backdoors continue to survive quantization with attack success rates above 90%. Our results expose a mismatch between how defenses are commonly evaluated (on FP32 models) and how models are actually deployed (in quantized form), and they highlight quantization robustness as a necessary axis in future evaluations and designs of backdoor defenses.

</details>


### [231] [A Unifying Human-Centered AI Fairness Framework](https://arxiv.org/abs/2512.06944)
*Munshi Mahbubur Rahman,Shimei Pan,James R. Foulds*

Main category: cs.LG

TL;DR: 提出一个统一的人类中心公平性框架，涵盖八种公平性指标，允许利益相关者根据价值观调整权重，并在四个真实数据集上验证了框架的有效性。


<details>
  <summary>Details</summary>
Motivation: AI在关键社会领域应用的增加引发了公平性担忧，但现有方法在平衡不同公平概念和预测准确性方面存在挑战，阻碍了公平AI系统的实际部署。

Method: 引入一个统一的人类中心公平性框架，系统覆盖八种公平性指标，结合个体与群体公平、边际内与交叉假设、结果导向与机会平等视角，使用一致的易理解公式，允许利益相关者分配权重。

Result: 在四个真实数据集（UCI Adult、COMPAS、German Credit、MEPS）上应用该框架，展示调整权重如何揭示不同公平性指标间的细微权衡，并通过司法决策和医疗保健案例研究验证框架的实用性。

Conclusion: 该框架为公平AI系统的实际部署提供了实用且价值敏感的方法，使利益相关者能够根据具体情境和价值观调整公平性目标，促进多方妥协和实际应用。

Abstract: The increasing use of Artificial Intelligence (AI) in critical societal domains has amplified concerns about fairness, particularly regarding unequal treatment across sensitive attributes such as race, gender, and socioeconomic status. While there has been substantial work on ensuring AI fairness, navigating trade-offs between competing notions of fairness as well as predictive accuracy remains challenging, creating barriers to the practical deployment of fair AI systems. To address this, we introduce a unifying human-centered fairness framework that systematically covers eight distinct fairness metrics, formed by combining individual and group fairness, infra-marginal and intersectional assumptions, and outcome-based and equality-of-opportunity (EOO) perspectives. This structure allows stakeholders to align fairness interventions with their values and contextual considerations. The framework uses a consistent and easy-to-understand formulation for all metrics to reduce the learning curve for non-experts. Rather than privileging a single fairness notion, the framework enables stakeholders to assign weights across multiple fairness objectives, reflecting their priorities and facilitating multi-stakeholder compromises. We apply this approach to four real-world datasets: the UCI Adult census dataset for income prediction, the COMPAS dataset for criminal recidivism, the German Credit dataset for credit risk assessment, and the MEPS dataset for healthcare utilization. We show that adjusting weights reveals nuanced trade-offs between different fairness metrics. Finally, through case studies in judicial decision-making and healthcare, we demonstrate how the framework can inform practical and value-sensitive deployment of fair AI systems.

</details>


### [232] [Zero Generalization Error Theorem for Random Interpolators via Algebraic Geometry](https://arxiv.org/abs/2512.06347)
*Naoki Yoshida,Isao Ishikawa,Masaaki Imaizumi*

Main category: cs.LG

TL;DR: 在师生框架下，随机采样插值器的泛化误差在训练样本超过特定阈值后会变为零


<details>
  <summary>Details</summary>
Motivation: 理解大规模模型（如深度神经网络）的高泛化能力是机器学习理论的核心开放问题。虽然近期研究将这种现象归因于随机梯度下降的隐式偏置，但实证证据表明这主要源于模型本身的特性，特别是随机采样的插值器也能有效泛化。

Method: 在师生框架下，利用代数几何工具分析参数空间中插值器集合的几何结构，证明随机采样插值器的泛化误差在训练样本数超过特定阈值后变为零。

Result: 证明了随机采样插值器的泛化误差在训练样本数超过由参数空间中插值器集合几何结构决定的阈值后会精确变为零。

Conclusion: 模型的泛化能力主要源于其自身的几何特性，而非优化算法的隐式偏置。当训练样本足够多时，随机选择的插值器也能实现完美泛化，这为理解大规模模型的泛化能力提供了新的理论视角。

Abstract: We theoretically demonstrate that the generalization error of interpolators for machine learning models under teacher-student settings becomes 0 once the number of training samples exceeds a certain threshold. Understanding the high generalization ability of large-scale models such as deep neural networks (DNNs) remains one of the central open problems in machine learning theory. While recent theoretical studies have attributed this phenomenon to the implicit bias of stochastic gradient descent (SGD) toward well-generalizing solutions, empirical evidences indicate that it primarily stems from properties of the model itself. Specifically, even randomly sampled interpolators, which are parameters that achieve zero training error, have been observed to generalize effectively. In this study, under a teacher-student framework, we prove that the generalization error of randomly sampled interpolators becomes exactly zero once the number of training samples exceeds a threshold determined by the geometric structure of the interpolator set in parameter space. As a proof technique, we leverage tools from algebraic geometry to mathematically characterize this geometric structure.

</details>


### [233] [Auto-exploration for online reinforcement learning](https://arxiv.org/abs/2512.06244)
*Caleb Ju,Guanghui Lan*

Main category: cs.LG

TL;DR: 提出具有自动探索能力的新RL方法，无需先验参数知识，实现O(ε⁻²)样本复杂度


<details>
  <summary>Details</summary>
Motivation: 现有RL算法需要假设充分探索状态和动作空间，这导致算法不可实现且性能次优，需要参数无关的自动探索方法

Method: 提出两种变体：表格设置和线性函数逼近，采用动态混合时间、折扣状态分布采样、鲁棒梯度估计器和优势间隙函数等创新技术

Result: 在存在探索最优策略的假设下，两种方法都达到O(ε⁻²)样本复杂度，且复杂度不包含算法相关参数

Conclusion: 新方法实现了参数无关的自动探索，简单易实现，解决了传统RL算法的探索-利用困境

Abstract: The exploration-exploitation dilemma in reinforcement learning (RL) is a fundamental challenge to efficient RL algorithms. Existing algorithms for finite state and action discounted RL problems address this by assuming sufficient exploration over both state and action spaces. However, this yields non-implementable algorithms and sub-optimal performance. To resolve these limitations, we introduce a new class of methods with auto-exploration, or methods that automatically explore both state and action spaces in a parameter-free way, i.e.,~without a priori knowledge of problem-dependent parameters. We present two variants: one for the tabular setting and one for linear function approximation. Under algorithm-independent assumptions on the existence of an exploring optimal policy, both methods attain $O(ε^{-2})$ sample complexity to solve to $ε$ error. Crucially, these complexities are novel since they are void of algorithm-dependent parameters seen in prior works, which may be arbitrarily large. The methods are also simple to implement because they are parameter-free and do not directly estimate the unknown parameters. These feats are achieved by new algorithmic innovations for RL, including a dynamic mixing time, a discounted state distribution for sampling, a simple robust gradient estimator, and a recent advantage gap function to certify convergence.

</details>


### [234] [Optimizing Optimizers for Fast Gradient-Based Learning](https://arxiv.org/abs/2512.06370)
*Jaerin Lee,Kyoung Mu Lee*

Main category: cs.LG

TL;DR: 提出自动化优化器设计的理论框架，基于贪心原则将优化器设计问题转化为最大化瞬时损失下降的凸优化问题，能够推导出多种现有优化器及其最优超参数。


<details>
  <summary>Details</summary>
Motivation: 为梯度学习中的优化器设计提供理论基础，实现优化器的自动化设计和超参数调优，解决手动设计优化器的复杂性和经验依赖问题。

Method: 基于贪心原则，将优化器设计问题形式化为最大化瞬时损失下降的优化问题。将优化器视为将梯度信号映射为参数更新的函数，将问题转化为一系列凸优化问题。在不同约束条件下求解这些凸优化问题。

Result: 该方法不仅能够推导出多种流行优化器（如SGD、Adam等）的闭式解，还能自动确定这些优化器在当前问题上的最优超参数。支持根据训练过程中收集的梯度统计信息动态调整优化器设计。

Conclusion: 建立了自动化优化器设计的理论框架，实现了"优化的优化"，能够系统性地设计和调优优化器，并支持训练过程中的动态调整，为梯度学习提供了更智能的优化方法。

Abstract: We lay the theoretical foundation for automating optimizer design in gradient-based learning. Based on the greedy principle, we formulate the problem of designing optimizers as maximizing the instantaneous decrease in loss. By treating an optimizer as a function that translates loss gradient signals into parameter motions, the problem reduces to a family of convex optimization problems over the space of optimizers. Solving these problems under various constraints not only recovers a wide range of popular optimizers as closed-form solutions, but also produces the optimal hyperparameters of these optimizers with respect to the problems at hand. This enables a systematic approach to design optimizers and tune their hyperparameters according to the gradient statistics that are collected during the training process. Furthermore, this optimization of optimization can be performed dynamically during training.

</details>


### [235] [Learning When to Switch: Adaptive Policy Selection via Reinforcement Learning](https://arxiv.org/abs/2512.06250)
*Chris Tava*

Main category: cs.LG

TL;DR: 提出基于强化学习的自适应策略切换方法，让智能体在迷宫导航中动态切换系统探索和目标导向路径规划，相比固定阈值方法显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 自主智能体需要多种策略解决复杂任务，但何时切换策略具有挑战性。现有方法通常使用固定阈值或手工启发式，缺乏适应性。

Method: 使用Q-learning学习两个正交导航策略间的切换阈值。将状态空间离散化为覆盖率和距离桶，基于覆盖百分比和目标距离自适应调整切换行为（20-60%阈值范围）。无需迷宫墙壁位置、最优阈值或手工启发式知识。

Result: 在240个测试配置中，自适应阈值学习优于单策略智能体和固定40%阈值基线：完成时间提升23-55%，运行时方差降低83%，最坏情况改善71%。性能增益随问题复杂度增加而提升。

Conclusion: 强化学习可以有效地学习策略切换阈值，自适应方法优于固定启发式，且随着问题复杂度增加，自适应策略选择的价值成比例增长。

Abstract: Autonomous agents often require multiple strategies to solve complex tasks, but determining when to switch between strategies remains challenging. This research introduces a reinforcement learning technique to learn switching thresholds between two orthogonal navigation policies. Using maze navigation as a case study, this work demonstrates how an agent can dynamically transition between systematic exploration (coverage) and goal-directed pathfinding (convergence) to improve task performance. Unlike fixed-threshold approaches, the agent uses Q-learning to adapt switching behavior based on coverage percentage and distance to goal, requiring only minimal domain knowledge: maze dimensions and target location. The agent does not require prior knowledge of wall positions, optimal threshold values, or hand-crafted heuristics; instead, it discovers effective switching strategies dynamically during each run. The agent discretizes its state space into coverage and distance buckets, then adapts which coverage threshold (20-60\%) to apply based on observed progress signals. Experiments across 240 test configurations (4 maze sizes from 16$\times$16 to 128$\times$128 $\times$ 10 unique mazes $\times$ 6 agent variants) demonstrate that adaptive threshold learning outperforms both single-strategy agents and fixed 40\% threshold baselines. Results show 23-55\% improvements in completion time, 83\% reduction in runtime variance, and 71\% improvement in worst-case scenarios. The learned switching behavior generalizes within each size class to unseen wall configurations. Performance gains scale with problem complexity: 23\% improvement for 16$\times$16 mazes, 34\% for 32$\times$32, and 55\% for 64$\times$64, demonstrating that as the space of possible maze structures grows, the value of adaptive policy selection over fixed heuristics increases proportionally.

</details>


### [236] [Learning Without Time-Based Embodiment Resets in Soft-Actor Critic](https://arxiv.org/abs/2512.06252)
*Homayoon Farrahi,A. Rupam Mahmood*

Main category: cs.LG

TL;DR: 本文研究在无终止和无重置的强化学习环境中使用SAC算法的挑战，提出持续SAC版本并通过增加策略熵来补偿无重置带来的探索问题。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习任务通常包含环境重置和分幕终止等辅助组件，这些设置虽然能加速学习，但可能导致不自然的任务设置并阻碍在真实世界中的长期性能。本文旨在探索在没有终止和机器人身体重置的情况下学习的挑战。

Method: 提出持续版本的Soft Actor-Critic (SAC)算法，通过简单修改现有任务的奖励函数。在无身体重置的情况下，通过增加策略熵来改善状态空间探索。在修改的Gym Reacher任务和其他模拟任务以及真实机器人视觉任务上进行实验验证。

Result: 持续SAC在无终止情况下表现与或优于分幕SAC，同时降低了对折扣率γ值的敏感性。无身体重置会导致状态空间探索不足和学习失败/变慢。增加策略熵能有效恢复因无重置而损失的性能。

Conclusion: 身体重置有助于SAC算法中的状态空间探索，移除重置会导致探索不足。通过增加策略熵可以补偿无重置带来的探索问题，使持续学习在无终止和无重置环境中成为可能。

Abstract: When creating new reinforcement learning tasks, practitioners often accelerate the learning process by incorporating into the task several accessory components, such as breaking the environment interaction into independent episodes and frequently resetting the environment. Although they can enable the learning of complex intelligent behaviors, such task accessories can result in unnatural task setups and hinder long-term performance in the real world. In this work, we explore the challenges of learning without episode terminations and robot embodiment resets using the Soft Actor-Critic (SAC) algorithm. To learn without terminations, we present a continuing version of the SAC algorithm and show that, with simple modifications to the reward functions of existing tasks, continuing SAC can perform as well as or better than episodic SAC while reducing the sensitivity of performance to the value of the discount rate $γ$. On a modified Gym Reacher task, we investigate possible explanations for the failure of continuing SAC when learning without embodiment resets. Our results suggest that embodiment resets help with exploration of the state space in the SAC algorithm, and removing embodiment resets can lead to poor exploration of the state space and failure of or significantly slower learning. Finally, on additional simulated tasks and a real-robot vision task, we show that increasing the entropy of the policy when performance trends worse or remains static is an effective intervention for recovering the performance lost due to not using embodiment resets.

</details>


### [237] [Prediction with Expert Advice under Local Differential Privacy](https://arxiv.org/abs/2512.06971)
*Ben Jacobsen,Kassem Fawaz*

Main category: cs.LG

TL;DR: 该论文研究了本地差分隐私约束下的专家建议预测问题，提出了两种改进算法：RW-AdaBatch利用LDP诱导的有限切换行为实现隐私增强，RW-Meta能够私有地选择非平凡学习算法作为专家，并在COVID-19医院数据预测任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 研究在本地差分隐私约束下的专家建议预测问题，现有方法主要考虑数据无关的专家，需要开发能够处理更复杂专家选择且具有更好隐私-效用权衡的算法。

Method: 首先证明经典算法自然满足LDP，然后提出两种新算法：1) RW-AdaBatch：利用LDP诱导的有限切换行为实现隐私增强，类似离线学习中的洗牌模型；2) RW-Meta：开发了私有选择非平凡学习算法专家的通用方法，在LDP下不增加额外隐私成本。

Result: RW-Meta在COVID-19疫情期间医院报告数据的实际评估中，在预测每周报告COVID患者密度最高的医院任务上，比经典基线和最先进的中心差分隐私算法性能提升1.5-3倍。

Conclusion: 该研究为本地差分隐私下的专家建议预测提供了新的算法框架，RW-AdaBatch实现了隐私增强而几乎不损失效用，RW-Meta能够有效选择复杂学习算法专家，在真实医疗数据预测任务中表现出显著优势。

Abstract: We study the classic problem of prediction with expert advice under the constraint of local differential privacy (LDP). In this context, we first show that a classical algorithm naturally satisfies LDP and then design two new algorithms that improve it: RW-AdaBatch and RW-Meta. For RW-AdaBatch, we exploit the limited-switching behavior induced by LDP to provide a novel form of privacy amplification that grows stronger on easier data, analogous to the shuffle model in offline learning. Drawing on the theory of random walks, we prove that this improvement carries essentially no utility cost. For RW-Meta, we develop a general method for privately selecting between experts that are themselves non-trivial learning algorithms, and we show that in the context of LDP this carries no extra privacy cost. In contrast, prior work has only considered data-independent experts. We also derive formal regret bounds that scale inversely with the degree of independence between experts. Our analysis is supplemented by evaluation on real-world data reported by hospitals during the COVID-19 pandemic; RW-Meta outperforms both the classical baseline and a state-of-the-art \textit{central} DP algorithm by 1.5-3$\times$ on the task of predicting which hospital will report the highest density of COVID patients each week.

</details>


### [238] [Networked Restless Multi-Arm Bandits with Reinforcement Learning](https://arxiv.org/abs/2512.06274)
*Hanmo Zhang,Zenghui Sun,Kai Wang*

Main category: cs.LG

TL;DR: 该论文提出了Networked RMAB框架，将多臂赌博机与独立级联模型结合以捕捉网络环境中的个体交互，通过证明贝尔曼方程的子模性并使用爬山算法获得近似保证，最终开发了高效的Q学习算法并在真实图数据上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 传统RMAB假设臂之间相互独立，无法捕捉现实世界中个体间的交互作用。许多实际应用（如公共卫生资源分配）中，个体间的网络效应显著影响决策效果，因此需要能够建模网络交互的RMAB框架。

Method: 1. 提出Networked RMAB框架，将RMAB与独立级联模型结合；2. 定义网络化RMAB的贝尔曼方程；3. 证明贝尔曼方程的子模性，应用爬山算法获得1-1/e近似保证；4. 通过改进的收缩分析证明近似贝尔曼更新的收敛性；5. 开发针对网络化设置的高效Q学习算法。

Result: 1. 理论证明：贝尔曼方程具有子模性，爬山算法可获得1-1/e近似保证，近似贝尔曼更新保证收敛；2. 实验验证：在真实图数据上，提出的Q学习方法优于k步前瞻方法和忽略网络的方法，证明了捕捉网络效应的重要性。

Conclusion: Networked RMAB框架成功地将网络交互纳入RMAB建模，通过理论分析和算法设计解决了计算挑战，并在实际应用中展示了显著优势，为考虑网络效应的序列决策问题提供了有效解决方案。

Abstract: Restless Multi-Armed Bandits (RMABs) are a powerful framework for sequential decision-making, widely applied in resource allocation and intervention optimization challenges in public health. However, traditional RMABs assume independence among arms, limiting their ability to account for interactions between individuals that can be common and significant in a real-world environment. This paper introduces Networked RMAB, a novel framework that integrates the RMAB model with the independent cascade model to capture interactions between arms in networked environments. We define the Bellman equation for networked RMAB and present its computational challenge due to exponentially large action and state spaces. To resolve the computational challenge, we establish the submodularity of Bellman equation and apply the hill-climbing algorithm to achieve a $1-\frac{1}{e}$ approximation guarantee in Bellman updates. Lastly, we prove that the approximate Bellman updates are guaranteed to converge by a modified contraction analysis. We experimentally verify these results by developing an efficient Q-learning algorithm tailored to the networked setting. Experimental results on real-world graph data demonstrate that our Q-learning approach outperforms both $k$-step look-ahead and network-blind approaches, highlighting the importance of capturing and leveraging network effects where they exist.

</details>


### [239] [Theoretical Compression Bounds for Wide Multilayer Perceptrons](https://arxiv.org/abs/2512.06288)
*Houssam El Cheairi,David Gamarnik,Rahul Mazumder*

Main category: cs.LG

TL;DR: 该论文提出了一种随机贪心压缩算法，用于后训练剪枝和量化，并严格证明了多层感知机（MLP）中存在性能有竞争力的剪枝/量化子网络。研究还扩展到MLP和CNN的结构化剪枝，展示了压缩性与网络宽度之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 虽然剪枝和量化技术在实践中能有效减少大型神经网络的参数量，但其理论依据不足。本文旨在为这些技术的经验成功提供理论支持，弥合理论与应用之间的差距。

Method: 提出了一种随机贪心压缩算法，该算法类似于最优脑损伤（OBD）的后训练随机版本。算法用于多层感知机（MLP）的剪枝和量化，并扩展到MLP和卷积神经网络（CNN）的结构化剪枝分析。

Result: 严格证明了在宽网络中存在性能有竞争力的剪枝/量化子网络，展示了压缩性与网络宽度之间的权衡关系。结果无需数据假设，为宽多层感知机中压缩技术的经验成功提供了理论依据。

Conclusion: 该研究为剪枝和量化技术的经验成功提供了理论支持，提出的随机贪心压缩算法和理论分析弥合了神经网络压缩领域理论与应用之间的差距，为宽网络中的压缩技术提供了理论依据。

Abstract: Pruning and quantization techniques have been broadly successful in reducing the number of parameters needed for large neural networks, yet theoretical justification for their empirical success falls short. We consider a randomized greedy compression algorithm for pruning and quantization post-training and use it to rigorously show the existence of pruned/quantized subnetworks of multilayer perceptrons (MLPs) with competitive performance. We further extend our results to structured pruning of MLPs and convolutional neural networks (CNNs), thus providing a unified analysis of pruning in wide networks. Our results are free of data assumptions, and showcase a tradeoff between compressibility and network width. The algorithm we consider bears some similarities with Optimal Brain Damage (OBD) and can be viewed as a post-training randomized version of it. The theoretical results we derive bridge the gap between theory and application for pruning/quantization, and provide a justification for the empirical success of compression in wide multilayer perceptrons.

</details>


### [240] [A Bootstrap Perspective on Stochastic Gradient Descent](https://arxiv.org/abs/2512.07676)
*Hongjian Lan,Yucong Liu,Florian Schäfer*

Main category: cs.LG

TL;DR: SGD通过梯度协方差矩阵的迹隐式正则化，利用批次采样的梯度变异性作为数据收集过程随机性的代理，从而提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 研究为什么随机梯度下降（SGD）比确定性梯度下降（GD）具有更好的泛化能力，从统计bootstrap的角度理解SGD如何提升模型泛化性能。

Method: 通过理论分析和数值实验，证明SGD隐式正则化梯度协方差矩阵的迹，控制算法变异性。在经验风险最小化实验中展示SGD选择对重采样鲁棒的解，并在神经网络训练中显式加入算法变异性估计作为正则器。

Result: SGD通过隐式正则化梯度协方差矩阵的迹，选择对采样噪声不敏感的解，从而提升泛化性能。在神经网络训练中，显式加入算法变异性估计作为正则器能改善测试性能。

Conclusion: SGD的泛化优势源于其通过梯度变异性作为数据收集过程随机性的代理，隐式进行bootstrap估计，从而选择更鲁棒的解。

Abstract: Machine learning models trained with \emph{stochastic} gradient descent (SGD) can generalize better than those trained with deterministic gradient descent (GD). In this work, we study SGD's impact on generalization through the lens of the statistical bootstrap: SGD uses gradient variability under batch sampling as a proxy for solution variability under the randomness of the data collection process. We use empirical results and theoretical analysis to substantiate this claim. In idealized experiments on empirical risk minimization, we show that SGD is drawn to parameter choices that are robust under resampling and thus avoids spurious solutions even if they lie in wider and deeper minima of the training loss. We prove rigorously that by implicitly regularizing the trace of the gradient covariance matrix, SGD controls the algorithmic variability. This regularization leads to solutions that are less sensitive to sampling noise, thereby improving generalization. Numerical experiments on neural network training show that explicitly incorporating the estimate of the algorithmic variability as a regularizer improves test performance. This fact supports our claim that bootstrap estimation underpins SGD's generalization advantages.

</details>


### [241] [Importance-aware Topic Modeling for Discovering Public Transit Risk from Noisy Social Media](https://arxiv.org/abs/2512.06293)
*Fatima Ashraf,Muhammad Ayub Sabir,Jiaxin Deng,Junbiao Pang,Haitao Yu*

Main category: cs.LG

TL;DR: 该论文提出了一种基于影响力加权关键词共现图和泊松反卷积分解的社交媒体主题建模方法，用于从稀疏的公共交通服务风险信号中提取可解释的主题。


<details>
  <summary>Details</summary>
Motivation: 城市交通机构越来越多地使用社交媒体监控服务风险（如拥挤、延误、安全事件），但这些关注信号稀疏、简短，容易被日常聊天淹没。现有方法难以有效提取这些稀疏但有影响力的风险信号。

Method: 1. 从清理后的帖子构建影响力加权的关键词共现图，使有社会影响力的帖子按比例贡献证据；2. 提出泊松反卷积分解（PDF）模型，将图分解为低秩主题结构和主题局部残差交互；3. 使用去相关正则化促进主题区分度；4. 通过一致性驱动的扫描选择主题数量。

Result: 在大规模社交媒体流数据上，该方法实现了最先进的主题一致性，并相比领先基线模型表现出更强的多样性。代码和数据集已公开。

Conclusion: 该方法通过联合建模语言交互和用户影响力，能够从稀疏的社交媒体信号中有效提取可解释的公共交通服务风险主题，为城市交通监控提供了有效的分析工具。

Abstract: Urban transit agencies increasingly turn to social media to monitor emerging service risks such as crowding, delays, and safety incidents, yet the signals of concern are sparse, short, and easily drowned by routine chatter. We address this challenge by jointly modeling linguistic interactions and user influence. First, we construct an influence-weighted keyword co-occurrence graph from cleaned posts so that socially impactful posts contributes proportionally to the underlying evidence. The core of our framework is a Poisson Deconvolution Factorization (PDF) that decomposes this graph into a low-rank topical structure and topic-localized residual interactions, producing an interpretable topic--keyword basis together with topic importance scores. A decorrelation regularizer \emph{promotes} distinct topics, and a lightweight optimization procedure ensures stable convergence under nonnegativity and normalization constraints. Finally, the number of topics is selected through a coherence-driven sweep that evaluates the quality and distinctness of the learned topics. On large-scale social streams, the proposed model achieves state-of-the-art topic coherence and strong diversity compared with leading baselines. The code and dataset are publicly available at https://github.com/pangjunbiao/Topic-Modeling_ITS.git

</details>


### [242] [Provable Long-Range Benefits of Next-Token Prediction](https://arxiv.org/abs/2512.07818)
*Xinyuan Cao,Santosh S. Vempala*

Main category: cs.LG

TL;DR: 论文证明，通过优化RNN的下一词预测，可以学习到训练分布的长程结构，使得模型生成的k个连续token与真实文档的k个token在计算上无法区分。


<details>
  <summary>Details</summary>
Motivation: 解释为什么现代语言模型（基于下一词预测训练）能够生成连贯文档并捕获长程结构，从理论角度提供复杂性解释。

Method: 使用循环神经网络（RNN），通过优化下一词预测目标，证明模型能够近似训练分布。提供多项式边界（关于k）来描述实现k-token不可区分性所需的模型大小。

Result: 证明对于从训练分布采样的文档，任何有界描述长度的算法在检查k个token时，都无法区分模型生成的k个token与真实文档的k个token。这为实践中观察到的长程连贯性提供了复杂性理论解释。

Conclusion: 下一词预测对于学习长程结构具有理论上的强大能力，即使使用常见的神经网络架构（如RNN）也能实现，这解释了现代语言模型在实践中表现出的文档连贯性。

Abstract: Why do modern language models, trained to do well on next-word prediction, appear to generate coherent documents and capture long-range structure? Here we show that next-token prediction is provably powerful for learning longer-range structure, even with common neural network architectures. Specifically, we prove that optimizing next-token prediction over a Recurrent Neural Network (RNN) yields a model that closely approximates the training distribution: for held-out documents sampled from the training distribution, no algorithm of bounded description length limited to examining the next $k$ tokens, for any $k$, can distinguish between $k$ consecutive tokens of such documents and $k$ tokens generated by the learned language model following the same prefix. We provide polynomial bounds (in $k$, independent of the document length) on the model size needed to achieve such $k$-token indistinguishability, offering a complexity-theoretic explanation for the long-range coherence observed in practice.

</details>


### [243] [Chemistry Integrated Language Model using Hierarchical Molecular Representation for Polymer Informatics](https://arxiv.org/abs/2512.06301)
*Jihun Ahn,Gabriella Pasya Irianti,Vikram Thapar,Su-Mi Hur*

Main category: cs.LG

TL;DR: CI-LLM框架结合HAPPY分子表示和数值描述符，实现聚合物性质预测和逆向设计，比SMILES模型更快更准。


<details>
  <summary>Details</summary>
Motivation: 机器学习在无机化合物和小分子材料发现中已广泛应用，但聚合物领域由于数据稀缺而难以应用。本文旨在通过策略性分子表示方法克服这一限制。

Method: 提出CI-LLM框架：1) HAPPY表示法将化学亚结构编码为token；2) De³BERTa编码器结合数值描述符进行性质预测；3) GPT基生成器实现逆向设计。

Result: De³BERTa比SMILES模型推理速度快3.5倍，R²分数提升0.9-4.1%；GPT生成器实现100%骨架保留，成功优化负相关多性质目标。

Conclusion: 策略性分子表示能有效推进聚合物机器学习应用，CI-LLM框架展示了聚合物性质预测和逆向设计的综合能力。

Abstract: Machine learning has transformed material discovery for inorganic compounds and small molecules, yet polymers remain largely inaccessible to these methods. While data scarcity is often cited as the primary bottleneck, we demonstrate that strategic molecular representations can overcome this limitation. We introduce CI-LLM (Chemically Informed Language Model), a framework combining HAPPY (Hierarchically Abstracted rePeat unit of PolYmer), which encodes chemical substructures as tokens, with numerical descriptors within transformer architectures. For property prediction, De$^3$BERTa, our descriptor-enriched encoder, achieves 3.5x faster inference than SMILES-based models with improved accuracy ($R^2$ score gains of 0.9-4.1 percent across four properties), while providing interpretable structure-property insights at the subgroup level. For inverse design, our GPT-based generator produces polymers with targeted properties, achieving 100 percent scaffold retention and successful multi-property optimization for negatively correlated objectives. This comprehensive framework demonstrates both forward prediction and inverse design capabilities, showcasing how strategic molecular representation advances machine learning applications in polymer science.

</details>


### [244] [Multimodal Graph Neural Networks for Prognostic Modeling of Brain Network Reorganization](https://arxiv.org/abs/2512.06303)
*Preksha Girish,Rachana Mysore,Kiran K. N.,Hiranmayee R.,Shipra Prashanth,Shrey Kumar*

Main category: cs.LG

TL;DR: 提出多模态图神经网络框架，整合结构MRI、DTI和功能MRI，建模脑网络时空重组，通过分数随机微分算子捕获动态演化，生成可解释生物标志物用于预测认知衰退风险。


<details>
  <summary>Details</summary>
Motivation: 理解脑网络动态重组对预测认知衰退、神经进展和临床结果个体差异至关重要。现有方法需要新的数据收集，而本研究旨在从现有影像数据中提取临床意义的生物标志物。

Method: 提出多模态图神经网络框架：1) 脑区作为节点，结构和功能连接作为边，构建纵向脑图；2) 使用分数随机微分算子嵌入图循环网络，建模长期依赖和随机波动；3) 注意力机制融合多模态信息；4) 生成网络能量熵、图曲率、分数记忆指数等可解释生物标志物；5) 组合成综合预后指数量化个体风险。

Result: 在纵向神经影像数据集上的实验展示了预测准确性和可解释性。结果表明该方法能够从现有影像数据中提取临床意义的生物标志物，无需新数据收集。

Conclusion: 数学严谨的多模态图神经网络方法具有从现有影像数据中提取临床意义生物标志物的潜力，为预测脑网络不稳定性和认知衰退提供了有效框架。

Abstract: Understanding the dynamic reorganization of brain networks is critical for predicting cognitive decline, neurological progression, and individual variability in clinical outcomes. This work proposes a multimodal graph neural network framework that integrates structural MRI, diffusion tensor imaging, and functional MRI to model spatiotemporal brain network reorganization. Brain regions are represented as nodes and structural and functional connectivity as edges, forming longitudinal brain graphs for each subject. Temporal evolution is captured via fractional stochastic differential operators embedded within graph-based recurrent networks, enabling the modeling of long-term dependencies and stochastic fluctuations in network dynamics. Attention mechanisms fuse multimodal information and generate interpretable biomarkers, including network energy entropy, graph curvature, fractional memory indices, and modality-specific attention scores. These biomarkers are combined into a composite prognostic index to quantify individual risk of network instability or cognitive decline. Experiments on longitudinal neuroimaging datasets demonstrate both predictive accuracy and interpretability. The results highlight the potential of mathematically rigorous, multimodal graph-based approaches for deriving clinically meaningful biomarkers from existing imaging data without requiring new data collection.

</details>


### [245] [Interpretive Efficiency: Information-Geometric Foundations of Data Usefulness](https://arxiv.org/abs/2512.06341)
*Ronald Katende*

Main category: cs.LG

TL;DR: 提出Interpretive Efficiency指标，量化解释性表示中任务相关信息传输效率，基于五条公理，与互信息相关，有理论保证和实验验证。


<details>
  <summary>Details</summary>
Motivation: 当前可解释性指标很少能有效量化数据对解释性表示的支持程度，需要一种理论支撑的实用诊断工具来评估表示设计。

Method: 提出Interpretive Efficiency，一个归一化的任务感知函数，测量通过解释性通道传输的任务相关信息比例。基于五条公理（有界性、Blackwell单调性、数据处理稳定性、容许不变性、渐近一致性），与互信息建立联系，推导局部Fisher几何展开，使用经验过程工具建立渐近和有限样本估计保证。

Result: 在受控图像和信号任务实验中，该指标能恢复理论排序，揭示被准确率掩盖的表示冗余，并与鲁棒性相关，证明其作为表示设计诊断工具的有效性。

Conclusion: Interpretive Efficiency是一个实用且有理论支撑的指标，可用于评估解释性表示设计，为可信机器学习提供量化工具。

Abstract: Interpretability is central to trustworthy machine learning, yet existing metrics rarely quantify how effectively data support an interpretive representation. We propose Interpretive Efficiency, a normalized, task-aware functional that measures the fraction of task-relevant information transmitted through an interpretive channel. The definition is grounded in five axioms ensuring boundedness, Blackwell-style monotonicity, data-processing stability, admissible invariance, and asymptotic consistency. We relate the functional to mutual information and derive a local Fisher-geometric expansion, then establish asymptotic and finite-sample estimation guarantees using standard empirical-process tools. Experiments on controlled image and signal tasks demonstrate that the measure recovers theoretical orderings, exposes representational redundancy masked by accuracy, and correlates with robustness, making it a practical, theory-backed diagnostic for representation design.

</details>


### [246] [When Distance Distracts: Representation Distance Bias in BT-Loss for Reward Models](https://arxiv.org/abs/2512.06343)
*Tong Xie,Andrew Bai,Yuanhao Ban,Yunqi Hong,Haoyu Li,Cho-jui Hsieh*

Main category: cs.LG

TL;DR: 论文分析了Bradley-Terry损失函数的梯度特性，发现其受表示距离影响，导致小距离对更新弱、大距离对更新强，提出NormBT归一化方法解决这一问题。


<details>
  <summary>Details</summary>
Motivation: Bradley-Terry损失函数是LLM对齐中奖励模型的核心目标函数，但其梯度分析显示存在表示距离偏差问题：小距离对的梯度更新过弱，即使排序错误；大距离对的梯度更新过强，这会影响学习效果。

Method: 提出NormBT方法，通过自适应成对归一化方案平衡表示距离效应，将学习信号聚焦于预测误差。该方法是对BT损失的轻量级、即插即用改进，开销可忽略。

Result: 在各种LLM骨干网络和数据集上，NormBT一致提升了奖励模型性能，在RewardBench的推理类别上取得了超过5%的显著提升，该类别包含许多小距离对。

Conclusion: 这项工作揭示了广泛使用的BT目标函数的关键局限性，并提供了一个简单有效的修正方案，有助于改善LLM对齐中的奖励建模。

Abstract: Reward models are central to Large Language Model (LLM) alignment within the framework of RLHF. The standard objective used in reward modeling is the Bradley-Terry (BT) loss, which learns from pairwise data consisting of a pair of chosen and rejected responses. In this work, we analyze the per-sample gradient of BT-loss and show that its norm scales with two distinct components: (1) the difference in predicted rewards between chosen and rejected responses, which reflects the prediction error, and critically, (2) representation distance between the pair measured in the output space of the final layer. While the first term captures the intended training signal, we show that the second term can significantly impact the update magnitude and misalign learning. Specifically, pairs with small representation distance often receive vanishingly weak updates, even when misranked, while pairs with large distance receive disproportionately strong updates. This leads to gradients from large-distance pairs to overshadow those from small-distance pairs, where fine-grained distinctions are especially important. To overcome this limitation, we propose NormBT, an adaptive pair-wise normalization scheme that balances representation-driven effects and focuses learning signals on prediction error. NormBT is a lightweight, drop-in integration to BT loss with negligible overhead. Across various LLM backbones and datasets, NormBT improves reward model performance consistently, with notable gains of over 5% on the Reasoning category of RewardBench, which contains numerous small-distance pairs. This work reveals a key limitation in the widely used BT objective and provides a simple, effective correction.

</details>


### [247] [LLM-Upgraded Graph Reinforcement Learning for Carbon-Aware Job Scheduling in Smart Manufacturing](https://arxiv.org/abs/2512.06351)
*Zhiying Yang,Fang Liu,Wei Zhang,Xin Lou,Malcolm Yoke Hean Low,Boon Ping Gan*

Main category: cs.LG

TL;DR: LUCA是一个结合大语言模型和图强化学习的框架，用于碳感知柔性作业车间调度，通过融合图神经网络和LLM的嵌入表示，在保持相同排放水平下显著降低完工时间。


<details>
  <summary>Details</summary>
Motivation: 智能制造系统中动态和可持续调度的挑战，需要同时优化完工时间和碳排放目标，传统方法难以有效处理这种多目标优化问题。

Method: 结合图神经网络和大语言模型，通过精心设计的提示策略生成融合嵌入，捕捉调度状态的结构特征和上下文语义，然后由深度强化学习策略网络生成实时调度决策。

Result: 在合成数据集上，相比最佳对比算法平均降低4.1%、最高降低12.2%的完工时间，同时保持相同排放水平；在公共数据集上，完工时间和排放都有额外改善。

Conclusion: LUCA框架在智能制造的碳感知调度中是有效且实用的，能够同时优化调度效率和可持续性目标。

Abstract: This paper presents \textsc{Luca}, a \underline{l}arge language model (LLM)-\underline{u}pgraded graph reinforcement learning framework for \underline{c}arbon-\underline{a}ware flexible job shop scheduling. \textsc{Luca} addresses the challenges of dynamic and sustainable scheduling in smart manufacturing systems by integrating a graph neural network and an LLM, guided by a carefully designed in-house prompting strategy, to produce a fused embedding that captures both structural characteristics and contextual semantics of the latest scheduling state. This expressive embedding is then processed by a deep reinforcement learning policy network, which generates real-time scheduling decisions optimized for both makespan and carbon emission objectives. To support sustainability goals, \textsc{Luca} incorporates a dual-objective reward function that encourages both energy efficiency and scheduling timeliness. Experimental results on both synthetic and public datasets demonstrate that \textsc{Luca} consistently outperforms comparison algorithms. For instance, on the synthetic dataset, it achieves an average of 4.1\% and up to 12.2\% lower makespan compared to the best-performing comparison algorithm while maintaining the same emission level. On public datasets, additional gains are observed for both makespan and emission. These results demonstrate that \textsc{Luca} is effective and practical for carbon-aware scheduling in smart manufacturing.

</details>


### [248] [DDFI: Diverse and Distribution-aware Missing Feature Imputation via Two-step Reconstruction](https://arxiv.org/abs/2512.06356)
*Yifan Song,Fenglin Yu,Yihong Luo,Xingjian Tao,Siya Qiu,Kai Han,Jing Tang*

Main category: cs.LG

TL;DR: DDFI提出了一种多样化和分布感知的缺失特征补全方法，通过结合特征传播和图掩码自编码器，解决了图神经网络中节点特征缺失的问题。


<details>
  <summary>Details</summary>
Motivation: 现实场景中节点特征经常不完整（如用户属性部分隐私），导致图神经网络性能显著下降。现有特征传播方法存在三个主要问题：1）难以处理非全连接图，2）补全特征面临过平滑问题，3）仅适用于转导任务，忽略了归纳任务中的特征分布偏移。

Method: DDFI结合特征传播和图掩码自编码器，提出：1）Co-Label Linking算法，随机连接训练集中相同标签的节点以增强多连通分量图的性能；2）两阶段推理表示生成过程，先通过特征传播补全特征，再通过整个MAE重构特征以减少分布偏移并增强特征多样性。

Result: 在六个公共数据集和新收集的Sailing数据集（包含自然缺失特征的航行记录）上的实验表明，DDFI在转导和归纳设置下均优于现有最先进方法。

Conclusion: DDFI通过创新的特征补全方法有效解决了图神经网络中节点特征缺失问题，特别是在处理非全连接图、避免过平滑以及应对归纳任务中的分布偏移方面表现优异。

Abstract: Incomplete node features are ubiquitous in real-world scenarios, e.g., the attributes of web users may be partly private, which causes the performance of Graph Neural Networks (GNNs) to decline significantly. Feature propagation (FP) is a well-known method that performs well for imputation of missing node features on graphs, but it still has the following three issues: 1) it struggles with graphs that are not fully connected, 2) imputed features face the over-smoothing problem, and 3) FP is tailored for transductive tasks, overlooking the feature distribution shift in inductive tasks. To address these challenges, we introduce DDFI, a Diverse and Distribution-aware Missing Feature Imputation method that combines feature propagation with a graph-based Masked AutoEncoder (MAE) in a nontrivial manner. It first designs a simple yet effective algorithm, namely Co-Label Linking (CLL), that randomly connects nodes in the training set with the same label to enhance the performance on graphs with numerous connected components. Then we develop a novel two-step representation generation process at the inference stage. Specifically, instead of directly using FP-imputed features as input during inference, DDFI further reconstructs the features through the whole MAE to reduce feature distribution shift in the inductive tasks and enhance the diversity of node features. Meanwhile, since existing feature imputation methods for graphs only evaluate by simulating the missing scenes with manually masking the features, we collect a new dataset called Sailing from the records of voyages that contains naturally missing features to help better evaluate the effectiveness. Extensive experiments conducted on six public datasets and Sailing show that DDFI outperforms the state-of-the-art methods under both transductive and inductive settings.

</details>


### [249] [Proportional integral derivative booster for neural networks-based time-series prediction: Case of water demand prediction](https://arxiv.org/abs/2512.06357)
*Tony Sallooma,Okyay Kaynak,Xinbo Yub,Wei He*

Main category: cs.LG

TL;DR: 提出基于PID控制思想的神经网络预测增强方法，用于提升周期性时间序列多步预测精度，同时保持系统复杂度基本不变


<details>
  <summary>Details</summary>
Motivation: 多步时间序列预测在工业决策中至关重要，但现有神经网络方法结构复杂且预测精度仍有提升空间，需要一种既能提高精度又不显著增加系统复杂度的方法

Method: 受PID控制方法启发，在神经网络预测的每个时间步上应用PID-based booster，将预测值向真实值修正。该方法在用水需求预测和能源消耗预测两个案例中进行验证

Result: 与原始预测模型相比，PID增强方法在预测精度和系统复杂度方面均表现出优越性，有效提升了周期性时间序列的多步预测性能

Conclusion: 基于PID控制思想的增强方法能够有效提升神经网络模型在周期性时间序列多步预测中的性能，且对系统复杂度影响极小，具有广泛适用性

Abstract: Multi-step time-series prediction is an essential supportive step for decision-makers in several industrial areas. Artificial intelligence techniques, which use a neural network component in various forms, have recently frequently been used to accomplish this step. However, the complexity of the neural network structure still stands up as a critical problem against prediction accuracy. In this paper, a method inspired by the proportional-integral-derivative (PID) control approach is investigated to enhance the performance of neural network models used for multi-step ahead prediction of periodic time-series information while maintaining a negligible impact on the complexity of the system. The PID-based method is applied to the predicted value at each time step to bring that value closer to the real value. The water demand forecasting problem is considered as a case study, where two deep neural network models from the literature are used to prove the effectiveness of the proposed boosting method. Furthermore, to prove the applicability of this PID-based booster to other types of periodic time-series prediction problems, it is applied to enhance the accuracy of a neural network model used for multi-step forecasting of hourly energy consumption. The comparison between the results of the original prediction models and the results after using the proposed technique demonstrates the superiority of the proposed method in terms of prediction accuracy and system complexity.

</details>


### [250] [RLAX: Large-Scale, Distributed Reinforcement Learning for Large Language Models on TPUs](https://arxiv.org/abs/2512.06392)
*Runlong Zhou,Lefan Zhang,Shang-Chen Wu,Kelvin Zou,Hanzhi Zhou,Ke Ye,Yihao Feng,Dong Yin,Alex Guillen Garcia,Dmytro Babych,Rohit Chatterjee,Matthew Hopkins,Xiang Kong,Chang Lan,Lezhi Li,Yiping Ma,Daniele Molinari,Senyu Tong,Yanchao Sun,Thomas Voice,Jianyu Wang,Chong Wang,Simon Wang,Floris Weers,Yechen Xu,Guolin Yin,Muyang Yu,Yi Zhang,Zheng Zhou,Danyang Zhuo,Ruoming Pang,Cheng Leong*

Main category: cs.LG

TL;DR: RLAX：一个在TPU上运行的可扩展强化学习框架，用于提升大语言模型的推理能力，通过参数服务器架构和系统优化技术实现高效训练。


<details>
  <summary>Details</summary>
Motivation: 强化学习已成为提升大语言模型推理能力的主要范式，但需要可扩展且能处理训练中断的框架来支持大规模训练。

Method: 采用参数服务器架构，主训练器定期推送更新权重，推理工作器拉取最新权重生成新数据；引入系统技术实现可扩展和可中断的RL训练；开发新的数据集整理和对齐技术加速收敛。

Result: 在1024个v5p TPU上仅用12小时48分钟，将QwQ-32B模型的pass@8准确率提升12.8%，同时保持对训练中断的鲁棒性。

Conclusion: RLAX框架成功实现了大规模、高效的强化学习训练，显著提升了大语言模型的推理性能，为RL在LLM训练中的实际应用提供了可行方案。

Abstract: Reinforcement learning (RL) has emerged as the de-facto paradigm for improving the reasoning capabilities of large language models (LLMs). We have developed RLAX, a scalable RL framework on TPUs. RLAX employs a parameter-server architecture. A master trainer periodically pushes updated model weights to the parameter server while a fleet of inference workers pull the latest weights and generates new rollouts. We introduce a suite of system techniques to enable scalable and preemptible RL for a diverse set of state-of-art RL algorithms. To accelerate convergence and improve model quality, we have devised new dataset curation and alignment techniques. Large-scale evaluations show that RLAX improves QwQ-32B's pass@8 accuracy by 12.8% in just 12 hours 48 minutes on 1024 v5p TPUs, while remaining robust to preemptions during training.

</details>


### [251] [Hankel-FNO: Fast Underwater Acoustic Charting Via Physics-Encoded Fourier Neural Operator](https://arxiv.org/abs/2512.06417)
*Yifan Sun,Lei Cheng,Jianlong Li,Peter Gerstoft*

Main category: cs.LG

TL;DR: Hankel-FNO：基于傅里叶神经算子的高效水下声学制图模型，结合声传播知识和地形数据，在保持高计算速度的同时实现高精度，特别擅长长距离预测。


<details>
  <summary>Details</summary>
Motivation: 传统水下声学制图方法依赖计算昂贵的数值求解器，无法满足大规模或实时应用需求。现有的深度学习替代模型存在固定分辨率限制或依赖显式偏微分方程公式等问题，限制了其在多样化环境中的适用性和泛化能力。

Method: 提出Hankel-FNO模型，基于傅里叶神经算子框架，结合声传播知识和海底地形数据，构建高效准确的水下声学制图方法。

Result: Hankel-FNO在速度上优于传统求解器，在精度上超越数据驱动替代方法，特别是在长距离预测方面表现突出。实验表明模型对多样化环境和声源设置具有良好的适应性，仅需少量微调。

Conclusion: Hankel-FNO为水下声学制图提供了一种高效准确的解决方案，能够满足大规模和实时应用需求，具有良好的环境适应性和泛化能力。

Abstract: Fast and accurate underwater acoustic charting is crucial for downstream tasks such as environment-aware sensor placement optimization and autonomous vehicle path planning. Conventional methods rely on computationally expensive while accurate numerical solvers, which are not scalable for large-scale or real-time applications. Although deep learning-based surrogate models can accelerate these computations, they often suffer from limitations such as fixed-resolution constraints or dependence on explicit partial differential equation formulations. These issues hinder their applicability and generalization across diverse environments. We propose Hankel-FNO, a Fourier Neural Operator (FNO)-based model for efficient and accurate acoustic charting. By incorporating sound propagation knowledge and bathymetry, our method has high accuracy while maintaining high computational speed. Results demonstrate that Hankel-FNO outperforms traditional solvers in speed and surpasses data-driven alternatives in accuracy, especially in long-range predictions. Experiments show the model's adaptability to diverse environments and sound source settings with minimal fine-tuning.

</details>


### [252] [A new initialisation to Control Gradients in Sinusoidal Neural network](https://arxiv.org/abs/2512.06427)
*Andrea Combette,Antoine Venaille,Nelly Pustelnik*

Main category: cs.LG

TL;DR: 提出一种针对正弦激活函数网络（如SIREN）的新初始化方法，通过控制梯度和预激活分布来改善训练和泛化性能


<details>
  <summary>Details</summary>
Motivation: 现有初始化方法对梯度爆炸/消失问题缺乏精确理论理解，特别是对于正弦激活函数网络，需要更好的初始化策略来控制梯度缩放和训练动态

Method: 通过预激活分布收敛和雅可比矩阵序列方差的固定点推导出参数的闭式初始化表达式，控制梯度并针对预激活消失，防止不适当频率的出现

Result: 新初始化方法在函数拟合和图像重建任务上一致优于原始SIREN方案和其他基线方法，包括物理信息神经网络任务

Conclusion: 提出的初始化策略通过精确的梯度控制和预激活分布管理，显著改善了正弦激活函数网络的训练动态和泛化能力

Abstract: Proper initialisation strategy is of primary importance to mitigate gradient explosion or vanishing when training neural networks. Yet, the impact of initialisation parameters still lacks a precise theoretical understanding for several well-established architectures. Here, we propose a new initialisation for networks with sinusoidal activation functions such as \texttt{SIREN}, focusing on gradients control, their scaling with network depth, their impact on training and on generalization. To achieve this, we identify a closed-form expression for the initialisation of the parameters, differing from the original \texttt{SIREN} scheme. This expression is derived from fixed points obtained through the convergence of pre-activation distribution and the variance of Jacobian sequences. Controlling both gradients and targeting vanishing pre-activation helps preventing the emergence of inappropriate frequencies during estimation, thereby improving generalization. We further show that this initialisation strongly influences training dynamics through the Neural Tangent Kernel framework (NTK). Finally, we benchmark \texttt{SIREN} with the proposed initialisation against the original scheme and other baselines on function fitting and image reconstruction. The new initialisation consistently outperforms state-of-the-art methods across a wide range of reconstruction tasks, including those involving physics-informed neural networks.

</details>


### [253] [Small-Gain Nash: Certified Contraction to Nash Equilibria in Differentiable Games](https://arxiv.org/abs/2512.06791)
*Vedansh Sharma*

Main category: cs.LG

TL;DR: 提出SGN（Small-Gain Nash）方法，通过自定义块加权几何中的块小增益条件，为梯度学习提供收敛保证，即使伪梯度在欧几里得几何中非单调。


<details>
  <summary>Details</summary>
Motivation: 传统博弈中基于梯度的学习收敛性要求伪梯度在欧几里得几何中具有（强）单调性，但这一条件即使在具有强跨玩家耦合的简单博弈中也经常失败。需要一种更通用的收敛性认证方法。

Method: 引入SGN（Small-Gain Nash）方法，在自定义块加权几何中建立块小增益条件。将局部曲率和跨玩家Lipschitz耦合边界转化为可处理的收缩证书。构建加权块度量，使伪梯度在该度量下具有强单调性，即使它在欧几里得意义下非单调。

Result: 连续流在设计的几何中呈指数收缩，投影Euler和RK4离散化在显式步长边界下收敛。在二次博弈中验证了框架的有效性，即使欧几里得单调性分析失败，SGN也能成功认证收敛性。扩展到马尔可夫博弈中的镜像/Fisher几何进行熵正则化策略梯度。

Conclusion: SGN提供了一种离线认证流程，可在紧凑区域估计曲率、耦合和Lipschitz参数，优化块权重以扩大SGN边界，为非单调博弈返回包含度量、收缩率和安全步长的结构性可计算收敛证书。

Abstract: Classical convergence guarantees for gradient-based learning in games require the pseudo-gradient to be (strongly) monotone in Euclidean geometry as shown by rosen(1965), a condition that often fails even in simple games with strong cross-player couplings. We introduce Small-Gain Nash (SGN), a block small-gain condition in a custom block-weighted geometry. SGN converts local curvature and cross-player Lipschitz coupling bounds into a tractable certificate of contraction. It constructs a weighted block metric in which the pseudo-gradient becomes strongly monotone on any region where these bounds hold, even when it is non-monotone in the Euclidean sense. The continuous flow is exponentially contracting in this designed geometry, and projected Euler and RK4 discretizations converge under explicit step-size bounds derived from the SGN margin and a local Lipschitz constant. Our analysis reveals a certified ``timescale band'', a non-asymptotic, metric-based certificate that plays a TTUR-like role: rather than forcing asymptotic timescale separation via vanishing, unequal step sizes, SGN identifies a finite band of relative metric weights for which a single-step-size dynamics is provably contractive. We validate the framework on quadratic games where Euclidean monotonicity analysis fails to predict convergence, but SGN successfully certifies it, and extend the construction to mirror/Fisher geometries for entropy-regularized policy gradient in Markov games. The result is an offline certification pipeline that estimates curvature, coupling, and Lipschitz parameters on compact regions, optimizes block weights to enlarge the SGN margin, and returns a structural, computable convergence certificate consisting of a metric, contraction rate, and safe step-sizes for non-monotone games.

</details>


### [254] [Neural expressiveness for beyond importance model compression](https://arxiv.org/abs/2512.06440)
*Angelos-Christos Maroudis,Sotirios Xydis*

Main category: cs.LG

TL;DR: 提出新的神经网络剪枝标准"表达性"，基于神经元激活重叠来评估信息重分配能力，实现数据无关的剪枝策略，在YOLOv8上达到55.4%参数减少和46.1%计算量降低，同时提升检测精度。


<details>
  <summary>Details</summary>
Motivation: 现有剪枝方法主要依赖权重的重要性评估，但缺乏对神经元信息重分配能力的考量。本文旨在探索更基础的剪枝标准，解决"何时剪枝"的问题，并实现数据无关的压缩策略。

Method: 提出"表达性"作为新的剪枝标准，基于神经元激活重叠来评估其信息重分配能力。该方法与网络初始化状态强相关，可仅用少量代表性数据或无数据实现，支持与重要性剪枝的混合策略。

Result: 在YOLOv8上实现55.4%参数减少和46.1%MACs降低，mAP50-95提升3%；相比权重方法获得10倍额外压缩增益，平均性能下降仅1%；表达性独立使用时也优于现有方法。

Conclusion: 表达性作为新的剪枝标准，为神经网络压缩提供了更基础的理论框架，支持数据无关策略，与重要性剪枝互补，在保持性能的同时实现显著的计算和存储效率提升。

Abstract: Neural Network Pruning has been established as driving force in the exploration of memory and energy efficient solutions with high throughput both during training and at test time. In this paper, we introduce a novel criterion for model compression, named "Expressiveness". Unlike existing pruning methods that rely on the inherent "Importance" of neurons' and filters' weights, ``Expressiveness" emphasizes a neuron's or group of neurons ability to redistribute informational resources effectively, based on the overlap of activations. This characteristic is strongly correlated to a network's initialization state, establishing criterion autonomy from the learning state stateless and thus setting a new fundamental basis for the expansion of compression strategies in regards to the "When to Prune" question. We show that expressiveness is effectively approximated with arbitrary data or limited dataset's representative samples, making ground for the exploration of Data-Agnostic strategies. Our work also facilitates a "hybrid" formulation of expressiveness and importance-based pruning strategies, illustrating their complementary benefits and delivering up to 10x extra gains w.r.t. weight-based approaches in parameter compression ratios, with an average of 1% in performance degradation. We also show that employing expressiveness (independently) for pruning leads to an improvement over top-performing and foundational methods in terms of compression efficiency. Finally, on YOLOv8, we achieve a 46.1% MACs reduction by removing 55.4\% of the parameters, with an increase of 3% in the mean Absolute Precision ($mAP_{50-95}$) for object detection on COCO dataset.

</details>


### [255] [BitStopper: An Efficient Transformer Attention Accelerator via Stage-fusion and Early Termination](https://arxiv.org/abs/2512.06457)
*Huizheng Wang,Hongbin Wang,Shaojun Wei,Yang Hu,Shouyi Yin*

Main category: cs.LG

TL;DR: BitStopper是一种细粒度算法-架构协同设计，通过位级稀疏推测和无预测器动态注意力机制，显著降低Transformer计算和内存开销。


<details>
  <summary>Details</summary>
Motivation: 基于注意力的大语言模型存在二次方计算成本，动态稀疏注意力虽然能缓解但硬件效率受限，主要问题在于预测阶段开销和内存流量过大。

Method: 提出BitStopper框架：1) 位串行使能阶段融合机制，重用内存访问并合并预测与执行阶段；2) 轻量自适应token选择策略；3) 位级异步处理策略；4) 精心设计的硬件架构。

Result: 相比SOTA Transformer加速器，BitStopper在Sanger和SOFA上分别实现2.03倍和1.89倍加速，能效提升2.4倍和2.1倍。

Conclusion: BitStopper通过算法-架构协同设计有效解决了动态稀疏注意力的硬件效率问题，实现了显著的性能提升和能效改进。

Abstract: Attention-based large language models (LLMs) have transformed modern AI applications, but the quadratic cost of self-attention imposes significant compute and memory overhead. Dynamic sparsity (DS) attention mitigates this, yet its hardware efficiency is limited by the added prediction stage and the heavy memory traffic it entails. To address these limitations, this paper proposes BitStopper, a fine-grained algorithm-architecture co-design that operates without a sparsity predictor. First, a bit-serial enable stage fusion (BESF) mechanism is proposed to reuse and minimize the memory access by progressively terminating trivial tokens and merging the prediction stage into the execution stage. Second, a lightweight and adaptive token selection (LATS) strategy is developed to work in concert with the bit-level sparsity speculation. Third, a bit-level asynchronous processing (BAP) strategy is employed to improve compute utilization during the on-demand bit-grained memory fetching. Finally, an elaborate architecture is designed to translate the theoretical complexity reduction into practical performance improvement. Extensive evaluations demonstrate that, compared to state-of-the-art (SOTA) Transformer accelerators, BitStopper achieves 2.03x and 1.89x speedups over Sanger and SOFA, respectively, while delivering 2.4x and 2.1x improvements in energy efficiency.

</details>


### [256] [Efficient Low-Tubal-Rank Tensor Estimation via Alternating Preconditioned Gradient Descent](https://arxiv.org/abs/2512.07490)
*Zhiyu Liu,Zhi Han,Yandong Tang,Jun Fan,Yao Wang*

Main category: cs.LG

TL;DR: 提出交替预条件梯度下降算法，解决低管秩张量估计中过参数化导致的收敛缓慢问题，实现与张量条件数无关的线性收敛。


<details>
  <summary>Details</summary>
Motivation: 传统张量奇异值分解计算昂贵，不适用于大规模张量；现有梯度下降方法需要准确估计张量秩，当秩被高估时收敛显著变慢甚至发散。

Method: 提出交替预条件梯度下降算法，在原始梯度基础上添加预条件项，交替更新两个因子张量，加速过参数化设置下的收敛。

Result: 在目标函数的几何假设下，为一般低管秩张量估计问题建立了线性收敛保证；具体分析了张量分解和张量恢复两种情形；理论结果表明APGD在过参数化下仍能实现线性收敛，且收敛率与张量条件数无关。

Conclusion: APGD算法有效解决了低管秩张量估计中过参数化导致的收敛问题，在合成数据上的广泛模拟验证了理论断言。

Abstract: The problem of low-tubal-rank tensor estimation is a fundamental task with wide applications across high-dimensional signal processing, machine learning, and image science. Traditional approaches tackle such a problem by performing tensor singular value decomposition, which is computationally expensive and becomes infeasible for large-scale tensors. Recent approaches address this issue by factorizing the tensor into two smaller factor tensors and solving the resulting problem using gradient descent. However, this kind of approach requires an accurate estimate of the tensor rank, and when the rank is overestimated, the convergence of gradient descent and its variants slows down significantly or even diverges. To address this problem, we propose an Alternating Preconditioned Gradient Descent (APGD) algorithm, which accelerates convergence in the over-parameterized setting by adding a preconditioning term to the original gradient and updating these two factors alternately. Based on certain geometric assumptions on the objective function, we establish linear convergence guarantees for more general low-tubal-rank tensor estimation problems. Then we further analyze the specific cases of low-tubal-rank tensor factorization and low-tubal-rank tensor recovery. Our theoretical results show that APGD achieves linear convergence even under over-parameterization, and the convergence rate is independent of the tensor condition number. Extensive simulations on synthetic data are carried out to validate our theoretical assertions.

</details>


### [257] [Why Goal-Conditioned Reinforcement Learning Works: Relation to Dual Control](https://arxiv.org/abs/2512.06471)
*Nathan P. Lawrence,Ali Mesbah*

Main category: cs.LG

TL;DR: 本文分析了基于最优控制的目标条件强化学习，推导了传统二次型目标与目标条件奖励之间的最优性差距，并将状态估计与概率奖励联系起来，验证了目标条件策略在非线性不确定环境中的优势。


<details>
  <summary>Details</summary>
Motivation: 目标条件强化学习关注训练智能体达到目标状态的概率最大化问题。传统密集奖励（如二次型）在复杂环境中可能失效，需要理解目标条件RL为何成功以及传统方法为何失败的理论基础。

Method: 基于最优控制理论分析目标条件设置，推导传统二次型目标与目标条件奖励之间的最优性差距。在部分可观测马尔可夫决策过程中，将状态估计与概率奖励联系起来，适用于双重控制问题。使用强化学习和预测控制技术在非线性不确定环境中验证。

Result: 阐明了目标条件RL成功的原因和传统密集奖励失败的原因，建立了状态估计与概率奖励的理论联系，在非线性不确定环境中验证了目标条件策略相对于传统方法的优势。

Conclusion: 目标条件强化学习在最优控制框架下具有理论优势，特别适用于部分可观测环境和双重控制问题，为复杂环境中的策略设计提供了更有效的理论和方法基础。

Abstract: Goal-conditioned reinforcement learning (RL) concerns the problem of training an agent to maximize the probability of reaching target goal states. This paper presents an analysis of the goal-conditioned setting based on optimal control. In particular, we derive an optimality gap between more classical, often quadratic, objectives and the goal-conditioned reward, elucidating the success of goal-conditioned RL and why classical ``dense'' rewards can falter. We then consider the partially observed Markov decision setting and connect state estimation to our probabilistic reward, further making the goal-conditioned reward well suited to dual control problems. The advantages of goal-conditioned policies are validated on nonlinear and uncertain environments using both RL and predictive control techniques.

</details>


### [258] [Optimizing LLMs Using Quantization for Mobile Execution](https://arxiv.org/abs/2512.06490)
*Agatsya Yadav,Renta Chintala Bhargavi*

Main category: cs.LG

TL;DR: 该论文研究了使用4位后训练量化技术压缩大型语言模型，使其能够在移动设备上运行，成功将Llama 3.2 3B模型大小减少68.66%并在Android设备上部署。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然功能强大，但其庞大的规模和计算需求阻碍了在资源受限的移动设备上的部署，需要寻找有效的压缩和优化方法。

Method: 使用BitsAndBytes库和Hugging Face Transformers框架对Meta的Llama 3.2 3B模型进行4位后训练量化，然后通过llama.cpp工具转换为GGUF格式以优化移动推理。

Result: 通过4位量化实现了68.66%的模型大小缩减，量化后的模型能够在Android设备上成功执行推理任务，使用Termux环境和Ollama框架验证了部署可行性。

Conclusion: 4位精度的后训练量化结合GGUF等移动优化格式，为在移动设备上部署功能强大的大型语言模型提供了实用途径，平衡了模型大小和性能。

Abstract: Large Language Models (LLMs) offer powerful capabilities, but their significant size and computational requirements hinder deployment on resource-constrained mobile devices. This paper investigates Post-Training Quantization (PTQ) for compressing LLMs for mobile execution. We apply 4-bit PTQ using the BitsAndBytes library with the Hugging Face Transformers framework to Meta's Llama 3.2 3B model. The quantized model is converted to GGUF format using llama.cpp tools for optimized mobile inference. The PTQ workflow achieves a 68.66% reduction in model size through 4-bit quantization, enabling the Llama 3.2 3B model to run efficiently on an Android device. Qualitative validation shows that the 4-bit quantized model can perform inference tasks successfully. We demonstrate the feasibility of running the quantized GGUF model on an Android device using the Termux environment and the Ollama framework. PTQ, especially at 4-bit precision combined with mobile-optimized formats like GGUF, provides a practical pathway for deploying capable LLMs on mobile devices, balancing model size and performance.

</details>


### [259] [Diagnosis-based mortality prediction for intensive care unit patients via transfer learning](https://arxiv.org/abs/2512.06511)
*Mengqi Xu,Subha Maity,Joel Dubin*

Main category: cs.LG

TL;DR: 该研究评估了迁移学习方法在ICU诊断特异性死亡率预测中的应用，发现迁移学习优于仅使用诊断特定数据或APACHE IVa评分的方法，且Youden截断值比传统0.5阈值更合适。


<details>
  <summary>Details</summary>
Motivation: ICU中危重疾病的根本原因在不同诊断间差异很大，但考虑诊断异质性的预测模型尚未得到系统研究。现有模型未能充分处理诊断特异性差异，需要更精准的预测方法。

Method: 使用eICU协作研究数据库，评估基于GLM和XGBoost的迁移学习方法，用于诊断特异性死亡率预测。比较迁移学习与仅使用诊断特定数据、APACHE IVa评分以及合并数据训练的模型。

Result: 迁移学习在诊断特异性死亡率预测中始终优于仅使用诊断特定数据的模型和APACHE IVa评分，同时比合并数据训练的模型具有更好的校准性。Youden截断值比传统0.5阈值更合适，且迁移学习在不同截断标准下保持高预测性能。

Conclusion: 迁移学习是ICU诊断特异性死亡率预测的有效方法，能处理诊断异质性并提高预测准确性。Youden截断值应作为更合适的决策阈值，迁移学习在不同阈值标准下表现稳定。

Abstract: In the intensive care unit, the underlying causes of critical illness vary substantially across diagnoses, yet prediction models accounting for diagnostic heterogeneity have not been systematically studied. To address the gap, we evaluate transfer learning approaches for diagnosis-specific mortality prediction and apply both GLM- and XGBoost-based models to the eICU Collaborative Research Database. Our results demonstrate that transfer learning consistently outperforms models trained only on diagnosis-specific data and those using a well-known ICU severity-of-illness score, i.e., APACHE IVa, alone, while also achieving better calibration than models trained on the pooled data. Our findings also suggest that the Youden cutoff is a more appropriate decision threshold than the conventional 0.5 for binary outcomes, and that transfer learning maintains consistently high predictive performance across various cutoff criteria.

</details>


### [260] [LLM-Driven Composite Neural Architecture Search for Multi-Source RL State Encoding](https://arxiv.org/abs/2512.06982)
*Yu Yu,Qian Xie,Nairen Cao,Li Jin*

Main category: cs.LG

TL;DR: 提出基于LLM的神经架构搜索方法，用于多源强化学习中的状态编码器设计，相比传统NAS方法更高效


<details>
  <summary>Details</summary>
Motivation: 多源强化学习（传感器测量、时序信号、图像观测、文本指令等）中的状态编码器设计缺乏系统方法，现有NAS方法忽略中间输出的有用信息，导致样本效率低下

Method: 将多源状态编码器设计形式化为复合神经架构搜索问题，提出LLM驱动的NAS流程，利用语言模型先验和中间输出信号指导搜索

Result: 在混合自主交通控制任务中，相比传统NAS基准和LLM-based GENIUS框架，该方法以更少的候选评估发现性能更高的架构

Conclusion: LLM驱动的NAS方法能够有效利用先验知识和中间信号，在多源强化学习场景中实现更高效的复合状态编码器搜索

Abstract: Designing state encoders for reinforcement learning (RL) with multiple information sources -- such as sensor measurements, time-series signals, image observations, and textual instructions -- remains underexplored and often requires manual design. We formalize this challenge as a problem of composite neural architecture search (NAS), where multiple source-specific modules and a fusion module are jointly optimized. Existing NAS methods overlook useful side information from the intermediate outputs of these modules -- such as their representation quality -- limiting sample efficiency in multi-source RL settings. To address this, we propose an LLM-driven NAS pipeline that leverages language-model priors and intermediate-output signals to guide sample-efficient search for high-performing composite state encoders. On a mixed-autonomy traffic control task, our approach discovers higher-performing architectures with fewer candidate evaluations than traditional NAS baselines and the LLM-based GENIUS framework.

</details>


### [261] [Hierarchical geometric deep learning enables scalable analysis of molecular dynamics](https://arxiv.org/abs/2512.06520)
*Zihan Pengmei,Spencer C. Guo,Chatipat Lorpaiboon,Aaron R. Dinner*

Main category: cs.LG

TL;DR: 提出一种基于图神经网络的方法，通过局部信息聚合来高效分析大规模生物分子动力学模拟，减少内存和计算需求，同时保持原子级细节。


<details>
  <summary>Details</summary>
Motivation: 分子动力学模拟产生原子级详细轨迹，但缺乏定量描述符时分析困难。传统图神经网络处理大规模生物分子系统（超过几百个残基）时面临长程相互作用捕获困难、内存和运行时间需求大的挑战。

Method: 开发一种基于图神经网络的方法，通过局部信息聚合来减少内存和运行时间需求，同时保持原子级细节。该方法能够处理数千残基的蛋白质-核酸复合物系统。

Result: 该方法能够在单GPU上几分钟内分析数千残基的蛋白质-核酸复合物模拟。对于数百残基的系统，该方法提高了性能表现和可解释性。

Conclusion: 局部信息聚合方法为大规模生物分子动力学分析提供了高效解决方案，克服了传统图神经网络在内存和计算效率方面的限制，同时保持了原子级细节和系统性能。

Abstract: Molecular dynamics simulations can generate atomically detailed trajectories of complex systems, but analyzing these dynamics can be challenging when systems lack well-established quantitative descriptors (features). Graph neural networks (GNNs) in which messages are passed between nodes that represent atoms that are spatial neighbors promise to obviate manual feature engineering, but the use of GNNs with biomolecular systems of more than a few hundred residues has been limited in the context of analyzing dynamics by both difficulties in capturing the details of long-range interactions with message passing and the memory and runtime requirements associated with large graphs. Here, we show how local information can be aggregated to reduce memory and runtime requirements without sacrificing atomic detail. We demonstrate that this approach opens the door to analyzing simulations of protein-nucleic acid complexes with thousands of residues on single GPUs within minutes. For systems with hundreds of residues, for which there are sufficient data to make quantitative comparisons, we show that the approach improves performance and interpretability.

</details>


### [262] [Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning](https://arxiv.org/abs/2512.06533)
*Ming Chen,Sheng Tang,Rong-Xi Tan,Ziniu Li,Jiacheng Chen,Ke Xue,Chao Qian*

Main category: cs.LG

TL;DR: 本文提出使用强化学习改进基于解码的回归方法，通过序列级奖励增强数值预测的全局一致性，在表格回归和代码度量回归任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 基于解码的回归方法将回归任务重新定义为序列生成任务，但现有方法存在离散token级目标与连续数值之间的不对齐问题。token级约束往往无法捕捉目标值的全局幅度，限制了预测精度和泛化能力。

Method: 提出使用强化学习解锁基于解码的回归潜力，将生成过程建模为马尔可夫决策过程，利用序列级奖励来强制全局数值一致性。具体采用ReMax和GRPO方法。

Result: 在表格回归和代码度量回归任务上的大量实验表明，该方法（特别是使用ReMax和GRPO）始终优于最先进的token级基线方法和传统回归头，展示了引入序列级信号的优势。

Conclusion: 强化学习显著提高了采样效率和预测精度，确立了基于解码的回归作为通用数值预测的稳健且准确的范式。

Abstract: Decoding-based regression, which reformulates regression as a sequence generation task, has emerged as a promising paradigm of applying large language models for numerical prediction. However, its progress is hindered by the misalignment between discrete token-level objectives (e.g., cross-entropy) and continuous numerical values. Existing approaches relying on token-level constraints often fail to capture the global magnitude of the target value, limiting their precision and generalization. In this paper, we propose to unlock the potential of decoding-based regression via Reinforcement Learning (RL). We formulate the generation process as a Markov Decision Process, utilizing sequence-level rewards to enforce global numerical coherence. Extensive experiments on tabular regression and code metric regression demonstrate that our method (specifically with ReMax and GRPO) consistently outperforms both state-of-the-art token-level baselines and traditional regression heads, showing the superiority of introducing sequence-level signals. Our analysis further reveals that RL significantly enhances sampling efficiency and predictive precision, establishing decoding-based regression as a robust and accurate paradigm for general-purpose numerical prediction.

</details>


### [263] [A-3PO: Accelerating Asynchronous LLM Training with Staleness-aware Proximal Policy Approximation](https://arxiv.org/abs/2512.06547)
*Xiaocan Li,Shiliang Wu,Zheng Shen*

Main category: cs.LG

TL;DR: A-3PO通过近似计算代理策略来消除异步强化学习中解耦损失的额外计算开销，减少18%训练时间同时保持性能


<details>
  <summary>Details</summary>
Motivation: 解耦损失在异步RL中处理数据陈旧性很有效，但代理策略需要额外的前向传播，对大型语言模型造成计算瓶颈

Method: 提出A-3PO方法，通过简单插值近似代理策略，避免显式计算，消除额外计算开销

Result: 减少18%训练时间，同时保持与原始方法相当的性能表现

Conclusion: A-3PO有效解决了异步RL中解耦损失的计算瓶颈问题，为大型语言模型的高效训练提供了实用方案

Abstract: Decoupled loss has been a successful reinforcement learning (RL) algorithm to deal with the high data staleness under the asynchronous RL setting. Decoupled loss improves coupled-loss style of algorithms' (e.g., PPO, GRPO) learning stability by introducing a proximal policy to decouple the off-policy corrections (importance weight) from the controlling policy updates (trust region). However, the proximal policy requires an extra forward pass through the network at each training step, creating a computational bottleneck for large language models. We observe that since the proximal policy only serves as a trust region anchor between the behavior and target policies, we can approximate it through simple interpolation without explicit computation. We call this approach A-3PO (APproximated Proximal Policy Optimization). A-3PO eliminates this overhead, reducing training time by 18% while maintaining comparable performance. Code & off-the-shelf example are available at: https://github.com/inclusionAI/AReaL/blob/main/docs/algorithms/prox_approx.md

</details>


### [264] [Deep Manifold Part 2: Neural Network Mathematics](https://arxiv.org/abs/2512.06563)
*Max Y. Ma,Gen-Hua Shi*

Main category: cs.LG

TL;DR: 该论文提出从流形几何、不动点理论和边界条件迭代的角度重新理解神经网络，将其视为由数据复杂性、学习复杂性和流形复杂性塑造的可学习数值计算，而非固定坐标和算子的集合。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络理论基于固定坐标和算子，难以解释现实世界数据的高度复杂性（范围广、规模大、小批量碎片化）以及训练过程中产生的学习复杂性（节点覆盖变化、曲率积累、可塑性涨落）。需要从几何和不动点理论的角度重新理解神经网络的学习机制和局限性。

Method: 通过堆叠分段流形、不动点理论和边界条件迭代来建立神经网络的全局方程。移除固定坐标和算子，将神经网络视为由流形复杂性、高阶非线性和边界条件塑造的可学习数值计算。

Result: 神经网络的能力只有在不动点区域稳定时才会出现，而神经网络并非一开始就具有不动点，而是通过残差驱动的迭代来构建它们。这种视角解释了为什么单一模型在几何和数据诱导的可塑性下存在局限性。

Conclusion: 该研究为分布式架构和联邦系统提供了理论基础，通过将流形复杂性分散到多个弹性模型中，形成一个基于几何、代数、不动点和真实数据复杂性的连贯世界建模框架。

Abstract: This work develops the global equations of neural networks through stacked piecewise manifolds, fixed-point theory, and boundary-conditioned iteration. Once fixed coordinates and operators are removed, a neural network appears as a learnable numerical computation shaped by manifold complexity, high-order nonlinearity, and boundary conditions. Real-world data impose strong data complexity, near-infinite scope, scale, and minibatch fragmentation, while training dynamics produce learning complexity through shifting node covers, curvature accumulation, and the rise and decay of plasticity. These forces constrain learnability and explain why capability emerges only when fixed-point regions stabilize. Neural networks do not begin with fixed points; they construct them through residual-driven iteration. This perspective clarifies the limits of monolithic models under geometric and data-induced plasticity and motivates architectures and federated systems that distribute manifold complexity across many elastic models, forming a coherent world-modeling framework grounded in geometry, algebra, fixed points, and real-data complexity.

</details>


### [265] [QL-LSTM: A Parameter-Efficient LSTM for Stable Long-Sequence Modeling](https://arxiv.org/abs/2512.06582)
*Isaac Kofi Nti*

Main category: cs.LG

TL;DR: QL-LSTM通过参数共享统一门控和分层门控循环改进传统LSTM，减少48%参数并提升长距离信息保留，在IMDB情感分类中表现竞争性但计算速度仍需优化。


<details>
  <summary>Details</summary>
Motivation: 传统循环神经网络（如LSTM和GRU）存在两个核心限制：1）门控特定参数冗余；2）长距离时间信息保留能力下降。需要设计更高效的循环架构来解决这些问题。

Method: 提出QL-LSTM架构，包含两个独立组件：1）参数共享统一门控机制（PSUG），用单一共享权重矩阵替代所有门控特定变换；2）分层门控循环与加法跳跃连接（HGR-ASC），添加无乘法路径改善长距离信息流。

Result: 在IMDB数据集（扩展文档长度）的情感分类任务中，QL-LSTM相比LSTM、GRU和BiLSTM参考模型，使用显著更少的参数（减少约48%）实现了竞争性的准确率。

Conclusion: QL-LSTM在参数效率方面优于传统循环架构，但受限于循环模型的固有顺序特性，当前原型在计算速度上尚未获得优势，需要进一步的内核级优化。

Abstract: Recurrent neural architectures such as LSTM and GRU remain widely used in sequence modeling, but they continue to face two core limitations: redundant gate-specific parameters and reduced ability to retain information across long temporal distances. This paper introduces the Quantum-Leap LSTM (QL-LSTM), a recurrent architecture designed to address both challenges through two independent components. The Parameter-Shared Unified Gating mechanism replaces all gate-specific transformations with a single shared weight matrix, reducing parameters by approximately 48 percent while preserving full gating behavior. The Hierarchical Gated Recurrence with Additive Skip Connections component adds a multiplication-free pathway that improves long-range information flow and reduces forget-gate degradation. We evaluate QL-LSTM on sentiment classification using the IMDB dataset with extended document lengths, comparing it to LSTM, GRU, and BiLSTM reference models. QL-LSTM achieves competitive accuracy while using substantially fewer parameters. Although the PSUG and HGR-ASC components are more efficient per time step, the current prototype remains limited by the inherent sequential nature of recurrent models and therefore does not yet yield wall-clock speed improvements without further kernel-level optimization.

</details>


### [266] [On fine-tuning Boltz-2 for protein-protein affinity prediction](https://arxiv.org/abs/2512.06592)
*James King,Lewis Cornwall,Andrei Cristian Nica,James Day,Aaron Sim,Neil Dalchau,Lilly Wollman,Joshua Meyers*

Main category: cs.LG

TL;DR: 将Boltz-2蛋白质-配体亲和力预测器适配到蛋白质-蛋白质相互作用，但性能不如序列模型，结合两者可互补提升


<details>
  <summary>Details</summary>
Motivation: 准确预测蛋白质-蛋白质结合亲和力对于理解分子相互作用和设计治疗方法至关重要，需要评估结构基方法在蛋白质-蛋白质亲和力预测中的表现

Method: 将最先进的结构基蛋白质-配体亲和力预测器Boltz-2适配为蛋白质-蛋白质亲和力回归模型(Boltz-2-PPI)，在TCR3d和PPB-affinity两个数据集上评估，并与序列基方法比较，还尝试结合结构和序列嵌入

Result: 尽管结构准确性高，但Boltz-2-PPI在大小规模数据上都表现不如序列基替代方法。结合Boltz-2-PPI嵌入和序列基嵌入能带来互补性改进，特别是对较弱的序列模型，表明结构和序列模型学习到了不同的信号

Conclusion: 当前结构基表示方法尚未准备好进行高性能亲和力预测，结果反映了结构数据训练中已知的偏差，结构和序列方法的结合显示出互补优势

Abstract: Accurate prediction of protein-protein binding affinity is vital for understanding molecular interactions and designing therapeutics. We adapt Boltz-2, a state-of-the-art structure-based protein-ligand affinity predictor, for protein-protein affinity regression and evaluate it on two datasets, TCR3d and PPB-affinity. Despite high structural accuracy, Boltz-2-PPI underperforms relative to sequence-based alternatives in both small- and larger-scale data regimes. Combining embeddings from Boltz-2-PPI with sequence-based embeddings yields complementary improvements, particularly for weaker sequence models, suggesting different signals are learned by sequence- and structure-based models. Our results echo known biases associated with training with structural data and suggest that current structure-based representations are not primed for performant affinity prediction.

</details>


### [267] [A Fast and Effective Solution to the Problem of Look-ahead Bias in LLMs](https://arxiv.org/abs/2512.06607)
*Humzah Merchant,Bradford Levy*

Main category: cs.LG

TL;DR: 提出一种通过调整大模型logits来消除前瞻性偏差的方法，使用一对小模型分别针对要遗忘和保留的信息进行微调，在推理时指导生成


<details>
  <summary>Details</summary>
Motivation: 在金融预测任务中应用LLMs面临前瞻性偏差挑战，因为模型训练使用了长时间序列数据，而重新训练前沿模型成本过高，需要一种快速、有效、低成本的替代方案

Method: 在推理时通过调整大基础模型的logits来指导生成，使用一对小模型：一个针对要遗忘的信息微调，另一个针对要保留的信息微调

Result: 该方法能有效消除字面和语义知识，纠正偏差，并且优于先前的方法

Conclusion: 提出了一种解决LLMs在金融预测中前瞻性偏差问题的实用方法，通过推理时调整而非重新训练，实现了快速、低成本的知识遗忘

Abstract: Applying LLMs to predictive tasks in finance is challenging due to look-ahead bias resulting from their training on long time-series data. This precludes the backtests typically employed in finance since retraining frontier models from scratch with a specific knowledge cutoff is prohibitive. In this paper, we introduce a fast, effective, and low-cost alternative. Our method guides generation at inference time by adjusting the logits of a large base model using a pair of smaller, specialized models -- one fine-tuned on information to be forgotten and another on information to be retained. We demonstrate that our method effectively removes both verbatim and semantic knowledge, corrects biases, and outperforms prior methods.

</details>


### [268] [Vector Quantization using Gaussian Variational Autoencoder](https://arxiv.org/abs/2512.06609)
*Tongda Xu,Wendi Zheng,Jiajun He,Jose Miguel Hernandez-Lobato,Yan Wang,Ya-Qin Zhang,Jie Tang*

Main category: cs.LG

TL;DR: 提出Gaussian Quant (GQ)技术，将带约束的高斯VAE转换为VQ-VAE，无需额外训练，通过理论证明和实践验证优于现有方法。


<details>
  <summary>Details</summary>
Motivation: VQ-VAE由于离散化难以训练，需要一种更简单有效的方法来构建离散自编码器。

Method: 提出Gaussian Quant技术：1) 生成随机高斯噪声作为码本；2) 寻找最接近后验均值的噪声；3) 提出目标散度约束(TDC)启发式训练方法。

Result: GQ在UNet和ViT架构上优于VQGAN、FSQ、LFQ、BSQ等现有VQ-VAE方法；TDC也改进了TokenBridge等高斯VAE离散化方法。

Conclusion: GQ提供了一种简单有效的VQ-VAE构建方法，通过理论保证和实验验证了其优越性，为离散表示学习提供了新思路。

Abstract: Vector quantized variational autoencoder (VQ-VAE) is a discrete auto-encoder that compresses images into discrete tokens. It is difficult to train due to discretization. In this paper, we propose a simple yet effective technique, dubbed Gaussian Quant (GQ), that converts a Gaussian VAE with certain constraint into a VQ-VAE without training. GQ generates random Gaussian noise as a codebook and finds the closest noise to the posterior mean. Theoretically, we prove that when the logarithm of the codebook size exceeds the bits-back coding rate of the Gaussian VAE, a small quantization error is guaranteed. Practically, we propose a heuristic to train Gaussian VAE for effective GQ, named target divergence constraint (TDC). Empirically, we show that GQ outperforms previous VQ-VAEs, such as VQGAN, FSQ, LFQ, and BSQ, on both UNet and ViT architectures. Furthermore, TDC also improves upon previous Gaussian VAE discretization methods, such as TokenBridge. The source code is provided in https://github.com/tongdaxu/VQ-VAE-from-Gaussian-VAE.

</details>


### [269] [Quantum Temporal Convolutional Neural Networks for Cross-Sectional Equity Return Prediction: A Comparative Benchmark Study](https://arxiv.org/abs/2512.06630)
*Chi-Sheng Chen,Xinyu Zhang,Rong Fu,Qiuzhe Xie,Fan Zhang*

Main category: cs.LG

TL;DR: 量子时间卷积神经网络(QTCNN)结合经典时间编码器和参数高效量子卷积电路，用于股票横截面收益预测，在JPX东京证券交易所数据集上实现了0.538的夏普比率，比最佳经典基线提升约72%。


<details>
  <summary>Details</summary>
Motivation: 传统预测模型在处理噪声输入、制度转换和有限泛化能力方面存在困难，特别是在复杂、噪声大、高度动态的金融环境中。量子机器学习为增强股票市场预测提供了有前景的途径。

Method: 提出量子时间卷积神经网络(QTCNN)，结合经典时间编码器提取时序技术指标的多尺度模式，以及参数高效的量子卷积电路利用量子叠加和纠缠增强特征表示并抑制过拟合。

Result: 在JPX东京证券交易所数据集上进行综合基准测试，通过构建多空组合并使用样本外夏普比率作为主要性能指标进行评估。QTCNN实现了0.538的夏普比率，比最佳经典基线提升约72%。

Conclusion: 研究结果突显了量子增强预测模型QTCNN在量化金融中稳健决策的实际潜力，为复杂金融环境下的股票预测提供了有效解决方案。

Abstract: Quantum machine learning offers a promising pathway for enhancing stock market prediction, particularly under complex, noisy, and highly dynamic financial environments. However, many classical forecasting models struggle with noisy input, regime shifts, and limited generalization capacity. To address these challenges, we propose a Quantum Temporal Convolutional Neural Network (QTCNN) that combines a classical temporal encoder with parameter-efficient quantum convolution circuits for cross-sectional equity return prediction. The temporal encoder extracts multi-scale patterns from sequential technical indicators, while the quantum processing leverages superposition and entanglement to enhance feature representation and suppress overfitting. We conduct a comprehensive benchmarking study on the JPX Tokyo Stock Exchange dataset and evaluate predictions through long-short portfolio construction using out-of-sample Sharpe ratio as the primary performance metric. QTCNN achieves a Sharpe ratio of 0.538, outperforming the best classical baseline by approximately 72\%. These results highlight the practical potential of quantum-enhanced forecasting model, QTCNN, for robust decision-making in quantitative finance.

</details>


### [270] [The Impact of Data Characteristics on GNN Evaluation for Detecting Fake News](https://arxiv.org/abs/2512.06638)
*Isha Karn,David Jensen*

Main category: cs.LG

TL;DR: 现有假新闻检测基准数据集（GossipCop和PolitiFact）的图结构过于简单，导致GNN模型无法展现其建模传播结构的优势，MLP与GNN性能相近，表明这些数据集无法有效评估结构建模方法。


<details>
  <summary>Details</summary>
Motivation: 当前假新闻检测研究广泛使用GNN建模新闻传播结构，但作者发现两个主流基准数据集（GossipCop和PolitiFact）的图结构过于简单，无法有效评估结构建模方法的实际效用，可能导致研究结论偏差。

Method: 1) 系统比较5种GNN架构与使用相同节点特征的结构无关MLP；2) 通过特征洗牌和边随机化控制实验分离结构和特征的贡献；3) 分析数据集图拓扑特征；4) 在合成数据集上验证GNN在结构信息丰富时的优势。

Result: 1) MLP与GNN性能相近（差距1-2%，置信区间重叠）；2) 特征洗牌导致性能崩溃，边随机化性能稳定，表明结构贡献可忽略；3) 超过75%节点距离根节点仅一跳，结构多样性极低；4) 在合成数据集上，当节点特征嘈杂而结构信息丰富时，GNN显著优于MLP。

Conclusion: 现有假新闻检测基准数据集无法有效测试结构建模方法的效用，因为它们具有浅层、自我中心式的图拓扑，缺乏结构多样性。这呼吁开发具有更丰富、更多样化图拓扑的数据集，以更好地评估GNN在假新闻检测中的价值。

Abstract: Graph neural networks (GNNs) are widely used for the detection of fake news by modeling the content and propagation structure of news articles on social media. We show that two of the most commonly used benchmark data sets - GossipCop and PolitiFact - are poorly suited to evaluating the utility of models that use propagation structure. Specifically, these data sets exhibit shallow, ego-like graph topologies that provide little or no ability to differentiate among modeling methods. We systematically benchmark five GNN architectures against a structure-agnostic multilayer perceptron (MLP) that uses the same node features. We show that MLPs match or closely trail the performance of GNNs, with performance gaps often within 1-2% and overlapping confidence intervals. To isolate the contribution of structure in these datasets, we conduct controlled experiments where node features are shuffled or edge structures randomized. We find that performance collapses under feature shuffling but remains stable under edge randomization. This suggests that structure plays a negligible role in these benchmarks. Structural analysis further reveals that over 75% of nodes are only one hop from the root, exhibiting minimal structural diversity. In contrast, on synthetic datasets where node features are noisy and structure is informative, GNNs significantly outperform MLPs. These findings provide strong evidence that widely used benchmarks do not meaningfully test the utility of modeling structural features, and they motivate the development of datasets with richer, more diverse graph topologies.

</details>


### [271] [Financial Fraud Identification and Interpretability Study for Listed Companies Based on Convolutional Neural Network](https://arxiv.org/abs/2512.06648)
*Xiao Li*

Main category: cs.LG

TL;DR: 本文提出基于卷积神经网络的中国A股上市公司财务舞弊检测框架，通过将面板数据转换为类图像表示来捕捉横截面和时间模式，实现提前预测，并在准确性、鲁棒性和预警性能上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 上市公司财务舞弊屡禁不止，传统统计模型难以处理非线性特征交互，机器学习模型缺乏可解释性，且现有方法多基于当年数据判断当年舞弊，时效性有限。

Method: 设计特征工程方案将公司年度面板数据转换为类图像表示，使用卷积神经网络捕捉横截面和时间模式；采用局部解释技术从实体、特征和时间三个维度分析模型可解释性。

Result: CNN在准确性、鲁棒性和早期预警性能上优于逻辑回归和LightGBM；发现偿债能力、比率结构、治理结构和内部控制是舞弊的通用预测因子；非舞弊公司特征模式稳定，舞弊公司特征模式异质且集中在短期窗口。

Conclusion: 基于CNN的财务舞弊检测框架能有效提前识别舞弊，通过适当的分类阈值调整和可解释性分析，为高风险环境下的财务监管提供实用工具。

Abstract: Since the emergence of joint-stock companies, financial fraud by listed firms has repeatedly undermined capital markets. Fraud is difficult to detect because of covert tactics and the high labor and time costs of audits. Traditional statistical models are interpretable but struggle with nonlinear feature interactions, while machine learning models are powerful but often opaque. In addition, most existing methods judge fraud only for the current year based on current year data, limiting timeliness.
  This paper proposes a financial fraud detection framework for Chinese A-share listed companies based on convolutional neural networks (CNNs). We design a feature engineering scheme that transforms firm-year panel data into image like representations, enabling the CNN to capture cross-sectional and temporal patterns and to predict fraud in advance. Experiments show that the CNN outperforms logistic regression and LightGBM in accuracy, robustness, and early-warning performance, and that proper tuning of the classification threshold is crucial in high-risk settings.
  To address interpretability, we analyze the model along the dimensions of entity, feature, and time using local explanation techniques. We find that solvency, ratio structure, governance structure, and internal control are general predictors of fraud, while environmental indicators matter mainly in high-pollution industries. Non-fraud firms share stable feature patterns, whereas fraud firms exhibit heterogeneous patterns concentrated in short time windows. A case study of Guanong Shares in 2022 shows that cash flow analysis, social responsibility, governance structure, and per-share indicators are the main drivers of the model's fraud prediction, consistent with the company's documented misconduct.

</details>


### [272] [Adaptive Test-Time Training for Predicting Need for Invasive Mechanical Ventilation in Multi-Center Cohorts](https://arxiv.org/abs/2512.06652)
*Xiaolei Lu,Shamim Nemati*

Main category: cs.LG

TL;DR: AdaTTT：一种针对ICU患者有创机械通气预测的自适应测试时训练框架，通过自监督学习、原型学习和部分最优传输来应对跨机构领域偏移问题。


<details>
  <summary>Details</summary>
Motivation: ICU患者有创机械通气预测模型在跨机构部署时，由于患者群体、临床实践和电子健康记录系统的差异导致领域偏移，影响模型泛化性能。测试时训练可以在无标签目标域数据的情况下动态适应模型，但需要改进以适应医疗场景。

Method: 提出自适应测试时训练框架AdaTTT：1）推导测试时预测误差的信息论边界；2）设计自监督学习框架，包含重建和掩码特征建模任务，采用动态掩码策略强调主任务关键特征；3）引入原型学习和部分最优传输进行灵活的部分特征对齐，保持临床有意义的患者表征。

Result: 在多中心ICU队列实验中，在不同测试时适应基准上展示了有竞争力的分类性能。

Conclusion: AdaTTT框架通过结合自监督学习、原型学习和部分最优传输，有效应对ICU有创机械通气预测中的跨机构领域偏移问题，提高了模型在部署时的适应性和鲁棒性。

Abstract: Accurate prediction of the need for invasive mechanical ventilation (IMV) in intensive care units (ICUs) patients is crucial for timely interventions and resource allocation. However, variability in patient populations, clinical practices, and electronic health record (EHR) systems across institutions introduces domain shifts that degrade the generalization performance of predictive models during deployment. Test-Time Training (TTT) has emerged as a promising approach to mitigate such shifts by adapting models dynamically during inference without requiring labeled target-domain data. In this work, we introduce Adaptive Test-Time Training (AdaTTT), an enhanced TTT framework tailored for EHR-based IMV prediction in ICU settings. We begin by deriving information-theoretic bounds on the test-time prediction error and demonstrate that it is constrained by the uncertainty between the main and auxiliary tasks. To enhance their alignment, we introduce a self-supervised learning framework with pretext tasks: reconstruction and masked feature modeling optimized through a dynamic masking strategy that emphasizes features critical to the main task. Additionally, to improve robustness against domain shifts, we incorporate prototype learning and employ Partial Optimal Transport (POT) for flexible, partial feature alignment while maintaining clinically meaningful patient representations. Experiments across multi-center ICU cohorts demonstrate competitive classification performance on different test-time adaptation benchmarks.

</details>


### [273] [GSAE: Graph-Regularized Sparse Autoencoders for Robust LLM Safety Steering](https://arxiv.org/abs/2512.06655)
*Jehyeok Yeon,Federico Cinus,Yifan Wu,Luca Luceri*

Main category: cs.LG

TL;DR: 提出Graph-Regularized Sparse Autoencoders (GSAEs)，通过图正则化惩罚神经元共激活图，学习分布式安全表示，实现运行时安全引导，显著提升有害内容拒绝率同时保持任务准确性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全防御方法存在局限性：黑盒护栏只能过滤输出，而基于内部激活的方法通常将安全概念简化为单一潜在特征或维度。但研究表明，抽象概念（如拒绝、时间性）实际上分布在多个特征中，而非孤立于单一特征。

Method: 提出图正则化稀疏自编码器(GSAEs)，在标准SAE基础上增加拉普拉斯平滑惩罚项，作用于神经元共激活图。该方法学习平滑、分布式的安全表示，作为跨越多个特征的连贯模式。采用两阶段门控机制：仅在检测到有害提示或续写时激活干预，自适应执行拒绝同时保持良性查询的实用性。

Result: GSAE引导在安全和QA基准测试中平均达到82%的选择性拒绝率，显著优于标准SAE引导(42%)，同时保持强大的任务准确性（TriviaQA 70%，TruthfulQA 65%，GSM8K 74%）。鲁棒性实验显示跨LLaMA-3、Mistral、Qwen和Phi家族的泛化能力，以及对越狱攻击（GCG、AutoDAN）的韧性，持续保持≥90%的有害内容拒绝率。

Conclusion: GSAEs能够学习分布式安全表示，有效解决现有安全引导方法的局限性，在保持模型实用性的同时显著提升对有害内容的拒绝能力，具有跨模型泛化性和抗攻击鲁棒性。

Abstract: Large language models (LLMs) face critical safety challenges, as they can be manipulated to generate harmful content through adversarial prompts and jailbreak attacks. Many defenses are typically either black-box guardrails that filter outputs, or internals-based methods that steer hidden activations by operationalizing safety as a single latent feature or dimension. While effective for simple concepts, this assumption is limiting, as recent evidence shows that abstract concepts such as refusal and temporality are distributed across multiple features rather than isolated in one. To address this limitation, we introduce Graph-Regularized Sparse Autoencoders (GSAEs), which extends SAEs with a Laplacian smoothness penalty on the neuron co-activation graph. Unlike standard SAEs that assign each concept to a single latent feature, GSAEs recover smooth, distributed safety representations as coherent patterns spanning multiple features. We empirically demonstrate that GSAE enables effective runtime safety steering, assembling features into a weighted set of safety-relevant directions and controlling them with a two-stage gating mechanism that activates interventions only when harmful prompts or continuations are detected during generation. This approach enforces refusals adaptively while preserving utility on benign queries. Across safety and QA benchmarks, GSAE steering achieves an average 82% selective refusal rate, substantially outperforming standard SAE steering (42%), while maintaining strong task accuracy (70% on TriviaQA, 65% on TruthfulQA, 74% on GSM8K). Robustness experiments further show generalization across LLaMA-3, Mistral, Qwen, and Phi families and resilience against jailbreak attacks (GCG, AutoDAN), consistently maintaining >= 90% refusal of harmful content.

</details>


### [274] [Rethinking Robustness: A New Approach to Evaluating Feature Attribution Methods](https://arxiv.org/abs/2512.06665)
*Panagiota Kiourti,Anu Singh,Preeti Duraipandian,Weichao Zhou,Wenchao Li*

Main category: cs.LG

TL;DR: 该论文研究深度神经网络特征归因方法的鲁棒性，提出新的相似输入定义、鲁棒性度量标准，以及基于生成对抗网络的方法来生成这些输入，挑战当前忽略模型输出差异的归因鲁棒性概念。


<details>
  <summary>Details</summary>
Motivation: 当前的特征归因鲁棒性评估方法主要忽略了模型输出之间的差异，导致评估不够客观。作者认为需要一种更准确的评估方法，能够揭示归因方法本身的弱点，而不是神经网络的弱点。

Method: 提出了三个主要方法：1）新的相似输入定义；2）新的鲁棒性度量标准；3）基于生成对抗网络的方法来生成这些相似输入。同时进行了与现有度量和最先进归因方法的全面评估。

Result: 研究结果表明，需要更客观的度量标准来准确评估归因方法的鲁棒性。新的评估框架能够更好地揭示归因方法本身的弱点，而不是神经网络的弱点。

Conclusion: 该论文挑战了当前归因鲁棒性的概念，提出了新的评估框架，强调需要更客观的度量标准来准确评估特征归因方法的鲁棒性，为未来研究提供了新的方向。

Abstract: This paper studies the robustness of feature attribution methods for deep neural networks. It challenges the current notion of attributional robustness that largely ignores the difference in the model's outputs and introduces a new way of evaluating the robustness of attribution methods. Specifically, we propose a new definition of similar inputs, a new robustness metric, and a novel method based on generative adversarial networks to generate these inputs. In addition, we present a comprehensive evaluation with existing metrics and state-of-the-art attribution methods. Our findings highlight the need for a more objective metric that reveals the weaknesses of an attribution method rather than that of the neural network, thus providing a more accurate evaluation of the robustness of attribution methods.

</details>


### [275] [The Meta-Learning Gap: Combining Hydra and Quant for Large-Scale Time Series Classification](https://arxiv.org/abs/2512.06666)
*Urav Maniar*

Main category: cs.LG

TL;DR: 研究时间序列分类中准确率与计算效率的权衡，探索两种高效算法（Hydra和Quant）的组合能否在保持计算可行性的同时获得集成学习的好处，发现当前元学习策略难以有效利用算法互补性。


<details>
  <summary>Details</summary>
Motivation: 时间序列分类面临准确率与计算效率的根本权衡。虽然像HIVE-COTE 2.0这样的综合集成方法能达到最先进的准确率，但其在UCR基准测试上340小时的训练时间使其在大规模数据集上不实用。研究者希望探索是否有针对性的两种高效算法组合能够在保持计算可行性的同时获得集成学习的好处。

Method: 结合Hydra（竞争卷积核）和Quant（分层区间分位数）两种来自互补范式的高效算法，在六种集成配置下进行评估。在10个大规模MONSTER数据集（7,898到1,168,774个训练实例）上测试性能，比较预测组合集成和特征连接方法。

Result: 最强配置将平均准确率从0.829提高到0.836，在10个数据集中的7个上取得成功。但预测组合集成仅捕获了11%的理论oracle潜力，显示出显著的元学习优化差距。特征连接方法通过学习新的决策边界超过了oracle界限，而预测级互补性与集成增益呈中等相关性。

Conclusion: 核心发现：挑战已从确保算法不同转变为学习如何有效组合它们。当前元学习策略难以利用oracle分析确认存在的互补性。改进的组合策略可能使集成增益在多样化时间序列分类应用中翻倍或三倍增长。

Abstract: Time series classification faces a fundamental trade-off between accuracy and computational efficiency. While comprehensive ensembles like HIVE-COTE 2.0 achieve state-of-the-art accuracy, their 340-hour training time on the UCR benchmark renders them impractical for large-scale datasets. We investigate whether targeted combinations of two efficient algorithms from complementary paradigms can capture ensemble benefits while maintaining computational feasibility. Combining Hydra (competing convolutional kernels) and Quant (hierarchical interval quantiles) across six ensemble configurations, we evaluate performance on 10 large-scale MONSTER datasets (7,898 to 1,168,774 training instances). Our strongest configuration improves mean accuracy from 0.829 to 0.836, succeeding on 7 of 10 datasets. However, prediction-combination ensembles capture only 11% of theoretical oracle potential, revealing a substantial meta-learning optimization gap. Feature-concatenation approaches exceeded oracle bounds by learning novel decision boundaries, while prediction-level complementarity shows moderate correlation with ensemble gains. The central finding: the challenge has shifted from ensuring algorithms are different to learning how to combine them effectively. Current meta-learning strategies struggle to exploit the complementarity that oracle analysis confirms exists. Improved combination strategies could potentially double or triple ensemble gains across diverse time series classification applications.

</details>


### [276] [GradientSpace: Unsupervised Data Clustering for Improved Instruction Tuning](https://arxiv.org/abs/2512.06678)
*Shrihari Sridharan,Deepak Ravikumar,Anand Raghunathan,Kaushik Roy*

Main category: cs.LG

TL;DR: GradientSpace：一种在完整梯度空间中直接聚类样本的框架，通过在线SVD算法识别潜在技能，训练专门的LoRA专家和轻量级路由器，在推理时选择最佳专家，优于现有聚类方法和微调技术。


<details>
  <summary>Details</summary>
Motivation: 指令调优是适配大语言模型的关键步骤，但真实世界数据集通常异构，包含多样信息，导致梯度干扰（conflicting gradients），这会降低模型性能。现有方法基于语义或嵌入相似性分组数据，但未能捕捉数据如何影响模型参数学习；而直接聚类梯度的方法需要随机投影降维导致精度损失，且依赖专家集成需要多次推理和昂贵的梯度计算。

Method: 提出GradientSpace框架：1）在完整梯度空间中直接聚类样本，避免降维精度损失；2）引入基于在线SVD的算法，在LoRA梯度上操作，识别潜在技能，无需存储所有样本梯度；3）为每个聚类训练专门的LoRA专家；4）训练轻量级路由器在推理时选择最佳专家（而非集成多个专家）。

Result: 在数学推理、代码生成、金融和创意写作任务上的实验表明：GradientSpace能产生连贯的专家专业化，相比最先进的聚类方法和微调技术获得一致的精度提升；路由到单个合适专家优于先前工作中的专家集成，同时显著降低推理延迟。

Conclusion: GradientSpace通过直接在完整梯度空间聚类、使用在线SVD算法识别技能、训练专门LoRA专家和轻量路由器，有效解决了指令调优中的梯度干扰问题，实现了更好的性能与效率平衡。

Abstract: Instruction tuning is one of the key steps required for adapting large language models (LLMs) to a broad spectrum of downstream applications. However, this procedure is difficult because real-world datasets are rarely homogeneous; they consist of a mixture of diverse information, causing gradient interference, where conflicting gradients pull the model in opposing directions, degrading performance. A common strategy to mitigate this issue is to group data based on semantic or embedding similarity. However, this fails to capture how data influences model parameters during learning. While recent works have attempted to cluster gradients directly, they randomly project gradients into lower dimensions to manage memory, which leads to accuracy loss. Moreover, these methods rely on expert ensembles which necessitates multiple inference passes and expensive on-the-fly gradient computations during inference. To address these limitations, we propose GradientSpace, a framework that clusters samples directly in full-dimensional gradient space. We introduce an online SVD-based algorithm that operates on LoRA gradients to identify latent skills without the infeasible cost of storing all sample gradients. Each cluster is used to train a specialized LoRA expert along with a lightweight router trained to select the best expert during inference. We show that routing to a single, appropriate expert outperforms expert ensembles used in prior work, while significantly reducing inference latency. Our experiments across mathematical reasoning, code generation, finance, and creative writing tasks demonstrate that GradientSpace leads to coherent expert specialization and consistent accuracy gains over state-of-the-art clustering methods and finetuning techniques.

</details>


### [277] [State Diversity Matters in Offline Behavior Distillation](https://arxiv.org/abs/2512.06692)
*Shiye Lei,Zhihao Cheng,Dacheng Tao*

Main category: cs.LG

TL;DR: 本文发现离线行为蒸馏(OBD)中存在原始数据集与蒸馏数据集之间的不对齐问题，并提出了一种基于状态密度加权的改进方法SDW-OBD来增强状态多样性，从而提升蒸馏效果。


<details>
  <summary>Details</summary>
Motivation: 离线行为蒸馏(OBD)可以将大量离线RL数据压缩成紧凑的合成行为数据集，用于高效策略训练。然而，作者发现原始数据集与蒸馏数据集之间存在不对齐问题：高质量的原始数据集不一定能产生优质的合成数据集。这种不对齐现象源于状态多样性与状态质量在不同训练损失水平下的不同重要性。

Method: 1. 通过实证分析发现，在训练损失较大时（OBD常见情况），状态多样性比状态质量更重要；而在训练损失较小时，状态质量更重要。
2. 理论分析表明，状态质量主要减少关键误差，状态多样性减少周围误差，当关键误差较大时，周围误差对策略性能影响更大。
3. 提出状态密度加权(SDW) OBD算法，通过使用状态密度的倒数加权蒸馏目标，强调状态多样性，将更多样化的状态信息蒸馏到合成数据中。

Result: 在多个D4RL数据集上的广泛实验证实，当原始数据集状态多样性有限时，SDW-OBD能显著提升OBD性能。该方法有效解决了原始数据集与蒸馏数据集之间的不对齐问题。

Conclusion: 本文揭示了OBD中原始数据集与蒸馏数据集之间的不对齐现象，并提出了SDW-OBD算法来增强状态多样性。理论分析和实验结果表明，在训练损失较大的OBD场景中，状态多样性比状态质量更为重要，SDW方法能有效提升蒸馏效果。

Abstract: Offline Behavior Distillation (OBD), which condenses massive offline RL data into a compact synthetic behavioral dataset, offers a promising approach for efficient policy training and can be applied across various downstream RL tasks. In this paper, we uncover a misalignment between original and distilled datasets, observing that a high-quality original dataset does not necessarily yield a superior synthetic dataset. Through an empirical analysis of policy performance under varying levels of training loss, we show that datasets with greater state diversity outperforms those with higher state quality when training loss is substantial, as is often the case in OBD, whereas the relationship reverses under minimal loss, which contributes to the misalignment. By associating state quality and diversity in reducing pivotal and surrounding error, respectively, our theoretical analysis establishes that surrounding error plays a more crucial role in policy performance when pivotal error is large, thereby highlighting the importance of state diversity in OBD scenario. Furthermore, we propose a novel yet simple algorithm, state density weighted (SDW) OBD, which emphasizes state diversity by weighting the distillation objective using the reciprocal of state density, thereby distilling a more diverse state information into synthetic data. Extensive experiments across multiple D4RL datasets confirm that SDW significantly enhances OBD performance when the original dataset exhibits limited state diversity.

</details>


### [278] [Mitigating Barren plateaus in quantum denoising diffusion probabilistic models](https://arxiv.org/abs/2512.06695)
*Haipeng Cao,Kaining Zhang,Dacheng Tao,Zhaofeng Su*

Main category: cs.LG

TL;DR: 本文发现量子去噪扩散概率模型(QuDDPM)存在训练高原问题，并提出改进方案


<details>
  <summary>Details</summary>
Motivation: 量子生成模型利用量子叠加和纠缠特性提升学习效率，QuDDPM作为量子生成学习框架表现出色，但存在训练高原问题限制了其性能

Method: 通过理论分析和实验验证确认原始QuDDPM存在训练高原，提出改进的QuDDPM使用与Haar分布保持一定距离的分布作为去噪过程输入

Result: 实验结果表明改进方法有效缓解了训练高原问题，生成样本质量更高

Conclusion: 改进的QuDDPM为可扩展和高效的量子生成学习铺平了道路

Abstract: Quantum generative models leverage quantum superposition and entanglement to enhance learning efficiency for both classical and quantum data. The quantum denoising diffusion probabilistic model (QuDDPM), inspired by its classical counterpart, has been proposed as a promising framework for quantum generative learning. QuDDPM is capable of efficiently learning and generating quantum data, and it demonstrates excellent performance in learning correlated quantum noise models, quantum many-body phases, and the topological structure of quantum data. However, we show that barren plateaus emerge in QuDDPMs due to the use of 2-design states as the input for the denoising process, which severely undermines the performance of QuDDPM. Through theoretical analysis and experimental validation, we confirm the presence of barren plateaus in the original QuDDPM. To address this issue, we introduce an improved QuDDPM that utilizes a distribution maintaining a certain distance from the Haar distribution, ensuring better trainability. Experimental results demonstrate that our approach effectively mitigates the barren plateau problem and generates samples with higher quality, paving the way for scalable and efficient quantum generative learning.

</details>


### [279] [Pathway to $O(\sqrt{d})$ Complexity bound under Wasserstein metric of flow-based models](https://arxiv.org/abs/2512.06702)
*Xiangjun Meng,Zhongjian Wang*

Main category: cs.LG

TL;DR: 本文为基于流的生成模型提供了可实现的误差分析工具，证明了在Wasserstein度量下，最优采样迭代复杂度与维度呈O(√d)关系，误差由两部分控制：与维度无关的推前映射Lipschitz性和局部离散误差的O(√d)缩放。


<details>
  <summary>Details</summary>
Motivation: 为基于流的生成模型提供可实现的误差分析工具，建立采样迭代复杂度与维度的理论界限，特别是在高维情况下的可扩展性分析。

Method: 通过分析误差的两个组成部分：1）反向流推前映射的Lipschitz性（与维度无关）；2）局部离散误差（与维度呈O(√d)关系）。这些分析基于Föllmer过程和1-整流流在Gaussian尾假设下的流基生成模型。

Result: 证明了采样迭代复杂度与协方差算子迹的平方根呈线性关系，这与前向过程的不变分布相关。具体来说，最优采样迭代复杂度为O(√d)。

Conclusion: 本文为流基生成模型提供了严格的误差分析框架，证明了在高维情况下采样复杂度的可扩展性，为实际应用提供了理论保证。

Abstract: We provide attainable analytical tools to estimate the error of flow-based generative models under the Wasserstein metric and to establish the optimal sampling iteration complexity bound with respect to dimension as $O(\sqrt{d})$. We show this error can be explicitly controlled by two parts: the Lipschitzness of the push-forward maps of the backward flow which scales independently of the dimension; and a local discretization error scales $O(\sqrt{d})$ in terms of dimension. The former one is related to the existence of Lipschitz changes of variables induced by the (heat) flow. The latter one consists of the regularity of the score function in both spatial and temporal directions.
  These assumptions are valid in the flow-based generative model associated with the Föllmer process and $1$-rectified flow under the Gaussian tail assumption. As a consequence, we show that the sampling iteration complexity grows linearly with the square root of the trace of the covariance operator, which is related to the invariant distribution of the forward process.

</details>


### [280] [A Novel Multimodal RUL Framework for Remaining Useful Life Estimation with Layer-wise Explanations](https://arxiv.org/abs/2512.06708)
*Waleed Razzaq,Yun-Bo Zhao*

Main category: cs.LG

TL;DR: 提出一种多模态RUL估计框架，结合图像表示和时间频率表示，通过注意力机制和可解释性技术提升轴承剩余寿命预测性能。


<details>
  <summary>Details</summary>
Motivation: 滚动轴承是机械故障的常见原因，现有RUL估计方法存在泛化能力差、鲁棒性不足、数据需求高和可解释性有限等问题，需要更有效的解决方案。

Method: 提出多模态RUL框架：1) 使用Bresenham线算法将振动信号转换为图像表示(ImR)；2) 使用连续小波变换转换为时频表示(TFR)；3) 三个分支架构：ImR和TFR分支使用扩张卷积块提取空间退化特征，融合分支通过LSTM建模时间退化模式，多头注意力机制突出重要特征，最后线性层回归RUL；4) 引入多模态层相关传播(multimodal-LRP)增强可解释性。

Result: 在XJTU-SY和PRONOSTIA基准数据集上验证，性能达到或超过最先进基线，在未见工况下表现良好；训练数据需求减少28%-48%；模型具有强噪声鲁棒性，multimodal-LRP可视化证实预测的可解释性和可信度。

Conclusion: 该多模态框架在RUL估计中表现出优越性能、数据效率和可解释性，特别适合实际工业部署，为机械系统健康管理提供了可靠解决方案。

Abstract: Estimating the Remaining Useful Life (RUL) of mechanical systems is pivotal in Prognostics and Health Management (PHM). Rolling-element bearings are among the most frequent causes of machinery failure, highlighting the need for robust RUL estimation methods. Existing approaches often suffer from poor generalization, lack of robustness, high data demands, and limited interpretability. This paper proposes a novel multimodal-RUL framework that jointly leverages image representations (ImR) and time-frequency representations (TFR) of multichannel, nonstationary vibration signals. The architecture comprises three branches: (1) an ImR branch and (2) a TFR branch, both employing multiple dilated convolutional blocks with residual connections to extract spatial degradation features; and (3) a fusion branch that concatenates these features and feeds them into an LSTM to model temporal degradation patterns. A multi-head attention mechanism subsequently emphasizes salient features, followed by linear layers for final RUL regression. To enable effective multimodal learning, vibration signals are converted into ImR via the Bresenham line algorithm and into TFR using Continuous Wavelet Transform. We also introduce multimodal Layer-wise Relevance Propagation (multimodal-LRP), a tailored explainability technique that significantly enhances model transparency. The approach is validated on the XJTU-SY and PRONOSTIA benchmark datasets. Results show that our method matches or surpasses state-of-the-art baselines under both seen and unseen operating conditions, while requiring ~28 % less training data on XJTU-SY and ~48 % less on PRONOSTIA. The model exhibits strong noise resilience, and multimodal-LRP visualizations confirm the interpretability and trustworthiness of predictions, making the framework highly suitable for real-world industrial deployment.

</details>


### [281] [A Novel Deep Neural Network Architecture for Real-Time Water Demand Forecasting](https://arxiv.org/abs/2512.06714)
*Tony Salloom,Okyay Kaynak,Wei He*

Main category: cs.LG

TL;DR: 提出一种结合虚拟数据扩展和GRU-K-means混合模型的短期用水需求预测方法，显著降低极端点误差和模型复杂度


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在短期用水需求预测中存在两个主要问题：1）模型参数过多导致复杂度高；2）极端点预测误差大。这是首个专门针对极端点预测问题的研究。

Method: 1）提出虚拟数据扩展方法，在实际数据中插入虚拟数据以缓解极端点周围的非线性；2）构建GRU-K-means混合模型，使用GRU处理时序关系，引入K-means无监督分类创建新特征以减少参数数量

Result: 1）模型复杂度降低至文献水平的六分之一，同时保持相同精度；2）数据扩展使预测误差降低约30%；3）使用中国两个不同水厂的实际数据进行验证

Conclusion: 提出的方法有效解决了短期用水需求预测中的极端点误差和模型复杂度问题，虚拟数据扩展显著提升预测精度，GRU-K-means混合模型在保持精度的同时大幅降低复杂度

Abstract: Short-term water demand forecasting (StWDF) is the foundation stone in the derivation of an optimal plan for controlling water supply systems. Deep learning (DL) approaches provide the most accurate solutions for this purpose. However, they suffer from complexity problem due to the massive number of parameters, in addition to the high forecasting error at the extreme points. In this work, an effective method to alleviate the error at these points is proposed. It is based on extending the data by inserting virtual data within the actual data to relieve the nonlinearity around them. To our knowledge, this is the first work that considers the problem related to the extreme points. Moreover, the water demand forecasting model proposed in this work is a novel DL model with relatively low complexity. The basic model uses the gated recurrent unit (GRU) to handle the sequential relationship in the historical demand data, while an unsupervised classification method, K-means, is introduced for the creation of new features to enhance the prediction accuracy with less number of parameters. Real data obtained from two different water plants in China are used to train and verify the model proposed. The prediction results and the comparison with the state-of-the-art illustrate that the method proposed reduces the complexity of the model six times of what achieved in the literature while conserving the same accuracy. Furthermore, it is found that extending the data set significantly reduces the error by about 30%. However, it increases the training time.

</details>


### [282] [Decoding Motor Behavior Using Deep Learning and Reservoir Computing](https://arxiv.org/abs/2512.06725)
*Tian Lan*

Main category: cs.LG

TL;DR: 提出ESNNet，结合CNN空间特征提取与ESN时序建模能力，用于EEG运动行为分类，在滑板技巧数据集上超越传统CNN基线


<details>
  <summary>Details</summary>
Motivation: 传统CNN架构（如EEGNet、DeepConvNet）能有效捕捉局部空间模式，但难以建模长程时序依赖和非线性动态。需要增强EEG解码中时序动态的建模能力

Method: 将回声状态网络（ESN）整合到解码流程中，ESN构建高维稀疏连接的循环储备池，擅长追踪时序动态，与CNN的空间表征能力互补，形成ESNNet架构

Result: 在经PREP预处理、MNE-Python实现的滑板技巧EEG数据集上，ESNNet获得83.2%的受试者内准确率和51.3%的留一受试者外准确率，超越广泛使用的CNN基线

Conclusion: ESNNet通过整合CNN空间特征提取与ESN时序建模，有效提升EEG运动行为分类性能，为无创脑机接口提供改进的解码方法

Abstract: We present a novel approach to EEG decoding for non-invasive brain machine interfaces (BMIs), with a focus on motor-behavior classification. While conventional convolutional architectures such as EEGNet and DeepConvNet are effective in capturing local spatial patterns, they are markedly less suited for modeling long-range temporal dependencies and nonlinear dynamics. To address this limitation, we integrate an Echo State Network (ESN), a prominent paradigm in reservoir computing into the decoding pipeline. ESNs construct a high-dimensional, sparsely connected recurrent reservoir that excels at tracking temporal dynamics, thereby complementing the spatial representational power of CNNs. Evaluated on a skateboard-trick EEG dataset preprocessed via the PREP pipeline and implemented in MNE-Python, our ESNNet achieves 83.2% within-subject and 51.3% LOSO accuracies, surpassing widely used CNN-based baselines. Code is available at https://github.com/Yutiankunkun/Motion-Decoding-Using-Biosignals

</details>


### [283] [KV-CAR: KV Cache Compression using Autoencoders and KV Reuse in Large Language Models](https://arxiv.org/abs/2512.06727)
*Sourjya Roy,Shrihari Sridharan,Surya Selvam,Anand Raghunathan*

Main category: cs.LG

TL;DR: KV CAR是一个统一的框架，通过自编码器压缩和相似性驱动的重用机制，显著减少KV缓存内存占用，实现最高47.85%的压缩率，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模和上下文长度的增加，KV缓存在自回归解码过程中的内存需求成为主要瓶颈，经常超过模型本身的内存占用，限制了可实现的批大小和上下文窗口。

Method: KV CAR结合两种互补技术：1) 轻量级自编码器学习键值张量在嵌入维度上的紧凑表示，在存储到KV缓存前压缩并在检索时恢复；2) 相似性驱动的重用机制识别相邻层间特定注意力头的KV张量重用机会。

Result: 在GPT-2和TinyLLaMA模型上的评估显示，KV CAR在Wikitext、C4、PIQA和Winogrande数据集上实现了最高47.85%的KV缓存内存减少，对困惑度和零样本准确率影响最小。在NVIDIA A40 GPU上的系统级测量表明，减少的KV占用直接转化为推理时更长的序列长度和更大的批大小。

Conclusion: KV CAR通过减少KV张量的维度和结构冗余，无需改变Transformer架构，就能实现内存高效的大语言模型推理，有效解决了KV缓存内存瓶颈问题。

Abstract: As Large Language Models (LLMs) scale in size and context length, the memory requirements of the key value (KV) cache have emerged as a major bottleneck during autoregressive decoding. The KV cache grows with sequence length and embedding dimension, often exceeding the memory footprint of the model itself and limiting achievable batch sizes and context windows. To address this challenge, we present KV CAR, a unified and architecture agnostic framework that significantly reduces KV cache storage while maintaining model fidelity. KV CAR combines two complementary techniques. First, a lightweight autoencoder learns compact representations of key and value tensors along the embedding dimension, compressing them before they are stored in the KV cache and restoring them upon retrieval. Second, a similarity driven reuse mechanism identifies opportunities to reuse KV tensors of specific attention heads across adjacent layers. Together, these methods reduce the dimensional and structural redundancy in KV tensors without requiring changes to the transformer architecture. Evaluations on GPT 2 and TinyLLaMA models across Wikitext, C4, PIQA, and Winogrande datasets demonstrate that KV CAR achieves up to 47.85 percent KV cache memory reduction with minimal impact on perplexity and zero shot accuracy. System level measurements on an NVIDIA A40 GPU show that the reduced KV footprint directly translates into longer sequence lengths and larger batch sizes during inference. These results highlight the effectiveness of KV CAR in enabling memory efficient LLM inference.

</details>


### [284] [Enhancing Interpretability of AR-SSVEP-Based Motor Intention Recognition via CNN-BiLSTM and SHAP Analysis on EEG Data](https://arxiv.org/abs/2512.06730)
*Lin Yang,Xiang Li,Xin Ma,Xinxin Zhao*

Main category: cs.LG

TL;DR: 提出基于增强现实的SSVEP脑机接口系统，结合多注意力机制的CNN-BiLSTM模型，提升运动功能障碍患者的康复训练参与度和运动意图识别能力。


<details>
  <summary>Details</summary>
Motivation: 运动功能障碍患者康复训练主观参与度低，传统SSVEP-BCI系统依赖外部视觉刺激设备，在现实环境中实用性受限，且治疗师工作负担重。

Method: 1) 设计基于HoloLens 2的四种EEG类别，采集7名健康受试者数据；2) 在传统CNN-BiLSTM架构基础上集成多头注意力机制(MACNN-BiLSTM)；3) 提取10个时频EEG特征，通过CNN学习高级表征，BiLSTM建模序列依赖，多头注意力突出运动意图相关模式；4) 使用SHAP方法可视化EEG特征对决策的贡献。

Result: 该方法提升了实时运动意图识别能力，支持运动障碍患者的康复恢复，增强了模型的可解释性。

Conclusion: AR-SSVEP系统结合MACNN-BiLSTM模型能有效解决患者主动性不足和治疗师工作负担问题，为运动功能障碍康复提供了更实用的BCI解决方案。

Abstract: Patients with motor dysfunction show low subjective engagement in rehabilitation training. Traditional SSVEP-based brain-computer interface (BCI) systems rely heavily on external visual stimulus equipment, limiting their practicality in real-world settings. This study proposes an augmented reality steady-state visually evoked potential (AR-SSVEP) system to address the lack of patient initiative and the high workload on therapists. Firstly, we design four HoloLens 2-based EEG classes and collect EEG data from seven healthy subjects for analysis. Secondly, we build upon the conventional CNN-BiLSTM architecture by integrating a multi-head attention mechanism (MACNN-BiLSTM). We extract ten temporal-spectral EEG features and feed them into a CNN to learn high-level representations. Then, we use BiLSTM to model sequential dependencies and apply a multi-head attention mechanism to highlight motor-intention-related patterns. Finally, the SHAP (SHapley Additive exPlanations) method is applied to visualize EEG feature contributions to the neural network's decision-making process, enhancing the model's interpretability. These findings enhance real-time motor intention recognition and support recovery in patients with motor impairments.

</details>


### [285] [Arc Gradient Descent: A Mathematically Derived Reformulation of Gradient Descent with Phase-Aware, User-Controlled Step Dynamics](https://arxiv.org/abs/2512.06737)
*Nikhil Verma,Joonas Linnosmaa,Espinosa-Leal Leonardo,Napat Vajragupta*

Main category: cs.LG

TL;DR: ArcGD优化器在非凸基准函数和真实ML数据集上表现优异，超越Adam等先进优化器，在CIFAR-10上获得最高准确率，并显示出与Lion优化器的理论联系。


<details>
  <summary>Details</summary>
Motivation: 现有优化器如Adam在复杂非凸问题上存在局限性，特别是在高维空间和长时间训练时可能出现过拟合或性能退化。需要开发能够更好处理非凸地形、具有更好泛化能力且不需要复杂调参的优化方法。

Method: 提出了ArcGD优化器，首先在具有挑战性的Rosenbrock函数上进行评估（2D到50,000D），消除学习率偏差进行公平比较。然后在CIFAR-10数据集上测试8种不同MLP架构，与Adam、AdamW、Lion、SGD等先进优化器对比。还展示了ArcGD变体可解释为Lion优化器的特例。

Result: 在Rosenbrock函数上，ArcGD在相同有效学习率下始终优于Adam；在CIFAR-10上，ArcGD在20,000次迭代后获得最高平均测试准确率50.7%，优于所有对比方法，在8种架构中6种获胜或持平。ArcGD在长时间训练中持续改进，而Adam/AdamW早期收敛后出现性能退化。

Conclusion: ArcGD在非凸优化和真实ML任务中表现出色，具有更好的泛化能力和抗过拟合特性，无需早期停止调参。与Lion优化器的理论联系揭示了优化方法间的内在机制关联，显示出广泛适用性，值得进一步探索。

Abstract: The paper presents the formulation, implementation, and evaluation of the ArcGD optimiser. The evaluation is conducted initially on a non-convex benchmark function and subsequently on a real-world ML dataset. The initial comparative study using the Adam optimiser is conducted on a stochastic variant of the highly non-convex and notoriously challenging Rosenbrock function, renowned for its narrow, curved valley, across dimensions ranging from 2D to 1000D and an extreme case of 50,000D. Two configurations were evaluated to eliminate learning-rate bias: (i) both using ArcGD's effective learning rate and (ii) both using Adam's default learning rate. ArcGD consistently outperformed Adam under the first setting and, although slower under the second, achieved super ior final solutions in most cases. In the second evaluation, ArcGD is evaluated against state-of-the-art optimizers (Adam, AdamW, Lion, SGD) on the CIFAR-10 image classification dataset across 8 diverse MLP architectures ranging from 1 to 5 hidden layers. ArcGD achieved the highest average test accuracy (50.7%) at 20,000 iterations, outperforming AdamW (46.6%), Adam (46.8%), SGD (49.6%), and Lion (43.4%), winning or tying on 6 of 8 architectures. Notably, while Adam and AdamW showed strong early convergence at 5,000 iterations, but regressed with extended training, whereas ArcGD continued improving, demonstrating generalization and resistance to overfitting without requiring early stopping tuning. Strong performance on geometric stress tests and standard deep-learning benchmarks indicates broad applicability, highlighting the need for further exploration. Moreover, it is also shown that a variant of ArcGD can be interpreted as a special case of the Lion optimiser, highlighting connections between the inherent mechanisms of such optimisation methods.

</details>


### [286] [Multi-Scale Protein Structure Modelling with Geometric Graph U-Nets](https://arxiv.org/abs/2512.06752)
*Chang Liu,Vivian Li,Linus Leong,Vladimir Radenkovic,Pietro Liò,Chaitanya K. Joshi*

Main category: cs.LG

TL;DR: 提出Geometric Graph U-Nets，一种通过递归粗化和细化蛋白质图来学习多尺度表示的新模型，在蛋白质折叠分类任务上显著优于现有几何GNN和Transformer基线。


<details>
  <summary>Details</summary>
Motivation: 现有几何GNN和Transformer依赖消息传递，无法捕捉蛋白质功能中的层次相互作用（如全局结构域和长程变构调节），因此需要网络架构本身反映这种生物层次结构。

Method: 引入Geometric Graph U-Nets，通过递归粗化和细化蛋白质图来学习多尺度表示，构建层次化设计，理论上比标准几何GNN更具表达能力。

Result: 在蛋白质折叠分类任务中，Geometric U-Nets显著优于不变和等变基线模型，证明其能够学习定义蛋白质折叠的全局结构模式。

Conclusion: 该工作为设计能够学习生物分子多尺度结构的几何深度学习架构提供了理论基础。

Abstract: Geometric Graph Neural Networks (GNNs) and Transformers have become state-of-the-art for learning from 3D protein structures. However, their reliance on message passing prevents them from capturing the hierarchical interactions that govern protein function, such as global domains and long-range allosteric regulation. In this work, we argue that the network architecture itself should mirror this biological hierarchy. We introduce Geometric Graph U-Nets, a new class of models that learn multi-scale representations by recursively coarsening and refining the protein graph. We prove that this hierarchical design can theoretically more expressive than standard Geometric GNNs. Empirically, on the task of protein fold classification, Geometric U-Nets substantially outperform invariant and equivariant baselines, demonstrating their ability to learn the global structural patterns that define protein folds. Our work provides a principled foundation for designing geometric deep learning architectures that can learn the multi-scale structure of biomolecules.

</details>


### [287] [Optimal Analysis for Bandit Learning in Matching Markets with Serial Dictatorship](https://arxiv.org/abs/2512.06758)
*Zilong Wang,Shuai Li*

Main category: cs.LG

TL;DR: 该论文提出了一个多级连续选择算法，在序列独裁假设下实现了与下界匹配的遗憾上界，解决了匹配市场与多臂老虎机问题中上下界之间的差距。


<details>
  <summary>Details</summary>
Motivation: 在线匹配市场中，参与者通常不确定自己的偏好，需要通过多轮交互学习。现有研究在序列独裁假设下存在遗憾下界Ω(Nlog(T)/Δ² + Klog(T)/Δ)和上界O(Klog(T)/Δ²)之间的差距（从N到K），不清楚是需要改进下界还是上界。

Method: 提出多级连续选择算法，在满足序列独裁假设的市场中工作。该算法通过多级选择机制来匹配遗憾下界。

Result: 算法实现了O(Nlog(T)/Δ² + Klog(T)/Δ)的遗憾上界，与Sankararaman等人提出的下界完全匹配，是首个在匹配市场与多臂老虎机问题中达到下界的算法。

Conclusion: 该工作填补了匹配市场与多臂老虎机问题中遗憾上下界之间的理论差距，证明了现有下界的紧致性，并为序列独裁假设下的在线匹配提供了最优算法。

Abstract: The problem of two-sided matching markets is well-studied in computer science and economics, owing to its diverse applications across numerous domains. Since market participants are usually uncertain about their preferences in various online matching platforms, an emerging line of research is dedicated to the online setting where one-side participants (players) learn their unknown preferences through multiple rounds of interactions with the other side (arms). Sankararaman et al. provide an $Ω\left( \frac{N\log(T)}{Δ^2} + \frac{K\log(T)}Δ \right)$ regret lower bound for this problem under serial dictatorship assumption, where $N$ is the number of players, $K (\geq N)$ is the number of arms, $Δ$ is the minimum reward gap across players and arms, and $T$ is the time horizon. Serial dictatorship assumes arms have the same preferences, which is common in reality when one side participants have a unified evaluation standard. Recently, the work of Kong and Li proposes the ET-GS algorithm and achieves an $O\left( \frac{K\log(T)}{Δ^2} \right)$ regret upper bound, which is the best upper bound attained so far. Nonetheless, a gap between the lower and upper bounds, ranging from $N$ to $K$, persists. It remains unclear whether the lower bound or the upper bound needs to be improved. In this paper, we propose a multi-level successive selection algorithm that obtains an $O\left( \frac{N\log(T)}{Δ^2} + \frac{K\log(T)}Δ \right)$ regret bound when the market satisfies serial dictatorship. To the best of our knowledge, we are the first to propose an algorithm that matches the lower bound in the problem of matching markets with bandits.

</details>


### [288] [Measuring Over-smoothing beyond Dirichlet energy](https://arxiv.org/abs/2512.06782)
*Weiqi Guan,Zihao Shi*

Main category: cs.LG

TL;DR: 本文提出基于高阶特征导数的节点相似性度量家族，用于更全面评估图神经网络中的过平滑现象，揭示了过平滑衰减率与图拉普拉斯谱隙的内在联系。


<details>
  <summary>Details</summary>
Motivation: 现有基于Dirichlet能量的过平滑度量仅能捕捉一阶特征导数，存在局限性。需要更全面的度量方法来量化图神经网络中的过平滑现象。

Method: 提出基于高阶特征导数能量的广义节点相似性度量家族，通过理论分析建立这些度量之间的关系，并分析连续热扩散和离散聚合算子下Dirichlet能量的衰减率。

Result: 理论分析揭示了过平滑衰减率与图拉普拉斯谱隙的内在联系，实证结果表明基于注意力的图神经网络在这些新度量下确实存在过平滑问题。

Conclusion: 高阶特征导数能量为过平滑提供了更全面的量化框架，揭示了过平滑与图谱特性的深层联系，为理解图神经网络局限性提供了新视角。

Abstract: While Dirichlet energy serves as a prevalent metric for quantifying over-smoothing, it is inherently restricted to capturing first-order feature derivatives. To address this limitation, we propose a generalized family of node similarity measures based on the energy of higher-order feature derivatives. Through a rigorous theoretical analysis of the relationships among these measures, we establish the decay rates of Dirichlet energy under both continuous heat diffusion and discrete aggregation operators. Furthermore, our analysis reveals an intrinsic connection between the over-smoothing decay rate and the spectral gap of the graph Laplacian. Finally, empirical results demonstrate that attention-based Graph Neural Networks (GNNs) suffer from over-smoothing when evaluated under these proposed metrics.

</details>


### [289] [Angular Regularization for Positive-Unlabeled Learning on the Hypersphere](https://arxiv.org/abs/2512.06785)
*Vasileios Sevetlidis,George Pavlidis,Antonios Gasteratos*

Main category: cs.LG

TL;DR: AngularPU：一种基于余弦相似度和角度间隔的PU学习新框架，通过可学习的原型向量表示正类，无需显式负类建模，在单位超球面上操作，通过角度正则化器改善未标记数据的分离效果。


<details>
  <summary>Details</summary>
Motivation: 现有PU学习方法要么依赖强分布假设（负风险估计），要么在高维设置中容易崩溃（伪标签方法），需要一种更稳健、可扩展且无需显式负类建模的PU学习框架。

Method: 在单位超球面上使用余弦相似度和角度间隔，通过可学习的原型向量表示正类，分类简化为阈值化嵌入向量与原型之间的余弦相似度。引入角度正则化器鼓励未标记集在超球面上分散，改善分离效果。

Result: 在基准数据集上的实验表明，AngularPU在正样本稀缺和高维嵌入设置中，相比最先进的PU方法实现了竞争性或更优的性能，同时提供几何可解释性和可扩展性。

Conclusion: AngularPU提供了一种新颖的PU学习框架，无需显式负类建模，通过角度决策规则和正则化机制，在理论和实验上都表现出优越性，特别适用于正样本稀缺和高维场景。

Abstract: Positive-Unlabeled (PU) learning addresses classification problems where only a subset of positive examples is labeled and the remaining data is unlabeled, making explicit negative supervision unavailable. Existing PU methods often rely on negative-risk estimation or pseudo-labeling, which either require strong distributional assumptions or can collapse in high-dimensional settings. We propose AngularPU, a novel PU framework that operates on the unit hypersphere using cosine similarity and angular margin. In our formulation, the positive class is represented by a learnable prototype vector, and classification reduces to thresholding the cosine similarity between an embedding and this prototype-eliminating the need for explicit negative modeling. To counteract the tendency of unlabeled embeddings to cluster near the positive prototype, we introduce an angular regularizer that encourages dispersion of the unlabeled set over the hypersphere, improving separation. We provide theoretical guarantees on the Bayes-optimality of the angular decision rule, consistency of the learned prototype, and the effect of the regularizer on the unlabeled distribution. Experiments on benchmark datasets demonstrate that AngularPU achieves competitive or superior performance compared to state-of-the-art PU methods, particularly in settings with scarce positives and high-dimensional embeddings, while offering geometric interpretability and scalability.

</details>


### [290] [Partial Inverse Design of High-Performance Concrete Using Cooperative Neural Networks for Constraint-Aware Mix Generation](https://arxiv.org/abs/2512.06813)
*Agung Nugraha,Heungjun Im,Jihwan Lee*

Main category: cs.LG

TL;DR: 提出一种用于高性能混凝土部分逆向设计的协同神经网络框架，通过耦合的插补模型和代理模型，在单次前向传播中生成满足约束的混合设计，相比现有方法显著提升精度和效率。


<details>
  <summary>Details</summary>
Motivation: 高性能混凝土需要复杂的混合设计，涉及许多相互依赖的变量和实际约束。虽然数据驱动方法在正向设计预测建模方面有所进展，但逆向设计（确定实现目标性能的混合组成）仍然有限，特别是在某些混合变量被约束固定的情况下。

Method: 提出协同神经网络框架，结合两个耦合的神经网络模型：1) 插补模型推断未确定变量；2) 代理模型预测抗压强度。通过协同学习，模型在单次前向传播中生成有效且性能一致的混合设计，无需重新训练即可适应不同的约束组合。

Result: 在基准数据集上评估，提出的模型实现了稳定且更高的R平方值（0.87-0.92），与自编码器基线相比平均减少50%的均方误差，与贝叶斯推理相比平均减少70%的均方误差。

Conclusion: 协同神经网络为混凝土工程中约束感知、数据驱动的混合配比提供了准确、鲁棒且计算高效的基础，解决了部分逆向设计的关键挑战。

Abstract: High-performance concrete offers exceptional strength and durability but requires complex mix designs involving many interdependent variables and practical constraints. While data-driven methods have advanced predictive modeling for forward design, inverse design, which focuses on determining mix compositions that achieve target performance, remains limited, particularly in design situations where some mix variables are fixed by constraints and only the remaining variables must be determined. This study proposes a cooperative neural network framework for the partial inverse design of high-performance concrete. The framework combines two coupled neural network models, an imputation model that infers the undetermined variables and a surrogate model that predicts compressive strength. Through cooperative learning, the model generates valid and performance-consistent mix designs in a single forward pass while accommodating different constraint combinations without retraining. Its performance is compared with both probabilistic and generative approaches, including Bayesian inference based on a Gaussian process surrogate and autoencoder-based models. Evaluated on a benchmark dataset, the proposed model achieves stable and higher R-squared values of 0.87-0.92 and reduces mean squared error by an average of 50 percent compared with autoencoder baselines and by an average of 70 percent compared with Bayesian inference. The results demonstrate that the cooperative neural network provides an accurate, robust, and computationally efficient foundation for constraint-aware, data-driven mix proportioning in concrete engineering.

</details>


### [291] [Neural Factorization-based Bearing Fault Diagnosis](https://arxiv.org/abs/2512.06837)
*Zhenhao Li,Xu Cheng,Yi Zhou*

Main category: cs.LG

TL;DR: 提出基于神经分解的分类框架用于高铁轴承故障诊断，通过多模态潜在特征向量嵌入和神经分解融合来挖掘复杂故障特征，实验证明优于传统方法


<details>
  <summary>Details</summary>
Motivation: 高铁轴承作为列车运行系统的核心部件，其健康状况直接影响运行安全。传统诊断方法在复杂工况下诊断精度不足，需要更有效的故障诊断方法

Method: 提出神经分解分类框架，包含两个核心思想：1) 将振动时间序列嵌入到多个模态潜在特征向量中以捕捉多样故障模式；2) 利用神经分解原理将这些向量融合为统一的振动表示。基于CP和Tucker融合方案分别实例化为CP-NFC和Tucker-NFC模型

Result: 实验结果表明，两种模型相比传统机器学习方法都取得了优越的诊断性能。比较分析为高铁轴承监测中选择有效诊断策略提供了有价值的经验证据和实践指导

Conclusion: 提出的神经分解分类框架能够有效从原始时间序列数据中挖掘复杂潜在故障特征，为高铁轴承故障诊断提供了有效的解决方案，具有实际应用价值

Abstract: This paper studies the key problems of bearing fault diagnosis of high-speed train. As the core component of the train operation system, the health of bearings is directly related to the safety of train operation. The traditional diagnostic methods are facing the challenge of insufficient diagnostic accuracy under complex conditions. To solve these problems, we propose a novel Neural Factorization-based Classification (NFC) framework for bearing fault diagnosis. It is built on two core idea: 1) Embedding vibration time series into multiple mode-wise latent feature vectors to capture diverse fault-related patterns; 2) Leveraging neural factorization principles to fuse these vectors into a unified vibration representation. This design enables effective mining of complex latent fault characteristics from raw time-series data. We further instantiate the framework with two models CP-NFC and Tucker-NFC based on CP and Tucker fusion schemes, respectively. Experimental results show that both models achieve superior diagnostic performance compared with traditional machine learning methods. The comparative analysis provides valuable empirical evidence and practical guidance for selecting effective diagnostic strategies in high-speed train bearing monitoring.

</details>


### [292] [Know your Trajectory -- Trustworthy Reinforcement Learning deployment through Importance-Based Trajectory Analysis](https://arxiv.org/abs/2512.06917)
*Clifford F,Devika Jay,Abhishek Sarkar,Satheesh K Perepu,Santhosh G S,Kaushik Dey,Balaraman Ravindran*

Main category: cs.LG

TL;DR: 提出一个新颖的轨迹级可解释性框架，通过结合Q值差异和"激进项"的状态重要性度量来排名完整轨迹，识别最优行为并提供对比解释。


<details>
  <summary>Details</summary>
Motivation: 当前可解释强化学习主要关注局部单步决策，缺乏对智能体长期行为的解释。随着RL在现实应用中的部署，需要确保行为透明可信，因此需要轨迹级分析来解释长期行为。

Method: 引入一个新颖框架，通过定义和聚合新的状态重要性度量来排名完整轨迹。该度量结合经典Q值差异和"激进项"（捕捉智能体达到目标的亲和力），提供更细致的状态关键性衡量。从关键状态生成反事实推演来对比不同路径。

Result: 方法成功从异构的智能体经验中识别出最优轨迹。通过从关键状态生成反事实推演，证明智能体选择的路径明显优于替代方案。在标准OpenAI Gym环境中的实验验证了所提重要性度量比经典方法更有效地识别最优行为。

Conclusion: 该框架为强化学习提供了强大的"为什么是这个，而不是那个？"解释，向可信自主系统迈出了重要一步，通过轨迹级分析增强了智能体长期行为的透明度和可信度。

Abstract: As Reinforcement Learning (RL) agents are increasingly deployed in real-world applications, ensuring their behavior is transparent and trustworthy is paramount. A key component of trust is explainability, yet much of the work in Explainable RL (XRL) focuses on local, single-step decisions. This paper addresses the critical need for explaining an agent's long-term behavior through trajectory-level analysis. We introduce a novel framework that ranks entire trajectories by defining and aggregating a new state-importance metric. This metric combines the classic Q-value difference with a "radical term" that captures the agent's affinity to reach its goal, providing a more nuanced measure of state criticality. We demonstrate that our method successfully identifies optimal trajectories from a heterogeneous collection of agent experiences. Furthermore, by generating counterfactual rollouts from critical states within these trajectories, we show that the agent's chosen path is robustly superior to alternatives, thereby providing a powerful "Why this, and not that?" explanation. Our experiments in standard OpenAI Gym environments validate that our proposed importance metric is more effective at identifying optimal behaviors compared to classic approaches, offering a significant step towards trustworthy autonomous systems.

</details>


### [293] [Parent-Guided Semantic Reward Model (PGSRM): Embedding-Based Reward Functions for Reinforcement Learning of Transformer Language Models](https://arxiv.org/abs/2512.06920)
*Alexandr Plashchinsky*

Main category: cs.LG

TL;DR: PGSRM是一种轻量级强化学习奖励框架，使用父模型参考输出嵌入与子模型生成输出的余弦相似度作为语义奖励，无需人工标注或额外模型训练


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法依赖二元正确性信号、人工偏好数据或训练奖励模型，这些方法成本高且复杂。PGSRM旨在提供一种简单、语义丰富且无需人工标注的替代方案

Method: PGSRM使用父模型参考输出嵌入与子模型生成输出的余弦相似度作为密集语义奖励，替代传统二元奖励或RLHF式奖励建模

Result: 在五个语言任务上，PGSRM比二元奖励基线产生更平滑的奖励改进和更稳定的PPO动态，表明嵌入语义奖励是小型Transformer模型对齐的实用替代方案

Conclusion: 基于嵌入的语义奖励是RLHF式奖励建模的实用替代方案，特别适用于父模型引导的小型Transformer模型对齐，提供更稳定高效的强化学习动态

Abstract: We introduce the Parent-Guided Semantic Reward Model (PGSRM), a lightweight reward framework for reinforcement learning (RL) of transformer language models. PGSRM replaces binary correctness signals, human preference data, and trained reward models with a simple signal: cosine similarity between a parent model's reference output embedding and a child model's generated output for the same input. This yields a dense, semantically meaningful reward with no human annotation or additional model training. We apply PGSRM on five language tasks and find that it produces smoother reward improvement and more stable PPO dynamics than a binary reward baseline, suggesting that embedding-based semantic rewards are a practical alternative to RLHF-style reward modeling for parent-guided alignment in smaller transformer models.

</details>


### [294] [Deep Reinforcement Learning for Phishing Detection with Transformer-Based Semantic Features](https://arxiv.org/abs/2512.06925)
*Aseer Al Faisal*

Main category: cs.LG

TL;DR: 提出QR-DQN方法结合RoBERTa语义嵌入和手工特征进行钓鱼检测，通过分位数回归建模回报分布，在105,000个URL数据集上达到99.86%准确率，泛化能力显著提升。


<details>
  <summary>Details</summary>
Motivation: 钓鱼攻击通过欺诈信息、误导广告和合法网站漏洞欺骗用户泄露个人信息，造成经济损失。传统DQN方法估计单一标量Q值，无法有效处理不确定性，需要更稳健的检测方法。

Method: 提出Quantile Regression Deep Q-Network (QR-DQN)方法，整合RoBERTa语义嵌入和手工设计的词汇特征。使用分位数回归建模回报分布而非单一Q值，提升模型稳定性和泛化能力。使用105,000个URL数据集（来自PhishTank、OpenPhish、Cloudflare等），采用80/20训练测试划分和五折交叉验证。

Result: 测试准确率99.86%，精确率99.75%，召回率99.96%，F1分数99.85%。相比仅使用词汇特征的标准DQN，混合QR-DQN将泛化差距从1.66%降至0.04%。五折交叉验证平均准确率99.90%，标准差0.04%。

Conclusion: 提出的混合QR-DQN方法能有效识别钓鱼威胁，适应不断演变的攻击策略，对未见数据具有良好的泛化能力，为钓鱼检测提供了更稳健的解决方案。

Abstract: Phishing is a cybercrime in which individuals are deceived into revealing personal information, often resulting in financial loss. These attacks commonly occur through fraudulent messages, misleading advertisements, and compromised legitimate websites. This study proposes a Quantile Regression Deep Q-Network (QR-DQN) approach that integrates RoBERTa semantic embeddings with handcrafted lexical features to enhance phishing detection while accounting for uncertainties. Unlike traditional DQN methods that estimate single scalar Q-values, QR-DQN leverages quantile regression to model the distribution of returns, improving stability and generalization on unseen phishing data. A diverse dataset of 105,000 URLs was curated from PhishTank, OpenPhish, Cloudflare, and other sources, and the model was evaluated using an 80/20 train-test split. The QR-DQN framework achieved a test accuracy of 99.86%, precision of 99.75%, recall of 99.96%, and F1-score of 99.85%, demonstrating high effectiveness. Compared to standard DQN with lexical features, the hybrid QR-DQN with lexical and semantic features reduced the generalization gap from 1.66% to 0.04%, indicating significant improvement in robustness. Five-fold cross-validation confirmed model reliability, yielding a mean accuracy of 99.90% with a standard deviation of 0.04%. These results suggest that the proposed hybrid approach effectively identifies phishing threats, adapts to evolving attack strategies, and generalizes well to unseen data.

</details>


### [295] [Evaluating the Sensitivity of BiLSTM Forecasting Models to Sequence Length and Input Noise](https://arxiv.org/abs/2512.06926)
*Salma Albelali,Moataz Ahmed*

Main category: cs.LG

TL;DR: 该研究系统分析了BiLSTM时间序列预测模型对输入序列长度和加性噪声的敏感性，发现长序列增加过拟合风险，噪声降低预测精度，两者同时存在时模型稳定性最差。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在时间序列预测中应用广泛，但现有研究对模型鲁棒性和泛化能力如何受输入数据特性影响缺乏深入探索。特别是BiLSTM架构对输入序列长度和噪声的敏感性尚未得到系统研究。

Method: 开发模块化可复现的预测流程，包含标准化预处理、序列生成、模型训练、验证和评估。在三个不同采样频率的真实数据集上进行控制实验，分析BiLSTM在不同输入条件下的表现。

Result: 主要发现：(1) 长输入序列显著增加过拟合和数据泄露风险，尤其在数据受限环境中；(2) 加性噪声在不同采样频率下都降低预测精度；(3) 两个因素同时存在时模型稳定性下降最严重。高采样频率数据集相对更鲁棒，但仍受双重挑战影响。

Conclusion: 当前基于深度学习的预测流程存在重要局限性，需要数据感知的设计策略。研究加深了对深度学习模型在动态时间序列环境中行为的理解，为开发更可靠、可泛化的预测系统提供了实用见解。

Abstract: Deep learning (DL) models, a specialized class of multilayer neural networks, have become central to time-series forecasting in critical domains such as environmental monitoring and the Internet of Things (IoT). Among these, Bidirectional Long Short-Term Memory (BiLSTM) architectures are particularly effective in capturing complex temporal dependencies. However, the robustness and generalization of such models are highly sensitive to input data characteristics - an aspect that remains underexplored in existing literature. This study presents a systematic empirical analysis of two key data-centric factors: input sequence length and additive noise. To support this investigation, a modular and reproducible forecasting pipeline is developed, incorporating standardized preprocessing, sequence generation, model training, validation, and evaluation. Controlled experiments are conducted on three real-world datasets with varying sampling frequencies to assess BiLSTM performance under different input conditions. The results yield three key findings: (1) longer input sequences significantly increase the risk of overfitting and data leakage, particularly in data-constrained environments; (2) additive noise consistently degrades predictive accuracy across sampling frequencies; and (3) the simultaneous presence of both factors results in the most substantial decline in model stability. While datasets with higher observation frequencies exhibit greater robustness, they remain vulnerable when both input challenges are present. These findings highlight important limitations in current DL-based forecasting pipelines and underscore the need for data-aware design strategies. This work contributes to a deeper understanding of DL model behavior in dynamic time-series environments and provides practical insights for developing more reliable and generalizable forecasting systems.

</details>


### [296] [Adaptive Normalization Mamba with Multi Scale Trend Decomposition and Patch MoE Encoding](https://arxiv.org/abs/2512.06929)
*MinCheol Jeon*

Main category: cs.LG

TL;DR: AdaMamba是一个统一的时间序列预测架构，通过自适应归一化、多尺度趋势提取和上下文序列建模来解决非平稳性、多尺度时间模式和分布偏移问题，相比传统Transformer基线在稳定性和准确性上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现实世界时间序列预测面临非平稳性、多尺度时间模式和分布偏移等挑战，这些因素会降低模型的稳定性和准确性。传统方法在处理这些复杂动态时存在局限性。

Method: AdaMamba采用模块化架构：1）自适应归一化块通过多尺度卷积趋势提取和通道级重新校准去除非平稳成分；2）上下文编码器结合补丁嵌入、位置编码和Mamba增强的Transformer层；3）轻量级预测头生成多步预测；4）反归一化机制通过重新整合局部趋势来重建输出。

Result: 实验评估表明，AdaMamba结合自适应归一化和专家增强的上下文建模，相比传统Transformer基线在稳定性和准确性方面取得了一致的改进。该架构有效缓解了协变量偏移，增强了跨异构数据集的预测可靠性。

Conclusion: AdaMamba提供了一个具有强大表示能力和模块化可扩展性的统一预测架构，支持确定性预测并兼容概率扩展。其设计有效处理了时间序列预测中的关键挑战，为实际应用提供了更稳健的解决方案。

Abstract: Time series forecasting in real world environments faces significant challenges non stationarity, multi scale temporal patterns, and distributional shifts that degrade model stability and accuracy. This study propose AdaMamba, a unified forecasting architecture that integrates adaptive normalization, multi scale trend extraction, and contextual sequence modeling to address these challenges. AdaMamba begins with an Adaptive Normalization Block that removes non stationary components through multi scale convolutional trend extraction and channel wise recalibration, enabling consistent detrending and variance stabilization. The normalized sequence is then processed by a Context Encoder that combines patch wise embeddings, positional encoding, and a Mamba enhanced Transformer layer with a mixture of experts feed forward module, allowing efficient modeling of both long range dependencies and local temporal dynamics. A lightweight prediction head generates multi horizon forecasts, and a denormalization mechanism reconstructs outputs by reintegrating local trends to ensure robustness under varying temporal conditions. AdaMamba provides strong representational capacity with modular extensibility, supporting deterministic prediction and compatibility with probabilistic extensions. Its design effectively mitigates covariate shift and enhances predictive reliability across heterogeneous datasets. Experimental evaluations demonstrate that AdaMamba's combination of adaptive normalization and expert augmented contextual modeling yields consistent improvements in stability and accuracy over conventional Transformer based baselines.

</details>


### [297] [Hidden Leaks in Time Series Forecasting: How Data Leakage Affects LSTM Evaluation Across Configurations and Validation Strategies](https://arxiv.org/abs/2512.06932)
*Salma Albelali,Moataz Ahmed*

Main category: cs.LG

TL;DR: 该研究评估了数据泄漏对LSTM时间序列预测模型性能的影响，发现验证设计（2-way、3-way、10-fold交叉验证）对泄漏敏感性有显著影响，其中10-fold交叉验证在长滞后步长下RMSE增益高达20.5%，而2-way和3-way分割更稳健。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测中广泛使用LSTM等深度学习模型，但评估完整性常因数据泄漏而受损。数据泄漏指在数据集划分前构建输入输出序列，导致未来信息无意中影响训练。本研究旨在探究数据泄漏对性能的影响，特别关注验证设计如何调节泄漏敏感性。

Method: 研究评估了三种常用验证技术（2-way分割、3-way分割、10-fold交叉验证）在泄漏（预分割序列生成）和清洁条件下的表现。清洁条件通过在数据分割前强制时间分离来缓解泄漏风险。使用RMSE增益（泄漏与清洁设置之间的百分比差异）评估泄漏影响，并分析输入窗口大小和滞后步长对泄漏敏感性的影响。

Result: 10-fold交叉验证在长滞后步长下表现出高达20.5%的RMSE增益，而2-way和3-way分割通常保持RMSE增益低于5%。较小的输入窗口和较长的滞后步长会增加泄漏风险，而较大的窗口有助于减少泄漏。

Conclusion: 验证设计对数据泄漏敏感性有显著影响，10-fold交叉验证对泄漏更敏感。需要配置感知、抗泄漏的评估流程来确保可靠的性能估计，特别是在使用小窗口和长滞后步长时。

Abstract: Deep learning models, particularly Long Short-Term Memory (LSTM) networks, are widely used in time series forecasting due to their ability to capture complex temporal dependencies. However, evaluation integrity is often compromised by data leakage, a methodological flaw in which input-output sequences are constructed before dataset partitioning, allowing future information to unintentionally influence training. This study investigates the impact of data leakage on performance, focusing on how validation design mediates leakage sensitivity. Three widely used validation techniques (2-way split, 3-way split, and 10-fold cross-validation) are evaluated under both leaky (pre-split sequence generation) and clean conditions, with the latter mitigating leakage risk by enforcing temporal separation during data splitting prior to sequence construction. The effect of leakage is assessed using RMSE Gain, which measures the relative increase in RMSE caused by leakage, computed as the percentage difference between leaky and clean setups. Empirical results show that 10-fold cross-validation exhibits RMSE Gain values of up to 20.5% at extended lag steps. In contrast, 2-way and 3-way splits demonstrate greater robustness, typically maintaining RMSE Gain below 5% across diverse configurations. Moreover, input window size and lag step significantly influence leakage sensitivity: smaller windows and longer lags increase the risk of leakage, whereas larger windows help reduce it. These findings underscore the need for configuration-aware, leakage-resistant evaluation pipelines to ensure reliable performance estimation.

</details>


### [298] [Comparing BFGS and OGR for Second-Order Optimization](https://arxiv.org/abs/2512.06969)
*Adrian Przybysz,Mikołaj Kołek,Franciszek Sobota,Jarek Duda*

Main category: cs.LG

TL;DR: 比较BFGS的Sherman-Morrison更新与新型在线梯度回归(OGR)方法在Hessian矩阵估计上的性能，OGR在非凸优化中表现更优


<details>
  <summary>Details</summary>
Motivation: 神经网络训练中Hessian矩阵估计面临高维度和高计算成本的挑战，传统BFGS方法在凸性假设下保持正定Hessian近似，但无法处理非凸结构

Method: 提出在线梯度回归(OGR)方法，使用指数移动平均对梯度与位置进行回归来在线估计二阶导数，无需Hessian求逆，可处理非正定Hessian矩阵

Result: 在标准测试函数上评估两种方法，OGR实现了更快的收敛速度和更低的损失，特别是在非凸优化场景中表现显著优于BFGS

Conclusion: OGR作为一种无需Hessian求逆的在线二阶导数估计方法，能够处理非凸优化问题，在神经网络训练等复杂场景中具有更好的性能表现

Abstract: Estimating the Hessian matrix, especially for neural network training, is a challenging problem due to high dimensionality and cost. In this work, we compare the classical Sherman-Morrison update used in the popular BFGS method (Broy-den-Fletcher-Goldfarb-Shanno), which maintains a positive definite Hessian approximation under a convexity assumption, with a novel approach called Online Gradient Regression (OGR). OGR performs regression of gradients against positions using an exponential moving average to estimate second derivatives online, without requiring Hessian inversion. Unlike BFGS, OGR allows estimation of a general (not necessarily positive definite) Hessian and can thus handle non-convex structures. We evaluate both methods across standard test functions and demonstrate that OGR achieves faster convergence and improved loss, particularly in non-convex settings.

</details>


### [299] [Flash Multi-Head Feed-Forward Network](https://arxiv.org/abs/2512.06989)
*Minshen Zhang,Xiang Hu,Jianguo Li,Wei Wu,Kewei Tu*

Main category: cs.LG

TL;DR: FlashMHF提出多头FFN替代传统FFN，通过融合内核和动态加权子网络设计，在提升性能的同时大幅降低内存使用并加速推理。


<details>
  <summary>Details</summary>
Motivation: 受多头注意力机制启发，探索多头FFN替代传统FFN。但直接应用面临两个挑战：内存消耗随头数线性增长，以及模型扩展时中间维度与头维度比例失衡导致可扩展性和表达能力下降。

Method: 提出Flash Multi-Head FFN (FlashMHF)，包含两个关键创新：1) I/O感知融合内核，类似FlashAttention在SRAM中在线计算输出；2) 使用动态加权并行子网络设计，保持中间维度与头维度的平衡比例。

Result: 在128M到1.3B参数规模的模型上验证，FlashMHF相比SwiGLU FFNs持续改善困惑度和下游任务准确率，同时将峰值内存使用降低3-5倍，推理速度提升最高1.08倍。

Conclusion: 多头设计是FFN的优越架构原则，FlashMHF作为Transformer中FFN的强大、高效且可扩展的替代方案。

Abstract: We explore Multi-Head FFN (MH-FFN) as a replacement of FFN in the Transformer architecture, motivated by the structural similarity between single-head attention and FFN. While multi-head mechanisms enhance expressivity in attention, naively applying them to FFNs faces two challenges: memory consumption scaling with the head count, and an imbalanced ratio between the growing intermediate size and the fixed head dimension as models scale, which degrades scalability and expressive power. To address these challenges, we propose Flash Multi-Head FFN (FlashMHF), with two key innovations: an I/O-aware fused kernel computing outputs online in SRAM akin to FlashAttention, and a design using dynamically weighted parallel sub-networks to maintain a balanced ratio between intermediate and head dimensions. Validated on models from 128M to 1.3B parameters, FlashMHF consistently improves perplexity and downstream task accuracy over SwiGLU FFNs, while reducing peak memory usage by 3-5x and accelerating inference by up to 1.08x. Our work establishes the multi-head design as a superior architectural principle for FFNs, presenting FlashMHF as a powerful, efficient, and scalable alternative to FFNs in Transformers.

</details>


### [300] [OXtal: An All-Atom Diffusion Model for Organic Crystal Structure Prediction](https://arxiv.org/abs/2512.06987)
*Emily Jin,Andrei Cristian Nica,Mikhail Galkin,Jarrid Rector-Brooks,Kin Long Kelvin Lee,Santiago Miret,Frances H. Arnold,Michael Bronstein,Avishek Joey Bose,Alexander Tong,Cheng-Hao Liu*

Main category: cs.LG

TL;DR: OXtal是一个100M参数的全原子扩散模型，直接从2D化学图预测3D分子晶体结构，在晶体结构预测任务上比现有方法有数量级提升。


<details>
  <summary>Details</summary>
Motivation: 准确预测分子晶体结构是计算化学中长期存在的开放挑战，对制药和有机半导体等领域至关重要，因为晶体堆积直接影响有机固体的物理化学性质。

Method: 提出OXtal大规模扩散模型，放弃显式等变架构，采用数据增强策略；提出结晶启发的无晶格训练方案S^4，避免显式晶格参数化，实现全原子分辨率下的可扩展架构。

Result: 在60万实验验证晶体结构数据集上，OXtal实现构象RMSD<0.5Å，达到80%以上的堆积相似率，比现有机器学习方法有数量级提升，比传统量子化学方法便宜数量级。

Conclusion: OXtal能够有效建模分子结晶的热力学和动力学规律，为晶体结构预测提供了高效准确的解决方案。

Abstract: Accurately predicting experimentally-realizable 3D molecular crystal structures from their 2D chemical graphs is a long-standing open challenge in computational chemistry called crystal structure prediction (CSP). Efficiently solving this problem has implications ranging from pharmaceuticals to organic semiconductors, as crystal packing directly governs the physical and chemical properties of organic solids. In this paper, we introduce OXtal, a large-scale 100M parameter all-atom diffusion model that directly learns the conditional joint distribution over intramolecular conformations and periodic packing. To efficiently scale OXtal, we abandon explicit equivariant architectures imposing inductive bias arising from crystal symmetries in favor of data augmentation strategies. We further propose a novel crystallization-inspired lattice-free training scheme, Stoichiometric Stochastic Shell Sampling ($S^4$), that efficiently captures long-range interactions while sidestepping explicit lattice parametrization -- thus enabling more scalable architectural choices at all-atom resolution. By leveraging a large dataset of 600K experimentally validated crystal structures (including rigid and flexible molecules, co-crystals, and solvates), OXtal achieves orders-of-magnitude improvements over prior ab initio machine learning CSP methods, while remaining orders of magnitude cheaper than traditional quantum-chemical approaches. Specifically, OXtal recovers experimental structures with conformer $\text{RMSD}_1<0.5$ Å and attains over 80\% packing similarity rate, demonstrating its ability to model both thermodynamic and kinetic regularities of molecular crystallization.

</details>


### [301] [Block Sparse Flash Attention](https://arxiv.org/abs/2512.07011)
*Daniel Ohayon,Itay Lamprecht,Itay Hubara,Israel Cohen,Daniel Soudry,Noam Elata*

Main category: cs.LG

TL;DR: BSFA是一种无需训练的注意力加速方法，通过计算精确的查询-键相似度选择最重要的值块，跳过约50%的计算和内存传输，在保持模型质量的同时实现长上下文推理加速。


<details>
  <summary>Details</summary>
Motivation: 现代大语言模型需要长上下文进行推理和多文档任务，但注意力机制的二次复杂度造成了严重的计算瓶颈，需要高效的加速方法。

Method: BSFA通过计算精确的查询-键相似度，为每个查询选择top-k最重要的值块，通过比较每块最大分数与校准阈值，跳过约50%的修剪块计算。采用无需训练的方法，仅需在小数据集上进行一次性阈值校准来学习每层每头的注意力分数分布。

Result: 在Llama-3.1-8B上，BSFA在真实世界推理基准上实现最高1.10倍加速，在"大海捞针"检索任务中实现最高1.24倍加速，同时保持99%以上的基线准确率，某些配置甚至通过关注最相关内容提高了准确率。

Conclusion: BSFA作为一种即插即用的FlashAttention替代方案，在保持模型质量的同时显著加速长上下文推理，大幅优于现有的稀疏注意力方法。

Abstract: Modern large language models increasingly require long contexts for reasoning and multi-document tasks, but attention's quadratic complexity creates a severe computational bottleneck. We present Block-Sparse FlashAttention (BSFA), a drop-in replacement that accelerates long-context inference while preserving model quality. Unlike methods that predict importance before computing scores, BSFA computes exact query-key similarities to select the top-k most important value blocks for each query. By comparing per-block maximum scores against calibrated thresholds, we skip approximately 50% of the computation and memory transfers for pruned blocks. Our training-free approach requires only a one-time threshold calibration on a small dataset to learn the per-layer and per-head attention score distributions. We provide a CUDA kernel implementation that can be used as a drop-in replacement for FlashAttention. On Llama-3.1-8B, BSFA achieves up to 1.10x speedup on real-world reasoning benchmarks and up to 1.24x for needle-in-a-haystack retrieval tasks while maintaining above 99% baseline accuracy, with certain configurations even improving accuracy by focusing on the most relevant content, substantially outperforming existing sparse attention methods. The implementation is available at https://github.com/Danielohayon/Block-Sparse-Flash-Attention

</details>


### [302] [Toward Reliable Machine Unlearning: Theory, Algorithms, and Evaluation](https://arxiv.org/abs/2512.06993)
*Ali Ebrahimpour-Boroojeny*

Main category: cs.LG

TL;DR: 本文提出两种新的机器学习遗忘方法：AMUN用于随机样本遗忘，通过对抗样本微调降低遗忘样本的置信度；TRW用于类别遗忘，通过倾斜重加权分布近似重训练模型的行为，均超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习遗忘方法存在不足，无法有效模拟重训练模型的行为，导致隐私泄露风险。需要开发更有效的遗忘方法，既能有效移除特定数据或类别的影响，又能保持模型在剩余数据上的性能。

Method: 1. AMUN方法：通过生成对抗样本来微调模型，降低对遗忘样本的置信度，同时保持对剩余样本的预测相似性。2. FastClip方法：通过层谱范数裁剪控制Lipschitz常数，提高模型平滑性。3. TRW方法：针对类别遗忘，估计类间相似性，构建倾斜重加权分布作为微调目标，近似重训练模型的行为。

Result: AMUN在图像分类任务上超越了现有方法，基于SOTA MIA评分表现最佳。TRW在多个基准测试中匹配或超越了现有遗忘方法。理论分析表明模型平滑性是影响AMUN性能的关键因素，而FastClip能有效控制模型平滑性。

Conclusion: 本文提出的AMUN和TRW方法为机器遗忘提供了新的有效解决方案，AMUN通过对抗样本微调实现样本级遗忘，TRW通过分布近似实现类别级遗忘，均能更好地模拟重训练模型的行为，减少隐私泄露风险。

Abstract: We propose new methodologies for both unlearning random set of samples and class unlearning and show that they outperform existing methods. The main driver of our unlearning methods is the similarity of predictions to a retrained model on both the forget and remain samples. We introduce Adversarial Machine UNlearning (AMUN), which surpasses prior state-of-the-art methods for image classification based on SOTA MIA scores. AMUN lowers the model's confidence on forget samples by fine-tuning on their corresponding adversarial examples. Through theoretical analysis, we identify factors governing AMUN's performance, including smoothness. To facilitate training of smooth models with a controlled Lipschitz constant, we propose FastClip, a scalable method that performs layer-wise spectral-norm clipping of affine layers. In a separate study, we show that increased smoothness naturally improves adversarial example transfer, thereby supporting the second factor above.
  Following the same principles for class unlearning, we show that existing methods fail in replicating a retrained model's behavior by introducing a nearest-neighbor membership inference attack (MIA-NN) that uses the probabilities assigned to neighboring classes to detect unlearned samples and demonstrate the vulnerability of such methods. We then propose a fine-tuning objective that mitigates this leakage by approximating, for forget-class inputs, the distribution over remaining classes that a model retrained from scratch would produce. To construct this approximation, we estimate inter-class similarity and tilt the target model's distribution accordingly. The resulting Tilted ReWeighting(TRW) distribution serves as the desired target during fine-tuning. Across multiple benchmarks, TRW matches or surpasses existing unlearning methods on prior metrics.

</details>


### [303] [Always Keep Your Promises: DynamicLRP, A Model-Agnostic Solution To Layer-Wise Relevance Propagation](https://arxiv.org/abs/2512.07010)
*Kevin Lee,Pablo Millan Arias*

Main category: cs.LG

TL;DR: DynamicLRP：首个模型无关的层间相关性传播框架，通过张量操作级分解和Promise系统实现真正的架构无关性，无需模型修改即可应用于任意计算图


<details>
  <summary>Details</summary>
Motivation: 现有LRP实现基于模块级别，需要架构特定的传播规则和修改，限制了目标模型的通用性和实现的可持续性，难以适应不断演进的神经网络架构

Method: 提出DynamicLRP框架：1）在计算图内将归因分解到单个操作级别；2）引入Promise系统实现延迟激活解析；3）独立于反向传播机制，无需模型修改

Result: 在31,465个计算图节点（15种架构）上实现99.92%节点覆盖率，包括Mamba、Whisper、DePlot等复杂模型；性能匹配或超过专用实现（VGG上ABPC 1.77 vs 1.69，ViT相当性能，RoBERTa-large和Flan-T5-large在SQuADv2上分别达到93.70%和95.06%的top-1归因准确率）

Conclusion: DynamicLRP通过操作级分解和Promise系统建立了可持续、可扩展的LRP基础，实现了真正的架构无关性，为不断演进的神经网络架构提供了通用的可解释性解决方案

Abstract: Layer-wise Relevance Propagation (LRP) provides principled attribution for neural networks through conservation properties and foundations in Deep Taylor Decomposition. However, existing implementations operate at the module level, requiring architecture-specific propagation rules and modifications. These limit the generality of target model and sustainability of implementations as architectures evolve. We introduce DynamicLRP, a model-agnostic LRP framework operating at the tensor operation level. By decomposing attribution to individual operations within computation graphs and introducing a novel mechanism for deferred activation resolution, named the Promise System, our approach achieves true architecture agnosticity while maintaining LRP's theoretical guarantees. This design operates independently of backpropagation machinery, enabling operation on arbitrary computation graphs without model modification and side-by-side execution with gradient backpropagation. Being based on computation graphs, this method is theoretically extensible to other deep learning libraries that support auto-differentiation. We demonstrate faithfulness matching or exceeding specialized implementations (1.77 vs 1.69 ABPC on VGG, equivalent performance on ViT, 93.70\% and 95.06\% top-1 attribution accuracy for explaining RoBERTa-large and Flan-T5-large answers on SQuADv2, respectively) while maintaining practical efficiency on models with hundreds of millions of parameters. We achieved 99.92\% node coverage across 31,465 computation graph nodes from 15 diverse architectures, including state-space models (Mamba), audio transformers (Whisper), and multimodal systems (DePlot) without any model-specific code with rules for 47 fundamental operations implemented. Our operation-level decomposition and Promise System establish a sustainable, extensible foundation for LRP across evolving architectures.

</details>


### [304] [Pay Less Attention to Function Words for Free Robustness of Vision-Language Models](https://arxiv.org/abs/2512.07222)
*Qiwei Tian,Chenhao Lin,Zhengyu Zhao,Chao Shen*

Main category: cs.LG

TL;DR: 提出Function-word De-Attention (FDA)方法，通过减除功能词的交叉注意力来增强视觉语言模型的对抗鲁棒性，在保持性能的同时显著降低攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有鲁棒视觉语言模型在鲁棒性和性能之间存在权衡，研究发现功能词是跨模态对抗攻击的脆弱点，需要一种方法来减轻功能词的影响。

Method: 提出FDA方法，类似差分放大器，计算原始交叉注意力和功能词交叉注意力，在注意力头中将后者从前者中差分减除，从而获得更对齐和鲁棒的视觉语言模型。

Result: 在3个模型、2个下游任务、3个数据集上的实验显示：检索任务上平均攻击成功率下降18/13/53%，性能仅下降0.2/0.3/0.6%；视觉定位任务上攻击成功率下降90%，性能提升0.3%。

Conclusion: FDA方法能有效提升视觉语言模型的对抗鲁棒性，同时保持或轻微提升性能，具有良好的可扩展性、泛化性和零样本性能。

Abstract: To address the trade-off between robustness and performance for robust VLM, we observe that function words could incur vulnerability of VLMs against cross-modal adversarial attacks, and propose Function-word De-Attention (FDA) accordingly to mitigate the impact of function words. Similar to differential amplifiers, our FDA calculates the original and the function-word cross-attention within attention heads, and differentially subtracts the latter from the former for more aligned and robust VLMs. Comprehensive experiments include 2 SOTA baselines under 6 different attacks on 2 downstream tasks, 3 datasets, and 3 models. Overall, our FDA yields an average 18/13/53% ASR drop with only 0.2/0.3/0.6% performance drops on the 3 tested models on retrieval, and a 90% ASR drop with a 0.3% performance gain on visual grounding. We demonstrate the scalability, generalization, and zero-shot performance of FDA experimentally, as well as in-depth ablation studies and analysis. Code will be made publicly at https://github.com/michaeltian108/FDA.

</details>


### [305] [Recover-to-Forget: Gradient Reconstruction from LoRA for Efficient LLM Unlearning](https://arxiv.org/abs/2512.07374)
*Yezi Liu,Hanning Chen,Wenjun Huang,Yang Ni,Mohsen Imani*

Main category: cs.LG

TL;DR: R2F是一种基于LoRA适配器重建完整模型梯度方向的高效大语言模型遗忘框架，通过训练梯度解码器近似完整模型梯度，无需完整模型微调或原始训练数据。


<details>
  <summary>Details</summary>
Motivation: 大型基础模型（如LLMs）的遗忘对于实现动态知识更新、执行数据删除权利和纠正模型行为至关重要。现有遗忘方法通常需要完整模型微调或访问原始训练数据，限制了其可扩展性和实用性。

Method: R2F框架通过使用多个改写提示计算LoRA参数的梯度，并训练梯度解码器来近似相应的完整模型梯度。解码器在代理模型上训练，然后迁移到目标模型，适用于大型或黑盒模型。

Result: 实验结果表明，R2F在保持模型整体性能的同时实现了有效的遗忘，为预训练LLMs提供了一种无需完整重新训练或访问内部参数的可扩展轻量级替代方案。

Conclusion: R2F通过LoRA适配器重建完整模型梯度方向，为大型语言模型提供了一种高效、可扩展的遗忘框架，解决了现有方法在可扩展性和实用性方面的限制。

Abstract: Unlearning in large foundation models (e.g., LLMs) is essential for enabling dynamic knowledge updates, enforcing data deletion rights, and correcting model behavior. However, existing unlearning methods often require full-model fine-tuning or access to the original training data, which limits their scalability and practicality. In this work, we introduce Recover-to-Forget (R2F), a novel framework for efficient unlearning in LLMs based on reconstructing full-model gradient directions from low-rank LoRA adapter updates. Rather than performing backpropagation through the full model, we compute gradients with respect to LoRA parameters using multiple paraphrased prompts and train a gradient decoder to approximate the corresponding full-model gradients. To ensure applicability to larger or black-box models, the decoder is trained on a proxy model and transferred to target models. We provide a theoretical analysis of cross-model generalization and demonstrate that our method achieves effective unlearning while preserving general model performance. Experimental results demonstrate that R2F offers a scalable and lightweight alternative for unlearning in pretrained LLMs without requiring full retraining or access to internal parameters.

</details>


### [306] [Transferring Clinical Knowledge into ECGs Representation](https://arxiv.org/abs/2512.07021)
*Jose Geraldo Fernandes,Luiz Facury de Souza,Pedro Robles Dutenhefner,Gisele L. Pappa,Wagner Meira*

Main category: cs.LG

TL;DR: 提出三阶段训练范式，将多模态临床数据知识迁移到单模态ECG编码器，提升ECG分类准确性和可解释性


<details>
  <summary>Details</summary>
Motivation: 深度学习ECG分类模型虽然准确率高，但黑盒特性缺乏可解释性，阻碍临床应用。需要建立更可信、可解释的ECG分类模型。

Method: 三阶段训练范式：1）自监督联合嵌入预训练，利用多模态临床数据（实验室检查、生命体征、生物特征）丰富ECG表示；2）仅需ECG信号进行推理；3）通过预测相关实验室异常来间接解释模型输出

Result: 在MIMIC-IV-ECG数据集上，模型在多标签诊断分类中优于标准单信号基线，显著缩小了与需要所有数据推理的全多模态模型的性能差距

Conclusion: 该方法为创建更准确、可信的ECG分类模型提供了实用有效的方法，通过将抽象预测转化为基于生理学的解释，为AI安全融入临床工作流程提供了有前景的路径

Abstract: Deep learning models have shown high accuracy in classifying electrocardiograms (ECGs), but their black box nature hinders clinical adoption due to a lack of trust and interpretability. To address this, we propose a novel three-stage training paradigm that transfers knowledge from multimodal clinical data (laboratory exams, vitals, biometrics) into a powerful, yet unimodal, ECG encoder. We employ a self-supervised, joint-embedding pre-training stage to create an ECG representation that is enriched with contextual clinical information, while only requiring the ECG signal at inference time. Furthermore, as an indirect way to explain the model's output we train it to also predict associated laboratory abnormalities directly from the ECG embedding. Evaluated on the MIMIC-IV-ECG dataset, our model outperforms a standard signal-only baseline in multi-label diagnosis classification and successfully bridges a substantial portion of the performance gap to a fully multimodal model that requires all data at inference. Our work demonstrates a practical and effective method for creating more accurate and trustworthy ECG classification models. By converting abstract predictions into physiologically grounded \emph{explanations}, our approach offers a promising path toward the safer integration of AI into clinical workflows.

</details>


### [307] [LUNE: Efficient LLM Unlearning via LoRA Fine-Tuning with Negative Examples](https://arxiv.org/abs/2512.07375)
*Yezi Liu,Hanning Chen,Wenjun Huang,Yang Ni,Mohsen Imani*

Main category: cs.LG

TL;DR: LUNE：基于LoRA的负例遗忘框架，通过仅更新低秩适配器实现轻量级知识遗忘，计算成本降低约一个数量级


<details>
  <summary>Details</summary>
Motivation: 大型语言模型难以移除特定信息，传统遗忘方法计算成本高，不适用于实际部署。需要一种轻量级、高效的遗忘方法来解决隐私、偏见缓解和知识修正问题。

Method: 提出LoRA-based Unlearning with Negative Examples (LUNE)框架，仅更新低秩适配器而冻结主干网络，通过负例训练针对中间表示进行知识抑制或替换。

Result: 在多个事实遗忘任务上的实验表明：1）LUNE在效果上与全微调和内存编辑方法相当；2）计算成本降低约一个数量级。

Conclusion: LUNE提供了一种高效、轻量级的模型遗忘解决方案，通过局部化编辑避免了破坏性全局变化，在保持性能的同时大幅降低了计算和内存需求。

Abstract: Large language models (LLMs) possess vast knowledge acquired from extensive training corpora, but they often cannot remove specific pieces of information when needed, which makes it hard to handle privacy, bias mitigation, and knowledge correction. Traditional model unlearning approaches require computationally expensive fine-tuning or direct weight editing, making them impractical for real-world deployment. In this work, we introduce LoRA-based Unlearning with Negative Examples (LUNE), a lightweight framework that performs negative-only unlearning by updating only low-rank adapters while freezing the backbone, thereby localizing edits and avoiding disruptive global changes. Leveraging Low-Rank Adaptation (LoRA), LUNE targets intermediate representations to suppress (or replace) requested knowledge with an order-of-magnitude lower compute and memory than full fine-tuning or direct weight editing. Extensive experiments on multiple factual unlearning tasks show that LUNE: (I) achieves effectiveness comparable to full fine-tuning and memory-editing methods, and (II) reduces computational cost by about an order of magnitude.

</details>


### [308] [Transformation of Biological Networks into Images via Semantic Cartography for Visual Interpretation and Scalable Deep Analysis](https://arxiv.org/abs/2512.07040)
*Sakib Mostafa,Lei Xing,Md. Tauhidul Islam*

Main category: cs.LG

TL;DR: Graph2Image将大型生物网络转换为二维图像，利用CNN分析，解决了传统方法在可扩展性、长距离依赖和可解释性方面的限制。


<details>
  <summary>Details</summary>
Motivation: 生物网络规模庞大且复杂，传统分析方法（包括深度学习方法）面临可扩展性有限、长距离依赖过平滑、多模态整合困难、表达能力受限和可解释性差等挑战。

Method: 将大型生物网络转换为二维图像集合：将代表性网络节点在2D网格上空间排列，使节点解耦为图像，从而可以利用具有全局感受野和多尺度金字塔的卷积神经网络。

Result: 在多个大规模生物网络数据集上，Graph2Image比现有方法提高了高达67.2%的分类准确率，提供了可解释的可视化，揭示了生物学上一致的模式，并能在个人计算机上分析超过10亿节点的超大网络。

Conclusion: Graph2Image为生物网络分析提供了一个可扩展、可解释且支持多模态的方法，为疾病诊断和复杂生物系统研究提供了新机会。

Abstract: Complex biological networks are fundamental to biomedical science, capturing interactions among molecules, cells, genes, and tissues. Deciphering these networks is critical for understanding health and disease, yet their scale and complexity represent a daunting challenge for current computational methods. Traditional biological network analysis methods, including deep learning approaches, while powerful, face inherent challenges such as limited scalability, oversmoothing long-range dependencies, difficulty in multimodal integration, expressivity bounds, and poor interpretability. We present Graph2Image, a framework that transforms large biological networks into sets of two-dimensional images by spatially arranging representative network nodes on a 2D grid. This transformation decouples the nodes as images, enabling the use of convolutional neural networks (CNNs) with global receptive fields and multi-scale pyramids, thus overcoming limitations of existing biological network analysis methods in scalability, memory efficiency, and long-range context capture. Graph2Image also facilitates seamless integration with other imaging and omics modalities and enhances interpretability through direct visualization of node-associated images. When applied to several large-scale biological network datasets, Graph2Image improved classification accuracy by up to 67.2% over existing methods and provided interpretable visualizations that revealed biologically coherent patterns. It also allows analysis of very large biological networks (nodes > 1 billion) on a personal computer. Graph2Image thus provides a scalable, interpretable, and multimodal-ready approach for biological network analysis, offering new opportunities for disease diagnosis and the study of complex biological systems.

</details>


### [309] [Self-Supervised Learning on Molecular Graphs: A Systematic Investigation of Masking Design](https://arxiv.org/abs/2512.07064)
*Jiannan Yang,Veronika Thost,Tengfei Ma*

Main category: cs.LG

TL;DR: 该研究将分子图的自监督学习预训练-微调流程统一到概率框架中，系统评估了掩码策略的三个核心设计维度，发现对于常见节点级预测任务，复杂掩码分布相比均匀采样并无优势，而预测目标选择和编码器架构的协同更为关键。


<details>
  <summary>Details</summary>
Motivation: 当前基于掩码的分子表示学习方法多基于启发式设计，缺乏系统评估，难以确定哪些设计选择真正有效。需要建立统一框架来透明比较和理解不同掩码策略。

Method: 将整个预训练-微调流程统一到概率框架中，在严格控制条件下系统研究三个核心设计维度：掩码分布、预测目标和编码器架构。使用信息论度量评估预训练信号的信息量，并与下游性能关联。

Result: 研究发现：1) 对于常见节点级预测任务，复杂掩码分布相比均匀采样没有一致优势；2) 预测目标选择和编码器架构的协同更为关键；3) 转向语义更丰富的预测目标能带来显著下游性能提升，特别是与表达能力强的图Transformer编码器结合时。

Conclusion: 研究为分子图自监督学习方法开发提供了实用指导：应更关注预测目标选择和编码器架构的协同优化，而非过度设计复杂掩码策略。语义丰富的预测目标与图Transformer编码器的组合能获得最佳性能。

Abstract: Self-supervised learning (SSL) plays a central role in molecular representation learning. Yet, many recent innovations in masking-based pretraining are introduced as heuristics and lack principled evaluation, obscuring which design choices are genuinely effective. This work cast the entire pretrain-finetune workflow into a unified probabilistic framework, enabling a transparent comparison and deeper understanding of masking strategies. Building on this formalism, we conduct a controlled study of three core design dimensions: masking distribution, prediction target, and encoder architecture, under rigorously controlled settings. We further employ information-theoretic measures to assess the informativeness of pretraining signals and connect them to empirically benchmarked downstream performance. Our findings reveal a surprising insight: sophisticated masking distributions offer no consistent benefit over uniform sampling for common node-level prediction tasks. Instead, the choice of prediction target and its synergy with the encoder architecture are far more critical. Specifically, shifting to semantically richer targets yields substantial downstream improvements, particularly when paired with expressive Graph Transformer encoders. These insights offer practical guidance for developing more effective SSL methods for molecular graphs.

</details>


### [310] [Group Representational Position Encoding](https://arxiv.org/abs/2512.07805)
*Yifan Zhang,Zixiang Chen,Yifeng Liu,Zhen Qin,Huizhuo Yuan,Kangping Xu,Yang Yuan,Quanquan Gu,Andrew Chi-Chih Yao*

Main category: cs.LG

TL;DR: GRAPE是一个基于群作用的统一位置编码框架，包含乘法旋转和加法logit偏置两种机制，统一了RoPE和ALiBi等现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有位置编码方法（如RoPE和ALiBi）缺乏统一的理论框架，GRAPE旨在通过群作用理论为位置编码提供系统化的设计空间。

Method: 基于群作用理论，提出两种机制：1）乘法GRAPE：使用SO(d)群中的旋转操作，通过矩阵指数实现相对、组合、保范的映射；2）加法GRAPE：使用GL群中的幂幺作用，产生加法logit偏置。

Result: GRAPE统一了现有位置编码方法：RoPE是乘法GRAPE在特定配置下的特例，ALiBi和FoX是加法GRAPE的特例。同时扩展了几何表达能力，支持跨子空间特征耦合。

Conclusion: GRAPE为长上下文模型中的位置几何提供了原则性的设计框架，统一并扩展了现有位置编码方法，为模型架构设计提供了理论基础。

Abstract: We present GRAPE (Group RepresentAtional Position Encoding), a unified framework for positional encoding based on group actions. GRAPE brings together two families of mechanisms: (i) multiplicative rotations (Multiplicative GRAPE) in $\mathrm{SO}(d)$ and (ii) additive logit biases (Additive GRAPE) arising from unipotent actions in the general linear group $\mathrm{GL}$. In Multiplicative GRAPE, a position $n \in \mathbb{Z}$ (or $t \in \mathbb{R}$) acts as $\mathbf{G}(n)=\exp(n\,ω\,\mathbf{L})$ with a rank-2 skew generator $\mathbf{L} \in \mathbb{R}^{d \times d}$, yielding a relative, compositional, norm-preserving map with a closed-form matrix exponential. RoPE is recovered exactly when the $d/2$ planes are the canonical coordinate pairs with log-uniform spectrum. Learned commuting subspaces and compact non-commuting mixtures strictly extend this geometry to capture cross-subspace feature coupling at $O(d)$ and $O(r d)$ cost per head, respectively. In Additive GRAPE, additive logits arise as rank-1 (or low-rank) unipotent actions, recovering ALiBi and the Forgetting Transformer (FoX) as exact special cases while preserving an exact relative law and streaming cacheability. Altogether, GRAPE supplies a principled design space for positional geometry in long-context models, subsuming RoPE and ALiBi as special cases. Project Page: https://github.com/model-architectures/GRAPE.

</details>


### [311] [Procrustean Bed for AI-Driven Retrosynthesis: A Unified Framework for Reproducible Evaluation](https://arxiv.org/abs/2512.07079)
*Anton Morgunov,Victor S. Batista*

Main category: cs.LG

TL;DR: RetroCast是一个统一的CASP评估套件，标准化模型输出以实现公平比较，揭示了可解性与路线质量之间的差异，并发现了搜索方法在复杂合成计划中的性能衰减问题。


<details>
  <summary>Details</summary>
Motivation: 当前计算机辅助合成规划领域缺乏标准化评估基础设施，现有指标过于关注拓扑完成度而忽视化学有效性，导致进展评估不准确。

Method: 开发RetroCast统一评估套件，将异构模型输出标准化为通用模式；包含可重复的基准测试流程（分层抽样和自助置信区间）和SynthArena交互式路线检查平台。

Result: 发现"可解性"（库存终止率）与路线质量存在差异：高可解性分数常掩盖化学无效性，且与实验真实结果不相关；搜索方法在复杂长程合成计划中存在"复杂度悬崖"性能衰减。

Conclusion: RetroCast为CASP领域提供了透明、可重复的评估框架，揭示了现有评估指标的局限性，并发布了完整框架、基准定义和标准化预测数据库以支持领域发展。

Abstract: Progress in computer-aided synthesis planning (CASP) is obscured by the lack of standardized evaluation infrastructure and the reliance on metrics that prioritize topological completion over chemical validity. We introduce RetroCast, a unified evaluation suite that standardizes heterogeneous model outputs into a common schema to enable statistically rigorous, apples-to-apples comparison. The framework includes a reproducible benchmarking pipeline with stratified sampling and bootstrapped confidence intervals, accompanied by SynthArena, an interactive platform for qualitative route inspection. We utilize this infrastructure to evaluate leading search-based and sequence-based algorithms on a new suite of standardized benchmarks. Our analysis reveals a divergence between "solvability" (stock-termination rate) and route quality; high solvability scores often mask chemical invalidity or fail to correlate with the reproduction of experimental ground truths. Furthermore, we identify a "complexity cliff" in which search-based methods, despite high solvability rates, exhibit a sharp performance decay in reconstructing long-range synthetic plans compared to sequence-based approaches. We release the full framework, benchmark definitions, and a standardized database of model predictions to support transparent and reproducible development in the field.

</details>


### [312] [TRACE: A Generalizable Drift Detector for Streaming Data-Driven Optimization](https://arxiv.org/abs/2512.07082)
*Yuan-Ting Zhong,Ting Huang,Xiaolin Xiao,Yue-Jiao Gong*

Main category: cs.LG

TL;DR: 提出TRACE方法，用于检测流数据中的概念漂移，具有可迁移性，可集成到流优化器中实现自适应优化


<details>
  <summary>Details</summary>
Motivation: 现有流数据驱动优化方法存在限制性假设（如固定漂移间隔、完全环境可观测性），难以适应多样化的动态环境，需要更灵活的概念漂移检测方法

Method: TRACE使用原则化的标记化策略从数据流中提取统计特征，通过基于注意力的序列学习建模漂移模式，实现对新数据集的准确检测和漂移模式的可迁移性

Result: 在多样化基准测试中表现出优越的泛化能力、鲁棒性和有效性，能够集成到流优化器中实现自适应优化

Conclusion: TRACE是一种可迁移的概念漂移估计器，能够有效检测不同时间尺度的流数据分布变化，为流数据驱动优化提供了灵活有效的解决方案

Abstract: Many optimization tasks involve streaming data with unknown concept drifts, posing a significant challenge as Streaming Data-Driven Optimization (SDDO). Existing methods, while leveraging surrogate model approximation and historical knowledge transfer, are often under restrictive assumptions such as fixed drift intervals and fully environmental observability, limiting their adaptability to diverse dynamic environments. We propose TRACE, a TRAnsferable C}oncept-drift Estimator that effectively detects distributional changes in streaming data with varying time scales. TRACE leverages a principled tokenization strategy to extract statistical features from data streams and models drift patterns using attention-based sequence learning, enabling accurate detection on unseen datasets and highlighting the transferability of learned drift patterns. Further, we showcase TRACE's plug-and-play nature by integrating it into a streaming optimizer, facilitating adaptive optimization under unknown drifts. Comprehensive experimental results on diverse benchmarks demonstrate the superior generalization, robustness, and effectiveness of our approach in SDDO scenarios.

</details>


### [313] [The Geometry of Persona: Disentangling Personality from Reasoning in Large Language Models](https://arxiv.org/abs/2512.07092)
*Zhixiang Wang*

Main category: cs.LG

TL;DR: Soul Engine框架通过线性表示假设，在不修改基础模型权重的情况下实现个性化LLM，避免了传统微调带来的"对齐税"问题。


<details>
  <summary>Details</summary>
Motivation: 当前个性化大语言模型的部署受到稳定性-可塑性困境的限制，传统对齐方法（如监督微调）依赖随机权重更新，往往导致"对齐税"——降低通用推理能力。

Method: 基于线性表示假设（认为人格特质存在于正交线性子空间中），提出Soul Engine框架。使用SoulBench数据集（通过动态上下文采样构建），在冻结的Qwen-2.5基础模型上采用双头架构，提取解耦的人格向量而不修改骨干权重。

Result: 实验展示了三个突破：1）高精度人格分析：模型对心理学真实数据的均方误差为0.011；2）几何正交性：T-SNE可视化确认人格流形是独特且连续的，支持"零样本人格注入"同时保持原始模型智能；3）确定性控制：通过向量算术实现稳健的行为控制，经过广泛的消融研究验证。

Conclusion: 这项工作挑战了微调对个性化的必要性。通过从概率提示转向确定性潜在干预，为安全、可控的AI个性化提供了数学上严谨的基础。

Abstract: Background: The deployment of personalized Large Language Models (LLMs) is currently constrained by the stability-plasticity dilemma. Prevailing alignment methods, such as Supervised Fine-Tuning (SFT), rely on stochastic weight updates that often incur an "alignment tax" -- degrading general reasoning capabilities.
  Methods: We propose the Soul Engine, a framework based on the Linear Representation Hypothesis, which posits that personality traits exist as orthogonal linear subspaces. We introduce SoulBench, a dataset constructed via dynamic contextual sampling. Using a dual-head architecture on a frozen Qwen-2.5 base, we extract disentangled personality vectors without modifying the backbone weights.
  Results: Our experiments demonstrate three breakthroughs. First, High-Precision Profiling: The model achieves a Mean Squared Error (MSE) of 0.011 against psychological ground truth. Second, Geometric Orthogonality: T-SNE visualization confirms that personality manifolds are distinct and continuous, allowing for "Zero-Shot Personality Injection" that maintains original model intelligence. Third, Deterministic Steering: We achieve robust control over behavior via vector arithmetic, validated through extensive ablation studies.
  Conclusion: This work challenges the necessity of fine-tuning for personalization. By transitioning from probabilistic prompting to deterministic latent intervention, we provide a mathematically rigorous foundation for safe, controllable AI personalization.

</details>


### [314] [Dual Refinement Cycle Learning: Unsupervised Text Classification of Mamba and Community Detection on Text Attributed Graph](https://arxiv.org/abs/2512.07100)
*Hong Wang,Yinglong Zhang,Hanhan Guo,Xuewen Xia,Xing Xu*

Main category: cs.LG

TL;DR: DRCL是一个完全无监督的框架，通过双向精炼循环整合图结构和文本语义信息，在无标签和类别定义的实际场景中提升社区检测质量。


<details>
  <summary>Details</summary>
Motivation: 预训练语言模型依赖大量标注数据难以部署到现实世界的文本属性网络中，而传统社区检测方法忽略文本语义信息，限制了在内容组织、推荐和风险监控等下游应用中的实用性。

Method: DRCL通过热启动初始化和GCN社区检测模块与文本语义建模模块之间的双向精炼循环，整合结构和语义信息。两个模块迭代交换伪标签，使语义线索增强结构聚类，结构模式指导文本表示学习。

Result: 在多个文本属性图数据集上，DRCL持续提升发现社区的结构和语义质量。仅使用DRCL社区信号训练的Mamba分类器达到与监督模型相当的准确率。

Conclusion: DRCL展示了在标注数据稀缺或成本高昂的大规模系统中部署的潜力，为无监督的文本属性网络分析提供了有效解决方案。

Abstract: Pretrained language models offer strong text understanding capabilities but remain difficult to deploy in real-world text-attributed networks due to their heavy dependence on labeled data. Meanwhile, community detection methods typically ignore textual semantics, limiting their usefulness in downstream applications such as content organization, recommendation, and risk monitoring. To overcome these limitations, we present Dual Refinement Cycle Learning (DRCL), a fully unsupervised framework designed for practical scenarios where no labels or category definitions are available.
  DRCL integrates structural and semantic information through a warm-start initialization and a bidirectional refinement cycle between a GCN-based Community Detection Module (GCN-CDM) and a Text Semantic Modeling Module (TSMM). The two modules iteratively exchange pseudo-labels, allowing semantic cues to enhance structural clustering and structural patterns to guide text representation learning without manual supervision.
  Across several text-attributed graph datasets, DRCL consistently improves the structural and semantic quality of discovered communities. Moreover, a Mamba-based classifier trained solely from DRCL's community signals achieves accuracy comparable to supervised models, demonstrating its potential for deployment in large-scale systems where labeled data are scarce or costly.

</details>


### [315] [FOAM: Blocked State Folding for Memory-Efficient LLM Training](https://arxiv.org/abs/2512.07112)
*Ziqing Wen,Jiahuan Wang,Ping Luo,Dongsheng Li,Tao Sun*

Main category: cs.LG

TL;DR: FOAM是一种内存高效的优化器，通过块级梯度均值和残差校正压缩优化器状态，减少50%训练内存和90%优化器状态内存开销，同时保持与Adam相当的收敛性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型训练面临严重的内存瓶颈，特别是使用Adam等内存密集型优化器时。现有内存高效方法存在计算开销大、需要额外内存或性能下降等问题。

Method: 提出FOAM方法：1) 通过计算块级梯度均值压缩优化器状态；2) 引入残差校正机制恢复丢失的信息；3) 与现有内存高效优化器兼容。

Result: 理论上在非凸优化设置下达到与标准Adam相当的收敛率；实证上减少约50%总训练内存，消除高达90%优化器状态内存开销，并加速收敛。

Conclusion: FOAM是一种高效的内存优化方法，在保持性能的同时显著减少内存需求，兼容现有技术，在性能和吞吐量上匹配或超越现有基准。

Abstract: Large language models (LLMs) have demonstrated remarkable performance due to their large parameter counts and extensive training data. However, their scale leads to significant memory bottlenecks during training, especially when using memory-intensive optimizers like Adam. Existing memory-efficient approaches often rely on techniques such as singular value decomposition (SVD), projections, or weight freezing, which can introduce substantial computational overhead, require additional memory for projections, or degrade model performance. In this paper, we propose Folded Optimizer with Approximate Moment (FOAM), a method that compresses optimizer states by computing block-wise gradient means and incorporates a residual correction to recover lost information. Theoretically, FOAM achieves convergence rates equivalent to vanilla Adam under standard non-convex optimization settings. Empirically, FOAM reduces total training memory by approximately 50\%, eliminates up to 90\% of optimizer state memory overhead, and accelerates convergence. Furthermore, FOAM is compatible with other memory-efficient optimizers, delivering performance and throughput that match or surpass both full-rank and existing memory-efficient baselines.

</details>


### [316] [PlantBiMoE: A Bidirectional Foundation Model with SparseMoE for Plant Genomes](https://arxiv.org/abs/2512.07113)
*Kepeng Lin,Qizhe Zhang,Rui Wang,Xuehai Hu,Wei Xu*

Main category: cs.LG

TL;DR: PlantBiMoE：一种轻量级且表达力强的植物基因组语言模型，结合双向Mamba和稀疏专家混合框架，在植物基因组任务中表现优异


<details>
  <summary>Details</summary>
Motivation: 现有植物基因组模型如AgroNT和PDLLMs存在参数过大或无法有效建模DNA双链双向性的问题，需要开发更高效且能捕捉双向结构依赖的模型

Method: 提出PlantBiMoE模型，集成双向Mamba以捕捉DNA正反链的结构依赖，采用稀疏专家混合(SparseMoE)框架减少激活参数数量，提升计算效率

Result: 在MPGB基准测试（31个数据集，11个任务）中，PlantBiMoE在20个数据集上取得最佳性能，平均表现优于现有模型

Conclusion: PlantBiMoE能有效表示植物基因组序列，为植物基因组学、基因编辑和合成生物学提供强大的计算工具，代码已开源

Abstract: Understanding the underlying linguistic rules of plant genomes remains a fundamental challenge in computational biology. Recent advances including AgroNT and PDLLMs have made notable progress although, they suffer from excessive parameter size and limited ability to model the bidirectional nature of DNA strands respectively. To address these limitations, we propose PlantBiMoE, a lightweight and expressive plant genome language model that integrates bidirectional Mamba and a Sparse Mixture-of-Experts (SparseMoE) framework. The bidirectional Mamba enables the model to effectively capture structural dependencies across both the forward and reverse DNA strands, while SparseMoE significantly reduces the number of active parameters, improving computational efficiency without sacrificing modeling capacity. We evaluated and tested our model on the Modified Plants Genome Benchmark (MPGB), an enhanced genomic benchmark, which consolidates 31 datasets across 11 representative tasks, with input sequence lengths ranging from 50 to 6,000 bp. Experimental results demonstrate that PlantBiMoE achieves the best performance on 20 out of 31 datasets and the average best when comparing with existing models. In summary, all above results demonstrate that our model can effectively represent plant genomic sequences, serving as a robust computational tool for diverse genomic tasks, while making substantive contributions to plant genomics, gene editing, and synthetic biology. The code is available at: https://github.com/HUST-Keep-Lin/PlantBiMoE

</details>


### [317] [Winning the Lottery by Preserving Network Training Dynamics with Concrete Ticket Search](https://arxiv.org/abs/2512.07142)
*Tanay Arora,Christof Teuscher*

Main category: cs.LG

TL;DR: CTS算法通过整体组合优化方法，在神经网络初始化阶段高效寻找高性能稀疏子网络，性能优于现有彩票票证重绕和基于显著性的方法。


<details>
  <summary>Details</summary>
Motivation: 现有彩票票证发现方法存在计算成本高（如LTR）或准确率-稀疏度权衡差（如PaI）的问题，PaI方法因依赖忽略权重间依赖关系的一阶显著性指标而表现不佳。

Method: 提出Concrete Ticket Search (CTS)算法，将子网络发现建模为整体组合优化问题，使用Concrete松弛离散搜索空间和GRADBALANCE梯度平衡方案控制稀疏度，并采用基于知识蒸馏的修剪目标（特别是最小化稀疏与密集网络输出的反向KL散度CTS-KL）。

Result: 在图像分类任务中，CTS生成的子网络能通过基本合理性检查，达到与LTR相当或更好的准确率，但计算时间大幅减少（如ResNet-20在CIFAR10上达到99.3%稀疏度时，CTS用时7.9分钟准确率74.0%，LTR用时95.2分钟准确率68.3%）。

Conclusion: CTS通过整体优化方法有效解决了彩票票证发现中的计算效率和性能权衡问题，特别是在高稀疏度区域表现优异，为神经网络稀疏化提供了高效实用的解决方案。

Abstract: The Lottery Ticket Hypothesis asserts the existence of highly sparse, trainable subnetworks ('winning tickets') within dense, randomly initialized neural networks. However, state-of-the-art methods of drawing these tickets, like Lottery Ticket Rewinding (LTR), are computationally prohibitive, while more efficient saliency-based Pruning-at-Initialization (PaI) techniques suffer from a significant accuracy-sparsity trade-off and fail basic sanity checks. In this work, we argue that PaI's reliance on first-order saliency metrics, which ignore inter-weight dependencies, contributes substantially to this performance gap, especially in the sparse regime. To address this, we introduce Concrete Ticket Search (CTS), an algorithm that frames subnetwork discovery as a holistic combinatorial optimization problem. By leveraging a Concrete relaxation of the discrete search space and a novel gradient balancing scheme (GRADBALANCE) to control sparsity, CTS efficiently identifies high-performing subnetworks near initialization without requiring sensitive hyperparameter tuning. Motivated by recent works on lottery ticket training dynamics, we further propose a knowledge distillation-inspired family of pruning objectives, finding that minimizing the reverse Kullback-Leibler divergence between sparse and dense network outputs (CTS-KL) is particularly effective. Experiments on varying image classification tasks show that CTS produces subnetworks that robustly pass sanity checks and achieve accuracy comparable to or exceeding LTR, while requiring only a small fraction of the computation. For example, on ResNet-20 on CIFAR10, it reaches 99.3% sparsity with 74.0% accuracy in 7.9 minutes, while LTR attains the same sparsity with 68.3% accuracy in 95.2 minutes. CTS's subnetworks outperform saliency-based methods across all sparsities, but its advantage over LTR is most pronounced in the highly sparse regime.

</details>


### [318] [FlowLPS: Langevin-Proximal Sampling for Flow-based Inverse Problem Solvers](https://arxiv.org/abs/2512.07150)
*Jonghyun Park,Jong Chul Ye*

Main category: cs.LG

TL;DR: FlowLPS：一种基于预训练流模型解决逆问题的训练免费框架，通过Langevin近端采样策略在FFHQ和DIV2K数据集上实现重建保真度和感知质量的平衡


<details>
  <summary>Details</summary>
Motivation: 现有训练免费方法在应用于流模型时存在两个主要问题：1）难以收敛到后验模式；2）在潜在空间中产生流形偏差。需要一种能同时解决这两个问题的新方法。

Method: FlowLPS框架结合了Langevin动力学和近端优化：Langevin动力学用于流形一致的探索，近端优化用于精确的模式搜索，形成Langevin近端采样策略。

Result: 在FFHQ和DIV2K数据集上的多个逆任务中，FlowLPS在重建保真度和感知质量之间取得了优越的平衡，超越了现有最先进的逆问题求解器。

Conclusion: FlowLPS通过Langevin近端采样策略有效解决了流模型在逆问题中的收敛和流形偏差问题，为训练免费的逆问题求解提供了新框架。

Abstract: Deep generative models have become powerful priors for solving inverse problems, and various training-free methods have been developed. However, when applied to latent flow models, existing methods often fail to converge to the posterior mode or suffer from manifold deviation within latent spaces. To mitigate this, here we introduce a novel training-free framework, FlowLPS, that solves inverse problems with pretrained flow models via a Langevin Proximal Sampling (LPS) strategy. Our method integrates Langevin dynamics for manifold-consistent exploration with proximal optimization for precise mode seeking, achieving a superior balance between reconstruction fidelity and perceptual quality across multiple inverse tasks on FFHQ and DIV2K, outperforming state of the art inverse solvers.

</details>


### [319] [Improving the Throughput of Diffusion-based Large Language Models via a Training-Free Confidence-Aware Calibration](https://arxiv.org/abs/2512.07173)
*Jucheng Shen,Gaurav Sarkar,Yeonju Ro,Sharath Nittur Sridhar,Zhangyang Wang,Aditya Akella,Souvik Kundu*

Main category: cs.LG

TL;DR: CadLLM是一种无需训练的推理加速方法，通过动态调整生成块大小、步长和阈值，以及动态词汇子集采样，显著提升扩散式大语言模型的推理吞吐量。


<details>
  <summary>Details</summary>
Motivation: 扩散式大语言模型(dLLMs)的推理过程通常较慢，现有方法在保持准确性的同时提升吞吐量方面存在局限。作者观察到不同块和步骤中token unmasking置信度的动态特性，这为自适应加速提供了机会。

Method: 1. 分析token unmasking置信度在不同块和步骤中的动态特性；2. 基于平均置信度动态控制生成块大小、步长和阈值；3. 通过动态利用词汇子集来减少softmax计算开销；4. 设计为即插即用、模型无关的方法，兼容基于KV缓存的dLLMs。

Result: 在四个流行任务上的实验表明，CadLLM相比最先进的基线方法，吞吐量提升最高达2.28倍，同时保持竞争力的准确性。

Conclusion: CadLLM提供了一种有效的训练免费推理加速方案，通过自适应控制机制和动态词汇采样，在保持模型性能的同时显著提升扩散式大语言模型的推理效率。

Abstract: We present CadLLM, a training-free method to accelerate the inference throughput of diffusion-based LLMs (dLLMs). We first investigate the dynamic nature of token unmasking confidence across blocks and steps. Based on this observation, we present a lightweight adaptive approach that controls the generation block size, step size, and threshold based on the average confidence of unmasked tokens. We further reduce softmax overhead by dynamically leveraging a subset of the vocabulary to regulate sampling breadth. CadLLM is a plug-and-play, model-agnostic method compatible with KV-cache-based dLLMs. Extensive experiments on four popular tasks demonstrate that CadLLM yields up to 2.28x throughput improvement over the state-of-the-art baseline with competitive accuracy.

</details>


### [320] [SPACE: Noise Contrastive Estimation Stabilizes Self-Play Fine-Tuning for Large Language Models](https://arxiv.org/abs/2512.07175)
*Yibo Wang,Qing-Guo Chen,Zhao Xu,Weihua Luo,Kaifu Zhang,Lijun Zhang*

Main category: cs.LG

TL;DR: SPACE提出了一种基于噪声对比估计的自博弈微调方法，通过将合成样本作为辅助成分，以二分类方式区分真实与合成数据，解决了现有基于差距的方法因忽略绝对奖励值而导致的训练不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 现有自博弈微调方法主要关注真实数据和合成数据之间的奖励差距，忽略了它们的绝对奖励值。通过理论分析发现，这种基于差距的方法由于目标函数可能退化，会导致训练过程不稳定。

Method: 提出SPACE方法，利用噪声对比估计来捕捉真实世界数据分布。该方法将合成样本作为辅助成分，以二分类方式区分真实样本和合成样本，从而独立优化每种类型数据的绝对奖励值。

Result: 理论上证明SPACE的最优解与真实世界数据的基础分布一致，并保证可证明的稳定收敛。实证结果表明，SPACE在各种任务上显著提升LLM性能，优于使用更多真实样本的监督微调，且相比基于差距的自博弈方法表现出显著优越性和稳定演化。

Conclusion: SPACE通过噪声对比估计解决了自博弈微调中的训练不稳定问题，提供了一种更稳定有效的LLM微调方法，在有限真实数据条件下实现了更好的性能。

Abstract: Self-play fine-tuning has demonstrated promising abilities in adapting large language models (LLMs) to downstream tasks with limited real-world data. The basic principle is to iteratively refine the model with real samples and synthetic ones generated from itself. However, the existing methods primarily focus on the relative gaps between the rewards for two types of data, neglecting their absolute values. Through theoretical analysis, we identify that the gap-based methods suffer from unstable evolution, due to the potentially degenerated objectives. To address this limitation, we introduce a novel self-play fine-tuning method, namely Self-PlAy via Noise Contrastive Estimation (SPACE), which leverages noise contrastive estimation to capture the real-world data distribution. Specifically, SPACE treats synthetic samples as auxiliary components, and discriminates them from the real ones in a binary classification manner. As a result, SPACE independently optimizes the absolute reward values for each type of data, ensuring a consistently meaningful objective and thereby avoiding the instability issue. Theoretically, we show that the optimal solution of the objective in SPACE aligns with the underlying distribution of real-world data, and SPACE guarantees a provably stable convergence to the optimal distribution. Empirically, we show that SPACE significantly improves the performance of LLMs over various tasks, and outperforms supervised fine-tuning that employs much more real-world samples. Compared to gap-based self-play fine-tuning methods, SPACE exhibits remarkable superiority and stable evolution.

</details>


### [321] [Geometric Prior-Guided Federated Prompt Calibration](https://arxiv.org/abs/2512.07208)
*Fei Luo,Ziwei Zhao,Mingxuan Wang,Duoyang Li,Zhe Qian,Jiayi Tuo,Chenyue Zhou,Yanbiao Ma*

Main category: cs.LG

TL;DR: GGTPC是一个联邦提示学习框架，通过全局几何先验校正本地训练偏差，有效缓解数据异构性问题


<details>
  <summary>Details</summary>
Motivation: 联邦提示学习面临数据异构性挑战，导致本地训练的提示产生偏差。现有方法聚焦于聚合或正则化，未能解决本地训练偏差的根本原因

Method: 提出几何引导文本提示校准框架，在服务器端以隐私保护方式重构全局数据分布的几何先验，客户端使用几何先验校准层将本地特征分布与全局先验对齐

Result: 在标签倾斜的CIFAR-100数据集上超越SOTA 2.15%，在极端倾斜情况下提升9.17%；在领域倾斜的Office-Home数据集上作为即插即用模块提升FedAvg性能4.60%

Conclusion: GGTPC通过校正本地训练偏差有效缓解数据异构性问题，可作为增强各种联邦学习算法的通用模块

Abstract: Federated Prompt Learning (FPL) offers a parameter-efficient solution for collaboratively training large models, but its performance is severely hindered by data heterogeneity, which causes locally trained prompts to become biased. Existing methods, focusing on aggregation or regularization, fail to address this root cause of local training bias. To this end, we propose Geometry-Guided Text Prompt Calibration (GGTPC), a novel framework that directly corrects this bias by providing clients with a global geometric prior. This prior, representing the shape of the global data distribution derived from the covariance matrix, is reconstructed on the server in a privacy-preserving manner. Clients then use a novel Geometry-Prior Calibration Layer (GPCL) to align their local feature distributions with this global prior during training. Extensive experiments show GGTPC's effectiveness. On the label-skewed CIFAR-100 dataset ($β$=0.1), it outperforms the state-of-the-art by 2.15\%. Under extreme skew ($β$=0.01), it improves upon the baseline by 9.17\%. Furthermore, as a plug-and-play module on the domain-skewed Office-Home dataset, it boosts FedAvg's performance by 4.60\%. These results demonstrate that GGTPC effectively mitigates data heterogeneity by correcting the fundamental local training bias, serving as a versatile module to enhance various FL algorithms.

</details>


### [322] [UniDiff: A Unified Diffusion Framework for Multimodal Time Series Forecasting](https://arxiv.org/abs/2512.07184)
*Da Zhang,Bingyu Li,Zhuyuan Zhao,Junyu Gao,Feiping Nie,Xuelong Li*

Main category: cs.LG

TL;DR: UniDiff：用于多模态时间序列预测的统一扩散框架，通过并行融合模块整合文本和时间戳信息，实现跨模态信息的高效利用


<details>
  <summary>Details</summary>
Motivation: 现实应用中多模态数据（如文本和时间戳）日益增多，但现有扩散模型在时间序列预测中主要局限于单模态数值序列建模，忽略了复杂异构数据中丰富的跨模态信号

Method: 1）将时间序列分块token化，通过轻量MLP映射到嵌入空间；2）核心是统一并行融合模块，使用单一交叉注意力机制自适应加权整合时间戳的结构信息和文本的语义上下文；3）提出新颖的用于多源条件化的无分类器引导机制，可在推理时解耦控制文本和时间信息的引导强度

Result: 在八个领域的真实世界基准数据集上进行广泛实验，证明UniDiff模型达到了最先进的性能

Conclusion: UniDiff成功解决了多模态时间序列预测的挑战，通过统一扩散框架有效整合异构信息，显著提升了模型鲁棒性和预测准确性

Abstract: As multimodal data proliferates across diverse real-world applications, leveraging heterogeneous information such as texts and timestamps for accurate time series forecasting (TSF) has become a critical challenge. While diffusion models demonstrate exceptional performance in generation tasks, their application to TSF remains largely confined to modeling single-modality numerical sequences, overlooking the abundant cross-modal signals inherent in complex heterogeneous data. To address this gap, we propose UniDiff, a unified diffusion framework for multimodal time series forecasting. To process the numerical sequence, our framework first tokenizes the time series into patches, preserving local temporal dynamics by mapping each patch to an embedding space via a lightweight MLP. At its core lies a unified and parallel fusion module, where a single cross-attention mechanism adaptively weighs and integrates structural information from timestamps and semantic context from texts in one step, enabling a flexible and efficient interplay between modalities. Furthermore, we introduce a novel classifier-free guidance mechanism designed for multi-source conditioning, allowing for decoupled control over the guidance strength of textual and temporal information during inference, which significantly enhances model robustness. Extensive experiments on real-world benchmark datasets across eight domains demonstrate that the proposed UniDiff model achieves state-of-the-art performance.

</details>


### [323] [Less is More: Non-uniform Road Segments are Efficient for Bus Arrival Prediction](https://arxiv.org/abs/2512.07200)
*Zhen Huang,Jiaxin Deng,Jiayu Xu,Junbiao Pang,Haitao Yu*

Main category: cs.LG

TL;DR: 提出基于强化学习的非均匀道路分段方法，用于公交车到达时间预测，通过两阶段解耦策略实现高效且自适应的路段选择，优于传统均匀分段方法。


<details>
  <summary>Details</summary>
Motivation: 传统公交车到达时间预测采用均匀道路分段策略，忽略了道路条件、交叉口、兴趣点等物理约束的差异性，限制了预测效率。需要一种能自适应学习非均匀路段的方法来提升预测性能。

Method: 提出基于强化学习的两阶段方法：1）使用RL框架根据影响分数提取非均匀道路分段；2）对选定路段应用线性预测模型进行预测。该方法在保持计算效率的同时实现最优路段选择。

Result: 实验结果表明，该方法在大规模基准测试中不仅提高了效率，还提升了学习性能。线性方法甚至能比更复杂的方法获得更好的性能。

Conclusion: 基于强化学习的非均匀道路分段方法为公交车到达时间预测提供了高效且有效的解决方案，优于传统均匀分段策略，展示了"少即是多"的设计理念。

Abstract: In bus arrival time prediction, the process of organizing road infrastructure network data into homogeneous entities is known as segmentation. Segmenting a road network is widely recognized as the first and most critical step in developing an arrival time prediction system, particularly for auto-regressive-based approaches. Traditional methods typically employ a uniform segmentation strategy, which fails to account for varying physical constraints along roads, such as road conditions, intersections, and points of interest, thereby limiting prediction efficiency. In this paper, we propose a Reinforcement Learning (RL)-based approach to efficiently and adaptively learn non-uniform road segments for arrival time prediction. Our method decouples the prediction process into two stages: 1) Non-uniform road segments are extracted based on their impact scores using the proposed RL framework; and 2) A linear prediction model is applied to the selected segments to make predictions. This method ensures optimal segment selection while maintaining computational efficiency, offering a significant improvement over traditional uniform approaches. Furthermore, our experimental results suggest that the linear approach can even achieve better performance than more complex methods. Extensive experiments demonstrate the superiority of the proposed method, which not only enhances efficiency but also improves learning performance on large-scale benchmarks. The dataset and the code are publicly accessible at: https://github.com/pangjunbiao/Less-is-More.

</details>


### [324] [IFFair: Influence Function-driven Sample Reweighting for Fair Classification](https://arxiv.org/abs/2512.07249)
*Jingran Yang,Min Zhang,Lingfeng Zhang,Zhaohui Wang,Yonggang Zhang*

Main category: cs.LG

TL;DR: 提出基于影响函数的预处理公平性方法IFFair，通过动态调整样本权重缓解算法偏见，不改变网络结构或数据特征。


<details>
  <summary>Details</summary>
Motivation: 机器学习辅助决策时，基于数据的算法会学习甚至放大样本中的偏见，导致对弱势群体的歧视性决策，损害社会福祉并阻碍应用发展。

Method: 基于影响函数，仅使用训练样本对不同群体的影响差异作为指导，在训练过程中动态调整样本权重，不修改网络结构、数据特征和决策边界。

Result: 在多个真实数据集和指标上的实验表明，IFFair能缓解分类设置中的多种公平性指标偏见（人口统计均等、均衡几率、机会均等、错误率均等），且相比先前预处理方法在效用与公平性权衡方面表现更好。

Conclusion: IFFair是一种有效的预处理公平性优化方法，通过影响函数指导的样本权重调整，在不改变模型结构的情况下实现多指标公平性提升。

Abstract: Because machine learning has significantly improved efficiency and convenience in the society, it's increasingly used to assist or replace human decision-making. However, the data-based pattern makes related algorithms learn and even exacerbate potential bias in samples, resulting in discriminatory decisions against certain unprivileged groups, depriving them of the rights to equal treatment, thus damaging the social well-being and hindering the development of related applications. Therefore, we propose a pre-processing method IFFair based on the influence function. Compared with other fairness optimization approaches, IFFair only uses the influence disparity of training samples on different groups as a guidance to dynamically adjust the sample weights during training without modifying the network structure, data features and decision boundaries. To evaluate the validity of IFFair, we conduct experiments on multiple real-world datasets and metrics. The experimental results show that our approach mitigates bias of multiple accepted metrics in the classification setting, including demographic parity, equalized odds, equality of opportunity and error rate parity without conflicts. It also demonstrates that IFFair achieves better trade-off between multiple utility and fairness metrics compared with previous pre-processing methods.

</details>


### [325] [SIT-Graph: State Integrated Tool Graph for Multi-Turn Agents](https://arxiv.org/abs/2512.07287)
*Sijia Li,Yuchen Huang,Zifan Liu,Zijian Li,Jingjing fu,Lei Song,Jiang Bian,Jun Zhang,Rui Wang*

Main category: cs.LG

TL;DR: SIT-Graph：一种通过利用部分重叠经验增强多轮工具使用的状态集成工具图方法


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体在多轮工具使用场景中存在挑战，要么将整个轨迹或预定义子任务视为不可分割单元，要么仅利用工具间依赖关系，难以适应状态和信息随轮次演变的动态环境

Method: 提出状态集成工具图(SIT-Graph)，从历史轨迹中捕获紧凑状态表示（类似情景记忆片段）和工具间依赖关系（类似程序性记忆例程）。首先从累积工具使用序列构建工具图，然后为每条边增强包含对话和工具历史的紧凑状态摘要

Result: 在多个有状态多轮工具使用基准测试中，SIT-Graph始终优于基于记忆和基于图的基线方法，提供更稳健的工具选择和更有效的经验迁移

Conclusion: SIT-Graph通过结合情景记忆和程序性记忆的灵感，实现了人类般的决策平衡，在需要回忆上下文时检索状态摘要，在常规步骤时遵循高置信度工具依赖，显著提升了多轮工具使用性能

Abstract: Despite impressive advances in agent systems, multi-turn tool-use scenarios remain challenging. It is mainly because intent is clarified progressively and the environment evolves with each tool call. While reusing past experience is natural, current LLM agents either treat entire trajectories or pre-defined subtasks as indivisible units, or solely exploit tool-to-tool dependencies, hindering adaptation as states and information evolve across turns. In this paper, we propose a State Integrated Tool Graph (SIT-Graph), which enhances multi-turn tool use by exploiting partially overlapping experience. Inspired by human decision-making that integrates episodic and procedural memory, SIT-Graph captures both compact state representations (episodic-like fragments) and tool-to-tool dependencies (procedural-like routines) from historical trajectories. Specifically, we first build a tool graph from accumulated tool-use sequences, and then augment each edge with a compact state summary of the dialog and tool history that may shape the next action. At inference time, SIT-Graph enables a human-like balance between episodic recall and procedural execution: when the next decision requires recalling prior context, the agent retrieves the state summaries stored on relevant edges and uses them to guide its next action; when the step is routine, it follows high-confidence tool dependencies without explicit recall. Experiments across multiple stateful multi-turn tool-use benchmarks show that SIT-Graph consistently outperforms strong memory- and graph-based baselines, delivering more robust tool selection and more effective experience transfer.

</details>


### [326] [PINE: Pipeline for Important Node Exploration in Attributed Networks](https://arxiv.org/abs/2512.07244)
*Elizaveta Kovtun,Maksim Makarenko,Natalia Semenova,Alexey Zaytsev,Semen Budennyy*

Main category: cs.LG

TL;DR: 提出PINE框架，通过注意力机制结合节点语义特征和网络结构，实现无监督的属性感知重要节点识别。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如PageRank）仅考虑网络结构而忽略节点属性，现有神经网络方法需要监督学习，缺乏同时满足无监督和属性感知的重要节点识别方法。

Method: 提出PINE框架，核心是基于注意力的图模型，将节点语义特征融入图结构属性的学习过程，利用注意力分布计算节点重要性得分。

Result: 在多种同质和异质属性网络上展示了优越性能，作为工业实现系统，成功应用于大规模企业图中关键实体的无监督识别。

Conclusion: PINE填补了无监督且属性感知的重要节点识别方法的空白，通过结合语义特征和结构特性，有效提升了系统监控和管理能力。

Abstract: A graph with semantically attributed nodes are a common data structure in a wide range of domains. It could be interlinked web data or citation networks of scientific publications. The essential problem for such a data type is to determine nodes that carry greater importance than all the others, a task that markedly enhances system monitoring and management. Traditional methods to identify important nodes in networks introduce centrality measures, such as node degree or more complex PageRank. However, they consider only the network structure, neglecting the rich node attributes. Recent methods adopt neural networks capable of handling node features, but they require supervision. This work addresses the identified gap--the absence of approaches that are both unsupervised and attribute-aware--by introducing a Pipeline for Important Node Exploration (PINE). At the core of the proposed framework is an attention-based graph model that incorporates node semantic features in the learning process of identifying the structural graph properties. The PINE's node importance scores leverage the obtained attention distribution. We demonstrate the superior performance of the proposed PINE method on various homogeneous and heterogeneous attributed networks. As an industry-implemented system, PINE tackles the real-world challenge of unsupervised identification of key entities within large-scale enterprise graphs.

</details>


### [327] [Local-Curvature-Aware Knowledge Graph Embedding: An Extended Ricci Flow Approach](https://arxiv.org/abs/2512.07332)
*Zhengquan Luo,Guy Tadmor,Or Amar,David Zeevi,Zhiqiang Xu*

Main category: cs.LG

TL;DR: RicciKGE提出了一种动态几何适应的知识图谱嵌入方法，通过Ricci流让嵌入空间几何与实体嵌入协同演化，解决传统方法使用固定几何流形无法适应知识图谱局部曲率变化的问题。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱嵌入方法将实体放置在预定义的均匀流形上（欧几里得、球面、双曲或其乘积/多曲率变体），但真实知识图谱在不同局部区域表现出显著变化的曲率。固定的几何流形无法适应这种局部曲率变化，导致实体间距离失真，限制了嵌入的表达能力。

Method: 提出RicciKGE方法，将KGE损失梯度与局部曲率通过扩展的Ricci流耦合，使实体嵌入与底层流形几何动态协同演化。当耦合系数有界且适当选择时，理论上证明：1）所有边曲率指数衰减，流形趋向欧几里得平坦；2）KGE距离严格收敛到全局最优，几何平坦化与嵌入优化相互促进。

Result: 在链接预测和节点分类基准测试中，RicciKGE表现出改进的性能，证明了该方法在适应异构知识图谱结构方面的有效性。

Conclusion: RicciKGE通过动态几何适应机制，解决了传统固定几何流形方法无法适应知识图谱局部曲率变化的问题，实现了嵌入空间几何与实体嵌入的协同优化，提升了知识图谱嵌入的表达能力和性能。

Abstract: Knowledge graph embedding (KGE) relies on the geometry of the embedding space to encode semantic and structural relations. Existing methods place all entities on one homogeneous manifold, Euclidean, spherical, hyperbolic, or their product/multi-curvature variants, to model linear, symmetric, or hierarchical patterns. Yet a predefined, homogeneous manifold cannot accommodate the sharply varying curvature that real-world graphs exhibit across local regions. Since this geometry is imposed a priori, any mismatch with the knowledge graph's local curvatures will distort distances between entities and hurt the expressiveness of the resulting KGE. To rectify this, we propose RicciKGE to have the KGE loss gradient coupled with local curvatures in an extended Ricci flow such that entity embeddings co-evolve dynamically with the underlying manifold geometry towards mutual adaptation. Theoretically, when the coupling coefficient is bounded and properly selected, we rigorously prove that i) all the edge-wise curvatures decay exponentially, meaning that the manifold is driven toward the Euclidean flatness; and ii) the KGE distances strictly converge to a global optimum, which indicates that geometric flattening and embedding optimization are promoting each other. Experimental improvements on link prediction and node classification benchmarks demonstrate RicciKGE's effectiveness in adapting to heterogeneous knowledge graph structures.

</details>


### [328] [Asymptotic analysis of shallow and deep forgetting in replay with Neural Collapse](https://arxiv.org/abs/2512.07400)
*Giulia Lanzillotta,Damiano Meier,Thomas Hofmann*

Main category: cs.LG

TL;DR: 论文揭示了持续学习中深度特征空间遗忘与浅层分类器遗忘的差异，发现小缓冲区足以防止特征几何漂移，但需要大缓冲区来缓解分类器遗忘，并提出通过修正统计伪影实现最小回放下的鲁棒性能。


<details>
  <summary>Details</summary>
Motivation: 持续学习中存在一个持久悖论：神经网络即使输出预测失败，仍能保留过去任务的线性可分表示。本文旨在形式化这种深度特征空间遗忘与浅层分类器遗忘的区别，并解释经验回放中缓冲区大小对这两种遗忘的不同影响。

Method: 将神经崩溃框架扩展到顺序设置，分析深度遗忘作为向分布外子空间的几何漂移，证明任何非零回放比例都能保证线性可分性的保留。同时识别小缓冲区引起的"强崩溃"导致协方差秩不足和类均值膨胀，使分类器无法识别真实群体边界。

Result: 发现最小缓冲区能成功锚定特征几何并防止深度遗忘，但缓解浅层遗忘通常需要更大的缓冲区容量。揭示了经验回放中的关键不对称性，并通过统一持续学习与分布外检测，挑战了对大缓冲区的依赖。

Conclusion: 通过显式修正统计伪影，可以在最小回放下实现鲁棒性能，为持续学习提供了新的理论框架和实践方向，减少了对大缓冲区的依赖。

Abstract: A persistent paradox in continual learning (CL) is that neural networks often retain linearly separable representations of past tasks even when their output predictions fail. We formalize this distinction as the gap between deep feature-space and shallow classifier-level forgetting. We reveal a critical asymmetry in Experience Replay: while minimal buffers successfully anchor feature geometry and prevent deep forgetting, mitigating shallow forgetting typically requires substantially larger buffer capacities. To explain this, we extend the Neural Collapse framework to the sequential setting. We characterize deep forgetting as a geometric drift toward out-of-distribution subspaces and prove that any non-zero replay fraction asymptotically guarantees the retention of linear separability. Conversely, we identify that the "strong collapse" induced by small buffers leads to rank-deficient covariances and inflated class means, effectively blinding the classifier to true population boundaries. By unifying CL with out-of-distribution detection, our work challenges the prevailing reliance on large buffers, suggesting that explicitly correcting these statistical artifacts could unlock robust performance with minimal replay.

</details>


### [329] [MIDG: Mixture of Invariant Experts with knowledge injection for Domain Generalization in Multimodal Sentiment Analysis](https://arxiv.org/abs/2512.07430)
*Yangle Li,Danli Luo,Haifeng Hu*

Main category: cs.LG

TL;DR: 提出MIDG框架，通过混合不变专家模型提取跨模态的域不变特征，并使用跨模态适配器注入知识，提升多模态情感分析的域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态情感分析域泛化方法存在两个问题：1) 提取不变特征时忽视模态间的协同作用，无法准确捕捉多模态数据的丰富语义信息；2) 知识注入技术存在跨模态知识碎片化问题，忽略了超越单模态界限的特定表示。

Method: 提出MIDG框架：1) 混合不变专家模型，提取域不变特征并增强模态间协同关系学习；2) 跨模态适配器，通过跨模态知识注入增强多模态表示的语义丰富性。

Result: 在三个数据集上进行的广泛域实验表明，MIDG取得了优越的性能表现。

Conclusion: 提出的MIDG框架通过有效提取跨模态域不变特征和注入跨模态知识，显著提升了多模态情感分析的域泛化能力。

Abstract: Existing methods in domain generalization for Multimodal Sentiment Analysis (MSA) often overlook inter-modal synergies during invariant features extraction, which prevents the accurate capture of the rich semantic information within multimodal data. Additionally, while knowledge injection techniques have been explored in MSA, they often suffer from fragmented cross-modal knowledge, overlooking specific representations that exist beyond the confines of unimodal. To address these limitations, we propose a novel MSA framework designed for domain generalization. Firstly, the framework incorporates a Mixture of Invariant Experts model to extract domain-invariant features, thereby enhancing the model's capacity to learn synergistic relationships between modalities. Secondly, we design a Cross-Modal Adapter to augment the semantic richness of multimodal representations through cross-modal knowledge injection. Extensive domain experiments conducted on three datasets demonstrate that the proposed MIDG achieves superior performance.

</details>


### [330] [Towards a Relationship-Aware Transformer for Tabular Data](https://arxiv.org/abs/2512.07310)
*Andrei V. Konstantinov,Valerii A. Zuev,Lev V. Utkin*

Main category: cs.LG

TL;DR: 提出基于改进注意力机制的模型，用于处理表格数据中样本间外部依赖关系，在回归和因果效应估计任务中表现优于梯度提升决策树。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型难以处理表格数据中样本间的外部依赖关系（如图结构），而图神经网络仅考虑相邻节点，不适用于稀疏图。需要一种能有效利用样本间关系的方法，特别是在因果效应估计等任务中。

Method: 提出基于改进注意力机制的解决方案，通过在注意力矩阵中添加额外项来考虑数据点间的可能关系。开发了多个模型变体，并在合成和真实数据集上进行回归任务测试，以及在IHDP数据集上进行因果效应估计任务评估。

Result: 所提模型在回归任务中相互比较并与梯度提升决策树对比，在因果效应估计任务中在IHDP数据集上表现出色，证明了改进注意力机制能有效利用样本间关系。

Conclusion: 基于改进注意力机制的模型能够有效处理表格数据中样本间的外部依赖关系，在回归和因果效应估计任务中优于传统方法，为解决稀疏图关系建模问题提供了新思路。

Abstract: Deep learning models for tabular data typically do not allow for imposing a graph of external dependencies between samples, which can be useful for accounting for relatedness in tasks such as treatment effect estimation. Graph neural networks only consider adjacent nodes, making them difficult to apply to sparse graphs. This paper proposes several solutions based on a modified attention mechanism, which accounts for possible relationships between data points by adding a term to the attention matrix. Our models are compared with each other and the gradient boosting decision trees in a regression task on synthetic and real-world datasets, as well as in a treatment effect estimation task on the IHDP dataset.

</details>


### [331] [KAN-Dreamer: Benchmarking Kolmogorov-Arnold Networks as Function Approximators in World Models](https://arxiv.org/abs/2512.07437)
*Chenwei Shi,Xueyu Luan*

Main category: cs.LG

TL;DR: KAN-Dreamer将KAN和FastKAN架构集成到DreamerV3框架中，替换部分MLP和卷积组件，在DeepMind Control Suite上实现了与原始MLP架构相当的性能。


<details>
  <summary>Details</summary>
Motivation: 结合DreamerV3的卓越样本效率和KAN网络的参数效率与可解释性优势，同时通过FastKAN变体缓解KAN的计算开销问题。

Method: 将DreamerV3中的特定MLP和卷积组件替换为KAN和FastKAN层，实现完全向量化的JAX版本并简化网格管理，在视觉感知、潜在预测和行为学习三个子系统中进行集成。

Result: 在DeepMind Control Suite的walker_walk任务上，使用适配的FastKAN作为奖励和继续预测器的替代方案，在样本效率、训练速度和渐进性能方面与原始MLP架构保持相当水平。

Conclusion: KAN-Dreamer作为初步研究，证明了KAN架构可以成功集成到模型强化学习框架中，为未来基于KAN的世界模型开发奠定了基础。

Abstract: DreamerV3 is a state-of-the-art online model-based reinforcement learning (MBRL) algorithm known for remarkable sample efficiency. Concurrently, Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to Multi-Layer Perceptrons (MLPs), offering superior parameter efficiency and interpretability. To mitigate KANs' computational overhead, variants like FastKAN leverage Radial Basis Functions (RBFs) to accelerate inference. In this work, we investigate integrating KAN architectures into the DreamerV3 framework. We introduce KAN-Dreamer, replacing specific MLP and convolutional components of DreamerV3 with KAN and FastKAN layers. To ensure efficiency within the JAX-based World Model, we implement a tailored, fully vectorized version with simplified grid management. We structure our investigation into three subsystems: Visual Perception, Latent Prediction, and Behavior Learning. Empirical evaluations on the DeepMind Control Suite (walker_walk) analyze sample efficiency, training time, and asymptotic performance. Experimental results demonstrate that utilizing our adapted FastKAN as a drop-in replacement for the Reward and Continue predictors yields performance on par with the original MLP-based architecture, maintaining parity in both sample efficiency and training speed. This report serves as a preliminary study for future developments in KAN-based world models.

</details>


### [332] [Learning-Augmented Ski Rental with Discrete Distributions: A Bayesian Approach](https://arxiv.org/abs/2512.07313)
*Bosun Kang,Hyejun Park,Chenglin Fan*

Main category: cs.LG

TL;DR: 提出基于贝叶斯决策和机器学习预测的滑雪租赁问题新框架，统一传统最坏情况算法与学习增强方法，通过精确后验分布实现不确定性量化和专家先验整合。


<details>
  <summary>Details</summary>
Motivation: 传统滑雪租赁问题算法只关注最坏情况成本，而近期学习增强方法虽然利用预测但缺乏理论框架。本文旨在统一这两种视角，为带有不完美预测的在线决策问题提供贝叶斯推理框架。

Method: 提出离散贝叶斯框架，维护时间范围的精确后验分布，支持不确定性量化和专家先验整合。算法实现先验依赖的竞争性保证，平滑插值于最坏情况和完全知情设置之间。

Result: 实验评估显示在多样化场景中具有优越的实证性能：在准确先验下达到接近最优结果，同时保持鲁棒的最坏情况保证。框架可自然扩展到多预测、非均匀先验和上下文信息。

Conclusion: 贝叶斯推理为带有不完美预测的在线决策问题提供了实用优势，统一了传统最坏情况分析和学习增强方法，实现了性能与鲁棒性的平衡。

Abstract: We revisit the classic ski rental problem through the lens of Bayesian decision-making and machine-learned predictions. While traditional algorithms minimize worst-case cost without assumptions, and recent learning-augmented approaches leverage noisy forecasts with robustness guarantees, our work unifies these perspectives. We propose a discrete Bayesian framework that maintains exact posterior distributions over the time horizon, enabling principled uncertainty quantification and seamless incorporation of expert priors. Our algorithm achieves prior-dependent competitive guarantees and gracefully interpolates between worst-case and fully-informed settings. Our extensive experimental evaluation demonstrates superior empirical performance across diverse scenarios, achieving near-optimal results under accurate priors while maintaining robust worst-case guarantees. This framework naturally extends to incorporate multiple predictions, non-uniform priors, and contextual information, highlighting the practical advantages of Bayesian reasoning in online decision problems with imperfect predictions.

</details>


### [333] [Forget and Explain: Transparent Verification of GNN Unlearning](https://arxiv.org/abs/2512.07450)
*Imran Ahsan,Hyunwook Yu,Jinsung Kim,Mucheol Kim*

Main category: cs.LG

TL;DR: 提出了一种基于可解释性的GNN遗忘验证器，通过对比删除前后的模型快照，使用归因偏移和局部结构变化作为透明证据来验证遗忘效果。


<details>
  <summary>Details</summary>
Motivation: 现有GNN遗忘方法主要关注效率和可扩展性，但缺乏透明度，难以验证信息是否真正被遗忘，特别是在GDPR等隐私法规要求下。

Method: 提出可解释性驱动的验证器，对比删除前后的模型快照，使用五种可解释性指标：残差归因、热图偏移、可解释性分数偏差、图编辑距离和诊断图规则偏移。

Result: 在两种骨干网络（GCN、GAT）和四种遗忘策略（Retrain、GraphEditor、GNNDelete、IDEA）上评估，结果显示Retrain和GNNDelete实现近乎完全遗忘，GraphEditor提供部分擦除，IDEA留下残差信号。

Conclusion: 解释性差异提供了主要的人类可读遗忘证据，同时成员推断ROC-AUC作为补充的图级隐私信号，为GNN遗忘提供了透明验证方法。

Abstract: Graph neural networks (GNNs) are increasingly used to model complex patterns in graph-structured data. However, enabling them to "forget" designated information remains challenging, especially under privacy regulations such as the GDPR. Existing unlearning methods largely optimize for efficiency and scalability, yet they offer little transparency, and the black-box nature of GNNs makes it difficult to verify whether forgetting has truly occurred. We propose an explainability-driven verifier for GNN unlearning that snapshots the model before and after deletion, using attribution shifts and localized structural changes (for example, graph edit distance) as transparent evidence. The verifier uses five explainability metrics: residual attribution, heatmap shift, explainability score deviation, graph edit distance, and a diagnostic graph rule shift. We evaluate two backbones (GCN, GAT) and four unlearning strategies (Retrain, GraphEditor, GNNDelete, IDEA) across five benchmarks (Cora, Citeseer, Pubmed, Coauthor-CS, Coauthor-Physics). Results show that Retrain and GNNDelete achieve near-complete forgetting, GraphEditor provides partial erasure, and IDEA leaves residual signals. These explanation deltas provide the primary, human-readable evidence of forgetting; we also report membership-inference ROC-AUC as a complementary, graph-wide privacy signal.

</details>


### [334] [Exploring possible vector systems for faster training of neural networks with preconfigured latent spaces](https://arxiv.org/abs/2512.07509)
*Nikita Gabdullin*

Main category: cs.LG

TL;DR: 论文提出使用预定义向量系统（如An根系统）作为潜在空间配置目标，无需分类层即可训练分类器，特别适用于大规模类别数据集，并能加速训练和减少嵌入维度。


<details>
  <summary>Details</summary>
Motivation: 神经网络性能与潜在空间嵌入分布特性密切相关。现有方法使用An根系统向量作为潜在空间配置目标，但需要更通用的向量系统框架来支持不同应用场景，特别是处理超大规模类别数据集时的训练效率和存储需求。

Method: 提出通用向量系统构建方法，用于配置编码器和视觉变换器的潜在空间。使用预定义向量系统作为训练目标，无需传统分类层。通过最小化特定类别数所需的潜在空间维度来优化收敛速度和存储效率。

Result: 在ImageNet-1K和50k-600k类别数据集上显著加速了潜在空间配置训练。使用最小潜在空间维度能实现更快收敛，同时为减少存储神经网络嵌入的向量数据库大小提供潜在优势。

Conclusion: 预定义向量系统为神经网络潜在空间配置提供了有效的通用框架，特别适用于大规模类别分类任务。最小化潜在空间维度不仅能加速训练收敛，还能减少嵌入存储需求，具有实际应用价值。

Abstract: The overall neural network (NN) performance is closely related to the properties of its embedding distribution in latent space (LS). It has recently been shown that predefined vector systems, specifically An root system vectors, can be used as targets for latent space configurations (LSC) to ensure the desired LS structure. One of the main LSC advantage is the possibility of training classifier NNs without classification layers, which facilitates training NNs on datasets with extremely large numbers of classes. This paper provides a more general overview of possible vector systems for NN training along with their properties and methods for vector system construction. These systems are used to configure LS of encoders and visual transformers to significantly speed up ImageNet-1K and 50k-600k classes LSC training. It is also shown that using the minimum number of LS dimensions for a specific number of classes results in faster convergence. The latter has potential advantages for reducing the size of vector databases used to store NN embeddings.

</details>


### [335] [Towards Reliable Test-Time Adaptation: Style Invariance as a Correctness Likelihood](https://arxiv.org/abs/2512.07390)
*Gilhyun Nam,Taewon Kim,Joonhyun Jeong,Eunho Yang*

Main category: cs.LG

TL;DR: SICL是一个基于风格不变性的测试时自适应不确定性校准框架，通过测量预测在不同风格变换下的一致性来估计实例级正确性概率，无需反向传播即可实现即插即用的校准。


<details>
  <summary>Details</summary>
Motivation: 测试时自适应（TTA）方法在实际部署中常常导致预测不确定性校准不佳，这在自动驾驶、金融和医疗等高风险领域是严重问题。现有校准方法通常假设固定模型或静态分布，在动态测试条件下性能下降。

Method: SICL框架利用风格不变性进行鲁棒的不确定性估计，通过测量预测在不同风格变换版本下的一致性来估计实例级正确性概率。该方法仅需模型前向传播，是一个即插即用、无需反向传播的校准模块，可与任何TTA方法兼容。

Result: 在四个基线方法、五种TTA方法和两种现实场景下的综合评估显示，SICL相比传统校准方法平均减少了13个百分点的校准误差。

Conclusion: SICL为测试时自适应提供了一种有效的、无需反向传播的不确定性校准解决方案，通过利用风格不变性原理，显著提升了在动态测试条件下的预测可靠性。

Abstract: Test-time adaptation (TTA) enables efficient adaptation of deployed models, yet it often leads to poorly calibrated predictive uncertainty - a critical issue in high-stakes domains such as autonomous driving, finance, and healthcare. Existing calibration methods typically assume fixed models or static distributions, resulting in degraded performance under real-world, dynamic test conditions. To address these challenges, we introduce Style Invariance as a Correctness Likelihood (SICL), a framework that leverages style-invariance for robust uncertainty estimation. SICL estimates instance-wise correctness likelihood by measuring prediction consistency across style-altered variants, requiring only the model's forward pass. This makes it a plug-and-play, backpropagation-free calibration module compatible with any TTA method. Comprehensive evaluations across four baselines, five TTA methods, and two realistic scenarios with three model architecture demonstrate that SICL reduces calibration error by an average of 13 percentage points compared to conventional calibration approaches.

</details>


### [336] [Empirical Results for Adjusting Truncated Backpropagation Through Time while Training Neural Audio Effects](https://arxiv.org/abs/2512.07393)
*Yann Bourdin,Pierrick Legrand,Fanny Roche*

Main category: cs.LG

TL;DR: 研究优化TBPTT在音频效果建模中的训练，特别关注动态范围压缩，通过调整序列数、批大小和序列长度等超参数提升模型性能


<details>
  <summary>Details</summary>
Motivation: 在数字音频效果建模中，特别是动态范围压缩任务，需要优化TBPTT训练方法来提高模型准确性、训练稳定性并降低计算需求

Method: 使用卷积-循环架构，通过大量实验评估TBPTT的关键超参数（序列数、批大小、序列长度）在不同数据集（有/无用户控制条件）下的影响

Result: 精心调整TBPTT参数能显著提升模型准确性和训练稳定性，同时降低计算需求；客观评估显示性能改善，主观听感测试表明优化配置保持高感知质量

Conclusion: TBPTT超参数的优化对于数字音频效果建模至关重要，特别是动态范围压缩任务，适当的参数配置能同时提升性能、稳定性和计算效率

Abstract: This paper investigates the optimization of Truncated Backpropagation Through Time (TBPTT) for training neural networks in digital audio effect modeling, with a focus on dynamic range compression. The study evaluates key TBPTT hyperparameters -- sequence number, batch size, and sequence length -- and their influence on model performance. Using a convolutional-recurrent architecture, we conduct extensive experiments across datasets with and without conditionning by user controls. Results demonstrate that carefully tuning these parameters enhances model accuracy and training stability, while also reducing computational demands. Objective evaluations confirm improved performance with optimized settings, while subjective listening tests indicate that the revised TBPTT configuration maintains high perceptual quality.

</details>


### [337] [Model-Based Reinforcement Learning Under Confounding](https://arxiv.org/abs/2512.07528)
*Nishanth Venkatesh,Andreas A. Malikopoulos*

Main category: cs.LG

TL;DR: 提出一种在上下文未观测的混淆C-MDPs中进行模型学习和规划的方法，通过代理变量识别混淆奖励期望，结合行为平均转移模型构建替代MDP


<details>
  <summary>Details</summary>
Motivation: 在上下文未观测的混淆C-MDPs中，传统模型学习方法存在根本不一致性，因为行为策略下的转移和奖励机制与评估状态策略所需的干预量不对应

Method: 采用近端离策略评估方法，利用代理变量在温和可逆条件下识别混淆奖励期望，结合行为平均转移模型构建替代MDP，与最大因果熵模型学习框架集成

Result: 构建的替代MDP的Bellman算子对状态策略是良定义且一致的，能够在上下文未观测、不可用或收集不实际的混淆环境中实现原则性模型学习和规划

Conclusion: 该方法解决了混淆C-MDPs中传统模型学习不一致的问题，为上下文信息缺失的环境提供了可行的模型学习和规划方案

Abstract: We investigate model-based reinforcement learning in contextual Markov decision processes (C-MDPs) in which the context is unobserved and induces confounding in the offline dataset. In such settings, conventional model-learning methods are fundamentally inconsistent, as the transition and reward mechanisms generated under a behavioral policy do not correspond to the interventional quantities required for evaluating a state-based policy. To address this issue, we adapt a proximal off-policy evaluation approach that identifies the confounded reward expectation using only observable state-action-reward trajectories under mild invertibility conditions on proxy variables. When combined with a behavior-averaged transition model, this construction yields a surrogate MDP whose Bellman operator is well defined and consistent for state-based policies, and which integrates seamlessly with the maximum causal entropy (MaxCausalEnt) model-learning framework. The proposed formulation enables principled model learning and planning in confounded environments where contextual information is unobserved, unavailable, or impractical to collect.

</details>


### [338] [Adaptive Tuning of Parameterized Traffic Controllers via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2512.07417)
*Giray Önür,Azita Dabiri,Bart De Schutter*

Main category: cs.LG

TL;DR: 提出多智能体强化学习框架，让智能体自适应调整状态反馈交通控制器的参数，结合状态反馈控制器的反应性和强化学习的适应性，提高训练效率和系统鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统交通管理策略（如路线引导、匝道控制和交通信号控制）通常依赖状态反馈控制器，虽然简单反应快但缺乏适应性，难以应对复杂多变的交通动态。

Method: 提出多智能体强化学习框架，每个智能体自适应调整状态反馈交通控制器的参数（而非直接高频控制），以较低频率调整参数，结合状态反馈控制器的反应性和强化学习的适应性，多智能体结构增强系统鲁棒性。

Result: 在模拟的多类别交通网络中进行评估，结果显示：提出的多智能体框架优于无控制和固定参数状态反馈控制，与单智能体RL自适应状态反馈控制性能相当，但对部分故障具有更好的恢复能力。

Conclusion: 多智能体强化学习框架成功结合了状态反馈控制器的反应性和强化学习的适应性，通过参数调整而非直接控制提高了训练效率，多智能体结构增强了系统鲁棒性，在交通控制中表现出优越性能。

Abstract: Effective traffic control is essential for mitigating congestion in transportation networks. Conventional traffic management strategies, including route guidance, ramp metering, and traffic signal control, often rely on state feedback controllers, used for their simplicity and reactivity; however, they lack the adaptability required to cope with complex and time-varying traffic dynamics. This paper proposes a multi-agent reinforcement learning framework in which each agent adaptively tunes the parameters of a state feedback traffic controller, combining the reactivity of state feedback controllers with the adaptability of reinforcement learning. By tuning parameters at a lower frequency rather than directly determining control actions at a high frequency, the reinforcement learning agents achieve improved training efficiency while maintaining adaptability to varying traffic conditions. The multi-agent structure further enhances system robustness, as local controllers can operate independently in the event of partial failures. The proposed framework is evaluated on a simulated multi-class transportation network under varying traffic conditions. Results show that the proposed multi-agent framework outperforms the no control and fixed-parameter state feedback control cases, while performing on par with the single-agent RL-based adaptive state feedback control, with a much better resilience to partial failures.

</details>


### [339] [Revolutionizing Mixed Precision Quantization: Towards Training-free Automatic Proxy Discovery via Large Language Models](https://arxiv.org/abs/2512.07419)
*Haidong Kang,Jun Du,Lihong Lin*

Main category: cs.LG

TL;DR: 提出TAP框架，利用大语言模型自动发现混合精度量化的训练免费代理，无需人工专家参与，通过DPO强化学习优化提示，实现最先进的量化性能。


<details>
  <summary>Details</summary>
Motivation: 现有混合精度量化方法要么依赖昂贵的可微分优化（效率低且不灵活），要么需要人工专家设计代理（劳动密集且需专业知识）。能否设计一个无需人工专家参与和训练的代理？

Method: 提出TAP框架：1) 利用大语言模型自动发现适合混合精度量化的训练免费代理；2) 提出基于直接策略优化的强化学习来优化提示，弥合黑盒大语言模型与复杂量化任务之间的差距，构建正反馈循环。

Result: 在主流基准测试上的广泛实验表明，TAP实现了最先进的性能，显著优于现有方法。

Conclusion: TAP为混合精度量化社区提供了新的大语言模型驱动设计范式，无需人工专家参与和训练，实现了自动化的高效代理发现。

Abstract: Mixed-Precision Quantization (MPQ) liberates the Deep Neural Networks (DNNs) from the Out-Of-Memory (OOM) bottleneck, which garnered increasing research attention. However, conventional methods either searched from costly differentiable optimization, which is neither efficient nor flexible, or learned a quantized DNN from the proxy (i.e., HAWQ) manually designed by human experts, which is labor-intensive and requires huge expert knowledge. Can we design a proxy without involving any human experts and training? In this paper, we provide an affirmative answer by proposing a novel Large Language Models (LLMs)-driven Training-free Automatic Proxy (dubbed TAP) discovery framework, which reforms the design paradigm of MPQ by utilizing LLMs to find superior TAP tailored for MPQ, automatically. In addition, to bridge the gap between black-box LLMs and the tough MPQ task, we ingeniously propose simple Direct Policy Optimization (DPO) based reinforcement learning to enhance LLMs' reasoning by optimizing prompts, which can construct a positive feedback loop between the LLM and the MPQ task, enabling LLMs to generate better TAP in the next evolution. Extensive experiments on mainstream benchmarks demonstrate that TAP achieves state-of-the-art performance. Finally, we truly believe that our TAP will significantly contribute to the MPQ community by providing a new perspective on LLM-driven design algorithms.

</details>


### [340] [Weighted Contrastive Learning for Anomaly-Aware Time-Series Forecasting](https://arxiv.org/abs/2512.07569)
*Joel Ekstrand,Tor Mattsson,Zahra Taghiyarrenani,Slawomir Nowaczyk,Jens Lundström,Mikael Lindén*

Main category: cs.LG

TL;DR: WECA是一种加权对比适应方法，通过对齐正常和异常增强表示来提升多元时间序列在异常条件下的预测可靠性，在ATM现金物流等应用中显著改善异常数据的预测性能。


<details>
  <summary>Details</summary>
Motivation: 现代深度预测模型在正常数据上表现良好，但在分布偏移（如ATM现金物流中的突发需求变化）发生时经常失败。需要一种方法能够在异常条件下保持可靠的预测性能。

Method: 提出加权对比适应（WECA），使用加权对比目标来对齐正常和异常增强的表示，在保持异常相关信息的同时，确保在良性变化下的一致性。

Result: 在全国ATM交易数据集上，使用领域知识注入的异常进行评估，WECA将异常影响数据的SMAPE提高了6.1个百分点，而对正常数据的性能下降可忽略不计。

Conclusion: WECA能够在异常条件下增强预测可靠性，同时不会牺牲正常操作期间的性能，为多元时间序列的稳健预测提供了有效解决方案。

Abstract: Reliable forecasting of multivariate time series under anomalous conditions is crucial in applications such as ATM cash logistics, where sudden demand shifts can disrupt operations. Modern deep forecasters achieve high accuracy on normal data but often fail when distribution shifts occur. We propose Weighted Contrastive Adaptation (WECA), a Weighted contrastive objective that aligns normal and anomaly-augmented representations, preserving anomaly-relevant information while maintaining consistency under benign variations. Evaluations on a nationwide ATM transaction dataset with domain-informed anomaly injection show that WECA improves SMAPE on anomaly-affected data by 6.1 percentage points compared to a normally trained baseline, with negligible degradation on normal data. These results demonstrate that WECA enhances forecasting reliability under anomalies without sacrificing performance during regular operations.

</details>


### [341] [Mitigating Bias in Graph Hyperdimensional Computing](https://arxiv.org/abs/2512.07433)
*Yezi Liu,William Youngwoo Chung,Yang Ni,Hanning Chen,Mohsen Imani*

Main category: cs.LG

TL;DR: FairGHDC：一种用于图超维计算的公平性感知训练框架，通过偏置校正项减少群体间的不公平性，同时保持计算效率优势。


<details>
  <summary>Details</summary>
Motivation: 图超维计算（HDC）在认知任务中显示出潜力，但其公平性影响尚未被充分探索。数据表示和决策规则中的偏置可能导致对不同群体的不平等对待，需要研究如何缓解这些偏置。

Method: 提出FairGHDC框架，引入基于差距的人口统计均等正则化器推导出的偏置校正项，将其转换为标量公平因子，用于缩放真实标签类别超向量的更新。该方法直接在超向量空间进行去偏，无需修改图编码器或反向传播。

Result: 在六个基准数据集上的实验表明，FairGHDC显著减少了人口统计均等和机会均等差距，同时保持与标准GNN和公平感知GNN相当的准确性。在训练时间上实现了约10倍的GPU加速。

Conclusion: FairGHDC成功解决了图超维计算中的公平性问题，在保持HDC计算效率优势的同时，有效减少了群体间的不公平性，为公平感知的图学习提供了高效解决方案。

Abstract: Graph hyperdimensional computing (HDC) has emerged as a promising paradigm for cognitive tasks, emulating brain-like computation with high-dimensional vectors known as hypervectors. While HDC offers robustness and efficiency on graph-structured data, its fairness implications remain largely unexplored. In this paper, we study fairness in graph HDC, where biases in data representation and decision rules can lead to unequal treatment of different groups. We show how hypervector encoding and similarity-based classification can propagate or even amplify such biases, and we propose a fairness-aware training framework, FairGHDC, to mitigate them. FairGHDC introduces a bias correction term, derived from a gap-based demographic-parity regularizer, and converts it into a scalar fairness factor that scales the update of the class hypervector for the ground-truth label. This enables debiasing directly in the hypervector space without modifying the graph encoder or requiring backpropagation. Experimental results on six benchmark datasets demonstrate that FairGHDC substantially reduces demographic-parity and equal-opportunity gaps while maintaining accuracy comparable to standard GNNs and fairness-aware GNNs. At the same time, FairGHDC preserves the computational advantages of HDC, achieving up to about one order of magnitude ($\approx 10\times$) speedup in training time on GPU compared to GNN and fairness-aware GNN baselines.

</details>


### [342] [Parallel Algorithms for Combined Regularized Support Vector Machines: Application in Music Genre Classification](https://arxiv.org/abs/2512.07463)
*Rongmei Liang,Zizheng Liu,Xiaofei Wu,Jingwen Tu*

Main category: cs.LG

TL;DR: 提出基于共识结构的统一优化框架，开发分布式并行ADMM算法处理分布式存储大数据中的组合正则化支持向量机，引入高斯回代法保证收敛，并应用于音乐信息检索。


<details>
  <summary>Details</summary>
Motivation: 人工智能快速发展需要有效数据处理和模型优化。组合正则化支持向量机(CR-SVMs)能有效处理数据特征结构信息，但在分布式存储大数据中缺乏高效算法。

Method: 提出基于共识结构的统一优化框架，适用于多种损失函数和组合正则化项，可扩展到非凸正则化。开发分布式并行ADMM算法处理分布式数据，引入高斯回代法保证收敛，并提出稀疏群套索支持向量机(SGL-SVM)模型应用于音乐信息检索。

Result: 理论分析表明算法计算复杂度不受正则化项和损失函数影响，具有普适性。在合成和免费音乐档案数据集上的实验证明算法可靠、稳定且高效。

Conclusion: 提出的统一优化框架和分布式并行ADMM算法有效解决了分布式存储大数据中CR-SVMs的计算问题，具有强可扩展性和通用性，在音乐信息检索等应用中表现良好。

Abstract: In the era of rapid development of artificial intelligence, its applications span across diverse fields, relying heavily on effective data processing and model optimization. Combined Regularized Support Vector Machines (CR-SVMs) can effectively handle the structural information among data features, but there is a lack of efficient algorithms in distributed-stored big data. To address this issue, we propose a unified optimization framework based on consensus structure. This framework is not only applicable to various loss functions and combined regularization terms but can also be effectively extended to non-convex regularization terms, showing strong scalability. Based on this framework, we develop a distributed parallel alternating direction method of multipliers (ADMM) algorithm to efficiently compute CR-SVMs when data is stored in a distributed manner. To ensure the convergence of the algorithm, we also introduce the Gaussian back-substitution method. Meanwhile, for the integrity of the paper, we introduce a new model, the sparse group lasso support vector machine (SGL-SVM), and apply it to music information retrieval. Theoretical analysis confirms that the computational complexity of the proposed algorithm is not affected by different regularization terms and loss functions, highlighting the universality of the parallel algorithm. Experiments on synthetic and free music archiv datasets demonstrate the reliability, stability, and efficiency of the algorithm.

</details>


### [343] [Time Series Foundation Models for Process Model Forecasting](https://arxiv.org/abs/2512.07624)
*Yongbo Yu,Jari Peeperkorn,Johannes De Smedt,Jochen De Weerdt*

Main category: cs.LG

TL;DR: 本文研究了时间序列基础模型在过程模型预测中的应用，发现预训练的时间序列基础模型在零样本设置下就能超越传统方法和专门训练的模型，微调带来的改进有限。


<details>
  <summary>Details</summary>
Motivation: 过程模型预测旨在预测业务流程控制流结构随时间的变化，但现有机器学习方法由于直接跟随关系时间序列的稀疏性和异质性，相比统计基线只有有限改进。本文探索时间序列基础模型作为替代方案。

Method: 使用真实事件日志生成的直接跟随关系时间序列，比较时间序列基础模型的零样本使用（无额外训练）与在过程模型预测数据上微调的变体，并与传统和专门训练的模型进行对比。

Result: 时间序列基础模型通常比传统和专门训练的模型获得更低的预测误差（MAE和RMSE），表明能够有效从非过程领域转移时间结构知识。微调虽然能进一步提高准确性，但改进有限，在小型或复杂数据集上可能消失。

Conclusion: 时间序列基础模型在过程相关时间序列上展现出良好的泛化能力和数据效率，零样本使用是强有力的默认选择。这是首次对时间基础模型在过程模型预测中的系统评估。

Abstract: Process Model Forecasting (PMF) aims to predict how the control-flow structure of a process evolves over time by modeling the temporal dynamics of directly-follows (DF) relations, complementing predictive process monitoring that focuses on single-case prefixes. Prior benchmarks show that machine learning and deep learning models provide only modest gains over statistical baselines, mainly due to the sparsity and heterogeneity of the DF time series. We investigate Time Series Foundation Models (TSFMs), large pre-trained models for generic time series, as an alternative for PMF. Using DF time series derived from real-life event logs, we compare zero-shot use of TSFMs, without additional training, with fine-tuned variants adapted on PMF-specific data. TSFMs generally achieve lower forecasting errors (MAE and RMSE) than traditional and specialized models trained from scratch on the same logs, indicating effective transfer of temporal structure from non-process domains. While fine-tuning can further improve accuracy, the gains are often small and may disappear on smaller or more complex datasets, so zero-shot use remains a strong default. Our study highlights the generalization capability and data efficiency of TSFMs for process-related time series and, to the best of our knowledge, provides the first systematic evaluation of temporal foundation models for PMF.

</details>


### [344] [Materium: An Autoregressive Approach for Material Generation](https://arxiv.org/abs/2512.07486)
*Niklas Dobberstein,Jan Hamaekers*

Main category: cs.LG

TL;DR: Materium是一个用于生成晶体结构的自回归Transformer模型，通过将3D材料表示转换为token序列，能够快速生成具有精确分数坐标的晶体结构。


<details>
  <summary>Details</summary>
Motivation: 现有扩散方法需要多次去噪步骤迭代优化原子位置，生成速度慢且计算成本高。需要开发一种能够快速、可扩展生成晶体结构的方法，同时能够基于多种材料属性进行条件生成。

Method: 将3D材料表示（包括元素及其氧化态、分数坐标和晶格参数）转换为token序列，使用自回归Transformer模型进行生成。与扩散方法不同，该方法直接生成精确的原子位置分数坐标。

Result: 模型可以在单个GPU上几小时内完成训练，在GPU和CPU上生成样本的速度远快于基于扩散的方法。在单一条件和组合条件下，模型均表现良好，生成的候选结构与输入条件一致。

Conclusion: Materium提供了一种快速、可扩展的晶体结构生成方法，能够基于多种材料属性进行条件生成，为材料发现和设计提供了高效的工具。

Abstract: We present Materium: an autoregressive transformer for generating crystal structures that converts 3D material representations into token sequences. These sequences include elements with oxidation states, fractional coordinates and lattice parameters. Unlike diffusion approaches, which refine atomic positions iteratively through many denoising steps, Materium places atoms at precise fractional coordinates, enabling fast, scalable generation. With this design, the model can be trained in a few hours on a single GPU and generate samples much faster on GPUs and CPUs than diffusion-based approaches. The model was trained and evaluated using multiple properties as conditions, including fundamental properties, such as density and space group, as well as more practical targets, such as band gap and magnetic density. In both single and combined conditions, the model performs consistently well, producing candidates that align with the requested inputs.

</details>


### [345] [A Mathematical Theory of Top-$k$ Sparse Attention via Total Variation Distance](https://arxiv.org/abs/2512.07647)
*Georgios Tzachristas,Lei Deng,Ioannis Tzachristas,Gong Zhang,Renhai Chen*

Main category: cs.LG

TL;DR: 本文提出了一个统一的数学框架，用于认证Top-k注意力截断，量化分布和输出层面的近似误差，推导出基于排序logits的确定性边界，并在高斯分数模型下给出闭式解。


<details>
  <summary>Details</summary>
Motivation: 注意力机制中的Top-k截断是提高效率的常用技术，但缺乏理论保证来量化近似误差。现有方法通常使用通用不等式，无法提供针对Top-k截断的精确误差边界。

Method: 建立统一的数学框架，证明总变差距离等于丢弃的softmax尾部质量，推导出基于排序logits的非渐近确定性边界。使用精确的头尾分解分析输出误差，并在i.i.d.高斯分数模型下推导闭式尾部质量和最小k选择规则。

Result: 理论分析表明总变差距离与丢弃的softmax尾部质量一致，输出误差可分解为TV距离与头尾均值差的乘积。实验验证了k_ε/n的预测缩放关系，在bert-base-uncased和合成logits上，认证Top-k可将评分键减少2-4倍，同时满足总变差预算。

Conclusion: 该框架为Top-k注意力截断提供了严格的误差认证，建立了分布和输出层面的精确误差边界，为高效注意力机制的设计提供了理论指导，并在实际模型中验证了有效性。

Abstract: We develop a unified mathematical framework for certified Top-$k$ attention truncation that quantifies approximation error at both the distribution and output levels. For a single attention distribution $P$ and its Top-$k$ truncation $\hat P$, we show that the total-variation distance coincides with the discarded softmax tail mass and satisfies $\mathrm{TV}(P,\hat P)=1-e^{-\mathrm{KL}(\hat P\Vert P)}$, yielding sharp Top-$k$-specific bounds in place of generic inequalities. From this we derive non-asymptotic deterministic bounds -- from a single boundary gap through multi-gap and blockwise variants -- that control $\mathrm{TV}(P,\hat P)$ using only the ordered logits. Using an exact head-tail decomposition, we prove that the output error factorizes as $\|\mathrm{Attn}(q,K,V)-\mathrm{Attn}_k(q,K,V)\|_2=τ\|μ_{\mathrm{tail}}-μ_{\mathrm{head}}\|_2$ with $τ=\mathrm{TV}(P,\hat P)$, yielding a new head-tail diameter bound $\|\mathrm{Attn}(q,K,V)-\mathrm{Attn}_k(q,K,V)\|_2\leτ\,\mathrm{diam}_{H,T}$ and refinements linking the error to $\mathrm{Var}_P(V)$. Under an i.i.d. Gaussian score model $s_i\sim\mathcal N(μ,σ^2)$ we derive closed-form tail masses and an asymptotic rule for the minimal $k_\varepsilon$ ensuring $\mathrm{TV}(P,\hat P)\le\varepsilon$, namely $k_\varepsilon/n\approxΦ_c(σ+Φ^{-1}(\varepsilon))$. Experiments on bert-base-uncased and synthetic logits confirm the predicted scaling of $k_\varepsilon/n$ and show that certified Top-$k$ can reduce scored keys by 2-4$\times$ on average while meeting the prescribed total-variation budget.

</details>


### [346] [In-Context and Few-Shots Learning for Forecasting Time Series Data based on Large Language Models](https://arxiv.org/abs/2512.07705)
*Saroj Gopali,Bipin Chhetri,Deepika Giri,Sima Siami-Namini,Akbar Siami Namin*

Main category: cs.LG

TL;DR: 本文比较了多种时间序列预测方法，发现Google的TimesFM在RMSE和推理时间上表现最佳，OpenAI的o4-mini在零样本学习中也表现良好，表明预训练时间序列基础模型是实时预测的有前景方向。


<details>
  <summary>Details</summary>
Motivation: 随着预训练基础模型（如LLMs和Google的TimesFM）的发展，需要研究这些基础模型是否能在时间序列数据分析预测中超越现有方法（ARIMA、Transformer、LSTM、TCN等）。

Method: 研究使用LLM进行时间序列预测，探索上下文学习、零样本学习和少样本学习方法，比较OpenAI o4-mini、Gemini 2.5 Flash Lite、Google的TimesFM以及TCN和LSTM网络。

Result: TimesFM表现最佳，RMSE最低（0.3023），推理时间有竞争力（266秒）。OpenAI的o4-mini在零样本学习中也表现出良好性能。

Conclusion: 预训练时间序列基础模型是实时预测的有前景方向，能够以最小的模型适应实现准确且可扩展的部署。

Abstract: Existing data-driven approaches in modeling and predicting time series data include ARIMA (Autoregressive Integrated Moving Average), Transformer-based models, LSTM (Long Short-Term Memory) and TCN (Temporal Convolutional Network). These approaches, and in particular deep learning-based models such as LSTM and TCN, have shown great results in predicting time series data. With the advancement of leveraging pre-trained foundation models such as Large Language Models (LLMs) and more notably Google's recent foundation model for time series data, {\it TimesFM} (Time Series Foundation Model), it is of interest to investigate whether these foundation models have the capability of outperforming existing modeling approaches in analyzing and predicting time series data.
  This paper investigates the performance of using LLM models for time series data prediction. We investigate the in-context learning methodology in the training of LLM models that are specific to the underlying application domain. More specifically, the paper explores training LLMs through in-context, zero-shot and few-shot learning and forecasting time series data with OpenAI {\tt o4-mini} and Gemini 2.5 Flash Lite, as well as the recent Google's Transformer-based TimesFM, a time series-specific foundation model, along with two deep learning models, namely TCN and LSTM networks. The findings indicate that TimesFM has the best overall performance with the lowest RMSE value (0.3023) and the competitive inference time (266 seconds). Furthermore, OpenAI's o4-mini also exhibits a good performance based on Zero Shot learning.
  These findings highlight pre-trained time series foundation models as a promising direction for real-time forecasting, enabling accurate and scalable deployment with minimal model adaptation.

</details>


### [347] [Machine Learning: Progress and Prospects](https://arxiv.org/abs/2512.07519)
*Alexander Gammerman*

Main category: cs.LG

TL;DR: 这篇1996年的就职讲座回顾了机器学习的历史起源，从亚里士多德到20世纪的发展，并讨论了该领域的多学科影响和不同研究方向。


<details>
  <summary>Details</summary>
Motivation: 讲座旨在探讨机器学习的起源和发展，追溯其思想根源，并展示该领域如何从不同学科中汲取灵感，形成今天多样化的研究格局。

Method: 采用历史回顾的方法，从哲学、数学和计算机科学的角度追溯机器学习思想的演变，包括引用亚里士多德、奥卡姆、休谟、费舍尔和香农等关键人物的贡献。

Result: 明确了机器学习并非单一学科，而是受到多学科影响的交叉领域，包含归纳学习、神经网络、聚类和学习理论等多个并行研究方向。

Conclusion: 机器学习的思想根源可以追溯到古代哲学，经过几个世纪的发展，形成了今天这个包含多个子领域的丰富学科，其发展体现了西方文明思想的连续性。

Abstract: This Inaugural Lecture was given at Royal Holloway University of London in 1996. It covers an introduction to machine learning and describes various theoretical advances and practical projects in the field. The Lecture here is presented in its original format, but a few remarks have been added in 2025 to reflect recent developments, and the list of references has been updated to enhance the convenience and accuracy for readers.
  When did machine learning start? Maybe a good starting point is 1949, when Claude Shannon proposed a learning algorithm for chess-playing programs. Or maybe we should go back to the 1930s when Ronald Fisher developed discriminant analysis - a type of learning where the problem is to construct a decision rule that separates two types of vectors. Or could it be the 18th century when David Hume discussed the idea of induction? Or the 14th century, when William of Ockham formulated the principle of "simplicity" known as "Ockham's razor" (Ockham, by the way, is a small village not far from Royal Holloway). Or it may be that, like almost everything else in Western civilisation and culture, the origin of these ideas lies in the Mediterranean. After all, it was Aristotle who said that "we learn some things only by doing things".
  The field of machine learning has been greatly influenced by other disciplines and the subject is in itself not a very homogeneous discipline, but includes separate, overlapping subfields. There are many parallel lines of research in ML: inductive learning, neural networks, clustering, and theories of learning. They are all part of the more general field of machine learning.

</details>


### [348] [Enabling Delayed-Full Charging Through Transformer-Based Real-Time-to-Departure Modeling for EV Battery Longevity](https://arxiv.org/abs/2512.07723)
*Yonggeon Lee,Jibin Hwang,Alfred Malengo Kondoro,Juhyun Song,Youngtae Noh*

Main category: cs.LG

TL;DR: 基于Transformer的实时事件预测模型，用于准确预测电动汽车用户出发时间，以优化充电策略延长电池寿命


<details>
  <summary>Details</summary>
Motivation: 电动汽车锂离子电池在长时间高电量状态下会加速退化，可以通过延迟充满电至出发前缓解，但这需要准确预测用户出发时间

Method: 提出基于Transformer的实时事件预测模型，将每天时间离散化为网格化token序列，利用流式上下文信息而非仅依赖历史模式来预测出发时间

Result: 在93名用户的真实世界研究中，使用被动智能手机数据进行评估，该方法能有效捕捉个体日常中的不规则出发模式，性能优于基线模型

Conclusion: 该方法具有实际部署潜力，能够为可持续交通系统做出贡献，通过优化充电策略延长电池寿命

Abstract: Electric vehicles (EVs) are key to sustainable mobility, yet their lithium-ion batteries (LIBs) degrade more rapidly under prolonged high states of charge (SOC). This can be mitigated by delaying full charging \ours until just before departure, which requires accurate prediction of user departure times. In this work, we propose Transformer-based real-time-to-event (TTE) model for accurate EV departure prediction. Our approach represents each day as a TTE sequence by discretizing time into grid-based tokens. Unlike previous methods primarily dependent on temporal dependency from historical patterns, our method leverages streaming contextual information to predict departures. Evaluation on a real-world study involving 93 users and passive smartphone data demonstrates that our method effectively captures irregular departure patterns within individual routines, outperforming baseline models. These results highlight the potential for practical deployment of the \ours algorithm and its contribution to sustainable transportation systems.

</details>


### [349] [FRWKV:Frequency-Domain Linear Attention for Long-Term Time Series Forecasting](https://arxiv.org/abs/2512.07539)
*Qingyuan Yang,Shizhuo,Dongyue Chen,Da Teng,Zehua Gan*

Main category: cs.LG

TL;DR: FRWKV：结合线性注意力与频域分析的时序预测框架，实现O(T)复杂度，在长序列预测中表现优异


<details>
  <summary>Details</summary>
Motivation: 传统Transformer在长序列时序预测中存在两个主要瓶颈：1）二次方复杂度O(T²)导致计算效率低下；2）未能有效利用频域信息。需要一种既能降低计算复杂度又能充分利用频域特征的解决方案。

Method: 提出FRWKV框架，结合线性注意力机制（受RWKV启发）与频域分析。通过线性注意力实现O(T)计算复杂度，同时引入频域编码器来增强时序特征表示，形成可扩展的长序列建模方法。

Result: 在8个真实世界数据集上，FRWKV取得了第一的平均排名。消融研究证实了线性注意力和频域编码器两个组件的关键作用，验证了方法的有效性。

Conclusion: 这项工作展示了线性注意力与频域分析之间的强大协同效应，为可扩展的时序建模建立了新范式。代码已开源，为后续研究提供了基础。

Abstract: Traditional Transformers face a major bottleneck in long-sequence time series forecasting due to their quadratic complexity $(\mathcal{O}(T^2))$ and their limited ability to effectively exploit frequency-domain information. Inspired by RWKV's $\mathcal{O}(T)$ linear attention and frequency-domain modeling, we propose FRWKV, a frequency-domain linear-attention framework that overcomes these limitations. Our model integrates linear attention mechanisms with frequency-domain analysis, achieving $\mathcal{O}(T)$ computational complexity in the attention path while exploiting spectral information to enhance temporal feature representations for scalable long-sequence modeling. Across eight real-world datasets, FRWKV achieves a first-place average rank. Our ablation studies confirm the critical roles of both the linear attention and frequency-encoder components. This work demonstrates the powerful synergy between linear attention and frequency analysis, establishing a new paradigm for scalable time series modeling. Code is available at this repository: https://github.com/yangqingyuan-byte/FRWKV.

</details>


### [350] [RRAEDy: Adaptive Latent Linearization of Nonlinear Dynamical Systems](https://arxiv.org/abs/2512.07542)
*Jad Mounayer,Sebastian Rodriguez,Jerome Tomezyk,Chady Ghnatios,Francisco Chinesta*

Main category: cs.LG

TL;DR: RRAEDy是一种新型潜在空间动态模型，通过自动发现合适的潜在维度并强制正则化和线性化动态，解决了现有模型需要预先固定维度、依赖复杂损失平衡和缺乏正则化的问题。


<details>
  <summary>Details</summary>
Motivation: 现有潜在空间动态模型存在三个主要限制：需要预先固定潜在维度、依赖复杂损失平衡来近似线性动态、缺乏对潜在变量的正则化。这些限制影响了模型的灵活性和稳定性。

Method: 基于秩约减自编码器(RRAEs)，RRAEDy通过奇异值自动排序和剪枝潜在变量，同时学习控制时间演化的潜在动态模式分解(DMD)算子。这种无结构但线性约束的公式使模型能够学习稳定低维动态，无需辅助损失或手动调参。

Result: 在Van der Pol振荡器、Burgers方程、2D Navier-Stokes和旋转高斯等基准测试中，RRAEDy实现了准确和鲁棒的预测。理论分析证明了学习算子的稳定性，并扩展了处理参数化ODE的能力。

Conclusion: RRAEDy通过自动发现潜在维度、强制正则化和线性化动态，提供了一种更灵活、稳定且无需复杂调参的动态系统建模方法，在多个基准测试中表现出色。

Abstract: Most existing latent-space models for dynamical systems require fixing the latent dimension in advance, they rely on complex loss balancing to approximate linear dynamics, and they don't regularize the latent variables. We introduce RRAEDy, a model that removes these limitations by discovering the appropriate latent dimension, while enforcing both regularized and linearized dynamics in the latent space. Built upon Rank-Reduction Autoencoders (RRAEs), RRAEDy automatically rank and prune latent variables through their singular values while learning a latent Dynamic Mode Decomposition (DMD) operator that governs their temporal progression. This structure-free yet linearly constrained formulation enables the model to learn stable and low-dimensional dynamics without auxiliary losses or manual tuning. We provide theoretical analysis demonstrating the stability of the learned operator and showcase the generality of our model by proposing an extension that handles parametric ODEs. Experiments on canonical benchmarks, including the Van der Pol oscillator, Burgers' equation, 2D Navier-Stokes, and Rotating Gaussians, show that RRAEDy achieves accurate and robust predictions. Our code is open-source and available at https://github.com/JadM133/RRAEDy. We also provide a video summarizing the main results at https://youtu.be/ox70mSSMGrM.

</details>


### [351] [ReLaX: Reasoning with Latent Exploration for Large Reasoning Models](https://arxiv.org/abs/2512.07558)
*Shimin Zhang,Xianwei Chen,Yufan Shen,Ziyuan Ye,Jibin Wu*

Main category: cs.LG

TL;DR: 本文提出ReLaX方法，通过分析大语言模型的潜在动态来调控探索与利用的平衡，解决RLVR中的熵崩溃问题，显著提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 虽然RLVR能增强大语言模型的推理能力，但常导致熵崩溃，造成策略过早收敛和性能饱和。现有方法主要操纵token级熵来促进探索，但作者认为token生成背后的潜在动态包含更丰富的计算结构，能更好地指导探索与利用的平衡。

Method: 1) 利用Koopman算子理论获得大语言模型隐藏状态动态的线性化表示；2) 提出动态谱分散度(DSD)指标来量化模型潜在动态的异质性，作为策略探索的直接指标；3) 提出ReLaX范式，在策略优化中显式地结合潜在动态来调控探索与利用。

Result: 在多模态和纯文本推理基准测试中的综合实验表明，ReLaX能显著缓解过早收敛问题，并持续实现最先进的性能。

Conclusion: 通过分析大语言模型的潜在动态来调控探索与利用的平衡，ReLaX方法有效解决了RLVR中的熵崩溃问题，提升了推理模型的性能，为强化学习在大语言模型中的应用提供了新思路。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated remarkable potential in enhancing the reasoning capability of Large Reasoning Models (LRMs). However, RLVR often leads to entropy collapse, resulting in premature policy convergence and performance saturation. While manipulating token-level entropy has proven effective for promoting policy exploration, we argue that the latent dynamics underlying token generation encode a far richer computational structure for steering policy optimization toward a more effective exploration-exploitation tradeoff. To enable tractable analysis and intervention of the latent dynamics of LRMs, we leverage Koopman operator theory to obtain a linearized representation of their hidden-state dynamics. This enables us to introduce Dynamic Spectral Dispersion (DSD), a new metric to quantify the heterogeneity of the model's latent dynamics, serving as a direct indicator of policy exploration. Building upon these foundations, we propose Reasoning with Latent eXploration (ReLaX), a paradigm that explicitly incorporates latent dynamics to regulate exploration and exploitation during policy optimization. Comprehensive experiments across a wide range of multimodal and text-only reasoning benchmarks show that ReLaX significantly mitigates premature convergence and consistently achieves state-of-the-art performance.

</details>


### [352] [Depth-Wise Activation Steering for Honest Language Models](https://arxiv.org/abs/2512.07667)
*Gracjan Góral,Marysia Winkels,Steven Basart*

Main category: cs.LG

TL;DR: 提出一种无需训练的高斯调度激活导向方法，通过跨网络深度加权导向强度，有效提升大语言模型的诚实性而非准确性


<details>
  <summary>Details</summary>
Motivation: 大语言模型有时会断言虚假信息，尽管内部表示正确答案，这是诚实性而非准确性的失败，损害了可审计性和安全性。现有方法主要优化事实正确性或依赖重新训练和脆弱的单层编辑，对真实报告的控制有限。

Method: 提出无需训练的激活导向方法，使用高斯调度在跨网络深度加权导向强度。该方法简单、模型无关、无需微调，为从模型现有能力中引出真实报告提供低成本控制手段。

Result: 在MASK基准测试中（区分诚实性与知识），评估了LLaMA、Qwen和Mistral家族的七个模型，发现高斯调度在六个模型中比无导向和单层基线提高了诚实性。在LLaMA-3.1-8B-Instruct和Qwen-2.5-7B-Instruct上的等预算消融实验显示，高斯调度优于随机、均匀和盒滤波器深度分配，表明干预在深度上的分布方式对结果有实质性影响。

Conclusion: 高斯调度激活导向方法有效提升大语言模型的诚实性，提供了一种简单、模型无关、无需训练的控制手段来引出模型的真实报告能力，超越了仅考虑总强度的干预方法。

Abstract: Large language models sometimes assert falsehoods despite internally representing the correct answer, failures of honesty rather than accuracy, which undermines auditability and safety. Existing approaches largely optimize factual correctness or depend on retraining and brittle single-layer edits, offering limited leverage over truthful reporting. We present a training-free activation steering method that weights steering strength across network depth using a Gaussian schedule. On the MASK benchmark, which separates honesty from knowledge, we evaluate seven models spanning the LLaMA, Qwen, and Mistral families and find that Gaussian scheduling improves honesty over no-steering and single-layer baselines in six of seven models. Equal-budget ablations on LLaMA-3.1-8B-Instruct and Qwen-2.5-7B-Instruct show the Gaussian schedule outperforms random, uniform, and box-filter depth allocations, indicating that how intervention is distributed across depth materially affects outcomes beyond total strength. The method is simple, model-agnostic, requires no finetuning, and provides a low-cost control knob for eliciting truthful reporting from models' existing capabilities.

</details>


### [353] [A multimodal Bayesian Network for symptom-level depression and anxiety prediction from voice and speech data](https://arxiv.org/abs/2512.07741)
*Agnes Norbury,George Fairs,Alexandra L. Georgescu,Matthew M. Nour,Emilia Molimpakis,Stefano Goria*

Main category: cs.LG

TL;DR: 论文提出使用贝叶斯网络建模分析语音特征来预测抑郁和焦虑症状，在大型数据集上验证了性能，并探讨了临床可用性和公平性。


<details>
  <summary>Details</summary>
Motivation: 临床评估中整合言语和非言语信息具有挑战性，需要智能工具支持，但目前临床尚未实现。贝叶斯网络可以解决采用障碍。

Method: 使用贝叶斯网络模型，基于大规模数据集（30,135名独特说话者）的语音特征预测抑郁和焦虑症状，评估性能、人口统计公平性和多模态整合。

Result: 模型性能良好：抑郁和焦虑的ROC-AUC分别为0.842和0.831，ECE分别为0.018和0.015；核心个体症状ROC-AUC>0.74。评估了公平性和临床可用性。

Conclusion: 在丰富的大规模多模态数据流支持下，贝叶斯网络模型是构建稳健评估支持工具的原则性方法，提供透明可解释的输出，适合临床专家监督。

Abstract: During psychiatric assessment, clinicians observe not only what patients report, but important nonverbal signs such as tone, speech rate, fluency, responsiveness, and body language. Weighing and integrating these different information sources is a challenging task and a good candidate for support by intelligence-driven tools - however this is yet to be realized in the clinic. Here, we argue that several important barriers to adoption can be addressed using Bayesian network modelling. To demonstrate this, we evaluate a model for depression and anxiety symptom prediction from voice and speech features in large-scale datasets (30,135 unique speakers). Alongside performance for conditions and symptoms (for depression, anxiety ROC-AUC=0.842,0.831 ECE=0.018,0.015; core individual symptom ROC-AUC>0.74), we assess demographic fairness and investigate integration across and redundancy between different input modality types. Clinical usefulness metrics and acceptability to mental health service users are explored. When provided with sufficiently rich and large-scale multimodal data streams and specified to represent common mental conditions at the symptom rather than disorder level, such models are a principled approach for building robust assessment support tools: providing clinically-relevant outputs in a transparent and explainable format that is directly amenable to expert clinical supervision.

</details>


### [354] [Formalized Hopfield Networks and Boltzmann Machines](https://arxiv.org/abs/2512.07766)
*Matteo Cipollina,Michail Karatarakis,Freek Wiedijk*

Main category: cs.LG

TL;DR: 该论文在Lean 4中形式化神经网络，包括确定性（Hopfield网络）和随机性（Boltzmann机）模型，证明了收敛性、Hebbian学习正确性以及Perron-Frobenius定理的形式化应用。


<details>
  <summary>Details</summary>
Motivation: 神经网络应用广泛但分析和验证困难，需要形式化方法来确保数学严谨性，为神经网络理论提供可靠的基础验证。

Method: 使用Lean 4定理证明器形式化神经网络：1）形式化Hopfield网络及其收敛性；2）证明Hebbian学习在正交模式下的正确性；3）形式化随机网络（Boltzmann机）及其动力学；4）形式化Perron-Frobenius定理并证明随机网络的遍历性。

Result: 成功在Lean 4中形式化了确定性Hopfield网络和随机Boltzmann机，证明了Hopfield网络的收敛性、Hebbian学习的正确性，以及Boltzmann机收敛到唯一平稳分布的遍历性。

Conclusion: 通过Lean 4形式化验证了神经网络的关键理论性质，为神经网络的形式化分析和验证提供了可靠基础，展示了定理证明器在复杂机器学习模型验证中的潜力。

Abstract: Neural networks are widely used, yet their analysis and verification remain challenging. In this work, we present a Lean 4 formalization of neural networks, covering both deterministic and stochastic models. We first formalize Hopfield networks, recurrent networks that store patterns as stable states. We prove convergence and the correctness of Hebbian learning, a training rule that updates network parameters to encode patterns, here limited to the case of pairwise-orthogonal patterns. We then consider stochastic networks, where updates are probabilistic and convergence is to a stationary distribution. As a canonical example, we formalize the dynamics of Boltzmann machines and prove their ergodicity, showing convergence to a unique stationary distribution using a new formalization of the Perron-Frobenius theorem.

</details>


### [355] [GatedFWA: Linear Flash Windowed Attention with Gated Associative Memory](https://arxiv.org/abs/2512.07782)
*Jiaxu Liu,Yuhe Bai,Christos-Savvas Bouganis*

Main category: cs.LG

TL;DR: 提出GatedFWA：一种内存门控的滑动窗口注意力机制，在保持线性时间复杂度的同时，通过可学习的收缩门稳定内存更新并控制梯度流，解决了传统滑动窗口注意力训练目标无界和Softmax注意力内存收缩的问题。


<details>
  <summary>Details</summary>
Motivation: 传统Softmax全注意力具有二次方复杂度，而滑动窗口注意力(SWA)虽然实现线性时间编码/解码，但在关联内存解释下其差分式更新导致训练目标无界。同时，Softmax注意力通过归一化更新会导致内存收缩和梯度消失问题。

Method: 提出GatedFWA机制：为每个token/head累积一个门控值作为衰减偏置添加到注意力logits中，作为内存递归中的可学习收缩门。实现了融合的单通道门预处理和与FlashAttention兼容的内核，在滑动掩码下注入门控，确保I/O效率和数值稳定性。

Result: 在语言建模基准测试中，GatedFWA以可忽略的开销实现了有竞争力的吞吐量，更好地利用了全局上下文，并能与NSA等token压缩/选择方法无缝集成，泛化到各种自回归领域。

Conclusion: GatedFWA在保持滑动窗口注意力效率优势的同时，通过内存门控机制解决了训练稳定性和梯度控制问题，为高效自回归建模提供了新的解决方案。

Abstract: Modern autoregressive models rely on attention, yet the Softmax full attention in Transformers scales quadratically with sequence length. Sliding Window Attention (SWA) achieves linear-time encoding/decoding by constraining the attention pattern, but under an \textit{Associative Memory} interpretation, its difference-style update renders the training objective effectively \emph{unbounded}. In contrast, Softmax attention normalizes updates, leading to \emph{memory shrinkage and gradient vanishing}. We propose GatedFWA: a Memory-\underline{Gated} (\underline{F}lash) \underline{W}indowed \underline{A}ttention mechanism that preserves SWAs efficiency while stabilizing memory updates and making gradient flow controllable. In essence, GatedFWA accumulate a per-token/head gate into a decay bias added to the attention logits, acting as a learnable contraction in the memory recurrence. We implement a fused one-pass gate preprocessing and a FlashAttention-compatible kernel that injects the gate under a sliding mask, ensuring I/O efficiency and numerical stability. On language modelling benchmarks, GatedFWA delivers competitive throughput with negligible overhead and better use of global context, and it integrates cleanly with token compression/selection methods such as NSA and generalizes to various autoregressive domains.

</details>


<div id='q-fin.PR'></div>

# q-fin.PR [[Back]](#toc)

### [356] [Amortizing Perpetual Options](https://arxiv.org/abs/2512.06505)
*Zachary Feinstein*

Main category: q-fin.PR

TL;DR: 提出摊销永续期权(AmPOs)，一种适合交易所交易的连续分期期权可替代品，通过摊销机制保持可替代性，在Black-Scholes框架下可简化为带股息资产的普通永续美式期权


<details>
  <summary>Details</summary>
Motivation: 传统分期期权在持有者停止支付时会失效，破坏了名义金额单位之间的可替代性，限制了在交易所的交易。需要一种保持可替代性的连续分期期权变体。

Method: 用摊销机制替代显式分期支付：通过名义金额的确定性衰减实现隐性支付，确保所有单位相同演化。在Black-Scholes框架下，将AmPO估值简化为带股息资产的普通永续美式期权。

Result: 获得了行权边界和风险中性估值的解析表达式，推导了希腊字母，研究了摊销率的比较静态分析。数值案例展示了摊销率如何影响期权行为和有效波动率敏感性的权衡。

Conclusion: AmPOs通过摊销机制解决了传统分期期权的可替代性问题，在Black-Scholes框架下可获得解析解，为交易所交易提供了实用的连续分期期权设计。

Abstract: In this work, we introduce amortizing perpetual options (AmPOs), a fungible variant of continuous-installment options suitable for exchange-based trading. Traditional installment options lapse when holders cease their payments, destroying fungibility across units of notional. AmPOs replace explicit installment payments and the need for lapsing logic with an implicit payment scheme via a deterministic decay in the claimable notional. This amortization ensures all units evolve identically, preserving fungibility. Under the Black-Scholes framework, AmPO valuation can be reduced to an equivalent vanilla perpetual American option on a dividend-paying asset. In this way, analytical expressions are possible for the exercise boundaries and risk-neutral valuations for calls and puts. These formulas and relations allow us to derive the Greeks and study comparative statics with respect to the amortization rate. Illustrative numerical case studies demonstrate how the amortization rate shapes option behavior and reveal the resulting tradeoffs in the effective volatility sensitivity.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [357] [The Suicide Region: Option Games and the Race to Artificial General Intelligence](https://arxiv.org/abs/2512.07526)
*David Tan*

Main category: q-fin.RM

TL;DR: 传统实物期权理论预测在极端波动或技术不确定性下应延迟投资，但中美AGI竞赛中却出现加速投资的反常现象。论文通过建立内生存在风险的抢占博弈模型，解释了这种"自杀区域"现象，并提出通过内部化风险成本来恢复等待期权价值。


<details>
  <summary>Details</summary>
Motivation: 解释为什么在AGI竞赛中，主权国家（特别是美国和中国）的行为与传统实物期权理论预测相反：尽管认识到AGI错位可能导致灾难性失败，它们仍在加速AI投资。这种理论与现实之间的差异构成了研究动机。

Method: 建立连续时间抢占博弈模型，引入内生存在风险参数。模型将失败成本从传统的沉没投资成本(I)扩展到系统性毁灭参数(D)，该参数与开发速度相关且在全球范围内共享。通过数学推导，分析风险项如何在均衡无差异条件下相互抵消。

Result: 发现存在"自杀区域"：竞争压力迫使理性主体提前部署AGI系统，即使风险调整后的净现值为负。警告性灾难无法阻止AGI加速，因为赢家通吃的竞赛性质保持不变。只有将毁灭成本内部化，使安全研究成为经济可行性的前提，才能停止竞赛。

Conclusion: AGI竞赛的加速投资行为源于系统性风险在博弈均衡中的抵消效应。要恢复等待的期权价值，需要达到关键的私人责任阈值。论文提出了机制设计干预措施，以确保安全的AGI研究和负责任的部署。

Abstract: Standard real options theory predicts delay in exercising the option to invest or deploy when extreme asset volatility or technological uncertainty are present. However, in the current race to develop artificial general intelligence (AGI), sovereign actors are exhibiting behaviors contrary to theoretical predictions: the US and China are accelerating AI investment despite acknowledging the potential for catastrophic failure from AGI misalignment. We resolve this puzzle by formalizing the AGI race as a continuous-time preemption game with endogenous existential risk. In our model, the cost of failure is no longer bounded only by the sunk cost of investment (I), but rather a systemic ruin parameter (D) that is correlated with development velocity and shared globally. As the disutility of catastrophe is embedded in both players' payoffs, the risk term mathematically cancels out of the equilibrium indifference condition. This creates a "suicide region" in the investment space where competitive pressures force rational agents to deploy AGI systems early, despite a negative risk-adjusted net present value. Furthermore, we show that "warning shots" (sub-existential disasters) will fail to deter AGI acceleration, as the winner-takes-all nature of the race remains intact. The race can only be halted if the cost of ruin is internalized, making safety research a prerequisite for economic viability. We derive the critical private liability threshold required to restore the option value of waiting and propose mechanism design interventions that can better ensure safe AGI research and socially responsible deployment.

</details>


### [358] [Learning to Hedge Swaptions](https://arxiv.org/abs/2512.06639)
*Zaniar Ahmadi,Frédéric Godin*

Main category: q-fin.RM

TL;DR: 本文研究基于强化学习的深度对冲框架在互换期权动态对冲中的应用，对比传统基于敏感度的rho对冲方法。结果表明使用两种互换作为对冲工具可实现接近最优的对冲效果，深度对冲策略在不同市场状态下动态调整风险敞口，即使在模型误设情况下也优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于敏感度的rho对冲方法在互换期权对冲中可能存在局限性，需要探索更灵活、适应性更强的对冲方法。强化学习框架能够捕捉复杂的市场动态和风险偏好，为动态对冲提供新思路。

Method: 采用基于强化学习的深度对冲框架，设计三种不同目标函数的智能体（均方误差、下行风险、条件风险价值），使用三因子无套利动态Nelson-Siegel模型进行模拟实验，对比传统rho对冲策略。

Result: 使用两种互换作为对冲工具可实现接近最优的对冲效果；深度对冲策略能动态调整对冲组合的风险敞口；即使在模型误设情况下，深度对冲策略仍优于传统rho对冲策略。

Conclusion: 强化学习在互换期权对冲中具有显著潜力，能够提供更高效、更具韧性的对冲策略，适应不同的风险偏好和市场条件，优于传统敏感度对冲方法。

Abstract: This paper investigates the deep hedging framework, based on reinforcement learning (RL), for the dynamic hedging of swaptions, contrasting its performance with traditional sensitivity-based rho-hedging. We design agents under three distinct objective functions (mean squared error, downside risk, and Conditional Value-at-Risk) to capture alternative risk preferences and evaluate how these objectives shape hedging styles. Relying on a three-factor arbitrage-free dynamic Nelson-Siegel model for our simulation experiments, our findings show that near-optimal hedging effectiveness is achieved when using two swaps as hedging instruments. Deep hedging strategies dynamically adapt the hedging portfolio's exposure to risk factors across states of the market. In our experiments, their out-performance over rho-hedging strategies persists even in the presence some of model misspecification. These results highlight RL's potential to deliver more efficient and resilient swaption hedging strategies.

</details>


### [359] [VaR at Its Extremes: Impossibilities and Conditions for One-Sided Random Variables](https://arxiv.org/abs/2512.07787)
*Nawaf Mohammed*

Main category: q-fin.RM

TL;DR: VaR在单边随机变量求和中的极值聚合行为：对于非负风险，VaR次可加性不可能，除非在完全可加性（仅在同单调时成立）的退化情况。为刻画完全超可加性，提出了两个结构性条件：联合分布的负单纯形依赖(NSD)和边缘依赖泛函的单纯形支配(SD)。


<details>
  <summary>Details</summary>
Motivation: 研究VaR在单边随机变量求和中的极值聚合行为，特别是其在不同概率水平下的可加性特性。传统上VaR的次可加性在风险管理中很重要，但本文发现对于非负风险，VaR次可加性实际上不可能，除非在退化情况，因此需要探索超可加性的条件。

Method: 引入两个结构性条件：1) 负单纯形依赖(NSD)用于描述联合分布；2) 单纯形支配(SD)用于边缘依赖泛函。这些条件提供了一个统一且易于验证的框架，能够处理非相同边缘分布、重尾分布和广泛的负依赖结构。

Result: 对于支撑在[0,∞)上的风险，VaR次可加性不可能，除非在完全可加性的退化情况（仅在同单调时成立）。当满足NSD和SD条件时，VaR是完全超可加性的。所有结果可推广到具有任意有限下界或上界的随机变量。

Conclusion: 本文为VaR在单边随机变量求和中的极值聚合行为提供了完整的刻画：次可加性不可能（除非退化情况），而超可加性可通过NSD和SD条件实现。这些结果为严格次可加性或超可加性何时发生提供了尖锐的约束条件。

Abstract: We investigate the extremal aggregation behavior of Value-at-Risk (VaR) -- that is, its additivity properties across all probability levels -- for sums of one-sided random variables. For risks supported on \([0,\infty)\), we show that VaR sub-additivity is impossible except in the degenerate case of exact additivity, which holds only under co-monotonicity. To characterize when VaR is instead fully super-additive, we introduce two structural conditions: negative simplex dependence (NSD) for the joint distribution and simplex dominance (SD) for a margin-dependent functional. Together, these conditions provide a unified and easily verifiable framework that accommodates non-identical margins, heavy-tailed laws, and a wide spectrum of negative dependence structures. All results extend to random variables with arbitrary finite lower or upper endpoints, yielding sharp constraints on when strict sub- or super-additivity can occur.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [360] [Contextual Strongly Convex Simulation Optimization: Optimize then Predict with Inexact Solutions](https://arxiv.org/abs/2512.06270)
*Nifei Lin,Heng Luo,L. Jeff Hong*

Main category: stat.ML

TL;DR: 研究上下文强凸模拟优化，采用"先优化后预测"方法进行实时决策，分析模拟优化算法的不精确性对最优性差距的影响，建立统一分析框架，推导收敛速率和计算预算分配规则。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽略了模拟优化算法生成解的不精确性对最优性差距的影响，需要开发一个能够同时考虑解偏差和方差的统一分析框架。

Method: 采用"先优化后预测"方法：离线阶段在协变量集上进行模拟优化以近似最优解函数；在线阶段通过评估该近似值获得决策。使用Polyak-Ruppert平均SGD作为示例模拟优化算法，分析四种平滑技术下的最优性差距。

Result: 建立了收敛速率，推导了计算预算Γ在协变量数量和每个协变量模拟努力之间的最优分配规则，证明在适当的平滑技术和样本分配规则下，收敛速率可近似达到Γ^{-1}。

Conclusion: 通过数值研究验证了理论发现，证明了所提方法的有效性和实用价值，为上下文模拟优化中的实时决策提供了理论分析和实用指导。

Abstract: In this work, we study contextual strongly convex simulation optimization and adopt an "optimize then predict" (OTP) approach for real-time decision making. In the offline stage, simulation optimization is conducted across a set of covariates to approximate the optimal-solution function; in the online stage, decisions are obtained by evaluating this approximation at the observed covariate. The central theoretical challenge is to understand how the inexactness of solutions generated by simulation-optimization algorithms affects the optimality gap, which is overlooked in existing studies. To address this, we develop a unified analysis framework that explicitly accounts for both solution bias and variance. Using Polyak-Ruppert averaging SGD as an illustrative simulation-optimization algorithm, we analyze the optimality gap of OTP under four representative smoothing techniques: $k$ nearest neighbor, kernel smoothing, linear regression, and kernel ridge regression. We establish convergence rates, derive the optimal allocation of the computational budget $Γ$ between the number of design covariates and the per-covariate simulation effort, and demonstrate the convergence rate can approximately achieve $Γ^{-1}$ under appropriate smoothing technique and sample-allocation rule. Finally, through a numerical study, we validate the theoretical findings and demonstrate the effectiveness and practical value of the proposed approach.

</details>


### [361] [Modeling Spatio-temporal Extremes via Conditional Variational Autoencoders](https://arxiv.org/abs/2512.06348)
*Xiaoyu Ma,Likun Zhang,Christopher K. Wikle*

Main category: stat.ML

TL;DR: 提出一种基于条件变分自编码器(cXVAE)的时空极端事件建模方法，通过卷积神经网络整合气候指数，能够高效模拟空间场、检测条件驱动的变化，并进行反事实实验。


<details>
  <summary>Details</summary>
Motivation: 极端天气事件的时空共现模式在气候变化下可能增强或减弱，需要一种能够整合气候指数、高效建模时空极端事件的方法来理解条件驱动的变化。

Method: 提出条件变分自编码器(cXVAE)，在解码器中嵌入卷积神经网络(CNN)，将气候指数与潜在空间中的空间依赖性进行卷积，使解码器依赖于气候变量。

Result: 通过模拟验证cXVAE能准确模拟空间场并恢复时空变化的极端依赖性，计算成本低；提供检测条件驱动变化的方法；支持反事实实验量化联合尾部风险、共现范围和重现期指标差异。

Conclusion: 该方法在澳大利亚东部2014-2024年火灾天气指数(FWI)分析中展示了实用性和性能，为理解气候条件如何影响极端事件时空依赖性提供了有效工具。

Abstract: Extreme weather events are widely studied in fields such as agriculture, ecology, and meteorology. The spatio-temporal co-occurrence of extreme events can strengthen or weaken under changing climate conditions. In this paper, we propose a novel approach to model spatio-temporal extremes by integrating climate indices via a conditional variational autoencoder (cXVAE). A convolutional neural network (CNN) is embedded in the decoder to convolve climatological indices with the spatial dependence within the latent space, thereby allowing the decoder to be dependent on the climate variables. There are three main contributions here. First, we demonstrate through extensive simulations that the proposed conditional XVAE accurately emulates spatial fields and recovers spatially and temporally varying extremal dependence with very low computational cost post training. Second, we provide a simple, scalable approach to detecting condition-driven shifts and whether the dependence structure is invariant to the conditioning variable. Third, when dependence is found to be condition-sensitive, the conditional XVAE supports counterfactual experiments allowing intervention on the climate covariate and propagating the associated change through the learned decoder to quantify differences in joint tail risk, co-occurrence ranges, and return metrics. To demonstrate the practical utility and performance of the model in real-world scenarios, we apply our method to analyze the monthly maximum Fire Weather Index (FWI) over eastern Australia from 2014 to 2024 conditioned on the El Niño/Southern Oscillation (ENSO) index.

</details>


### [362] [Canonical Tail Dependence for Soft Extremal Clustering of Multichannel Brain Signals](https://arxiv.org/abs/2512.06435)
*Mara Sherlin Talento,Jordan Richards,Raphael Huser,Hernando Ombao*

Main category: stat.ML

TL;DR: 提出了一种基于尾部极值依赖性的脑信号分析方法，通过尾部连接性特征更准确地识别癫痫等极端事件，并开发了尾部典型相关方法来可视化极端通道贡献。


<details>
  <summary>Details</summary>
Motivation: 现有脑连接性分析方法在识别极端事件（如癫痫发作）方面存在局限，传统尾部依赖建模方法无法识别驱动最大尾部依赖的特定通道，而这些信息对癫痫患者分析至关重要。

Method: 将传统典型相关分析扩展到尾部依赖领域，开发尾部典型依赖度量，通过尾部成对依赖矩阵（TPDM）构建计算高效的估计器，实现极端通道贡献的可视化。

Result: 尾部连接性提供了额外的判别能力，能够更准确地识别极端相关事件，改进癫痫风险管理，成功应用于新生儿频率软聚类，区分有癫痫和无癫痫的婴儿。

Conclusion: 尾部依赖分析为脑信号极端事件识别提供了新视角，尾部典型相关方法能够有效可视化极端通道贡献，在癫痫诊断和风险管理中具有重要应用价值。

Abstract: We develop a novel characterization of extremal dependence between two cortical regions of the brain when its signals display extremely large amplitudes. We show that connectivity in the tails of the distribution reveals unique features of extreme events (e.g., seizures) that can help to identify their occurrence. Numerous studies have established that connectivity-based features are effective for discriminating brain states. Here, we demonstrate the advantage of the proposed approach: that tail connectivity provides additional discriminatory power, enabling more accurate identification of extreme-related events and improved seizure risk management. Common approaches in tail dependence modeling use pairwise summary measures or parametric models. However, these approaches do not identify channels that drive the maximal tail dependence between two groups of signals -- an information that is useful when analyzing electroencephalography of epileptic patients where specific channels are responsible for seizure occurrences. A familiar approach in traditional signal processing is canonical correlation, which we extend to the tails to develop a visualization of extremal channel-contributions. Through the tail pairwise dependence matrix (TPDM), we develop a computationally-efficient estimator for our canonical tail dependence measure. Our method is then used for accurate frequency-based soft clustering of neonates, distinguishing those with seizures from those without.

</details>


### [363] [Latent Nonlinear Denoising Score Matching for Enhanced Learning of Structured Distributions](https://arxiv.org/abs/2512.06615)
*Kaichen Shen,Wei Zhu*

Main category: stat.ML

TL;DR: 提出LNDSM方法，将非线性前向动力学与VAE潜在SGM框架结合，通过欧拉-丸山方案重构交叉熵项，移除方差爆炸项，在MNIST变体上实现更快合成和更好分布学习。


<details>
  <summary>Details</summary>
Motivation: 现有基于分数的生成模型在潜在空间中通常使用线性前向动力学，限制了处理结构化分布的能力。需要一种方法将非线性前向动力学与VAE潜在SGM框架结合，以提高样本质量和学习效率。

Method: 提出潜在非线性去噪分数匹配(LNDSM)：1) 将非线性前向动力学整合到VAE潜在SGM框架中；2) 使用欧拉-丸山方案诱导的近似高斯转移重构交叉熵项；3) 识别并移除由小时间步长引起的两个零均值但方差爆炸的项以确保数值稳定性。

Result: 在MNIST数据集变体上的实验表明：1) 实现了更快的合成速度；2) 增强了固有结构化分布的学习能力；3) 相比基准的结构不可知潜在SGMs，LNDSM始终获得更优的样本质量和多样性。

Conclusion: LNDSM成功整合了非线性前向动力学与潜在SGM框架，通过数值稳定的训练目标实现了更高效的生成建模，为处理结构化分布提供了有效解决方案。

Abstract: We present latent nonlinear denoising score matching (LNDSM), a novel training objective for score-based generative models that integrates nonlinear forward dynamics with the VAE-based latent SGM framework. This combination is achieved by reformulating the cross-entropy term using the approximate Gaussian transition induced by the Euler-Maruyama scheme. To ensure numerical stability, we identify and remove two zero-mean but variance exploding terms arising from small time steps. Experiments on variants of the MNIST dataset demonstrate that the proposed method achieves faster synthesis and enhanced learning of inherently structured distributions. Compared to benchmark structure-agnostic latent SGMs, LNDSM consistently attains superior sample quality and variability.

</details>


### [364] [ADAM Optimization with Adaptive Batch Selection](https://arxiv.org/abs/2512.06795)
*Gyu Yeol Kim,Min-hwan Oh*

Main category: stat.ML

TL;DR: 提出AdamCB，将组合多臂老虎机技术融入Adam优化器，通过自适应采样提升训练效率，相比现有方法有更好的理论保证和实际性能。


<details>
  <summary>Details</summary>
Motivation: Adam优化器在神经网络训练中广泛使用，但传统Adam对所有数据样本平等对待，忽略了不同样本对模型更新的影响差异，导致收敛效率低下。现有基于老虎机的自适应采样方法虽然有所改进，但理论保证有限。

Method: 提出Adam with Combinatorial Bandit Sampling (AdamCB)，将组合老虎机技术集成到Adam优化器中。该方法能够同时利用多个样本的反馈信息，通过自适应采样分布选择对模型更新更有价值的样本。

Result: 理论分析表明AdamCB相比包括先前老虎机变体在内的Adam类方法具有更快的收敛速度。数值实验显示AdamCB在多个任务上持续优于现有方法。

Conclusion: AdamCB通过组合老虎机采样技术有效解决了Adam优化器中样本平等对待的问题，在理论和实践层面都取得了显著改进，为自适应优化器设计提供了新思路。

Abstract: Adam is a widely used optimizer in neural network training due to its adaptive learning rate. However, because different data samples influence model updates to varying degrees, treating them equally can lead to inefficient convergence. To address this, a prior work proposed adapting the sampling distribution using a bandit framework to select samples adaptively. While promising, the bandit-based variant of Adam suffers from limited theoretical guarantees. In this paper, we introduce Adam with Combinatorial Bandit Sampling (AdamCB), which integrates combinatorial bandit techniques into Adam to resolve these issues. AdamCB is able to fully utilize feedback from multiple samples at once, enhancing both theoretical guarantees and practical performance. Our regret analysis shows that AdamCB achieves faster convergence than Adam-based methods including the previous bandit-based variant. Numerical experiments demonstrate that AdamCB consistently outperforms existing methods.

</details>


### [365] [Symmetric Aggregation of Conformity Scores for Efficient Uncertainty Sets](https://arxiv.org/abs/2512.06945)
*Nabil Alami,Jad Zakharia,Souhaib Ben Taieb*

Main category: stat.ML

TL;DR: SACP是一种新颖的对称聚合共形预测方法，通过将多个预测模型的非共形分数转换为e值并使用对称聚合函数进行组合，从而生成更精确的预测集。


<details>
  <summary>Details</summary>
Motivation: 在许多应用中，针对同一任务训练多个预测模型变得越来越普遍。然而，如何聚合这些模型的预测不确定性以产生可靠且高效的量化仍然是一个关键但尚未充分探索的挑战，特别是在共形预测框架内。虽然共形预测方法可以从每个模型生成单独的预测集，但将它们组合成单个更信息丰富的集合仍然是一个难题。

Method: 提出SACP（对称聚合共形预测）方法，该方法聚合来自多个预测器的非共形分数。SACP将这些分数转换为e值，并使用任何对称聚合函数进行组合。这种灵活的设计使得能够采用数据驱动的框架来选择产生更尖锐预测集的聚合策略。

Result: 在多样化数据集上的广泛实验表明，SACP持续提高了效率，并且通常优于最先进的模型聚合基线方法。

Conclusion: SACP为聚合多个预测模型的预测不确定性提供了一个稳健、数据驱动的框架，能够生成更精确的预测集，同时提供了理论见解来证明该方法的有效性和性能。

Abstract: Access to multiple predictive models trained for the same task, whether in regression or classification, is increasingly common in many applications. Aggregating their predictive uncertainties to produce reliable and efficient uncertainty quantification is therefore a critical but still underexplored challenge, especially within the framework of conformal prediction (CP). While CP methods can generate individual prediction sets from each model, combining them into a single, more informative set remains a challenging problem. To address this, we propose SACP (Symmetric Aggregated Conformal Prediction), a novel method that aggregates nonconformity scores from multiple predictors. SACP transforms these scores into e-values and combines them using any symmetric aggregation function. This flexible design enables a robust, data-driven framework for selecting aggregation strategies that yield sharper prediction sets. We also provide theoretical insights that help justify the validity and performance of the SACP approach. Extensive experiments on diverse datasets show that SACP consistently improves efficiency and often outperforms state-of-the-art model aggregation baselines.

</details>


### [366] [PARIS: Pruning Algorithm via the Representer theorem for Imbalanced Scenarios](https://arxiv.org/abs/2512.06950)
*Enrico Camporeale*

Main category: stat.ML

TL;DR: PARIS提出了一种基于表示定理的剪枝算法，通过优化训练集本身来解决不平衡回归问题，无需重新训练即可计算删除单个训练点对验证损失的影响，从而高效剪除无信息或性能下降的样本。


<details>
  <summary>Details</summary>
Motivation: 传统经验风险最小化（ERM）在不平衡回归中偏向数据分布的高频区域，导致对罕见但高影响的"尾部"事件预测性能严重下降。现有方法如损失重加权或合成过采样会引入噪声、扭曲底层分布或增加算法复杂度。

Method: PARIS利用神经网络表示定理计算闭式表示器删除残差，量化删除单个训练点对验证损失的确切影响而无需重新训练。结合高效的Cholesky秩一下降方案，实现快速迭代剪枝，消除无信息或性能下降的样本。

Result: 在真实空间天气案例中，PARIS将训练集减少高达75%，同时保持或改善整体RMSE，优于重加权、合成过采样和提升基准方法。

Conclusion: 表示器引导的数据集剪枝是一种强大、可解释且计算高效的方法，可用于罕见事件回归问题。

Abstract: The challenge of \textbf{imbalanced regression} arises when standard Empirical Risk Minimization (ERM) biases models toward high-frequency regions of the data distribution, causing severe degradation on rare but high-impact ``tail'' events. Existing strategies uch as loss re-weighting or synthetic over-sampling often introduce noise, distort the underlying distribution, or add substantial algorithmic complexity.
  We introduce \textbf{PARIS} (Pruning Algorithm via the Representer theorem for Imbalanced Scenarios), a principled framework that mitigates imbalance by \emph{optimizing the training set itself}. PARIS leverages the representer theorem for neural networks to compute a \textbf{closed-form representer deletion residual}, which quantifies the exact change in validation loss caused by removing a single training point \emph{without retraining}. Combined with an efficient Cholesky rank-one downdating scheme, PARIS performs fast, iterative pruning that eliminates uninformative or performance-degrading samples.
  We use a real-world space weather example, where PARIS reduces the training set by up to 75\% while preserving or improving overall RMSE, outperforming re-weighting, synthetic oversampling, and boosting baselines. Our results demonstrate that representer-guided dataset pruning is a powerful, interpretable, and computationally efficient approach to rare-event regression.

</details>


### [367] [Statistical analysis of Inverse Entropy-regularized Reinforcement Learning](https://arxiv.org/abs/2512.06956)
*Denis Belomestny,Alexey Naumov,Sergey Samsonov*

Main category: stat.ML

TL;DR: 本文提出了一种统计框架，通过结合熵正则化和软贝尔曼残差的最小二乘重构，解决了逆强化学习中奖励函数不唯一的问题，得到了唯一的最小二乘奖励函数。


<details>
  <summary>Details</summary>
Motivation: 经典逆强化学习面临的核心问题是奖励函数的非唯一性：许多不同的奖励函数可以诱导出相同的最优策略，这使得逆问题不适定。需要一种统计框架来消除这种模糊性。

Method: 结合熵正则化和软贝尔曼残差的最小二乘重构，构建唯一的最小二乘奖励函数。将专家演示建模为马尔可夫链，通过惩罚最大似然估计在动作空间的条件分布类中估计策略。使用覆盖数来量化策略类的统计复杂度。

Result: 建立了估计策略与专家策略之间超额Kullback-Leibler散度的高概率界，揭示了平滑（熵正则化）、模型复杂度和样本量之间的相互作用。得到了最小二乘奖励函数的非渐近极小极大最优收敛率。

Conclusion: 该框架解决了逆强化学习中奖励函数不唯一的问题，在行为克隆、逆强化学习和现代统计学习理论之间建立了桥梁，为逆熵正则化强化学习提供了统计理论基础。

Abstract: Inverse reinforcement learning aims to infer the reward function that explains expert behavior observed through trajectories of state--action pairs. A long-standing difficulty in classical IRL is the non-uniqueness of the recovered reward: many reward functions can induce the same optimal policy, rendering the inverse problem ill-posed. In this paper, we develop a statistical framework for Inverse Entropy-regularized Reinforcement Learning that resolves this ambiguity by combining entropy regularization with a least-squares reconstruction of the reward from the soft Bellman residual. This combination yields a unique and well-defined so-called least-squares reward consistent with the expert policy. We model the expert demonstrations as a Markov chain with the invariant distribution defined by an unknown expert policy $π^\star$ and estimate the policy by a penalized maximum-likelihood procedure over a class of conditional distributions on the action space. We establish high-probability bounds for the excess Kullback--Leibler divergence between the estimated policy and the expert policy, accounting for statistical complexity through covering numbers of the policy class. These results lead to non-asymptotic minimax optimal convergence rates for the least-squares reward function, revealing the interplay between smoothing (entropy regularization), model complexity, and sample size. Our analysis bridges the gap between behavior cloning, inverse reinforcement learning, and modern statistical learning theory.

</details>


### [368] [Learning Conditional Independence Differential Graphs From Time-Dependent Data](https://arxiv.org/abs/2512.06960)
*Jitendra K Tugnait*

Main category: stat.ML

TL;DR: 提出了一种基于频率域的惩罚D-trace损失函数方法，用于估计两个时间序列高斯图模型的条件独立图差异，考虑了数据的时间依赖性，显著优于现有的i.i.d.建模方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注独立同分布数据中两个精度矩阵的差异估计，但缺乏对时间依赖数据的处理。本文旨在估计两个时间序列高斯图模型的逆功率谱密度差异，以刻画时间依赖数据中条件依赖关系的变化。

Method: 在频率域中使用惩罚D-trace损失函数方法，结合Wirtinger微积分进行微分图学习。采用凸惩罚（群lasso）和非凸惩罚（log-sum和SCAD群惩罚）函数，并提出了交替方向乘子法（ADMM）来优化目标函数。

Result: 在高维设置下建立了逆功率谱密度在Frobenius范数下收敛到真实值的一致性和图恢复的充分条件。合成数据实验显示，log-sum惩罚的微分时间序列图估计器显著优于lasso基估计器，而后者又显著优于现有的lasso惩罚i.i.d.建模方法（以F1分数为性能指标）。

Conclusion: 提出的方法能够有效估计时间序列数据中条件独立图的差异，考虑了数据的时间依赖性，在合成和真实数据中都表现出优越性能，为时间依赖数据的微分图学习提供了有效工具。

Abstract: Estimation of differences in conditional independence graphs (CIGs) of two time series Gaussian graphical models (TSGGMs) is investigated where the two TSGGMs are known to have similar structure. The TSGGM structure is encoded in the inverse power spectral density (IPSD) of the time series. In several existing works, one is interested in estimating the difference in two precision matrices to characterize underlying changes in conditional dependencies of two sets of data consisting of independent and identically distributed (i.i.d.) observations. In this paper we consider estimation of the difference in two IPSDs to characterize the underlying changes in conditional dependencies of two sets of time-dependent data. Our approach accounts for data time dependencies unlike past work. We analyze a penalized D-trace loss function approach in the frequency domain for differential graph learning, using Wirtinger calculus. We consider both convex (group lasso) and non-convex (log-sum and SCAD group penalties) penalty/regularization functions. An alternating direction method of multipliers (ADMM) algorithm is presented to optimize the objective function. We establish sufficient conditions in a high-dimensional setting for consistency (convergence of the inverse power spectral density to true value in the Frobenius norm) and graph recovery. Both synthetic and real data examples are presented in support of the proposed approaches. In synthetic data examples, our log-sum-penalized differential time-series graph estimator significantly outperformed our lasso based differential time-series graph estimator which, in turn, significantly outperformed an existing lasso-penalized i.i.d. modeling approach, with $F_1$ score as the performance metric.

</details>


### [369] [Exact Synthetic Populations for Scalable Societal and Market Modeling](https://arxiv.org/abs/2512.07306)
*Thierry Petit,Arnault Pachot*

Main category: stat.ML

TL;DR: 提出一个基于约束规划的合成人口生成框架，能够精确复现目标统计数据并确保个体一致性，无需微观数据


<details>
  <summary>Details</summary>
Motivation: 传统数据驱动方法需要从样本推断分布，而本方法旨在直接编码聚合统计数据和结构关系，实现对人口特征的精确控制，同时保护个人隐私数据

Method: 采用约束规划框架，直接编码目标统计数据和结构约束，生成合成人口，确保个体层面的一致性，无需任何微观数据输入

Result: 在官方人口统计数据上验证了方法的有效性，研究了分布偏差对下游分析的影响，证明能够高精度复现目标统计特征

Conclusion: 该方法为生成合成人口提供了新途径，能够在不使用个人数据的情况下支持社会行为建模、市场和政策场景探索，提供可重复的决策级洞察

Abstract: We introduce a constraint-programming framework for generating synthetic populations that reproduce target statistics with high precision while enforcing full individual consistency. Unlike data-driven approaches that infer distributions from samples, our method directly encodes aggregated statistics and structural relations, enabling exact control of demographic profiles without requiring any microdata. We validate the approach on official demographic sources and study the impact of distributional deviations on downstream analyses. This work is conducted within the Pollitics project developed by Emotia, where synthetic populations can be queried through large language models to model societal behaviors, explore market and policy scenarios, and provide reproducible decision-grade insights without personal data.

</details>


### [370] [Machine learning in an expectation-maximisation framework for nowcasting](https://arxiv.org/abs/2512.07335)
*Paul Wilsens,Katrien Antonio,Gerda Claeskens*

Main category: stat.ML

TL;DR: 提出一个基于EM框架的nowcasting方法，使用机器学习技术建模事件发生和报告过程，支持高维协变量，在非线性效应下优于传统GLM方法。


<details>
  <summary>Details</summary>
Motivation: 决策制定常面临信息不完整问题，导致风险估计不足或过高。实践中，信息不完整通常源于报告或观察延迟。需要利用可观测信息学习完整信息（nowcasting），特别是当存在高维协变量和非线性效应时。

Method: 提出基于期望最大化（EM）框架的nowcasting方法，使用神经网络和XGBoost等机器学习技术建模事件发生和报告过程。允许包含事件发生和报告期间的协变量信息以及实体特征。定制化最大化步骤和EM迭代间的信息流以利用预测能力。

Result: 模拟实验显示能有效建模高维协变量下的事件发生和报告过程。在非线性效应存在时，该方法优于使用广义线性模型（GLM）的现有EM-based nowcasting框架。在阿根廷新冠病例报告应用中，XGBoost方法表现最佳。

Conclusion: 提出的EM框架结合机器学习技术能有效处理nowcasting问题，特别是在高维协变量和非线性效应场景下，为决策制定提供更准确的风险估计。

Abstract: Decision making often occurs in the presence of incomplete information, leading to the under- or overestimation of risk. Leveraging the observable information to learn the complete information is called nowcasting. In practice, incomplete information is often a consequence of reporting or observation delays. In this paper, we propose an expectation-maximisation (EM) framework for nowcasting that uses machine learning techniques to model both the occurrence as well as the reporting process of events. We allow for the inclusion of covariate information specific to the occurrence and reporting periods as well as characteristics related to the entity for which events occurred. We demonstrate how the maximisation step and the information flow between EM iterations can be tailored to leverage the predictive power of neural networks and (extreme) gradient boosting machines (XGBoost). With simulation experiments, we show that we can effectively model both the occurrence and reporting of events when dealing with high-dimensional covariate information. In the presence of non-linear effects, we show that our methodology outperforms existing EM-based nowcasting frameworks that use generalised linear models in the maximisation step. Finally, we apply the framework to the reporting of Argentinian Covid-19 cases, where the XGBoost-based approach again is most performant.

</details>


### [371] [High-Dimensional Change Point Detection using Graph Spanning Ratio](https://arxiv.org/abs/2512.07541)
*Youngwen Sun,Katerina Papagiannouli,Vladimir Spokoiny*

Main category: stat.ML

TL;DR: 提出一种基于图的新颖图跨越算法，用于检测低维到高维数据中的变化，适用于欧几里得和图结构数据，控制错误概率，在变化幅度超过最小最大分离率下界时具有高检测能力。


<details>
  <summary>Details</summary>
Motivation: 需要一种通用的变化检测方法，能够处理从低维到高维的数据，适用于欧几里得和图结构数据，且数据分布未知。特别是在在线环境中，需要及时准确地检测变化，即使观测窗口较小。

Method: 基于图的方法论，提出一种新颖的图跨越算法。该方法适用于欧几里得和图结构数据，能够处理未知分布的数据，同时控制错误概率。算法在变化幅度超过最小最大分离率下界时具有理论保证。

Result: 算法在变化幅度超过最小最大分离率下界时达到高检测能力，该下界尺度为√(nd)。在准确率方面优于其他技术，对高斯和非高斯数据都有效。即使在小的观测窗口下也能保持强大的检测能力，特别适合在线环境。

Conclusion: 提出的图跨越算法是一种有效的通用变化检测方法，特别适用于在线环境，能够在小的观测窗口下及时准确地检测变化，对高斯和非高斯数据都表现出色，具有理论保证和实际优势。

Abstract: Inspired by graph-based methodologies, we introduce a novel graph-spanning algorithm designed to identify changes in both offline and online data across low to high dimensions. This versatile approach is applicable to Euclidean and graph-structured data with unknown distributions, while maintaining control over error probabilities. Theoretically, we demonstrate that the algorithm achieves high detection power when the magnitude of the change surpasses the lower bound of the minimax separation rate, which scales on the order of $\sqrt{nd}$. Our method outperforms other techniques in terms of accuracy for both Gaussian and non-Gaussian data. Notably, it maintains strong detection power even with small observation windows, making it particularly effective for online environments where timely and precise change detection is critical.

</details>


### [372] [On Conditional Independence Graph Learning From Multi-Attribute Gaussian Dependent Time Series](https://arxiv.org/abs/2512.07557)
*Jitendra K. Tugnait*

Main category: stat.ML

TL;DR: 提出了一种用于高维多属性时间序列的条件独立图估计方法，采用频域惩罚对数似然函数，包含凸和非凸惩罚项，建立了无需不可表示性条件的理论保证


<details>
  <summary>Details</summary>
Motivation: 现有图估计方法主要针对单属性时间序列（每个节点对应标量时间序列），而实际应用中常遇到多属性数据（每个节点对应向量时间序列），需要开发适用于多属性图模型的理论和方法

Method: 基于频域离散傅里叶变换构建惩罚对数似然目标函数，考虑凸惩罚（稀疏组lasso）和非凸惩罚（log-sum和SCAD组惩罚），在高维设置下进行理论分析

Result: 建立了无需不可表示性条件的充分条件，证明了Frobenius范数下的一致性、非凸惩罚的局部凸性以及图恢复能力，并通过合成和真实数据验证了方法

Conclusion: 提出的多属性图学习方法为高维依赖时间序列的图估计提供了统一的理论框架，在无需严格假设条件下实现了理论保证，并通过实际应用验证了有效性

Abstract: Estimation of the conditional independence graph (CIG) of high-dimensional multivariate Gaussian time series from multi-attribute data is considered. Existing methods for graph estimation for such data are based on single-attribute models where one associates a scalar time series with each node. In multi-attribute graphical models, each node represents a random vector or vector time series. In this paper we provide a unified theoretical analysis of multi-attribute graph learning for dependent time series using a penalized log-likelihood objective function formulated in the frequency domain using the discrete Fourier transform of the time-domain data. We consider both convex (sparse-group lasso) and non-convex (log-sum and SCAD group penalties) penalty/regularization functions. We establish sufficient conditions in a high-dimensional setting for consistency (convergence of the inverse power spectral density to true value in the Frobenius norm), local convexity when using non-convex penalties, and graph recovery. We do not impose any incoherence or irrepresentability condition for our convergence results. We also empirically investigate selection of the tuning parameters based on the Bayesian information criterion, and illustrate our approach using numerical examples utilizing both synthetic and real data.

</details>


### [373] [$φ$-test: Global Feature Selection and Inference for Shapley Additive Explanations](https://arxiv.org/abs/2512.07578)
*Dongseok Kim,Hyoungsun Choi,Mohamed Jismy Aashik Rasool,Gisung Oh*

Main category: stat.ML

TL;DR: φ-test是一种结合Shapley归因和选择性推断的全局特征选择和显著性检验方法，用于黑盒预测模型，提供特征重要性评分和统计推断结果。


<details>
  <summary>Details</summary>
Motivation: 现有黑盒模型的特征重要性解释方法缺乏统计推断框架，无法提供p值和置信区间等传统统计指标，限制了特征选择的可靠性和可解释性。

Method: 基于SHAP指导的特征筛选，通过选择性推断规则拟合线性代理模型，为保留的特征计算Shapley全局评分、代理系数、后选择p值和置信区间。

Result: 在真实表格回归任务中，φ-test能够保留原始模型的大部分预测能力，仅使用少量特征，且特征集在不同重采样和模型架构间保持稳定。

Conclusion: φ-test作为实用的全局解释层，将Shapley重要性总结与经典统计推断联系起来，为黑盒模型提供了统计可靠的特征选择框架。

Abstract: We propose $φ$-test, a global feature-selection and significance procedure for black-box predictors that combines Shapley attributions with selective inference. Given a trained model and an evaluation dataset, $φ$-test performs SHAP-guided screening and fits a linear surrogate on the screened features via a selection rule with a tractable selective-inference form. For each retained feature, it outputs a Shapley-based global score, a surrogate coefficient, and post-selection $p$-values and confidence intervals in a global feature-importance table. Experiments on real tabular regression tasks with tree-based and neural backbones suggest that $φ$-test can retain much of the predictive ability of the original model while using only a few features and producing feature sets that remain fairly stable across resamples and backbone classes. In these settings, $φ$-test acts as a practical global explanation layer linking Shapley-based importance summaries with classical statistical inference.

</details>


### [374] [Physics-Informed Neural Networks for Source Inversion and Parameters Estimation in Atmospheric Dispersion](https://arxiv.org/abs/2512.07755)
*Brenda Anague,Bamdad Hosseini,Issa Karambal,Jean Medard Ngnotchouye*

Main category: stat.ML

TL;DR: 提出基于神经正切核的加权自适应PINNs方法，用于同时估计二维和三维对流-扩散方程中的排放源位置、速度场和扩散系数等未知参数


<details>
  <summary>Details</summary>
Motivation: 在环境监测和大气科学中，从稀疏数据中同时估计排放源位置和多个模型参数（如速度剖面和扩散参数）是一个具有挑战性的任务。传统方法难以处理这种高度不适定的反问题。

Method: 扩展物理信息神经网络（PINNs），提出基于神经正切核的加权自适应方法。将偏微分方程作为约束条件，联合恢复解、源项和未知参数，更有效地利用测量中的有限信息。

Result: 通过多种数值实验（使用不同类型的测量数据模拟实际工程系统），证明该方法在二维和三维对流-扩散方程中成功且鲁棒，对测量噪声具有良好适应性。

Conclusion: 提出的加权自适应PINNs方法能够有效解决高度不适定的源反演和参数估计问题，为环境监测和大气科学中的多参数联合估计提供了新工具。

Abstract: Recent studies have shown the success of deep learning in solving forward and inverse problems in engineering and scientific computing domains, such as physics-informed neural networks (PINNs). In the fields of atmospheric science and environmental monitoring, estimating emission source locations is a central task that further relies on multiple model parameters that dictate velocity profiles and diffusion parameters. Estimating these parameters at the same time as emission sources from scarce data is a difficult task. In this work, we achieve this by leveraging the flexibility and generality of PINNs. We use a weighted adaptive method based on the neural tangent kernels to solve a source inversion problem with parameter estimation on the 2D and 3D advection-diffusion equations with unknown velocity and diffusion coefficients that may vary in space and time. Our proposed weighted adaptive method is presented as an extension of PINNs for forward PDE problems to a highly ill-posed source inversion and parameter estimation problem. The key idea behind our methodology is to attempt the joint recovery of the solution, the sources along with the unknown parameters, thereby using the underlying partial differential equation as a constraint that couples multiple unknown functional parameters, leading to more efficient use of the limited information in the measurements. We present various numerical experiments, using different types of measurements that model practical engineering systems, to show that our proposed method is indeed successful and robust to additional noise in the measurements.

</details>


### [375] [Distribution-informed Online Conformal Prediction](https://arxiv.org/abs/2512.07770)
*Dongjian Hu,Junxi Wu,Shu-Tao Xia,Changliang Zou*

Main category: stat.ML

TL;DR: 提出COP算法，一种在线保形预测方法，通过纳入数据模式来减少预测集的保守性，同时保持覆盖保证


<details>
  <summary>Details</summary>
Motivation: 现有在线保形预测方法在完全对抗环境下处理数据分布漂移时，会产生过于保守的预测集，需要一种能利用数据模式同时保持覆盖保证的方法

Method: COP算法通过非一致性分数的累积分布函数估计，将底层数据模式纳入更新规则，在可预测模式存在时产生更紧的预测集

Result: 建立了覆盖和遗憾的联合界，证明了COP在任意学习率下实现分布无关的有限样本覆盖，当分数独立同分布时可收敛，实验显示COP能获得有效覆盖并构建更短的预测区间

Conclusion: COP算法成功解决了在线保形预测中的保守性问题，通过利用数据模式产生更紧的预测集，同时保持理论上的覆盖保证，在实际应用中表现优于基线方法

Abstract: Conformal prediction provides a pivotal and flexible technique for uncertainty quantification by constructing prediction sets with a predefined coverage rate. Many online conformal prediction methods have been developed to address data distribution shifts in fully adversarial environments, resulting in overly conservative prediction sets. We propose Conformal Optimistic Prediction (COP), an online conformal prediction algorithm incorporating underlying data pattern into the update rule. Through estimated cumulative distribution function of non-conformity scores, COP produces tighter prediction sets when predictable pattern exists, while retaining valid coverage guarantees even when estimates are inaccurate. We establish a joint bound on coverage and regret, which further confirms the validity of our approach. We also prove that COP achieves distribution-free, finite-sample coverage under arbitrary learning rates and can converge when scores are $i.i.d.$. The experimental results also show that COP can achieve valid coverage and construct shorter prediction intervals than other baselines.

</details>
