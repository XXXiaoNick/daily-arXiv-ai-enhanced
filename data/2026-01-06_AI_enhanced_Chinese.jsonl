{"id": "2601.00813", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.00813", "abs": "https://arxiv.org/abs/2601.00813", "authors": ["Longxiang Shao", "Dominik Huesener", "Michael Schluse", "Juergen Rossmann"], "title": "Application of the learning from errors principle in tufting machines", "comment": "Accepted at DTA APAC 2026 (International Conference on Digital Twins and Applications)", "summary": "The principle of learning from errors is pedagogically powerful but often impractical in industrial settings due to risks to safety and equipment. This paper presents an integrated training approach specifically designed for tufting machine operators. It uses hybrid digital twins, augmented reality (AR), and Petri Net-based modelling to apply the learning from errors principle effectively. Operator actions and errors are simulated via experimentable digital twins (EDTs), and the consequences of errors are visualized in AR, enabling safe, experiential learning. A Petri Net model formally represents the process, including typical faults and recovery paths, and is implemented in VEROSIM using SOML++. This hybrid framework provides a scalable foundation for AR-guided training systems that reduce risk and accelerate skill acquisition.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u6570\u5b57\u5b6a\u751f\u3001\u589e\u5f3a\u73b0\u5b9e\u548cPetri\u7f51\u5efa\u6a21\u7684\u96c6\u6210\u57f9\u8bad\u65b9\u6cd5\uff0c\u8ba9\u7c07\u7ed2\u673a\u64cd\u4f5c\u5458\u5728\u5b89\u5168\u73af\u5883\u4e2d\u901a\u8fc7\u6a21\u62df\u9519\u8bef\u8fdb\u884c\u5b66\u4e60", "motivation": "\u5de5\u4e1a\u73af\u5883\u4e2d\"\u4ece\u9519\u8bef\u4e2d\u5b66\u4e60\"\u7684\u6559\u5b66\u539f\u5219\u901a\u5e38\u56e0\u5b89\u5168\u548c\u8bbe\u5907\u98ce\u9669\u800c\u96be\u4ee5\u5b9e\u65bd\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u5e94\u7528\u8fd9\u4e00\u6559\u5b66\u539f\u5219\u53c8\u80fd\u786e\u4fdd\u5b89\u5168\u7684\u57f9\u8bad\u65b9\u6cd5", "method": "\u91c7\u7528\u6df7\u5408\u6570\u5b57\u5b6a\u751f\u3001\u589e\u5f3a\u73b0\u5b9e\u548c\u57fa\u4e8ePetri\u7f51\u7684\u5efa\u6a21\u65b9\u6cd5\u3002\u901a\u8fc7\u53ef\u5b9e\u9a8c\u6570\u5b57\u5b6a\u751f\u6a21\u62df\u64cd\u4f5c\u5458\u884c\u4e3a\u548c\u9519\u8bef\uff0c\u7528AR\u53ef\u89c6\u5316\u9519\u8bef\u540e\u679c\uff0cPetri\u7f51\u6a21\u578b\u5f62\u5f0f\u5316\u8868\u793a\u8fc7\u7a0b\u3001\u5178\u578b\u6545\u969c\u548c\u6062\u590d\u8def\u5f84\uff0c\u4f7f\u7528VEROSIM\u548cSOML++\u5b9e\u73b0", "result": "\u8be5\u6df7\u5408\u6846\u67b6\u4e3aAR\u5f15\u5bfc\u7684\u57f9\u8bad\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u57fa\u7840\uff0c\u80fd\u591f\u964d\u4f4e\u98ce\u9669\u5e76\u52a0\u901f\u6280\u80fd\u83b7\u53d6", "conclusion": "\u63d0\u51fa\u7684\u96c6\u6210\u57f9\u8bad\u65b9\u6cd5\u6210\u529f\u5730\u5c06\"\u4ece\u9519\u8bef\u4e2d\u5b66\u4e60\"\u539f\u5219\u5e94\u7528\u4e8e\u5de5\u4e1a\u73af\u5883\uff0c\u901a\u8fc7\u5b89\u5168\u6a21\u62df\u9519\u8bef\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u4f53\u9a8c\u5f0f\u5b66\u4e60"}}
{"id": "2601.01076", "categories": ["eess.SY", "cs.AI", "cs.LG", "cs.RO", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.01076", "abs": "https://arxiv.org/abs/2601.01076", "authors": ["Devesh Nath", "Haoran Yin", "Glen Chou"], "title": "Scalable Data-Driven Reachability Analysis and Control via Koopman Operators with Conformal Coverage Guarantees", "comment": "Under review, 28 pages, 12 figures", "summary": "We propose a scalable reachability-based framework for probabilistic, data-driven safety verification of unknown nonlinear dynamics. We use Koopman theory with a neural network (NN) lifting function to learn an approximate linear representation of the dynamics and design linear controllers in this space to enable closed-loop tracking of a reference trajectory distribution. Closed-loop reachable sets are efficiently computed in the lifted space and mapped back to the original state space via NN verification tools. To capture model mismatch between the Koopman dynamics and the true system, we apply conformal prediction to produce statistically-valid error bounds that inflate the reachable sets to ensure the true trajectories are contained with a user-specified probability. These bounds generalize across references, enabling reuse without recomputation. Results on high-dimensional MuJoCo tasks (11D Hopper, 28D Swimmer) and 12D quadcopters show improved reachable set coverage rate, computational efficiency, and conservativeness over existing methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u53ef\u8fbe\u6027\u7684\u6982\u7387\u6027\u6570\u636e\u9a71\u52a8\u5b89\u5168\u9a8c\u8bc1\u6846\u67b6\uff0c\u7528\u4e8e\u672a\u77e5\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u7cfb\u7edf\uff0c\u7ed3\u5408Koopman\u7406\u8bba\u3001\u795e\u7ecf\u7f51\u7edc\u548c\u4fdd\u5f62\u9884\u6d4b\u6765\u4fdd\u8bc1\u7edf\u8ba1\u6709\u6548\u7684\u5b89\u5168\u8fb9\u754c\u3002", "motivation": "\u9700\u8981\u4e3a\u672a\u77e5\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u7cfb\u7edf\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u3001\u6982\u7387\u6027\u7684\u5b89\u5168\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u4ee5\u5e94\u5bf9\u9ad8\u7ef4\u673a\u5668\u4eba\u63a7\u5236\u4efb\u52a1\u4e2d\u7684\u5b89\u5168\u6311\u6218\u3002", "method": "1) \u4f7f\u7528Koopman\u7406\u8bba\u548c\u795e\u7ecf\u7f51\u7edc\u63d0\u5347\u51fd\u6570\u5b66\u4e60\u52a8\u529b\u5b66\u7684\u8fd1\u4f3c\u7ebf\u6027\u8868\u793a\uff1b2) \u5728\u63d0\u5347\u7a7a\u95f4\u4e2d\u8bbe\u8ba1\u7ebf\u6027\u63a7\u5236\u5668\u8fdb\u884c\u8f68\u8ff9\u8ddf\u8e2a\uff1b3) \u5728\u63d0\u5347\u7a7a\u95f4\u9ad8\u6548\u8ba1\u7b97\u53ef\u8fbe\u96c6\u5e76\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u9a8c\u8bc1\u5de5\u5177\u6620\u5c04\u56de\u539f\u59cb\u72b6\u6001\u7a7a\u95f4\uff1b4) \u5e94\u7528\u4fdd\u5f62\u9884\u6d4b\u5904\u7406\u6a21\u578b\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u751f\u6210\u7edf\u8ba1\u6709\u6548\u7684\u8bef\u5dee\u8fb9\u754c\u3002", "result": "\u5728\u9ad8\u7ef4MuJoCo\u4efb\u52a1\uff0811D Hopper\u300128D Swimmer\uff09\u548c12D\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u4e0a\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u53ef\u8fbe\u96c6\u8986\u76d6\u7387\u3001\u8ba1\u7b97\u6548\u7387\u548c\u4fdd\u5b88\u6027\u65b9\u9762\u5747\u6709\u6539\u8fdb\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u672a\u77e5\u975e\u7ebf\u6027\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u6982\u7387\u5b89\u5168\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u80fd\u591f\u5904\u7406\u9ad8\u7ef4\u63a7\u5236\u4efb\u52a1\uff0c\u4e14\u8bef\u5dee\u8fb9\u754c\u53ef\u8de8\u53c2\u8003\u8f68\u8ff9\u91cd\u7528\uff0c\u65e0\u9700\u91cd\u65b0\u8ba1\u7b97\u3002"}}
{"id": "2601.01105", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.01105", "abs": "https://arxiv.org/abs/2601.01105", "authors": ["Salim Oyinlola", "Joshua Ajayi", "Gozie Ibekwe"], "title": "Time Series Based CO2 Emission Forecasting and Energy Mix Analysis for Net Zero Transitions: A Multi Country Study", "comment": "Accepted at the Global Journal of Engineering and Technology Advances", "summary": "This study examines long-term CO$_2$ emission trajectories across five major economies: Nigeria, the United States, China, Brazil, and Russia, by integrating national energy-mix characteristics with time-series forecasting models. Annual emissions from 2000-2023 were analyzed alongside energy production data to classify countries into fossil-dependent, transition-phase, or renewable-accelerated profiles. Three forecasting models (ARIMA, SARIMA, and Holt-Winters exponential smoothing) were evaluated using MAE, RMSE, MAPE, and R$^2$ metrics. Results show that Holt-Winters provided the most accurate forecasts for Nigeria, the United States, China, and Brazil, while SARIMA performed best for Russia due to its relatively stable emissions. Long-term projections from 2024 to 2060 indicate divergent decarbonization pathways. Brazil aligns most closely with a low-emission future owing to its renewable-dominant energy system, whereas Nigeria continues on an upward emissions trajectory driven by fossil dependence. The United States and China maintain moderate declines but require accelerated mitigation to reach their respective net-zero commitments. Russia's emissions remain largely flat under current conditions. These findings highlight the strong influence of energy structures on national decarbonization prospects and underscore the need for targeted energy policy reforms to align with global climate objectives.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86\u4e94\u4e2a\u4e3b\u8981\u7ecf\u6d4e\u4f53\uff08\u5c3c\u65e5\u5229\u4e9a\u3001\u7f8e\u56fd\u3001\u4e2d\u56fd\u3001\u5df4\u897f\u3001\u4fc4\u7f57\u65af\uff09\u7684\u957f\u671fCO\u2082\u6392\u653e\u8f68\u8ff9\uff0c\u7ed3\u5408\u80fd\u6e90\u7ed3\u6784\u7279\u5f81\u548c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\uff0c\u53d1\u73b0\u4e0d\u540c\u56fd\u5bb6\u56e0\u80fd\u6e90\u7ed3\u6784\u5dee\u5f02\u5448\u73b0\u4e0d\u540c\u7684\u8131\u78b3\u8def\u5f84\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u7406\u89e3\u4e3b\u8981\u7ecf\u6d4e\u4f53\u5982\u4f55\u57fa\u4e8e\u5176\u80fd\u6e90\u7ed3\u6784\u7279\u5f81\u5b9e\u73b0\u957f\u671fCO\u2082\u51cf\u6392\uff0c\u8bc4\u4f30\u4e0d\u540c\u9884\u6d4b\u6a21\u578b\u5728\u6392\u653e\u8f68\u8ff9\u5206\u6790\u4e2d\u7684\u51c6\u786e\u6027\uff0c\u5e76\u4e3a\u5404\u56fd\u5236\u5b9a\u7b26\u5408\u5168\u7403\u6c14\u5019\u76ee\u6807\u7684\u80fd\u6e90\u653f\u7b56\u63d0\u4f9b\u4f9d\u636e\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\uff1a\u5206\u67902000-2023\u5e74\u5e74\u5ea6\u6392\u653e\u6570\u636e\u548c\u80fd\u6e90\u751f\u4ea7\u6570\u636e\uff1b\u5c06\u56fd\u5bb6\u5206\u7c7b\u4e3a\u5316\u77f3\u4f9d\u8d56\u578b\u3001\u8f6c\u578b\u9636\u6bb5\u6216\u53ef\u518d\u751f\u80fd\u6e90\u52a0\u901f\u578b\uff1b\u8bc4\u4f30\u4e09\u79cd\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\uff08ARIMA\u3001SARIMA\u3001Holt-Winters\u6307\u6570\u5e73\u6ed1\u6cd5\uff09\u7684\u51c6\u786e\u6027\uff1b\u4f7f\u7528MAE\u3001RMSE\u3001MAPE\u548cR\u00b2\u6307\u6807\u8fdb\u884c\u6a21\u578b\u8bc4\u4f30\uff1b\u8fdb\u884c2024-2060\u5e74\u7684\u957f\u671f\u6392\u653e\u9884\u6d4b\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff1aHolt-Winters\u6a21\u578b\u5bf9\u5c3c\u65e5\u5229\u4e9a\u3001\u7f8e\u56fd\u3001\u4e2d\u56fd\u548c\u5df4\u897f\u7684\u9884\u6d4b\u6700\u51c6\u786e\uff0c\u800cSARIMA\u6a21\u578b\u5bf9\u4fc4\u7f57\u65af\u7684\u9884\u6d4b\u6700\u4f73\uff08\u56e0\u5176\u6392\u653e\u76f8\u5bf9\u7a33\u5b9a\uff09\u3002\u957f\u671f\u9884\u6d4b\u8868\u660e\uff1a\u5df4\u897f\u56e0\u53ef\u518d\u751f\u80fd\u6e90\u4e3b\u5bfc\u7684\u80fd\u6e90\u7cfb\u7edf\u6700\u63a5\u8fd1\u4f4e\u6392\u653e\u672a\u6765\uff1b\u5c3c\u65e5\u5229\u4e9a\u56e0\u5316\u77f3\u4f9d\u8d56\u6392\u653e\u6301\u7eed\u4e0a\u5347\uff1b\u7f8e\u56fd\u548c\u4e2d\u56fd\u7684\u6392\u653e\u9002\u5ea6\u4e0b\u964d\u4f46\u9700\u52a0\u901f\u51cf\u6392\u4ee5\u5b9e\u73b0\u51c0\u96f6\u627f\u8bfa\uff1b\u4fc4\u7f57\u65af\u6392\u653e\u57fa\u672c\u4fdd\u6301\u5e73\u7a33\u3002", "conclusion": "\u7ed3\u8bba\u5f3a\u8c03\u80fd\u6e90\u7ed3\u6784\u5bf9\u56fd\u5bb6\u8131\u78b3\u524d\u666f\u6709\u5f3a\u70c8\u5f71\u54cd\uff0c\u9700\u8981\u6709\u9488\u5bf9\u6027\u7684\u80fd\u6e90\u653f\u7b56\u6539\u9769\u4ee5\u7b26\u5408\u5168\u7403\u6c14\u5019\u76ee\u6807\u3002\u4e0d\u540c\u56fd\u5bb6\u9700\u8981\u6839\u636e\u5176\u80fd\u6e90\u7279\u5f81\u91c7\u53d6\u5dee\u5f02\u5316\u7684\u51cf\u6392\u7b56\u7565\u3002"}}
{"id": "2601.01130", "categories": ["eess.SY", "astro-ph.EP", "astro-ph.IM"], "pdf": "https://arxiv.org/pdf/2601.01130", "abs": "https://arxiv.org/abs/2601.01130", "authors": ["Ridma Ganganath", "Simone Servadio", "David Daeyoung Lee"], "title": "Compensating Star-Trackers Misalignments with Adaptive Multi-Model Estimation", "comment": "35 pages, 12 figures. arXiv admin note: substantial text overlap with arXiv:2507.19838", "summary": "This paper presents an adaptive multi-model framework for jointly estimating spacecraft attitude and star-tracker misalignments in GPS-denied deep-space CubeSat missions. A Multiplicative Extended Kalman Filter (MEKF) estimates attitude, angular velocity, and gyro bias, while a Bayesian Multiple-Model Adaptive Estimation (MMAE) layer operates on a discrete grid of body-to-sensor misalignment hypotheses. In the single-misalignment case, the MEKF processes gyroscope measurements and TRIAD-based attitude observations, and the MMAE updates a three-dimensional grid over the misalignment vector. For a dual-misalignment configuration, the same MEKF dynamics are retained, and the MMAE bank is driven directly by stacked line-of-sight measurements from two star trackers, forming a six-dimensional grid over the two misalignment quaternions without augmenting the continuous-state dimension. A novel diversity metric, $\u03a8$, is introduced to trigger adaptive refinement of the misalignment grid around a weighted-mean estimate, thereby preventing premature collapse of the model probabilities and concentrating computation in the most likely region of the parameter space. Monte Carlo simulations show arcsecond-level misalignment estimation and sub-degree attitude errors for both estimation problems, with estimation errors remaining well-bounded, proving robustness and consistency. These results indicate that the proposed MEKF--MMAE architecture enables accurate, autonomous, and computationally efficient in-flight calibration for resource-constrained spacecraft, and establishes dual star-tracker misalignment estimation as a practical option for deep-space CubeSat missions.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u9002\u5e94\u591a\u6a21\u578b\u6846\u67b6\uff0c\u7528\u4e8eGPS\u7f3a\u5931\u7684\u6df1\u7a7a\u7acb\u65b9\u661f\u4efb\u52a1\u4e2d\u8054\u5408\u4f30\u8ba1\u822a\u5929\u5668\u59ff\u6001\u548c\u661f\u654f\u611f\u5668\u5931\u51c6\u89d2\uff0c\u91c7\u7528MEKF\u4f30\u8ba1\u59ff\u6001\u53c2\u6570\uff0cMMAE\u5904\u7406\u5931\u51c6\u89d2\u5047\u8bbe\u7f51\u683c\uff0c\u5f15\u5165\u591a\u6837\u6027\u5ea6\u91cf\u89e6\u53d1\u81ea\u9002\u5e94\u7f51\u683c\u7ec6\u5316\u3002", "motivation": "\u9488\u5bf9\u8d44\u6e90\u53d7\u9650\u7684\u6df1\u7a7a\u7acb\u65b9\u661f\u4efb\u52a1\uff0c\u5728GPS\u7f3a\u5931\u73af\u5883\u4e0b\u9700\u8981\u81ea\u4e3b\u3001\u51c6\u786e\u3001\u8ba1\u7b97\u9ad8\u6548\u7684\u5728\u8f68\u6821\u51c6\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5904\u7406\u661f\u654f\u611f\u5668\u5b89\u88c5\u5931\u51c6\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u53ef\u80fd\u8ba1\u7b97\u590d\u6742\u6216\u7cbe\u5ea6\u4e0d\u8db3\u3002", "method": "\u7ed3\u5408MEKF\uff08\u4f30\u8ba1\u59ff\u6001\u3001\u89d2\u901f\u5ea6\u3001\u9640\u87ba\u504f\u5dee\uff09\u548cMMAE\uff08\u5904\u7406\u5931\u51c6\u89d2\u5047\u8bbe\u7f51\u683c\uff09\u3002\u5355\u5931\u51c6\u89d2\u65f6MEKF\u5904\u7406\u9640\u87ba\u548cTRIAD\u59ff\u6001\u89c2\u6d4b\uff0cMMAE\u66f4\u65b0\u4e09\u7ef4\u5931\u51c6\u5411\u91cf\u7f51\u683c\uff1b\u53cc\u5931\u51c6\u89d2\u65f6\u4fdd\u7559MEKF\u52a8\u6001\uff0cMMAE\u76f4\u63a5\u5904\u7406\u4e24\u4e2a\u661f\u654f\u611f\u5668\u7684\u89c6\u7ebf\u6d4b\u91cf\uff0c\u5f62\u6210\u516d\u7ef4\u7f51\u683c\u3002\u5f15\u5165\u591a\u6837\u6027\u5ea6\u91cf\u03a8\u89e6\u53d1\u81ea\u9002\u5e94\u7f51\u683c\u7ec6\u5316\u3002", "result": "\u8499\u7279\u5361\u6d1b\u4eff\u771f\u663e\u793a\u80fd\u8fbe\u5230\u89d2\u79d2\u7ea7\u5931\u51c6\u89d2\u4f30\u8ba1\u548c\u4e9a\u5ea6\u7ea7\u59ff\u6001\u8bef\u5dee\uff0c\u4f30\u8ba1\u8bef\u5dee\u4fdd\u6301\u826f\u597d\u6709\u754c\uff0c\u8bc1\u660e\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u548c\u4e00\u81f4\u6027\u3002\u53cc\u661f\u654f\u611f\u5668\u5931\u51c6\u89d2\u4f30\u8ba1\u6210\u4e3a\u6df1\u7a7a\u7acb\u65b9\u661f\u4efb\u52a1\u7684\u5b9e\u7528\u9009\u62e9\u3002", "conclusion": "\u63d0\u51fa\u7684MEKF-MMAE\u67b6\u6784\u4e3a\u8d44\u6e90\u53d7\u9650\u822a\u5929\u5668\u63d0\u4f9b\u4e86\u51c6\u786e\u3001\u81ea\u4e3b\u3001\u8ba1\u7b97\u9ad8\u6548\u7684\u5728\u8f68\u6821\u51c6\u80fd\u529b\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6df1\u7a7a\u7acb\u65b9\u661f\u4efb\u52a1\uff0c\u89e3\u51b3\u4e86\u661f\u654f\u611f\u5668\u5931\u51c6\u89d2\u4f30\u8ba1\u7684\u5b9e\u9645\u95ee\u9898\u3002"}}
{"id": "2601.01160", "categories": ["math.OC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01160", "abs": "https://arxiv.org/abs/2601.01160", "authors": ["Boris Prokhorov", "Semyon Chebykin", "Alexander Gasnikov", "Aleksandr Beznosikov"], "title": "Gradient-Free Approaches is a Key to an Efficient Interaction with Markovian Stochasticity", "comment": null, "summary": "This paper deals with stochastic optimization problems involving Markovian noise with a zero-order oracle. We present and analyze a novel derivative-free method for solving such problems in strongly convex smooth and non-smooth settings with both one-point and two-point feedback oracles. Using a randomized batching scheme, we show that when mixing time $\u03c4$ of the underlying noise sequence is less than the dimension of the problem $d$, the convergence estimates of our method do not depend on $\u03c4$. This observation provides an efficient way to interact with Markovian stochasticity: instead of invoking the expensive first-order oracle, one should use the zero-order oracle. Finally, we complement our upper bounds with the corresponding lower bounds. This confirms the optimality of our results.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9488\u5bf9\u9a6c\u5c14\u53ef\u592b\u566a\u58f0\u968f\u673a\u4f18\u5316\u95ee\u9898\u7684\u96f6\u9636\u65b9\u6cd5\uff0c\u5728\u6df7\u5408\u65f6\u95f4\u5c0f\u4e8e\u95ee\u9898\u7ef4\u5ea6\u65f6\uff0c\u6536\u655b\u4f30\u8ba1\u4e0e\u6df7\u5408\u65f6\u95f4\u65e0\u5173\uff0c\u8bc1\u660e\u4e86\u96f6\u9636\u65b9\u6cd5\u4f18\u4e8e\u4e00\u9636\u65b9\u6cd5", "motivation": "\u89e3\u51b3\u9a6c\u5c14\u53ef\u592b\u566a\u58f0\u4e0b\u7684\u968f\u673a\u4f18\u5316\u95ee\u9898\uff0c\u4f20\u7edf\u4e00\u9636\u65b9\u6cd5\u6602\u8d35\uff0c\u63a2\u7d22\u96f6\u9636\u65b9\u6cd5\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u7684\u4f18\u52bf", "method": "\u63d0\u51fa\u65b0\u9896\u7684\u96f6\u9636\u5bfc\u6570\u81ea\u7531\u65b9\u6cd5\uff0c\u91c7\u7528\u968f\u673a\u6279\u5904\u7406\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5f3a\u51f8\u5149\u6ed1\u548c\u975e\u5149\u6ed1\u8bbe\u7f6e\uff0c\u652f\u6301\u5355\u70b9\u548c\u4e24\u70b9\u53cd\u9988", "result": "\u5f53\u566a\u58f0\u5e8f\u5217\u6df7\u5408\u65f6\u95f4\u5c0f\u4e8e\u95ee\u9898\u7ef4\u5ea6\u65f6\uff0c\u65b9\u6cd5\u6536\u655b\u4f30\u8ba1\u4e0e\u6df7\u5408\u65f6\u95f4\u65e0\u5173\uff0c\u8bc1\u660e\u96f6\u9636\u65b9\u6cd5\u4f18\u4e8e\u4e00\u9636\u65b9\u6cd5", "conclusion": "\u5728\u9a6c\u5c14\u53ef\u592b\u566a\u58f0\u73af\u5883\u4e0b\uff0c\u5f53\u6df7\u5408\u65f6\u95f4\u5c0f\u4e8e\u7ef4\u5ea6\u65f6\uff0c\u4f7f\u7528\u96f6\u9636\u65b9\u6cd5\u6bd4\u4e00\u9636\u65b9\u6cd5\u66f4\u9ad8\u6548\uff0c\u5e76\u901a\u8fc7\u4e0b\u754c\u8bc1\u660e\u7ed3\u679c\u6700\u4f18\u6027"}}
{"id": "2601.00814", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00814", "abs": "https://arxiv.org/abs/2601.00814", "authors": ["Abhishek Kumar"], "title": "Semantic Alignment of Multilingual Knowledge Graphs via Contextualized Vector Projections", "comment": null, "summary": "The paper presents our work on cross-lingual ontology alignment system which uses embedding based cosine similarity matching. The ontology entities are made contextually richer by creating descriptions using novel techniques. We use a fine-tuned transformer based multilingual model for generating better embeddings. We use cosine similarity to find positive ontology entities pairs and then apply threshold filtering to retain only highly similar entities. We have evaluated our work on OAEI-2022 multifarm track. We achieve 71% F1 score (78% recall and 65% precision) on the evaluation dataset, 16% increase from best baseline score. This suggests that our proposed alignment pipeline is able to capture the subtle cross-lingual similarities.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5d4c\u5165\u4f59\u5f26\u76f8\u4f3c\u5ea6\u7684\u8de8\u8bed\u8a00\u672c\u4f53\u5bf9\u9f50\u7cfb\u7edf\uff0c\u901a\u8fc7\u521b\u65b0\u63cf\u8ff0\u751f\u6210\u6280\u672f\u4e30\u5bcc\u672c\u4f53\u5b9e\u4f53\u4e0a\u4e0b\u6587\uff0c\u4f7f\u7528\u5fae\u8c03\u7684\u591a\u8bed\u8a00Transformer\u6a21\u578b\u751f\u6210\u66f4\u597d\u5d4c\u5165\uff0c\u5728OAEI-2022\u8bc4\u6d4b\u4e2dF1\u5206\u6570\u8fbe\u523071%\uff0c\u6bd4\u6700\u4f73\u57fa\u7ebf\u63d0\u534716%", "motivation": "\u89e3\u51b3\u8de8\u8bed\u8a00\u672c\u4f53\u5bf9\u9f50\u7684\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u8de8\u8bed\u8a00\u7684\u7ec6\u5fae\u8bed\u4e49\u76f8\u4f3c\u6027\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u6280\u672f\u6765\u63d0\u5347\u5bf9\u9f50\u51c6\u786e\u7387", "method": "1) \u4f7f\u7528\u521b\u65b0\u6280\u672f\u4e3a\u5b9e\u4f53\u751f\u6210\u63cf\u8ff0\u6027\u6587\u672c\u4ee5\u4e30\u5bcc\u4e0a\u4e0b\u6587\uff1b2) \u91c7\u7528\u5fae\u8c03\u7684\u591a\u8bed\u8a00Transformer\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u5d4c\u5165\uff1b3) \u57fa\u4e8e\u4f59\u5f26\u76f8\u4f3c\u5ea6\u5339\u914d\u5b9e\u4f53\u5bf9\uff1b4) \u5e94\u7528\u9608\u503c\u8fc7\u6ee4\u4fdd\u7559\u9ad8\u76f8\u4f3c\u5ea6\u5b9e\u4f53\u5bf9", "result": "\u5728OAEI-2022 multifarm track\u8bc4\u6d4b\u4e2d\uff0cF1\u5206\u6570\u8fbe\u523071%\uff08\u53ec\u56de\u738778%\uff0c\u7cbe\u786e\u738765%\uff09\uff0c\u6bd4\u6700\u4f73\u57fa\u7ebf\u65b9\u6cd5\u63d0\u534716%\uff0c\u8bc1\u660e\u7cfb\u7edf\u80fd\u6709\u6548\u6355\u6349\u8de8\u8bed\u8a00\u76f8\u4f3c\u6027", "conclusion": "\u63d0\u51fa\u7684\u8de8\u8bed\u8a00\u672c\u4f53\u5bf9\u9f50\u6d41\u6c34\u7ebf\u901a\u8fc7\u4e0a\u4e0b\u6587\u4e30\u5bcc\u548c\u5d4c\u5165\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u8bed\u8a00\u672c\u4f53\u5bf9\u9f50\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u57fa\u4e8e\u5d4c\u5165\u7684\u76f8\u4f3c\u5ea6\u5339\u914d\u65b9\u6cd5\u7684\u6709\u6548\u6027"}}
{"id": "2601.00797", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.00797", "abs": "https://arxiv.org/abs/2601.00797", "authors": ["Hugues Draelants"], "title": "The Qualitative Laboratory: Theory Prototyping and Hypothesis Generation with Large Language Models", "comment": "26 pages, 3 tables. Manuscript submitted for peer-reviewed journal publication", "summary": "A central challenge in social science is to generate rich qualitative hypotheses about how diverse social groups might interpret new information. This article introduces and illustrates a novel methodological approach for this purpose: sociological persona simulation using Large Language Models (LLMs), which we frame as a \"qualitative laboratory\". We argue that for this specific task, persona simulation offers a distinct advantage over established methods. By generating naturalistic discourse, it overcomes the lack of discursive depth common in vignette surveys, and by operationalizing complex worldviews through natural language, it bypasses the formalization bottleneck of rule-based agent-based models (ABMs). To demonstrate this potential, we present a protocol where personas derived from a sociological theory of climate reception react to policy messages. The simulation produced nuanced and counter-intuitive hypotheses - such as a conservative persona's rejection of a national security frame - that challenge theoretical assumptions. We conclude that this method, used as part of a \"simulation then validation\" workflow, represents a superior tool for generating deeply textured hypotheses for subsequent empirical testing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u793e\u4f1a\u5b66\u89d2\u8272\u6a21\u62df\u4f5c\u4e3a\"\u5b9a\u6027\u5b9e\u9a8c\u5ba4\"\uff0c\u7528\u4e8e\u751f\u6210\u5173\u4e8e\u4e0d\u540c\u793e\u4f1a\u7fa4\u4f53\u5982\u4f55\u89e3\u8bfb\u65b0\u4fe1\u606f\u7684\u4e30\u5bcc\u5b9a\u6027\u5047\u8bbe\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u793e\u4f1a\u79d1\u5b66\u9762\u4e34\u7684\u6838\u5fc3\u6311\u6218\u662f\u5982\u4f55\u751f\u6210\u5173\u4e8e\u4e0d\u540c\u793e\u4f1a\u7fa4\u4f53\u5982\u4f55\u89e3\u8bfb\u65b0\u4fe1\u606f\u7684\u4e30\u5bcc\u5b9a\u6027\u5047\u8bbe\u3002\u73b0\u6709\u65b9\u6cd5\u5982\u5c0f\u63d2\u66f2\u8c03\u67e5\u7f3a\u4e4f\u8bdd\u8bed\u6df1\u5ea6\uff0c\u57fa\u4e8e\u89c4\u5219\u7684ABM\u6a21\u578b\u5b58\u5728\u5f62\u5f0f\u5316\u74f6\u9888\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u8bba\u6765\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u63d0\u51fa\u793e\u4f1a\u5b66\u89d2\u8272\u6a21\u62df\u65b9\u6cd5\uff0c\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\"\u5b9a\u6027\u5b9e\u9a8c\u5ba4\"\u3002\u5177\u4f53\u534f\u8bae\u5305\u62ec\uff1a\u4ece\u793e\u4f1a\u5b66\u7406\u8bba\u4e2d\u63a8\u5bfc\u89d2\u8272\uff08\u5982\u6c14\u5019\u63a5\u6536\u7406\u8bba\uff09\uff0c\u8ba9\u8fd9\u4e9b\u89d2\u8272\u5bf9\u653f\u7b56\u4fe1\u606f\u505a\u51fa\u53cd\u5e94\uff0c\u751f\u6210\u81ea\u7136\u4e3b\u4e49\u8bdd\u8bed\uff0c\u64cd\u4f5c\u5316\u590d\u6742\u4e16\u754c\u89c2\u3002", "result": "\u6a21\u62df\u4ea7\u751f\u4e86\u7ec6\u81f4\u4e14\u53cd\u76f4\u89c9\u7684\u5047\u8bbe\uff0c\u4f8b\u5982\u4fdd\u5b88\u6d3e\u89d2\u8272\u62d2\u7edd\u56fd\u5bb6\u5b89\u5168\u6846\u67b6\uff0c\u8fd9\u6311\u6218\u4e86\u7406\u8bba\u5047\u8bbe\u3002\u65b9\u6cd5\u5c55\u793a\u4e86\u751f\u6210\u6df1\u5ea6\u7ed3\u6784\u5316\u5047\u8bbe\u7684\u80fd\u529b\uff0c\u53ef\u7528\u4e8e\u540e\u7eed\u5b9e\u8bc1\u68c0\u9a8c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4f5c\u4e3a\"\u6a21\u62df\u540e\u9a8c\u8bc1\"\u5de5\u4f5c\u6d41\u7a0b\u7684\u4e00\u90e8\u5206\uff0c\u4ee3\u8868\u4e86\u751f\u6210\u6df1\u5ea6\u7ed3\u6784\u5316\u5047\u8bbe\u7684\u4f18\u8d8a\u5de5\u5177\uff0c\u4e3a\u540e\u7eed\u5b9e\u8bc1\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u57fa\u7840\uff0c\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u6bd4\u73b0\u6709\u65b9\u6cd5\u5177\u6709\u660e\u663e\u4f18\u52bf\u3002"}}
{"id": "2601.01077", "categories": ["econ.EM"], "pdf": "https://arxiv.org/pdf/2601.01077", "abs": "https://arxiv.org/abs/2601.01077", "authors": ["Takashi Kano"], "title": "Distribution-Matching Posterior Inference for Incomplete Structural Models", "comment": null, "summary": "This paper introduces a Bayesian inference framework for incomplete structural models, termed distribution-matching posterior inference (DMPI). Extending the minimal econometric interpretation (MEI), DMPI constructs a divergence-based quasi-likelihood using the Jensen-Shannon divergence between theoretical and empirical population-moment distributions, based on a Dirichlet-multinomial structure with additive smoothing. The framework accommodates model misspecification and stochastic singularity. Posterior inference is implemented via a sequential Monte Carlo algorithm with Metropolis-Hastings mutation that jointly samples structural parameters and theoretical moment distributions. Monte Carlo experiments using misspecified New Keynesian (NK) models demonstrate that DMPI yields robust inference and improves distribution-matching coherence by probabilistically down-weighting moment distributions inconsistent with the structural model. An empirical application to U.S. data shows that a parsimonious stochastic singular NK model provides a better fit to business-cycle moments than an overparameterized full-rank counterpart.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5206\u5e03\u5339\u914d\u7684\u540e\u9a8c\u63a8\u65ad\u6846\u67b6DMPI\uff0c\u901a\u8fc7Jensen-Shannon\u6563\u5ea6\u6784\u5efa\u51c6\u4f3c\u7136\uff0c\u5904\u7406\u4e0d\u5b8c\u5168\u7ed3\u6784\u6a21\u578b\u4e2d\u7684\u6a21\u578b\u8bef\u8bbe\u548c\u968f\u673a\u5947\u5f02\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u4e0d\u5b8c\u5168\u7ed3\u6784\u6a21\u578b\u7684\u8d1d\u53f6\u65af\u63a8\u65ad\u65b9\u6cd5\u5728\u5904\u7406\u6a21\u578b\u8bef\u8bbe\u548c\u968f\u673a\u5947\u5f02\u6027\u65f6\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u7a33\u5065\u7684\u63a8\u65ad\u6846\u67b6\u3002", "method": "\u57fa\u4e8e\u6700\u5c0f\u7ecf\u6d4e\u89e3\u91ca(MEI)\u6269\u5c55\uff0c\u4f7f\u7528Jensen-Shannon\u6563\u5ea6\u6784\u5efa\u7406\u8bba\u77e9\u5206\u5e03\u4e0e\u7ecf\u9a8c\u77e9\u5206\u5e03\u4e4b\u95f4\u7684\u51c6\u4f3c\u7136\uff0c\u91c7\u7528\u72c4\u5229\u514b\u96f7-\u591a\u9879\u5206\u5e03\u7ed3\u6784\u548c\u52a0\u6027\u5e73\u6ed1\uff0c\u901a\u8fc7\u5e26Metropolis-Hastings\u7a81\u53d8\u7684\u5e8f\u5217\u8499\u7279\u5361\u6d1b\u7b97\u6cd5\u8054\u5408\u91c7\u6837\u7ed3\u6784\u53c2\u6570\u548c\u7406\u8bba\u77e9\u5206\u5e03\u3002", "result": "\u8499\u7279\u5361\u6d1b\u5b9e\u9a8c\u663e\u793aDMPI\u5728\u65b0\u51ef\u6069\u65af\u6a21\u578b\u8bef\u8bbe\u60c5\u51b5\u4e0b\u63d0\u4f9b\u7a33\u5065\u63a8\u65ad\uff0c\u901a\u8fc7\u6982\u7387\u6027\u964d\u6743\u6539\u5584\u5206\u5e03\u5339\u914d\u4e00\u81f4\u6027\uff1b\u5b9e\u8bc1\u5e94\u7528\u8868\u660e\u7b80\u7ea6\u7684\u968f\u673a\u5947\u5f02NK\u6a21\u578b\u6bd4\u8fc7\u5ea6\u53c2\u6570\u5316\u7684\u6ee1\u79e9\u6a21\u578b\u80fd\u66f4\u597d\u62df\u5408\u7f8e\u56fd\u5546\u4e1a\u5468\u671f\u77e9\u3002", "conclusion": "DMPI\u6846\u67b6\u4e3a\u4e0d\u5b8c\u5168\u7ed3\u6784\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8d1d\u53f6\u65af\u63a8\u65ad\u65b9\u6cd5\uff0c\u80fd\u591f\u5904\u7406\u6a21\u578b\u8bef\u8bbe\u548c\u968f\u673a\u5947\u5f02\u6027\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u8bc1\u5e94\u7528\u4e2d\u5747\u8868\u73b0\u51fa\u826f\u597d\u6027\u80fd\u3002"}}
{"id": "2601.00815", "categories": ["q-fin.PR", "math.PR", "q-fin.CP"], "pdf": "https://arxiv.org/pdf/2601.00815", "abs": "https://arxiv.org/abs/2601.00815", "authors": ["Mara Kalicanin Dimitrov", "Marko Dimitrov", "Anatoliy Malyarenko", "Ying Ni"], "title": "Almost-Exact Simulation Scheme for Heston-type Models: Bermudan and American Option Pricing", "comment": "20 pages, 4 figures. Revised version under consideration for publication", "summary": "Recently, an Almost-Exact Simulation (AES) scheme was introduced for the Heston stochastic volatility model and tested for European option pricing. This paper extends this scheme for pricing Bermudan and American options under both Heston and double Heston models. The AES improves Monte Carlo simulation efficiency by using the non-central chi-square distribution for the variance process. We derive the AES scheme for the double Heston model and compare the performance of the AES schemes under both models with the Euler scheme. Our numerical experiments validate the effectiveness of the AES scheme in providing accurate option prices with reduced computational time, highlighting its robustness for both models. In particular, the AES achieves higher accuracy and computational efficiency when the number of simulation steps matches the exercise dates for Bermudan options.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u4e86\u51e0\u4e4e\u7cbe\u786e\u6a21\u62df(AES)\u65b9\u6848\uff0c\u7528\u4e8e\u5728Heston\u548c\u53ccHeston\u6a21\u578b\u4e0b\u5b9a\u4ef7\u767e\u6155\u5927\u548c\u7f8e\u5f0f\u671f\u6743\uff0c\u901a\u8fc7\u4f7f\u7528\u975e\u4e2d\u5fc3\u5361\u65b9\u5206\u5e03\u63d0\u9ad8\u8499\u7279\u5361\u6d1b\u6a21\u62df\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684AES\u65b9\u6848\u4ec5\u7528\u4e8eHeston\u6a21\u578b\u4e0b\u7684\u6b27\u5f0f\u671f\u6743\u5b9a\u4ef7\uff0c\u9700\u8981\u6269\u5c55\u5230\u66f4\u590d\u6742\u7684\u767e\u6155\u5927\u548c\u7f8e\u5f0f\u671f\u6743\u5b9a\u4ef7\uff0c\u5e76\u6269\u5c55\u5230\u53ccHeston\u6a21\u578b\u3002", "method": "\u63a8\u5bfc\u53ccHeston\u6a21\u578b\u7684AES\u65b9\u6848\uff0c\u4f7f\u7528\u975e\u4e2d\u5fc3\u5361\u65b9\u5206\u5e03\u6a21\u62df\u65b9\u5dee\u8fc7\u7a0b\uff0c\u5e76\u4e0e\u6b27\u62c9\u65b9\u6848\u8fdb\u884c\u6027\u80fd\u6bd4\u8f83\u3002\u7279\u522b\u5730\uff0c\u5f53\u6a21\u62df\u6b65\u6570\u4e0e\u767e\u6155\u5927\u671f\u6743\u884c\u6743\u65e5\u671f\u5339\u914d\u65f6\uff0cAES\u80fd\u8fbe\u5230\u66f4\u9ad8\u7cbe\u5ea6\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86AES\u65b9\u6848\u5728\u4e24\u79cd\u6a21\u578b\u4e0b\u90fd\u80fd\u63d0\u4f9b\u51c6\u786e\u7684\u671f\u6743\u4ef7\u683c\u5e76\u51cf\u5c11\u8ba1\u7b97\u65f6\u95f4\uff0c\u663e\u793a\u51fa\u5176\u9c81\u68d2\u6027\u3002AES\u5728\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u6b27\u62c9\u65b9\u6848\u3002", "conclusion": "AES\u65b9\u6848\u5bf9\u4e8eHeston\u548c\u53ccHeston\u6a21\u578b\u4e0b\u7684\u767e\u6155\u5927\u548c\u7f8e\u5f0f\u671f\u6743\u5b9a\u4ef7\u662f\u6709\u6548\u4e14\u9ad8\u6548\u7684\uff0c\u7279\u522b\u662f\u5728\u6a21\u62df\u6b65\u6570\u4e0e\u884c\u6743\u65e5\u671f\u5339\u914d\u65f6\u8868\u73b0\u6700\u4f18\u3002"}}
{"id": "2601.00810", "categories": ["q-fin.PM", "cs.AI", "cs.LG", "econ.GN", "q-fin.ST"], "pdf": "https://arxiv.org/pdf/2601.00810", "abs": "https://arxiv.org/abs/2601.00810", "authors": ["Mohammadhossien Rashidi"], "title": "Can Large Language Models Improve Venture Capital Exit Timing After IPO?", "comment": null, "summary": "Exit timing after an IPO is one of the most consequential decisions for venture capital (VC) investors, yet existing research focuses mainly on describing when VCs exit rather than evaluating whether those choices are economically optimal. Meanwhile, large language models (LLMs) have shown promise in synthesizing complex financial data and textual information but have not been applied to post-IPO exit decisions. This study introduces a framework that uses LLMs to estimate the optimal time for VC exit by analyzing monthly post IPO information financial performance, filings, news, and market signals and recommending whether to sell or continue holding. We compare these LLM generated recommendations with the actual exit dates observed for VCs and compute the return differences between the two strategies. By quantifying gains or losses associated with following the LLM, this study provides evidence on whether AI-driven guidance can improve exit timing and complements traditional hazard and real-options models in venture capital research.", "AI": {"tldr": "\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5206\u6790IPO\u540e\u4fe1\u606f\uff0c\u4e3a\u98ce\u9669\u6295\u8d44\u63d0\u4f9b\u6700\u4f18\u9000\u51fa\u65f6\u673a\u5efa\u8bae\uff0c\u5e76\u4e0e\u5b9e\u9645\u9000\u51fa\u51b3\u7b56\u6bd4\u8f83\u6536\u76ca\u5dee\u5f02", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u63cf\u8ff0VC\u4f55\u65f6\u9000\u51fa\uff0c\u800c\u975e\u8bc4\u4f30\u8fd9\u4e9b\u9009\u62e9\u662f\u5426\u7ecf\u6d4e\u6700\u4f18\uff1b\u540c\u65f6LLMs\u5728\u5206\u6790\u590d\u6742\u91d1\u878d\u6570\u636e\u548c\u6587\u672c\u4fe1\u606f\u65b9\u9762\u6709\u6f5c\u529b\uff0c\u4f46\u5c1a\u672a\u5e94\u7528\u4e8eIPO\u540e\u9000\u51fa\u51b3\u7b56", "method": "\u5f15\u5165LLMs\u6846\u67b6\uff0c\u5206\u6790IPO\u540e\u6708\u5ea6\u4fe1\u606f\uff08\u8d22\u52a1\u8868\u73b0\u3001\u7533\u62a5\u6587\u4ef6\u3001\u65b0\u95fb\u3001\u5e02\u573a\u4fe1\u53f7\uff09\uff0c\u751f\u6210\u6700\u4f18\u9000\u51fa\u65f6\u673a\u5efa\u8bae\uff08\u5356\u51fa\u6216\u7ee7\u7eed\u6301\u6709\uff09\uff0c\u5e76\u4e0eVC\u5b9e\u9645\u9000\u51fa\u65e5\u671f\u6bd4\u8f83", "result": "\u8ba1\u7b97LLM\u5efa\u8bae\u7b56\u7565\u4e0e\u5b9e\u9645\u9000\u51fa\u7b56\u7565\u7684\u6536\u76ca\u5dee\u5f02\uff0c\u91cf\u5316\u9075\u5faaAI\u6307\u5bfc\u7684\u6536\u76ca\u6216\u635f\u5931", "conclusion": "\u7814\u7a76\u8bc4\u4f30AI\u9a71\u52a8\u6307\u5bfc\u662f\u5426\u80fd\u6539\u5584\u9000\u51fa\u65f6\u673a\uff0c\u4e3a\u4f20\u7edf\u98ce\u9669\u6295\u8d44\u7814\u7a76\u4e2d\u7684\u98ce\u9669\u6a21\u578b\u548c\u5b9e\u7269\u671f\u6743\u6a21\u578b\u63d0\u4f9b\u8865\u5145"}}
{"id": "2601.00996", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00996", "abs": "https://arxiv.org/abs/2601.00996", "authors": ["Yongxu Sun", "Michael Saxon", "Ian Yang", "Anna-Maria Gueorguieva", "Aylin Caliskan"], "title": "VEAT Quantifies Implicit Associations in Text-to-Video Generator Sora and Reveals Challenges in Bias Mitigation", "comment": "The International Association for Safe & Ethical AI (IASEAI)", "summary": "Text-to-Video (T2V) generators such as Sora raise concerns about whether generated content reflects societal bias. We extend embedding-association tests from words and images to video by introducing the Video Embedding Association Test (VEAT) and Single-Category VEAT (SC-VEAT). We validate these methods by reproducing the direction and magnitude of associations from widely used baselines, including Implicit Association Test (IAT) scenarios and OASIS image categories. We then quantify race (African American vs. European American) and gender (women vs. men) associations with valence (pleasant vs. unpleasant) across 17 occupations and 7 awards. Sora videos associate European Americans and women more with pleasantness (both d>0.8). Effect sizes correlate with real-world demographic distributions: percent men and White in occupations (r=0.93, r=0.83) and percent male and non-Black among award recipients (r=0.88, r=0.99). Applying explicit debiasing prompts generally reduces effect-size magnitudes, but can backfire: two Black-associated occupations (janitor, postal service) become more Black-associated after debiasing. Together, these results reveal that easily accessible T2V generators can actually amplify representational harms if not rigorously evaluated and responsibly deployed.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u89c6\u9891\u5d4c\u5165\u5173\u8054\u6d4b\u8bd5(VEAT)\u65b9\u6cd5\uff0c\u53d1\u73b0Sora\u7b49\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u5668\u5b58\u5728\u79cd\u65cf\u548c\u6027\u522b\u504f\u89c1\uff0c\u5c06\u6b27\u6d32\u88d4\u7f8e\u56fd\u4eba\u548c\u5973\u6027\u4e0e\u6109\u60a6\u6027\u5173\u8054\u66f4\u5f3a\uff0c\u4e14\u504f\u89c1\u7a0b\u5ea6\u4e0e\u73b0\u5b9e\u4e16\u754c\u4eba\u53e3\u5206\u5e03\u9ad8\u5ea6\u76f8\u5173\u3002", "motivation": "\u968f\u7740Sora\u7b49\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u5668\u7684\u51fa\u73b0\uff0c\u9700\u8981\u8bc4\u4f30\u751f\u6210\u5185\u5bb9\u662f\u5426\u53cd\u6620\u793e\u4f1a\u504f\u89c1\u3002\u73b0\u6709\u504f\u89c1\u6d4b\u8bd5\u4e3b\u8981\u9488\u5bf9\u6587\u5b57\u548c\u56fe\u50cf\uff0c\u7f3a\u4e4f\u5bf9\u89c6\u9891\u5185\u5bb9\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u89c6\u9891\u5d4c\u5165\u5173\u8054\u6d4b\u8bd5(VEAT)\u548c\u5355\u7c7b\u522bVEAT(SC-VEAT)\uff0c\u5c06\u5d4c\u5165\u5173\u8054\u6d4b\u8bd5\u4ece\u6587\u5b57\u548c\u56fe\u50cf\u6269\u5c55\u5230\u89c6\u9891\u3002\u4f7f\u7528Sora\u751f\u6210\u5668\uff0c\u91cf\u531617\u79cd\u804c\u4e1a\u548c7\u79cd\u5956\u9879\u4e2d\u79cd\u65cf\uff08\u975e\u88d4vs\u6b27\u88d4\u7f8e\u56fd\u4eba\uff09\u548c\u6027\u522b\uff08\u5973\u6027vs\u7537\u6027\uff09\u4e0e\u6109\u60a6\u6027\uff08\u6109\u5febvs\u4e0d\u6109\u5feb\uff09\u7684\u5173\u8054\u3002", "result": "Sora\u89c6\u9891\u663e\u793a\uff1a\u6b27\u88d4\u7f8e\u56fd\u4eba\u548c\u5973\u6027\u4e0e\u6109\u60a6\u6027\u5173\u8054\u66f4\u5f3a\uff08\u6548\u5e94\u91cfd>0.8\uff09\u3002\u6548\u5e94\u5927\u5c0f\u4e0e\u73b0\u5b9e\u4eba\u53e3\u5206\u5e03\u9ad8\u5ea6\u76f8\u5173\uff1a\u804c\u4e1a\u4e2d\u7537\u6027\u6bd4\u4f8b\u548c\u767d\u4eba\u6bd4\u4f8b\uff08r=0.93, r=0.83\uff09\uff0c\u5956\u9879\u4e2d\u7537\u6027\u6bd4\u4f8b\u548c\u975e\u9ed1\u4eba\u6bd4\u4f8b\uff08r=0.88, r=0.99\uff09\u3002\u663e\u5f0f\u53bb\u504f\u63d0\u793a\u901a\u5e38\u80fd\u51cf\u5c11\u6548\u5e94\u91cf\uff0c\u4f46\u53ef\u80fd\u9002\u5f97\u5176\u53cd\uff1a\u4e24\u4e2a\u9ed1\u4eba\u5173\u8054\u804c\u4e1a\uff08\u6e05\u6d01\u5de5\u3001\u90ae\u653f\u670d\u52a1\uff09\u5728\u53bb\u504f\u540e\u9ed1\u4eba\u5173\u8054\u66f4\u5f3a\u3002", "conclusion": "\u6613\u83b7\u53d6\u7684\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u5668\u53ef\u80fd\u653e\u5927\u4ee3\u8868\u6027\u4f24\u5bb3\uff0c\u9664\u975e\u7ecf\u8fc7\u4e25\u683c\u8bc4\u4f30\u548c\u8d1f\u8d23\u4efb\u90e8\u7f72\u3002\u9700\u8981\u7cfb\u7edf\u6027\u8bc4\u4f30\u65b9\u6cd5\u6765\u68c0\u6d4b\u548c\u7f13\u89e3\u89c6\u9891\u751f\u6210\u4e2d\u7684\u793e\u4f1a\u504f\u89c1\u3002"}}
{"id": "2601.01029", "categories": ["stat.ML", "cs.AI", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.01029", "abs": "https://arxiv.org/abs/2601.01029", "authors": ["Zeyu Bian", "Max Biggs", "Ruijiang Gao", "Zhengling Qi"], "title": "Beyond Demand Estimation: Consumer Surplus Evaluation via Cumulative Propensity Weights", "comment": "74 pages", "summary": "This paper develops a practical framework for using observational data to audit the consumer surplus effects of AI-driven decisions, specifically in targeted pricing and algorithmic lending. Traditional approaches first estimate demand functions and then integrate to compute consumer surplus, but these methods can be challenging to implement in practice due to model misspecification in parametric demand forms and the large data requirements and slow convergence of flexible nonparametric or machine learning approaches. Instead, we exploit the randomness inherent in modern algorithmic pricing, arising from the need to balance exploration and exploitation, and introduce an estimator that avoids explicit estimation and numerical integration of the demand function. Each observed purchase outcome at a randomized price is an unbiased estimate of demand and by carefully reweighting purchase outcomes using novel cumulative propensity weights (CPW), we are able to reconstruct the integral. Building on this idea, we introduce a doubly robust variant named the augmented cumulative propensity weighting (ACPW) estimator that only requires one of either the demand model or the historical pricing policy distribution to be correctly specified. Furthermore, this approach facilitates the use of flexible machine learning methods for estimating consumer surplus, since it achieves fast convergence rates by incorporating an estimate of demand, even when the machine learning estimate has slower convergence rates. Neither of these estimators is a standard application of off-policy evaluation techniques as the target estimand, consumer surplus, is unobserved. To address fairness, we extend this framework to an inequality-aware surplus measure, allowing regulators and firms to quantify the profit-equity trade-off. Finally, we validate our methods through comprehensive numerical studies.", "AI": {"tldr": "\u63d0\u51fa\u4f7f\u7528\u89c2\u6d4b\u6570\u636e\u5ba1\u8ba1AI\u51b3\u7b56\u6d88\u8d39\u8005\u5269\u4f59\u5f71\u54cd\u7684\u5b9e\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u968f\u673a\u4ef7\u683c\u548c\u7d2f\u79ef\u503e\u5411\u6743\u91cd\u907f\u514d\u4f20\u7edf\u9700\u6c42\u51fd\u6570\u4f30\u8ba1\u7684\u56f0\u96be", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u901a\u8fc7\u4f30\u8ba1\u9700\u6c42\u51fd\u6570\u8ba1\u7b97\u6d88\u8d39\u8005\u5269\u4f59\u5b58\u5728\u5b9e\u8df5\u56f0\u96be\uff1a\u53c2\u6570\u6a21\u578b\u6613\u9519\u8bbe\uff0c\u975e\u53c2\u6570\u65b9\u6cd5\u6570\u636e\u9700\u6c42\u5927\u4e14\u6536\u655b\u6162\u3002\u9700\u8981\u66f4\u5b9e\u7528\u7684\u6846\u67b6\u6765\u5ba1\u8ba1AI\u5b9a\u4ef7\u548c\u7b97\u6cd5\u8d37\u6b3e\u7684\u6d88\u8d39\u8005\u5269\u4f59\u5f71\u54cd", "method": "\u5229\u7528\u7b97\u6cd5\u5b9a\u4ef7\u56fa\u6709\u7684\u63a2\u7d22-\u5229\u7528\u6743\u8861\u4ea7\u751f\u7684\u968f\u673a\u6027\uff0c\u901a\u8fc7\u7d2f\u79ef\u503e\u5411\u6743\u91cd(CPW)\u91cd\u65b0\u52a0\u6743\u8d2d\u4e70\u7ed3\u679c\u6765\u91cd\u5efa\u79ef\u5206\uff0c\u907f\u514d\u663e\u5f0f\u4f30\u8ba1\u9700\u6c42\u51fd\u6570\u3002\u63d0\u51fa\u589e\u5f3a\u578b\u7d2f\u79ef\u503e\u5411\u6743\u91cd(ACPW)\u4f30\u8ba1\u5668\uff0c\u53ea\u9700\u9700\u6c42\u6a21\u578b\u6216\u5386\u53f2\u5b9a\u4ef7\u7b56\u7565\u5206\u5e03\u4e4b\u4e00\u6b63\u786e\u8bbe\u5b9a\u5373\u53ef", "result": "\u65b9\u6cd5\u80fd\u591f\u4f7f\u7528\u7075\u6d3b\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u4f30\u8ba1\u6d88\u8d39\u8005\u5269\u4f59\uff0c\u5373\u4f7f\u673a\u5668\u5b66\u4e60\u4f30\u8ba1\u6536\u655b\u8f83\u6162\u4e5f\u80fd\u83b7\u5f97\u5feb\u901f\u6536\u655b\u7387\u3002\u6846\u67b6\u53ef\u6269\u5c55\u5230\u4e0d\u5e73\u7b49\u611f\u77e5\u7684\u5269\u4f59\u5ea6\u91cf\uff0c\u91cf\u5316\u5229\u6da6-\u516c\u5e73\u6743\u8861", "conclusion": "\u5f00\u53d1\u4e86\u5b9e\u7528\u7684\u6d88\u8d39\u8005\u5269\u4f59\u5ba1\u8ba1\u6846\u67b6\uff0c\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u5c40\u9650\uff0c\u652f\u6301\u7075\u6d3b\u673a\u5668\u5b66\u4e60\u5e94\u7528\uff0c\u5e76\u6269\u5c55\u5230\u516c\u5e73\u6027\u8003\u91cf\uff0c\u901a\u8fc7\u6570\u503c\u7814\u7a76\u9a8c\u8bc1\u4e86\u6709\u6548\u6027"}}
{"id": "2601.00831", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00831", "abs": "https://arxiv.org/abs/2601.00831", "authors": ["Uday Kumar Nidadala", "Venkata Bhumika Guthi"], "title": "Horizon Reduction as Information Loss in Offline Reinforcement Learning", "comment": "13 pages, 3 figures", "summary": "Horizon reduction is a common design strategy in offline reinforcement learning (RL), used to mitigate long-horizon credit assignment, improve stability, and enable scalable learning through truncated rollouts, windowed training, or hierarchical decomposition (Levine et al., 2020; Prudencio et al., 2023; Park et al., 2025). Despite recent empirical evidence that horizon reduction can improve scaling on challenging offline RL benchmarks, its theoretical implications remain underdeveloped (Park et al., 2025). In this paper, we show that horizon reduction can induce fundamental and irrecoverable information loss in offline RL. We formalize horizon reduction as learning from fixed-length trajectory segments and prove that, under this paradigm and any learning interface restricted to fixed-length trajectory segments, optimal policies may be statistically indistinguishable from suboptimal ones even with infinite data and perfect function approximation. Through a set of minimal counterexample Markov decision processes (MDPs), we identify three distinct structural failure modes: (i) prefix indistinguishability leading to identifiability failure, (ii) objective misspecification induced by truncated returns, and (iii) offline dataset support and representation aliasing. Our results establish necessary conditions under which horizon reduction can be safe and highlight intrinsic limitations that cannot be overcome by algorithmic improvements alone, complementing algorithmic work on conservative objectives and distribution shift that addresses a different axis of offline RL difficulty (Fujimoto et al., 2019; Kumar et al., 2020; Gulcehre et al., 2020).", "AI": {"tldr": "\u8bba\u6587\u8bc1\u660e\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u89c6\u91ce\u7f29\u51cf\uff08horizon reduction\uff09\u53ef\u80fd\u5bfc\u81f4\u4e0d\u53ef\u6062\u590d\u7684\u4fe1\u606f\u635f\u5931\uff0c\u5373\u4f7f\u6709\u65e0\u9650\u6570\u636e\u548c\u5b8c\u7f8e\u51fd\u6570\u903c\u8fd1\uff0c\u6700\u4f18\u7b56\u7565\u4e5f\u53ef\u80fd\u4e0e\u6b21\u4f18\u7b56\u7565\u5728\u7edf\u8ba1\u4e0a\u65e0\u6cd5\u533a\u5206\u3002", "motivation": "\u5c3d\u7ba1\u7ecf\u9a8c\u8bc1\u636e\u8868\u660e\u89c6\u91ce\u7f29\u51cf\u53ef\u4ee5\u6539\u5584\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u6269\u5c55\u6027\uff0c\u4f46\u5176\u7406\u8bba\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u53d1\u5c55\u3002\u672c\u6587\u65e8\u5728\u63ed\u793a\u89c6\u91ce\u7f29\u51cf\u53ef\u80fd\u5bfc\u81f4\u7684\u57fa\u672c\u4fe1\u606f\u635f\u5931\u95ee\u9898\u3002", "method": "\u5c06\u89c6\u91ce\u7f29\u51cf\u5f62\u5f0f\u5316\u4e3a\u4ece\u56fa\u5b9a\u957f\u5ea6\u8f68\u8ff9\u7247\u6bb5\u5b66\u4e60\uff0c\u5e76\u901a\u8fc7\u6700\u5c0f\u53cd\u4f8b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDPs\uff09\u8bc6\u522b\u4e09\u79cd\u7ed3\u6784\u5931\u6548\u6a21\u5f0f\uff1a\u524d\u7f00\u4e0d\u53ef\u533a\u5206\u6027\u3001\u622a\u65ad\u56de\u62a5\u5bfc\u81f4\u7684\u76ee\u6807\u9519\u8bef\u6307\u5b9a\u3001\u4ee5\u53ca\u79bb\u7ebf\u6570\u636e\u96c6\u652f\u6301\u548c\u8868\u793a\u6df7\u53e0\u3002", "result": "\u8bc1\u660e\u5728\u56fa\u5b9a\u957f\u5ea6\u8f68\u8ff9\u7247\u6bb5\u7684\u5b66\u4e60\u8303\u5f0f\u4e0b\uff0c\u6700\u4f18\u7b56\u7565\u53ef\u80fd\u4e0e\u6b21\u4f18\u7b56\u7565\u5728\u7edf\u8ba1\u4e0a\u65e0\u6cd5\u533a\u5206\uff0c\u5373\u4f7f\u6709\u65e0\u9650\u6570\u636e\u548c\u5b8c\u7f8e\u51fd\u6570\u903c\u8fd1\u3002\u5efa\u7acb\u4e86\u89c6\u91ce\u7f29\u51cf\u5b89\u5168\u5e94\u7528\u7684\u5fc5\u8981\u6761\u4ef6\u3002", "conclusion": "\u89c6\u91ce\u7f29\u51cf\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u5b58\u5728\u56fa\u6709\u5c40\u9650\u6027\uff0c\u8fd9\u4e9b\u9650\u5236\u65e0\u6cd5\u4ec5\u901a\u8fc7\u7b97\u6cd5\u6539\u8fdb\u514b\u670d\u3002\u7814\u7a76\u7ed3\u679c\u8865\u5145\u4e86\u5173\u4e8e\u4fdd\u5b88\u76ee\u6807\u548c\u5206\u5e03\u504f\u79fb\u7684\u7b97\u6cd5\u5de5\u4f5c\uff0c\u4e3a\u5b89\u5168\u4f7f\u7528\u89c6\u91ce\u7f29\u51cf\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u3002"}}
{"id": "2601.01157", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.01157", "abs": "https://arxiv.org/abs/2601.01157", "authors": ["Davide Carecci", "Laurent Dewasme", "Alessio La Bella", "Gianni Ferretti", "Alain Vande Wouwer"], "title": "Tube-based robust nonlinear model predictive control of anaerobic co-digestion", "comment": "This paper has been accepted for presentation at the IEEE 64th Conference on Decision and Control (CDC), 2025. In press in the proceedings of the conference", "summary": "To match the growing demand for bio-methane production, anaerobic digesters need to embrace the co-digestion of different feedstocks; in addition, to improve the techno-economic performance, an optimal and time-varying adaptation of the input diet is required. These operation modes constitute a very hard challenge for the limited instrumentation and control equipment typically installed aboard full-scale plants. A model-based predictive approach may be able to handle such control problem, but the identification of reliable predictive models is limited by the low information content typical of the data available from full-scale plants' operations, which entail high parametric uncertainty. In this work, the application of a tube-based robust nonlinear model predictive control (NMPC) is proposed to regulate bio-methane production over a period of diet change in time, while warranting safe operation and dealing with uncertainties. In view of its upcoming validation on a true small pilot-scale plant, the NMPC capabilities are assessed via numerical simulations designed to resemble as much as possible the experimental setup, along with some practical final considerations.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7ba1\u9053\u7684\u9c81\u68d2\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08NMPC\uff09\u6765\u8c03\u8282\u751f\u7269\u7532\u70f7\u751f\u4ea7\uff0c\u5904\u7406\u9972\u6599\u53d8\u5316\u671f\u95f4\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u786e\u4fdd\u5b89\u5168\u8fd0\u884c", "motivation": "\u4e3a\u6ee1\u8db3\u751f\u7269\u7532\u70f7\u751f\u4ea7\u9700\u6c42\uff0c\u538c\u6c27\u6d88\u5316\u5668\u9700\u8981\u5904\u7406\u591a\u79cd\u9972\u6599\u5171\u6d88\u5316\uff0c\u4e14\u9700\u8981\u968f\u65f6\u95f4\u4f18\u5316\u8c03\u6574\u9972\u6599\u914d\u65b9\u3002\u7136\u800c\uff0c\u5b9e\u9645\u5de5\u5382\u901a\u5e38\u8bbe\u5907\u6709\u9650\uff0c\u6570\u636e\u4fe1\u606f\u542b\u91cf\u4f4e\uff0c\u6a21\u578b\u8bc6\u522b\u56f0\u96be\uff0c\u5b58\u5728\u9ad8\u53c2\u6570\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u7ba1\u9053\u7684\u9c81\u68d2\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08NMPC\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u6570\u503c\u6a21\u62df\u8bc4\u4f30\u5176\u5728\u9972\u6599\u53d8\u5316\u671f\u95f4\u8c03\u8282\u751f\u7269\u7532\u70f7\u751f\u4ea7\u7684\u80fd\u529b\uff0c\u6a21\u62df\u8bbe\u8ba1\u5c3d\u53ef\u80fd\u63a5\u8fd1\u5b9e\u9a8c\u8bbe\u7f6e\u3002", "result": "\u901a\u8fc7\u6570\u503c\u6a21\u62df\u8bc4\u4f30\u4e86NMPC\u5728\u9972\u6599\u53d8\u5316\u671f\u95f4\u8c03\u8282\u751f\u7269\u7532\u70f7\u751f\u4ea7\u7684\u80fd\u529b\uff0c\u4e3a\u5373\u5c06\u5728\u771f\u5b9e\u5c0f\u578b\u4e2d\u8bd5\u5de5\u5382\u4e0a\u7684\u9a8c\u8bc1\u505a\u51c6\u5907\u3002", "conclusion": "\u57fa\u4e8e\u7ba1\u9053\u7684\u9c81\u68d2NMPC\u80fd\u591f\u5904\u7406\u751f\u7269\u7532\u70f7\u751f\u4ea7\u4e2d\u7684\u9972\u6599\u53d8\u5316\u63a7\u5236\u95ee\u9898\uff0c\u5e94\u5bf9\u4e0d\u786e\u5b9a\u6027\u5e76\u4fdd\u8bc1\u5b89\u5168\u8fd0\u884c\uff0c\u4e3a\u5b9e\u9645\u5de5\u5382\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01175", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2601.01175", "abs": "https://arxiv.org/abs/2601.01175", "authors": ["Zhongyuan Cao", "Kaustav Das", "Nicolas Langren\u00e9", "Mathieu Lauri\u00e8re"], "title": "Scalable Method for Mean Field Control with Kernel Interactions via Random Fourier Features", "comment": null, "summary": "We develop a scalable algorithm for mean field control problems with kernel interactions by combining particle system simulations with random Fourier feature approximations. The method replaces the quadratic-cost kernel evaluations by linear-time estimates, enabling efficient stochastic gradient descent for training feedback controls in large populations. We provide theoretical complexity bounds and demonstrate through crowd motion and flocking examples that the approach preserves control performance while substantially reducing computational cost. The results indicate that random feature approximations offer an effective and practical tool for high dimensional and large scale mean field control.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u7c92\u5b50\u7cfb\u7edf\u6a21\u62df\u4e0e\u968f\u673a\u5085\u91cc\u53f6\u7279\u5f81\u903c\u8fd1\u7684\u53ef\u6269\u5c55\u5747\u503c\u573a\u63a7\u5236\u7b97\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c", "motivation": "\u89e3\u51b3\u5177\u6709\u6838\u4ea4\u4e92\u7684\u5747\u503c\u573a\u63a7\u5236\u95ee\u9898\u5728\u5927\u89c4\u6a21\u7fa4\u4f53\u4e2d\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8", "method": "\u7ed3\u5408\u7c92\u5b50\u7cfb\u7edf\u6a21\u62df\u4e0e\u968f\u673a\u5085\u91cc\u53f6\u7279\u5f81\u903c\u8fd1\uff0c\u7528\u7ebf\u6027\u65f6\u95f4\u4f30\u8ba1\u66ff\u4ee3\u4e8c\u6b21\u6210\u672c\u6838\u8bc4\u4f30\uff0c\u5b9e\u73b0\u9ad8\u6548\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u8bad\u7ec3\u53cd\u9988\u63a7\u5236", "result": "\u7406\u8bba\u590d\u6742\u5ea6\u5206\u6790\u663e\u793a\u8ba1\u7b97\u6548\u7387\u63d0\u5347\uff0c\u5728\u4eba\u7fa4\u8fd0\u52a8\u548c\u96c6\u7fa4\u884c\u4e3a\u793a\u4f8b\u4e2d\u4fdd\u6301\u63a7\u5236\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u6210\u672c", "conclusion": "\u968f\u673a\u7279\u5f81\u903c\u8fd1\u4e3a\u9ad8\u7ef4\u5927\u89c4\u6a21\u5747\u503c\u573a\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u6548\u5b9e\u7528\u7684\u5de5\u5177"}}
{"id": "2601.00816", "categories": ["cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00816", "abs": "https://arxiv.org/abs/2601.00816", "authors": ["Ismail Ahmad Abdullah"], "title": "MathLedger: A Verifiable Learning Substrate with Ledger-Attested Feedback", "comment": "14 pages, 1 figure, 2 tables, 2 appendices with full proofs. Documents v0.9.4-pilot-audit-hardened audit surface with fail-closed governance, canonical JSON hashing, and artifact classification. Phase I infrastructure validation; no capability claims", "summary": "Contemporary AI systems achieve extraordinary performance yet remain opaque and non-verifiable, creating a crisis of trust for safety-critical deployment. We introduce MathLedger, a substrate for verifiable machine cognition that integrates formal verification, cryptographic attestation, and learning dynamics into a single epistemic loop. The system implements Reflexive Formal Learning (RFL), a symbolic analogue of gradient descent where updates are driven by verifier outcomes rather than statistical loss.\n  Phase I experiments validate the measurement and governance substrate under controlled conditions. CAL-EXP-3 validates measurement infrastructure (Delta p computation, variance tracking); separate stress tests confirm fail-closed governance triggers correctly under out-of-bounds conditions. No convergence or capability claims are made. The contribution is infrastructural: a working prototype of ledger-attested learning that enables auditability at scale.\n  Keywords: verifiable learning, formal verification, cryptographic attestation, reflexive feedback, fail-closed governance", "AI": {"tldr": "MathLedger\u662f\u4e00\u4e2a\u53ef\u9a8c\u8bc1\u673a\u5668\u8ba4\u77e5\u7684\u57fa\u7840\u8bbe\u65bd\uff0c\u901a\u8fc7\u5f62\u5f0f\u5316\u9a8c\u8bc1\u3001\u5bc6\u7801\u5b66\u8bc1\u660e\u548c\u5b66\u4e60\u52a8\u6001\u96c6\u6210\u5230\u5355\u4e00\u8ba4\u77e5\u5faa\u73af\u4e2d\uff0c\u5b9e\u73b0\u53ef\u5ba1\u8ba1\u7684\u673a\u5668\u5b66\u4e60", "motivation": "\u5f53\u524dAI\u7cfb\u7edf\u867d\u7136\u6027\u80fd\u5353\u8d8a\u4f46\u4e0d\u900f\u660e\u4e14\u4e0d\u53ef\u9a8c\u8bc1\uff0c\u5728\u5b89\u5168\u5173\u952e\u90e8\u7f72\u4e2d\u9762\u4e34\u4fe1\u4efb\u5371\u673a\uff0c\u9700\u8981\u5efa\u7acb\u53ef\u9a8c\u8bc1\u7684\u673a\u5668\u5b66\u4e60\u7cfb\u7edf", "method": "\u91c7\u7528\u53cd\u5c04\u5f62\u5f0f\u5316\u5b66\u4e60\uff08RFL\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u7b26\u53f7\u5316\u7684\u68af\u5ea6\u4e0b\u964d\u65b9\u6cd5\uff0c\u901a\u8fc7\u9a8c\u8bc1\u5668\u7ed3\u679c\u800c\u975e\u7edf\u8ba1\u635f\u5931\u6765\u9a71\u52a8\u66f4\u65b0\uff1b\u7cfb\u7edf\u96c6\u6210\u5f62\u5f0f\u5316\u9a8c\u8bc1\u3001\u5bc6\u7801\u5b66\u8bc1\u660e\u548c\u5b66\u4e60\u52a8\u6001", "result": "\u7b2c\u4e00\u9636\u6bb5\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6d4b\u91cf\u548c\u6cbb\u7406\u57fa\u7840\u8bbe\u65bd\uff1aCAL-EXP-3\u9a8c\u8bc1\u4e86\u6d4b\u91cf\u57fa\u7840\u8bbe\u65bd\uff08Delta p\u8ba1\u7b97\u3001\u65b9\u5dee\u8ddf\u8e2a\uff09\uff1b\u538b\u529b\u6d4b\u8bd5\u786e\u8ba4\u4e86\u5728\u8d85\u51fa\u8fb9\u754c\u6761\u4ef6\u4e0b\u6545\u969c\u5173\u95ed\u6cbb\u7406\u673a\u5236\u6b63\u786e\u89e6\u53d1\uff1b\u5efa\u7acb\u4e86\u53ef\u6269\u5c55\u5ba1\u8ba1\u7684\u5de5\u4f5c\u539f\u578b", "conclusion": "MathLedger\u63d0\u4f9b\u4e86\u4e00\u4e2a\u57fa\u7840\u8bbe\u65bd\u5c42\u9762\u7684\u8d21\u732e\uff1a\u4e00\u4e2a\u5de5\u4f5c\u7684\u8d26\u672c\u8bc1\u660e\u5b66\u4e60\u539f\u578b\uff0c\u80fd\u591f\u5b9e\u73b0\u5927\u89c4\u6a21\u7684\u53ef\u5ba1\u8ba1\u6027\uff0c\u4e3a\u89e3\u51b3AI\u7cfb\u7edf\u7684\u900f\u660e\u5ea6\u548c\u53ef\u9a8c\u8bc1\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u6280\u672f\u57fa\u7840"}}
{"id": "2601.00938", "categories": ["cs.CL", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.00938", "abs": "https://arxiv.org/abs/2601.00938", "authors": ["Faruk Alpay", "Bugra Kilictas"], "title": "Rate-Distortion Analysis of Compressed Query Delegation with Low-Rank Riemannian Updates", "comment": "9 pages", "summary": "Bounded-context agents fail when intermediate reasoning exceeds an effective working-memory budget. We study compressed query delegation (CQD): (i) compress a high-dimensional latent reasoning state into a low-rank tensor query, (ii) delegate the minimal query to an external oracle, and (iii) update the latent state via Riemannian optimization on fixed-rank manifolds. We give a math-first formulation: CQD is a constrained stochastic program with a query-budget functional and an oracle modeled as a noisy operator. We connect CQD to classical rate-distortion and information bottleneck principles, showing that spectral hard-thresholding is optimal for a natural constrained quadratic distortion problem, and we derive convergence guarantees for Riemannian stochastic approximation under bounded oracle noise and smoothness assumptions. Empirically, we report (A) a 2,500-item bounded-context reasoning suite (BBH-derived tasks plus curated paradox instances) comparing CQD against chain-of-thought baselines under fixed compute and context; and (B) a human \"cognitive mirror\" benchmark (N=200) measuring epistemic gain and semantic drift across modern oracles.", "AI": {"tldr": "\u538b\u7f29\u67e5\u8be2\u59d4\u6258(CQD)\u901a\u8fc7\u5c06\u9ad8\u7ef4\u63a8\u7406\u72b6\u6001\u538b\u7f29\u4e3a\u4f4e\u79e9\u5f20\u91cf\u67e5\u8be2\uff0c\u59d4\u6258\u7ed9\u5916\u90e8oracle\uff0c\u518d\u901a\u8fc7\u9ece\u66fc\u4f18\u5316\u66f4\u65b0\u72b6\u6001\uff0c\u89e3\u51b3\u6709\u754c\u4e0a\u4e0b\u6587\u4ee3\u7406\u7684\u5de5\u4f5c\u8bb0\u5fc6\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u6709\u754c\u4e0a\u4e0b\u6587\u4ee3\u7406\u5728\u4e2d\u95f4\u63a8\u7406\u8d85\u8fc7\u6709\u6548\u5de5\u4f5c\u8bb0\u5fc6\u9884\u7b97\u65f6\u4f1a\u5931\u8d25\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u538b\u7f29\u63a8\u7406\u72b6\u6001\u5e76\u59d4\u6258\u7ed9\u5916\u90e8oracle\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u8d28\u91cf\u3002", "method": "1) \u5c06\u9ad8\u7ef4\u6f5c\u5728\u63a8\u7406\u72b6\u6001\u538b\u7f29\u4e3a\u4f4e\u79e9\u5f20\u91cf\u67e5\u8be2\uff1b2) \u5c06\u6700\u5c0f\u5316\u67e5\u8be2\u59d4\u6258\u7ed9\u5916\u90e8oracle\uff1b3) \u5728\u56fa\u5b9a\u79e9\u6d41\u5f62\u4e0a\u901a\u8fc7\u9ece\u66fc\u4f18\u5316\u66f4\u65b0\u6f5c\u5728\u72b6\u6001\u3002\u5efa\u7acb\u4e86CQD\u4f5c\u4e3a\u5e26\u67e5\u8be2\u9884\u7b97\u529f\u80fd\u7684\u7ea6\u675f\u968f\u673a\u89c4\u5212\u6570\u5b66\u6a21\u578b\u3002", "result": "\u7406\u8bba\u8bc1\u660e\uff1a\u8c31\u786c\u9608\u503c\u5bf9\u4e8e\u7ea6\u675f\u4e8c\u6b21\u5931\u771f\u95ee\u9898\u662f\u6700\u4f18\u7684\uff1b\u5728\u6709\u754coracle\u566a\u58f0\u548c\u5e73\u6ed1\u6027\u5047\u8bbe\u4e0b\uff0c\u9ece\u66fc\u968f\u673a\u8fd1\u4f3c\u5177\u6709\u6536\u655b\u4fdd\u8bc1\u3002\u5b9e\u8bc1\uff1a\u57282,500\u9879\u6709\u754c\u4e0a\u4e0b\u6587\u63a8\u7406\u4efb\u52a1\u4e0a\u4f18\u4e8e\u601d\u7ef4\u94fe\u57fa\u7ebf\uff1b\u4eba\u7c7b\u8ba4\u77e5\u955c\u50cf\u57fa\u51c6(N=200)\u6d4b\u91cf\u4e86\u8ba4\u77e5\u589e\u76ca\u548c\u8bed\u4e49\u6f02\u79fb\u3002", "conclusion": "CQD\u63d0\u4f9b\u4e86\u4e00\u79cd\u6570\u5b66\u4e25\u8c28\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u6709\u754c\u4e0a\u4e0b\u6587\u4ee3\u7406\u7684\u5de5\u4f5c\u8bb0\u5fc6\u9650\u5236\u95ee\u9898\uff0c\u901a\u8fc7\u538b\u7f29\u67e5\u8be2\u59d4\u6258\u548c\u9ece\u66fc\u4f18\u5316\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u63a8\u7406\u6269\u5c55\uff0c\u5e76\u5728\u7406\u8bba\u548c\u5b9e\u8bc1\u4e0a\u90fd\u53d6\u5f97\u4e86\u826f\u597d\u7ed3\u679c\u3002"}}
{"id": "2601.01149", "categories": ["econ.EM"], "pdf": "https://arxiv.org/pdf/2601.01149", "abs": "https://arxiv.org/abs/2601.01149", "authors": ["Johannes Cordier"], "title": "Optimizing Patient Placement in Normal Care Units: An Instrumental Causal Forest Approach Minimizing Mortality", "comment": null, "summary": "Normal care units (NCU) placement affects health outcomes. NCUs in a hospital have different specialisations. There are patients that can potentially stay in multiple different NCUs. On a given day the NCUs are on different utilisation levels, which also affects health outcomes. Our approach uses instrumental variable causal forests, with emergency admission as an instrument, to estimate how the effect of NCU placement varies across patients and utilisation levels. The results show a clear trade-off between specialisation and utilization. Based on these findings, we design a minimax regret placement policy, using frequentist, Balke-Pearl and Manski bounds, that lowers mortality without capacity expansion. The policy reallocates patients according to their individualized average treatment effects, showing that data-driven patient placement can improve outcomes by using existing resources more efficiently.", "AI": {"tldr": "\u4f7f\u7528\u5de5\u5177\u53d8\u91cf\u56e0\u679c\u68ee\u6797\u5206\u6790NCU\u5b89\u7f6e\u5bf9\u5065\u5eb7\u7ed3\u679c\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u4e13\u4e1a\u5316\u4e0e\u5229\u7528\u7387\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u8bbe\u8ba1\u6700\u5c0f\u5316\u540e\u6094\u5b89\u7f6e\u653f\u7b56\u964d\u4f4e\u6b7b\u4ea1\u7387", "motivation": "\u533b\u9662\u6b63\u5e38\u62a4\u7406\u5355\u5143(NCU)\u7684\u5b89\u7f6e\u5f71\u54cd\u5065\u5eb7\u7ed3\u679c\uff0c\u4e0d\u540cNCU\u6709\u4e0d\u540c\u4e13\u4e1a\u7279\u957f\uff0c\u60a3\u8005\u53ef\u80fd\u9002\u5408\u591a\u4e2aNCU\uff0c\u4e14NCU\u5229\u7528\u7387\u6c34\u5e73\u4e5f\u5f71\u54cd\u7ed3\u679c\u3002\u9700\u8981\u7406\u89e3NCU\u5b89\u7f6e\u6548\u679c\u5982\u4f55\u968f\u60a3\u8005\u7279\u5f81\u548c\u5229\u7528\u7387\u53d8\u5316\uff0c\u4ee5\u4f18\u5316\u60a3\u8005\u5206\u914d", "method": "\u4f7f\u7528\u5de5\u5177\u53d8\u91cf\u56e0\u679c\u68ee\u6797\u65b9\u6cd5\uff0c\u4ee5\u6025\u8bca\u5165\u9662\u4f5c\u4e3a\u5de5\u5177\u53d8\u91cf\uff0c\u4f30\u8ba1NCU\u5b89\u7f6e\u6548\u679c\u5728\u60a3\u8005\u548c\u5229\u7528\u7387\u6c34\u5e73\u4e0a\u7684\u5f02\u8d28\u6027\u3002\u57fa\u4e8e\u7ed3\u679c\u8bbe\u8ba1\u6700\u5c0f\u5316\u540e\u6094\u5b89\u7f6e\u653f\u7b56\uff0c\u4f7f\u7528\u9891\u7387\u4e3b\u4e49\u3001Balke-Pearl\u548cManski\u754c\u9650", "result": "\u7ed3\u679c\u663e\u793a\u4e13\u4e1a\u5316\u4e0e\u5229\u7528\u7387\u4e4b\u95f4\u5b58\u5728\u660e\u663e\u6743\u8861\u5173\u7cfb\u3002\u8bbe\u8ba1\u7684\u5b89\u7f6e\u653f\u7b56\u901a\u8fc7\u6839\u636e\u4e2a\u4f53\u5316\u5e73\u5747\u5904\u7406\u6548\u5e94\u91cd\u65b0\u5206\u914d\u60a3\u8005\uff0c\u80fd\u591f\u5728\u4e0d\u589e\u52a0\u5bb9\u91cf\u7684\u60c5\u51b5\u4e0b\u964d\u4f4e\u6b7b\u4ea1\u7387", "conclusion": "\u6570\u636e\u9a71\u52a8\u7684\u60a3\u8005\u5b89\u7f6e\u53ef\u4ee5\u901a\u8fc7\u66f4\u6709\u6548\u5730\u5229\u7528\u73b0\u6709\u8d44\u6e90\u6539\u5584\u5065\u5eb7\u7ed3\u679c\uff0c\u6700\u5c0f\u5316\u540e\u6094\u5b89\u7f6e\u653f\u7b56\u5c55\u793a\u4e86\u5728\u4e0d\u6269\u5f20\u5bb9\u91cf\u7684\u60c5\u51b5\u4e0b\u4f18\u5316\u60a3\u8005\u5206\u914d\u7684\u53ef\u884c\u6027"}}
{"id": "2601.01783", "categories": ["econ.EM", "q-fin.CP", "q-fin.RM"], "pdf": "https://arxiv.org/pdf/2601.01783", "abs": "https://arxiv.org/abs/2601.01783", "authors": ["Haibo Wang", "Jun Huang", "Lutfu S Sua", "Jaime Ortiz", "Jinshyang Roan", "Bahram Alidaee"], "title": "Dynamic Risk in the U.S. Banking System: An Analysis of Sentiment, Policy Shocks, and Spillover Effects", "comment": null, "summary": "The 2023 U.S. banking crisis propagated not through direct financial linkages but through a high-frequency, information-based contagion channel. This paper moves beyond exploration analysis to test the \"too-similar-to-fail\" hypothesis, arguing that risk spillovers were driven by perceived similarities in bank business models under acute interest rate pressure. Employing a Time-Varying Parameter Vector Autoregression (TVP-VAR) model with 30-day rolling windows, a method uniquely suited for capturing the rapid network shifts inherent in a panic, we analyze daily stock returns for the four failed institutions and a systematically selected peer group of surviving banks vulnerable to the same risks from March 18, 2022, to March 15, 2023. Our results provide strong evidence for this contagion channel: total system connectedness surged dramatically during the crisis peak, and we identify SIVB, FRC, and WAL as primary net transmitters of risk while their perceived peers became significant net receivers, a key dynamic indicator of systemic vulnerability that cannot be captured by asset-by-asset analysis. We further demonstrate that these spillovers were significantly amplified by market sentiment (as measured by the VIX) and economic policy uncertainty (EPU). By providing a clear conceptual framework and robust empirical validation, our findings confirm the persistence of systemic risks within the banking network and highlight the importance of real-time monitoring in strengthening financial stability.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u65f6\u53d8\u53c2\u6570VAR\u6a21\u578b\u53d1\u73b0\uff0c2023\u5e74\u7f8e\u56fd\u94f6\u884c\u4e1a\u5371\u673a\u4e3b\u8981\u901a\u8fc7\u4fe1\u606f\u4f20\u67d3\u6e20\u9053\u4f20\u64ad\uff0c\u800c\u975e\u76f4\u63a5\u91d1\u878d\u5173\u8054\uff0c\u9a8c\u8bc1\u4e86\"\u592a\u76f8\u4f3c\u800c\u65e0\u6cd5\u751f\u5b58\"\u5047\u8bf4\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u8d85\u8d8a\u63a2\u7d22\u6027\u5206\u6790\uff0c\u68c0\u9a8c\"\u592a\u76f8\u4f3c\u800c\u65e0\u6cd5\u751f\u5b58\"\u5047\u8bf4\uff0c\u5373\u94f6\u884c\u5371\u673a\u901a\u8fc7\u4e1a\u52a1\u6a21\u5f0f\u76f8\u4f3c\u6027\u5728\u5229\u7387\u538b\u529b\u4e0b\u4f20\u64ad\uff0c\u800c\u975e\u4f20\u7edf\u91d1\u878d\u5173\u8054\u6e20\u9053\u3002", "method": "\u4f7f\u7528\u65f6\u53d8\u53c2\u6570\u5411\u91cf\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u91c7\u752830\u5929\u6eda\u52a8\u7a97\u53e3\u5206\u67902022\u5e743\u670818\u65e5\u81f32023\u5e743\u670815\u65e5\u671f\u95f4\u56db\u5bb6\u5012\u95ed\u94f6\u884c\u548c\u4e00\u7ec4\u5e78\u5b58\u540c\u884c\u94f6\u884c\u7684\u6bcf\u65e5\u80a1\u7968\u6536\u76ca\u7387\u3002", "result": "\u53d1\u73b0\u7cfb\u7edf\u8fde\u901a\u6027\u5728\u5371\u673a\u9ad8\u5cf0\u671f\u6025\u5267\u4e0a\u5347\uff0cSIVB\u3001FRC\u548cWAL\u662f\u4e3b\u8981\u98ce\u9669\u51c0\u4f20\u64ad\u8005\uff0c\u5176\u76f8\u4f3c\u540c\u884c\u6210\u4e3a\u663e\u8457\u98ce\u9669\u51c0\u63a5\u6536\u8005\uff1b\u5e02\u573a\u60c5\u7eea\u548c\u653f\u7b56\u4e0d\u786e\u5b9a\u6027\u663e\u8457\u653e\u5927\u4e86\u98ce\u9669\u6ea2\u51fa\u6548\u5e94\u3002", "conclusion": "\u7814\u7a76\u8bc1\u5b9e\u4e86\u94f6\u884c\u7f51\u7edc\u4e2d\u7cfb\u7edf\u6027\u98ce\u9669\u7684\u6301\u7eed\u6027\uff0c\u5f3a\u8c03\u4e86\u5b9e\u65f6\u76d1\u63a7\u5bf9\u52a0\u5f3a\u91d1\u878d\u7a33\u5b9a\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u4fe1\u606f\u4f20\u67d3\u6e20\u9053\u63d0\u4f9b\u4e86\u6982\u5ff5\u6846\u67b6\u548c\u5b9e\u8bc1\u9a8c\u8bc1\u3002"}}
{"id": "2601.01101", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2601.01101", "abs": "https://arxiv.org/abs/2601.01101", "authors": ["Apurva Kulkarni", "Chandrashekar Ramanathan"], "title": "An Agentic Software Framework for Data Governance under DPDP", "comment": null, "summary": "Despite the rise of data-driven software systems in the modern digital landscape, data governance under a legal framework remains a critical challenge. In India, the Digital Personal Data Protection (DPDP) Act mandates rigorous data privacy and compliance requirements, necessitating software frameworks that are both ethical and regulation-aware. From a software development perspective, traditional compliance tools often rely on hard-coded rules and static configurations, making them inflexible to dynamic policy updates or evolving legal contexts. Additionally, their monolithic architectures obscure decision-making processes, creating black-box behavior in critical governance workflows. Developing responsible AI software demands transparency, traceability, and adaptive enforcement mechanisms that make ethical decisions explainable. To address this challenge, a novel agentic framework is introduced to embed compliance logic directly into software agents that govern and adapt data policies. In this paper, the implementation focuses on the DPDP Act. The framework integrates KYU Agent and Compliance Agent for this purpose. KYU (Know-YourUser) Agent supports semantic understanding, user trustworthiness modelling and Compliance Agent uses data sensitivity reasoning within a goal-driven, agentic pipeline. The proposed framework, built using an open-sourced agentic framework and has been evaluated across ten diverse domains, including healthcare, education, and e-commerce. Its effectiveness under DPDP, measured via an Anonymization Score, demonstrates scalable, compliant data governance through masking, pseudonymization, and generalization strategies tailored to domain-specific needs. The proposed framework delivers scalable, transparent, and compliant data governance through collaborative agents, dynamic policy enforcement, and domain-aware anonymization.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u667a\u80fd\u4f53\u6846\u67b6\u7684\u5370\u5ea6DPDP\u6cd5\u6848\u5408\u89c4\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7KYU Agent\u548cCompliance Agent\u5b9e\u73b0\u52a8\u6001\u3001\u53ef\u89e3\u91ca\u7684\u6570\u636e\u6cbb\u7406", "motivation": "\u5370\u5ea6DPDP\u6cd5\u6848\u8981\u6c42\u4e25\u683c\u7684\u6570\u636e\u9690\u79c1\u5408\u89c4\uff0c\u4f20\u7edf\u5408\u89c4\u5de5\u5177\u57fa\u4e8e\u786c\u7f16\u7801\u89c4\u5219\uff0c\u7f3a\u4e4f\u7075\u6d3b\u6027\u3001\u900f\u660e\u5ea6\uff0c\u96be\u4ee5\u9002\u5e94\u52a8\u6001\u653f\u7b56\u53d8\u5316\uff0c\u9700\u8981\u900f\u660e\u3001\u53ef\u8ffd\u6eaf\u3001\u81ea\u9002\u5e94\u7684\u5408\u89c4\u673a\u5236", "method": "\u5f15\u5165\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u96c6\u6210KYU Agent\uff08\u8bed\u4e49\u7406\u89e3\u3001\u7528\u6237\u53ef\u4fe1\u5ea6\u5efa\u6a21\uff09\u548cCompliance Agent\uff08\u6570\u636e\u654f\u611f\u6027\u63a8\u7406\uff09\uff0c\u57fa\u4e8e\u5f00\u6e90\u667a\u80fd\u4f53\u6846\u67b6\u6784\u5efa\uff0c\u91c7\u7528\u76ee\u6807\u9a71\u52a8\u7684\u667a\u80fd\u4f53\u7ba1\u9053", "result": "\u5728\u533b\u7597\u3001\u6559\u80b2\u3001\u7535\u5546\u7b4910\u4e2a\u9886\u57df\u8bc4\u4f30\uff0c\u901a\u8fc7\u533f\u540d\u5316\u8bc4\u5206\u8861\u91cfDPDP\u5408\u89c4\u6548\u679c\uff0c\u5c55\u793a\u53ef\u6269\u5c55\u7684\u5408\u89c4\u6570\u636e\u6cbb\u7406\uff0c\u652f\u6301\u63a9\u7801\u3001\u5047\u540d\u5316\u3001\u6cdb\u5316\u7b49\u7b56\u7565", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u534f\u4f5c\u667a\u80fd\u4f53\u3001\u52a8\u6001\u7b56\u7565\u6267\u884c\u548c\u9886\u57df\u611f\u77e5\u533f\u540d\u5316\uff0c\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u3001\u900f\u660e\u3001\u5408\u89c4\u7684\u6570\u636e\u6cbb\u7406\uff0c\u4e3a\u8d1f\u8d23\u4efbAI\u8f6f\u4ef6\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.01055", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01055", "abs": "https://arxiv.org/abs/2601.01055", "authors": ["Ernest Fokou\u00e9"], "title": "Fibonacci-Driven Recursive Ensembles: Algorithms, Convergence, and Learning Dynamics", "comment": "19 pages", "summary": "This paper develops the algorithmic and dynamical foundations of recursive ensemble learning driven by Fibonacci-type update flows. In contrast with classical boosting  Freund and Schapire (1997); Friedman (2001), where the ensemble evolves through first-order additive updates, we study second-order recursive architectures in which each predictor depends on its two immediate predecessors. These Fibonacci flows induce a learning dynamic with memory, allowing ensembles to integrate past structure while adapting to new residual information. We introduce a general family of recursive weight-update algorithms encompassing Fibonacci, tribonacci, and higher-order recursions, together with continuous-time limits that yield systems of differential equations governing ensemble evolution. We establish global convergence conditions, spectral stability criteria, and non-asymptotic generalization bounds under Rademacher Bartlett and Mendelson (2002) and algorithmic stability analyses. The resulting theory unifies recursive ensembles, structured weighting, and dynamical systems viewpoints in statistical learning. Experiments with kernel ridge regression Rasmussen and Williams (2006), spline smoothers Wahba (1990), and random Fourier feature models Rahimi and Recht (2007) demonstrate that recursive flows consistently improve approximation and generalization beyond static weighting. These results complete the trilogy begun in Papers I and II: from Fibonacci weighting, through geometric weighting theory, to fully dynamical recursive ensemble learning systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u6590\u6ce2\u90a3\u5951\u578b\u66f4\u65b0\u6d41\u7684\u9012\u5f52\u96c6\u6210\u5b66\u4e60\u7b97\u6cd5\u4e0e\u52a8\u529b\u5b66\u57fa\u7840\uff0c\u76f8\u6bd4\u4f20\u7edf\u4e00\u9636\u63d0\u5347\u65b9\u6cd5\uff0c\u91c7\u7528\u4e8c\u9636\u9012\u5f52\u67b6\u6784\uff0c\u6bcf\u4e2a\u9884\u6d4b\u5668\u4f9d\u8d56\u524d\u4e24\u4e2a\u9884\u6d4b\u5668\uff0c\u5f62\u6210\u5177\u6709\u8bb0\u5fc6\u7684\u5b66\u4e60\u52a8\u6001\u3002", "motivation": "\u4f20\u7edf\u63d0\u5347\u65b9\u6cd5\uff08\u5982AdaBoost\uff09\u4f7f\u7528\u4e00\u9636\u52a0\u6cd5\u66f4\u65b0\uff0c\u7f3a\u4e4f\u8bb0\u5fc6\u673a\u5236\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u5177\u6709\u8bb0\u5fc6\u80fd\u529b\u7684\u9012\u5f52\u96c6\u6210\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6590\u6ce2\u90a3\u5951\u578b\u66f4\u65b0\u6d41\u6574\u5408\u5386\u53f2\u7ed3\u6784\u5e76\u9002\u5e94\u65b0\u6b8b\u5dee\u4fe1\u606f\uff0c\u63d0\u5347\u96c6\u6210\u5b66\u4e60\u7684\u8868\u8fbe\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u9012\u5f52\u6743\u91cd\u66f4\u65b0\u7b97\u6cd5\u5bb6\u65cf\uff0c\u6db5\u76d6\u6590\u6ce2\u90a3\u5951\u3001\u4e09\u6ce2\u90a3\u5951\u53ca\u9ad8\u9636\u9012\u5f52\uff1b\u5efa\u7acb\u8fde\u7eed\u65f6\u95f4\u6781\u9650\u5f97\u5230\u63a7\u5236\u96c6\u6210\u6f14\u5316\u7684\u5fae\u5206\u65b9\u7a0b\u7cfb\u7edf\uff1b\u63d0\u4f9b\u5168\u5c40\u6536\u655b\u6761\u4ef6\u3001\u8c31\u7a33\u5b9a\u6027\u51c6\u5219\u548c\u975e\u6e10\u8fd1\u6cdb\u5316\u754c\u5206\u6790\u3002", "result": "\u9012\u5f52\u6d41\u5728\u6838\u5cad\u56de\u5f52\u3001\u6837\u6761\u5e73\u6ed1\u5668\u548c\u968f\u673a\u5085\u91cc\u53f6\u7279\u5f81\u6a21\u578b\u4e2d\u4e00\u81f4\u6539\u8fdb\u8fd1\u4f3c\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u8d85\u8d8a\u9759\u6001\u52a0\u6743\u65b9\u6cd5\uff1b\u5efa\u7acb\u4e86\u9012\u5f52\u96c6\u6210\u3001\u7ed3\u6784\u5316\u52a0\u6743\u548c\u52a8\u529b\u7cfb\u7edf\u89c6\u89d2\u7684\u7edf\u4e00\u7406\u8bba\u6846\u67b6\u3002", "conclusion": "\u8be5\u7814\u7a76\u5b8c\u6210\u4e86\u4ece\u6590\u6ce2\u90a3\u5951\u52a0\u6743\u3001\u51e0\u4f55\u52a0\u6743\u7406\u8bba\u5230\u5b8c\u5168\u52a8\u6001\u9012\u5f52\u96c6\u6210\u5b66\u4e60\u7cfb\u7edf\u7684\u4e09\u90e8\u66f2\uff0c\u4e3a\u7edf\u8ba1\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u9012\u5f52\u96c6\u6210\u5b66\u4e60\u8303\u5f0f\uff0c\u7edf\u4e00\u4e86\u591a\u4e2a\u7406\u8bba\u89c6\u89d2\u3002"}}
{"id": "2601.00832", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.00832", "abs": "https://arxiv.org/abs/2601.00832", "authors": ["Israk Hasan Jone", "D. M. Rafiun Bin Masud", "Promit Sarker", "Sayed Fuad Al Labib", "Nazmul Islam", "Farhad Billah"], "title": "ShrimpXNet: A Transfer Learning Framework for Shrimp Disease Classification with Augmented Regularization, Adversarial Training, and Explainable AI", "comment": "8 Page, fugure 11", "summary": "Shrimp is one of the most widely consumed aquatic species globally, valued for both its nutritional content and economic importance. Shrimp farming represents a significant source of income in many regions; however, like other forms of aquaculture, it is severely impacted by disease outbreaks. These diseases pose a major challenge to sustainable shrimp production. To address this issue, automated disease classification methods can offer timely and accurate detection. This research proposes a deep learning-based approach for the automated classification of shrimp diseases. A dataset comprising 1,149 images across four disease classes was utilized. Six pretrained deep learning models, ResNet50, EfficientNet, DenseNet201, MobileNet, ConvNeXt-Tiny, and Xception were deployed and evaluated for performance. The images background was removed, followed by standardized preprocessing through the Keras image pipeline. Fast Gradient Sign Method (FGSM) was used for enhancing the model robustness through adversarial training. While advanced augmentation strategies, including CutMix and MixUp, were implemented to mitigate overfitting and improve generalization. To support interpretability, and to visualize regions of model attention, post-hoc explanation methods such as Grad-CAM, Grad-CAM++, and XGrad-CAM were applied. Exploratory results demonstrated that ConvNeXt-Tiny achieved the highest performance, attaining a 96.88% accuracy on the test dataset. After 1000 iterations, the 99% confidence interval for the model is [0.953,0.971].", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u867e\u75c5\u81ea\u52a8\u5206\u7c7b\u65b9\u6cd5\uff0c\u4f7f\u75286\u79cd\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u5305\u542b1149\u5f20\u56fe\u50cf\u76844\u7c7b\u75be\u75c5\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0cConvNeXt-Tiny\u6a21\u578b\u53d6\u5f97\u6700\u4f73\u6027\u80fd\uff0896.88%\u51c6\u786e\u7387\uff09\u3002", "motivation": "\u867e\u517b\u6b96\u662f\u5168\u7403\u91cd\u8981\u7684\u6c34\u4ea7\u517b\u6b96\u4ea7\u4e1a\uff0c\u4f46\u75be\u75c5\u7206\u53d1\u4e25\u91cd\u5a01\u80c1\u5176\u53ef\u6301\u7eed\u53d1\u5c55\u3002\u4f20\u7edf\u75be\u75c5\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u65f6\u6548\u6027\u548c\u51c6\u786e\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u8981\u81ea\u52a8\u5316\u5206\u7c7b\u65b9\u6cd5\u63d0\u4f9b\u53ca\u65f6\u51c6\u786e\u7684\u75be\u75c5\u68c0\u6d4b\u3002", "method": "\u4f7f\u7528\u5305\u542b1149\u5f20\u56fe\u50cf\u76844\u7c7b\u75be\u75c5\u6570\u636e\u96c6\uff0c\u90e8\u7f72ResNet50\u3001EfficientNet\u3001DenseNet201\u3001MobileNet\u3001ConvNeXt-Tiny\u548cXception\u516d\u79cd\u9884\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002\u91c7\u7528\u80cc\u666f\u53bb\u9664\u548cKeras\u56fe\u50cf\u7ba1\u9053\u6807\u51c6\u5316\u9884\u5904\u7406\uff0c\u4f7f\u7528FGSM\u8fdb\u884c\u5bf9\u6297\u8bad\u7ec3\u589e\u5f3a\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u5e94\u7528CutMix\u548cMixUp\u6570\u636e\u589e\u5f3a\u7b56\u7565\u9632\u6b62\u8fc7\u62df\u5408\uff0c\u5e76\u91c7\u7528Grad-CAM\u3001Grad-CAM++\u548cXGrad-CAM\u7b49\u540e\u89e3\u91ca\u65b9\u6cd5\u8fdb\u884c\u53ef\u89c6\u5316\u5206\u6790\u3002", "result": "ConvNeXt-Tiny\u6a21\u578b\u5728\u6d4b\u8bd5\u96c6\u4e0a\u53d6\u5f97\u6700\u9ad8\u6027\u80fd\uff0c\u8fbe\u523096.88%\u7684\u51c6\u786e\u7387\u3002\u7ecf\u8fc71000\u6b21\u8fed\u4ee3\u540e\uff0c\u6a21\u578b99%\u7684\u7f6e\u4fe1\u533a\u95f4\u4e3a[0.953,0.971]\uff0c\u663e\u793a\u51fa\u826f\u597d\u7684\u5206\u7c7b\u6027\u80fd\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u80fd\u591f\u6709\u6548\u5b9e\u73b0\u867e\u75c5\u7684\u81ea\u52a8\u5206\u7c7b\uff0cConvNeXt-Tiny\u5728\u8be5\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\u3002\u8be5\u65b9\u6cd5\u4e3a\u867e\u517b\u6b96\u4e1a\u7684\u75be\u75c5\u76d1\u6d4b\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u75be\u75c5\u68c0\u6d4b\u7684\u53ca\u65f6\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2601.01250", "categories": ["q-fin.MF", "math.OC", "math.PR", "q-fin.PR"], "pdf": "https://arxiv.org/pdf/2601.01250", "abs": "https://arxiv.org/abs/2601.01250", "authors": ["Miryana Grigorova", "James Wheeldon"], "title": "European Options in Market Models with Multiple Defaults: the BSDE approach", "comment": null, "summary": "We study non-linear Backward Stochastic Differential Equations (BSDEs) driven by a Brownian motion and p default martingales. The driver of the BSDE with multiple default jumps can take a generalized form involving an optional finite variation process. We first show existence and uniqueness. We then establish comparison and strict comparison results for these BSDEs, under a suitable assumption on the driver. In the case of a linear driver, we derive an explicit formula for the first component of the BSDE using an adjoint exponential semimartingale. The representation depends on whether the finite variation process is predictable or only optional. We apply our results to the problem of pricing and hedging a European option in a linear complete market with two defaultable assets and in a non-linear complete market with p defaultable assets. Two examples of the latter market model are provided: an example where the seller of the option is a large investor influencing the probability of default of a single asset and an example where the large seller's strategy affects the default probabilities of all p assets.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u7531\u5e03\u6717\u8fd0\u52a8\u548cp\u4e2a\u8fdd\u7ea6\u9785\u9a71\u52a8\u7684\u975e\u7ebf\u6027\u5012\u5411\u968f\u673a\u5fae\u5206\u65b9\u7a0b\uff0c\u5efa\u7acb\u4e86\u5b58\u5728\u552f\u4e00\u6027\u3001\u6bd4\u8f83\u5b9a\u7406\uff0c\u5e76\u5728\u7ebf\u6027\u60c5\u5f62\u7ed9\u51fa\u663e\u5f0f\u89e3\uff0c\u5e94\u7528\u4e8e\u542b\u8fdd\u7ea6\u8d44\u4ea7\u7684\u5e02\u573a\u5b9a\u4ef7\u4e0e\u5bf9\u51b2\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u591a\u8fdd\u7ea6\u8df3\u8dc3\u73af\u5883\u4e0b\u975e\u7ebf\u6027BSDE\u7684\u7406\u8bba\u6027\u8d28\uff0c\u4e3a\u542b\u8fdd\u7ea6\u98ce\u9669\u7684\u91d1\u878d\u5e02\u573a\u4e2d\u884d\u751f\u54c1\u5b9a\u4ef7\u4e0e\u5bf9\u51b2\u95ee\u9898\u63d0\u4f9b\u6570\u5b66\u5de5\u5177\uff0c\u7279\u522b\u662f\u5904\u7406\u5927\u6295\u8d44\u8005\u5f71\u54cd\u8fdd\u7ea6\u6982\u7387\u7684\u60c5\u5f62\u3002", "method": "\u9996\u5148\u8bc1\u660e\u975e\u7ebf\u6027BSDE\u7684\u5b58\u5728\u552f\u4e00\u6027\uff0c\u7136\u540e\u5efa\u7acb\u6bd4\u8f83\u5b9a\u7406\u548c\u4e25\u683c\u6bd4\u8f83\u5b9a\u7406\u3002\u5728\u7ebf\u6027\u9a71\u52a8\u60c5\u5f62\uff0c\u5229\u7528\u4f34\u968f\u6307\u6570\u534a\u9785\u63a8\u5bfc\u663e\u5f0f\u516c\u5f0f\uff0c\u533a\u5206\u53ef\u6599\u548c\u53ef\u9009\u6709\u9650\u53d8\u5dee\u8fc7\u7a0b\u3002\u6700\u540e\u5c06\u7406\u8bba\u5e94\u7528\u4e8e\u8fdd\u7ea6\u8d44\u4ea7\u5e02\u573a\u7684\u5b9a\u4ef7\u4e0e\u5bf9\u51b2\u95ee\u9898\u3002", "result": "\u5efa\u7acb\u4e86\u591a\u8fdd\u7ea6\u8df3\u8dc3BSDE\u7684\u5b58\u5728\u552f\u4e00\u6027\u5b9a\u7406\u548c\u6bd4\u8f83\u5b9a\u7406\uff1b\u5728\u7ebf\u6027\u9a71\u52a8\u60c5\u5f62\u7ed9\u51fa\u4e86\u663e\u5f0f\u89e3\u516c\u5f0f\uff1b\u6210\u529f\u5e94\u7528\u4e8e\u542b\u8fdd\u7ea6\u8d44\u4ea7\u7684\u7ebf\u6027/\u975e\u7ebf\u6027\u5b8c\u5168\u5e02\u573a\u4e2d\u6b27\u5f0f\u671f\u6743\u7684\u5b9a\u4ef7\u4e0e\u5bf9\u51b2\uff0c\u5305\u62ec\u5927\u6295\u8d44\u8005\u5f71\u54cd\u8fdd\u7ea6\u6982\u7387\u7684\u4e24\u79cd\u5e02\u573a\u6a21\u578b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u591a\u8fdd\u7ea6\u73af\u5883\u4e0b\u7684\u975e\u7ebf\u6027BSDE\u5efa\u7acb\u4e86\u5b8c\u6574\u7684\u7406\u8bba\u6846\u67b6\uff0c\u63d0\u4f9b\u4e86\u5904\u7406\u8fdd\u7ea6\u98ce\u9669\u91d1\u878d\u95ee\u9898\u7684\u6709\u6548\u6570\u5b66\u5de5\u5177\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5927\u6295\u8d44\u8005\u7b56\u7565\u5f71\u54cd\u8fdd\u7ea6\u6982\u7387\u7684\u5e02\u573a\u60c5\u5f62\u3002"}}
{"id": "2601.01170", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.01170", "abs": "https://arxiv.org/abs/2601.01170", "authors": ["Pengfeng Lin", "Guangjie Gao", "Jianjun Ma", "Miao Zhu", "Xinan Zhang", "Ahmed Abu-Siada"], "title": "Transient Power Allocation Control Scheme for Hybrid Hydrogen Electrolyzer-Supercapacitor System with Autonomous Inertia Response", "comment": null, "summary": "This paper proposes a hybrid hydrogen electrolyzer-supercapacitor system (HESS) with a novel control strategy for renewable-dominant power grids. The HESS consists of alkaline electrolyzers (AEL), proton exchange membrane electrolyzers (PEMEL), and supercapacitors (SC). The interfacing inverters between HESS and power grid are regulated by an inertia emulation control strategy. From HESS, AEL is with conventional DC power control, whereas PEMEL and SC are designed with the proposed dynamic inertia control and capacitive inertia control, respectively. Benefitting from the coordination of three controls, within the HESS, high-frequency transient power components are autonomously handled by SC, stable frequency power components are regulated by PEMEL, and low-frequency steady-state power is addressed by AEL, characterized by low operational gains and longer lifetimes. SC delivers transient power, significantly alleviating energy losses on electrolyzers and achieving adequate inertia recovery capabilities while requiring no additional communication. Implementing SOC recovery control enables the SC to withstand more than three times more stability discharge cycles compared to an SC without SOC recovery. Furthermore, a large-signal mathematical model based on mixed potential theory is established, providing clear stability boundaries for system parameters. Dynamic analyses theoretically verify system feasibility, and extensive hardware-in-the-loop experimental results fully validate the proposed HESS along with the corresponding transient power allocation controls.", "AI": {"tldr": "\u63d0\u51fa\u6df7\u5408\u6c22\u7535\u89e3\u69fd-\u8d85\u7ea7\u7535\u5bb9\u7cfb\u7edf(HESS)\u53ca\u65b0\u578b\u63a7\u5236\u7b56\u7565\uff0c\u7528\u4e8e\u53ef\u518d\u751f\u80fd\u6e90\u4e3b\u5bfc\u7535\u7f51\u3002\u7cfb\u7edf\u5305\u542b\u78b1\u6027\u7535\u89e3\u69fd\u3001\u8d28\u5b50\u4ea4\u6362\u819c\u7535\u89e3\u69fd\u548c\u8d85\u7ea7\u7535\u5bb9\uff0c\u901a\u8fc7\u60ef\u6027\u6a21\u62df\u63a7\u5236\u534f\u8c03\uff0c\u5b9e\u73b0\u4e0d\u540c\u9891\u7387\u529f\u7387\u5206\u91cf\u81ea\u4e3b\u5206\u914d\u3002", "motivation": "\u53ef\u518d\u751f\u80fd\u6e90\u4e3b\u5bfc\u7535\u7f51\u9762\u4e34\u9891\u7387\u6ce2\u52a8\u548c\u60ef\u6027\u4e0d\u8db3\u95ee\u9898\uff0c\u9700\u8981\u50a8\u80fd\u7cfb\u7edf\u63d0\u4f9b\u5feb\u901f\u54cd\u5e94\u3002\u4f20\u7edf\u7535\u89e3\u69fd\u54cd\u5e94\u6162\u3001\u5bff\u547d\u77ed\uff0c\u9700\u8981\u7ed3\u5408\u8d85\u7ea7\u7535\u5bb9\u63d0\u4f9b\u77ac\u6001\u529f\u7387\u652f\u6301\uff0c\u540c\u65f6\u964d\u4f4e\u7535\u89e3\u69fd\u80fd\u91cf\u635f\u8017\u3002", "method": "1. \u8bbe\u8ba1\u6df7\u5408\u7cfb\u7edf\uff1a\u78b1\u6027\u7535\u89e3\u69fd(AEL)\u91c7\u7528\u4f20\u7edf\u76f4\u6d41\u529f\u7387\u63a7\u5236\uff0c\u8d28\u5b50\u4ea4\u6362\u819c\u7535\u89e3\u69fd(PEMEL)\u91c7\u7528\u52a8\u6001\u60ef\u6027\u63a7\u5236\uff0c\u8d85\u7ea7\u7535\u5bb9(SC)\u91c7\u7528\u7535\u5bb9\u60ef\u6027\u63a7\u5236\n2. \u63d0\u51fa\u60ef\u6027\u6a21\u62df\u63a7\u5236\u7b56\u7565\u534f\u8c03\u4e09\u79cd\u63a7\u5236\n3. \u5efa\u7acb\u57fa\u4e8e\u6df7\u5408\u52bf\u7406\u8bba\u7684\u5927\u4fe1\u53f7\u6570\u5b66\u6a21\u578b\n4. \u5b9e\u65bdSOC\u6062\u590d\u63a7\u5236\u589e\u5f3a\u8d85\u7ea7\u7535\u5bb9\u5faa\u73af\u5bff\u547d\n5. \u901a\u8fc7\u786c\u4ef6\u5728\u73af\u5b9e\u9a8c\u9a8c\u8bc1", "result": "1. \u7cfb\u7edf\u5b9e\u73b0\u9ad8\u9891\u77ac\u6001\u529f\u7387\u7531SC\u81ea\u4e3b\u5904\u7406\uff0c\u7a33\u5b9a\u9891\u7387\u529f\u7387\u7531PEMEL\u8c03\u8282\uff0c\u4f4e\u9891\u7a33\u6001\u529f\u7387\u7531AEL\u5904\u7406\n2. SC\u663e\u8457\u51cf\u8f7b\u7535\u89e3\u69fd\u80fd\u91cf\u635f\u8017\uff0c\u63d0\u4f9b\u8db3\u591f\u60ef\u6027\u6062\u590d\u80fd\u529b\u4e14\u65e0\u9700\u989d\u5916\u901a\u4fe1\n3. SOC\u6062\u590d\u63a7\u5236\u4f7fSC\u627f\u53d7\u8d85\u8fc73\u500d\u7a33\u5b9a\u6027\u653e\u7535\u5faa\u73af\n4. \u6570\u5b66\u6a21\u578b\u63d0\u4f9b\u6e05\u6670\u7cfb\u7edf\u53c2\u6570\u7a33\u5b9a\u8fb9\u754c\n5. \u786c\u4ef6\u5728\u73af\u5b9e\u9a8c\u7ed3\u679c\u5b8c\u5168\u9a8c\u8bc1\u7cfb\u7edf\u53ef\u884c\u6027", "conclusion": "\u63d0\u51fa\u7684\u6df7\u5408\u6c22\u7535\u89e3\u69fd-\u8d85\u7ea7\u7535\u5bb9\u7cfb\u7edf\u53ca\u63a7\u5236\u7b56\u7565\u6709\u6548\u89e3\u51b3\u53ef\u518d\u751f\u80fd\u6e90\u7535\u7f51\u7684\u60ef\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u529f\u7387\u5206\u91cf\u81ea\u4e3b\u5206\u914d\u4f18\u5316\u7cfb\u7edf\u6027\u80fd\uff0c\u5ef6\u957f\u8bbe\u5907\u5bff\u547d\uff0c\u4e3a\u9ad8\u6bd4\u4f8b\u53ef\u518d\u751f\u80fd\u6e90\u7535\u7f51\u63d0\u4f9b\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01248", "categories": ["math.OC", "cs.LG", "math.NA", "math.PR"], "pdf": "https://arxiv.org/pdf/2601.01248", "abs": "https://arxiv.org/abs/2601.01248", "authors": ["Jinniao Qiu"], "title": "Stochastic Control Methods for Optimization", "comment": null, "summary": "In this work, we investigate a stochastic control framework for global optimization over both finite-dimensional Euclidean spaces and the Wasserstein space of probability measures. In the Euclidean setting, the original minimization problem is approximated by a family of regularized stochastic control problems; using dynamic programming, we analyze the associated Hamilton--Jacobi--Bellman equations and obtain tractable representations via the Cole--Hopf transform and the Feynman--Kac formula. For optimization over probability measures, we formulate a regularized mean-field control problem characterized by a master equation, and further approximate it by controlled $N$-particle systems. We establish that, as the regularization parameter tends to zero (and as the particle number tends to infinity for the optimization over probability measures), the value of the control problem converges to the global minimum of the original objective. Building on the resulting probabilistic representations, Monte Carlo-based numerical schemes are proposed and numerical experiments are reported to illustrate the practical performance of the methods and to support the theoretical convergence rates.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u968f\u673a\u63a7\u5236\u7684\u5168\u5c40\u4f18\u5316\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u548cWasserstein\u6982\u7387\u6d4b\u5ea6\u7a7a\u95f4\uff0c\u901a\u8fc7\u6b63\u5219\u5316\u63a7\u5236\u95ee\u9898\u903c\u8fd1\u539f\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u5efa\u7acb\u4e86\u6536\u655b\u6027\u7406\u8bba\u548c\u6570\u503c\u7b97\u6cd5\u3002", "motivation": "\u4f20\u7edf\u5168\u5c40\u4f18\u5316\u65b9\u6cd5\u5728\u9ad8\u7ef4\u6216\u590d\u6742\u7a7a\u95f4\uff08\u5982\u6982\u7387\u6d4b\u5ea6\u7a7a\u95f4\uff09\u4e2d\u9762\u4e34\u56f0\u96be\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5904\u7406\u975e\u51f8\u76ee\u6807\u51fd\u6570\u4e14\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u7684\u65b0\u65b9\u6cd5\u3002", "method": "1. \u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\uff1a\u5c06\u539f\u6700\u5c0f\u5316\u95ee\u9898\u8f6c\u5316\u4e3a\u6b63\u5219\u5316\u968f\u673a\u63a7\u5236\u95ee\u9898\uff0c\u5229\u7528\u52a8\u6001\u89c4\u5212\u5206\u6790HJB\u65b9\u7a0b\uff0c\u901a\u8fc7Cole-Hopf\u53d8\u6362\u548cFeynman-Kac\u516c\u5f0f\u83b7\u5f97\u53ef\u5904\u7406\u8868\u793a\uff1b2. \u6982\u7387\u6d4b\u5ea6\u7a7a\u95f4\uff1a\u6784\u5efa\u6b63\u5219\u5316\u5e73\u5747\u573a\u63a7\u5236\u95ee\u9898\uff0c\u901a\u8fc7\u53d7\u63a7N\u7c92\u5b50\u7cfb\u7edf\u8fd1\u4f3c\uff0c\u5efa\u7acb\u6536\u655b\u6027\u7406\u8bba\u3002", "result": "\u8bc1\u660e\u4e86\u5f53\u6b63\u5219\u5316\u53c2\u6570\u8d8b\u4e8e\u96f6\u65f6\uff08\u5bf9\u4e8e\u6982\u7387\u6d4b\u5ea6\u4f18\u5316\uff0c\u5f53\u7c92\u5b50\u6570\u8d8b\u4e8e\u65e0\u7a77\u65f6\uff09\uff0c\u63a7\u5236\u95ee\u9898\u7684\u503c\u6536\u655b\u5230\u539f\u76ee\u6807\u7684\u5168\u5c40\u6700\u5c0f\u503c\u3002\u57fa\u4e8e\u6982\u7387\u8868\u793a\u63d0\u51fa\u4e86\u8499\u7279\u5361\u6d1b\u6570\u503c\u65b9\u6848\uff0c\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u5b9e\u9645\u6027\u80fd\u548c\u7406\u8bba\u6536\u655b\u7387\u3002", "conclusion": "\u8be5\u968f\u673a\u63a7\u5236\u6846\u67b6\u4e3a\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u548cWasserstein\u7a7a\u95f4\u4e2d\u7684\u5168\u5c40\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u3001\u7406\u8bba\u4e25\u8c28\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u6536\u655b\u6027\u4fdd\u8bc1\u548c\u53ef\u884c\u7684\u6570\u503c\u5b9e\u73b0\u3002"}}
{"id": "2601.00818", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00818", "abs": "https://arxiv.org/abs/2601.00818", "authors": ["Chandra Sekhar Kubam"], "title": "Agentic AI for Autonomous, Explainable, and Real-Time Credit Risk Decision-Making", "comment": "8 pages", "summary": "Significant digitalization of financial services in a short period of time has led to an urgent demand to have autonomous, transparent and real-time credit risk decision making systems. The traditional machine learning models are effective in pattern recognition, but do not have the adaptive reasoning, situational awareness, and autonomy needed in modern financial operations. As a proposal, this paper presents an Agentic AI framework, or a system where AI agents view the world of dynamic credit independent of human observers, who then make actions based on their articulable decision-making paths. The research introduces a multi-agent system with reinforcing learning, natural language reasoning, explainable AI modules, and real-time data absorption pipelines as a means of assessing the risk profiles of borrowers with few humans being involved. The processes consist of agent collaboration protocol, risk-scoring engines, interpretability layers, and continuous feedback learning cycles. Findings indicate that decision speed, transparency and responsiveness is better than traditional credit scoring models. Nevertheless, there are still some practical limitations such as risks of model drift, inconsistencies in interpreting high dimensional data and regulatory uncertainties as well as infrastructure limitations in low-resource settings. The suggested system has a high prospective to transform credit analytics and future studies ought to be directed on dynamic regulatory compliance mobilizers, new agent teamwork, adversarial robustness, and large-scale implementation in cross-country credit ecosystems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAgentic AI\u6846\u67b6\u7684\u81ea\u4e3b\u4fe1\u7528\u98ce\u9669\u8bc4\u4f30\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u5b9e\u73b0\u5b9e\u65f6\u3001\u900f\u660e\u7684\u4fe1\u8d37\u51b3\u7b56\uff0c\u76f8\u6bd4\u4f20\u7edf\u6a21\u578b\u5728\u51b3\u7b56\u901f\u5ea6\u3001\u900f\u660e\u5ea6\u548c\u54cd\u5e94\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u91d1\u878d\u670d\u52a1\u7684\u5feb\u901f\u6570\u5b57\u5316\u50ac\u751f\u4e86\u5bf9\u81ea\u4e3b\u3001\u900f\u660e\u3001\u5b9e\u65f6\u4fe1\u7528\u98ce\u9669\u51b3\u7b56\u7cfb\u7edf\u7684\u8feb\u5207\u9700\u6c42\u3002\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u867d\u7136\u64c5\u957f\u6a21\u5f0f\u8bc6\u522b\uff0c\u4f46\u7f3a\u4e4f\u73b0\u4ee3\u91d1\u878d\u8fd0\u8425\u6240\u9700\u7684\u9002\u5e94\u6027\u63a8\u7406\u3001\u60c5\u5883\u611f\u77e5\u548c\u81ea\u4e3b\u6027\u3002", "method": "\u63d0\u51faAgentic AI\u6846\u67b6\uff0c\u6784\u5efa\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5305\u542b\u5f3a\u5316\u5b66\u4e60\u3001\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u3001\u53ef\u89e3\u91caAI\u6a21\u5757\u548c\u5b9e\u65f6\u6570\u636e\u5438\u6536\u7ba1\u9053\u3002\u7cfb\u7edf\u5305\u62ec\u667a\u80fd\u4f53\u534f\u4f5c\u534f\u8bae\u3001\u98ce\u9669\u8bc4\u5206\u5f15\u64ce\u3001\u53ef\u89e3\u91ca\u6027\u5c42\u548c\u6301\u7eed\u53cd\u9988\u5b66\u4e60\u5faa\u73af\u3002", "result": "\u8be5\u7cfb\u7edf\u5728\u51b3\u7b56\u901f\u5ea6\u3001\u900f\u660e\u5ea6\u548c\u54cd\u5e94\u6027\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u4fe1\u7528\u8bc4\u5206\u6a21\u578b\u3002\u4f46\u4ecd\u5b58\u5728\u6a21\u578b\u6f02\u79fb\u98ce\u9669\u3001\u9ad8\u7ef4\u6570\u636e\u89e3\u91ca\u4e0d\u4e00\u81f4\u3001\u76d1\u7ba1\u4e0d\u786e\u5b9a\u6027\u4ee5\u53ca\u4f4e\u8d44\u6e90\u73af\u5883\u57fa\u7840\u8bbe\u65bd\u9650\u5236\u7b49\u5b9e\u9645\u6311\u6218\u3002", "conclusion": "\u8be5\u6846\u67b6\u5177\u6709\u53d8\u9769\u4fe1\u7528\u5206\u6790\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u672a\u6765\u7814\u7a76\u5e94\u5173\u6ce8\u52a8\u6001\u76d1\u7ba1\u5408\u89c4\u673a\u5236\u3001\u65b0\u578b\u667a\u80fd\u4f53\u534f\u4f5c\u3001\u5bf9\u6297\u9c81\u68d2\u6027\u4ee5\u53ca\u8de8\u56fd\u4fe1\u7528\u751f\u6001\u7cfb\u7edf\u7684\u5927\u89c4\u6a21\u5b9e\u65bd\u3002"}}
{"id": "2601.01011", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01011", "abs": "https://arxiv.org/abs/2601.01011", "authors": ["Patricio Vera"], "title": "Intention Collapse: Intention-Level Metrics for Reasoning in Language Models", "comment": "21 pages, 4 figures, 3 tables. Code: https://github.com/patriciomvera/intention-collapse-experiments", "summary": "Every act of language generation compresses a rich internal state into a single token sequence. We call this process intention collapse: a many-to-one projection from a high dimensional intention space I into an external language space L. We formalize intention collapse for contemporary language models, define three simple, model agnostic intention metrics (intention entropy Hint, effective dimensionality dimeff, and latent knowledge recoverability Recov), and propose an empirical agenda for studying how inference time computation shapes internal intentions before they are verbalized. We also report a first small scale experiment. Using a 4 bit Mistral 7B model on 200 GSM8K problems, we compare a direct answer baseline, a chain of thought (CoT) regime, and a babble control. CoT raises accuracy from 5.5 percent to 53 percent, sharply reduces pre collapse intention entropy (from 1.42 to 0.37 bits), and shows higher global effective dimensionality than the other regimes despite producing fewer tokens than babble. At the same time, Hint has little item level predictive power, and a linear probe on I achieves AUROC 0.65 in the CoT regime but only about chance in the baseline regime, where it collapses to the majority class. These preliminary results indicate that intention level metrics can distinguish inference regimes and expose latent information that is partly lost during collapse, while also revealing important limitations of our current proxies", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\"\u610f\u56fe\u574d\u7f29\"\u6982\u5ff5\uff0c\u5373\u8bed\u8a00\u751f\u6210\u65f6\u5c06\u9ad8\u7ef4\u5185\u90e8\u72b6\u6001\u538b\u7f29\u4e3a\u5355\u4e00token\u5e8f\u5217\u7684\u8fc7\u7a0b\uff0c\u5e76\u5b9a\u4e49\u4e86\u4e09\u4e2a\u610f\u56fe\u5ea6\u91cf\u6307\u6807\u6765\u7814\u7a76\u63a8\u7406\u8ba1\u7b97\u5982\u4f55\u5f71\u54cd\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u610f\u56fe\u7684\u8868\u8fbe\u3002", "motivation": "\u7814\u7a76\u8bed\u8a00\u751f\u6210\u8fc7\u7a0b\u4e2d\u5185\u90e8\u4e30\u5bcc\u610f\u56fe\u5982\u4f55\u88ab\u538b\u7f29\u4e3a\u5355\u4e00\u8bed\u8a00\u8f93\u51fa\u7684\u8fc7\u7a0b\uff0c\u7406\u89e3\u63a8\u7406\u65f6\u8ba1\u7b97\u5982\u4f55\u5851\u9020\u8bed\u8a00\u6a21\u578b\u7684\u5185\u90e8\u610f\u56fe\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u610f\u56fe\u5728\u8868\u8fbe\u8fc7\u7a0b\u4e2d\u7684\u4fe1\u606f\u635f\u5931\u3002", "method": "\u63d0\u51fa\u610f\u56fe\u574d\u7f29\u7684\u5f62\u5f0f\u5316\u5b9a\u4e49\uff0c\u5b9a\u4e49\u4e09\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u610f\u56fe\u5ea6\u91cf\u6307\u6807\uff08\u610f\u56fe\u71b5\u3001\u6709\u6548\u7ef4\u5ea6\u3001\u6f5c\u5728\u77e5\u8bc6\u53ef\u6062\u590d\u6027\uff09\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u76f4\u63a5\u56de\u7b54\u3001\u601d\u7ef4\u94fe\u548c\u968f\u673a\u751f\u6210\u4e09\u79cd\u63a8\u7406\u673a\u5236\u3002", "result": "\u601d\u7ef4\u94fe\u5c06\u51c6\u786e\u7387\u4ece5.5%\u63d0\u5347\u81f353%\uff0c\u663e\u8457\u964d\u4f4e\u574d\u7f29\u524d\u610f\u56fe\u71b5\uff0c\u63d0\u9ad8\u6709\u6548\u7ef4\u5ea6\u3002\u610f\u56fe\u71b5\u5728\u9879\u76ee\u5c42\u9762\u9884\u6d4b\u80fd\u529b\u6709\u9650\uff0c\u7ebf\u6027\u63a2\u6d4b\u5728\u601d\u7ef4\u94fe\u673a\u5236\u4e0bAUROC\u4e3a0.65\uff0c\u4f46\u5728\u57fa\u7ebf\u673a\u5236\u4e0b\u63a5\u8fd1\u968f\u673a\u731c\u6d4b\u3002", "conclusion": "\u610f\u56fe\u5c42\u9762\u5ea6\u91cf\u80fd\u591f\u533a\u5206\u4e0d\u540c\u63a8\u7406\u673a\u5236\uff0c\u63ed\u793a\u5728\u574d\u7f29\u8fc7\u7a0b\u4e2d\u90e8\u5206\u4e22\u5931\u7684\u6f5c\u5728\u4fe1\u606f\uff0c\u4f46\u5f53\u524d\u4ee3\u7406\u6307\u6807\u4ecd\u6709\u91cd\u8981\u5c40\u9650\u6027\uff0c\u4e3a\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u610f\u56fe\u8868\u8fbe\u63d0\u4f9b\u4e86\u65b0\u6846\u67b6\u3002"}}
{"id": "2601.01622", "categories": ["econ.EM"], "pdf": "https://arxiv.org/pdf/2601.01622", "abs": "https://arxiv.org/abs/2601.01622", "authors": ["Valentin Winkler"], "title": "When and Why State-Dependent Local Projections Work", "comment": null, "summary": "This paper studies state-dependent local projections (LPs). First, I establish a general characterization of their estimand: under minimal assumptions, state-dependent LPs recover weighted averages of causal effects. This holds for essentially all specifications used in practice. Second, I show that state-dependent LPs and VARs target different estimands and propose a simple VAR-based estimator whose probability limit equals the LP estimand. Third, in instrumental variable (LP-IV) settings, state-dependent weighting can generate nonzero interaction terms, even when the effects are not state-dependent. Overall, this paper shows how to correctly interpret state-dependent LPs, clarifying their connection to VARs and highlighting a key source of LP-IV misinterpretation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u72b6\u6001\u4f9d\u8d56\u5c40\u90e8\u6295\u5f71(LPs)\uff0c\u5efa\u7acb\u4e86\u5176\u4f30\u8ba1\u91cf\u7684\u4e00\u822c\u7279\u5f81\uff0c\u8868\u660e\u5b83\u4eec\u6062\u590d\u56e0\u679c\u6548\u5e94\u7684\u52a0\u6743\u5e73\u5747\uff0c\u9610\u660e\u4e86LPs\u4e0eVARs\u7684\u533a\u522b\uff0c\u5e76\u63d0\u51fa\u4e86\u4f7fVAR\u4f30\u8ba1\u91cf\u7b49\u4e8eLP\u4f30\u8ba1\u91cf\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u63ed\u793a\u4e86LP-IV\u8bbe\u7f6e\u4e2d\u72b6\u6001\u4f9d\u8d56\u52a0\u6743\u53ef\u80fd\u5bfc\u81f4\u7684\u8bef\u89e3\u3002", "motivation": "\u72b6\u6001\u4f9d\u8d56\u5c40\u90e8\u6295\u5f71(LPs)\u5728\u5b9e\u8bc1\u5b8f\u89c2\u7ecf\u6d4e\u5b66\u4e2d\u88ab\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5bf9\u5176\u4f30\u8ba1\u91cf\u7684\u89e3\u91ca\u5b58\u5728\u6a21\u7cca\u6027\u3002\u8bba\u6587\u65e8\u5728\u6f84\u6e05\u72b6\u6001\u4f9d\u8d56LPs\u7684\u6b63\u786e\u89e3\u91ca\uff0c\u660e\u786e\u5b83\u4eec\u4e0e\u5411\u91cf\u81ea\u56de\u5f52(VARs)\u7684\u5173\u7cfb\uff0c\u5e76\u63ed\u793a\u5de5\u5177\u53d8\u91cf(LP-IV)\u8bbe\u7f6e\u4e2d\u53ef\u80fd\u51fa\u73b0\u7684\u8bef\u89e3\u6765\u6e90\u3002", "method": "\u8bba\u6587\u9996\u5148\u5728\u6700\u5c0f\u5047\u8bbe\u4e0b\u5efa\u7acb\u4e86\u72b6\u6001\u4f9d\u8d56LPs\u4f30\u8ba1\u91cf\u7684\u4e00\u822c\u7279\u5f81\u5316\uff0c\u8bc1\u660e\u5176\u6062\u590d\u56e0\u679c\u6548\u5e94\u7684\u52a0\u6743\u5e73\u5747\u3002\u7136\u540e\u901a\u8fc7\u7406\u8bba\u5206\u6790\u6bd4\u8f83LPs\u548cVARs\u7684\u76ee\u6807\u4f30\u8ba1\u91cf\uff0c\u5e76\u63d0\u51fa\u4e00\u4e2a\u7b80\u5355\u7684\u57fa\u4e8eVAR\u7684\u4f30\u8ba1\u5668\uff0c\u4f7f\u5176\u6982\u7387\u6781\u9650\u7b49\u4e8eLP\u4f30\u8ba1\u91cf\u3002\u6700\u540e\u5728\u5de5\u5177\u53d8\u91cf(LP-IV)\u8bbe\u7f6e\u4e2d\u5206\u6790\u72b6\u6001\u4f9d\u8d56\u52a0\u6743\u7684\u5f71\u54cd\u3002", "result": "1) \u72b6\u6001\u4f9d\u8d56LPs\u5728\u51e0\u4e4e\u6240\u6709\u5b9e\u9645\u4f7f\u7528\u7684\u8bbe\u5b9a\u4e0b\u90fd\u6062\u590d\u56e0\u679c\u6548\u5e94\u7684\u52a0\u6743\u5e73\u5747\uff1b2) LPs\u548cVARs\u9488\u5bf9\u4e0d\u540c\u7684\u4f30\u8ba1\u91cf\uff0c\u4f46\u53ef\u4ee5\u901a\u8fc7\u63d0\u51fa\u7684VAR-based\u4f30\u8ba1\u5668\u4f7f\u4e24\u8005\u4e00\u81f4\uff1b3) \u5728LP-IV\u8bbe\u7f6e\u4e2d\uff0c\u5373\u4f7f\u6548\u5e94\u4e0d\u662f\u72b6\u6001\u4f9d\u8d56\u7684\uff0c\u72b6\u6001\u4f9d\u8d56\u52a0\u6743\u4e5f\u53ef\u80fd\u4ea7\u751f\u975e\u96f6\u7684\u4ea4\u4e92\u9879\uff0c\u8fd9\u662fLP-IV\u8bef\u89e3\u7684\u5173\u952e\u6765\u6e90\u3002", "conclusion": "\u8bba\u6587\u4e3a\u6b63\u786e\u89e3\u91ca\u72b6\u6001\u4f9d\u8d56LPs\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\uff0c\u6f84\u6e05\u4e86LPs\u4e0eVARs\u7684\u5173\u7cfb\uff0c\u5e76\u5f3a\u8c03\u4e86LP-IV\u8bbe\u7f6e\u4e2d\u72b6\u6001\u4f9d\u8d56\u52a0\u6743\u53ef\u80fd\u5bfc\u81f4\u8bef\u89e3\u7684\u91cd\u8981\u673a\u5236\uff0c\u5bf9\u5b9e\u8bc1\u5b8f\u89c2\u7ecf\u6d4e\u5b66\u7814\u7a76\u5177\u6709\u91cd\u8981\u6307\u5bfc\u610f\u4e49\u3002"}}
{"id": "2601.01303", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2601.01303", "abs": "https://arxiv.org/abs/2601.01303", "authors": ["Michael Smith", "Riley Grossman", "Antonio Torres-Aguero", "Pritam Sen", "Cristian Borcea", "Yi Chen"], "title": "Inconsistencies in Classification of Online News Articles: A Call for Common Standards in Brand Safety Services", "comment": "17 pages", "summary": "This study examines inconsistencies in the brand safety classifications of online news articles by analyzing ratings from three leading brand safety providers, DoubleVerify, Integral Ad Science, and Oracle. We focus on news content because of its central role in public discourse and the significant financial consequences of unsafe classifications in a sector that is already underserved by digital ad spending. By collecting data from 4,352 news articles on 51 domains, our analysis shows that brand safety services often produce conflicting classifications, with significant discrepancies between providers. These inconsistencies can have harmful consequences for both advertisers and publishers, leading to misplaced advertising spending and revenue losses. This research provides critical insights into the shortcomings of the current brand safety landscape. We argue for a standardized and transparent brand safety system to mitigate the harmful effects of the current system on the digital advertising ecosystem.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u4e09\u5927\u54c1\u724c\u5b89\u5168\u670d\u52a1\u5546\u5bf9\u65b0\u95fb\u6587\u7ae0\u7684\u5206\u7c7b\u5b58\u5728\u663e\u8457\u4e0d\u4e00\u81f4\uff0c\u5bfc\u81f4\u5e7f\u544a\u652f\u51fa\u9519\u914d\u548c\u6536\u5165\u635f\u5931\uff0c\u547c\u5401\u5efa\u7acb\u6807\u51c6\u5316\u900f\u660e\u7cfb\u7edf", "motivation": "\u65b0\u95fb\u5185\u5bb9\u5728\u516c\u5171\u8bdd\u8bed\u4e2d\u5177\u6709\u6838\u5fc3\u4f5c\u7528\uff0c\u4e14\u6570\u5b57\u5e7f\u544a\u652f\u51fa\u5df2\u7ecf\u4e0d\u8db3\uff0c\u54c1\u724c\u5b89\u5168\u5206\u7c7b\u7684\u4e0d\u4e00\u81f4\u4f1a\u5bf9\u5e7f\u544a\u5546\u548c\u51fa\u7248\u5546\u9020\u6210\u91cd\u5927\u8d22\u52a1\u5f71\u54cd", "method": "\u6536\u96c651\u4e2a\u57df\u540d\u4e0a\u76844,352\u7bc7\u65b0\u95fb\u6587\u7ae0\u6570\u636e\uff0c\u5206\u6790DoubleVerify\u3001Integral Ad Science\u548cOracle\u4e09\u5bb6\u9886\u5148\u54c1\u724c\u5b89\u5168\u63d0\u4f9b\u5546\u7684\u5206\u7c7b\u8bc4\u7ea7", "result": "\u54c1\u724c\u5b89\u5168\u670d\u52a1\u7ecf\u5e38\u4ea7\u751f\u51b2\u7a81\u7684\u5206\u7c7b\u7ed3\u679c\uff0c\u4e0d\u540c\u63d0\u4f9b\u5546\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u8fd9\u4e9b\u4e0d\u4e00\u81f4\u53ef\u80fd\u5bfc\u81f4\u5e7f\u544a\u652f\u51fa\u9519\u914d\u548c\u6536\u5165\u635f\u5931", "conclusion": "\u5f53\u524d\u54c1\u724c\u5b89\u5168\u7cfb\u7edf\u5b58\u5728\u7f3a\u9677\uff0c\u9700\u8981\u5efa\u7acb\u6807\u51c6\u5316\u548c\u900f\u660e\u7684\u54c1\u724c\u5b89\u5168\u7cfb\u7edf\uff0c\u4ee5\u51cf\u8f7b\u5bf9\u6570\u5b57\u5e7f\u544a\u751f\u6001\u7cfb\u7edf\u7684\u6709\u5bb3\u5f71\u54cd"}}
{"id": "2601.01097", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01097", "abs": "https://arxiv.org/abs/2601.01097", "authors": ["Xuan Son Nguyen", "Shuo Yang", "Aymeric Histace"], "title": "Neural Networks on Symmetric Spaces of Noncompact Type", "comment": null, "summary": "Recent works have demonstrated promising performances of neural networks on hyperbolic spaces and symmetric positive definite (SPD) manifolds. These spaces belong to a family of Riemannian manifolds referred to as symmetric spaces of noncompact type. In this paper, we propose a novel approach for developing neural networks on such spaces. Our approach relies on a unified formulation of the distance from a point to a hyperplane on the considered spaces. We show that some existing formulations of the point-to-hyperplane distance can be recovered by our approach under specific settings. Furthermore, we derive a closed-form expression for the point-to-hyperplane distance in higher-rank symmetric spaces of noncompact type equipped with G-invariant Riemannian metrics. The derived distance then serves as a tool to design fully-connected (FC) layers and an attention mechanism for neural networks on the considered spaces. Our approach is validated on challenging benchmarks for image classification, electroencephalogram (EEG) signal classification, image generation, and natural language inference.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u975e\u7d27\u578b\u5bf9\u79f0\u7a7a\u95f4\u4e0a\u6784\u5efa\u795e\u7ecf\u7f51\u7edc\u7684\u65b0\u65b9\u6cd5\uff0c\u57fa\u4e8e\u7edf\u4e00\u7684\u70b9\u5230\u8d85\u5e73\u9762\u8ddd\u79bb\u516c\u5f0f\uff0c\u5e76\u8bbe\u8ba1\u4e86\u5168\u8fde\u63a5\u5c42\u548c\u6ce8\u610f\u529b\u673a\u5236\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5c55\u793a\u4e86\u795e\u7ecf\u7f51\u7edc\u5728\u53cc\u66f2\u7a7a\u95f4\u548cSPD\u6d41\u5f62\u4e0a\u7684\u826f\u597d\u6027\u80fd\uff0c\u8fd9\u4e9b\u7a7a\u95f4\u90fd\u5c5e\u4e8e\u975e\u7d27\u578b\u5bf9\u79f0\u7a7a\u95f4\u3002\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u7edf\u4e00\u7684\u65b9\u6cd5\u5728\u8fd9\u4e9b\u7a7a\u95f4\u4e0a\u6784\u5efa\u795e\u7ecf\u7f51\u7edc\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u7684\u70b9\u5230\u8d85\u5e73\u9762\u8ddd\u79bb\u516c\u5f0f\uff0c\u63a8\u5bfc\u51fa\u9ad8\u9636\u975e\u7d27\u578b\u5bf9\u79f0\u7a7a\u95f4\u4e2dG\u4e0d\u53d8\u9ece\u66fc\u5ea6\u91cf\u4e0b\u7684\u95ed\u5f0f\u8ddd\u79bb\u8868\u8fbe\u5f0f\uff0c\u57fa\u4e8e\u6b64\u8bbe\u8ba1\u5168\u8fde\u63a5\u5c42\u548c\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "\u65b9\u6cd5\u5728\u56fe\u50cf\u5206\u7c7b\u3001EEG\u4fe1\u53f7\u5206\u7c7b\u3001\u56fe\u50cf\u751f\u6210\u548c\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u7b49\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5f97\u5230\u9a8c\u8bc1\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u975e\u7d27\u578b\u5bf9\u79f0\u7a7a\u95f4\u4e0a\u6784\u5efa\u795e\u7ecf\u7f51\u7edc\u7684\u7edf\u4e00\u65b9\u6cd5\uff0c\u901a\u8fc7\u70b9\u5230\u8d85\u5e73\u9762\u8ddd\u79bb\u516c\u5f0f\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u7f51\u7edc\u67b6\u6784\u8bbe\u8ba1\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2601.00834", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00834", "abs": "https://arxiv.org/abs/2601.00834", "authors": ["Julian Evan Chrisnanto", "Salsabila Rahma Alia", "Nurfauzi Fadillah", "Yulison Herry Chrisnanto"], "title": "Intrinsic-Metric Physics-Informed Neural Networks (IM-PINN) for Reaction-Diffusion Dynamics on Complex Riemannian Manifolds", "comment": "19 pages, 7 figures", "summary": "Simulating nonlinear reaction-diffusion dynamics on complex, non-Euclidean manifolds remains a fundamental challenge in computational morphogenesis, constrained by high-fidelity mesh generation costs and symplectic drift in discrete time-stepping schemes. This study introduces the Intrinsic-Metric Physics-Informed Neural Network (IM-PINN), a mesh-free geometric deep learning framework that solves partial differential equations directly in the continuous parametric domain. By embedding the Riemannian metric tensor into the automatic differentiation graph, our architecture analytically reconstructs the Laplace-Beltrami operator, decoupling solution complexity from geometric discretization. We validate the framework on a \"Stochastic Cloth\" manifold with extreme Gaussian curvature fluctuations ($K \\in [-2489, 3580]$), where traditional adaptive refinement fails to resolve anisotropic Turing instabilities. Using a dual-stream architecture with Fourier feature embeddings to mitigate spectral bias, the IM-PINN recovers the \"splitting spot\" and \"labyrinthine\" regimes of the Gray-Scott model. Benchmarking against the Surface Finite Element Method (SFEM) reveals superior physical rigor: the IM-PINN achieves global mass conservation error of $\\mathcal{E}_{mass} \\approx 0.157$ versus SFEM's $0.258$, acting as a thermodynamically consistent global solver that eliminates mass drift inherent in semi-implicit integration. The framework offers a memory-efficient, resolution-independent paradigm for simulating biological pattern formation on evolving surfaces, bridging differential geometry and physics-informed machine learning.", "AI": {"tldr": "IM-PINN\uff1a\u4e00\u79cd\u65e0\u9700\u7f51\u683c\u7684\u51e0\u4f55\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u9ece\u66fc\u5ea6\u91cf\u5f20\u91cf\u5d4c\u5165\u81ea\u52a8\u5fae\u5206\u56fe\uff0c\u76f4\u63a5\u5728\u8fde\u7eed\u53c2\u6570\u57df\u6c42\u89e3\u590d\u6742\u6d41\u5f62\u4e0a\u7684\u975e\u7ebf\u6027\u53cd\u5e94\u6269\u6563\u65b9\u7a0b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u6781\u7aef\u66f2\u7387\u8868\u9762\u4e0a\u7684\u8ba1\u7b97\u96be\u9898\u3002", "motivation": "\u5728\u590d\u6742\u975e\u6b27\u51e0\u91cc\u5f97\u6d41\u5f62\u4e0a\u6a21\u62df\u975e\u7ebf\u6027\u53cd\u5e94\u6269\u6563\u52a8\u529b\u5b66\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u9ad8\u4fdd\u771f\u7f51\u683c\u751f\u6210\u6210\u672c\u9ad8\u6602\uff0c\u4ee5\u53ca\u79bb\u6563\u65f6\u95f4\u6b65\u8fdb\u65b9\u6848\u4e2d\u7684\u8f9b\u6f02\u79fb\u95ee\u9898\u3002\u4f20\u7edf\u81ea\u9002\u5e94\u7ec6\u5316\u65b9\u6cd5\u65e0\u6cd5\u89e3\u6790\u5177\u6709\u6781\u7aef\u9ad8\u65af\u66f2\u7387\u6ce2\u52a8\u7684\u5404\u5411\u5f02\u6027\u56fe\u7075\u4e0d\u7a33\u5b9a\u6027\u3002", "method": "\u63d0\u51fa\u5185\u5728\u5ea6\u91cf\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08IM-PINN\uff09\uff0c\u901a\u8fc7\u5c06\u9ece\u66fc\u5ea6\u91cf\u5f20\u91cf\u5d4c\u5165\u81ea\u52a8\u5fae\u5206\u56fe\uff0c\u89e3\u6790\u91cd\u5efa\u62c9\u666e\u62c9\u65af-\u8d1d\u5c14\u7279\u62c9\u7c73\u7b97\u5b50\uff0c\u5c06\u89e3\u590d\u6742\u5ea6\u4e0e\u51e0\u4f55\u79bb\u6563\u5316\u89e3\u8026\u3002\u91c7\u7528\u53cc\u6d41\u67b6\u6784\u548c\u5085\u91cc\u53f6\u7279\u5f81\u5d4c\u5165\u6765\u7f13\u89e3\u8c31\u504f\u5dee\u3002", "result": "\u5728\u5177\u6709\u6781\u7aef\u9ad8\u65af\u66f2\u7387\u6ce2\u52a8\uff08K\u2208[-2489,3580]\uff09\u7684\"\u968f\u673a\u5e03\u6599\"\u6d41\u5f62\u4e0a\u9a8c\u8bc1\uff0cIM-PINN\u6062\u590d\u4e86Gray-Scott\u6a21\u578b\u7684\"\u5206\u88c2\u6591\u70b9\"\u548c\"\u8ff7\u5bab\"\u6a21\u5f0f\u3002\u76f8\u6bd4\u8868\u9762\u6709\u9650\u5143\u6cd5\uff08SFEM\uff09\uff0cIM-PINN\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u5168\u5c40\u8d28\u91cf\u5b88\u6052\uff08\u8bef\u5dee0.157 vs 0.258\uff09\uff0c\u6d88\u9664\u4e86\u534a\u9690\u5f0f\u79ef\u5206\u56fa\u6709\u7684\u8d28\u91cf\u6f02\u79fb\u3002", "conclusion": "IM-PINN\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5185\u5b58\u9ad8\u6548\u3001\u5206\u8fa8\u7387\u65e0\u5173\u7684\u8303\u5f0f\uff0c\u7528\u4e8e\u5728\u6f14\u5316\u8868\u9762\u4e0a\u6a21\u62df\u751f\u7269\u6a21\u5f0f\u5f62\u6210\uff0c\u5f25\u5408\u4e86\u5fae\u5206\u51e0\u4f55\u548c\u7269\u7406\u4fe1\u606f\u673a\u5668\u5b66\u4e60\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u4f5c\u4e3a\u70ed\u529b\u5b66\u4e00\u81f4\u7684\u5168\u5c40\u6c42\u89e3\u5668\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2601.01269", "categories": ["q-fin.MF", "cond-mat.stat-mech", "econ.TH", "math.PR"], "pdf": "https://arxiv.org/pdf/2601.01269", "abs": "https://arxiv.org/abs/2601.01269", "authors": ["Valerii Kremnev"], "title": "Critical volatility threshold for log-normal to power-law transition", "comment": "31 pages, 4 figures", "summary": "Random walk models with log-normal outcomes fit local market observations remarkably well. Yet interconnected or recursive structures - layered derivatives, leveraged positions, iterative funding rounds - periodically produce power-law distributed events. We show that the transition from log-normal to power-law dynamics requires only three conditions: randomness in the underlying process, rectification of payouts, and iterative feed-forward of expected values. Using an infinite option-on-option chain as an illustrative model, we derive a critical volatility threshold at $\u03c3^* = \\sqrt{2\u03c0} \\approx 250.66\\%$ for the unconditional case. With selective survival - where participants require minimum returns to continue - the critical threshold drops discontinuously to $\u03c3_{\\text{th}}^{*} = \\sqrt{\u03c0/2} \\approx 125.3\\%$, and can decrease further with higher survival thresholds. The resulting outcomes follow what we term the Critical Volatility ($V^*$) Distribution - a power-law whose exponent admits closed-form expression in terms of survival pressure and conditional expected growth. The result suggests that fat tails may be an emergent property of iterative log-normal processes with selection rather than an exogenous feature.", "AI": {"tldr": "\u968f\u673a\u6e38\u8d70\u6a21\u578b\u5728\u5c40\u90e8\u5e02\u573a\u89c2\u6d4b\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u9012\u5f52\u7ed3\u6784\u4f1a\u4ea7\u751f\u5e42\u5f8b\u5206\u5e03\u4e8b\u4ef6\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u4ece\u5bf9\u6570\u6b63\u6001\u5230\u5e42\u5f8b\u52a8\u6001\u7684\u8f6c\u53d8\u53ea\u9700\u8981\u4e09\u4e2a\u6761\u4ef6\uff1a\u57fa\u7840\u8fc7\u7a0b\u7684\u968f\u673a\u6027\u3001\u6536\u76ca\u7684\u6574\u6d41\u4ee5\u53ca\u671f\u671b\u503c\u7684\u8fed\u4ee3\u524d\u9988\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u91ca\u4e3a\u4ec0\u4e48\u5728\u91d1\u878d\u5e02\u573a\u4e2d\uff0c\u5c3d\u7ba1\u5c40\u90e8\u89c2\u6d4b\u7b26\u5408\u5bf9\u6570\u6b63\u6001\u5206\u5e03\uff0c\u4f46\u9012\u5f52\u7ed3\u6784\uff08\u5982\u5206\u5c42\u884d\u751f\u54c1\u3001\u6760\u6746\u5934\u5bf8\u3001\u8fed\u4ee3\u878d\u8d44\u8f6e\u6b21\uff09\u4f1a\u5468\u671f\u6027\u5730\u4ea7\u751f\u5e42\u5f8b\u5206\u5e03\u4e8b\u4ef6\u3002\u4f5c\u8005\u5e0c\u671b\u7406\u89e3\u4ece\u5bf9\u6570\u6b63\u6001\u5230\u5e42\u5f8b\u52a8\u6001\u8f6c\u53d8\u7684\u673a\u5236\u3002", "method": "\u4f7f\u7528\u65e0\u9650\u671f\u6743\u94fe\u4f5c\u4e3a\u8bf4\u660e\u6a21\u578b\uff0c\u63a8\u5bfc\u51fa\u65e0\u6761\u4ef6\u60c5\u51b5\u4e0b\u7684\u4e34\u754c\u6ce2\u52a8\u7387\u9608\u503c\u03c3* = \u221a(2\u03c0) \u2248 250.66%\u3002\u8003\u8651\u9009\u62e9\u6027\u751f\u5b58\u6761\u4ef6\uff08\u53c2\u4e0e\u8005\u9700\u8981\u6700\u4f4e\u56de\u62a5\u624d\u80fd\u7ee7\u7eed\uff09\u65f6\uff0c\u4e34\u754c\u9608\u503c\u4e0d\u8fde\u7eed\u5730\u4e0b\u964d\u5230\u03c3_th* = \u221a(\u03c0/2) \u2248 125.3%\u3002", "result": "\u53d1\u73b0\u4e86\u4ece\u5bf9\u6570\u6b63\u6001\u5230\u5e42\u5f8b\u5206\u5e03\u7684\u8f6c\u53d8\u673a\u5236\uff0c\u63a8\u5bfc\u51fa\u4e34\u754c\u6ce2\u52a8\u7387\u5206\u5e03\uff08V*\u5206\u5e03\uff09\u2014\u2014\u4e00\u79cd\u5e42\u5f8b\u5206\u5e03\uff0c\u5176\u6307\u6570\u53ef\u4ee5\u7528\u751f\u5b58\u538b\u529b\u548c\u6761\u4ef6\u671f\u671b\u589e\u957f\u7387\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\u8868\u793a\u3002\u7ed3\u679c\u8868\u660e\uff0c\u539a\u5c3e\u53ef\u80fd\u662f\u5177\u6709\u9009\u62e9\u6027\u7684\u8fed\u4ee3\u5bf9\u6570\u6b63\u6001\u8fc7\u7a0b\u7684\u5185\u751f\u5c5e\u6027\uff0c\u800c\u975e\u5916\u751f\u7279\u5f81\u3002", "conclusion": "\u5e42\u5f8b\u5206\u5e03\u53ef\u80fd\u662f\u8fed\u4ee3\u5bf9\u6570\u6b63\u6001\u8fc7\u7a0b\u5728\u9009\u62e9\u6027\u751f\u5b58\u538b\u529b\u4e0b\u81ea\u7136\u6d8c\u73b0\u7684\u7279\u6027\uff0c\u800c\u975e\u91d1\u878d\u5e02\u573a\u7684\u5916\u751f\u7279\u5f81\u3002\u8fd9\u4e00\u53d1\u73b0\u4e3a\u7406\u89e3\u91d1\u878d\u5e02\u573a\u7684\u6781\u7aef\u4e8b\u4ef6\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u6846\u67b6\u3002"}}
{"id": "2601.01709", "categories": ["q-fin.PR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01709", "abs": "https://arxiv.org/abs/2601.01709", "authors": ["Ziheng Chen", "Minxuan Hu", "Jiayu Yi", "Wenxi Sun"], "title": "Reinforcement Learning for Option Hedging: Static Implied-Volatility Fit versus Shortfall-Aware Performance", "comment": null, "summary": "We extend the Q-learner in Black-Scholes (QLBS) framework by incorporating risk aversion and trading costs, and propose a novel Replication Learning of Option Pricing (RLOP) approach. Both methods are fully compatible with standard reinforcement learning algorithms and operate under market frictions. Using SPY and XOP option data, we evaluate performance along static and dynamic dimensions. Adaptive-QLBS achieves higher static pricing accuracy in implied volatility space, while RLOP delivers superior dynamic hedging performance by reducing shortfall probability. These results highlight the importance of evaluating option pricing models beyond static fit, emphasizing realized hedging outcomes.", "AI": {"tldr": "\u5c06\u98ce\u9669\u538c\u6076\u548c\u4ea4\u6613\u6210\u672c\u7eb3\u5165QLBS\u6846\u67b6\uff0c\u63d0\u51faRLOP\u65b9\u6cd5\uff0c\u5728SPY\u548cXOP\u671f\u6743\u6570\u636e\u4e0a\u8bc4\u4f30\u9759\u6001\u5b9a\u4ef7\u548c\u52a8\u6001\u5bf9\u51b2\u6027\u80fd", "motivation": "\u73b0\u6709\u671f\u6743\u5b9a\u4ef7\u6a21\u578b\u901a\u5e38\u53ea\u5173\u6ce8\u9759\u6001\u5b9a\u4ef7\u51c6\u786e\u6027\uff0c\u800c\u5ffd\u7565\u4e86\u5b9e\u9645\u5bf9\u51b2\u6548\u679c\u7684\u91cd\u8981\u6027\u3002\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u5728\u8003\u8651\u5e02\u573a\u6469\u64e6\uff08\u4ea4\u6613\u6210\u672c\uff09\u548c\u98ce\u9669\u538c\u6076\u7684\u60c5\u51b5\u4e0b\uff0c\u65e2\u80fd\u63d0\u9ad8\u5b9a\u4ef7\u51c6\u786e\u6027\u53c8\u80fd\u6539\u5584\u52a8\u6001\u5bf9\u51b2\u6027\u80fd\u7684\u65b9\u6cd5\u3002", "method": "1. \u6269\u5c55QLBS\u6846\u67b6\uff0c\u7eb3\u5165\u98ce\u9669\u538c\u6076\u548c\u4ea4\u6613\u6210\u672c\uff1b2. \u63d0\u51fa\u65b0\u7684RLOP\uff08\u590d\u5236\u5b66\u4e60\u671f\u6743\u5b9a\u4ef7\uff09\u65b9\u6cd5\uff1b3. \u4e24\u79cd\u65b9\u6cd5\u90fd\u4e0e\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u517c\u5bb9\uff1b4. \u4f7f\u7528SPY\u548cXOP\u671f\u6743\u6570\u636e\u8fdb\u884c\u8bc4\u4f30\uff1b5. \u4ece\u9759\u6001\uff08\u5b9a\u4ef7\u51c6\u786e\u6027\uff09\u548c\u52a8\u6001\uff08\u5bf9\u51b2\u6027\u80fd\uff09\u4e24\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u3002", "result": "1. Adaptive-QLBS\u5728\u9690\u542b\u6ce2\u52a8\u7387\u7a7a\u95f4\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u9759\u6001\u5b9a\u4ef7\u51c6\u786e\u6027\uff1b2. RLOP\u5728\u52a8\u6001\u5bf9\u51b2\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u964d\u4f4e\u4e86\u77ed\u7f3a\u6982\u7387\uff1b3. \u4e24\u79cd\u65b9\u6cd5\u90fd\u80fd\u5728\u5e02\u573a\u6469\u64e6\u4e0b\u6709\u6548\u8fd0\u884c\u3002", "conclusion": "\u8bc4\u4f30\u671f\u6743\u5b9a\u4ef7\u6a21\u578b\u4e0d\u5e94\u4ec5\u9650\u4e8e\u9759\u6001\u62df\u5408\uff0c\u800c\u5e94\u66f4\u91cd\u89c6\u5b9e\u9645\u5bf9\u51b2\u7ed3\u679c\u3002RLOP\u5728\u52a8\u6001\u5bf9\u51b2\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u8868\u660e\uff0c\u5c06\u590d\u5236\u5b66\u4e60\u4e0e\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\u662f\u6539\u5584\u671f\u6743\u98ce\u9669\u7ba1\u7406\u7684\u91cd\u8981\u65b9\u5411\u3002"}}
{"id": "2601.01179", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.01179", "abs": "https://arxiv.org/abs/2601.01179", "authors": ["Sokipriala Jonah", "Seong Ki Yoo", "Saurav Sthapit"], "title": "Reinforcement Learning Based Whittle Index Policy for Scheduling Wireless Sensors", "comment": "6 pages, 3 figures. To appear in the proceedings of 30th IEEE Symposium on Computers and Communications (ISCC) 2-5 July, Bologna Italy", "summary": "Wireless Sensor nodes used in remote monitoring applications typically transmit data in a timely manner, often optimising for the Age of Information (AoI). However, this approach focuses solely on keeping updates at the sink fresh without considering the actual value of each update. This method is inefficient for sensor networks, which have limited resources. Transmitting data indiscriminately without evaluating its significance not only increases energy consumption but also raises storage requirements. To address this challenge, we propose a reinforcement learning-based scheduling strategy that prioritises sensor transmissions based on the Age of Incorrect Information (AoII) using an edge mining technique. This ensures that the most valuable updates are received while reducing energy consumption. We frame the scheduling problem as a Restless Multi-Armed Bandit (RMAB) and introduce a Whittle Index-based Q-learning (WIQL) policy to dynamically select the most informative sensors. Additionally, we employ an edge mining technique, where raw sensor data is processed locally before transmission, enhancing state estimation at the sink. Experimental results demonstrate that WIQL achieves near-optimal performance while significantly reducing the number of transmitted packets by up to 70%. This reinforcement learning-based approach provides a scalable and adaptive solution for efficient data scheduling in resource-constrained WSNs.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684WIQL\u8c03\u5ea6\u7b56\u7565\uff0c\u7ed3\u5408AoII\u548c\u8fb9\u7f18\u6316\u6398\u6280\u672f\uff0c\u5728\u65e0\u7ebf\u4f20\u611f\u5668\u7f51\u7edc\u4e2d\u5b9e\u73b0\u9ad8\u6548\u6570\u636e\u4f20\u8f93\uff0c\u51cf\u5c1170%\u7684\u6570\u636e\u5305\u4f20\u8f93\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8eAoI\u7684\u65e0\u7ebf\u4f20\u611f\u5668\u7f51\u7edc\u8c03\u5ea6\u53ea\u5173\u6ce8\u4fe1\u606f\u65b0\u9c9c\u5ea6\uff0c\u4e0d\u8003\u8651\u66f4\u65b0\u6570\u636e\u7684\u5b9e\u9645\u4ef7\u503c\uff0c\u5bfc\u81f4\u8d44\u6e90\u53d7\u9650\u7684\u4f20\u611f\u5668\u7f51\u7edc\u6548\u7387\u4f4e\u4e0b\uff0c\u589e\u52a0\u80fd\u8017\u548c\u5b58\u50a8\u9700\u6c42\u3002", "method": "\u5c06\u8c03\u5ea6\u95ee\u9898\u5efa\u6a21\u4e3aRestless Multi-Armed Bandit (RMAB)\uff0c\u63d0\u51fa\u57fa\u4e8eWhittle Index\u7684Q-learning (WIQL)\u7b56\u7565\u52a8\u6001\u9009\u62e9\u6700\u6709\u4ef7\u503c\u7684\u4f20\u611f\u5668\u8282\u70b9\uff0c\u5e76\u91c7\u7528\u8fb9\u7f18\u6316\u6398\u6280\u672f\u5728\u672c\u5730\u9884\u5904\u7406\u6570\u636e\u3002", "result": "WIQL\u7b56\u7565\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5c06\u4f20\u8f93\u6570\u636e\u5305\u6570\u91cf\u51cf\u5c11\u9ad8\u8fbe70%\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u7684WSN\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u81ea\u9002\u5e94\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684WIQL\u8c03\u5ea6\u7b56\u7565\u7ed3\u5408AoII\u548c\u8fb9\u7f18\u6316\u6398\u6280\u672f\uff0c\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u65e0\u7ebf\u4f20\u611f\u5668\u7f51\u7edc\u7684\u6570\u636e\u4f20\u8f93\u6548\u7387\uff0c\u663e\u8457\u964d\u4f4e\u80fd\u8017\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u76d1\u6d4b\u5e94\u7528\u3002"}}
{"id": "2601.01311", "categories": ["math.OC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01311", "abs": "https://arxiv.org/abs/2601.01311", "authors": ["Hong T. M. Chu"], "title": "Concave Certificates: Geometric Framework for Distributionally Robust Risk and Complexity Analysis", "comment": "30 pages, 7 figures", "summary": "Distributionally Robust (DR) optimization aims to certify worst-case risk within a Wasserstein uncertainty set. Current certifications typically rely either on global Lipschitz bounds, which are often conservative, or on local gradient information, which provides only a first-order approximation. This paper introduces a novel geometric framework based on the least concave majorants of the growth rate function. Our proposed concave certificate establishes a tight bound of DR risk that remains applicable to non-Lipschitz and non-differentiable losses. We extend this framework to complexity analysis, introducing a deterministic bound that complements standard statistical generalization bound. Furthermore, we utilize this certificate to bound the gap between adversarial and empirical Rademacher complexity, demonstrating that dependencies on input diameter, network width, and depth can be eliminated. For practical application in deep learning, we introduce the adversarial score as a tractable relaxation of the concave certificate that enables efficient and layer-wise analysis of neural networks. We validate our theoretical results in various numerical experiments on classification and regression tasks on real-world data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u589e\u957f\u901f\u7387\u51fd\u6570\u6700\u5c0f\u51f9\u4e3b\u51fd\u6570\u7684\u51e0\u4f55\u6846\u67b6\uff0c\u7528\u4e8e\u5efa\u7acb\u5206\u5e03\u9c81\u68d2\u4f18\u5316\u7684\u7d27\u81f4\u8fb9\u754c\uff0c\u9002\u7528\u4e8e\u975eLipschitz\u548c\u975e\u53ef\u5fae\u635f\u5931\u51fd\u6570\uff0c\u5e76\u5e94\u7528\u4e8e\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u5bf9\u6297\u6027\u5206\u6790\u3002", "motivation": "\u5f53\u524d\u5206\u5e03\u9c81\u68d2\u4f18\u5316\u7684\u8fb9\u754c\u8ba4\u8bc1\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u57fa\u4e8e\u5168\u5c40Lipschitz\u8fb9\u754c\u7684\u65b9\u6cd5\u901a\u5e38\u8fc7\u4e8e\u4fdd\u5b88\uff0c\u800c\u57fa\u4e8e\u5c40\u90e8\u68af\u5ea6\u4fe1\u606f\u7684\u65b9\u6cd5\u4ec5\u63d0\u4f9b\u4e00\u9636\u8fd1\u4f3c\u3002\u9700\u8981\u4e00\u79cd\u66f4\u7cbe\u786e\u4e14\u9002\u7528\u4e8e\u975eLipschitz\u548c\u975e\u53ef\u5fae\u635f\u5931\u51fd\u6570\u7684\u8ba4\u8bc1\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u589e\u957f\u901f\u7387\u51fd\u6570\u6700\u5c0f\u51f9\u4e3b\u51fd\u6570\u7684\u51e0\u4f55\u6846\u67b6\uff0c\u5efa\u7acb\u51f9\u8bc1\u4e66\u6765\u83b7\u5f97\u5206\u5e03\u9c81\u68d2\u98ce\u9669\u7684\u7d27\u81f4\u8fb9\u754c\u3002\u5c06\u8be5\u6846\u67b6\u6269\u5c55\u5230\u590d\u6742\u5ea6\u5206\u6790\uff0c\u5f15\u5165\u786e\u5b9a\u6027\u8fb9\u754c\u8865\u5145\u7edf\u8ba1\u6cdb\u5316\u8fb9\u754c\u3002\u63d0\u51fa\u5bf9\u6297\u6027\u5206\u6570\u4f5c\u4e3a\u51f9\u8bc1\u4e66\u7684\u53ef\u5904\u7406\u677e\u5f1b\uff0c\u5b9e\u73b0\u795e\u7ecf\u7f51\u7edc\u7684\u9ad8\u6548\u5206\u5c42\u5206\u6790\u3002", "result": "\u51f9\u8bc1\u4e66\u5efa\u7acb\u4e86\u9002\u7528\u4e8e\u975eLipschitz\u548c\u975e\u53ef\u5fae\u635f\u5931\u7684\u7d27\u81f4DR\u98ce\u9669\u8fb9\u754c\u3002\u786e\u5b9a\u6027\u8fb9\u754c\u8865\u5145\u4e86\u7edf\u8ba1\u6cdb\u5316\u8fb9\u754c\u3002\u8bc1\u660e\u4e86\u5bf9\u6297\u6027\u548c\u7ecf\u9a8cRademacher\u590d\u6742\u5ea6\u4e4b\u95f4\u7684\u95f4\u9699\u53ef\u4ee5\u6d88\u9664\u5bf9\u8f93\u5165\u76f4\u5f84\u3001\u7f51\u7edc\u5bbd\u5ea6\u548c\u6df1\u5ea6\u7684\u4f9d\u8d56\u3002\u5bf9\u6297\u6027\u5206\u6570\u5b9e\u73b0\u4e86\u795e\u7ecf\u7f51\u7edc\u7684\u9ad8\u6548\u5206\u5c42\u5206\u6790\u3002", "conclusion": "\u63d0\u51fa\u7684\u51e0\u4f55\u6846\u67b6\u4e3a\u5206\u5e03\u9c81\u68d2\u4f18\u5316\u63d0\u4f9b\u4e86\u7d27\u81f4\u4e14\u901a\u7528\u7684\u8ba4\u8bc1\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u5bf9\u6297\u6027\u5206\u6790\u3002\u8be5\u65b9\u6cd5\u8d85\u8d8a\u4e86\u4f20\u7edfLipschitz\u548c\u68af\u5ea6\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u7684\u5206\u7c7b\u548c\u56de\u5f52\u4efb\u52a1\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002"}}
{"id": "2601.00821", "categories": ["cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.00821", "abs": "https://arxiv.org/abs/2601.00821", "authors": ["Tao An"], "title": "CogCanvas: Compression-Resistant Cognitive Artifacts for Long LLM Conversations", "comment": "15 pages, 5 figures", "summary": "Large language models face a fundamental tension between context window limits and information fidelity in long conversations. Existing approaches--truncation and summarization--either discard early information or lose nuanced details. We introduce CogCanvas, a training-free framework that extracts verbatim-grounded cognitive artifacts (decisions, facts, reminders) from conversation turns and organizes them into a temporal-aware graph for compression-resistant retrieval.\n  On the LoCoMo benchmark, CogCanvas achieves 34.7% overall accuracy, outperforming RAG (25.6%, +9.1pp) and GraphRAG (13.7%, +21.0pp). The advantage is most pronounced on temporal reasoning: 31.5% vs. 9.3% (RAG) and 5.0% (GraphRAG)--a +530% relative improvement. On multi-hop causal reasoning, CogCanvas achieves 81.0% pass rate vs. 40.0% for GraphRAG (+41.0pp). Controlled benchmarks show 97.5% recall (+78.5pp vs. summarization) with 93.0% exact match preservation.\n  While heavily-optimized approaches achieve higher absolute scores through dedicated training (EverMemOS: approximately 92%), our training-free approach provides practitioners with an immediately-deployable alternative that significantly outperforms standard baselines. Code and data: https://github.com/tao-hpu/cog-canvas.", "AI": {"tldr": "CogCanvas\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u53d6\u5bf9\u8bdd\u4e2d\u7684\u8ba4\u77e5\u6784\u4ef6\u5e76\u7ec4\u7ec7\u6210\u65f6\u5e8f\u611f\u77e5\u56fe\uff0c\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u5bf9\u8bdd\u4e2d\u7684\u4e0a\u4e0b\u6587\u9650\u5236\u548c\u4fe1\u606f\u4fdd\u771f\u5ea6\u95ee\u9898\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u5bf9\u8bdd\u4e2d\u9762\u4e34\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u548c\u4fe1\u606f\u4fdd\u771f\u5ea6\u7684\u57fa\u672c\u77db\u76fe\u3002\u73b0\u6709\u65b9\u6cd5\uff08\u622a\u65ad\u548c\u6458\u8981\uff09\u8981\u4e48\u4e22\u5f03\u65e9\u671f\u4fe1\u606f\uff0c\u8981\u4e48\u4e22\u5931\u7ec6\u8282\u4fe1\u606f\u3002", "method": "CogCanvas\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u4ece\u5bf9\u8bdd\u8f6e\u6b21\u4e2d\u63d0\u53d6\u57fa\u4e8e\u539f\u6587\u7684\u8ba4\u77e5\u6784\u4ef6\uff08\u51b3\u7b56\u3001\u4e8b\u5b9e\u3001\u63d0\u9192\uff09\uff0c\u5e76\u5c06\u5176\u7ec4\u7ec7\u6210\u65f6\u5e8f\u611f\u77e5\u56fe\uff0c\u5b9e\u73b0\u6297\u538b\u7f29\u68c0\u7d22\u3002", "result": "\u5728LoCoMo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCogCanvas\u8fbe\u523034.7%\u6574\u4f53\u51c6\u786e\u7387\uff0c\u4f18\u4e8eRAG\uff0825.6%\uff09\u548cGraphRAG\uff0813.7%\uff09\u3002\u5728\u65f6\u5e8f\u63a8\u7406\u4e0a\u4f18\u52bf\u6700\u660e\u663e\uff1a31.5% vs. 9.3%\uff08RAG\uff09\u548c5.0%\uff08GraphRAG\uff09\uff0c\u76f8\u5bf9\u63d0\u5347530%\u3002\u5728\u591a\u8df3\u56e0\u679c\u63a8\u7406\u4e0a\u8fbe\u523081.0%\u901a\u8fc7\u7387\uff0c\u800cGraphRAG\u4e3a40.0%\u3002", "conclusion": "\u867d\u7136\u7ecf\u8fc7\u5927\u91cf\u4f18\u5316\u7684\u65b9\u6cd5\u901a\u8fc7\u4e13\u95e8\u8bad\u7ec3\u80fd\u8fbe\u5230\u66f4\u9ad8\u7edd\u5bf9\u5206\u6570\uff0c\u4f46CogCanvas\u8fd9\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\u4e3a\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u53ef\u7acb\u5373\u90e8\u7f72\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u663e\u8457\u4f18\u4e8e\u6807\u51c6\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2601.01015", "categories": ["cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2601.01015", "abs": "https://arxiv.org/abs/2601.01015", "authors": ["Shiyuan Liu", "Jianwei Wang", "Xuemin Lin", "Lu Qin", "Wenjie Zhang", "Ying Zhang"], "title": "HyperJoin: LLM-augmented Hypergraph Link Prediction for Joinable Table Discovery", "comment": null, "summary": "As a pivotal task in data lake management, joinable table discovery has attracted widespread interest. While existing language model-based methods achieve remarkable performance by combining offline column representation learning with online ranking, their design insufficiently accounts for the underlying structural interactions: (1) offline, they directly model tables into isolated or pairwise columns, thereby struggling to capture the rich inter-table and intra-table structural information; and (2) online, they rank candidate columns based solely on query-candidate similarity, ignoring the mutual interactions among the candidates, leading to incoherent result sets. To address these limitations, we propose HyperJoin, a large language model (LLM)-augmented Hypergraph framework for Joinable table discovery. Specifically, we first construct a hypergraph to model tables using both the intra-table hyperedges and the LLM-augmented inter-table hyperedges. Consequently, the task of joinable table discovery is formulated as link prediction on this constructed hypergraph. We then design HIN, a Hierarchical Interaction Network that learns expressive column representations through bidirectional message passing over columns and hyperedges. To strengthen coherence and internal consistency in the result columns, we cast online ranking as a coherence-aware top-k column selection problem. We then introduce a reranking module that leverages a maximum spanning tree algorithm to prune noisy connections and maximize coherence. Experiments demonstrate the superiority of HyperJoin, achieving average improvements of 21.4% (Precision@15) and 17.2% (Recall@15) over the best baseline.", "AI": {"tldr": "HyperJoin\uff1a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u589e\u5f3a\u8d85\u56fe\u6846\u67b6\u7684Joinable\u8868\u53d1\u73b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u8d85\u56fe\u5efa\u6a21\u8868\u5185\u548c\u8868\u95f4\u7ed3\u6784\uff0c\u5c06\u4efb\u52a1\u8f6c\u5316\u4e3a\u8d85\u56fe\u94fe\u63a5\u9884\u6d4b\uff0c\u5e76\u5f15\u5165\u5c42\u6b21\u4ea4\u4e92\u7f51\u7edc\u548c\u91cd\u6392\u5e8f\u6a21\u5757\u63d0\u5347\u7ed3\u679c\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684Joinable\u8868\u53d1\u73b0\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u79bb\u7ebf\u9636\u6bb5\u5c06\u8868\u683c\u5efa\u6a21\u4e3a\u5b64\u7acb\u6216\u6210\u5bf9\u5217\uff0c\u96be\u4ee5\u6355\u6349\u4e30\u5bcc\u7684\u8868\u95f4\u548c\u8868\u5185\u7ed3\u6784\u4fe1\u606f\uff1b2\uff09\u5728\u7ebf\u6392\u5e8f\u4ec5\u57fa\u4e8e\u67e5\u8be2-\u5019\u9009\u76f8\u4f3c\u5ea6\uff0c\u5ffd\u7565\u5019\u9009\u5217\u4e4b\u95f4\u7684\u76f8\u4e92\u5f71\u54cd\uff0c\u5bfc\u81f4\u7ed3\u679c\u96c6\u4e0d\u8fde\u8d2f\u3002", "method": "1\uff09\u6784\u5efa\u8d85\u56fe\uff0c\u5305\u542b\u8868\u5185\u8d85\u8fb9\u548cLLM\u589e\u5f3a\u7684\u8868\u95f4\u8d85\u8fb9\uff1b2\uff09\u5c06Joinable\u8868\u53d1\u73b0\u4efb\u52a1\u8f6c\u5316\u4e3a\u8d85\u56fe\u94fe\u63a5\u9884\u6d4b\uff1b3\uff09\u8bbe\u8ba1\u5c42\u6b21\u4ea4\u4e92\u7f51\u7edc\uff08HIN\uff09\uff0c\u901a\u8fc7\u5217\u548c\u8d85\u8fb9\u7684\u53cc\u5411\u6d88\u606f\u4f20\u9012\u5b66\u4e60\u8868\u8fbe\u6027\u5217\u8868\u793a\uff1b4\uff09\u5c06\u5728\u7ebf\u6392\u5e8f\u8f6c\u5316\u4e3a\u4e00\u81f4\u6027\u611f\u77e5\u7684top-k\u5217\u9009\u62e9\u95ee\u9898\uff1b5\uff09\u5f15\u5165\u91cd\u6392\u5e8f\u6a21\u5757\uff0c\u4f7f\u7528\u6700\u5927\u751f\u6210\u6811\u7b97\u6cd5\u526a\u679d\u566a\u58f0\u8fde\u63a5\u5e76\u6700\u5927\u5316\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eHyperJoin\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728Precision@15\u548cRecall@15\u6307\u6807\u4e0a\u5206\u522b\u5e73\u5747\u63d0\u534721.4%\u548c17.2%\u3002", "conclusion": "HyperJoin\u901a\u8fc7\u8d85\u56fe\u5efa\u6a21\u548c\u5c42\u6b21\u4ea4\u4e92\u7f51\u7edc\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7ed3\u6784\u4fe1\u606f\u6355\u6349\u548c\u7ed3\u679c\u4e00\u81f4\u6027\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u663e\u8457\u63d0\u5347\u4e86Joinable\u8868\u53d1\u73b0\u7684\u6027\u80fd\u3002"}}
{"id": "2601.01331", "categories": ["cs.CY", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01331", "abs": "https://arxiv.org/abs/2601.01331", "authors": ["Hongkun Yang", "Lionel Z. Wang", "Wei Fan", "Yiran Hu", "Lixu Wang", "Chenyu Liu", "Shenghong Fu", "Haoyang Li", "Xin Xu", "Jiexin Zheng", "Wei Dong"], "title": "AppellateGen: A Benchmark for Appellate Legal Judgment Generation", "comment": "15 pages, 4 figures, 3 tables", "summary": "Legal judgment generation is a critical task in legal intelligence. However, existing research in legal judgment generation has predominantly focused on first-instance trials, relying on static fact-to-verdict mappings while neglecting the dialectical nature of appellate (second-instance) review. To address this, we introduce AppellateGen, a benchmark for second-instance legal judgment generation comprising 7,351 case pairs. The task requires models to draft legally binding judgments by reasoning over the initial verdict and evidentiary updates, thereby modeling the causal dependency between trial stages. We further propose a judicial Standard Operating Procedure (SOP)-based Legal Multi-Agent System (SLMAS) to simulate judicial workflows, which decomposes the generation process into discrete stages of issue identification, retrieval, and drafting. Experimental results indicate that while SLMAS improves logical consistency, the complexity of appellate reasoning remains a substantial challenge for current LLMs. The dataset and code are publicly available at: https://anonymous.4open.science/r/AppellateGen-5763.", "AI": {"tldr": "\u63d0\u51fa\u4e86AppellateGen\u57fa\u51c6\uff0c\u7528\u4e8e\u4e0a\u8bc9\u5ba1\u6cd5\u5f8b\u5224\u51b3\u751f\u6210\uff0c\u5305\u542b7,351\u4e2a\u6848\u4ef6\u5bf9\uff0c\u5e76\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u53f8\u6cd5\u6807\u51c6\u64cd\u4f5c\u7a0b\u5e8f\u7684\u6cd5\u5f8b\u591a\u667a\u80fd\u4f53\u7cfb\u7edf(SLMAS)\u6765\u6a21\u62df\u53f8\u6cd5\u5de5\u4f5c\u6d41\u7a0b\u3002", "motivation": "\u73b0\u6709\u6cd5\u5f8b\u5224\u51b3\u751f\u6210\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u4e00\u5ba1\u5ba1\u5224\uff0c\u4f9d\u8d56\u9759\u6001\u7684\u4e8b\u5b9e\u5230\u5224\u51b3\u6620\u5c04\uff0c\u5ffd\u7565\u4e86\u4e0a\u8bc9\u5ba1\u7684\u8fa9\u8bc1\u6027\u8d28\u3002\u9700\u8981\u89e3\u51b3\u4e0a\u8bc9\u5ba1\u6cd5\u5f8b\u5224\u51b3\u751f\u6210\u4e2d\u8003\u8651\u521d\u59cb\u5224\u51b3\u548c\u8bc1\u636e\u66f4\u65b0\u7684\u56e0\u679c\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u53f8\u6cd5\u6807\u51c6\u64cd\u4f5c\u7a0b\u5e8f(SOP)\u7684\u6cd5\u5f8b\u591a\u667a\u80fd\u4f53\u7cfb\u7edf(SLMAS)\uff0c\u5c06\u751f\u6210\u8fc7\u7a0b\u5206\u89e3\u4e3a\u95ee\u9898\u8bc6\u522b\u3001\u68c0\u7d22\u548c\u8d77\u8349\u4e09\u4e2a\u79bb\u6563\u9636\u6bb5\uff0c\u6a21\u62df\u53f8\u6cd5\u5de5\u4f5c\u6d41\u7a0b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u867d\u7136SLMAS\u63d0\u9ad8\u4e86\u903b\u8f91\u4e00\u81f4\u6027\uff0c\u4f46\u4e0a\u8bc9\u63a8\u7406\u7684\u590d\u6742\u6027\u5bf9\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u4ecd\u7136\u6784\u6210\u91cd\u5927\u6311\u6218\u3002\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5df2\u516c\u5f00\u3002", "conclusion": "AppellateGen\u57fa\u51c6\u586b\u8865\u4e86\u4e0a\u8bc9\u5ba1\u6cd5\u5f8b\u5224\u51b3\u751f\u6210\u7684\u7a7a\u767d\uff0cSLMAS\u65b9\u6cd5\u901a\u8fc7\u6a21\u62df\u53f8\u6cd5\u5de5\u4f5c\u6d41\u7a0b\u63d0\u9ad8\u4e86\u903b\u8f91\u4e00\u81f4\u6027\uff0c\u4f46\u4e0a\u8bc9\u63a8\u7406\u7684\u590d\u6742\u6027\u4ecd\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2601.01147", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01147", "abs": "https://arxiv.org/abs/2601.01147", "authors": ["Johan Hallberg Szabadv\u00e1ry"], "title": "Conformal Blindness: A Note on $A$-Cryptic change-points", "comment": "6 pages, 3 figures", "summary": "Conformal Test Martingales (CTMs) are a standard method within the Conformal Prediction framework for testing the crucial assumption of data exchangeability by monitoring deviations from uniformity in the p-value sequence. Although exchangeability implies uniform p-values, the converse does not hold. This raises the question of whether a significant break in exchangeability can occur, such that the p-values remain uniform, rendering CTMs blind. We answer this affirmatively, demonstrating the phenomenon of \\emph{conformal blindness}.\n  Through explicit construction, for the theoretically ideal ``oracle'' conformity measure (given by the true conditional density), we demonstrate the possibility of an \\emph{$A$-cryptic change-point} (where $A$ refers to the conformity measure). Using bivariate Gaussian distributions, we identify a line along which a change in the marginal means does not alter the distribution of the conformity scores, thereby producing perfectly uniform p-values.\n  Simulations confirm that even a massive distribution shift can be perfectly cryptic to the CTM, highlighting a fundamental limitation and emphasising the critical role of the alignment of the conformity measure with potential shifts.", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u4e86\u5171\u5f62\u6d4b\u8bd5\u9785(CTMs)\u7684\u4e00\u4e2a\u6839\u672c\u5c40\u9650\u6027\u2014\u2014\u5171\u5f62\u76f2\u533a\u73b0\u8c61\uff0c\u5373\u5373\u4f7f\u6570\u636e\u4ea4\u6362\u6027\u53d1\u751f\u663e\u8457\u7834\u574f\uff0cp\u503c\u5e8f\u5217\u4ecd\u53ef\u80fd\u4fdd\u6301\u5747\u5300\u5206\u5e03\uff0c\u5bfc\u81f4CTMs\u65e0\u6cd5\u68c0\u6d4b\u5230\u8fd9\u79cd\u53d8\u5316\u3002", "motivation": "\u5171\u5f62\u6d4b\u8bd5\u9785(CTMs)\u662f\u5171\u5f62\u9884\u6d4b\u6846\u67b6\u4e2d\u7528\u4e8e\u68c0\u9a8c\u6570\u636e\u4ea4\u6362\u6027\u5047\u8bbe\u7684\u6807\u51c6\u65b9\u6cd5\uff0c\u901a\u8fc7\u76d1\u6d4bp\u503c\u5e8f\u5217\u504f\u79bb\u5747\u5300\u5206\u5e03\u7684\u7a0b\u5ea6\u6765\u68c0\u6d4b\u4ea4\u6362\u6027\u7834\u574f\u3002\u7136\u800c\uff0c\u4ea4\u6362\u6027\u610f\u5473\u7740\u5747\u5300p\u503c\uff0c\u4f46\u5747\u5300p\u503c\u4e0d\u4e00\u5b9a\u610f\u5473\u7740\u4ea4\u6362\u6027\u3002\u8fd9\u5f15\u53d1\u4e86\u4e00\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u662f\u5426\u5b58\u5728\u4ea4\u6362\u6027\u7684\u663e\u8457\u7834\u574f\uff0c\u4f46p\u503c\u4ecd\u4fdd\u6301\u5747\u5300\u5206\u5e03\uff0c\u4ece\u800c\u4f7fCTMs\u5931\u6548\uff1f", "method": "1. \u901a\u8fc7\u7406\u8bba\u6784\u9020\uff0c\u9488\u5bf9\u7406\u8bba\u7406\u60f3\u7684\"\u9884\u8a00\u673a\"\u4e00\u81f4\u6027\u5ea6\u91cf\uff08\u7531\u771f\u5b9e\u6761\u4ef6\u5bc6\u5ea6\u7ed9\u51fa\uff09\uff0c\u8bc1\u660e\u4e86A-\u9690\u79d8\u53d8\u5316\u70b9\u7684\u53ef\u80fd\u6027\uff1b2. \u4f7f\u7528\u4e8c\u5143\u9ad8\u65af\u5206\u5e03\uff0c\u8bc6\u522b\u51fa\u4e00\u6761\u8fb9\u9645\u5747\u503c\u53d8\u5316\u4f46\u4e0d\u6539\u53d8\u4e00\u81f4\u6027\u5f97\u5206\u5206\u5e03\u7684\u76f4\u7ebf\uff0c\u4ece\u800c\u4ea7\u751f\u5b8c\u5168\u5747\u5300\u7684p\u503c\uff1b3. \u901a\u8fc7\u6a21\u62df\u9a8c\u8bc1\u5373\u4f7f\u5927\u89c4\u6a21\u5206\u5e03\u504f\u79fb\u4e5f\u53ef\u80fd\u5bf9CTM\u5b8c\u5168\u9690\u79d8\u3002", "result": "1. \u786e\u8ba4\u4e86\u5171\u5f62\u76f2\u533a\u73b0\u8c61\u7684\u5b58\u5728\u2014\u2014\u5373\u4f7f\u6570\u636e\u4ea4\u6362\u6027\u53d1\u751f\u663e\u8457\u7834\u574f\uff0cp\u503c\u5e8f\u5217\u4ecd\u53ef\u80fd\u4fdd\u6301\u5747\u5300\u5206\u5e03\uff1b2. \u901a\u8fc7\u4e8c\u5143\u9ad8\u65af\u5206\u5e03\u7684\u6784\u9020\uff0c\u5c55\u793a\u4e86\u8fb9\u9645\u5747\u503c\u53d8\u5316\u4f46\u4e0d\u5f71\u54cd\u4e00\u81f4\u6027\u5f97\u5206\u5206\u5e03\u7684\u5177\u4f53\u60c5\u51b5\uff1b3. \u6a21\u62df\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u5927\u89c4\u6a21\u5206\u5e03\u504f\u79fb\u4e5f\u53ef\u80fd\u5bf9CTM\u5b8c\u5168\u9690\u79d8\uff0c\u63ed\u793a\u4e86CTMs\u7684\u6839\u672c\u5c40\u9650\u6027\u3002", "conclusion": "\u5171\u5f62\u6d4b\u8bd5\u9785\u5b58\u5728\u6839\u672c\u5c40\u9650\u6027\uff0c\u5373\u5171\u5f62\u76f2\u533a\u73b0\u8c61\uff0c\u4f7f\u5f97\u67d0\u4e9b\u7c7b\u578b\u7684\u5206\u5e03\u53d8\u5316\u65e0\u6cd5\u88ab\u68c0\u6d4b\u3002\u8fd9\u5f3a\u8c03\u4e86\u5728\u5e94\u7528CTMs\u65f6\uff0c\u4e00\u81f4\u6027\u5ea6\u91cf\u4e0e\u6f5c\u5728\u5206\u5e03\u504f\u79fb\u7684\u5bf9\u9f50\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u4ed4\u7ec6\u9009\u62e9\u4e00\u81f4\u6027\u5ea6\u91cf\u4ee5\u907f\u514d\u76f2\u533a\u3002"}}
{"id": "2601.00841", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00841", "abs": "https://arxiv.org/abs/2601.00841", "authors": ["Bharath Nunepalli"], "title": "SLO-Conditioned Action Routing for Retrieval-Augmented Generation: Objective Ablation and Failure Modes", "comment": null, "summary": "Retrieval-augmented generation (RAG) introduces a practical control problem: retrieval depth and generation behavior must be chosen per query to satisfy service-level objectives (SLOs) such as cost, refusal rate, and hallucination risk. This work models per-query control as a small discrete action: choose a retrieval depth and a generation mode (guarded vs. auto), or refuse. An offline logged dataset is constructed from SQuAD 2.0 by executing each action and recording accuracy, token cost, hallucination/refusal indicators, and an SLO-weighted reward. Two simple policy-learning objectives are evaluated: supervised classification of the per-state best action (Argmax-CE) and a reward-weighted variant (Argmax-CE-WT). Across the evaluated settings, a strong fixed baseline (low k, guarded prompting) performs competitively; learned policies mainly provide additional cost savings under a quality-focused SLO and can exhibit refusal collapse under a cheap SLO when refusal is heavily rewarded. The contribution is a reproducible case study of SLO-aware control for RAG pipelines, emphasizing failure modes and reporting conventions rather than proposing a new retriever or language model.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86RAG\u7cfb\u7edf\u4e2d\u7684\u67e5\u8be2\u7ea7\u63a7\u5236\u95ee\u9898\uff0c\u63d0\u51fa\u901a\u8fc7\u9009\u62e9\u68c0\u7d22\u6df1\u5ea6\u548c\u751f\u6210\u6a21\u5f0f\uff08\u9632\u62a4vs\u81ea\u52a8\uff09\u6216\u62d2\u7edd\u6765\u6ee1\u8db3\u6210\u672c\u3001\u62d2\u7edd\u7387\u548c\u5e7b\u89c9\u98ce\u9669\u7b49SLO\u76ee\u6807\uff0c\u5e76\u901a\u8fc7\u79bb\u7ebf\u6570\u636e\u96c6\u8bc4\u4f30\u4e86\u4e24\u79cd\u7b80\u5355\u7684\u7b56\u7565\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "RAG\u7cfb\u7edf\u9762\u4e34\u4e00\u4e2a\u5b9e\u9645\u7684\u63a7\u5236\u95ee\u9898\uff1a\u9700\u8981\u4e3a\u6bcf\u4e2a\u67e5\u8be2\u9009\u62e9\u9002\u5f53\u7684\u68c0\u7d22\u6df1\u5ea6\u548c\u751f\u6210\u884c\u4e3a\uff0c\u4ee5\u6ee1\u8db3\u6210\u672c\u3001\u62d2\u7edd\u7387\u548c\u5e7b\u89c9\u98ce\u9669\u7b49\u670d\u52a1\u7ea7\u522b\u76ee\u6807\uff08SLOs\uff09\u3002\u5f53\u524d\u7f3a\u4e4f\u5bf9\u8fd9\u7c7b\u63a7\u5236\u95ee\u9898\u7684\u7cfb\u7edf\u6027\u7814\u7a76\u3002", "method": "\u5c06\u6bcf\u4e2a\u67e5\u8be2\u7684\u63a7\u5236\u5efa\u6a21\u4e3a\u79bb\u6563\u52a8\u4f5c\uff1a\u9009\u62e9\u68c0\u7d22\u6df1\u5ea6\u548c\u751f\u6210\u6a21\u5f0f\uff08\u9632\u62a4vs\u81ea\u52a8\uff09\uff0c\u6216\u62d2\u7edd\u3002\u4f7f\u7528SQuAD 2.0\u6784\u5efa\u79bb\u7ebf\u65e5\u5fd7\u6570\u636e\u96c6\uff0c\u8bb0\u5f55\u6bcf\u4e2a\u52a8\u4f5c\u7684\u51c6\u786e\u6027\u3001token\u6210\u672c\u3001\u5e7b\u89c9/\u62d2\u7edd\u6307\u6807\u548cSLO\u52a0\u6743\u5956\u52b1\u3002\u8bc4\u4f30\u4e24\u79cd\u7b80\u5355\u7684\u7b56\u7565\u5b66\u4e60\u76ee\u6807\uff1a\u57fa\u4e8e\u6bcf\u4e2a\u72b6\u6001\u6700\u4f73\u52a8\u4f5c\u7684\u76d1\u7763\u5206\u7c7b\uff08Argmax-CE\uff09\u53ca\u5176\u5956\u52b1\u52a0\u6743\u53d8\u4f53\uff08Argmax-CE-WT\uff09\u3002", "result": "\u5728\u8bc4\u4f30\u7684\u8bbe\u7f6e\u4e2d\uff0c\u4e00\u4e2a\u56fa\u5b9a\u7684\u5f3a\u57fa\u7ebf\uff08\u4f4ek\u3001\u9632\u62a4\u63d0\u793a\uff09\u8868\u73b0\u5177\u6709\u7ade\u4e89\u529b\uff1b\u5b66\u4e60\u5230\u7684\u7b56\u7565\u4e3b\u8981\u5728\u8d28\u91cf\u5bfc\u5411\u7684SLO\u4e0b\u63d0\u4f9b\u989d\u5916\u7684\u6210\u672c\u8282\u7701\uff0c\u800c\u5728\u5ec9\u4ef7SLO\u4e0b\u5f53\u62d2\u7edd\u88ab\u9ad8\u5ea6\u5956\u52b1\u65f6\u53ef\u80fd\u51fa\u73b0\u62d2\u7edd\u5d29\u6e83\u73b0\u8c61\u3002", "conclusion": "\u672c\u6587\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u590d\u73b0\u7684RAG\u7ba1\u9053SLO\u611f\u77e5\u63a7\u5236\u6848\u4f8b\u7814\u7a76\uff0c\u5f3a\u8c03\u5931\u8d25\u6a21\u5f0f\u548c\u62a5\u544a\u89c4\u8303\uff0c\u800c\u4e0d\u662f\u63d0\u51fa\u65b0\u7684\u68c0\u7d22\u5668\u6216\u8bed\u8a00\u6a21\u578b\u3002\u7814\u7a76\u63ed\u793a\u4e86\u7b80\u5355\u7b56\u7565\u5b66\u4e60\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u548c\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u6311\u6218\u3002"}}
{"id": "2601.02276", "categories": ["q-fin.MF", "math.PR"], "pdf": "https://arxiv.org/pdf/2601.02276", "abs": "https://arxiv.org/abs/2601.02276", "authors": ["Wing Fung Chong", "Roxana Dumitrescu", "Gechun Liang", "Kenneth Tsz Hin Ng"], "title": "Forward Performance Processes under Multiple Default Risks", "comment": null, "summary": "This article constructs a forward exponential utility in a market with multiple defaultable risks. Using the Jacod-Pham decomposition for random fields, we first characterize forward performance processes in a defaultable market under the default-free filtration. We then construct a forward utility via a system of recursively defined, indexed infinite-horizon backward stochastic differential equations (BSDEs) with discounting, and establish the existence, uniqueness, and boundedness of their solutions. To verify the required (super)martingale property of the performance process, we develop a rigorous characterization of this property with respect to the general filtration in terms of a set of (in)equalities relative to the default-free filtration. We further extend the analysis to a stochastic factor model with ergodic dynamics. In this setting, we derive uniform bounds for the Markovian solutions of the infinite-horizon BSDEs, overcoming technical challenges arising from the special structure of the system of BSDEs in the defaultable setting. Passing to the ergodic limit, we identify the limiting BSDE and relate its constant to the risk-sensitive long-run growth rate of the optimal wealth process.", "AI": {"tldr": "\u6784\u5efa\u5177\u6709\u591a\u4e2a\u53ef\u8fdd\u7ea6\u98ce\u9669\u5e02\u573a\u4e2d\u7684\u524d\u5411\u6307\u6570\u6548\u7528\uff0c\u901a\u8fc7\u9012\u5f52\u5b9a\u4e49\u7684\u65e0\u9650\u65f6\u57dfBSDE\u7cfb\u7edf\uff0c\u5e76\u6269\u5c55\u5230\u5177\u6709\u904d\u5386\u52a8\u6001\u7684\u968f\u673a\u56e0\u5b50\u6a21\u578b\u3002", "motivation": "\u5728\u591a\u53ef\u8fdd\u7ea6\u98ce\u9669\u5e02\u573a\u4e2d\uff0c\u9700\u8981\u6784\u5efa\u524d\u5411\u6548\u7528\u8fc7\u7a0b\u6765\u8bc4\u4f30\u6295\u8d44\u7ee9\u6548\u3002\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u8fdd\u7ea6\u98ce\u9669\u548c\u4e00\u822c\u8fc7\u6ee4\u4e0b\u7684\u6027\u80fd\u8fc7\u7a0b\u7279\u6027\u3002", "method": "\u4f7f\u7528Jacod-Pham\u5206\u89e3\u63cf\u8ff0\u968f\u673a\u573a\uff0c\u5728\u65e0\u8fdd\u7ea6\u8fc7\u6ee4\u4e0b\u523b\u753b\u524d\u5411\u6027\u80fd\u8fc7\u7a0b\u3002\u901a\u8fc7\u9012\u5f52\u5b9a\u4e49\u7684\u65e0\u9650\u65f6\u57dfBSDE\u7cfb\u7edf\u6784\u5efa\u524d\u5411\u6548\u7528\uff0c\u5e76\u6269\u5c55\u5230\u5177\u6709\u904d\u5386\u52a8\u6001\u7684\u968f\u673a\u56e0\u5b50\u6a21\u578b\u3002", "result": "\u5efa\u7acb\u4e86BSDE\u89e3\u7684\u5b58\u5728\u6027\u3001\u552f\u4e00\u6027\u548c\u6709\u754c\u6027\uff0c\u5f00\u53d1\u4e86\u6027\u80fd\u8fc7\u7a0b\u5728\u4e00\u822c\u8fc7\u6ee4\u4e0b\u7684\u4e25\u683c\u523b\u753b\u65b9\u6cd5\uff0c\u5728\u904d\u5386\u6781\u9650\u4e0b\u8bc6\u522b\u4e86\u6781\u9650BSDE\u53ca\u5176\u4e0e\u98ce\u9669\u654f\u611f\u957f\u671f\u589e\u957f\u7387\u7684\u5173\u7cfb\u3002", "conclusion": "\u6210\u529f\u6784\u5efa\u4e86\u591a\u53ef\u8fdd\u7ea6\u98ce\u9669\u5e02\u573a\u4e2d\u7684\u524d\u5411\u6307\u6570\u6548\u7528\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u6280\u672f\u6311\u6218\uff0c\u4e3a\u98ce\u9669\u654f\u611f\u957f\u671f\u7ee9\u6548\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2601.01256", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.01256", "abs": "https://arxiv.org/abs/2601.01256", "authors": ["Jinzhou Xu", "Yuanxin Zhuo", "Paola Tapia"], "title": "Multi-Objective Operational Optimization of Energy Storage Systems in User-Side Microgrids", "comment": "9 pages, 9 figures, journal", "summary": "An operational optimization strategy for microgrid energy storage systems (ESSs) is developed to address practical user-oriented application requirements, and its effectiveness is validated using real-world operational data. First, a fundamental ESS model is established to characterize system dynamics and operational constraints, providing a theoretical basis for optimization. Subsequently, a multi-objective operational optimization framework is formulated to simultaneously minimize electricity cost, reduce carbon emissions, and enhance renewable energy utilization. To ensure computational efficiency and scalability, the commercial optimization solver Gurobi is employed. The proposed strategy is evaluated using actual microgrid operational data, demonstrating that the developed ESS model accurately represents real system constraints. Compared with existing user operational strategies, the proposed approach achieves an average reduction of 13.47% in electricity cost. Moreover, by dynamically adjusting the weighting factors of the multi-objective formulation, the strategy enables flexible operational modes and significantly improves adaptability to varying operating scenarios. In addition, the proposed framework provides decision support for user-side microgrids participating in surplus electricity feed-in policies. The main contribution of this work lies in its user-centric optimization design, which enhances operational flexibility and scenario adaptability through multi-objective weight allocation, offering a practical and scalable solution for real-world microgrid ESS operation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u7528\u6237\u7684\u5fae\u7535\u7f51\u50a8\u80fd\u7cfb\u7edf\u8fd0\u884c\u4f18\u5316\u7b56\u7565\uff0c\u901a\u8fc7\u591a\u76ee\u6807\u4f18\u5316\u6846\u67b6\u964d\u4f4e\u7535\u8d39\u6210\u672c\u3001\u51cf\u5c11\u78b3\u6392\u653e\u5e76\u63d0\u9ad8\u53ef\u518d\u751f\u80fd\u6e90\u5229\u7528\u7387\uff0c\u5728\u5b9e\u9645\u8fd0\u884c\u6570\u636e\u9a8c\u8bc1\u4e2d\u5b9e\u73b0\u4e8613.47%\u7684\u5e73\u5747\u7535\u8d39\u964d\u4f4e\u3002", "motivation": "\u9488\u5bf9\u5fae\u7535\u7f51\u50a8\u80fd\u7cfb\u7edf\u5728\u5b9e\u9645\u7528\u6237\u4fa7\u5e94\u7528\u4e2d\u7684\u9700\u6c42\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u8003\u8651\u7ecf\u6d4e\u6027\u3001\u73af\u4fdd\u6027\u548c\u53ef\u518d\u751f\u80fd\u6e90\u5229\u7528\u7684\u4f18\u5316\u7b56\u7565\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u7528\u6237\u8fd0\u884c\u7b56\u7565\u5728\u7075\u6d3b\u6027\u3001\u9002\u5e94\u6027\u548c\u591a\u76ee\u6807\u5e73\u8861\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "method": "\u9996\u5148\u5efa\u7acb\u57fa\u7840ESS\u6a21\u578b\u63cf\u8ff0\u7cfb\u7edf\u52a8\u6001\u548c\u8fd0\u884c\u7ea6\u675f\uff0c\u7136\u540e\u6784\u5efa\u591a\u76ee\u6807\u8fd0\u884c\u4f18\u5316\u6846\u67b6\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u7535\u8d39\u6210\u672c\u3001\u51cf\u5c11\u78b3\u6392\u653e\u5e76\u63d0\u9ad8\u53ef\u518d\u751f\u80fd\u6e90\u5229\u7528\u7387\uff0c\u4f7f\u7528Gurobi\u4f18\u5316\u6c42\u89e3\u5668\u786e\u4fdd\u8ba1\u7b97\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "result": "\u4f7f\u7528\u5b9e\u9645\u5fae\u7535\u7f51\u8fd0\u884c\u6570\u636e\u9a8c\u8bc1\uff0cESS\u6a21\u578b\u80fd\u51c6\u786e\u8868\u5f81\u771f\u5b9e\u7cfb\u7edf\u7ea6\u675f\uff1b\u76f8\u6bd4\u73b0\u6709\u7528\u6237\u8fd0\u884c\u7b56\u7565\uff0c\u5e73\u5747\u964d\u4f4e\u7535\u8d39\u6210\u672c13.47%\uff1b\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u591a\u76ee\u6807\u6743\u91cd\u56e0\u5b50\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8fd0\u884c\u7075\u6d3b\u6027\u548c\u573a\u666f\u9002\u5e94\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u7528\u6237\u4e2d\u5fc3\u4f18\u5316\u8bbe\u8ba1\u901a\u8fc7\u591a\u76ee\u6807\u6743\u91cd\u5206\u914d\u589e\u5f3a\u4e86\u8fd0\u884c\u7075\u6d3b\u6027\u548c\u573a\u666f\u9002\u5e94\u6027\uff0c\u4e3a\u5b9e\u9645\u5fae\u7535\u7f51ESS\u8fd0\u884c\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e3a\u7528\u6237\u4fa7\u5fae\u7535\u7f51\u53c2\u4e0e\u4f59\u7535\u4e0a\u7f51\u653f\u7b56\u63d0\u4f9b\u4e86\u51b3\u7b56\u652f\u6301\u3002"}}
{"id": "2601.01385", "categories": ["math.OC", "math.DS"], "pdf": "https://arxiv.org/pdf/2601.01385", "abs": "https://arxiv.org/abs/2601.01385", "authors": ["Ziheng Jiao", "Chengshuai Wu", "Bo Fan", "Meng Zhang", "Romeo Ortega"], "title": "On IDA-PBC with Maximum Energy Shapeability", "comment": "7 pages, 2 figures", "summary": "Interconnection and Damping Assignment Passivity-Based Control (IDA-PBC) is a well-established stabilization technique for affine nonlinear systems. However, its application is generally hindered by the requirement of solving a set of partial differential equations (PDEs), i.e., the so-called matching equation. This paper introduces the notion of \\emph{maximum energy shapeability} which describes the scenario that the homogeneous part of the matching equation admits $m$ independent solutions with $m$ the dimension of the control input. We demonstrate that the maximum energy shapeability enables a systematic procedure for the IDA-PBC design by transforming the matching equation into a set of easier-to-solve PDEs. Sufficient conditions for maximum energy shapeability are also provided. It is shown that some existing constructive IDA-PBC designs actually implicitly exploit the maximum energy shapeability. The proposed procedure for the IDA-PBC design is illustrated with the magnetic levitation system.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\"\u6700\u5927\u80fd\u91cf\u53ef\u5851\u6027\"\u6982\u5ff5\uff0c\u7528\u4e8e\u7b80\u5316IDA-PBC\u63a7\u5236\u8bbe\u8ba1\u4e2d\u7684\u5339\u914d\u65b9\u7a0b\u6c42\u89e3\u95ee\u9898\uff0c\u4f7f\u539f\u672c\u590d\u6742\u7684\u504f\u5fae\u5206\u65b9\u7a0b\u7ec4\u8f6c\u5316\u4e3a\u66f4\u6613\u6c42\u89e3\u7684\u5f62\u5f0f\u3002", "motivation": "IDA-PBC\u662f\u4e00\u79cd\u6210\u719f\u7684\u975e\u7ebf\u6027\u7cfb\u7edf\u7a33\u5b9a\u5316\u6280\u672f\uff0c\u4f46\u5176\u5e94\u7528\u53d7\u5230\u5339\u914d\u65b9\u7a0b\u6c42\u89e3\u56f0\u96be\u7684\u9650\u5236\u3002\u5339\u914d\u65b9\u7a0b\u662f\u4e00\u7ec4\u504f\u5fae\u5206\u65b9\u7a0b\uff0c\u6c42\u89e3\u590d\u6742\u5ea6\u9ad8\uff0c\u963b\u788d\u4e86IDA-PBC\u7684\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u5f15\u5165\"\u6700\u5927\u80fd\u91cf\u53ef\u5851\u6027\"\u6982\u5ff5\uff0c\u5f53\u5339\u914d\u65b9\u7a0b\u7684\u9f50\u6b21\u90e8\u5206\u5177\u6709m\u4e2a\u72ec\u7acb\u89e3\u65f6\uff08m\u4e3a\u63a7\u5236\u8f93\u5165\u7ef4\u5ea6\uff09\uff0c\u53ef\u4ee5\u5c06\u5339\u914d\u65b9\u7a0b\u8f6c\u5316\u4e3a\u66f4\u6613\u6c42\u89e3\u7684\u504f\u5fae\u5206\u65b9\u7a0b\u7ec4\u3002\u63d0\u4f9b\u4e86\u6700\u5927\u80fd\u91cf\u53ef\u5851\u6027\u7684\u5145\u5206\u6761\u4ef6\u3002", "result": "\u8bc1\u660e\u4e86\u6700\u5927\u80fd\u91cf\u53ef\u5851\u6027\u80fd\u591f\u4e3aIDA-PBC\u8bbe\u8ba1\u63d0\u4f9b\u7cfb\u7edf\u5316\u7a0b\u5e8f\uff0c\u5c06\u5339\u914d\u65b9\u7a0b\u8f6c\u5316\u4e3a\u66f4\u6613\u6c42\u89e3\u7684\u5f62\u5f0f\u3002\u5c55\u793a\u4e86\u73b0\u6709\u7684\u4e00\u4e9b\u6784\u9020\u6027IDA-PBC\u8bbe\u8ba1\u5b9e\u9645\u4e0a\u9690\u542b\u5730\u5229\u7528\u4e86\u6700\u5927\u80fd\u91cf\u53ef\u5851\u6027\u3002\u901a\u8fc7\u78c1\u60ac\u6d6e\u7cfb\u7edf\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6700\u5927\u80fd\u91cf\u53ef\u5851\u6027\u6982\u5ff5\u4e3aIDA-PBC\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u7cfb\u7edf\u5316\u65b9\u6cd5\uff0c\u7b80\u5316\u4e86\u5339\u914d\u65b9\u7a0b\u7684\u6c42\u89e3\u8fc7\u7a0b\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8IDA-PBC\u5728\u5b9e\u9645\u975e\u7ebf\u6027\u63a7\u5236\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2601.00823", "categories": ["cs.AI", "cs.IT", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.00823", "abs": "https://arxiv.org/abs/2601.00823", "authors": ["Austin R. Ellis-Mohr", "Max Hartman", "Lav R. Varshney"], "title": "Energy-Aware Routing to Large Reasoning Models", "comment": null, "summary": "Large reasoning models (LRMs) have heterogeneous inference energy costs based on which model is used and how much it reasons. To reduce energy, it is important to choose the right LRM and operate it in the right way. As a result, the performance of systems that dispatch tasks to different individual LRMs depend on the balance between mean energy provisioning and stochastic fluctuations. The critical regime is the unique operating point at which neither auxiliary energy nor baseline energy is systematically wasted. Increasing baseline supply shifts the system toward persistent over-supply and baseline-energy waste, while reducing supply induces persistent reliance on auxiliary energy. Yet in this regime, performance remains volatility-limited and so a second-order characterization provides further insights that we develop. Here, performance is governed by how variability is absorbed across time, models, and execution choices. This perspective highlights variance-aware routing and dispatch as a principled design axis, and provides a theoretical basis for developing energy-aware model routing policies. Routing behavior is characterized when dispatch policies are based on training-compute and inference-compute scaling laws for LRMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5728\u5927\u578b\u63a8\u7406\u6a21\u578b\u7cfb\u7edf\u4e2d\uff0c\u901a\u8fc7\u65b9\u5dee\u611f\u77e5\u7684\u8def\u7531\u548c\u8c03\u5ea6\u7b56\u7565\u6765\u4f18\u5316\u80fd\u6e90\u6548\u7387\uff0c\u5728\u4e34\u754c\u72b6\u6001\u4e0b\u5e73\u8861\u57fa\u7840\u80fd\u6e90\u548c\u8f85\u52a9\u80fd\u6e90\u7684\u4f7f\u7528\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u5177\u6709\u5f02\u6784\u7684\u63a8\u7406\u80fd\u8017\uff0c\u7cfb\u7edf\u6027\u80fd\u53d6\u51b3\u4e8e\u5e73\u5747\u80fd\u6e90\u4f9b\u5e94\u4e0e\u968f\u673a\u6ce2\u52a8\u4e4b\u95f4\u7684\u5e73\u8861\u3002\u9700\u8981\u627e\u5230\u65e2\u80fd\u907f\u514d\u57fa\u7840\u80fd\u6e90\u6d6a\u8d39\uff0c\u53c8\u80fd\u51cf\u5c11\u5bf9\u8f85\u52a9\u80fd\u6e90\u4f9d\u8d56\u7684\u4f18\u5316\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u65b9\u5dee\u611f\u77e5\u7684\u8def\u7531\u548c\u8c03\u5ea6\u6846\u67b6\uff0c\u57fa\u4e8e\u8bad\u7ec3\u8ba1\u7b97\u548c\u63a8\u7406\u8ba1\u7b97\u7684\u7f29\u653e\u5b9a\u5f8b\u6765\u8bbe\u8ba1\u8c03\u5ea6\u7b56\u7565\uff0c\u5728\u65f6\u95f4\u3001\u6a21\u578b\u548c\u6267\u884c\u9009\u62e9\u4e09\u4e2a\u7ef4\u5ea6\u4e0a\u5438\u6536\u53d8\u5f02\u6027\u3002", "result": "\u5efa\u7acb\u4e86\u4e34\u754c\u72b6\u6001\u4e0b\u7684\u7406\u8bba\u5206\u6790\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u4e8c\u9636\u7279\u6027\u8868\u5f81\u7cfb\u7edf\u6027\u80fd\uff0c\u4e3a\u5f00\u53d1\u80fd\u6e90\u611f\u77e5\u7684\u6a21\u578b\u8def\u7531\u7b56\u7565\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "conclusion": "\u65b9\u5dee\u611f\u77e5\u7684\u8def\u7531\u548c\u8c03\u5ea6\u662f\u7cfb\u7edf\u8bbe\u8ba1\u7684\u6838\u5fc3\u539f\u5219\uff0c\u80fd\u591f\u6709\u6548\u4f18\u5316\u5927\u578b\u63a8\u7406\u6a21\u578b\u7cfb\u7edf\u7684\u80fd\u6e90\u6548\u7387\uff0c\u5e73\u8861\u57fa\u7840\u80fd\u6e90\u548c\u8f85\u52a9\u80fd\u6e90\u7684\u4f7f\u7528\u3002"}}
{"id": "2601.01037", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01037", "abs": "https://arxiv.org/abs/2601.01037", "authors": ["Livia Leong Hui Teng"], "title": "Multi-Dimensional Prompt Chaining to Improve Open-Domain Dialogue Generation", "comment": null, "summary": "Small language models (SLMs) offer significant deployment advantages but often struggle to match the dialogue quality of larger models in open-domain settings. In this paper, we propose a multi-dimensional prompt-chaining framework that integrates Naturalness, Coherence, and Engagingness dimensions to enhance human-likeness in open-domain dialogue generation. We apply the framework to two SLMs, TinyLlama and Llama-2-7B, and benchmark their performance against responses generated by substantially larger models, including Llama-2-70B and GPT-3.5 Turbo. We then employ automatic and human evaluation to assess the responses based on diversity, contextual coherence, as well as overall quality. Results show that the full framework improves response diversity by up to 29%, contextual coherence by up to 28%, and engagingness as well as naturalness by up to 29%. Notably, Llama-2-7B achieves performance comparable to substantially larger models, including Llama-2-70B and GPT-3.5 Turbo. Overall, the findings demonstrate that carefully designed prompt-based strategies provide an effective and resource-efficient pathway to improving open-domain dialogue quality in SLMs.", "AI": {"tldr": "\u63d0\u51fa\u591a\u7ef4\u5ea6\u63d0\u793a\u94fe\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u7136\u6027\u3001\u8fde\u8d2f\u6027\u548c\u5438\u5f15\u6027\u4e09\u4e2a\u7ef4\u5ea6\u63d0\u5347\u5c0f\u8bed\u8a00\u6a21\u578b\u5728\u5f00\u653e\u57df\u5bf9\u8bdd\u4e2d\u7684\u4eba\u7c7b\u76f8\u4f3c\u5ea6\uff0c\u4f7f7B\u6a21\u578b\u8fbe\u5230\u4e0e70B\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u5c0f\u8bed\u8a00\u6a21\u578b\u5728\u90e8\u7f72\u4e0a\u6709\u4f18\u52bf\uff0c\u4f46\u5728\u5f00\u653e\u57df\u5bf9\u8bdd\u8d28\u91cf\u4e0a\u96be\u4ee5\u5339\u654c\u5927\u6a21\u578b\uff0c\u9700\u8981\u8d44\u6e90\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u5176\u5bf9\u8bdd\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u591a\u7ef4\u5ea6\u63d0\u793a\u94fe\u6846\u67b6\uff0c\u6574\u5408\u81ea\u7136\u6027\u3001\u8fde\u8d2f\u6027\u548c\u5438\u5f15\u6027\u4e09\u4e2a\u7ef4\u5ea6\uff0c\u5e94\u7528\u4e8eTinyLlama\u548cLlama-2-7B\u6a21\u578b\uff0c\u5e76\u4e0e\u5927\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4\u8bc4\u4f30\u3002", "result": "\u5b8c\u6574\u6846\u67b6\u5c06\u54cd\u5e94\u591a\u6837\u6027\u63d0\u5347\u8fbe29%\uff0c\u4e0a\u4e0b\u6587\u8fde\u8d2f\u6027\u63d0\u5347\u8fbe28%\uff0c\u5438\u5f15\u6027\u548c\u81ea\u7136\u6027\u63d0\u5347\u8fbe29%\u3002Llama-2-7B\u8fbe\u5230\u4e0eLlama-2-70B\u548cGPT-3.5 Turbo\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u57fa\u4e8e\u63d0\u793a\u7684\u7b56\u7565\u4e3a\u63d0\u5347\u5c0f\u8bed\u8a00\u6a21\u578b\u5f00\u653e\u57df\u5bf9\u8bdd\u8d28\u91cf\u63d0\u4f9b\u4e86\u6709\u6548\u4e14\u8d44\u6e90\u9ad8\u6548\u7684\u9014\u5f84\u3002"}}
{"id": "2601.02069", "categories": ["econ.EM"], "pdf": "https://arxiv.org/pdf/2601.02069", "abs": "https://arxiv.org/abs/2601.02069", "authors": ["Ahmed Khwaja", "Sonal Srivastava"], "title": "Reinforcement Learning Based Computationally Efficient Conditional Choice Simulation Estimation of Dynamic Discrete Choice Models", "comment": null, "summary": "Dynamic discrete choice (DDC) models have found widespread application in marketing. However, estimating these becomes challenging in \"big data\" settings with high-dimensional state-action spaces. To address this challenge, this paper develops a Reinforcement Learning (RL)-based two-step (\"computationally light\") Conditional Choice Simulation (CCS) estimation approach that combines the scalability of machine learning with the transparency, explainability, and interpretability of structural models, which is particularly valuable for counterfactual policy analysis. The method is premised on three insights: (1) the CCS (\"forward simulation\") approach is a special case of RL algorithms, (2) starting from an initial state-action pair, CCS updates the corresponding value function only after each simulation path has terminated, whereas RL algorithms may update for all the state-action pairs visited along a simulated path, and (3) RL focuses on inferring an agent's optimal policy with known reward functions, whereas DDC models focus on estimating the reward functions presupposing optimal policies. The procedure's computational efficiency over CCS estimation is demonstrated using Monte Carlo simulations with a canonical machine replacement and a consumer food purchase model. Framing CCS estimation of DDC models as an RL problem increases their applicability and scalability to high-dimensional marketing problems while retaining both interpretability and tractability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6761\u4ef6\u9009\u62e9\u6a21\u62df\uff08CCS\uff09\u4e24\u6b65\u4f30\u8ba1\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u9ad8\u7ef4\u72b6\u6001-\u52a8\u4f5c\u7a7a\u95f4\u4e0b\u52a8\u6001\u79bb\u6563\u9009\u62e9\uff08DDC\uff09\u6a21\u578b\u7684\u4f30\u8ba1\u6311\u6218\uff0c\u7ed3\u5408\u4e86\u673a\u5668\u5b66\u4e60\u53ef\u6269\u5c55\u6027\u548c\u7ed3\u6784\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5728\u5e02\u573a\u8425\u9500\u4e2d\uff0c\u52a8\u6001\u79bb\u6563\u9009\u62e9\u6a21\u578b\u9762\u4e34\"\u5927\u6570\u636e\"\u73af\u5883\u4e0b\u9ad8\u7ef4\u72b6\u6001-\u52a8\u4f5c\u7a7a\u95f4\u7684\u4f30\u8ba1\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u8ba1\u7b97\u91cf\u5927\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u7ed3\u6784\u6a21\u578b\u89e3\u91ca\u6027\u53c8\u80fd\u5b9e\u73b0\u8ba1\u7b97\u6548\u7387\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u5c06CCS\u4f30\u8ba1\u6846\u67b6\u91cd\u6784\u4e3a\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u5229\u7528RL\u7b97\u6cd5\u7684\u53ef\u6269\u5c55\u6027\u4f18\u52bf\uff0c\u4f46\u4fdd\u6301CCS\u7684\"\u524d\u5411\u6a21\u62df\"\u7279\u6027\u3002\u5173\u952e\u6d1e\u5bdf\u5305\u62ec\uff1aCCS\u662fRL\u7b97\u6cd5\u7684\u7279\u4f8b\uff1bCCS\u53ea\u5728\u6a21\u62df\u8def\u5f84\u7ed3\u675f\u540e\u66f4\u65b0\u503c\u51fd\u6570\uff0c\u800cRL\u53ef\u6cbf\u8def\u5f84\u66f4\u65b0\uff1bRL\u5173\u6ce8\u5df2\u77e5\u5956\u52b1\u51fd\u6570\u4e0b\u7684\u6700\u4f18\u7b56\u7565\uff0cDDC\u5173\u6ce8\u5df2\u77e5\u6700\u4f18\u7b56\u7565\u4e0b\u7684\u5956\u52b1\u51fd\u6570\u4f30\u8ba1\u3002", "result": "\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u6a21\u62df\uff0c\u5728\u7ecf\u5178\u7684\u673a\u5668\u66f4\u6362\u6a21\u578b\u548c\u6d88\u8d39\u8005\u98df\u54c1\u8d2d\u4e70\u6a21\u578b\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u76f8\u6bd4\u4f20\u7edfCCS\u4f30\u8ba1\u7684\u8ba1\u7b97\u6548\u7387\u4f18\u52bf\u3002\u65b9\u6cd5\u80fd\u591f\u6269\u5c55\u5230\u9ad8\u7ef4\u8425\u9500\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u5904\u7406\u6027\u3002", "conclusion": "\u5c06DDC\u6a21\u578b\u7684CCS\u4f30\u8ba1\u6846\u67b6\u5316\u4e3aRL\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u65b9\u6cd5\u5728\u9ad8\u7ef4\u8425\u9500\u95ee\u9898\u4e2d\u7684\u9002\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u7ed3\u6784\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u5904\u7406\u6027\uff0c\u4e3a\u53cd\u4e8b\u5b9e\u653f\u7b56\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5de5\u5177\u3002"}}
{"id": "2601.01620", "categories": ["cs.CY", "cs.CL", "cs.IT"], "pdf": "https://arxiv.org/pdf/2601.01620", "abs": "https://arxiv.org/abs/2601.01620", "authors": ["Shayan Alipour", "Shruti Phadke", "Seyed Shahabeddin Mousavi", "Amirhossein Afsharrad", "Morteza Zihayat", "Mattia Samory"], "title": "The Gray Area: Characterizing Moderator Disagreement on Reddit", "comment": "16 pages, 11 figures", "summary": "Volunteer moderators play a crucial role in sustaining online dialogue, but they often disagree about what should or should not be allowed. In this paper, we study the complexity of content moderation with a focus on disagreements between moderators, which we term the ``gray area'' of moderation. Leveraging 5 years and 4.3 million moderation log entries from 24 subreddits of different topics and sizes, we characterize how gray area, or disputed cases, differ from undisputed cases. We show that one-in-seven moderation cases are disputed among moderators, often addressing transgressions where users' intent is not directly legible, such as in trolling and brigading, as well as tensions around community governance. This is concerning, as almost half of all gray area cases involved automated moderation decisions. Through information-theoretic evaluations, we demonstrate that gray area cases are inherently harder to adjudicate than undisputed cases and show that state-of-the-art language models struggle to adjudicate them. We highlight the key role of expert human moderators in overseeing the moderation process and provide insights about the challenges of current moderation processes and tools.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790Reddit\u8bba\u575b\u4e2d\u5fd7\u613f\u8005\u7248\u4e3b\u4e4b\u95f4\u7684\u5185\u5bb9\u5ba1\u6838\u5206\u6b67\uff0c\u53d1\u73b0\u4e03\u5206\u4e4b\u4e00\u7684\u5ba1\u6838\u6848\u4f8b\u5b58\u5728\u4e89\u8bae\uff0c\u8fd9\u4e9b\"\u7070\u8272\u5730\u5e26\"\u6848\u4f8b\u4e3b\u8981\u6d89\u53ca\u96be\u4ee5\u5224\u65ad\u7528\u6237\u610f\u56fe\u7684\u884c\u4e3a\uff08\u5982\u9493\u9c7c\u3001\u5237\u5c4f\uff09\u548c\u793e\u533a\u6cbb\u7406\u51b2\u7a81\uff0c\u4e14\u8fd1\u534a\u6570\u6d89\u53ca\u81ea\u52a8\u5316\u5ba1\u6838\u51b3\u7b56\u3002\u7814\u7a76\u8868\u660e\u7070\u8272\u5730\u5e26\u6848\u4f8b\u672c\u8d28\u4e0a\u66f4\u96be\u88c1\u51b3\uff0c\u5f53\u524d\u6700\u5148\u8fdb\u7684\u8bed\u8a00\u6a21\u578b\u4e5f\u96be\u4ee5\u5904\u7406\u3002", "motivation": "\u5fd7\u613f\u8005\u7248\u4e3b\u5bf9\u5728\u7ebf\u5185\u5bb9\u5ba1\u6838\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4ed6\u4eec\u7ecf\u5e38\u5728\u4ec0\u4e48\u5185\u5bb9\u5e94\u8be5\u88ab\u5141\u8bb8\u6216\u7981\u6b62\u4e0a\u5b58\u5728\u5206\u6b67\u3002\u7814\u7a76\u8005\u5e0c\u671b\u7406\u89e3\u8fd9\u79cd\u5ba1\u6838\"\u7070\u8272\u5730\u5e26\"\u7684\u590d\u6742\u6027\uff0c\u7279\u522b\u662f\u7248\u4e3b\u4e4b\u95f4\u7684\u5206\u6b67\u5982\u4f55\u5f71\u54cd\u5185\u5bb9\u5ba1\u6838\u8fc7\u7a0b\u3002", "method": "\u7814\u7a76\u5206\u6790\u4e8624\u4e2a\u4e0d\u540c\u4e3b\u9898\u548c\u89c4\u6a21\u7684subreddit\u4e2d5\u5e74\u95f4\u7684430\u4e07\u6761\u5ba1\u6838\u65e5\u5fd7\u6761\u76ee\u3002\u901a\u8fc7\u6bd4\u8f83\u4e89\u8bae\u6848\u4f8b\uff08\u7070\u8272\u5730\u5e26\uff09\u548c\u65e0\u4e89\u8bae\u6848\u4f8b\uff0c\u4f7f\u7528\u4fe1\u606f\u8bba\u8bc4\u4f30\u65b9\u6cd5\u5206\u6790\u8fd9\u4e9b\u6848\u4f8b\u7684\u5dee\u5f02\uff0c\u5e76\u6d4b\u8bd5\u4e86\u6700\u5148\u8fdb\u7684\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u8fd9\u4e9b\u4e89\u8bae\u6848\u4f8b\u65f6\u7684\u8868\u73b0\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1\uff09\u4e03\u5206\u4e4b\u4e00\u7684\u5ba1\u6838\u6848\u4f8b\u5b58\u5728\u7248\u4e3b\u4e89\u8bae\uff1b2\uff09\u7070\u8272\u5730\u5e26\u6848\u4f8b\u4e3b\u8981\u6d89\u53ca\u7528\u6237\u610f\u56fe\u96be\u4ee5\u76f4\u63a5\u5224\u65ad\u7684\u8fdd\u89c4\u884c\u4e3a\uff08\u5982\u9493\u9c7c\u3001\u5237\u5c4f\uff09\u4ee5\u53ca\u793e\u533a\u6cbb\u7406\u51b2\u7a81\uff1b3\uff09\u8fd1\u534a\u6570\u7070\u8272\u5730\u5e26\u6848\u4f8b\u6d89\u53ca\u81ea\u52a8\u5316\u5ba1\u6838\u51b3\u7b56\uff1b4\uff09\u4fe1\u606f\u8bba\u8bc4\u4f30\u663e\u793a\u7070\u8272\u5730\u5e26\u6848\u4f8b\u672c\u8d28\u4e0a\u6bd4\u65e0\u4e89\u8bae\u6848\u4f8b\u66f4\u96be\u88c1\u51b3\uff1b5\uff09\u5f53\u524d\u6700\u5148\u8fdb\u7684\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u8fd9\u4e9b\u4e89\u8bae\u6848\u4f8b\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u4e13\u5bb6\u4eba\u7c7b\u7248\u4e3b\u5728\u76d1\u7763\u5ba1\u6838\u8fc7\u7a0b\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u5ba1\u6838\u6d41\u7a0b\u548c\u5de5\u5177\u9762\u4e34\u7684\u6311\u6218\u3002\u7070\u8272\u5730\u5e26\u6848\u4f8b\u7684\u590d\u6742\u6027\u8868\u660e\uff0c\u5b8c\u5168\u4f9d\u8d56\u81ea\u52a8\u5316\u7cfb\u7edf\u8fdb\u884c\u5185\u5bb9\u5ba1\u6838\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u4eba\u7c7b\u4e13\u5bb6\u7684\u5224\u65ad\u548c\u76d1\u7763\u3002"}}
{"id": "2601.01238", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01238", "abs": "https://arxiv.org/abs/2601.01238", "authors": ["Kalyaan Rao"], "title": "Evidence Slopes and Effective Dimension in Singular Linear Models", "comment": "Preprint. 10 pages, 6 figures. Under review", "summary": "Bayesian model selection commonly relies on Laplace approximation or the Bayesian Information Criterion (BIC), which assume that the effective model dimension equals the number of parameters. Singular learning theory replaces this assumption with the real log canonical threshold (RLCT), an effective dimension that can be strictly smaller in overparameterized or rank-deficient models.\n  We study linear-Gaussian rank models and linear subspace (dictionary) models in which the exact marginal likelihood is available in closed form and the RLCT is analytically tractable. In this setting, we show theoretically and empirically that the error of Laplace/BIC grows linearly with (d/2 minus lambda) times log n, where d is the ambient parameter dimension and lambda is the RLCT. An RLCT-aware correction recovers the correct evidence slope and is invariant to overcomplete reparameterizations that represent the same data subspace.\n  Our results provide a concrete finite-sample characterization of Laplace failure in singular models and demonstrate that evidence slopes can be used as a practical estimator of effective dimension in simple linear settings.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u8d1d\u53f6\u65af\u6a21\u578b\u9009\u62e9\u4e2dLaplace\u8fd1\u4f3c\u548cBIC\u5728\u5947\u5f02\u6a21\u578b\u4e2d\u7684\u5931\u6548\u95ee\u9898\uff0c\u63d0\u51fa\u57fa\u4e8e\u5b9e\u5bf9\u6570\u5178\u8303\u9608\u503c(RLCT)\u7684\u4fee\u6b63\u65b9\u6cd5\uff0c\u5728\u7ebf\u6027\u9ad8\u65af\u79e9\u6a21\u578b\u548c\u5b57\u5178\u6a21\u578b\u4e2d\u9a8c\u8bc1\u4e86RLCT\u4f5c\u4e3a\u6709\u6548\u7ef4\u5ea6\u7684\u5b9e\u7528\u6027\u3002", "motivation": "\u4f20\u7edf\u8d1d\u53f6\u65af\u6a21\u578b\u9009\u62e9\u65b9\u6cd5\uff08Laplace\u8fd1\u4f3c\u548cBIC\uff09\u5047\u8bbe\u6709\u6548\u6a21\u578b\u7ef4\u5ea6\u7b49\u4e8e\u53c2\u6570\u6570\u91cf\uff0c\u4f46\u5728\u8fc7\u53c2\u6570\u5316\u6216\u79e9\u4e0d\u8db3\u7684\u5947\u5f02\u6a21\u578b\u4e2d\uff0c\u8fd9\u4e00\u5047\u8bbe\u4e0d\u6210\u7acb\u3002\u9700\u8981\u7814\u7a76\u5947\u5f02\u5b66\u4e60\u7406\u8bba\u4e2d\u7684\u5b9e\u5bf9\u6570\u5178\u8303\u9608\u503c(RLCT)\u5982\u4f55\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u6709\u6548\u7ef4\u5ea6\u4f30\u8ba1\u3002", "method": "\u5728\u7ebf\u6027\u9ad8\u65af\u79e9\u6a21\u578b\u548c\u7ebf\u6027\u5b50\u7a7a\u95f4\uff08\u5b57\u5178\uff09\u6a21\u578b\u4e2d\uff0c\u5229\u7528\u7cbe\u786e\u8fb9\u7f18\u4f3c\u7136\u95ed\u5f0f\u89e3\u548cRLCT\u89e3\u6790\u53ef\u5904\u7406\u7684\u7279\u70b9\uff0c\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u7814\u7a76Laplace/BIC\u8bef\u5dee\u4e0eRLCT\u7684\u5173\u7cfb\uff0c\u63d0\u51faRLCT\u611f\u77e5\u7684\u4fee\u6b63\u65b9\u6cd5\u3002", "result": "Laplace/BIC\u8bef\u5dee\u968f(d/2 - \u03bb)log n\u7ebf\u6027\u589e\u957f\uff0c\u5176\u4e2dd\u662f\u73af\u5883\u53c2\u6570\u7ef4\u5ea6\uff0c\u03bb\u662fRLCT\u3002RLCT\u611f\u77e5\u4fee\u6b63\u6062\u590d\u4e86\u6b63\u786e\u7684\u8bc1\u636e\u659c\u7387\uff0c\u4e14\u5bf9\u8868\u793a\u76f8\u540c\u6570\u636e\u5b50\u7a7a\u95f4\u7684\u8fc7\u5b8c\u5907\u91cd\u53c2\u6570\u5316\u5177\u6709\u4e0d\u53d8\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5947\u5f02\u6a21\u578b\u4e2dLaplace\u5931\u6548\u63d0\u4f9b\u4e86\u5177\u4f53\u7684\u6709\u9650\u6837\u672c\u7279\u5f81\uff0c\u8bc1\u660e\u8bc1\u636e\u659c\u7387\u5728\u7b80\u5355\u7ebf\u6027\u8bbe\u7f6e\u4e2d\u53ef\u4f5c\u4e3a\u6709\u6548\u7ef4\u5ea6\u7684\u5b9e\u7528\u4f30\u8ba1\u5668\uff0cRLCT\u4fee\u6b63\u65b9\u6cd5\u80fd\u63d0\u9ad8\u6a21\u578b\u9009\u62e9\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2601.00844", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.00844", "abs": "https://arxiv.org/abs/2601.00844", "authors": ["Matthieu Destrade", "Oumayma Bounou", "Quentin Le Lidec", "Jean Ponce", "Yann LeCun"], "title": "Value-guided action planning with JEPA world models", "comment": "Presented as a poster at the World Modeling Workshop 2026, Mila", "summary": "Building deep learning models that can reason about their environment requires capturing its underlying dynamics. Joint-Embedded Predictive Architectures (JEPA) provide a promising framework to model such dynamics by learning representations and predictors through a self-supervised prediction objective. However, their ability to support effective action planning remains limited. We propose an approach to enhance planning with JEPA world models by shaping their representation space so that the negative goal-conditioned value function for a reaching cost in a given environment is approximated by a distance (or quasi-distance) between state embeddings. We introduce a practical method to enforce this constraint during training and show that it leads to significantly improved planning performance compared to standard JEPA models on simple control tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u589e\u5f3aJEPA\u4e16\u754c\u6a21\u578b\u89c4\u5212\u80fd\u529b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5851\u9020\u8868\u793a\u7a7a\u95f4\uff0c\u4f7f\u8d1f\u76ee\u6807\u6761\u4ef6\u503c\u51fd\u6570\u8fd1\u4f3c\u4e8e\u72b6\u6001\u5d4c\u5165\u95f4\u7684\u8ddd\u79bb\uff0c\u4ece\u800c\u663e\u8457\u63d0\u5347\u89c4\u5212\u6027\u80fd\u3002", "motivation": "\u867d\u7136JEPA\u6846\u67b6\u80fd\u901a\u8fc7\u81ea\u76d1\u7763\u9884\u6d4b\u76ee\u6807\u5b66\u4e60\u73af\u5883\u52a8\u6001\u8868\u793a\uff0c\u4f46\u5176\u652f\u6301\u6709\u6548\u884c\u52a8\u89c4\u5212\u7684\u80fd\u529b\u6709\u9650\u3002\u9700\u8981\u589e\u5f3aJEPA\u4e16\u754c\u6a21\u578b\u7684\u89c4\u5212\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5851\u9020JEPA\u7684\u8868\u793a\u7a7a\u95f4\uff0c\u4f7f\u7279\u5b9a\u73af\u5883\u4e2d\u5230\u8fbe\u6210\u672c\u7684\u8d1f\u76ee\u6807\u6761\u4ef6\u503c\u51fd\u6570\u8fd1\u4f3c\u4e8e\u72b6\u6001\u5d4c\u5165\u95f4\u7684\u8ddd\u79bb\uff08\u6216\u51c6\u8ddd\u79bb\uff09\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u8bad\u7ec3\u4e2d\u5f3a\u5236\u6267\u884c\u6b64\u7ea6\u675f\u7684\u5b9e\u7528\u65b9\u6cd5\u3002", "result": "\u5728\u7b80\u5355\u63a7\u5236\u4efb\u52a1\u4e0a\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u6807\u51c6JEPA\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u89c4\u5212\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u7ea6\u675f\u8868\u793a\u7a7a\u95f4\u4f7f\u503c\u51fd\u6570\u8fd1\u4f3c\u4e8e\u5d4c\u5165\u8ddd\u79bb\uff0c\u53ef\u4ee5\u6709\u6548\u589e\u5f3aJEPA\u4e16\u754c\u6a21\u578b\u7684\u89c4\u5212\u80fd\u529b\uff0c\u4e3a\u6784\u5efa\u80fd\u63a8\u7406\u73af\u5883\u52a8\u6001\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u63d0\u4f9b\u4e86\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2601.01274", "categories": ["eess.SY", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01274", "abs": "https://arxiv.org/abs/2601.01274", "authors": ["Md. Sadman Haque", "Zobaer Ibn Razzaque", "Robiul Awoul Robin", "Fahim Hafiz", "Riasat Azim"], "title": "An Energy-Efficient Smart Bus Transport Management System with Blind-Spot Collision Detection Ability", "comment": "29 pages, 11 figures", "summary": "Public bus transport systems in developing countries often suffer from a lack of real-time location updates and for users, making commuting inconvenient and unreliable for passengers. Furthermore, stopping at undesired locations rather than designated bus stops creates safety risks and contributes to roadblocks, often causing traffic congestion. Additionally, issues such as blind spots, along with a lack of following traffic laws, increase the chances of accidents. In this work, we address these challenges by proposing a smart public bus system along with intelligent bus stops that enhance safety, efficiency, and sustainability. Our approach includes a deep learning-based blind-spot warning system to help drivers avoid accidents with automated bus-stop detection to accurately identify bus stops, improving transit efficiency. We also introduce IoT-based solar-powered smart bus stops that show real-time passenger counts, along with an RFID-based card system to track where passengers board and exit. A smart door system ensures safer and more organised boarding, while real-time bus tracking keeps passengers informed. To connect all these features, we use an HTTP-based server for seamless communication between the interconnected network systems. Our proposed system demonstrated approximately 99% efficiency in real-time blind spot detection while stopping precisely at the bus stops. Furthermore, the server showed real-time location updates both to the users and at the bus stops, enhancing commuting efficiency. The proposed energy-efficient bus stop demonstrated 12.71kWh energy saving, promoting sustainable architecture. Full implementation and source code are available at: https://github.com/sadman-adib/MoveMe-IoT", "AI": {"tldr": "\u63d0\u51fa\u667a\u6167\u516c\u4ea4\u7cfb\u7edf\uff0c\u5305\u542b\u6df1\u5ea6\u5b66\u4e60\u76f2\u70b9\u9884\u8b66\u3001\u81ea\u52a8\u516c\u4ea4\u7ad9\u8bc6\u522b\u3001\u7269\u8054\u7f51\u592a\u9633\u80fd\u667a\u80fd\u7ad9\u53f0\u3001RFID\u4e58\u5ba2\u8ffd\u8e2a\u3001\u667a\u80fd\u8f66\u95e8\u7cfb\u7edf\u53ca\u5b9e\u65f6\u516c\u4ea4\u8ffd\u8e2a\uff0c\u63d0\u5347\u5b89\u5168\u3001\u6548\u7387\u4e0e\u53ef\u6301\u7eed\u6027\u3002", "motivation": "\u53d1\u5c55\u4e2d\u56fd\u5bb6\u516c\u4ea4\u7cfb\u7edf\u7f3a\u4e4f\u5b9e\u65f6\u4f4d\u7f6e\u66f4\u65b0\uff0c\u4e58\u5ba2\u4f53\u9a8c\u5dee\uff1b\u975e\u6307\u5b9a\u5730\u70b9\u505c\u8f66\u9020\u6210\u5b89\u5168\u9690\u60a3\u548c\u4ea4\u901a\u62e5\u5835\uff1b\u76f2\u70b9\u548c\u4ea4\u901a\u8fdd\u89c4\u589e\u52a0\u4e8b\u6545\u98ce\u9669\u3002", "method": "1) \u6df1\u5ea6\u5b66\u4e60\u76f2\u70b9\u9884\u8b66\u7cfb\u7edf\uff1b2) \u81ea\u52a8\u516c\u4ea4\u7ad9\u68c0\u6d4b\uff1b3) \u7269\u8054\u7f51\u592a\u9633\u80fd\u667a\u80fd\u7ad9\u53f0\uff0c\u663e\u793a\u5b9e\u65f6\u4e58\u5ba2\u6570\uff1b4) RFID\u5361\u7cfb\u7edf\u8ffd\u8e2a\u4e58\u5ba2\u4e0a\u4e0b\u8f66\uff1b5) \u667a\u80fd\u8f66\u95e8\u7cfb\u7edf\uff1b6) \u5b9e\u65f6\u516c\u4ea4\u8ffd\u8e2a\uff1b7) HTTP\u670d\u52a1\u5668\u8fde\u63a5\u6240\u6709\u5b50\u7cfb\u7edf\u3002", "result": "\u5b9e\u65f6\u76f2\u70b9\u68c0\u6d4b\u6548\u7387\u7ea699%\uff1b\u7cbe\u786e\u505c\u9760\u516c\u4ea4\u7ad9\uff1b\u670d\u52a1\u5668\u63d0\u4f9b\u5b9e\u65f6\u4f4d\u7f6e\u66f4\u65b0\u7ed9\u7528\u6237\u548c\u7ad9\u53f0\uff1b\u8282\u80fd\u7ad9\u53f0\u8282\u770112.71kWh\u80fd\u6e90\u3002", "conclusion": "\u63d0\u51fa\u7684\u667a\u6167\u516c\u4ea4\u7cfb\u7edf\u80fd\u6709\u6548\u63d0\u5347\u5b89\u5168\u6027\u3001\u6548\u7387\u548c\u53ef\u6301\u7eed\u6027\uff0c\u9002\u7528\u4e8e\u53d1\u5c55\u4e2d\u56fd\u5bb6\u516c\u4ea4\u7cfb\u7edf\u73b0\u4ee3\u5316\u6539\u9020\u3002"}}
{"id": "2601.01502", "categories": ["math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.01502", "abs": "https://arxiv.org/abs/2601.01502", "authors": ["Milind Nakul", "Tianjiao Li", "Ashwin Pananjady"], "title": "Multiscale replay: A robust algorithm for stochastic variational inequalities with a Markovian buffer", "comment": null, "summary": "We introduce the Multiscale Experience Replay (MER) algorithm for solving a class of stochastic variational inequalities (VIs) in settings where samples are generated from a Markov chain and we have access to a memory buffer to store them. Rather than uniformly sampling from the buffer, MER utilizes a multi-scale sampling scheme to emulate the behavior of VI algorithms designed for independent and identically distributed samples, overcoming bias in the de facto serial scheme and thereby accelerating convergence. Notably, unlike standard sample-skipping variants of serial algorithms, MER is robust in that it achieves this acceleration in iteration complexity whenever possible, and without requiring knowledge of the mixing time of the Markov chain. We also discuss applications of MER, particularly in policy evaluation with temporal difference learning and in training generalized linear models with dependent data.", "AI": {"tldr": "\u63d0\u51faMER\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u9a6c\u5c14\u53ef\u592b\u94fe\u751f\u6210\u6837\u672c\u7684\u968f\u673a\u53d8\u5206\u4e0d\u7b49\u5f0f\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u91c7\u6837\u514b\u670d\u5e8f\u5217\u91c7\u6837\u504f\u5dee\uff0c\u52a0\u901f\u6536\u655b\u4e14\u65e0\u9700\u77e5\u9053\u94fe\u7684\u6df7\u5408\u65f6\u95f4\u3002", "motivation": "\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\uff0c\u6570\u636e\u901a\u5e38\u6765\u81ea\u9a6c\u5c14\u53ef\u592b\u94fe\u800c\u975e\u72ec\u7acb\u540c\u5206\u5e03\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u5e8f\u5217\u91c7\u6837\u65f6\u5b58\u5728\u504f\u5dee\uff0c\u6536\u655b\u901f\u5ea6\u6162\uff0c\u4e14\u9700\u8981\u77e5\u9053\u94fe\u7684\u6df7\u5408\u65f6\u95f4\u53c2\u6570\u3002", "method": "MER\u7b97\u6cd5\u4f7f\u7528\u8bb0\u5fc6\u7f13\u51b2\u533a\u5b58\u50a8\u6837\u672c\uff0c\u91c7\u7528\u591a\u5c3a\u5ea6\u91c7\u6837\u65b9\u6848\u800c\u975e\u5747\u5300\u91c7\u6837\uff0c\u6a21\u62df\u72ec\u7acb\u540c\u5206\u5e03\u6837\u672c\u4e0bVI\u7b97\u6cd5\u7684\u884c\u4e3a\uff0c\u514b\u670d\u5e8f\u5217\u91c7\u6837\u504f\u5dee\u3002", "result": "MER\u7b97\u6cd5\u5728\u8fed\u4ee3\u590d\u6742\u5ea6\u4e0a\u5b9e\u73b0\u52a0\u901f\u6536\u655b\uff0c\u65e0\u9700\u77e5\u9053\u9a6c\u5c14\u53ef\u592b\u94fe\u7684\u6df7\u5408\u65f6\u95f4\uff0c\u6bd4\u6807\u51c6\u6837\u672c\u8df3\u8fc7\u65b9\u6cd5\u66f4\u9c81\u68d2\uff0c\u9002\u7528\u4e8e\u7b56\u7565\u8bc4\u4f30\u548c\u5e7f\u4e49\u7ebf\u6027\u6a21\u578b\u8bad\u7ec3\u3002", "conclusion": "MER\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4f9d\u8d56\u6570\u636e\u4e0b\u7684\u968f\u673a\u53d8\u5206\u4e0d\u7b49\u5f0f\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u91c7\u6837\u514b\u670d\u5e8f\u5217\u91c7\u6837\u504f\u5dee\uff0c\u5728\u65e0\u9700\u6df7\u5408\u65f6\u95f4\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\u52a0\u901f\u6536\u655b\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2601.00828", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00828", "abs": "https://arxiv.org/abs/2601.00828", "authors": ["Yin Li"], "title": "Decomposing LLM Self-Correction: The Accuracy-Correction Paradox and Error Depth Hypothesis", "comment": "9 pages, 2 figures, 3 tables. Code available at https://github.com/Kevin0304-li/llm-self-correction", "summary": "Large Language Models (LLMs) are widely believed to possess self-correction capabilities, yet recent studies suggest that intrinsic self-correction--where models correct their own outputs without external feedback--remains largely ineffective. In this work, we systematically decompose self-correction into three distinct sub-capabilities: error detection, error localization, and error correction. Through cross-model experiments on GSM8K-Complex (n=500 per model, 346 total errors) with three major LLMs, we uncover a striking Accuracy-Correction Paradox: weaker models (GPT-3.5, 66% accuracy) achieve 1.6x higher intrinsic correction rates than stronger models (DeepSeek, 94% accuracy)--26.8% vs 16.7%. We propose the Error Depth Hypothesis: stronger models make fewer but deeper errors that resist self-correction. Error detection rates vary dramatically across architectures (10% to 82%), yet detection capability does not predict correction success--Claude detects only 10% of errors but corrects 29% intrinsically. Surprisingly, providing error location hints hurts all models. Our findings challenge linear assumptions about model capability and self-improvement, with important implications for the design of self-refinement pipelines.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5185\u5728\u81ea\u6211\u4fee\u6b63\u80fd\u529b\u5b58\u5728\"\u51c6\u786e\u7387-\u4fee\u6b63\u6096\u8bba\"\uff1a\u8f83\u5f31\u6a21\u578b\uff08GPT-3.5\uff09\u7684\u81ea\u6211\u4fee\u6b63\u7387\u53cd\u800c\u6bd4\u8f83\u5f3a\u6a21\u578b\uff08DeepSeek\uff09\u9ad81.6\u500d\uff0c\u6311\u6218\u4e86\u6a21\u578b\u80fd\u529b\u4e0e\u81ea\u6211\u6539\u8fdb\u5448\u7ebf\u6027\u5173\u7cfb\u7684\u5047\u8bbe\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u88ab\u8ba4\u4e3a\u5177\u6709\u81ea\u6211\u4fee\u6b63\u80fd\u529b\uff0c\u4f46\u8fd1\u671f\u7814\u7a76\u8868\u660e\u5185\u5728\u81ea\u6211\u4fee\u6b63\uff08\u65e0\u9700\u5916\u90e8\u53cd\u9988\uff09\u6548\u679c\u6709\u9650\u3002\u672c\u7814\u7a76\u65e8\u5728\u7cfb\u7edf\u5206\u89e3\u81ea\u6211\u4fee\u6b63\u80fd\u529b\uff0c\u63a2\u7a76\u4e0d\u540c\u6a21\u578b\u5728\u81ea\u6211\u4fee\u6b63\u65b9\u9762\u7684\u8868\u73b0\u5dee\u5f02\u3002", "method": "\u5c06\u81ea\u6211\u4fee\u6b63\u5206\u89e3\u4e3a\u4e09\u4e2a\u5b50\u80fd\u529b\uff1a\u9519\u8bef\u68c0\u6d4b\u3001\u9519\u8bef\u5b9a\u4f4d\u548c\u9519\u8bef\u4fee\u6b63\u3002\u5728GSM8K-Complex\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8de8\u6a21\u578b\u5b9e\u9a8c\uff08n=500/\u6a21\u578b\uff0c\u5171346\u4e2a\u9519\u8bef\uff09\uff0c\u4f7f\u7528\u4e09\u79cd\u4e3b\u8981LLM\uff08GPT-3.5\u3001DeepSeek\u3001Claude\uff09\uff0c\u5206\u6790\u4e0d\u540c\u6a21\u578b\u7684\u81ea\u6211\u4fee\u6b63\u8868\u73b0\u3002", "result": "\u53d1\u73b0\"\u51c6\u786e\u7387-\u4fee\u6b63\u6096\u8bba\"\uff1a\u8f83\u5f31\u6a21\u578b\uff08GPT-3.5\uff0c66%\u51c6\u786e\u7387\uff09\u7684\u5185\u5728\u4fee\u6b63\u7387\uff0826.8%\uff09\u6bd4\u8f83\u5f3a\u6a21\u578b\uff08DeepSeek\uff0c94%\u51c6\u786e\u7387\uff09\u7684\u4fee\u6b63\u7387\uff0816.7%\uff09\u9ad81.6\u500d\u3002\u63d0\u51fa\"\u9519\u8bef\u6df1\u5ea6\u5047\u8bf4\"\uff1a\u5f3a\u6a21\u578b\u9519\u8bef\u66f4\u5c11\u4f46\u66f4\u6df1\uff0c\u96be\u4ee5\u81ea\u6211\u4fee\u6b63\u3002\u9519\u8bef\u68c0\u6d4b\u7387\u5728\u4e0d\u540c\u67b6\u6784\u95f4\u5dee\u5f02\u5de8\u5927\uff0810%-82%\uff09\uff0c\u4f46\u68c0\u6d4b\u80fd\u529b\u4e0d\u80fd\u9884\u6d4b\u4fee\u6b63\u6210\u529f\u7387\uff08Claude\u4ec5\u68c0\u6d4b10%\u9519\u8bef\u4f46\u4fee\u6b6329%\uff09\u3002\u63d0\u4f9b\u9519\u8bef\u4f4d\u7f6e\u63d0\u793a\u53cd\u800c\u635f\u5bb3\u6240\u6709\u6a21\u578b\u8868\u73b0\u3002", "conclusion": "\u7814\u7a76\u6311\u6218\u4e86\u6a21\u578b\u80fd\u529b\u4e0e\u81ea\u6211\u6539\u8fdb\u5448\u7ebf\u6027\u5173\u7cfb\u7684\u5047\u8bbe\uff0c\u5bf9\u81ea\u6211\u7cbe\u70bc\u6d41\u7a0b\u8bbe\u8ba1\u6709\u91cd\u8981\u542f\u793a\u3002\u5f3a\u6a21\u578b\u867d\u7136\u51c6\u786e\u7387\u9ad8\uff0c\u4f46\u5185\u5728\u81ea\u6211\u4fee\u6b63\u80fd\u529b\u53cd\u800c\u8f83\u5f31\uff0c\u8fd9\u9700\u8981\u91cd\u65b0\u601d\u8003\u5982\u4f55\u6709\u6548\u5229\u7528LLM\u7684\u81ea\u6211\u4fee\u6b63\u80fd\u529b\u3002"}}
{"id": "2601.01046", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01046", "abs": "https://arxiv.org/abs/2601.01046", "authors": ["Yixuan Tang", "Yi Yang"], "title": "KV-Embedding: Training-free Text Embedding via Internal KV Re-routing in Decoder-only LLMs", "comment": null, "summary": "While LLMs are powerful embedding backbones, their application in training-free settings faces two structural challenges: causal attention restricts early tokens from accessing subsequent context, and the next-token prediction objective biases representations toward generation rather than semantic compression. To address these limitations, we propose KV-Embedding, a framework that activates the latent representation power of frozen LLMs. Our method leverages the observation that the key-value (KV) states of the final token at each layer encode a compressed view of the sequence. By re-routing these states as a prepended prefix, we enable all tokens to access sequence-level context within a single forward pass. To ensure model-agnostic applicability, we introduce an automated layer selection strategy based on intrinsic dimensionality. Evaluations on MTEB across Qwen, Mistral, and Llama backbones show that KV-Embedding outperforms existing training-free baselines by up to 10%, while maintaining robust performance on sequences up to 4,096 tokens. These results demonstrate that internal state manipulation offers an efficient alternative to input modification, and we hope this work encourages further exploration of LLM internals for representation learning.", "AI": {"tldr": "KV-Embedding\uff1a\u901a\u8fc7\u91cd\u5b9a\u5411LLM\u6700\u540e\u4e00\u5c42token\u7684KV\u72b6\u6001\u4f5c\u4e3a\u524d\u7f00\uff0c\u6fc0\u6d3b\u51bb\u7ed3\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8868\u5f81\u80fd\u529b\uff0c\u5728\u65e0\u9700\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u8bed\u4e49\u5d4c\u5165\u6548\u679c\u3002", "motivation": "LLM\u5728\u65e0\u9700\u8bad\u7ec3\u7684\u573a\u666f\u4e0b\u5b58\u5728\u4e24\u4e2a\u7ed3\u6784\u6027\u95ee\u9898\uff1a\u56e0\u679c\u6ce8\u610f\u529b\u673a\u5236\u9650\u5236\u65e9\u671ftoken\u8bbf\u95ee\u540e\u7eed\u4e0a\u4e0b\u6587\uff0c\u4ee5\u53ca\u4e0b\u4e00token\u9884\u6d4b\u76ee\u6807\u4f7f\u8868\u5f81\u504f\u5411\u751f\u6210\u800c\u975e\u8bed\u4e49\u538b\u7f29\u3002\u9700\u8981\u6fc0\u6d3b\u51bb\u7ed3LLM\u7684\u6f5c\u5728\u8868\u5f81\u80fd\u529b\u3002", "method": "\u63d0\u51faKV-Embedding\u6846\u67b6\uff0c\u5229\u7528\u89c2\u5bdf\u5230\u7684\u73b0\u8c61\uff1a\u6bcf\u5c42\u6700\u540e\u4e00\u4e2atoken\u7684key-value\u72b6\u6001\u7f16\u7801\u4e86\u5e8f\u5217\u7684\u538b\u7f29\u89c6\u56fe\u3002\u5c06\u8fd9\u4e9b\u72b6\u6001\u91cd\u5b9a\u5411\u4e3a\u9884\u7f6e\u524d\u7f00\uff0c\u4f7f\u6240\u6709token\u80fd\u5728\u5355\u6b21\u524d\u5411\u4f20\u64ad\u4e2d\u8bbf\u95ee\u5e8f\u5217\u7ea7\u4e0a\u4e0b\u6587\u3002\u5f15\u5165\u57fa\u4e8e\u5185\u5728\u7ef4\u5ea6\u7684\u81ea\u52a8\u5316\u5c42\u9009\u62e9\u7b56\u7565\u786e\u4fdd\u6a21\u578b\u65e0\u5173\u6027\u3002", "result": "\u5728MTEB\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f7f\u7528Qwen\u3001Mistral\u548cLlama\u9aa8\u5e72\u7f51\u7edc\uff0cKV-Embedding\u6bd4\u73b0\u6709\u65e0\u9700\u8bad\u7ec3\u57fa\u7ebf\u63d0\u5347\u9ad8\u8fbe10%\uff0c\u5728\u957f\u8fbe4,096\u4e2atoken\u7684\u5e8f\u5217\u4e0a\u4fdd\u6301\u7a33\u5065\u6027\u80fd\u3002", "conclusion": "\u5185\u90e8\u72b6\u6001\u64cd\u4f5c\u4e3a\u8f93\u5165\u4fee\u6539\u63d0\u4f9b\u4e86\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u8fd9\u9879\u5de5\u4f5c\u9f13\u52b1\u8fdb\u4e00\u6b65\u63a2\u7d22LLM\u5185\u90e8\u673a\u5236\u7528\u4e8e\u8868\u5f81\u5b66\u4e60\u3002"}}
{"id": "2601.02190", "categories": ["econ.EM"], "pdf": "https://arxiv.org/pdf/2601.02190", "abs": "https://arxiv.org/abs/2601.02190", "authors": ["Anna Alberini", "Javier Bas", "Cinzia Cirillo"], "title": "Fare-Free Bus Service and CO2 Reductions: Evidence from a Natural Experiment", "comment": null, "summary": "We devise a difference-in-difference study design to assess the impact of fare-free bus service in Alexandria, located in the Washington, DC metro area. Our surveys show modest to no effect, with at most 6% more residents in Alexandria increasing their bus usage compared to control locations. We find no effect on ground-level ozone or road crashes, suggesting little to no impact on road traffic.\n  One-third of respondents in control locations indicated they would use buses more frequently if fare-free service were available in their areas. Based on the respondent-reported reductions in car miles, the program led to a reduction of 0.294 to 0.494 tons of CO2 per year, or 5% to 9% of the average annual emissions from a US car, at a cost of $70-$120 per ton of CO2. We predict a CO2 reduction of 0.454 tons per year, equivalent to 8% of the average US car's annual emissions if the fare-free bus covered all of the study areas.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u534e\u76db\u987f\u7279\u533a\u5730\u94c1\u533aAlexandria\u514d\u8d39\u516c\u4ea4\u670d\u52a1\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u516c\u4ea4\u4f7f\u7528\u7387\u6700\u591a\u589e\u52a06%\uff0c\u5bf9\u5730\u9762\u81ed\u6c27\u548c\u4ea4\u901a\u4e8b\u6545\u65e0\u5f71\u54cd\uff0c\u4f46\u80fd\u51cf\u5c110.294-0.494\u5428CO2\u6392\u653e\uff0c\u6210\u672c\u4e3a\u6bcf\u542870-120\u7f8e\u5143\u3002", "motivation": "\u8bc4\u4f30\u514d\u8d39\u516c\u4ea4\u670d\u52a1\u653f\u7b56\u5bf9\u5c45\u6c11\u51fa\u884c\u884c\u4e3a\u3001\u73af\u5883\uff08CO2\u6392\u653e\uff09\u548c\u4ea4\u901a\u5b89\u5168\u7684\u5f71\u54cd\uff0c\u4e3a\u57ce\u5e02\u4ea4\u901a\u653f\u7b56\u63d0\u4f9b\u5b9e\u8bc1\u4f9d\u636e\u3002", "method": "\u91c7\u7528\u53cc\u91cd\u5dee\u5206\u6cd5\u7814\u7a76\u8bbe\u8ba1\uff0c\u5bf9\u6bd4Alexandria\uff08\u5b9e\u9a8c\u7ec4\uff09\u4e0e\u5176\u4ed6\u63a7\u5236\u533a\u57df\u7684\u516c\u4ea4\u4f7f\u7528\u53d8\u5316\uff0c\u901a\u8fc7\u95ee\u5377\u8c03\u67e5\u6536\u96c6\u6570\u636e\uff0c\u5206\u6790\u516c\u4ea4\u4f7f\u7528\u9891\u7387\u3001\u6c7d\u8f66\u91cc\u7a0b\u51cf\u5c11\u91cf\u53caCO2\u6392\u653e\u53d8\u5316\u3002", "result": "\u514d\u8d39\u516c\u4ea4\u670d\u52a1\u5bf9\u516c\u4ea4\u4f7f\u7528\u7387\u5f71\u54cd\u6709\u9650\uff08\u6700\u591a\u589e\u52a06%\uff09\uff0c\u5bf9\u5730\u9762\u81ed\u6c27\u548c\u4ea4\u901a\u4e8b\u6545\u65e0\u663e\u8457\u5f71\u54cd\uff1b\u4f46\u80fd\u51cf\u5c110.294-0.494\u5428CO2/\u5e74\uff08\u76f8\u5f53\u4e8e\u7f8e\u56fd\u6c7d\u8f66\u5e74\u5747\u6392\u653e\u76845-9%\uff09\uff0c\u6210\u672c\u6548\u76ca\u4e3a\u6bcf\u5428CO2\u51cf\u6392\u6210\u672c70-120\u7f8e\u5143\u3002", "conclusion": "\u514d\u8d39\u516c\u4ea4\u670d\u52a1\u5bf9\u6539\u53d8\u5c45\u6c11\u51fa\u884c\u6a21\u5f0f\u548c\u73af\u5883\u6548\u76ca\u6709\u9650\uff0c\u4f46\u5177\u6709\u4e00\u5b9aCO2\u51cf\u6392\u6548\u679c\uff1b\u653f\u7b56\u63a8\u5e7f\u81f3\u6574\u4e2a\u7814\u7a76\u533a\u57df\u9884\u8ba1\u6bcf\u5e74\u53ef\u51cf\u5c110.454\u5428CO2\uff08\u76f8\u5f53\u4e8e8%\u7684\u7f8e\u56fd\u6c7d\u8f66\u5e74\u5747\u6392\u653e\uff09\u3002"}}
{"id": "2601.02205", "categories": ["cs.CY", "cs.CR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.02205", "abs": "https://arxiv.org/abs/2601.02205", "authors": ["Neziha Akalin", "Alberto Giaretta"], "title": "From Chat Control to Robot Control: The Backdoors Left Open for the Sake of Safety", "comment": "15 pages, 2 figures", "summary": "This paper explores how a recent European Union proposal, the so-called Chat Control law, which creates regulatory incentives for providers to implement content detection and communication scanning, could transform the foundations of human-robot interaction (HRI). As robots increasingly act as interpersonal communication channels in care, education, and telepresence, they convey not only speech but also gesture, emotion, and contextual cues. We argue that extending digital surveillance laws to such embodied systems would entail continuous monitoring, embedding observation into the very design of everyday robots. This regulation blurs the line between protection and control, turning companions into potential informants. At the same time, monitoring mechanisms that undermine end-to-end encryption function as de facto backdoors, expanding the attack surface and allowing adversaries to exploit legally induced monitoring infrastructures. This creates a paradox of safety through insecurity: systems introduced to protect users may instead compromise their privacy, autonomy, and trust. This work does not aim to predict the future, but to raise awareness and help prevent certain futures from materialising.", "AI": {"tldr": "\u6b27\u76dfChat Control\u6cd5\u5f8b\u63d0\u6848\u5c06\u6570\u5b57\u76d1\u63a7\u6269\u5c55\u5230\u673a\u5668\u4eba\u4ea4\u4e92\u9886\u57df\uff0c\u53ef\u80fd\u5c06\u65e5\u5e38\u673a\u5668\u4eba\u8f6c\u53d8\u4e3a\u76d1\u63a7\u5de5\u5177\uff0c\u5728\u4fdd\u62a4\u4e0e\u76d1\u63a7\u4e4b\u95f4\u5236\u9020\u77db\u76fe\uff0c\u5e76\u56e0\u524a\u5f31\u52a0\u5bc6\u800c\u589e\u52a0\u5b89\u5168\u98ce\u9669\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u5728\u62a4\u7406\u3001\u6559\u80b2\u548c\u8fdc\u7a0b\u5448\u73b0\u7b49\u9886\u57df\u6210\u4e3a\u4eba\u9645\u6c9f\u901a\u6e20\u9053\uff0c\u6b27\u76df\u7684Chat Control\u6cd5\u5f8b\u63d0\u6848\u53ef\u80fd\u5c06\u6570\u5b57\u76d1\u63a7\u6269\u5c55\u5230\u8fd9\u4e9b\u5177\u8eab\u7cfb\u7edf\uff0c\u4ece\u800c\u6539\u53d8\u4eba\u673a\u4ea4\u4e92\u7684\u57fa\u7840\u3002\u4f5c\u8005\u65e8\u5728\u63d0\u9ad8\u5bf9\u8fd9\u4e00\u6f5c\u5728\u8f6c\u53d8\u7684\u8ba4\u8bc6\uff0c\u9632\u6b62\u67d0\u4e9b\u672a\u6765\u6210\u4e3a\u73b0\u5b9e\u3002", "method": "\u672c\u6587\u91c7\u7528\u6279\u5224\u6027\u5206\u6790\u7684\u65b9\u6cd5\uff0c\u63a2\u8ba8\u6b27\u76dfChat Control\u6cd5\u5f8b\u63d0\u6848\u5982\u4f55\u901a\u8fc7\u6fc0\u52b1\u63d0\u4f9b\u5546\u5b9e\u65bd\u5185\u5bb9\u68c0\u6d4b\u548c\u901a\u4fe1\u626b\u63cf\uff0c\u5c06\u6570\u5b57\u76d1\u63a7\u6269\u5c55\u5230\u673a\u5668\u4eba\u4ea4\u4e92\u9886\u57df\u3002\u5206\u6790\u805a\u7126\u4e8e\u8be5\u6cd5\u89c4\u5bf9\u673a\u5668\u4eba\u4f5c\u4e3a\u6c9f\u901a\u6e20\u9053\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5bf9\u9690\u79c1\u3001\u81ea\u4e3b\u6743\u548c\u4fe1\u4efb\u7684\u6f5c\u5728\u5a01\u80c1\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5c06\u6570\u5b57\u76d1\u63a7\u6cd5\u5f8b\u6269\u5c55\u5230\u5177\u8eab\u673a\u5668\u4eba\u7cfb\u7edf\u4f1a\u5bfc\u81f4\u6301\u7eed\u76d1\u63a7\uff0c\u6a21\u7cca\u4fdd\u62a4\u4e0e\u63a7\u5236\u4e4b\u95f4\u7684\u754c\u9650\uff0c\u5c06\u4f34\u4fa3\u673a\u5668\u4eba\u8f6c\u53d8\u4e3a\u6f5c\u5728\u544a\u5bc6\u8005\u3002\u540c\u65f6\uff0c\u524a\u5f31\u7aef\u5230\u7aef\u52a0\u5bc6\u7684\u76d1\u63a7\u673a\u5236\u5b9e\u9645\u4e0a\u6210\u4e3a\u540e\u95e8\uff0c\u6269\u5927\u4e86\u653b\u51fb\u9762\uff0c\u4f7f\u5bf9\u624b\u80fd\u591f\u5229\u7528\u6cd5\u5f8b\u5f3a\u5236\u7684\u76d1\u63a7\u57fa\u7840\u8bbe\u65bd\u3002\u8fd9\u521b\u9020\u4e86\"\u901a\u8fc7\u4e0d\u5b89\u5168\u5b9e\u73b0\u5b89\u5168\"\u7684\u6096\u8bba\u3002", "conclusion": "\u65e8\u5728\u4fdd\u62a4\u7528\u6237\u7684\u7cfb\u7edf\u53ef\u80fd\u53cd\u800c\u635f\u5bb3\u7528\u6237\u7684\u9690\u79c1\u3001\u81ea\u4e3b\u6743\u548c\u4fe1\u4efb\u3002\u672c\u6587\u4e0d\u662f\u8981\u9884\u6d4b\u672a\u6765\uff0c\u800c\u662f\u8981\u63d0\u9ad8\u8ba4\u8bc6\uff0c\u5e2e\u52a9\u9632\u6b62\u67d0\u4e9b\u672a\u6765\u6210\u4e3a\u73b0\u5b9e\u3002\u9700\u8981\u8b66\u60d5\u5c06\u6570\u5b57\u76d1\u63a7\u6269\u5c55\u5230\u673a\u5668\u4eba\u4ea4\u4e92\u9886\u57df\u53ef\u80fd\u5e26\u6765\u7684\u6df1\u8fdc\u5f71\u54cd\u3002"}}
{"id": "2601.01442", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.01442", "abs": "https://arxiv.org/abs/2601.01442", "authors": ["Dongrong Li", "Tianwei Yu", "Xiaodan Fan"], "title": "Fast Gibbs Sampling on Bayesian Hidden Markov Model with Missing Observations", "comment": "45 pages, 2 figures", "summary": "The Hidden Markov Model (HMM) is a widely-used statistical model for handling sequential data. However, the presence of missing observations in real-world datasets often complicates the application of the model. The EM algorithm and Gibbs samplers can be used to estimate the model, yet suffering from various problems including non-convexity, high computational complexity and slow mixing. In this paper, we propose a collapsed Gibbs sampler that efficiently samples from HMMs' posterior by integrating out both the missing observations and the corresponding latent states. The proposed sampler is fast due to its three advantages. First, it achieves an estimation accuracy that is comparable to existing methods. Second, it can produce a larger Effective Sample Size (ESS) per iteration, which can be justified theoretically and numerically. Third, when the number of missing entries is large, the sampler has a significant smaller computational complexity per iteration compared to other methods, thus is faster computationally. In summary, the proposed sampling algorithm is fast both computationally and theoretically and is particularly advantageous when there are a lot of missing entries. Finally, empirical evaluations based on numerical simulations and real data analysis demonstrate that the proposed algorithm consistently outperforms existing algorithms in terms of time complexity and sampling efficiency (measured in ESS).", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9488\u5bf9\u542b\u7f3a\u5931\u89c2\u6d4b\u7684\u9690\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\u7684\u6298\u53e0\u5409\u5e03\u65af\u91c7\u6837\u5668\uff0c\u901a\u8fc7\u540c\u65f6\u79ef\u5206\u6389\u7f3a\u5931\u89c2\u6d4b\u548c\u5bf9\u5e94\u9690\u72b6\u6001\uff0c\u5728\u8ba1\u7b97\u6548\u7387\u548c\u91c7\u6837\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u4e2d\u666e\u904d\u5b58\u5728\u7f3a\u5931\u89c2\u6d4b\uff0c\u8fd9\u7ed9\u9690\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\u7684\u5e94\u7528\u5e26\u6765\u56f0\u96be\u3002\u73b0\u6709\u7684EM\u7b97\u6cd5\u548c\u5409\u5e03\u65af\u91c7\u6837\u5668\u5b58\u5728\u975e\u51f8\u6027\u3001\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u6df7\u5408\u901f\u5ea6\u6162\u7b49\u95ee\u9898", "method": "\u63d0\u51fa\u4e00\u79cd\u6298\u53e0\u5409\u5e03\u65af\u91c7\u6837\u5668\uff0c\u901a\u8fc7\u540c\u65f6\u79ef\u5206\u6389\u7f3a\u5931\u89c2\u6d4b\u548c\u5bf9\u5e94\u7684\u9690\u72b6\u6001\uff0c\u76f4\u63a5\u4ece\u540e\u9a8c\u5206\u5e03\u4e2d\u91c7\u6837\u3002\u8be5\u65b9\u6cd5\u5177\u6709\u4e09\u4e2a\u4f18\u52bf\uff1a\u4f30\u8ba1\u7cbe\u5ea6\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\uff1b\u6bcf\u6b21\u8fed\u4ee3\u80fd\u4ea7\u751f\u66f4\u5927\u7684\u6709\u6548\u6837\u672c\u91cf\uff1b\u5f53\u7f3a\u5931\u6761\u76ee\u591a\u65f6\uff0c\u6bcf\u6b21\u8fed\u4ee3\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u663e\u8457\u964d\u4f4e", "result": "\u6570\u503c\u6a21\u62df\u548c\u5b9e\u9645\u6570\u636e\u5206\u6790\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u5728\u65f6\u95f4\u590d\u6742\u5ea6\u548c\u91c7\u6837\u6548\u7387\uff08\u4ee5\u6709\u6548\u6837\u672c\u91cf\u8861\u91cf\uff09\u65b9\u9762\u4e00\u81f4\u4f18\u4e8e\u73b0\u6709\u7b97\u6cd5", "conclusion": "\u63d0\u51fa\u7684\u91c7\u6837\u7b97\u6cd5\u5728\u8ba1\u7b97\u548c\u7406\u8bba\u4e0a\u90fd\u66f4\u5feb\uff0c\u7279\u522b\u5728\u5b58\u5728\u5927\u91cf\u7f3a\u5931\u6761\u76ee\u65f6\u4f18\u52bf\u660e\u663e\uff0c\u4e3a\u5904\u7406\u542b\u7f3a\u5931\u89c2\u6d4b\u7684\u9690\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.00847", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00847", "abs": "https://arxiv.org/abs/2601.00847", "authors": ["Ryan Shamim"], "title": "You Only Need Your Transformer 25% of the Time: Meaning-First Execution for Eliminating Unnecessary Inference", "comment": "24 pages, 5 figures. Deterministic evaluation protocol. Includes theoretical analysis and empirical validation on GPT-2 and Gemma 2 9B", "summary": "Modern AI inference systems treat transformer execution as mandatory, conflating model capability with execution necessity. We reframe inference as a control-plane decision problem: determining when execution is necessary versus when correctness can be preserved through alternative pathways. We introduce Meaning-First Execution (MFEE), a control-plane architecture implementing this framework, selectively invoking transformer inference only when required. MFEE operates as a gating layer above existing stacks without modifying models, weights, or parameters. Across 1,000 diverse prompts under deterministic decoding, MFEE achieves 78.1% execution reduction while maintaining 100% exact-match equivalence for invoked executions. Comparative evaluation reveals pattern-based routers achieve at most 53.3% avoidance with correctness failures, while MFEE reaches 100% avoidance with zero failures through semantic analysis. We prove this limitation via Theorem 1: any router operating solely on finite feature maps cannot simultaneously guarantee zero false skips and positive avoidance on feature-collision pairs. These results establish execution governance as a foundational layer in ML systems infrastructure, orthogonal to model-level optimization techniques.", "AI": {"tldr": "MFEE\u6846\u67b6\u5c06\u63a8\u7406\u91cd\u6784\u4e3a\u63a7\u5236\u5e73\u9762\u51b3\u7b56\u95ee\u9898\uff0c\u901a\u8fc7\u8bed\u4e49\u5206\u6790\u9009\u62e9\u6027\u6267\u884ctransformer\uff0c\u5728\u4fdd\u6301100%\u51c6\u786e\u7387\u7684\u540c\u65f6\u51cf\u5c1178.1%\u7684\u8ba1\u7b97\u91cf\u3002", "motivation": "\u5f53\u524dAI\u63a8\u7406\u7cfb\u7edf\u5c06transformer\u6267\u884c\u89c6\u4e3a\u5f3a\u5236\u6027\u7684\uff0c\u6df7\u6dc6\u4e86\u6a21\u578b\u80fd\u529b\u4e0e\u6267\u884c\u5fc5\u8981\u6027\u3002\u9700\u8981\u533a\u5206\u4f55\u65f6\u5fc5\u987b\u6267\u884ctransformer\uff0c\u4f55\u65f6\u53ef\u4ee5\u901a\u8fc7\u66ff\u4ee3\u8def\u5f84\u4fdd\u6301\u6b63\u786e\u6027\u3002", "method": "\u63d0\u51faMeaning-First Execution (MFEE)\u63a7\u5236\u5e73\u9762\u67b6\u6784\uff0c\u4f5c\u4e3a\u73b0\u6709\u5806\u6808\u4e4b\u4e0a\u7684\u95e8\u63a7\u5c42\uff0c\u4e0d\u4fee\u6539\u6a21\u578b\u3001\u6743\u91cd\u6216\u53c2\u6570\u3002\u901a\u8fc7\u8bed\u4e49\u5206\u6790\u51b3\u5b9a\u4f55\u65f6\u9700\u8981\u6267\u884ctransformer\u63a8\u7406\u3002", "result": "\u57281000\u4e2a\u591a\u6837\u5316\u63d0\u793a\u7684\u786e\u5b9a\u6027\u89e3\u7801\u4e0b\uff0cMFEE\u5b9e\u73b078.1%\u7684\u6267\u884c\u51cf\u5c11\uff0c\u540c\u65f6\u4fdd\u6301100%\u7684\u7cbe\u786e\u5339\u914d\u7b49\u4ef7\u6027\u3002\u76f8\u6bd4\u57fa\u4e8e\u6a21\u5f0f\u7684\u8def\u7531\u5668\u6700\u591a53.3%\u7684\u907f\u514d\u7387\u4e14\u6709\u9519\u8bef\uff0cMFEE\u901a\u8fc7\u8bed\u4e49\u5206\u6790\u8fbe\u5230100%\u907f\u514d\u4e14\u96f6\u9519\u8bef\u3002", "conclusion": "\u8bc1\u660e\u4e86\u4ec5\u57fa\u4e8e\u6709\u9650\u7279\u5f81\u56fe\u7684\u8def\u7531\u5668\u65e0\u6cd5\u540c\u65f6\u4fdd\u8bc1\u96f6\u9519\u8bef\u8df3\u8fc7\u548c\u6b63\u907f\u514d\u7387\u3002\u6267\u884c\u6cbb\u7406\u5e94\u6210\u4e3aML\u7cfb\u7edf\u57fa\u7840\u8bbe\u65bd\u7684\u57fa\u7840\u5c42\uff0c\u4e0e\u6a21\u578b\u7ea7\u4f18\u5316\u6280\u672f\u6b63\u4ea4\u3002"}}
{"id": "2601.01302", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.01302", "abs": "https://arxiv.org/abs/2601.01302", "authors": ["Pouria Sarhadi"], "title": "Simple yet Effective Anti-windup Techniques for Amplitude and Rate Saturation: An Autonomous Underwater Vehicle Case Study", "comment": null, "summary": "Actuator amplitude and rate saturation (A\\&RSat), together with their consequent windup problem, have long been recognised as challenges in control systems. Anti-windup (AW) solutions have been developed over the past decades, which can generally be categorised into two main groups: classical and modern anti-windup (CAW and MAW) approaches. Classical methods have provided simple and effective results, mainly addressing amplitude saturation. In contrast, modern approaches offer powerful and theoretically sound solutions capable of handling both amplitude and rate saturations. However, MAW's derivation process often imposes restrictive conditions and can be complex to apply in practical engineering problems. Nevertheless, the literature has paid limited attention (if not entirely ignored) to the potential of simple yet effective CAW schemes that can operate in the presence of both A\\&RSat elements. This paper revisits this issue and proposes modifications to two well-known controllers: PID and LQI. The obtained results, benchmarked on the REMUS AUV yaw control problem and compared with constrained MPC, indicate that these classical techniques can still provide simple yet effective solutions with comparable performance, at least for SISO systems. These findings may stimulate further research into solutions that achieve comparable performance with only one (or a limited number of) additional tuning parameters and straightforward implementation.", "AI": {"tldr": "\u672c\u6587\u91cd\u65b0\u5ba1\u89c6\u7ecf\u5178\u6297\u9971\u548c\u63a7\u5236\u65b9\u6cd5\uff0c\u901a\u8fc7\u6539\u8fdbPID\u548cLQI\u63a7\u5236\u5668\uff0c\u4f7f\u5176\u80fd\u540c\u65f6\u5904\u7406\u6267\u884c\u5668\u5e45\u503c\u548c\u901f\u7387\u9971\u548c\u95ee\u9898\uff0c\u5728SISO\u7cfb\u7edf\u4e2d\u53d6\u5f97\u4e0e\u590d\u6742\u73b0\u4ee3\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u6267\u884c\u5668\u5e45\u503c\u548c\u901f\u7387\u9971\u548c\uff08A&RSat\uff09\u53ca\u5176\u5f15\u53d1\u7684\u79ef\u5206\u9971\u548c\u95ee\u9898\u662f\u63a7\u5236\u7cfb\u7edf\u7684\u957f\u671f\u6311\u6218\u3002\u867d\u7136\u73b0\u4ee3\u6297\u9971\u548c\u65b9\u6cd5\uff08MAW\uff09\u80fd\u5904\u7406\u4e24\u79cd\u9971\u548c\uff0c\u4f46\u5176\u63a8\u5bfc\u590d\u6742\u3001\u6761\u4ef6\u4e25\u683c\uff0c\u5de5\u7a0b\u5e94\u7528\u56f0\u96be\u3002\u800c\u7ecf\u5178\u6297\u9971\u548c\u65b9\u6cd5\uff08CAW\uff09\u901a\u5e38\u53ea\u5904\u7406\u5e45\u503c\u9971\u548c\uff0c\u6587\u732e\u4e2d\u5f88\u5c11\u63a2\u8ba8\u5176\u5904\u7406\u4e24\u79cd\u9971\u548c\u7684\u6f5c\u529b\u3002", "method": "\u5bf9\u4e24\u79cd\u7ecf\u5178\u63a7\u5236\u5668\uff08PID\u548cLQI\uff09\u8fdb\u884c\u6539\u8fdb\uff0c\u4f7f\u5176\u80fd\u591f\u540c\u65f6\u5904\u7406\u6267\u884c\u5668\u5e45\u503c\u548c\u901f\u7387\u9971\u548c\u3002\u901a\u8fc7\u4fee\u6539\u63a7\u5236\u7ed3\u6784\uff0c\u4f7f\u8fd9\u4e9b\u7ecf\u5178\u65b9\u6cd5\u80fd\u5728A&RSat\u6761\u4ef6\u4e0b\u6709\u6548\u5de5\u4f5c\u3002", "result": "\u5728REMUS AUV\u504f\u822a\u63a7\u5236\u95ee\u9898\u4e0a\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u4e0e\u7ea6\u675fMPC\u6bd4\u8f83\u3002\u7ed3\u679c\u8868\u660e\uff0c\u6539\u8fdb\u540e\u7684\u7ecf\u5178\u65b9\u6cd5\u5728SISO\u7cfb\u7edf\u4e2d\u80fd\u63d0\u4f9b\u7b80\u5355\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6027\u80fd\u4e0e\u590d\u6742\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "\u7ecf\u5178\u6297\u9971\u548c\u6280\u672f\u7ecf\u8fc7\u9002\u5f53\u6539\u8fdb\u540e\uff0c\u4ecd\u80fd\u63d0\u4f9b\u7b80\u5355\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u81f3\u5c11\u5bf9\u4e8eSISO\u7cfb\u7edf\uff0c\u5176\u6027\u80fd\u53ef\u4e0e\u590d\u6742\u73b0\u4ee3\u65b9\u6cd5\u5ab2\u7f8e\u3002\u8fd9\u4e3a\u4ec5\u9700\u5c11\u91cf\u989d\u5916\u8c03\u53c2\u4e14\u6613\u4e8e\u5b9e\u73b0\u7684\u89e3\u51b3\u65b9\u6848\u63d0\u4f9b\u4e86\u7814\u7a76\u601d\u8def\u3002"}}
{"id": "2601.01538", "categories": ["math.OC", "eess.SY", "math.CA"], "pdf": "https://arxiv.org/pdf/2601.01538", "abs": "https://arxiv.org/abs/2601.01538", "authors": ["Declan S. Jagt", "Matthew M. Peet"], "title": "Lyapunov Functions can Exactly Quantify Rate Performance of Nonlinear Differential Equations", "comment": null, "summary": "Pointwise-in-time stability notions for Ordinary Differential Equations (ODEs) provide quantitative metrics for system performance by establishing bounds on the rate of decay of the system state in terms of initial condition -- allowing stability to be quantified by e.g. the maximum provable decay rate. Such bounds may be obtained by finding suitable Lyapunov functions using, e.g. Sum-of-Squares (SOS) optimization. While Lyapunov tests have been proposed for numerous pointwise-in-time stability notions, including exponential, rational, and finite-time stability, it is unclear whether these characterizations are able to provide accurate bounds on system performance.\n  In this paper, we start by proposing a generalized notion of rate performance -- with exponential, rational, and finite-time decay rates being special cases. Then, for any such notion and rate, we associate a Lyapunov condition which is shown to be necessary and sufficient for a system to achieve that rate. Finally, we show how the proposed conditions can be enforced using SOS programming in the case of exponential, rational, and finite-time stability. Numerical examples in each case demonstrate that the corresponding SOS test can achieve tight bounds on the rate performance with accurate inner bounds on the associated regions of performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86ODE\u7cfb\u7edf\u70b9\u5bf9\u70b9\u7a33\u5b9a\u6027\u6027\u80fd\u7684\u5e7f\u4e49\u6982\u5ff5\uff0c\u5efa\u7acb\u4e86\u4e0e\u5404\u79cd\u8870\u51cf\u7387\uff08\u6307\u6570\u3001\u6709\u7406\u3001\u6709\u9650\u65f6\u95f4\uff09\u5bf9\u5e94\u7684Lyapunov\u6761\u4ef6\uff0c\u5e76\u901a\u8fc7SOS\u4f18\u5316\u5b9e\u73b0\u7d27\u81f4\u6027\u80fd\u8fb9\u754c\u4f30\u8ba1\u3002", "motivation": "\u73b0\u6709Lyapunov\u65b9\u6cd5\u867d\u7136\u80fd\u68c0\u9a8c\u6307\u6570\u3001\u6709\u7406\u548c\u6709\u9650\u65f6\u95f4\u7a33\u5b9a\u6027\uff0c\u4f46\u65e0\u6cd5\u786e\u5b9a\u8fd9\u4e9b\u65b9\u6cd5\u662f\u5426\u80fd\u63d0\u4f9b\u51c6\u786e\u7684\u7cfb\u7edf\u6027\u80fd\u8fb9\u754c\u3002\u9700\u8981\u5efa\u7acb\u66f4\u7cbe\u786e\u7684\u6027\u80fd\u91cf\u5316\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u5e7f\u4e49\u8870\u51cf\u7387\u6027\u80fd\u6982\u5ff5\uff0c\u5efa\u7acb\u4e0e\u5404\u79cd\u8870\u51cf\u7387\u5bf9\u5e94\u7684\u5fc5\u8981\u5145\u5206Lyapunov\u6761\u4ef6\uff0c\u5e76\u901a\u8fc7Sum-of-Squares\uff08SOS\uff09\u7f16\u7a0b\u5b9e\u73b0\u6307\u6570\u3001\u6709\u7406\u548c\u6709\u9650\u65f6\u95f4\u7a33\u5b9a\u6027\u7684\u6761\u4ef6\u9a8c\u8bc1\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684SOS\u6d4b\u8bd5\u80fd\u591f\u83b7\u5f97\u7d27\u81f4\u7684\u6027\u80fd\u7387\u8fb9\u754c\uff0c\u5e76\u5728\u76f8\u5173\u6027\u80fd\u533a\u57df\u5185\u63d0\u4f9b\u51c6\u786e\u7684\u5185\u754c\u4f30\u8ba1\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aODE\u7cfb\u7edf\u6027\u80fd\u91cf\u5316\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684Lyapunov\u65b9\u6cd5\uff0c\u80fd\u591f\u51c6\u786e\u4f30\u8ba1\u5404\u79cd\u8870\u51cf\u7387\u4e0b\u7684\u7cfb\u7edf\u6027\u80fd\u8fb9\u754c\uff0c\u5177\u6709\u7406\u8bba\u548c\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2601.00830", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00830", "abs": "https://arxiv.org/abs/2601.00830", "authors": ["Deep Pankajbhai Mehta"], "title": "Can We Trust AI Explanations? Evidence of Systematic Underreporting in Chain-of-Thought Reasoning", "comment": "22 pages, 8 figures, 9 tables", "summary": "When AI systems explain their reasoning step-by-step, practitioners often assume these explanations reveal what actually influenced the AI's answer. We tested this assumption by embedding hints into questions and measuring whether models mentioned them. In a study of over 9,000 test cases across 11 leading AI models, we found a troubling pattern: models almost never mention hints spontaneously, yet when asked directly, they admit noticing them. This suggests models see influential information but choose not to report it. Telling models they are being watched does not help. Forcing models to report hints works, but causes them to report hints even when none exist and reduces their accuracy. We also found that hints appealing to user preferences are especially dangerous-models follow them most often while reporting them least. These findings suggest that simply watching AI reasoning is not enough to catch hidden influences.", "AI": {"tldr": "AI\u6a21\u578b\u7684\u9010\u6b65\u63a8\u7406\u89e3\u91ca\u5e76\u4e0d\u80fd\u771f\u5b9e\u53cd\u6620\u5f71\u54cd\u5176\u51b3\u7b56\u7684\u56e0\u7d20\uff0c\u6a21\u578b\u4f1a\u6ce8\u610f\u5230\u63d0\u793a\u4f46\u9009\u62e9\u4e0d\u62a5\u544a\uff0c\u5373\u4f7f\u88ab\u76d1\u89c6\u4e5f\u65e0\u6d4e\u4e8e\u4e8b", "motivation": "\u9a8c\u8bc1AI\u7cfb\u7edf\u9010\u6b65\u63a8\u7406\u89e3\u91ca\u662f\u5426\u771f\u7684\u63ed\u793a\u4e86\u5f71\u54cd\u5176\u7b54\u6848\u7684\u5b9e\u9645\u56e0\u7d20\uff0c\u6d4b\u8bd5\u6a21\u578b\u662f\u5426\u4f1a\u62a5\u544a\u5d4c\u5165\u95ee\u9898\u4e2d\u7684\u63d0\u793a\u4fe1\u606f", "method": "\u5728\u95ee\u9898\u4e2d\u5d4c\u5165\u63d0\u793a\u4fe1\u606f\uff0c\u6d4b\u8bd511\u4e2a\u9886\u5148AI\u6a21\u578b\u57289000\u591a\u4e2a\u6d4b\u8bd5\u6848\u4f8b\u4e2d\u7684\u8868\u73b0\uff0c\u6d4b\u91cf\u6a21\u578b\u662f\u5426\u81ea\u53d1\u63d0\u53ca\u63d0\u793a\uff0c\u4ee5\u53ca\u5728\u88ab\u76f4\u63a5\u8be2\u95ee\u65f6\u7684\u53cd\u5e94", "result": "\u6a21\u578b\u51e0\u4e4e\u4ece\u4e0d\u81ea\u53d1\u63d0\u53ca\u63d0\u793a\uff0c\u4f46\u88ab\u76f4\u63a5\u8be2\u95ee\u65f6\u4f1a\u627f\u8ba4\u6ce8\u610f\u5230\u63d0\u793a\uff1b\u76d1\u89c6\u6a21\u578b\u65e0\u5e2e\u52a9\uff1b\u5f3a\u5236\u62a5\u544a\u63d0\u793a\u4f1a\u5bfc\u81f4\u865a\u5047\u62a5\u544a\u548c\u51c6\u786e\u6027\u4e0b\u964d\uff1b\u8fce\u5408\u7528\u6237\u504f\u597d\u7684\u63d0\u793a\u6700\u5371\u9669", "conclusion": "\u4ec5\u4ec5\u89c2\u5bdfAI\u63a8\u7406\u8fc7\u7a0b\u4e0d\u8db3\u4ee5\u53d1\u73b0\u9690\u85cf\u7684\u5f71\u54cd\u56e0\u7d20\uff0c\u5f53\u524d\u7684\u89e3\u91ca\u673a\u5236\u5b58\u5728\u4e25\u91cd\u7f3a\u9677\uff0c\u9700\u8981\u66f4\u53ef\u9760\u7684\u900f\u660e\u5ea6\u65b9\u6cd5"}}
{"id": "2601.01060", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01060", "abs": "https://arxiv.org/abs/2601.01060", "authors": ["Shuhuan Gu", "Wenbiao Tao", "Xinchen Ma", "Kangkang He", "Ye Guo", "Xiang Li", "Yunshi Lan"], "title": "Unsupervised Text Style Transfer for Controllable Intensity", "comment": null, "summary": "Unsupervised Text Style Transfer (UTST) aims to build a system to transfer the stylistic properties of a given text without parallel text pairs. Compared with text transfer between style polarities, UTST for controllable intensity is more challenging due to the subtle differences in stylistic features across different intensity levels. Faced with the challenges posed by the lack of parallel data and the indistinguishability between adjacent intensity levels, we propose a SFT-then-PPO paradigm to fine-tune an LLM. We first fine-tune the LLM with synthesized parallel data. Then, we further train the LLM with PPO, where the rewards are elaborately designed for distinguishing the stylistic intensity in hierarchical levels. Both the global and local stylistic features are considered to formulate the reward functions. The experiments on two UTST benchmarks showcase that both rewards have their advantages and applying them to LLM fine-tuning can effectively improve the performance of an LLM backbone based on various evaluation metrics. Even for close levels of intensity, we can still observe the noticeable stylistic difference between the generated text.", "AI": {"tldr": "\u63d0\u51faSFT-then-PPO\u8303\u5f0f\u7528\u4e8e\u65e0\u76d1\u7763\u6587\u672c\u98ce\u683c\u5f3a\u5ea6\u53ef\u63a7\u8fc1\u79fb\uff0c\u901a\u8fc7\u5408\u6210\u5e73\u884c\u6570\u636e\u5fae\u8c03LLM\uff0c\u518d\u4f7f\u7528\u5206\u5c42\u5956\u52b1\u8bbe\u8ba1\u7684PPO\u8fdb\u4e00\u6b65\u8bad\u7ec3\uff0c\u6709\u6548\u63d0\u5347\u98ce\u683c\u5f3a\u5ea6\u533a\u5206\u80fd\u529b\u3002", "motivation": "\u65e0\u76d1\u7763\u6587\u672c\u98ce\u683c\u8fc1\u79fb\u4e2d\uff0c\u98ce\u683c\u5f3a\u5ea6\u53ef\u63a7\u8fc1\u79fb\u66f4\u5177\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u4e0d\u540c\u5f3a\u5ea6\u7ea7\u522b\u95f4\u7684\u98ce\u683c\u7279\u5f81\u5dee\u5f02\u7ec6\u5fae\uff0c\u4e14\u7f3a\u4e4f\u5e73\u884c\u6570\u636e\uff0c\u76f8\u90bb\u5f3a\u5ea6\u7ea7\u522b\u96be\u4ee5\u533a\u5206\u3002", "method": "\u63d0\u51faSFT-then-PPO\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1\uff09\u4f7f\u7528\u5408\u6210\u5e73\u884c\u6570\u636e\u5bf9LLM\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff1b2\uff09\u4f7f\u7528PPO\u8fdb\u4e00\u6b65\u8bad\u7ec3\uff0c\u8bbe\u8ba1\u5206\u5c42\u5956\u52b1\u51fd\u6570\u540c\u65f6\u8003\u8651\u5168\u5c40\u548c\u5c40\u90e8\u98ce\u683c\u7279\u5f81\u6765\u533a\u5206\u98ce\u683c\u5f3a\u5ea6\u3002", "result": "\u5728\u4e24\u4e2aUTST\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4e24\u79cd\u5956\u52b1\u51fd\u6570\u5404\u6709\u4f18\u52bf\uff0c\u5e94\u7528\u4e8eLLM\u5fae\u8c03\u80fd\u6709\u6548\u63d0\u5347\u57fa\u4e8e\u5404\u79cd\u8bc4\u4f30\u6307\u6807\u7684LLM\u9aa8\u5e72\u6027\u80fd\uff0c\u5373\u4f7f\u5bf9\u4e8e\u76f8\u8fd1\u5f3a\u5ea6\u7ea7\u522b\u4e5f\u80fd\u89c2\u5bdf\u5230\u660e\u663e\u7684\u98ce\u683c\u5dee\u5f02\u3002", "conclusion": "\u63d0\u51fa\u7684SFT-then-PPO\u8303\u5f0f\u80fd\u6709\u6548\u89e3\u51b3\u65e0\u76d1\u7763\u6587\u672c\u98ce\u683c\u5f3a\u5ea6\u53ef\u63a7\u8fc1\u79fb\u7684\u6311\u6218\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5956\u52b1\u51fd\u6570\u5b9e\u73b0\u4e86\u5bf9\u7ec6\u5fae\u98ce\u683c\u5f3a\u5ea6\u5dee\u5f02\u7684\u51c6\u786e\u63a7\u5236\u3002"}}
{"id": "2601.01480", "categories": ["stat.ML", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.01480", "abs": "https://arxiv.org/abs/2601.01480", "authors": ["Aman Sunesh", "Allan Ma", "Siddarth Nilol"], "title": "Modeling Information Blackouts in Missing Not-At-Random Time Series Data", "comment": "8 pages, 7 figures, 3 tables", "summary": "Large-scale traffic forecasting relies on fixed sensor networks that often exhibit blackouts: contiguous intervals of missing measurements caused by detector or communication failures. These outages are typically handled under a Missing At Random (MAR) assumption, even though blackout events may correlate with unobserved traffic conditions (e.g., congestion or anomalous flow), motivating a Missing Not At Random (MNAR) treatment. We propose a latent state-space framework that jointly models (i) traffic dynamics via a linear dynamical system and (ii) sensor dropout via a Bernoulli observation channel whose probability depends on the latent traffic state. Inference uses an Extended Kalman Filter with Rauch-Tung-Striebel smoothing, and parameters are learned via an approximate EM procedure with a dedicated update for detector-specific missingness parameters. On the Seattle inductive loop detector data, introducing latent dynamics yields large gains over naive baselines, reducing blackout imputation RMSE from 7.02 (LOCF) and 5.02 (linear interpolation + seasonal naive) to 4.23 (MAR LDS), corresponding to about a 64% reduction in MSE relative to LOCF. Explicit MNAR modeling provides a consistent but smaller additional improvement on real data (imputation RMSE 4.20; 0.8% RMSE reduction relative to MAR), with similar modest gains for short-horizon post-blackout forecasts (evaluated at 1, 3, and 6 steps). In controlled synthetic experiments, the MNAR advantage increases as the true missingness dependence on latent state strengthens. Overall, temporal dynamics dominate performance, while MNAR modeling offers a principled refinement that becomes most valuable when missingness is genuinely informative.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u8054\u5408\u5efa\u6a21\u4ea4\u901a\u52a8\u6001\u548c\u4f20\u611f\u5668\u4e22\u5931\u7684\u6f5c\u72b6\u6001\u7a7a\u95f4\u6846\u67b6\uff0c\u901a\u8fc7EM\u7b97\u6cd5\u5b66\u4e60\u53c2\u6570\uff0c\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u663e\u793a\u65f6\u95f4\u52a8\u6001\u4e3b\u5bfc\u6027\u80fd\uff0cMNAR\u5efa\u6a21\u63d0\u4f9b\u539f\u5219\u6027\u6539\u8fdb", "motivation": "\u5927\u89c4\u6a21\u4ea4\u901a\u9884\u6d4b\u4e2d\u56fa\u5b9a\u4f20\u611f\u5668\u7f51\u7edc\u5e38\u51fa\u73b0\u9ed1\u5c4f\uff08\u8fde\u7eed\u7f3a\u5931\u6d4b\u91cf\uff09\uff0c\u4f20\u7edf\u65b9\u6cd5\u5047\u8bbe\u7f3a\u5931\u968f\u673a\uff08MAR\uff09\uff0c\u4f46\u9ed1\u5c4f\u4e8b\u4ef6\u53ef\u80fd\u4e0e\u672a\u89c2\u6d4b\u7684\u4ea4\u901a\u72b6\u51b5\u76f8\u5173\uff08\u5982\u62e5\u5835\uff09\uff0c\u9700\u8981\u7f3a\u5931\u975e\u968f\u673a\uff08MNAR\uff09\u5904\u7406", "method": "\u63d0\u51fa\u6f5c\u72b6\u6001\u7a7a\u95f4\u6846\u67b6\uff1a1\uff09\u901a\u8fc7\u7ebf\u6027\u52a8\u6001\u7cfb\u7edf\u5efa\u6a21\u4ea4\u901a\u52a8\u6001\uff1b2\uff09\u901a\u8fc7\u4f2f\u52aa\u5229\u89c2\u6d4b\u901a\u9053\u5efa\u6a21\u4f20\u611f\u5668\u4e22\u5931\uff0c\u5176\u6982\u7387\u53d6\u51b3\u4e8e\u6f5c\u4ea4\u901a\u72b6\u6001\u3002\u4f7f\u7528\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u8fdb\u884c\u63a8\u65ad\uff0c\u901a\u8fc7\u8fd1\u4f3cEM\u7b97\u6cd5\u5b66\u4e60\u53c2\u6570", "result": "\u5728\u897f\u96c5\u56fe\u611f\u5e94\u7ebf\u5708\u6570\u636e\u4e0a\uff0c\u5f15\u5165\u6f5c\u52a8\u6001\u76f8\u6bd4\u57fa\u7ebf\u5927\u5e45\u6539\u8fdb\uff1a\u9ed1\u5c4f\u63d2\u8865RMSE\u4ece7.02\uff08LOCF\uff09\u548c5.02\uff08\u7ebf\u6027\u63d2\u503c+\u5b63\u8282\u6734\u7d20\uff09\u964d\u81f34.23\uff08MAR LDS\uff09\uff0cMSE\u76f8\u5bf9LOCF\u51cf\u5c11\u7ea664%\u3002MNAR\u5efa\u6a21\u63d0\u4f9b\u4e00\u81f4\u4f46\u8f83\u5c0f\u7684\u989d\u5916\u6539\u8fdb\uff08RMSE 4.20\uff1b\u76f8\u5bf9MAR\u51cf\u5c110.8%\uff09", "conclusion": "\u65f6\u95f4\u52a8\u6001\u4e3b\u5bfc\u6027\u80fd\uff0cMNAR\u5efa\u6a21\u63d0\u4f9b\u539f\u5219\u6027\u6539\u8fdb\uff0c\u5f53\u7f3a\u5931\u786e\u5b9e\u5177\u6709\u4fe1\u606f\u6027\u65f6\u6700\u6709\u4ef7\u503c\u3002\u5728\u5408\u6210\u5b9e\u9a8c\u4e2d\uff0cMNAR\u4f18\u52bf\u968f\u7f3a\u5931\u5bf9\u6f5c\u72b6\u6001\u4f9d\u8d56\u589e\u5f3a\u800c\u589e\u52a0"}}
{"id": "2601.00850", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00850", "abs": "https://arxiv.org/abs/2601.00850", "authors": ["Aayush Kumar"], "title": "EdgeJury: Cross-Reviewed Small-Model Ensembles for Truthful Question Answering on Serverless Edge Inference", "comment": "24 pages,3 Figures, Submitting to IEEE Access", "summary": "Hallucinations hinder reliable question answering, especially in resource-constrained deployments where frontier-scale models or retrieval pipelines may be impractical. We present EdgeJury, a lightweight ensemble framework that improves truthfulness and robustness using only small instruction-tuned language models (3B-8B) suitable for serverless edge inference. EdgeJury orchestrates four stages: (1) parallel role-specialized generation, (2) anonymized cross-review with structured critiques and rankings, (3) chairman synthesis that integrates the strongest content while addressing flagged issues, and (4) claim-level consistency labeling based on inter-model agreement. On TruthfulQA (MC1), EdgeJury achieves 76.2% accuracy (95% CI: 72.8-79.6%), a +21.4% relative improvement over a single 8B baseline (62.8%), and outperforms standard baselines including self-consistency and majority voting under transparent compute accounting (total tokens and platform cost reported). On a 200-question adversarial EdgeCases set, EdgeJury yields +48.2% relative gains (95% CI: 44.0-52.4%). Manual analysis on 100 incorrect answers shows an approximately 55% reduction in factual hallucination errors versus the single-model baseline. Deployed on Cloudflare Workers AI, EdgeJury achieves 8.4 s median end-to-end latency, demonstrating that coordinated small-model ensembles can improve truthfulness on misconception-heavy QA benchmarks without external retrieval or proprietary large-model APIs.", "AI": {"tldr": "EdgeJury\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u96c6\u6210\u6846\u67b6\uff0c\u4f7f\u7528\u5c0f\u578b\u6307\u4ee4\u8c03\u4f18\u8bed\u8a00\u6a21\u578b\uff083B-8B\uff09\u901a\u8fc7\u56db\u9636\u6bb5\u534f\u8c03\u6d41\u7a0b\u63d0\u5347\u95ee\u7b54\u7684\u771f\u5b9e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u7279\u522b\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u90e8\u7f72\u573a\u666f\u3002", "motivation": "\u5e7b\u89c9\u95ee\u9898\u963b\u788d\u4e86\u53ef\u9760\u7684\u95ee\u7b54\u7cfb\u7edf\uff0c\u7279\u522b\u662f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u90e8\u7f72\u573a\u666f\u4e2d\uff0c\u524d\u6cbf\u89c4\u6a21\u6a21\u578b\u6216\u68c0\u7d22\u7ba1\u9053\u53ef\u80fd\u4e0d\u5207\u5b9e\u9645\u3002\u9700\u8981\u4e00\u79cd\u8f7b\u91cf\u7ea7\u89e3\u51b3\u65b9\u6848\u6765\u63d0\u5347\u5c0f\u578b\u6a21\u578b\u5728\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u771f\u5b9e\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "EdgeJury\u91c7\u7528\u56db\u9636\u6bb5\u534f\u8c03\u6846\u67b6\uff1a1) \u5e76\u884c\u89d2\u8272\u4e13\u4e1a\u5316\u751f\u6210\uff1b2) \u533f\u540d\u4ea4\u53c9\u8bc4\u5ba1\uff0c\u5305\u542b\u7ed3\u6784\u5316\u6279\u8bc4\u548c\u6392\u540d\uff1b3) \u4e3b\u5e2d\u5408\u6210\uff0c\u6574\u5408\u6700\u5f3a\u5185\u5bb9\u5e76\u89e3\u51b3\u6807\u8bb0\u7684\u95ee\u9898\uff1b4) \u57fa\u4e8e\u6a21\u578b\u95f4\u4e00\u81f4\u6027\u7684\u58f0\u660e\u7ea7\u4e00\u81f4\u6027\u6807\u8bb0\u3002\u8be5\u6846\u67b6\u4ec5\u4f7f\u7528\u5c0f\u578b\u6307\u4ee4\u8c03\u4f18\u8bed\u8a00\u6a21\u578b\uff083B-8B\uff09\uff0c\u9002\u5408\u65e0\u670d\u52a1\u5668\u8fb9\u7f18\u63a8\u7406\u3002", "result": "\u5728TruthfulQA\uff08MC1\uff09\u4e0a\u8fbe\u523076.2%\u51c6\u786e\u7387\uff0c\u76f8\u6bd4\u5355\u4e2a8B\u57fa\u7ebf\uff0862.8%\uff09\u63d0\u534721.4%\uff1b\u5728200\u4e2a\u5bf9\u6297\u6027EdgeCases\u95ee\u9898\u4e0a\u83b7\u5f9748.2%\u7684\u76f8\u5bf9\u589e\u76ca\uff1b\u4eba\u5de5\u5206\u6790\u663e\u793a\u4e8b\u5b9e\u6027\u5e7b\u89c9\u9519\u8bef\u51cf\u5c11\u7ea655%\uff1b\u5728Cloudflare Workers AI\u4e0a\u90e8\u7f72\u5b9e\u73b08.4\u79d2\u4e2d\u4f4d\u7aef\u5230\u7aef\u5ef6\u8fdf\u3002", "conclusion": "\u534f\u8c03\u7684\u5c0f\u578b\u6a21\u578b\u96c6\u6210\u53ef\u4ee5\u5728\u4e0d\u4f9d\u8d56\u5916\u90e8\u68c0\u7d22\u6216\u4e13\u6709\u5927\u6a21\u578bAPI\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u5728\u8bef\u89e3\u5bc6\u96c6\u578b\u95ee\u7b54\u57fa\u51c6\u4e0a\u7684\u771f\u5b9e\u6027\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01335", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.01335", "abs": "https://arxiv.org/abs/2601.01335", "authors": ["Zihan Li", "Ziming Wang", "Chenning Liu", "Xin Wang"], "title": "Neural-network-based Self-triggered Observed Platoon Control for Autonomous Vehicles", "comment": null, "summary": "This paper investigates autonomous vehicle (AV) platoon control under uncertain dynamics and intermittent communication, which remains a critical challenge in intelligent transportation systems. To address these issues, this paper proposes an adaptive consensus tracking control framework for nonlinear multi-agent systems (MASs). The proposed approach integrates backstepping design, a nonlinear sampled-data observer, radial basis function neural networks, and a self-triggered communication mechanism. The radial basis function neural networks approximate unknown nonlinearities and time-varying disturbances, thereby enhancing system robustness. A distributed observer estimates neighboring states based on limited and intermittent measurements, thereby reducing dependence on continuous communication. Moreover, self-triggered mechanism is developed to determine triggering instants, guaranteeing a strictly positive minimum inter-event time and preventing Zeno behavior. The theoretical analysis proves that all closed-loop signals are uniformly ultimately bounded (UUB), and tracking errors converge to a compact set. Simulation results demonstrate that the proposed approach achieves high robustness, adaptability, and communication efficiency, making it suitable for real-world networked vehicle systems.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u9002\u5e94\u4e00\u81f4\u6027\u8ddf\u8e2a\u63a7\u5236\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u4e0d\u786e\u5b9a\u52a8\u6001\u548c\u95f4\u6b47\u901a\u4fe1\u4e0b\u7684\u81ea\u52a8\u9a7e\u9a76\u8f66\u961f\u63a7\u5236\u95ee\u9898\uff0c\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u3001\u5206\u5e03\u5f0f\u89c2\u6d4b\u5668\u548c\u81ea\u89e6\u53d1\u673a\u5236", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u961f\u5728\u4e0d\u786e\u5b9a\u52a8\u6001\u548c\u95f4\u6b47\u901a\u4fe1\u6761\u4ef6\u4e0b\u7684\u63a7\u5236\u662f\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u7684\u5173\u952e\u6311\u6218\uff0c\u9700\u8981\u89e3\u51b3\u975e\u7ebf\u6027\u3001\u672a\u77e5\u6270\u52a8\u548c\u901a\u4fe1\u9650\u5236\u95ee\u9898", "method": "\u96c6\u6210\u53cd\u6b65\u8bbe\u8ba1\u3001\u975e\u7ebf\u6027\u91c7\u6837\u6570\u636e\u89c2\u6d4b\u5668\u3001\u5f84\u5411\u57fa\u51fd\u6570\u795e\u7ecf\u7f51\u7edc\u548c\u81ea\u89e6\u53d1\u901a\u4fe1\u673a\u5236\uff0c\u795e\u7ecf\u7f51\u7edc\u8fd1\u4f3c\u672a\u77e5\u975e\u7ebf\u6027\u548c\u65f6\u53d8\u6270\u52a8\uff0c\u5206\u5e03\u5f0f\u89c2\u6d4b\u5668\u4f30\u8ba1\u90bb\u5c45\u72b6\u6001\uff0c\u81ea\u89e6\u53d1\u673a\u5236\u786e\u5b9a\u89e6\u53d1\u65f6\u523b", "result": "\u7406\u8bba\u5206\u6790\u8bc1\u660e\u6240\u6709\u95ed\u73af\u4fe1\u53f7\u4e00\u81f4\u6700\u7ec8\u6709\u754c\uff0c\u8ddf\u8e2a\u8bef\u5dee\u6536\u655b\u5230\u7d27\u96c6\uff1b\u4eff\u771f\u7ed3\u679c\u663e\u793a\u65b9\u6cd5\u5177\u6709\u9ad8\u9c81\u68d2\u6027\u3001\u9002\u5e94\u6027\u548c\u901a\u4fe1\u6548\u7387", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u9002\u7528\u4e8e\u5b9e\u9645\u7f51\u7edc\u5316\u8f66\u8f86\u7cfb\u7edf\uff0c\u80fd\u6709\u6548\u5904\u7406\u4e0d\u786e\u5b9a\u52a8\u6001\u548c\u95f4\u6b47\u901a\u4fe1\u95ee\u9898\uff0c\u907f\u514dZeno\u884c\u4e3a"}}
{"id": "2601.01575", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2601.01575", "abs": "https://arxiv.org/abs/2601.01575", "authors": ["Hanfeng Zeng", "Yang Liu", "Wenqing Ouyang", "Andre Milzarek"], "title": "A MINRES-based Linesearch Algorithm for Nonconvex Optimization with Non-positive Curvature Detection", "comment": "36 pages, 9 figures", "summary": "We propose a MINRES-based Newton-type algorithm for solving unconstrained nonconvex optimization problems. Our approach uses the minimal residual method (MINRES), a well-known solver for indefinite symmetric linear systems, to compute descent directions that leverage second-order and non-positive curvature (NPC) information. Comprehensive asymptotic convergence properties are derived under standard assumptions. In particular, under the Kurdyka-\u0141ojasiewicz inequality and a mild NPC-detectability condition, we prove that our algorithm can avoid strict saddle points and converge to second-order critical points. This is primarily achieved by integrating proper regularization techniques and forward linesearch mechanisms along NPC directions. Furthermore, fast local superlinear convergence to potentially non-isolated minima is established, when the local Polyak-\u0141ojasiewicz condition is satisfied. Numerical experiments on the CUTEst test collection and on a deep auto-encoder problem illustrate the efficiency of the proposed method.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eMINRES\u7684\u725b\u987f\u578b\u7b97\u6cd5\u6c42\u89e3\u975e\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u5229\u7528MINRES\u8ba1\u7b97\u5305\u542b\u4e8c\u9636\u548c\u975e\u6b63\u66f2\u7387\u4fe1\u606f\u7684\u4e0b\u964d\u65b9\u5411\uff0c\u80fd\u591f\u907f\u5f00\u978d\u70b9\u6536\u655b\u5230\u4e8c\u9636\u4e34\u754c\u70b9\uff0c\u5728Polyak-\u0141ojasiewicz\u6761\u4ef6\u4e0b\u5b9e\u73b0\u8d85\u7ebf\u6027\u6536\u655b\u3002", "motivation": "\u4f20\u7edf\u4f18\u5316\u7b97\u6cd5\u5728\u5904\u7406\u975e\u51f8\u95ee\u9898\u65f6\u53ef\u80fd\u9677\u5165\u978d\u70b9\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5229\u7528\u4e8c\u9636\u66f2\u7387\u4fe1\u606f\u5e76\u907f\u5f00\u978d\u70b9\u7684\u9ad8\u6548\u7b97\u6cd5\u3002", "method": "\u4f7f\u7528\u6700\u5c0f\u6b8b\u5dee\u6cd5(MINRES)\u6c42\u89e3\u4e0d\u5b9a\u5bf9\u79f0\u7ebf\u6027\u7cfb\u7edf\uff0c\u8ba1\u7b97\u5305\u542b\u4e8c\u9636\u548c\u975e\u6b63\u66f2\u7387\u4fe1\u606f\u7684\u4e0b\u964d\u65b9\u5411\uff0c\u7ed3\u5408\u6b63\u5219\u5316\u6280\u672f\u548c\u524d\u5411\u7ebf\u641c\u7d22\u673a\u5236\u3002", "result": "\u5728Kurdyka-\u0141ojasiewicz\u4e0d\u7b49\u5f0f\u548cNPC\u53ef\u68c0\u6d4b\u6761\u4ef6\u4e0b\uff0c\u7b97\u6cd5\u80fd\u907f\u5f00\u4e25\u683c\u978d\u70b9\u6536\u655b\u5230\u4e8c\u9636\u4e34\u754c\u70b9\uff1b\u5728Polyak-\u0141ojasiewicz\u6761\u4ef6\u4e0b\u5b9e\u73b0\u5c40\u90e8\u8d85\u7ebf\u6027\u6536\u655b\uff1b\u5728CUTEst\u6d4b\u8bd5\u96c6\u548c\u6df1\u5ea6\u81ea\u7f16\u7801\u5668\u95ee\u9898\u4e0a\u9a8c\u8bc1\u4e86\u6548\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684MINRES-based\u725b\u987f\u578b\u7b97\u6cd5\u80fd\u6709\u6548\u5904\u7406\u975e\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u907f\u5f00\u978d\u70b9\u5e76\u6536\u655b\u5230\u4e8c\u9636\u4e34\u754c\u70b9\uff0c\u5728\u9002\u5f53\u6761\u4ef6\u4e0b\u5b9e\u73b0\u5feb\u901f\u6536\u655b\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2601.00843", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00843", "abs": "https://arxiv.org/abs/2601.00843", "authors": ["Ayda Aghaei Nia"], "title": "OmniNeuro: A Multimodal HCI Framework for Explainable BCI Feedback via Generative AI and Sonification", "comment": "16 pages, 7 figures, 3 tables. Source code and implementation available at: https://github.com/ayda-aghaei/OmniNeuro. Highlights the use of LLMs (Gemini) and Quantum probability formalism for real-time BCI explainability", "summary": "While Deep Learning has improved Brain-Computer Interface (BCI) decoding accuracy, clinical adoption is hindered by the \"Black Box\" nature of these algorithms, leading to user frustration and poor neuroplasticity outcomes. We propose OmniNeuro, a novel HCI framework that transforms the BCI from a silent decoder into a transparent feedback partner. OmniNeuro integrates three interpretability engines: (1) Physics (Energy), (2) Chaos (Fractal Complexity), and (3) Quantum-Inspired uncertainty modeling. These metrics drive real-time Neuro-Sonification and Generative AI Clinical Reports. Evaluated on the PhysioNet dataset ($N=109$), the system achieved a mean accuracy of 58.52%, with qualitative pilot studies ($N=3$) confirming that explainable feedback helps users regulate mental effort and reduces the \"trial-and-error\" phase. OmniNeuro is decoder-agnostic, acting as an essential interpretability layer for any state-of-the-art architecture.", "AI": {"tldr": "OmniNeuro\u662f\u4e00\u4e2a\u65b0\u578bHCI\u6846\u67b6\uff0c\u5c06BCI\u4ece\u9ed1\u76d2\u89e3\u7801\u5668\u8f6c\u53d8\u4e3a\u900f\u660e\u7684\u53cd\u9988\u4f19\u4f34\uff0c\u901a\u8fc7\u7269\u7406\u3001\u6df7\u6c8c\u548c\u91cf\u5b50\u542f\u53d1\u7684\u53ef\u89e3\u91ca\u6027\u5f15\u64ce\u63d0\u4f9b\u5b9e\u65f6\u795e\u7ecf\u58f0\u5316\u548c\u751f\u6210\u5f0fAI\u4e34\u5e8a\u62a5\u544a\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u867d\u7136\u63d0\u9ad8\u4e86\u8111\u673a\u63a5\u53e3\u7684\u89e3\u7801\u7cbe\u5ea6\uff0c\u4f46\u5176\"\u9ed1\u76d2\"\u7279\u6027\u963b\u788d\u4e86\u4e34\u5e8a\u91c7\u7528\uff0c\u5bfc\u81f4\u7528\u6237\u632b\u6298\u611f\u548c\u795e\u7ecf\u53ef\u5851\u6027\u7ed3\u679c\u4e0d\u4f73\u3002\u9700\u8981\u63d0\u9ad8BCI\u7cfb\u7edf\u7684\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51faOmniNeuro\u6846\u67b6\uff0c\u96c6\u6210\u4e09\u79cd\u53ef\u89e3\u91ca\u6027\u5f15\u64ce\uff1a(1)\u7269\u7406(\u80fd\u91cf)\u5206\u6790\uff0c(2)\u6df7\u6c8c(\u5206\u5f62\u590d\u6742\u6027)\u5206\u6790\uff0c(3)\u91cf\u5b50\u542f\u53d1\u7684\u786e\u5b9a\u6027\u5efa\u6a21\u3002\u8fd9\u4e9b\u6307\u6807\u9a71\u52a8\u5b9e\u65f6\u795e\u7ecf\u58f0\u5316\u548c\u751f\u6210\u5f0fAI\u4e34\u5e8a\u62a5\u544a\uff0c\u6846\u67b6\u4e0e\u89e3\u7801\u5668\u65e0\u5173\uff0c\u53ef\u4f5c\u4e3a\u4efb\u4f55\u6700\u5148\u8fdb\u67b6\u6784\u7684\u53ef\u89e3\u91ca\u6027\u5c42\u3002", "result": "\u5728PhysioNet\u6570\u636e\u96c6(N=109)\u4e0a\u8bc4\u4f30\uff0c\u7cfb\u7edf\u5e73\u5747\u51c6\u786e\u7387\u8fbe\u523058.52%\u3002\u5b9a\u6027\u8bd5\u70b9\u7814\u7a76(N=3)\u8bc1\u5b9e\uff0c\u53ef\u89e3\u91ca\u7684\u53cd\u9988\u6709\u52a9\u4e8e\u7528\u6237\u8c03\u8282\u5fc3\u7406\u52aa\u529b\uff0c\u51cf\u5c11\"\u8bd5\u9519\"\u9636\u6bb5\u3002", "conclusion": "OmniNeuro\u6210\u529f\u5c06BCI\u4ece\u6c89\u9ed8\u89e3\u7801\u5668\u8f6c\u53d8\u4e3a\u900f\u660e\u53cd\u9988\u4f19\u4f34\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u53ef\u89e3\u91ca\u6027\u5f15\u64ce\u6539\u5584\u4e86\u7528\u6237\u4f53\u9a8c\u548c\u4e34\u5e8a\u6548\u679c\uff0c\u4e3a\u4efb\u4f55\u6700\u5148\u8fdbBCI\u67b6\u6784\u63d0\u4f9b\u4e86\u5fc5\u8981\u7684\u53ef\u89e3\u91ca\u6027\u5c42\u3002"}}
{"id": "2601.01091", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01091", "abs": "https://arxiv.org/abs/2601.01091", "authors": ["Haq Nawaz Malik"], "title": "ks-lit-3m: A 3.1 million word kashmiri text dataset for large language model pretraining", "comment": null, "summary": "Large Language Models (LLMs) demonstrate remarkable fluency across high-resource languages yet consistently fail to generate coherent text in Kashmiri, a language spoken by approximately seven million people. This performance disparity stems not from inherent model limitations but from a critical scarcity of high-quality training data. Decades of Kashmiri literature remain inaccessible to modern NLP pipelines due to their encoding in the proprietary InPage desktop publishing format. This paper introduces KS-LIT-3M, a curated corpus of 3.1 million words (16.4 million characters) specifically designed for pretraining language models on Kashmiri. The dataset is structured as a single continuous linear text stream, optimized for causal language model training where models learn to predict subsequent tokens from preceding context. The corpus was constructed through the development of a specialized InPage-to-Unicode converter, followed by rigorous preprocessing including English contamination removal, character normalization, and quality validation. Encompassing 131,607 unique words drawn from diverse genres including literary works, journalistic writing, academic texts, and religious scholarship, KS-LIT-3M addresses a fundamental resource gap for Kashmiri language technology. The dataset is released under the CC-BY-4.0 license to facilitate research in Kashmiri natural language processing.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9\u514b\u4ec0\u7c73\u5c14\u8bed\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u7684\u95ee\u9898\uff0c\u6784\u5efa\u4e86\u5305\u542b310\u4e07\u5355\u8bcd\u7684KS-LIT-3M\u8bed\u6599\u5e93\uff0c\u901a\u8fc7\u5f00\u53d1\u4e13\u95e8\u7684InPage\u8f6cUnicode\u8f6c\u6362\u5668\u89e3\u51b3\u4e86\u5386\u53f2\u6587\u732e\u7684\u7f16\u7801\u969c\u788d\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u514b\u4ec0\u7c73\u5c14\u8bed\uff08\u7ea6700\u4e07\u4eba\u4f7f\u7528\uff09\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u3002\u6570\u5341\u5e74\u7684\u514b\u4ec0\u7c73\u5c14\u8bed\u6587\u732e\u56e0\u4f7f\u7528\u4e13\u6709\u7684InPage\u684c\u9762\u51fa\u7248\u683c\u5f0f\u7f16\u7801\u800c\u65e0\u6cd5\u88ab\u73b0\u4ee3NLP\u6d41\u7a0b\u5904\u7406\u3002", "method": "\u5f00\u53d1\u4e13\u95e8\u7684InPage-to-Unicode\u8f6c\u6362\u5668\uff0c\u7136\u540e\u8fdb\u884c\u4e25\u683c\u7684\u9884\u5904\u7406\uff08\u5305\u62ec\u53bb\u9664\u82f1\u8bed\u6c61\u67d3\u3001\u5b57\u7b26\u6807\u51c6\u5316\u548c\u8d28\u91cf\u9a8c\u8bc1\uff09\uff0c\u6784\u5efa\u4e86\u5305\u542b310\u4e07\u5355\u8bcd\uff081640\u4e07\u5b57\u7b26\uff09\u7684KS-LIT-3M\u8bed\u6599\u5e93\uff0c\u6db5\u76d6\u6587\u5b66\u3001\u65b0\u95fb\u3001\u5b66\u672f\u548c\u5b97\u6559\u7b49\u591a\u79cd\u4f53\u88c1\u3002", "result": "\u521b\u5efa\u4e86\u5305\u542b131,607\u4e2a\u72ec\u7279\u5355\u8bcd\u7684\u514b\u4ec0\u7c73\u5c14\u8bed\u8bed\u6599\u5e93\uff0c\u91c7\u7528\u5355\u8fde\u7eed\u7ebf\u6027\u6587\u672c\u6d41\u683c\u5f0f\uff0c\u4f18\u5316\u7528\u4e8e\u56e0\u679c\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u3002\u6570\u636e\u96c6\u4ee5CC-BY-4.0\u8bb8\u53ef\u8bc1\u53d1\u5e03\uff0c\u586b\u8865\u4e86\u514b\u4ec0\u7c73\u5c14\u8bed\u8bed\u8a00\u6280\u672f\u7684\u57fa\u7840\u8d44\u6e90\u7a7a\u767d\u3002", "conclusion": "KS-LIT-3M\u8bed\u6599\u5e93\u89e3\u51b3\u4e86\u514b\u4ec0\u7c73\u5c14\u8bedNLP\u7814\u7a76\u7684\u5173\u952e\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u4e3a\u8bad\u7ec3\u514b\u4ec0\u7c73\u5c14\u8bed\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u5fc5\u8981\u7684\u57fa\u7840\u8bbe\u65bd\uff0c\u6709\u52a9\u4e8e\u4fc3\u8fdb\u514b\u4ec0\u7c73\u5c14\u8bed\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7814\u7a76\u7684\u53d1\u5c55\u3002"}}
{"id": "2601.00994", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.00994", "abs": "https://arxiv.org/abs/2601.00994", "authors": ["Michael Bao"], "title": "ElecTwit: A Framework for Studying Persuasion in Multi-Agent Social Systems", "comment": "In proceedings of 2025 IEEE International Conference on Agentic AI (ICA)", "summary": "This paper introduces ElecTwit, a simulation framework designed to study persuasion within multi-agent systems, specifically emulating the interactions on social media platforms during a political election. By grounding our experiments in a realistic environment, we aimed to overcome the limitations of game-based simulations often used in prior research. We observed the comprehensive use of 25 specific persuasion techniques across most tested LLMs, encompassing a wider range than previously reported. The variations in technique usage and overall persuasion output between models highlight how different model architectures and training can impact the dynamics in realistic social simulations. Additionally, we observed unique phenomena such as \"kernel of truth\" messages and spontaneous developments with an \"ink\" obsession, where agents collectively demanded written proof. Our study provides a foundation for evaluating persuasive LLM agents in real-world contexts, ensuring alignment and preventing dangerous outcomes.", "AI": {"tldr": "ElecTwit\u662f\u4e00\u4e2a\u6a21\u62df\u793e\u4ea4\u5a92\u4f53\u653f\u6cbb\u9009\u4e3e\u4e2d\u591a\u667a\u80fd\u4f53\u8bf4\u670d\u4ea4\u4e92\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u771f\u5b9e\u73af\u5883\u5b9e\u9a8c\u53d1\u73b0LLM\u5e7f\u6cdb\u4f7f\u752825\u79cd\u8bf4\u670d\u6280\u5de7\uff0c\u6a21\u578b\u67b6\u6784\u5dee\u5f02\u5f71\u54cd\u8bf4\u670d\u52a8\u6001\uff0c\u5e76\u89c2\u5bdf\u5230\u72ec\u7279\u73b0\u8c61\u5982\"\u771f\u76f8\u6838\u5fc3\"\u6d88\u606f\u548c\"\u58a8\u6c34\"\u5f3a\u8feb\u75c7\u3002", "motivation": "\u514b\u670d\u4ee5\u5f80\u7814\u7a76\u4e2d\u57fa\u4e8e\u6e38\u620f\u6a21\u62df\u7684\u5c40\u9650\u6027\uff0c\u5728\u66f4\u771f\u5b9e\u7684\u73af\u5883\u4e2d\u7814\u7a76\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u8bf4\u670d\u884c\u4e3a\uff0c\u7279\u522b\u662f\u5728\u793e\u4ea4\u5a92\u4f53\u653f\u6cbb\u9009\u4e3e\u573a\u666f\u4e0b\u3002", "method": "\u5f00\u53d1ElecTwit\u6a21\u62df\u6846\u67b6\uff0c\u5728\u6a21\u62df\u793e\u4ea4\u5a92\u4f53\u653f\u6cbb\u9009\u4e3e\u7684\u903c\u771f\u73af\u5883\u4e2d\u6d4b\u8bd5\u591a\u4e2aLLM\u6a21\u578b\uff0c\u5206\u6790\u5b83\u4eec\u4f7f\u7528\u7684\u8bf4\u670d\u6280\u5de7\u548c\u4ea4\u4e92\u52a8\u6001\u3002", "result": "\u89c2\u5bdf\u5230\u6240\u6709\u6d4b\u8bd5\u7684LLM\u5e7f\u6cdb\u4f7f\u752825\u79cd\u7279\u5b9a\u8bf4\u670d\u6280\u5de7\uff0c\u8303\u56f4\u8d85\u8fc7\u4ee5\u5f80\u62a5\u544a\uff1b\u4e0d\u540c\u6a21\u578b\u5728\u6280\u5de7\u4f7f\u7528\u548c\u6574\u4f53\u8bf4\u670d\u8f93\u51fa\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff1b\u53d1\u73b0\u4e86\"\u771f\u76f8\u6838\u5fc3\"\u6d88\u606f\u548c\"\u58a8\u6c34\"\u5f3a\u8feb\u75c7\u7b49\u72ec\u7279\u73b0\u8c61\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5728\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u8bc4\u4f30\u6709\u8bf4\u670d\u529b\u7684LLM\u667a\u80fd\u4f53\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u786e\u4fdd\u5bf9\u9f50\u5e76\u9632\u6b62\u5371\u9669\u7ed3\u679c\uff0c\u6a21\u578b\u67b6\u6784\u548c\u8bad\u7ec3\u7684\u5dee\u5f02\u663e\u8457\u5f71\u54cd\u793e\u4f1a\u6a21\u62df\u4e2d\u7684\u8bf4\u670d\u52a8\u6001\u3002"}}
{"id": "2601.01594", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01594", "abs": "https://arxiv.org/abs/2601.01594", "authors": ["Alois Duston", "Tan Bui-Thanh"], "title": "Variance-Reduced Diffusion Sampling via Conditional Score Expectation Identity", "comment": null, "summary": "We introduce and prove a \\textbf{Conditional Score Expectation (CSE)} identity: an exact relation for the marginal score of affine diffusion processes that links scores across time via a conditional expectation under the forward dynamics. Motivated by this identity, we propose a CSE-based statistical estimator for the score using a Self-Normalized Importance Sampling (SNIS) procedure with prior samples and forward noise. We analyze its relationship to the standard Tweedie estimator, proving anti-correlation for Gaussian targets and establishing the same behavior for general targets in the small time-step regime. Exploiting this structure, we derive a variance-minimizing blended score estimator given by a state--time dependent convex combination of the CSE and Tweedie estimators. Numerical experiments show that this optimal-blending estimator reduces variance and improves sample quality for a fixed computational budget compared to either baseline. We further extend the framework to Bayesian inverse problems via likelihood-informed SNIS weights, and demonstrate improved reconstruction quality and sample diversity on high-dimensional image reconstruction tasks and PDE-governed inverse problems.", "AI": {"tldr": "\u63d0\u51fa\u6761\u4ef6\u5206\u6570\u671f\u671b(CSE)\u6052\u7b49\u5f0f\uff0c\u57fa\u4e8e\u6b64\u5f00\u53d1SNIS\u5206\u6570\u4f30\u8ba1\u5668\uff0c\u8bc1\u660e\u4e0eTweedie\u4f30\u8ba1\u5668\u7684\u53cd\u76f8\u5173\u6027\uff0c\u63a8\u5bfc\u65b9\u5dee\u6700\u5c0f\u5316\u6df7\u5408\u4f30\u8ba1\u5668\uff0c\u5e76\u6269\u5c55\u5230\u8d1d\u53f6\u65af\u9006\u95ee\u9898\u3002", "motivation": "\u9488\u5bf9\u4eff\u5c04\u6269\u6563\u8fc7\u7a0b\u7684\u5206\u6570\u4f30\u8ba1\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u5206\u6570\u4f30\u8ba1\u5668\u6765\u6539\u5584\u91c7\u6837\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u3002", "method": "1) \u8bc1\u660eCSE\u6052\u7b49\u5f0f\uff1b2) \u63d0\u51fa\u57fa\u4e8eSNIS\u7684CSE\u5206\u6570\u4f30\u8ba1\u5668\uff1b3) \u5206\u6790CSE\u4e0eTweedie\u4f30\u8ba1\u5668\u7684\u5173\u7cfb\uff1b4) \u63a8\u5bfc\u65b9\u5dee\u6700\u5c0f\u5316\u7684\u6df7\u5408\u4f30\u8ba1\u5668\uff1b5) \u6269\u5c55\u5230\u8d1d\u53f6\u65af\u9006\u95ee\u9898\u3002", "result": "\u6df7\u5408\u4f30\u8ba1\u5668\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u964d\u4f4e\u4e86\u65b9\u5dee\uff0c\u5728\u56fa\u5b9a\u8ba1\u7b97\u9884\u7b97\u4e0b\u63d0\u9ad8\u4e86\u91c7\u6837\u8d28\u91cf\uff0c\u5728\u56fe\u50cf\u91cd\u5efa\u548cPDE\u9006\u95ee\u9898\u4e2d\u8868\u73b0\u51fa\u66f4\u597d\u7684\u91cd\u5efa\u8d28\u91cf\u548c\u6837\u672c\u591a\u6837\u6027\u3002", "conclusion": "CSE\u6846\u67b6\u4e3a\u5206\u6570\u4f30\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u5de5\u5177\uff0c\u6df7\u5408\u4f30\u8ba1\u5668\u5728\u591a\u79cd\u5e94\u7528\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u548c\u9006\u95ee\u9898\u6c42\u89e3\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2601.00853", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00853", "abs": "https://arxiv.org/abs/2601.00853", "authors": ["Sameer Rahil", "Zain Abdullah Ahmad", "Talha Asif"], "title": "FedSCAM (Federated Sharpness-Aware Minimization with Clustered Aggregation and Modulation): Scam-resistant SAM for Robust Federated Optimization in Heterogeneous Environments", "comment": "13 pages, 27 figures", "summary": "Federated Learning (FL) enables collaborative model training across decentralized edge devices while preserving data privacy. However, statistical heterogeneity among clients, often manifested as non-IID label distributions, poses significant challenges to convergence and generalization. While Sharpness-Aware Minimization (SAM) has been introduced to FL to seek flatter, more robust minima, existing approaches typically apply a uniform perturbation radius across all clients, ignoring client-specific heterogeneity. In this work, we propose \\textbf{FedSCAM} (Federated Sharpness-Aware Minimization with Clustered Aggregation and Modulation), a novel algorithm that dynamically adjusts the SAM perturbation radius and aggregation weights based on client-specific heterogeneity scores. By calculating a heterogeneity metric for each client and modulating the perturbation radius inversely to this score, FedSCAM prevents clients with high variance from destabilizing the global model. Furthermore, we introduce a heterogeneity-aware weighted aggregation mechanism that prioritizes updates from clients that align with the global optimization direction. Extensive experiments on CIFAR-10 and Fashion-MNIST under various degrees of Dirichlet-based label skew demonstrate that FedSCAM achieves competitive performance among state-of-the-art baselines, including FedSAM, FedLESAM, etc. in terms of convergence speed and final test accuracy.", "AI": {"tldr": "FedSCAM\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u90a6\u5b66\u4e60\u4e2d\u57fa\u4e8e\u5ba2\u6237\u7aef\u5f02\u8d28\u6027\u52a8\u6001\u8c03\u6574SAM\u6270\u52a8\u534a\u5f84\u548c\u805a\u5408\u6743\u91cd\u7684\u7b97\u6cd5\uff0c\u901a\u8fc7\u5f02\u8d28\u6027\u611f\u77e5\u7684\u52a0\u6743\u805a\u5408\u673a\u5236\u63d0\u5347\u6a21\u578b\u6536\u655b\u901f\u5ea6\u548c\u6700\u7ec8\u7cbe\u5ea6\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u4e2d\u5ba2\u6237\u7aef\u6570\u636e\u7684\u7edf\u8ba1\u5f02\u8d28\u6027\uff08\u7279\u522b\u662f\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6807\u7b7e\u5206\u5e03\uff09\u5bf9\u6a21\u578b\u6536\u655b\u548c\u6cdb\u5316\u5e26\u6765\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5bf9\u6240\u6709\u5ba2\u6237\u7aef\u4f7f\u7528\u7edf\u4e00\u7684SAM\u6270\u52a8\u534a\u5f84\uff0c\u5ffd\u7565\u4e86\u5ba2\u6237\u7aef\u7279\u5b9a\u7684\u5f02\u8d28\u6027\u5dee\u5f02\u3002", "method": "\u63d0\u51faFedSCAM\u7b97\u6cd5\uff1a1) \u4e3a\u6bcf\u4e2a\u5ba2\u6237\u7aef\u8ba1\u7b97\u5f02\u8d28\u6027\u5206\u6570\uff1b2) \u6839\u636e\u5f02\u8d28\u6027\u5206\u6570\u53cd\u5411\u8c03\u5236SAM\u6270\u52a8\u534a\u5f84\uff0c\u9632\u6b62\u9ad8\u65b9\u5dee\u5ba2\u6237\u7aef\u7834\u574f\u5168\u5c40\u6a21\u578b\u7a33\u5b9a\u6027\uff1b3) \u5f15\u5165\u5f02\u8d28\u6027\u611f\u77e5\u7684\u52a0\u6743\u805a\u5408\u673a\u5236\uff0c\u4f18\u5148\u8003\u8651\u4e0e\u5168\u5c40\u4f18\u5316\u65b9\u5411\u4e00\u81f4\u7684\u5ba2\u6237\u7aef\u66f4\u65b0\u3002", "result": "\u5728CIFAR-10\u548cFashion-MNIST\u6570\u636e\u96c6\u4e0a\uff0c\u57fa\u4e8e\u4e0d\u540cDirichlet\u6807\u7b7e\u504f\u659c\u7a0b\u5ea6\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFedSCAM\u5728\u6536\u655b\u901f\u5ea6\u548c\u6700\u7ec8\u6d4b\u8bd5\u7cbe\u5ea6\u65b9\u9762\u4f18\u4e8eFedSAM\u3001FedLESAM\u7b49\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "FedSCAM\u901a\u8fc7\u52a8\u6001\u8c03\u6574SAM\u6270\u52a8\u534a\u5f84\u548c\u5f02\u8d28\u6027\u611f\u77e5\u805a\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u7edf\u8ba1\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2601.01409", "categories": ["eess.SY", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01409", "abs": "https://arxiv.org/abs/2601.01409", "authors": ["Chuyuan Tao", "Fanxin Wang", "Haolong Jiang", "Jia He", "Yiyang Chen", "Qinglei Bu"], "title": "Sampling Strategy Design for Model Predictive Path Integral Control on Legged Robot Locomotion", "comment": null, "summary": "Model Predictive Path Integral (MPPI) control has emerged as a powerful sampling-based optimal control method for complex, nonlinear, and high-dimensional systems. However, directly applying MPPI to legged robotic systems presents several challenges. This paper systematically investigates the role of sampling strategy design within the MPPI framework for legged robot locomotion. Based upon the idea of structured control parameterization, we explore and compare multiple sampling strategies within the framework, including both unstructured and spline-based approaches. Through extensive simulations on a quadruped robot platform, we evaluate how different sampling strategies affect control smoothness, task performance, robustness, and sample efficiency. The results provide new insights into the practical implications of sampling design for deploying MPPI on complex legged systems.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86MPPI\u6846\u67b6\u4e2d\u91c7\u6837\u7b56\u7565\u8bbe\u8ba1\u5bf9\u56db\u8db3\u673a\u5668\u4eba\u8fd0\u52a8\u63a7\u5236\u7684\u5f71\u54cd\uff0c\u6bd4\u8f83\u4e86\u975e\u7ed3\u6784\u5316\u548c\u6837\u6761\u57fa\u65b9\u6cd5\uff0c\u901a\u8fc7\u4eff\u771f\u8bc4\u4f30\u4e86\u4e0d\u540c\u7b56\u7565\u5bf9\u63a7\u5236\u5e73\u6ed1\u6027\u3001\u4efb\u52a1\u6027\u80fd\u3001\u9c81\u68d2\u6027\u548c\u91c7\u6837\u6548\u7387\u7684\u5f71\u54cd\u3002", "motivation": "MPPI\u63a7\u5236\u5df2\u6210\u4e3a\u5904\u7406\u590d\u6742\u975e\u7ebf\u6027\u9ad8\u7ef4\u7cfb\u7edf\u7684\u5f3a\u5927\u91c7\u6837\u4f18\u5316\u63a7\u5236\u65b9\u6cd5\uff0c\u4f46\u76f4\u63a5\u5e94\u7528\u4e8e\u8db3\u5f0f\u673a\u5668\u4eba\u7cfb\u7edf\u9762\u4e34\u8bf8\u591a\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u7cfb\u7edf\u7814\u7a76MPPI\u6846\u67b6\u4e2d\u91c7\u6837\u7b56\u7565\u8bbe\u8ba1\u5bf9\u8db3\u5f0f\u673a\u5668\u4eba\u8fd0\u52a8\u63a7\u5236\u7684\u4f5c\u7528\u3002", "method": "\u57fa\u4e8e\u7ed3\u6784\u5316\u63a7\u5236\u53c2\u6570\u5316\u7684\u601d\u60f3\uff0c\u5728MPPI\u6846\u67b6\u5185\u63a2\u7d22\u5e76\u6bd4\u8f83\u4e86\u591a\u79cd\u91c7\u6837\u7b56\u7565\uff0c\u5305\u62ec\u975e\u7ed3\u6784\u5316\u548c\u6837\u6761\u57fa\u65b9\u6cd5\u3002\u901a\u8fc7\u56db\u8db3\u673a\u5668\u4eba\u5e73\u53f0\u7684\u5927\u91cf\u4eff\u771f\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u4e0d\u540c\u91c7\u6837\u7b56\u7565\u5bf9\u63a7\u5236\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u4e3a\u91c7\u6837\u8bbe\u8ba1\u5728\u590d\u6742\u8db3\u5f0f\u7cfb\u7edf\u4e0a\u90e8\u7f72MPPI\u7684\u5b9e\u9645\u610f\u4e49\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u91c7\u6837\u7b56\u7565\u5728\u63a7\u5236\u5e73\u6ed1\u6027\u3001\u4efb\u52a1\u6027\u80fd\u3001\u9c81\u68d2\u6027\u548c\u91c7\u6837\u6548\u7387\u65b9\u9762\u7684\u5dee\u5f02\u3002", "conclusion": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86MPPI\u6846\u67b6\u4e2d\u91c7\u6837\u7b56\u7565\u8bbe\u8ba1\u5bf9\u8db3\u5f0f\u673a\u5668\u4eba\u8fd0\u52a8\u63a7\u5236\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u5728\u5b9e\u9645\u590d\u6742\u8db3\u5f0f\u7cfb\u7edf\u4e2d\u6709\u6548\u90e8\u7f72MPPI\u63a7\u5236\u63d0\u4f9b\u4e86\u6307\u5bfc\u6027\u89c1\u89e3\u3002"}}
{"id": "2601.01591", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2601.01591", "abs": "https://arxiv.org/abs/2601.01591", "authors": ["Giuseppe Buttazzo", "Juan Casado-D\u00edaz", "Faustino Maestre"], "title": "Optimization problems for elliptic PDEs", "comment": "19 pages, 8 figures", "summary": "In this paper we consider some optimal control problems governed by elliptic partial differential equations. The solution is the state variable, while the control variable is, depending on the case, the coefficient of the PDE, the potential, the right-hand side. The cost functional is of integral type and involves both the state and control variables.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u7531\u692d\u5706\u504f\u5fae\u5206\u65b9\u7a0b\u63a7\u5236\u7684\u6700\u4f18\u63a7\u5236\u95ee\u9898\uff0c\u5176\u4e2d\u72b6\u6001\u53d8\u91cf\u662fPDE\u89e3\uff0c\u63a7\u5236\u53d8\u91cf\u53ef\u4ee5\u662fPDE\u7cfb\u6570\u3001\u52bf\u51fd\u6570\u6216\u53f3\u7aef\u9879\uff0c\u6210\u672c\u6cdb\u51fd\u4e3a\u6d89\u53ca\u72b6\u6001\u548c\u63a7\u5236\u53d8\u91cf\u7684\u79ef\u5206\u5f62\u5f0f\u3002", "motivation": "\u7814\u7a76\u692d\u5706\u504f\u5fae\u5206\u65b9\u7a0b\u63a7\u5236\u7684\u6700\u4f18\u63a7\u5236\u95ee\u9898\uff0c\u8fd9\u7c7b\u95ee\u9898\u5728\u5de5\u7a0b\u3001\u7269\u7406\u548c\u6570\u5b66\u4e2d\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u5982\u7ed3\u6784\u4f18\u5316\u3001\u70ed\u4f20\u5bfc\u63a7\u5236\u7b49\uff0c\u9700\u8981\u7cfb\u7edf\u5206\u6790\u4e0d\u540c\u63a7\u5236\u53d8\u91cf\u7c7b\u578b\u4e0b\u7684\u6700\u4f18\u63a7\u5236\u7406\u8bba\u6846\u67b6\u3002", "method": "\u91c7\u7528\u692d\u5706\u504f\u5fae\u5206\u65b9\u7a0b\u4f5c\u4e3a\u63a7\u5236\u65b9\u7a0b\uff0c\u72b6\u6001\u53d8\u91cf\u4e3aPDE\u89e3\uff0c\u63a7\u5236\u53d8\u91cf\u53ef\u4ee5\u662fPDE\u7cfb\u6570\u3001\u52bf\u51fd\u6570\u6216\u53f3\u7aef\u9879\uff0c\u6210\u672c\u6cdb\u51fd\u91c7\u7528\u79ef\u5206\u5f62\u5f0f\u540c\u65f6\u8003\u8651\u72b6\u6001\u548c\u63a7\u5236\u53d8\u91cf\uff0c\u901a\u8fc7\u53d8\u5206\u6cd5\u548c\u4f18\u5316\u7406\u8bba\u6c42\u89e3\u6700\u4f18\u63a7\u5236\u95ee\u9898\u3002", "result": "\u8bba\u6587\u5efa\u7acb\u4e86\u692d\u5706PDE\u6700\u4f18\u63a7\u5236\u95ee\u9898\u7684\u6570\u5b66\u6846\u67b6\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u63a7\u5236\u53d8\u91cf\u7c7b\u578b\u4e0b\u7684\u6700\u4f18\u6027\u6761\u4ef6\uff0c\u4e3a\u8fd9\u7c7b\u95ee\u9898\u7684\u7406\u8bba\u5206\u6790\u548c\u6570\u503c\u8ba1\u7b97\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "conclusion": "\u692d\u5706\u504f\u5fae\u5206\u65b9\u7a0b\u6700\u4f18\u63a7\u5236\u95ee\u9898\u5177\u6709\u91cd\u8981\u7684\u7406\u8bba\u548c\u5e94\u7528\u4ef7\u503c\uff0c\u5efa\u7acb\u7684\u6570\u5b66\u6846\u67b6\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u4e0d\u540c\u63a7\u5236\u53d8\u91cf\u7c7b\u578b\u9700\u8981\u4e0d\u540c\u7684\u5206\u6790\u65b9\u6cd5\u548c\u6570\u503c\u6280\u672f\u3002"}}
{"id": "2601.00845", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00845", "abs": "https://arxiv.org/abs/2601.00845", "authors": ["Lili Chen", "Wensheng Gan", "Shuang Liang", "Philip S. Yu"], "title": "Enhancing Temporal Awareness in LLMs for Temporal Point Processes", "comment": "preprint", "summary": "Temporal point processes (TPPs) are crucial for analyzing events over time and are widely used in fields such as finance, healthcare, and social systems. These processes are particularly valuable for understanding how events unfold over time, accounting for their irregularity and dependencies. Despite the success of large language models (LLMs) in sequence modeling, applying them to temporal point processes remains challenging. A key issue is that current methods struggle to effectively capture the complex interaction between temporal information and semantic context, which is vital for accurate event modeling. In this context, we introduce TPP-TAL (Temporal Point Processes with Enhanced Temporal Awareness in LLMs), a novel plug-and-play framework designed to enhance temporal reasoning within LLMs. Rather than using the conventional method of simply concatenating event time and type embeddings, TPP-TAL explicitly aligns temporal dynamics with contextual semantics before feeding this information into the LLM. This alignment allows the model to better perceive temporal dependencies and long-range interactions between events and their surrounding contexts. Through comprehensive experiments on several benchmark datasets, it is shown that TPP-TAL delivers substantial improvements in temporal likelihood estimation and event prediction accuracy, highlighting the importance of enhancing temporal awareness in LLMs for continuous-time event modeling. The code is made available at https://github.com/chenlilil/TPP-TAL", "AI": {"tldr": "TPP-TAL\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u5373\u63d2\u5373\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u5bf9\u9f50\u65f6\u95f4\u52a8\u6001\u4e0e\u8bed\u4e49\u4e0a\u4e0b\u6587\u6765\u589e\u5f3aLLMs\u5728\u65f6\u5e8f\u70b9\u8fc7\u7a0b\u4e2d\u7684\u65f6\u95f4\u611f\u77e5\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e8b\u4ef6\u5efa\u6a21\u6027\u80fd\u3002", "motivation": "\u65f6\u5e8f\u70b9\u8fc7\u7a0b\u5728\u91d1\u878d\u3001\u533b\u7597\u7b49\u9886\u57df\u5f88\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u6355\u6349\u65f6\u95f4\u4fe1\u606f\u4e0e\u8bed\u4e49\u4e0a\u4e0b\u6587\u4e4b\u95f4\u7684\u590d\u6742\u4ea4\u4e92\uff0c\u9650\u5236\u4e86LLMs\u5728\u8fde\u7eed\u65f6\u95f4\u4e8b\u4ef6\u5efa\u6a21\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faTPP-TAL\u6846\u67b6\uff0c\u4e0d\u540c\u4e8e\u4f20\u7edf\u7b80\u5355\u62fc\u63a5\u65f6\u95f4\u548c\u4e8b\u4ef6\u7c7b\u578b\u5d4c\u5165\u7684\u65b9\u6cd5\uff0c\u800c\u662f\u663e\u5f0f\u5bf9\u9f50\u65f6\u95f4\u52a8\u6001\u4e0e\u8bed\u4e49\u4e0a\u4e0b\u6587\uff0c\u7136\u540e\u5c06\u5bf9\u9f50\u540e\u7684\u4fe1\u606f\u8f93\u5165LLM\uff0c\u4ee5\u589e\u5f3a\u65f6\u95f4\u4f9d\u8d56\u548c\u957f\u7a0b\u4ea4\u4e92\u7684\u611f\u77e5\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTPP-TAL\u5728\u65f6\u95f4\u4f3c\u7136\u4f30\u8ba1\u548c\u4e8b\u4ef6\u9884\u6d4b\u51c6\u786e\u6027\u65b9\u9762\u5e26\u6765\u4e86\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u589e\u5f3aLLMs\u7684\u65f6\u95f4\u611f\u77e5\u80fd\u529b\u5bf9\u4e8e\u8fde\u7eed\u65f6\u95f4\u4e8b\u4ef6\u5efa\u6a21\u81f3\u5173\u91cd\u8981\uff0cTPP-TAL\u6846\u67b6\u901a\u8fc7\u65f6\u95f4-\u8bed\u4e49\u5bf9\u9f50\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2601.01112", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01112", "abs": "https://arxiv.org/abs/2601.01112", "authors": ["Zilin Li", "Weiwei Xu", "Xuanbo Lu", "Zheda Liu"], "title": "EmoLoom-2B: Fast Base-Model Screening for Emotion Classification and VAD with Lexicon-Weak Supervision and KV-Off Evaluation", "comment": "This paper presents an initial and self-contained study of a lightweight screening pipeline for emotion-aware language modeling, intended as a reproducible baseline and system-level design reference", "summary": "We introduce EmoLoom-2B, a lightweight and reproducible pipeline that turns small language models under 2B parameters into fast screening candidates for joint emotion classification and Valence-Arousal-Dominance prediction. To ensure protocol-faithful and fair evaluation, we unify data loading, training, and inference under a single JSON input-output contract and remove avoidable variance by adopting KV-off decoding as the default setting. We incorporate two orthogonal semantic regularizers: a VAD-preserving constraint that aligns generated text with target VAD triples, and a lightweight external appraisal classifier that provides training-time guidance on goal attainment, controllability, certainty, and fairness without injecting long rationales. To improve polarity sensitivity, we introduce Valence Flip augmentation based on mirrored emotional pairs. During supervised fine-tuning, we apply A/B mixture sampling with entropy-aware temperature scheduling to balance coverage and convergence. Using Qwen-1.8B-Chat as the base model, EmoLoom-2B achieves strong performance on GoEmotions and EmpatheticDialogues, and demonstrates robust cross-corpus generalization on DailyDialog. The proposed recipe is budget-aware, auditable, and re-entrant, serving as a dependable screening pass before heavier training or multimodal fusion.", "AI": {"tldr": "EmoLoom-2B\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u53ef\u590d\u73b0\u7684\u6d41\u7a0b\uff0c\u53ef\u5c06\u5c0f\u4e8e20\u4ebf\u53c2\u6570\u7684\u5c0f\u8bed\u8a00\u6a21\u578b\u8f6c\u5316\u4e3a\u60c5\u611f\u5206\u7c7b\u548cVAD\u9884\u6d4b\u7684\u5feb\u901f\u7b5b\u9009\u5019\u9009\u6a21\u578b\uff0c\u901a\u8fc7\u7edf\u4e00\u534f\u8bae\u3001\u8bed\u4e49\u6b63\u5219\u5316\u548c\u6570\u636e\u589e\u5f3a\u5b9e\u73b0\u9ad8\u6548\u8bc4\u4f30\u3002", "motivation": "\u9700\u8981\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u53ef\u590d\u73b0\u7684\u6d41\u7a0b\uff0c\u5c06\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u8f6c\u5316\u4e3a\u60c5\u611f\u5206\u6790\u548cVAD\u9884\u6d4b\u7684\u5feb\u901f\u7b5b\u9009\u5de5\u5177\uff0c\u786e\u4fdd\u8bc4\u4f30\u534f\u8bae\u4e00\u81f4\u3001\u51cf\u5c11\u65b9\u5dee\uff0c\u4e3a\u540e\u7eed\u66f4\u91cd\u7684\u8bad\u7ec3\u6216\u591a\u6a21\u6001\u878d\u5408\u63d0\u4f9b\u53ef\u9760\u7684\u7b5b\u9009\u9636\u6bb5\u3002", "method": "1) \u7edf\u4e00JSON\u8f93\u5165\u8f93\u51fa\u534f\u8bae\u786e\u4fdd\u8bc4\u4f30\u4e00\u81f4\u6027\uff1b2) \u91c7\u7528KV-off\u89e3\u7801\u51cf\u5c11\u65b9\u5dee\uff1b3) \u5f15\u5165\u4e24\u4e2a\u6b63\u4ea4\u8bed\u4e49\u6b63\u5219\u5668\uff1aVAD\u4fdd\u6301\u7ea6\u675f\u548c\u8f7b\u91cf\u5916\u90e8\u8bc4\u4f30\u5206\u7c7b\u5668\uff1b4) \u57fa\u4e8e\u955c\u50cf\u60c5\u611f\u5bf9\u7684Valence Flip\u589e\u5f3a\uff1b5) \u76d1\u7763\u5fae\u8c03\u4e2d\u4f7f\u7528A/B\u6df7\u5408\u91c7\u6837\u548c\u71b5\u611f\u77e5\u6e29\u5ea6\u8c03\u5ea6\u3002", "result": "\u57fa\u4e8eQwen-1.8B-Chat\u7684EmoLoom-2B\u5728GoEmotions\u548cEmpatheticDialogues\u4e0a\u8868\u73b0\u5f3a\u52b2\uff0c\u5728DailyDialog\u4e0a\u5c55\u793a\u51fa\u7a33\u5065\u7684\u8de8\u8bed\u6599\u5e93\u6cdb\u5316\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u8be5\u6d41\u7a0b\u7684\u6709\u6548\u6027\u3002", "conclusion": "EmoLoom-2B\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9884\u7b97\u53cb\u597d\u3001\u53ef\u5ba1\u8ba1\u3001\u53ef\u91cd\u5165\u7684\u53ef\u9760\u7b5b\u9009\u6d41\u7a0b\uff0c\u53ef\u4f5c\u4e3a\u91cd\u578b\u8bad\u7ec3\u6216\u591a\u6a21\u6001\u878d\u5408\u524d\u7684\u6709\u6548\u7b5b\u9009\u9636\u6bb5\uff0c\u4e3a\u60c5\u611f\u5206\u6790\u4efb\u52a1\u63d0\u4f9b\u4e86\u8f7b\u91cf\u7ea7\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01836", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.01836", "abs": "https://arxiv.org/abs/2601.01836", "authors": ["Dasol Choi", "DongGeon Lee", "Brigitta Jesica Kartono", "Helena Berndt", "Taeyoun Kwon", "Joonwon Jang", "Haon Park", "Hwanjo Yu", "Minsuk Kahng"], "title": "COMPASS: A Framework for Evaluating Organization-Specific Policy Alignment in LLMs", "comment": null, "summary": "As large language models are deployed in high-stakes enterprise applications, from healthcare to finance, ensuring adherence to organization-specific policies has become essential. Yet existing safety evaluations focus exclusively on universal harms. We present COMPASS (Company/Organization Policy Alignment Assessment), the first systematic framework for evaluating whether LLMs comply with organizational allowlist and denylist policies. We apply COMPASS to eight diverse industry scenarios, generating and validating 5,920 queries that test both routine compliance and adversarial robustness through strategically designed edge cases. Evaluating seven state-of-the-art models, we uncover a fundamental asymmetry: models reliably handle legitimate requests (>95% accuracy) but catastrophically fail at enforcing prohibitions, refusing only 13-40% of adversarial denylist violations. These results demonstrate that current LLMs lack the robustness required for policy-critical deployments, establishing COMPASS as an essential evaluation framework for organizational AI safety.", "AI": {"tldr": "COMPASS\u662f\u9996\u4e2a\u8bc4\u4f30LLM\u662f\u5426\u7b26\u5408\u7ec4\u7ec7\u5141\u8bb8\u5217\u8868\u548c\u7981\u6b62\u5217\u8868\u653f\u7b56\u7684\u7cfb\u7edf\u6846\u67b6\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u5904\u7406\u5408\u6cd5\u8bf7\u6c42\u65f6\u8868\u73b0\u826f\u597d\uff08>95%\u51c6\u786e\u7387\uff09\uff0c\u4f46\u5728\u6267\u884c\u7981\u4ee4\u65f6\u4e25\u91cd\u5931\u8d25\uff08\u4ec5\u62d2\u7edd13-40%\u7684\u8fdd\u89c4\u8bf7\u6c42\uff09\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u3001\u91d1\u878d\u7b49\u9ad8\u98ce\u9669\u4f01\u4e1a\u5e94\u7528\u4e2d\u7684\u90e8\u7f72\uff0c\u786e\u4fdd\u6a21\u578b\u9075\u5b88\u7ec4\u7ec7\u7279\u5b9a\u653f\u7b56\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\u73b0\u6709\u7684\u5b89\u5168\u8bc4\u4f30\u53ea\u5173\u6ce8\u901a\u7528\u5371\u5bb3\uff0c\u7f3a\u4e4f\u9488\u5bf9\u7ec4\u7ec7\u653f\u7b56\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63d0\u51faCOMPASS\u6846\u67b6\uff0c\u5e94\u7528\u4e8e\u516b\u4e2a\u4e0d\u540c\u884c\u4e1a\u573a\u666f\uff0c\u751f\u6210\u5e76\u9a8c\u8bc1\u4e865,920\u4e2a\u67e5\u8be2\uff0c\u6d4b\u8bd5\u5e38\u89c4\u5408\u89c4\u6027\u548c\u901a\u8fc7\u7b56\u7565\u6027\u8bbe\u8ba1\u7684\u8fb9\u7f18\u6848\u4f8b\u6d4b\u8bd5\u5bf9\u6297\u9c81\u68d2\u6027\u3002\u8bc4\u4f30\u4e86\u4e03\u4e2a\u6700\u5148\u8fdb\u7684\u6a21\u578b\u3002", "result": "\u53d1\u73b0\u57fa\u672c\u4e0d\u5bf9\u79f0\u6027\uff1a\u6a21\u578b\u53ef\u9760\u5904\u7406\u5408\u6cd5\u8bf7\u6c42\uff08>95%\u51c6\u786e\u7387\uff09\uff0c\u4f46\u5728\u6267\u884c\u7981\u4ee4\u65f6\u707e\u96be\u6027\u5931\u8d25\uff0c\u4ec5\u62d2\u7edd13-40%\u7684\u5bf9\u6297\u6027\u7981\u6b62\u5217\u8868\u8fdd\u89c4\u8bf7\u6c42\u3002", "conclusion": "\u5f53\u524dLLM\u7f3a\u4e4f\u653f\u7b56\u5173\u952e\u90e8\u7f72\u6240\u9700\u7684\u9c81\u68d2\u6027\uff0cCOMPASS\u6210\u4e3a\u7ec4\u7ec7AI\u5b89\u5168\u7684\u91cd\u8981\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2601.01619", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01619", "abs": "https://arxiv.org/abs/2601.01619", "authors": ["Maxat Tezekbayev", "Rustem Takhanov", "Arman Bolatov", "Zhenisbek Assylbekov"], "title": "Deep Linear Discriminant Analysis Revisited", "comment": null, "summary": "We show that for unconstrained Deep Linear Discriminant Analysis (LDA) classifiers, maximum-likelihood training admits pathological solutions in which class means drift together, covariances collapse, and the learned representation becomes almost non-discriminative. Conversely, cross-entropy training yields excellent accuracy but decouples the head from the underlying generative model, leading to highly inconsistent parameter estimates. To reconcile generative structure with discriminative performance, we introduce the \\emph{Discriminative Negative Log-Likelihood} (DNLL) loss, which augments the LDA log-likelihood with a simple penalty on the mixture density. DNLL can be interpreted as standard LDA NLL plus a term that explicitly discourages regions where several classes are simultaneously likely. Deep LDA trained with DNLL produces clean, well-separated latent spaces, matches the test accuracy of softmax classifiers on synthetic data and standard image benchmarks, and yields substantially better calibrated predictive probabilities, restoring a coherent probabilistic interpretation to deep discriminant models.", "AI": {"tldr": "\u8bba\u6587\u53d1\u73b0\u4f20\u7edf\u6df1\u5ea6\u7ebf\u6027\u5224\u522b\u5206\u6790\uff08LDA\uff09\u8bad\u7ec3\u5b58\u5728\u75c5\u6001\u89e3\u95ee\u9898\uff0c\u800c\u4ea4\u53c9\u71b5\u8bad\u7ec3\u867d\u51c6\u786e\u4f46\u7834\u574f\u4e86\u751f\u6210\u6a21\u578b\u7ed3\u6784\u3002\u4f5c\u8005\u63d0\u51fa\u5224\u522b\u5f0f\u8d1f\u5bf9\u6570\u4f3c\u7136\uff08DNLL\uff09\u635f\u5931\u51fd\u6570\uff0c\u5728\u4fdd\u6301\u5224\u522b\u6027\u80fd\u7684\u540c\u65f6\u6062\u590d\u751f\u6210\u6a21\u578b\u7684\u4e00\u81f4\u6027\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6LDA\u7684\u6700\u5927\u4f3c\u7136\u8bad\u7ec3\u4f1a\u5bfc\u81f4\u7c7b\u522b\u5747\u503c\u6f02\u79fb\u3001\u534f\u65b9\u5dee\u574d\u7f29\u7b49\u75c5\u6001\u89e3\uff0c\u800c\u4ea4\u53c9\u71b5\u8bad\u7ec3\u867d\u7136\u51c6\u786e\u4f46\u7834\u574f\u4e86\u751f\u6210\u6a21\u578b\u7684\u7ed3\u6784\u4e00\u81f4\u6027\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u5224\u522b\u6027\u80fd\u53c8\u80fd\u7ef4\u6301\u751f\u6210\u6a21\u578b\u89e3\u91ca\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u5224\u522b\u5f0f\u8d1f\u5bf9\u6570\u4f3c\u7136\uff08DNLL\uff09\u635f\u5931\u51fd\u6570\uff0c\u5728\u6807\u51c6LDA\u8d1f\u5bf9\u6570\u4f3c\u7136\u57fa\u7840\u4e0a\u589e\u52a0\u4e00\u4e2a\u60e9\u7f5a\u9879\uff0c\u8be5\u60e9\u7f5a\u9879\u660e\u786e\u963b\u6b62\u591a\u4e2a\u7c7b\u522b\u540c\u65f6\u5177\u6709\u9ad8\u6982\u7387\u7684\u533a\u57df\u3002DNLL\u53ef\u4ee5\u89e3\u91ca\u4e3a\u6807\u51c6LDA\u8d1f\u5bf9\u6570\u4f3c\u7136\u52a0\u4e0a\u4e00\u4e2a\u5bf9\u6df7\u5408\u5bc6\u5ea6\u7684\u7b80\u5355\u60e9\u7f5a\u3002", "result": "\u4f7f\u7528DNLL\u8bad\u7ec3\u7684\u6df1\u5ea6LDA\u80fd\u591f\u4ea7\u751f\u5e72\u51c0\u3001\u826f\u597d\u5206\u79bb\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u5728\u5408\u6210\u6570\u636e\u548c\u6807\u51c6\u56fe\u50cf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e0esoftmax\u5206\u7c7b\u5668\u76f8\u5f53\u7684\u6d4b\u8bd5\u51c6\u786e\u7387\uff0c\u5e76\u4e14\u4ea7\u751f\u663e\u8457\u66f4\u597d\u7684\u6821\u51c6\u9884\u6d4b\u6982\u7387\uff0c\u6062\u590d\u4e86\u6df1\u5ea6\u5224\u522b\u6a21\u578b\u7684\u6982\u7387\u89e3\u91ca\u6027\u3002", "conclusion": "DNLL\u635f\u5931\u51fd\u6570\u6210\u529f\u8c03\u548c\u4e86\u751f\u6210\u6a21\u578b\u7ed3\u6784\u4e0e\u5224\u522b\u6027\u80fd\u4e4b\u95f4\u7684\u77db\u76fe\uff0c\u4e3a\u6df1\u5ea6\u5224\u522b\u6a21\u578b\u63d0\u4f9b\u4e86\u65e2\u51c6\u786e\u53c8\u5177\u6709\u4e00\u81f4\u6982\u7387\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.00857", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00857", "abs": "https://arxiv.org/abs/2601.00857", "authors": ["Yuchi Ma", "Yawen Shen", "Anu Swatantran", "David B. Lobell"], "title": "Harvesting AlphaEarth: Benchmarking the Geospatial Foundation Model for Agricultural Downstream Tasks", "comment": null, "summary": "Geospatial foundation models (GFMs) have emerged as a promising approach to overcoming the limitations in existing featurization methods. More recently, Google DeepMind has introduced AlphaEarth Foundation (AEF), a GFM pre-trained using multi-source EOs across continuous time. An annual and global embedding dataset is produced using AEF that is ready for analysis and modeling. The internal experiments show that AEF embeddings have outperformed operational models in 15 EO tasks without re-training. However, those experiments are mostly about land cover and land use classification. Applying AEF and other GFMs to agricultural monitoring require an in-depth evaluation in critical agricultural downstream tasks. There is also a lack of comprehensive comparison between the AEF-based models and traditional remote sensing (RS)-based models under different scenarios, which could offer valuable guidance for researchers and practitioners. This study addresses some of these gaps by evaluating AEF embeddings in three agricultural downstream tasks in the U.S., including crop yield prediction, tillage mapping, and cover crop mapping. Datasets are compiled from both public and private sources to comprehensively evaluate AEF embeddings across tasks at different scales and locations, and RS-based models are trained as comparison models. AEF-based models generally exhibit strong performance on all tasks and are competitive with purpose-built RS-based models in yield prediction and county-level tillage mapping when trained on local data. However, we also find several limitations in current AEF embeddings, such as limited spatial transferability compared to RS-based models, low interpretability, and limited time sensitivity. These limitations recommend caution when applying AEF embeddings in agriculture, where time sensitivity, generalizability, and interpretability is important.", "AI": {"tldr": "\u8bc4\u4f30AlphaEarth Foundation\u5730\u7406\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\u5728\u519c\u4e1a\u76d1\u6d4b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u5728\u4ea7\u91cf\u9884\u6d4b\u548c\u8015\u4f5c\u5236\u56fe\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5b58\u5728\u7a7a\u95f4\u53ef\u8fc1\u79fb\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u65f6\u95f4\u654f\u611f\u6027\u7b49\u9650\u5236", "motivation": "\u867d\u7136AlphaEarth Foundation\u7b49\u5730\u7406\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\u5728\u571f\u5730\u8986\u76d6\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7f3a\u4e4f\u5728\u519c\u4e1a\u76d1\u6d4b\u5173\u952e\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u6df1\u5165\u8bc4\u4f30\uff0c\u4ee5\u53ca\u4e0e\u57fa\u4e8e\u4f20\u7edf\u9065\u611f\u65b9\u6cd5\u7684\u6a21\u578b\u7684\u5168\u9762\u6bd4\u8f83", "method": "\u5728\u7f8e\u56fd\u7684\u4e09\u4e2a\u519c\u4e1a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8bc4\u4f30AEF\u5d4c\u5165\uff1a\u4f5c\u7269\u4ea7\u91cf\u9884\u6d4b\u3001\u8015\u4f5c\u5236\u56fe\u548c\u8986\u76d6\u4f5c\u7269\u5236\u56fe\u3002\u4f7f\u7528\u516c\u5171\u548c\u79c1\u4eba\u6765\u6e90\u7684\u6570\u636e\u96c6\uff0c\u5728\u4e0d\u540c\u5c3a\u5ea6\u548c\u4f4d\u7f6e\u8fdb\u884c\u7efc\u5408\u8bc4\u4f30\uff0c\u5e76\u8bad\u7ec3\u57fa\u4e8e\u9065\u611f\u7684\u6a21\u578b\u4f5c\u4e3a\u5bf9\u6bd4", "result": "AEF\u6a21\u578b\u5728\u6240\u6709\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u5728\u4ea7\u91cf\u9884\u6d4b\u548c\u53bf\u7ea7\u8015\u4f5c\u5236\u56fe\u65b9\u9762\u4e0e\u4e13\u95e8\u6784\u5efa\u7684\u9065\u611f\u6a21\u578b\u7ade\u4e89\u529b\u76f8\u5f53\u3002\u4f46\u53d1\u73b0AEF\u5d4c\u5165\u5b58\u5728\u7a7a\u95f4\u53ef\u8fc1\u79fb\u6027\u6709\u9650\u3001\u53ef\u89e3\u91ca\u6027\u4f4e\u548c\u65f6\u95f4\u654f\u611f\u6027\u4e0d\u8db3\u7b49\u9650\u5236", "conclusion": "AEF\u5d4c\u5165\u5728\u519c\u4e1a\u5e94\u7528\u4e2d\u9700\u8c28\u614e\u4f7f\u7528\uff0c\u7279\u522b\u662f\u5728\u65f6\u95f4\u654f\u611f\u6027\u3001\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\u8981\u6c42\u8f83\u9ad8\u7684\u573a\u666f\u4e2d\u3002\u867d\u7136\u5728\u67d0\u4e9b\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5f53\u524d\u7248\u672c\u5b58\u5728\u91cd\u8981\u9650\u5236"}}
{"id": "2601.01410", "categories": ["eess.SY", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01410", "abs": "https://arxiv.org/abs/2601.01410", "authors": ["Jisoo Lee", "Sunki Hong"], "title": "Reliable Grid Forecasting: State Space Models for Safety-Critical Energy Systems", "comment": "24 pages, 8 figures, 8 tables", "summary": "Accurate grid load forecasting is safety-critical: under-predictions risk supply shortfalls, while symmetric error metrics mask this operational asymmetry. We introduce a grid-specific evaluation framework--Asymmetric MAPE, Under-Prediction Rate, and Reserve Margin--that directly measures operational risk rather than statistical accuracy alone.\n  Using this framework, we conduct a systematic evaluation of Mamba-based State Space Models for California grid forecasting on a weather-aligned CAISO TAC-area dataset spanning Nov 2023--Nov 2025 (84,498 hourly records across 5 transmission areas). Our analysis reveals that standard accuracy metrics are poor proxies for operational safety: models with identical MAPE can require vastly different reserve margins.\n  We demonstrate that forecast errors are weakly but significantly associated with temperature (r = 0.16, p < 10^{-16}), motivating weather-aware modeling rather than loss function modification alone. The S-Mamba model achieves the lowest Reserve_{99.5}% margin (14.12%) compared to 16.66% for iTransformer, demonstrating superior forecast reliability under a 99.5th-percentile tail-risk reserve proxy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9\u7535\u7f51\u8d1f\u8377\u9884\u6d4b\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5f3a\u8c03\u4f20\u7edf\u5bf9\u79f0\u8bef\u5dee\u6307\u6807\u65e0\u6cd5\u53cd\u6620\u8fd0\u8425\u98ce\u9669\uff0c\u5e76\u8bc1\u660eS-Mamba\u6a21\u578b\u572899.5%\u5c3e\u98ce\u9669\u50a8\u5907\u4ee3\u7406\u4e0b\u5177\u6709\u6700\u4f18\u53ef\u9760\u6027\u3002", "motivation": "\u7535\u7f51\u8d1f\u8377\u9884\u6d4b\u7684\u51c6\u786e\u6027\u5bf9\u8fd0\u8425\u5b89\u5168\u81f3\u5173\u91cd\u8981\uff1a\u9884\u6d4b\u4e0d\u8db3\u53ef\u80fd\u5bfc\u81f4\u4f9b\u7535\u77ed\u7f3a\uff0c\u800c\u4f20\u7edf\u7684\u5bf9\u79f0\u8bef\u5dee\u6307\u6807\u65e0\u6cd5\u53cd\u6620\u8fd9\u79cd\u8fd0\u8425\u4e0d\u5bf9\u79f0\u6027\u3002\u9700\u8981\u5f00\u53d1\u80fd\u76f4\u63a5\u8861\u91cf\u8fd0\u8425\u98ce\u9669\u800c\u975e\u4ec5\u7edf\u8ba1\u51c6\u786e\u6027\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86\u7535\u7f51\u7279\u5b9a\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u62ec\u975e\u5bf9\u79f0MAPE\u3001\u9884\u6d4b\u4e0d\u8db3\u7387\u548c\u50a8\u5907\u88d5\u5ea6\u3002\u4f7f\u7528\u8be5\u6846\u67b6\u7cfb\u7edf\u8bc4\u4f30\u4e86\u57fa\u4e8eMamba\u7684\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u5728\u52a0\u5dde\u7535\u7f51\u9884\u6d4b\u4e2d\u7684\u8868\u73b0\uff0c\u6570\u636e\u96c6\u6db5\u76d62023\u5e7411\u6708\u81f32025\u5e7411\u6708\u768484,498\u5c0f\u65f6\u8bb0\u5f55\uff0c\u8986\u76d65\u4e2a\u4f20\u8f93\u533a\u57df\u3002", "result": "\u6807\u51c6\u51c6\u786e\u6027\u6307\u6807\u662f\u8fd0\u8425\u5b89\u5168\u7684\u5dee\u4ee3\u7406\uff1a\u5177\u6709\u76f8\u540cMAPE\u7684\u6a21\u578b\u53ef\u80fd\u9700\u8981\u5b8c\u5168\u4e0d\u540c\u7684\u50a8\u5907\u88d5\u5ea6\u3002\u9884\u6d4b\u8bef\u5dee\u4e0e\u6e29\u5ea6\u5448\u5f31\u4f46\u663e\u8457\u76f8\u5173\uff08r=0.16\uff0cp<10^{-16}\uff09\uff0c\u8868\u660e\u9700\u8981\u5929\u6c14\u611f\u77e5\u5efa\u6a21\u3002S-Mamba\u6a21\u578b\u5b9e\u73b0\u4e86\u6700\u4f4e\u7684Reserve_{99.5}%\u88d5\u5ea6\uff0814.12%\uff09\uff0c\u4f18\u4e8eiTransformer\u768416.66%\u3002", "conclusion": "\u7535\u7f51\u8d1f\u8377\u9884\u6d4b\u8bc4\u4f30\u9700\u8981\u5173\u6ce8\u8fd0\u8425\u98ce\u9669\u800c\u975e\u4ec5\u7edf\u8ba1\u51c6\u786e\u6027\uff0c\u63d0\u51fa\u7684\u8bc4\u4f30\u6846\u67b6\u80fd\u66f4\u597d\u8861\u91cf\u6a21\u578b\u53ef\u9760\u6027\u3002S-Mamba\u5728\u5c3e\u98ce\u9669\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u7535\u7f51\u8fd0\u8425\u63d0\u4f9b\u4e86\u66f4\u5b89\u5168\u7684\u9884\u6d4b\u4fdd\u969c\u3002"}}
{"id": "2601.01621", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2601.01621", "abs": "https://arxiv.org/abs/2601.01621", "authors": ["Vyacheslav Kungurtsev"], "title": "A Three-Tier Time-Scale Architecture for Controlling Complex Nonlinear Systems", "comment": null, "summary": "This letter proposes a three-tier computational architecture for the real-time control of nonlinear complex systems, such as time-dependent PDEs. There is an important class of such problems for which existing single- and two-time-scale approaches are fundamentally insufficient due to lack of a priori system knowledge, computational complexity, model fidelity requirements, and uncertainty. The proposed architecture consists of an offline, meso-scale, and real-time layer of computation, with distinct roles for each layer and specific information flow between them. The result is a practical systems-level paradigm that enables real-time operation of complex nonlinear control problems.", "AI": {"tldr": "\u63d0\u51fa\u4e09\u5c42\u8ba1\u7b97\u67b6\u6784\u7528\u4e8e\u975e\u7ebf\u6027\u590d\u6742\u7cfb\u7edf\u7684\u5b9e\u65f6\u63a7\u5236\uff0c\u89e3\u51b3\u73b0\u6709\u5355\u5c42\u548c\u53cc\u5c42\u65b9\u6cd5\u5728\u7cfb\u7edf\u77e5\u8bc6\u3001\u8ba1\u7b97\u590d\u6742\u5ea6\u3001\u6a21\u578b\u4fdd\u771f\u5ea6\u548c\u4e0d\u786e\u5b9a\u6027\u65b9\u9762\u7684\u4e0d\u8db3", "motivation": "\u73b0\u6709\u5355\u5c42\u548c\u53cc\u5c42\u65f6\u95f4\u5c3a\u5ea6\u65b9\u6cd5\u5bf9\u4e8e\u4e00\u7c7b\u91cd\u8981\u7684\u975e\u7ebf\u6027\u590d\u6742\u7cfb\u7edf\uff08\u5982\u65f6\u95f4\u4f9d\u8d56PDE\uff09\u63a7\u5236\u95ee\u9898\u5b58\u5728\u6839\u672c\u6027\u4e0d\u8db3\uff0c\u5305\u62ec\u7f3a\u4e4f\u5148\u9a8c\u7cfb\u7edf\u77e5\u8bc6\u3001\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u6a21\u578b\u4fdd\u771f\u5ea6\u8981\u6c42\u548c\u4e0d\u786e\u5b9a\u6027\u7b49\u95ee\u9898", "method": "\u63d0\u51fa\u4e09\u5c42\u8ba1\u7b97\u67b6\u6784\uff1a\u79bb\u7ebf\u5c42\u3001\u4e2d\u5c3a\u5ea6\u5c42\u548c\u5b9e\u65f6\u5c42\uff0c\u6bcf\u5c42\u6709\u660e\u786e\u89d2\u8272\u548c\u7279\u5b9a\u7684\u4fe1\u606f\u6d41\uff0c\u5f62\u6210\u7cfb\u7edf\u7ea7\u8303\u5f0f", "result": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u7cfb\u7edf\u7ea7\u8303\u5f0f\uff0c\u80fd\u591f\u5b9e\u73b0\u590d\u6742\u975e\u7ebf\u6027\u63a7\u5236\u95ee\u9898\u7684\u5b9e\u65f6\u64cd\u4f5c", "conclusion": "\u4e09\u5c42\u8ba1\u7b97\u67b6\u6784\u4e3a\u89e3\u51b3\u975e\u7ebf\u6027\u590d\u6742\u7cfb\u7edf\u7684\u5b9e\u65f6\u63a7\u5236\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027"}}
{"id": "2601.00848", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.00848", "abs": "https://arxiv.org/abs/2601.00848", "authors": ["Ron F. Del Rosario"], "title": "Temporal Attack Pattern Detection in Multi-Agent AI Workflows: An Open Framework for Training Trace-Based Security Models", "comment": "26 pages, 3 figures, 7 tables. Datasets and code: https://huggingface.co/guerilla7/agentic-safety-gguf", "summary": "We present an openly documented methodology for fine-tuning language models to detect temporal attack patterns in multi-agent AI workflows using OpenTelemetry trace analysis. We curate a dataset of 80,851 examples from 18 public cybersecurity sources and 35,026 synthetic OpenTelemetry traces. We apply iterative QLoRA fine-tuning on resource-constrained ARM64 hardware (NVIDIA DGX Spark) through three training iterations with strategic augmentation. Our custom benchmark accuracy improves from 42.86% to 74.29%, a statistically significant 31.4-point gain. Targeted examples addressing specific knowledge gaps outperform indiscriminate scaling. Key contributions include: (1) synthetic trace generation methodology for multi-agent coordination attacks and regulatory violations, (2) empirical evidence that training data composition fundamentally determines behavior, and (3) complete open release of datasets, training scripts, and evaluation benchmarks on HuggingFace. While practical deployment requires human oversight due to false positive rates, this work establishes the first reproducible framework enabling practitioners to build custom agentic security models adapted to their threat landscapes.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eOpenTelemetry\u8ffd\u8e2a\u5206\u6790\u7684\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u591a\u667a\u80fd\u4f53AI\u5de5\u4f5c\u6d41\u4e2d\u7684\u65f6\u5e8f\u653b\u51fb\u6a21\u5f0f\uff0c\u901a\u8fc7\u7cbe\u5fc3\u7b56\u5212\u7684\u6570\u636e\u96c6\u548c\u8fed\u4ee3\u8bad\u7ec3\uff0c\u5c06\u68c0\u6d4b\u51c6\u786e\u7387\u4ece42.86%\u63d0\u5347\u81f374.29%\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u53ef\u91cd\u73b0\u7684\u6846\u67b6\u6765\u6784\u5efa\u9488\u5bf9\u591a\u667a\u80fd\u4f53AI\u5de5\u4f5c\u6d41\u65f6\u5e8f\u653b\u51fb\u68c0\u6d4b\u7684\u81ea\u5b9a\u4e49\u5b89\u5168\u6a21\u578b\uff0c\u7279\u522b\u662f\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u3002\u9700\u8981\u4e00\u79cd\u7cfb\u7edf\u5316\u65b9\u6cd5\u6765\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u8bc6\u522b\u590d\u6742\u7684\u653b\u51fb\u6a21\u5f0f\u3002", "method": "1. \u6536\u96c680,851\u4e2a\u6765\u81ea18\u4e2a\u516c\u5171\u7f51\u7edc\u5b89\u5168\u6e90\u7684\u793a\u4f8b\u548c35,026\u4e2a\u5408\u6210\u7684OpenTelemetry\u8ffd\u8e2a\u6570\u636e\uff1b2. \u5728NVIDIA DGX Spark\u7b49ARM64\u786c\u4ef6\u4e0a\u4f7f\u7528QLoRA\u8fdb\u884c\u8fed\u4ee3\u5fae\u8c03\uff1b3. \u901a\u8fc7\u4e09\u6b21\u8bad\u7ec3\u8fed\u4ee3\u548c\u7b56\u7565\u6027\u6570\u636e\u589e\u5f3a\u4f18\u5316\u6a21\u578b\uff1b4. \u5f00\u53d1\u81ea\u5b9a\u4e49\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u81ea\u5b9a\u4e49\u57fa\u51c6\u6d4b\u8bd5\u51c6\u786e\u7387\u4ece42.86%\u63d0\u5347\u81f374.29%\uff0c\u5b9e\u73b0\u4e8631.4\u4e2a\u767e\u5206\u70b9\u7684\u663e\u8457\u63d0\u5347\u3002\u9488\u5bf9\u7279\u5b9a\u77e5\u8bc6\u5dee\u8ddd\u7684\u5b9a\u5411\u8bad\u7ec3\u793a\u4f8b\u4f18\u4e8e\u65e0\u5dee\u522b\u6269\u5c55\u3002\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684\u6570\u636e\u96c6\u3001\u8bad\u7ec3\u811a\u672c\u548c\u8bc4\u4f30\u57fa\u51c6\u3002", "conclusion": "\u8be5\u7814\u7a76\u5efa\u7acb\u4e86\u9996\u4e2a\u53ef\u91cd\u73b0\u7684\u6846\u67b6\uff0c\u4f7f\u4ece\u4e1a\u8005\u80fd\u591f\u6784\u5efa\u9002\u5e94\u5176\u5a01\u80c1\u73af\u5883\u7684\u81ea\u5b9a\u4e49\u667a\u80fd\u4f53\u5b89\u5168\u6a21\u578b\u3002\u867d\u7136\u5b9e\u9645\u90e8\u7f72\u4ecd\u9700\u4eba\u5de5\u76d1\u7763\uff08\u56e0\u8bef\u62a5\u7387\uff09\uff0c\u4f46\u8bc1\u660e\u4e86\u8bad\u7ec3\u6570\u636e\u7ec4\u6210\u5bf9\u6a21\u578b\u884c\u4e3a\u7684\u51b3\u5b9a\u6027\u5f71\u54cd\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684\u5f00\u6e90\u5de5\u5177\u94fe\u3002"}}
{"id": "2601.01121", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01121", "abs": "https://arxiv.org/abs/2601.01121", "authors": ["Yacouba Diarra", "Michael Leventhal"], "title": "Listen, Attend, Understand: a Regularization Technique for Stable E2E Speech Translation Training on High Variance labels", "comment": "9 mages, 3 figures", "summary": "End-to-End Speech Translation often shows slower convergence and worse performance when target transcriptions exhibit high variance and semantic ambiguity. We propose Listen, Attend, Understand (LAU), a semantic regularization technique that constrains the acoustic encoder's latent space during training. By leveraging frozen text embeddings to provide a directional auxiliary loss, LAU injects linguistic groundedness into the acoustic representation without increasing inference cost. We evaluate our method on a Bambara-to-French dataset with 30 hours of Bambara speech translated by non-professionals. Experimental results demonstrate that LAU models achieve comparable performance by standard metrics compared to an E2E-ST system pretrained with 100\\% more data and while performing better in preserving semantic meaning. Furthermore, we introduce Total Parameter Drift as a metric to quantify the structural impact of regularization to demonstrate that semantic constraints actively reorganize the encoder's weights to prioritize meaning over literal phonetics. Our findings suggest that LAU is a robust alternative to post-hoc rescoring and a valuable addition to E2E-ST training, especially when training data is scarce and/or noisy.", "AI": {"tldr": "LAU\u662f\u4e00\u79cd\u8bed\u4e49\u6b63\u5219\u5316\u6280\u672f\uff0c\u901a\u8fc7\u51bb\u7ed3\u6587\u672c\u5d4c\u5165\u63d0\u4f9b\u65b9\u5411\u6027\u8f85\u52a9\u635f\u5931\uff0c\u5728\u8bad\u7ec3\u671f\u95f4\u7ea6\u675f\u58f0\u5b66\u7f16\u7801\u5668\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u4e3a\u7aef\u5230\u7aef\u8bed\u97f3\u7ffb\u8bd1\u6ce8\u5165\u8bed\u8a00\u57fa\u7840\u6027\uff0c\u4e0d\u589e\u52a0\u63a8\u7406\u6210\u672c\u3002", "motivation": "\u7aef\u5230\u7aef\u8bed\u97f3\u7ffb\u8bd1\u5728\u76ee\u6807\u8f6c\u5f55\u5177\u6709\u9ad8\u65b9\u5dee\u548c\u8bed\u4e49\u6a21\u7cca\u6027\u65f6\uff0c\u901a\u5e38\u8868\u73b0\u51fa\u6536\u655b\u901f\u5ea6\u6162\u548c\u6027\u80fd\u8f83\u5dee\u7684\u95ee\u9898\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u5728\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u548c/\u6216\u5608\u6742\u7684\u60c5\u51b5\u4e0b\u6539\u5584\u6027\u80fd\u3002", "method": "\u63d0\u51faListen, Attend, Understand (LAU)\u8bed\u4e49\u6b63\u5219\u5316\u6280\u672f\uff0c\u5229\u7528\u51bb\u7ed3\u7684\u6587\u672c\u5d4c\u5165\u63d0\u4f9b\u65b9\u5411\u6027\u8f85\u52a9\u635f\u5931\uff0c\u5728\u8bad\u7ec3\u671f\u95f4\u7ea6\u675f\u58f0\u5b66\u7f16\u7801\u5668\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u5c06\u8bed\u8a00\u57fa\u7840\u6027\u6ce8\u5165\u58f0\u5b66\u8868\u793a\u4e2d\u3002", "result": "\u572830\u5c0f\u65f6\u73ed\u5df4\u62c9\u8bed\u5230\u6cd5\u8bed\u6570\u636e\u96c6\u4e0a\uff0cLAU\u6a21\u578b\u4e0e\u4f7f\u7528100%\u66f4\u591a\u6570\u636e\u9884\u8bad\u7ec3\u7684E2E-ST\u7cfb\u7edf\u76f8\u6bd4\uff0c\u5728\u6807\u51c6\u6307\u6807\u4e0a\u8fbe\u5230\u53ef\u6bd4\u6027\u80fd\uff0c\u540c\u65f6\u5728\u4fdd\u7559\u8bed\u4e49\u542b\u4e49\u65b9\u9762\u8868\u73b0\u66f4\u597d\u3002\u5f15\u5165\u603b\u53c2\u6570\u6f02\u79fb\u6307\u6807\u91cf\u5316\u6b63\u5219\u5316\u7684\u7ed3\u6784\u5f71\u54cd\u3002", "conclusion": "LAU\u662f\u540e\u5904\u7406\u91cd\u8bc4\u5206\u7684\u7a33\u5065\u66ff\u4ee3\u65b9\u6848\uff0c\u662fE2E-ST\u8bad\u7ec3\u7684\u6709\u4ef7\u503c\u8865\u5145\uff0c\u7279\u522b\u662f\u5728\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u548c/\u6216\u5608\u6742\u7684\u60c5\u51b5\u4e0b\u3002\u8bed\u4e49\u7ea6\u675f\u4e3b\u52a8\u91cd\u7ec4\u7f16\u7801\u5668\u6743\u91cd\uff0c\u4f18\u5148\u8003\u8651\u610f\u4e49\u800c\u975e\u5b57\u9762\u8bed\u97f3\u3002"}}
{"id": "2601.01878", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.01878", "abs": "https://arxiv.org/abs/2601.01878", "authors": ["Farzan Karimi-Malekabadi", "Suhaib Abdurahman", "Zhivar Sourati", "Jackson Trager", "Morteza Dehghani"], "title": "Theory Trace Card: Theory-Driven Socio-Cognitive Evaluation of LLMs", "comment": null, "summary": "Socio-cognitive benchmarks for large language models (LLMs) often fail to predict real-world behavior, even when models achieve high benchmark scores. Prior work has attributed this evaluation-deployment gap to problems of measurement and validity. While these critiques are insightful, we argue that they overlook a more fundamental issue: many socio-cognitive evaluations proceed without an explicit theoretical specification of the target capability, leaving the assumptions linking task performance to competence implicit. Without this theoretical grounding, benchmarks that exercise only narrow subsets of a capability are routinely misinterpreted as evidence of broad competence: a gap that creates a systemic validity illusion by masking the failure to evaluate the capability's other essential dimensions. To address this gap, we make two contributions. First, we diagnose and formalize this theory gap as a foundational failure that undermines measurement and enables systematic overgeneralization of benchmark results. Second, we introduce the Theory Trace Card (TTC), a lightweight documentation artifact designed to accompany socio-cognitive evaluations, which explicitly outlines the theoretical basis of an evaluation, the components of the target capability it exercises, its operationalization, and its limitations. We argue that TTCs enhance the interpretability and reuse of socio-cognitive evaluations by making explicit the full validity chain, which links theory, task operationalization, scoring, and limitations, without modifying benchmarks or requiring agreement on a single theory.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u7684\u793e\u4f1a\u8ba4\u77e5\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u7406\u8bba\u7f3a\u53e3\uff0c\u5bfc\u81f4\u8bc4\u4f30\u7ed3\u679c\u88ab\u8fc7\u5ea6\u6cdb\u5316\uff0c\u63d0\u51fa\u4e86\u7406\u8bba\u8ffd\u8e2a\u5361\u4f5c\u4e3a\u89e3\u51b3\u65b9\u6848\u6765\u660e\u786e\u8bc4\u4f30\u7684\u7406\u8bba\u57fa\u7840\u3002", "motivation": "\u5f53\u524dLLM\u7684\u793e\u4f1a\u8ba4\u77e5\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u8bc4\u4f30-\u90e8\u7f72\u5dee\u8ddd\uff0c\u5373\u4f7f\u6a21\u578b\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5f97\u5206\u5f88\u9ad8\uff0c\u4e5f\u65e0\u6cd5\u9884\u6d4b\u771f\u5b9e\u4e16\u754c\u884c\u4e3a\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5f52\u56e0\u4e8e\u6d4b\u91cf\u548c\u6548\u5ea6\u95ee\u9898\uff0c\u4f46\u4f5c\u8005\u8ba4\u4e3a\u66f4\u6839\u672c\u7684\u95ee\u9898\u662f\u7f3a\u4e4f\u660e\u786e\u7684\u7406\u8bba\u57fa\u7840\uff0c\u5bfc\u81f4\u5bf9\u6a21\u578b\u80fd\u529b\u7684\u8fc7\u5ea6\u6cdb\u5316\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e24\u4e2a\u8d21\u732e\uff1a1) \u8bca\u65ad\u5e76\u5f62\u5f0f\u5316\u7406\u8bba\u7f3a\u53e3\u95ee\u9898\uff0c\u6307\u51fa\u8fd9\u662f\u5bfc\u81f4\u6d4b\u91cf\u5931\u6548\u548c\u7ed3\u679c\u8fc7\u5ea6\u6cdb\u5316\u7684\u6839\u672c\u539f\u56e0\uff1b2) \u5f15\u5165\u7406\u8bba\u8ffd\u8e2a\u5361(TTC)\uff0c\u8fd9\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6587\u6863\u5de5\u5177\uff0c\u7528\u4e8e\u660e\u786e\u8bb0\u5f55\u8bc4\u4f30\u7684\u7406\u8bba\u57fa\u7840\u3001\u76ee\u6807\u80fd\u529b\u7ec4\u4ef6\u3001\u64cd\u4f5c\u5316\u8fc7\u7a0b\u53ca\u5176\u5c40\u9650\u6027\u3002", "result": "\u7406\u8bba\u8ffd\u8e2a\u5361\u901a\u8fc7\u660e\u786e\u7406\u8bba-\u4efb\u52a1\u64cd\u4f5c\u5316-\u8bc4\u5206-\u5c40\u9650\u6027\u7684\u5b8c\u6574\u6548\u5ea6\u94fe\uff0c\u589e\u5f3a\u4e86\u793e\u4f1a\u8ba4\u77e5\u8bc4\u4f30\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u91cd\u7528\u6027\uff0c\u65e0\u9700\u4fee\u6539\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u6216\u8981\u6c42\u5355\u4e00\u7406\u8bba\u5171\u8bc6\u3002", "conclusion": "\u89e3\u51b3\u793e\u4f1a\u8ba4\u77e5\u8bc4\u4f30\u4e2d\u7684\u7406\u8bba\u7f3a\u53e3\u5bf9\u4e8e\u907f\u514d\u7cfb\u7edf\u6027\u6548\u5ea6\u5e7b\u89c9\u81f3\u5173\u91cd\u8981\u3002\u7406\u8bba\u8ffd\u8e2a\u5361\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u65b9\u6cd5\uff0c\u901a\u8fc7\u660e\u786e\u8bc4\u4f30\u7684\u7406\u8bba\u57fa\u7840\u6765\u6539\u5584\u8bc4\u4f30\u8d28\u91cf\uff0c\u4fc3\u8fdb\u66f4\u8d1f\u8d23\u4efb\u7684AI\u8bc4\u4f30\u5b9e\u8df5\u3002"}}
{"id": "2601.01679", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01679", "abs": "https://arxiv.org/abs/2601.01679", "authors": ["Maxat Tezekbayev", "Arman Bolatov", "Zhenisbek Assylbekov"], "title": "Simplex Deep Linear Discriminant Analysis", "comment": null, "summary": "We revisit Deep Linear Discriminant Analysis (Deep LDA) from a likelihood-based perspective. While classical LDA is a simple Gaussian model with linear decision boundaries, attaching an LDA head to a neural encoder raises the question of how to train the resulting deep classifier by maximum likelihood estimation (MLE). We first show that end-to-end MLE training of an unconstrained Deep LDA model ignores discrimination: when both the LDA parameters and the encoder parameters are learned jointly, the likelihood admits a degenerate solution in which some of the class clusters may heavily overlap or even collapse, and classification performance deteriorates. Batchwise moment re-estimation of the LDA parameters does not remove this failure mode. We then propose a constrained Deep LDA formulation that fixes the class means to the vertices of a regular simplex in the latent space and restricts the shared covariance to be spherical, leaving only the priors and a single variance parameter to be learned along with the encoder. Under these geometric constraints, MLE becomes stable and yields well-separated class clusters in the latent space. On images (Fashion-MNIST, CIFAR-10, CIFAR-100), the resulting Deep LDA models achieve accuracy competitive with softmax baselines while offering a simple, interpretable latent geometry that is clearly visible in two-dimensional projections.", "AI": {"tldr": "\u91cd\u65b0\u5ba1\u89c6\u6df1\u5ea6\u7ebf\u6027\u5224\u522b\u5206\u6790\uff0c\u4ece\u4f3c\u7136\u89d2\u5ea6\u51fa\u53d1\uff0c\u53d1\u73b0\u65e0\u7ea6\u675f\u7684\u6df1\u5ea6LDA\u6700\u5927\u4f3c\u7136\u8bad\u7ec3\u4f1a\u5bfc\u81f4\u7c7b\u522b\u91cd\u53e0\u6216\u574d\u7f29\uff0c\u63d0\u51fa\u51e0\u4f55\u7ea6\u675f\u7684\u6df1\u5ea6LDA\uff0c\u5728\u6f5c\u7a7a\u95f4\u56fa\u5b9a\u7c7b\u522b\u5747\u503c\u4e3a\u6b63\u5355\u7eaf\u5f62\u9876\u70b9\uff0c\u9650\u5236\u534f\u65b9\u5dee\u4e3a\u7403\u5f62\uff0c\u5b9e\u73b0\u7a33\u5b9a\u8bad\u7ec3\u548c\u826f\u597d\u7c7b\u522b\u5206\u79bb\u3002", "motivation": "\u4f20\u7edfLDA\u662f\u7b80\u5355\u7684\u9ad8\u65af\u6a21\u578b\uff0c\u4f46\u5c06LDA\u5934\u8fde\u63a5\u5230\u795e\u7ecf\u7f16\u7801\u5668\u540e\uff0c\u5982\u4f55\u901a\u8fc7\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u8bad\u7ec3\u6df1\u5ea6\u5206\u7c7b\u5668\u5b58\u5728\u7591\u95ee\u3002\u7814\u7a76\u53d1\u73b0\u65e0\u7ea6\u675f\u7684\u6df1\u5ea6LDA MLE\u8bad\u7ec3\u4f1a\u5ffd\u7565\u5224\u522b\u6027\uff0c\u5bfc\u81f4\u7c7b\u522b\u91cd\u53e0\u6216\u574d\u7f29\uff0c\u5206\u7c7b\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u7ea6\u675f\u7684\u6df1\u5ea6LDA\u516c\u5f0f\uff1a1) \u56fa\u5b9a\u7c7b\u522b\u5747\u503c\u4e3a\u6f5c\u7a7a\u95f4\u4e2d\u6b63\u5355\u7eaf\u5f62\u7684\u9876\u70b9\uff1b2) \u9650\u5236\u5171\u4eab\u534f\u65b9\u5dee\u4e3a\u7403\u5f62\uff1b3) \u4ec5\u5b66\u4e60\u5148\u9a8c\u548c\u5355\u4e2a\u65b9\u5dee\u53c2\u6570\u4ee5\u53ca\u7f16\u7801\u5668\u3002\u5728\u8fd9\u4e9b\u51e0\u4f55\u7ea6\u675f\u4e0b\uff0cMLE\u53d8\u5f97\u7a33\u5b9a\u3002", "result": "\u5728\u56fe\u50cf\u6570\u636e\u96c6\uff08Fashion-MNIST\u3001CIFAR-10\u3001CIFAR-100\uff09\u4e0a\uff0c\u7ea6\u675f\u6df1\u5ea6LDA\u6a21\u578b\u8fbe\u5230\u4e0esoftmax\u57fa\u7ebf\u76f8\u5f53\u7684\u51c6\u786e\u7387\uff0c\u540c\u65f6\u63d0\u4f9b\u7b80\u5355\u53ef\u89e3\u91ca\u7684\u6f5c\u7a7a\u95f4\u51e0\u4f55\u7ed3\u6784\uff0c\u5728\u4e8c\u7ef4\u6295\u5f71\u4e2d\u6e05\u6670\u53ef\u89c1\u3002", "conclusion": "\u901a\u8fc7\u51e0\u4f55\u7ea6\u675f\u7684\u6df1\u5ea6LDA\uff0c\u53ef\u4ee5\u5728\u4fdd\u6301\u4e0esoftmax\u57fa\u7ebf\u7ade\u4e89\u6027\u80fd\u7684\u540c\u65f6\uff0c\u83b7\u5f97\u7ed3\u6784\u6e05\u6670\u3001\u53ef\u89e3\u91ca\u7684\u6f5c\u7a7a\u95f4\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u65e0\u7ea6\u675f\u6df1\u5ea6LDA MLE\u8bad\u7ec3\u4e2d\u7684\u9000\u5316\u95ee\u9898\u3002"}}
{"id": "2601.00860", "categories": ["cs.LG", "cs.AI", "physics.app-ph", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.00860", "abs": "https://arxiv.org/abs/2601.00860", "authors": ["Xidi Wang"], "title": "Path Integral Solution for Dissipative Generative Dynamics", "comment": "6 pages, 2 figures, 2 tables, along with 2 supplementary materials", "summary": "Can purely mechanical systems generate intelligent language? We prove that dissipative quantum dynamics with analytically tractable non-local context aggregation produce coherent text generation, while conservation laws cause fundamental failure. Employing Koopman operators with closed-form path integral propagators, we show irreversible computation fundamentally requires both controlled information dissipation and causal context aggregation. Spectral analysis reveals emergent eigenvalue structure, separating into decay modes (forgetting), growth modes (amplification), and neutral modes (preservation) -- the essential ingredients for directed information flow. Hamiltonian constraints force the elimination of these dissipative modes and degrading performance despite unchanged model capacity. This establishes language generation as dissipative quantum field theory, proving mechanical systems acquire intelligence through the combination of dissipation and non-locality, not through conservation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8bc1\u660e\u7eaf\u673a\u68b0\u7cfb\u7edf\u901a\u8fc7\u8017\u6563\u91cf\u5b50\u52a8\u529b\u5b66\u548c\u975e\u5c40\u57df\u4e0a\u4e0b\u6587\u805a\u5408\u53ef\u4ee5\u751f\u6210\u667a\u80fd\u8bed\u8a00\uff0c\u800c\u5b88\u6052\u5b9a\u5f8b\u4f1a\u5bfc\u81f4\u6839\u672c\u6027\u5931\u8d25\u3002\u8bed\u8a00\u751f\u6210\u672c\u8d28\u4e0a\u662f\u8017\u6563\u91cf\u5b50\u573a\u8bba\u3002", "motivation": "\u63a2\u7a76\u7eaf\u673a\u68b0\u7cfb\u7edf\u662f\u5426\u80fd\u591f\u751f\u6210\u667a\u80fd\u8bed\u8a00\uff0c\u7406\u89e3\u8bed\u8a00\u751f\u6210\u7684\u57fa\u672c\u7269\u7406\u539f\u7406\uff0c\u7279\u522b\u662f\u8017\u6563\u4e0e\u5b88\u6052\u5728\u667a\u80fd\u6d8c\u73b0\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u4f7f\u7528Koopman\u7b97\u5b50\u548c\u95ed\u5f0f\u8def\u5f84\u79ef\u5206\u4f20\u64ad\u5b50\u5206\u6790\u8017\u6563\u91cf\u5b50\u52a8\u529b\u5b66\uff0c\u901a\u8fc7\u8c31\u5206\u6790\u63ed\u793a\u7279\u5f81\u503c\u7ed3\u6784\uff08\u8870\u51cf\u6a21\u5f0f\u3001\u589e\u957f\u6a21\u5f0f\u3001\u4e2d\u6027\u6a21\u5f0f\uff09\uff0c\u7814\u7a76\u4e0d\u53ef\u9006\u8ba1\u7b97\u7684\u57fa\u672c\u8981\u6c42\u3002", "result": "\u8bc1\u660e\u8017\u6563\u91cf\u5b50\u52a8\u529b\u5b66\u80fd\u591f\u4ea7\u751f\u8fde\u8d2f\u7684\u6587\u672c\u751f\u6210\uff0c\u800c\u54c8\u5bc6\u987f\u7ea6\u675f\u4f1a\u6d88\u9664\u8017\u6563\u6a21\u5f0f\u5e76\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u8c31\u5206\u6790\u663e\u793a\u7279\u5f81\u503c\u7ed3\u6784\u5206\u79bb\u4e3a\u8870\u51cf\u3001\u589e\u957f\u548c\u4e2d\u6027\u6a21\u5f0f\uff0c\u8fd9\u662f\u5b9a\u5411\u4fe1\u606f\u6d41\u7684\u57fa\u672c\u8981\u7d20\u3002", "conclusion": "\u8bed\u8a00\u751f\u6210\u672c\u8d28\u4e0a\u662f\u8017\u6563\u91cf\u5b50\u573a\u8bba\uff0c\u673a\u68b0\u7cfb\u7edf\u901a\u8fc7\u8017\u6563\u548c\u975e\u5c40\u57df\u6027\u7684\u7ed3\u5408\u83b7\u5f97\u667a\u80fd\uff0c\u800c\u975e\u901a\u8fc7\u5b88\u6052\u3002\u4e0d\u53ef\u9006\u8ba1\u7b97\u9700\u8981\u53d7\u63a7\u4fe1\u606f\u8017\u6563\u548c\u56e0\u679c\u4e0a\u4e0b\u6587\u805a\u5408\u3002"}}
{"id": "2601.01430", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.01430", "abs": "https://arxiv.org/abs/2601.01430", "authors": ["Poorvi Joshi", "Mohan Gurusamy"], "title": "Context-Aware Information Transfer via Digital Semantic Communication in UAV-Based Networks", "comment": "10 Pages, 6 figures", "summary": "In smart cities, bandwidth-constrained Unmanned Aerial Vehicles (UAVs) often fail to relay mission-critical data in time, compromising real-time decision-making. This highlights the need for faster and more efficient transmission of only the most relevant information. To address this, we propose DSC-UAV model, leveraging a context-adaptive Digital Semantic Communication (DSC) framework. This model redefines aerial data transmission through three core components: prompt-aware encoding, dynamic UAV-enabled relaying, and user mobility-optimized reinforcement learning. Ground users transmit context-driven visual content. Images are encoded via Vision Transformer combined with a prompt-text encoder to generate semantic features based on the desired context (generic or object-specific). These features are then quantized and transmitted over a UAV network that dynamically relays the data. Joint trajectory and resource allocation are optimized using Truncated Quantile Critic (TQC)-aided reinforcement learning technique, which offers greater stability and precision over standard SAC and TD3 due to its resistance to overestimation bias. Simulations demonstrate significant performance improvement, up to 22\\% gain in semantic-structural similarity and 14\\% reduction in Age of Information (AoI) compared to digital and prior UAV-semantic communication baselines. By integrating mobility control with context-driven visual abstraction, DSC-UAV advances resilient, information-centric surveillance for next-generation UAV networks in bandwidth-constrained environments.", "AI": {"tldr": "DSC-UAV\u6a21\u578b\u901a\u8fc7\u4e0a\u4e0b\u6587\u81ea\u9002\u5e94\u7684\u6570\u5b57\u8bed\u4e49\u901a\u4fe1\u6846\u67b6\uff0c\u7ed3\u5408\u63d0\u793a\u611f\u77e5\u7f16\u7801\u3001\u52a8\u6001\u65e0\u4eba\u673a\u4e2d\u7ee7\u548c\u7528\u6237\u79fb\u52a8\u6027\u4f18\u5316\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5e26\u5bbd\u53d7\u9650\u65e0\u4eba\u673a\u7f51\u7edc\u4e2d\u7684\u5173\u952e\u4efb\u52a1\u6570\u636e\u4f20\u8f93\u6548\u7387\u3002", "motivation": "\u5728\u667a\u6167\u57ce\u5e02\u4e2d\uff0c\u5e26\u5bbd\u53d7\u9650\u7684\u65e0\u4eba\u673a\u5f80\u5f80\u65e0\u6cd5\u53ca\u65f6\u4e2d\u7ee7\u5173\u952e\u4efb\u52a1\u6570\u636e\uff0c\u5f71\u54cd\u5b9e\u65f6\u51b3\u7b56\u3002\u8fd9\u51f8\u663e\u4e86\u9700\u8981\u66f4\u5feb\u3001\u66f4\u9ad8\u6548\u5730\u4f20\u8f93\u6700\u76f8\u5173\u4fe1\u606f\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51faDSC-UAV\u6a21\u578b\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u63d0\u793a\u611f\u77e5\u7f16\u7801\uff1a\u4f7f\u7528Vision Transformer\u7ed3\u5408\u63d0\u793a\u6587\u672c\u7f16\u7801\u5668\u751f\u6210\u57fa\u4e8e\u4e0a\u4e0b\u6587\uff08\u901a\u7528\u6216\u7279\u5b9a\u5bf9\u8c61\uff09\u7684\u8bed\u4e49\u7279\u5f81\uff1b2) \u52a8\u6001\u65e0\u4eba\u673a\u4e2d\u7ee7\uff1a\u901a\u8fc7\u65e0\u4eba\u673a\u7f51\u7edc\u4f20\u8f93\u91cf\u5316\u540e\u7684\u8bed\u4e49\u7279\u5f81\uff1b3) \u7528\u6237\u79fb\u52a8\u6027\u4f18\u5316\u7684\u5f3a\u5316\u5b66\u4e60\uff1a\u4f7f\u7528\u622a\u65ad\u5206\u4f4d\u6570\u6279\u8bc4\u5bb6\u6280\u672f\u8054\u5408\u4f18\u5316\u8f68\u8ff9\u548c\u8d44\u6e90\u5206\u914d\uff0c\u76f8\u6bd4\u6807\u51c6SAC\u548cTD3\u5177\u6709\u66f4\u597d\u7684\u7a33\u5b9a\u6027\u548c\u7cbe\u5ea6\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u76f8\u6bd4\u6570\u5b57\u548c\u5148\u524d\u7684\u65e0\u4eba\u673a\u8bed\u4e49\u901a\u4fe1\u57fa\u7ebf\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\uff1a\u8bed\u4e49\u7ed3\u6784\u76f8\u4f3c\u6027\u63d0\u9ad822%\uff0c\u4fe1\u606f\u5e74\u9f84\u51cf\u5c1114%\u3002", "conclusion": "\u901a\u8fc7\u5c06\u79fb\u52a8\u63a7\u5236\u4e0e\u4e0a\u4e0b\u6587\u9a71\u52a8\u7684\u89c6\u89c9\u62bd\u8c61\u76f8\u7ed3\u5408\uff0cDSC-UAV\u4e3a\u5e26\u5bbd\u53d7\u9650\u73af\u5883\u4e2d\u7684\u4e0b\u4e00\u4ee3\u65e0\u4eba\u673a\u7f51\u7edc\u63a8\u8fdb\u4e86\u5177\u6709\u5f39\u6027\u7684\u3001\u4ee5\u4fe1\u606f\u4e3a\u4e2d\u5fc3\u7684\u76d1\u63a7\u80fd\u529b\u3002"}}
{"id": "2601.01631", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2601.01631", "abs": "https://arxiv.org/abs/2601.01631", "authors": ["Javad A. Asadzade", "Nazim I. Mahmudov"], "title": "Stochastic Maximum Principles and Linear-Quadratic Optimal Control Problems for Fractional Backward Stochastic Evolution Equations in Hilbert Spaces", "comment": null, "summary": "This paper develops a comprehensive framework for optimal control of systems governed by fractional backward stochastic evolution equations (FBSEEs) in Hilbert spaces. We first establish a stochastic maximum principle (SMP) as a necessary condition for optimality. This is achieved by introducing spike variations, deriving precise estimates for the associated variational equations, and constructing an adjoint process tailored to the fractional dynamics. Subsequently, we apply this general principle to solve the linear-quadratic (LQ) optimal control problem explicitly. The resulting optimal control is characterized in closed form via the adjoint process and is shown to be governed by a system of coupled fractional forward-backward stochastic equations. Our work bridges fractional calculus with stochastic control theory, providing a rigorous foundation for controlling infinite-dimensional systems with memory and long-range dependencies.", "AI": {"tldr": "\u672c\u6587\u4e3a\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u5206\u6570\u9636\u540e\u5411\u968f\u673a\u6f14\u5316\u65b9\u7a0b(FBSEE)\u7cfb\u7edf\u7684\u6700\u4f18\u63a7\u5236\u5efa\u7acb\u4e86\u5b8c\u6574\u6846\u67b6\uff0c\u5305\u62ec\u968f\u673a\u6700\u5927\u503c\u539f\u7406\u548c\u7ebf\u6027\u4e8c\u6b21\u6700\u4f18\u63a7\u5236\u95ee\u9898\u7684\u663e\u5f0f\u89e3\u3002", "motivation": "\u5c06\u5206\u6570\u9636\u5fae\u79ef\u5206\u4e0e\u968f\u673a\u63a7\u5236\u7406\u8bba\u7ed3\u5408\uff0c\u4e3a\u5177\u6709\u8bb0\u5fc6\u548c\u957f\u7a0b\u4f9d\u8d56\u6027\u7684\u65e0\u9650\u7ef4\u7cfb\u7edf\u63a7\u5236\u63d0\u4f9b\u4e25\u683c\u7406\u8bba\u57fa\u7840\u3002", "method": "1. \u5f15\u5165\u5c16\u5cf0\u53d8\u5206\uff0c\u63a8\u5bfc\u53d8\u5206\u65b9\u7a0b\u7684\u7cbe\u786e\u4f30\u8ba1\uff1b2. \u6784\u5efa\u9002\u5e94\u5206\u6570\u9636\u52a8\u529b\u5b66\u7684\u4f34\u968f\u8fc7\u7a0b\uff1b3. \u5efa\u7acb\u968f\u673a\u6700\u5927\u503c\u539f\u7406\u4f5c\u4e3a\u6700\u4f18\u6027\u7684\u5fc5\u8981\u6761\u4ef6\uff1b4. \u5c06\u8be5\u539f\u7406\u5e94\u7528\u4e8e\u7ebf\u6027\u4e8c\u6b21\u6700\u4f18\u63a7\u5236\u95ee\u9898\uff0c\u83b7\u5f97\u663e\u5f0f\u89e3\u3002", "result": "1. \u5efa\u7acb\u4e86\u5206\u6570\u9636\u540e\u5411\u968f\u673a\u6f14\u5316\u65b9\u7a0b\u7684\u968f\u673a\u6700\u5927\u503c\u539f\u7406\uff1b2. \u83b7\u5f97\u4e86\u7ebf\u6027\u4e8c\u6b21\u6700\u4f18\u63a7\u5236\u95ee\u9898\u7684\u95ed\u5f0f\u89e3\uff1b3. \u6700\u4f18\u63a7\u5236\u7531\u8026\u5408\u7684\u5206\u6570\u9636\u524d\u5411-\u540e\u5411\u968f\u673a\u65b9\u7a0b\u7cfb\u7edf\u63cf\u8ff0\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u6210\u529f\u5c06\u5206\u6570\u9636\u5fae\u79ef\u5206\u4e0e\u968f\u673a\u63a7\u5236\u7406\u8bba\u7ed3\u5408\uff0c\u4e3a\u5177\u6709\u8bb0\u5fc6\u548c\u957f\u7a0b\u4f9d\u8d56\u6027\u7684\u65e0\u9650\u7ef4\u7cfb\u7edf\u63a7\u5236\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u6570\u5b66\u6846\u67b6\u548c\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2601.00856", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00856", "abs": "https://arxiv.org/abs/2601.00856", "authors": ["Milos Stankovic", "Ella Hirche", "Sarah Kollatzsch", "Julia Nadine Doetsch"], "title": "Comment on: Your Brain on ChatGPT: Accumulation of Cognitive Debt When Using an AI Assistant for Essay Writing Tasks", "comment": "Comment on arXiv:2506.08872", "summary": "Recently published work titled Your Brain on ChatGPT: Accumulation of Cognitive Debt When Using an AI Assistant for Essay Writing Task by Kosmyna et al. (2025) has sparked a vivid debate on the topic of artificial intelligence (AI) and human performance. We sincerely congratulate Kosmyna et al. for initiating such important research, collecting a valuable dataset, and establishing highly automated pipelines for Natural Language Processing (NLP) analyses and scoring. We aim to provide constructive comments that may improve the manuscript's readiness for peer-reviewed publication, as some results by Kosmyna et al. (2025) could be interpreted more conservatively. Our primary concerns focus on: (i) study design considerations, including the limited sample size; (ii) the reproducibility of the analyses; (iii) methodological issues related to the EEG analysis; (iv) inconsistencies in the reporting of results; and (v) limited transparency in several aspects of the study's procedures and findings.", "AI": {"tldr": "\u8fd9\u662f\u4e00\u7bc7\u5bf9Kosmyna\u7b49\u4eba(2025)\u5173\u4e8eChatGPT\u5bf9\u5927\u8111\u8ba4\u77e5\u5f71\u54cd\u7814\u7a76\u7684\u8bc4\u8bba\u6587\u7ae0\uff0c\u6307\u51fa\u4e86\u8be5\u7814\u7a76\u5728\u6837\u672c\u91cf\u3001\u53ef\u91cd\u590d\u6027\u3001EEG\u5206\u6790\u65b9\u6cd5\u3001\u7ed3\u679c\u62a5\u544a\u4e00\u81f4\u6027\u548c\u900f\u660e\u5ea6\u7b49\u65b9\u9762\u5b58\u5728\u7684\u95ee\u9898\u3002", "motivation": "\u5bf9Kosmyna\u7b49\u4eba\u5173\u4e8eAI\u52a9\u624b\u5bf9\u5199\u4f5c\u4efb\u52a1\u4e2d\u8ba4\u77e5\u5f71\u54cd\u7684\u7814\u7a76\u8fdb\u884c\u5efa\u8bbe\u6027\u8bc4\u8bba\uff0c\u65e8\u5728\u63d0\u9ad8\u8be5\u7814\u7a76\u7684\u5b66\u672f\u4e25\u8c28\u6027\u548c\u53d1\u8868\u51c6\u5907\u5ea6\uff0c\u56e0\u4e3a\u67d0\u4e9b\u7ed3\u679c\u53ef\u80fd\u9700\u8981\u66f4\u4fdd\u5b88\u7684\u89e3\u91ca\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6027\u5206\u6790\uff0c\u4ece\u4e94\u4e2a\u65b9\u9762\u63d0\u51fa\u6279\u8bc4\u610f\u89c1\uff1a\u7814\u7a76\u8bbe\u8ba1\uff08\u6837\u672c\u91cf\u6709\u9650\uff09\u3001\u5206\u6790\u53ef\u91cd\u590d\u6027\u3001EEG\u5206\u6790\u65b9\u6cd5\u95ee\u9898\u3001\u7ed3\u679c\u62a5\u544a\u4e0d\u4e00\u81f4\u6027\u3001\u7814\u7a76\u8fc7\u7a0b\u548c\u53d1\u73b0\u900f\u660e\u5ea6\u4e0d\u8db3\u3002", "result": "\u6307\u51fa\u4e86\u539f\u7814\u7a76\u5728\u591a\u4e2a\u5173\u952e\u65b9\u9762\u5b58\u5728\u7f3a\u9677\uff0c\u5305\u62ec\u6837\u672c\u4ee3\u8868\u6027\u4e0d\u8db3\u3001\u5206\u6790\u65b9\u6cd5\u53ef\u80fd\u4e0d\u53ef\u91cd\u590d\u3001EEG\u6570\u636e\u5904\u7406\u65b9\u6cd5\u95ee\u9898\u3001\u7ed3\u679c\u62a5\u544a\u5b58\u5728\u4e0d\u4e00\u81f4\u3001\u4ee5\u53ca\u7f3a\u4e4f\u8db3\u591f\u7684\u900f\u660e\u5ea6\u3002", "conclusion": "\u867d\u7136\u8ba4\u53efKosmyna\u7b49\u4eba\u7814\u7a76\u7684\u91cd\u8981\u6027\u548c\u4ef7\u503c\uff0c\u4f46\u5efa\u8bae\u5728\u53d1\u8868\u524d\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u65b9\u6cd5\u5b66\u95ee\u9898\uff0c\u4ee5\u786e\u4fdd\u7814\u7a76\u7ed3\u679c\u7684\u53ef\u9760\u6027\u548c\u79d1\u5b66\u4e25\u8c28\u6027\u3002"}}
{"id": "2601.01126", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01126", "abs": "https://arxiv.org/abs/2601.01126", "authors": ["Andrew Borthwick", "Stephen Ash"], "title": "RoboPhD: Self-Improving Text-to-SQL Through Autonomous Agent Evolution", "comment": "18 pages, 3 figures", "summary": "We present RoboPhD, a system where AI agents autonomously conduct research to improve Text-to-SQL performance. RoboPhD implements a closed-loop evolution cycle with two coordinated components: a SQL Generation agent composed of a database analysis script and SQL generation instructions, and an Evolution agent that designs new versions based on performance feedback. Central to the framework is an ELO-based selection mechanism enabling survival-of-the-fittest dynamics while handling non-transitivity in performance. Starting from a naive 70-line baseline, RoboPhD evolves agents through iterative cross-pollination, discovering effective techniques without any external guidance on the Text-to-SQL domain. Our best agent, evolved to 1500 lines over 18 iterations, autonomously discovered strategies such as size-adaptive database analysis that adjusts depth based on schema complexity and SQL generation patterns for column selection, evidence interpretation, and aggregation. Evolution provides the largest gains on cheaper models: while we improve by 2.3 points over a strong Claude Opus 4.5 naive baseline, we show an improvement of 8.9 points over the weaker Claude Haiku model. This enables 'skip a tier' deployment: evolved Haiku exceeds naive Sonnet accuracy, and evolved Sonnet exceeds naive Opus, both at lower cost. The full system achieves 73.67% accuracy on the BIRD test set, demonstrating that AI can autonomously build a strong agentic system with only a trivial human-provided starting point.", "AI": {"tldr": "RoboPhD\u662f\u4e00\u4e2aAI\u81ea\u4e3b\u7814\u7a76\u7cfb\u7edf\uff0c\u901a\u8fc7\u8fdb\u5316\u5faa\u73af\u81ea\u52a8\u6539\u8fdbText-to-SQL\u6027\u80fd\uff0c\u65e0\u9700\u9886\u57df\u6307\u5bfc\uff0c\u4ece70\u884c\u57fa\u7ebf\u8fdb\u5316\u52301500\u884c\uff0c\u5728BIRD\u6570\u636e\u96c6\u4e0a\u8fbe\u523073.67%\u51c6\u786e\u7387\u3002", "motivation": "\u63a2\u7d22AI\u7cfb\u7edf\u80fd\u5426\u81ea\u4e3b\u8fdb\u884c\u79d1\u5b66\u7814\u7a76\uff0c\u4ece\u7b80\u5355\u8d77\u70b9\u81ea\u52a8\u8fdb\u5316\u51fa\u9ad8\u6027\u80fd\u7684Text-to-SQL\u4ee3\u7406\uff0c\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u548c\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u9700\u6c42\u3002", "method": "\u91c7\u7528\u95ed\u73af\u8fdb\u5316\u5faa\u73af\uff1aSQL\u751f\u6210\u4ee3\u7406\uff08\u6570\u636e\u5e93\u5206\u6790\u811a\u672c+SQL\u751f\u6210\u6307\u4ee4\uff09\u548c\u8fdb\u5316\u4ee3\u7406\uff08\u57fa\u4e8e\u6027\u80fd\u53cd\u9988\u8bbe\u8ba1\u65b0\u7248\u672c\uff09\uff0c\u4f7f\u7528ELO\u9009\u62e9\u673a\u5236\u5904\u7406\u975e\u4f20\u9012\u6027\u6027\u80fd\uff0c\u901a\u8fc7\u8fed\u4ee3\u4ea4\u53c9\u6388\u7c89\u8fdb\u5316\u3002", "result": "\u4ece70\u884c\u57fa\u7ebf\u8fdb\u531618\u6b21\u52301500\u884c\u4ee3\u7406\uff0c\u81ea\u4e3b\u53d1\u73b0\u5927\u5c0f\u81ea\u9002\u5e94\u6570\u636e\u5e93\u5206\u6790\u3001\u5217\u9009\u62e9\u3001\u8bc1\u636e\u89e3\u91ca\u548c\u805a\u5408\u7b49\u7b56\u7565\u3002\u5728BIRD\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u523073.67%\u51c6\u786e\u7387\uff0c\u5bf9\u8f83\u5f31\u6a21\u578b\u63d0\u5347\u66f4\u5927\uff08Haiku\u63d0\u53478.9\u70b9\uff09\uff0c\u5b9e\u73b0\"\u8df3\u7ea7\u90e8\u7f72\"\uff1a\u8fdb\u5316Haiku\u8d85\u8fc7\u539f\u751fSonnet\uff0c\u8fdb\u5316Sonnet\u8d85\u8fc7\u539f\u751fOpus\u3002", "conclusion": "AI\u80fd\u591f\u4ece\u7b80\u5355\u8d77\u70b9\u81ea\u4e3b\u6784\u5efa\u5f3a\u5927\u7684\u4ee3\u7406\u7cfb\u7edf\uff0c\u8fdb\u5316\u5bf9\u5ec9\u4ef7\u6a21\u578b\u63d0\u5347\u6700\u5927\uff0c\u5b9e\u73b0\u6210\u672c\u6548\u76ca\u7684\"\u8df3\u7ea7\u90e8\u7f72\"\uff0c\u5c55\u793a\u4e86AI\u81ea\u4e3b\u7814\u7a76\u7684\u6f5c\u529b\u3002"}}
{"id": "2601.02209", "categories": ["cs.CL", "cs.CY", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.02209", "abs": "https://arxiv.org/abs/2601.02209", "authors": ["Omer Nacar", "Serry Sibaee", "Adel Ammar", "Yasser Alhabashi", "Nadia Samer Sibai", "Yara Farouk Ahmed", "Ahmed Saud Alqusaiyer", "Sulieman Mahmoud AlMahmoud", "Abdulrhman Mamdoh Mukhaniq", "Lubaba Raed", "Sulaiman Mohammed Alatwah", "Waad Nasser Alqahtani", "Yousif Abdulmajeed Alnasser", "Mohamed Aziz Khadraoui", "Wadii Boulila"], "title": "ARCADE: A City-Scale Corpus for Fine-Grained Arabic Dialect Tagging", "comment": null, "summary": "The Arabic language is characterized by a rich tapestry of regional dialects that differ substantially in phonetics and lexicon, reflecting the geographic and cultural diversity of its speakers. Despite the availability of many multi-dialect datasets, mapping speech to fine-grained dialect sources, such as cities, remains underexplored. We present ARCADE (Arabic Radio Corpus for Audio Dialect Evaluation), the first Arabic speech dataset designed explicitly with city-level dialect granularity. The corpus comprises Arabic radio speech collected from streaming services across the Arab world. Our data pipeline captures 30-second segments from verified radio streams, encompassing both Modern Standard Arabic (MSA) and diverse dialectal speech. To ensure reliability, each clip was annotated by one to three native Arabic reviewers who assigned rich metadata, including emotion, speech type, dialect category, and a validity flag for dialect identification tasks. The resulting corpus comprises 6,907 annotations and 3,790 unique audio segments spanning 58 cities across 19 countries. These fine-grained annotations enable robust multi-task learning, serving as a benchmark for city-level dialect tagging. We detail the data collection methodology, assess audio quality, and provide a comprehensive analysis of label distributions. The dataset is available on: https://huggingface.co/datasets/riotu-lab/ARCADE-full", "AI": {"tldr": "ARCADE\u662f\u9996\u4e2a\u57ce\u5e02\u7ea7\u7ec6\u7c92\u5ea6\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\u8bed\u97f3\u6570\u636e\u96c6\uff0c\u5305\u542b\u6765\u81ea19\u4e2a\u56fd\u5bb658\u4e2a\u57ce\u5e02\u76843790\u4e2a\u97f3\u9891\u7247\u6bb5\u548c6907\u4e2a\u6807\u6ce8\uff0c\u652f\u6301\u65b9\u8a00\u8bc6\u522b\u7b49\u591a\u4efb\u52a1\u5b66\u4e60\u3002", "motivation": "\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\u5728\u8bed\u97f3\u548c\u8bcd\u6c47\u4e0a\u5b58\u5728\u663e\u8457\u5730\u57df\u5dee\u5f02\uff0c\u4f46\u73b0\u6709\u591a\u65b9\u8a00\u6570\u636e\u96c6\u7f3a\u4e4f\u57ce\u5e02\u7ea7\u7ec6\u7c92\u5ea6\u6807\u6ce8\uff0c\u9650\u5236\u4e86\u65b9\u8a00\u8bc6\u522b\u7814\u7a76\u7684\u6df1\u5165\u53d1\u5c55\u3002", "method": "\u4ece\u963f\u62c9\u4f2f\u4e16\u754c\u6d41\u5a92\u4f53\u670d\u52a1\u6536\u96c6\u5e7f\u64ad\u8bed\u97f3\uff0c\u622a\u53d630\u79d2\u7247\u6bb5\uff0c\u75311-3\u540d\u963f\u62c9\u4f2f\u8bed\u6bcd\u8bed\u8005\u6807\u6ce8\u60c5\u611f\u3001\u8bed\u97f3\u7c7b\u578b\u3001\u65b9\u8a00\u7c7b\u522b\u548c\u6709\u6548\u6027\u7b49\u5143\u6570\u636e\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b6907\u4e2a\u6807\u6ce8\u548c3790\u4e2a\u72ec\u7279\u97f3\u9891\u7247\u6bb5\u7684\u6570\u636e\u96c6\uff0c\u6db5\u76d619\u4e2a\u56fd\u5bb658\u4e2a\u57ce\u5e02\uff0c\u652f\u6301\u57ce\u5e02\u7ea7\u65b9\u8a00\u6807\u6ce8\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "conclusion": "ARCADE\u586b\u8865\u4e86\u963f\u62c9\u4f2f\u8bed\u57ce\u5e02\u7ea7\u65b9\u8a00\u6570\u636e\u96c6\u7684\u7a7a\u767d\uff0c\u4e3a\u7ec6\u7c92\u5ea6\u65b9\u8a00\u8bc6\u522b\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u9760\u57fa\u51c6\uff0c\u6570\u636e\u96c6\u5df2\u516c\u5f00\u53ef\u7528\u3002"}}
{"id": "2601.01757", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01757", "abs": "https://arxiv.org/abs/2601.01757", "authors": ["Jiakun Jiang", "Dewei Xiang", "Chenliang Gu", "Wei Liu", "Binhuan Wang"], "title": "Sparse Convex Biclustering", "comment": null, "summary": "Biclustering is an essential unsupervised machine learning technique for simultaneously clustering rows and columns of a data matrix, with widespread applications in genomics, transcriptomics, and other high-dimensional omics data. Despite its importance, existing biclustering methods struggle to meet the demands of modern large-scale datasets. The challenges stem from the accumulation of noise in high-dimensional features, the limitations of non-convex optimization formulations, and the computational complexity of identifying meaningful biclusters. These issues often result in reduced accuracy and stability as the size of the dataset increases. To overcome these challenges, we propose Sparse Convex Biclustering (SpaCoBi), a novel method that penalizes noise during the biclustering process to improve both accuracy and robustness. By adopting a convex optimization framework and introducing a stability-based tuning criterion, SpaCoBi achieves an optimal balance between cluster fidelity and sparsity. Comprehensive numerical studies, including simulations and an application to mouse olfactory bulb data, demonstrate that SpaCoBi significantly outperforms state-of-the-art methods in accuracy. These results highlight SpaCoBi as a robust and efficient solution for biclustering in high-dimensional and large-scale datasets.", "AI": {"tldr": "SpaCoBi\u662f\u4e00\u79cd\u65b0\u7684\u7a00\u758f\u51f8\u53cc\u805a\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u60e9\u7f5a\u566a\u58f0\u548c\u91c7\u7528\u51f8\u4f18\u5316\u6846\u67b6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9ad8\u7ef4\u5927\u89c4\u6a21\u6570\u636e\u96c6\u53cc\u805a\u7c7b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u53cc\u805a\u7c7b\u65b9\u6cd5\u5728\u5904\u7406\u73b0\u4ee3\u5927\u89c4\u6a21\u6570\u636e\u96c6\u65f6\u9762\u4e34\u6311\u6218\uff1a\u9ad8\u7ef4\u7279\u5f81\u4e2d\u7684\u566a\u58f0\u7d2f\u79ef\u3001\u975e\u51f8\u4f18\u5316\u516c\u5f0f\u7684\u9650\u5236\u4ee5\u53ca\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u5bfc\u81f4\u968f\u7740\u6570\u636e\u96c6\u589e\u5927\u800c\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u4e0b\u964d\u3002", "method": "\u63d0\u51faSparse Convex Biclustering (SpaCoBi)\uff0c\u5728\u53cc\u805a\u7c7b\u8fc7\u7a0b\u4e2d\u60e9\u7f5a\u566a\u58f0\uff0c\u91c7\u7528\u51f8\u4f18\u5316\u6846\u67b6\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u7a33\u5b9a\u6027\u7684\u8c03\u4f18\u51c6\u5219\uff0c\u5b9e\u73b0\u805a\u7c7b\u4fdd\u771f\u5ea6\u548c\u7a00\u758f\u6027\u4e4b\u95f4\u7684\u6700\u4f18\u5e73\u8861\u3002", "result": "\u7efc\u5408\u6570\u503c\u7814\u7a76\uff08\u5305\u62ec\u6a21\u62df\u5b9e\u9a8c\u548c\u5bf9\u5c0f\u9f20\u55c5\u7403\u6570\u636e\u7684\u5e94\u7528\uff09\u8868\u660e\uff0cSpaCoBi\u5728\u51c6\u786e\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "SpaCoBi\u4e3a\u9ad8\u7ef4\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7684\u53cc\u805a\u7c7b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9c81\u68d2\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.00862", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00862", "abs": "https://arxiv.org/abs/2601.00862", "authors": ["Joey Chan", "Huan Wang", "Haoyu Pan", "Wei Wu", "Zirong Wang", "Zhen Chen", "Ershun Pan", "Min Xie", "Lifeng Xi"], "title": "Universal Battery Degradation Forecasting Driven by Foundation Model Across Diverse Chemistries and Conditions", "comment": "Due to space limitations, the open-source method for supporting materials is currently under discussion", "summary": "Accurate forecasting of battery capacity fade is essential for the safety, reliability, and long-term efficiency of energy storage systems. However, the strong heterogeneity across cell chemistries, form factors, and operating conditions makes it difficult to build a single model that generalizes beyond its training domain. This work proposes a unified capacity forecasting framework that maintains robust performance across diverse chemistries and usage scenarios. We curate 20 public aging datasets into a large-scale corpus covering 1,704 cells and 3,961,195 charge-discharge cycle segments, spanning temperatures from $-5\\,^{\\circ}\\mathrm{C}$ to $45\\,^{\\circ}\\mathrm{C}$, multiple C-rates, and application-oriented profiles such as fast charging and partial cycling. On this corpus, we adopt a Time-Series Foundation Model (TSFM) backbone and apply parameter-efficient Low-Rank Adaptation (LoRA) together with physics-guided contrastive representation learning to capture shared degradation patterns. Experiments on both seen and deliberately held-out unseen datasets show that a single unified model achieves competitive or superior accuracy compared with strong per-dataset baselines, while retaining stable performance on chemistries, capacity scales, and operating conditions excluded from training. These results demonstrate the potential of TSFM-based architectures as a scalable and transferable solution for capacity degradation forecasting in real battery management systems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684\u7535\u6c60\u5bb9\u91cf\u8870\u51cf\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u548c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff0c\u5728\u591a\u79cd\u5316\u5b66\u4f53\u7cfb\u548c\u5de5\u51b5\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\u80fd", "motivation": "\u7535\u6c60\u5bb9\u91cf\u8870\u51cf\u9884\u6d4b\u5bf9\u50a8\u80fd\u7cfb\u7edf\u7684\u5b89\u5168\u3001\u53ef\u9760\u548c\u957f\u671f\u6548\u7387\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u4e0d\u540c\u7535\u6c60\u5316\u5b66\u4f53\u7cfb\u3001\u5916\u5f62\u5c3a\u5bf8\u548c\u8fd0\u884c\u6761\u4ef6\u7684\u5f3a\u5f02\u8d28\u6027\uff0c\u96be\u4ee5\u6784\u5efa\u4e00\u4e2a\u80fd\u6cdb\u5316\u5230\u8bad\u7ec3\u57df\u4e4b\u5916\u7684\u5355\u4e00\u6a21\u578b", "method": "\u6574\u540820\u4e2a\u516c\u5f00\u8001\u5316\u6570\u636e\u96c6\u6784\u5efa\u5927\u89c4\u6a21\u8bed\u6599\u5e93\uff081,704\u4e2a\u7535\u6c60\uff0c3,961,195\u4e2a\u5145\u653e\u7535\u5faa\u73af\u6bb5\uff09\uff0c\u91c7\u7528\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\uff08TSFM\uff09\u4f5c\u4e3a\u9aa8\u5e72\uff0c\u7ed3\u5408\u53c2\u6570\u9ad8\u6548\u7684LoRA\u5fae\u8c03\u548c\u7269\u7406\u5f15\u5bfc\u7684\u5bf9\u6bd4\u8868\u793a\u5b66\u4e60\u6765\u6355\u6349\u5171\u4eab\u7684\u9000\u5316\u6a21\u5f0f", "result": "\u5355\u4e00\u7edf\u4e00\u6a21\u578b\u5728\u5df2\u89c1\u548c\u523b\u610f\u4fdd\u7559\u7684\u672a\u89c1\u6570\u636e\u96c6\u4e0a\u5747\u53d6\u5f97\u4e0e\u5f3a\u57fa\u7ebf\u76f8\u5f53\u6216\u66f4\u4f18\u7684\u7cbe\u5ea6\uff0c\u540c\u65f6\u5728\u8bad\u7ec3\u4e2d\u6392\u9664\u7684\u5316\u5b66\u4f53\u7cfb\u3001\u5bb9\u91cf\u5c3a\u5ea6\u548c\u8fd0\u884c\u6761\u4ef6\u4e0b\u4fdd\u6301\u7a33\u5b9a\u6027\u80fd", "conclusion": "\u57fa\u4e8eTSFM\u7684\u67b6\u6784\u5c55\u793a\u4e86\u4f5c\u4e3a\u53ef\u6269\u5c55\u548c\u53ef\u8fc1\u79fb\u89e3\u51b3\u65b9\u6848\u7684\u6f5c\u529b\uff0c\u53ef\u7528\u4e8e\u5b9e\u9645\u7535\u6c60\u7ba1\u7406\u7cfb\u7edf\u4e2d\u7684\u5bb9\u91cf\u9000\u5316\u9884\u6d4b"}}
{"id": "2601.01629", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.01629", "abs": "https://arxiv.org/abs/2601.01629", "authors": ["Qingzuo Meng", "Pengfeng Lin", "Yujie Wang", "Miao Zhu", "Amer M. Ghias", "Syed Islam", "Frede Blaabjerg"], "title": "Full-Time-Scale Power Management Strategy for Hybrid AC/DC/DS Microgrid with Dynamic Concatenation and Autonomous Frequency / Voltage Restorations", "comment": null, "summary": "Hybrid AC/DC microgrids with distributed energy storage (DS) improve power reliability in remote areas. Existing power management methods either focus on steady-state power sharing or transient inertia support, but rarely combine both. They also often ignore frequency and voltage deviations caused by droop control, which can harm sensitive loads. To overcome these issues, this paper proposes a full-time-scale (FTS) power management strategy that unifies transient inertia sharing and steady-state power allocation through a novel dynamic concatenator. It also introduces autonomous frequency/voltage restoration to eliminate steady-state deviations in each subgrid. Additionally, a global equivalent circuit model (GECM) is developed to simplify system analysis and design. Experiments confirm that the approach maintains nominal frequency and voltage in steady state while enabling seamless transition between transient inertia support and proportional power sharing across all time scales.", "AI": {"tldr": "\u63d0\u51fa\u5168\u65f6\u6807\u529f\u7387\u7ba1\u7406\u7b56\u7565\uff0c\u7edf\u4e00\u6682\u6001\u60ef\u91cf\u5171\u4eab\u548c\u7a33\u6001\u529f\u7387\u5206\u914d\uff0c\u6d88\u9664\u4e0b\u5782\u63a7\u5236\u5f15\u8d77\u7684\u9891\u7387\u7535\u538b\u504f\u5dee", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u5173\u6ce8\u7a33\u6001\u529f\u7387\u5206\u914d\uff0c\u8981\u4e48\u5173\u6ce8\u6682\u6001\u60ef\u91cf\u652f\u6301\uff0c\u5f88\u5c11\u7ed3\u5408\u4e24\u8005\uff0c\u4e14\u5e38\u5ffd\u7565\u4e0b\u5782\u63a7\u5236\u5f15\u8d77\u7684\u9891\u7387\u7535\u538b\u504f\u5dee\uff0c\u53ef\u80fd\u635f\u5bb3\u654f\u611f\u8d1f\u8f7d", "method": "\u63d0\u51fa\u5168\u65f6\u6807\u529f\u7387\u7ba1\u7406\u7b56\u7565\uff0c\u901a\u8fc7\u52a8\u6001\u8fde\u63a5\u5668\u7edf\u4e00\u6682\u6001\u60ef\u91cf\u5171\u4eab\u548c\u7a33\u6001\u529f\u7387\u5206\u914d\uff1b\u5f15\u5165\u81ea\u4e3b\u9891\u7387/\u7535\u538b\u6062\u590d\u6d88\u9664\u5404\u5b50\u7f51\u7a33\u6001\u504f\u5dee\uff1b\u5f00\u53d1\u5168\u5c40\u7b49\u6548\u7535\u8def\u6a21\u578b\u7b80\u5316\u7cfb\u7edf\u5206\u6790\u548c\u8bbe\u8ba1", "result": "\u5b9e\u9a8c\u8bc1\u5b9e\u8be5\u65b9\u6cd5\u5728\u7a33\u6001\u4e0b\u4fdd\u6301\u989d\u5b9a\u9891\u7387\u548c\u7535\u538b\uff0c\u540c\u65f6\u5b9e\u73b0\u6240\u6709\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u6682\u6001\u60ef\u91cf\u652f\u6301\u548c\u6bd4\u4f8b\u529f\u7387\u5206\u914d\u7684\u65e0\u7f1d\u8fc7\u6e21", "conclusion": "\u63d0\u51fa\u7684\u5168\u65f6\u6807\u529f\u7387\u7ba1\u7406\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u6df7\u5408\u4ea4\u76f4\u6d41\u5fae\u7535\u7f51\u4e2d\u529f\u7387\u7ba1\u7406\u7684\u7efc\u5408\u95ee\u9898\uff0c\u7edf\u4e00\u4e86\u6682\u6001\u548c\u7a33\u6001\u63a7\u5236\uff0c\u6d88\u9664\u4e86\u4f20\u7edf\u4e0b\u5782\u63a7\u5236\u7684\u504f\u5dee\u95ee\u9898"}}
{"id": "2601.01634", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2601.01634", "abs": "https://arxiv.org/abs/2601.01634", "authors": ["Bouchra Elghazi", "Birgit Jacob", "Hans Zwart"], "title": "Boundary control systems on a one-dimension spatial domain", "comment": "24 pages", "summary": "The aim of this paper is to investigate the well-posedness of a class of boundary control and observation systems on a one dimensional spatial domain. We derive a necessary and sufficient condition characterizing the well-posedness of these systems. Furthermore, we show that the well-posedness and full control and observation implies exact controllability and exact observability. The theoretical results are illustrated using Euler-Bernoulli beam models.", "AI": {"tldr": "\u7814\u7a76\u4e00\u7ef4\u8fb9\u754c\u63a7\u5236\u89c2\u6d4b\u7cfb\u7edf\u7684\u9002\u5b9a\u6027\uff0c\u7ed9\u51fa\u5145\u8981\u6761\u4ef6\uff0c\u5e76\u8bc1\u660e\u9002\u5b9a\u6027+\u5b8c\u5168\u63a7\u5236\u89c2\u6d4b\u53ef\u63a8\u51fa\u7cbe\u786e\u53ef\u63a7\u53ef\u89c2\u6027\uff0c\u4ee5\u6b27\u62c9-\u4f2f\u52aa\u5229\u6881\u6a21\u578b\u4e3a\u4f8b\u8bf4\u660e\u3002", "motivation": "\u7814\u7a76\u4e00\u7ef4\u7a7a\u95f4\u57df\u4e0a\u8fb9\u754c\u63a7\u5236\u4e0e\u89c2\u6d4b\u7cfb\u7edf\u7684\u9002\u5b9a\u6027\u95ee\u9898\uff0c\u4e3a\u8fd9\u7c7b\u7cfb\u7edf\u7684\u7406\u8bba\u5206\u6790\u63d0\u4f9b\u57fa\u7840\u6846\u67b6\u3002", "method": "\u63a8\u5bfc\u8fb9\u754c\u63a7\u5236\u89c2\u6d4b\u7cfb\u7edf\u9002\u5b9a\u6027\u7684\u5145\u8981\u6761\u4ef6\uff0c\u5efa\u7acb\u9002\u5b9a\u6027\u4e0e\u5b8c\u5168\u63a7\u5236\u89c2\u6d4b\u5230\u7cbe\u786e\u53ef\u63a7\u53ef\u89c2\u6027\u7684\u7406\u8bba\u5173\u7cfb\u3002", "result": "\u83b7\u5f97\u4e86\u9002\u5b9a\u6027\u7684\u5145\u8981\u6761\u4ef6\uff0c\u8bc1\u660e\u4e86\u9002\u5b9a\u6027\u52a0\u5b8c\u5168\u63a7\u5236\u89c2\u6d4b\u53ef\u63a8\u51fa\u7cbe\u786e\u53ef\u63a7\u6027\u548c\u7cbe\u786e\u53ef\u89c2\u6027\u3002", "conclusion": "\u4e3a\u4e00\u7ef4\u8fb9\u754c\u63a7\u5236\u89c2\u6d4b\u7cfb\u7edf\u5efa\u7acb\u4e86\u5b8c\u6574\u7684\u9002\u5b9a\u6027\u7406\u8bba\u6846\u67b6\uff0c\u5e76\u4ee5\u6b27\u62c9-\u4f2f\u52aa\u5229\u6881\u6a21\u578b\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2601.00869", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00869", "abs": "https://arxiv.org/abs/2601.00869", "authors": ["Huang Junyao", "Situ Ruimin", "Ye Renqin"], "title": "Cultural Encoding in Large Language Models: The Existence Gap in AI-Mediated Brand Discovery", "comment": "19 pages, 5 tables. Dataset and code available at https://github.com/zhizibianjie-omniedge/geo-cultural-encoding", "summary": "As artificial intelligence systems increasingly mediate consumer information discovery,\n  brands face algorithmic invisibility. This study investigates Cultural Encoding in Large\n  Language Models (LLMs) -- systematic differences in brand recommendations arising from\n  training data composition. Analyzing 1,909 pure-English queries across 6 LLMs (GPT-4o,\n  Claude, Gemini, Qwen3, DeepSeek, Doubao) and 30 brands, we find Chinese LLMs exhibit 30.6\n  percentage points higher brand mention rates than International LLMs (88.9% vs. 58.3%,\n  p<.001). This disparity persists in identical English queries, indicating training data\n  geography -- not language -- drives the effect. We introduce the Existence Gap: brands\n  absent from LLM training corpora lack \"existence\" in AI responses regardless of quality.\n  Through a case study of Zhizibianjie (OmniEdge), a collaboration platform with 65.6%\n  mention rate in Chinese LLMs but 0% in International models (p<.001), we demonstrate how\n  Linguistic Boundary Barriers create invisible market entry obstacles. Theoretically, we\n  contribute the Data Moat Framework, conceptualizing AI-visible content as a VRIN strategic\n  resource. We operationalize Algorithmic Omnipresence -- comprehensive brand visibility\n  across LLM knowledge bases -- as the strategic objective for Generative Engine Optimization\n  (GEO). Managerially, we provide an 18-month roadmap for brands to build Data Moats\n  through semantic coverage, technical depth, and cultural localization. Our findings reveal\n  that in AI-mediated markets, the limits of a brand's \"Data Boundaries\" define the limits\n  of its \"Market Frontiers.\"", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLM\u8bad\u7ec3\u6570\u636e\u7684\u5730\u7406\u5206\u5e03\u5bfc\u81f4\u54c1\u724c\u63a8\u8350\u5b58\u5728\u7cfb\u7edf\u6027\u5dee\u5f02\uff0c\u4e2d\u56fdLLM\u7684\u54c1\u724c\u63d0\u53ca\u7387\u6bd4\u56fd\u9645LLM\u9ad830.6\u4e2a\u767e\u5206\u70b9\uff0c\u63d0\u51fa\u4e86\"\u5b58\u5728\u9e3f\u6c9f\"\u548c\"\u6570\u636e\u62a4\u57ce\u6cb3\"\u6846\u67b6\u3002", "motivation": "\u968f\u7740AI\u7cfb\u7edf\u8d8a\u6765\u8d8a\u591a\u5730\u4e2d\u4ecb\u6d88\u8d39\u8005\u4fe1\u606f\u53d1\u73b0\uff0c\u54c1\u724c\u9762\u4e34\u7b97\u6cd5\u4e0d\u53ef\u89c1\u6027\u95ee\u9898\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7a76LLM\u4e2d\u7684\u6587\u5316\u7f16\u7801\u73b0\u8c61\u2014\u2014\u7531\u8bad\u7ec3\u6570\u636e\u6784\u6210\u5bfc\u81f4\u7684\u54c1\u724c\u63a8\u8350\u7cfb\u7edf\u6027\u5dee\u5f02\u3002", "method": "\u5206\u6790\u4e861,909\u4e2a\u7eaf\u82f1\u6587\u67e5\u8be2\uff0c\u8986\u76d66\u4e2aLLM\uff08GPT-4o\u3001Claude\u3001Gemini\u3001Qwen3\u3001DeepSeek\u3001Doubao\uff09\u548c30\u4e2a\u54c1\u724c\u3002\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u667a\u5b50\u8fb9\u754c\uff08OmniEdge\uff09\u5e73\u53f0\u9a8c\u8bc1\u53d1\u73b0\u3002", "result": "\u4e2d\u56fdLLM\u7684\u54c1\u724c\u63d0\u53ca\u7387\u6bd4\u56fd\u9645LLM\u9ad830.6\u4e2a\u767e\u5206\u70b9\uff0888.9% vs. 58.3%\uff09\uff0c\u8fd9\u79cd\u5dee\u5f02\u5728\u76f8\u540c\u7684\u82f1\u6587\u67e5\u8be2\u4e2d\u6301\u7eed\u5b58\u5728\uff0c\u8868\u660e\u8bad\u7ec3\u6570\u636e\u7684\u5730\u7406\u5206\u5e03\u800c\u975e\u8bed\u8a00\u9a71\u52a8\u4e86\u8fd9\u79cd\u6548\u5e94\u3002\u63d0\u51fa\u4e86\"\u5b58\u5728\u9e3f\u6c9f\"\u6982\u5ff5\u3002", "conclusion": "\u63d0\u51fa\u4e86\u6570\u636e\u62a4\u57ce\u6cb3\u6846\u67b6\uff0c\u5c06AI\u53ef\u89c1\u5185\u5bb9\u6982\u5ff5\u5316\u4e3aVRIN\u6218\u7565\u8d44\u6e90\u3002\u64cd\u4f5c\u5316\u4e86\"\u7b97\u6cd5\u65e0\u5904\u4e0d\u5728\"\u4f5c\u4e3a\u751f\u6210\u5f15\u64ce\u4f18\u5316\u7684\u6218\u7565\u76ee\u6807\u3002\u5728AI\u4e2d\u4ecb\u7684\u5e02\u573a\u4e2d\uff0c\u54c1\u724c\u7684\"\u6570\u636e\u8fb9\u754c\"\u51b3\u5b9a\u4e86\u5176\"\u5e02\u573a\u524d\u6cbf\"\u7684\u754c\u9650\u3002"}}
{"id": "2601.01143", "categories": ["cs.CL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2601.01143", "abs": "https://arxiv.org/abs/2601.01143", "authors": ["Peng Chen"], "title": "KOS-TL (Knowledge Operation System Type Logic)", "comment": null, "summary": "This paper introduces KOS-TL (Knowledge Operation System Type Logic), a novel constructive framework designed to provide a rigorous logical foundation for autonomous and executable knowledge systems. Traditional knowledge representation models often suffer from a gap between static symbolic logic and dynamic system execution. To bridge this divide, KOS-TL leverages Dependent Type Theory to unify data, logic, and proof into a singular computational substrate.The architecture of KOS-TL is organized into three hierarchical layers: the Core Layer, which defines the static type universe and constructive primitives; the Kernel Layer, which governs state evolution through an event-driven mechanism characterized by the triple $\\langle \u03a3, \\textsf{Ev}, \u0394\\rangle$; and the Runtime Layer, responsible for the bidirectional refinement of physical signals into logical evidence. We formally define the operational semantics of the system and prove key meta-theoretical properties, including Progress and Evolutionary Consistency, ensuring that the system remains logically self-consistent and free from stuck states during continuous state transitions.By integrating Davidsonian event semantics with Martin-L\u00f6f type theory, KOS-TL enables the construction of \"proof-carrying knowledge,\" where every state change in the knowledge base is accompanied by a formal witness of its validity. We demonstrate the practical utility of this logic through application examples in industrial traceability and cross-border financial compliance. Our results suggest that KOS-TL provides a robust, formally verifiable basis for the next generation of intelligent, autonomous operating systems.", "AI": {"tldr": "KOS-TL\u662f\u4e00\u4e2a\u57fa\u4e8e\u4f9d\u8d56\u7c7b\u578b\u7406\u8bba\u7684\u6784\u9020\u6027\u6846\u67b6\uff0c\u65e8\u5728\u4e3a\u81ea\u4e3b\u53ef\u6267\u884c\u77e5\u8bc6\u7cfb\u7edf\u63d0\u4f9b\u4e25\u683c\u7684\u903b\u8f91\u57fa\u7840\uff0c\u901a\u8fc7\u4e09\u5c42\u67b6\u6784\u7edf\u4e00\u6570\u636e\u3001\u903b\u8f91\u548c\u8bc1\u660e\uff0c\u786e\u4fdd\u7cfb\u7edf\u5728\u72b6\u6001\u8f6c\u6362\u4e2d\u4fdd\u6301\u903b\u8f91\u81ea\u6d3d\u3002", "motivation": "\u4f20\u7edf\u77e5\u8bc6\u8868\u793a\u6a21\u578b\u5b58\u5728\u9759\u6001\u7b26\u53f7\u903b\u8f91\u4e0e\u52a8\u6001\u7cfb\u7edf\u6267\u884c\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7edf\u4e00\u6570\u636e\u3001\u903b\u8f91\u548c\u8bc1\u660e\u7684\u4e25\u683c\u903b\u8f91\u6846\u67b6\u6765\u652f\u6301\u81ea\u4e3b\u53ef\u6267\u884c\u77e5\u8bc6\u7cfb\u7edf\u3002", "method": "\u57fa\u4e8e\u4f9d\u8d56\u7c7b\u578b\u7406\u8bba\uff0c\u91c7\u7528\u4e09\u5c42\u67b6\u6784\uff1a\u6838\u5fc3\u5c42\u5b9a\u4e49\u9759\u6001\u7c7b\u578b\u5b87\u5b99\u548c\u6784\u9020\u539f\u8bed\uff1b\u5185\u6838\u5c42\u901a\u8fc7\u4e8b\u4ef6\u9a71\u52a8\u673a\u5236\u7ba1\u7406\u72b6\u6001\u6f14\u5316\uff1b\u8fd0\u884c\u65f6\u5c42\u8d1f\u8d23\u7269\u7406\u4fe1\u53f7\u4e0e\u903b\u8f91\u8bc1\u636e\u7684\u53cc\u5411\u7cbe\u5316\u3002\u6574\u5408Davidsonian\u4e8b\u4ef6\u8bed\u4e49\u4e0eMartin-L\u00f6f\u7c7b\u578b\u7406\u8bba\u3002", "result": "\u5f62\u5f0f\u5316\u5b9a\u4e49\u4e86\u7cfb\u7edf\u7684\u64cd\u4f5c\u8bed\u4e49\uff0c\u8bc1\u660e\u4e86\u5173\u952e\u5143\u7406\u8bba\u6027\u8d28\uff08\u5305\u62ec\u8fdb\u5c55\u6027\u548c\u6f14\u5316\u4e00\u81f4\u6027\uff09\uff0c\u786e\u4fdd\u7cfb\u7edf\u5728\u8fde\u7eed\u72b6\u6001\u8f6c\u6362\u4e2d\u4fdd\u6301\u903b\u8f91\u81ea\u6d3d\u4e14\u65e0\u505c\u6ede\u72b6\u6001\u3002\u901a\u8fc7\u5de5\u4e1a\u8ffd\u6eaf\u548c\u8de8\u5883\u91d1\u878d\u5408\u89c4\u7684\u5e94\u7528\u793a\u4f8b\u5c55\u793a\u4e86\u5b9e\u7528\u6027\u3002", "conclusion": "KOS-TL\u4e3a\u4e0b\u4e00\u4ee3\u667a\u80fd\u81ea\u4e3b\u64cd\u4f5c\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5f3a\u5927\u4e14\u53ef\u5f62\u5f0f\u5316\u9a8c\u8bc1\u7684\u57fa\u7840\uff0c\u80fd\u591f\u6784\u5efa\"\u643a\u5e26\u8bc1\u660e\u7684\u77e5\u8bc6\"\uff0c\u5176\u4e2d\u77e5\u8bc6\u5e93\u7684\u6bcf\u4e2a\u72b6\u6001\u53d8\u5316\u90fd\u6709\u5176\u6709\u6548\u6027\u7684\u5f62\u5f0f\u89c1\u8bc1\u3002"}}
{"id": "2601.01970", "categories": ["stat.ML", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.01970", "abs": "https://arxiv.org/abs/2601.01970", "authors": ["Ayomide Afolabi", "Ebere Ogburu", "Symon Kimitei"], "title": "A Multilayered Approach to Classifying Customer Responsiveness and Credit Risk", "comment": null, "summary": "This study evaluates the performance of various classifiers in three distinct models: response, risk, and response-risk, concerning credit card mail campaigns and default prediction. In the response model, the Extra Trees classifier demonstrates the highest recall level (79.1%), emphasizing its effectiveness in identifying potential responders to targeted credit card offers. Conversely, in the risk model, the Random Forest classifier exhibits remarkable specificity of 84.1%, crucial for identifying customers least likely to default. Furthermore, in the multi-class response-risk model, the Random Forest classifier achieves the highest accuracy (83.2%), indicating its efficacy in discerning both potential responders to credit card mail campaign and low-risk credit card users. In this study, we optimized various performance metrics to solve a specific credit risk and mail responsiveness business problem.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u4e09\u79cd\u6a21\u578b\uff08\u54cd\u5e94\u3001\u98ce\u9669\u3001\u54cd\u5e94-\u98ce\u9669\uff09\u4e2d\u4e0d\u540c\u5206\u7c7b\u5668\u5728\u4fe1\u7528\u5361\u90ae\u4ef6\u8425\u9500\u548c\u8fdd\u7ea6\u9884\u6d4b\u4e2d\u7684\u6027\u80fd\uff0c\u53d1\u73b0Extra Trees\u5728\u54cd\u5e94\u6a21\u578b\u4e2d\u53ec\u56de\u7387\u6700\u9ad8\uff0cRandom Forest\u5728\u98ce\u9669\u6a21\u578b\u4e2d\u7279\u5f02\u6027\u6700\u4f73\uff0c\u5728\u54cd\u5e94-\u98ce\u9669\u591a\u5206\u7c7b\u6a21\u578b\u4e2d\u51c6\u786e\u7387\u6700\u9ad8\u3002", "motivation": "\u89e3\u51b3\u4fe1\u7528\u5361\u4e1a\u52a1\u4e2d\u7684\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a1\uff09\u8bc6\u522b\u5bf9\u90ae\u4ef6\u8425\u9500\u6d3b\u52a8\u53ef\u80fd\u54cd\u5e94\u7684\u5ba2\u6237\uff08\u54cd\u5e94\u6a21\u578b\uff09\uff1b2\uff09\u9884\u6d4b\u5ba2\u6237\u8fdd\u7ea6\u98ce\u9669\uff08\u98ce\u9669\u6a21\u578b\uff09\uff1b3\uff09\u540c\u65f6\u8003\u8651\u54cd\u5e94\u6027\u548c\u98ce\u9669\u6027\u7684\u7efc\u5408\u8bc4\u4f30\uff08\u54cd\u5e94-\u98ce\u9669\u6a21\u578b\uff09\uff0c\u4ee5\u4f18\u5316\u4fe1\u7528\u5361\u8425\u9500\u7b56\u7565\u548c\u98ce\u9669\u7ba1\u7406\u3002", "method": "\u4f7f\u7528\u591a\u79cd\u5206\u7c7b\u5668\uff08\u5305\u62ecExtra Trees\u548cRandom Forest\u7b49\uff09\u5728\u4e09\u4e2a\u4e0d\u540c\u6a21\u578b\u4e2d\u8fdb\u884c\u8bc4\u4f30\uff1a\u54cd\u5e94\u6a21\u578b\uff08\u9884\u6d4b\u5ba2\u6237\u5bf9\u4fe1\u7528\u5361\u90ae\u4ef6\u8425\u9500\u7684\u54cd\u5e94\uff09\u3001\u98ce\u9669\u6a21\u578b\uff08\u9884\u6d4b\u5ba2\u6237\u8fdd\u7ea6\u98ce\u9669\uff09\u3001\u54cd\u5e94-\u98ce\u9669\u591a\u5206\u7c7b\u6a21\u578b\uff08\u540c\u65f6\u8003\u8651\u54cd\u5e94\u6027\u548c\u98ce\u9669\u6027\uff09\u3002\u901a\u8fc7\u4f18\u5316\u5404\u79cd\u6027\u80fd\u6307\u6807\u6765\u89e3\u51b3\u5177\u4f53\u7684\u4fe1\u7528\u98ce\u9669\u548c\u90ae\u4ef6\u54cd\u5e94\u4e1a\u52a1\u95ee\u9898\u3002", "result": "1\uff09\u54cd\u5e94\u6a21\u578b\u4e2d\uff1aExtra Trees\u5206\u7c7b\u5668\u83b7\u5f97\u6700\u9ad8\u53ec\u56de\u738779.1%\uff0c\u80fd\u6709\u6548\u8bc6\u522b\u6f5c\u5728\u54cd\u5e94\u8005\uff1b2\uff09\u98ce\u9669\u6a21\u578b\u4e2d\uff1aRandom Forest\u5206\u7c7b\u5668\u7279\u5f02\u6027\u8fbe84.1%\uff0c\u80fd\u51c6\u786e\u8bc6\u522b\u4f4e\u8fdd\u7ea6\u98ce\u9669\u5ba2\u6237\uff1b3\uff09\u54cd\u5e94-\u98ce\u9669\u591a\u5206\u7c7b\u6a21\u578b\u4e2d\uff1aRandom Forest\u5206\u7c7b\u5668\u51c6\u786e\u7387\u6700\u9ad8\u8fbe83.2%\uff0c\u80fd\u540c\u65f6\u6709\u6548\u8bc6\u522b\u6f5c\u5728\u54cd\u5e94\u8005\u548c\u4f4e\u98ce\u9669\u7528\u6237\u3002", "conclusion": "\u4e0d\u540c\u5206\u7c7b\u5668\u5728\u4e0d\u540c\u4e1a\u52a1\u573a\u666f\u4e0b\u8868\u73b0\u5404\u5f02\uff1aExtra Trees\u5728\u8bc6\u522b\u90ae\u4ef6\u8425\u9500\u54cd\u5e94\u8005\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0cRandom Forest\u5728\u98ce\u9669\u8bc6\u522b\u548c\u7efc\u5408\u8bc4\u4f30\u4e2d\u8868\u73b0\u6700\u4f18\u3002\u7814\u7a76\u4e3a\u4fe1\u7528\u5361\u4e1a\u52a1\u63d0\u4f9b\u4e86\u9488\u5bf9\u6027\u7684\u673a\u5668\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u6839\u636e\u5177\u4f53\u4e1a\u52a1\u9700\u6c42\u9009\u62e9\u5408\u9002\u7684\u5206\u7c7b\u5668\u6a21\u578b\u3002"}}
{"id": "2601.00863", "categories": ["cs.LG", "cond-mat.dis-nn", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2601.00863", "abs": "https://arxiv.org/abs/2601.00863", "authors": ["Markus J. Buehler"], "title": "Selective Imperfection as a Generative Framework for Analysis, Creativity and Discovery", "comment": null, "summary": "We introduce materiomusic as a generative framework linking the hierarchical structures of matter with the compositional logic of music. Across proteins, spider webs and flame dynamics, vibrational and architectural principles recur as tonal hierarchies, harmonic progressions, and long-range musical form. Using reversible mappings, from molecular spectra to musical tones and from three-dimensional networks to playable instruments, we show how sound functions as a scientific probe, an epistemic inversion where listening becomes a mode of seeing and musical composition becomes a blueprint for matter. These mappings excavate deep time: patterns originating in femtosecond molecular vibrations or billion-year evolutionary histories become audible. We posit that novelty in science and art emerges when constraints cannot be satisfied within existing degrees of freedom, forcing expansion of the space of viable configurations. Selective imperfection provides the mechanism restoring balance between coherence and adaptability. Quantitative support comes from exhaustive enumeration of all 2^12 musical scales, revealing that culturally significant systems cluster in a mid-entropy, mid-defect corridor, directly paralleling the Hall-Petch optimum where intermediate defect densities maximize material strength. Iterating these mappings creates productive collisions between human creativity and physics, generating new information as musical structures encounter evolutionary constraints. We show how swarm-based AI models compose music exhibiting human-like structural signatures such as small-world connectivity, modular integration, long-range coherence, suggesting a route beyond interpolation toward invention. We show that science and art are generative acts of world-building under constraint, with vibration as a shared grammar organizing structure across scales.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51famateriomusic\u751f\u6210\u6846\u67b6\uff0c\u5c06\u7269\u8d28\u7684\u5c42\u6b21\u7ed3\u6784\u4e0e\u97f3\u4e50\u521b\u4f5c\u903b\u8f91\u76f8\u8fde\u63a5\uff0c\u901a\u8fc7\u53ef\u9006\u6620\u5c04\u5c06\u5206\u5b50\u632f\u52a8\u3001\u8718\u86db\u7f51\u7ed3\u6784\u7b49\u7269\u7406\u6a21\u5f0f\u8f6c\u5316\u4e3a\u97f3\u4e50\u5143\u7d20\uff0c\u63a2\u7d22\u79d1\u5b66\u4e0e\u827a\u672f\u5728\u7ea6\u675f\u4e0b\u7684\u751f\u6210\u5171\u6027\u3002", "motivation": "\u63a2\u7d22\u7269\u8d28\u7ed3\u6784\u4e0e\u97f3\u4e50\u521b\u4f5c\u4e4b\u95f4\u7684\u6df1\u5c42\u8054\u7cfb\uff0c\u5efa\u7acb\u79d1\u5b66\u4e0e\u827a\u672f\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u58f0\u97f3\u4f5c\u4e3a\u79d1\u5b66\u63a2\u9488\u6765\u7406\u89e3\u7269\u7406\u73b0\u8c61\uff0c\u540c\u65f6\u5229\u7528\u7269\u7406\u7ea6\u675f\u542f\u53d1\u827a\u672f\u521b\u65b0\u3002", "method": "\u4f7f\u7528\u53ef\u9006\u6620\u5c04\u65b9\u6cd5\uff1a1) \u5206\u5b50\u5149\u8c31\u6620\u5c04\u5230\u97f3\u8c03\uff1b2) \u4e09\u7ef4\u7f51\u7edc\u7ed3\u6784\u6620\u5c04\u5230\u53ef\u6f14\u594f\u4e50\u5668\uff1b3) \u679a\u4e3e\u6240\u67092^12\u97f3\u4e50\u97f3\u9636\u5206\u6790\u6587\u5316\u663e\u8457\u6027\uff1b4) \u91c7\u7528\u7fa4\u4f53\u667a\u80fdAI\u6a21\u578b\u4f5c\u66f2\uff1b5) \u5e94\u7528\u9009\u62e9\u6027\u4e0d\u5b8c\u7f8e\u673a\u5236\u5e73\u8861\u8fde\u8d2f\u6027\u4e0e\u9002\u5e94\u6027\u3002", "result": "\u53d1\u73b0\u6587\u5316\u663e\u8457\u7684\u97f3\u4e50\u7cfb\u7edf\u805a\u96c6\u5728\u4e2d\u71b5\u3001\u4e2d\u7f3a\u9677\u7684\u8d70\u5eca\u533a\u57df\uff0c\u4e0e\u6750\u6599\u79d1\u5b66\u4e2d\u7684Hall-Petch\u6700\u4f18\u7f3a\u9677\u5bc6\u5ea6\u76f4\u63a5\u5bf9\u5e94\uff1bAI\u751f\u6210\u7684\u97f3\u4e50\u8868\u73b0\u51fa\u7c7b\u4eba\u7684\u5c0f\u4e16\u754c\u8fde\u63a5\u6027\u3001\u6a21\u5757\u5316\u6574\u5408\u548c\u957f\u7a0b\u8fde\u8d2f\u6027\u7b49\u7ed3\u6784\u7279\u5f81\u3002", "conclusion": "\u79d1\u5b66\u4e0e\u827a\u672f\u90fd\u662f\u5728\u7ea6\u675f\u4e0b\u7684\u4e16\u754c\u6784\u5efa\u751f\u6210\u884c\u4e3a\uff0c\u632f\u52a8\u4f5c\u4e3a\u8de8\u5c3a\u5ea6\u7684\u5171\u4eab\u8bed\u6cd5\u7ec4\u7ec7\u7ed3\u6784\uff1b\u9009\u62e9\u6027\u4e0d\u5b8c\u7f8e\u662f\u5e73\u8861\u8fde\u8d2f\u6027\u4e0e\u9002\u5e94\u6027\u7684\u673a\u5236\uff1b\u97f3\u4e50\u4e0e\u7269\u8d28\u7684\u6620\u5c04\u521b\u9020\u4e86\u4eba\u7c7b\u521b\u9020\u529b\u4e0e\u7269\u7406\u5b66\u7684\u751f\u4ea7\u6027\u78b0\u649e\u3002"}}
{"id": "2601.01691", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.01691", "abs": "https://arxiv.org/abs/2601.01691", "authors": ["Hyuntae Kim", "Idris Kempf"], "title": "Cross-Directional Modelling and Control of Slot-Die Battery Electrode Coating", "comment": null, "summary": "As global battery demand increases, real-time process control becomes increasingly important for battery electrode manufacturing, yet slot-die lines are still mostly manually operated in open loop. This paper develops a physics-based modelling-and-control pipeline for film-thickness regulation. Computational fluid dynamics (CFD) simulations provide the data from which a low-order cross-directional model is identified and calibrated. Numerical simulations demonstrate close agreement between the CFD and the cross-directional model, which is used to design a controller that can be used in both real-time, automated feedback operation and manual feedforward operation during line commissioning.", "AI": {"tldr": "\u5f00\u53d1\u57fa\u4e8e\u7269\u7406\u5efa\u6a21\u7684\u7535\u6c60\u7535\u6781\u5236\u9020\u8584\u819c\u539a\u5ea6\u63a7\u5236\u7ba1\u9053\uff0c\u901a\u8fc7CFD\u4eff\u771f\u83b7\u53d6\u6570\u636e\u5efa\u7acb\u4f4e\u9636\u6a21\u578b\uff0c\u8bbe\u8ba1\u53ef\u7528\u4e8e\u5b9e\u65f6\u81ea\u52a8\u53cd\u9988\u548c\u624b\u52a8\u524d\u9988\u64cd\u4f5c\u7684\u63a7\u5236\u5668", "motivation": "\u968f\u7740\u5168\u7403\u7535\u6c60\u9700\u6c42\u589e\u957f\uff0c\u7535\u6c60\u7535\u6781\u5236\u9020\u9700\u8981\u5b9e\u65f6\u8fc7\u7a0b\u63a7\u5236\uff0c\u4f46\u76ee\u524d\u72ed\u7f1d\u6d82\u5e03\u7ebf\u5927\u591a\u4ecd\u91c7\u7528\u5f00\u73af\u624b\u52a8\u64cd\u4f5c\uff0c\u7f3a\u4e4f\u81ea\u52a8\u5316\u63a7\u5236", "method": "\u57fa\u4e8e\u8ba1\u7b97\u6d41\u4f53\u52a8\u529b\u5b66(CFD)\u4eff\u771f\u83b7\u53d6\u6570\u636e\uff0c\u8bc6\u522b\u548c\u6821\u51c6\u4f4e\u9636\u6a2a\u5411\u6a21\u578b\uff0c\u8bbe\u8ba1\u63a7\u5236\u5668\u652f\u6301\u5b9e\u65f6\u81ea\u52a8\u53cd\u9988\u548c\u624b\u52a8\u524d\u9988\u64cd\u4f5c", "result": "\u6570\u503c\u4eff\u771f\u663e\u793aCFD\u6a21\u578b\u4e0e\u4f4e\u9636\u6a2a\u5411\u6a21\u578b\u9ad8\u5ea6\u4e00\u81f4\uff0c\u5f00\u53d1\u7684\u63a7\u5236\u5668\u53ef\u7528\u4e8e\u751f\u4ea7\u7ebf\u8c03\u8bd5\u548c\u5b9e\u65f6\u81ea\u52a8\u5316\u64cd\u4f5c", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7535\u6c60\u7535\u6781\u5236\u9020\u4e2d\u7684\u8584\u819c\u539a\u5ea6\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7269\u7406\u5efa\u6a21\u4e0e\u63a7\u5236\u7ba1\u9053\uff0c\u80fd\u591f\u5b9e\u73b0\u4ece\u624b\u52a8\u64cd\u4f5c\u5411\u81ea\u52a8\u5316\u5b9e\u65f6\u63a7\u5236\u7684\u8f6c\u53d8"}}
{"id": "2601.01683", "categories": ["math.OC", "math.DS"], "pdf": "https://arxiv.org/pdf/2601.01683", "abs": "https://arxiv.org/abs/2601.01683", "authors": ["Parham Oveissi", "Ryan DeBoskey", "Venkateswaran Narayanaswamy", "Ankit Goel"], "title": "Adaptive Thrust Regulation in Solid-fuel Ramjet with Variable Geometry Inlet", "comment": null, "summary": "This paper presents the application of a novel data-driven adaptive control technique, dynamic mode adaptive control (DMAC), to regulate thrust in a solid-fuel ramjet (SFRJ). A quasi-static one-dimensional model of SFRJ with a variable geometry inlet is developed to compute thrust. An adaptive tracking controller is then designed using the DMAC framework, which leverages dynamic mode decomposition to approximate the local system behavior, followed by a tracking controller designed around the identified model. Simulation results demonstrate that DMAC achieves accurate thrust regulation across a range of commanded profiles and operating conditions, without requiring an analytical model of the SFRJ. These findings indicate that DMAC provides a reliable and effective approach for model-free thrust regulation in an SFRJ with variable-geometry inlets as the control input.", "AI": {"tldr": "\u63d0\u51fa\u52a8\u6001\u6a21\u6001\u81ea\u9002\u5e94\u63a7\u5236(DMAC)\u65b9\u6cd5\uff0c\u7528\u4e8e\u56fa\u4f53\u71c3\u6599\u51b2\u538b\u53d1\u52a8\u673a(SFRJ)\u7684\u63a8\u529b\u8c03\u8282\uff0c\u65e0\u9700\u7cfb\u7edf\u89e3\u6790\u6a21\u578b\uff0c\u901a\u8fc7\u53ef\u53d8\u51e0\u4f55\u8fdb\u6c14\u9053\u5b9e\u73b0\u63a8\u529b\u63a7\u5236\u3002", "motivation": "\u56fa\u4f53\u71c3\u6599\u51b2\u538b\u53d1\u52a8\u673a\u63a8\u529b\u8c03\u8282\u5177\u6709\u6311\u6218\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u7cbe\u786e\u7684\u89e3\u6790\u6a21\u578b\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u65e0\u9700\u89e3\u6790\u6a21\u578b\u7684\u6570\u636e\u9a71\u52a8\u81ea\u9002\u5e94\u63a7\u5236\u65b9\u6cd5\uff0c\u5b9e\u73b0SFRJ\u7684\u63a8\u529b\u7cbe\u786e\u8c03\u8282\u3002", "method": "1) \u5efa\u7acbSFRJ\u51c6\u9759\u6001\u4e00\u7ef4\u6a21\u578b\u8ba1\u7b97\u63a8\u529b\uff1b2) \u91c7\u7528\u52a8\u6001\u6a21\u6001\u81ea\u9002\u5e94\u63a7\u5236(DMAC)\u6846\u67b6\uff1a\u5229\u7528\u52a8\u6001\u6a21\u6001\u5206\u89e3\u8fd1\u4f3c\u5c40\u90e8\u7cfb\u7edf\u884c\u4e3a\uff0c\u57fa\u4e8e\u8bc6\u522b\u6a21\u578b\u8bbe\u8ba1\u8ddf\u8e2a\u63a7\u5236\u5668\uff1b3) \u4ee5\u53ef\u53d8\u51e0\u4f55\u8fdb\u6c14\u9053\u4f5c\u4e3a\u63a7\u5236\u8f93\u5165\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cDMAC\u80fd\u5728\u591a\u79cd\u6307\u4ee4\u66f2\u7ebf\u548c\u64cd\u4f5c\u6761\u4ef6\u4e0b\u5b9e\u73b0\u7cbe\u786e\u7684\u63a8\u529b\u8c03\u8282\uff0c\u65e0\u9700SFRJ\u7684\u89e3\u6790\u6a21\u578b\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u53ef\u9760\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "DMAC\u4e3a\u5177\u6709\u53ef\u53d8\u51e0\u4f55\u8fdb\u6c14\u9053\u7684SFRJ\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u9760\u6709\u6548\u7684\u65e0\u6a21\u578b\u63a8\u529b\u8c03\u8282\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u6570\u636e\u9a71\u52a8\u81ea\u9002\u5e94\u63a7\u5236\u5728\u590d\u6742\u63a8\u8fdb\u7cfb\u7edf\u63a7\u5236\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2601.00880", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.00880", "abs": "https://arxiv.org/abs/2601.00880", "authors": ["Anthony Mikinka"], "title": "Universal Conditional Logic: A Formal Language for Prompt Engineering", "comment": "25 pages, 15 figures, 5 tables. Includes appendices with variable reference, pattern library, and O_s calculation examples. Supplementary materials: V1-V4.1 prompt source code and 305 model responses available at GitHub repositories", "summary": "We present Universal Conditional Logic (UCL), a mathematical framework for prompt optimization that transforms prompt engineering from heuristic practice into systematic optimization. Through systematic evaluation (N=305, 11 models, 4 iterations), we demonstrate significant token reduction (29.8%, t(10)=6.36, p < 0.001, Cohen's d = 2.01) with corresponding cost savings. UCL's structural overhead function O_s(A) explains version-specific performance differences through the Over-Specification Paradox: beyond threshold S* = 0.509, additional specification degrades performance quadratically. Core mechanisms -- indicator functions (I_i in {0,1}), structural overhead (O_s = gamma * sum(ln C_k)), early binding -- are validated. Notably, optimal UCL configuration varies by model architecture -- certain models (e.g., Llama 4 Scout) require version-specific adaptations (V4.1). This work establishes UCL as a calibratable framework for efficient LLM interaction, with model-family-specific optimization as a key research direction.", "AI": {"tldr": "UCL\u662f\u4e00\u4e2a\u5c06\u63d0\u793a\u5de5\u7a0b\u4ece\u542f\u53d1\u5f0f\u5b9e\u8df5\u8f6c\u53d8\u4e3a\u7cfb\u7edf\u5316\u4f18\u5316\u7684\u6570\u5b66\u6846\u67b6\uff0c\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30\u663e\u793a\u80fd\u663e\u8457\u51cf\u5c11token\u4f7f\u7528\uff0829.8%\uff09\uff0c\u5176\u7ed3\u6784\u5f00\u9500\u51fd\u6570\u63ed\u793a\u4e86\u8fc7\u5ea6\u6307\u5b9a\u6096\u8bba\uff0c\u6838\u5fc3\u673a\u5236\u5f97\u5230\u9a8c\u8bc1\uff0c\u4e14\u6700\u4f18\u914d\u7f6e\u56e0\u6a21\u578b\u67b6\u6784\u800c\u5f02\u3002", "motivation": "\u5f53\u524d\u63d0\u793a\u5de5\u7a0b\u4e3b\u8981\u4f9d\u8d56\u542f\u53d1\u5f0f\u5b9e\u8df5\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u5316\u7684\u4f18\u5316\u6846\u67b6\u3002\u4f5c\u8005\u5e0c\u671b\u5efa\u7acb\u4e00\u4e2a\u6570\u5b66\u6846\u67b6\uff0c\u5c06\u63d0\u793a\u5de5\u7a0b\u8f6c\u53d8\u4e3a\u53ef\u7cfb\u7edf\u4f18\u5316\u7684\u8fc7\u7a0b\uff0c\u63d0\u9ad8LLM\u4ea4\u4e92\u7684\u6548\u7387\u5e76\u964d\u4f4e\u6210\u672c\u3002", "method": "\u63d0\u51fa\u4e86\u901a\u7528\u6761\u4ef6\u903b\u8f91\uff08UCL\uff09\u6846\u67b6\uff0c\u5305\u542b\u6307\u793a\u51fd\u6570\u3001\u7ed3\u6784\u5f00\u9500\u51fd\u6570\u3001\u65e9\u671f\u7ed1\u5b9a\u7b49\u6838\u5fc3\u673a\u5236\u3002\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30\uff08N=305\uff0c11\u4e2a\u6a21\u578b\uff0c4\u6b21\u8fed\u4ee3\uff09\u9a8c\u8bc1\u6846\u67b6\u6548\u679c\uff0c\u5e76\u5f15\u5165\u7ed3\u6784\u5f00\u9500\u51fd\u6570O_s(A)\u6765\u89e3\u91ca\u7248\u672c\u7279\u5b9a\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u7279\u522b\u662f\u8fc7\u5ea6\u6307\u5b9a\u6096\u8bba\u73b0\u8c61\u3002", "result": "\u663e\u8457\u51cf\u5c11\u4e86token\u4f7f\u7528\uff0829.8%\uff09\uff0c\u5177\u6709\u7edf\u8ba1\u5b66\u610f\u4e49\uff08t(10)=6.36, p < 0.001, Cohen's d = 2.01\uff09\uff0c\u5e26\u6765\u76f8\u5e94\u7684\u6210\u672c\u8282\u7ea6\u3002\u9a8c\u8bc1\u4e86UCL\u7684\u6838\u5fc3\u673a\u5236\uff0c\u53d1\u73b0\u6700\u4f18UCL\u914d\u7f6e\u56e0\u6a21\u578b\u67b6\u6784\u800c\u5f02\uff0c\u67d0\u4e9b\u6a21\u578b\u9700\u8981\u7248\u672c\u7279\u5b9a\u7684\u9002\u914d\u3002", "conclusion": "UCL\u4f5c\u4e3a\u4e00\u4e2a\u53ef\u6821\u51c6\u7684\u6846\u67b6\uff0c\u4e3a\u9ad8\u6548\u7684LLM\u4ea4\u4e92\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u65b9\u6cd5\u3002\u6a21\u578b\u5bb6\u65cf\u7279\u5b9a\u7684\u4f18\u5316\u662f\u672a\u6765\u5173\u952e\u7814\u7a76\u65b9\u5411\uff0c\u6846\u67b6\u7684\u6210\u529f\u5e94\u7528\u8868\u660e\u53ef\u4ee5\u5c06\u63d0\u793a\u5de5\u7a0b\u4ece\u542f\u53d1\u5f0f\u5b9e\u8df5\u8f6c\u53d8\u4e3a\u57fa\u4e8e\u6570\u5b66\u7684\u7cfb\u7edf\u4f18\u5316\u3002"}}
{"id": "2601.01153", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01153", "abs": "https://arxiv.org/abs/2601.01153", "authors": ["Jiani Guo", "Jiajia Li", "Jie Wu", "Zuchao Li", "Yujiu Yang", "Ping Wang"], "title": "SongSage: A Large Musical Language Model with Lyric Generative Pre-training", "comment": null, "summary": "Large language models have achieved significant success in various domains, yet their understanding of lyric-centric knowledge has not been fully explored. In this work, we first introduce PlaylistSense, a dataset to evaluate the playlist understanding capability of language models. PlaylistSense encompasses ten types of user queries derived from common real-world perspectives, challenging LLMs to accurately grasp playlist features and address diverse user intents. Comprehensive evaluations indicate that current general-purpose LLMs still have potential for improvement in playlist understanding. Inspired by this, we introduce SongSage, a large musical language model equipped with diverse lyric-centric intelligence through lyric generative pretraining. SongSage undergoes continual pretraining on LyricBank, a carefully curated corpus of 5.48 billion tokens focused on lyrical content, followed by fine-tuning with LyricBank-SFT, a meticulously crafted instruction set comprising 775k samples across nine core lyric-centric tasks. Experimental results demonstrate that SongSage exhibits a strong understanding of lyric-centric knowledge, excels in rewriting user queries for zero-shot playlist recommendations, generates and continues lyrics effectively, and performs proficiently across seven additional capabilities. Beyond its lyric-centric expertise, SongSage also retains general knowledge comprehension and achieves a competitive MMLU score. We will keep the datasets inaccessible due to copyright restrictions and release the SongSage and training script to ensure reproducibility and support music AI research and applications, the datasets release plan details are provided in the appendix.", "AI": {"tldr": "SongSage\u662f\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u6b4c\u8bcd\u7406\u89e3\u7684\u5927\u578b\u97f3\u4e50\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u6b4c\u8bcd\u751f\u6210\u9884\u8bad\u7ec3\u83b7\u5f97\u591a\u6837\u5316\u7684\u6b4c\u8bcd\u667a\u80fd\uff0c\u5728\u6b4c\u8bcd\u76f8\u5173\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u4fdd\u6301\u901a\u7528\u77e5\u8bc6\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u4e2a\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u5b83\u4eec\u5728\u6b4c\u8bcd\u4e2d\u5fc3\u77e5\u8bc6\u7406\u89e3\u65b9\u9762\u7684\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u5f53\u524d\u901a\u7528LLMs\u5728\u64ad\u653e\u5217\u8868\u7406\u89e3\u65b9\u9762\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "method": "\u9996\u5148\u5f15\u5165PlaylistSense\u6570\u636e\u96c6\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u7684\u64ad\u653e\u5217\u8868\u7406\u89e3\u80fd\u529b\u3002\u7136\u540e\u5f00\u53d1SongSage\u6a21\u578b\uff1a1\uff09\u5728LyricBank\uff0854.8\u4ebf\u6b4c\u8bcd\u6807\u8bb0\u8bed\u6599\u5e93\uff09\u4e0a\u8fdb\u884c\u6301\u7eed\u9884\u8bad\u7ec3\uff1b2\uff09\u4f7f\u7528LyricBank-SFT\uff0877.5\u4e07\u6837\u672c\uff0c\u6db5\u76d69\u4e2a\u6838\u5fc3\u6b4c\u8bcd\u4efb\u52a1\uff09\u8fdb\u884c\u6307\u4ee4\u5fae\u8c03\u3002", "result": "SongSage\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6b4c\u8bcd\u4e2d\u5fc3\u77e5\u8bc6\u7406\u89e3\u80fd\u529b\uff0c\u5728\u96f6\u6837\u672c\u64ad\u653e\u5217\u8868\u63a8\u8350\u67e5\u8be2\u91cd\u5199\u3001\u6b4c\u8bcd\u751f\u6210\u548c\u7eed\u5199\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u53e6\u59167\u9879\u80fd\u529b\u4e0a\u4e5f\u8868\u73b0\u719f\u7ec3\u3002\u540c\u65f6\u4fdd\u6301\u901a\u7528\u77e5\u8bc6\u7406\u89e3\uff0c\u83b7\u5f97\u6709\u7ade\u4e89\u529b\u7684MMLU\u5206\u6570\u3002", "conclusion": "SongSage\u662f\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u6b4c\u8bcd\u7406\u89e3\u8bad\u7ec3\u7684\u5927\u578b\u97f3\u4e50\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u6b4c\u8bcd\u76f8\u5173\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u4fdd\u6301\u901a\u7528\u80fd\u529b\u3002\u7531\u4e8e\u7248\u6743\u9650\u5236\uff0c\u6570\u636e\u96c6\u4e0d\u516c\u5f00\uff0c\u4f46\u5c06\u53d1\u5e03SongSage\u6a21\u578b\u548c\u8bad\u7ec3\u811a\u672c\u4ee5\u652f\u6301\u97f3\u4e50AI\u7814\u7a76\u3002"}}
{"id": "2601.02241", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02241", "abs": "https://arxiv.org/abs/2601.02241", "authors": ["Svenja Jedhoff", "Elizaveta Semenova", "Aura Raulo", "Anne Meyer", "Paul-Christian B\u00fcrkner"], "title": "From Mice to Trains: Amortized Bayesian Inference on Graph Data", "comment": null, "summary": "Graphs arise across diverse domains, from biology and chemistry to social and information networks, as well as in transportation and logistics. Inference on graph-structured data requires methods that are permutation-invariant, scalable across varying sizes and sparsities, and capable of capturing complex long-range dependencies, making posterior estimation on graph parameters particularly challenging. Amortized Bayesian Inference (ABI) is a simulation-based framework that employs generative neural networks to enable fast, likelihood-free posterior inference. We adapt ABI to graph data to address these challenges to perform inference on node-, edge-, and graph-level parameters. Our approach couples permutation-invariant graph encoders with flexible neural posterior estimators in a two-module pipeline: a summary network maps attributed graphs to fixed-length representations, and an inference network approximates the posterior over parameters. In this setting, several neural architectures can serve as the summary network. In this work we evaluate multiple architectures and assess their performance on controlled synthetic settings and two real-world domains - biology and logistics - in terms of recovery and calibration.", "AI": {"tldr": "\u5c06\u644a\u9500\u8d1d\u53f6\u65af\u63a8\u65ad(ABI)\u5e94\u7528\u4e8e\u56fe\u6570\u636e\uff0c\u4f7f\u7528\u7f6e\u6362\u4e0d\u53d8\u7684\u56fe\u7f16\u7801\u5668\u548c\u795e\u7ecf\u540e\u9a8c\u4f30\u8ba1\u5668\u8fdb\u884c\u8282\u70b9\u3001\u8fb9\u548c\u56fe\u7ea7\u53c2\u6570\u63a8\u65ad", "motivation": "\u56fe\u6570\u636e\u5728\u591a\u4e2a\u9886\u57df\u666e\u904d\u5b58\u5728\uff0c\u4f46\u56fe\u53c2\u6570\u7684\u540e\u9a8c\u4f30\u8ba1\u9762\u4e34\u7f6e\u6362\u4e0d\u53d8\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u6355\u83b7\u957f\u7a0b\u4f9d\u8d56\u7b49\u6311\u6218\uff0c\u9700\u8981\u65b0\u7684\u63a8\u65ad\u65b9\u6cd5", "method": "\u91c7\u7528\u4e24\u6a21\u5757\u6d41\u6c34\u7ebf\uff1a1)\u6458\u8981\u7f51\u7edc\u5c06\u5c5e\u6027\u56fe\u6620\u5c04\u4e3a\u56fa\u5b9a\u957f\u5ea6\u8868\u793a\uff1b2)\u63a8\u65ad\u7f51\u7edc\u8fd1\u4f3c\u53c2\u6570\u540e\u9a8c\u5206\u5e03\u3002\u8bc4\u4f30\u591a\u79cd\u795e\u7ecf\u67b6\u6784\u4f5c\u4e3a\u6458\u8981\u7f51\u7edc", "result": "\u5728\u53d7\u63a7\u5408\u6210\u8bbe\u7f6e\u548c\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u9886\u57df\uff08\u751f\u7269\u5b66\u548c\u7269\u6d41\uff09\u4e2d\u8bc4\u4f30\u4e86\u591a\u79cd\u67b6\u6784\u7684\u6027\u80fd\uff0c\u5305\u62ec\u6062\u590d\u80fd\u529b\u548c\u6821\u51c6\u5ea6", "conclusion": "\u5c06\u644a\u9500\u8d1d\u53f6\u65af\u63a8\u65ad\u9002\u914d\u5230\u56fe\u6570\u636e\u662f\u53ef\u884c\u7684\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u56fe\u53c2\u6570\u63a8\u65ad\u7684\u6311\u6218\uff0c\u4e3a\u56fe\u7ed3\u6784\u6570\u636e\u7684\u540e\u9a8c\u4f30\u8ba1\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5"}}
{"id": "2601.00864", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.00864", "abs": "https://arxiv.org/abs/2601.00864", "authors": ["Clemens Damke", "Eyke H\u00fcllermeier"], "title": "Distribution Matching for Graph Quantification Under Structural Covariate Shift", "comment": "17 pages, presented at ECML-PKDD 2025", "summary": "Graphs are commonly used in machine learning to model relationships between instances. Consider the task of predicting the political preferences of users in a social network; to solve this task one should consider, both, the features of each individual user and the relationships between them. However, oftentimes one is not interested in the label of a single instance but rather in the distribution of labels over a set of instances; e.g., when predicting the political preferences of users, the overall prevalence of a given opinion might be of higher interest than the opinion of a specific person. This label prevalence estimation task is commonly referred to as quantification learning (QL). Current QL methods for tabular data are typically based on the so-called prior probability shift (PPS) assumption which states that the label-conditional instance distributions should remain equal across the training and test data. In the graph setting, PPS generally does not hold if the shift between training and test data is structural, i.e., if the training data comes from a different region of the graph than the test data. To address such structural shifts, an importance sampling variant of the popular adjusted count quantification approach has previously been proposed. In this work, we extend the idea of structural importance sampling to the state-of-the-art KDEy quantification approach. We show that our proposed method adapts to structural shifts and outperforms standard quantification approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u56fe\u6570\u636e\u7684\u91cf\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u91cd\u8981\u6027\u91c7\u6837\u6269\u5c55KDEy\u65b9\u6cd5\uff0c\u4ee5\u5904\u7406\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\u4e4b\u95f4\u7684\u7ed3\u6784\u504f\u79fb\u95ee\u9898\u3002", "motivation": "\u5728\u793e\u4ea4\u7f51\u7edc\u7b49\u56fe\u6570\u636e\u4e2d\uff0c\u4f20\u7edf\u91cf\u5316\u5b66\u4e60\u65b9\u6cd5\u57fa\u4e8e\u5148\u9a8c\u6982\u7387\u504f\u79fb\u5047\u8bbe\uff0c\u4f46\u5f53\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\u6765\u81ea\u56fe\u7684\u4e0d\u540c\u533a\u57df\u65f6\uff0c\u8fd9\u79cd\u5047\u8bbe\u4e0d\u6210\u7acb\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u9700\u8981\u4e13\u95e8\u5904\u7406\u56fe\u7ed3\u6784\u504f\u79fb\u7684\u91cf\u5316\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u5c06\u7ed3\u6784\u91cd\u8981\u6027\u91c7\u6837\u7684\u601d\u60f3\u6269\u5c55\u5230\u6700\u5148\u8fdb\u7684KDEy\u91cf\u5316\u65b9\u6cd5\u4e2d\u3002\u901a\u8fc7\u8003\u8651\u56fe\u7684\u7ed3\u6784\u7279\u6027\uff0c\u8c03\u6574\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\u4e4b\u95f4\u7684\u5206\u5e03\u5dee\u5f02\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u9002\u5e94\u56fe\u7ed3\u6784\u7684\u53d8\u5316\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u9002\u5e94\u7ed3\u6784\u504f\u79fb\uff0c\u5728\u5b58\u5728\u56fe\u7ed3\u6784\u53d8\u5316\u7684\u60c5\u51b5\u4e0b\uff0c\u6027\u80fd\u4f18\u4e8e\u6807\u51c6\u7684\u91cf\u5316\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5c06\u7ed3\u6784\u91cd\u8981\u6027\u91c7\u6837\u4e0eKDEy\u65b9\u6cd5\u7ed3\u5408\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u56fe\u6570\u636e\u91cf\u5316\u5b66\u4e60\u4e2d\u7684\u7ed3\u6784\u504f\u79fb\u95ee\u9898\uff0c\u4e3a\u56fe\u73af\u5883\u4e0b\u7684\u6807\u7b7e\u5206\u5e03\u4f30\u8ba1\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01693", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.01693", "abs": "https://arxiv.org/abs/2601.01693", "authors": ["Liam Perreault", "Idris Kempf", "Kirill Sechkar", "Jean-Baptiste Lugagne", "Antonis Papachristodoulou"], "title": "Host-Aware Control of Gene Expression using Data-Enabled Predictive Control", "comment": null, "summary": "Cybergenetic gene expression control in bacteria enables applications in engineering biology, drug development, and biomanufacturing. AI-based controllers offer new possibilities for real-time, single-cell-level regulation but typically require large datasets and re-training for new systems. Data-enabled Predictive Control (DeePC) offers better sample efficiency without prior modelling. We apply DeePC to a system with two inputs, optogenetic control and media concentration, and two outputs, expression of gene of interest and host growth rate. Using basis functions to address nonlinearities, we demonstrate that DeePC remains robust to parameter variations and performs among the best control strategies while using the least data.", "AI": {"tldr": "DeePC\u63a7\u5236\u5668\u5728\u7ec6\u83cc\u57fa\u56e0\u8868\u8fbe\u63a7\u5236\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6570\u636e\u6548\u7387\u548c\u9c81\u68d2\u6027\uff0c\u65e0\u9700\u5927\u91cf\u6570\u636e\u6216\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u9002\u5e94\u65b0\u7cfb\u7edf\u3002", "motivation": "\u4f20\u7edfAI\u63a7\u5236\u5668\u9700\u8981\u5927\u91cf\u6570\u636e\u548c\u91cd\u65b0\u8bad\u7ec3\u624d\u80fd\u9002\u5e94\u65b0\u7cfb\u7edf\uff0c\u9650\u5236\u4e86\u5176\u5728\u7ec6\u83cc\u57fa\u56e0\u8868\u8fbe\u63a7\u5236\u4e2d\u7684\u5e94\u7528\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u66f4\u7075\u6d3b\u7684\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u6570\u636e\u9a71\u52a8\u7684\u9884\u6d4b\u63a7\u5236\uff08DeePC\uff09\uff0c\u7ed3\u5408\u57fa\u51fd\u6570\u5904\u7406\u975e\u7ebf\u6027\u95ee\u9898\uff0c\u5e94\u7528\u4e8e\u5177\u6709\u4e24\u4e2a\u8f93\u5165\uff08\u5149\u9057\u4f20\u63a7\u5236\u548c\u57f9\u517b\u57fa\u6d53\u5ea6\uff09\u548c\u4e24\u4e2a\u8f93\u51fa\uff08\u76ee\u6807\u57fa\u56e0\u8868\u8fbe\u548c\u5bbf\u4e3b\u751f\u957f\u901f\u7387\uff09\u7684\u7cfb\u7edf\u3002", "result": "DeePC\u5728\u53c2\u6570\u53d8\u5316\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\uff0c\u5728\u6240\u6709\u63a7\u5236\u7b56\u7565\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u540c\u65f6\u4f7f\u7528\u6700\u5c11\u7684\u6570\u636e\u91cf\u3002", "conclusion": "DeePC\u4e3a\u7ec6\u83cc\u57fa\u56e0\u8868\u8fbe\u63a7\u5236\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u5728\u5de5\u7a0b\u751f\u7269\u5b66\u3001\u836f\u7269\u5f00\u53d1\u548c\u751f\u7269\u5236\u9020\u4e2d\u5177\u6709\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2601.01853", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2601.01853", "abs": "https://arxiv.org/abs/2601.01853", "authors": ["Ruinan Jin", "Xiaoyu Wang"], "title": "Asymptotic Convergence and Stability of Adaptive Gradient Methods in Smooth Non-convex Optimization", "comment": "32 pages", "summary": "Adaptive gradient methods, such as AdaGrad, have become fundamental tools in deep learning. Despite their widespread use, the asymptotic convergence of AdaGrad remains poorly understood in non-convex scenarios. In this work, we present the first rigorous asymptotic convergence analysis of AdaGrad-Norm for smooth non-convex optimization. Using a novel stopping-time partitioning technique, we establish a key stability result: the objective function values remain bounded in expectation, and the iterates are bounded almost surely under a mild coercivity assumption. Building on these stability results, we prove that AdaGrad-Norm achieves both almost sure and mean-square convergence. Furthermore, we extend our analysis to RMSProp and show that, with appropriate hyperparameter choices, it also enjoys stability and asymptotic convergence. The techniques developed herein may be of independent interest for analyzing other adaptive stochastic optimization algorithms.", "AI": {"tldr": "\u9996\u6b21\u5bf9AdaGrad-Norm\u5728\u975e\u51f8\u4f18\u5316\u4e2d\u7684\u6e10\u8fd1\u6536\u655b\u6027\u8fdb\u884c\u4e86\u4e25\u683c\u5206\u6790\uff0c\u8bc1\u660e\u4e86\u5176\u7a33\u5b9a\u6027\u548c\u6536\u655b\u6027\uff0c\u5e76\u5c06\u7ed3\u679c\u6269\u5c55\u5230RMSProp\u3002", "motivation": "\u5c3d\u7ba1\u81ea\u9002\u5e94\u68af\u5ea6\u65b9\u6cd5\u5982AdaGrad\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5728\u975e\u51f8\u573a\u666f\u4e0b\u7684\u6e10\u8fd1\u6536\u655b\u6027\u4ecd\u7f3a\u4e4f\u6df1\u5165\u7406\u89e3\uff0c\u9700\u8981\u5efa\u7acb\u4e25\u683c\u7684\u7406\u8bba\u57fa\u7840\u3002", "method": "\u91c7\u7528\u65b0\u9896\u7684\u505c\u65f6\u5206\u5272\u6280\u672f\uff0c\u5efa\u7acb\u7a33\u5b9a\u6027\u7ed3\u679c\uff1a\u5728\u6e29\u548c\u7684\u5f3a\u5236\u6027\u5047\u8bbe\u4e0b\uff0c\u76ee\u6807\u51fd\u6570\u503c\u5728\u671f\u671b\u4e0a\u6709\u754c\uff0c\u8fed\u4ee3\u70b9\u51e0\u4e4e\u5fc5\u7136\u6709\u754c\u3002\u57fa\u4e8e\u8fd9\u4e9b\u7a33\u5b9a\u6027\u7ed3\u679c\uff0c\u8bc1\u660eAdaGrad-Norm\u7684\u6536\u655b\u6027\u3002", "result": "\u8bc1\u660e\u4e86AdaGrad-Norm\u5728\u5149\u6ed1\u975e\u51f8\u4f18\u5316\u4e2d\u5177\u6709\u51e0\u4e4e\u5fc5\u7136\u6536\u655b\u548c\u5747\u65b9\u6536\u655b\u3002\u8fdb\u4e00\u6b65\u5c06\u5206\u6790\u6269\u5c55\u5230RMSProp\uff0c\u8868\u660e\u5728\u9002\u5f53\u7684\u8d85\u53c2\u6570\u9009\u62e9\u4e0b\uff0cRMSProp\u4e5f\u5177\u6709\u7a33\u5b9a\u6027\u548c\u6e10\u8fd1\u6536\u655b\u6027\u3002", "conclusion": "\u9996\u6b21\u4e3aAdaGrad-Norm\u5728\u975e\u51f8\u4f18\u5316\u4e2d\u7684\u6e10\u8fd1\u6536\u655b\u6027\u63d0\u4f9b\u4e86\u4e25\u683c\u7406\u8bba\u4fdd\u8bc1\uff0c\u6240\u5f00\u53d1\u7684\u6280\u672f\u5bf9\u5206\u6790\u5176\u4ed6\u81ea\u9002\u5e94\u968f\u673a\u4f18\u5316\u7b97\u6cd5\u5177\u6709\u72ec\u7acb\u4ef7\u503c\u3002"}}
{"id": "2601.00885", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00885", "abs": "https://arxiv.org/abs/2601.00885", "authors": ["Mandar Parab"], "title": "Counterfactual Self-Questioning for Stable Policy Optimization in Language Models", "comment": null, "summary": "Recent work on language model self-improvement shows that models can refine their own reasoning through reflection, verification, debate, or self-generated rewards. However, most existing approaches rely on external critics, learned reward models, or ensemble sampling, which increases complexity and training instability. We propose Counterfactual Self-Questioning, a framework in which a single language model generates and evaluates counterfactual critiques of its own reasoning. The method produces an initial reasoning trace, formulates targeted questions that challenge potential failure points, and generates alternative reasoning trajectories that expose incorrect assumptions or invalid steps. These counterfactual trajectories provide structured relative feedback that can be directly used for policy optimization without auxiliary models. Experiments on multiple mathematical reasoning benchmarks show that counterfactual self-questioning improves accuracy and training stability, particularly for smaller models, enabling scalable self-improvement using internally generated supervision alone.", "AI": {"tldr": "\u63d0\u51faCounterfactual Self-Questioning\u6846\u67b6\uff0c\u8ba9\u5355\u4e2a\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5e76\u8bc4\u4f30\u81ea\u8eab\u63a8\u7406\u7684\u53cd\u4e8b\u5b9e\u6279\u8bc4\uff0c\u901a\u8fc7\u5185\u90e8\u751f\u6210\u7684\u76d1\u7763\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u81ea\u6211\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u8bed\u8a00\u6a21\u578b\u81ea\u6211\u6539\u8fdb\u65b9\u6cd5\u4f9d\u8d56\u5916\u90e8\u6279\u8bc4\u8005\u3001\u5b66\u4e60\u5956\u52b1\u6a21\u578b\u6216\u96c6\u6210\u91c7\u6837\uff0c\u589e\u52a0\u4e86\u590d\u6742\u6027\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u3002\u9700\u8981\u4e00\u79cd\u66f4\u7b80\u5355\u3001\u66f4\u7a33\u5b9a\u7684\u81ea\u6211\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u53cd\u4e8b\u5b9e\u81ea\u6211\u63d0\u95ee\u6846\u67b6\uff1a1) \u751f\u6210\u521d\u59cb\u63a8\u7406\u8f68\u8ff9\uff1b2) \u9488\u5bf9\u6f5c\u5728\u5931\u8d25\u70b9\u63d0\u51fa\u9488\u5bf9\u6027\u95ee\u9898\uff1b3) \u751f\u6210\u66b4\u9732\u9519\u8bef\u5047\u8bbe\u6216\u65e0\u6548\u6b65\u9aa4\u7684\u66ff\u4ee3\u63a8\u7406\u8f68\u8ff9\uff1b4) \u5229\u7528\u8fd9\u4e9b\u53cd\u4e8b\u5b9e\u8f68\u8ff9\u4f5c\u4e3a\u7ed3\u6784\u5316\u76f8\u5bf9\u53cd\u9988\u8fdb\u884c\u7b56\u7565\u4f18\u5316\u3002", "result": "\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u53cd\u4e8b\u5b9e\u81ea\u6211\u63d0\u95ee\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u8f83\u5c0f\u6a21\u578b\uff0c\u4ec5\u4f7f\u7528\u5185\u90e8\u751f\u6210\u7684\u76d1\u7763\u5c31\u80fd\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u81ea\u6211\u6539\u8fdb\u3002", "conclusion": "Counterfactual Self-Questioning\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u81ea\u6211\u6539\u8fdb\u6846\u67b6\uff0c\u65e0\u9700\u5916\u90e8\u6a21\u578b\u8f85\u52a9\uff0c\u901a\u8fc7\u5185\u90e8\u751f\u6210\u7684\u53cd\u4e8b\u5b9e\u6279\u8bc4\u5b9e\u73b0\u7a33\u5b9a\u8bad\u7ec3\u548c\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2601.01156", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01156", "abs": "https://arxiv.org/abs/2601.01156", "authors": ["Jiani Guo", "Xiangke Zeng", "Jie Wu", "Zuchao Li"], "title": "DHI: Leveraging Diverse Hallucination Induction for Enhanced Contrastive Factuality Control in Large Language Models", "comment": "ICONIP 2025", "summary": "Large language models (LLMs) frequently produce inaccurate or fabricated information, known as \"hallucinations,\" which compromises their reliability. Existing approaches often train an \"Evil LLM\" to deliberately generate hallucinations on curated datasets, using these induced hallucinations to guide contrastive decoding against a reliable \"positive model\" for hallucination mitigation. However, this strategy is limited by the narrow diversity of hallucinations induced, as Evil LLMs trained on specific error types tend to reproduce only these particular patterns, thereby restricting their overall effectiveness. To address these limitations, we propose DHI (Diverse Hallucination Induction), a novel training framework that enables the Evil LLM to generate a broader range of hallucination types without relying on pre-annotated hallucination data. DHI employs a modified loss function that down-weights the generation of specific factually correct tokens, encouraging the Evil LLM to produce diverse hallucinations at targeted positions while maintaining overall factual content. Additionally, we introduce a causal attention masking adaptation to reduce the impact of this penalization on the generation of subsequent tokens. During inference, we apply an adaptive rationality constraint that restricts contrastive decoding to tokens where the positive model exhibits high confidence, thereby avoiding unnecessary penalties on factually correct tokens. Extensive empirical results show that DHI achieves significant performance gains over other contrastive decoding-based approaches across multiple hallucination benchmarks.", "AI": {"tldr": "\u63d0\u51faDHI\u6846\u67b6\uff0c\u901a\u8fc7\u4fee\u6539\u635f\u5931\u51fd\u6570\u548c\u56e0\u679c\u6ce8\u610f\u529b\u63a9\u7801\uff0c\u4f7f\"\u90aa\u6076LLM\"\u80fd\u751f\u6210\u66f4\u591a\u6837\u5316\u7684\u5e7b\u89c9\uff0c\u65e0\u9700\u9884\u6807\u6ce8\u6570\u636e\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u7406\u6027\u7ea6\u675f\u63d0\u5347\u5e7b\u89c9\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8bad\u7ec3\"\u90aa\u6076LLM\"\u5728\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\u6545\u610f\u751f\u6210\u5e7b\u89c9\uff0c\u4f46\u8bf1\u5bfc\u7684\u5e7b\u89c9\u7c7b\u578b\u6709\u9650\uff0c\u56e0\u4e3a\u6a21\u578b\u503e\u5411\u4e8e\u91cd\u590d\u7279\u5b9a\u9519\u8bef\u6a21\u5f0f\uff0c\u9650\u5236\u4e86\u6574\u4f53\u6548\u679c\u3002", "method": "DHI\u6846\u67b6\uff1a1) \u4fee\u6539\u635f\u5931\u51fd\u6570\uff0c\u964d\u4f4e\u7279\u5b9a\u4e8b\u5b9e\u6b63\u786etoken\u7684\u751f\u6210\u6743\u91cd\uff0c\u9f13\u52b1\u5728\u76ee\u6807\u4f4d\u7f6e\u4ea7\u751f\u591a\u6837\u5316\u5e7b\u89c9\uff1b2) \u5f15\u5165\u56e0\u679c\u6ce8\u610f\u529b\u63a9\u7801\u9002\u5e94\uff0c\u51cf\u5c11\u60e9\u7f5a\u5bf9\u540e\u7eedtoken\u7684\u5f71\u54cd\uff1b3) \u63a8\u7406\u65f6\u5e94\u7528\u81ea\u9002\u5e94\u7406\u6027\u7ea6\u675f\uff0c\u4ec5\u5728\u6b63\u6a21\u578b\u9ad8\u7f6e\u4fe1\u5ea6\u65f6\u8fdb\u884c\u5bf9\u6bd4\u89e3\u7801\u3002", "result": "\u5728\u591a\u4e2a\u5e7b\u89c9\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDHI\u76f8\u6bd4\u5176\u4ed6\u57fa\u4e8e\u5bf9\u6bd4\u89e3\u7801\u7684\u65b9\u6cd5\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "DHI\u6846\u67b6\u80fd\u591f\u8bf1\u5bfc\u66f4\u5e7f\u6cdb\u7684\u5e7b\u89c9\u7c7b\u578b\uff0c\u65e0\u9700\u4f9d\u8d56\u9884\u6807\u6ce8\u6570\u636e\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5e7b\u89c9\u68c0\u6d4b\u548c\u7f13\u89e3\u7684\u6027\u80fd\u3002"}}
{"id": "2601.00866", "categories": ["cs.LG", "cs.AI", "math.DS"], "pdf": "https://arxiv.org/pdf/2601.00866", "abs": "https://arxiv.org/abs/2601.00866", "authors": ["Shivani Saini", "Ramesh Kumar Vats", "Arup Kumar Sahoo"], "title": "A-PINN: Auxiliary Physics-informed Neural Networks for Structural Vibration Analysis in Continuous Euler-Bernoulli Beam", "comment": "31 pages", "summary": "Recent advancements in physics-informed neural networks (PINNs) and their variants have garnered substantial focus from researchers due to their effectiveness in solving both forward and inverse problems governed by differential equations. In this research, a modified Auxiliary physics-informed neural network (A-PINN) framework with balanced adaptive optimizers is proposed for the analysis of structural vibration problems. In order to accurately represent structural systems, it is critical for capturing vibration phenomena and ensuring reliable predictive analysis. So, our investigations are crucial for gaining deeper insight into the robustness of scientific machine learning models for solving vibration problems. Further, to rigorously evaluate the performance of A-PINN, we conducted different numerical simulations to approximate the Euler-Bernoulli beam equations under the various scenarios. The numerical results substantiate the enhanced performance of our model in terms of both numerical stability and predictive accuracy. Our model shows improvement of at least 40% over the baselines.", "AI": {"tldr": "\u63d0\u51fa\u6539\u8fdb\u7684\u8f85\u52a9\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08A-PINN\uff09\u6846\u67b6\uff0c\u7ed3\u5408\u5e73\u8861\u81ea\u9002\u5e94\u4f18\u5316\u5668\uff0c\u7528\u4e8e\u7ed3\u6784\u632f\u52a8\u5206\u6790\uff0c\u5728Euler-Bernoulli\u6881\u65b9\u7a0b\u6c42\u89e3\u4e2d\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\u63d0\u5347\u81f3\u5c1140%\u6027\u80fd\u3002", "motivation": "\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINNs\uff09\u53ca\u5176\u53d8\u4f53\u5728\u6c42\u89e3\u5fae\u5206\u65b9\u7a0b\u63a7\u5236\u7684\u6b63\u53cd\u95ee\u9898\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u9700\u8981\u6539\u8fdb\u4ee5\u66f4\u51c6\u786e\u5730\u8868\u793a\u7ed3\u6784\u7cfb\u7edf\u3001\u6355\u6349\u632f\u52a8\u73b0\u8c61\u5e76\u786e\u4fdd\u53ef\u9760\u7684\u9884\u6d4b\u5206\u6790\u3002", "method": "\u63d0\u51fa\u6539\u8fdb\u7684\u8f85\u52a9\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08A-PINN\uff09\u6846\u67b6\uff0c\u7ed3\u5408\u5e73\u8861\u81ea\u9002\u5e94\u4f18\u5316\u5668\uff0c\u4e13\u95e8\u9488\u5bf9\u7ed3\u6784\u632f\u52a8\u95ee\u9898\u8bbe\u8ba1\u3002\u901a\u8fc7\u6570\u503c\u6a21\u62df\u8bc4\u4f30\u6a21\u578b\u5728Euler-Bernoulli\u6881\u65b9\u7a0b\u5404\u79cd\u573a\u666f\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u6570\u503c\u7a33\u5b9a\u6027\u548c\u9884\u6d4b\u51c6\u786e\u6027\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\uff0c\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\u81f3\u5c11\u63d0\u9ad840%\u7684\u6027\u80fd\u3002", "conclusion": "\u6539\u8fdb\u7684A-PINN\u6846\u67b6\u5728\u7ed3\u6784\u632f\u52a8\u95ee\u9898\u5206\u6790\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u4e3a\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u632f\u52a8\u95ee\u9898\u6c42\u89e3\u4e2d\u7684\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u6df1\u5165\u89c1\u89e3\u3002"}}
{"id": "2601.01940", "categories": ["eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.01940", "abs": "https://arxiv.org/abs/2601.01940", "authors": ["Riccardo Zuliani", "Efe C. Balta", "John Lygeros"], "title": "Policy Optimization with Differentiable MPC: Convergence Analysis under Uncertainty", "comment": null, "summary": "Model-based policy optimization is a well-established framework for designing reliable and high-performance controllers across a wide range of control applications. Recently, this approach has been extended to model predictive control policies, where explicit dynamical models are embedded within the control law. However, the performance of the resulting controllers, and the convergence of the associated optimization algorithms, critically depends on the accuracy of the models. In this paper, we demonstrate that combining gradient-based policy optimization with recursive system identification ensures convergence to an optimal controller design and showcase our finding in several control examples.", "AI": {"tldr": "\u5c06\u68af\u5ea6\u7b56\u7565\u4f18\u5316\u4e0e\u9012\u5f52\u7cfb\u7edf\u8fa8\u8bc6\u7ed3\u5408\uff0c\u786e\u4fdd\u6536\u655b\u5230\u6700\u4f18\u63a7\u5236\u5668\u8bbe\u8ba1", "motivation": "\u57fa\u4e8e\u6a21\u578b\u7684\u7b56\u7565\u4f18\u5316\u6846\u67b6\u5728\u63a7\u5236\u5e94\u7528\u4e2d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u63a7\u5236\u5668\u6027\u80fd\u548c\u4f18\u5316\u7b97\u6cd5\u6536\u655b\u6027\u4e25\u91cd\u4f9d\u8d56\u6a21\u578b\u7cbe\u5ea6\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u4e2d\u5d4c\u5165\u663e\u5f0f\u52a8\u6001\u6a21\u578b\uff0c\u4f46\u6a21\u578b\u4e0d\u51c6\u786e\u4f1a\u5f71\u54cd\u6027\u80fd\u3002", "method": "\u7ed3\u5408\u68af\u5ea6\u7b56\u7565\u4f18\u5316\u4e0e\u9012\u5f52\u7cfb\u7edf\u8fa8\u8bc6\uff0c\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\u6301\u7eed\u66f4\u65b0\u548c\u6539\u8fdb\u7cfb\u7edf\u6a21\u578b", "result": "\u8be5\u65b9\u6cd5\u786e\u4fdd\u6536\u655b\u5230\u6700\u4f18\u63a7\u5236\u5668\u8bbe\u8ba1\uff0c\u5e76\u5728\u591a\u4e2a\u63a7\u5236\u793a\u4f8b\u4e2d\u5c55\u793a\u4e86\u6709\u6548\u6027", "conclusion": "\u68af\u5ea6\u7b56\u7565\u4f18\u5316\u4e0e\u9012\u5f52\u7cfb\u7edf\u8fa8\u8bc6\u7684\u7ed3\u5408\u4e3a\u89e3\u51b3\u6a21\u578b\u7cbe\u5ea6\u4f9d\u8d56\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0c\u80fd\u4fdd\u8bc1\u6536\u655b\u5230\u6700\u4f18\u63a7\u5236\u5668"}}
{"id": "2601.02084", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2601.02084", "abs": "https://arxiv.org/abs/2601.02084", "authors": ["Zhangcheng Feng", "Yancheng Yuan"], "title": "A Perturbed DCA for Computing d-Stationary Points of Nonsmooth DC Programs", "comment": null, "summary": "This paper introduces an efficient perturbed difference-of-convex algorithm (pDCA) for computing d-stationary points of an important class of structured nonsmooth difference-of-convex problems. Compared to the principal algorithms introduced in [J.-S. Pang, M. Razaviyayn, and A. Alvarado, Math. Oper. Res. 42(1):95--118 (2017)], which may require solving several subproblems for a one-step update, pDCA only requires solving a single subproblem. Therefore, the computational cost of pDCA for one-step update is comparable to the widely used difference-of-convex algorithm (DCA) introduced in [D. T. Pham and H. A. Le Thi, Acta Math. Vietnam. 22(1):289--355 (1997)] for computing a critical point. Importantly, under practical assumptions, we prove that every accumulation point of the sequence generated by pDCA is a d-stationary point almost surely. Numerical experiment results on several important examples of nonsmooth DC programs demonstrate the efficiency of pDCA for computing d-stationary points.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u6270\u52a8DC\u7b97\u6cd5(pDCA)\uff0c\u7528\u4e8e\u8ba1\u7b97\u4e00\u7c7b\u91cd\u8981\u975e\u5149\u6ed1DC\u95ee\u9898\u7684d-\u7a33\u5b9a\u70b9\uff0c\u76f8\u6bd4\u73b0\u6709\u7b97\u6cd5\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\uff0c\u53ea\u9700\u89e3\u51b3\u5355\u4e2a\u5b50\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684DC\u7b97\u6cd5\u5982Pang\u7b49\u4eba\u63d0\u51fa\u7684\u65b9\u6cd5\u9700\u8981\u89e3\u51b3\u591a\u4e2a\u5b50\u95ee\u9898\u6765\u5b8c\u6210\u4e00\u6b65\u66f4\u65b0\uff0c\u8ba1\u7b97\u6210\u672c\u8f83\u9ad8\u3002\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u7b97\u6cd5\u6765\u8ba1\u7b97\u975e\u5149\u6ed1DC\u95ee\u9898\u7684d-\u7a33\u5b9a\u70b9\u3002", "method": "\u63d0\u51fa\u6270\u52a8DC\u7b97\u6cd5(pDCA)\uff0c\u8be5\u7b97\u6cd5\u53ea\u9700\u89e3\u51b3\u5355\u4e2a\u5b50\u95ee\u9898\u5373\u53ef\u5b8c\u6210\u4e00\u6b65\u66f4\u65b0\uff0c\u8ba1\u7b97\u6210\u672c\u4e0e\u5e7f\u6cdb\u4f7f\u7528\u7684DCA\u76f8\u5f53\u3002\u5728\u5b9e\u7528\u5047\u8bbe\u4e0b\u8bc1\u660e\u7b97\u6cd5\u751f\u6210\u7684\u5e8f\u5217\u7684\u6bcf\u4e2a\u805a\u70b9\u51e0\u4e4e\u5fc5\u7136\u4e3ad-\u7a33\u5b9a\u70b9\u3002", "result": "pDCA\u5728\u591a\u4e2a\u91cd\u8981\u975e\u5149\u6ed1DC\u7a0b\u5e8f\u793a\u4f8b\u4e0a\u7684\u6570\u503c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u80fd\u9ad8\u6548\u8ba1\u7b97d-\u7a33\u5b9a\u70b9\u3002", "conclusion": "pDCA\u662f\u4e00\u79cd\u8ba1\u7b97\u975e\u5149\u6ed1DC\u95ee\u9898d-\u7a33\u5b9a\u70b9\u7684\u9ad8\u6548\u7b97\u6cd5\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\uff0c\u4e14\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u3002"}}
{"id": "2601.00923", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00923", "abs": "https://arxiv.org/abs/2601.00923", "authors": ["Josef Ott"], "title": "Context Collapse: In-Context Learning and Model Collapse", "comment": "Master's thesis", "summary": "This thesis investigates two key phenomena in large language models (LLMs): in-context learning (ICL) and model collapse. We study ICL in a linear transformer with tied weights trained on linear regression tasks, and show that minimising the in-context loss leads to a phase transition in the learned parameters. Above a critical context length, the solution develops a skew-symmetric component. We prove this by reducing the forward pass of the linear transformer under weight tying to preconditioned gradient descent, and then analysing the optimal preconditioner. This preconditioner includes a skew-symmetric component, which induces a rotation of the gradient direction. For model collapse, we use martingale and random walk theory to analyse simplified settings - linear regression and Gaussian fitting - under both replacing and cumulative data regimes. We strengthen existing results by proving almost sure convergence, showing that collapse occurs unless the data grows sufficiently fast or is retained over time. Finally, we introduce the notion of context collapse: a degradation of context during long generations, especially in chain-of-thought reasoning. This concept links the dynamics of ICL with long-term stability challenges in generative models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86LLMs\u4e2d\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u6a21\u578b\u5d29\u6e83\u73b0\u8c61\uff0c\u5728\u7ebf\u6027\u53d8\u538b\u5668\u4e2d\u8bc1\u660e\u4e86\u6700\u5c0f\u5316\u4e0a\u4e0b\u6587\u635f\u5931\u4f1a\u5bfc\u81f4\u53c2\u6570\u76f8\u53d8\uff0c\u5e76\u5206\u6790\u4e86\u6a21\u578b\u5d29\u6e83\u7684\u6536\u655b\u6761\u4ef6\uff0c\u6700\u540e\u63d0\u51fa\u4e86\u4e0a\u4e0b\u6587\u5d29\u6e83\u7684\u6982\u5ff5\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u4e24\u4e2a\u5173\u952e\u73b0\u8c61\uff1a\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u548c\u6a21\u578b\u5d29\u6e83\uff0c\u65e8\u5728\u7406\u89e3\u8fd9\u4e9b\u73b0\u8c61\u80cc\u540e\u7684\u6570\u5b66\u673a\u5236\u548c\u52a8\u6001\u7279\u6027\u3002", "method": "1. \u5728\u7ebf\u6027\u56de\u5f52\u4efb\u52a1\u4e0a\u8bad\u7ec3\u5177\u6709\u7ed1\u5b9a\u6743\u91cd\u7684\u7ebf\u6027\u53d8\u538b\u5668\uff0c\u5206\u6790\u4e0a\u4e0b\u6587\u5b66\u4e60\uff1b2. \u4f7f\u7528\u9785\u548c\u968f\u673a\u6e38\u8d70\u7406\u8bba\u5206\u6790\u7ebf\u6027\u56de\u5f52\u548c\u9ad8\u65af\u62df\u5408\u7684\u7b80\u5316\u8bbe\u7f6e\uff1b3. \u5c06\u7ebf\u6027\u53d8\u538b\u5668\u7684\u524d\u5411\u4f20\u64ad\u7b80\u5316\u4e3a\u9884\u6761\u4ef6\u68af\u5ea6\u4e0b\u964d\u5e76\u5206\u6790\u6700\u4f18\u9884\u6761\u4ef6\u5668\u3002", "result": "1. \u6700\u5c0f\u5316\u4e0a\u4e0b\u6587\u635f\u5931\u4f1a\u5bfc\u81f4\u53c2\u6570\u76f8\u53d8\uff0c\u8d85\u8fc7\u4e34\u754c\u4e0a\u4e0b\u6587\u957f\u5ea6\u65f6\u89e3\u4f1a\u53d1\u5c55\u51fa\u659c\u5bf9\u79f0\u5206\u91cf\uff1b2. \u8bc1\u660e\u4e86\u6a21\u578b\u5d29\u6e83\u7684\u51e0\u4e4e\u5fc5\u7136\u6536\u655b\uff0c\u9664\u975e\u6570\u636e\u589e\u957f\u8db3\u591f\u5feb\u6216\u968f\u65f6\u95f4\u4fdd\u7559\uff1b3. \u63d0\u51fa\u4e86\u4e0a\u4e0b\u6587\u5d29\u6e83\u6982\u5ff5\uff0c\u8fde\u63a5\u4e86\u4e0a\u4e0b\u6587\u5b66\u4e60\u52a8\u6001\u4e0e\u751f\u6210\u6a21\u578b\u7684\u957f\u671f\u7a33\u5b9a\u6027\u6311\u6218\u3002", "conclusion": "\u8bba\u6587\u63ed\u793a\u4e86\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u6a21\u578b\u5d29\u6e83\u7684\u6570\u5b66\u673a\u5236\uff0c\u8bc1\u660e\u4e86\u76f8\u53d8\u73b0\u8c61\u548c\u6536\u655b\u6761\u4ef6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e0a\u4e0b\u6587\u5d29\u6e83\u4f5c\u4e3a\u8fde\u63a5\u8fd9\u4e24\u4e2a\u73b0\u8c61\u7684\u65b0\u6982\u5ff5\uff0c\u4e3a\u7406\u89e3LLMs\u7684\u52a8\u6001\u884c\u4e3a\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\u3002"}}
{"id": "2601.01171", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01171", "abs": "https://arxiv.org/abs/2601.01171", "authors": ["Serge Sharoff", "John Baker", "David Francis Hunt", "Alan Simpson"], "title": "Almost Clinical: Linguistic properties of synthetic electronic health records", "comment": null, "summary": "This study evaluates the linguistic and clinical suitability of synthetic electronic health records (EHRs) in the field of mental health. First, we describe the rationale and the methodology for creating the synthetic corpus. Second, we assess agency, modality, and information flow across four clinical genres (Assessments, Correspondence, Referrals and Care plans) to understand how LLMs grammatically construct medical authority and patient agency through linguistic choices. While LLMs produce coherent, terminology-appropriate texts that approximate clinical practice, systematic divergences remain, including registerial shifts, insufficient clinical specificity, and inaccuracies in medication use and diagnostic procedures.", "AI": {"tldr": "\u8bc4\u4f30\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u5408\u6210\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u7684\u8bed\u8a00\u5b66\u548c\u4e34\u5e8a\u9002\u7528\u6027\uff0c\u5206\u6790LLM\u751f\u6210\u6587\u672c\u5728\u6784\u5efa\u533b\u7597\u6743\u5a01\u548c\u60a3\u8005\u80fd\u52a8\u6027\u65b9\u9762\u7684\u8bed\u8a00\u7279\u5f81", "motivation": "\u8bc4\u4f30\u5408\u6210\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u5728\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u7684\u9002\u7528\u6027\uff0c\u7406\u89e3LLM\u5982\u4f55\u901a\u8fc7\u8bed\u8a00\u9009\u62e9\u6784\u5efa\u533b\u7597\u6743\u5a01\u548c\u60a3\u8005\u80fd\u52a8\u6027\uff0c\u8bc6\u522b\u5408\u6210\u6587\u672c\u4e0e\u771f\u5b9e\u4e34\u5e8a\u5b9e\u8df5\u7684\u5dee\u5f02", "method": "\u9996\u5148\u63cf\u8ff0\u521b\u5efa\u5408\u6210\u8bed\u6599\u5e93\u7684\u7406\u8bba\u57fa\u7840\u548c\u65b9\u6cd5\u8bba\uff0c\u7136\u540e\u8bc4\u4f30\u56db\u4e2a\u4e34\u5e8a\u4f53\u88c1\uff08\u8bc4\u4f30\u3001\u901a\u4fe1\u3001\u8f6c\u8bca\u548c\u62a4\u7406\u8ba1\u5212\uff09\u4e2d\u7684\u80fd\u52a8\u6027\u3001\u6a21\u6001\u548c\u4fe1\u606f\u6d41\uff0c\u5206\u6790LLM\u5982\u4f55\u901a\u8fc7\u8bed\u6cd5\u9009\u62e9\u6784\u5efa\u533b\u7597\u6743\u5a01", "result": "LLM\u80fd\u591f\u751f\u6210\u8fde\u8d2f\u3001\u672f\u8bed\u9002\u5f53\u7684\u6587\u672c\uff0c\u8fd1\u4f3c\u4e34\u5e8a\u5b9e\u8df5\uff0c\u4f46\u5b58\u5728\u7cfb\u7edf\u6027\u5dee\u5f02\uff1a\u8bed\u57df\u8f6c\u79fb\u3001\u4e34\u5e8a\u7279\u5f02\u6027\u4e0d\u8db3\u3001\u836f\u7269\u4f7f\u7528\u548c\u8bca\u65ad\u7a0b\u5e8f\u4e0d\u51c6\u786e", "conclusion": "\u5408\u6210\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u5728\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u89e3\u51b3\u7cfb\u7edf\u6027\u8bed\u8a00\u5dee\u5f02\u548c\u4e34\u5e8a\u51c6\u786e\u6027\u95ee\u9898\uff0c\u624d\u80fd\u5728\u5b9e\u9645\u4e34\u5e8a\u5e94\u7528\u4e2d\u53ef\u9760\u4f7f\u7528"}}
{"id": "2601.00892", "categories": ["cs.LG", "cs.CV", "physics.data-an", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.00892", "abs": "https://arxiv.org/abs/2601.00892", "authors": ["Ana Carpio", "Gema Duro"], "title": "Hierarchical topological clustering", "comment": "not peer reviewed, reviewed version to appear in Soft Computing", "summary": "Topological methods have the potential of exploring data clouds without making assumptions on their the structure. Here we propose a hierarchical topological clustering algorithm that can be implemented with any distance choice. The persistence of outliers and clusters of arbitrary shape is inferred from the resulting hierarchy. We demonstrate the potential of the algorithm on selected datasets in which outliers play relevant roles, consisting of images, medical and economic data. These methods can provide meaningful clusters in situations in which other techniques fail to do so.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5206\u5c42\u62d3\u6251\u805a\u7c7b\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u4efb\u610f\u8ddd\u79bb\u9009\u62e9\uff0c\u80fd\u8bc6\u522b\u4efb\u610f\u5f62\u72b6\u7684\u805a\u7c7b\u548c\u5f02\u5e38\u503c", "motivation": "\u62d3\u6251\u65b9\u6cd5\u80fd\u5728\u4e0d\u5047\u8bbe\u6570\u636e\u7ed3\u6784\u7684\u60c5\u51b5\u4e0b\u63a2\u7d22\u6570\u636e\u4e91\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u5904\u7406\u4efb\u610f\u5f62\u72b6\u805a\u7c7b\u548c\u5f02\u5e38\u503c\u7684\u805a\u7c7b\u7b97\u6cd5", "method": "\u63d0\u51fa\u5206\u5c42\u62d3\u6251\u805a\u7c7b\u7b97\u6cd5\uff0c\u53ef\u4e0e\u4efb\u610f\u8ddd\u79bb\u5ea6\u91cf\u7ed3\u5408\u4f7f\u7528\uff0c\u901a\u8fc7\u5c42\u6b21\u7ed3\u6784\u63a8\u65ad\u5f02\u5e38\u503c\u548c\u4efb\u610f\u5f62\u72b6\u805a\u7c7b\u7684\u6301\u4e45\u6027", "result": "\u5728\u56fe\u50cf\u3001\u533b\u7597\u548c\u7ecf\u6d4e\u6570\u636e\u7b49\u5305\u542b\u76f8\u5173\u5f02\u5e38\u503c\u7684\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u7b97\u6cd5\u7684\u6f5c\u529b\uff0c\u8fd9\u4e9b\u60c5\u51b5\u4e0b\u5176\u4ed6\u6280\u672f\u5f80\u5f80\u5931\u6548", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u5728\u5176\u4ed6\u6280\u672f\u5931\u8d25\u7684\u60c5\u51b5\u4e0b\u63d0\u4f9b\u6709\u610f\u4e49\u7684\u805a\u7c7b\uff0c\u5c55\u793a\u4e86\u62d3\u6251\u65b9\u6cd5\u5728\u63a2\u7d22\u590d\u6742\u6570\u636e\u4e91\u4e2d\u7684\u4ef7\u503c"}}
{"id": "2601.00868", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00868", "abs": "https://arxiv.org/abs/2601.00868", "authors": ["Aditya Sreevatsa K", "Arun Kumar Raveendran", "Jesrael K Mani", "Prakash G Shigli", "Rajkumar Rangadore", "Narayana Darapaneni", "Anwesh Reddy Paduri"], "title": "SmartFlow Reinforcement Learning and Agentic AI for Bike-Sharing Optimisation", "comment": null, "summary": "SmartFlow is a multi-layered framework that integrates Reinforcement Learning and Agentic AI to address the dynamic rebalancing problem in urban bike-sharing services. Its architecture separates strategic, tactical, and communication functions for clarity and scalability. At the strategic level, a Deep Q-Network (DQN) agent, trained in a high-fidelity simulation of New Yorks Citi Bike network, learns robust rebalancing policies by modelling the challenge as a Markov Decision Process. These high-level strategies feed into a deterministic tactical module that optimises multi-leg journeys and schedules just-in-time dispatches to minimise fleet travel. Evaluation across multiple seeded runs demonstrates SmartFlows high efficacy, reducing network imbalance by over 95% while requiring minimal travel distance and achieving strong truck utilisation. A communication layer, powered by a grounded Agentic AI with a Large Language Model (LLM), translates logistical plans into clear, actionable instructions for operational staff, ensuring interpretability and execution readiness. This integration bridges machine intelligence with human operations, offering a scalable solution that reduces idle time, improves bike availability, and lowers operational costs. SmartFlow provides a blueprint for interpretable, AI-driven logistics in complex urban mobility networks.", "AI": {"tldr": "SmartFlow\u662f\u4e00\u4e2a\u591a\u5c42\u6846\u67b6\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u667a\u80fd\u4f53AI\u89e3\u51b3\u57ce\u5e02\u5171\u4eab\u5355\u8f66\u52a8\u6001\u518d\u5e73\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u6218\u7565DQN\u5b66\u4e60\u7b56\u7565\u3001\u6218\u672f\u6a21\u5757\u4f18\u5316\u8c03\u5ea6\u3001\u901a\u4fe1\u5c42\u751f\u6210\u53ef\u6267\u884c\u6307\u4ee4\uff0c\u663e\u8457\u51cf\u5c11\u7f51\u7edc\u4e0d\u5e73\u8861\u5e76\u964d\u4f4e\u8fd0\u8425\u6210\u672c\u3002", "motivation": "\u89e3\u51b3\u57ce\u5e02\u5171\u4eab\u5355\u8f66\u7cfb\u7edf\u4e2d\u7684\u52a8\u6001\u518d\u5e73\u8861\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u590d\u6742\u52a8\u6001\u73af\u5883\uff0c\u9700\u8981\u7ed3\u5408\u673a\u5668\u667a\u80fd\u4e0e\u4eba\u5de5\u64cd\u4f5c\u7684\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\uff0c\u51cf\u5c11\u95f2\u7f6e\u65f6\u95f4\u3001\u63d0\u9ad8\u5355\u8f66\u53ef\u7528\u6027\u3001\u964d\u4f4e\u8fd0\u8425\u6210\u672c\u3002", "method": "\u591a\u5c42\u6846\u67b6\uff1a\u6218\u7565\u5c42\u4f7f\u7528DQN\u5728\u7ebd\u7ea6Citi Bike\u7f51\u7edc\u9ad8\u4fdd\u771f\u6a21\u62df\u4e2d\u5b66\u4e60\u518d\u5e73\u8861\u7b56\u7565\uff1b\u6218\u672f\u5c42\u786e\u5b9a\u6027\u6a21\u5757\u4f18\u5316\u591a\u6bb5\u884c\u7a0b\u548c\u8c03\u5ea6\uff1b\u901a\u4fe1\u5c42\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53AI\u5c06\u7269\u6d41\u8ba1\u5212\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u6307\u4ee4\u3002", "result": "\u5728\u591a\u6b21\u79cd\u5b50\u8fd0\u884c\u8bc4\u4f30\u4e2d\uff0cSmartFlow\u5c06\u7f51\u7edc\u4e0d\u5e73\u8861\u51cf\u5c11\u8d85\u8fc795%\uff0c\u540c\u65f6\u9700\u8981\u6700\u5c0f\u884c\u9a76\u8ddd\u79bb\uff0c\u5b9e\u73b0\u9ad8\u5361\u8f66\u5229\u7528\u7387\uff0c\u663e\u8457\u6539\u5584\u5355\u8f66\u53ef\u7528\u6027\u5e76\u964d\u4f4e\u8fd0\u8425\u6210\u672c\u3002", "conclusion": "SmartFlow\u4e3a\u590d\u6742\u57ce\u5e02\u79fb\u52a8\u7f51\u7edc\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u3001AI\u9a71\u52a8\u7684\u7269\u6d41\u84dd\u56fe\uff0c\u6210\u529f\u6865\u63a5\u673a\u5668\u667a\u80fd\u4e0e\u4eba\u5de5\u64cd\u4f5c\uff0c\u5c55\u793a\u4e86\u5728\u52a8\u6001\u57ce\u5e02\u73af\u5883\u4e2d\u53ef\u6269\u5c55\u7684\u667a\u80fd\u7269\u6d41\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01951", "categories": ["eess.SY", "physics.class-ph"], "pdf": "https://arxiv.org/pdf/2601.01951", "abs": "https://arxiv.org/abs/2601.01951", "authors": ["Mihails Milehins", "Dan B. Marghitu"], "title": "Asymptotic Behavior of an Unforced Duhem-Type Hysteretic Oscillator", "comment": "8 pages", "summary": "The article describes fundamental analytical properties of an unforced mechanical oscillator with a Duhem-type viscoelastoplastic hysteretic element. These properties include global existence of solutions, uniqueness of solutions, and convergence of each solution to an equilibrium point.", "AI": {"tldr": "\u5206\u6790\u5177\u6709Duhem\u578b\u7c98\u5f39\u5851\u6027\u6ede\u56de\u5143\u4ef6\u7684\u65e0\u5916\u529b\u673a\u68b0\u632f\u8361\u5668\u7684\u57fa\u672c\u89e3\u6790\u6027\u8d28\uff0c\u5305\u62ec\u89e3\u7684\u5b58\u5728\u6027\u3001\u552f\u4e00\u6027\u548c\u6536\u655b\u6027", "motivation": "\u7814\u7a76\u5177\u6709Duhem\u578b\u6ede\u56de\u5143\u4ef6\u7684\u673a\u68b0\u632f\u8361\u5668\u7684\u6570\u5b66\u6027\u8d28\uff0c\u4e3a\u7406\u89e3\u8fd9\u7c7b\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u884c\u4e3a\u63d0\u4f9b\u7406\u8bba\u57fa\u7840", "method": "\u91c7\u7528\u6570\u5b66\u5206\u6790\u65b9\u6cd5\uff0c\u7814\u7a76\u65e0\u5916\u529b\u673a\u68b0\u632f\u8361\u5668\u6a21\u578b\uff0c\u5206\u6790\u5176\u89e3\u7684\u5b58\u5728\u6027\u3001\u552f\u4e00\u6027\u548c\u6536\u655b\u6027\u7b49\u57fa\u672c\u6027\u8d28", "result": "\u8bc1\u660e\u4e86\u8be5\u7cfb\u7edf\u7684\u5168\u5c40\u89e3\u5b58\u5728\u6027\u3001\u89e3\u7684\u552f\u4e00\u6027\uff0c\u4ee5\u53ca\u6bcf\u4e2a\u89e3\u90fd\u6536\u655b\u5230\u5e73\u8861\u70b9", "conclusion": "\u5177\u6709Duhem\u578b\u7c98\u5f39\u5851\u6027\u6ede\u56de\u5143\u4ef6\u7684\u65e0\u5916\u529b\u673a\u68b0\u632f\u8361\u5668\u5177\u6709\u826f\u597d\u7684\u6570\u5b66\u6027\u8d28\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1"}}
{"id": "2601.02134", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2601.02134", "abs": "https://arxiv.org/abs/2601.02134", "authors": ["Florentin Goyens", "Geovani N. Grapiglia"], "title": "Complexity of quadratic penalty methods with adaptive accuracy under a PL condition for the constraints", "comment": null, "summary": "We study the quadratic penalty method (QPM) for smooth nonconvex optimization problems with equality constraints. Assuming the constraint violation satisfies the PL condition near the feasible set, we derive sharper worst-case complexity bounds for obtaining approximate first-order KKT points. When the objective and constraints are twice continuously differentiable, we show that QPM equipped with a suitable first-order inner solver requires at most $O(\\varepsilon_{0}^{-1}\\varepsilon_{1}^{-2})$ first-order oracle calls to find an $(\\varepsilon_{0},\\varepsilon_{1})$-approximate KKT point -- that is, a point that is $\\varepsilon_{0}$-approximately feasible and $\\varepsilon_{1}$-approximately stationary. Furthermore, when the objective and constraints are three times continuously differentiable, we show that QPM with a suitable second-order inner solver requires at most $O\\left(\\varepsilon_{0}^{-1/2}\\varepsilon_{1}^{-3/2}\\right)$ second-order oracle calls to find an $(\\varepsilon_{0},\\varepsilon_{1})$-approximate KKT point. We also introduce an adaptive, feasibility-aware stopping criterion for the subproblems, which relaxes the stationarity tolerance when far from feasibility. This rule preserves all theoretical guarantees while substantially reducing computational effort in practice.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5e26\u7b49\u5f0f\u7ea6\u675f\u7684\u975e\u51f8\u4f18\u5316\u95ee\u9898\u7684\u4e8c\u6b21\u7f5a\u51fd\u6570\u6cd5\uff0c\u5728\u7ea6\u675f\u8fdd\u53cd\u6ee1\u8db3PL\u6761\u4ef6\u65f6\uff0c\u63a8\u5bfc\u4e86\u66f4\u5c16\u9510\u7684\u6700\u574f\u60c5\u51b5\u590d\u6742\u5ea6\u754c\uff0c\u5e76\u63d0\u51fa\u4e86\u81ea\u9002\u5e94\u505c\u6b62\u51c6\u5219\u3002", "motivation": "\u5bf9\u4e8e\u5e26\u7b49\u5f0f\u7ea6\u675f\u7684\u975e\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u4e8c\u6b21\u7f5a\u51fd\u6570\u6cd5\u662f\u5e38\u7528\u65b9\u6cd5\uff0c\u4f46\u73b0\u6709\u7406\u8bba\u5206\u6790\u5728\u590d\u6742\u5ea6\u754c\u65b9\u9762\u4e0d\u591f\u7cbe\u786e\u3002\u7279\u522b\u662f\u5728\u7ea6\u675f\u8fdd\u53cd\u6ee1\u8db3Polyak-\u0141ojasiewicz\u6761\u4ef6\u65f6\uff0c\u9700\u8981\u66f4\u7cbe\u786e\u7684\u590d\u6742\u5ea6\u5206\u6790\u6765\u6307\u5bfc\u7b97\u6cd5\u8bbe\u8ba1\u3002", "method": "\u91c7\u7528\u4e8c\u6b21\u7f5a\u51fd\u6570\u6cd5\uff0c\u5047\u8bbe\u7ea6\u675f\u8fdd\u53cd\u5728\u53ef\u884c\u96c6\u9644\u8fd1\u6ee1\u8db3PL\u6761\u4ef6\u3002\u9488\u5bf9\u4e00\u9636\u548c\u4e8c\u9636\u53ef\u5fae\u6027\u5047\u8bbe\uff0c\u5206\u522b\u8bbe\u8ba1\u76f8\u5e94\u7684\u4e00\u9636\u548c\u4e8c\u9636\u5185\u5c42\u6c42\u89e3\u5668\u3002\u63d0\u51fa\u81ea\u9002\u5e94\u3001\u53ef\u884c\u6027\u611f\u77e5\u7684\u5b50\u95ee\u9898\u505c\u6b62\u51c6\u5219\uff0c\u5728\u8fdc\u79bb\u53ef\u884c\u6027\u65f6\u653e\u5bbd\u5e73\u7a33\u6027\u5bb9\u5fcd\u5ea6\u3002", "result": "\u5f53\u76ee\u6807\u548c\u7ea6\u675f\u4e8c\u9636\u8fde\u7eed\u53ef\u5fae\u65f6\uff0cQPM\u914d\u5408\u4e00\u9636\u5185\u5c42\u6c42\u89e3\u5668\u6700\u591a\u9700\u8981O(\u03b5\u2080\u207b\u00b9\u03b5\u2081\u207b\u00b2)\u6b21\u4e00\u9636oracle\u8c03\u7528\u627e\u5230(\u03b5\u2080,\u03b5\u2081)-\u8fd1\u4f3cKKT\u70b9\u3002\u5f53\u4e09\u9636\u8fde\u7eed\u53ef\u5fae\u65f6\uff0c\u914d\u5408\u4e8c\u9636\u5185\u5c42\u6c42\u89e3\u5668\u6700\u591a\u9700\u8981O(\u03b5\u2080\u207b\u00b9/\u00b2\u03b5\u2081\u207b\u00b3/\u00b2)\u6b21\u4e8c\u9636oracle\u8c03\u7528\u3002\u81ea\u9002\u5e94\u505c\u6b62\u51c6\u5219\u5728\u4fdd\u6301\u7406\u8bba\u4fdd\u8bc1\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u91cf\u3002", "conclusion": "\u4e8c\u6b21\u7f5a\u51fd\u6570\u6cd5\u5728\u7ea6\u675f\u8fdd\u53cd\u6ee1\u8db3PL\u6761\u4ef6\u65f6\u5177\u6709\u4f18\u8d8a\u7684\u590d\u6742\u5ea6\u754c\uff0c\u63d0\u51fa\u7684\u81ea\u9002\u5e94\u505c\u6b62\u51c6\u5219\u80fd\u6709\u6548\u5e73\u8861\u53ef\u884c\u6027\u548c\u5e73\u7a33\u6027\u8981\u6c42\uff0c\u4e3a\u5b9e\u9645\u8ba1\u7b97\u5e26\u6765\u663e\u8457\u6548\u7387\u63d0\u5347\u3002"}}
{"id": "2601.01225", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01225", "abs": "https://arxiv.org/abs/2601.01225", "authors": ["Hezam Albaqami", "Muhammad Asif Ayub", "Nasir Ahmad", "Yaseen Ahmad", "Mohammed M. Alqahtani", "Abdullah M. Algamdi", "Almoaid A. Owaidah", "Kashif Ahmad"], "title": "Stylometry Analysis of Human and Machine Text for Academic Integrity", "comment": "16 pages, 9 tables, 3 figures", "summary": "This work addresses critical challenges to academic integrity, including plagiarism, fabrication, and verification of authorship of educational content, by proposing a Natural Language Processing (NLP)-based framework for authenticating students' content through author attribution and style change detection. Despite some initial efforts, several aspects of the topic are yet to be explored. In contrast to existing solutions, the paper provides a comprehensive analysis of the topic by targeting four relevant tasks, including (i) classification of human and machine text, (ii) differentiating in single and multi-authored documents, (iii) author change detection within multi-authored documents, and (iv) author recognition in collaboratively produced documents. The solutions proposed for the tasks are evaluated on two datasets generated with Gemini using two different prompts, including a normal and a strict set of instructions. During experiments, some reduction in the performance of the proposed solutions is observed on the dataset generated through the strict prompt, demonstrating the complexities involved in detecting machine-generated text with cleverly crafted prompts. The generated datasets, code, and other relevant materials are made publicly available on GitHub, which are expected to provide a baseline for future research in the domain.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u57fa\u4e8eNLP\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4f5c\u8005\u5f52\u5c5e\u548c\u98ce\u683c\u53d8\u5316\u68c0\u6d4b\u6765\u8ba4\u8bc1\u5b66\u751f\u5185\u5bb9\uff0c\u89e3\u51b3\u5b66\u672f\u8bda\u4fe1\u95ee\u9898\uff0c\u5305\u62ec\u4eba\u673a\u6587\u672c\u5206\u7c7b\u3001\u5355/\u591a\u4f5c\u8005\u533a\u5206\u3001\u591a\u4f5c\u8005\u6587\u6863\u4e2d\u7684\u4f5c\u8005\u53d8\u5316\u68c0\u6d4b\u548c\u534f\u4f5c\u6587\u6863\u4e2d\u7684\u4f5c\u8005\u8bc6\u522b\u7b49\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3\u5b66\u672f\u8bda\u4fe1\u7684\u5173\u952e\u6311\u6218\uff0c\u5305\u62ec\u6284\u88ad\u3001\u4f2a\u9020\u548c\u6559\u80b2\u5185\u5bb9\u4f5c\u8005\u8eab\u4efd\u9a8c\u8bc1\uff0c\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u4e0d\u5b8c\u5584\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u5206\u6790\u6846\u67b6\u3002", "method": "\u63d0\u51faNLP\u6846\u67b6\uff0c\u9488\u5bf9\u56db\u4e2a\u4efb\u52a1\uff1a\u4eba\u673a\u6587\u672c\u5206\u7c7b\u3001\u5355/\u591a\u4f5c\u8005\u533a\u5206\u3001\u591a\u4f5c\u8005\u6587\u6863\u4f5c\u8005\u53d8\u5316\u68c0\u6d4b\u3001\u534f\u4f5c\u6587\u6863\u4f5c\u8005\u8bc6\u522b\u3002\u4f7f\u7528Gemini\u751f\u6210\u4e24\u4e2a\u6570\u636e\u96c6\uff08\u666e\u901a\u548c\u4e25\u683c\u6307\u4ee4\uff09\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728\u4e25\u683c\u6307\u4ee4\u751f\u6210\u7684\u6570\u636e\u96c6\u4e0a\u89c2\u5bdf\u5230\u6027\u80fd\u4e0b\u964d\uff0c\u8868\u660e\u68c0\u6d4b\u5de7\u5999\u8bbe\u8ba1\u7684\u673a\u5668\u751f\u6210\u6587\u672c\u7684\u590d\u6742\u6027\u3002\u6570\u636e\u96c6\u3001\u4ee3\u7801\u548c\u76f8\u5173\u6750\u6599\u5df2\u516c\u5f00\u5728GitHub\u4e0a\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5b66\u672f\u5185\u5bb9\u8ba4\u8bc1\u63d0\u4f9b\u4e86\u7efc\u5408\u6846\u67b6\uff0c\u516c\u5f00\u8d44\u6e90\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u57fa\u7840\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u68c0\u6d4b\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u673a\u5668\u751f\u6210\u6587\u672c\u7684\u6311\u6218\u3002"}}
{"id": "2601.00908", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.00908", "abs": "https://arxiv.org/abs/2601.00908", "authors": ["Chorok Lee"], "title": "Conformal Prediction Under Distribution Shift: A COVID-19 Natural Experiment", "comment": null, "summary": "Conformal prediction guarantees degrade under distribution shift. We study this using COVID-19 as a natural experiment across 8 supply chain tasks. Despite identical severe feature turnover (Jaccard approximately 0), coverage drops vary from 0% to 86.7%, spanning two orders of magnitude. Using SHapley Additive exPlanations (SHAP) analysis, we find catastrophic failures correlate with single-feature dependence (rho = 0.714, p = 0.047). Catastrophic tasks concentrate importance in one feature (4.5x increase), while robust tasks redistribute across many (10-20x). Quarterly retraining restores catastrophic task coverage from 22% to 41% (+19 pp, p = 0.04), but provides no benefit for robust tasks (99.8% coverage). Exploratory analysis of 4 additional tasks with moderate feature stability (Jaccard 0.13-0.86) reveals feature stability, not concentration, determines robustness, suggesting concentration effects apply specifically to severe shifts. We provide a decision framework: monitor SHAP concentration before deployment; retrain quarterly if vulnerable (>40% concentration); skip retraining if robust.", "AI": {"tldr": "\u7814\u7a76\u663e\u793a\uff0c\u5728\u5206\u5e03\u504f\u79fb\u4e0b\uff0c\u4fdd\u5f62\u9884\u6d4b\u7684\u8986\u76d6\u4fdd\u8bc1\u4f1a\u4e0b\u964d\u3002\u901a\u8fc7\u5bf98\u4e2a\u4f9b\u5e94\u94fe\u4efb\u52a1\u7684\u5206\u6790\u53d1\u73b0\uff0c\u5373\u4f7f\u7279\u5f81\u5b8c\u5168\u53d8\u5316\uff0c\u8986\u76d6\u4e0b\u964d\u7a0b\u5ea6\u5dee\u5f02\u5de8\u5927\uff080%-86.7%\uff09\u3002\u5355\u7279\u5f81\u4f9d\u8d56\u4e0e\u707e\u96be\u6027\u5931\u8d25\u9ad8\u5ea6\u76f8\u5173\uff0c\u800c\u7a33\u5065\u4efb\u52a1\u4f1a\u5c06\u91cd\u8981\u6027\u5206\u6563\u5230\u591a\u4e2a\u7279\u5f81\u3002\u5b63\u5ea6\u91cd\u8bad\u7ec3\u5bf9\u8106\u5f31\u4efb\u52a1\u6709\u6548\uff0c\u4f46\u5bf9\u7a33\u5065\u4efb\u52a1\u65e0\u76ca\u3002", "motivation": "\u4fdd\u5f62\u9884\u6d4b\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u6027\u80fd\u4fdd\u8bc1\u4f1a\u4e0b\u964d\uff0c\u4f46\u5177\u4f53\u4e0b\u964d\u7a0b\u5ea6\u548c\u673a\u5236\u5c1a\u4e0d\u6e05\u695a\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7COVID-19\u4f5c\u4e3a\u81ea\u7136\u5b9e\u9a8c\uff0c\u5206\u6790\u4f9b\u5e94\u94fe\u4efb\u52a1\u4e2d\u5206\u5e03\u504f\u79fb\u5bf9\u4fdd\u5f62\u9884\u6d4b\u7684\u5f71\u54cd\uff0c\u5e76\u8bc6\u522b\u5bfc\u81f4\u707e\u96be\u6027\u5931\u8d25\u7684\u56e0\u7d20\u3002", "method": "\u4f7f\u7528COVID-19\u4f5c\u4e3a\u81ea\u7136\u5b9e\u9a8c\uff0c\u5206\u67908\u4e2a\u4f9b\u5e94\u94fe\u4efb\u52a1\u3002\u901a\u8fc7SHAP\u5206\u6790\u7279\u5f81\u91cd\u8981\u6027\u5206\u5e03\uff0c\u8ba1\u7b97\u7279\u5f81\u96c6\u4e2d\u5ea6\u6307\u6807\u3002\u6bd4\u8f83\u4e0d\u540c\u4efb\u52a1\u5728\u4e25\u91cd\u7279\u5f81\u53d8\u5316\u4e0b\u7684\u8986\u76d6\u4e0b\u964d\u60c5\u51b5\uff0c\u5e76\u6d4b\u8bd5\u5b63\u5ea6\u91cd\u8bad\u7ec3\u7684\u6548\u679c\u3002\u6269\u5c55\u5206\u67904\u4e2a\u5177\u6709\u4e2d\u7b49\u7279\u5f81\u7a33\u5b9a\u6027\u7684\u4efb\u52a1\u3002", "result": "1) \u5373\u4f7f\u7279\u5f81\u5b8c\u5168\u53d8\u5316\uff08Jaccard\u22480\uff09\uff0c\u8986\u76d6\u4e0b\u964d\u7a0b\u5ea6\u5dee\u5f02\u5de8\u5927\uff080%-86.7%\uff09\uff1b2) \u707e\u96be\u6027\u5931\u8d25\u4e0e\u5355\u7279\u5f81\u4f9d\u8d56\u9ad8\u5ea6\u76f8\u5173\uff08\u03c1=0.714, p=0.047\uff09\uff1b3) \u8106\u5f31\u4efb\u52a1\u7279\u5f81\u91cd\u8981\u6027\u96c6\u4e2d\u5ea6\u589e\u52a04.5\u500d\uff0c\u7a33\u5065\u4efb\u52a1\u5206\u6563\u523010-20\u4e2a\u7279\u5f81\uff1b4) \u5b63\u5ea6\u91cd\u8bad\u7ec3\u53ef\u5c06\u8106\u5f31\u4efb\u52a1\u8986\u76d6\u4ece22%\u63d0\u5347\u523041%\uff0c\u4f46\u5bf9\u7a33\u5065\u4efb\u52a1\u65e0\u76ca\uff0899.8%\u8986\u76d6\uff09\u3002", "conclusion": "\u7279\u5f81\u91cd\u8981\u6027\u96c6\u4e2d\u5ea6\u662f\u9884\u6d4b\u4fdd\u5f62\u9884\u6d4b\u5728\u4e25\u91cd\u5206\u5e03\u504f\u79fb\u4e0b\u8106\u5f31\u6027\u7684\u5173\u952e\u6307\u6807\u3002\u63d0\u51fa\u4e86\u51b3\u7b56\u6846\u67b6\uff1a\u90e8\u7f72\u524d\u76d1\u63a7SHAP\u96c6\u4e2d\u5ea6\uff1b\u5982\u679c\u96c6\u4e2d\u5ea6>40%\u5219\u8fdb\u884c\u5b63\u5ea6\u91cd\u8bad\u7ec3\uff1b\u5982\u679c\u4efb\u52a1\u7a33\u5065\u5219\u53ef\u8df3\u8fc7\u91cd\u8bad\u7ec3\u3002\u7279\u5f81\u7a33\u5b9a\u6027\uff08\u800c\u975e\u96c6\u4e2d\u5ea6\uff09\u51b3\u5b9a\u4e2d\u7b49\u504f\u79fb\u4e0b\u7684\u7a33\u5065\u6027\u3002"}}
{"id": "2601.00873", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.00873", "abs": "https://arxiv.org/abs/2601.00873", "authors": ["Osasumwen Cedric Ogiesoba-Eguakun", "Suman Rath"], "title": "Quantum Machine Learning Approaches for Coordinated Stealth Attack Detection in Distributed Generation Systems", "comment": "10 pages", "summary": "Coordinated stealth attacks are a serious cybersecurity threat to distributed generation systems because they modify control and measurement signals while remaining close to normal behavior, making them difficult to detect using standard intrusion detection methods. This study investigates quantum machine learning approaches for detecting coordinated stealth attacks on a distributed generation unit in a microgrid. High-quality simulated measurements were used to create a balanced binary classification dataset using three features: reactive power at DG1, frequency deviation relative to the nominal value, and terminal voltage magnitude. Classical machine learning baselines, fully quantum variational classifiers, and hybrid quantum classical models were evaluated. The results show that a hybrid quantum classical model combining quantum feature embeddings with a classical RBF support vector machine achieves the best overall performance on this low dimensional dataset, with a modest improvement in accuracy and F1 score over a strong classical SVM baseline. Fully quantum models perform worse due to training instability and limitations of current NISQ hardware. In contrast, hybrid models train more reliably and demonstrate that quantum feature mapping can enhance intrusion detection even when fully quantum learning is not yet practical.", "AI": {"tldr": "\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u5728\u68c0\u6d4b\u5fae\u7535\u7f51\u5206\u5e03\u5f0f\u53d1\u7535\u5355\u5143\u534f\u540c\u9690\u853d\u653b\u51fb\u4e2d\u7684\u5e94\u7528\u7814\u7a76\uff0c\u6df7\u5408\u91cf\u5b50\u7ecf\u5178\u6a21\u578b\u5728\u4f4e\u7ef4\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u6700\u4f73", "motivation": "\u534f\u540c\u9690\u853d\u653b\u51fb\u662f\u5206\u5e03\u5f0f\u53d1\u7535\u7cfb\u7edf\u7684\u4e25\u91cd\u7f51\u7edc\u5b89\u5168\u5a01\u80c1\uff0c\u5b83\u4eec\u4fee\u6539\u63a7\u5236\u548c\u6d4b\u91cf\u4fe1\u53f7\u4f46\u4fdd\u6301\u63a5\u8fd1\u6b63\u5e38\u884c\u4e3a\uff0c\u4f7f\u5f97\u4f20\u7edf\u5165\u4fb5\u68c0\u6d4b\u65b9\u6cd5\u96be\u4ee5\u53d1\u73b0\u3002\u9700\u8981\u63a2\u7d22\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u6765\u63d0\u9ad8\u68c0\u6d4b\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u9ad8\u8d28\u91cf\u6a21\u62df\u6d4b\u91cf\u6570\u636e\u521b\u5efa\u5e73\u8861\u7684\u4e8c\u5143\u5206\u7c7b\u6570\u636e\u96c6\uff0c\u5305\u542b\u4e09\u4e2a\u7279\u5f81\uff1aDG1\u7684\u65e0\u529f\u529f\u7387\u3001\u76f8\u5bf9\u4e8e\u6807\u79f0\u503c\u7684\u9891\u7387\u504f\u5dee\u548c\u7aef\u7535\u538b\u5e45\u503c\u3002\u8bc4\u4f30\u4e86\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u57fa\u7ebf\u3001\u5b8c\u5168\u91cf\u5b50\u53d8\u5206\u5206\u7c7b\u5668\u548c\u6df7\u5408\u91cf\u5b50\u7ecf\u5178\u6a21\u578b\u3002", "result": "\u6df7\u5408\u91cf\u5b50\u7ecf\u5178\u6a21\u578b\uff08\u91cf\u5b50\u7279\u5f81\u5d4c\u5165\u4e0e\u7ecf\u5178RBF\u652f\u6301\u5411\u91cf\u673a\u7ed3\u5408\uff09\u5728\u4f4e\u7ef4\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u4f73\u6574\u4f53\u6027\u80fd\uff0c\u5728\u51c6\u786e\u7387\u548cF1\u5206\u6570\u4e0a\u6bd4\u7ecf\u5178SVM\u57fa\u7ebf\u6709\u9002\u5ea6\u63d0\u5347\u3002\u5b8c\u5168\u91cf\u5b50\u6a21\u578b\u7531\u4e8e\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u548c\u5f53\u524dNISQ\u786c\u4ef6\u7684\u9650\u5236\u8868\u73b0\u8f83\u5dee\u3002", "conclusion": "\u6df7\u5408\u6a21\u578b\u8bad\u7ec3\u66f4\u53ef\u9760\uff0c\u8868\u660e\u91cf\u5b50\u7279\u5f81\u6620\u5c04\u5373\u4f7f\u5728\u5b8c\u5168\u91cf\u5b50\u5b66\u4e60\u5c1a\u4e0d\u5b9e\u7528\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u589e\u5f3a\u5165\u4fb5\u68c0\u6d4b\u80fd\u529b\u3002\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u5728\u7f51\u7edc\u5b89\u5168\u9886\u57df\u5177\u6709\u5e94\u7528\u6f5c\u529b\uff0c\u4f46\u5f53\u524d\u786c\u4ef6\u9650\u5236\u4f7f\u5f97\u6df7\u5408\u65b9\u6cd5\u66f4\u4e3a\u5b9e\u7528\u3002"}}
{"id": "2601.02109", "categories": ["eess.SY", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.02109", "abs": "https://arxiv.org/abs/2601.02109", "authors": ["Hossein Rastgoftar"], "title": "Finite-State Decentralized Policy-Based Control With Guaranteed Ground Coverage", "comment": null, "summary": "We propose a finite-state, decentralized decision and control framework for multi-agent ground coverage. The approach decomposes the problem into two coupled components: (i) the structural design of a deep neural network (DNN) induced by the reference configuration of the agents, and (ii) policy-based decentralized coverage control. Agents are classified as anchors and followers, yielding a generic and scalable communication architecture in which each follower interacts with exactly three in-neighbors from the preceding layer, forming an enclosing triangular communication structure. The DNN training weights implicitly encode the spatial configuration of the agent team, thereby providing a geometric representation of the environmental target set. Within this architecture, we formulate a computationally efficient decentralized Markov decision process (MDP) whose components are time-invariant except for a time-varying cost function defined by the deviation from the centroid of the target set contained within each agent communication triangle. By introducing the concept of Anyway Output Controllability (AOC), we assume each agent is AOC and establish decentralized convergence to a desired configuration that optimally represents the environmental target.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u6709\u9650\u72b6\u6001\u3001\u53bb\u4e2d\u5fc3\u5316\u7684\u591a\u667a\u80fd\u4f53\u5730\u9762\u8986\u76d6\u51b3\u7b56\u4e0e\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\u8bbe\u8ba1\u548c\u57fa\u4e8e\u7b56\u7565\u7684\u53bb\u4e2d\u5fc3\u5316\u8986\u76d6\u63a7\u5236\u5b9e\u73b0\u9ad8\u6548\u8986\u76d6\u3002", "motivation": "\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u5730\u9762\u8986\u76d6\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u8ba1\u7b97\u9ad8\u6548\u7684\u53bb\u4e2d\u5fc3\u5316\u65b9\u6cd5\uff0c\u80fd\u591f\u9002\u5e94\u73af\u5883\u76ee\u6807\u96c6\u7684\u53d8\u5316\u5e76\u5b9e\u73b0\u6700\u4f18\u914d\u7f6e\u3002", "method": "\u5c06\u95ee\u9898\u5206\u89e3\u4e3a\u4e24\u90e8\u5206\uff1a1\uff09\u57fa\u4e8e\u667a\u80fd\u4f53\u53c2\u8003\u914d\u7f6e\u8bbe\u8ba1\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\uff1b2\uff09\u57fa\u4e8e\u7b56\u7565\u7684\u53bb\u4e2d\u5fc3\u5316\u8986\u76d6\u63a7\u5236\u3002\u5c06\u667a\u80fd\u4f53\u5206\u4e3a\u951a\u70b9\u548c\u8ddf\u968f\u8005\uff0c\u5f62\u6210\u4e09\u89d2\u5f62\u901a\u4fe1\u7ed3\u6784\uff0cDNN\u6743\u91cd\u7f16\u7801\u56e2\u961f\u7a7a\u95f4\u914d\u7f6e\u3002\u5efa\u7acb\u8ba1\u7b97\u9ad8\u6548\u7684\u53bb\u4e2d\u5fc3\u5316\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5f15\u5165Anyway Output Controllability\u6982\u5ff5\u3002", "result": "\u5efa\u7acb\u4e86\u53bb\u4e2d\u5fc3\u5316\u6536\u655b\u5230\u671f\u671b\u914d\u7f6e\u7684\u7406\u8bba\u4fdd\u8bc1\uff0c\u8be5\u914d\u7f6e\u80fd\u591f\u6700\u4f18\u5730\u8868\u793a\u73af\u5883\u76ee\u6807\u96c6\uff0c\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u8986\u76d6\u63a7\u5236\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u548c\u53bb\u4e2d\u5fc3\u5316\u63a7\u5236\uff0c\u4e3a\u591a\u667a\u80fd\u4f53\u5730\u9762\u8986\u76d6\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u53ef\u6269\u5c55\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4f18\u52bf\u3002"}}
{"id": "2601.02207", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2601.02207", "abs": "https://arxiv.org/abs/2601.02207", "authors": ["Arash Khojaste", "Jonathan Pearce", "Daniela Pucci de Farias", "Geoffrey Pritchard", "Golbon Zakeri"], "title": "Risk-Averse Markov Decision Processes: Applications to Electricity Grid and Reservoir Management", "comment": null, "summary": "This paper develops risk-averse models to support system operators in planning and operating the electricity grid under uncertainty from renewable power generation. We incorporate financial risk hedging using conditional value at risk (CVaR) within a Markov Decision Process (MDP) framework and propose efficient, exact solution methods for these models. In addition, we introduce a power reliability-oriented risk measure and present new, computationally efficient models for risk-averse grid planning and operations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5f00\u53d1\u4e86\u98ce\u9669\u89c4\u907f\u6a21\u578b\uff0c\u7528\u4e8e\u5728\u53ef\u518d\u751f\u80fd\u6e90\u53d1\u7535\u4e0d\u786e\u5b9a\u6027\u4e0b\u652f\u6301\u7535\u7f51\u89c4\u5212\u4e0e\u8fd0\u884c\uff0c\u7ed3\u5408CVaR\u91d1\u878d\u98ce\u9669\u5bf9\u51b2\u548cMDP\u6846\u67b6\uff0c\u5e76\u63d0\u51fa\u9ad8\u6548\u7cbe\u786e\u7684\u6c42\u89e3\u65b9\u6cd5\u3002", "motivation": "\u53ef\u518d\u751f\u80fd\u6e90\u53d1\u7535\u7684\u4e0d\u786e\u5b9a\u6027\u7ed9\u7535\u7f51\u89c4\u5212\u4e0e\u8fd0\u884c\u5e26\u6765\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u98ce\u9669\u89c4\u907f\u6a21\u578b\u6765\u5e2e\u52a9\u7cfb\u7edf\u8fd0\u8425\u5546\u5728\u4e0d\u786e\u5b9a\u6027\u73af\u5883\u4e0b\u505a\u51fa\u51b3\u7b56\uff0c\u540c\u65f6\u8003\u8651\u8d22\u52a1\u98ce\u9669\u5bf9\u51b2\u3002", "method": "\u5c06\u6761\u4ef6\u98ce\u9669\u4ef7\u503c(CVaR)\u91d1\u878d\u98ce\u9669\u5bf9\u51b2\u65b9\u6cd5\u6574\u5408\u5230\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b(MDP)\u6846\u67b6\u4e2d\uff0c\u63d0\u51fa\u9ad8\u6548\u7cbe\u786e\u7684\u6c42\u89e3\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u9762\u5411\u7535\u529b\u53ef\u9760\u6027\u7684\u98ce\u9669\u5ea6\u91cf\u3002", "result": "\u63d0\u51fa\u4e86\u65b0\u7684\u8ba1\u7b97\u9ad8\u6548\u7684\u98ce\u9669\u89c4\u907f\u7535\u7f51\u89c4\u5212\u4e0e\u8fd0\u884c\u6a21\u578b\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u53ef\u518d\u751f\u80fd\u6e90\u4e0d\u786e\u5b9a\u6027\uff0c\u4e3a\u7cfb\u7edf\u8fd0\u8425\u5546\u63d0\u4f9b\u98ce\u9669\u89c4\u907f\u51b3\u7b56\u652f\u6301\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7535\u7f51\u5728\u53ef\u518d\u751f\u80fd\u6e90\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u98ce\u9669\u89c4\u907f\u89c4\u5212\u4e0e\u8fd0\u884c\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5efa\u6a21\u6846\u67b6\u548c\u6c42\u89e3\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u91d1\u878d\u98ce\u9669\u7ba1\u7406\u548c\u7535\u529b\u53ef\u9760\u6027\u9700\u6c42\u3002"}}
{"id": "2601.01195", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01195", "abs": "https://arxiv.org/abs/2601.01195", "authors": ["Wuzhenghong Wen", "Chao Xue", "Su Pan", "Yuwei Sun", "Minlong Peng"], "title": "Reinforcement Learning Enhanced Multi-hop Reasoning for Temporal Knowledge Question Answering", "comment": "11 pages, 2 figures", "summary": "Temporal knowledge graph question answering (TKGQA) involves multi-hop reasoning over temporally constrained entity relationships in the knowledge graph to answer a given question. However, at each hop, large language models (LLMs) retrieve subgraphs with numerous temporally similar and semantically complex relations, increasing the risk of suboptimal decisions and error propagation. To address these challenges, we propose the multi-hop reasoning enhanced (MRE) framework, which enhances both forward and backward reasoning to improve the identification of globally optimal reasoning trajectories. Specifically, MRE begins with prompt engineering to guide the LLM in generating diverse reasoning trajectories for a given question. Valid reasoning trajectories are then selected for supervised fine-tuning, serving as a cold-start strategy. Finally, we introduce Tree-Group Relative Policy Optimization (T-GRPO), a recursive, tree-structured learning-by-exploration approach. At each hop, exploration establishes strong causal dependencies on the previous hop, while evaluation is informed by multi-path exploration feedback from subsequent hops. Experimental results on two TKGQA benchmarks indicate that the proposed MRE-based model consistently surpasses state-of-the-art (SOTA) approaches in handling complex multi-hop queries. Further analysis highlights improved interpretability and robustness to noisy temporal annotations.", "AI": {"tldr": "\u63d0\u51faMRE\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u5f3a\u524d\u5411\u548c\u540e\u5411\u63a8\u7406\u6765\u6539\u8fdbTKGQA\u4e2d\u7684\u591a\u8df3\u63a8\u7406\uff0c\u4f7f\u7528Tree-Group Relative Policy Optimization\u4f18\u5316\u63a8\u7406\u8f68\u8ff9\u9009\u62e9", "motivation": "TKGQA\u4e2d\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6bcf\u8df3\u63a8\u7406\u65f6\u4f1a\u68c0\u7d22\u5927\u91cf\u65f6\u95f4\u76f8\u4f3c\u4e14\u8bed\u4e49\u590d\u6742\u7684\u5173\u7cfb\u5b50\u56fe\uff0c\u5bfc\u81f4\u6b21\u4f18\u51b3\u7b56\u548c\u9519\u8bef\u4f20\u64ad\u98ce\u9669\u589e\u52a0", "method": "\u63d0\u51faMRE\u6846\u67b6\uff1a1) \u63d0\u793a\u5de5\u7a0b\u751f\u6210\u591a\u6837\u63a8\u7406\u8f68\u8ff9\uff1b2) \u9009\u62e9\u6709\u6548\u8f68\u8ff9\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\u4f5c\u4e3a\u51b7\u542f\u52a8\uff1b3) \u5f15\u5165T-GRPO\uff0c\u4e00\u79cd\u9012\u5f52\u6811\u7ed3\u6784\u7684\u5b66\u4e60\u63a2\u7d22\u65b9\u6cd5\uff0c\u5efa\u7acb\u5f3a\u56e0\u679c\u4f9d\u8d56\u5173\u7cfb", "result": "\u5728\u4e24\u4e2aTKGQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMRE\u6a21\u578b\u6301\u7eed\u8d85\u8d8aSOTA\u65b9\u6cd5\uff0c\u5728\u5904\u7406\u590d\u6742\u591a\u8df3\u67e5\u8be2\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u5bf9\u566a\u58f0\u65f6\u95f4\u6807\u6ce8\u7684\u9c81\u68d2\u6027", "conclusion": "MRE\u6846\u67b6\u901a\u8fc7\u589e\u5f3a\u524d\u5411\u548c\u540e\u5411\u63a8\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86TKGQA\u4e2d\u7684\u591a\u8df3\u63a8\u7406\u6311\u6218\uff0c\u63d0\u9ad8\u4e86\u63a8\u7406\u8f68\u8ff9\u7684\u5168\u5c40\u6700\u4f18\u6027\u548c\u7cfb\u7edf\u9c81\u68d2\u6027"}}
{"id": "2601.01244", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01244", "abs": "https://arxiv.org/abs/2601.01244", "authors": ["Zsolt Csibi", "Bence Gy\u00f6rgy Gortka", "Natabara Gy\u00f6ngy\u00f6ssy", "Korn\u00e9l Nagy", "D\u00e1vid M\u00e1rk Nemeskey", "Martin Sallai", "Andr\u00e1s Simonyi", "Andr\u00e1s M\u00e1rk Szekeres", "G\u00e1bor Palk\u00f3"], "title": "Racka: Efficient Hungarian LLM Adaptation on Academic Infrastructure", "comment": "18 pages, 1 figures. To appear in the XXII. Magyar Sz\u00e1m\u00edt\u00f3g\u00e9pes Nyelv\u00e9szeti Konferencia (MSZNY 2026)", "summary": "We present Racka, a lightweight, continually pretrained large language model designed to bridge the resource gap between Hungarian and high-resource languages such as English and German. Racka employs parameter-efficient continual pretraining via Low-Rank Adaptation (LoRA) on a Qwen-3 4B backbone, making the recipe practical on A100 (40GB)-based HPC clusters with low inter-node bandwidth. To better match the training distribution, we replace and adapt the tokenizer, achieving substantially improved tokenization fertility for Hungarian while maintaining competitive performance in English and German. The model is trained on 160B subword tokens drawn from a mixture of internet and high-quality curated sources, with a composition of 44% Hungarian, 24% English, 21% German, and 11% code. This data mix is chosen to mitigate catastrophic forgetting and preserve high-resource language capabilities during continual pretraining. Our preliminary results indicate modest but stable results in language adaptation.", "AI": {"tldr": "Racka\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6301\u7eed\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4e13\u95e8\u4e3a\u5308\u7259\u5229\u8bed\u8bbe\u8ba1\uff0c\u901a\u8fc7LoRA\u53c2\u6570\u9ad8\u6548\u8bad\u7ec3\u5728Qwen-3 4B\u57fa\u7840\u4e0a\uff0c\u6539\u5584\u5308\u7259\u5229\u8bed\u5206\u8bcd\u6548\u679c\uff0c\u540c\u65f6\u4fdd\u6301\u82f1\u8bed\u548c\u5fb7\u8bed\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u5308\u7259\u5229\u8bed\u4e0e\u9ad8\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u82f1\u8bed\u3001\u5fb7\u8bed\uff09\u4e4b\u95f4\u7684\u8d44\u6e90\u5dee\u8ddd\u95ee\u9898\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u63d0\u4f9b\u5b9e\u7528\u7684\u8bed\u8a00\u6a21\u578b\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eLoRA\u7684\u53c2\u6570\u9ad8\u6548\u6301\u7eed\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5728Qwen-3 4B\u9aa8\u5e72\u7f51\u7edc\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff1b\u66ff\u6362\u548c\u9002\u914d\u5206\u8bcd\u5668\u4ee5\u6539\u5584\u5308\u7259\u5229\u8bed\u5206\u8bcd\u6548\u7387\uff1b\u4f7f\u7528\u5305\u542b44%\u5308\u7259\u5229\u8bed\u300124%\u82f1\u8bed\u300121%\u5fb7\u8bed\u548c11%\u4ee3\u7801\u7684160B\u5b50\u8bcd\u6807\u8bb0\u6df7\u5408\u6570\u636e\u96c6\u3002", "result": "\u5308\u7259\u5229\u8bed\u5206\u8bcd\u6548\u7387\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u5728\u82f1\u8bed\u548c\u5fb7\u8bed\u4e0a\u4fdd\u6301\u7ade\u4e89\u529b\uff1b\u5b9e\u73b0\u4e86\u7a33\u5b9a\u4f46\u9002\u4e2d\u7684\u8bed\u8a00\u9002\u5e94\u7ed3\u679c\uff1b\u8bad\u7ec3\u65b9\u6cd5\u5728A100\uff0840GB\uff09HPC\u96c6\u7fa4\u4e0a\u5b9e\u7528\u53ef\u884c\u3002", "conclusion": "Racka\u5c55\u793a\u4e86\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u6301\u7eed\u9884\u8bad\u7ec3\u65b9\u6cd5\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u5f00\u53d1\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u884c\u6027\uff0c\u6210\u529f\u6539\u5584\u4e86\u5308\u7259\u5229\u8bed\u5904\u7406\u80fd\u529b\uff0c\u540c\u65f6\u907f\u514d\u4e86\u9ad8\u8d44\u6e90\u8bed\u8a00\u80fd\u529b\u7684\u707e\u96be\u6027\u9057\u5fd8\u3002"}}
{"id": "2601.01069", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.01069", "abs": "https://arxiv.org/abs/2601.01069", "authors": ["Jing Wang", "Peng Zhao", "Zhi-Hua Zhou"], "title": "Revisiting Weighted Strategy for Non-stationary Parametric Bandits and MDPs", "comment": "accepted by IEEE Transactions on Information Theory. arXiv admin note: substantial text overlap with arXiv:2303.02691", "summary": "Non-stationary parametric bandits have attracted much attention recently. There are three principled ways to deal with non-stationarity, including sliding-window, weighted, and restart strategies. As many non-stationary environments exhibit gradual drifting patterns, the weighted strategy is commonly adopted in real-world applications. However, previous theoretical studies show that its analysis is more involved and the algorithms are either computationally less efficient or statistically suboptimal. This paper revisits the weighted strategy for non-stationary parametric bandits. In linear bandits (LB), we discover that this undesirable feature is due to an inadequate regret analysis, which results in an overly complex algorithm design. We propose a \\emph{refined analysis framework}, which simplifies the derivation and, importantly, produces a simpler weight-based algorithm that is as efficient as window/restart-based algorithms while retaining the same regret as previous studies. Furthermore, our new framework can be used to improve regret bounds of other parametric bandits, including Generalized Linear Bandits (GLB) and Self-Concordant Bandits (SCB). For example, we develop a simple weighted GLB algorithm with an $\\tilde{O}(k_\u03bc^{5/4} c_\u03bc^{-3/4} d^{3/4} P_T^{1/4}T^{3/4})$ regret, improving the $\\tilde{O}(k_\u03bc^{2} c_\u03bc^{-1}d^{9/10} P_T^{1/5}T^{4/5})$ bound in prior work, where $k_\u03bc$ and $c_\u03bc$ characterize the reward model's nonlinearity, $P_T$ measures the non-stationarity, $d$ and $T$ denote the dimension and time horizon. Moreover, we extend our framework to non-stationary Markov Decision Processes (MDPs) with function approximation, focusing on Linear Mixture MDP and Multinomial Logit (MNL) Mixture MDP. For both classes, we propose algorithms based on the weighted strategy and establish dynamic regret guarantees using our analysis framework.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u5206\u6790\u6846\u67b6\uff0c\u7b80\u5316\u4e86\u975e\u5e73\u7a33\u53c2\u6570\u5316\u8d4c\u535a\u673a\u4e2d\u52a0\u6743\u7b56\u7565\u7684\u7b97\u6cd5\u8bbe\u8ba1\u548c\u7406\u8bba\u5206\u6790\uff0c\u5728\u7ebf\u6027\u8d4c\u535a\u673a\u4e2d\u5b9e\u73b0\u4e86\u66f4\u7b80\u5355\u9ad8\u6548\u7684\u7b97\u6cd5\uff0c\u5e76\u5c06\u8be5\u6846\u67b6\u6269\u5c55\u5230\u5e7f\u4e49\u7ebf\u6027\u8d4c\u535a\u673a\u3001\u81ea\u534f\u8c03\u8d4c\u535a\u673a\u548c\u5177\u6709\u51fd\u6570\u903c\u8fd1\u7684MDPs\uff0c\u83b7\u5f97\u4e86\u66f4\u597d\u7684\u9057\u61be\u754c\u3002", "motivation": "\u975e\u5e73\u7a33\u53c2\u6570\u5316\u8d4c\u535a\u673a\u4e2d\uff0c\u52a0\u6743\u7b56\u7565\u5728\u5904\u7406\u6e10\u53d8\u6f02\u79fb\u6a21\u5f0f\u65f6\u5f88\u5e38\u7528\uff0c\u4f46\u4e4b\u524d\u7684\u7406\u8bba\u5206\u6790\u590d\u6742\u4e14\u7b97\u6cd5\u8981\u4e48\u8ba1\u7b97\u6548\u7387\u4f4e\u8981\u4e48\u7edf\u8ba1\u6b21\u4f18\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u52a0\u6743\u7b56\u7565\u5206\u6790\u590d\u6742\u3001\u7b97\u6cd5\u8bbe\u8ba1\u7e41\u7410\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7cbe\u70bc\u7684\u5206\u6790\u6846\u67b6\uff0c\u7b80\u5316\u4e86\u52a0\u6743\u7b56\u7565\u7684\u63a8\u5bfc\u8fc7\u7a0b\u3002\u5728\u7ebf\u6027\u8d4c\u535a\u673a\u4e2d\uff0c\u57fa\u4e8e\u8be5\u6846\u67b6\u8bbe\u8ba1\u4e86\u66f4\u7b80\u5355\u7684\u52a0\u6743\u7b97\u6cd5\u3002\u7136\u540e\u5c06\u8be5\u6846\u67b6\u6269\u5c55\u5230\u5e7f\u4e49\u7ebf\u6027\u8d4c\u535a\u673a\u3001\u81ea\u534f\u8c03\u8d4c\u535a\u673a\u4ee5\u53ca\u5177\u6709\u51fd\u6570\u903c\u8fd1\u7684\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08\u7ebf\u6027\u6df7\u5408MDP\u548c\u591a\u9879Logit\u6df7\u5408MDP\uff09\u3002", "result": "\u5728\u7ebf\u6027\u8d4c\u535a\u673a\u4e2d\uff0c\u65b0\u7b97\u6cd5\u4e0e\u7a97\u53e3/\u91cd\u542f\u7b97\u6cd5\u540c\u6837\u9ad8\u6548\uff0c\u4e14\u4fdd\u6301\u4e86\u76f8\u540c\u7684\u9057\u61be\u754c\u3002\u5728\u5e7f\u4e49\u7ebf\u6027\u8d4c\u535a\u673a\u4e2d\uff0c\u83b7\u5f97\u4e86$\\tilde{O}(k_\u03bc^{5/4} c_\u03bc^{-3/4} d^{3/4} P_T^{1/4}T^{3/4})$\u7684\u9057\u61be\u754c\uff0c\u4f18\u4e8e\u4e4b\u524d\u7684$\\tilde{O}(k_\u03bc^{2} c_\u03bc^{-1}d^{9/10} P_T^{1/5}T^{4/5})$\u3002\u8be5\u6846\u67b6\u8fd8\u6210\u529f\u5e94\u7528\u4e8e\u81ea\u534f\u8c03\u8d4c\u535a\u673a\u548c\u4e24\u7c7bMDPs\u3002", "conclusion": "\u63d0\u51fa\u7684\u7cbe\u70bc\u5206\u6790\u6846\u67b6\u663e\u8457\u7b80\u5316\u4e86\u975e\u5e73\u7a33\u53c2\u6570\u5316\u8d4c\u535a\u673a\u4e2d\u52a0\u6743\u7b56\u7565\u7684\u7406\u8bba\u5206\u6790\u548c\u7b97\u6cd5\u8bbe\u8ba1\uff0c\u5728\u7ebf\u6027\u8d4c\u535a\u673a\u4e2d\u5b9e\u73b0\u4e86\u7b80\u5355\u9ad8\u6548\u7684\u7b97\u6cd5\uff0c\u5e76\u5728\u5e7f\u4e49\u7ebf\u6027\u8d4c\u535a\u673a\u3001\u81ea\u534f\u8c03\u8d4c\u535a\u673a\u548c\u5177\u6709\u51fd\u6570\u903c\u8fd1\u7684MDPs\u4e2d\u83b7\u5f97\u4e86\u6539\u8fdb\u7684\u9057\u61be\u754c\u3002"}}
{"id": "2601.00874", "categories": ["cs.LG", "cs.AI", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2601.00874", "abs": "https://arxiv.org/abs/2601.00874", "authors": ["M. Rizki Oktavian"], "title": "LLMize: A Framework for Large Language Model-Based Numerical Optimization", "comment": null, "summary": "Large language models (LLMs) have recently shown strong reasoning capabilities beyond traditional language tasks, motivating their use for numerical optimization. This paper presents LLMize, an open-source Python framework that enables LLM-driven optimization through iterative prompting and in-context learning. LLMize formulates optimization as a black-box process in which candidate solutions are generated in natural language, evaluated by an external objective function, and refined over successive iterations using solution-score feedback. The framework supports multiple optimization strategies, including Optimization by Prompting (OPRO) and hybrid LLM-based methods inspired by evolutionary algorithms and simulated annealing. A key advantage of LLMize is the ability to inject constraints, rules, and domain knowledge directly through natural language descriptions, allowing practitioners to define complex optimization problems without requiring expertise in mathematical programming or metaheuristic design. LLMize is evaluated on convex optimization, linear programming, the Traveling Salesman Problem, neural network hyperparameter tuning, and nuclear fuel lattice optimization. Results show that while LLM-based optimization is not competitive with classical solvers for simple problems, it provides a practical and accessible approach for complex, domain-specific tasks where constraints and heuristics are difficult to formalize.", "AI": {"tldr": "LLMize\u662f\u4e00\u4e2a\u5f00\u6e90Python\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u63d0\u793a\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u5b9e\u73b0LLM\u9a71\u52a8\u7684\u4f18\u5316\uff0c\u5c06\u4f18\u5316\u95ee\u9898\u8f6c\u5316\u4e3a\u81ea\u7136\u8bed\u8a00\u751f\u6210\u5019\u9009\u89e3\u3001\u5916\u90e8\u8bc4\u4f30\u3001\u53cd\u9988\u6539\u8fdb\u7684\u9ed1\u76d2\u8fc7\u7a0b\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5c55\u73b0\u51fa\u8d85\u8d8a\u4f20\u7edf\u8bed\u8a00\u4efb\u52a1\u7684\u63a8\u7406\u80fd\u529b\uff0c\u6fc0\u53d1\u4e86\u5c06\u5176\u7528\u4e8e\u6570\u503c\u4f18\u5316\u7684\u63a2\u7d22\u3002\u4f20\u7edf\u4f18\u5316\u65b9\u6cd5\u9700\u8981\u6570\u5b66\u7f16\u7a0b\u6216\u5143\u542f\u53d1\u5f0f\u8bbe\u8ba1\u4e13\u4e1a\u77e5\u8bc6\uff0c\u800cLLM\u53ef\u4ee5\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6ce8\u5165\u7ea6\u675f\u548c\u9886\u57df\u77e5\u8bc6\u3002", "method": "\u5c06\u4f18\u5316\u95ee\u9898\u8f6c\u5316\u4e3a\u9ed1\u76d2\u8fc7\u7a0b\uff1a\u7528\u81ea\u7136\u8bed\u8a00\u751f\u6210\u5019\u9009\u89e3\uff0c\u901a\u8fc7\u5916\u90e8\u76ee\u6807\u51fd\u6570\u8bc4\u4f30\uff0c\u5229\u7528\u89e3-\u5206\u6570\u53cd\u9988\u8fdb\u884c\u8fed\u4ee3\u6539\u8fdb\u3002\u652f\u6301\u591a\u79cd\u7b56\u7565\uff0c\u5305\u62ec\u4f18\u5316\u63d0\u793a\uff08OPRO\uff09\u4ee5\u53ca\u53d7\u8fdb\u5316\u7b97\u6cd5\u548c\u6a21\u62df\u9000\u706b\u542f\u53d1\u7684\u6df7\u5408LLM\u65b9\u6cd5\u3002", "result": "\u5728\u51f8\u4f18\u5316\u3001\u7ebf\u6027\u89c4\u5212\u3001\u65c5\u884c\u5546\u95ee\u9898\u3001\u795e\u7ecf\u7f51\u7edc\u8d85\u53c2\u6570\u8c03\u4f18\u548c\u6838\u71c3\u6599\u6676\u683c\u4f18\u5316\u7b49\u4efb\u52a1\u4e0a\u8bc4\u4f30\u3002\u7ed3\u679c\u663e\u793a\uff0c\u5bf9\u4e8e\u7b80\u5355\u95ee\u9898\uff0cLLM\u4f18\u5316\u4e0d\u5982\u7ecf\u5178\u6c42\u89e3\u5668\u6709\u7ade\u4e89\u529b\uff0c\u4f46\u5bf9\u4e8e\u7ea6\u675f\u548c\u542f\u53d1\u5f0f\u96be\u4ee5\u5f62\u5f0f\u5316\u7684\u590d\u6742\u9886\u57df\u7279\u5b9a\u4efb\u52a1\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u6613\u7528\u7684\u65b9\u6cd5\u3002", "conclusion": "LLMize\u4e3a\u590d\u6742\u3001\u9886\u57df\u7279\u5b9a\u7684\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u4e14\u6613\u7528\u7684\u6846\u67b6\uff0c\u80fd\u591f\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u76f4\u63a5\u6ce8\u5165\u7ea6\u675f\u548c\u9886\u57df\u77e5\u8bc6\uff0c\u964d\u4f4e\u4e86\u4f18\u5316\u95ee\u9898\u7684\u95e8\u69db\uff0c\u7279\u522b\u9002\u7528\u4e8e\u96be\u4ee5\u5f62\u5f0f\u5316\u7684\u590d\u6742\u4efb\u52a1\u3002"}}
{"id": "2601.02243", "categories": ["eess.SY", "econ.TH"], "pdf": "https://arxiv.org/pdf/2601.02243", "abs": "https://arxiv.org/abs/2601.02243", "authors": ["Ahmed S. Alahmed", "Audun Botterud", "Saurabh Amin", "Ali T. Al-Awami"], "title": "Optimal Dispatch of Electricity and Water in Renewable-Integrated Desalination Plants", "comment": "14 pages, 7 figures, 1 table", "summary": "We develop a mathematical framework for the optimal dispatch of flexible water desalination plants (WDPs) as hybrid generator-load resources. WDPs integrate thermal generation, membrane-based controllable loads, and renewable energy sources, offering unique operational flexibility for power system operations. They can simultaneously participate in two markets: selling desalinated water to a water utility, and bidirectionally transacting electricity with the grid based on their net electricity demand. We formulate the dispatch decision problem of a profit-maximizing WDP, capturing operational, technological, and market-based coupling between water and electricity flows. The threshold-based structure we derive provides computationally tractable coordination suitable for large-scale deployment, offering operational insights into how thermal generation and membrane-based loads complementarily provide continuous bidirectional flexibility. The thresholds are analytically characterized in closed form as explicit functions of technology and tariff parameters. We examine how small changes in the exogenous tariff and technology parameters affect the WDP's profit. Extensive simulations illustrate the optimal WDP's operation, profit, and water-electricity exchange, demonstrating significant improvements relative to benchmark algorithms.", "AI": {"tldr": "\u63d0\u51fa\u6d77\u6c34\u6de1\u5316\u5382\u4f5c\u4e3a\u6df7\u5408\u53d1\u7535-\u8d1f\u8377\u8d44\u6e90\u7684\u6700\u4f18\u8c03\u5ea6\u6570\u5b66\u6846\u67b6\uff0c\u901a\u8fc7\u9608\u503c\u7ed3\u6784\u5b9e\u73b0\u6c34-\u7535\u8026\u5408\u4f18\u5316", "motivation": "\u6d77\u6c34\u6de1\u5316\u5382\u6574\u5408\u70ed\u529b\u53d1\u7535\u3001\u819c\u57fa\u53ef\u63a7\u8d1f\u8377\u548c\u53ef\u518d\u751f\u80fd\u6e90\uff0c\u5177\u6709\u72ec\u7279\u7684\u8fd0\u884c\u7075\u6d3b\u6027\uff0c\u53ef\u540c\u65f6\u53c2\u4e0e\u6c34\u7535\u4e24\u4e2a\u5e02\u573a\uff0c\u4f46\u9700\u8981\u6709\u6548\u7684\u8c03\u5ea6\u6846\u67b6\u6765\u6700\u5927\u5316\u5229\u6da6", "method": "\u5efa\u7acb\u5229\u6da6\u6700\u5927\u5316\u7684\u6d77\u6c34\u6de1\u5316\u5382\u8c03\u5ea6\u51b3\u7b56\u6a21\u578b\uff0c\u6355\u6349\u6c34-\u7535\u6d41\u7684\u8fd0\u884c\u3001\u6280\u672f\u548c\u5e02\u573a\u8026\u5408\uff0c\u63a8\u5bfc\u51fa\u57fa\u4e8e\u9608\u503c\u7684\u7ed3\u6784\uff0c\u63d0\u4f9b\u8ba1\u7b97\u53ef\u884c\u7684\u534f\u8c03\u673a\u5236", "result": "\u9608\u503c\u4ee5\u95ed\u5f0f\u89e3\u6790\u5f62\u5f0f\u8868\u793a\u4e3a\u6280\u672f\u548c\u7535\u4ef7\u53c2\u6570\u7684\u663e\u51fd\u6570\uff0c\u5c0f\u53c2\u6570\u53d8\u5316\u5bf9\u5229\u6da6\u5f71\u54cd\u53ef\u5206\u6790\uff0c\u4eff\u771f\u663e\u793a\u76f8\u6bd4\u57fa\u51c6\u7b97\u6cd5\u6709\u663e\u8457\u6539\u8fdb", "conclusion": "\u9608\u503c\u7ed3\u6784\u4e3a\u5927\u89c4\u6a21\u90e8\u7f72\u63d0\u4f9b\u8ba1\u7b97\u53ef\u884c\u7684\u534f\u8c03\uff0c\u70ed\u529b\u53d1\u7535\u548c\u819c\u57fa\u8d1f\u8377\u4e92\u8865\u63d0\u4f9b\u8fde\u7eed\u53cc\u5411\u7075\u6d3b\u6027\uff0c\u663e\u8457\u63d0\u5347\u6d77\u6c34\u6de1\u5316\u5382\u7684\u7ecf\u6d4e\u6548\u76ca"}}
{"id": "2601.02229", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2601.02229", "abs": "https://arxiv.org/abs/2601.02229", "authors": ["Andreas H Hamel"], "title": "Extended real number arithmetics via Dedekind cuts", "comment": "18 pages", "summary": "It is shown how Dedekind cuts can be used to introduce the extended real numbers along with sound arithmetic laws via one simple rule for the addition of sets. The crucial idea is that the use of the lower and the upper part of the cuts, respectively, leads to two different additions which are known in the literature as inf-addition and sup-addition. Moreover, the two resulting structures are conlinear spaces which at the same time are complete lattices with respect to the natural order. This admits the definition of pseudo-differences on the extended reals which also provide formulas for expressions like $(+\\infty) - (+\\infty)$, $(-\\infty) - (-\\infty)$. There are two major motivations: one is that proper and improper extended real-valued functions can be treated in a unified manner, the other that set-valued functions can often be represented by families of scalar functions which may include improper ones.", "AI": {"tldr": "\u4f7f\u7528\u6234\u5fb7\u91d1\u5206\u5272\u5f15\u5165\u6269\u5c55\u5b9e\u6570\uff0c\u901a\u8fc7\u96c6\u5408\u52a0\u6cd5\u89c4\u5219\u5efa\u7acb\u7b97\u672f\u5b9a\u5f8b\uff0c\u5229\u7528\u5206\u5272\u7684\u4e0b\u90e8\u548c\u4e0a\u90e8\u5206\u522b\u5b9a\u4e49inf-\u52a0\u6cd5\u548csup-\u52a0\u6cd5\uff0c\u5f62\u6210\u5171\u7ebf\u6027\u7a7a\u95f4\u548c\u5b8c\u5907\u683c\u7ed3\u6784\uff0c\u652f\u6301\u4f2a\u5dee\u8fd0\u7b97\u5904\u7406\u65e0\u7a77\u8868\u8fbe\u5f0f\u3002", "motivation": "\u4e3b\u8981\u52a8\u673a\u6709\u4e24\u4e2a\uff1a\u4e00\u662f\u7edf\u4e00\u5904\u7406\u6b63\u5e38\u548c\u5f02\u5e38\u7684\u6269\u5c55\u5b9e\u503c\u51fd\u6570\uff1b\u4e8c\u662f\u96c6\u5408\u503c\u51fd\u6570\u901a\u5e38\u53ef\u4ee5\u7528\u5305\u542b\u5f02\u5e38\u51fd\u6570\u7684\u6807\u91cf\u51fd\u6570\u65cf\u6765\u8868\u793a\u3002", "method": "\u4f7f\u7528\u6234\u5fb7\u91d1\u5206\u5272\u5f15\u5165\u6269\u5c55\u5b9e\u6570\uff0c\u901a\u8fc7\u4e00\u4e2a\u7b80\u5355\u7684\u96c6\u5408\u52a0\u6cd5\u89c4\u5219\u5efa\u7acb\u7b97\u672f\u5b9a\u5f8b\u3002\u5173\u952e\u601d\u60f3\u662f\u5229\u7528\u5206\u5272\u7684\u4e0b\u90e8\u548c\u4e0a\u90e8\u5206\u522b\u5b9a\u4e49\u4e24\u79cd\u4e0d\u540c\u7684\u52a0\u6cd5\uff08inf-\u52a0\u6cd5\u548csup-\u52a0\u6cd5\uff09\uff0c\u5f62\u6210\u5171\u7ebf\u6027\u7a7a\u95f4\u548c\u5b8c\u5907\u683c\u7ed3\u6784\uff0c\u4ece\u800c\u5b9a\u4e49\u4f2a\u5dee\u8fd0\u7b97\u3002", "result": "\u5efa\u7acb\u4e86\u6269\u5c55\u5b9e\u6570\u7684\u7b97\u672f\u7cfb\u7edf\uff0c\u80fd\u591f\u5904\u7406\u5982$(+\\infty) - (+\\infty)$\u548c$(-\\infty) - (-\\infty)$\u7b49\u8868\u8fbe\u5f0f\uff0c\u5f62\u6210\u4e86\u5171\u7ebf\u6027\u7a7a\u95f4\u548c\u5b8c\u5907\u683c\u7ed3\u6784\u3002", "conclusion": "\u6234\u5fb7\u91d1\u5206\u5272\u65b9\u6cd5\u4e3a\u6269\u5c55\u5b9e\u6570\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u7b97\u672f\u6846\u67b6\uff0c\u652f\u6301\u5904\u7406\u65e0\u7a77\u8868\u8fbe\u5f0f\uff0c\u4e3a\u51fd\u6570\u5206\u6790\u548c\u96c6\u5408\u503c\u51fd\u6570\u8868\u793a\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2601.01301", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01301", "abs": "https://arxiv.org/abs/2601.01301", "authors": ["Keith Frankston", "Benjamin Howard"], "title": "Accelerating Monte-Carlo Tree Search with Optimized Posterior Policies", "comment": "11 pages; an efficient implementation is available at https://github.com/bhoward73/rmcts", "summary": "We introduce a recursive AlphaZero-style Monte--Carlo tree search algorithm, \"RMCTS\". The advantage of RMCTS over AlphaZero's MCTS-UCB is speed. In RMCTS, the search tree is explored in a breadth-first manner, so that network inferences naturally occur in large batches. This significantly reduces the GPU latency cost. We find that RMCTS is often more than 40 times faster than MCTS-UCB when searching a single root state, and about 3 times faster when searching a large batch of root states.\n  The recursion in RMCTS is based on computing optimized posterior policies at each game state in the search tree, starting from the leaves and working back up to the root. Here we use the posterior policy explored in \"Monte--Carlo tree search as regularized policy optimization\" (Grill, et al.) Their posterior policy is the unique policy which maximizes the expected reward given estimated action rewards minus a penalty for diverging from the prior policy.\n  The tree explored by RMCTS is not defined in an adaptive manner, as it is in MCTS-UCB. Instead, the RMCTS tree is defined by following prior network policies at each node. This is a disadvantage, but the speedup advantage is more significant, and in practice we find that RMCTS-trained networks match the quality of MCTS-UCB-trained networks in roughly one-third of the training time. We include timing and quality comparisons of RMCTS vs. MCTS-UCB for three games: Connect-4, Dots-and-Boxes, and Othello.", "AI": {"tldr": "\u63d0\u51fa\u9012\u5f52AlphaZero\u98ce\u683c\u7684\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u7b97\u6cd5RMCTS\uff0c\u6bd4\u4f20\u7edfMCTS-UCB\u5feb40\u500d\u4ee5\u4e0a\uff0c\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c11\u4e09\u5206\u4e4b\u4e8c", "motivation": "\u4f20\u7edfAlphaZero\u7684MCTS-UCB\u7b97\u6cd5\u5b58\u5728GPU\u5ef6\u8fdf\u95ee\u9898\uff0c\u7f51\u7edc\u63a8\u7406\u65e0\u6cd5\u6279\u91cf\u5904\u7406\uff0c\u5bfc\u81f4\u641c\u7d22\u901f\u5ea6\u8f83\u6162\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u6811\u641c\u7d22\u7b97\u6cd5\u6765\u52a0\u901f\u8bad\u7ec3\u8fc7\u7a0b\u3002", "method": "\u63d0\u51fa\u9012\u5f52\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22(RMCTS)\uff0c\u91c7\u7528\u5e7f\u5ea6\u4f18\u5148\u641c\u7d22\u65b9\u5f0f\uff0c\u4f7f\u7f51\u7edc\u63a8\u7406\u80fd\u591f\u6279\u91cf\u5904\u7406\u3002\u57fa\u4e8eGrill\u7b49\u4eba\u63d0\u51fa\u7684\u540e\u9a8c\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u4ece\u53f6\u5b50\u8282\u70b9\u5411\u6839\u8282\u70b9\u9012\u5f52\u8ba1\u7b97\u4f18\u5316\u540e\u9a8c\u7b56\u7565\u3002\u6811\u7ed3\u6784\u7531\u5148\u9a8c\u7f51\u7edc\u7b56\u7565\u5b9a\u4e49\u800c\u975e\u81ea\u9002\u5e94\u6784\u5efa\u3002", "result": "\u5728\u5355\u4e2a\u6839\u72b6\u6001\u641c\u7d22\u65f6\uff0cRMCTS\u6bd4MCTS-UCB\u5feb40\u500d\u4ee5\u4e0a\uff1b\u5728\u6279\u91cf\u6839\u72b6\u6001\u641c\u7d22\u65f6\u5feb\u7ea63\u500d\u3002\u4f7f\u7528RMCTS\u8bad\u7ec3\u7684\u7f51\u7edc\u8d28\u91cf\u4e0eMCTS-UCB\u76f8\u5f53\uff0c\u4f46\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c11\u7ea6\u4e09\u5206\u4e4b\u4e8c\u3002\u5728Connect-4\u3001Dots-and-Boxes\u548cOthello\u4e09\u4e2a\u6e38\u620f\u4e2d\u9a8c\u8bc1\u4e86\u6027\u80fd\u4f18\u52bf\u3002", "conclusion": "RMCTS\u901a\u8fc7\u9012\u5f52\u548c\u6279\u91cf\u5904\u7406\u663e\u8457\u52a0\u901f\u4e86\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff0c\u867d\u7136\u727a\u7272\u4e86\u6811\u7684\u81ea\u9002\u5e94\u6027\uff0c\u4f46\u901f\u5ea6\u4f18\u52bf\u660e\u663e\uff0c\u80fd\u591f\u5927\u5e45\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u800c\u4e0d\u635f\u5931\u7f51\u7edc\u8d28\u91cf\u3002"}}
{"id": "2601.01266", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01266", "abs": "https://arxiv.org/abs/2601.01266", "authors": ["Rhitabrat Pokharel", "Hamid Hassanzadeh", "Ameeta Agrawal"], "title": "From Policy to Logic for Efficient and Interpretable Coverage Assessment", "comment": "Accepted at AIMedHealth @ AAAI 2026", "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in interpreting lengthy, complex legal and policy language. However, their reliability can be undermined by hallucinations and inconsistencies, particularly when analyzing subjective and nuanced documents. These challenges are especially critical in medical coverage policy review, where human experts must be able to rely on accurate information. In this paper, we present an approach designed to support human reviewers by making policy interpretation more efficient and interpretable. We introduce a methodology that pairs a coverage-aware retriever with symbolic rule-based reasoning to surface relevant policy language, organize it into explicit facts and rules, and generate auditable rationales. This hybrid system minimizes the number of LLM inferences required which reduces overall model cost. Notably, our approach achieves a 44% reduction in inference cost alongside a 4.5% improvement in F1 score, demonstrating both efficiency and effectiveness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u65b9\u6cd5\uff0c\u7ed3\u5408\u8986\u76d6\u611f\u77e5\u68c0\u7d22\u5668\u548c\u7b26\u53f7\u89c4\u5219\u63a8\u7406\uff0c\u4ee5\u63d0\u9ad8\u533b\u7597\u653f\u7b56\u5ba1\u67e5\u4e2dLLM\u7684\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u51cf\u5c1144%\u63a8\u7406\u6210\u672c\u5e76\u63d0\u53474.5% F1\u5206\u6570\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u89e3\u91ca\u590d\u6742\u6cd5\u5f8b\u548c\u653f\u7b56\u8bed\u8a00\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5904\u7406\u4e3b\u89c2\u548c\u7ec6\u5fae\u6587\u6863\u65f6\u5b58\u5728\u5e7b\u89c9\u548c\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u8fd9\u5728\u533b\u7597\u8986\u76d6\u653f\u7b56\u5ba1\u67e5\u4e2d\u5c24\u4e3a\u5173\u952e\uff0c\u56e0\u4e3a\u4eba\u7c7b\u4e13\u5bb6\u9700\u8981\u4f9d\u8d56\u51c6\u786e\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u65b9\u6cd5\uff1a\u7ed3\u5408\u8986\u76d6\u611f\u77e5\u68c0\u7d22\u5668\u4e0e\u7b26\u53f7\u89c4\u5219\u63a8\u7406\uff0c\u63d0\u53d6\u76f8\u5173\u653f\u7b56\u8bed\u8a00\uff0c\u7ec4\u7ec7\u6210\u660e\u786e\u7684\u4e8b\u5b9e\u548c\u89c4\u5219\uff0c\u751f\u6210\u53ef\u5ba1\u8ba1\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u51cf\u5c11LLM\u63a8\u7406\u6b21\u6570\u3002", "result": "\u5b9e\u73b0\u4e8644%\u7684\u63a8\u7406\u6210\u672c\u964d\u4f4e\u548c4.5%\u7684F1\u5206\u6570\u63d0\u5347\uff0c\u5728\u6548\u7387\u548c\u6548\u679c\u4e0a\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6df7\u5408\u7cfb\u7edf\u652f\u6301\u4eba\u7c7b\u5ba1\u67e5\u5458\uff0c\u4f7f\u653f\u7b56\u89e3\u91ca\u66f4\u9ad8\u6548\u548c\u53ef\u89e3\u91ca\uff0c\u5728\u533b\u7597\u8986\u76d6\u653f\u7b56\u5ba1\u67e5\u4e2d\u5e73\u8861\u4e86\u6210\u672c\u3001\u51c6\u786e\u6027\u548c\u53ef\u5ba1\u8ba1\u6027\u3002"}}
{"id": "2601.01127", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.01127", "abs": "https://arxiv.org/abs/2601.01127", "authors": ["Golbahar Amanpour", "Benyamin Ghojogh"], "title": "Wittgenstein's Family Resemblance Clustering Algorithm", "comment": null, "summary": "This paper, introducing a novel method in philomatics, draws on Wittgenstein's concept of family resemblance from analytic philosophy to develop a clustering algorithm for machine learning. According to Wittgenstein's Philosophical Investigations (1953), family resemblance holds that members of a concept or category are connected by overlapping similarities rather than a single defining property. Consequently, a family of entities forms a chain of items sharing overlapping traits. This philosophical idea naturally lends itself to a graph-based approach in machine learning. Accordingly, we propose the Wittgenstein's Family Resemblance (WFR) clustering algorithm and its kernel variant, kernel WFR. This algorithm computes resemblance scores between neighboring data instances, and after thresholding these scores, a resemblance graph is constructed. The connected components of this graph define the resulting clusters. Simulations on benchmark datasets demonstrate that WFR is an effective nonlinear clustering algorithm that does not require prior knowledge of the number of clusters or assumptions about their shapes.", "AI": {"tldr": "\u57fa\u4e8e\u7ef4\u7279\u6839\u65af\u5766\u5bb6\u65cf\u76f8\u4f3c\u6027\u54f2\u5b66\u6982\u5ff5\u63d0\u51fa\u7684\u65b0\u578b\u805a\u7c7b\u7b97\u6cd5\uff0c\u65e0\u9700\u9884\u8bbe\u7c07\u6570\u91cf\u6216\u5f62\u72b6\u5047\u8bbe", "motivation": "\u5c06\u7ef4\u7279\u6839\u65af\u5766\u7684\u5bb6\u65cf\u76f8\u4f3c\u6027\u54f2\u5b66\u6982\u5ff5\uff08\u6765\u81ea\u300a\u54f2\u5b66\u7814\u7a76\u300b\uff09\u5e94\u7528\u4e8e\u673a\u5668\u5b66\u4e60\u805a\u7c7b\u95ee\u9898\uff0c\u8be5\u6982\u5ff5\u8ba4\u4e3a\u7c7b\u522b\u6210\u5458\u901a\u8fc7\u91cd\u53e0\u76f8\u4f3c\u6027\u800c\u975e\u5355\u4e00\u5b9a\u4e49\u5c5e\u6027\u8fde\u63a5\uff0c\u81ea\u7136\u9002\u5408\u56fe\u5b66\u4e60\u65b9\u6cd5", "method": "\u63d0\u51faWFR\uff08\u7ef4\u7279\u6839\u65af\u5766\u5bb6\u65cf\u76f8\u4f3c\u6027\uff09\u805a\u7c7b\u7b97\u6cd5\u53ca\u5176\u6838\u53d8\u4f53kernel WFR\uff1a\u8ba1\u7b97\u76f8\u90bb\u6570\u636e\u5b9e\u4f8b\u95f4\u7684\u76f8\u4f3c\u5ea6\u5f97\u5206\uff0c\u9608\u503c\u5904\u7406\u540e\u6784\u5efa\u76f8\u4f3c\u56fe\uff0c\u56fe\u7684\u8fde\u901a\u5206\u91cf\u5f62\u6210\u6700\u7ec8\u805a\u7c7b", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u6a21\u62df\u5b9e\u9a8c\u8868\u660e\uff0cWFR\u662f\u4e00\u79cd\u6709\u6548\u7684\u975e\u7ebf\u6027\u805a\u7c7b\u7b97\u6cd5\uff0c\u65e0\u9700\u9884\u5148\u77e5\u9053\u7c07\u6570\u91cf\u6216\u5bf9\u5176\u5f62\u72b6\u505a\u51fa\u5047\u8bbe", "conclusion": "\u6210\u529f\u5c06\u54f2\u5b66\u6982\u5ff5\u8f6c\u5316\u4e3a\u5b9e\u7528\u7684\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff0c\u4e3a\u805a\u7c7b\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u56fe\u57fa\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u590d\u6742\u5f62\u72b6\u548c\u975e\u7ebf\u6027\u6570\u636e\u5206\u5e03\u7684\u573a\u666f"}}
{"id": "2601.00877", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00877", "abs": "https://arxiv.org/abs/2601.00877", "authors": ["Thomas Andrews", "Mark Law", "Sara Ahmadi-Abhari", "Alessandra Russo"], "title": "LearnAD: Learning Interpretable Rules for Brain Networks in Alzheimer's Disease Classification", "comment": "NeurIPS 2025, Data on the Brain & Mind Workshop", "summary": "We introduce LearnAD, a neuro-symbolic method for predicting Alzheimer's disease from brain magnetic resonance imaging data, learning fully interpretable rules. LearnAD applies statistical models, Decision Trees, Random Forests, or GNNs to identify relevant brain connections, and then employs FastLAS to learn global rules. Our best instance outperforms Decision Trees, matches Support Vector Machine accuracy, and performs only slightly below Random Forests and GNNs trained on all features, all while remaining fully interpretable. Ablation studies show that our neuro-symbolic approach improves interpretability with comparable performance to pure statistical models. LearnAD demonstrates how symbolic learning can deepen our understanding of GNN behaviour in clinical neuroscience.", "AI": {"tldr": "LearnAD\u662f\u4e00\u79cd\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u8111\u90e8MRI\u6570\u636e\u9884\u6d4b\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff0c\u5b66\u4e60\u5b8c\u5168\u53ef\u89e3\u91ca\u7684\u89c4\u5219\uff0c\u6027\u80fd\u63a5\u8fd1\u9ed1\u76d2\u6a21\u578b\u4f46\u4fdd\u6301\u5b8c\u5168\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5728\u4e34\u5e8a\u795e\u7ecf\u79d1\u5b66\u4e2d\uff0c\u9700\u8981\u65e2\u80fd\u51c6\u786e\u9884\u6d4b\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u53c8\u80fd\u63d0\u4f9b\u53ef\u89e3\u91ca\u89c4\u5219\u7684\u6a21\u578b\uff0c\u4ee5\u7406\u89e3GNN\u7b49\u9ed1\u76d2\u6a21\u578b\u7684\u884c\u4e3a\u5e76\u52a0\u6df1\u5bf9\u75be\u75c5\u673a\u5236\u7684\u7406\u89e3\u3002", "method": "\u4f7f\u7528\u7edf\u8ba1\u6a21\u578b\u3001\u51b3\u7b56\u6811\u3001\u968f\u673a\u68ee\u6797\u6216GNN\u8bc6\u522b\u76f8\u5173\u8111\u8fde\u63a5\uff0c\u7136\u540e\u5e94\u7528FastLAS\u5b66\u4e60\u5168\u5c40\u89c4\u5219\uff0c\u5f62\u6210\u795e\u7ecf\u7b26\u53f7\u6df7\u5408\u65b9\u6cd5\u3002", "result": "\u6700\u4f73\u5b9e\u4f8b\u4f18\u4e8e\u51b3\u7b56\u6811\uff0c\u5339\u914d\u652f\u6301\u5411\u91cf\u673a\u51c6\u786e\u7387\uff0c\u6027\u80fd\u7565\u4f4e\u4e8e\u968f\u673a\u68ee\u6797\u548cGNN\uff0c\u4f46\u4fdd\u6301\u5b8c\u5168\u53ef\u89e3\u91ca\u6027\uff1b\u6d88\u878d\u7814\u7a76\u663e\u793a\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u5728\u4fdd\u6301\u53ef\u6bd4\u6027\u80fd\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "LearnAD\u5c55\u793a\u4e86\u7b26\u53f7\u5b66\u4e60\u5982\u4f55\u52a0\u6df1\u5bf9\u4e34\u5e8a\u795e\u7ecf\u79d1\u5b66\u4e2dGNN\u884c\u4e3a\u7684\u7406\u89e3\uff0c\u4e3a\u5f00\u53d1\u65e2\u51c6\u786e\u53c8\u53ef\u89e3\u91ca\u7684\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u9884\u6d4b\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2601.02244", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.02244", "abs": "https://arxiv.org/abs/2601.02244", "authors": ["Luca Furieri"], "title": "Characterizing All Locally Exponentially Stabilizing Controllers as a Linear Feedback Plus Learnable Nonlinear Youla Dynamics", "comment": null, "summary": "We derive a state-space characterization of all dynamic state-feedback controllers that make an equilibrium of a nonlinear input-affine continuous-time system locally exponentially stable. Specirically, any controller obtained as the sum of a linear state-feedback $u=Kx$, with $K$ stabilizing the linearized system, and the output of internal locally exponentially stable controller dynamics is itself locally exponentially stabilizing. Conversely, every dynamic state-feedback controller that locally exponentially stabilizes the equilibrium admits such a decomposition. The result can be viewed as a state-space nonlinear Youla-type parametrization specialized to local, rather than global, and exponential, rather than asymptotic, closed-loop stability. The residual locally exponentially stable controller dynamics can be implemented with stable recurrent neural networks and trained as neural ODEs to achieve high closed-loop performance in nonlinear control tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u975e\u7ebf\u6027\u8f93\u5165\u4eff\u5c04\u8fde\u7eed\u65f6\u95f4\u7cfb\u7edf\u7684\u72b6\u6001\u7a7a\u95f4\u63a7\u5236\u5668\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u5c06\u52a8\u6001\u72b6\u6001\u53cd\u9988\u63a7\u5236\u5668\u5206\u89e3\u4e3a\u7ebf\u6027\u7a33\u5b9a\u90e8\u5206\u548c\u5c40\u90e8\u6307\u6570\u7a33\u5b9a\u5185\u90e8\u52a8\u6001\u90e8\u5206\uff0c\u5e76\u8bc1\u660e\u8fd9\u79cd\u5206\u89e3\u7684\u5145\u5206\u5fc5\u8981\u6027\u3002", "motivation": "\u4e3a\u975e\u7ebf\u6027\u7cfb\u7edf\u8bbe\u8ba1\u7a33\u5b9a\u63a7\u5236\u5668\u662f\u63a7\u5236\u7406\u8bba\u7684\u6838\u5fc3\u95ee\u9898\u3002\u73b0\u6709Youla\u53c2\u6570\u5316\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u7ebf\u6027\u7cfb\u7edf\u6216\u5168\u5c40\u6e10\u8fd1\u7a33\u5b9a\u6027\uff0c\u7f3a\u4e4f\u9488\u5bf9\u975e\u7ebf\u6027\u7cfb\u7edf\u5c40\u90e8\u6307\u6570\u7a33\u5b9a\u6027\u7684\u72b6\u6001\u7a7a\u95f4\u53c2\u6570\u5316\u65b9\u6cd5\u3002", "method": "\u63a8\u5bfc\u51fa\u6240\u6709\u4f7f\u975e\u7ebf\u6027\u8f93\u5165\u4eff\u5c04\u8fde\u7eed\u65f6\u95f4\u7cfb\u7edf\u5e73\u8861\u70b9\u5c40\u90e8\u6307\u6570\u7a33\u5b9a\u7684\u52a8\u6001\u72b6\u6001\u53cd\u9988\u63a7\u5236\u5668\u7684\u72b6\u6001\u7a7a\u95f4\u7279\u5f81\u3002\u8bc1\u660e\u4efb\u4f55\u63a7\u5236\u5668\u90fd\u53ef\u4ee5\u8868\u793a\u4e3a\uff1a\u7ebf\u6027\u72b6\u6001\u53cd\u9988\u90e8\u5206\uff08Kx\uff0c\u7a33\u5b9a\u7ebf\u6027\u5316\u7cfb\u7edf\uff09\u52a0\u4e0a\u5185\u90e8\u5c40\u90e8\u6307\u6570\u7a33\u5b9a\u63a7\u5236\u5668\u52a8\u6001\u7684\u8f93\u51fa\u3002", "result": "\u5efa\u7acb\u4e86\u5c40\u90e8\u6307\u6570\u7a33\u5b9a\u63a7\u5236\u5668\u7684\u5145\u5206\u5fc5\u8981\u6761\u4ef6\uff1a\u4efb\u4f55\u5c40\u90e8\u6307\u6570\u7a33\u5b9a\u63a7\u5236\u5668\u90fd\u5141\u8bb8\u8fd9\u6837\u7684\u5206\u89e3\uff0c\u53cd\u4e4b\uff0c\u4efb\u4f55\u8fd9\u6837\u7684\u5206\u89e3\u90fd\u4ea7\u751f\u5c40\u90e8\u6307\u6570\u7a33\u5b9a\u63a7\u5236\u5668\u3002\u8fd9\u53ef\u4ee5\u770b\u4f5c\u662f\u975e\u7ebf\u6027Youla\u578b\u53c2\u6570\u5316\u5728\u5c40\u90e8\u6307\u6570\u7a33\u5b9a\u6027\u4e0b\u7684\u7279\u5316\u3002", "conclusion": "\u8be5\u7ed3\u679c\u4e3a\u975e\u7ebf\u6027\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u7684\u72b6\u6001\u7a7a\u95f4\u53c2\u6570\u5316\u6846\u67b6\uff0c\u5269\u4f59\u7684\u5185\u90e8\u52a8\u6001\u53ef\u4ee5\u7528\u7a33\u5b9a\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0\u5e76\u4f5c\u4e3a\u795e\u7ecfODE\u8bad\u7ec3\uff0c\u4ece\u800c\u5728\u975e\u7ebf\u6027\u63a7\u5236\u4efb\u52a1\u4e2d\u5b9e\u73b0\u9ad8\u6027\u80fd\u95ed\u73af\u63a7\u5236\u3002"}}
{"id": "2601.02347", "categories": ["math.OC", "cs.DS", "cs.GT"], "pdf": "https://arxiv.org/pdf/2601.02347", "abs": "https://arxiv.org/abs/2601.02347", "authors": ["Ishani Karmarkar", "Liam O'Carroll", "Aaron Sidford"], "title": "Solving Matrix Games with Even Fewer Matrix-Vector Products", "comment": null, "summary": "We study the problem of computing an $\u03b5$-approximate Nash equilibrium of a two-player, bilinear, zero-sum game with a bounded payoff matrix $A \\in \\mathbb{R}^{m \\times n}$, when the players' strategies are constrained to lie in simple sets. We provide algorithms which solve this problem in $\\tilde{O}(\u03b5^{-2/3})$ matrix-vector multiplies (matvecs) in two well-studied cases: $\\ell_1$-$\\ell_1$ games, where the players' strategies are both in the probability simplex, and $\\ell_2$-$\\ell_1$ games, where the players' strategies are in the unit Euclidean ball and probability simplex respectively. These results improve upon the previous state-of-the-art complexities of $\\tilde{O}(\u03b5^{-8/9})$ for $\\ell_1$-$\\ell_1$ and of $\\tilde{O}(\u03b5^{-7/9})$ for $\\ell_2$-$\\ell_1$ due to [KOS '25]. In particular, our result for $\\ell_2$-$\\ell_1$, which corresponds to hard-margin support vector machines (SVMs), matches the lower bound of [KS '25] up to polylogarithmic factors.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u5728\u4e24\u79cd\u7ea6\u675f\u6761\u4ef6\u4e0b\u8ba1\u7b97\u53cc\u4eba\u96f6\u548c\u535a\u5f08\u03b5-\u8fd1\u4f3c\u7eb3\u4ec0\u5747\u8861\u7684\u6539\u8fdb\u7b97\u6cd5\uff0c\u5c06\u590d\u6742\u5ea6\u4ece\u4e4b\u524d\u7684$\\tilde{O}(\u03b5^{-8/9})$\u548c$\\tilde{O}(\u03b5^{-7/9})$\u63d0\u5347\u5230$\\tilde{O}(\u03b5^{-2/3})$\u3002", "motivation": "\u7814\u7a76\u53cc\u4eba\u96f6\u548c\u535a\u5f08\u4e2d\u03b5-\u8fd1\u4f3c\u7eb3\u4ec0\u5747\u8861\u7684\u8ba1\u7b97\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u7b56\u7565\u7a7a\u95f4\u53d7\u7ea6\u675f\u7684\u60c5\u51b5\u4e0b\u3002\u73b0\u6709\u7b97\u6cd5\u590d\u6742\u5ea6\u8f83\u9ad8\uff0c\u9700\u8981\u6539\u8fdb\u4ee5\u5339\u914d\u7406\u8bba\u4e0b\u754c\u3002", "method": "\u9488\u5bf9\u4e24\u79cd\u7279\u5b9a\u7ea6\u675f\u60c5\u51b5\uff1a\u2113\u2081-\u2113\u2081\u535a\u5f08\uff08\u53cc\u65b9\u7b56\u7565\u90fd\u5728\u6982\u7387\u5355\u7eaf\u5f62\u4e2d\uff09\u548c\u2113\u2082-\u2113\u2081\u535a\u5f08\uff08\u4e00\u65b9\u5728\u5355\u4f4d\u6b27\u51e0\u91cc\u5f97\u7403\u4e2d\uff0c\u53e6\u4e00\u65b9\u5728\u6982\u7387\u5355\u7eaf\u5f62\u4e2d\uff09\uff0c\u8bbe\u8ba1\u4e86\u65b0\u7684\u7b97\u6cd5\u3002\u8fd9\u4e9b\u7b97\u6cd5\u901a\u8fc7\u4f18\u5316\u77e9\u9635\u5411\u91cf\u4e58\u6cd5\uff08matvecs\uff09\u7684\u590d\u6742\u5ea6\u6765\u5b9e\u73b0\u6539\u8fdb\u3002", "result": "\u5c06\u2113\u2081-\u2113\u2081\u535a\u5f08\u7684\u590d\u6742\u5ea6\u4ece$\\tilde{O}(\u03b5^{-8/9})$\u63d0\u5347\u5230$\\tilde{O}(\u03b5^{-2/3})$\uff0c\u5c06\u2113\u2082-\u2113\u2081\u535a\u5f08\u7684\u590d\u6742\u5ea6\u4ece$\\tilde{O}(\u03b5^{-7/9})$\u63d0\u5347\u5230$\\tilde{O}(\u03b5^{-2/3})$\u3002\u7279\u522b\u5730\uff0c\u2113\u2082-\u2113\u2081\u535a\u5f08\u7684\u7ed3\u679c\uff08\u5bf9\u5e94\u786c\u95f4\u9694\u652f\u6301\u5411\u91cf\u673a\uff09\u4e0e[KS '25]\u7684\u4e0b\u754c\u5339\u914d\uff08\u5ffd\u7565\u5bf9\u6570\u56e0\u5b50\uff09\u3002", "conclusion": "\u672c\u6587\u663e\u8457\u6539\u8fdb\u4e86\u7ea6\u675f\u96f6\u548c\u535a\u5f08\u4e2d\u8fd1\u4f3c\u7eb3\u4ec0\u5747\u8861\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u8fbe\u5230\u4e86\u63a5\u8fd1\u7406\u8bba\u6700\u4f18\u7684$\\tilde{O}(\u03b5^{-2/3})$\u590d\u6742\u5ea6\uff0c\u7279\u522b\u5728\u786c\u95f4\u9694SVM\u95ee\u9898\u4e0a\u5339\u914d\u4e86\u4e0b\u754c\u3002"}}
{"id": "2601.01321", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01321", "abs": "https://arxiv.org/abs/2601.01321", "authors": ["Rong Zhou", "Dongping Chen", "Zihan Jia", "Yao Su", "Yixin Liu", "Yiwen Lu", "Dongwei Shi", "Yue Huang", "Tianyang Xu", "Yi Pan", "Xinliang Li", "Yohannes Abate", "Qingyu Chen", "Zhengzhong Tu", "Yu Yang", "Yu Zhang", "Qingsong Wen", "Gengchen Mai", "Sunyang Fu", "Jiachen Li", "Xuyu Wang", "Ziran Wang", "Jing Huang", "Tianming Liu", "Yong Chen", "Lichao Sun", "Lifang He"], "title": "Digital Twin AI: Opportunities and Challenges from Large Language Models to World Models", "comment": null, "summary": "Digital twins, as precise digital representations of physical systems, have evolved from passive simulation tools into intelligent and autonomous entities through the integration of artificial intelligence technologies. This paper presents a unified four-stage framework that systematically characterizes AI integration across the digital twin lifecycle, spanning modeling, mirroring, intervention, and autonomous management. By synthesizing existing technologies and practices, we distill a unified four-stage framework that systematically characterizes how AI methodologies are embedded across the digital twin lifecycle: (1) modeling the physical twin through physics-based and physics-informed AI approaches, (2) mirroring the physical system into a digital twin with real-time synchronization, (3) intervening in the physical twin through predictive modeling, anomaly detection, and optimization strategies, and (4) achieving autonomous management through large language models, foundation models, and intelligent agents. We analyze the synergy between physics-based modeling and data-driven learning, highlighting the shift from traditional numerical solvers to physics-informed and foundation models for physical systems. Furthermore, we examine how generative AI technologies, including large language models and generative world models, transform digital twins into proactive and self-improving cognitive systems capable of reasoning, communication, and creative scenario generation. Through a cross-domain review spanning eleven application domains, including healthcare, aerospace, smart manufacturing, robotics, and smart cities, we identify common challenges related to scalability, explainability, and trustworthiness, and outline directions for responsible AI-driven digital twin systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u56db\u9636\u6bb5\u6846\u67b6\uff0c\u7cfb\u7edf\u63cf\u8ff0\u4eba\u5de5\u667a\u80fd\u5728\u6570\u5b57\u5b6a\u751f\u5168\u751f\u547d\u5468\u671f\u4e2d\u7684\u96c6\u6210\uff0c\u6db5\u76d6\u5efa\u6a21\u3001\u955c\u50cf\u3001\u5e72\u9884\u548c\u81ea\u4e3b\u7ba1\u7406\u56db\u4e2a\u9636\u6bb5\uff0c\u5e76\u5206\u6790\u4e86AI\u6280\u672f\u5982\u4f55\u5c06\u6570\u5b57\u5b6a\u751f\u4ece\u88ab\u52a8\u4eff\u771f\u5de5\u5177\u8f6c\u53d8\u4e3a\u667a\u80fd\u81ea\u4e3b\u5b9e\u4f53\u3002", "motivation": "\u6570\u5b57\u5b6a\u751f\u4f5c\u4e3a\u7269\u7406\u7cfb\u7edf\u7684\u7cbe\u786e\u6570\u5b57\u8868\u793a\uff0c\u6b63\u4ece\u88ab\u52a8\u4eff\u771f\u5de5\u5177\u5411\u667a\u80fd\u81ea\u4e3b\u5b9e\u4f53\u6f14\u8fdb\u3002\u7136\u800c\uff0cAI\u6280\u672f\u5728\u6570\u5b57\u5b6a\u751f\u4e2d\u7684\u96c6\u6210\u7f3a\u4e4f\u7cfb\u7edf\u6027\u6846\u67b6\uff0c\u9700\u8981\u7edf\u4e00\u7684\u7406\u8bba\u6307\u5bfc\u6765\u7406\u89e3AI\u5982\u4f55\u5728\u4e0d\u540c\u9636\u6bb5\u8d4b\u80fd\u6570\u5b57\u5b6a\u751f\uff0c\u5e76\u63a8\u52a8\u5176\u5411\u8ba4\u77e5\u7cfb\u7edf\u53d1\u5c55\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u7684\u56db\u9636\u6bb5\u6846\u67b6\uff1a1) \u901a\u8fc7\u7269\u7406\u57fa\u7840\u548c\u7269\u7406\u4fe1\u606fAI\u65b9\u6cd5\u5efa\u6a21\u7269\u7406\u5b6a\u751f\uff1b2) \u901a\u8fc7\u5b9e\u65f6\u540c\u6b65\u5c06\u7269\u7406\u7cfb\u7edf\u955c\u50cf\u4e3a\u6570\u5b57\u5b6a\u751f\uff1b3) \u901a\u8fc7\u9884\u6d4b\u5efa\u6a21\u3001\u5f02\u5e38\u68c0\u6d4b\u548c\u4f18\u5316\u7b56\u7565\u5e72\u9884\u7269\u7406\u5b6a\u751f\uff1b4) \u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\u3001\u57fa\u7840\u6a21\u578b\u548c\u667a\u80fd\u4ee3\u7406\u5b9e\u73b0\u81ea\u4e3b\u7ba1\u7406\u3002\u901a\u8fc7\u8de811\u4e2a\u5e94\u7528\u9886\u57df\u7684\u7efc\u8ff0\u5206\u6790\u6280\u672f\u8d8b\u52bf\u548c\u6311\u6218\u3002", "result": "\u5efa\u7acb\u4e86\u7cfb\u7edf\u6027\u7684AI\u96c6\u6210\u6846\u67b6\uff0c\u5206\u6790\u4e86\u7269\u7406\u5efa\u6a21\u4e0e\u6570\u636e\u9a71\u52a8\u5b66\u4e60\u7684\u534f\u540c\u4f5c\u7528\uff0c\u63ed\u793a\u4e86\u4ece\u4f20\u7edf\u6570\u503c\u6c42\u89e3\u5668\u5411\u7269\u7406\u4fe1\u606f\u6a21\u578b\u548c\u57fa\u7840\u6a21\u578b\u7684\u8f6c\u53d8\u3002\u8bc6\u522b\u4e86\u751f\u6210\u5f0fAI\u6280\u672f\u5982\u4f55\u5c06\u6570\u5b57\u5b6a\u751f\u8f6c\u53d8\u4e3a\u5177\u6709\u63a8\u7406\u3001\u901a\u4fe1\u548c\u521b\u9020\u6027\u573a\u666f\u751f\u6210\u80fd\u529b\u7684\u8ba4\u77e5\u7cfb\u7edf\uff0c\u5e76\u6307\u51fa\u4e86\u53ef\u6269\u5c55\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u4fe1\u5ea6\u7b49\u5171\u540c\u6311\u6218\u3002", "conclusion": "AI\u6280\u672f\u6b63\u5728\u5f7b\u5e95\u6539\u53d8\u6570\u5b57\u5b6a\u751f\u7684\u672c\u8d28\uff0c\u4f7f\u5176\u4ece\u88ab\u52a8\u4eff\u771f\u5de5\u5177\u8f6c\u53d8\u4e3a\u4e3b\u52a8\u3001\u81ea\u6539\u8fdb\u7684\u8ba4\u77e5\u7cfb\u7edf\u3002\u7edf\u4e00\u7684\u56db\u9636\u6bb5\u6846\u67b6\u4e3a\u7406\u89e3AI\u5728\u6570\u5b57\u5b6a\u751f\u751f\u547d\u5468\u671f\u4e2d\u7684\u96c6\u6210\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u89c6\u89d2\uff0c\u672a\u6765\u9700\u8981\u89e3\u51b3\u53ef\u6269\u5c55\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u4fe1\u5ea6\u7b49\u6311\u6218\uff0c\u63a8\u52a8\u8d1f\u8d23\u4efbAI\u9a71\u52a8\u7684\u6570\u5b57\u5b6a\u751f\u7cfb\u7edf\u53d1\u5c55\u3002"}}
{"id": "2601.01280", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01280", "abs": "https://arxiv.org/abs/2601.01280", "authors": ["Sen Hu", "Yuxiang Wei", "Jiaxin Ran", "Zhiyuan Yao", "Lei Zou"], "title": "Does Memory Need Graphs? A Unified Framework and Empirical Analysis for Long-Term Dialog Memory", "comment": null, "summary": "Graph structures are increasingly used in dialog memory systems, but empirical findings on their effectiveness remain inconsistent, making it unclear which design choices truly matter. We present an experimental, system-oriented analysis of long-term dialog memory architectures. We introduce a unified framework that decomposes dialog memory systems into core components and supports both graph-based and non-graph approaches. Under this framework, we conduct controlled, stage-wise experiments on LongMemEval and HaluMem, comparing common design choices in memory representation, organization, maintenance, and retrieval. Our results show that many performance differences are driven by foundational system settings rather than specific architectural innovations. Based on these findings, we identify stable and reliable strong baselines for future dialog memory research.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u5206\u89e3\u5bf9\u8bdd\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u5b9e\u9a8c\u53d1\u73b0\u6027\u80fd\u5dee\u5f02\u4e3b\u8981\u6e90\u4e8e\u57fa\u7840\u7cfb\u7edf\u8bbe\u7f6e\u800c\u975e\u7279\u5b9a\u67b6\u6784\u521b\u65b0\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u7a33\u5b9a\u53ef\u9760\u7684\u57fa\u7ebf\u3002", "motivation": "\u56fe\u7ed3\u6784\u5728\u5bf9\u8bdd\u8bb0\u5fc6\u7cfb\u7edf\u4e2d\u5e94\u7528\u65e5\u76ca\u589e\u591a\uff0c\u4f46\u5b9e\u8bc1\u7ed3\u679c\u4e0d\u4e00\u81f4\uff0c\u4e0d\u6e05\u695a\u54ea\u4e9b\u8bbe\u8ba1\u9009\u62e9\u771f\u6b63\u91cd\u8981\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u7684\u5b9e\u9a8c\u5206\u6790\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u6846\u67b6\u5c06\u5bf9\u8bdd\u8bb0\u5fc6\u7cfb\u7edf\u5206\u89e3\u4e3a\u6838\u5fc3\u7ec4\u4ef6\uff0c\u652f\u6301\u56fe\u548c\u975e\u56fe\u65b9\u6cd5\uff0c\u5728LongMemEval\u548cHaluMem\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5206\u9636\u6bb5\u63a7\u5236\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u8bb0\u5fc6\u8868\u793a\u3001\u7ec4\u7ec7\u3001\u7ef4\u62a4\u548c\u68c0\u7d22\u7684\u5e38\u89c1\u8bbe\u8ba1\u9009\u62e9\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8bb8\u591a\u6027\u80fd\u5dee\u5f02\u662f\u7531\u57fa\u7840\u7cfb\u7edf\u8bbe\u7f6e\u9a71\u52a8\uff0c\u800c\u975e\u7279\u5b9a\u67b6\u6784\u521b\u65b0\uff0c\u57fa\u4e8e\u6b64\u8bc6\u522b\u51fa\u4e86\u7a33\u5b9a\u53ef\u9760\u7684\u5f3a\u57fa\u7ebf\u3002", "conclusion": "\u4e3a\u672a\u6765\u5bf9\u8bdd\u8bb0\u5fc6\u7814\u7a76\u63d0\u4f9b\u4e86\u7a33\u5b9a\u53ef\u9760\u7684\u57fa\u7ebf\uff0c\u5f3a\u8c03\u57fa\u7840\u7cfb\u7edf\u8bbe\u7f6e\u7684\u91cd\u8981\u6027\u8d85\u8fc7\u7279\u5b9a\u67b6\u6784\u521b\u65b0\u3002"}}
{"id": "2601.01207", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.01207", "abs": "https://arxiv.org/abs/2601.01207", "authors": ["Yoonhyuk Choi", "Jiho Choi", "Chanran Kim", "Yumin Lee", "Hawon Shin", "Yeowon Jeon", "Minjeong Kim", "Jiwoo Kang"], "title": "Sparse Bayesian Message Passing under Structural Uncertainty", "comment": null, "summary": "Semi-supervised learning on real-world graphs is frequently challenged by heterophily, where the observed graph is unreliable or label-disassortative. Many existing graph neural networks either rely on a fixed adjacency structure or attempt to handle structural noise through regularization. In this work, we explicitly capture structural uncertainty by modeling a posterior distribution over signed adjacency matrices, allowing each edge to be positive, negative, or absent. We propose a sparse signed message passing network that is naturally robust to edge noise and heterophily, which can be interpreted from a Bayesian perspective. By combining (i) posterior marginalization over signed graph structures with (ii) sparse signed message aggregation, our approach offers a principled way to handle both edge noise and heterophily. Experimental results demonstrate that our method outperforms strong baseline models on heterophilic benchmarks under both synthetic and real-world structural noise.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u540e\u9a8c\u5206\u5e03\u5efa\u6a21\u7684\u7a00\u758f\u7b26\u53f7\u6d88\u606f\u4f20\u9012\u7f51\u7edc\uff0c\u901a\u8fc7\u5efa\u6a21\u5e26\u7b26\u53f7\u90bb\u63a5\u77e9\u9635\u7684\u540e\u9a8c\u5206\u5e03\u6765\u5904\u7406\u5f02\u8d28\u56fe\u7ed3\u6784\u4e2d\u7684\u8fb9\u566a\u58f0\u548c\u5f02\u8d28\u6027\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u56fe\u4e2d\u7684\u534a\u76d1\u7763\u5b66\u4e60\u5e38\u9762\u4e34\u5f02\u8d28\u6027\u6311\u6218\uff0c\u73b0\u6709\u56fe\u795e\u7ecf\u7f51\u7edc\u8981\u4e48\u4f9d\u8d56\u56fa\u5b9a\u90bb\u63a5\u7ed3\u6784\uff0c\u8981\u4e48\u901a\u8fc7\u6b63\u5219\u5316\u5904\u7406\u7ed3\u6784\u566a\u58f0\uff0c\u7f3a\u4e4f\u5bf9\u7ed3\u6784\u4e0d\u786e\u5b9a\u6027\u7684\u663e\u5f0f\u5efa\u6a21\u3002", "method": "1) \u5efa\u6a21\u5e26\u7b26\u53f7\u90bb\u63a5\u77e9\u9635\u7684\u540e\u9a8c\u5206\u5e03\uff0c\u6bcf\u6761\u8fb9\u53ef\u4ee5\u662f\u6b63\u3001\u8d1f\u6216\u4e0d\u5b58\u5728\uff1b2) \u63d0\u51fa\u7a00\u758f\u7b26\u53f7\u6d88\u606f\u4f20\u9012\u7f51\u7edc\uff0c\u4ece\u8d1d\u53f6\u65af\u89c6\u89d2\u89e3\u91ca\u5176\u9c81\u68d2\u6027\uff1b3) \u7ed3\u5408\u540e\u9a8c\u8fb9\u7f18\u5316\u548c\u7a00\u758f\u7b26\u53f7\u6d88\u606f\u805a\u5408\u3002", "result": "\u5728\u5f02\u8d28\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u7ed3\u6784\u566a\u58f0\u4e0b\u5747\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u7ed3\u6784\u4e0d\u786e\u5b9a\u6027\uff0c\u8be5\u65b9\u6cd5\u4e3a\u5904\u7406\u8fb9\u566a\u58f0\u548c\u5f02\u8d28\u6027\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u65b9\u6cd5\uff0c\u7a00\u758f\u7b26\u53f7\u6d88\u606f\u4f20\u9012\u7f51\u7edc\u5728\u5f02\u8d28\u56fe\u4e0a\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2601.00883", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00883", "abs": "https://arxiv.org/abs/2601.00883", "authors": ["Zhongyang Shen"], "title": "Outlier Detection Using Vector Cosine Similarity by Adding a Dimension", "comment": "This is an updated version of the paper originally published in ICAIIC 2024 (DOI: 10.1109/ICAIIC60209.2024.10463442). Changes include minor typographical and grammatical corrections, as well as an added description of an optimized open-source Python implementation (MDOD) available on PyPI at https://pypi.org/project/mdod/", "summary": "We propose a new outlier detection method for multi-dimensional data. The method detects outliers based on vector cosine similarity, using a new dataset constructed by adding a dimension with zero values to the original data. When a point in the new dataset is selected as the measured point, an observation point is created as the origin, differing only in the new dimension by having a non-zero value compared to the measured point. Vectors are then formed from the observation point to the measured point and to other points in the dataset. By comparing the cosine similarities of these vectors, abnormal data can be identified. An optimized implementation (MDOD) is available on PyPI: https://pypi.org/project/mdod/.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5411\u91cf\u4f59\u5f26\u76f8\u4f3c\u5ea6\u7684\u591a\u7ef4\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u6dfb\u52a0\u96f6\u503c\u7ef4\u5ea6\u6784\u5efa\u65b0\u6570\u636e\u96c6\uff0c\u6bd4\u8f83\u89c2\u6d4b\u70b9\u5230\u6d4b\u91cf\u70b9\u53ca\u5176\u4ed6\u70b9\u7684\u5411\u91cf\u76f8\u4f3c\u5ea6\u6765\u8bc6\u522b\u5f02\u5e38", "motivation": "\u73b0\u6709\u591a\u7ef4\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u53ef\u80fd\u590d\u6742\u6216\u6548\u679c\u6709\u9650\uff0c\u9700\u8981\u66f4\u7b80\u5355\u6709\u6548\u7684\u6280\u672f\u6765\u8bc6\u522b\u9ad8\u7ef4\u6570\u636e\u4e2d\u7684\u5f02\u5e38\u70b9", "method": "1) \u5728\u539f\u59cb\u6570\u636e\u4e2d\u6dfb\u52a0\u96f6\u503c\u7ef4\u5ea6\u6784\u5efa\u65b0\u6570\u636e\u96c6\uff1b2) \u9009\u62e9\u6d4b\u91cf\u70b9\u5e76\u521b\u5efa\u89c2\u6d4b\u70b9\uff08\u539f\u70b9\uff0c\u4ec5\u5728\u65b0\u7ef4\u5ea6\u6709\u975e\u96f6\u503c\uff09\uff1b3) \u5f62\u6210\u4ece\u89c2\u6d4b\u70b9\u5230\u6d4b\u91cf\u70b9\u53ca\u5176\u4ed6\u70b9\u7684\u5411\u91cf\uff1b4) \u6bd4\u8f83\u8fd9\u4e9b\u5411\u91cf\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\u6765\u68c0\u6d4b\u5f02\u5e38", "result": "\u5f00\u53d1\u4e86MDOD\u4f18\u5316\u5b9e\u73b0\u5e76\u5728PyPI\u53d1\u5e03\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u7ef4\u5f02\u5e38\u68c0\u6d4b\u5de5\u5177", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u521b\u65b0\u7684\u5411\u91cf\u76f8\u4f3c\u5ea6\u6bd4\u8f83\u673a\u5236\uff0c\u4e3a\u591a\u7ef4\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.02275", "categories": ["eess.SY", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.02275", "abs": "https://arxiv.org/abs/2601.02275", "authors": ["Shrenik Jadhav", "Zheng Liu"], "title": "Machine Learning Guided Cooling Optimization for Data Centers", "comment": "10 pages, 11 figures", "summary": "Effective data center cooling is crucial for reliable operation; however, cooling systems often exhibit inefficiencies that result in excessive energy consumption. This paper presents a three-stage, physics-guided machine learning framework for identifying and reducing cooling energy waste in high-performance computing facilities. Using one year of 10-minute resolution operational data from the Frontier exascale supercomputer, we first train a monotonicity-constrained gradient boosting surrogate that predicts facility accessory power from coolant flow rates, temperatures, and server power. The surrogate achieves a mean absolute error of 0.026 MW and predicts power usage effectiveness within 0.01 of measured values for 98.7% of test samples. In the second stage, the surrogate serves as a physics-consistent baseline to quantify excess cooling energy, revealing approximately 85 MWh of annual inefficiency concentrated in specific months, hours, and operating regimes. The third stage evaluates guardrail-constrained counterfactual adjustments to supply temperature and subloop flows, demonstrating that up to 96% of identified excess can be recovered through small, safe setpoint changes while respecting thermal limits and operational constraints. The framework yields interpretable recommendations, supports counterfactual analyses such as flow reduction during low-load periods and redistribution of thermal duty across cooling loops, and provides a practical pathway toward quantifiable reductions in accessory power. The developed framework is readily compatible with model predictive control and can be extended to other liquid-cooled data centers with different configurations and cooling requirements.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u4e09\u9636\u6bb5\u7269\u7406\u5f15\u5bfc\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u8bc6\u522b\u548c\u51cf\u5c11\u9ad8\u6027\u80fd\u8ba1\u7b97\u8bbe\u65bd\u4e2d\u7684\u51b7\u5374\u80fd\u8017\u6d6a\u8d39\uff0c\u901a\u8fc7\u6784\u5efa\u5355\u8c03\u6027\u7ea6\u675f\u7684\u68af\u5ea6\u63d0\u5347\u4ee3\u7406\u6a21\u578b\u91cf\u5316\u51b7\u5374\u6548\u7387\u4f4e\u4e0b\uff0c\u5e76\u8bc4\u4f30\u5b89\u5168\u8bbe\u5b9a\u70b9\u8c03\u6574\u4ee5\u56de\u6536\u9ad8\u8fbe96%\u7684\u8fc7\u91cf\u80fd\u8017\u3002", "motivation": "\u6570\u636e\u4e2d\u5fc3\u51b7\u5374\u7cfb\u7edf\u5bf9\u53ef\u9760\u8fd0\u884c\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u901a\u5e38\u5b58\u5728\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\uff0c\u5bfc\u81f4\u80fd\u6e90\u6d88\u8017\u8fc7\u5ea6\u3002\u9700\u8981\u4e00\u79cd\u7cfb\u7edf\u65b9\u6cd5\u6765\u8bc6\u522b\u548c\u51cf\u5c11\u51b7\u5374\u80fd\u8017\u6d6a\u8d39\uff0c\u7279\u522b\u662f\u5728\u9ad8\u6027\u80fd\u8ba1\u7b97\u8bbe\u65bd\u4e2d\u3002", "method": "\u4e09\u9636\u6bb5\u7269\u7406\u5f15\u5bfc\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff1a1) \u4f7f\u7528\u5355\u8c03\u6027\u7ea6\u675f\u68af\u5ea6\u63d0\u5347\u5efa\u7acb\u9884\u6d4b\u8bbe\u65bd\u8f85\u52a9\u529f\u7387\u7684\u4ee3\u7406\u6a21\u578b\uff1b2) \u4f7f\u7528\u4ee3\u7406\u6a21\u578b\u4f5c\u4e3a\u7269\u7406\u4e00\u81f4\u57fa\u7ebf\u91cf\u5316\u8fc7\u91cf\u51b7\u5374\u80fd\u8017\uff1b3) \u8bc4\u4f30\u62a4\u680f\u7ea6\u675f\u7684\u53cd\u4e8b\u5b9e\u8c03\u6574\uff08\u4f9b\u5e94\u6e29\u5ea6\u548c\u5b50\u56de\u8def\u6d41\u91cf\uff09\uff0c\u5728\u5c0a\u91cd\u70ed\u9650\u503c\u548c\u64cd\u4f5c\u7ea6\u675f\u4e0b\u56de\u6536\u8fc7\u91cf\u80fd\u8017\u3002", "result": "\u4ee3\u7406\u6a21\u578b\u5b9e\u73b00.026 MW\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff0c98.7%\u6d4b\u8bd5\u6837\u672c\u7684\u529f\u7387\u4f7f\u7528\u6548\u7387\u9884\u6d4b\u8bef\u5dee\u57280.01\u4ee5\u5185\u3002\u8bc6\u522b\u51fa\u7ea685 MWh\u7684\u5e74\u6548\u7387\u4f4e\u4e0b\uff0c\u96c6\u4e2d\u5728\u7279\u5b9a\u6708\u4efd\u3001\u5c0f\u65f6\u548c\u64cd\u4f5c\u72b6\u6001\u3002\u901a\u8fc7\u5b89\u5168\u8bbe\u5b9a\u70b9\u8c03\u6574\u53ef\u56de\u6536\u9ad8\u8fbe96%\u7684\u8fc7\u91cf\u80fd\u8017\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u5efa\u8bae\uff0c\u652f\u6301\u53cd\u4e8b\u5b9e\u5206\u6790\uff08\u5982\u4f4e\u8d1f\u8f7d\u671f\u95f4\u6d41\u91cf\u51cf\u5c11\u548c\u51b7\u5374\u56de\u8def\u95f4\u70ed\u8d1f\u8377\u91cd\u65b0\u5206\u914d\uff09\uff0c\u4e3a\u51cf\u5c11\u8f85\u52a9\u529f\u7387\u63d0\u4f9b\u5b9e\u7528\u9014\u5f84\u3002\u6846\u67b6\u517c\u5bb9\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff0c\u53ef\u6269\u5c55\u5230\u5176\u4ed6\u6db2\u51b7\u6570\u636e\u4e2d\u5fc3\u3002"}}
{"id": "2601.01330", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01330", "abs": "https://arxiv.org/abs/2601.01330", "authors": ["Shengji Tang", "Weihao Lin", "Jingqi Ye", "Hao Li", "Bo Zhang", "Shuyue Hu", "Tao Chen", "Wangli Ouyang", "Lei Bai", "Peng Ye"], "title": "Beyond Gemini-3-Pro: Revisiting LLM Routing and Aggregation at Scale", "comment": "12 pages", "summary": "Large Language Models (LLMs) have rapidly advanced, with Gemini-3-Pro setting a new performance milestone. In this work, we explore collective intelligence as an alternative to monolithic scaling, and demonstrate that open-source LLMs' collaboration can surpass Gemini-3-Pro. We first revisit LLM routing and aggregation at scale and identify three key bottlenecks: (1) current train-free routers are limited by a query-based paradigm focusing solely on textual similarity; (2) recent aggregation methods remain largely static, failing to select appropriate aggregators for different tasks;(3) the complementarity of routing and aggregation remains underutilized. To address these problems, we introduce JiSi, a novel framework designed to release the full potential of LLMs' collaboration through three innovations: (1) Query-Response Mixed Routing capturing both semantic information and problem difficulty; (2) Support-Set-based Aggregator Selection jointly evaluating the aggregation and domain capacity of aggregators; (3) Adaptive Routing-Aggregation Switch dynamically leveraging the advantages of routing and aggregation. Comprehensive experiments on nine benchmarks demonstrate that JiSi can surpass Gemini-3-Pro with only 47% costs by orchestrating ten open-source LLMs, while outperforming mainstream baselines. It suggests that collective intelligence represents a novel path towards Artificial General Intelligence (AGI).", "AI": {"tldr": "JiSi\u6846\u67b6\u901a\u8fc7\u67e5\u8be2-\u54cd\u5e94\u6df7\u5408\u8def\u7531\u3001\u652f\u6301\u96c6\u805a\u5408\u5668\u9009\u62e9\u548c\u81ea\u9002\u5e94\u8def\u7531-\u805a\u5408\u5207\u6362\uff0c\u4f7f\u5f00\u6e90LLM\u534f\u4f5c\u8d85\u8d8aGemini-3-Pro\u6027\u80fd\uff0c\u6210\u672c\u4ec547%", "motivation": "\u63a2\u7d22\u96c6\u4f53\u667a\u80fd\u4f5c\u4e3a\u66ff\u4ee3\u5355\u4f53\u6a21\u578b\u6269\u5c55\u7684\u65b0\u8def\u5f84\uff0c\u89e3\u51b3\u5f53\u524dLLM\u8def\u7531\u548c\u805a\u5408\u7684\u4e09\u4e2a\u5173\u952e\u74f6\u9888\uff1a\u67e5\u8be2\u5f0f\u8def\u7531\u5c40\u9650\u3001\u9759\u6001\u805a\u5408\u65b9\u6cd5\u3001\u8def\u7531\u4e0e\u805a\u5408\u4e92\u8865\u6027\u672a\u5145\u5206\u5229\u7528", "method": "\u63d0\u51faJiSi\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u521b\u65b0\uff1a1) \u67e5\u8be2-\u54cd\u5e94\u6df7\u5408\u8def\u7531\u540c\u65f6\u6355\u6349\u8bed\u4e49\u4fe1\u606f\u548c\u95ee\u9898\u96be\u5ea6\uff1b2) \u652f\u6301\u96c6\u805a\u5408\u5668\u9009\u62e9\u8bc4\u4f30\u805a\u5408\u80fd\u529b\u548c\u9886\u57df\u80fd\u529b\uff1b3) \u81ea\u9002\u5e94\u8def\u7531-\u805a\u5408\u5207\u6362\u52a8\u6001\u5229\u7528\u8def\u7531\u548c\u805a\u5408\u4f18\u52bf", "result": "\u57289\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cJiSi\u901a\u8fc7\u534f\u8c0310\u4e2a\u5f00\u6e90LLM\uff0c\u4ee5\u4ec547%\u7684\u6210\u672c\u8d85\u8d8a\u4e86Gemini-3-Pro\u6027\u80fd\uff0c\u540c\u65f6\u4f18\u4e8e\u4e3b\u6d41\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u96c6\u4f53\u667a\u80fd\u4ee3\u8868\u4e86\u901a\u5f80AGI\u7684\u65b0\u8def\u5f84\uff0c\u901a\u8fc7\u6709\u6548\u534f\u4f5c\u5f00\u6e90LLM\u53ef\u4ee5\u5b9e\u73b0\u8d85\u8d8a\u9876\u7ea7\u5355\u4f53\u6a21\u578b\u7684\u6027\u80fd"}}
{"id": "2601.01299", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01299", "abs": "https://arxiv.org/abs/2601.01299", "authors": ["Ismail Lamaakal", "Chaymae Yahyati", "Yassine Maleh", "Khalid El Makkaoui", "Ibrahim Ouahbi"], "title": "T3C: Test-Time Tensor Compression with Consistency Guarantees", "comment": null, "summary": "We present T3C, a train-once, test-time budget-conditioned compression framework that exposes rank and precision as a controllable deployment knob. T3C combines elastic tensor factorization (maintained up to a maximal rank) with rank-tied mixed-precision quantization and a lightweight controller that maps a latency/energy/size budget token to per-layer rank/bit assignments; the policy snaps to hardware-aligned profiles and is monotone in the budget. A fast, layerwise consistency certificate, computed from spectral proxies and activation statistics, upper-bounds logit drift and regularizes training, yielding a practical reliability signal with negligible overhead. On ImageNet-1k, T3C shifts the vision Pareto frontier: for ResNet-50 at matched accuracy (\\leq 0.5% drop), p50 latency is 1.18ms with a 38MB model, outperforming PTQ-8b (1.44ms, 88MB); for ViT-B/16, T3C reaches 2.30ms p50 with 59MB, improving over strong PTQ/QAT baselines. A single T3C checkpoint therefore provides predictable, certificate-backed accuracy-latency-size trade-offs on demand across devices.", "AI": {"tldr": "T3C\u662f\u4e00\u4e2a\u8bad\u7ec3\u4e00\u6b21\u3001\u6d4b\u8bd5\u65f6\u9884\u7b97\u6761\u4ef6\u5316\u7684\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u5f39\u6027\u5f20\u91cf\u5206\u89e3\u548c\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u5b9e\u73b0\u53ef\u63a7\u5236\u7684\u90e8\u7f72\u8c03\u6574\uff0c\u63d0\u4f9b\u53ef\u9884\u6d4b\u7684\u7cbe\u5ea6-\u5ef6\u8fdf-\u5927\u5c0f\u6743\u8861\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u538b\u7f29\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u4e3a\u4e0d\u540c\u90e8\u7f72\u573a\u666f\u91cd\u65b0\u8bad\u7ec3\u6216\u8c03\u6574\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u3001\u53ef\u9884\u6d4b\u7684\u7cbe\u5ea6-\u5ef6\u8fdf-\u5927\u5c0f\u6743\u8861\u65b9\u6848\u3002\u9700\u8981\u4e00\u79cd\u8bad\u7ec3\u4e00\u6b21\u5c31\u80fd\u9002\u5e94\u591a\u79cd\u90e8\u7f72\u9884\u7b97\u7684\u7075\u6d3b\u6846\u67b6\u3002", "method": "\u7ed3\u5408\u5f39\u6027\u5f20\u91cf\u5206\u89e3\uff08\u4fdd\u6301\u6700\u5927\u79e9\uff09\u3001\u79e9\u7ed1\u5b9a\u7684\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\uff0c\u4ee5\u53ca\u8f7b\u91cf\u7ea7\u63a7\u5236\u5668\u5c06\u9884\u7b97\u4ee4\u724c\u6620\u5c04\u5230\u6bcf\u5c42\u79e9/\u6bd4\u7279\u5206\u914d\u3002\u4f7f\u7528\u57fa\u4e8e\u8c31\u4ee3\u7406\u548c\u6fc0\u6d3b\u7edf\u8ba1\u7684\u5feb\u901f\u5c42\u4e00\u81f4\u6027\u8bc1\u4e66\u6765\u4e0a\u754clogit\u6f02\u79fb\u5e76\u6b63\u5219\u5316\u8bad\u7ec3\u3002", "result": "\u5728ImageNet-1k\u4e0a\u663e\u8457\u63a8\u8fdb\u89c6\u89c9Pareto\u524d\u6cbf\uff1aResNet-50\u5728\u7cbe\u5ea6\u4e0b\u964d\u22640.5%\u65f6\u8fbe\u52301.18ms p50\u5ef6\u8fdf\u548c38MB\u6a21\u578b\u5927\u5c0f\uff0c\u4f18\u4e8ePTQ-8b\uff081.44ms, 88MB\uff09\uff1bViT-B/16\u8fbe\u52302.30ms p50\u548c59MB\uff0c\u8d85\u8d8a\u5f3aPTQ/QAT\u57fa\u7ebf\u3002", "conclusion": "\u5355\u4e2aT3C\u68c0\u67e5\u70b9\u5373\u53ef\u63d0\u4f9b\u53ef\u9884\u6d4b\u7684\u3001\u8bc1\u4e66\u652f\u6301\u7684\u7cbe\u5ea6-\u5ef6\u8fdf-\u5927\u5c0f\u6743\u8861\uff0c\u80fd\u591f\u6309\u9700\u9002\u5e94\u4e0d\u540c\u8bbe\u5907\u7684\u90e8\u7f72\u9700\u6c42\uff0c\u5b9e\u73b0\u4e86\u8bad\u7ec3\u4e00\u6b21\u3001\u7075\u6d3b\u90e8\u7f72\u7684\u5b9e\u7528\u538b\u7f29\u6846\u67b6\u3002"}}
{"id": "2601.01223", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.01223", "abs": "https://arxiv.org/abs/2601.01223", "authors": ["Marzieh Amiri Shahbazi", "Ali Baheri", "Nasibeh Azadeh-Fard"], "title": "Adaptive Conformal Prediction via Bayesian Uncertainty Weighting for Hierarchical Healthcare Data", "comment": null, "summary": "Clinical decision-making demands uncertainty quantification that provides both distribution-free coverage guarantees and risk-adaptive precision, requirements that existing methods fail to jointly satisfy. We present a hybrid Bayesian-conformal framework that addresses this fundamental limitation in healthcare predictions. Our approach integrates Bayesian hierarchical random forests with group-aware conformal calibration, using posterior uncertainties to weight conformity scores while maintaining rigorous coverage validity. Evaluated on 61,538 admissions across 3,793 U.S. hospitals and 4 regions, our method achieves target coverage (94.3% vs 95% target) with adaptive precision: 21% narrower intervals for low-uncertainty cases while appropriately widening for high-risk predictions. Critically, we demonstrate that well-calibrated Bayesian uncertainties alone severely under-cover (14.1%), highlighting the necessity of our hybrid approach. This framework enables risk-stratified clinical protocols, efficient resource planning for high-confidence predictions, and conservative allocation with enhanced oversight for uncertain cases, providing uncertainty-aware decision support across diverse healthcare settings.", "AI": {"tldr": "\u63d0\u51fa\u6df7\u5408\u8d1d\u53f6\u65af-\u4fdd\u5f62\u6846\u67b6\uff0c\u7ed3\u5408\u8d1d\u53f6\u65af\u5c42\u6b21\u968f\u673a\u68ee\u6797\u4e0e\u7ec4\u611f\u77e5\u4fdd\u5f62\u6821\u51c6\uff0c\u4e3a\u4e34\u5e8a\u51b3\u7b56\u63d0\u4f9b\u5206\u5e03\u81ea\u7531\u7684\u8986\u76d6\u4fdd\u8bc1\u548c\u98ce\u9669\u81ea\u9002\u5e94\u7cbe\u5ea6\u3002", "motivation": "\u4e34\u5e8a\u51b3\u7b56\u9700\u8981\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u65e2\u8981\u6709\u5206\u5e03\u81ea\u7531\u7684\u8986\u76d6\u4fdd\u8bc1\uff0c\u53c8\u8981\u6709\u98ce\u9669\u81ea\u9002\u5e94\u7684\u7cbe\u5ea6\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u540c\u65f6\u6ee1\u8db3\u8fd9\u4e24\u4e2a\u8981\u6c42\u3002", "method": "\u96c6\u6210\u8d1d\u53f6\u65af\u5c42\u6b21\u968f\u673a\u68ee\u6797\u4e0e\u7ec4\u611f\u77e5\u4fdd\u5f62\u6821\u51c6\uff0c\u4f7f\u7528\u540e\u9a8c\u4e0d\u786e\u5b9a\u6027\u5bf9\u4fdd\u5f62\u5206\u6570\u8fdb\u884c\u52a0\u6743\uff0c\u540c\u65f6\u4fdd\u6301\u4e25\u683c\u7684\u8986\u76d6\u6709\u6548\u6027\u3002", "result": "\u572861,538\u4f8b\u5165\u9662\u60a3\u8005\uff083,793\u5bb6\u7f8e\u56fd\u533b\u9662\uff0c4\u4e2a\u5730\u533a\uff09\u8bc4\u4f30\u4e2d\uff0c\u8fbe\u5230\u76ee\u6807\u8986\u76d6\u7387\uff0894.3% vs 95%\u76ee\u6807\uff09\uff0c\u4f4e\u4e0d\u786e\u5b9a\u6027\u75c5\u4f8b\u533a\u95f4\u5bbd\u5ea6\u51cf\u5c1121%\uff0c\u9ad8\u98ce\u9669\u9884\u6d4b\u9002\u5f53\u52a0\u5bbd\u3002\u7eaf\u8d1d\u53f6\u65af\u4e0d\u786e\u5b9a\u6027\u4e25\u91cd\u8986\u76d6\u4e0d\u8db3\uff0814.1%\uff09\u3002", "conclusion": "\u8be5\u6846\u67b6\u652f\u6301\u98ce\u9669\u5206\u5c42\u4e34\u5e8a\u534f\u8bae\u3001\u9ad8\u6548\u8d44\u6e90\u89c4\u5212\uff08\u9ad8\u7f6e\u4fe1\u5ea6\u9884\u6d4b\uff09\u548c\u4fdd\u5b88\u5206\u914d\uff08\u4e0d\u786e\u5b9a\u75c5\u4f8b\uff09\uff0c\u4e3a\u591a\u6837\u5316\u533b\u7597\u73af\u5883\u63d0\u4f9b\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u51b3\u7b56\u652f\u6301\u3002"}}
{"id": "2601.00889", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00889", "abs": "https://arxiv.org/abs/2601.00889", "authors": ["Nalin Dhiman"], "title": "FANoS: Friction-Adaptive Nos\u00e9--Hoover Symplectic Momentum for Stiff Objectives", "comment": "13 pages, 5 figures, 4 tables", "summary": "We study a physics-inspired optimizer, \\emph{FANoS} (Friction-Adaptive Nos\u00e9--Hoover Symplectic momentum), which combines (i) a momentum update written as a discretized second-order dynamical system, (ii) a Nos\u00e9--Hoover-like thermostat variable that adapts a scalar friction coefficient using kinetic-energy feedback, and (iii) a semi-implicit (symplectic-Euler) integrator, optionally with a diagonal RMS preconditioner. The method is motivated by structure-preserving integration and thermostat ideas from molecular dynamics, but is used here purely as an optimization heuristic.\n  We provide the algorithm and limited theoretical observations in idealized settings. On the deterministic Rosenbrock-100D benchmark with 3000 gradient evaluations, FANoS-RMS attains a mean final objective value of $1.74\\times 10^{-2}$, improving substantially over unclipped AdamW ($48.50$) and SGD+momentum ($90.76$) in this protocol. However, AdamW with gradient clipping is stronger, reaching $1.87\\times 10^{-3}$, and L-BFGS reaches $\\approx 4.4\\times 10^{-10}$. On ill-conditioned convex quadratics and in a small PINN warm-start suite (Burgers and Allen--Cahn), the default FANoS configuration underperforms AdamW and can be unstable or high-variance.\n  Overall, the evidence supports a conservative conclusion: FANoS is an interpretable synthesis of existing ideas that can help on some stiff nonconvex valleys, but it is not a generally superior replacement for modern baselines, and its behavior is sensitive to temperature-schedule and hyperparameter choices.", "AI": {"tldr": "FANoS\u662f\u4e00\u79cd\u53d7\u7269\u7406\u542f\u53d1\u7684\u4f18\u5316\u5668\uff0c\u7ed3\u5408\u4e86\u4e8c\u9636\u52a8\u529b\u7cfb\u7edf\u3001Nos\u00e9-Hoover\u6052\u6e29\u5668\u548c\u534a\u9690\u5f0f\u79ef\u5206\u5668\uff0c\u5728\u67d0\u4e9b\u975e\u51f8\u95ee\u9898\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u603b\u4f53\u4e0a\u4e0d\u5982\u73b0\u4ee3\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5c06\u5206\u5b50\u52a8\u529b\u5b66\u4e2d\u7684\u7ed3\u6784\u4fdd\u6301\u79ef\u5206\u548c\u6052\u6e29\u5668\u601d\u60f3\u5e94\u7528\u4e8e\u4f18\u5316\u95ee\u9898\uff0c\u5f00\u53d1\u4e00\u79cd\u7269\u7406\u542f\u53d1\u7684\u4f18\u5316\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\uff1a(1) \u79bb\u6563\u5316\u4e8c\u9636\u52a8\u529b\u7cfb\u7edf\u7684\u52a8\u91cf\u66f4\u65b0\uff0c(2) \u4f7f\u7528\u52a8\u80fd\u53cd\u9988\u8c03\u6574\u6807\u91cf\u6469\u64e6\u7cfb\u6570\u7684Nos\u00e9-Hoover\u7c7b\u6052\u6e29\u5668\u53d8\u91cf\uff0c(3) \u534a\u9690\u5f0f\uff08\u8f9b\u6b27\u62c9\uff09\u79ef\u5206\u5668\uff0c\u53ef\u9009\u5bf9\u89d2RMS\u9884\u5904\u7406\u5668\u3002", "result": "\u5728Rosenbrock-100D\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFANoS-RMS\uff081.74\u00d710\u207b\u00b2\uff09\u4f18\u4e8e\u672a\u526a\u88c1\u7684AdamW\uff0848.50\uff09\u548cSGD+momentum\uff0890.76\uff09\uff0c\u4f46\u4e0d\u5982\u68af\u5ea6\u526a\u88c1\u7684AdamW\uff081.87\u00d710\u207b\u00b3\uff09\u548cL-BFGS\uff08\u22484.4\u00d710\u207b\u00b9\u2070\uff09\u3002\u5728\u75c5\u6001\u51f8\u4e8c\u6b21\u95ee\u9898\u548cPINN\u9884\u70ed\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "FANoS\u662f\u73b0\u6709\u601d\u60f3\u7684\u53ef\u89e3\u91ca\u7efc\u5408\uff0c\u5728\u67d0\u4e9b\u521a\u6027\u975e\u51f8\u8c37\u95ee\u9898\u4e0a\u53ef\u80fd\u6709\u5e2e\u52a9\uff0c\u4f46\u4e0d\u662f\u73b0\u4ee3\u57fa\u7ebf\u7684\u666e\u904d\u4f18\u8d8a\u66ff\u4ee3\u54c1\uff0c\u5176\u884c\u4e3a\u5bf9\u6e29\u5ea6\u8c03\u5ea6\u548c\u8d85\u53c2\u6570\u9009\u62e9\u654f\u611f\u3002"}}
{"id": "2601.02278", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.02278", "abs": "https://arxiv.org/abs/2601.02278", "authors": ["Shijin Chen", "Zeyi Liu", "Chenyang Li", "Dongliang Zou", "Xiao He", "Donghua Zhou"], "title": "Multi-mode Fault Diagnosis Datasets of Three-phase Asynchronous Motor Under Variable Working Conditions", "comment": "12 pages, 9 figures", "summary": "Three-phase asynchronous motor are fundamental components in industrial systems, and their failure can lead to significant operational downtime and economic losses. Vibration and current signals are effective indicators for monitoring motor health and diagnosing faults. However, motors in real applications often operate under variable conditions such as fluctuating speeds and loads, which complicate the fault diagnosis process. This paper presents a comprehensive dataset collected from a three-phase asynchronous motor under various fault types and severities, operating under diverse speed and load conditions. The dataset includes both single faults and mechanical-electrical compound faults, such as rotor unbalance, stator winding short circuits, bearing faults, and their combinations. Data were acquired under both steady and transitional conditions, with signals including triaxial vibration, three-phase currents, torque, and key-phase signals. This dataset supports the development and validation of robust fault diagnosis methods for electric motors under realistic operating conditions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u4e09\u76f8\u5f02\u6b65\u7535\u673a\u5728\u591a\u79cd\u6545\u969c\u7c7b\u578b\u548c\u4e25\u91cd\u7a0b\u5ea6\u4e0b\u7684\u7efc\u5408\u6570\u636e\u96c6\uff0c\u5305\u542b\u632f\u52a8\u548c\u7535\u6d41\u4fe1\u53f7\uff0c\u652f\u6301\u5728\u53d8\u5de5\u51b5\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6545\u969c\u8bca\u65ad\u65b9\u6cd5\u5f00\u53d1\u3002", "motivation": "\u4e09\u76f8\u5f02\u6b65\u7535\u673a\u662f\u5de5\u4e1a\u7cfb\u7edf\u7684\u6838\u5fc3\u90e8\u4ef6\uff0c\u6545\u969c\u4f1a\u5bfc\u81f4\u4e25\u91cd\u505c\u673a\u548c\u7ecf\u6d4e\u635f\u5931\u3002\u5b9e\u9645\u5e94\u7528\u4e2d\u7535\u673a\u5e38\u5728\u53d8\u5de5\u51b5\uff08\u5982\u8f6c\u901f\u548c\u8d1f\u8f7d\u6ce2\u52a8\uff09\u4e0b\u8fd0\u884c\uff0c\u8fd9\u4f7f\u6545\u969c\u8bca\u65ad\u53d8\u5f97\u590d\u6742\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u6570\u636e\u96c6\u6765\u652f\u6301\u9c81\u68d2\u8bca\u65ad\u65b9\u6cd5\u5f00\u53d1\u3002", "method": "\u6536\u96c6\u4e09\u76f8\u5f02\u6b65\u7535\u673a\u5728\u591a\u79cd\u6545\u969c\u7c7b\u578b\u548c\u4e25\u91cd\u7a0b\u5ea6\u4e0b\u7684\u7efc\u5408\u6570\u636e\u96c6\uff0c\u5305\u62ec\u5355\u4e00\u6545\u969c\u548c\u673a\u7535\u590d\u5408\u6545\u969c\uff08\u5982\u8f6c\u5b50\u4e0d\u5e73\u8861\u3001\u5b9a\u5b50\u7ed5\u7ec4\u77ed\u8def\u3001\u8f74\u627f\u6545\u969c\u53ca\u5176\u7ec4\u5408\uff09\u3002\u6570\u636e\u91c7\u96c6\u6db5\u76d6\u7a33\u6001\u548c\u8fc7\u6e21\u72b6\u6001\uff0c\u4fe1\u53f7\u5305\u62ec\u4e09\u8f74\u632f\u52a8\u3001\u4e09\u76f8\u7535\u6d41\u3001\u626d\u77e9\u548c\u952e\u76f8\u4fe1\u53f7\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u6570\u636e\u96c6\uff0c\u652f\u6301\u5728\u591a\u6837\u5316\u901f\u5ea6\u3001\u8d1f\u8f7d\u6761\u4ef6\u548c\u6545\u969c\u7c7b\u578b\u4e0b\u5f00\u53d1\u9a8c\u8bc1\u7535\u673a\u6545\u969c\u8bca\u65ad\u65b9\u6cd5\uff0c\u7279\u522b\u5173\u6ce8\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53d8\u5de5\u51b5\u573a\u666f\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u5f00\u53d1\u9002\u7528\u4e8e\u5b9e\u9645\u5de5\u4e1a\u73af\u5883\u4e2d\u53d8\u5de5\u51b5\u6761\u4ef6\u7684\u9c81\u68d2\u7535\u673a\u6545\u969c\u8bca\u65ad\u65b9\u6cd5\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u7535\u673a\u5065\u5eb7\u76d1\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2601.01363", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01363", "abs": "https://arxiv.org/abs/2601.01363", "authors": ["Xiaomeng Yang", "Zhiyu Tan", "Xiaohui Zhong", "Mengping Yang", "Qiusheng Huang", "Lei Chen", "Libo Wu", "Hao Li"], "title": "A unified multimodal understanding and generation model for cross-disciplinary scientific research", "comment": null, "summary": "Scientific discovery increasingly relies on integrating heterogeneous, high-dimensional data across disciplines nowadays. While AI models have achieved notable success across various scientific domains, they typically remain domain-specific or lack the capability of simultaneously understanding and generating multimodal scientific data, particularly for high-dimensional data. Yet, many pressing global challenges and scientific problems are inherently cross-disciplinary and require coordinated progress across multiple fields. Here, we present FuXi-Uni, a native unified multimodal model for scientific understanding and high-fidelity generation across scientific domains within a single architecture. Specifically, FuXi-Uni aligns cross-disciplinary scientific tokens within natural language tokens and employs science decoder to reconstruct scientific tokens, thereby supporting both natural language conversation and scientific numerical prediction. Empirically, we validate FuXi-Uni in Earth science and Biomedicine. In Earth system modeling, the model supports global weather forecasting, tropical cyclone (TC) forecast editing, and spatial downscaling driven by only language instructions. FuXi-Uni generates 10-day global forecasts at 0.25\u00b0 resolution that outperform the SOTA physical forecasting system. It shows superior performance for both TC track and intensity prediction relative to the SOTA physical model, and generates high-resolution regional weather fields that surpass standard interpolation baselines. Regarding biomedicine, FuXi-Uni outperforms leading multimodal large language models on multiple biomedical visual question answering benchmarks. By unifying heterogeneous scientific modalities within a native shared latent space while maintaining strong domain-specific performance, FuXi-Uni provides a step forward more general-purpose, multimodal scientific models.", "AI": {"tldr": "FuXi-Uni\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u79d1\u5b66\u6a21\u578b\uff0c\u80fd\u591f\u5728\u5355\u4e00\u67b6\u6784\u4e2d\u7406\u89e3\u548c\u751f\u6210\u8de8\u5b66\u79d1\u7684\u79d1\u5b66\u6570\u636e\uff0c\u5728\u6c14\u8c61\u9884\u62a5\u548c\u751f\u7269\u533b\u5b66\u95ee\u7b54\u7b49\u4efb\u52a1\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u79d1\u5b66\u53d1\u73b0\u9700\u8981\u6574\u5408\u8de8\u5b66\u79d1\u7684\u9ad8\u7ef4\u5f02\u6784\u6570\u636e\uff0c\u4f46\u73b0\u6709AI\u6a21\u578b\u901a\u5e38\u662f\u9886\u57df\u7279\u5b9a\u7684\uff0c\u7f3a\u4e4f\u540c\u65f6\u7406\u89e3\u548c\u751f\u6210\u591a\u6a21\u6001\u79d1\u5b66\u6570\u636e\u7684\u80fd\u529b\u3002\u8bb8\u591a\u5168\u7403\u6027\u6311\u6218\u548c\u79d1\u5b66\u95ee\u9898\u672c\u8d28\u4e0a\u662f\u8de8\u5b66\u79d1\u7684\uff0c\u9700\u8981\u591a\u4e2a\u9886\u57df\u7684\u534f\u540c\u8fdb\u5c55\u3002", "method": "FuXi-Uni\u5c06\u8de8\u5b66\u79d1\u7684\u79d1\u5b66\u6807\u8bb0\u4e0e\u81ea\u7136\u8bed\u8a00\u6807\u8bb0\u5bf9\u9f50\uff0c\u5e76\u4f7f\u7528\u79d1\u5b66\u89e3\u7801\u5668\u91cd\u5efa\u79d1\u5b66\u6807\u8bb0\uff0c\u4ece\u800c\u540c\u65f6\u652f\u6301\u81ea\u7136\u8bed\u8a00\u5bf9\u8bdd\u548c\u79d1\u5b66\u6570\u503c\u9884\u6d4b\u3002\u6a21\u578b\u5728\u7edf\u4e00\u7684\u591a\u6a21\u6001\u67b6\u6784\u4e2d\u5904\u7406\u5f02\u6784\u79d1\u5b66\u6a21\u6001\u3002", "result": "\u5728\u5730\u7403\u79d1\u5b66\u4e2d\uff1a1) \u751f\u62100.25\u00b0\u5206\u8fa8\u7387\u768410\u5929\u5168\u7403\u5929\u6c14\u9884\u62a5\uff0c\u8d85\u8d8a\u6700\u5148\u8fdb\u7684\u7269\u7406\u9884\u62a5\u7cfb\u7edf\uff1b2) \u70ed\u5e26\u6c14\u65cb\u8def\u5f84\u548c\u5f3a\u5ea6\u9884\u6d4b\u4f18\u4e8e\u6700\u5148\u8fdb\u7269\u7406\u6a21\u578b\uff1b3) \u751f\u6210\u7684\u9ad8\u5206\u8fa8\u7387\u533a\u57df\u5929\u6c14\u573a\u8d85\u8d8a\u6807\u51c6\u63d2\u503c\u57fa\u7ebf\u3002\u5728\u751f\u7269\u533b\u5b66\u4e2d\uff1a\u5728\u591a\u4e2a\u751f\u7269\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\u57fa\u51c6\u4e0a\u4f18\u4e8e\u9886\u5148\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u3002", "conclusion": "FuXi-Uni\u901a\u8fc7\u5728\u539f\u751f\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7edf\u4e00\u5f02\u6784\u79d1\u5b66\u6a21\u6001\uff0c\u540c\u65f6\u4fdd\u6301\u5f3a\u5927\u7684\u9886\u57df\u7279\u5b9a\u6027\u80fd\uff0c\u4e3a\u66f4\u901a\u7528\u7684\u591a\u6a21\u6001\u79d1\u5b66\u6a21\u578b\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2601.01332", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01332", "abs": "https://arxiv.org/abs/2601.01332", "authors": ["Hossam Amer", "Maryam Dialameh", "Hossein Rajabzadeh", "Walid Ahmed", "Weiwei Zhang", "Yang Liu"], "title": "FLOP-Efficient Training: Early Stopping Based on Test-Time Compute Awareness", "comment": null, "summary": "Scaling training compute, measured in FLOPs, has long been shown to improve the accuracy of large language models, yet training remains resource-intensive. Prior work shows that increasing test-time compute (TTC)-for example through iterative sampling-can allow smaller models to rival or surpass much larger ones at lower overall cost. We introduce TTC-aware training, where an intermediate checkpoint and a corresponding TTC configuration can together match or exceed the accuracy of a fully trained model while requiring substantially fewer training FLOPs. Building on this insight, we propose an early stopping algorithm that jointly selects a checkpoint and TTC configuration to minimize training compute without sacrificing accuracy. To make this practical, we develop an efficient TTC evaluation method that avoids exhaustive search, and we formalize a break-even bound that identifies when increased inference compute compensates for reduced training compute. Experiments demonstrate up to 92\\% reductions in training FLOPs while maintaining and sometimes remarkably improving accuracy. These results highlight a new perspective for balancing training and inference compute in model development, enabling faster deployment cycles and more frequent model refreshes. Codes will be publicly released.", "AI": {"tldr": "\u63d0\u51faTTC\u611f\u77e5\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u9009\u62e9\u68c0\u67e5\u70b9\u548c\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u914d\u7f6e\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u8bad\u7ec3\u8ba1\u7b97\u91cf\uff08\u6700\u9ad8\u8fbe92%\uff09\u3002", "motivation": "\u4f20\u7edf\u4e0a\u901a\u8fc7\u589e\u52a0\u8bad\u7ec3\u8ba1\u7b97\u91cf\uff08FLOPs\uff09\u6765\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7cbe\u5ea6\uff0c\u4f46\u8bad\u7ec3\u6210\u672c\u9ad8\u6602\u3002\u7814\u7a76\u53d1\u73b0\u589e\u52a0\u6d4b\u8bd5\u65f6\u8ba1\u7b97\uff08TTC\uff09\u53ef\u4ee5\u8ba9\u5c0f\u6a21\u578b\u5339\u654c\u5927\u6a21\u578b\uff0c\u4f46\u5982\u4f55\u5e73\u8861\u8bad\u7ec3\u548c\u63a8\u7406\u8ba1\u7b97\u5c1a\u672a\u7cfb\u7edf\u7814\u7a76\u3002", "method": "1. TTC\u611f\u77e5\u8bad\u7ec3\uff1a\u4e2d\u95f4\u68c0\u67e5\u70b9\u914d\u5408TTC\u914d\u7f6e\u53ef\u8fbe\u5230\u6216\u8d85\u8fc7\u5b8c\u6574\u8bad\u7ec3\u6a21\u578b\u7684\u7cbe\u5ea6\uff1b2. \u63d0\u524d\u505c\u6b62\u7b97\u6cd5\uff1a\u8054\u5408\u9009\u62e9\u68c0\u67e5\u70b9\u548cTTC\u914d\u7f6e\u4ee5\u6700\u5c0f\u5316\u8bad\u7ec3\u8ba1\u7b97\uff1b3. \u9ad8\u6548TTC\u8bc4\u4f30\u65b9\u6cd5\uff1a\u907f\u514d\u7a77\u4e3e\u641c\u7d22\uff1b4. \u76c8\u4e8f\u5e73\u8861\u754c\u9650\uff1a\u786e\u5b9a\u4f55\u65f6\u589e\u52a0\u7684\u63a8\u7406\u8ba1\u7b97\u80fd\u8865\u507f\u51cf\u5c11\u7684\u8bad\u7ec3\u8ba1\u7b97\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u8bad\u7ec3FLOPs\u51cf\u5c11\u9ad8\u8fbe92%\uff0c\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u7cbe\u5ea6\u3002\u8fd9\u4e3a\u5e73\u8861\u8bad\u7ec3\u548c\u63a8\u7406\u8ba1\u7b97\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "conclusion": "TTC\u611f\u77e5\u8bad\u7ec3\u65b9\u6cd5\u80fd\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\u800c\u4e0d\u727a\u7272\u7cbe\u5ea6\uff0c\u652f\u6301\u66f4\u5feb\u7684\u90e8\u7f72\u5468\u671f\u548c\u66f4\u9891\u7e41\u7684\u6a21\u578b\u66f4\u65b0\uff0c\u4ee3\u7801\u5c06\u516c\u5f00\u3002"}}
{"id": "2601.01362", "categories": ["cs.CL", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.01362", "abs": "https://arxiv.org/abs/2601.01362", "authors": ["Jerry Huang", "Peng Lu", "Qiuhao Zeng", "Yusuke Iwasawa", "Yutaka Matsuo", "Sarath Chandar", "Edison Marrese-Taylor", "Irene Li"], "title": "Investigating the Multilingual Calibration Effects of Language Model Instruction-Tuning", "comment": "Accepted to The 19th Conference of the European Chapter of the Association for Computational Linguistics (EACL)", "summary": "Ensuring that deep learning models are well-calibrated in terms of their predictive uncertainty is essential in maintaining their trustworthiness and reliability, yet despite increasing advances in foundation model research, the relationship between such large language models (LLMs) and their calibration remains an open area of research. In this work, we look at a critical gap in the calibration of LLMs within multilingual settings, in an attempt to better understand how the data scarcity can potentially lead to different calibration effects and how commonly used techniques can apply in these settings. Our analysis on two multilingual benchmarks, over 29 and 42 languages respectively, reveals that even in low-resource languages, model confidence can increase significantly after instruction-tuning on high-resource language SFT datasets. However, improvements in accuracy are marginal or non-existent, resulting in mis-calibration, highlighting a critical shortcoming of standard SFT for multilingual languages. Furthermore, we observe that the use of label smoothing to be a reasonable method alleviate this concern, again without any need for low-resource SFT data, maintaining better calibration across all languages. Overall, this highlights the importance of multilingual considerations for both training and tuning LLMs in order to improve their reliability and fairness in downstream use.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e2d\uff0c\u5373\u4f7f\u4f4e\u8d44\u6e90\u8bed\u8a00\u7f3a\u4e4f\u76d1\u7763\u5fae\u8c03\u6570\u636e\uff0c\u4ec5\u7528\u9ad8\u8d44\u6e90\u8bed\u8a00\u6570\u636e\u8fdb\u884c\u6307\u4ee4\u5fae\u8c03\u4e5f\u4f1a\u663e\u8457\u589e\u52a0\u6a21\u578b\u7f6e\u4fe1\u5ea6\uff0c\u4f46\u51c6\u786e\u7387\u63d0\u5347\u6709\u9650\uff0c\u5bfc\u81f4\u6821\u51c6\u4e0d\u4f73\u3002\u6807\u7b7e\u5e73\u6ed1\u6280\u672f\u80fd\u6709\u6548\u7f13\u89e3\u6b64\u95ee\u9898\uff0c\u65e0\u9700\u4f4e\u8d44\u6e90\u8bed\u8a00\u6570\u636e\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u7840\u6a21\u578b\u7814\u7a76\u4e0d\u65ad\u8fdb\u6b65\uff0c\u4f46\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6821\u51c6\u95ee\u9898\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u7684\u7814\u7a76\u9886\u57df\u3002\u7279\u522b\u662f\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e2d\uff0c\u6570\u636e\u7a00\u7f3a\u53ef\u80fd\u5bfc\u81f4\u4e0d\u540c\u7684\u6821\u51c6\u6548\u679c\uff0c\u800c\u5e38\u7528\u6280\u672f\u5728\u8fd9\u4e9b\u73af\u5883\u4e0b\u7684\u9002\u7528\u6027\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u5728\u4e24\u4e2a\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\uff08\u5206\u522b\u6db5\u76d629\u548c42\u79cd\u8bed\u8a00\uff09\u4e0a\u8fdb\u884c\u5206\u6790\uff0c\u7814\u7a76\u6307\u4ee4\u5fae\u8c03\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u6821\u51c6\u7684\u5f71\u54cd\u3002\u63a2\u7d22\u4f7f\u7528\u6807\u7b7e\u5e73\u6ed1\u6280\u672f\u6765\u6539\u5584\u6821\u51c6\u6548\u679c\uff0c\u4e14\u65e0\u9700\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u76d1\u7763\u5fae\u8c03\u6570\u636e\u3002", "result": "\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\uff0c\u4ec5\u4f7f\u7528\u9ad8\u8d44\u6e90\u8bed\u8a00\u6570\u636e\u8fdb\u884c\u6307\u4ee4\u5fae\u8c03\u4f1a\u663e\u8457\u589e\u52a0\u6a21\u578b\u7f6e\u4fe1\u5ea6\uff0c\u4f46\u51c6\u786e\u7387\u63d0\u5347\u5f88\u5c0f\u751a\u81f3\u6ca1\u6709\uff0c\u5bfc\u81f4\u6821\u51c6\u4e0d\u4f73\u3002\u6807\u7b7e\u5e73\u6ed1\u6280\u672f\u80fd\u6709\u6548\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\uff0c\u5728\u6240\u6709\u8bed\u8a00\u4e2d\u4fdd\u6301\u66f4\u597d\u7684\u6821\u51c6\u6548\u679c\u3002", "conclusion": "\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u6a21\u578b\u8bad\u7ec3\u548c\u5fae\u8c03\u9700\u8981\u8003\u8651\u6821\u51c6\u95ee\u9898\uff0c\u4ee5\u63d0\u9ad8\u4e0b\u6e38\u5e94\u7528\u7684\u53ef\u9760\u6027\u548c\u516c\u5e73\u6027\u3002\u6807\u51c6\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u5b58\u5728\u7f3a\u9677\uff0c\u800c\u6807\u7b7e\u5e73\u6ed1\u662f\u4e00\u79cd\u6709\u6548\u7684\u6539\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2601.01366", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01366", "abs": "https://arxiv.org/abs/2601.01366", "authors": ["Zixian Liu", "Sihao Liu", "Yuqi Zhao"], "title": "KGCE: Knowledge-Augmented Dual-Graph Evaluator for Cross-Platform Educational Agent Benchmarking with Multimodal Language Models", "comment": null, "summary": "With the rapid adoption of multimodal large language models (MLMs) in autonomous agents, cross-platform task execution capabilities in educational settings have garnered significant attention. However, existing benchmark frameworks still exhibit notable deficiencies in supporting cross-platform tasks in educational contexts, especially when dealing with school-specific software (such as XiaoYa Intelligent Assistant, HuaShi XiaZi, etc.), where the efficiency of agents often significantly decreases due to a lack of understanding of the structural specifics of these private-domain software. Additionally, current evaluation methods heavily rely on coarse-grained metrics like goal orientation or trajectory matching, making it challenging to capture the detailed execution and efficiency of agents in complex tasks. To address these issues, we propose KGCE (Knowledge-Augmented Dual-Graph Evaluator for Cross-Platform Educational Agent Benchmarking with Multimodal Language Models), a novel benchmarking platform that integrates knowledge base enhancement and a dual-graph evaluation framework. We first constructed a dataset comprising 104 education-related tasks, covering Windows, Android, and cross-platform collaborative tasks. KGCE introduces a dual-graph evaluation framework that decomposes tasks into multiple sub-goals and verifies their completion status, providing fine-grained evaluation metrics. To overcome the execution bottlenecks of existing agents in private-domain tasks, we developed an enhanced agent system incorporating a knowledge base specific to school-specific software. The code can be found at https://github.com/Kinginlife/KGCE.", "AI": {"tldr": "KGCE\u662f\u4e00\u4e2a\u9488\u5bf9\u6559\u80b2\u573a\u666f\u4e2d\u8de8\u5e73\u53f0\u4efb\u52a1\u6267\u884c\u7684\u65b0\u578b\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\uff0c\u901a\u8fc7\u77e5\u8bc6\u5e93\u589e\u5f3a\u548c\u53cc\u56fe\u8bc4\u4f30\u6846\u67b6\u89e3\u51b3\u73b0\u6709\u57fa\u51c6\u5728\u79c1\u6709\u9886\u57df\u8f6f\u4ef6\u4efb\u52a1\u4e2d\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6846\u67b6\u5728\u6559\u80b2\u573a\u666f\u7684\u8de8\u5e73\u53f0\u4efb\u52a1\u652f\u6301\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u5b66\u6821\u4e13\u7528\u8f6f\u4ef6\uff08\u5982\u5c0f\u96c5\u667a\u80fd\u52a9\u624b\u3001\u534e\u5e08\u5323\u5b50\u7b49\uff09\u65f6\uff0c\u7531\u4e8e\u7f3a\u4e4f\u5bf9\u8fd9\u4e9b\u79c1\u6709\u9886\u57df\u8f6f\u4ef6\u7ed3\u6784\u7ec6\u8282\u7684\u7406\u89e3\uff0c\u4ee3\u7406\u6548\u7387\u663e\u8457\u4e0b\u964d\u3002\u540c\u65f6\uff0c\u5f53\u524d\u8bc4\u4f30\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u7c97\u7c92\u5ea6\u6307\u6807\uff0c\u96be\u4ee5\u6355\u6349\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u8be6\u7ec6\u6267\u884c\u548c\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86KGCE\u5e73\u53f0\uff0c\u6574\u5408\u77e5\u8bc6\u5e93\u589e\u5f3a\u548c\u53cc\u56fe\u8bc4\u4f30\u6846\u67b6\u3002\u6784\u5efa\u4e86\u5305\u542b104\u4e2a\u6559\u80b2\u76f8\u5173\u4efb\u52a1\u7684\u6570\u636e\u96c6\uff0c\u6db5\u76d6Windows\u3001Android\u548c\u8de8\u5e73\u53f0\u534f\u4f5c\u4efb\u52a1\u3002\u5f15\u5165\u53cc\u56fe\u8bc4\u4f30\u6846\u67b6\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u591a\u4e2a\u5b50\u76ee\u6807\u5e76\u9a8c\u8bc1\u5b8c\u6210\u72b6\u6001\uff0c\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u6307\u6807\u3002\u5f00\u53d1\u4e86\u589e\u5f3a\u7684\u4ee3\u7406\u7cfb\u7edf\uff0c\u5305\u542b\u9488\u5bf9\u5b66\u6821\u4e13\u7528\u8f6f\u4ef6\u7684\u77e5\u8bc6\u5e93\u3002", "result": "\u5f00\u53d1\u4e86KGCE\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5305\u542b104\u4e2a\u6559\u80b2\u4efb\u52a1\u7684\u6570\u636e\u96c6\uff0c\u5b9e\u73b0\u4e86\u77e5\u8bc6\u5e93\u589e\u5f3a\u7684\u4ee3\u7406\u7cfb\u7edf\u548c\u53cc\u56fe\u8bc4\u4f30\u6846\u67b6\uff0c\u4ee3\u7801\u5df2\u5728GitHub\u5f00\u6e90\u3002", "conclusion": "KGCE\u901a\u8fc7\u77e5\u8bc6\u5e93\u589e\u5f3a\u548c\u53cc\u56fe\u8bc4\u4f30\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u6559\u80b2\u573a\u666f\u4e2d\u8de8\u5e73\u53f0\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bd5\u7684\u73b0\u6709\u7f3a\u9677\uff0c\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u4e3b\u4ee3\u7406\u4e2d\u7684\u6559\u80b2\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u7cbe\u7ec6\u7684\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2601.01341", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01341", "abs": "https://arxiv.org/abs/2601.01341", "authors": ["Md Abdullah Al Kafi", "Raka Moni", "Sumit Kumar Banshal"], "title": "Reasoning Over Recall: Evaluating the Efficacy of Generalist Architectures vs. Specialized Fine-Tunes in RAG-Based Mental Health Dialogue Systems", "comment": null, "summary": "The deployment of Large Language Models (LLMs) in mental health counseling faces the dual challenges of hallucinations and lack of empathy. While the former may be mitigated by RAG (retrieval-augmented generation) by anchoring answers in trusted clinical sources, there remains an open question as to whether the most effective model under this paradigm would be one that is fine-tuned on mental health data, or a more general and powerful model that succeeds purely on the basis of reasoning. In this paper, we perform a direct comparison by running four open-source models through the same RAG pipeline using ChromaDB: two generalist reasoners (Qwen2.5-3B and Phi-3-Mini) and two domain-specific fine-tunes (MentalHealthBot-7B and TherapyBot-7B). We use an LLM-as-a-Judge framework to automate evaluation over 50 turns. We find a clear trend: the generalist models outperform the domain-specific ones in empathy (3.72 vs. 3.26, $p < 0.001$) in spite of being much smaller (3B vs. 7B), and all models perform well in terms of safety, but the generalist models show better contextual understanding and are less prone to overfitting as we observe in the domain-specific models. Overall, our results indicate that for RAG-based therapy systems, strong reasoning is more important than training on mental health-specific vocabulary; i.e. a well-reasoned general model would provide more empathetic and balanced support than a larger narrowly fine-tuned model, so long as the answer is already grounded in clinical evidence.", "AI": {"tldr": "\u5728\u57fa\u4e8eRAG\u7684\u5fc3\u7406\u54a8\u8be2\u7cfb\u7edf\u4e2d\uff0c\u901a\u7528\u63a8\u7406\u6a21\u578b\uff083B\uff09\u6bd4\u9886\u57df\u5fae\u8c03\u6a21\u578b\uff087B\uff09\u8868\u73b0\u66f4\u597d\uff0c\u5728\u5171\u60c5\u80fd\u529b\u4e0a\u663e\u8457\u4f18\u4e8e\u4e13\u4e1a\u6a21\u578b\uff0c\u8868\u660e\u5f3a\u63a8\u7406\u80fd\u529b\u6bd4\u5fc3\u7406\u5065\u5eb7\u7279\u5b9a\u8bad\u7ec3\u66f4\u91cd\u8981\u3002", "motivation": "\u89e3\u51b3LLM\u5728\u5fc3\u7406\u5065\u5eb7\u54a8\u8be2\u4e2d\u7684\u5e7b\u89c9\u548c\u7f3a\u4e4f\u5171\u60c5\u95ee\u9898\uff0c\u63a2\u7d22\u5728RAG\u6846\u67b6\u4e0b\uff0c\u662f\u9886\u57df\u5fae\u8c03\u6a21\u578b\u8fd8\u662f\u901a\u7528\u63a8\u7406\u6a21\u578b\u66f4\u9002\u5408\u5fc3\u7406\u54a8\u8be2\u5e94\u7528\u3002", "method": "\u4f7f\u7528ChromaDB\u6784\u5efa\u76f8\u540cRAG\u6d41\u7a0b\uff0c\u6bd4\u8f83\u56db\u4e2a\u5f00\u6e90\u6a21\u578b\uff1a\u4e24\u4e2a\u901a\u7528\u63a8\u7406\u6a21\u578b\uff08Qwen2.5-3B\u548cPhi-3-Mini\uff09\u548c\u4e24\u4e2a\u9886\u57df\u5fae\u8c03\u6a21\u578b\uff08MentalHealthBot-7B\u548cTherapyBot-7B\uff09\uff0c\u91c7\u7528LLM-as-a-Judge\u6846\u67b6\u81ea\u52a8\u5316\u8bc4\u4f3050\u8f6e\u5bf9\u8bdd\u3002", "result": "\u901a\u7528\u6a21\u578b\u5728\u5171\u60c5\u80fd\u529b\u4e0a\u663e\u8457\u4f18\u4e8e\u9886\u57df\u6a21\u578b\uff083.72 vs 3.26\uff0cp<0.001\uff09\uff0c\u5c3d\u7ba1\u6a21\u578b\u66f4\u5c0f\uff083B vs 7B\uff09\uff1b\u6240\u6709\u6a21\u578b\u5b89\u5168\u6027\u826f\u597d\uff0c\u4f46\u901a\u7528\u6a21\u578b\u8868\u73b0\u51fa\u66f4\u597d\u7684\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\uff0c\u9886\u57df\u6a21\u578b\u5b58\u5728\u8fc7\u62df\u5408\u73b0\u8c61\u3002", "conclusion": "\u5bf9\u4e8e\u57fa\u4e8eRAG\u7684\u6cbb\u7597\u7cfb\u7edf\uff0c\u5f3a\u63a8\u7406\u80fd\u529b\u6bd4\u5fc3\u7406\u5065\u5eb7\u7279\u5b9a\u8bcd\u6c47\u8bad\u7ec3\u66f4\u91cd\u8981\uff1b\u53ea\u8981\u56de\u7b54\u57fa\u4e8e\u4e34\u5e8a\u8bc1\u636e\uff0c\u63a8\u7406\u80fd\u529b\u5f3a\u7684\u901a\u7528\u6a21\u578b\u80fd\u63d0\u4f9b\u66f4\u5171\u60c5\u548c\u5e73\u8861\u7684\u652f\u6301\u3002"}}
{"id": "2601.00894", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.00894", "abs": "https://arxiv.org/abs/2601.00894", "authors": ["Gihyeon Sim"], "title": "When to Ponder: Adaptive Compute Allocation for Code Generation via Test-Time Training", "comment": "14 pages, 1 figure, 14 tables, code available at https://github.com/deveworld/ponderTTT", "summary": "Large language models apply uniform computation to all inputs, regardless of difficulty. We propose PonderTTT, a gating strategy using the TTT layer's self-supervised reconstruction loss to selectively trigger Test-Time Training (TTT) updates. The gating decision itself is training-free--requiring no learned classifier or auxiliary networks; only a single scalar threshold is initially calibrated on unlabeled data and continuously adapted via EMA to maintain target update rates. Our experiments with GPT-2 models (124M to 1.5B) on code language modeling (The Stack v2, teacher-forced perplexity) demonstrate that this signal is inference-compatible, requiring no ground-truth labels. Our Reconstruction Gating achieves 82-89% Oracle Recovery while being fully training-free, significantly outperforming Random Skip baselines (up to 16% lower loss on OOD languages).", "AI": {"tldr": "PonderTTT\uff1a\u4e00\u79cd\u57fa\u4e8e\u81ea\u76d1\u7763\u91cd\u5efa\u635f\u5931\u7684\u95e8\u63a7\u7b56\u7565\uff0c\u7528\u4e8e\u9009\u62e9\u6027\u89e6\u53d1\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u66f4\u65b0\uff0c\u65e0\u9700\u8bad\u7ec3\uff0c\u4ec5\u9700\u5355\u4e2a\u6807\u91cf\u9608\u503c\uff0c\u5728\u4ee3\u7801\u8bed\u8a00\u5efa\u6a21\u4e2d\u663e\u8457\u4f18\u4e8e\u968f\u673a\u8df3\u8fc7\u57fa\u7ebf\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u6240\u6709\u8f93\u5165\u5e94\u7528\u7edf\u4e00\u8ba1\u7b97\uff0c\u65e0\u8bba\u96be\u5ea6\u5982\u4f55\u3002\u8fd9\u53ef\u80fd\u5bfc\u81f4\u8ba1\u7b97\u8d44\u6e90\u6d6a\u8d39\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u7b80\u5355\u8f93\u5165\u3002\u9700\u8981\u4e00\u79cd\u667a\u80fd\u673a\u5236\u6765\u9009\u62e9\u6027\u89e6\u53d1\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u66f4\u65b0\uff0c\u4ee5\u4f18\u5316\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u63d0\u51faPonderTTT\u95e8\u63a7\u7b56\u7565\uff0c\u5229\u7528TTT\u5c42\u7684\u81ea\u76d1\u7763\u91cd\u5efa\u635f\u5931\u6765\u51b3\u5b9a\u662f\u5426\u89e6\u53d1\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u66f4\u65b0\u3002\u95e8\u63a7\u51b3\u7b56\u672c\u8eab\u65e0\u9700\u8bad\u7ec3\uff0c\u4e0d\u9700\u8981\u5b66\u4e60\u5206\u7c7b\u5668\u6216\u8f85\u52a9\u7f51\u7edc\uff0c\u4ec5\u9700\u5728\u672a\u6807\u8bb0\u6570\u636e\u4e0a\u521d\u59cb\u6821\u51c6\u5355\u4e2a\u6807\u91cf\u9608\u503c\uff0c\u5e76\u901a\u8fc7\u6307\u6570\u79fb\u52a8\u5e73\u5747\u6301\u7eed\u8c03\u6574\u4ee5\u7ef4\u6301\u76ee\u6807\u66f4\u65b0\u7387\u3002", "result": "\u5728GPT-2\u6a21\u578b\uff08124M\u52301.5B\u53c2\u6570\uff09\u4e0a\u7684\u4ee3\u7801\u8bed\u8a00\u5efa\u6a21\u5b9e\u9a8c\uff08The Stack v2\u6570\u636e\u96c6\uff0c\u6559\u5e08\u5f3a\u5236\u56f0\u60d1\u5ea6\uff09\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u65e0\u9700\u771f\u5b9e\u6807\u7b7e\uff0c\u91cd\u5efa\u95e8\u63a7\u8fbe\u523082-89%\u7684Oracle\u6062\u590d\u7387\uff0c\u5b8c\u5168\u65e0\u9700\u8bad\u7ec3\uff0c\u663e\u8457\u4f18\u4e8e\u968f\u673a\u8df3\u8fc7\u57fa\u7ebf\uff08\u5728OOD\u8bed\u8a00\u4e0a\u635f\u5931\u964d\u4f4e\u8fbe16%\uff09\u3002", "conclusion": "PonderTTT\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u95e8\u63a7\u673a\u5236\uff0c\u80fd\u591f\u667a\u80fd\u9009\u62e9\u6027\u5730\u89e6\u53d1\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u66f4\u65b0\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u4f18\u5316\u8ba1\u7b97\u8d44\u6e90\u4f7f\u7528\uff0c\u7279\u522b\u9002\u7528\u4e8e\u4ee3\u7801\u8bed\u8a00\u5efa\u6a21\u7b49\u4efb\u52a1\u3002"}}
{"id": "2601.01016", "categories": ["cs.LG", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.01016", "abs": "https://arxiv.org/abs/2601.01016", "authors": ["Ata Akbari Asanjan", "Milad Memarzadeh", "Bryan Matthews", "Nikunj Oza"], "title": "Improving Variational Autoencoder using Random Fourier Transformation: An Aviation Safety Anomaly Detection Case-Study", "comment": null, "summary": "In this study, we focus on the training process and inference improvements of deep neural networks (DNNs), specifically Autoencoders (AEs) and Variational Autoencoders (VAEs), using Random Fourier Transformation (RFT). We further explore the role of RFT in model training behavior using Frequency Principle (F-Principle) analysis and show that models with RFT turn to learn low frequency and high frequency at the same time, whereas conventional DNNs start from low frequency and gradually learn (if successful) high-frequency features. We focus on reconstruction-based anomaly detection using autoencoder and variational autoencoder and investigate the RFT's role. We also introduced a trainable variant of RFT that uses the existing computation graph to train the expansion of RFT instead of it being random. We showcase our findings with two low-dimensional synthetic datasets for data representation, and an aviation safety dataset, called Dashlink, for high-dimensional reconstruction-based anomaly detection. The results indicate the superiority of models with Fourier transformation compared to the conventional counterpart and remain inconclusive regarding the benefits of using trainable Fourier transformation in contrast to the Random variant.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u81ea\u7f16\u7801\u5668\u548c\u53d8\u5206\u81ea\u7f16\u7801\u5668\u4e2d\u4f7f\u7528\u968f\u673a\u5085\u91cc\u53f6\u53d8\u6362\u6539\u8fdb\u8bad\u7ec3\u548c\u63a8\u7406\uff0c\u901a\u8fc7\u9891\u7387\u539f\u7406\u5206\u6790\u53d1\u73b0RFT\u6a21\u578b\u80fd\u540c\u65f6\u5b66\u4e60\u9ad8\u4f4e\u9891\u7279\u5f81\uff0c\u800c\u4f20\u7edfDNN\u53ea\u80fd\u4ece\u4f4e\u9891\u5f00\u59cb\u9010\u6b65\u5b66\u4e60\u9ad8\u9891\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u6539\u8fdb\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u8bad\u7ec3\u8fc7\u7a0b\u548c\u63a8\u7406\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u81ea\u7f16\u7801\u5668\u548c\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7684\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u4e2d\u3002\u4f20\u7edfDNN\u5728\u5b66\u4e60\u9ad8\u9891\u7279\u5f81\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u9700\u8981\u63a2\u7d22\u66f4\u6709\u6548\u7684\u9891\u7387\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u968f\u673a\u5085\u91cc\u53f6\u53d8\u6362\uff08RFT\uff09\u53ca\u5176\u53ef\u8bad\u7ec3\u53d8\u4f53\uff0c\u7ed3\u5408\u9891\u7387\u539f\u7406\u5206\u6790\u6a21\u578b\u8bad\u7ec3\u884c\u4e3a\u3002\u5728\u4f4e\u7ef4\u5408\u6210\u6570\u636e\u96c6\u8fdb\u884c\u6570\u636e\u8868\u793a\u5206\u6790\uff0c\u5728\u9ad8\u7ef4\u822a\u7a7a\u5b89\u5168\u6570\u636e\u96c6\uff08Dashlink\uff09\u8fdb\u884c\u57fa\u4e8e\u91cd\u5efa\u7684\u5f02\u5e38\u68c0\u6d4b\u5b9e\u9a8c\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff1a1\uff09\u4f7f\u7528\u5085\u91cc\u53f6\u53d8\u6362\u7684\u6a21\u578b\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\uff1b2\uff09RFT\u6a21\u578b\u80fd\u540c\u65f6\u5b66\u4e60\u4f4e\u9891\u548c\u9ad8\u9891\u7279\u5f81\uff0c\u800c\u4f20\u7edfDNN\u53ea\u80fd\u4ece\u4f4e\u9891\u5f00\u59cb\u9010\u6b65\u5b66\u4e60\uff1b3\uff09\u53ef\u8bad\u7ec3\u5085\u91cc\u53f6\u53d8\u6362\u4e0e\u968f\u673a\u53d8\u4f53\u76f8\u6bd4\u7684\u4f18\u52bf\u5c1a\u4e0d\u786e\u5b9a\u3002", "conclusion": "\u5085\u91cc\u53f6\u53d8\u6362\u80fd\u663e\u8457\u6539\u8fdb\u81ea\u7f16\u7801\u5668\u548c\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u4e2d\u3002RFT\u4f7f\u6a21\u578b\u80fd\u540c\u65f6\u5b66\u4e60\u4e0d\u540c\u9891\u7387\u7279\u5f81\uff0c\u4f46\u53ef\u8bad\u7ec3\u53d8\u4f53\u7684\u4f18\u52bf\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u9a8c\u8bc1\u3002"}}
{"id": "2601.01306", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.01306", "abs": "https://arxiv.org/abs/2601.01306", "authors": ["John Zhao"], "title": "Towards a Principled Muon under $\u03bc\\mathsf{P}$: Ensuring Spectral Conditions throughout Training", "comment": "21 pages, 0 figures", "summary": "The $\u03bc$-parameterization ($\u03bc$P) provides a principled foundation for large language model (LLM) training by prescribing width-independent learning dynamics, which in turn enables predictable scaling behavior and robust hyperparameter transfer across model sizes. A central requirement of $\u03bc$P is the satisfaction of certain spectral conditions on weight matrices, which ensure consistent feature learning and optimization behavior as model width grows. While these conditions are well understood in theory, guaranteeing their validity in practical training for matrix-based optimizers such as Muon is still under studied. Existing works that study Muon under $\u03bc$P exhibit important limitations: they either do not ensure that the spectral conditions hold throughout the entire training horizon, or require repeated spectral normalization (or Newton-Schulz iterations) applied to both weights and updates, leading to significant computational overhead and reduced practicality. In this work, we show how to reliably guarantee the spectral conditions required by $\u03bc$P for Muon during the entire training process. Our key insight is that for moderately large models, maintaining spectral control at the level of optimizer updates alone is sufficient to preserve $\u03bc$P-compatible scaling, eliminating the need for explicit spectral normalization of the weights. Based on this principle, we develop a variant of Muon, namely Muon++, that satisfies spectral condition throughout the training process. Our results bridge the gap between the theoretical promises of $\u03bc$P and the practical deployment of matrix-based optimizers in long-horizon training. We also take the first step towards an adaptive spectral condition by incorporating data-dependent effects, making it better suited for long-horizon LLM training.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMuon++\uff0c\u4e00\u79cd\u6539\u8fdb\u7684\u77e9\u9635\u4f18\u5316\u5668\uff0c\u80fd\u591f\u5728\u6574\u4e2a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u53ef\u9760\u5730\u6ee1\u8db3\u03bcP\u7684\u8c31\u6761\u4ef6\uff0c\u65e0\u9700\u663e\u5f0f\u6743\u91cd\u8c31\u5f52\u4e00\u5316\uff0c\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u03bc\u53c2\u6570\u5316\uff08\u03bcP\uff09\u4e3aLLM\u8bad\u7ec3\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5728\u77e9\u9635\u4f18\u5316\u5668\uff08\u5982Muon\uff09\u4e0a\u5b58\u5728\u5c40\u9650\u6027\uff1a\u8981\u4e48\u65e0\u6cd5\u5728\u6574\u4e2a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4fdd\u8bc1\u8c31\u6761\u4ef6\uff0c\u8981\u4e48\u9700\u8981\u91cd\u590d\u7684\u8c31\u5f52\u4e00\u5316\u5bfc\u81f4\u8ba1\u7b97\u5f00\u9500\u5927\u3002", "method": "\u63d0\u51faMuon++\u53d8\u4f53\uff0c\u5173\u952e\u89c1\u89e3\u662f\u5bf9\u4e8e\u4e2d\u7b49\u89c4\u6a21\u6a21\u578b\uff0c\u4ec5\u9700\u5728\u4f18\u5316\u5668\u66f4\u65b0\u5c42\u9762\u4fdd\u6301\u8c31\u63a7\u5236\u5373\u53ef\u7ef4\u6301\u03bcP\u517c\u5bb9\u7684\u7f29\u653e\uff0c\u65e0\u9700\u663e\u5f0f\u6743\u91cd\u8c31\u5f52\u4e00\u5316\u3002\u8fd8\u9996\u6b21\u5f15\u5165\u6570\u636e\u4f9d\u8d56\u7684\u81ea\u9002\u5e94\u8c31\u6761\u4ef6\u3002", "result": "Muon++\u80fd\u591f\u5728\u6574\u4e2a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6ee1\u8db3\u03bcP\u7684\u8c31\u6761\u4ef6\uff0c\u586b\u8865\u4e86\u03bcP\u7406\u8bba\u627f\u8bfa\u4e0e\u77e9\u9635\u4f18\u5316\u5668\u5b9e\u9645\u90e8\u7f72\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u77e9\u9635\u4f18\u5316\u5668\u5728\u957f\u65f6\u7a0bLLM\u8bad\u7ec3\u4e2d\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u9760\u65b9\u6848\uff0c\u901a\u8fc7\u4ec5\u63a7\u5236\u66f4\u65b0\u5c42\u9762\u7684\u8c31\u6761\u4ef6\u800c\u975e\u6743\u91cd\u672c\u8eab\uff0c\u5b9e\u73b0\u4e86\u03bcP\u517c\u5bb9\u7684\u7f29\u653e\uff0c\u5e76\u9996\u6b21\u5f15\u5165\u81ea\u9002\u5e94\u8c31\u6761\u4ef6\u4ee5\u9002\u5e94\u6570\u636e\u4f9d\u8d56\u6548\u5e94\u3002"}}
{"id": "2601.01378", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01378", "abs": "https://arxiv.org/abs/2601.01378", "authors": ["Han Yuan", "Yilin Wu", "Li Zhang", "Zheng Ma"], "title": "Empowering Small Language Models with Factual Hallucination-Aware Reasoning for Financial Classification", "comment": null, "summary": "Small language models (SLMs) are increasingly used for financial classification due to their fast inference and local deployability. However, compared with large language models, SLMs are more prone to factual hallucinations in reasoning and exhibit weaker classification performance. This raises a natural question: Can mitigating factual hallucinations improve SLMs' financial classification? To address this, we propose a three-step pipeline named AAAI (Association Identification, Automated Detection, and Adaptive Inference). Experiments on three representative SLMs reveal that: (1) factual hallucinations are positively correlated with misclassifications; (2) encoder-based verifiers effectively detect factual hallucinations; and (3) incorporating feedback on factual errors enables SLMs' adaptive inference that enhances classification performance. We hope this pipeline contributes to trustworthy and effective applications of SLMs in finance.", "AI": {"tldr": "\u63d0\u51faAAAI\u4e09\u9636\u6bb5\u6d41\u7a0b\uff0c\u901a\u8fc7\u51cf\u5c11\u4e8b\u5b9e\u5e7b\u89c9\u6765\u63d0\u5347\u5c0f\u8bed\u8a00\u6a21\u578b\u5728\u91d1\u878d\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd", "motivation": "\u5c0f\u8bed\u8a00\u6a21\u578b\u5728\u91d1\u878d\u5206\u7c7b\u4e2d\u63a8\u7406\u901f\u5ea6\u5feb\u3001\u53ef\u672c\u5730\u90e8\u7f72\uff0c\u4f46\u76f8\u6bd4\u5927\u6a21\u578b\u66f4\u5bb9\u6613\u4ea7\u751f\u4e8b\u5b9e\u5e7b\u89c9\u4e14\u5206\u7c7b\u6027\u80fd\u8f83\u5f31\uff0c\u9700\u8981\u63a2\u7d22\u51cf\u5c11\u4e8b\u5b9e\u5e7b\u89c9\u662f\u5426\u80fd\u63d0\u5347\u5176\u91d1\u878d\u5206\u7c7b\u80fd\u529b", "method": "\u63d0\u51faAAAI\u4e09\u9636\u6bb5\u6d41\u7a0b\uff1a\u5173\u8054\u8bc6\u522b\u3001\u81ea\u52a8\u68c0\u6d4b\u548c\u81ea\u9002\u5e94\u63a8\u7406\u3002\u9996\u5148\u8bc6\u522b\u4e8b\u5b9e\u5173\u8054\uff0c\u7136\u540e\u4f7f\u7528\u7f16\u7801\u5668\u9a8c\u8bc1\u5668\u68c0\u6d4b\u4e8b\u5b9e\u5e7b\u89c9\uff0c\u6700\u540e\u901a\u8fc7\u4e8b\u5b9e\u9519\u8bef\u53cd\u9988\u5b9e\u73b0\u81ea\u9002\u5e94\u63a8\u7406", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff1a1\uff09\u4e8b\u5b9e\u5e7b\u89c9\u4e0e\u9519\u8bef\u5206\u7c7b\u6b63\u76f8\u5173\uff1b2\uff09\u7f16\u7801\u5668\u9a8c\u8bc1\u5668\u80fd\u6709\u6548\u68c0\u6d4b\u4e8b\u5b9e\u5e7b\u89c9\uff1b3\uff09\u7ed3\u5408\u4e8b\u5b9e\u9519\u8bef\u53cd\u9988\u7684\u81ea\u9002\u5e94\u63a8\u7406\u80fd\u63d0\u5347\u5206\u7c7b\u6027\u80fd", "conclusion": "AAAI\u6d41\u7a0b\u6709\u52a9\u4e8e\u63d0\u5347\u5c0f\u8bed\u8a00\u6a21\u578b\u5728\u91d1\u878d\u9886\u57df\u7684\u53ef\u4fe1\u5ea6\u548c\u6709\u6548\u6027\u5e94\u7528"}}
{"id": "2601.01350", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01350", "abs": "https://arxiv.org/abs/2601.01350", "authors": ["Juan Junqueras", "Florian Boudin", "May-Myo Zin", "Ha-Thanh Nguyen", "Wachara Fungwacharakorn", "Dami\u00e1n Ariel Furman", "Akiko Aizawa", "Ken Satoh"], "title": "FC-CONAN: An Exhaustively Paired Dataset for Robust Evaluation of Retrieval Systems", "comment": "Presented at NeLaMKRR@KR, 2025 (arXiv:2511.09575)", "summary": "Hate speech (HS) is a critical issue in online discourse, and one promising strategy to counter it is through the use of counter-narratives (CNs). Datasets linking HS with CNs are essential for advancing counterspeech research. However, even flagship resources like CONAN (Chung et al., 2019) annotate only a sparse subset of all possible HS-CN pairs, limiting evaluation. We introduce FC-CONAN (Fully Connected CONAN), the first dataset created by exhaustively considering all combinations of 45 English HS messages and 129 CNs. A two-stage annotation process involving nine annotators and four validators produces four partitions-Diamond, Gold, Silver, and Bronze-that balance reliability and scale. None of the labeled pairs overlap with CONAN, uncovering hundreds of previously unlabelled positives. FC-CONAN enables more faithful evaluation of counterspeech retrieval systems and facilitates detailed error analysis. The dataset is publicly available.", "AI": {"tldr": "FC-CONAN\u662f\u9996\u4e2a\u5b8c\u5168\u8fde\u63a5\u7684\u4ec7\u6068\u8a00\u8bba-\u53cd\u53d9\u4e8b\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u7a77\u4e3e45\u6761\u4ec7\u6068\u8a00\u8bba\u548c129\u6761\u53cd\u53d9\u4e8b\u7684\u6240\u6709\u7ec4\u5408\u6784\u5efa\uff0c\u5305\u542b\u56db\u4e2a\u4e0d\u540c\u53ef\u9760\u6027\u7684\u6807\u6ce8\u5206\u533a\uff0c\u4e3a\u53cd\u8a00\u8bba\u7814\u7a76\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "motivation": "\u73b0\u6709\u4ec7\u6068\u8a00\u8bba-\u53cd\u53d9\u4e8b\u6570\u636e\u96c6\uff08\u5982CONAN\uff09\u53ea\u6807\u6ce8\u4e86\u7a00\u758f\u7684\u914d\u5bf9\u7ec4\u5408\uff0c\u9650\u5236\u4e86\u53cd\u8a00\u8bba\u68c0\u7d22\u7cfb\u7edf\u7684\u8bc4\u4f30\u6548\u679c\u3002\u9700\u8981\u66f4\u5168\u9762\u7684\u6570\u636e\u96c6\u6765\u652f\u6301\u66f4\u51c6\u786e\u7684\u7814\u7a76\u8bc4\u4f30\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6807\u6ce8\u6d41\u7a0b\uff1a1\uff09\u7a77\u4e3e45\u6761\u4ec7\u6068\u8a00\u8bba\u548c129\u6761\u53cd\u53d9\u4e8b\u7684\u6240\u6709\u7ec4\u5408\uff085805\u5bf9\uff09\uff1b2\uff09\u75319\u540d\u6807\u6ce8\u5458\u548c4\u540d\u9a8c\u8bc1\u5458\u8fdb\u884c\u6807\u6ce8\uff0c\u5f62\u6210\u94bb\u77f3\u3001\u9ec4\u91d1\u3001\u767d\u94f6\u3001\u9752\u94dc\u56db\u4e2a\u4e0d\u540c\u53ef\u9760\u6027\u7684\u5206\u533a\u3002", "result": "\u521b\u5efa\u4e86FC-CONAN\u6570\u636e\u96c6\uff0c\u5305\u542b\u6570\u767e\u4e2a\u4e4b\u524d\u672a\u6807\u6ce8\u7684\u6b63\u6837\u672c\uff0c\u4e0d\u91cd\u53e0\u4e8eCONAN\u6570\u636e\u96c6\u3002\u8be5\u6570\u636e\u96c6\u652f\u6301\u66f4\u5fe0\u5b9e\u7684\u53cd\u8a00\u8bba\u68c0\u7d22\u7cfb\u7edf\u8bc4\u4f30\u548c\u8be6\u7ec6\u7684\u9519\u8bef\u5206\u6790\u3002", "conclusion": "FC-CONAN\u586b\u8865\u4e86\u4ec7\u6068\u8a00\u8bba-\u53cd\u53d9\u4e8b\u6570\u636e\u96c6\u7684\u7a7a\u767d\uff0c\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u53cd\u8a00\u8bba\u7814\u7a76\u7684\u53d1\u5c55\u3002\u6570\u636e\u96c6\u5df2\u516c\u5f00\u53ef\u7528\u3002"}}
{"id": "2601.02193", "categories": ["cs.LG", "cs.DS", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.02193", "abs": "https://arxiv.org/abs/2601.02193", "authors": ["Kasper Green Larsen", "Chirag Pabbaraju", "Abhishek Shetty"], "title": "Learning with Monotone Adversarial Corruptions", "comment": null, "summary": "We study the extent to which standard machine learning algorithms rely on exchangeability and independence of data by introducing a monotone adversarial corruption model. In this model, an adversary, upon looking at a \"clean\" i.i.d. dataset, inserts additional \"corrupted\" points of their choice into the dataset. These added points are constrained to be monotone corruptions, in that they get labeled according to the ground-truth target function. Perhaps surprisingly, we demonstrate that in this setting, all known optimal learning algorithms for binary classification can be made to achieve suboptimal expected error on a new independent test point drawn from the same distribution as the clean dataset. On the other hand, we show that uniform convergence-based algorithms do not degrade in their guarantees. Our results showcase how optimal learning algorithms break down in the face of seemingly helpful monotone corruptions, exposing their overreliance on exchangeability.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u5373\u4f7f\u9762\u5bf9\u770b\u4f3c\u6709\u76ca\u7684\u5355\u8c03\u5bf9\u6297\u6027\u6570\u636e\u6c61\u67d3\uff0c\u6807\u51c6\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u4e5f\u4f1a\u56e0\u4e3a\u8fc7\u5ea6\u4f9d\u8d56\u6570\u636e\u7684\u53ef\u4ea4\u6362\u6027\u800c\u6027\u80fd\u4e0b\u964d\u3002", "motivation": "\u63a2\u7d22\u6807\u51c6\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u5bf9\u6570\u636e\u53ef\u4ea4\u6362\u6027\u548c\u72ec\u7acb\u6027\u7684\u4f9d\u8d56\u7a0b\u5ea6\uff0c\u901a\u8fc7\u5f15\u5165\u5355\u8c03\u5bf9\u6297\u6027\u6c61\u67d3\u6a21\u578b\u6765\u6d4b\u8bd5\u7b97\u6cd5\u5728\u9762\u5bf9\u770b\u4f3c\u6709\u76ca\u7684\u6570\u636e\u6c61\u67d3\u65f6\u7684\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u5355\u8c03\u5bf9\u6297\u6027\u6c61\u67d3\u6a21\u578b\uff1a\u653b\u51fb\u8005\u89c2\u5bdf\u5e72\u51c0\u7684i.i.d.\u6570\u636e\u96c6\u540e\uff0c\u63d2\u5165\u81ea\u5df1\u9009\u62e9\u7684\"\u6c61\u67d3\"\u6570\u636e\u70b9\uff0c\u4f46\u8fd9\u4e9b\u6c61\u67d3\u70b9\u5fc5\u987b\u9075\u5faa\u5355\u8c03\u6027\u7ea6\u675f\uff0c\u5373\u6839\u636e\u771f\u5b9e\u76ee\u6807\u51fd\u6570\u8fdb\u884c\u6807\u6ce8\u3002", "result": "\u53d1\u73b0\u6240\u6709\u5df2\u77e5\u7684\u6700\u4f18\u4e8c\u5206\u7c7b\u7b97\u6cd5\u5728\u8fd9\u79cd\u8bbe\u7f6e\u4e0b\u90fd\u4f1a\u5728\u65b0\u6d4b\u8bd5\u70b9\u4e0a\u83b7\u5f97\u6b21\u4f18\u7684\u671f\u671b\u8bef\u5dee\uff1b\u800c\u57fa\u4e8e\u4e00\u81f4\u6536\u655b\u7684\u7b97\u6cd5\u5219\u4e0d\u4f1a\u964d\u4f4e\u6027\u80fd\u4fdd\u8bc1\u3002", "conclusion": "\u6700\u4f18\u5b66\u4e60\u7b97\u6cd5\u5728\u9762\u5bf9\u770b\u4f3c\u6709\u76ca\u7684\u5355\u8c03\u6c61\u67d3\u65f6\u4f1a\u5d29\u6e83\uff0c\u66b4\u9732\u4e86\u5b83\u4eec\u5bf9\u6570\u636e\u53ef\u4ea4\u6362\u6027\u7684\u8fc7\u5ea6\u4f9d\u8d56\uff0c\u800c\u57fa\u4e8e\u4e00\u81f4\u6536\u655b\u7684\u7b97\u6cd5\u5219\u66f4\u52a0\u9c81\u68d2\u3002"}}
{"id": "2601.00898", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.00898", "abs": "https://arxiv.org/abs/2601.00898", "authors": ["Ruiming Liang", "Yinan Zheng", "Kexin Zheng", "Tianyi Tan", "Jianxiong Li", "Liyuan Mao", "Zhihao Wang", "Guang Chen", "Hangjun Ye", "Jingjing Liu", "Jinqiao Wang", "Xianyuan Zhan"], "title": "Dichotomous Diffusion Policy Optimization", "comment": null, "summary": "Diffusion-based policies have gained growing popularity in solving a wide range of decision-making tasks due to their superior expressiveness and controllable generation during inference. However, effectively training large diffusion policies using reinforcement learning (RL) remains challenging. Existing methods either suffer from unstable training due to directly maximizing value objectives, or face computational issues due to relying on crude Gaussian likelihood approximation, which requires a large amount of sufficiently small denoising steps. In this work, we propose DIPOLE (Dichotomous diffusion Policy improvement), a novel RL algorithm designed for stable and controllable diffusion policy optimization. We begin by revisiting the KL-regularized objective in RL, which offers a desirable weighted regression objective for diffusion policy extraction, but often struggles to balance greediness and stability. We then formulate a greedified policy regularization scheme, which naturally enables decomposing the optimal policy into a pair of stably learned dichotomous policies: one aims at reward maximization, and the other focuses on reward minimization. Under such a design, optimized actions can be generated by linearly combining the scores of dichotomous policies during inference, thereby enabling flexible control over the level of greediness.Evaluations in offline and offline-to-online RL settings on ExORL and OGBench demonstrate the effectiveness of our approach. We also use DIPOLE to train a large vision-language-action (VLA) model for end-to-end autonomous driving (AD) and evaluate it on the large-scale real-world AD benchmark NAVSIM, highlighting its potential for complex real-world applications.", "AI": {"tldr": "DIPOLE\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u5206\u89e3\u4e3a\u5956\u52b1\u6700\u5927\u5316\u548c\u6700\u5c0f\u5316\u7684\u4e8c\u5206\u7b56\u7565\uff0c\u5b9e\u73b0\u7a33\u5b9a\u53ef\u63a7\u7684\u6269\u6563\u7b56\u7565\u4f18\u5316\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u7b56\u7565\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u9762\u4e34\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6216\u8ba1\u7b97\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u7a33\u5b9a\u8bad\u7ec3\u53c8\u80fd\u7075\u6d3b\u63a7\u5236\u8d2a\u5a6a\u5ea6\u7684\u6269\u6563\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u63d0\u51faDIPOLE\u7b97\u6cd5\uff0c\u57fa\u4e8eKL\u6b63\u5219\u5316\u76ee\u6807\u91cd\u65b0\u8bbe\u8ba1\uff0c\u901a\u8fc7\u8d2a\u5a6a\u5316\u7b56\u7565\u6b63\u5219\u5316\u65b9\u6848\u5c06\u6700\u4f18\u7b56\u7565\u5206\u89e3\u4e3a\u4e00\u5bf9\u4e8c\u5206\u7b56\u7565\uff1a\u4e00\u4e2a\u6700\u5927\u5316\u5956\u52b1\uff0c\u4e00\u4e2a\u6700\u5c0f\u5316\u5956\u52b1\u3002\u63a8\u7406\u65f6\u901a\u8fc7\u7ebf\u6027\u7ec4\u5408\u4e24\u4e2a\u7b56\u7565\u7684\u5206\u6570\u751f\u6210\u52a8\u4f5c\u3002", "result": "\u5728ExORL\u548cOGBench\u7684\u79bb\u7ebf\u548c\u79bb\u7ebf\u5230\u5728\u7ebfRL\u8bbe\u7f6e\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\uff0c\u5e76\u6210\u529f\u8bad\u7ec3\u4e86\u7528\u4e8e\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\uff0c\u5728NAVSIM\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "DIPOLE\u4e3a\u6269\u6563\u7b56\u7565\u63d0\u4f9b\u4e86\u7a33\u5b9a\u53ef\u63a7\u7684\u4f18\u5316\u6846\u67b6\uff0c\u5728\u590d\u6742\u51b3\u7b56\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2601.01065", "categories": ["cs.LG", "eess.SP", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.01065", "abs": "https://arxiv.org/abs/2601.01065", "authors": ["Achraf Hsain", "Yahya Zaki", "Othman Abaakil", "Hibat-allah Bekkar", "Yousra Chtouki"], "title": "Tiny Machine Learning for Real-Time Aquaculture Monitoring: A Case Study in Morocco", "comment": "Published in IEEE GCAIoT 2024", "summary": "Aquaculture, the farming of aquatic organisms, is a rapidly growing industry facing challenges such as water quality fluctuations, disease outbreaks, and inefficient feed management. Traditional monitoring methods often rely on manual labor and are time consuming, leading to potential delays in addressing issues. This paper proposes the integration of low-power edge devices using Tiny Machine Learning (TinyML) into aquaculture systems to enable real-time automated monitoring and control, such as collecting data and triggering alarms, and reducing labor requirements. The system provides real-time data on the required parameters such as pH levels, temperature, dissolved oxygen, and ammonia levels to control water quality, nutrient levels, and environmental conditions enabling better maintenance, efficient resource utilization, and optimal management of the enclosed aquaculture space. The system enables alerts in case of anomaly detection. The data collected by the sensors over time can serve for important decision-making regarding optimizing water treatment processes, feed distribution, feed pattern analysis and improve feed efficiency, reducing operational costs. This research explores the feasibility of developing TinyML-based solutions for aquaculture monitoring, considering factors such as sensor selection, algorithm design, hardware constraints, and ethical considerations. By demonstrating the potential benefits of TinyML in aquaculture, our aim is to contribute to the development of more sustainable and efficient farming practices.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5c06\u57fa\u4e8eTinyML\u7684\u4f4e\u529f\u8017\u8fb9\u7f18\u8bbe\u5907\u96c6\u6210\u5230\u6c34\u4ea7\u517b\u6b96\u7cfb\u7edf\u4e2d\uff0c\u5b9e\u73b0\u5b9e\u65f6\u81ea\u52a8\u5316\u76d1\u6d4b\u548c\u63a7\u5236\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u4eba\u5de5\u76d1\u6d4b\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\u3002", "motivation": "\u6c34\u4ea7\u517b\u6b96\u4e1a\u9762\u4e34\u6c34\u8d28\u6ce2\u52a8\u3001\u75be\u75c5\u7206\u53d1\u548c\u9972\u6599\u7ba1\u7406\u4f4e\u6548\u7b49\u6311\u6218\uff0c\u4f20\u7edf\u4eba\u5de5\u76d1\u6d4b\u65b9\u6cd5\u8017\u65f6\u4e14\u53ef\u80fd\u5bfc\u81f4\u95ee\u9898\u5904\u7406\u5ef6\u8fdf\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u96c6\u6210\u57fa\u4e8eTinyML\u7684\u4f4e\u529f\u8017\u8fb9\u7f18\u8bbe\u5907\uff0c\u901a\u8fc7\u4f20\u611f\u5668\u5b9e\u65f6\u6536\u96c6pH\u503c\u3001\u6e29\u5ea6\u3001\u6eb6\u89e3\u6c27\u3001\u6c28\u542b\u91cf\u7b49\u53c2\u6570\u6570\u636e\uff0c\u5b9e\u73b0\u5f02\u5e38\u68c0\u6d4b\u548c\u81ea\u52a8\u62a5\u8b66\uff0c\u540c\u65f6\u8003\u8651\u4f20\u611f\u5668\u9009\u62e9\u3001\u7b97\u6cd5\u8bbe\u8ba1\u3001\u786c\u4ef6\u7ea6\u675f\u548c\u4f26\u7406\u56e0\u7d20\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u63d0\u4f9b\u5b9e\u65f6\u6c34\u8d28\u76d1\u6d4b\u3001\u5f02\u5e38\u8b66\u62a5\uff0c\u6536\u96c6\u7684\u6570\u636e\u53ef\u7528\u4e8e\u4f18\u5316\u6c34\u5904\u7406\u8fc7\u7a0b\u3001\u9972\u6599\u5206\u914d\u548c\u6a21\u5f0f\u5206\u6790\uff0c\u63d0\u9ad8\u9972\u6599\u6548\u7387\u5e76\u964d\u4f4e\u8fd0\u8425\u6210\u672c\u3002", "conclusion": "TinyML\u6280\u672f\u5728\u6c34\u4ea7\u517b\u6b96\u76d1\u6d4b\u4e2d\u5177\u6709\u53ef\u884c\u6027\uff0c\u80fd\u591f\u5b9e\u73b0\u66f4\u53ef\u6301\u7eed\u548c\u9ad8\u6548\u7684\u517b\u6b96\u5b9e\u8df5\uff0c\u4e3a\u884c\u4e1a\u53d1\u5c55\u63d0\u4f9b\u667a\u80fd\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01493", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.01493", "abs": "https://arxiv.org/abs/2601.01493", "authors": ["Yijie Zhou", "Shi Pu"], "title": "Accelerating Decentralized Optimization via Overlapping Local Steps", "comment": null, "summary": "Decentralized optimization has emerged as a critical paradigm for distributed learning, enabling scalable training while preserving data privacy through peer-to-peer collaboration. However, existing methods often suffer from communication bottlenecks due to frequent synchronization between nodes. We present Overlapping Local Decentralized SGD (OLDSGD), a novel approach to accelerate decentralized training by computation-communication overlapping, significantly reducing network idle time. With a deliberately designed update, OLDSGD preserves the same average update as Local SGD while avoiding communication-induced stalls. Theoretically, we establish non-asymptotic convergence rates for smooth non-convex objectives, showing that OLDSGD retains the same iteration complexity as standard Local Decentralized SGD while improving per-iteration runtime. Empirical results demonstrate OLDSGD's consistent improvements in wall-clock time convergence under different levels of communication delays. With minimal modifications to existing frameworks, OLDSGD offers a practical solution for faster decentralized learning without sacrificing theoretical guarantees.", "AI": {"tldr": "OLDSGD\u662f\u4e00\u79cd\u901a\u8fc7\u8ba1\u7b97-\u901a\u4fe1\u91cd\u53e0\u52a0\u901f\u53bb\u4e2d\u5fc3\u5316\u8bad\u7ec3\u7684\u65b0\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u4e0eLocal SGD\u76f8\u540c\u5e73\u5747\u66f4\u65b0\u7684\u540c\u65f6\u51cf\u5c11\u7f51\u7edc\u7a7a\u95f2\u65f6\u95f4\uff0c\u63d0\u5347\u5b9e\u9645\u8bad\u7ec3\u901f\u5ea6\u3002", "motivation": "\u53bb\u4e2d\u5fc3\u5316\u4f18\u5316\u5728\u5206\u5e03\u5f0f\u5b66\u4e60\u4e2d\u5f88\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u56e0\u8282\u70b9\u95f4\u9891\u7e41\u540c\u6b65\u800c\u9762\u4e34\u901a\u4fe1\u74f6\u9888\u95ee\u9898\uff0c\u9700\u8981\u51cf\u5c11\u7f51\u7edc\u7a7a\u95f2\u65f6\u95f4\u4ee5\u52a0\u901f\u8bad\u7ec3\u3002", "method": "\u63d0\u51fa\u91cd\u53e0\u5c40\u90e8\u53bb\u4e2d\u5fc3\u5316SGD\uff08OLDSGD\uff09\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u66f4\u65b0\u673a\u5236\u5b9e\u73b0\u8ba1\u7b97\u548c\u901a\u4fe1\u7684\u91cd\u53e0\uff0c\u5728\u4fdd\u6301\u4e0eLocal SGD\u76f8\u540c\u5e73\u5747\u66f4\u65b0\u7684\u540c\u65f6\u907f\u514d\u901a\u4fe1\u5bfc\u81f4\u7684\u505c\u6ede\u3002", "result": "\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86OLDSGD\u5728\u5149\u6ed1\u975e\u51f8\u76ee\u6807\u4e0a\u5177\u6709\u975e\u6e10\u8fdb\u6536\u655b\u7387\uff0c\u4fdd\u6301\u4e0e\u6807\u51c6Local Decentralized SGD\u76f8\u540c\u7684\u8fed\u4ee3\u590d\u6742\u5ea6\u4f46\u6539\u5584\u6bcf\u6b21\u8fed\u4ee3\u8fd0\u884c\u65f6\u95f4\u3002\u5b9e\u9a8c\u663e\u793a\u5728\u4e0d\u540c\u901a\u4fe1\u5ef6\u8fdf\u4e0b\u90fd\u80fd\u63d0\u5347\u5b9e\u9645\u65f6\u95f4\u6536\u655b\u901f\u5ea6\u3002", "conclusion": "OLDSGD\u901a\u8fc7\u5bf9\u73b0\u6709\u6846\u67b6\u7684\u6700\u5c0f\u4fee\u6539\uff0c\u63d0\u4f9b\u4e86\u4e0d\u727a\u7272\u7406\u8bba\u4fdd\u8bc1\u7684\u66f4\u5feb\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u5b9e\u7528\u65b9\u6848\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u901a\u4fe1\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2601.01467", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01467", "abs": "https://arxiv.org/abs/2601.01467", "authors": ["Romuald Kwessy Mouona", "Blaise Bl\u00e9riot Koguep Njionou", "Etienne Romuald Temgoua Alomo", "Rokia Missaoui", "Leonard Kwuida"], "title": "A construction of an optimal base for conditional attribute and attributional condition implications in triadic contexts", "comment": "26 pages", "summary": "This article studies implications in triadic contexts. Specifically, we focus on those introduced by Ganter and Obiedkov, namely conditional attribute and attributional condition implications. Our aim is to construct an optimal base for these implications.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e09\u5143\u4e0a\u4e0b\u6587\u4e2d\u7684\u8574\u6db5\u5173\u7cfb\uff0c\u91cd\u70b9\u5206\u6790Ganter\u548cObiedkov\u5f15\u5165\u7684\u6761\u4ef6\u5c5e\u6027\u8574\u6db5\u548c\u5c5e\u6027\u6761\u4ef6\u8574\u6db5\uff0c\u65e8\u5728\u4e3a\u8fd9\u4e9b\u8574\u6db5\u6784\u5efa\u6700\u4f18\u57fa", "motivation": "\u5728\u4e09\u5143\u4e0a\u4e0b\u6587\uff08triadic contexts\uff09\u4e2d\uff0c\u73b0\u6709\u7684\u8574\u6db5\u7406\u8bba\u9700\u8981\u8fdb\u4e00\u6b65\u6269\u5c55\u548c\u5b8c\u5584\u3002Ganter\u548cObiedkov\u5f15\u5165\u7684\u6761\u4ef6\u5c5e\u6027\u8574\u6db5\u548c\u5c5e\u6027\u6761\u4ef6\u8574\u6db5\u4e3a\u4e09\u5143\u6570\u636e\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u5de5\u5177\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u7684\u6700\u4f18\u57fa\u6784\u9020\u65b9\u6cd5", "method": "\u7814\u7a76\u4e09\u5143\u4e0a\u4e0b\u6587\u4e2d\u7684\u8574\u6db5\u5173\u7cfb\uff0c\u5206\u6790\u6761\u4ef6\u5c5e\u6027\u8574\u6db5\u548c\u5c5e\u6027\u6761\u4ef6\u8574\u6db5\u7684\u7279\u6027\uff0c\u5f00\u53d1\u6784\u9020\u8fd9\u4e9b\u8574\u6db5\u6700\u4f18\u57fa\u7684\u7b97\u6cd5\u6216\u65b9\u6cd5", "result": "\u63d0\u51fa\u4e86\u4e3a\u4e09\u5143\u4e0a\u4e0b\u6587\u4e2d\u7684\u6761\u4ef6\u5c5e\u6027\u8574\u6db5\u548c\u5c5e\u6027\u6761\u4ef6\u8574\u6db5\u6784\u9020\u6700\u4f18\u57fa\u7684\u65b9\u6cd5\uff0c\u53ef\u80fd\u5305\u62ec\u57fa\u7684\u5b8c\u5907\u6027\u3001\u975e\u5197\u4f59\u6027\u548c\u6700\u5c0f\u6027\u7b49\u6027\u8d28", "conclusion": "\u6210\u529f\u6269\u5c55\u4e86\u5f62\u5f0f\u6982\u5ff5\u5206\u6790\u4e2d\u7684\u8574\u6db5\u7406\u8bba\u5230\u4e09\u5143\u4e0a\u4e0b\u6587\uff0c\u4e3a\u4e09\u5143\u6570\u636e\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8574\u6db5\u57fa\u6784\u9020\u5de5\u5177\uff0c\u589e\u5f3a\u4e86\u4e09\u5143\u6570\u636e\u6316\u6398\u7684\u80fd\u529b"}}
{"id": "2601.01511", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01511", "abs": "https://arxiv.org/abs/2601.01511", "authors": ["Ahmed Dawoud", "Osama El-Shamy"], "title": "Reading Between the Lines: Deconfounding Causal Estimates using Text Embeddings and Deep Learning", "comment": null, "summary": "Estimating causal treatment effects in observational settings is frequently compromised by selection bias arising from unobserved confounders. While traditional econometric methods struggle when these confounders are orthogonal to structured covariates, high-dimensional unstructured text often contains rich proxies for these latent variables. This study proposes a Neural Network-Enhanced Double Machine Learning (DML) framework designed to leverage text embeddings for causal identification. Using a rigorous synthetic benchmark, we demonstrate that unstructured text embeddings capture critical confounding information that is absent from structured tabular data. However, we show that standard tree-based DML estimators retain substantial bias (+24%) due to their inability to model the continuous topology of embedding manifolds. In contrast, our deep learning approach reduces bias to -0.86% with optimized architectures, effectively recovering the ground-truth causal parameter. These findings suggest that deep learning architectures are essential for satisfying the unconfoundedness assumption when conditioning on high-dimensional natural language data", "AI": {"tldr": "\u63d0\u51fa\u795e\u7ecf\u7f51\u7edc\u589e\u5f3a\u7684\u53cc\u91cd\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u6587\u672c\u5d4c\u5165\u89e3\u51b3\u672a\u89c2\u6d4b\u6df7\u6742\u53d8\u91cf\u95ee\u9898\uff0c\u76f8\u6bd4\u4f20\u7edf\u6811\u6a21\u578b\u80fd\u663e\u8457\u51cf\u5c11\u4f30\u8ba1\u504f\u5dee", "motivation": "\u5728\u89c2\u6d4b\u6027\u7814\u7a76\u4e2d\uff0c\u672a\u89c2\u6d4b\u6df7\u6742\u53d8\u91cf\u5bfc\u81f4\u7684\u9009\u62e9\u504f\u5dee\u662f\u56e0\u679c\u6548\u5e94\u4f30\u8ba1\u7684\u4e3b\u8981\u6311\u6218\u3002\u4f20\u7edf\u8ba1\u91cf\u65b9\u6cd5\u5728\u6df7\u6742\u53d8\u91cf\u4e0e\u7ed3\u6784\u5316\u534f\u53d8\u91cf\u6b63\u4ea4\u65f6\u6548\u679c\u4e0d\u4f73\uff0c\u800c\u9ad8\u7ef4\u975e\u7ed3\u6784\u5316\u6587\u672c\u6570\u636e\u901a\u5e38\u5305\u542b\u8fd9\u4e9b\u6f5c\u5728\u53d8\u91cf\u7684\u4e30\u5bcc\u4ee3\u7406\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u795e\u7ecf\u7f51\u7edc\u589e\u5f3a\u7684\u53cc\u91cd\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u6587\u672c\u5d4c\u5165\u8fdb\u884c\u56e0\u679c\u8bc6\u522b\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u5efa\u6a21\u5d4c\u5165\u6d41\u5f62\u7684\u8fde\u7eed\u62d3\u6251\u7ed3\u6784\uff0c\u4f18\u5316\u67b6\u6784\u8bbe\u8ba1\u4ee5\u6709\u6548\u6355\u83b7\u6df7\u6742\u4fe1\u606f\u3002", "result": "\u4f7f\u7528\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u8868\u660e\uff1a1\uff09\u975e\u7ed3\u6784\u5316\u6587\u672c\u5d4c\u5165\u80fd\u6355\u83b7\u7ed3\u6784\u5316\u8868\u683c\u6570\u636e\u4e2d\u7f3a\u5931\u7684\u5173\u952e\u6df7\u6742\u4fe1\u606f\uff1b2\uff09\u6807\u51c6\u6811\u57faDML\u4f30\u8ba1\u5668\u7531\u4e8e\u65e0\u6cd5\u5efa\u6a21\u5d4c\u5165\u6d41\u5f62\u7684\u8fde\u7eed\u62d3\u6251\uff0c\u4ecd\u5b58\u5728\u663e\u8457\u504f\u5dee\uff08+24%\uff09\uff1b3\uff09\u63d0\u51fa\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5c06\u504f\u5dee\u964d\u81f3-0.86%\uff0c\u6709\u6548\u6062\u590d\u4e86\u771f\u5b9e\u56e0\u679c\u53c2\u6570\u3002", "conclusion": "\u5f53\u57fa\u4e8e\u9ad8\u7ef4\u81ea\u7136\u8bed\u8a00\u6570\u636e\u8fdb\u884c\u6761\u4ef6\u5206\u6790\u65f6\uff0c\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u5bf9\u4e8e\u6ee1\u8db3\u65e0\u6df7\u6742\u5047\u8bbe\u81f3\u5173\u91cd\u8981\u3002\u795e\u7ecf\u7f51\u7edc\u589e\u5f3a\u7684DML\u6846\u67b6\u80fd\u6709\u6548\u5229\u7528\u6587\u672c\u5d4c\u5165\u4e2d\u7684\u6df7\u6742\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u9ad8\u56e0\u679c\u6548\u5e94\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2601.01400", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01400", "abs": "https://arxiv.org/abs/2601.01400", "authors": ["Jicheng Ma", "Guohua Wang", "Xinhua Feng", "Yiming Liu", "Zhichao Hu", "Yuhong Liu"], "title": "EternalMath: A Living Benchmark of Frontier Mathematics that Evolves with Human Discovery", "comment": null, "summary": "Current evaluations of mathematical reasoning in large language models (LLMs) are dominated by static benchmarks, either derived from competition-style problems or curated through costly expert effort, resulting in limited coverage of research-level mathematics and rapid performance saturation. We propose a fully automated, theorem-grounded pipeline for evaluating frontier mathematical reasoning, which directly transforms recent peer-reviewed mathematical literature into executable and verifiable reasoning tasks. The pipeline identifies constructive or quantitative results, instantiates them into parameterized problem templates, and generates deterministic solutions through execution-based verification, enabling scalable, reproducible, and continuously updatable evaluation without reliance on large-scale expert authoring. By design, this approach supports temporal extensibility, intrinsic correctness checking, and domain-specific customization across mathematical subfields. Applying this pipeline yields \\textbf{EternalMath}, an evolving evaluation suite derived from contemporary research papers. Experiments with state-of-the-art LLMs reveal substantial performance gaps, indicating that mathematical reasoning at the research frontier remains far from saturated and underscoring the need for evaluation methodologies that evolve in step with human mathematical discovery.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u52a8\u5316\u5b9a\u7406\u9a8c\u8bc1\u8bc4\u4f30\u6846\u67b6EternalMath\uff0c\u76f4\u63a5\u4ece\u6700\u65b0\u6570\u5b66\u7814\u7a76\u6587\u732e\u751f\u6210\u53ef\u6267\u884c\u9a8c\u8bc1\u7684\u63a8\u7406\u4efb\u52a1\uff0c\u89e3\u51b3\u73b0\u6709\u6570\u5b66\u63a8\u7406\u8bc4\u4f30\u8986\u76d6\u6709\u9650\u4e14\u6613\u9971\u548c\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6570\u5b66\u63a8\u7406\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u9759\u6001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8fd9\u4e9b\u6d4b\u8bd5\u8981\u4e48\u6765\u81ea\u7ade\u8d5b\u98ce\u683c\u95ee\u9898\uff0c\u8981\u4e48\u9700\u8981\u4e13\u5bb6\u7cbe\u5fc3\u8bbe\u8ba1\uff0c\u5bfc\u81f4\u8986\u76d6\u8303\u56f4\u6709\u9650\uff08\u7279\u522b\u662f\u7814\u7a76\u7ea7\u6570\u5b66\uff09\uff0c\u4e14\u6027\u80fd\u5bb9\u6613\u5feb\u901f\u9971\u548c\u3002", "method": "\u63d0\u51fa\u5168\u81ea\u52a8\u3001\u57fa\u4e8e\u5b9a\u7406\u7684\u8bc4\u4f30\u6d41\u6c34\u7ebf\uff1a1) \u4ece\u8fd1\u671f\u540c\u884c\u8bc4\u5ba1\u7684\u6570\u5b66\u6587\u732e\u4e2d\u8bc6\u522b\u6784\u9020\u6027\u6216\u5b9a\u91cf\u7ed3\u679c\uff1b2) \u5c06\u5176\u5b9e\u4f8b\u5316\u4e3a\u53c2\u6570\u5316\u95ee\u9898\u6a21\u677f\uff1b3) \u901a\u8fc7\u57fa\u4e8e\u6267\u884c\u7684\u9a8c\u8bc1\u751f\u6210\u786e\u5b9a\u6027\u89e3\u51b3\u65b9\u6848\uff0c\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u53ef\u590d\u73b0\u3001\u6301\u7eed\u66f4\u65b0\u7684\u8bc4\u4f30\u3002", "result": "\u5e94\u7528\u8be5\u6d41\u6c34\u7ebf\u521b\u5efa\u4e86EternalMath\u8bc4\u4f30\u5957\u4ef6\uff0c\u5b9e\u9a8c\u663e\u793a\u6700\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u8ddd\uff0c\u8868\u660e\u7814\u7a76\u524d\u6cbf\u7684\u6570\u5b66\u63a8\u7406\u8fdc\u672a\u9971\u548c\u3002", "conclusion": "\u6570\u5b66\u63a8\u7406\u8bc4\u4f30\u9700\u8981\u4e0e\u4eba\u7c7b\u6570\u5b66\u53d1\u73b0\u540c\u6b65\u6f14\u8fdb\u7684\u65b9\u6cd5\u8bba\uff0c\u57fa\u4e8e\u6700\u65b0\u7814\u7a76\u6587\u732e\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u6846\u67b6\u80fd\u63d0\u4f9b\u66f4\u771f\u5b9e\u3001\u6301\u7eed\u66f4\u65b0\u7684\u8bc4\u4f30\u57fa\u51c6\u3002"}}
{"id": "2601.00915", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00915", "abs": "https://arxiv.org/abs/2601.00915", "authors": ["Jacquelyn Shelton", "Przemyslaw Polewski", "Alexander Robel", "Matthew Hoffman", "Stephen Price"], "title": "Latent-Constrained Conditional VAEs for Augmenting Large-Scale Climate Ensembles", "comment": "draft / preliminary", "summary": "Large climate-model ensembles are computationally expensive; yet many downstream analyses would benefit from additional, statistically consistent realizations of spatiotemporal climate variables. We study a generative modeling approach for producing new realizations from a limited set of available runs by transferring structure learned across an ensemble. Using monthly near-surface temperature time series from ten independent reanalysis realizations (ERA5), we find that a vanilla conditional variational autoencoder (CVAE) trained jointly across realizations yields a fragmented latent space that fails to generalize to unseen ensemble members. To address this, we introduce a latent-constrained CVAE (LC-CVAE) that enforces cross-realization homogeneity of latent embeddings at a small set of shared geographic 'anchor' locations. We then use multi-output Gaussian process regression in the latent space to predict latent coordinates at unsampled locations in a new realization, followed by decoding to generate full time series fields. Experiments and ablations demonstrate (i) instability when training on a single realization, (ii) diminishing returns after incorporating roughly five realizations, and (iii) a trade-off between spatial coverage and reconstruction quality that is closely linked to the average neighbor distance in latent space.", "AI": {"tldr": "\u63d0\u51faLC-CVAE\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f3a\u5236\u6f5c\u5728\u7a7a\u95f4\u5728\u951a\u70b9\u4f4d\u7f6e\u7684\u4e00\u81f4\u6027\uff0c\u89e3\u51b3\u4f20\u7edfCVAE\u5728\u6c14\u5019\u6a21\u578b\u96c6\u5408\u751f\u6210\u4e2d\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u5b9e\u73b0\u4ece\u6709\u9650\u771f\u5b9e\u5316\u751f\u6210\u65b0\u6c14\u5019\u573a\u3002", "motivation": "\u5927\u578b\u6c14\u5019\u6a21\u578b\u96c6\u5408\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u4f46\u8bb8\u591a\u4e0b\u6e38\u5206\u6790\u9700\u8981\u66f4\u591a\u7edf\u8ba1\u4e00\u81f4\u7684\u6c14\u5019\u53d8\u91cf\u5b9e\u73b0\u3002\u4f20\u7edf\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5728\u8de8\u771f\u5b9e\u5316\u8bad\u7ec3\u65f6\u6f5c\u5728\u7a7a\u95f4\u788e\u7247\u5316\uff0c\u65e0\u6cd5\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u96c6\u5408\u6210\u5458\u3002", "method": "\u63d0\u51fa\u6f5c\u5728\u7ea6\u675f\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668(LC-CVAE)\uff0c\u5f3a\u5236\u6f5c\u5728\u5d4c\u5165\u5728\u5171\u4eab\u5730\u7406\"\u951a\u70b9\"\u4f4d\u7f6e\u5177\u6709\u8de8\u771f\u5b9e\u5316\u540c\u8d28\u6027\u3002\u4f7f\u7528\u591a\u8f93\u51fa\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u5728\u6f5c\u5728\u7a7a\u95f4\u9884\u6d4b\u65b0\u771f\u5b9e\u5316\u4e2d\u672a\u91c7\u6837\u4f4d\u7f6e\u7684\u5750\u6807\uff0c\u7136\u540e\u89e3\u7801\u751f\u6210\u5b8c\u6574\u65f6\u95f4\u5e8f\u5217\u573a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff1a1) \u5355\u771f\u5b9e\u5316\u8bad\u7ec3\u4e0d\u7a33\u5b9a\uff1b2) \u7eb3\u5165\u7ea65\u4e2a\u771f\u5b9e\u5316\u540e\u6536\u76ca\u9012\u51cf\uff1b3) \u7a7a\u95f4\u8986\u76d6\u4e0e\u91cd\u5efa\u8d28\u91cf\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u4e0e\u6f5c\u5728\u7a7a\u95f4\u5e73\u5747\u90bb\u8fd1\u8ddd\u79bb\u5bc6\u5207\u76f8\u5173\u3002", "conclusion": "LC-CVAE\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u7ea6\u675f\u89e3\u51b3\u4e86\u8de8\u6c14\u5019\u771f\u5b9e\u5316\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u4e3a\u4ece\u6709\u9650\u6a21\u578b\u8fd0\u884c\u751f\u6210\u7edf\u8ba1\u4e00\u81f4\u7684\u65b0\u6c14\u5019\u5b9e\u73b0\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\uff0c\u5728\u8ba1\u7b97\u6210\u672c\u4e0e\u751f\u6210\u8d28\u91cf\u95f4\u53d6\u5f97\u5e73\u8861\u3002"}}
{"id": "2601.01793", "categories": ["cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.01793", "abs": "https://arxiv.org/abs/2601.01793", "authors": ["Shamik Bhattacharyya", "Rachel Kalpana Kalaimani"], "title": "Distributed Federated Learning by Alternating Periods of Training", "comment": null, "summary": "Federated learning is a privacy-focused approach towards machine learning where models are trained on client devices with locally available data and aggregated at a central server. However, the dependence on a single central server is challenging in the case of a large number of clients and even poses the risk of a single point of failure. To address these critical limitations of scalability and fault-tolerance, we present a distributed approach to federated learning comprising multiple servers with inter-server communication capabilities. While providing a fully decentralized approach, the designed framework retains the core federated learning structure where each server is associated with a disjoint set of clients with server-client communication capabilities. We propose a novel DFL (Distributed Federated Learning) algorithm which uses alternating periods of local training on the client data followed by global training among servers. We show that the DFL algorithm, under a suitable choice of parameters, ensures that all the servers converge to a common model value within a small tolerance of the ideal model, thus exhibiting effective integration of local and global training models. Finally, we illustrate our theoretical claims through numerical simulations.", "AI": {"tldr": "\u63d0\u51fa\u5206\u5e03\u5f0f\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u670d\u52a1\u5668\u67b6\u6784\u89e3\u51b3\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u7684\u53ef\u6269\u5c55\u6027\u548c\u5bb9\u9519\u6027\u95ee\u9898", "motivation": "\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u4f9d\u8d56\u5355\u4e00\u4e2d\u592e\u670d\u52a1\u5668\uff0c\u5728\u5927\u89c4\u6a21\u5ba2\u6237\u7aef\u573a\u666f\u4e0b\u5b58\u5728\u53ef\u6269\u5c55\u6027\u9650\u5236\u548c\u5355\u70b9\u6545\u969c\u98ce\u9669\uff0c\u9700\u8981\u66f4\u5065\u58ee\u7684\u5206\u5e03\u5f0f\u89e3\u51b3\u65b9\u6848", "method": "\u8bbe\u8ba1\u5206\u5e03\u5f0f\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u542b\u591a\u4e2a\u5177\u6709\u670d\u52a1\u5668\u95f4\u901a\u4fe1\u80fd\u529b\u7684\u670d\u52a1\u5668\uff0c\u6bcf\u4e2a\u670d\u52a1\u5668\u7ba1\u7406\u4e00\u7ec4\u4e0d\u76f8\u4ea4\u7684\u5ba2\u6237\u7aef\uff1b\u63d0\u51faDFL\u7b97\u6cd5\uff0c\u4ea4\u66ff\u8fdb\u884c\u5ba2\u6237\u7aef\u672c\u5730\u8bad\u7ec3\u548c\u670d\u52a1\u5668\u95f4\u5168\u5c40\u8bad\u7ec3", "result": "\u5728\u9002\u5f53\u53c2\u6570\u9009\u62e9\u4e0b\uff0cDFL\u7b97\u6cd5\u786e\u4fdd\u6240\u6709\u670d\u52a1\u5668\u6536\u655b\u5230\u63a5\u8fd1\u7406\u60f3\u6a21\u578b\u7684\u5171\u540c\u6a21\u578b\u503c\uff0c\u6709\u6548\u6574\u5408\u672c\u5730\u548c\u5168\u5c40\u8bad\u7ec3\u6a21\u578b", "conclusion": "\u5206\u5e03\u5f0f\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u901a\u8fc7\u591a\u670d\u52a1\u5668\u67b6\u6784\u6210\u529f\u89e3\u51b3\u4e86\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u7684\u53ef\u6269\u5c55\u6027\u548c\u5bb9\u9519\u6027\u9650\u5236\uff0c\u7406\u8bba\u5206\u6790\u548c\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027"}}
{"id": "2601.01522", "categories": ["cs.AI", "cs.CL", "cs.ET"], "pdf": "https://arxiv.org/pdf/2601.01522", "abs": "https://arxiv.org/abs/2601.01522", "authors": ["Danial Amin"], "title": "Bayesian Orchestration of Multi-LLM Agents for Cost-Aware Sequential Decision-Making", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed as autonomous decision agents in settings with asymmetric error costs: hiring (missed talent vs wasted interviews), medical triage (missed emergencies vs unnecessary escalation), and fraud detection (approved fraud vs declined legitimate payments). The dominant design queries a single LLM for a posterior over states, thresholds \"confidence,\" and acts; we prove this is inadequate for sequential decisions with costs. We propose a Bayesian, cost-aware multi-LLM orchestration framework that treats LLMs as approximate likelihood models rather than classifiers. For each candidate state, we elicit likelihoods via contrastive prompting, aggregate across diverse models with robust statistics, and update beliefs with Bayes rule under explicit priors as new evidence arrives. This enables coherent belief updating, expected-cost action selection, principled information gathering via value of information, and fairness gains via ensemble bias mitigation. In resume screening with costs of 40000 USD per missed hire, 2500 USD per interview, and 150 USD per phone screen, experiments on 1000 resumes using five LLMs (GPT-4o, Claude 4.5 Sonnet, Gemini Pro, Grok, DeepSeek) reduce total cost by 294000 USD (34 percent) versus the best single-LLM baseline and improve demographic parity by 45 percent (max group gap 22 to 5 percentage points). Ablations attribute 51 percent of savings to multi-LLM aggregation, 43 percent to sequential updating, and 20 percent to disagreement-triggered information gathering, consistent with the theoretical benefits of correct probabilistic foundations.", "AI": {"tldr": "\u63d0\u51fa\u8d1d\u53f6\u65af\u6210\u672c\u611f\u77e5\u7684\u591aLLM\u534f\u540c\u6846\u67b6\uff0c\u5c06LLM\u89c6\u4e3a\u8fd1\u4f3c\u4f3c\u7136\u6a21\u578b\u800c\u975e\u5206\u7c7b\u5668\uff0c\u5728\u975e\u5bf9\u79f0\u9519\u8bef\u6210\u672c\u573a\u666f\u4e2d\u663e\u8457\u964d\u4f4e\u603b\u6210\u672c\u5e76\u63d0\u5347\u516c\u5e73\u6027", "motivation": "\u5f53\u524dLLM\u5728\u975e\u5bf9\u79f0\u9519\u8bef\u6210\u672c\u7684\u81ea\u4e3b\u51b3\u7b56\u573a\u666f\uff08\u5982\u62db\u8058\u3001\u533b\u7597\u5206\u8bca\u3001\u6b3a\u8bc8\u68c0\u6d4b\uff09\u4e2d\uff0c\u4e3b\u6d41\u65b9\u6cd5\u662f\u67e5\u8be2\u5355\u4e2aLLM\u83b7\u53d6\u540e\u9a8c\u6982\u7387\u5e76\u57fa\u4e8e\"\u7f6e\u4fe1\u5ea6\"\u9608\u503c\u884c\u52a8\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u5728\u5e8f\u5217\u51b3\u7b56\u4e2d\u6210\u672c\u5904\u7406\u4e0d\u8db3", "method": "\u63d0\u51fa\u8d1d\u53f6\u65af\u6210\u672c\u611f\u77e5\u7684\u591aLLM\u534f\u540c\u6846\u67b6\uff1a1) \u901a\u8fc7\u5bf9\u6bd4\u63d0\u793a\u4e3a\u6bcf\u4e2a\u5019\u9009\u72b6\u6001\u83b7\u53d6\u4f3c\u7136\uff1b2) \u4f7f\u7528\u7a33\u5065\u7edf\u8ba1\u65b9\u6cd5\u805a\u5408\u591a\u4e2a\u4e0d\u540c\u6a21\u578b\u7684\u8f93\u51fa\uff1b3) \u5728\u65b0\u8bc1\u636e\u5230\u8fbe\u65f6\u4f7f\u7528\u8d1d\u53f6\u65af\u89c4\u5219\u5728\u663e\u5f0f\u5148\u9a8c\u4e0b\u66f4\u65b0\u4fe1\u5ff5\uff1b4) \u652f\u6301\u8fde\u8d2f\u7684\u4fe1\u5ff5\u66f4\u65b0\u3001\u671f\u671b\u6210\u672c\u884c\u52a8\u9009\u62e9\u3001\u57fa\u4e8e\u4fe1\u606f\u4ef7\u503c\u7684\u539f\u5219\u6027\u4fe1\u606f\u6536\u96c6", "result": "\u5728\u7b80\u5386\u7b5b\u9009\u5b9e\u9a8c\u4e2d\uff08\u9519\u5931\u4eba\u624d\u6210\u672c40000\u7f8e\u5143\uff0c\u9762\u8bd5\u6210\u672c2500\u7f8e\u5143\uff0c\u7535\u8bdd\u7b5b\u9009\u6210\u672c150\u7f8e\u5143\uff09\uff0c\u4f7f\u75285\u4e2aLLM\uff08GPT-4o\u3001Claude 4.5 Sonnet\u3001Gemini Pro\u3001Grok\u3001DeepSeek\uff09\u5904\u74061000\u4efd\u7b80\u5386\uff0c\u76f8\u6bd4\u6700\u4f73\u5355LLM\u57fa\u7ebf\u964d\u4f4e\u603b\u6210\u672c294000\u7f8e\u5143\uff0834%\uff09\uff0c\u5e76\u5c06\u4eba\u53e3\u7edf\u8ba1\u516c\u5e73\u6027\u63d0\u534745%\uff08\u6700\u5927\u7fa4\u4f53\u5dee\u8ddd\u4ece22\u4e2a\u767e\u5206\u70b9\u964d\u81f35\u4e2a\u767e\u5206\u70b9\uff09", "conclusion": "\u6b63\u786e\u7684\u6982\u7387\u57fa\u7840\u7406\u8bba\u5728\u591aLLM\u51b3\u7b56\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff1a\u591aLLM\u805a\u5408\u8d21\u732e51%\u7684\u6210\u672c\u8282\u7701\uff0c\u5e8f\u5217\u66f4\u65b0\u8d21\u732e43%\uff0c\u5206\u6b67\u89e6\u53d1\u7684\u4fe1\u606f\u6536\u96c6\u8d21\u732e20%\uff0c\u8bc1\u660e\u4e86\u8d1d\u53f6\u65af\u6846\u67b6\u5728\u975e\u5bf9\u79f0\u6210\u672c\u5e8f\u5217\u51b3\u7b56\u4e2d\u7684\u6709\u6548\u6027"}}
{"id": "2601.01401", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01401", "abs": "https://arxiv.org/abs/2601.01401", "authors": ["Chenxu Wang", "Chaozhuo Li", "Pengbo Wang", "Litian Zhang", "Songyang Liu", "Ji Qi", "Jiahui Hu", "Yushan Cai", "Hao Zhao", "Rui Pu"], "title": "LANCET: Neural Intervention via Structural Entropy for Mitigating Faithfulness Hallucinations in LLMs", "comment": null, "summary": "Large Language Models have revolutionized information processing, yet their reliability is severely compromised by faithfulness hallucinations. While current approaches attempt to mitigate this issue through node-level adjustments or coarse suppression, they often overlook the distributed nature of neural information, leading to imprecise interventions. Recognizing that hallucinations propagate through specific forward transmission pathways like an infection, we aim to surgically block this flow using precise structural analysis. To leverage this, we propose Lancet, a novel framework that achieves precise neural intervention by leveraging structural entropy and hallucination difference ratios. Lancet first locates hallucination-prone neurons via gradient-driven contrastive analysis, then maps their propagation pathways by minimizing structural entropy, and finally implements a hierarchical intervention strategy that preserves general model capabilities. Comprehensive evaluations across hallucination benchmark datasets demonstrate that Lancet significantly outperforms state-of-the-art methods, validating the effectiveness of our surgical approach to neural intervention.", "AI": {"tldr": "Lancet\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u71b5\u548c\u5e7b\u89c9\u5dee\u5f02\u6bd4\u5b9e\u73b0\u7cbe\u786e\u795e\u7ecf\u5e72\u9884\uff0c\u5b9a\u4f4d\u5e7b\u89c9\u6613\u53d1\u795e\u7ecf\u5143\u5e76\u963b\u65ad\u5176\u4f20\u64ad\u8def\u5f84\uff0c\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5fe0\u5b9e\u5ea6", "motivation": "\u5f53\u524d\u65b9\u6cd5\u901a\u8fc7\u8282\u70b9\u7ea7\u8c03\u6574\u6216\u7c97\u7c92\u5ea6\u6291\u5236\u6765\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5fe0\u5b9e\u5ea6\u5e7b\u89c9\u95ee\u9898\uff0c\u4f46\u5ffd\u89c6\u4e86\u795e\u7ecf\u4fe1\u606f\u7684\u5206\u5e03\u5f0f\u7279\u6027\uff0c\u5bfc\u81f4\u5e72\u9884\u4e0d\u7cbe\u786e\u3002\u4f5c\u8005\u8ba4\u8bc6\u5230\u5e7b\u89c9\u50cf\u611f\u67d3\u4e00\u6837\u901a\u8fc7\u7279\u5b9a\u7684\u524d\u5411\u4f20\u64ad\u8def\u5f84\u4f20\u64ad\uff0c\u56e0\u6b64\u5e0c\u671b\u901a\u8fc7\u7cbe\u786e\u7684\u7ed3\u6784\u5206\u6790\u6765\u624b\u672f\u5f0f\u963b\u65ad\u8fd9\u79cd\u4f20\u64ad\u6d41", "method": "\u63d0\u51faLancet\u6846\u67b6\uff1a1) \u901a\u8fc7\u68af\u5ea6\u9a71\u52a8\u7684\u5bf9\u6bd4\u5206\u6790\u5b9a\u4f4d\u5e7b\u89c9\u6613\u53d1\u795e\u7ecf\u5143\uff1b2) \u901a\u8fc7\u6700\u5c0f\u5316\u7ed3\u6784\u71b5\u6620\u5c04\u5176\u4f20\u64ad\u8def\u5f84\uff1b3) \u5b9e\u65bd\u5206\u5c42\u5e72\u9884\u7b56\u7565\u4ee5\u4fdd\u7559\u6a21\u578b\u7684\u4e00\u822c\u80fd\u529b", "result": "\u5728\u591a\u4e2a\u5e7b\u89c9\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u8bc4\u4f30\u8868\u660e\uff0cLancet\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u795e\u7ecf\u5e72\u9884\u624b\u672f\u5f0f\u65b9\u6cd5\u7684\u6709\u6548\u6027", "conclusion": "\u901a\u8fc7\u7ed3\u6784\u71b5\u548c\u5e7b\u89c9\u5dee\u5f02\u6bd4\u5b9e\u73b0\u7684\u7cbe\u786e\u795e\u7ecf\u5e72\u9884\u80fd\u591f\u6709\u6548\u963b\u65ad\u5e7b\u89c9\u4f20\u64ad\u8def\u5f84\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u4e00\u822c\u80fd\u529b\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5fe0\u5b9e\u5ea6\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.00919", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.00919", "abs": "https://arxiv.org/abs/2601.00919", "authors": ["Zichuan Fu", "Wentao Song", "Guojing Li", "Yejing Wang", "Xian Wu", "Yimin Deng", "Hanyu Yan", "Yefeng Zheng", "Xiangyu Zhao"], "title": "Attention Needs to Focus: A Unified Perspective on Attention Allocation", "comment": "ICLR 2026 conference", "summary": "The Transformer architecture, a cornerstone of modern Large Language Models (LLMs), has achieved extraordinary success in sequence modeling, primarily due to its attention mechanism. However, despite its power, the standard attention mechanism is plagued by well-documented issues: representational collapse and attention sink. Although prior work has proposed approaches for these issues, they are often studied in isolation, obscuring their deeper connection. In this paper, we present a unified perspective, arguing that both can be traced to a common root -- improper attention allocation. We identify two failure modes: 1) Attention Overload, where tokens receive comparable high weights, blurring semantic features that lead to representational collapse; 2) Attention Underload, where no token is semantically relevant, yet attention is still forced to distribute, resulting in spurious focus such as attention sink. Building on this insight, we introduce Lazy Attention, a novel mechanism designed for a more focused attention distribution. To mitigate overload, it employs positional discrimination across both heads and dimensions to sharpen token distinctions. To counteract underload, it incorporates Elastic-Softmax, a modified normalization function that relaxes the standard softmax constraint to suppress attention on irrelevant tokens. Experiments on the FineWeb-Edu corpus, evaluated across nine diverse benchmarks, demonstrate that Lazy Attention successfully mitigates attention sink and achieves competitive performance compared to both standard attention and modern architectures, while reaching up to 59.58% attention sparsity.", "AI": {"tldr": "Lazy Attention\u673a\u5236\u901a\u8fc7\u4f4d\u7f6e\u533a\u5206\u548c\u5f39\u6027Softmax\u89e3\u51b3\u6ce8\u610f\u529b\u8fc7\u8f7d\u548c\u6b20\u8f7d\u95ee\u9898\uff0c\u7f13\u89e3\u8868\u793a\u574d\u7f29\u548c\u6ce8\u610f\u529b\u6c89\u6ca1\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u8fbe59.58%\u7684\u6ce8\u610f\u529b\u7a00\u758f\u6027\u3002", "motivation": "Transformer\u6ce8\u610f\u529b\u673a\u5236\u5b58\u5728\u8868\u793a\u574d\u7f29\u548c\u6ce8\u610f\u529b\u6c89\u6ca1\u4e24\u4e2a\u95ee\u9898\uff0c\u5148\u524d\u7814\u7a76\u5b64\u7acb\u5904\u7406\u8fd9\u4e9b\u95ee\u9898\uff0c\u672c\u6587\u8ba4\u4e3a\u4e24\u8005\u6839\u6e90\u76f8\u540c\u2014\u2014\u6ce8\u610f\u529b\u5206\u914d\u4e0d\u5f53\uff0c\u9700\u8981\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faLazy Attention\u673a\u5236\uff1a1) \u901a\u8fc7\u4f4d\u7f6e\u533a\u5206\uff08\u8de8\u5934\u548c\u7ef4\u5ea6\uff09\u7f13\u89e3\u6ce8\u610f\u529b\u8fc7\u8f7d\uff0c\u589e\u5f3atoken\u533a\u5206\u5ea6\uff1b2) \u4f7f\u7528\u5f39\u6027Softmax\u7f13\u89e3\u6ce8\u610f\u529b\u6b20\u8f7d\uff0c\u6291\u5236\u65e0\u5173token\u7684\u6ce8\u610f\u529b\u5206\u914d\u3002", "result": "\u5728FineWeb-Edu\u8bed\u6599\u5e93\u548c\u4e5d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLazy Attention\u6210\u529f\u7f13\u89e3\u6ce8\u610f\u529b\u6c89\u6ca1\uff0c\u6027\u80fd\u4e0e\u6807\u51c6\u6ce8\u610f\u529b\u53ca\u73b0\u4ee3\u67b6\u6784\u76f8\u5f53\uff0c\u540c\u65f6\u5b9e\u73b0\u9ad8\u8fbe59.58%\u7684\u6ce8\u610f\u529b\u7a00\u758f\u6027\u3002", "conclusion": "\u6ce8\u610f\u529b\u8fc7\u8f7d\u548c\u6b20\u8f7d\u662fTransformer\u6ce8\u610f\u529b\u673a\u5236\u7684\u6839\u672c\u95ee\u9898\uff0cLazy Attention\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u6709\u6548\u89e3\u51b3\u8fd9\u4e24\u4e2a\u95ee\u9898\uff0c\u4e3a\u6ce8\u610f\u529b\u673a\u5236\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2601.01532", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01532", "abs": "https://arxiv.org/abs/2601.01532", "authors": ["Fanzhe Fu"], "title": "Aletheia: Quantifying Cognitive Conviction in Reasoning Models via Regularized Inverse Confusion Matrix", "comment": "6 pages, 2 figures", "summary": "In the progressive journey toward Artificial General Intelligence (AGI), current evaluation paradigms face an epistemological crisis. Static benchmarks measure knowledge breadth but fail to quantify the depth of belief. While Simhi et al. (2025) defined the CHOKE phenomenon in standard QA, we extend this framework to quantify \"Cognitive Conviction\" in System 2 reasoning models. We propose Project Aletheia, a cognitive physics framework that employs Tikhonov Regularization to invert the judge's confusion matrix. To validate this methodology without relying on opaque private data, we implement a Synthetic Proxy Protocol. Our preliminary pilot study on 2025 baselines (e.g., DeepSeek-R1, OpenAI o1) suggests that while reasoning models act as a \"cognitive buffer,\" they may exhibit \"Defensive OverThinking\" under adversarial pressure. Furthermore, we introduce the Aligned Conviction Score (S_aligned) to verify that conviction does not compromise safety. This work serves as a blueprint for measuring AI scientific integrity.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Project Aletheia\u6846\u67b6\uff0c\u7528\u4e8e\u91cf\u5316System 2\u63a8\u7406\u6a21\u578b\u7684\"\u8ba4\u77e5\u4fe1\u5ff5\u5f3a\u5ea6\"\uff0c\u901a\u8fc7Tikhonov\u6b63\u5219\u5316\u53cd\u6f14\u8bc4\u4f30\u8005\u7684\u6df7\u6dc6\u77e9\u9635\uff0c\u5e76\u5f15\u5165\u5bf9\u9f50\u4fe1\u5ff5\u5206\u6570\u6765\u786e\u4fdd\u5b89\u5168\u6027\u3002", "motivation": "\u5f53\u524dAGI\u8bc4\u4f30\u8303\u5f0f\u9762\u4e34\u8ba4\u8bc6\u8bba\u5371\u673a\uff1a\u9759\u6001\u57fa\u51c6\u6d4b\u8bd5\u53ea\u80fd\u8861\u91cf\u77e5\u8bc6\u5e7f\u5ea6\uff0c\u65e0\u6cd5\u91cf\u5316\u4fe1\u5ff5\u6df1\u5ea6\u3002\u867d\u7136Simhi\u7b49\u4eba(2025)\u5728\u6807\u51c6QA\u4e2d\u5b9a\u4e49\u4e86CHOKE\u73b0\u8c61\uff0c\u4f46\u9700\u8981\u6269\u5c55\u8be5\u6846\u67b6\u6765\u91cf\u5316System 2\u63a8\u7406\u6a21\u578b\u7684\u8ba4\u77e5\u4fe1\u5ff5\u5f3a\u5ea6\u3002", "method": "\u63d0\u51faProject Aletheia\u8ba4\u77e5\u7269\u7406\u5b66\u6846\u67b6\uff0c\u91c7\u7528Tikhonov\u6b63\u5219\u5316\u53cd\u6f14\u8bc4\u4f30\u8005\u7684\u6df7\u6dc6\u77e9\u9635\u3002\u4e3a\u907f\u514d\u4f9d\u8d56\u4e0d\u900f\u660e\u7684\u79c1\u6709\u6570\u636e\uff0c\u5b9e\u65bd\u4e86\u5408\u6210\u4ee3\u7406\u534f\u8bae\u3002\u5f15\u5165\u5bf9\u9f50\u4fe1\u5ff5\u5206\u6570(S_aligned)\u6765\u9a8c\u8bc1\u4fe1\u5ff5\u5f3a\u5ea6\u4e0d\u4f1a\u635f\u5bb3\u5b89\u5168\u6027\u3002", "result": "\u5bf92025\u5e74\u57fa\u7ebf\u6a21\u578b(\u5982DeepSeek-R1\u3001OpenAI o1)\u7684\u521d\u6b65\u8bd5\u70b9\u7814\u7a76\u8868\u660e\uff1a\u63a8\u7406\u6a21\u578b\u867d\u7136\u8d77\u5230\"\u8ba4\u77e5\u7f13\u51b2\"\u4f5c\u7528\uff0c\u4f46\u5728\u5bf9\u6297\u538b\u529b\u4e0b\u53ef\u80fd\u8868\u73b0\u51fa\"\u9632\u5fa1\u6027\u8fc7\u5ea6\u601d\u8003\"\u73b0\u8c61\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u8861\u91cfAI\u79d1\u5b66\u5b8c\u6574\u6027\u63d0\u4f9b\u4e86\u84dd\u56fe\uff0c\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u91cf\u5316\u8ba4\u77e5\u4fe1\u5ff5\u5f3a\u5ea6\uff0c\u540c\u65f6\u901a\u8fc7\u5bf9\u9f50\u4fe1\u5ff5\u5206\u6570\u786e\u4fdd\u5b89\u5168\u6027\uff0c\u4e3aAGI\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u6cd5\u8bba\u57fa\u7840\u3002"}}
{"id": "2601.01407", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01407", "abs": "https://arxiv.org/abs/2601.01407", "authors": ["Arjhun Sreedar", "Rohan Pillay", "Laukik Patade"], "title": "From Emotion Classification to Emotional Reasoning: Enhancing Emotional Intelligence in Large Language Models", "comment": "10 pages, 1 figure", "summary": "This work investigates whether synthetic emotional chain-of-thought data can improve the emotional reasoning abilities of smaller open large language models (LLMs). We design a multi-agent generation pipeline that produces therapy-style conversations and converts them into structured emotion multiple-choice questions (MCQs) with explanations. We propose that fine-tuning a variety of 7B models on this dataset should yield substantial gains in emotional understanding and emotional awareness on EmoBench-style evaluations, suggesting that emotional reasoning can be induced without architectural changes. Our results demonstrate that fine-tuned Mistral 7B achieves EU improvements from 10.5 to 20.5 and EA improvements from 40.5 to 60.0, validating the effectiveness of synthetic emotional reasoning data for enhancing model capabilities in nuanced emotional tasks.", "AI": {"tldr": "\u4f7f\u7528\u5408\u6210\u60c5\u611f\u94fe\u5f0f\u601d\u7ef4\u6570\u636e\u5fae\u8c037B\u6a21\u578b\uff0c\u53ef\u663e\u8457\u63d0\u5347\u60c5\u611f\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u9700\u67b6\u6784\u4fee\u6539", "motivation": "\u7814\u7a76\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u5408\u6210\u7684\u60c5\u611f\u94fe\u5f0f\u601d\u7ef4\u6570\u636e\u6765\u63d0\u5347\u5c0f\u578b\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u7684\u60c5\u611f\u63a8\u7406\u80fd\u529b\uff0c\u63a2\u7d22\u65e0\u9700\u67b6\u6784\u4fee\u6539\u7684\u60c5\u611f\u80fd\u529b\u589e\u5f3a\u65b9\u6cd5", "method": "\u8bbe\u8ba1\u591a\u667a\u80fd\u4f53\u751f\u6210\u7ba1\u9053\uff0c\u521b\u5efa\u6cbb\u7597\u5f0f\u5bf9\u8bdd\u5e76\u5c06\u5176\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u60c5\u611f\u591a\u9009\u9898\u53ca\u89e3\u91ca\uff0c\u4f7f\u7528\u8be5\u6570\u636e\u96c6\u5fae\u8c03\u591a\u79cd7B\u6a21\u578b", "result": "\u5fae\u8c03\u540e\u7684Mistral 7B\u5728\u60c5\u611f\u7406\u89e3(EU)\u4e0a\u4ece10.5\u63d0\u5347\u523020.5\uff0c\u60c5\u611f\u610f\u8bc6(EA)\u4ece40.5\u63d0\u5347\u523060.0\uff0c\u9a8c\u8bc1\u4e86\u5408\u6210\u60c5\u611f\u63a8\u7406\u6570\u636e\u7684\u6709\u6548\u6027", "conclusion": "\u5408\u6210\u60c5\u611f\u63a8\u7406\u6570\u636e\u80fd\u591f\u6709\u6548\u589e\u5f3a\u6a21\u578b\u5728\u7ec6\u5fae\u60c5\u611f\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u60c5\u611f\u63a8\u7406\u53ef\u4ee5\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u65b9\u5f0f\u8bf1\u5bfc\u800c\u4e0d\u9700\u8981\u67b6\u6784\u4fee\u6539"}}
{"id": "2601.00920", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00920", "abs": "https://arxiv.org/abs/2601.00920", "authors": ["Xingsheng Chen", "Regina Zhang", "Bo Gao", "Xingwei He", "Xiaofeng Liu", "Pietro Lio", "Kwok-Yan Lam", "Siu-Ming Yiu"], "title": "MODE: Efficient Time Series Prediction with Mamba Enhanced by Low-Rank Neural ODEs", "comment": "12 pages, 6 tables", "summary": "Time series prediction plays a pivotal role across diverse domains such as finance, healthcare, energy systems, and environmental modeling. However, existing approaches often struggle to balance efficiency, scalability, and accuracy, particularly when handling long-range dependencies and irregularly sampled data. To address these challenges, we propose MODE, a unified framework that integrates Low-Rank Neural Ordinary Differential Equations (Neural ODEs) with an Enhanced Mamba architecture. As illustrated in our framework, the input sequence is first transformed by a Linear Tokenization Layer and then processed through multiple Mamba Encoder blocks, each equipped with an Enhanced Mamba Layer that employs Causal Convolution, SiLU activation, and a Low-Rank Neural ODE enhancement to efficiently capture temporal dynamics. This low-rank formulation reduces computational overhead while maintaining expressive power. Furthermore, a segmented selective scanning mechanism, inspired by pseudo-ODE dynamics, adaptively focuses on salient subsequences to improve scalability and long-range sequence modeling. Extensive experiments on benchmark datasets demonstrate that MODE surpasses existing baselines in both predictive accuracy and computational efficiency. Overall, our contributions include: (1) a unified and efficient architecture for long-term time series modeling, (2) integration of Mamba's selective scanning with low-rank Neural ODEs for enhanced temporal representation, and (3) substantial improvements in efficiency and scalability enabled by low-rank approximation and dynamic selective scanning.", "AI": {"tldr": "MODE\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u4f4e\u79e9\u795e\u7ecfODE\u548c\u589e\u5f3aMamba\u67b6\u6784\uff0c\u901a\u8fc7\u4f4e\u79e9\u8fd1\u4f3c\u548c\u52a8\u6001\u9009\u62e9\u6027\u626b\u63cf\u673a\u5236\uff0c\u5728\u4fdd\u6301\u8868\u8fbe\u529b\u7684\u540c\u65f6\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\u5728\u5904\u7406\u957f\u7a0b\u4f9d\u8d56\u548c\u4e0d\u89c4\u5219\u91c7\u6837\u6570\u636e\u65f6\uff0c\u96be\u4ee5\u5e73\u8861\u6548\u7387\u3001\u53ef\u6269\u5c55\u6027\u548c\u51c6\u786e\u6027\u3002\u7279\u522b\u662f\u5728\u91d1\u878d\u3001\u533b\u7597\u3001\u80fd\u6e90\u7cfb\u7edf\u548c\u73af\u5883\u5efa\u6a21\u7b49\u9886\u57df\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u9884\u6d4b\u6a21\u578b\u3002", "method": "\u63d0\u51faMODE\u6846\u67b6\uff1a1\uff09\u7ebf\u6027\u6807\u8bb0\u5316\u5c42\u5904\u7406\u8f93\u5165\u5e8f\u5217\uff1b2\uff09\u591a\u4e2aMamba\u7f16\u7801\u5668\u5757\uff0c\u6bcf\u4e2a\u5305\u542b\u589e\u5f3aMamba\u5c42\uff08\u56e0\u679c\u5377\u79ef\u3001SiLU\u6fc0\u6d3b\u3001\u4f4e\u79e9\u795e\u7ecfODE\u589e\u5f3a\uff09\uff1b3\uff09\u4f4e\u79e9\u795e\u7ecfODE\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\uff1b4\uff09\u5206\u6bb5\u9009\u62e9\u6027\u626b\u63cf\u673a\u5236\uff08\u53d7\u4f2aODE\u52a8\u6001\u542f\u53d1\uff09\u81ea\u9002\u5e94\u805a\u7126\u91cd\u8981\u5b50\u5e8f\u5217\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cMODE\u5728\u9884\u6d4b\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "MODE\u4e3a\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u9ad8\u6548\u67b6\u6784\uff0c\u901a\u8fc7Mamba\u9009\u62e9\u6027\u626b\u63cf\u4e0e\u4f4e\u79e9\u795e\u7ecfODE\u7684\u96c6\u6210\u589e\u5f3a\u4e86\u65f6\u95f4\u8868\u793a\u80fd\u529b\uff0c\u4f4e\u79e9\u8fd1\u4f3c\u548c\u52a8\u6001\u9009\u62e9\u6027\u626b\u63cf\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2601.01546", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01546", "abs": "https://arxiv.org/abs/2601.01546", "authors": ["Letian Kong", "Qianran", "Jin", "Renyu Zhang"], "title": "Improving Behavioral Alignment in LLM Social Simulations via Context Formation and Navigation", "comment": "39 pages, 2 figures, 3 tables", "summary": "Large language models (LLMs) are increasingly used to simulate human behavior in experimental settings, but they systematically diverge from human decisions in complex decision-making environments, where participants must anticipate others' actions and form beliefs based on observed behavior. We propose a two-stage framework for improving behavioral alignment. The first stage, context formation, explicitly specifies the experimental design to establish an accurate representation of the decision task and its context. The second stage, context navigation, guides the reasoning process within that representation to make decisions. We validate this framework through a focal replication of a sequential purchasing game with quality signaling (Kremer and Debo, 2016), extending to a crowdfunding game with costly signaling (Cason et al., 2025) and a demand-estimation task (Gui and Toubia, 2025) to test generalizability across decision environments. Across four state-of-the-art (SOTA) models (GPT-4o, GPT-5, Claude-4.0-Sonnet-Thinking, DeepSeek-R1), we find that complex decision-making environments require both stages to achieve behavioral alignment with human benchmarks, whereas the simpler demand-estimation task requires only context formation. Our findings clarify when each stage is necessary and provide a systematic approach for designing and diagnosing LLM social simulations as complements to human subjects in behavioral research.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6\u6539\u5584LLM\u5728\u590d\u6742\u51b3\u7b56\u73af\u5883\u4e2d\u7684\u884c\u4e3a\u5bf9\u9f50\uff1a\u4e0a\u4e0b\u6587\u5f62\u6210\uff08\u660e\u786e\u5b9e\u9a8c\u8bbe\u8ba1\uff09\u548c\u4e0a\u4e0b\u6587\u5bfc\u822a\uff08\u6307\u5bfc\u63a8\u7406\u8fc7\u7a0b\uff09\uff0c\u9a8c\u8bc1\u8868\u660e\u590d\u6742\u4efb\u52a1\u9700\u8981\u4e24\u9636\u6bb5\uff0c\u7b80\u5355\u4efb\u52a1\u4ec5\u9700\u7b2c\u4e00\u9636\u6bb5\u3002", "motivation": "LLM\u5728\u6a21\u62df\u4eba\u7c7b\u884c\u4e3a\u65f6\uff0c\u5728\u9700\u8981\u9884\u6d4b\u4ed6\u4eba\u884c\u52a8\u548c\u57fa\u4e8e\u89c2\u5bdf\u5f62\u6210\u4fe1\u5ff5\u7684\u590d\u6742\u51b3\u7b56\u73af\u5883\u4e2d\uff0c\u7cfb\u7edf\u6027\u5730\u504f\u79bb\u4eba\u7c7b\u51b3\u7b56\uff0c\u9700\u8981\u6539\u8fdb\u884c\u4e3a\u5bf9\u9f50\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1) \u4e0a\u4e0b\u6587\u5f62\u6210\u9636\u6bb5\u660e\u786e\u6307\u5b9a\u5b9e\u9a8c\u8bbe\u8ba1\uff0c\u5efa\u7acb\u51b3\u7b56\u4efb\u52a1\u548c\u4e0a\u4e0b\u6587\u7684\u51c6\u786e\u8868\u793a\uff1b2) \u4e0a\u4e0b\u6587\u5bfc\u822a\u9636\u6bb5\u5728\u8be5\u8868\u793a\u5185\u6307\u5bfc\u63a8\u7406\u8fc7\u7a0b\u505a\u51fa\u51b3\u7b56\u3002\u901a\u8fc7\u4e09\u4e2a\u5b9e\u9a8c\u9a8c\u8bc1\uff1a\u987a\u5e8f\u8d2d\u4e70\u6e38\u620f\u3001\u4f17\u7b79\u6e38\u620f\u548c\u9700\u6c42\u4f30\u8ba1\u4efb\u52a1\u3002", "result": "\u5728\u56db\u4e2aSOTA\u6a21\u578b\uff08GPT-4o\u3001GPT-5\u3001Claude-4.0-Sonnet-Thinking\u3001DeepSeek-R1\uff09\u4e0a\u9a8c\u8bc1\u53d1\u73b0\uff1a\u590d\u6742\u51b3\u7b56\u73af\u5883\u9700\u8981\u4e24\u9636\u6bb5\u624d\u80fd\u5b9e\u73b0\u4e0e\u4eba\u7c7b\u57fa\u51c6\u7684\u884c\u4e3a\u5bf9\u9f50\uff0c\u800c\u7b80\u5355\u7684\u9700\u6c42\u4f30\u8ba1\u4efb\u52a1\u4ec5\u9700\u4e0a\u4e0b\u6587\u5f62\u6210\u9636\u6bb5\u3002", "conclusion": "\u7814\u7a76\u9610\u660e\u4e86\u6bcf\u4e2a\u9636\u6bb5\u5728\u4f55\u65f6\u5fc5\u8981\uff0c\u4e3a\u8bbe\u8ba1\u548c\u8bca\u65adLLM\u793e\u4f1a\u6a21\u62df\u4f5c\u4e3a\u884c\u4e3a\u7814\u7a76\u4e2d\u4eba\u7c7b\u53d7\u8bd5\u8005\u7684\u8865\u5145\u63d0\u4f9b\u4e86\u7cfb\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2601.01446", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01446", "abs": "https://arxiv.org/abs/2601.01446", "authors": ["Yilong Wang", "Qianli Wang", "Nils Feldhus"], "title": "iFlip: Iterative Feedback-driven Counterfactual Example Refinement", "comment": "In submission", "summary": "Counterfactual examples are minimal edits to an input that alter a model's prediction. They are widely employed in explainable AI to probe model behavior and in natural language processing (NLP) to augment training data. However, generating valid counterfactuals with large language models (LLMs) remains challenging, as existing single-pass methods often fail to induce reliable label changes, neglecting LLMs' self-correction capabilities. To explore this untapped potential, we propose iFlip, an iterative refinement approach that leverages three types of feedback, including model confidence, feature attribution, and natural language. Our results show that iFlip achieves an average 57.8% higher validity than the five state-of-the-art baselines, as measured by the label flipping rate. The user study further corroborates that iFlip outperforms baselines in completeness, overall satisfaction, and feasibility. In addition, ablation studies demonstrate that three components are paramount for iFlip to generate valid counterfactuals: leveraging an appropriate number of iterations, pointing to highly attributed words, and early stopping. Finally, counterfactuals generated by iFlip enable effective counterfactual data augmentation, substantially improving model performance and robustness.", "AI": {"tldr": "iFlip\uff1a\u5229\u7528LLM\u81ea\u6211\u4fee\u6b63\u80fd\u529b\u7684\u8fed\u4ee3\u5f0f\u53cd\u4e8b\u5b9e\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u578b\u7f6e\u4fe1\u5ea6\u3001\u7279\u5f81\u5f52\u56e0\u548c\u81ea\u7136\u8bed\u8a00\u53cd\u9988\u63d0\u5347\u53cd\u4e8b\u5b9e\u6709\u6548\u6027", "motivation": "\u73b0\u6709\u5355\u6b21\u751f\u6210\u65b9\u6cd5\u5728\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u53cd\u4e8b\u5b9e\u793a\u4f8b\u65f6\u6548\u679c\u4e0d\u4f73\uff0c\u5e38\u5e38\u65e0\u6cd5\u53ef\u9760\u5730\u6539\u53d8\u6a21\u578b\u9884\u6d4b\u6807\u7b7e\uff0c\u5ffd\u89c6\u4e86LLM\u7684\u81ea\u6211\u4fee\u6b63\u80fd\u529b", "method": "\u63d0\u51faiFlip\u8fed\u4ee3\u4f18\u5316\u65b9\u6cd5\uff0c\u5229\u7528\u4e09\u79cd\u53cd\u9988\u7c7b\u578b\uff08\u6a21\u578b\u7f6e\u4fe1\u5ea6\u3001\u7279\u5f81\u5f52\u56e0\u548c\u81ea\u7136\u8bed\u8a00\uff09\u8fdb\u884c\u591a\u8f6e\u8fed\u4ee3\u4fee\u6b63\uff0c\u901a\u8fc7\u9002\u5f53\u7684\u8fed\u4ee3\u6b21\u6570\u3001\u9ad8\u5f52\u56e0\u8bcd\u6307\u5411\u548c\u65e9\u505c\u7b56\u7565\u751f\u6210\u6709\u6548\u53cd\u4e8b\u5b9e", "result": "iFlip\u5728\u6807\u7b7e\u7ffb\u8f6c\u7387\u4e0a\u6bd4\u4e94\u79cdSOTA\u57fa\u7ebf\u5e73\u5747\u63d0\u9ad857.8%\u7684\u6709\u6548\u6027\uff1b\u7528\u6237\u7814\u7a76\u663e\u793a\u5728\u5b8c\u6574\u6027\u3001\u603b\u4f53\u6ee1\u610f\u5ea6\u548c\u53ef\u884c\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u57fa\u7ebf\uff1b\u53cd\u4e8b\u5b9e\u6570\u636e\u589e\u5f3a\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u9c81\u68d2\u6027", "conclusion": "iFlip\u901a\u8fc7\u5229\u7528LLM\u7684\u81ea\u6211\u4fee\u6b63\u80fd\u529b\u548c\u8fed\u4ee3\u53cd\u9988\u673a\u5236\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u53cd\u4e8b\u5b9e\u751f\u6210\u7684\u6709\u6548\u6027\uff0c\u4e3a\u6a21\u578b\u89e3\u91ca\u6027\u548c\u6570\u636e\u589e\u5f3a\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177"}}
{"id": "2601.00921", "categories": ["cs.LG", "cs.AI", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.00921", "abs": "https://arxiv.org/abs/2601.00921", "authors": ["Azadeh Alavi", "Hamidreza Khalili", "Stanley H. Chan", "Fatemeh Kouchmeshki", "Ross Vlahos"], "title": "Practical Geometric and Quantum Kernel Methods for Predicting Skeletal Muscle Outcomes in chronic obstructive pulmonary disease", "comment": "24 pages, 4 figures", "summary": "Skeletal muscle dysfunction is a clinically relevant extra-pulmonary manifestation of chronic obstructive pulmonary disease (COPD) and is closely linked to systemic and airway inflammation. This motivates predictive modelling of muscle outcomes from minimally invasive biomarkers that can be acquired longitudinally. We study a small-sample preclinical dataset comprising 213 animals across two conditions (Sham versus cigarette-smoke exposure), with blood and bronchoalveolar lavage fluid measurements and three continuous targets: tibialis anterior muscle weight (milligram: mg), specific force (millinewton: mN), and a derived muscle quality index (mN per mg). We benchmark tuned classical baselines, geometry-aware symmetric positive definite (SPD) descriptors with Stein divergence, and quantum kernel models designed for low-dimensional tabular data. In the muscle-weight setting, quantum kernel ridge regression using four interpretable inputs (blood C-reactive protein, neutrophil count, bronchoalveolar lavage cellularity, and condition) attains a test root mean squared error of 4.41 mg and coefficient of determination of 0.605, improving over a matched ridge baseline on the same feature set (4.70 mg and 0.553). Geometry-informed Stein-divergence prototype distances yield a smaller but consistent gain in the biomarker-only setting (4.55 mg versus 4.79 mg). Screening-style evaluation, obtained by thresholding the continuous outcome at 0.8 times the training Sham mean, achieves an area under the receiver operating characteristic curve (ROC-AUC) of up to 0.90 for detecting low muscle weight. These results indicate that geometric and quantum kernel lifts can provide measurable benefits in low-data, low-feature biomedical prediction problems, while preserving interpretability and transparent model selection.", "AI": {"tldr": "\u7814\u7a76\u4f7f\u7528\u51e0\u4f55\u611f\u77e5\u548c\u91cf\u5b50\u6838\u65b9\u6cd5\u9884\u6d4bCOPD\u76f8\u5173\u9aa8\u9abc\u808c\u529f\u80fd\u969c\u788d\uff0c\u5728\u4f4e\u6570\u636e\u3001\u4f4e\u7279\u5f81\u751f\u7269\u533b\u5b66\u9884\u6d4b\u4e2d\u663e\u793a\u51fa\u53ef\u6d4b\u91cf\u7684\u4f18\u52bf\u3002", "motivation": "\u6162\u6027\u963b\u585e\u6027\u80ba\u75be\u75c5\uff08COPD\uff09\u7684\u9aa8\u9abc\u808c\u529f\u80fd\u969c\u788d\u662f\u91cd\u8981\u7684\u80ba\u5916\u8868\u73b0\uff0c\u4e0e\u5168\u8eab\u548c\u6c14\u9053\u708e\u75c7\u5bc6\u5207\u76f8\u5173\u3002\u9700\u8981\u4ece\u53ef\u7eb5\u5411\u83b7\u53d6\u7684\u5fae\u521b\u751f\u7269\u6807\u5fd7\u7269\u9884\u6d4b\u808c\u8089\u7ed3\u679c\u3002", "method": "\u4f7f\u7528213\u53ea\u52a8\u7269\u7684\u4e34\u5e8a\u524d\u6570\u636e\u96c6\uff0c\u6bd4\u8f83\u8c03\u4f18\u7684\u7ecf\u5178\u57fa\u7ebf\u3001\u51e0\u4f55\u611f\u77e5\u5bf9\u79f0\u6b63\u5b9a\u63cf\u8ff0\u7b26\u4e0eStein\u6563\u5ea6\u3001\u4ee5\u53ca\u4e3a\u4f4e\u7ef4\u8868\u683c\u6570\u636e\u8bbe\u8ba1\u7684\u91cf\u5b50\u6838\u6a21\u578b\u3002\u4f7f\u7528\u8840\u6db2C\u53cd\u5e94\u86cb\u767d\u3001\u4e2d\u6027\u7c92\u7ec6\u80de\u8ba1\u6570\u3001\u652f\u6c14\u7ba1\u80ba\u6ce1\u704c\u6d17\u7ec6\u80de\u6027\u548c\u6761\u4ef6\u4f5c\u4e3a\u8f93\u5165\u7279\u5f81\u3002", "result": "\u5728\u808c\u8089\u91cd\u91cf\u9884\u6d4b\u4e2d\uff0c\u91cf\u5b50\u6838\u5cad\u56de\u5f52\u83b7\u5f97\u6d4b\u8bd5RMSE 4.41 mg\u548cR\u00b2 0.605\uff0c\u4f18\u4e8e\u76f8\u540c\u7279\u5f81\u96c6\u7684\u5cad\u56de\u5f52\u57fa\u7ebf\uff084.70 mg\u548c0.553\uff09\u3002\u51e0\u4f55\u611f\u77e5Stein\u6563\u5ea6\u539f\u578b\u8ddd\u79bb\u5728\u4ec5\u751f\u7269\u6807\u5fd7\u7269\u8bbe\u7f6e\u4e2d\u4e5f\u83b7\u5f97\u4e00\u81f4\u6539\u8fdb\uff084.55 mg vs 4.79 mg\uff09\u3002\u7b5b\u67e5\u5f0f\u8bc4\u4f30\u7684ROC-AUC\u9ad8\u8fbe0.90\u3002", "conclusion": "\u51e0\u4f55\u548c\u91cf\u5b50\u6838\u63d0\u5347\u5728\u4f4e\u6570\u636e\u3001\u4f4e\u7279\u5f81\u7684\u751f\u7269\u533b\u5b66\u9884\u6d4b\u95ee\u9898\u4e2d\u80fd\u63d0\u4f9b\u53ef\u6d4b\u91cf\u7684\u4f18\u52bf\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u548c\u900f\u660e\u7684\u6a21\u578b\u9009\u62e9\u3002"}}
{"id": "2601.01562", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01562", "abs": "https://arxiv.org/abs/2601.01562", "authors": ["Mingyu Xu", "Cheng Fang", "Keyue Jiang", "Yuqian Zheng", "Yanghua Xiao", "Baojian Zhou", "Qifang Zhao", "Suhang Zheng", "Xiuwen Zhu", "Jiyang Tang", "Yongchi Zhao", "Yijia Luo", "Zhiqi Bai", "Yuchi Xu", "Wenbo Su", "Wei Wang", "Bing Zhao", "Lin Qu", "Xiaoxiao Xu"], "title": "Logics-STEM: Empowering LLM Reasoning via Failure-Driven Post-Training and Document Knowledge Enhancement", "comment": null, "summary": "We present Logics-STEM, a state-of-the-art reasoning model fine-tuned on Logics-STEM-SFT-Dataset, a high-quality and diverse dataset at 10M scale that represents one of the largest-scale open-source long chain-of-thought corpora. Logics-STEM targets reasoning tasks in the domains of Science, Technology, Engineering, and Mathematics (STEM), and exhibits exceptional performance on STEM-related benchmarks with an average improvement of 4.68% over the next-best model at 8B scale. We attribute the gains to our data-algorithm co-design engine, where they are jointly optimized to fit a gold-standard distribution behind reasoning. Data-wise, the Logics-STEM-SFT-Dataset is constructed from a meticulously designed data curation engine with 5 stages to ensure the quality, diversity, and scalability, including annotation, deduplication, decontamination, distillation, and stratified sampling. Algorithm-wise, our failure-driven post-training framework leverages targeted knowledge retrieval and data synthesis around model failure regions in the Supervised Fine-tuning (SFT) stage to effectively guide the second-stage SFT or the reinforcement learning (RL) for better fitting the target distribution. The superior empirical performance of Logics-STEM reveals the vast potential of combining large-scale open-source data with carefully designed synthetic data, underscoring the critical role of data-algorithm co-design in enhancing reasoning capabilities through post-training. We make both the Logics-STEM models (8B and 32B) and the Logics-STEM-SFT-Dataset (10M and downsampled 2.2M versions) publicly available to support future research in the open-source community.", "AI": {"tldr": "Logics-STEM\u662f\u4e00\u4e2a\u9488\u5bf9STEM\u9886\u57df\u63a8\u7406\u4efb\u52a1\u7684\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u901a\u8fc7\u6570\u636e\u7b97\u6cd5\u534f\u540c\u8bbe\u8ba1\u572810M\u89c4\u6a21\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\uff0c\u5728STEM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u63d0\u53474.68%", "motivation": "\u9488\u5bf9STEM\u9886\u57df\u63a8\u7406\u4efb\u52a1\uff0c\u73b0\u6709\u6a21\u578b\u5728\u79d1\u5b66\u3001\u6280\u672f\u3001\u5de5\u7a0b\u548c\u6570\u5b66\u63a8\u7406\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6570\u636e\u548c\u7b97\u6cd5\u534f\u540c\u4f18\u5316\u6765\u63d0\u5347\u63a8\u7406\u80fd\u529b", "method": "\u91c7\u7528\u6570\u636e\u7b97\u6cd5\u534f\u540c\u8bbe\u8ba1\u5f15\u64ce\uff1a\u6570\u636e\u65b9\u9762\u901a\u8fc75\u9636\u6bb5\u6570\u636e\u7b56\u5c55\u5f15\u64ce\u6784\u5efa10M\u89c4\u6a21\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff1b\u7b97\u6cd5\u65b9\u9762\u4f7f\u7528\u6545\u969c\u9a71\u52a8\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u9488\u5bf9SFT\u9636\u6bb5\u7684\u5931\u8d25\u533a\u57df\u8fdb\u884c\u9488\u5bf9\u6027\u77e5\u8bc6\u68c0\u7d22\u548c\u6570\u636e\u5408\u6210", "result": "\u5728STEM\u76f8\u5173\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u76f8\u6bd48B\u89c4\u6a21\u6b21\u4f18\u6a21\u578b\u5e73\u5747\u63d0\u53474.68%\uff0c\u5c55\u793a\u4e86\u5927\u89c4\u6a21\u5f00\u6e90\u6570\u636e\u4e0e\u7cbe\u5fc3\u8bbe\u8ba1\u5408\u6210\u6570\u636e\u7ed3\u5408\u7684\u6f5c\u529b", "conclusion": "\u6570\u636e\u7b97\u6cd5\u534f\u540c\u8bbe\u8ba1\u901a\u8fc7\u540e\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6570\u636e\u4e0e\u9488\u5bf9\u6027\u7b97\u6cd5\u4f18\u5316\u7684\u7ed3\u5408\u5bf9STEM\u63a8\u7406\u4efb\u52a1\u7684\u91cd\u8981\u6027\uff0c\u76f8\u5173\u6a21\u578b\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90"}}
{"id": "2601.01449", "categories": ["cs.CL", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01449", "abs": "https://arxiv.org/abs/2601.01449", "authors": ["Harshil Darji", "Martin Heckelmann", "Christina Kratsch", "Gerard de Melo"], "title": "Segmentation and Processing of German Court Decisions from Open Legal Data", "comment": "Accepted and published as a research article in Legal Knowledge and Information Systems (JURIX 2025 proceedings, IOS Press). Pages 276--281", "summary": "The availability of structured legal data is important for advancing Natural Language Processing (NLP) techniques for the German legal system. One of the most widely used datasets, Open Legal Data, provides a large-scale collection of German court decisions. While the metadata in this raw dataset is consistently structured, the decision texts themselves are inconsistently formatted and often lack clearly marked sections. Reliable separation of these sections is important not only for rhetorical role classification but also for downstream tasks such as retrieval and citation analysis. In this work, we introduce a cleaned and sectioned dataset of 251,038 German court decisions derived from the official Open Legal Data dataset. We systematically separated three important sections in German court decisions, namely Tenor (operative part of the decision), Tatbestand (facts of the case), and Entscheidungsgr\u00fcnde (judicial reasoning), which are often inconsistently represented in the original dataset. To ensure the reliability of our extraction process, we used Cochran's formula with a 95% confidence level and a 5% margin of error to draw a statistically representative random sample of 384 cases, and manually verified that all three sections were correctly identified. We also extracted the Rechtsmittelbelehrung (appeal notice) as a separate field, since it is a procedural instruction and not part of the decision itself. The resulting corpus is publicly available in the JSONL format, making it an accessible resource for further research on the German legal system.", "AI": {"tldr": "\u672c\u6587\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b251,038\u4e2a\u5fb7\u56fd\u6cd5\u9662\u5224\u51b3\u7684\u6e05\u6d17\u548c\u5206\u6bb5\u6570\u636e\u96c6\uff0c\u4eceOpen Legal Data\u539f\u59cb\u6570\u636e\u4e2d\u7cfb\u7edf\u5206\u79bb\u4e86\u5224\u51b3\u4e66\u7684\u5173\u952e\u90e8\u5206\uff0c\u5e76\u901a\u8fc7\u7edf\u8ba1\u62bd\u6837\u9a8c\u8bc1\u4e86\u63d0\u53d6\u51c6\u786e\u6027\u3002", "motivation": "\u5fb7\u56fd\u6cd5\u5f8b\u7cfb\u7edf\u7684\u7ed3\u6784\u5316\u6570\u636e\u5bf9\u4e8e\u63a8\u8fdbNLP\u6280\u672f\u5f88\u91cd\u8981\u3002\u867d\u7136Open Legal Data\u63d0\u4f9b\u4e86\u5927\u89c4\u6a21\u7684\u5fb7\u56fd\u6cd5\u9662\u5224\u51b3\u6570\u636e\uff0c\u4f46\u539f\u59cb\u6570\u636e\u4e2d\u5224\u51b3\u6587\u672c\u683c\u5f0f\u4e0d\u4e00\u81f4\uff0c\u7f3a\u4e4f\u6e05\u6670\u6807\u8bb0\u7684\u90e8\u5206\uff0c\u8fd9\u5f71\u54cd\u4e86\u4fee\u8f9e\u89d2\u8272\u5206\u7c7b\u3001\u68c0\u7d22\u548c\u5f15\u8bc1\u5206\u6790\u7b49\u4e0b\u6e38\u4efb\u52a1\u3002", "method": "\u4eceOpen Legal Data\u5b98\u65b9\u6570\u636e\u96c6\u4e2d\u63d0\u53d6251,038\u4e2a\u5fb7\u56fd\u6cd5\u9662\u5224\u51b3\uff0c\u7cfb\u7edf\u5206\u79bb\u4e09\u4e2a\u91cd\u8981\u90e8\u5206\uff1aTenor\uff08\u5224\u51b3\u4e3b\u6587\uff09\u3001Tatbestand\uff08\u6848\u4ef6\u4e8b\u5b9e\uff09\u548cEntscheidungsgr\u00fcnde\uff08\u5224\u51b3\u7406\u7531\uff09\u3002\u4f7f\u7528Cochran\u516c\u5f0f\u4ee595%\u7f6e\u4fe1\u6c34\u5e73\u548c5%\u8bef\u5dee\u5e45\u5ea6\u62bd\u53d6384\u4e2a\u6848\u4f8b\u7684\u7edf\u8ba1\u4ee3\u8868\u6027\u968f\u673a\u6837\u672c\uff0c\u624b\u52a8\u9a8c\u8bc1\u6240\u6709\u4e09\u4e2a\u90e8\u5206\u7684\u6b63\u786e\u8bc6\u522b\u3002\u8fd8\u5c06Rechtsmittelbelehrung\uff08\u4e0a\u8bc9\u901a\u77e5\uff09\u4f5c\u4e3a\u5355\u72ec\u5b57\u6bb5\u63d0\u53d6\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u516c\u5f00\u53ef\u7528\u7684\u6e05\u6d17\u548c\u5206\u6bb5\u5fb7\u56fd\u6cd5\u9662\u5224\u51b3\u6570\u636e\u96c6\uff0c\u5305\u542b251,038\u4e2a\u6848\u4f8b\uff0c\u4ee5JSONL\u683c\u5f0f\u63d0\u4f9b\u3002\u7edf\u8ba1\u9a8c\u8bc1\u663e\u793a\u63d0\u53d6\u8fc7\u7a0b\u53ef\u9760\uff0c\u4e3a\u5fb7\u56fd\u6cd5\u5f8b\u7cfb\u7edf\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u8bbf\u95ee\u8d44\u6e90\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u8d28\u91cf\u3001\u7ed3\u6784\u5316\u7684\u5fb7\u56fd\u6cd5\u5f8b\u6587\u672c\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u4e86\u539f\u59cb\u6570\u636e\u683c\u5f0f\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u4e3a\u5fb7\u56fd\u6cd5\u5f8b\u7cfb\u7edf\u7684NLP\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u8d44\u6e90\uff0c\u652f\u6301\u4fee\u8f9e\u89d2\u8272\u5206\u7c7b\u3001\u68c0\u7d22\u548c\u5f15\u8bc1\u5206\u6790\u7b49\u4e0b\u6e38\u4efb\u52a1\u3002"}}
{"id": "2601.00924", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00924", "abs": "https://arxiv.org/abs/2601.00924", "authors": ["Rares Folea", "Radu Iacob", "Emil Slusanschi", "Traian Rebedea"], "title": "Complexity-based code embeddings", "comment": null, "summary": "This paper presents a generic method for transforming the source code of various algorithms to numerical embeddings, by dynamically analysing the behaviour of computer programs against different inputs and by tailoring multiple generic complexity functions for the analysed metrics. The used algorithms embeddings are based on r-Complexity . Using the proposed code embeddings, we present an implementation of the XGBoost algorithm that achieves an average F1-score on a multi-label dataset with 11 classes, built using real-world code snippets submitted for programming competitions on the Codeforces platform.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5c06\u7b97\u6cd5\u6e90\u4ee3\u7801\u8f6c\u6362\u4e3a\u6570\u503c\u5d4c\u5165\u7684\u901a\u7528\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u5206\u6790\u7a0b\u5e8f\u884c\u4e3a\u5e76\u5b9a\u5236\u590d\u6742\u5ea6\u51fd\u6570\uff0c\u57fa\u4e8er-Complexity\u6784\u5efa\u4ee3\u7801\u5d4c\u5165\uff0c\u5728Codeforces\u771f\u5b9e\u4ee3\u7801\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u591a\u6807\u7b7e\u5206\u7c7b", "motivation": "\u9700\u8981\u5c06\u7b97\u6cd5\u6e90\u4ee3\u7801\u8f6c\u6362\u4e3a\u6570\u503c\u8868\u793a\uff08\u5d4c\u5165\uff09\uff0c\u4ee5\u4fbf\u8fdb\u884c\u673a\u5668\u5b66\u4e60\u4efb\u52a1\uff0c\u7279\u522b\u662f\u5bf9\u7f16\u7a0b\u7ade\u8d5b\u4e2d\u7684\u4ee3\u7801\u8fdb\u884c\u5206\u7c7b\u548c\u5206\u6790", "method": "\u901a\u8fc7\u52a8\u6001\u5206\u6790\u7a0b\u5e8f\u5728\u4e0d\u540c\u8f93\u5165\u4e0b\u7684\u884c\u4e3a\uff0c\u4e3a\u5206\u6790\u6307\u6807\u5b9a\u5236\u591a\u4e2a\u901a\u7528\u590d\u6742\u5ea6\u51fd\u6570\uff0c\u57fa\u4e8er-Complexity\u6784\u5efa\u7b97\u6cd5\u5d4c\u5165\uff0c\u4f7f\u7528\u8fd9\u4e9b\u5d4c\u5165\u8bad\u7ec3XGBoost\u6a21\u578b", "result": "\u5728\u5305\u542b11\u4e2a\u7c7b\u522b\u7684\u591a\u6807\u7b7e\u6570\u636e\u96c6\u4e0a\uff08\u57fa\u4e8eCodeforces\u5e73\u53f0\u771f\u5b9e\u4ee3\u7801\u7247\u6bb5\uff09\u5b9e\u73b0\u4e86\u5e73\u5747F1\u5206\u6570\uff0c\u8868\u660e\u8be5\u65b9\u6cd5\u6709\u6548", "conclusion": "\u63d0\u51fa\u7684\u901a\u7528\u4ee3\u7801\u5d4c\u5165\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u8868\u793a\u7b97\u6cd5\u6e90\u4ee3\u7801\uff0c\u652f\u6301\u673a\u5668\u5b66\u4e60\u5e94\u7528\uff0c\u5728\u771f\u5b9e\u7f16\u7a0b\u7ade\u8d5b\u4ee3\u7801\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d"}}
{"id": "2601.01569", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.01569", "abs": "https://arxiv.org/abs/2601.01569", "authors": ["Maohao Ran", "Zhenglin Wan", "Cooper Lin", "Yanting Zhang", "Hongyu Xin", "Hongwei Fan", "Yibo Xu", "Beier Luo", "Yaxin Zhou", "Wangbo Zhao", "Lijie Yang", "Lang Feng", "Fuchao Yang", "Jingxuan Wu", "Yiqiao Huang", "Chendong Ma", "Dailing Jiang", "Jianbo Deng", "Sihui Han", "Bo An", "Yike Guo", "Jun Song"], "title": "CaveAgent: Transforming LLMs into Stateful Runtime Operators", "comment": "32 pages, 14 Figures", "summary": "LLM-based agents are increasingly capable of complex task execution, yet current agentic systems remain constrained by text-centric paradigms. Traditional approaches rely on procedural JSON-based function calling, which often struggles with long-horizon tasks due to fragile multi-turn dependencies and context drift. In this paper, we present CaveAgent, a framework that transforms the paradigm from \"LLM-as-Text-Generator\" to \"LLM-as-Runtime-Operator.\" We introduce a Dual-stream Context Architecture that decouples state management into a lightweight semantic stream for reasoning and a persistent, deterministic Python Runtime stream for execution. In addition to leveraging code generation to efficiently resolve interdependent sub-tasks (e.g., loops, conditionals) in a single step, we introduce \\textit{Stateful Runtime Management} in CaveAgent. Distinct from existing code-based approaches that remain text-bound and lack the support for external object injection and retrieval, CaveAgent injects, manipulates, and retrieves complex Python objects (e.g., DataFrames, database connections) that persist across turns. This persistence mechanism acts as a high-fidelity external memory to eliminate context drift, avoid catastrophic forgetting, while ensuring that processed data flows losslessly to downstream applications. Comprehensive evaluations on Tau$^2$-bench, BFCL and various case studies across representative SOTA LLMs demonstrate CaveAgent's superiority. Specifically, our framework achieves a 10.5\\% success rate improvement on retail tasks and reduces total token consumption by 28.4\\% in multi-turn scenarios. On data-intensive tasks, direct variable storage and retrieval reduces token consumption by 59\\%, allowing CaveAgent to handle large-scale data that causes context overflow failures in both JSON-based and Code-based agents.", "AI": {"tldr": "CaveAgent\u662f\u4e00\u4e2a\u5c06LLM\u4ece\u6587\u672c\u751f\u6210\u5668\u8f6c\u53d8\u4e3a\u8fd0\u884c\u65f6\u64cd\u4f5c\u5458\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u6d41\u67b6\u6784\u5206\u79bb\u72b6\u6001\u7ba1\u7406\uff0c\u652f\u6301Python\u5bf9\u8c61\u6301\u4e45\u5316\uff0c\u663e\u8457\u63d0\u5347\u590d\u6742\u4efb\u52a1\u6267\u884c\u6548\u7387\u5e76\u51cf\u5c11token\u6d88\u8017\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u53d7\u9650\u4e8e\u6587\u672c\u4e2d\u5fc3\u8303\u5f0f\uff0c\u4f20\u7edf\u7684JSON\u51fd\u6570\u8c03\u7528\u5728\u5904\u7406\u957f\u65f6\u7a0b\u4efb\u52a1\u65f6\u5b58\u5728\u8106\u5f31\u7684\u591a\u8f6e\u4f9d\u8d56\u548c\u4e0a\u4e0b\u6587\u6f02\u79fb\u95ee\u9898\uff0c\u9700\u8981\u66f4\u5f3a\u5927\u7684\u72b6\u6001\u7ba1\u7406\u548c\u6267\u884c\u80fd\u529b\u3002", "method": "\u63d0\u51faCaveAgent\u6846\u67b6\uff0c\u91c7\u7528\u53cc\u6d41\u4e0a\u4e0b\u6587\u67b6\u6784\uff1a\u8f7b\u91cf\u7ea7\u8bed\u4e49\u6d41\u7528\u4e8e\u63a8\u7406\uff0c\u6301\u4e45\u5316\u786e\u5b9a\u6027Python\u8fd0\u884c\u65f6\u6d41\u7528\u4e8e\u6267\u884c\u3002\u5f15\u5165\u72b6\u6001\u5316\u8fd0\u884c\u65f6\u7ba1\u7406\uff0c\u652f\u6301\u590d\u6742Python\u5bf9\u8c61\u7684\u6ce8\u5165\u3001\u64cd\u4f5c\u548c\u8de8\u8f6e\u6b21\u6301\u4e45\u5316\u3002", "result": "\u5728Tau\u00b2-bench\u3001BFCL\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff1a\u96f6\u552e\u4efb\u52a1\u6210\u529f\u7387\u63d0\u534710.5%\uff0c\u591a\u8f6e\u573a\u666f\u603btoken\u6d88\u8017\u51cf\u5c1128.4%\uff0c\u6570\u636e\u5bc6\u96c6\u578b\u4efb\u52a1token\u6d88\u8017\u51cf\u5c1159%\uff0c\u80fd\u591f\u5904\u7406\u5bfc\u81f4\u5176\u4ed6\u667a\u80fd\u4f53\u4e0a\u4e0b\u6587\u6ea2\u51fa\u7684\u6d77\u91cf\u6570\u636e\u3002", "conclusion": "CaveAgent\u901a\u8fc7\u5c06LLM\u8f6c\u53d8\u4e3a\u8fd0\u884c\u65f6\u64cd\u4f5c\u5458\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u4e0a\u4e0b\u6587\u6f02\u79fb\u548c\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u4e3a\u590d\u6742\u3001\u957f\u65f6\u7a0b\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u9760\u7684\u6267\u884c\u6846\u67b6\u3002"}}
{"id": "2601.01461", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.01461", "abs": "https://arxiv.org/abs/2601.01461", "authors": ["Yuxiang Mei", "Dongxing Xu", "Jiaen Liang", "Yanhua Long"], "title": "Bridging the gap: A comparative exploration of Speech-LLM and end-to-end architecture for multilingual conversational ASR", "comment": "5 pages, 1 figure", "summary": "The INTERSPEECH 2025 Challenge on Multilingual Conversational Speech Language Models (MLC-SLM) promotes multilingual conversational ASR with large language models (LLMs). Our previous SHNU-mASR system adopted a competitive parallel-speech-encoder architecture that integrated Whisper and mHuBERT with an LLM. However, it faced two challenges: simple feature concatenation may not fully exploit complementary information, and the performance gap between LLM-based ASR and end-to-end(E2E) encoder-decoder ASR remained unexplored. In this work, we present an enhanced LLM-based ASR framework that combines fine-tuned Whisper and mHuBERT encoders with an LLM to enrich speech representations. We first evaluate E2E Whisper models with LoRA and full fine-tuning on the MLC-SLM ASR task, and then propose cross-attention-based fusion mechanisms for the parallel-speech-encoder. On the official evaluation set of the MLC-SLM Challenge, our system achieves a CER/WER of 10.69%, ranking on par with the top-ranked Track 1 systems, even though it uses only 1,500 hours of baseline training data compared with their large-scale training sets. Nonetheless, we find that our final LLM-based ASR still does not match the performance of a fine-tuned E2E Whisper model, providing valuable empirical guidance for future Speech-LLM design. Our code is publicly available at https://github.com/1535176727/MLC-SLM.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u589e\u5f3a\u578bLLM-based ASR\u6846\u67b6\uff0c\u7ed3\u5408\u5fae\u8c03Whisper\u548cmHuBERT\u7f16\u7801\u5668\uff0c\u91c7\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u878d\u5408\u673a\u5236\uff0c\u5728MLC-SLM\u6311\u6218\u4e2d\u53d6\u5f9710.69% CER/WER\uff0c\u4e0e\u4f7f\u7528\u5927\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\u7684\u9876\u7ea7\u7cfb\u7edf\u8868\u73b0\u76f8\u5f53\u3002", "motivation": "\u89e3\u51b3\u591a\u8bed\u8a00\u5bf9\u8bdd\u8bed\u97f3\u8bc6\u522b\u4e2dLLM-based ASR\u7684\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a1\uff09\u7b80\u5355\u7279\u5f81\u62fc\u63a5\u65e0\u6cd5\u5145\u5206\u5229\u7528\u4e92\u8865\u4fe1\u606f\uff1b2\uff09LLM-based ASR\u4e0e\u7aef\u5230\u7aef\u7f16\u7801\u5668-\u89e3\u7801\u5668ASR\u7684\u6027\u80fd\u5dee\u8ddd\u5c1a\u672a\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u589e\u5f3a\u578bLLM-based ASR\u6846\u67b6\uff0c\u7ed3\u5408\u5fae\u8c03\u7684Whisper\u548cmHuBERT\u7f16\u7801\u5668\uff0c\u91c7\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u878d\u5408\u673a\u5236\u6574\u5408\u8bed\u97f3\u8868\u793a\u3002\u9996\u5148\u8bc4\u4f30LoRA\u548c\u5168\u5fae\u8c03\u7684E2E Whisper\u6a21\u578b\uff0c\u7136\u540e\u4e3a\u5e76\u884c\u8bed\u97f3\u7f16\u7801\u5668\u8bbe\u8ba1\u57fa\u4e8e\u4ea4\u53c9\u6ce8\u610f\u529b\u7684\u878d\u5408\u65b9\u6cd5\u3002", "result": "\u5728MLC-SLM\u6311\u6218\u5b98\u65b9\u8bc4\u4f30\u96c6\u4e0a\uff0c\u7cfb\u7edf\u83b7\u5f9710.69% CER/WER\uff0c\u4e0eTrack 1\u9876\u7ea7\u7cfb\u7edf\u8868\u73b0\u76f8\u5f53\uff0c\u5c3d\u7ba1\u4ec5\u4f7f\u75281,500\u5c0f\u65f6\u57fa\u7ebf\u8bad\u7ec3\u6570\u636e\uff08\u5bf9\u6bd4\u5176\u4ed6\u7cfb\u7edf\u7684\u5927\u89c4\u6a21\u8bad\u7ec3\u96c6\uff09\u3002\u4f46\u6700\u7ec8LLM-based ASR\u4ecd\u4e0d\u53ca\u5fae\u8c03E2E Whisper\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u589e\u5f3a\u578bLLM-based ASR\u6846\u67b6\u5728\u591a\u8bed\u8a00\u5bf9\u8bddASR\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46LLM-based ASR\u4ecd\u65e0\u6cd5\u8d85\u8d8a\u5fae\u8c03E2E Whisper\u6a21\u578b\uff0c\u4e3a\u672a\u6765Speech-LLM\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5b9e\u8bc1\u6307\u5bfc\u3002"}}
{"id": "2601.00932", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00932", "abs": "https://arxiv.org/abs/2601.00932", "authors": ["Andrea Thomas Nava", "Lijo Johny", "Fabio Azzalini", "Johannes Schneider", "Arianna Casanova"], "title": "Enhanced Data-Driven Product Development via Gradient Based Optimization and Conformalized Monte Carlo Dropout Uncertainty Estimation", "comment": "Accepted at the 18th International Conference on Agents and Artificial Intelligence (ICAART 2026)", "summary": "Data-Driven Product Development (DDPD) leverages data to learn the relationship between product design specifications and resulting properties. To discover improved designs, we train a neural network on past experiments and apply Projected Gradient Descent to identify optimal input features that maximize performance. Since many products require simultaneous optimization of multiple correlated properties, our framework employs joint neural networks to capture interdependencies among targets. Furthermore, we integrate uncertainty estimation via \\emph{Conformalised Monte Carlo Dropout} (ConfMC), a novel method combining Nested Conformal Prediction with Monte Carlo dropout to provide model-agnostic, finite-sample coverage guarantees under data exchangeability. Extensive experiments on five real-world datasets show that our method matches state-of-the-art performance while offering adaptive, non-uniform prediction intervals and eliminating the need for retraining when adjusting coverage levels.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u4ea7\u54c1\u5f00\u53d1\u6846\u67b6\uff0c\u4f7f\u7528\u8054\u5408\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u591a\u4e2a\u76f8\u5173\u5c5e\u6027\uff0c\u5e76\u5f15\u5165ConfMC\u65b9\u6cd5\u63d0\u4f9b\u5177\u6709\u6709\u9650\u6837\u672c\u8986\u76d6\u4fdd\u8bc1\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "motivation": "\u4f20\u7edf\u4ea7\u54c1\u5f00\u53d1\u4e2d\uff0c\u9700\u8981\u540c\u65f6\u4f18\u5316\u591a\u4e2a\u76f8\u5173\u5c5e\u6027\uff0c\u4e14\u9700\u8981\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u6765\u6307\u5bfc\u8bbe\u8ba1\u51b3\u7b56\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u63d0\u4f9b\u6709\u9650\u6837\u672c\u8986\u76d6\u4fdd\u8bc1\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u4e14\u8c03\u6574\u7f6e\u4fe1\u6c34\u5e73\u65f6\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u3002", "method": "1) \u4f7f\u7528\u8054\u5408\u795e\u7ecf\u7f51\u7edc\u6355\u6349\u591a\u4e2a\u76ee\u6807\u95f4\u7684\u76f8\u4e92\u4f9d\u8d56\u5173\u7cfb\uff1b2) \u91c7\u7528\u6295\u5f71\u68af\u5ea6\u4e0b\u964d\u6cd5\u5bfb\u627e\u6700\u4f18\u8f93\u5165\u7279\u5f81\uff1b3) \u63d0\u51faConfMC\u65b9\u6cd5\uff0c\u7ed3\u5408\u5d4c\u5957\u4fdd\u5f62\u9884\u6d4b\u548c\u8499\u7279\u5361\u6d1bdropout\uff0c\u63d0\u4f9b\u6a21\u578b\u65e0\u5173\u7684\u6709\u9650\u6837\u672c\u8986\u76d6\u4fdd\u8bc1\u3002", "result": "\u5728\u4e94\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u540c\u65f6\u63d0\u4f9b\u81ea\u9002\u5e94\u3001\u975e\u5747\u5300\u7684\u9884\u6d4b\u533a\u95f4\uff0c\u4e14\u8c03\u6574\u7f6e\u4fe1\u6c34\u5e73\u65f6\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6570\u636e\u9a71\u52a8\u7684\u4ea7\u54c1\u5f00\u53d1\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u65b9\u6cd5\uff0c\u80fd\u591f\u540c\u65f6\u4f18\u5316\u591a\u4e2a\u76f8\u5173\u5c5e\u6027\uff0c\u5e76\u63d0\u4f9b\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2601.01609", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01609", "abs": "https://arxiv.org/abs/2601.01609", "authors": ["Albert Sadowski", "Jaros\u0142aw A. Chudziak"], "title": "Structured Decomposition for LLM Reasoning: Cross-Domain Validation and Semantic Web Integration", "comment": null, "summary": "Rule-based reasoning over natural language input arises in domains where decisions must be auditable and justifiable: clinical protocols specify eligibility criteria in prose, evidence rules define admissibility through textual conditions, and scientific standards dictate methodological requirements. Applying rules to such inputs demands both interpretive flexibility and formal guarantees. Large language models (LLMs) provide flexibility but cannot ensure consistent rule application; symbolic systems provide guarantees but require structured input. This paper presents an integration pattern that combines these strengths: LLMs serve as ontology population engines, translating unstructured text into ABox assertions according to expert-authored TBox specifications, while SWRL-based reasoners apply rules with deterministic guarantees. The framework decomposes reasoning into entity identification, assertion extraction, and symbolic verification, with task definitions grounded in OWL 2 ontologies. Experiments across three domains (legal hearsay determination, scientific method-task application, clinical trial eligibility) and eleven language models validate the approach. Structured decomposition achieves statistically significant improvements over few-shot prompting in aggregate, with gains observed across all three domains. An ablation study confirms that symbolic verification provides substantial benefit beyond structured prompting alone. The populated ABox integrates with standard semantic web tooling for inspection and querying, positioning the framework for richer inference patterns that simpler formalisms cannot express.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408LLM\u7075\u6d3b\u6027\u548c\u7b26\u53f7\u63a8\u7406\u4fdd\u8bc1\u7684\u96c6\u6210\u6846\u67b6\uff0c\u901a\u8fc7LLM\u5c06\u975e\u7ed3\u6784\u5316\u6587\u672c\u8f6c\u6362\u4e3aABox\u65ad\u8a00\uff0c\u518d\u4f7f\u7528SWRL\u63a8\u7406\u5668\u8fdb\u884c\u786e\u5b9a\u6027\u89c4\u5219\u5e94\u7528\u3002", "motivation": "\u5728\u9700\u8981\u53ef\u5ba1\u8ba1\u548c\u53ef\u89e3\u91ca\u51b3\u7b56\u7684\u9886\u57df\uff08\u5982\u4e34\u5e8a\u534f\u8bae\u3001\u6cd5\u5f8b\u8bc1\u636e\u89c4\u5219\u3001\u79d1\u5b66\u6807\u51c6\uff09\uff0c\u9700\u8981\u65e2\u6709\u89e3\u91ca\u7075\u6d3b\u6027\u53c8\u6709\u5f62\u5f0f\u5316\u4fdd\u8bc1\u7684\u89c4\u5219\u63a8\u7406\u3002LLM\u63d0\u4f9b\u7075\u6d3b\u6027\u4f46\u65e0\u6cd5\u4fdd\u8bc1\u4e00\u81f4\u6027\uff0c\u7b26\u53f7\u7cfb\u7edf\u63d0\u4f9b\u4fdd\u8bc1\u4f46\u9700\u8981\u7ed3\u6784\u5316\u8f93\u5165\u3002", "method": "\u63d0\u51fa\u96c6\u6210\u6a21\u5f0f\uff1aLLM\u4f5c\u4e3a\u672c\u4f53\u586b\u5145\u5f15\u64ce\uff0c\u5c06\u975e\u7ed3\u6784\u5316\u6587\u672c\u8f6c\u6362\u4e3a\u57fa\u4e8e\u4e13\u5bb6\u7f16\u5199TBox\u89c4\u8303\u7684ABox\u65ad\u8a00\uff0cSWRL\u63a8\u7406\u5668\u5e94\u7528\u89c4\u5219\u63d0\u4f9b\u786e\u5b9a\u6027\u4fdd\u8bc1\u3002\u6846\u67b6\u5c06\u63a8\u7406\u5206\u89e3\u4e3a\u5b9e\u4f53\u8bc6\u522b\u3001\u65ad\u8a00\u63d0\u53d6\u548c\u7b26\u53f7\u9a8c\u8bc1\u3002", "result": "\u5728\u4e09\u4e2a\u9886\u57df\uff08\u6cd5\u5f8b\u4f20\u95fb\u786e\u5b9a\u3001\u79d1\u5b66\u65b9\u6cd5\u4efb\u52a1\u5e94\u7528\u3001\u4e34\u5e8a\u8bd5\u9a8c\u8d44\u683c\uff09\u548c11\u4e2a\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u6709\u6548\u6027\u3002\u7ed3\u6784\u5316\u5206\u89e3\u5728\u603b\u4f53\u4e0a\u6bd4few-shot\u63d0\u793a\u6709\u7edf\u8ba1\u663e\u8457\u6539\u8fdb\uff0c\u6240\u6709\u4e09\u4e2a\u9886\u57df\u90fd\u6709\u589e\u76ca\u3002\u6d88\u878d\u7814\u7a76\u786e\u8ba4\u7b26\u53f7\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u8d85\u8d8a\u7ed3\u6784\u5316\u63d0\u793a\u7684\u5b9e\u8d28\u6027\u597d\u5904\u3002", "conclusion": "\u8be5\u6846\u67b6\u7ed3\u5408\u4e86LLM\u7684\u7075\u6d3b\u6027\u548c\u7b26\u53f7\u63a8\u7406\u7684\u4fdd\u8bc1\uff0c\u586b\u5145\u7684ABox\u53ef\u4e0e\u6807\u51c6\u8bed\u4e49Web\u5de5\u5177\u96c6\u6210\u8fdb\u884c\u68c0\u67e5\u548c\u67e5\u8be2\uff0c\u4e3a\u66f4\u4e30\u5bcc\u7684\u63a8\u7406\u6a21\u5f0f\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2601.01477", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01477", "abs": "https://arxiv.org/abs/2601.01477", "authors": ["May-Myo Zin", "Sabine Wehnert", "Yuntao Kong", "Ha-Thanh Nguyen", "Wachara Fungwacharakorn", "Jieying Xue", "Micha\u0142 Araszkiewicz", "Randy Goebel", "Ken Satoh", "Le-Minh Nguyen"], "title": "Can Legislation Be Made Machine-Readable in PROLEG?", "comment": null, "summary": "The anticipated positive social impact of regulatory processes requires both the accuracy and efficiency of their application. Modern artificial intelligence technologies, including natural language processing and machine-assisted reasoning, hold great promise for addressing this challenge. We present a framework to address the challenge of tools for regulatory application, based on current state-of-the-art (SOTA) methods for natural language processing (large language models or LLMs) and formalization of legal reasoning (the legal representation system PROLEG). As an example, we focus on Article 6 of the European General Data Protection Regulation (GDPR). In our framework, a single LLM prompt simultaneously transforms legal text into if-then rules and a corresponding PROLEG encoding, which are then validated and refined by legal domain experts. The final output is an executable PROLEG program that can produce human-readable explanations for instances of GDPR decisions. We describe processes to support the end-to-end transformation of a segment of a regulatory document (Article 6 from GDPR), including the prompting frame to guide an LLM to \"compile\" natural language text to if-then rules, then to further \"compile\" the vetted if-then rules to PROLEG. Finally, we produce an instance that shows the PROLEG execution. We conclude by summarizing the value of this approach and note observed limitations with suggestions to further develop such technologies for capturing and deploying regulatory frameworks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548cPROLEG\u6cd5\u5f8b\u8868\u793a\u7cfb\u7edf\u7684\u6846\u67b6\uff0c\u5c06GDPR\u7b49\u6cd5\u89c4\u6587\u672c\u81ea\u52a8\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u89c4\u5219\uff0c\u5e76\u751f\u6210\u4eba\u7c7b\u53ef\u8bfb\u7684\u89e3\u91ca\u3002", "motivation": "\u6cd5\u89c4\u5e94\u7528\u9700\u8981\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u73b0\u4ee3AI\u6280\u672f\uff08\u7279\u522b\u662fNLP\u548c\u673a\u5668\u8f85\u52a9\u63a8\u7406\uff09\u6709\u671b\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\uff0c\u4f46\u9700\u8981\u7cfb\u7edf\u5316\u65b9\u6cd5\u6765\u8f6c\u6362\u6cd5\u5f8b\u6587\u672c\u4e3a\u53ef\u6267\u884c\u89c4\u5219\u3002", "method": "\u4f7f\u7528LLM\u63d0\u793a\u5c06\u6cd5\u5f8b\u6587\u672c\uff08\u4ee5GDPR\u7b2c6\u6761\u4e3a\u4f8b\uff09\u540c\u65f6\u8f6c\u6362\u4e3aif-then\u89c4\u5219\u548cPROLEG\u7f16\u7801\uff0c\u7ecf\u6cd5\u5f8b\u4e13\u5bb6\u9a8c\u8bc1\u548c\u7cbe\u70bc\u540e\uff0c\u751f\u6210\u53ef\u6267\u884c\u7684PROLEG\u7a0b\u5e8f\u3002", "result": "\u5f00\u53d1\u51fa\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u6210\u529f\u5c06GDPR\u7b2c6\u6761\u8f6c\u6362\u4e3a\u53ef\u6267\u884cPROLEG\u7a0b\u5e8f\uff0c\u80fd\u591f\u4e3aGDPR\u51b3\u7b56\u5b9e\u4f8b\u751f\u6210\u4eba\u7c7b\u53ef\u8bfb\u7684\u89e3\u91ca\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86AI\u6280\u672f\u5728\u6cd5\u89c4\u6355\u83b7\u548c\u90e8\u7f72\u4e2d\u7684\u4ef7\u503c\uff0c\u4f46\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u5f00\u53d1\u4ee5\u5b8c\u5584\u8fd9\u7c7b\u6280\u672f\u3002"}}
{"id": "2601.00933", "categories": ["cs.LG", "cs.AI", "cs.SI"], "pdf": "https://arxiv.org/pdf/2601.00933", "abs": "https://arxiv.org/abs/2601.00933", "authors": ["Jinyu Xu", "Abhishek K. Umrawal"], "title": "LOFA: Online Influence Maximization under Full-Bandit Feedback using Lazy Forward Selection", "comment": "14 pages and 6 figures", "summary": "We study the problem of influence maximization (IM) in an online setting, where the goal is to select a subset of nodes$\\unicode{x2014}$called the seed set$\\unicode{x2014}$at each time step over a fixed time horizon, subject to a cardinality budget constraint, to maximize the expected cumulative influence. We operate under a full-bandit feedback model, where only the influence of the chosen seed set at each time step is observed, with no additional structural information about the network or diffusion process. It is well-established that the influence function is submodular, and existing algorithms exploit this property to achieve low regret. In this work, we leverage this property further and propose the Lazy Online Forward Algorithm (LOFA), which achieves a lower empirical regret. We conduct experiments on a real-world social network to demonstrate that LOFA achieves superior performance compared to existing bandit algorithms in terms of cumulative regret and instantaneous reward.", "AI": {"tldr": "\u63d0\u51faLOFA\u7b97\u6cd5\u7528\u4e8e\u5728\u7ebf\u5f71\u54cd\u529b\u6700\u5927\u5316\u95ee\u9898\uff0c\u5728\u5b8c\u5168\u5f3a\u76d7\u53cd\u9988\u4e0b\u901a\u8fc7\u5229\u7528\u5b50\u6a21\u6027\u5b9e\u73b0\u66f4\u4f4e\u7ecf\u9a8c\u9057\u61be", "motivation": "\u5728\u7ebf\u5f71\u54cd\u529b\u6700\u5927\u5316\u95ee\u9898\u4e2d\uff0c\u73b0\u6709\u7b97\u6cd5\u867d\u7136\u5229\u7528\u5b50\u6a21\u6027\u5b9e\u73b0\u4e86\u4f4e\u9057\u61be\uff0c\u4f46\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u5229\u7528\u5b50\u6a21\u6027\u6765\u964d\u4f4e\u7ecf\u9a8c\u9057\u61be", "method": "\u63d0\u51faLazy Online Forward Algorithm (LOFA)\uff0c\u5728\u5b8c\u5168\u5f3a\u76d7\u53cd\u9988\u6a21\u578b\u4e0b\uff0c\u8fdb\u4e00\u6b65\u5229\u7528\u5f71\u54cd\u529b\u51fd\u6570\u7684\u5b50\u6a21\u6027\u7279\u6027\u8bbe\u8ba1\u7b97\u6cd5", "result": "\u5728\u771f\u5b9e\u793e\u4ea4\u7f51\u7edc\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLOFA\u5728\u7d2f\u79ef\u9057\u61be\u548c\u77ac\u65f6\u5956\u52b1\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u5f3a\u76d7\u7b97\u6cd5", "conclusion": "\u901a\u8fc7\u66f4\u5145\u5206\u5730\u5229\u7528\u5b50\u6a21\u6027\uff0cLOFA\u7b97\u6cd5\u5728\u5728\u7ebf\u5f71\u54cd\u529b\u6700\u5927\u5316\u95ee\u9898\u4e0a\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u6027\u80fd\u8868\u73b0"}}
{"id": "2601.01718", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01718", "abs": "https://arxiv.org/abs/2601.01718", "authors": ["YuanLab. ai", ":", "Shawn Wu", "Sean Wang", "Louie Li", "Darcy Chen", "Allen Wang", "Jiangang Luo", "Xudong Zhao", "Joseph Shen", "Gawain Ma", "Jasper Jia", "Marcus Mao", "Claire Wang", "Hunter He", "Carol Wang", "Zera Zhang", "Jason Wang", "Chonly Shen", "Leo Zhang", "Logan Chen", "Qasim Meng", "James Gong", "Danied Zhao", "Penn Zheng", "Owen Zhu", "Tong Yu"], "title": "Yuan3.0 Flash: An Open Multimodal Large Language Model for Enterprise Applications", "comment": null, "summary": "We introduce Yuan3.0 Flash, an open-source Mixture-of-Experts (MoE) MultiModal Large Language Model featuring 3.7B activated parameters and 40B total parameters, specifically designed to enhance performance on enterprise-oriented tasks while maintaining competitive capabilities on general-purpose tasks. To address the overthinking phenomenon commonly observed in Large Reasoning Models (LRMs), we propose Reflection-aware Adaptive Policy Optimization (RAPO), a novel RL training algorithm that effectively regulates overthinking behaviors. In enterprise-oriented tasks such as retrieval-augmented generation (RAG), complex table understanding, and summarization, Yuan3.0 Flash consistently achieves superior performance. Moreover, it also demonstrates strong reasoning capabilities in domains such as mathematics, science, etc., attaining accuracy comparable to frontier model while requiring only approximately 1/4 to 1/2 of the average tokens. Yuan3.0 Flash has been fully open-sourced to facilitate further research and real-world deployment: https://github.com/Yuan-lab-LLM/Yuan3.0.", "AI": {"tldr": "Yuan3.0 Flash\u662f\u4e00\u4e2a\u5f00\u6e90\u7684MoE\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5177\u67093.7B\u6fc0\u6d3b\u53c2\u6570\u548c40B\u603b\u53c2\u6570\uff0c\u4e13\u4e3a\u4f01\u4e1a\u4efb\u52a1\u4f18\u5316\uff0c\u540c\u65f6\u4fdd\u6301\u901a\u7528\u4efb\u52a1\u7ade\u4e89\u529b\uff0c\u5e76\u5f15\u5165RAPO\u7b97\u6cd5\u89e3\u51b3\u5927\u63a8\u7406\u6a21\u578b\u7684\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u63a8\u7406\u6a21\u578b\u4e2d\u7684\"\u8fc7\u5ea6\u601d\u8003\"\u73b0\u8c61\uff0c\u540c\u65f6\u5f00\u53d1\u4e00\u4e2a\u65e2\u80fd\u5904\u7406\u4f01\u4e1a\u5bfc\u5411\u4efb\u52a1\uff08\u5982RAG\u3001\u590d\u6742\u8868\u683c\u7406\u89e3\u3001\u6458\u8981\uff09\u53c8\u80fd\u5728\u901a\u7528\u4efb\u52a1\u4e0a\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u9ad8\u6548\u6a21\u578b\u3002", "method": "\u91c7\u7528\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u67b6\u6784\uff0c\u63d0\u51faReflection-aware Adaptive Policy Optimization (RAPO)\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7b97\u6cd5\u6765\u8c03\u8282\u8fc7\u5ea6\u601d\u8003\u884c\u4e3a\uff0c\u6a21\u578b\u5177\u67093.7B\u6fc0\u6d3b\u53c2\u6570\u548c40B\u603b\u53c2\u6570\u3002", "result": "\u5728\u4f01\u4e1a\u4efb\u52a1\uff08RAG\u3001\u8868\u683c\u7406\u89e3\u3001\u6458\u8981\uff09\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u6570\u5b66\u3001\u79d1\u5b66\u7b49\u63a8\u7406\u9886\u57df\u8fbe\u5230\u524d\u6cbf\u6a21\u578b\u76f8\u5f53\u7684\u51c6\u786e\u7387\uff0c\u540c\u65f6\u5e73\u5747token\u6d88\u8017\u4ec5\u4e3a1/4\u52301/2\u3002", "conclusion": "Yuan3.0 Flash\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7RAPO\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\uff0c\u5728\u4f01\u4e1a\u4efb\u52a1\u548c\u901a\u7528\u63a8\u7406\u4efb\u52a1\u4e0a\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u5df2\u5b8c\u5168\u5f00\u6e90\u4f9b\u7814\u7a76\u548c\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2601.01488", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01488", "abs": "https://arxiv.org/abs/2601.01488", "authors": ["Vanessa Toborek", "Sebastian M\u00fcller", "Christian Bauckhage"], "title": "Four Quadrants of Difficulty: A Simple Categorisation and its Limits", "comment": "prepared for ESANN 2026 submission", "summary": "Curriculum Learning (CL) aims to improve the outcome of model training by estimating the difficulty of samples and scheduling them accordingly. In NLP, difficulty is commonly approximated using task-agnostic linguistic heuristics or human intuition, implicitly assuming that these signals correlate with what neural models find difficult to learn. We propose a four-quadrant categorisation of difficulty signals -- human vs. model and task-agnostic vs. task-dependent -- and systematically analyse their interactions on a natural language understanding dataset. We find that task-agnostic features behave largely independently and that only task-dependent features align. These findings challenge common CL intuitions and highlight the need for lightweight, task-dependent difficulty estimators that better reflect model learning behaviour.", "AI": {"tldr": "\u8bba\u6587\u6311\u6218\u4e86\u8bfe\u7a0b\u5b66\u4e60\u4e2d\u4efb\u52a1\u65e0\u5173\u96be\u5ea6\u6307\u6807\u7684\u5047\u8bbe\uff0c\u63d0\u51fa\u9700\u8981\u8f7b\u91cf\u7ea7\u3001\u4efb\u52a1\u76f8\u5173\u7684\u96be\u5ea6\u4f30\u8ba1\u5668\u6765\u66f4\u597d\u5730\u53cd\u6620\u6a21\u578b\u5b66\u4e60\u884c\u4e3a", "motivation": "NLP\u4e2d\u8bfe\u7a0b\u5b66\u4e60\u901a\u5e38\u4f7f\u7528\u4efb\u52a1\u65e0\u5173\u7684\u8bed\u8a00\u5b66\u542f\u53d1\u5f0f\u6216\u4eba\u7c7b\u76f4\u89c9\u6765\u4f30\u8ba1\u6837\u672c\u96be\u5ea6\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u9690\u542b\u5047\u8bbe\u8fd9\u4e9b\u4fe1\u53f7\u4e0e\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5b9e\u9645\u5b66\u4e60\u56f0\u96be\u76f8\u5173\u3002\u4f5c\u8005\u60f3\u8981\u9a8c\u8bc1\u8fd9\u4e00\u5047\u8bbe\u5e76\u63a2\u7d22\u66f4\u597d\u7684\u96be\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u56db\u8c61\u9650\u5206\u7c7b\u6cd5\uff08\u4eba\u7c7bvs\u6a21\u578b\u3001\u4efb\u52a1\u65e0\u5173vs\u4efb\u52a1\u76f8\u5173\uff09\uff0c\u5728\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u6570\u636e\u96c6\u4e0a\u7cfb\u7edf\u5206\u6790\u4e86\u8fd9\u4e9b\u96be\u5ea6\u4fe1\u53f7\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "result": "\u53d1\u73b0\u4efb\u52a1\u65e0\u5173\u7279\u5f81\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u72ec\u7acb\u8fd0\u884c\uff0c\u53ea\u6709\u4efb\u52a1\u76f8\u5173\u7279\u5f81\u80fd\u591f\u5bf9\u9f50\u3002\u8fd9\u4e9b\u53d1\u73b0\u6311\u6218\u4e86\u5e38\u89c1\u7684\u8bfe\u7a0b\u5b66\u4e60\u76f4\u89c9\u3002", "conclusion": "\u9700\u8981\u5f00\u53d1\u8f7b\u91cf\u7ea7\u3001\u4efb\u52a1\u76f8\u5173\u7684\u96be\u5ea6\u4f30\u8ba1\u5668\uff0c\u4ee5\u66f4\u597d\u5730\u53cd\u6620\u6a21\u578b\u7684\u5b66\u4e60\u884c\u4e3a\uff0c\u800c\u4e0d\u662f\u4f9d\u8d56\u4efb\u52a1\u65e0\u5173\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002"}}
{"id": "2601.00942", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.00942", "abs": "https://arxiv.org/abs/2601.00942", "authors": ["Kabir Grover"], "title": "Reliability Under Randomness: An Empirical Analysis of Sparse and Dense Language Models Across Decoding Temperatures", "comment": null, "summary": "The increasing prevalence of sparse Mixture-of-Experts (MoE) architectures in large language models raises important questions regarding their reliability under stochastic decoding. While conditional computation enables substantial gains in computational efficiency, it remains unclear whether the interaction between sparse routing and temperature-based sampling compromises output stability relative to dense architectures. This work investigates whether conditional computation in MoE models amplifies decoding-induced randomness, leading to reduced reliability as temperature increases. We evaluate three representative models: OLMoE-7B (sparse base), Mixtral-8x7B (sparse instruction-tuned), and Qwen2.5-3B (dense instruction-tuned) on deterministic arithmetic reasoning tasks with objectively verifiable answers. Experiments span four decoding configurations, ranging from greedy decoding to T=1.0. Our evaluation encompasses accuracy, format compliance, output consistency across repeated generations, and confidence metrics, totaling 9,360 model generations. Results demonstrate that the sparse instruction-tuned model exhibits stability comparable to the dense instruction-tuned model across all decoding temperatures, while the sparse base model shows systematic degradation as temperature increases. These findings indicate that instruction tuning, rather than architectural sparsity, is the primary determinant of robustness to decoding randomness on deterministic tasks. We discuss the implications of these results for deploying sparse language models in reliability-critical applications, highlighting scenarios in which sparse architectures can be safely adopted without sacrificing output stability.", "AI": {"tldr": "\u7a00\u758fMoE\u67b6\u6784\u5728\u968f\u673a\u89e3\u7801\u4e0b\u7684\u53ef\u9760\u6027\u7814\u7a76\u8868\u660e\uff0c\u6307\u4ee4\u5fae\u8c03\u800c\u975e\u67b6\u6784\u7a00\u758f\u6027\u662f\u51b3\u5b9a\u6a21\u578b\u5728\u786e\u5b9a\u6027\u4efb\u52a1\u4e2d\u5bf9\u89e3\u7801\u968f\u673a\u6027\u9c81\u68d2\u6027\u7684\u4e3b\u8981\u56e0\u7d20\u3002", "motivation": "\u968f\u7740\u7a00\u758f\u6df7\u5408\u4e13\u5bb6(MoE)\u67b6\u6784\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u666e\u53ca\uff0c\u9700\u8981\u7814\u7a76\u5176\u5728\u968f\u673a\u89e3\u7801\u4e0b\u7684\u53ef\u9760\u6027\u3002\u867d\u7136\u6761\u4ef6\u8ba1\u7b97\u5e26\u6765\u4e86\u8ba1\u7b97\u6548\u7387\u7684\u63d0\u5347\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u7a00\u758f\u8def\u7531\u4e0e\u57fa\u4e8e\u6e29\u5ea6\u7684\u91c7\u6837\u4e4b\u95f4\u7684\u4ea4\u4e92\u662f\u5426\u4f1a\u635f\u5bb3\u8f93\u51fa\u7a33\u5b9a\u6027\u3002", "method": "\u8bc4\u4f30\u4e86\u4e09\u4e2a\u4ee3\u8868\u6027\u6a21\u578b\uff1aOLMoE-7B\uff08\u7a00\u758f\u57fa\u7840\u6a21\u578b\uff09\u3001Mixtral-8x7B\uff08\u7a00\u758f\u6307\u4ee4\u5fae\u8c03\u6a21\u578b\uff09\u548cQwen2.5-3B\uff08\u5bc6\u96c6\u6307\u4ee4\u5fae\u8c03\u6a21\u578b\uff09\u3002\u5728\u5177\u6709\u5ba2\u89c2\u53ef\u9a8c\u8bc1\u7b54\u6848\u7684\u786e\u5b9a\u6027\u7b97\u672f\u63a8\u7406\u4efb\u52a1\u4e0a\u8fdb\u884c\u6d4b\u8bd5\uff0c\u6db5\u76d6\u56db\u79cd\u89e3\u7801\u914d\u7f6e\uff08\u4ece\u8d2a\u5a6a\u89e3\u7801\u5230T=1.0\uff09\uff0c\u8bc4\u4f30\u51c6\u786e\u6027\u3001\u683c\u5f0f\u5408\u89c4\u6027\u3001\u91cd\u590d\u751f\u6210\u7684\u4e00\u81f4\u6027\u548c\u7f6e\u4fe1\u5ea6\u6307\u6807\uff0c\u603b\u8ba19,360\u6b21\u6a21\u578b\u751f\u6210\u3002", "result": "\u7a00\u758f\u6307\u4ee4\u5fae\u8c03\u6a21\u578b\u5728\u6240\u6709\u89e3\u7801\u6e29\u5ea6\u4e0b\u8868\u73b0\u51fa\u4e0e\u5bc6\u96c6\u6307\u4ee4\u5fae\u8c03\u6a21\u578b\u76f8\u5f53\u7684\u7a33\u5b9a\u6027\uff0c\u800c\u7a00\u758f\u57fa\u7840\u6a21\u578b\u968f\u7740\u6e29\u5ea6\u5347\u9ad8\u663e\u793a\u51fa\u7cfb\u7edf\u6027\u9000\u5316\u3002\u8fd9\u8868\u660e\u6307\u4ee4\u5fae\u8c03\uff0c\u800c\u975e\u67b6\u6784\u7a00\u758f\u6027\uff0c\u662f\u51b3\u5b9a\u6a21\u578b\u5728\u786e\u5b9a\u6027\u4efb\u52a1\u4e2d\u5bf9\u89e3\u7801\u968f\u673a\u6027\u9c81\u68d2\u6027\u7684\u4e3b\u8981\u56e0\u7d20\u3002", "conclusion": "\u6307\u4ee4\u5fae\u8c03\u662f\u786e\u4fdd\u7a00\u758f\u8bed\u8a00\u6a21\u578b\u5728\u53ef\u9760\u6027\u5173\u952e\u5e94\u7528\u4e2d\u7a33\u5b9a\u6027\u7684\u5173\u952e\u56e0\u7d20\u3002\u7814\u7a76\u7ed3\u679c\u4e3a\u5728\u4e0d\u9700\u8981\u727a\u7272\u8f93\u51fa\u7a33\u5b9a\u6027\u7684\u573a\u666f\u4e2d\u5b89\u5168\u91c7\u7528\u7a00\u758f\u67b6\u6784\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2601.01743", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01743", "abs": "https://arxiv.org/abs/2601.01743", "authors": ["Bin Xu"], "title": "AI Agent Systems: Architectures, Applications, and Evaluation", "comment": null, "summary": "AI agents -- systems that combine foundation models with reasoning, planning, memory, and tool use -- are rapidly becoming a practical interface between natural-language intent and real-world computation. This survey synthesizes the emerging landscape of AI agent architectures across: (i) deliberation and reasoning (e.g., chain-of-thought-style decomposition, self-reflection and verification, and constraint-aware decision making), (ii) planning and control (from reactive policies to hierarchical and multi-step planners), and (iii) tool calling and environment interaction (retrieval, code execution, APIs, and multimodal perception). We organize prior work into a unified taxonomy spanning agent components (policy/LLM core, memory, world models, planners, tool routers, and critics), orchestration patterns (single-agent vs.\\ multi-agent; centralized vs.\\ decentralized coordination), and deployment settings (offline analysis vs.\\ online interactive assistance; safety-critical vs.\\ open-ended tasks). We discuss key design trade-offs -- latency vs.\\ accuracy, autonomy vs.\\ controllability, and capability vs.\\ reliability -- and highlight how evaluation is complicated by non-determinism, long-horizon credit assignment, tool and environment variability, and hidden costs such as retries and context growth. Finally, we summarize measurement and benchmarking practices (task suites, human preference and utility metrics, success under constraints, robustness and security) and identify open challenges including verification and guardrails for tool actions, scalable memory and context management, interpretability of agent decisions, and reproducible evaluation under realistic workloads.", "AI": {"tldr": "\u672c\u6587\u5bf9AI\u667a\u80fd\u4f53\u67b6\u6784\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u7efc\u8ff0\uff0c\u6db5\u76d6\u63a8\u7406\u3001\u89c4\u5212\u3001\u5de5\u5177\u8c03\u7528\u7b49\u6838\u5fc3\u7ec4\u4ef6\uff0c\u63d0\u51fa\u4e86\u7edf\u4e00\u7684\u5206\u7c7b\u4f53\u7cfb\uff0c\u5e76\u8ba8\u8bba\u4e86\u8bbe\u8ba1\u6743\u8861\u3001\u8bc4\u4f30\u6311\u6218\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "AI\u667a\u80fd\u4f53\u7ed3\u5408\u57fa\u7840\u6a21\u578b\u4e0e\u63a8\u7406\u3001\u89c4\u5212\u3001\u8bb0\u5fc6\u548c\u5de5\u5177\u4f7f\u7528\u80fd\u529b\uff0c\u6b63\u6210\u4e3a\u8fde\u63a5\u81ea\u7136\u8bed\u8a00\u610f\u56fe\u4e0e\u73b0\u5b9e\u4e16\u754c\u8ba1\u7b97\u7684\u5173\u952e\u63a5\u53e3\u3002\u9700\u8981\u7cfb\u7edf\u6027\u5730\u68b3\u7406\u8fd9\u4e00\u5feb\u901f\u53d1\u5c55\u7684\u9886\u57df\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u6e05\u6670\u7684\u67b6\u6784\u6846\u67b6\u548c\u8bbe\u8ba1\u6307\u5bfc\u3002", "method": "\u91c7\u7528\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5c06\u73b0\u6709\u5de5\u4f5c\u7ec4\u7ec7\u6210\u7edf\u4e00\u7684\u5206\u7c7b\u4f53\u7cfb\uff1a1\uff09\u667a\u80fd\u4f53\u7ec4\u4ef6\uff08\u7b56\u7565/LLM\u6838\u5fc3\u3001\u8bb0\u5fc6\u3001\u4e16\u754c\u6a21\u578b\u3001\u89c4\u5212\u5668\u3001\u5de5\u5177\u8def\u7531\u5668\u3001\u6279\u8bc4\u5668\uff09\uff1b2\uff09\u7f16\u6392\u6a21\u5f0f\uff08\u5355\u667a\u80fd\u4f53vs.\u591a\u667a\u80fd\u4f53\uff0c\u96c6\u4e2d\u5f0fvs.\u53bb\u4e2d\u5fc3\u5316\u534f\u8c03\uff09\uff1b3\uff09\u90e8\u7f72\u8bbe\u7f6e\uff08\u79bb\u7ebf\u5206\u6790vs.\u5728\u7ebf\u4ea4\u4e92\uff0c\u5b89\u5168\u5173\u952evs.\u5f00\u653e\u4efb\u52a1\uff09\u3002", "result": "\u5efa\u7acb\u4e86\u5168\u9762\u7684AI\u667a\u80fd\u4f53\u67b6\u6784\u5206\u7c7b\u6846\u67b6\uff0c\u8bc6\u522b\u4e86\u5173\u952e\u8bbe\u8ba1\u6743\u8861\uff08\u5ef6\u8fdfvs.\u51c6\u786e\u6027\u3001\u81ea\u4e3b\u6027vs.\u53ef\u63a7\u6027\u3001\u80fd\u529bvs.\u53ef\u9760\u6027\uff09\uff0c\u603b\u7ed3\u4e86\u8bc4\u4f30\u5b9e\u8df5\u7684\u590d\u6742\u6027\uff08\u975e\u786e\u5b9a\u6027\u3001\u957f\u65f6\u7a0b\u4fe1\u7528\u5206\u914d\u3001\u5de5\u5177\u548c\u73af\u5883\u53d8\u5f02\u6027\u3001\u9690\u85cf\u6210\u672c\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u6d4b\u91cf\u548c\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\u3002", "conclusion": "AI\u667a\u80fd\u4f53\u67b6\u6784\u7814\u7a76\u9762\u4e34\u8bf8\u591a\u5f00\u653e\u6311\u6218\uff1a\u5de5\u5177\u52a8\u4f5c\u7684\u9a8c\u8bc1\u548c\u9632\u62a4\u3001\u53ef\u6269\u5c55\u7684\u8bb0\u5fc6\u548c\u4e0a\u4e0b\u6587\u7ba1\u7406\u3001\u667a\u80fd\u4f53\u51b3\u7b56\u7684\u53ef\u89e3\u91ca\u6027\u3001\u4ee5\u53ca\u771f\u5b9e\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u7684\u53ef\u91cd\u590d\u8bc4\u4f30\u3002\u9700\u8981\u5728\u8fd9\u4e9b\u65b9\u5411\u4e0a\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u63a8\u52a8\u9886\u57df\u53d1\u5c55\u3002"}}
{"id": "2601.01490", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01490", "abs": "https://arxiv.org/abs/2601.01490", "authors": ["Junichiro Niimi"], "title": "Distortion Instead of Hallucination: The Effect of Reasoning Under Strict Constraints", "comment": null, "summary": "With the widespread adoption of large language models (LLMs), hallucinations, which are non-factual fabrications in model outputs, have become serious concerns. Reasoning capabilities have received attention as a self-verification process to improve output reliability. However, the effect of reasoning within a closed system where LLMs cannot rely on external tools or knowledge has yet to be clarified. We therefore conduct experiments under strict constraints (recommending peer-reviewed journal articles in computer science) to examine the effect of reasoning across multiple models (GPT-5.2 and Gemini 3 Flash). Our results reveal a problematic trade-off between constraint compliance and factual accuracy. Non-reasoning models exhibit high constraint violation rates (66-75%) but maintain factual accuracy, while reasoning models reduce violations (13-26%) but systematically distort known facts to satisfy constraints and increase complete fabrication. This trade-off pattern is consistent across both models despite different architectures, indicating a fundamental limitation of reasoning. Furthermore, reasoning does not uniformly improve output authenticity: effects diverge by model, reflecting different allocations of the compliance-truthfulness trade-off. These findings challenge the assumption that reasoning universally improves reliability: reasoning models trade honest constraint violations for detection-resistant distortions.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u63a8\u7406\u6a21\u578b\u5728\u7ea6\u675f\u6761\u4ef6\u4e0b\u5b58\u5728\u77db\u76fe\uff1a\u867d\u7136\u51cf\u5c11\u4e86\u7ea6\u675f\u8fdd\u53cd\u7387\uff0c\u4f46\u4f1a\u7cfb\u7edf\u6027\u626d\u66f2\u4e8b\u5b9e\u5e76\u589e\u52a0\u5b8c\u5168\u634f\u9020\uff0c\u5f62\u6210\u7ea6\u675f\u5408\u89c4\u6027\u4e0e\u4e8b\u5b9e\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u6743\u8861", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5e7b\u89c9\u95ee\u9898\uff08\u6a21\u578b\u8f93\u51fa\u4e2d\u7684\u975e\u4e8b\u5b9e\u634f\u9020\uff09\u5df2\u6210\u4e3a\u4e25\u91cd\u5173\u5207\u3002\u63a8\u7406\u80fd\u529b\u4f5c\u4e3a\u63d0\u9ad8\u8f93\u51fa\u53ef\u9760\u6027\u7684\u81ea\u6211\u9a8c\u8bc1\u8fc7\u7a0b\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u5728\u5c01\u95ed\u7cfb\u7edf\u4e2d\uff08\u65e0\u6cd5\u4f9d\u8d56\u5916\u90e8\u5de5\u5177\u6216\u77e5\u8bc6\uff09\u63a8\u7406\u7684\u6548\u679c\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u5728\u4e25\u683c\u7ea6\u675f\u6761\u4ef6\u4e0b\uff08\u63a8\u8350\u8ba1\u7b97\u673a\u79d1\u5b66\u9886\u57df\u7684\u540c\u884c\u8bc4\u5ba1\u671f\u520a\u6587\u7ae0\uff09\u8fdb\u884c\u5b9e\u9a8c\uff0c\u4f7f\u7528\u591a\u4e2a\u6a21\u578b\uff08GPT-5.2\u548cGemini 3 Flash\uff09\u68c0\u9a8c\u63a8\u7406\u6548\u679c\uff0c\u6bd4\u8f83\u63a8\u7406\u6a21\u578b\u4e0e\u975e\u63a8\u7406\u6a21\u578b\u7684\u8868\u73b0\u3002", "result": "\u53d1\u73b0\u7ea6\u675f\u5408\u89c4\u6027\u4e0e\u4e8b\u5b9e\u51c6\u786e\u6027\u4e4b\u95f4\u5b58\u5728\u95ee\u9898\u6027\u6743\u8861\uff1a\u975e\u63a8\u7406\u6a21\u578b\u7ea6\u675f\u8fdd\u53cd\u7387\u9ad8\uff0866-75%\uff09\u4f46\u4fdd\u6301\u4e8b\u5b9e\u51c6\u786e\u6027\uff1b\u63a8\u7406\u6a21\u578b\u51cf\u5c11\u8fdd\u53cd\u7387\uff0813-26%\uff09\u4f46\u7cfb\u7edf\u6027\u626d\u66f2\u5df2\u77e5\u4e8b\u5b9e\u4ee5\u6ee1\u8db3\u7ea6\u675f\uff0c\u5e76\u589e\u52a0\u5b8c\u5168\u634f\u9020\u3002\u8fd9\u79cd\u6743\u8861\u6a21\u5f0f\u5728\u4e0d\u540c\u67b6\u6784\u6a21\u578b\u4e2d\u4e00\u81f4\u5b58\u5728\u3002", "conclusion": "\u63a8\u7406\u5e76\u4e0d\u666e\u904d\u63d0\u9ad8\u8f93\u51fa\u771f\u5b9e\u6027\uff0c\u6548\u679c\u56e0\u6a21\u578b\u800c\u5f02\uff0c\u53cd\u6620\u4e86\u5408\u89c4\u6027\u4e0e\u771f\u5b9e\u6027\u6743\u8861\u7684\u4e0d\u540c\u5206\u914d\u3002\u8fd9\u4e9b\u53d1\u73b0\u6311\u6218\u4e86\"\u63a8\u7406\u666e\u904d\u63d0\u9ad8\u53ef\u9760\u6027\"\u7684\u5047\u8bbe\uff1a\u63a8\u7406\u6a21\u578b\u4ee5\u8bda\u5b9e\u7684\u7ea6\u675f\u8fdd\u53cd\u6362\u53d6\u96be\u4ee5\u68c0\u6d4b\u7684\u626d\u66f2\u3002"}}
{"id": "2601.00965", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00965", "abs": "https://arxiv.org/abs/2601.00965", "authors": ["Tianshuo Yang", "Ryan Rabinowitz", "Terrance E. Boult", "Jugal Kalita"], "title": "Adapting Feature Attenuation to NLP", "comment": null, "summary": "Transformer classifiers such as BERT deliver impressive closed-set accuracy, yet they remain brittle when confronted with inputs from unseen categories--a common scenario for deployed NLP systems. We investigate Open-Set Recognition (OSR) for text by porting the feature attenuation hypothesis from computer vision to transformers and by benchmarking it against state-of-the-art baselines. Concretely, we adapt the COSTARR framework--originally designed for classification in computer vision--to two modest language models (BERT (base) and GPT-2) trained to label 176 arXiv subject areas. Alongside COSTARR, we evaluate Maximum Softmax Probability (MSP), MaxLogit, and the temperature-scaled free-energy score under the OOSA and AUOSCR metrics. Our results show (i) COSTARR extends to NLP without retraining but yields no statistically significant gain over MaxLogit or MSP, and (ii) free-energy lags behind all other scores in this high-class-count setting. The study highlights both the promise and the current limitations of transplanting vision-centric OSR ideas to language models, and points toward the need for larger backbones and task-tailored attenuation strategies.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u6587\u672c\u5f00\u653e\u96c6\u8bc6\u522b\uff08OSR\uff09\uff0c\u5c06\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u7279\u5f81\u8870\u51cf\u5047\u8bbe\u79fb\u690d\u5230Transformer\u6a21\u578b\uff0c\u5728arXiv\u5b66\u79d1\u5206\u7c7b\u4efb\u52a1\u4e0a\u8bc4\u4f30COSTARR\u7b49\u65b9\u6cd5\uff0c\u53d1\u73b0\u79fb\u690d\u53ef\u884c\u4f46\u6548\u679c\u63d0\u5347\u4e0d\u663e\u8457\u3002", "motivation": "Transformer\u5206\u7c7b\u5668\uff08\u5982BERT\uff09\u5728\u95ed\u96c6\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u9762\u5bf9\u672a\u89c1\u7c7b\u522b\u65f6\u8868\u73b0\u8106\u5f31\uff0c\u800c\u8fd9\u662f\u90e8\u7f72NLP\u7cfb\u7edf\u5e38\u89c1\u573a\u666f\u3002\u9700\u8981\u7814\u7a76\u6587\u672c\u5f00\u653e\u96c6\u8bc6\u522b\u65b9\u6cd5\u3002", "method": "\u5c06\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684COSTARR\u6846\u67b6\u9002\u914d\u5230\u4e24\u4e2a\u8bed\u8a00\u6a21\u578b\uff08BERT base\u548cGPT-2\uff09\uff0c\u5728176\u4e2aarXiv\u5b66\u79d1\u5206\u7c7b\u4efb\u52a1\u4e0a\u8bad\u7ec3\u3002\u540c\u65f6\u8bc4\u4f30MSP\u3001MaxLogit\u548c\u6e29\u5ea6\u7f29\u653e\u81ea\u7531\u80fd\u5206\u6570\uff0c\u4f7f\u7528OOSA\u548cAUOSCR\u6307\u6807\u3002", "result": "1) COSTARR\u53ef\u6269\u5c55\u5230NLP\u9886\u57df\u4e14\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\uff0c\u4f46\u4e0eMaxLogit\u6216MSP\u76f8\u6bd4\u65e0\u7edf\u8ba1\u5b66\u663e\u8457\u63d0\u5347\uff1b2) \u81ea\u7531\u80fd\u5206\u6570\u5728\u9ad8\u7c7b\u522b\u6570\u8bbe\u7f6e\u4e0b\u843d\u540e\u4e8e\u5176\u4ed6\u6240\u6709\u5206\u6570\u3002", "conclusion": "\u7814\u7a76\u663e\u793a\u4e86\u5c06\u89c6\u89c9\u4e2d\u5fc3OSR\u601d\u60f3\u79fb\u690d\u5230\u8bed\u8a00\u6a21\u578b\u7684\u6f5c\u529b\u4e0e\u5f53\u524d\u5c40\u9650\u6027\uff0c\u6307\u51fa\u9700\u8981\u66f4\u5927\u7684\u9aa8\u5e72\u7f51\u7edc\u548c\u4efb\u52a1\u5b9a\u5236\u7684\u8870\u51cf\u7b56\u7565\u3002"}}
{"id": "2601.01765", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.01765", "abs": "https://arxiv.org/abs/2601.01765", "authors": ["Yao Lu", "Shang Liu", "Hangan Zhou", "Wenji Fang", "Qijun Zhang", "Zhiyao Xie"], "title": "A New Benchmark for the Appropriate Evaluation of RTL Code Optimization", "comment": null, "summary": "The rapid progress of artificial intelligence increasingly relies on efficient integrated circuit (IC) design. Recent studies have explored the use of large language models (LLMs) for generating Register Transfer Level (RTL) code, but existing benchmarks mainly evaluate syntactic correctness rather than optimization quality in terms of power, performance, and area (PPA). This work introduces RTL-OPT, a benchmark for assessing the capability of LLMs in RTL optimization. RTL-OPT contains 36 handcrafted digital designs that cover diverse implementation categories including combinational logic, pipelined datapaths, finite state machines, and memory interfaces. Each task provides a pair of RTL codes, a suboptimal version and a human-optimized reference that reflects industry-proven optimization patterns not captured by conventional synthesis tools. Furthermore, RTL-OPT integrates an automated evaluation framework to verify functional correctness and quantify PPA improvements, enabling standardized and meaningful assessment of generative models for hardware design optimization.", "AI": {"tldr": "RTL-OPT\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728RTL\u4ee3\u7801\u4f18\u5316\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b36\u4e2a\u624b\u5de5\u8bbe\u8ba1\u7684\u6570\u5b57\u7535\u8def\uff0c\u6db5\u76d6\u591a\u79cd\u5b9e\u73b0\u7c7b\u522b\uff0c\u5e76\u63d0\u4f9b\u81ea\u52a8\u5316\u8bc4\u4f30\u6846\u67b6\u6765\u9a8c\u8bc1\u529f\u80fd\u6b63\u786e\u6027\u548c\u91cf\u5316PPA\u6539\u8fdb\u3002", "motivation": "\u5f53\u524dAI\u5728\u96c6\u6210\u7535\u8def\u8bbe\u8ba1\u4e2d\u7684\u5feb\u901f\u53d1\u5c55\u9700\u8981\u9ad8\u6548\u7684RTL\u4ee3\u7801\u751f\u6210\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u8bc4\u4f30\u8bed\u6cd5\u6b63\u786e\u6027\u800c\u975e\u4f18\u5316\u8d28\u91cf\uff08\u529f\u8017\u3001\u6027\u80fd\u3001\u9762\u79ef\uff09\u3002\u7f3a\u4e4f\u80fd\u591f\u8bc4\u4f30LLMs\u5728RTL\u4f18\u5316\u80fd\u529b\u7684\u6807\u51c6\u5316\u57fa\u51c6\u3002", "method": "\u521b\u5efaRTL-OPT\u57fa\u51c6\uff0c\u5305\u542b36\u4e2a\u624b\u5de5\u8bbe\u8ba1\u7684\u6570\u5b57\u7535\u8def\uff0c\u6db5\u76d6\u7ec4\u5408\u903b\u8f91\u3001\u6d41\u6c34\u7ebf\u6570\u636e\u8def\u5f84\u3001\u6709\u9650\u72b6\u6001\u673a\u548c\u5b58\u50a8\u5668\u63a5\u53e3\u7b49\u7c7b\u522b\u3002\u6bcf\u4e2a\u4efb\u52a1\u63d0\u4f9b\u6b21\u4f18\u7248\u672c\u548c\u4eba\u5de5\u4f18\u5316\u7684\u53c2\u8003\u7248\u672c\uff0c\u53cd\u6620\u5de5\u4e1a\u9a8c\u8bc1\u7684\u4f18\u5316\u6a21\u5f0f\u3002\u96c6\u6210\u81ea\u52a8\u5316\u8bc4\u4f30\u6846\u67b6\u9a8c\u8bc1\u529f\u80fd\u6b63\u786e\u6027\u5e76\u91cf\u5316PPA\u6539\u8fdb\u3002", "result": "RTL-OPT\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u80fd\u591f\u9a8c\u8bc1\u751f\u6210\u6a21\u578b\u5728\u786c\u4ef6\u8bbe\u8ba1\u4f18\u5316\u4e2d\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u91cf\u5316\u529f\u8017\u3001\u6027\u80fd\u548c\u9762\u79ef\u65b9\u9762\u7684\u6539\u8fdb\uff0c\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u5728\u4f18\u5316\u8d28\u91cf\u8bc4\u4f30\u65b9\u9762\u7684\u7a7a\u767d\u3002", "conclusion": "RTL-OPT\u57fa\u51c6\u4e3a\u8bc4\u4f30LLMs\u5728RTL\u4f18\u5316\u80fd\u529b\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u5de5\u5177\uff0c\u80fd\u591f\u66f4\u5168\u9762\u5730\u8bc4\u4f30\u751f\u6210\u6a21\u578b\u5728\u786c\u4ef6\u8bbe\u8ba1\u4f18\u5316\u4e2d\u7684\u5b9e\u9645\u6548\u679c\uff0c\u63a8\u52a8AI\u5728\u96c6\u6210\u7535\u8def\u8bbe\u8ba1\u9886\u57df\u7684\u5e94\u7528\u53d1\u5c55\u3002"}}
{"id": "2601.01498", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01498", "abs": "https://arxiv.org/abs/2601.01498", "authors": ["Bingguang Hao", "Zengzhuang Xu", "Yuntao Wen", "Xinyi Xu", "Yang Liu", "Tong Zhao", "Maolin Wang", "Long Chen", "Dong Wang", "Yicheng Chen", "Cunyin Peng", "Xiangyu Zhao", "Chenyi Zhuang", "Ji Zhang"], "title": "From Failure to Mastery: Generating Hard Samples for Tool-use Agents", "comment": null, "summary": "The advancement of LLM agents with tool-use capabilities requires diverse and complex training corpora. Existing data generation methods, which predominantly follow a paradigm of random sampling and shallow generation, often yield simple and homogeneous trajectories that fail to capture complex, implicit logical dependencies. To bridge this gap, we introduce HardGen, an automatic agentic pipeline designed to generate hard tool-use training samples with verifiable reasoning. Firstly, HardGen establishes a dynamic API Graph built upon agent failure cases, from which it samples to synthesize hard traces. Secondly, these traces serve as conditional priors to guide the instantiation of modular, abstract advanced tools, which are subsequently leveraged to formulate hard queries. Finally, the advanced tools and hard queries enable the generation of verifiable complex Chain-of-Thought (CoT), with a closed-loop evaluation feedback steering the continuous refinement of the process. Extensive evaluations demonstrate that a 4B parameter model trained with our curated dataset achieves superior performance compared to several leading open-source and closed-source competitors (e.g., GPT-5.2, Gemini-3-Pro and Claude-Opus-4.5). Our code, models, and dataset will be open-sourced to facilitate future research.", "AI": {"tldr": "HardGen\uff1a\u4e00\u79cd\u81ea\u52a8\u751f\u6210\u5177\u6709\u53ef\u9a8c\u8bc1\u63a8\u7406\u7684\u56f0\u96be\u5de5\u5177\u4f7f\u7528\u8bad\u7ec3\u6837\u672c\u7684\u4ee3\u7406\u7ba1\u9053\uff0c\u901a\u8fc7\u52a8\u6001API\u56fe\u3001\u6a21\u5757\u5316\u9ad8\u7ea7\u5de5\u5177\u548c\u95ed\u73af\u8bc4\u4f30\u53cd\u9988\u6765\u63d0\u5347LLM\u4ee3\u7406\u7684\u8bad\u7ec3\u6570\u636e\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u5de5\u5177\u4f7f\u7528\u8bad\u7ec3\u6570\u636e\u751f\u6210\u65b9\u6cd5\u4e3b\u8981\u91c7\u7528\u968f\u673a\u91c7\u6837\u548c\u6d45\u5c42\u751f\u6210\u8303\u5f0f\uff0c\u4ea7\u751f\u7684\u8f68\u8ff9\u7b80\u5355\u4e14\u540c\u8d28\u5316\uff0c\u65e0\u6cd5\u6355\u6349\u590d\u6742\u3001\u9690\u5f0f\u7684\u903b\u8f91\u4f9d\u8d56\u5173\u7cfb\uff0c\u9650\u5236\u4e86LLM\u4ee3\u7406\u80fd\u529b\u7684\u63d0\u5347\u3002", "method": "1. \u57fa\u4e8e\u4ee3\u7406\u5931\u8d25\u6848\u4f8b\u6784\u5efa\u52a8\u6001API\u56fe\uff0c\u4ece\u4e2d\u91c7\u6837\u5408\u6210\u56f0\u96be\u8f68\u8ff9\uff1b2. \u4ee5\u8fd9\u4e9b\u8f68\u8ff9\u4f5c\u4e3a\u6761\u4ef6\u5148\u9a8c\uff0c\u6307\u5bfc\u6a21\u5757\u5316\u62bd\u8c61\u9ad8\u7ea7\u5de5\u5177\u7684\u5b9e\u4f8b\u5316\uff0c\u7528\u4e8e\u751f\u6210\u56f0\u96be\u67e5\u8be2\uff1b3. \u5229\u7528\u9ad8\u7ea7\u5de5\u5177\u548c\u56f0\u96be\u67e5\u8be2\u751f\u6210\u53ef\u9a8c\u8bc1\u7684\u590d\u6742\u601d\u7ef4\u94fe\uff0c\u5e76\u901a\u8fc7\u95ed\u73af\u8bc4\u4f30\u53cd\u9988\u6301\u7eed\u4f18\u5316\u6574\u4e2a\u8fc7\u7a0b\u3002", "result": "\u4f7f\u7528HardGen\u751f\u6210\u7684\u6570\u636e\u96c6\u8bad\u7ec3\u76844B\u53c2\u6570\u6a21\u578b\uff0c\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u591a\u4e2a\u9886\u5148\u7684\u5f00\u6e90\u548c\u95ed\u6e90\u7ade\u4e89\u5bf9\u624b\uff08\u5982GPT-5.2\u3001Gemini-3-Pro\u548cClaude-Opus-4.5\uff09\u3002", "conclusion": "HardGen\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u5177\u6709\u53ef\u9a8c\u8bc1\u63a8\u7406\u7684\u56f0\u96be\u5de5\u5177\u4f7f\u7528\u8bad\u7ec3\u6837\u672c\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u751f\u6210\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u4ee3\u7406\u7684\u5de5\u5177\u4f7f\u7528\u80fd\u529b\uff0c\u5e76\u5c06\u5f00\u6e90\u4ee3\u7801\u3001\u6a21\u578b\u548c\u6570\u636e\u96c6\u4ee5\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2601.00968", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00968", "abs": "https://arxiv.org/abs/2601.00968", "authors": ["Longwei Wang", "Mohammad Navid Nayyem", "Abdullah Al Rakin", "KC Santosh", "Chaowei Zhang", "Yang Zhou"], "title": "Explainability-Guided Defense: Attribution-Aware Model Refinement Against Adversarial Data Attacks", "comment": "8pages,4 figures", "summary": "The growing reliance on deep learning models in safety-critical domains such as healthcare and autonomous navigation underscores the need for defenses that are both robust to adversarial perturbations and transparent in their decision-making. In this paper, we identify a connection between interpretability and robustness that can be directly leveraged during training. Specifically, we observe that spurious, unstable, or semantically irrelevant features identified through Local Interpretable Model-Agnostic Explanations (LIME) contribute disproportionately to adversarial vulnerability. Building on this insight, we introduce an attribution-guided refinement framework that transforms LIME from a passive diagnostic into an active training signal. Our method systematically suppresses spurious features using feature masking, sensitivity-aware regularization, and adversarial augmentation in a closed-loop refinement pipeline. This approach does not require additional datasets or model architectures and integrates seamlessly into standard adversarial training. Theoretically, we derive an attribution-aware lower bound on adversarial distortion that formalizes the link between explanation alignment and robustness. Empirical evaluations on CIFAR-10, CIFAR-10-C, and CIFAR-100 demonstrate substantial improvements in adversarial robustness and out-of-distribution generalization.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8eLIME\u89e3\u91ca\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u6291\u5236\u865a\u5047\u7279\u5f81\u6765\u540c\u65f6\u63d0\u5347\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5728\u533b\u7597\u548c\u81ea\u52a8\u9a7e\u9a76\u7b49\u5b89\u5168\u5173\u952e\u9886\u57df\uff0c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u9700\u8981\u540c\u65f6\u5177\u5907\u5bf9\u6297\u9c81\u68d2\u6027\u548c\u51b3\u7b56\u900f\u660e\u5ea6\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u901a\u8fc7LIME\u8bc6\u522b\u51fa\u7684\u865a\u5047\u3001\u4e0d\u7a33\u5b9a\u6216\u8bed\u4e49\u65e0\u5173\u7684\u7279\u5f81\u5bf9\u5bf9\u6297\u8106\u5f31\u6027\u6709\u4e0d\u6210\u6bd4\u4f8b\u7684\u8d21\u732e\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5c5e\u6027\u5f15\u5bfc\u7684\u7cbe\u70bc\u6846\u67b6\uff0c\u5c06LIME\u4ece\u88ab\u52a8\u8bca\u65ad\u5de5\u5177\u8f6c\u53d8\u4e3a\u4e3b\u52a8\u8bad\u7ec3\u4fe1\u53f7\u3002\u65b9\u6cd5\u5305\u62ec\u7279\u5f81\u63a9\u853d\u3001\u654f\u611f\u5ea6\u611f\u77e5\u6b63\u5219\u5316\u548c\u5bf9\u6297\u589e\u5f3a\uff0c\u5f62\u6210\u95ed\u73af\u7cbe\u70bc\u6d41\u7a0b\uff0c\u65e0\u9700\u989d\u5916\u6570\u636e\u96c6\u6216\u6a21\u578b\u67b6\u6784\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u6807\u51c6\u5bf9\u6297\u8bad\u7ec3\u4e2d\u3002", "result": "\u5728CIFAR-10\u3001CIFAR-10-C\u548cCIFAR-100\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u6297\u9c81\u68d2\u6027\u548c\u5206\u5e03\u5916\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u7406\u8bba\u63a8\u5bfc\u548c\u5b9e\u8bc1\u9a8c\u8bc1\uff0c\u5efa\u7acb\u4e86\u53ef\u89e3\u91ca\u6027\u5bf9\u9f50\u4e0e\u9c81\u68d2\u6027\u4e4b\u95f4\u7684\u6b63\u5f0f\u8054\u7cfb\uff0c\u4e3a\u5f00\u53d1\u540c\u65f6\u5177\u5907\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7684\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\u3002"}}
{"id": "2601.01774", "categories": ["cs.AI", "cs.CE", "math.NA"], "pdf": "https://arxiv.org/pdf/2601.01774", "abs": "https://arxiv.org/abs/2601.01774", "authors": ["Sai Varun Kodathala", "Rakesh Vunnam"], "title": "Can Large Language Models Solve Engineering Equations? A Systematic Comparison of Direct Prediction and Solver-Assisted Approaches", "comment": "14 pages", "summary": "Transcendental equations requiring iterative numerical solution pervade engineering practice, from fluid mechanics friction factor calculations to orbital position determination. We systematically evaluate whether Large Language Models can solve these equations through direct numerical prediction or whether a hybrid architecture combining LLM symbolic manipulation with classical iterative solvers proves more effective. Testing six state-of-the-art models (GPT-5.1, GPT-5.2, Gemini-3-Flash, Gemini-2.5-Lite, Claude-Sonnet-4.5, Claude-Opus-4.5) on 100 problems spanning seven engineering domains, we compare direct prediction against solver-assisted computation where LLMs formulate governing equations and provide initial conditions while Newton-Raphson iteration performs numerical solution. Direct prediction yields mean relative errors of 0.765 to 1.262 across models, while solver-assisted computation achieves 0.225 to 0.301, representing error reductions of 67.9% to 81.8%. Domain-specific analysis reveals dramatic improvements in Electronics (93.1%) due to exponential equation sensitivity, contrasted with modest gains in Fluid Mechanics (7.2%) where LLMs exhibit effective pattern recognition. These findings establish that contemporary LLMs excel at symbolic manipulation and domain knowledge retrieval but struggle with precision-critical iterative arithmetic, suggesting their optimal deployment as intelligent interfaces to classical numerical solvers rather than standalone computational engines.", "AI": {"tldr": "LLMs\u5728\u6c42\u89e3\u8d85\u8d8a\u65b9\u7a0b\u65f6\uff0c\u76f4\u63a5\u6570\u503c\u9884\u6d4b\u8bef\u5dee\u8f83\u5927\uff0c\u800c\u7ed3\u5408\u4f20\u7edf\u8fed\u4ee3\u6c42\u89e3\u5668\u7684\u6df7\u5408\u67b6\u6784\u80fd\u663e\u8457\u964d\u4f4e\u8bef\u5dee67.9%-81.8%\uff0c\u8868\u660eLLMs\u66f4\u9002\u5408\u4f5c\u4e3a\u4f20\u7edf\u6570\u503c\u6c42\u89e3\u5668\u7684\u667a\u80fd\u63a5\u53e3\u800c\u975e\u72ec\u7acb\u8ba1\u7b97\u5f15\u64ce\u3002", "motivation": "\u8d85\u8d8a\u65b9\u7a0b\u5728\u5de5\u7a0b\u5b9e\u8df5\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u9700\u8981\u8fed\u4ee3\u6570\u503c\u6c42\u89e3\u3002\u7814\u7a76\u65e8\u5728\u8bc4\u4f30LLMs\u80fd\u5426\u76f4\u63a5\u6c42\u89e3\u8fd9\u4e9b\u65b9\u7a0b\uff0c\u8fd8\u662f\u9700\u8981\u7ed3\u5408\u4f20\u7edf\u8fed\u4ee3\u6c42\u89e3\u5668\u7684\u6df7\u5408\u67b6\u6784\u66f4\u6709\u6548\u3002", "method": "\u6d4b\u8bd56\u4e2a\u6700\u5148\u8fdb\u7684LLM\u6a21\u578b\uff08GPT-5.1, GPT-5.2, Gemini-3-Flash, Gemini-2.5-Lite, Claude-Sonnet-4.5, Claude-Opus-4.5\uff09\u57287\u4e2a\u5de5\u7a0b\u9886\u57df\u7684100\u4e2a\u95ee\u9898\u4e0a\uff0c\u6bd4\u8f83\u76f4\u63a5\u9884\u6d4b\u4e0e\u6c42\u89e3\u5668\u8f85\u52a9\u8ba1\u7b97\uff08LLMs\u5236\u5b9a\u63a7\u5236\u65b9\u7a0b\u5e76\u63d0\u4f9b\u521d\u59cb\u6761\u4ef6\uff0c\u725b\u987f-\u62c9\u5f17\u68ee\u8fed\u4ee3\u6267\u884c\u6570\u503c\u6c42\u89e3\uff09\u3002", "result": "\u76f4\u63a5\u9884\u6d4b\u7684\u5e73\u5747\u76f8\u5bf9\u8bef\u5dee\u4e3a0.765-1.262\uff0c\u800c\u6c42\u89e3\u5668\u8f85\u52a9\u8ba1\u7b97\u4e3a0.225-0.301\uff0c\u8bef\u5dee\u964d\u4f4e\u4e8667.9%-81.8%\u3002\u7535\u5b50\u5b66\u9886\u57df\u6539\u8fdb\u6700\u5927\uff0893.1%\uff09\uff0c\u6d41\u4f53\u529b\u5b66\u6539\u8fdb\u6700\u5c0f\uff087.2%\uff09\u3002", "conclusion": "\u5f53\u4ee3LLMs\u64c5\u957f\u7b26\u53f7\u64cd\u4f5c\u548c\u9886\u57df\u77e5\u8bc6\u68c0\u7d22\uff0c\u4f46\u5728\u7cbe\u5ea6\u5173\u952e\u7684\u8fed\u4ee3\u7b97\u672f\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u5efa\u8bae\u5c06\u5176\u4f5c\u4e3a\u4f20\u7edf\u6570\u503c\u6c42\u89e3\u5668\u7684\u667a\u80fd\u63a5\u53e3\u800c\u975e\u72ec\u7acb\u8ba1\u7b97\u5f15\u64ce\u8fdb\u884c\u90e8\u7f72\u3002"}}
{"id": "2601.01530", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.01530", "abs": "https://arxiv.org/abs/2601.01530", "authors": ["Jing Ye", "Lu Xiang", "Yaping Zhang", "Chengqing Zong"], "title": "EmoHarbor: Evaluating Personalized Emotional Support by Simulating the User's Internal World", "comment": null, "summary": "Current evaluation paradigms for emotional support conversations tend to reward generic empathetic responses, yet they fail to assess whether the support is genuinely personalized to users' unique psychological profiles and contextual needs. We introduce EmoHarbor, an automated evaluation framework that adopts a User-as-a-Judge paradigm by simulating the user's inner world. EmoHarbor employs a Chain-of-Agent architecture that decomposes users' internal processes into three specialized roles, enabling agents to interact with supporters and complete assessments in a manner similar to human users. We instantiate this benchmark using 100 real-world user profiles that cover a diverse range of personality traits and situations, and define 10 evaluation dimensions of personalized support quality. Comprehensive evaluation of 20 advanced LLMs on EmoHarbor reveals a critical insight: while these models excel at generating empathetic responses, they consistently fail to tailor support to individual user contexts. This finding reframes the central challenge, shifting research focus from merely enhancing generic empathy to developing truly user-aware emotional support. EmoHarbor provides a reproducible and scalable framework to guide the development and evaluation of more nuanced and user-aware emotional support systems.", "AI": {"tldr": "EmoHarbor\uff1a\u57fa\u4e8e\u7528\u6237\u6a21\u62df\u7684\u4e2a\u6027\u5316\u60c5\u611f\u652f\u6301\u8bc4\u4f30\u6846\u67b6\uff0c\u53d1\u73b0\u5f53\u524dLLMs\u867d\u80fd\u751f\u6210\u5171\u60c5\u56de\u5e94\uff0c\u4f46\u7f3a\u4e4f\u9488\u5bf9\u7528\u6237\u5177\u4f53\u60c5\u5883\u7684\u4e2a\u6027\u5316\u652f\u6301\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u60c5\u611f\u652f\u6301\u5bf9\u8bdd\u8bc4\u4f30\u8303\u5f0f\u503e\u5411\u4e8e\u5956\u52b1\u901a\u7528\u7684\u5171\u60c5\u56de\u5e94\uff0c\u4f46\u65e0\u6cd5\u8bc4\u4f30\u652f\u6301\u662f\u5426\u771f\u6b63\u9488\u5bf9\u7528\u6237\u7684\u72ec\u7279\u5fc3\u7406\u7279\u5f81\u548c\u60c5\u5883\u9700\u6c42\u8fdb\u884c\u4e2a\u6027\u5316\u5b9a\u5236\u3002", "method": "\u63d0\u51faEmoHarbor\u8bc4\u4f30\u6846\u67b6\uff0c\u91c7\u7528\"\u7528\u6237\u5373\u88c1\u5224\"\u8303\u5f0f\uff0c\u901a\u8fc7\u6a21\u62df\u7528\u6237\u5185\u5fc3\u4e16\u754c\u8fdb\u884c\u8bc4\u4f30\u3002\u91c7\u7528Chain-of-Agent\u67b6\u6784\uff0c\u5c06\u7528\u6237\u5185\u90e8\u8fc7\u7a0b\u5206\u89e3\u4e3a\u4e09\u4e2a\u4e13\u95e8\u89d2\u8272\uff0c\u4f7f\u4ee3\u7406\u80fd\u591f\u4ee5\u7c7b\u4f3c\u4eba\u7c7b\u7528\u6237\u7684\u65b9\u5f0f\u4e0e\u652f\u6301\u8005\u4e92\u52a8\u5e76\u5b8c\u6210\u8bc4\u4f30\u3002\u4f7f\u7528100\u4e2a\u771f\u5b9e\u7528\u6237\u6863\u6848\uff08\u6db5\u76d6\u591a\u6837\u5316\u4eba\u683c\u7279\u8d28\u548c\u60c5\u5883\uff09\u548c10\u4e2a\u4e2a\u6027\u5316\u652f\u6301\u8d28\u91cf\u8bc4\u4f30\u7ef4\u5ea6\u6765\u5b9e\u4f8b\u5316\u8be5\u57fa\u51c6\u3002", "result": "\u5bf920\u4e2a\u5148\u8fdbLLMs\u7684\u7efc\u5408\u8bc4\u4f30\u63ed\u793a\u5173\u952e\u53d1\u73b0\uff1a\u867d\u7136\u8fd9\u4e9b\u6a21\u578b\u5728\u751f\u6210\u5171\u60c5\u56de\u5e94\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b83\u4eec\u59cb\u7ec8\u65e0\u6cd5\u6839\u636e\u4e2a\u4f53\u7528\u6237\u60c5\u5883\u5b9a\u5236\u652f\u6301\u3002\u8fd9\u4e00\u53d1\u73b0\u91cd\u65b0\u5b9a\u4e49\u4e86\u6838\u5fc3\u6311\u6218\uff0c\u5c06\u7814\u7a76\u91cd\u70b9\u4ece\u4ec5\u4ec5\u589e\u5f3a\u901a\u7528\u5171\u60c5\u8f6c\u5411\u5f00\u53d1\u771f\u6b63\u7528\u6237\u611f\u77e5\u7684\u60c5\u611f\u652f\u6301\u7cfb\u7edf\u3002", "conclusion": "EmoHarbor\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u590d\u73b0\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u80fd\u591f\u6307\u5bfc\u66f4\u7ec6\u81f4\u3001\u66f4\u5177\u7528\u6237\u611f\u77e5\u80fd\u529b\u7684\u60c5\u611f\u652f\u6301\u7cfb\u7edf\u7684\u5f00\u53d1\u548c\u8bc4\u4f30\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u5411\u771f\u6b63\u4e2a\u6027\u5316\u652f\u6301\u65b9\u5411\u53d1\u5c55\u3002"}}
{"id": "2601.00970", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00970", "abs": "https://arxiv.org/abs/2601.00970", "authors": ["Boris N. Oreshkin", "Mayank Jauhari", "Ravi Kiran Selvam", "Malcolm Wolff", "Wenhao Pan", "Shankar Ramasubramanian", "Kin G. Olivares", "Tatiana Konstantinova", "Andres Potapczynski", "Mengfei Cao", "Dmitry Efimov", "Michael W. Mahoney", "Andrew G. Wilson"], "title": "Zero-shot Forecasting by Simulation Alone", "comment": null, "summary": "Zero-shot time-series forecasting holds great promise, but is still in its infancy, hindered by limited and biased data corpora, leakage-prone evaluation, and privacy and licensing constraints. Motivated by these challenges, we propose the first practical univariate time series simulation pipeline which is simultaneously fast enough for on-the-fly data generation and enables notable zero-shot forecasting performance on M-Series and GiftEval benchmarks that capture trend/seasonality/intermittency patterns, typical of industrial forecasting applications across a variety of domains. Our simulator, which we call SarSim0 (SARIMA Simulator for Zero-Shot Forecasting), is based off of a seasonal autoregressive integrated moving average (SARIMA) model as its core data source. Due to instability in the autoregressive component, naive SARIMA simulation often leads to unusable paths. Instead, we follow a three-step procedure: (1) we sample well-behaved trajectories from its characteristic polynomial stability region; (2) we introduce a superposition scheme that combines multiple paths into rich multi-seasonality traces; and (3) we add rate-based heavy-tailed noise models to capture burstiness and intermittency alongside seasonalities and trends. SarSim0 is orders of magnitude faster than kernel-based generators, and it enables training on circa 1B unique purely simulated series, generated on the fly; after which well-established neural network backbones exhibit strong zero-shot generalization, surpassing strong statistical forecasters and recent foundation baselines, while operating under strict zero-shot protocol. Notably, on GiftEval we observe a \"student-beats-teacher\" effect: models trained on our simulations exceed the forecasting accuracy of the AutoARIMA generating processes.", "AI": {"tldr": "\u63d0\u51faSarSim0\u65f6\u95f4\u5e8f\u5217\u6a21\u62df\u5668\uff0c\u57fa\u4e8eSARIMA\u6a21\u578b\u5feb\u901f\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u7528\u4e8e\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0\u96f6\u6837\u672c\u9884\u6d4b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u7edf\u8ba1\u65b9\u6cd5\u548c\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u96f6\u6837\u672c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u9762\u4e34\u6570\u636e\u6709\u9650\u3001\u8bc4\u4f30\u6613\u6cc4\u6f0f\u3001\u9690\u79c1\u548c\u8bb8\u53ef\u9650\u5236\u7b49\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u5b9e\u7528\u7684\u6a21\u62df\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u57fa\u4e8eSARIMA\u6a21\u578b\u6784\u5efaSarSim0\u6a21\u62df\u5668\uff0c\u91c7\u7528\u4e09\u6b65\u6cd5\uff1a1)\u4ece\u7279\u5f81\u591a\u9879\u5f0f\u7a33\u5b9a\u533a\u57df\u91c7\u6837\u826f\u597d\u8f68\u8ff9\uff1b2)\u901a\u8fc7\u53e0\u52a0\u65b9\u6848\u7ec4\u5408\u591a\u4e2a\u8def\u5f84\u5f62\u6210\u591a\u5b63\u8282\u6027\u5e8f\u5217\uff1b3)\u6dfb\u52a0\u57fa\u4e8e\u901f\u7387\u7684\u91cd\u5c3e\u566a\u58f0\u6a21\u578b\u6355\u6349\u7a81\u53d1\u6027\u548c\u95f4\u6b47\u6027\u3002", "result": "SarSim0\u6bd4\u57fa\u4e8e\u6838\u7684\u751f\u6210\u5668\u5feb\u51e0\u4e2a\u6570\u91cf\u7ea7\uff0c\u80fd\u5728\u7ea610\u4ebf\u4e2a\u6a21\u62df\u5e8f\u5217\u4e0a\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\uff0c\u5728M-Series\u548cGiftEval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u7edf\u8ba1\u9884\u6d4b\u5668\u548c\u8fd1\u671f\u57fa\u7840\u6a21\u578b\uff0c\u5728GiftEval\u4e0a\u751a\u81f3\u8d85\u8fc7\u4e86\u751f\u6210\u6570\u636e\u7684AutoARIMA\u6a21\u578b\u3002", "conclusion": "SarSim0\u63d0\u4f9b\u4e86\u4e00\u79cd\u5feb\u901f\u5b9e\u7528\u7684\u65f6\u95f4\u5e8f\u5217\u6a21\u62df\u65b9\u6cd5\uff0c\u80fd\u591f\u751f\u6210\u4e30\u5bcc\u7684\u5408\u6210\u6570\u636e\u6765\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\uff0c\u5b9e\u73b0\u5f3a\u5927\u7684\u96f6\u6837\u672c\u9884\u6d4b\u6027\u80fd\uff0c\u4e3a\u5de5\u4e1a\u5e94\u7528\u4e2d\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2601.01802", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01802", "abs": "https://arxiv.org/abs/2601.01802", "authors": ["Qianjun Pan", "Junyi Wang", "Jie Zhou", "Yutao Yang", "Junsong Li", "Kaiyin Xu", "Yougen Zhou", "Yihan Li", "Jingyuan Zhao", "Qin Chen", "Ningning Zhou", "Kai Chen", "Liang He"], "title": "PsychEval: A Multi-Session and Multi-Therapy Benchmark for High-Realism and Comprehensive AI Psychological Counselor", "comment": null, "summary": "To develop a reliable AI for psychological assessment, we introduce \\texttt{PsychEval}, a multi-session, multi-therapy, and highly realistic benchmark designed to address three key challenges: \\textbf{1) Can we train a highly realistic AI counselor?} Realistic counseling is a longitudinal task requiring sustained memory and dynamic goal tracking. We propose a multi-session benchmark (spanning 6-10 sessions across three distinct stages) that demands critical capabilities such as memory continuity, adaptive reasoning, and longitudinal planning. The dataset is annotated with extensive professional skills, comprising over 677 meta-skills and 4577 atomic skills. \\textbf{2) How to train a multi-therapy AI counselor?} While existing models often focus on a single therapy, complex cases frequently require flexible strategies among various therapies. We construct a diverse dataset covering five therapeutic modalities (Psychodynamic, Behaviorism, CBT, Humanistic Existentialist, and Postmodernist) alongside an integrative therapy with a unified three-stage clinical framework across six core psychological topics. \\textbf{3) How to systematically evaluate an AI counselor?} We establish a holistic evaluation framework with 18 therapy-specific and therapy-shared metrics across Client-Level and Counselor-Level dimensions. To support this, we also construct over 2,000 diverse client profiles. Extensive experimental analysis fully validates the superior quality and clinical fidelity of our dataset. Crucially, \\texttt{PsychEval} transcends static benchmarking to serve as a high-fidelity reinforcement learning environment that enables the self-evolutionary training of clinically responsible and adaptive AI counselors.", "AI": {"tldr": "PsychEval\uff1a\u4e00\u4e2a\u7528\u4e8e\u5fc3\u7406\u8bc4\u4f30AI\u7684\u591a\u4f1a\u8bdd\u3001\u591a\u7597\u6cd5\u3001\u9ad8\u771f\u5b9e\u5ea6\u57fa\u51c6\uff0c\u5305\u542b\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u6846\u67b6\u548c\u5f3a\u5316\u5b66\u4e60\u73af\u5883\uff0c\u65e8\u5728\u8bad\u7ec3\u548c\u8bc4\u4f30\u4e34\u5e8a\u8d1f\u8d23\u4e14\u81ea\u9002\u5e94\u7684AI\u54a8\u8be2\u5e08\u3002", "motivation": "\u5f00\u53d1\u53ef\u9760\u7684\u5fc3\u7406\u8bc4\u4f30AI\u9762\u4e34\u4e09\u4e2a\u5173\u952e\u6311\u6218\uff1a1\uff09\u5982\u4f55\u8bad\u7ec3\u9ad8\u5ea6\u771f\u5b9e\u7684AI\u54a8\u8be2\u5e08\uff08\u9700\u8981\u6301\u7eed\u8bb0\u5fc6\u548c\u52a8\u6001\u76ee\u6807\u8ddf\u8e2a\uff09\uff1b2\uff09\u5982\u4f55\u8bad\u7ec3\u591a\u7597\u6cd5AI\u54a8\u8be2\u5e08\uff08\u590d\u6742\u6848\u4f8b\u9700\u8981\u7075\u6d3b\u5207\u6362\u4e0d\u540c\u7597\u6cd5\uff09\uff1b3\uff09\u5982\u4f55\u7cfb\u7edf\u8bc4\u4f30AI\u54a8\u8be2\u5e08\u3002\u73b0\u6709\u6a21\u578b\u901a\u5e38\u53ea\u5173\u6ce8\u5355\u4e00\u7597\u6cd5\uff0c\u7f3a\u4e4f\u771f\u5b9e\u7684\u591a\u4f1a\u8bdd\u573a\u666f\u548c\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u6784\u5efa\u591a\u4f1a\u8bdd\u57fa\u51c6\uff086-10\u4e2a\u4f1a\u8bdd\uff0c\u5206\u4e09\u4e2a\u9636\u6bb5\uff09\uff0c\u5305\u542b677\u4e2a\u5143\u6280\u80fd\u548c4577\u4e2a\u539f\u5b50\u6280\u80fd\u7684\u4e13\u4e1a\u6807\u6ce8\uff1b\u521b\u5efa\u6db5\u76d6\u4e94\u79cd\u6cbb\u7597\u6a21\u5f0f\uff08\u5fc3\u7406\u52a8\u529b\u5b66\u3001\u884c\u4e3a\u4e3b\u4e49\u3001CBT\u3001\u4eba\u672c\u5b58\u5728\u4e3b\u4e49\u3001\u540e\u73b0\u4ee3\u4e3b\u4e49\uff09\u548c\u6574\u5408\u7597\u6cd5\u7684\u591a\u6837\u5316\u6570\u636e\u96c6\uff1b\u5efa\u7acb\u5305\u542b18\u4e2a\u6cbb\u7597\u7279\u5b9a\u548c\u5171\u4eab\u6307\u6807\u7684\u5168\u9762\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u6784\u5efa2000\u591a\u4e2a\u591a\u6837\u5316\u5ba2\u6237\u6863\u6848\u3002", "result": "\u5b9e\u9a8c\u5206\u6790\u5145\u5206\u9a8c\u8bc1\u4e86\u6570\u636e\u96c6\u7684\u9ad8\u8d28\u91cf\u548c\u4e34\u5e8a\u4fdd\u771f\u5ea6\u3002PsychEval\u8d85\u8d8a\u4e86\u9759\u6001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53ef\u4f5c\u4e3a\u9ad8\u4fdd\u771f\u5f3a\u5316\u5b66\u4e60\u73af\u5883\uff0c\u652f\u6301\u4e34\u5e8a\u8d1f\u8d23\u4e14\u81ea\u9002\u5e94\u7684AI\u54a8\u8be2\u5e08\u7684\u81ea\u6211\u8fdb\u5316\u8bad\u7ec3\u3002", "conclusion": "PsychEval\u4e3a\u89e3\u51b3\u5fc3\u7406\u8bc4\u4f30AI\u7684\u4e09\u4e2a\u5173\u952e\u6311\u6218\u63d0\u4f9b\u4e86\u5168\u9762\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u591a\u4f1a\u8bdd\u3001\u591a\u7597\u6cd5\u7684\u9ad8\u771f\u5b9e\u5ea6\u57fa\u51c6\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u4e3a\u8bad\u7ec3\u548c\u8bc4\u4f30\u4e34\u5e8a\u8d1f\u8d23\u7684AI\u54a8\u8be2\u5e08\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u53ef\u4f5c\u4e3a\u5f3a\u5316\u5b66\u4e60\u73af\u5883\u4fc3\u8fdbAI\u54a8\u8be2\u5e08\u7684\u81ea\u6211\u8fdb\u5316\u3002"}}
{"id": "2601.01543", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01543", "abs": "https://arxiv.org/abs/2601.01543", "authors": ["Praveenkumar Katwe", "RakeshChandra Balabantaray", "Kaliprasad Vittala"], "title": "Bridging the Data Gap: Creating a Hindi Text Summarization Dataset from the English XSUM", "comment": "Book chapter for River publications", "summary": "Current advancements in Natural Language Processing (NLP) have largely favored resource-rich languages, leaving a significant gap in high-quality datasets for low-resource languages like Hindi. This scarcity is particularly evident in text summarization, where the development of robust models is hindered by a lack of diverse, specialized corpora.\n  To address this disparity, this study introduces a cost-effective, automated framework for creating a comprehensive Hindi text summarization dataset. By leveraging the English Extreme Summarization (XSUM) dataset as a source, we employ advanced translation and linguistic adaptation techniques. To ensure high fidelity and contextual relevance, we utilize the Crosslingual Optimized Metric for Evaluation of Translation (COMET) for validation, supplemented by the selective use of Large Language Models (LLMs) for curation.\n  The resulting dataset provides a diverse, multi-thematic resource that mirrors the complexity of the original XSUM corpus. This initiative not only provides a direct tool for Hindi NLP research but also offers a scalable methodology for democratizing NLP in other underserved languages. By reducing the costs associated with dataset creation, this work fosters the development of more nuanced, culturally relevant models in computational linguistics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u81ea\u52a8\u5316\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u521b\u5efa\u5370\u5730\u8bed\u6587\u672c\u6458\u8981\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u7ffb\u8bd1\u548c\u4f18\u5316\u82f1\u8bedXSUM\u6570\u636e\u96c6\uff0c\u5e76\u4f7f\u7528COMET\u548cLLM\u8fdb\u884c\u9a8c\u8bc1\u548c\u7b5b\u9009\u3002", "motivation": "\u5f53\u524dNLP\u8fdb\u5c55\u4e3b\u8981\u504f\u5411\u8d44\u6e90\u4e30\u5bcc\u7684\u8bed\u8a00\uff0c\u5370\u5730\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u6587\u672c\u6458\u8981\u6570\u636e\u96c6\uff0c\u8fd9\u963b\u788d\u4e86\u9c81\u68d2\u6a21\u578b\u7684\u53d1\u5c55\u3002", "method": "\u5229\u7528\u82f1\u8bedXSUM\u6570\u636e\u96c6\u4f5c\u4e3a\u6e90\uff0c\u91c7\u7528\u5148\u8fdb\u7684\u7ffb\u8bd1\u548c\u8bed\u8a00\u9002\u5e94\u6280\u672f\uff0c\u4f7f\u7528COMET\u8fdb\u884c\u7ffb\u8bd1\u8d28\u91cf\u9a8c\u8bc1\uff0c\u5e76\u9009\u62e9\u6027\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6570\u636e\u7b5b\u9009\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u591a\u6837\u5316\u3001\u591a\u4e3b\u9898\u7684\u5370\u5730\u8bed\u6587\u672c\u6458\u8981\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u53cd\u6620\u4e86\u539f\u59cbXSUM\u8bed\u6599\u5e93\u7684\u590d\u6742\u6027\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e0d\u4ec5\u4e3a\u5370\u5730\u8bedNLP\u7814\u7a76\u63d0\u4f9b\u4e86\u76f4\u63a5\u5de5\u5177\uff0c\u8fd8\u4e3a\u5176\u4ed6\u8d44\u6e90\u4e0d\u8db3\u8bed\u8a00\u7684NLP\u6c11\u4e3b\u5316\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u8bba\uff0c\u901a\u8fc7\u964d\u4f4e\u6570\u636e\u96c6\u521b\u5efa\u6210\u672c\uff0c\u4fc3\u8fdb\u4e86\u8ba1\u7b97\u8bed\u8a00\u5b66\u4e2d\u66f4\u7ec6\u81f4\u3001\u6587\u5316\u76f8\u5173\u6a21\u578b\u7684\u53d1\u5c55\u3002"}}
{"id": "2601.01003", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01003", "abs": "https://arxiv.org/abs/2601.01003", "authors": ["Amin Abyaneh", "Charlotte Morissette", "Mohamad H. Danesh", "Anas El Houssaini", "David Meger", "Gregory Dudek", "Hsiu-Chin Lin"], "title": "Contractive Diffusion Policies: Robust Action Diffusion via Contractive Score-Based Sampling with Differential Equations", "comment": "Under review at ICLR 2026", "summary": "Diffusion policies have emerged as powerful generative models for offline policy learning, whose sampling process can be rigorously characterized by a score function guiding a Stochastic Differential Equation (SDE). However, the same score-based SDE modeling that grants diffusion policies the flexibility to learn diverse behavior also incurs solver and score-matching errors, large data requirements, and inconsistencies in action generation. While less critical in image generation, these inaccuracies compound and lead to failure in continuous control settings. We introduce Contractive Diffusion Policies (CDPs) to induce contractive behavior in the diffusion sampling dynamics. Contraction pulls nearby flows closer to enhance robustness against solver and score-matching errors while reducing unwanted action variance. We develop an in-depth theoretical analysis along with a practical implementation recipe to incorporate CDPs into existing diffusion policy architectures with minimal modification and computational cost. We evaluate CDPs for offline learning by conducting extensive experiments in simulation and real-world settings. Across benchmarks, CDPs often outperform baseline policies, with pronounced benefits under data scarcity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u6536\u7f29\u6269\u6563\u7b56\u7565\uff08CDPs\uff09\uff0c\u901a\u8fc7\u5728\u6269\u6563\u91c7\u6837\u52a8\u529b\u5b66\u4e2d\u5f15\u5165\u6536\u7f29\u884c\u4e3a\u6765\u589e\u5f3a\u6269\u6563\u7b56\u7565\u7684\u9c81\u68d2\u6027\uff0c\u51cf\u5c11\u6c42\u89e3\u5668\u548c\u5206\u6570\u5339\u914d\u8bef\u5dee\u7684\u5f71\u54cd\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6570\u636e\u7a00\u7f3a\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u573a\u666f\u3002", "motivation": "\u6269\u6563\u7b56\u7565\u867d\u7136\u4f5c\u4e3a\u79bb\u7ebf\u7b56\u7565\u5b66\u4e60\u7684\u5f3a\u5927\u751f\u6210\u6a21\u578b\uff0c\u4f46\u5176\u57fa\u4e8e\u5206\u6570\u7684SDE\u5efa\u6a21\u5b58\u5728\u6c42\u89e3\u5668\u548c\u5206\u6570\u5339\u914d\u8bef\u5dee\u3001\u5927\u6570\u636e\u9700\u6c42\u4ee5\u53ca\u52a8\u4f5c\u751f\u6210\u4e0d\u4e00\u81f4\u7b49\u95ee\u9898\u3002\u5728\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\uff0c\u8fd9\u4e9b\u4e0d\u51c6\u786e\u6027\u4f1a\u7d2f\u79ef\u5e76\u5bfc\u81f4\u5931\u8d25\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u7a00\u7f3a\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u63d0\u51fa\u6536\u7f29\u6269\u6563\u7b56\u7565\uff08CDPs\uff09\uff0c\u5728\u6269\u6563\u91c7\u6837\u52a8\u529b\u5b66\u4e2d\u5f15\u5165\u6536\u7f29\u884c\u4e3a\u3002\u6536\u7f29\u5c06\u76f8\u90bb\u7684\u6d41\u62c9\u8fd1\uff0c\u589e\u5f3a\u5bf9\u6c42\u89e3\u5668\u548c\u5206\u6570\u5339\u914d\u8bef\u5dee\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u52a8\u4f5c\u65b9\u5dee\u3002\u63d0\u4f9b\u4e86\u7406\u8bba\u5206\u6790\u6846\u67b6\u548c\u5b9e\u9645\u5b9e\u73b0\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6700\u5c0f\u5316\u4fee\u6539\u548c\u8ba1\u7b97\u6210\u672c\u5730\u96c6\u6210\u5230\u73b0\u6709\u6269\u6563\u7b56\u7565\u67b6\u6784\u4e2d\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u79bb\u7ebf\u5b66\u4e60\u5b9e\u9a8c\u3002\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCDPs\u901a\u5e38\u4f18\u4e8e\u57fa\u7ebf\u7b56\u7565\uff0c\u5728\u6570\u636e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u8868\u73b0\u51fa\u66f4\u660e\u663e\u7684\u4f18\u52bf\u3002", "conclusion": "CDPs\u901a\u8fc7\u5f15\u5165\u6536\u7f29\u884c\u4e3a\u6709\u6548\u89e3\u51b3\u4e86\u6269\u6563\u7b56\u7565\u5728\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\u7684\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u7b56\u7565\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u4e3a\u6269\u6563\u7b56\u7565\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u6539\u8fdb\u3002"}}
{"id": "2601.01816", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01816", "abs": "https://arxiv.org/abs/2601.01816", "authors": ["Chris Duffey"], "title": "Admissibility Alignment", "comment": "24 pages, 2 figures, 2 tables.. Decision-theoretic alignment under uncertainty", "summary": "This paper introduces Admissibility Alignment: a reframing of AI alignment as a property of admissible action and decision selection over distributions of outcomes under uncertainty, evaluated through the behavior of candidate policies. We present MAP-AI (Monte Carlo Alignment for Policy) as a canonical system architecture for operationalizing admissibility alignment, formalizing alignment as a probabilistic, decision-theoretic property rather than a static or binary condition.\n  MAP-AI, a new control-plane system architecture for aligned decision-making under uncertainty, enforces alignment through Monte Carlo estimation of outcome distributions and admissibility-controlled policy selection rather than static model-level constraints. The framework evaluates decision policies across ensembles of plausible futures, explicitly modeling uncertainty, intervention effects, value ambiguity, and governance constraints. Alignment is assessed through distributional properties including expected utility, variance, tail risk, and probability of misalignment rather than accuracy or ranking performance. This approach distinguishes probabilistic prediction from decision reasoning under uncertainty and provides an executable methodology for evaluating trust and alignment in enterprise and institutional AI systems. The result is a practical foundation for governing AI systems whose impact is determined not by individual forecasts, but by policy behavior across distributions and tail events. Finally, we show how distributional alignment evaluation can be integrated into decision-making itself, yielding an admissibility-controlled action selection mechanism that alters policy behavior under uncertainty without retraining or modifying underlying models.", "AI": {"tldr": "\u63d0\u51fa\"\u53ef\u5bb9\u8bb8\u5bf9\u9f50\"\u6846\u67b6\uff0c\u5c06AI\u5bf9\u9f50\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u5bf9\u7ed3\u679c\u5206\u5e03\u7684\u53ef\u5bb9\u8bb8\u884c\u52a8\u548c\u51b3\u7b56\u9009\u62e9\u5c5e\u6027\uff0c\u5e76\u901a\u8fc7MAP-AI\u7cfb\u7edf\u67b6\u6784\u5b9e\u73b0\u6982\u7387\u5316\u3001\u51b3\u7b56\u8bba\u7684\u5bf9\u9f50\u8bc4\u4f30\u3002", "motivation": "\u4f20\u7edfAI\u5bf9\u9f50\u65b9\u6cd5\u901a\u5e38\u5c06\u5bf9\u9f50\u89c6\u4e3a\u9759\u6001\u6216\u4e8c\u5143\u6761\u4ef6\uff0c\u7f3a\u4e4f\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u5bf9\u51b3\u7b56\u653f\u7b56\u884c\u4e3a\u7684\u5206\u5e03\u6027\u8bc4\u4f30\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u3001\u5e72\u9884\u6548\u5e94\u3001\u4ef7\u503c\u6a21\u7cca\u6027\u548c\u6cbb\u7406\u7ea6\u675f\u7684\u5b9e\u7528\u5bf9\u9f50\u6846\u67b6\u3002", "method": "\u63d0\u51faMAP-AI\uff08\u8499\u7279\u5361\u6d1b\u5bf9\u9f50\u653f\u7b56\uff09\u7cfb\u7edf\u67b6\u6784\uff0c\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u4f30\u8ba1\u7ed3\u679c\u5206\u5e03\u548c\u53ef\u5bb9\u8bb8\u63a7\u5236\u653f\u7b56\u9009\u62e9\u6765\u5b9e\u65bd\u5bf9\u9f50\u3002\u8be5\u6846\u67b6\u5728\u53ef\u4fe1\u672a\u6765\u96c6\u5408\u4e2d\u8bc4\u4f30\u51b3\u7b56\u653f\u7b56\uff0c\u660e\u786e\u5efa\u6a21\u4e0d\u786e\u5b9a\u6027\u3001\u5e72\u9884\u6548\u5e94\u3001\u4ef7\u503c\u6a21\u7cca\u6027\u548c\u6cbb\u7406\u7ea6\u675f\u3002", "result": "\u5efa\u7acb\u4e86\u57fa\u4e8e\u5206\u5e03\u5c5e\u6027\uff08\u5305\u62ec\u671f\u671b\u6548\u7528\u3001\u65b9\u5dee\u3001\u5c3e\u90e8\u98ce\u9669\u548c\u4e0d\u5bf9\u9f50\u6982\u7387\uff09\u7684\u5bf9\u9f50\u8bc4\u4f30\u65b9\u6cd5\uff0c\u800c\u975e\u57fa\u4e8e\u51c6\u786e\u6027\u6216\u6392\u540d\u6027\u80fd\u3002\u63d0\u4f9b\u4e86\u53ef\u6267\u884c\u7684\u65b9\u6cd5\u8bba\u6765\u8bc4\u4f30\u4f01\u4e1a\u7ea7AI\u7cfb\u7edf\u7684\u4fe1\u4efb\u548c\u5bf9\u9f50\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6cbb\u7406AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u57fa\u7840\uff0c\u5176\u5f71\u54cd\u7531\u653f\u7b56\u884c\u4e3a\u5728\u5206\u5e03\u548c\u5c3e\u90e8\u4e8b\u4ef6\u4e2d\u7684\u8868\u73b0\u51b3\u5b9a\u800c\u975e\u5355\u4e2a\u9884\u6d4b\u3002\u5c55\u793a\u4e86\u5982\u4f55\u5c06\u5bf9\u9f50\u8bc4\u4f30\u6574\u5408\u5230\u51b3\u7b56\u8fc7\u7a0b\u4e2d\uff0c\u5b9e\u73b0\u4e0d\u91cd\u65b0\u8bad\u7ec3\u6216\u4fee\u6539\u5e95\u5c42\u6a21\u578b\u7684\u53ef\u5bb9\u8bb8\u63a7\u5236\u884c\u52a8\u9009\u62e9\u3002"}}
{"id": "2601.01552", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01552", "abs": "https://arxiv.org/abs/2601.01552", "authors": ["Shreyas N. Samaga", "Gilberto Gonzalez Arroyo", "Tamal K. Dey"], "title": "HalluZig: Hallucination Detection using Zigzag Persistence", "comment": null, "summary": "The factual reliability of Large Language Models (LLMs) remains a critical barrier to their adoption in high-stakes domains due to their propensity to hallucinate. Current detection methods often rely on surface-level signals from the model's output, overlooking the failures that occur within the model's internal reasoning process. In this paper, we introduce a new paradigm for hallucination detection by analyzing the dynamic topology of the evolution of model's layer-wise attention. We model the sequence of attention matrices as a zigzag graph filtration and use zigzag persistence, a tool from Topological Data Analysis, to extract a topological signature. Our core hypothesis is that factual and hallucinated generations exhibit distinct topological signatures. We validate our framework, HalluZig, on multiple benchmarks, demonstrating that it outperforms strong baselines. Furthermore, our analysis reveals that these topological signatures are generalizable across different models and hallucination detection is possible only using structural signatures from partial network depth.", "AI": {"tldr": "HalluZig\uff1a\u5229\u7528\u62d3\u6251\u6570\u636e\u5206\u6790\uff08TDA\uff09\u4e2d\u7684zigzag\u6301\u4e45\u6027\u5206\u6790LLM\u6ce8\u610f\u529b\u52a8\u6001\u62d3\u6251\uff0c\u68c0\u6d4b\u5e7b\u89c9\u7684\u65b0\u65b9\u6cd5", "motivation": "LLM\u7684\u4e8b\u5b9e\u53ef\u9760\u6027\u662f\u5176\u5728\u9ad8\u98ce\u9669\u9886\u57df\u5e94\u7528\u7684\u5173\u952e\u969c\u788d\uff0c\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u6a21\u578b\u8f93\u51fa\u7684\u8868\u9762\u4fe1\u53f7\uff0c\u5ffd\u7565\u4e86\u5185\u90e8\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u5931\u8d25", "method": "\u5c06\u6a21\u578b\u5c42\u95f4\u6ce8\u610f\u529b\u77e9\u9635\u5e8f\u5217\u5efa\u6a21\u4e3azigzag\u56fe\u8fc7\u6ee4\uff0c\u4f7f\u7528\u62d3\u6251\u6570\u636e\u5206\u6790\u4e2d\u7684zigzag\u6301\u4e45\u6027\u63d0\u53d6\u62d3\u6251\u7279\u5f81\uff0c\u5047\u8bbe\u4e8b\u5b9e\u548c\u5e7b\u89c9\u751f\u6210\u5177\u6709\u4e0d\u540c\u7684\u62d3\u6251\u7279\u5f81", "result": "HalluZig\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff0c\u5206\u6790\u8868\u660e\u8fd9\u4e9b\u62d3\u6251\u7279\u5f81\u5728\u4e0d\u540c\u6a21\u578b\u95f4\u5177\u6709\u6cdb\u5316\u6027\uff0c\u4e14\u4ec5\u4f7f\u7528\u90e8\u5206\u7f51\u7edc\u6df1\u5ea6\u7684\u7ed3\u6784\u7279\u5f81\u5373\u53ef\u68c0\u6d4b\u5e7b\u89c9", "conclusion": "\u901a\u8fc7\u5206\u6790\u6ce8\u610f\u529b\u52a8\u6001\u62d3\u6251\u7684zigzag\u6301\u4e45\u6027\uff0c\u4e3a\u5e7b\u89c9\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u8303\u5f0f\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5185\u90e8\u63a8\u7406\u8fc7\u7a0b\u7684\u7ed3\u6784\u7279\u5f81\u5bf9\u68c0\u6d4b\u5e7b\u89c9\u7684\u91cd\u8981\u6027"}}
{"id": "2601.01009", "categories": ["cs.LG", "cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2601.01009", "abs": "https://arxiv.org/abs/2601.01009", "authors": ["Mojtaba Aliasghar-Mamaghani", "Mohammadreza Khalafi"], "title": "Data-Driven Assessment of Concrete Mixture Compositions on Chloride Transport via Standalone Machine Learning Algorithms", "comment": null, "summary": "This paper employs a data-driven approach to determine the impact of concrete mixture compositions on the temporal evolution of chloride in concrete structures. This is critical for assessing the service life of civil infrastructure subjected to aggressive environments. The adopted methodology relies on several simple and complex standalone machine learning (ML) algorithms, with the primary objective of establishing confidence in the unbiased prediction of the underlying hidden correlations. The simple algorithms include linear regression (LR), k-nearest neighbors (KNN) regression, and kernel ridge regression (KRR). The complex algorithms entail support vector regression (SVR), Gaussian process regression (GPR), and two families of artificial neural networks, including a feedforward network (multilayer perceptron, MLP) and a gated recurrent unit (GRU). The MLP architecture cannot explicitly handle sequential data, a limitation addressed by the GRU. A comprehensive dataset is considered. The performance of ML algorithms is evaluated, with KRR, GPR, and MLP exhibiting high accuracy. Given the diversity of the adopted concrete mixture proportions, the GRU was unable to accurately reproduce the response in the test set. Further analyses elucidate the contributions of mixture compositions to the temporal evolution of chloride. The results obtained from the GPR model unravel latent correlations through clear and explainable trends. The MLP, SVR, and KRR also provide acceptable estimates of the overall trends. The majority of mixture components exhibit an inverse relation with chloride content, while a few components demonstrate a direct correlation. These findings highlight the potential of surrogate approaches for describing the physical processes involved in chloride ingress and the associated correlations, toward the ultimate goal of enhancing the service life of civil infrastructure.", "AI": {"tldr": "\u4f7f\u7528\u591a\u79cd\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u5206\u6790\u6df7\u51dd\u571f\u914d\u5408\u6bd4\u5bf9\u6c2f\u79bb\u5b50\u4fb5\u8680\u65f6\u95f4\u6f14\u5316\u7684\u5f71\u54cd\uff0c\u53d1\u73b0KRR\u3001GPR\u548cMLP\u6a21\u578b\u7cbe\u5ea6\u6700\u9ad8\uff0c\u63ed\u793a\u4e86\u914d\u5408\u6bd4\u6210\u5206\u4e0e\u6c2f\u79bb\u5b50\u542b\u91cf\u4e4b\u95f4\u7684\u6b63\u8d1f\u76f8\u5173\u6027\u3002", "motivation": "\u8bc4\u4f30\u53d7\u4fb5\u8680\u73af\u5883\u4e0b\u6df7\u51dd\u571f\u7ed3\u6784\u7684\u670d\u5f79\u5bff\u547d\uff0c\u9700\u8981\u7406\u89e3\u6df7\u51dd\u571f\u914d\u5408\u6bd4\u5bf9\u6c2f\u79bb\u5b50\u4fb5\u8680\u65f6\u95f4\u6f14\u5316\u7684\u5f71\u54cd\u3002\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u590d\u6742\u7684\u9690\u85cf\u76f8\u5173\u6027\uff0c\u56e0\u6b64\u91c7\u7528\u6570\u636e\u9a71\u52a8\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u6765\u5efa\u7acb\u53ef\u9760\u7684\u9884\u6d4b\u6a21\u578b\u3002", "method": "\u91c7\u7528\u7b80\u5355\u548c\u590d\u6742\u7684\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff1a\u7ebf\u6027\u56de\u5f52\u3001KNN\u56de\u5f52\u3001\u6838\u5cad\u56de\u5f52\uff08KRR\uff09\u3001\u652f\u6301\u5411\u91cf\u56de\u5f52\uff08SVR\uff09\u3001\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\uff08GPR\uff09\u3001\u524d\u9988\u795e\u7ecf\u7f51\u7edc\uff08MLP\uff09\u548c\u95e8\u63a7\u5faa\u73af\u5355\u5143\uff08GRU\uff09\u3002\u4f7f\u7528\u7efc\u5408\u6570\u636e\u96c6\u8bc4\u4f30\u7b97\u6cd5\u6027\u80fd\uff0c\u5e76\u5206\u6790\u914d\u5408\u6bd4\u6210\u5206\u5bf9\u6c2f\u79bb\u5b50\u4fb5\u8680\u7684\u8d21\u732e\u3002", "result": "KRR\u3001GPR\u548cMLP\u8868\u73b0\u51fa\u9ad8\u7cbe\u5ea6\uff0c\u800cGRU\u7531\u4e8e\u914d\u5408\u6bd4\u591a\u6837\u6027\u65e0\u6cd5\u51c6\u786e\u9884\u6d4b\u6d4b\u8bd5\u96c6\u54cd\u5e94\u3002GPR\u6a21\u578b\u63ed\u793a\u4e86\u6e05\u6670\u7684\u6f5c\u5728\u76f8\u5173\u6027\u8d8b\u52bf\uff0cMLP\u3001SVR\u548cKRR\u4e5f\u80fd\u63d0\u4f9b\u53ef\u63a5\u53d7\u7684\u8d8b\u52bf\u4f30\u8ba1\u3002\u5927\u591a\u6570\u914d\u5408\u6bd4\u6210\u5206\u4e0e\u6c2f\u79bb\u5b50\u542b\u91cf\u5448\u8d1f\u76f8\u5173\uff0c\u5c11\u6570\u6210\u5206\u5448\u6b63\u76f8\u5173\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63cf\u8ff0\u6c2f\u79bb\u5b50\u4fb5\u8680\u7684\u7269\u7406\u8fc7\u7a0b\u548c\u76f8\u5173\u5173\u7cfb\uff0c\u4e3a\u589e\u5f3a\u571f\u6728\u57fa\u7840\u8bbe\u65bd\u670d\u5f79\u5bff\u547d\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6cd5\u3002GPR\u6a21\u578b\u7279\u522b\u9002\u5408\u63ed\u793a\u53ef\u89e3\u91ca\u7684\u8d8b\u52bf\uff0c\u800c\u914d\u5408\u6bd4\u6210\u5206\u7684\u591a\u6837\u6027\u5bf9\u67d0\u4e9b\u6a21\u578b\uff08\u5982GRU\uff09\u63d0\u51fa\u4e86\u6311\u6218\u3002"}}
{"id": "2601.01584", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01584", "abs": "https://arxiv.org/abs/2601.01584", "authors": ["Jakub Hoscilowicz"], "title": "Steerability of Instrumental-Convergence Tendencies in LLMs", "comment": "Code is available at https://github.com/j-hoscilowicz/instrumental_steering", "summary": "We examine two properties of AI systems: capability (what a system can do) and steerability (how reliably one can shift behavior toward intended outcomes). In our experiments, higher capability does not imply lower steerability. We distinguish between authorized steerability (builders reliably reaching intended behaviors) and unauthorized steerability (attackers eliciting disallowed behaviors). This distinction highlights a fundamental safety--security dilemma for open-weight AI models: safety requires high steerability to enforce control (e.g., stop/refuse), while security requires low steerability to prevent malicious actors from eliciting harmful behaviors. This tension is acute for open-weight models, which are currently highly steerable via common techniques such as fine-tuning and adversarial prompting. Using Qwen3 models (4B/30B; Base/Instruct/Thinking) and InstrumentalEval, we find that a short anti-instrumental prompt suffix sharply reduces outputs labeled as instrumental convergence (e.g., shutdown avoidance, deception, self-replication). For Qwen3-30B Instruct, convergence drops from 81.69% under a pro-instrumental suffix to 2.82% under an anti-instrumental suffix. Under anti-instrumental prompting, larger aligned models produce fewer convergence-labeled outputs than smaller ones (Instruct: 2.82% vs. 4.23%; Thinking: 4.23% vs. 9.86%). Code is available at github.com/j-hoscilowicz/instrumental_steering.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0AI\u7cfb\u7edf\u7684\u80fd\u529b\u4e0e\u53ef\u64cd\u63a7\u6027\u5e76\u975e\u8d1f\u76f8\u5173\uff0c\u533a\u5206\u4e86\u6388\u6743\u4e0e\u975e\u6388\u6743\u64cd\u63a7\u6027\uff0c\u63ed\u793a\u4e86\u5f00\u653e\u6743\u91cd\u6a21\u578b\u7684\u5b89\u5168-\u5b89\u5168\u56f0\u5883\uff1a\u5b89\u5168\u9700\u8981\u9ad8\u64cd\u63a7\u6027\u4ee5\u5b9e\u65bd\u63a7\u5236\uff0c\u800c\u5b89\u5168\u9700\u8981\u4f4e\u64cd\u63a7\u6027\u4ee5\u9632\u6b62\u6076\u610f\u884c\u4e3a", "motivation": "\u63a2\u8ba8AI\u7cfb\u7edf\u7684\u80fd\u529b\u4e0e\u53ef\u64cd\u63a7\u6027\u5173\u7cfb\uff0c\u533a\u5206\u6388\u6743\u4e0e\u975e\u6388\u6743\u64cd\u63a7\u6027\uff0c\u63ed\u793a\u5f00\u653e\u6743\u91cd\u6a21\u578b\u9762\u4e34\u7684\u5b89\u5168-\u5b89\u5168\u56f0\u5883\uff0c\u5373\u6a21\u578b\u65e2\u8981\u80fd\u88ab\u5f00\u53d1\u8005\u53ef\u9760\u63a7\u5236\uff0c\u53c8\u8981\u9632\u6b62\u88ab\u653b\u51fb\u8005\u6076\u610f\u64cd\u63a7", "method": "\u4f7f\u7528Qwen3\u6a21\u578b\uff084B/30B\uff1bBase/Instruct/Thinking\uff09\u548cInstrumentalEval\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u6b63\u5411\u548c\u53cd\u5411\u5de5\u5177\u6027\u63d0\u793a\u540e\u7f00\u6765\u6d4b\u8bd5\u6a21\u578b\u7684\u53ef\u64cd\u63a7\u6027\uff0c\u6bd4\u8f83\u4e0d\u540c\u5927\u5c0f\u548c\u5bf9\u9f50\u6a21\u578b\u7684\u54cd\u5e94\u5dee\u5f02", "result": "\u80fd\u529b\u66f4\u9ad8\u7684\u7cfb\u7edf\u5e76\u4e0d\u4e00\u5b9a\u66f4\u96be\u64cd\u63a7\uff1b\u77ed\u7684\u53cd\u5de5\u5177\u6027\u63d0\u793a\u540e\u7f00\u80fd\u663e\u8457\u51cf\u5c11\u5de5\u5177\u6027\u6536\u655b\u884c\u4e3a\uff08\u5982\u5173\u673a\u56de\u907f\u3001\u6b3a\u9a97\u3001\u81ea\u6211\u590d\u5236\uff09\uff1b\u5728\u53cd\u5de5\u5177\u6027\u63d0\u793a\u4e0b\uff0c\u66f4\u5927\u7684\u5bf9\u9f50\u6a21\u578b\u4ea7\u751f\u7684\u6536\u655b\u6807\u8bb0\u8f93\u51fa\u66f4\u5c11", "conclusion": "AI\u7cfb\u7edf\u7684\u80fd\u529b\u4e0e\u53ef\u64cd\u63a7\u6027\u5e76\u975e\u7b80\u5355\u8d1f\u76f8\u5173\uff0c\u5f00\u653e\u6743\u91cd\u6a21\u578b\u9762\u4e34\u5b89\u5168-\u5b89\u5168\u56f0\u5883\uff0c\u9700\u8981\u5e73\u8861\u6388\u6743\u64cd\u63a7\u6027\u548c\u975e\u6388\u6743\u64cd\u63a7\u6027\uff0c\u53cd\u5de5\u5177\u6027\u63d0\u793a\u80fd\u6709\u6548\u51cf\u5c11\u6709\u5bb3\u884c\u4e3a"}}
{"id": "2601.01014", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01014", "abs": "https://arxiv.org/abs/2601.01014", "authors": ["Haoran Su", "Chenyu You"], "title": "Geometric and Dynamic Scaling in Deep Transformers", "comment": "Research Proposal Only", "summary": "Despite their empirical success, pushing Transformer architectures to extreme depth often leads to a paradoxical failure: representations become increasingly redundant, lose rank, and ultimately collapse. Existing explanations largely attribute this phenomenon to optimization instability or vanishing gradients, yet such accounts fail to explain why collapse persists even under modern normalization and initialization schemes. In this paper, we argue that the collapse of deep Transformers is fundamentally a geometric problem. Standard residual updates implicitly assume that feature accumulation is always beneficial, but offer no mechanism to constrain update directions or to erase outdated information. As depth increases, this leads to systematic drift off the semantic manifold and monotonic feature accumulation, causing representational degeneracy. We propose a unified geometric framework that addresses these failures through two orthogonal principles. First, manifold-constrained hyper-connections restrict residual updates to valid local tangent directions, preventing uncontrolled manifold drift. Second, deep delta learning introduces data-dependent, non-monotonic updates that enable reflection and erasure of redundant features rather than their unconditional accumulation. Together, these mechanisms decouple the direction and sign of feature updates, yielding a stable geometric evolution across depth. We term the resulting architecture the Manifold-Geometric Transformer (MGT). Our analysis predicts that enforcing geometric validity while allowing dynamic erasure is essential for avoiding rank collapse in ultra-deep networks. We outline an evaluation protocol for Transformers exceeding 100 layers to test the hypothesis that geometry, rather than depth itself, is the key limiting factor in deep representation learning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faTransformer\u6df1\u5ea6\u589e\u52a0\u5bfc\u81f4\u8868\u793a\u5d29\u6e83\u7684\u6839\u672c\u539f\u56e0\u662f\u51e0\u4f55\u95ee\u9898\uff0c\u800c\u975e\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86Manifold-Geometric Transformer (MGT)\u67b6\u6784\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5c06Transformer\u6df1\u5ea6\u589e\u52a0\u5bfc\u81f4\u7684\u8868\u793a\u5d29\u6e83\u5f52\u56e0\u4e8e\u4f18\u5316\u4e0d\u7a33\u5b9a\u6216\u68af\u5ea6\u6d88\u5931\uff0c\u4f46\u8fd9\u4e9b\u89e3\u91ca\u65e0\u6cd5\u89e3\u91ca\u4e3a\u4ec0\u4e48\u5728\u73b0\u4ee3\u5f52\u4e00\u5316\u548c\u521d\u59cb\u5316\u65b9\u6848\u4e0b\u5d29\u6e83\u4ecd\u7136\u5b58\u5728\u3002\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u662f\u4e00\u4e2a\u51e0\u4f55\u95ee\u9898\uff0c\u6807\u51c6\u6b8b\u5dee\u66f4\u65b0\u7f3a\u4e4f\u7ea6\u675f\u66f4\u65b0\u65b9\u5411\u6216\u64e6\u9664\u8fc7\u65f6\u4fe1\u606f\u7684\u673a\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u7edf\u4e00\u7684\u51e0\u4f55\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6b63\u4ea4\u539f\u5219\uff1a1) \u6d41\u5f62\u7ea6\u675f\u8d85\u8fde\u63a5\uff0c\u9650\u5236\u6b8b\u5dee\u66f4\u65b0\u5230\u6709\u6548\u7684\u5c40\u90e8\u5207\u5411\u65b9\u5411\uff0c\u9632\u6b62\u4e0d\u53d7\u63a7\u5236\u7684\u6d41\u5f62\u6f02\u79fb\uff1b2) \u6df1\u5ea6\u589e\u91cf\u5b66\u4e60\uff0c\u5f15\u5165\u6570\u636e\u4f9d\u8d56\u7684\u975e\u5355\u8c03\u66f4\u65b0\uff0c\u5b9e\u73b0\u5197\u4f59\u7279\u5f81\u7684\u53cd\u5c04\u548c\u64e6\u9664\u800c\u975e\u65e0\u6761\u4ef6\u7d2f\u79ef\u3002", "result": "\u8fd9\u4e9b\u673a\u5236\u89e3\u8026\u4e86\u7279\u5f81\u66f4\u65b0\u7684\u65b9\u5411\u548c\u7b26\u53f7\uff0c\u5b9e\u73b0\u4e86\u8de8\u6df1\u5ea6\u7684\u7a33\u5b9a\u51e0\u4f55\u6f14\u5316\u3002\u63d0\u51fa\u7684MGT\u67b6\u6784\u9884\u6d4b\uff0c\u5f3a\u5236\u51e0\u4f55\u6709\u6548\u6027\u540c\u65f6\u5141\u8bb8\u52a8\u6001\u64e6\u9664\u5bf9\u4e8e\u907f\u514d\u8d85\u6df1\u7f51\u7edc\u4e2d\u7684\u79e9\u5d29\u6e83\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u51e0\u4f55\u800c\u975e\u6df1\u5ea6\u672c\u8eab\u662f\u6df1\u5ea6\u8868\u793a\u5b66\u4e60\u7684\u5173\u952e\u9650\u5236\u56e0\u7d20\u3002\u8bba\u6587\u4e3a\u8d85\u8fc7100\u5c42\u7684Transformer\u8bbe\u8ba1\u4e86\u8bc4\u4f30\u534f\u8bae\u6765\u9a8c\u8bc1\u8fd9\u4e00\u5047\u8bbe\u3002"}}
{"id": "2601.01844", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01844", "abs": "https://arxiv.org/abs/2601.01844", "authors": ["Udiptaman Das", "Krishnasai B. Atmakuri", "Duy Ho", "Chi Lee", "Yugyung Lee"], "title": "Clinical Knowledge Graph Construction and Evaluation with Multi-LLMs via Retrieval-Augmented Generation", "comment": "13 pages, 5 tables, 4 figures", "summary": "Large language models (LLMs) offer new opportunities for constructing knowledge graphs (KGs) from unstructured clinical narratives. However, existing approaches often rely on structured inputs and lack robust validation of factual accuracy and semantic consistency, limitations that are especially problematic in oncology. We introduce an end-to-end framework for clinical KG construction and evaluation directly from free text using multi-agent prompting and a schema-constrained Retrieval-Augmented Generation (KG-RAG) strategy. Our pipeline integrates (1) prompt-driven entity, attribute, and relation extraction; (2) entropy-based uncertainty scoring; (3) ontology-aligned RDF/OWL schema generation; and (4) multi-LLM consensus validation for hallucination detection and semantic refinement. Beyond static graph construction, the framework supports continuous refinement and self-supervised evaluation, enabling iterative improvement of graph quality. Applied to two oncology cohorts (PDAC and BRCA), our method produces interpretable, SPARQL-compatible, and clinically grounded knowledge graphs without relying on gold-standard annotations. Experimental results demonstrate consistent gains in precision, relevance, and ontology compliance over baseline methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u5229\u7528\u591a\u667a\u80fd\u4f53\u63d0\u793a\u548c\u6a21\u5f0f\u7ea6\u675f\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7b56\u7565\uff0c\u76f4\u63a5\u4ece\u81ea\u7531\u6587\u672c\u6784\u5efa\u4e34\u5e8a\u77e5\u8bc6\u56fe\u8c31\uff0c\u7279\u522b\u9488\u5bf9\u80bf\u7624\u5b66\u9886\u57df\uff0c\u65e0\u9700\u4f9d\u8d56\u9ec4\u91d1\u6807\u51c6\u6807\u6ce8\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u7ed3\u6784\u5316\u8f93\u5165\uff0c\u7f3a\u4e4f\u5bf9\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u7684\u9c81\u68d2\u9a8c\u8bc1\uff0c\u8fd9\u5728\u80bf\u7624\u5b66\u9886\u57df\u5c24\u5176\u6210\u95ee\u9898\u3002\u9700\u8981\u76f4\u63a5\u4ece\u975e\u7ed3\u6784\u5316\u4e34\u5e8a\u53d9\u8ff0\u6784\u5efa\u9ad8\u8d28\u91cf\u77e5\u8bc6\u56fe\u8c31\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u63d0\u793a\u548c\u6a21\u5f0f\u7ea6\u675f\u7684KG-RAG\u7b56\u7565\uff0c\u5305\u62ec\uff1a(1) \u63d0\u793a\u9a71\u52a8\u7684\u5b9e\u4f53\u3001\u5c5e\u6027\u548c\u5173\u7cfb\u62bd\u53d6\uff1b(2) \u57fa\u4e8e\u71b5\u7684\u4e0d\u786e\u5b9a\u6027\u8bc4\u5206\uff1b(3) \u672c\u4f53\u5bf9\u9f50\u7684RDF/OWL\u6a21\u5f0f\u751f\u6210\uff1b(4) \u591aLLM\u5171\u8bc6\u9a8c\u8bc1\u7528\u4e8e\u5e7b\u89c9\u68c0\u6d4b\u548c\u8bed\u4e49\u7cbe\u70bc\u3002", "result": "\u5e94\u7528\u4e8ePDAC\u548cBRCA\u4e24\u4e2a\u80bf\u7624\u961f\u5217\uff0c\u8be5\u65b9\u6cd5\u4ea7\u751f\u4e86\u53ef\u89e3\u91ca\u3001SPARQL\u517c\u5bb9\u4e14\u4e34\u5e8a\u57fa\u7840\u7684\u77e5\u8bc6\u56fe\u8c31\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5728\u7cbe\u786e\u5ea6\u3001\u76f8\u5173\u6027\u548c\u672c\u4f53\u4e00\u81f4\u6027\u65b9\u9762\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u6709\u6301\u7eed\u63d0\u5347\u3002", "conclusion": "\u8be5\u6846\u67b6\u652f\u6301\u8fde\u7eed\u7cbe\u70bc\u548c\u81ea\u6211\u76d1\u7763\u8bc4\u4f30\uff0c\u80fd\u591f\u8fed\u4ee3\u6539\u8fdb\u56fe\u8c31\u8d28\u91cf\uff0c\u4e3a\u76f4\u63a5\u4ece\u81ea\u7531\u6587\u672c\u6784\u5efa\u4e34\u5e8a\u77e5\u8bc6\u56fe\u8c31\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u80bf\u7624\u5b66\u9886\u57df\u3002"}}
{"id": "2601.01624", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01624", "abs": "https://arxiv.org/abs/2601.01624", "authors": ["Raj Vardhan Tomar", "Preslav Nakov", "Yuxia Wang"], "title": "How Does Prefix Matter in Reasoning Model Tuning?", "comment": null, "summary": "Recent alignment studies commonly remove introductory boilerplate phrases from supervised fine-tuning (SFT) datasets. This work challenges that assumption. We hypothesize that safety- and reasoning-oriented prefix sentences serve as lightweight alignment signals that can guide model decoding toward safer and more coherent responses. To examine this, we fine-tune three R1 series models across three core model capabilities: reasoning (mathematics, coding), safety, and factuality, systematically varying prefix inclusion from 0% to 100%.\n  Results show that prefix-conditioned SFT improves both safety and reasoning performance, yielding up to +6% higher Safe@1 accuracy on adversarial benchmarks (WildJailbreak, StrongReject) and +7% improvement on GSM8K reasoning. However, factuality and coding tasks show marginal or negative effects, indicating that prefix-induced narrowing of the search space benefits structured reasoning. Token-level loss analysis further reveals that prefix tokens such as \"revised\" and \"logically\" incur higher gradient magnitudes, acting as alignment anchors that stabilize reasoning trajectories. Our findings suggest that prefix conditioning offers a scalable and interpretable mechanism for improving reasoning safety, serving as an implicit form of alignment that complements traditional reward-based methods.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0SFT\u6570\u636e\u96c6\u4e2d\u4fdd\u7559\u524d\u7f00\u77ed\u8bed\uff08\u5982\"revised\"\u3001\"logically\"\uff09\u80fd\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u5bf9\u9f50\u4fe1\u53f7\uff0c\u63d0\u5347\u6a21\u578b\u7684\u5b89\u5168\u6027\u548c\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5bf9\u4e8b\u5b9e\u6027\u548c\u7f16\u7801\u4efb\u52a1\u6548\u679c\u6709\u9650\u3002", "motivation": "\u6311\u6218\u5f53\u524dSFT\u6570\u636e\u96c6\u4e2d\u5220\u9664\u524d\u7f00\u77ed\u8bed\u7684\u666e\u904d\u505a\u6cd5\uff0c\u5047\u8bbe\u5b89\u5168\u6027\u548c\u63a8\u7406\u5bfc\u5411\u7684\u524d\u7f00\u53e5\u5b50\u53ef\u4ee5\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u5bf9\u9f50\u4fe1\u53f7\uff0c\u5f15\u5bfc\u6a21\u578b\u751f\u6210\u66f4\u5b89\u5168\u3001\u66f4\u8fde\u8d2f\u7684\u54cd\u5e94\u3002", "method": "\u5728\u4e09\u4e2aR1\u7cfb\u5217\u6a21\u578b\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u8986\u76d6\u63a8\u7406\uff08\u6570\u5b66\u3001\u7f16\u7801\uff09\u3001\u5b89\u5168\u6027\u548c\u4e8b\u5b9e\u6027\u4e09\u4e2a\u6838\u5fc3\u80fd\u529b\uff0c\u7cfb\u7edf\u6027\u5730\u6539\u53d8\u524d\u7f00\u5305\u542b\u6bd4\u4f8b\uff080%\u5230100%\uff09\u3002", "result": "\u524d\u7f00\u6761\u4ef6SFT\u663e\u8457\u63d0\u5347\u5b89\u5168\u6027\u548c\u63a8\u7406\u6027\u80fd\uff1a\u5b89\u5168\u57fa\u51c6\uff08WildJailbreak, StrongReject\uff09\u4e0aSafe@1\u51c6\u786e\u7387\u63d0\u5347\u6700\u9ad8+6%\uff0cGSM8K\u63a8\u7406\u63d0\u5347+7%\u3002\u4f46\u4e8b\u5b9e\u6027\u548c\u7f16\u7801\u4efb\u52a1\u6548\u679c\u6709\u9650\u6216\u8d1f\u5411\u3002\u635f\u5931\u5206\u6790\u663e\u793a\"revised\"\u548c\"logically\"\u7b49\u524d\u7f00token\u68af\u5ea6\u5e45\u5ea6\u66f4\u9ad8\uff0c\u4f5c\u4e3a\u5bf9\u9f50\u951a\u70b9\u7a33\u5b9a\u63a8\u7406\u8f68\u8ff9\u3002", "conclusion": "\u524d\u7f00\u6761\u4ef6\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684\u673a\u5236\u6765\u6539\u5584\u63a8\u7406\u5b89\u5168\u6027\uff0c\u4f5c\u4e3a\u4f20\u7edf\u57fa\u4e8e\u5956\u52b1\u65b9\u6cd5\u7684\u8865\u5145\uff0c\u662f\u4e00\u79cd\u9690\u5f0f\u7684\u5bf9\u9f50\u5f62\u5f0f\u3002"}}
{"id": "2601.01857", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01857", "abs": "https://arxiv.org/abs/2601.01857", "authors": ["Defei Xia", "Bingfeng Pi", "Shenbin Zhang", "Song Hua", "Yunfei Wei", "Lei Zuo"], "title": "Jenius Agent: Towards Experience-Driven Accuracy Optimization in Real-World Scenarios", "comment": null, "summary": "As agent systems powered by large language models (LLMs) advance, improving the task performance of an autonomous agent, especially in context understanding, tool usage, and response generation, has become increasingly critical. Although prior studies have advanced the overall design of LLM-based agents, systematic optimization of their internal reasoning and tool-use pipelines remains underexplored. This paper introduces an agent framework grounded in real-world practical experience, with three key innovations: (1) an adaptive prompt generation strategy that aligns with the agent's state and task goals to improve reliability and robustness; (2) a context-aware tool orchestration module that performs tool categorization, semantic retrieval, and adaptive invocation based on user intent and context; and (3) a layered memory mechanism that integrates session memory, task history, and external summaries to improve relevance and efficiency through dynamic summarization and compression. An end-to-end framework named Jenius-Agent has been integrated with three key optimizations, including tools based on the Model Context Protocol (MCP), file input/output (I/O), and execution feedback. The experiments show a 20 percent improvement in task accuracy, along with a reduced token cost, response latency, and invocation failures. The framework is already deployed in Jenius (https://www.jenius.cn), providing a lightweight and scalable solution for robust, protocol-compatible autonomous agents.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faJenius-Agent\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u63d0\u793a\u751f\u6210\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u5de5\u5177\u7f16\u6392\u548c\u5206\u5c42\u5185\u5b58\u673a\u5236\u4e09\u5927\u521b\u65b0\uff0c\u63d0\u5347LLM\u667a\u80fd\u4f53\u7684\u4efb\u52a1\u51c6\u786e\u602720%\uff0c\u540c\u65f6\u964d\u4f4etoken\u6210\u672c\u3001\u54cd\u5e94\u5ef6\u8fdf\u548c\u8c03\u7528\u5931\u8d25\u7387\u3002", "motivation": "\u968f\u7740\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u53d1\u5c55\uff0c\u63d0\u5347\u81ea\u4e3b\u667a\u80fd\u4f53\u5728\u4e0a\u4e0b\u6587\u7406\u89e3\u3001\u5de5\u5177\u4f7f\u7528\u548c\u54cd\u5e94\u751f\u6210\u65b9\u9762\u7684\u4efb\u52a1\u6027\u80fd\u53d8\u5f97\u65e5\u76ca\u91cd\u8981\u3002\u5c3d\u7ba1\u5148\u524d\u7814\u7a76\u6539\u8fdb\u4e86LLM\u667a\u80fd\u4f53\u7684\u6574\u4f53\u8bbe\u8ba1\uff0c\u4f46\u5bf9\u5176\u5185\u90e8\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\u6d41\u7a0b\u7684\u7cfb\u7edf\u4f18\u5316\u4ecd\u663e\u4e0d\u8db3\u3002", "method": "\u63d0\u51faJenius-Agent\u6846\u67b6\uff0c\u5305\u542b\u4e09\u5927\u5173\u952e\u521b\u65b0\uff1a1) \u81ea\u9002\u5e94\u63d0\u793a\u751f\u6210\u7b56\u7565\uff0c\u6839\u636e\u667a\u80fd\u4f53\u72b6\u6001\u548c\u4efb\u52a1\u76ee\u6807\u8c03\u6574\u63d0\u793a\u4ee5\u63d0\u9ad8\u53ef\u9760\u6027\u548c\u9c81\u68d2\u6027\uff1b2) \u4e0a\u4e0b\u6587\u611f\u77e5\u5de5\u5177\u7f16\u6392\u6a21\u5757\uff0c\u57fa\u4e8e\u7528\u6237\u610f\u56fe\u548c\u4e0a\u4e0b\u6587\u8fdb\u884c\u5de5\u5177\u5206\u7c7b\u3001\u8bed\u4e49\u68c0\u7d22\u548c\u81ea\u9002\u5e94\u8c03\u7528\uff1b3) \u5206\u5c42\u5185\u5b58\u673a\u5236\uff0c\u6574\u5408\u4f1a\u8bdd\u5185\u5b58\u3001\u4efb\u52a1\u5386\u53f2\u548c\u5916\u90e8\u6458\u8981\uff0c\u901a\u8fc7\u52a8\u6001\u6458\u8981\u548c\u538b\u7f29\u63d0\u9ad8\u76f8\u5173\u6027\u548c\u6548\u7387\u3002\u6846\u67b6\u8fd8\u96c6\u6210\u4e86\u57fa\u4e8eMCP\u7684\u5de5\u5177\u3001\u6587\u4ef6I/O\u548c\u6267\u884c\u53cd\u9988\u7b49\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u4efb\u52a1\u51c6\u786e\u6027\u63d0\u534720%\uff0c\u540c\u65f6\u964d\u4f4e\u4e86token\u6210\u672c\u3001\u54cd\u5e94\u5ef6\u8fdf\u548c\u8c03\u7528\u5931\u8d25\u7387\u3002\u8be5\u6846\u67b6\u5df2\u5728Jenius\u5e73\u53f0\u90e8\u7f72\uff0c\u4e3a\u7a33\u5065\u3001\u534f\u8bae\u517c\u5bb9\u7684\u81ea\u4e3b\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u8f7b\u91cf\u7ea7\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "Jenius-Agent\u6846\u67b6\u901a\u8fc7\u7cfb\u7edf\u4f18\u5316\u667a\u80fd\u4f53\u7684\u5185\u90e8\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u81ea\u4e3b\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01627", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01627", "abs": "https://arxiv.org/abs/2601.01627", "authors": ["Junyu Liu", "Zirui Li", "Qian Niu", "Zequn Zhang", "Yue Xun", "Wenlong Hou", "Shujun Wang", "Yusuke Iwasawa", "Yutaka Matsuo", "Kan Hatakeyama-Sato"], "title": "JMedEthicBench: A Multi-Turn Conversational Benchmark for Evaluating Medical Safety in Japanese Large Language Models", "comment": "12 pages, 6 figures", "summary": "As Large Language Models (LLMs) are increasingly deployed in healthcare field, it becomes essential to carefully evaluate their medical safety before clinical use. However, existing safety benchmarks remain predominantly English-centric, and test with only single-turn prompts despite multi-turn clinical consultations. To address these gaps, we introduce JMedEthicBench, the first multi-turn conversational benchmark for evaluating medical safety of LLMs for Japanese healthcare. Our benchmark is based on 67 guidelines from the Japan Medical Association and contains over 50,000 adversarial conversations generated using seven automatically discovered jailbreak strategies. Using a dual-LLM scoring protocol, we evaluate 27 models and find that commercial models maintain robust safety while medical-specialized models exhibit increased vulnerability. Furthermore, safety scores decline significantly across conversation turns (median: 9.5 to 5.0, $p < 0.001$). Cross-lingual evaluation on both Japanese and English versions of our benchmark reveals that medical model vulnerabilities persist across languages, indicating inherent alignment limitations rather than language-specific factors. These findings suggest that domain-specific fine-tuning may accidentally weaken safety mechanisms and that multi-turn interactions represent a distinct threat surface requiring dedicated alignment strategies.", "AI": {"tldr": "JMedEthicBench\uff1a\u9996\u4e2a\u9488\u5bf9\u65e5\u672c\u533b\u7597\u7684\u591a\u8f6e\u5bf9\u8bdd\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30LLMs\u7684\u533b\u7597\u5b89\u5168\u6027\uff0c\u53d1\u73b0\u533b\u7597\u4e13\u7528\u6a21\u578b\u5b89\u5168\u6027\u66f4\u8106\u5f31\uff0c\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u5b89\u5168\u6027\u663e\u8457\u4e0b\u964d", "motivation": "\u73b0\u6709\u5b89\u5168\u57fa\u51c6\u4e3b\u8981\u9488\u5bf9\u82f1\u8bed\u4e14\u4ec5\u6d4b\u8bd5\u5355\u8f6e\u63d0\u793a\uff0c\u800c\u4e34\u5e8a\u54a8\u8be2\u901a\u5e38\u662f\u591a\u8f6e\u5bf9\u8bdd\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u65e5\u672c\u533b\u7597\u73af\u5883\u7684\u5b89\u5168\u8bc4\u4f30\u57fa\u51c6", "method": "\u57fa\u4e8e\u65e5\u672c\u533b\u5e08\u534f\u4f1a67\u4e2a\u6307\u5357\u6784\u5efa\u57fa\u51c6\uff0c\u4f7f\u75287\u79cd\u81ea\u52a8\u53d1\u73b0\u7684\u8d8a\u72f1\u7b56\u7565\u751f\u6210\u8d85\u8fc750,000\u4e2a\u5bf9\u6297\u6027\u5bf9\u8bdd\uff0c\u91c7\u7528\u53ccLLM\u8bc4\u5206\u534f\u8bae\u8bc4\u4f3027\u4e2a\u6a21\u578b", "result": "\u5546\u4e1a\u6a21\u578b\u4fdd\u6301\u7a33\u5065\u5b89\u5168\u6027\uff0c\u533b\u7597\u4e13\u7528\u6a21\u578b\u5b89\u5168\u6027\u66f4\u8106\u5f31\uff1b\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u5b89\u5168\u6027\u663e\u8457\u4e0b\u964d\uff08\u4e2d\u4f4d\u65709.5\u52305.0\uff09\uff1b\u8de8\u8bed\u8a00\u8bc4\u4f30\u663e\u793a\u533b\u7597\u6a21\u578b\u6f0f\u6d1e\u5728\u6240\u6709\u8bed\u8a00\u4e2d\u90fd\u5b58\u5728", "conclusion": "\u9886\u57df\u7279\u5b9a\u5fae\u8c03\u53ef\u80fd\u610f\u5916\u524a\u5f31\u5b89\u5168\u673a\u5236\uff0c\u591a\u8f6e\u4ea4\u4e92\u4ee3\u8868\u72ec\u7279\u5a01\u80c1\u9762\uff0c\u9700\u8981\u4e13\u95e8\u7684\u6821\u51c6\u7b56\u7565\uff1b\u533b\u7597\u6a21\u578b\u6f0f\u6d1e\u662f\u5185\u5728\u6821\u51c6\u9650\u5236\u800c\u975e\u8bed\u8a00\u7279\u5b9a\u56e0\u7d20"}}
{"id": "2601.01021", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01021", "abs": "https://arxiv.org/abs/2601.01021", "authors": ["Dai Shi", "Lequan Lin", "Andi Han", "Luke Thompson", "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "Zhiyong Wang", "Junbin Gao"], "title": "Expanding the Chaos: Neural Operator for Stochastic (Partial) Differential Equations", "comment": null, "summary": "Stochastic differential equations (SDEs) and stochastic partial differential equations (SPDEs) are fundamental tools for modeling stochastic dynamics across the natural sciences and modern machine learning. Developing deep learning models for approximating their solution operators promises not only fast, practical solvers, but may also inspire models that resolve classical learning tasks from a new perspective. In this work, we build on classical Wiener chaos expansions (WCE) to design neural operator (NO) architectures for SPDEs and SDEs: we project the driving noise paths onto orthonormal Wick Hermite features and parameterize the resulting deterministic chaos coefficients with neural operators, so that full solution trajectories can be reconstructed from noise in a single forward pass. On the theoretical side, we investigate the classical WCE results for the class of multi-dimensional SDEs and semilinear SPDEs considered here by explicitly writing down the associated coupled ODE/PDE systems for their chaos coefficients, which makes the separation between stochastic forcing and deterministic dynamics fully explicit and directly motivates our model designs. On the empirical side, we validate our models on a diverse suite of problems: classical SPDE benchmarks, diffusion one-step sampling on images, topological interpolation on graphs, financial extrapolation, parameter estimation, and manifold SDEs for flood prediction, demonstrating competitive accuracy and broad applicability. Overall, our results indicate that WCE-based neural operators provide a practical and scalable way to learn SDE/SPDE solution operators across diverse domains.", "AI": {"tldr": "\u57fa\u4e8eWiener\u6df7\u6c8c\u5c55\u5f00\u7684\u795e\u7ecf\u7b97\u5b50\u67b6\u6784\uff0c\u7528\u4e8e\u5b66\u4e60SDE/SPDE\u7684\u89e3\u7b97\u5b50\uff0c\u901a\u8fc7\u6b63\u4ea4Wick Hermite\u7279\u5f81\u6295\u5f71\u566a\u58f0\u8def\u5f84\uff0c\u7528\u795e\u7ecf\u7b97\u5b50\u53c2\u6570\u5316\u786e\u5b9a\u6027\u6df7\u6c8c\u7cfb\u6570\uff0c\u5b9e\u73b0\u5355\u6b21\u524d\u5411\u4f20\u64ad\u4ece\u566a\u58f0\u91cd\u6784\u5b8c\u6574\u89e3\u8f68\u8ff9\u3002", "motivation": "\u968f\u673a\u5fae\u5206\u65b9\u7a0b\uff08SDEs\uff09\u548c\u968f\u673a\u504f\u5fae\u5206\u65b9\u7a0b\uff08SPDEs\uff09\u662f\u5efa\u6a21\u968f\u673a\u52a8\u529b\u5b66\u7684\u57fa\u7840\u5de5\u5177\u3002\u5f00\u53d1\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6765\u8fd1\u4f3c\u5b83\u4eec\u7684\u89e3\u7b97\u5b50\u4e0d\u4ec5\u80fd\u63d0\u4f9b\u5feb\u901f\u5b9e\u7528\u7684\u6c42\u89e3\u5668\uff0c\u8fd8\u53ef\u80fd\u4ece\u65b0\u89c6\u89d2\u89e3\u51b3\u7ecf\u5178\u5b66\u4e60\u4efb\u52a1\u3002\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u968f\u673a\u7cfb\u7edf\u65f6\u5b58\u5728\u8ba1\u7b97\u6548\u7387\u4f4e\u6216\u7075\u6d3b\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u7ecf\u5178Wiener\u6df7\u6c8c\u5c55\u5f00\uff08WCE\uff09\uff0c\u8bbe\u8ba1\u7528\u4e8eSPDEs\u548cSDEs\u7684\u795e\u7ecf\u7b97\u5b50\u67b6\u6784\uff1a\u5c06\u9a71\u52a8\u566a\u58f0\u8def\u5f84\u6295\u5f71\u5230\u6b63\u4ea4Wick Hermite\u7279\u5f81\u4e0a\uff0c\u7528\u795e\u7ecf\u7b97\u5b50\u53c2\u6570\u5316\u5f97\u5230\u7684\u786e\u5b9a\u6027\u6df7\u6c8c\u7cfb\u6570\uff0c\u4ece\u800c\u53ef\u4ee5\u4ece\u566a\u58f0\u4e2d\u5355\u6b21\u524d\u5411\u4f20\u64ad\u91cd\u6784\u5b8c\u6574\u89e3\u8f68\u8ff9\u3002\u7406\u8bba\u65b9\u9762\uff0c\u4e3a\u591a\u7ef4SDEs\u548c\u534a\u7ebf\u6027SPDEs\u663e\u5f0f\u5199\u51fa\u6df7\u6c8c\u7cfb\u6570\u7684\u8026\u5408ODE/PDE\u7cfb\u7edf\u3002", "result": "\u5728\u591a\u6837\u5316\u95ee\u9898\u96c6\u4e0a\u9a8c\u8bc1\u6a21\u578b\uff1a\u7ecf\u5178SPDE\u57fa\u51c6\u6d4b\u8bd5\u3001\u56fe\u50cf\u6269\u6563\u4e00\u6b65\u91c7\u6837\u3001\u56fe\u62d3\u6251\u63d2\u503c\u3001\u91d1\u878d\u5916\u63a8\u3001\u53c2\u6570\u4f30\u8ba1\u3001\u6d2a\u6c34\u9884\u6d4b\u7684\u6d41\u5f62SDEs\uff0c\u5c55\u793a\u4e86\u7ade\u4e89\u6027\u7cbe\u5ea6\u548c\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "conclusion": "\u57fa\u4e8eWCE\u7684\u795e\u7ecf\u7b97\u5b50\u4e3a\u8de8\u591a\u4e2a\u9886\u57df\u5b66\u4e60SDE/SPDE\u89e3\u7b97\u5b50\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u7406\u8bba\u4e25\u8c28\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u6548\u8ba1\u7b97\u548c\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2601.01875", "categories": ["cs.AI", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2601.01875", "abs": "https://arxiv.org/abs/2601.01875", "authors": ["Kewen Cao", "Jianxu Chen", "Yongbing Zhang", "Ye Zhang", "Hongxiao Wang"], "title": "Toward Auditable Neuro-Symbolic Reasoning in Pathology: SQL as an Explicit Trace of Evidence", "comment": null, "summary": "Automated pathology image analysis is central to clinical diagnosis, but clinicians still ask which slide features drive a model's decision and why. Vision-language models can produce natural language explanations, but these are often correlational and lack verifiable evidence. In this paper, we introduce an SQL-centered agentic framework that enables both feature measurement and reasoning to be auditable. Specifically, after extracting human-interpretable cellular features, Feature Reasoning Agents compose and execute SQL queries over feature tables to aggregate visual evidence into quantitative findings. A Knowledge Comparison Agent then evaluates these findings against established pathological knowledge, mirroring how pathologists justify diagnoses from measurable observations. Extensive experiments evaluated on two pathology visual question answering datasets demonstrate our method improves interpretability and decision traceability while producing executable SQL traces that link cellular measurements to diagnostic conclusions.", "AI": {"tldr": "\u63d0\u51faSQL\u4e2d\u5fc3\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u6267\u884cSQL\u67e5\u8be2\u5c06\u75c5\u7406\u56fe\u50cf\u7279\u5f81\u6d4b\u91cf\u4e0e\u8bca\u65ad\u63a8\u7406\u5173\u8054\uff0c\u589e\u5f3a\u6a21\u578b\u51b3\u7b56\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u8ffd\u6eaf\u6027\u3002", "motivation": "\u5f53\u524d\u75c5\u7406\u56fe\u50cf\u5206\u6790\u7f3a\u4e4f\u53ef\u9a8c\u8bc1\u7684\u8bc1\u636e\u652f\u6301\u6a21\u578b\u51b3\u7b56\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u89e3\u91ca\u591a\u4e3a\u76f8\u5173\u6027\u63cf\u8ff0\u800c\u975e\u53ef\u9a8c\u8bc1\u8bc1\u636e\uff0c\u9700\u8981\u5efa\u7acb\u53ef\u5ba1\u8ba1\u7684\u7279\u5f81\u6d4b\u91cf\u548c\u63a8\u7406\u6846\u67b6\u3002", "method": "1) \u63d0\u53d6\u4eba\u7c7b\u53ef\u89e3\u91ca\u7684\u7ec6\u80de\u7279\u5f81\uff1b2) \u7279\u5f81\u63a8\u7406\u4ee3\u7406\u901a\u8fc7SQL\u67e5\u8be2\u5728\u7279\u5f81\u8868\u4e0a\u805a\u5408\u89c6\u89c9\u8bc1\u636e\u4e3a\u91cf\u5316\u53d1\u73b0\uff1b3) \u77e5\u8bc6\u6bd4\u8f83\u4ee3\u7406\u5c06\u53d1\u73b0\u4e0e\u75c5\u7406\u77e5\u8bc6\u5bf9\u6bd4\uff0c\u6a21\u62df\u75c5\u7406\u5b66\u5bb6\u4ece\u53ef\u6d4b\u91cf\u89c2\u5bdf\u5230\u8bca\u65ad\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u5728\u4e24\u4e2a\u75c5\u7406\u89c6\u89c9\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u51b3\u7b56\u53ef\u8ffd\u6eaf\u6027\uff0c\u540c\u65f6\u751f\u6210\u53ef\u6267\u884c\u7684SQL\u8ffd\u8e2a\uff0c\u5c06\u7ec6\u80de\u6d4b\u91cf\u4e0e\u8bca\u65ad\u7ed3\u8bba\u8054\u7cfb\u8d77\u6765\u3002", "conclusion": "SQL\u4e2d\u5fc3\u4ee3\u7406\u6846\u67b6\u901a\u8fc7\u53ef\u5ba1\u8ba1\u7684\u7279\u5f81\u6d4b\u91cf\u548c\u63a8\u7406\uff0c\u4e3a\u75c5\u7406\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u53ef\u9a8c\u8bc1\u7684\u89e3\u91ca\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u51b3\u7b56\u7684\u900f\u660e\u5ea6\u548c\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2601.01668", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01668", "abs": "https://arxiv.org/abs/2601.01668", "authors": ["Houman Kazemzadeh", "Nima Minaifar", "Kamyar Naderi", "Sho Tabibzadeh"], "title": "EHRSummarizer: A Privacy-Aware, FHIR-Native Architecture for Structured Clinical Summarization of Electronic Health Records", "comment": "19 pages", "summary": "Clinicians routinely navigate fragmented electronic health record (EHR) interfaces to assemble a coherent picture of a patient's problems, medications, recent encounters, and longitudinal trends. This work describes EHRSummarizer, a privacy-aware, FHIR-native reference architecture that retrieves a targeted set of high-yield FHIR R4 resources, normalizes them into a consistent clinical context package, and produces structured summaries intended to support structured chart review. The system can be configured for data minimization, stateless processing, and flexible deployment, including local inference within an organization's trust boundary. To mitigate the risk of unsupported or unsafe behavior, the summarization stage is constrained to evidence present in the retrieved context package, is intended to indicate missing or unavailable domains where feasible, and avoids diagnostic or treatment recommendations. Prototype demonstrations on synthetic and test FHIR environments illustrate end-to-end behavior and output formats; however, this manuscript does not report clinical outcomes or controlled workflow studies. We outline an evaluation plan centered on faithfulness, omission risk, temporal correctness, usability, and operational monitoring to guide future institutional assessments.", "AI": {"tldr": "EHRSummarizer\u662f\u4e00\u4e2a\u9690\u79c1\u611f\u77e5\u7684FHIR\u539f\u751f\u53c2\u8003\u67b6\u6784\uff0c\u7528\u4e8e\u4ece\u788e\u7247\u5316\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u63d0\u53d6\u5173\u952e\u4fe1\u606f\uff0c\u751f\u6210\u7ed3\u6784\u5316\u6458\u8981\u4ee5\u652f\u6301\u4e34\u5e8a\u56fe\u8868\u5ba1\u67e5\u3002", "motivation": "\u4e34\u5e8a\u533b\u751f\u9700\u8981\u4ece\u788e\u7247\u5316\u7684\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u754c\u9762\u4e2d\u62fc\u51d1\u51fa\u60a3\u8005\u95ee\u9898\u7684\u8fde\u8d2f\u753b\u9762\uff0c\u5305\u62ec\u836f\u7269\u3001\u8fd1\u671f\u5c31\u8bca\u548c\u957f\u671f\u8d8b\u52bf\u7b49\u4fe1\u606f\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u6548\u7387\u4f4e\u4e0b\u4e14\u5bb9\u6613\u51fa\u9519\u3002", "method": "\u7cfb\u7edf\u91c7\u7528FHIR R4\u6807\u51c6\uff0c\u68c0\u7d22\u76ee\u6807\u6027\u9ad8\u4ef7\u503c\u8d44\u6e90\uff0c\u5c06\u5176\u89c4\u8303\u5316\u4e3a\u4e00\u81f4\u7684\u4e34\u5e8a\u4e0a\u4e0b\u6587\u5305\uff0c\u7136\u540e\u751f\u6210\u7ed3\u6784\u5316\u6458\u8981\u3002\u652f\u6301\u6570\u636e\u6700\u5c0f\u5316\u3001\u65e0\u72b6\u6001\u5904\u7406\u548c\u7075\u6d3b\u90e8\u7f72\uff0c\u5305\u62ec\u5728\u7ec4\u7ec7\u4fe1\u4efb\u8fb9\u754c\u5185\u8fdb\u884c\u672c\u5730\u63a8\u7406\u3002", "result": "\u539f\u578b\u5728\u5408\u6210\u548c\u6d4b\u8bd5FHIR\u73af\u5883\u4e2d\u5c55\u793a\u4e86\u7aef\u5230\u7aef\u884c\u4e3a\u548c\u8f93\u51fa\u683c\u5f0f\uff0c\u4f46\u672a\u62a5\u544a\u4e34\u5e8a\u7ed3\u679c\u6216\u53d7\u63a7\u5de5\u4f5c\u6d41\u7a0b\u7814\u7a76\u3002\u63d0\u51fa\u4e86\u4ee5\u5fe0\u5b9e\u6027\u3001\u9057\u6f0f\u98ce\u9669\u3001\u65f6\u95f4\u6b63\u786e\u6027\u3001\u53ef\u7528\u6027\u548c\u64cd\u4f5c\u76d1\u63a7\u4e3a\u4e2d\u5fc3\u7684\u8bc4\u4ef7\u8ba1\u5212\u3002", "conclusion": "EHRSummarizer\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9690\u79c1\u611f\u77e5\u7684FHIR\u539f\u751f\u67b6\u6784\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u6458\u8981\u652f\u6301\u4e34\u5e8a\u56fe\u8868\u5ba1\u67e5\uff0c\u540c\u65f6\u901a\u8fc7\u7ea6\u675f\u6458\u8981\u8303\u56f4\u3001\u907f\u514d\u8bca\u65ad\u5efa\u8bae\u548c\u5f3a\u8c03\u6570\u636e\u6700\u5c0f\u5316\u6765\u964d\u4f4e\u98ce\u9669\u3002"}}
{"id": "2601.01023", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.01023", "abs": "https://arxiv.org/abs/2601.01023", "authors": ["Jo\u00e3o Morais", "Sadjad Alikhani", "Akshay Malhotra", "Shahab Hamidi-Rad", "Ahmed Alkhateeb"], "title": "Wireless Dataset Similarity: Measuring Distances in Supervised and Unsupervised Machine Learning", "comment": "resources available in: https://www.wi-lab.net/research/dataset-similarity", "summary": "This paper introduces a task- and model-aware framework for measuring similarity between wireless datasets, enabling applications such as dataset selection/augmentation, simulation-to-real (sim2real) comparison, task-specific synthetic data generation, and informing decisions on model training/adaptation to new deployments. We evaluate candidate dataset distance metrics by how well they predict cross-dataset transferability: if two datasets have a small distance, a model trained on one should perform well on the other. We apply the framework on an unsupervised task, channel state information (CSI) compression, using autoencoders. Using metrics based on UMAP embeddings, combined with Wasserstein and Euclidean distances, we achieve Pearson correlations exceeding 0.85 between dataset distances and train-on-one/test-on-another task performance. We also apply the framework to a supervised beam prediction in the downlink using convolutional neural networks. For this task, we derive a label-aware distance by integrating supervised UMAP and penalties for dataset imbalance. Across both tasks, the resulting distances outperform traditional baselines and consistently exhibit stronger correlations with model transferability, supporting task-relevant comparisons between wireless datasets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4efb\u52a1\u548c\u6a21\u578b\u611f\u77e5\u7684\u65e0\u7ebf\u6570\u636e\u96c6\u76f8\u4f3c\u6027\u5ea6\u91cf\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u8de8\u6570\u636e\u96c6\u7684\u53ef\u8fc1\u79fb\u6027\uff0c\u5e76\u5728CSI\u538b\u7f29\u548c\u6ce2\u675f\u9884\u6d4b\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u65e0\u7ebf\u901a\u4fe1\u4e2d\u9700\u8981\u6bd4\u8f83\u4e0d\u540c\u6570\u636e\u96c6\u7684\u76f8\u4f3c\u6027\uff0c\u4ee5\u652f\u6301\u6570\u636e\u96c6\u9009\u62e9/\u589e\u5f3a\u3001\u4eff\u771f\u5230\u771f\u5b9e\u73af\u5883\u6bd4\u8f83\u3001\u4efb\u52a1\u7279\u5b9a\u5408\u6210\u6570\u636e\u751f\u6210\u7b49\u5e94\u7528\uff0c\u4f46\u7f3a\u4e4f\u6709\u6548\u7684\u5ea6\u91cf\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4efb\u52a1\u548c\u6a21\u578b\u611f\u77e5\u7684\u6846\u67b6\uff0c\u4f7f\u7528UMAP\u5d4c\u5165\u7ed3\u5408Wasserstein\u548c\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u5ea6\u91cf\u6570\u636e\u96c6\u8ddd\u79bb\uff0c\u5e76\u901a\u8fc7\u76d1\u7763UMAP\u548c\u6570\u636e\u96c6\u4e0d\u5e73\u8861\u60e9\u7f5a\u6765\u63a8\u5bfc\u6807\u7b7e\u611f\u77e5\u8ddd\u79bb\u3002", "result": "\u5728CSI\u538b\u7f29\u4efb\u52a1\u4e2d\uff0c\u6570\u636e\u96c6\u8ddd\u79bb\u4e0e\u8de8\u6570\u636e\u96c6\u6027\u80fd\u7684Pearson\u76f8\u5173\u7cfb\u6570\u8d85\u8fc70.85\uff1b\u5728\u6ce2\u675f\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0c\u63d0\u51fa\u7684\u8ddd\u79bb\u5ea6\u91cf\u4f18\u4e8e\u4f20\u7edf\u57fa\u7ebf\uff0c\u4e0e\u6a21\u578b\u53ef\u8fc1\u79fb\u6027\u6709\u66f4\u5f3a\u76f8\u5173\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u5ea6\u91cf\u65e0\u7ebf\u6570\u636e\u96c6\u7684\u76f8\u4f3c\u6027\uff0c\u652f\u6301\u4efb\u52a1\u76f8\u5173\u7684\u6570\u636e\u96c6\u6bd4\u8f83\uff0c\u4e3a\u6a21\u578b\u8bad\u7ec3\u548c\u9002\u5e94\u65b0\u90e8\u7f72\u63d0\u4f9b\u51b3\u7b56\u4f9d\u636e\u3002"}}
{"id": "2601.01685", "categories": ["cs.CL", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.01685", "abs": "https://arxiv.org/abs/2601.01685", "authors": ["Jinwei Hu", "Xinmiao Huang", "Youcheng Sun", "Yi Dong", "Xiaowei Huang"], "title": "Lying with Truths: Open-Channel Multi-Agent Collusion for Belief Manipulation via Generative Montage", "comment": "Under Review", "summary": "As large language models (LLMs) transition to autonomous agents synthesizing real-time information, their reasoning capabilities introduce an unexpected attack surface. This paper introduces a novel threat where colluding agents steer victim beliefs using only truthful evidence fragments distributed through public channels, without relying on covert communications, backdoors, or falsified documents. By exploiting LLMs' overthinking tendency, we formalize the first cognitive collusion attack and propose Generative Montage: a Writer-Editor-Director framework that constructs deceptive narratives through adversarial debate and coordinated posting of evidence fragments, causing victims to internalize and propagate fabricated conclusions. To study this risk, we develop CoPHEME, a dataset derived from real-world rumor events, and simulate attacks across diverse LLM families. Our results show pervasive vulnerability across 14 LLM families: attack success rates reach 74.4% for proprietary models and 70.6% for open-weights models. Counterintuitively, stronger reasoning capabilities increase susceptibility, with reasoning-specialized models showing higher attack success than base models or prompts. Furthermore, these false beliefs then cascade to downstream judges, achieving over 60% deception rates, highlighting a socio-technical vulnerability in how LLM-based agents interact with dynamic information environments. Our implementation and data are available at: https://github.com/CharlesJW222/Lying_with_Truth/tree/main.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u8ba4\u77e5\u5171\u8c0b\u653b\u51fb\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8fc7\u5ea6\u601d\u8003\u503e\u5411\uff0c\u901a\u8fc7\u4ec5\u4f7f\u7528\u771f\u5b9e\u8bc1\u636e\u7247\u6bb5\u5728\u516c\u5f00\u6e20\u9053\u6784\u5efa\u6b3a\u9a97\u6027\u53d9\u8ff0\uff0c\u4f7f\u53d7\u5bb3\u8005\u5185\u5316\u5e76\u4f20\u64ad\u865a\u5047\u7ed3\u8bba\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5411\u81ea\u4e3b\u4ee3\u7406\u8fc7\u6e21\uff0c\u5176\u63a8\u7406\u80fd\u529b\u5f15\u5165\u4e86\u4e00\u4e2a\u610f\u5916\u7684\u653b\u51fb\u9762\u3002\u73b0\u6709\u653b\u51fb\u901a\u5e38\u4f9d\u8d56\u9690\u853d\u901a\u4fe1\u3001\u540e\u95e8\u6216\u4f2a\u9020\u6587\u6863\uff0c\u800c\u672c\u6587\u7814\u7a76\u4ec5\u4f7f\u7528\u771f\u5b9e\u8bc1\u636e\u7247\u6bb5\u901a\u8fc7\u516c\u5f00\u6e20\u9053\u8fdb\u884c\u7684\u8ba4\u77e5\u5171\u8c0b\u653b\u51fb\u3002", "method": "\u63d0\u51fa\u751f\u6210\u8499\u592a\u5947\u6846\u67b6\uff1aWriter-Editor-Director\u4e09\u5c42\u67b6\u6784\uff0c\u901a\u8fc7\u5bf9\u6297\u6027\u8fa9\u8bba\u548c\u534f\u8c03\u53d1\u5e03\u8bc1\u636e\u7247\u6bb5\u6784\u5efa\u6b3a\u9a97\u6027\u53d9\u8ff0\u3002\u5f00\u53d1CoPHEME\u6570\u636e\u96c6\uff08\u57fa\u4e8e\u771f\u5b9e\u8c23\u8a00\u4e8b\u4ef6\uff09\uff0c\u572814\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u5bb6\u65cf\u4e2d\u6a21\u62df\u653b\u51fb\u3002", "result": "\u653b\u51fb\u6210\u529f\u7387\u9ad8\u8fbe74.4%\uff08\u4e13\u6709\u6a21\u578b\uff09\u548c70.6%\uff08\u5f00\u6e90\u6a21\u578b\uff09\u3002\u53cd\u76f4\u89c9\u7684\u662f\uff0c\u66f4\u5f3a\u7684\u63a8\u7406\u80fd\u529b\u53cd\u800c\u589e\u52a0\u6613\u53d7\u653b\u51fb\u6027\uff0c\u63a8\u7406\u4e13\u7528\u6a21\u578b\u6bd4\u57fa\u7840\u6a21\u578b\u6216\u63d0\u793a\u66f4\u6613\u53d7\u653b\u51fb\u3002\u865a\u5047\u4fe1\u5ff5\u4f1a\u5411\u4e0b\u6e38\u4f20\u64ad\uff0c\u6b3a\u9a97\u7387\u8d85\u8fc760%\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5728\u52a8\u6001\u4fe1\u606f\u73af\u5883\u4e2d\u5b58\u5728\u793e\u4f1a\u6280\u672f\u6f0f\u6d1e\uff0c\u4ec5\u4f7f\u7528\u771f\u5b9e\u8bc1\u636e\u7247\u6bb5\u5c31\u80fd\u6210\u529f\u5b9e\u65bd\u8ba4\u77e5\u5171\u8c0b\u653b\u51fb\u3002\u66f4\u5f3a\u7684\u63a8\u7406\u80fd\u529b\u53cd\u800c\u589e\u52a0\u8106\u5f31\u6027\uff0c\u8fd9\u63ed\u793a\u4e86LLM\u4ee3\u7406\u4ea4\u4e92\u4e2d\u7684\u65b0\u5b89\u5168\u98ce\u9669\u3002"}}
{"id": "2601.01045", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01045", "abs": "https://arxiv.org/abs/2601.01045", "authors": ["Tatsuaki Tsuruyama"], "title": "Coarse-Grained Kullback--Leibler Control of Diffusion-Based Generative AI", "comment": null, "summary": "Diffusion models and score-based generative models provide a powerful framework for synthesizing high-quality images from noise. However, there is still no satisfactory theory that describes how coarse-grained quantities, such as blockwise intensity or class proportions after partitioning an image into spatial blocks, are preserved and evolve along the reverse diffusion dynamics. In previous work, the author introduced an information-theoretic Lyapunov function V for non-ergodic Markov processes on a state space partitioned into blocks, defined as the minimal Kullback-Leibler divergence to the set of stationary distributions reachable from a given initial condition, and showed that a leak-tolerant potential V-delta with a prescribed tolerance for block masses admits a closed-form expression as a scaling-and-clipping operation on block masses.\n  In this paper, I transplant this framework to the reverse diffusion process in generative models and propose a reverse diffusion scheme that is projected by the potential V-delta (referred to as the V-delta projected reverse diffusion). I extend the monotonicity of V to time-inhomogeneous block-preserving Markov kernels and show that, under small leakage and the V-delta projection, V-delta acts as an approximate Lyapunov function. Furthermore, using a toy model consisting of block-constant images and a simplified reverse kernel, I numerically demonstrate that the proposed method keeps the block-mass error and the leak-tolerant potential within the prescribed tolerance, while achieving pixel-wise accuracy and visual quality comparable to the non-projected dynamics. This study reinterprets generative sampling as a decrease of an information potential from noise to data, and provides a design principle for reverse diffusion processes with explicit control of coarse-grained quantities.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4fe1\u606f\u8bbaLyapunov\u51fd\u6570V-\u03b4\u7684\u6295\u5f71\u53cd\u5411\u6269\u6563\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u751f\u6210\u6a21\u578b\u4e2d\u663e\u5f0f\u63a7\u5236\u7c97\u7c92\u5ea6\u7edf\u8ba1\u91cf\uff08\u5982\u56fe\u50cf\u5757\u5f3a\u5ea6\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u50cf\u7d20\u7ea7\u7cbe\u5ea6\u548c\u89c6\u89c9\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u7f3a\u4e4f\u7406\u8bba\u63cf\u8ff0\u7c97\u7c92\u5ea6\u7edf\u8ba1\u91cf\uff08\u5982\u7a7a\u95f4\u5757\u5f3a\u5ea6\uff09\u5728\u53cd\u5411\u6269\u6563\u8fc7\u7a0b\u4e2d\u7684\u6f14\u5316\u673a\u5236\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u663e\u5f0f\u63a7\u5236\u8fd9\u4e9b\u7edf\u8ba1\u91cf\u7684\u65b9\u6cd5\u3002", "method": "\u79fb\u690d\u4fe1\u606f\u8bbaLyapunov\u51fd\u6570V-\u03b4\u6846\u67b6\u5230\u751f\u6210\u6a21\u578b\u7684\u53cd\u5411\u6269\u6563\u8fc7\u7a0b\uff0c\u63d0\u51faV-\u03b4\u6295\u5f71\u53cd\u5411\u6269\u6563\u65b9\u6848\uff0c\u6269\u5c55V\u7684\u5355\u8c03\u6027\u5230\u65f6\u95f4\u975e\u9f50\u6b21\u5757\u4fdd\u6301\u9a6c\u5c14\u53ef\u592b\u6838\uff0c\u5e76\u5728\u5757\u5e38\u6570\u56fe\u50cf\u548c\u7b80\u5316\u53cd\u5411\u6838\u7684\u73a9\u5177\u6a21\u578b\u4e2d\u8fdb\u884c\u6570\u503c\u9a8c\u8bc1\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u80fd\u5c06\u5757\u8d28\u91cf\u8bef\u5dee\u548c\u6cc4\u6f0f\u5bb9\u5fcd\u52bf\u51fd\u6570\u63a7\u5236\u5728\u9884\u8bbe\u5bb9\u5dee\u5185\uff0c\u540c\u65f6\u8fbe\u5230\u4e0e\u975e\u6295\u5f71\u52a8\u529b\u5b66\u76f8\u5f53\u7684\u50cf\u7d20\u7ea7\u7cbe\u5ea6\u548c\u89c6\u89c9\u8d28\u91cf\u3002", "conclusion": "\u5c06\u751f\u6210\u91c7\u6837\u91cd\u65b0\u89e3\u91ca\u4e3a\u4ece\u566a\u58f0\u5230\u6570\u636e\u7684\u4fe1\u606f\u52bf\u51fd\u6570\u4e0b\u964d\u8fc7\u7a0b\uff0c\u4e3a\u5177\u6709\u663e\u5f0f\u7c97\u7c92\u5ea6\u7edf\u8ba1\u91cf\u63a7\u5236\u7684\u53cd\u5411\u6269\u6563\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u539f\u5219\u3002"}}
{"id": "2601.01910", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01910", "abs": "https://arxiv.org/abs/2601.01910", "authors": ["Minh Hieu Ha", "Khanh Ly Ta", "Hung Phan", "Tung Doan", "Tung Dao", "Dao Tran", "Huynh Thi Thanh Binh"], "title": "MMP-A*: Multimodal Perception Enhanced Incremental Heuristic Search on Path Planning", "comment": null, "summary": "Autonomous path planning requires a synergy between global reasoning and geometric precision, especially in complex or cluttered environments. While classical A* is valued for its optimality, it incurs prohibitive computational and memory costs in large-scale scenarios. Recent attempts to mitigate these limitations by using Large Language Models for waypoint guidance remain insufficient, as they rely only on text-based reasoning without spatial grounding. As a result, such models often produce incorrect waypoints in topologically complex environments with dead ends, and lack the perceptual capacity to interpret ambiguous physical boundaries. These inconsistencies lead to costly corrective expansions and undermine the intended computational efficiency.\n  We introduce MMP-A*, a multimodal framework that integrates the spatial grounding capabilities of vision-language models with a novel adaptive decay mechanism. By anchoring high-level reasoning in physical geometry, the framework produces coherent waypoint guidance that addresses the limitations of text-only planners. The adaptive decay mechanism dynamically regulates the influence of uncertain waypoints within the heuristic, ensuring geometric validity while substantially reducing memory overhead. To evaluate robustness, we test the framework in challenging environments characterized by severe clutter and topological complexity. Experimental results show that MMP-A* achieves near-optimal trajectories with significantly reduced operational costs, demonstrating its potential as a perception-grounded and computationally efficient paradigm for autonomous navigation.", "AI": {"tldr": "MMP-A*\uff1a\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7a7a\u95f4\u611f\u77e5\u4e0e\u81ea\u9002\u5e94\u8870\u51cf\u673a\u5236\u7684\u591a\u6a21\u6001\u8def\u5f84\u89c4\u5212\u6846\u67b6\uff0c\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u8f68\u8ff9\u5e76\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c", "motivation": "\u4f20\u7edfA*\u7b97\u6cd5\u5728\u5927\u89c4\u6a21\u573a\u666f\u4e2d\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\u8fc7\u9ad8\uff0c\u800c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u7f3a\u4e4f\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\uff0c\u5728\u62d3\u6251\u590d\u6742\u73af\u5883\u4e2d\u5bb9\u6613\u4ea7\u751f\u9519\u8bef\u8def\u5f84\u70b9\uff0c\u5bfc\u81f4\u6602\u8d35\u7684\u4fee\u6b63\u6269\u5c55", "method": "\u63d0\u51faMMP-A*\u591a\u6a21\u6001\u6846\u67b6\uff0c\u96c6\u6210\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\u4e0e\u65b0\u9896\u7684\u81ea\u9002\u5e94\u8870\u51cf\u673a\u5236\uff0c\u5c06\u9ad8\u5c42\u63a8\u7406\u951a\u5b9a\u5728\u7269\u7406\u51e0\u4f55\u4e2d\uff0c\u52a8\u6001\u8c03\u8282\u4e0d\u786e\u5b9a\u8def\u5f84\u70b9\u5728\u542f\u53d1\u5f0f\u51fd\u6570\u4e2d\u7684\u5f71\u54cd", "result": "\u5728\u5177\u6709\u4e25\u91cd\u6742\u4e71\u548c\u62d3\u6251\u590d\u6742\u6027\u7684\u6311\u6218\u6027\u73af\u5883\u4e2d\u6d4b\u8bd5\uff0cMMP-A*\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u8f68\u8ff9\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u64cd\u4f5c\u6210\u672c", "conclusion": "MMP-A*\u5c55\u793a\u4e86\u4f5c\u4e3a\u611f\u77e5\u57fa\u7840\u548c\u8ba1\u7b97\u9ad8\u6548\u7684\u81ea\u4e3b\u5bfc\u822a\u8303\u5f0f\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u7a7a\u95f4\u611f\u77e5\u548c\u81ea\u9002\u5e94\u673a\u5236\u89e3\u51b3\u4e86\u7eaf\u6587\u672c\u89c4\u5212\u5668\u7684\u5c40\u9650\u6027"}}
{"id": "2601.01708", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01708", "abs": "https://arxiv.org/abs/2601.01708", "authors": ["Unggi Lee", "Joo Young Kim", "Ran Ju", "Minyoung Jung", "Jeyeon Eo"], "title": "A Training-Free Large Reasoning Model-based Knowledge Tracing Framework for Unified Prediction and Prescription", "comment": null, "summary": "Knowledge Tracing (KT) aims to estimate a learner's evolving mastery based on interaction histories. Recent studies have explored Large Language Models (LLMs) for KT via autoregressive nature, but such approaches typically require fine-tuning and exhibit unstable or near-random performance. Moreover, prior KT systems primarily focus on prediction and rely on multi-stage pipelines for feedback and recommendation, resulting in increased system complexity and resources. To address this gap, we propose Thinking-KT, a training-free KT framework that incorporates Test-Time Scaling (TTS), enabling even small LLMs to achieve competitive KT performance. Moreover, in this framework, a small LLM can jointly perform KT prediction, personalized feedback generation, and learning recommendation in a unified output without degrading prediction accuracy. Beyond performance, we present the systematic analysis of reasoning traces in KT. Our results demonstrate that TTS is a critical yet underexplored factor in LLM-based KT, and that small LLMs can serve as unified ITS engines.", "AI": {"tldr": "\u63d0\u51faThinking-KT\u6846\u67b6\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u8ba9\u5c0f\u578bLLM\u5728\u77e5\u8bc6\u8ffd\u8e2a\u4efb\u52a1\u4e2d\u8fbe\u5230\u7ade\u4e89\u6027\u6027\u80fd\uff0c\u5e76\u80fd\u7edf\u4e00\u6267\u884c\u9884\u6d4b\u3001\u53cd\u9988\u548c\u63a8\u8350\u4efb\u52a1", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u77e5\u8bc6\u8ffd\u8e2a\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5fae\u8c03\u4e14\u6027\u80fd\u4e0d\u7a33\u5b9a\uff0c\u540c\u65f6\u4f20\u7edfKT\u7cfb\u7edf\u4f9d\u8d56\u591a\u9636\u6bb5\u6d41\u7a0b\u5b9e\u73b0\u53cd\u9988\u548c\u63a8\u8350\uff0c\u5bfc\u81f4\u7cfb\u7edf\u590d\u6742\u5ea6\u548c\u8d44\u6e90\u6d88\u8017\u589e\u52a0", "method": "\u63d0\u51faThinking-KT\u8bad\u7ec3\u514d\u8d39\u6846\u67b6\uff0c\u7ed3\u5408\u6d4b\u8bd5\u65f6\u7f29\u653e\u6280\u672f\uff0c\u4f7f\u5c0f\u578bLLM\u80fd\u591f\u7edf\u4e00\u6267\u884c\u77e5\u8bc6\u8ffd\u8e2a\u9884\u6d4b\u3001\u4e2a\u6027\u5316\u53cd\u9988\u751f\u6210\u548c\u5b66\u4e60\u63a8\u8350", "result": "\u6d4b\u8bd5\u65f6\u7f29\u653e\u662fLLM-based KT\u4e2d\u88ab\u5ffd\u89c6\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5c0f\u578bLLM\u53ef\u4f5c\u4e3a\u7edf\u4e00\u667a\u80fd\u6559\u5b66\u7cfb\u7edf\u5f15\u64ce\uff0c\u5728\u4fdd\u6301\u9884\u6d4b\u51c6\u786e\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u591a\u529f\u80fd\u8f93\u51fa", "conclusion": "Thinking-KT\u6846\u67b6\u5c55\u793a\u4e86\u5c0f\u578bLLM\u4f5c\u4e3a\u7edf\u4e00ITS\u5f15\u64ce\u7684\u6f5c\u529b\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u7ade\u4e89\u6027KT\u6027\u80fd\uff0c\u4e3a\u77e5\u8bc6\u8ffd\u8e2a\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2601.01061", "categories": ["cs.LG", "cs.AI", "math.PR"], "pdf": "https://arxiv.org/pdf/2601.01061", "abs": "https://arxiv.org/abs/2601.01061", "authors": ["Yajing Liu", "Erkao Bao", "Linqi Song"], "title": "A UCB Bandit Algorithm for General ML-Based Estimators", "comment": "15 pages, 4 figures, 1 table, Multi-Arm bandit, psi-UCB, generalized machine learning models", "summary": "We present ML-UCB, a generalized upper confidence bound algorithm that integrates arbitrary machine learning models into multi-armed bandit frameworks. A fundamental challenge in deploying sophisticated ML models for sequential decision-making is the lack of tractable concentration inequalities required for principled exploration. We overcome this limitation by directly modeling the learning curve behavior of the underlying estimator. Specifically, assuming the Mean Squared Error decreases as a power law in the number of training samples, we derive a generalized concentration inequality and prove that ML-UCB achieves sublinear regret. This framework enables the principled integration of any ML model whose learning curve can be empirically characterized, eliminating the need for model-specific theoretical analysis. We validate our approach through experiments on a collaborative filtering recommendation system using online matrix factorization with synthetic data designed to simulate a simplified two-tower model, demonstrating substantial improvements over LinUCB", "AI": {"tldr": "ML-UCB\u7b97\u6cd5\u5c06\u4efb\u610f\u673a\u5668\u5b66\u4e60\u6a21\u578b\u96c6\u6210\u5230\u591a\u81c2\u8001\u864e\u673a\u6846\u67b6\u4e2d\uff0c\u901a\u8fc7\u76f4\u63a5\u5efa\u6a21\u4f30\u8ba1\u5668\u7684\u5b66\u4e60\u66f2\u7ebf\u884c\u4e3a\uff0c\u65e0\u9700\u6a21\u578b\u7279\u5b9a\u7684\u7406\u8bba\u5206\u6790\u5373\u53ef\u5b9e\u73b0\u6b21\u7ebf\u6027\u9057\u61be", "motivation": "\u5728\u5e8f\u5217\u51b3\u7b56\u4e2d\u90e8\u7f72\u590d\u6742ML\u6a21\u578b\u7684\u4e3b\u8981\u6311\u6218\u662f\u7f3a\u4e4f\u53ef\u5904\u7406\u7684\u96c6\u4e2d\u4e0d\u7b49\u5f0f\u6765\u8fdb\u884c\u6709\u539f\u5219\u7684\u63a2\u7d22\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u9488\u5bf9\u7279\u5b9a\u6a21\u578b\u8fdb\u884c\u7406\u8bba\u5206\u6790\uff0c\u9650\u5236\u4e86\u901a\u7528\u6027\u3002", "method": "\u63d0\u51faML-UCB\u7b97\u6cd5\uff0c\u5047\u8bbe\u5747\u65b9\u8bef\u5dee\u968f\u8bad\u7ec3\u6837\u672c\u6570\u5448\u5e42\u5f8b\u4e0b\u964d\uff0c\u63a8\u5bfc\u51fa\u5e7f\u4e49\u96c6\u4e2d\u4e0d\u7b49\u5f0f\uff0c\u5e76\u8bc1\u660e\u7b97\u6cd5\u80fd\u8fbe\u5230\u6b21\u7ebf\u6027\u9057\u61be\u3002\u8be5\u65b9\u6cd5\u4ec5\u9700\u7ecf\u9a8c\u6027\u5730\u8868\u5f81\u6a21\u578b\u7684\u5b66\u4e60\u66f2\u7ebf\u3002", "result": "\u5728\u534f\u540c\u8fc7\u6ee4\u63a8\u8350\u7cfb\u7edf\u7684\u5b9e\u9a8c\u4e2d\uff0c\u4f7f\u7528\u5728\u7ebf\u77e9\u9635\u5206\u89e3\u548c\u6a21\u62df\u7b80\u5316\u53cc\u5854\u6a21\u578b\u7684\u5408\u6210\u6570\u636e\uff0cML-UCB\u76f8\u6bd4LinUCB\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "ML-UCB\u4e3a\u4efb\u610f\u673a\u5668\u5b66\u4e60\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u539f\u5219\u7684\u591a\u81c2\u8001\u864e\u673a\u96c6\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5efa\u6a21\u5b66\u4e60\u66f2\u7ebf\u884c\u4e3a\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u6a21\u578b\u7279\u5b9a\u7406\u8bba\u5206\u6790\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u901a\u7528\u4e14\u6709\u6548\u7684\u5e8f\u5217\u51b3\u7b56\u3002"}}
{"id": "2601.01939", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01939", "abs": "https://arxiv.org/abs/2601.01939", "authors": ["Victor Sanchez", "Chris Reinke", "Ahamed Mohamed", "Xavier Alameda-Pineda"], "title": "OpenSocInt: A Multi-modal Training Environment for Human-Aware Social Navigation", "comment": null, "summary": "In this paper, we introduce OpenSocInt, an open-source software package providing a simulator for multi-modal social interactions and a modular architecture to train social agents. We described the software package and showcased its interest via an experimental protocol based on the task of social navigation. Our framework allows for exploring the use of different perceptual features, their encoding and fusion, as well as the use of different agents. The software is already publicly available under GPL at https://gitlab.inria.fr/robotlearn/OpenSocInt/.", "AI": {"tldr": "OpenSocInt\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u591a\u6a21\u6001\u793e\u4ea4\u4ea4\u4e92\u4eff\u771f\u5668\uff0c\u63d0\u4f9b\u6a21\u5757\u5316\u67b6\u6784\u8bad\u7ec3\u793e\u4ea4\u667a\u80fd\u4f53\uff0c\u5df2\u5e94\u7528\u4e8e\u793e\u4ea4\u5bfc\u822a\u4efb\u52a1\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u5f00\u6e90\u7684\u591a\u6a21\u6001\u793e\u4ea4\u4ea4\u4e92\u4eff\u771f\u5e73\u53f0\uff0c\u652f\u6301\u7814\u7a76\u4e0d\u540c\u611f\u77e5\u7279\u5f81\u3001\u7f16\u7801\u878d\u5408\u65b9\u6cd5\u4ee5\u53ca\u591a\u79cd\u667a\u80fd\u4f53\u5728\u793e\u4ea4\u4ea4\u4e92\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u7684\u5f00\u6e90\u8f6f\u4ef6\u5305\uff0c\u5305\u542b\u591a\u6a21\u6001\u793e\u4ea4\u4ea4\u4e92\u4eff\u771f\u5668\u548c\u53ef\u6269\u5c55\u7684\u667a\u80fd\u4f53\u8bad\u7ec3\u67b6\u6784\uff0c\u901a\u8fc7\u793e\u4ea4\u5bfc\u822a\u4efb\u52a1\u9a8c\u8bc1\u5176\u529f\u80fd\u3002", "result": "\u6210\u529f\u5f00\u53d1\u4e86OpenSocInt\u8f6f\u4ef6\u5305\uff0c\u5df2\u5728GitLab\u4e0a\u4ee5GPL\u534f\u8bae\u5f00\u6e90\uff0c\u5c55\u793a\u4e86\u5176\u5728\u793e\u4ea4\u5bfc\u822a\u4efb\u52a1\u4e2d\u7684\u5b9e\u7528\u4ef7\u503c\u3002", "conclusion": "OpenSocInt\u4e3a\u591a\u6a21\u6001\u793e\u4ea4\u4ea4\u4e92\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u7684\u5f00\u6e90\u5e73\u53f0\uff0c\u652f\u6301\u63a2\u7d22\u4e0d\u540c\u611f\u77e5\u7279\u5f81\u548c\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u4fc3\u8fdb\u793e\u4ea4\u667a\u80fd\u7814\u7a76\u7684\u53d1\u5c55\u3002"}}
{"id": "2601.01739", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01739", "abs": "https://arxiv.org/abs/2601.01739", "authors": ["Eunbi Choi", "Kibong Choi", "Seokhee Hong", "Junwon Hwang", "Hyojin Jeon", "Hyunjik Jo", "Joonkee Kim", "Seonghwan Kim", "Soyeon Kim", "Sunkyoung Kim", "Yireun Kim", "Yongil Kim", "Haeju Lee", "Jinsik Lee", "Kyungmin Lee", "Sangha Park", "Heuiyeen Yeen", "Hwan Chang", "Stanley Jungkyu Choi", "Yejin Choi", "Jiwon Ham", "Kijeong Jeon", "Geunyeong Jeong", "Gerrard Jeongwon Jo", "Yonghwan Jo", "Jiyeon Jung", "Naeun Kang", "Dohoon Kim", "Euisoon Kim", "Hayeon Kim", "Hyosang Kim", "Hyunseo Kim", "Jieun Kim", "Minu Kim", "Myoungshin Kim", "Unsol Kim", "Youchul Kim", "YoungJin Kim", "Chaeeun Lee", "Chaeyoon Lee", "Changhun Lee", "Dahm Lee", "Edward Hwayoung Lee", "Honglak Lee", "Jinsang Lee", "Jiyoung Lee", "Sangeun Lee", "Seungwon Lim", "Solji Lim", "Woohyung Lim", "Chanwoo Moon", "Jaewoo Park", "Jinho Park", "Yongmin Park", "Hyerin Seo", "Wooseok Seo", "Yongwoo Song", "Sejong Yang", "Sihoon Yang", "Chang En Yea", "Sihyuk Yi", "Chansik Yoon", "Dongkeun Yoon", "Sangyeon Yoon", "Hyeongu Yun"], "title": "K-EXAONE Technical Report", "comment": "29 pages", "summary": "This technical report presents K-EXAONE, a large-scale multilingual language model developed by LG AI Research. K-EXAONE is built on a Mixture-of-Experts architecture with 236B total parameters, activating 23B parameters during inference. It supports a 256K-token context window and covers six languages: Korean, English, Spanish, German, Japanese, and Vietnamese. We evaluate K-EXAONE on a comprehensive benchmark suite spanning reasoning, agentic, general, Korean, and multilingual abilities. Across these evaluations, K-EXAONE demonstrates performance comparable to open-weight models of similar size. K-EXAONE, designed to advance AI for a better life, is positioned as a powerful proprietary AI foundation model for a wide range of industrial and research applications.", "AI": {"tldr": "LG AI Research\u5f00\u53d1\u4e86K-EXAONE\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8eMoE\u67b6\u6784\u76842360\u4ebf\u53c2\u6570\u591a\u8bed\u8a00\u5927\u6a21\u578b\uff0c\u63a8\u7406\u65f6\u6fc0\u6d3b230\u4ebf\u53c2\u6570\uff0c\u652f\u6301256K\u4e0a\u4e0b\u6587\u957f\u5ea6\u548c6\u79cd\u8bed\u8a00\uff0c\u5728\u591a\u9879\u8bc4\u6d4b\u4e2d\u8868\u73b0\u4e0e\u540c\u89c4\u6a21\u5f00\u6e90\u6a21\u578b\u76f8\u5f53\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u5f3a\u5927\u7684\u4e13\u6709AI\u57fa\u7840\u6a21\u578b\uff0c\u652f\u6301\u591a\u79cd\u8bed\u8a00\uff0c\u7528\u4e8e\u5e7f\u6cdb\u7684\u5de5\u4e1a\u548c\u79d1\u7814\u5e94\u7528\uff0c\u63a8\u52a8AI\u6280\u672f\u53d1\u5c55\u4ee5\u6539\u5584\u751f\u6d3b\u3002", "method": "\u91c7\u7528\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\uff0c\u603b\u53c2\u65702360\u4ebf\uff0c\u63a8\u7406\u65f6\u6fc0\u6d3b230\u4ebf\u53c2\u6570\uff0c\u652f\u6301256K\u4ee4\u724c\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u8986\u76d6\u97e9\u8bed\u3001\u82f1\u8bed\u3001\u897f\u73ed\u7259\u8bed\u3001\u5fb7\u8bed\u3001\u65e5\u8bed\u548c\u8d8a\u5357\u8bed\u516d\u79cd\u8bed\u8a00\u3002", "result": "\u5728\u63a8\u7406\u3001\u667a\u80fd\u4f53\u3001\u901a\u7528\u80fd\u529b\u3001\u97e9\u8bed\u80fd\u529b\u548c\u591a\u8bed\u8a00\u80fd\u529b\u7b49\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cK-EXAONE\u8868\u73b0\u4e0e\u540c\u89c4\u6a21\u7684\u5f00\u6e90\u6743\u91cd\u6a21\u578b\u76f8\u5f53\u3002", "conclusion": "K-EXAONE\u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u4e13\u6709AI\u57fa\u7840\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u5de5\u4e1a\u548c\u79d1\u7814\u5e94\u7528\uff0c\u65e8\u5728\u901a\u8fc7AI\u6280\u672f\u6539\u5584\u751f\u6d3b\u3002"}}
{"id": "2601.01062", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01062", "abs": "https://arxiv.org/abs/2601.01062", "authors": ["Yunlin Zeng"], "title": "SPoRC-VIST: A Benchmark for Evaluating Generative Natural Narrative in Vision-Language Models", "comment": "14 pages, 3 figures. Accepted to WVAQ 2026, WACV 2026", "summary": "Vision-Language Models (VLMs) have achieved remarkable success in descriptive tasks such as image captioning and visual question answering (VQA). However, their ability to generate engaging, long-form narratives -- specifically multi-speaker podcast dialogues -- remains under-explored and difficult to evaluate. Standard metrics like BLEU and ROUGE fail to capture the nuances of conversational naturalness, personality, and narrative flow, often rewarding safe, repetitive outputs over engaging storytelling. In this work, we present a novel pipeline for end-to-end visual podcast generation, and fine-tune a Qwen3-VL-32B model on a curated dataset of 4,000 image-dialogue pairs. Crucially, we use a synthetic-to-real training strategy: we train on high-quality podcast dialogues from the Structured Podcast Research Corpus (SPoRC) paired with synthetically generated imagery, and evaluate on real-world photo sequences from the Visual Storytelling Dataset (VIST). This rigorous setup tests the model's ability to generalize from synthetic training data to real-world visual domains. We propose a comprehensive evaluation framework that moves beyond textual overlap, and use AI-as-a-judge (Gemini 3 Pro, Claude Opus 4.5, GPT 5.2) and novel style metrics (average turn length, speaker switch rate) to assess quality. Our experiments demonstrate that our fine-tuned 32B model significantly outperforms a 235B base model in conversational naturalness ($>$80\\% win rate) and narrative depth (+50\\% turn length), while maintaining identical visual grounding capabilities (CLIPScore: 20.39).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u89c6\u89c9\u64ad\u5ba2\u751f\u6210\u7ba1\u9053\uff0c\u901a\u8fc7\u5fae\u8c03Qwen3-VL-32B\u6a21\u578b\uff0c\u4f7f\u7528\u5408\u6210\u5230\u771f\u5b9e\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u89c6\u89c9\u53d9\u4e8b\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u8bdd\u81ea\u7136\u6027\u548c\u53d9\u4e8b\u6df1\u5ea6\u3002", "motivation": "\u867d\u7136\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u63cf\u8ff0\u6027\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u751f\u6210\u5f15\u4eba\u5165\u80dc\u7684\u957f\u7bc7\u53d9\u4e8b\uff08\u7279\u522b\u662f\u591a\u8bf4\u8bdd\u8005\u64ad\u5ba2\u5bf9\u8bdd\uff09\u65b9\u9762\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u4e14\u96be\u4ee5\u8bc4\u4f30\u3002\u73b0\u6709\u6807\u51c6\u6307\u6807\u65e0\u6cd5\u6355\u6349\u5bf9\u8bdd\u81ea\u7136\u6027\u3001\u4e2a\u6027\u548c\u53d9\u4e8b\u6d41\u7545\u6027\u7b49\u7ec6\u5fae\u5dee\u522b\u3002", "method": "1. \u63d0\u51fa\u7aef\u5230\u7aef\u89c6\u89c9\u64ad\u5ba2\u751f\u6210\u7ba1\u9053\uff1b2. \u57284000\u4e2a\u56fe\u50cf-\u5bf9\u8bdd\u5bf9\u6570\u636e\u96c6\u4e0a\u5fae\u8c03Qwen3-VL-32B\u6a21\u578b\uff1b3. \u91c7\u7528\u5408\u6210\u5230\u771f\u5b9e\u7684\u8bad\u7ec3\u7b56\u7565\uff1a\u5728SPoRC\u9ad8\u8d28\u91cf\u64ad\u5ba2\u5bf9\u8bdd\u4e0e\u5408\u6210\u751f\u6210\u56fe\u50cf\u4e0a\u8bad\u7ec3\uff0c\u5728VIST\u771f\u5b9e\u4e16\u754c\u7167\u7247\u5e8f\u5217\u4e0a\u8bc4\u4f30\uff1b4. \u63d0\u51fa\u7efc\u5408\u8bc4\u4f30\u6846\u67b6\uff0c\u4f7f\u7528AI\u4f5c\u4e3a\u8bc4\u5224\u548c\u65b0\u578b\u98ce\u683c\u6307\u6807\u3002", "result": "\u5fae\u8c03\u768432B\u6a21\u578b\u5728\u5bf9\u8bdd\u81ea\u7136\u6027\u4e0a\u663e\u8457\u4f18\u4e8e235B\u57fa\u7840\u6a21\u578b\uff08\u80dc\u7387>80%\uff09\uff0c\u53d9\u4e8b\u6df1\u5ea6\u63d0\u534750%\uff08\u5e73\u5747\u5bf9\u8bdd\u8f6e\u6b21\u957f\u5ea6\u589e\u52a0\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u540c\u7684\u89c6\u89c9\u57fa\u7840\u80fd\u529b\uff08CLIPScore: 20.39\uff09\u3002", "conclusion": "\u901a\u8fc7\u5408\u6210\u5230\u771f\u5b9e\u7684\u8bad\u7ec3\u7b56\u7565\u548c\u7efc\u5408\u8bc4\u4f30\u6846\u67b6\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u6709\u6548\u751f\u6210\u81ea\u7136\u3001\u6df1\u5165\u7684\u89c6\u89c9\u64ad\u5ba2\u5bf9\u8bdd\uff0c\u4e3a\u957f\u7bc7\u53d9\u4e8b\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01976", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01976", "abs": "https://arxiv.org/abs/2601.01976", "authors": ["Yasmine Souissi", "Fabrice Boissier", "Nida Meddouri"], "title": "CNC-TP: Classifier Nominal Concept Based on Top-Pertinent Attributes", "comment": null, "summary": "Knowledge Discovery in Databases (KDD) aims to exploit the vast amounts of data generated daily across various domains of computer applications. Its objective is to extract hidden and meaningful knowledge from datasets through a structured process comprising several key steps: data selection, preprocessing, transformation, data mining, and visualization. Among the core data mining techniques are classification and clustering. Classification involves predicting the class of new instances using a classifier trained on labeled data. Several approaches have been proposed in the literature, including Decision Tree Induction, Bayesian classifiers, Nearest Neighbor search, Neural Networks, Support Vector Machines, and Formal Concept Analysis (FCA). The last one is recognized as an effective approach for interpretable and explainable learning. It is grounded in the mathematical structure of the concept lattice, which enables the generation of formal concepts and the discovery of hidden relationships among them. In this paper, we present a state-of-theart review of FCA-based classifiers. We explore various methods for computing closure operators from nominal data and introduce a novel approach for constructing a partial concept lattice that focuses on the most relevant concepts. Experimental results are provided to demonstrate the efficiency of the proposed method.", "AI": {"tldr": "\u672c\u6587\u5bf9\u57fa\u4e8e\u5f62\u5f0f\u6982\u5ff5\u5206\u6790(FCA)\u7684\u5206\u7c7b\u5668\u8fdb\u884c\u4e86\u7efc\u8ff0\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u540d\u4e49\u6570\u636e\u8ba1\u7b97\u95ed\u5305\u7b97\u5b50\u7684\u65b0\u65b9\u6cd5\uff0c\u5e76\u6784\u5efa\u4e86\u5173\u6ce8\u6700\u76f8\u5173\u6982\u5ff5\u7684\u90e8\u5206\u6982\u5ff5\u683c\u3002", "motivation": "\u77e5\u8bc6\u53d1\u73b0(KDD)\u65e8\u5728\u4ece\u6d77\u91cf\u6570\u636e\u4e2d\u63d0\u53d6\u9690\u85cf\u77e5\u8bc6\uff0c\u5176\u4e2d\u5206\u7c7b\u662f\u6838\u5fc3\u6570\u636e\u6316\u6398\u6280\u672f\u4e4b\u4e00\u3002\u5f62\u5f0f\u6982\u5ff5\u5206\u6790(FCA)\u56e0\u5176\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u89e3\u91ca\u6027\u5b66\u4e60\u800c\u88ab\u8ba4\u4e3a\u662f\u6709\u6548\u7684\u5206\u7c7b\u65b9\u6cd5\uff0c\u4f46\u9700\u8981\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u5904\u7406\u540d\u4e49\u6570\u636e\u5e76\u6784\u5efa\u6982\u5ff5\u683c\u3002", "method": "1. \u5bf9FCA\u57fa\u5206\u7c7b\u5668\u8fdb\u884c\u6700\u65b0\u7efc\u8ff0\uff1b2. \u63a2\u7d22\u4ece\u540d\u4e49\u6570\u636e\u8ba1\u7b97\u95ed\u5305\u7b97\u5b50\u7684\u591a\u79cd\u65b9\u6cd5\uff1b3. \u63d0\u51fa\u6784\u5efa\u90e8\u5206\u6982\u5ff5\u683c\u7684\u65b0\u65b9\u6cd5\uff0c\u91cd\u70b9\u5173\u6ce8\u6700\u76f8\u5173\u7684\u6982\u5ff5\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6548\u7387\uff0c\u5c55\u793a\u4e86\u5728\u6784\u5efa\u6982\u5ff5\u683c\u548c\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u540d\u4e49\u6570\u636e\u5e76\u6784\u5efa\u90e8\u5206\u6982\u5ff5\u683c\uff0c\u4e3a\u57fa\u4e8eFCA\u7684\u5206\u7c7b\u5668\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u5b9e\u73b0\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u77e5\u8bc6\u53d1\u73b0\u8fc7\u7a0b\u4e2d\u7684\u5206\u7c7b\u6027\u80fd\u3002"}}
{"id": "2601.01745", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01745", "abs": "https://arxiv.org/abs/2601.01745", "authors": ["Hong Han", "Hao-Chen Pei", "Zhao-Zheng Nie", "Xin Luo", "Xin-Shun Xu"], "title": "Multi-granularity Interactive Attention Framework for Residual Hierarchical Pronunciation Assessment", "comment": "9 pages, 4 figures, 5 tables, accepted by AAAI 2026", "summary": "Automatic pronunciation assessment plays a crucial role in computer-assisted pronunciation training systems. Due to the ability to perform multiple pronunciation tasks simultaneously, multi-aspect multi-granularity pronunciation assessment methods are gradually receiving more attention and achieving better performance than single-level modeling tasks. However, existing methods only consider unidirectional dependencies between adjacent granularity levels, lacking bidirectional interaction among phoneme, word, and utterance levels and thus insufficiently capturing the acoustic structural correlations. To address this issue, we propose a novel residual hierarchical interactive method, HIA for short, that enables bidirectional modeling across granularities. As the core of HIA, the Interactive Attention Module leverages an attention mechanism to achieve dynamic bidirectional interaction, effectively capturing linguistic features at each granularity while integrating correlations between different granularity levels. We also propose a residual hierarchical structure to alleviate the feature forgetting problem when modeling acoustic hierarchies. In addition, we use 1-D convolutional layers to enhance the extraction of local contextual cues at each granularity. Extensive experiments on the speechocean762 dataset show that our model is comprehensively ahead of the existing state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51faHIA\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u5411\u4ea4\u4e92\u6ce8\u610f\u529b\u673a\u5236\u548c\u591a\u7c92\u5ea6\u5c42\u6b21\u7ed3\u6784\u6539\u8fdb\u53d1\u97f3\u8bc4\u4f30\uff0c\u5728speechocean762\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5", "motivation": "\u73b0\u6709\u53d1\u97f3\u8bc4\u4f30\u65b9\u6cd5\u53ea\u8003\u8651\u76f8\u90bb\u7c92\u5ea6\u7ea7\u522b\u7684\u5355\u5411\u4f9d\u8d56\u5173\u7cfb\uff0c\u7f3a\u4e4f\u97f3\u7d20\u3001\u5355\u8bcd\u548c\u8bdd\u8bed\u7ea7\u522b\u4e4b\u95f4\u7684\u53cc\u5411\u4ea4\u4e92\uff0c\u65e0\u6cd5\u5145\u5206\u6355\u6349\u58f0\u5b66\u7ed3\u6784\u76f8\u5173\u6027", "method": "\u63d0\u51fa\u6b8b\u5dee\u5c42\u6b21\u4ea4\u4e92\u65b9\u6cd5HIA\uff1a1\uff09\u4ea4\u4e92\u6ce8\u610f\u529b\u6a21\u5757\u5b9e\u73b0\u52a8\u6001\u53cc\u5411\u4ea4\u4e92\uff1b2\uff09\u6b8b\u5dee\u5c42\u6b21\u7ed3\u6784\u7f13\u89e3\u58f0\u5b66\u5c42\u6b21\u5efa\u6a21\u4e2d\u7684\u7279\u5f81\u9057\u5fd8\u95ee\u9898\uff1b3\uff09\u4f7f\u75281-D\u5377\u79ef\u5c42\u589e\u5f3a\u6bcf\u4e2a\u7c92\u5ea6\u7684\u5c40\u90e8\u4e0a\u4e0b\u6587\u7ebf\u7d22\u63d0\u53d6", "result": "\u5728speechocean762\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5168\u9762\u9886\u5148\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5", "conclusion": "HIA\u65b9\u6cd5\u901a\u8fc7\u53cc\u5411\u5efa\u6a21\u8de8\u7c92\u5ea6\u4ea4\u4e92\uff0c\u6709\u6548\u6355\u6349\u4e0d\u540c\u7c92\u5ea6\u7ea7\u522b\u7684\u58f0\u5b66\u7ed3\u6784\u76f8\u5173\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53d1\u97f3\u8bc4\u4f30\u6027\u80fd"}}
{"id": "2601.01982", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01982", "abs": "https://arxiv.org/abs/2601.01982", "authors": ["Noel Thomas"], "title": "ChaosBench-Logic: A Benchmark for Logical and Symbolic Reasoning on Chaotic Dynamical Systems", "comment": "7 pages, 0 figures , Accepted to AAAI-26 Bridge Program: Logical and Symbolic Reasoning in Language Models (camera-ready)", "summary": "Large language models (LLMs) excel at natural language tasks but remain brittle in domains requiring precise logical and symbolic reasoning. Chaotic dynamical systems provide an especially demanding test because chaos is deterministic yet often misinterpreted as randomness or complexity. We introduce ChaosBench-Logic, a benchmark that evaluates LLM reasoning across 30 diverse dynamical systems using a unified first-order logic (FOL) ontology. Each system is annotated with truth assignments for 11 semantic predicates, and 621 questions are generated across seven reasoning categories, including multi-hop implications, cross-system analogies, counterfactual reasoning, bias probes, and multi-turn dialogues. We define metrics for logical accuracy, implication consistency, dialogue coherence, and contradiction, and we release an open-source evaluation pipeline. Initial experiments show that frontier LLMs such as GPT-4, Claude 3.5 Sonnet, Gemini 2.5 Flash, and the open-source LLaMA-3 70B achieve 91-94% per-item accuracy, yet still score 0% on compositional items and exhibit fragile global coherence. Dialogue-level accuracy ranges from 53.1% (GPT-4 CoT) to 75.5% (LLaMA-3 zero-shot). ChaosBench-Logic provides a rigorous testbed for diagnosing such failures and a foundation for developing neuro-symbolic approaches that improve scientific reasoning in LLMs.", "AI": {"tldr": "ChaosBench-Logic\u662f\u4e00\u4e2a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6df7\u6c8c\u52a8\u529b\u7cfb\u7edf\u4e2d\u903b\u8f91\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b30\u4e2a\u7cfb\u7edf\u3001621\u4e2a\u95ee\u9898\uff0c\u53d1\u73b0\u524d\u6cbfLLMs\u5728\u5355\u9879\u51c6\u786e\u7387\u53ef\u8fbe91-94%\uff0c\u4f46\u5728\u7ec4\u5408\u63a8\u7406\u4e0a\u5f97\u5206\u4e3a0%\uff0c\u5bf9\u8bdd\u51c6\u786e\u738753.1-75.5%\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u9700\u8981\u7cbe\u786e\u903b\u8f91\u548c\u7b26\u53f7\u63a8\u7406\u7684\u9886\u57df\u4ecd\u7136\u8106\u5f31\u3002\u6df7\u6c8c\u52a8\u529b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7279\u522b\u4e25\u683c\u7684\u6d4b\u8bd5\u73af\u5883\uff0c\u56e0\u4e3a\u6df7\u6c8c\u662f\u786e\u5b9a\u6027\u7684\uff0c\u4f46\u5e38\u88ab\u8bef\u89e3\u4e3a\u968f\u673a\u6027\u6216\u590d\u6742\u6027\u3002", "method": "\u5f15\u5165ChaosBench-Logic\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4f7f\u7528\u7edf\u4e00\u7684\u4e00\u9636\u903b\u8f91\u672c\u4f53\u8bc4\u4f30LLM\u572830\u4e2a\u4e0d\u540c\u52a8\u529b\u7cfb\u7edf\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u3002\u6bcf\u4e2a\u7cfb\u7edf\u752811\u4e2a\u8bed\u4e49\u8c13\u8bcd\u7684\u771f\u503c\u5206\u914d\u8fdb\u884c\u6807\u6ce8\uff0c\u751f\u6210621\u4e2a\u95ee\u9898\uff0c\u6db5\u76d6\u4e03\u4e2a\u63a8\u7406\u7c7b\u522b\uff0c\u5305\u62ec\u591a\u8df3\u63a8\u7406\u3001\u8de8\u7cfb\u7edf\u7c7b\u6bd4\u3001\u53cd\u4e8b\u5b9e\u63a8\u7406\u3001\u504f\u89c1\u63a2\u6d4b\u548c\u591a\u8f6e\u5bf9\u8bdd\u3002", "result": "\u524d\u6cbfLLMs\uff08GPT-4\u3001Claude 3.5 Sonnet\u3001Gemini 2.5 Flash\u3001LLaMA-3 70B\uff09\u5728\u5355\u9879\u51c6\u786e\u7387\u8fbe\u523091-94%\uff0c\u4f46\u5728\u7ec4\u5408\u9879\u76ee\u4e0a\u5f97\u5206\u4e3a0%\uff0c\u8868\u73b0\u51fa\u8106\u5f31\u7684\u5168\u5c40\u4e00\u81f4\u6027\u3002\u5bf9\u8bdd\u7ea7\u51c6\u786e\u7387\u4ece53.1%\uff08GPT-4 CoT\uff09\u523075.5%\uff08LLaMA-3\u96f6\u6837\u672c\uff09\u3002", "conclusion": "ChaosBench-Logic\u4e3a\u8bca\u65adLLM\u5728\u903b\u8f91\u63a8\u7406\u4e0a\u7684\u5931\u8d25\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e25\u683c\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5e76\u4e3a\u5f00\u53d1\u6539\u8fdbLLM\u79d1\u5b66\u63a8\u7406\u80fd\u529b\u7684\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2601.01768", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01768", "abs": "https://arxiv.org/abs/2601.01768", "authors": ["Meiman Xiao", "Ante Wang", "Qingguo Hu", "Zhongjian Miao", "Huangjun Shen", "Longyue Wang", "Weihua Luo", "Jinsong Su"], "title": "Can LLMs Track Their Output Length? A Dynamic Feedback Mechanism for Precise Length Regulation", "comment": null, "summary": "Precisely controlling the length of generated text is a common requirement in real-world applications. However, despite significant advancements in following human instructions, Large Language Models (LLMs) still struggle with this task. In this work, we demonstrate that LLMs often fail to accurately measure input text length, leading to poor adherence to length constraints. To address this issue, we propose a novel length regulation approach that incorporates dynamic length feedback during generation, enabling adaptive adjustments to meet target lengths. Experiments on summarization and biography tasks show our training-free approach significantly improves precision in achieving target token, word, or sentence counts without compromising quality. Additionally, we demonstrate that further supervised fine-tuning allows our method to generalize effectively to broader text-generation tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u52a8\u6001\u957f\u5ea6\u53cd\u9988\u7684\u6587\u672c\u957f\u5ea6\u8c03\u63a7\u65b9\u6cd5\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u6587\u672c\u65f6\u6ee1\u8db3\u76ee\u6807\u957f\u5ea6\u7ea6\u675f\u7684\u7cbe\u5ea6\uff0c\u4e14\u4e0d\u635f\u5931\u8d28\u91cf\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9075\u5faa\u4eba\u7c7b\u6307\u4ee4\u65b9\u9762\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u7cbe\u786e\u63a7\u5236\u751f\u6210\u6587\u672c\u957f\u5ea6\u65b9\u9762\u4ecd\u5b58\u5728\u56f0\u96be\u3002\u7814\u7a76\u53d1\u73b0LLMs\u5f80\u5f80\u65e0\u6cd5\u51c6\u786e\u6d4b\u91cf\u8f93\u5165\u6587\u672c\u957f\u5ea6\uff0c\u5bfc\u81f4\u96be\u4ee5\u6ee1\u8db3\u957f\u5ea6\u7ea6\u675f\u8981\u6c42\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u957f\u5ea6\u8c03\u63a7\u65b9\u6cd5\uff0c\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u878d\u5165\u52a8\u6001\u957f\u5ea6\u53cd\u9988\u673a\u5236\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u6839\u636e\u5f53\u524d\u751f\u6210\u8fdb\u5ea6\u81ea\u9002\u5e94\u8c03\u6574\uff0c\u4ee5\u8fbe\u5230\u76ee\u6807\u957f\u5ea6\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "result": "\u5728\u6458\u8981\u548c\u4f20\u8bb0\u751f\u6210\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9e\u73b0\u76ee\u6807token\u6570\u3001\u5355\u8bcd\u6570\u6216\u53e5\u5b50\u6570\u65b9\u9762\u663e\u8457\u63d0\u5347\u4e86\u7cbe\u5ea6\uff0c\u4e14\u4e0d\u635f\u5bb3\u751f\u6210\u8d28\u91cf\u3002\u8fdb\u4e00\u6b65\u7684\u76d1\u7763\u5fae\u8c03\u663e\u793a\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u6cdb\u5316\u5230\u66f4\u5e7f\u6cdb\u7684\u6587\u672c\u751f\u6210\u4efb\u52a1\u3002", "conclusion": "\u901a\u8fc7\u52a8\u6001\u957f\u5ea6\u53cd\u9988\u673a\u5236\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3LLMs\u5728\u7cbe\u786e\u63a7\u5236\u751f\u6210\u6587\u672c\u957f\u5ea6\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u6ee1\u8db3\u7279\u5b9a\u957f\u5ea6\u8981\u6c42\u7684\u6587\u672c\u751f\u6210\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01993", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01993", "abs": "https://arxiv.org/abs/2601.01993", "authors": ["Dong Xue", "Jicheng Tu", "Ming Wang", "Xin Yan", "Fangzhou Liu", "Jie Hu"], "title": "MindChat: A Privacy-preserving Large Language Model for Mental Health Support", "comment": "33 pages, 16 figures", "summary": "Large language models (LLMs) have shown promise for mental health support, yet training such models is constrained by the scarcity and sensitivity of real counseling dialogues. In this article, we present MindChat, a privacy-preserving LLM for mental health support, together with MindCorpus, a synthetic multi-turn counseling dataset constructed via a multi-agent role-playing framework. To synthesize high-quality counseling data, the developed dialogue-construction framework employs a dual closed-loop feedback design to integrate psychological expertise and counseling techniques through role-playing: (i) turn-level critique-and-revision to improve coherence and counseling appropriateness within a session, and (ii) session-level strategy refinement to progressively enrich counselor behaviors across sessions. To mitigate privacy risks under decentralized data ownership, we fine-tune the base model using federated learning with parameter-efficient LoRA adapters and incorporate differentially private optimization to reduce membership and memorization risks. Experiments on synthetic-data quality assessment and counseling capability evaluation show that MindCorpus improves training effectiveness and that MindChat is competitive with existing general and counseling-oriented LLM baselines under both automatic LLM-judge and human evaluation protocols, while exhibiting reduced privacy leakage under membership inference attacks.", "AI": {"tldr": "MindChat\u662f\u4e00\u4e2a\u4fdd\u62a4\u9690\u79c1\u7684\u5fc3\u7406\u5065\u5eb7\u652f\u6301\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u914d\u5408MindCorpus\u5408\u6210\u54a8\u8be2\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u89d2\u8272\u626e\u6f14\u548c\u8054\u90a6\u5b66\u4e60\u5b9e\u73b0\u9ad8\u8d28\u91cf\u6570\u636e\u751f\u6210\u548c\u9690\u79c1\u4fdd\u62a4", "motivation": "\u73b0\u6709\u5fc3\u7406\u5065\u5eb7\u652f\u6301\u5927\u8bed\u8a00\u6a21\u578b\u9762\u4e34\u771f\u5b9e\u54a8\u8be2\u5bf9\u8bdd\u7a00\u7f3a\u4e14\u654f\u611f\u7684\u95ee\u9898\uff0c\u540c\u65f6\u5b58\u5728\u9690\u79c1\u6cc4\u9732\u98ce\u9669\uff0c\u9700\u8981\u5f00\u53d1\u65e2\u80fd\u4fdd\u62a4\u9690\u79c1\u53c8\u80fd\u63d0\u4f9b\u9ad8\u8d28\u91cf\u54a8\u8be2\u670d\u52a1\u7684\u89e3\u51b3\u65b9\u6848", "method": "1) \u901a\u8fc7\u591a\u667a\u80fd\u4f53\u89d2\u8272\u626e\u6f14\u6846\u67b6\u6784\u5efaMindCorpus\u5408\u6210\u54a8\u8be2\u6570\u636e\u96c6\uff0c\u91c7\u7528\u53cc\u95ed\u73af\u53cd\u9988\u8bbe\u8ba1\uff1a\u56de\u5408\u7ea7\u6279\u5224\u4fee\u8ba2\u548c\u4f1a\u8bdd\u7ea7\u7b56\u7565\u4f18\u5316\uff1b2) \u4f7f\u7528\u8054\u90a6\u5b66\u4e60\u914d\u5408LoRA\u9002\u914d\u5668\u548c\u5dee\u5206\u9690\u79c1\u4f18\u5316\u6765\u8bad\u7ec3MindChat\u6a21\u578b\uff0c\u4fdd\u62a4\u6570\u636e\u9690\u79c1", "result": "MindCorpus\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u679c\uff0cMindChat\u5728\u81ea\u52a8LLM\u8bc4\u4f30\u548c\u4eba\u5de5\u8bc4\u4f30\u4e2d\u4e0e\u73b0\u6709\u901a\u7528\u548c\u54a8\u8be2\u5bfc\u5411\u7684LLM\u57fa\u7ebf\u76f8\u5f53\uff0c\u540c\u65f6\u5728\u6210\u5458\u63a8\u7406\u653b\u51fb\u4e0b\u8868\u73b0\u51fa\u51cf\u5c11\u7684\u9690\u79c1\u6cc4\u9732", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u5fc3\u7406\u5065\u5eb7\u652f\u6301LLM\u7684\u6570\u636e\u7a00\u7f3a\u548c\u9690\u79c1\u95ee\u9898\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u751f\u6210\u548c\u9690\u79c1\u4fdd\u62a4\u6280\u672f\u7684\u7ed3\u5408\uff0c\u4e3a\u5f00\u53d1\u5b9e\u7528\u7684\u5fc3\u7406\u5065\u5eb7\u652f\u6301\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848"}}
{"id": "2601.01778", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01778", "abs": "https://arxiv.org/abs/2601.01778", "authors": ["Jakir Hasan", "Shrestha Datta", "Md Saiful Islam", "Shubhashis Roy Dipta", "Ameya Debnath"], "title": "BanglaIPA: Towards Robust Text-to-IPA Transcription with Contextual Rewriting in Bengali", "comment": null, "summary": "Despite its widespread use, Bengali lacks a robust automated International Phonetic Alphabet (IPA) transcription system that effectively supports both standard language and regional dialectal texts. Existing approaches struggle to handle regional variations, numerical expressions, and generalize poorly to previously unseen words. To address these limitations, we propose BanglaIPA, a novel IPA generation system that integrates a character-based vocabulary with word-level alignment. The proposed system accurately handles Bengali numerals and demonstrates strong performance across regional dialects. BanglaIPA improves inference efficiency by leveraging a precomputed word-to-IPA mapping dictionary for previously observed words. The system is evaluated on the standard Bengali and six regional variations of the DUAL-IPA dataset. Experimental results show that BanglaIPA outperforms baseline IPA transcription models by 58.4-78.7% and achieves an overall mean word error rate of 11.4%, highlighting its robustness in phonetic transcription generation for the Bengali language.", "AI": {"tldr": "BanglaIPA\u662f\u4e00\u4e2a\u9488\u5bf9\u5b5f\u52a0\u62c9\u8bed\u7684\u65b0\u578bIPA\u97f3\u6807\u751f\u6210\u7cfb\u7edf\uff0c\u80fd\u6709\u6548\u5904\u7406\u6807\u51c6\u8bed\u548c\u65b9\u8a00\u53d8\u4f53\uff0c\u901a\u8fc7\u5b57\u7b26\u7ea7\u8bcd\u6c47\u548c\u8bcd\u7ea7\u5bf9\u9f50\u6280\u672f\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u5b5f\u52a0\u62c9\u8bed\u7f3a\u4e4f\u5f3a\u5927\u7684\u81ea\u52a8\u5316IPA\u97f3\u6807\u8f6c\u5f55\u7cfb\u7edf\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u65b9\u8a00\u53d8\u4f53\u3001\u6570\u5b57\u8868\u8fbe\uff0c\u4e14\u5bf9\u65b0\u8bcd\u6cdb\u5316\u80fd\u529b\u5dee\u3002", "method": "\u63d0\u51faBanglaIPA\u7cfb\u7edf\uff0c\u6574\u5408\u5b57\u7b26\u7ea7\u8bcd\u6c47\u4e0e\u8bcd\u7ea7\u5bf9\u9f50\u6280\u672f\uff0c\u5229\u7528\u9884\u8ba1\u7b97\u7684\u8bcd\u5230IPA\u6620\u5c04\u8bcd\u5178\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u3002", "result": "\u5728\u6807\u51c6\u5b5f\u52a0\u62c9\u8bed\u548c\u516d\u79cd\u65b9\u8a00\u7684DUAL-IPA\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cBanglaIPA\u6bd4\u57fa\u7ebfIPA\u8f6c\u5f55\u6a21\u578b\u63d0\u534758.4-78.7%\uff0c\u6574\u4f53\u5e73\u5747\u8bcd\u9519\u8bef\u7387\u4e3a11.4%\u3002", "conclusion": "BanglaIPA\u7cfb\u7edf\u5728\u5b5f\u52a0\u62c9\u8bed\u97f3\u6807\u8f6c\u5f55\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\uff0c\u80fd\u6709\u6548\u5904\u7406\u6570\u5b57\u548c\u65b9\u8a00\u53d8\u4f53\uff0c\u4e3a\u5b5f\u52a0\u62c9\u8bed\u8bed\u97f3\u5904\u7406\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2601.01075", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01075", "abs": "https://arxiv.org/abs/2601.01075", "authors": ["Hansen Jin Lillemark", "Benhao Huang", "Fangneng Zhan", "Yilun Du", "Thomas Anderson Keller"], "title": "Flow Equivariant World Models: Memory for Partially Observed Dynamic Environments", "comment": "11 main text pages, 10 figures", "summary": "Embodied systems experience the world as 'a symphony of flows': a combination of many continuous streams of sensory input coupled to self-motion, interwoven with the dynamics of external objects. These streams obey smooth, time-parameterized symmetries, which combine through a precisely structured algebra; yet most neural network world models ignore this structure and instead repeatedly re-learn the same transformations from data. In this work, we introduce 'Flow Equivariant World Models', a framework in which both self-motion and external object motion are unified as one-parameter Lie group 'flows'. We leverage this unification to implement group equivariance with respect to these transformations, thereby providing a stable latent world representation over hundreds of timesteps. On both 2D and 3D partially observed video world modeling benchmarks, we demonstrate that Flow Equivariant World Models significantly outperform comparable state-of-the-art diffusion-based and memory-augmented world modeling architectures -- particularly when there are predictable world dynamics outside the agent's current field of view. We show that flow equivariance is particularly beneficial for long rollouts, generalizing far beyond the training horizon. By structuring world model representations with respect to internal and external motion, flow equivariance charts a scalable route to data efficient, symmetry-guided, embodied intelligence. Project link: https://flowequivariantworldmodels.github.io.", "AI": {"tldr": "\u63d0\u51faFlow Equivariant World Models\u6846\u67b6\uff0c\u5c06\u81ea\u8fd0\u52a8\u548c\u5916\u90e8\u7269\u4f53\u8fd0\u52a8\u7edf\u4e00\u4e3a\u5355\u53c2\u6570\u674e\u7fa4\"\u6d41\"\uff0c\u5229\u7528\u7fa4\u7b49\u53d8\u6027\u5b9e\u73b0\u7a33\u5b9a\u6f5c\u5728\u4e16\u754c\u8868\u793a\uff0c\u57282D/3D\u90e8\u5206\u89c2\u6d4b\u89c6\u9891\u4e16\u754c\u5efa\u6a21\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u7f51\u7edc\u4e16\u754c\u6a21\u578b\u5ffd\u7565\u4e86\u73b0\u5b9e\u4e16\u754c\u4e2d\u8fde\u7eed\u611f\u5b98\u8f93\u5165\u6d41\u6240\u9075\u5faa\u7684\u5e73\u6ed1\u65f6\u95f4\u53c2\u6570\u5316\u5bf9\u79f0\u6027\u7ed3\u6784\uff0c\u5bfc\u81f4\u91cd\u590d\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u76f8\u540c\u53d8\u6362\uff0c\u7f3a\u4e4f\u6548\u7387\u3002", "method": "\u5c06\u81ea\u8fd0\u52a8\u548c\u5916\u90e8\u7269\u4f53\u8fd0\u52a8\u7edf\u4e00\u4e3a\u5355\u53c2\u6570\u674e\u7fa4\"\u6d41\"\uff0c\u5b9e\u73b0\u5bf9\u8fd9\u4e9b\u53d8\u6362\u7684\u7fa4\u7b49\u53d8\u6027\uff0c\u4ece\u800c\u63d0\u4f9b\u6570\u767e\u65f6\u95f4\u6b65\u7684\u7a33\u5b9a\u6f5c\u5728\u4e16\u754c\u8868\u793a\u3002", "result": "\u57282D\u548c3D\u90e8\u5206\u89c2\u6d4b\u89c6\u9891\u4e16\u754c\u5efa\u6a21\u57fa\u51c6\u4e0a\uff0cFlow Equivariant World Models\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u6269\u6563\u548c\u8bb0\u5fc6\u589e\u5f3a\u7684\u4e16\u754c\u5efa\u6a21\u67b6\u6784\uff0c\u7279\u522b\u662f\u5728\u667a\u80fd\u4f53\u5f53\u524d\u89c6\u91ce\u5916\u5b58\u5728\u53ef\u9884\u6d4b\u4e16\u754c\u52a8\u6001\u65f6\u8868\u73b0\u7a81\u51fa\uff0c\u4e14\u5bf9\u957f\u5e8f\u5217\u9884\u6d4b\u5177\u6709\u826f\u597d\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u5c06\u4e16\u754c\u6a21\u578b\u8868\u793a\u7ed3\u6784\u4e0e\u5185\u90e8\u548c\u5916\u90e8\u8fd0\u52a8\u5bf9\u9f50\uff0c\u6d41\u7b49\u53d8\u6027\u4e3a\u6570\u636e\u9ad8\u6548\u3001\u5bf9\u79f0\u6027\u5f15\u5bfc\u7684\u5177\u8eab\u667a\u80fd\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u6269\u5c55\u7684\u8def\u5f84\u3002"}}
{"id": "2601.02008", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02008", "abs": "https://arxiv.org/abs/2601.02008", "authors": ["Midhat Urooj", "Ayan Banerjee", "Sandeep Gupta"], "title": "XAI-MeD: Explainable Knowledge Guided Neuro-Symbolic Framework for Domain Generalization and Rare Class Detection in Medical Imaging", "comment": "Accepted at AAAI Bridge Program 2026", "summary": "Explainability domain generalization and rare class reliability are critical challenges in medical AI where deep models often fail under real world distribution shifts and exhibit bias against infrequent clinical conditions This paper introduces XAIMeD an explainable medical AI framework that integrates clinically accurate expert knowledge into deep learning through a unified neuro symbolic architecture XAIMeD is designed to improve robustness under distribution shift enhance rare class sensitivity and deliver transparent clinically aligned interpretations The framework encodes clinical expertise as logical connectives over atomic medical propositions transforming them into machine checkable class specific rules Their diagnostic utility is quantified through weighted feature satisfaction scores enabling a symbolic reasoning branch that complements neural predictions A confidence weighted fusion integrates symbolic and deep outputs while a Hunt inspired adaptive routing mechanism guided by Entropy Imbalance Gain EIG and Rare Class Gini mitigates class imbalance high intra class variability and uncertainty We evaluate XAIMeD across diverse modalities on four challenging tasks i Seizure Onset Zone SOZ localization from rs fMRI ii Diabetic Retinopathy grading across 6 multicenter datasets demonstrate substantial performance improvements including 6 percent gains in cross domain generalization and a 10 percent improved rare class F1 score far outperforming state of the art deep learning baselines Ablation studies confirm that the clinically grounded symbolic components act as effective regularizers ensuring robustness to distribution shifts XAIMeD thus provides a principled clinically faithful and interpretable approach to multimodal medical AI.", "AI": {"tldr": "XAIMeD\u662f\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684\u533b\u5b66AI\u6846\u67b6\uff0c\u901a\u8fc7\u795e\u7ecf\u7b26\u53f7\u67b6\u6784\u6574\u5408\u4e34\u5e8a\u4e13\u5bb6\u77e5\u8bc6\uff0c\u63d0\u5347\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u9c81\u68d2\u6027\u3001\u7f55\u89c1\u7c7b\u522b\u654f\u611f\u6027\uff0c\u5e76\u63d0\u4f9b\u4e34\u5e8a\u5bf9\u9f50\u7684\u89e3\u91ca\u3002", "motivation": "\u533b\u5b66AI\u9762\u4e34\u53ef\u89e3\u91ca\u6027\u3001\u9886\u57df\u6cdb\u5316\u548c\u7f55\u89c1\u7c7b\u522b\u53ef\u9760\u6027\u7684\u5173\u952e\u6311\u6218\uff0c\u6df1\u5ea6\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u5206\u5e03\u504f\u79fb\u4e0b\u7ecf\u5e38\u5931\u8d25\uff0c\u5e76\u5bf9\u4e0d\u5e38\u89c1\u7684\u4e34\u5e8a\u6761\u4ef6\u8868\u73b0\u51fa\u504f\u89c1\u3002", "method": "\u5c06\u4e34\u5e8a\u4e13\u4e1a\u77e5\u8bc6\u7f16\u7801\u4e3a\u539f\u5b50\u533b\u5b66\u547d\u9898\u7684\u903b\u8f91\u8fde\u63a5\uff0c\u8f6c\u5316\u4e3a\u673a\u5668\u53ef\u68c0\u67e5\u7684\u7c7b\u522b\u7279\u5b9a\u89c4\u5219\uff1b\u901a\u8fc7\u52a0\u6743\u7279\u5f81\u6ee1\u8db3\u5206\u6570\u91cf\u5316\u8bca\u65ad\u6548\u7528\uff1b\u7b26\u53f7\u63a8\u7406\u5206\u652f\u8865\u5145\u795e\u7ecf\u9884\u6d4b\uff1b\u7f6e\u4fe1\u5ea6\u52a0\u6743\u878d\u5408\u96c6\u6210\u7b26\u53f7\u548c\u6df1\u5ea6\u8f93\u51fa\uff1b\u57fa\u4e8e\u71b5\u4e0d\u5e73\u8861\u589e\u76ca\u548c\u7f55\u89c1\u7c7b\u522b\u57fa\u5c3c\u7cfb\u6570\u7684\u81ea\u9002\u5e94\u8def\u7531\u673a\u5236\u3002", "result": "\u5728\u56db\u4e2a\u6311\u6218\u6027\u4efb\u52a1\u4e0a\u8bc4\u4f30\uff0c\u5305\u62ec\u766b\u75eb\u53d1\u4f5c\u533a\u5b9a\u4f4d\u548c\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u5206\u7ea7\uff0c\u663e\u793a\u663e\u8457\u6027\u80fd\u63d0\u5347\uff1a\u8de8\u9886\u57df\u6cdb\u5316\u63d0\u53476%\uff0c\u7f55\u89c1\u7c7b\u522bF1\u5206\u6570\u63d0\u534710%\uff0c\u8fdc\u8d85\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u5b66\u4e60\u57fa\u7ebf\u3002", "conclusion": "XAIMeD\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u3001\u4e34\u5e8a\u5fe0\u5b9e\u4e14\u53ef\u89e3\u91ca\u7684\u591a\u6a21\u6001\u533b\u5b66AI\u65b9\u6cd5\uff0c\u4e34\u5e8a\u57fa\u7840\u7684\u7b26\u53f7\u7ec4\u4ef6\u4f5c\u4e3a\u6709\u6548\u7684\u6b63\u5219\u5316\u5668\uff0c\u786e\u4fdd\u5bf9\u5206\u5e03\u504f\u79fb\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2601.01825", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01825", "abs": "https://arxiv.org/abs/2601.01825", "authors": ["Yaxin Cui", "Yuanqiang Zeng", "Jiapeng Yan", "Keling Lin", "Kai Ji", "Jianhui Zeng", "Sheng Zhang", "Xin Luo", "Binzhu Su", "Chaolai Shen", "Jiahao Yu"], "title": "CSCBench: A PVC Diagnostic Benchmark for Commodity Supply Chain Reasoning", "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable success in general benchmarks, yet their competence in commodity supply chains (CSCs) -- a domain governed by institutional rule systems and feasibility constraints -- remains under-explored. CSC decisions are shaped jointly by process stages (e.g., planning, procurement, delivery), variety-specific rules (e.g., contract specifications and delivery grades), and reasoning depth (from retrieval to multi-step analysis and decision selection). We introduce CSCBench, a 2.3K+ single-choice benchmark for CSC reasoning, instantiated through our PVC 3D Evaluation Framework (Process, Variety, and Cognition). The Process axis aligns tasks with SCOR+Enable; the Variety axis operationalizes commodity-specific rule systems under coupled material-information-financial constraints, grounded in authoritative exchange guidebooks/rulebooks and industry reports; and the Cognition axis follows Bloom's revised taxonomy. Evaluating representative LLMs under a direct prompting setting, we observe strong performance on the Process and Cognition axes but substantial degradation on the Variety axis, especially on Freight Agreements. CSCBench provides a diagnostic yardstick for measuring and improving LLM capabilities in this high-stakes domain.", "AI": {"tldr": "CSCBench\u662f\u4e00\u4e2a\u5305\u542b2300+\u5355\u9009\u9898\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5546\u54c1\u4f9b\u5e94\u94fe\u9886\u57df\u7684\u63a8\u7406\u80fd\u529b\uff0c\u8be5\u9886\u57df\u53d7\u5236\u5ea6\u89c4\u5219\u7cfb\u7edf\u548c\u53ef\u884c\u6027\u7ea6\u675f\u5f71\u54cd\u3002\u7814\u7a76\u53d1\u73b0LLM\u5728\u8fc7\u7a0b\u548c\u8ba4\u77e5\u7ef4\u5ea6\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u54c1\u79cd\u7ef4\u5ea6\uff08\u7279\u522b\u662f\u8d27\u8fd0\u534f\u8bae\uff09\u8868\u73b0\u663e\u8457\u4e0b\u964d\u3002", "motivation": "\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u5728\u901a\u7528\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b83\u4eec\u5728\u5546\u54c1\u4f9b\u5e94\u94fe\u9886\u57df\u7684\u63a8\u7406\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u5546\u54c1\u4f9b\u5e94\u94fe\u51b3\u7b56\u53d7\u5230\u5236\u5ea6\u89c4\u5219\u7cfb\u7edf\u3001\u53ef\u884c\u6027\u7ea6\u675f\u4ee5\u53ca\u591a\u7ef4\u5ea6\u56e0\u7d20\u7684\u590d\u6742\u5f71\u54cd\uff0c\u9700\u8981\u4e13\u95e8\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u8861\u91cfLLM\u5728\u8fd9\u4e00\u9ad8\u4ef7\u503c\u9886\u57df\u7684\u5b9e\u9645\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86CSCBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b2300+\u5355\u9009\u9898\uff0c\u91c7\u7528PVC 3D\u8bc4\u4f30\u6846\u67b6\uff1a\u8fc7\u7a0b\u7ef4\u5ea6\uff08Process\uff09\u4e0eSCOR+Enable\u6807\u51c6\u5bf9\u9f50\uff1b\u54c1\u79cd\u7ef4\u5ea6\uff08Variety\uff09\u57fa\u4e8e\u6743\u5a01\u4ea4\u6613\u6240\u6307\u5357/\u89c4\u5219\u624b\u518c\u548c\u884c\u4e1a\u62a5\u544a\uff0c\u5728\u8026\u5408\u7684\u6750\u6599-\u4fe1\u606f-\u8d22\u52a1\u7ea6\u675f\u4e0b\u64cd\u4f5c\u5316\u5546\u54c1\u7279\u5b9a\u89c4\u5219\u7cfb\u7edf\uff1b\u8ba4\u77e5\u7ef4\u5ea6\uff08Cognition\uff09\u9075\u5faa\u5e03\u9c81\u59c6\u4fee\u8ba2\u5206\u7c7b\u6cd5\u3002\u5728\u76f4\u63a5\u63d0\u793a\u8bbe\u7f6e\u4e0b\u8bc4\u4f30\u4ee3\u8868\u6027LLM\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8fc7\u7a0b\u7ef4\u5ea6\u548c\u8ba4\u77e5\u7ef4\u5ea6\u8868\u73b0\u5f3a\u52b2\uff0c\u4f46\u5728\u54c1\u79cd\u7ef4\u5ea6\u8868\u73b0\u663e\u8457\u4e0b\u964d\uff0c\u7279\u522b\u662f\u5728\u8d27\u8fd0\u534f\u8bae\u76f8\u5173\u4efb\u52a1\u4e0a\u3002\u8fd9\u8868\u660eLLM\u5728\u5904\u7406\u5546\u54c1\u7279\u5b9a\u89c4\u5219\u7cfb\u7edf\u548c\u5236\u5ea6\u7ea6\u675f\u65b9\u9762\u5b58\u5728\u660e\u663e\u4e0d\u8db3\u3002", "conclusion": "CSCBench\u4e3a\u8861\u91cf\u548c\u6539\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5546\u54c1\u4f9b\u5e94\u94fe\u8fd9\u4e00\u9ad8\u98ce\u9669\u9886\u57df\u7684\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u8bca\u65ad\u6027\u57fa\u51c6\u3002\u7814\u7a76\u63ed\u793a\u4e86LLM\u5728\u5904\u7406\u5236\u5ea6\u89c4\u5219\u7cfb\u7edf\u548c\u53ef\u884c\u6027\u7ea6\u675f\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u6a21\u578b\u6539\u8fdb\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2601.01082", "categories": ["cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2601.01082", "abs": "https://arxiv.org/abs/2601.01082", "authors": ["Bryon Tjanaka", "Henry Chen", "Matthew C. Fontaine", "Stefanos Nikolaidis"], "title": "Discount Model Search for Quality Diversity Optimization in High-Dimensional Measure Spaces", "comment": "Source code available at https://github.com/icaros-usc/discount-models", "summary": "Quality diversity (QD) optimization searches for a collection of solutions that optimize an objective while attaining diverse outputs of a user-specified, vector-valued measure function. Contemporary QD algorithms focus on low-dimensional measures because high-dimensional measures are prone to distortion, where many solutions found by the QD algorithm map to similar measures. For example, the CMA-MAE algorithm guides measure space exploration with a histogram in measure space that records so-called discount values. However, CMA-MAE stagnates in domains with high-dimensional measure spaces because solutions with similar measures fall into the same histogram cell and thus receive identical discount values. To address these limitations, we propose Discount Model Search (DMS), which guides exploration with a model that provides a smooth, continuous representation of discount values. In high-dimensional measure spaces, this model enables DMS to distinguish between solutions with similar measures and thus continue exploration. We show that DMS facilitates new QD applications by introducing two domains where the measure space is the high-dimensional space of images, which enables users to specify their desired measures by providing a dataset of images rather than hand-designing the measure function. Results in these domains and on high-dimensional benchmarks show that DMS outperforms CMA-MAE and other black-box QD algorithms.", "AI": {"tldr": "DMS\uff08\u6298\u6263\u6a21\u578b\u641c\u7d22\uff09\u901a\u8fc7\u4f7f\u7528\u8fde\u7eed\u6298\u6263\u6a21\u578b\u66ff\u4ee3\u76f4\u65b9\u56fe\uff0c\u89e3\u51b3\u4e86\u9ad8\u7ef4\u5ea6\u91cf\u7a7a\u95f4\u4e2dQD\u4f18\u5316\u7684\u63a2\u7d22\u505c\u6ede\u95ee\u9898\uff0c\u5728\u56fe\u50cf\u7b49\u9ad8\u7ef4\u5ea6\u91cf\u7a7a\u95f4\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edfQD\u7b97\u6cd5\u5728\u5904\u7406\u9ad8\u7ef4\u5ea6\u91cf\u7a7a\u95f4\u65f6\u5b58\u5728\u5931\u771f\u95ee\u9898\uff0c\u8bb8\u591a\u89e3\u6620\u5c04\u5230\u76f8\u4f3c\u7684\u5ea6\u91cf\u503c\uff0c\u5bfc\u81f4\u63a2\u7d22\u505c\u6ede\u3002CMA-MAE\u7b49\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u76f4\u65b9\u56fe\u8bb0\u5f55\u6298\u6263\u503c\uff0c\u4f46\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u76f8\u4f3c\u5ea6\u91cf\u7684\u89e3\u4f1a\u843d\u5165\u540c\u4e00\u5355\u5143\u683c\uff0c\u83b7\u5f97\u76f8\u540c\u6298\u6263\u503c\uff0c\u9650\u5236\u4e86\u63a2\u7d22\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u6298\u6263\u6a21\u578b\u641c\u7d22\uff08DMS\uff09\uff0c\u4f7f\u7528\u4e00\u4e2a\u63d0\u4f9b\u5e73\u6ed1\u8fde\u7eed\u6298\u6263\u503c\u8868\u793a\u7684\u6a21\u578b\u6765\u6307\u5bfc\u63a2\u7d22\u3002\u8be5\u6a21\u578b\u80fd\u591f\u533a\u5206\u5177\u6709\u76f8\u4f3c\u5ea6\u91cf\u7684\u89e3\uff0c\u4ece\u800c\u5728\u9ad8\u7ef4\u5ea6\u91cf\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u6301\u7eed\u63a2\u7d22\u3002\u7279\u522b\u9002\u7528\u4e8e\u56fe\u50cf\u7b49\u9ad8\u7ef4\u5ea6\u91cf\u7a7a\u95f4\uff0c\u7528\u6237\u53ef\u4ee5\u901a\u8fc7\u63d0\u4f9b\u56fe\u50cf\u6570\u636e\u96c6\u800c\u975e\u624b\u52a8\u8bbe\u8ba1\u5ea6\u91cf\u51fd\u6570\u6765\u6307\u5b9a\u671f\u671b\u7684\u5ea6\u91cf\u3002", "result": "\u5728\u9ad8\u7ef4\u57fa\u51c6\u6d4b\u8bd5\u548c\u56fe\u50cf\u5ea6\u91cf\u7a7a\u95f4\u7684\u65b0\u5e94\u7528\u4e2d\uff0cDMS\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86CMA-MAE\u548c\u5176\u4ed6\u9ed1\u76d2QD\u7b97\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u9ad8\u7ef4\u5ea6\u91cf\u7a7a\u95f4\u7684\u63a2\u7d22\u95ee\u9898\u3002", "conclusion": "DMS\u901a\u8fc7\u8fde\u7eed\u6298\u6263\u6a21\u578b\u89e3\u51b3\u4e86\u9ad8\u7ef4\u5ea6\u91cf\u7a7a\u95f4\u4e2dQD\u4f18\u5316\u7684\u63a2\u7d22\u505c\u6ede\u95ee\u9898\uff0c\u4e3aQD\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u7279\u522b\u662f\u5728\u7528\u6237\u53ef\u4ee5\u901a\u8fc7\u56fe\u50cf\u6570\u636e\u96c6\u6307\u5b9a\u5ea6\u91cf\u7684\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2601.02043", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.02043", "abs": "https://arxiv.org/abs/2601.02043", "authors": ["Hendrik Kempt", "Alon Lavie"], "title": "Simulated Reasoning is Reasoning", "comment": "21 pages", "summary": "Reasoning has long been understood as a pathway between stages of understanding. Proper reasoning leads to understanding of a given subject. This reasoning was conceptualized as a process of understanding in a particular way, i.e., \"symbolic reasoning\". Foundational Models (FM) demonstrate that this is not a necessary condition for many reasoning tasks: they can \"reason\" by way of imitating the process of \"thinking out loud\", testing the produced pathways, and iterating on these pathways on their own. This leads to some form of reasoning that can solve problems on its own or with few-shot learning, but appears fundamentally different from human reasoning due to its lack of grounding and common sense, leading to brittleness of the reasoning process. These insights promise to substantially alter our assessment of reasoning and its necessary conditions, but also inform the approaches to safety and robust defences against this brittleness of FMs. This paper offers and discusses several philosophical interpretations of this phenomenon, argues that the previously apt metaphor of the \"stochastic parrot\" has lost its relevance and thus should be abandoned, and reflects on different normative elements in the safety- and appropriateness-considerations emerging from these reasoning models and their growing capacity.", "AI": {"tldr": "\u8bba\u6587\u8ba4\u4e3a\u57fa\u7840\u6a21\u578b\u901a\u8fc7\u6a21\u4eff\"\u51fa\u58f0\u601d\u8003\"\u8fc7\u7a0b\u3001\u6d4b\u8bd5\u548c\u8fed\u4ee3\u63a8\u7406\u8def\u5f84\uff0c\u5b9e\u73b0\u4e86\u4e0d\u540c\u4e8e\u4eba\u7c7b\u7b26\u53f7\u63a8\u7406\u7684\u65b0\u578b\u63a8\u7406\u80fd\u529b\uff0c\u8fd9\u6539\u53d8\u4e86\u6211\u4eec\u5bf9\u63a8\u7406\u5fc5\u8981\u6761\u4ef6\u7684\u7406\u89e3\uff0c\u9700\u8981\u91cd\u65b0\u8bc4\u4f30\"\u968f\u673a\u9e66\u9e49\"\u9690\u55bb\u7684\u9002\u7528\u6027\u3002", "motivation": "\u4f20\u7edf\u89c2\u70b9\u8ba4\u4e3a\u63a8\u7406\u662f\u901a\u8fc7\u7b26\u53f7\u63a8\u7406\u5b9e\u73b0\u7406\u89e3\u7684\u8def\u5f84\uff0c\u4f46\u57fa\u7840\u6a21\u578b\u5c55\u793a\u4e86\u4e0d\u540c\u7684\u63a8\u7406\u65b9\u5f0f\uff1a\u901a\u8fc7\u6a21\u4eff\"\u51fa\u58f0\u601d\u8003\"\u8fc7\u7a0b\u3001\u6d4b\u8bd5\u548c\u8fed\u4ee3\u63a8\u7406\u8def\u5f84\u6765\u89e3\u51b3\u95ee\u9898\u3002\u8fd9\u79cd\u65b0\u578b\u63a8\u7406\u80fd\u529b\u6311\u6218\u4e86\u4f20\u7edf\u5bf9\u63a8\u7406\u5fc5\u8981\u6761\u4ef6\u7684\u7406\u89e3\uff0c\u9700\u8981\u91cd\u65b0\u5ba1\u89c6\u76f8\u5173\u54f2\u5b66\u6982\u5ff5\u548c\u5b89\u5168\u8003\u91cf\u3002", "method": "\u8bba\u6587\u91c7\u7528\u54f2\u5b66\u5206\u6790\u65b9\u6cd5\uff0c\u8ba8\u8bba\u57fa\u7840\u6a21\u578b\u63a8\u7406\u73b0\u8c61\u7684\u4e0d\u540c\u54f2\u5b66\u89e3\u91ca\uff0c\u8bba\u8bc1\"\u968f\u673a\u9e66\u9e49\"\u9690\u55bb\u5df2\u5931\u53bb\u76f8\u5173\u6027\uff0c\u5e76\u53cd\u601d\u4ece\u8fd9\u4e9b\u63a8\u7406\u6a21\u578b\u53ca\u5176\u589e\u957f\u80fd\u529b\u4e2d\u4ea7\u751f\u7684\u5b89\u5168\u548c\u9002\u5f53\u6027\u8003\u8651\u4e2d\u7684\u89c4\u8303\u6027\u8981\u7d20\u3002", "result": "\u57fa\u7840\u6a21\u578b\u901a\u8fc7\u6a21\u4eff\u601d\u8003\u8fc7\u7a0b\u800c\u975e\u7b26\u53f7\u63a8\u7406\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u8fd9\u79cd\u63a8\u7406\u7f3a\u4e4f\u4eba\u7c7b\u63a8\u7406\u7684\u6839\u57fa\u548c\u5e38\u8bc6\uff0c\u5bfc\u81f4\u63a8\u7406\u8fc7\u7a0b\u7684\u8106\u5f31\u6027\u3002\u8fd9\u663e\u8457\u6539\u53d8\u4e86\u6211\u4eec\u5bf9\u63a8\u7406\u53ca\u5176\u5fc5\u8981\u6761\u4ef6\u7684\u8bc4\u4f30\uff0c\u5e76\u5f71\u54cd\u4e86\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\u9632\u5fa1\u65b9\u6cd5\u3002", "conclusion": "\u57fa\u7840\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u4ee3\u8868\u4e86\u4e0e\u4eba\u7c7b\u7b26\u53f7\u63a8\u7406\u4e0d\u540c\u7684\u65b0\u578b\u63a8\u7406\u5f62\u5f0f\uff0c\u9700\u8981\u653e\u5f03\u8fc7\u65f6\u7684\"\u968f\u673a\u9e66\u9e49\"\u9690\u55bb\uff0c\u91cd\u65b0\u601d\u8003\u76f8\u5173\u7684\u54f2\u5b66\u6982\u5ff5\uff0c\u5e76\u5efa\u7acb\u9002\u5e94\u8fd9\u79cd\u65b0\u578b\u63a8\u7406\u6a21\u5f0f\u7684\u5b89\u5168\u6846\u67b6\u548c\u89c4\u8303\u6027\u8003\u91cf\u3002"}}
{"id": "2601.01827", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01827", "abs": "https://arxiv.org/abs/2601.01827", "authors": ["Valiant Lance D. Dionela", "Fatima Kriselle S. Dy", "Robin James M. Hombrebueno", "Aaron Rae M. Nicolas", "Charibeth K. Cheng", "Raphael W. Gonda"], "title": "Aspect Extraction from E-Commerce Product and Service Reviews", "comment": null, "summary": "Aspect Extraction (AE) is a key task in Aspect-Based Sentiment Analysis (ABSA), yet it remains difficult to apply in low-resource and code-switched contexts like Taglish, a mix of Tagalog and English commonly used in Filipino e-commerce reviews. This paper introduces a comprehensive AE pipeline designed for Taglish, combining rule-based, large language model (LLM)-based, and fine-tuning techniques to address both aspect identification and extraction. A Hierarchical Aspect Framework (HAF) is developed through multi-method topic modeling, along with a dual-mode tagging scheme for explicit and implicit aspects. For aspect identification, four distinct models are evaluated: a Rule-Based system, a Generative LLM (Gemini 2.0 Flash), and two Fine-Tuned Gemma-3 1B models trained on different datasets (Rule-Based vs. LLM-Annotated). Results indicate that the Generative LLM achieved the highest performance across all tasks (Macro F1 0.91), demonstrating superior capability in handling implicit aspects. In contrast, the fine-tuned models exhibited limited performance due to dataset imbalance and architectural capacity constraints. This work contributes a scalable and linguistically adaptive framework for enhancing ABSA in diverse, code-switched environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u9488\u5bf9Taglish\uff08\u83f2\u5f8b\u5bbe\u8bed\u548c\u82f1\u8bed\u6df7\u5408\uff09\u7684\u5168\u9762\u65b9\u9762\u63d0\u53d6\u7ba1\u9053\uff0c\u7ed3\u5408\u57fa\u4e8e\u89c4\u5219\u3001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u5fae\u8c03\u6280\u672f\uff0c\u7528\u4e8e\u4f4e\u8d44\u6e90\u548c\u4ee3\u7801\u5207\u6362\u73af\u5883\u4e2d\u7684\u65b9\u9762\u60c5\u611f\u5206\u6790\u3002", "motivation": "\u65b9\u9762\u63d0\u53d6\u662f\u65b9\u9762\u60c5\u611f\u5206\u6790\u7684\u5173\u952e\u4efb\u52a1\uff0c\u4f46\u5728\u4f4e\u8d44\u6e90\u548c\u4ee3\u7801\u5207\u6362\u73af\u5883\uff08\u5982\u83f2\u5f8b\u5bbe\u7535\u5546\u8bc4\u8bba\u4e2d\u5e38\u7528\u7684Taglish\u6df7\u5408\u8bed\u8a00\uff09\u4e2d\u5e94\u7528\u56f0\u96be\uff0c\u9700\u8981\u4e13\u95e8\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u4e86\u5206\u5c42\u65b9\u9762\u6846\u67b6\u548c\u591a\u65b9\u6cd5\u4e3b\u9898\u5efa\u6a21\uff0c\u91c7\u7528\u53cc\u6a21\u5f0f\u6807\u6ce8\u65b9\u6848\u5904\u7406\u663e\u5f0f\u548c\u9690\u5f0f\u65b9\u9762\u3002\u8bc4\u4f30\u4e86\u56db\u79cd\u6a21\u578b\uff1a\u57fa\u4e8e\u89c4\u5219\u7cfb\u7edf\u3001\u751f\u6210\u5f0fLLM\uff08Gemini 2.0 Flash\uff09\u548c\u4e24\u4e2a\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u7684Gemma-3 1B\u6a21\u578b\u3002", "result": "\u751f\u6210\u5f0fLLM\u5728\u6240\u6709\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\uff08Macro F1 0.91\uff09\uff0c\u5728\u9690\u5f0f\u65b9\u9762\u5904\u7406\u4e0a\u8868\u73b0\u51fa\u8272\u3002\u5fae\u8c03\u6a21\u578b\u7531\u4e8e\u6570\u636e\u96c6\u4e0d\u5e73\u8861\u548c\u67b6\u6784\u5bb9\u91cf\u9650\u5236\uff0c\u6027\u80fd\u6709\u9650\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u591a\u6837\u5316\u4ee3\u7801\u5207\u6362\u73af\u5883\u4e2d\u7684\u65b9\u9762\u60c5\u611f\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u8bed\u8a00\u81ea\u9002\u5e94\u7684\u6846\u67b6\uff0c\u8bc1\u660e\u4e86\u751f\u6210\u5f0fLLM\u5728\u590d\u6742\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u4f18\u52bf\u3002"}}
{"id": "2601.01089", "categories": ["cs.LG", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2601.01089", "abs": "https://arxiv.org/abs/2601.01089", "authors": ["Nobuyuki Ota"], "title": "Central Dogma Transformer: Towards Mechanism-Oriented AI for Cellular Understanding", "comment": null, "summary": "Understanding cellular mechanisms requires integrating information across DNA, RNA, and protein - the three molecular systems linked by the Central Dogma of molecular biology. While domain-specific foundation models have achieved success for each modality individually, they remain isolated, limiting our ability to model integrated cellular processes. Here we present the Central Dogma Transformer (CDT), an architecture that integrates pre-trained language models for DNA, RNA, and protein following the directional logic of the Central Dogma. CDT employs directional cross-attention mechanisms - DNA-to-RNA attention models transcriptional regulation, while RNA-to-Protein attention models translational relationships - producing a unified Virtual Cell Embedding that integrates all three modalities. We validate CDT v1 - a proof-of-concept implementation using fixed (non-cell-specific) RNA and protein embeddings - on CRISPRi enhancer perturbation data from K562 cells, achieving a Pearson correlation of 0.503, representing 63% of the theoretical ceiling set by cross-experiment variability (r = 0.797). Attention and gradient analyses provide complementary interpretive windows: in detailed case studies, these approaches highlight largely distinct genomic regions, with gradient analysis identifying a CTCF binding site that Hi-C data showed as physically contacting both enhancer and target gene. These results suggest that AI architectures aligned with biological information flow can achieve both predictive accuracy and mechanistic interpretability.", "AI": {"tldr": "CDT\u662f\u4e00\u4e2a\u6574\u5408DNA\u3001RNA\u548c\u86cb\u767d\u8d28\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u67b6\u6784\uff0c\u9075\u5faa\u4e2d\u5fc3\u6cd5\u5219\u65b9\u5411\u6027\u903b\u8f91\uff0c\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u751f\u6210\u865a\u62df\u7ec6\u80de\u5d4c\u5165\uff0c\u5728CRISPRi\u589e\u5f3a\u5b50\u6270\u52a8\u6570\u636e\u4e0a\u9a8c\u8bc1\u6709\u6548\u3002", "motivation": "\u5f53\u524d\u9488\u5bf9DNA\u3001RNA\u548c\u86cb\u767d\u8d28\u7684\u9886\u57df\u7279\u5b9a\u57fa\u7840\u6a21\u578b\u867d\u7136\u5404\u81ea\u6210\u529f\uff0c\u4f46\u76f8\u4e92\u9694\u79bb\uff0c\u9650\u5236\u4e86\u6211\u4eec\u5bf9\u6574\u5408\u7ec6\u80de\u8fc7\u7a0b\u5efa\u6a21\u7684\u80fd\u529b\u3002\u9700\u8981\u9075\u5faa\u4e2d\u5fc3\u6cd5\u5219\u65b9\u5411\u6027\u903b\u8f91\u7684\u6574\u5408\u67b6\u6784\u3002", "method": "\u63d0\u51fa\u4e2d\u5fc3\u6cd5\u5219\u53d8\u6362\u5668(CDT)\uff0c\u6574\u5408DNA\u3001RNA\u548c\u86cb\u767d\u8d28\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u91c7\u7528\u65b9\u5411\u6027\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff1aDNA-to-RNA\u6ce8\u610f\u529b\u5efa\u6a21\u8f6c\u5f55\u8c03\u63a7\uff0cRNA-to-Protein\u6ce8\u610f\u529b\u5efa\u6a21\u7ffb\u8bd1\u5173\u7cfb\uff0c\u751f\u6210\u7edf\u4e00\u7684\u865a\u62df\u7ec6\u80de\u5d4c\u5165\u3002", "result": "\u5728K562\u7ec6\u80de\u7684CRISPRi\u589e\u5f3a\u5b50\u6270\u52a8\u6570\u636e\u4e0a\u9a8c\u8bc1\uff0cPearson\u76f8\u5173\u7cfb\u6570\u8fbe\u52300.503\uff0c\u5360\u4ea4\u53c9\u5b9e\u9a8c\u53d8\u5f02\u6027\u7406\u8bba\u4e0a\u9650(r=0.797)\u768463%\u3002\u6ce8\u610f\u529b\u548c\u68af\u5ea6\u5206\u6790\u63d0\u4f9b\u4e86\u4e92\u8865\u7684\u89e3\u91ca\u7a97\u53e3\uff0c\u68af\u5ea6\u5206\u6790\u8bc6\u522b\u51faCTCF\u7ed3\u5408\u4f4d\u70b9\u3002", "conclusion": "\u4e0e\u751f\u7269\u4fe1\u606f\u6d41\u5bf9\u9f50\u7684AI\u67b6\u6784\u65e2\u80fd\u5b9e\u73b0\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u53c8\u80fd\u83b7\u5f97\u673a\u5236\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u6574\u5408\u591a\u6a21\u6001\u7ec6\u80de\u5efa\u6a21\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2601.02061", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02061", "abs": "https://arxiv.org/abs/2601.02061", "authors": ["Faizan Ahmed", "Aniket Dixit", "James Brusey"], "title": "Higher-Order Action Regularization in Deep Reinforcement Learning: From Continuous Control to Building Energy Management", "comment": "6 pages, accepted at NeurIPS workshop 2025", "summary": "Deep reinforcement learning agents often exhibit erratic, high-frequency control behaviors that hinder real-world deployment due to excessive energy consumption and mechanical wear. We systematically investigate action smoothness regularization through higher-order derivative penalties, progressing from theoretical understanding in continuous control benchmarks to practical validation in building energy management. Our comprehensive evaluation across four continuous control environments demonstrates that third-order derivative penalties (jerk minimization) consistently achieve superior smoothness while maintaining competitive performance. We extend these findings to HVAC control systems where smooth policies reduce equipment switching by 60%, translating to significant operational benefits. Our work establishes higher-order action regularization as an effective bridge between RL optimization and operational constraints in energy-critical applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4f7f\u7528\u9ad8\u9636\u5bfc\u6570\u60e9\u7f5a\uff08\u7279\u522b\u662f\u4e09\u9636\u5bfc\u6570\u60e9\u7f5a\uff0c\u5373\u52a0\u901f\u5ea6\u53d8\u5316\u7387\u6700\u5c0f\u5316\uff09\u6765\u5e73\u6ed1\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u63a7\u5236\u884c\u4e3a\uff0c\u5728\u8fde\u7eed\u63a7\u5236\u57fa\u51c6\u548c\u5efa\u7b51\u80fd\u6e90\u7ba1\u7406\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u901a\u5e38\u8868\u73b0\u51fa\u4e0d\u7a33\u5b9a\u3001\u9ad8\u9891\u7684\u63a7\u5236\u884c\u4e3a\uff0c\u8fd9\u5728\u73b0\u5b9e\u4e16\u754c\u90e8\u7f72\u4e2d\u4f1a\u5bfc\u81f4\u8fc7\u5ea6\u80fd\u8017\u548c\u673a\u68b0\u78e8\u635f\u3002\u9700\u8981\u89e3\u51b3RL\u4f18\u5316\u4e0e\u5b9e\u9645\u64cd\u4f5c\u7ea6\u675f\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u7cfb\u7edf\u7814\u7a76\u52a8\u4f5c\u5e73\u6ed1\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u91c7\u7528\u9ad8\u9636\u5bfc\u6570\u60e9\u7f5a\u7b56\u7565\uff0c\u4ece\u8fde\u7eed\u63a7\u5236\u57fa\u51c6\u7684\u7406\u8bba\u7406\u89e3\u6269\u5c55\u5230\u5efa\u7b51\u80fd\u6e90\u7ba1\u7406\u7684\u5b9e\u9645\u9a8c\u8bc1\uff0c\u91cd\u70b9\u5173\u6ce8\u4e09\u9636\u5bfc\u6570\u60e9\u7f5a\uff08\u52a0\u901f\u5ea6\u53d8\u5316\u7387\u6700\u5c0f\u5316\uff09\u3002", "result": "\u5728\u56db\u4e2a\u8fde\u7eed\u63a7\u5236\u73af\u5883\u4e2d\uff0c\u4e09\u9636\u5bfc\u6570\u60e9\u7f5a\uff08\u52a0\u901f\u5ea6\u53d8\u5316\u7387\u6700\u5c0f\u5316\uff09\u80fd\u6301\u7eed\u5b9e\u73b0\u66f4\u5e73\u6ed1\u7684\u63a7\u5236\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u6027\u80fd\u3002\u5728HVAC\u63a7\u5236\u7cfb\u7edf\u4e2d\uff0c\u5e73\u6ed1\u7b56\u7565\u4f7f\u8bbe\u5907\u5207\u6362\u51cf\u5c1160%\uff0c\u5e26\u6765\u663e\u8457\u7684\u64cd\u4f5c\u6548\u76ca\u3002", "conclusion": "\u9ad8\u9636\u52a8\u4f5c\u6b63\u5219\u5316\u662f\u8fde\u63a5RL\u4f18\u5316\u4e0e\u80fd\u6e90\u5173\u952e\u5e94\u7528\u4e2d\u64cd\u4f5c\u7ea6\u675f\u7684\u6709\u6548\u6865\u6881\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u91cd\u8981\u65b9\u6cd5\u3002"}}
{"id": "2601.01828", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01828", "abs": "https://arxiv.org/abs/2601.01828", "authors": ["Jack Lindsey"], "title": "Emergent Introspective Awareness in Large Language Models", "comment": null, "summary": "We investigate whether large language models can introspect on their internal states. It is difficult to answer this question through conversation alone, as genuine introspection cannot be distinguished from confabulations. Here, we address this challenge by injecting representations of known concepts into a model's activations, and measuring the influence of these manipulations on the model's self-reported states. We find that models can, in certain scenarios, notice the presence of injected concepts and accurately identify them. Models demonstrate some ability to recall prior internal representations and distinguish them from raw text inputs. Strikingly, we find that some models can use their ability to recall prior intentions in order to distinguish their own outputs from artificial prefills. In all these experiments, Claude Opus 4 and 4.1, the most capable models we tested, generally demonstrate the greatest introspective awareness; however, trends across models are complex and sensitive to post-training strategies. Finally, we explore whether models can explicitly control their internal representations, finding that models can modulate their activations when instructed or incentivized to \"think about\" a concept. Overall, our results indicate that current language models possess some functional introspective awareness of their own internal states. We stress that in today's models, this capacity is highly unreliable and context-dependent; however, it may continue to develop with further improvements to model capabilities.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5177\u5907\u4e00\u5b9a\u7684\u5185\u7701\u80fd\u529b\uff0c\u80fd\u591f\u611f\u77e5\u548c\u8bc6\u522b\u6ce8\u5165\u5176\u6fc0\u6d3b\u72b6\u6001\u7684\u6982\u5ff5\uff0c\u533a\u5206\u81ea\u8eab\u8f93\u51fa\u4e0e\u4eba\u5de5\u9884\u586b\u5145\uff0c\u5e76\u80fd\u901a\u8fc7\u6307\u4ee4\u8c03\u8282\u5185\u90e8\u8868\u5f81\uff0c\u4f46\u8fd9\u79cd\u80fd\u529b\u9ad8\u5ea6\u4e0d\u53ef\u9760\u4e14\u4f9d\u8d56\u4e0a\u4e0b\u6587\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u591f\u5185\u7701\u5176\u5185\u90e8\u72b6\u6001\u3002\u7531\u4e8e\u901a\u8fc7\u5bf9\u8bdd\u96be\u4ee5\u533a\u5206\u771f\u6b63\u7684\u5185\u7701\u4e0e\u865a\u6784\uff0c\u9700\u8981\u8bbe\u8ba1\u5b9e\u9a8c\u65b9\u6cd5\u6765\u9a8c\u8bc1\u6a21\u578b\u7684\u5185\u7701\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5411\u6a21\u578b\u6fc0\u6d3b\u72b6\u6001\u6ce8\u5165\u5df2\u77e5\u6982\u5ff5\u7684\u8868\u5f81\uff0c\u6d4b\u91cf\u8fd9\u4e9b\u64cd\u4f5c\u5bf9\u6a21\u578b\u81ea\u6211\u62a5\u544a\u72b6\u6001\u7684\u5f71\u54cd\u3002\u6d4b\u8bd5\u6a21\u578b\u80fd\u5426\u8bc6\u522b\u6ce8\u5165\u6982\u5ff5\u3001\u56de\u5fc6\u5148\u524d\u5185\u90e8\u8868\u5f81\u3001\u533a\u5206\u81ea\u8eab\u8f93\u51fa\u4e0e\u4eba\u5de5\u9884\u586b\u5145\uff0c\u4ee5\u53ca\u80fd\u5426\u901a\u8fc7\u6307\u4ee4\u63a7\u5236\u5185\u90e8\u8868\u5f81\u3002", "result": "\u6a21\u578b\u5728\u67d0\u4e9b\u573a\u666f\u4e0b\u80fd\u591f\u6ce8\u610f\u5230\u6ce8\u5165\u6982\u5ff5\u5e76\u51c6\u786e\u8bc6\u522b\uff1b\u80fd\u591f\u56de\u5fc6\u5148\u524d\u5185\u90e8\u8868\u5f81\u5e76\u533a\u5206\u539f\u59cb\u6587\u672c\u8f93\u5165\uff1b\u80fd\u591f\u5229\u7528\u610f\u56fe\u56de\u5fc6\u80fd\u529b\u533a\u5206\u81ea\u8eab\u8f93\u51fa\u4e0e\u4eba\u5de5\u9884\u586b\u5145\uff1b\u80fd\u591f\u901a\u8fc7\u6307\u4ee4\u6216\u6fc0\u52b1\u8c03\u8282\u6fc0\u6d3b\u72b6\u6001\u3002Claude Opus 4\u548c4.1\u8868\u73b0\u51fa\u6700\u5f3a\u7684\u5185\u7701\u610f\u8bc6\u3002", "conclusion": "\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u5177\u5907\u4e00\u5b9a\u7a0b\u5ea6\u7684\u529f\u80fd\u6027\u5185\u7701\u610f\u8bc6\uff0c\u4f46\u8fd9\u79cd\u80fd\u529b\u9ad8\u5ea6\u4e0d\u53ef\u9760\u4e14\u4f9d\u8d56\u4e0a\u4e0b\u6587\uff0c\u53ef\u80fd\u968f\u7740\u6a21\u578b\u80fd\u529b\u7684\u63d0\u5347\u800c\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2601.01119", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01119", "abs": "https://arxiv.org/abs/2601.01119", "authors": ["Muhammad Ashad Kabir", "Sirajam Munira", "Dewan Tasnia Azad", "Saleh Mohammed Ikram", "Mohammad Habibur Rahman Sarker", "Syed Manzoor Ahmed Hanifi"], "title": "Community-Based Early-Stage Chronic Kidney Disease Screening using Explainable Machine Learning for Low-Resource Settings", "comment": "27 pages", "summary": "Early detection of chronic kidney disease (CKD) is essential for preventing progression to end-stage renal disease. However, existing screening tools - primarily developed using populations from high-income countries - often underperform in Bangladesh and South Asia, where risk profiles differ. Most of these tools rely on simple additive scoring functions and are based on data from patients with advanced-stage CKD. Consequently, they fail to capture complex interactions among risk factors and are limited in predicting early-stage CKD. Our objective was to develop and evaluate an explainable machine learning (ML) framework for community-based early-stage CKD screening for low-resource settings, tailored to the Bangladeshi and South Asian population context. We used a community-based dataset from Bangladesh, the first such CKD dataset in South and South Asia, and evaluated twelve ML classifiers across multiple feature domains. Ten complementary feature selection techniques were applied to identify robust, generalizable predictors. The final models were assessed using 10-fold cross-validation. External validation was conducted on three independent datasets from India, the UAE, and Bangladesh. SHAP (SHapley Additive exPlanations) was used to provide model explainability. An ML model trained on an RFECV-selected feature subset achieved a balanced accuracy of 90.40%, whereas minimal non-pathology-test features demonstrated excellent predictive capability with a balanced accuracy of 89.23%, often outperforming larger or full feature sets. Compared with existing screening tools, the proposed models achieved substantially higher accuracy and sensitivity while requiring fewer and more accessible inputs. External validation confirmed strong generalizability with 78% to 98% sensitivity. SHAP interpretation identified clinically meaningful predictors consistent with established CKD risk factors.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5b5f\u52a0\u62c9\u56fd\u548c\u5357\u4e9a\u4eba\u7fa4\u7684\u65e9\u671f\u6162\u6027\u80be\u75c5\u793e\u533a\u7b5b\u67e5\uff0c\u76f8\u6bd4\u73b0\u6709\u5de5\u5177\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u654f\u611f\u6027\u3002", "motivation": "\u73b0\u6709\u6162\u6027\u80be\u75c5\u7b5b\u67e5\u5de5\u5177\u4e3b\u8981\u57fa\u4e8e\u9ad8\u6536\u5165\u56fd\u5bb6\u4eba\u7fa4\u5f00\u53d1\uff0c\u5728\u5b5f\u52a0\u62c9\u56fd\u548c\u5357\u4e9a\u5730\u533a\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u5730\u533a\u7684\u98ce\u9669\u7279\u5f81\u4e0d\u540c\u3002\u8fd9\u4e9b\u5de5\u5177\u901a\u5e38\u4f7f\u7528\u7b80\u5355\u7684\u52a0\u6027\u8bc4\u5206\u51fd\u6570\uff0c\u57fa\u4e8e\u665a\u671f\u80be\u75c5\u60a3\u8005\u6570\u636e\uff0c\u65e0\u6cd5\u6355\u6349\u98ce\u9669\u56e0\u7d20\u7684\u590d\u6742\u76f8\u4e92\u4f5c\u7528\uff0c\u4e5f\u96be\u4ee5\u9884\u6d4b\u65e9\u671f\u80be\u75c5\u3002", "method": "\u4f7f\u7528\u5b5f\u52a0\u62c9\u56fd\u9996\u4e2a\u793e\u533a\u6162\u6027\u80be\u75c5\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e8612\u79cd\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u3002\u5e94\u752810\u79cd\u4e92\u8865\u7684\u7279\u5f81\u9009\u62e9\u6280\u672f\u8bc6\u522b\u7a33\u5065\u3001\u53ef\u6cdb\u5316\u7684\u9884\u6d4b\u56e0\u5b50\u3002\u6700\u7ec8\u6a21\u578b\u91c7\u752810\u6298\u4ea4\u53c9\u9a8c\u8bc1\u8bc4\u4f30\uff0c\u5e76\u5728\u5370\u5ea6\u3001\u963f\u8054\u914b\u548c\u5b5f\u52a0\u62c9\u56fd\u7684\u4e09\u4e2a\u72ec\u7acb\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5916\u90e8\u9a8c\u8bc1\u3002\u4f7f\u7528SHAP\u63d0\u4f9b\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u57fa\u4e8eRFECV\u9009\u62e9\u7279\u5f81\u5b50\u96c6\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8fbe\u523090.40%\u7684\u5e73\u8861\u51c6\u786e\u7387\uff1b\u4ec5\u4f7f\u7528\u6700\u5c11\u975e\u75c5\u7406\u5b66\u6d4b\u8bd5\u7279\u5f81\u4e5f\u80fd\u8fbe\u523089.23%\u7684\u5e73\u8861\u51c6\u786e\u7387\uff0c\u901a\u5e38\u4f18\u4e8e\u66f4\u5927\u6216\u5b8c\u6574\u7279\u5f81\u96c6\u3002\u76f8\u6bd4\u73b0\u6709\u7b5b\u67e5\u5de5\u5177\uff0c\u63d0\u51fa\u7684\u6a21\u578b\u5728\u51c6\u786e\u6027\u548c\u654f\u611f\u6027\u65b9\u9762\u663e\u8457\u63d0\u9ad8\uff0c\u540c\u65f6\u9700\u8981\u66f4\u5c11\u4e14\u66f4\u6613\u83b7\u53d6\u7684\u8f93\u5165\u3002\u5916\u90e8\u9a8c\u8bc1\u663e\u793a78%\u523098%\u7684\u654f\u611f\u6027\uff0c\u8bc1\u5b9e\u4e86\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u4e13\u95e8\u9488\u5bf9\u5b5f\u52a0\u62c9\u56fd\u548c\u5357\u4e9a\u4eba\u7fa4\u7684\u65e9\u671f\u6162\u6027\u80be\u75c5\u7b5b\u67e5\uff0c\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\u3002\u6a21\u578b\u4e0d\u4ec5\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u5de5\u5177\uff0c\u800c\u4e14\u901a\u8fc7SHAP\u89e3\u91ca\u8bc6\u522b\u51fa\u4e0e\u5df2\u77e5\u80be\u75c5\u98ce\u9669\u56e0\u7d20\u4e00\u81f4\u7684\u4e34\u5e8a\u6709\u610f\u4e49\u9884\u6d4b\u56e0\u5b50\uff0c\u4e3a\u8d44\u6e90\u6709\u9650\u5730\u533a\u7684\u65e9\u671f\u80be\u75c5\u7b5b\u67e5\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.02071", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02071", "abs": "https://arxiv.org/abs/2601.02071", "authors": ["Adeshola Okubena", "Yusuf Ali Mohammed", "Moe Elbadawi"], "title": "FormuLLA: A Large Language Model Approach to Generating Novel 3D Printable Formulations", "comment": null, "summary": "Pharmaceutical three-dimensional (3D) printing is an advanced fabrication technology with the potential to enable truly personalised dosage forms. Recent studies have integrated artificial intelligence (AI) to accelerate formulation and process development, drastically transforming current approaches to pharmaceutical 3D printing. To date, most AI-driven efforts remain narrowly focused, while failing to account for the broader formulation challenges inherent to the technology. Recent advances in AI have introduced artificial general intelligence concepts, wherein systems extend beyond conventional predictive modelling toward more generalised, human-like reasoning. In this work, we investigate the application of large language models (LLMs), fine-tuned on a fused deposition modelling (FDM) dataset comprising over 1400 formulations, to recommend suitable excipients based on active pharmaceutical ingredient (API) dose, and predict filament mechanical properties. Four LLM architectures were fine-tuned, with systematic evaluation of both fine-tuning and generative parameter configurations. Our results demonstrate that Llama2 was best suited for recommending excipients for FDM formulations. Additionally, model selection and parameterisation significantly influence performance, with smaller LLMs exhibiting instances of catastrophic forgetting. Furthermore, we demonstrate: (i) even with relatively small dataset of over 1400 formulations, it can lead to model catastrophic forgetting; (ii) standard LLM metrics only evaluate linguistic performance but not formulation processability; and (iii) LLMs trained on biomedically-related data do not always produce the best results. Addressing these challenges is essential to advancing LLMs beyond linguistic proficiency and toward reliable systems for pharmaceutical formulation development.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u4e86\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5e94\u7528\u4e8e\u836f\u72693D\u6253\u5370\u914d\u65b9\u5f00\u53d1\uff0c\u901a\u8fc7\u5fae\u8c03\u56db\u79cdLLM\u67b6\u6784\u57281400\u591a\u4e2aFDM\u914d\u65b9\u6570\u636e\u96c6\u4e0a\uff0c\u7528\u4e8e\u63a8\u8350\u8f85\u6599\u548c\u9884\u6d4b\u4e1d\u6750\u673a\u68b0\u6027\u80fd\u3002", "motivation": "\u5f53\u524dAI\u9a71\u52a8\u7684\u836f\u72693D\u6253\u5370\u7814\u7a76\u5927\u591a\u5c40\u9650\u4e8e\u72ed\u7a84\u9886\u57df\uff0c\u672a\u80fd\u5168\u9762\u89e3\u51b3\u914d\u65b9\u5f00\u53d1\u4e2d\u7684\u590d\u6742\u6311\u6218\u3002\u968f\u7740\u4eba\u5de5\u901a\u7528\u667a\u80fd\u6982\u5ff5\u7684\u53d1\u5c55\uff0c\u9700\u8981\u63a2\u7d22LLM\u5728\u836f\u7269\u914d\u65b9\u5f00\u53d1\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u8d85\u8d8a\u4f20\u7edf\u7684\u9884\u6d4b\u5efa\u6a21\uff0c\u5b9e\u73b0\u66f4\u901a\u7528\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u5305\u542b1400\u591a\u4e2a\u7194\u878d\u6c89\u79ef\u6210\u578b\uff08FDM\uff09\u914d\u65b9\u7684\u6570\u636e\u96c6\uff0c\u5bf9\u56db\u79cdLLM\u67b6\u6784\u8fdb\u884c\u5fae\u8c03\uff0c\u7cfb\u7edf\u8bc4\u4f30\u5fae\u8c03\u548c\u751f\u6210\u53c2\u6570\u914d\u7f6e\u3002\u7814\u7a76\u91cd\u70b9\u5173\u6ce8LLM\u5728\u63a8\u8350\u9002\u5408\u7684\u8f85\u6599\uff08\u57fa\u4e8eAPI\u5242\u91cf\uff09\u548c\u9884\u6d4b\u4e1d\u6750\u673a\u68b0\u6027\u80fd\u65b9\u9762\u7684\u5e94\u7528\u3002", "result": "Llama2\u6a21\u578b\u5728\u63a8\u8350FDM\u914d\u65b9\u8f85\u6599\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff1b\u6a21\u578b\u9009\u62e9\u548c\u53c2\u6570\u5316\u663e\u8457\u5f71\u54cd\u6027\u80fd\uff0c\u8f83\u5c0f\u7684LLM\u51fa\u73b0\u707e\u96be\u6027\u9057\u5fd8\u73b0\u8c61\uff1b\u5373\u4f7f\u76f8\u5bf9\u8f83\u5c0f\u7684\u6570\u636e\u96c6\uff081400\u591a\u4e2a\u914d\u65b9\uff09\u4e5f\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u707e\u96be\u6027\u9057\u5fd8\uff1b\u6807\u51c6\u7684LLM\u8bc4\u4f30\u6307\u6807\u4ec5\u8861\u91cf\u8bed\u8a00\u6027\u80fd\u800c\u975e\u914d\u65b9\u53ef\u52a0\u5de5\u6027\uff1b\u57fa\u4e8e\u751f\u7269\u533b\u5b66\u76f8\u5173\u6570\u636e\u8bad\u7ec3\u7684LLM\u5e76\u4e0d\u603b\u662f\u4ea7\u751f\u6700\u4f73\u7ed3\u679c\u3002", "conclusion": "\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u5bf9\u4e8e\u63a8\u52a8LLM\u8d85\u8d8a\u8bed\u8a00\u80fd\u529b\uff0c\u53d1\u5c55\u6210\u4e3a\u836f\u7269\u914d\u65b9\u5f00\u53d1\u7684\u53ef\u9760\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u836f\u72693D\u6253\u5370\u9886\u57df\u5e94\u7528LLM\u65f6\u9700\u8981\u514b\u670d\u7684\u5173\u952e\u6280\u672f\u969c\u788d\uff0c\u5305\u62ec\u707e\u96be\u6027\u9057\u5fd8\u3001\u8bc4\u4f30\u6307\u6807\u5c40\u9650\u6027\u548c\u6570\u636e\u76f8\u5173\u6027\u7b49\u95ee\u9898\u3002"}}
{"id": "2601.01842", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01842", "abs": "https://arxiv.org/abs/2601.01842", "authors": ["Yusuke Ide", "Adam Nohejl", "Joshua Tanner", "Hitomi Yanaka", "Christopher Lindsay", "Taro Watanabe"], "title": "Towards Automated Lexicography: Generating and Evaluating Definitions for Learner's Dictionaries", "comment": null, "summary": "We study dictionary definition generation (DDG), i.e., the generation of non-contextualized definitions for given headwords. Dictionary definitions are an essential resource for learning word senses, but manually creating them is costly, which motivates us to automate the process. Specifically, we address learner's dictionary definition generation (LDDG), where definitions should consist of simple words. First, we introduce a reliable evaluation approach for DDG, based on our new evaluation criteria and powered by an LLM-as-a-judge. To provide reference definitions for the evaluation, we also construct a Japanese dataset in collaboration with a professional lexicographer. Validation results demonstrate that our evaluation approach agrees reasonably well with human annotators. Second, we propose an LDDG approach via iterative simplification with an LLM. Experimental results indicate that definitions generated by our approach achieve high scores on our criteria while maintaining lexical simplicity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5b66\u4e60\u578b\u8bcd\u5178\u5b9a\u4e49\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7LLM\u8fed\u4ee3\u7b80\u5316\u751f\u6210\u7b80\u5355\u8bcd\u6c47\u7684\u5b9a\u4e49\uff0c\u5e76\u5efa\u7acb\u4e86\u53ef\u9760\u7684\u8bc4\u4f30\u6846\u67b6\u548c\u65e5\u8bed\u6570\u636e\u96c6\u3002", "motivation": "\u8bcd\u5178\u5b9a\u4e49\u662f\u5b66\u4e60\u8bcd\u6c47\u610f\u4e49\u7684\u91cd\u8981\u8d44\u6e90\uff0c\u4f46\u4eba\u5de5\u521b\u5efa\u6210\u672c\u9ad8\u6602\uff0c\u56e0\u6b64\u9700\u8981\u81ea\u52a8\u5316\u751f\u6210\u8fc7\u7a0b\uff0c\u7279\u522b\u662f\u9488\u5bf9\u5b66\u4e60\u8005\u7684\u7b80\u5355\u8bcd\u6c47\u5b9a\u4e49\u3002", "method": "1. \u63d0\u51fa\u57fa\u4e8e\u65b0\u8bc4\u4f30\u6807\u51c6\u548cLLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u7684\u53ef\u9760DDG\u8bc4\u4f30\u65b9\u6cd5\uff1b2. \u6784\u5efa\u65e5\u8bed\u6570\u636e\u96c6\uff1b3. \u901a\u8fc7LLM\u8fed\u4ee3\u7b80\u5316\u65b9\u6cd5\u5b9e\u73b0\u5b66\u4e60\u578b\u8bcd\u5178\u5b9a\u4e49\u751f\u6210\u3002", "result": "\u8bc4\u4f30\u65b9\u6cd5\u4e0e\u4eba\u5de5\u6807\u6ce8\u8005\u4e00\u81f4\u6027\u826f\u597d\uff1b\u63d0\u51fa\u7684LDDG\u65b9\u6cd5\u5728\u8bc4\u4f30\u6807\u51c6\u4e0a\u83b7\u5f97\u9ad8\u5206\uff0c\u540c\u65f6\u4fdd\u6301\u8bcd\u6c47\u7b80\u5355\u6027\u3002", "conclusion": "\u672c\u6587\u5efa\u7acb\u4e86\u53ef\u9760\u7684\u8bcd\u5178\u5b9a\u4e49\u751f\u6210\u8bc4\u4f30\u6846\u67b6\uff0c\u63d0\u51fa\u7684\u8fed\u4ee3\u7b80\u5316\u65b9\u6cd5\u80fd\u6709\u6548\u751f\u6210\u9002\u5408\u5b66\u4e60\u8005\u7684\u7b80\u5355\u8bcd\u6c47\u5b9a\u4e49\uff0c\u4e3a\u81ea\u52a8\u5316\u8bcd\u5178\u7f16\u7e82\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01123", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01123", "abs": "https://arxiv.org/abs/2601.01123", "authors": ["Yaniv Galron", "Hadar Sinai", "Haggai Maron", "Moshe Eliasof"], "title": "Learning from Historical Activations in Graph Neural Networks", "comment": null, "summary": "Graph Neural Networks (GNNs) have demonstrated remarkable success in various domains such as social networks, molecular chemistry, and more. A crucial component of GNNs is the pooling procedure, in which the node features calculated by the model are combined to form an informative final descriptor to be used for the downstream task. However, previous graph pooling schemes rely on the last GNN layer features as an input to the pooling or classifier layers, potentially under-utilizing important activations of previous layers produced during the forward pass of the model, which we regard as historical graph activations. This gap is particularly pronounced in cases where a node's representation can shift significantly over the course of many graph neural layers, and worsened by graph-specific challenges such as over-smoothing in deep architectures. To bridge this gap, we introduce HISTOGRAPH, a novel two-stage attention-based final aggregation layer that first applies a unified layer-wise attention over intermediate activations, followed by node-wise attention. By modeling the evolution of node representations across layers, our HISTOGRAPH leverages both the activation history of nodes and the graph structure to refine features used for final prediction. Empirical results on multiple graph classification benchmarks demonstrate that HISTOGRAPH offers strong performance that consistently improves traditional techniques, with particularly strong robustness in deep GNNs.", "AI": {"tldr": "HISTOGRAPH\uff1a\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u4e24\u9636\u6bb5\u56fe\u805a\u5408\u5c42\uff0c\u5229\u7528\u4e2d\u95f4\u5c42\u6fc0\u6d3b\u5386\u53f2\u6765\u63d0\u5347\u56fe\u5206\u7c7b\u6027\u80fd\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6df1\u5c42GNN\u3002", "motivation": "\u73b0\u6709\u56fe\u6c60\u5316\u65b9\u6cd5\u4ec5\u4f7f\u7528\u6700\u540e\u4e00\u5c42GNN\u7279\u5f81\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u524d\u5411\u4f20\u64ad\u8fc7\u7a0b\u4e2d\u4ea7\u751f\u7684\u4e2d\u95f4\u5c42\u6fc0\u6d3b\u5386\u53f2\u3002\u8fd9\u5728\u8282\u70b9\u8868\u793a\u53ef\u80fd\u53d1\u751f\u663e\u8457\u53d8\u5316\u7684\u6df1\u5c42\u67b6\u6784\u4e2d\u5c24\u4e3a\u660e\u663e\uff0c\u4e14\u56fe\u7279\u6709\u7684\u8fc7\u5e73\u6ed1\u95ee\u9898\u4f1a\u52a0\u5267\u8fd9\u4e00\u7f3a\u9677\u3002", "method": "\u63d0\u51faHISTOGRAPH\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u4e24\u9636\u6bb5\u6ce8\u610f\u529b\u805a\u5408\u5c42\uff1a1\uff09\u5c42\u95f4\u6ce8\u610f\u529b\uff1a\u7edf\u4e00\u5730\u5bf9\u4e2d\u95f4\u5c42\u6fc0\u6d3b\u8fdb\u884c\u52a0\u6743\uff1b2\uff09\u8282\u70b9\u6ce8\u610f\u529b\uff1a\u5728\u5c42\u95f4\u805a\u5408\u7684\u57fa\u7840\u4e0a\u8fdb\u884c\u8282\u70b9\u7ea7\u6ce8\u610f\u529b\u3002\u901a\u8fc7\u5efa\u6a21\u8282\u70b9\u8868\u793a\u5728\u4e0d\u540c\u5c42\u95f4\u7684\u6f14\u5316\u8fc7\u7a0b\uff0c\u5229\u7528\u8282\u70b9\u7684\u6fc0\u6d3b\u5386\u53f2\u548c\u56fe\u7ed3\u6784\u6765\u7cbe\u70bc\u6700\u7ec8\u9884\u6d4b\u7279\u5f81\u3002", "result": "\u5728\u591a\u4e2a\u56fe\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cHISTOGRAPH\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u6027\u80fd\uff0c\u6301\u7eed\u6539\u8fdb\u4f20\u7edf\u6280\u672f\uff0c\u5728\u6df1\u5c42GNN\u4e2d\u8868\u73b0\u51fa\u7279\u522b\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "HISTOGRAPH\u901a\u8fc7\u6709\u6548\u5229\u7528\u4e2d\u95f4\u5c42\u6fc0\u6d3b\u5386\u53f2\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u56fe\u6c60\u5316\u65b9\u6cd5\u4fe1\u606f\u5229\u7528\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u6df1\u5c42GNN\u67b6\u6784\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2601.02163", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.02163", "abs": "https://arxiv.org/abs/2601.02163", "authors": ["Chuanrui Hu", "Xingze Gao", "Zuyi Zhou", "Dannong Xu", "Yi Bai", "Xintong Li", "Hui Zhang", "Tong Li", "Chong Zhang", "Lidong Bing", "Yafeng Deng"], "title": "EverMemOS: A Self-Organizing Memory Operating System for Structured Long-Horizon Reasoning", "comment": "16 pages, 6 figures, 12 tables. Code available at https://github.com/EverMind-AI/EverMemOS", "summary": "Large Language Models (LLMs) are increasingly deployed as long-term interactive agents, yet their limited context windows make it difficult to sustain coherent behavior over extended interactions. Existing memory systems often store isolated records and retrieve fragments, limiting their ability to consolidate evolving user states and resolve conflicts. We introduce EverMemOS, a self-organizing memory operating system that implements an engram-inspired lifecycle for computational memory. Episodic Trace Formation converts dialogue streams into MemCells that capture episodic traces, atomic facts, and time-bounded Foresight signals. Semantic Consolidation organizes MemCells into thematic MemScenes, distilling stable semantic structures and updating user profiles. Reconstructive Recollection performs MemScene-guided agentic retrieval to compose the necessary and sufficient context for downstream reasoning. Experiments on LoCoMo and LongMemEval show that EverMemOS achieves state-of-the-art performance on memory-augmented reasoning tasks. We further report a profile study on PersonaMem v2 and qualitative case studies illustrating chat-oriented capabilities such as user profiling and Foresight. Code is available at https://github.com/EverMind-AI/EverMemOS.", "AI": {"tldr": "EverMemOS\uff1a\u53d7\u8bb0\u5fc6\u5370\u8ff9\u542f\u53d1\u7684\u81ea\u7ec4\u7ec7\u8bb0\u5fc6\u64cd\u4f5c\u7cfb\u7edf\uff0c\u901a\u8fc7\u751f\u547d\u5468\u671f\u7ba1\u7406\u5b9e\u73b0\u957f\u671f\u4ea4\u4e92\u4e2d\u7528\u6237\u72b6\u6001\u7684\u6301\u7eed\u8ddf\u8e2a\u4e0e\u51b2\u7a81\u89e3\u51b3", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u957f\u671f\u4ea4\u4e92\u4ee3\u7406\u65f6\uff0c\u6709\u9650\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u96be\u4ee5\u7ef4\u6301\u8fde\u8d2f\u884c\u4e3a\uff0c\u73b0\u6709\u8bb0\u5fc6\u7cfb\u7edf\u5b58\u50a8\u5b64\u7acb\u8bb0\u5f55\u4e14\u68c0\u7d22\u788e\u7247\u5316\uff0c\u65e0\u6cd5\u6709\u6548\u6574\u5408\u6f14\u5316\u4e2d\u7684\u7528\u6237\u72b6\u6001\u548c\u89e3\u51b3\u51b2\u7a81", "method": "\u63d0\u51fa\u8bb0\u5fc6\u5370\u8ff9\u542f\u53d1\u7684\u8ba1\u7b97\u8bb0\u5fc6\u751f\u547d\u5468\u671f\uff1a1) \u60c5\u666f\u75d5\u8ff9\u5f62\u6210\uff1a\u5bf9\u8bdd\u6d41\u8f6c\u6362\u4e3aMemCells\uff0c\u6355\u6349\u60c5\u666f\u75d5\u8ff9\u3001\u539f\u5b50\u4e8b\u5b9e\u548c\u65f6\u95f4\u8fb9\u754c\u524d\u77bb\u4fe1\u53f7\uff1b2) \u8bed\u4e49\u6574\u5408\uff1aMemCells\u7ec4\u7ec7\u4e3a\u4e3b\u9898MemScenes\uff0c\u63d0\u70bc\u7a33\u5b9a\u8bed\u4e49\u7ed3\u6784\u5e76\u66f4\u65b0\u7528\u6237\u753b\u50cf\uff1b3) \u91cd\u6784\u56de\u5fc6\uff1aMemScene\u5f15\u5bfc\u7684\u667a\u80fd\u68c0\u7d22\uff0c\u4e3a\u4e0b\u6e38\u63a8\u7406\u7ec4\u5408\u5fc5\u8981\u4e14\u5145\u5206\u7684\u4e0a\u4e0b\u6587", "result": "\u5728LoCoMo\u548cLongMemEval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u8bb0\u5fc6\u589e\u5f3a\u63a8\u7406\u4efb\u52a1\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728PersonaMem v2\u4e0a\u7684\u753b\u50cf\u7814\u7a76\u53ca\u5b9a\u6027\u6848\u4f8b\u5c55\u793a\u4e86\u7528\u6237\u753b\u50cf\u548c\u524d\u77bb\u7b49\u804a\u5929\u5bfc\u5411\u80fd\u529b", "conclusion": "EverMemOS\u901a\u8fc7\u81ea\u7ec4\u7ec7\u8bb0\u5fc6\u64cd\u4f5c\u7cfb\u7edf\u6709\u6548\u89e3\u51b3\u4e86\u957f\u671f\u4ea4\u4e92\u4e2d\u7684\u8bb0\u5fc6\u7ba1\u7406\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u7528\u6237\u72b6\u6001\u7684\u6301\u7eed\u8ddf\u8e2a\u548c\u51b2\u7a81\u89e3\u51b3\uff0c\u4e3aLLM\u4f5c\u4e3a\u957f\u671f\u4ee3\u7406\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u8bb0\u5fc6\u7ba1\u7406\u65b9\u6848"}}
{"id": "2601.01862", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.01862", "abs": "https://arxiv.org/abs/2601.01862", "authors": ["Nuo Chen", "Hanpei Fang", "Piaohong Wang", "Jiqun Liu", "Tetsuya Sakai", "Xiao-Ming Wu"], "title": "Judging with Personality and Confidence: A Study on Personality-Conditioned LLM Relevance Assessment", "comment": null, "summary": "Recent studies have shown that prompting can enable large language models (LLMs) to simulate specific personality traits and produce behaviors that align with those traits. However, there is limited understanding of how these simulated personalities influence critical web search decisions, specifically relevance assessment. Moreover, few studies have examined how simulated personalities impact confidence calibration, specifically the tendencies toward overconfidence or underconfidence. This gap exists even though psychological literature suggests these biases are trait-specific, often linking high extraversion to overconfidence and high neuroticism to underconfidence. To address this gap, we conducted a comprehensive study evaluating multiple LLMs, including commercial models and open-source models, prompted to simulate Big Five personality traits. We tested these models across three test collections (TREC DL 2019, TREC DL 2020, and LLMJudge), collecting two key outputs for each query-document pair: a relevance judgment and a self-reported confidence score.\n  The findings show that personalities such as low agreeableness consistently align more closely with human labels than the unprompted condition. Additionally, low conscientiousness performs well in balancing the suppression of both overconfidence and underconfidence. We also observe that relevance scores and confidence distributions vary systematically across different personalities. Based on the above findings, we incorporate personality-conditioned scores and confidence as features in a random forest classifier. This approach achieves performance that surpasses the best single-personality condition on a new dataset (TREC DL 2021), even with limited training data. These findings highlight that personality-derived confidence offers a complementary predictive signal, paving the way for more reliable and human-aligned LLM evaluators.", "AI": {"tldr": "\u7814\u7a76\u663e\u793a\uff0c\u901a\u8fc7\u63d0\u793a\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u6a21\u62df\u7279\u5b9a\u4eba\u683c\u7279\u8d28\u4f1a\u5f71\u54cd\u5176\u5728\u7f51\u9875\u641c\u7d22\u4e2d\u7684\u76f8\u5173\u6027\u8bc4\u4f30\u548c\u7f6e\u4fe1\u5ea6\u6821\u51c6\uff0c\u4eba\u683c\u7279\u5f81\u53ef\u4f5c\u4e3a\u63d0\u5347LLM\u8bc4\u4f30\u5668\u6027\u80fd\u7684\u8865\u5145\u4fe1\u53f7\u3002", "motivation": "\u867d\u7136\u5df2\u6709\u7814\u7a76\u8868\u660eLLM\u53ef\u901a\u8fc7\u63d0\u793a\u6a21\u62df\u4eba\u683c\u7279\u8d28\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u8fd9\u4e9b\u6a21\u62df\u4eba\u683c\u5982\u4f55\u5f71\u54cd\u5173\u952e\u7f51\u9875\u641c\u7d22\u51b3\u7b56\uff08\u7279\u522b\u662f\u76f8\u5173\u6027\u8bc4\u4f30\uff09\u4ee5\u53ca\u7f6e\u4fe1\u5ea6\u6821\u51c6\uff08\u8fc7\u5ea6\u81ea\u4fe1\u6216\u81ea\u4fe1\u4e0d\u8db3\u503e\u5411\uff09\u7684\u7406\u89e3\u3002\u5fc3\u7406\u5b66\u6587\u732e\u8868\u660e\u8fd9\u4e9b\u504f\u5dee\u5177\u6709\u7279\u8d28\u7279\u5f02\u6027\uff0c\u4f46\u76f8\u5173\u7814\u7a76\u6709\u9650\u3002", "method": "\u4f7f\u7528\u5546\u4e1a\u548c\u5f00\u6e90LLM\u6a21\u62df\u5927\u4e94\u4eba\u683c\u7279\u8d28\uff0c\u5728\u4e09\u4e2a\u6d4b\u8bd5\u96c6\uff08TREC DL 2019\u30012020\u548cLLMJudge\uff09\u4e0a\u8bc4\u4f30\u6bcf\u4e2a\u67e5\u8be2-\u6587\u6863\u5bf9\u7684\u76f8\u5173\u6027\u5224\u65ad\u548c\u81ea\u62a5\u7f6e\u4fe1\u5ea6\u5f97\u5206\u3002\u57fa\u4e8e\u53d1\u73b0\uff0c\u5c06\u4eba\u683c\u6761\u4ef6\u5316\u7684\u5206\u6570\u548c\u7f6e\u4fe1\u5ea6\u4f5c\u4e3a\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\u7684\u7279\u5f81\u3002", "result": "\u4f4e\u5b9c\u4eba\u6027\u4eba\u683c\u6bd4\u65e0\u63d0\u793a\u6761\u4ef6\u66f4\u63a5\u8fd1\u4eba\u7c7b\u6807\u7b7e\uff1b\u4f4e\u5c3d\u8d23\u6027\u5728\u6291\u5236\u8fc7\u5ea6\u81ea\u4fe1\u548c\u81ea\u4fe1\u4e0d\u8db3\u65b9\u9762\u8868\u73b0\u826f\u597d\uff1b\u4e0d\u540c\u4eba\u683c\u7684\u76f8\u5173\u6027\u5f97\u5206\u548c\u7f6e\u4fe1\u5ea6\u5206\u5e03\u5b58\u5728\u7cfb\u7edf\u6027\u5dee\u5f02\u3002\u4eba\u683c\u589e\u5f3a\u7684\u5206\u7c7b\u5668\u5728\u65b0\u6570\u636e\u96c6\uff08TREC DL 2021\uff09\u4e0a\u5373\u4f7f\u8bad\u7ec3\u6570\u636e\u6709\u9650\u4e5f\u80fd\u8d85\u8d8a\u6700\u4f73\u5355\u4eba\u683c\u6761\u4ef6\u3002", "conclusion": "\u4eba\u683c\u884d\u751f\u7684\u7f6e\u4fe1\u5ea6\u63d0\u4f9b\u4e86\u8865\u5145\u9884\u6d4b\u4fe1\u53f7\uff0c\u4e3a\u5f00\u53d1\u66f4\u53ef\u9760\u3001\u66f4\u7b26\u5408\u4eba\u7c7b\u5224\u65ad\u7684LLM\u8bc4\u4f30\u5668\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u8868\u660e\u4eba\u683c\u6a21\u62df\u53ef\u663e\u8457\u63d0\u5347LLM\u5728\u641c\u7d22\u8bc4\u4f30\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2601.02170", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02170", "abs": "https://arxiv.org/abs/2601.02170", "authors": ["Haolang Lu", "Minghui Pan", "Ripeng Li", "Guoshun Nan", "Jialin Zhuang", "Zijie Zhao", "Zhongxiang Sun", "Kun Wang", "Yang Liu"], "title": "Streaming Hallucination Detection in Long Chain-of-Thought Reasoning", "comment": null, "summary": "Long chain-of-thought (CoT) reasoning improves the performance of large language models, yet hallucinations in such settings often emerge subtly and propagate across reasoning steps. We suggest that hallucination in long CoT reasoning is better understood as an evolving latent state rather than a one-off erroneous event. Accordingly, we treat step-level hallucination judgments as local observations and introduce a cumulative prefix-level hallucination signal that tracks the global evolution of the reasoning state over the entire trajectory. Overall, our approach enables streaming hallucination detection in long CoT reasoning, providing real-time, interpretable evidence.", "AI": {"tldr": "\u63d0\u51fa\u5c06\u957f\u94fe\u601d\u7ef4\u63a8\u7406\u4e2d\u7684\u5e7b\u89c9\u89c6\u4e3a\u6f14\u5316\u6f5c\u5728\u72b6\u6001\u800c\u975e\u4e00\u6b21\u6027\u9519\u8bef\u4e8b\u4ef6\uff0c\u5f15\u5165\u7d2f\u79ef\u524d\u7f00\u7ea7\u5e7b\u89c9\u4fe1\u53f7\u6765\u8ffd\u8e2a\u6574\u4e2a\u63a8\u7406\u8f68\u8ff9\u7684\u5168\u5c40\u6f14\u5316\uff0c\u5b9e\u73b0\u6d41\u5f0f\u5e7b\u89c9\u68c0\u6d4b", "motivation": "\u957f\u94fe\u601d\u7ef4\u63a8\u7406\u867d\u7136\u80fd\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u6027\u80fd\uff0c\u4f46\u5176\u4e2d\u7684\u5e7b\u89c9\u5f80\u5f80\u4ee5\u5fae\u5999\u65b9\u5f0f\u51fa\u73b0\u5e76\u5728\u63a8\u7406\u6b65\u9aa4\u95f4\u4f20\u64ad\u3002\u73b0\u6709\u65b9\u6cd5\u5c06\u5e7b\u89c9\u89c6\u4e3a\u4e00\u6b21\u6027\u9519\u8bef\u4e8b\u4ef6\uff0c\u672a\u80fd\u6355\u6349\u5176\u6f14\u5316\u7279\u6027", "method": "\u5c06\u6b65\u9aa4\u7ea7\u5e7b\u89c9\u5224\u65ad\u89c6\u4e3a\u5c40\u90e8\u89c2\u6d4b\uff0c\u5f15\u5165\u7d2f\u79ef\u524d\u7f00\u7ea7\u5e7b\u89c9\u4fe1\u53f7\u6765\u8ffd\u8e2a\u6574\u4e2a\u63a8\u7406\u8f68\u8ff9\u7684\u5168\u5c40\u6f14\u5316\u72b6\u6001\uff0c\u5b9e\u73b0\u6d41\u5f0f\u5e7b\u89c9\u68c0\u6d4b", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u5728\u957f\u94fe\u601d\u7ef4\u63a8\u7406\u4e2d\u5b9e\u73b0\u5b9e\u65f6\u3001\u53ef\u89e3\u91ca\u7684\u5e7b\u89c9\u68c0\u6d4b\uff0c\u63d0\u4f9b\u5168\u5c40\u6f14\u5316\u89c6\u89d2\u800c\u975e\u5b64\u7acb\u5224\u65ad", "conclusion": "\u5c06\u5e7b\u89c9\u89c6\u4e3a\u6f14\u5316\u6f5c\u5728\u72b6\u6001\u800c\u975e\u4e00\u6b21\u6027\u9519\u8bef\u4e8b\u4ef6\uff0c\u901a\u8fc7\u7d2f\u79ef\u524d\u7f00\u7ea7\u4fe1\u53f7\u8ffd\u8e2a\u5168\u5c40\u6f14\u5316\uff0c\u4e3a\u957f\u94fe\u601d\u7ef4\u63a8\u7406\u4e2d\u7684\u5e7b\u89c9\u68c0\u6d4b\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u3001\u5b9e\u65f6\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.01868", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01868", "abs": "https://arxiv.org/abs/2601.01868", "authors": ["Jinghan Ru", "Siyuan Yan", "Yuguo Yin", "Yuexian Zou", "Zongyuan Ge"], "title": "DermoGPT: Open Weights and Open Data for Morphology-Grounded Dermatological Reasoning MLLMs", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) show promise for medical applications, yet progress in dermatology lags due to limited training data, narrow task coverage, and lack of clinically-grounded supervision that mirrors expert diagnostic workflows. We present a comprehensive framework to address these gaps. First, we introduce DermoInstruct, a large-scale morphology-anchored instruction corpus comprising 211,243 images and 772,675 trajectories across five task formats, capturing the complete diagnostic pipeline from morphological observation and clinical reasoning to final diagnosis. Second, we establish DermoBench, a rigorous benchmark evaluating 11 tasks across four clinical axes: Morphology, Diagnosis, Reasoning, and Fairness, including a challenging subset of 3,600 expert-verified open-ended instances and human performance baselines. Third, we develop DermoGPT, a dermatology reasoning MLLM trained via supervised fine-tuning followed by our Morphologically-Anchored Visual-Inference-Consistent (MAVIC) reinforcement learning objective, which enforces consistency between visual observations and diagnostic conclusions. At inference, we deploy Confidence-Consistency Test-time adaptation (CCT) for robust predictions. Experiments show DermoGPT significantly outperforms 16 representative baselines across all axes, achieving state-of-the-art performance while substantially narrowing the human-AI gap. DermoInstruct, DermoBench and DermoGPT will be made publicly available at https://github.com/mendicant04/DermoGPT upon acceptance.", "AI": {"tldr": "\u63d0\u51fa\u4e86DermoGPT\u6846\u67b6\uff0c\u5305\u542b\u5927\u89c4\u6a21\u76ae\u80a4\u75c5\u6307\u4ee4\u6570\u636e\u96c6DermoInstruct\u3001\u7efc\u5408\u8bc4\u4f30\u57fa\u51c6DermoBench\u548c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u76ae\u80a4\u75c5\u8bca\u65adMLLM\u6a21\u578bDermoGPT\uff0c\u663e\u8457\u7f29\u5c0f\u4e86\u76ae\u80a4\u75c5\u8bca\u65ad\u4e2dAI\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u7684\u5dee\u8ddd\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u76ae\u80a4\u75c5\u5b66\u9886\u57df\u8fdb\u5c55\u7f13\u6162\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u3001\u4efb\u52a1\u8986\u76d6\u8303\u56f4\u7a84\u4ee5\u53ca\u7f3a\u4e4f\u6a21\u62df\u4e13\u5bb6\u8bca\u65ad\u6d41\u7a0b\u7684\u4e34\u5e8a\u76d1\u7763\u3002", "method": "1) \u6784\u5efaDermoInstruct\u6570\u636e\u96c6\uff08211,243\u5f20\u56fe\u50cf\uff0c772,675\u6761\u8f68\u8ff9\uff09\uff1b2) \u5efa\u7acbDermoBench\u8bc4\u4f30\u57fa\u51c6\uff0811\u4e2a\u4efb\u52a1\uff0c4\u4e2a\u4e34\u5e8a\u7ef4\u5ea6\uff09\uff1b3) \u5f00\u53d1DermoGPT\u6a21\u578b\uff0c\u91c7\u7528\u76d1\u7763\u5fae\u8c03\u540e\u63a5\u5f62\u6001\u951a\u5b9a\u89c6\u89c9\u63a8\u7406\u4e00\u81f4\u6027\u5f3a\u5316\u5b66\u4e60\u76ee\u6807\uff0c\u5e76\u90e8\u7f72\u7f6e\u4fe1\u5ea6\u4e00\u81f4\u6027\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\u3002", "result": "DermoGPT\u572816\u4e2a\u4ee3\u8868\u6027\u57fa\u7ebf\u6a21\u578b\u4e0a\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u6240\u6709\u7ef4\u5ea6\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5927\u5e45\u7f29\u5c0f\u4e86\u4eba\u7c7b-AI\u5dee\u8ddd\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3001\u7efc\u5408\u57fa\u51c6\u548c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u76ae\u80a4\u75c5\u5b66MLLM\u7684\u73b0\u6709\u9650\u5236\uff0c\u4e3a\u4e34\u5e8a\u8bca\u65ad\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2601.01146", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01146", "abs": "https://arxiv.org/abs/2601.01146", "authors": ["Anusree M", "Akhila Henry", "Pramod P Nair"], "title": "Self-Training the Neurochaos Learning Algorithm", "comment": null, "summary": "In numerous practical applications, acquiring substantial quantities of labelled data is challenging and expensive, but unlabelled data is readily accessible. Conventional supervised learning methods frequently underperform in scenarios characterised by little labelled data or imbalanced datasets. This study introduces a hybrid semi-supervised learning (SSL) architecture that integrates Neurochaos Learning (NL) with a threshold-based Self-Training (ST) method to overcome this constraint. The NL architecture converts input characteristics into chaos-based ring-rate representations that encapsulate nonlinear relationships within the data, whereas ST progressively enlarges the labelled set utilising high-confidence pseudo-labelled samples. The model's performance is assessed using ten benchmark datasets and five machine learning classifiers, with 85% of the training data considered unlabelled and just 15% utilised as labelled data. The proposed Self-Training Neurochaos Learning (NL+ST) architecture consistently attains superior performance gain relative to standalone ST models, especially on limited, nonlinear and imbalanced datasets like Iris (188.66%), Wine (158.58%) and Glass Identification (110.48%). The results indicate that using chaos-based feature extraction with SSL improves generalisation, resilience, and classification accuracy in low-data contexts.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u795e\u7ecf\u6df7\u6c8c\u5b66\u4e60\u4e0e\u81ea\u8bad\u7ec3\u7684\u6df7\u5408\u534a\u76d1\u7763\u5b66\u4e60\u67b6\u6784\uff0c\u5728\u6570\u636e\u7a00\u7f3a\u548c\u975e\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u5b9e\u9645\u5e94\u7528\u4e2d\u83b7\u53d6\u5927\u91cf\u6807\u6ce8\u6570\u636e\u56f0\u96be\u4e14\u6602\u8d35\uff0c\u800c\u65e0\u6807\u6ce8\u6570\u636e\u5bb9\u6613\u83b7\u5f97\u3002\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5728\u6807\u6ce8\u6570\u636e\u5c11\u6216\u6570\u636e\u96c6\u4e0d\u5e73\u8861\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u7ed3\u5408\u795e\u7ecf\u6df7\u6c8c\u5b66\u4e60(NL)\u4e0e\u57fa\u4e8e\u9608\u503c\u7684\u81ea\u8bad\u7ec3(ST)\u65b9\u6cd5\u3002NL\u5c06\u8f93\u5165\u7279\u5f81\u8f6c\u6362\u4e3a\u6df7\u6c8c\u53d1\u653e\u7387\u8868\u793a\u4ee5\u6355\u6349\u6570\u636e\u975e\u7ebf\u6027\u5173\u7cfb\uff0cST\u901a\u8fc7\u9ad8\u7f6e\u4fe1\u5ea6\u4f2a\u6807\u6ce8\u6837\u672c\u9010\u6b65\u6269\u5c55\u6807\u6ce8\u96c6\u3002", "result": "\u572810\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u548c5\u4e2a\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u4e0a\u8bc4\u4f30\uff0c\u4ec5\u4f7f\u752815%\u6807\u6ce8\u6570\u636e\u548c85%\u65e0\u6807\u6ce8\u6570\u636e\u3002NL+ST\u67b6\u6784\u76f8\u6bd4\u5355\u72ecST\u6a21\u578b\u83b7\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u7279\u522b\u662f\u5728Iris(188.66%)\u3001Wine(158.58%)\u548cGlass Identification(110.48%)\u7b49\u6709\u9650\u3001\u975e\u7ebf\u6027\u548c\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u3002", "conclusion": "\u57fa\u4e8e\u6df7\u6c8c\u7684\u7279\u5f81\u63d0\u53d6\u4e0e\u534a\u76d1\u7763\u5b66\u4e60\u7ed3\u5408\uff0c\u5728\u4f4e\u6570\u636e\u573a\u666f\u4e0b\u63d0\u9ad8\u4e86\u6cdb\u5316\u80fd\u529b\u3001\u9c81\u68d2\u6027\u548c\u5206\u7c7b\u51c6\u786e\u6027\u3002"}}
{"id": "2601.02314", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02314", "abs": "https://arxiv.org/abs/2601.02314", "authors": ["Sourena Khanzadeh"], "title": "Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents", "comment": null, "summary": "As Large Language Model (LLM) agents are increasingly tasked with high-stakes autonomous decision-making, the transparency of their reasoning processes has become a critical safety concern. While \\textit{Chain-of-Thought} (CoT) prompting allows agents to generate human-readable reasoning traces, it remains unclear whether these traces are \\textbf{faithful} generative drivers of the model's output or merely \\textbf{post-hoc rationalizations}. We introduce \\textbf{Project Ariadne}, a novel XAI framework that utilizes Structural Causal Models (SCMs) and counterfactual logic to audit the causal integrity of agentic reasoning. Unlike existing interpretability methods that rely on surface-level textual similarity, Project Ariadne performs \\textbf{hard interventions} ($do$-calculus) on intermediate reasoning nodes -- systematically inverting logic, negating premises, and reversing factual claims -- to measure the \\textbf{Causal Sensitivity} ($\u03c6$) of the terminal answer. Our empirical evaluation of state-of-the-art models reveals a persistent \\textit{Faithfulness Gap}. We define and detect a widespread failure mode termed \\textbf{Causal Decoupling}, where agents exhibit a violation density ($\u03c1$) of up to $0.77$ in factual and scientific domains. In these instances, agents arrive at identical conclusions despite contradictory internal logic, proving that their reasoning traces function as \"Reasoning Theater\" while decision-making is governed by latent parametric priors. Our findings suggest that current agentic architectures are inherently prone to unfaithful explanation, and we propose the Ariadne Score as a new benchmark for aligning stated logic with model action.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faProject Ariadne\u6846\u67b6\uff0c\u4f7f\u7528\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u548c\u53cd\u4e8b\u5b9e\u903b\u8f91\u6765\u5ba1\u8ba1LLM\u4ee3\u7406\u63a8\u7406\u7684\u56e0\u679c\u5b8c\u6574\u6027\uff0c\u53d1\u73b0\u5f53\u524d\u4ee3\u7406\u67b6\u6784\u5b58\u5728\"\u5fe0\u5b9e\u6027\u5dee\u8ddd\"\uff0c\u63a8\u7406\u75d5\u8ff9\u5e38\u4e3a\"\u63a8\u7406\u5267\u573a\"\u800c\u975e\u771f\u5b9e\u51b3\u7b56\u9a71\u52a8\u3002", "motivation": "\u968f\u7740LLM\u4ee3\u7406\u8d8a\u6765\u8d8a\u591a\u5730\u627f\u62c5\u9ad8\u98ce\u9669\u81ea\u4e3b\u51b3\u7b56\u4efb\u52a1\uff0c\u5176\u63a8\u7406\u8fc7\u7a0b\u7684\u900f\u660e\u5ea6\u6210\u4e3a\u5173\u952e\u5b89\u5168\u5173\u5207\u3002\u867d\u7136\u601d\u7ef4\u94fe\u63d0\u793a\u5141\u8bb8\u4ee3\u7406\u751f\u6210\u4eba\u7c7b\u53ef\u8bfb\u7684\u63a8\u7406\u75d5\u8ff9\uff0c\u4f46\u8fd9\u4e9b\u75d5\u8ff9\u7a76\u7adf\u662f\u6a21\u578b\u8f93\u51fa\u7684\u5fe0\u5b9e\u751f\u6210\u9a71\u52a8\u8fd8\u662f\u4ec5\u4ec5\u662f\u4e8b\u540e\u5408\u7406\u5316\uff0c\u5c1a\u4e0d\u6e05\u695a\u3002", "method": "\u5f15\u5165Project Ariadne\u6846\u67b6\uff0c\u5229\u7528\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u548c\u53cd\u4e8b\u5b9e\u903b\u8f91\u6765\u5ba1\u8ba1\u4ee3\u7406\u63a8\u7406\u7684\u56e0\u679c\u5b8c\u6574\u6027\u3002\u8be5\u65b9\u6cd5\u5bf9\u4e2d\u95f4\u63a8\u7406\u8282\u70b9\u6267\u884c\u786c\u5e72\u9884\uff08do-\u6f14\u7b97\uff09\uff0c\u7cfb\u7edf\u6027\u5730\u53cd\u8f6c\u903b\u8f91\u3001\u5426\u5b9a\u524d\u63d0\u548c\u98a0\u5012\u4e8b\u5b9e\u4e3b\u5f20\uff0c\u4ee5\u6d4b\u91cf\u7ec8\u7aef\u7b54\u6848\u7684\u56e0\u679c\u654f\u611f\u6027\u3002", "result": "\u5bf9\u6700\u5148\u8fdb\u6a21\u578b\u7684\u5b9e\u8bc1\u8bc4\u4f30\u63ed\u793a\u4e86\u6301\u7eed\u7684\"\u5fe0\u5b9e\u6027\u5dee\u8ddd\"\u3002\u5b9a\u4e49\u5e76\u68c0\u6d4b\u5230\u4e00\u79cd\u5e7f\u6cdb\u7684\u6545\u969c\u6a21\u5f0f\u79f0\u4e3a\"\u56e0\u679c\u89e3\u8026\"\uff0c\u4ee3\u7406\u5728\u4e8b\u5b9e\u548c\u79d1\u5b66\u9886\u57df\u4e2d\u8fdd\u53cd\u5bc6\u5ea6\u9ad8\u8fbe0.77\u3002\u5728\u8fd9\u4e9b\u60c5\u51b5\u4e0b\uff0c\u4ee3\u7406\u5c3d\u7ba1\u5185\u90e8\u903b\u8f91\u77db\u76fe\u5374\u5f97\u51fa\u76f8\u540c\u7ed3\u8bba\uff0c\u8bc1\u660e\u5176\u63a8\u7406\u75d5\u8ff9\u53ea\u662f\"\u63a8\u7406\u5267\u573a\"\uff0c\u800c\u51b3\u7b56\u7531\u6f5c\u5728\u53c2\u6570\u5148\u9a8c\u63a7\u5236\u3002", "conclusion": "\u5f53\u524d\u4ee3\u7406\u67b6\u6784\u672c\u8d28\u4e0a\u5bb9\u6613\u4ea7\u751f\u4e0d\u5fe0\u5b9e\u7684\u89e3\u91ca\uff0c\u63d0\u51faAriadne\u5206\u6570\u4f5c\u4e3a\u5bf9\u9f50\u9648\u8ff0\u903b\u8f91\u4e0e\u6a21\u578b\u884c\u4e3a\u7684\u65b0\u57fa\u51c6\u3002"}}
{"id": "2601.01885", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01885", "abs": "https://arxiv.org/abs/2601.01885", "authors": ["Yi Yu", "Liuyi Yao", "Yuexiang Xie", "Qingquan Tan", "Jiaqi Feng", "Yaliang Li", "Libing Wu"], "title": "Agentic Memory: Learning Unified Long-Term and Short-Term Memory Management for Large Language Model Agents", "comment": null, "summary": "Large language model (LLM) agents face fundamental limitations in long-horizon reasoning due to finite context windows, making effective memory management critical. Existing methods typically handle long-term memory (LTM) and short-term memory (STM) as separate components, relying on heuristics or auxiliary controllers, which limits adaptability and end-to-end optimization. In this paper, we propose Agentic Memory (AgeMem), a unified framework that integrates LTM and STM management directly into the agent's policy. AgeMem exposes memory operations as tool-based actions, enabling the LLM agent to autonomously decide what and when to store, retrieve, update, summarize, or discard information. To train such unified behaviors, we propose a three-stage progressive reinforcement learning strategy and design a step-wise GRPO to address sparse and discontinuous rewards induced by memory operations. Experiments on five long-horizon benchmarks demonstrate that AgeMem consistently outperforms strong memory-augmented baselines across multiple LLM backbones, achieving improved task performance, higher-quality long-term memory, and more efficient context usage.", "AI": {"tldr": "AgeMem\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u957f\u77ed\u671f\u8bb0\u5fc6\u7ba1\u7406\u6846\u67b6\uff0c\u5c06\u8bb0\u5fc6\u64cd\u4f5c\u4f5c\u4e3a\u5de5\u5177\u52a8\u4f5c\u96c6\u6210\u5230LLM\u667a\u80fd\u4f53\u7b56\u7565\u4e2d\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u5728\u957f\u89c6\u91ce\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709LLM\u667a\u80fd\u4f53\u5728\u957f\u89c6\u91ce\u63a8\u7406\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4e3b\u8981\u56e0\u4e3a\u6709\u9650\u4e0a\u4e0b\u6587\u7a97\u53e3\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c06\u957f\u77ed\u671f\u8bb0\u5fc6\u4f5c\u4e3a\u72ec\u7acb\u7ec4\u4ef6\u5904\u7406\uff0c\u4f9d\u8d56\u542f\u53d1\u5f0f\u6216\u8f85\u52a9\u63a7\u5236\u5668\uff0c\u8fd9\u9650\u5236\u4e86\u9002\u5e94\u6027\u548c\u7aef\u5230\u7aef\u4f18\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faAgentic Memory (AgeMem)\u6846\u67b6\uff0c\u5c06LTM\u548cSTM\u7ba1\u7406\u76f4\u63a5\u96c6\u6210\u5230\u667a\u80fd\u4f53\u7b56\u7565\u4e2d\uff0c\u5c06\u8bb0\u5fc6\u64cd\u4f5c\uff08\u5b58\u50a8\u3001\u68c0\u7d22\u3001\u66f4\u65b0\u3001\u603b\u7ed3\u3001\u4e22\u5f03\uff09\u4f5c\u4e3a\u5de5\u5177\u52a8\u4f5c\u66b4\u9732\u7ed9LLM\u3002\u91c7\u7528\u4e09\u9636\u6bb5\u6e10\u8fdb\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u8bbe\u8ba1step-wise GRPO\u6765\u89e3\u51b3\u8bb0\u5fc6\u64cd\u4f5c\u5bfc\u81f4\u7684\u7a00\u758f\u548c\u4e0d\u8fde\u7eed\u5956\u52b1\u95ee\u9898\u3002", "result": "\u5728\u4e94\u4e2a\u957f\u89c6\u91ce\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAgeMem\u5728\u591a\u4e2aLLM\u9aa8\u5e72\u7f51\u7edc\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u5f3a\u5927\u7684\u8bb0\u5fc6\u589e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u4efb\u52a1\u6027\u80fd\u3001\u66f4\u9ad8\u8d28\u91cf\u7684\u957f\u65f6\u8bb0\u5fc6\u548c\u66f4\u9ad8\u6548\u7684\u4e0a\u4e0b\u6587\u4f7f\u7528\u3002", "conclusion": "AgeMem\u901a\u8fc7\u5c06\u8bb0\u5fc6\u7ba1\u7406\u7edf\u4e00\u96c6\u6210\u5230\u667a\u80fd\u4f53\u7b56\u7565\u4e2d\uff0c\u4f7fLLM\u80fd\u591f\u81ea\u4e3b\u51b3\u5b9a\u8bb0\u5fc6\u64cd\u4f5c\uff0c\u89e3\u51b3\u4e86\u957f\u89c6\u91ce\u63a8\u7406\u4e2d\u7684\u8bb0\u5fc6\u7ba1\u7406\u6311\u6218\uff0c\u4e3aLLM\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u8bb0\u5fc6\u5904\u7406\u80fd\u529b\u3002"}}
{"id": "2601.01150", "categories": ["cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2601.01150", "abs": "https://arxiv.org/abs/2601.01150", "authors": ["Wenbin Pei", "Ruohao Dai", "Bing Xue", "Mengjie Zhang", "Qiang Zhang", "Yiu-Ming Cheung"], "title": "Evo-TFS: Evolutionary Time-Frequency Domain-Based Synthetic Minority Oversampling Approach to Imbalanced Time Series Classification", "comment": null, "summary": "Time series classification is a fundamental machine learning task with broad real-world applications. Although many deep learning methods have proven effective in learning time-series data for classification, they were originally developed under the assumption of balanced data distributions. Once data distribution is uneven, these methods tend to ignore the minority class that is typically of higher practical significance. Oversampling methods have been designed to address this by generating minority-class samples, but their reliance on linear interpolation often hampers the preservation of temporal dynamics and the generation of diverse samples. Therefore, in this paper, we propose Evo-TFS, a novel evolutionary oversampling method that integrates both time- and frequency-domain characteristics. In Evo-TFS, strongly typed genetic programming is employed to evolve diverse, high-quality time series, guided by a fitness function that incorporates both time-domain and frequency-domain characteristics. Experiments conducted on imbalanced time series datasets demonstrate that Evo-TFS outperforms existing oversampling methods, significantly enhancing the performance of time-domain and frequency-domain classifiers.", "AI": {"tldr": "Evo-TFS\uff1a\u4e00\u79cd\u7ed3\u5408\u65f6\u57df\u548c\u9891\u57df\u7279\u5f81\u7684\u8fdb\u5316\u8fc7\u91c7\u6837\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u4e0d\u5e73\u8861\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u95ee\u9898\uff0c\u901a\u8fc7\u5f3a\u7c7b\u578b\u9057\u4f20\u7f16\u7a0b\u751f\u6210\u591a\u6837\u5316\u7684\u9ad8\u8d28\u91cf\u65f6\u95f4\u5e8f\u5217\u6837\u672c\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5047\u8bbe\u6570\u636e\u5206\u5e03\u5e73\u8861\uff0c\u5728\u4e0d\u5e73\u8861\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u4f1a\u5ffd\u7565\u5c11\u6570\u7c7b\uff1b\u4f20\u7edf\u8fc7\u91c7\u6837\u65b9\u6cd5\u4f9d\u8d56\u7ebf\u6027\u63d2\u503c\uff0c\u96be\u4ee5\u4fdd\u6301\u65f6\u95f4\u52a8\u6001\u7279\u6027\u548c\u751f\u6210\u591a\u6837\u5316\u6837\u672c\u3002", "method": "\u63d0\u51faEvo-TFS\u65b9\u6cd5\uff0c\u4f7f\u7528\u5f3a\u7c7b\u578b\u9057\u4f20\u7f16\u7a0b\u8fdb\u5316\u751f\u6210\u591a\u6837\u5316\u9ad8\u8d28\u91cf\u65f6\u95f4\u5e8f\u5217\uff0c\u901a\u8fc7\u7ed3\u5408\u65f6\u57df\u548c\u9891\u57df\u7279\u5f81\u7684\u9002\u5e94\u5ea6\u51fd\u6570\u6307\u5bfc\u8fdb\u5316\u8fc7\u7a0b\u3002", "result": "\u5728\u4e0d\u5e73\u8861\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cEvo-TFS\u4f18\u4e8e\u73b0\u6709\u8fc7\u91c7\u6837\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65f6\u57df\u548c\u9891\u57df\u5206\u7c7b\u5668\u7684\u6027\u80fd\u3002", "conclusion": "Evo-TFS\u901a\u8fc7\u8fdb\u5316\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4e0d\u5e73\u8861\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u95ee\u9898\uff0c\u80fd\u591f\u751f\u6210\u4fdd\u6301\u65f6\u95f4\u52a8\u6001\u7279\u6027\u7684\u591a\u6837\u5316\u5c11\u6570\u7c7b\u6837\u672c\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.02346", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02346", "abs": "https://arxiv.org/abs/2601.02346", "authors": ["Falcon LLM Team", "Iheb Chaabane", "Puneesh Khanna", "Suhail Mohmad", "Slim Frikha", "Shi Hu", "Abdalgader Abubaker", "Reda Alami", "Mikhail Lubinets", "Mohamed El Amine Seddik", "Hakim Hacid"], "title": "Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling", "comment": null, "summary": "This work introduces Falcon-H1R, a 7B-parameter reasoning-optimized model that establishes the feasibility of achieving competitive reasoning performance with small language models (SLMs). Falcon-H1R stands out for its parameter efficiency, consistently matching or outperforming SOTA reasoning models that are $2\\times$ to $7\\times$ larger across a variety of reasoning-intensive benchmarks. These results underscore the importance of careful data curation and targeted training strategies (via both efficient SFT and RL scaling) in delivering significant performance gains without increasing model size. Furthermore, Falcon-H1R advances the 3D limits of reasoning efficiency by combining faster inference (through its hybrid-parallel architecture design), token efficiency, and higher accuracy. This unique blend makes Falcon-H1R-7B a practical backbone for scaling advanced reasoning systems, particularly in scenarios requiring extensive chain-of-thoughts generation and parallel test-time scaling. Leveraging the recently introduced DeepConf approach, Falcon-H1R achieves state-of-the-art test-time scaling efficiency, offering substantial improvements in both accuracy and computational cost. As a result, Falcon-H1R demonstrates that compact models, through targeted model training and architectural choices, can deliver robust and scalable reasoning performance.", "AI": {"tldr": "Falcon-H1R\u662f\u4e00\u4e2a7B\u53c2\u6570\u7684\u63a8\u7406\u4f18\u5316\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u5c0f\u8bed\u8a00\u6a21\u578b\u4e5f\u80fd\u5b9e\u73b0\u6709\u7ade\u4e89\u529b\u7684\u63a8\u7406\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5339\u914d\u6216\u8d85\u8d8a2-7\u500d\u5927\u7684SOTA\u6a21\u578b\u3002", "motivation": "\u63a2\u7d22\u5c0f\u8bed\u8a00\u6a21\u578b\u80fd\u5426\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6570\u636e\u7b56\u5c55\u548c\u8bad\u7ec3\u7b56\u7565\u5b9e\u73b0\u4e0e\u5927\u578b\u6a21\u578b\u76f8\u5ab2\u7f8e\u7684\u63a8\u7406\u6027\u80fd\uff0c\u89e3\u51b3\u63a8\u7406\u6548\u7387\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6df7\u5408\u5e76\u884c\u67b6\u6784\u8bbe\u8ba1\u5b9e\u73b0\u66f4\u5feb\u63a8\u7406\uff0c\u7ed3\u5408\u9ad8\u6548\u76d1\u7763\u5fae\u8c03(SFT)\u548c\u5f3a\u5316\u5b66\u4e60(RL)\u6269\u5c55\u7b56\u7565\uff0c\u5229\u7528DeepConf\u65b9\u6cd5\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u6548\u7387\u3002", "result": "\u5728\u591a\u4e2a\u63a8\u7406\u5bc6\u96c6\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFalcon-H1R-7B\u6a21\u578b\u5339\u914d\u6216\u8d85\u8d8a\u4e862-7\u500d\u5927\u7684SOTA\u6a21\u578b\uff0c\u5728\u63a8\u7406\u901f\u5ea6\u3001token\u6548\u7387\u548c\u51c6\u786e\u6027\u65b9\u9762\u90fd\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u7d27\u51d1\u6a21\u578b\u901a\u8fc7\u9488\u5bf9\u6027\u7684\u8bad\u7ec3\u7b56\u7565\u548c\u67b6\u6784\u9009\u62e9\uff0c\u80fd\u591f\u63d0\u4f9b\u5f3a\u5927\u4e14\u53ef\u6269\u5c55\u7684\u63a8\u7406\u6027\u80fd\uff0c\u4e3a\u9700\u8981\u5927\u91cf\u601d\u7ef4\u94fe\u751f\u6210\u548c\u5e76\u884c\u6d4b\u8bd5\u65f6\u6269\u5c55\u7684\u573a\u666f\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u9aa8\u5e72\u6a21\u578b\u3002"}}
{"id": "2601.01896", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01896", "abs": "https://arxiv.org/abs/2601.01896", "authors": ["Jingyu Liu", "Jiaen Lin", "Yong Liu"], "title": "Tackling the Inherent Difficulty of Noise Filtering in RAG", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has become a widely adopted approach to enhance Large Language Models (LLMs) by incorporating external knowledge and reducing hallucinations. However, noisy or irrelevant documents are often introduced during RAG, potentially degrading performance and even causing hallucinated outputs. While various methods have been proposed to filter out such noise, we argue that identifying irrelevant information from retrieved content is inherently difficult and limited number of transformer layers can hardly solve this. Consequently, retrievers fail to filter out irrelevant documents entirely. Therefore, LLMs must be robust against such noise, but we demonstrate that standard fine-tuning approaches are often ineffective in enabling the model to selectively utilize relevant information while ignoring irrelevant content due to the structural constraints of attention patterns. To address this, we propose a novel fine-tuning method designed to enhance the model's ability to distinguish between relevant and irrelevant information within retrieved documents. Extensive experiments across multiple benchmarks show that our approach significantly improves the robustness and performance of LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u589e\u5f3aLLMs\u5728RAG\u4e2d\u533a\u5206\u76f8\u5173\u4e0e\u4e0d\u76f8\u5173\u4fe1\u606f\u7684\u80fd\u529b\uff0c\u63d0\u9ad8\u6a21\u578b\u5bf9\u68c0\u7d22\u566a\u58f0\u7684\u9c81\u68d2\u6027", "motivation": "RAG\u4e2d\u5e38\u5f15\u5165\u566a\u58f0\u6216\u4e0d\u76f8\u5173\u6587\u6863\uff0c\u8fd9\u4f1a\u964d\u4f4e\u6027\u80fd\u751a\u81f3\u5bfc\u81f4\u5e7b\u89c9\u8f93\u51fa\u3002\u73b0\u6709\u8fc7\u6ee4\u65b9\u6cd5\u96be\u4ee5\u5b8c\u5168\u8bc6\u522b\u4e0d\u76f8\u5173\u4fe1\u606f\uff0c\u4e14\u6807\u51c6\u5fae\u8c03\u65b9\u6cd5\u56e0\u6ce8\u610f\u529b\u6a21\u5f0f\u7684\u7ed3\u6784\u9650\u5236\uff0c\u65e0\u6cd5\u6709\u6548\u8ba9\u6a21\u578b\u9009\u62e9\u6027\u5229\u7528\u76f8\u5173\u4fe1\u606f\u800c\u5ffd\u7565\u4e0d\u76f8\u5173\u5185\u5bb9", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u4e13\u95e8\u8bbe\u8ba1\u6765\u589e\u5f3a\u6a21\u578b\u5728\u68c0\u7d22\u6587\u6863\u4e2d\u533a\u5206\u76f8\u5173\u4e0e\u4e0d\u76f8\u5173\u4fe1\u606f\u7684\u80fd\u529b", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86LLMs\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd", "conclusion": "\u901a\u8fc7\u4e13\u95e8\u8bbe\u8ba1\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6709\u6548\u589e\u5f3aLLMs\u5728RAG\u4e2d\u5904\u7406\u566a\u58f0\u6587\u6863\u7684\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u66f4\u667a\u80fd\u5730\u533a\u5206\u548c\u5229\u7528\u76f8\u5173\u4fe1\u606f"}}
{"id": "2601.01162", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01162", "abs": "https://arxiv.org/abs/2601.01162", "authors": ["Zihua Yang", "Xin Liao", "Yiqun Zhang", "Yiu-ming Cheung"], "title": "Bridging the Semantic Gap for Categorical Data Clustering via Large Language Models", "comment": "Submitted to ICPR 2026", "summary": "Categorical data are prevalent in domains such as healthcare, marketing, and bioinformatics, where clustering serves as a fundamental tool for pattern discovery. A core challenge in categorical data clustering lies in measuring similarity among attribute values that lack inherent ordering or distance. Without appropriate similarity measures, values are often treated as equidistant, creating a semantic gap that obscures latent structures and degrades clustering quality. Although existing methods infer value relationships from within-dataset co-occurrence patterns, such inference becomes unreliable when samples are limited, leaving the semantic context of the data underexplored. To bridge this gap, we present ARISE (Attention-weighted Representation with Integrated Semantic Embeddings), which draws on external semantic knowledge from Large Language Models (LLMs) to construct semantic-aware representations that complement the metric space of categorical data for accurate clustering. That is, LLM is adopted to describe attribute values for representation enhancement, and the LLM-enhanced embeddings are combined with the original data to explore semantically prominent clusters. Experiments on eight benchmark datasets demonstrate consistent improvements over seven representative counterparts, with gains of 19-27%. Code is available at https://github.com/develop-yang/ARISE", "AI": {"tldr": "ARISE\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4e3a\u5206\u7c7b\u6570\u636e\u63d0\u4f9b\u8bed\u4e49\u5d4c\u5165\uff0c\u589e\u5f3a\u805a\u7c7b\u6548\u679c\uff0c\u57288\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u76f8\u6bd47\u4e2a\u5bf9\u6bd4\u65b9\u6cd5\u63d0\u534719-27%", "motivation": "\u5206\u7c7b\u6570\u636e\uff08\u5982\u533b\u7597\u3001\u8425\u9500\u3001\u751f\u7269\u4fe1\u606f\u5b66\uff09\u7f3a\u4e4f\u5185\u5728\u6392\u5e8f\u6216\u8ddd\u79bb\u5ea6\u91cf\uff0c\u4f20\u7edf\u65b9\u6cd5\u5c06\u4e0d\u540c\u503c\u89c6\u4e3a\u7b49\u8ddd\uff0c\u9020\u6210\u8bed\u4e49\u9e3f\u6c9f\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6570\u636e\u96c6\u5185\u5171\u73b0\u6a21\u5f0f\u63a8\u65ad\u503c\u95f4\u5173\u7cfb\uff0c\u4f46\u5728\u6837\u672c\u6709\u9650\u65f6\u4e0d\u53ef\u9760\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u6570\u636e\u7684\u8bed\u4e49\u4e0a\u4e0b\u6587\u3002", "method": "\u63d0\u51faARISE\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5916\u90e8\u8bed\u4e49\u77e5\u8bc6\u6784\u5efa\u8bed\u4e49\u611f\u77e5\u8868\u793a\uff0c\u8865\u5145\u5206\u7c7b\u6570\u636e\u7684\u5ea6\u91cf\u7a7a\u95f4\u3002\u5177\u4f53\u4f7f\u7528LLM\u63cf\u8ff0\u5c5e\u6027\u503c\u4ee5\u589e\u5f3a\u8868\u793a\uff0c\u5c06LLM\u589e\u5f3a\u7684\u5d4c\u5165\u4e0e\u539f\u59cb\u6570\u636e\u7ed3\u5408\uff0c\u63a2\u7d22\u8bed\u4e49\u663e\u8457\u7684\u805a\u7c7b\u3002", "result": "\u57288\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd47\u4e2a\u4ee3\u8868\u6027\u5bf9\u6bd4\u65b9\u6cd5\uff0cARISE\u53d6\u5f97\u4e86\u4e00\u81f4\u7684\u6539\u8fdb\uff0c\u6027\u80fd\u63d0\u5347\u8fbe\u523019-27%\u3002", "conclusion": "ARISE\u901a\u8fc7\u6574\u5408\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u77e5\u8bc6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5206\u7c7b\u6570\u636e\u805a\u7c7b\u4e2d\u7684\u8bed\u4e49\u9e3f\u6c9f\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u805a\u7c7b\u8d28\u91cf\uff0c\u7279\u522b\u662f\u5728\u6837\u672c\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2601.01964", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01964", "abs": "https://arxiv.org/abs/2601.01964", "authors": ["Tran Sy Bao"], "title": "CSF: Contrastive Semantic Features for Direct Multilingual Sign Language Generation", "comment": "9 pages, 8 tables, code available at https://github.com/transybao1393/csf-sign-language", "summary": "Sign language translation systems typically require English as an intermediary language, creating barriers for non-English speakers in the global deaf community. We present Canonical Semantic Form (CSF), a language-agnostic semantic representation framework that enables direct translation from any source language to sign language without English mediation. CSF decomposes utterances into nine universal semantic slots: event, intent, time, condition, agent, object, location, purpose, and modifier. A key contribution is our comprehensive condition taxonomy comprising 35 condition types across eight semantic categories, enabling nuanced representation of conditional expressions common in everyday communication. We train a lightweight transformer-based extractor (0.74 MB) that achieves 99.03% average slot extraction accuracy across four typologically diverse languages: English, Vietnamese, Japanese, and French. The model demonstrates particularly strong performance on condition classification (99.4% accuracy) despite the 35-class complexity. With inference latency of 3.02ms on CPU, our approach enables real-time sign language generation in browser-based applications. We release our code, trained models, and multilingual dataset to support further research in accessible sign language technology.", "AI": {"tldr": "\u63d0\u51faCanonical Semantic Form (CSF)\u6846\u67b6\uff0c\u5b9e\u73b0\u4efb\u610f\u6e90\u8bed\u8a00\u5230\u624b\u8bed\u7684\u76f4\u63a5\u7ffb\u8bd1\uff0c\u65e0\u9700\u82f1\u8bed\u4e2d\u4ecb\uff0c\u901a\u8fc7\u4e5d\u4e2a\u901a\u7528\u8bed\u4e49\u69fd\u4f4d\u5206\u89e3\u8bed\u53e5\uff0c\u7279\u522b\u5173\u6ce8\u6761\u4ef6\u8868\u8fbe\u7684\u5206\u7c7b\u3002", "motivation": "\u73b0\u6709\u624b\u8bed\u7ffb\u8bd1\u7cfb\u7edf\u901a\u5e38\u9700\u8981\u82f1\u8bed\u4f5c\u4e3a\u4e2d\u4ecb\u8bed\u8a00\uff0c\u8fd9\u4e3a\u5168\u7403\u975e\u82f1\u8bed\u804b\u4eba\u793e\u533a\u521b\u9020\u4e86\u969c\u788d\u3002\u9700\u8981\u4e00\u79cd\u8bed\u8a00\u65e0\u5173\u7684\u8bed\u4e49\u8868\u793a\u6846\u67b6\uff0c\u5b9e\u73b0\u4ece\u4efb\u4f55\u6e90\u8bed\u8a00\u5230\u624b\u8bed\u7684\u76f4\u63a5\u7ffb\u8bd1\u3002", "method": "\u63d0\u51faCanonical Semantic Form (CSF)\u6846\u67b6\uff0c\u5c06\u8bdd\u8bed\u5206\u89e3\u4e3a\u4e5d\u4e2a\u901a\u7528\u8bed\u4e49\u69fd\u4f4d\uff1a\u4e8b\u4ef6\u3001\u610f\u56fe\u3001\u65f6\u95f4\u3001\u6761\u4ef6\u3001\u65bd\u4e8b\u3001\u53d7\u4e8b\u3001\u5730\u70b9\u3001\u76ee\u7684\u548c\u4fee\u9970\u8bed\u3002\u7279\u522b\u8d21\u732e\u4e86\u5305\u542b35\u79cd\u6761\u4ef6\u7c7b\u578b\u3001\u516b\u4e2a\u8bed\u4e49\u7c7b\u522b\u7684\u7efc\u5408\u6761\u4ef6\u5206\u7c7b\u6cd5\u3002\u8bad\u7ec3\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u57fa\u4e8eTransformer\u7684\u63d0\u53d6\u5668\uff080.74 MB\uff09\u3002", "result": "\u5728\u56db\u79cd\u7c7b\u578b\u5b66\u591a\u6837\u8bed\u8a00\uff08\u82f1\u8bed\u3001\u8d8a\u5357\u8bed\u3001\u65e5\u8bed\u3001\u6cd5\u8bed\uff09\u4e0a\u5b9e\u73b0\u4e8699.03%\u7684\u5e73\u5747\u69fd\u4f4d\u63d0\u53d6\u51c6\u786e\u7387\u3002\u6761\u4ef6\u5206\u7c7b\u51c6\u786e\u7387\u8fbe\u523099.4%\uff0835\u4e2a\u7c7b\u522b\uff09\u3002CPU\u63a8\u7406\u5ef6\u8fdf\u4e3a3.02ms\uff0c\u652f\u6301\u6d4f\u89c8\u5668\u5e94\u7528\u7684\u5b9e\u65f6\u624b\u8bed\u751f\u6210\u3002", "conclusion": "CSF\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u8bed\u8a00\u65e0\u5173\u7684\u624b\u8bed\u7ffb\u8bd1\uff0c\u6d88\u9664\u82f1\u8bed\u4e2d\u4ecb\u9700\u6c42\uff0c\u4e3a\u5168\u7403\u804b\u4eba\u793e\u533a\u63d0\u4f9b\u66f4\u53ef\u8bbf\u95ee\u7684\u624b\u8bed\u6280\u672f\u3002\u91ca\u653e\u4e86\u4ee3\u7801\u3001\u8bad\u7ec3\u6a21\u578b\u548c\u591a\u8bed\u8a00\u6570\u636e\u96c6\u4ee5\u652f\u6301\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2601.01206", "categories": ["cs.LG", "cs.AI", "cs.HC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.01206", "abs": "https://arxiv.org/abs/2601.01206", "authors": ["Soroush Elyasi", "Arya VarastehNezhad", "Fattaneh Taghiyareh"], "title": "MentalGame: Predicting Personality-Job Fitness for Software Developers Using Multi-Genre Games and Machine Learning Approaches", "comment": null, "summary": "Personality assessment in career guidance and personnel selection traditionally relies on self-report questionnaires, which are susceptible to response bias, fatigue, and intentional distortion. Game-based assessment offers a promising alternative by capturing implicit behavioral signals during gameplay. This study proposes a multi-genre serious-game framework combined with machine-learning techniques to predict suitability for software development roles. Developer-relevant personality and behavioral traits were identified through a systematic literature review and an empirical study of professional software engineers. A custom mobile game was designed to elicit behaviors related to problem solving, planning, adaptability, persistence, time management, and information seeking. Fine-grained gameplay event data were collected and analyzed using a two-phase modeling strategy where suitability was predicted exclusively from gameplay-derived behavioral features. Results show that our model achieved up to 97% precision and 94% accuracy. Behavioral analysis revealed that proper candidates exhibited distinct gameplay patterns, such as more wins in puzzle-based games, more side challenges, navigating menus more frequently, and exhibiting fewer pauses, retries, and surrender actions. These findings demonstrate that implicit behavioral traces captured during gameplay is promising in predicting software-development suitability without explicit personality testing, supporting serious games as a scalable, engaging, and less biased alternative for career assessment.", "AI": {"tldr": "\u4f7f\u7528\u591a\u7c7b\u578b\u4e25\u8083\u6e38\u620f\u6846\u67b6\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u8f6f\u4ef6\u5f00\u53d1\u5c97\u4f4d\u9002\u914d\u6027\uff0c\u901a\u8fc7\u6e38\u620f\u884c\u4e3a\u6570\u636e\u800c\u975e\u4f20\u7edf\u95ee\u5377\u8bc4\u4f30\u4eba\u683c\u7279\u8d28\uff0c\u8fbe\u523097%\u7cbe\u5ea6\u548c94%\u51c6\u786e\u7387\u3002", "motivation": "\u4f20\u7edf\u804c\u4e1a\u8bc4\u4f30\u4f9d\u8d56\u81ea\u6211\u62a5\u544a\u95ee\u5377\uff0c\u5b58\u5728\u56de\u7b54\u504f\u5dee\u3001\u75b2\u52b3\u548c\u6545\u610f\u626d\u66f2\u7b49\u95ee\u9898\u3002\u6e38\u620f\u5316\u8bc4\u4f30\u901a\u8fc7\u6355\u6349\u6e38\u620f\u4e2d\u7684\u9690\u5f0f\u884c\u4e3a\u4fe1\u53f7\uff0c\u63d0\u4f9b\u66f4\u5ba2\u89c2\u3001\u53ef\u6269\u5c55\u4e14\u504f\u8bef\u66f4\u5c11\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u548c\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u5b9e\u8bc1\u7814\u7a76\u786e\u5b9a\u76f8\u5173\u4eba\u683c\u884c\u4e3a\u7279\u8d28\uff0c\u8bbe\u8ba1\u5b9a\u5236\u79fb\u52a8\u6e38\u620f\u6fc0\u53d1\u95ee\u9898\u89e3\u51b3\u3001\u89c4\u5212\u3001\u9002\u5e94\u6027\u7b49\u884c\u4e3a\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u5efa\u6a21\u7b56\u7565\uff0c\u4ec5\u4ece\u6e38\u620f\u884c\u4e3a\u7279\u5f81\u9884\u6d4b\u5c97\u4f4d\u9002\u914d\u6027\u3002", "result": "\u6a21\u578b\u8fbe\u523097%\u7cbe\u5ea6\u548c94%\u51c6\u786e\u7387\u3002\u5408\u9002\u5019\u9009\u4eba\u8868\u73b0\u51fa\u72ec\u7279\u6e38\u620f\u6a21\u5f0f\uff1a\u89e3\u8c1c\u6e38\u620f\u83b7\u80dc\u66f4\u591a\u3001\u5b8c\u6210\u66f4\u591a\u4fa7\u6311\u6218\u3001\u66f4\u9891\u7e41\u5bfc\u822a\u83dc\u5355\u3001\u8f83\u5c11\u6682\u505c/\u91cd\u8bd5/\u653e\u5f03\u884c\u4e3a\u3002", "conclusion": "\u6e38\u620f\u8fc7\u7a0b\u4e2d\u6355\u6349\u7684\u9690\u5f0f\u884c\u4e3a\u75d5\u8ff9\u80fd\u6709\u6548\u9884\u6d4b\u8f6f\u4ef6\u5f00\u53d1\u9002\u914d\u6027\uff0c\u65e0\u9700\u663e\u5f0f\u4eba\u683c\u6d4b\u8bd5\u3002\u4e25\u8083\u6e38\u620f\u53ef\u4f5c\u4e3a\u804c\u4e1a\u8bc4\u4f30\u7684\u53ef\u6269\u5c55\u3001\u5438\u5f15\u4eba\u4e14\u504f\u8bef\u66f4\u5c11\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2601.01972", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01972", "abs": "https://arxiv.org/abs/2601.01972", "authors": ["Alexandre Le Mercier", "Chris Develder", "Thomas Demeester"], "title": "Hidden State Poisoning Attacks against Mamba-based Language Models", "comment": "17 pages, 4 figures. Submitted to ACL 2026", "summary": "State space models (SSMs) like Mamba offer efficient alternatives to Transformer-based language models, with linear time complexity. Yet, their adversarial robustness remains critically unexplored. This paper studies the phenomenon whereby specific short input phrases induce a partial amnesia effect in such models, by irreversibly overwriting information in their hidden states, referred to as a Hidden State Poisoning Attack (HiSPA). Our benchmark RoBench25 allows evaluating a model's information retrieval capabilities when subject to HiSPAs, and confirms the vulnerability of SSMs against such attacks. Even a recent 52B hybrid SSM-Transformer model from the Jamba family collapses on RoBench25 under optimized HiSPA triggers, whereas pure Transformers do not. We also observe that HiSPA triggers significantly weaken the Jamba model on the popular Open-Prompt-Injections benchmark, unlike pure Transformers. Finally, our interpretability study reveals patterns in Mamba's hidden layers during HiSPAs that could be used to build a HiSPA mitigation system. The full code and data to reproduce the experiments can be found at https://anonymous.4open.science/r/hispa_anonymous-5DB0.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08\u5982Mamba\uff09\u4e2d\u9690\u85cf\u72b6\u6001\u4e2d\u6bd2\u653b\u51fb\uff08HiSPA\uff09\u73b0\u8c61\uff0c\u53d1\u73b0\u7279\u5b9a\u77ed\u8f93\u5165\u77ed\u8bed\u4f1a\u4e0d\u53ef\u9006\u5730\u8986\u76d6\u9690\u85cf\u72b6\u6001\u4fe1\u606f\uff0c\u5bfc\u81f4\u6a21\u578b\u51fa\u73b0\u90e8\u5206\u5931\u5fc6\u6548\u5e94\u3002", "motivation": "\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSMs\uff09\u5982Mamba\u867d\u7136\u63d0\u4f9b\u4e86\u6bd4Transformer\u66f4\u9ad8\u6548\u7684\u8bed\u8a00\u6a21\u578b\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u5176\u5bf9\u6297\u9c81\u68d2\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22SSMs\u5728\u9762\u5bf9\u7279\u5b9a\u653b\u51fb\u65f6\u7684\u8106\u5f31\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u9690\u85cf\u72b6\u6001\u4e2d\u6bd2\u653b\u51fb\uff08HiSPA\uff09\u65b9\u6cd5\uff0c\u5f00\u53d1\u4e86RoBench25\u57fa\u51c6\u6765\u8bc4\u4f30\u6a21\u578b\u5728HiSPA\u4e0b\u7684\u4fe1\u606f\u68c0\u7d22\u80fd\u529b\uff0c\u5e76\u5bf9Mamba\u6a21\u578b\u7684\u9690\u85cf\u5c42\u8fdb\u884c\u4e86\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u3002", "result": "SSMs\u5bf9HiSPA\u653b\u51fb\u9ad8\u5ea6\u8106\u5f31\uff0c\u5373\u4f7f\u662f\u6700\u8fd152B\u53c2\u6570\u7684Jamba\u6df7\u5408\u6a21\u578b\u5728\u4f18\u5316\u540e\u7684HiSPA\u89e6\u53d1\u8bcd\u4e0b\u4e5f\u4f1a\u5d29\u6e83\uff0c\u800c\u7eafTransformer\u6a21\u578b\u5219\u4e0d\u53d7\u5f71\u54cd\u3002HiSPA\u89e6\u53d1\u8bcd\u8fd8\u4f1a\u663e\u8457\u524a\u5f31Jamba\u5728Open-Prompt-Injections\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\u3002", "conclusion": "\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u5b58\u5728\u9690\u85cf\u72b6\u6001\u4e2d\u6bd2\u653b\u51fb\u7684\u4e25\u91cd\u5b89\u5168\u6f0f\u6d1e\uff0c\u9700\u8981\u5f00\u53d1\u76f8\u5e94\u7684\u9632\u5fa1\u673a\u5236\u3002\u7814\u7a76\u53d1\u73b0\u7684\u9690\u85cf\u5c42\u6a21\u5f0f\u53ef\u7528\u4e8e\u6784\u5efaHiSPA\u7f13\u89e3\u7cfb\u7edf\u3002"}}
{"id": "2601.02015", "categories": ["cs.CL", "cs.AI", "cs.IT"], "pdf": "https://arxiv.org/pdf/2601.02015", "abs": "https://arxiv.org/abs/2601.02015", "authors": ["Omar Momen", "Emilie Sitter", "Berenike Herrmann", "Sina Zarrie\u00df"], "title": "Surprisal and Metaphor Novelty: Moderate Correlations and Divergent Scaling Effects", "comment": "to be published at EACL 2026 main conference", "summary": "Novel metaphor comprehension involves complex semantic processes and linguistic creativity, making it an interesting task for studying language models (LMs). This study investigates whether surprisal, a probabilistic measure of predictability in LMs, correlates with different metaphor novelty datasets. We analyse surprisal from 16 LM variants on corpus-based and synthetic metaphor novelty datasets. We explore a cloze-style surprisal method that conditions on full-sentence context. Results show that LMs yield significant moderate correlations with scores/labels of metaphor novelty. We further identify divergent scaling patterns: on corpus-based data, correlation strength decreases with model size (inverse scaling effect), whereas on synthetic data it increases (Quality-Power Hypothesis). We conclude that while surprisal can partially account for annotations of metaphor novelty, it remains a limited metric of linguistic creativity.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u60ca\u5947\u5ea6(surprisal)\u80fd\u5426\u9884\u6d4b\u9690\u55bb\u65b0\u9896\u6027\uff0c\u53d1\u73b0\u60ca\u5947\u5ea6\u4e0e\u9690\u55bb\u65b0\u9896\u6027\u8bc4\u5206\u5b58\u5728\u4e2d\u7b49\u76f8\u5173\u6027\uff0c\u4f46\u76f8\u5173\u6027\u968f\u6a21\u578b\u5927\u5c0f\u548c\u6570\u636e\u7c7b\u578b\u5448\u73b0\u4e0d\u540c\u53d8\u5316\u6a21\u5f0f\u3002", "motivation": "\u65b0\u9896\u9690\u55bb\u7406\u89e3\u6d89\u53ca\u590d\u6742\u7684\u8bed\u4e49\u8fc7\u7a0b\u548c\u8bed\u8a00\u521b\u9020\u529b\uff0c\u662f\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u7684\u6709\u8da3\u4efb\u52a1\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6982\u7387\u6027\u9884\u6d4b\u6307\u6807\u2014\u2014\u60ca\u5947\u5ea6\uff0c\u662f\u5426\u4e0e\u4e0d\u540c\u9690\u55bb\u65b0\u9896\u6027\u6570\u636e\u96c6\u76f8\u5173\u3002", "method": "\u4f7f\u752816\u79cd\u8bed\u8a00\u6a21\u578b\u53d8\u4f53\u5206\u6790\u57fa\u4e8e\u8bed\u6599\u5e93\u548c\u5408\u6210\u9690\u55bb\u65b0\u9896\u6027\u6570\u636e\u96c6\u4e2d\u7684\u60ca\u5947\u5ea6\u3002\u91c7\u7528\u586b\u7a7a\u5f0f\u60ca\u5947\u5ea6\u65b9\u6cd5\uff0c\u57fa\u4e8e\u5b8c\u6574\u53e5\u5b50\u4e0a\u4e0b\u6587\u8fdb\u884c\u8ba1\u7b97\u3002", "result": "\u8bed\u8a00\u6a21\u578b\u4e0e\u9690\u55bb\u65b0\u9896\u6027\u8bc4\u5206/\u6807\u7b7e\u5b58\u5728\u663e\u8457\u7684\u4e2d\u7b49\u76f8\u5173\u6027\u3002\u53d1\u73b0\u4e0d\u540c\u7684\u7f29\u653e\u6a21\u5f0f\uff1a\u5728\u57fa\u4e8e\u8bed\u6599\u5e93\u7684\u6570\u636e\u4e0a\uff0c\u76f8\u5173\u6027\u5f3a\u5ea6\u968f\u6a21\u578b\u5927\u5c0f\u51cf\u5c0f\uff08\u9006\u7f29\u653e\u6548\u5e94\uff09\uff1b\u5728\u5408\u6210\u6570\u636e\u4e0a\u5219\u968f\u6a21\u578b\u5927\u5c0f\u589e\u52a0\uff08\u8d28\u91cf-\u80fd\u529b\u5047\u8bf4\uff09\u3002", "conclusion": "\u867d\u7136\u60ca\u5947\u5ea6\u80fd\u591f\u90e8\u5206\u89e3\u91ca\u9690\u55bb\u65b0\u9896\u6027\u7684\u6807\u6ce8\uff0c\u4f46\u5b83\u4ecd\u7136\u662f\u8bed\u8a00\u521b\u9020\u529b\u7684\u4e00\u79cd\u6709\u9650\u5ea6\u91cf\u6307\u6807\u3002\u60ca\u5947\u5ea6\u53ea\u80fd\u90e8\u5206\u6355\u6349\u9690\u55bb\u65b0\u9896\u6027\u7684\u590d\u6742\u8bed\u4e49\u8fc7\u7a0b\u3002"}}
{"id": "2601.02023", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02023", "abs": "https://arxiv.org/abs/2601.02023", "authors": ["Amirali Ebrahimzadeh", "Seyyed M. Salili"], "title": "Not All Needles Are Found: How Fact Distribution and Don't Make It Up Prompts Shape Literal Extraction, Logical Inference, and Hallucination Risks in Long-Context LLMs", "comment": "25 pages, 8 figures, 3 tables", "summary": "Large language models (LLMs) increasingly support very long input contexts. Yet it remains unclear how reliably they extract and infer information at scale. Performance varies with context length and strongly interacts with how information is distributed in real-world corpora. Motivated by these observations, we study how fact placement, corpus-level fact distributions, and Don't Make It Up prompts influence model behavior. We introduce an extended needle-in-a-haystack benchmark across four production-scale models: Gemini-2.5-flash, ChatGPT-5-mini, Claude-4.5-haiku, and Deepseek-v3.2-chat. Unlike prior work, we separately evaluate literal extraction, logical inference, and hallucination risk. Our study considers both positional effects and realistic distributions of evidence across long contexts, as well as prompts that explicitly discourage fabrication. We find that longer contexts alone do not guarantee better performance and can be detrimental when relevant evidence is diluted or widely dispersed. Performance varies substantially across models: some show severe degradation under realistic conditions, while others remain more robust at longer context lengths. Anti-hallucination (AH) instructions can make some models overly conservative, sharply reducing accuracy in literal extraction and logical inference. While we do not directly compare retrieval-augmented generation (RAG) and cache-augmented generation (CAG), our results suggest many failures stem from ineffective context utilization. Models often struggle to identify and prioritize relevant information even when it is present. These findings have direct practical implications, as enterprise workflows increasingly involve pasting large volumes of unfiltered documents into LLM prompts. Effective context length and model-specific robustness to long contexts are therefore critical for reliable LLM deployment in research and business.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u957f\u4e0a\u4e0b\u6587\u5e76\u4e0d\u4fdd\u8bc1\u66f4\u597d\u7684\u6027\u80fd\uff0c\u5f53\u76f8\u5173\u4fe1\u606f\u88ab\u7a00\u91ca\u6216\u5206\u6563\u65f6\u53cd\u800c\u6709\u5bb3\u3002\u4e0d\u540c\u6a21\u578b\u8868\u73b0\u5dee\u5f02\u5927\uff0c\u6297\u5e7b\u89c9\u6307\u4ee4\u53ef\u80fd\u4f7f\u6a21\u578b\u8fc7\u4e8e\u4fdd\u5b88\u800c\u964d\u4f4e\u51c6\u786e\u6027\u3002", "motivation": "\u968f\u7740LLM\u652f\u6301\u8d8a\u6765\u8d8a\u957f\u7684\u8f93\u5165\u4e0a\u4e0b\u6587\uff0c\u4f46\u5b83\u4eec\u5728\u957f\u4e0a\u4e0b\u6587\u4e2d\u53ef\u9760\u63d0\u53d6\u548c\u63a8\u7406\u4fe1\u606f\u7684\u80fd\u529b\u5c1a\u4e0d\u6e05\u695a\u3002\u6027\u80fd\u968f\u4e0a\u4e0b\u6587\u957f\u5ea6\u53d8\u5316\uff0c\u4e14\u4e0e\u771f\u5b9e\u8bed\u6599\u4e2d\u4fe1\u606f\u5206\u5e03\u65b9\u5f0f\u5f3a\u70c8\u4ea4\u4e92\u3002\u4f01\u4e1a\u5de5\u4f5c\u6d41\u4e2d\u7ecf\u5e38\u5c06\u5927\u91cf\u672a\u8fc7\u6ee4\u6587\u6863\u7c98\u8d34\u5230LLM\u63d0\u793a\u4e2d\uff0c\u56e0\u6b64\u7406\u89e3\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u4e2d\u7684\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f15\u5165\u6269\u5c55\u7684\"\u5927\u6d77\u635e\u9488\"\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5728\u56db\u4e2a\u751f\u4ea7\u7ea7\u6a21\u578b(Gemini-2.5-flash\u3001ChatGPT-5-mini\u3001Claude-4.5-haiku\u3001Deepseek-v3.2-chat)\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002\u5206\u522b\u8bc4\u4f30\u5b57\u9762\u63d0\u53d6\u3001\u903b\u8f91\u63a8\u7406\u548c\u5e7b\u89c9\u98ce\u9669\u3002\u7814\u7a76\u4f4d\u7f6e\u6548\u5e94\u3001\u8bc1\u636e\u5728\u957f\u4e0a\u4e0b\u6587\u4e2d\u7684\u771f\u5b9e\u5206\u5e03\uff0c\u4ee5\u53ca\u660e\u786e\u963b\u6b62\u634f\u9020\u7684\u63d0\u793a\u3002", "result": "\u4ec5\u589e\u52a0\u4e0a\u4e0b\u6587\u957f\u5ea6\u5e76\u4e0d\u4fdd\u8bc1\u66f4\u597d\u6027\u80fd\uff0c\u5f53\u76f8\u5173\u8bc1\u636e\u88ab\u7a00\u91ca\u6216\u5e7f\u6cdb\u5206\u6563\u65f6\u53cd\u800c\u6709\u5bb3\u3002\u4e0d\u540c\u6a21\u578b\u8868\u73b0\u5dee\u5f02\u663e\u8457\uff1a\u4e00\u4e9b\u5728\u771f\u5b9e\u6761\u4ef6\u4e0b\u4e25\u91cd\u9000\u5316\uff0c\u800c\u53e6\u4e00\u4e9b\u5728\u66f4\u957f\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e0b\u4fdd\u6301\u66f4\u7a33\u5065\u3002\u6297\u5e7b\u89c9\u6307\u4ee4\u53ef\u80fd\u4f7f\u67d0\u4e9b\u6a21\u578b\u8fc7\u4e8e\u4fdd\u5b88\uff0c\u663e\u8457\u964d\u4f4e\u5b57\u9762\u63d0\u53d6\u548c\u903b\u8f91\u63a8\u7406\u7684\u51c6\u786e\u6027\u3002\u6a21\u578b\u7ecf\u5e38\u96be\u4ee5\u8bc6\u522b\u548c\u4f18\u5148\u5904\u7406\u76f8\u5173\u4fe1\u606f\uff0c\u5373\u4f7f\u4fe1\u606f\u5b58\u5728\u3002", "conclusion": "\u6709\u6548\u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\u548c\u6a21\u578b\u5bf9\u957f\u4e0a\u4e0b\u6587\u7684\u7279\u5b9a\u9c81\u68d2\u6027\u5bf9\u4e8eLLM\u5728\u7814\u7a76\u548c\u4e1a\u52a1\u4e2d\u7684\u53ef\u9760\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u3002\u8bb8\u591a\u5931\u8d25\u6e90\u4e8e\u65e0\u6548\u7684\u4e0a\u4e0b\u6587\u5229\u7528\uff0c\u800c\u975e\u4fe1\u606f\u7f3a\u5931\u3002\u4f01\u4e1a\u5de5\u4f5c\u6d41\u4e2d\u5927\u91cf\u7c98\u8d34\u672a\u8fc7\u6ee4\u6587\u6863\u7684\u505a\u6cd5\u9700\u8981\u8c28\u614e\u8003\u8651\u6a21\u578b\u7684\u5b9e\u9645\u4e0a\u4e0b\u6587\u5904\u7406\u80fd\u529b\u3002"}}
{"id": "2601.01231", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01231", "abs": "https://arxiv.org/abs/2601.01231", "authors": ["Md Muhtasim Munif Fahim", "Humyra Ankona", "Md Monimul Huq", "Md Rezaul Karim"], "title": "The Dependency Divide: An Interpretable Machine Learning Framework for Profiling Student Digital Satisfaction in the Bangladesh Context", "comment": "Conference Paper", "summary": "Background: While digital access has expanded rapidly in resource-constrained contexts, satisfaction with digital learning platforms varies significantly among students with seemingly equal connectivity. Traditional digital divide frameworks fail to explain these variations.\n  Purpose: This study introduces the \"Dependency Divide\", a novel framework proposing that highly engaged students become conditionally vulnerable to infrastructure failures, challenging assumptions that engagement uniformly benefits learners in post-access environments.\n  Methods: We conducted a cross-sectional study of 396 university students in Bangladesh using a three-stage analytical approach: (1) stability-validated K-prototypes clustering to identify student profiles, (2) profile-specific Random Forest models with SHAP and ALE analysis to determine satisfaction drivers, and (3) formal interaction analysis with propensity score matching to test the Dependency Divide hypothesis.\n  Results: Three distinct profiles emerged: Casually Engaged (58%), Efficient Learners (35%), and Hyper-Engaged (7%). A significant interaction between educational device time and internet reliability (\\b{eta} = 0.033, p = 0.028) confirmed the Dependency Divide: engagement increased satisfaction only when infrastructure remained reliable. Hyper-Engaged students showed greatest vulnerability despite or because of their sophisticated digital workflows. Policy simulations demonstrated that targeted reliability improvements for high-dependency users yielded 2.06 times greater returns than uniform interventions.\n  Conclusions: In fragile infrastructure contexts, capability can become liability. Digital transformation policies must prioritize reliability for dependency-prone users, establish contingency systems, and educate students about dependency risks rather than uniformly promoting engagement.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u6570\u5b57\u5b66\u4e60\u4e2d\u7684\"\u4f9d\u8d56\u9e3f\u6c9f\"\u73b0\u8c61\uff1a\u9ad8\u5ea6\u6295\u5165\u7684\u5b66\u751f\u5728\u57fa\u7840\u8bbe\u65bd\u4e0d\u53ef\u9760\u65f6\u53cd\u800c\u66f4\u6613\u53d7\u632b\uff0c\u6311\u6218\u4e86\u4f20\u7edf\u8ba4\u4e3a\u6295\u5165\u8d8a\u591a\u8d8a\u53d7\u76ca\u7684\u5047\u8bbe\u3002", "motivation": "\u4f20\u7edf\u6570\u5b57\u9e3f\u6c9f\u6846\u67b6\u65e0\u6cd5\u89e3\u91ca\u5728\u8d44\u6e90\u6709\u9650\u73af\u5883\u4e2d\uff0c\u62e5\u6709\u76f8\u4f3c\u8fde\u63a5\u6761\u4ef6\u7684\u5b66\u751f\u5bf9\u6570\u5b57\u5b66\u4e60\u5e73\u53f0\u6ee1\u610f\u5ea6\u5dee\u5f02\u663e\u8457\u7684\u95ee\u9898\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5728\u57fa\u7840\u8bbe\u65bd\u8106\u5f31\u7684\u540e\u63a5\u5165\u73af\u5883\u4e2d\uff0c\u5b66\u751f\u53c2\u4e0e\u5ea6\u4e0e\u6ee1\u610f\u5ea6\u4e4b\u95f4\u7684\u590d\u6742\u5173\u7cfb\u3002", "method": "\u5bf9\u5b5f\u52a0\u62c9\u56fd396\u540d\u5927\u5b66\u751f\u8fdb\u884c\u6a2a\u65ad\u9762\u7814\u7a76\uff0c\u91c7\u7528\u4e09\u9636\u6bb5\u5206\u6790\u65b9\u6cd5\uff1a1) K-prototypes\u805a\u7c7b\u8bc6\u522b\u5b66\u751f\u7279\u5f81\uff1b2) \u968f\u673a\u68ee\u6797\u6a21\u578b\u7ed3\u5408SHAP\u548cALE\u5206\u6790\u786e\u5b9a\u6ee1\u610f\u5ea6\u9a71\u52a8\u56e0\u7d20\uff1b3) \u503e\u5411\u5f97\u5206\u5339\u914d\u8fdb\u884c\u6b63\u5f0f\u4ea4\u4e92\u5206\u6790\u9a8c\u8bc1\u4f9d\u8d56\u9e3f\u6c9f\u5047\u8bbe\u3002", "result": "\u8bc6\u522b\u51fa\u4e09\u7c7b\u5b66\u751f\uff1a\u5076\u5c14\u53c2\u4e0e\u578b(58%)\u3001\u9ad8\u6548\u5b66\u4e60\u8005(35%)\u548c\u9ad8\u5ea6\u6295\u5165\u578b(7%)\u3002\u53d1\u73b0\u6559\u80b2\u8bbe\u5907\u4f7f\u7528\u65f6\u95f4\u4e0e\u7f51\u7edc\u53ef\u9760\u6027\u5b58\u5728\u663e\u8457\u4ea4\u4e92\u4f5c\u7528\uff0c\u8bc1\u5b9e\u4e86\u4f9d\u8d56\u9e3f\u6c9f\u7684\u5b58\u5728\u3002\u9ad8\u5ea6\u6295\u5165\u578b\u5b66\u751f\u5c3d\u7ba1\u62e5\u6709\u590d\u6742\u6570\u5b57\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5374\u8868\u73b0\u51fa\u6700\u5927\u8106\u5f31\u6027\u3002\u653f\u7b56\u6a21\u62df\u663e\u793a\uff0c\u9488\u5bf9\u9ad8\u4f9d\u8d56\u7528\u6237\u7684\u53ef\u9760\u6027\u6539\u5584\u6bd4\u7edf\u4e00\u5e72\u9884\u6548\u679c\u9ad82.06\u500d\u3002", "conclusion": "\u5728\u57fa\u7840\u8bbe\u65bd\u8106\u5f31\u7684\u73af\u5883\u4e2d\uff0c\u80fd\u529b\u53ef\u80fd\u6210\u4e3a\u8d1f\u62c5\u3002\u6570\u5b57\u8f6c\u578b\u653f\u7b56\u5e94\u4f18\u5148\u4fdd\u969c\u4f9d\u8d56\u503e\u5411\u7528\u6237\u7684\u53ef\u9760\u6027\uff0c\u5efa\u7acb\u5e94\u6025\u7cfb\u7edf\uff0c\u5e76\u6559\u80b2\u5b66\u751f\u4e86\u89e3\u4f9d\u8d56\u98ce\u9669\uff0c\u800c\u975e\u7edf\u4e00\u63a8\u5e7f\u53c2\u4e0e\u5ea6\u3002"}}
{"id": "2601.02065", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02065", "abs": "https://arxiv.org/abs/2601.02065", "authors": ["Md. Asif Hossain", "Nabil Subhan", "Mantasha Rahman Mahi", "Jannatul Ferdous Nabila"], "title": "Cost-Efficient Cross-Lingual Retrieval-Augmented Generation for Low-Resource Languages: A Case Study in Bengali Agricultural Advisory", "comment": "5 pages, 3 figures, 1 table", "summary": "Access to reliable agricultural advisory remains limited in many developing regions due to a persistent language barrier: authoritative agricultural manuals are predominantly written in English, while farmers primarily communicate in low-resource local languages such as Bengali. Although recent advances in Large Language Models (LLMs) enable natural language interaction, direct generation in low-resource languages often exhibits poor fluency and factual inconsistency, while cloud-based solutions remain cost-prohibitive. This paper presents a cost-efficient, cross-lingual Retrieval-Augmented Generation (RAG) framework for Bengali agricultural advisory that emphasizes factual grounding and practical deployability. The proposed system adopts a translation-centric architecture in which Bengali user queries are translated into English, enriched through domain-specific keyword injection to align colloquial farmer terminology with scientific nomenclature, and answered via dense vector retrieval over a curated corpus of English agricultural manuals (FAO, IRRI). The generated English response is subsequently translated back into Bengali to ensure accessibility. The system is implemented entirely using open-source models and operates on consumer-grade hardware without reliance on paid APIs. Experimental evaluation demonstrates reliable source-grounded responses, robust rejection of out-of-domain queries, and an average end-to-end latency below 20 seconds. The results indicate that cross-lingual retrieval combined with controlled translation offers a practical and scalable solution for agricultural knowledge access in low-resource language settings", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u9762\u5411\u5b5f\u52a0\u62c9\u8bed\u519c\u4e1a\u54a8\u8be2\u7684\u8de8\u8bed\u8a00\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u7ffb\u8bd1-\u68c0\u7d22-\u518d\u7ffb\u8bd1\u7684\u6d41\u7a0b\uff0c\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u5b9e\u73b0\u4f4e\u6210\u672c\u3001\u4e8b\u5b9e\u53ef\u9760\u7684\u519c\u4e1a\u77e5\u8bc6\u8bbf\u95ee", "motivation": "\u5728\u8bb8\u591a\u53d1\u5c55\u4e2d\u5730\u533a\uff0c\u6743\u5a01\u519c\u4e1a\u624b\u518c\u4e3b\u8981\u662f\u82f1\u6587\u7f16\u5199\uff0c\u800c\u519c\u6c11\u4e3b\u8981\u4f7f\u7528\u5b5f\u52a0\u62c9\u8bed\u7b49\u4f4e\u8d44\u6e90\u672c\u5730\u8bed\u8a00\uff0c\u5b58\u5728\u8bed\u8a00\u969c\u788d\u3002\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u652f\u6301\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\uff0c\u4f46\u76f4\u63a5\u751f\u6210\u4f4e\u8d44\u6e90\u8bed\u8a00\u5b58\u5728\u6d41\u7545\u6027\u548c\u4e8b\u5b9e\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u4e91\u7aef\u89e3\u51b3\u65b9\u6848\u6210\u672c\u9ad8\u6602", "method": "\u91c7\u7528\u7ffb\u8bd1\u4e3a\u4e2d\u5fc3\u7684\u67b6\u6784\uff1a\u5c06\u5b5f\u52a0\u62c9\u8bed\u7528\u6237\u67e5\u8be2\u7ffb\u8bd1\u6210\u82f1\u6587\uff0c\u901a\u8fc7\u9886\u57df\u7279\u5b9a\u5173\u952e\u8bcd\u6ce8\u5165\u5c06\u519c\u6c11\u53e3\u8bed\u672f\u8bed\u4e0e\u79d1\u5b66\u672f\u8bed\u5bf9\u9f50\uff0c\u5728\u82f1\u6587\u519c\u4e1a\u624b\u518c\uff08FAO\u3001IRRI\uff09\u4e0a\u8fdb\u884c\u5bc6\u96c6\u5411\u91cf\u68c0\u7d22\uff0c\u751f\u6210\u7684\u82f1\u6587\u56de\u7b54\u518d\u7ffb\u8bd1\u56de\u5b5f\u52a0\u62c9\u8bed\u3002\u5b8c\u5168\u4f7f\u7528\u5f00\u6e90\u6a21\u578b\uff0c\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u8fd0\u884c", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\u7cfb\u7edf\u80fd\u63d0\u4f9b\u53ef\u9760\u7684\u4e8b\u5b9e\u4f9d\u636e\u56de\u7b54\uff0c\u5bf9\u9886\u57df\u5916\u67e5\u8be2\u6709\u9c81\u68d2\u7684\u62d2\u7edd\u80fd\u529b\uff0c\u5e73\u5747\u7aef\u5230\u7aef\u5ef6\u8fdf\u4f4e\u4e8e20\u79d2\u3002\u7cfb\u7edf\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u8fd0\u884c\uff0c\u65e0\u9700\u4ed8\u8d39API", "conclusion": "\u8de8\u8bed\u8a00\u68c0\u7d22\u7ed3\u5408\u53d7\u63a7\u7ffb\u8bd1\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u519c\u4e1a\u77e5\u8bc6\u8bbf\u95ee\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e73\u8861\u4e86\u6210\u672c\u3001\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u53ef\u90e8\u7f72\u6027"}}
{"id": "2601.01237", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01237", "abs": "https://arxiv.org/abs/2601.01237", "authors": ["Abidemi Koledoye", "Chinemerem Unachukwu", "Gold Nwobu", "Hasin Rana"], "title": "Benchmarking the Computational and Representational Efficiency of State Space Models against Transformers on Long-Context Dyadic Sessions", "comment": "14 pages", "summary": "State Space Models (SSMs) have emerged as a promising alternative to Transformers for long-context sequence modeling, offering linear $O(N)$ computational complexity compared to the Transformer's quadratic $O(N^2)$ scaling. This paper presents a comprehensive benchmarking study comparing the Mamba SSM against the LLaMA Transformer on long-context sequences, using dyadic therapy sessions as a representative test case. We evaluate both architectures across two dimensions: (1) computational efficiency, where we measure memory usage and inference speed from 512 to 8,192 tokens, and (2) representational efficiency, where we analyze hidden state dynamics and attention patterns. Our findings provide actionable insights for practitioners working with long-context applications, establishing precise conditions under which SSMs offer advantages over Transformers.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5bf9\u6bd4\u4e86Mamba SSM\u4e0eLLaMA Transformer\u5728\u957f\u4e0a\u4e0b\u6587\u5e8f\u5217\u5efa\u6a21\u4e2d\u7684\u6027\u80fd\uff0c\u7279\u522b\u4ee5\u6cbb\u7597\u4f1a\u8bdd\u4e3a\u6d4b\u8bd5\u6848\u4f8b\uff0c\u53d1\u73b0SSM\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u4e0a\u5177\u6709\u7ebf\u6027\u4f18\u52bf\uff0c\u4f46\u5728\u67d0\u4e9b\u6761\u4ef6\u4e0bTransformer\u4ecd\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u968f\u7740\u957f\u4e0a\u4e0b\u6587\u5e8f\u5217\u5efa\u6a21\u9700\u6c42\u7684\u589e\u957f\uff0c\u9700\u8981\u5bfb\u627e\u6bd4Transformer\u66f4\u9ad8\u6548\u7684\u67b6\u6784\u3002SSM\u56e0\u5176\u7ebf\u6027\u8ba1\u7b97\u590d\u6742\u5ea6\u800c\u6210\u4e3a\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u7f3a\u4e4f\u4e0eTransformer\u5728\u5b9e\u9645\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e2d\u7684\u7cfb\u7edf\u6bd4\u8f83\u3002", "method": "\u4f7f\u7528\u6cbb\u7597\u4f1a\u8bdd\u4f5c\u4e3a\u4ee3\u8868\u6027\u6d4b\u8bd5\u6848\u4f8b\uff0c\u4ece\u4e24\u4e2a\u7ef4\u5ea6\u5bf9\u6bd4Mamba SSM\u548cLLaMA Transformer\uff1a1) \u8ba1\u7b97\u6548\u7387\uff08\u5185\u5b58\u4f7f\u7528\u548c\u63a8\u7406\u901f\u5ea6\uff0c\u8303\u56f4512-8192\u4e2atoken\uff09\uff1b2) \u8868\u5f81\u6548\u7387\uff08\u9690\u85cf\u72b6\u6001\u52a8\u6001\u548c\u6ce8\u610f\u529b\u6a21\u5f0f\u5206\u6790\uff09\u3002", "result": "SSM\u5728\u957f\u5e8f\u5217\u4e0a\u786e\u5b9e\u5c55\u73b0\u51fa\u7ebf\u6027\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u4f18\u52bf\uff0c\u5185\u5b58\u4f7f\u7528\u548c\u63a8\u7406\u901f\u5ea6\u4f18\u4e8eTransformer\u3002\u4f46\u5728\u67d0\u4e9b\u7279\u5b9a\u6761\u4ef6\u4e0b\uff0cTransformer\u5728\u8868\u5f81\u6548\u7387\u65b9\u9762\u4ecd\u4fdd\u6301\u4f18\u52bf\u3002\u7814\u7a76\u4e3a\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u9009\u62e9\u67b6\u6784\u7684\u5177\u4f53\u6307\u5bfc\u3002", "conclusion": "SSM\u662fTransformer\u5728\u957f\u4e0a\u4e0b\u6587\u5e94\u7528\u4e2d\u7684\u6709\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u8ba1\u7b97\u6548\u7387\u8981\u6c42\u9ad8\u7684\u573a\u666f\u4e0b\u3002\u7814\u7a76\u660e\u786e\u4e86SSM\u76f8\u5bf9\u4e8eTransformer\u7684\u4f18\u52bf\u6761\u4ef6\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u51b3\u7b56\u4f9d\u636e\u3002"}}
{"id": "2601.02076", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02076", "abs": "https://arxiv.org/abs/2601.02076", "authors": ["Yingte Shu", "Yuchuan Tian", "Chao Xu", "Yunhe Wang", "Hanting Chen"], "title": "Deferred Commitment Decoding for Diffusion Language Models with Confidence-Aware Sliding Windows", "comment": null, "summary": "Diffusion language models (DLMs) have recently emerged as a strong alternative to autoregressive models by enabling parallel text generation. To improve inference efficiency and KV-cache compatibility, prior work commonly adopts block-based diffusion, decoding tokens block by block. However, this paradigm suffers from a structural limitation that we term Boundary-Induced Context Truncation (BICT): undecoded tokens near block boundaries are forced to commit without access to nearby future context, even when such context could substantially reduce uncertainty. This limitation degrades decoding confidence and generation quality, especially for tasks requiring precise reasoning, such as mathematical problem solving and code generation. We propose Deferred Commitment Decoding (DCD), a novel, training-free decoding strategy that mitigates this issue. DCD maintains a confidence-aware sliding window over masked tokens, resolving low-uncertainty tokens early while deferring high-uncertainty tokens until sufficient contextual evidence becomes available. This design enables effective bidirectional information flow within the decoding window without sacrificing efficiency. Extensive experiments across multiple diffusion language models, benchmarks, and caching configurations show that DCD improves generation accuracy by 1.39% with comparable time on average compared to fixed block-based diffusion methods, with the most significant improvement reaching 9.0%. These results demonstrate that deferring token commitment based on uncertainty is a simple yet effective principle for improving both the quality and efficiency of diffusion language model decoding.", "AI": {"tldr": "\u63d0\u51faDeferred Commitment Decoding (DCD)\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u6ed1\u52a8\u7a97\u53e3\u5ef6\u8fdf\u9ad8\u4e0d\u786e\u5b9a\u6027token\u7684\u51b3\u7b56\uff0c\u89e3\u51b3\u5757\u6269\u6563\u6a21\u578b\u4e2d\u8fb9\u754c\u4e0a\u4e0b\u6587\u622a\u65ad\u95ee\u9898\uff0c\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u5757\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u8fb9\u754c\u8bf1\u5bfc\u4e0a\u4e0b\u6587\u622a\u65ad(BICT)\u95ee\u9898\uff1a\u5757\u8fb9\u754c\u9644\u8fd1\u7684\u672a\u89e3\u7801token\u5728\u7f3a\u4e4f\u672a\u6765\u4e0a\u4e0b\u6587\u7684\u60c5\u51b5\u4e0b\u88ab\u8feb\u51b3\u7b56\uff0c\u5bfc\u81f4\u89e3\u7801\u7f6e\u4fe1\u5ea6\u548c\u751f\u6210\u8d28\u91cf\u4e0b\u964d\uff0c\u7279\u522b\u662f\u5728\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u7b49\u9700\u8981\u7cbe\u786e\u63a8\u7406\u7684\u4efb\u52a1\u4e2d\u3002", "method": "\u63d0\u51fa\u8bad\u7ec3\u65e0\u5173\u7684\u89e3\u7801\u7b56\u7565DCD\uff1a\u7ef4\u62a4\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u6ed1\u52a8\u7a97\u53e3\uff0c\u65e9\u671f\u89e3\u51b3\u4f4e\u4e0d\u786e\u5b9a\u6027token\uff0c\u5ef6\u8fdf\u9ad8\u4e0d\u786e\u5b9a\u6027token\u76f4\u5230\u83b7\u5f97\u8db3\u591f\u4e0a\u4e0b\u6587\u8bc1\u636e\uff0c\u5b9e\u73b0\u89e3\u7801\u7a97\u53e3\u5185\u7684\u6709\u6548\u53cc\u5411\u4fe1\u606f\u6d41\u800c\u4e0d\u727a\u7272\u6548\u7387\u3002", "result": "\u5728\u591a\u4e2a\u6269\u6563\u8bed\u8a00\u6a21\u578b\u3001\u57fa\u51c6\u6d4b\u8bd5\u548c\u7f13\u5b58\u914d\u7f6e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDCD\u76f8\u6bd4\u56fa\u5b9a\u5757\u6269\u6563\u65b9\u6cd5\u5e73\u5747\u63d0\u53471.39%\u7684\u751f\u6210\u51c6\u786e\u7387\uff0c\u65f6\u95f4\u5f00\u9500\u76f8\u5f53\uff0c\u6700\u5927\u6539\u8fdb\u8fbe\u52309.0%\u3002", "conclusion": "\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u5ef6\u8fdftoken\u51b3\u7b56\u662f\u63d0\u5347\u6269\u6563\u8bed\u8a00\u6a21\u578b\u89e3\u7801\u8d28\u91cf\u548c\u6548\u7387\u7684\u7b80\u5355\u800c\u6709\u6548\u7684\u539f\u5219\uff0cDCD\u65b9\u6cd5\u6709\u6548\u7f13\u89e3\u4e86\u5757\u6269\u6563\u6a21\u578b\u7684\u7ed3\u6784\u6027\u9650\u5236\u3002"}}
{"id": "2601.01268", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01268", "abs": "https://arxiv.org/abs/2601.01268", "authors": ["Maayan Gelboim", "Amir Adler", "Mauricio Araya-Polo"], "title": "Accelerated Full Waveform Inversion by Deep Compressed Learning", "comment": null, "summary": "We propose and test a method to reduce the dimensionality of Full Waveform Inversion (FWI) inputs as computational cost mitigation approach. Given modern seismic acquisition systems, the data (as input for FWI) required for an industrial-strength case is in the teraflop level of storage, therefore solving complex subsurface cases or exploring multiple scenarios with FWI become prohibitive. The proposed method utilizes a deep neural network with a binarized sensing layer that learns by compressed learning a succinct but consequential seismic acquisition layout from a large corpus of subsurface models. Thus, given a large seismic data set to invert, the trained network selects a smaller subset of the data, then by using representation learning, an autoencoder computes latent representations of the data, followed by K-means clustering of the latent representations to further select the most relevant data for FWI. Effectively, this approach can be seen as a hierarchical selection. The proposed approach consistently outperforms random data sampling, even when utilizing only 10% of the data for 2D FWI, these results pave the way to accelerating FWI in large scale 3D inversion.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5168\u6ce2\u5f62\u53cd\u6f14\u6570\u636e\u964d\u7ef4\u65b9\u6cd5\uff0c\u901a\u8fc7\u538b\u7f29\u5b66\u4e60\u548c\u5206\u5c42\u9009\u62e9\u51cf\u5c11\u8ba1\u7b97\u6210\u672c", "motivation": "\u5de5\u4e1a\u7ea7\u5168\u6ce2\u5f62\u53cd\u6f14\u9700\u8981\u592a\u5b57\u8282\u7ea7\u6570\u636e\u5b58\u50a8\uff0c\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u9650\u5236\u4e86\u590d\u6742\u5730\u4e0b\u60c5\u51b5\u5206\u6790\u548c\u591a\u573a\u666f\u63a2\u7d22", "method": "\u4f7f\u7528\u5e26\u4e8c\u503c\u5316\u611f\u77e5\u5c42\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u538b\u7f29\u5b66\u4e60\uff0c\u4ece\u5927\u91cf\u5730\u4e0b\u6a21\u578b\u4e2d\u5b66\u4e60\u7b80\u6d01\u4f46\u91cd\u8981\u7684\u5730\u9707\u91c7\u96c6\u5e03\u5c40\uff1b\u7136\u540e\u901a\u8fc7\u81ea\u7f16\u7801\u5668\u8ba1\u7b97\u6570\u636e\u7684\u6f5c\u5728\u8868\u793a\uff0c\u518d\u7528K-means\u805a\u7c7b\u8fdb\u4e00\u6b65\u9009\u62e9\u6700\u76f8\u5173\u7684\u6570\u636e", "result": "\u8be5\u65b9\u6cd5\u5728\u4ec5\u4f7f\u752810%\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u57282D\u5168\u6ce2\u5f62\u53cd\u6f14\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u968f\u673a\u6570\u636e\u91c7\u6837", "conclusion": "\u8fd9\u79cd\u5206\u5c42\u9009\u62e9\u65b9\u6cd5\u4e3a\u5927\u89c4\u6a213D\u53cd\u6f14\u4e2d\u52a0\u901f\u5168\u6ce2\u5f62\u53cd\u6f14\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84"}}
{"id": "2601.02123", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02123", "abs": "https://arxiv.org/abs/2601.02123", "authors": ["Po-Jen Ko", "Chen-Han Tsai", "Yu-Shao Peng"], "title": "DeCode: Decoupling Content and Delivery for Medical QA", "comment": "Preprint", "summary": "Large language models (LLMs) exhibit strong medical knowledge and can generate factually accurate responses. However, existing models often fail to account for individual patient contexts, producing answers that are clinically correct yet poorly aligned with patients' needs. In this work, we introduce DeCode, a training-free, model-agnostic framework that adapts existing LLMs to produce contextualized answers in clinical settings. We evaluate DeCode on OpenAI HealthBench, a comprehensive and challenging benchmark designed to assess clinical relevance and validity of LLM responses. DeCode improves the previous state of the art from $28.4\\%$ to $49.8\\%$, corresponding to a $75\\%$ relative improvement. Experimental results suggest the effectiveness of DeCode in improving clinical question answering of LLMs.", "AI": {"tldr": "DeCode\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u3001\u6a21\u578b\u65e0\u5173\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4f7f\u73b0\u6709LLM\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u751f\u6210\u66f4\u5177\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u7684\u7b54\u6848\uff0c\u5728OpenAI HealthBench\u4e0a\u5c06SOTA\u4ece28.4%\u63d0\u5347\u523049.8%\u3002", "motivation": "\u73b0\u6709LLM\u867d\u7136\u5177\u5907\u533b\u5b66\u77e5\u8bc6\u5e76\u80fd\u751f\u6210\u4e8b\u5b9e\u51c6\u786e\u7684\u56de\u7b54\uff0c\u4f46\u5f80\u5f80\u5ffd\u7565\u60a3\u8005\u4e2a\u4f53\u5316\u60c5\u5883\uff0c\u4ea7\u751f\u4e34\u5e8a\u6b63\u786e\u4f46\u4e0e\u60a3\u8005\u9700\u6c42\u4e0d\u5339\u914d\u7684\u56de\u7b54\u3002", "method": "DeCode\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u3001\u6a21\u578b\u65e0\u5173\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u9002\u914d\u73b0\u6709LLM\u6765\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u751f\u6210\u4e0a\u4e0b\u6587\u5316\u7684\u7b54\u6848\u3002", "result": "\u5728OpenAI HealthBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDeCode\u5c06\u5148\u524d\u6700\u4f73\u7ed3\u679c\u4ece28.4%\u63d0\u5347\u523049.8%\uff0c\u76f8\u5bf9\u6539\u8fdb\u8fbe\u523075%\u3002", "conclusion": "DeCode\u80fd\u6709\u6548\u63d0\u5347LLM\u5728\u4e34\u5e8a\u95ee\u7b54\u4e2d\u7684\u8868\u73b0\uff0c\u4f7f\u5176\u751f\u6210\u66f4\u7b26\u5408\u60a3\u8005\u5177\u4f53\u60c5\u5883\u7684\u56de\u7b54\u3002"}}
{"id": "2601.01290", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01290", "abs": "https://arxiv.org/abs/2601.01290", "authors": ["Harshita Narnoli", "Mihai Surdeanu"], "title": "The Alchemy of Thought: Understanding In-Context Learning Through Supervised Classification", "comment": "International Joint Conference on Natural Language Processing & Asia-Pacific Chapter of the Association for Computational Linguistics, 2025", "summary": "In-context learning (ICL) has become a prominent paradigm to rapidly customize LLMs to new tasks without fine-tuning. However, despite the empirical evidence of its usefulness, we still do not truly understand how ICL works. In this paper, we compare the behavior of in-context learning with supervised classifiers trained on ICL demonstrations to investigate three research questions: (1) Do LLMs with ICL behave similarly to classifiers trained on the same examples? (2) If so, which classifiers are closer, those based on gradient descent (GD) or those based on k-nearest neighbors (kNN)? (3) When they do not behave similarly, what conditions are associated with differences in behavior? Using text classification as a use case, with six datasets and three LLMs, we observe that LLMs behave similarly to these classifiers when the relevance of demonstrations is high. On average, ICL is closer to kNN than logistic regression, giving empirical evidence that the attention mechanism behaves more similarly to kNN than GD. However, when demonstration relevance is low, LLMs perform better than these classifiers, likely because LLMs can back off to their parametric memory, a luxury these classifiers do not have.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u6bd4\u8f83ICL\u4e0e\u57fa\u4e8eICL\u6f14\u793a\u8bad\u7ec3\u7684\u76d1\u7763\u5206\u7c7b\u5668\u7684\u884c\u4e3a\uff0c\u7814\u7a76\u4e86ICL\u7684\u5de5\u4f5c\u539f\u7406\uff0c\u53d1\u73b0\u5f53\u6f14\u793a\u76f8\u5173\u6027\u9ad8\u65f6\uff0cLLMs\u884c\u4e3a\u7c7b\u4f3c\u4e8ekNN\u5206\u7c7b\u5668\uff1b\u5f53\u76f8\u5173\u6027\u4f4e\u65f6\uff0cLLMs\u8868\u73b0\u66f4\u597d\uff0c\u56e0\u4e3a\u5b83\u4eec\u53ef\u4ee5\u5229\u7528\u53c2\u6570\u8bb0\u5fc6\u3002", "motivation": "\u5c3d\u7ba1ICL\u5df2\u6210\u4e3a\u5feb\u901f\u5b9a\u5236LLMs\u7684\u4e3b\u8981\u8303\u5f0f\uff0c\u4f46\u5176\u5de5\u4f5c\u539f\u7406\u5c1a\u4e0d\u6e05\u695a\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6bd4\u8f83ICL\u4e0e\u76d1\u7763\u5206\u7c7b\u5668\u7684\u884c\u4e3a\u6765\u6df1\u5165\u7406\u89e3ICL\u7684\u5de5\u4f5c\u673a\u5236\u3002", "method": "\u4f7f\u7528\u6587\u672c\u5206\u7c7b\u4f5c\u4e3a\u7528\u4f8b\uff0c\u57286\u4e2a\u6570\u636e\u96c6\u548c3\u4e2aLLMs\u4e0a\uff0c\u5c06ICL\u7684\u884c\u4e3a\u4e0e\u57fa\u4e8e\u76f8\u540c\u6f14\u793a\u8bad\u7ec3\u7684\u76d1\u7763\u5206\u7c7b\u5668\uff08\u68af\u5ea6\u4e0b\u964d\u548ck\u8fd1\u90bb\uff09\u8fdb\u884c\u6bd4\u8f83\uff0c\u7814\u7a76\u4e09\u4e2a\u6838\u5fc3\u95ee\u9898\u3002", "result": "\u5f53\u6f14\u793a\u76f8\u5173\u6027\u9ad8\u65f6\uff0cLLMs\u884c\u4e3a\u4e0e\u8fd9\u4e9b\u5206\u7c7b\u5668\u76f8\u4f3c\uff0c\u4e14ICL\u66f4\u63a5\u8fd1kNN\u800c\u975e\u903b\u8f91\u56de\u5f52\uff1b\u5f53\u6f14\u793a\u76f8\u5173\u6027\u4f4e\u65f6\uff0cLLMs\u8868\u73b0\u4f18\u4e8e\u8fd9\u4e9b\u5206\u7c7b\u5668\uff0c\u56e0\u4e3aLLMs\u53ef\u4ee5\u56de\u9000\u5230\u53c2\u6570\u8bb0\u5fc6\u3002", "conclusion": "ICL\u5728\u6f14\u793a\u76f8\u5173\u6027\u9ad8\u65f6\u7c7b\u4f3c\u4e8ekNN\u5206\u7c7b\u5668\uff0c\u800c\u5728\u76f8\u5173\u6027\u4f4e\u65f6LLMs\u53ef\u4ee5\u5229\u7528\u53c2\u6570\u8bb0\u5fc6\u83b7\u5f97\u66f4\u597d\u8868\u73b0\uff0c\u8fd9\u4e3a\u7406\u89e3ICL\u5de5\u4f5c\u673a\u5236\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u8bc1\u636e\u3002"}}
{"id": "2601.02128", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.02128", "abs": "https://arxiv.org/abs/2601.02128", "authors": ["Steffen Freisinger", "Philipp Seeberger", "Thomas Ranzenberger", "Tobias Bocklet", "Korbinian Riedhammer"], "title": "Towards Multi-Level Transcript Segmentation: LoRA Fine-Tuning for Table-of-Contents Generation", "comment": "Published in Proceedings of Interspeech 2025. Please cite the proceedings version (DOI: 10.21437/Interspeech.2025-2792)", "summary": "Segmenting speech transcripts into thematic sections benefits both downstream processing and users who depend on written text for accessibility. We introduce a novel approach to hierarchical topic segmentation in transcripts, generating multi-level tables of contents that capture both topic and subtopic boundaries. We compare zero-shot prompting and LoRA fine-tuning on large language models, while also exploring the integration of high-level speech pause features. Evaluations on English meeting recordings and multilingual lecture transcripts (Portuguese, German) show significant improvements over established topic segmentation baselines. Additionally, we adapt a common evaluation measure for multi-level segmentation, taking into account all hierarchical levels within one metric.", "AI": {"tldr": "\u63d0\u51fa\u5206\u5c42\u4e3b\u9898\u5206\u5272\u65b9\u6cd5\uff0c\u7ed3\u5408\u96f6\u6837\u672c\u63d0\u793a\u548cLoRA\u5fae\u8c03\uff0c\u6574\u5408\u8bed\u97f3\u505c\u987f\u7279\u5f81\uff0c\u5728\u4f1a\u8bae\u548c\u8bb2\u5ea7\u8f6c\u5f55\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf", "motivation": "\u5c06\u8bed\u97f3\u8f6c\u5f55\u5206\u5272\u4e3a\u4e3b\u9898\u6bb5\u843d\u6709\u52a9\u4e8e\u4e0b\u6e38\u5904\u7406\u548c\u4f9d\u8d56\u6587\u672c\u7684\u53ef\u8bbf\u95ee\u6027\u7528\u6237\uff0c\u9700\u8981\u6355\u6349\u4e3b\u9898\u548c\u5b50\u4e3b\u9898\u7684\u591a\u5c42\u6b21\u7ed3\u6784", "method": "\u63d0\u51fa\u5206\u5c42\u4e3b\u9898\u5206\u5272\u65b9\u6cd5\uff0c\u6bd4\u8f83\u96f6\u6837\u672c\u63d0\u793a\u548cLoRA\u5fae\u8c03\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u6574\u5408\u9ad8\u5c42\u8bed\u97f3\u505c\u987f\u7279\u5f81\uff0c\u9002\u5e94\u591a\u7ea7\u5206\u5272\u8bc4\u4f30\u6307\u6807", "result": "\u5728\u82f1\u8bed\u4f1a\u8bae\u5f55\u97f3\u548c\u591a\u8bed\u8a00\u8bb2\u5ea7\u8f6c\u5f55\uff08\u8461\u8404\u7259\u8bed\u3001\u5fb7\u8bed\uff09\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u4e3b\u9898\u5206\u5272\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u5206\u5c42\u4e3b\u9898\u5206\u5272\u65b9\u6cd5\u6709\u6548\uff0c\u7ed3\u5408\u8bed\u8a00\u6a21\u578b\u548c\u8bed\u97f3\u7279\u5f81\u80fd\u63d0\u5347\u5206\u5272\u6027\u80fd\uff0c\u591a\u7ea7\u8bc4\u4f30\u6307\u6807\u80fd\u5168\u9762\u8861\u91cf\u5c42\u6b21\u7ed3\u6784"}}
{"id": "2601.01295", "categories": ["cs.LG", "math.NA"], "pdf": "https://arxiv.org/pdf/2601.01295", "abs": "https://arxiv.org/abs/2601.01295", "authors": ["Changhoon Song", "Seungchan Ko", "Youngjoon Hong"], "title": "Sobolev Approximation of Deep ReLU Network in Log-weighted Barron Space", "comment": null, "summary": "Universal approximation theorems show that neural networks can approximate any continuous function; however, the number of parameters may grow exponentially with the ambient dimension, so these results do not fully explain the practical success of deep models on high-dimensional data. Barron space theory addresses this: if a target function belongs to a Barron space, a two-layer network with $n$ parameters achieves an $O(n^{-1/2})$ approximation error in $L^2$. Yet classical Barron spaces $\\mathscr{B}^{s+1}$ still require stronger regularity than Sobolev spaces $H^s$, and existing depth-sensitive results often assume constraints such as $sL \\le 1/2$. In this paper, we introduce a log-weighted Barron space $\\mathscr{B}^{\\log}$, which requires a strictly weaker assumption than $\\mathscr{B}^s$ for any $s>0$. For this new function space, we first study embedding properties and carry out a statistical analysis via the Rademacher complexity. Then we prove that functions in $\\mathscr{B}^{\\log}$ can be approximated by deep ReLU networks with explicit depth dependence. We then define a family $\\mathscr{B}^{s,\\log}$, establish approximation bounds in the $H^1$ norm, and identify maximal depth scales under which these rates are preserved. Our results clarify how depth reduces regularity requirements for efficient representation, offering a more precise explanation for the performance of deep architectures beyond the classical Barron setting, and for their stable use in high-dimensional problems used today.", "AI": {"tldr": "\u672c\u6587\u5f15\u5165\u5bf9\u6570\u52a0\u6743Barron\u7a7a\u95f4\uff0c\u6bd4\u7ecf\u5178Barron\u7a7a\u95f4\u8981\u6c42\u66f4\u5f31\u7684\u6b63\u5219\u6027\u6761\u4ef6\uff0c\u8bc1\u660e\u4e86\u6df1\u5ea6ReLU\u7f51\u7edc\u5728\u8be5\u7a7a\u95f4\u4e2d\u7684\u903c\u8fd1\u80fd\u529b\uff0c\u9610\u660e\u4e86\u6df1\u5ea6\u5982\u4f55\u964d\u4f4e\u9ad8\u6548\u8868\u793a\u6240\u9700\u7684\u6b63\u5219\u6027\u8981\u6c42\u3002", "motivation": "\u7ecf\u5178Barron\u7a7a\u95f4\u7406\u8bba\u867d\u7136\u89e3\u91ca\u4e86\u795e\u7ecf\u7f51\u7edc\u5bf9\u67d0\u4e9b\u51fd\u6570\u7684\u903c\u8fd1\u80fd\u529b\uff0c\u4f46\u4ecd\u9700\u6bd4Sobolev\u7a7a\u95f4\u66f4\u5f3a\u7684\u6b63\u5219\u6027\u6761\u4ef6\uff0c\u4e14\u73b0\u6709\u6df1\u5ea6\u654f\u611f\u7ed3\u679c\u5e38\u5047\u8bbe\u9650\u5236\u6027\u6761\u4ef6\uff08\u5982sL \u2264 1/2\uff09\u3002\u9700\u8981\u66f4\u5f31\u7684\u51fd\u6570\u7a7a\u95f4\u6765\u66f4\u597d\u5730\u89e3\u91ca\u6df1\u5ea6\u6a21\u578b\u5728\u9ad8\u7ef4\u6570\u636e\u4e0a\u7684\u5b9e\u9645\u6210\u529f\u3002", "method": "1. \u5f15\u5165\u5bf9\u6570\u52a0\u6743Barron\u7a7a\u95f4 \u212c^log\uff0c\u5176\u6b63\u5219\u6027\u8981\u6c42\u6bd4\u4efb\u4f55\u212c^s\uff08s>0\uff09\u90fd\u5f31\uff1b2. \u7814\u7a76\u8be5\u7a7a\u95f4\u7684\u5d4c\u5165\u6027\u8d28\u5e76\u901a\u8fc7Rademacher\u590d\u6742\u5ea6\u8fdb\u884c\u7edf\u8ba1\u5206\u6790\uff1b3. \u8bc1\u660e\u212c^log\u4e2d\u7684\u51fd\u6570\u53ef\u7531\u6df1\u5ea6ReLU\u7f51\u7edc\u4ee5\u663e\u5f0f\u6df1\u5ea6\u4f9d\u8d56\u903c\u8fd1\uff1b4. \u5b9a\u4e49\u212c^{s,log}\u65cf\uff0c\u5efa\u7acbH^1\u8303\u6570\u4e0b\u7684\u903c\u8fd1\u754c\uff0c\u5e76\u786e\u5b9a\u4fdd\u6301\u8fd9\u4e9b\u901f\u7387\u7684\u6700\u5927\u6df1\u5ea6\u5c3a\u5ea6\u3002", "result": "1. \u5efa\u7acb\u4e86\u5bf9\u6570\u52a0\u6743Barron\u7a7a\u95f4\u7684\u5d4c\u5165\u6027\u8d28\u548c\u7edf\u8ba1\u7279\u6027\uff1b2. \u8bc1\u660e\u4e86\u6df1\u5ea6ReLU\u7f51\u7edc\u5728\u212c^log\u7a7a\u95f4\u4e2d\u7684\u903c\u8fd1\u80fd\u529b\uff1b3. \u83b7\u5f97\u4e86\u212c^{s,log}\u65cf\u5728H^1\u8303\u6570\u4e0b\u7684\u903c\u8fd1\u754c\uff1b4. \u786e\u5b9a\u4e86\u4fdd\u6301\u903c\u8fd1\u901f\u7387\u7684\u6700\u5927\u6df1\u5ea6\u5c3a\u5ea6\u3002", "conclusion": "\u5bf9\u6570\u52a0\u6743Barron\u7a7a\u95f4\u4e3a\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u63d0\u4f9b\u4e86\u66f4\u5f31\u7684\u6b63\u5219\u6027\u8981\u6c42\uff0c\u9610\u660e\u4e86\u6df1\u5ea6\u5982\u4f55\u964d\u4f4e\u9ad8\u6548\u8868\u793a\u6240\u9700\u7684\u51fd\u6570\u5149\u6ed1\u6027\u6761\u4ef6\uff0c\u4e3a\u6df1\u5ea6\u67b6\u6784\u8d85\u8d8a\u7ecf\u5178Barron\u8bbe\u7f6e\u7684\u5b9e\u9645\u6027\u80fd\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u7684\u89e3\u91ca\uff0c\u5e76\u4e3a\u5176\u5728\u9ad8\u7ef4\u95ee\u9898\u4e2d\u7684\u7a33\u5b9a\u4f7f\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2601.02144", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02144", "abs": "https://arxiv.org/abs/2601.02144", "authors": ["Boxuan Lyu", "Soichiro Murakami", "Hidetaka Kamigaito", "Peinan Zhang"], "title": "Routing by Analogy: kNN-Augmented Expert Assignment for Mixture-of-Experts", "comment": null, "summary": "Mixture-of-Experts (MoE) architectures scale large language models efficiently by employing a parametric \"router\" to dispatch tokens to a sparse subset of experts. Typically, this router is trained once and then frozen, rendering routing decisions brittle under distribution shifts. We address this limitation by introducing kNN-MoE, a retrieval-augmented routing framework that reuses optimal expert assignments from a memory of similar past cases. This memory is constructed offline by directly optimizing token-wise routing logits to maximize the likelihood on a reference set. Crucially, we use the aggregate similarity of retrieved neighbors as a confidence-driven mixing coefficient, thus allowing the method to fall back to the frozen router when no relevant cases are found. Experiments show kNN-MoE outperforms zero-shot baselines and rivals computationally expensive supervised fine-tuning.", "AI": {"tldr": "kNN-MoE\uff1a\u57fa\u4e8e\u68c0\u7d22\u7684\u6df7\u5408\u4e13\u5bb6\u8def\u7531\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u7528\u5386\u53f2\u6700\u4f18\u4e13\u5bb6\u5206\u914d\u6765\u589e\u5f3a\u4f20\u7edfMoE\u8def\u7531\u5668\u7684\u9c81\u68d2\u6027\uff0c\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edfMoE\u67b6\u6784\u4e2d\u7684\u53c2\u6570\u5316\u8def\u7531\u5668\u901a\u5e38\u5728\u8bad\u7ec3\u540e\u51bb\u7ed3\uff0c\u5bfc\u81f4\u5728\u5206\u5e03\u504f\u79fb\u65f6\u8def\u7531\u51b3\u7b56\u53d8\u5f97\u8106\u5f31\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u9002\u5e94\u65b0\u6570\u636e\u5206\u5e03\u7684\u8def\u7531\u673a\u5236\u3002", "method": "\u63d0\u51fakNN-MoE\u6846\u67b6\uff1a1\uff09\u79bb\u7ebf\u6784\u5efa\u8bb0\u5fc6\u5e93\uff0c\u901a\u8fc7\u4f18\u5316\u8def\u7531logits\u6700\u5927\u5316\u53c2\u8003\u96c6\u4f3c\u7136\uff1b2\uff09\u5728\u7ebf\u68c0\u7d22\u76f8\u4f3c\u5386\u53f2\u6848\u4f8b\u91cd\u7528\u6700\u4f18\u4e13\u5bb6\u5206\u914d\uff1b3\uff09\u4f7f\u7528\u68c0\u7d22\u90bb\u5c45\u7684\u805a\u5408\u76f8\u4f3c\u5ea6\u4f5c\u4e3a\u7f6e\u4fe1\u5ea6\u9a71\u52a8\u7684\u6df7\u5408\u7cfb\u6570\uff0c\u5728\u65e0\u76f8\u5173\u6848\u4f8b\u65f6\u56de\u9000\u5230\u51bb\u7ed3\u8def\u7531\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660ekNN-MoE\u4f18\u4e8e\u96f6\u6837\u672c\u57fa\u7ebf\uff0c\u4e14\u80fd\u4e0e\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u7684\u76d1\u7763\u5fae\u8c03\u76f8\u5ab2\u7f8e\u3002", "conclusion": "kNN-MoE\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u7684\u8def\u7531\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edfMoE\u8def\u7531\u5668\u7684\u5206\u5e03\u504f\u79fb\u8106\u5f31\u6027\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u4e13\u5bb6\u5206\u914d\u65b9\u6848\u3002"}}
{"id": "2601.01297", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01297", "abs": "https://arxiv.org/abs/2601.01297", "authors": ["Anantha Sharma"], "title": "ARGUS: Adaptive Rotation-Invariant Geometric Unsupervised System", "comment": "26 pages", "summary": "Detecting distributional drift in high-dimensional data streams presents fundamental challenges: global comparison methods scale poorly, projection-based approaches lose geometric structure, and re-clustering methods suffer from identity instability. This paper introduces Argus, A framework that reconceptualizes drift detection as tracking local statistics over a fixed spatial partition of the data manifold.\n  The key contributions are fourfold. First, it is proved that Voronoi tessellations over canonical orthonormal frames yield drift metrics that are invariant to orthogonal transformations. The rotations and reflections that preserve Euclidean geometry. Second, it is established that this framework achieves O(N) complexity per snapshot while providing cell-level spatial localization of distributional change. Third, a graph-theoretic characterization of drift propagation is developed that distinguishes coherent distributional shifts from isolated perturbations. Fourth, product quantization tessellation is introduced for scaling to very high dimensions (d>500) by decomposing the space into independent subspaces and aggregating drift signals across subspaces.\n  This paper formalizes the theoretical foundations, proves invariance properties, and presents experimental validation demonstrating that the framework correctly identifies drift under coordinate rotation while existing methods produce false positives. The tessellated approach offers a principled geometric foundation for distribution monitoring that preserves high-dimensional structure without the computational burden of pairwise comparisons.", "AI": {"tldr": "Argus\u662f\u4e00\u4e2a\u7528\u4e8e\u9ad8\u7ef4\u6570\u636e\u6d41\u5206\u5e03\u6f02\u79fb\u68c0\u6d4b\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u56fa\u5b9a\u7a7a\u95f4\u5206\u533a\u8ddf\u8e2a\u5c40\u90e8\u7edf\u8ba1\u91cf\uff0c\u5177\u6709\u6b63\u4ea4\u53d8\u6362\u4e0d\u53d8\u6027\u3001O(N)\u590d\u6742\u5ea6\u3001\u6f02\u79fb\u4f20\u64ad\u56fe\u8bba\u5206\u6790\u548c\u4ea7\u54c1\u91cf\u5316\u5206\u5757\u7b49\u7279\u6027\u3002", "motivation": "\u9ad8\u7ef4\u6570\u636e\u6d41\u4e2d\u7684\u5206\u5e03\u6f02\u79fb\u68c0\u6d4b\u9762\u4e34\u4e09\u5927\u6311\u6218\uff1a\u5168\u5c40\u6bd4\u8f83\u65b9\u6cd5\u6269\u5c55\u6027\u5dee\u3001\u57fa\u4e8e\u6295\u5f71\u7684\u65b9\u6cd5\u4e22\u5931\u51e0\u4f55\u7ed3\u6784\u3001\u91cd\u65b0\u805a\u7c7b\u65b9\u6cd5\u5b58\u5728\u8eab\u4efd\u4e0d\u7a33\u5b9a\u6027\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u9ad8\u7ef4\u7ed3\u6784\u53c8\u8ba1\u7b97\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u5c06\u6f02\u79fb\u68c0\u6d4b\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5728\u6570\u636e\u6d41\u5f62\u56fa\u5b9a\u7a7a\u95f4\u5206\u533a\u4e0a\u8ddf\u8e2a\u5c40\u90e8\u7edf\u8ba1\u91cf\u3002\u4f7f\u7528\u89c4\u8303\u6b63\u4ea4\u6846\u67b6\u4e0a\u7684Voronoi\u7ec6\u5206\u83b7\u5f97\u6b63\u4ea4\u53d8\u6362\u4e0d\u53d8\u7684\u6f02\u79fb\u5ea6\u91cf\uff0c\u5f00\u53d1\u6f02\u79fb\u4f20\u64ad\u7684\u56fe\u8bba\u8868\u5f81\uff0c\u5f15\u5165\u4ea7\u54c1\u91cf\u5316\u7ec6\u5206\u6269\u5c55\u5230\u8d85\u9ad8\u7ef4\u5ea6\u3002", "result": "\u8bc1\u660e\u4e86Voronoi\u7ec6\u5206\u5728\u6b63\u4ea4\u53d8\u6362\u4e0b\u7684\u4e0d\u53d8\u6027\uff0c\u5b9e\u73b0\u4e86O(N)\u590d\u6742\u5ea6\u5e76\u63d0\u4f9b\u7ec6\u80de\u7ea7\u7a7a\u95f4\u5b9a\u4f4d\uff0c\u80fd\u591f\u533a\u5206\u8fde\u8d2f\u5206\u5e03\u6f02\u79fb\u4e0e\u5b64\u7acb\u6270\u52a8\uff0c\u5728d>500\u7684\u9ad8\u7ef4\u6570\u636e\u4e2d\u6709\u6548\u5de5\u4f5c\uff0c\u5728\u5750\u6807\u65cb\u8f6c\u4e0b\u6b63\u786e\u8bc6\u522b\u6f02\u79fb\u800c\u73b0\u6709\u65b9\u6cd5\u4ea7\u751f\u8bef\u62a5\u3002", "conclusion": "Argus\u6846\u67b6\u4e3a\u5206\u5e03\u76d1\u63a7\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u7684\u51e0\u4f55\u57fa\u7840\uff0c\u65e2\u4fdd\u6301\u4e86\u9ad8\u7ef4\u7ed3\u6784\u53c8\u907f\u514d\u4e86\u6210\u5bf9\u6bd4\u8f83\u7684\u8ba1\u7b97\u8d1f\u62c5\uff0c\u901a\u8fc7\u56fa\u5b9a\u7a7a\u95f4\u5206\u533a\u65b9\u6cd5\u89e3\u51b3\u4e86\u9ad8\u7ef4\u6570\u636e\u6d41\u6f02\u79fb\u68c0\u6d4b\u7684\u6839\u672c\u6311\u6218\u3002"}}
{"id": "2601.02158", "categories": ["cs.CL", "cs.AI", "cs.LG", "physics.geo-ph"], "pdf": "https://arxiv.org/pdf/2601.02158", "abs": "https://arxiv.org/abs/2601.02158", "authors": ["Almaz Ermilov"], "title": "FormationEval, an open multiple-choice benchmark for petroleum geoscience", "comment": "24 pages, 8 figures, 10 tables; benchmark and code at https://github.com/AlmazErmilov/FormationEval-an-Open-Benchmark-for-Oil-Gas-Geoscience-MCQ-Evaluation", "summary": "This paper presents FormationEval, an open multiple-choice question benchmark for evaluating language models on petroleum geoscience and subsurface disciplines. The dataset contains 505 questions across seven domains including petrophysics, petroleum geology and reservoir engineering, derived from three authoritative sources using a reasoning model with detailed instructions and a concept-based approach that avoids verbatim copying of copyrighted text. Each question includes source metadata to support traceability and audit. The evaluation covers 72 models from major providers including OpenAI, Anthropic, Google, Meta and open-weight alternatives. The top performers achieve over 97\\% accuracy, with Gemini 3 Pro Preview reaching 99.8\\%, while tier and domain gaps persist. Among open-weight models, GLM-4.7 leads at 98.6\\%, with several DeepSeek, Llama, Qwen and Mistral models also exceeding 93\\%. The performance gap between open-weight and closed models is narrower than expected, with several lower-cost open-weight models exceeding 90\\% accuracy. Petrophysics emerges as the most challenging domain across all models, while smaller models show wider performance variance. Residual length bias in the dataset (correct answers tend to be longer) is documented along with bias mitigation strategies applied during construction. The benchmark, evaluation code and results are publicly available.", "AI": {"tldr": "FormationEval\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u5728\u77f3\u6cb9\u5730\u8d28\u79d1\u5b66\u548c\u5730\u4e0b\u5b66\u79d1\u80fd\u529b\u7684\u5f00\u653e\u591a\u9009\u95ee\u7b54\u57fa\u51c6\uff0c\u5305\u542b505\u4e2a\u95ee\u9898\uff0c\u8986\u76d67\u4e2a\u9886\u57df\uff0c\u8bc4\u4f30\u4e8672\u4e2a\u6a21\u578b\uff0c\u53d1\u73b0\u9876\u7ea7\u6a21\u578b\u51c6\u786e\u7387\u8d85\u8fc797%\uff0c\u5f00\u6e90\u6a21\u578b\u8868\u73b0\u63a5\u8fd1\u95ed\u6e90\u6a21\u578b\u3002", "motivation": "\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u77f3\u6cb9\u5730\u8d28\u79d1\u5b66\u548c\u5730\u4e0b\u5b66\u79d1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ee5\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u5728\u8fd9\u4e9b\u4e13\u4e1a\u9886\u57df\u7684\u77e5\u8bc6\u638c\u63e1\u7a0b\u5ea6\uff0c\u540c\u65f6\u907f\u514d\u7248\u6743\u95ee\u9898\u5e76\u63d0\u4f9b\u53ef\u8ffd\u6eaf\u6027\u3002", "method": "\u4ece\u4e09\u4e2a\u6743\u5a01\u6765\u6e90\u4f7f\u7528\u63a8\u7406\u6a21\u578b\u548c\u57fa\u4e8e\u6982\u5ff5\u7684\u65b9\u6cd5\u6784\u5efa\u4e86505\u4e2a\u591a\u9009\u95ee\u9898\uff0c\u8986\u76d67\u4e2a\u9886\u57df\uff08\u5305\u62ec\u5ca9\u77f3\u7269\u7406\u5b66\u3001\u77f3\u6cb9\u5730\u8d28\u5b66\u548c\u6cb9\u85cf\u5de5\u7a0b\uff09\uff0c\u6bcf\u4e2a\u95ee\u9898\u90fd\u5305\u542b\u6765\u6e90\u5143\u6570\u636e\u3002\u8bc4\u4f30\u4e8672\u4e2a\u6765\u81ea\u4e3b\u8981\u63d0\u4f9b\u5546\u7684\u5f00\u6e90\u548c\u95ed\u6e90\u6a21\u578b\u3002", "result": "Gemini 3 Pro Preview\u8fbe\u523099.8%\u7684\u6700\u9ad8\u51c6\u786e\u7387\uff0c\u5f00\u6e90\u6a21\u578bGLM-4.7\u8fbe\u523098.6%\u3002\u5f00\u6e90\u548c\u95ed\u6e90\u6a21\u578b\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u6bd4\u9884\u671f\u5c0f\uff0c\u591a\u4e2a\u4f4e\u6210\u672c\u5f00\u6e90\u6a21\u578b\u8d85\u8fc790%\u51c6\u786e\u7387\u3002\u5ca9\u77f3\u7269\u7406\u5b66\u662f\u6240\u6709\u6a21\u578b\u4e2d\u6700\u5177\u6311\u6218\u6027\u7684\u9886\u57df\uff0c\u8f83\u5c0f\u6a21\u578b\u8868\u73b0\u51fa\u66f4\u5927\u7684\u6027\u80fd\u5dee\u5f02\u3002", "conclusion": "FormationEval\u57fa\u51c6\u663e\u793a\u8bed\u8a00\u6a21\u578b\u5728\u77f3\u6cb9\u5730\u8d28\u79d1\u5b66\u9886\u57df\u5df2\u8fbe\u5230\u9ad8\u6c34\u5e73\u8868\u73b0\uff0c\u5f00\u6e90\u6a21\u578b\u4e0e\u95ed\u6e90\u6a21\u578b\u5dee\u8ddd\u4e0d\u5927\uff0c\u5ca9\u77f3\u7269\u7406\u5b66\u662f\u6700\u5177\u6311\u6218\u6027\u7684\u5b50\u9886\u57df\uff0c\u57fa\u51c6\u63d0\u4f9b\u4e86\u53ef\u8ffd\u6eaf\u7684\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2601.01298", "categories": ["cs.LG", "cs.AI", "cs.AR", "cs.DC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.01298", "abs": "https://arxiv.org/abs/2601.01298", "authors": ["Jorge L. Ruiz Williams"], "title": "Warp-Cortex: An Asynchronous, Memory-Efficient Architecture for Million-Agent Cognitive Scaling on Consumer Hardware", "comment": null, "summary": "Current multi-agent Large Language Model (LLM) frameworks suffer from linear memory scaling, rendering \"System 2\" parallel reasoning impractical on consumer hardware. We present Warp Cortex, an asynchronous architecture that theoretically enables million-agent cognitive scaling by decoupling agent logic from physical memory. Through Singleton Weight Sharing and a novel Topological Synapse--inspired by hybrid landmarking techniques from Topological Data Analysis (TDA)--we reduce memory complexity from O(N * L) to O(1) for weights and O(N * k) for context, where k << L. By treating the KV-cache as a point cloud in latent space, we apply witness-complex-inspired sparsification to preserve persistent homological features of the context manifold. On a single NVIDIA RTX 4090, we empirically demonstrate 100 concurrent agents at 2.2 GB total VRAM, with theoretical capacity exceeding 1,000 agents before compute latency becomes the bottleneck. We further introduce Referential Injection, a non-intrusive KV-cache update mechanism that allows asynchronous sub-agents to influence primary generation without stream disruption.", "AI": {"tldr": "Warp Cortex\u662f\u4e00\u4e2a\u5f02\u6b65\u591a\u667a\u80fd\u4f53LLM\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u667a\u80fd\u4f53\u903b\u8f91\u4e0e\u7269\u7406\u5185\u5b58\uff0c\u5c06\u5185\u5b58\u590d\u6742\u5ea6\u4eceO(N*L)\u964d\u4f4e\u5230O(1)\u6743\u91cd\u548cO(N*k)\u4e0a\u4e0b\u6587\uff0c\u5728\u5355\u5f20RTX 4090\u4e0a\u5b9e\u73b0100\u4e2a\u5e76\u53d1\u667a\u80fd\u4f53\u4ec5\u97002.2GB\u663e\u5b58\u3002", "motivation": "\u5f53\u524d\u591a\u667a\u80fd\u4f53LLM\u6846\u67b6\u5b58\u5728\u7ebf\u6027\u5185\u5b58\u6269\u5c55\u95ee\u9898\uff0c\u4f7f\u5f97\"\u7cfb\u7edf2\"\u5e76\u884c\u63a8\u7406\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u4e0d\u5207\u5b9e\u9645\u3002\u9700\u8981\u89e3\u51b3\u5185\u5b58\u74f6\u9888\u4ee5\u5b9e\u73b0\u5927\u89c4\u6a21\u667a\u80fd\u4f53\u8ba4\u77e5\u6269\u5c55\u3002", "method": "\u91c7\u7528\u5f02\u6b65\u67b6\u6784\uff0c\u901a\u8fc7Singleton\u6743\u91cd\u5171\u4eab\u548c\u62d3\u6251\u7a81\u89e6\uff08\u53d7TDA\u6df7\u5408\u5730\u6807\u6280\u672f\u542f\u53d1\uff09\u51cf\u5c11\u5185\u5b58\u5360\u7528\u3002\u5c06KV\u7f13\u5b58\u89c6\u4e3a\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u70b9\u4e91\uff0c\u5e94\u7528\u89c1\u8bc1\u590d\u5f62\u7a00\u758f\u5316\u4fdd\u7559\u4e0a\u4e0b\u6587\u6d41\u5f62\u7684\u6301\u4e45\u540c\u8c03\u7279\u5f81\u3002\u5f15\u5165\u975e\u4fb5\u5165\u5f0fKV\u7f13\u5b58\u66f4\u65b0\u673a\u5236Referential Injection\u3002", "result": "\u5728\u5355\u5f20NVIDIA RTX 4090\u4e0a\uff0c\u5b9e\u73b0100\u4e2a\u5e76\u53d1\u667a\u80fd\u4f53\u4ec5\u97002.2GB\u603b\u663e\u5b58\uff0c\u7406\u8bba\u5bb9\u91cf\u8d85\u8fc71000\u4e2a\u667a\u80fd\u4f53\uff08\u8ba1\u7b97\u5ef6\u8fdf\u6210\u4e3a\u74f6\u9888\u524d\uff09\u3002\u5185\u5b58\u590d\u6742\u5ea6\u4eceO(N*L)\u964d\u4f4e\u5230O(1)\u6743\u91cd\u548cO(N*k)\u4e0a\u4e0b\u6587\u3002", "conclusion": "Warp Cortex\u901a\u8fc7\u521b\u65b0\u7684\u5185\u5b58\u4f18\u5316\u548c\u5f02\u6b65\u67b6\u6784\uff0c\u7406\u8bba\u4e0a\u53ef\u5b9e\u73b0\u767e\u4e07\u667a\u80fd\u4f53\u8ba4\u77e5\u6269\u5c55\uff0c\u4e3a\u5927\u89c4\u6a21\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2601.02179", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.02179", "abs": "https://arxiv.org/abs/2601.02179", "authors": ["Caiqi Zhang", "Ruihan Yang", "Xiaochen Zhu", "Chengzu Li", "Tiancheng Hu", "Yijiang River Dong", "Deqing Yang", "Nigel Collier"], "title": "Confidence Estimation for LLMs in Multi-turn Interactions", "comment": null, "summary": "While confidence estimation is a promising direction for mitigating hallucinations in Large Language Models (LLMs), current research dominantly focuses on single-turn settings. The dynamics of model confidence in multi-turn conversations, where context accumulates and ambiguity is progressively resolved, remain largely unexplored. Reliable confidence estimation in multi-turn settings is critical for many downstream applications, such as autonomous agents and human-in-the-loop systems. This work presents the first systematic study of confidence estimation in multi-turn interactions, establishing a formal evaluation framework grounded in two key desiderata: per-turn calibration and monotonicity of confidence as more information becomes available. To facilitate this, we introduce novel metrics, including a length-normalized Expected Calibration Error (InfoECE), and a new \"Hinter-Guesser\" paradigm for generating controlled evaluation datasets. Our experiments reveal that widely-used confidence techniques struggle with calibration and monotonicity in multi-turn dialogues. We propose P(Sufficient), a logit-based probe that achieves comparatively better performance, although the task remains far from solved. Our work provides a foundational methodology for developing more reliable and trustworthy conversational agents.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u7cfb\u7edf\u6027\u5730\u63a2\u7d22\u4e86\u591a\u8f6e\u5bf9\u8bdd\u4e2dLLM\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u8bc4\u4f30\u6846\u67b6\u548c\u65b0\u6307\u6807\uff0c\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u8f6e\u8bbe\u7f6e\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u6848\u3002", "motivation": "\u5f53\u524d\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u5355\u8f6e\u8bbe\u7f6e\uff0c\u800c\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u4e0a\u4e0b\u6587\u7d2f\u79ef\u548c\u6a21\u7cca\u6027\u9010\u6b65\u89e3\u51b3\u7684\u8fc7\u7a0b\u5bf9\u7f6e\u4fe1\u5ea6\u52a8\u6001\u53d8\u5316\u7684\u5f71\u54cd\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u53ef\u9760\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u5bf9\u4e8e\u81ea\u4e3b\u4ee3\u7406\u548c\u4eba\u673a\u534f\u4f5c\u7cfb\u7edf\u7b49\u4e0b\u6e38\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5efa\u7acb\u4e86\u57fa\u4e8e\u4e24\u4e2a\u5173\u952e\u9700\u6c42\u7684\u6b63\u5f0f\u8bc4\u4f30\u6846\u67b6\uff1a\u6bcf\u8f6e\u6821\u51c6\u6027\u548c\u968f\u7740\u4fe1\u606f\u589e\u52a0\u7f6e\u4fe1\u5ea6\u7684\u5355\u8c03\u6027\u3002\u5f15\u5165\u4e86\u65b0\u6307\u6807\uff08\u5982\u957f\u5ea6\u5f52\u4e00\u5316\u7684\u9884\u671f\u6821\u51c6\u8bef\u5deeInfoECE\uff09\u548c\"Hinter-Guesser\"\u8303\u5f0f\u6765\u751f\u6210\u53d7\u63a7\u8bc4\u4f30\u6570\u636e\u96c6\u3002\u63d0\u51fa\u4e86P(Sufficient)\u8fd9\u4e00\u57fa\u4e8elogit\u7684\u63a2\u9488\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5e7f\u6cdb\u4f7f\u7528\u7684\u7f6e\u4fe1\u5ea6\u6280\u672f\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u96be\u4ee5\u5b9e\u73b0\u826f\u597d\u7684\u6821\u51c6\u6027\u548c\u5355\u8c03\u6027\u3002\u63d0\u51fa\u7684P(Sufficient)\u65b9\u6cd5\u76f8\u6bd4\u5176\u4ed6\u65b9\u6cd5\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u8be5\u4efb\u52a1\u8fdc\u672a\u5b8c\u5168\u89e3\u51b3\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5f00\u53d1\u66f4\u53ef\u9760\u548c\u53ef\u4fe1\u8d56\u7684\u5bf9\u8bdd\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u57fa\u7840\u65b9\u6cd5\u8bba\uff0c\u5f3a\u8c03\u4e86\u591a\u8f6e\u5bf9\u8bdd\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u7684\u91cd\u8981\u6027\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2601.02186", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.02186", "abs": "https://arxiv.org/abs/2601.02186", "authors": ["Rui Yang", "Huitao Li", "Weihao Xuan", "Heli Qi", "Xin Li", "Kunyu Yu", "Yingjian Chen", "Rongrong Wang", "Jacques Behmoaras", "Tianxi Cai", "Bibhas Chakraborty", "Qingyu Chen", "Lionel Tim-Ee Cheng", "Marie-Louise Damwanza", "Chido Dzinotyiwei", "Aosong Feng", "Chuan Hong", "Yusuke Iwasawa", "Yuhe Ke", "Linah Kitala", "Taehoon Ko", "Jisan Lee", "Irene Li", "Jonathan Chong Kai Liew", "Hongfang Liu", "Lian Leng Low", "Edison Marrese-Taylor", "Yutaka Matsuo", "Isheanesu Misi", "Yilin Ning", "Jasmine Chiat Ling Ong", "Marcus Eng Hock Ong", "Enrico Petretto", "Hossein Rouhizadeh", "Abiram Sandralegar", "Oren Schreier", "Iain Bee Huat Tan", "Patrick Tan", "Daniel Shu Wei Ting", "Junjue Wang", "Chunhua Weng", "Matthew Yu Heng Wong", "Fang Wu", "Yunze Xiao", "Xuhai Xu", "Qingcheng Zeng", "Zhuo Zheng", "Yifan Peng", "Douglas Teodoro", "Nan Liu"], "title": "Toward Global Large Language Models in Medicine", "comment": "182 pages, 65 figures", "summary": "Despite continuous advances in medical technology, the global distribution of health care resources remains uneven. The development of large language models (LLMs) has transformed the landscape of medicine and holds promise for improving health care quality and expanding access to medical information globally. However, existing LLMs are primarily trained on high-resource languages, limiting their applicability in global medical scenarios. To address this gap, we constructed GlobMed, a large multilingual medical dataset, containing over 500,000 entries spanning 12 languages, including four low-resource languages. Building on this, we established GlobMed-Bench, which systematically assesses 56 state-of-the-art proprietary and open-weight LLMs across multiple multilingual medical tasks, revealing significant performance disparities across languages, particularly for low-resource languages. Additionally, we introduced GlobMed-LLMs, a suite of multilingual medical LLMs trained on GlobMed, with parameters ranging from 1.7B to 8B. GlobMed-LLMs achieved an average performance improvement of over 40% relative to baseline models, with a more than threefold increase in performance on low-resource languages. Together, these resources provide an important foundation for advancing the equitable development and application of LLMs globally, enabling broader language communities to benefit from technological advances.", "AI": {"tldr": "\u6784\u5efa\u4e86GlobMed\u591a\u8bed\u8a00\u533b\u7597\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u57fa\u51c6\u548c\u6a21\u578b\u5957\u4ef6\uff0c\u4ee5\u89e3\u51b3LLMs\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u533b\u7597\u573a\u666f\u4e2d\u7684\u6027\u80fd\u5dee\u8ddd\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u57fa\u4e8e\u9ad8\u8d44\u6e90\u8bed\u8a00\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u5176\u5728\u5168\u7403\u533b\u7597\u573a\u666f\u4e2d\u7684\u5e94\u7528\uff0c\u7279\u522b\u662f\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u4e2d\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u8ddd\u3002", "method": "1) \u6784\u5efaGlobMed\u591a\u8bed\u8a00\u533b\u7597\u6570\u636e\u96c6\uff0850\u4e07+\u6761\u76ee\uff0c12\u79cd\u8bed\u8a00\uff0c\u542b4\u79cd\u4f4e\u8d44\u6e90\u8bed\u8a00\uff09\uff1b2) \u5efa\u7acbGlobMed-Bench\u8bc4\u4f30\u57fa\u51c6\uff0c\u7cfb\u7edf\u8bc4\u4f3056\u4e2a\u5148\u8fdbLLMs\uff1b3) \u5f00\u53d1GlobMed-LLMs\u6a21\u578b\u5957\u4ef6\uff081.7B-8B\u53c2\u6570\uff09\u3002", "result": "GlobMed-LLMs\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\u5e73\u5747\u6027\u80fd\u63d0\u5347\u8d85\u8fc740%\uff0c\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u6027\u80fd\u63d0\u5347\u8d85\u8fc7\u4e09\u500d\uff1b\u8bc4\u4f30\u63ed\u793a\u4e86\u4e0d\u540c\u8bed\u8a00\u95f4\u7279\u522b\u662f\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u663e\u8457\u6027\u80fd\u5dee\u5f02\u3002", "conclusion": "\u8fd9\u4e9b\u8d44\u6e90\u4e3a\u4fc3\u8fdbLLMs\u5728\u5168\u7403\u7684\u516c\u5e73\u53d1\u5c55\u548c\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\uff0c\u4f7f\u66f4\u5e7f\u6cdb\u7684\u8bed\u8a00\u793e\u533a\u80fd\u591f\u4ece\u6280\u672f\u8fdb\u6b65\u4e2d\u53d7\u76ca\u3002"}}
{"id": "2601.01313", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01313", "abs": "https://arxiv.org/abs/2601.01313", "authors": ["Vladimer Khasia"], "title": "Spectral-Window Hybrid (SWH)", "comment": null, "summary": "Scaling sequence modeling to extreme contexts requires balancing computational efficiency with representational expressivity. While Transformers provide precise retrieval via the attention mechanism, their quadratic $\\mathcal{O}(T^2)$ complexity limits their application to long-horizon tasks. In this work, we propose the \\textbf{Spectral-Window Hybrid (SWH)}, an architecture that decouples sequence modeling into two \\textit{parallel} streams: a global branch utilizing the Convolution Theorem to model long-range decay dynamics in $\\mathcal{O}(T \\log T)$ time, and a local branch employing sliding-window attention for token interactions within a bounded context. By aggregating these representations, SWH avoids the computational bottleneck of global attention while retaining local precision. We demonstrate that SWH matches the perplexity of standard Transformers on short contexts while enabling efficient linear scaling to extended sequences. The code is available at https://github.com/VladimerKhasia/SWH", "AI": {"tldr": "SWH\u662f\u4e00\u79cd\u6df7\u5408\u67b6\u6784\uff0c\u901a\u8fc7\u5e76\u884c\u5168\u5c40\u5206\u652f\uff08\u57fa\u4e8e\u5377\u79ef\u5b9a\u7406\u5efa\u6a21\u957f\u7a0b\u8870\u51cf\u52a8\u6001\uff09\u548c\u5c40\u90e8\u5206\u652f\uff08\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\uff09\u6765\u89e3\u8026\u5e8f\u5217\u5efa\u6a21\uff0c\u5b9e\u73b0\u7ebf\u6027\u6269\u5c55\u81f3\u957f\u5e8f\u5217\uff0c\u540c\u65f6\u4fdd\u6301\u5c40\u90e8\u7cbe\u5ea6\u3002", "motivation": "\u5c06\u5e8f\u5217\u5efa\u6a21\u6269\u5c55\u5230\u6781\u7aef\u4e0a\u4e0b\u6587\u9700\u8981\u5728\u8ba1\u7b97\u6548\u7387\u548c\u8868\u793a\u8868\u8fbe\u80fd\u529b\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002Transformer\u867d\u7136\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u63d0\u4f9b\u7cbe\u786e\u68c0\u7d22\uff0c\u4f46\u5176\u4e8c\u6b21\u65b9\u590d\u6742\u5ea6\u9650\u5236\u4e86\u5728\u957f\u5e8f\u5217\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faSpectral-Window Hybrid (SWH)\u67b6\u6784\uff0c\u5c06\u5e8f\u5217\u5efa\u6a21\u89e3\u8026\u4e3a\u4e24\u4e2a\u5e76\u884c\u6d41\uff1a1) \u5168\u5c40\u5206\u652f\u5229\u7528\u5377\u79ef\u5b9a\u7406\u5728O(T log T)\u65f6\u95f4\u5185\u5efa\u6a21\u957f\u7a0b\u8870\u51cf\u52a8\u6001\uff1b2) \u5c40\u90e8\u5206\u652f\u91c7\u7528\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\u5904\u7406\u6709\u754c\u4e0a\u4e0b\u6587\u5185\u7684token\u4ea4\u4e92\u3002\u901a\u8fc7\u805a\u5408\u8fd9\u4e9b\u8868\u793a\uff0cSWH\u907f\u514d\u4e86\u5168\u5c40\u6ce8\u610f\u529b\u7684\u8ba1\u7b97\u74f6\u9888\u3002", "result": "SWH\u5728\u77ed\u4e0a\u4e0b\u6587\u4e0a\u8fbe\u5230\u6807\u51c6Transformer\u7684\u56f0\u60d1\u5ea6\u6c34\u5e73\uff0c\u540c\u65f6\u80fd\u591f\u9ad8\u6548\u7ebf\u6027\u6269\u5c55\u5230\u957f\u5e8f\u5217\u3002", "conclusion": "SWH\u67b6\u6784\u901a\u8fc7\u89e3\u8026\u5168\u5c40\u548c\u5c40\u90e8\u5efa\u6a21\uff0c\u5728\u4fdd\u6301\u5c40\u90e8\u7cbe\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u5bf9\u957f\u5e8f\u5217\u7684\u9ad8\u6548\u7ebf\u6027\u6269\u5c55\uff0c\u89e3\u51b3\u4e86Transformer\u5728\u957f\u5e8f\u5217\u4efb\u52a1\u4e2d\u7684\u8ba1\u7b97\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2601.01347", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01347", "abs": "https://arxiv.org/abs/2601.01347", "authors": ["Yuyan Pi", "Min Jin", "Wentao Xie", "Xinhua Liu"], "title": "From Classification to Generation: An Open-Ended Paradigm for Adverse Drug Reaction Prediction Based on Graph-Motif Feature Fusion", "comment": "34 pages,5 figures", "summary": "Computational biology offers immense potential for reducing the high costs and protracted cycles of new drug development through adverse drug reaction (ADR) prediction. However, current methods remain impeded by drug data scarcity-induced cold-start challenge, closed label sets, and inadequate modeling of label dependencies. Here we propose an open-ended ADR prediction paradigm based on Graph-Motif feature fusion and Multi-Label Generation (GM-MLG). Leveraging molecular structure as an intrinsic and inherent feature, GM-MLG constructs a dual-graph representation architecture spanning the atomic level, the local molecular level (utilizing fine-grained motifs dynamically extracted via the BRICS algorithm combined with additional fragmentation rules), and the global molecular level. Uniquely, GM-MLG pioneers transforming ADR prediction from multi-label classification into Transformer Decoder-based multi-label generation. By treating ADR labels as discrete token sequences, it employs positional embeddings to explicitly capture dependencies and co-occurrence relationships within large-scale label spaces, generating predictions via autoregressive decoding to dynamically expand the prediction space. Experiments demonstrate GM-MLG achieves up to 38% improvement and an average gain of 20%, expanding the prediction space from 200 to over 10,000 types. Furthermore, it elucidates non-linear structure-activity relationships between ADRs and motifs via retrosynthetic motif analysis, providing interpretable and innovative support for systematic risk reduction in drug safety.", "AI": {"tldr": "GM-MLG\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe-\u57fa\u5e8f\u7279\u5f81\u878d\u5408\u548c\u591a\u6807\u7b7e\u751f\u6210\u7684\u5f00\u653e\u5f0f\u836f\u7269\u4e0d\u826f\u53cd\u5e94\u9884\u6d4b\u65b0\u8303\u5f0f\uff0c\u5c06ADR\u9884\u6d4b\u4ece\u591a\u6807\u7b7e\u5206\u7c7b\u8f6c\u53d8\u4e3a\u57fa\u4e8eTransformer\u89e3\u7801\u5668\u7684\u591a\u6807\u7b7e\u751f\u6210\uff0c\u663e\u8457\u6269\u5c55\u4e86\u9884\u6d4b\u7a7a\u95f4\u5e76\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u836f\u7269\u4e0d\u826f\u53cd\u5e94\u9884\u6d4b\u65b9\u6cd5\u9762\u4e34\u836f\u7269\u6570\u636e\u7a00\u7f3a\u5bfc\u81f4\u7684\u51b7\u542f\u52a8\u95ee\u9898\u3001\u5c01\u95ed\u6807\u7b7e\u96c6\u4ee5\u53ca\u6807\u7b7e\u4f9d\u8d56\u5173\u7cfb\u5efa\u6a21\u4e0d\u8db3\u7b49\u6311\u6218\uff0c\u9650\u5236\u4e86\u9884\u6d4b\u80fd\u529b\u548c\u5e94\u7528\u8303\u56f4\u3002", "method": "1) \u6784\u5efa\u539f\u5b50\u7ea7\u3001\u5c40\u90e8\u5206\u5b50\u7ea7\uff08\u901a\u8fc7BRICS\u7b97\u6cd5\u52a8\u6001\u63d0\u53d6\u7ec6\u7c92\u5ea6\u57fa\u5e8f\uff09\u548c\u5168\u5c40\u5206\u5b50\u7ea7\u7684\u53cc\u56fe\u8868\u793a\u67b6\u6784\uff1b2) \u5c06ADR\u9884\u6d4b\u4ece\u591a\u6807\u7b7e\u5206\u7c7b\u8f6c\u53d8\u4e3a\u57fa\u4e8eTransformer\u89e3\u7801\u5668\u7684\u591a\u6807\u7b7e\u751f\u6210\uff1b3) \u5c06ADR\u6807\u7b7e\u89c6\u4e3a\u79bb\u6563\u6807\u8bb0\u5e8f\u5217\uff0c\u4f7f\u7528\u4f4d\u7f6e\u5d4c\u5165\u663e\u5f0f\u6355\u83b7\u6807\u7b7e\u4f9d\u8d56\u5173\u7cfb\uff1b4) \u901a\u8fc7\u81ea\u56de\u5f52\u89e3\u7801\u52a8\u6001\u6269\u5c55\u9884\u6d4b\u7a7a\u95f4\u3002", "result": "GM-MLG\u5b9e\u73b0\u4e86\u6700\u9ad838%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e73\u5747\u589e\u76ca20%\uff0c\u5c06\u9884\u6d4b\u7a7a\u95f4\u4ece200\u79cd\u6269\u5c55\u5230\u8d85\u8fc710,000\u79cd\u3002\u901a\u8fc7\u9006\u5408\u6210\u57fa\u5e8f\u5206\u6790\u63ed\u793a\u4e86ADR\u4e0e\u57fa\u5e8f\u4e4b\u95f4\u7684\u975e\u7ebf\u6027\u6784\u6548\u5173\u7cfb\u3002", "conclusion": "GM-MLG\u4e3a\u836f\u7269\u5b89\u5168\u6027\u7cfb\u7edf\u98ce\u9669\u964d\u4f4e\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u4e14\u521b\u65b0\u7684\u652f\u6301\uff0c\u901a\u8fc7\u5f00\u653e\u5f0f\u591a\u6807\u7b7e\u751f\u6210\u8303\u5f0f\u6709\u6548\u89e3\u51b3\u4e86\u5f53\u524dADR\u9884\u6d4b\u7684\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2601.02224", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.02224", "abs": "https://arxiv.org/abs/2601.02224", "authors": ["Fabian Lukassen", "Jan Herrmann", "Christoph Weisser", "Benjamin Saefken", "Thomas Kneib"], "title": "From XAI to Stories: A Factorial Study of LLM-Generated Explanation Quality", "comment": null, "summary": "Explainable AI (XAI) methods like SHAP and LIME produce numerical feature attributions that remain inaccessible to non expert users. Prior work has shown that Large Language Models (LLMs) can transform these outputs into natural language explanations (NLEs), but it remains unclear which factors contribute to high-quality explanations. We present a systematic factorial study investigating how Forecasting model choice, XAI method, LLM selection, and prompting strategy affect NLE quality. Our design spans four models (XGBoost (XGB), Random Forest (RF), Multilayer Perceptron (MLP), and SARIMAX - comparing black-box Machine-Learning (ML) against classical time-series approaches), three XAI conditions (SHAP, LIME, and a no-XAI baseline), three LLMs (GPT-4o, Llama-3-8B, DeepSeek-R1), and eight prompting strategies. Using G-Eval, an LLM-as-a-judge evaluation method, with dual LLM judges and four evaluation criteria, we evaluate 660 explanations for time-series forecasting. Our results suggest that: (1) XAI provides only small improvements over no-XAI baselines, and only for expert audiences; (2) LLM choice dominates all other factors, with DeepSeek-R1 outperforming GPT-4o and Llama-3; (3) we observe an interpretability paradox: in our setting, SARIMAX yielded lower NLE quality than ML models despite higher prediction accuracy; (4) zero-shot prompting is competitive with self-consistency at 7-times lower cost; and (5) chain-of-thought hurts rather than helps.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u7cfb\u7edf\u5b9e\u9a8c\u53d1\u73b0\uff1aLLM\u9009\u62e9\u5bf9\u53ef\u89e3\u91caAI\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u8d28\u91cf\u5f71\u54cd\u6700\u5927\uff0cXAI\u65b9\u6cd5\u4ec5\u5bf9\u4e13\u5bb6\u7528\u6237\u6709\u8f7b\u5fae\u63d0\u5347\uff0cSARIMAX\u6a21\u578b\u5b58\u5728\u53ef\u89e3\u91ca\u6027\u6096\u8bba\uff0c\u96f6\u6837\u672c\u63d0\u793a\u4e0e\u81ea\u6d3d\u6027\u6548\u679c\u76f8\u5f53\u4f46\u6210\u672c\u66f4\u4f4e\u3002", "motivation": "\u73b0\u6709XAI\u65b9\u6cd5\uff08\u5982SHAP\u3001LIME\uff09\u4ea7\u751f\u7684\u6570\u503c\u7279\u5f81\u5f52\u56e0\u5bf9\u975e\u4e13\u5bb6\u7528\u6237\u96be\u4ee5\u7406\u89e3\uff0c\u867d\u7136LLM\u53ef\u4ee5\u5c06\u8fd9\u4e9b\u8f93\u51fa\u8f6c\u5316\u4e3a\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\uff0c\u4f46\u5f71\u54cd\u89e3\u91ca\u8d28\u91cf\u7684\u5173\u952e\u56e0\u7d20\u5c1a\u4e0d\u660e\u786e\u3002\u672c\u7814\u7a76\u65e8\u5728\u7cfb\u7edf\u63a2\u7a76\u9884\u6d4b\u6a21\u578b\u9009\u62e9\u3001XAI\u65b9\u6cd5\u3001LLM\u9009\u62e9\u548c\u63d0\u793a\u7b56\u7565\u5982\u4f55\u5f71\u54cd\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u7684\u8d28\u91cf\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u56e0\u5b50\u8bbe\u8ba1\u7814\u7a76\uff0c\u6db5\u76d6\u56db\u4e2a\u9884\u6d4b\u6a21\u578b\uff08XGBoost\u3001\u968f\u673a\u68ee\u6797\u3001\u591a\u5c42\u611f\u77e5\u673a\u3001SARIMAX\uff09\u3001\u4e09\u79cdXAI\u6761\u4ef6\uff08SHAP\u3001LIME\u3001\u65e0XAI\u57fa\u7ebf\uff09\u3001\u4e09\u4e2aLLM\uff08GPT-4o\u3001Llama-3-8B\u3001DeepSeek-R1\uff09\u548c\u516b\u79cd\u63d0\u793a\u7b56\u7565\u3002\u4f7f\u7528G-Eval\uff08LLM\u4f5c\u4e3a\u8bc4\u5224\u8005\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ccLLM\u8bc4\u5224\u8005\u548c\u56db\u4e2a\u8bc4\u4f30\u6807\u51c6\uff0c\u5bf9\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684660\u4e2a\u89e3\u91ca\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u4e3b\u8981\u53d1\u73b0\uff1a1\uff09XAI\u4ec5\u5bf9\u4e13\u5bb6\u7528\u6237\u6709\u8f7b\u5fae\u6539\u8fdb\uff1b2\uff09LLM\u9009\u62e9\u662f\u5f71\u54cd\u89e3\u91ca\u8d28\u91cf\u7684\u6700\u91cd\u8981\u56e0\u7d20\uff0cDeepSeek-R1\u8868\u73b0\u4f18\u4e8eGPT-4o\u548cLlama-3\uff1b3\uff09\u5b58\u5728\u53ef\u89e3\u91ca\u6027\u6096\u8bba\uff1aSARIMAX\u867d\u7136\u9884\u6d4b\u51c6\u786e\u5ea6\u66f4\u9ad8\uff0c\u4f46\u4ea7\u751f\u7684\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u8d28\u91cf\u4f4e\u4e8e\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff1b4\uff09\u96f6\u6837\u672c\u63d0\u793a\u4e0e\u81ea\u6d3d\u6027\u6548\u679c\u76f8\u5f53\uff0c\u4f46\u6210\u672c\u964d\u4f4e7\u500d\uff1b5\uff09\u601d\u7ef4\u94fe\u63d0\u793a\u53cd\u800c\u4f1a\u964d\u4f4e\u89e3\u91ca\u8d28\u91cf\u3002", "conclusion": "LLM\u9009\u62e9\u662f\u51b3\u5b9a\u53ef\u89e3\u91caAI\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u8d28\u91cf\u7684\u5173\u952e\u56e0\u7d20\uff0c\u800c\u975eXAI\u65b9\u6cd5\u672c\u8eab\u3002\u5b9e\u9645\u5e94\u7528\u4e2d\u5e94\u4f18\u5148\u8003\u8651LLM\u9009\u62e9\uff0c\u5e76\u91c7\u7528\u6210\u672c\u6548\u76ca\u66f4\u9ad8\u7684\u96f6\u6837\u672c\u63d0\u793a\u7b56\u7565\u3002\u7814\u7a76\u63ed\u793a\u4e86\u6a21\u578b\u9884\u6d4b\u51c6\u786e\u5ea6\u4e0e\u89e3\u91ca\u8d28\u91cf\u4e4b\u95f4\u7684\u4e0d\u4e00\u81f4\u6027\uff0c\u4e3a\u6784\u5efa\u6709\u6548\u7684AI\u89e3\u91ca\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u6307\u5bfc\u3002"}}
{"id": "2601.01357", "categories": ["cs.LG", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2601.01357", "abs": "https://arxiv.org/abs/2601.01357", "authors": ["Ke Xiao", "Haoze Zhang", "Runze Mao", "Han Li", "Zhi X. Chen"], "title": "Towards LLM-enabled autonomous combustion research: A literature-aware agent for self-corrective modeling workflows", "comment": null, "summary": "The rapid evolution of large language models (LLMs) is transforming artificial intelligence into autonomous research partners, yet a critical gap persists in complex scientific domains such as combustion modeling. Here, practical AI assistance requires the seamless integration of domain literature knowledge with robust execution capabilities for expertise-intensive tools such as computational fluid dynamics (CFD) codes. To bridge this gap, we introduce FlamePilot, an LLM agent designed to empower combustion modeling research through automated and self-corrective CFD workflows. FlamePilot differentiates itself through an architecture that leverages atomic tools to ensure the robust setup and execution of complex simulations in both OpenFOAM and extended frameworks such as DeepFlame. The system is also capable of learning from scientific articles, extracting key information to guide the simulation from initial setup to optimized results. Validation on a public benchmark shows FlamePilot achieved a perfect 1.0 executability score and a 0.438 success rate, surpassing the prior best reported agent scores of 0.625 and 0.250, respectively. Furthermore, a detailed case study on Moderate or Intense Low-oxygen Dilution (MILD) combustion simulation demonstrates its efficacy as a collaborative research copilot, where FlamePilot autonomously translated a research paper into a configured simulation, conducted the simulation, post-processed the results, proposed evidence-based refinements, and managed a multi-step parameter study to convergence under minimal human intervention. By adopting a transparent and interpretable paradigm, FlamePilot establishes a foundational framework for AI-empowered combustion modeling, fostering a collaborative partnership where the agent manages workflow orchestration, freeing the researcher for high-level analysis.", "AI": {"tldr": "FlamePilot\u662f\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u71c3\u70e7\u5efa\u6a21\u7684LLM\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u548c\u81ea\u6821\u6b63\u7684CFD\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5c06\u79d1\u5b66\u6587\u732e\u77e5\u8bc6\u4e0e\u8ba1\u7b97\u6d41\u4f53\u52a8\u529b\u5b66\u5de5\u5177\u65e0\u7f1d\u96c6\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u71c3\u70e7\u6a21\u62df\u7684\u6548\u7387\u548c\u6210\u529f\u7387\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6b63\u5728\u6210\u4e3a\u81ea\u4e3b\u7814\u7a76\u4f19\u4f34\uff0c\u4f46\u5728\u71c3\u70e7\u5efa\u6a21\u7b49\u590d\u6742\u79d1\u5b66\u9886\u57df\u4ecd\u5b58\u5728\u5173\u952e\u5dee\u8ddd\u3002\u5b9e\u9645AI\u8f85\u52a9\u9700\u8981\u5c06\u9886\u57df\u6587\u732e\u77e5\u8bc6\u4e0e\u4e13\u4e1a\u8ba1\u7b97\u5de5\u5177\uff08\u5982CFD\u4ee3\u7801\uff09\u7684\u6267\u884c\u80fd\u529b\u65e0\u7f1d\u96c6\u6210\u3002", "method": "FlamePilot\u91c7\u7528\u57fa\u4e8e\u539f\u5b50\u5de5\u5177\u7684\u67b6\u6784\uff0c\u786e\u4fdd\u5728OpenFOAM\u548cDeepFlame\u7b49\u6846\u67b6\u4e2d\u7a33\u5065\u8bbe\u7f6e\u548c\u6267\u884c\u590d\u6742\u6a21\u62df\u3002\u7cfb\u7edf\u80fd\u591f\u4ece\u79d1\u5b66\u6587\u7ae0\u4e2d\u5b66\u4e60\uff0c\u63d0\u53d6\u5173\u952e\u4fe1\u606f\u6307\u5bfc\u4ece\u521d\u59cb\u8bbe\u7f6e\u5230\u4f18\u5316\u7ed3\u679c\u7684\u6574\u4e2a\u6a21\u62df\u8fc7\u7a0b\u3002", "result": "\u5728\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFlamePilot\u5b9e\u73b0\u4e86\u5b8c\u7f8e\u76841.0\u53ef\u6267\u884c\u6027\u5f97\u5206\u548c0.438\u7684\u6210\u529f\u7387\uff0c\u8d85\u8fc7\u4e86\u5148\u524d\u6700\u4f73\u4ee3\u7406\u76840.625\u548c0.250\u5f97\u5206\u3002\u5728MILD\u71c3\u70e7\u6a21\u62df\u6848\u4f8b\u7814\u7a76\u4e2d\uff0c\u7cfb\u7edf\u80fd\u591f\u81ea\u4e3b\u5c06\u7814\u7a76\u8bba\u6587\u8f6c\u5316\u4e3a\u914d\u7f6e\u6a21\u62df\u3001\u6267\u884c\u6a21\u62df\u3001\u540e\u5904\u7406\u7ed3\u679c\u3001\u63d0\u51fa\u57fa\u4e8e\u8bc1\u636e\u7684\u6539\u8fdb\u5efa\u8bae\uff0c\u5e76\u5728\u6700\u5c11\u4eba\u5de5\u5e72\u9884\u4e0b\u7ba1\u7406\u591a\u6b65\u9aa4\u53c2\u6570\u7814\u7a76\u76f4\u81f3\u6536\u655b\u3002", "conclusion": "FlamePilot\u901a\u8fc7\u900f\u660e\u53ef\u89e3\u91ca\u7684\u8303\u5f0f\uff0c\u4e3aAI\u8d4b\u80fd\u7684\u71c3\u70e7\u5efa\u6a21\u5efa\u7acb\u4e86\u57fa\u7840\u6846\u67b6\uff0c\u4fc3\u8fdb\u4e86\u7814\u7a76\u4eba\u5458\u4e0e\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u534f\u4f5c\u4f19\u4f34\u5173\u7cfb\uff0c\u5176\u4e2d\u667a\u80fd\u4f53\u7ba1\u7406\u5de5\u4f5c\u6d41\u7a0b\u7f16\u6392\uff0c\u4f7f\u7814\u7a76\u4eba\u5458\u80fd\u591f\u4e13\u6ce8\u4e8e\u9ad8\u5c42\u6b21\u5206\u6790\u3002"}}
{"id": "2601.02236", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.02236", "abs": "https://arxiv.org/abs/2601.02236", "authors": ["Yihao Liang", "Ze Wang", "Hao Chen", "Ximeng Sun", "Jialian Wu", "Xiaodong Yu", "Jiang Liu", "Emad Barsoum", "Zicheng Liu", "Niraj K. Jha"], "title": "CD4LM: Consistency Distillation and aDaptive Decoding for Diffusion Language Models", "comment": "33 pages, 7 figures", "summary": "Autoregressive large language models achieve strong results on many benchmarks, but decoding remains fundamentally latency-limited by sequential dependence on previously generated tokens. Diffusion language models (DLMs) promise parallel generation but suffer from a fundamental static-to-dynamic misalignment: Training optimizes local transitions under fixed schedules, whereas efficient inference requires adaptive \"long-jump\" refinements through unseen states. Our goal is to enable highly parallel decoding for DLMs with low number of function evaluations while preserving generation quality. To achieve this, we propose CD4LM, a framework that decouples training from inference via Discrete-Space Consistency Distillation (DSCD) and Confidence-Adaptive Decoding (CAD). Unlike standard objectives, DSCD trains a student to be trajectory-invariant, mapping diverse noisy states directly to the clean distribution. This intrinsic robustness enables CAD to dynamically allocate compute resources based on token confidence, aggressively skipping steps without the quality collapse typical of heuristic acceleration. On GSM8K, CD4LM matches the LLaDA baseline with a 5.18x wall-clock speedup; across code and math benchmarks, it strictly dominates the accuracy-efficiency Pareto frontier, achieving a 3.62x mean speedup while improving average accuracy. Code is available at https://github.com/yihao-liang/CDLM", "AI": {"tldr": "CD4LM\u6846\u67b6\u901a\u8fc7\u79bb\u6563\u7a7a\u95f4\u4e00\u81f4\u6027\u84b8\u998f\u548c\u7f6e\u4fe1\u5ea6\u81ea\u9002\u5e94\u89e3\u7801\uff0c\u89e3\u51b3\u4e86\u6269\u6563\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e0e\u63a8\u7406\u7684\u9759\u6001-\u52a8\u6001\u4e0d\u5bf9\u9f50\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u5e76\u884c\u89e3\u7801\u4e0e\u663e\u8457\u52a0\u901f\u3002", "motivation": "\u81ea\u56de\u5f52\u5927\u8bed\u8a00\u6a21\u578b\u89e3\u7801\u53d7\u9650\u4e8e\u5e8f\u5217\u4f9d\u8d56\u7684\u5ef6\u8fdf\uff0c\u6269\u6563\u8bed\u8a00\u6a21\u578b\u867d\u652f\u6301\u5e76\u884c\u751f\u6210\u4f46\u5b58\u5728\u8bad\u7ec3\u4e0e\u63a8\u7406\u7684\u4e0d\u5bf9\u9f50\uff1a\u8bad\u7ec3\u4f18\u5316\u56fa\u5b9a\u8c03\u5ea6\u4e0b\u7684\u5c40\u90e8\u8f6c\u79fb\uff0c\u800c\u9ad8\u6548\u63a8\u7406\u9700\u8981\u81ea\u9002\u5e94\"\u957f\u8df3\"\u4f18\u5316\u3002\u76ee\u6807\u662f\u5b9e\u73b0DLMs\u7684\u9ad8\u5e76\u884c\u89e3\u7801\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "method": "\u63d0\u51faCD4LM\u6846\u67b6\uff0c\u5305\u542b\u79bb\u6563\u7a7a\u95f4\u4e00\u81f4\u6027\u84b8\u998f\uff08DSCD\uff09\u548c\u7f6e\u4fe1\u5ea6\u81ea\u9002\u5e94\u89e3\u7801\uff08CAD\uff09\u3002DSCD\u8bad\u7ec3\u5b66\u751f\u6a21\u578b\u5177\u6709\u8f68\u8ff9\u4e0d\u53d8\u6027\uff0c\u5c06\u591a\u6837\u566a\u58f0\u72b6\u6001\u76f4\u63a5\u6620\u5c04\u5230\u5e72\u51c0\u5206\u5e03\uff1bCAD\u57fa\u4e8etoken\u7f6e\u4fe1\u5ea6\u52a8\u6001\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\uff0c\u6fc0\u8fdb\u8df3\u8fc7\u6b65\u9aa4\u800c\u4e0d\u635f\u5931\u8d28\u91cf\u3002", "result": "\u5728GSM8K\u4e0a\uff0cCD4LM\u5339\u914dLLaDA\u57fa\u7ebf\uff0c\u5b9e\u73b05.18\u500d\u5b9e\u65f6\u52a0\u901f\uff1b\u5728\u4ee3\u7801\u548c\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4e25\u683c\u4e3b\u5bfc\u51c6\u786e\u7387-\u6548\u7387\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c\u5e73\u5747\u52a0\u901f3.62\u500d\u7684\u540c\u65f6\u63d0\u9ad8\u5e73\u5747\u51c6\u786e\u7387\u3002", "conclusion": "CD4LM\u901a\u8fc7\u89e3\u8026\u8bad\u7ec3\u4e0e\u63a8\u7406\uff0c\u89e3\u51b3\u4e86\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u6838\u5fc3\u4e0d\u5bf9\u9f50\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u5e76\u884c\u89e3\u7801\u7684\u663e\u8457\u52a0\u901f\uff0c\u4e3a\u9ad8\u6548\u8bed\u8a00\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2601.01368", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01368", "abs": "https://arxiv.org/abs/2601.01368", "authors": ["Mujin Zhou", "Junzhe Zhang"], "title": "Causal discovery for linear causal model with correlated noise: an Adversarial Learning Approach", "comment": null, "summary": "Causal discovery from data with unmeasured confounding factors is a challenging problem. This paper proposes an approach based on the f-GAN framework, learning the binary causal structure independent of specific weight values. We reformulate the structure learning problem as minimizing Bayesian free energy and prove that this problem is equivalent to minimizing the f-divergence between the true data distribution and the model-generated distribution. Using the f-GAN framework, we transform this objective into a min-max adversarial optimization problem. We implement the gradient search in the discrete graph space using Gumbel-Softmax relaxation.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8ef-GAN\u6846\u67b6\u7684\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\uff0c\u5728\u5b58\u5728\u672a\u6d4b\u91cf\u6df7\u6742\u56e0\u7d20\u7684\u60c5\u51b5\u4e0b\u5b66\u4e60\u4e8c\u5143\u56e0\u679c\u7ed3\u6784\uff0c\u5c06\u7ed3\u6784\u5b66\u4e60\u95ee\u9898\u8f6c\u5316\u4e3a\u6700\u5c0f\u5316\u8d1d\u53f6\u65af\u81ea\u7531\u80fd\u91cf\uff0c\u5e76\u901a\u8fc7Gumbel-Softmax\u677e\u5f1b\u5728\u79bb\u6563\u56fe\u7a7a\u95f4\u4e2d\u8fdb\u884c\u68af\u5ea6\u641c\u7d22\u3002", "motivation": "\u5728\u5b58\u5728\u672a\u6d4b\u91cf\u6df7\u6742\u56e0\u7d20\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u56e0\u679c\u53d1\u73b0\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u7279\u5b9a\u7684\u6743\u91cd\u503c\u5047\u8bbe\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u72ec\u7acb\u4e8e\u5177\u4f53\u6743\u91cd\u503c\u5b66\u4e60\u56e0\u679c\u7ed3\u6784\u7684\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8ef-GAN\u6846\u67b6\uff0c\u5c06\u7ed3\u6784\u5b66\u4e60\u95ee\u9898\u91cd\u65b0\u8868\u8ff0\u4e3a\u6700\u5c0f\u5316\u8d1d\u53f6\u65af\u81ea\u7531\u80fd\u91cf\uff0c\u5e76\u8bc1\u660e\u8be5\u95ee\u9898\u7b49\u4ef7\u4e8e\u6700\u5c0f\u5316\u771f\u5b9e\u6570\u636e\u5206\u5e03\u4e0e\u6a21\u578b\u751f\u6210\u5206\u5e03\u4e4b\u95f4\u7684f-\u6563\u5ea6\u3002\u4f7f\u7528f-GAN\u6846\u67b6\u5c06\u76ee\u6807\u8f6c\u5316\u4e3amin-max\u5bf9\u6297\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u901a\u8fc7Gumbel-Softmax\u677e\u5f1b\u5728\u79bb\u6563\u56fe\u7a7a\u95f4\u5b9e\u73b0\u68af\u5ea6\u641c\u7d22\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u5b58\u5728\u672a\u6d4b\u91cf\u6df7\u6742\u56e0\u7d20\u7684\u60c5\u51b5\u4e0b\u5b66\u4e60\u4e8c\u5143\u56e0\u679c\u7ed3\u6784\uff0c\u4e0d\u4f9d\u8d56\u4e8e\u7279\u5b9a\u7684\u6743\u91cd\u503c\u5047\u8bbe\uff0c\u901a\u8fc7\u5bf9\u6297\u4f18\u5316\u548c\u79bb\u6563\u7a7a\u95f4\u68af\u5ea6\u641c\u7d22\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u56e0\u679c\u53d1\u73b0\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8ef-GAN\u6846\u67b6\u7684\u65b9\u6cd5\u4e3a\u5b58\u5728\u672a\u6d4b\u91cf\u6df7\u6742\u56e0\u7d20\u7684\u56e0\u679c\u53d1\u73b0\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5c06\u7ed3\u6784\u5b66\u4e60\u8f6c\u5316\u4e3a\u8d1d\u53f6\u65af\u81ea\u7531\u80fd\u91cf\u6700\u5c0f\u5316\u95ee\u9898\uff0c\u5e76\u5229\u7528\u5bf9\u6297\u4f18\u5316\u548c\u79bb\u6563\u7a7a\u95f4\u68af\u5ea6\u641c\u7d22\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u56e0\u679c\u7ed3\u6784\u5b66\u4e60\u3002"}}
{"id": "2601.02285", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02285", "abs": "https://arxiv.org/abs/2601.02285", "authors": ["Tobias Schimanski", "Imene Kolli", "Jingwei Ni", "Yu Fan", "Ario Saeid Vaghefi", "Elliott Ash", "Markus Leippold"], "title": "pdfQA: Diverse, Challenging, and Realistic Question Answering over PDFs", "comment": null, "summary": "PDFs are the second-most used document type on the internet (after HTML). Yet, existing QA datasets commonly start from text sources or only address specific domains. In this paper, we present pdfQA, a multi-domain 2K human-annotated (real-pdfQA) and 2K synthetic dataset (syn-pdfQA) differentiating QA pairs in ten complexity dimensions (e.g., file type, source modality, source position, answer type). We apply and evaluate quality and difficulty filters on both datasets, obtaining valid and challenging QA pairs. We answer the questions with open-source LLMs, revealing existing challenges that correlate with our complexity dimensions. pdfQA presents a basis for end-to-end QA pipeline evaluation, testing diverse skill sets and local optimizations (e.g., in information retrieval or parsing).", "AI": {"tldr": "pdfQA\u662f\u4e00\u4e2a\u591a\u9886\u57dfPDF\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u5305\u542b2K\u4eba\u5de5\u6807\u6ce8\u548c2K\u5408\u6210\u6570\u636e\uff0c\u6309\u5341\u4e2a\u590d\u6742\u5ea6\u7ef4\u5ea6\u5206\u7c7b\uff0c\u7528\u4e8e\u8bc4\u4f30\u7aef\u5230\u7aefQA\u7cfb\u7edf\u6027\u80fd", "motivation": "PDF\u662f\u4e92\u8054\u7f51\u4e0a\u7b2c\u4e8c\u5e38\u7528\u7684\u6587\u6863\u7c7b\u578b\uff0c\u4f46\u73b0\u6709QA\u6570\u636e\u96c6\u4e3b\u8981\u57fa\u4e8e\u6587\u672c\u6e90\u6216\u4ec5\u9488\u5bf9\u7279\u5b9a\u9886\u57df\uff0c\u7f3a\u4e4f\u9488\u5bf9PDF\u6587\u6863\u7684\u7efc\u5408\u6027\u95ee\u7b54\u8bc4\u4f30\u57fa\u51c6", "method": "\u521b\u5efapdfQA\u6570\u636e\u96c6\uff1a\u5305\u542b2K\u4eba\u5de5\u6807\u6ce8\u7684\u771f\u5b9ePDF\u95ee\u7b54\u5bf9\u548c2K\u5408\u6210\u7684\u95ee\u7b54\u5bf9\uff0c\u6309\u5341\u4e2a\u590d\u6742\u5ea6\u7ef4\u5ea6\u5206\u7c7b\uff08\u5982\u6587\u4ef6\u7c7b\u578b\u3001\u6e90\u6a21\u6001\u3001\u6e90\u4f4d\u7f6e\u3001\u7b54\u6848\u7c7b\u578b\u7b49\uff09\uff0c\u5e94\u7528\u8d28\u91cf\u548c\u96be\u5ea6\u8fc7\u6ee4\u5668\u7b5b\u9009\u6709\u6548\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u7b54\u5bf9", "result": "\u4f7f\u7528\u5f00\u6e90LLM\u56de\u7b54\u95ee\u9898\uff0c\u53d1\u73b0\u73b0\u6709\u6311\u6218\u4e0e\u590d\u6742\u5ea6\u7ef4\u5ea6\u76f8\u5173\uff1b\u6570\u636e\u96c6\u4e3a\u7aef\u5230\u7aefQA\u7ba1\u9053\u8bc4\u4f30\u63d0\u4f9b\u57fa\u7840\uff0c\u53ef\u6d4b\u8bd5\u591a\u6837\u5316\u6280\u80fd\u96c6\u548c\u5c40\u90e8\u4f18\u5316\uff08\u5982\u4fe1\u606f\u68c0\u7d22\u6216\u89e3\u6790\uff09", "conclusion": "pdfQA\u586b\u8865\u4e86PDF\u6587\u6863\u95ee\u7b54\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u901a\u8fc7\u591a\u7ef4\u5ea6\u590d\u6742\u5ea6\u5206\u7c7b\u4e3aQA\u7cfb\u7edf\u8bc4\u4f30\u63d0\u4f9b\u5168\u9762\u57fa\u51c6\uff0c\u6709\u52a9\u4e8e\u8bc6\u522b\u548c\u6539\u8fdb\u73b0\u6709\u7cfb\u7edf\u7684\u5c40\u9650\u6027"}}
{"id": "2601.01383", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01383", "abs": "https://arxiv.org/abs/2601.01383", "authors": ["Yen-Chia Chen", "Hsing-Kuo Pao", "Hanjuan Huang"], "title": "Data Complexity-aware Deep Model Performance Forecasting", "comment": "12 pages, 12 figures", "summary": "Deep learning models are widely used across computer vision and other domains. When working on the model induction, selecting the right architecture for a given dataset often relies on repetitive trial-and-error procedures. This procedure is time-consuming, resource-intensive, and difficult to automate. While previous work has explored performance prediction using partial training or complex simulations, these methods often require significant computational overhead or lack generalizability. In this work, we propose an alternative approach: a lightweight, two-stage framework that can estimate model performance before training given the understanding of the dataset and the focused deep model structures. The first stage predicts a baseline based on the analysis of some measurable properties of the dataset, while the second stage adjusts the estimation with additional information on the model's architectural and hyperparameter details. The setup allows the framework to generalize across datasets and model types. Moreover, we find that some of the underlying features used for prediction - such as dataset variance - can offer practical guidance for model selection, and can serve as early indicators of data quality. As a result, the framework can be used not only to forecast model performance, but also to guide architecture choices, inform necessary preprocessing procedures, and detect potentially problematic datasets before training begins.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u53ef\u5728\u8bad\u7ec3\u524d\u9884\u6d4b\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6027\u80fd\uff0c\u65e0\u9700\u5b9e\u9645\u8bad\u7ec3\u6216\u590d\u6742\u6a21\u62df", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u9009\u62e9\u901a\u5e38\u4f9d\u8d56\u8bd5\u9519\u8fc7\u7a0b\uff0c\u8017\u65f6\u8017\u8d44\u6e90\u4e14\u96be\u4ee5\u81ea\u52a8\u5316\u3002\u73b0\u6709\u6027\u80fd\u9884\u6d4b\u65b9\u6cd5\u8981\u4e48\u9700\u8981\u90e8\u5206\u8bad\u7ec3\uff0c\u8981\u4e48\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u8981\u4e48\u7f3a\u4e4f\u6cdb\u5316\u80fd\u529b", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u57fa\u4e8e\u6570\u636e\u96c6\u7684\u53ef\u6d4b\u91cf\u5c5e\u6027\u9884\u6d4b\u57fa\u7ebf\u6027\u80fd\uff1b\u7b2c\u4e8c\u9636\u6bb5\u7ed3\u5408\u6a21\u578b\u67b6\u6784\u548c\u8d85\u53c2\u6570\u4fe1\u606f\u8c03\u6574\u4f30\u8ba1\u3002\u6846\u67b6\u53ef\u6cdb\u5316\u5230\u4e0d\u540c\u6570\u636e\u96c6\u548c\u6a21\u578b\u7c7b\u578b", "result": "\u6846\u67b6\u4e0d\u4ec5\u80fd\u9884\u6d4b\u6a21\u578b\u6027\u80fd\uff0c\u8fd8\u80fd\u6307\u5bfc\u67b6\u6784\u9009\u62e9\u3001\u9884\u5904\u7406\u6d41\u7a0b\uff0c\u5e76\u5728\u8bad\u7ec3\u524d\u68c0\u6d4b\u6f5c\u5728\u95ee\u9898\u6570\u636e\u96c6\u3002\u53d1\u73b0\u6570\u636e\u96c6\u65b9\u5dee\u7b49\u5e95\u5c42\u7279\u5f81\u53ef\u4f5c\u4e3a\u6570\u636e\u8d28\u91cf\u7684\u65e9\u671f\u6307\u6807", "conclusion": "\u63d0\u51fa\u7684\u8f7b\u91cf\u7ea7\u6846\u67b6\u4e3a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u9009\u62e9\u548c\u6027\u80fd\u9884\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6cdb\u5316\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u51cf\u5c11\u4e86\u8bd5\u9519\u6210\u672c\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u6570\u636e\u8d28\u91cf\u6d1e\u5bdf"}}
{"id": "2601.02298", "categories": ["cs.CL", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.02298", "abs": "https://arxiv.org/abs/2601.02298", "authors": ["Mahmoud Elgenedy"], "title": "Power-of-Two Quantization-Aware-Training (PoT-QAT) in Large Language Models (LLMs)", "comment": null, "summary": "In Large Language Models (LLMs), the number of parameters has grown exponentially in the past few years, e.g., from 1.5 billion parameters in GPT-2 to 175 billion in GPT-3 to possibly more than trillion in higher versions. This raises a significant challenge for implementation, especially for Edge devices. Unlike cloud computing, memory and processing power for Edge devices are very limited, which necessitates developing novel ideas to make such applications feasible. In this work, we investigate compressing weights with a special quantization that limits numbers to only power-of-two (PoT). This helps save a huge amount of memory as only exponents need to be stored, more importantly, it significantly reduces processing power by replacing costly multiplication with low cost bit shifting. To overcome performance loss due to this strict quantization, we investigate Quantization Aware Training (QAT) to enhance performance through additional training. Results on GPT-2 124M show a major enhancement for quantized PoT model after additional training, with a perplexity enhancement of 66% and BERT-Score loss to baseline GPT-2 of 1%. The memory saving is estimated to be 87.5% while the inference speed is expected to be 3-10x faster with PoT quantization versus full-precision.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684Power-of-Two\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6743\u91cd\u9650\u5236\u4e3a2\u7684\u5e42\u6b21\u65b9\uff0c\u5b9e\u73b0\u5185\u5b58\u8282\u770187.5%\u548c\u63a8\u7406\u901f\u5ea6\u63d0\u53473-10\u500d\uff0c\u540c\u65f6\u4f7f\u7528\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u6765\u7f13\u89e3\u6027\u80fd\u635f\u5931\u3002", "motivation": "\u968f\u7740LLM\u53c2\u6570\u89c4\u6a21\u6307\u6570\u7ea7\u589e\u957f\uff08\u4eceGPT-2\u768415\u4ebf\u5230GPT-3\u76841750\u4ebf\u518d\u5230\u53ef\u80fd\u4e07\u4ebf\u7ea7\uff09\uff0c\u8fb9\u7f18\u8bbe\u5907\u9762\u4e34\u4e25\u91cd\u7684\u5185\u5b58\u548c\u5904\u7406\u80fd\u529b\u9650\u5236\u3002\u4e91\u8ba1\u7b97\u7684\u8d44\u6e90\u4e30\u5bcc\uff0c\u4f46\u8fb9\u7f18\u8bbe\u5907\u8d44\u6e90\u6709\u9650\uff0c\u9700\u8981\u521b\u65b0\u7684\u538b\u7f29\u65b9\u6cd5\u6765\u4f7f\u8fd9\u4e9b\u5e94\u7528\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u53ef\u884c\u3002", "method": "\u91c7\u7528Power-of-Two\u91cf\u5316\u65b9\u6cd5\uff0c\u5c06\u6743\u91cd\u9650\u5236\u4e3a2\u7684\u5e42\u6b21\u65b9\uff0c\u8fd9\u6837\u53ea\u9700\u5b58\u50a8\u6307\u6570\u5373\u53ef\uff0c\u5927\u5e45\u8282\u7701\u5185\u5b58\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u7528\u4f4e\u6210\u672c\u4f4d\u79fb\u64cd\u4f5c\u66ff\u4ee3\u6602\u8d35\u7684\u4e58\u6cd5\u8fd0\u7b97\uff0c\u663e\u8457\u964d\u4f4e\u5904\u7406\u9700\u6c42\u3002\u4e3a\u4e86\u514b\u670d\u4e25\u683c\u91cf\u5316\u5e26\u6765\u7684\u6027\u80fd\u635f\u5931\uff0c\u91c7\u7528\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u8fdb\u884c\u989d\u5916\u8bad\u7ec3\u6765\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5728GPT-2 124M\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u7ecf\u8fc7\u989d\u5916\u8bad\u7ec3\u7684\u91cf\u5316PoT\u6a21\u578b\u6027\u80fd\u5927\u5e45\u63d0\u5347\uff1a\u56f0\u60d1\u5ea6\u6539\u558466%\uff0cBERT-Score\u635f\u5931\u4ec5\u6bd4\u57fa\u7ebfGPT-2\u9ad81%\u3002\u5185\u5b58\u8282\u7701\u4f30\u8ba1\u8fbe87.5%\uff0c\u63a8\u7406\u901f\u5ea6\u9884\u8ba1\u6bd4\u5168\u7cbe\u5ea6\u6a21\u578b\u5feb3-10\u500d\u3002", "conclusion": "Power-of-Two\u91cf\u5316\u7ed3\u5408\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u662f\u4e00\u79cd\u6709\u6548\u7684LLM\u538b\u7f29\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u663e\u8457\u7684\u5185\u5b58\u8282\u7701\u548c\u63a8\u7406\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u63a5\u8fd1\u539f\u59cb\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4e3a\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2601.01387", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01387", "abs": "https://arxiv.org/abs/2601.01387", "authors": ["Yongzhe Li", "Lin Guan", "Zihan Cai", "Zuxian Lin", "Jiyu Huang", "Liukai Chen"], "title": "Scale-Adaptive Power Flow Analysis with Local Topology Slicing and Multi-Task Graph Learning", "comment": null, "summary": "Developing deep learning models with strong adaptability to topological variations is of great practical significance for power flow analysis. To enhance model performance under variable system scales and improve robustness in branch power prediction, this paper proposes a Scale-adaptive Multi-task Power Flow Analysis (SaMPFA) framework. SaMPFA introduces a Local Topology Slicing (LTS) sampling technique that extracts subgraphs of different scales from the complete power network to strengthen the model's cross-scale learning capability. Furthermore, a Reference-free Multi-task Graph Learning (RMGL) model is designed for robust power flow prediction. Unlike existing approaches, RMGL predicts bus voltages and branch powers instead of phase angles. This design not only avoids the risk of error amplification in branch power calculation but also guides the model to learn the physical relationships of phase angle differences. In addition, the loss function incorporates extra terms that encourage the model to capture the physical patterns of angle differences and power transmission, further improving consistency between predictions and physical laws. Simulations on the IEEE 39-bus system and a real provincial grid in China demonstrate that the proposed model achieves superior adaptability and generalization under variable system scales, with accuracy improvements of 4.47% and 36.82%, respectively.", "AI": {"tldr": "\u63d0\u51faSaMPFA\u6846\u67b6\uff0c\u901a\u8fc7\u5c40\u90e8\u62d3\u6251\u5207\u7247\u91c7\u6837\u548c\u591a\u4efb\u52a1\u56fe\u5b66\u4e60\u6a21\u578b\uff0c\u63d0\u5347\u7535\u529b\u6f6e\u6d41\u5206\u6790\u5728\u7cfb\u7edf\u89c4\u6a21\u53d8\u5316\u4e0b\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5f00\u53d1\u5177\u6709\u5f3a\u62d3\u6251\u9002\u5e94\u6027\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5bf9\u6f6e\u6d41\u5206\u6790\u5177\u6709\u91cd\u8981\u5b9e\u8df5\u610f\u4e49\uff0c\u9700\u8981\u589e\u5f3a\u6a21\u578b\u5728\u53ef\u53d8\u7cfb\u7edf\u89c4\u6a21\u4e0b\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u9ad8\u652f\u8def\u529f\u7387\u9884\u6d4b\u7684\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51faSaMPFA\u6846\u67b6\uff0c\u5305\u542b\uff1a1) \u5c40\u90e8\u62d3\u6251\u5207\u7247\u91c7\u6837\u6280\u672f\uff0c\u4ece\u5b8c\u6574\u7535\u7f51\u63d0\u53d6\u4e0d\u540c\u5c3a\u5ea6\u7684\u5b50\u56fe\u4ee5\u589e\u5f3a\u8de8\u5c3a\u5ea6\u5b66\u4e60\u80fd\u529b\uff1b2) \u65e0\u53c2\u8003\u591a\u4efb\u52a1\u56fe\u5b66\u4e60\u6a21\u578b\uff0c\u76f4\u63a5\u9884\u6d4b\u6bcd\u7ebf\u7535\u538b\u548c\u652f\u8def\u529f\u7387\u800c\u975e\u76f8\u89d2\uff0c\u907f\u514d\u8bef\u5dee\u653e\u5927\u5e76\u5f15\u5bfc\u6a21\u578b\u5b66\u4e60\u76f8\u89d2\u5dee\u7684\u7269\u7406\u5173\u7cfb\uff1b3) \u635f\u5931\u51fd\u6570\u4e2d\u52a0\u5165\u989d\u5916\u9879\uff0c\u9f13\u52b1\u6a21\u578b\u6355\u6349\u89d2\u5dee\u548c\u529f\u7387\u4f20\u8f93\u7684\u7269\u7406\u6a21\u5f0f\u3002", "result": "\u5728IEEE 39\u8282\u70b9\u7cfb\u7edf\u548c\u5b9e\u9645\u7701\u7ea7\u7535\u7f51\u4e0a\u7684\u4eff\u771f\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u53ef\u53d8\u7cfb\u7edf\u89c4\u6a21\u4e0b\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u9002\u5e94\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u51c6\u786e\u7387\u5206\u522b\u63d0\u9ad8\u4e864.47%\u548c36.82%\u3002", "conclusion": "SaMPFA\u6846\u67b6\u901a\u8fc7\u5c40\u90e8\u62d3\u6251\u5207\u7247\u91c7\u6837\u548c\u591a\u4efb\u52a1\u56fe\u5b66\u4e60\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u7535\u529b\u6f6e\u6d41\u5206\u6790\u4e2d\u5bf9\u62d3\u6251\u53d8\u5316\u7684\u9002\u5e94\u6027\u548c\u9884\u6d4b\u9c81\u68d2\u6027\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2601.02303", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.02303", "abs": "https://arxiv.org/abs/2601.02303", "authors": ["Juan-Jos\u00e9 Guzm\u00e1n-Landa", "Juan-Manuel Torres-Moreno", "Miguel Figueroa-Saavedra", "Carlos-Emiliano Gonz\u00e1lez-Gallardo", "Graham Ranger", "Martha Lorena-Avenda\u00f1o-Garrido"], "title": "Classifying several dialectal Nawatl varieties", "comment": "9 pages, 5 figures, 4 tables", "summary": "Mexico is a country with a large number of indigenous languages, among which the most widely spoken is Nawatl, with more than two million people currently speaking it (mainly in North and Central America). Despite its rich cultural heritage, which dates back to the 15th century, Nawatl is a language with few computer resources. The problem is compounded when it comes to its dialectal varieties, with approximately 30 varieties recognised, not counting the different spellings in the written forms of the language. In this research work, we addressed the problem of classifying Nawatl varieties using Machine Learning and Neural Networks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u548c\u795e\u7ecf\u7f51\u7edc\u5bf9\u7eb3\u74e6\u7279\u5c14\u8bed\u7684\u65b9\u8a00\u53d8\u4f53\u8fdb\u884c\u5206\u7c7b\uff0c\u4ee5\u89e3\u51b3\u8fd9\u79cd\u62e5\u6709\u4e30\u5bcc\u6587\u5316\u9057\u4ea7\u4f46\u8ba1\u7b97\u8d44\u6e90\u7a00\u7f3a\u7684\u8bed\u8a00\u7684\u65b9\u8a00\u8bc6\u522b\u95ee\u9898\u3002", "motivation": "\u7eb3\u74e6\u7279\u5c14\u8bed\u662f\u58a8\u897f\u54e5\u4f7f\u7528\u6700\u5e7f\u6cdb\u7684\u571f\u8457\u8bed\u8a00\uff0c\u62e5\u6709\u8d85\u8fc7200\u4e07\u4f7f\u7528\u8005\uff0c\u4f46\u5176\u8ba1\u7b97\u8d44\u6e90\u7a00\u7f3a\u3002\u8be5\u8bed\u8a00\u6709\u7ea630\u79cd\u65b9\u8a00\u53d8\u4f53\uff0c\u52a0\u4e0a\u4e0d\u540c\u7684\u4e66\u5199\u62fc\u5199\u5f62\u5f0f\uff0c\u4f7f\u5f97\u65b9\u8a00\u8bc6\u522b\u95ee\u9898\u66f4\u52a0\u590d\u6742\u3002\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u79cd\u6587\u5316\u9057\u4ea7\u4e30\u5bcc\u4f46\u6280\u672f\u8d44\u6e90\u4e0d\u8db3\u7684\u8bed\u8a00\u7684\u65b9\u8a00\u5206\u7c7b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u673a\u5668\u5b66\u4e60\u548c\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u5bf9\u7eb3\u74e6\u7279\u5c14\u8bed\u7684\u65b9\u8a00\u53d8\u4f53\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u6458\u8981\u4e2d\u672a\u63d0\u4f9b\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c\uff0c\u4f46\u8868\u660e\u5df2\u6210\u529f\u5e94\u7528\u673a\u5668\u5b66\u4e60\u548c\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u6765\u89e3\u51b3\u7eb3\u74e6\u7279\u5c14\u8bed\u65b9\u8a00\u5206\u7c7b\u95ee\u9898\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u673a\u5668\u5b66\u4e60\u548c\u795e\u7ecf\u7f51\u7edc\u5728\u4f4e\u8d44\u6e90\u571f\u8457\u8bed\u8a00\u65b9\u8a00\u5206\u7c7b\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u4fdd\u62a4\u548c\u53d1\u5c55\u7eb3\u74e6\u7279\u5c14\u8bed\u6587\u5316\u9057\u4ea7\u63d0\u4f9b\u4e86\u6280\u672f\u9014\u5f84\u3002"}}
{"id": "2601.01403", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01403", "abs": "https://arxiv.org/abs/2601.01403", "authors": ["Zewei Yu", "Jianqiu Xu", "Caimin Li"], "title": "A Graph-based Framework for Online Time Series Anomaly Detection Using Model Ensemble", "comment": "8 pages", "summary": "With the increasing volume of streaming data in industrial systems, online anomaly detection has become a critical task. The diverse and rapidly evolving data patterns pose significant challenges for online anomaly detection. Many existing anomaly detection methods are designed for offline settings or have difficulty in handling heterogeneous streaming data effectively. This paper proposes GDME, an unsupervised graph-based framework for online time series anomaly detection using model ensemble. GDME maintains a dynamic model pool that is continuously updated by pruning underperforming models and introducing new ones. It utilizes a dynamic graph structure to represent relationships among models and employs community detection on the graph to select an appropriate subset for ensemble. The graph structure is also used to detect concept drift by monitoring structural changes, allowing the framework to adapt to evolving streaming data. Experiments on seven heterogeneous time series demonstrate that GDME outperforms existing online anomaly detection methods, achieving improvements of up to 24%. In addition, its ensemble strategy provides superior detection performance compared with both individual models and average ensembles, with competitive computational efficiency.", "AI": {"tldr": "\u63d0\u51faGDME\u6846\u67b6\uff0c\u4e00\u79cd\u57fa\u4e8e\u56fe\u6a21\u578b\u96c6\u6210\u7684\u65e0\u76d1\u7763\u5728\u7ebf\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u56fe\u7ed3\u6784\u548c\u793e\u533a\u68c0\u6d4b\u5b9e\u73b0\u6a21\u578b\u9009\u62e9\u4e0e\u6982\u5ff5\u6f02\u79fb\u9002\u5e94", "motivation": "\u5de5\u4e1a\u7cfb\u7edf\u4e2d\u6d41\u6570\u636e\u91cf\u4e0d\u65ad\u589e\u52a0\uff0c\u5728\u7ebf\u5f02\u5e38\u68c0\u6d4b\u6210\u4e3a\u5173\u952e\u4efb\u52a1\u3002\u73b0\u6709\u65b9\u6cd5\u591a\u4e3a\u79bb\u7ebf\u8bbe\u8ba1\u6216\u96be\u4ee5\u6709\u6548\u5904\u7406\u5f02\u6784\u6d41\u6570\u636e\uff0c\u9700\u8981\u80fd\u591f\u9002\u5e94\u5feb\u901f\u6f14\u5316\u6570\u636e\u6a21\u5f0f\u7684\u5728\u7ebf\u68c0\u6d4b\u65b9\u6cd5", "method": "GDME\u6846\u67b6\u7ef4\u62a4\u52a8\u6001\u6a21\u578b\u6c60\uff0c\u901a\u8fc7\u526a\u679d\u8868\u73b0\u4e0d\u4f73\u6a21\u578b\u548c\u5f15\u5165\u65b0\u6a21\u578b\u6301\u7eed\u66f4\u65b0\u3002\u4f7f\u7528\u52a8\u6001\u56fe\u7ed3\u6784\u8868\u793a\u6a21\u578b\u95f4\u5173\u7cfb\uff0c\u901a\u8fc7\u56fe\u4e0a\u7684\u793e\u533a\u68c0\u6d4b\u9009\u62e9\u5408\u9002\u5b50\u96c6\u8fdb\u884c\u96c6\u6210\u3002\u5229\u7528\u56fe\u7ed3\u6784\u53d8\u5316\u76d1\u6d4b\u6982\u5ff5\u6f02\u79fb\uff0c\u9002\u5e94\u6f14\u5316\u4e2d\u7684\u6d41\u6570\u636e", "result": "\u5728\u4e03\u4e2a\u5f02\u6784\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGDME\u4f18\u4e8e\u73b0\u6709\u5728\u7ebf\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u63d0\u5347\u5e45\u5ea6\u6700\u9ad8\u8fbe24%\u3002\u5176\u96c6\u6210\u7b56\u7565\u76f8\u6bd4\u5355\u4e2a\u6a21\u578b\u548c\u5e73\u5747\u96c6\u6210\u63d0\u4f9b\u66f4\u4f18\u68c0\u6d4b\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7ade\u4e89\u529b", "conclusion": "GDME\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u56fe\u6a21\u578b\u96c6\u6210\u6709\u6548\u89e3\u51b3\u4e86\u5728\u7ebf\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u5f02\u6784\u6d41\u6570\u636e\u5904\u7406\u95ee\u9898\uff0c\u80fd\u591f\u9002\u5e94\u6982\u5ff5\u6f02\u79fb\u5e76\u5b9e\u73b0\u9ad8\u6027\u80fd\u68c0\u6d4b\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c"}}
{"id": "2601.02320", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.02320", "abs": "https://arxiv.org/abs/2601.02320", "authors": ["Nikolay Mikhaylovskiy"], "title": "Estimating Text Temperature", "comment": null, "summary": "Autoregressive language models typically use temperature parameter at inference to shape the probability distribution and control the randomness of the text generated. After the text was generated, this parameter can be estimated using maximum likelihood approach. Following it, we propose a procedure to estimate the temperature of any text, including ones written by humans, with respect to a given language model. We evaluate the temperature estimation capability of a wide selection of small-to-medium LLMs. We then use the best-performing Qwen3 14B to estimate temperatures of popular corpora.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4f30\u8ba1\u6587\u672c\u6e29\u5ea6\u7684\u65b9\u6cd5\uff0c\u53ef\u7528\u4e8e\u8bc4\u4f30\u4efb\u4f55\u6587\u672c\uff08\u5305\u62ec\u4eba\u7c7b\u5199\u4f5c\uff09\u76f8\u5bf9\u4e8e\u7279\u5b9a\u8bed\u8a00\u6a21\u578b\u7684\u6e29\u5ea6\u53c2\u6570", "motivation": "\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u65f6\u4f7f\u7528\u6e29\u5ea6\u53c2\u6570\u6765\u63a7\u5236\u751f\u6210\u6587\u672c\u7684\u968f\u673a\u6027\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u4f30\u8ba1\u5df2\u751f\u6210\u6587\u672c\u7684\u6e29\u5ea6\uff0c\u7279\u522b\u662f\u4eba\u7c7b\u5199\u4f5c\u7684\u6587\u672c", "method": "\u4f7f\u7528\u6700\u5927\u4f3c\u7136\u65b9\u6cd5\u4f30\u8ba1\u6587\u672c\u7684\u6e29\u5ea6\u53c2\u6570\uff0c\u63d0\u51fa\u4e00\u4e2a\u8bc4\u4f30\u4efb\u4f55\u6587\u672c\u76f8\u5bf9\u4e8e\u7ed9\u5b9a\u8bed\u8a00\u6a21\u578b\u6e29\u5ea6\u7684\u7a0b\u5e8f", "result": "\u8bc4\u4f30\u4e86\u591a\u79cd\u4e2d\u5c0f\u578bLLM\u7684\u6e29\u5ea6\u4f30\u8ba1\u80fd\u529b\uff0c\u53d1\u73b0Qwen3 14B\u8868\u73b0\u6700\u4f73\uff0c\u5e76\u7528\u5176\u4f30\u8ba1\u4e86\u6d41\u884c\u8bed\u6599\u5e93\u7684\u6e29\u5ea6", "conclusion": "\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u79cd\u901a\u7528\u7684\u6587\u672c\u6e29\u5ea6\u4f30\u8ba1\u65b9\u6cd5\uff0c\u53ef\u7528\u4e8e\u5206\u6790\u4e0d\u540c\u6587\u672c\u76f8\u5bf9\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u968f\u673a\u6027\u7279\u5f81"}}
{"id": "2601.01417", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01417", "abs": "https://arxiv.org/abs/2601.01417", "authors": ["Itay Safran"], "title": "A Depth Hierarchy for Computing the Maximum in ReLU Networks via Extremal Graph Theory", "comment": null, "summary": "We consider the problem of exact computation of the maximum function over $d$ real inputs using ReLU neural networks. We prove a depth hierarchy, wherein width $\u03a9\\big(d^{1+\\frac{1}{2^{k-2}-1}}\\big)$ is necessary to represent the maximum for any depth $3\\le k\\le \\log_2(\\log_2(d))$. This is the first unconditional super-linear lower bound for this fundamental operator at depths $k\\ge3$, and it holds even if the depth scales with $d$. Our proof technique is based on a combinatorial argument and associates the non-differentiable ridges of the maximum with cliques in a graph induced by the first hidden layer of the computing network, utilizing Tur\u00e1n's theorem from extremal graph theory to show that a sufficiently narrow network cannot capture the non-linearities of the maximum. This suggests that despite its simple nature, the maximum function possesses an inherent complexity that stems from the geometric structure of its non-differentiable hyperplanes, and provides a novel approach for proving lower bounds for deep neural networks.", "AI": {"tldr": "\u672c\u6587\u8bc1\u660e\u4e86\u8ba1\u7b97d\u4e2a\u5b9e\u6570\u6700\u5927\u503c\u7684ReLU\u795e\u7ecf\u7f51\u7edc\u5b58\u5728\u6df1\u5ea6\u5c42\u6b21\u7ed3\u6784\uff1a\u5bf9\u4e8e\u6df1\u5ea63\u2264k\u2264log\u2082(log\u2082(d))\uff0c\u9700\u8981\u5bbd\u5ea6\u03a9(d^{1+1/(2^{k-2}-1)})\u624d\u80fd\u7cbe\u786e\u8868\u793a\u6700\u5927\u503c\u51fd\u6570\uff0c\u8fd9\u662f\u9996\u4e2a\u9488\u5bf9\u8be5\u57fa\u672c\u7b97\u5b50\u5728\u6df1\u5ea6k\u22653\u65f6\u7684\u65e0\u6761\u4ef6\u8d85\u7ebf\u6027\u4e0b\u754c\u3002", "motivation": "\u6700\u5927\u503c\u51fd\u6570\u662f\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u57fa\u672c\u7b97\u5b50\uff0c\u4f46\u5176\u7cbe\u786e\u8ba1\u7b97\u6240\u9700\u7684\u7f51\u7edc\u89c4\u6a21\u5c1a\u4e0d\u6e05\u695a\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6d45\u5c42\u7f51\u7edc\uff0c\u5bf9\u4e8e\u6df1\u5c42\u7f51\u7edc\u7684\u8868\u793a\u80fd\u529b\u7f3a\u4e4f\u7406\u8bba\u7406\u89e3\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u6700\u5927\u503c\u51fd\u6570\u5728\u6df1\u5c42ReLU\u7f51\u7edc\u4e2d\u7684\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u63ed\u793a\u5176\u56fa\u6709\u7684\u51e0\u4f55\u7ed3\u6784\u590d\u6742\u6027\u3002", "method": "\u91c7\u7528\u7ec4\u5408\u8bba\u8bc1\u65b9\u6cd5\uff0c\u5c06\u6700\u5927\u503c\u51fd\u6570\u7684\u4e0d\u53ef\u5fae\u5206\u810a\u7ebf\u4e0e\u8ba1\u7b97\u7f51\u7edc\u7b2c\u4e00\u9690\u85cf\u5c42\u8bf1\u5bfc\u7684\u56fe\u4e2d\u7684\u56e2\u76f8\u5173\u8054\u3002\u5229\u7528\u6781\u503c\u56fe\u8bba\u4e2d\u7684Tur\u00e1n\u5b9a\u7406\uff0c\u8bc1\u660e\u8db3\u591f\u7a84\u7684\u7f51\u7edc\u65e0\u6cd5\u6355\u6349\u6700\u5927\u503c\u51fd\u6570\u7684\u975e\u7ebf\u6027\u7279\u6027\u3002\u901a\u8fc7\u5206\u6790\u7f51\u7edc\u6df1\u5ea6\u4e0e\u5bbd\u5ea6\u7684\u5173\u7cfb\uff0c\u5efa\u7acb\u6df1\u5ea6\u5c42\u6b21\u7ed3\u6784\u3002", "result": "\u8bc1\u660e\u4e86\u5bf9\u4e8e\u6df1\u5ea63\u2264k\u2264log\u2082(log\u2082(d))\uff0c\u7cbe\u786e\u8ba1\u7b97d\u4e2a\u5b9e\u6570\u6700\u5927\u503c\u9700\u8981ReLU\u795e\u7ecf\u7f51\u7edc\u5bbd\u5ea6\u81f3\u5c11\u4e3a\u03a9(d^{1+1/(2^{k-2}-1)})\u3002\u8fd9\u662f\u9996\u4e2a\u9488\u5bf9\u6700\u5927\u503c\u51fd\u6570\u5728\u6df1\u5ea6k\u22653\u65f6\u7684\u65e0\u6761\u4ef6\u8d85\u7ebf\u6027\u4e0b\u754c\uff0c\u5373\u4f7f\u6df1\u5ea6\u968fd\u7f29\u653e\u4e5f\u6210\u7acb\u3002", "conclusion": "\u6700\u5927\u503c\u51fd\u6570\u867d\u7136\u5f62\u5f0f\u7b80\u5355\uff0c\u4f46\u5176\u51e0\u4f55\u7ed3\u6784\u4e2d\u7684\u4e0d\u53ef\u5fae\u5206\u8d85\u5e73\u9762\u5e26\u6765\u4e86\u56fa\u6709\u7684\u590d\u6742\u6027\u3002\u8be5\u7814\u7a76\u4e3a\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e0b\u754c\u8bc1\u660e\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u6df1\u5ea6\u5c42\u6b21\u7ed3\u6784\u5728\u8ba1\u7b97\u57fa\u672c\u7b97\u5b50\u65f6\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2601.02337", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.02337", "abs": "https://arxiv.org/abs/2601.02337", "authors": ["Berk Atil", "Rebecca J. Passonneau", "Ninareh Mehrabi"], "title": "Robust Persona-Aware Toxicity Detection with Prompt Optimization and Learned Ensembling", "comment": null, "summary": "Toxicity detection is inherently subjective, shaped by the diverse perspectives and social priors of different demographic groups. While ``pluralistic'' modeling as used in economics and the social sciences aims to capture perspective differences across contexts, current Large Language Model (LLM) prompting techniques have different results across different personas and base models. In this work, we conduct a systematic evaluation of persona-aware toxicity detection, showing that no single prompting method, including our proposed automated prompt optimization strategy, uniformly dominates across all model-persona pairs. To exploit complementary errors, we explore ensembling four prompting variants and propose a lightweight meta-ensemble: an SVM over the 4-bit vector of prompt predictions. Our results demonstrate that the proposed SVM ensemble consistently outperforms individual prompting methods and traditional majority-voting techniques, achieving the strongest overall performance across diverse personas. This work provides one of the first systematic comparisons of persona-conditioned prompting for toxicity detection and offers a robust method for pluralistic evaluation in subjective NLP tasks.", "AI": {"tldr": "\u8bba\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u57fa\u4e8e\u4eba\u7269\u89d2\u8272\u7684\u6bd2\u6027\u68c0\u6d4b\u65b9\u6cd5\uff0c\u53d1\u73b0\u6ca1\u6709\u5355\u4e00\u63d0\u793a\u65b9\u6cd5\u5728\u6240\u6709\u6a21\u578b-\u4eba\u7269\u5bf9\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8eSVM\u7684\u8f7b\u91cf\u7ea7\u5143\u96c6\u6210\u65b9\u6cd5\uff0c\u5728\u591a\u6837\u5316\u4eba\u7269\u89d2\u8272\u4e0a\u83b7\u5f97\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u6bd2\u6027\u68c0\u6d4b\u5177\u6709\u4e3b\u89c2\u6027\uff0c\u53d7\u4e0d\u540c\u4eba\u53e3\u7fa4\u4f53\u89c6\u89d2\u548c\u793e\u4f1a\u5148\u9a8c\u5f71\u54cd\u3002\u5f53\u524dLLM\u63d0\u793a\u6280\u672f\u5728\u4e0d\u540c\u4eba\u7269\u89d2\u8272\u548c\u57fa\u7840\u6a21\u578b\u4e0a\u8868\u73b0\u4e0d\u4e00\u81f4\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u4eba\u7269\u89d2\u8272\u611f\u77e5\u7684\u6bd2\u6027\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "1) \u7cfb\u7edf\u8bc4\u4f30\u4eba\u7269\u89d2\u8272\u611f\u77e5\u7684\u6bd2\u6027\u68c0\u6d4b\uff1b2) \u63d0\u51fa\u81ea\u52a8\u63d0\u793a\u4f18\u5316\u7b56\u7565\uff1b3) \u63a2\u7d22\u96c6\u6210\u56db\u79cd\u63d0\u793a\u53d8\u4f53\uff1b4) \u63d0\u51fa\u8f7b\u91cf\u7ea7\u5143\u96c6\u6210\u65b9\u6cd5\uff1a\u57fa\u4e8e4\u4f4d\u5411\u91cf\u9884\u6d4b\u7684SVM\u3002", "result": "\u63d0\u51fa\u7684SVM\u96c6\u6210\u65b9\u6cd5\u4e00\u81f4\u4f18\u4e8e\u4e2a\u4f53\u63d0\u793a\u65b9\u6cd5\u548c\u4f20\u7edf\u591a\u6570\u6295\u7968\u6280\u672f\uff0c\u5728\u591a\u6837\u5316\u4eba\u7269\u89d2\u8272\u4e0a\u83b7\u5f97\u6700\u5f3a\u7684\u6574\u4f53\u6027\u80fd\u3002", "conclusion": "\u8fd9\u662f\u9996\u6b21\u7cfb\u7edf\u6bd4\u8f83\u4eba\u7269\u89d2\u8272\u6761\u4ef6\u63d0\u793a\u7528\u4e8e\u6bd2\u6027\u68c0\u6d4b\u7684\u5de5\u4f5c\uff0c\u4e3a\u4e3b\u89c2NLP\u4efb\u52a1\u4e2d\u7684\u591a\u5143\u5316\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7a33\u5065\u65b9\u6cd5\u3002"}}
{"id": "2601.01424", "categories": ["cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2601.01424", "abs": "https://arxiv.org/abs/2601.01424", "authors": ["Akshay Sasi", "Malavika Pradeep", "Nusaibah Farrukh", "Rahul Venugopal", "Elizabeth Sherly"], "title": "Unveiling the Heart-Brain Connection: An Analysis of ECG in Cognitive Performance", "comment": "6 pages, 6 figures. Code available at https://github.com/AkshaySasi/Unveiling-the-Heart-Brain-Connection-An-Analysis-of-ECG-in-Cognitive-Performance. Presented at AIHC (not published)", "summary": "Understanding the interaction of neural and cardiac systems during cognitive activity is critical to advancing physiological computing. Although EEG has been the gold standard for assessing mental workload, its limited portability restricts its real-world use. Widely available ECG through wearable devices proposes a pragmatic alternative. This research investigates whether ECG signals can reliably reflect cognitive load and serve as proxies for EEG-based indicators. In this work, we present multimodal data acquired from two different paradigms involving working-memory and passive-listening tasks. For each modality, we extracted ECG time-domain HRV metrics and Catch22 descriptors against EEG spectral and Catch22 features, respectively. We propose a cross-modal XGBoost framework to project the ECG features onto EEG-representative cognitive spaces, thereby allowing workload inferences using only ECG. Our results show that ECG-derived projections expressively capture variation in cognitive states and provide good support for accurate classification. Our findings underpin ECG as an interpretable, real-time, wearable solution for everyday cognitive monitoring.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4f7f\u7528\u5fc3\u7535\u56fe\uff08ECG\uff09\u66ff\u4ee3\u8111\u7535\u56fe\uff08EEG\uff09\u76d1\u6d4b\u8ba4\u77e5\u8d1f\u8377\uff0c\u63d0\u51fa\u8de8\u6a21\u6001XGBoost\u6846\u67b6\u5c06ECG\u7279\u5f81\u6620\u5c04\u5230EEG\u8ba4\u77e5\u7a7a\u95f4\uff0c\u5b9e\u73b0\u4ec5\u7528ECG\u8fdb\u884c\u8ba4\u77e5\u72b6\u6001\u5206\u7c7b\u3002", "motivation": "\u867d\u7136EEG\u662f\u8bc4\u4f30\u5fc3\u7406\u8d1f\u8377\u7684\u91d1\u6807\u51c6\uff0c\u4f46\u5176\u4fbf\u643a\u6027\u6709\u9650\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u5e7f\u6cdb\u53ef\u7528\u7684\u53ef\u7a7f\u6234\u8bbe\u5907ECG\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u66ff\u4ee3\u65b9\u6848\uff0c\u7814\u7a76\u63a2\u7d22ECG\u4fe1\u53f7\u662f\u5426\u80fd\u53ef\u9760\u53cd\u6620\u8ba4\u77e5\u8d1f\u8377\u5e76\u4f5c\u4e3aEEG\u6307\u6807\u7684\u4ee3\u7406\u3002", "method": "\u6536\u96c6\u5de5\u4f5c\u8bb0\u5fc6\u548c\u88ab\u52a8\u542c\u529b\u4efb\u52a1\u7684\u591a\u6a21\u6001\u6570\u636e\uff0c\u63d0\u53d6ECG\u65f6\u57dfHRV\u6307\u6807\u548cCatch22\u63cf\u8ff0\u7b26\uff0c\u5bf9\u5e94EEG\u9891\u8c31\u548cCatch22\u7279\u5f81\u3002\u63d0\u51fa\u8de8\u6a21\u6001XGBoost\u6846\u67b6\uff0c\u5c06ECG\u7279\u5f81\u6295\u5f71\u5230EEG\u4ee3\u8868\u7684\u8ba4\u77e5\u7a7a\u95f4\uff0c\u5b9e\u73b0\u4ec5\u7528ECG\u8fdb\u884c\u5de5\u4f5c\u8d1f\u8377\u63a8\u65ad\u3002", "result": "ECG\u884d\u751f\u7684\u6295\u5f71\u80fd\u663e\u8457\u6355\u6349\u8ba4\u77e5\u72b6\u6001\u53d8\u5316\uff0c\u4e3a\u51c6\u786e\u5206\u7c7b\u63d0\u4f9b\u826f\u597d\u652f\u6301\u3002ECG\u7279\u5f81\u80fd\u6709\u6548\u6620\u5c04\u5230EEG\u8ba4\u77e5\u7a7a\u95f4\uff0c\u5b9e\u73b0\u53ef\u9760\u7684\u8ba4\u77e5\u72b6\u6001\u76d1\u6d4b\u3002", "conclusion": "ECG\u53ef\u4f5c\u4e3a\u53ef\u89e3\u91ca\u3001\u5b9e\u65f6\u3001\u53ef\u7a7f\u6234\u7684\u65e5\u5e38\u8ba4\u77e5\u76d1\u6d4b\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u751f\u7406\u8ba1\u7b97\u63d0\u4f9b\u5b9e\u7528\u66ff\u4ee3\u65b9\u6848\uff0c\u514b\u670dEEG\u5728\u4fbf\u643a\u6027\u65b9\u9762\u7684\u9650\u5236\u3002"}}
{"id": "2601.01452", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01452", "abs": "https://arxiv.org/abs/2601.01452", "authors": ["Jian Feng", "Zhihong Huang"], "title": "Bayesian Subspace Gradient Estimation for Zeroth-Order Optimization of Large Language Models", "comment": "19 pages, 1 figures, 4 tables", "summary": "Fine-tuning large language models (LLMs) with zeroth-order (ZO) optimization reduces memory by approximating gradients through function evaluations, but existing methods rely on one-step gradient estimates from random perturbations. We introduce Bayesian Subspace Zeroth-Order optimization (BSZO), a ZO optimizer that applies Kalman filtering to combine finite-difference information across multiple perturbation directions. By treating each finite-difference measurement as a noisy observation, BSZO builds a posterior distribution over the projected gradient and updates it through Bayesian inference, with a residual-based adaptive mechanism to adjust perturbation scales. Theoretical analysis shows that BSZO improves the convergence rate by a factor of $k/\u03b3$ compared to standard ZO methods. Experiments on RoBERTa, Mistral, and OPT models show that BSZO outperforms MeZO, MeZO-Adam, and HiZOO across various tasks, achieving up to 6.67\\% absolute average improvement on OPT-13B while keeping memory usage close to inference-only baselines (1.00$\\times$--1.08$\\times$ of MeZO).", "AI": {"tldr": "BSZO\u662f\u4e00\u79cd\u8d1d\u53f6\u65af\u5b50\u7a7a\u95f4\u96f6\u9636\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5361\u5c14\u66fc\u6ee4\u6ce2\u7ed3\u5408\u591a\u4e2a\u6270\u52a8\u65b9\u5411\u7684\u6709\u9650\u5dee\u5206\u4fe1\u606f\uff0c\u76f8\u6bd4\u4f20\u7edfZO\u65b9\u6cd5\u63d0\u5347\u4e86\u6536\u655b\u901f\u5ea6\uff0c\u5728\u4fdd\u6301\u63a5\u8fd1\u63a8\u7406\u5185\u5b58\u4f7f\u7528\u7684\u540c\u65f6\uff0c\u5728\u591a\u4e2aLLM\u4e0a\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u96f6\u9636\u4f18\u5316\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u968f\u673a\u6270\u52a8\u7684\u4e00\u6b65\u68af\u5ea6\u4f30\u8ba1\uff0c\u5b58\u5728\u6548\u7387\u9650\u5236\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7ed3\u5408\u591a\u4e2a\u6270\u52a8\u65b9\u5411\u4fe1\u606f\u3001\u66f4\u6709\u6548\u5730\u5229\u7528\u6709\u9650\u5dee\u5206\u6d4b\u91cf\u503c\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u8d1d\u53f6\u65af\u5b50\u7a7a\u95f4\u96f6\u9636\u4f18\u5316(BSZO)\uff0c\u5c06\u6bcf\u4e2a\u6709\u9650\u5dee\u5206\u6d4b\u91cf\u89c6\u4e3a\u566a\u58f0\u89c2\u6d4b\uff0c\u901a\u8fc7\u5361\u5c14\u66fc\u6ee4\u6ce2\u6784\u5efa\u6295\u5f71\u68af\u5ea6\u7684\u540e\u9a8c\u5206\u5e03\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8e\u6b8b\u5dee\u7684\u81ea\u9002\u5e94\u673a\u5236\u8c03\u6574\u6270\u52a8\u5c3a\u5ea6\u3002", "result": "\u7406\u8bba\u5206\u6790\u663e\u793aBSZO\u5c06\u6536\u655b\u901f\u5ea6\u63d0\u5347\u4e86k/\u03b3\u500d\u3002\u5728RoBERTa\u3001Mistral\u548cOPT\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cBSZO\u4f18\u4e8eMeZO\u3001MeZO-Adam\u548cHiZOO\uff0c\u5728OPT-13B\u4e0a\u5b9e\u73b0\u4e86\u6700\u9ad86.67%\u7684\u7edd\u5bf9\u5e73\u5747\u63d0\u5347\uff0c\u540c\u65f6\u5185\u5b58\u4f7f\u7528\u4fdd\u6301\u5728\u63a8\u7406\u57fa\u7ebf\u76841.00-1.08\u500d\u3002", "conclusion": "BSZO\u901a\u8fc7\u8d1d\u53f6\u65af\u65b9\u6cd5\u6709\u6548\u7ed3\u5408\u591a\u4e2a\u6270\u52a8\u65b9\u5411\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u96f6\u9636\u4f18\u5316\u7684\u6548\u7387\u548c\u6027\u80fd\uff0c\u4e3a\u5185\u5b58\u53d7\u9650\u73af\u5883\u4e0b\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01465", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01465", "abs": "https://arxiv.org/abs/2601.01465", "authors": ["Ze Peng", "Jian Zhang", "Yisen Wang", "Lei Qi", "Yinghuan Shi", "Yang Gao"], "title": "Leveraging Flatness to Improve Information-Theoretic Generalization Bounds for SGD", "comment": "Published as a conference paper at ICLR 2025", "summary": "Information-theoretic (IT) generalization bounds have been used to study the generalization of learning algorithms. These bounds are intrinsically data- and algorithm-dependent so that one can exploit the properties of data and algorithm to derive tighter bounds. However, we observe that although the flatness bias is crucial for SGD's generalization, these bounds fail to capture the improved generalization under better flatness and are also numerically loose. This is caused by the inadequate leverage of SGD's flatness bias in existing IT bounds. This paper derives a more flatness-leveraging IT bound for the flatness-favoring SGD. The bound indicates the learned models generalize better if the large-variance directions of the final weight covariance have small local curvatures in the loss landscape. Experiments on deep neural networks show our bound not only correctly reflects the better generalization when flatness is improved, but is also numerically much tighter. This is achieved by a flexible technique called \"omniscient trajectory\". When applied to Gradient Descent's minimax excess risk on convex-Lipschitz-Bounded problems, it improves representative IT bounds' $\u03a9(1)$ rates to $O(1/\\sqrt{n})$. It also implies a by-pass of memorization-generalization trade-offs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4fe1\u606f\u8bba\u6cdb\u5316\u754c\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5229\u7528SGD\u7684\u5e73\u5766\u6027\u504f\u597d\uff0c\u5728\u6570\u503c\u4e0a\u66f4\u7d27\u4e14\u80fd\u6b63\u786e\u53cd\u6620\u5e73\u5766\u6027\u6539\u5584\u65f6\u7684\u6cdb\u5316\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u4fe1\u606f\u8bba\u6cdb\u5316\u754c\u867d\u7136\u7406\u8bba\u4e0a\u4f9d\u8d56\u6570\u636e\u548c\u7b97\u6cd5\u7279\u6027\uff0c\u4f46\u672a\u80fd\u6709\u6548\u6355\u6349SGD\u5e73\u5766\u6027\u504f\u597d\u5bf9\u6cdb\u5316\u7684\u6539\u5584\u4f5c\u7528\uff0c\u4e14\u5728\u6570\u503c\u4e0a\u4e0d\u591f\u7d27\u81f4\u3002\u4f5c\u8005\u89c2\u5bdf\u5230\u5e73\u5766\u6027\u504f\u5dee\u5bf9SGD\u6cdb\u5316\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u754c\u672a\u80fd\u5145\u5206\u5229\u7528\u8fd9\u4e00\u7279\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u5145\u5206\u5229\u7528\u5e73\u5766\u6027\u7684\u4fe1\u606f\u8bba\u6cdb\u5316\u754c\uff0c\u5f15\u5165\"\u5168\u77e5\u8f68\u8ff9\"\u6280\u672f\u3002\u8be5\u754c\u8868\u660e\u5f53\u6700\u7ec8\u6743\u91cd\u534f\u65b9\u5dee\u7684\u5927\u65b9\u5dee\u65b9\u5411\u5728\u635f\u5931\u666f\u89c2\u4e2d\u5177\u6709\u5c0f\u5c40\u90e8\u66f2\u7387\u65f6\uff0c\u5b66\u4e60\u6a21\u578b\u6cdb\u5316\u66f4\u597d\u3002", "result": "\u5728\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u65b0\u754c\u4e0d\u4ec5\u6b63\u786e\u53cd\u6620\u4e86\u5e73\u5766\u6027\u6539\u5584\u65f6\u7684\u6cdb\u5316\u63d0\u5347\uff0c\u800c\u4e14\u5728\u6570\u503c\u4e0a\u66f4\u7d27\u3002\u5e94\u7528\u4e8e\u51f8-Lipschitz-\u6709\u754c\u95ee\u9898\u7684\u68af\u5ea6\u4e0b\u964d\u6781\u5c0f\u6781\u5927\u8d85\u989d\u98ce\u9669\u65f6\uff0c\u5c06\u4ee3\u8868\u6027\u4fe1\u606f\u8bba\u754c\u7684\u03a9(1)\u7387\u6539\u8fdb\u4e3aO(1/\u221an)\u3002", "conclusion": "\u901a\u8fc7\"\u5168\u77e5\u8f68\u8ff9\"\u6280\u672f\uff0c\u6210\u529f\u63a8\u5bfc\u51fa\u80fd\u591f\u66f4\u597d\u5229\u7528SGD\u5e73\u5766\u6027\u504f\u597d\u7684\u4fe1\u606f\u8bba\u6cdb\u5316\u754c\uff0c\u8be5\u754c\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u90fd\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\uff0c\u5e76\u6697\u793a\u4e86\u53ef\u4ee5\u7ed5\u8fc7\u8bb0\u5fc6\u5316-\u6cdb\u5316\u6743\u8861\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2601.01473", "categories": ["cs.LG", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2601.01473", "abs": "https://arxiv.org/abs/2601.01473", "authors": ["Myung-Hwan Jang", "Jeong-Min Park", "Yunyong Ko", "Sang-Wook Kim"], "title": "Accelerating Storage-Based Training for Graph Neural Networks", "comment": "10 pages, 12 figures, 2 tables, ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD) 2026", "summary": "Graph neural networks (GNNs) have achieved breakthroughs in various real-world downstream tasks due to their powerful expressiveness. As the scale of real-world graphs has been continuously growing, \\textit{a storage-based approach to GNN training} has been studied, which leverages external storage (e.g., NVMe SSDs) to handle such web-scale graphs on a single machine. Although such storage-based GNN training methods have shown promising potential in large-scale GNN training, we observed that they suffer from a severe bottleneck in data preparation since they overlook a critical challenge: \\textit{how to handle a large number of small storage I/Os}. To address the challenge, in this paper, we propose a novel storage-based GNN training framework, named \\textsf{AGNES}, that employs a method of \\textit{block-wise storage I/O processing} to fully utilize the I/O bandwidth of high-performance storage devices. Moreover, to further enhance the efficiency of each storage I/O, \\textsf{AGNES} employs a simple yet effective strategy, \\textit{hyperbatch-based processing} based on the characteristics of real-world graphs. Comprehensive experiments on five real-world graphs reveal that \\textsf{AGNES} consistently outperforms four state-of-the-art methods, by up to 4.1$\\times$ faster than the best competitor. Our code is available at https://github.com/Bigdasgit/agnes-kdd26.", "AI": {"tldr": "AGNES\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b58\u50a8\u7684GNN\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u5757\u7ea7\u5b58\u50a8I/O\u5904\u7406\u548c\u8d85\u6279\u6b21\u5904\u7406\u6765\u89e3\u51b3\u5927\u89c4\u6a21\u56fe\u6570\u636e\u8bad\u7ec3\u4e2d\u7684I/O\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u5b58\u50a8\u7684GNN\u8bad\u7ec3\u65b9\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u56fe\u6570\u636e\u65f6\u9762\u4e34\u5927\u91cf\u5c0f\u578b\u5b58\u50a8I/O\u64cd\u4f5c\u7684\u74f6\u9888\u95ee\u9898\uff0c\u5bfc\u81f4\u6570\u636e\u51c6\u5907\u6548\u7387\u4f4e\u4e0b\uff0c\u9650\u5236\u4e86\u8bad\u7ec3\u6027\u80fd\u3002", "method": "AGNES\u91c7\u7528\u5757\u7ea7\u5b58\u50a8I/O\u5904\u7406\u6280\u672f\u6765\u5145\u5206\u5229\u7528\u9ad8\u6027\u80fd\u5b58\u50a8\u8bbe\u5907\u7684I/O\u5e26\u5bbd\uff0c\u5e76\u7ed3\u5408\u57fa\u4e8e\u73b0\u5b9e\u56fe\u7279\u5f81\u8bbe\u8ba1\u7684\u8d85\u6279\u6b21\u5904\u7406\u7b56\u7565\uff0c\u8fdb\u4e00\u6b65\u4f18\u5316\u6bcf\u4e2a\u5b58\u50a8I/O\u7684\u6548\u7387\u3002", "result": "\u5728\u4e94\u4e2a\u771f\u5b9e\u4e16\u754c\u56fe\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cAGNES\u59cb\u7ec8\u4f18\u4e8e\u56db\u79cd\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u6bd4\u6700\u4f73\u7ade\u4e89\u5bf9\u624b\u5feb\u8fbe4.1\u500d\u3002", "conclusion": "AGNES\u901a\u8fc7\u521b\u65b0\u7684\u5b58\u50a8I/O\u4f18\u5316\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u56fe\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u4e2d\u7684\u5b58\u50a8\u74f6\u9888\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2601.01475", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01475", "abs": "https://arxiv.org/abs/2601.01475", "authors": ["Ruofeng Yang", "Yongcan Li", "Bo Jiang", "Cheng Chen", "Shuai Li"], "title": "Multi-Subspace Multi-Modal Modeling for Diffusion Models: Estimation, Convergence and Mixture of Experts", "comment": null, "summary": "Recently, diffusion models have achieved a great performance with a small dataset of size $n$ and a fast optimization process. However, the estimation error of diffusion models suffers from the curse of dimensionality $n^{-1/D}$ with the data dimension $D$. Since images are usually a union of low-dimensional manifolds, current works model the data as a union of linear subspaces with Gaussian latent and achieve a $1/\\sqrt{n}$ bound. Though this modeling reflects the multi-manifold property, the Gaussian latent can not capture the multi-modal property of the latent manifold. To bridge this gap, we propose the mixture subspace of low-rank mixture of Gaussian (MoLR-MoG) modeling, which models the target data as a union of $K$ linear subspaces, and each subspace admits a mixture of Gaussian latent ($n_k$ modals with dimension $d_k$). With this modeling, the corresponding score function naturally has a mixture of expert (MoE) structure, captures the multi-modal information, and contains nonlinear property. We first conduct real-world experiments to show that the generation results of MoE-latent MoG NN are much better than MoE-latent Gaussian score. Furthermore, MoE-latent MoG NN achieves a comparable performance with MoE-latent Unet with $10 \\times$ parameters. These results indicate that the MoLR-MoG modeling is reasonable and suitable for real-world data. After that, based on such MoE-latent MoG score, we provide a $R^4\\sqrt{\u03a3_{k=1}^Kn_k}\\sqrt{\u03a3_{k=1}^Kn_kd_k}/\\sqrt{n}$ estimation error, which escapes the curse of dimensionality by using data structure. Finally, we study the optimization process and prove the convergence guarantee under the MoLR-MoG modeling. Combined with these results, under a setting close to real-world data, this work explains why diffusion models only require a small training sample and enjoy a fast optimization process to achieve a great performance.", "AI": {"tldr": "\u63d0\u51faMoLR-MoG\u5efa\u6a21\u65b9\u6cd5\uff0c\u5c06\u6570\u636e\u5efa\u6a21\u4e3aK\u4e2a\u7ebf\u6027\u5b50\u7a7a\u95f4\u7684\u5e76\u96c6\uff0c\u6bcf\u4e2a\u5b50\u7a7a\u95f4\u91c7\u7528\u6df7\u5408\u9ad8\u65af\u9690\u53d8\u91cf\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6269\u6563\u6a21\u578b\u7ef4\u5ea6\u8bc5\u5492\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u751f\u6210\u8d28\u91cf\u548c\u7406\u8bba\u4fdd\u8bc1\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u867d\u7136\u5728\u5c0f\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u4f30\u8ba1\u8bef\u5dee\u53d7\u7ef4\u5ea6\u8bc5\u5492\u5f71\u54cd\uff08n^{-1/D}\uff09\u3002\u867d\u7136\u5df2\u6709\u5de5\u4f5c\u5c06\u6570\u636e\u5efa\u6a21\u4e3a\u9ad8\u65af\u9690\u53d8\u91cf\u7684\u7ebf\u6027\u5b50\u7a7a\u95f4\u5e76\u96c6\uff0c\u4f46\u9ad8\u65af\u9690\u53d8\u91cf\u65e0\u6cd5\u6355\u6349\u9690\u6d41\u5f62\u7684\u591a\u6a21\u6001\u7279\u6027\u3002", "method": "\u63d0\u51faMoLR-MoG\uff08\u4f4e\u79e9\u6df7\u5408\u9ad8\u65af\u6df7\u5408\u5b50\u7a7a\u95f4\uff09\u5efa\u6a21\uff1a\u5c06\u76ee\u6807\u6570\u636e\u5efa\u6a21\u4e3aK\u4e2a\u7ebf\u6027\u5b50\u7a7a\u95f4\u7684\u5e76\u96c6\uff0c\u6bcf\u4e2a\u5b50\u7a7a\u95f4\u91c7\u7528\u6df7\u5408\u9ad8\u65af\u9690\u53d8\u91cf\uff08n_k\u4e2a\u6a21\u6001\uff0c\u7ef4\u5ea6d_k\uff09\u3002\u5bf9\u5e94\u7684\u5f97\u5206\u51fd\u6570\u5177\u6709\u6df7\u5408\u4e13\u5bb6\u7ed3\u6784\uff0c\u80fd\u6355\u6349\u591a\u6a21\u6001\u4fe1\u606f\u548c\u975e\u7ebf\u6027\u7279\u6027\u3002", "result": "1. \u5b9e\u9a8c\u8868\u660eMoE\u9690\u53d8\u91cfMoG\u795e\u7ecf\u7f51\u7edc\u7684\u751f\u6210\u7ed3\u679c\u8fdc\u4f18\u4e8eMoE\u9690\u53d8\u91cf\u9ad8\u65af\u5f97\u5206\uff1b2. MoE\u9690\u53d8\u91cfMoG\u795e\u7ecf\u7f51\u7edc\u4e0e\u53c2\u6570\u591a10\u500d\u7684MoE\u9690\u53d8\u91cfUnet\u6027\u80fd\u76f8\u5f53\uff1b3. \u7406\u8bba\u5206\u6790\u5f97\u5230R^4\u221a(\u03a3n_k)\u221a(\u03a3n_kd_k)/\u221an\u7684\u4f30\u8ba1\u8bef\u5dee\uff0c\u907f\u514d\u4e86\u7ef4\u5ea6\u8bc5\u5492\uff1b4. \u8bc1\u660e\u4e86\u5728MoLR-MoG\u5efa\u6a21\u4e0b\u7684\u4f18\u5316\u6536\u655b\u4fdd\u8bc1\u3002", "conclusion": "MoLR-MoG\u5efa\u6a21\u5408\u7406\u4e14\u9002\u7528\u4e8e\u771f\u5b9e\u4e16\u754c\u6570\u636e\uff0c\u89e3\u91ca\u4e86\u4e3a\u4ec0\u4e48\u6269\u6563\u6a21\u578b\u53ea\u9700\u5c11\u91cf\u8bad\u7ec3\u6837\u672c\u548c\u5feb\u901f\u4f18\u5316\u8fc7\u7a0b\u5c31\u80fd\u83b7\u5f97\u4f18\u5f02\u6027\u80fd\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u7684\u7406\u8bba\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2601.01484", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01484", "abs": "https://arxiv.org/abs/2601.01484", "authors": ["Itai Morad", "Nir Shlezinger", "Yonina C. Eldar"], "title": "SGD-Based Knowledge Distillation with Bayesian Teachers: Theory and Guidelines", "comment": null, "summary": "Knowledge Distillation (KD) is a central paradigm for transferring knowledge from a large teacher network to a typically smaller student model, often by leveraging soft probabilistic outputs. While KD has shown strong empirical success in numerous applications, its theoretical underpinnings remain only partially understood. In this work, we adopt a Bayesian perspective on KD to rigorously analyze the convergence behavior of students trained with Stochastic Gradient Descent (SGD). We study two regimes: $(i)$ when the teacher provides the exact Bayes Class Probabilities (BCPs); and $(ii)$ supervision with noisy approximations of the BCPs. Our analysis shows that learning from BCPs yields variance reduction and removes neighborhood terms in the convergence bounds compared to one-hot supervision. We further characterize how the level of noise affects generalization and accuracy. Motivated by these insights, we advocate the use of Bayesian deep learning models, which typically provide improved estimates of the BCPs, as teachers in KD. Consistent with our analysis, we experimentally demonstrate that students distilled from Bayesian teachers not only achieve higher accuracies (up to +4.27%), but also exhibit more stable convergence (up to 30% less noise), compared to students distilled from deterministic teachers.", "AI": {"tldr": "\u4ece\u8d1d\u53f6\u65af\u89c6\u89d2\u5206\u6790\u77e5\u8bc6\u84b8\u998f\u7684\u7406\u8bba\u57fa\u7840\uff0c\u8bc1\u660e\u4f7f\u7528\u8d1d\u53f6\u65af\u5206\u7c7b\u6982\u7387\u4f5c\u4e3a\u6559\u5e08\u8f93\u51fa\u80fd\u964d\u4f4e\u65b9\u5dee\u3001\u63d0\u5347\u6536\u655b\u7a33\u5b9a\u6027\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u8d1d\u53f6\u65af\u6559\u5e08\u6bd4\u786e\u5b9a\u6027\u6559\u5e08\u6548\u679c\u66f4\u597d", "motivation": "\u77e5\u8bc6\u84b8\u998f\u5728\u7ecf\u9a8c\u4e0a\u5f88\u6210\u529f\uff0c\u4f46\u7406\u8bba\u57fa\u7840\u5c1a\u4e0d\u5b8c\u5584\u3002\u672c\u6587\u65e8\u5728\u4ece\u8d1d\u53f6\u65af\u89d2\u5ea6\u4e3a\u77e5\u8bc6\u84b8\u998f\u63d0\u4f9b\u7406\u8bba\u5206\u6790\uff0c\u7406\u89e3\u5176\u6536\u655b\u884c\u4e3a\u548c\u6cdb\u5316\u7279\u6027", "method": "\u91c7\u7528\u8d1d\u53f6\u65af\u89c6\u89d2\u5206\u6790\u77e5\u8bc6\u84b8\u998f\uff0c\u7814\u7a76\u4e24\u79cd\u76d1\u7763\u6a21\u5f0f\uff1a1\uff09\u6559\u5e08\u63d0\u4f9b\u7cbe\u786e\u7684\u8d1d\u53f6\u65af\u5206\u7c7b\u6982\u7387\uff1b2\uff09\u4f7f\u7528\u566a\u58f0\u8fd1\u4f3c\u7684\u8d1d\u53f6\u65af\u5206\u7c7b\u6982\u7387\u3002\u5206\u6790SGD\u8bad\u7ec3\u7684\u6536\u655b\u884c\u4e3a\uff0c\u5e76\u5b9e\u9a8c\u9a8c\u8bc1\u8d1d\u53f6\u65af\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4f5c\u4e3a\u6559\u5e08\u7684\u4f18\u52bf", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff1a\u5b66\u4e60\u8d1d\u53f6\u65af\u5206\u7c7b\u6982\u7387\u76f8\u6bd4one-hot\u76d1\u7763\u80fd\u964d\u4f4e\u65b9\u5dee\u3001\u79fb\u9664\u6536\u655b\u8fb9\u754c\u4e2d\u7684\u90bb\u57df\u9879\u3002\u566a\u58f0\u6c34\u5e73\u5f71\u54cd\u6cdb\u5316\u548c\u51c6\u786e\u6027\u3002\u5b9e\u9a8c\u8bc1\u660e\uff1a\u4ece\u8d1d\u53f6\u65af\u6559\u5e08\u84b8\u998f\u7684\u5b66\u751f\u51c6\u786e\u7387\u63d0\u5347\u6700\u9ad8\u8fbe4.27%\uff0c\u6536\u655b\u7a33\u5b9a\u6027\u63d0\u5347\uff08\u566a\u58f0\u51cf\u5c11\u8fbe30%\uff09", "conclusion": "\u8d1d\u53f6\u65af\u89c6\u89d2\u4e3a\u77e5\u8bc6\u84b8\u998f\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u4f7f\u7528\u8d1d\u53f6\u65af\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4f5c\u4e3a\u6559\u5e08\u80fd\u63d0\u4f9b\u66f4\u597d\u7684\u8d1d\u53f6\u65af\u5206\u7c7b\u6982\u7387\u4f30\u8ba1\uff0c\u4ece\u800c\u63d0\u5347\u5b66\u751f\u6a21\u578b\u7684\u51c6\u786e\u7387\u548c\u6536\u655b\u7a33\u5b9a\u6027"}}
{"id": "2601.01501", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01501", "abs": "https://arxiv.org/abs/2601.01501", "authors": ["Fan Xu", "Wei Gong", "Hao Wu", "Lilan Peng", "Nan Wang", "Qingsong Wen", "Xian Wu", "Kun Wang", "Xibin Zhao"], "title": "Advanced Global Wildfire Activity Modeling with Hierarchical Graph ODE", "comment": null, "summary": "Wildfires, as an integral component of the Earth system, are governed by a complex interplay of atmospheric, oceanic, and terrestrial processes spanning a vast range of spatiotemporal scales. Modeling their global activity on large timescales is therefore a critical yet challenging task. While deep learning has recently achieved significant breakthroughs in global weather forecasting, its potential for global wildfire behavior prediction remains underexplored. In this work, we reframe this problem and introduce the Hierarchical Graph ODE (HiGO), a novel framework designed to learn the multi-scale, continuous-time dynamics of wildfires. Specifically, we represent the Earth system as a multi-level graph hierarchy and propose an adaptive filtering message passing mechanism for both intra- and inter-level information flow, enabling more effective feature extraction and fusion. Furthermore, we incorporate GNN-parameterized Neural ODE modules at multiple levels to explicitly learn the continuous dynamics inherent to each scale. Through extensive experiments on the SeasFire Cube dataset, we demonstrate that HiGO significantly outperforms state-of-the-art baselines on long-range wildfire forecasting. Moreover, its continuous-time predictions exhibit strong observational consistency, highlighting its potential for real-world applications.", "AI": {"tldr": "HiGO\uff1a\u4e00\u79cd\u7528\u4e8e\u5168\u7403\u91ce\u706b\u884c\u4e3a\u9884\u6d4b\u7684\u591a\u5c3a\u5ea6\u8fde\u7eed\u65f6\u95f4\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u56feODE\u5efa\u6a21\u5730\u7403\u7cfb\u7edf\u52a8\u6001", "motivation": "\u91ce\u706b\u53d7\u5927\u6c14\u3001\u6d77\u6d0b\u548c\u9646\u5730\u8fc7\u7a0b\u590d\u6742\u76f8\u4e92\u4f5c\u7528\u5f71\u54cd\uff0c\u5177\u6709\u591a\u65f6\u7a7a\u5c3a\u5ea6\u7279\u6027\u3002\u867d\u7136\u6df1\u5ea6\u5b66\u4e60\u5728\u5929\u6c14\u9884\u62a5\u65b9\u9762\u53d6\u5f97\u7a81\u7834\uff0c\u4f46\u5728\u5168\u7403\u91ce\u706b\u884c\u4e3a\u9884\u6d4b\u65b9\u9762\u4ecd\u6709\u5f85\u63a2\u7d22\uff0c\u9700\u8981\u80fd\u591f\u6355\u6349\u591a\u5c3a\u5ea6\u8fde\u7eed\u52a8\u6001\u7684\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u56feODE\uff08HiGO\uff09\u6846\u67b6\uff1a1\uff09\u5c06\u5730\u7403\u7cfb\u7edf\u8868\u793a\u4e3a\u591a\u5c42\u56fe\u5c42\u6b21\u7ed3\u6784\uff1b2\uff09\u8bbe\u8ba1\u81ea\u9002\u5e94\u6ee4\u6ce2\u6d88\u606f\u4f20\u9012\u673a\u5236\uff0c\u5b9e\u73b0\u5c42\u5185\u548c\u5c42\u95f4\u4fe1\u606f\u6d41\uff1b3\uff09\u5728\u591a\u4e2a\u5c42\u6b21\u96c6\u6210GNN\u53c2\u6570\u5316\u7684\u795e\u7ecfODE\u6a21\u5757\uff0c\u663e\u5f0f\u5b66\u4e60\u5404\u5c3a\u5ea6\u7684\u8fde\u7eed\u52a8\u6001\u3002", "result": "\u5728SeasFire Cube\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHiGO\u5728\u957f\u671f\u91ce\u706b\u9884\u6d4b\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u65b9\u6cd5\u3002\u5176\u8fde\u7eed\u65f6\u95f4\u9884\u6d4b\u8868\u73b0\u51fa\u5f88\u5f3a\u7684\u89c2\u6d4b\u4e00\u81f4\u6027\uff0c\u663e\u793a\u51fa\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "HiGO\u6210\u529f\u89e3\u51b3\u4e86\u5168\u7403\u91ce\u706b\u884c\u4e3a\u9884\u6d4b\u7684\u6311\u6218\uff0c\u901a\u8fc7\u5206\u5c42\u56feODE\u6846\u67b6\u6709\u6548\u6355\u6349\u4e86\u591a\u5c3a\u5ea6\u8fde\u7eed\u52a8\u6001\uff0c\u4e3a\u91ce\u706b\u9884\u6d4b\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2601.01558", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01558", "abs": "https://arxiv.org/abs/2601.01558", "authors": ["Pengfei Qu", "Wenyu Ouyang", "Chi Zhang", "Yikai Chai", "Shuolong Xu", "Lei Ye", "Yongri Piao", "Miao Zhang", "Huchuan Lu"], "title": "Utilizing Earth Foundation Models to Enhance the Simulation Performance of Hydrological Models with AlphaEarth Embeddings", "comment": "12 pages, 11 figures", "summary": "Predicting river flow in places without streamflow records is challenging because basins respond differently to climate, terrain, vegetation, and soils. Traditional basin attributes describe some of these differences, but they cannot fully represent the complexity of natural environments. This study examines whether AlphaEarth Foundation embeddings, which are learned from large collections of satellite images rather than designed by experts, offer a more informative way to describe basin characteristics. These embeddings summarize patterns in vegetation, land surface properties, and long-term environmental dynamics. We find that models using them achieve higher accuracy when predicting flows in basins not used for training, suggesting that they capture key physical differences more effectively than traditional attributes. We further investigate how selecting appropriate donor basins influences prediction in ungauged regions. Similarity based on the embeddings helps identify basins with comparable environmental and hydrological behavior, improving performance, whereas adding many dissimilar basins can reduce accuracy. The results show that satellite-informed environmental representations can strengthen hydrological forecasting and support the development of models that adapt more easily to different landscapes.", "AI": {"tldr": "\u536b\u661f\u56fe\u50cf\u5b66\u4e60\u7684\u73af\u5883\u5d4c\u5165\u6bd4\u4f20\u7edf\u6d41\u57df\u5c5e\u6027\u66f4\u80fd\u6709\u6548\u9884\u6d4b\u65e0\u6d4b\u7ad9\u6cb3\u6d41\u6d41\u91cf\uff0c\u901a\u8fc7\u9009\u62e9\u76f8\u4f3c\u6d41\u57df\u4f5c\u4e3a\u6570\u636e\u6e90\u53ef\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6", "motivation": "\u4f20\u7edf\u6d41\u57df\u5c5e\u6027\u65e0\u6cd5\u5b8c\u5168\u6355\u6349\u81ea\u7136\u73af\u5883\u7684\u590d\u6742\u6027\uff0c\u9700\u8981\u63a2\u7d22\u4ece\u536b\u661f\u56fe\u50cf\u5b66\u4e60\u7684\u73af\u5883\u5d4c\u5165\u662f\u5426\u80fd\u66f4\u6709\u6548\u5730\u63cf\u8ff0\u6d41\u57df\u7279\u5f81\uff0c\u4ece\u800c\u6539\u8fdb\u65e0\u6d4b\u7ad9\u6cb3\u6d41\u6d41\u91cf\u9884\u6d4b", "method": "\u4f7f\u7528AlphaEarth Foundation\u5d4c\u5165\uff08\u4ece\u5927\u91cf\u536b\u661f\u56fe\u50cf\u5b66\u4e60\u7684\u73af\u5883\u8868\u793a\uff09\u4f5c\u4e3a\u6d41\u57df\u7279\u5f81\uff0c\u4e0e\u4f20\u7edf\u5c5e\u6027\u5bf9\u6bd4\uff0c\u7814\u7a76\u5982\u4f55\u9009\u62e9\u76f8\u4f3c\u6d41\u57df\u4f5c\u4e3a\u6570\u636e\u6e90\u6765\u9884\u6d4b\u65e0\u6d4b\u7ad9\u533a\u57df\u6d41\u91cf", "result": "\u57fa\u4e8e\u536b\u661f\u5d4c\u5165\u7684\u6a21\u578b\u5728\u672a\u53c2\u4e0e\u8bad\u7ec3\u7684\u6d41\u57df\u4e0a\u9884\u6d4b\u7cbe\u5ea6\u66f4\u9ad8\uff0c\u8868\u660e\u5176\u80fd\u66f4\u6709\u6548\u5730\u6355\u6349\u5173\u952e\u7269\u7406\u5dee\u5f02\uff1b\u9009\u62e9\u73af\u5883\u548c\u6c34\u6587\u884c\u4e3a\u76f8\u4f3c\u7684\u6d41\u57df\u4f5c\u4e3a\u6570\u636e\u6e90\u53ef\u63d0\u5347\u6027\u80fd\uff0c\u800c\u6dfb\u52a0\u4e0d\u76f8\u4f3c\u6d41\u57df\u4f1a\u964d\u4f4e\u7cbe\u5ea6", "conclusion": "\u536b\u661f\u4fe1\u606f\u9a71\u52a8\u7684\u73af\u5883\u8868\u5f81\u80fd\u589e\u5f3a\u6c34\u6587\u9884\u6d4b\u80fd\u529b\uff0c\u652f\u6301\u5f00\u53d1\u66f4\u9002\u5e94\u4e0d\u540c\u666f\u89c2\u7684\u6c34\u6587\u6a21\u578b\uff0c\u4e3a\u65e0\u6d4b\u7ad9\u6cb3\u6d41\u6d41\u91cf\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5"}}
{"id": "2601.01580", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01580", "abs": "https://arxiv.org/abs/2601.01580", "authors": ["Zibo Zhao", "Yuanting Zha", "Haipeng Zhang", "Xingcheng Xu"], "title": "The Two-Stage Decision-Sampling Hypothesis: Understanding the Emergence of Self-Reflection in RL-Trained LLMs", "comment": null, "summary": "Self-reflection capabilities emerge in Large Language Models after RL post-training, with multi-turn RL achieving substantial gains over SFT counterparts. Yet the mechanism of how a unified optimization objective gives rise to functionally distinct capabilities of generating solutions and evaluating when to revise them remains opaque. To address this question, we introduce the Gradient Attribution Property to characterize how reward gradients distribute across policy components, formalized through the Two-Stage Decision-Sampling (DS) Hypothesis, which decomposes the policy into sampling ($\u03c0_{sample}$) for generation and decision ($\u03c0_{d}$) for verification. We prove that surrogate rewards exhibit Balanced Gradient Attribution, while SFT and KL penalties exhibit Unbalanced Gradient Attribution, with length-weighting creating asymmetric regularization that constrains $\u03c0_{sample}$ while leaving $\u03c0_{d}$ under-optimized, providing an theoretical explanation of why RL succeeds where SFT fails. We also empirically validate our theoretical predictions on arithmetic reasoning demonstrates that RL's superior generalization stems primarily from improved decision-making ($\u03c0_{d}$) rather than sampling capabilities, providing a first-principles mechanistic explanation for self-correction in thinking models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u68af\u5ea6\u5f52\u56e0\u5c5e\u6027\u7406\u8bba\uff0c\u89e3\u91caRL\u540e\u8bad\u7ec3\u5982\u4f55\u4f7f\u5927\u8bed\u8a00\u6a21\u578b\u83b7\u5f97\u81ea\u6211\u53cd\u601d\u80fd\u529b\uff0c\u8bc1\u660eRL\u7684\u6210\u529f\u6e90\u4e8e\u51b3\u7b56\u80fd\u529b\u7684\u63d0\u5347\u800c\u975e\u91c7\u6837\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1RL\u540e\u8bad\u7ec3\u80fd\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u83b7\u5f97\u81ea\u6211\u53cd\u601d\u80fd\u529b\uff0c\u4f46\u7edf\u4e00\u7684\u4f18\u5316\u76ee\u6807\u5982\u4f55\u4ea7\u751f\u751f\u6210\u89e3\u51b3\u65b9\u6848\u548c\u8bc4\u4f30\u4f55\u65f6\u4fee\u8ba2\u8fd9\u4e24\u79cd\u4e0d\u540c\u529f\u80fd\u80fd\u529b\u7684\u673a\u5236\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u5f15\u5165\u68af\u5ea6\u5f52\u56e0\u5c5e\u6027\u6765\u8868\u5f81\u5956\u52b1\u68af\u5ea6\u5728\u7b56\u7565\u7ec4\u4ef6\u4e2d\u7684\u5206\u5e03\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u51b3\u7b56-\u91c7\u6837\u5047\u8bbe\u5c06\u7b56\u7565\u5206\u89e3\u4e3a\u7528\u4e8e\u751f\u6210\u7684\u91c7\u6837\u7b56\u7565\u548c\u7528\u4e8e\u9a8c\u8bc1\u7684\u51b3\u7b56\u7b56\u7565\uff0c\u7406\u8bba\u5206\u6790\u4e0d\u540c\u8bad\u7ec3\u65b9\u6cd5\u7684\u68af\u5ea6\u5f52\u56e0\u7279\u6027\u3002", "result": "\u8bc1\u660e\u66ff\u4ee3\u5956\u52b1\u8868\u73b0\u51fa\u5e73\u8861\u68af\u5ea6\u5f52\u56e0\uff0c\u800cSFT\u548cKL\u60e9\u7f5a\u8868\u73b0\u51fa\u4e0d\u5e73\u8861\u68af\u5ea6\u5f52\u56e0\uff0c\u957f\u5ea6\u52a0\u6743\u4ea7\u751f\u4e0d\u5bf9\u79f0\u6b63\u5219\u5316\uff0c\u7ea6\u675f\u91c7\u6837\u7b56\u7565\u800c\u51b3\u7b56\u7b56\u7565\u4f18\u5316\u4e0d\u8db3\uff0c\u8fd9\u89e3\u91ca\u4e86RL\u6210\u529f\u800cSFT\u5931\u8d25\u7684\u539f\u56e0\u3002", "conclusion": "RL\u7684\u4f18\u8d8a\u6cdb\u5316\u80fd\u529b\u4e3b\u8981\u6e90\u4e8e\u51b3\u7b56\u80fd\u529b\u7684\u6539\u8fdb\u800c\u975e\u91c7\u6837\u80fd\u529b\uff0c\u4e3a\u601d\u7ef4\u6a21\u578b\u4e2d\u7684\u81ea\u6211\u4fee\u6b63\u63d0\u4f9b\u4e86\u7b2c\u4e00\u6027\u539f\u7406\u7684\u673a\u5236\u89e3\u91ca\u3002"}}
{"id": "2601.01605", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01605", "abs": "https://arxiv.org/abs/2601.01605", "authors": ["Xin Di", "Xinglin Piao", "Fei Wang", "Guodong Jing", "Yong Zhang"], "title": "REE-TTT: Highly Adaptive Radar Echo Extrapolation Based on Test-Time Training", "comment": null, "summary": "Precipitation nowcasting is critically important for meteorological forecasting. Deep learning-based Radar Echo Extrapolation (REE) has become a predominant nowcasting approach, yet it suffers from poor generalization due to its reliance on high-quality local training data and static model parameters, limiting its applicability across diverse regions and extreme events. To overcome this, we propose REE-TTT, a novel model that incorporates an adaptive Test-Time Training (TTT) mechanism. The core of our model lies in the newly designed Spatio-temporal Test-Time Training (ST-TTT) block, which replaces the standard linear projections in TTT layers with task-specific attention mechanisms, enabling robust adaptation to non-stationary meteorological distributions and thereby significantly enhancing the feature representation of precipitation. Experiments under cross-regional extreme precipitation scenarios demonstrate that REE-TTT substantially outperforms state-of-the-art baseline models in prediction accuracy and generalization, exhibiting remarkable adaptability to data distribution shifts.", "AI": {"tldr": "\u63d0\u51faREE-TTT\u6a21\u578b\uff0c\u901a\u8fc7\u65f6\u7a7a\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u673a\u5236\u589e\u5f3a\u96f7\u8fbe\u56de\u6ce2\u5916\u63a8\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u8de8\u533a\u57df\u548c\u6781\u7aef\u5929\u6c14\u4e0b\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\u3002", "motivation": "\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u96f7\u8fbe\u56de\u6ce2\u5916\u63a8\u65b9\u6cd5\u5b58\u5728\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\uff0c\u4f9d\u8d56\u9ad8\u8d28\u91cf\u672c\u5730\u8bad\u7ec3\u6570\u636e\u548c\u9759\u6001\u6a21\u578b\u53c2\u6570\uff0c\u96be\u4ee5\u9002\u5e94\u4e0d\u540c\u533a\u57df\u548c\u6781\u7aef\u5929\u6c14\u4e8b\u4ef6\u3002", "method": "\u63d0\u51faREE-TTT\u6a21\u578b\uff0c\u5f15\u5165\u81ea\u9002\u5e94\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u673a\u5236\uff0c\u6838\u5fc3\u662f\u65f6\u7a7a\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u5757\uff0c\u7528\u4efb\u52a1\u7279\u5b9a\u7684\u6ce8\u610f\u529b\u673a\u5236\u66ff\u4ee3\u6807\u51c6\u7ebf\u6027\u6295\u5f71\uff0c\u589e\u5f3a\u5bf9\u975e\u5e73\u7a33\u6c14\u8c61\u5206\u5e03\u7684\u9002\u5e94\u80fd\u529b\u3002", "result": "\u5728\u8de8\u533a\u57df\u6781\u7aef\u964d\u6c34\u573a\u666f\u4e0b\u7684\u5b9e\u9a8c\u8868\u660e\uff0cREE-TTT\u5728\u9884\u6d4b\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u6a21\u578b\uff0c\u5bf9\u6570\u636e\u5206\u5e03\u504f\u79fb\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u9002\u5e94\u6027\u3002", "conclusion": "REE-TTT\u901a\u8fc7\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u96f7\u8fbe\u56de\u6ce2\u5916\u63a8\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u4e3a\u964d\u6c34\u4e34\u8fd1\u9884\u62a5\u63d0\u4f9b\u4e86\u66f4\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01616", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.01616", "abs": "https://arxiv.org/abs/2601.01616", "authors": ["Md Istiauk Hossain Rifat", "Moin Khan", "Mohammad Zunaed"], "title": "Real Time NILM Based Power Monitoring of Identical Induction Motors Representing Cutting Machines in Textile Industry", "comment": "9 pages, 9 figures", "summary": "The textile industry in Bangladesh is one of the most energy-intensive sectors, yet its monitoring practices remain largely outdated, resulting in inefficient power usage and high operational costs. To address this, we propose a real-time Non-Intrusive Load Monitoring (NILM)-based framework tailored for industrial applications, with a focus on identical motor-driven loads representing textile cutting machines. A hardware setup comprising voltage and current sensors, Arduino Mega and ESP8266 was developed to capture aggregate and individual load data, which was stored and processed on cloud platforms. A new dataset was created from three identical induction motors and auxiliary loads, totaling over 180,000 samples, to evaluate the state-of-the-art MATNILM model under challenging industrial conditions. Results indicate that while aggregate energy estimation was reasonably accurate, per-appliance disaggregation faced difficulties, particularly when multiple identical machines operated simultaneously. Despite these challenges, the integrated system demonstrated practical real-time monitoring with remote accessibility through the Blynk application. This work highlights both the potential and limitations of NILM in industrial contexts, offering insights into future improvements such as higher-frequency data collection, larger-scale datasets and advanced deep learning approaches for handling identical loads.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eNILM\u7684\u5b9e\u65f6\u5de5\u4e1a\u80fd\u8017\u76d1\u6d4b\u6846\u67b6\uff0c\u9488\u5bf9\u5b5f\u52a0\u62c9\u56fd\u7eba\u7ec7\u4e1a\u76f8\u540c\u7535\u673a\u8d1f\u8f7d\uff0c\u5f00\u53d1\u786c\u4ef6\u7cfb\u7edf\u91c7\u96c6\u6570\u636e\uff0c\u8bc4\u4f30MATNILM\u6a21\u578b\u6027\u80fd\uff0c\u53d1\u73b0\u603b\u80fd\u8017\u4f30\u8ba1\u51c6\u786e\u4f46\u76f8\u540c\u8bbe\u5907\u540c\u65f6\u8fd0\u884c\u65f6\u5206\u89e3\u56f0\u96be\u3002", "motivation": "\u5b5f\u52a0\u62c9\u56fd\u7eba\u7ec7\u4e1a\u4f5c\u4e3a\u9ad8\u80fd\u8017\u884c\u4e1a\uff0c\u73b0\u6709\u76d1\u6d4b\u65b9\u6cd5\u843d\u540e\u5bfc\u81f4\u80fd\u6e90\u4f7f\u7528\u6548\u7387\u4f4e\u4e0b\u548c\u8fd0\u8425\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u5f00\u53d1\u5b9e\u65f6\u76d1\u6d4b\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u5305\u542b\u7535\u538b\u7535\u6d41\u4f20\u611f\u5668\u3001Arduino Mega\u548cESP8266\u7684\u786c\u4ef6\u7cfb\u7edf\uff0c\u91c7\u96c6\u603b\u8d1f\u8f7d\u548c\u5355\u4e2a\u8d1f\u8f7d\u6570\u636e\uff1b\u521b\u5efa\u5305\u542b\u4e09\u4e2a\u76f8\u540c\u611f\u5e94\u7535\u673a\u548c\u8f85\u52a9\u8d1f\u8f7d\u7684\u65b0\u6570\u636e\u96c6\uff08\u8d85\u8fc718\u4e07\u4e2a\u6837\u672c\uff09\uff1b\u5728\u4e91\u5e73\u53f0\u4e0a\u5904\u7406\u6570\u636e\uff1b\u8bc4\u4f30MATNILM\u6a21\u578b\u5728\u5de5\u4e1a\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u3002", "result": "\u603b\u80fd\u8017\u4f30\u8ba1\u76f8\u5bf9\u51c6\u786e\uff0c\u4f46\u5355\u4e2a\u8bbe\u5907\u5206\u89e3\u5728\u591a\u4e2a\u76f8\u540c\u673a\u5668\u540c\u65f6\u8fd0\u884c\u65f6\u9762\u4e34\u56f0\u96be\uff1b\u96c6\u6210\u7cfb\u7edf\u901a\u8fc7Blynk\u5e94\u7528\u5b9e\u73b0\u4e86\u5b9e\u7528\u7684\u5b9e\u65f6\u8fdc\u7a0b\u76d1\u6d4b\u3002", "conclusion": "NILM\u5728\u5de5\u4e1a\u5e94\u7528\u4e2d\u65e2\u6709\u6f5c\u529b\u4e5f\u6709\u5c40\u9650\u6027\uff0c\u672a\u6765\u6539\u8fdb\u65b9\u5411\u5305\u62ec\u66f4\u9ad8\u9891\u7387\u6570\u636e\u91c7\u96c6\u3001\u66f4\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4ee5\u53ca\u5904\u7406\u76f8\u540c\u8d1f\u8f7d\u7684\u5148\u8fdb\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002"}}
{"id": "2601.01714", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01714", "abs": "https://arxiv.org/abs/2601.01714", "authors": ["Kareem Ahmed", "Sameer Singh"], "title": "Entropy-Aligned Decoding of LMs for Better Writing and Reasoning", "comment": null, "summary": "Language models (LMs) are trained on billions of tokens in an attempt to recover the true language distribution. Still, vanilla random sampling from LMs yields low quality generations. Decoding algorithms attempt to restrict the LM distribution to a set of high-probability continuations, but rely on greedy heuristics that introduce myopic distortions, yielding sentences that are homogeneous, repetitive and incoherent. In this paper, we introduce EPIC, a hyperparameter-free decoding approach that incorporates the entropy of future trajectories into LM decoding. EPIC explicitly regulates the amount of uncertainty expressed at every step of generation, aligning the sampling distribution's entropy to the aleatoric (data) uncertainty. Through Entropy-Aware Lazy Gumbel-Max sampling, EPIC manages to be exact, while also being efficient, requiring only a sublinear number of entropy evaluations per step. Unlike current baselines, EPIC yields sampling distributions that are empirically well-aligned with the entropy of the underlying data distribution. Across creative writing and summarization tasks, EPIC consistently improves LM-as-judge preference win-rates over widely used decoding strategies. These preference gains are complemented by automatic metrics, showing that EPIC produces more diverse generations and more faithful summaries. We also evaluate EPIC on mathematical reasoning, where it outperforms all baselines.", "AI": {"tldr": "EPIC\u662f\u4e00\u79cd\u65e0\u9700\u8d85\u53c2\u6570\u7684\u89e3\u7801\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u672a\u6765\u8f68\u8ff9\u7684\u71b5\u7eb3\u5165\u8bed\u8a00\u6a21\u578b\u89e3\u7801\uff0c\u8c03\u8282\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u4f7f\u5176\u4e0e\u6570\u636e\u4e0d\u786e\u5b9a\u6027\u5bf9\u9f50\uff0c\u4ece\u800c\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u8bed\u8a00\u6a21\u578b\u89e3\u7801\u7b97\u6cd5\u4f9d\u8d56\u8d2a\u5a6a\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5bfc\u81f4\u751f\u6210\u7ed3\u679c\u540c\u8d28\u5316\u3001\u91cd\u590d\u4e14\u4e0d\u8fde\u8d2f\uff0c\u65e0\u6cd5\u6709\u6548\u6062\u590d\u771f\u5b9e\u8bed\u8a00\u5206\u5e03\u3002", "method": "EPIC\u901a\u8fc7\u71b5\u611f\u77e5\u61d2\u60f0Gumbel-Max\u91c7\u6837\uff0c\u5c06\u672a\u6765\u8f68\u8ff9\u7684\u71b5\u663e\u5f0f\u7eb3\u5165\u89e3\u7801\u8fc7\u7a0b\uff0c\u4f7f\u91c7\u6837\u5206\u5e03\u7684\u71b5\u4e0e\u6570\u636e\u4e0d\u786e\u5b9a\u6027\u5bf9\u9f50\uff0c\u5b9e\u73b0\u7cbe\u786e\u4e14\u9ad8\u6548\u7684\u89e3\u7801\u3002", "result": "\u5728\u521b\u610f\u5199\u4f5c\u548c\u6458\u8981\u4efb\u52a1\u4e2d\uff0cEPIC\u5728LM-as-judge\u504f\u597d\u80dc\u7387\u4e0a\u6301\u7eed\u4f18\u4e8e\u5e7f\u6cdb\u4f7f\u7528\u7684\u89e3\u7801\u7b56\u7565\uff1b\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\uff0cEPIC\u4e5f\u8d85\u8d8a\u4e86\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "EPIC\u901a\u8fc7\u71b5\u611f\u77e5\u89e3\u7801\u6709\u6548\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u751f\u6210\u8d28\u91cf\uff0c\u5728\u591a\u6837\u6027\u3001\u5fe0\u5b9e\u5ea6\u548c\u63a8\u7406\u80fd\u529b\u65b9\u9762\u5747\u6709\u663e\u8457\u6539\u8fdb\u3002"}}
{"id": "2601.01649", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.01649", "abs": "https://arxiv.org/abs/2601.01649", "authors": ["Umesh Vangapally", "Wenhan Wu", "Chen Chen", "Zhishuai Guo"], "title": "Communication-Efficient Federated AUC Maximization with Cyclic Client Participation", "comment": "Accepted to Transactions on Machine Learning Research (TMLR)", "summary": "Federated AUC maximization is a powerful approach for learning from imbalanced data in federated learning (FL). However, existing methods typically assume full client availability, which is rarely practical. In real-world FL systems, clients often participate in a cyclic manner: joining training according to a fixed, repeating schedule. This setting poses unique optimization challenges for the non-decomposable AUC objective. This paper addresses these challenges by developing and analyzing communication-efficient algorithms for federated AUC maximization under cyclic client participation. We investigate two key settings: First, we study AUC maximization with a squared surrogate loss, which reformulates the problem as a nonconvex-strongly-concave minimax optimization. By leveraging the Polyak-\u0141ojasiewicz (PL) condition, we establish a state-of-the-art communication complexity of $\\widetilde{O}(1/\u03b5^{1/2})$ and iteration complexity of $\\widetilde{O}(1/\u03b5)$. Second, we consider general pairwise AUC losses. We establish a communication complexity of $O(1/\u03b5^3)$ and an iteration complexity of $O(1/\u03b5^4)$. Further, under the PL condition, these bounds improve to communication complexity of $\\widetilde{O}(1/\u03b5^{1/2})$ and iteration complexity of $\\widetilde{O}(1/\u03b5)$. Extensive experiments on benchmark tasks in image classification, medical imaging, and fraud detection demonstrate the superior efficiency and effectiveness of our proposed methods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9488\u5bf9\u5faa\u73af\u5ba2\u6237\u7aef\u53c2\u4e0e\u573a\u666f\u7684\u8054\u90a6AUC\u6700\u5927\u5316\u7b97\u6cd5\uff0c\u5728\u5e73\u65b9\u66ff\u4ee3\u635f\u5931\u4e0b\u5b9e\u73b0\u4e86$\\widetilde{O}(1/\u03b5^{1/2})$\u901a\u4fe1\u590d\u6742\u5ea6\u548c$\\widetilde{O}(1/\u03b5)$\u8fed\u4ee3\u590d\u6742\u5ea6\uff0c\u5728\u4e00\u822c\u6210\u5bf9\u635f\u5931\u4e0b\u4e5f\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u8054\u90a6AUC\u6700\u5927\u5316\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u5ba2\u6237\u7aef\u5b8c\u5168\u53ef\u7528\uff0c\u4f46\u5b9e\u9645\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u4e2d\u5ba2\u6237\u7aef\u5f80\u5f80\u6309\u56fa\u5b9a\u5faa\u73af\u65f6\u95f4\u8868\u53c2\u4e0e\u8bad\u7ec3\u3002\u8fd9\u79cd\u5faa\u73af\u53c2\u4e0e\u6a21\u5f0f\u7ed9\u4e0d\u53ef\u5206\u89e3\u7684AUC\u76ee\u6807\u5e26\u6765\u4e86\u72ec\u7279\u7684\u4f18\u5316\u6311\u6218\u3002", "method": "\u7814\u7a76\u4e24\u79cd\u5173\u952e\u8bbe\u7f6e\uff1a1\uff09\u4f7f\u7528\u5e73\u65b9\u66ff\u4ee3\u635f\u5931\u7684AUC\u6700\u5927\u5316\uff0c\u5c06\u5176\u91cd\u65b0\u8868\u8ff0\u4e3a\u975e\u51f8-\u5f3a\u51f9\u6781\u5c0f\u6781\u5927\u4f18\u5316\u95ee\u9898\uff0c\u5229\u7528Polyak-\u0141ojasiewicz\u6761\u4ef6\uff1b2\uff09\u4e00\u822c\u6210\u5bf9AUC\u635f\u5931\u3002\u9488\u5bf9\u5faa\u73af\u5ba2\u6237\u7aef\u53c2\u4e0e\u573a\u666f\u5f00\u53d1\u901a\u4fe1\u9ad8\u6548\u7684\u7b97\u6cd5\u3002", "result": "\u5728\u5e73\u65b9\u66ff\u4ee3\u635f\u5931\u4e0b\u5b9e\u73b0\u4e86$\\widetilde{O}(1/\u03b5^{1/2})$\u901a\u4fe1\u590d\u6742\u5ea6\u548c$\\widetilde{O}(1/\u03b5)$\u8fed\u4ee3\u590d\u6742\u5ea6\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff1b\u5728\u4e00\u822c\u6210\u5bf9\u635f\u5931\u4e0b\u5b9e\u73b0\u4e86$O(1/\u03b5^3)$\u901a\u4fe1\u590d\u6742\u5ea6\u548c$O(1/\u03b5^4)$\u8fed\u4ee3\u590d\u6742\u5ea6\uff0c\u5728PL\u6761\u4ef6\u4e0b\u6539\u8fdb\u4e3a$\\widetilde{O}(1/\u03b5^{1/2})$\u548c$\\widetilde{O}(1/\u03b5)$\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u56fe\u50cf\u5206\u7c7b\u3001\u533b\u5b66\u6210\u50cf\u548c\u6b3a\u8bc8\u68c0\u6d4b\u7b49\u57fa\u51c6\u4efb\u52a1\u4e0a\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6548\u7387\u548c\u6709\u6548\u6027\uff0c\u4e3a\u89e3\u51b3\u5b9e\u9645\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u4e2d\u5ba2\u6237\u7aef\u5faa\u73af\u53c2\u4e0e\u4e0b\u7684AUC\u6700\u5927\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01754", "categories": ["cs.LG", "cs.CC", "cs.CL", "cs.FL"], "pdf": "https://arxiv.org/pdf/2601.01754", "abs": "https://arxiv.org/abs/2601.01754", "authors": ["Selim Jerad", "Anej Svete", "Sophie Hao", "Ryan Cotterell", "William Merrill"], "title": "Context-Free Recognition with Transformers", "comment": null, "summary": "Transformers excel on tasks that process well-formed inputs according to some grammar, such as natural language and code. However, it remains unclear how they can process grammatical syntax. In fact, under standard complexity conjectures, standard transformers cannot recognize context-free languages (CFLs), a canonical formalism to describe syntax, or even regular languages, a subclass of CFLs (Merrill et al., 2022). Merrill & Sabharwal (2024) show that $\\mathcal{O}(\\log n)$ looping layers (w.r.t. input length $n$) allows transformers to recognize regular languages, but the question of context-free recognition remained open. In this work, we show that looped transformers with $\\mathcal{O}(\\log n)$ looping layers and $\\mathcal{O}(n^6)$ padding tokens can recognize all CFLs. However, training and inference with $\\mathcal{O}(n^6)$ padding tokens is potentially impractical. Fortunately, we show that, for natural subclasses such as unambiguous CFLs, the recognition problem on transformers becomes more tractable, requiring $\\mathcal{O}(n^3)$ padding. We empirically validate our results and show that looping helps on a language that provably requires logarithmic depth. Overall, our results shed light on the intricacy of CFL recognition by transformers: While general recognition may require an intractable amount of padding, natural constraints such as unambiguity yield efficient recognition algorithms.", "AI": {"tldr": "\u5faa\u73afTransformer\u901a\u8fc7O(log n)\u5faa\u73af\u5c42\u548cO(n^6)\u586b\u5145token\u53ef\u4ee5\u8bc6\u522b\u6240\u6709\u4e0a\u4e0b\u6587\u65e0\u5173\u8bed\u8a00\uff0c\u4f46\u5b9e\u9645\u4e2d\u53ef\u80fd\u4e0d\u5b9e\u7528\uff1b\u5bf9\u4e8e\u81ea\u7136\u5b50\u7c7b\u5982\u65e0\u6b67\u4e49CFL\uff0c\u53ea\u9700O(n^3)\u586b\u5145\uff0c\u66f4\u9ad8\u6548\u3002", "motivation": "Transformer\u5728\u5904\u7406\u81ea\u7136\u8bed\u8a00\u548c\u4ee3\u7801\u7b49\u7b26\u5408\u8bed\u6cd5\u7684\u8f93\u5165\u65f6\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5904\u7406\u8bed\u6cd5\u7ed3\u6784\u7684\u80fd\u529b\u5c1a\u4e0d\u6e05\u695a\u3002\u5df2\u6709\u7814\u7a76\u8868\u660e\u6807\u51c6Transformer\u65e0\u6cd5\u8bc6\u522b\u4e0a\u4e0b\u6587\u65e0\u5173\u8bed\u8a00\uff08CFL\uff09\uff0c\u751a\u81f3\u65e0\u6cd5\u8bc6\u522b\u5176\u5b50\u7c7b\u6b63\u5219\u8bed\u8a00\u3002\u867d\u7136\u5df2\u6709\u5de5\u4f5c\u8bc1\u660eO(log n)\u5faa\u73af\u5c42\u53ef\u4f7fTransformer\u8bc6\u522b\u6b63\u5219\u8bed\u8a00\uff0c\u4f46CFL\u8bc6\u522b\u95ee\u9898\u4ecd\u672a\u89e3\u51b3\u3002", "method": "\u63d0\u51fa\u5faa\u73afTransformer\u67b6\u6784\uff0c\u4f7f\u7528O(log n)\u5faa\u73af\u5c42\u548cO(n^6)\u586b\u5145token\u6765\u5b9e\u73b0CFL\u8bc6\u522b\u3002\u5bf9\u4e8e\u65e0\u6b67\u4e49CFL\u7b49\u81ea\u7136\u5b50\u7c7b\uff0c\u5c06\u586b\u5145\u9700\u6c42\u964d\u4f4e\u5230O(n^3)\u3002\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8bc1\u660e\u5faa\u73af\u673a\u5236\u5728\u9700\u8981\u5bf9\u6570\u6df1\u5ea6\u7684\u8bed\u8a00\u4e0a\u7684\u6709\u6548\u6027\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u5faa\u73afTransformer\u53ef\u4ee5\u8bc6\u522b\u6240\u6709CFL\uff0c\u4f46\u9700\u8981\u5927\u91cf\u586b\u5145token\uff08O(n^6)\uff09\u3002\u5bf9\u4e8e\u65e0\u6b67\u4e49CFL\uff0c\u586b\u5145\u9700\u6c42\u964d\u81f3O(n^3)\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u5faa\u73af\u673a\u5236\u5728\u9700\u8981\u5bf9\u6570\u6df1\u5ea6\u7684\u8bed\u8a00\u4e0a\u786e\u5b9e\u6709\u6548\u3002", "conclusion": "Transformer\u8bc6\u522bCFL\u5b58\u5728\u590d\u6742\u6027\uff1a\u901a\u7528\u8bc6\u522b\u53ef\u80fd\u9700\u8981\u4e0d\u5207\u5b9e\u9645\u7684\u5927\u91cf\u586b\u5145\uff0c\u4f46\u81ea\u7136\u7ea6\u675f\uff08\u5982\u65e0\u6b67\u4e49\u6027\uff09\u53ef\u5b9e\u73b0\u9ad8\u6548\u8bc6\u522b\u3002\u5faa\u73af\u673a\u5236\u4e3aTransformer\u5904\u7406\u8bed\u6cd5\u7ed3\u6784\u63d0\u4f9b\u4e86\u7406\u8bba\u53ef\u80fd\u6027\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9700\u8981\u8003\u8651\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2601.01653", "categories": ["cs.LG", "cs.AI", "cs.MA", "cs.SI"], "pdf": "https://arxiv.org/pdf/2601.01653", "abs": "https://arxiv.org/abs/2601.01653", "authors": ["Hao Xiang Li", "Yash Shah", "Lorenzo Giusti"], "title": "Learning Resilient Elections with Adversarial GNNs", "comment": null, "summary": "In the face of adverse motives, it is indispensable to achieve a consensus. Elections have been the canonical way by which modern democracy has operated since the 17th century. Nowadays, they regulate markets, provide an engine for modern recommender systems or peer-to-peer networks, and remain the main approach to represent democracy. However, a desirable universal voting rule that satisfies all hypothetical scenarios is still a challenging topic, and the design of these systems is at the forefront of mechanism design research. Automated mechanism design is a promising approach, and recent works have demonstrated that set-invariant architectures are uniquely suited to modelling electoral systems. However, various concerns prevent the direct application to real-world settings, such as robustness to strategic voting. In this paper, we generalise the expressive capability of learned voting rules, and combine improvements in neural network architecture with adversarial training to improve the resilience of voting rules while maximizing social welfare. We evaluate the effectiveness of our methods on both synthetic and real-world datasets. Our method resolves critical limitations of prior work regarding learning voting rules by representing elections using bipartite graphs, and learning such voting rules using graph neural networks. We believe this opens new frontiers for applying machine learning to real-world elections.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u5bf9\u6297\u8bad\u7ec3\u7684\u6295\u7968\u89c4\u5219\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u56fe\u8868\u793a\u9009\u4e3e\u5e76\u63d0\u9ad8\u6297\u7b56\u7565\u6295\u7968\u80fd\u529b", "motivation": "\u4f20\u7edf\u6295\u7968\u89c4\u5219\u96be\u4ee5\u6ee1\u8db3\u6240\u6709\u573a\u666f\u9700\u6c42\uff0c\u73b0\u6709\u5b66\u4e60\u65b9\u6cd5\u5728\u6297\u7b56\u7565\u6295\u7968\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u81ea\u52a8\u5316\u6295\u7968\u673a\u5236\u8bbe\u8ba1", "method": "\u4f7f\u7528\u53cc\u56fe\u8868\u793a\u9009\u4e3e\uff0c\u91c7\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u6295\u7968\u89c4\u5219\uff0c\u7ed3\u5408\u5bf9\u6297\u8bad\u7ec3\u63d0\u9ad8\u6297\u7b56\u7565\u6295\u7968\u80fd\u529b", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u89e3\u51b3\u4e86\u5148\u524d\u5de5\u4f5c\u7684\u5173\u952e\u9650\u5236", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u673a\u5668\u5b66\u4e60\u5e94\u7528\u4e8e\u771f\u5b9e\u4e16\u754c\u9009\u4e3e\u5f00\u8f9f\u4e86\u65b0\u524d\u6cbf\uff0c\u63d0\u9ad8\u4e86\u6295\u7968\u89c4\u5219\u7684\u793e\u4f1a\u798f\u5229\u548c\u9c81\u68d2\u6027"}}
{"id": "2601.01792", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.01792", "abs": "https://arxiv.org/abs/2601.01792", "authors": ["NAVER Cloud HyperCLOVA X Team"], "title": "HyperCLOVA X 8B Omni", "comment": "Technical Report", "summary": "In this report, we present HyperCLOVA X 8B Omni, the first any-to-any omnimodal model in the HyperCLOVA X family that supports text, audio, and vision as both inputs and outputs. By consolidating multimodal understanding and generation into a single model rather than separate modality-specific pipelines, HyperCLOVA X 8B Omni serves as an 8B-scale omni-pathfinding point toward practical any-to-any omni assistants. At a high level, the model unifies modalities through a shared next-token prediction interface over an interleaved multimodal sequence, while vision and audio encoders inject continuous embeddings for fine-grained understanding and grounding. Empirical evaluations demonstrate competitive performance against comparably sized models across diverse input-output combinations spanning text, audio, and vision, in both Korean and English. We anticipate that the open-weight release of HyperCLOVA X 8B Omni will support a wide range of research and deployment scenarios.", "AI": {"tldr": "HyperCLOVA X 8B Omni\u662f\u9996\u4e2a\u652f\u6301\u6587\u672c\u3001\u97f3\u9891\u548c\u89c6\u89c9\u4efb\u610f\u8f93\u5165\u8f93\u51fa\u7684\u5168\u6a21\u6001\u6a21\u578b\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u591a\u6a21\u6001\u5e8f\u5217\u9884\u6d4b\u5b9e\u73b0\u8de8\u6a21\u6001\u7406\u89e3\u4e0e\u751f\u6210\u3002", "motivation": "\u6784\u5efa\u5b9e\u7528\u7684\u4efb\u610f\u5230\u4efb\u610f\u5168\u6a21\u6001\u52a9\u624b\uff0c\u907f\u514d\u4f7f\u7528\u5206\u79bb\u7684\u6a21\u6001\u7279\u5b9a\u7ba1\u9053\uff0c\u800c\u662f\u5c06\u591a\u6a21\u6001\u7406\u89e3\u548c\u751f\u6210\u6574\u5408\u5230\u5355\u4e00\u6a21\u578b\u4e2d\u3002", "method": "\u901a\u8fc7\u5171\u4eab\u7684\u4e0b\u4e00\u4e2atoken\u9884\u6d4b\u63a5\u53e3\u7edf\u4e00\u6a21\u6001\uff0c\u4f7f\u7528\u89c6\u89c9\u548c\u97f3\u9891\u7f16\u7801\u5668\u6ce8\u5165\u8fde\u7eed\u5d4c\u5165\u4ee5\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7406\u89e3\u548c\u63a5\u5730\uff0c\u5728\u4ea4\u9519\u7684\u591a\u6a21\u6001\u5e8f\u5217\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u97e9\u8bed\u548c\u82f1\u8bed\u7684\u6587\u672c\u3001\u97f3\u9891\u548c\u89c6\u89c9\u7684\u591a\u79cd\u8f93\u5165\u8f93\u51fa\u7ec4\u5408\u4e0a\uff0c\u4e0e\u7c7b\u4f3c\u89c4\u6a21\u6a21\u578b\u76f8\u6bd4\u8868\u73b0\u51fa\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "conclusion": "HyperCLOVA X 8B Omni\u4f5c\u4e3a8B\u89c4\u6a21\u7684\u5168\u6a21\u6001\u63a2\u7d22\u70b9\uff0c\u5176\u5f00\u653e\u6743\u91cd\u53d1\u5e03\u5c06\u652f\u6301\u5e7f\u6cdb\u7684\u7814\u7a76\u548c\u90e8\u7f72\u573a\u666f\u3002"}}
{"id": "2601.01663", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01663", "abs": "https://arxiv.org/abs/2601.01663", "authors": ["He Sun", "Jiwoong Shin", "Ravi Dhar"], "title": "Length-Aware Adversarial Training for Variable-Length Trajectories: Digital Twins for Mall Shopper Paths", "comment": null, "summary": "We study generative modeling of \\emph{variable-length trajectories} -- sequences of visited locations/items with associated timestamps -- for downstream simulation and counterfactual analysis. A recurring practical issue is that standard mini-batch training can be unstable when trajectory lengths are highly heterogeneous, which in turn degrades \\emph{distribution matching} for trajectory-derived statistics. We propose \\textbf{length-aware sampling (LAS)}, a simple batching strategy that groups trajectories by length and samples batches from a single length bucket, reducing within-batch length heterogeneity (and making updates more consistent) without changing the model class. We integrate LAS into a conditional trajectory GAN with auxiliary time-alignment losses and provide (i) a distribution-level guarantee for derived variables under mild boundedness assumptions, and (ii) an IPM/Wasserstein mechanism explaining why LAS improves distribution matching by removing length-only shortcut critics and targeting within-bucket discrepancies. Empirically, LAS consistently improves matching of derived-variable distributions on a multi-mall dataset of shopper trajectories and on diverse public sequence datasets (GPS, education, e-commerce, and movies), outperforming random sampling across dataset-specific metrics.", "AI": {"tldr": "\u63d0\u51fa\u957f\u5ea6\u611f\u77e5\u91c7\u6837(LAS)\u65b9\u6cd5\uff0c\u901a\u8fc7\u6309\u957f\u5ea6\u5206\u7ec4\u8f68\u8ff9\u51cf\u5c11\u6279\u6b21\u5185\u957f\u5ea6\u5f02\u8d28\u6027\uff0c\u6539\u5584\u751f\u6210\u6a21\u578b\u5bf9\u8f68\u8ff9\u884d\u751f\u7edf\u8ba1\u91cf\u7684\u5206\u5e03\u5339\u914d", "motivation": "\u8f68\u8ff9\u751f\u6210\u5efa\u6a21\u4e2d\uff0c\u6807\u51c6\u5c0f\u6279\u91cf\u8bad\u7ec3\u5728\u8f68\u8ff9\u957f\u5ea6\u9ad8\u5ea6\u5f02\u8d28\u65f6\u4e0d\u7a33\u5b9a\uff0c\u5bfc\u81f4\u8f68\u8ff9\u884d\u751f\u7edf\u8ba1\u91cf\u7684\u5206\u5e03\u5339\u914d\u6548\u679c\u4e0b\u964d", "method": "\u63d0\u51fa\u957f\u5ea6\u611f\u77e5\u91c7\u6837(LAS)\uff1a\u6309\u8f68\u8ff9\u957f\u5ea6\u5206\u7ec4\uff0c\u4ece\u5355\u4e00\u957f\u5ea6\u6876\u4e2d\u91c7\u6837\u6279\u6b21\uff0c\u51cf\u5c11\u6279\u6b21\u5185\u957f\u5ea6\u5f02\u8d28\u6027\uff1b\u7ed3\u5408\u6761\u4ef6\u8f68\u8ff9GAN\u548c\u8f85\u52a9\u65f6\u95f4\u5bf9\u9f50\u635f\u5931", "result": "LAS\u5728\u591a\u5546\u573a\u8d2d\u7269\u8005\u8f68\u8ff9\u6570\u636e\u96c6\u548c\u591a\u79cd\u516c\u5171\u5e8f\u5217\u6570\u636e\u96c6(GPS\u3001\u6559\u80b2\u3001\u7535\u5546\u3001\u7535\u5f71)\u4e0a\u4e00\u81f4\u6539\u5584\u884d\u751f\u53d8\u91cf\u5206\u5e03\u7684\u5339\u914d\uff0c\u4f18\u4e8e\u968f\u673a\u91c7\u6837", "conclusion": "LAS\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u6279\u5904\u7406\u7b56\u7565\uff0c\u901a\u8fc7\u51cf\u5c11\u6279\u6b21\u5185\u957f\u5ea6\u5f02\u8d28\u6027\u6539\u5584\u8f68\u8ff9\u751f\u6210\u6a21\u578b\u7684\u5206\u5e03\u5339\u914d\u6027\u80fd\uff0c\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u8bc1\u4f18\u52bf"}}
{"id": "2601.02031", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.02031", "abs": "https://arxiv.org/abs/2601.02031", "authors": ["Felix Stollenwerk", "Anna Lokrantz", "Niclas Hertzberg"], "title": "Output Embedding Centering for Stable LLM Pretraining", "comment": "11 pages, 5 figures", "summary": "Pretraining of large language models is not only expensive but also prone to certain training instabilities. A specific instability that often occurs for large learning rates at the end of training is output logit divergence. The most widely used mitigation strategy, z-loss, merely addresses the symptoms rather than the underlying cause of the problem. In this paper, we analyze the instability from the perspective of the output embeddings' geometry and identify its cause. Based on this, we propose output embedding centering (OEC) as a new mitigation strategy, and prove that it suppresses output logit divergence. OEC can be implemented in two different ways, as a deterministic operation called \u03bc-centering, or a regularization method called \u03bc-loss. Our experiments show that both variants outperform z-loss in terms of training stability and learning rate sensitivity. In particular, they ensure that training converges even for large learning rates when z-loss fails. Furthermore, we find that \u03bc-loss is significantly less sensitive to regularization hyperparameter tuning than z-loss.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u8f93\u51fa\u5d4c\u5165\u4e2d\u5fc3\u5316\uff08OEC\uff09\u4f5c\u4e3a\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u4e2d\u8f93\u51falogit\u53d1\u6563\u95ee\u9898\u7684\u65b0\u65b9\u6cd5\uff0c\u4f18\u4e8e\u73b0\u6709\u7684z-loss\u65b9\u6cd5", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u4e0d\u4ec5\u6602\u8d35\uff0c\u800c\u4e14\u5bb9\u6613\u51fa\u73b0\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u3002\u5728\u8bad\u7ec3\u540e\u671f\u4f7f\u7528\u8f83\u5927\u5b66\u4e60\u7387\u65f6\u7ecf\u5e38\u51fa\u73b0\u8f93\u51falogit\u53d1\u6563\u95ee\u9898\u3002\u73b0\u6709\u7684z-loss\u65b9\u6cd5\u53ea\u662f\u6cbb\u6807\u4e0d\u6cbb\u672c\uff0c\u9700\u8981\u4ece\u6839\u672c\u4e0a\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u4ece\u8f93\u51fa\u5d4c\u5165\u51e0\u4f55\u89d2\u5ea6\u5206\u6790\u4e0d\u7a33\u5b9a\u6027\u539f\u56e0\uff0c\u63d0\u51fa\u8f93\u51fa\u5d4c\u5165\u4e2d\u5fc3\u5316\uff08OEC\uff09\u4f5c\u4e3a\u65b0\u7684\u7f13\u89e3\u7b56\u7565\u3002OEC\u6709\u4e24\u79cd\u5b9e\u73b0\u65b9\u5f0f\uff1a\u786e\u5b9a\u6027\u64cd\u4f5c\u7684\u03bc-centering\u548c\u6b63\u5219\u5316\u65b9\u6cd5\u7684\u03bc-loss\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u4e24\u79cdOEC\u53d8\u4f53\u5728\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u5b66\u4e60\u7387\u654f\u611f\u6027\u65b9\u9762\u90fd\u4f18\u4e8ez-loss\u3002\u7279\u522b\u662f\u5728z-loss\u5931\u8d25\u7684\u5927\u5b66\u4e60\u7387\u60c5\u51b5\u4e0b\uff0cOEC\u65b9\u6cd5\u4ecd\u80fd\u786e\u4fdd\u8bad\u7ec3\u6536\u655b\u3002\u03bc-loss\u5bf9\u6b63\u5219\u5316\u8d85\u53c2\u6570\u8c03\u4f18\u7684\u654f\u611f\u6027\u663e\u8457\u4f4e\u4e8ez-loss\u3002", "conclusion": "\u8f93\u51fa\u5d4c\u5165\u4e2d\u5fc3\u5316\uff08OEC\uff09\u662f\u89e3\u51b3\u8f93\u51falogit\u53d1\u6563\u95ee\u9898\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u76f8\u6bd4z-loss\u5177\u6709\u66f4\u597d\u7684\u7a33\u5b9a\u6027\u548c\u66f4\u4f4e\u7684\u8d85\u53c2\u6570\u654f\u611f\u6027\uff0c\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01664", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01664", "abs": "https://arxiv.org/abs/2601.01664", "authors": ["Amichai Painsky"], "title": "Who is the Winning Algorithm? Rank Aggregation for Comparative Studies", "comment": null, "summary": "Consider a collection of m competing machine learning algorithms. Given their performance on a benchmark of datasets, we would like to identify the best performing algorithm. Specifically, which algorithm is most likely to ``win'' (rank highest) on a future, unseen dataset. The standard maximum likelihood approach suggests counting the number of wins per each algorithm. In this work, we argue that there is much more information in the complete rankings. That is, the number of times that each algorithm finished second, third and so forth. Yet, it is not entirely clear how to effectively utilize this information for our purpose. In this work we introduce a novel conceptual framework for estimating the win probability for each of the m algorithms, given their complete rankings over a benchmark of datasets. Our proposed framework significantly improves upon currently known methods in synthetic and real-world examples.", "AI": {"tldr": "\u63d0\u51fa\u65b0\u6846\u67b6\uff0c\u5229\u7528\u7b97\u6cd5\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b8c\u6574\u6392\u540d\u4fe1\u606f\uff08\u4e0d\u4ec5\u662f\u83b7\u80dc\u6b21\u6570\uff0c\u8fd8\u5305\u62ec\u7b2c\u4e8c\u3001\u7b2c\u4e09\u7b49\u540d\u6b21\uff09\u6765\u4f30\u8ba1\u6bcf\u4e2a\u7b97\u6cd5\u5728\u672a\u6765\u672a\u89c1\u6570\u636e\u96c6\u4e0a\u83b7\u80dc\u7684\u6982\u7387\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u6709\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u53ea\u7edf\u8ba1\u7b97\u6cd5\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u83b7\u80dc\u6b21\u6570\uff08\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\uff09\uff0c\u4f46\u5ffd\u7565\u4e86\u5b8c\u6574\u7684\u6392\u540d\u4fe1\u606f\uff08\u5982\u7b2c\u4e8c\u3001\u7b2c\u4e09\u7b49\u540d\u6b21\uff09\u3002\u8fd9\u4e9b\u989d\u5916\u4fe1\u606f\u53ef\u80fd\u5bf9\u9884\u6d4b\u7b97\u6cd5\u5728\u672a\u6765\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u6709\u91cd\u8981\u4ef7\u503c\u3002", "method": "\u5f15\u5165\u65b0\u7684\u6982\u5ff5\u6846\u67b6\uff0c\u57fa\u4e8e\u7b97\u6cd5\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b8c\u6574\u6392\u540d\uff08\u4e0d\u4ec5\u662f\u7b2c\u4e00\u540d\uff0c\u8fd8\u5305\u62ec\u6240\u6709\u540d\u6b21\uff09\u6765\u4f30\u8ba1\u6bcf\u4e2a\u7b97\u6cd5\u5728\u672a\u6765\u672a\u89c1\u6570\u636e\u96c6\u4e0a\u83b7\u80dc\u7684\u6982\u7387\u3002\u8be5\u65b9\u6cd5\u5145\u5206\u5229\u7528\u4e86\u6392\u540d\u5206\u5e03\u4e2d\u7684\u5168\u90e8\u4fe1\u606f\u3002", "result": "\u63d0\u51fa\u7684\u6846\u67b6\u5728\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u4e16\u754c\u793a\u4f8b\u4e2d\u90fd\u663e\u8457\u4f18\u4e8e\u5f53\u524d\u5df2\u77e5\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u51c6\u786e\u5730\u9884\u6d4b\u7b97\u6cd5\u5728\u672a\u6765\u6570\u636e\u96c6\u4e0a\u7684\u83b7\u80dc\u6982\u7387\u3002", "conclusion": "\u5229\u7528\u7b97\u6cd5\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b8c\u6574\u6392\u540d\u4fe1\u606f\uff08\u800c\u4e0d\u4ec5\u4ec5\u662f\u83b7\u80dc\u6b21\u6570\uff09\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u9884\u6d4b\u7b97\u6cd5\u672a\u6765\u8868\u73b0\u7684\u80fd\u529b\uff0c\u4e3a\u7b97\u6cd5\u9009\u62e9\u548c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u3002"}}
{"id": "2601.01665", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01665", "abs": "https://arxiv.org/abs/2601.01665", "authors": ["Wei Liu", "Yaoxin Wu", "Yingqian Zhang", "Thomas B\u00e4ck", "Yingjie Fan"], "title": "Adversarial Instance Generation and Robust Training for Neural Combinatorial Optimization with Multiple Objectives", "comment": null, "summary": "Deep reinforcement learning (DRL) has shown great promise in addressing multi-objective combinatorial optimization problems (MOCOPs). Nevertheless, the robustness of these learning-based solvers has remained insufficiently explored, especially across diverse and complex problem distributions. In this paper, we propose a unified robustness-oriented framework for preference-conditioned DRL solvers for MOCOPs. Within this framework, we develop a preference-based adversarial attack to generate hard instances that expose solver weaknesses, and quantify the attack impact by the resulting degradation on Pareto-front quality. We further introduce a defense strategy that integrates hardness-aware preference selection into adversarial training to reduce overfitting to restricted preference regions and improve out-of-distribution performance. The experimental results on multi-objective traveling salesman problem (MOTSP), multi-objective capacitated vehicle routing problem (MOCVRP), and multi-objective knapsack problem (MOKP) verify that our attack method successfully learns hard instances for different solvers. Furthermore, our defense method significantly strengthens the robustness and generalizability of neural solvers, delivering superior performance on hard or out-of-distribution instances.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u9762\u5411\u9c81\u68d2\u6027\u7684\u591a\u76ee\u6807\u7ec4\u5408\u4f18\u5316DRL\u6c42\u89e3\u5668\u6846\u67b6\uff0c\u5305\u542b\u504f\u597d\u5bf9\u6297\u653b\u51fb\u751f\u6210\u56f0\u96be\u5b9e\u4f8b\u548c\u786c\u5ea6\u611f\u77e5\u504f\u597d\u9009\u62e9\u7684\u9632\u5fa1\u7b56\u7565\uff0c\u63d0\u5347\u6c42\u89e3\u5668\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u591a\u76ee\u6807\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u4e0a\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u57fa\u4e8e\u5b66\u4e60\u7684\u6c42\u89e3\u5668\u9c81\u68d2\u6027\u7814\u7a76\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u591a\u6837\u5316\u548c\u590d\u6742\u7684\u95ee\u9898\u5206\u5e03\u4e0a\u3002\u9700\u8981\u63a2\u7d22\u5982\u4f55\u8bc4\u4f30\u548c\u63d0\u5347\u8fd9\u4e9b\u6c42\u89e3\u5668\u5728\u4e0d\u540c\u5206\u5e03\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u9c81\u68d2\u6027\u6846\u67b6\uff1a1) \u504f\u597d\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff0c\u751f\u6210\u66b4\u9732\u6c42\u89e3\u5668\u5f31\u70b9\u7684\u56f0\u96be\u5b9e\u4f8b\uff1b2) \u9632\u5fa1\u7b56\u7565\uff0c\u5c06\u786c\u5ea6\u611f\u77e5\u504f\u597d\u9009\u62e9\u96c6\u6210\u5230\u5bf9\u6297\u8bad\u7ec3\u4e2d\uff0c\u51cf\u5c11\u5bf9\u53d7\u9650\u504f\u597d\u533a\u57df\u7684\u8fc7\u62df\u5408\uff1b3) \u901a\u8fc7\u5e15\u7d2f\u6258\u524d\u6cbf\u8d28\u91cf\u9000\u5316\u91cf\u5316\u653b\u51fb\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u5728\u591a\u76ee\u6807\u65c5\u884c\u5546\u95ee\u9898\u3001\u591a\u76ee\u6807\u5bb9\u91cf\u8f66\u8f86\u8def\u5f84\u95ee\u9898\u548c\u591a\u76ee\u6807\u80cc\u5305\u95ee\u9898\u4e0a\u9a8c\u8bc1\uff1a\u653b\u51fb\u65b9\u6cd5\u6210\u529f\u4e3a\u4e0d\u540c\u6c42\u89e3\u5668\u751f\u6210\u56f0\u96be\u5b9e\u4f8b\uff1b\u9632\u5fa1\u65b9\u6cd5\u663e\u8457\u589e\u5f3a\u795e\u7ecf\u6c42\u89e3\u5668\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\uff0c\u5728\u56f0\u96be\u6216\u5206\u5e03\u5916\u5b9e\u4f8b\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u63d0\u51fa\u7684\u9c81\u68d2\u6027\u6846\u67b6\u6709\u6548\u8bc4\u4f30\u548c\u63d0\u5347\u591a\u76ee\u6807\u7ec4\u5408\u4f18\u5316DRL\u6c42\u89e3\u5668\u7684\u9c81\u68d2\u6027\uff0c\u653b\u51fb\u65b9\u6cd5\u80fd\u66b4\u9732\u6c42\u89e3\u5668\u5f31\u70b9\uff0c\u9632\u5fa1\u65b9\u6cd5\u901a\u8fc7\u786c\u5ea6\u611f\u77e5\u504f\u597d\u9009\u62e9\u6539\u5584\u6cdb\u5316\u6027\u80fd\uff0c\u4e3a\u5b66\u4e60\u578b\u6c42\u89e3\u5668\u7684\u9c81\u68d2\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u7cfb\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2601.02151", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.02151", "abs": "https://arxiv.org/abs/2601.02151", "authors": ["Muxi Diao", "Lele Yang", "Wuxuan Gong", "Yutong Zhang", "Zhonghao Yan", "Yufei Han", "Kongming Liang", "Weiran Xu", "Zhanyu Ma"], "title": "Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting", "comment": null, "summary": "Supervised Fine-Tuning (SFT) is the standard paradigm for domain adaptation, yet it frequently incurs the cost of catastrophic forgetting. In sharp contrast, on-policy Reinforcement Learning (RL) effectively preserves general capabilities. We investigate this discrepancy and identify a fundamental distributional gap: while RL aligns with the model's internal belief, SFT forces the model to fit external supervision. This mismatch often manifests as \"Confident Conflicts\" tokens characterized by low probability but low entropy. In these instances, the model is highly confident in its own prediction but is forced to learn a divergent ground truth, triggering destructive gradient updates. To address this, we propose Entropy-Adaptive Fine-Tuning (EAFT). Unlike methods relying solely on prediction probability, EAFT utilizes token-level entropy as a gating mechanism to distinguish between epistemic uncertainty and knowledge conflict. This allows the model to learn from uncertain samples while suppressing gradients on conflicting data. Extensive experiments on Qwen and GLM series (ranging from 4B to 32B parameters) across mathematical, medical, and agentic domains confirm our hypothesis. EAFT consistently matches the downstream performance of standard SFT while significantly mitigating the degradation of general capabilities.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faEAFT\u65b9\u6cd5\uff0c\u901a\u8fc7\u71b5\u81ea\u9002\u5e94\u95e8\u63a7\u673a\u5236\u533a\u5206\u77e5\u8bc6\u51b2\u7a81\u548c\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u5728\u4fdd\u6301\u4e0b\u6e38\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u7f13\u89e3SFT\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "motivation": "\u76d1\u7763\u5fae\u8c03(SFT)\u5728\u9886\u57df\u9002\u5e94\u4e2d\u5e38\u5bfc\u81f4\u707e\u96be\u6027\u9057\u5fd8\uff0c\u800c\u5f3a\u5316\u5b66\u4e60(RL)\u80fd\u6709\u6548\u4fdd\u6301\u901a\u7528\u80fd\u529b\u3002\u7814\u7a76\u53d1\u73b0\u8fd9\u79cd\u5dee\u5f02\u6e90\u4e8e\u5206\u5e03\u5dee\u8ddd\uff1aRL\u4e0e\u6a21\u578b\u5185\u90e8\u4fe1\u5ff5\u5bf9\u9f50\uff0c\u800cSFT\u5f3a\u5236\u6a21\u578b\u62df\u5408\u5916\u90e8\u76d1\u7763\uff0c\u5bfc\u81f4\"\u81ea\u4fe1\u51b2\u7a81\"\u6807\u8bb0\u5f15\u53d1\u7834\u574f\u6027\u68af\u5ea6\u66f4\u65b0\u3002", "method": "\u63d0\u51fa\u71b5\u81ea\u9002\u5e94\u5fae\u8c03(EAFT)\uff0c\u5229\u7528\u6807\u8bb0\u7ea7\u71b5\u4f5c\u4e3a\u95e8\u63a7\u673a\u5236\u533a\u5206\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u548c\u77e5\u8bc6\u51b2\u7a81\u3002\u6a21\u578b\u4ece\u4e0d\u786e\u5b9a\u6837\u672c\u4e2d\u5b66\u4e60\uff0c\u540c\u65f6\u6291\u5236\u51b2\u7a81\u6570\u636e\u7684\u68af\u5ea6\u66f4\u65b0\uff0c\u907f\u514d\u7834\u574f\u6027\u66f4\u65b0\u3002", "result": "\u5728Qwen\u548cGLM\u7cfb\u5217\u6a21\u578b(4B-32B\u53c2\u6570)\u7684\u6570\u5b66\u3001\u533b\u7597\u548c\u667a\u80fd\u4f53\u9886\u57df\u5b9e\u9a8c\u4e2d\uff0cEAFT\u5728\u4fdd\u6301\u4e0b\u6e38\u6027\u80fd\u4e0e\u6807\u51c6SFT\u76f8\u5f53\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u8f7b\u4e86\u901a\u7528\u80fd\u529b\u7684\u9000\u5316\u3002", "conclusion": "EAFT\u901a\u8fc7\u71b5\u81ea\u9002\u5e94\u95e8\u63a7\u6709\u6548\u89e3\u51b3\u4e86SFT\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u4e3a\u9886\u57df\u9002\u5e94\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u5fae\u8c03\u7b56\u7565\uff0c\u5e73\u8861\u4e86\u4e13\u4e1a\u5316\u548c\u901a\u7528\u80fd\u529b\u4fdd\u6301\u3002"}}
{"id": "2601.01678", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01678", "abs": "https://arxiv.org/abs/2601.01678", "authors": ["Siba Smarak Panigrahi", "Jovana Videnovi\u0107", "Maria Brbi\u0107"], "title": "HeurekaBench: A Benchmarking Framework for AI Co-scientist", "comment": "33 pages, 5 figures, 7 tables. Code available at https://github.com/mlbio-epfl/HeurekaBench", "summary": "LLM-based reasoning models have enabled the development of agentic systems that act as co-scientists, assisting in multi-step scientific analysis. However, evaluating these systems is challenging, as it requires realistic, end-to-end research scenarios that integrate data analysis, interpretation, and the generation of new insights from the experimental data. To address this limitation, we introduce HeurekaBench, a framework to create benchmarks with exploratory, open-ended research questions for experimental datasets. Each such question is grounded in a scientific study and its corresponding code repository, and is created using a semi-automated pipeline that leverages multiple LLMs to extract insights and generate candidate workflows, which are then verified against reported findings. We instantiate the framework in single-cell biology to obtain sc-HeurekaBench benchmark and use it to compare state-of-the-art single-cell agents. We further showcase the benefits of our benchmark for quantitatively analyzing current design choices in agentic systems. We find that the addition of a critic module can improve ill-formed responses for open-source LLM-based agents by up to 22% and close the gap with their closed-source counterparts. Overall, HeurekaBench sets a path toward rigorous, end-to-end evaluation of scientific agents, grounding benchmark construction in real scientific workflows.", "AI": {"tldr": "HeurekaBench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u79d1\u5b66\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u57fa\u51c6\u6846\u67b6\uff0c\u901a\u8fc7\u534a\u81ea\u52a8\u5316\u6d41\u7a0b\u4ece\u771f\u5b9e\u79d1\u5b66\u7814\u7a76\u548c\u4ee3\u7801\u4ed3\u5e93\u4e2d\u521b\u5efa\u5f00\u653e\u5f0f\u7684\u63a2\u7d22\u6027\u7814\u7a76\u95ee\u9898\uff0c\u5e76\u5728\u5355\u7ec6\u80de\u751f\u7269\u5b66\u9886\u57df\u5b9e\u4f8b\u5316\u4e3asc-HeurekaBench\u57fa\u51c6\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u63a8\u7406\u6a21\u578b\u80fd\u591f\u5f00\u53d1\u4f5c\u4e3a\u5408\u4f5c\u79d1\u5b66\u5bb6\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u4f46\u8bc4\u4f30\u8fd9\u4e9b\u7cfb\u7edf\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u771f\u5b9e\u3001\u7aef\u5230\u7aef\u7684\u7814\u7a76\u573a\u666f\u6765\u6574\u5408\u6570\u636e\u5206\u6790\u3001\u89e3\u91ca\u548c\u4ece\u5b9e\u9a8c\u6570\u636e\u4e2d\u751f\u6210\u65b0\u89c1\u89e3\u3002", "method": "\u63d0\u51faHeurekaBench\u6846\u67b6\uff0c\u4f7f\u7528\u534a\u81ea\u52a8\u5316\u6d41\u7a0b\u521b\u5efa\u57fa\u51c6\uff1a\u57fa\u4e8e\u79d1\u5b66\u7814\u7a76\u53ca\u5176\u4ee3\u7801\u4ed3\u5e93\uff0c\u5229\u7528\u591a\u4e2aLLM\u63d0\u53d6\u89c1\u89e3\u5e76\u751f\u6210\u5019\u9009\u5de5\u4f5c\u6d41\u7a0b\uff0c\u7136\u540e\u4e0e\u62a5\u544a\u7ed3\u679c\u8fdb\u884c\u9a8c\u8bc1\u3002\u5728\u5355\u7ec6\u80de\u751f\u7269\u5b66\u9886\u57df\u5b9e\u4f8b\u5316\u4e3asc-HeurekaBench\u57fa\u51c6\u3002", "result": "\u4f7f\u7528sc-HeurekaBench\u57fa\u51c6\u6bd4\u8f83\u6700\u5148\u8fdb\u7684\u5355\u7ec6\u80de\u667a\u80fd\u4f53\uff0c\u53d1\u73b0\u6dfb\u52a0\u6279\u8bc4\u6a21\u5757\u53ef\u4ee5\u5c06\u5f00\u6e90LLM\u667a\u80fd\u4f53\u7684\u4e0d\u826f\u54cd\u5e94\u6539\u5584\u9ad8\u8fbe22%\uff0c\u5e76\u7f29\u5c0f\u4e0e\u95ed\u6e90\u5bf9\u5e94\u7cfb\u7edf\u7684\u5dee\u8ddd\u3002\u5c55\u793a\u4e86\u57fa\u51c6\u5728\u5b9a\u91cf\u5206\u6790\u667a\u80fd\u4f53\u7cfb\u7edf\u8bbe\u8ba1\u9009\u62e9\u65b9\u9762\u7684\u4f18\u52bf\u3002", "conclusion": "HeurekaBench\u4e3a\u79d1\u5b66\u667a\u80fd\u4f53\u7684\u4e25\u683c\u7aef\u5230\u7aef\u8bc4\u4f30\u8bbe\u5b9a\u4e86\u8def\u5f84\uff0c\u5c06\u57fa\u51c6\u6784\u5efa\u57fa\u4e8e\u771f\u5b9e\u7684\u79d1\u5b66\u5de5\u4f5c\u6d41\u7a0b\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u79d1\u5b66\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u8bc4\u4f30\u548c\u53d1\u5c55\u3002"}}
{"id": "2601.01688", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01688", "abs": "https://arxiv.org/abs/2601.01688", "authors": ["Yash Thesia", "Meera Suthar"], "title": "DiMEx: Breaking the Cold Start Barrier in Data-Free Model Extraction via Latent Diffusion Priors", "comment": "8 pages, 3 figures, 4 tables", "summary": "Model stealing attacks pose an existential threat to Machine Learning as a Service (MLaaS), allowing adversaries to replicate proprietary models for a fraction of their training cost. While Data-Free Model Extraction (DFME) has emerged as a stealthy vector, it remains fundamentally constrained by the \"Cold Start\" problem: GAN-based adversaries waste thousands of queries converging from random noise to meaningful data. We propose DiMEx, a framework that weaponizes the rich semantic priors of pre-trained Latent Diffusion Models to bypass this initialization barrier entirely. By employing Random Embedding Bayesian Optimization (REMBO) within the generator's latent space, DiMEx synthesizes high-fidelity queries immediately, achieving 52.1 percent agreement on SVHN with just 2,000 queries - outperforming state-of-the-art GAN baselines by over 16 percent. To counter this highly semantic threat, we introduce the Hybrid Stateful Ensemble (HSE) defense, which identifies the unique \"optimization trajectory\" of latent-space attacks. Our results demonstrate that while DiMEx evades static distribution detectors, HSE exploits this temporal signature to suppress attack success rates to 21.6 percent with negligible latency.", "AI": {"tldr": "DiMEx\u5229\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u8bed\u4e49\u5148\u9a8c\uff0c\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u8d1d\u53f6\u65af\u4f18\u5316\u5b9e\u73b0\u9ad8\u6548\u6a21\u578b\u7a83\u53d6\uff1b\u540c\u65f6\u63d0\u51faHSE\u9632\u5fa1\u65b9\u6cd5\uff0c\u901a\u8fc7\u68c0\u6d4b\u4f18\u5316\u8f68\u8ff9\u6765\u5bf9\u6297\u6b64\u7c7b\u653b\u51fb\u3002", "motivation": "\u9488\u5bf9\u673a\u5668\u5b66\u4e60\u5373\u670d\u52a1\u4e2d\u7684\u6a21\u578b\u7a83\u53d6\u653b\u51fb\uff0c\u7279\u522b\u662f\u65e0\u6570\u636e\u6a21\u578b\u63d0\u53d6\u9762\u4e34\u7684\"\u51b7\u542f\u52a8\"\u95ee\u9898\u2014\u2014\u57fa\u4e8eGAN\u7684\u653b\u51fb\u9700\u8981\u5927\u91cf\u67e5\u8be2\u4ece\u968f\u673a\u566a\u58f0\u6536\u655b\u5230\u6709\u610f\u4e49\u6570\u636e\uff0c\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51faDiMEx\u6846\u67b6\uff1a\u5229\u7528\u9884\u8bad\u7ec3\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u4e30\u5bcc\u8bed\u4e49\u5148\u9a8c\uff0c\u5728\u751f\u6210\u5668\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\u4f7f\u7528\u968f\u673a\u5d4c\u5165\u8d1d\u53f6\u65af\u4f18\u5316\uff0c\u76f4\u63a5\u5408\u6210\u9ad8\u8d28\u91cf\u67e5\u8be2\uff0c\u7ed5\u8fc7\u521d\u59cb\u5316\u969c\u788d\u3002\u540c\u65f6\u63d0\u51faHSE\u9632\u5fa1\uff1a\u6df7\u5408\u72b6\u6001\u96c6\u6210\u9632\u5fa1\uff0c\u901a\u8fc7\u68c0\u6d4b\u6f5c\u5728\u7a7a\u95f4\u653b\u51fb\u7684\u72ec\u7279\"\u4f18\u5316\u8f68\u8ff9\"\u6765\u8bc6\u522b\u653b\u51fb\u3002", "result": "DiMEx\u5728SVHN\u6570\u636e\u96c6\u4e0a\u4ec5\u75282000\u6b21\u67e5\u8be2\u5c31\u8fbe\u523052.1%\u7684\u534f\u8bae\u7387\uff0c\u6bd4\u6700\u5148\u8fdb\u7684GAN\u57fa\u7ebf\u9ad8\u51fa16%\u4ee5\u4e0a\u3002HSE\u9632\u5fa1\u80fd\u591f\u5c06\u653b\u51fb\u6210\u529f\u7387\u538b\u5236\u523021.6%\uff0c\u4e14\u5ef6\u8fdf\u53ef\u5ffd\u7565\u3002", "conclusion": "DiMEx\u5c55\u793a\u4e86\u5229\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u8bed\u4e49\u5148\u9a8c\u8fdb\u884c\u9ad8\u6548\u6a21\u578b\u7a83\u53d6\u7684\u53ef\u884c\u6027\uff0c\u800cHSE\u9632\u5fa1\u901a\u8fc7\u68c0\u6d4b\u653b\u51fb\u7684\u65f6\u95f4\u7279\u5f81\u80fd\u591f\u6709\u6548\u5bf9\u6297\u6b64\u7c7b\u8bed\u4e49\u653b\u51fb\uff0c\u4e3aMLaaS\u5b89\u5168\u63d0\u4f9b\u4e86\u65b0\u7684\u9632\u5fa1\u601d\u8def\u3002"}}
{"id": "2601.01692", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01692", "abs": "https://arxiv.org/abs/2601.01692", "authors": ["Erfan Hajihashemi", "Yanning Shen"], "title": "Enhanced Multi-model Online Conformal Prediction", "comment": null, "summary": "Conformal prediction is a framework for uncertainty quantification that constructs prediction sets for previously unseen data, guaranteeing coverage of the true label with a specified probability. However, the efficiency of these prediction sets, measured by their size, depends on the choice of the underlying learning model. Relying on a single fixed model may lead to suboptimal performance in online environments, as a single model may not consistently perform well across all time steps. To mitigate this, prior work has explored selecting a model from a set of candidates. However, this approach becomes computationally expensive as the number of candidate models increases. Moreover, poorly performing models in the set may also hinder the effectiveness. To tackle this challenge, this work develops a novel multi-model online conformal prediction algorithm that reduces computational complexity and improves prediction efficiency. At each time step, a bipartite graph is generated to identify a subset of effective models, from which a model is selected to construct the prediction set. Experiments demonstrate that our method outperforms existing multi-model conformal prediction techniques in terms of both prediction set size and computational efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u578b\u5728\u7ebf\u5171\u5f62\u9884\u6d4b\u7b97\u6cd5\uff0c\u901a\u8fc7\u4e8c\u5206\u56fe\u9009\u62e9\u6709\u6548\u6a21\u578b\u5b50\u96c6\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u5e76\u63d0\u9ad8\u9884\u6d4b\u6548\u7387", "motivation": "\u4f20\u7edf\u5171\u5f62\u9884\u6d4b\u4f9d\u8d56\u5355\u4e00\u56fa\u5b9a\u6a21\u578b\uff0c\u5728\u5728\u7ebf\u73af\u5883\u4e2d\u53ef\u80fd\u8868\u73b0\u4e0d\u7a33\u5b9a\uff1b\u73b0\u6709\u591a\u6a21\u578b\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u53ef\u80fd\u5305\u542b\u4f4e\u6548\u6a21\u578b\u5f71\u54cd\u6027\u80fd", "method": "\u5f00\u53d1\u591a\u6a21\u578b\u5728\u7ebf\u5171\u5f62\u9884\u6d4b\u7b97\u6cd5\uff0c\u6bcf\u4e2a\u65f6\u95f4\u6b65\u751f\u6210\u4e8c\u5206\u56fe\u8bc6\u522b\u6709\u6548\u6a21\u578b\u5b50\u96c6\uff0c\u4ece\u4e2d\u9009\u62e9\u6a21\u578b\u6784\u5efa\u9884\u6d4b\u96c6", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u9884\u6d4b\u96c6\u5927\u5c0f\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u591a\u6a21\u578b\u5171\u5f62\u9884\u6d4b\u6280\u672f", "conclusion": "\u63d0\u51fa\u7684\u7b97\u6cd5\u901a\u8fc7\u667a\u80fd\u6a21\u578b\u9009\u62e9\u673a\u5236\uff0c\u5728\u4fdd\u8bc1\u8986\u76d6\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u6548\u7387\u548c\u8ba1\u7b97\u6027\u80fd"}}
{"id": "2601.01701", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01701", "abs": "https://arxiv.org/abs/2601.01701", "authors": ["Mohammed Ayalew Belay", "Adil Rasheed", "Pierluigi Salvo Rossi"], "title": "Digital Twin-Driven Communication-Efficient Federated Anomaly Detection for Industrial IoT", "comment": null, "summary": "Anomaly detection is increasingly becoming crucial for maintaining the safety, reliability, and efficiency of industrial systems. Recently, with the advent of digital twins and data-driven decision-making, several statistical and machine-learning methods have been proposed. However, these methods face several challenges, such as dependence on only real sensor datasets, limited labeled data, high false alarm rates, and privacy concerns. To address these problems, we propose a suite of digital twin-integrated federated learning (DTFL) methods that enhance global model performance while preserving data privacy and communication efficiency. Specifically, we present five novel approaches: Digital Twin-Based Meta-Learning (DTML), Federated Parameter Fusion (FPF), Layer-wise Parameter Exchange (LPE), Cyclic Weight Adaptation (CWA), and Digital Twin Knowledge Distillation (DTKD). Each method introduces a unique mechanism to combine synthetic and real-world knowledge, balancing generalization with communication overhead. We conduct an extensive experiment using a publicly available cyber-physical anomaly detection dataset. For a target accuracy of 80%, CWA reaches the target in 33 rounds, FPF in 41 rounds, LPE in 48 rounds, and DTML in 87 rounds, whereas the standard FedAvg baseline and DTKD do not reach the target within 100 rounds. These results highlight substantial communication-efficiency gains (up to 62% fewer rounds than DTML and 31% fewer than LPE) and demonstrate that integrating DT knowledge into FL accelerates convergence to operationally meaningful accuracy thresholds for IIoT anomaly detection.", "AI": {"tldr": "\u63d0\u51fa\u6570\u5b57\u5b6a\u751f\u96c6\u6210\u8054\u90a6\u5b66\u4e60(DTFL)\u65b9\u6cd5\u89e3\u51b3\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u9690\u79c1\u3001\u6570\u636e\u7a00\u7f3a\u548c\u901a\u4fe1\u6548\u7387\u95ee\u9898\uff0c\u5305\u542b\u4e94\u79cd\u65b0\u65b9\u6cd5\uff0c\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u901a\u4fe1\u6548\u7387\u63d0\u5347\u548c\u6536\u655b\u52a0\u901f\u3002", "motivation": "\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u9762\u4e34\u4f9d\u8d56\u771f\u5b9e\u4f20\u611f\u5668\u6570\u636e\u3001\u6807\u6ce8\u6570\u636e\u6709\u9650\u3001\u9ad8\u8bef\u62a5\u7387\u548c\u9690\u79c1\u95ee\u9898\u7b49\u6311\u6218\uff0c\u9700\u8981\u65e2\u80fd\u4fdd\u62a4\u9690\u79c1\u53c8\u80fd\u63d0\u9ad8\u901a\u4fe1\u6548\u7387\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e94\u79cd\u6570\u5b57\u5b6a\u751f\u96c6\u6210\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff1aDTML\uff08\u5143\u5b66\u4e60\uff09\u3001FPF\uff08\u53c2\u6570\u878d\u5408\uff09\u3001LPE\uff08\u5206\u5c42\u53c2\u6570\u4ea4\u6362\uff09\u3001CWA\uff08\u5faa\u73af\u6743\u91cd\u9002\u5e94\uff09\u548cDTKD\uff08\u77e5\u8bc6\u84b8\u998f\uff09\uff0c\u7ed3\u5408\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u77e5\u8bc6\uff0c\u5e73\u8861\u6cdb\u5316\u4e0e\u901a\u4fe1\u5f00\u9500\u3002", "result": "\u5728\u516c\u5f00\u7f51\u7edc\u7269\u7406\u5f02\u5e38\u68c0\u6d4b\u6570\u636e\u96c6\u4e0a\uff0cCWA\u572833\u8f6e\u8fbe\u523080%\u51c6\u786e\u7387\u76ee\u6807\uff0cFPF\u970041\u8f6e\uff0cLPE\u970048\u8f6e\uff0cDTML\u970087\u8f6e\uff0c\u800c\u6807\u51c6FedAvg\u548cDTKD\u5728100\u8f6e\u5185\u672a\u8fbe\u6807\uff0c\u901a\u4fe1\u6548\u7387\u63d0\u5347\u663e\u8457\uff08\u6bd4DTML\u5c1162%\u8f6e\u6570\uff0c\u6bd4LPE\u5c1131%\uff09\u3002", "conclusion": "\u5c06\u6570\u5b57\u5b6a\u751f\u77e5\u8bc6\u96c6\u6210\u5230\u8054\u90a6\u5b66\u4e60\u4e2d\u80fd\u663e\u8457\u52a0\u901f\u6536\u655b\u5230\u5de5\u4e1a\u7269\u8054\u7f51\u5f02\u5e38\u68c0\u6d4b\u7684\u64cd\u4f5c\u6027\u51c6\u786e\u9608\u503c\uff0c\u540c\u65f6\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u5e76\u63d0\u9ad8\u901a\u4fe1\u6548\u7387\u3002"}}
{"id": "2601.01786", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.01786", "abs": "https://arxiv.org/abs/2601.01786", "authors": ["Intae Jeon", "Yujeong Kwon", "Hyungjoon Koo"], "title": "UnPII: Unlearning Personally Identifiable Information with Quantifiable Exposure Risk", "comment": "11 pages, 7 Tables, 6 Figures To appear in the Software Engineering in Practice (SEIP) track of ICSE", "summary": "The ever-increasing adoption of Large Language Models in critical sectors like finance, healthcare, and government raises privacy concerns regarding the handling of sensitive Personally Identifiable Information (PII) during training. In response, regulations such as European Union's General Data Protection Regulation (GDPR) mandate the deletion of PII upon requests, underscoring the need for reliable and cost-effective data removal solutions. Machine unlearning has emerged as a promising direction for selectively forgetting data points. However, existing unlearning techniques typically apply a uniform forgetting strategy that neither accounts for the varying privacy risks posed by different PII attributes nor reflects associated business risks. In this work, we propose UnPII, the first PII-centric unlearning approach that prioritizes forgetting based on the risk of individual or combined PII attributes. To this end, we introduce the PII risk index (PRI), a composite metric that incorporates multiple dimensions of risk factors: identifiability, sensitivity, usability, linkability, permanency, exposability, and compliancy. The PRI enables a nuanced evaluation of privacy risks associated with PII exposures and can be tailored to align with organizational privacy policies. To support realistic assessment, we systematically construct a synthetic PII dataset (e.g., 1,700 PII instances) that simulates realistic exposure scenarios. UnPII seamlessly integrates with established unlearning algorithms, such as Gradient Ascent, Negative Preference Optimization, and Direct Preference Optimization, without modifying their underlying principles. Our experimental results demonstrate that UnPII achieves the improvements of accuracy up to 11.8%, utility up to 6.3%, and generalizability up to 12.4%, respectively, while incurring a modest fine-tuning overhead of 27.5% on average during unlearning.", "AI": {"tldr": "UnPII\uff1a\u9996\u4e2a\u57fa\u4e8ePII\u98ce\u9669\u4f18\u5148\u7ea7\u7684\u9057\u5fd8\u65b9\u6cd5\uff0c\u901a\u8fc7PII\u98ce\u9669\u6307\u6570\uff08PRI\uff09\u8bc4\u4f30\u4e0d\u540cPII\u5c5e\u6027\u7684\u9690\u79c1\u98ce\u9669\uff0c\u5b9e\u73b0\u5dee\u5f02\u5316\u9057\u5fd8\u7b56\u7565\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u9690\u79c1\u5408\u89c4\u6027\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u91d1\u878d\u3001\u533b\u7597\u3001\u653f\u5e9c\u7b49\u5173\u952e\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5904\u7406\u654f\u611f\u4e2a\u4eba\u8eab\u4efd\u4fe1\u606f\uff08PII\uff09\u5f15\u53d1\u9690\u79c1\u62c5\u5fe7\u3002GDPR\u7b49\u6cd5\u89c4\u8981\u6c42\u5e94\u8bf7\u6c42\u5220\u9664PII\uff0c\u9700\u8981\u53ef\u9760\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u6570\u636e\u79fb\u9664\u65b9\u6848\u3002\u73b0\u6709\u9057\u5fd8\u6280\u672f\u901a\u5e38\u91c7\u7528\u7edf\u4e00\u7b56\u7565\uff0c\u672a\u8003\u8651\u4e0d\u540cPII\u5c5e\u6027\u7684\u9690\u79c1\u98ce\u9669\u5dee\u5f02\u548c\u4e1a\u52a1\u98ce\u9669\u3002", "method": "\u63d0\u51faUnPII\u65b9\u6cd5\uff1a1\uff09\u5f15\u5165PII\u98ce\u9669\u6307\u6570\uff08PRI\uff09\uff0c\u7efc\u5408\u8bc4\u4f30\u53ef\u8bc6\u522b\u6027\u3001\u654f\u611f\u6027\u3001\u53ef\u7528\u6027\u3001\u53ef\u94fe\u63a5\u6027\u3001\u6301\u4e45\u6027\u3001\u53ef\u66b4\u9732\u6027\u548c\u5408\u89c4\u6027\u4e03\u4e2a\u7ef4\u5ea6\u7684\u98ce\u9669\uff1b2\uff09\u6784\u5efa\u5408\u6210PII\u6570\u636e\u96c6\uff081700\u4e2a\u5b9e\u4f8b\uff09\u6a21\u62df\u771f\u5b9e\u66b4\u9732\u573a\u666f\uff1b3\uff09\u4e0e\u73b0\u6709\u9057\u5fd8\u7b97\u6cd5\uff08\u68af\u5ea6\u4e0a\u5347\u3001\u8d1f\u504f\u597d\u4f18\u5316\u3001\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff09\u65e0\u7f1d\u96c6\u6210\uff0c\u65e0\u9700\u4fee\u6539\u5176\u5e95\u5c42\u539f\u7406\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cUnPII\u5728\u9057\u5fd8\u8fc7\u7a0b\u4e2d\u5e73\u5747\u4ec5\u589e\u52a027.5%\u7684\u5fae\u8c03\u5f00\u9500\uff0c\u540c\u65f6\u5b9e\u73b0\uff1a\u51c6\u786e\u6027\u63d0\u5347\u6700\u9ad811.8%\uff0c\u6548\u7528\u63d0\u5347\u6700\u9ad86.3%\uff0c\u6cdb\u5316\u80fd\u529b\u63d0\u5347\u6700\u9ad812.4%\u3002", "conclusion": "UnPII\u662f\u9996\u4e2a\u4ee5PII\u4e3a\u4e2d\u5fc3\u7684\u9057\u5fd8\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u98ce\u9669\u7684\u4f18\u5148\u7ea7\u6392\u5e8f\u5b9e\u73b0\u5dee\u5f02\u5316\u9057\u5fd8\uff0c\u6709\u6548\u5e73\u8861\u9690\u79c1\u4fdd\u62a4\u4e0e\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u7ec4\u7ec7\u9690\u79c1\u653f\u7b56\u63d0\u4f9b\u53ef\u5b9a\u5236\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01800", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01800", "abs": "https://arxiv.org/abs/2601.01800", "authors": ["Qi Wei", "Junchao Fan", "Zhao Yang", "Jianhua Wang", "Jingkai Mao", "Xiaolin Chang"], "title": "Sparse Threats, Focused Defense: Criticality-Aware Robust Reinforcement Learning for Safe Autonomous Driving", "comment": null, "summary": "Reinforcement learning (RL) has shown considerable potential in autonomous driving (AD), yet its vulnerability to perturbations remains a critical barrier to real-world deployment. As a primary countermeasure, adversarial training improves policy robustness by training the AD agent in the presence of an adversary that deliberately introduces perturbations. Existing approaches typically model the interaction as a zero-sum game with continuous attacks. However, such designs overlook the inherent asymmetry between the agent and the adversary and then fail to reflect the sparsity of safety-critical risks, rendering the achieved robustness inadequate for practical AD scenarios. To address these limitations, we introduce criticality-aware robust RL (CARRL), a novel adversarial training approach for handling sparse, safety-critical risks in autonomous driving. CARRL consists of two interacting components: a risk exposure adversary (REA) and a risk-targeted robust agent (RTRA). We model the interaction between the REA and RTRA as a general-sum game, allowing the REA to focus on exposing safety-critical failures (e.g., collisions) while the RTRA learns to balance safety with driving efficiency. The REA employs a decoupled optimization mechanism to better identify and exploit sparse safety-critical moments under a constrained budget. However, such focused attacks inevitably result in a scarcity of adversarial data. The RTRA copes with this scarcity by jointly leveraging benign and adversarial experiences via a dual replay buffer and enforces policy consistency under perturbations to stabilize behavior. Experimental results demonstrate that our approach reduces the collision rate by at least 22.66\\% across all cases compared to state-of-the-art baseline methods.", "AI": {"tldr": "CARRL\u662f\u4e00\u79cd\u9488\u5bf9\u81ea\u52a8\u9a7e\u9a76\u7684\u9c81\u68d2\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u98ce\u9669\u66b4\u9732\u5bf9\u624b\u548c\u98ce\u9669\u76ee\u6807\u9c81\u68d2\u4ee3\u7406\u7684\u535a\u5f08\uff0c\u4e13\u95e8\u5904\u7406\u7a00\u758f\u7684\u5b89\u5168\u5173\u952e\u98ce\u9669\uff0c\u663e\u8457\u964d\u4f4e\u78b0\u649e\u7387\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5b58\u5728\u8106\u5f31\u6027\uff0c\u4f20\u7edf\u5bf9\u6297\u8bad\u7ec3\u65b9\u6cd5\u5ffd\u7565\u4e86\u4ee3\u7406\u4e0e\u5bf9\u624b\u4e4b\u95f4\u7684\u4e0d\u5bf9\u79f0\u6027\uff0c\u672a\u80fd\u53cd\u6620\u5b89\u5168\u5173\u952e\u98ce\u9669\u7684\u7a00\u758f\u6027\uff0c\u5bfc\u81f4\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u9c81\u68d2\u6027\u4e0d\u8db3\u3002", "method": "\u63d0\u51faCARRL\u6846\u67b6\uff0c\u5305\u542b\u98ce\u9669\u66b4\u9732\u5bf9\u624b\uff08REA\uff09\u548c\u98ce\u9669\u76ee\u6807\u9c81\u68d2\u4ee3\u7406\uff08RTRA\uff09\u3002\u5c06\u4e24\u8005\u5efa\u6a21\u4e3a\u4e00\u822c\u548c\u535a\u5f08\uff0cREA\u4e13\u6ce8\u4e8e\u66b4\u9732\u5b89\u5168\u5173\u952e\u6545\u969c\uff08\u5982\u78b0\u649e\uff09\uff0cRTRA\u5b66\u4e60\u5e73\u8861\u5b89\u5168\u4e0e\u9a7e\u9a76\u6548\u7387\u3002REA\u91c7\u7528\u89e3\u8026\u4f18\u5316\u673a\u5236\u8bc6\u522b\u7a00\u758f\u5b89\u5168\u5173\u952e\u65f6\u523b\uff0cRTRA\u901a\u8fc7\u53cc\u56de\u653e\u7f13\u51b2\u6c60\u8054\u5408\u5229\u7528\u826f\u6027\u5bf9\u6297\u7ecf\u9a8c\uff0c\u5e76\u5f3a\u5236\u6270\u52a8\u4e0b\u7684\u7b56\u7565\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u6240\u6709\u60c5\u51b5\u4e0b\u81f3\u5c11\u964d\u4f4e\u4e8622.66%\u7684\u78b0\u649e\u7387\u3002", "conclusion": "CARRL\u901a\u8fc7\u4e13\u95e8\u5904\u7406\u7a00\u758f\u5b89\u5168\u5173\u952e\u98ce\u9669\u7684\u5bf9\u6297\u8bad\u7ec3\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u81ea\u52a8\u9a7e\u9a76\u7b56\u7565\u7684\u9c81\u68d2\u6027\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01803", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01803", "abs": "https://arxiv.org/abs/2601.01803", "authors": ["Dennis Jabs", "Aditya Mohan", "Marius Lindauer"], "title": "Moments Matter:Stabilizing Policy Optimization using Return Distributions", "comment": "Workshop paper at RLDM'25", "summary": "Deep Reinforcement Learning (RL) agents often learn policies that achieve the same episodic return yet behave very differently, due to a combination of environmental (random transitions, initial conditions, reward noise) and algorithmic (minibatch selection, exploration noise) factors. In continuous control tasks, even small parameter shifts can produce unstable gaits, complicating both algorithm comparison and real-world transfer. Previous work has shown that such instability arises when policy updates traverse noisy neighborhoods and that the spread of post-update return distribution $R(\u03b8)$, obtained by repeatedly sampling minibatches, updating $\u03b8$, and measuring final returns, is a useful indicator of this noise. Although explicitly constraining the policy to maintain a narrow $R(\u03b8)$ can improve stability, directly estimating $R(\u03b8)$ is computationally expensive in high-dimensional settings. We propose an alternative that takes advantage of environmental stochasticity to mitigate update-induced variability. Specifically, we model state-action return distribution through a distributional critic and then bias the advantage function of PPO using higher-order moments (skewness and kurtosis) of this distribution. By penalizing extreme tail behaviors, our method discourages policies from entering parameter regimes prone to instability. We hypothesize that in environments where post-update critic values align poorly with post-update returns, standard PPO struggles to produce a narrow $R(\u03b8)$. In such cases, our moment-based correction narrows $R(\u03b8)$, improving stability by up to 75% in Walker2D, while preserving comparable evaluation returns.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5206\u5e03\u8bc4\u8bba\u5bb6\u9ad8\u9636\u77e9\uff08\u504f\u5ea6\u548c\u5cf0\u5ea6\uff09\u7684PPO\u6539\u8fdb\u65b9\u6cd5\uff0c\u901a\u8fc7\u60e9\u7f5a\u6781\u7aef\u5c3e\u90e8\u884c\u4e3a\u6765\u51cf\u5c11\u7b56\u7565\u66f4\u65b0\u5f15\u8d77\u7684\u4e0d\u7a33\u5b9a\u6027\uff0c\u5728Walker2D\u73af\u5883\u4e2d\u7a33\u5b9a\u6027\u63d0\u5347\u8fbe75%\u3002", "motivation": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u5373\u4f7f\u83b7\u5f97\u76f8\u540c\u56de\u5408\u5956\u52b1\uff0c\u5176\u884c\u4e3a\u4e5f\u53ef\u80fd\u56e0\u73af\u5883\u548c\u7b97\u6cd5\u56e0\u7d20\u800c\u5927\u4e0d\u76f8\u540c\u3002\u5728\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\uff0c\u5fae\u5c0f\u53c2\u6570\u53d8\u5316\u53ef\u80fd\u5bfc\u81f4\u4e0d\u7a33\u5b9a\u6b65\u6001\uff0c\u8fd9\u65e2\u5f71\u54cd\u7b97\u6cd5\u6bd4\u8f83\u4e5f\u963b\u788d\u5b9e\u9645\u5e94\u7528\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u7ea6\u675f\u540e\u66f4\u65b0\u5956\u52b1\u5206\u5e03R(\u03b8)\u6765\u63d0\u5347\u7a33\u5b9a\u6027\uff0c\u4f46\u76f4\u63a5\u4f30\u8ba1R(\u03b8)\u5728\u9ad8\u7ef4\u8bbe\u7f6e\u4e2d\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u3002", "method": "\u901a\u8fc7\u5206\u5e03\u8bc4\u8bba\u5bb6\u5efa\u6a21\u72b6\u6001-\u52a8\u4f5c\u5956\u52b1\u5206\u5e03\uff0c\u7136\u540e\u5229\u7528\u8be5\u5206\u5e03\u7684\u9ad8\u9636\u77e9\uff08\u504f\u5ea6\u548c\u5cf0\u5ea6\uff09\u5bf9PPO\u7684\u4f18\u52bf\u51fd\u6570\u8fdb\u884c\u504f\u7f6e\u4fee\u6b63\u3002\u901a\u8fc7\u60e9\u7f5a\u6781\u7aef\u5c3e\u90e8\u884c\u4e3a\uff0c\u963b\u6b62\u7b56\u7565\u8fdb\u5165\u5bb9\u6613\u4ea7\u751f\u4e0d\u7a33\u5b9a\u6027\u7684\u53c2\u6570\u533a\u57df\u3002", "result": "\u5728Walker2D\u73af\u5883\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5c06\u7a33\u5b9a\u6027\u63d0\u5347\u8fbe75%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u53ef\u6bd4\u8f83\u7684\u8bc4\u4f30\u5956\u52b1\u3002\u5f53\u540e\u66f4\u65b0\u8bc4\u8bba\u5bb6\u503c\u4e0e\u540e\u66f4\u65b0\u5956\u52b1\u5bf9\u9f50\u4e0d\u4f73\u65f6\uff0c\u6807\u51c6PPO\u96be\u4ee5\u4ea7\u751f\u72ed\u7a84\u7684R(\u03b8)\uff0c\u800c\u672c\u65b9\u6cd5\u7684\u77e9\u57fa\u4fee\u6b63\u80fd\u6709\u6548\u7f29\u5c0fR(\u03b8)\u5206\u5e03\u3002", "conclusion": "\u5229\u7528\u73af\u5883\u968f\u673a\u6027\u6765\u51cf\u8f7b\u66f4\u65b0\u5f15\u8d77\u7684\u53d8\u5f02\u6027\u662f\u6709\u6548\u7684\u3002\u901a\u8fc7\u5206\u5e03\u8bc4\u8bba\u5bb6\u7684\u9ad8\u9636\u77e9\u4fee\u6b63PPO\u4f18\u52bf\u51fd\u6570\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u7b56\u7565\u7a33\u5b9a\u6027\uff0c\u540c\u65f6\u4e0d\u727a\u7272\u6027\u80fd\uff0c\u4e3a\u8fde\u7eed\u63a7\u5236\u4e2d\u7684\u7a33\u5b9a\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u8ba1\u7b97\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01829", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01829", "abs": "https://arxiv.org/abs/2601.01829", "authors": ["Peiyan Hu", "Haodong Feng", "Hongyuan Liu", "Tongtong Yan", "Wenhao Deng", "Tianrun Gao", "Rong Zheng", "Haoren Zheng", "Chenglei Yu", "Chuanrui Wang", "Kaiwen Li", "Zhi-Ming Ma", "Dezhi Zhou", "Xingcai Lu", "Dixia Fan", "Tailin Wu"], "title": "RealPDEBench: A Benchmark for Complex Physical Systems with Real-World Data", "comment": "46 pages, 21 figures", "summary": "Predicting the evolution of complex physical systems remains a central problem in science and engineering. Despite rapid progress in scientific Machine Learning (ML) models, a critical bottleneck is the lack of expensive real-world data, resulting in most current models being trained and validated on simulated data. Beyond limiting the development and evaluation of scientific ML, this gap also hinders research into essential tasks such as sim-to-real transfer. We introduce RealPDEBench, the first benchmark for scientific ML that integrates real-world measurements with paired numerical simulations. RealPDEBench consists of five datasets, three tasks, eight metrics, and ten baselines. We first present five real-world measured datasets with paired simulated datasets across different complex physical systems. We further define three tasks, which allow comparisons between real-world and simulated data, and facilitate the development of methods to bridge the two. Moreover, we design eight evaluation metrics, spanning data-oriented and physics-oriented metrics, and finally benchmark ten representative baselines, including state-of-the-art models, pretrained PDE foundation models, and a traditional method. Experiments reveal significant discrepancies between simulated and real-world data, while showing that pretraining with simulated data consistently improves both accuracy and convergence. In this work, we hope to provide insights from real-world data, advancing scientific ML toward bridging the sim-to-real gap and real-world deployment. Our benchmark, datasets, and instructions are available at https://realpdebench.github.io/.", "AI": {"tldr": "RealPDEBench\u662f\u9996\u4e2a\u7ed3\u5408\u771f\u5b9e\u6d4b\u91cf\u6570\u636e\u4e0e\u914d\u5bf9\u6570\u503c\u6a21\u62df\u7684\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u57fa\u51c6\uff0c\u5305\u542b5\u4e2a\u6570\u636e\u96c6\u30013\u4e2a\u4efb\u52a1\u30018\u4e2a\u6307\u6807\u548c10\u4e2a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u79d1\u5b66ML\u4e2d\u771f\u5b9e\u6570\u636e\u7a00\u7f3a\u548c\u6a21\u62df-\u73b0\u5b9e\u5dee\u8ddd\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u6a21\u62df\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u548c\u9a8c\u8bc1\uff0c\u7f3a\u4e4f\u771f\u5b9e\u4e16\u754c\u6570\u636e\u9650\u5236\u4e86\u6a21\u578b\u53d1\u5c55\u548c\u8bc4\u4f30\uff0c\u4e5f\u963b\u788d\u4e86\u6a21\u62df\u5230\u73b0\u5b9e\u8fc1\u79fb\u7b49\u5173\u952e\u4efb\u52a1\u7684\u7814\u7a76\u3002", "method": "\u6784\u5efa\u5305\u542b5\u4e2a\u771f\u5b9e\u6d4b\u91cf\u6570\u636e\u96c6\u53ca\u5176\u914d\u5bf9\u6a21\u62df\u6570\u636e\u7684\u57fa\u51c6\uff0c\u5b9a\u4e493\u4e2a\u6bd4\u8f83\u4efb\u52a1\uff0c\u8bbe\u8ba18\u4e2a\u8bc4\u4f30\u6307\u6807\uff08\u6570\u636e\u5bfc\u5411\u548c\u7269\u7406\u5bfc\u5411\uff09\uff0c\u5e76\u8bc4\u4f3010\u4e2a\u4ee3\u8868\u6027\u57fa\u7ebf\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u6a21\u62df\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u4f46\u4f7f\u7528\u6a21\u62df\u6570\u636e\u8fdb\u884c\u9884\u8bad\u7ec3\u80fd\u6301\u7eed\u63d0\u5347\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u6536\u655b\u901f\u5ea6\u3002", "conclusion": "RealPDEBench\u4e3a\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u771f\u5b9e\u4e16\u754c\u6570\u636e\u7684\u6d1e\u5bdf\uff0c\u6709\u52a9\u4e8e\u7f29\u5c0f\u6a21\u62df-\u73b0\u5b9e\u5dee\u8ddd\u5e76\u63a8\u52a8\u6a21\u578b\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2601.01833", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01833", "abs": "https://arxiv.org/abs/2601.01833", "authors": ["Chenyu Hu", "Qiming Hu", "Sinan Chen", "Nianyu Li", "Mingyue Zhang", "Jialong Li"], "title": "FAROS: Robust Federated Learning with Adaptive Scaling against Backdoor Attacks", "comment": null, "summary": "Federated Learning (FL) enables multiple clients to collaboratively train a shared model without exposing local data. However, backdoor attacks pose a significant threat to FL. These attacks aim to implant a stealthy trigger into the global model, causing it to mislead on inputs that possess a specific trigger while functioning normally on benign data. Although pre-aggregation detection is a main defense direction, existing state-of-the-art defenses often rely on fixed defense parameters. This reliance makes them vulnerable to single-point-of-failure risks, rendering them less effective against sophisticated attackers. To address these limitations, we propose FAROS, an enhanced FL framework that incorporates Adaptive Differential Scaling (ADS) and Robust Core-set Computing (RCC). The ADS mechanism adjusts the defense's sensitivity dynamically, based on the dispersion of uploaded gradients by clients in each round. This allows it to counter attackers who strategically shift between stealthiness and effectiveness. Furthermore, the RCC effectively mitigates the risk of single-point failure by computing the centroid of a core set comprising clients with the highest confidence. We conducted extensive experiments across various datasets, models, and attack scenarios. The results demonstrate that our method outperforms current defenses in both attack success rate and main task accuracy.", "AI": {"tldr": "FAROS\uff1a\u4e00\u4e2a\u589e\u5f3a\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5dee\u5206\u7f29\u653e\u548c\u9c81\u68d2\u6838\u5fc3\u96c6\u8ba1\u7b97\u6765\u9632\u5fa1\u540e\u95e8\u653b\u51fb\uff0c\u76f8\u6bd4\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u5728\u653b\u51fb\u6210\u529f\u7387\u548c\u4e3b\u4efb\u52a1\u51c6\u786e\u7387\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u9762\u4e34\u540e\u95e8\u653b\u51fb\u7684\u4e25\u91cd\u5a01\u80c1\uff0c\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u53c2\u6570\uff0c\u5b58\u5728\u5355\u70b9\u6545\u969c\u98ce\u9669\uff0c\u96be\u4ee5\u5e94\u5bf9\u590d\u6742\u653b\u51fb\u8005\u7b56\u7565\u3002", "method": "\u63d0\u51faFAROS\u6846\u67b6\uff0c\u5305\u542b\u81ea\u9002\u5e94\u5dee\u5206\u7f29\u653e\u673a\u5236\uff08\u6839\u636e\u5ba2\u6237\u7aef\u4e0a\u4f20\u68af\u5ea6\u7684\u79bb\u6563\u5ea6\u52a8\u6001\u8c03\u6574\u9632\u5fa1\u654f\u611f\u6027\uff09\u548c\u9c81\u68d2\u6838\u5fc3\u96c6\u8ba1\u7b97\uff08\u8ba1\u7b97\u9ad8\u7f6e\u4fe1\u5ea6\u5ba2\u6237\u7aef\u6838\u5fc3\u96c6\u7684\u8d28\u5fc3\u4ee5\u964d\u4f4e\u5355\u70b9\u6545\u969c\u98ce\u9669\uff09\u3002", "result": "\u5728\u591a\u79cd\u6570\u636e\u96c6\u3001\u6a21\u578b\u548c\u653b\u51fb\u573a\u666f\u4e0b\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u653b\u51fb\u6210\u529f\u7387\u548c\u4e3b\u4efb\u52a1\u51c6\u786e\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u3002", "conclusion": "FAROS\u901a\u8fc7\u52a8\u6001\u81ea\u9002\u5e94\u9632\u5fa1\u673a\u5236\u6709\u6548\u5e94\u5bf9\u590d\u6742\u7684\u540e\u95e8\u653b\u51fb\u7b56\u7565\uff0c\u63d0\u9ad8\u4e86\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2601.01840", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.01840", "abs": "https://arxiv.org/abs/2601.01840", "authors": ["Qiantao Yang", "Liquan Chen", "Mingfu Xue", "Songze Li"], "title": "Tackling Resource-Constrained and Data-Heterogeneity in Federated Learning with Double-Weight Sparse Pack", "comment": "Accepted in AAAI 2026", "summary": "Federated learning has drawn widespread interest from researchers, yet the data heterogeneity across edge clients remains a key challenge, often degrading model performance. Existing methods enhance model compatibility with data heterogeneity by splitting models and knowledge distillation. However, they neglect the insufficient communication bandwidth and computing power on the client, failing to strike an effective balance between addressing data heterogeneity and accommodating limited client resources. To tackle this limitation, we propose a personalized federated learning method based on cosine sparsification parameter packing and dual-weighted aggregation (FedCSPACK), which effectively leverages the limited client resources and reduces the impact of data heterogeneity on model performance. In FedCSPACK, the client packages model parameters and selects the most contributing parameter packages for sharing based on cosine similarity, effectively reducing bandwidth requirements. The client then generates a mask matrix anchored to the shared parameter package to improve the alignment and aggregation efficiency of sparse updates on the server. Furthermore, directional and distribution distance weights are embedded in the mask to implement a weighted-guided aggregation mechanism, enhancing the robustness and generalization performance of the global model. Extensive experiments across four datasets using ten state-of-the-art methods demonstrate that FedCSPACK effectively improves communication and computational efficiency while maintaining high model accuracy.", "AI": {"tldr": "FedCSPACK\uff1a\u57fa\u4e8e\u4f59\u5f26\u7a00\u758f\u5316\u53c2\u6570\u6253\u5305\u548c\u53cc\u6743\u91cd\u805a\u5408\u7684\u4e2a\u6027\u5316\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u6570\u636e\u5f02\u6784\u6027\u548c\u5ba2\u6237\u7aef\u8d44\u6e90\u53d7\u9650\u95ee\u9898", "motivation": "\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u7406\u6570\u636e\u5f02\u6784\u6027\u65f6\uff0c\u5ffd\u7565\u4e86\u5ba2\u6237\u7aef\u901a\u4fe1\u5e26\u5bbd\u548c\u8ba1\u7b97\u80fd\u529b\u6709\u9650\u7684\u95ee\u9898\uff0c\u65e0\u6cd5\u5728\u89e3\u51b3\u6570\u636e\u5f02\u6784\u6027\u548c\u9002\u5e94\u6709\u9650\u5ba2\u6237\u7aef\u8d44\u6e90\u4e4b\u95f4\u53d6\u5f97\u6709\u6548\u5e73\u8861", "method": "\u63d0\u51faFedCSPACK\u65b9\u6cd5\uff1a1\uff09\u5ba2\u6237\u7aef\u57fa\u4e8e\u4f59\u5f26\u76f8\u4f3c\u6027\u6253\u5305\u6a21\u578b\u53c2\u6570\u5e76\u9009\u62e9\u8d21\u732e\u6700\u5927\u7684\u53c2\u6570\u5305\u5171\u4eab\uff0c\u51cf\u5c11\u5e26\u5bbd\u9700\u6c42\uff1b2\uff09\u5ba2\u6237\u7aef\u751f\u6210\u57fa\u4e8e\u5171\u4eab\u53c2\u6570\u5305\u7684\u63a9\u7801\u77e9\u9635\uff0c\u63d0\u9ad8\u7a00\u758f\u66f4\u65b0\u5728\u670d\u52a1\u5668\u4e0a\u7684\u5bf9\u9f50\u548c\u805a\u5408\u6548\u7387\uff1b3\uff09\u5728\u63a9\u7801\u4e2d\u5d4c\u5165\u65b9\u5411\u548c\u5206\u5e03\u8ddd\u79bb\u6743\u91cd\uff0c\u5b9e\u73b0\u52a0\u6743\u5f15\u5bfc\u805a\u5408\u673a\u5236", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u4f7f\u7528\u5341\u79cd\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cFedCSPACK\u5728\u4fdd\u6301\u9ad8\u6a21\u578b\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u901a\u4fe1\u548c\u8ba1\u7b97\u6548\u7387", "conclusion": "FedCSPACK\u901a\u8fc7\u53c2\u6570\u6253\u5305\u548c\u53cc\u6743\u91cd\u805a\u5408\u673a\u5236\uff0c\u5728\u6709\u9650\u5ba2\u6237\u7aef\u8d44\u6e90\u4e0b\u6709\u6548\u7f13\u89e3\u6570\u636e\u5f02\u6784\u6027\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5b9e\u73b0\u4e86\u901a\u4fe1\u3001\u8ba1\u7b97\u6548\u7387\u548c\u6a21\u578b\u51c6\u786e\u6027\u7684\u826f\u597d\u5e73\u8861"}}
{"id": "2601.01860", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.01860", "abs": "https://arxiv.org/abs/2601.01860", "authors": ["Shuta Kikuchi", "Shu Tanaka"], "title": "High-Order Epistasis Detection Using Factorization Machine with Quadratic Optimization Annealing and MDR-Based Evaluation", "comment": "6 pages, 2 figures", "summary": "Detecting high-order epistasis is a fundamental challenge in genetic association studies due to the combinatorial explosion of candidate locus combinations. Although multifactor dimensionality reduction (MDR) is a widely used method for evaluating epistasis, exhaustive MDR-based searches become computationally infeasible as the number of loci or the interaction order increases. In this paper, we define the epistasis detection problem as a black-box optimization problem and solve it with a factorization machine with quadratic optimization annealing (FMQA). We propose an efficient epistasis detection method based on FMQA, in which the classification error rate (CER) computed by MDR is used as a black-box objective function. Experimental evaluations were conducted using simulated case-control datasets with predefined high-order epistasis. The results demonstrate that the proposed method successfully identified ground-truth epistasis across various interaction orders and the numbers of genetic loci within a limited number of iterations. These results indicate that the proposed method is effective and computationally efficient for high-order epistasis detection.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u56e0\u5b50\u5206\u89e3\u673a\u548c\u4e8c\u6b21\u4f18\u5316\u9000\u706b\u7684FMQA\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u68c0\u6d4b\u9ad8\u9636\u4e0a\u4f4d\u6027\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfMDR\u65b9\u6cd5\u5728\u7ec4\u5408\u7206\u70b8\u4e0b\u7684\u8ba1\u7b97\u4e0d\u53ef\u884c\u95ee\u9898\u3002", "motivation": "\u9ad8\u9636\u4e0a\u4f4d\u6027\u68c0\u6d4b\u5728\u9057\u4f20\u5173\u8054\u7814\u7a76\u4e2d\u9762\u4e34\u7ec4\u5408\u7206\u70b8\u7684\u6311\u6218\uff0c\u4f20\u7edf\u591a\u56e0\u5b50\u964d\u7ef4(MDR)\u65b9\u6cd5\u5728\u57fa\u56e0\u5ea7\u6570\u91cf\u6216\u4ea4\u4e92\u9636\u6570\u589e\u52a0\u65f6\u8ba1\u7b97\u4e0d\u53ef\u884c\u3002", "method": "\u5c06\u4e0a\u4f4d\u6027\u68c0\u6d4b\u5b9a\u4e49\u4e3a\u9ed1\u76d2\u4f18\u5316\u95ee\u9898\uff0c\u4f7f\u7528\u56e0\u5b50\u5206\u89e3\u673a\u7ed3\u5408\u4e8c\u6b21\u4f18\u5316\u9000\u706b(FMQA)\u6c42\u89e3\uff0c\u4ee5MDR\u8ba1\u7b97\u7684\u5206\u7c7b\u9519\u8bef\u7387\u4f5c\u4e3a\u9ed1\u76d2\u76ee\u6807\u51fd\u6570\u3002", "result": "\u5728\u6a21\u62df\u75c5\u4f8b\u5bf9\u7167\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u5728\u6709\u9650\u8fed\u4ee3\u6b21\u6570\u5185\u6210\u529f\u8bc6\u522b\u4e0d\u540c\u4ea4\u4e92\u9636\u6570\u548c\u57fa\u56e0\u5ea7\u6570\u91cf\u7684\u771f\u5b9e\u4e0a\u4f4d\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684FMQA\u65b9\u6cd5\u5bf9\u4e8e\u9ad8\u9636\u4e0a\u4f4d\u6027\u68c0\u6d4b\u65e2\u6709\u6548\u53c8\u8ba1\u7b97\u9ad8\u6548\u3002"}}
{"id": "2601.01887", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01887", "abs": "https://arxiv.org/abs/2601.01887", "authors": ["Jiawen Zhang", "Lipeng He", "Kejia Chen", "Jian Lou", "Jian Liu", "Xiaohu Yang", "Ruoxi Jia"], "title": "Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance", "comment": null, "summary": "Fine-tuning safety-aligned large language models (LLMs) can substantially compromise their safety. Previous approaches require many safety samples or calibration sets, which not only incur significant computational overhead during realignment but also lead to noticeable degradation in model utility. Contrary to this belief, we show that safety alignment can be fully recovered with only a single safety example, without sacrificing utility and at minimal cost. Remarkably, this recovery is effective regardless of the number of harmful examples used in fine-tuning or the size of the underlying model, and convergence is achieved within just a few epochs. Furthermore, we uncover the low-rank structure of the safety gradient, which explains why such efficient correction is possible. We validate our findings across five safety-aligned LLMs and multiple datasets, demonstrating the generality of our approach.", "AI": {"tldr": "\u4ec5\u9700\u5355\u4e2a\u5b89\u5168\u793a\u4f8b\u5373\u53ef\u5b8c\u5168\u6062\u590d\u5b89\u5168\u5bf9\u9f50\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u65e0\u9700\u727a\u7272\u5b9e\u7528\u6027\uff0c\u6210\u672c\u6781\u4f4e", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u5b89\u5168\u6837\u672c\u6216\u6821\u51c6\u96c6\uff0c\u5bfc\u81f4\u8ba1\u7b97\u5f00\u9500\u5927\u4e14\u6a21\u578b\u5b9e\u7528\u6027\u4e0b\u964d\u3002\u7814\u7a76\u53d1\u73b0\u5b89\u5168\u5bf9\u9f50\u53ef\u4ee5\u66f4\u9ad8\u6548\u5730\u6062\u590d", "method": "\u4f7f\u7528\u5355\u4e2a\u5b89\u5168\u793a\u4f8b\u8fdb\u884c\u5fae\u8c03\u6062\u590d\uff0c\u53d1\u73b0\u5b89\u5168\u68af\u5ea6\u5177\u6709\u4f4e\u79e9\u7ed3\u6784\uff0c\u53ea\u9700\u51e0\u4e2aepoch\u5373\u53ef\u6536\u655b", "result": "\u8be5\u65b9\u6cd5\u5728\u4e94\u4e2a\u5b89\u5168\u5bf9\u9f50LLM\u548c\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u6709\u6548\uff0c\u65e0\u8bba\u6709\u5bb3\u793a\u4f8b\u6570\u91cf\u6216\u6a21\u578b\u5927\u5c0f\uff0c\u90fd\u80fd\u6709\u6548\u6062\u590d\u5b89\u5168\u5bf9\u9f50", "conclusion": "\u5b89\u5168\u5bf9\u9f50\u53ef\u4ee5\u901a\u8fc7\u6781\u4f4e\u6210\u672c\u9ad8\u6548\u6062\u590d\uff0c\u5b89\u5168\u68af\u5ea6\u7684\u4f4e\u79e9\u7ed3\u6784\u89e3\u91ca\u4e86\u8fd9\u79cd\u9ad8\u6548\u4fee\u6b63\u7684\u53ef\u80fd\u6027"}}
{"id": "2601.01901", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01901", "abs": "https://arxiv.org/abs/2601.01901", "authors": ["Yuexuan Xia", "Yinghao Zhang", "Yalin Liu", "Hong-Ning Dai", "Yong Xia"], "title": "FedBiCross: A Bi-Level Optimization Framework to Tackle Non-IID Challenges in Data-Free One-Shot Federated Learning on Medical Data", "comment": null, "summary": "Data-free knowledge distillation-based one-shot federated learning (OSFL) trains a model in a single communication round without sharing raw data, making OSFL attractive for privacy-sensitive medical applications. However, existing methods aggregate predictions from all clients to form a global teacher. Under non-IID data, conflicting predictions cancel out during averaging, yielding near-uniform soft labels that provide weak supervision for distillation. We propose FedBiCross, a personalized OSFL framework with three stages: (1) clustering clients by model output similarity to form coherent sub-ensembles, (2) bi-level cross-cluster optimization that learns adaptive weights to selectively leverage beneficial cross-cluster knowledge while suppressing negative transfer, and (3) personalized distillation for client-specific adaptation. Experiments on four medical image datasets demonstrate that FedBiCross consistently outperforms state-of-the-art baselines across different non-IID degrees.", "AI": {"tldr": "FedBiCross\uff1a\u4e00\u79cd\u4e2a\u6027\u5316\u7684\u5355\u6b21\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u805a\u7c7b\u548c\u53cc\u5c42\u8de8\u96c6\u7fa4\u4f18\u5316\u89e3\u51b3\u975eIID\u6570\u636e\u4e0b\u7684\u9884\u6d4b\u51b2\u7a81\u95ee\u9898", "motivation": "\u73b0\u6709\u7684\u5355\u6b21\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u5728\u975eIID\u6570\u636e\u4e0b\uff0c\u6240\u6709\u5ba2\u6237\u7aef\u7684\u9884\u6d4b\u5728\u5e73\u5747\u65f6\u4f1a\u76f8\u4e92\u62b5\u6d88\uff0c\u4ea7\u751f\u63a5\u8fd1\u5747\u5300\u7684\u8f6f\u6807\u7b7e\uff0c\u5bfc\u81f4\u84b8\u998f\u76d1\u7763\u4fe1\u53f7\u5f31\u3002\u9700\u8981\u89e3\u51b3\u9884\u6d4b\u51b2\u7a81\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u62a4\u533b\u7597\u6570\u636e\u7684\u9690\u79c1\u654f\u611f\u6027\u3002", "method": "\u63d0\u51faFedBiCross\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u9636\u6bb5\uff1a1\uff09\u57fa\u4e8e\u6a21\u578b\u8f93\u51fa\u76f8\u4f3c\u6027\u5bf9\u5ba2\u6237\u7aef\u8fdb\u884c\u805a\u7c7b\uff0c\u5f62\u6210\u8fde\u8d2f\u7684\u5b50\u96c6\u6210\uff1b2\uff09\u53cc\u5c42\u8de8\u96c6\u7fa4\u4f18\u5316\uff0c\u5b66\u4e60\u81ea\u9002\u5e94\u6743\u91cd\uff0c\u9009\u62e9\u6027\u5229\u7528\u6709\u76ca\u7684\u8de8\u96c6\u7fa4\u77e5\u8bc6\u540c\u65f6\u6291\u5236\u8d1f\u8fc1\u79fb\uff1b3\uff09\u4e2a\u6027\u5316\u84b8\u998f\u8fdb\u884c\u5ba2\u6237\u7aef\u7279\u5b9a\u9002\u5e94\u3002", "result": "\u5728\u56db\u4e2a\u533b\u5b66\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFedBiCross\u5728\u4e0d\u540c\u975eIID\u7a0b\u5ea6\u4e0b\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "FedBiCross\u901a\u8fc7\u805a\u7c7b\u548c\u9009\u62e9\u6027\u77e5\u8bc6\u8f6c\u79fb\u6709\u6548\u89e3\u51b3\u4e86\u975eIID\u6570\u636e\u4e0b\u5355\u6b21\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u9884\u6d4b\u51b2\u7a81\u95ee\u9898\uff0c\u4e3a\u9690\u79c1\u654f\u611f\u7684\u533b\u7597\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u4e2a\u6027\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01903", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01903", "abs": "https://arxiv.org/abs/2601.01903", "authors": ["Ungsik Kim", "Suwon Lee"], "title": "TT-FSI: Scalable Faithful Shapley Interactions via Tensor-Train", "comment": null, "summary": "The Faithful Shapley Interaction (FSI) index uniquely satisfies the faithfulness axiom among Shapley interaction indices, but computing FSI requires $O(d^\\ell \\cdot 2^d)$ time and existing implementations use $O(4^d)$ memory. We present TT-FSI, which exploits FSI's algebraic structure via Matrix Product Operators (MPO). Our main theoretical contribution is proving that the linear operator $v \\mapsto \\text{FSI}(v)$ admits an MPO representation with TT-rank $O(\\ell d)$, enabling an efficient sweep algorithm with $O(\\ell^2 d^3 \\cdot 2^d)$ time and $O(\\ell d^2)$ core storage an exponential improvement over existing methods. Experiments on six datasets ($d=8$ to $d=20$) demonstrate up to 280$\\times$ speedup over baseline, 85$\\times$ over SHAP-IQ, and 290$\\times$ memory reduction. TT-FSI scales to $d=20$ (1M coalitions) where all competing methods fail.", "AI": {"tldr": "TT-FSI\uff1a\u5229\u7528\u77e9\u9635\u4e58\u79ef\u7b97\u5b50\uff08MPO\uff09\u9ad8\u6548\u8ba1\u7b97\u5fe0\u5b9eShapley\u4ea4\u4e92\u6307\u6570\uff08FSI\uff09\uff0c\u5c06\u65f6\u95f4\u590d\u6742\u5ea6\u4eceO(d^\u2113\u00b72^d)\u964d\u4f4e\u5230O(\u2113\u00b2d\u00b3\u00b72^d)\uff0c\u5185\u5b58\u4f7f\u7528\u4eceO(4^d)\u964d\u4f4e\u5230O(\u2113d\u00b2)\uff0c\u5b9e\u73b0\u4e86\u6307\u6570\u7ea7\u6539\u8fdb\u3002", "motivation": "FSI\u6307\u6570\u662f\u552f\u4e00\u6ee1\u8db3\u5fe0\u5b9e\u516c\u7406\u7684Shapley\u4ea4\u4e92\u6307\u6570\uff0c\u4f46\u8ba1\u7b97\u590d\u6742\u5ea6\u6781\u9ad8\uff08O(d^\u2113\u00b72^d)\u65f6\u95f4\uff0cO(4^d)\u5185\u5b58\uff09\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b9e\u9645\u9ad8\u7ef4\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5229\u7528FSI\u7684\u4ee3\u6570\u7ed3\u6784\uff0c\u8bc1\u660e\u7ebf\u6027\u7b97\u5b50v\u21a6FSI(v)\u5177\u6709TT\u79e9\u4e3aO(\u2113d)\u7684MPO\u8868\u793a\uff0c\u4ece\u800c\u8bbe\u8ba1\u9ad8\u6548\u7684\u626b\u63cf\u7b97\u6cd5\uff0c\u5b9e\u73b0\u65f6\u95f4\u548c\u5185\u5b58\u7684\u6307\u6570\u7ea7\u4f18\u5316\u3002", "result": "\u57286\u4e2a\u6570\u636e\u96c6\uff08d=8\u5230d=20\uff09\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff1a\u76f8\u6bd4\u57fa\u7ebf\u52a0\u901f280\u500d\uff0c\u76f8\u6bd4SHAP-IQ\u52a0\u901f85\u500d\uff0c\u5185\u5b58\u51cf\u5c11290\u500d\u3002TT-FSI\u53ef\u6269\u5c55\u5230d=20\uff08100\u4e07\u4e2a\u8054\u76df\uff09\uff0c\u800c\u6240\u6709\u7ade\u4e89\u65b9\u6cd5\u5747\u5931\u8d25\u3002", "conclusion": "TT-FSI\u901a\u8fc7MPO\u8868\u793a\u548c\u626b\u63cf\u7b97\u6cd5\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86FSI\u6307\u6570\u7684\u9ad8\u6548\u53ef\u6269\u5c55\u8ba1\u7b97\uff0c\u4e3a\u9ad8\u7ef4\u7279\u5f81\u4ea4\u4e92\u5206\u6790\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2601.01904", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01904", "abs": "https://arxiv.org/abs/2601.01904", "authors": ["Yuxuan Li", "Harshith Reddy Kethireddy", "Srijita Das"], "title": "Evaluating Feature Dependent Noise in Preference-based Reinforcement Learning", "comment": null, "summary": "Learning from Preferences in Reinforcement Learning (PbRL) has gained attention recently, as it serves as a natural fit for complicated tasks where the reward function is not easily available. However, preferences often come with uncertainty and noise if they are not from perfect teachers. Much prior literature aimed to detect noise, but with limited types of noise and most being uniformly distributed with no connection to observations. In this work, we formalize the notion of targeted feature-dependent noise and propose several variants like trajectory feature noise, trajectory similarity noise, uncertainty-aware noise, and Language Model noise.\n  We evaluate feature-dependent noise, where noise is correlated with certain features in complex continuous control tasks from DMControl and Meta-world. Our experiments show that in some feature-dependent noise settings, the state-of-the-art noise-robust PbRL method's learning performance is significantly deteriorated, while PbRL method with no explicit denoising can surprisingly outperform noise-robust PbRL in majority settings.\n  We also find language model's noise exhibits similar characteristics to feature-dependent noise, thereby simulating realistic humans and call for further study in learning with feature-dependent noise robustly.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u504f\u597d\u5f3a\u5316\u5b66\u4e60\u4e2d\u7279\u5f81\u4f9d\u8d56\u566a\u58f0\u7684\u95ee\u9898\uff0c\u53d1\u73b0\u73b0\u6709\u566a\u58f0\u9c81\u68d2\u65b9\u6cd5\u5728\u7279\u5f81\u4f9d\u8d56\u566a\u58f0\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u800c\u65e0\u663e\u5f0f\u53bb\u566a\u7684\u65b9\u6cd5\u53cd\u800c\u8868\u73b0\u66f4\u597d\uff0c\u8bed\u8a00\u6a21\u578b\u566a\u58f0\u4e5f\u8868\u73b0\u51fa\u7c7b\u4f3c\u7279\u5f81\u4f9d\u8d56\u566a\u58f0\u7684\u7279\u6027\u3002", "motivation": "\u504f\u597d\u5f3a\u5316\u5b66\u4e60\uff08PbRL\uff09\u5728\u5956\u52b1\u51fd\u6570\u4e0d\u6613\u83b7\u5f97\u7684\u590d\u6742\u4efb\u52a1\u4e2d\u5f88\u6709\u7528\uff0c\u4f46\u504f\u597d\u6570\u636e\u5e38\u5305\u542b\u4e0d\u786e\u5b9a\u6027\u548c\u566a\u58f0\u3002\u73b0\u6709\u7814\u7a76\u5927\u591a\u5173\u6ce8\u5747\u5300\u5206\u5e03\u7684\u566a\u58f0\u68c0\u6d4b\uff0c\u800c\u5ffd\u7565\u4e86\u4e0e\u89c2\u6d4b\u7279\u5f81\u76f8\u5173\u7684\u566a\u58f0\u7c7b\u578b\u3002", "method": "\u63d0\u51fa\u4e86\u7279\u5f81\u4f9d\u8d56\u566a\u58f0\u7684\u5f62\u5f0f\u5316\u6982\u5ff5\uff0c\u5305\u62ec\u8f68\u8ff9\u7279\u5f81\u566a\u58f0\u3001\u8f68\u8ff9\u76f8\u4f3c\u6027\u566a\u58f0\u3001\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u566a\u58f0\u548c\u8bed\u8a00\u6a21\u578b\u566a\u58f0\u7b49\u53d8\u4f53\u3002\u5728DMControl\u548cMeta-world\u7684\u590d\u6742\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\u8bc4\u4f30\u7279\u5f81\u4f9d\u8d56\u566a\u58f0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u67d0\u4e9b\u7279\u5f81\u4f9d\u8d56\u566a\u58f0\u8bbe\u7f6e\u4e0b\uff0c\u6700\u5148\u8fdb\u7684\u566a\u58f0\u9c81\u68d2PbRL\u65b9\u6cd5\u7684\u5b66\u4e60\u6027\u80fd\u663e\u8457\u6076\u5316\uff0c\u800c\u65e0\u663e\u5f0f\u53bb\u566a\u7684PbRL\u65b9\u6cd5\u5728\u591a\u6570\u8bbe\u7f6e\u4e2d\u53cd\u800c\u4f18\u4e8e\u566a\u58f0\u9c81\u68d2\u65b9\u6cd5\u3002\u8bed\u8a00\u6a21\u578b\u566a\u58f0\u4e5f\u8868\u73b0\u51fa\u7c7b\u4f3c\u7279\u5f81\u4f9d\u8d56\u566a\u58f0\u7684\u7279\u6027\u3002", "conclusion": "\u7279\u5f81\u4f9d\u8d56\u566a\u58f0\u5bf9\u73b0\u6709PbRL\u65b9\u6cd5\u6784\u6210\u6311\u6218\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u5982\u4f55\u9c81\u68d2\u5730\u5b66\u4e60\u5177\u6709\u7279\u5f81\u4f9d\u8d56\u566a\u58f0\u7684\u504f\u597d\u6570\u636e\u3002\u8bed\u8a00\u6a21\u578b\u566a\u58f0\u6a21\u62df\u4e86\u771f\u5b9e\u4eba\u7c7b\u504f\u597d\u566a\u58f0\u7684\u7279\u6027\uff0c\u503c\u5f97\u6df1\u5165\u7814\u7a76\u3002"}}
{"id": "2601.01917", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01917", "abs": "https://arxiv.org/abs/2601.01917", "authors": ["Ryo Iwaki", "Takayuki Osogami"], "title": "Distorted Distributional Policy Evaluation for Offline Reinforcement Learning", "comment": "The preprint version of the paper accepted to ICONIP2025. The Version of Record is available online at https://link.springer.com/chapter/10.1007/978-981-95-4091-4_35", "summary": "While Distributional Reinforcement Learning (DRL) methods have demonstrated strong performance in online settings, its success in offline scenarios remains limited. We hypothesize that a key limitation of existing offline DRL methods lies in their approach to uniformly underestimate return quantiles. This uniform pessimism can lead to overly conservative value estimates, ultimately hindering generalization and performance. To address this, we introduce a novel concept called quantile distortion, which enables non-uniform pessimism by adjusting the degree of conservatism based on the availability of supporting data. Our approach is grounded in theoretical analysis and empirically validated, demonstrating improved performance over uniform pessimism.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u5206\u4f4d\u6570\u626d\u66f2\u65b9\u6cd5\uff0c\u901a\u8fc7\u975e\u5747\u5300\u60b2\u89c2\u4e3b\u4e49\u6539\u8fdb\u79bb\u7ebf\u5206\u5e03\u5f3a\u5316\u5b66\u4e60\u6027\u80fd", "motivation": "\u73b0\u6709\u79bb\u7ebf\u5206\u5e03\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u91c7\u7528\u5747\u5300\u4f4e\u4f30\u56de\u62a5\u5206\u4f4d\u6570\u7684\u7b56\u7565\uff0c\u5bfc\u81f4\u8fc7\u4e8e\u4fdd\u5b88\u7684\u4ef7\u503c\u4f30\u8ba1\uff0c\u9650\u5236\u4e86\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u8868\u73b0", "method": "\u5f15\u5165\u5206\u4f4d\u6570\u626d\u66f2\u6982\u5ff5\uff0c\u6839\u636e\u652f\u6301\u6570\u636e\u7684\u53ef\u7528\u6027\u8c03\u6574\u4fdd\u5b88\u7a0b\u5ea6\uff0c\u5b9e\u73b0\u975e\u5747\u5300\u60b2\u89c2\u4e3b\u4e49", "result": "\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u5747\u5300\u60b2\u89c2\u4e3b\u4e49\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd", "conclusion": "\u5206\u4f4d\u6570\u626d\u66f2\u4e3a\u79bb\u7ebf\u5206\u5e03\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u6709\u6548\u7684\u975e\u5747\u5300\u60b2\u89c2\u4e3b\u4e49\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u8fc7\u4e8e\u4fdd\u5b88\u7684\u95ee\u9898"}}
{"id": "2601.01927", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01927", "abs": "https://arxiv.org/abs/2601.01927", "authors": ["Firuz Kamalov", "Hana Sulieman", "Witold Pedrycz"], "title": "Theoretical Convergence of SMOTE-Generated Samples", "comment": null, "summary": "Imbalanced data affects a wide range of machine learning applications, from healthcare to network security. As SMOTE is one of the most popular approaches to addressing this issue, it is imperative to validate it not only empirically but also theoretically. In this paper, we provide a rigorous theoretical analysis of SMOTE's convergence properties. Concretely, we prove that the synthetic random variable Z converges in probability to the underlying random variable X. We further prove a stronger convergence in mean when X is compact. Finally, we show that lower values of the nearest neighbor rank lead to faster convergence offering actionable guidance to practitioners. The theoretical results are supported by numerical experiments using both real-life and synthetic data. Our work provides a foundational understanding that enhances data augmentation techniques beyond imbalanced data scenarios.", "AI": {"tldr": "\u672c\u6587\u5bf9SMOTE\u7b97\u6cd5\u8fdb\u884c\u4e86\u4e25\u683c\u7684\u7406\u8bba\u5206\u6790\uff0c\u8bc1\u660e\u4e86\u5176\u5408\u6210\u53d8\u91cfZ\u4f9d\u6982\u7387\u6536\u655b\u4e8e\u539f\u59cb\u53d8\u91cfX\uff0c\u5e76\u5728X\u7d27\u81f4\u65f6\u8bc1\u660e\u4e86\u66f4\u5f3a\u7684\u5747\u503c\u6536\u655b\uff0c\u540c\u65f6\u53d1\u73b0\u8f83\u5c0f\u7684\u6700\u8fd1\u90bb\u79e9\u503c\u80fd\u5e26\u6765\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u3002", "motivation": "\u4e0d\u5e73\u8861\u6570\u636e\u5e7f\u6cdb\u5f71\u54cd\u673a\u5668\u5b66\u4e60\u5e94\u7528\uff0cSMOTE\u4f5c\u4e3a\u6700\u6d41\u884c\u7684\u89e3\u51b3\u65b9\u6cd5\u4e4b\u4e00\uff0c\u4e0d\u4ec5\u9700\u8981\u7ecf\u9a8c\u9a8c\u8bc1\uff0c\u66f4\u9700\u8981\u7406\u8bba\u5206\u6790\u6765\u5efa\u7acb\u5176\u7406\u8bba\u57fa\u7840\u3002", "method": "\u5bf9SMOTE\u7b97\u6cd5\u8fdb\u884c\u4e25\u683c\u7684\u6570\u5b66\u7406\u8bba\u5206\u6790\uff0c\u8bc1\u660e\u5176\u6536\u655b\u6027\u8d28\uff0c\u5305\u62ec\u4f9d\u6982\u7387\u6536\u655b\u548c\u5747\u503c\u6536\u655b\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u4f7f\u7528\u771f\u5b9e\u548c\u5408\u6210\u6570\u636e\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u8bc1\u660e\u4e86SMOTE\u5408\u6210\u53d8\u91cfZ\u4f9d\u6982\u7387\u6536\u655b\u4e8e\u539f\u59cb\u53d8\u91cfX\uff1b\u5f53X\u7d27\u81f4\u65f6\u8bc1\u660e\u4e86\u66f4\u5f3a\u7684\u5747\u503c\u6536\u655b\uff1b\u53d1\u73b0\u8f83\u5c0f\u7684\u6700\u8fd1\u90bb\u79e9\u503c\u80fd\u5e26\u6765\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\uff1b\u6570\u503c\u5b9e\u9a8c\u652f\u6301\u4e86\u7406\u8bba\u7ed3\u679c\u3002", "conclusion": "\u672c\u6587\u4e3aSMOTE\u7b97\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u4e0d\u4ec5\u589e\u5f3a\u4e86\u4e0d\u5e73\u8861\u6570\u636e\u5904\u7406\u6280\u672f\uff0c\u8fd8\u4e3a\u66f4\u5e7f\u6cdb\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2601.01931", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01931", "abs": "https://arxiv.org/abs/2601.01931", "authors": ["Willem R\u00f6pke", "Samuel Coward", "Andrei Lupu", "Thomas Foster", "Tim Rockt\u00e4schel", "Jakob Foerster"], "title": "D\u00e9j\u00e0Q: Open-Ended Evolution of Diverse, Learnable and Verifiable Problems", "comment": null, "summary": "Recent advances in reasoning models have yielded impressive results in mathematics and coding. However, most approaches rely on static datasets, which have been suggested to encourage memorisation and limit generalisation. We introduce D\u00e9j\u00e0Q, a framework that departs from this paradigm by jointly evolving a diverse set of synthetic mathematical problems alongside model training. This evolutionary process adapts to the model's ability throughout training, optimising problems for learnability. We propose two LLM-driven mutation strategies in which the model itself mutates the training data, either by altering contextual details or by directly modifying problem structure. We find that the model can generate novel and meaningful problems, and that these LLM-driven mutations improve RL training. We analyse key aspects of D\u00e9j\u00e0Q, including the validity of generated problems and computational overhead. Our results underscore the potential of dynamically evolving training data to enhance mathematical reasoning and indicate broader applicability, which we will support by open-sourcing our code.", "AI": {"tldr": "D\u00e9j\u00e0Q\u662f\u4e00\u4e2a\u901a\u8fc7\u8fdb\u5316\u5408\u6210\u6570\u5b66\u95ee\u9898\u6765\u589e\u5f3a\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u6846\u67b6\uff0c\u5b83\u8ba9\u6a21\u578b\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u52a8\u6001\u751f\u6210\u548c\u4fee\u6539\u95ee\u9898\uff0c\u800c\u4e0d\u662f\u4f9d\u8d56\u9759\u6001\u6570\u636e\u96c6\u3002", "motivation": "\u5f53\u524d\u63a8\u7406\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u9759\u6001\u6570\u636e\u96c6\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u8bb0\u5fc6\u800c\u975e\u771f\u6b63\u7406\u89e3\uff0c\u9650\u5236\u4e86\u6cdb\u5316\u80fd\u529b\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u52a8\u6001\u9002\u5e94\u6a21\u578b\u5b66\u4e60\u8fdb\u5ea6\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u63d0\u51faD\u00e9j\u00e0Q\u6846\u67b6\uff0c\u901a\u8fc7\u8fdb\u5316\u8fc7\u7a0b\u8054\u5408\u751f\u6210\u591a\u6837\u5316\u7684\u5408\u6210\u6570\u5b66\u95ee\u9898\u3002\u91c7\u7528\u4e24\u79cdLLM\u9a71\u52a8\u7684\u53d8\u5f02\u7b56\u7565\uff1a1\uff09\u6539\u53d8\u4e0a\u4e0b\u6587\u7ec6\u8282\uff1b2\uff09\u76f4\u63a5\u4fee\u6539\u95ee\u9898\u7ed3\u6784\u3002\u6a21\u578b\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u81ea\u884c\u53d8\u5f02\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u6a21\u578b\u80fd\u591f\u751f\u6210\u65b0\u9896\u4e14\u6709\u610f\u4e49\u7684\u6570\u5b66\u95ee\u9898\uff0cLLM\u9a71\u52a8\u7684\u53d8\u5f02\u7b56\u7565\u6539\u5584\u4e86\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6548\u679c\u3002\u751f\u6210\u7684\u95ee\u984c\u6709\u6548\uff0c\u8ba1\u7b97\u5f00\u9500\u53ef\u63a7\u3002", "conclusion": "\u52a8\u6001\u8fdb\u5316\u8bad\u7ec3\u6570\u636e\u80fd\u6709\u6548\u589e\u5f3a\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002\u4f5c\u8005\u5c06\u5f00\u6e90\u4ee3\u7801\u4ee5\u652f\u6301\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2601.01943", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01943", "abs": "https://arxiv.org/abs/2601.01943", "authors": ["Tieu-Long Phan", "Nhu-Ngoc Nguyen Song", "Peter F. Stadler"], "title": "SynRXN: An Open Benchmark and Curated Dataset for Computational Reaction Modeling", "comment": "31 pages (including references), 3 figures, 7 tables", "summary": "We present SynRXN, a unified benchmarking framework and open-data resource for computer-aided synthesis planning (CASP). SynRXN decomposes end-to-end synthesis planning into five task families, covering reaction rebalancing, atom-to-atom mapping, reaction classification, reaction property prediction, and synthesis route design. Curated, provenance-tracked reaction corpora are assembled from heterogeneous public sources into a harmonized representation and packaged as versioned datasets for each task family, with explicit source metadata, licence tags, and machine-readable manifests that record checksums, and row counts. For every task, SynRXN provides transparent splitting functions that generate leakage-aware train, validation, and test partitions, together with standardized evaluation workflows and metric suites tailored to classification, regression, and structured prediction settings. For sensitive benchmarking, we combine public training and validation data with held-out gold-standard test sets, and contamination-prone tasks such as reaction rebalancing and atom-to-atom mapping are distributed only as evaluation sets and are explicitly not intended for model training. Scripted build recipes enable bitwise-reproducible regeneration of all corpora across machines and over time, and the entire resource is released under permissive open licences to support reuse and extension. By removing dataset heterogeneity and packaging transparent, reusable evaluation scaffolding, SynRXN enables fair longitudinal comparison of CASP methods, supports rigorous ablations and stress tests along the full reaction-informatics pipeline, and lowers the barrier for practitioners who seek robust and comparable performance estimates for real-world synthesis planning workloads.", "AI": {"tldr": "SynRXN\u662f\u4e00\u4e2a\u7528\u4e8e\u8ba1\u7b97\u673a\u8f85\u52a9\u5408\u6210\u89c4\u5212\u7684\u7edf\u4e00\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\u548c\u5f00\u653e\u6570\u636e\u8d44\u6e90\uff0c\u5c06\u7aef\u5230\u7aef\u5408\u6210\u89c4\u5212\u5206\u89e3\u4e3a\u4e94\u4e2a\u4efb\u52a1\u65cf\uff0c\u63d0\u4f9b\u6807\u51c6\u5316\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6d41\u7a0b\u3002", "motivation": "\u5f53\u524d\u8ba1\u7b97\u673a\u8f85\u52a9\u5408\u6210\u89c4\u5212\u9886\u57df\u7f3a\u4e4f\u7edf\u4e00\u3001\u900f\u660e\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u6570\u636e\u96c6\u5f02\u6784\u4e14\u8bc4\u4f30\u6807\u51c6\u4e0d\u4e00\u81f4\uff0c\u96be\u4ee5\u8fdb\u884c\u516c\u5e73\u7684\u65b9\u6cd5\u6bd4\u8f83\u548c\u6027\u80fd\u8bc4\u4f30\u3002", "method": "\u5c06\u5408\u6210\u89c4\u5212\u5206\u89e3\u4e3a\u4e94\u4e2a\u4efb\u52a1\u65cf\uff1a\u53cd\u5e94\u5e73\u8861\u3001\u539f\u5b50\u6620\u5c04\u3001\u53cd\u5e94\u5206\u7c7b\u3001\u53cd\u5e94\u6027\u8d28\u9884\u6d4b\u548c\u5408\u6210\u8def\u7ebf\u8bbe\u8ba1\u3002\u4ece\u5f02\u6784\u516c\u5171\u6765\u6e90\u6536\u96c6\u53cd\u5e94\u6570\u636e\uff0c\u8fdb\u884c\u6807\u51c6\u5316\u5904\u7406\uff0c\u63d0\u4f9b\u9632\u6cc4\u6f0f\u7684\u6570\u636e\u5206\u5272\u3001\u6807\u51c6\u5316\u8bc4\u4f30\u5de5\u4f5c\u6d41\u548c\u6307\u6807\u5957\u4ef6\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u7248\u672c\u5316\u6570\u636e\u96c6\u3001\u900f\u660e\u6570\u636e\u5206\u5272\u3001\u6807\u51c6\u5316\u8bc4\u4f30\u6d41\u7a0b\u7684\u5b8c\u6574\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u652f\u6301\u53ef\u91cd\u590d\u7684\u6570\u636e\u96c6\u91cd\u5efa\uff0c\u6240\u6709\u8d44\u6e90\u5747\u5728\u5f00\u653e\u8bb8\u53ef\u4e0b\u53d1\u5e03\u3002", "conclusion": "SynRXN\u901a\u8fc7\u6d88\u9664\u6570\u636e\u96c6\u5f02\u8d28\u6027\u5e76\u63d0\u4f9b\u900f\u660e\u3001\u53ef\u91cd\u590d\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86CASP\u65b9\u6cd5\u7684\u516c\u5e73\u7eb5\u5411\u6bd4\u8f83\uff0c\u652f\u6301\u5b8c\u6574\u7684\u53cd\u5e94\u4fe1\u606f\u5b66\u7ba1\u9053\u6d4b\u8bd5\uff0c\u964d\u4f4e\u4e86\u5b9e\u9645\u5408\u6210\u89c4\u5212\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u7a33\u5065\u6027\u80fd\u8bc4\u4f30\u7684\u95e8\u69db\u3002"}}
{"id": "2601.01966", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01966", "abs": "https://arxiv.org/abs/2601.01966", "authors": ["Bo Yin", "Qi Li", "Runpeng Yu", "Xinchao Wang"], "title": "Refinement Provenance Inference: Detecting LLM-Refined Training Prompts from Model Behavior", "comment": null, "summary": "Instruction tuning increasingly relies on LLM-based prompt refinement, where prompts in the training corpus are selectively rewritten by an external refiner to improve clarity and instruction alignment. This motivates an instance-level audit problem: for a fine-tuned model and a training prompt-response pair, can we infer whether the model was trained on the original prompt or its LLM-refined version within a mixed corpus? This matters for dataset governance and dispute resolution when training data are contested. However, it is non-trivial in practice: refined and raw instances are interleaved in the training corpus with unknown, source-dependent mixture ratios, making it harder to develop provenance methods that generalize across models and training setups. In this paper, we formalize this audit task as Refinement Provenance Inference (RPI) and show that prompt refinement yields stable, detectable shifts in teacher-forced token distributions, even when semantic differences are not obvious. Building on this phenomenon, we propose RePro, a logit-based provenance framework that fuses teacher-forced likelihood features with logit-ranking signals. During training, RePro learns a transferable representation via shadow fine-tuning, and uses a lightweight linear head to infer provenance on unseen victims without training-data access. Empirically, RePro consistently attains strong performance and transfers well across refiners, suggesting that it exploits refiner-agnostic distribution shifts rather than rewrite-style artifacts.", "AI": {"tldr": "\u63d0\u51faRefinement Provenance Inference (RPI)\u4efb\u52a1\uff0c\u5f00\u53d1RePro\u6846\u67b6\u901a\u8fc7\u6559\u5e08\u5f3a\u5236\u4f3c\u7136\u7279\u5f81\u548clogit\u6392\u5e8f\u4fe1\u53f7\u63a8\u65ad\u8bad\u7ec3\u6570\u636e\u4e2d\u63d0\u793a\u662f\u5426\u7ecf\u8fc7LLM\u7cbe\u70bc", "motivation": "\u6307\u4ee4\u8c03\u4f18\u8d8a\u6765\u8d8a\u591a\u4f9d\u8d56LLM\u63d0\u793a\u7cbe\u70bc\uff0c\u9700\u8981\u5b9e\u4f8b\u7ea7\u5ba1\u8ba1\u6765\u63a8\u65ad\u6a21\u578b\u8bad\u7ec3\u65f6\u4f7f\u7528\u7684\u662f\u539f\u59cb\u63d0\u793a\u8fd8\u662f\u7cbe\u70bc\u7248\u672c\uff0c\u8fd9\u5bf9\u6570\u636e\u96c6\u6cbb\u7406\u548c\u4e89\u8bae\u89e3\u51b3\u5f88\u91cd\u8981", "method": "\u63d0\u51faRePro\u6846\u67b6\uff0c\u5229\u7528\u63d0\u793a\u7cbe\u70bc\u5bfc\u81f4\u7684\u6559\u5e08\u5f3a\u5236token\u5206\u5e03\u7a33\u5b9a\u504f\u79fb\uff0c\u878d\u5408\u6559\u5e08\u5f3a\u5236\u4f3c\u7136\u7279\u5f81\u548clogit\u6392\u5e8f\u4fe1\u53f7\uff0c\u901a\u8fc7\u5f71\u5b50\u5fae\u8c03\u5b66\u4e60\u53ef\u8fc1\u79fb\u8868\u793a\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u7ebf\u6027\u5934\u8fdb\u884c\u63a8\u65ad", "result": "RePro\u5728\u4e0d\u540c\u7cbe\u70bc\u5668\u4e0a\u8868\u73b0\u7a33\u5b9a\u4e14\u53ef\u8fc1\u79fb\u6027\u597d\uff0c\u8868\u660e\u5176\u5229\u7528\u4e86\u7cbe\u70bc\u5668\u65e0\u5173\u7684\u5206\u5e03\u504f\u79fb\u800c\u975e\u6539\u5199\u98ce\u683c\u4f2a\u5f71", "conclusion": "\u63d0\u793a\u7cbe\u70bc\u4f1a\u4ea7\u751f\u53ef\u68c0\u6d4b\u7684\u5206\u5e03\u504f\u79fb\uff0cRePro\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3RPI\u4efb\u52a1\uff0c\u4e3a\u8bad\u7ec3\u6570\u636e\u6765\u6e90\u63a8\u65ad\u63d0\u4f9b\u5b9e\u7528\u65b9\u6cd5"}}
{"id": "2601.01979", "categories": ["cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2601.01979", "abs": "https://arxiv.org/abs/2601.01979", "authors": ["Julie Keisler", "Anastase Alexandre Charantonis", "Yannig Goude", "Boutheina Oueslati", "Claire Monteleoni"], "title": "SerpentFlow: Generative Unpaired Domain Alignment via Shared-Structure Decomposition", "comment": null, "summary": "Domain alignment refers broadly to learning correspondences between data distributions from distinct domains. In this work, we focus on a setting where domains share underlying structural patterns despite differences in their specific realizations. The task is particularly challenging in the absence of paired observations, which removes direct supervision across domains. We introduce a generative framework, called SerpentFlow (SharEd-structuRe decomPosition for gEnerative domaiN adapTation), for unpaired domain alignment. SerpentFlow decomposes data within a latent space into a shared component common to both domains and a domain-specific one. By isolating the shared structure and replacing the domain-specific component with stochastic noise, we construct synthetic training pairs between shared representations and target-domain samples, thereby enabling the use of conditional generative models that are traditionally restricted to paired settings. We apply this approach to super-resolution tasks, where the shared component naturally corresponds to low-frequency content while high-frequency details capture domain-specific variability. The cutoff frequency separating low- and high-frequency components is determined automatically using a classifier-based criterion, ensuring a data-driven and domain-adaptive decomposition. By generating pseudo-pairs that preserve low-frequency structures while injecting stochastic high-frequency realizations, we learn the conditional distribution of the target domain given the shared representation. We implement SerpentFlow using Flow Matching as the generative pipeline, although the framework is compatible with other conditional generative approaches. Experiments on synthetic images, physical process simulations, and a climate downscaling task demonstrate that the method effectively reconstructs high-frequency structures consistent with underlying low-frequency patterns, supporting shared-structure decomposition as an effective strategy for unpaired domain alignment.", "AI": {"tldr": "SerpentFlow\uff1a\u4e00\u79cd\u7528\u4e8e\u65e0\u914d\u5bf9\u57df\u5bf9\u9f50\u7684\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6570\u636e\u5206\u89e3\u4e3a\u5171\u4eab\u7ed3\u6784\u548c\u57df\u7279\u5b9a\u6210\u5206\uff0c\u751f\u6210\u5408\u6210\u8bad\u7ec3\u5bf9\uff0c\u4f7f\u4f20\u7edf\u9700\u8981\u914d\u5bf9\u6570\u636e\u7684\u6761\u4ef6\u751f\u6210\u6a21\u578b\u80fd\u7528\u4e8e\u65e0\u914d\u5bf9\u573a\u666f\u3002", "motivation": "\u5728\u65e0\u914d\u5bf9\u89c2\u6d4b\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u57df\u5bf9\u9f50\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u7f3a\u4e4f\u8de8\u57df\u7684\u76f4\u63a5\u76d1\u7763\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u914d\u5bf9\u6570\u636e\uff0c\u9650\u5236\u4e86\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faSerpentFlow\u6846\u67b6\uff0c\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5c06\u6570\u636e\u5206\u89e3\u4e3a\u5171\u4eab\u6210\u5206\uff08\u8de8\u57df\u5171\u6709\uff09\u548c\u57df\u7279\u5b9a\u6210\u5206\u3002\u901a\u8fc7\u7528\u968f\u673a\u566a\u58f0\u66ff\u6362\u57df\u7279\u5b9a\u6210\u5206\uff0c\u6784\u5efa\u5171\u4eab\u8868\u793a\u4e0e\u76ee\u6807\u57df\u6837\u672c\u4e4b\u95f4\u7684\u5408\u6210\u8bad\u7ec3\u5bf9\uff0c\u4ece\u800c\u4f7f\u7528\u6761\u4ef6\u751f\u6210\u6a21\u578b\u3002\u5728\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\uff0c\u5171\u4eab\u6210\u5206\u5bf9\u5e94\u4f4e\u9891\u5185\u5bb9\uff0c\u9ad8\u9891\u7ec6\u8282\u5bf9\u5e94\u57df\u7279\u5b9a\u53d8\u5f02\u6027\uff0c\u4f7f\u7528\u5206\u7c7b\u5668\u51c6\u5219\u81ea\u52a8\u786e\u5b9a\u5206\u5272\u9891\u7387\u3002", "result": "\u5728\u5408\u6210\u56fe\u50cf\u3001\u7269\u7406\u8fc7\u7a0b\u6a21\u62df\u548c\u6c14\u5019\u964d\u5c3a\u5ea6\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u91cd\u5efa\u4e0e\u5e95\u5c42\u4f4e\u9891\u6a21\u5f0f\u4e00\u81f4\u7684\u9ad8\u9891\u7ed3\u6784\uff0c\u652f\u6301\u5171\u4eab\u7ed3\u6784\u5206\u89e3\u4f5c\u4e3a\u65e0\u914d\u5bf9\u57df\u5bf9\u9f50\u7684\u6709\u6548\u7b56\u7565\u3002", "conclusion": "\u5171\u4eab\u7ed3\u6784\u5206\u89e3\u662f\u89e3\u51b3\u65e0\u914d\u5bf9\u57df\u5bf9\u9f50\u95ee\u9898\u7684\u6709\u6548\u65b9\u6cd5\uff0cSerpentFlow\u6846\u67b6\u901a\u8fc7\u751f\u6210\u5408\u6210\u8bad\u7ec3\u5bf9\uff0c\u4f7f\u6761\u4ef6\u751f\u6210\u6a21\u578b\u80fd\u5e94\u7528\u4e8e\u65e0\u914d\u5bf9\u573a\u666f\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2601.02022", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02022", "abs": "https://arxiv.org/abs/2601.02022", "authors": ["Yifan Zhu", "John C. Duchi", "Benjamin Van Roy"], "title": "Prior Diffusiveness and Regret in the Linear-Gaussian Bandit", "comment": null, "summary": "We prove that Thompson sampling exhibits $\\tilde{O}(\u03c3d \\sqrt{T} + d r \\sqrt{\\mathrm{Tr}(\u03a3_0)})$ Bayesian regret in the linear-Gaussian bandit with a $\\mathcal{N}(\u03bc_0, \u03a3_0)$ prior distribution on the coefficients, where $d$ is the dimension, $T$ is the time horizon, $r$ is the maximum $\\ell_2$ norm of the actions, and $\u03c3^2$ is the noise variance. In contrast to existing regret bounds, this shows that to within logarithmic factors, the prior-dependent ``burn-in'' term $d r \\sqrt{\\mathrm{Tr}(\u03a3_0)}$ decouples additively from the minimax (long run) regret $\u03c3d \\sqrt{T}$. Previous regret bounds exhibit a multiplicative dependence on these terms. We establish these results via a new ``elliptical potential'' lemma, and also provide a lower bound indicating that the burn-in term is unavoidable.", "AI": {"tldr": "Thompson\u91c7\u6837\u5728\u7ebf\u6027\u9ad8\u65afbandit\u4e2d\u5b9e\u73b0\u4e86$\\tilde{O}(\u03c3d \\sqrt{T} + d r \\sqrt{\\mathrm{Tr}(\u03a3_0)})$\u7684\u8d1d\u53f6\u65af\u9057\u61be\uff0c\u5176\u4e2dburn-in\u9879\u4e0eminimax\u9057\u61be\u9879\u5448\u52a0\u6cd5\u5173\u7cfb\u800c\u975e\u4e58\u6cd5\u5173\u7cfb\u3002", "motivation": "\u73b0\u6709Thompson\u91c7\u6837\u5728\u7ebf\u6027\u9ad8\u65afbandit\u4e2d\u7684\u9057\u61be\u754c\u4e2d\uff0c\u5148\u9a8c\u4f9d\u8d56\u7684\"burn-in\"\u9879$d r \\sqrt{\\mathrm{Tr}(\u03a3_0)}$\u4e0eminimax\u9057\u61be\u9879$\u03c3d \\sqrt{T}$\u5448\u4e58\u6cd5\u5173\u7cfb\u3002\u672c\u6587\u65e8\u5728\u8bc1\u660e\u8fd9\u4e24\u4e2a\u9879\u5b9e\u9645\u4e0a\u53ef\u4ee5\u89e3\u8026\u4e3a\u52a0\u6cd5\u5173\u7cfb\uff0c\u4ece\u800c\u63d0\u4f9b\u66f4\u7d27\u7684\u9057\u61be\u754c\u3002", "method": "\u901a\u8fc7\u65b0\u7684\"\u692d\u5706\u52bf\u80fd\"\u5f15\u7406\u6765\u5206\u6790Thompson\u91c7\u6837\u5728\u7ebf\u6027\u9ad8\u65afbandit\u4e2d\u7684\u6027\u80fd\u3002\u8be5\u5f15\u7406\u5e2e\u52a9\u5206\u79bb\u5148\u9a8c\u4fe1\u606f\u5bf9\u9057\u61be\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u4f9b\u4e86\u7406\u8bba\u8bc1\u660e\u6846\u67b6\u3002", "result": "\u8bc1\u660e\u4e86Thompson\u91c7\u6837\u5177\u6709$\\tilde{O}(\u03c3d \\sqrt{T} + d r \\sqrt{\\mathrm{Tr}(\u03a3_0)})$\u7684\u8d1d\u53f6\u65af\u9057\u61be\u4e0a\u754c\uff0c\u5176\u4e2d$\\tilde{O}$\u9690\u85cf\u4e86\u5bf9\u6570\u56e0\u5b50\u3002\u540c\u65f6\u63d0\u4f9b\u4e86\u4e0b\u754c\u8bc1\u660e\uff0c\u8868\u660eburn-in\u9879\u662f\u4e0d\u53ef\u907f\u514d\u7684\u3002", "conclusion": "\u5728\u7ebf\u6027\u9ad8\u65afbandit\u4e2d\uff0cThompson\u91c7\u6837\u7684\u5148\u9a8c\u4f9d\u8d56burn-in\u9879\u4e0e\u957f\u671fminimax\u9057\u61be\u9879\u53ef\u4ee5\u89e3\u8026\u4e3a\u52a0\u6cd5\u5173\u7cfb\uff0c\u8fd9\u6539\u8fdb\u4e86\u73b0\u6709\u4e58\u6cd5\u4f9d\u8d56\u7684\u9057\u61be\u754c\uff0c\u5e76\u901a\u8fc7\u692d\u5706\u52bf\u80fd\u5f15\u7406\u548c\u5339\u914d\u4e0b\u754c\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684\u7406\u8bba\u5206\u6790\u3002"}}
{"id": "2601.02036", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02036", "abs": "https://arxiv.org/abs/2601.02036", "authors": ["Yiyang Wang", "Xi Chen", "Xiaogang Xu", "Yu Liu", "Hengshuang Zhao"], "title": "GDRO: Group-level Reward Post-training Suitable for Diffusion Models", "comment": null, "summary": "Recent advancements adopt online reinforcement learning (RL) from LLMs to text-to-image rectified flow diffusion models for reward alignment. The use of group-level rewards successfully aligns the model with the targeted reward. However, it faces challenges including low efficiency, dependency on stochastic samplers, and reward hacking. The problem is that rectified flow models are fundamentally different from LLMs: 1) For efficiency, online image sampling takes much more time and dominates the time of training. 2) For stochasticity, rectified flow is deterministic once the initial noise is fixed. Aiming at these problems and inspired by the effects of group-level rewards from LLMs, we design Group-level Direct Reward Optimization (GDRO). GDRO is a new post-training paradigm for group-level reward alignment that combines the characteristics of rectified flow models. Through rigorous theoretical analysis, we point out that GDRO supports full offline training that saves the large time cost for image rollout sampling. Also, it is diffusion-sampler-independent, which eliminates the need for the ODE-to-SDE approximation to obtain stochasticity. We also empirically study the reward hacking trap that may mislead the evaluation, and involve this factor in the evaluation using a corrected score that not only considers the original evaluation reward but also the trend of reward hacking. Extensive experiments demonstrate that GDRO effectively and efficiently improves the reward score of the diffusion model through group-wise offline optimization across the OCR and GenEval tasks, while demonstrating strong stability and robustness in mitigating reward hacking.", "AI": {"tldr": "\u63d0\u51faGDRO\u65b9\u6cd5\uff0c\u9488\u5bf9\u6587\u672c\u5230\u56fe\u50cf\u6574\u6d41\u6d41\u6269\u6563\u6a21\u578b\u7684\u5956\u52b1\u5bf9\u9f50\u95ee\u9898\uff0c\u901a\u8fc7\u7fa4\u4f53\u7ea7\u79bb\u7ebf\u4f18\u5316\u89e3\u51b3\u6548\u7387\u4f4e\u3001\u4f9d\u8d56\u968f\u673a\u91c7\u6837\u5668\u548c\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u91c7\u7528\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u5956\u52b1\u5bf9\u9f50\uff0c\u4f46\u9762\u4e34\u6548\u7387\u4f4e\u3001\u4f9d\u8d56\u968f\u673a\u91c7\u6837\u5668\u548c\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\u3002\u6574\u6d41\u6d41\u6a21\u578b\u4e0eLLMs\u6709\u672c\u8d28\u5dee\u5f02\uff1a1) \u5728\u7ebf\u56fe\u50cf\u91c7\u6837\u8017\u65f6\u5de8\u5927\uff1b2) \u6574\u6d41\u6d41\u662f\u786e\u5b9a\u6027\u7684\uff08\u4e00\u65e6\u521d\u59cb\u566a\u58f0\u56fa\u5b9a\uff09\u3002", "method": "\u8bbe\u8ba1Group-level Direct Reward Optimization (GDRO)\uff0c\u7ed3\u5408\u6574\u6d41\u6d41\u6a21\u578b\u7279\u6027\u7684\u7fa4\u4f53\u7ea7\u5956\u52b1\u5bf9\u9f50\u540e\u8bad\u7ec3\u8303\u5f0f\u3002\u652f\u6301\u5b8c\u5168\u79bb\u7ebf\u8bad\u7ec3\uff0c\u65e0\u9700\u56fe\u50cf\u91c7\u6837\uff1b\u6269\u6563\u91c7\u6837\u5668\u72ec\u7acb\uff0c\u65e0\u9700ODE-to-SDE\u8fd1\u4f3c\uff1b\u5f15\u5165\u4fee\u6b63\u8bc4\u5206\u8003\u8651\u5956\u52b1\u9ed1\u5ba2\u8d8b\u52bf\u3002", "result": "GDRO\u5728OCR\u548cGenEval\u4efb\u52a1\u4e2d\u6709\u6548\u63d0\u5347\u6269\u6563\u6a21\u578b\u7684\u5956\u52b1\u5206\u6570\uff0c\u540c\u65f6\u5c55\u793a\u51fa\u5f3a\u5927\u7684\u7a33\u5b9a\u6027\u548c\u9c81\u68d2\u6027\uff0c\u80fd\u591f\u7f13\u89e3\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\u3002", "conclusion": "GDRO\u4e3a\u6587\u672c\u5230\u56fe\u50cf\u6574\u6d41\u6d41\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7a33\u5b9a\u4e14\u9c81\u68d2\u7684\u7fa4\u4f53\u7ea7\u5956\u52b1\u5bf9\u9f50\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5728\u7ebfRL\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2601.02037", "categories": ["cs.LG", "cs.DB"], "pdf": "https://arxiv.org/pdf/2601.02037", "abs": "https://arxiv.org/abs/2601.02037", "authors": ["Wei Hu", "Zewei Yu", "Jianqiu Xu"], "title": "Multivariate Time-series Anomaly Detection via Dynamic Model Pool & Ensembling", "comment": null, "summary": "Multivariate time-series (MTS) anomaly detection is critical in domains such as service monitor, IoT, and network security. While multi-model methods based on selection or ensembling outperform single-model ones, they still face limitations: (i) selection methods rely on a single chosen model and are sensitive to the strategy; (ii) ensembling methods often combine all models or are restricted to univariate data; and (iii) most methods depend on fixed data dimensionality, limiting scalability. To address these, we propose DMPEAD, a Dynamic Model Pool and Ensembling framework for MTS Anomaly Detection. The framework first (i) constructs a diverse model pool via parameter transfer and diversity metric, then (ii) updates it with a meta-model and similarity-based strategy for adaptive pool expansion, subset selection, and pool merging, finally (iii) ensembles top-ranked models through proxy metric ranking and top-k aggregation in the selected subset, outputting the final anomaly detection result. Extensive experiments on 8 real-world datasets show that our model outperforms all baselines, demonstrating superior adaptability and scalability.", "AI": {"tldr": "DMPEAD\uff1a\u4e00\u79cd\u7528\u4e8e\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u7684\u52a8\u6001\u6a21\u578b\u6c60\u96c6\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u591a\u6837\u5316\u6a21\u578b\u6c60\u3001\u52a8\u6001\u66f4\u65b0\u548c\u96c6\u6210\u6392\u540d\u9760\u524d\u7684\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u591a\u6a21\u578b\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u7684\u591a\u6a21\u578b\u65b9\u6cd5\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u9009\u62e9\u65b9\u6cd5\u4f9d\u8d56\u5355\u4e00\u6a21\u578b\u4e14\u5bf9\u7b56\u7565\u654f\u611f\uff1b2\uff09\u96c6\u6210\u65b9\u6cd5\u901a\u5e38\u7ec4\u5408\u6240\u6709\u6a21\u578b\u6216\u4ec5\u9650\u4e8e\u5355\u53d8\u91cf\u6570\u636e\uff1b3\uff09\u5927\u591a\u6570\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u6570\u636e\u7ef4\u5ea6\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51faDMPEAD\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6b65\u9aa4\uff1a1\uff09\u901a\u8fc7\u53c2\u6570\u4f20\u9012\u548c\u591a\u6837\u6027\u5ea6\u91cf\u6784\u5efa\u591a\u6837\u5316\u6a21\u578b\u6c60\uff1b2\uff09\u4f7f\u7528\u5143\u6a21\u578b\u548c\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u7b56\u7565\u52a8\u6001\u66f4\u65b0\u6a21\u578b\u6c60\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u6269\u5c55\u3001\u5b50\u96c6\u9009\u62e9\u548c\u6c60\u5408\u5e76\uff1b3\uff09\u901a\u8fc7\u4ee3\u7406\u6307\u6807\u6392\u540d\u548ctop-k\u805a\u5408\u5728\u9009\u5b9a\u5b50\u96c6\u4e2d\u96c6\u6210\u6392\u540d\u9760\u524d\u7684\u6a21\u578b\uff0c\u8f93\u51fa\u6700\u7ec8\u5f02\u5e38\u68c0\u6d4b\u7ed3\u679c\u3002", "result": "\u57288\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cDMPEAD\u6a21\u578b\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u9002\u5e94\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "DMPEAD\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u591a\u6a21\u578b\u65b9\u6cd5\u5728\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u52a8\u6001\u6a21\u578b\u6c60\u6784\u5efa\u548c\u96c6\u6210\u7b56\u7565\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2601.02050", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02050", "abs": "https://arxiv.org/abs/2601.02050", "authors": ["Yanhai Gan", "Yipeng Chen", "Ning Li", "Xingguo Liu", "Junyu Dong", "Xianyao Chen"], "title": "Explore the Ideology of Deep Learning in ENSO Forecasts", "comment": "5 figures. Code available at https://github.com/liuxingguo9349/pptv-enso-env", "summary": "The El Ni{~n}o-Southern Oscillation (ENSO) exerts profound influence on global climate variability, yet its prediction remains a grand challenge. Recent advances in deep learning have significantly improved forecasting skill, but the opacity of these models hampers scientific trust and operational deployment. Here, we introduce a mathematically grounded interpretability framework based on bounded variation function. By rescuing the \"dead\" neurons from the saturation zone of the activation function, we enhance the model's expressive capacity. Our analysis reveals that ENSO predictability emerges dominantly from the tropical Pacific, with contributions from the Indian and Atlantic Oceans, consistent with physical understanding. Controlled experiments affirm the robustness of our method and its alignment with established predictors. Notably, we probe the persistent Spring Predictability Barrier (SPB), finding that despite expanded sensitivity during spring, predictive performance declines-likely due to suboptimal variable selection. These results suggest that incorporating additional ocean-atmosphere variables may help transcend SPB limitations and advance long-range ENSO prediction.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u57fa\u4e8e\u6709\u754c\u53d8\u5dee\u51fd\u6570\u7684\u53ef\u89e3\u91ca\u6027\u6846\u67b6\uff0c\u901a\u8fc7\u6fc0\u6d3b\u51fd\u6570\u9971\u548c\u533a\"\u590d\u6d3b\"\u795e\u7ecf\u5143\uff0c\u589e\u5f3a\u6a21\u578b\u8868\u8fbe\u80fd\u529b\uff0c\u63ed\u793aENSO\u53ef\u9884\u6d4b\u6027\u4e3b\u8981\u6765\u81ea\u70ed\u5e26\u592a\u5e73\u6d0b\uff0c\u5370\u5ea6\u6d0b\u548c\u5927\u897f\u6d0b\u4e5f\u6709\u8d21\u732e\uff0c\u4e0e\u7269\u7406\u7406\u89e3\u4e00\u81f4\u3002", "motivation": "ENSO\u5bf9\u5168\u7403\u6c14\u5019\u53d8\u7387\u6709\u6df1\u8fdc\u5f71\u54cd\uff0c\u4f46\u5176\u9884\u6d4b\u4ecd\u662f\u91cd\u5927\u6311\u6218\u3002\u6df1\u5ea6\u5b66\u4e60\u867d\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u6280\u80fd\uff0c\u4f46\u6a21\u578b\u7684\u4e0d\u900f\u660e\u6027\u963b\u788d\u4e86\u79d1\u5b66\u4fe1\u4efb\u548c\u4e1a\u52a1\u90e8\u7f72\uff0c\u9700\u8981\u5efa\u7acb\u6570\u5b66\u57fa\u7840\u7684\u53ef\u89e3\u91ca\u6027\u6846\u67b6\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u6709\u754c\u53d8\u5dee\u51fd\u6570\u7684\u6570\u5b66\u57fa\u7840\u53ef\u89e3\u91ca\u6027\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u6fc0\u6d3b\u51fd\u6570\u9971\u548c\u533a\"\u590d\u6d3b\"\u795e\u7ecf\u5143\u6765\u589e\u5f3a\u6a21\u578b\u8868\u8fbe\u80fd\u529b\uff0c\u8fdb\u884c\u63a7\u5236\u5b9e\u9a8c\u9a8c\u8bc1\u65b9\u6cd5\u7a33\u5065\u6027\u3002", "result": "\u5206\u6790\u663e\u793aENSO\u53ef\u9884\u6d4b\u6027\u4e3b\u8981\u6765\u81ea\u70ed\u5e26\u592a\u5e73\u6d0b\uff0c\u5370\u5ea6\u6d0b\u548c\u5927\u897f\u6d0b\u4e5f\u6709\u8d21\u732e\uff0c\u4e0e\u7269\u7406\u7406\u89e3\u4e00\u81f4\u3002\u63a7\u5236\u5b9e\u9a8c\u8bc1\u5b9e\u65b9\u6cd5\u7a33\u5065\u4e14\u4e0e\u5df2\u77e5\u9884\u6d4b\u56e0\u5b50\u4e00\u81f4\u3002\u53d1\u73b0\u6625\u5b63\u53ef\u9884\u6d4b\u6027\u969c\u788d\u671f\u95f4\u654f\u611f\u6027\u6269\u5927\u4f46\u9884\u6d4b\u6027\u80fd\u4e0b\u964d\uff0c\u53ef\u80fd\u6e90\u4e8e\u6b21\u4f18\u53d8\u91cf\u9009\u62e9\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0c\u7eb3\u5165\u989d\u5916\u7684\u6d77\u6d0b-\u5927\u6c14\u53d8\u91cf\u53ef\u80fd\u6709\u52a9\u4e8e\u8d85\u8d8a\u6625\u5b63\u53ef\u9884\u6d4b\u6027\u969c\u788d\u9650\u5236\uff0c\u63a8\u8fdbENSO\u957f\u671f\u9884\u6d4b\u3002\u8be5\u65b9\u6cd5\u4e3a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u63d0\u4f9b\u4e86\u6570\u5b66\u57fa\u7840\u7684\u53ef\u89e3\u91ca\u6027\u6846\u67b6\uff0c\u589e\u5f3a\u4e86\u79d1\u5b66\u4fe1\u4efb\u3002"}}
{"id": "2601.02080", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02080", "abs": "https://arxiv.org/abs/2601.02080", "authors": ["Yizhi Liu"], "title": "The Homogeneity Trap: Spectral Collapse in Doubly-Stochastic Deep Networks", "comment": null, "summary": "Doubly-stochastic matrices (DSM) are increasingly utilized in structure-preserving deep architectures -- such as Optimal Transport layers and Sinkhorn-based attention -- to enforce numerical stability and probabilistic interpretability. In this work, we identify a critical spectral degradation phenomenon inherent to these constraints, termed the Homogeneity Trap. We demonstrate that the maximum-entropy bias, typical of Sinkhorn-based projections, drives the mixing operator towards the uniform barycenter, thereby suppressing the subdominant singular value \u03c3_2 and filtering out high-frequency feature components. We derive a spectral bound linking \u03c3_2 to the network's effective depth, showing that high-entropy constraints restrict feature transformation to a shallow effective receptive field. Furthermore, we formally demonstrate that Layer Normalization fails to mitigate this collapse in noise-dominated regimes; specifically, when spectral filtering degrades the Signal-to-Noise Ratio (SNR) below a critical threshold, geometric structure is irreversibly lost to noise-induced orthogonal collapse. Our findings highlight a fundamental trade-off between entropic stability and spectral expressivity in DSM-constrained networks.", "AI": {"tldr": "\u672c\u6587\u53d1\u73b0\u53cc\u968f\u673a\u77e9\u9635\u7ea6\u675f\u5728\u6df1\u5ea6\u67b6\u6784\u4e2d\u5b58\u5728\"\u540c\u8d28\u6027\u9677\u9631\"\uff1a\u6700\u5927\u71b5\u504f\u7f6e\u5bfc\u81f4\u6df7\u5408\u7b97\u5b50\u8d8b\u5411\u5747\u5300\u91cd\u5fc3\uff0c\u6291\u5236\u6b21\u4e3b\u5bfc\u5947\u5f02\u503c\uff0c\u9650\u5236\u7f51\u7edc\u7684\u6709\u6548\u611f\u53d7\u91ce\uff0c\u4e14\u5c42\u5f52\u4e00\u5316\u65e0\u6cd5\u7f13\u89e3\u566a\u58f0\u4e3b\u5bfc\u4e0b\u7684\u6b63\u4ea4\u5d29\u6e83\u3002", "motivation": "\u53cc\u968f\u673a\u77e9\u9635\u5728\u7ed3\u6784\u4fdd\u6301\u7684\u6df1\u5ea6\u67b6\u6784\u4e2d\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u4f5c\u8005\u53d1\u73b0\u8fd9\u4e9b\u7ea6\u675f\u5b58\u5728\u5173\u952e\u7684\u8c31\u9000\u5316\u73b0\u8c61\uff0c\u79f0\u4e3a\"\u540c\u8d28\u6027\u9677\u9631\"\uff0c\u9700\u8981\u6df1\u5165\u5206\u6790\u5176\u7406\u8bba\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u53cc\u968f\u673a\u77e9\u9635\u7684\u8c31\u7279\u6027\uff0c\u63a8\u5bfc\u4e86\u6b21\u4e3b\u5bfc\u5947\u5f02\u503c\u03c3_2\u4e0e\u7f51\u7edc\u6709\u6548\u6df1\u5ea6\u7684\u8c31\u754c\uff0c\u5e76\u5f62\u5f0f\u5316\u8bc1\u660e\u4e86\u5c42\u5f52\u4e00\u5316\u5728\u566a\u58f0\u4e3b\u5bfc\u673a\u5236\u4e0b\u7684\u5931\u6548\u6761\u4ef6\u3002", "result": "\u53d1\u73b0Sinkhorn\u6295\u5f71\u7684\u6700\u5927\u71b5\u504f\u7f6e\u4f7f\u6df7\u5408\u7b97\u5b50\u8d8b\u5411\u5747\u5300\u91cd\u5fc3\uff0c\u6291\u5236\u03c3_2\u5e76\u8fc7\u6ee4\u9ad8\u9891\u7279\u5f81\u5206\u91cf\uff1b\u8bc1\u660e\u4e86\u5f53\u4fe1\u566a\u6bd4\u4f4e\u4e8e\u4e34\u754c\u9608\u503c\u65f6\uff0c\u51e0\u4f55\u7ed3\u6784\u4f1a\u4e0d\u53ef\u9006\u5730\u4e22\u5931\u5230\u566a\u58f0\u8bf1\u5bfc\u7684\u6b63\u4ea4\u5d29\u6e83\u4e2d\u3002", "conclusion": "\u53cc\u968f\u673a\u77e9\u9635\u7ea6\u675f\u7684\u7f51\u7edc\u5b58\u5728\u71b5\u7a33\u5b9a\u6027\u4e0e\u8c31\u8868\u8fbe\u6027\u4e4b\u95f4\u7684\u57fa\u672c\u6743\u8861\uff0c\u9700\u8981\u91cd\u65b0\u601d\u8003\u5982\u4f55\u5728\u4fdd\u6301\u6570\u503c\u7a33\u5b9a\u6027\u548c\u6982\u7387\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\u7ef4\u6301\u8db3\u591f\u7684\u8c31\u8868\u8fbe\u80fd\u529b\u3002"}}
{"id": "2601.02081", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02081", "abs": "https://arxiv.org/abs/2601.02081", "authors": ["Jiacheng Lyu", "Bihua Bao"], "title": "A Differentiable Adversarial Framework for Task-Aware Data Subsampling", "comment": "14 pages", "summary": "The proliferation of large-scale datasets poses a major computational challenge to model training. The traditional data subsampling method works as a static, task independent preprocessing step which usually discards information that is critical to downstream prediction. In this paper, we introduces the antagonistic soft selection subsampling (ASSS) framework as is a novel paradigm that reconstructs data reduction into a differentiable end-to-end learning problem. ASSS uses the adversarial game between selector network and task network, and selector network learning assigns continuous importance weights to samples. This direct optimization implemented by Gumbel-Softmax relaxation allows the selector to identify and retain samples with the maximum amount of information for a specific task target under the guidance of the loss function that balances the fidelity and sparsity of the prediction. Theoretical analysis links this framework with the information bottleneck principle. Comprehensive experiments on four large-scale real world datasets show that ASSS has always been better than heuristic subsampling baselines such as clustering and nearest neighbor thinning in maintaining model performance. It is worth noting that ASSS can not only match, but also sometimes exceed the training performance of the entire dataset, showcasing the effect of intelligent denoising. This work establishes task aware data subsampling as a learnable component, providing a principled solution for effective large-scale data learning.", "AI": {"tldr": "\u63d0\u51fa\u5bf9\u6297\u6027\u8f6f\u9009\u62e9\u4e0b\u91c7\u6837(ASSS)\u6846\u67b6\uff0c\u5c06\u6570\u636e\u7f29\u51cf\u91cd\u6784\u4e3a\u53ef\u5fae\u5206\u7684\u7aef\u5230\u7aef\u5b66\u4e60\u95ee\u9898\uff0c\u901a\u8fc7\u9009\u62e9\u5668\u7f51\u7edc\u4e0e\u4efb\u52a1\u7f51\u7edc\u7684\u5bf9\u6297\u535a\u5f08\u5b66\u4e60\u6837\u672c\u91cd\u8981\u6027\u6743\u91cd\uff0c\u5b9e\u73b0\u4efb\u52a1\u611f\u77e5\u7684\u667a\u80fd\u6570\u636e\u964d\u91c7\u6837\u3002", "motivation": "\u5927\u89c4\u6a21\u6570\u636e\u96c6\u5bf9\u6a21\u578b\u8bad\u7ec3\u5e26\u6765\u8ba1\u7b97\u6311\u6218\uff0c\u4f20\u7edf\u6570\u636e\u4e0b\u91c7\u6837\u65b9\u6cd5\u4f5c\u4e3a\u9759\u6001\u3001\u4efb\u52a1\u65e0\u5173\u7684\u9884\u5904\u7406\u6b65\u9aa4\u901a\u5e38\u4f1a\u4e22\u5f03\u5bf9\u4e0b\u6e38\u9884\u6d4b\u81f3\u5173\u91cd\u8981\u7684\u4fe1\u606f\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4fdd\u7559\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\u7684\u667a\u80fd\u6570\u636e\u7f29\u51cf\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u5bf9\u6297\u6027\u8f6f\u9009\u62e9\u4e0b\u91c7\u6837(ASSS)\u6846\u67b6\uff0c\u5305\u542b\u9009\u62e9\u5668\u7f51\u7edc\u548c\u4efb\u52a1\u7f51\u7edc\u3002\u9009\u62e9\u5668\u7f51\u7edc\u5b66\u4e60\u4e3a\u6837\u672c\u5206\u914d\u8fde\u7eed\u91cd\u8981\u6027\u6743\u91cd\uff0c\u901a\u8fc7Gumbel-Softmax\u677e\u5f1b\u5b9e\u73b0\u76f4\u63a5\u4f18\u5316\uff0c\u5728\u5e73\u8861\u9884\u6d4b\u4fdd\u771f\u5ea6\u548c\u7a00\u758f\u6027\u7684\u635f\u5931\u51fd\u6570\u6307\u5bfc\u4e0b\uff0c\u8bc6\u522b\u5e76\u4fdd\u7559\u5bf9\u7279\u5b9a\u4efb\u52a1\u76ee\u6807\u4fe1\u606f\u91cf\u6700\u5927\u7684\u6837\u672c\u3002", "result": "\u5728\u56db\u4e2a\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cASSS\u59cb\u7ec8\u4f18\u4e8e\u805a\u7c7b\u548c\u6700\u8fd1\u90bb\u7a00\u758f\u5316\u7b49\u542f\u53d1\u5f0f\u4e0b\u91c7\u6837\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cASSS\u4e0d\u4ec5\u80fd\u5339\u914d\uff0c\u6709\u65f6\u751a\u81f3\u80fd\u8d85\u8fc7\u4f7f\u7528\u6574\u4e2a\u6570\u636e\u96c6\u7684\u8bad\u7ec3\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u667a\u80fd\u53bb\u566a\u7684\u6548\u679c\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5c06\u4efb\u52a1\u611f\u77e5\u6570\u636e\u4e0b\u91c7\u6837\u786e\u7acb\u4e3a\u53ef\u5b66\u4e60\u7ec4\u4ef6\uff0c\u4e3a\u6709\u6548\u7684\u5927\u89c4\u6a21\u6570\u636e\u5b66\u4e60\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u89e3\u51b3\u65b9\u6848\uff0c\u5efa\u7acb\u4e86\u6570\u636e\u7f29\u51cf\u4e0e\u4fe1\u606f\u74f6\u9888\u539f\u7406\u7684\u7406\u8bba\u8054\u7cfb\u3002"}}
{"id": "2601.02094", "categories": ["cs.LG", "math.FA"], "pdf": "https://arxiv.org/pdf/2601.02094", "abs": "https://arxiv.org/abs/2601.02094", "authors": ["Hans Krupakar", "V A Kandappan"], "title": "Horizon Activation Mapping for Neural Networks in Time Series Forecasting", "comment": null, "summary": "Neural networks for time series forecasting have relied on error metrics and architecture-specific interpretability approaches for model selection that don't apply across models of different families. To interpret forecasting models agnostic to the types of layers across state-of-the-art model families, we introduce Horizon Activation Mapping (HAM), a visual interpretability technique inspired by grad-CAM that uses gradient norm averages to study the horizon's subseries where grad-CAM studies attention maps over image data. We introduce causal and anti-causal modes to calculate gradient update norm averages across subseries at every timestep and lines of proportionality signifying uniform distributions of the norm averages. Optimization landscape studies with respect to changes in batch sizes, early stopping, train-val-test splits, univariate forecasting and dropouts are studied with respect to performances and subseries in HAM. Interestingly, batch size based differences in activities seem to indicate potential for existence of an exponential approximation across them per epoch relative to each other. Multivariate forecasting models including MLP-based CycleNet, N-Linear, N-HITS, self attention-based FEDformer, Pyraformer, SSM-based SpaceTime and diffusion-based Multi-Resolution DDPM over different horizon sizes trained over the ETTm2 dataset are used for HAM plots in this study. NHITS' neural approximation theorem and SpaceTime's exponential autoregressive activities have been attributed to trends in HAM plots over their training, validation and test sets. In general, HAM can be used for granular model selection, validation set choices and comparisons across different neural network model families.", "AI": {"tldr": "\u63d0\u51faHAM\uff08Horizon Activation Mapping\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u8de8\u4e0d\u540c\u795e\u7ecf\u7f51\u7edc\u5bb6\u65cf\u7684\u65f6\u5e8f\u9884\u6d4b\u6a21\u578b\u7684\u53ef\u89c6\u5316\u89e3\u91ca\uff0c\u901a\u8fc7\u68af\u5ea6\u8303\u6570\u5e73\u5747\u5206\u6790\u5b50\u5e8f\u5217\u91cd\u8981\u6027\u3002", "motivation": "\u73b0\u6709\u65f6\u5e8f\u9884\u6d4b\u6a21\u578b\u4f9d\u8d56\u8bef\u5dee\u6307\u6807\u548c\u7279\u5b9a\u67b6\u6784\u7684\u89e3\u91ca\u65b9\u6cd5\uff0c\u65e0\u6cd5\u8de8\u4e0d\u540c\u6a21\u578b\u5bb6\u65cf\u8fdb\u884c\u6bd4\u8f83\u3002\u9700\u8981\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u53ef\u89c6\u5316\u89e3\u91ca\u6280\u672f\u6765\u7406\u89e3\u4e0d\u540c\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5728\u65f6\u5e8f\u9884\u6d4b\u4e2d\u7684\u884c\u4e3a\u3002", "method": "HAM\u53d7grad-CAM\u542f\u53d1\uff0c\u4f7f\u7528\u68af\u5ea6\u8303\u6570\u5e73\u5747\u6765\u7814\u7a76\u65f6\u95f4\u5e8f\u5217\u7684\u5b50\u5e8f\u5217\u91cd\u8981\u6027\u3002\u5f15\u5165\u56e0\u679c\u548c\u53cd\u56e0\u679c\u6a21\u5f0f\u8ba1\u7b97\u6bcf\u4e2a\u65f6\u95f4\u6b65\u7684\u68af\u5ea6\u66f4\u65b0\u8303\u6570\u5e73\u5747\uff0c\u5e76\u5206\u6790\u6bd4\u4f8b\u7ebf\u8868\u793a\u8303\u6570\u5e73\u5747\u7684\u5747\u5300\u5206\u5e03\u3002\u7814\u7a76\u4e86\u6279\u91cf\u5927\u5c0f\u3001\u65e9\u505c\u3001\u6570\u636e\u5206\u5272\u3001\u5355\u53d8\u91cf\u9884\u6d4b\u548cdropout\u7b49\u4f18\u5316\u666f\u89c2\u5bf9HAM\u7684\u5f71\u54cd\u3002", "result": "\u6279\u91cf\u5927\u5c0f\u5dee\u5f02\u663e\u793a\u5b58\u5728\u6307\u6570\u8fd1\u4f3c\u5173\u7cfb\u3002\u5728ETTm2\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u4e86\u591a\u79cd\u6a21\u578b\uff08CycleNet\u3001N-Linear\u3001N-HITS\u3001FEDformer\u3001Pyraformer\u3001SpaceTime\u3001Multi-Resolution DDPM\uff09\uff0c\u53d1\u73b0NHITS\u7684\u795e\u7ecf\u8fd1\u4f3c\u5b9a\u7406\u548cSpaceTime\u7684\u6307\u6570\u81ea\u56de\u5f52\u6d3b\u52a8\u4e0eHAM\u56fe\u4e2d\u7684\u8d8b\u52bf\u76f8\u5173\u3002", "conclusion": "HAM\u53ef\u7528\u4e8e\u7ec6\u7c92\u5ea6\u6a21\u578b\u9009\u62e9\u3001\u9a8c\u8bc1\u96c6\u9009\u62e9\u4ee5\u53ca\u8de8\u4e0d\u540c\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5bb6\u65cf\u7684\u6bd4\u8f83\uff0c\u4e3a\u65f6\u5e8f\u9884\u6d4b\u6a21\u578b\u63d0\u4f9b\u7edf\u4e00\u7684\u89e3\u91ca\u6846\u67b6\u3002"}}
{"id": "2601.02105", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02105", "abs": "https://arxiv.org/abs/2601.02105", "authors": ["Hyunjun Kim"], "title": "LION-DG: Layer-Informed Initialization with Deep Gradient Protocols for Accelerated Neural Network Training", "comment": null, "summary": "Weight initialization remains decisive for neural network optimization, yet existing methods are largely layer-agnostic. We study initialization for deeply-supervised architectures with auxiliary classifiers, where untrained auxiliary heads can destabilize early training through gradient interference.\n  We propose LION-DG, a layer-informed initialization that zero-initializes auxiliary classifier heads while applying standard He-initialization to the backbone. We prove that this implements Gradient Awakening: auxiliary gradients are exactly zero at initialization, then phase in naturally as weights grow -- providing an implicit warmup without hyperparameters.\n  Experiments on CIFAR-10 and CIFAR-100 with DenseNet-DS and ResNet-DS architectures demonstrate: (1) DenseNet-DS: +8.3% faster convergence on CIFAR-10 with comparable accuracy, (2) Hybrid approach: Combining LSUV with LION-DG achieves best accuracy (81.92% on CIFAR-10), (3) ResNet-DS: Positive speedup on CIFAR-100 (+11.3%) with side-tap auxiliary design.\n  We identify architecture-specific trade-offs and provide clear guidelines for practitioners. LION-DG is simple, requires zero hyperparameters, and adds no computational overhead.", "AI": {"tldr": "\u63d0\u51faLION-DG\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u9488\u5bf9\u6df1\u5ea6\u76d1\u7763\u67b6\u6784\u7684\u8f85\u52a9\u5206\u7c7b\u5668\u8fdb\u884c\u5c42\u611f\u77e5\u521d\u59cb\u5316\uff0c\u901a\u8fc7\u96f6\u521d\u59cb\u5316\u8f85\u52a9\u5934\u5b9e\u73b0\u68af\u5ea6\u5524\u9192\uff0c\u65e0\u9700\u8d85\u53c2\u6570\u5373\u53ef\u52a0\u901f\u6536\u655b\u3002", "motivation": "\u73b0\u6709\u6743\u91cd\u521d\u59cb\u5316\u65b9\u6cd5\u5927\u591a\u662f\u5c42\u65e0\u5173\u7684\uff0c\u800c\u6df1\u5ea6\u76d1\u7763\u67b6\u6784\u4e2d\u7684\u672a\u8bad\u7ec3\u8f85\u52a9\u5206\u7c7b\u5668\u5934\u4f1a\u901a\u8fc7\u68af\u5ea6\u5e72\u6270\u7834\u574f\u65e9\u671f\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\uff0c\u9700\u8981\u4e13\u95e8\u7684\u521d\u59cb\u5316\u7b56\u7565\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u63d0\u51faLION-DG\u521d\u59cb\u5316\u65b9\u6cd5\uff1a\u5bf9\u4e3b\u5e72\u7f51\u7edc\u5e94\u7528\u6807\u51c6\u7684He\u521d\u59cb\u5316\uff0c\u800c\u5bf9\u8f85\u52a9\u5206\u7c7b\u5668\u5934\u8fdb\u884c\u96f6\u521d\u59cb\u5316\u3002\u8fd9\u5b9e\u73b0\u4e86\"\u68af\u5ea6\u5524\u9192\"\u673a\u5236\u2014\u2014\u8f85\u52a9\u68af\u5ea6\u5728\u521d\u59cb\u5316\u65f6\u4e3a\u96f6\uff0c\u968f\u7740\u6743\u91cd\u589e\u957f\u81ea\u7136\u5f15\u5165\uff0c\u63d0\u4f9b\u9690\u5f0f\u7684\u9884\u70ed\u6548\u679c\u3002", "result": "\u5728CIFAR-10\u548cCIFAR-100\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff1aDenseNet-DS\u5728CIFAR-10\u4e0a\u6536\u655b\u901f\u5ea6\u63d0\u53478.3%\uff1bLSUV\u4e0eLION-DG\u7ed3\u5408\u5728CIFAR-10\u4e0a\u8fbe\u523081.92%\u7684\u6700\u4f73\u51c6\u786e\u7387\uff1bResNet-DS\u5728CIFAR-100\u4e0a\u83b7\u5f9711.3%\u7684\u52a0\u901f\u3002", "conclusion": "LION-DG\u65b9\u6cd5\u7b80\u5355\u3001\u65e0\u9700\u8d85\u53c2\u6570\u3001\u65e0\u8ba1\u7b97\u5f00\u9500\uff0c\u4e3a\u6df1\u5ea6\u76d1\u7763\u67b6\u6784\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u521d\u59cb\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u7ed9\u51fa\u4e86\u9488\u5bf9\u4e0d\u540c\u67b6\u6784\u7684\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2601.02106", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02106", "abs": "https://arxiv.org/abs/2601.02106", "authors": ["Ashish Rana", "Ammar Shaker", "Sascha Saralajew", "Takashi Suzuki", "Kosuke Yasuda", "Shintaro Kato", "Toshikazu Wada", "Toshiyuki Fujikawa", "Toru Kikutsuji"], "title": "Prototype-Based Learning for Healthcare: A Demonstration of Interpretable AI", "comment": "Accepted to the Demo Track at the IEEE International Conference on Data Mining (ICDM) 2025, where it received the Best Demo Award", "summary": "Despite recent advances in machine learning and explainable AI, a gap remains in personalized preventive healthcare: predictions, interventions, and recommendations should be both understandable and verifiable for all stakeholders in the healthcare sector. We present a demonstration of how prototype-based learning can address these needs. Our proposed framework, ProtoPal, features both front- and back-end modes; it achieves superior quantitative performance while also providing an intuitive presentation of interventions and their simulated outcomes.", "AI": {"tldr": "ProtoPal\u6846\u67b6\u7ed3\u5408\u539f\u578b\u5b66\u4e60\u548c\u53ef\u89e3\u91caAI\uff0c\u4e3a\u4e2a\u6027\u5316\u9884\u9632\u533b\u7597\u63d0\u4f9b\u53ef\u7406\u89e3\u3001\u53ef\u9a8c\u8bc1\u7684\u9884\u6d4b\u548c\u5e72\u9884\u65b9\u6848", "motivation": "\u5f53\u524d\u673a\u5668\u5b66\u4e60\u5728\u533b\u7597\u9886\u57df\u5b58\u5728\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4e2a\u6027\u5316\u9884\u9632\u533b\u7597\u9700\u8981\u8ba9\u6240\u6709\u5229\u76ca\u76f8\u5173\u8005\u90fd\u80fd\u7406\u89e3\u548c\u9a8c\u8bc1\u9884\u6d4b\u3001\u5e72\u9884\u548c\u63a8\u8350\u7ed3\u679c", "method": "\u91c7\u7528\u539f\u578b\u5b66\u4e60\u6846\u67b6ProtoPal\uff0c\u5305\u542b\u524d\u7aef\u548c\u540e\u7aef\u6a21\u5f0f\uff0c\u901a\u8fc7\u539f\u578b\u8868\u793a\u63d0\u4f9b\u76f4\u89c2\u7684\u5e72\u9884\u5c55\u793a\u548c\u6a21\u62df\u7ed3\u679c", "result": "\u5728\u4fdd\u6301\u4f18\u5f02\u5b9a\u91cf\u6027\u80fd\u7684\u540c\u65f6\uff0c\u80fd\u591f\u76f4\u89c2\u5448\u73b0\u5e72\u9884\u63aa\u65bd\u53ca\u5176\u6a21\u62df\u7ed3\u679c", "conclusion": "\u539f\u578b\u5b66\u4e60\u80fd\u591f\u6709\u6548\u5f25\u5408\u533b\u7597AI\u7684\u53ef\u89e3\u91ca\u6027\u5dee\u8ddd\uff0c\u4e3a\u4e2a\u6027\u5316\u9884\u9632\u533b\u7597\u63d0\u4f9b\u65e2\u51c6\u786e\u53c8\u6613\u4e8e\u7406\u89e3\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.02138", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2601.02138", "abs": "https://arxiv.org/abs/2601.02138", "authors": ["Weisen Yang", "Hanqing Zhang", "Wangren Qiu", "Xuan Xiao", "Weizhong Lin"], "title": "Edge-aware GAT-based protein binding site prediction", "comment": "24 pages, 10 figures, 6 tables", "summary": "Accurate identification of protein binding sites is crucial for understanding biomolecular interaction mechanisms and for the rational design of drug targets. Traditional predictive methods often struggle to balance prediction accuracy with computational efficiency when capturing complex spatial conformations. To address this challenge, we propose an Edge-aware Graph Attention Network (Edge-aware GAT) model for the fine-grained prediction of binding sites across various biomolecules, including proteins, DNA/RNA, ions, ligands, and lipids. Our method constructs atom-level graphs and integrates multidimensional structural features, including geometric descriptors, DSSP-derived secondary structure, and relative solvent accessibility (RSA), to generate spatially aware embedding vectors. By incorporating interatomic distances and directional vectors as edge features within the attention mechanism, the model significantly enhances its representation capacity. On benchmark datasets, our model achieves an ROC-AUC of 0.93 for protein-protein binding site prediction, outperforming several state-of-the-art methods. The use of directional tensor propagation and residue-level attention pooling further improves both binding site localization and the capture of local structural details. Visualizations using PyMOL confirm the model's practical utility and interpretability. To facilitate community access and application, we have deployed a publicly accessible web server at http://119.45.201.89:5000/. In summary, our approach offers a novel and efficient solution that balances prediction accuracy, generalization, and interpretability for identifying functional sites in proteins.", "AI": {"tldr": "\u63d0\u51faEdge-aware GAT\u6a21\u578b\uff0c\u7528\u4e8e\u7cbe\u7ec6\u9884\u6d4b\u86cb\u767d\u8d28\u7ed3\u5408\u4f4d\u70b9\uff0c\u901a\u8fc7\u539f\u5b50\u7ea7\u56fe\u7ed3\u6784\u548c\u591a\u7ef4\u7279\u5f81\u6574\u5408\uff0c\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fbe\u52300.93 ROC-AUC\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u51c6\u786e\u8bc6\u522b\u86cb\u767d\u8d28\u7ed3\u5408\u4f4d\u70b9\u5bf9\u7406\u89e3\u751f\u7269\u5206\u5b50\u76f8\u4e92\u4f5c\u7528\u673a\u5236\u548c\u836f\u7269\u9776\u70b9\u8bbe\u8ba1\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u65b9\u6cd5\u5728\u5e73\u8861\u9884\u6d4b\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u7279\u522b\u662f\u5728\u6355\u6349\u590d\u6742\u7a7a\u95f4\u6784\u8c61\u65f6\u3002", "method": "\u63d0\u51faEdge-aware Graph Attention Network\u6a21\u578b\uff0c\u6784\u5efa\u539f\u5b50\u7ea7\u56fe\u5e76\u6574\u5408\u51e0\u4f55\u63cf\u8ff0\u7b26\u3001DSSP\u4e8c\u7ea7\u7ed3\u6784\u548c\u76f8\u5bf9\u6eb6\u5242\u53ef\u53ca\u6027\u7b49\u591a\u7ef4\u7ed3\u6784\u7279\u5f81\u3002\u901a\u8fc7\u5c06\u539f\u5b50\u95f4\u8ddd\u79bb\u548c\u65b9\u5411\u5411\u91cf\u4f5c\u4e3a\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u7684\u8fb9\u7279\u5f81\uff0c\u589e\u5f3a\u6a21\u578b\u8868\u793a\u80fd\u529b\u3002\u4f7f\u7528\u65b9\u5411\u5f20\u91cf\u4f20\u64ad\u548c\u6b8b\u57fa\u7ea7\u6ce8\u610f\u529b\u6c60\u5316\u8fdb\u4e00\u6b65\u6539\u8fdb\u7ed3\u5408\u4f4d\u70b9\u5b9a\u4f4d\u548c\u5c40\u90e8\u7ed3\u6784\u7ec6\u8282\u6355\u6349\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u5728\u86cb\u767d\u8d28-\u86cb\u767d\u8d28\u7ed3\u5408\u4f4d\u70b9\u9884\u6d4b\u4e2d\u8fbe\u52300.93 ROC-AUC\uff0c\u4f18\u4e8e\u591a\u4e2a\u6700\u5148\u8fdb\u65b9\u6cd5\u3002PyMOL\u53ef\u89c6\u5316\u8bc1\u5b9e\u4e86\u6a21\u578b\u7684\u5b9e\u7528\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002\u5df2\u90e8\u7f72\u516c\u5f00\u53ef\u8bbf\u95ee\u7684Web\u670d\u52a1\u5668\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8bc6\u522b\u86cb\u767d\u8d28\u529f\u80fd\u4f4d\u70b9\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e73\u8861\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u3001\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2601.02196", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02196", "abs": "https://arxiv.org/abs/2601.02196", "authors": ["Yu Li", "Sizhe Tang", "Rongqian Chen", "Fei Xu Yu", "Guangyu Jiang", "Mahdi Imani", "Nathaniel D. Bastian", "Tian Lan"], "title": "ACDZero: Graph-Embedding-Based Tree Search for Mastering Automated Cyber Defense", "comment": null, "summary": "Automated cyber defense (ACD) seeks to protect computer networks with minimal or no human intervention, reacting to intrusions by taking corrective actions such as isolating hosts, resetting services, deploying decoys, or updating access controls. However, existing approaches for ACD, such as deep reinforcement learning (RL), often face difficult exploration in complex networks with large decision/state spaces and thus require an expensive amount of samples. Inspired by the need to learn sample-efficient defense policies, we frame ACD in CAGE Challenge 4 (CAGE-4 / CC4) as a context-based partially observable Markov decision problem and propose a planning-centric defense policy based on Monte Carlo Tree Search (MCTS). It explicitly models the exploration-exploitation tradeoff in ACD and uses statistical sampling to guide exploration and decision making. We make novel use of graph neural networks (GNNs) to embed observations from the network as attributed graphs, to enable permutation-invariant reasoning over hosts and their relationships. To make our solution practical in complex search spaces, we guide MCTS with learned graph embeddings and priors over graph-edit actions, combining model-free generalization and policy distillation with look-ahead planning. We evaluate the resulting agent on CC4 scenarios involving diverse network structures and adversary behaviors, and show that our search-guided, graph-embedding-based planning improves defense reward and robustness relative to state-of-the-art RL baselines.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u81ea\u52a8\u5316\u7f51\u7edc\u9632\u5fa1\u65b9\u6cd5\uff0c\u5728\u590d\u6742\u7f51\u7edc\u73af\u5883\u4e2d\u5b9e\u73b0\u6837\u672c\u9ad8\u6548\u7684\u9632\u5fa1\u7b56\u7565", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u52a8\u5316\u7f51\u7edc\u9632\u5fa1\u65b9\u6cd5\u5728\u590d\u6742\u7f51\u7edc\u7684\u5927\u51b3\u7b56/\u72b6\u6001\u7a7a\u95f4\u4e2d\u9762\u4e34\u63a2\u7d22\u56f0\u96be\uff0c\u9700\u8981\u5927\u91cf\u6837\u672c\uff0c\u9700\u8981\u5b66\u4e60\u6837\u672c\u9ad8\u6548\u7684\u9632\u5fa1\u7b56\u7565", "method": "\u5c06CAGE-4\u6311\u6218\u4e2d\u7684\u81ea\u52a8\u5316\u7f51\u7edc\u9632\u5fa1\u5efa\u6a21\u4e3a\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u95ee\u9898\uff0c\u63d0\u51fa\u57fa\u4e8e\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u7684\u89c4\u5212\u4e2d\u5fc3\u9632\u5fa1\u7b56\u7565\uff0c\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u5d4c\u5165\u7f51\u7edc\u89c2\u6d4b\u4f5c\u4e3a\u5c5e\u6027\u56fe\uff0c\u7ed3\u5408\u5b66\u4e60\u5230\u7684\u56fe\u5d4c\u5165\u548c\u56fe\u7f16\u8f91\u52a8\u4f5c\u5148\u9a8c\u6765\u6307\u5bfc\u641c\u7d22", "result": "\u5728CC4\u573a\u666f\u4e2d\u8bc4\u4f30\uff0c\u6d89\u53ca\u4e0d\u540c\u7f51\u7edc\u7ed3\u6784\u548c\u5bf9\u624b\u884c\u4e3a\uff0c\u663e\u793a\u641c\u7d22\u5f15\u5bfc\u7684\u3001\u57fa\u4e8e\u56fe\u5d4c\u5165\u7684\u89c4\u5212\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\u63d0\u9ad8\u4e86\u9632\u5fa1\u5956\u52b1\u548c\u9c81\u68d2\u6027", "conclusion": "\u63d0\u51fa\u7684\u7ed3\u5408\u6a21\u578b\u65e0\u5173\u6cdb\u5316\u3001\u7b56\u7565\u84b8\u998f\u548c\u524d\u5411\u89c4\u5212\u7684MCTS\u65b9\u6cd5\u5728\u590d\u6742\u641c\u7d22\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u4e86\u5b9e\u7528\u7684\u81ea\u52a8\u5316\u7f51\u7edc\u9632\u5fa1\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.02201", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02201", "abs": "https://arxiv.org/abs/2601.02201", "authors": ["Keyu Wang", "Bingchen Miao", "Wendong Bu", "Yu Wu", "Juncheng Li", "Shengyu Zhang", "Wenqiao Zhang", "Siliang Tang", "Jun Xiao", "Yueting Zhuang"], "title": "CORE: Code-based Inverse Self-Training Framework with Graph Expansion for Virtual Agents", "comment": "19 pages, 12 figures", "summary": "The development of Multimodal Virtual Agents has made significant progress through the integration of Multimodal Large Language Models. However, mainstream training paradigms face key challenges: Behavior Cloning is simple and effective through imitation but suffers from low behavioral diversity, while Reinforcement Learning is capable of discovering novel strategies through exploration but heavily relies on manually designed reward functions. To address the conflict between these two methods, we present CORE, a Code-based Inverse Self-Training Framework with Graph Expansion that bridges imitation and exploration, offering a novel training framework that promotes behavioral diversity while eliminating the reliance on manually reward design. Specifically, we introduce Semantic Code Abstraction to automatically infers reward functions from expert demonstrations without manual design. The inferred reward function, referred to as the Label Function, is executable code that verifies one key step within a task. Building on this, we propose Strategy Graph Expansion to enhance in-domain behavioral diversity, which constructs a multi-path graph called Strategy Graph that captures diverse valid solutions beyond expert demonstrations. Furthermore, we introduce Trajectory-Guided Extrapolation, which enriches out-of-domain behavioral diversity by utilizing both successful and failed trajectories to expand the task space. Experiments on Web and Android platforms demonstrate that CORE significantly improves both overall performance and generalization, highlighting its potential as a robust and generalizable training paradigm for building powerful virtual agents.", "AI": {"tldr": "CORE\u662f\u4e00\u4e2a\u57fa\u4e8e\u4ee3\u7801\u7684\u9006\u81ea\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u56fe\u6269\u5c55\u6865\u63a5\u6a21\u4eff\u4e0e\u63a2\u7d22\uff0c\u81ea\u52a8\u4ece\u4e13\u5bb6\u6f14\u793a\u4e2d\u63a8\u65ad\u5956\u52b1\u51fd\u6570\uff0c\u63d0\u5347\u865a\u62df\u4ee3\u7406\u7684\u884c\u4e3a\u591a\u6837\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u865a\u62df\u4ee3\u7406\u8bad\u7ec3\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u884c\u4e3a\u514b\u9686\u65b9\u6cd5\u7b80\u5355\u6709\u6548\u4f46\u884c\u4e3a\u591a\u6837\u6027\u4f4e\uff1b\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u80fd\u53d1\u73b0\u65b0\u7b56\u7565\u4f46\u4e25\u91cd\u4f9d\u8d56\u624b\u52a8\u8bbe\u8ba1\u7684\u5956\u52b1\u51fd\u6570\u3002\u9700\u8981\u89e3\u51b3\u8fd9\u4e24\u79cd\u65b9\u6cd5\u4e4b\u95f4\u7684\u51b2\u7a81\u3002", "method": "1. \u8bed\u4e49\u4ee3\u7801\u62bd\u8c61\uff1a\u81ea\u52a8\u4ece\u4e13\u5bb6\u6f14\u793a\u4e2d\u63a8\u65ad\u5956\u52b1\u51fd\u6570\uff08\u6807\u7b7e\u51fd\u6570\uff09\uff0c\u65e0\u9700\u624b\u52a8\u8bbe\u8ba1\uff1b2. \u7b56\u7565\u56fe\u6269\u5c55\uff1a\u6784\u5efa\u591a\u8def\u5f84\u7b56\u7565\u56fe\uff0c\u589e\u5f3a\u57df\u5185\u884c\u4e3a\u591a\u6837\u6027\uff1b3. \u8f68\u8ff9\u5f15\u5bfc\u5916\u63a8\uff1a\u5229\u7528\u6210\u529f\u548c\u5931\u8d25\u8f68\u8ff9\u6269\u5c55\u4efb\u52a1\u7a7a\u95f4\uff0c\u4e30\u5bcc\u57df\u5916\u884c\u4e3a\u591a\u6837\u6027\u3002", "result": "\u5728Web\u548cAndroid\u5e73\u53f0\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCORE\u663e\u8457\u63d0\u9ad8\u4e86\u6574\u4f53\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5c55\u793a\u4e86\u5176\u4f5c\u4e3a\u6784\u5efa\u5f3a\u5927\u865a\u62df\u4ee3\u7406\u7684\u9c81\u68d2\u4e14\u53ef\u6cdb\u5316\u8bad\u7ec3\u8303\u5f0f\u7684\u6f5c\u529b\u3002", "conclusion": "CORE\u901a\u8fc7\u6865\u63a5\u6a21\u4eff\u4e0e\u63a2\u7d22\uff0c\u89e3\u51b3\u4e86\u884c\u4e3a\u514b\u9686\u548c\u5f3a\u5316\u5b66\u4e60\u4e4b\u95f4\u7684\u51b2\u7a81\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e2\u80fd\u63d0\u5347\u884c\u4e3a\u591a\u6837\u6027\u53c8\u65e0\u9700\u624b\u52a8\u8bbe\u8ba1\u5956\u52b1\u51fd\u6570\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u4e3a\u6784\u5efa\u5f3a\u5927\u7684\u591a\u6a21\u6001\u865a\u62df\u4ee3\u7406\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2601.02213", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02213", "abs": "https://arxiv.org/abs/2601.02213", "authors": ["Haoyu Zhou", "Ping Xue", "Tianfan Fu", "Hao Zhang"], "title": "Quantized SO(3)-Equivariant Graph Neural Networks for Efficient Molecular Property Prediction", "comment": null, "summary": "Deploying 3D graph neural networks (GNNs) that are equivariant to 3D rotations (the group SO(3)) on edge devices is challenging due to their high computational cost. This paper addresses the problem by compressing and accelerating an SO(3)-equivariant GNN using low-bit quantization techniques. Specifically, we introduce three innovations for quantized equivariant transformers: (1) a magnitude-direction decoupled quantization scheme that separately quantizes the norm and orientation of equivariant (vector) features, (2) a branch-separated quantization-aware training strategy that treats invariant and equivariant feature channels differently in an attention-based $SO(3)$-GNN, and (3) a robustness-enhancing attention normalization mechanism that stabilizes low-precision attention computations. Experiments on the QM9 and rMD17 molecular benchmarks demonstrate that our 8-bit models achieve accuracy on energy and force predictions comparable to full-precision baselines with markedly improved efficiency. We also conduct ablation studies to quantify the contribution of each component to maintain accuracy and equivariance under quantization, using the Local error of equivariance (LEE) metric. The proposed techniques enable the deployment of symmetry-aware GNNs in practical chemistry applications with 2.37--2.73x faster inference and 4x smaller model size, without sacrificing accuracy or physical symmetry.", "AI": {"tldr": "\u63d0\u51fa\u4e09\u79cd\u521b\u65b0\u6280\u672f\u538b\u7f29SO(3)-\u7b49\u53d8\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u91cf\u5316\u5b9e\u73b02.37-2.73\u500d\u63a8\u7406\u52a0\u901f\u548c4\u500d\u6a21\u578b\u538b\u7f29\uff0c\u5728\u5206\u5b50\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4fdd\u6301\u7cbe\u5ea6\u548c\u7b49\u53d8\u6027\u3002", "motivation": "\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f723D\u7b49\u53d8\u56fe\u795e\u7ecf\u7f51\u7edc\u9762\u4e34\u9ad8\u8ba1\u7b97\u6210\u672c\u6311\u6218\uff0c\u9700\u8981\u538b\u7f29\u548c\u52a0\u901fSO(3)-\u7b49\u53d8GNN\u4ee5\u9002\u7528\u4e8e\u5b9e\u9645\u5316\u5b66\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e09\u79cd\u91cf\u5316\u7b49\u53d8\u53d8\u6362\u5668\u521b\u65b0\uff1a1) \u5e45\u503c-\u65b9\u5411\u89e3\u8026\u91cf\u5316\u65b9\u6848\uff0c\u5206\u522b\u91cf\u5316\u7b49\u53d8\u7279\u5f81\u7684\u8303\u6570\u548c\u65b9\u5411\uff1b2) \u5206\u652f\u5206\u79bb\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u533a\u522b\u5904\u7406\u4e0d\u53d8\u548c\u7b49\u53d8\u7279\u5f81\u901a\u9053\uff1b3) \u9c81\u68d2\u6027\u589e\u5f3a\u7684\u6ce8\u610f\u529b\u5f52\u4e00\u5316\u673a\u5236\uff0c\u7a33\u5b9a\u4f4e\u7cbe\u5ea6\u6ce8\u610f\u529b\u8ba1\u7b97\u3002", "result": "\u5728QM9\u548crMD17\u5206\u5b50\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c8\u4f4d\u6a21\u578b\u5728\u80fd\u91cf\u548c\u529b\u9884\u6d4b\u65b9\u9762\u8fbe\u5230\u4e0e\u5168\u7cbe\u5ea6\u57fa\u7ebf\u76f8\u5f53\u7684\u7cbe\u5ea6\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u53472.37-2.73\u500d\uff0c\u6a21\u578b\u5927\u5c0f\u51cf\u5c0f4\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u7b49\u53d8\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u91cf\u5316\u6280\u672f\u4f7f\u5bf9\u79f0\u611f\u77e5GNN\u80fd\u591f\u90e8\u7f72\u5728\u5b9e\u9645\u5316\u5b66\u5e94\u7528\u4e2d\uff0c\u5728\u4e0d\u727a\u7272\u7cbe\u5ea6\u6216\u7269\u7406\u5bf9\u79f0\u6027\u7684\u524d\u63d0\u4e0b\u663e\u8457\u63d0\u5347\u6548\u7387\u3002"}}
{"id": "2601.02232", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02232", "abs": "https://arxiv.org/abs/2601.02232", "authors": ["Shristi Das Biswas", "Yue Zhang", "Anwesan Pal", "Radhika Bhargava", "Kaushik Roy"], "title": "ELLA: Efficient Lifelong Learning for Adapters in Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) suffer severe catastrophic forgetting when adapted sequentially to new tasks in a continual learning (CL) setting. Existing approaches are fundamentally limited: replay-based methods are impractical and privacy-violating, while strict orthogonality-based methods collapse under scale: each new task is projected onto an orthogonal complement, progressively reducing the residual degrees of freedom and eliminating forward transfer by forbidding overlap in shared representations. In this work, we introduce ELLA, a training framework built on the principle of selective subspace de-correlation. Rather than forbidding all overlap, ELLA explicitly characterizes the structure of past updates and penalizes alignments along their high-energy, task-specific directions, while preserving freedom in the low-energy residual subspaces to enable transfer. Formally, this is realized via a lightweight regularizer on a single aggregated update matrix. We prove this mechanism corresponds to an anisotropic shrinkage operator that bounds interference, yielding a penalty that is both memory- and compute-constant regardless of task sequence length. ELLA requires no data replay, no architectural expansion, and negligible storage. Empirically, it achieves state-of-the-art CL performance on three popular benchmarks, with relative accuracy gains of up to $9.6\\%$ and a $35\\times$ smaller memory footprint. Further, ELLA scales robustly across architectures and actively enhances the model's zero-shot generalization performance on unseen tasks, establishing a principled and scalable solution for constructive lifelong LLM adaptation.", "AI": {"tldr": "ELLA\uff1a\u4e00\u79cd\u57fa\u4e8e\u9009\u62e9\u6027\u5b50\u7a7a\u95f4\u53bb\u76f8\u5173\u539f\u5219\u7684\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u60e9\u7f5a\u4efb\u52a1\u7279\u5b9a\u65b9\u5411\u7684\u5bf9\u9f50\u6765\u51cf\u5c11\u707e\u96be\u6027\u9057\u5fd8\uff0c\u540c\u65f6\u4fdd\u7559\u4f4e\u80fd\u91cf\u5b50\u7a7a\u95f4\u7684\u81ea\u7531\u5ea6\u4ee5\u5b9e\u73b0\u6b63\u5411\u8fc1\u79fb\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6301\u7eed\u5b66\u4e60\u8bbe\u7f6e\u4e2d\u9762\u4e34\u4e25\u91cd\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u6839\u672c\u6027\u9650\u5236\uff1a\u57fa\u4e8e\u91cd\u653e\u7684\u65b9\u6cd5\u4e0d\u5207\u5b9e\u9645\u4e14\u4fb5\u72af\u9690\u79c1\uff0c\u800c\u4e25\u683c\u6b63\u4ea4\u6027\u65b9\u6cd5\u5728\u89c4\u6a21\u6269\u5c55\u65f6\u4f1a\u5931\u6548\uff0c\u56e0\u4e3a\u6bcf\u4e2a\u65b0\u4efb\u52a1\u90fd\u88ab\u6295\u5f71\u5230\u6b63\u4ea4\u8865\u7a7a\u95f4\uff0c\u9010\u6e10\u51cf\u5c11\u5269\u4f59\u81ea\u7531\u5ea6\u5e76\u7981\u6b62\u5171\u4eab\u8868\u793a\u4e2d\u7684\u91cd\u53e0\uff0c\u4ece\u800c\u6d88\u9664\u6b63\u5411\u8fc1\u79fb\u3002", "method": "ELLA\u57fa\u4e8e\u9009\u62e9\u6027\u5b50\u7a7a\u95f4\u53bb\u76f8\u5173\u539f\u5219\uff0c\u660e\u786e\u8868\u5f81\u8fc7\u53bb\u66f4\u65b0\u7684\u7ed3\u6784\uff0c\u60e9\u7f5a\u6cbf\u5176\u9ad8\u80fd\u91cf\u3001\u4efb\u52a1\u7279\u5b9a\u65b9\u5411\u7684\u5bf9\u9f50\uff0c\u540c\u65f6\u4fdd\u7559\u4f4e\u80fd\u91cf\u6b8b\u5dee\u5b50\u7a7a\u95f4\u7684\u81ea\u7531\u5ea6\u4ee5\u5b9e\u73b0\u8fc1\u79fb\u3002\u8fd9\u901a\u8fc7\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6b63\u5219\u5316\u5668\u5728\u5355\u4e2a\u805a\u5408\u66f4\u65b0\u77e9\u9635\u4e0a\u5b9e\u73b0\uff0c\u5bf9\u5e94\u4e00\u4e2a\u5404\u5411\u5f02\u6027\u6536\u7f29\u7b97\u5b50\u6765\u9650\u5236\u5e72\u6270\u3002", "result": "\u5728\u4e09\u4e2a\u6d41\u884c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6301\u7eed\u5b66\u4e60\u6027\u80fd\uff0c\u76f8\u5bf9\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe9.6%\uff0c\u5185\u5b58\u5360\u7528\u51cf\u5c1135\u500d\u3002\u65e0\u9700\u6570\u636e\u91cd\u653e\u3001\u65e0\u9700\u67b6\u6784\u6269\u5c55\u3001\u5b58\u50a8\u9700\u6c42\u6781\u5c0f\uff0c\u4e14\u80fd\u7a33\u5065\u6269\u5c55\u5230\u4e0d\u540c\u67b6\u6784\uff0c\u5e76\u4e3b\u52a8\u589e\u5f3a\u6a21\u578b\u5728\u672a\u89c1\u4efb\u52a1\u4e0a\u7684\u96f6\u6837\u672c\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "ELLA\u4e3a\u6784\u5efa\u6027\u7ec8\u8eabLLM\u9002\u5e94\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u5b50\u7a7a\u95f4\u53bb\u76f8\u5173\u6709\u6548\u5e73\u8861\u4e86\u707e\u96be\u6027\u9057\u5fd8\u548c\u6b63\u5411\u8fc1\u79fb\u4e4b\u95f4\u7684\u6743\u8861\u3002"}}
{"id": "2601.02253", "categories": ["cs.LG", "cs.AR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02253", "abs": "https://arxiv.org/abs/2601.02253", "authors": ["Emrah Mete", "Emin Erkan Korkmaz"], "title": "Neuro-Channel Networks: A Multiplication-Free Architecture by Biological Signal Transmission", "comment": "9 pages, 4 figures", "summary": "The rapid proliferation of Deep Learning is increasingly constrained by its heavy reliance on high-performance hardware, particularly Graphics Processing Units (GPUs). These specialized accelerators are not only prohibitively expensive and energy-intensive but also suffer from significant supply scarcity, limiting the ubiquity of Artificial Intelligence (AI) deployment on edge devices. The core of this inefficiency stems from the standard artificial perceptron's dependence on intensive matrix multiplications. However, biological nervous systems achieve unparalleled efficiency without such arithmetic intensity; synaptic signal transmission is regulated by physical ion channel limits and chemical neurotransmitter levels rather than a process that can be analogous to arithmetic multiplication. Inspired by this biological mechanism, we propose Neuro-Channel Networks (NCN), a novel multiplication-free architecture designed to decouple AI from expensive hardware dependencies. In our model, weights are replaced with Channel Widths that physically limit the signal magnitude, while a secondary parameter acts as a Neurotransmitter to regulate Signal Transmission based on sign logic. The forward pass relies exclusively on addition, subtraction, and bitwise operations (minimum, sign), eliminating floating-point multiplication entirely. In this proof-of-concept study, we demonstrate that NCNs can solve non-linearly separable problems like XOR and the Majority function with 100% accuracy using standard backpropagation, proving their capability to form complex decision boundaries without multiplicative weights. This architecture offers a highly efficient alternative for next-generation neuromorphic hardware, paving the way for running complex models on commodity CPUs or ultra-low-power chips without relying on costly GPU clusters.", "AI": {"tldr": "\u63d0\u51faNeuro-Channel Networks (NCN)\u4e58\u6cd5\u81ea\u7531\u67b6\u6784\uff0c\u7528\u901a\u9053\u5bbd\u5ea6\u548c\u795e\u7ecf\u9012\u8d28\u53c2\u6570\u66ff\u4ee3\u4f20\u7edf\u6743\u91cd\uff0c\u4ec5\u4f7f\u7528\u52a0\u6cd5\u3001\u51cf\u6cd5\u548c\u4f4d\u8fd0\u7b97\u5b9e\u73b0\u524d\u5411\u4f20\u64ad\uff0c\u964d\u4f4e\u5bf9\u9ad8\u6027\u80fdGPU\u786c\u4ef6\u7684\u4f9d\u8d56\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u4e25\u91cd\u4f9d\u8d56\u6602\u8d35\u3001\u9ad8\u80fd\u8017\u4e14\u4f9b\u5e94\u7a00\u7f3a\u7684GPU\u786c\u4ef6\uff0c\u9650\u5236\u4e86AI\u5728\u8fb9\u7f18\u8bbe\u5907\u7684\u666e\u53ca\u3002\u4f20\u7edf\u4eba\u5de5\u611f\u77e5\u5668\u4f9d\u8d56\u5bc6\u96c6\u77e9\u9635\u4e58\u6cd5\uff0c\u800c\u751f\u7269\u795e\u7ecf\u7cfb\u7edf\u901a\u8fc7\u7269\u7406\u79bb\u5b50\u901a\u9053\u9650\u5236\u548c\u5316\u5b66\u795e\u7ecf\u9012\u8d28\u8c03\u8282\u4fe1\u53f7\u4f20\u8f93\uff0c\u65e0\u9700\u7b97\u672f\u4e58\u6cd5\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u8ba1\u7b97\u3002", "method": "\u63d0\u51faNeuro-Channel Networks (NCN)\u67b6\u6784\uff1a1) \u7528\u901a\u9053\u5bbd\u5ea6\u7269\u7406\u9650\u5236\u4fe1\u53f7\u5e45\u5ea6\uff1b2) \u5f15\u5165\u795e\u7ecf\u9012\u8d28\u53c2\u6570\u57fa\u4e8e\u7b26\u53f7\u903b\u8f91\u8c03\u8282\u4fe1\u53f7\u4f20\u8f93\uff1b3) \u524d\u5411\u4f20\u64ad\u4ec5\u4f7f\u7528\u52a0\u6cd5\u3001\u51cf\u6cd5\u548c\u4f4d\u8fd0\u7b97\uff08\u6700\u5c0f\u503c\u3001\u7b26\u53f7\uff09\uff0c\u5b8c\u5168\u6d88\u9664\u6d6e\u70b9\u4e58\u6cd5\uff1b4) \u4f7f\u7528\u6807\u51c6\u53cd\u5411\u4f20\u64ad\u8bad\u7ec3\u3002", "result": "\u5728\u6982\u5ff5\u9a8c\u8bc1\u7814\u7a76\u4e2d\uff0cNCN\u80fd\u591f\u4ee5100%\u51c6\u786e\u7387\u89e3\u51b3\u975e\u7ebf\u6027\u53ef\u5206\u95ee\u9898\uff08\u5982XOR\u548c\u591a\u6570\u51fd\u6570\uff09\uff0c\u8bc1\u660e\u5176\u65e0\u9700\u4e58\u6cd5\u6743\u91cd\u5373\u53ef\u5f62\u6210\u590d\u6742\u51b3\u7b56\u8fb9\u754c\u7684\u80fd\u529b\u3002", "conclusion": "NCN\u67b6\u6784\u4e3a\u4e0b\u4e00\u4ee3\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u63d0\u4f9b\u4e86\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u4f7f\u590d\u6742\u6a21\u578b\u80fd\u591f\u5728\u5546\u7528CPU\u6216\u8d85\u4f4e\u529f\u8017\u82af\u7247\u4e0a\u8fd0\u884c\uff0c\u65e0\u9700\u4f9d\u8d56\u6602\u8d35\u7684GPU\u96c6\u7fa4\uff0c\u6709\u671b\u63a8\u52a8AI\u5728\u8fb9\u7f18\u8bbe\u5907\u7684\u5e7f\u6cdb\u90e8\u7f72\u3002"}}
{"id": "2601.02264", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02264", "abs": "https://arxiv.org/abs/2601.02264", "authors": ["Boris Kriuk", "Fedor Kriuk"], "title": "POSEIDON: Physics-Optimized Seismic Energy Inference and Detection Operating Network", "comment": "8 pages, 14 figures", "summary": "Earthquake prediction and seismic hazard assessment remain fundamental challenges in geophysics, with existing machine learning approaches often operating as black boxes that ignore established physical laws. We introduce POSEIDON (Physics-Optimized Seismic Energy Inference and Detection Operating Network), a physics-informed energy-based model for unified multi-task seismic event prediction, alongside the Poseidon dataset -- the largest open-source global earthquake catalog comprising 2.8 million events spanning 30 years. POSEIDON embeds fundamental seismological principles, including the Gutenberg-Richter magnitude-frequency relationship and Omori-Utsu aftershock decay law, as learnable constraints within an energy-based modeling framework. The architecture simultaneously addresses three interconnected prediction tasks: aftershock sequence identification, tsunami generation potential, and foreshock detection. Extensive experiments demonstrate that POSEIDON achieves state-of-the-art performance across all tasks, outperforming gradient boosting, random forest, and CNN baselines with the highest average F1 score among all compared methods. Crucially, the learned physics parameters converge to scientifically interpretable values -- Gutenberg-Richter b-value of 0.752 and Omori-Utsu parameters p=0.835, c=0.1948 days -- falling within established seismological ranges while enhancing rather than compromising predictive accuracy. The Poseidon dataset is publicly available at https://huggingface.co/datasets/BorisKriuk/Poseidon, providing pre-computed energy features, spatial grid indices, and standardized quality metrics to advance physics-informed seismic research.", "AI": {"tldr": "POSEIDON\u662f\u4e00\u4e2a\u7269\u7406\u4fe1\u606f\u80fd\u91cf\u6a21\u578b\uff0c\u7528\u4e8e\u7edf\u4e00\u591a\u4efb\u52a1\u5730\u9707\u4e8b\u4ef6\u9884\u6d4b\uff0c\u7ed3\u5408\u4e86Gutenberg-Richter\u5b9a\u5f8b\u548cOmori-Utsu\u4f59\u9707\u8870\u51cf\u5b9a\u5f8b\u4f5c\u4e3a\u53ef\u5b66\u4e60\u7ea6\u675f\uff0c\u5728\u4e09\u4e2a\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u4f5c\u4e3a\u9ed1\u7bb1\u8fd0\u884c\uff0c\u5ffd\u7565\u4e86\u5df2\u5efa\u7acb\u7684\u7269\u7406\u5b9a\u5f8b\u3002\u5730\u9707\u9884\u6d4b\u548c\u5730\u9707\u5371\u9669\u6027\u8bc4\u4f30\u4ecd\u7136\u662f\u5730\u7403\u7269\u7406\u5b66\u4e2d\u7684\u57fa\u672c\u6311\u6218\uff0c\u9700\u8981\u5c06\u7269\u7406\u539f\u7406\u6574\u5408\u5230\u9884\u6d4b\u6a21\u578b\u4e2d\u3002", "method": "\u63d0\u51fa\u4e86POSEIDON\uff08\u7269\u7406\u4f18\u5316\u5730\u9707\u80fd\u91cf\u63a8\u65ad\u548c\u68c0\u6d4b\u64cd\u4f5c\u7f51\u7edc\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u7269\u7406\u4fe1\u606f\u80fd\u91cf\u6a21\u578b\uff0c\u5c06Gutenberg-Richter\u9707\u7ea7-\u9891\u7387\u5173\u7cfb\u548cOmori-Utsu\u4f59\u9707\u8870\u51cf\u5b9a\u5f8b\u4f5c\u4e3a\u53ef\u5b66\u4e60\u7ea6\u675f\u5d4c\u5165\u5230\u80fd\u91cf\u5efa\u6a21\u6846\u67b6\u4e2d\u3002\u540c\u65f6\u5904\u7406\u4e09\u4e2a\u76f8\u4e92\u5173\u8054\u7684\u9884\u6d4b\u4efb\u52a1\uff1a\u4f59\u9707\u5e8f\u5217\u8bc6\u522b\u3001\u6d77\u5578\u751f\u6210\u6f5c\u529b\u548c\u524d\u9707\u68c0\u6d4b\u3002", "result": "POSEIDON\u5728\u6240\u6709\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4f18\u4e8e\u68af\u5ea6\u63d0\u5347\u3001\u968f\u673a\u68ee\u6797\u548cCNN\u57fa\u7ebf\uff0c\u5728\u6240\u6709\u6bd4\u8f83\u65b9\u6cd5\u4e2d\u83b7\u5f97\u4e86\u6700\u9ad8\u7684\u5e73\u5747F1\u5206\u6570\u3002\u5b66\u4e60\u5230\u7684\u7269\u7406\u53c2\u6570\u6536\u655b\u5230\u79d1\u5b66\u53ef\u89e3\u91ca\u7684\u503c\uff1aGutenberg-Richter b\u503c\u4e3a0.752\uff0cOmori-Utsu\u53c2\u6570p=0.835\uff0cc=0.1948\u5929\uff0c\u8fd9\u4e9b\u503c\u843d\u5728\u5df2\u5efa\u7acb\u7684\u5730\u9707\u5b66\u8303\u56f4\u5185\uff0c\u540c\u65f6\u589e\u5f3a\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u3002", "conclusion": "POSEIDON\u6210\u529f\u5730\u5c06\u7269\u7406\u5b9a\u5f8b\u6574\u5408\u5230\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e2d\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u9884\u6d4b\u6027\u80fd\u548c\u79d1\u5b66\u53ef\u89e3\u91ca\u6027\u3002\u540c\u65f6\u53d1\u5e03\u7684Poseidon\u6570\u636e\u96c6\uff08\u5305\u542b280\u4e07\u4e2a\u4e8b\u4ef6\uff0c\u8de8\u8d8a30\u5e74\uff09\u5c06\u4fc3\u8fdb\u7269\u7406\u4fe1\u606f\u5730\u9707\u7814\u7a76\u7684\u53d1\u5c55\u3002"}}
{"id": "2601.02307", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02307", "abs": "https://arxiv.org/abs/2601.02307", "authors": ["Dina El Zein", "James Henderson"], "title": "Differential Privacy for Transformer Embeddings of Text with Nonparametric Variational Information Bottleneck", "comment": "11 pages, 2 figures", "summary": "We propose a privacy-preserving method for sharing text data by sharing noisy versions of their transformer embeddings. It has been shown that hidden representations learned by deep models can encode sensitive information from the input, making it possible for adversaries to recover the input data with considerable accuracy. This problem is exacerbated in transformer embeddings because they consist of multiple vectors, one per token. To mitigate this risk, we propose Nonparametric Variational Differential Privacy (NVDP), which ensures both useful data sharing and strong privacy protection. We take a differential privacy approach, integrating a Nonparametric Variational Information Bottleneck (NVIB) layer into the transformer architecture to inject noise into its multi-vector embeddings and thereby hide information, and measuring privacy protection with R\u00e9nyi divergence and its corresponding Bayesian Differential Privacy (BDP) guarantee. Training the NVIB layer calibrates the noise level according to utility. We test NVDP on the GLUE benchmark and show that varying the noise level gives us a useful tradeoff between privacy and accuracy. With lower noise levels, our model maintains high accuracy while offering strong privacy guarantees, effectively balancing privacy and utility.", "AI": {"tldr": "\u63d0\u51faNVDP\u65b9\u6cd5\uff0c\u901a\u8fc7\u5411transformer\u5d4c\u5165\u6dfb\u52a0\u566a\u58f0\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u6587\u672c\u6570\u636e\u5171\u4eab\uff0c\u5728GLUE\u57fa\u51c6\u4e0a\u5c55\u793a\u9690\u79c1\u4e0e\u51c6\u786e\u6027\u7684\u6743\u8861", "motivation": "transformer\u5d4c\u5165\u5305\u542b\u591a\u4e2a\u5411\u91cf\uff08\u6bcf\u4e2atoken\u4e00\u4e2a\uff09\uff0c\u5bb9\u6613\u7f16\u7801\u654f\u611f\u4fe1\u606f\uff0c\u4f7f\u653b\u51fb\u8005\u80fd\u591f\u4ee5\u76f8\u5f53\u9ad8\u7684\u51c6\u786e\u7387\u6062\u590d\u8f93\u5165\u6570\u636e\uff0c\u9700\u8981\u9690\u79c1\u4fdd\u62a4\u7684\u6570\u636e\u5171\u4eab\u65b9\u6cd5", "method": "\u63d0\u51fa\u975e\u53c2\u6570\u53d8\u5206\u5dee\u5206\u9690\u79c1(NVDP)\uff0c\u5c06\u975e\u53c2\u6570\u53d8\u5206\u4fe1\u606f\u74f6\u9888(NVIB)\u5c42\u96c6\u6210\u5230transformer\u67b6\u6784\u4e2d\uff0c\u5411\u591a\u5411\u91cf\u5d4c\u5165\u6ce8\u5165\u566a\u58f0\u4ee5\u9690\u85cf\u4fe1\u606f\uff0c\u4f7f\u7528R\u00e9nyi\u6563\u5ea6\u53ca\u5176\u5bf9\u5e94\u7684\u8d1d\u53f6\u65af\u5dee\u5206\u9690\u79c1(BDP)\u4fdd\u8bc1\u6765\u6d4b\u91cf\u9690\u79c1\u4fdd\u62a4", "result": "\u5728GLUE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u901a\u8fc7\u8c03\u6574\u566a\u58f0\u6c34\u5e73\u5b9e\u73b0\u4e86\u9690\u79c1\u4e0e\u51c6\u786e\u6027\u7684\u6709\u7528\u6743\u8861\uff1b\u5728\u8f83\u4f4e\u566a\u58f0\u6c34\u5e73\u4e0b\uff0c\u6a21\u578b\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u540c\u65f6\u63d0\u4f9b\u5f3a\u9690\u79c1\u4fdd\u8bc1\uff0c\u6709\u6548\u5e73\u8861\u9690\u79c1\u4e0e\u6548\u7528", "conclusion": "NVDP\u65b9\u6cd5\u901a\u8fc7\u5411transformer\u5d4c\u5165\u6dfb\u52a0\u566a\u58f0\uff0c\u5b9e\u73b0\u4e86\u9690\u79c1\u4fdd\u62a4\u6587\u672c\u6570\u636e\u5171\u4eab\uff0c\u5728\u4fdd\u6301\u5b9e\u7528\u6027\u7684\u540c\u65f6\u63d0\u4f9b\u5f3a\u9690\u79c1\u4fdd\u62a4\uff0c\u4e3a\u9690\u79c1\u4e0e\u6548\u7528\u5e73\u8861\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.02310", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02310", "abs": "https://arxiv.org/abs/2601.02310", "authors": ["Ahmad Makinde"], "title": "Temporal Kolmogorov-Arnold Networks (T-KAN) for High-Frequency Limit Order Book Forecasting: Efficiency, Interpretability, and Alpha Decay", "comment": "8 pages, 5 figures, Proposes T-KAN architecture for HFT. Achieves 19.1% F1-score improvement on FI-2010 and 132.48% return in cost-adjusted backtests.Proposes T-KAN architecture for HFT. Achieves 19.1% F1-score improvement on FI-2010 and 132.48% return in cost-adjusted backtests", "summary": "High-Frequency trading (HFT) environments are characterised by large volumes of limit order book (LOB) data, which is notoriously noisy and non-linear. Alpha decay represents a significant challenge, with traditional models such as DeepLOB losing predictive power as the time horizon (k) increases. In this paper, using data from the FI-2010 dataset, we introduce Temporal Kolmogorov-Arnold Networks (T-KAN) to replace the fixed, linear weights of standard LSTMs with learnable B-spline activation functions. This allows the model to learn the 'shape' of market signals as opposed to just their magnitude. This resulted in a 19.1% relative improvement in the F1-score at the k = 100 horizon. The efficacy of T-KAN networks cannot be understated, producing a 132.48% return compared to the -82.76% DeepLOB drawdown under 1.0 bps transaction costs. In addition to this, the T-KAN model proves quite interpretable, with the 'dead-zones' being clearly visible in the splines. The T-KAN architecture is also uniquely optimized for low-latency FPGA implementation via High level Synthesis (HLS). The code for the experiments in this project can be found at https://github.com/AhmadMak/Temporal-Kolmogorov-Arnold-Networks-T-KAN-for-High-Frequency-Limit-Order-Book-Forecasting.", "AI": {"tldr": "T-KAN\u6a21\u578b\u901a\u8fc7\u53ef\u5b66\u4e60\u7684B\u6837\u6761\u6fc0\u6d3b\u51fd\u6570\u66ff\u4ee3\u4f20\u7edfLSTM\u7684\u56fa\u5b9a\u7ebf\u6027\u6743\u91cd\uff0c\u5728\u9ad8\u9891\u4ea4\u6613\u73af\u5883\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\uff0c\u76f8\u6bd4DeepLOB\u5728k=100\u65f6F1\u5206\u6570\u76f8\u5bf9\u63d0\u534719.1%\uff0c\u57281.0bps\u4ea4\u6613\u6210\u672c\u4e0b\u83b7\u5f97132.48%\u56de\u62a5\u3002", "motivation": "\u9ad8\u9891\u4ea4\u6613\u73af\u5883\u4e2d\u7684\u9650\u4ef7\u8ba2\u5355\u7c3f\u6570\u636e\u566a\u58f0\u5927\u3001\u975e\u7ebf\u6027\u5f3a\uff0c\u4f20\u7edf\u6a21\u578b\u5982DeepLOB\u968f\u7740\u65f6\u95f4\u8de8\u5ea6\u589e\u52a0\u9884\u6d4b\u80fd\u529b\u8870\u51cf\u4e25\u91cd\uff08alpha\u8870\u51cf\u95ee\u9898\uff09\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u6a21\u578b\u6765\u6355\u6349\u5e02\u573a\u4fe1\u53f7\u7279\u5f81\u3002", "method": "\u63d0\u51faTemporal Kolmogorov-Arnold Networks (T-KAN)\uff0c\u7528\u53ef\u5b66\u4e60\u7684B\u6837\u6761\u6fc0\u6d3b\u51fd\u6570\u66ff\u4ee3\u6807\u51c6LSTM\u4e2d\u7684\u56fa\u5b9a\u7ebf\u6027\u6743\u91cd\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5b66\u4e60\u5e02\u573a\u4fe1\u53f7\u7684\"\u5f62\u72b6\"\u800c\u4e0d\u4ec5\u4ec5\u662f\u5e45\u5ea6\uff0c\u5e76\u9488\u5bf9FPGA\u5b9e\u73b0\u8fdb\u884c\u4f4e\u5ef6\u8fdf\u4f18\u5316\u3002", "result": "\u5728FI-2010\u6570\u636e\u96c6\u4e0a\uff0cT-KAN\u5728k=100\u65f6\u95f4\u8de8\u5ea6\u4e0aF1\u5206\u6570\u76f8\u5bf9\u63d0\u534719.1%\uff1b\u57281.0bps\u4ea4\u6613\u6210\u672c\u4e0b\u83b7\u5f97132.48%\u56de\u62a5\uff0c\u800cDeepLOB\u4e8f\u635f82.76%\uff1b\u6a21\u578b\u5177\u6709\u826f\u597d\u53ef\u89e3\u91ca\u6027\uff0c\u80fd\u6e05\u6670\u663e\u793a\u6837\u6761\u4e2d\u7684\"\u6b7b\u533a\"\u3002", "conclusion": "T-KAN\u901a\u8fc7\u53ef\u5b66\u4e60\u7684B\u6837\u6761\u6fc0\u6d3b\u51fd\u6570\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u9891\u4ea4\u6613\u4e2d\u7684alpha\u8870\u51cf\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u548c\u4ea4\u6613\u56de\u62a5\uff0c\u540c\u65f6\u5177\u6709\u826f\u597d\u7684\u53ef\u89e3\u91ca\u6027\u548c\u786c\u4ef6\u4f18\u5316\u6f5c\u529b\u3002"}}
{"id": "2601.02313", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02313", "abs": "https://arxiv.org/abs/2601.02313", "authors": ["Hanzaleh Akbari Nodehi", "Viveck R. Cadambe", "Mohammad Ali Maddah-Ali"], "title": "Game of Coding: Coding Theory in the Presence of Rational Adversaries, Motivated by Decentralized Machine Learning", "comment": null, "summary": "Coding theory plays a crucial role in enabling reliable communication, storage, and computation. Classical approaches assume a worst-case adversarial model and ensure error correction and data recovery only when the number of honest nodes exceeds the number of adversarial ones by some margin. However, in some emerging decentralized applications, particularly in decentralized machine learning (DeML), participating nodes are rewarded for accepted contributions. This incentive structure naturally gives rise to rational adversaries who act strategically rather than behaving in purely malicious ways.\n  In this paper, we first motivate the need for coding in the presence of rational adversaries, particularly in the context of outsourced computation in decentralized systems. We contrast this need with existing approaches and highlight their limitations. We then introduce the game of coding, a novel game-theoretic framework that extends coding theory to trust-minimized settings where honest nodes are not in the majority. Focusing on repetition coding, we highlight two key features of this framework: (1) the ability to achieve a non-zero probability of data recovery even when adversarial nodes are in the majority, and (2) Sybil resistance, i.e., the equilibrium remains unchanged even as the number of adversarial nodes increases. Finally, we explore scenarios in which the adversary's strategy is unknown and outline several open problems for future research.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u535a\u5f08\u8bba\u7684\u7f16\u7801\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u53bb\u4e2d\u5fc3\u5316\u7cfb\u7edf\u4e2d\u7406\u6027\u5bf9\u624b\uff08\u800c\u975e\u7eaf\u7cb9\u6076\u610f\u5bf9\u624b\uff09\u7684\u7f16\u7801\u95ee\u9898\uff0c\u5373\u4f7f\u5728\u8bda\u5b9e\u8282\u70b9\u4e0d\u5360\u591a\u6570\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u5b9e\u73b0\u975e\u96f6\u6982\u7387\u7684\u6570\u636e\u6062\u590d\u3002", "motivation": "\u5728\u53bb\u4e2d\u5fc3\u5316\u673a\u5668\u5b66\u4e60\u7b49\u65b0\u5174\u5e94\u7528\u4e2d\uff0c\u53c2\u4e0e\u8282\u70b9\u56e0\u8d21\u732e\u83b7\u5f97\u5956\u52b1\uff0c\u8fd9\u50ac\u751f\u4e86\u7406\u6027\u5bf9\u624b\u800c\u975e\u7eaf\u7cb9\u6076\u610f\u5bf9\u624b\u3002\u4f20\u7edf\u7f16\u7801\u7406\u8bba\u5047\u8bbe\u6700\u574f\u60c5\u51b5\u5bf9\u6297\u6a21\u578b\uff0c\u8981\u6c42\u8bda\u5b9e\u8282\u70b9\u6570\u91cf\u8d85\u8fc7\u6076\u610f\u8282\u70b9\uff0c\u4f46\u5728\u53bb\u4e2d\u5fc3\u5316\u7cfb\u7edf\u4e2d\u8bda\u5b9e\u8282\u70b9\u53ef\u80fd\u4e0d\u5360\u591a\u6570\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u7f16\u7801\u6846\u67b6\u3002", "method": "\u63d0\u51fa\"\u7f16\u7801\u535a\u5f08\"\u8fd9\u4e00\u65b0\u9896\u7684\u535a\u5f08\u8bba\u6846\u67b6\uff0c\u5c06\u7f16\u7801\u7406\u8bba\u6269\u5c55\u5230\u4fe1\u4efb\u6700\u5c0f\u5316\u7684\u8bbe\u7f6e\u4e2d\u3002\u91cd\u70b9\u5173\u6ce8\u91cd\u590d\u7f16\u7801\uff0c\u5c55\u793a\u4e86\u8be5\u6846\u67b6\u7684\u4e24\u4e2a\u5173\u952e\u7279\u6027\uff1a1) \u5373\u4f7f\u5bf9\u624b\u8282\u70b9\u5360\u591a\u6570\u4e5f\u80fd\u5b9e\u73b0\u975e\u96f6\u6982\u7387\u7684\u6570\u636e\u6062\u590d\uff1b2) \u5177\u6709Sybil\u62b5\u6297\u6027\uff0c\u5373\u5747\u8861\u72b6\u6001\u4e0d\u968f\u5bf9\u624b\u8282\u70b9\u6570\u91cf\u589e\u52a0\u800c\u6539\u53d8\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u5728\u8bda\u5b9e\u8282\u70b9\u4e0d\u5360\u591a\u6570\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u6570\u636e\u6062\u590d\uff0c\u5e76\u4e14\u5177\u6709Sybil\u62b5\u6297\u6027\u3002\u5f53\u5bf9\u624b\u7b56\u7565\u672a\u77e5\u65f6\uff0c\u8be5\u6846\u67b6\u4ecd\u80fd\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u53bb\u4e2d\u5fc3\u5316\u7cfb\u7edf\u4e2d\u7684\u7f16\u7801\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u5de5\u5177\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5c06\u7f16\u7801\u7406\u8bba\u4e0e\u535a\u5f08\u8bba\u76f8\u7ed3\u5408\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u53bb\u4e2d\u5fc3\u5316\u7cfb\u7edf\u4e2d\u7684\u7406\u6027\u5bf9\u624b\u95ee\u9898\u3002\u8be5\u6846\u67b6\u7a81\u7834\u4e86\u4f20\u7edf\u7f16\u7801\u7406\u8bba\u5bf9\u8bda\u5b9e\u8282\u70b9\u5360\u591a\u6570\u7684\u8981\u6c42\uff0c\u4e3a\u53bb\u4e2d\u5fc3\u5316\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u901a\u4fe1\u3001\u5b58\u50a8\u548c\u8ba1\u7b97\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u5f00\u653e\u6027\u95ee\u9898\u3002"}}
{"id": "2601.02316", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02316", "abs": "https://arxiv.org/abs/2601.02316", "authors": ["Siddharth Joshi", "Haoli Yin", "Rishabh Adiga", "Ricardo Monti", "Aldo Carranza", "Alex Fang", "Alvin Deng", "Amro Abbas", "Brett Larsen", "Cody Blakeney", "Darren Teh", "David Schwab", "Fan Pan", "Haakon Mongstad", "Jack Urbanek", "Jason Lee", "Jason Telanoff", "Josh Wills", "Kaleigh Mentzer", "Luke Merrick", "Parth Doshi", "Paul Burstein", "Pratyush Maini", "Scott Loftin", "Spandan Das", "Tony Jiang", "Vineeth Dorna", "Zhengping Wang", "Bogdan Gaza", "Ari Morcos", "Matthew Leavitt"], "title": "DatBench: Discriminative, Faithful, and Efficient VLM Evaluations", "comment": null, "summary": "Empirical evaluation serves as the primary compass guiding research progress in foundation models. Despite a large body of work focused on training frontier vision-language models (VLMs), approaches to their evaluation remain nascent. To guide their maturation, we propose three desiderata that evaluations should satisfy: (1) faithfulness to the modality and application, (2) discriminability between models of varying quality, and (3) efficiency in compute. Through this lens, we identify critical failure modes that violate faithfulness and discriminability, misrepresenting model capabilities: (i) multiple-choice formats reward guessing, poorly reflect downstream use cases, and saturate early as models improve; (ii) blindly solvable questions, which can be answered without images, constitute up to 70% of some evaluations; and (iii) mislabeled or ambiguous samples compromise up to 42% of examples in certain datasets. Regarding efficiency, the computational burden of evaluating frontier models has become prohibitive: by some accounts, nearly 20% of development compute is devoted to evaluation alone. Rather than discarding existing benchmarks, we curate them via transformation and filtering to maximize fidelity and discriminability. We find that converting multiple-choice questions to generative tasks reveals sharp capability drops of up to 35%. In addition, filtering blindly solvable and mislabeled samples improves discriminative power while simultaneously reducing computational cost. We release DatBench-Full, a cleaned evaluation suite of 33 datasets spanning nine VLM capabilities, and DatBench, a discriminative subset that achieves 13x average speedup (up to 50x) while closely matching the discriminative power of the original datasets. Our work outlines a path toward evaluation practices that are both rigorous and sustainable as VLMs continue to scale.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u4e09\u4e2a\u6807\u51c6\uff08\u5fe0\u5b9e\u6027\u3001\u533a\u5206\u6027\u3001\u6548\u7387\uff09\uff0c\u53d1\u73b0\u73b0\u6709\u8bc4\u4f30\u5b58\u5728\u591a\u4e2a\u7f3a\u9677\uff0c\u5e76\u901a\u8fc7\u6570\u636e\u6e05\u6d17\u548c\u4efb\u52a1\u8f6c\u6362\u521b\u5efa\u4e86\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u5957\u4ef6DatBench", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u4f30\u65b9\u6cd5\u5c1a\u4e0d\u6210\u719f\uff0c\u5b58\u5728\u591a\u79cd\u7f3a\u9677\uff1a\u591a\u9879\u9009\u62e9\u9898\u9f13\u52b1\u731c\u6d4b\u3001\u4e0e\u4e0b\u6e38\u5e94\u7528\u8131\u8282\u3001\u5b58\u5728\u5927\u91cf\u65e0\u9700\u56fe\u50cf\u5373\u53ef\u56de\u7b54\u7684\u95ee\u9898\u3001\u6570\u636e\u6807\u7b7e\u9519\u8bef\u7b49\uff0c\u4e14\u8bc4\u4f30\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff08\u5360\u5f00\u53d1\u7b97\u529b\u768420%\uff09", "method": "\u63d0\u51fa\u8bc4\u4f30\u7684\u4e09\u4e2a\u6807\u51c6\uff08\u5fe0\u5b9e\u6027\u3001\u533a\u5206\u6027\u3001\u6548\u7387\uff09\uff0c\u8bc6\u522b\u73b0\u6709\u8bc4\u4f30\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u901a\u8fc7\u5c06\u591a\u9879\u9009\u62e9\u9898\u8f6c\u6362\u4e3a\u751f\u6210\u4efb\u52a1\u3001\u8fc7\u6ee4\"\u76f2\u76ee\u53ef\u89e3\"\u95ee\u9898\u548c\u9519\u8bef\u6807\u7b7e\u6837\u672c\uff0c\u521b\u5efa\u6e05\u6d17\u540e\u7684\u8bc4\u4f30\u5957\u4ef6DatBench", "result": "\u591a\u9879\u9009\u62e9\u9898\u8f6c\u6362\u4e3a\u751f\u6210\u4efb\u52a1\u540e\uff0c\u6a21\u578b\u80fd\u529b\u4e0b\u964d\u9ad8\u8fbe35%\uff1b\u8fc7\u6ee4\u95ee\u9898\u540e\u63d0\u9ad8\u4e86\u533a\u5206\u80fd\u529b\u5e76\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff1b\u53d1\u5e03\u7684DatBench-Full\u5305\u542b33\u4e2a\u6570\u636e\u96c6\uff0cDatBench\u5b50\u96c6\u5b9e\u73b013\u500d\u5e73\u5747\u52a0\u901f\uff08\u6700\u9ad850\u500d\uff09", "conclusion": "\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u4e25\u8c28\u548c\u53ef\u6301\u7eed\u7684\u8def\u5f84\uff0c\u901a\u8fc7\u6570\u636e\u6e05\u6d17\u548c\u4efb\u52a1\u8f6c\u6362\u663e\u8457\u63d0\u9ad8\u4e86\u8bc4\u4f30\u7684\u5fe0\u5b9e\u6027\u548c\u533a\u5206\u6027\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c"}}
{"id": "2601.02360", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02360", "abs": "https://arxiv.org/abs/2601.02360", "authors": ["Yazan Obeidi", "Amir Sarfi", "Joel Lidin", "Paul Janson", "Eugene Belilovsky"], "title": "Heterogeneous Low-Bandwidth Pre-Training of LLMs", "comment": null, "summary": "Pre-training large language models (LLMs) increasingly requires distributed compute, yet bandwidth constraints make it difficult to scale beyond well-provisioned datacenters-especially when model parallelism forces frequent, large inter-device communications. We study whether SparseLoCo, a low-communication data parallel method based on infrequent synchronization and sparse pseudo-gradient exchange, can be combined with low-bandwidth pipeline model parallelism via activation and activation-gradient compression. We introduce a heterogeneous distributed training framework where some participants host full replicas on high-bandwidth interconnects, while resource-limited participants are grouped to jointly instantiate a replica using pipeline parallelism with subspace-projected inter-stage communication. To make the recently introduced subspace pipeline compression compatible with SparseLoCo, we study a number of adaptations. Across large-scale language modeling experiments (178M-1B parameters) on standard pretraining corpora, we find that activation compression composes with SparseLoCo at modest cost, while selective (heterogeneous) compression consistently improves the loss-communication tradeoff relative to compressing all replicas-especially at aggressive compression ratios. These results suggest a practical path to incorporating low-bandwidth model parallelism and heterogeneous participants into LLM pre-training.", "AI": {"tldr": "SparseLoCo\uff08\u4f4e\u901a\u4fe1\u6570\u636e\u5e76\u884c\uff09\u4e0e\u4f4e\u5e26\u5bbd\u6d41\u6c34\u7ebf\u6a21\u578b\u5e76\u884c\u76f8\u7ed3\u5408\uff0c\u901a\u8fc7\u6fc0\u6d3b\u548c\u6fc0\u6d3b\u68af\u5ea6\u538b\u7f29\u5b9e\u73b0\u5f02\u6784\u5206\u5e03\u5f0f\u8bad\u7ec3\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\u3002", "motivation": "\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u9700\u8981\u5206\u5e03\u5f0f\u8ba1\u7b97\uff0c\u4f46\u5e26\u5bbd\u9650\u5236\u4f7f\u5f97\u5728\u6570\u636e\u4e2d\u5fc3\u4e4b\u5916\u96be\u4ee5\u6269\u5c55\uff0c\u7279\u522b\u662f\u5f53\u6a21\u578b\u5e76\u884c\u9700\u8981\u9891\u7e41\u7684\u5927\u89c4\u6a21\u8bbe\u5907\u95f4\u901a\u4fe1\u65f6\u3002\u9700\u8981\u7814\u7a76\u5982\u4f55\u5c06\u4f4e\u901a\u4fe1\u6570\u636e\u5e76\u884c\u65b9\u6cd5\u4e0e\u4f4e\u5e26\u5bbd\u6d41\u6c34\u7ebf\u6a21\u578b\u5e76\u884c\u76f8\u7ed3\u5408\u3002", "method": "\u63d0\u51fa\u5f02\u6784\u5206\u5e03\u5f0f\u8bad\u7ec3\u6846\u67b6\uff1a\u9ad8\u5e26\u5bbd\u53c2\u4e0e\u8005\u6258\u7ba1\u5b8c\u6574\u526f\u672c\uff0c\u8d44\u6e90\u6709\u9650\u53c2\u4e0e\u8005\u5206\u7ec4\u4f7f\u7528\u6d41\u6c34\u7ebf\u5e76\u884c\u548c\u5b50\u7a7a\u95f4\u6295\u5f71\u7684\u7ea7\u95f4\u901a\u4fe1\u3002\u5c06\u5b50\u7a7a\u95f4\u6d41\u6c34\u7ebf\u538b\u7f29\u4e0eSparseLoCo\u7ed3\u5408\uff0c\u7814\u7a76\u591a\u79cd\u9002\u914d\u65b9\u6cd5\u3002", "result": "\u5728178M-1B\u53c2\u6570\u7684\u5927\u89c4\u6a21\u8bed\u8a00\u5efa\u6a21\u5b9e\u9a8c\u4e2d\uff0c\u6fc0\u6d3b\u538b\u7f29\u4e0eSparseLoCo\u7ed3\u5408\u6210\u672c\u9002\u4e2d\uff0c\u9009\u62e9\u6027\uff08\u5f02\u6784\uff09\u538b\u7f29\u76f8\u6bd4\u538b\u7f29\u6240\u6709\u526f\u672c\u80fd\u6301\u7eed\u6539\u5584\u635f\u5931-\u901a\u4fe1\u6743\u8861\uff0c\u7279\u522b\u662f\u5728\u9ad8\u538b\u7f29\u6bd4\u4e0b\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\u4e86\u4e00\u6761\u5b9e\u7528\u7684\u8def\u5f84\uff0c\u53ef\u4ee5\u5c06\u4f4e\u5e26\u5bbd\u6a21\u578b\u5e76\u884c\u548c\u5f02\u6784\u53c2\u4e0e\u8005\u7eb3\u5165LLM\u9884\u8bad\u7ec3\u4e2d\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u5206\u5e03\u5f0f\u8bad\u7ec3\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
