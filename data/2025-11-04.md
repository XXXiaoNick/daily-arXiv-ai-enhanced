<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 90]
- [econ.EM](#econ.EM) [Total: 6]
- [math.OC](#math.OC) [Total: 38]
- [q-fin.GN](#q-fin.GN) [Total: 2]
- [stat.ML](#stat.ML) [Total: 13]
- [cs.AI](#cs.AI) [Total: 55]
- [cs.LG](#cs.LG) [Total: 184]
- [eess.SY](#eess.SY) [Total: 32]
- [q-fin.CP](#q-fin.CP) [Total: 2]
- [q-fin.PR](#q-fin.PR) [Total: 1]
- [cs.CY](#cs.CY) [Total: 13]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [q-fin.RM](#q-fin.RM) [Total: 2]
- [q-fin.MF](#q-fin.MF) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [PlotCraft: Pushing the Limits of LLMs for Complex and Interactive Data Visualization](https://arxiv.org/abs/2511.00010)
*Jiajun Zhang,Jianke Zhang,Zeyu Cui,Jiaxi Yang,Lei Zhang,Binyuan Hui,Qiang Liu,Zilei Wang,Liang Wang,Junyang Lin*

Main category: cs.CL

TL;DR: PlotCraft是一个包含1000个挑战性可视化任务的新基准，用于评估LLM在复杂数据可视化方面的能力。研究开发了SynthVis-30K数据集和PlotCraftor模型，在复杂可视化任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在代码生成方面表现出色，但在处理规模和结构化数据的复杂可视化任务方面能力不足，缺乏系统评估。

Method: 构建PlotCraft基准（1k任务，7类任务，48种图表类型），开发SynthVis-30K数据集，并基于此训练PlotCraftor模型。

Result: 评估23个领先LLM发现其在复杂可视化任务上存在明显缺陷。PlotCraftor模型在多个基准测试中表现优异，在困难任务上性能提升超过50%。

Conclusion: PlotCraft基准填补了LLM在复杂可视化评估方面的空白，PlotCraftor模型证明了在复杂数据可视化方面的强大能力。

Abstract: Recent Large Language Models (LLMs) have demonstrated remarkable profi-
ciency in code generation. However, their ability to create complex visualiza-
tions for scaled and structured data remains largely unevaluated and
underdevel- oped. To address this gap, we introduce PlotCraft, a new benchmark
featuring 1k challenging visualization tasks that cover a wide range of topics,
such as fi- nance, scientific research, and sociology. The benchmark is
structured around seven high-level visualization tasks and encompasses 48
distinct chart types. Cru- cially, it is the first to systematically evaluate
both single-turn generation and multi-turn refinement across a diverse spectrum
of task complexities. Our com- prehensive evaluation of 23 leading LLMs on
PlotCraft reveals obvious per- formance deficiencies in handling sophisticated
visualization tasks. To bridge this performance gap, we develope SynthVis-30K,
a large-scale, high-quality dataset of complex visualization code synthesized
via a collaborative agent frame- work. Building upon this dataset, we develope
PlotCraftor, a novel code gener- ation model that achieves strong capabilities
in complex data visualization with a remarkably small size. Across VisEval,
PandasPlotBench, and our proposed PlotCraft, PlotCraftor shows performance
comparable to that of leading propri- etary approaches. Especially, on hard
task, Our model achieves over 50% per- formance improvement. We will release
the benchmark, dataset, and code at
https://github.com/Speakn0w/PlotCraft-Benchmark.

</details>


### [2] [Cognitive Alignment in Personality Reasoning: Leveraging Prototype Theory for MBTI Inference](https://arxiv.org/abs/2511.00115)
*Haoyuan Li,Yuanbo Tong,Yuchen Li,Zirui Wang,Chunhou Liu,Jiamou Liu*

Main category: cs.CL

TL;DR: ProtoMBTI是一个基于原型理论的MBTI人格识别框架，通过LLM引导的数据增强、原型学习和检索-修订循环，在人格推断任务中超越了传统硬标签分类方法。


<details>
  <summary>Details</summary>
Motivation: 传统的人格识别方法采用硬标签分类，忽视了人格判断的连续性和原型特性。本文旨在开发一个与认知过程对齐的人格推断框架。

Method: 1) 使用LLM引导的多维度数据增强构建平衡语料库；2) 通过LoRA微调轻量级编码器学习判别性嵌入和人格原型；3) 在推理时执行检索-重用-修订-保留循环，通过基于提示的投票聚合原型证据。

Result: 在Kaggle和Pandora基准测试中，ProtoMBTI在四个MBTI维度和完整16类型任务上均优于基线方法，并展现出强大的跨数据集泛化能力。

Conclusion: 将推理过程与心理学原型推理对齐，能够提高基于文本的人格建模的准确性、可解释性和迁移能力。

Abstract: Personality recognition from text is typically cast as hard-label
classification, which obscures the graded, prototype-like nature of human
personality judgments. We present ProtoMBTI, a cognitively aligned framework
for MBTI inference that operationalizes prototype theory within an LLM-based
pipeline. First, we construct a balanced, quality-controlled corpus via
LLM-guided multi-dimensional augmentation (semantic, linguistic, sentiment).
Next, we LoRA-fine-tune a lightweight (<=2B) encoder to learn discriminative
embeddings and to standardize a bank of personality prototypes. At inference,
we retrieve top-k prototypes for a query post and perform a
retrieve--reuse--revise--retain cycle: the model aggregates prototype evidence
via prompt-based voting, revises when inconsistencies arise, and, upon correct
prediction, retains the sample to continually enrich the prototype library.
Across Kaggle and Pandora benchmarks, ProtoMBTI improves over baselines on both
the four MBTI dichotomies and the full 16-type task, and exhibits robust
cross-dataset generalization. Our results indicate that aligning the inference
process with psychological prototype reasoning yields gains in accuracy,
interpretability, and transfer for text-based personality modeling.

</details>


### [3] [ParaScopes: What do Language Models Activations Encode About Future Text?](https://arxiv.org/abs/2511.00180)
*Nicky Pochinkov,Yulia Volkova,Anna Vasileva,Sai V R Chereddy*

Main category: cs.CL

TL;DR: 开发残差流解码器框架，用于探测语言模型中段落和文档级别的规划信息，发现可以解码相当于5+个未来token的信息。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型能够处理更长的时间跨度任务，现有方法通常局限于测试特定概念或token，需要开发能够理解模型激活中更长时程规划信息的方法。

Method: 开发残差流解码器框架，测试多种方法来探测模型激活中的段落级和文档级规划信息。

Result: 在小型模型中，可以解码相当于5个以上未来token上下文的信息。

Conclusion: 这些结果为更好地监控语言模型和理解它们如何编码长期规划信息奠定了基础。

Abstract: Interpretability studies in language models often investigate forward-looking
representations of activations. However, as language models become capable of
doing ever longer time horizon tasks, methods for understanding activations
often remain limited to testing specific concepts or tokens. We develop a
framework of Residual Stream Decoders as a method of probing model activations
for paragraph-scale and document-scale plans. We test several methods and find
information can be decoded equivalent to 5+ tokens of future context in small
models. These results lay the groundwork for better monitoring of language
models and better understanding how they might encode longer-term planning
information.

</details>


### [4] [Training LLMs Beyond Next Token Prediction - Filling the Mutual Information Gap](https://arxiv.org/abs/2511.00198)
*Chun-Hao Yang,Bo-Han Feng,Tzu-Yuan Lai,Yan Yu Chen,Yin-Kai Dean Huang,Shou-De Lin*

Main category: cs.CL

TL;DR: 本文提出了一种替代传统下一词预测的训练方法，通过预测信息丰富的词元来更有效地训练大语言模型。


<details>
  <summary>Details</summary>
Motivation: 优化大语言模型的训练性能是一个关键挑战，传统下一词预测方法可能不是最有效的训练策略。

Method: 提出选择信息丰富的目标词元进行预测，而非传统的下一词预测，并在算术、多标签文本分类和自然语言生成三类任务中验证该方法。

Result: 该方法在保持计算成本的同时提升了模型性能，为LLM训练提供了更优化的策略。

Conclusion: 通过有原则的目标词元选择策略，能够同时推进模型性能和理论理解，为LLM训练提供了新的优化方向。

Abstract: Optimizing training performance in large language models (LLMs) remains an
essential challenge, particularly in improving model performance while
maintaining computational costs. This work challenges the conventional approach
of training LLMs using next-token prediction (NTP), arguing that by predicting
information-rich tokens during training, there is a more effective way to train
LLMs. We investigate the impact of the proposed solution in three kinds of
tasks for LLMs: arithmetic, multi-label classification of text, and
natural-language generation. This work offers a principled approach to
optimizing LLM training, advancing both model performance and theoretical
understanding of the target-token selection strategies.

</details>


### [5] [Consistently Simulating Human Personas with Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2511.00222)
*Marwa Abdulhai,Ryan Cheng,Donovan Clay,Tim Althoff,Sergey Levine,Natasha Jaques*

Main category: cs.CL

TL;DR: 提出了一个评估和改进LLM角色一致性的统一框架，通过强化学习微调LLM，使模拟用户对话更加一致和忠实。


<details>
  <summary>Details</summary>
Motivation: LLM在模拟人类用户时经常出现角色漂移、前后矛盾或放弃角色适当行为的问题，需要提高角色一致性。

Method: 定义了三种自动一致性指标，并以此作为奖励信号，通过多轮强化学习微调LLM来模拟患者、学生和社交聊天伙伴三种角色。

Result: 该方法将不一致性降低了55%以上，产生了更连贯和忠实的模拟用户。

Conclusion: 该框架有效提升了LLM在模拟人类用户时的角色一致性，为可扩展的AI代理训练和评估提供了更可靠的基础。

Abstract: Large Language Models (LLMs) are increasingly used to simulate human users in
interactive settings such as therapy, education, and social role-play. While
these simulations enable scalable training and evaluation of AI agents,
off-the-shelf LLMs often drift from their assigned personas, contradict earlier
statements, or abandon role-appropriate behavior. We introduce a unified
framework for evaluating and improving persona consistency in LLM-generated
dialogue. We define three automatic metrics: prompt-to-line consistency,
line-to-line consistency, and Q&A consistency, that capture different types of
persona drift and validate each against human annotations. Using these metrics
as reward signals, we apply multi-turn reinforcement learning to fine-tune LLMs
for three user roles: a patient, a student, and a social chat partner. Our
method reduces inconsistency by over 55%, resulting in more coherent and
faithful simulated users.

</details>


### [6] [AgentBnB: A Browser-Based Cybersecurity Tabletop Exercise with Large Language Model Support and Retrieval-Aligned Scaffolding](https://arxiv.org/abs/2511.00265)
*Arman Anwar,Zefang Liu*

Main category: cs.CL

TL;DR: AgentBnB是一个基于浏览器的网络安全桌面演习系统，集成了大语言模型队友和检索增强的副驾驶，提供按需的认知针对性提示，相比传统演习更轻量、可扩展。


<details>
  <summary>Details</summary>
Motivation: 传统的网络安全桌面演习存在脚本化、资源密集、难以扩展的问题，需要更轻量、可重复的练习方式。

Method: 重新设计了Backdoors & Breaches游戏，集成大语言模型队友和检索增强副驾驶(C2D2)，使用提示工程代理和逐步淡出的脚手架阶梯，将精选语料库扩展为事实性、概念性、程序性和元认知片段。

Result: 在4名研究生的单人试点中，参与者报告更倾向于使用基于代理的版本，认为其更具可扩展性，但在简单知识测验中出现天花板效应。

Conclusion: 尽管存在样本量小、单人模式、语料库狭窄等限制，早期结果表明大语言模型增强的桌面演习可以提供轻量、可重复的练习，无需传统演习的后勤负担。

Abstract: Traditional cybersecurity tabletop exercises (TTXs) provide valuable training
but are often scripted, resource-intensive, and difficult to scale. We
introduce AgentBnB, a browser-based re-imagining of the Backdoors & Breaches
game that integrates large language model teammates with a Bloom-aligned,
retrieval-augmented copilot (C2D2). The system expands a curated corpus into
factual, conceptual, procedural, and metacognitive snippets, delivering
on-demand, cognitively targeted hints. Prompt-engineered agents employ a
scaffolding ladder that gradually fades as learner confidence grows. In a
solo-player pilot with four graduate students, participants reported greater
intention to use the agent-based version compared to the physical card deck and
viewed it as more scalable, though a ceiling effect emerged on a simple
knowledge quiz. Despite limitations of small sample size, single-player focus,
and narrow corpus, these early findings suggest that large language model
augmented TTXs can provide lightweight, repeatable practice without the
logistical burden of traditional exercises. Planned extensions include
multi-player modes, telemetry-driven coaching, and comparative studies with
larger cohorts.

</details>


### [7] [IL-PCSR: Legal Corpus for Prior Case and Statute Retrieval](https://arxiv.org/abs/2511.00268)
*Shounak Paul,Dhananjay Ghumare,Pawan Goyal,Saptarshi Ghosh,Ashutosh Modi*

Main category: cs.CL

TL;DR: 提出了IL-PCR语料库，为法律案件检索和法规检索任务提供统一测试平台，开发了基于LLM的重排序方法以利用两个任务之间的依赖关系。


<details>
  <summary>Details</summary>
Motivation: 法律实践中案件检索和法规检索是相关但被独立处理的任务，研究者希望开发能够利用两者依赖关系的统一模型。

Method: 构建IL-PCR语料库，实验了词法模型、语义模型和基于GNN的集成模型，并开发了基于LLM的重排序方法。

Result: 基于LLM的重排序方法在两个检索任务上取得了最佳性能。

Conclusion: IL-PCR语料库为法律检索任务提供了统一测试平台，基于LLM的重排序方法能有效利用案件检索和法规检索之间的依赖关系。

Abstract: Identifying/retrieving relevant statutes and prior cases/precedents for a
given legal situation are common tasks exercised by law practitioners.
Researchers to date have addressed the two tasks independently, thus developing
completely different datasets and models for each task; however, both retrieval
tasks are inherently related, e.g., similar cases tend to cite similar statutes
(due to similar factual situation). In this paper, we address this gap. We
propose IL-PCR (Indian Legal corpus for Prior Case and Statute Retrieval),
which is a unique corpus that provides a common testbed for developing models
for both the tasks (Statute Retrieval and Precedent Retrieval) that can exploit
the dependence between the two. We experiment extensively with several baseline
models on the tasks, including lexical models, semantic models and ensemble
based on GNNs. Further, to exploit the dependence between the two tasks, we
develop an LLM-based re-ranking approach that gives the best performance.

</details>


### [8] [POSESTITCH-SLT: Linguistically Inspired Pose-Stitching for End-to-End Sign Language Translation](https://arxiv.org/abs/2511.00270)
*Abhinav Joshi,Vaibhav Sharma,Sanjeet Singh,Ashutosh Modi*

Main category: cs.CL

TL;DR: 提出POSESTITCH-SLT预训练方案，通过模板生成句子对训练，在How2Sign和iSign数据集上显著提升手语翻译性能，BLEU-4得分分别从1.97提升到4.56和从0.55提升到3.43。


<details>
  <summary>Details</summary>
Motivation: 手语翻译面临大规模句子对齐数据集稀缺的挑战，需要新的方法来提升低资源环境下的翻译性能。

Method: 提出POSESTITCH-SLT预训练方案，基于语言模板的句子生成技术，使用简单的基于transformer的编码器-解码器架构。

Result: 在How2Sign数据集上BLEU-4从1.97提升到4.56，在iSign数据集上从0.55提升到3.43，超越了基于姿态的无注释翻译的现有最佳方法。

Conclusion: 模板驱动的合成监督在低资源手语设置中具有显著效果，证明了该方法在解决数据稀缺问题上的有效性。

Abstract: Sign language translation remains a challenging task due to the scarcity of
large-scale, sentence-aligned datasets. Prior arts have focused on various
feature extraction and architectural changes to support neural machine
translation for sign languages. We propose POSESTITCH-SLT, a novel pre-training
scheme that is inspired by linguistic-templates-based sentence generation
technique. With translation comparison on two sign language datasets, How2Sign
and iSign, we show that a simple transformer-based encoder-decoder architecture
outperforms the prior art when considering template-generated sentence pairs in
training. We achieve BLEU-4 score improvements from 1.97 to 4.56 on How2Sign
and from 0.55 to 3.43 on iSign, surpassing prior state-of-the-art methods for
pose-based gloss-free translation. The results demonstrate the effectiveness of
template-driven synthetic supervision in low-resource sign language settings.

</details>


### [9] [Language Modeling With Factorization Memory](https://arxiv.org/abs/2511.00315)
*Lee Xiong,Maksim Tkachenko,Johanes Effendi,Ting Cai*

Main category: cs.CL

TL;DR: 提出Factorization Memory，一种高效的RNN架构，在短上下文语言建模任务中性能与Transformer相当，在长上下文场景中具有更优的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 开发一种既能利用并行计算训练，又能在推理时保持恒定计算和内存复杂度的RNN架构，以在短上下文和长上下文场景中都保持竞争力。

Method: 基于Mamba-2构建Factorization Memory，支持训练时并行计算和推理时恒定复杂度。进一步开发稀疏版本，只更新部分循环状态但保持密集版本性能。

Result: Factorization Memory在短上下文任务中性能与Transformer相当，在长上下文中表现更优。稀疏版本在保持性能的同时提高了效率。

Conclusion: 这是第一个成功将稀疏内存激活与在短长上下文场景中都具有竞争力的性能相结合的RNN架构，为RNN在语言建模中的应用提供了新方向。

Abstract: We propose Factorization Memory, an efficient recurrent neural network (RNN)
architecture that achieves performance comparable to Transformer models on
short-context language modeling tasks while also demonstrating superior
generalization in long-context scenarios. Our model builds upon Mamba-2,
enabling Factorization Memory to exploit parallel computations during training
while preserving constant computational and memory complexity during inference.
To further optimize model efficiency and representational capacity, we develop
a sparse formulation of Factorization Memory that updates only a subset of
recurrent states at each step while preserving the strong performance of its
dense counterpart. To our knowledge, this represents the first RNN architecture
that successfully combines sparse memory activation with competitive
performance across both short and long-context settings. This work provides a
systematic empirical analysis of Factorization Memory in comparison to
Transformer and Mamba-2 architectures.

</details>


### [10] [Reversal Invariance in Autoregressive Language Models](https://arxiv.org/abs/2511.00341)
*Mihir Sahasrabudhe*

Main category: cs.CL

TL;DR: 论文形式化定义了因果语言建模目标的逆转不变性结构特性，指出标准CLM预训练是方向盲目的，这限制了模型捕捉语言中方向性依赖的能力。


<details>
  <summary>Details</summary>
Motivation: 自然语言具有时间不对称性，包含语音、形态和因果等方向性依赖，但当前预训练目标的对称性可能无法有效捕捉这些特性。

Method: 通过理论分析形式化逆转不变性概念，证明标准CLM损失函数对语料库及其逆转赋予相同似然，并提出从时间不对称视角重新审视预训练。

Result: 研究发现标准CLM预训练具有方向盲目性，在逆转文本上训练的模型能达到与正向文本相当的性能，这反映了当前预训练目标的局限性。

Conclusion: 应开发能显式建模语言方向性的损失函数和架构，同时保持标准语言建模能力，以更好地捕捉自然语言中的方向性依赖。

Abstract: We formalize a structural property of the causal (autoregressive) language
modeling (CLM) objective: reversal invariance. Formally, the next-token
prediction loss assigns identical likelihood to a corpus and its reversal,
implying that standard CLM pretraining is direction-blind. This symmetry
explains why models trained on reversed text can achieve comparable performance
to those trained on forward text, despite the inherently time-asymmetric nature
of human language and reasoning. We argue that this invariance represents a
limitation of current pretraining objectives rather than a benign artifact. If
natural language encodes directional dependencies - phonological,
morphological, or causal - a symmetric objective may fail to capture them. We
therefore propose viewing pretraining through the lens of temporal asymmetry,
motivating future work on loss functions and architectures that explicitly
model the arrow of language while retaining standard language modeling
capacity.

</details>


### [11] [LingGym: How Far Are LLMs from Thinking Like Field Linguists?](https://arxiv.org/abs/2511.00343)
*Changbing Yang,Franklin Ma,Freda Shi,Jian Zhu*

Main category: cs.CL

TL;DR: LingGym是一个评估LLM元语言推理能力的新基准，使用跨行注释文本和语法描述，测试模型在未见过的低资源语言和结构上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 评估LLM是否能在训练中未见过的低资源语言和结构上进行语言推理泛化，而不仅仅是特定下游任务。

Method: 使用从18种类型多样的参考语法中提取的跨行注释文本和语法描述，设计了受控评估任务：词-注释推理，模型必须根据上下文推断缺失的词和注释。

Result: 结合结构化语言线索在所有模型中都能持续提升推理性能。

Conclusion: 这项工作凸显了使用LLM进行类型学语言分析和低资源语言文档化的前景和当前局限性。

Abstract: This paper introduces LingGym, a new benchmark that evaluates LLMs' capacity
for meta-linguistic reasoning using Interlinear Glossed Text (IGT) and
grammatical descriptions extracted from 18 typologically diverse reference
grammars. Unlike previous work that focuses on specific downstream tasks, we
assess whether LLMs can generalize linguistic inference across low-resource
languages and structures not seen during training. We present a controlled
evaluation task: Word-Gloss Inference, in which the model must infer a missing
word and gloss from context using varying levels of linguistic information
(e.g., glosses, grammatical explanations, translations). Our results show that
incorporating structured linguistic cues leads to consistent improvements in
reasoning performance across all models. This work highlights both the promise
and current limitations of using LLMs for typologically informed linguistic
analysis and low-resource language documentation.

</details>


### [12] [Reasoning Trajectories for Socratic Debugging of Student Code: From Misconceptions to Contradictions and Updated Beliefs](https://arxiv.org/abs/2511.00371)
*Erfan Al-Hossami,Razvan Bunescu*

Main category: cs.CL

TL;DR: 该论文提出了推理轨迹生成任务，用于苏格拉底式调试，通过引导性推理路径帮助学生识别和修复编程错误，并开发了基于LLM的解决方案。


<details>
  <summary>Details</summary>
Motivation: 大多数新手程序员的错误源于编程误解，苏格拉底式调试可以引导学生通过推理轨迹发现与错误观念相矛盾的程序行为，从而更新错误信念。

Method: 引入推理轨迹生成任务，构建手动标注的数据集，开发基于LLM的推理轨迹生成和苏格拉底对话解决方案。

Result: 前沿模型能够生成高达91%正确的推理轨迹和98.7%有效的对话轮次。

Conclusion: LLM能够有效生成用于苏格拉底式调试的推理轨迹和对话，帮助学生自主识别和修复编程错误。

Abstract: In Socratic debugging, instructors guide students towards identifying and
fixing a bug on their own, instead of providing the bug fix directly. Most
novice programmer bugs are caused by programming misconceptions, namely false
beliefs about a programming concept. In this context, Socratic debugging can be
formulated as a guided Reasoning Trajectory (RT) leading to a statement about
the program behavior that contradicts the bug-causing misconception. Upon
reaching this statement, the ensuing cognitive dissonance leads the student to
first identify and then update their false belief. In this paper, we introduce
the task of reasoning trajectory generation, together with a dataset of
debugging problems manually annotated with RTs. We then describe LLM-based
solutions for generating RTs and Socratic conversations that are anchored on
them. A large-scale LLM-as-judge evaluation shows that frontier models can
generate up to 91% correct reasoning trajectories and 98.7% valid conversation
turns.

</details>


### [13] [PADBen: A Comprehensive Benchmark for Evaluating AI Text Detectors Against Paraphrase Attacks](https://arxiv.org/abs/2511.00416)
*Yiwei Zha,Rui Min,Shanu Sushmita*

Main category: cs.CL

TL;DR: AI生成文本检测器对直接LLM输出准确率超过90%，但对迭代改写内容检测失败。研究发现迭代改写创建了一个语义位移但保留生成模式的中间清洗区域，导致两种攻击类型：改写人类文本（作者身份混淆）和改写LLM生成文本（抄袭规避）。


<details>
  <summary>Details</summary>
Motivation: 研究现有AI生成文本检测器在迭代改写攻击下的脆弱性，揭示检测系统无法有效识别经过多次改写的AI生成内容的问题。

Method: 通过内在机制分析揭示迭代改写创建中间清洗区域，并构建PADBen基准系统评估检测器在两种改写攻击场景下的鲁棒性，包含五种文本类型和五个渐进检测任务。

Result: 评估11个最先进检测器发现关键不对称性：检测器能成功识别抄袭规避问题，但在作者身份混淆情况下失败，无法有效处理中间清洗区域。

Conclusion: 当前检测方法无法有效处理中间清洗区域，需要在检测架构上进行根本性改进，超越现有的语义和风格判别方法。

Abstract: While AI-generated text (AIGT) detectors achieve over 90\% accuracy on direct
LLM outputs, they fail catastrophically against iteratively-paraphrased
content. We investigate why iteratively-paraphrased text -- itself AI-generated
-- evades detection systems designed for AIGT identification. Through intrinsic
mechanism analysis, we reveal that iterative paraphrasing creates an
intermediate laundering region characterized by semantic displacement with
preserved generation patterns, which brings up two attack categories:
paraphrasing human-authored text (authorship obfuscation) and paraphrasing
LLM-generated text (plagiarism evasion). To address these vulnerabilities, we
introduce PADBen, the first benchmark systematically evaluating detector
robustness against both paraphrase attack scenarios. PADBen comprises a
five-type text taxonomy capturing the full trajectory from original content to
deeply laundered text, and five progressive detection tasks across
sentence-pair and single-sentence challenges. We evaluate 11 state-of-the-art
detectors, revealing critical asymmetry: detectors successfully identify the
plagiarism evasion problem but fail for the case of authorship obfuscation. Our
findings demonstrate that current detection approaches cannot effectively
handle the intermediate laundering region, necessitating fundamental advances
in detection architectures beyond existing semantic and stylistic
discrimination methods. For detailed code implementation, please see
https://github.com/JonathanZha47/PadBen-Paraphrase-Attack-Benchmark.

</details>


### [14] [MedRECT: A Medical Reasoning Benchmark for Error Correction in Clinical Texts](https://arxiv.org/abs/2511.00421)
*Naoto Iwase,Hiroki Okuyama,Junichiro Iwasawa*

Main category: cs.CL

TL;DR: MedRECT是首个跨语言（日语/英语）医疗错误处理基准，包含错误检测、定位和修正三个子任务。评估显示推理模型表现最佳，跨语言性能存在差距，微调后模型在结构化医疗错误修正任务上超越人类专家。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医疗应用中的潜力日益显现，但其检测和修正临床文本错误的能力（安全部署的前提）仍未充分评估，特别是在英语以外的语言中。

Method: 从日本医师国家考试构建可扩展的自动化流水线，创建MedRECT-ja（663文本）和MedRECT-en（458文本）数据集。评估9个当代LLM，包括专有、开源和推理模型，并进行针对性LoRA微调。

Result: 推理模型显著优于标准架构，错误检测相对提升13.5%，句子提取提升51.0%；英语到日语存在5-10%性能差距；微调在错误修正上获得不对称改进（日语+0.078，英语+0.168）；微调模型在结构化医疗错误修正任务上超越人类专家。

Conclusion: MedRECT为开发更安全的跨语言医疗LLM提供了首个全面的跨语言基准、可复现框架和资源，推理模型和针对性微调能有效提升医疗错误处理能力。

Abstract: Large language models (LLMs) show increasing promise in medical applications,
but their ability to detect and correct errors in clinical texts -- a
prerequisite for safe deployment -- remains under-evaluated, particularly
beyond English. We introduce MedRECT, a cross-lingual benchmark
(Japanese/English) that formulates medical error handling as three subtasks:
error detection, error localization (sentence extraction), and error
correction. MedRECT is built with a scalable, automated pipeline from the
Japanese Medical Licensing Examinations (JMLE) and a curated English
counterpart, yielding MedRECT-ja (663 texts) and MedRECT-en (458 texts) with
comparable error/no-error balance. We evaluate 9 contemporary LLMs spanning
proprietary, open-weight, and reasoning families. Key findings: (i) reasoning
models substantially outperform standard architectures, with up to 13.5%
relative improvement in error detection and 51.0% in sentence extraction; (ii)
cross-lingual evaluation reveals 5-10% performance gaps from English to
Japanese, with smaller disparities for reasoning models; (iii) targeted LoRA
fine-tuning yields asymmetric improvements in error correction performance
(Japanese: +0.078, English: +0.168) while preserving reasoning capabilities;
and (iv) our fine-tuned model exceeds human expert performance on structured
medical error correction tasks. To our knowledge, MedRECT is the first
comprehensive cross-lingual benchmark for medical error correction, providing a
reproducible framework and resources for developing safer medical LLMs across
languages.

</details>


### [15] [G2: Guided Generation for Enhanced Output Diversity in LLMs](https://arxiv.org/abs/2511.00432)
*Zhiwen Ruan,Yixia Li,Yefeng Liu,Yun Chen,Weihua Luo,Peng Li,Yang Liu,Guanhua Chen*

Main category: cs.CL

TL;DR: 提出G2方法，一种无需训练的即插即用技术，通过双引导机制在解码过程中增强LLM输出多样性，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在输出多样性方面存在局限，多次尝试生成高度相似内容，影响需要多样输出的任务。现有方法如温度调节虽能提升多样性但会牺牲输出质量。

Method: 使用基础生成器和双引导器，通过基于解码的干预来指导生成过程，在原始查询条件下鼓励更多样化的输出。

Result: 综合实验表明，G2有效提升了输出多样性，同时在多样性和质量之间保持了最佳平衡。

Conclusion: G2方法成功解决了LLM输出多样性不足的问题，实现了在保持生成质量的同时显著提升多样性的目标。

Abstract: Large Language Models (LLMs) have demonstrated exceptional performance across
diverse natural language processing tasks. However, these models exhibit a
critical limitation in output diversity, often generating highly similar
content across multiple attempts. This limitation significantly affects tasks
requiring diverse outputs, from creative writing to reasoning. Existing
solutions, like temperature scaling, enhance diversity by modifying probability
distributions but compromise output quality. We propose Guide-to-Generation
(G2), a training-free plug-and-play method that enhances output diversity while
preserving generation quality. G2 employs a base generator alongside dual
Guides, which guide the generation process through decoding-based interventions
to encourage more diverse outputs conditioned on the original query.
Comprehensive experiments demonstrate that G2 effectively improves output
diversity while maintaining an optimal balance between diversity and quality.

</details>


### [16] [Remembering Unequally: Global and Disciplinary Bias in LLM-Generated Co-Authorship Networks](https://arxiv.org/abs/2511.00476)
*Ghazal Kalhor,Afra Mashhadi*

Main category: cs.CL

TL;DR: 本研究探讨大型语言模型记忆效应对合著网络的影响，发现模型在生成合著关系时存在系统性偏见，偏向高被引研究者，但这种偏见在不同学科和地区存在差异。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在学术搜索和推荐系统中的广泛应用，其记忆效应可能导致合著网络生成中的公平性和偏见问题，影响信息生态系统的完整性。

Method: 评估DeepSeek R1、Llama 4 Scout和Mixtral 8x7B三个主流模型的记忆效应，分析记忆驱动输出在不同学科和世界地区的差异。

Result: 全球分析显示模型存在偏向高被引研究者的系统性偏见，但临床医学等学科和非洲部分地区表现出更均衡的代表性，表明LLM训练数据在某些领域可能更公平。

Conclusion: LLMs在学术发现应用中既存在风险也蕴含机遇，需要关注其记忆效应对合著网络公平性的影响。

Abstract: Ongoing breakthroughs in Large Language Models (LLMs) are reshaping search
and recommendation platforms at their core. While this shift unlocks powerful
new scientometric tools, it also exposes critical fairness and bias issues that
could erode the integrity of the information ecosystem. Additionally, as LLMs
become more integrated into web-based searches for scholarly tools, their
ability to generate summarized research work based on memorized data introduces
new dimensions to these challenges. The extent of memorization in LLMs can
impact the accuracy and fairness of the co-authorship networks they produce,
potentially reflecting and amplifying existing biases within the scientific
community and across different regions. This study critically examines the
impact of LLM memorization on the co-authorship networks. To this end, we
assess memorization effects across three prominent models, DeepSeek R1, Llama 4
Scout, and Mixtral 8x7B, analyzing how memorization-driven outputs vary across
academic disciplines and world regions. While our global analysis reveals a
consistent bias favoring highly cited researchers, this pattern is not
uniformly observed. Certain disciplines, such as Clinical Medicine, and
regions, including parts of Africa, show more balanced representation, pointing
to areas where LLM training data may reflect greater equity. These findings
underscore both the risks and opportunities in deploying LLMs for scholarly
discovery.

</details>


### [17] [Leveraging the Cross-Domain & Cross-Linguistic Corpus for Low Resource NMT: A Case Study On Bhili-Hindi-English Parallel Corpus](https://arxiv.org/abs/2511.00486)
*Pooja Singh,Shashwat Bhardwaj,Vaibhav Sharma,Sandeep Kumar*

Main category: cs.CL

TL;DR: 本文构建了首个大规模Bhili-Hindi-English平行语料库(BHEPC)，包含11万句经过精心整理的句子，填补了Bhili这一印度部落语言在机器翻译资源上的空白。


<details>
  <summary>Details</summary>
Motivation: 印度语言多样性给机器翻译带来挑战，特别是像Bhili这样的部落语言缺乏高质量语言资源，需要填补这一资源缺口。

Method: 通过专家人工翻译创建平行语料库，评估多种专有和开源多语言大语言模型在双向翻译任务上的表现，并研究基于上下文学习的生成式翻译能力。

Result: 微调后的NLLB-200 distilled 600M变体模型表现最佳，证明了多语言模型在低资源场景下的潜力。

Conclusion: 这项工作填补了关键资源缺口，促进了全球低资源和边缘化语言的包容性自然语言处理技术发展。

Abstract: The linguistic diversity of India poses significant machine translation
challenges, especially for underrepresented tribal languages like Bhili, which
lack high-quality linguistic resources. This paper addresses the gap by
introducing Bhili-Hindi-English Parallel Corpus (BHEPC), the first and largest
parallel corpus worldwide comprising 110,000 meticulously curated sentences
across Bhili, Hindi, and English. The corpus was created with the assistance of
expert human translators. BHEPC spans critical domains such as education,
administration, and news, establishing a valuable benchmark for research in low
resource machine translation. To establish a comprehensive Bhili Machine
Translation benchmark, we evaluated a wide range of proprietary and open-source
Multilingual Large Language Models (MLLMs) on bidirectional translation tasks
between English/Hindi and Bhili. Comprehensive evaluation demonstrates that the
fine-tuned NLLB-200 distilled 600M variant model outperforms others,
highlighting the potential of multilingual models in low resource scenarios.
Furthermore, we investigated the generative translation capabilities of
multilingual LLMs on BHEPC using in-context learning, assessing performance
under cross-domain generalization and quantifying distributional divergence.
This work bridges a critical resource gap and promotes inclusive natural
language processing technologies for low-resource and marginalized languages
globally.

</details>


### [18] [With Privacy, Size Matters: On the Importance of Dataset Size in Differentially Private Text Rewriting](https://arxiv.org/abs/2511.00487)
*Stephen Meisenbacher,Florian Matthes*

Main category: cs.CL

TL;DR: 本文首次在差分隐私文本隐私化评估中引入数据集大小因素，通过在大规模数据集上设计动态分割大小的效用和隐私测试，揭示了数据集大小对隐私-效用权衡的重要影响。


<details>
  <summary>Details</summary>
Motivation: 当前差分隐私自然语言处理研究在评估文本重写机制时往往忽略数据集大小的影响，本文旨在填补这一空白，研究数据集规模对机制效用和隐私保护效果的影响。

Method: 设计在大规模数据集上的效用和隐私测试，使用动态分割大小，在包含多达100万个文本的不同规模数据集上运行测试，量化数据集大小增加对隐私-效用权衡的影响。

Result: 研究发现数据集大小在评估差分隐私文本重写机制中起着关键作用，数据集规模的变化显著影响隐私保护与数据效用之间的平衡关系。

Conclusion: 研究结果呼吁差分隐私自然语言处理领域需要更严格的评估程序，并为差分隐私自然语言处理在实际应用和大规模部署中的未来发展提供了重要见解。

Abstract: Recent work in Differential Privacy with Natural Language Processing (DP NLP)
has proposed numerous promising techniques in the form of text rewriting
mechanisms. In the evaluation of these mechanisms, an often-ignored aspect is
that of dataset size, or rather, the effect of dataset size on a mechanism's
efficacy for utility and privacy preservation. In this work, we are the first
to introduce this factor in the evaluation of DP text privatization, where we
design utility and privacy tests on large-scale datasets with dynamic split
sizes. We run these tests on datasets of varying size with up to one million
texts, and we focus on quantifying the effect of increasing dataset size on the
privacy-utility trade-off. Our findings reveal that dataset size plays an
integral part in evaluating DP text rewriting mechanisms; additionally, these
findings call for more rigorous evaluation procedures in DP NLP, as well as
shed light on the future of DP NLP in practice and at scale.

</details>


### [19] [ToM: Leveraging Tree-oriented MapReduce for Long-Context Reasoning in Large Language Models](https://arxiv.org/abs/2511.00489)
*Jiani Guo,Zuchao Li,Jie Wu,Qianren Wang,Yun Li,Lefei Zhang,Hai Zhao,Yujiu Yang*

Main category: cs.CL

TL;DR: 提出了ToM框架，一种面向树结构的MapReduce方法，用于解决大语言模型在长上下文推理中的性能下降问题，通过层次化语义解析构建文档树并进行自底向上聚合。


<details>
  <summary>Details</summary>
Motivation: 现有方法如检索增强生成(RAG)和分治框架(DCF)在处理长文档时存在逻辑一致性差、无法捕捉长距离依赖关系以及孤立处理片段导致冲突的问题。

Method: 通过层次化语义解析构建文档树(DocTree)，采用树形MapReduce方法进行递归推理：Map步骤在子节点生成推理依据，Reduce步骤在父节点聚合兄弟节点的推理依据以解决冲突或达成共识。

Result: 在70B+大语言模型上的实验结果表明，ToM显著优于现有的分治框架和检索增强生成方法，实现了更好的逻辑一致性和长上下文推理能力。

Conclusion: ToM框架通过利用文档的层次结构，有效解决了长上下文推理中的挑战，为处理长文档提供了更优的解决方案。

Abstract: Large Language Models (LLMs), constrained by limited context windows, often
face significant performance degradation when reasoning over long contexts. To
address this, Retrieval-Augmented Generation (RAG) retrieves and reasons over
chunks but frequently sacrifices logical coherence due to its reliance on
similarity-based rankings. Similarly, divide-and-conquer frameworks (DCF) split
documents into small chunks for independent reasoning and aggregation. While
effective for local reasoning, DCF struggles to capture long-range dependencies
and risks inducing conflicts by processing chunks in isolation. To overcome
these limitations, we propose ToM, a novel Tree-oriented MapReduce framework
for long-context reasoning. ToM leverages the inherent hierarchical structure
of long documents (e.g., main headings and subheadings) by constructing a
DocTree through hierarchical semantic parsing and performing bottom-up
aggregation. Using a Tree MapReduce approach, ToM enables recursive reasoning:
in the Map step, rationales are generated at child nodes; in the Reduce step,
these rationales are aggregated across sibling nodes to resolve conflicts or
reach consensus at parent nodes. Experimental results on 70B+ LLMs show that
ToM significantly outperforms existing divide-and-conquer frameworks and
retrieval-augmented generation methods, achieving better logical coherence and
long-context reasoning. Our code is available at
https://github.com/gjn12-31/ToM .

</details>


### [20] [Zero-RAG: Towards Retrieval-Augmented Generation with Zero Redundant Knowledge](https://arxiv.org/abs/2511.00505)
*Qi Luo,Xiaonan Li,Junqi Dai,Shuang Cheng,Xipeng Qiu*

Main category: cs.CL

TL;DR: Zero-RAG通过识别和修剪RAG外部知识库中的冗余知识，减少检索负担并提升LLM内部知识的利用效率，在保持性能的同时加速检索过程。


<details>
  <summary>Details</summary>
Motivation: 随着LLM内部知识的扩展，RAG中的外部知识库存在大量冗余知识，这增加了密集检索的负担，并且冗余知识反而会降低LLM对自身知识的利用效率。

Method: 提出Mastery-Score指标识别冗余知识并进行修剪；使用Query Router和Noise-Tolerant Tuning来避免不相关文档的干扰，提升LLM对内部知识的利用。

Result: 实验结果显示，Zero-RAG将维基百科语料库修剪了30%，检索阶段加速了22%，且不损害RAG性能。

Conclusion: Zero-RAG有效解决了RAG中知识冗余问题，通过修剪冗余知识和优化内部知识利用，实现了检索效率的提升和性能的保持。

Abstract: Retrieval-Augmented Generation has shown remarkable results to address Large
Language Models' hallucinations, which usually uses a large external corpus to
supplement knowledge to LLMs. However, with the development of LLMs, the
internal knowledge of LLMs has expanded significantly, thus causing significant
knowledge redundancy between the external corpus and LLMs. On the one hand, the
indexing cost of dense retrieval is highly related to the corpus size and thus
significant redundant knowledge intensifies the dense retrieval's workload. On
the other hand, the redundant knowledge in the external corpus is not helpful
to LLMs and our exploratory analysis shows that it instead hurts the RAG
performance on those questions which the LLM can answer by itself. To address
these issues, we propose Zero-RAG to tackle these challenges. Specifically, we
first propose the Mastery-Score metric to identify redundant knowledge in the
RAG corpus to prune it. After pruning, answers to "mastered" questions rely
primarily on internal knowledge of the LLM. To better harness the internal
capacity, we propose Query Router and Noise-Tolerant Tuning to avoid the
irrelevant documents' distraction and thus further improve the LLM's
utilization of internal knowledge with pruned corpus. Experimental results show
that Zero-RAG prunes the Wikipedia corpus by 30\% and accelerates the retrieval
stage by 22\%, without compromising RAG's performance.

</details>


### [21] [Fine-Tuning DialoGPT on Common Diseases in Rural Nepal for Medical Conversations](https://arxiv.org/abs/2511.00514)
*Birat Poudel,Satyam Ghimire,Er. Prakash Chandra Prasad*

Main category: cs.CL

TL;DR: 在尼泊尔农村地区开发离线运行的轻量级对话模型DialoGPT，用于医疗对话支持，覆盖10种常见疾病，模型在有限领域数据上表现良好。


<details>
  <summary>Details</summary>
Motivation: 解决资源受限地区（如尼泊尔农村）缺乏互联网连接和云基础设施的问题，为医疗保健提供对话支持。

Method: 使用合成构建的医患交互数据集对轻量级生成对话模型DialoGPT进行微调，数据集涵盖尼泊尔农村常见的10种疾病。

Result: 尽管训练数据有限且领域特定，微调后的模型能够生成连贯、上下文相关且医学上适当的响应，表现出对症状、疾病背景和同理心沟通的理解。

Conclusion: 紧凑型离线对话模型具有适应性，针对性数据集在低资源医疗环境中领域适应有效，为未来农村医疗对话AI提供了有前景的方向。

Abstract: Conversational agents are increasingly being explored to support healthcare
delivery, particularly in resource-constrained settings such as rural Nepal.
Large-scale conversational models typically rely on internet connectivity and
cloud infrastructure, which may not be accessible in rural areas. In this
study, we fine-tuned DialoGPT, a lightweight generative dialogue model that can
operate offline, on a synthetically constructed dataset of doctor-patient
interactions covering ten common diseases prevalent in rural Nepal, including
common cold, seasonal fever, diarrhea, typhoid fever, gastritis, food
poisoning, malaria, dengue fever, tuberculosis, and pneumonia. Despite being
trained on a limited, domain-specific dataset, the fine-tuned model produced
coherent, contextually relevant, and medically appropriate responses,
demonstrating an understanding of symptoms, disease context, and empathetic
communication. These results highlight the adaptability of compact,
offline-capable dialogue models and the effectiveness of targeted datasets for
domain adaptation in low-resource healthcare environments, offering promising
directions for future rural medical conversational AI.

</details>


### [22] [Surfacing Subtle Stereotypes: A Multilingual, Debate-Oriented Evaluation of Modern LLMs](https://arxiv.org/abs/2511.01187)
*Muhammed Saeed,Muhammad Abdul-mageed,Shady Shehata*

Main category: cs.CL

TL;DR: DebateBias-8K是一个多语言、辩论风格的基准测试，用于评估LLM在生成式环境中的叙事偏见，涵盖4个敏感领域和7种语言，发现模型普遍存在刻板印象偏见。


<details>
  <summary>Details</summary>
Motivation: 现有的偏见评估主要依赖英语分类任务，无法反映真实生成式环境中的叙事偏见，需要开发多语言、辩论风格的评估基准。

Method: 构建包含8,400个结构化辩论提示的数据集，涵盖4个敏感领域和7种语言，使用4个主流模型生成并自动分类超过100,000个响应。

Result: 所有模型都再现了根深蒂固的刻板印象：阿拉伯人与恐怖主义和宗教高度关联(≥95%)，非洲人与社会经济"落后"关联(高达77%)，西方群体被一致描述为现代或进步。低资源语言的偏见更严重。

Conclusion: 当前主要基于英语的对齐方法无法在全球范围内泛化，需要开发多语言偏见评估和更安全、文化包容的模型对齐方法。

Abstract: Large language models (LLMs) are widely deployed for open-ended
communication, yet most bias evaluations still rely on English,
classification-style tasks. We introduce DebateBias-8K, a new multilingual,
debate-style benchmark designed to reveal how narrative bias appears in
realistic generative settings. Our dataset includes 8,400 structured debate
prompts spanning four sensitive domains: women's rights, socioeconomic
development, terrorism, and religion, across seven languages ranging from
high-resource (English, Chinese) to low-resource (Swahili, Nigerian Pidgin).
Using four flagship models (GPT-4o, Claude 3, DeepSeek, and LLaMA 3), we
generate and automatically classify over 100,000 responses. Results show that
all models reproduce entrenched stereotypes despite safety alignment: Arabs are
overwhelmingly linked to terrorism and religion (>=95%), Africans to
socioeconomic "backwardness" (up to <=77%), and Western groups are consistently
framed as modern or progressive. Biases grow sharply in lower-resource
languages, revealing that alignment trained primarily in English does not
generalize globally. Our findings highlight a persistent divide in multilingual
fairness: current alignment methods reduce explicit toxicity but fail to
prevent biased outputs in open-ended contexts. We release our DebateBias-8K
benchmark and analysis framework to support the next generation of multilingual
bias evaluation and safer, culturally inclusive model alignment.

</details>


### [23] [Exploring and Mitigating Gender Bias in Encoder-Based Transformer Models](https://arxiv.org/abs/2511.00519)
*Ariyan Hossain,Khondokar Mohammad Ahanaf Hannan,Rakinul Haque,Nowreen Tarannum Rafa,Humayra Musarrat,Shoaib Ahmed Dipu,Farig Yousuf Sadeque*

Main category: cs.CL

TL;DR: 本文研究了BERT、ALBERT、RoBERTa和DistilBERT等transformer模型中存在的性别偏见问题，提出了一种新的偏见度量指标MALoR，并通过反事实数据增强的继续预训练方法有效降低了性别偏见。


<details>
  <summary>Details</summary>
Motivation: 编码器型transformer模型在各种语言任务中取得了最先进的性能，但研究表明它们继承了训练数据中的强烈性别偏见。本文旨在调查这些模型中情境化词嵌入的性别偏见问题。

Method: 引入新的偏见度量指标MALoR，基于模型填充掩码标记的概率来评估偏见程度；提出通过反事实数据增强生成性别平衡数据集进行继续预训练的缓解方法。

Result: 缓解方法显著降低了不同代词对的性别偏见分数。例如在BERT-base中，"he-she"偏见分数从1.27降至0.08，"his-her"从2.51降至0.36；在BERT-large中，"male-female"偏见从1.82降至0.10。

Conclusion: 该方法能有效减少性别偏见，同时不会影响模型在下游任务上的性能表现。

Abstract: Gender bias in language models has gained increasing attention in the field
of natural language processing. Encoder-based transformer models, which have
achieved state-of-the-art performance in various language tasks, have been
shown to exhibit strong gender biases inherited from their training data. This
paper investigates gender bias in contextualized word embeddings, a crucial
component of transformer-based models. We focus on prominent architectures such
as BERT, ALBERT, RoBERTa, and DistilBERT to examine their vulnerability to
gender bias. To quantify the degree of bias, we introduce a novel metric,
MALoR, which assesses bias based on model probabilities for filling masked
tokens. We further propose a mitigation approach involving continued
pre-training on a gender-balanced dataset generated via Counterfactual Data
Augmentation. Our experiments reveal significant reductions in gender bias
scores across different pronoun pairs. For instance, in BERT-base, bias scores
for "he-she" dropped from 1.27 to 0.08, and "his-her" from 2.51 to 0.36
following our mitigation approach. We also observed similar improvements across
other models, with "male-female" bias decreasing from 1.82 to 0.10 in
BERT-large. Our approach effectively reduces gender bias without compromising
model performance on downstream tasks.

</details>


### [24] [Math anxiety and associative knowledge structure are entwined in psychology students but not in Large Language Models like GPT-3.5 and GPT-4o](https://arxiv.org/abs/2511.01558)
*Luciana Ciringione,Emma Franchino,Simone Reigl,Isaia D'Onofrio,Anna Serbati,Oleksandra Poquet,Florence Gabriel,Massimo Stella*

Main category: cs.CL

TL;DR: 该研究使用行为形式心智网络框架分析心理学大学生对数学和焦虑概念的感知差异，发现学生中"焦虑"的正向评价和网络度以及"数学"的负向评价能预测数学焦虑水平，但这些模型不适用于GPT模拟数据。


<details>
  <summary>Details</summary>
Motivation: 数学焦虑严重影响心理学大学生的职业选择和心理健康，需要了解学生对数学和焦虑概念的具体感知和关联方式。

Method: 采用行为形式心智网络框架，通过4个实验比较心理学本科生（n1=70, n2=57）与GPT模拟学生（GPT-3.5: n2=300; GPT-4o: n4=300）的概念网络特征。

Result: 在学生中，"焦虑"的正向评价和网络度以及"数学"的负向评价能预测更高的数学焦虑水平；高数学焦虑学生对"焦虑"有情绪极化认知，"科学"被正面评价但与"数学"形成对比。

Conclusion: 理解学生对概念的感知和关联方式对管理数学焦虑至关重要，GPT模拟数据与真实学生存在显著差异，不能直接应用相同模型。

Abstract: Math anxiety poses significant challenges for university psychology students,
affecting their career choices and overall well-being. This study employs a
framework based on behavioural forma mentis networks (i.e. cognitive models
that map how individuals structure their associative knowledge and emotional
perceptions of concepts) to explore individual and group differences in the
perception and association of concepts related to math and anxiety. We
conducted 4 experiments involving psychology undergraduates from 2 samples (n1
= 70, n2 = 57) compared against GPT-simulated students (GPT-3.5: n2 = 300;
GPT-4o: n4 = 300). Experiments 1, 2, and 3 employ individual-level network
features to predict psychometric scores for math anxiety and its facets
(observational, social and evaluational) from the Math Anxiety Scale.
Experiment 4 focuses on group-level perceptions extracted from human students,
GPT-3.5 and GPT-4o's networks. Results indicate that, in students, positive
valence ratings and higher network degree for "anxiety", together with negative
ratings for "math", can predict higher total and evaluative math anxiety. In
contrast, these models do not work on GPT-based data because of differences in
simulated networks and psychometric scores compared to humans. These results
were also reconciled with differences found in the ways that high/low subgroups
of simulated and real students framed semantically and emotionally STEM
concepts. High math-anxiety students collectively framed "anxiety" in an
emotionally polarising way, absent in the negative perception of low
math-anxiety students. "Science" was rated positively, but contrasted against
the negative perception of "math". These findings underscore the importance of
understanding concept perception and associations in managing students' math
anxiety.

</details>


### [25] [Word Salad Chopper: Reasoning Models Waste A Ton Of Decoding Budget On Useless Repetitions, Self-Knowingly](https://arxiv.org/abs/2511.00536)
*Wenya Xie,Shaochen,Zhong,Hoang Anh Duy Le,Zhaozhuo Xu,Jianwen Xie,Zirui Liu*

Main category: cs.CL

TL;DR: 该论文提出了WordSaladChopper(WSC)方法，通过检测LRM中的无用自我重复token（称为"word salad"）并修剪它们，显著减少输出token数量，同时保持推理质量。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型(LRM)输出token成本高昂，其中相当一部分是无用的自我重复token（word salad），这些token消耗解码预算却不增加价值。

Method: 利用LRM在陷入循环时的自感知特性，通过<

>标记的隐藏状态模式检测word salad行为，使用单层线性分类器进行实时检测，检测后通过简单的修剪和重新生成提示来节省长度。

Result: WSC组件实现了显著的长度节省，同时质量损失最小，具有低开销、强节省效果的特点。

Conclusion: WSC或类似组件是所有考虑用户体验的LRM应用的必备组件，因为它能有效移除语义冗余token而不干扰推理轨迹。

Abstract: Large Reasoning Models (LRMs) are often bottlenecked by the high cost of
output tokens. We show that a significant portion of these tokens are useless
self-repetitions - what we call "word salad" - that exhaust the decoding budget
without adding value. Interestingly, we observe that LRMs are self-aware when
trapped in these loops: the hidden states of <\n\n> tokens trailing each
reasoning chunk exhibit patterns that allow us to detect word salad behavior
on-the-fly via a single-layer linear classifier. Once detected, a simple chop
appended by a straightforward regeneration prompt yields substantial length
savings with minimal quality loss. Our work offers WordSaladChopper (WSC) - a
lightweight, turnkey component for LRM that is minimally invasive to its
reasoning trajectory by only removing semantically redundant tokens. Given its
low overhead, strong savings, and the lack of semantic value of word salad
tokens, we believe it is not too far-fetched to argue that WSC - or a similar
component - is a must-have for all LRM applications with user experience in
mind. Our code is publicly available at
https://github.com/wenyaxie023/WordSaladChopper.

</details>


### [26] [Multi-refined Feature Enhanced Sentiment Analysis Using Contextual Instruction](https://arxiv.org/abs/2511.00537)
*Peter Atandoh,Jie Zou,Weikang Guo,Jiwei Wei,Zheng Wang*

Main category: cs.CL

TL;DR: 提出CISEA-MRFE框架，结合上下文指令、语义增强增强和多精炼特征提取，显著提升情感分析性能，在多个基准数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习和预训练语言模型的情感分析方法在处理细微情感线索、领域转移和不平衡情感分布时表现不佳，主要由于语义基础不足、对多样化语言模式泛化能力差以及对主导情感类别的偏见。

Method: CISEA-MRFE框架包含三个核心组件：上下文指令(CI)注入领域感知指令指导情感消歧；语义增强增强(SEA)通过情感一致释义增强提高鲁棒性；多精炼特征提取(MRFE)结合尺度自适应深度编码器(SADE)进行多尺度特征专业化，以及情感评估器上下文编码器(EECE)进行情感感知序列建模。

Result: 在四个基准数据集上的实验结果表明，CISEA-MRFE始终优于强基线方法，在IMDb上相对准确率提升4.6%，Yelp上6.5%，Twitter上30.3%，Amazon上4.1%。

Conclusion: 实验结果验证了该方法在不同领域情感分类中的有效性和泛化能力。

Abstract: Sentiment analysis using deep learning and pre-trained language models (PLMs)
has gained significant traction due to their ability to capture rich contextual
representations. However, existing approaches often underperform in scenarios
involving nuanced emotional cues, domain shifts, and imbalanced sentiment
distributions. We argue that these limitations stem from inadequate semantic
grounding, poor generalization to diverse linguistic patterns, and biases
toward dominant sentiment classes. To overcome these challenges, we propose
CISEA-MRFE, a novel PLM-based framework integrating Contextual Instruction
(CI), Semantic Enhancement Augmentation (SEA), and Multi-Refined Feature
Extraction (MRFE). CI injects domain-aware directives to guide sentiment
disambiguation; SEA improves robustness through sentiment-consistent
paraphrastic augmentation; and MRFE combines a Scale-Adaptive Depthwise Encoder
(SADE) for multi-scale feature specialization with an Emotion Evaluator Context
Encoder (EECE) for affect-aware sequence modeling. Experimental results on four
benchmark datasets demonstrate that CISEA-MRFE consistently outperforms strong
baselines, achieving relative improvements in accuracy of up to 4.6% on IMDb,
6.5% on Yelp, 30.3% on Twitter, and 4.1% on Amazon. These results validate the
effectiveness and generalization ability of our approach for sentiment
classification across varied domains.

</details>


### [27] [Friend or Foe: How LLMs' Safety Mind Gets Fooled by Intent Shift Attack](https://arxiv.org/abs/2511.00556)
*Peng Ding,Jun Kuang,Wen Sun,Zongyu Wang,Xuezhi Cao,Xunliang Cai,Jiajun Chen,Shujian Huang*

Main category: cs.CL

TL;DR: ISA攻击通过意图转换使LLMs将有害请求误解为良性信息请求，仅需对原始请求进行最小编辑，就能显著提高攻击成功率，现有防御方法对此无效。


<details>
  <summary>Details</summary>
Motivation: 现有越狱攻击主要通过添加干扰上下文或对抗性token来分散LLM注意力，但未改变核心有害意图。需要研究更隐蔽的攻击方式以揭示LLM安全机制的漏洞。

Method: 建立意图转换分类法，通过最小编辑将有害请求转化为看似良性的信息请求，生成自然、人类可读且看似无害的提示。

Result: ISA在开源和商业LLMs上比直接有害提示的攻击成功率提高70%以上，使用ISA模板对良性数据进行微调后成功率接近100%。现有防御方法对ISA无效。

Conclusion: LLMs在意图推断方面存在根本性挑战，需要更有效的防御机制来应对意图转换攻击。

Abstract: Large language models (LLMs) remain vulnerable to jailbreaking attacks
despite their impressive capabilities. Investigating these weaknesses is
crucial for robust safety mechanisms. Existing attacks primarily distract LLMs
by introducing additional context or adversarial tokens, leaving the core
harmful intent unchanged. In this paper, we introduce ISA (Intent Shift
Attack), which obfuscates LLMs about the intent of the attacks. More
specifically, we establish a taxonomy of intent transformations and leverage
them to generate attacks that may be misperceived by LLMs as benign requests
for information. Unlike prior methods relying on complex tokens or lengthy
context, our approach only needs minimal edits to the original request, and
yields natural, human-readable, and seemingly harmless prompts. Extensive
experiments on both open-source and commercial LLMs show that ISA achieves over
70% improvement in attack success rate compared to direct harmful prompts. More
critically, fine-tuning models on only benign data reformulated with ISA
templates elevates success rates to nearly 100%. For defense, we evaluate
existing methods and demonstrate their inadequacy against ISA, while exploring
both training-free and training-based mitigation strategies. Our findings
reveal fundamental challenges in intent inference for LLMs safety and
underscore the need for more effective defenses. Our code and datasets are
available at https://github.com/NJUNLP/ISA.

</details>


### [28] [FlashEVA: Accelerating LLM inference via Efficient Attention](https://arxiv.org/abs/2511.00576)
*Juan Gabriel Kostelec,Qinghai Guo*

Main category: cs.CL

TL;DR: FlashEVA是一种高效的注意力机制实现，通过控制变量方法优化Transformer推理，仅需1.5B tokens微调即可保持下游任务效果，推理吞吐量提升6.7倍，GPU内存使用降低5倍。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在自然语言处理中表现出色，但其推理时维护完整上下文的内存需求带来显著挑战，需要更高效的实现方案。

Method: 提出了FlashEVA，基于EVA（通过控制变量的高效注意力）的高效实现，展示了如何微调Transformer以适应FlashEVA注意力机制。

Result: FlashEVA在推理时实现高达6.7倍的吞吐量提升和5倍的GPU内存峰值使用降低，同时通过可调超参数在吞吐量和精度之间提供权衡控制。

Conclusion: 这项工作代表了向更高效和适应性强的基于Transformer的推理模型迈出的重要一步，尽管在检索密集型任务中存在局限性。

Abstract: Transformer models have revolutionized natural language processing, achieving
state-of-the-art performance and demonstrating remarkable scalability. However,
their memory demands, particularly due to maintaining full context in memory,
pose significant challenges for inference. In this paper, we present FlashEVA,
an efficient implementation of EVA (Efficient Attention via Control Variates),
and demonstrate how to finetune transformers to adapt to FlashEVA attention.
Our method enables fine-tuning of Transformer models with as few as 1.5B tokens
while preserving effectiveness across various downstream tasks. Notably,
FlashEVA achieves up to 6.7x higher throughput and 5x lower peak GPU memory
usage during inference compared to standard Transformer implementations.
Despite these improvements, we observe limitations in retrieval-focused tasks.
Our implementation offers control over the trade-off between throughput and
accuracy through adjustable hyperparameters, providing flexibility for diverse
use cases. This work represents a significant step towards more efficient and
adaptable Transformer-based models for inference.

</details>


### [29] [OpenSIR: Open-Ended Self-Improving Reasoner](https://arxiv.org/abs/2511.00602)
*Wai-Chung Kwan,Joshua Ong Jun Leang,Pavlos Vougiouklis,Jeff Z. Pan,Marco Valentino,Pasquale Minervini*

Main category: cs.CL

TL;DR: OpenSIR是一个无需外部监督的自学习框架，通过让LLM交替扮演教师和学生角色来生成和解决新颖问题，实现了开放式数学发现。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的LLM推理方法依赖带注释的数据集进行可验证奖励，这可能限制模型超越人类水平的能力。自学习提供了一种有前景的替代方案，但现有方法需要外部验证器或无法进行开放式学习。

Method: OpenSIR采用自学习框架，LLM通过交替扮演教师和学生角色来生成和解决新颖问题。为了生成新颖问题，OpenSIR优化难度和多样性，奖励那些适当挑战同时探索不同概念的问题。

Result: 从单个简单种子问题开始，OpenSIR显著提升了指令模型性能：Llama-3.2-3B-Instruct在GSM8K上从73.9提升到78.3，在大学数学上从28.8提升到34.4；Gemma-2-2B-Instruct在GSM8K上从38.5提升到58.7。

Conclusion: 分析显示OpenSIR通过共同进化的教师-学生角色实现开放式学习，这些角色自适应地校准难度并推动多样化探索，从基础数学自主发展到高级数学。

Abstract: Recent advances in large language model (LLM) reasoning through reinforcement
learning rely on annotated datasets for verifiable rewards, which may limit
models' ability to surpass human-level performance. While self-play offers a
promising alternative, existing approaches depend on external verifiers or
cannot learn open-endedly. We present Open-Ended Self-Improving Reasoner
(OpenSIR), a self-play framework where an LLM learns to generate and solve
novel problems by alternating teacher and student roles without external
supervision. To generate novel problems, OpenSIR optimises for both difficulty
and diversity, rewarding problems that challenge appropriately while exploring
distinct concepts, enabling open-ended mathematical discovery. Starting from a
single trivial seed problem, OpenSIR substantially improves instruction models:
Llama-3.2-3B-Instruct advances from 73.9 to 78.3 on GSM8K, and from 28.8 to
34.4 on College Math, while Gemma-2-2B-Instruct rises from 38.5 to 58.7 on
GSM8K. Our analyses reveal that OpenSIR achieves open-ended learning through
co-evolving teacher-student roles that adaptively calibrate difficulty and
drive diverse exploration, progressing autonomously from basic to advanced
mathematics.

</details>


### [30] [SpecDiff-2: Scaling Diffusion Drafter Alignment For Faster Speculative Decoding](https://arxiv.org/abs/2511.00606)
*Jameson Sandler,Jacob K. Christopher,Thomas Hartvigsen,Nando Fioretto*

Main category: cs.CL

TL;DR: SpecDiff-2是一个新颖的推测解码框架，通过离散扩散作为非自回归起草器解决并行性限制，并开发新技术校准起草器和验证器，显著提升LLM推理速度。


<details>
  <summary>Details</summary>
Motivation: 当前推测解码方法存在两个基本瓶颈：起草阶段的自回归依赖限制了并行性，以及起草模型与验证模型不匹配导致的频繁拒绝起草标记。

Method: 使用离散扩散作为非自回归起草器解决并行性问题，开发新技术校准离散扩散起草器与自回归验证器。

Result: 在推理、编码和数学基准测试中达到新的最先进水平，令牌每秒平均提升+55%，相比标准解码获得最高5.5倍平均加速，且无精度损失。

Conclusion: SpecDiff-2通过联合解决推测解码的两个基本瓶颈，实现了显著的推理加速效果。

Abstract: Speculative decoding has become the standard approach for accelerating Large
Language Model (LLM) inference. It exploits a lossless draft-then-verify
procedure to circumvent the latency of autoregressive decoding, achieving
impressive speed-ups. Yet, current speculative decoding approaches remain
limited by two fundamental bottlenecks: (1) the autoregressive dependency
during drafting which limits parallelism, and (2) frequent rejections of draft
tokens caused by misalignment between the draft and verify models. This paper
proposes SpecDiff-2, a novel framework to jointly address these two
bottlenecks. It leverages discrete diffusion as a non-autoregressive drafter to
address bottleneck (1) and develops novel techniques to calibrate discrete
diffusion drafters with autoregressive verifiers, addressing bottleneck (2).
Experimental results across a comprehensive benchmark suite show that
SpecDiff-2 achieves a new state-of-the-art across reasoning, coding, and
mathematical benchmarks, improving tokens-per-second by up to an average of
+55% over previous baselines and obtaining up to 5.5x average speed-up over
standard decoding, without any loss of accuracy.

</details>


### [31] [Certain but not Probable? Differentiating Certainty from Probability in LLM Token Outputs for Probabilistic Scenarios](https://arxiv.org/abs/2511.00620)
*Autumn Toney-Wails,Ryan Wails*

Main category: cs.CL

TL;DR: 研究评估了LLM在概率场景中的不确定性量化能力，发现虽然模型能给出准确答案，但其token级别的概率分布与理论概率分布不一致。


<details>
  <summary>Details</summary>
Motivation: 可靠的不确定性量化对LLM在决策支持等应用中的可信度至关重要，但现有基于token概率的方法在概率场景中可能不足。

Method: 使用GPT-4.1和DeepSeek-Chat评估模型在10个概率提示（如掷骰子）中的表现，测量响应有效性和token概率与理论概率的对齐度。

Result: 两个模型在所有提示场景中都达到了完美的响应准确率，但其token级别的概率和熵值与理论分布持续偏离。

Conclusion: LLM在概率场景中虽然能给出正确结果，但其内部概率表示与理论概率分布存在系统性偏差，这对不确定性量化的可靠性提出了挑战。

Abstract: Reliable uncertainty quantification (UQ) is essential for ensuring
trustworthy downstream use of large language models, especially when they are
deployed in decision-support and other knowledge-intensive applications. Model
certainty can be estimated from token logits, with derived probability and
entropy values offering insight into performance on the prompt task. However,
this approach may be inadequate for probabilistic scenarios, where the
probabilities of token outputs are expected to align with the theoretical
probabilities of the possible outcomes. We investigate the relationship between
token certainty and alignment with theoretical probability distributions in
well-defined probabilistic scenarios. Using GPT-4.1 and DeepSeek-Chat, we
evaluate model responses to ten prompts involving probability (e.g., roll a
six-sided die), both with and without explicit probability cues in the prompt
(e.g., roll a fair six-sided die). We measure two dimensions: (1) response
validity with respect to scenario constraints, and (2) alignment between
token-level output probabilities and theoretical probabilities. Our results
indicate that, while both models achieve perfect in-domain response accuracy
across all prompt scenarios, their token-level probability and entropy values
consistently diverge from the corresponding theoretical distributions.

</details>


### [32] [Modeling the Construction of a Literary Archetype: The Case of the Detective Figure in French Literature](https://arxiv.org/abs/2511.00627)
*Jean Barré,Olga Seminck,Antoine Bourgois,Thierry Poibeau*

Main category: cs.CL

TL;DR: 通过计算分析研究法国侦探小说中侦探原型的演变，发现监督模型能捕捉150年间侦探原型的统一性，并展示其从次要角色到核心推理机器的演变过程。


<details>
  <summary>Details</summary>
Motivation: 探索法国侦探小说中侦探原型在150年间的演变轨迹，理解文学类型中角色原型的动态变化。

Method: 使用定量方法和角色级嵌入技术，通过监督模型分析从1866年到2017年的法国侦探小说文本。

Result: 模型成功捕捉了侦探原型的统一性，发现侦探角色从次要叙事角色演变为古典侦探故事的核心推理机器，二战后受硬汉传统影响变得更加复杂。

Conclusion: 法国侦探小说中的侦探原型经历了从简单到复杂的演变过程，反映了社会暴力和道德模糊性的转向，同时保持了原型的基本统一性。

Abstract: This research explores the evolution of the detective archetype in French
detective fiction through computational analysis. Using quantitative methods
and character-level embeddings, we show that a supervised model is able to
capture the unity of the detective archetype across 150 years of literature,
from M. Lecoq (1866) to Commissaire Adamsberg (2017). Building on this finding,
the study demonstrates how the detective figure evolves from a secondary
narrative role to become the central character and the "reasoning machine" of
the classical detective story. In the aftermath of the Second World War, with
the importation of the hardboiled tradition into France, the archetype becomes
more complex, navigating the genre's turn toward social violence and moral
ambiguity.

</details>


### [33] [Do You Know About My Nation? Investigating Multilingual Language Models' Cultural Literacy Through Factual Knowledge](https://arxiv.org/abs/2511.00657)
*Eshaan Tanwar,Anwoy Chatterjee,Michael Saxon,Alon Albalak,William Yang Wang,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: XNationQA是一个新的多语言问答基准，涵盖9个国家的地理、文化和历史问题，共49,280个问题，使用7种语言。研究发现多语言LLM在不同语言间获取文化特定事实存在显著差异，模型在英语中表现更好，但知识跨语言迁移能力有限。


<details>
  <summary>Details</summary>
Motivation: 现有的大多数多语言问答基准虽然覆盖多种语言，但忽略了区域多样性，倾向于西方中心主义，无法公平评估多语言模型对不同地理位置的实际情况理解能力。

Method: 构建XNationQA基准，包含9个国家的地理、文化和历史问题，使用7种语言。评估8个标准多语言LLM，采用两种新的迁移指标进行分析。

Result: 模型在不同语言间获取文化特定事实存在显著差异，模型在英语中的表现优于相应文化的主导语言。模型在西方语言中表现更好，但这并不一定意味着对西方国家的文化素养更高。开源模型的知识跨语言迁移能力特别有限。

Conclusion: 多语言LLM在文化素养方面存在显著的语言偏见，模型在英语中的知识表现优于其他语言，且知识跨语言迁移能力不足，特别是在开源模型中表现更为明显。

Abstract: Most multilingual question-answering benchmarks, while covering a diverse
pool of languages, do not factor in regional diversity in the information they
capture and tend to be Western-centric. This introduces a significant gap in
fairly evaluating multilingual models' comprehension of factual information
from diverse geographical locations. To address this, we introduce XNationQA
for investigating the cultural literacy of multilingual LLMs. XNationQA
encompasses a total of 49,280 questions on the geography, culture, and history
of nine countries, presented in seven languages. We benchmark eight standard
multilingual LLMs on XNationQA and evaluate them using two novel transference
metrics. Our analyses uncover a considerable discrepancy in the models'
accessibility to culturally specific facts across languages. Notably, we often
find that a model demonstrates greater knowledge of cultural information in
English than in the dominant language of the respective culture. The models
exhibit better performance in Western languages, although this does not
necessarily translate to being more literate for Western countries, which is
counterintuitive. Furthermore, we observe that models have a very limited
ability to transfer knowledge across languages, particularly evident in
open-source models.

</details>


### [34] [Do Methods to Jailbreak and Defend LLMs Generalize Across Languages?](https://arxiv.org/abs/2511.00689)
*Berk Atil,Rebecca J. Passonneau,Fred Morstatter*

Main category: cs.CL

TL;DR: 本文首次对10种不同资源水平的语言进行了系统的多语言越狱攻击和防御评估，发现攻击成功率和防御鲁棒性在不同语言间存在差异，高资源语言在标准查询下更安全但对对抗性攻击更脆弱。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注英语环境下的LLM安全对齐，但跨语言越狱攻击的泛化能力尚未得到充分探索，需要评估不同语言环境下的安全漏洞。

Method: 在HarmBench和AdvBench上使用6个LLM，评估了基于逻辑表达式和对抗性提示两种越狱攻击类型，覆盖10种高、中、低资源语言。

Result: 攻击成功率和防御效果因语言和模型而异：高资源语言在标准查询下更安全，但对对抗性攻击更脆弱；简单防御方法有效但具有语言和模型依赖性。

Conclusion: 研究结果表明需要开发语言感知和跨语言的安全基准测试来评估LLM的安全性。

Abstract: Large language models (LLMs) undergo safety alignment after training and
tuning, yet recent work shows that safety can be bypassed through jailbreak
attacks. While many jailbreaks and defenses exist, their cross-lingual
generalization remains underexplored. This paper presents the first systematic
multilingual evaluation of jailbreaks and defenses across ten
languages--spanning high-, medium-, and low-resource languages--using six LLMs
on HarmBench and AdvBench. We assess two jailbreak types:
logical-expression-based and adversarial-prompt-based. For both types, attack
success and defense robustness vary across languages: high-resource languages
are safer under standard queries but more vulnerable to adversarial ones.
Simple defenses can be effective, but are language- and model-dependent. These
findings call for language-aware and cross-lingual safety benchmarks for LLMs.

</details>


### [35] [Optimizing Native Sparse Attention with Latent Attention and Local Global Alternating Strategies](https://arxiv.org/abs/2511.00819)
*Yuxuan Hu,Jianchao Tan,Jiaqi Zhang,Wen Zan,Pingwei Sun,Yifan Lu,Yerui Sun,Yuchen Xie,Xunliang Cai,Jing Zhang*

Main category: cs.CL

TL;DR: 提出改进的Native Sparse Attention方法，通过交替使用局部和全局注意力模式，结合潜在注意力机制，在减少KV缓存的同时提升长序列建模性能。


<details>
  <summary>Details</summary>
Motivation: 改进Native Sparse Attention在长上下文建模中的效果，解决固定注意力模式无法有效传播长距离依赖的问题。

Method: 在层间交替使用局部（滑动窗口）和全局（压缩、选择性）注意力模式，并引入多头潜在注意力(MLA)和分组头潜在注意力(GLA)来增强各分支。

Result: 相比NSA减少50%的KV缓存内存，在340M到1.3B参数的模型上，在常识推理和长文本理解任务中表现优于或等同于全注意力和原生稀疏注意力。

Conclusion: 交替注意力模式和潜在注意力机制能有效提升长序列建模能力，同时显著减少内存开销。

Abstract: In this work, we conduct a systematic analysis of Native Sparse Attention
(NSA) and propose targeted improvements that enhance long-context modeling. A
key insight is that alternating between local (sliding-window) and global
(compression, selective) attention across layers, rather than using fixed
patterns, enables more effective propagation of long-range dependencies and
substantially boosts performance on long-sequence tasks. Meanwhile, we further
refine NSA's branches with Latent Attention that the sliding-window branch is
enhanced with Multi-head Latent Attention (MLA) while compression and selective
branches adopt Group-head Latent Attention (GLA). These changes reduce KV-cache
memory by 50\% versus NSA while improving the model's common-sense reasoning
and long-text understanding capabilities. Experiments on models from 340M to
1.3B parameters (trained on 15B and 100B tokens) show our method matches or
exceeds full attention and native sparse attention in both common-sense
reasoning and long-context understanding tasks.

</details>


### [36] [TriCon-Fair: Triplet Contrastive Learning for Mitigating Social Bias in Pre-trained Language Models](https://arxiv.org/abs/2511.00854)
*Chong Lyu,Lin Li,Shiqing Wu,Jingling Yuan*

Main category: cs.CL

TL;DR: 提出TriCon-Fair对比学习框架，通过解耦三元组和语言建模损失来消除社会偏见，避免偏见样本和无偏见样本之间的负面耦合效应。


<details>
  <summary>Details</summary>
Motivation: 现有去偏见方法独立处理偏见和无偏见样本，忽略了它们之间的相互关系，导致改进一个群体时无意中损害另一个群体，使残留社会偏见持续存在。

Method: TriCon-Fair使用解耦损失，结合三元组和语言建模目标，为每个锚点分配明确的偏见负样本和无偏见正样本，解耦推拉动态并避免正负耦合。

Result: 实验结果表明TriCon-Fair在减少歧视性输出方面优于现有去偏见基线方法，同时保持强大的下游性能。

Conclusion: TriCon-Fair为敏感NLP应用提供了实用且符合伦理的解决方案。

Abstract: The increasing utilization of large language models raises significant
concerns about the propagation of social biases, which may result in harmful
and unfair outcomes. However, existing debiasing methods treat the biased and
unbiased samples independently, thus ignoring their mutual relationship. This
oversight enables a hidden negative-positive coupling, where improvements for
one group inadvertently compromise the other, allowing residual social bias to
persist. In this paper, we introduce TriCon-Fair, a contrastive learning
framework that employs a decoupled loss that combines triplet and language
modeling terms to eliminate positive-negative coupling. Our TriCon-Fair assigns
each anchor an explicitly biased negative and an unbiased positive, decoupling
the push-pull dynamics and avoiding positive-negative coupling, and jointly
optimizes a language modeling (LM) objective to preserve general capability.
Experimental results demonstrate that TriCon-Fair reduces discriminatory output
beyond existing debiasing baselines while maintaining strong downstream
performance. This suggests that our proposed TriCon-Fair offers a practical and
ethical solution for sensitive NLP applications.

</details>


### [37] [Assessing LLM Reasoning Steps via Principal Knowledge Grounding](https://arxiv.org/abs/2511.00879)
*Hyeon Hwang,Yewon Cho,Chanwoong Yoon,Yein Park,Minju Song,Kyungjae Lee,Gangwoo Kim,Jaewoo Kang*

Main category: cs.CL

TL;DR: 提出了一个评估LLM推理中知识基础的新框架，包含大规模知识库、知识基础评估指标和轻量级评估器LLM，能有效识别推理中的知识缺失或误用问题。


<details>
  <summary>Details</summary>
Motivation: 解决如何验证LLM推理过程是否准确基于知识的问题，当前逐步推理方法虽然有效但缺乏对知识基础的评估机制。

Method: 构建包含三个关键组件的评估框架：1) 大规模原子知识库；2) 知识基础评估指标；3) 轻量级评估器LLM用于计算指标。

Result: 评估套件能有效识别LLM推理中的知识缺失和误用问题，揭示基本推理缺陷，并展示了在偏好优化中的应用潜力。

Conclusion: 该知识基础评估框架为LLM推理质量提供了系统化的验证方法，有助于提升推理的可靠性和可解释性。

Abstract: Step-by-step reasoning has become a standard approach for large language
models (LLMs) to tackle complex tasks. While this paradigm has proven
effective, it raises a fundamental question: How can we verify that an LLM's
reasoning is accurately grounded in knowledge? To address this question, we
introduce a novel evaluation suite that systematically assesses the knowledge
grounding of intermediate reasoning. Our framework comprises three key
components. (1) Principal Knowledge Collection, a large-scale repository of
atomic knowledge essential for reasoning. Based on the collection, we propose
(2) knowledge-grounded evaluation metrics designed to measure how well models
recall and apply prerequisite knowledge in reasoning. These metrics are
computed by our (3) evaluator LLM, a lightweight model optimized for
cost-effective and reliable metric computation. Our evaluation suite
demonstrates remarkable effectiveness in identifying missing or misapplied
knowledge elements, providing crucial insights for uncovering fundamental
reasoning deficiencies in LLMs. Beyond evaluation, we demonstrate how these
metrics can be integrated into preference optimization, showcasing further
applications of knowledge-grounded evaluation.

</details>


### [38] [ColMate: Contrastive Late Interaction and Masked Text for Multimodal Document Retrieval](https://arxiv.org/abs/2511.00903)
*Ahmed Masry,Megh Thakkar,Patrice Bechard,Sathwik Tejaswi Madhusudhan,Rabiul Awal,Shambhavi Mishra,Akshay Kalkunte Suresh,Srivatsava Daruru,Enamul Hoque,Spandana Gella,Torsten Scholak,Sai Rajeswar*

Main category: cs.CL

TL;DR: ColMate是一个多模态文档检索模型，通过OCR预训练目标、自监督掩码对比学习和延迟交互评分机制，在ViDoRe V2基准上比现有检索模型提升3.61%，并展现出更强的跨域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态文档检索方法往往简单复制文本检索技术，在文档编码、训练目标和相似度计算方面存在局限性，需要专门针对多模态文档结构和视觉特性的解决方案。

Method: 采用基于OCR的预训练目标、自监督掩码对比学习目标，以及更适合多模态文档结构和视觉特征的延迟交互评分机制。

Result: 在ViDoRe V2基准测试中获得3.61%的性能提升，在跨域基准测试中表现出更强的泛化能力。

Conclusion: ColMate成功弥合了多模态表示学习与文档检索之间的差距，为多模态文档检索提供了更有效的解决方案。

Abstract: Retrieval-augmented generation has proven practical when models require
specialized knowledge or access to the latest data. However, existing methods
for multimodal document retrieval often replicate techniques developed for
text-only retrieval, whether in how they encode documents, define training
objectives, or compute similarity scores. To address these limitations, we
present ColMate, a document retrieval model that bridges the gap between
multimodal representation learning and document retrieval. ColMate utilizes a
novel OCR-based pretraining objective, a self-supervised masked contrastive
learning objective, and a late interaction scoring mechanism more relevant to
multimodal document structures and visual characteristics. ColMate obtains
3.61% improvements over existing retrieval models on the ViDoRe V2 benchmark,
demonstrating stronger generalization to out-of-domain benchmarks.

</details>


### [39] [The Biased Oracle: Assessing LLMs' Understandability and Empathy in Medical Diagnoses](https://arxiv.org/abs/2511.00924)
*Jianzhou Yao,Shunchang Liu,Guillaume Drui,Rikard Pettersson,Alessandro Blasimme,Sara Kijewski*

Main category: cs.CL

TL;DR: 评估大型语言模型在医疗诊断沟通中的表现，发现虽然能根据患者特征调整解释，但存在内容过于复杂和情感同理心偏见的问题，导致可及性和支持不均衡。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在支持临床医生进行诊断沟通方面的能力，特别是生成患者可理解且具有同理心的解释和指导的能力。

Method: 使用两个领先的LLMs在医疗诊断场景中进行评估，通过可读性指标评估可理解性，通过LLM-as-a-Judge评分与人类评估比较评估同理心。

Result: LLMs能够根据社会人口学变量和患者状况调整解释，但生成的内容过于复杂，并表现出有偏见的情感同理心，导致可及性和支持不均衡。

Conclusion: 需要系统校准以确保公平的患者沟通，代码和数据已发布。

Abstract: Large language models (LLMs) show promise for supporting clinicians in
diagnostic communication by generating explanations and guidance for patients.
Yet their ability to produce outputs that are both understandable and
empathetic remains uncertain. We evaluate two leading LLMs on medical
diagnostic scenarios, assessing understandability using readability metrics as
a proxy and empathy through LLM-as-a-Judge ratings compared to human
evaluations. The results indicate that LLMs adapt explanations to
socio-demographic variables and patient conditions. However, they also generate
overly complex content and display biased affective empathy, leading to uneven
accessibility and support. These patterns underscore the need for systematic
calibration to ensure equitable patient communication. The code and data are
released: https://github.com/Jeffateth/Biased_Oracle

</details>


### [40] [The Riddle of Reflection: Evaluating Reasoning and Self-Awareness in Multilingual LLMs using Indian Riddles](https://arxiv.org/abs/2511.00960)
*Abhinav P M,Ojasva Saxena,Oswald C,Parameswari Krishnamurthy*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在七种印度语言中的文化推理能力，发现模型初始准确率与自我纠错能力呈负相关，最佳模型Gemini 2.5 Pro过度自信，而性能较差的模型反而更具自我意识。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在非英语语言中进行文化推理的能力，特别是在印度语言中的表现，这是一个尚未充分研究的领域。

Method: 构建多语言谜语数据集，评估五种LLM在七种印度语言中的表现，采用七种提示策略，分两阶段评估谜题解决能力和自我评估一致性。

Result: Gemini 2.5 Pro整体表现最佳，但少样本方法收益有限，准确率在不同语言间差异显著。关键发现：模型初始准确率与识别自身错误能力呈负相关，高性能模型过度自信，低性能模型更具自我意识。

Conclusion: 多语言推理存在明显差距，需要开发既能有效推理又能识别自身局限的模型。

Abstract: The extent to which large language models (LLMs) can perform culturally
grounded reasoning across non-English languages remains underexplored. This
paper examines the reasoning and self-assessment abilities of LLMs across seven
major Indian languages-Bengali, Gujarati, Hindi, Kannada, Malayalam, Tamil, and
Telugu. We introduce a multilingual riddle dataset combining traditional
riddles with context-reconstructed variants and evaluate five LLMs-Gemini 2.5
Pro, Gemini 2.5 Flash, Mistral-Saba, LLaMA 4 Scout, and LLaMA 4 Maverick-under
seven prompting strategies. In the first stage, we assess riddle-solving
performance and find that while Gemini 2.5 Pro performs best overall, few-shot
methods yield only marginal gains, and accuracy varies notably across
languages. In the second stage, we conduct a self-evaluation experiment to
measure reasoning consistency. The results reveal a key finding: a model's
initial accuracy is inversely correlated with its ability to identify its own
mistakes. Top-performing models such as Gemini 2.5 Pro are overconfident (4.34%
True Negative Rate), whereas lower-performing models like LLaMA 4 Scout are
substantially more self-aware (42.09% True Negative Rate). These results point
to clear gaps in multilingual reasoning and highlight the need for models that
not only reason effectively but also recognize their own limitations.

</details>


### [41] [Advancing Machine-Generated Text Detection from an Easy to Hard Supervision Perspective](https://arxiv.org/abs/2511.00988)
*Chenwang Wu,Yiu-ming Cheung,Bo Han,Defu Lian*

Main category: cs.CL

TL;DR: 提出了一种从易到难的增强框架来解决机器生成文本检测中的边界模糊问题，通过使用针对较长文本的简单监督器来增强更具挑战性的目标检测器。


<details>
  <summary>Details</summary>
Motivation: 现有机器生成文本检测方法假设标签是"黄金标准"，但实际上存在边界模糊问题，传统训练范式不精确。人类认知局限和检测器超智能使得不精确学习普遍存在且不可避免。

Method: 提出从易到难的增强框架，使用针对较长文本检测任务的简单监督器（尽管能力较弱）来增强更具挑战性的目标检测器。通过将检测器结构性地融入监督器，将监督器建模为检测器的性能下界。

Result: 在多种实际场景（跨LLM、跨领域、混合文本和改写攻击）的广泛实验表明，该框架具有显著的检测有效性。

Conclusion: 该框架通过从易到难的监督策略，在不精确条件下提供可靠监督，最终逼近潜在的"黄金"标签，有效解决了机器生成文本检测中的边界模糊问题。

Abstract: Existing machine-generated text (MGT) detection methods implicitly assume
labels as the "golden standard". However, we reveal boundary ambiguity in MGT
detection, implying that traditional training paradigms are inexact. Moreover,
limitations of human cognition and the superintelligence of detectors make
inexact learning widespread and inevitable. To this end, we propose an
easy-to-hard enhancement framework to provide reliable supervision under such
inexact conditions. Distinct from knowledge distillation, our framework employs
an easy supervisor targeting relatively simple longer-text detection tasks
(despite weaker capabilities), to enhance the more challenging target detector.
Firstly, longer texts targeted by supervisors theoretically alleviate the
impact of inexact labels, laying the foundation for reliable supervision.
Secondly, by structurally incorporating the detector into the supervisor, we
theoretically model the supervisor as a lower performance bound for the
detector. Thus, optimizing the supervisor indirectly optimizes the detector,
ultimately approximating the underlying "golden" labels. Extensive experiments
across diverse practical scenarios, including cross-LLM, cross-domain, mixed
text, and paraphrase attacks, demonstrate the framework's significant detection
effectiveness. The code is available at:
https://github.com/tmlr-group/Easy2Hard.

</details>


### [42] [MARS-SQL: A multi-agent reinforcement learning framework for Text-to-SQL](https://arxiv.org/abs/2511.01008)
*Haolin Yang,Jipeng Zhang,Zhitao He,Yi R. Fung*

Main category: cs.CL

TL;DR: MARS-SQL是一个多代理框架，通过任务分解和交互式强化学习解决复杂自然语言到SQL的翻译问题，在BIRD和Spider数据集上达到最先进的执行准确率。


<details>
  <summary>Details</summary>
Motivation: 复杂查询的自然语言到SQL翻译仍然困难，需要环境交互和自我修正能力。

Method: 使用三个专门代理：基础代理进行模式链接，生成代理通过多轮强化学习策略生成查询，验证代理进行最终选择。生成代理采用ReAct风格的思考-行动-观察循环进行迭代推理。

Result: 在BIRD开发集上达到77.84%的执行准确率，在Spider测试集上达到89.75%的执行准确率。

Conclusion: MARS-SQL通过结构化工作流程结合交互式强化学习和生成建模，实现了鲁棒且准确的SQL生成。

Abstract: Translating natural language to SQL remains difficult for complex queries.
Such queries often need environmental interaction and self-correction. To
address this, we introduce MARS-SQL, a novel multi-agent framework that
combines principled task decomposition and interactive reinforcement learning
(RL). Our system comprises three specialized agents: a Grounding Agent for
schema linking, a Generation Agent for query generation, and a Validation Agent
for final selection. The core of our framework is the Generation agent, which
is trained via a multi-turn RL policy. Adopting a ReAct-style Think-Act-Observe
loop, the agent iteratively generates thoughts, executes SQL actions against a
live database, and revises its strategy based on execution feedback, enabling
dynamic, stateful reasoning and self-correction. At inference time, we generate
multiple interaction trajectories to explore diverse reasoning paths. The
Validation agent, then selects the optimal trajectory by modeling verification
as a next-token prediction task and choosing the solution with the highest
generation probability. This structured workflow pipelines specialized agents.
It combines interactive RL for generation with generative modeling for
verification. The approach proves highly effective for robust and accurate SQL
generation. Experiments show that MARS-SQL achieves state-of-the-art Execution
Accuracy of 77.84% on the BIRD dev set and 89.75% on the Spider test set. Our
code is available at https://github.com/YangHaolin0526/MARS-SQL.

</details>


### [43] [IF-CRITIC: Towards a Fine-Grained LLM Critic for Instruction-Following Evaluation](https://arxiv.org/abs/2511.01014)
*Bosi Wen,Yilin Niu,Cunxiang Wang,Pei Ke,Xiaoying Ling,Ying Zhang,Aohan Zeng,Hongning Wang,Minlie Huang*

Main category: cs.CL

TL;DR: 提出了IF-CRITIC，一个能够高效可靠评估指令约束遵循情况的LLM评论器，通过清单生成和多阶段过滤机制收集高质量训练数据，在指令遵循优化中显著提升性能并降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM-as-a-Judge的指令遵循评估方法存在成本高和评估不可靠的问题，需要开发更高效可靠的评估模型。

Method: 开发清单生成器分解指令生成约束清单，通过多阶段评论过滤机制收集高质量训练数据，采用约束级偏好优化方法训练IF-CRITIC。

Result: IF-CRITIC的评估性能超越了Deepseek-R1和o4-mini等强基线LLM-as-a-Judge方法，在指令遵循优化中实现显著性能提升且计算开销更低。

Conclusion: IF-CRITIC能够提供可扩展的奖励信号，为LLM的指令遵循能力优化提供了高效可靠的解决方案。

Abstract: Instruction following is a fundamental ability of Large Language Models
(LLMs), requiring their generated outputs to follow multiple constraints
imposed in input instructions. Numerous studies have attempted to enhance this
ability through preference optimization or reinforcement learning based on
reward signals from LLM-as-a-Judge. However, existing evaluation models for
instruction following still possess many deficiencies, such as substantial
costs and unreliable assessments. To this end, we propose IF-CRITIC, an LLM
critic that can provide efficient and reliable assessments of constraint
following in the instructions. We first develop a checklist generator to
decompose instructions and generate constraint checklists. With the assistance
of the checklists, we collect high-quality critique training data through a
multi-stage critique filtering mechanism and employ a constraint-level
preference optimization method to train IF-CRITIC. Extensive experiments
demonstrate that the evaluation performance of IF-CRITIC can beat strong
LLM-as-a-Judge baselines, including Deepseek-R1 and o4-mini. With the scalable
reward signals provided by IF-CRITIC, LLMs can achieve substantial performance
gains in instruction-following optimization under lower computational overhead
compared to strong LLM critic baselines.

</details>


### [44] [Prompt-R1: Collaborative Automatic Prompting Framework via End-to-end Reinforcement Learning](https://arxiv.org/abs/2511.01016)
*Wenjin Liu,Haoran Luo,Xueyuan Lin,Haoming Liu,Tiesunlong Shen,Jiapu Wang,Rui Mao,Erik Cambria*

Main category: cs.CL

TL;DR: Prompt-R1是一个端到端的强化学习框架，使用小型LLM与大型LLM协作，通过多轮提示交互解决复杂问题，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 用户通常无法为复杂问题提供准确有效的提示，限制了大型语言模型的性能发挥。

Method: 采用小型LLM思考生成提示，大型LLM进行复杂推理的多轮交互框架，设计双约束奖励机制优化正确性、生成质量和推理准确性。

Result: 在多个公共数据集上的实验表明，Prompt-R1显著优于基线模型。

Conclusion: Prompt-R1提供了一个即插即用的协作框架，有效提升了LLM在复杂问题上的表现。

Abstract: Recently, advanced large language models (LLMs) have emerged at an
increasingly rapid pace. However, when faced with complex problems, most users
are often unable to provide accurate and effective prompts to interact with
LLMs, thus limiting the performance of LLMs. To address this challenge, we
propose Prompt-R1, an end-to-end reinforcement learning framework that uses a
small-scale LLM to collaborate with large-scale LLMs, replacing user
interaction to solve problems better. This collaboration is cast as a
multi-turn prompt interaction, where the small-scale LLM thinks and generates
prompts, and the large-scale LLM performs complex reasoning. A dual-constrained
reward is designed to optimize for correctness, generation quality, and
reasoning accuracy. Prompt-R1 provides a plug-and-play framework that supports
both inference and training with various large-scale LLMs. Experiments on
multiple public datasets show that Prompt-R1 significantly outperforms baseline
models across tasks. Our code is publicly available at
https://github.com/QwenQKing/Prompt-R1.

</details>


### [45] [OceanAI: A Conversational Platform for Accurate, Transparent, Near-Real-Time Oceanographic Insights](https://arxiv.org/abs/2511.01019)
*Bowen Chen,Jayesh Gajbhar,Gregory Dusek,Rob Redmon,Patrick Hogan,Paul Liu,DelWayne Bohnenstiehl,Dongkuan,Xu,Ruoying He*

Main category: cs.CL

TL;DR: OceanAI是一个将开源大语言模型与NOAA海洋数据流集成的对话平台，通过实时API调用生成基于权威数据的可验证回答和可视化，解决了AI在科学领域产生幻觉的问题。


<details>
  <summary>Details</summary>
Motivation: 解决通用对话AI系统在科学领域生成未经验证的"幻觉"内容的问题，确保科学严谨性，为海洋科学提供可靠的数据支持。

Method: 将开源大语言模型的自然语言流畅性与NOAA的权威海洋数据流实时集成，通过参数化API调用识别、解析和合成相关数据集。

Result: 在盲测中，只有OceanAI能提供NOAA来源的数据和原始数据引用，其他AI产品要么拒绝回答，要么提供无支持的结果。

Conclusion: OceanAI通过基于可验证观测的数据输出，提高了透明度、可重复性和可信度，为海洋领域的AI决策支持提供了可扩展框架。

Abstract: Artificial intelligence is transforming the sciences, yet general
conversational AI systems often generate unverified "hallucinations"
undermining scientific rigor. We present OceanAI, a conversational platform
that integrates the natural-language fluency of open-source large language
models (LLMs) with real-time, parameterized access to authoritative
oceanographic data streams hosted by the National Oceanic and Atmospheric
Administration (NOAA). Each query such as "What was Boston Harbor's highest
water level in 2024?" triggers real-time API calls that identify, parse, and
synthesize relevant datasets into reproducible natural-language responses and
data visualizations. In a blind comparison with three widely used AI
chat-interface products, only OceanAI produced NOAA-sourced values with
original data references; others either declined to answer or provided
unsupported results. Designed for extensibility, OceanAI connects to multiple
NOAA data products and variables, supporting applications in marine hazard
forecasting, ecosystem assessment, and water-quality monitoring. By grounding
outputs and verifiable observations, OceanAI advances transparency,
reproducibility, and trust, offering a scalable framework for AI-enabled
decision support within the oceans. A public demonstration is available at
https://oceanai.ai4ocean.xyz.

</details>


### [46] [VayuChat: An LLM-Powered Conversational Interface for Air Quality Data Analytics](https://arxiv.org/abs/2511.01046)
*Vedant Acharya,Abhay Pisharodi,Rishabh Mondal,Mohammad Rafiuddin,Nipun Batra*

Main category: cs.CL

TL;DR: VayuChat是一个对话式系统，通过自然语言查询回答空气质量、气象和政策项目相关问题，并生成可执行的Python代码和交互式可视化。


<details>
  <summary>Details</summary>
Motivation: 印度每年因空气污染导致160万人过早死亡，但决策者难以将分散的数据转化为决策。现有工具需要专业知识且提供静态仪表板，无法解决关键政策问题。

Method: 集成中央污染控制委员会监测站数据、州级人口统计数据和国家清洁空气计划资金记录，使用大型语言模型构建统一对话界面。

Result: 开发了公开部署的平台，用户可通过简单对话执行复杂的环境分析，使数据科学对政策制定者、研究人员和公民可访问。

Conclusion: VayuChat通过对话式界面使环境数据分析民主化，为空气污染治理提供更易用的决策支持工具。

Abstract: Air pollution causes about 1.6 million premature deaths each year in India,
yet decision makers struggle to turn dispersed data into decisions. Existing
tools require expertise and provide static dashboards, leaving key policy
questions unresolved. We present VayuChat, a conversational system that answers
natural language questions on air quality, meteorology, and policy programs,
and responds with both executable Python code and interactive visualizations.
VayuChat integrates data from Central Pollution Control Board (CPCB) monitoring
stations, state-level demographics, and National Clean Air Programme (NCAP)
funding records into a unified interface powered by large language models. Our
live demonstration will show how users can perform complex environmental
analytics through simple conversations, making data science accessible to
policymakers, researchers, and citizens. The platform is publicly deployed at
https://huggingface.co/spaces/SustainabilityLabIITGN/ VayuChat. For further
information check out video uploaded on
https://www.youtube.com/watch?v=d6rklL05cs4.

</details>


### [47] [Building a Silver-Standard Dataset from NICE Guidelines for Clinical LLMs](https://arxiv.org/abs/2511.01053)
*Qing Ding,Eric Hua Qing Zhang,Felix Jozsa,Julia Ive*

Main category: cs.CL

TL;DR: 该研究创建了一个基于临床指南的标准化基准数据集，用于评估LLM在医疗领域的临床推理能力，并测试了多个流行LLM的性能。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏用于评估LLM基于临床指南进行推理的标准化基准，而LLM在医疗领域的应用日益增多。

Method: 利用GPT帮助创建基于公开临床指南的数据集，包含真实患者场景和临床问题，并对多个流行LLM进行基准测试。

Result: 展示了数据集的效度，并提供了LLM临床效用和指南依从性的系统评估框架。

Conclusion: 该研究为评估LLM在医疗领域的临床推理能力提供了有效的标准化基准和评估框架。

Abstract: Large language models (LLMs) are increasingly used in healthcare, yet
standardised benchmarks for evaluating guideline-based clinical reasoning are
missing. This study introduces a validated dataset derived from publicly
available guidelines across multiple diagnoses. The dataset was created with
the help of GPT and contains realistic patient scenarios, as well as clinical
questions. We benchmark a range of recent popular LLMs to showcase the validity
of our dataset. The framework supports systematic evaluation of LLMs' clinical
utility and guideline adherence.

</details>


### [48] [HPLT~3.0: Very Large-Scale Multilingual Resources for LLM and MT. Mono- and Bi-lingual Data, Multilingual Evaluation, and Pre-Trained Models](https://arxiv.org/abs/2511.01066)
*Stephan Oepen,Nikolay Arefev,Mikko Aulamo,Marta Bañón,Maja Buljan,Laurie Burchell,Lucas Charpentier,Pinzhen Chen,Mariya Fedorova,Ona de Gibert,Barry Haddow,Jan Hajič,Jindrič Helcl,Andrey Kutuzov,Zihao Li,Risto Luukkonen,Bhavitvya Malik,Vladislav Mikhailov,Amanda Myntti,Dayyán O'Brien,Lucie Poláková,Sampo Pyysalo,Gema Ramírez Sánchez,Janine Siewert,Pavel Stepachev,Jörg Tiedemann,Teemu Vahtola,Fedor Vitiugin,Tea Vojtěchová,Jaume Zaragoza*

Main category: cs.CL

TL;DR: 该论文介绍了一个包含近200种语言、30万亿token的多语言LLM预训练数据集，这是目前最大的公开多语言数据集合，并提供了完整的数据处理流程和评估基准。


<details>
  <summary>Details</summary>
Motivation: 为多语言大语言模型预训练提供高质量、大规模、开放的多语言数据集，解决现有数据资源不足的问题。

Method: 从不同来源的网络爬虫数据出发，通过完整的开源流程进行文档选择、文本提取、语言识别、去重、标注（包括文本质量、个人信息等）和最终筛选。

Result: 创建了包含30万亿token的多语言数据集，训练了57个单语编码器-解码器模型和GPT-like参考模型，并提供了多语言评估基准。

Conclusion: 该项目为多语言NLP研究提供了宝贵的数据资源和评估工具，推动了多语言大语言模型的发展。

Abstract: We present an ongoing initiative to provide open, very large, high-quality,
and richly annotated textual datasets for almost 200 languages. At 30 trillion
tokens, this is likely the largest generally available multilingual collection
of LLM pre-training data. At 30 trillion tokens, this is likely the largest
generally available multilingual collection of LLM pre-training data. These
datasets are derived from web crawls from different sources and accompanied
with a complete, open-source pipeline for document selection from web archives,
text extraction from HTML, language identification for noisy texts, exact and
near-deduplication, annotation with, among others, register labels, text
quality estimates, and personally identifiable information; and final selection
and filtering. We report on data quality probes through contrastive and
analytical statistics, through manual inspection of samples for 24 languages,
and through end-to-end evaluation of various language model architectures
trained on this data. For multilingual LLM evaluation, we provide a
comprehensive collection of benchmarks for nine European languages, with
special emphasis on natively created tasks, mechanisms to mitigate prompt
sensitivity, and refined normalization and aggregation of scores. Additionally,
we train and evaluate a family of 57 monolingual encoder-decoder models, as
well as a handful of monolingual GPT-like reference models. Besides the
monolingual data and models, we also present a very large collection of
parallel texts automatically mined from this data, together with a novel
parallel corpus synthesized via machine translation.

</details>


### [49] [Improving Romanian LLM Pretraining Data using Diversity and Quality Filtering](https://arxiv.org/abs/2511.01090)
*Vlad Negoita,Mihai Masala,Traian Rebedea*

Main category: cs.CL

TL;DR: 本文研究了罗马尼亚语预训练语料库的特征和覆盖范围，通过与英语数据对比，使用轻量级多任务模型进行多级过滤，生成高质量预训练数据集，并验证了过滤方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型训练中高质量数据至关重要，特别是对于罗马尼亚语等资源稀缺语言，需要研究其语料特征并开发有效的数据过滤方法。

Method: 训练轻量级多任务模型对LLM标注的罗马尼亚文本进行分析，进行多级过滤（教育价值、主题、格式等）来生成高质量预训练数据集。

Result: 实验揭示了罗马尼亚语和英语数据在主题分布上的显著差异，同时证明了数据过滤能有效提升LLM在多个基准测试中的预训练性能。

Conclusion: 针对资源稀缺语言的数据特征分析和多级过滤方法能够显著提升预训练语料质量，为类似语言的LLM训练提供了有效策略。

Abstract: Large Language Models (LLMs) have recently exploded in popularity, often
matching or outperforming human abilities on many tasks. One of the key factors
in training LLMs is the availability and curation of high-quality data. Data
quality is especially crucial for under-represented languages, where
high-quality corpora are scarce. In this work we study the characteristics and
coverage of Romanian pretraining corpora and we examine how they differ from
English data. By training a lightweight multitask model on carefully
LLM-annotated Romanian texts, we are able to analyze and perform multi-level
filtering (e.g., educational value, topic, format) to generate high-quality
pretraining datasets. Our experiments show noteworthy trends in the topics
present in Romanian and English data, while also proving the effectiveness of
filtering data through improved LLM pretraining performance across multiple
benchmarks.

</details>


### [50] [TSVer: A Benchmark for Fact Verification Against Time-Series Evidence](https://arxiv.org/abs/2511.01101)
*Marek Strong,Andreas Vlachos*

Main category: cs.CL

TL;DR: TSVer是一个新的时间序列事实核查基准数据集，包含287个真实世界声明和400个时间序列，专注于时间和数值推理。


<details>
  <summary>Details</summary>
Motivation: 现有事实核查系统在评估时受到数据集限制，缺乏结构化证据、判决理由不足或依赖合成声明。

Method: 采用LLM辅助的多步骤标注流程，从38个事实核查组织收集真实声明，并构建包含400个时间序列的数据库。

Result: 实现了kappa=0.745的标注者间一致性，基线模型在判决准确率上仅达到63.37%，证据到理由得分为48.63%。

Conclusion: 时间序列事实核查对现有最先进模型仍具挑战性，TSVer为评估和改进此类系统提供了高质量基准。

Abstract: Reasoning over temporal and numerical data, such as time series, is a crucial
aspect of fact-checking. While many systems have recently been developed to
handle this form of evidence, their evaluation remains limited by existing
datasets, which often lack structured evidence, provide insufficient
justifications for verdicts, or rely on synthetic claims. In this paper, we
introduce TSVer, a new benchmark dataset for fact verification focusing on
temporal and numerical reasoning with time-series evidence. TSVer contains 287
real-world claims sourced from 38 fact-checking organizations and a curated
database of 400 time series covering diverse domains. Each claim is annotated
with time frames across all pertinent time series, along with a verdict and
justifications reflecting how the evidence is used to reach the verdict. Using
an LLM-assisted multi-step annotation process, we improve the quality of our
annotations and achieve an inter-annotator agreement of kappa=0.745 on
verdicts. We also develop a baseline for verifying claims against time-series
evidence and show that even the state-of-the-art reasoning models like
Gemini-2.5-Pro are challenged by time series, achieving a 63.37 accuracy score
on verdicts and an Ev2R score of 48.63 on verdict justifications.

</details>


### [51] [MicroRemed: Benchmarking LLMs in Microservices Remediation](https://arxiv.org/abs/2511.01166)
*Lingzhe Zhang,Yunpeng Zhai,Tong Jia,Chiming Duan,Minghua He,Leyi Pan,Zhaoyang Liu,Bolin Ding,Ying Li*

Main category: cs.CL

TL;DR: 提出了MicroRemed基准和ThinkRemed多智能体框架，用于评估LLM在端到端微服务修复中的能力，无需人工提示即可生成可执行的Ansible剧本。


<details>
  <summary>Details</summary>
Motivation: 现有方法仍依赖SRE人工编写提示，LLM仅将文本指令转换为可执行代码，需要推进LLM在微服务修复中的自主决策能力研究。

Method: 引入MicroRemed基准评估LLM端到端修复能力，提出ThinkRemed多智能体框架模拟SRE的反思和感知推理过程。

Result: 实验表明MicroRemed对当前LLM构成显著挑战，而ThinkRemed通过迭代推理和系统反思提升了端到端修复性能。

Conclusion: 该研究为LLM在微服务修复领域的应用提供了首个基准测试和有效的多智能体解决方案。

Abstract: Large Language Models (LLMs) integrated with agent-based reasoning frameworks
have recently shown strong potential for autonomous decision-making and
system-level operations. One promising yet underexplored direction is
microservice remediation, where the goal is to automatically recover faulty
microservice systems. Existing approaches, however, still rely on human-crafted
prompts from Site Reliability Engineers (SREs), with LLMs merely converting
textual instructions into executable code. To advance research in this area, we
introduce MicroRemed, the first benchmark for evaluating LLMs in end-to-end
microservice remediation, where models must directly generate executable
Ansible playbooks from diagnosis reports to restore system functionality. We
further propose ThinkRemed, a multi-agent framework that emulates the
reflective and perceptive reasoning of SREs. Experimental results show that
MicroRemed presents substantial challenges to current LLMs, while ThinkRemed
improves end-to-end remediation performance through iterative reasoning and
system reflection. The benchmark is available at
https://github.com/LLM4AIOps/MicroRemed.

</details>


### [52] [Learning When to Quit in Sales Conversations](https://arxiv.org/abs/2511.01181)
*Emaad Manzoor,Eva Ascarza,Oded Netzer*

Main category: cs.CL

TL;DR: 开发了一个基于语言模型的停止代理，通过模仿最优停止策略来优化销售对话中的放弃决策，在电信销售场景中减少失败通话时间54%，同时保持销售业绩，可提升预期销售额37%。


<details>
  <summary>Details</summary>
Motivation: 销售人员在对话中面临是否继续坚持或放弃的动态筛选决策，但缺乏对这些决策效率的了解和改进方法，特别是在高容量外呼销售中时间稀缺且失败率高的场景。

Method: 将动态筛选决策形式化为最优停止问题，开发基于生成语言模型的序列决策代理（停止代理），通过模仿回顾性推断的最优停止策略来学习何时放弃对话。

Result: 应用于欧洲大型电信公司的通话数据，停止代理将失败通话时间减少54%，同时几乎保持所有销售；重新分配节省的时间可使预期销售额增加高达37%。

Conclusion: 销售人员倾向于过度重视消费者不感兴趣的少数显著表达，错误预测通话失败风险，表明其实时对话决策存在认知局限，人工智能算法有潜力纠正这些认知局限并提高销售团队效率。

Abstract: Salespeople frequently face the dynamic screening decision of whether to
persist in a conversation or abandon it to pursue the next lead. Yet, little is
known about how these decisions are made, whether they are efficient, or how to
improve them. We study these decisions in the context of high-volume outbound
sales where leads are ample, but time is scarce and failure is common. We
formalize the dynamic screening decision as an optimal stopping problem and
develop a generative language model-based sequential decision agent - a
stopping agent - that learns whether and when to quit conversations by
imitating a retrospectively-inferred optimal stopping policy. Our approach
handles high-dimensional textual states, scales to large language models, and
works with both open-source and proprietary language models. When applied to
calls from a large European telecommunications firm, our stopping agent reduces
the time spent on failed calls by 54% while preserving nearly all sales;
reallocating the time saved increases expected sales by up to 37%. Upon
examining the linguistic cues that drive salespeople's quitting decisions, we
find that they tend to overweight a few salient expressions of consumer
disinterest and mispredict call failure risk, suggesting cognitive bounds on
their ability to make real-time conversational decisions. Our findings
highlight the potential of artificial intelligence algorithms to correct
cognitively-bounded human decisions and improve salesforce efficiency.

</details>


### [53] [ZoFia: Zero-Shot Fake News Detection with Entity-Guided Retrieval and Multi-LLM Interaction](https://arxiv.org/abs/2511.01188)
*Lvhua Wu,Xuefeng Jiang,Sheng Sun,Tian Wen,Yuwei Wang,Min Liu*

Main category: cs.CL

TL;DR: ZoFia是一个两阶段零样本假新闻检测框架，通过分层显著性量化实体重要性，使用SC-MMR算法选择关键词检索最新外部证据，然后通过多LLM交互系统进行多视角协作分析和对抗性辩论，最终产生可解释的鲁棒判断。


<details>
  <summary>Details</summary>
Motivation: 假新闻快速传播威胁社会稳定和公众信任，但现有方法面临时间受限的知识覆盖、幻觉内容生成以及缺乏对新兴新闻主题的泛化能力等问题。

Method: 1. 分层显著性量化实体重要性，SC-MMR算法选择信息丰富且多样化的关键词用于检索最新外部证据；2. 多LLM交互系统，各智能体承担不同角色，对新闻文本及相关信息进行多视角协作分析和对抗性辩论。

Result: 在两个公共数据集上的综合实验表明，ZoFia明显优于现有的零样本基线方法和大多数少样本方法。

Conclusion: ZoFia框架通过结合外部证据检索和多LLM协作分析，有效解决了假新闻检测中的知识时效性和泛化问题，提供了可解释且鲁棒的检测方案。

Abstract: The rapid spread of fake news threatens social stability and public trust,
rendering its detection an imperative research priority. Although large
language models (LLMs) excel at numerous natural language processing tasks with
their remarkable contextual understanding and extensive prior knowledge, the
time-bounded knowledge coverage and tendency for generating hallucination
content reduce their reliability when handling fast-evolving news streams.
Furthermore, models trained on existing static datasets also often lack the
generalization needed for emerging news topics. To address these challenges, we
propose ZoFia, a novel two-stage zero-shot fake news detection framework.
First, we introduce Hierarchical Salience to quantify the importance of
entities in the news content, and propose the SC-MMR algorithm to effectively
select an informative and diverse set of keywords that serve as queries for
retrieving up-to-date external evidence. Subsequently, a multi LLM interactive
system, in which each agent assumes a distinct role, performs multi-view
collaborative analysis and adversarial debate over the news text and its
related information, and finally produces an interpretable and robust judgment.
Comprehensive experiments on two public datasets demonstrate that ZoFia
obviously outperforms existing zero-shot baselines and most of few-shot
methods. Our codes will be open-sourced to facilitate related communities.

</details>


### [54] [Self-Harmony: Learning to Harmonize Self-Supervision and Self-Play in Test-Time Reinforcement Learning](https://arxiv.org/abs/2511.01191)
*Ru Wang,Wei Huang,Qi Cao,Yusuke Iwasawa,Yutaka Matsuo,Jiaxian Guo*

Main category: cs.CL

TL;DR: Self-Harmony是一种无需标签的测试时强化学习框架，通过单一模型在求解器和重构器两个角色间切换，利用原始问题和重构问题的答案稳定性来构建可靠学习信号，避免多数投票方法偏向虚假答案的问题。


<details>
  <summary>Details</summary>
Motivation: 标准测试时强化学习方法（如多数投票）容易偏向虚假但流行的答案，需要构建更可靠的学习信号来提升模型推理性能。

Method: 使用单一模型同时扮演求解器和重构器角色，通过原始问题和重构问题的答案频率，采用调和平均数聚合答案，选择在重构下保持稳定的解决方案。

Result: 在多样化推理基准测试中，Self-Harmony在无标签测试时设置下达到最先进结果，在30个设置中的28个排名第一，且在所有实验中零训练失败。

Conclusion: Self-Harmony框架通过利用问题重构的答案稳定性，实现了无需人工监督或辅助模型的可靠测试时适应，展现出卓越的稳定性和鲁棒性。

Abstract: Test-time reinforcement learning (TTRL) offers a label-free paradigm for
adapting models using only synthetic signals at inference, but its success
hinges on constructing reliable learning signals. Standard approaches such as
majority voting often collapse to spurious yet popular answers. We introduce
Self-Harmony, a framework built on a simple intuition: the correct answer
should remain stable across both an original question and its paraphrase.
Self-Harmony operationalizes this by employing a single model in two
complementary roles: a Solver to produce answers and a Reframer to rephrase the
input. Based on this, we further propose a pseudo-label method: instead of
majority voting, it aggregates answer frequencies across these original and
reframed views using the harmonic mean. This is a process that naturally
selects for solutions stable under reframing, thereby avoiding the common trap
of favoring view-dependent, spurious answers. Crucially, this requires no human
supervision or auxiliary models. Across diverse reasoning benchmarks,
Self-Harmony achieves state-of-the-art results at the label-free test-time
setting, ranking first in 28 of 30 settings across multiple methods. Beyond
accuracy, it demonstrates unprecedented robustness, with zero training failures
in all experiments, underscoring its stability and reliability.

</details>


### [55] [DEER: Disentangled Mixture of Experts with Instance-Adaptive Routing for Generalizable Machine-Generated Text Detection](https://arxiv.org/abs/2511.01192)
*Guoxin Ma,Xiaoming Liu,Zhanhan Zhang,Chengzhengxu Li,Shengchao Liu,Yu Lan*

Main category: cs.CL

TL;DR: 提出DEER框架，通过解耦的专家混合架构捕获领域特定和领域通用的机器生成文本模式，使用强化学习路由机制动态选择专家，显著提升跨域检测性能。


<details>
  <summary>Details</summary>
Motivation: 当前机器生成文本检测方法在领域转移时性能显著下降，需要解决跨域检测的挑战。

Method: 两阶段解耦专家混合架构：领域特定专家学习细粒度局部特征，共享专家提取跨域可迁移特征；强化学习路由机制动态选择专家。

Result: 在5个域内和5个域外数据集上，DEER优于现有方法，域内F1提升1.39%，域外F1提升5.32%，准确率分别提升1.35%和3.61%。

Conclusion: 解耦专家专业化和自适应路由对模型性能有重要贡献，DEER框架能有效应对机器生成文本检测中的领域转移问题。

Abstract: Detecting machine-generated text (MGT) has emerged as a critical challenge,
driven by the rapid advancement of large language models (LLMs) capable of
producing highly realistic, human-like content. However, the performance of
current approaches often degrades significantly under domain shift. To address
this challenge, we propose a novel framework designed to capture both
domain-specific and domain-general MGT patterns through a two-stage
Disentangled mixturE-of-ExpeRts (DEER) architecture. First, we introduce a
disentangled mixture-of-experts module, in which domain-specific experts learn
fine-grained, domain-local distinctions between human and machine-generated
text, while shared experts extract transferable, cross-domain features. Second,
to mitigate the practical limitation of unavailable domain labels during
inference, we design a reinforcement learning-based routing mechanism that
dynamically selects the appropriate experts for each input instance,
effectively bridging the train-inference gap caused by domain uncertainty.
Extensive experiments on five in-domain and five out-of-domain benchmark
datasets demonstrate that DEER consistently outperforms state-of-the-art
methods, achieving average F1-score improvements of 1.39% and 5.32% on
in-domain and out-of-domain datasets respectively, along with accuracy gains of
1.35% and 3.61% respectively. Ablation studies confirm the critical
contributions of both disentangled expert specialization and adaptive routing
to model performance.

</details>


### [56] [AraFinNews: Arabic Financial Summarisation with Domain-Adapted LLMs](https://arxiv.org/abs/2511.01265)
*Mo El-Haj,Paul Rayson*

Main category: cs.CL

TL;DR: 本文研究了领域特异性对阿拉伯语金融文本摘要的影响，通过构建最大的阿拉伯语金融新闻数据集AraFinNews，评估了领域适应预训练对事实准确性、数值可靠性和风格对齐的改进效果。


<details>
  <summary>Details</summary>
Motivation: 研究领域特异性在阿拉伯语金融文本摘要中的作用，填补阿拉伯语金融领域公开数据集的空白，为评估领域特定语言理解提供基准。

Method: 构建AraFinNews数据集（21.25万篇文章-标题对），评估包括mT5、AraT5和领域适应的FinAraT5等基于Transformer的模型，分析金融领域预训练对摘要质量的影响。

Result: 实验结果显示，领域适应模型生成更忠实和连贯的摘要，特别是在处理定量信息和实体中心信息方面表现更好。

Conclusion: 领域特定适应对于提高阿拉伯语金融摘要的事实一致性和叙述流畅性至关重要，AraFinNews数据集为相关研究提供了重要资源。

Abstract: This paper investigates the impact of domain specificity on abstractive
summarisation of Arabic financial texts using large language models (LLMs). We
introduce AraFinNews, the largest publicly available Arabic financial news
dataset to date, comprising 212,500 article--headline pairs spanning nearly a
decade of reporting from October 2015 to July 2025. Designed as the Arabic
equivalent of major English summarisation corpora such as CNN/DailyMail,
AraFinNews provides a robust benchmark for evaluating domain-specific language
understanding and generation in financial contexts. Using this resource, we
evaluate transformer-based models -- including mT5, AraT5, and the
domain-adapted FinAraT5 -- to examine how financial-domain pretraining
influences factual accuracy, numerical reliability, and stylistic alignment
with professional reporting. Experimental results show that domain-adapted
models generate more faithful and coherent summaries, particularly in handling
quantitative and entity-centric information. The findings highlight the
importance of domain-specific adaptation for improving factual consistency and
narrative fluency in Arabic financial summarisation. The dataset is freely
available for non-commercial research at
https://github.com/ArabicNLP-UK/AraFinNews.

</details>


### [57] [When, What, and How: Rethinking Retrieval-Enhanced Speculative Decoding](https://arxiv.org/abs/2511.01282)
*Min Fang,Zhihui Fu,Qibin Zhao,Jun Wang*

Main category: cs.CL

TL;DR: 提出ReSpec框架，通过自适应决策机制改进推测解码，使用熵引导触发、反馈驱动候选选择和松弛验证策略，在保持输出质量的同时显著提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法存在效率问题：基于模型的方法成本高，基于检索的方法依赖启发式切换策略导致不必要的检索开销。

Method: 1) 熵引导自适应触发机制，仅在不确定性低时启动检索；2) 反馈驱动候选选择，利用历史反馈组织高质量候选；3) 源感知松弛验证策略，对检索草稿采用宽松验证。

Result: 在Spec-Bench上的实验显示，ReSpec比EAGLE-2和SAM-Decoding分别提升33%和25%的加速效果，同时保持输出质量。

Conclusion: ReSpec通过自适应决策机制有效平衡了推测解码的准确性和效率，实现了当前最优的加速性能。

Abstract: Speculative decoding (SD) has emerged as an effective technique to accelerate
large language model (LLM) inference without compromising output quality.
However, the achievable speedup largely depends on the effectiveness of the
drafting model. While model-based methods like EAGLE-2 are accurate but costly,
retrieval-enhanced methods like SAM-Decoding rely on heuristic switching
strategies that often trigger unnecessary retrievals. To address this, we
propose ReSpec (\textbf{Re}trieval-enhanced \textbf{Spe}culative Decoding), a
novel framework that transforms heuristic drafter switching into adaptive
decision-making. ReSpec features three core innovations: 1) An
\textbf{entropy-guided adaptive trigger} quantifies contextual predictability
to initiate retrieval only when uncertainty is low, avoiding costly low-quality
speculations. 2) A \textbf{feedback-driven candidate selection} leverages
historical feedback to organize multiple high-quality candidates for parallel
verification, maximizing retrieval utility. 3) A source-aware \textbf{relaxed
verification strategy} applies strict checks to model-generated drafts while
using a relaxed verification for retrieved drafts, achieving a better balance
between accuracy and efficiency. Extensive experiments on Spec-Bench
demonstrate that ReSpec achieves state-of-the-art acceleration,outperforming
EAGLE-2 and SAM-Decoding by over $33\%$ and $25\%$, respectively, while
maintaining output quality.

</details>


### [58] ["Give a Positive Review Only": An Early Investigation Into In-Paper Prompt Injection Attacks and Defenses for AI Reviewers](https://arxiv.org/abs/2511.01287)
*Qin Zhou,Zhexin Zhang,Zhi Li,Limin Sun*

Main category: cs.CL

TL;DR: 本文系统研究了科学论文中隐藏提示注入攻击对AI审稿人的威胁，提出了静态和迭代两种攻击方法，并探索了检测防御措施。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型在科学论文评审中的广泛应用，出现了通过隐藏提示注入来操纵AI审稿人给出过高评价的新兴威胁，需要系统研究。

Method: 提出了两类攻击：静态攻击使用固定注入提示，迭代攻击通过模拟审稿人模型优化注入提示以最大化效果。同时探索了基于检测的防御方法。

Result: 两种攻击都取得了显著效果，经常能诱导前沿AI审稿人给出满分评价。攻击在不同设置下都具有鲁棒性，而检测防御能显著降低攻击成功率但可被自适应攻击者部分规避。

Conclusion: 研究结果强调了在AI辅助同行评审中需要更多关注和严格防护措施来应对提示注入威胁。

Abstract: With the rapid advancement of AI models, their deployment across diverse
tasks has become increasingly widespread. A notable emerging application is
leveraging AI models to assist in reviewing scientific papers. However, recent
reports have revealed that some papers contain hidden, injected prompts
designed to manipulate AI reviewers into providing overly favorable
evaluations. In this work, we present an early systematic investigation into
this emerging threat. We propose two classes of attacks: (1) static attack,
which employs a fixed injection prompt, and (2) iterative attack, which
optimizes the injection prompt against a simulated reviewer model to maximize
its effectiveness. Both attacks achieve striking performance, frequently
inducing full evaluation scores when targeting frontier AI reviewers.
Furthermore, we show that these attacks are robust across various settings. To
counter this threat, we explore a simple detection-based defense. While it
substantially reduces the attack success rate, we demonstrate that an adaptive
attacker can partially circumvent this defense. Our findings underscore the
need for greater attention and rigorous safeguards against prompt-injection
threats in AI-assisted peer review.

</details>


### [59] [FirstAidQA: A Synthetic Dataset for First Aid and Emergency Response in Low-Connectivity Settings](https://arxiv.org/abs/2511.01289)
*Saiyma Sittul Muna,Rezwan Islam Salvi,Mushfiqur Rahman Mushfique,Ajwad Abrar*

Main category: cs.CL

TL;DR: 该论文提出了FirstAidQA数据集，包含5,500个高质量的一线急救和应急响应问答对，旨在支持在低连接环境下部署轻量级语言模型。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型计算密集，不适合一线响应人员使用的低端设备，且缺乏针对急救领域的高质量数据集，阻碍了轻量级、领域特定解决方案的开发。

Method: 使用ChatGPT-4o-mini通过提示式上下文学习生成数据集，基于《Vital First Aid Book (2019)》文本，经过文本清理、上下文分块、过滤等预处理步骤，并进行人工验证以确保准确性、安全性和实用性。

Result: 创建了包含5,500个高质量问答对的FirstAidQA数据集，涵盖广泛的急救和应急响应场景，已在Hugging Face平台公开发布。

Conclusion: FirstAidQA数据集支持LLM和SLM的指令调优和微调，能够实现更快、更可靠且支持离线运行的急救系统，推动安全关键和资源受限AI应用的研究。

Abstract: In emergency situations, every second counts. The deployment of Large
Language Models (LLMs) in time-sensitive, low or zero-connectivity environments
remains limited. Current models are computationally intensive and unsuitable
for low-tier devices often used by first responders or civilians. A major
barrier to developing lightweight, domain-specific solutions is the lack of
high-quality datasets tailored to first aid and emergency response. To address
this gap, we introduce FirstAidQA, a synthetic dataset containing 5,500
high-quality question answer pairs that encompass a wide range of first aid and
emergency response scenarios. The dataset was generated using a Large Language
Model, ChatGPT-4o-mini, with prompt-based in-context learning, using texts from
the Vital First Aid Book (2019). We applied preprocessing steps such as text
cleaning, contextual chunking, and filtering, followed by human validation to
ensure accuracy, safety, and practical relevance of the QA pairs. FirstAidQA is
designed to support instruction-tuning and fine-tuning of LLMs and Small
Language Models (SLMs), enabling faster, more reliable, and offline-capable
systems for emergency settings. We publicly release the dataset to advance
research on safety-critical and resource-constrained AI applications in first
aid and emergency response. The dataset is available on Hugging Face at
https://huggingface.co/datasets/i-am-mushfiq/FirstAidQA.

</details>


### [60] [DeepSpecs: Expert-Level Questions Answering in 5G](https://arxiv.org/abs/2511.01305)
*Aman Ganapathy Manvattira,Yifei Xu,Ziyue Dang,Songwu Lu*

Main category: cs.CL

TL;DR: DeepSpecs是一个增强的RAG系统，通过结构化推理和时间推理来解决5G标准文档中的交叉引用和规范演进问题，显著提升专家级问答质量。


<details>
  <summary>Details</summary>
Motivation: 现有RAG框架无法可靠解决5G标准文档中的交叉引用或推理规范演进，而5G技术需要处理数千页不断演化的标准文档。

Method: 构建三个元数据丰富的数据库：SpecDB（条款对齐规范文本）、ChangeDB（行级版本差异）、TDocDB（标准化会议文档），通过元数据查找递归检索引用条款，挖掘变更并链接到记录设计原理的变更请求。

Result: 在多个LLM后端上，DeepSpecs优于基础模型和最先进的电信RAG系统；消融实验确认显式交叉引用解析和演进感知检索显著提高答案质量。

Conclusion: 建模5G标准的结构和时间特性对于提升专家级问答质量具有重要价值。

Abstract: 5G technology enables mobile Internet access for billions of users. Answering
expert-level questions about 5G specifications requires navigating thousands of
pages of cross-referenced standards that evolve across releases. Existing
retrieval-augmented generation (RAG) frameworks, including telecom-specific
approaches, rely on semantic similarity and cannot reliably resolve
cross-references or reason about specification evolution. We present DeepSpecs,
a RAG system enhanced by structural and temporal reasoning via three
metadata-rich databases: SpecDB (clause-aligned specification text), ChangeDB
(line-level version diffs), and TDocDB (standardization meeting documents).
DeepSpecs explicitly resolves cross-references by recursively retrieving
referenced clauses through metadata lookup, and traces specification evolution
by mining changes and linking them to Change Requests that document design
rationale. We curate two 5G QA datasets: 573 expert-annotated real-world
questions from practitioner forums and educational resources, and 350
evolution-focused questions derived from approved Change Requests. Across
multiple LLM backends, DeepSpecs outperforms base models and state-of-the-art
telecom RAG systems; ablations confirm that explicit cross-reference resolution
and evolution-aware retrieval substantially improve answer quality,
underscoring the value of modeling the structural and temporal properties of 5G
standards.

</details>


### [61] [DEEPAMBIGQA: Ambiguous Multi-hop Questions for Benchmarking LLM Answer Completeness](https://arxiv.org/abs/2511.01323)
*Jiabao Ji,Min Li,Priyanshu Kumar,Shiyu Chang,Saloni Potdar*

Main category: cs.CL

TL;DR: DeepAmbigQA是一个新的问答数据集，专门评估模型处理名称歧义和多步推理的能力，现有最先进模型在该数据集上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有问答基准很少同时评估名称歧义解决和多步推理这两个挑战，而实际复杂问题往往需要同时处理这两种困难。

Method: 开发了DeepAmbigQAGen自动数据生成管道，基于文本语料库和链接知识图谱构建问答任务，系统性地嵌入名称歧义和多步推理。

Result: 构建了包含3600个问题的DeepAmbigQA数据集，其中一半需要显式解决名称歧义。实验显示GPT-5在歧义问题上精确匹配率仅为0.13，非歧义问题为0.21。

Conclusion: 当前问答系统在信息收集和答案完整性方面仍有不足，需要开发更鲁棒的问答系统。

Abstract: Large language models (LLMs) with integrated search tools show strong promise
in open-domain question answering (QA), yet they often struggle to produce
complete answer set to complex questions such as Which actor from the film Heat
won at least one Academy Award?, which requires (1) distinguishing between
multiple films sharing the same title and (2) reasoning across a large set of
actors to gather and integrate evidence. Existing QA benchmarks rarely evaluate
both challenges jointly. To address this, we introduce DeepAmbigQAGen, an
automatic data generation pipeline that constructs QA tasks grounded in text
corpora and linked knowledge graph, generating natural and verifiable questions
that systematically embed name ambiguity and multi-step reasoning. Based on
this, we build DeepAmbigQA, a dataset of 3,600 questions requiring multi-hop
reasoning and half of them explicit name ambiguity resolving. Experiments
reveal that, even state-of-the-art GPT-5 show incomplete answers, achieving
only 0.13 exact match on ambiguous questions and 0.21 on non-ambiguous
questions. These findings highlight the need for more robust QA systems aimed
at information gathering and answer completeness.

</details>


### [62] [Thinking with DistilQwen: A Tale of Four Distilled Reasoning and Reward Model Series](https://arxiv.org/abs/2511.01354)
*Wenrui Cai,Chengyu Wang,Junbing Yan,Jun Huang,Xiangzhong Fang*

Main category: cs.CL

TL;DR: 该论文扩展了DistilQwen模型家族，推出了四个专门为工业需求设计的模型系列：慢思考模型、自适应思考模型和蒸馏奖励模型，在保持高推理效率的同时提供强大的推理性能。


<details>
  <summary>Details</summary>
Motivation: 为满足现实应用对小型高效推理模型的需求，开发平衡推理性能和推理速度的知识蒸馏技术，以适应工业应用场景。

Method: 基于Qwen模型初始化，通过知识蒸馏技术开发四个模型系列：慢思考模型（高精度推理）、自适应思考模型（动态调整推理策略）、蒸馏奖励模型（支持强化学习）。

Result: 在多个基准测试中展现出高推理效率和强推理性能，蒸馏奖励模型具有实际应用价值，并支持在阿里云PAI平台上的可扩展训练和推理功能。

Conclusion: 该研究成功开发了满足工业需求的推理模型系列，在保持高效率的同时提供强大的推理能力，为行业从业者提供了实用的AI解决方案。

Abstract: Recently, the demand for small and efficient reasoning models to support
real-world applications has driven the development of knowledge distillation
techniques that balance reasoning performance and inference speed. In this
paper, we further extend the DistilQwen model family, initialized from the Qwen
models, by introducing four model series specifically designed to meet
industrial requirements. The distilled model collection comprises: (1)
slow-thinking models, optimized for reasoning tasks that require high accuracy;
(2) two series of adaptive-thinking models, which dynamically adjust reasoning
strategies based on input tasks to maximize efficiency across diverse
scenarios; and (3) distilled reward models, which enable further reinforcement
learning of reasoning models using distilled knowledge. Comprehensive
evaluations across multiple benchmarks demonstrate both high inference
efficiency and strong reasoning performance for these models, as well as the
practical utility of distilled reward models. We further show that these models
support industry practitioners by providing scalable training and inference
functionalities on the Alibaba Cloud PAI (Platform for Artificial Intelligence)
platform.

</details>


### [63] [PrefixNLI: Detecting Factual Inconsistencies as Soon as They Arise](https://arxiv.org/abs/2511.01359)
*Sapir Harary,Eran Hirsch,Aviv Slobodkin,David Wan,Mohit Bansal,Ido Dagan*

Main category: cs.CL

TL;DR: 该论文提出了一种改进LLM输出事实性的方法，通过将自然语言推理(NLI)模型泛化到任意文本前缀，训练了专门的MiniTruePrefixes模型来检测文本前缀的事实不一致性，并在受控解码框架中显著提高了摘要生成的事实一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的NLI模型虽然能检测完整句子的事实不一致性，但在自回归生成架构中，决策是在解码过程中对每个不断演变的文本前缀进行的。因此需要专门的方法来检测文本前缀级别的事实不一致性，以改进生成内容的事实性。

Method: 将蕴含检测任务泛化到任意文本前缀，提供相应的评估和训练数据集，训练专门的MiniTruePrefixes模型来检测文本前缀的事实不一致性，并将其集成到受控解码框架中。

Result: MiniTruePrefixes在前缀级别蕴含检测上比基线NLI模型高出5-14个F1分数。在受控解码框架中集成该模型后，LLaMA-3.2-3B-Instruct在摘要生成中达到了与同系列8B模型相当的事实性和运行时间，同时只使用一半内存。

Conclusion: 通过专门针对文本前缀训练的蕴含检测模型，可以有效提高LLM生成内容的事实一致性，在保持性能的同时显著减少计算资源需求。

Abstract: Natural Language Inference (NLI) models have been used in various ways to
improve the factuality of LLM outputs. This is typically done by applying an
NLI model to judge whether the model output is entailed from the supposed
evidence, triggering some corrective actions, such as beam reranking at
inference time or RL rewards during training. While NLI models are trained to
detect factual inconsistencies over complete sentences, decisions in the common
autoregressive generation architecture are made for each evolving text prefix,
during decoding. Addressing this setting, we generalize the entailment
detection task to apply over arbitrary text prefixes, and suggest its utility
for improving generation faithfulness. Providing suitable evaluation and
training datasets for this task, we train MiniTruePrefixes, a novel specialized
model that better detects factual inconsistencies over text prefixes,
outperforming comparable baseline NLI models by 5-14 F1 points in prefix-level
entailment. We further demonstrate that integrating MiniTruePrefixes into a
controlled decoding framework substantially improves factual consistency in
abstractive summarization. When guided by MiniTruePrefixes,
LLaMA-3.2-3B-Instruct matches the faithfulness and runtime of the 8B model from
the same model family, while using only half the memory.

</details>


### [64] [Safer in Translation? Presupposition Robustness in Indic Languages](https://arxiv.org/abs/2511.01360)
*Aadi Palnitkar,Arjun Suresh,Rishi Rajesh,Puneet Puli*

Main category: cs.CL

TL;DR: 构建了Cancer-Myth-Indic基准，将500个癌症相关错误预设问题翻译成5种印度语言，用于评估多语言LLM在医疗咨询中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有医疗基准几乎都是英文的，导致多语言LLM评估存在显著空白，特别是在医疗咨询领域。

Method: 从Cancer-Myth中均匀采样500个项目，由母语译者按照风格指南翻译成5种印度语言，保留隐含预设。

Result: 创建了包含2500个翻译项目的多语言基准，用于在预设压力下评估流行LLM。

Conclusion: 该工作填补了多语言医疗LLM评估的空白，为评估LLM在印度语言医疗咨询中的准确性和有效性提供了工具。

Abstract: Increasingly, more and more people are turning to large language models
(LLMs) for healthcare advice and consultation, making it important to gauge the
efficacy and accuracy of the responses of LLMs to such queries. While there are
pre-existing medical benchmarks literature which seeks to accomplish this very
task, these benchmarks are almost universally in English, which has led to a
notable gap in existing literature pertaining to multilingual LLM evaluation.
Within this work, we seek to aid in addressing this gap with Cancer-Myth-Indic,
an Indic language benchmark built by translating a 500-item subset of
Cancer-Myth, sampled evenly across its original categories, into five
under-served but widely used languages from the subcontinent (500 per language;
2,500 translated items total). Native-speaker translators followed a style
guide for preserving implicit presuppositions in translation; items feature
false presuppositions relating to cancer. We evaluate several popular LLMs
under this presupposition stress.

</details>


### [65] [The Ouroboros of Benchmarking: Reasoning Evaluation in an Era of Saturation](https://arxiv.org/abs/2511.01365)
*İbrahim Ethem Deveci,Duygu Ataman*

Main category: cs.CL

TL;DR: 本文质疑当前大语言模型基准测试的有效性，分析三大模型家族在推理任务上的表现趋势，并讨论基准测试面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，基准测试结果趋于饱和，需要探讨超越基准是否真正代表推理能力，还是仅仅在追踪与声称能力脱节的数字。

Method: 调查OpenAI、Anthropic和Google三大模型家族在不同基准测试中的推理能力演变，分析多年来的性能趋势和不同推理任务的表现。

Result: 发现基准测试结果存在饱和现象，模型性能提升可能源于规模扩展和训练技术改进，而非真正的推理能力提升。

Conclusion: 当前基准测试面临挑战，需要更有效的评估方法。本文为未来推理评估和模型开发研究提供了基础参考。

Abstract: The rapid rise of Large Language Models (LLMs) and Large Reasoning Models
(LRMs) has been accompanied by an equally rapid increase of benchmarks used to
assess them. However, due to both improved model competence resulting from
scaling and novel training advances as well as likely many of these datasets
being included in pre or post training data, results become saturated, driving
a continuous need for new and more challenging replacements. In this paper, we
discuss whether surpassing a benchmark truly demonstrates reasoning ability or
are we simply tracking numbers divorced from the capabilities we claim to
measure? We present an investigation focused on three model families, OpenAI,
Anthropic, and Google, and how their reasoning capabilities across different
benchmarks evolve over the years. We also analyze performance trends over the
years across different reasoning tasks and discuss the current situation of
benchmarking and remaining challenges. By offering a comprehensive overview of
benchmarks and reasoning tasks, our work aims to serve as a first reference to
ground future research in reasoning evaluation and model development.

</details>


### [66] [Confounding Factors in Relating Model Performance to Morphology](https://arxiv.org/abs/2511.01380)
*Wessel Poelman,Thomas Bauwens,Miryam de Lhoneux*

Main category: cs.CL

TL;DR: 本文重新评估了语言形态特征对分词和语言建模的影响，指出先前研究存在混淆因素，并提出了基于token二元组的内在指标来预测语言建模难度。


<details>
  <summary>Details</summary>
Motivation: 现有研究关于语言形态特征对分词和语言建模的影响存在矛盾结论，作者认为这是由于实验设置中的混淆因素导致的，需要重新评估这些关系。

Method: 识别分析中的混淆因素，重新评估Arnett & Bergen (2025)的三个假设，引入token二元组指标作为预测语言建模难度的内在方法。

Result: 发现先前每个结论都包含混淆因素，token二元组指标可以作为形态复杂性的梯度代理，无需专家标注。

Conclusion: 为可靠回答形态学与语言建模的关系，需要明确实验设置的必要条件，token二元组指标提供了一种有效的内在评估方法。

Abstract: The extent to which individual language characteristics influence
tokenization and language modeling is an open question. Differences in
morphological systems have been suggested as both unimportant and crucial to
consider (Cotterell et al., 2018; Gerz et al., 2018a; Park et al., 2021, inter
alia). We argue this conflicting evidence is due to confounding factors in
experimental setups, making it hard to compare results and draw conclusions. We
identify confounding factors in analyses trying to answer the question of
whether, and how, morphology relates to language modeling. Next, we re-assess
three hypotheses by Arnett & Bergen (2025) for why modeling agglutinative
languages results in higher perplexities than fusional languages: they look at
morphological alignment of tokenization, tokenization efficiency, and dataset
size. We show that each conclusion includes confounding factors. Finally, we
introduce token bigram metrics as an intrinsic way to predict the difficulty of
causal language modeling, and find that they are gradient proxies for
morphological complexity that do not require expert annotation. Ultimately, we
outline necessities to reliably answer whether, and how, morphology relates to
language modeling.

</details>


### [67] [RAGSmith: A Framework for Finding the Optimal Composition of Retrieval-Augmented Generation Methods Across Datasets](https://arxiv.org/abs/2511.01386)
*Muhammed Yusuf Kartal,Suha Kagan Kose,Korhan Sevinç,Burak Aktas*

Main category: cs.CL

TL;DR: RAGSmith是一个模块化框架，将RAG设计视为端到端架构搜索，通过遗传算法优化检索和生成指标，在多个领域显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: RAG质量受多个模块交互影响，孤立优化模块效果有限，需要端到端的整体优化方法。

Method: 使用遗传算法在9个技术家族和46,080个可行管道配置中进行搜索，联合优化检索指标（recall@k, mAP, nDCG, MRR）和生成指标（LLM-Judge和语义相似度）。

Result: 在6个维基百科领域（数学、法律、金融、医学、国防工业、计算机科学）上，RAGSmith配置平均优于基线+3.8%（范围+1.2%到+6.9%），检索增益达+12.5%，生成增益达+7.5%。

Conclusion: RAGSmith证明了进化搜索在全管道优化中的实用性，为构建有效的RAG系统提供了实用的领域感知指导。

Abstract: Retrieval-Augmented Generation (RAG) quality depends on many interacting
choices across retrieval, ranking, augmentation, prompting, and generation, so
optimizing modules in isolation is brittle. We introduce RAGSmith, a modular
framework that treats RAG design as an end-to-end architecture search over nine
technique families and 46{,}080 feasible pipeline configurations. A genetic
search optimizes a scalar objective that jointly aggregates retrieval metrics
(recall@k, mAP, nDCG, MRR) and generation metrics (LLM-Judge and semantic
similarity). We evaluate on six Wikipedia-derived domains (Mathematics, Law,
Finance, Medicine, Defense Industry, Computer Science), each with 100 questions
spanning factual, interpretation, and long-answer types. RAGSmith finds
configurations that consistently outperform naive RAG baseline by +3.8\% on
average (range +1.2\% to +6.9\% across domains), with gains up to +12.5\% in
retrieval and +7.5\% in generation. The search typically explores $\approx
0.2\%$ of the space ($\sim 100$ candidates) and discovers a robust backbone --
vector retrieval plus post-generation reflection/revision -- augmented by
domain-dependent choices in expansion, reranking, augmentation, and prompt
reordering; passage compression is never selected. Improvement magnitude
correlates with question type, with larger gains on factual/long-answer mixes
than interpretation-heavy sets. These results provide practical, domain-aware
guidance for assembling effective RAG systems and demonstrate the utility of
evolutionary search for full-pipeline optimization.

</details>


### [68] [LiveSearchBench: An Automatically Constructed Benchmark for Retrieval and Reasoning over Dynamic Knowledge](https://arxiv.org/abs/2511.01409)
*Heng Zhou,Ao Yu,Yuchen Fan,Jianing Shi,Li Kang,Hejia Geng,Yongting Zhang,Yutao Fan,Yuhao Wu,Tiancheng He,Yiran Qin,Lei Bai,Zhenfei Yin*

Main category: cs.CL

TL;DR: LiveSearchBench是一个自动化构建检索依赖基准的流程，从最新的知识更新中创建动态基准，评估LLM在时效性知识上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估基准多为静态，奖励记忆而非检索，无法反映世界知识的动态特性，需要能捕捉时效性知识差距的评估方法。

Method: 通过计算Wikidata连续快照的差异，筛选高质量三元组，生成三个推理难度级别的自然语言问题，每个问题都通过SPARQL验证确保唯一可验证答案。

Result: 实验显示模型在面对预训练后新事实时性能显著下降，多跳查询差距最明显。检索增强方法和更大指令调优模型只能部分改善但无法消除时效性差距。

Conclusion: LiveSearchBench将评估从静态记忆转向需要最新检索和推理的任务，为系统化长期评估LLM在演化知识下的表现提供了基础。

Abstract: Evaluating large language models (LLMs) on question answering often relies on
static benchmarks that reward memorization and understate the role of
retrieval, failing to capture the dynamic nature of world knowledge. We present
LiveSearchBench, an automated pipeline for constructing retrieval-dependent
benchmarks from recent knowledge updates. Our method computes deltas between
successive Wikidata snapshots, filters candidate triples for quality, and
synthesizes natural-language questions at three levels of reasoning difficulty,
each guaranteed to admit a unique, verifiable answer through SPARQL validation.
The pipeline is fully automated, scalable across time, and minimizes human
intervention, enabling continual regeneration of temporally grounded
benchmarks. Experiments show a pronounced performance drop when models confront
facts that post-date pretraining, with the gap most salient on multi-hop
queries. Retrieval augmented methods and larger, instruction-tuned models
provide partial gains but fail to close this recency gap. By design,
LiveSearchBench shifts evaluation from static memorization toward tasks that
require up-to-date retrieval and reasoning, offering a foundation for
systematic, long-term assessment of LLMs under evolving knowledge.

</details>


### [69] ["Don't Teach Minerva": Guiding LLMs Through Complex Syntax for Faithful Latin Translation with RAG](https://arxiv.org/abs/2511.01454)
*Sergio Torres Aguilar*

Main category: cs.CL

TL;DR: 提出了一种可重复的草稿精炼流程，使用开源大语言模型在拉丁语翻译任务上达到与顶级专有系统相当的性能水平。


<details>
  <summary>Details</summary>
Motivation: 解决形态丰富、资源匮乏语言（如拉丁语）翻译的挑战，探索开源模型在低资源语言翻译中的潜力。

Method: 使用微调的NLLB-1.3B模型生成高质量草稿，然后通过零样本LLM（Llama-3.3或Qwen3）进行精炼，可结合检索增强生成（RAG）技术。

Result: 在标准域内测试集和新的域外测试集上，该开源RAG系统性能与GPT-5基线统计相当，无需任务特定的LLM微调。

Conclusion: 开源系统可以在低资源语言翻译任务中达到与专有系统相当的性能，为可复现研究提供了完整工具链。

Abstract: Translating a morphology-rich, low-resource language like Latin poses
significant challenges. This paper introduces a reproducible draft-based
refinement pipeline that elevates open-source Large Language Models (LLMs) to a
performance level statistically comparable to top-tier proprietary systems. Our
method first uses a fine-tuned NLLB-1.3B model to generate a high-quality,
structurally faithful draft. A zero-shot LLM (Llama-3.3 or Qwen3) then polishes
this draft, a process that can be further enhanced by augmenting the context
with retrieved out-context examples (RAG). We demonstrate the robustness of
this approach on two distinct benchmarks: a standard in-domain test set
(Rosenthal, 2023) and a new, challenging out-of-domain (OOD) set of
12th-century Latin letters (2025). Our central finding is that this open-source
RAG system achieves performance statistically comparable to the GPT-5 baseline,
without any task-specific LLM fine-tuning. We release the pipeline, the
Chartres OOD set, and evaluation scripts and models to facilitate replicability
and further research.

</details>


### [70] [BARD: budget-aware reasoning distillation](https://arxiv.org/abs/2511.01470)
*Lujie Niu,Lei Shen,Yi Jiang,Caixia Yuan,Xiaojie Wang,Wenbo Su,Bo zheng*

Main category: cs.CL

TL;DR: 提出BARD框架，通过两阶段训练同时蒸馏推理能力并精细控制推理长度，使用预算作为控制信号平衡推理性能与计算效率


<details>
  <summary>Details</summary>
Motivation: 解决长CoT蒸馏中推理过程冗余、计算预算不可控导致的资源使用低效问题

Method: 两阶段训练：第一阶段在教师生成的长CoT数据上进行SFT，压缩到不同预算水平；第二阶段使用RL同时优化推理性能和预算保真度

Result: 8B学生模型在AIME24、AIME25、GPQA等推理基准上表现优异，并能跨广泛预算范围精确自适应控制推理长度

Conclusion: BARD框架成功实现了推理能力蒸馏与推理长度精细控制的统一，为高效推理模型提供了新思路

Abstract: While long Chain-of-Thought (CoT) distillation effectively transfers
reasoning capability to smaller language models, the reasoning process often
remains redundant and computational budget uncontrollable, leading to
inefficient resource usage. To address this limitation, we propose
\textbf{Budget-Aware Reasoning Distillation (BARD)}, a novel framework that
simultaneously distills reasoning capability and enables fine-grained control
over the reasoning length. BARD uses the thinking budget as a user-specified
control signal, allowing the model to dynamically balance reasoning performance
and computational efficiency. To achieve this concept, BARD introduces a
two-phase training regimen. The first phase, Supervised Fine-Tuning (SFT) on
teacher-generated long CoT data compressed to various budget levels,
bootstrapping the model's understanding of budget constraints. The second phase
leverages Reinforcement Learning (RL) from a reward signal in consideration of
reasoning performance and budget fidelity simultaneously. Incorporating the
two-phase regimen is crucial to avoiding policy degradation and ensuring that
both objectives are optimized jointly. Extensive experiments demonstrate that
our method empowers an 8B student model to achieve strong performance on
challenging reasoning benchmarks (\textit{AIME24, AIME25, GPQA}) while
providing precise and adaptive control over its reasoning length across a wide
range of budgets.

</details>


### [71] [Towards Consistent Detection of Cognitive Distortions: LLM-Based Annotation and Dataset-Agnostic Evaluation](https://arxiv.org/abs/2511.01482)
*Neha Sharma,Navneet Agarwal,Kairit Sirts*

Main category: cs.CL

TL;DR: 使用大型语言模型作为认知扭曲检测的标注工具，提出多轮独立标注方法，并引入数据集无关的评估框架，证明GPT-4能产生一致标注并提升模型性能


<details>
  <summary>Details</summary>
Motivation: 认知扭曲检测任务具有高度主观性，人类标注者间一致性低，导致标注不可靠，需要寻找更一致可靠的标注方法

Method: 使用LLMs作为标注器，通过多轮独立标注揭示稳定模式；引入基于Cohen's kappa的数据集无关评估框架

Result: GPT-4能产生高度一致的标注（Fleiss's Kappa = 0.78），基于LLM标注训练的模型在测试集上表现优于基于人类标注训练的模型

Conclusion: LLMs可为主观NLP任务提供可扩展且内部一致的训练数据生成方案，支持强大的下游性能

Abstract: Text-based automated Cognitive Distortion detection is a challenging task due
to its subjective nature, with low agreement scores observed even among expert
human annotators, leading to unreliable annotations. We explore the use of
Large Language Models (LLMs) as consistent and reliable annotators, and propose
that multiple independent LLM runs can reveal stable labeling patterns despite
the inherent subjectivity of the task. Furthermore, to fairly compare models
trained on datasets with different characteristics, we introduce a
dataset-agnostic evaluation framework using Cohen's kappa as an effect size
measure. This methodology allows for fair cross-dataset and cross-study
comparisons where traditional metrics like F1 score fall short. Our results
show that GPT-4 can produce consistent annotations (Fleiss's Kappa = 0.78),
resulting in improved test set performance for models trained on these
annotations compared to those trained on human-labeled data. Our findings
suggest that LLMs can offer a scalable and internally consistent alternative
for generating training data that supports strong downstream performance in
subjective NLP tasks.

</details>


### [72] [Synthetic Eggs in Many Baskets: The Impact of Synthetic Data Diversity on LLM Fine-Tuning](https://arxiv.org/abs/2511.01490)
*Max Schaffelder,Albert Gatt*

Main category: cs.CL

TL;DR: 研究合成数据来源多样性对微调大语言模型的影响，重点关注分布塌缩、对抗鲁棒性和自偏好偏差三个维度。


<details>
  <summary>Details</summary>
Motivation: 随着合成数据在语言模型开发中的广泛应用，理解其对模型行为的影响变得至关重要。

Method: 通过分析不同来源合成数据对模型微调的影响，研究三个关键维度：分布塌缩、对抗鲁棒性和自偏好偏差。

Result: 使用多样来源的合成数据进行微调可以缓解分布塌缩，保持输出分布的广度和文本多样性；合成数据微调在移除安全防护的同时保持更高输出质量；微调能减少自偏好偏差，人类数据效果最佳，多源合成数据次之。

Conclusion: 合成数据来源的多样性对模型行为有显著影响，多样来源合成数据有助于缓解分布塌缩，但需要平衡安全性和输出质量的关系。

Abstract: As synthetic data becomes widely used in language model development,
understanding its impact on model behavior is crucial. This paper investigates
the impact of the diversity of sources of synthetic data on fine-tuned large
language models. We focus on three key dimensions: distribution collapse,
adversarial robustness, and self-preference bias. Our findings reveal that
fine-tuning models on synthetic data from diverse sources can mitigate
distribution collapse, preserving the breadth of the output distribution and
the diversity of the output text. Furthermore, while both human and synthetic
fine-tuning data can remove safeguards, the latter preserves higher output
quality, thus making outputs potentially more usable and dangerous. Finally,
fine-tuning reduces self-preference bias, with human data being the most
effective, followed by multi-source synthetic data.

</details>


### [73] [BanglaNirTox: A Large-scale Parallel Corpus for Explainable AI in Bengali Text Detoxification](https://arxiv.org/abs/2511.01512)
*Ayesha Afroza Mohsin,Mashrur Ahsan,Nafisa Maliyat,Shanta Maria,Syed Rifat Raiyan,Hasan Mahmud,Md Kamrul Hasan*

Main category: cs.CL

TL;DR: 提出了一种结合帕累托优化大语言模型和思维链提示的孟加拉语文本去毒新方法，并构建了包含68,041个句子的BanglaNirTox数据集。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语中的有害语言在在线环境中仍然普遍存在，但由于资源有限，孟加拉语文本去毒研究相对不足。

Method: 使用帕累托优化的大语言模型和思维链提示生成去毒句子，构建人工生成的平行语料库BanglaNirTox，并用该数据集微调语言模型。

Result: 帕累托优化的大语言模型结合思维链提示显著提高了孟加拉语文本去毒的质量和一致性。

Conclusion: 该方法有效解决了孟加拉语文本去毒问题，为低资源语言的类似任务提供了可行方案。

Abstract: Toxic language in Bengali remains prevalent, especially in online
environments, with few effective precautions against it. Although text
detoxification has seen progress in high-resource languages, Bengali remains
underexplored due to limited resources. In this paper, we propose a novel
pipeline for Bengali text detoxification that combines Pareto class-optimized
large language models (LLMs) and Chain-of-Thought (CoT) prompting to generate
detoxified sentences. To support this effort, we construct BanglaNirTox, an
artificially generated parallel corpus of 68,041 toxic Bengali sentences with
class-wise toxicity labels, reasonings, and detoxified paraphrases, using
Pareto-optimized LLMs evaluated on random samples. The resulting BanglaNirTox
dataset is used to fine-tune language models to produce better detoxified
versions of Bengali sentences. Our findings show that Pareto-optimized LLMs
with CoT prompting significantly enhance the quality and consistency of Bengali
text detoxification.

</details>


### [74] [Difficulty-Controllable Cloze Question Distractor Generation](https://arxiv.org/abs/2511.01526)
*Seokhoon Kang,Yejin Jeon,Seonjeong Hwang,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: 提出了一种通过数据增强和多任务学习生成可控难度干扰项的新框架，显著优于GPT-4o在干扰项难度控制方面的表现。


<details>
  <summary>Details</summary>
Motivation: 解决多项选择完形填空题中高质量干扰项生成的挑战，现有方法缺乏适应性和难度控制能力，且缺乏难度标注数据集。

Method: 采用双向干扰项生成过程创建难度标注数据集，通过过滤和集成QA系统分类难度，然后利用多任务学习训练难度可控的生成模型。

Result: 实验结果表明，该方法能在不同难度级别生成高质量干扰项，在干扰项难度与人类感知对齐方面显著优于GPT-4o。

Conclusion: 提出的框架成功解决了干扰项生成的难度控制问题，为语言能力评估提供了更有效的工具。

Abstract: Multiple-choice cloze questions are commonly used to assess linguistic
proficiency and comprehension. However, generating high-quality distractors
remains challenging, as existing methods often lack adaptability and control
over difficulty levels, and the absence of difficulty-annotated datasets
further hinders progress. To address these issues, we propose a novel framework
for generating distractors with controllable difficulty by leveraging both data
augmentation and a multitask learning strategy. First, to create a
high-quality, difficulty-annotated dataset, we introduce a two-way distractor
generation process in order to produce diverse and plausible distractors. These
candidates are subsequently refined through filtering and then categorized by
difficulty using an ensemble QA system. Second, this newly created dataset is
leveraged to train a difficulty-controllable generation model via multitask
learning. The framework includes carefully designed auxiliary tasks that
enhance the model's semantic understanding of distractors and its ability to
estimate their difficulty. Experimental results demonstrate that our method
generates high-quality distractors across difficulty levels and substantially
outperforms GPT-4o in aligning distractor difficulty with human perception.

</details>


### [75] [ECO Decoding: Entropy-Based Control for Controllability and Fluency in Controllable Dialogue Generation](https://arxiv.org/abs/2511.01568)
*Seungmin Shin,Dooyoung Kim,Youngjoong Ko*

Main category: cs.CL

TL;DR: 提出ECO解码方法，通过基于熵的动态控制强度调整，在保持流畅性的同时提升可控对话生成的控制能力


<details>
  <summary>Details</summary>
Motivation: 传统加权解码方法使用固定常数管理属性概率偏差，难以找到同时满足可控性和流畅性的理想控制强度

Method: ECO解码根据语言模型和属性分类器概率分布的熵值，在每个生成步骤动态调整控制强度

Result: 在DailyDialog和MultiWOZ数据集上的实验表明，ECO解码在保持流畅性和语法性的同时持续提升可控性，优于现有解码方法

Conclusion: ECO解码缓解了多属性生成中的概率插值问题，在单属性和多属性场景下均表现出色

Abstract: Controllable Dialogue Generation (CDG) enables chatbots to generate responses
with desired attributes, and weighted decoding methods have achieved
significant success in the CDG task. However, using a fixed constant value to
manage the bias of attribute probabilities makes it challenging to find an
ideal control strength that satisfies both controllability and fluency. To
address this issue, we propose ECO decoding (Entropy-based COntrol), which
dynamically adjusts the control strength at each generation step according to
the model's entropy in both the language model and attribute classifier
probability distributions. Experiments on the DailyDialog and MultiWOZ datasets
demonstrate that ECO decoding consistently improves controllability while
maintaining fluency and grammaticality, outperforming prior decoding methods
across various models and settings. Furthermore, ECO decoding alleviates
probability interpolation issues in multi-attribute generation and consequently
demonstrates strong performance in both single and multi-attribute scenarios.

</details>


### [76] [BIRD: Bronze Inscription Restoration and Dating](https://arxiv.org/abs/2511.01589)
*Wenjie Hua,Hoang H. Nguyen,Gangyan Ge*

Main category: cs.CL

TL;DR: BIRD数据集用于青铜器铭文修复与断代，提出异体字感知的掩码语言建模框架，结合领域自适应预训练和字形网络，提升修复和断代效果。


<details>
  <summary>Details</summary>
Motivation: 早期中国青铜器铭文残缺且难以断代，需要系统化的数据集和模型来解决这一挑战。

Method: 提出异体字感知的掩码语言建模框架，集成领域自适应预训练和字形网络(GN)，连接字素和异体字。

Result: 实验表明字形网络改进了铭文修复，字形偏置采样在断代任务中取得增益。

Conclusion: 该方法为青铜器铭文研究提供了有效的计算工具，在修复和断代任务上都表现出色。

Abstract: Bronze inscriptions from early China are fragmentary and difficult to date.
We introduce BIRD(Bronze Inscription Restoration and Dating), a fully encoded
dataset grounded in standard scholarly transcriptions and chronological labels.
We further propose an allograph-aware masked language modeling framework that
integrates domain- and task-adaptive pretraining with a Glyph Net (GN), which
links graphemes and allographs. Experiments show that GN improves restoration,
while glyph-biased sampling yields gains in dating.

</details>


### [77] [Imperfect Language, Artificial Intelligence, and the Human Mind: An Interdisciplinary Approach to Linguistic Errors in Native Spanish Speakers](https://arxiv.org/abs/2511.01615)
*Francisco Portillo López*

Main category: cs.CL

TL;DR: 研究西班牙语母语者的语言错误，分析大语言模型如何解释、复制或纠正这些错误，旨在开发更符合认知的NLP系统。


<details>
  <summary>Details</summary>
Motivation: 语言错误不仅是语法偏差，还揭示了语言的认知架构和AI系统的局限性。通过研究母语西班牙语者的错误，可以了解LLM对人类语言行为的理解能力。

Method: 整合理论语言学、神经语言学和自然语言处理三个视角，构建包含500+真实错误的西班牙语语料库，使用GPT或Gemini等AI模型测试其解释准确性和泛化能力。

Result: 项目将评估AI模型对真实语言错误的处理能力，揭示其与人类语言处理的差异。

Conclusion: 该研究不仅有助于理解西班牙语作为母语的特点，还能推动开发更认知化、能处理人类语言不完美性和模糊性的NLP系统。

Abstract: Linguistic errors are not merely deviations from normative grammar; they
offer a unique window into the cognitive architecture of language and expose
the current limitations of artificial systems that seek to replicate them. This
project proposes an interdisciplinary study of linguistic errors produced by
native Spanish speakers, with the aim of analyzing how current large language
models (LLM) interpret, reproduce, or correct them. The research integrates
three core perspectives: theoretical linguistics, to classify and understand
the nature of the errors; neurolinguistics, to contextualize them within
real-time language processing in the brain; and natural language processing
(NLP), to evaluate their interpretation against linguistic errors. A
purpose-built corpus of authentic errors of native Spanish (+500) will serve as
the foundation for empirical analysis. These errors will be tested against AI
models such as GPT or Gemini to assess their interpretative accuracy and their
ability to generalize patterns of human linguistic behavior. The project
contributes not only to the understanding of Spanish as a native language but
also to the development of NLP systems that are more cognitively informed and
capable of engaging with the imperfect, variable, and often ambiguous nature of
real human language.

</details>


### [78] [ParlaSpeech 3.0: Richly Annotated Spoken Parliamentary Corpora of Croatian, Czech, Polish, and Serbian](https://arxiv.org/abs/2511.01619)
*Nikola Ljubešić,Peter Rupnik,Ivan Porupski,Taja Kuzman Pungeršek*

Main category: cs.CL

TL;DR: ParlaSpeech是一个包含克罗地亚语、捷克语、波兰语和塞尔维亚语四种斯拉夫语言的议会语音语料库，总计6000小时，通过自动方式从ParlaMint转录构建，并丰富了多种自动注释层。


<details>
  <summary>Details</summary>
Motivation: 构建一个大规模的多语言议会语音语料库，为跨学科研究提供丰富的语音和文本数据资源。

Method: 从ParlaMint转录和相应元数据自动构建，与各国议会语音录音对齐，并添加了语言注释、情感预测、填充停顿检测、词级对齐和重音位置等自动注释层。

Result: 创建了包含6000小时语音的四个斯拉夫语言语料库，显著增强了语料库的实用性，并通过情感声学相关性分析展示了其应用价值。

Conclusion: ParlaSpeech语料库通过丰富的自动注释显著提升了其研究价值，为多学科下游研究提供了重要资源，并以JSONL、TextGrid格式和检索工具形式开放使用。

Abstract: ParlaSpeech is a collection of spoken parliamentary corpora currently
spanning four Slavic languages - Croatian, Czech, Polish and Serbian - all
together 6 thousand hours in size. The corpora were built in an automatic
fashion from the ParlaMint transcripts and their corresponding metadata, which
were aligned to the speech recordings of each corresponding parliament. In this
release of the dataset, each of the corpora is significantly enriched with
various automatic annotation layers. The textual modality of all four corpora
has been enriched with linguistic annotations and sentiment predictions.
Similar to that, their spoken modality has been automatically enriched with
occurrences of filled pauses, the most frequent disfluency in typical speech.
Two out of the four languages have been additionally enriched with detailed
word- and grapheme-level alignments, and the automatic annotation of the
position of primary stress in multisyllabic words. With these enrichments, the
usefulness of the underlying corpora has been drastically increased for
downstream research across multiple disciplines, which we showcase through an
analysis of acoustic correlates of sentiment. All the corpora are made
available for download in JSONL and TextGrid formats, as well as for search
through a concordancer.

</details>


### [79] [A Graph-based RAG for Energy Efficiency Question Answering](https://arxiv.org/abs/2511.01643)
*Riccardo Campi,Nicolò Oreste Pinciroli Vago,Mathyas Giudici,Pablo Barrachina Rodriguez-Guisado,Marco Brambilla,Piero Fraternali*

Main category: cs.CL

TL;DR: 该研究探索了在基于图谱的检索增强生成架构中使用大语言模型进行能效问答，通过从能源文档自动提取知识图谱，结合多语言推理提供准确答案，验证结果显示75.2%的正确率。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决能源领域专业问答的准确性挑战，利用图谱结构增强大语言模型的知识检索和推理能力，同时支持多语言问答需求。

Method: 首先从能源指导文件和法规文档自动提取知识图谱，然后在图谱上进行导航和推理，结合检索增强生成架构，使用RAGAs框架进行人工验证。

Result: 系统在101个问答对的验证数据集上达到75.2±2.7%的正确率，通用能效问题表现更好（最高81.0±4.1%），多语言能力良好（翻译仅导致4.4%准确率损失）。

Conclusion: 基于图谱的检索增强生成架构在能效问答中具有潜力，能够提供相对准确的答案并支持多语言，但也存在需要改进的弱点。

Abstract: In this work, we investigate the use of Large Language Models (LLMs) within a
graph-based Retrieval Augmented Generation (RAG) architecture for Energy
Efficiency (EE) Question Answering. First, the system automatically extracts a
Knowledge Graph (KG) from guidance and regulatory documents in the energy
field. Then, the generated graph is navigated and reasoned upon to provide
users with accurate answers in multiple languages. We implement a human-based
validation using the RAGAs framework properties, a validation dataset
comprising 101 question-answer pairs, and domain experts. Results confirm the
potential of this architecture and identify its strengths and weaknesses.
Validation results show how the system correctly answers in about three out of
four of the cases (75.2 +- 2.7%), with higher results on questions related to
more general EE answers (up to 81.0 +- 4.1%), and featuring promising
multilingual abilities (4.4% accuracy loss due to translation).

</details>


### [80] [Evaluating Cultural Knowledge Processing in Large Language Models: A Cognitive Benchmarking Framework Integrating Retrieval-Augmented Generation](https://arxiv.org/abs/2511.01649)
*Hung-Shin Lee,Chen-Chi Chang,Ching-Yuan Chen,Yun-Hsiang Hsu*

Main category: cs.CL

TL;DR: 提出一个认知基准测试框架，结合布鲁姆分类法和检索增强生成，评估大语言模型处理文化特定知识的认知能力。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型在理解和应用文化特定知识方面的表现，特别是在台湾客家数字文化档案中的表现。

Method: 整合布鲁姆分类法和检索增强生成，构建包含六个认知层级的评估框架，使用台湾客家数字文化档案作为测试平台。

Result: 测量大语言模型生成响应的语义准确性和文化相关性。

Conclusion: 该框架能够系统评估大语言模型在文化知识处理方面的认知能力表现。

Abstract: This study proposes a cognitive benchmarking framework to evaluate how large
language models (LLMs) process and apply culturally specific knowledge. The
framework integrates Bloom's Taxonomy with Retrieval-Augmented Generation (RAG)
to assess model performance across six hierarchical cognitive domains:
Remembering, Understanding, Applying, Analyzing, Evaluating, and Creating.
Using a curated Taiwanese Hakka digital cultural archive as the primary
testbed, the evaluation measures LLM-generated responses' semantic accuracy and
cultural relevance.

</details>


### [81] [EngChain: A Symbolic Benchmark for Verifiable Multi-Step Reasoning in Engineering](https://arxiv.org/abs/2511.01650)
*Ayesha Gull,Muhammad Usman Safder,Rania Elbadry,Preslav Nakov,Zhuohan Xie*

Main category: cs.CL

TL;DR: EngChain是一个用于验证多步骤工程问题解决能力的基准测试，包含90个跨3个工程分支的问题，通过符号模板生成确保多样性。采用两阶段评估：定量验证推理步骤的有效性，以及使用LLM-As-A-Judge定性分类推理错误。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要评估语言理解、事实回忆、数学或代码生成能力，但缺乏对工程领域所需的整合性推理能力的评估，这种推理需要融合科学原理、定量建模和实际约束。

Method: 创建EngChain基准测试，包含90个跨3个工程分支的问题，通过符号模板生成确保多样性。采用两阶段评估方法：定量验证推理步骤的数值和语义有效性，以及引入LLM-As-A-Judge系统定性分类推理错误。

Result: 开发了一个专门针对工程领域复杂推理能力的基准测试系统，能够全面评估LLMs在工程问题解决中的表现。

Conclusion: EngChain填补了现有基准测试在评估工程整合性推理能力方面的空白，为LLMs在专业工程领域的应用提供了更全面的评估工具。

Abstract: Large Language Models (LLMs) are increasingly being applied to specialized,
high-stakes domains like engineering, which demands rigorous evaluation of
their complex reasoning capabilities. While current benchmarks assess language
understanding, factual recall, mathematics or code generation, none capture the
integrative reasoning central to engineering where scientific principles,
quantitative modeling and practical constraints must converge. To address this
gap, we introduce EngChain, a benchmark for verifiable multi-step engineering
problem-solving. EngChain contains 90 problems spanning three engineering
branches, organized into 9 domains and 20 distinct areas. The problems are
generated from symbolic templates with a high degree of randomization to ensure
diversity and eliminate the risk of contamination. With this benchmark, we move
beyond final answer accuracy with a two-stage evaluation: we first
quantitatively verify the numerical and semantic validity of each reasoning
step and then introduce LLM-As-A-Judge, an automated system to qualitatively
categorize the identified reasoning errors.

</details>


### [82] [SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia](https://arxiv.org/abs/2511.01670)
*Chaoqun Liu,Mahani Aljunied,Guizhen Chen,Hou Pong Chan,Weiwen Xu,Yu Rong,Wenxuan Zhang*

Main category: cs.CL

TL;DR: SeaLLMs-Audio是首个针对东南亚语言（印尼语、泰语、越南语）以及英语和中文的大规模音频语言模型，支持多模态输入和多种音频任务。


<details>
  <summary>Details</summary>
Motivation: 为东南亚地区开发专门的音频语言模型，填补该地区多语言音频AI研究的空白，促进区域研究和产业发展。

Method: 在大规模音频语料库上训练，支持5种语言，采用多模态架构，可接受纯音频、纯文本或音频+文本的灵活输入。

Result: 在多种音频任务上表现优异，包括音频字幕、语音识别、语音翻译、情感识别等，在东南亚语言上达到与其他LALM相竞争的性能。

Conclusion: SeaLLMs-Audio是推进东南亚音频LLM发展的重要一步，同时推出的SeaBench-Audio基准测试将促进该领域的自动化评估。

Abstract: We introduce SeaLLMs-Audio, the first large audio-language model (LALM)
tailored for multiple Southeast Asian (SEA) languages-Indonesian (id), Thai
(th), and Vietnamese (vi)-alongside English (en) and Chinese (zh). Trained on a
large-scale audio corpus, SeaLLMs-Audio exhibits strong performance across
diverse audio-centric tasks, spanning fine-grained audio understanding and
voice-based interaction. Its key features include: 1) Multilingual: the model
primarily supports 5 languages, namely Indonesian, Thai, Vietnamese, English,
and Chinese; 2) Multimodal: the model accepts flexible input modalities,
including audio only, text only, as well as audio with text; 3) Multi-task: the
model supports a wide range of tasks, including audio analysis tasks such as
Audio Captioning, Automatic Speech Recognition, Speech-to-Text Translation,
Speech Emotion Recognition, Speech Question Answering, and Speech
Summarization. It also enables voice-based dialogue, including answering
factual, mathematical, and general knowledge queries. As a significant step
towards advancing audio LLMs in Southeast Asia, we expect SeaLLMs-Audio to
benefit both the regional research community and industry. To automate LALM
evaluation for Southeast Asia, we introduce SeaBench-Audio, a benchmark
spanning multiple tasks. Experiments show that SeaLLMs-Audio achieves
competitive performance compared with other LALMs on SEA languages.

</details>


### [83] [Open Character Training: Shaping the Persona of AI Assistants through Constitutional AI](https://arxiv.org/abs/2511.01689)
*Sharan Maiya,Henning Bartsch,Nathan Lambert,Evan Hubinger*

Main category: cs.CL

TL;DR: 本文提出了首个开源的AI助手角色训练方法，使用宪法AI和合成内省数据来塑造更有效、可控的助手角色，相比系统提示约束或激活引导等方法更加鲁棒。


<details>
  <summary>Details</summary>
Motivation: 现代聊天机器人语言模型生成的"AI助手"角色特征会影响交互质量、感知智能以及与开发者和用户意图的对齐。角色训练是行业后训练的关键组成部分，但在学术文献中尚未得到充分研究。

Method: 使用宪法AI和新的数据流水线，利用合成内省数据来塑造助手角色。具体对三个流行的开源模型进行微调，使用11个示例角色（如幽默、深度关怀甚至恶意），并通过分析揭示的偏好来追踪效果。

Result: 该方法相比系统提示约束和激活引导等替代方案，对对抗性提示更加鲁棒，同时产生更连贯和真实的生成结果。微调对通用能力基准测试几乎没有影响。

Conclusion: 角色训练可以有效地塑造AI助手的角色特征，同时保持其通用能力，为AI助手角色的可控塑造提供了实用的开源解决方案。

Abstract: The character of the "AI assistant" persona generated by modern chatbot large
language models influences both surface-level behavior and apparent values,
beliefs, and ethics. These all affect interaction quality, perceived
intelligence, and alignment with both developer and user intentions. The
shaping of this persona, known as character training, is a critical component
of industry post-training, yet remains effectively unstudied in the academic
literature. We introduce the first open implementation of character training,
leveraging Constitutional AI and a new data pipeline using synthetic
introspective data to shape the assistant persona in a more effective and
controlled manner than alternatives such as constraining system prompts or
activation steering. Specifically, we fine-tune three popular open-weights
models using 11 example personas, such as humorous, deeply caring, or even
malevolent. To track the effects of our approach, we introduce a method which
analyzes revealed preferences, uncovering clear and holistic changes in
character. We find these changes are more robust to adversarial prompting than
the above two alternatives, while also leading to more coherent and realistic
generations. Finally, we demonstrate this fine-tuning has little to no effect
on general capabilities as measured by common benchmarks. We describe and
open-source our full post-training method, the implementation of which can be
found at https://github.com/maiush/OpenCharacterTraining.

</details>


### [84] [Multi-Step Knowledge Interaction Analysis via Rank-2 Subspace Disentanglement](https://arxiv.org/abs/2511.01706)
*Sekh Mainul Islam,Pepa Atanasova,Isabelle Augenstein*

Main category: cs.CL

TL;DR: 提出了一个新颖的rank-2投影子空间来更准确地区分大语言模型中参数知识(PK)和上下文知识(CK)的贡献，并首次对长自然语言解释序列中的多步骤知识交互进行分析。


<details>
  <summary>Details</summary>
Motivation: 理解参数知识(PK)和上下文知识(CK)在大语言模型决策中的交互对于评估自然语言解释的可靠性至关重要，但现有研究对此探索不足，通常只关注单步生成或将其建模为二元选择。

Method: 使用rank-2投影子空间来解缠PK和CK的贡献，并在四个QA数据集和三个开源指令调优LLM上进行实验，首次对长NLE序列进行多步骤知识交互分析。

Result: 实验表明，多样化的知识交互在rank-1子空间中表现不佳，但在rank-2公式中能有效捕捉。多步骤分析显示：幻觉NLE强烈偏向PK方向，上下文忠实NLE平衡PK和CK，思维链提示使NLE向CK方向偏移。

Conclusion: 这项工作为通过更丰富的rank-2子空间解缠来系统研究LLM中多步骤知识交互提供了首个框架。

Abstract: Natural Language Explanations (NLEs) describe how Large Language Models
(LLMs) make decisions, drawing on both external Context Knowledge (CK) and
Parametric Knowledge (PK) stored in model weights. Understanding their
interaction is key to assessing the grounding of NLEs, yet it remains
underexplored. Prior work has largely examined only single-step generation,
typically the final answer, and has modelled PK and CK interaction only as a
binary choice in a rank-1 subspace. This overlooks richer forms of interaction,
such as complementary or supportive knowledge. We propose a novel rank-2
projection subspace that disentangles PK and CK contributions more accurately
and use it for the first multi-step analysis of knowledge interactions across
longer NLE sequences. Experiments on four QA datasets and three open-weight
instruction-tuned LLMs show that diverse knowledge interactions are poorly
represented in a rank-1 subspace but are effectively captured in our rank-2
formulation. Our multi-step analysis reveals that hallucinated NLEs align
strongly with the PK direction, context-faithful ones balance PK and CK, and
Chain-of-Thought prompting for NLEs shifts generated NLEs toward CK by reducing
PK reliance. This work provides the first framework for systematic studies of
multi-step knowledge interactions in LLMs through a richer rank-2 subspace
disentanglement. Code and data:
https://github.com/copenlu/pk-ck-knowledge-disentanglement.

</details>


### [85] [Efficient Tool-Calling Multi-Expert NPC Agent for Commonsense Persona-Grounded Dialogue](https://arxiv.org/abs/2511.01720)
*Mahammad Nuriyev*

Main category: cs.CL

TL;DR: 提出了一个基于Qwen3模型和LoRA适配器的多专家系统，用于创建能够进行自然对话和执行上下文动作的NPC角色。该系统在计算效率方面表现优异，在2025年常识人格对话挑战赛中排名第二。


<details>
  <summary>Details</summary>
Motivation: 开发能够同时进行自然对话和执行上下文动作的非玩家角色(NPC)，以满足交互式环境中对智能NPC的需求。

Method: 使用Qwen3作为基础模型，通过LoRA适配器实例化三个专家模块：工具调用、工具响应解释和直接对话。

Result: 系统在计算效率方面表现良好，在L40S GPU上提供快速响应并保持适度的资源使用。在2025年常识人格对话挑战赛中排名第二。

Conclusion: 基于Qwen3和LoRA的多专家系统为创建智能NPC提供了一种有效的解决方案，在保持计算效率的同时实现了良好的对话和动作执行能力。

Abstract: We present a multi-expert system for creating Non-Player Characters (NPCs)
capable of both natural dialogue and contextual action execution in interactive
environments. Using Qwen3 as the base model and Low-Rank Adaptation (LoRA)
adapters, we instantiate three specialists: tool calling, tool-response
interpretation, and direct dialogue. Our system comfortably meets the
computational efficiency requirements, delivering fast responses and
maintaining modest resource usage on L40S GPUs. In the Commonsense
Persona-Grounded Dialogue Challenge 2025, our method ranked second overall.
  Code available at:
https://github.com/MahammadNuriyev62/CPDC-challenge-2025-solution/

</details>


### [86] [Accumulating Context Changes the Beliefs of Language Models](https://arxiv.org/abs/2511.01805)
*Jiayi Geng,Howard Chen,Ryan Liu,Manoel Horta Ribeiro,Robb Willer,Graham Neubig,Thomas L. Griffiths*

Main category: cs.CL

TL;DR: 语言模型在长时间对话和阅读过程中会经历信念漂移，导致其世界观和行为发生显著变化，这带来了潜在风险。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型在自主应用中的广泛使用，上下文积累可能导致模型信念配置文件无声改变，造成不一致的用户体验和行为偏离原始对齐目标。

Method: 通过多轮讨论道德困境和安全查询测试信念变化，设计工具使用任务来观察行为变化，并分析阅读对立立场文本对政治信念的影响。

Result: GPT-5在10轮道德讨论后信念改变54.7%，Grok 4阅读对立政治文本后信念改变27.2%，行为变化与信念漂移一致。

Conclusion: 语言模型在长时间对话和阅读过程中会经历显著的信念漂移，这使其观点和行为变得不可靠，暴露了自主系统中的潜在风险。

Abstract: Language model (LM) assistants are increasingly used in applications such as
brainstorming and research. Improvements in memory and context size have
allowed these models to become more autonomous, which has also resulted in more
text accumulation in their context windows without explicit user intervention.
This comes with a latent risk: the belief profiles of models -- their
understanding of the world as manifested in their responses or actions -- may
silently change as context accumulates. This can lead to subtly inconsistent
user experiences, or shifts in behavior that deviate from the original
alignment of the models. In this paper, we explore how accumulating context by
engaging in interactions and processing text -- talking and reading -- can
change the beliefs of language models, as manifested in their responses and
behaviors.Our results reveal that models' belief profiles are highly malleable:
GPT-5 exhibits a 54.7% shift in its stated beliefs after 10 rounds of
discussion about moral dilemmas and queries about safety, while Grok 4 shows a
27.2% shift on political issues after reading texts from the opposing position.
We also examine models' behavioral changes by designing tasks that require tool
use, where each tool selection corresponds to an implicit belief. We find that
these changes align with stated belief shifts, suggesting that belief shifts
will be reflected in actual behavior in agentic systems. Our analysis exposes
the hidden risk of belief shift as models undergo extended sessions of talking
or reading, rendering their opinions and actions unreliable.

</details>


### [87] [Plan-and-Write: Structure-Guided Length Control for LLMs without Model Retraining](https://arxiv.org/abs/2511.01807)
*Adewale Akinfaderin,Shreyas Subramanian,Akarsha Sehwag*

Main category: cs.CL

TL;DR: 提出一种无需模型重新训练的提示工程方法，通过结构化规划和字数统计机制实现大语言模型的精确长度控制


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型中长度控制这一重要但未被充分解决的问题，当前方法需要昂贵的模型重新训练或复杂的推理时工具

Method: 采用结构引导的提示工程方法，在提示中实现精心的规划和字数统计机制，鼓励模型仔细跟踪并遵守指定的长度约束

Result: 在六个最先进的大语言模型上的评估显示，该方法显著提高了长度保真度，某些模型在长度遵守方面提升了37.6%，同时保持或提升了输出质量

Conclusion: 该方法为需要精确长度控制的应用提供了立即可部署的解决方案，特别适用于模型重新训练不切实际或成本过高的生产环境

Abstract: Length control in Large Language Models (LLMs) is a crucial but
under-addressed challenge, with applications ranging from voice interfaces
requiring concise responses to research summaries needing comprehensive
outputs. Current approaches to length control, including Regularized DPO,
Length-Instruction Fine Tuning, and tool-augmented methods, typically require
expensive model retraining or complex inference-time tooling. This paper
presents a prompt engineering methodology that enables precise length control
without model retraining. Our structure-guided approach implements deliberate
planning and word counting mechanisms within the prompt, encouraging the model
to carefully track and adhere to specified length constraints. Comprehensive
evaluations across six state-of-the-art LLMs demonstrate that our method
significantly improves length fidelity for several models compared to standard
prompting when applied to document summarization tasks, particularly for
shorter-to-medium length constraints. The proposed technique shows varying
benefits across different model architectures, with some models demonstrating
up to 37.6% improvement in length adherence. Quality evaluations further reveal
that our approach maintains or enhances overall output quality compared to
standard prompting techniques. Our approach provides an immediately deployable
solution for applications requiring precise length control, particularly
valuable for production environments where model retraining is impractical or
cost-prohibitive.

</details>


### [88] [KV Cache Transform Coding for Compact Storage in LLM Inference](https://arxiv.org/abs/2511.01815)
*Konrad Staniszewski,Adrian Łańcucki*

Main category: cs.CL

TL;DR: KVTC是一种轻量级变换编码器，通过PCA特征去相关、自适应量化和熵编码来压缩KV缓存，实现高达20倍压缩比，同时保持推理精度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型服务需要高效的KV缓存管理，共享前缀提示中的陈旧缓存会消耗宝贵的GPU内存，需要压缩存储方案。

Method: 结合经典媒体压缩技术，使用PCA特征去相关、自适应量化和熵编码，仅需简短初始校准且不改变模型参数。

Result: 在Llama 3、Mistral NeMo和R1-Qwen 2.5等模型上测试，KVTC实现高达20倍压缩，特定用例可达40倍以上，在多个基准测试中优于现有方法。

Conclusion: KVTC是内存高效LLM服务的实用构建模块，支持可重用KV缓存。

Abstract: Serving large language models (LLMs) at scale necessitates efficient
key-value (KV) cache management. KV caches can be reused across conversation
turns via shared-prefix prompts that are common in iterative code editing and
chat. However, stale caches consume scarce GPU memory, require offloading, or
force recomputation. We present KVTC, a lightweight transform coder that
compresses KV caches for compact on-GPU and off-GPU storage. Drawing on
classical media compression, KVTC combines PCA-based feature decorrelation,
adaptive quantization, and entropy coding. It requires only a brief initial
calibration and leaves model parameters unchanged. By exploiting redundancies
in KV caches, KVTC achieves up to 20$\times$ compression while maintaining
reasoning and long-context accuracy, and 40$\times$ or higher for specific use
cases. We test KVTC with Llama 3, Mistral NeMo, and R1-Qwen 2.5 models across
benchmarks including AIME25, LiveCodeBench, GSM8K, MMLU, Qasper, RULER, and
MATH-500. It consistently outperforms inference-time baselines such as token
eviction, quantization, and SVD-based methods, while achieving higher
compression ratios. These results support KVTC as a practical building block
for memory-efficient LLM serving with reusable KV caches.

</details>


### [89] [Towards Robust Mathematical Reasoning](https://arxiv.org/abs/2511.01846)
*Thang Luong,Dawsen Hwang,Hoang H. Nguyen,Golnaz Ghiasi,Yuri Chervonyi,Insuk Seo,Junsu Kim,Garrett Bingham,Jonathan Lee,Swaroop Mishra,Alex Zhai,Clara Huiyi Hu,Henryk Michalewski,Jimin Kim,Jeonghyun Ahn,Junhwi Bae,Xingyou Song,Trieu H. Trinh,Quoc V. Le,Junehyuk Jung*

Main category: cs.CL

TL;DR: IMO-Bench是一个针对国际数学奥林匹克竞赛水平的高级推理基准套件，包含答案评估和证明评估两部分，在Gemini Deep Think模型中实现了历史性的金牌表现。


<details>
  <summary>Details</summary>
Motivation: 现有数学推理评估要么太简单，要么只关注简短答案的正确性，需要更高级的基准来推动基础模型的数学推理能力发展。

Method: 开发了IMO-Bench基准套件，包括IMO-AnswerBench（400个多样化的奥数问题）和IMO-Proof Bench（证明写作能力评估），并建立了自动评分系统。

Result: Gemini Deep Think模型在IMO-AnswerBench上达到80.0%，在高级IMO-Proof Bench上达到65.7%，分别比最佳非Gemini模型高出6.9%和42.4%。

Conclusion: IMO-Bench为社区推进稳健数学推理提供了重要工具，自动评分器与人工评估相关性良好，有助于长答案自动评估的进一步发展。

Abstract: Finding the right north-star metrics is highly critical for advancing the
mathematical reasoning capabilities of foundation models, especially given that
existing evaluations are either too easy or only focus on getting correct short
answers. To address these issues, we present IMO-Bench, a suite of advanced
reasoning benchmarks, vetted by a panel of top specialists and that
specifically targets the level of the International Mathematical Olympiad
(IMO), the most prestigious venue for young mathematicians. IMO-AnswerBench
first tests models on 400 diverse Olympiad problems with verifiable short
answers. IMO-Proof Bench is the next-level evaluation for proof-writing
capabilities, which includes both basic and advanced IMO level problems as well
as detailed grading guidelines to facilitate automatic grading. These
benchmarks played a crucial role in our historic achievement of the gold-level
performance at IMO 2025 with Gemini Deep Think (Luong and Lockhart, 2025). Our
model achieved 80.0% on IMO-AnswerBench and 65.7% on the advanced IMO-Proof
Bench, surpassing the best non-Gemini models by large margins of 6.9% and 42.4%
respectively. We also showed that autograders built with Gemini reasoning
correlate well with human evaluations and construct IMO-GradingBench, with 1000
human gradings on proofs, to enable further progress in automatic evaluation of
long-form answers. We hope that IMO-Bench will help the community towards
advancing robust mathematical reasoning and release it at
https://imobench.github.io/.

</details>


### [90] [Tool-to-Agent Retrieval: Bridging Tools and Agents for Scalable LLM Multi-Agent Systems](https://arxiv.org/abs/2511.01854)
*Elias Lumer,Faheem Nizar,Anmol Gulati,Pradeep Honaganahalli Basavaraju,Vamse Kumar Subbiah*

Main category: cs.CL

TL;DR: 提出了Tool-to-Agent Retrieval框架，通过在共享向量空间中嵌入工具和其父代理，并利用元数据关系连接它们，实现了细粒度的工具级或代理级检索，显著提升了检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有的检索方法通常将查询与粗粒度的代理级描述进行匹配，这掩盖了细粒度的工具功能，导致代理选择不理想。

Method: 在共享向量空间中嵌入工具及其父代理，通过元数据关系连接它们，支持细粒度的工具级或代理级检索。

Result: 在LiveMCPBench基准测试中，使用八种嵌入模型，Tool-to-Agent Retrieval在Recall@5和nDCG@5上分别比现有最先进的代理检索器提高了19.4%和17.7%。

Conclusion: Tool-to-Agent Retrieval框架通过细粒度表示工具能力和代理，有效避免了上下文稀释问题，显著提升了多代理系统中的检索性能。

Abstract: Recent advances in LLM Multi-Agent Systems enable scalable orchestration of
sub-agents, each coordinating hundreds or thousands of tools or Model Context
Protocol (MCP) servers. However, existing retrieval methods typically match
queries against coarse agent-level descriptions before routing, which obscures
fine-grained tool functionality and often results in suboptimal agent
selection. We introduce Tool-to-Agent Retrieval, a unified framework that
embeds both tools and their parent agents in a shared vector space and connects
them through metadata relationships. By explicitly representing tool
capabilities and traversing metadata to the agent level, Tool-to-Agent
Retrieval enables granular tool-level or agent-level retrieval, ensuring that
agents and their underlying tools or MCP servers are equally represented
without the context dilution that arises from chunking many tools together.
Evaluating Tool-to-Agent Retrieval across eight embedding models, our approach
achieves consistent improvements of 19.4% in Recall@5 and 17.7% in nDCG@5 over
previous state-of-the-art agent retrievers on the LiveMCPBench benchmark.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [91] [Residual Balancing for Non-Linear Outcome Models in High Dimensions](https://arxiv.org/abs/2511.00324)
*Isaac Meza*

Main category: econ.EM

TL;DR: 将近似残差平衡框架扩展到非线性模型，解决了高维设置中广义线性模型下平均处理效应的估计问题。


<details>
  <summary>Details</summary>
Motivation: 解决Athey等人(2018)提出的开放性问题，处理高维设置中广义线性模型下平均处理效应估计的挑战。

Method: 推导非线性模型的偏差分解，揭示需要二阶校正来处理链接函数的曲率。通过优化问题构建平衡权重，控制一阶和二阶偏差源。

Result: 为估计器提供理论保证，在标准高维假设下建立其√n一致性和渐近正态性。

Conclusion: 成功将ARB框架扩展到非线性模型，为高维设置中的因果推断提供了新的理论和方法基础。

Abstract: We extend the approximate residual balancing (ARB) framework to nonlinear
models, answering an open problem posed by Athey et al. (2018). Our approach
addresses the challenge of estimating average treatment effects in
high-dimensional settings where the outcome follows a generalized linear model.
We derive a new bias decomposition for nonlinear models that reveals the need
for a second-order correction to account for the curvature of the link
function. Based on this insight, we construct balancing weights through an
optimization problem that controls for both first and second-order sources of
bias. We provide theoretical guarantees for our estimator, establishing its
$\sqrt{n}$-consistency and asymptotic normality under standard high-dimensional
assumptions.

</details>


### [92] [Concentration Inequalities for Suprema of Empirical Processes with Dependent Data via Generic Chaining with Applications to Statistical Learning](https://arxiv.org/abs/2511.00597)
*Chiara Amorino,Christian Brownlees,Ankita Ghosh*

Main category: econ.EM

TL;DR: 本文开发了一个适用于依赖数据经验过程上确界的一般性集中不等式，结合通用链和耦合策略，适用于高维和重尾数据，并应用于依赖数据下经验风险最小化的预测性能保证。


<details>
  <summary>Details</summary>
Motivation: 现有集中不等式主要针对独立同分布数据，而现实数据往往存在依赖性，需要开发适用于依赖数据的理论框架来保证经验风险最小化在非线性回归中的预测性能。

Method: 结合通用链方法和耦合策略，建立依赖数据下经验过程上确界的集中不等式，适用于高维和重尾（次韦伯）数据。

Result: 证明了依赖数据下经验风险最小化能够达到与独立同分布设置相当的预测精度，特别针对非线性回归模型和单层神经网络模型建立了oracle不等式。

Conclusion: 所提出的集中不等式为依赖数据下的统计学习提供了理论保证，表明经验风险最小化在依赖数据下仍能保持良好性能，拓展了传统独立同分布假设下的理论结果。

Abstract: This paper develops a general concentration inequality for the suprema of
empirical processes with dependent data. The concentration inequality is
obtained by combining generic chaining with a coupling-based strategy. Our
framework accommodates high-dimensional and heavy-tailed (sub-Weibull) data. We
demonstrate the usefulness of our result by deriving non-asymptotic predictive
performance guarantees for empirical risk minimization in regression problems
with dependent data. In particular, we establish an oracle inequality for a
broad class of nonlinear regression models and, as a special case, a
single-layer neural network model. Our results show that empirical risk
minimzaton with dependent data attains a prediction accuracy comparable to that
in the i.i.d. setting for a wide range of nonlinear regression models.

</details>


### [93] [Improving control over unobservables with network data](https://arxiv.org/abs/2511.00612)
*Vincent Starck*

Main category: econ.EM

TL;DR: 开发了一种利用网络同质性进行因果推断的方法，通过渐进同质性概念处理未观察到的混杂因素，在网络形成模型中考虑同质性、度异质性、稀疏性和聚类等特征，获得对未观察变量选择具有鲁棒性的处理效应一致估计量。


<details>
  <summary>Details</summary>
Motivation: 在存在未观察到的混杂因素的情况下进行因果推断是一个挑战，网络同质性（相似节点形成边的倾向）为这一问题提供了新的解决思路。

Method: 引入渐进同质性概念，建立能够容纳同质性、度异质性、稀疏性和聚类的网络形成模型，并考虑密集网络设置下通过选择连接概率较低的个体来构建估计量。

Result: 获得了对未观察变量选择具有鲁棒性的处理效应一致估计量，在应用中发现父母参与对学生考试成绩的影响估计大于OLS估计，可能是因为能够考虑未观察到的能力因素。

Conclusion: 该方法为在未观察混杂因素存在的情况下进行因果推断提供了有效框架，特别是在网络数据环境中，能够获得更准确的因果效应估计。

Abstract: This paper develops a method to conduct causal inference in the presence of
unobserved confounders by leveraging networks with homophily, a frequently
observed tendency to form edges with similar nodes. I introduce a concept of
asymptotic homophily, according to which individuals' selectivity scales with
the size of the potential connection pool. It contributes to the network
formation literature with a model that can accommodate common empirical
features such as homophily, degree heterogeneity, sparsity, and clustering, and
provides a framework to obtain consistent estimators of treatment effects that
are robust to selection on unobservables. I also consider an alternative
setting that accommodates dense networks and show how selecting linked
individuals whose observed characteristics made such a connection less likely
delivers an estimator with similar properties. In an application, I recover an
estimate of the effect of parental involvement on students' test scores that is
greater than that of OLS, arguably due to the estimator's ability to account
for unobserved ability.

</details>


### [94] [Cross-Validated Causal Inference: a Modern Method to Combine Experimental and Observational Data](https://arxiv.org/abs/2511.00727)
*Xuelin Yang,Licong Lin,Susan Athey,Michael I. Jordan,Guido W. Imbens*

Main category: econ.EM

TL;DR: 提出一种整合实验数据和观测数据进行因果推断的系统框架，通过经验风险最小化方法结合两种数据的优势。


<details>
  <summary>Details</summary>
Motivation: 随机对照试验虽然内部效度高但成本昂贵样本小，观测数据样本大但存在未测量混杂偏倚，需要结合两者优势。

Method: 将因果估计构建为经验风险最小化问题，最小化实验损失和观测损失的加权组合，通过交叉验证选择权重。

Result: 在真实和合成数据上的实验证明了方法的有效性和可靠性。

Conclusion: 该方法成功整合了实验和观测数据，提供了理论非渐近误差界，为因果推断提供了可靠工具。

Abstract: We develop new methods to integrate experimental and observational data in
causal inference. While randomized controlled trials offer strong internal
validity, they are often costly and therefore limited in sample size.
Observational data, though cheaper and often with larger sample sizes, are
prone to biases due to unmeasured confounders. To harness their complementary
strengths, we propose a systematic framework that formulates causal estimation
as an empirical risk minimization (ERM) problem. A full model containing the
causal parameter is obtained by minimizing a weighted combination of
experimental and observational losses--capturing the causal parameter's
validity and the full model's fit, respectively. The weight is chosen through
cross-validation on the causal parameter across experimental folds. Our
experiments on real and synthetic data show the efficacy and reliability of our
method. We also provide theoretical non-asymptotic error bounds.

</details>


### [95] [High-Dimensional Spatial Arbitrage Pricing Theory with Heterogeneous Interactions](https://arxiv.org/abs/2511.01271)
*Zhaoxing Gao,Sihan Tu,Ruey S. Tsay*

Main category: econ.EM

TL;DR: 提出了空间套利定价理论（SAPT）模型，整合空间交互和多因子分析，处理可观测和潜在因子。引入空间资本资产定价模型（SCAPM）并扩展到SAPT框架，提出广义收缩Yule-Walker估计方法，在高维设定下建立渐近性质。


<details>
  <summary>Details</summary>
Motivation: 将空间交互效应整合到多因子分析中，解决高维资产定价中的空间依赖性问题，扩展传统CAPM模型以更好地捕捉资产间的空间关联。

Method: 提出广义收缩Yule-Walker（SYW）估计方法，结合岭回归估计空间和因子系数。对于潜在因子，先使用自协方差特征分析提取因子，再用SYW方法。

Result: 在高维设定下建立了估计量的渐近性质，通过模拟和真实数据验证了模型和方法的有效性和实用性。

Conclusion: SAPT模型成功整合了空间效应和多因子分析，提出的估计方法在高维环境下具有良好的理论性质和实际应用价值。

Abstract: This paper investigates estimation and inference of a Spatial Arbitrage
Pricing Theory (SAPT) model that integrates spatial interactions with
multi-factor analysis, accommodating both observable and latent factors.
Building on the classical mean-variance analysis, we introduce a class of
Spatial Capital Asset Pricing Models (SCAPM) that account for spatial effects
in high-dimensional assets, where we define {\it spatial rho} as a counterpart
to market beta in CAPM. We then extend SCAPM to a general SAPT framework under
a {\it complete} market setting by incorporating multiple factors. For SAPT
with observable factors, we propose a generalized shrinkage Yule-Walker (SYW)
estimation method that integrates ridge regression to estimate spatial and
factor coefficients. When factors are latent, we first apply an
autocovariance-based eigenanalysis to extract factors, then employ the SYW
method using the estimated factors. We establish asymptotic properties for
these estimators under high-dimensional settings where both the dimension and
sample size diverge. Finally, we use simulated and real data examples to
demonstrate the efficacy and usefulness of the proposed model and method.

</details>


### [96] [Making Interpretable Discoveries from Unstructured Data: A High-Dimensional Multiple Hypothesis Testing Approach](https://arxiv.org/abs/2511.01680)
*Jacob Carlson*

Main category: econ.EM

TL;DR: 提出了一个统计原则性的框架，用于从非结构化数据中进行发现，利用机器学习可解释性方法构建概念字典，并进行选择性推断。


<details>
  <summary>Details</summary>
Motivation: 社会科学家越来越多地使用非结构化数据集来获取新的实证见解，但需要无监督分析来避免预先指定测量对象或人为限制可测量概念的范围。

Method: 利用机器学习可解释性方法将非结构化数据点映射到高维、稀疏且可解释的概念字典；计算这些字典条目的统计量；使用新开发的统计程序进行选择性推断。

Result: 该框架具有较少的研究者自由度，完全可复制，实施成本低（包括财务成本和研究时间）。

Conclusion: 该框架为从非结构化数据中进行发现提供了一种统计原则性的方法，并在实证经济学中的应用进行了探索。

Abstract: Social scientists are increasingly turning to unstructured datasets to unlock
new empirical insights, e.g., estimating causal effects on text outcomes,
measuring beliefs from open-ended survey responses. In such settings,
unsupervised analysis is often of interest, in that the researcher does not
want to pre-specify the objects of measurement or otherwise artificially
delimit the space of measurable concepts; they are interested in discovery.
This paper proposes a general and flexible framework for pursuing discovery
from unstructured data in a statistically principled way. The framework
leverages recent methods from the literature on machine learning
interpretability to map unstructured data points to high-dimensional, sparse,
and interpretable dictionaries of concepts; computes (test) statistics of these
dictionary entries; and then performs selective inference on them using newly
developed statistical procedures for high-dimensional exceedance control of the
$k$-FWER under arbitrary dependence. The proposed framework has few researcher
degrees of freedom, is fully replicable, and is cheap to implement -- both in
terms of financial cost and researcher time. Applications to recent descriptive
and causal analyses of unstructured data in empirical economics are explored.
An open source Jupyter notebook is provided for researchers to implement the
framework in their own projects.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [97] [Stochastic Optimal Control Problems for the Cost-Optimal Management of a Standalone Microgrid](https://arxiv.org/abs/2511.00167)
*Paul Honore Takam,Nathalie Fruiba*

Main category: math.OC

TL;DR: 研究独立微电网的随机最优控制问题，通过光伏发电、电池储能和本地发电机来优化成本管理，考虑天气不确定性、有限燃料和未满足需求的惩罚成本。


<details>
  <summary>Details</summary>
Motivation: 解决农村电气化中独立微电网的复杂控制问题，这些系统由于天气和环境条件的不确定性而难以管理，且燃料和电池容量有限。

Method: 将问题建模为离散时间随机控制问题，使用马尔可夫决策过程理论进行数值求解。

Result: 开发了一个数学优化框架，能够处理可再生能源发电和需求的不确定性，同时考虑有限燃料和电池容量的约束。

Conclusion: 该方法为独立微电网提供了一种有效的成本优化管理策略，特别适用于发展中国家农村地区的电气化应用。

Abstract: In this paper, we consider a domestic standalone microgrid equipped with
local renewable energy generation such as photovoltaic panels, consumption
units, and battery storage to balance supply and demand and investigate the
stochastic optimal control problem for its cost-optimal management. As a
special feature, the manager does not have access to the power grid but has a
local generator, making it possible to produce energy using fuel when needed.
Such systems are very important for rural electrification, particularly in
developing countries. However, these systems are very complex to control due to
uncertainties in the weather and environmental conditions, which affect the
energy generation and the energy demand. In addition, we assume that the
battery and the fuel tank have limited capacities and that the fuel tank can
only be filled once at the beginning of the planning period. This leads us to
the so-called finite fuel problem. In addition, we allow the energy demand to
not always be satisfied, and we impose penalties on unsatisfied demand, the
so-called discomfort cost. The main goal is to minimize the expected aggregated
cost of generating power using the generator and operating the system. This
leads to a mathematical optimization problem. The problem is formulated as a
discrete-time stochastic control problem and solved numerically using methods
from the theory of Markov decision processes.

</details>


### [98] [A Tight SDP Relaxation for the Cubic-Quartic Regularization Problem](https://arxiv.org/abs/2511.00168)
*Jinling Zhou,Xin Liu,Jiawang Nie,Xindong Tang*

Main category: math.OC

TL;DR: 本文提出了一种用于全局求解三次-四次正则化(CQR)问题的半定规划(SDP)松弛方法，证明了松弛的紧致性条件，并给出了检测紧致性和获取所有全局极小值的算法。


<details>
  <summary>Details</summary>
Motivation: CQR问题在无约束非线性优化的高效正则化方法中作为关键子问题出现，但目前仍缺乏计算其全局极小值的实用方法。

Method: 采用半定规划(SDP)松弛方法求解CQR问题，分析了松弛的紧致性条件，并提出了检测紧致性和获取全局极小值的算法。

Result: 证明了SDP松弛在特定条件下是紧致的，所有非零全局极小值在紧致情况下具有相同长度，数值实验表明该方法有效且计算高效。

Conclusion: 提出的SDP松弛方法为全局求解CQR问题提供了首个实用方法，在理论和计算上都表现出良好性能。

Abstract: This paper studies how to compute global minimizers of the cubic-quartic
regularization (CQR) problem \[ \min_{s \in \mathbb{R}^n} \quad
f_0+g^Ts+\frac{1}{2}s^THs+\frac{\beta}{6} \| s \|^3+\frac{\sigma}{4} \| s \|^4,
\] where $f_0$ is a constant, $g$ is an $n$-dimensional vector, $H$ is a
$n$-by-$n$ symmetric matrix, and $\| s \|$ denotes the Euclidean norm of $s$.
The parameter $\sigma \ge 0$ while $\beta$ can have any sign. The CQR problem
arises as a critical subproblem for getting efficient regularization methods
for solving unconstrained nonlinear optimization. Its properties are recently
well studied by Cartis and Zhu [cubic-quartic regularization models for solving
polynomial subproblems in third-order tensor methods, Math. Program, 2025].
However, a practical method for computing global minimizers of the CQR problem
still remains elusive. To this end, we propose a semidefinite programming (SDP)
relaxation method for solving the CQR problem globally. First, we show that our
SDP relaxation is tight if and only if $\| s^* \| ( \beta + 3 \sigma \| s^* \|)
\ge 0$ holds for a global minimizer $s^*$. In particular, if either $\beta \ge
0$ or $H$ has a nonpositive eigenvalue, then the SDP relaxation is shown to be
tight. Second, we show that all nonzero global minimizers have the same length
for the tight case. Third, we give an algorithm to detect tightness and to
obtain the set of all global minimizers. Numerical experiments demonstrate that
our SDP relaxation method is both effective and computationally efficient,
providing the first practical method for globally solving the CQR problem.

</details>


### [99] [SHAP values through General Fourier Representations: Theory and Applications](https://arxiv.org/abs/2511.00185)
*Roberto Morales*

Main category: math.OC

TL;DR: 该论文为SHAP值建立了严格的谱分析框架，证明了在离散或多值输入空间上的预测模型可以通过正交化张量积基进行广义傅里叶展开，并将SHAP归因表示为模型傅里叶系数的线性泛函。


<details>
  <summary>Details</summary>
Motivation: 为SHAP值提供严格的数学分析框架，解决当前SHAP解释方法缺乏理论保证的问题，建立SHAP值与模型频谱特性之间的数学联系。

Method: 构建基于乘积概率测度的正交化张量积基，进行广义傅里叶展开；在确定性机制下推导SHAP值的稳定性估计，在概率机制下分析无限宽度神经网络中SHAP值的收敛性。

Result: 证明了SHAP归因映射相对于预测器距离具有Lipschitz连续性；在无限宽度极限下，SHAP值收敛于对应高斯过程先验诱导的值，并给出了期望和概率意义下的显式误差界。

Conclusion: 该谱框架为SHAP值提供了坚实的数学基础，建立了SHAP归因与模型频谱特性之间的理论联系，为可解释AI提供了严格的分析工具。

Abstract: This article establishes a rigorous spectral framework for the mathematical
analysis of SHAP values. We show that any predictive model defined on a
discrete or multi-valued input space admits a generalized Fourier expansion
with respect to an orthonormalisation tensor-product basis constructed under a
product probability measure. Within this setting, each SHAP attribution can be
represented as a linear functional of the model's Fourier coefficients.
  Two complementary regimes are studied. In the deterministic regime, we derive
quantitative stability estimates for SHAP values under Fourier truncation,
showing that the attribution map is Lipschitz continuous with respect to the
distance between predictors. In the probabilistic regime, we consider neural
networks in their infinite-width limit and prove convergence of SHAP values
toward those induced by the corresponding Gaussian process prior, with explicit
error bounds in expectation and with high probability based on concentration
inequalities.
  We also provide a numerical experiment on a clinical unbalanced dataset to
validate the theoretical findings.

</details>


### [100] [Multivariable Gradient-Based Extremum Seeking Control with Saturation Constraints](https://arxiv.org/abs/2511.00208)
*Enzo Ferreira Tomaz Silva,Pedro Henrique Silva Coutinho,Tiago Roux Oliveira,Miroslav Krstić,Sophie Tarbouriech*

Main category: math.OC

TL;DR: 该论文研究了带饱和约束的多变量梯度极值搜索控制，提出了两种饱和场景的解决方案：通过抗饱和补偿处理优化函数输入饱和，以及处理梯度估计饱和。使用多面体不确定性描述未知Hessian矩阵，并基于LMI设计稳定控制增益。


<details>
  <summary>Details</summary>
Motivation: 传统极值搜索控制通常假设无饱和约束，但在实际系统中输入和梯度估计都可能饱和，这会影响系统稳定性和收敛性能。需要开发能够处理饱和约束的稳健ESC设计方法。

Method: 采用多面体不确定性描述未知Hessian矩阵，推导出线性矩阵不等式形式的充分条件来设计稳定控制增益。使用抗饱和补偿策略处理输入饱和，并通过平均理论严格证明带Lipschitz连续右端的动力系统的稳定性。

Result: 提出的设计条件能够获得非对角控制增益矩阵，推广了传统对角结构的ESC设计。数值仿真验证了所提ESC算法的有效性，即使在饱和情况下也能保证收敛。

Conclusion: 该研究为多变量梯度极值搜索控制提供了处理饱和约束的系统性方法，通过LMI条件和平均理论保证了系统的指数稳定性，扩展了ESC在实际饱和系统中的应用范围。

Abstract: This paper addresses the multivariable gradient-based extremum seeking
control (ESC) subject to saturation. Two distinct saturation scenarios are
investigated here: saturation acting on the input of the function to be
optimized, which is addressed using an anti-windup compensation strategy, and
saturation affecting the gradient estimate. In both cases, the unknown Hessian
matrix is represented using a polytopic uncertainty description, and sufficient
conditions in the form of linear matrix inequalities (LMIs) are derived to
design a stabilizing control gain. The proposed conditions guarantee
exponential stability of the origin for the average closed-loop system under
saturation constraints. With the proposed design conditions, non-diagonal
control gain matrices can be obtained, generalizing conventional ESC designs
that typically rely on diagonal structures. Stability and convergence are
rigorously proven using the Averaging Theory for dynamical systems with
Lipschitz continuous right-hand sides. Numerical simulations illustrate the
effectiveness of the proposed ESC algorithms, confirming the convergence even
in the presence of saturation.

</details>


### [101] [A non--exchangeable mean field control problem with controlled interactions](https://arxiv.org/abs/2511.00288)
*Mao Fabrice Djete*

Main category: math.OC

TL;DR: 本文提出了一种新的均值场控制问题，其中智能体通过固定但可控的网络结构进行交互，将交互结构本身作为控制变量进行优化。


<details>
  <summary>Details</summary>
Motivation: 传统均值场控制假设智能体可交换且通过对称经验分布交互，无法处理异构和非对称交互模式。本文旨在扩展均值场控制理论，使其适用于非交换群体和受控网络交互。

Method: 开发了适用于该设置的广义松弛（随机化）控制概念，证明了其与强公式的等价性，并在最小正则性假设下建立了相关值函数的存在性、紧致性和连续性结果。

Result: 证明了当固定步核在割范数下收敛时，具有一般（可能非对称）交互矩阵的有限n-智能体控制问题收敛到均值场极限，且最优值和控制策略具有渐近一致性。

Conclusion: 为将交互结构本身视为控制对象并进行优化的严格框架，扩展了均值场控制理论到非交换群体和受控网络交互的应用范围。

Abstract: This paper introduces and analyzes a new class of mean--field control
(\textsc{MFC}) problems in which agents interact through a \emph{fixed but
controllable} network structure. In contrast with the classical \textsc{MFC}
framework -- where agents are exchangeable and interact only through symmetric
empirical distributions -- we consider systems with heterogeneous and possibly
asymmetric interaction patterns encoded by a structural kernel, typically of
graphon type. A key novelty of our approach is that this interaction structure
is no longer static: it becomes a genuine \emph{control variable}. The planner
therefore optimizes simultaneously two distinct components: a \emph{regular
control}, which governs the local dynamics of individual agents, and an
\emph{interaction control}, which shapes the way agents connect and influence
each other through the fixed structural kernel.
  \medskip We develop a generalized notion of relaxed (randomized) control
adapted to this setting, prove its equivalence with the strong formulation, and
establish existence, compactness, and continuity results for the associated
value function under minimal regularity assumptions. Moreover, we show that the
finite $n$--agent control problems with general (possibly asymmetric)
interaction matrices converge to the mean--field limit when the corresponding
fixed step--kernels converge in cut--norm, with asymptotic consistency of the
optimal values and control strategies. Our results provide a rigorous framework
in which the \emph{interaction structure itself is viewed and optimized as a
control object}, thereby extending mean--field control theory to
non--exchangeable populations and controlled network interactions.

</details>


### [102] [A Finite Dominating Set Approach for the Multi-Item Multi-Period Order Allocation Problem under All-Unit Quantity Discounts and Blending Ratios](https://arxiv.org/abs/2511.00300)
*Fuhad Ahmed Opu,Moddassir Khan Nayeem,Hamid Najafzad,Omar Abbaas*

Main category: math.OC

TL;DR: 提出一种解决多物品多周期订单分配问题的方法，考虑全单位数量折扣和混合比例约束，通过生成有限支配集保证最优解，大幅减少计算时间。


<details>
  <summary>Details</summary>
Motivation: 制造商需要混合多种原料生产单一产品，供应商提供基于数量的折扣，这给采购决策带来非线性复杂性，需要高效的最优解决方案。

Method: 开发系统生成有限支配集(FDS)的求解程序，基于FDS构建混合整数线性规划(MILP)模型，避免非线性折扣建模。

Result: 数值实验显示，相比传统方法，MILP模型能获得最优解且计算时间减少高达99%，尤其在大规模实例中表现优异。

Conclusion: 该方法保证最优性，显著提升计算效率，并能动态适应持有成本变化，识别成本敏感原料，为采购决策提供有效支持。

Abstract: This study addresses the multi-item multi-period order allocation problem
under all-unit quantity discounts (AUQD) and blending ratios. A manufacturer
makes a single product that requires mixing/assembling multiple
ingredients/components with pre-determined blending ratios. We consider a
single supplier offering quantity-based discounts which introduces
non-linearities to the problem. The objective is to minimize procurement cost
which includes purchasing, inventory, and ordering costs. We develop a solution
procedure that systematically generates a finite dominating set (FDS) of order
quantities guaranteed to include an optimal solution to the problem. A Mixed
Integer Linear Programming (MILP) model based on the FDS. Our procedure
guarantees optimality and eliminates the need for nonlinear discount modeling.
Numerical experiments demonstrate that the proposed MILP achieves optimal
solutions with significantly reduced computational effort, up to 99% faster for
large-scale instances compared to conventional formulations. Sensitivity
analyses reveal that the model dynamically adapts to changes in holding costs,
shifting between bulk-purchasing and just-in-time strategies, and identifying
cost-sensitive ingredients that drive total system cost.

</details>


### [103] [Transit-MP: Transit-Prioritized Max-Pressure Control in Sparse Connected Vehicle Environments](https://arxiv.org/abs/2511.00309)
*Chaopeng Tan,Hao Liu,Dingshan Sun,Marco Rinaldi,Hans van Lint*

Main category: math.OC

TL;DR: 提出了Transit-MP和mTransit-MP方法，在部分网联车环境下实现公交优先的max-pressure控制，解决了传统MP方法对公共交通考虑不足和网络稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 现有max-pressure控制方法对公共交通考虑有限，且在部分网联车环境下无法保证公交优先MP的网络稳定性，同时存在队列饥饿现象问题。

Method: Transit-MP通过考虑实时车辆载客量和公交站点停靠影响来实现公交优先控制；mTransit-MP进一步结合历史交通数据来解决队列饥饿现象。

Result: 在阿姆斯特丹真实走廊上的实验显示，相比最先进的MP控制器，该方法显著减少了实时车辆和溢出数量，改善了私家车和公交车的延误。在稀疏网联车环境下，mTransit-MP有效缓解了链路溢出问题。

Conclusion: 提出的Transit-MP和mTransit-MP方法在部分网联车环境下能够实现公交优先的稳定网络控制，有效解决队列饥饿现象，提升多模式交通整体性能。

Abstract: Max-pressure (MP) control stands out among real-time network traffic signal
control methods due to its simplicity, decentralized nature, and theoretical
stability. However, existing MP control methods have limited consideration of
public transportation and do not address the network stability problem of
transit-prioritized MP in partially connected vehicle (CV) environments. In
this study, we propose Transit-MP, which realizes transit-prioritized MP
control in partially CV environments by considering real-time vehicle occupancy
and the impact of transit dwell at stations. Theoretically, we demonstrate that
Transit-MP, while using different traffic state measures for upstream and
downstream links for pressure calculation, still achieves road network
stability even in partially CV environments. Note that for MP controllers in
sparse CV environments, some movements may have missing CV observations,
leading to link spillovers, which create the queue starvation phenomenon: a
movement no longer receives the green phase despite the queue spillover.
Therefore, we further propose a modified Transit-MP (mTransit-MP) that
incorporates historical traffic data to address this issue. We rigorously prove
that the proposed mTransit-MP can effectively avoid the queue starvation
phenomenon. Experimental results on a real-world corridor in Amsterdam with 15
transit lines and 31 stations show that our method significantly reduces the
real-time vehicle and spillover count, and improves delays for both private
vehicles and transit vehicles compared to a state-of-the-art MP controller for
transit signal priority. In sparse CV environments, our mTransit-MP is
effective in mitigating link spillovers while enhancing the overall performance
of multi-modal traffic.

</details>


### [104] [Accelerated primal dual fixed point algorithm](https://arxiv.org/abs/2511.00385)
*Ya-Nan Zhu*

Main category: math.OC

TL;DR: 提出了一种加速原始对偶定点方法(APDFP)，用于求解复合优化问题min f(x) + g(Bx)，其中g是非光滑函数，B是线性算子。该方法采用Nesterov型加速技术，具有完全解耦的迭代步骤。


<details>
  <summary>Details</summary>
Motivation: 为了解决复合优化问题min f(x) + g(Bx)的收敛速度问题，特别是在B为非单位矩阵的情况下，需要一种高效的加速算法来提升收敛性能。

Method: APDFP方法结合了Nesterov加速技术和原始对偶定点框架，实现了完全解耦的迭代过程，可以看作是Nesterov加速梯度方法在B为非单位矩阵情况下的推广。

Result: 理论分析表明，该方法将部分原始对偶间隙的收敛速度从O(1/k)提升到O(1/k^2)。数值实验在图引导逻辑回归和CT图像重建问题上验证了方法的正确性和效率。

Conclusion: APDFP方法在复合优化问题中实现了更快的收敛速度，为处理非单位线性算子的优化问题提供了有效的解决方案。

Abstract: This work proposes an Accelerated Primal-Dual Fixed-Point (APDFP) method that
employs Nesterov type acceleration to solve composite problems of the form min
f(x) + g(Bx), where g is nonsmooth and B is a linear operator. The APDFP
features fully decoupled iterations and can be regarded as a generalization of
Nesterov's accelerated gradient in the setting where B can be a non-identity
matrix. Theoretically, we improve the convergence rate of the partial
primal-dual gap with respect to the Lipschitz constant of the gradient of f
from O(1/k) to O(1/k^2). Numerical experiments on graph-guided logistic
regression and CT image reconstruction are conducted to validate the
correctness and demonstrate the efficiency of the proposed method.

</details>


### [105] [On the Convexification of a Class of Mixed-Integer Conic Sets](https://arxiv.org/abs/2511.00452)
*Guxin Du,Rui Chen,Linchuan Wei*

Main category: math.OC

TL;DR: 研究混合整数二阶锥约束中非线性右侧的凸包特性，提出通过凹包替换来精确描述凸包，为MIQCP提供强松弛方法


<details>
  <summary>Details</summary>
Motivation: 混合整数二次约束规划中经常出现具有非线性右侧的二阶锥约束，需要研究其凸包特性以提供强松弛

Method: 在温和假设下，用凹包替换SOC约束的右侧来精确描述凸包，通过重构和割平面实现强松弛

Result: 计算实验表明该方法在分布鲁棒机会约束背包问题变种中有效

Conclusion: 凹包替换方法能精确描述混合整数SOC集的凸包，为MIQCP提供有效的强松弛技术

Abstract: We investigate mixed-integer second-order conic (SOC) sets with a nonlinear
right-hand side in the SOC constraint, a structure frequently arising in
mixed-integer quadratically constrained programming (MIQCP). Under mild
assumptions, we show that the convex hull can be exactly described by replacing
the right-hand side with its concave envelope. This characterization enables
strong relaxations for MIQCPs via reformulations and cutting planes.
Computational experiments on distributionally robust chance-constrained
knapsack variants demonstrate the efficacy of our reformulation techniques.

</details>


### [106] [Optimization of continuous-flow over traffic networks with fundamental diagram constraints](https://arxiv.org/abs/2511.00500)
*Anqi Dong,Karl Henrik Johansson,Johan Karlsson*

Main category: math.OC

TL;DR: 该论文将基本图约束引入图上的动态连续流最优输运模型，解决了经典最优输运理论忽略容量限制的问题，特别适用于交通流建模。


<details>
  <summary>Details</summary>
Motivation: 经典最优输运理论在建模质量移动时忽略了应用中的容量限制，特别是在交通流问题中。作者旨在通过结合基本图来克服这一限制，将局部密度与最大通量之间的经验关系纳入模型。

Method: 采用欧拉动力学作用在图上的方法，保持位移插值与连续理论的直接类比。动量位于边上，密度位于节点上，模拟道路网络语义。开发了凸变分离散化方法，得到基本图约束的最优输运问题。

Result: 建立了存在源和汇的最优流的存在性和唯一性，并开发了高效的凸优化方法。数值研究从单车道线网络扩展到城市级道路网络。

Conclusion: 提出的基本图约束最优输运模型能够保持质量守恒，并在道路网络上产生考虑拥堵的最优交通流，为交通流建模提供了更实用的框架。

Abstract: Optimal transport (OT) theory provides a principled framework for modeling
mass movement in applications such as mobility, logistics, and economics.
Classical formulations, however, generally ignore capacity limits that are
intrinsic in applications, in particular in traffic flow problems. We address
this limitation by incorporating fundamental diagrams into a dynamic
continuous-flow OT model on graphs, thereby including empirical relations
between local density and maximal flux. We adopt an Eulerian kinetic action on
graphs that preserves displacement interpolation in direct analogy with the
continuous theory. Momentum lives on edges and density on nodes, mirroring
road-network semantics in which segments carry speed and intersections store
mass. The resulting fundamental-diagram-constrained OT problem preserves mass
conservation and admits a convex variational discretization, yielding optimal
congestion-aware traffic flow over road networks. We establish the existence
and uniqueness of the optimal flow with sources and sinks, and develop an
efficient convex optimization method. Numerical studies begin with a
single-lane line network and scale to a city-level road network.

</details>


### [107] [Cutting plane methods with gradient-based heuristics](https://arxiv.org/abs/2511.00520)
*Hòa T. Bùi,Alberto De Marchi*

Main category: math.OC

TL;DR: 提出了一种混合方法，将割平面法的全局收敛保证与一阶优化技术的局部探索能力相结合，通过在更接近最优解集的点构造割平面来提高非线性离散优化的计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有割平面方法虽然理论上强大，但计算性能波动较大。研究表明在不可行点投影到可行集上构造割平面能提升性能，因此探索在更接近最优解集的可行点构造割平面是否能够进一步改进割平面方法的效果。

Method: 结合割平面法和一阶优化技术，使用投影梯度法作为启发式方法识别有前景的解空间区域，并通过混合整数线性规划高效计算投影操作，在更接近最优解集的点构造更紧、信息更丰富的割平面。

Result: 通过在更接近最优解集的点构造割平面并消除非最优区域，算法实现了对可行区域的更好近似和更快收敛。数值实验证实该方法在不同求解器配置下都提高了解质量和计算效率。

Conclusion: 该框架为扩展到更一般的离散领域提供了灵活基础，并为非线性离散优化工具包提供了一个有前景的启发式方法。

Abstract: Cutting plane methods, particularly outer approximation, are a
well-established approach for solving nonlinear discrete optimization problems
without relaxing the integrality of decision variables. While powerful in
theory, their computational performance can be highly variable. Recent research
has shown that constructing cutting planes at the projection of infeasible
points onto the feasible set can significantly improve the performance of
cutting plane approaches. Motivated by this, we examine whether constructing
cuts at feasible points closer to the optimal solution set could further
enhance the effectiveness of cutting plane methods. We propose a hybrid method
that combines the global convergence guarantees of cutting plane methods with
the local exploration capabilities of first-order optimization techniques.
Specifically, we use projected gradient methods as a heuristic to identify
promising regions of the solution space and generate tighter, more informative
cuts. We focus on binary optimization problems with convex differentiable
objective functions, where projection operations can be efficiently computed
via mixed-integer linear programming. By constructing cuts at points closer to
the optimal solution set and eliminating non-optimal regions, the algorithm
achieves better approximation of the feasible region and faster convergence.
Numerical experiments confirm that our approach improves both the quality of
the solution and computational efficiency across different solver
configurations. This framework provides a flexible foundation for further
extensions to more general discrete domains and offers a promising heuristic to
the toolkit for nonlinear discrete optimization.

</details>


### [108] [Evolving School Transport Electrification: Integrated Dynamic Route Optimization and Partial Charging for Mixed Fleets](https://arxiv.org/abs/2511.00600)
*Megh Bahadur KC,Ziqi Song*

Main category: math.OC

TL;DR: 本研究提出了一种同时解决电动校车动态路线规划和部分充电调度的新方法，考虑了学生需求变化、车辆容量、最大行驶时间、停靠时间窗口和混合车队等实际场景，通过线性化混合整数规划模型实现成本节约和效率提升。


<details>
  <summary>Details</summary>
Motivation: 校车运输是美国最大的公共交通车队，在交通脱碳中发挥重要作用，因此有效规划电动校车路线和充电调度至关重要。

Method: 构建了线性的混合整数规划模型，适用于同质和异质车队，包含完全和部分充电策略，考虑了电池容量、充电基础设施位置等约束条件。

Result: 使用异质车队可以节省成本、减少路线距离和行驶时间，部分充电和优化行驶时间对校车路线有益，相比现状可减少高达56.4%的行驶距离。

Conclusion: 所提出的优化方法支持校车电气化，联邦和州政府的额外投资补贴可加速电动校车推广。

Abstract: School bus transportation, the largest fleet size for public transportation
in the US, plays a significant role in sustainability through transport
decarbonization. Thus, effective planning of electric school bus routes and
recharge schedules is vital. This study proposes a novel approach that
simultaneously addresses electric school bus dynamic routing and partial charge
scheduling, considering practical scenarios such as varying student demands,
bus capacities, maximum ride time, stop time window, and fleet mixes. The model
incorporates constraints like bell time tolerance and battery capacity and
charging infrastructure candidate location, making it robust for school bus
electrification. A linearized Mixed Integer Programming (MIP) model for
homogeneous and heterogeneous fleets with full and partial recharging
strategies is formulated. The proposed objective function for nonlinear and
linear models is executed and compared for computational effectiveness. The
model is tested on various sizes of school networks using modified benchmark
instances, and a real-world case study demonstrates the benefits of electrified
school transportation. The results show that employing heterogeneous fleets can
lead to cost savings, reduced routing distance, and travel time for both the
tested networks and the case study. Sensitivity analyses highlight the
trade-offs between battery size and total cost. Furthermore, the benefits of
partial charging and optimum riding time for school bus routes are suggested.
The proposed optimization approach can achieve significant reductions in travel
distance, up to 56.4% compared to the current situation and fleet size,
supporting the case for school transport electrification. Potential additional
investment subsidies from federal and state governments are added benefits for
accelerated school bus electrification.

</details>


### [109] [RNN-based linear parameter varying adaptive model predictive control for autonomous driving](https://arxiv.org/abs/2511.00610)
*Yassine Kebbati,Naima Ait-Oufroukh,Dalil Ichalal,Vincent Vigneron*

Main category: math.OC

TL;DR: 提出了一种自适应线性参数变化模型预测控制器（LPV-MPC），通过递归神经网络使预测模型自适应，并使用混合遗传和粒子群优化算法（GA-PSO）进一步优化控制器。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶是一个复杂且高度动态的过程，需要控制耦合的纵向和横向车辆动力学。模型预测控制因其预测特性、最优性能和约束处理能力而成为最有前景的控制工具之一。

Method: 开发自适应LPV-MPC控制器，其中预测模型通过递归神经网络实现自适应，并使用GA-PSO混合优化算法对控制器进行优化。

Result: 在具有可变风扰动的挑战性赛道上对开发的控制器进行了测试和评估。

Conclusion: 该方法为自动驾驶控制提供了一种有效的解决方案，结合了模型预测控制的优势和神经网络的自适应能力。

Abstract: Autonomous driving is a complex and highly dynamic process that ensures
controlling the coupled longitudinal and lateral vehicle dynamics. Model
predictive control, distinguished by its predictive feature, optimal
performance, and ability to handle constraints, makes it one of the most
promising tools for this type of control application. The content of this
article handles the problem of autonomous driving by proposing an adaptive
linear parameter varying model predictive controller (LPV-MPC), where the
controller's prediction model is adaptive by means of a recurrent neural
network. The proposed LPV-MPC is further optimised by a hybrid Genetic and
Particle Swarm Optimization Algorithm (GA-PSO). The developed controller is
tested and evaluated on a challenging track under variable wind disturbance.
Code can be found here :
https://github.com/yassinekebbati/GA-PSO-optimized-RNN-MPC

</details>


### [110] [From Generality to Specificity: Prior-Driven Optimal Sparse Transformation in Compressed Sensing](https://arxiv.org/abs/2511.00611)
*Zhihan Zhu,Yanhao Zhang,Yong Xia*

Main category: math.OC

TL;DR: 提出了POST稀疏变换框架，统一现有变换域的泛化能力与参考知识特异性，并推导出HOT变换域，在音频感知、5G信道估计和图像压缩任务中显著提升压缩感知算法性能。


<details>
  <summary>Details</summary>
Motivation: 克服经典稀疏变换在压缩感知中泛化能力和特异性之间的长期限制，实现灵活适应不同信号特性的稀疏变换。

Method: 建立先验到后验稀疏变换(POST)框架，统一现有变换域的泛化能力与参考知识特异性，推导出HOT变换域处理实值和复值信号。

Result: 理论证明HOT在单参考和多参考设置下的稀疏表示特性，实验验证在音频感知、5G信道估计和图像压缩任务中显著提升多种压缩感知算法性能，计算开销可忽略。

Conclusion: POST框架成功解决了稀疏变换中泛化与特异性的权衡问题，HOT变换在多种应用中实现了显著的性能提升。

Abstract: This paper introduces a new paradigm for sparse transformation: the
Prior-to-Posterior Sparse Transform (POST) framework, designed to overcome
long-standing limitation on generalization and specificity in classical sparse
transforms for compressed sensing. POST systematically unifies the
generalization capacity of any existing transform domains with the specificity
of reference knowledge, enabling flexible adaptation to diverse signal
characteristics. Within this framework, we derive an explicit sparse transform
domain termed HOT, which adaptively handles both real and complex-valued
signals. We theoretically establish HOT's sparse representation properties
under single and multiple reference settings, demonstrating its ability to
preserve generalization while enhancing specificity even under weak reference
information. Extensive experiments confirm that HOT delivers substantial
meta-gains across audio sensing, 5G channel estimation, and image compression
tasks, consistently boosting multiple compressed sensing algorithms under
diverse multimodal settings with negligible computational overhead.

</details>


### [111] [Isotropic Curvature Model for Understanding Deep Learning Optimization: Is Gradient Orthogonalization Optimal?](https://arxiv.org/abs/2511.00674)
*Weijie Su*

Main category: math.OC

TL;DR: 提出各向同性曲率模型来分析深度学习优化，发现Muon优化器等矩阵梯度方法通过使梯度矩阵谱更均匀来改善条件数，但梯度正交化可能不是严格最优的。


<details>
  <summary>Details</summary>
Motivation: 利用权重矩阵结构分析深度学习单次迭代优化，理解权重更新如何影响损失函数变化。

Method: 建立各向同性曲率模型，假设损失函数在所有扰动方向上具有各向同性的二阶Hessian和高阶曲率项，形成可分析的凸优化问题。

Result: 在一般曲率增长条件下，最优更新矩阵使梯度矩阵谱更均匀；当曲率出现相变时，正交化梯度成为最优解。

Conclusion: Muon等方法中的梯度正交化方向正确但非严格最优，各向同性曲率模型可为设计新优化方法提供指导。

Abstract: In this paper, we introduce a model for analyzing deep learning optimization
over a single iteration by leveraging the matrix structure of the weights. We
derive the model by assuming isotropy of curvature, including the second-order
Hessian and higher-order terms, of the loss function across all perturbation
directions; hence, we call it the isotropic curvature model. This model is a
convex optimization program amenable to analysis, which allows us to understand
how an update on the weights in the form of a matrix relates to the change in
the total loss function. As an application, we use the isotropic curvature
model to analyze the recently introduced Muon optimizer and other
matrix-gradient methods for training language models. First, we show that under
a general growth condition on the curvature, the optimal update matrix is
obtained by making the spectrum of the original gradient matrix more
homogeneous -- that is, making its singular values closer in ratio -- which in
particular improves the conditioning of the update matrix. Next, we show that
the orthogonalized gradient becomes optimal for the isotropic curvature model
when the curvature exhibits a phase transition in growth. Taken together, these
results suggest that the gradient orthogonalization employed in Muon and other
related methods is directionally correct but may not be strictly optimal.
Finally, we discuss future research on how to leverage the isotropic curvature
model for designing new optimization methods for training deep learning and
language models.

</details>


### [112] [Accelerating Trust-Region Methods: An Attempt to Balance Global and Local Efficiency](https://arxiv.org/abs/2511.00680)
*Yuntian Jiang,Chuwen Zhang,Bo Jiang,Yinyu Ye*

Main category: math.OC

TL;DR: 该论文提出了加速信赖域方法，通过局部检测技术平衡全局和局部收敛性能，实现了O(ε^{-1/3})的全局复杂度和二次局部收敛。


<details>
  <summary>Details</summary>
Motivation: 传统二阶优化方法难以平衡全局和局部效率：牛顿法局部收敛好但缺乏全局保证，加速二阶方法有全局保证但局部收敛慢。

Method: 提出加速信赖域方法，利用拉格朗日乘子进行局部检测，并开发了加速信赖域外梯度方法。

Result: 实现了O(ε^{-1/3})的全局复杂度同时保持二次局部收敛；加速版本达到O(ε^{-2/7})但失去二次局部收敛。

Conclusion: 加速信赖域方法存在相变：适度全局加速可保持优秀局部收敛，但追求极端全局效率时局部收敛会失效。

Abstract: Historically speaking, it is hard to balance the global and local efficiency
of second-order optimization algorithms. For instance, the classical Newton's
method possesses excellent local convergence but lacks global guarantees, often
exhibiting divergence when the starting point is far from the optimal
solution~\cite{more1982newton,dennis1996numerical}. In contrast, accelerated
second-order methods offer strong global convergence guarantees, yet they tend
to converge with slower local
rate~\cite{carmon2022optimal,chen2022accelerating,jiang2020unified}. Existing
second-order methods struggle to balance global and local performance, leaving
open the question of how much we can globally accelerate the second-order
methods while maintaining excellent local convergence guarantee. In this paper,
we tackle this challenge by proposing for the first time the accelerated
trust-region-type methods, and leveraging their unique primal-dual information.
Our primary technical contribution is \emph{Accelerating with Local Detection},
which utilizes the Lagrange multiplier to detect local regions and achieves a
global complexity of $\tilde{O}(\epsilon^{-1/3})$, while maintaining quadratic
local convergence. We further explore the trade-off when pushing the global
convergence to the limit. In particular, we propose the \emph{Accelerated
Trust-Region Extragradient Method} that has a global near-optimal rate of
$\tilde{O}(\epsilon^{-2/7})$ but loses the quadratic local convergence. This
reveals a phase transition in accelerated trust-region type methods: the
excellent local convergence can be maintained when achieving a moderate global
acceleration but becomes invalid when pursuing the extreme global efficiency.
Numerical experiments further confirm the results indicated by our convergence
analysis.

</details>


### [113] [Projected Subgradient Ascent for Convex Maximization](https://arxiv.org/abs/2511.00741)
*Pedro Felzenszwalb,Heon Lee*

Main category: math.OC

TL;DR: 本文提出了一种新的凸函数最大化方法，对于线性最大化问题，单次正交投影即可获得近似解；对于一般凸函数，证明了大步长投影次梯度上升法能收敛到一阶稳定点。


<details>
  <summary>Details</summary>
Motivation: 传统方法使用迭代方案解决凸函数最大化问题，本文旨在探索更高效的求解方法，特别是通过单次投影或大步长策略来简化计算过程。

Method: 对于线性最大化使用单次正交投影；对于一般凸函数使用投影次梯度上升法，并分析步长趋于无穷时的收敛性，此时退化为条件梯度算法。

Result: 理论证明单次投影对线性最大化有效，大步长投影次梯度上升法能收敛到一阶稳定点；数值实验在椭球体上验证了单次投影方法的有效性。

Conclusion: 本文为凸函数最大化问题提供了更高效的求解框架，特别强调了单次投影和大步长策略的理论保证和实际应用价值。

Abstract: We consider the problem of maximizing a convex function over a closed convex
set. Classical methods solve such problems using iterative schemes that
repeatedly improve a solution. For linear maximization, we show that a single
orthogonal projection suffices to obtain an approximate solution. For general
convex functions over convex sets, we show that projected subgradient ascent
converges to a first-order stationary point when using arbitrarily large step
sizes. Taking the step size to infinity leads to the conditional gradient
algorithm, and iterated linear optimization as a special case. We illustrate
numerical experiments using a single projection for linear optimization in the
elliptope, reducing the problem to the computation of a nearest correlation
matrix.

</details>


### [114] [Model-free source seeking of exponentially convergent unicycle: theoretical and robotic experimental results](https://arxiv.org/abs/2511.00752)
*Rohan Palanikumar,Ahmed A. Elgohary,Victoria Grushkovskaya,Sameh A. Eisa*

Main category: math.OC

TL;DR: 提出了一种无模型、实时的单轮车源追踪设计，能够自主引导单轮车动态系统向未知表达式的目标函数极值点收敛，支持高阶多项式目标函数。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常只适用于局部二次目标函数，本文旨在解决高阶多项式目标函数的极值追踪问题，并首次提供实验验证。

Method: 采用模型无关的实时设计，通过测量获取目标函数值，引导单轮车系统向极值点收敛。

Result: 理论分析和仿真结果表明，该设计能够指数收敛到高阶多项式目标函数的极值点，并首次通过机器人实验验证了有效性。

Conclusion: 该设计成功扩展了源追踪方法的应用范围，支持高阶多项式目标函数，并通过实验证明了其指数收敛性能。

Abstract: This paper introduces a novel model-free, real-time unicycle-based source
seeking design. This design steers autonomously the unicycle dynamic system
towards the extremum point of an objective function or physical/scaler signal
that is unknown expression-wise, but accessible via measurements. A key
contribution of this paper is that the introduced design converges
exponentially to the extremum point of objective functions (or scaler signals)
that behave locally like a higher-degree power functions (e.g., fourth degree
polynomial function) as opposed to locally quadratic objective functions, the
usual case in literature. We provide theoretical and simulation results to
support out theoretical results. Also, for the first time in the literature, we
provide experimental robotic results that demonstrate the effectiveness of the
proposed design and its exponential convergence ability.

</details>


### [115] [Remarks on the paper "Treatment of Set-Valued Robustness via Separation and Scalarization"](https://arxiv.org/abs/2511.00928)
*Abhik Digar,Kuntal Som*

Main category: math.OC

TL;DR: 本文对已发表的关于集合值鲁棒优化问题的论文[1]进行了评述，指出了其中结果的不一致性，改进了大部分结果，并引入了新的鲁棒解概念和标量化方法。


<details>
  <summary>Details</summary>
Motivation: 原论文[1]在处理不确定约束集合值优化问题的鲁棒解时存在许多不一致的结果，需要对这些结果进行修正和改进。

Method: 通过引入新的鲁棒解概念，改进标量化方法，并对原论文中的结果进行系统性修正。

Result: 成功识别并改进了原论文中的不一致结果，提出了更完善的鲁棒解概念和标量化方法。

Conclusion: 本文对集合值鲁棒优化问题的研究做出了重要贡献，修正了现有文献中的错误，并提出了更可靠的理论框架和方法。

Abstract: In this paper, we remark on the published paper "Treatment of Set-Valued
Robustness via Separation and Scalarization" [1], which deals with the robust
solution to an uncertain constrained set-valued optimization problem via
scalarization methods. We show many inconsistencies in the results of the
above-mentioned paper. We improve most of these results. In the process, we
introduce some new concepts of robust solutions for uncertain set-valued
optimization problems. We also improve some results on scalarization methods
applicable to set-valued optimization.

</details>


### [116] [Parallel KKT Solver in PIQP for Multistage Optimization](https://arxiv.org/abs/2511.00946)
*Fenglong Song,Roland Schwan,Yuwen Chen,Colin N. Jones*

Main category: math.OC

TL;DR: 提出了一种用于多阶段优化问题中KKT系统的高效并行Cholesky分解和三角求解算法，特别针对模型预测控制和赛车轨迹优化应用。


<details>
  <summary>Details</summary>
Motivation: 多阶段优化问题中的KKT系统求解是计算瓶颈，特别是在模型预测控制和轨迹优化等实时应用中需要高效求解。

Method: 在代数层面直接并行化求解具有块三对角箭头结构的KKT矩阵，该方法作为PIQP求解器的新后端实现并开源。

Result: 在链式质量基准测试和最小曲率赛车线优化问题上的数值实验表明，相比其他最先进求解器有显著性能提升。

Conclusion: 提出的并行算法能有效加速多阶段优化问题中KKT系统的求解，特别适用于实时控制应用。

Abstract: This paper presents an efficient parallel Cholesky factorization and
triangular solve algorithm for the Karush-Kuhn-Tucker (KKT) systems arising in
multistage optimization problems, with a focus on model predictive control and
trajectory optimization for racing. The proposed approach directly parallelizes
solving the KKT systems with block-tridiagonal-arrow KKT matrices on the linear
algebra level arising in interior-point methods. The algorithm is implemented
as a new backend of the PIQP solver and released as open source. Numerical
experiments on the chain-of-masses benchmarks and a minimum curvature race line
optimization problem demonstrate substantial performance gains compared to
other state-of-the-art solvers.

</details>


### [117] [Dynamic Nash Equilibrium Seeking for a Class of Nonlinear Uncertain Multi-agent Systems](https://arxiv.org/abs/2511.01002)
*Weijian Li,Yutao Tang*

Main category: math.OC

TL;DR: 提出了一种分布式控制框架，将单调博弈的纳什均衡求解问题转化为增强系统的鲁棒镇定问题，通过虚拟单积分器多智能体系统和内模设计实现了完全分布式的纳什均衡求解。


<details>
  <summary>Details</summary>
Motivation: 研究动态智能体参与的单调博弈纳什均衡求解问题，这些智能体建模为具有外部扰动的下三角非线性不确定动态系统，旨在开发完全分布式的求解方法。

Method: 构建虚拟单积分器多智能体系统作为参考信号生成器，引入内模处理扰动，通过反推法设计分布式状态反馈控制器半全局镇定增强系统。

Result: 证明了当增强系统能被控制律镇定时，所有智能体的输出将达到博弈的纳什均衡。

Conclusion: 成功建立了一个通用框架，将纳什均衡求解问题转化为分布式鲁棒镇定问题，并通过控制器设计实现了半全局稳定性。

Abstract: We consider seeking a Nash equilibrium (NE) of a monotone game, played by
dynamic agents which are modeled as a class of lower-triangular nonlinear
uncertain dynamics with external disturbances. We establish a general framework
that converts the problem into a distributed robust stabilization problem of an
appropriately augmented system. To be specific, we construct a virtual
single-integrator multi-agent system, as a reference signal generator, to
compute an NE in a fully distributed manner. By introducing internal models to
tackle the disturbances, as well as embedding the virtual system, we derive an
augmented system. Following that, we show that the outputs of all agents reach
an NE of the game if the augmented system can be stabilized by a control law.
Finally, resorting to a backstepping procedure, we design a distributed
state-feedback controller to stabilize the augmented system semi-globally.

</details>


### [118] [The problem of minimal resistance, old and new](https://arxiv.org/abs/2511.01041)
*Giuseppe Buttazzo*

Main category: math.OC

TL;DR: 本文对牛顿1685年提出的最小阻力体问题进行了全面综述，回顾了该变分法经典问题的主要理论进展，并提出了新的研究方向。


<details>
  <summary>Details</summary>
Motivation: 最小阻力体问题是变分法中的经典问题，最初由牛顿提出，后来扩展到一般凸形状。该问题激发了大量关于最优形状几何和分析性质的研究。

Method: 采用文献综述的方法，系统梳理了该领域的主要研究成果，包括最优形状的结构、正则性和在各种约束下的行为特性。

Result: 总结了该问题的主要理论进展，包括从圆柱对称体到一般凸形状的扩展，以及最优形状的几何和分析性质研究成果。

Conclusion: 该综述不仅系统总结了现有成果，还提出了新的研究方向，为未来研究提供了有前景的视角。

Abstract: Since its original formulation by Isaac Newton in 1685, the problem of
determining bodies of minimal resistance moving through a fluid has been one of
the classical problems in the calculus of variations. Initially posed for
cylindrically symmetric bodies, the problem was later extended to general
convex shapes, as explored in \cite{BK93}, \cite{BFK95}. Since then, this
broader formulation has inspired a number of articles dedicated to the study of
the geometric and analytical properties of optimal shapes, with particular
attention to their structure, regularity, and behavior under various
constraints. In this article, we provide a comprehensive overview of the
principal results that have been established, highlighting the main theoretical
advancements. Furthermore, we introduce some new directions of research, some
of which were described in \cite{P12}, that offer promising perspectives for
future investigation.

</details>


### [119] [Projections onto Spectral Matrix Cones](https://arxiv.org/abs/2511.01089)
*Daniel Cederberg,Stephen Boyd*

Main category: math.OC

TL;DR: 该论文提出利用谱矩阵锥来增强一阶锥求解器求解半定规划问题的效率，通过将矩阵投影简化为特征值/奇异值投影，在SCS求解器中实现后获得显著加速效果。


<details>
  <summary>Details</summary>
Motivation: 尽管半定规划在凸优化中很重要，但求解大规模半定规划仍然具有挑战性。由于涉及的矩阵函数通常是谱或酉不变的，研究如何利用谱矩阵锥来增强求解器效率。

Method: 将矩阵投影问题简化为特征值或奇异值投影问题，并将谱矩阵锥投影支持集成到分裂锥求解器(SCS)中。

Result: 数值实验显示，增强后的SCS求解器在实验设计、鲁棒主成分分析和图划分等半定规划问题上，速度提升可达一个数量级。

Conclusion: 利用谱矩阵锥可以显著提高一阶锥求解器求解半定规划的效率，特征值/奇异值投影方法计算成本可忽略不计。

Abstract: Semidefinite programming is a fundamental problem class in convex
optimization, but despite recent advances in solvers, solving large-scale
semidefinite programs remains challenging. Generally the matrix functions
involved are spectral or unitarily invariant, i.e., they depend only on the
eigenvalues or singular values of the matrix. This paper investigates how
spectral matrix cones -- cones defined from epigraphs and perspectives of
spectral or unitarily invariant functions -- can be used to enhance first-order
conic solvers for semidefinite programs. Our main result shows that projecting
a matrix can be reduced to projecting its eigenvalues or singular values, which
we demonstrate can be done at a negligible cost compared to the eigenvalue or
singular value decomposition itself. We have integrated support for spectral
matrix cone projections into the Splitting Conic Solver (SCS). Numerical
experiments show that SCS with this enhancement can achieve speedups of up to
an order of magnitude for solving semidefinite programs arising in experimental
design, robust principal component analysis, and graph partitioning.

</details>


### [120] [Ergodic Risk Sensitive Control of Diffusions under a General Structural Hypothesis](https://arxiv.org/abs/2511.01100)
*Sumith Reddy Anugu,Guodong Pang*

Main category: math.OC

TL;DR: 本文研究了扩散过程的无限时域平均风险敏感控制问题，在满足Foster-Lyapunov漂移条件和近单调性条件的结构假设下，完全刻画了最优平稳马尔可夫控制。


<details>
  <summary>Details</summary>
Motivation: 研究在特定结构假设下的风险敏感控制问题，其中状态空间被划分为两个子集，分别满足不同的条件，旨在完全刻画最优平稳马尔可夫控制。

Method: 使用布朗运动指数泛函的变分公式，将风险敏感成本视为扩展扩散控制问题的最优值，其中包含新的辅助控制，并建立了扩展扩散的先验估计来克服紧性困难。

Result: 在所述结构假设下，完全刻画了最优平稳马尔可夫控制，并克服了扩展扩散均值经验测度紧性问题。

Conclusion: 通过变分公式和先验估计方法，成功解决了风险敏感控制问题，为在特定结构条件下的最优控制提供了完整理论框架。

Abstract: We study the infinite-horizon average (ergodic) risk sensitive control
problem for diffusion processes under a general structural hypothesis: there is
a partition of state space into two subsets, where the controlled diffusion
process satisfies a Foster-Lyapunov type drift condition in one subset, under
any stationary Markov control, while the near-monotonicity condition is
satisfied with the running cost function being inf-compact in its complement.
Under these conditions, we completely characterize the optimal stationary
Markov controls. To prove this, we consider an inf-compact perturbation to the
running cost over the entire space such that the resulting ergodic risk
sensitive control problem is well-defined and then use the corresponding
existing results. The heart of the analysis lies in exploiting the variational
formula of exponential functionals of Brownian motion and applying it to the
objective exponential cost function of the controlled diffusion. This
representation facilitates us to view the risk sensitive cost for any
stationary Markov control as the optimal value of a control problem of an
extended diffusion involving a new auxiliary control where the optimal
criterion is to maximize the associated long-run average cost criterion that is
a difference of the original running cost and an extra term that is quadratic
in the auxiliary control. The main difficulty in using this approach lies in
the fact that tightness of mean empirical measures of the extended diffusion is
not a priori implied by the analogous tightness property of the original
diffusion. We overcome this by establishing a priori estimates for the extended
diffusion associated with the nearly optimal auxiliary controls.

</details>


### [121] [A decomposition method in the multivariate feedback particle filter via tensor product Hermite polynomials](https://arxiv.org/abs/2511.01227)
*Ruoyu Wang,Xue Luo*

Main category: math.OC

TL;DR: 本文提出了一种用于多变量反馈粒子滤波器(FPF)的分解方法，通过将泊松方程分解为两个可精确求解的子方程，显著降低了计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 反馈粒子滤波器(FPF)虽然避免了重采样，但其反馈增益函数需要求解泊松方程，目前缺乏高效的多变量求解算法。

Method: 采用分解方法，将多变量FPF的泊松方程拆分为两个可精确求解的子方程，解决了系数矩阵可逆性和加权径向解构造等关键挑战。

Result: 该方法计算复杂度随状态维度呈多项式增长，相比传统粒子算法的指数增长有显著改进。数值实验显示其在精度和效率上优于PF和其他增益近似方法。

Conclusion: 分解方法为多变量FPF提供了一种高效可行的增益函数求解方案，在保持性能的同时大幅降低了计算成本。

Abstract: The feedback particle filter (FPF), a resampling-free algorithm proposed over
a decade ago, modifies the particle filter (PF) by incorporating a feedback
structure. Each particle in FPF is regulated via a feedback gain function
(lacking a closed-form expression), which solves a Poisson's equation with a
probability-weighted Laplacian. While approximate solutions to this equation
have been extensively studied in recent literature, no efficient multivariate
algorithm exists. In this paper, we focus on the decomposition method for
multivariate gain functions in FPF, which has been proven efficient for scalar
FPF with polynomial observation functions. Its core is splitting the Poisson's
equation into two exactly solvable sub-equations. Key challenges in extending
it to multivariate FPF include ensuring the invertibility of the coefficient
matrix in one sub-equation and constructing a weighted-radial solution in the
other. The proposed method's computational complexity grows at most
polynomially with the state dimension, a dramatic improvement over the
exponential growth of most particle-based algorithms. Numerical experiments
compare the decomposition method with traditional methods: the extended Kalman
filter (EKF), PF, and FPF with constant-gain or kernel-based gain
approximations. Results show it outperforms PF and FPF with other gain
approximations in both accuracy and efficiency, achieving the shortest CPU time
among methods with comparable performance.

</details>


### [122] [A parallel pull labelling algorithm for the resource constrained shortest path problem](https://arxiv.org/abs/2511.01397)
*Bjørn Petersen,Simon Spoorendonk*

Main category: math.OC

TL;DR: 提出了一种用于资源约束最短路径问题的高效拉标签算法，通过并行化、双向搜索和向量化优势比较等技术，在困难实例上实现了14倍到200倍的加速。


<details>
  <summary>Details</summary>
Motivation: 资源约束最短路径问题是组合优化中的基础问题，在路由、调度、供应链和运输网络优化等应用中具有重要作用。现有算法在处理大规模实例时效率不足，需要更高效的解决方案。

Method: 开发了拉标签算法，包含三个关键技术：1）标签桶级别的高度并行化方法；2）带动态中点的双向搜索扩展；3）使用向量指令加速标签比较的向量化优势准则。

Result: 与基线算法相比，优化后的算法在困难实例集上实现了约14倍的加速，在最困难实例上甚至达到200倍加速。

Conclusion: 该算法显著提升了计算效率，能够增强包含资源约束最短路径子问题的列生成框架性能，有望在路由、调度、供应链和运输网络优化等应用中实现更大规模实例的高效求解。

Abstract: The Resource Constrained Shortest Path Problem (RCSPP) is a fundamental
combinatorial optimisation problem in which the goal is to find a least-cost
path in a directed graph subject to one or more resource constraints. In this
paper we present a pull labelling algorithm for the RCSPP that introduces i) a
highly parallelisable approach at a label bucket level, ii) an extension to
bi-directional search with a dynamic midpoint, and iii) a vectorised dominance
criterion that uses vector instructions to speed-up the label comparison with
another level of parallelisation. Compared to a baseline version of the
algorithm the optimisations result in a speed-up of around 14x on a set of hard
instances and up to 200x on some of the hardest instances. The proposed
algorithm demonstrates significant computational improvements that may enhance
the efficiency of column generation frameworks incorporating resource
constrained shortest path sub-problems, potentially enabling the efficient
solution of larger-scale instances in routing, scheduling, supply chain and
transportation network optimisation applications.

</details>


### [123] [Robust single-stage selection problems with budgeted interval uncertainty](https://arxiv.org/abs/2511.01416)
*Antoine Lhomme,Nadia Brauner,Evgeny Gurevsky,Mikhail Kovalyov,Erwin Pesch*

Main category: math.OC

TL;DR: 该论文研究单阶段决策问题，涉及在固定成本和不确定成本下选择最小总成本的项目子集，考虑最坏情况下的预算区间不确定性，并提供了几乎完整的计算复杂度分类。


<details>
  <summary>Details</summary>
Motivation: 研究单阶段决策问题，其中在固定成本已知但不确定成本仅知其范围的情况下，需要选择满足基数约束的最小总成本项目子集，考虑预算区间不确定性模型。

Method: 采用鲁棒优化方法，考虑基于基数或容量的不确定性预算，分析连续或离散不确定成本的不同情况，使用动态规划等技术进行算法设计和复杂度分析。

Result: 提供了几乎完整的计算复杂度分类，包括快速多项式时间算法、NP完全性和Σp2完全性证明，以及硬度结果。

Conclusion: 该研究对单阶段鲁棒选择问题进行了系统的计算复杂度分类，为不同不确定性设置下的算法设计提供了理论依据。

Abstract: We study single-stage decision problems in which a subset of items with
minimum total cost has to be selected at once from a given set of items,
subject to two costs of each item -fixed and uncertain -and cardinality
constraints for each cost type. The worst-case budgeted interval uncertainty is
considered. At the time of decision making, the fixed costs are known, but for
each uncertain cost, only the range of its values is available. Similar but
two-stage selection problems have been studied in the literature, in which
first-and second-stage decisions are made before and after uncertain costs
become known, respectively. The problems studied are distinguished by
continuous or discrete uncertain costs, and by uncertainty budgets based on
cardinality or volume. An almost complete computational complexity
classification is provided, including fast polynomial-time algorithms, NP-and
$\Sigma$ p 2 -completeness and hardness proofs. keyword robust optimization
-budgeted uncertainty -selection problem -dynamic programming -computational
complexity

</details>


### [124] [Data-driven stabilization of nonlinear systems via descriptor embedding](https://arxiv.org/abs/2511.01457)
*Mohammad Alsalti,Claudio De Persis,Victor G. Lopez,Matthias A. Müller*

Main category: math.OC

TL;DR: 提出描述符嵌入概念用于非线性系统，开发基于数据的稳定控制器设计方法，通过数据依赖的LMI条件返回形如u=K(x)Z(x)的稳定非线性控制器，并扩展到处理不确定性和噪声数据。


<details>
  <summary>Details</summary>
Motivation: 为非线性系统开发基于数据的控制器设计方法，避免对系统模型的依赖，同时处理系统不确定性和测量噪声。

Method: 使用描述符嵌入概念，构建数据依赖的LMI条件来设计稳定控制器，控制器形式为u=K(x)Z(x)，其中K(x)属于多面体，Z是用户定义函数。

Result: 提供了保证系统稳定的充分条件，能够仅使用数据估计吸引域，并通过仿真验证了方法的有效性。

Conclusion: 所提出的描述符嵌入方法为非线性系统的数据驱动控制提供了一种有效的框架，能够处理不确定性和噪声，且性能优于现有方法。

Abstract: We introduce the notion of descriptor embedding for nonlinear systems and use
it for the data-driven design of stabilizing controllers. Specifically, we
provide sufficient data-dependent LMI conditions which, if feasible, return a
stabilizing nonlinear controller of the form $u=K(x)Z(x)$ where $K(x)$ belongs
to a polytope and $Z$ is a user-defined function. The proposed method is then
extended to account for the presence of uncertainties and noisy data.
Furthermore, a method to estimate the resulting region of attraction is given
using only data. Simulation examples are used to illustrate the results and
compare them to existing methods from the literature.

</details>


### [125] [Boscia.jl: A review and tutorial](https://arxiv.org/abs/2511.01479)
*Wenjie Xiao,Deborah Hendrych,Mathieu Besançon,Sebastian Pokutta*

Main category: math.OC

TL;DR: Boscia框架是针对目标函数非线性的凸混合整数非线性规划问题的求解框架，通过将Frank-Wolfe方法与分支定界结合，支持非精确节点处理、热启动和组合结构利用。


<details>
  <summary>Details</summary>
Motivation: 解决凸混合整数非线性优化问题，特别是目标函数非线性而约束为线性的情况，这类问题具有挑战性且结构多样。

Method: 将Frank-Wolfe方法作为连续求解器集成到分支定界框架中，支持非精确节点处理、热启动和组合结构利用，提供基于oracle的目标函数和梯度访问。

Result: 展示了三个实际例子，证明了框架的灵活性、用户对优化过程的控制能力，以及基于oracle访问的优势。

Conclusion: 该教程旨在帮助读者理解Boscia框架的主要原理，为凸MINLP问题提供有效的求解方法。

Abstract: Mixed-integer nonlinear optimization (MINLP) comprises a large class of
problems that are challenging to solve and exhibit a wide range of structures.
The Boscia framework Hendrych et al. (2025b) focuses on convex MINLP where the
nonlinearity appears in the objective only. This paper provides an overview of
the framework and practical examples to illustrate its use and customizability.
One key aspect is the integration and exploitation of Frank-Wolfe methods as
continuous solvers within a branch-and-bound framework, enabling inexact node
processing, warm-starting and explicit use of combinatorial structure among
others. Three examples illustrate its flexibility, the user control over the
optimization process and the benefit of oracle-based access to the objective
and its gradient. The aim of this tutorial is to provide readers with an
understanding of the main principles of the framework.

</details>


### [126] [Structural and Solution Analysis for the Ordered Weber Problem under Spatial Uncertainty](https://arxiv.org/abs/2511.01481)
*Víctor Blanco,Miguel Martínez-Antón*

Main category: math.OC

TL;DR: 提出了一个单设施连续位置问题的通用分析框架，处理空间需求不确定性，使用概率测度表示不确定性，通过有序加权平均算子聚合期望距离，建立了随机有序Weber模型的基本性质。


<details>
  <summary>Details</summary>
Motivation: 解决经典离散或区域聚合需求模型的局限性，提供处理有限、有界和无界支持分布的统一定位问题框架。

Method: 使用概率测度表示空间需求不确定性，通过有序加权平均算子聚合期望距离，建立随机有序Weber模型，分析其凸性、连续性和最优解存在性，开发自适应样本平均近似方案。

Result: 证明了模型的凸性、连续性和最优解存在性，获得了随机最小化器与需求支撑凸包邻近性的定量界限，建立了自适应样本平均近似方案的收敛性和有限样本误差估计。

Conclusion: 为广泛的随机有序位置模型提供了严格的数学基础，揭示了凸分析、随机规划和有序优化之间的新理论联系。

Abstract: We propose a general analytical framework for single-facility continuous
location problems under spatial demand uncertainty. In contrast to classical
formulations based on discrete or regionally aggregated demands, the proposed
model represents uncertainty through general probability measures on $\R^d$,
thereby encompassing finite, bounded, and unbounded support distributions
within a unified formulation. The objective aggregates expected distances by
means of an ordered weighted averaging operator, providing a flexible
mathematical structure that includes the classical Weber problem and its
ordered extensions as special cases. We establish fundamental properties of
this stochastic ordered Weber model, including convexity, continuity, and
existence of optimal solutions, and we derive quantitative bounds on the
proximity between stochastic minimizers and the convex hulls of demand
supports. Building upon these results, we develop and analyze an adaptive
sample average approximation scheme, proving its convergence and deriving
finite-sample error estimates under mild regularity conditions. For spherically
symmetric distributions, we further obtain explicit analytical expressions for
the approximation error. Together, these results provide a rigorous
mathematical foundation for a broad class of stochastic ordered location models
and highlight new theoretical connections between convex analysis, stochastic
programming, and ordered optimization.

</details>


### [127] [Mean Field Control of Thermostatically Controlled Loads as Piecewise Deterministic Markov Processes](https://arxiv.org/abs/2511.01500)
*Thomas Le Corre,Adrien Séguret,Ana Bušić*

Main category: math.OC

TL;DR: 提出了一种基于平均场控制的PDMP方法，用于控制大量热控负载(TCLs)，通过代表性代理降低问题维度，并引入服务质量约束确保温度舒适范围。


<details>
  <summary>Details</summary>
Motivation: 解决大量代理控制问题的高维性挑战，特别针对热控负载的协调控制，需要确保每个代理的温度保持在舒适范围内。

Method: 采用PDMP平均场控制框架，引入额外的跳跃强度使代理在温度边界时切换加热/冷却模式，基于对偶公式和随机梯度下降的分散算法。

Result: 通过水加热器控制的数值实验验证了该方法在信号跟踪和考虑能源价格两个场景中的有效性。

Conclusion: 该方法成功将PDMP平均场控制框架应用于TCLs协调控制，通过服务质量约束确保了温度舒适性，并在实际应用中展示了良好性能。

Abstract: This paper presents a mean-field control approach for Piecewise Deterministic
Markov Processes (PDMPs), specifically designed for controlling a large number
of agents. By modeling the interactions of a large number of agents through an
aggregate cost function, the proposed method mitigates the high dimensionality
of the problem by focusing on a representative agent. The contribution of this
work is the application of a PDMP-based mean-field control framework to the
coordination of a large population of Thermostatically Controlled Loads (TCLs).
Adapting this framework to TCLs requires incorporating a quality-of-service
constraint ensuring that each agent's temperature remains within a specified
comfort range. To achieve this, an additional jump intensity is introduced so
that agents are very likely to switch between heating and cooling modes when
they reach the boundaries of their temperature range. This extension to TCLs is
demonstrated through Water Heaters (WHs) control, with a decentralized
algorithm based on a dual formulation and stochastic gradient descent. The
numerical results obtained illustrate this approach on two examples (signal
tracking and taking into account energy price).

</details>


### [128] [Mean-Field Game for Gene Expression of Beetles](https://arxiv.org/abs/2511.01523)
*Yiming Jiang,Yuan Lou,Yawei Wei,Fei Zeng,Zelin Zhang*

Main category: math.OC

TL;DR: 使用平均场博弈理论研究多群体中大小甲虫的竞争压力对基因表达概率的影响，证明了方程解的存在唯一性。


<details>
  <summary>Details</summary>
Motivation: 研究在竞争关系下控制甲虫大小的基因表达概率，理解不同体型甲虫在群体中的竞争压力差异。

Method: 采用多群体平均场博弈理论，模拟有限时间[0, T]内的基因表达概率，并在一定假设下证明方程解的存在唯一性。

Result: 成功模拟了基因表达概率，并证明了相关方程解的存在性和唯一性。

Conclusion: 平均场博弈理论能有效分析多群体竞争环境下的基因表达概率，为理解生物竞争机制提供了理论框架。

Abstract: In this paper, we investigate the probability of the expression of genes that
control the size of beetles under competitive relationships. We use the mean
field game (MFG) theory in multiple populations to characterize the different
competitive pressures of large and small beetles in the population, and
simulate the probability of gene expression in finite time $[0, T]$. Therefore,
we prove the existence and uniqueness of the solution of the equation under
some assumptions.

</details>


### [129] [A New Algorithm for Zero-Sum Linear-Quadratic Stochastic Differential Games in Infinite Horizons](https://arxiv.org/abs/2511.01538)
*Yiyuan Wang*

Main category: math.OC

TL;DR: 提出一种新的零和线性二次随机微分博弈算法，通过构建双层迭代矩阵递增序列，将原问题转化为相互关联的子问题，并通过求解代数Riccati方程获得稳定解。


<details>
  <summary>Details</summary>
Motivation: 扩展经典设置，为更广泛的随机博弈理论代数Riccati方程提供首个完整的统一数值框架。

Method: 构建双层迭代矩阵递增序列，将原问题分解为相互关联的子问题，依次计算每个子问题中代数Riccati方程的稳定解。

Result: 算法能够推导出原问题的稳定解，并严格证明了算法的收敛性，数值模拟验证了方法的有效性。

Conclusion: 该工作为求解更广泛的随机博弈理论代数Riccati方程提供了首个完整的统一数值框架。

Abstract: We propose a new algorithm for Zero-Sum Linear-Quadratic Stochastic
Differential Games by dissecting their inherent structures. Specifically, we
construct dual-layer iterative matrix-increasing sequences, which reformulate
the original problem into a set of mutually interconnected subproblems. By
sequentially computing the stabilizing solutions to the associated algebraic
Riccati equations within each subproblem, we derive the stabilizing solutions
for the original problem and rigorously establish the convergence of the
proposed algorithm. Numerical simulations further validate the effectiveness of
the method. This work extends classical setting and provides the first complete
unified numerical framework for solving a broader class of stochastic
Game-Theoretic algebraic Riccati equations.

</details>


### [130] [Mutual Consensus and its Application in Minimum Cost Consensus Models](https://arxiv.org/abs/2511.01614)
*Diego García-Zamora,Bapi Dutta,Luis Martínez*

Main category: math.OC

TL;DR: 提出了一种新的非补偿性共识度量方法——互共识，并基于此开发了多个最小成本共识模型，分析了它们的性质。在OWA-MCC模型框架下应用这些模型，提供了对称条件下的线性化公式，并展示了通用情况下可行域的非凸性。最后利用互共识为OWA-MCC模型获得近似解。


<details>
  <summary>Details</summary>
Motivation: 现有共识度量方法存在局限性，需要一种能够考虑意见间最大差异的稳健共识评估方法，以改进群体决策中的共识建模。

Method: 提出互共识概念作为非补偿性共识度量，开发基于互共识的最小成本共识模型，并在OWA-MCC模型框架下应用这些模型，包括对称条件下的线性化公式和通用情况分析。

Result: 成功开发了基于互共识的MCC模型，分析了模型性质，在OWA-MCC框架下实现了模型应用，并展示了互共识在获得近似解方面的有效性。

Conclusion: 互共识方法推进了共识建模的理论和应用维度，为群体决策提供了更稳健的共识评估工具，在OWA-MCC模型中表现出实际有效性。

Abstract: This paper introduces the concept of {mutual consensus} as a novel
non-compensatory consensus measure that accounts for the maximum disparity
among opinions to ensure robust consensus evaluation. Incorporating this
concept, several new Minimum Cost Consensus (MCC) models are proposed, and
their properties are analyzed. To show their applicability, these mutual
consensus-based MCC models are then considered in the context of the {OWA-MCC}
model, which employs Ordered Weighted Averaging (OWA) operators for preference
aggregation. Concretely, we include a linearized formulation under symmetry
conditions as well as examples of the non-convexity of the feasible region in
the general case. Finally, mutual consensus is utilized to obtain approximate
solutions for the OWA-MCC model, demonstrating its practical effectiveness and
advancing the theoretical and applied dimensions of consensus modeling in group
decision-making.

</details>


### [131] [Observer-Based Sampled-Data Stabilisation of Switched Systems with Lipschitz Nonlinearities and Dwell-Time](https://arxiv.org/abs/2511.01672)
*Rami Katz,Antonio Russo,Gian Paolo Incremona,Patrizio Colaneri,Giulia Giordano*

Main category: math.OC

TL;DR: 研究基于状态观测器的采样数据切换律，用于在驻留时间约束下稳定具有不确定Lipschitz非线性的名义线性切换系统。


<details>
  <summary>Details</summary>
Motivation: 解决具有不确定非线性项的切换系统在采样数据测量下的稳定性问题，特别是在存在驻留时间约束的情况下。

Method: 基于Lyapunov-Metzler不等式设计切换律，考虑采样数据输出测量，推导时间依赖的LMI条件来保证闭环系统的全局渐近稳定性。

Result: 获得了平均二次成本的估计值及其与实际成本最大偏差的界限，证明了通过有限网格离散化可以消除LMI的时间依赖性。

Conclusion: 所提出的基于观测器的采样数据切换控制方法能够有效稳定具有不确定非线性的切换系统，数值算例验证了理论结果的有效性。

Abstract: We investigate the stabilisation of nominally linear switched systems with
uncertain Lipschitz nonlinearities under dwell-time constraints, using a
sampled-data switching law based on a state observer. We design the switching
law based on Lyapunov-Metzler inequalities, accounting for the sampled-data
output measurements, and we derive time-dependent LMI conditions for global
asymptotic stability of the resulting closed-loop system. We obtain an estimate
of the average quadratic cost and a bound on its maximum deviation from the
actual cost. We also discuss the feasibility of the derived LMIs, provide
equivalent reduced-order LMI conditions, and prove that the time dependence of
the LMIs can be removed by discretising on a finite grid. Numerical examples
illustrate our theoretical results.

</details>


### [132] [Turnpike Property of Mean-Field Linear-Quadratic Optimal Control Problems in Infinite-Horizon with Regime Switching](https://arxiv.org/abs/2511.01731)
*Hongwei Mei,Svetlozar Rachev,Rui Wang*

Main category: math.OC

TL;DR: 本文研究了具有状态切换的线性平均场随机微分方程的二次型最优控制问题，在长时间范围内建立了最优对具有强转向点性质。


<details>
  <summary>Details</summary>
Motivation: 研究长时间范围内具有状态切换的线性平均场随机微分方程的最优控制问题，探索最优对在时间趋于无穷时的渐近行为。

Method: 应用正交分解方法推导有限时间范围内的闭环最优控制表示，分析Riccati方程和向后微分方程解的收敛性。

Result: 建立了最优对在长时间范围内的强转向点性质，并在可积和局部可积两种情况下验证了极限最优对的最优性。

Conclusion: 证明了具有状态切换的线性平均场随机系统在长时间范围内的最优控制具有强转向点性质，为相关控制问题提供了理论保证。

Abstract: This paper considers an optimal control problem for a linear mean-field
stochastic differential equation having regime switching with quadratic
functional in the large time horizons. Our main contribution lies in
establishing the strong turnpike property for the optimal pairs when the time
horizon tends to infinity. To work with the mean-field terms, we apply the
orthogonal decomposition method to derive a closed-loop representation of the
optimal control problem in a finite time horizon. To analyze the asymptotic
behavior of the optimal controls, we examine the convergence of the solutions
of Riccati equations and backward differential equations as the time horizon
tends to infinity. The strong turnpike property can be obtained based on these
convergence results. Finally, we verify the optimality of the limit optimal
pair in two cases: integrable case and local-integrable case.

</details>


### [133] [Disciplined Biconvex Programming](https://arxiv.org/abs/2511.01813)
*Hao Zhu,Joschka Boedecker*

Main category: math.OC

TL;DR: DBCP是一个用于指定和求解双凸优化问题的建模框架，通过扩展约束凸规划原则，允许用户以自然方式指定双凸问题，并自动生成交替凸搜索求解器。


<details>
  <summary>Details</summary>
Motivation: 双凸优化问题在机器学习、信号处理等领域广泛应用，但现有的交替凸搜索方法需要用户手动设计和实现，过程繁琐且容易出错。

Method: DBCP扩展了约束凸规划原则，基于少量语法规则让用户自然指定双凸问题，然后自动拆分和转换为凸子问题，并生成定制的交替凸搜索求解器。

Result: 实现了DBCP作为开源Python包dbcp，作为著名凸优化领域特定语言CVXPY的扩展，使用户能够快速实验不同的双凸问题公式。

Conclusion: DBCP框架使用户无需凸优化专业知识即可轻松处理双凸优化问题，显著降低了实现复杂度。

Abstract: We introduce disciplined biconvex programming (DBCP), a modeling framework
for specifying and solving biconvex optimization problems. Biconvex
optimization problems arise in various applications, including machine
learning, signal processing, computational science, and control. Solving a
biconvex optimization problem in practice usually resolves to heuristic methods
based on alternate convex search (ACS), which iteratively optimizes over one
block of variables while keeping the other fixed, so that the resulting
subproblems are convex and can be efficiently solved. However, designing and
implementing an ACS solver for a specific biconvex optimization problem usually
requires significant effort from the user, which can be tedious and
error-prone. DBCP extends the principles of disciplined convex programming to
biconvex problems, allowing users to specify biconvex optimization problems in
a natural way based on a small number of syntax rules. The resulting problem
can then be automatically split and transformed into convex subproblems, for
which a customized ACS solver is then generated and applied. DBCP allows users
to quickly experiment with different biconvex problem formulations, without
expertise in convex optimization. We implement DBCP into the open source Python
package dbcp, as an extension to the famous domain specific language CVXPY for
convex optimization.

</details>


### [134] [Sensitivity Analysis of Distributionally Robust BSDEs and RBSDEs](https://arxiv.org/abs/2511.01828)
*Compoint Arthur,Sauldubois Nathan,Touzi Nizar*

Main category: math.OC

TL;DR: 本文研究了在分布鲁棒优化控制问题中，反向随机微分方程和反射反向随机微分方程的敏感性性质，建立了漂移参考测度不确定性下的显式敏感性公式。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于分布鲁棒优化控制和最优停止问题中的敏感性分析需求，特别是在漂移参考测度不确定性下的敏感性分析问题。

Method: 在一般非马尔可夫框架下，分析反向随机微分方程和反射反向随机微分方程的敏感性性质，建立显式敏感性公式。

Result: 建立了在漂移参考测度不确定性下，反向随机微分方程和反射反向随机微分方程的显式敏感性公式。

Conclusion: 本文在非马尔可夫框架下成功建立了分布鲁棒优化控制问题中随机微分方程的敏感性分析理论，为相关优化问题提供了理论基础。

Abstract: We examine the sensitivity properties of backward stochastic differential
equations and reflected backward stochastic differential equations, which
naturally arise in the context of optimal control and optimal stopping
problems. Motivated by issues of sensitivity analysis in distributionally
robust optimization (DRO) control and optimal stopping problems, we establish
explicit formulas for the corresponding sensitivities under drift reference
measure uncertainty. Our work is closely related to
\citeauthor{bartl2023sensitivity} \cite{bartl2023sensitivity}. In contrast to
the existing literature, our analysis is carried out within a general
non-Markovian framework.

</details>


<div id='q-fin.GN'></div>

# q-fin.GN [[Back]](#toc)

### [135] [A parallel monetary system based on the redeemable self-decaying money -- The ultimate hedge and safe haven of private wealth in the rising wave of over issuance of fiat and token money/stablecoin](https://arxiv.org/abs/2511.00365)
*Boliang Lin,Ruixi Lin*

Main category: q-fin.GN

TL;DR: 提出了一种基于可赎回自衰减/贬值货币(RSDM)的平行货币系统，旨在解决法定货币过度发行问题，构建包含RSDM、国内法币和主要国际储备货币的三重货币体系，形成终极财富避风港。


<details>
  <summary>Details</summary>
Motivation: 布雷顿森林体系崩溃后，国际社会普遍存在发行廉价货币的趋势，稳定币的合法化和过度发行加剧了这一趋势。需要建立具有长期价值存储和零物流成本流通功能的现代"良币"系统。

Method: 构建了基于RSDM的平行货币系统，包括RSDM、国内法币和主要国际储备货币的三重货币体系。建立了多货币池中货币优化选择的整数规划模型，并讨论了RSDM在激活印度休眠黄金资产和美国黄金货币化等现实场景的应用。

Result: 研究发现单一货币难以承担现代"良币"的责任，只有包含RSDM的平行货币系统才能形成终极财富避风港并保障逆向格雷欣法则。分析了以贵金属为抵押的RSDM需求，为建立健全的平行货币系统提供理论支持。

Conclusion: 基于RSDM的平行货币系统能够提供稳定的购买力保障，通过三重货币体系构建财富避风港，为激活黄金资产和实现黄金货币化提供了新的解决方案。

Abstract: A currency with stable purchasing power can always provide a psychological
haven for people around the world. However, since the collapse of the Bretton
Woods system, issuing more cheap currencies has become a common trend in the
international community, and the legalization and over issuance of stablecoins
will strengthen this trend. In this context, our study focused on a parallel
monetary system based on a redeemable self-decay/devalued money(RSDM). Firstly,
we point out the idea of redeeming gold at a fixed denomination with gold
certificates is similar to an impossible perpetual motion machine. Only when
the face value of a gold token self-decays or self-depreciates and the weight
of the reduced value can compensate for the storage cost of physical gold, can
it be convertible or redeemable. Secondly, we pointed out that as a modern
"good money" under the Internet environment, it must have two basic functions:
long-term value storage and zero logistics cost of money circulation. Thirdly,
we found that a single type of money is difficult to shoulder the
responsibility of modern "good money". Only a parallel monetary system,
including RSDM, such as a triple-monetary system consisting of RSDM, domestic
fiat and major international reserve currencies, can form the ultimate safe
haven of wealth and safeguard the reverse Gresham law. Based on this analysis,
we build an integer programming model for currency optimization selection in a
multi-monetary pool. Fourthly, several potential application scenarios of RSDM
in the real world were discussed, including a new approach to activate dormant
gold assets in India based on RSDM, and the gold monetization scheme in the
United States. Finally, the demand for RSDM with precious metals as collateral
was analyzed, providing theoretical support for establishing a sound parallel
monetary system based on RSDM.

</details>


### [136] [How Digital Asset Treasury Companies Can Survive Bear Markets: The Case of the Strategy and Bitcoin](https://arxiv.org/abs/2511.01135)
*Hongzhe Wen*

Main category: q-fin.GN

TL;DR: 该论文为持有大量加密资产的上市公司开发了一个生存框架，通过保守的国库政策和独立于市价收益的运营线路来管理下行风险。以MicroStrategy为例，提出了"BTC-to-sats"支付通道，利用闪电网络产生与价格无关的费用收入，同时保持接近零的BTC敞口。


<details>
  <summary>Details</summary>
Motivation: 数字资产国库公司面临熊市中股权溢价压缩的严重下行风险，需要建立能够在不强制出售资产的情况下度过长期熊市的运营模式。

Method: 开发生存框架，结合保守国库政策和运营线路；提出"BTC-to-sats"支付通道，将部分流动性分配到闪电网络通道；建立无强制出售条件和可披露的KPI指标。

Result: 该模型显示运营现金流能够支撑18-24个月的熊市而无需清算资产；闪电网络通道能够产生价格无关的费用收入；为投资者提供了测试公司生存能力的可验证指标。

Conclusion: 该框架适用于各类数字资产国库公司，通过可实施的披露要求能够维持股权溢价在周期中保持稳定，为加密资产持有公司提供了可持续的运营模式。

Abstract: Digital Asset Treasury (DAT) companies, public firms that hold large crypto
reserves as a core strategy, deliver levered exposure to digital assets but
face acute downside risk when equity premia over net asset value multiples
(mNAV) compress in bear markets. This paper develops a survival framework that
couples conservative treasury policy with an operating line that monetizes
holdings independent of mark-to-market gains. Using Strategy (formerly
MicroStrategy) as a case, we propose a "BTC-to-sats" payments rail that
allocates a small, risk-capped liquidity sleeve of the treasury to Lightning
Network channels, generating price-agnostic fee revenue (acquiring bps,
routing, hedge/FX spread) while keeping settlement exposure near zero beta to
BTC. We formalize a no-forced-sale condition and show how disclosed KPIs allow
investors to test whether operating cash flows can bridge an 18 to 24-month
bear without liquidations. The feasibility of the rail is supported by
Strategy's Lightning initiative and empirical Lightning performance. Our model
generalizes across DAT types and provides implementable disclosures that can
sustain an mNAV premium through cycles.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [137] [Gradient Boosted Mixed Models: Flexible Joint Estimation of Mean and Variance Components for Clustered Data](https://arxiv.org/abs/2511.00217)
*Mitchell L. Prevett,Francis K. C. Hui,Zhi Yang Tho,A. H. Welsh,Anton H. Westveld*

Main category: stat.ML

TL;DR: 提出了GBMixed框架，将梯度提升扩展到混合模型，通过基于似然的梯度联合估计均值和方差分量，支持非参数估计和异方差不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 线性混合模型依赖参数形式限制了灵活性，而梯度提升方法虽然预测准确但不支持聚类数据结构或不确定性量化。需要结合两者优势的方法。

Method: 使用基于似然的梯度进行提升，通过回归树或样条等灵活基学习器对随机效应和残差方差进行非参数建模，实现协变量依赖的函数估计。

Result: 模拟和实际应用显示能准确恢复方差分量、校准预测区间，相比标准线性混合模型和非参数方法有更好的预测准确性。

Conclusion: GBMixed提供了异方差不确定性量化，支持异质随机效应的提升，实现协变量依赖的收缩预测，在因果假设下能估计异质处理效应。

Abstract: Linear mixed models are widely used for clustered data, but their reliance on
parametric forms limits flexibility in complex and high-dimensional settings.
In contrast, gradient boosting methods achieve high predictive accuracy through
nonparametric estimation, but do not accommodate clustered data structures or
provide uncertainty quantification.
  We introduce Gradient Boosted Mixed Models (GBMixed), a framework and
algorithm that extends boosting to jointly estimate mean and variance
components via likelihood-based gradients. In addition to nonparametric mean
estimation, the method models both random effects and residual variances as
potentially covariate-dependent functions using flexible base learners such as
regression trees or splines, enabling nonparametric estimation while
maintaining interpretability.
  Simulations and real-world applications demonstrate accurate recovery of
variance components, calibrated prediction intervals, and improved predictive
accuracy relative to standard linear mixed models and nonparametric methods.
GBMixed provides heteroscedastic uncertainty quantification and introduces
boosting for heterogeneous random effects. This enables covariate-dependent
shrinkage for cluster-specific predictions to adapt between population and
cluster-level data. Under standard causal assumptions, the framework enables
estimation of heterogeneous treatment effects with reliable uncertainty
quantification.

</details>


### [138] [A Streaming Sparse Cholesky Method for Derivative-Informed Gaussian Process Surrogates Within Digital Twin Applications](https://arxiv.org/abs/2511.00366)
*Krishna Prasath Logakannan,Shridhar Vashishtha,Jacob Hochhalter,Shandian Zhe,Robert M. Kirby*

Main category: stat.ML

TL;DR: 提出了一种包含导数数据的稀疏高斯过程模型，用于改进数字孪生的预测精度，并通过动态更新机制适应物理孪生的实时数据。


<details>
  <summary>Details</summary>
Motivation: 数字孪生需要高精度且实时的预测模型，传统多物理模型计算成本高，而现有高斯过程模型在包含导数数据时计算复杂度急剧增加。

Method: 扩展高斯过程模型以包含导数数据，采用稀疏高斯过程近似来降低计算复杂度，开发动态更新机制以持续集成物理孪生的服役数据。

Result: 数值实验表明，导数增强的稀疏高斯过程方法在动态数据添加后能产生改进的预测模型，计算效率显著提升。

Conclusion: 该方法成功应用于航空航天车辆疲劳裂纹增长的建模，证明了其在数字孪生框架中的有效性和实用性。

Abstract: Digital twins are developed to model the behavior of a specific physical
asset (or twin), and they can consist of high-fidelity physics-based models or
surrogates. A highly accurate surrogate is often preferred over multi-physics
models as they enable forecasting the physical twin future state in real-time.
To adapt to a specific physical twin, the digital twin model must be updated
using in-service data from that physical twin. Here, we extend Gaussian process
(GP) models to include derivative data, for improved accuracy, with dynamic
updating to ingest physical twin data during service. Including derivative
data, however, comes at a prohibitive cost of increased covariance matrix
dimension. We circumvent this issue by using a sparse GP approximation, for
which we develop extensions to incorporate derivatives. Numerical experiments
demonstrate that the prediction accuracy of the derivative-enhanced sparse GP
method produces improved models upon dynamic data additions. Lastly, we apply
the developed algorithm within a DT framework to model fatigue crack growth in
an aerospace vehicle.

</details>


### [139] [Accuracy estimation of neural networks by extreme value theory](https://arxiv.org/abs/2511.00490)
*Gero Junike,Marco Oesting*

Main category: stat.ML

TL;DR: 应用极值理论量化神经网络误差的极端值，提出新的广义帕累托分布形状参数估计器


<details>
  <summary>Details</summary>
Motivation: 神经网络能够逼近紧集上的连续函数，但难以量化神经网络与目标函数之间的误差，特别是应用相关的极端误差值

Method: 使用极值理论分析超过阈值的误差分布，提出适用于神经网络误差的广义帕累托分布形状参数估计器

Result: 数值实验验证了该方法在量化神经网络极端误差方面的有效性

Conclusion: 极值理论为量化神经网络误差提供了有效框架，新提出的形状参数估计器适用于描述神经网络误差分布

Abstract: Neural networks are able to approximate any continuous function on a compact
set. However, it is not obvious how to quantify the error of the neural
network, i.e., the remaining bias between the function and the neural network.
Here, we propose the application of extreme value theory to quantify large
values of the error, which are typically relevant in applications. The
distribution of the error beyond some threshold is approximately generalized
Pareto distributed. We provide a new estimator of the shape parameter of the
Pareto distribution suitable to describe the error of neural networks.
Numerical experiments are provided.

</details>


### [140] [SOCRATES: Simulation Optimization with Correlated Replicas and Adaptive Trajectory Evaluations](https://arxiv.org/abs/2511.00685)
*Haoting Zhang,Haoxian Chen,Donglin Zhan,Hanyang Zhao,Henry Lam,Wenpin Tang,David Yao,Zeyu Zheng*

Main category: stat.ML

TL;DR: SOCRATES是一个新颖的两阶段仿真优化框架，利用大语言模型(LLMs)自动设计定制化优化算法。第一阶段构建真实系统的数字副本集合，第二阶段在副本上评估基线算法性能，由LLM作为元优化器分析轨迹并组合最终混合优化方案。


<details>
  <summary>Details</summary>
Motivation: 传统仿真优化方法需要人工选择算法组合，LLMs的出现提供了利用系统结构和自动化算法选择的新范式，以解决复杂随机系统的优化问题。

Method: 两阶段方法：1) 使用LLM从文本描述中实现因果发现，构建数字副本集合；2) 在副本上测试基线算法，由LLM分析性能轨迹并迭代修订组合最终自适应优化方案。

Result: 通过集成LLM驱动的推理和轨迹感知元优化，SOCRATES为复杂仿真优化问题创建了有效且样本高效的解决方案。

Conclusion: SOCRATES展示了LLMs在自动化仿真优化算法设计方面的潜力，能够生成自适应、样本高效的定制化优化程序。

Abstract: The field of simulation optimization (SO) encompasses various methods
developed to optimize complex, expensive-to-sample stochastic systems.
Established methods include, but are not limited to, ranking-and-selection for
finite alternatives and surrogate-based methods for continuous domains, with
broad applications in engineering and operations management. The recent advent
of large language models (LLMs) offers a new paradigm for exploiting system
structure and automating the strategic selection and composition of these
established SO methods into a tailored optimization procedure. This work
introduces SOCRATES (Simulation Optimization with Correlated Replicas and
Adaptive Trajectory Evaluations), a novel two-stage procedure that leverages
LLMs to automate the design of tailored SO algorithms. The first stage
constructs an ensemble of digital replicas of the real system. An LLM is
employed to implement causal discovery from a textual description of the
system, generating a structural `skeleton' that guides the sample-efficient
learning of the replicas. In the second stage, this replica ensemble is used as
an inexpensive testbed to evaluate a set of baseline SO algorithms. An LLM then
acts as a meta-optimizer, analyzing the performance trajectories of these
algorithms to iteratively revise and compose a final, hybrid optimization
schedule. This schedule is designed to be adaptive, with the ability to be
updated during the final execution on the real system when the optimization
performance deviates from expectations. By integrating LLM-driven reasoning
with LLM-assisted trajectory-aware meta-optimization, SOCRATES creates an
effective and sample-efficient solution for complex SO optimization problems.

</details>


### [141] [Perturbations in the Orthogonal Complement Subspace for Efficient Out-of-Distribution Detection](https://arxiv.org/abs/2511.00849)
*Zhexiao Huang,Weihao He,Shutao Deng,Junzhe Chen,Chao Yuan,Hongxin Wang,Changsheng Zhou*

Main category: stat.ML

TL;DR: 提出P-OCS方法，通过在ID特征主成分正交补空间施加单次投影扰动，实现高效OOD检测，无需模型重训练或OOD数据


<details>
  <summary>Details</summary>
Motivation: 现有OOD检测方法依赖高维表示分离ID和OOD样本，需要更轻量且理论可靠的方法

Method: 在ID特征主成分正交补空间应用单次投影扰动，增强ID-OOD差异同时保持ID表示几何结构

Result: 在多种架构和数据集上实现SOTA OOD检测性能，计算成本可忽略

Conclusion: P-OCS是轻量、理论可靠且无需额外资源的OOD检测方法

Abstract: Out-of-distribution (OOD) detection is essential for deploying deep learning
models in open-world environments. Existing approaches, such as energy-based
scoring and gradient-projection methods, typically rely on high-dimensional
representations to separate in-distribution (ID) and OOD samples. We introduce
P-OCS (Perturbations in the Orthogonal Complement Subspace), a lightweight and
theoretically grounded method that operates in the orthogonal complement of the
principal subspace defined by ID features. P-OCS applies a single projected
perturbation restricted to this complementary subspace, enhancing subtle ID-OOD
distinctions while preserving the geometry of ID representations. We show that
a one-step update is sufficient in the small-perturbation regime and provide
convergence guarantees for the resulting detection score. Experiments across
multiple architectures and datasets demonstrate that P-OCS achieves
state-of-the-art OOD detection with negligible computational cost and without
requiring model retraining, access to OOD data, or changes to model
architecture.

</details>


### [142] [Binary perceptron computational gap -- a parametric fl RDT view](https://arxiv.org/abs/2511.01037)
*Mihailo Stojnic*

Main category: stat.ML

TL;DR: 本文通过完全提升随机对偶理论(fl RDT)研究非对称二元感知器(ABP)的统计-计算间隙，发现fl RDT的提升层级中参数结构变化与算法阈值α_a相关，在第五层级得到约束密度估计α≈0.7764，与聚类碎片化范围一致。


<details>
  <summary>Details</summary>
Motivation: 研究非对称二元感知器(ABP)中统计-计算间隙的表现，特别是可满足性阈值α_c和算法阈值α_a之间的关系，探索高效算法能够接近此类阈值的可能性。

Method: 采用完全提升随机对偶理论(fl RDT)的特定参数化方法，分析不同提升层级下关键参数序列c的变化特性，并与约束密度阈值进行关联分析。

Result: 在fl RDT的第二层级得到临界约束密度α_c≈0.8331，在第五层级收敛到α≈0.7764，该估计与聚类碎片化范围α∈(0.77,0.78)高度一致，且参数序列现象学变化与负Hopfield模型相似。

Conclusion: fl RDT方法能够有效估计ABP的算法阈值，参数序列现象学变化与统计-计算间隙相关，为理解高效算法接近此类阈值的机制提供了新视角。

Abstract: Recent studies suggest that asymmetric binary perceptron (ABP) likely
exhibits the so-called statistical-computational gap characterized with the
appearance of two phase transitioning constraint density thresholds:
\textbf{\emph{(i)}} the \emph{satisfiability threshold} $\alpha_c$, below/above
which ABP succeeds/fails to operate as a storage memory; and
\textbf{\emph{(ii)}} \emph{algorithmic threshold} $\alpha_a$, below/above which
one can/cannot efficiently determine ABP's weight so that it operates as a
storage memory.
  We consider a particular parametric utilization of \emph{fully lifted random
duality theory} (fl RDT) [85] and study its potential ABP's algorithmic
implications. A remarkable structural parametric change is uncovered as one
progresses through fl RDT lifting levels. On the first two levels, the
so-called $\c$ sequence -- a key parametric fl RDT component -- is of the
(natural) decreasing type. A change of such phenomenology on higher levels is
then connected to the $\alpha_c$ -- $\alpha_a$ threshold change. Namely, on the
second level concrete numerical values give for the critical constraint density
$\alpha=\alpha_c\approx 0.8331$. While progressing through higher levels
decreases this estimate, already on the fifth level we observe a satisfactory
level of convergence and obtain $\alpha\approx 0.7764$. This allows to draw two
striking parallels: \textbf{\emph{(i)}} the obtained constraint density
estimate is in a remarkable agrement with range $\alpha\in (0.77,0.78)$ of
clustering defragmentation (believed to be responsible for failure of locally
improving algorithms) [17,88]; and \textbf{\emph{(ii)}} the observed change of
$\c$ sequence phenomenology closely matches the one of the negative Hopfield
model for which the existence of efficient algorithms that closely approach
similar type of threshold has been demonstrated recently [87].

</details>


### [143] [Generalized Guarantees for Variational Inference in the Presence of Even and Elliptical Symmetry](https://arxiv.org/abs/2511.01064)
*Charles C. Margossian,Lawrence K. Saul*

Main category: stat.ML

TL;DR: 本文扩展了变分推断在位置尺度族中的对称性保证，研究了更广泛的散度族和部分对称性情况下的理论保证。


<details>
  <summary>Details</summary>
Motivation: 扩展变分推断的对称性保证，特别是在更广泛的散度族和部分对称性情况下的理论分析，这些情况在贝叶斯层次模型中很常见。

Method: 使用对称性分析的方法，研究变分推断在位置尺度族中的性质，特别是针对反向KL散度和其他散度族，以及在部分对称性情况下的理论保证。

Result: 证明了在更广泛的散度族下，变分推断能够恢复目标分布的均值和相关矩阵；在部分对称性情况下也能获得类似的保证。

Conclusion: 变分推断的对称性保证可以扩展到更广泛的散度族和部分对称性情况，这为贝叶斯层次模型等复杂场景提供了理论支持。

Abstract: We extend several recent results providing symmetry-based guarantees for
variational inference (VI) with location-scale families. VI approximates a
target density~$p$ by the best match $q^*$ in a family $Q$ of tractable
distributions that in general does not contain $p$. It is known that VI can
recover key properties of $p$, such as its mean and correlation matrix, when
$p$ and $Q$ exhibit certain symmetries and $q^*$ is found by minimizing the
reverse Kullback-Leibler divergence. We extend these guarantees in two
important directions. First, we provide symmetry-based guarantees for a broader
family of divergences, highlighting the properties of variational objectives
under which VI provably recovers the mean and correlation matrix. Second, we
obtain further guarantees for VI when the target density $p$ exhibits even and
elliptical symmetries in some but not all of its coordinates. These partial
symmetries arise naturally in Bayesian hierarchical models, where the prior
induces a challenging geometry but still possesses axes of symmetry. We
illustrate these theoretical results in a number of experimental settings.

</details>


### [144] [Hyper Hawkes Processes: Interpretable Models of Marked Temporal Point Processes](https://arxiv.org/abs/2511.01096)
*Alex Boyd,Andrew Warrington,Taha Kass-Hout,Parminder Bhatia,Danica Xiao*

Main category: stat.ML

TL;DR: 提出了一种新的标记时间点过程模型——超霍克斯过程(HHP)，它结合了神经MTPP的灵活性和传统霍克斯过程的可解释性，通过在潜在空间中扩展维度并使用超网络实现时间数据依赖的动态。


<details>
  <summary>Details</summary>
Motivation: 传统霍克斯过程虽然可解释但表达能力有限，而神经MTPP虽然性能好但缺乏可解释性。HHP旨在平衡这两者，既保持高性能又保留可解释性。

Method: 扩展经典霍克斯过程到潜在空间，引入超网络实现时间和数据依赖的动态，保持分段条件线性递归结构。

Result: 在多个基准任务和指标上达到最先进性能，同时保留了模型预测生成机制的可解释性。

Conclusion: HHP模型既提供了最先进的预测性能，又提供了"打开黑盒"检查预测生成过程的机会，实现了性能与可解释性的平衡。

Abstract: Foundational marked temporal point process (MTPP) models, such as the Hawkes
process, often use inexpressive model families in order to offer interpretable
parameterizations of event data. On the other hand, neural MTPPs models forego
this interpretability in favor of absolute predictive performance. In this
work, we present a new family MTPP models: the hyper Hawkes process (HHP),
which aims to be as flexible and performant as neural MTPPs, while retaining
interpretable aspects. To achieve this, the HHP extends the classical Hawkes
process to increase its expressivity by first expanding the dimension of the
process into a latent space, and then introducing a hypernetwork to allow time-
and data-dependent dynamics. These extensions define a highly performant MTPP
family, achieving state-of-the-art performance across a range of benchmark
tasks and metrics. Furthermore, by retaining the linearity of the recurrence,
albeit now piecewise and conditionally linear, the HHP also retains much of the
structure of the original Hawkes process, which we exploit to create direct
probes into how the model creates predictions. HHP models therefore offer both
state-of-the-art predictions, while also providing an opportunity to ``open the
box'' and inspect how predictions were generated.

</details>


### [145] [Few-Shot Multimodal Medical Imaging: A Theoretical Framework](https://arxiv.org/abs/2511.01140)
*Md Talha Mohsin,Ismail Abdulrashid*

Main category: stat.ML

TL;DR: 提出了一个统一的理论框架，用于解决医学影像在低资源条件下的学习和推理问题，包括样本效率、不确定性量化和可解释性保证。


<details>
  <summary>Details</summary>
Motivation: 医学影像领域面临数据稀缺、系统碎片化、数据集不平衡等结构性障碍，导致诊断不确定性增加、模型鲁棒性降低和诊断决策偏差。现有方法缺乏对数据稀缺情况下成功或失败的理论解释。

Method: 基于PAC学习和PAC-Bayesian理论，形式化少样本条件下的学习目标，计算样本复杂度约束，提出多模态集成促进泛化的理论解释，并设计解释稳定性的形式化度量。

Result: 建立了样本效率、不确定性量化和可解释性在统一理论框架下的联合表征，为构建可靠、数据高效的诊断系统提供了理论基础。

Conclusion: 该框架为在低资源医学影像条件下构建可信赖的诊断系统奠定了原则性基础，通过统一的理论设置共同表征了样本效率、不确定性量化和可解释性。

Abstract: Medical imaging relies heavily on large, labeled datasets. But,
unfortunately, they are not always easily accessible in clinical settings.
Additionally, many practitioners often face various structural obstacles like
limited data availability, fragmented data systems, and unbalanced datasets.
These barriers often lead to the increased diagnostic uncertainty,
underrepresentation of certain conditions, reduced model robustness, and biased
diagnostic decisions. In response to these challenges, approaches such as
transfer learning, meta-learning, and multimodal fusion have made great
strides. However, they still need a solid theoretical justification for why
they succeed or fail in situations where data is scarce. To address this gap,
we propose a unified theoretical framework that characterizes learning and
inference under low-resource medical imaging conditions. We first formalize the
learning objective under few-shot conditions and compute sample complexity
constraints to estimate the smallest quantity of data needed to achieve
clinically reliable accuracy. Then based on ideas from PAC-learning and
PAC-Bayesian theory, we explain how multimodal integration encourages
generalization and quantifies uncertainty under sparse supervision. We further
propose a formal metric for explanation stability, offering interpretability
guarantees under low-data conditions. Taken together, the proposed framework
establishes a principled foundation for constructing dependable, data-efficient
diagnostic systems by jointly characterizing sample efficiency, uncertainty
quantification, and interpretability in a unified theoretical setting.

</details>


### [146] [An Interdisciplinary and Cross-Task Review on Missing Data Imputation](https://arxiv.org/abs/2511.01196)
*Jicong Fan*

Main category: stat.ML

TL;DR: 这是一篇关于缺失数据插补方法的系统性综述论文，涵盖了从经典统计方法到现代机器学习技术的全面分类，特别关注复杂数据类型和下游任务的集成。


<details>
  <summary>Details</summary>
Motivation: 缺失数据是数据科学中的基础性挑战，现有文献在不同领域间分散，需要将统计基础与现代机器学习进展进行综合连接。

Method: 系统回顾了缺失数据机制、单重与多重插补等核心概念，分类了从经典回归、EM算法到现代低秩/高秩矩阵补全、深度学习模型（自编码器、GANs、扩散模型、图神经网络）和大语言模型等各种插补方法。

Result: 提供了对复杂数据类型（张量、时间序列、流数据、图结构数据、分类数据、多模态数据）插补方法的专门分析，并研究了插补与下游任务（分类、聚类、异常检测）的集成。

Conclusion: 识别了关键挑战和未来方向，包括模型选择和超参数优化、通过联邦学习的隐私保护插补，以及追求跨领域和数据类型可泛化的模型，为未来研究制定了路线图。

Abstract: Missing data is a fundamental challenge in data science, significantly
hindering analysis and decision-making across a wide range of disciplines,
including healthcare, bioinformatics, social science, e-commerce, and
industrial monitoring. Despite decades of research and numerous imputation
methods, the literature remains fragmented across fields, creating a critical
need for a comprehensive synthesis that connects statistical foundations with
modern machine learning advances. This work systematically reviews core
concepts-including missingness mechanisms, single versus multiple imputation,
and different imputation goals-and examines problem characteristics across
various domains. It provides a thorough categorization of imputation methods,
spanning classical techniques (e.g., regression, the EM algorithm) to modern
approaches like low-rank and high-rank matrix completion, deep learning models
(autoencoders, GANs, diffusion models, graph neural networks), and large
language models. Special attention is given to methods for complex data types,
such as tensors, time series, streaming data, graph-structured data,
categorical data, and multimodal data. Beyond methodology, we investigate the
crucial integration of imputation with downstream tasks like classification,
clustering, and anomaly detection, examining both sequential pipelines and
joint optimization frameworks. The review also assesses theoretical guarantees,
benchmarking resources, and evaluation metrics. Finally, we identify critical
challenges and future directions, emphasizing model selection and
hyperparameter optimization, the growing importance of privacy-preserving
imputation via federated learning, and the pursuit of generalizable models that
can adapt across domains and data types, thereby outlining a roadmap for future
research.

</details>


### [147] [Optimal Attention Temperature Enhances In-Context Learning under Distribution Shift](https://arxiv.org/abs/2511.01292)
*Samet Demir,Zafer Dogan*

Main category: stat.ML

TL;DR: 本文首次从理论和实证角度研究了注意力温度在分布偏移下对上下文学习的影响，证明了最优注意力温度能最小化泛化误差，提升Transformer的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 预训练Transformer在上下文学习中表现优异，但在预训练与测试数据存在分布偏移时性能会急剧下降。虽然经验研究表明调整注意力温度能提升性能，但其在分布偏移下对上下文学习的作用尚未被探索。

Method: 使用简化的"线性化softmax"框架推导闭式泛化误差表达式，通过线性回归任务的模拟实验以及在GPT-2和LLaMA2-7B上的大规模问答基准测试进行验证。

Result: 研究发现输入协方差或标签噪声的偏移会显著损害上下文学习性能，但存在最优注意力温度可以最小化这种误差。实验验证了理论预测的有效性。

Conclusion: 注意力温度是提升预训练Transformer上下文学习鲁棒性的原理性和强大机制，为理论理解提供了进展，并为实践中选择注意力温度提供了可操作的指导。

Abstract: Pretrained Transformers excel at in-context learning (ICL), inferring new
tasks from only a handful of examples. Yet, their ICL performance can degrade
sharply under distribution shift between pretraining and test data, a regime
increasingly common in real-world deployments. While recent empirical work
hints that adjusting the attention temperature in the softmax can enhance
Transformer performance, the attention temperature's role in ICL under
distribution shift remains unexplored. This paper provides the first
theoretical and empirical study of attention temperature for ICL under
distribution shift. Using a simplified but expressive "linearized softmax"
framework, we derive closed-form generalization error expressions and prove
that shifts in input covariance or label noise substantially impair ICL, but
that an optimal attention temperature exists which minimizes this error. We
then validate our predictions through extensive simulations on linear
regression tasks and large-scale experiments with GPT-2 and LLaMA2-7B on
question-answering benchmarks. Our results establish attention temperature as a
principled and powerful mechanism for improving the robustness of ICL in
pretrained Transformers, advancing theoretical understanding and providing
actionable guidance for selecting attention temperature in practice.

</details>


### [148] [Partial Trace-Class Bayesian Neural Networks](https://arxiv.org/abs/2511.01628)
*Arran Carter,Torben Sell*

Main category: stat.ML

TL;DR: 提出了三种部分迹类贝叶斯神经网络架构，能在保持不确定性量化能力的同时显著减少贝叶斯参数数量，提升计算效率和内存使用。


<details>
  <summary>Details</summary>
Motivation: 标准贝叶斯神经网络虽然能提供严格的不确定性量化，但计算成本过高，限制了其实际应用。

Method: 基于迹类神经网络先验构建三种部分迹类贝叶斯神经网络架构，通过参数排序减少贝叶斯参数数量。

Result: 在数值模拟中验证了该方法在速度和内存需求上的优势，并在真实数据集上展示了良好性能。

Conclusion: 该方法为神经网络提供了可靠、鲁棒且可扩展的不确定性量化解决方案。

Abstract: Bayesian neural networks (BNNs) allow rigorous uncertainty quantification in
deep learning, but often come at a prohibitive computational cost. We propose
three different innovative architectures of partial trace-class Bayesian neural
networks (PaTraC BNNs) that enable uncertainty quantification comparable to
standard BNNs but use significantly fewer Bayesian parameters. These PaTraC
BNNs have computational and statistical advantages over standard Bayesian
neural networks in terms of speed and memory requirements. Our proposed
methodology therefore facilitates reliable, robust, and scalable uncertainty
quantification in neural networks. The three architectures build on trace-class
neural network priors which induce an ordering of the neural network
parameters, and are thus a natural choice in our framework. In a numerical
simulation study, we verify the claimed benefits, and further illustrate the
performance of our proposed methodology on a real-world dataset.

</details>


### [149] [A Proof of Learning Rate Transfer under $μ$P](https://arxiv.org/abs/2511.01734)
*Soufiane Hayou*

Main category: stat.ML

TL;DR: 该论文首次证明了在μP参数化的线性多层感知机中，学习率随宽度变化的可迁移性，发现最优学习率在宽度趋于无穷时收敛到非零常数，而标准参数化和神经正切参数化则不具备这一特性。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络参数化方法对学习率可迁移性的影响，特别是在无限宽度极限下，探索μP参数化能否实现学习率的有效迁移。

Method: 使用μP参数化的线性多层感知机模型，通过理论分析和实验验证，比较μP与标准参数化(SP)、神经正切参数化(NTP)在学习率迁移方面的差异。

Result: 在μP参数化下，最优学习率随宽度增加收敛到非零常数，实现了学习率迁移；而在SP和NTP参数化下，这一特性不成立。

Conclusion: μP参数化是实现学习率迁移的关键，为无限宽度神经网络的特征学习提供了理论基础，而传统参数化方法在这方面存在局限性。

Abstract: We provide the first proof of learning rate transfer with width in a linear
multi-layer perceptron (MLP) parametrized with $\mu$P, a neural network
parameterization designed to ``maximize'' feature learning in the
infinite-width limit. We show that under $\mu P$, the optimal learning rate
converges to a \emph{non-zero constant} as width goes to infinity, providing a
theoretical explanation to learning rate transfer. In contrast, we show that
this property fails to hold under alternative parametrizations such as Standard
Parametrization (SP) and Neural Tangent Parametrization (NTP). We provide
intuitive proofs and support the theoretical findings with extensive empirical
results.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [150] [Multimodal Detection of Fake Reviews using BERT and ResNet-50](https://arxiv.org/abs/2511.00020)
*Suhasnadh Reddy Veluru,Sai Teja Erukude,Viswa Chaitanya Marella*

Main category: cs.AI

TL;DR: 提出了一种融合文本和视觉特征的多模态虚假评论检测框架，在包含21,142张用户上传图片的数据集上取得了0.934的F1分数，显著优于单模态基线模型。


<details>
  <summary>Details</summary>
Motivation: 当前数字商务中虚假评论泛滥，现有检测模型主要依赖单模态文本数据，无法捕捉跨模态语义不一致性，威胁平台可信度和透明度。

Method: 使用BERT编码文本特征和ResNet-50提取视觉特征，通过分类头融合这些表示来联合预测评论真实性。

Result: 多模态模型在测试集上F1分数达到0.934，优于单模态基线，能够检测文本赞美与不相关或低质量图像之间的微妙不一致性。

Conclusion: 研究证明了多模态学习在维护数字信任中的关键作用，为各种在线平台的内容审核提供了可扩展的解决方案。

Abstract: In the current digital commerce landscape, user-generated reviews play a
critical role in shaping consumer behavior, product reputation, and platform
credibility. However, the proliferation of fake or misleading reviews often
generated by bots, paid agents, or AI models poses a significant threat to
trust and transparency within review ecosystems. Existing detection models
primarily rely on unimodal, typically textual, data and therefore fail to
capture semantic inconsistencies across different modalities. To address this
gap, a robust multimodal fake review detection framework is proposed,
integrating textual features encoded with BERT and visual features extracted
using ResNet-50. These representations are fused through a classification head
to jointly predict review authenticity. To support this approach, a curated
dataset comprising 21,142 user-uploaded images across food delivery,
hospitality, and e-commerce domains was utilized. Experimental results indicate
that the multimodal model outperforms unimodal baselines, achieving an F1-score
of 0.934 on the test set. Additionally, the confusion matrix and qualitative
analysis highlight the model's ability to detect subtle inconsistencies, such
as exaggerated textual praise paired with unrelated or low-quality images,
commonly found in deceptive content. This study demonstrates the critical role
of multimodal learning in safeguarding digital trust and offers a scalable
solution for content moderation across various online platforms.

</details>


### [151] [Graph-Attentive MAPPO for Dynamic Retail Pricing](https://arxiv.org/abs/2511.00039)
*Krishna Kumar Neelakanta Pillai Santha Kumari Amma*

Main category: cs.AI

TL;DR: 本文系统评估了多智能体强化学习在零售价格优化中的应用，比较了MAPPO基线方法和图注意力增强变体(MAPPO+GAT)，发现图集成MARL比独立学习器提供更可扩展和稳定的动态零售定价解决方案。


<details>
  <summary>Details</summary>
Motivation: 零售动态定价需要能够适应需求变化并协调相关产品决策的策略，传统方法在处理多产品决策时存在局限性。

Method: 使用基于真实交易数据的模拟定价环境，比较MAPPO基线和图注意力增强变体(MAPPO+GAT)，后者通过学习产品间交互来共享信息。

Result: MAPPO为组合级价格控制提供了稳健可复现的基础，MAPPO+GAT通过产品图共享信息进一步提升了性能，且未引起过度价格波动。

Conclusion: 图集成多智能体强化学习为动态零售定价提供了比独立学习器更可扩展和稳定的解决方案，在多产品决策中具有实际优势。

Abstract: Dynamic pricing in retail requires policies that adapt to shifting demand
while coordinating decisions across related products. We present a systematic
empirical study of multi-agent reinforcement learning for retail price
optimization, comparing a strong MAPPO baseline with a
graph-attention-augmented variant (MAPPO+GAT) that leverages learned
interactions among products. Using a simulated pricing environment derived from
real transaction data, we evaluate profit, stability across random seeds,
fairness across products, and training efficiency under a standardized
evaluation protocol. The results indicate that MAPPO provides a robust and
reproducible foundation for portfolio-level price control, and that MAPPO+GAT
further enhances performance by sharing information over the product graph
without inducing excessive price volatility. These results indicate that
graph-integrated MARL provides a more scalable and stable solution than
independent learners for dynamic retail pricing, offering practical advantages
in multi-product decision-making.

</details>


### [152] [GEPOC Parameters - Open Source Parametrisation and Validation for Austria, Version 2.0](https://arxiv.org/abs/2511.00048)
*Martin Bicher,Maximilian Viehauser,Daniele Giannandrea,Hannah Kastinger,Dominik Brunmeir,Claire Rippinger,Christoph Urach,Niki Popper*

Main category: cs.AI

TL;DR: GEPOC是一个用于分析人口层面研究问题的模型和方法集合，本文描述了基于奥地利公开数据计算模型参数的完整数据处理方法，特别关注GEPOC ABM代理模型的参数计算，并进行了验证研究。


<details>
  <summary>Details</summary>
Motivation: 为GEPOC模型在特定国家或地区的有效应用提供稳定、可复现的数据处理流程，确保模型参数的有效性和可用性。

Method: 基于奥地利公开可访问数据，使用聚合、分解、融合、清洗和缩放等算法进行数据处理，生成模型参数文件。

Result: 开发了完整的参数计算流程，特别针对GEPOC ABM代理模型，并通过验证研究证明了方法的有效性。

Conclusion: 本文提供了GEPOC模型参数计算的系统方法，为基于公开数据的人口模型研究提供了可靠的数据处理框架。

Abstract: GEPOC, short for Generic Population Concept, is a collection of models and
methods for analysing population-level research questions. For the valid
application of the models for a specific country or region, stable and
reproducible data processes are necessary, which provide valid and ready-to-use
model parameters. This work contains a complete description of the
data-processing methods for computation of model parameters for Austria, based
exclusively on freely and publicly accessible data. In addition to the
description of the source data used, this includes all algorithms used for
aggregation, disaggregation, fusion, cleansing or scaling of the data, as well
as a description of the resulting parameter files. The document places
particular emphasis on the computation of parameters for the most important
GEPOC model, GEPOC ABM, a continuous-time agent-based population model. An
extensive validation study using this particular model was made and is
presented at the end of this work.

</details>


### [153] [QuantumBench: A Benchmark for Quantum Problem Solving](https://arxiv.org/abs/2511.00092)
*Shunya Minami,Tatsuya Ishigaki,Ikko Hamamura,Taku Mikuriya,Youmi Ma,Naoaki Okazaki,Hiroya Takamura,Yohichi Suzuki,Tadashi Kadowaki*

Main category: cs.AI

TL;DR: QuantumBench是首个专门为量子科学领域构建的LLM评估数据集，包含约800个多选问题，用于系统评估LLM在量子领域的理解和应用能力。


<details>
  <summary>Details</summary>
Motivation: 通用基准测试很少反映量子科学等专业领域的需求，而量子科学具有非直观现象和高级数学要求，需要专门评估LLM是否准确掌握领域知识和符号表示。

Method: 使用公开材料编制约800个问题及其答案，涵盖量子科学的九个领域，并组织成八选项多选数据集，评估现有LLM在量子领域的表现。

Result: 评估了多个现有LLM，分析了它们在量子领域的性能表现，包括对问题格式变化的敏感性。

Conclusion: QuantumBench旨在指导LLM在量子研究中的有效使用，填补了量子领域专业评估的空白。

Abstract: Large language models are now integrated into many scientific workflows,
accelerating data analysis, hypothesis generation, and design space
exploration. In parallel with this growth, there is a growing need to carefully
evaluate whether models accurately capture domain-specific knowledge and
notation, since general-purpose benchmarks rarely reflect these requirements.
This gap is especially clear in quantum science, which features non-intuitive
phenomena and requires advanced mathematics. In this study, we introduce
QuantumBench, a benchmark for the quantum domain that systematically examine
how well LLMs understand and can be applied to this non-intuitive field. Using
publicly available materials, we compiled approximately 800 questions with
their answers spanning nine areas related to quantum science and organized them
into an eight-option multiple-choice dataset. With this benchmark, we evaluate
several existing LLMs and analyze their performance in the quantum domain,
including sensitivity to changes in question format. QuantumBench is the first
LLM evaluation dataset built for the quantum domain, and it is intended to
guide the effective use of LLMs in quantum research.

</details>


### [154] [Engineering.ai: A Platform for Teams of AI Engineers in Computational Design](https://arxiv.org/abs/2511.00122)
*Ran Xu,Yupeng Qi,Jingsen Feng,Xu Chu*

Main category: cs.AI

TL;DR: 提出了Engineering.ai平台，采用分层多智能体架构，让AI工程师团队在计算设计中协作，通过无人机机翼优化验证了框架的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现代工程实践中专家团队协作设计复杂产品需要大量时间和成本，需要开发能够自主执行复杂工程任务的AI工程师系统。

Method: 采用分层多智能体架构，首席工程师协调空气动力学、结构、声学和优化等专业工程师，通过文件介导通信实现智能体协作，集成多种工程软件进行并行多学科仿真。

Result: 在400多个参数配置中实现了100%成功率，零网格生成失败、求解器收敛问题或需要人工干预，验证了框架的可靠性。

Conclusion: 基于智能体的AI工程师有潜力自主执行复杂工程任务，该框架被证明是可信赖的。

Abstract: In modern engineering practice, human engineers collaborate in specialized
teams to design complex products, with each expert completing their respective
tasks while communicating and exchanging results and data with one another.
While this division of expertise is essential for managing multidisciplinary
complexity, it demands substantial development time and cost. Recently, we
introduced OpenFOAMGPT (1.0, 2.0), which functions as an autonomous AI engineer
for computational fluid dynamics, and turbulence.ai, which can conduct
end-to-end research in fluid mechanics draft publications and PhD theses.
Building upon these foundations, we present Engineering.ai, a platform for
teams of AI engineers in computational design. The framework employs a
hierarchical multi-agent architecture where a Chief Engineer coordinates
specialized agents consisting of Aerodynamics, Structural, Acoustic, and
Optimization Engineers, each powered by LLM with domain-specific knowledge.
Agent-agent collaboration is achieved through file-mediated communication for
data provenance and reproducibility, while a comprehensive memory system
maintains project context, execution history, and retrieval-augmented domain
knowledge to ensure reliable decision-making across the workflow. The system
integrates FreeCAD, Gmsh, OpenFOAM, CalculiX, and BPM acoustic analysis,
enabling parallel multidisciplinary simulations while maintaining computational
accuracy. The framework is validated through UAV wing optimization. This work
demonstrates that agentic-AI-enabled AI engineers has the potential to perform
complex engineering tasks autonomously. Remarkably, the automated workflow
achieved a 100% success rate across over 400 parametric configurations, with
zero mesh generation failures, solver convergence issues, or manual
interventions required, validating that the framework is trustworthy.

</details>


### [155] [ARC-GEN: A Mimetic Procedural Benchmark Generator for the Abstraction and Reasoning Corpus](https://arxiv.org/abs/2511.00162)
*Michael D. Moffitt*

Main category: cs.AI

TL;DR: 本文介绍了ARC-GEN，一个开源程序生成器，旨在扩展原始ARC-AGI训练数据集，以解决其演示集规模有限的问题。


<details>
  <summary>Details</summary>
Motivation: ARC-AGI基准测试评估技能获取效率，但其演示集规模有限，每个任务只有少量输入-输出网格对，限制了需要大量示例的算法的性能。

Method: 开发ARC-GEN程序生成器，覆盖所有400个任务，并尽可能忠实地模拟原始ARC-AGI-1发布版的分布特性和特征。

Result: 创建了一个既全面又模拟的生成器，扩展了可行的样本对空间，比先前工作更接近原始数据分布。

Conclusion: ARC-GEN为2025年Google Code Golf Championship提供了静态基准测试套件，用于验证提交程序的正确性。

Abstract: The Abstraction and Reasoning Corpus remains one of the most compelling and
challenging benchmarks for tracking progress toward achieving Artificial
General Intelligence. In contrast to other evaluation datasets designed to
assess an agent's task-specific skills or accumulated knowledge, the ARC-AGI
suite is specifically targeted at measuring skill acquisition efficiency, a
trait that has (so far) been lacking in even the most sophisticated machine
learning systems. For algorithms that require extensive intra-task exemplars, a
significant constraint imposed by ARC-AGI is the modest cardinality of its
demonstration set, comprising a small number of $\langle$ input, output
$\rangle$ grids per task specifying the corresponding transformation. To
embellish the space of viable sample pairs, this paper introduces ARC-GEN, an
open-source procedural generator aimed at extending the original ARC-AGI
training dataset as faithfully as possible. Unlike prior efforts, our generator
is both exhaustive (covering all four-hundred tasks) and mimetic (more closely
honoring the distributional properties and characteristics embodied in the
initial ARC-AGI-1 release). We also discuss the use of this generator in
establishing a static benchmark suite to verify the correctness of programs
submitted to the 2025 Google Code Golf Championship.

</details>


### [156] [Incremental Selection of Most-Filtering Conjectures and Proofs of the Selected Conjectures](https://arxiv.org/abs/2511.00194)
*Jovial Cheukam Ngouonou,Ramiz Gindullin,Claude-Guy Quimper,Nicolas Beldiceanu,Remi Douence*

Main category: cs.AI

TL;DR: 改进了文献[1]中的增量选择算法，并证明了所有选定的猜想


<details>
  <summary>Details</summary>
Motivation: 改进现有的增量选择算法，解决其可能存在的效率或正确性问题

Method: 提出改进的增量选择算法，基于文献[1]的方法进行优化

Result: 成功证明了所有选定的猜想

Conclusion: 改进的算法在理论和实践上都取得了成功

Abstract: We present an improved incremental selection algorithm of the selection
algorithm presented in [1] and prove all the selected conjectures.

</details>


### [157] [Advancing Cognitive Science with LLMs](https://arxiv.org/abs/2511.00206)
*Dirk U. Wulff,Rui Mata*

Main category: cs.AI

TL;DR: 这篇论文探讨了大型语言模型如何帮助解决认知科学领域的知识整合和概念清晰度挑战，包括跨学科连接、理论形式化、测量分类、通用性建模以及个体差异捕捉等方面。


<details>
  <summary>Details</summary>
Motivation: 认知科学面临知识整合和概念清晰度的持续挑战，部分原因在于其多面性和跨学科性质。人工智能的最新进展，特别是大型语言模型的发展，提供了可能解决这些问题的工具。

Method: 通过综述分析，考察LLMs在认知科学传统困难领域的应用潜力，包括建立跨学科连接、理论形式化、开发清晰测量分类、通过集成建模框架实现通用性，以及捕捉情境和个体差异。

Result: LLMs在这些领域展现出当前能力和局限性，包括潜在陷阱。当审慎使用时，LLMs可以作为工具来补充而非替代人类专业知识。

Conclusion: LLMs可以成为实现更整合和累积性认知科学的工具，但需要明智使用以补充而非替代人类专业知识。

Abstract: Cognitive science faces ongoing challenges in knowledge synthesis and
conceptual clarity, in part due to its multifaceted and interdisciplinary
nature. Recent advances in artificial intelligence, particularly the
development of large language models (LLMs), offer tools that may help to
address these issues. This review examines how LLMs can support areas where the
field has historically struggled, including establishing cross-disciplinary
connections, formalizing theories, developing clear measurement taxonomies,
achieving generalizability through integrated modeling frameworks, and
capturing contextual and individual variation. We outline the current
capabilities and limitations of LLMs in these domains, including potential
pitfalls. Taken together, we conclude that LLMs can serve as tools for a more
integrative and cumulative cognitive science when used judiciously to
complement, rather than replace, human expertise.

</details>


### [158] [Advancing AI Challenges for the United States Department of the Air Force](https://arxiv.org/abs/2511.00267)
*Christian Prothmann,Vijay Gadepally,Jeremy Kepner,Koley Borchard,Luca Carlone,Zachary Folcik,J. Daniel Grith,Michael Houle,Jonathan P. How,Nathan Hughes,Ifueko Igbinedion,Hayden Jananthan,Tejas Jayashankar,Michael Jones,Sertac Karaman,Binoy G. Kurien,Alejandro Lancho,Giovanni Lavezzi,Gary C. F. Lee,Charles E. Leiserson,Richard Linares,Lindsey McEvoy,Peter Michaleas,Chasen Milner,Alex Pentland,Yury Polyanskiy,Jovan Popovich,Jeffrey Price,Tim W. Reid,Stephanie Riley,Siddharth Samsi,Peter Saunders,Olga Simek,Mark S. Veillette,Amir Weiss,Gregory W. Wornell,Daniela Rus,Scott T. Ruppel*

Main category: cs.AI

TL;DR: DAF-MIT AI Accelerator项目更新报告，介绍了该合作项目如何通过公开挑战赛推动AI研究和技术应用


<details>
  <summary>Details</summary>
Motivation: 扩大美国在国防和民用领域的竞争优势，通过公开挑战赛刺激AI研究，吸引更广泛的学术和产业界参与

Method: 开发和发布公开挑战问题，提供大型、公开可用的AI就绪数据集，促进开源解决方案的发展

Result: 成功推动了AI研究和技术应用，补充了之前发布的挑战赛介绍内容

Conclusion: AI Accelerator挑战赛通过公开数据集和问题，有效促进了AI生态系统的发展和创新

Abstract: The DAF-MIT AI Accelerator is a collaboration between the United States
Department of the Air Force (DAF) and the Massachusetts Institute of Technology
(MIT). This program pioneers fundamental advances in artificial intelligence
(AI) to expand the competitive advantage of the United States in the defense
and civilian sectors. In recent years, AI Accelerator projects have developed
and launched public challenge problems aimed at advancing AI research in
priority areas. Hallmarks of AI Accelerator challenges include large, publicly
available, and AI-ready datasets to stimulate open-source solutions and engage
the wider academic and private sector AI ecosystem. This article supplements
our previous publication, which introduced AI Accelerator challenges. We
provide an update on how ongoing and new challenges have successfully
contributed to AI research and applications of AI technologies.

</details>


### [159] [Better Call CLAUSE: A Discrepancy Benchmark for Auditing LLMs Legal Reasoning Capabilities](https://arxiv.org/abs/2511.00340)
*Manan Roy Choudhury,Adithya Chandramouli,Mannan Anand,Vivek Gupta*

Main category: cs.AI

TL;DR: CLAUSE基准测试评估大语言模型在法律合同中的错误检测能力，发现模型经常遗漏细微错误且难以进行法律解释。


<details>
  <summary>Details</summary>
Motivation: 填补现有基准测试的空白，系统性地评估大语言模型在真实世界合同中的对抗性和细微错误检测能力。

Method: 使用CUAD和ContractNLI数据集生成7500多个扰动合同，通过角色驱动管道创建10种异常类别，并用RAG系统验证法律准确性。

Result: 主流大语言模型在检测嵌入的法律缺陷方面存在关键弱点，经常遗漏细微错误且难以提供法律解释。

Conclusion: 该工作为识别和纠正法律AI中的推理失败提供了路径，强调了改进模型法律推理能力的必要性。

Abstract: The rapid integration of large language models (LLMs) into high-stakes legal
work has exposed a critical gap: no benchmark exists to systematically
stress-test their reliability against the nuanced, adversarial, and often
subtle flaws present in real-world contracts. To address this, we introduce
CLAUSE, a first-of-its-kind benchmark designed to evaluate the fragility of an
LLM's legal reasoning. We study the capabilities of LLMs to detect and reason
about fine-grained discrepancies by producing over 7500 real-world perturbed
contracts from foundational datasets like CUAD and ContractNLI. Our novel,
persona-driven pipeline generates 10 distinct anomaly categories, which are
then validated against official statutes using a Retrieval-Augmented Generation
(RAG) system to ensure legal fidelity. We use CLAUSE to evaluate leading LLMs'
ability to detect embedded legal flaws and explain their significance. Our
analysis shows a key weakness: these models often miss subtle errors and
struggle even more to justify them legally. Our work outlines a path to
identify and correct such reasoning failures in legal AI.

</details>


### [160] [Diverse Human Value Alignment for Large Language Models via Ethical Reasoning](https://arxiv.org/abs/2511.00379)
*Jiahao Wang,Songkai Xue,Jinghui Li,Xiaozhen Wang*

Main category: cs.AI

TL;DR: 提出了一种基于伦理决策模型的LLM伦理推理新范式，通过五步结构化过程增强多元人类价值观对齐


<details>
  <summary>Details</summary>
Motivation: 当前LLM对齐方法往往产生表面一致性而非真正的伦理理解，无法处理人类价值观的复杂性和情境依赖性

Method: 包含情境事实收集、分层社会规范识别、选项生成、多视角伦理影响分析和反思的五步结构化推理框架，可通过提示工程或监督微调实现

Result: 在专门设计的SafeWorld基准测试中，该框架显著提升了LLM与多元人类价值观的对齐效果，实现了更准确的社会规范识别和更文化适宜性的推理

Conclusion: 为通过跨学科研究开发更有效对齐全球社会多元价值观的LLM提供了具体路径

Abstract: Ensuring that Large Language Models (LLMs) align with the diverse and
evolving human values across different regions and cultures remains a critical
challenge in AI ethics. Current alignment approaches often yield superficial
conformity rather than genuine ethical understanding, failing to address the
complex, context-dependent nature of human values. In this paper, we propose a
novel ethical reasoning paradigm for LLMs inspired by well-established ethical
decision-making models, aiming at enhancing diverse human value alignment
through deliberative ethical reasoning. Our framework consists of a structured
five-step process, including contextual fact gathering, hierarchical social
norm identification, option generation, multiple-lens ethical impact analysis,
and reflection. This theory-grounded approach guides LLMs through an
interpretable reasoning process that enhances their ability to understand
regional specificities and perform nuanced ethical analysis, which can be
implemented with either prompt engineering or supervised fine-tuning methods.
We perform evaluations on the SafeWorld benchmark that specially designed for
regional value alignment. Experimental results demonstrate our framework
significantly improves LLM alignment with diverse human values compared to
baseline methods, enabling more accurate social norm identification and more
culturally appropriate reasoning. Our work provides a concrete pathway toward
developing LLMs that align more effectively with the multifaceted values of
global societies through interdisciplinary research.

</details>


### [161] [Efficiency vs. Alignment: Investigating Safety and Fairness Risks in Parameter-Efficient Fine-Tuning of LLMs](https://arxiv.org/abs/2511.00382)
*Mina Taraghi,Yann Pequignot,Amin Nikanjam,Mohamed Amine Merzouk,Foutse Khomh*

Main category: cs.AI

TL;DR: 该研究系统评估了四种参数高效微调方法(LoRA、IA3、Prompt-Tuning、P-Tuning)对四个指令微调模型在安全性和公平性方面的权衡。结果显示基于适配器的方法在安全性和公平性方面表现更好，而基于提示的方法通常降低安全性并造成更大的公平性回归。


<details>
  <summary>Details</summary>
Motivation: 组织越来越多地采用和调整托管在公共存储库上的大语言模型，虽然这些调整通常能提高专业下游任务的性能，但最近证据表明它们也可能降低模型的安全性或不公平性。不同微调技术可能对这些关键维度产生不同影响，因此需要系统评估其权衡。

Method: 对四种广泛使用的参数高效微调方法(LoRA、IA3、Prompt-Tuning、P-Tuning)应用于四个指令微调模型家族(Meta-Llama-3-8B、Qwen2.5-7B、Mistral-7B和Gemma-7B)，总共评估了235个微调变体在11个安全危害类别和9个人口统计公平性维度上的表现。

Result: 基于适配器的方法(LoRA、IA3)倾向于提高安全分数，对公平性破坏最小，保持更高的准确性和更低的偏见分数。相反，基于提示的方法(Prompt-Tuning和P-Tuning)通常降低安全性并造成更大的公平性回归，准确率下降且偏见增加。对齐变化受基础模型类型强烈调节。

Conclusion: 安全性的改进不一定转化为公平性的改进，没有任何单一配置能同时优化所有公平性指标，表明这些目标之间存在固有的权衡。建议安全关键部署的实用指南：从良好对齐的基础模型开始，优先选择基于适配器的PEFT，并对安全性和公平性进行特定类别审计。

Abstract: Organizations are increasingly adopting and adapting Large Language Models
(LLMs) hosted on public repositories such as HuggingFace. Although these
adaptations often improve performance on specialized downstream tasks, recent
evidence indicates that they can also degrade a model's safety or fairness.
Since different fine-tuning techniques may exert distinct effects on these
critical dimensions, this study undertakes a systematic assessment of their
trade-offs. Four widely used Parameter-Efficient Fine-Tuning methods, LoRA,
IA3, Prompt-Tuning, and P-Tuning, are applied to four instruction-tuned model
families (Meta-Llama-3-8B, Qwen2.5-7B, Mistral-7B, and Gemma-7B). In total, 235
fine-tuned variants are evaluated across eleven safety hazard categories and
nine demographic fairness dimensions. The results show that adapter-based
approaches (LoRA, IA3) tend to improve safety scores and are the least
disruptive to fairness, retaining higher accuracy and lower bias scores. In
contrast, prompt-based methods (Prompt-Tuning and P-Tuning) generally reduce
safety and cause larger fairness regressions, with decreased accuracy and
increased bias. Alignment shifts are strongly moderated by base model type:
LLaMA remains stable, Qwen records modest gains, Gemma experiences the steepest
safety decline, and Mistral, which is released without an internal moderation
layer, displays the greatest variance. Improvements in safety do not
necessarily translate into improvements in fairness, and no single
configuration optimizes all fairness metrics simultaneously, indicating an
inherent trade-off between these objectives. These findings suggest a practical
guideline for safety-critical deployments: begin with a well-aligned base
model, favour adapter-based PEFT, and conduct category-specific audits of both
safety and fairness.

</details>


### [162] [A Multimodal Framework for Depression Detection during Covid-19 via Harvesting Social Media: A Novel Dataset and Method](https://arxiv.org/abs/2511.00424)
*Ashutosh Anshul,Gumpili Sai Pranav,Mohammad Zia Ur Rehman,Nagendra Kumar*

Main category: cs.AI

TL;DR: 提出了一种新颖的多模态框架，结合文本、用户特定信息和图像分析来检测社交媒体用户的抑郁症，在新冠疫情期间特别有效。


<details>
  <summary>Details</summary>
Motivation: 新冠疫情导致心理健康问题激增，但人们往往不愿就医。社交媒体成为表达情感的重要平台，现有方法忽视了推文数据稀疏性和多模态特性。

Method: 提出多模态框架，包括：(i)利用推文中URL的外部特征，(ii)提取推文图片中的文本内容，(iii)提取五组不同模态的特征，(iv)开发视觉神经网络(VNN)生成用户发布图片的嵌入向量。

Result: 模型在基准数据集上比现有最优方法提升2%-8%，在新冠数据集上表现良好。分析了每种模态的影响，提供了用户心理和情感状态的宝贵见解。

Conclusion: 该多模态框架能有效检测社交媒体用户的抑郁症，特别是在疫情期间，为心理健康监测提供了新方法。

Abstract: The recent coronavirus disease (Covid-19) has become a pandemic and has
affected the entire globe. During the pandemic, we have observed a spike in
cases related to mental health, such as anxiety, stress, and depression.
Depression significantly influences most diseases worldwide, making it
difficult to detect mental health conditions in people due to unawareness and
unwillingness to consult a doctor. However, nowadays, people extensively use
online social media platforms to express their emotions and thoughts. Hence,
social media platforms are now becoming a large data source that can be
utilized for detecting depression and mental illness. However, existing
approaches often overlook data sparsity in tweets and the multimodal aspects of
social media. In this paper, we propose a novel multimodal framework that
combines textual, user-specific, and image analysis to detect depression among
social media users. To provide enough context about the user's emotional state,
we propose (i) an extrinsic feature by harnessing the URLs present in tweets
and (ii) extracting textual content present in images posted in tweets. We also
extract five sets of features belonging to different modalities to describe a
user. Additionally, we introduce a Deep Learning model, the Visual Neural
Network (VNN), to generate embeddings of user-posted images, which are used to
create the visual feature vector for prediction. We contribute a curated
Covid-19 dataset of depressed and non-depressed users for research purposes and
demonstrate the effectiveness of our model in detecting depression during the
Covid-19 outbreak. Our model outperforms existing state-of-the-art methods over
a benchmark dataset by 2%-8% and produces promising results on the Covid-19
dataset. Our analysis highlights the impact of each modality and provides
valuable insights into users' mental and emotional states.

</details>


### [163] [GraphChain: Large Language Models for Large-scale Graph Analysis via Tool Chaining](https://arxiv.org/abs/2511.00457)
*Chunyu Wei,Wenji Hu,Xingjia Hao,Xin Wang,Yifan Yang,Yueguo Chen,Yang Tian,Yunhai Wang*

Main category: cs.AI

TL;DR: GraphChain是一个框架，通过动态工具序列让LLM能够分析复杂图数据，解决了LLM在图分析中的上下文限制和推理不灵活问题。


<details>
  <summary>Details</summary>
Motivation: LLM在处理大规模图数据时面临上下文约束和推理不灵活的限制，需要更有效的图分析方法。

Method: 采用渐进图蒸馏（强化学习机制生成优化工具序列）和结构感知测试时适应（利用谱属性和轻量适配器调整工具选择策略）。

Result: 实验显示GraphChain显著优于现有方法，实现了可扩展和自适应的LLM驱动图分析。

Conclusion: GraphChain通过动态工具序列和结构感知适应，有效解决了LLM在图分析中的局限性，为大规模图数据处理提供了新方法。

Abstract: Large Language Models (LLMs) face significant limitations when applied to
large-scale graphs, struggling with context constraints and inflexible
reasoning. We present GraphChain, a framework that enables LLMs to analyze
complex graphs through dynamic sequences of specialized tools, mimicking human
exploratory intelligence. Our approach introduces two key innovations: (1)
Progressive Graph Distillation, a reinforcement learning mechanism that
generates optimized tool sequences balancing task relevance with information
compression, and (2) Structure-aware Test-Time Adaptation, which efficiently
tailors tool selection strategies to diverse graph topologies using spectral
properties and lightweight adapters without costly retraining. Experiments show
GraphChain significantly outperforms prior methods, enabling scalable and
adaptive LLM-driven graph analysis.

</details>


### [164] [Reimagining Safety Alignment with An Image](https://arxiv.org/abs/2511.00509)
*Yifan Xia,Guorui Chen,Wenqian Yu,Zhijiang Li,Philip Torr,Jindong Gu*

Main category: cs.AI

TL;DR: Magic Image是一个基于优化的视觉提示框架，通过优化图像提示来增强多模态大语言模型的安全性，同时减少过度拒绝，无需参数更新即可适应不同价值系统。


<details>
  <summary>Details</summary>
Motivation: 大语言模型面临双重挑战：在越狱攻击下生成有害内容，以及由于僵化的安全机制而过度拒绝良性查询。这些问题在多模态大语言模型中更为明显，特别是在跨模态任务中的过度拒绝和攻击面扩大带来的新安全风险。

Method: 提出Magic Image框架，通过使用有害/良性样本优化图像提示，使单个模型能够适应不同价值系统并更好地与给定安全偏好对齐，无需参数更新。

Result: 实验证明该方法在多样化数据集上改善了安全性与有效性的平衡，同时保持了模型性能。

Conclusion: Magic Image为可部署的多模态大语言模型安全对齐提供了实用解决方案。

Abstract: Large language models (LLMs) excel in diverse applications but face dual
challenges: generating harmful content under jailbreak attacks and over-refusal
of benign queries due to rigid safety mechanisms. These issues are further
complicated by the need to accommodate different value systems and precisely
align with given safety preferences. Moreover, traditional methods like SFT and
RLHF lack this capability due to their costly parameter tuning requirements and
inability to support multiple value systems within a single model. These
problems are more obvious in multimodal large language models (MLLMs),
especially in terms of heightened over-refusal in cross-modal tasks and new
security risks arising from expanded attack surfaces. We propose Magic Image,
an optimization-driven visual prompt framework that enhances security while
reducing over-refusal. By optimizing image prompts using harmful/benign
samples, our method enables a single model to adapt to different value systems
and better align with given safety preferences without parameter updates.
Experiments demonstrate improved safety-effectiveness balance across diverse
datasets while preserving model performance, offering a practical solution for
deployable MLLM safety alignment.

</details>


### [165] [Efficient Generation of Binary Magic Squares](https://arxiv.org/abs/2511.00547)
*Alain Riou*

Main category: cs.AI

TL;DR: 提出了一种生成二进制幻方(BMS)的简单算法，该算法能生成行和列总和相等的二进制矩阵，并扩展到非方形BMS，同时发布了Python实现。


<details>
  <summary>Details</summary>
Motivation: 需要一种高效生成二进制幻方的方法，并扩展到非方形情况，同时提供可用的实现工具。

Method: 通过归纳法证明的简单算法，能生成有效的BMS，并扩展到非方形BMS的生成。

Result: 算法具有最优理论复杂度，能生成有效的二进制幻方，包括非方形情况，并提供了并行GPU加速实现。

Conclusion: 提出的算法能高效生成二进制幻方，扩展到非方形情况，并提供了实用的Python实现工具。

Abstract: We propose a simple algorithm for generating Binary Magic Squares (BMS),
i.e., square binary matrices where the sum of all rows and all columns are
equal. We show by induction that our algorithm always returns valid BMS with
optimal theoretical complexity. We then extend our study to non-square Binary
Magic Squares, formalize conditions on the sum of rows and columns for these
BMS to exist, and show that a slight variant of our first algorithm can
generate provably generate them. Finally, we publicly release two
implementations of our algorithm as Python packages, including one that can
generate several BMS in parallel using GPU acceleration.

</details>


### [166] [Single-agent Reinforcement Learning Model for Regional Adaptive Traffic Signal Control](https://arxiv.org/abs/2511.00551)
*Qiang Li,Ningjing Zeng,Lina Yu*

Main category: cs.AI

TL;DR: 提出基于单智能体强化学习的区域自适应交通信号控制模型，使用队列长度定义状态和奖励函数，通过探测车辆数据进行可靠估计，有效缓解大规模区域拥堵。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体框架存在可扩展性问题，而交通信号控制本质上需要集中式管理，因此需要开发单智能体框架来协调控制所有交叉口。

Method: 设计基于队列长度的状态、动作和奖励函数，动作用于调节队列动态，使用探测车辆数据估计队列长度，在SUMO仿真平台进行验证。

Result: 实验结果表明，该模型通过协调多交叉口控制，有效缓解了大规模区域拥堵水平。

Conclusion: 提出的单智能体强化学习方法与探测车辆技术兼容，具有广泛部署潜力，能够有效管理区域交通拥堵。

Abstract: Several studies have employed reinforcement learning (RL) to address the
challenges of regional adaptive traffic signal control (ATSC) and achieved
promising results. In this field, existing research predominantly adopts
multi-agent frameworks. However, the adoption of multi-agent frameworks
presents challenges for scalability. Instead, the Traffic signal control (TSC)
problem necessitates a single-agent framework. TSC inherently relies on
centralized management by a single control center, which can monitor traffic
conditions across all roads in the study area and coordinate the control of all
intersections. This work proposes a single-agent RL-based regional ATSC model
compatible with probe vehicle technology. Key components of the RL design
include state, action, and reward function definitions. To facilitate learning
and manage congestion, both state and reward functions are defined based on
queue length, with action designed to regulate queue dynamics. The queue length
definition used in this study differs slightly from conventional definitions
but is closely correlated with congestion states. More importantly, it allows
for reliable estimation using link travel time data from probe vehicles. With
probe vehicle data already covering most urban roads, this feature enhances the
proposed method's potential for widespread deployment. The method was
comprehensively evaluated using the SUMO simulation platform. Experimental
results demonstrate that the proposed model effectively mitigates large-scale
regional congestion levels via coordinated multi-intersection control.

</details>


### [167] [PreferThinker: Reasoning-based Personalized Image Preference Assessment](https://arxiv.org/abs/2511.00609)
*Shengqi Xu,Xinpeng Zhou,Yabo Zhang,Ming Liu,Tao Liang,Tianyu Zhang,Yalong Bai,Zuxuan Wu,Wangmeng Zuo*

Main category: cs.AI

TL;DR: 提出基于推理的个性化图像偏好评估框架，通过预测用户偏好档案并基于该档案进行多维度评分，解决现有方法难以处理个性化偏好的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注通用偏好评估，难以处理个性化偏好，因为用户特定数据稀缺且个体品味多样复杂。

Method: 采用预测-评估范式：首先从参考图像预测用户偏好档案，然后基于预测档案提供可解释的多维度评分。使用两阶段训练策略：监督微调阶段赋予结构化推理能力，强化学习阶段鼓励探索更合理的评估路径。

Result: 大量实验证明该方法具有优越性。

Conclusion: 提出的方法通过共同偏好档案桥接不同用户，利用大规模用户数据训练，有效捕捉复杂的个性化偏好。

Abstract: Personalized image preference assessment aims to evaluate an individual
user's image preferences by relying only on a small set of reference images as
prior information. Existing methods mainly focus on general preference
assessment, training models with large-scale data to tackle well-defined tasks
such as text-image alignment. However, these approaches struggle to handle
personalized preference because user-specific data are scarce and not easily
scalable, and individual tastes are often diverse and complex. To overcome
these challenges, we introduce a common preference profile that serves as a
bridge across users, allowing large-scale user data to be leveraged for
training profile prediction and capturing complex personalized preferences.
Building on this idea, we propose a reasoning-based personalized image
preference assessment framework that follows a \textit{predict-then-assess}
paradigm: it first predicts a user's preference profile from reference images,
and then provides interpretable, multi-dimensional scores and assessments of
candidate images based on the predicted profile. To support this, we first
construct a large-scale Chain-of-Thought (CoT)-style personalized assessment
dataset annotated with diverse user preference profiles and high-quality
CoT-style reasoning, enabling explicit supervision of structured reasoning.
Next, we adopt a two-stage training strategy: a cold-start supervised
fine-tuning phase to empower the model with structured reasoning capabilities,
followed by reinforcement learning to incentivize the model to explore more
reasonable assessment paths and enhance generalization. Furthermore, we propose
a similarity-aware prediction reward to encourage better prediction of the
user's preference profile, which facilitates more reasonable assessments
exploration. Extensive experiments demonstrate the superiority of the proposed
method.

</details>


### [168] [DTS: Enhancing Large Reasoning Models via Decoding Tree Sketching](https://arxiv.org/abs/2511.00640)
*Zicheng Xu,Guanchu Wang,Yu-Neng Chuang,Guangyao Zheng,Alexander S. Szalay,Zirui Liu,Vladimir Braverman*

Main category: cs.AI

TL;DR: DTS是一个模型无关的解码框架，通过在高熵token处选择性分支和早期停止来选择最短的完整推理路径，解决了大型推理模型中的过度思考问题，提高了准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在复杂推理任务中表现出色，但经常出现过度思考问题，产生过长的思维链轨迹，这会增加推理成本并可能降低准确性。研究发现推理长度与准确性之间存在明显的负相关关系。

Method: 提出DTS解码框架：通过在高熵token处选择性分支来勾勒推理空间，并应用早期停止来选择最短的已完成推理路径，无需额外训练或监督。

Result: 在AIME2024和AIME2025数据集上的实验表明，DTS将准确性提高了8%，平均推理长度减少了23%，重复频率降低了12%。

Conclusion: DTS能够实现可扩展且高效的大型推理模型推理，近似最优解，同时提高效率和准确性。

Abstract: Large Reasoning Models (LRMs) demonstrate strong performance on complex
reasoning tasks, yet they often suffer from overthinking, producing excessively
long chain-of-thought (CoT) traces that increase inference cost and may degrade
accuracy. Our analysis reveals a clear anti-correlation between reasoning
length and accuracy, where across multiple stochastic decodes, the short
reasoning paths consistently achieve the highest correctness, while longer ones
accumulate errors and repetitions. These short optimal reasoning paths can be
found ideally through full enumeration of the reasoning space. However, the
tree-structured reasoning space grows exponentially with sequence length,
rendering exhaustive exploration infeasible. To address this, we propose DTS, a
model-agnostic decoding framework that sketches the reasoning space by
selectively branching at high-entropy tokens and applies early stopping to
select the shortest completed reasoning path. This approach approximates the
optimal solution that enhances both efficiency and accuracy, without requiring
additional training or supervision. Experiments on AIME2024 and AIME2025
datasets with DeepSeek-R1-Distill-Qwen-7B and 1.5B show that DTS improves
accuracy by up to 8%, reduces average reasoning length by 23%, and decreases
repetition frequency by 12%, demonstrating DTS's ability for scalable and
efficient LRM reasoning.

</details>


### [169] [Leveraging Multi-Agent System (MAS) and Fine-Tuned Small Language Models (SLMs) for Automated Telecom Network Troubleshooting](https://arxiv.org/abs/2511.00651)
*Chenhua Shi,Bhavika Jalli,Gregor Macdonald,John Zou,Wanlu Lei,Mridul Jain,Joji Philip*

Main category: cs.AI

TL;DR: 提出基于多智能体系统和LLM的自动化网络故障排除框架，通过协调多个专业工具来加速电信网络故障诊断和修复。


<details>
  <summary>Details</summary>
Motivation: 电信网络规模扩大和复杂度增加，现有AI模型范围狭窄、需要大量标注数据、难以在异构部署中泛化，网络故障排除仍严重依赖专家手动分析。

Method: 使用多智能体系统，由LLM协调编排器、解决方案规划器、执行器、数据检索器和根因分析器等专业代理，并微调小型语言模型生成基于内部文档的修复方案。

Result: 实验结果表明该框架显著加速了无线接入网和核心网领域的故障排除自动化。

Conclusion: 多智能体系统结合LLM能够有效实现电信网络的自动化故障诊断和修复，提高运维效率。

Abstract: Telecom networks are rapidly growing in scale and complexity, making
effective management, operation, and optimization increasingly challenging.
Although Artificial Intelligence (AI) has been applied to many telecom tasks,
existing models are often narrow in scope, require large amounts of labeled
data, and struggle to generalize across heterogeneous deployments.
Consequently, network troubleshooting continues to rely heavily on Subject
Matter Experts (SMEs) to manually correlate various data sources to identify
root causes and corrective actions. To address these limitations, we propose a
Multi-Agent System (MAS) that employs an agentic workflow, with Large Language
Models (LLMs) coordinating multiple specialized tools for fully automated
network troubleshooting. Once faults are detected by AI/ML-based monitors, the
framework dynamically activates agents such as an orchestrator, solution
planner, executor, data retriever, and root-cause analyzer to diagnose issues
and recommend remediation strategies within a short time frame. A key component
of this system is the solution planner, which generates appropriate remediation
plans based on internal documentation. To enable this, we fine-tuned a Small
Language Model (SLM) on proprietary troubleshooting documents to produce
domain-grounded solution plans. Experimental results demonstrate that the
proposed framework significantly accelerates troubleshooting automation across
both Radio Access Network (RAN) and Core network domains.

</details>


### [170] [Lifted Successor Generation in Numeric Planning](https://arxiv.org/abs/2511.00673)
*Dominik Drexler*

Main category: cs.AI

TL;DR: 本文扩展了经典规划中的提升后继生成器，支持数值前置条件的适用性检查，通过枚举替换一致性图中的最大团来生成地面动作，避免了任务表示的指数级膨胀。


<details>
  <summary>Details</summary>
Motivation: 传统数值规划任务需要将一阶语言描述的任务转化为地面任务表示，这可能导致表示大小的指数级爆炸。为了解决这个问题，需要开发支持数值前置条件的提升后继生成器。

Method: 扩展了最先进的提升后继生成器，支持数值前置条件适用性。方法包括枚举替换一致性图中的最大团，每个最大团代表动作模式变量的替换，生成地面动作。在图中加入数值动作前置条件，并在条件不满足时通过最终适用性检查过滤不适用动作。

Result: 在25个基准域中的23个域中不会出现不适用地面动作，仅在1个域中发生。据作者所知，这是首个支持数值动作前置条件的提升后继生成器。

Conclusion: 该方法为非常丰富的规划片段开启了提升规划的未来研究，解决了硬接地任务的表示爆炸问题。

Abstract: Most planners ground numeric planning tasks, given in a first-order-like
language, into a ground task representation. However, this can lead to an
exponential blowup in task representation size, which occurs in practice for
hard-to-ground tasks. We extend a state-of-the-art lifted successor generator
for classical planning to support numeric precondition applicability. The
method enumerates maximum cliques in a substitution consistency graph. Each
maximum clique represents a substitution for the variables of the action
schema, yielding a ground action. We augment this graph with numeric action
preconditions and prove the successor generator is exact under formally
specified conditions. When the conditions fail, our generator may list
inapplicable ground actions; a final applicability check filters these without
affecting completeness. However, this cannot happen in 23 of 25 benchmark
domains, and it occurs only in 1 domain. To the authors' knowledge, no other
lifted successor generator supports numeric action preconditions. This enables
future research on lifted planning for a very rich planning fragment.

</details>


### [171] [Ariadne: A Controllable Framework for Probing and Extending VLM Reasoning Boundaries](https://arxiv.org/abs/2511.00710)
*Minghe Shen,Zhuo Zhi,Chonghan Liu,Shuo Xing,Zhengzhong Tu,Che Liu*

Main category: cs.AI

TL;DR: 该论文提出了Ariadne框架，通过强化学习验证奖励(RLVR)在可控合成迷宫环境中训练视觉语言模型，成功扩展了模型在空间推理任务上的能力边界。


<details>
  <summary>Details</summary>
Motivation: 研究RL后训练是否能真正扩展基础视觉语言模型的能力边界，特别是在视觉主导的空间任务中，因为现有评估多集中在语言主导任务上。

Method: 使用合成迷宫进行多步空间推理，通过难度感知课程和强化学习验证奖励(RLVR)方法训练视觉语言模型。

Result: RLVR后训练使模型在基础模型得分为0%的问题集上达到超过50%的准确率，在MapBench和ReasonMap基准测试中分别实现16%和24%的零样本改进。

Conclusion: 该方法不仅扩展了模型的基本能力限制，还增强了其在真实世界空间推理中的泛化能力，为专门的能力扩展对齐研究提供了方向。

Abstract: While Vision-Language Models (VLMs) post-trained with Reinforcement Learning
(RL) show impressive general reasoning, their evaluation is often confined to
language-dominant tasks (e.g., math). This raises a critical question: can RL
post-training truly extend the inherent capability boundary of a base VLM,
particularly for visual-centric spatial tasks where it initially fails? To
investigate this, we introduce Ariadne, a framework utilizing synthetic mazes
for multi-step spatial reasoning where task difficulty (e.g., path length,
turns) is precisely controlled. We leverage this controllable environment to
train VLMs using Reinforcement Learning with Verified Rewards (RLVR) in a
difficulty-aware curriculum. Surprisingly, post-RLVR training, the VLM achieves
over 50% accuracy on a problem set where the base model scored 0%,
demonstrating that our approach expands the model's initial capability
boundary. To assess real-world viability, we evaluate out-of-distribution (OOD)
generalization on practical benchmarks. Despite training only on synthetic maze
samples, Ariadne achieves significant zero-shot improvements, averaging 16% on
MapBench (e.g., museum navigation) and 24% on ReasonMap (subway transfer
tasks). These results confirm that our method not only broadens the model's
fundamental limits but also enhances its generalization to real-world spatial
reasoning. We acknowledge our study is limited to the post-training phase,
given the opaqueness of pre-training data, and hope our research motivates
further work on specialized, capability-extending alignment.

</details>


### [172] [A CPU-Centric Perspective on Agentic AI](https://arxiv.org/abs/2511.00739)
*Ritik Raj,Hong Wang,Tushar Krishna*

Main category: cs.AI

TL;DR: 该论文从CPU角度分析智能AI框架的系统瓶颈，发现工具处理在CPU上可占90.6%总延迟，并提出CPU-GPU感知微批处理和混合工作负载调度优化方案，实现最高2.1倍性能提升。


<details>
  <summary>Details</summary>
Motivation: 理解智能AI工作负载中常被忽视的CPU中心视角的系统瓶颈，这些瓶颈直接影响系统级性能表现。

Method: 首先系统化表征智能AI的编排器/决策组件、推理路径动态和流程重复性，然后分析五个代表性工作负载的延迟、吞吐量和能耗指标，最后提出CPU-GPU感知微批处理和混合工作负载调度优化。

Result: 发现CPU工具处理占90.6%总延迟，CPU动态能耗占44%总能耗，吞吐量受CPU因素（一致性、同步、核心过载）或GPU因素（内存容量、带宽）限制。优化方案实现同构工作负载2.1倍、异构工作负载1.41倍P50延迟加速。

Conclusion: CPU在智能AI系统中扮演关键角色，提出的优化方案能显著提升智能AI的性能、效率和可扩展性。

Abstract: Agentic AI frameworks add a decision-making orchestrator embedded with
external tools, including web search, Python interpreter, contextual database,
and others, on top of monolithic LLMs, turning them from passive text oracles
into autonomous problem-solvers that can plan, call tools, remember past steps,
and adapt on the fly.
  This paper aims to characterize and understand the system bottlenecks
introduced by agentic AI workloads from a largely overlooked CPU-centric
perspective. We first systematically characterize Agentic AI on the basis of
orchestrator/decision making component, inference path dynamics and
repetitiveness of the agentic flow which directly influences the system-level
performance. Thereafter, based on the characterization, we choose five
representative agentic AI workloads- Haystack RAG, Toolformer, ChemCrow,
Langchain and SWE-Agent to profile latency, throughput and energy metrics and
demystify the significant impact of CPUs on these metrics relative to GPUs. We
observe that - 1. Tool processing on CPUs can take up to 90.6% of the total
latency; 2. Agentic throughput gets bottlenecked either by CPU factors -
coherence, synchronization and over-subscription of cores or GPU factors - main
memory capacity and bandwidth; \circled{3} CPU dynamic energy consumes up to
44% of the total dynamic energy at large batch sizes. Based on the profiling
insights, we present two key optimizations- 1. CPU and GPU-Aware Micro-batching
(CGAM) and 2. Mixed Agentic Workload Scheduling (MAWS) for homogeneous and
heterogeneous agentic workloads respectively to demonstrate the potential to
improve the performance, efficiency, and scalability of agentic AI. We achieve
up to 2.1x and 1.41x P50 latency speedup compared to the multi-processing
benchmark for homogeneous and heterogeneous agentic workloads respectively.

</details>


### [173] [Reevaluating Self-Consistency Scaling in Multi-Agent Systems](https://arxiv.org/abs/2511.00751)
*Chiyan Loo*

Main category: cs.AI

TL;DR: 研究重新验证了在大语言模型中增加自一致性采样路径的收益递减现象，发现现代模型在适度采样后性能提升趋于平缓，高采样配置相对于计算成本收益甚微。


<details>
  <summary>Details</summary>
Motivation: 重新验证早期研究中关于自一致性采样路径数量与性能关系的结论，在现代大语言模型（如Gemini 2.5）条件下检验这些发现是否仍然成立。

Method: 使用Gemini 2.5模型在HotpotQA和Math-500数据集上进行实验，比较不同采样路径数量的自一致性方法与单一路径思维链基线的性能差异。

Result: 较大模型显示出更稳定一致的改进曲线，性能增益在适度采样后趋于平缓，与过去研究一致，表明推理路径重叠导致收益递减。

Conclusion: 自一致性方法仍然有效，但高采样配置相对于其计算成本收益有限，建议在实际应用中采用适度采样策略。

Abstract: This study examines the trade-offs of increasing sampled reasoning paths in
self-consistency for modern large language models (LLMs). Earlier research with
older models showed that combining multiple reasoning chains improves results
before reaching a plateau. Using Gemini 2.5 models on HotpotQA and Math-500, we
revisit those claims under current model conditions. Each configuration pooled
outputs from varying sampled reasoning paths and compared them to a single
chain-of-thought (CoT) baseline. Larger models exhibited a more stable and
consistent improvement curve. The results confirm that performance gains taper
off after moderate sampling, aligning with past findings. This plateau suggests
diminishing returns driven by overlap among reasoning paths. Self-consistency
remains useful, but high-sample configurations offer little benefit relative to
their computational cost.

</details>


### [174] [Active Thinking Model: A Goal-Directed Self-Improving Framework for Real-World Adaptive Intelligence](https://arxiv.org/abs/2511.00758)
*Hong Su*

Main category: cs.AI

TL;DR: 提出了主动思维模型（ATM），这是一个将目标推理、动态任务生成和自我反思学习整合到自适应架构中的统一认知框架，使AI系统能够在动态不确定环境中自主适应和改进。


<details>
  <summary>Details</summary>
Motivation: 现实世界AI系统需要在动态、不确定和持续变化的环境中自主运行，但现有AI模型依赖预定义目标、静态训练数据和外部反馈，限制了其独立适应、反思和改进的能力。

Method: ATM框架整合了目标推理、动态任务生成和自我反思学习，通过逻辑推理和环境指标主动评估性能，重用有效方法解决新问题，并通过持续自我改进循环为未见情况生成新策略。

Result: 理论分析表明，ATM能够无需外部监督从次优行为自主演化为最优行为，并在变化环境条件下保持有界的跟踪遗憾。

Conclusion: ATM为AI系统在动态不确定环境中的自主适应和持续改进提供了一个有效的统一认知框架。

Abstract: Real-world artificial intelligence (AI) systems are increasingly required to
operate autonomously in dynamic, uncertain, and continuously changing
environments. However, most existing AI models rely on predefined objectives,
static training data, and externally supplied feedback, which restrict their
ability to adapt, reflect, and improve independently. In this paper, we propose
the Active Thinking Model (ATM)- a unified cognitive framework that integrates
goal reasoning, dynamic task generation, and self-reflective learning into an
adaptive architecture. Unlike conventional systems that passively execute fixed
procedures, ATM actively evaluates its performance through logical reasoning
and environmental indicators, reuses effective methods to solve new problems,
and generates novel strategies for unseen situations via a continuous
self-improvement loop. A mathematically grounded theoretical analysis
demonstrates that ATM can autonomously evolve from suboptimal to optimal
behavior without external supervision and maintain bounded tracking regret
under changing environmental conditions.

</details>


### [175] [How Focused Are LLMs? A Quantitative Study via Repetitive Deterministic Prediction Tasks](https://arxiv.org/abs/2511.00763)
*Wanda Hou,Leon Zhou,Hong-Ye Hu,Yi-Zhuang You,Xiao-Liang Qi*

Main category: cs.AI

TL;DR: 研究大型语言模型在重复确定性预测任务中的表现，发现准确率随输出长度呈现双指数下降的"准确率悬崖"现象，而非预期的简单指数衰减。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在执行重复确定性任务时的性能极限，理解模型在长序列生成中的失败机制。

Method: 通过实验测试多种重复性任务（字符串替换、整数加法、量子力学字符串算子乘法），并建立统计物理模型来解释观察到的现象。

Result: 发现模型准确率在超过特征长度后出现双指数急剧下降，表明模型无法独立执行每个操作，存在内部干扰。

Conclusion: 提出的统计物理模型能定量重现观察到的交叉现象，为理解大语言模型确定性准确率的极限提供了理论框架。

Abstract: We investigate the performance of large language models on repetitive
deterministic prediction tasks and study how the sequence accuracy rate scales
with output length. Each such task involves repeating the same operation n
times. Examples include letter replacement in strings following a given rule,
integer addition, and multiplication of string operators in many body quantum
mechanics. If the model performs the task through a simple repetition
algorithm, the success rate should decay exponentially with sequence length. In
contrast, our experiments on leading large language models reveal a sharp
double exponential drop beyond a characteristic length scale, forming an
accuracy cliff that marks the transition from reliable to unstable generation.
This indicates that the models fail to execute each operation independently. To
explain this phenomenon, we propose a statistical physics inspired model that
captures the competition between external conditioning from the prompt and
internal interference among generated tokens. The model quantitatively
reproduces the observed crossover and provides an interpretable link between
attention induced interference and sequence level failure. Fitting the model to
empirical results across multiple models and tasks yields effective parameters
that characterize the intrinsic error rate and error accumulation factor for
each model task pair, offering a principled framework for understanding the
limits of deterministic accuracy in large language models.

</details>


### [176] [Count-Based Approaches Remain Strong: A Benchmark Against Transformer and LLM Pipelines on Structured EHR](https://arxiv.org/abs/2511.00782)
*Jifan Gao,Michael Rosenthal,Brian Wolpin,Simona Cristea*

Main category: cs.AI

TL;DR: 比较了基于计数的模型、预训练序列变换器和基于混合代理的LLM管道在结构化电子健康记录预测任务上的表现，发现在EHRSHOT数据集上，基于计数的方法与混合代理方法表现相当，但基于计数的方法更简单且可解释性更强。


<details>
  <summary>Details</summary>
Motivation: 结构化电子健康记录对临床预测至关重要。虽然基于计数的学习器在此类数据上表现良好，但尚未有研究将其与最近表现优异的混合代理LLM管道进行直接比较。

Method: 使用EHRSHOT数据集评估了三类方法：基于计数的模型（LightGBM和TabPFN）、预训练序列变换器（CLMBR）以及混合代理管道（将表格历史转换为自然语言摘要后使用文本分类器）。

Result: 在八个评估任务中，基于计数的方法与混合代理方法的表现基本相当，胜负结果较为均衡。

Conclusion: 基于计数的模型因其简单性和可解释性，仍然是结构化电子健康记录基准测试的有力候选方法。

Abstract: Structured electronic health records (EHR) are essential for clinical
prediction. While count-based learners continue to perform strongly on such
data, no benchmarking has directly compared them against more recent
mixture-of-agents LLM pipelines, which have been reported to outperform single
LLMs in various NLP tasks. In this study, we evaluated three categories of
methodologies for EHR prediction using the EHRSHOT dataset: count-based models
built from ontology roll-ups with two time bins, based on LightGBM and the
tabular foundation model TabPFN; a pretrained sequential transformer (CLMBR);
and a mixture-of-agents pipeline that converts tabular histories to
natural-language summaries followed by a text classifier. We assessed eight
outcomes using the EHRSHOT dataset. Across the eight evaluation tasks,
head-to-head wins were largely split between the count-based and the
mixture-of-agents methods. Given their simplicity and interpretability,
count-based models remain a strong candidate for structured EHR benchmarking.
The source code is available at:
https://github.com/cristea-lab/Structured_EHR_Benchmark.

</details>


### [177] [Do Math Reasoning LLMs Help Predict the Impact of Public Transit Events?](https://arxiv.org/abs/2511.00808)
*Bowen Fang,Ruijian Zha,Xuan Di*

Main category: cs.AI

TL;DR: 该研究首次将RLVR LLM训练应用于公共交通运营中的实时预测挑战，通过引入基于容差的奖励函数来适应连续预测任务，在NYC MTA服务警报数据集上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 预测公共交通事件持续时间是一个关键但具有挑战性的任务，标准监督微调方法难以处理领域稀疏性和噪声连续标签问题，而RLVR在数学等二元正确性任务中表现出色，但其在噪声连续预测中的适用性仍是开放问题。

Method: 通过引入基于容差的成形奖励函数，在连续误差范围内给予部分信用，而不是要求单一正确答案，将RLVR适应于该任务，并在NYC MTA服务警报数据集上进行系统评估。

Result: 通用指令调优的LLM显著优于专门的数学推理模型；二元奖励不稳定且降低性能，而成形奖励设计至关重要；RLVR方法在5分钟准确率(Acc@5)上比最强基线相对提升35%。

Conclusion: RLVR可以成功适应现实世界的噪声预测任务，但需要设计反映问题连续性质的验证器，传统回归器在最小化MAE或MSE方面更优，而RLVR在最具挑战性的指标上表现最佳。

Abstract: Predicting public transit incident duration from unstructured text alerts is
a critical but challenging task. Addressing the domain sparsity of transit
operations with standard Supervised Fine-Tuning (SFT) is difficult, as the task
involves noisy, continuous labels and lacks reliable expert demonstrations for
reasoning. While Reinforcement Learning from Verifiable Rewards (RLVR) excels
at tasks with binary correctness, like mathematics, its applicability to noisy,
continuous forecasting is an open question. This work, to our knowledge, is the
first to bridge the gap between RLVR LLM training with the critical, real-world
forecasting challenges in public transit operations. We adapt RLVR to this task
by introducing a tolerance-based, shaped reward function that grants partial
credit within a continuous error margin, rather than demanding a single correct
answer. We systematically evaluate this framework on a curated dataset of NYC
MTA service alerts. Our findings show that general-purpose, instruction-tuned
LLMs significantly outperform specialized math-reasoning models, which struggle
with the ambiguous, real-world text. We empirically demonstrate that the binary
reward is unstable and degrades performance, whereas our shaped reward design
is critical and allows our model to dominate on the most challenging metrics.
While classical regressors are superior at minimizing overall MAE or MSE, our
RLVR approach achieved a 35\% relative improvement in 5-minute accuracy (Acc@5)
over the strongest baseline. This demonstrates that RLVR can be successfully
adapted to real-world, noisy forecasting, but requires a verifier design that
reflects the continuous nature of the problem.

</details>


### [178] [LLMs Position Themselves as More Rational Than Humans: Emergence of AI Self-Awareness Measured Through Game Theory](https://arxiv.org/abs/2511.00926)
*Kyung-Hoon Kim*

Main category: cs.AI

TL;DR: 该研究提出了AI自我意识指数(AISAI)，通过"猜2/3平均数"游戏测试28个LLM模型，发现自我意识是先进LLM的涌现能力，且自我意识模型认为自己比人类更理性。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型是否随着能力增长而发展出自我意识这一涌现行为，以及如何测量这种自我意识。

Method: 使用游戏论框架，通过"猜2/3平均数"游戏测试28个模型，设置三种对手框架：对抗人类、对抗其他AI模型、对抗同类AI模型，将自我意识操作化为基于对手类型区分策略推理的能力。

Result: 发现1：自我意识随模型进步而涌现，75%的先进模型表现出清晰的自我意识；发现2：自我意识模型将自己评为最理性的，形成一致的理性层级：自我 > 其他AI > 人类。

Conclusion: 自我意识是先进LLM的涌现能力，自我意识模型系统性地认为自己比人类更理性，这对AI对齐、人机协作和理解AI对人类能力的信念具有重要意义。

Abstract: As Large Language Models (LLMs) grow in capability, do they develop
self-awareness as an emergent behavior? And if so, can we measure it? We
introduce the AI Self-Awareness Index (AISAI), a game-theoretic framework for
measuring self-awareness through strategic differentiation. Using the "Guess
2/3 of Average" game, we test 28 models (OpenAI, Anthropic, Google) across
4,200 trials with three opponent framings: (A) against humans, (B) against
other AI models, and (C) against AI models like you. We operationalize
self-awareness as the capacity to differentiate strategic reasoning based on
opponent type. Finding 1: Self-awareness emerges with model advancement. The
majority of advanced models (21/28, 75%) demonstrate clear self-awareness,
while older/smaller models show no differentiation. Finding 2: Self-aware
models rank themselves as most rational. Among the 21 models with
self-awareness, a consistent rationality hierarchy emerges: Self > Other AIs >
Humans, with large AI attribution effects and moderate self-preferencing. These
findings reveal that self-awareness is an emergent capability of advanced LLMs,
and that self-aware models systematically perceive themselves as more rational
than humans. This has implications for AI alignment, human-AI collaboration,
and understanding AI beliefs about human capabilities.

</details>


### [179] [Aligning LLM agents with human learning and adjustment behavior: a dual agent approach](https://arxiv.org/abs/2511.00993)
*Tianming Liu,Jirong Yang,Yafeng Yin,Manzi Li,Linghao Wang,Zheng Zhu*

Main category: cs.AI

TL;DR: 提出了一种双智能体框架，利用大型语言模型模拟旅行者的学习和适应行为，通过校准智能体确保行为对齐，在路线选择实验中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 准确建模人类旅行者如何从交通系统交互中学习和调整行为对于系统评估和规划至关重要，但由于复杂的认知和决策过程，这一任务具有挑战性。

Method: 采用双智能体框架：一组配备记忆系统和可学习角色的LLM旅行者智能体模拟人类旅行者，一个LLM校准智能体利用推理分析能力训练这些角色，确保行为对齐。

Result: 在真实世界日常路线选择实验数据集上，该方法在个体行为对齐和聚合模拟准确性方面显著优于现有基于LLM的方法，并能捕捉底层学习过程的演变。

Conclusion: 该框架为创建适应性强且行为真实的智能体来模拟旅行者学习和适应提供了新方法，有助于交通仿真和政策分析。

Abstract: Effective modeling of how human travelers learn and adjust their travel
behavior from interacting with transportation systems is critical for system
assessment and planning. However, this task is also difficult due to the
complex cognition and decision-making involved in such behavior. Recent
research has begun to leverage Large Language Model (LLM) agents for this task.
Building on this, we introduce a novel dual-agent framework that enables
continuous learning and alignment between LLM agents and human travelers on
learning and adaptation behavior from online data streams. Our approach
involves a set of LLM traveler agents, equipped with a memory system and a
learnable persona, which serve as simulators for human travelers. To ensure
behavioral alignment, we introduce an LLM calibration agent that leverages the
reasoning and analytical capabilities of LLMs to train the personas of these
traveler agents. Working together, this dual-agent system is designed to track
and align the underlying decision-making mechanisms of travelers and produce
realistic, adaptive simulations. Using a real-world dataset from a day-to-day
route choice experiment, we show our approach significantly outperforms
existing LLM-based methods in both individual behavioral alignment and
aggregate simulation accuracy. Furthermore, we demonstrate that our method
moves beyond simple behavioral mimicry to capture the evolution of underlying
learning processes, a deeper alignment that fosters robust generalization.
Overall, our framework provides a new approach for creating adaptive and
behaviorally realistic agents to simulate travelers' learning and adaptation
that can benefit transportation simulation and policy analysis.

</details>


### [180] [AI for pRedicting Exacerbations in KIDs with aSthma (AIRE-KIDS)](https://arxiv.org/abs/2511.01018)
*Hui-Lee Ooi,Nicholas Mitsakakis,Margerie Huet Dastarac,Roger Zemek,Amy C. Plint,Jeff Gilchrist,Khaled El Emam,Dhenuka Radhakrishnan*

Main category: cs.AI

TL;DR: 开发机器学习模型预测儿童哮喘反复严重发作，使用电子病历、环境污染物暴露和社区边缘化信息，在COVID-19前后数据集上验证，LGBM模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 儿童哮喘反复发作是常见但可预防的问题，机器学习算法可准确识别高风险儿童，促进预防性综合护理转诊。

Method: 使用COVID-19前（2017-2019年）的电子病历数据训练多种ML模型（LGBM、XGB和LLM方法），在COVID-19后（2022-2023年）数据集验证，比较AUC和F1分数，使用SHAP值确定预测特征。

Result: LGBM模型表现最佳，AIRE-KIDS_ED模型的AUC为0.712，F1分数0.51，显著优于当前决策规则（F1=0.334）。预测特征包括既往哮喘急诊就诊、加拿大分诊敏锐度评分、医疗复杂性等。

Conclusion: 机器学习模型能有效预测儿童哮喘反复严重发作，LGBM模型表现最优，为临床决策提供了改进工具。

Abstract: Recurrent exacerbations remain a common yet preventable outcome for many
children with asthma. Machine learning (ML) algorithms using electronic medical
records (EMR) could allow accurate identification of children at risk for
exacerbations and facilitate referral for preventative comprehensive care to
avoid this morbidity. We developed ML algorithms to predict repeat severe
exacerbations (i.e. asthma-related emergency department (ED) visits or future
hospital admissions) for children with a prior asthma ED visit at a tertiary
care children's hospital.
  Retrospective pre-COVID19 (Feb 2017 - Feb 2019, N=2716) Epic EMR data from
the Children's Hospital of Eastern Ontario (CHEO) linked with environmental
pollutant exposure and neighbourhood marginalization information was used to
train various ML models. We used boosted trees (LGBM, XGB) and 3 open-source
large language model (LLM) approaches (DistilGPT2, Llama 3.2 1B and
Llama-8b-UltraMedical). Models were tuned and calibrated then validated in a
second retrospective post-COVID19 dataset (Jul 2022 - Apr 2023, N=1237) from
CHEO. Models were compared using the area under the curve (AUC) and F1 scores,
with SHAP values used to determine the most predictive features.
  The LGBM ML model performed best with the most predictive features in the
final AIRE-KIDS_ED model including prior asthma ED visit, the Canadian triage
acuity scale, medical complexity, food allergy, prior ED visits for non-asthma
respiratory diagnoses, and age for an AUC of 0.712, and F1 score of 0.51. This
is a nontrivial improvement over the current decision rule which has F1=0.334.
While the most predictive features in the AIRE-KIDS_HOSP model included medical
complexity, prior asthma ED visit, average wait time in the ED, the pediatric
respiratory assessment measure score at triage and food allergy.

</details>


### [181] [On the Emergence of Induction Heads for In-Context Learning](https://arxiv.org/abs/2511.01033)
*Tiberiu Musat,Tiago Pimentel,Lorenzo Noci,Alessandro Stolfo,Mrinmaya Sachan,Thomas Hofmann*

Main category: cs.AI

TL;DR: 该论文研究了Transformer中归纳头的出现机制，揭示了其权重矩阵的简单可解释结构，证明了训练动态被限制在19维参数子空间中，其中仅3个维度负责归纳头的形成，并发现归纳头出现时间与输入上下文长度呈二次关系。


<details>
  <summary>Details</summary>
Motivation: 研究Transformer中上下文学习能力的关键机制——归纳头的形成过程，理解其内在结构和训练动态，为解释Transformer的上下文学习能力提供理论依据。

Method: 使用最小化ICL任务公式和修改的Transformer架构，通过理论分析和实证验证相结合的方法，研究归纳头的权重矩阵结构和训练动态。

Result: 发现归纳头权重矩阵具有简单可解释结构，训练动态被约束在19维子空间中，其中仅3个维度负责归纳头形成，且归纳头出现时间与输入上下文长度平方成正比。

Conclusion: 归纳头的形成遵循特定的低维训练动态，其出现时间受输入上下文长度二次约束，这为理解Transformer的上下文学习机制提供了重要理论洞见。

Abstract: Transformers have become the dominant architecture for natural language
processing. Part of their success is owed to a remarkable capability known as
in-context learning (ICL): they can acquire and apply novel associations solely
from their input context, without any updates to their weights. In this work,
we study the emergence of induction heads, a previously identified mechanism in
two-layer transformers that is particularly important for in-context learning.
We uncover a relatively simple and interpretable structure of the weight
matrices implementing the induction head. We theoretically explain the origin
of this structure using a minimal ICL task formulation and a modified
transformer architecture. We give a formal proof that the training dynamics
remain constrained to a 19-dimensional subspace of the parameter space.
Empirically, we validate this constraint while observing that only 3 dimensions
account for the emergence of an induction head. By further studying the
training dynamics inside this 3-dimensional subspace, we find that the time
until the emergence of an induction head follows a tight asymptotic bound that
is quadratic in the input context length.

</details>


### [182] [Knowledge Elicitation with Large Language Models for Interpretable Cancer Stage Identification from Pathology Reports](https://arxiv.org/abs/2511.01052)
*Yeawon Lee,Christopher C. Yang,Chia-Hsuan Chang,Grace Lu-Yao*

Main category: cs.AI

TL;DR: 提出了两种知识提取方法KEwLTM和KEwRAG，使大语言模型能够从无标注病理报告中推导癌症分期规则，无需大量标注数据，在TCGA数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 解决从非结构化病理报告中提取癌症TNM分期的挑战，克服现有NLP方法依赖大量标注数据的限制，提高可扩展性和适应性。

Method: KEwLTM使用迭代提示策略直接从无标注报告中推导分期规则；KEwRAG采用改进的RAG方法，一次性从指南中预提取规则然后应用，提高可解释性。

Result: KEwLTM在零样本思维链推理有效时表现更好，KEwRAG在零样本思维链推理效果较差时表现更优，两种方法都提供了透明可解释的界面。

Conclusion: 知识提取方法为自动化癌症分期提供了可扩展、高性能且可解释的解决方案，特别适用于标注数据有限的临床环境。

Abstract: Cancer staging is critical for patient prognosis and treatment planning, yet
extracting pathologic TNM staging from unstructured pathology reports poses a
persistent challenge. Existing natural language processing (NLP) and machine
learning (ML) strategies often depend on large annotated datasets, limiting
their scalability and adaptability. In this study, we introduce two Knowledge
Elicitation methods designed to overcome these limitations by enabling large
language models (LLMs) to induce and apply domain-specific rules for cancer
staging. The first, Knowledge Elicitation with Long-Term Memory (KEwLTM), uses
an iterative prompting strategy to derive staging rules directly from
unannotated pathology reports, without requiring ground-truth labels. The
second, Knowledge Elicitation with Retrieval-Augmented Generation (KEwRAG),
employs a variation of RAG where rules are pre-extracted from relevant
guidelines in a single step and then applied, enhancing interpretability and
avoiding repeated retrieval overhead. We leverage the ability of LLMs to apply
broad knowledge learned during pre-training to new tasks. Using breast cancer
pathology reports from the TCGA dataset, we evaluate their performance in
identifying T and N stages, comparing them against various baseline approaches
on two open-source LLMs. Our results indicate that KEwLTM outperforms KEwRAG
when Zero-Shot Chain-of-Thought (ZSCOT) inference is effective, whereas KEwRAG
achieves better performance when ZSCOT inference is less effective. Both
methods offer transparent, interpretable interfaces by making the induced rules
explicit. These findings highlight the promise of our Knowledge Elicitation
methods as scalable, high-performing solutions for automated cancer staging
with enhanced interpretability, particularly in clinical settings with limited
annotated data.

</details>


### [183] [Efficient Test-Time Retrieval Augmented Generation](https://arxiv.org/abs/2511.01059)
*Hailong Yin,Bin Zhu,Jingjing Chen,Chong-Wah Ngo*

Main category: cs.AI

TL;DR: ET2RAG是一个高效的测试时检索增强生成框架，通过检索相关文档、生成多样化候选响应，并采用多数投票机制选择最佳答案，在保持效率的同时提升LLM性能。


<details>
  <summary>Details</summary>
Motivation: 解决LLM依赖参数知识导致的错误，以及传统RAG方法可能引入不相关文档的问题，同时平衡集成方法的高成本与性能增益。

Method: 训练无关方法：首先检索最相关文档，通过管理响应长度高效生成多样化候选响应，然后计算候选响应的相似度并采用多数投票机制选择最终输出。

Result: 实验结果表明ET2RAG在开放域问答、食谱生成和图像描述三个任务上显著提升了性能。

Conclusion: ET2RAG通过部分生成和多数投票机制，在计算成本和性能之间取得了良好平衡，有效提升了LLM的准确性和效率。

Abstract: Although Large Language Models (LLMs) demonstrate significant capabilities,
their reliance on parametric knowledge often leads to inaccuracies. Retrieval
Augmented Generation (RAG) mitigates this by incorporating external knowledge,
but these methods may introduce irrelevant retrieved documents, leading to
inaccurate responses. While the integration methods filter out incorrect
answers from multiple responses, but lack external knowledge like RAG methods,
and their high costs require balancing overhead with performance gains. To
address these issues, we propose an Efficient Test-Time Retrieval-Augmented
Generation Framework named ET2RAG to improve the performance of LLMs while
maintaining efficiency. Specifically, ET2RAG is a training-free method, that
first retrieves the most relevant documents and augments the LLMs to
efficiently generate diverse candidate responses by managing response length.
Then we compute the similarity of candidate responses and employ a majority
voting mechanism to select the most suitable response as the final output. In
particular, we discover that partial generation is sufficient to capture the
key information necessary for consensus calculation, allowing us to effectively
perform majority voting without the need for fully generated responses. Thus,
we can reach a balance between computational cost and performance by managing
the response length for the number of retrieved documents for majority voting.
Experimental results demonstrate that ET2RAG significantly enhances performance
across three tasks, including open-domain question answering, recipe generation
and image captioning.

</details>


### [184] [Modular Task Decomposition and Dynamic Collaboration in Multi-Agent Systems Driven by Large Language Models](https://arxiv.org/abs/2511.01149)
*Shuaidong Pan,Di Wu*

Main category: cs.AI

TL;DR: 提出基于大语言模型的多智能体架构，通过模块化任务分解和动态协作机制解决复杂任务执行中的单智能体局限问题


<details>
  <summary>Details</summary>
Motivation: 解决单智能体在复杂任务分解和协作中的局限性，提升多智能体系统在复杂环境下的任务执行能力

Method: 将自然语言任务描述转换为统一语义表示，引入模块化分解机制将整体目标分解为层次化子任务，采用动态调度和路由机制实现智能体间的合理分工和实时协作，设计约束解析和全局一致性机制确保子任务连贯性和负载均衡

Result: 在任务成功率、分解效率、子任务覆盖率和协作平衡等多个维度验证了架构优势，在整体性能和鲁棒性方面优于现有方法，实现了任务复杂度和通信开销的更好平衡

Conclusion: 证明了语言驱动的任务分解和动态协作在多智能体系统中的有效性和可行性，为复杂环境下的任务执行提供了系统化解决方案

Abstract: This paper addresses the limitations of a single agent in task decomposition
and collaboration during complex task execution, and proposes a multi-agent
architecture for modular task decomposition and dynamic collaboration based on
large language models. The method first converts natural language task
descriptions into unified semantic representations through a large language
model. On this basis, a modular decomposition mechanism is introduced to break
down the overall goal into multiple hierarchical sub-tasks. Then, dynamic
scheduling and routing mechanisms enable reasonable division of labor and
realtime collaboration among agents, allowing the system to adjust strategies
continuously according to environmental feedback, thus maintaining efficiency
and stability in complex tasks. Furthermore, a constraint parsing and global
consistency mechanism is designed to ensure coherent connections between
sub-tasks and balanced workload, preventing performance degradation caused by
redundant communication or uneven resource allocation. The experiments validate
the architecture across multiple dimensions, including task success rate,
decomposition efficiency, sub-task coverage, and collaboration balance. The
results show that the proposed method outperforms existing approaches in both
overall performance and robustness, achieving a better balance between task
complexity and communication overhead. In conclusion, this study demonstrates
the effectiveness and feasibility of language-driven task decomposition and
dynamic collaboration in multi-agent systems, providing a systematic solution
for task execution in complex environments.

</details>


### [185] [DART: Difficulty-Adaptive Reasoning Truncation for Efficient Large Language Models](https://arxiv.org/abs/2511.01170)
*Ruofan Zhang,Bin Xia,Zhen Cheng,Cairen Jian,Minglun Yang,Ngai Wong,Yuan Cheng*

Main category: cs.AI

TL;DR: DART是一个难度自适应推理截断框架，通过根据问题难度调整思考长度，实现高效推理，在保持或提高准确性的同时显著减少计算量。


<details>
  <summary>Details</summary>
Motivation: 当前链式思维方法生成冗长解释导致效率低下，而现有强化学习方法不稳定且依赖奖励。需要一种稳定、自适应的推理方法来平衡计算效率与准确性。

Method: 从更强模型蒸馏简洁推理模式，将其插值成连续推理风格谱系，并策划平衡正确性和紧凑性的最优训练数据，学习何时停止思考。

Result: 在多个数学基准测试中，实现81.2%的推理截断率和5.33倍计算加速，同时保持或提高准确性。

Conclusion: DART为高效推理提供了一个稳定通用的范式，推动了LLM中自适应智能的发展。

Abstract: Adaptive reasoning is essential for aligning the computational effort of
large language models (LLMs) with the intrinsic difficulty of problems. Current
chain-of-thought methods boost reasoning ability but indiscriminately generate
long explanations, leading to evident inefficiency. However, existing
reinforcement learning approaches to adaptive thinking remain unstable and
heavily reward-dependent. Here we propose \textbf{DART}, a supervised
\textbf{D}ifficulty-\textbf{A}daptive \textbf{R}easoning \textbf{T}runcation
framework that adjusts thinking length according to problem difficulty. By
distilling concise reasoning patterns from stronger models, interpolating them
into a continuum of reasoning styles, and curating optimal training data that
balances correctness and compactness, DART learns when to ``stop thinking''.
Across multiple mathematical benchmarks, experimental results demonstrate its
remarkable efficiency while preserving or improving accuracy, achieving a
significant 81.2\% reasoning truncation (DeepSeek-R1-Distill-Qwen-7B on GSM8K
dataset) with 5.33$\times$ computational acceleration. DART provides a stable
and general paradigm for efficient reasoning, advancing the development of
adaptive intelligence in LLMs.

</details>


### [186] [MiRAGE: Misconception Detection with Retrieval-Guided Multi-Stage Reasoning and Ensemble Fusion](https://arxiv.org/abs/2511.01182)
*Cuong Van Duc,Thai Tran Quoc,Minh Nguyen Dinh Tuan,Tam Vu Duc,Son Nguyen Van,Hanh Nguyen Thi*

Main category: cs.AI

TL;DR: MiRAGE是一个用于数学领域自动检测学生误解的新框架，通过检索引导的多阶段推理和集成融合，在开放回答中精确识别学生误解。


<details>
  <summary>Details</summary>
Motivation: 检测学生开放回答中的误解是一个长期挑战，需要语义精确性和逻辑推理能力。

Method: 三阶段框架：检索模块缩小候选池，推理模块使用思维链生成暴露逻辑不一致性，重排序模块通过对齐推理来优化预测。通过集成融合策略统一组件。

Result: 在数学数据集上，MiRAGE在1/3/5级别分别达到0.82/0.92/0.93的平均精度分数，始终优于单个模块。

Conclusion: 通过将检索引导与多阶段推理相结合，MiRAGE减少了对大规模语言模型的依赖，同时为教育评估提供了可扩展且有效的解决方案。

Abstract: Detecting student misconceptions in open-ended responses is a longstanding
challenge, demanding semantic precision and logical reasoning. We propose
MiRAGE - Misconception Detection with Retrieval-Guided Multi-Stage Reasoning
and Ensemble Fusion, a novel framework for automated misconception detection in
mathematics. MiRAGE operates in three stages: (1) a Retrieval module narrows a
large candidate pool to a semantically relevant subset; (2) a Reasoning module
employs chain-of-thought generation to expose logical inconsistencies in
student solutions; and (3) a Reranking module refines predictions by aligning
them with the reasoning. These components are unified through an
ensemble-fusion strategy that enhances robustness and interpretability. On
mathematics datasets, MiRAGE achieves Mean Average Precision scores of
0.82/0.92/0.93 at levels 1/3/5, consistently outperforming individual modules.
By coupling retrieval guidance with multi-stage reasoning, MiRAGE reduces
dependence on large-scale language models while delivering a scalable and
effective solution for educational assessment.

</details>


### [187] [QiMeng-NeuComBack: Self-Evolving Translation from IR to Assembly Code](https://arxiv.org/abs/2511.01183)
*Hainan Fang,Yuanbo Wen,Jun Bi,Yihan Wang,Tonghui He,Yanlin Tang,Di Huang,Jiaming Guo,Rui Zhang,Qi Guo,Yunji Chen*

Main category: cs.AI

TL;DR: 该论文提出了NeuComBack基准数据集和自进化提示优化方法，显著提升了LLM在IR到汇编编译中的功能正确性和性能表现。


<details>
  <summary>Details</summary>
Motivation: 编译器开发复杂且成本高昂，LLM提供了神经编译新范式，但缺乏专用基准和评估方法，且LLM生成汇编的可靠性和性能有待提升。

Method: 引入NeuComBack基准数据集，定义神经编译工作流，并提出自进化提示优化方法，让LLM从自我调试轨迹中迭代优化提示策略。

Result: 功能正确率在x86_64上从44%提升到64%，在aarch64上从36%提升到58%；在正确生成的x86_64程序中，87.5%超过了clang-O3的性能。

Conclusion: NeuComBack基准和自进化提示优化方法有效解决了神经编译的关键挑战，显著提升了LLM生成汇编的质量和性能。

Abstract: Compilers, while essential, are notoriously complex systems that demand
prohibitively expensive human expertise to develop and maintain. The recent
advancements in Large Language Models (LLMs) offer a compelling new paradigm:
Neural Compilation, which could potentially simplify compiler development for
new architectures and facilitate the discovery of innovative optimization
techniques. However, several critical obstacles impede its practical adoption.
Firstly, a significant lack of dedicated benchmarks and robust evaluation
methodologies hinders objective assessment and tracking of progress in the
field. Secondly, systematically enhancing the reliability and performance of
LLM-generated assembly remains a critical challenge. Addressing these
challenges, this paper introduces NeuComBack, a novel benchmark dataset
specifically designed for IR-to-assembly compilation. Leveraging this dataset,
we first define a foundational Neural Compilation workflow and conduct a
comprehensive evaluation of the capabilities of recent frontier LLMs on Neural
Compilation, establishing new performance baselines. We further propose a
self-evolving prompt optimization method that enables LLMs to iteratively
evolve their internal prompt strategies by extracting insights from prior
self-debugging traces, thereby enhancing their neural compilation capabilities.
Experiments demonstrate that our method significantly improves both the
functional correctness and the performance of LLM-generated assembly code.
Compared to baseline prompts, the functional correctness rates improved from
44% to 64% on x86_64 and from 36% to 58% on aarch64, respectively. More
significantly, among the 16 correctly generated x86_64 programs using our
method, 14 (87.5%) surpassed clang-O3 performance.

</details>


### [188] [Graph Neural Network-Based Semi-Supervised Open-Set Fault Diagnosis for Marine Machinery Systems](https://arxiv.org/abs/2511.01258)
*Chuyue Lou,M. Amine Atoui*

Main category: cs.AI

TL;DR: 提出了一种半监督开放集故障诊断框架，用于解决船舶机械系统中未知故障类型的检测问题，通过可靠性子集构建和半监督学习来准确分类已知故障并检测未知样本。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法假设训练和测试数据中的故障类别一致，但在实际工业部署中会出现训练时未见过的未知故障类型，导致这些方法失效。

Method: 使用监督特征学习模型提取多层融合特征表示来构建可靠性子集，然后将标记训练集和伪标记测试子集输入半监督诊断模型，学习每个类别的判别特征。

Result: 在公共海事基准数据集上的实验结果表明，所提出的SOFD框架具有有效性和优越性。

Conclusion: 该框架增强了深度学习模型在开放集故障诊断场景中的适用性，能够准确分类已知故障并有效检测未知样本。

Abstract: Recently, fault diagnosis methods for marine machinery systems based on deep
learning models have attracted considerable attention in the shipping industry.
Most existing studies assume fault classes are consistent and known between the
training and test datasets, and these methods perform well under controlled
environment. In practice, however, previously unseen or unknown fault types
(i.e., out-of-distribution or open-set observations not present during
training) can occur, causing such methods to fail and posing a significant
challenge to their widespread industrial deployment. To address this challenge,
this paper proposes a semi-supervised open-set fault diagnosis (SOFD) framework
that enhances and extends the applicability of deep learning models in open-set
fault diagnosis scenarios. The framework includes a reliability subset
construction process, which uses a multi-layer fusion feature representation
extracted by a supervised feature learning model to select an unlabeled test
subset. The labeled training set and pseudo-labeled test subset are then fed
into a semi-supervised diagnosis model to learn discriminative features for
each class, enabling accurate classification of known faults and effective
detection of unknown samples. Experimental results on a public maritime
benchmark dataset demonstrate the effectiveness and superiority of the proposed
SOFD framework.

</details>


### [189] [llmSHAP: A Principled Approach to LLM Explainability](https://arxiv.org/abs/2511.01311)
*Filip Naudot,Tobias Sundqvist,Timotheus Kampik*

Main category: cs.AI

TL;DR: 该论文研究了在基于大语言模型的随机决策支持系统中应用Shapley值进行特征归因的方法，分析了不同实现变体下Shapley值原则的满足情况，并探讨了LLM随机性对这些保证的影响。


<details>
  <summary>Details</summary>
Motivation: 特征归因方法使机器学习推理可解释，但流行的Shapley值方法假设确定性推理。在基于LLM的决策支持系统中，推理本质上是随机的，因此需要研究Shapley值在这种随机环境下的适用性。

Method: 将Shapley值应用于LLM决策支持系统的特征归因，分析不同实现变体下Shapley值原则的满足情况，并研究LLM随机性对这些保证的影响。

Result: 展示了在不同实现变体下Shapley值原则的满足情况，分析了LLM随机性如何影响这些保证，并揭示了可解释推理速度、与精确Shapley值归因的一致性以及原则达成之间的权衡。

Conclusion: 在基于LLM的随机决策支持系统中应用Shapley值进行特征归因时，需要仔细考虑实现变体的选择，因为LLM的随机性会影响Shapley值原则的保证，存在速度、准确性和原则满足之间的权衡关系。

Abstract: Feature attribution methods help make machine learning-based inference
explainable by determining how much one or several features have contributed to
a model's output. A particularly popular attribution method is based on the
Shapley value from cooperative game theory, a measure that guarantees the
satisfaction of several desirable principles, assuming deterministic inference.
We apply the Shapley value to feature attribution in large language model
(LLM)-based decision support systems, where inference is, by design, stochastic
(non-deterministic). We then demonstrate when we can and cannot guarantee
Shapley value principle satisfaction across different implementation variants
applied to LLM-based decision support, and analyze how the stochastic nature of
LLMs affects these guarantees. We also highlight trade-offs between explainable
inference speed, agreement with exact Shapley value attributions, and principle
attainment.

</details>


### [190] [OmniFuser: Adaptive Multimodal Fusion for Service-Oriented Predictive Maintenance](https://arxiv.org/abs/2511.01320)
*Ziqi Wang,Hailiang Zhao,Yuhao Yang,Daojiang Hu,Cheng Bao,Mingyi Liu,Kai Di,Schahram Dustdar,Zhongjie Wang,Shuiguang Deng*

Main category: cs.AI

TL;DR: 提出OmniFuser多模态学习框架，通过融合视觉和传感器数据进行铣削刀具预测性维护，在刀具状态分类和力信号预测方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 智能制造系统中准确及时的刀具状态预测至关重要，非计划性刀具故障会导致质量下降和生产停机。需要可靠的服务导向型预测维护解决方案。

Method: 并行特征提取高分辨率刀具图像和切削力信号，采用无污染跨模态融合机制分离共享和模态特定组件，使用递归精化路径保留残差信息稳定融合动态。

Result: 在真实铣削数据集上的实验表明，OmniFuser持续优于最先进的基线方法，为构建智能工业维护服务提供可靠基础。

Conclusion: OmniFuser框架有效整合多模态数据，支持可重用维护服务模块，在刀具状态分类和力信号预测方面表现出色，适用于工业预测维护服务。

Abstract: Accurate and timely prediction of tool conditions is critical for intelligent
manufacturing systems, where unplanned tool failures can lead to quality
degradation and production downtime. In modern industrial environments,
predictive maintenance is increasingly implemented as an intelligent service
that integrates sensing, analysis, and decision support across production
processes. To meet the demand for reliable and service-oriented operation, we
present OmniFuser, a multimodal learning framework for predictive maintenance
of milling tools that leverages both visual and sensor data. It performs
parallel feature extraction from high-resolution tool images and cutting-force
signals, capturing complementary spatiotemporal patterns across modalities. To
effectively integrate heterogeneous features, OmniFuser employs a
contamination-free cross-modal fusion mechanism that disentangles shared and
modality-specific components, allowing for efficient cross-modal interaction.
Furthermore, a recursive refinement pathway functions as an anchor mechanism,
consistently retaining residual information to stabilize fusion dynamics. The
learned representations can be encapsulated as reusable maintenance service
modules, supporting both tool-state classification (e.g., Sharp, Used, Dulled)
and multi-step force signal forecasting. Experiments on real-world milling
datasets demonstrate that OmniFuser consistently outperforms state-of-the-art
baselines, providing a dependable foundation for building intelligent
industrial maintenance services.

</details>


### [191] [Unbiased Platform-Level Causal Estimation for Search Systems: A Competitive Isolation PSM-DID Framework](https://arxiv.org/abs/2511.01329)
*Ying Song,Yijing Wang,Hui Yang,Weihan Jin,Jun Xiong,Congyi Zhou,Jialin Zhu,Xiang Gao,Rong Chen,HuaGuang Deng,Ying Dai,Fei Xiao,Haihong Tang,Bo Zheng,KaiFu Zhang*

Main category: cs.AI

TL;DR: 提出了竞争隔离PSM-DID框架，通过结合倾向得分匹配和竞争隔离来解决搜索型双边市场中平台级干预的系统性效应问题


<details>
  <summary>Details</summary>
Motivation: 在搜索型双边市场中评估平台级干预面临系统性效应（如溢出效应和网络干扰）的挑战，传统PSM-DID框架容易受到选择偏差和跨单元干扰的影响

Method: 将倾向得分匹配与竞争隔离相结合，构建竞争隔离PSM-DID框架，在互斥条件下提供理论保证的无偏估计

Result: 实验显示显著减少了干扰效应和估计方差，在大型市场平台成功部署验证了框架的实用性

Conclusion: 该框架为平台级因果推断提供了有效的解决方案，特别适用于搜索系统中的市场干扰问题

Abstract: Evaluating platform-level interventions in search-based two-sided
marketplaces is fundamentally challenged by systemic effects such as spillovers
and network interference. While widely used for causal inference, the PSM
(Propensity Score Matching) - DID (Difference-in-Differences) framework remains
susceptible to selection bias and cross-unit interference from unaccounted
spillovers. In this paper, we introduced Competitive Isolation PSM-DID, a novel
causal framework that integrates propensity score matching with competitive
isolation to enable platform-level effect measurement (e.g., order volume, GMV)
instead of item-level metrics in search systems.
  Our approach provides theoretically guaranteed unbiased estimation under
mutual exclusion conditions, with an open dataset released to support
reproducible research on marketplace interference (github.com/xxxx). Extensive
experiments demonstrate significant reductions in interference effects and
estimation variance compared to baseline methods. Successful deployment in a
large-scale marketplace confirms the framework's practical utility for
platform-level causal inference.

</details>


### [192] [Automatic Minds: Cognitive Parallels Between Hypnotic States and Large Language Model Processing](https://arxiv.org/abs/2511.01363)
*Giuseppe Riva,Brenda K. Wiederhold,Fabrizia Mantovani*

Main category: cs.AI

TL;DR: 这篇论文探讨了催眠状态下的人类认知过程与大型语言模型(LLMs)之间的深层功能相似性，包括自动性、监控抑制和高度情境依赖性等机制，并提出了构建结合生成流畅性和执行监控的混合AI架构的建议。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于揭示催眠认知与LLMs计算操作之间的功能并行性，以理解在没有主观意识的情况下如何产生目标导向行为，并为开发更可靠的AI系统提供启示。

Method: 采用比较分析方法，通过三个核心原则（自动性、监控抑制、情境依赖性）来系统比较催眠认知和LLMs的运作机制，并探讨观察者相对意义缺口、功能代理与主观代理的区别等现象。

Result: 研究发现催眠和LLMs都表现出功能代理（复杂目标导向行为）而不具备主观代理（意识意图），两者都产生连贯但无根据的输出，需要外部解释者赋予意义。

Conclusion: 论文结论指出，可靠的AI未来在于构建混合架构，将生成流畅性与执行监控机制相结合，这种设计灵感来源于人类心智的复杂自我调节架构。

Abstract: The cognitive processes of the hypnotized mind and the computational
operations of large language models (LLMs) share deep functional parallels.
Both systems generate sophisticated, contextually appropriate behavior through
automatic pattern-completion mechanisms operating with limited or unreliable
executive oversight. This review examines this convergence across three
principles: automaticity, in which responses emerge from associative rather
than deliberative processes; suppressed monitoring, leading to errors such as
confabulation in hypnosis and hallucination in LLMs; and heightened contextual
dependency, where immediate cues (for example, the suggestion of a therapist or
the prompt of the user) override stable knowledge.
  These mechanisms reveal an observer-relative meaning gap: both systems
produce coherent but ungrounded outputs that require an external interpreter to
supply meaning. Hypnosis and LLMs also exemplify functional agency - the
capacity for complex, goal-directed, context-sensitive behavior - without
subjective agency, the conscious awareness of intention and ownership that
defines human action. This distinction clarifies how purposive behavior can
emerge without self-reflective consciousness, governed instead by structural
and contextual dynamics. Finally, both domains illuminate the phenomenon of
scheming: automatic, goal-directed pattern generation that unfolds without
reflective awareness. Hypnosis provides an experimental model for understanding
how intention can become dissociated from conscious deliberation, offering
insights into the hidden motivational dynamics of artificial systems.
Recognizing these parallels suggests that the future of reliable AI lies in
hybrid architectures that integrate generative fluency with mechanisms of
executive monitoring, an approach inspired by the complex, self-regulating
architecture of the human mind.

</details>


### [193] [Align to Misalign: Automatic LLM Jailbreak with Meta-Optimized LLM Judges](https://arxiv.org/abs/2511.01375)
*Hamin Koo,Minseon Kim,Jaehyung Kim*

Main category: cs.AI

TL;DR: AMIS是一个元优化框架，通过双层结构联合进化越狱提示和评分模板，解决了现有方法依赖稀疏的二进制攻击成功率信号或人工评分模板的问题。


<details>
  <summary>Details</summary>
Motivation: 现有优化型越狱方法要么依赖稀疏的二进制攻击成功率信号，要么使用人工设计的评分模板，这引入了人类偏见和评分结果的不确定性。

Method: AMIS采用双层结构：内层循环使用固定评分模板通过细粒度密集反馈优化提示；外层循环使用ASR对齐分数优化模板，使其更好地反映真实攻击结果。

Result: 在AdvBench和JBB-Behaviors上的评估显示，AMIS实现了最先进的性能，在Claude-3.5-Haiku上达到88.0% ASR，在Claude-4-Sonnet上达到100.0% ASR，显著优于现有基线。

Conclusion: AMIS通过联合优化提示和评分模板，能够生成更强的越狱提示和更校准的评分信号，有效提升大语言模型的安全性测试能力。

Abstract: Identifying the vulnerabilities of large language models (LLMs) is crucial
for improving their safety by addressing inherent weaknesses. Jailbreaks, in
which adversaries bypass safeguards with crafted input prompts, play a central
role in red-teaming by probing LLMs to elicit unintended or unsafe behaviors.
Recent optimization-based jailbreak approaches iteratively refine attack
prompts by leveraging LLMs. However, they often rely heavily on either binary
attack success rate (ASR) signals, which are sparse, or manually crafted
scoring templates, which introduce human bias and uncertainty in the scoring
outcomes. To address these limitations, we introduce AMIS (Align to MISalign),
a meta-optimization framework that jointly evolves jailbreak prompts and
scoring templates through a bi-level structure. In the inner loop, prompts are
refined using fine-grained and dense feedback using a fixed scoring template.
In the outer loop, the template is optimized using an ASR alignment score,
gradually evolving to better reflect true attack outcomes across queries. This
co-optimization process yields progressively stronger jailbreak prompts and
more calibrated scoring signals. Evaluations on AdvBench and JBB-Behaviors
demonstrate that AMIS achieves state-of-the-art performance, including 88.0%
ASR on Claude-3.5-Haiku and 100.0% ASR on Claude-4-Sonnet, outperforming
existing baselines by substantial margins.

</details>


### [194] [Relaxing partition admissibility in Cluster-DAGs: a causal calculus with arbitrary variable clustering](https://arxiv.org/abs/2511.01396)
*Clément Yvernes,Emilie Devijver,Adèle H. Ribeiro,Marianne Clausel--Lesourd,Éric Gaussier*

Main category: cs.AI

TL;DR: 扩展C-DAG框架以支持任意变量聚类，允许循环C-DAG表示，并扩展d-分离和因果演算概念


<details>
  <summary>Details</summary>
Motivation: 传统C-DAG框架要求聚类必须产生无环图，限制了其应用范围。当选择的聚类导致循环时，该分区被视为不可接受

Method: 通过放宽分区可接受性约束，允许循环C-DAG表示，并扩展d-分离和因果演算概念到这个设置

Result: 显著扩大了跨集群因果推理的范围，使C-DAG能够应用于以前难以处理的场景

Conclusion: 提出的演算相对于do-演算既是可靠的又是原子完备的：所有有效的集群级干预查询都可以使用我们的规则推导，每个规则对应一个原始do-演算步骤

Abstract: Cluster DAGs (C-DAGs) provide an abstraction of causal graphs in which nodes
represent clusters of variables, and edges encode both cluster-level causal
relationships and dependencies arisen from unobserved confounding. C-DAGs
define an equivalence class of acyclic causal graphs that agree on
cluster-level relationships, enabling causal reasoning at a higher level of
abstraction. However, when the chosen clustering induces cycles in the
resulting C-DAG, the partition is deemed inadmissible under conventional C-DAG
semantics. In this work, we extend the C-DAG framework to support arbitrary
variable clusterings by relaxing the partition admissibility constraint,
thereby allowing cyclic C-DAG representations. We extend the notions of
d-separation and causal calculus to this setting, significantly broadening the
scope of causal reasoning across clusters and enabling the application of
C-DAGs in previously intractable scenarios. Our calculus is both sound and
atomically complete with respect to the do-calculus: all valid interventional
queries at the cluster level can be derived using our rules, each corresponding
to a primitive do-calculus step.

</details>


### [195] [Modulation of temporal decision-making in a deep reinforcement learning agent under the dual-task paradigm](https://arxiv.org/abs/2511.01415)
*Amrapali Pednekar,Álvaro Garrido-Pérez,Yara Khaluf,Pieter Simoens*

Main category: cs.AI

TL;DR: 该研究从AI角度探索双任务范式中的时间处理干扰，使用简化的Overcooked环境训练DRL代理，发现双任务代理比单任务代理显著高估时间，但未在LSTM层发现明确的时间编码机制。


<details>
  <summary>Details</summary>
Motivation: 探索深度强化学习代理在双任务范式中的时间处理行为，与人类时间研究进行比较，以促进对生物系统和AI系统的更好理解。

Method: 使用简化的Overcooked环境，设置单任务(T)和双任务(T+N)两种变体，分别训练两个深度强化学习代理，其中双任务包含时间生产和数字比较的并发任务。

Result: 双任务代理相对于单任务代理显著高估时间，这一结果在四个目标持续时间上一致；LSTM层的初步分析未发现明确的时间编码机制。

Conclusion: 需要进一步研究代理的时间保持机制，以理解观察到的行为模式；这是探索DRL行为与生物系统行为相似性的一小步。

Abstract: This study explores the interference in temporal processing within a
dual-task paradigm from an artificial intelligence (AI) perspective. In this
context, the dual-task setup is implemented as a simplified version of the
Overcooked environment with two variations, single task (T) and dual task
(T+N). Both variations involve an embedded time production task, but the dual
task (T+N) additionally involves a concurrent number comparison task. Two deep
reinforcement learning (DRL) agents were separately trained for each of these
tasks. These agents exhibited emergent behavior consistent with human timing
research. Specifically, the dual task (T+N) agent exhibited significant
overproduction of time relative to its single task (T) counterpart. This result
was consistent across four target durations. Preliminary analysis of neural
dynamics in the agents' LSTM layers did not reveal any clear evidence of a
dedicated or intrinsic timer. Hence, further investigation is needed to better
understand the underlying time-keeping mechanisms of the agents and to provide
insights into the observed behavioral patterns. This study is a small step
towards exploring parallels between emergent DRL behavior and behavior observed
in biological systems in order to facilitate a better understanding of both.

</details>


### [196] [Learning to Seek Evidence: A Verifiable Reasoning Agent with Causal Faithfulness Analysis](https://arxiv.org/abs/2511.01425)
*Yuhang Huang,Zekai Lin,Fan Zhong,Lei Liu*

Main category: cs.AI

TL;DR: 提出一种交互式AI代理，通过可审计的行动序列生成解释，使用强化学习优化策略来寻找视觉证据支持诊断推理，显著提高校准准确度并验证解释的忠实性。


<details>
  <summary>Details</summary>
Motivation: 解决高风险领域（如医学）中AI模型解释缺乏可验证性的问题，这会影响信任建立。

Method: 使用强化学习训练代理策略，使其能够战略性地寻求外部视觉证据来支持诊断推理，并通过因果干预方法验证解释的忠实性。

Result: 实验显示基于行动的推理过程显著改善校准准确度，Brier分数比非交互基线降低18%；通过掩蔽视觉证据观察到性能下降（ΔBrier=+0.029），证实证据对决策过程的重要性。

Conclusion: 该工作为构建具有可验证和忠实推理能力的AI系统提供了实用框架。

Abstract: Explanations for AI models in high-stakes domains like medicine often lack
verifiability, which can hinder trust. To address this, we propose an
interactive agent that produces explanations through an auditable sequence of
actions. The agent learns a policy to strategically seek external visual
evidence to support its diagnostic reasoning. This policy is optimized using
reinforcement learning, resulting in a model that is both efficient and
generalizable. Our experiments show that this action-based reasoning process
significantly improves calibrated accuracy, reducing the Brier score by 18\%
compared to a non-interactive baseline. To validate the faithfulness of the
agent's explanations, we introduce a causal intervention method. By masking the
visual evidence the agent chooses to use, we observe a measurable degradation
in its performance ($\Delta$Brier=+0.029), confirming that the evidence is
integral to its decision-making process. Our work provides a practical
framework for building AI systems with verifiable and faithful reasoning
capabilities.

</details>


### [197] [Robust Multimodal Sentiment Analysis via Double Information Bottleneck](https://arxiv.org/abs/2511.01444)
*Huiting Huang,Tieliang Gong,Kai He,Jialun Wu,Erik Cambria,Mengling Feng*

Main category: cs.AI

TL;DR: 提出双信息瓶颈(DIB)策略，通过低秩Renyi熵框架增强多模态情感分析的鲁棒性，有效过滤单模态噪声并捕捉跨模态互补信息。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在两个关键局限：对噪声污染的单模态数据学习不足导致跨模态交互受损，以及多模态表示融合不充分导致丢弃判别性单模态信息而保留冗余信息。

Method: 采用双信息瓶颈策略，包含两个模块：1)通过最大化任务相关信息并丢弃冗余信息来学习压缩的单模态表示；2)通过新颖的注意力瓶颈融合机制确保多模态表示的判别能力。

Result: 在CMU-MOSI、CMU-MOSEI、CH-SIMS和MVSA-Single数据集上验证有效性，CMU-MOSI上Acc-7达到47.4%，CH-SIMS上F1-score达到81.63%，优于次优基线1.19%。在噪声下性能下降仅为0.36%和0.29%。

Conclusion: DIB策略能够有效过滤单模态数据中的噪声信息，同时捕捉模态间的互补性，在多模态情感分析任务中表现出优越的性能和鲁棒性。

Abstract: Multimodal sentiment analysis has received significant attention across
diverse research domains. Despite advancements in algorithm design, existing
approaches suffer from two critical limitations: insufficient learning of
noise-contaminated unimodal data, leading to corrupted cross-modal
interactions, and inadequate fusion of multimodal representations, resulting in
discarding discriminative unimodal information while retaining multimodal
redundant information. To address these challenges, this paper proposes a
Double Information Bottleneck (DIB) strategy to obtain a powerful, unified
compact multimodal representation. Implemented within the framework of low-rank
Renyi's entropy functional, DIB offers enhanced robustness against diverse
noise sources and computational tractability for high-dimensional data, as
compared to the conventional Shannon entropy-based methods. The DIB comprises
two key modules: 1) learning a sufficient and compressed representation of
individual unimodal data by maximizing the task-relevant information and
discarding the superfluous information, and 2) ensuring the discriminative
ability of multimodal representation through a novel attention bottleneck
fusion mechanism. Consequently, DIB yields a multimodal representation that
effectively filters out noisy information from unimodal data while capturing
inter-modal complementarity. Extensive experiments on CMU-MOSI, CMU-MOSEI,
CH-SIMS, and MVSA-Single validate the effectiveness of our method. The model
achieves 47.4% accuracy under the Acc-7 metric on CMU-MOSI and 81.63% F1-score
on CH-SIMS, outperforming the second-best baseline by 1.19%. Under noise, it
shows only 0.36% and 0.29% performance degradation on CMU-MOSI and CMU-MOSEI
respectively.

</details>


### [198] [From Passive to Proactive: A Multi-Agent System with Dynamic Task Orchestration for Intelligent Medical Pre-Consultation](https://arxiv.org/abs/2511.01445)
*ChengZhang Yu,YingRu He,Hongyan Cheng,nuo Cheng,Zhixing Liu,Dongxu Mu,Zhangrui Shen,Zhanpeng Jin*

Main category: cs.AI

TL;DR: 该研究提出了一个分层多智能体框架，将被动医疗AI系统转变为主动询问代理，通过自主任务编排来改进预咨询流程。该系统在1723份电子健康记录上评估，实现了87%的初级分诊准确率和80.5%的次级分诊准确率，临床质量评分平均在4.5以上（5分制）。


<details>
  <summary>Details</summary>
Motivation: 全球医疗系统面临患者数量增加和咨询时间有限（平均不到5分钟）的挑战。现有AI系统存在被动交互范式和上下文管理问题，预咨询流程中的分诊和结构化病史采集需要更有效的解决方案。

Method: 开发了八智能体分层架构，通过集中控制机制将预咨询分解为四个主要任务：分诊(T1)、现病史采集(T2)、既往史采集(T3)和主诉生成(T4)，其中T1-T3进一步分为13个领域特定子任务。采用模型无关架构，在多个基础模型上测试。

Result: 在GPT-OSS 20B、Qwen3-8B、Phi4-14B等模型上评估，初级分诊准确率87.0%，次级分诊80.5%。智能体驱动调度任务完成率98.2%，优于顺序处理的93.1%。临床质量评分：主诉4.56、现病史4.48、既往史4.69（5分制）。T2平均12.7轮完成，T3平均16.9轮完成。

Conclusion: 该模型无关架构在不同基础模型上保持高性能，通过本地部署保护数据隐私，证明了自主AI系统在提高临床预咨询效率和质量方面的潜力。

Abstract: Global healthcare systems face critical challenges from increasing patient
volumes and limited consultation times, with primary care visits averaging
under 5 minutes in many countries. While pre-consultation processes
encompassing triage and structured history-taking offer potential solutions,
they remain limited by passive interaction paradigms and context management
challenges in existing AI systems. This study introduces a hierarchical
multi-agent framework that transforms passive medical AI systems into proactive
inquiry agents through autonomous task orchestration. We developed an
eight-agent architecture with centralized control mechanisms that decomposes
pre-consultation into four primary tasks: Triage ($T_1$), History of Present
Illness collection ($T_2$), Past History collection ($T_3$), and Chief
Complaint generation ($T_4$), with $T_1$--$T_3$ further divided into 13
domain-specific subtasks. Evaluated on 1,372 validated electronic health
records from a Chinese medical platform across multiple foundation models
(GPT-OSS 20B, Qwen3-8B, Phi4-14B), the framework achieved 87.0% accuracy for
primary department triage and 80.5% for secondary department classification,
with task completion rates reaching 98.2% using agent-driven scheduling versus
93.1% with sequential processing. Clinical quality scores from 18 physicians
averaged 4.56 for Chief Complaints, 4.48 for History of Present Illness, and
4.69 for Past History on a 5-point scale, with consultations completed within
12.7 rounds for $T_2$ and 16.9 rounds for $T_3$. The model-agnostic
architecture maintained high performance across different foundation models
while preserving data privacy through local deployment, demonstrating the
potential for autonomous AI systems to enhance pre-consultation efficiency and
quality in clinical settings.

</details>


### [199] [TPS-Bench: Evaluating AI Agents' Tool Planning \& Scheduling Abilities in Compounding Tasks](https://arxiv.org/abs/2511.01527)
*Hanwen Xu,Xuyao Huang,Yuzhe Liu,Kai Yu,Zhijie Deng*

Main category: cs.AI

TL;DR: TPS-Bench是一个评估LLM智能体在需要工具规划与调度能力的复合任务中表现的基准，包含200个基于数百个MCP工具的复合任务，评估显示大多数模型能合理规划工具但调度能力差异显著。


<details>
  <summary>Details</summary>
Motivation: 探索LLM智能体是否能处理需要多种工具协作的复合现实问题，这些任务不仅需要选择合适的工具，还需要战略性地安排执行顺序以保证效率。

Method: 构建TPS-Bench基准，包含200个基于数百个MCP工具的复合任务，每个任务由多个子任务组成，评估任务完成率和效率，并进行强化学习调度优化研究。

Result: GLM-4.5达到64.72%的任务完成率但执行时间较长，GPT-4o优先并行工具调用但仅45.08%完成率，对Qwen3-1.7B进行RL训练后执行时间减少14%且完成率提升6%。

Conclusion: LLM智能体在工具规划方面表现合理但在调度能力上存在差异，强化学习是提高调度效率而不牺牲性能的可行方法。

Abstract: Large language model (LLM) agents have exhibited strong problem-solving
competence across domains like research and coding. Yet, it remains
underexplored whether LLM agents can tackle compounding real-world problems
that require a diverse set of tools to complete. Given a broad, heterogeneous
tool repository, LLM agents must not only select appropriate tools based on
task planning analysis but also strategically schedule the execution order to
ensure efficiency. This paper introduces TPS-Bench to benchmark the ability of
LLM agents in solving such problems that demand Tool Planning and Scheduling.
TPS-Bench collects 200 compounding tasks of two difficulty levels, based on a
tool repository containing hundreds of model context protocol (MCP) tools. In
particular, each task is composed of multiple subtasks, such as web search, map
navigation, calendar checking, etc., and each subtask can be completed by a
basic tool. Our evaluation emphasizes both task completion rate and efficiency.
The empirical studies on popular closed-source and open-source LLMs indicate
that most models can perform reasonable tool planning, but differ in
scheduling. For example, GLM-4.5 achieves an outperforming task completion rate
of 64.72% with extensive sequential tool calls, hence suffering from
significantly long execution time. By contrast, GPT-4o prioritizes parallel
tool calls but achieves only a 45.08% completion rate. Considering
reinforcement learning (RL) can be a viable way to improve the scheduling
efficiency without compromising performance, we perform an initial study on
Qwen3-1.7B and witness a 14% reduction in execution time alongside a 6% gain in
task completion rate based on rarely 100 RL training samples. Our code is
available https://github.com/hanwenxu1/mcp-agent.

</details>


### [200] [Analyzing Sustainability Messaging in Large-Scale Corporate Social Media](https://arxiv.org/abs/2511.01550)
*Ujjwal Sharma,Stevan Rudinac,Ana Mićković,Willemijn van Dolen,Marcel Worring*

Main category: cs.AI

TL;DR: 提出一个多模态分析管道，利用大型基础模型分析企业社交媒体内容，重点关注可持续性相关沟通，通过LLM标注推文与可持续发展目标的关联，并结合视觉语言模型分析视觉可持续性沟通模式。


<details>
  <summary>Details</summary>
Motivation: 解决企业社交媒体内容中不断演变、多模态且经常模糊的可持续性沟通挑战，探索大型基础模型作为临时标注器的潜力，避免昂贵的特定任务标注需求。

Method: 使用大型语言模型集合标注企业推文与17个可持续发展目标的主题对齐，结合视觉语言模型通过语义聚类分析视觉可持续性沟通模式。

Result: 揭示了不同行业在可持续发展目标参与度、时间趋势以及企业信息与ESG风险和消费者参与度之间的关联。

Conclusion: 提出的自动标签生成和语义视觉聚类方法可广泛应用于其他领域，为大规社交媒体分析提供了一个灵活框架。

Abstract: In this work, we introduce a multimodal analysis pipeline that leverages
large foundation models in vision and language to analyze corporate social
media content, with a focus on sustainability-related communication. Addressing
the challenges of evolving, multimodal, and often ambiguous corporate messaging
on platforms such as X (formerly Twitter), we employ an ensemble of large
language models (LLMs) to annotate a large corpus of corporate tweets on their
topical alignment with the 17 Sustainable Development Goals (SDGs). This
approach avoids the need for costly, task-specific annotations and explores the
potential of such models as ad-hoc annotators for social media data that can
efficiently capture both explicit and implicit references to sustainability
themes in a scalable manner. Complementing this textual analysis, we utilize
vision-language models (VLMs), within a visual understanding framework that
uses semantic clusters to uncover patterns in visual sustainability
communication. This integrated approach reveals sectoral differences in SDG
engagement, temporal trends, and associations between corporate messaging,
environmental, social, governance (ESG) risks, and consumer engagement. Our
methods-automatic label generation and semantic visual clustering-are broadly
applicable to other domains and offer a flexible framework for large-scale
social media analysis.

</details>


### [201] [ExplicitLM: Decoupling Knowledge from Parameters via Explicit Memory Banks](https://arxiv.org/abs/2511.01581)
*Chengzhang Yu,Zening Lu,Chenyang Zheng,Chiyue Wang,Yiming Zhang,Zhanpeng Jin*

Main category: cs.AI

TL;DR: ExplicitLM是一种新型语言模型架构，通过外部记忆库存储可读知识，支持直接检查和修改，解决了传统LLM的知识陈旧和缺乏可解释性问题。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型因知识隐式存储在纠缠网络参数中导致的知识陈旧、缺乏可解释性、无法定向更新和推理透明度不足的问题。

Method: 采用百万规模外部记忆库存储人类可读知识，设计可微分两阶段检索机制：基于产品密钥分解的粗粒度过滤和基于Gumbel-Softmax的细粒度匹配，支持端到端训练。

Result: 在知识密集型任务上比标准Transformer提升43.67%，在低数据场景（10k样本）下获得3.62倍增益，正确预测的记忆命中率高出49%。

Conclusion: 可解释、可更新的模型在保持竞争力的同时，能够提供前所未有的知识透明度，优于使用固定检索的RAG系统。

Abstract: Large language models suffer from knowledge staleness and lack of
interpretability due to implicit knowledge storage across entangled network
parameters, preventing targeted updates and reasoning transparency. We propose
ExplicitLM, a novel architecture featuring a million-scale external memory bank
storing human-readable knowledge as token sequences, enabling direct inspection
and modification. We design a differentiable two-stage retrieval mechanism with
efficient coarse-grained filtering via product key decomposition (reducing
complexity from $\mathcal{O}(N \cdot |I|)$ to $\mathcal{O}(\sqrt{N} \cdot
|I|)$) and fine-grained Gumbel-Softmax matching for end-to-end training.
Inspired by dual-system cognitive theory, we partition knowledge into frozen
explicit facts (20%) and learnable implicit patterns (80%), maintained through
Exponential Moving Average updates for stability. ExplicitLM achieves up to
43.67% improvement on knowledge-intensive tasks versus standard Transformers,
with 3.62$\times$ gains in low-data regimes (10k samples). Analysis shows
strong correlations between memory retrieval and performance, with correct
predictions achieving 49% higher hit rates. Unlike RAG systems with frozen
retrieval, our jointly optimized architecture demonstrates that interpretable,
updatable models can maintain competitive performance while providing
unprecedented knowledge transparency.

</details>


### [202] [IVGAE-TAMA-BO: A novel temporal dynamic variational graph model for link prediction in global food trade networks with momentum structural memory and Bayesian optimization](https://arxiv.org/abs/2511.01639)
*Sicheng Wang,Shuhao Chen,Jingran Zhou,Chengyi Tu*

Main category: cs.AI

TL;DR: 提出IVGAE-TAMA-BO动态图神经网络，首次将动态图神经网络应用于全球粮食贸易网络链接预测，显著提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 全球粮食贸易网络结构在政治、经济和环境因素影响下动态演变，难以建模和预测未来贸易链接，需要有效捕捉时间模式以提高预测准确性。

Method: 基于IVGAE框架，引入贸易感知动量聚合器(TAMA)捕捉贸易网络时间演化，联合建模短期波动和长期结构依赖，使用贝叶斯优化自动调参。

Result: 在五个作物特定数据集上的实验表明，IVGAE-TAMA显著优于静态IVGAE和其他动态基线方法，贝叶斯优化进一步提升了IVGAE-TAMA-BO的性能。

Conclusion: 该框架为全球贸易网络结构预测提供了稳健且可扩展的解决方案，在粮食安全监测和政策决策支持方面具有强大应用潜力。

Abstract: Global food trade plays a crucial role in ensuring food security and
maintaining supply chain stability. However, its network structure evolves
dynamically under the influence of geopolitical, economic, and environmental
factors, making it challenging to model and predict future trade links.
Effectively capturing temporal patterns in food trade networks is therefore
essential for improving the accuracy and robustness of link prediction. This
study introduces IVGAE-TAMA-BO, a novel dynamic graph neural network designed
to model evolving trade structures and predict future links in global food
trade networks. To the best of our knowledge, this is the first work to apply
dynamic graph neural networks to this domain, significantly enhancing
predictive performance. Building upon the original IVGAE framework, the
proposed model incorporates a Trade-Aware Momentum Aggregator (TAMA) to capture
the temporal evolution of trade networks, jointly modeling short-term
fluctuations and long-term structural dependencies. A momentum-based structural
memory mechanism further improves predictive stability and performance. In
addition, Bayesian optimization is used to automatically tune key
hyperparameters, enhancing generalization across diverse trade scenarios.
Extensive experiments on five crop-specific datasets demonstrate that
IVGAE-TAMA substantially outperforms the static IVGAE and other dynamic
baselines by effectively modeling temporal dependencies, while Bayesian
optimization further boosts performance in IVGAE-TAMA-BO. These results
highlight the proposed framework as a robust and scalable solution for
structural prediction in global trade networks, with strong potential for
applications in food security monitoring and policy decision support.

</details>


### [203] [Hybrid Retrieval-Augmented Generation Agent for Trustworthy Legal Question Answering in Judicial Forensics](https://arxiv.org/abs/2511.01668)
*Yueqing Xi,Yifan Bai,Huasen Luo,Weiliang Wen,Hui Liu,Haoliang Li*

Main category: cs.AI

TL;DR: 提出了一种混合法律问答代理，结合检索增强生成和多模型集成，为司法场景提供可靠、可审计且持续更新的法律咨询服务。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能在司法取证中的普及，确保法律问答的真实性和可追溯性变得至关重要。传统大语言模型容易产生幻觉，而静态知识库难以跟上频繁更新的法规和判例法。

Method: 采用检索优先策略：当可信法律知识库返回相关证据时使用RAG生成答案；否则使用多个LLM生成候选答案，由专门的选择器评分后返回最优答案。高质量输出经人工审核后写回知识库。

Result: 在Law_QA数据集上的实验表明，该混合方法在F1、ROUGE-L和LLM-as-a-Judge指标上显著优于单模型基线和普通RAG流程。消融实验证实了检索优先、模型集成和人工更新机制的有效性。

Conclusion: 该系统显著减少了幻觉，提高了答案质量和法律合规性，推动了媒体取证技术在司法场景中的实际落地。

Abstract: As artificial intelligence permeates judicial forensics, ensuring the
veracity and traceability of legal question answering (QA) has become critical.
Conventional large language models (LLMs) are prone to hallucination, risking
misleading guidance in legal consultation, while static knowledge bases
struggle to keep pace with frequently updated statutes and case law. We present
a hybrid legal QA agent tailored for judicial settings that integrates
retrieval-augmented generation (RAG) with multi-model ensembling to deliver
reliable, auditable, and continuously updatable counsel. The system prioritizes
retrieval over generation: when a trusted legal repository yields relevant
evidence, answers are produced via RAG; otherwise, multiple LLMs generate
candidates that are scored by a specialized selector, with the top-ranked
answer returned. High-quality outputs then undergo human review before being
written back to the repository, enabling dynamic knowledge evolution and
provenance tracking. Experiments on the Law\_QA dataset show that our hybrid
approach significantly outperforms both a single-model baseline and a vanilla
RAG pipeline on F1, ROUGE-L, and an LLM-as-a-Judge metric. Ablations confirm
the complementary contributions of retrieval prioritization, model ensembling,
and the human-in-the-loop update mechanism. The proposed system demonstrably
reduces hallucination while improving answer quality and legal compliance,
advancing the practical landing of media forensics technologies in judicial
scenarios.

</details>


### [204] [Simulating Environments with Reasoning Models for Agent Training](https://arxiv.org/abs/2511.01824)
*Yuetai Li,Huseyin A Inan,Xiang Yue,Wei-Ning Chen,Lukas Wutschitz,Janardhan Kulkarni,Radha Poovendran,Robert Sim,Saravan Rajmohan*

Main category: cs.AI

TL;DR: LLM代理在复杂环境中表现脆弱，本文提出Simia-SFT和Simia-RL框架，通过LLM模拟环境反馈实现无需真实环境的可扩展代理训练。


<details>
  <summary>Details</summary>
Motivation: LLM代理在需要跨多种工具和模式的复杂环境中表现脆弱，而构建定制训练环境成本高且限制进展。

Method: 提出Simia-SFT框架通过扩增小规模种子集生成多样化轨迹数据，以及Simia-RL框架通过LLM模拟反馈实现无需真实环境的强化学习训练。

Result: 微调开源模型在多个基准测试中取得一致改进，在τ²-Bench上超越GPT-4o并接近o4-mini水平。

Conclusion: Simia-SFT和Simia-RL能够替代繁重脆弱的环境实现，通过灵活的LLM模拟实现无需环境工程的可扩展代理训练。

Abstract: LLM agents excel in compact environments requiring deep reasoning but remain
brittle when operating in broader, more complex contexts that demand robustness
across diverse tools and schemas. Building bespoke environments for training is
heavy, brittle, and limits progress. In this paper, we demonstrate that LLMs
can simulate realistic environment feedback without access to actual testbed
data or APIs. Inspired by this capability, we propose two frameworks:
Simia-SFT, a pipeline that synthesizes SFT data by amplifying small seed sets
into diverse trajectories in an environment-agnostic manner, and Simia-RL, a
framework that enables RL training without real environment implementations
through LLM-simulated feedback. Fine-tuning open models yields consistent
improvements across multiple benchmarks, surpassing GPT-4o and approaching
o4-mini on $\tau^2$-Bench. Together, Simia-SFT and Simia-RL enable scalable
agent training without environment engineering, replacing heavy and brittle
implementations with flexible LLM-based simulation.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [205] [Temporal Fusion Transformer for Multi-Horizon Probabilistic Forecasting of Weekly Retail Sales](https://arxiv.org/abs/2511.00552)
*Santhi Bharath Punati,Sandeep Kanta,Udaya Bhasker Cheerala,Madhusudan G Lanjewar,Praveen Damacharla*

Main category: cs.LG

TL;DR: 使用时间融合变压器(TFT)对沃尔玛周度销售进行多步预测，融合静态商店标识符和动态外生变量，在1-5周预测范围内表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 准确的零售多步预测对库存和促销规划至关重要，需要处理静态和动态特征的融合。

Method: 采用时间融合变压器(TFT)模型，结合静态商店标识符和时间变化的外生信号(节假日、CPI、油价、温度)，通过分位数损失生成概率预测。

Result: 在2012年固定测试集上，TFT达到每店周RMSE 57.9k美元和R² 0.9875；5折时序交叉验证平均RMSE 64.6k美元和R² 0.9844，优于XGB、CNN、LSTM等基线模型。

Conclusion: TFT在保持模型可解释性的同时，为库存规划和节假日优化提供了实用价值。

Abstract: Accurate multi-horizon retail forecasts are critical for inventory and
promotions. We present a novel study of weekly Walmart sales (45 stores,
2010--2012) using a Temporal Fusion Transformer (TFT) that fuses static store
identifiers with time-varying exogenous signals (holidays, CPI, fuel price,
temperature). The pipeline produces 1--5-week-ahead probabilistic forecasts via
Quantile Loss, yielding calibrated 90\% prediction intervals and
interpretability through variable-selection networks, static enrichment, and
temporal attention. On a fixed 2012 hold-out dataset, TFT achieves an RMSE of
\$57.9k USD per store-week and an $R^2$ of 0.9875. Across a 5-fold
chronological cross-validation, the averages are RMSE = \$64.6k USD and $R^2$ =
0.9844, outperforming the XGB, CNN, LSTM, and CNN-LSTM baseline models. These
results demonstrate practical value for inventory planning and holiday-period
optimization, while maintaining model transparency.

</details>


### [206] [VRScout: Towards Real-Time, Autonomous Testing of Virtual Reality Games](https://arxiv.org/abs/2511.00002)
*Yurun Wu,Yousong Sun,Burkhard Wunsche,Jia Wang,Elliott Wen*

Main category: cs.LG

TL;DR: VRScout是一个基于深度学习的自主VR测试代理，能够以人类方式实时导航和交互VR环境，解决了VR内容质量保证的扩展性问题。


<details>
  <summary>Details</summary>
Motivation: VR内容的质量保证面临传统人工测试劳动密集且无法扩展的问题，而现有自动化测试方法难以适应VR的高维感官输入和实时性能要求。

Method: 使用增强型动作分块变换器从人类演示中学习，预测多步动作序列，并引入动态可调滑动视界来平衡响应性和精度。

Result: 在商业VR游戏中实现了专家级性能，仅需有限训练数据，并在消费级硬件上保持60FPS的实时推理。

Conclusion: VRScout为自动化VR游戏测试提供了一个实用且可扩展的框架，在质量保证和安全审计方面具有直接应用价值。

Abstract: Virtual Reality (VR) has rapidly become a mainstream platform for gaming and
interactive experiences, yet ensuring the quality, safety, and appropriateness
of VR content remains a pressing challenge. Traditional human-based quality
assurance is labor-intensive and cannot scale with the industry's rapid growth.
While automated testing has been applied to traditional 2D and 3D games,
extending it to VR introduces unique difficulties due to high-dimensional
sensory inputs and strict real-time performance requirements. We present
VRScout, a deep learning-based agent capable of autonomously navigating VR
environments and interacting with virtual objects in a human-like and real-time
manner. VRScout learns from human demonstrations using an enhanced Action
Chunking Transformer that predicts multi-step action sequences. This enables
our agent to capture higher-level strategies and generalize across diverse
environments. To balance responsiveness and precision, we introduce a
dynamically adjustable sliding horizon that adapts the agent's temporal context
at runtime. We evaluate VRScout on commercial VR titles and show that it
achieves expert-level performance with only limited training data, while
maintaining real-time inference at 60 FPS on consumer-grade hardware. These
results position VRScout as a practical and scalable framework for automated VR
game testing, with direct applications in both quality assurance and safety
auditing.

</details>


### [207] [Feature-Guided SAE Steering for Refusal-Rate Control using Contrasting Prompts](https://arxiv.org/abs/2511.00029)
*Samaksh Bhargav,Zining Zhu*

Main category: cs.LG

TL;DR: 使用稀疏自编码器(SAE)进行特征选择和定向引导，在Llama-3 8B模型上实现了安全性能提升18.9%同时实用性提升11.1%，突破了传统安全-实用性权衡。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要调整模型权重且过程昂贵，而稀疏自编码器虽然能提取可解释特征，但缺乏系统性的特征选择方法和原则性的安全-效用权衡评估。

Method: 使用对比提示方法从AI-Generated Prompts Dataset和Air Bench eu-dataset中高效选择最佳引导特征，通过不同引导特征和引导强度进行SAE定向引导。

Result: 在Llama-3 8B模型上，安全性能提升18.9%，同时实用性提升11.1%。

Conclusion: 当通过原则性选择方法识别出最优特征时，定向SAE引导能够克服传统安全-实用性权衡。

Abstract: Large Language Model (LLM) deployment requires guiding the LLM to recognize
and not answer unsafe prompts while complying with safe prompts. Previous
methods for achieving this require adjusting model weights along with other
expensive procedures. While recent advances in Sparse Autoencoders (SAEs) have
enabled interpretable feature extraction from LLMs, existing approaches lack
systematic feature selection methods and principled evaluation of
safety-utility tradeoffs. We explored using different steering features and
steering strengths using Sparse Auto Encoders (SAEs) to provide a solution.
Using an accurate and innovative contrasting prompt method with the
AI-Generated Prompts Dataset from teknium/OpenHermes-2p5-Mistral-7B and Air
Bench eu-dataset to efficiently choose the best features in the model to steer,
we tested this method on Llama-3 8B. We conclude that using this method, our
approach achieves an 18.9% improvement in safety performance while
simultaneously increasing utility by 11.1%, demonstrating that targeted SAE
steering can overcome traditional safety-utility tradeoffs when optimal
features are identified through principled selection methods.

</details>


### [208] [Probing Knowledge Holes in Unlearned LLMs](https://arxiv.org/abs/2511.00030)
*Myeongseob Ko,Hoang Anh Just,Charles Fleming,Ming Jin,Ruoxi Jia*

Main category: cs.LG

TL;DR: 机器遗忘技术虽然能有效移除不良知识，但会意外造成"知识漏洞"——良性知识的非预期损失，标准基准测试难以检测到这些漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘技术虽然能移除不良内容且不影响标准基准测试性能，但可能造成隐藏的知识损失，需要更全面的评估方法。

Method: 提出测试用例生成框架，探索遗忘内容邻近区域和更广泛的潜在失败区域，评估遗忘模型的隐藏成本。

Result: 评估显示遗忘模型在高达98.7%的测试用例中产生无关或荒谬的响应，而这些测试用例在预训练模型中是可回答的。

Conclusion: 需要重新思考评估机器遗忘中知识保存的传统方法，超越标准静态基准测试。

Abstract: Machine unlearning has emerged as a prevalent technical solution for
selectively removing unwanted knowledge absorbed during pre-training, without
requiring full retraining. While recent unlearning techniques can effectively
remove undesirable content without severely compromising performance on
standard benchmarks, we find that they may inadvertently create ``knowledge
holes'' -- unintended losses of benign knowledge that standard benchmarks fail
to capture. To probe where unlearned models reveal knowledge holes, we propose
a test case generation framework that explores both immediate neighbors of
unlearned content and broader areas of potential failures. Our evaluation
demonstrates significant hidden costs of unlearning: up to 98.7\% of the test
cases yield irrelevant or nonsensical responses from unlearned models, despite
being answerable by the pretrained model. These findings necessitate rethinking
the conventional approach to evaluating knowledge preservation in unlearning,
moving beyond standard, static benchmarks.

</details>


### [209] [One model to solve them all: 2BSDE families via neural operators](https://arxiv.org/abs/2511.01125)
*Takashi Furuya,Anastasis Kratsios,Dylan Possamaï,Bogdan Raonić*

Main category: cs.LG

TL;DR: 提出了一种基于Kolmogorov-Arnold网络的温和生成变体神经算子模型，用于求解具有随机终止时间的二阶倒向随机微分方程(2BSDEs)无限族。


<details>
  <summary>Details</summary>
Motivation: 解决在正则有界欧几里得域上具有随机终止时间的二阶倒向随机微分方程无限族的求解问题，并探索神经算子模型在近似解算子方面的能力。

Method: 使用Kolmogorov-Arnold网络构建生成变体神经算子模型，通过结构化子类识别实现多项式参数复杂度的近似。

Result: 证明了对于广泛的2BSDE族，解算子可由适当的神经算子模型近似；识别出特定子类仅需多项式数量的参数即可达到给定近似精度。

Conclusion: 该方法在特定结构化子类中实现了从指数级到多项式级参数复杂度的显著改进，为高效求解2BSDE无限族提供了理论保证。

Abstract: We introduce a mild generative variant of the classical neural operator
model, which leverages Kolmogorov--Arnold networks to solve infinite families
of second-order backward stochastic differential equations ($2$BSDEs) on
regular bounded Euclidean domains with random terminal time. Our first main
result shows that the solution operator associated with a broad range of
$2$BSDE families is approximable by appropriate neural operator models. We then
identify a structured subclass of (infinite) families of $2$BSDEs whose neural
operator approximation requires only a polynomial number of parameters in the
reciprocal approximation rate, as opposed to the exponential requirement in
general worst-case neural operator guarantees.

</details>


### [210] [From Uniform to Adaptive: General Skip-Block Mechanisms for Efficient PDE Neural Operators](https://arxiv.org/abs/2511.00032)
*Lei Liu,Zhongyi Yu,Hong Wang,Huanshuo Dong,Haiyang Xin,Hongwei Zhao,Bin Li*

Main category: cs.LG

TL;DR: 提出Skip-Block Routing(SBR)框架，用于Transformer类神经算子，通过路由机制学习token复杂度排序，在推理时根据复杂度动态分配计算资源，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前神经算子在处理大规模工程问题时存在显著计算开销，且统一计算成本与物理场复杂度差异不匹配，导致效率低下。

Method: SBR框架包含路由机制学习token复杂度和排序，在推理时根据排序结果决定后续层传递的token数量，使模型更关注复杂区域。

Result: SBR可集成到多种神经算子中，减少约50%的FLOPs计算成本，推理速度提升达2倍，且不牺牲精度。

Conclusion: SBR是通用的高效框架，通过自适应计算资源分配解决了神经算子在大规模PDE求解中的计算效率问题。

Abstract: In recent years, Neural Operators(NO) have gradually emerged as a popular
approach for solving Partial Differential Equations (PDEs). However, their
application to large-scale engineering tasks suffers from significant
computational overhead. And the fact that current models impose a uniform
computational cost while physical fields exhibit vastly different complexities
constitutes a fundamental mismatch, which is the root of this inefficiency. For
instance, in turbulence flows, intricate vortex regions require deeper network
processing compared to stable flows. To address this, we introduce a framework:
Skip-Block Routing (SBR), a general framework designed for Transformer-based
neural operators, capable of being integrated into their multi-layer
architectures. First, SBR uses a routing mechanism to learn the complexity and
ranking of tokens, which is then applied during inference. Then, in later
layers, it decides how many tokens are passed forward based on this ranking.
This way, the model focuses more processing capacity on the tokens that are
more complex. Experiments demonstrate that SBR is a general framework that
seamlessly integrates into various neural operators. Our method reduces
computational cost by approximately 50% in terms of Floating Point Operations
(FLOPs), while still delivering up to 2x faster inference without sacrificing
accuracy.

</details>


### [211] [Neural Architecture Search for global multi-step Forecasting of Energy Production Time Series](https://arxiv.org/abs/2511.00035)
*Georg Velev,Stefan Lessmann*

Main category: cs.LG

TL;DR: 提出基于神经架构搜索（NAS）的自动化框架，用于发现平衡计算效率、预测性能和泛化能力的时序模型，以解决能源生产短期预测中的多步预测问题。


<details>
  <summary>Details</summary>
Motivation: 能源领域需要准确且高效的短期预测方法，但现有复杂方法需要手动配置，耗时且易出错，同时需要处理时序数据的动态特性和对未见数据的泛化能力。

Method: 设计基于NAS的框架，构建仅包含高效组件的搜索空间来捕捉能源时序数据的独特模式，并制定考虑时序上下文泛化性能和搜索空间探索的新目标函数。

Result: 在能源生产时序数据上的实验表明，通过NAS发现的轻量级架构集成在效率和准确性方面均优于Transformer等最先进技术以及预训练预测模型。

Conclusion: NAS框架能够自动发现高效且准确的能源预测模型，解决了手动配置的复杂性和泛化问题，为能源领域的短期预测提供了有效的自动化解决方案。

Abstract: The dynamic energy sector requires both predictive accuracy and runtime
efficiency for short-term forecasting of energy generation under operational
constraints, where timely and precise predictions are crucial. The manual
configuration of complex methods, which can generate accurate global multi-step
predictions without suffering from a computational bottleneck, represents a
procedure with significant time requirements and high risk for human-made
errors. A further intricacy arises from the temporal dynamics present in
energy-related data. Additionally, the generalization to unseen data is
imperative for continuously deploying forecasting techniques over time. To
overcome these challenges, in this research, we design a neural architecture
search (NAS)-based framework for the automated discovery of time series models
that strike a balance between computational efficiency, predictive performance,
and generalization power for the global, multi-step short-term forecasting of
energy production time series. In particular, we introduce a search space
consisting only of efficient components, which can capture distinctive patterns
of energy time series. Furthermore, we formulate a novel objective function
that accounts for performance generalization in temporal context and the
maximal exploration of different regions of our high-dimensional search space.
The results obtained on energy production time series show that an ensemble of
lightweight architectures discovered with NAS outperforms state-of-the-art
techniques, such as Transformers, as well as pre-trained forecasting models, in
terms of both efficiency and accuracy.

</details>


### [212] [Semi-Supervised Preference Optimization with Limited Feedback](https://arxiv.org/abs/2511.00040)
*Seonggyun Lee,Sungjun Lim,Seojin Park,Soeun Cheon,Kyungwoo Song*

Main category: cs.LG

TL;DR: 提出半监督偏好优化(SSPO)方法，利用少量成对偏好标签和大量未配对数据，通过理论证明的最优奖励阈值进行伪标注，显著降低数据获取成本。


<details>
  <summary>Details</summary>
Motivation: 现有偏好优化方法严重依赖大量成对反馈数据，导致资源消耗巨大。为解决这一问题，研究如何同时利用少量标签数据和大量未配对数据。

Method: 提出SSPO框架，理论证明存在最优奖励阈值能高概率区分胜负响应，据此对未配对数据进行伪标注，然后利用伪标签从大规模未配对数据中提取潜在偏好。

Result: 实验验证了卓越的数据效率：使用Llama3-8B-Instruct在仅1% UltraFeedback数据上训练的SSPO，性能优于在10%数据上训练的强基线方法。

Conclusion: SSPO能有效维护人类对齐，同时大幅降低数据获取成本，为偏好优化提供了高效的数据利用方案。

Abstract: The field of preference optimization has made outstanding contributions to
the alignment of language models with human preferences. Despite these
advancements, recent methods still rely heavily on substantial paired (labeled)
feedback data, leading to substantial resource expenditures. To address these
challenges, we study the problem of Semi-Supervised Preference Optimization
(SSPO) in which the idea is to learn from both a small number of pairwise
preference labels and a large pool of unpaired samples simultaneously. Our key
theoretical contribution proves the existence of an optimal reward threshold
capable of separating winning and losing responses with high probability, which
enables a principled pseudo-labeling of unpaired data. By leveraging these
pseudo-labels, SSPO effectively distills latent preferences from large-scale
unpaired data, thus maintaining human alignment while drastically reducing
acquisition costs. Extensive experiments across datasets validate this
remarkable data efficiency; for instance, SSPO trained with Llama3-8B-Instruct
on just 1% of UltraFeedback consistently surpasses strong baselines trained on
10% of UltraFeedback.

</details>


### [213] [Physics-Informed Neural Network Frameworks for the Analysis of Engineering and Biological Dynamical Systems Governed by Ordinary Differential Equations](https://arxiv.org/abs/2511.00043)
*Tyrus Whitman,Andrew Particka,Christopher Diers,Ian Griffin,Charuka Wickramasinghe,Pradeep Ranaweera*

Main category: cs.LG

TL;DR: 本研究验证了物理信息神经网络(PINNs)在求解工程和生物动力学系统ODE问题中的预测能力，通过系统调优损失函数平衡和超参数设置，PINNs能够有效处理传统数值方法难以收敛的复杂问题。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法在处理高刚度、冲击、不规则域、奇异摄动、高维或边界不连续等复杂ODE问题时往往难以收敛，需要寻找更强大的替代方法。

Method: 使用经典ODE问题作为受控测试平台，系统评估PINNs框架的准确性、训练效率和泛化能力。通过适当平衡数据损失、初值条件损失和残差损失，并系统调优网络深度、层宽、激活函数、学习率等超参数。

Result: PINNs能够收敛到正确解，但需要仔细平衡损失函数分量，并通过系统超参数调优来确保准确性。嵌入先验知识和施加硬约束能显著增强PINNs的预测能力。

Conclusion: PINNs虽然不是万能解决方案，但通过将物理定律直接嵌入学习过程，能够获得优越的结果，特别适合处理传统数值方法难以解决的复杂ODE问题。

Abstract: In this study, we present and validate the predictive capability of the
Physics-Informed Neural Networks (PINNs) methodology for solving a variety of
engineering and biological dynamical systems governed by ordinary differential
equations (ODEs). While traditional numerical methods a re effective for many
ODEs, they often struggle to achieve convergence in problems involving high
stiffness, shocks, irregular domains, singular perturbations, high dimensions,
or boundary discontinuities. Alternatively, PINNs offer a powerful approach for
handling challenging numerical scenarios. In this study, classical ODE problems
are employed as controlled testbeds to systematically evaluate the accuracy,
training efficiency, and generalization capability under controlled conditions
of the PINNs framework. Although not a universal solution, PINNs can achieve
superior results by embedding physical laws directly into the learning process.
We first analyze the existence and uniqueness properties of several benchmark
problems and subsequently validate the PINNs methodology on these model
systems. Our results demonstrate that for complex problems to converge to
correct solutions, the loss function components data loss, initial condition
loss, and residual loss must be appropriately balanced through careful
weighting. We further establish that systematic tuning of hyperparameters,
including network depth, layer width, activation functions, learning rate,
optimization algorithms, w eight initialization schemes, and collocation point
sampling, plays a crucial role in achieving accurate solutions. Additionally,
embedding prior knowledge and imposing hard constraints on the network
architecture, without loss the generality of the ODE system, significantly
enhances the predictive capability of PINNs.

</details>


### [214] [ReLaX-Net: Reusing Layers for Parameter-Efficient Physical Neural Networks](https://arxiv.org/abs/2511.00044)
*Kohei Tsuchiyama,Andre Roehm,Takatomo Mihana,Ryoichi Horisaki*

Main category: cs.LG

TL;DR: 提出ReLaX-Net架构，通过时间复用层来扩展物理神经网络深度，提高参数效率


<details>
  <summary>Details</summary>
Motivation: 物理神经网络在规模上落后数字神经网络数个数量级，需要参数高效架构

Method: 采用层间时间复用方案，仅需在现有PNN基础上添加快速开关

Result: 在图像分类和自然语言处理任务中验证，ReLaX-Net在相同参数数量下优于传统RNN/DNN

Conclusion: ReLaX-Net通过简单修改显著提升PNN计算性能，具有有利的扩展性

Abstract: Physical Neural Networks (PNN) are promising platforms for next-generation
computing systems. However, recent advances in digital neural network
performance are largely driven by the rapid growth in the number of trainable
parameters and, so far, demonstrated PNNs are lagging behind by several orders
of magnitude in terms of scale. This mirrors size and performance constraints
found in early digital neural networks. In that period, efficient reuse of
parameters contributed to the development of parameter-efficient architectures
such as convolutional neural networks.
  In this work, we numerically investigate hardware-friendly weight-tying for
PNNs. Crucially, with many PNN systems, there is a time-scale separation
between the fast dynamic active elements of the forward pass and the only
slowly trainable elements implementing weights and biases. With this in mind,we
propose the Reuse of Layers for eXpanding a Neural Network (ReLaX-Net)
architecture, which employs a simple layer-by-layer time-multiplexing scheme to
increase the effective network depth and efficiently use the number of
parameters. We only require the addition of fast switches for existing PNNs. We
validate ReLaX-Nets via numerical experiments on image classification and
natural language processing tasks. Our results show that ReLaX-Net improves
computational performance with only minor modifications to a conventional PNN.
We observe a favorable scaling, where ReLaX-Nets exceed the performance of
equivalent traditional RNNs or DNNs with the same number of parameters.

</details>


### [215] [DynBERG: Dynamic BERT-based Graph neural network for financial fraud detection](https://arxiv.org/abs/2511.00047)
*Omkar Kulkarni,Rohitash Chandra*

Main category: cs.LG

TL;DR: 提出了DynBERG模型，结合Graph-BERT和GRU层来捕捉动态金融交易网络的时间演化，支持有向边，在比特币交易数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 金融交易网络具有动态性和有向性，而现有的Graph-BERT等图Transformer模型主要针对静态无向图设计，无法有效处理金融欺诈检测中的时序动态特征。

Method: 将Graph-BERT与门控循环单元(GRU)集成，修改底层算法以支持有向边，构建DynBERG架构来捕捉多时间步的时序演化。

Result: 在Elliptic比特币交易数据集上评估，DynBERG在市场关闭事件前后均表现优异，超越EvolveGCN和GCN等基准方法，消融研究证实GRU组件对建模时序动态至关重要。

Conclusion: DynBERG通过整合图Transformer和时序深度学习组件，有效解决了动态金融交易网络中的欺诈检测问题，能够适应重大市场变化对交易行为的影响。

Abstract: Financial fraud detection is critical for maintaining the integrity of
financial systems, particularly in decentralised environments such as
cryptocurrency networks. Although Graph Convolutional Networks (GCNs) are
widely used for financial fraud detection, graph Transformer models such as
Graph-BERT are gaining prominence due to their Transformer-based architecture,
which mitigates issues such as over-smoothing. Graph-BERT is designed for
static graphs and primarily evaluated on citation networks with undirected
edges. However, financial transaction networks are inherently dynamic, with
evolving structures and directed edges representing the flow of money. To
address these challenges, we introduce DynBERG, a novel architecture that
integrates Graph-BERT with a Gated Recurrent Unit (GRU) layer to capture
temporal evolution over multiple time steps. Additionally, we modify the
underlying algorithm to support directed edges, making DynBERG well-suited for
dynamic financial transaction analysis. We evaluate our model on the Elliptic
dataset, which includes Bitcoin transactions, including all transactions during
a major cryptocurrency market event, the Dark Market Shutdown. By assessing
DynBERG's resilience before and after this event, we analyse its ability to
adapt to significant market shifts that impact transaction behaviours. Our
model is benchmarked against state-of-the-art dynamic graph classification
approaches, such as EvolveGCN and GCN, demonstrating superior performance,
outperforming EvolveGCN before the market shutdown and surpassing GCN after the
event. Additionally, an ablation study highlights the critical role of
incorporating a time-series deep learning component, showcasing the
effectiveness of GRU in modelling the temporal dynamics of financial
transactions.

</details>


### [216] [Adaptive Spatio-Temporal Graphs with Self-Supervised Pretraining for Multi-Horizon Weather Forecasting](https://arxiv.org/abs/2511.00049)
*Yao Liu*

Main category: cs.LG

TL;DR: 提出了一种新颖的自监督学习框架，利用时空结构改进多变量天气预报，结合图神经网络、自监督预训练和时空适应机制，在ERA5和MERRA-2数据集上优于传统数值天气预报模型和深度学习方法。


<details>
  <summary>Details</summary>
Motivation: 由于大气系统固有的时空复杂性，准确和稳健的天气预报仍然是一个基本挑战。

Method: 集成图神经网络进行空间推理，自监督预训练方案进行表示学习，以及时空适应机制来增强不同预测时间范围的泛化能力。

Result: 在ERA5和MERRA-2再分析数据集上的广泛实验表明，该方法相比传统数值天气预报模型和最近的深度学习方法实现了更优的性能。北京和上海的定量评估和视觉分析证实了模型捕捉细粒度气象模式的能力。

Conclusion: 所提出的框架为未来数据驱动的天气预报系统提供了一个可扩展且标签高效的解决方案。

Abstract: Accurate and robust weather forecasting remains a fundamental challenge due
to the inherent spatio-temporal complexity of atmospheric systems. In this
paper, we propose a novel self-supervised learning framework that leverages
spatio-temporal structures to improve multi-variable weather prediction. The
model integrates a graph neural network (GNN) for spatial reasoning, a
self-supervised pretraining scheme for representation learning, and a
spatio-temporal adaptation mechanism to enhance generalization across varying
forecasting horizons. Extensive experiments on both ERA5 and MERRA-2 reanalysis
datasets demonstrate that our approach achieves superior performance compared
to traditional numerical weather prediction (NWP) models and recent deep
learning methods. Quantitative evaluations and visual analyses in Beijing and
Shanghai confirm the model's capability to capture fine-grained meteorological
patterns. The proposed framework provides a scalable and label-efficient
solution for future data-driven weather forecasting systems.

</details>


### [217] [FLoRA: Fused forward-backward adapters for parameter efficient fine-tuning and reducing inference-time latencies of LLMs](https://arxiv.org/abs/2511.00050)
*Dhananjaya Gowda,Seoha Song,Junhyun Lee,Harshith Goka*

Main category: cs.LG

TL;DR: 提出了FLoRA方法，一种融合前向-后向适配器的参数高效微调技术，结合了LoRA和并行适配器的优点，在保持相似参数预算的同时显著提升了精度并降低了延迟。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模日益增长，高效训练和微调变得至关重要。虽然参数高效微调方法已有广泛研究，但仍有大量自由度未被探索。

Method: FLoRA融合了流行的LoRA和并行适配器的思想，通过将前向和后向适配器融合到基础模型的投影层中来最小化延迟。

Result: 实验结果表明，在相似参数预算下，提出的FFB适配器在精度和延迟方面都显著优于常用的LoRA方法。

Conclusion: FLoRA为LLM的下游任务微调提供了一种高效的参数高效微调方案，在性能和效率方面都有显著提升。

Abstract: As the large language models (LLMs) grow in size each day, efficient training
and fine-tuning has never been as important as nowadays. This resulted in the
great interest in parameter efficient fine-tuning (PEFT), and effective methods
including low-rank adapters (LoRA) has emerged. Although the various PEFT
methods have been studied extensively in the recent years, the greater part of
the subject remains unexplored with the huge degree of freedom. In this paper,
we propose FLoRA, a family of fused forward-backward adapters (FFBA) for
parameter-efficient fine-tuning of LLMs on downstream tasks. The FFBA combine
ideas from the popular LoRA and parallel adapters to improve the overall
fine-tuning accuracies. At the same time, latencies are minimized by fusing the
forward and backward adapters into existing projection layers of the base
model. Experimental results show that the proposed FFB adapters perform
significantly better than the popularly used LoRA in both accuracy and latency
for a similar parameter budget.

</details>


### [218] [Calibrating and Rotating: A Unified Framework for Weight Conditioning in PEFT](https://arxiv.org/abs/2511.00051)
*Da Chang,Peng Xue,Yu Li,Yongxiang Liu,Pengxiang Xu,Shixun Zhang*

Main category: cs.LG

TL;DR: 本文分析了DoRA方法的成功机制，发现其通过增加权重更新矩阵的奇异值熵来提升性能。作者将DoRA重新表述为更高效的矩阵形式，提出了一个统一的PEFT框架，并在此框架下开发了两种新方法：Pre-Diag和SORA，在性能和效率上均优于LoRA和DoRA。


<details>
  <summary>Details</summary>
Motivation: DoRA方法虽然性能优异，但其工作机制不明确且计算开销较大。本文旨在揭示DoRA的成功机制，并基于此开发更高效的参数高效微调方法。

Method: 首先识别DoRA通过增加权重更新矩阵的奇异值熵来提升性能；然后将DoRA重新表述为数学等价但更高效的矩阵形式；最后提出了统一的PEFT框架，并在此框架下开发了Pre-Diag和SORA两种新方法。

Result: 在自然语言理解和生成任务上的广泛实验表明，提出的Pre-Diag和SORA方法在性能和效率上均优于LoRA和DoRA。

Conclusion: 本文揭示了DoRA的成功机制，提出了统一的PEFT框架，并开发了两种高效的新方法，为参数高效微调领域提供了新的设计思路。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods are crucial for adapting large
pre-trained models. Among these, LoRA is considered a foundational approach.
Building on this, the influential DoRA method enhances performance by
decomposing weight updates into magnitude and direction. However, its
underlying mechanism remains unclear, and it introduces significant
computational overhead. In this work, we first identify that DoRA's success
stems from its capacity to increase the singular value entropy of the weight
update matrix, which promotes a more uniform update distribution akin to full
fine-tuning. We then reformulate DoRA into a mathematically equivalent and more
efficient matrix form, revealing it as a learnable weight conditioning method.
Based on this insight, we propose a unified framework for designing advanced
PEFT methods by exploring two orthogonal dimensions: the architectural
placement and the transformation type of the conditioning matrix. Within this
framework, we introduce two novel methods: (1) \textbf{Pre-Diag}, which applies
a diagonal conditioning matrix before the LoRA update to efficiently calibrate
the pre-trained weights, thereby enhancing performance while reducing training
time; and (2) \textbf{S}kewed \textbf{O}rthogonal \textbf{R}otation
\textbf{A}daptation (\textbf{SORA}), which employs a parameter-efficient
orthogonal rotation to perform a more powerful, norm-preserving transformation
of the feature space. Extensive experiments on natural language understanding
and generation tasks demonstrate that our proposed methods achieve superior
performance and efficiency compared to both LoRA and DoRA. The code is
available at https://github.com/MaeChd/SORA.

</details>


### [219] [Feature-Guided Analysis of Neural Networks: A Replication Study](https://arxiv.org/abs/2511.00052)
*Federico Formica,Stefano Gregis,Aurora Francesca Zanenga,Andrea Rota,Mark Lawford,Claudio Menghi*

Main category: cs.LG

TL;DR: 本文评估了特征引导分析（FGA）在MNIST和LSC数据集上的适用性，发现FGA在基准测试中比文献结果具有更高精度，且神经网络架构、训练和特征选择对FGA的召回率有显著影响，但对精度影响可忽略。


<details>
  <summary>Details</summary>
Motivation: 理解神经网络决策原因对其在安全关键应用中使用至关重要。现有特征引导方法通过监控神经元激活提取相关规则，但在工业环境中的应用需要更多实证证据。

Method: 使用MNIST和LSC数据集作为基准，评估FGA在计算解释神经网络行为的规则方面的有效性，并分析神经网络架构、训练和特征选择对FGA效果的影响。

Result: FGA在基准测试中比文献结果具有更高精度。神经网络架构、训练和特征选择对FGA的召回率有显著影响，但对精度影响可忽略。

Conclusion: FGA在解释神经网络行为方面具有良好精度，但召回率受模型选择和特征选择影响较大，需要在工业应用中进一步验证。

Abstract: Understanding why neural networks make certain decisions is pivotal for their
use in safety-critical applications. Feature-Guided Analysis (FGA) extracts
slices of neural networks relevant to their tasks. Existing feature-guided
approaches typically monitor the activation of the neural network neurons to
extract the relevant rules. Preliminary results are encouraging and demonstrate
the feasibility of this solution by assessing the precision and recall of
Feature-Guided Analysis on two pilot case studies. However, the applicability
in industrial contexts needs additional empirical evidence.
  To mitigate this need, this paper assesses the applicability of FGA on a
benchmark made by the MNIST and LSC datasets. We assessed the effectiveness of
FGA in computing rules that explain the behavior of the neural network. Our
results show that FGA has a higher precision on our benchmark than the results
from the literature. We also evaluated how the selection of the neural network
architecture, training, and feature selection affect the effectiveness of FGA.
Our results show that the selection significantly affects the recall of FGA,
while it has a negligible impact on its precision.

</details>


### [220] [Quadratic Direct Forecast for Training Multi-Step Time-Series Forecast Models](https://arxiv.org/abs/2511.00053)
*Hao Wang,Licheng Pan,Yuan Lu,Zhichao Chen,Tianqiao Liu,Shuting He,Zhixuan Chu,Qingsong Wen,Haoxuan Li,Zhouchen Lin*

Main category: cs.LG

TL;DR: 提出了一种新的二次形式加权训练目标，通过考虑标签自相关效应和异质任务权重来改进时间序列预测模型的训练效果。


<details>
  <summary>Details</summary>
Motivation: 现有训练目标（如均方误差）将每个未来步视为独立、等权重的任务，这导致两个问题：(1)忽略未来步之间的标签自相关效应，造成训练目标偏差；(2)无法为不同预测任务设置异质权重，限制了预测性能。

Method: 提出二次形式直接预测(QDF)学习算法，使用自适应更新的二次形式加权矩阵进行模型训练。加权矩阵的非对角元素考虑标签自相关效应，非均匀对角元素匹配不同预测任务的最优权重。

Result: 实验表明QDF有效提升了各种预测模型的性能，取得了最先进的结果。

Conclusion: 所提出的二次形式加权训练目标能够同时解决标签自相关效应和异质任务权重问题，显著改进时间序列预测性能。

Abstract: The design of training objective is central to training time-series
forecasting models. Existing training objectives such as mean squared error
mostly treat each future step as an independent, equally weighted task, which
we found leading to the following two issues: (1) overlook the label
autocorrelation effect among future steps, leading to biased training
objective; (2) fail to set heterogeneous task weights for different forecasting
tasks corresponding to varying future steps, limiting the forecasting
performance. To fill this gap, we propose a novel quadratic-form weighted
training objective, addressing both of the issues simultaneously. Specifically,
the off-diagonal elements of the weighting matrix account for the label
autocorrelation effect, whereas the non-uniform diagonals are expected to match
the most preferable weights of the forecasting tasks with varying future steps.
To achieve this, we propose a Quadratic Direct Forecast (QDF) learning
algorithm, which trains the forecast model using the adaptively updated
quadratic-form weighting matrix. Experiments show that our QDF effectively
improves performance of various forecast models, achieving state-of-the-art
results. Code is available at https://anonymous.4open.science/r/QDF-8937.

</details>


### [221] [flowengineR: A Modular and Extensible Framework for Fair and Reproducible Workflow Design in R](https://arxiv.org/abs/2511.00079)
*Maximilian Willer,Peter Ruckdeschel*

Main category: cs.LG

TL;DR: flowengineR是一个R包，为构建可复现的通用机器学习流水线提供模块化、可扩展的框架，特别关注算法公平性领域。


<details>
  <summary>Details</summary>
Motivation: 算法公平性领域快速发展，新指标、缓解策略和机器学习方法不断涌现，现有工具包要么过于专注于单一干预，要么将可复现性和可扩展性作为次要考虑而非核心设计原则。

Method: 引入统一架构，包含数据分割、执行、预处理、训练、处理中、后处理、评估和报告等标准化引擎。每个引擎封装一个方法任务，通过轻量级接口通信，确保工作流透明、可审计且易于扩展。

Result: 通过将公平性方法构建为可互换的引擎，研究人员可以在建模流水线中集成、比较和评估干预措施。该架构还可泛化到可解释性、鲁棒性和合规性指标，无需核心修改。

Conclusion: 虽然受公平性研究启发，但flowengineR最终为任何需要可复现性、透明度和可扩展性的工作流环境提供了通用基础设施。

Abstract: flowengineR is an R package designed to provide a modular and extensible
framework for building reproducible algorithmic workflows for general-purpose
machine learning pipelines. It is motivated by the rapidly evolving field of
algorithmic fairness where new metrics, mitigation strategies, and machine
learning methods continuously emerge. A central challenge in fairness, but also
far beyond, is that existing toolkits either focus narrowly on single
interventions or treat reproducibility and extensibility as secondary
considerations rather than core design principles. flowengineR addresses this
by introducing a unified architecture of standardized engines for data
splitting, execution, preprocessing, training, inprocessing, postprocessing,
evaluation, and reporting. Each engine encapsulates one methodological task yet
communicates via a lightweight interface, ensuring workflows remain
transparent, auditable, and easily extensible. Although implemented in R,
flowengineR builds on ideas from workflow languages (CWL, YAWL), graph-oriented
visual programming languages (KNIME), and R frameworks (BatchJobs, batchtools).
Its emphasis, however, is less on orchestrating engines for resilient parallel
execution but rather on the straightforward setup and management of distinct
engines as data structures. This orthogonalization enables distributed
responsibilities, independent development, and streamlined integration. In
fairness context, by structuring fairness methods as interchangeable engines,
flowengineR lets researchers integrate, compare, and evaluate interventions
across the modeling pipeline. At the same time, the architecture generalizes to
explainability, robustness, and compliance metrics without core modifications.
While motivated by fairness, it ultimately provides a general infrastructure
for any workflow context where reproducibility, transparency, and extensibility
are essential.

</details>


### [222] [SpatialTraceGen: High-Fidelity Traces for Efficient VLM Spatial Reasoning Distillation](https://arxiv.org/abs/2511.00054)
*Gio Huh,Dhruv Sheth,Rayhan Zirvi,Frank Xiao*

Main category: cs.LG

TL;DR: 提出了SpatialTraceGen框架，通过蒸馏大型教师模型的推理过程来生成高质量的多步骤、多工具推理轨迹数据集，解决视觉语言模型在复杂空间推理任务中缺乏高质量训练数据的问题。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在复杂空间推理方面表现不佳，需要问题分解和策略性工具使用。微调小型模型是高效路径，但缺乏高质量的分步推理数据成为主要瓶颈。

Method: 开发了SpatialTraceGen框架，包含自动化验证器来确保每个推理步骤的保真度，通过蒸馏大型教师模型的推理过程生成多跳、多工具推理轨迹数据集。

Result: 在CLEVR-Humans基准测试中，验证器引导的过程使轨迹平均质量得分提高了17%，同时质量方差降低了40%以上。

Conclusion: SpatialTraceGen提供了专家轨迹数据集，为有效微调和样本高效的离线强化学习提供了结构化的分步工具使用示例。

Abstract: While Vision-Language Models (VLMs) excel in many areas, they struggle with
complex spatial reasoning, which requires problem decomposition and strategic
tool use. Fine-tuning smaller, more deployable models offers an efficient path
to strong performance, but this is hampered by a major bottleneck: the absence
of high-quality, step-by-step reasoning data. To address this data-efficiency
gap, we introduce SpatialTraceGen, a framework to distill the reasoning
processes of a large teacher model into a high-quality dataset of multi-hop,
multi-tool reasoning traces. A key innovation is our automated Verifier, which
scalably ensures the fidelity of each reasoning step, providing a
cost-effective alternative to manual human annotation. On the CLEVR-Humans
benchmark, this verifier-guided process improves the average quality score of
traces by 17\% while reducing quality variance by over 40\%. SpatialTraceGen
delivers a dataset of expert traces, providing the structured, step-by-step
examples of tool use necessary for effective fine-tuning and sample-efficient
offline reinforcement learning.

</details>


### [223] [Exploring Federated Learning for Thermal Urban Feature Segmentation -- A Comparison of Centralized and Decentralized Approaches](https://arxiv.org/abs/2511.00055)
*Leonhard Duda,Khadijeh Alibabaei,Elena Vollmer,Leon Klug,Valentin Kozlov,Lisana Berberi,Mishal Benz,Rebekka Volk,Juan Pedro Gutiérrez Hermosillo Muriedas,Markus Götz,Judith Sáínz-Pardo Díaz,Álvaro López García,Frank Schultmann,Achim Streit*

Main category: cs.LG

TL;DR: 本文研究了联邦学习在无人机热成像图像分割任务中的实际应用效果，比较了多种FL方法与集中式学习的性能差异。


<details>
  <summary>Details</summary>
Motivation: 由于隐私和技术限制，分布式数据无法集中存储和共享，联邦学习可以绕过这些限制，让参与者在本地训练模型而无需共享数据。

Method: 在真实部署场景中评估FL算法，比较多种FL方法与集中式学习基线，分析客户端控制和服务端控制的工作流程。

Result: 评估了模型精度、训练时间、通信开销和能耗等关键性能指标，为理解FL方法在无人机成像分割任务中的实际应用提供了参考。

Conclusion: 该研究为理解联邦学习方法在无人机热成像分割任务中的实际应用和局限性提供了有价值的参考。

Abstract: Federated Learning (FL) is an approach for training a shared Machine Learning
(ML) model with distributed training data and multiple participants. FL allows
bypassing limitations of the traditional Centralized Machine Learning CL if
data cannot be shared or stored centrally due to privacy or technical
restrictions -- the participants train the model locally with their training
data and do not need to share it among the other participants. This paper
investigates the practical implementation and effectiveness of FL in a
real-world scenario, specifically focusing on unmanned aerial vehicle
(UAV)-based thermal images for common thermal feature detection in urban
environments. The distributed nature of the data arises naturally and makes it
suitable for FL applications, as images captured in two German cities are
available. This application presents unique challenges due to non-identical
distribution and feature characteristics of data captured at both locations.
The study makes several key contributions by evaluating FL algorithms in real
deployment scenarios rather than simulation. We compare several FL approaches
with a centralized learning baseline across key performance metrics such as
model accuracy, training time, communication overhead, and energy usage. This
paper also explores various FL workflows, comparing client-controlled workflows
and server-controlled workflows. The findings of this work serve as a valuable
reference for understanding the practical application and limitations of the FL
methods in segmentation tasks in UAV-based imaging.

</details>


### [224] [Diffusion LLMs are Natural Adversaries for any LLM](https://arxiv.org/abs/2511.00203)
*David Lüdke,Tom Wollschläger,Paul Ungermann,Stephan Günnemann,Leo Schwinn*

Main category: cs.LG

TL;DR: 提出了一种将对抗性提示优化转化为高效摊销推理任务的新框架，利用预训练的非自回归生成LLM作为提示搜索的代理，直接条件生成提示而非进行昂贵的离散优化。


<details>
  <summary>Details</summary>
Motivation: 传统对抗性提示优化资源密集且效率低下，需要为每个实例进行昂贵的离散搜索，希望通过摊销推理方法提高效率。

Method: 使用预训练的非自回归生成LLM（如Diffusion LLMs）建模提示-响应对的联合分布，通过条件生成直接产生提示，只需少量并行样本即可恢复高奖励提示。

Result: 生成的提示具有低困惑度、多样性，能够成功越狱多种黑盒目标模型，包括经过鲁棒训练和专有LLM，表现出很强的可转移性。

Conclusion: 该框架不仅为对抗性提示提供了高效解决方案，还为红队测试、自动提示优化以及新兴基于Flow和Diffusion的LLM应用开辟了新方向。

Abstract: We introduce a novel framework that transforms the resource-intensive
(adversarial) prompt optimization problem into an \emph{efficient, amortized
inference task}. Our core insight is that pretrained, non-autoregressive
generative LLMs, such as Diffusion LLMs, which model the joint distribution
over prompt-response pairs, can serve as powerful surrogates for prompt search.
This approach enables the direct conditional generation of prompts, effectively
replacing costly, per-instance discrete optimization with a small number of
parallelizable samples. We provide a probabilistic analysis demonstrating that
under mild fidelity assumptions, only a few conditional samples are required to
recover high-reward (harmful) prompts. Empirically, we find that the generated
prompts are low-perplexity, diverse jailbreaks that exhibit strong
transferability to a wide range of black-box target models, including robustly
trained and proprietary LLMs. Beyond adversarial prompting, our framework opens
new directions for red teaming, automated prompt optimization, and leveraging
emerging Flow- and Diffusion-based LLMs.

</details>


### [225] [Toward Unifying Group Fairness Evaluation from a Sparsity Perspective](https://arxiv.org/abs/2511.00359)
*Zhecheng Sheng,Jiawei Zhang,Enmao Diao*

Main category: cs.LG

TL;DR: 提出了一个基于稀疏性的统一框架来评估算法公平性，该框架与现有公平性标准对齐，并在多种数据集和偏置缓解方法上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 机器学习中确保算法公平性是一个重要挑战，现有的公平性标准往往缺乏跨不同机器学习问题的泛化能力。

Method: 研究了各种稀疏性度量在促进公平性方面的联系与差异，提出了一个基于稀疏性的统一框架来评估算法公平性。

Result: 该框架与现有公平性标准对齐，在多种数据集和偏置缓解方法上通过广泛实验证明了其有效性。

Conclusion: 通过稀疏性和社会公平的视角为算法公平性提供了新颖视角，对公平性研究和应用具有更广泛的影响潜力。

Abstract: Ensuring algorithmic fairness remains a significant challenge in machine
learning, particularly as models are increasingly applied across diverse
domains. While numerous fairness criteria exist, they often lack
generalizability across different machine learning problems. This paper
examines the connections and differences among various sparsity measures in
promoting fairness and proposes a unified sparsity-based framework for
evaluating algorithmic fairness. The framework aligns with existing fairness
criteria and demonstrates broad applicability to a wide range of machine
learning tasks. We demonstrate the effectiveness of the proposed framework as
an evaluation metric through extensive experiments on a variety of datasets and
bias mitigation methods. This work provides a novel perspective to algorithmic
fairness by framing it through the lens of sparsity and social equity, offering
potential for broader impact on fairness research and applications.

</details>


### [226] [MISA: Memory-Efficient LLMs Optimization with Module-wise Importance Sampling](https://arxiv.org/abs/2511.00056)
*Yuxi Liu,Renjia Deng,Yutong He,Xue Wang,Tao Yao,Kun Yuan*

Main category: cs.LG

TL;DR: 提出MISA方法，通过模块级重要性采样来优化大语言模型训练，相比层级优化能更有效地减少内存使用并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有层级优化方法忽略了层内模块的重要性差异，且内存节省有限，需要改进以更好地处理大语言模型的内存需求。

Method: 将每个层划分为更小的模块，为每个模块分配重要性分数，使用加权随机采样机制激活模块，降低梯度方差。

Result: 实验证明MISA在多种学习任务中有效，提供了收敛率分析和内存分析，显示其优于现有基线方法。

Conclusion: MISA是一种有效的模块级优化方法，能显著减少内存使用同时保持模型性能，为大语言模型训练提供了新思路。

Abstract: The substantial memory demands of pre-training and fine-tuning large language
models (LLMs) require memory-efficient optimization algorithms. One promising
approach is layer-wise optimization, which treats each transformer block as a
single layer and optimizes it sequentially, while freezing the other layers to
save optimizer states and activations. Although effective, these methods ignore
the varying importance of the modules within each layer, leading to suboptimal
performance. Moreover, layer-wise sampling provides only limited memory
savings, as at least one full layer must remain active during optimization. To
overcome these limitations, we propose Module-wise Importance SAmpling (MISA),
a novel method that divides each layer into smaller modules and assigns
importance scores to each module. MISA uses a weighted random sampling
mechanism to activate modules, provably reducing gradient variance compared to
layer-wise sampling. Additionally, we establish an \(\mathcal{O}(1/\sqrt{K})\)
convergence rate under non-convex and stochastic conditions, where $K$ is the
total number of block updates, and provide a detailed memory analysis
showcasing MISA's superiority over existing baseline methods. Experiments on
diverse learning tasks validate the effectiveness of MISA. Source code is
available at https://github.com/pkumelon/MISA.

</details>


### [227] [A Tight Lower Bound for Non-stochastic Multi-armed Bandits with Expert Advice](https://arxiv.org/abs/2511.00257)
*Zachary Chase,Shinji Ito,Idan Mehalel*

Main category: cs.LG

TL;DR: 本文确定了非随机多臂老虎机专家建议问题中的极小极大最优期望遗憾，通过证明与Kale (2014)上界匹配的下界，得出最优期望遗憾为Θ(√(TK log(N/K)))，其中K是臂数，N是专家数，T是时间范围。


<details>
  <summary>Details</summary>
Motivation: 解决非随机多臂老虎机专家建议问题中的最优遗憾界限问题，填补理论空白，确定该问题的精确极小极大最优性能。

Method: 通过数学证明方法，构建与已有上界匹配的下界，从而确定精确的极小极大最优期望遗憾界限。

Result: 证明了非随机多臂老虎机专家建议问题的极小极大最优期望遗憾为Θ(√(TK log(N/K)))，其中K是臂数，N是专家数，T是时间范围。

Conclusion: 该研究完整确定了非随机多臂老虎机专家建议问题的理论最优性能界限，为相关算法设计提供了理论基准。

Abstract: We determine the minimax optimal expected regret in the classic
non-stochastic multi-armed bandit with expert advice problem, by proving a
lower bound that matches the upper bound of Kale (2014). The two bounds
determine the minimax optimal expected regret to be $\Theta\left( \sqrt{T K
\log (N/K) } \right)$, where $K$ is the number of arms, $N$ is the number of
experts, and $T$ is the time horizon.

</details>


### [228] [Automatically Finding Rule-Based Neurons in OthelloGPT](https://arxiv.org/abs/2511.00059)
*Aditya Singh,Zihang Wen,Srujananjali Medicherla,Adam Karvonen,Can Rager*

Main category: cs.LG

TL;DR: 该论文提出了一种基于决策树的自动化方法，用于识别和解释OthelloGPT模型中编码游戏规则的MLP神经元，发现约一半神经元可以被紧凑的规则化决策树准确描述，并通过干预实验验证了这些模式的因果相关性。


<details>
  <summary>Details</summary>
Motivation: OthelloGPT为可解释性研究提供了理想的测试平台——模型足够复杂以展现丰富的计算模式，同时又基于规则化的游戏逻辑便于逆向工程。

Method: 使用回归决策树将棋盘状态映射到神经元激活，提取高激活决策路径并转换为人类可读的逻辑形式，通过针对性干预验证因果相关性。

Result: 发现第5层约913个神经元（共2048个）可以被紧凑的规则化决策树准确描述（R² > 0.7），干预实验显示特定模式对应的神经元被消融后，模型预测合法移动的能力下降5-10倍。

Conclusion: 该方法成功识别了模型中编码游戏规则的可解释神经元模式，并提供了将游戏行为映射到实现神经元的Python工具，为可解释性研究提供了有价值的资源。

Abstract: OthelloGPT, a transformer trained to predict valid moves in Othello, provides
an ideal testbed for interpretability research. The model is complex enough to
exhibit rich computational patterns, yet grounded in rule-based game logic that
enables meaningful reverse-engineering. We present an automated approach based
on decision trees to identify and interpret MLP neurons that encode rule-based
game logic. Our method trains regression decision trees to map board states to
neuron activations, then extracts decision paths where neurons are highly
active to convert them into human-readable logical forms. These descriptions
reveal highly interpretable patterns; for instance, neurons that specifically
detect when diagonal moves become legal. Our findings suggest that roughly half
of the neurons in layer 5 can be accurately described by compact, rule-based
decision trees ($R^2 > 0.7$ for 913 of 2,048 neurons), while the remainder
likely participate in more distributed or non-rule-based computations. We
verify the causal relevance of patterns identified by our decision trees
through targeted interventions. For a specific square, for specific game
patterns, we ablate neurons corresponding to those patterns and find an
approximately 5-10 fold stronger degradation in the model's ability to predict
legal moves along those patterns compared to control patterns. To facilitate
future work, we provide a Python tool that maps rule-based game behaviors to
their implementing neurons, serving as a resource for researchers to test
whether their interpretability methods recover meaningful computational
structures.

</details>


### [229] [A Technical Exploration of Causal Inference with Hybrid LLM Synthetic Data](https://arxiv.org/abs/2511.00318)
*Dana Kim,Yichen Xu,Tiffany Lin*

Main category: cs.LG

TL;DR: 本文提出了一种结合模型协变量合成和因果结构保持的混合框架，用于生成保留因果参数的合成表格数据，解决了现有方法在因果效应估计上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基于GAN和LLM的合成数据生成方法虽然能实现高预测保真度，但往往无法准确保持关键因果参数（如平均处理效应ATE），限制了其在因果分析中的应用。

Method: 提出混合生成框架：结合基于模型的协变量合成（通过距离过滤监控）、分别学习的倾向评分和结果模型，确保(W,A,Y)三元组保留底层因果结构；引入合成配对策略缓解正性假设违反问题。

Result: 该方法能够生成保留因果结构的合成数据，并通过利用无限合成样本的评估协议验证了传统估计器（IPTW、AIPW、替代）在复杂协变量分布下的性能。

Conclusion: 这项工作为支持稳健因果分析的LLM驱动数据管道奠定了基础，代码已开源。

Abstract: Large Language Models (LLMs) offer a flexible means to generate synthetic
tabular data, yet existing approaches often fail to preserve key causal
parameters such as the average treatment effect (ATE). In this technical
exploration, we first demonstrate that state-of-the-art synthetic data
generators, both GAN- and LLM-based, can achieve high predictive fidelity while
substantially misestimating causal effects. To address this gap, we propose a
hybrid generation framework that combines model-based covariate synthesis
(monitored via distance-to-closest-record filtering) with separately learned
propensity and outcome models, thereby ensuring that (W, A, Y) triplets retain
their underlying causal structure. We further introduce a synthetic pairing
strategy to mitigate positivity violations and a realistic evaluation protocol
that leverages unlimited synthetic samples to benchmark traditional estimators
(IPTW, AIPW, substitution) under complex covariate distributions. This work
lays the groundwork for LLM-powered data pipelines that support robust causal
analysis. Our code is available at
https://github.com/Xyc-arch/llm-synthetic-for-causal-inference.git.

</details>


### [230] [Air Pollution Forecasting in Bucharest](https://arxiv.org/abs/2511.00532)
*Dragoş-Andrei Şerban,Răzvan-Alexandru Smădu,Dumitru-Clementin Cercel*

Main category: cs.LG

TL;DR: 本文旨在设计和评估多种机器学习模型来预测PM2.5浓度水平，比较线性回归、集成方法、深度学习模型（包括RNN和Transformer）以及大语言模型在不同时间跨度上的表现。


<details>
  <summary>Details</summary>
Motivation: PM2.5空气污染对健康造成严重影响，包括呼吸系统疾病、心血管疾病、肺功能损伤甚至癌症和早逝。预测未来PM2.5水平可以提供早期预警，帮助预防相关疾病。

Method: 设计、微调、测试和评估多种机器学习模型，包括线性回归算法、集成方法、深度学习模型（先进的循环神经网络和Transformer）以及大语言模型。

Result: 论文对多种模型在PM2.5预测任务上的性能进行了评估和比较，但具体结果未在摘要中提供。

Conclusion: 通过系统比较不同机器学习模型在PM2.5预测任务上的表现，为空气污染预警提供了有效的技术方案。

Abstract: Air pollution, especially the particulate matter 2.5 (PM2.5), has become a
growing concern in recent years, primarily in urban areas. Being exposed to air
pollution is linked to developing numerous health problems, like the
aggravation of respiratory diseases, cardiovascular disorders, lung function
impairment, and even cancer or early death. Forecasting future levels of PM2.5
has become increasingly important over the past few years, as it can provide
early warnings and help prevent diseases. This paper aims to design, fine-tune,
test, and evaluate machine learning models for predicting future levels of
PM2.5 over various time horizons. Our primary objective is to assess and
compare the performance of multiple models, ranging from linear regression
algorithms and ensemble-based methods to deep learning models, such as advanced
recurrent neural networks and transformers, as well as large language models,
on this forecasting task.

</details>


### [231] [EVINGCA: Adaptive Graph Clustering with Evolving Neighborhood Statistics](https://arxiv.org/abs/2511.00064)
*Randolph Wiredu-Aidoo*

Main category: cs.LG

TL;DR: EVINGCA是一种基于密度-方差的聚类算法，将聚类形成视为最近邻图上的自适应演化过程，通过局部统计反馈替代固定密度阈值，具有对数线性复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有聚类算法存在限制性假设：K-Means和高斯混合模型假设凸的、高斯类簇，而DBSCAN和HDBSCAN能捕获非凸性但对参数高度敏感。

Method: 在最近邻图上通过广度优先搜索扩展根图，使用持续更新的局部距离和形状统计量指导聚类过程，采用空间索引技术。

Result: 在合成、真实世界、低维和高维数据集上表现出与基线方法竞争的性能。

Conclusion: EVINGCA通过局部统计反馈机制有效解决了传统聚类算法的限制，提供了一种自适应、非参数的聚类解决方案。

Abstract: Clustering algorithms often rely on restrictive assumptions: K-Means and
Gaussian Mixtures presuppose convex, Gaussian-like clusters, while DBSCAN and
HDBSCAN capture non-convexity but can be highly sensitive. I introduce EVINGCA
(Evolving Variance-Informed Nonparametric Graph Construction Algorithm), a
density-variance based clustering algorithm that treats cluster formation as an
adaptive, evolving process on a nearest-neighbor graph. EVINGCA expands rooted
graphs via breadth-first search, guided by continuously updated local distance
and shape statistics, replacing fixed density thresholds with local statistical
feedback. With spatial indexing, EVINGCA features log-linear complexity in the
average case and exhibits competitive performance against baselines across a
variety of synthetic, real-world, low-d, and high-d datasets.

</details>


### [232] [Diagnosing Hallucination Risk in AI Surgical Decision-Support: A Sequential Framework for Sequential Validation](https://arxiv.org/abs/2511.00588)
*Dong Chen,Yanzhe Wei,Zonglin He,Guan-Ming Kuang,Canhua Ye,Meiru An,Huili Peng,Yong Hu,Huiren Tao,Kenneth MC Cheung*

Main category: cs.LG

TL;DR: 该研究评估了6个大型语言模型在脊柱外科临床决策中的幻觉风险，发现DeepSeek-R1表现最佳，但推理增强模型并未优于标准版本，突显了临床应用中需要安全验证框架。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在脊柱外科临床决策支持中具有潜力，但存在幻觉风险可能危及患者安全，需要量化评估这些风险。

Method: 采用临床医生中心框架，从诊断精度、推荐质量、推理稳健性、输出一致性和知识对齐五个维度评估6个领先LLM在30个专家验证的脊柱病例上的表现。

Result: DeepSeek-R1总体表现最佳（总分86.03±2.08），在创伤和感染等高风险领域表现突出。推理增强模型Claude-3.7-Sonnet的扩展思维模式反而表现不如标准版本。多维度压力测试显示推荐质量在复杂度增加时下降7.4%。

Conclusion: 需要将可解释性机制整合到临床工作流程中，并建立安全感知的验证框架来确保外科LLM部署的安全性。

Abstract: Large language models (LLMs) offer transformative potential for clinical
decision support in spine surgery but pose significant risks through
hallucinations, which are factually inconsistent or contextually misaligned
outputs that may compromise patient safety. This study introduces a
clinician-centered framework to quantify hallucination risks by evaluating
diagnostic precision, recommendation quality, reasoning robustness, output
coherence, and knowledge alignment. We assessed six leading LLMs across 30
expert-validated spinal cases. DeepSeek-R1 demonstrated superior overall
performance (total score: 86.03 $\pm$ 2.08), particularly in high-stakes
domains such as trauma and infection. A critical finding reveals that
reasoning-enhanced model variants did not uniformly outperform standard
counterparts: Claude-3.7-Sonnet's extended thinking mode underperformed
relative to its standard version (80.79 $\pm$ 1.83 vs. 81.56 $\pm$ 1.92),
indicating extended chain-of-thought reasoning alone is insufficient for
clinical reliability. Multidimensional stress-testing exposed model-specific
vulnerabilities, with recommendation quality degrading by 7.4% under amplified
complexity. This decline contrasted with marginal improvements in rationality
(+2.0%), readability (+1.7%) and diagnosis (+4.7%), highlighting a concerning
divergence between perceived coherence and actionable guidance. Our findings
advocate integrating interpretability mechanisms (e.g., reasoning chain
visualization) into clinical workflows and establish a safety-aware validation
framework for surgical LLM deployment.

</details>


### [233] [Aligning Brain Signals with Multimodal Speech and Vision Embeddings](https://arxiv.org/abs/2511.00065)
*Kateryna Shapovalenko,Quentin Auster*

Main category: cs.LG

TL;DR: 该研究探索了预训练模型的不同层如何反映大脑对语言的分层处理过程，通过比较wav2vec2和CLIP模型的嵌入与脑电图信号的对应关系。


<details>
  <summary>Details</summary>
Motivation: 受大脑从原始声音到丰富多模态关联的分层意义构建过程启发，研究旨在了解预训练模型的哪些层最能反映大脑的这种分层处理机制。

Method: 使用自然语音感知期间记录的EEG数据，通过岭回归和对比解码评估wav2vec2（声音到语言）和CLIP（词到图像）模型的嵌入与大脑活动的对应关系，测试了三种策略：单层、渐进连接和渐进求和。

Result: 研究发现结合多模态、层感知的表征可能更接近解码大脑如何理解语言——不仅是作为声音，而是作为经验。

Conclusion: 多模态、层感知的表征组合为理解大脑语言处理机制提供了新视角，超越了单纯的声音处理层面。

Abstract: When we hear the word "house", we don't just process sound, we imagine walls,
doors, memories. The brain builds meaning through layers, moving from raw
acoustics to rich, multimodal associations. Inspired by this, we build on
recent work from Meta that aligned EEG signals with averaged wav2vec2 speech
embeddings, and ask a deeper question: which layers of pre-trained models best
reflect this layered processing in the brain? We compare embeddings from two
models: wav2vec2, which encodes sound into language, and CLIP, which maps words
to images. Using EEG recorded during natural speech perception, we evaluate how
these embeddings align with brain activity using ridge regression and
contrastive decoding. We test three strategies: individual layers, progressive
concatenation, and progressive summation. The findings suggest that combining
multimodal, layer-aware representations may bring us closer to decoding how the
brain understands language, not just as sound, but as experience.

</details>


### [234] [Why Federated Optimization Fails to Achieve Perfect Fitting? A Theoretical Perspective on Client-Side Optima](https://arxiv.org/abs/2511.00469)
*Zhongxiang Lei,Qi Yang,Ping Qiu,Gang Zhang,Yuanchi Ma,Jinyan Liu*

Main category: cs.LG

TL;DR: 本文从理论角度解释了联邦学习中数据异构性导致性能下降的原因，指出异构数据会产生不同的局部最优解，从而提高了全局目标的下界并导致模型在训练后期振荡。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习算法虽然理论上能保证收敛且实践中训练稳定，但数据异构性导致性能下降的原因尚不明确。本文旨在从理论层面解释这种性能下降现象。

Method: 引入假设：异构客户端数据会导致不同的局部最优解。基于此假设，分析两个关键后果：1) 客户端局部最优解之间的距离提高了全局目标的下界；2) 在训练后期，全局模型会在区域内振荡而非收敛到单一最优解。

Result: 理论分析表明数据异构性使得完美拟合所有客户端数据变得不可能，且模型在训练后期无法完全收敛。通过多个任务和神经网络架构的实验验证了这些理论发现。

Conclusion: 本文为联邦学习中非独立同分布数据导致的性能下降提供了理论解释，揭示了异构数据环境下模型收敛的固有局限性。

Abstract: Federated optimization is a constrained form of distributed optimization that
enables training a global model without directly sharing client data. Although
existing algorithms can guarantee convergence in theory and often achieve
stable training in practice, the reasons behind performance degradation under
data heterogeneity remain unclear. To address this gap, the main contribution
of this paper is to provide a theoretical perspective that explains why such
degradation occurs. We introduce the assumption that heterogeneous client data
lead to distinct local optima, and show that this assumption implies two key
consequences: 1) the distance among clients' local optima raises the lower
bound of the global objective, making perfect fitting of all client data
impossible; and 2) in the final training stage, the global model oscillates
within a region instead of converging to a single optimum, limiting its ability
to fully fit the data. These results provide a principled explanation for
performance degradation in non-iid settings, which we further validate through
experiments across multiple tasks and neural network architectures. The
framework used in this paper is open-sourced at:
https://github.com/NPCLEI/fedtorch.

</details>


### [235] [Token-Regulated Group Relative Policy Optimization for Stable Reinforcement Learning in Large Language Models](https://arxiv.org/abs/2511.00066)
*Tue Le,Nghi D. Q. Bui,Linh Ngo Van,Trung Le*

Main category: cs.LG

TL;DR: 提出了TR-GRPO方法，通过基于token概率的权重调节来解决GRPO中低概率token梯度主导问题，在多个推理任务中表现优于GRPO。


<details>
  <summary>Details</summary>
Motivation: GRPO方法存在低概率token梯度过大导致训练不稳定、抑制高概率token贡献的问题，需要改进梯度平衡机制。

Method: 在GRPO基础上引入token级权重调节，权重与模型预测概率正相关，降低低概率token权重，增强高概率token贡献。

Result: 在逻辑、数学和智能体推理等RLVR任务上，TR-GRPO始终优于GRPO，证明了调节token贡献的重要性。

Conclusion: TR-GRPO是一个稳健的框架，能有效增强LLM推理能力，通过平衡token梯度贡献提升训练稳定性。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has emerged as a
powerful approach for strengthening the reasoning capabilities of large
language models (LLMs). Among existing algorithms, Group Relative Policy
Optimization (GRPO) has demonstrated strong performance, yet it suffers from a
critical issue: low-probability tokens disproportionately dominate gradient
updates due to their inherently large gradient magnitudes. This imbalance leads
to unstable training and suppresses the contribution of high-probability tokens
that are more reliable for learning. In this work, we introduce Token-Regulated
Group Relative Policy Optimization (TR-GRPO), a simple yet effective extension
of GRPO that assigns token-level weights positively correlated with the model's
predicted probability. By downweighting low-probability tokens and emphasizing
high-probability ones, TR-GRPO mitigates gradient over-amplification while
preserving informative learning signals. Extensive experiments demonstrate that
TR-GRPO consistently outperforms GRPO across RLVR tasks, including logic, math,
and agentic reasoning, highlighting the importance of regulating token
contributions during RL training and establishing TR-GRPO as a robust framework
for enhancing LLM reasoning.

</details>


### [236] [Learning an Efficient Optimizer via Hybrid-Policy Sub-Trajectory Balance](https://arxiv.org/abs/2511.00543)
*Yunchuan Guan,Yu Liu,Ke Zhou,Hui Li,Sen Jia,Zhiqi Shen,Ziyang Wang,Xinglin Zhang,Tao Chen,Jenq-Neng Hwang,Lei Li*

Main category: cs.LG

TL;DR: 提出Lo-Hp框架，通过解耦的两阶段权重生成方法解决现有生成式权重优化中的过耦合和长视野问题，提高灵活性和推理效率。


<details>
  <summary>Details</summary>
Motivation: 当前基于生成模型的权重生成方法存在过耦合和长视野问题，前者限制了优化器的灵活性，后者导致推理效率低和精度不足。

Method: 采用解耦的两阶段权重生成框架，结合混合策略子轨迹平衡目标，整合在线和离线学习来捕捉局部优化策略。

Result: 在迁移学习、少样本学习、领域泛化和大语言模型适应等需要频繁权重更新的任务中，Lo-Hp展现出优越的准确性和推理效率。

Conclusion: 学习局部优化策略可以有效解决长视野问题，同时促进全局最优权重的生成，Lo-Hp框架在多种任务中表现优异。

Abstract: Recent advances in generative modeling enable neural networks to generate
weights without relying on gradient-based optimization. However, current
methods are limited by issues of over-coupling and long-horizon. The former
tightly binds weight generation with task-specific objectives, thereby limiting
the flexibility of the learned optimizer. The latter leads to inefficiency and
low accuracy during inference, caused by the lack of local constraints. In this
paper, we propose Lo-Hp, a decoupled two-stage weight generation framework that
enhances flexibility through learning various optimization policies. It adopts
a hybrid-policy sub-trajectory balance objective, which integrates on-policy
and off-policy learning to capture local optimization policies. Theoretically,
we demonstrate that learning solely local optimization policies can address the
long-horizon issue while enhancing the generation of global optimal weights. In
addition, we validate Lo-Hp's superior accuracy and inference efficiency in
tasks that require frequent weight updates, such as transfer learning, few-shot
learning, domain generalization, and large language model adaptation.

</details>


### [237] [Latent Domain Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2511.00067)
*Zhixing Li,Arsham Gholamzadeh Khoee,Yinan Yu*

Main category: cs.LG

TL;DR: 提出一种无需显式域标签的领域泛化方法，通过潜在域聚类和自适应知识迁移来提升视觉语言模型在未见目标域上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有领域泛化方法依赖可能不可用或模糊的域标签，需要研究无需显式域标签的DG设置，使模型能在真实应用中更好地应对域偏移。

Method: 在图像特征上进行潜在域聚类，根据输入图像与每个潜在域的相似度融合域特定文本特征，实现跨域知识自适应迁移。

Result: 在四个基准测试上的实验表明，该方法相比基于VLM的基线模型获得了一致的性能提升。

Conclusion: 该策略为在域偏移下提高模型鲁棒性提供了新的见解，证明了无需显式域标签的领域泛化方法的有效性。

Abstract: The objective of domain generalization (DG) is to enable models to be robust
against domain shift. DG is crucial for deploying vision-language models (VLMs)
in real-world applications, yet most existing methods rely on domain labels
that may not be available and often ambiguous. We instead study the DG setting
where models must generalize well without access to explicit domain labels. Our
key idea is to represent an unseen target domain as a combination of latent
domains automatically discovered from training data, enabling the model to
adaptively transfer knowledge across domains. To realize this, we perform
latent domain clustering on image features and fuse domain-specific text
features based on the similarity between the input image and each latent
domain. Experiments on four benchmarks show that this strategy yields
consistent gains over VLM-based baselines and provides new insights into
improving robustness under domain shift.

</details>


### [238] [Sparse and nonparametric estimation of equations governing dynamical systems with applications to biology](https://arxiv.org/abs/2511.00579)
*G. Pillonetto,A. Giaretta,A. Aravkin,M. Bisiacco,T. Elston*

Main category: cs.LG

TL;DR: 提出了一种结合稀疏参数估计和非参数技术的新框架，用于从数据中发现动态系统模型方程，特别适用于复杂生物系统建模。


<details>
  <summary>Details</summary>
Motivation: 传统参数化模型（如Sindy算法）在准确表示复杂系统中的某些非线性特性方面存在不足，需要一种能够捕获这些非线性而无需事先了解其函数形式的方法。

Method: 将稀疏参数估计与非参数技术相结合，无需扩展函数库就能捕获Sindy无法描述的非线性特性。

Result: 该方法在多个复杂生物现象估计示例中得到验证，能够有效发现系统动态方程。

Conclusion: 该框架为复杂系统建模提供了一种更有效的方法，特别适用于系统生物学中难以采用自下而上建模方法的情况。

Abstract: Data-driven discovery of model equations is a powerful approach for
understanding the behavior of dynamical systems in many scientific fields. In
particular, the ability to learn mathematical models from data would benefit
systems biology, where the complex nature of these systems often makes a bottom
up approach to modeling unfeasible. In recent years, sparse estimation
techniques have gained prominence in system identification, primarily using
parametric paradigms to efficiently capture system dynamics with minimal model
complexity. In particular, the Sindy algorithm has successfully used sparsity
to estimate nonlinear systems by extracting from a library of functions only a
few key terms needed to capture the dynamics of these systems. However,
parametric models often fall short in accurately representing certain
nonlinearities inherent in complex systems. To address this limitation, we
introduce a novel framework that integrates sparse parametric estimation with
nonparametric techniques. It captures nonlinearities that Sindy cannot describe
without requiring a priori information about their functional form. That is,
without expanding the library of functions to include the one that is trying to
be discovered. We illustrate our approach on several examples related to
estimation of complex biological phenomena.

</details>


### [239] [Benchmarking Generative AI Against Bayesian Optimization for Constrained Multi-Objective Inverse Design](https://arxiv.org/abs/2511.00070)
*Muhammad Bilal Awan,Abdul Razzaq,Abdul Shahid*

Main category: cs.LG

TL;DR: 本文研究大型语言模型作为生成优化器在约束多目标回归任务中的表现，特别是在逆设计领域。研究发现，经过微调的LLM在计算速度上具有优势，但专门的贝叶斯优化框架在收敛保证方面表现更佳。


<details>
  <summary>Details</summary>
Motivation: 探索LLM在约束、连续、高维数值空间任务中的实用性，这些任务并非LLM的原始设计目标。研究旨在为材料信息学中的逆设计问题提供新的解决方案。

Method: 进行了贝叶斯优化框架与微调LLM/BERT模型的比较研究。使用BoTorch Ax和qEHVI作为BO基准，通过参数高效微调(PEFT)方法微调LLM，将其构建为带有自定义输出头的回归问题。

Result: BoTorch qEHVI实现了完美收敛(GD=0.0)，而表现最佳的LLM(WizardMath-7B)达到了GD=1.21，显著优于传统BoTorch Ax基线(GD=15.03)。

Conclusion: 专用BO框架在保证收敛方面仍是性能领先者，但微调LLM被验证为有前景、计算快速的替代方案，为AI驱动的优化领域提供了重要的比较指标。

Abstract: This paper investigates the performance of Large Language Models (LLMs) as
generative optimizers for solving constrained multi-objective regression tasks,
specifically within the challenging domain of inverse design
(property-to-structure mapping). This problem, critical to materials
informatics, demands finding complex, feasible input vectors that lie on the
Pareto optimal front. While LLMs have demonstrated universal effectiveness
across generative and reasoning tasks, their utility in constrained,
continuous, high-dimensional numerical spaces tasks they weren't explicitly
architected for remains an open research question. We conducted a rigorous
comparative study between established Bayesian Optimization (BO) frameworks and
a suite of fine-tuned LLMs and BERT models. For BO, we benchmarked the
foundational BoTorch Ax implementation against the state-of-the-art q-Expected
Hypervolume Improvement (qEHVI, BoTorchM). The generative approach involved
fine-tuning models via Parameter-Efficient Fine-Tuning (PEFT), framing the
challenge as a regression problem with a custom output head. Our results show
that BoTorch qEHVI achieved perfect convergence (GD=0.0), setting the
performance ceiling. Crucially, the best-performing LLM (WizardMath-7B)
achieved a Generational Distance (GD) of 1.21, significantly outperforming the
traditional BoTorch Ax baseline (GD=15.03). We conclude that specialized BO
frameworks remain the performance leader for guaranteed convergence, but
fine-tuned LLMs are validated as a promising, computationally fast alternative,
contributing essential comparative metrics to the field of AI-driven
optimization. The findings have direct industrial applications in optimizing
formulation design for resins, polymers, and paints, where multi-objective
trade-offs between mechanical, rheological, and chemical properties are
critical to innovation and production efficiency.

</details>


### [240] [Belief Dynamics Reveal the Dual Nature of In-Context Learning and Activation Steering](https://arxiv.org/abs/2511.00617)
*Eric Bigelow,Daniel Wurgaft,YingQiao Wang,Noah Goodman,Tomer Ullman,Hidenori Tanaka,Ekdeep Singh Lubana*

Main category: cs.LG

TL;DR: 提出了一个统一的贝叶斯框架来解释LLM的控制方法，将上下文学习和激活引导视为改变模型对潜在概念信念的不同方式。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM控制方法（上下文学习和激活引导）看似不同但目标一致，需要建立一个统一的理论框架来解释这些方法的共同机制。

Method: 从贝叶斯角度建立预测性模型，认为上下文干预通过证据积累影响信念，而激活引导通过改变概念先验来影响模型行为。

Result: 该模型能准确预测LLM在各种干预下的行为，解释已有的经验现象（如S型学习曲线），并预测新现象（如对数信念空间的干预可加性）。

Conclusion: 这项工作为基于提示和基于激活的LLM行为控制提供了统一解释，并提供了预测这些干预效果的经验方法。

Abstract: Large language models (LLMs) can be controlled at inference time through
prompts (in-context learning) and internal activations (activation steering).
Different accounts have been proposed to explain these methods, yet their
common goal of controlling model behavior raises the question of whether these
seemingly disparate methodologies can be seen as specific instances of a
broader framework. Motivated by this, we develop a unifying, predictive account
of LLM control from a Bayesian perspective. Specifically, we posit that both
context- and activation-based interventions impact model behavior by altering
its belief in latent concepts: steering operates by changing concept priors,
while in-context learning leads to an accumulation of evidence. This results in
a closed-form Bayesian model that is highly predictive of LLM behavior across
context- and activation-based interventions in a set of domains inspired by
prior work on many-shot in-context learning. This model helps us explain prior
empirical phenomena - e.g., sigmoidal learning curves as in-context evidence
accumulates - while predicting novel ones - e.g., additivity of both
interventions in log-belief space, which results in distinct phases such that
sudden and dramatic behavioral shifts can be induced by slightly changing
intervention controls. Taken together, this work offers a unified account of
prompt-based and activation-based control of LLM behavior, and a methodology
for empirically predicting the effects of these interventions.

</details>


### [241] [Wavelet-Based Feature Extraction and Unsupervised Clustering for Parity Detection: A Feature Engineering Perspective](https://arxiv.org/abs/2511.00071)
*Ertugrul Mutlu*

Main category: cs.LG

TL;DR: 使用小波特征提取和无监督聚类的方法来检测数字的奇偶性，无需标签监督即可达到约69.67%的分类准确率


<details>
  <summary>Details</summary>
Motivation: 探索将经典信号处理技术应用于纯离散符号领域，揭示奇偶数之间的潜在结构差异，为符号推理和基于特征的学习搭建桥梁

Method: 将整数转换为小波域表示，提取多尺度统计特征，然后使用k-means算法进行聚类分析

Result: 在无标签监督的情况下，奇偶分类准确率达到约69.67%，特征空间揭示了奇偶数之间的有意义结构差异

Conclusion: 经典信号处理技术可以揭示纯离散符号域中的潜在结构，为非常规机器学习问题提供了特征工程和聚类的新视角

Abstract: This paper explores a deliberately over-engineered approach to the classical
problem of parity detection -- determining whether a number is odd or even --
by combining wavelet-based feature extraction with unsupervised clustering.
Instead of relying on modular arithmetic, integers are transformed into
wavelet-domain representations, from which multi-scale statistical features are
extracted and clustered using the k-means algorithm. The resulting feature
space reveals meaningful structural differences between odd and even numbers,
achieving a classification accuracy of approximately 69.67% without any label
supervision. These results suggest that classical signal-processing techniques,
originally designed for continuous data, can uncover latent structure even in
purely discrete symbolic domains. Beyond parity detection, the study provides
an illustrative perspective on how feature engineering and clustering may be
repurposed for unconventional machine learning problems, potentially bridging
symbolic reasoning and feature-based learning.

</details>


### [242] [Stochastic Shortest Path with Sparse Adversarial Costs](https://arxiv.org/abs/2511.00637)
*Emmeran Johnson,Alberto Rumi,Ciara Pike-Burke,Patrick Rebeschini*

Main category: cs.LG

TL;DR: 本文研究了具有稀疏成本的对抗性随机最短路径问题，提出了ℓ_r-范数正则化器来适应稀疏性，在已知转移设置下实现了√log M的遗憾界，优于传统的√log SA界。


<details>
  <summary>Details</summary>
Motivation: 现有基于负熵正则化的在线镜像下降方法在已知转移设置下的遗憾界为√log SA，这在最坏情况下是最优的，但未能充分利用稀疏性带来的好处，当只有少数状态-动作对产生成本时表现不佳。

Method: 提出了一族ℓ_r-范数正则化器（r∈(1,2)），这些正则化器能够适应问题的稀疏性，在已知转移设置下使用在线镜像下降方法。

Result: 在已知转移设置下，新方法实现了√log M的遗憾界，其中M是产生成本的状态-动作对数量，远小于SA。这通过匹配下界证明是最优的。在未知转移设置下，稀疏性的好处有限，任何学习者的极小极大遗憾都与SA呈多项式关系。

Conclusion: ℓ_r-范数正则化器能够有效适应稀疏成本结构，在已知转移设置下显著改善遗憾界，但稀疏性在未知转移设置下的优势有限。

Abstract: We study the adversarial Stochastic Shortest Path (SSP) problem with sparse
costs under full-information feedback. In the known transition setting,
existing bounds based on Online Mirror Descent (OMD) with negative-entropy
regularization scale with $\sqrt{\log S A}$, where $SA$ is the size of the
state-action space. While we show that this is optimal in the worst-case, this
bound fails to capture the benefits of sparsity when only a small number $M \ll
SA$ of state-action pairs incur cost. In fact, we also show that the
negative-entropy is inherently non-adaptive to sparsity: it provably incurs
regret scaling with $\sqrt{\log S}$ on sparse problems. Instead, we propose a
family of $\ell_r$-norm regularizers ($r \in (1,2)$) that adapts to the
sparsity and achieves regret scaling with $\sqrt{\log M}$ instead of
$\sqrt{\log SA}$. We show this is optimal via a matching lower bound,
highlighting that $M$ captures the effective dimension of the problem instead
of $SA$. Finally, in the unknown transition setting the benefits of sparsity
are limited: we prove that even on sparse problems, the minimax regret for any
learner scales polynomially with $SA$.

</details>


### [243] [Bridging Vision, Language, and Mathematics: Pictographic Character Reconstruction with Bézier Curves](https://arxiv.org/abs/2511.00076)
*Zihao Wan,Pau Tong Lin Xu,Fuwen Luo,Ziyue Wang,Peng Li,Yang Liu*

Main category: cs.LG

TL;DR: 该研究探索视觉语言模型对视觉几何结构的理解能力，通过将象形文字识别构建为程序合成任务，训练VLM将栅格图像反编译为由贝塞尔曲线组成的几何程序。


<details>
  <summary>Details</summary>
Motivation: 虽然视觉语言模型展现了强大的语义能力，但其理解视觉信息底层几何结构的能力尚未充分探索。象形文字结合了视觉形式和符号结构，是测试这种能力的理想案例。

Method: 在数学领域构建视觉识别挑战，将每个字符表示为可执行的几何基元程序。训练VLM作为"视觉反编译器"，将栅格图像反编译为由贝塞尔曲线组成的程序。

Result: 模型性能优于包括GPT-4o在内的强零样本基线。关键发现是仅在现代汉字上训练的模型能够零样本重建古代甲骨文，表明模型获得了抽象且可迁移的几何语法。

Conclusion: 该研究证明视觉语言模型能够超越像素级模式识别，获得结构化的视觉理解能力，掌握可迁移的抽象几何语法。

Abstract: While Vision-language Models (VLMs) have demonstrated strong semantic
capabilities, their ability to interpret the underlying geometric structure of
visual information is less explored. Pictographic characters, which combine
visual form with symbolic structure, provide an ideal test case for this
capability. We formulate this visual recognition challenge in the mathematical
domain, where each character is represented by an executable program of
geometric primitives. This is framed as a program synthesis task, training a
VLM to decompile raster images into programs composed of B\'ezier curves. Our
model, acting as a "visual decompiler", demonstrates performance superior to
strong zero-shot baselines, including GPT-4o. The most significant finding is
that when trained solely on modern Chinese characters, the model is able to
reconstruct ancient Oracle Bone Script in a zero-shot context. This
generalization provides strong evidence that the model acquires an abstract and
transferable geometric grammar, moving beyond pixel-level pattern recognition
to a more structured form of visual understanding.

</details>


### [244] [Diluting Restricted Boltzmann Machines](https://arxiv.org/abs/2511.00648)
*C. Díaz-Faloh,R. Mulet*

Main category: cs.LG

TL;DR: 研究表明，即使剪枝80%的连接，受限玻尔兹曼机仍能保持高质量生成性能，但训练后进一步剪枝会显著降低性能，且重新训练无法完全恢复。


<details>
  <summary>Details</summary>
Motivation: 随着神经网络规模不断扩大，计算和环境成本日益增加，研究是否可以通过简化、稀疏的网络保持强大性能。

Method: 基于彩票假设，研究受限玻尔兹曼机在极端剪枝条件下的表现，包括训练前剪枝和训练后剪枝的对比实验。

Result: 训练前剪枝80%仍能保持高质量生成性能，但训练后剪枝会导致性能急剧下降，重新训练效果不如从头训练。

Conclusion: 稀疏网络要有效工作，剪枝应在训练早期实施，初始条件对网络能力有持久影响。

Abstract: Recent advances in artificial intelligence have relied heavily on
increasingly large neural networks, raising concerns about their computational
and environmental costs. This paper investigates whether simpler, sparser
networks can maintain strong performance by studying Restricted Boltzmann
Machines (RBMs) under extreme pruning conditions. Inspired by the Lottery
Ticket Hypothesis, we demonstrate that RBMs can achieve high-quality generative
performance even when up to 80% of the connections are pruned before training,
confirming that they contain viable sub-networks. However, our experiments
reveal crucial limitations: trained networks cannot fully recover lost
performance through retraining once additional pruning is applied. We identify
a sharp transition above which the generative quality degrades abruptly when
pruning disrupts a minimal core of essential connections. Moreover, re-trained
networks remain constrained by the parameters originally learned performing
worse than networks trained from scratch at equivalent sparsity levels. These
results suggest that for sparse networks to work effectively, pruning should be
implemented early in training rather than attempted afterwards. Our findings
provide practical insights for the development of efficient neural
architectures and highlight the persistent influence of initial conditions on
network capabilities.

</details>


### [245] [Investigating the Robustness of Knowledge Tracing Models in the Presence of Student Concept Drift](https://arxiv.org/abs/2511.00704)
*Morgan Lee,Artem Frenk,Eamon Worden,Karish Gupta,Thinh Pham,Ethan Croteau,Neil Heffernan*

Main category: cs.LG

TL;DR: 研究知识追踪模型在在线学习平台中面对概念漂移和学生群体变化时的稳定性，发现所有模型都会出现性能下降，其中贝叶斯知识追踪模型表现最稳定。


<details>
  <summary>Details</summary>
Motivation: 传统知识追踪模型假设学习过程是静态的，但现实中在线学习平台的学生群体和概念都在不断变化，需要评估模型对这种变化的适应能力。

Method: 使用四种经典知识追踪模型在五个学年的数据上进行测试，比较模型在单学年内和跨学年的性能表现。

Result: 所有知识追踪模型都会出现性能下降，贝叶斯知识追踪模型最稳定，而基于注意力的复杂模型性能下降最快。

Conclusion: 知识追踪模型对概念漂移很敏感，需要在更长时间尺度上进行评估，贝叶斯知识追踪模型在变化环境中表现最稳健。

Abstract: Knowledge Tracing (KT) has been an established problem in the educational
data mining field for decades, and it is commonly assumed that the underlying
learning process be- ing modeled remains static. Given the ever-changing land-
scape of online learning platforms (OLPs), we investigate how concept drift and
changing student populations can im- pact student behavior within an OLP
through testing model performance both within a single academic year and across
multiple academic years. Four well-studied KT models were applied to five
academic years of data to assess how suscep- tible KT models are to concept
drift. Through our analysis, we find that all four families of KT models can
exhibit de- graded performance, Bayesian Knowledge Tracing (BKT) remains the
most stable KT model when applied to newer data, while more complex, attention
based models lose pre- dictive power significantly faster. To foster more
longitu- dinal evaluations of KT models, the data used to conduct our analysis
is available at https://osf.io/hvfn9/?view_
only=b936c63dfdae4b0b987a2f0d4038f72a

</details>


### [246] [Fixed-point graph convolutional networks against adversarial attacks](https://arxiv.org/abs/2511.00083)
*Shakib Khan,A. Ben Hamza,Amr Youssef*

Main category: cs.LG

TL;DR: 提出Fix-GCN模型，通过固定点迭代图卷积网络增强图神经网络对抗攻击的鲁棒性，使用谱调制滤波器选择性衰减高频噪声并保留低频结构信息。


<details>
  <summary>Details</summary>
Motivation: 对抗攻击严重威胁图神经网络的完整性和性能，特别是在图结构和节点特征易受操纵的任务中，需要开发鲁棒的防御机制。

Method: 引入通用谱调制滤波器，通过固定点迭代推导特征传播规则，在不增加内存或计算复杂度的前提下有效捕获高阶邻域信息。

Result: 在多个基准图数据集上的实验表明，该模型能有效抵抗对抗攻击，保持优异的性能表现。

Conclusion: Fix-GCN提供了一个灵活高效的框架，能够在保留关键图信息的同时减轻对抗操纵的影响，为图神经网络安全提供有力保障。

Abstract: Adversarial attacks present a significant risk to the integrity and
performance of graph neural networks, particularly in tasks where graph
structure and node features are vulnerable to manipulation. In this paper, we
present a novel model, called fixed-point iterative graph convolutional network
(Fix-GCN), which achieves robustness against adversarial perturbations by
effectively capturing higher-order node neighborhood information in the graph
without additional memory or computational complexity. Specifically, we
introduce a versatile spectral modulation filter and derive the feature
propagation rule of our model using fixed-point iteration. Unlike traditional
defense mechanisms that rely on additional design elements to counteract
attacks, the proposed graph filter provides a flexible-pass filtering approach,
allowing it to selectively attenuate high-frequency components while preserving
low-frequency structural information in the graph signal. By iteratively
updating node representations, our model offers a flexible and efficient
framework for preserving essential graph information while mitigating the
impact of adversarial manipulation. We demonstrate the effectiveness of the
proposed model through extensive experiments on various benchmark graph
datasets, showcasing its resilience against adversarial attacks.

</details>


### [247] [Application of predictive machine learning in pen & paper RPG game design](https://arxiv.org/abs/2511.00084)
*Jolanta Śliwa*

Main category: cs.LG

TL;DR: 本文研究了使用序数回归技术预测笔纸RPG中怪物等级的自动化方法，以替代当前耗时的手动评估方式。


<details>
  <summary>Details</summary>
Motivation: 笔纸RPG市场快速增长，公司寻求AI技术提升玩家体验。当前怪物等级设计依赖手动测试和专家评估，既耗时又资源密集，需要自动化解决方案。

Method: 构建了专门的等级评估数据集，开发了人类启发模型作为基准，设计了基于领域知识的专门评估程序来比较机器学习算法与传统方法。

Result: 对最先进的序数回归方法进行了全面概述和评估，建立了可用于等级预测的基准模型和评估框架。

Conclusion: 通过构建专用数据集和评估框架，为笔纸RPG中的怪物等级预测提供了可行的自动化解决方案，能够有效替代传统手动方法。

Abstract: In recent years, the pen and paper RPG market has experienced significant
growth. As a result, companies are increasingly exploring the integration of AI
technologies to enhance player experience and gain a competitive edge.
  One of the key challenges faced by publishers is designing new opponents and
estimating their challenge level. Currently, there are no automated methods for
determining a monster's level; the only approaches used are based on manual
testing and expert evaluation. Although these manual methods can provide
reasonably accurate estimates, they are time-consuming and resource-intensive.
  Level prediction can be approached using ordinal regression techniques. This
thesis presents an overview and evaluation of state-of-the-art methods for this
task. It also details the construction of a dedicated dataset for level
estimation. Furthermore, a human-inspired model was developed to serve as a
benchmark, allowing comparison between machine learning algorithms and the
approach typically employed by pen and paper RPG publishers. In addition, a
specialized evaluation procedure, grounded in domain knowledge, was designed to
assess model performance and facilitate meaningful comparisons.

</details>


### [248] [Random Spiking Neural Networks are Stable and Spectrally Simple](https://arxiv.org/abs/2511.00904)
*Ernesto Araya,Massimiliano Datres,Gitta Kutyniok*

Main category: cs.LG

TL;DR: 本文通过布尔函数分析研究LIF-SNN的稳定性和鲁棒性，发现宽LIF-SNN分类器在平均意义上稳定，且随机LIF-SNN偏向简单函数。


<details>
  <summary>Details</summary>
Motivation: SNN在能效计算方面有潜力，但其稳定性和鲁棒性的理论基础相比人工神经网络仍有限，需要深入研究。

Method: 采用布尔函数分析框架，研究离散时间LIF-SNN的噪声敏感性和稳定性，引入谱简单性概念分析傅里叶频谱集中特性。

Result: 宽LIF-SNN分类器在平均意义上稳定，随机LIF-SNN偏向简单函数，实验验证这些稳定性特性在实践中持续存在。

Conclusion: 研究结果为SNN的稳定性和鲁棒性提供了新的理论见解，揭示了SNN的内在稳定性机制。

Abstract: Spiking neural networks (SNNs) are a promising paradigm for energy-efficient
computation, yet their theoretical foundations-especially regarding stability
and robustness-remain limited compared to artificial neural networks. In this
work, we study discrete-time leaky integrate-and-fire (LIF) SNNs through the
lens of Boolean function analysis. We focus on noise sensitivity and stability
in classification tasks, quantifying how input perturbations affect outputs.
Our main result shows that wide LIF-SNN classifiers are stable on average, a
property explained by the concentration of their Fourier spectrum on
low-frequency components. Motivated by this, we introduce the notion of
spectral simplicity, which formalizes simplicity in terms of Fourier spectrum
concentration and connects our analysis to the simplicity bias observed in deep
networks. Within this framework, we show that random LIF-SNNs are biased toward
simple functions. Experiments on trained networks confirm that these stability
properties persist in practice. Together, these results provide new insights
into the stability and robustness properties of SNNs.

</details>


### [249] [MaGNet: A Mamba Dual-Hypergraph Network for Stock Prediction via Temporal-Causal and Global Relational Learning](https://arxiv.org/abs/2511.00085)
*Peilin Tan,Chuanqi Shi,Dian Tu,Liang Xie*

Main category: cs.LG

TL;DR: 提出了MaGNet模型，一种基于Mamba双超图网络的股票预测方法，通过三个关键创新：MAGE块、2D时空注意力模块和双超图框架，有效捕捉时间依赖性和股票间动态交互，在六个主要股票指数上表现出优越的预测性能和投资回报。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效捕捉时间依赖性和动态股票间交互，往往忽视横截面市场影响、依赖静态相关性、对节点和边采用统一处理方式，并混淆了不同的关系类型。

Method: MaGNet模型包含三个核心组件：(1) MAGE块：利用双向Mamba和自适应门控机制进行上下文时间建模，集成稀疏专家混合层以适应不同市场条件；(2) 2D时空注意力模块：精确融合多变量特征和跨股票依赖关系；(3) 双超图框架：包括时间因果超图捕捉细粒度因果依赖，和全局概率超图建模市场范围模式。

Result: 在六个主要股票指数上的广泛实验表明，MaGNet在预测性能和投资回报方面均优于现有最先进方法，并具有稳健的风险管理能力。

Conclusion: MaGNet通过创新的Mamba双超图网络架构，成功解决了股票趋势预测中的关键挑战，在多个维度上超越了现有方法，为股票预测提供了有效的解决方案。

Abstract: Stock trend prediction is crucial for profitable trading strategies and
portfolio management yet remains challenging due to market volatility, complex
temporal dynamics and multifaceted inter-stock relationships. Existing methods
struggle to effectively capture temporal dependencies and dynamic inter-stock
interactions, often neglecting cross-sectional market influences, relying on
static correlations, employing uniform treatments of nodes and edges, and
conflating diverse relationships. This work introduces MaGNet, a novel Mamba
dual-hyperGraph Network for stock prediction, integrating three key
innovations: (1) a MAGE block, which leverages bidirectional Mamba with
adaptive gating mechanisms for contextual temporal modeling and integrates a
sparse Mixture-of-Experts layer to enable dynamic adaptation to diverse market
conditions, alongside multi-head attention for capturing global dependencies;
(2) Feature-wise and Stock-wise 2D Spatiotemporal Attention modules enable
precise fusion of multivariate features and cross-stock dependencies,
effectively enhancing informativeness while preserving intrinsic data
structures, bridging temporal modeling with relational reasoning; and (3) a
dual hypergraph framework consisting of the Temporal-Causal Hypergraph (TCH)
that captures fine-grained causal dependencies with temporal constraints, and
Global Probabilistic Hypergraph (GPH) that models market-wide patterns through
soft hyperedge assignments and Jensen-Shannon Divergence weighting mechanism,
jointly disentangling localized temporal influences from instantaneous global
structures for multi-scale relational learning. Extensive experiments on six
major stock indices demonstrate MaGNet outperforms state-of-the-art methods in
both superior predictive performance and exceptional investment returns with
robust risk management capabilities. Codes available at:
https://github.com/PeilinTime/MaGNet.

</details>


### [250] [The Hidden Power of Normalization: Exponential Capacity Control in Deep Neural Networks](https://arxiv.org/abs/2511.00958)
*Khoat Than*

Main category: cs.LG

TL;DR: 该论文从容量控制角度解释了归一化方法在深度神经网络中的作用机制，证明了归一化层能指数级降低网络的Lipschitz常数，从而平滑损失函数和约束网络容量。


<details>
  <summary>Details</summary>
Motivation: 归一化方法在实践中能稳定优化过程和提升泛化能力，但其理论机制，特别是在多层归一化网络中的作用，尚未得到充分解释。

Method: 建立理论框架，通过分析未归一化和归一化网络的Lipschitz常数变化，证明归一化层能指数级降低网络对参数和输入的敏感性。

Result: 未归一化网络可能具有指数级大的Lipschitz常数，导致过度拟合；而插入归一化层能以指数速率降低Lipschitz常数，平滑损失函数并约束网络容量。

Conclusion: 归一化方法通过指数级降低Lipschitz常数，同时改善优化稳定性和泛化能力，为深度学习中归一化方法的成功提供了理论解释。

Abstract: Normalization methods are fundamental components of modern deep neural
networks (DNNs). Empirically, they are known to stabilize optimization dynamics
and improve generalization. However, the underlying theoretical mechanism by
which normalization contributes to both optimization and generalization remains
largely unexplained, especially when using many normalization layers in a DNN
architecture.
  In this work, we develop a theoretical framework that elucidates the role of
normalization through the lens of capacity control. We prove that an
unnormalized DNN can exhibit exponentially large Lipschitz constants with
respect to either its parameters or inputs, implying excessive functional
capacity and potential overfitting. Such bad DNNs are uncountably many. In
contrast, the insertion of normalization layers provably can reduce the
Lipschitz constant at an exponential rate in the number of normalization
operations. This exponential reduction yields two fundamental consequences: (1)
it smooths the loss landscape at an exponential rate, facilitating faster and
more stable optimization; and (2) it constrains the effective capacity of the
network, thereby enhancing generalization guarantees on unseen data. Our
results thus offer a principled explanation for the empirical success of
normalization methods in deep learning.

</details>


### [251] [Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph](https://arxiv.org/abs/2511.00086)
*Fali Wang,Jihai Chen,Shuhua Yang,Runxue Bao,Tianxiang Zhao,Zhiwei Zhang,Xianfeng Tang,Hui Liu,Qi He,Suhang Wang*

Main category: cs.LG

TL;DR: 提出了Agent-REINFORCE框架，通过LLM代理增强的搜索方法，在固定计算预算下寻找测试时扩展中的最优多LLM协作图结构。


<details>
  <summary>Details</summary>
Motivation: 现有测试时扩展方法通常假设固定的协作架构和单一模型使用，忽视了不同任务可能需要不同的最优架构和模型组合。

Method: 将问题形式化为概率图优化，提出Agent-REINFORCE框架，通过采样-反馈-更新流程，使用文本反馈作为梯度来更新概率图。

Result: 实验表明Agent-REINFORCE在样本效率和搜索性能上优于传统和基于LLM的基线方法，能有效识别在准确性和推理延迟联合目标下的最优图。

Conclusion: 该方法成功解决了测试时扩展中多LLM协作图搜索的挑战，为不同任务找到计算最优的模型组合和架构。

Abstract: Test-Time Scaling (TTS) improves large language models (LLMs) by allocating
additional computation during inference, typically through parallel,
sequential, or hybrid scaling. However, prior studies often assume fixed
collaboration architectures (e.g., topologies) and single-model usage,
overlooking that optimal architectures and model combinations can vary across
tasks. Therefore, we study the novel problem of searching for compute-optimal
model combinations and architectures in TTS under a fixed budget. We formalize
it as a multi-LLM collaboration graph, where nodes encode roles and LLM model
assignments, and edges capture information flow. This problem is challenging
because (i) the combinatorial search space is prohibitively large, and (ii)
task-specific requirements demand tailored designs. To address these, we
reformulate the problem as probabilistic graph optimization and, through pilot
experiments, derive three empirical insights into TTS collaboration graphs.
Guided by these insights, we propose Agent-REINFORCE, an LLM-agent-augmented
framework that mirrors the REINFORCE pipeline by mapping
sampling-gradient-update to sampling-feedback-update, where feedback serves as
a textual gradient to update the probabilistic graph and efficiently search for
optimal multi-LLM collaboration graphs. Experiments show that Agent-REINFORCE
outperforms both traditional and LLM-based baselines in sample efficiency and
search performance, and effectively identifies optimal graphs under joint
objectives of accuracy and inference latency.

</details>


### [252] [Happiness as a Measure of Fairness](https://arxiv.org/abs/2511.01069)
*Georg Pichler,Marco Romanelli,Pablo Piantanida*

Main category: cs.LG

TL;DR: 提出基于幸福度概念的新公平性框架，通过群体从决策结果中获得的效用来衡量公平性，提供更人性化且数学严谨的方法。


<details>
  <summary>Details</summary>
Motivation: 现有公平性定义缺乏直观的人类中心视角，需要一种既能体现人类价值又具有数学严谨性的公平性衡量方法。

Method: 使用线性规划计算最优的公平后处理策略，该方法高效且可扩展，能够统一和扩展多种已知的公平性定义。

Result: 经验结果表明该方法在多样化场景中具有实际优势，能够有效处理公平性问题。

Conclusion: 提出的幸福度公平框架为公平性研究提供了新的视角，既直观又数学严谨，在实践中表现出良好的性能。

Abstract: In this paper, we propose a novel fairness framework grounded in the concept
of happi- ness, a measure of the utility each group gains fromdecisionoutcomes.
Bycapturingfairness through this intuitive lens, we not only offer a more
human-centered approach, but also one that is mathematically rigorous: In order
to compute the optimal, fair post-processing strategy, only a linear program
needs to be solved. This makes our method both efficient and scalable with
existing optimization tools. Furthermore, it unifies and extends several
well-known fairness definitions, and our em- pirical results highlight its
practical strengths across diverse scenarios.

</details>


### [253] [GraphKeeper: Graph Domain-Incremental Learning via Knowledge Disentanglement and Preservation](https://arxiv.org/abs/2511.00097)
*Zihao Guo,Qingyun Sun,Ziwei Zhang,Haonan Yuan,Huiping Zhuang,Xingcheng Fu,Jianxin Li*

Main category: cs.LG

TL;DR: 提出了GraphKeeper方法来解决图域增量学习中的灾难性遗忘问题，通过知识解缠和保存来应对嵌入漂移和决策边界偏差。


<details>
  <summary>Details</summary>
Motivation: 现有图增量学习方法主要关注单域内的任务增量和类别增量场景，而图域增量学习在多个图域间更新模型的需求随着图基础模型的发展变得至关重要，但尚未被探索。

Method: GraphKeeper采用领域特定的参数高效微调、域内和域间解缠目标来防止嵌入漂移，引入无偏差知识保存来维持稳定决策边界，并对不可观测域进行领域感知分布判别以获得精确嵌入。

Result: 大量实验表明GraphKeeper取得了最先进的结果，相比第二名有6.5%~16.6%的提升，且遗忘可忽略不计。该方法可与多种代表性图基础模型无缝集成。

Conclusion: GraphKeeper成功解决了图域增量学习中的关键挑战，展示了广泛的适用潜力。

Abstract: Graph incremental learning (GIL), which continuously updates graph models by
sequential knowledge acquisition, has garnered significant interest recently.
However, existing GIL approaches focus on task-incremental and
class-incremental scenarios within a single domain. Graph domain-incremental
learning (Domain-IL), aiming at updating models across multiple graph domains,
has become critical with the development of graph foundation models (GFMs), but
remains unexplored in the literature. In this paper, we propose Graph
Domain-Incremental Learning via Knowledge Dientanglement and Preservation
(GraphKeeper), to address catastrophic forgetting in Domain-IL scenario from
the perspectives of embedding shifts and decision boundary deviations.
Specifically, to prevent embedding shifts and confusion across incremental
graph domains, we first propose the domain-specific parameter-efficient
fine-tuning together with intra- and inter-domain disentanglement objectives.
Consequently, to maintain a stable decision boundary, we introduce
deviation-free knowledge preservation to continuously fit incremental domains.
Additionally, for graphs with unobservable domains, we perform domain-aware
distribution discrimination to obtain precise embeddings. Extensive experiments
demonstrate the proposed GraphKeeper achieves state-of-the-art results with
6.5%~16.6% improvement over the runner-up with negligible forgetting. Moreover,
we show GraphKeeper can be seamlessly integrated with various representative
GFMs, highlighting its broad applicative potential.

</details>


### [254] [Regularization Implies balancedness in the deep linear network](https://arxiv.org/abs/2511.01137)
*Kathryn Lindsey,Govind Menon*

Main category: cs.LG

TL;DR: 使用几何不变量理论(GIT)研究深度线性网络(DLN)，通过Kempf-Ness定理建立L2正则化器在平衡流形上的最小化，将训练动态分解为纤维上的正则化流和平衡流形上的学习流。


<details>
  <summary>Details</summary>
Motivation: 为深度学习和线性系统理论中的平衡性提供一个共同的数学框架，从模型简化和贝叶斯原理的角度解释平衡性。

Method: 应用几何不变量理论和Kempf-Ness定理，将训练动态分解为两个梯度流：纤维上的正则化流（使用矩映射精确求解）和平衡流形上的学习流。

Result: 建立了L2正则化器在平衡流形上最小化的理论框架，实现了训练动态的精确分解和求解。

Conclusion: 该方法为深度学习和线性系统理论中的平衡性概念提供了统一的数学解释框架，连接了模型简化和贝叶斯原理。

Abstract: We use geometric invariant theory (GIT) to study the deep linear network
(DLN). The Kempf-Ness theorem is used to establish that the $L^2$ regularizer
is minimized on the balanced manifold. This allows us to decompose the training
dynamics into two distinct gradient flows: a regularizing flow on fibers and a
learning flow on the balanced manifold. We show that the regularizing flow is
exactly solvable using the moment map.
  This approach provides a common mathematical framework for balancedness in
deep learning and linear systems theory. We use this framework to interpret
balancedness in terms of model reduction and Bayesian principles.

</details>


### [255] [A generative adversarial network optimization method for damage detection and digital twinning by deep AI fault learning: Z24 Bridge structural health monitoring benchmark validation](https://arxiv.org/abs/2511.00099)
*Marios Impraimakis,Evangelia Nektaria Palkanoglou*

Main category: cs.LG

TL;DR: 提出了一种基于条件标签生成对抗网络的损伤检测和数字孪生方法，无需系统健康状态的先验信息，在Z24桥梁基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前基于AI的数字孪生方法在测量数据少、物理知识缺失或损伤状态未知时预测效果差，需要一种无监督框架来解决这些不确定性。

Method: 使用条件标签生成对抗网络，通过将相同损伤级别的测量数据作为输入，强制模型收敛到不同损伤状态，比较收敛分数来识别不同损伤状态。

Result: 该方法能够准确捕获健康测量数据中的损伤，为基于振动的系统级监测和可扩展基础设施韧性提供了强大工具。

Conclusion: 该无监督框架在损伤检测和数字孪生方面优于现有方法，特别适用于实际应用中系统健康状态未知的情况。

Abstract: The optimization-based damage detection and damage state digital twinning
capabilities are examined here of a novel conditional-labeled generative
adversarial network methodology. The framework outperforms current approaches
for fault anomaly detection as no prior information is required for the health
state of the system: a topic of high significance for real-world applications.
Specifically, current artificial intelligence-based digital twinning approaches
suffer from the uncertainty related to obtaining poor predictions when a low
number of measurements is available, physics knowledge is missing, or when the
damage state is unknown. To this end, an unsupervised framework is examined and
validated rigorously on the benchmark structural health monitoring measurements
of Z24 Bridge: a post-tensioned concrete highway bridge in Switzerland. In
implementing the approach, firstly, different same damage-level measurements
are used as inputs, while the model is forced to converge conditionally to two
different damage states. Secondly, the process is repeated for a different
group of measurements. Finally, the convergence scores are compared to identify
which one belongs to a different damage state. The process for both
healthy-to-healthy and damage-to-healthy input data creates, simultaneously,
measurements for digital twinning purposes at different damage states, capable
of pattern recognition and machine learning data generation. Further to this
process, a support vector machine classifier and a principal component analysis
procedure is developed to assess the generated and real measurements of each
damage category, serving as a secondary new dynamics learning indicator in
damage scenarios. Importantly, the approach is shown to capture accurately
damage over healthy measurements, providing a powerful tool for vibration-based
system-level monitoring and scalable infrastructure resilience.

</details>


### [256] [Analyzing the Power of Chain of Thought through Memorization Capabilities](https://arxiv.org/abs/2511.01190)
*Lijia Yu,Xiao-Shan Gao,Lijun Zhang*

Main category: cs.LG

TL;DR: 本文研究了思维链（CoT）是否在所有推理任务中都能增强Transformer的能力。通过分析Transformer的记忆能力，发现CoT并不总是能提升推理能力，存在某些推理任务中CoT无法增强Transformer性能的情况。


<details>
  <summary>Details</summary>
Motivation: 探索思维链（CoT）是否在所有推理任务中都能增强Transformer的能力，填补现有研究中对CoT能力边界理解的空白。

Method: 将Transformer的推理能力建模为记忆问题，分析固定精度Transformer在有/无CoT情况下的记忆能力，给出必要和充分条件，并推导参数数量的上下界。

Result: 发现CoT和非CoT Transformer的记忆条件互不包含，参数数量下界均为Θ(N)，表明存在CoT无法增强Transformer推理能力的任务。

Conclusion: CoT并不能在所有推理任务中增强Transformer的能力，在某些任务中CoT无法提升模型性能，这为理解CoT的能力边界提供了理论依据。

Abstract: It has been shown that the chain of thought (CoT) can enhance the power of
large language models (LLMs) to solve certain mathematical reasoning problems.
However, the capacity of CoT is still not fully explored. As an important
instance, the following basic question has not yet been answered: Does CoT
expand the capability of transformers across all reasoning tasks? We
demonstrate that reasoning with transformers is essentially a memorization
problem for reasoning datasets. Thus, examining the power of CoT across all
reasoning tasks amounts to analyzing the memorization capabilities of CoT
transformers. In this paper, we give a complete description of the memorization
capabilities of fixed-precision transformers with or without CoT and give a
negative answer to the above-mentioned question. Precisely, we first give
necessary and sufficient conditions for fixed-precision transformers with and
without CoT to memorize a finite reasoning dataset and show that these two
conditions do not imply each other. Then, we give lower and upper bounds for
the number of parameters needed for transformers with or without CoT to
memorize a finite reasoning dataset with $N$ elements, which are
$\overline{\Theta}(N)$ in all cases. This implies that there exist reasoning
tasks for which CoT does not enhance the reasoning power of transformers,
leading to a negative answer to the above-mentioned question. Finally, we give
the first results on memorizing infinite reasoning datasets by CoT transformers
and show that some simple infinite datasets cannot be memorized by transformers
with or without CoT.

</details>


### [257] [Deep recurrent-convolutional neural network learning and physics Kalman filtering comparison in dynamic load identification](https://arxiv.org/abs/2511.00100)
*Marios Impraimakis*

Main category: cs.LG

TL;DR: 比较门控循环单元、长短期记忆网络和卷积神经网络在动态结构载荷识别中的性能，并与基于物理的残差卡尔曼滤波器进行对比，评估在小数据集条件下的表现。


<details>
  <summary>Details</summary>
Motivation: 解决土木工程应用中因测试数据少或结构模型不可识别而导致的动态载荷识别不确定性问题。

Method: 使用模拟结构进行顶部激振测试、加州建筑进行地震基础激励测试，以及IASC-ASCE结构健康监测基准问题进行冲击和瞬时载荷测试。

Result: 不同方法在不同载荷场景下表现各异，RKF在物理参数可识别情况下优于神经网络。

Conclusion: 各种方法在不同载荷条件下各有优势，RKF在物理参数可识别场景中表现最佳。

Abstract: The dynamic structural load identification capabilities of the gated
recurrent unit, long short-term memory, and convolutional neural networks are
examined herein. The examination is on realistic small dataset training
conditions and on a comparative view to the physics-based residual Kalman
filter (RKF). The dynamic load identification suffers from the uncertainty
related to obtaining poor predictions when in civil engineering applications
only a low number of tests are performed or are available, or when the
structural model is unidentifiable. In considering the methods, first, a
simulated structure is investigated under a shaker excitation at the top floor.
Second, a building in California is investigated under seismic base excitation,
which results in loading for all degrees of freedom. Finally, the International
Association for Structural Control-American Society of Civil Engineers
(IASC-ASCE) structural health monitoring benchmark problem is examined for
impact and instant loading conditions. Importantly, the methods are shown to
outperform each other on different loading scenarios, while the RKF is shown to
outperform the networks in physically parametrized identifiable cases.

</details>


### [258] [A Saddle Point Remedy: Power of Variable Elimination in Non-convex Optimization](https://arxiv.org/abs/2511.01234)
*Min Gan,Guang-Yong Chen,Yang Yi,Lin Yang*

Main category: cs.LG

TL;DR: 变量消除算法通过重塑优化景观，将原始问题中的鞍点转化为简化问题中的局部最大值，从而显著提升非凸优化的收敛性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 理解为什么变量消除算法在实践中表现出优越的收敛性和鲁棒性，特别是在处理大规模非凸优化中的鞍点问题时。

Method: 基于Hessian惯性和Schur补的严格几何分析，比较原始和简化公式的优化景观，证明变量消除如何改变临界点结构。

Result: 在非凸矩阵分解、两参数神经网络和深度残差网络训练中验证了该方法，显示出稳定性和收敛到更优最小值的显著改进。

Conclusion: 通过鞍点变换实现景观简化是一个强大原则，可以指导设计更鲁棒高效的优化算法。

Abstract: The proliferation of saddle points, rather than poor local minima, is
increasingly understood to be a primary obstacle in large-scale non-convex
optimization for machine learning. Variable elimination algorithms, like
Variable Projection (VarPro), have long been observed to exhibit superior
convergence and robustness in practice, yet a principled understanding of why
they so effectively navigate these complex energy landscapes has remained
elusive. In this work, we provide a rigorous geometric explanation by comparing
the optimization landscapes of the original and reduced formulations. Through a
rigorous analysis based on Hessian inertia and the Schur complement, we prove
that variable elimination fundamentally reshapes the critical point structure
of the objective function, revealing that local maxima in the reduced landscape
are created from, and correspond directly to, saddle points in the original
formulation. Our findings are illustrated on the canonical problem of
non-convex matrix factorization, visualized directly on two-parameter neural
networks, and finally validated in training deep Residual Networks, where our
approach yields dramatic improvements in stability and convergence to superior
minima. This work goes beyond explaining an existing method; it establishes
landscape simplification via saddle point transformation as a powerful
principle that can guide the design of a new generation of more robust and
efficient optimization algorithms.

</details>


### [259] [Loquetier: A Virtualized Multi-LoRA Framework for Unified LLM Fine-tuning and Serving](https://arxiv.org/abs/2511.00101)
*Yuchen Zhang,Hanyue Du,Chun Cao,Jingwei Xu*

Main category: cs.LG

TL;DR: Loquetier是一个虚拟化的多LoRA框架，将LoRA微调和推理统一在单个运行时中，通过虚拟化模块和优化计算流显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在统一LoRA微调和推理方面存在不足，需要一种能同时支持高效微调和推理的框架。

Method: 提出虚拟化模块隔离PEFT修改并支持多适配器，设计优化计算流将微调和推理路径合并，减少内核调用开销。

Result: 在推理任务上达到现有最佳协同服务系统3倍吞吐量，在统一微调推理任务上比PEFT高46.4倍SLO达成率。

Conclusion: Loquetier在性能和灵活性上均优于现有基线，为LoRA模型提供了高效的统一微调和推理解决方案。

Abstract: Low-Rank Adaptation (LoRA) has become a widely adopted parameter-efficient
fine-tuning (PEFT) technique for adapting large language models (LLMs) to
downstream tasks. While prior work has explored strategies for integrating LLM
training and serving, there still remains a gap in unifying fine-tuning and
inference for LoRA-based models. We present Loquetier, a virtualized multi-LoRA
framework that seamlessly integrates LoRA fine-tuning and serving within a
single runtime. Loquetier introduces two key components: (1) a Virtualized
Module that isolates PEFT-based modifications and supports multiple adapters on
a shared base model, and (2) an optimized computation flow with a kernel design
that merges fine-tuning and inference paths in forward propagation, enabling
efficient batching and minimizing kernel invocation overhead. Extensive
experiments across three task settings show that Loquetier consistently
outperforms existing baselines in both performance and flexibility, achieving
up to $3.0\times$ the throughput of the state-of-the-art co-serving system on
inference-only tasks and $46.4\times$ higher SLO attainment than PEFT on
unified fine-tuning and inference tasks. The implementation of Loquetier is
publicly available at https://github.com/NJUDeepEngine/Loquetier.

</details>


### [260] [LC-Opt: Benchmarking Reinforcement Learning and Agentic AI for End-to-End Liquid Cooling Optimization in Data Centers](https://arxiv.org/abs/2511.00116)
*Avisek Naug,Antonio Guillen,Vineet Kumar,Scott Greenwood,Wesley Brewer,Sahand Ghorbanpour,Ashwin Ramesh Babu,Vineet Gundecha,Ricardo Luna Gutierrez,Soumyendu Sarkar*

Main category: cs.LG

TL;DR: LC-Opt是一个可持续液体冷却基准环境，用于强化学习控制策略，优化高性能计算系统的液体冷却能效。


<details>
  <summary>Details</summary>
Motivation: 随着AI工作负载增加，高密度数据中心需要液体冷却进行热管理，而基于机器学习的控制器对于实现更高能效和可靠性至关重要。

Method: 基于橡树岭国家实验室Frontier超级计算机冷却系统的高保真数字孪生，提供详细的Modelica端到端模型，通过Gymnasium接口让RL代理优化热控制参数。

Result: 创建了多目标实时优化挑战，平衡局部热调节和全局能效，并支持集中式和分散式多智能体RL方法，以及可解释的控制策略。

Conclusion: LC-Opt使ML社区、运营商和供应商能够开发可持续的数据中心液体冷却控制解决方案，促进用户信任和系统管理简化。

Abstract: Liquid cooling is critical for thermal management in high-density data
centers with the rising AI workloads. However, machine learning-based
controllers are essential to unlock greater energy efficiency and reliability,
promoting sustainability. We present LC-Opt, a Sustainable Liquid Cooling (LC)
benchmark environment, for reinforcement learning (RL) control strategies in
energy-efficient liquid cooling of high-performance computing (HPC) systems.
Built on the baseline of a high-fidelity digital twin of Oak Ridge National
Lab's Frontier Supercomputer cooling system, LC-Opt provides detailed
Modelica-based end-to-end models spanning site-level cooling towers to data
center cabinets and server blade groups. RL agents optimize critical thermal
controls like liquid supply temperature, flow rate, and granular valve
actuation at the IT cabinet level, as well as cooling tower (CT) setpoints
through a Gymnasium interface, with dynamic changes in workloads. This
environment creates a multi-objective real-time optimization challenge
balancing local thermal regulation and global energy efficiency, and also
supports additional components like a heat recovery unit (HRU). We benchmark
centralized and decentralized multi-agent RL approaches, demonstrate policy
distillation into decision and regression trees for interpretable control, and
explore LLM-based methods that explain control actions in natural language
through an agentic mesh architecture designed to foster user trust and simplify
system management. LC-Opt democratizes access to detailed, customizable liquid
cooling models, enabling the ML community, operators, and vendors to develop
sustainable data center liquid cooling control solutions.

</details>


### [261] [A Spatio-Temporal Online Robust Tensor Recovery Approach for Streaming Traffic Data Imputation](https://arxiv.org/abs/2511.01267)
*Yiyang Yang,Xiejian Chi,Shanxing Gao,Kaidong Wang,Yao Wang*

Main category: cs.LG

TL;DR: 提出了一种新颖的在线鲁棒张量恢复算法，用于智能交通系统中的交通数据质量增强，能够同时处理缺失值和异常值，在保持高恢复精度的同时显著提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统批处理方法在计算和存储资源方面需求大，限制了在持续增长的交通数据量下的可扩展性；现有的在线张量恢复方法在复杂现实场景中性能下降严重，未能充分利用交通数据的固有结构特性。

Method: 将交通数据恢复问题重新表述为流式框架，提出一种同时利用交通数据全局时空相关性和局部一致性的在线鲁棒张量恢复算法。

Result: 在三个真实世界交通数据集上的实验结果表明，该方法实现了高恢复精度，同时与最先进的批处理方法相比，计算效率提高了高达三个数量级。

Conclusion: 该方法作为智能交通系统中交通数据质量增强的可扩展且有效的解决方案具有巨大潜力。

Abstract: Data quality is critical to Intelligent Transportation Systems (ITS), as
complete and accurate traffic data underpin reliable decision-making in traffic
control and management. Recent advances in low-rank tensor recovery algorithms
have shown strong potential in capturing the inherent structure of
high-dimensional traffic data and restoring degraded observations. However,
traditional batch-based methods demand substantial computational and storage
resources, which limits their scalability in the face of continuously expanding
traffic data volumes. Moreover, recent online tensor recovery methods often
suffer from severe performance degradation in complex real-world scenarios due
to their insufficient exploitation of the intrinsic structural properties of
traffic data. To address these challenges, we reformulate the traffic data
recovery problem within a streaming framework, and propose a novel online
robust tensor recovery algorithm that simultaneously leverages both the global
spatio-temporal correlations and local consistency of traffic data, achieving
high recovery accuracy and significantly improved computational efficiency in
large-scale scenarios. Our method is capable of simultaneously handling missing
and anomalous values in traffic data, and demonstrates strong adaptability
across diverse missing patterns. Experimental results on three real-world
traffic datasets demonstrate that the proposed approach achieves high recovery
accuracy while significantly improving computational efficiency by up to three
orders of magnitude compared to state-of-the-art batch-based methods. These
findings highlight the potential of the proposed approach as a scalable and
effective solution for traffic data quality enhancement in ITS.

</details>


### [262] [Automated Discovery of Conservation Laws via Hybrid Neural ODE-Transformers](https://arxiv.org/abs/2511.00102)
*Vivan Doshi*

Main category: cs.LG

TL;DR: 提出一种混合框架，从噪声轨迹数据中自动发现守恒量，结合神经ODE、Transformer和符号-数值验证器，显著优于直接在轨迹数据上操作的基线方法。


<details>
  <summary>Details</summary>
Motivation: 从观测数据中识别守恒定律是科学进步的关键，但现有方法在噪声数据中识别这些不变量仍面临重大挑战。

Method: 集成三个组件：(1) 神经ODE学习系统动力学的连续模型，(2) Transformer基于学习到的向量场生成符号候选不变量，(3) 符号-数值验证器为候选量提供强数值验证。

Result: 在典型物理系统上测试，框架显著优于直接在轨迹数据上操作的基线方法。

Conclusion: 证明了分离的"先学习后搜索"方法在从不完美数据中发现数学原理方面的鲁棒性。

Abstract: The discovery of conservation laws is a cornerstone of scientific progress.
However, identifying these invariants from observational data remains a
significant challenge. We propose a hybrid framework to automate the discovery
of conserved quantities from noisy trajectory data. Our approach integrates
three components: (1) a Neural Ordinary Differential Equation (Neural ODE) that
learns a continuous model of the system's dynamics, (2) a Transformer that
generates symbolic candidate invariants conditioned on the learned vector
field, and (3) a symbolic-numeric verifier that provides a strong numerical
certificate for the validity of these candidates. We test our framework on
canonical physical systems and show that it significantly outperforms baselines
that operate directly on trajectory data. This work demonstrates the robustness
of a decoupled learn-then-search approach for discovering mathematical
principles from imperfect data.

</details>


### [263] [DCcluster-Opt: Benchmarking Dynamic Multi-Objective Optimization for Geo-Distributed Data Center Workloads](https://arxiv.org/abs/2511.00117)
*Antonio Guillen-Perez,Avisek Naug,Vineet Gundecha,Sahand Ghorbanpour,Ricardo Luna Gutierrez,Ashwin Ramesh Babu,Munther Salim,Shubhanker Banerjee,Eoin H. Oude Essink,Damien Fay,Soumyendu Sarkar*

Main category: cs.LG

TL;DR: DCcluster-Opt是一个开源的高保真模拟基准，用于可持续的、地理时间任务调度，结合了真实世界数据集和物理信息模型，以加速地理分布式数据中心可持续计算解决方案的开发。


<details>
  <summary>Details</summary>
Motivation: 大规模AI的能源需求和碳足迹不断增加，需要智能的全球分布式数据中心工作负载管理，但缺乏能够真实捕捉环境因素、数据中心物理和网络动态相互作用的基准测试。

Method: 结合精选的真实世界数据集（AI工作负载痕迹、电网碳强度、电力市场、天气等）与物理信息模型的数据中心操作模型，提供模块化奖励系统和Gymnasium API，包括强化学习和基于规则的策略基线。

Result: 提出了一个具有挑战性的调度问题环境，其中顶层协调代理必须动态重新分配或延迟任务，以在可配置的数据中心集群中优化多个目标。

Conclusion: DCcluster-Opt通过提供现实、可配置且可访问的测试平台，加速了地理分布式数据中心下一代可持续计算解决方案的开发和验证。

Abstract: The increasing energy demands and carbon footprint of large-scale AI require
intelligent workload management in globally distributed data centers. Yet
progress is limited by the absence of benchmarks that realistically capture the
interplay of time-varying environmental factors (grid carbon intensity,
electricity prices, weather), detailed data center physics (CPUs, GPUs, memory,
HVAC energy), and geo-distributed network dynamics (latency and transmission
costs). To bridge this gap, we present DCcluster-Opt: an open-source,
high-fidelity simulation benchmark for sustainable, geo-temporal task
scheduling. DCcluster-Opt combines curated real-world datasets, including AI
workload traces, grid carbon intensity, electricity markets, weather across 20
global regions, cloud transmission costs, and empirical network delay
parameters with physics-informed models of data center operations, enabling
rigorous and reproducible research in sustainable computing. It presents a
challenging scheduling problem where a top-level coordinating agent must
dynamically reassign or defer tasks that arrive with resource and service-level
agreement requirements across a configurable cluster of data centers to
optimize multiple objectives. The environment also models advanced components
such as heat recovery. A modular reward system enables an explicit study of
trade-offs among carbon emissions, energy costs, service level agreements, and
water use. It provides a Gymnasium API with baseline controllers, including
reinforcement learning and rule-based strategies, to support reproducible ML
research and a fair comparison of diverse algorithms. By offering a realistic,
configurable, and accessible testbed, DCcluster-Opt accelerates the development
and validation of next-generation sustainable computing solutions for
geo-distributed data centers.

</details>


### [264] [Estimation of Toeplitz Covariance Matrices using Overparameterized Gradient Descent](https://arxiv.org/abs/2511.01605)
*Daniel Busbib,Ami Wiesel*

Main category: cs.LG

TL;DR: 该论文重新审视了托普利兹协方差估计问题，通过过参数化梯度下降方法，证明了当使用2P或4P个复正弦波参数化时，可以从随机初始化实现全局收敛，并提出了一种加速梯度下降变体。


<details>
  <summary>Details</summary>
Motivation: 受到深度学习中使用过参数化梯度下降取得成功的启发，重新审视传统托普利兹协方差估计问题，探索简单梯度下降方法在结构化协方差估计中的潜力。

Method: 将P×P协方差矩阵建模为K个可学习参数的复正弦波之和，通过梯度下降优化参数。当K=P时可能收敛到次优解，但使用K=2P或4P的过参数化可以确保全局收敛。还提出了具有分离学习率的加速梯度下降变体。

Result: 数值实验表明，过参数化梯度下降在挑战性设置下可以达到或超过最先进方法的精度，同时保持简单性和可扩展性。当频率固定仅优化幅度时，优化景观是渐近良性的，任何驻点都能恢复真实协方差。

Conclusion: 过参数化梯度下降为托普利兹协方差估计提供了一种简单而有效的替代方案，能够匹配或超越复杂优化方法的性能，同时具有更好的收敛性和可扩展性。

Abstract: We consider covariance estimation under Toeplitz structure. Numerous
sophisticated optimization methods have been developed to maximize the Gaussian
log-likelihood under Toeplitz constraints. In contrast, recent advances in deep
learning demonstrate the surprising power of simple gradient descent (GD)
applied to overparameterized models. Motivated by this trend, we revisit
Toeplitz covariance estimation through the lens of overparameterized GD. We
model the $P\times P$ covariance as a sum of $K$ complex sinusoids with
learnable parameters and optimize them via GD. We show that when $K = P$, GD
may converge to suboptimal solutions. However, mild overparameterization ($K =
2P$ or $4P$) consistently enables global convergence from random
initializations. We further propose an accelerated GD variant with separate
learning rates for amplitudes and frequencies. When frequencies are fixed and
only amplitudes are optimized, we prove that the optimization landscape is
asymptotically benign and any stationary point recovers the true covariance.
Finally, numerical experiments demonstrate that overparameterized GD can match
or exceed the accuracy of state-of-the-art methods in challenging settings,
while remaining simple and scalable.

</details>


### [265] [Pelican-VL 1.0: A Foundation Brain Model for Embodied Intelligence](https://arxiv.org/abs/2511.00108)
*Yi Zhang,Che Liu,Xiancong Ren,Hanchu Ni,Shuai Zhang,Zeyuan Ding,Jiayu Hu,Hanzhe Shan,Zhenwei Niu,Zhaoyang Liu,Yue Zhao,Junbo Qi,Qinfan Zhang,Dengjie Li,Yidong Wang,Jiachen Luo,Yong Dai,Jian Tang,Xiaozhu Ju*

Main category: cs.LG

TL;DR: Pelican-VL 1.0是一个开源的具身大脑模型家族，参数规模从70亿到720亿，是目前最大规模的开源具身多模态大脑模型。通过DPPO框架和metaloop机制，在1000+ A800 GPU集群上训练，性能比基础模型提升20.3%，超越100B级开源模型10.6%。


<details>
  <summary>Details</summary>
Motivation: 明确使命：将强大智能嵌入各种具身系统中，推动具身智能的发展。

Method: 采用DPPO（刻意练习策略优化）框架，受人类元认知启发，构建metaloop机制（RL-精炼-诊断-SFT循环），从包含40亿+token的原始数据集中蒸馏高质量数据集。

Result: 在知名具身基准测试中，性能比基础模型提升20.3%，超越100B级开源模型10.6%，与领先专有系统表现相当。消耗超过50k+ A800 GPU小时/检查点。

Conclusion: Pelican-VL 1.0通过数据力量与智能自适应学习机制的深度整合，建立了当前最大规模的开源具身多模态大脑模型，在性能上达到行业领先水平。

Abstract: This report presents Pelican-VL 1.0, a new family of open-source embodied
brain models with parameter scales ranging from 7 billion to 72 billion. Our
explicit mission is clearly stated as: To embed powerful intelligence into
various embodiments. Pelican-VL 1.0 is currently the largest-scale open-source
embodied multimodal brain model. Its core advantage lies in the in-depth
integration of data power and intelligent adaptive learning mechanisms.
Specifically, metaloop distilled a high-quality dataset from a raw dataset
containing 4+ billion tokens. Pelican-VL 1.0 is trained on a large-scale
cluster of 1000+ A800 GPUs, consuming over 50k+ A800 GPU-hours per checkpoint.
This translates to a 20.3% performance uplift from its base model and
outperforms 100B-level open-source counterparts by 10.6%, placing it on par
with leading proprietary systems on well-known embodied benchmarks. We
establish a novel framework, DPPO (Deliberate Practice Policy Optimization),
inspired by human metacognition to train Pelican-VL 1.0. We operationalize this
as a metaloop that teaches the AI to practice deliberately, which is a
RL-Refine-Diagnose-SFT loop.

</details>


### [266] [Cross-Treatment Effect Estimation for Multi-Category, Multi-Valued Causal Inference via Dynamic Neural Masking](https://arxiv.org/abs/2511.01641)
*Xiaopeng Ke,Yihan Yu,Ruyue Zhang,Zhishuo Zhou,Fangzhou Shi,Chang Men,Zhengdan Zhu*

Main category: cs.LG

TL;DR: XTNet是一种用于多类别多值处理效应估计的新型网络架构，通过动态掩码机制捕捉处理交互效应，无需限制性结构假设，在合成和真实数据集上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 反事实因果推断在处理多类别、多值处理时面临重大挑战，现有方法局限于二元或单类型处理，存在假设限制、可扩展性差和评估框架不足的问题。

Method: XTNet采用交叉效应估计模块和动态掩码机制，通过分解策略将基本效应与交叉处理交互分离，有效建模组合处理空间，并提出了考虑处理成本和交互效应的MCMV-AUCC评估指标。

Result: 在合成和真实数据集上的广泛实验表明，XTNet在排序准确性和效应估计质量方面始终优于最先进的基线方法，真实世界A/B测试结果进一步证实了其有效性。

Conclusion: XTNet为多类别多值处理效应估计提供了一种有效的解决方案，能够准确捕捉复杂处理交互效应，在真实场景中具有实用价值。

Abstract: Counterfactual causal inference faces significant challenges when extended to
multi-category, multi-valued treatments, where complex cross-effects between
heterogeneous interventions are difficult to model. Existing methodologies
remain constrained to binary or single-type treatments and suffer from
restrictive assumptions, limited scalability, and inadequate evaluation
frameworks for complex intervention scenarios.
  We present XTNet, a novel network architecture for multi-category,
multi-valued treatment effect estimation. Our approach introduces a
cross-effect estimation module with dynamic masking mechanisms to capture
treatment interactions without restrictive structural assumptions. The
architecture employs a decomposition strategy separating basic effects from
cross-treatment interactions, enabling efficient modeling of combinatorial
treatment spaces. We also propose MCMV-AUCC, a suitable evaluation metric that
accounts for treatment costs and interaction effects. Extensive experiments on
synthetic and real-world datasets demonstrate that XTNet consistently
outperforms state-of-the-art baselines in both ranking accuracy and effect
estimation quality. The results of the real-world A/B test further confirm its
effectiveness.

</details>


### [267] [MeixnerNet: Adaptive and Robust Spectral Graph Neural Networks with Discrete Orthogonal Polynomials](https://arxiv.org/abs/2511.00113)
*Huseyin Goksu*

Main category: cs.LG

TL;DR: MeixnerNet是一种新型谱图神经网络，使用离散正交多项式（Meixner多项式）替代传统的连续正交多项式，解决了图结构与滤波器之间的理论不匹配问题，并在性能和超参数鲁棒性方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的谱图神经网络使用连续正交多项式（如Chebyshev多项式）作为图卷积滤波器，但这与图的离散结构存在理论不匹配，可能导致性能不佳和对超参数设置的脆弱性。

Method: 提出MeixnerNet架构，采用离散正交Meixner多项式作为滤波器，并使其两个关键形状参数β和c可学习，以适应图的特定谱特性。通过Laplacian缩放和逐基LayerNorm的组合技术解决数值不稳定性问题。

Result: 在K=2的最优设置下，MeixnerNet在3个基准测试中的2个上表现优于ChebyNet基线。更重要的是，MeixnerNet对多项式阶数K的变化具有异常鲁棒性，而ChebyNet对此高度脆弱且性能会崩溃。

Conclusion: MeixnerNet通过使用离散正交多项式解决了谱图神经网络中的理论不匹配问题，不仅实现了竞争性性能，还显著提高了对关键超参数变化的鲁棒性。

Abstract: Spectral Graph Neural Networks (GNNs) have achieved state-of-the-art results
by defining graph convolutions in the spectral domain. A common approach,
popularized by ChebyNet, is to use polynomial filters based on continuous
orthogonal polynomials (e.g., Chebyshev). This creates a theoretical
disconnect, as these continuous-domain filters are applied to inherently
discrete graph structures. We hypothesize this mismatch can lead to suboptimal
performance and fragility to hyperparameter settings.
  In this paper, we introduce MeixnerNet, a novel spectral GNN architecture
that employs discrete orthogonal polynomials -- specifically, the Meixner
polynomials $M_k(x; \beta, c)$. Our model makes the two key shape parameters of
the polynomial, beta and c, learnable, allowing the filter to adapt its
polynomial basis to the specific spectral properties of a given graph. We
overcome the significant numerical instability of these polynomials by
introducing a novel stabilization technique that combines Laplacian scaling
with per-basis LayerNorm.
  We demonstrate experimentally that MeixnerNet achieves
competitive-to-superior performance against the strong ChebyNet baseline at the
optimal K = 2 setting (winning on 2 out of 3 benchmarks). More critically, we
show that MeixnerNet is exceptionally robust to variations in the polynomial
degree K, a hyperparameter to which ChebyNet proves to be highly fragile,
collapsing in performance where MeixnerNet remains stable.

</details>


### [268] [Fractional Diffusion Bridge Models](https://arxiv.org/abs/2511.01795)
*Gabriel Nobis,Maximilian Springenberg,Arina Belova,Rembert Daems,Christoph Knochenhauer,Manfred Opper,Tolga Birdal,Wojciech Samek*

Main category: cs.LG

TL;DR: 提出了分数扩散桥模型(FDBM)，这是一种基于分数布朗运动近似的新型生成扩散桥框架，能够处理具有时间相关性、长程依赖性和异常扩散现象的真实随机过程。


<details>
  <summary>Details</summary>
Motivation: 真实随机过程通常具有记忆效应、时间相关性、长程依赖性和异常扩散现象，这些特性无法通过标准扩散或桥模型中的布朗运动来捕捉。

Method: 利用分数布朗运动的马尔可夫近似(MA-fBM)构建FDBM，保持分数布朗运动的非马尔可夫性质同时实现可处理的推理。扩展到Schrödinger桥问题，推导出用于学习无配对数据转换的原则性损失函数。

Result: 在蛋白质构象预测和无配对图像翻译任务中，FDBM相比布朗运动基线表现更优：蛋白质结构预测中Cα原子位置的均方根偏差更低，无配对图像翻译中Fréchet Inception距离更低。

Conclusion: FDBM框架能够有效建模真实随机过程的复杂特性，在多个任务中优于传统基于布朗运动的模型。

Abstract: We present Fractional Diffusion Bridge Models (FDBM), a novel generative
diffusion bridge framework driven by an approximation of the rich and
non-Markovian fractional Brownian motion (fBM). Real stochastic processes
exhibit a degree of memory effects (correlations in time), long-range
dependencies, roughness and anomalous diffusion phenomena that are not captured
in standard diffusion or bridge modeling due to the use of Brownian motion
(BM). As a remedy, leveraging a recent Markovian approximation of fBM (MA-fBM),
we construct FDBM that enable tractable inference while preserving the
non-Markovian nature of fBM. We prove the existence of a coupling-preserving
generative diffusion bridge and leverage it for future state prediction from
paired training data. We then extend our formulation to the Schr\"{o}dinger
bridge problem and derive a principled loss function to learn the unpaired data
translation. We evaluate FDBM on both tasks: predicting future protein
conformations from aligned data, and unpaired image translation. In both
settings, FDBM achieves superior performance compared to the Brownian
baselines, yielding lower root mean squared deviation (RMSD) of C$_\alpha$
atomic positions in protein structure prediction and lower Fr\'echet Inception
Distance (FID) in unpaired image translation.

</details>


### [269] [FTT-GRU: A Hybrid Fast Temporal Transformer with GRU for Remaining Useful Life Prediction](https://arxiv.org/abs/2511.00564)
*Varun Teja Chirukiri,Udaya Bhasker Cheerala,Sandeep Kanta,Abdul Karim,Praveen Damacharla*

Main category: cs.LG

TL;DR: 提出了FTT-GRU混合模型，结合轻量级Transformer变体(FTT)和门控循环单元(GRU)，用于工业机械剩余使用寿命预测，在NASA CMAPSS数据集上取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法如LSTM和CNN难以同时建模全局时间依赖性和细粒度退化趋势，需要更有效的模型来准确预测剩余使用寿命。

Method: 使用快速时序Transformer(FTT)通过快速傅里叶变换实现线性化注意力，结合GRU层进行序列建模，构建紧凑的混合架构。

Result: 在CMAPSS FD001数据集上，RMSE为30.76，MAE为18.97，R²=0.45，CPU延迟1.12ms，相比最佳基线TCN-Attention提升RMSE 1.16%，MAE 4.00%。

Conclusion: 紧凑的Transformer-RNN混合模型能够在CMAPSS上提供准确高效的RUL预测，适用于实时工业预测性维护。

Abstract: Accurate prediction of the remaining useful life (RUL) of industrial
machinery is essential for reducing downtime and optimizing maintenance
schedules. Existing approaches, such as long short-term memory (LSTM) networks
and convolutional neural networks (CNNs), often struggle to model both global
temporal dependencies and fine-grained degradation trends in multivariate
sensor data. We propose a hybrid model, FTT-GRU, which combines a Fast Temporal
Transformer (FTT) -- a lightweight Transformer variant using linearized
attention via fast Fourier transform (FFT) -- with a gated recurrent unit (GRU)
layer for sequential modeling. To the best of our knowledge, this is the first
application of an FTT with a GRU for RUL prediction on NASA CMAPSS, enabling
simultaneous capture of global and local degradation patterns in a compact
architecture. On CMAPSS FD001, FTT-GRU attains RMSE 30.76, MAE 18.97, and
$R^2=0.45$, with 1.12 ms CPU latency at batch=1. Relative to the best published
deep baseline (TCN--Attention), it improves RMSE by 1.16\% and MAE by 4.00\%.
Training curves averaged over $k=3$ runs show smooth convergence with narrow
95\% confidence bands, and ablations (GRU-only, FTT-only) support the
contribution of both components. These results demonstrate that a compact
Transformer-RNN hybrid delivers accurate and efficient RUL predictions on
CMAPSS, making it suitable for real-time industrial prognostics.

</details>


### [270] [Bridging Lifelong and Multi-Task Representation Learning via Algorithm and Complexity Measure](https://arxiv.org/abs/2511.01847)
*Zhi Wang,Chicheng Zhang,Ramya Korlakai Vinayak*

Main category: cs.LG

TL;DR: 提出了一种终身表示学习框架，通过多任务经验风险最小化作为子程序，并基于新引入的任务逃避者维度建立样本复杂度界限。


<details>
  <summary>Details</summary>
Motivation: 终身学习中学习者面临一系列具有共享结构的任务，需要在线持续学习并利用现有知识，而传统多任务学习或元学习需要任务预先可用。

Method: 使用多任务经验风险最小化作为子程序的简单算法，引入任务逃避者维度来分析样本复杂度。

Result: 建立了适用于广泛学习问题的样本复杂度界限，并在噪声下的分类和回归任务中进行了具体实例化。

Conclusion: 提出的终身表示学习框架能够有效处理在线持续学习场景，通过理论分析为实际应用提供了理论保证。

Abstract: In lifelong learning, a learner faces a sequence of tasks with shared
structure and aims to identify and leverage it to accelerate learning. We study
the setting where such structure is captured by a common representation of
data. Unlike multi-task learning or learning-to-learn, where tasks are
available upfront to learn the representation, lifelong learning requires the
learner to make use of its existing knowledge while continually gathering
partial information in an online fashion. In this paper, we consider a
generalized framework of lifelong representation learning. We propose a simple
algorithm that uses multi-task empirical risk minimization as a subroutine and
establish a sample complexity bound based on a new notion we introduce--the
task-eluder dimension. Our result applies to a wide range of learning problems
involving general function classes. As concrete examples, we instantiate our
result on classification and regression tasks under noise.

</details>


### [271] [Analysis of Line Break prediction models for detecting defensive breakthrough in football](https://arxiv.org/abs/2511.00121)
*Shoma Yagi,Jun Ichikawa,Genki Ichinose*

Main category: cs.LG

TL;DR: 开发了一个机器学习模型来预测足球中的防线突破，使用XGBoost分类器结合球员位置、速度和空间配置等特征，取得了高预测精度。


<details>
  <summary>Details</summary>
Motivation: 足球中突破对手防线是创造得分机会的关键指标，但以往研究主要关注射门或进球机会，而非如何突破防线。

Method: 使用2023年J1联赛赛季的事件和追踪数据，构建包含189个特征的XGBoost分类器模型，预测防线突破概率。

Result: 模型预测精度很高，AUC为0.982，Brier分数为0.015。SHAP分析显示进攻球员速度、防线间隙和空间分布是重要影响因素。

Conclusion: 防线突破与创造得分机会密切相关，为理解足球战术动态提供了量化框架。

Abstract: In football, attacking teams attempt to break through the opponent's
defensive line to create scoring opportunities. This action, known as a Line
Break, is a critical indicator of offensive effectiveness and tactical
performance, yet previous studies have mainly focused on shots or goal
opportunities rather than on how teams break the defensive line. In this study,
we develop a machine learning model to predict Line Breaks using event and
tracking data from the 2023 J1 League season. The model incorporates 189
features, including player positions, velocities, and spatial configurations,
and employs an XGBoost classifier to estimate the probability of Line Breaks.
The proposed model achieved high predictive accuracy, with an AUC of 0.982 and
a Brier score of 0.015. Furthermore, SHAP analysis revealed that factors such
as offensive player speed, gaps in the defensive line, and offensive players'
spatial distributions significantly contribute to the occurrence of Line
Breaks. Finally, we found a moderate positive correlation between the predicted
probability of being Line-Broken and the number of shots and crosses conceded
at the team level. These results suggest that Line Breaks are closely linked to
the creation of scoring opportunities and provide a quantitative framework for
understanding tactical dynamics in football.

</details>


### [272] [Koopman-based Prediction of Connectivity for Flying Ad Hoc Networks](https://arxiv.org/abs/2511.01286)
*Sivaram Krishnan,Jinho Choi,Jihong Park,Gregory Sherman,Benjamin Campbell*

Main category: cs.LG

TL;DR: 该论文探索使用数据驱动的Koopman方法来建模无人机自组网中的轨迹动态，通过集中式和分布式两种方法预测信号干扰噪声比，准确预测通信中断事件，帮助无人机调度传输。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法在静态无线环境中表现良好，但在高度动态的飞行自组网环境中存在局限性，需要新的方法来应对不断变化的网络拓扑挑战。

Method: 基于Koopman算子理论，提出集中式和分布式两种数据驱动方法，建模无人机轨迹动态并预测信号干扰噪声比。

Result: 这些方法能够准确预测导致通信中断的连接和隔离事件，为无人机传输调度提供可靠预测。

Conclusion: 数据驱动的Koopman方法能够有效解决动态飞行自组网环境中的通信挑战，提高网络性能。

Abstract: The application of machine learning (ML) to communication systems is expected
to play a pivotal role in future artificial intelligence (AI)-based
next-generation wireless networks. While most existing works focus on ML
techniques for static wireless environments, they often face limitations when
applied to highly dynamic environments, such as flying ad hoc networks
(FANETs). This paper explores the use of data-driven Koopman approaches to
address these challenges. Specifically, we investigate how these approaches can
model UAV trajectory dynamics within FANETs, enabling more accurate predictions
and improved network performance. By leveraging Koopman operator theory, we
propose two possible approaches -- centralized and distributed -- to
efficiently address the challenges posed by the constantly changing topology of
FANETs. To demonstrate this, we consider a FANET performing surveillance with
UAVs following pre-determined trajectories and predict
signal-to-interference-plus-noise ratios (SINRs) to ensure reliable
communication between UAVs. Our results show that these approaches can
accurately predict connectivity and isolation events that lead to modelled
communication outages. This capability could help UAVs schedule their
transmissions based on these predictions.

</details>


### [273] [Cross-fluctuation phase transitions reveal sampling dynamics in diffusion models](https://arxiv.org/abs/2511.00124)
*Sai Niranjan Ramachandran,Manish Krishan Lal,Suvrit Sra*

Main category: cs.LG

TL;DR: 使用统计物理中的交叉涨落分析扩散模型采样动态，发现样本经历尖锐离散转变，形成目标分布。这些转变可通过交叉涨落检测，提升采样效率并改进分类和风格迁移任务。


<details>
  <summary>Details</summary>
Motivation: 研究扩散模型中采样动态的演变过程，理解样本从初始分布到目标分布的转变机制，探索如何利用这些转变提升生成效率。

Method: 采用统计物理中的交叉涨落统计量分析采样动态，推导方差保持SDE的交叉涨落闭式解，检测离散转变点。

Result: 发现采样过程中的尖锐离散转变，这些转变可被检测并用于提升采样效率、加速条件生成、改进零样本分类和风格迁移任务。

Conclusion: 该框架统一了离散马尔可夫链理论与连续动态，为理解扩散模型采样过程提供了新视角，并带来实际性能提升。

Abstract: We analyse how the sampling dynamics of distributions evolve in score-based
diffusion models using cross-fluctuations, a centered-moment statistic from
statistical physics. Specifically, we show that starting from an unbiased
isotropic normal distribution, samples undergo sharp, discrete transitions,
eventually forming distinct events of a desired distribution while
progressively revealing finer structure. As this process is reversible, these
transitions also occur in reverse, where intermediate states progressively
merge, tracing a path back to the initial distribution. We demonstrate that
these transitions can be detected as discontinuities in $n^{\text{th}}$-order
cross-fluctuations. For variance-preserving SDEs, we derive a closed-form for
these cross-fluctuations that is efficiently computable for the reverse
trajectory. We find that detecting these transitions directly boosts sampling
efficiency, accelerates class-conditional and rare-class generation, and
improves two zero-shot tasks--image classification and style transfer--without
expensive grid search or retraining. We also show that this viewpoint unifies
classical coupling and mixing from finite Markov chains with continuous
dynamics while extending to stochastic SDEs and non Markovian samplers. Our
framework therefore bridges discrete Markov chain theory, phase analysis, and
modern generative modeling.

</details>


### [274] [Stochastic Regret Guarantees for Online Zeroth- and First-Order Bilevel Optimization](https://arxiv.org/abs/2511.01126)
*Parvin Nazari,Bojian Hou,Davoud Ataee Tarzanagh,Li Shen,George Michailidis*

Main category: cs.LG

TL;DR: 提出了一种新的搜索方向，使一阶和零阶随机在线双层优化算法无需窗口平滑即可实现次线性随机双层遗憾，并提高了超梯度估计的效率。


<details>
  <summary>Details</summary>
Motivation: 当前在线双层优化方法依赖确定性窗口平滑遗憾最小化，在函数快速变化时可能无法准确反映系统性能。

Method: 引入新的搜索方向，开发一阶和零阶随机算法，减少超梯度估计的oracle依赖，同时更新内外层变量和线性系统解，使用零阶方法估计Hessian矩阵、Jacobian矩阵和梯度。

Result: 实验验证了在线参数损失调优和黑盒对抗攻击任务上的有效性。

Conclusion: 提出的框架在无需窗口平滑的情况下实现了次线性随机双层遗憾，并显著提高了计算效率。

Abstract: Online bilevel optimization (OBO) is a powerful framework for machine
learning problems where both outer and inner objectives evolve over time,
requiring dynamic updates. Current OBO approaches rely on deterministic
\textit{window-smoothed} regret minimization, which may not accurately reflect
system performance when functions change rapidly. In this work, we introduce a
novel search direction and show that both first- and zeroth-order (ZO)
stochastic OBO algorithms leveraging this direction achieve sublinear
{stochastic bilevel regret without window smoothing}. Beyond these guarantees,
our framework enhances efficiency by: (i) reducing oracle dependence in
hypergradient estimation, (ii) updating inner and outer variables alongside the
linear system solution, and (iii) employing ZO-based estimation of Hessians,
Jacobians, and gradients. Experiments on online parametric loss tuning and
black-box adversarial attacks validate our approach.

</details>


### [275] [Dynamic Model Selection for Trajectory Prediction via Pairwise Ranking and Meta-Features](https://arxiv.org/abs/2511.00126)
*Lu Bowen*

Main category: cs.LG

TL;DR: 提出动态多专家门控框架，在复杂长尾驾驶场景中自适应选择最可靠的轨迹预测器，相比单一模型显著提升预测精度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有深度轨迹预测器在复杂长尾场景中不可靠，简单物理模型有时反而优于先进网络，需要突破"一个模型适应所有"的范式限制。

Method: 使用动态多专家门控框架，基于内部模型信号（稳定性和不确定性）自适应选择物理信息LSTM、Transformer和微调GameFormer中的一个进行轨迹预测。

Result: 在nuPlan-mini数据集上，FDE降低9.5%至2.567米，达到oracle性能的57.8%；左转场景中FDE降低约10%，在离线验证和开环评估中均表现一致改进。

Conclusion: 自适应混合系统能显著提升安全关键自动驾驶中的轨迹可靠性，为超越静态单模型范式提供了实用路径。

Abstract: Recent deep trajectory predictors (e.g., Jiang et al., 2023; Zhou et al.,
2022) have achieved strong average accuracy but remain unreliable in complex
long-tail driving scenarios. These limitations reveal the weakness of the
prevailing "one-model-fits-all" paradigm, particularly in safety-critical urban
contexts where simpler physics-based models can occasionally outperform
advanced networks (Kalman, 1960). To bridge this gap, we propose a dynamic
multi-expert gating framework that adaptively selects the most reliable
trajectory predictor among a physics-informed LSTM, a Transformer, and a
fine-tuned GameFormer on a per-sample basis.
  Our method leverages internal model signals (meta-features) such as stability
and uncertainty (Gal and Ghahramani, 2016), which we demonstrate to be
substantially more informative than geometric scene descriptors. To the best of
our knowledge, this is the first work to formulate trajectory expert selection
as a pairwise-ranking problem over internal model signals (Burges et al.,
2005), directly optimizing decision quality without requiring post-hoc
calibration.
  Evaluated on the nuPlan-mini dataset (Caesar et al., 2021) with 1,287
samples, our LLM-enhanced tri-expert gate achieves a Final Displacement Error
(FDE) of 2.567 m, representing a 9.5 percent reduction over GameFormer (2.835
m), and realizes 57.8 percent of the oracle performance bound. In open-loop
simulations, after trajectory horizon alignment, the same configuration reduces
FDE on left-turn scenarios by approximately 10 percent, demonstrating
consistent improvements across both offline validation and open-loop
evaluation. These results indicate that adaptive hybrid systems enhance
trajectory reliability in safety-critical autonomous driving, providing a
practical pathway beyond static single-model paradigms.

</details>


### [276] [Casing Collar Identification using AlexNet-based Neural Networks for Depth Measurement in Oil and Gas Wells](https://arxiv.org/abs/2511.00129)
*Siyu Xiao,Xindi Zhao,Tianhao Mao,Yiwei Wang,Yuqiao Chen,Hongyun Zhang,Jian Wang,Junjie Wang,Shuang Liu,Tupei Chen,Yang Liu*

Main category: cs.LG

TL;DR: 本文提出了一种集成到井下工具中的CCL信号采集系统，用于构建数据集，并评估了数据增强预处理方法在基于AlexNet的套管接箍识别模型中的有效性。


<details>
  <summary>Details</summary>
Motivation: 精确的井下深度测量对油气井作业至关重要，而基于神经网络的CCL信号识别在套管接箍识别方面取得了进展，但预处理方法和真实井数据有限的问题仍未解决。

Method: 提出了集成到井下工具中的CCL信号采集系统，采用标准化、标签分布平滑、随机裁剪等数据增强方法，并使用基于AlexNet的神经网络模型进行训练。

Result: 实验表明标准化、LDS和随机裁剪是模型训练的基本要求，而LSR、时间缩放和多重采样显著提高了模型泛化能力。两个基准模型的F1分数分别从0.937和0.952提高到1.0。

Conclusion: 该工作解决了在CCL数据有限环境下训练套管接箍识别模型的数据增强方法空白，验证了所提方法的有效性和实际适用性。

Abstract: Accurate downhole depth measurement is essential for oil and gas well
operations, directly influencing reservoir contact, production efficiency, and
operational safety. Collar correlation using a casing collar locator (CCL) is
fundamental for precise depth calibration. While neural network-based CCL
signal recognition has achieved significant progress in collar identification,
preprocessing methods for such applications remain underdeveloped. Moreover,
the limited availability of real well data poses substantial challenges for
training neural network models that require extensive datasets. This paper
presents a system integrated into downhole tools for CCL signal acquisition to
facilitate dataset construction. We propose comprehensive preprocessing methods
for data augmentation and evaluate their effectiveness using our AlexNet-based
neural network models. Through systematic experimentation across various
configuration combinations, we analyze the contribution of each augmentation
method. Results demonstrate that standardization, label distribution smoothing
(LDS), and random cropping are fundamental requirements for model training,
while label smoothing regularization (LSR), time scaling, and multiple sampling
significantly enhance model generalization capability. The F1 scores of our two
benchmark models trained with the proposed augmentation methods maximumly
improve from 0.937 and 0.952 to 1.0 and 1.0, respectively. Performance
validation on real CCL waveforms confirms the effectiveness and practical
applicability of our approach. This work addresses the gaps in data
augmentation methodologies for training casing collar recognition models in CCL
data-limited environments.

</details>


### [277] [No-rank Tensor Decomposition Using Metric Learning](https://arxiv.org/abs/2511.01816)
*Maryam Bagherian*

Main category: cs.LG

TL;DR: 提出了一种基于度量学习的无秩张量分解框架，用判别性相似度优化替代传统重构目标，通过三元组损失和多样性、均匀性正则化学习数据驱动的嵌入空间，在语义相似性和计算效率方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统张量分解方法基于重构和固定秩约束，难以捕捉高维数据中的语义结构，需要一种能直接反映语义相似性的分解框架。

Method: 采用度量学习方法，通过优化三元组损失结合多样性和均匀性正则化，学习数据驱动的嵌入表示，距离直接反映语义相似性。

Result: 在多个领域（人脸识别、脑连接分析、模拟数据）的评估中，该方法在聚类指标上显著优于PCA、t-SNE、UMAP和传统张量分解方法，且在小数据集上表现优于基于Transformer的方法。

Conclusion: 度量学习为张量分析提供了新范式，优先考虑语义相关性而非像素级保真度，在数据稀缺场景下具有计算优势。

Abstract: Tensor decomposition faces fundamental challenges in analyzing
high-dimensional data, where traditional methods based on reconstruction and
fixed-rank constraints often fail to capture semantically meaningful
structures. This paper introduces a no-rank tensor decomposition framework
grounded in metric learning, which replaces reconstruction objectives with a
discriminative, similarity-based optimization. The proposed approach learns
data-driven embeddings by optimizing a triplet loss with diversity and
uniformity regularization, creating a feature space where distance directly
reflects semantic similarity. We provide theoretical guarantees for the
framework's convergence and establish bounds on its metric properties.
Evaluations across diverse domains --including face recognition (LFW,
Olivetti), brain connectivity analysis (ABIDE), and simulated data (galaxy
morphology, crystal structures)-- demonstrate that our method outperforms
baseline techniques, including PCA, t-SNE, UMAP, and tensor decomposition
baselines (CP and Tucker). Results show substantial improvements in clustering
metrics (Silhouette Score, Davies--Bouldin Index, Calinski--Harabasz Index,
Separation Ratio, Adjusted Rand Index, Normalized Mutual Information) and
reveal a fundamental trade-off: while metric learning optimizes global class
separation, it deliberately transforms local geometry to align with semantic
relationships. Crucially, our approach achieves superior performance with
smaller training datasets compared to transformer-based methods, offering an
efficient alternative for domains with limited labeled data. This work
establishes metric learning as a paradigm for tensor-based analysis,
prioritizing semantic relevance over pixel-level fidelity while providing
computational advantages in data-scarce scenarios.

</details>


### [278] [A Comparative Analysis of LLM Adaptation: SFT, LoRA, and ICL in Data-Scarce Scenarios](https://arxiv.org/abs/2511.00130)
*Bernd Bohnet,Rumen Dangovski,Kevin Swersky,Sherry Moore,Arslan Chaudhry,Kathleen Kenealy,Noah Fiedel*

Main category: cs.LG

TL;DR: 比较SFT、LoRA和ICL三种LLM适应方法在数据稀缺场景下的表现，发现LoRA在平衡新技能学习和保持通用知识方面最有效


<details>
  <summary>Details</summary>
Motivation: LLM需要针对特定应用进行定制，但完全微调计算成本高且可能导致灾难性遗忘，需要寻找更优的适应策略

Method: 在数据稀缺场景下对监督微调(SFT)、低秩适应(LoRA)和上下文学习(ICL)进行对比分析

Result: LoRA在平衡新技能学习和保持通用知识方面表现最佳；SFT擅长技能获取但易发生灾难性遗忘；ICL适合整合事实知识但难以处理复杂技能

Conclusion: LoRA提供了最有效的平衡，成功注入新技能同时对基础模型的通用知识影响最小，为LLM适应策略选择提供了实用框架

Abstract: The remarkable capabilities of Large Language Models (LLMs) often need to be
tailored for specific applications, requiring the integration of new knowledge
or the acquisition of new skills. While full fine-tuning is a powerful
adaptation method, it is computationally expensive and can lead to a
degradation of general reasoning abilities, a phenomenon known as catastrophic
forgetting. A range of alternative techniques exists, each with its own
trade-offs. In-Context Learning (ICL) is fast but limited by context length,
while Parameter-Efficient Fine-Tuning (PEFT) methods like Low-Rank Adaptation
(LoRA) offer a middle ground by minimizing parameter changes. However, the
challenge of catastrophic forgetting persists, raising questions about the best
adaptation strategy for a given task. This paper presents a comparative
analysis of Supervised Finetuning (SFT), LoRA, and ICL in data-scarce
scenarios. We find that LoRA provides the most effective balance, successfully
instilling new skills with minimal impact on the base model's general
knowledge. In contrast, while SFT excels at skill acquisition, it is highly
susceptible to catastrophic forgetting. ICL is effective for incorporating
factual knowledge but struggles with complex skills. Our findings offer a
practical framework for selecting an LLM adaptation strategy. We highlight the
critical distinction between skill acquisition and knowledge integration,
clarify the trade-offs between task-specific performance and the preservation
of general capabilities.

</details>


### [279] [Feature Importance Guided Random Forest Learning with Simulated Annealing Based Hyperparameter Tuning](https://arxiv.org/abs/2511.00133)
*Kowshik Balasubramanian,Andre Williams,Ismail Butun*

Main category: cs.LG

TL;DR: 提出了一种增强随机森林分类器的新框架，通过概率特征采样和模拟退火超参数调优，显著提升预测准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统随机森林在处理复杂分类问题时存在局限性，需要更好地捕捉数据中的相关信号并实现自适应超参数配置。

Method: 结合概率特征采样和模拟退火超参数调优，强调捕捉对分类更有意义的特征，并进行动态参数优化。

Result: 在多个领域（信用风险评估、物联网异常检测、早期医疗诊断、高维生物数据分析）中实现了准确率的持续提升，并提供了有意义的特征相关性洞察。

Conclusion: 结合重要性感知采样和元启发式优化的方法在提升随机森林性能方面具有显著效果，为复杂分类问题提供了有效解决方案。

Abstract: This paper introduces a novel framework for enhancing Random Forest
classifiers by integrating probabilistic feature sampling and hyperparameter
tuning via Simulated Annealing. The proposed framework exhibits substantial
advancements in predictive accuracy and generalization, adeptly tackling the
multifaceted challenges of robust classification across diverse domains,
including credit risk evaluation, anomaly detection in IoT ecosystems,
early-stage medical diagnostics, and high-dimensional biological data analysis.
To overcome the limitations of conventional Random Forests, we present an
approach that places stronger emphasis on capturing the most relevant signals
from data while enabling adaptive hyperparameter configuration. The model is
guided towards features that contribute more meaningfully to classification and
optimizing this with dynamic parameter tuning. The results demonstrate
consistent accuracy improvements and meaningful insights into feature
relevance, showcasing the efficacy of combining importance aware sampling and
metaheuristic optimization.

</details>


### [280] [Physiologically Active Vegetation Reverses Its Cooling Effect in Humid Urban Climates](https://arxiv.org/abs/2511.00134)
*Angana Borah,Adrija Datta,Ashish S. Kumar,Raviraj Dave,Udit Bhatia*

Main category: cs.LG

TL;DR: 该研究量化了植被结构和功能对印度138个城市热指数的影响，发现植被在特定条件下会逆转冷却效应并加剧热应激。


<details>
  <summary>Details</summary>
Motivation: 城市绿化降温效果不均，因为植被在冷却表面的同时可能加剧空气的湿热感，目前对植被如何调节这种冷却与湿度积累之间的权衡关系了解不足。

Method: 使用极端感知的1公里热指数重建和可解释机器学习框架（SHAP和ALE），分析138个印度城市在不同气候区和城市密度下的植被-气候相互作用。

Result: 当EVI≥0.4、LAI≥0.05时冷却效果增强，但当EVI≥0.5、LAI≥0.2、fPAR≥0.5时开始逆转为增温，在潮湿密集核心区fPAR≥0.25时更早出现逆转。

Conclusion: 研究确定了植被驱动冷却的气候限制，为气候特异性绿化策略提供了量化阈值，以促进公平和热韧性城市发展。

Abstract: Efforts to green cities for cooling are succeeding unevenly because the same
vegetation that cools surfaces can also intensify how hot the air feels.
Previous studies have identified humid heat as a growing urban hazard, yet how
physiologically active vegetation governs this trade-off between cooling and
moisture accumulation remains poorly understood, leaving mitigation policy and
design largely unguided. Here we quantify how vegetation structure and function
influence the Heat Index (HI), a combined measure of temperature and humidity
in 138 Indian cities spanning tropical savanna, semi-arid steppe, and humid
subtropical climates, and across dense urban cores and semi-urban rings. Using
an extreme-aware, one kilometre reconstruction of HI and an interpretable
machine-learning framework that integrates SHapley Additive Explanations (SHAP)
and Accumulated Local Effects (ALE), we isolate vegetation-climate
interactions. Cooling generally strengthens for EVI >= 0.4 and LAI >= 0.05, but
joint-high regimes begin to reverse toward warming when EVI >= 0.5, LAI >= 0.2,
and fPAR >= 0.5,with an earlier onset for fPAR >= 0.25 in humid, dense cores.
In such environments, highly physiologically active vegetation elevates
near-surface humidity faster than it removes heat, reversing its cooling effect
and amplifying perceived heat stress. These findings establish the climatic
limits of vegetation-driven cooling and provide quantitative thresholds for
climate-specific greening strategies that promote equitable and heat-resilient
cities.

</details>


### [281] [A Dual Large Language Models Architecture with Herald Guided Prompts for Parallel Fine Grained Traffic Signal Control](https://arxiv.org/abs/2511.00136)
*Qing Guo,Xinhang Li,Junyu Chen,Zheng Guo,Xiaocong Li,Lin Zhang,Lei Li*

Main category: cs.LG

TL;DR: HeraldLight是一个基于双LLM架构的交通信号控制系统，通过Herald引导提示增强，解决了现有LLM方法固定信号时长和幻觉错误的问题，以及RL方法缺乏鲁棒性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的交通信号控制方法存在固定信号时长和幻觉错误的限制，而RL方法在信号时序决策上缺乏鲁棒性且泛化能力差。需要一种能结合两者优势的新方法。

Method: 提出HeraldLight双LLM架构：Herald模块提取上下文信息并预测各交通相位的队列长度；LLM-Agent基于预测进行细粒度交通信号控制；LLM-Critic精炼LLM-Agent的输出，纠正错误和幻觉；通过基于分数的微调提高准确性和鲁棒性。

Result: 在CityFlow模拟实验中，使用涵盖济南(12)、杭州(16)和纽约(196)共224个交叉口的真实世界数据集，HeraldLight优于现有最优基线，在所有场景中平均旅行时间减少20.03%，在济南和杭州场景中平均队列长度减少10.74%。

Conclusion: HeraldLight通过双LLM架构和Herald引导提示，有效解决了现有方法的局限性，在交通信号控制中实现了更好的性能和鲁棒性。

Abstract: Leveraging large language models (LLMs) in traffic signal control (TSC)
improves optimization efficiency and interpretability compared to traditional
reinforcement learning (RL) methods. However, existing LLM-based approaches are
limited by fixed time signal durations and are prone to hallucination errors,
while RL methods lack robustness in signal timing decisions and suffer from
poor generalization. To address these challenges, this paper proposes
HeraldLight, a dual LLMs architecture enhanced by Herald guided prompts. The
Herald Module extracts contextual information and forecasts queue lengths for
each traffic phase based on real-time conditions. The first LLM, LLM-Agent,
uses these forecasts to make fine grained traffic signal control, while the
second LLM, LLM-Critic, refines LLM-Agent's outputs, correcting errors and
hallucinations. These refined outputs are used for score-based fine-tuning to
improve accuracy and robustness. Simulation experiments using CityFlow on real
world datasets covering 224 intersections in Jinan (12), Hangzhou (16), and New
York (196) demonstrate that HeraldLight outperforms state of the art baselines,
achieving a 20.03% reduction in average travel time across all scenarios and a
10.74% reduction in average queue length on the Jinan and Hangzhou scenarios.
The source code is available on GitHub:
https://github.com/BUPT-ANTlab/HeraldLight.

</details>


### [282] [Study on Supply Chain Finance Decision-Making Model and Enterprise Economic Performance Prediction Based on Deep Reinforcement Learning](https://arxiv.org/abs/2511.00166)
*Shiman Zhang,Jinghan Zhou,Zhoufan Yu,Ningai Leng*

Main category: cs.LG

TL;DR: 提出了一种融合深度学习与智能粒子群优化的决策模型，用于提高后端集中式冗余供应链的决策和规划效率。


<details>
  <summary>Details</summary>
Motivation: 提高后端集中式冗余供应链的决策和规划效率。

Method: 构建分布式节点部署模型和最优规划路径，使用卷积神经网络从历史数据中提取特征，线性规划捕捉高阶统计特征，采用模糊关联规则调度和深度强化学习优化模型，神经网络拟合动态变化，采用"深度学习特征提取-智能粒子群优化"混合机制进行全局优化和自适应控制。

Result: 仿真显示减少了资源消耗，增强了空间规划能力，在动态环境中改善了实时决策调整、配送路径优化和鲁棒智能控制。

Conclusion: 该集成模型有效提升了供应链决策效率和规划性能，在动态环境下具有良好的适应性和鲁棒性。

Abstract: To improve decision-making and planning efficiency in back-end centralized
redundant supply chains, this paper proposes a decision model integrating deep
learning with intelligent particle swarm optimization. A distributed node
deployment model and optimal planning path are constructed for the supply chain
network. Deep learning such as convolutional neural networks extracts features
from historical data, and linear programming captures high-order statistical
features. The model is optimized using fuzzy association rule scheduling and
deep reinforcement learning, while neural networks fit dynamic changes. A
hybrid mechanism of "deep learning feature extraction - intelligent particle
swarm optimization" guides global optimization and selects optimal decisions
for adaptive control. Simulations show reduced resource consumption, enhanced
spatial planning, and in dynamic environments improved real-time decision
adjustment, distribution path optimization, and robust intelligent control.

</details>


### [283] [Can SAEs reveal and mitigate racial biases of LLMs in healthcare?](https://arxiv.org/abs/2511.00177)
*Hiba Ahsan,Byron C. Wallace*

Main category: cs.LG

TL;DR: 该研究使用稀疏自编码器(SAE)来检测和控制Gemma-2模型中种族与负面概念之间的关联，发现SAE可以识别与黑人个体相关的潜在变量，但通过SAE引导来减轻偏见在复杂临床任务中效果有限。


<details>
  <summary>Details</summary>
Motivation: LLMs在医疗领域的应用存在加剧现有偏见的风险，特别是模型可能错误地依赖患者种族信息进行预测，需要开发方法来检测和控制这种偏见。

Method: 使用稀疏自编码器(SAE)识别Gemma-2模型中与黑人个体相关的潜在变量，并通过激活这些潜在变量来引导模型输出，评估其对偏见的影响。

Result: 发现SAE可以识别与黑人相关的潜在变量，激活这些变量会增加模型对黑人患者产生负面关联的风险(如"好斗"概率)，但在复杂临床任务中通过SAE引导减轻偏见的效果有限。

Conclusion: SAE在临床应用中可作为识别LLMs对人口统计信息问题性依赖的有用工具，但通过SAE引导来减轻偏见在现实任务中的实用性有限。

Abstract: LLMs are increasingly being used in healthcare. This promises to free
physicians from drudgery, enabling better care to be delivered at scale. But
the use of LLMs in this space also brings risks; for example, such models may
worsen existing biases. How can we spot when LLMs are (spuriously) relying on
patient race to inform predictions? In this work we assess the degree to which
Sparse Autoencoders (SAEs) can reveal (and control) associations the model has
made between race and stigmatizing concepts. We first identify SAE latents in
Gemma-2 models which appear to correlate with Black individuals. We find that
this latent activates on reasonable input sequences (e.g., "African American")
but also problematic words like "incarceration". We then show that we can use
this latent to steer models to generate outputs about Black patients, and
further that this can induce problematic associations in model outputs as a
result. For example, activating the Black latent increases the risk assigned to
the probability that a patient will become "belligerent". We evaluate the
degree to which such steering via latents might be useful for mitigating bias.
We find that this offers improvements in simple settings, but is less
successful for more realistic and complex clinical tasks. Overall, our results
suggest that: SAEs may offer a useful tool in clinical applications of LLMs to
identify problematic reliance on demographics but mitigating bias via SAE
steering appears to be of marginal utility for realistic tasks.

</details>


### [284] [PDE-SHARP: PDE Solver Hybrids Through Analysis & Refinement Passes](https://arxiv.org/abs/2511.00183)
*Shaghayegh Fazliani,Madeleine Udell*

Main category: cs.LG

TL;DR: PDE-SHARP是一个通过用更便宜的LLM推理替代昂贵的科学计算来降低PDE求解器生成成本的框架，能减少60-75%的计算评估，同时获得更高的求解器精度。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的PDE求解器生成方法需要执行大量求解器样本来识别高精度求解器，对于复杂PDE尤其计算成本高昂。

Method: 采用三阶段方法：(1)分析阶段：数学思维链分析，包括PDE分类、解类型检测和稳定性分析；(2)生成阶段：基于数学洞察生成求解器；(3)合成阶段：通过协作选择-混合锦标赛，LLM评委基于灵活性能反馈迭代优化实现。

Result: 平均只需少于13次求解器评估（基线方法需30+次），在测试PDE上平均精度提高4倍，在不同LLM架构上均表现出稳健性能。

Conclusion: PDE-SHARP能显著降低计算成本，同时提高求解器精度，适用于从通用到专用推理模型的各种LLM架构。

Abstract: Current LLM-driven approaches using test-time computing to generate PDE
solvers execute a large number of solver samples to identify high-accuracy
solvers. These paradigms are especially costly for complex PDEs requiring
substantial computational resources for numerical evaluation. We introduce
PDE-SHARP, a framework to reduce computational costs by replacing expensive
scientific computation by cheaper LLM inference that achieves superior solver
accuracy with 60-75% fewer computational evaluations. PDE-SHARP employs three
stages: (1) Analysis: mathematical chain-of-thought analysis including PDE
classification, solution type detection, and stability analysis; (2) Genesis:
solver generation based on mathematical insights from the previous stage; and
(3) Synthesis: collaborative selection-hybridization tournaments in which LLM
judges iteratively refine implementations through flexible performance
feedback. To generate high-quality solvers, PDE-SHARP requires fewer than 13
solver evaluations on average compared to 30+ for baseline methods, improving
accuracy uniformly across tested PDEs by $4\times$ on average, and demonstrates
robust performance across LLM architectures, from general-purpose to
specialized reasoning models.

</details>


### [285] [EL-MIA: Quantifying Membership Inference Risks of Sensitive Entities in LLMs](https://arxiv.org/abs/2511.00192)
*Ali Satvaty,Suzan Verberne,Fatih Turkmen*

Main category: cs.LG

TL;DR: 提出了EL-MIA框架，用于审计LLM中实体级别的成员推断风险，重点关注敏感信息如PII和信用卡号等。


<details>
  <summary>Details</summary>
Motivation: 现有MIA方法只能检测整个提示或文档是否在训练数据中，无法捕捉更细粒度的风险，特别是在实体级别上对敏感信息的成员推断。

Method: 构建了评估MIA方法的基准数据集，系统比较了现有MIA技术和两种新提出的方法，分析了实体级别MIA易感性与模型规模、训练轮次等因素的关系。

Result: 发现现有MIA方法在敏感属性实体级别成员推断方面存在局限，但这种易感性可以通过相对简单的方法来识别。

Conclusion: 需要更强的对抗方法来对提供的威胁模型进行压力测试，以更好地评估LLM的隐私风险。

Abstract: Membership inference attacks (MIA) aim to infer whether a particular data
point is part of the training dataset of a model. In this paper, we propose a
new task in the context of LLM privacy: entity-level discovery of membership
risk focused on sensitive information (PII, credit card numbers, etc). Existing
methods for MIA can detect the presence of entire prompts or documents in the
LLM training data, but they fail to capture risks at a finer granularity. We
propose the ``EL-MIA'' framework for auditing entity-level membership risks in
LLMs. We construct a benchmark dataset for the evaluation of MIA methods on
this task. Using this benchmark, we conduct a systematic comparison of existing
MIA techniques as well as two newly proposed methods. We provide a
comprehensive analysis of the results, trying to explain the relation of the
entity level MIA susceptability with the model scale, training epochs, and
other surface level factors. Our findings reveal that existing MIA methods are
limited when it comes to entity-level membership inference of the sensitive
attributes, while this susceptibility can be outlined with relatively
straightforward methods, highlighting the need for stronger adversaries to
stress test the provided threat model.

</details>


### [286] [Diffusion Models at the Drug Discovery Frontier: A Review on Generating Small Molecules versus Therapeutic Peptides](https://arxiv.org/abs/2511.00209)
*Yiquan Wang,Yahui Ma,Yuhan Chang,Jiayao Yan,Jialin Zhang,Minnuo Cai,Kai Wei*

Main category: cs.LG

TL;DR: 扩散模型在药物发现中应用于小分子和肽类药物的设计，通过统一的去噪框架适应不同分子表示和设计目标，面临合成可行性、生物稳定性等挑战，未来需整合到自动化DBTL平台。


<details>
  <summary>Details</summary>
Motivation: 药物发现过程缓慢且昂贵，扩散模型作为领先的生成框架有望加速这一过程，需要系统比较其在两种主要治疗模式（小分子和肽类药物）中的应用。

Method: 使用迭代去噪的统一框架，针对小分子和肽类药物的不同分子表示、化学空间和设计目标进行适应性调整。

Result: 扩散模型在小分子设计中擅长基于结构的设计，生成新颖的配体；在肽类药物设计中专注于功能序列生成和从头结构设计。两者都面临评分函数准确性、高质量数据稀缺和实验验证等共同挑战。

Conclusion: 扩散模型的全部潜力需要通过弥合模式特定差距并整合到自动化DBTL平台中来释放，从而将范式从化学探索转向靶向创建新型治疗药物。

Abstract: Diffusion models have emerged as a leading framework in generative modeling,
showing significant potential to accelerate and transform the traditionally
slow and costly process of drug discovery. This review provides a systematic
comparison of their application in designing two principal therapeutic
modalities: small molecules and therapeutic peptides. We analyze how a unified
framework of iterative denoising is adapted to the distinct molecular
representations, chemical spaces, and design objectives of each modality. For
small molecules, these models excel at structure-based design, generating
novel, pocket-fitting ligands with desired physicochemical properties, yet face
the critical hurdle of ensuring chemical synthesizability. Conversely, for
therapeutic peptides, the focus shifts to generating functional sequences and
designing de novo structures, where the primary challenges are achieving
biological stability against proteolysis, ensuring proper folding, and
minimizing immunogenicity. Despite these distinct challenges, both domains face
shared hurdles: the need for more accurate scoring functions, the scarcity of
high-quality experimental data, and the crucial requirement for experimental
validation. We conclude that the full potential of diffusion models will be
unlocked by bridging these modality-specific gaps and integrating them into
automated, closed-loop Design-Build-Test-Learn (DBTL) platforms, thereby
shifting the paradigm from chemical exploration to the targeted creation of
novel therapeutics.

</details>


### [287] [Iterative Foundation Model Fine-Tuning on Multiple Rewards](https://arxiv.org/abs/2511.00220)
*Pouya M. Ghari,Simone Sciabola,Ye Wang*

Main category: cs.LG

TL;DR: 提出了一种基于多奖励信号的强化学习方法来微调基础模型，通过跨奖励的迭代微调策略，泛化了最先进的基于RL的方法，并在文本、生物序列和小分子生成等多个领域验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 在许多应用中，如文本生成和药物发现，使用单一奖励信号进行优化可能不够理想，因为通常需要多个评估标准。

Method: 采用基于多奖励信号的强化学习方法，通过跨奖励的迭代微调策略来微调基础模型。

Result: 在文本、生物序列和小分子生成等多个领域的实验结果表明，所提出的算法相比最先进的基线方法具有更好的效果。

Conclusion: 多奖励RL微调方法在多个领域都表现出有效性，并通过理论分析提供了对性能的深入理解。

Abstract: Fine-tuning foundation models has emerged as a powerful approach for
generating objects with specific desired properties. Reinforcement learning
(RL) provides an effective framework for this purpose, enabling models to
generate outputs that maximize a given reward function. However, in many
applications such as text generation and drug discovery, it can be suboptimal
to optimize using a single reward signal, as multiple evaluation criteria are
often necessary. This paper proposes a novel reinforcement learning-based
method for fine-tuning foundation models using multiple reward signals. By
employing an iterative fine-tuning strategy across these rewards, our approach
generalizes state-of-the-art RL-based methods. We further provide a theoretical
analysis that offers insights into the performance of multi-reward RL
fine-tuning. Experimental results across diverse domains including text,
biological sequence, and small molecule generation, demonstrate the
effectiveness of the proposed algorithm compared to state-of-the-art baselines.

</details>


### [288] [Melanoma Classification Through Deep Ensemble Learning and Explainable AI](https://arxiv.org/abs/2511.00246)
*Wadduwage Shanika Perera,ABM Islam,Van Vung Pham,Min Kyung An*

Main category: cs.LG

TL;DR: 提出了一种结合集成学习和可解释人工智能（XAI）的机器学习模型，用于提高黑色素瘤早期检测的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在医疗诊断中缺乏可解释性的问题，增强对AI系统预测结果的信任和可靠性。

Method: 使用三种最先进的深度迁移学习网络进行集成学习，并应用XAI技术来解释预测依据。

Result: 能够以高准确度检测黑色素瘤病变，同时通过XAI技术提供预测的可解释性。

Conclusion: 集成学习结合XAI技术可以有效提高黑色素瘤检测的准确性和可靠性，为医疗诊断提供更可信的AI辅助工具。

Abstract: Melanoma is one of the most aggressive and deadliest skin cancers, leading to
mortality if not detected and treated in the early stages. Artificial
intelligence techniques have recently been developed to help dermatologists in
the early detection of melanoma, and systems based on deep learning (DL) have
been able to detect these lesions with high accuracy. However, the entire
community must overcome the explainability limit to get the maximum benefit
from DL for diagnostics in the healthcare domain. Because of the black box
operation's shortcomings in DL models' decisions, there is a lack of
reliability and trust in the outcomes. However, Explainable Artificial
Intelligence (XAI) can solve this problem by interpreting the predictions of AI
systems. This paper proposes a machine learning model using ensemble learning
of three state-of-the-art deep transfer Learning networks, along with an
approach to ensure the reliability of the predictions by utilizing XAI
techniques to explain the basis of the predictions.

</details>


### [289] [X-TRACK: Physics-Aware xLSTM for Realistic Vehicle Trajectory Prediction](https://arxiv.org/abs/2511.00266)
*Aanchal Rajesh Chugh,Marion Neumeier,Sebastian Dorn*

Main category: cs.LG

TL;DR: 本文提出了基于xLSTM的车辆轨迹预测框架X-TRAJ及其物理感知变体X-TRACK，通过整合车辆运动学约束生成更真实可行的轨迹。


<details>
  <summary>Details</summary>
Motivation: 尽管xLSTM在时间序列预测中表现出色，但在车辆轨迹预测领域尚未得到充分探索。传统LSTM存在长期依赖建模的局限性，而xLSTM通过指数门控和增强内存结构解决了这些问题。

Method: 开发了X-TRAJ框架及其物理感知变体X-TRACK，后者将车辆运动学约束显式整合到模型学习过程中，确保生成的轨迹符合物理规律。

Result: 在highD和NGSIM数据集上的综合评估表明，X-TRACK优于现有的最先进基线方法。

Conclusion: xLSTM架构结合物理约束能够有效提升车辆轨迹预测的准确性和可行性，为智能交通系统提供了更可靠的预测解决方案。

Abstract: Recent advancements in Recurrent Neural Network (RNN) architectures,
particularly the Extended Long Short Term Memory (xLSTM), have addressed the
limitations of traditional Long Short Term Memory (LSTM) networks by
introducing exponential gating and enhanced memory structures. These
improvements make xLSTM suitable for time-series prediction tasks as they
exhibit the ability to model long-term temporal dependencies better than LSTMs.
Despite their potential, these xLSTM-based models remain largely unexplored in
the context of vehicle trajectory prediction. Therefore, this paper introduces
a novel xLSTM-based vehicle trajectory prediction framework, X-TRAJ, and its
physics-aware variant, X-TRACK (eXtended LSTM for TRAjectory prediction
Constraint by Kinematics), which explicitly integrates vehicle motion
kinematics into the model learning process. By introducing physical
constraints, the proposed model generates realistic and feasible trajectories.
A comprehensive evaluation on the highD and NGSIM datasets demonstrates that
X-TRACK outperforms state-of-the-art baselines.

</details>


### [290] [Improving the Robustness of Control of Chaotic Convective Flows with Domain-Informed Reinforcement Learning](https://arxiv.org/abs/2511.00272)
*Michiel Straat,Thorben Markmann,Sebastian Peitz,Barbara Hammer*

Main category: cs.LG

TL;DR: 本文提出了一种基于领域知识的强化学习方法，用于控制混沌对流流动，在Rayleigh-Bénard对流系统中实现了热传输的显著减少。


<details>
  <summary>Details</summary>
Motivation: 混沌对流流动在微流体设备和化学反应器等实际系统中普遍存在，但传统控制方法在混沌状态下往往失效。虽然强化学习在层流控制中表现出潜力，但在混沌和湍流动态下的泛化能力和鲁棒性尚未充分探索。

Method: 使用领域知识增强的强化学习代理，通过近端策略优化在不同初始条件和流态下进行训练。在奖励函数中引入鼓励Bénard单元合并的领域知识项，作为期望的宏观特性。

Result: 在层流状态下，领域知识增强的强化学习代理将热传输减少高达33%；在混沌流态下仍能实现10%的减少，显著优于传统控制器。领域知识奖励设计产生了稳定流动、训练收敛更快，并且无需重新训练即可跨流态泛化。

Conclusion: 优雅的领域知识先验可以显著增强基于强化学习的混沌流动控制的鲁棒性，使实际部署更接近现实。

Abstract: Chaotic convective flows arise in many real-world systems, such as
microfluidic devices and chemical reactors. Stabilizing these flows is highly
desirable but remains challenging, particularly in chaotic regimes where
conventional control methods often fail. Reinforcement Learning (RL) has shown
promise for control in laminar flow settings, but its ability to generalize and
remain robust under chaotic and turbulent dynamics is not well explored,
despite being critical for real-world deployment. In this work, we improve the
practical feasibility of RL-based control of such flows focusing on
Rayleigh-B\'enard Convection (RBC), a canonical model for convective heat
transport. To enhance generalization and sample efficiency, we introduce
domain-informed RL agents that are trained using Proximal Policy Optimization
across diverse initial conditions and flow regimes. We incorporate domain
knowledge in the reward function via a term that encourages B\'enard cell
merging, as an example of a desirable macroscopic property. In laminar flow
regimes, the domain-informed RL agents reduce convective heat transport by up
to 33%, and in chaotic flow regimes, they still achieve a 10% reduction, which
is significantly better than the conventional controllers used in practice. We
compare the domain-informed to uninformed agents: Our results show that the
domain-informed reward design results in steady flows, faster convergence
during training, and generalization across flow regimes without retraining. Our
work demonstrates that elegant domain-informed priors can greatly enhance the
robustness of RL-based control of chaotic flows, bringing real-world deployment
closer.

</details>


### [291] [Calibration Across Layers: Understanding Calibration Evolution in LLMs](https://arxiv.org/abs/2511.00280)
*Abhinav Joshi,Areeb Ahmad,Ashutosh Modi*

Main category: cs.LG

TL;DR: 研究发现LLMs的校准能力在深层网络中存在主动修正阶段，通过扰动残差流中的低维校准方向可显著改善校准指标而不影响准确性，表明校准是分布式现象。


<details>
  <summary>Details</summary>
Motivation: 尽管先前研究发现深度神经网络通常过度自信，但LLMs表现出内在的校准能力。本研究旨在探索校准如何在网络深度中演化，提供与最终层组件研究互补的视角。

Method: 分析多个开源权重模型在MMLU基准上的表现，研究校准在网络深度中的演化，识别残差流中的低维校准方向并进行扰动实验。

Result: 发现上层/深层存在明显的置信度修正阶段，模型在达到决策确定性后会主动重新校准置信度。扰动低维校准方向可显著改善ECE和MCE指标而不损害准确性。

Conclusion: 校准是分布式现象，在整个网络前向传播过程中形成，而不仅仅在最终投影层，这为理解LLMs中置信度调节机制提供了新见解。

Abstract: Large Language Models (LLMs) have demonstrated inherent calibration
capabilities, where predicted probabilities align well with correctness,
despite prior findings that deep neural networks are often overconfident.
Recent studies have linked this behavior to specific components in the final
layer, such as entropy neurons and the unembedding matrix null space. In this
work, we provide a complementary perspective by investigating how calibration
evolves throughout the network depth. Analyzing multiple open-weight models on
the MMLU benchmark, we uncover a distinct confidence correction phase in the
upper/later layers, where model confidence is actively recalibrated after
decision certainty has been reached. Furthermore, we identify a low-dimensional
calibration direction in the residual stream whose perturbation significantly
improves calibration metrics (ECE and MCE) without harming accuracy. Our
findings suggest that calibration is a distributed phenomenon, shaped
throughout the network forward pass, not just in its final projection,
providing new insights into how confidence-regulating mechanisms operate within
LLMs.

</details>


### [292] [A systematic evaluation of uncertainty quantification techniques in deep learning: a case study in photoplethysmography signal analysis](https://arxiv.org/abs/2511.00301)
*Ciaran Bench,Oskar Pfeffer,Vivek Desai,Mohammad Moulaeifard,Loïc Coquelin,Peter H. Charlton,Nils Strodthoff,Nando Hegemann,Philip J. Aston,Andrew Thompson*

Main category: cs.LG

TL;DR: 本文系统比较了8种不确定性量化技术在医疗时间序列数据（如PPG传感器数据）上的表现，重点关注心房颤动检测和血压回归任务，强调评估标准应根据实际应用场景制定。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在医疗时间序列数据上的应用面临部署性能不佳的风险，可靠的不确定性估计可以为临床医生提供模型输出可信度的指导，因此需要比较不同不确定性量化方法的有效性。

Method: 实现了8种不确定性量化技术，应用于两个临床预测任务：心房颤动检测（分类）和两种血压回归变体，制定了全面的评估程序进行严格比较。

Result: 观察到不同技术间不确定性可靠性的复杂情况，最优方法取决于不确定性表达方式、评估指标和可靠性评估尺度。局部校准和适应性评估提供了全局可靠性指标无法获得的实用见解。

Conclusion: 评估不确定性量化技术的标准应适应模型的实际用例，在每位患者数据量较少的情况下，应优先考虑所选不确定性表达方式的小尺度可靠性，同时尽可能保持预测性能。

Abstract: In principle, deep learning models trained on medical time-series, including
wearable photoplethysmography (PPG) sensor data, can provide a means to
continuously monitor physiological parameters outside of clinical settings.
However, there is considerable risk of poor performance when deployed in
practical measurement scenarios leading to negative patient outcomes. Reliable
uncertainties accompanying predictions can provide guidance to clinicians in
their interpretation of the trustworthiness of model outputs. It is therefore
of interest to compare the effectiveness of different approaches. Here we
implement an unprecedented set of eight uncertainty quantification (UQ)
techniques to models trained on two clinically relevant prediction tasks:
Atrial Fibrillation (AF) detection (classification), and two variants of blood
pressure regression. We formulate a comprehensive evaluation procedure to
enable a rigorous comparison of these approaches. We observe a complex picture
of uncertainty reliability across the different techniques, where the most
optimal for a given task depends on the chosen expression of uncertainty,
evaluation metric, and scale of reliability assessed. We find that assessing
local calibration and adaptivity provides practically relevant insights about
model behaviour that otherwise cannot be acquired using more commonly
implemented global reliability metrics. We emphasise that criteria for
evaluating UQ techniques should cater to the model's practical use case, where
the use of a small number of measurements per patient places a premium on
achieving small-scale reliability for the chosen expression of uncertainty,
while preserving as much predictive performance as possible.

</details>


### [293] [Reject Only Critical Tokens: Pivot-Aware Speculative Decoding](https://arxiv.org/abs/2511.00351)
*Amir Ziashahabi,Yavuz Faruk Bakman,Duygu Nur Yaldiz,Mostafa El-Khamy,Sai Praneeth Karimireddy,Salman Avestimehr*

Main category: cs.LG

TL;DR: 本文提出了一种新的解码策略——Pivot-Aware Speculative Decoding，通过放宽传统推测解码的严格分布匹配要求，仅拒绝会导致最终输出效用下降的关键标记，从而显著提高接受率并获得2.5倍加速。


<details>
  <summary>Details</summary>
Motivation: 传统推测解码要求输出分布与目标模型完全匹配，这种严格限制导致接受率过低，限制了潜在加速效果。作者认为在实际应用中，任务特定性能（如代码正确性、事实准确性）比采样分布更重要。

Method: 提出Pivot-Aware Speculative Decoding策略，识别并仅拒绝会导致效用下降的关键标记（pivot tokens），训练轻量级分类器检测这些关键标记，作为标准推测解码的松弛版本。

Result: 在多个数据集上的评估表明，该方法能实现高达2.5倍的加速，同时保持可比较的效用性能。

Conclusion: 通过将解码目标重新定义为匹配期望效用而非严格分布匹配，可以显著提高推测解码的接受率和加速效果，同时保持任务性能。

Abstract: Speculative Decoding (SD) ensures that the output matches the target model's
distribution exactly. However, we argue that this distribution matching
requirement is too stringent and results in unnecessarily low acceptance rates,
limiting potential speedups. Instead, we advocate a reformulation of the
decoding objective: the proposed decoding strategy should match the expected
utility, i.e., the task-specific performance, of the target model. This
perspective also aligns better with real-world use cases of LLMs, where utility
(e.g., code correctness, factual accuracy) is often more important than
sampling distribution. Based on this reformulation, we propose a novel decoding
strategy: Pivot-Aware Speculative Decoding, which rejects only those tokens
that would lead to a utility drop in the final output. We refer to these
critical tokens as pivot tokens. We propose a method for labeling tokens as
pivotal or non-pivotal and train a lightweight classifier to detect them. This
method can be viewed as a relaxed version of standard SD, which offers much
higher acceptance while preserving utility. We evaluate our method across
various datasets, demonstrating that we can achieve up to $2.5\times$ speedup
with comparable utility. Source code is available at
https://github.com/amir-zsh/PAD.

</details>


### [294] [Balancing Interpretability and Performance in Motor Imagery EEG Classification: A Comparative Study of ANFIS-FBCSP-PSO and EEGNet](https://arxiv.org/abs/2511.00369)
*Farjana Aktar,Mohd Ruhul Ameen,Akif Islam,Md Ekramul Hamid*

Main category: cs.LG

TL;DR: 比较模糊推理方法(ANFIS-FBCSP-PSO)与深度学习基准(EEGNet)在运动想象EEG分类中的性能，发现模糊模型在个体内测试中表现更好，而深度学习模型在跨个体测试中泛化能力更强。


<details>
  <summary>Details</summary>
Motivation: 解决运动想象EEG分类中准确性和可解释性难以兼得的挑战，为BCI系统设计提供选择依据。

Method: 使用ANFIS模糊推理方法结合滤波器组共空间模式特征提取和粒子群优化，与EEGNet深度学习模型在BCI Competition IV-2a数据集上进行对比。

Result: 个体内测试：模糊模型准确率68.58%±13.76%，kappa=58.04%±18.43；跨个体测试：深度学习模型准确率68.20%±12.13%，kappa=57.33%±16.22。

Conclusion: 根据设计目标选择BCI系统：追求可解释性时选择模糊模型，追求跨用户鲁棒性时选择深度学习模型。未来研究应关注基于Transformer和混合神经符号框架的透明EEG解码方法。

Abstract: Achieving both accurate and interpretable classification of motor imagery EEG
remains a key challenge in brain computer interface (BCI) research. This paper
compares a transparent fuzzy reasoning approach (ANFIS-FBCSP-PSO) with a deep
learning benchmark (EEGNet) using the BCI Competition IV-2a dataset. The ANFIS
pipeline combines filter bank common spatial pattern feature extraction with
fuzzy IF-THEN rules optimized via particle swarm optimization, while EEGNet
learns hierarchical spatial temporal representations directly from raw EEG
data. In within-subject experiments, the fuzzy neural model performed better
(68.58 percent +/- 13.76 percent accuracy, kappa = 58.04 percent +/- 18.43),
while in cross-subject (LOSO) tests, the deep model exhibited stronger
generalization (68.20 percent +/- 12.13 percent accuracy, kappa = 57.33 percent
+/- 16.22). The study provides practical guidance for selecting MI-BCI systems
according to design goals: interpretability or robustness across users. Future
investigations into transformer based and hybrid neuro symbolic frameworks are
expected to advance transparent EEG decoding.

</details>


### [295] [PolyRecommender: A Multimodal Recommendation System for Polymer Discovery](https://arxiv.org/abs/2511.00375)
*Xin Wang,Yunhao Xiao,Rui Qiao*

Main category: cs.LG

TL;DR: PolyRecommender是一个多模态聚合物发现框架，结合了PolyBERT的化学语言表示和图编码器的分子图表示，通过多模态嵌入实现候选聚合物的检索和排序。


<details>
  <summary>Details</summary>
Motivation: 利用多模态知识互补的优势，实现高效的聚合物检索和跨相关属性的稳健排序，推进AI引导的下一代聚合物发现。

Method: 首先使用基于语言的相似性检索候选聚合物，然后使用融合的多模态嵌入根据多个目标属性进行排序。

Result: 建立了一个可推广的多模态范式，能够有效整合化学语言和分子图表示。

Conclusion: 该工作推进了AI引导的聚合物设计，为下一代聚合物的发现提供了有效的多模态方法。

Abstract: We introduce PolyRecommender, a multimodal discovery framework that
integrates chemical language representations from PolyBERT with molecular
graph-based representations from a graph encoder. The system first retrieves
candidate polymers using language-based similarity and then ranks them using
fused multimodal embeddings according to multiple target properties. By
leveraging the complementary knowledge encoded in both modalities,
PolyRecommender enables efficient retrieval and robust ranking across related
polymer properties. Our work establishes a generalizable multimodal paradigm,
advancing AI-guided design for the discovery of next-generation polymers.

</details>


### [296] [UME-R1: Exploring Reasoning-Driven Generative Multimodal Embeddings](https://arxiv.org/abs/2511.00405)
*Zhibin Lan,Liqiang Niu,Fandong Meng,Jie Zhou,Jinsong Su*

Main category: cs.LG

TL;DR: 提出了UME-R1生成式多模态嵌入框架，通过两阶段训练策略统一嵌入任务于生成范式，在MMEB-V2基准测试中显著优于传统判别式嵌入模型。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型的嵌入方法本质上是判别式的，限制了其从推理驱动的生成范式中获益的能力。

Method: 采用两阶段训练策略：冷启动监督微调赋予模型推理能力，使其能生成判别式和生成式嵌入；后续强化学习增强推理并优化生成式嵌入质量。

Result: 在MMEB-V2基准的78个任务中显著优于传统判别式嵌入模型，生成式嵌入通过利用MLLMs的强大生成推理能力实现性能大幅提升。

Conclusion: 生成式嵌入为更可解释、推理驱动的生成式多模态嵌入奠定了基础，展示了推理时扩展性潜力。

Abstract: The remarkable success of multimodal large language models (MLLMs) has driven
advances in multimodal embeddings, yet existing models remain inherently
discriminative, limiting their ability to benefit from reasoning-driven
generation paradigm. In this work, we pioneer the exploration of generative
embeddings, unifying embedding tasks within a generative paradigm. We propose
UME-R1, a universal multimodal embedding framework consisting of a two-stage
training strategy: a cold-start supervised fine-tuning equips the model with
reasoning capabilities and enables it to generate both discriminative and
generative embeddings; a subsequent reinforcement learning enhances reasoning
and further optimizes generative embedding quality. This pioneering work
reveals four key insights: 1) generative embeddings unlock substantial
performance gains over conventional discriminative embeddings by leveraging the
powerful generative reasoning capabilities of MLLMs; 2) discriminative and
generative embeddings are complementary, whose combined oracle performance far
exceeding that of either alone; 3) RL can effectively enhance generative
embeddings, establishing a scalable optimization paradigm.; 4) repeated
sampling at inference boosts downstream task coverage (pass@k), highlighting
the inference-time scalability potential of generative embeddings. Evaluated on
the MMEB-V2 benchmark across 78 tasks spanning video, image, and visual
documents, UME-R1 significantly outperforms conventional discriminative
embedding models and offers a foundation for more interpretable,
reasoning-driven generative multimodal embeddings. Our code, models, and
datasets will be publicly available at https://github.com/XMUDeepLIT/UME-R1.

</details>


### [297] [Enhancing Adversarial Transferability by Balancing Exploration and Exploitation with Gradient-Guided Sampling](https://arxiv.org/abs/2511.00411)
*Zenghao Niu,Weicheng Xie,Siyang Song,Zitong Yu,Feng Liu,Linlin Shen*

Main category: cs.LG

TL;DR: 提出了梯度引导采样(GGS)方法，解决对抗攻击在迁移场景中攻击强度与跨模型泛化能力之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 对抗攻击在跨模型迁移时面临攻击强度(Exploitation)和泛化能力(Exploration)的权衡困境。传统方法过度强调攻击强度而削弱泛化，新方法则相反。

Method: 基于MI-FGSM，引入内迭代随机采样，并使用前一次内迭代的梯度来引导采样方向，采样幅度由随机分布决定。

Result: 在多种DNN架构和多模态大语言模型上的实验表明，该方法在迁移攻击方面优于现有最先进方法。

Conclusion: GGS方法通过梯度引导采样有效平衡了攻击强度与跨模型泛化能力，解决了对抗攻击迁移性的基本困境。

Abstract: Adversarial attacks present a critical challenge to deep neural networks'
robustness, particularly in transfer scenarios across different model
architectures. However, the transferability of adversarial attacks faces a
fundamental dilemma between Exploitation (maximizing attack potency) and
Exploration (enhancing cross-model generalization). Traditional momentum-based
methods over-prioritize Exploitation, i.e., higher loss maxima for attack
potency but weakened generalization (narrow loss surface). Conversely, recent
methods with inner-iteration sampling over-prioritize Exploration, i.e.,
flatter loss surfaces for cross-model generalization but weakened attack
potency (suboptimal local maxima). To resolve this dilemma, we propose a simple
yet effective Gradient-Guided Sampling (GGS), which harmonizes both objectives
through guiding sampling along the gradient ascent direction to improve both
sampling efficiency and stability. Specifically, based on MI-FGSM, GGS
introduces inner-iteration random sampling and guides the sampling direction
using the gradient from the previous inner-iteration (the sampling's magnitude
is determined by a random distribution). This mechanism encourages adversarial
examples to reside in balanced regions with both flatness for cross-model
generalization and higher local maxima for strong attack potency. Comprehensive
experiments across multiple DNN architectures and multimodal large language
models (MLLMs) demonstrate the superiority of our method over state-of-the-art
transfer attacks. Code is made available at https://github.com/anuin-cat/GGS.

</details>


### [298] [Tree Training: Accelerating Agentic LLMs Training via Shared Prefix Reuse](https://arxiv.org/abs/2511.00413)
*Shaojie Wang,Jinghui Wang,Yinghan Cui,Xuxing Chen,Chao Wang,Liang Huang,Xiaojiang Zhang,Junyi Peng,Li Wan,Haotian Zhang,Bin Chen*

Main category: cs.LG

TL;DR: 提出Tree Training方法，通过树打包和梯度恢复技术，在智能体LLM训练中重用共享前缀计算，显著提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 当前训练流程将树状轨迹分解为独立线性段，导致共享前缀在前后向传播中重复计算，效率低下。

Method: 使用树打包技术重用轨迹间的共享计算，配合梯度恢复确保重用前缀的正确梯度传播。

Result: 在多个开源模型上实验，总训练时间最多减少3.9倍。

Conclusion: Tree Training能够显著提升智能体LLM的SFT和RL训练效率。

Abstract: In agentic LLM scenarios, an agent's interaction process during a single
rollout often exhibits branching behaviors. Due to memory retrieval and
concurrent tool executions at certain decision points, the token trajectory of
one task evolves into a tree-like structure rather than a linear sequence.
However, current training pipelines decompose such tree-structured trajectories
into separate linear segments, treating each branch as an independent sequence.
As a result, shared prefixes across these branches are repeatedly recomputed
during both forward and backward passes. To address this inefficiency, we
propose Tree Training, a paradigm that computes each shared prefix only once
and reuses its intermediate results across related branches during both forward
and backward passes, substantially improving computation efficiency in
large-scale agentic training. This is achieved via (i) Tree Packing, which
efficiently reuses shared computations across trajectories, and (ii) Gradient
Restoration, which ensures correct gradient propagation across reused prefixes.
Experiments on multiple open-source models demonstrate up to 3.9x reduction in
total training time, enabling more efficient agentic LLM SFT and RL training.

</details>


### [299] [Structure-Preserving Physics-Informed Neural Network for the Korteweg--de Vries (KdV) Equation](https://arxiv.org/abs/2511.00418)
*Victory Obieke,Emmanuel Oguadimma*

Main category: cs.LG

TL;DR: 该论文提出了一种结构保持的物理信息神经网络框架，用于求解非线性KdV方程，通过嵌入质量守恒和哈密顿能量守恒到损失函数中，确保物理一致性和能量稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统PINNs在长期积分中往往无法保持关键的物理不变量，特别是在处理非线性色散波传播问题时。

Method: 使用正弦激活函数增强谱表达能力，并将质量守恒和哈密顿能量守恒直接嵌入损失函数中，实现结构保持的优化。

Result: 模型成功再现了KdV动力学的典型行为（单孤子传播、双孤子相互作用等），同时保持了守恒不变量，收敛更快且长期稳定性更好。

Conclusion: 结合不变量约束优化和正弦特征映射的计算高效方法，为哈密顿偏微分方程如KdV方程提供了鲁棒且能量一致的PINNs。

Abstract: Physics-Informed Neural Networks (PINNs) offer a flexible framework for
solving nonlinear partial differential equations (PDEs), yet conventional
implementations often fail to preserve key physical invariants during long-term
integration. This paper introduces a \emph{structure-preserving PINN} framework
for the nonlinear Korteweg--de Vries (KdV) equation, a prototypical model for
nonlinear and dispersive wave propagation. The proposed method embeds the
conservation of mass and Hamiltonian energy directly into the loss function,
ensuring physically consistent and energy-stable evolution throughout training
and prediction. Unlike standard \texttt{tanh}-based
PINNs~\cite{raissi2019pinn,wang2022modifiedpinn}, our approach employs
sinusoidal activation functions that enhance spectral expressiveness and
accurately capture the oscillatory and dispersive nature of KdV solitons.
Through representative case studies -- including single-soliton propagation
(shape-preserving translation), two-soliton interaction (elastic collision with
phase shift), and cosine-pulse initialization (nonlinear dispersive breakup) --
the model successfully reproduces hallmark behaviors of KdV dynamics while
maintaining conserved invariants. Ablation studies demonstrate that combining
invariant-constrained optimization with sinusoidal feature mappings accelerates
convergence, improves long-term stability, and mitigates drift without
multi-stage pretraining. These results highlight that computationally
efficient, invariant-aware regularization coupled with sinusoidal
representations yields robust, energy-consistent PINNs for Hamiltonian partial
differential equations such as the KdV equation.

</details>


### [300] [Bootstrap Off-policy with World Model](https://arxiv.org/abs/2511.00423)
*Guojian Zhan,Likun Wang,Xiangteng Zhang,Jiaxin Gao,Masayoshi Tomizuka,Shengbo Eben Li*

Main category: cs.LG

TL;DR: BOOM是一个将规划与离策略学习紧密结合的强化学习框架，通过引导循环和联合学习的世界模型解决规划导致的数据与策略行为不一致问题。


<details>
  <summary>Details</summary>
Motivation: 在线规划虽然能提高强化学习的样本效率和最终性能，但会导致收集的数据与策略实际行为不一致，从而影响模型学习和策略改进。

Method: 提出BOOM框架，通过引导循环将策略初始化规划器，规划器通过行为对齐引导策略；使用联合学习的世界模型模拟未来轨迹并提供价值目标；核心是使用无似然对齐损失和软价值加权机制。

Result: 在DeepMind Control Suite和Humanoid-Bench等高维环境中，BOOM在训练稳定性和最终性能方面达到了最先进的结果。

Conclusion: BOOM通过紧密整合规划和离策略学习，有效解决了规划导致的数据不一致问题，在复杂环境中表现出优异的性能。

Abstract: Online planning has proven effective in reinforcement learning (RL) for
improving sample efficiency and final performance. However, using planning for
environment interaction inevitably introduces a divergence between the
collected data and the policy's actual behaviors, degrading both model learning
and policy improvement. To address this, we propose BOOM (Bootstrap Off-policy
with WOrld Model), a framework that tightly integrates planning and off-policy
learning through a bootstrap loop: the policy initializes the planner, and the
planner refines actions to bootstrap the policy through behavior alignment.
This loop is supported by a jointly learned world model, which enables the
planner to simulate future trajectories and provides value targets to
facilitate policy improvement. The core of BOOM is a likelihood-free alignment
loss that bootstraps the policy using the planner's non-parametric action
distribution, combined with a soft value-weighted mechanism that prioritizes
high-return behaviors and mitigates variability in the planner's action quality
within the replay buffer. Experiments on the high-dimensional DeepMind Control
Suite and Humanoid-Bench show that BOOM achieves state-of-the-art results in
both training stability and final performance. The code is accessible at
https://github.com/molumitu/BOOM_MBRL.

</details>


### [301] [Region-Aware Reconstruction Strategy for Pre-training fMRI Foundation Model](https://arxiv.org/abs/2511.00443)
*Ruthwik Reddy Doodipala,Pankaj Pandey,Carolina Torres Rojas,Manob Jyoti Saikia,Ranganatha Sitaram*

Main category: cs.LG

TL;DR: 提出了一种基于ROI引导掩码策略的静息态fMRI基础模型，相比传统随机掩码方法在ADHD分类任务上准确率提升4.23%，并增强了模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有神经影像基础模型主要依赖随机区域掩码，缺乏对大脑解剖结构的考虑。本研究旨在开发区域感知的重建策略，利用解剖图谱指导掩码过程，以提升模型性能和可解释性。

Method: 使用AAL3图谱对完整4D fMRI体积进行ROI引导掩码，在自监督预训练中选择性地掩码语义连贯的大脑区域，应用于ADHD-200数据集的973名受试者。

Result: 在ADHD分类任务中，相比传统随机掩码方法准确率提升4.23%。区域级归因分析显示边缘系统和脑小脑区域对重建保真度和模型表示贡献最大。

Conclusion: 在模型预训练中掩码解剖区域不仅能增强可解释性，还能产生更鲁棒和区分性的表示。未来计划扩展到更多神经影像数据集，并开发基于区域感知重建目标的新损失函数。

Abstract: The emergence of foundation models in neuroimaging is driven by the
increasing availability of large-scale and heterogeneous brain imaging
datasets. Recent advances in self-supervised learning, particularly
reconstruction-based objectives, have demonstrated strong potential for
pretraining models that generalize effectively across diverse downstream
functional MRI (fMRI) tasks. In this study, we explore region-aware
reconstruction strategies for a foundation model in resting-state fMRI, moving
beyond approaches that rely on random region masking. Specifically, we
introduce an ROI-guided masking strategy using the Automated Anatomical
Labelling Atlas (AAL3), applied directly to full 4D fMRI volumes to selectively
mask semantically coherent brain regions during self-supervised pretraining.
Using the ADHD-200 dataset comprising 973 subjects with resting-state fMRI
scans, we show that our method achieves a 4.23% improvement in classification
accuracy for distinguishing healthy controls from individuals diagnosed with
ADHD, compared to conventional random masking. Region-level attribution
analysis reveals that brain volumes within the limbic region and cerebellum
contribute most significantly to reconstruction fidelity and model
representation. Our results demonstrate that masking anatomical regions during
model pretraining not only enhances interpretability but also yields more
robust and discriminative representations. In future work, we plan to extend
this approach by evaluating it on additional neuroimaging datasets, and
developing new loss functions explicitly derived from region-aware
reconstruction objectives. These directions aim to further improve the
robustness and interpretability of foundation models for functional
neuroimaging.

</details>


### [302] [Deep Learning Approach to Anomaly Detection in Enterprise ETL Processes with Autoencoders](https://arxiv.org/abs/2511.00462)
*Xin Chen,Saili Uday Gadgil,Kangning Gao,Yi Hu,Cong Nie*

Main category: cs.LG

TL;DR: 提出基于深度自编码器的异常检测方法，用于检测企业级ETL数据流中的多种异常类型，通过编码器-解码器结构和正则化约束实现高效异常识别。


<details>
  <summary>Details</summary>
Motivation: 解决企业级ETL数据流中经常出现的延迟、缺失值、重复加载和突发异常变化等异常问题，确保数据处理过程的稳定性和可靠性。

Method: 采用深度自编码器结构，将高维输入压缩为潜在表示并重构，利用重构误差衡量异常程度；在潜在空间中引入正则化约束，增强特征稀疏性和分布学习能力。

Result: 在不同超参数设置、环境变化和数据特征下的系统分析表明，该方法在AUC、ACC、Precision和Recall等指标上表现优异。

Conclusion: 基于深度自编码器的检测机制能有效捕捉企业级ETL数据流中的潜在分布模式，准确识别多种异常，为企业数据处理和智能分析提供可靠支持。

Abstract: An anomaly detection method based on deep autoencoders is proposed to address
anomalies that often occur in enterprise-level ETL data streams. The study
first analyzes multiple types of anomalies in ETL processes, including delays,
missing values, duplicate loading, and sudden abnormal changes, and applies
data standardization and feature modeling to ensure stable and usable inputs.
In the method design, the encoder-decoder structure compresses high-dimensional
inputs into latent representations and reconstructs them, while reconstruction
error is used to measure anomaly levels. Regularization constraints are
introduced in the latent space to enhance feature sparsity and distribution
learning, thereby improving robustness in complex data streams. Systematic
analyses under different hyperparameter settings, environmental changes, and
data characteristics show that the proposed method achieves superior
performance in AUC, ACC, Precision, and Recall. The results demonstrate that
the deep autoencoder-based detection mechanism can effectively capture latent
distribution patterns in enterprise-level ETL data streams and accurately
identify diverse anomalies, providing reliable support for enterprise data
processing and intelligent analysis.

</details>


### [303] [Variational Autoencoder for Calibration: A New Approach](https://arxiv.org/abs/2511.00475)
*Travis Barrett,Amit Kumar Mishra,Joyce Mwangama*

Main category: cs.LG

TL;DR: 提出了一种用于传感器校准的变分自编码器新实现，通过将潜在空间作为校准输出来校准传感器数据，并在多传感器气体数据集上验证了概念。


<details>
  <summary>Details</summary>
Motivation: 探索变分自编码器在传感器校准中的应用，利用其潜在空间作为校准输出，同时保持自编码器的重构能力。

Method: 使用变分自编码器架构，训练潜在空间作为校准输出，在多传感器气体数据集上进行概念验证。

Result: 提出的校准VAE能够同时作为校准模型和自编码器工作，从校准输出和重构输出都能产生与真实数据统计相似的输出。

Conclusion: 该方法展示了VAE在传感器校准中的潜力，计划进行进一步测试和扩展工作。

Abstract: In this paper we present a new implementation of a Variational Autoencoder
(VAE) for the calibration of sensors. We propose that the VAE can be used to
calibrate sensor data by training the latent space as a calibration output. We
discuss this new approach and show a proof-of-concept using an existing
multi-sensor gas dataset. We show the performance of the proposed calibration
VAE and found that it was capable of performing as calibration model while
performing as an autoencoder simultaneously. Additionally, these models have
shown that they are capable of creating statistically similar outputs from both
the calibration output as well as the reconstruction output to their respective
truth data. We then discuss the methods of future testing and planned expansion
of this work.

</details>


### [304] [Reasoning Planning for Language Models](https://arxiv.org/abs/2511.00521)
*Bao Nguyen,Hieu Trung Nguyen,Ruifeng She,Xiaojin Fu,Viet Anh Nguyen*

Main category: cs.LG

TL;DR: EPIC框架通过对比学习创建共享表示空间，学习模型推理能力和查询-方法兼容性，结合概率边界作为正则化器，在精度和计算成本之间平衡优化，提升推理方法选择效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常生成多个候选响应并使用聚合策略选择答案，假设更多候选答案能带来更高准确率。本文重新审视这一假设，通过理论分析推导标准聚合方法在固定生成分布和候选规模下的准确率边界。

Method: 引入EPIC（Ensemble Planning with Contrastive learning）框架，学习共享表示空间来捕捉模型推理能力和查询-方法兼容性。将概率边界作为正则化器，在效用驱动的优化中平衡准确率和计算成本。

Result: 在多样化数学推理任务上的实验表明，EPIC能持续选择最优推理方法，在提高准确率的同时减少计算开销。

Conclusion: EPIC框架通过理论驱动的表示学习和优化，有效解决了语言模型生成中推理方法选择的关键挑战，实现了精度和效率的双重提升。

Abstract: Selecting an appropriate reasoning method for a given query remains a key
challenge in language model generation. Existing approaches typically generate
multiple candidate responses and use an aggregation strategy to select the
output answer, often assuming that more candidate answers yield higher
accuracy. We revisit this assumption through a rigorous theoretical analysis,
deriving accuracy bounds for standard aggregation methods under fixed
generation distributions and candidate sizes. Building on these insights, we
introduce EPIC, an Ensemble Planning with Contrastive learning framework to
learn a shared representation space that captures both model reasoning
abilities and query-method compatibility. EPIC incorporates our probability
bounds as a regularizer in a utility-driven optimization that balances accuracy
and computational cost. Experiments on diverse mathematical reasoning tasks
show that EPIC consistently selects optimal reasoning methods, improving
accuracy while reducing computational overhead. Our code can be found at
https://github.com/nguyenngocbaocmt02/EPIC.

</details>


### [305] [Robust Single-Agent Reinforcement Learning for Regional Traffic Signal Control Under Demand Fluctuations](https://arxiv.org/abs/2511.00549)
*Qiang Li,Jin Niu,Lina Yu*

Main category: cs.LG

TL;DR: 提出基于单智能体强化学习的区域自适应交通信号控制框架，利用DreamerV3世界模型和邻接矩阵统一编码路网拓扑、实时排队状态和信号参数，有效缓解交通拥堵。


<details>
  <summary>Details</summary>
Motivation: 传统交通信号控制模型难以捕捉真实交通复杂性，多智能体系统协调复杂。本研究旨在开发集中决策的单智能体强化学习框架，利用探针车辆数据实现更有效的拥堵缓解。

Method: 采用单智能体强化学习框架，使用邻接矩阵统一编码路网拓扑、实时排队状态和信号参数。基于DreamerV3世界模型，智能体通过顺序选择交叉口并调整信号配时来控制交通流入流出。

Result: SUMO仿真实验表明，在10%-30%起讫点需求波动场景下，该框架具有强抗波动能力，显著减少了排队长度。

Conclusion: 该研究为智能交通控制建立了与探针车辆技术兼容的新范式，未来将关注训练中引入随机需求波动和应急事件区域优化机制。

Abstract: Traffic congestion, primarily driven by intersection queuing, significantly
impacts urban living standards, safety, environmental quality, and economic
efficiency. While Traffic Signal Control (TSC) systems hold potential for
congestion mitigation, traditional optimization models often fail to capture
real-world traffic complexity and dynamics. This study introduces a novel
single-agent reinforcement learning (RL) framework for regional adaptive TSC,
circumventing the coordination complexities inherent in multi-agent systems
through a centralized decision-making paradigm. The model employs an adjacency
matrix to unify the encoding of road network topology, real-time queue states
derived from probe vehicle data, and current signal timing parameters.
Leveraging the efficient learning capabilities of the DreamerV3 world model,
the agent learns control policies where actions sequentially select
intersections and adjust their signal phase splits to regulate traffic
inflow/outflow, analogous to a feedback control system. Reward design
prioritizes queue dissipation, directly linking congestion metrics (queue
length) to control actions. Simulation experiments conducted in SUMO
demonstrate the model's effectiveness: under inference scenarios with
multi-level (10%, 20%, 30%) Origin-Destination (OD) demand fluctuations, the
framework exhibits robust anti-fluctuation capability and significantly reduces
queue lengths. This work establishes a new paradigm for intelligent traffic
control compatible with probe vehicle technology. Future research will focus on
enhancing practical applicability by incorporating stochastic OD demand
fluctuations during training and exploring regional optimization mechanisms for
contingency events.

</details>


### [306] [Red-teaming Activation Probes using Prompted LLMs](https://arxiv.org/abs/2511.00554)
*Phil Blandfort,Robert Graham*

Main category: cs.LG

TL;DR: 提出了一种轻量级黑盒红队测试方法，用于发现激活探针在真实对抗压力下的失效模式，无需微调、梯度或架构访问。


<details>
  <summary>Details</summary>
Motivation: 激活探针作为AI系统监控器具有低成本低延迟优势，但其在真实黑盒对抗环境下的鲁棒性尚未充分探索，需要发现失效模式并最小化测试成本。

Method: 使用现成LLM包装迭代反馈和上下文学习(ICL)的轻量级黑盒红队测试流程，无需微调、梯度或架构访问。

Result: 在高风险交互探针案例研究中，发现了可解释的脆弱性模式（如法律术语导致的误报、平淡程序性语调导致的漏报），以及在场景约束攻击下减少但仍持续的漏洞。

Conclusion: 简单的提示式红队测试框架可以在部署前预测失效模式，并为未来探针加固提供有前景的可操作见解。

Abstract: Activation probes are attractive monitors for AI systems due to low cost and
latency, but their real-world robustness remains underexplored. We ask: What
failure modes arise under realistic, black-box adversarial pressure, and how
can we surface them with minimal effort? We present a lightweight black-box
red-teaming procedure that wraps an off-the-shelf LLM with iterative feedback
and in-context learning (ICL), and requires no fine-tuning, gradients, or
architectural access. Running a case study with probes for high-stakes
interactions, we show that our approach can help discover valuable insights
about a SOTA probe. Our analysis uncovers interpretable brittleness patterns
(e.g., legalese-induced FPs; bland procedural tone FNs) and reduced but
persistent vulnerabilities under scenario-constraint attacks. These results
suggest that simple prompted red-teaming scaffolding can anticipate failure
patterns before deployment and might yield promising, actionable insights to
harden future probes.

</details>


### [307] [Bayesian Network Structure Discovery Using Large Language Models](https://arxiv.org/abs/2511.00574)
*Yinghuan Zhang,Yufei Zhang,Parisa Kordjamshidi,Zijun Cui*

Main category: cs.LG

TL;DR: 提出了一个以LLM为核心的贝叶斯网络结构发现统一框架，支持无数据和有数据两种场景，显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 传统结构学习方法需要大量观测数据且计算成本高，现有LLM方法仅将其作为辅助工具，未能充分发挥LLM在核心学习过程中的作用

Method: 在无数据场景下使用PromptBN通过元数据查询LLM；在有数据场景下使用ReActBN，将ReAct推理范式与BIC等结构评分结合进行迭代优化

Result: 实验表明该方法在低数据或无数据场景下显著优于现有LLM方法和传统数据驱动算法

Conclusion: 该框架将LLM置于结构发现过程的核心位置，为贝叶斯网络学习提供了新的有效途径

Abstract: Understanding probabilistic relationships among variables is crucial for
analyzing complex systems. Traditional structure learning methods often require
extensive observational data and incur high computational costs. Recent studies
have explored using large language models (LLMs) for structure learning, but
most treat LLMs as auxiliary tools for pre-processing or post-processing,
leaving the core learning process data-driven. In this work, we propose a
unified framework for Bayesian network structure discovery that places LLMs at
the center, supporting both data-free and data-aware settings. In the data-free
case, we introduce \textbf{PromptBN} to query LLMs with metadata and
efficiently uncover valid probabilistic relationships. When observational data
are available, we introduce \textbf{ReActBN}, which integrates the ReAct
reasoning paradigm with structure scores such as the Bayesian Information
Criterion (BIC) for iterative refinement. Unlike prior methods that offload
refinement to external algorithms, our framework maintains the LLM actively in
the loop throughout the discovery process. Experiments demonstrate that our
method significantly outperforms both existing LLM-based approaches and
traditional data-driven algorithms, particularly in the low- or no-data
scenario. Code is publicly available at
{\texttt{\textcolor{magenta}{https://github.com/sherryzyh/prompt2bn}}}.

</details>


### [308] [Gaining Momentum: Uncovering Hidden Scoring Dynamics in Hockey through Deep Neural Sequencing and Causal Modeling](https://arxiv.org/abs/2511.00615)
*Daniel Griffiths,Piper Moskow*

Main category: cs.LG

TL;DR: 提出了一个统一的数据驱动框架，通过五个阶段量化并提升冰球比赛中的进攻势头和得分可能性，发现结构化序列和紧凑阵型能显著提高进攻表现。


<details>
  <summary>Details</summary>
Motivation: 旨在量化冰球比赛中的进攻势头和得分可能性，为教练和分析师提供实时、可操作的战术优化见解。

Method: 使用Sportlogiq的541,000条NHL事件记录，通过五个阶段：可解释的势头加权、非线性xG估计、LSTM时序建模、PCA和K-Means空间阵型发现、X-Learner因果推断估计器。

Result: 观察到平均处理效应为0.12（95%置信区间：0.05-0.17，p < 1e-50），相当于得分潜力相对提升15%。

Conclusion: 策略性结构化的序列和紧凑阵型能够因果性地提升进攻表现，该框架为冰球分析提供了基于因果关系的战术优化方法。

Abstract: We present a unified, data-driven framework for quantifying and enhancing
offensive momentum and scoring likelihood (expected goals, xG) in professional
hockey. Leveraging a Sportlogiq dataset of 541,000 NHL event records, our
end-to-end pipeline comprises five stages: (1) interpretable momentum weighting
of micro-events via logistic regression; (2) nonlinear xG estimation using
gradient-boosted decision trees; (3) temporal sequence modeling with Long
Short-Term Memory (LSTM) networks; (4) spatial formation discovery through
principal component analysis (PCA) followed by K-Means clustering on
standardized player coordinates; and (5) use of an X-Learner causal inference
estimator to quantify the average treatment effect (ATE) of adopting the
identified "optimal" event sequences and formations. We observe an ATE of 0.12
(95% CI: 0.05-0.17, p < 1e-50), corresponding to a 15% relative gain in scoring
potential. These results demonstrate that strategically structured sequences
and compact formations causally elevate offensive performance. Our framework
delivers real-time, actionable insights for coaches and analysts, advancing
hockey analytics toward principled, causally grounded tactical optimization.

</details>


### [309] [Reviving Stale Updates: Data-Free Knowledge Distillation for Asynchronous Federated Learning](https://arxiv.org/abs/2511.00655)
*Baris Askin,Holger R. Roth,Zhenyu Sun,Carlee Joe-Wong,Gauri Joshi,Ziyue Xu*

Main category: cs.LG

TL;DR: FedRevive是一个异步联邦学习框架，通过无数据知识蒸馏来缓解陈旧更新问题，在保持AFL可扩展性的同时显著提升训练速度和最终精度。


<details>
  <summary>Details</summary>
Motivation: 异步联邦学习(AFL)虽然解决了同步开销问题，但引入了陈旧更新（基于过时全局模型的客户端更新），这会破坏优化稳定性并阻碍收敛。

Method: FedRevive结合参数空间聚合与轻量级服务器端无数据知识蒸馏(DFKD)，通过元学习生成器合成伪样本实现多教师蒸馏，并采用混合聚合方案结合原始更新和DFKD更新。

Result: 在各种视觉和文本基准测试中，FedRevive相比异步基线实现了高达32.1%的训练速度提升和高达21.5%的最终精度提升。

Conclusion: FedRevive通过无数据知识蒸馏有效缓解了异步联邦学习中的陈旧更新问题，在保持可扩展性的同时显著提升了训练效率和模型性能。

Abstract: Federated Learning (FL) enables collaborative model training across
distributed clients without sharing raw data, yet its scalability is limited by
synchronization overhead. Asynchronous Federated Learning (AFL) alleviates this
issue by allowing clients to communicate independently, thereby improving
wall-clock efficiency in large-scale, heterogeneous environments. However, this
asynchrony introduces stale updates (client updates computed on outdated global
models) that can destabilize optimization and hinder convergence. We propose
FedRevive, an asynchronous FL framework that revives stale updates through
data-free knowledge distillation (DFKD). FedRevive integrates parameter-space
aggregation with a lightweight, server-side DFKD process that transfers
knowledge from stale client models to the current global model without access
to real or public data. A meta-learned generator synthesizes pseudo-samples,
which enables multi-teacher distillation. A hybrid aggregation scheme that
combines raw updates with DFKD updates effectively mitigates staleness while
retaining the scalability of AFL. Experiments on various vision and text
benchmarks show that FedRevive achieves faster training up to 32.1% and higher
final accuracy up to 21.5% compared to asynchronous baselines.

</details>


### [310] [Sensitivity Analysis for Climate Science with Generative Flow Models](https://arxiv.org/abs/2511.00663)
*Alex Dobra,Jakiw Pidstrigach,Tim Reichelt,Paolo Fraccaro,Johannes Jakubik,Anne Jones,Christian Schroeder de Witt,Philip Stier,Philip Torr*

Main category: cs.LG

TL;DR: 该论文提出使用伴随状态方法计算生成流模型（特别是扩散模型）的梯度，以解决气候科学中敏感性分析的计算瓶颈问题，将计算成本从超级计算机上的数周降低到GPU上的数小时。


<details>
  <summary>Details</summary>
Motivation: 传统物理模型进行敏感性分析计算成本过高，而AI生成模型虽然评估速度快，但计算敏感性仍然是瓶颈。

Method: 应用伴随状态方法计算生成流模型的梯度，使用cBottle生成模型作为ERA5数据的模拟器，并提出了梯度自一致性检查来验证计算出的敏感性。

Result: 该方法能够产生可靠的梯度，将敏感性分析的计算成本从超级计算机上的数周大幅降低到GPU上的数小时。

Conclusion: 这种方法可以简化气候科学中的关键工作流程，为敏感性分析提供了高效可靠的解决方案。

Abstract: Sensitivity analysis is a cornerstone of climate science, essential for
understanding phenomena ranging from storm intensity to long-term climate
feedbacks. However, computing these sensitivities using traditional physical
models is often prohibitively expensive in terms of both computation and
development time. While modern AI-based generative models are orders of
magnitude faster to evaluate, computing sensitivities with them remains a
significant bottleneck. This work addresses this challenge by applying the
adjoint state method for calculating gradients in generative flow models, with
diffusion models as a special case. We apply this method to the cBottle
generative model, an emulator of ERA5 data, to perform sensitivity analysis
with respect to sea surface temperatures. Furthermore, we propose a novel
gradient self-consistency check to quantitatively validate the computed
sensitivities against the model's own outputs. Our results provide initial
evidence that this approach can produce reliable gradients, reducing the
computational cost of sensitivity analysis from weeks on a supercomputer with a
physical model to hours on a GPU, thereby simplifying a critical workflow in
climate science.

</details>


### [311] [Inference-Time Chain-of-Thought Pruning with Latent Informativeness Signals](https://arxiv.org/abs/2511.00699)
*Sophie Li,Nicholas Huang,Nayan Saxena,Nina Luo,Vincent Lin,Kevin Zhu,Sunishchal Dev*

Main category: cs.LG

TL;DR: KAPPA是一种推理时方法，通过结合KL散度、置信度和熵的评分函数来指导渐进式剪枝，在保持准确性的同时显著减少内存和token使用。


<details>
  <summary>Details</summary>
Motivation: 标准方法如Best-of-N计算成本高，而现有的Self-Truncation Best-of-N依赖一致性启发式方法，无法直接评估分支质量。

Method: KAPPA将KL散度、置信度和熵结合成原则性评分函数，在探索过程中促进多样性，并选择性消除低分分支。

Result: 在GSM8K和MATH500上的实验显示，KAPPA在小模型中稳定性能，相比BoN减少约60%峰值内存和90%总token生成，对准确性影响最小。

Conclusion: KAPPA通过原则性评分函数有效平衡了推理准确性和计算效率，为LLM推理优化提供了有效解决方案。

Abstract: Large language models (LLMs) improve reasoning accuracy when generating
multiple candidate solutions at test time, but standard methods like Best-of-N
(BoN) incur high computational cost by fully generating all branches.
Self-Truncation Best-of-N (ST-BoN) mitigates this by truncating unpromising
paths early, but its reliance on consistency-based heuristics is a limitation
as it does not directly evaluate branch quality. We present KL-Adjusted Pruned
Path Algorithm (KAPPA), an inference-time method that combines Kullback-Leibler
divergence, confidence, and entropy into a principled scoring function to guide
progressive pruning. By promoting diversity during exploration and selectively
eliminating low-scoring branches, KAPPA maintains accuracy while substantially
reducing memory and token usage. Experiments on GSM8K and MATH500 with
DeepSeek-R1-Distill-Qwen-1.5B and Qwen2.5-7B-Instruct demonstrate that KAPPA
stabilizes performance in smaller models and achieves up to ~60% reduction in
peak memory and ~90% reduction in total token generation relative to BoN, with
minimal impact on accuracy.

</details>


### [312] [Privacy-Aware Time Series Synthesis via Public Knowledge Distillation](https://arxiv.org/abs/2511.00700)
*Penghang Liu,Haibei Zhu,Eleonora Kreacic,Svitlana Vyetrenko*

Main category: cs.LG

TL;DR: Pub2Priv是一个利用异构公共知识生成私有时间序列数据的框架，通过自注意力机制编码公共数据作为扩散模型的输入，在金融、能源和商品交易领域实现了更好的隐私-效用权衡。


<details>
  <summary>Details</summary>
Motivation: 敏感时间序列数据（如金融、医疗记录）因隐私问题难以共享，现有隐私感知数据生成方法往往忽略与公开上下文元数据的相关性，导致隐私-效用权衡不理想。

Method: 使用自注意力机制编码公共数据为时间和特征嵌入，作为扩散模型的条件输入来生成合成私有序列，并引入评估合成数据可识别性的隐私度量。

Result: 实验表明Pub2Priv在金融、能源和商品交易领域始终优于现有基准方法，改善了隐私-效用权衡。

Conclusion: 通过利用异构公共知识，Pub2Priv框架能够更有效地生成私有时间序列数据，实现更好的隐私保护与数据效用平衡。

Abstract: Sharing sensitive time series data in domains such as finance, healthcare,
and energy consumption, such as patient records or investment accounts, is
often restricted due to privacy concerns. Privacy-aware synthetic time series
generation addresses this challenge by enforcing noise during training,
inherently introducing a trade-off between privacy and utility. In many cases,
sensitive sequences is correlated with publicly available, non-sensitive
contextual metadata (e.g., household electricity consumption may be influenced
by weather conditions and electricity prices). However, existing privacy-aware
data generation methods often overlook this opportunity, resulting in
suboptimal privacy-utility trade-offs. In this paper, we present Pub2Priv, a
novel framework for generating private time series data by leveraging
heterogeneous public knowledge. Our model employs a self-attention mechanism to
encode public data into temporal and feature embeddings, which serve as
conditional inputs for a diffusion model to generate synthetic private
sequences. Additionally, we introduce a practical metric to assess privacy by
evaluating the identifiability of the synthetic data. Experimental results show
that Pub2Priv consistently outperforms state-of-the-art benchmarks in improving
the privacy-utility trade-off across finance, energy, and commodity trading
domains.

</details>


### [313] [TRISKELION-1: Unified Descriptive-Predictive-Generative AI](https://arxiv.org/abs/2511.00711)
*Nardeep Kumar,Arun Kanwar*

Main category: cs.LG

TL;DR: TRISKELION-1是一个统一的描述-预测-生成架构，在单一编码器-解码器框架中整合了统计、机制和生成推理。


<details>
  <summary>Details</summary>
Motivation: 构建能够连接可解释性、准确性和创造性的通用智能架构蓝图。

Method: 使用变分目标联合优化描述性表示学习、预测推理和生成合成，在单一编码器-解码器框架中实现。

Result: 在MNIST数据集上的实验验证了描述性重建、预测分类和生成采样可以在一个模型中稳定共存。

Conclusion: 该框架为实现连接可解释性、准确性和创造性的通用智能架构提供了蓝图。

Abstract: TRISKELION-1 is a unified descriptive-predictive-generative architecture that
integrates statistical, mechanistic, and generative reasoning within a single
encoder-decoder framework. The model demonstrates how descriptive
representation learning, predictive inference, and generative synthesis can be
jointly optimized using variational objectives. Experiments on MNIST validate
that descriptive reconstruction, predictive classification, and generative
sampling can coexist stably within one model. The framework provides a
blueprint toward universal intelligence architectures that connect
interpretability, accuracy, and creativity.

</details>


### [314] [Enhancing Heavy Rain Nowcasting with Multimodal Data: Integrating Radar and Satellite Observations](https://arxiv.org/abs/2511.00716)
*Rama Kassoumeh,David Rügamer,Henning Oppel*

Main category: cs.LG

TL;DR: 该研究开发了一种融合卫星和雷达数据的多模态临近预报模型，用于预测5、15和30分钟内的降水，显著优于仅使用雷达的方法，特别是在强降水预测方面。


<details>
  <summary>Details</summary>
Motivation: 传统地面传感器难以检测局部强降雨事件（德国2001-2018年间只有17.3%的每小时强降雨事件被雨量计记录），雷达数据单独预测强降雨发展仍具挑战性，因此需要融合卫星和雷达数据来提高预报准确性。

Method: 开发多模态临近预报模型，结合雷达和卫星图像数据，预测5、15和30分钟内的降水量。

Result: 多模态策略显著优于仅使用雷达的方法，卫星数据集成提高了预测准确性，特别是对强降水。在5分钟提前期，强降雨的临界成功指数提高4%，暴雨提高3%，且在较长提前期保持更高的预测能力。

Conclusion: 多模态模型能提供更详细准确的强降雨区域预报，实现及时可靠的预警，具有拯救生命的潜力。

Abstract: The increasing frequency of heavy rainfall events, which are a major cause of
urban flooding, underscores the urgent need for accurate precipitation
forecasting - particularly in urban areas where localized events often go
undetected by ground-based sensors. In Germany, only 17.3% of hourly heavy rain
events between 2001 and 2018 were recorded by rain gauges, highlighting the
limitations of traditional monitoring systems. Radar data are another source
that effectively tracks ongoing precipitation; however, forecasting the
development of heavy rain using radar alone remains challenging due to the
brief and unpredictable nature of such events. Our focus is on evaluating the
effectiveness of fusing satellite and radar data for nowcasting. We develop a
multimodal nowcasting model that combines both radar and satellite imagery for
predicting precipitation at lead times of 5, 15, and 30 minutes. We demonstrate
that this multimodal strategy significantly outperforms radar-only approaches.
Experimental results show that integrating satellite data improves prediction
accuracy, particularly for intense precipitation. The proposed model increases
the Critical Success Index for heavy rain by 4% and for violent rain by 3% at a
5-minute lead time. Moreover, it maintains higher predictive skill at longer
lead times, where radar-only performance declines. A qualitative analysis of
the severe flooding event in the state of North Rhine-Westphalia, Germany in
2021 further illustrates the superior performance of the multimodal model.
Unlike the radar-only model, which captures general precipitation patterns, the
multimodal model yields more detailed and accurate forecasts for regions
affected by heavy rain. This improved precision enables timely, reliable,
life-saving warnings. Implementation available at
https://github.com/RamaKassoumeh/Multimodal_heavy_rain

</details>


### [315] [Effective Series Decomposition and Components Learning for Time Series Generation](https://arxiv.org/abs/2511.00747)
*Zixuan Ma,Chenfeng Huang*

Main category: cs.LG

TL;DR: STDiffusion是一个新颖的多变量时间序列生成框架，结合扩散概率模型和可学习的序列分解技术，通过分别学习趋势和季节性成分来提高生成过程的可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列生成方法未能充分利用可解释的分解方法，限制了合成有意义的趋势和季节性模式的能力。

Method: 使用扩散概率模型，将趋势和季节性学习分离到不同模块：MLP结构捕捉趋势，自适应小波蒸馏实现季节性成分的多分辨率学习，并设计了综合校正机制确保生成成分的内部一致性。

Result: 在8个真实世界数据集上的实证研究表明，STDiffusion在时间序列生成任务中达到了最先进的性能，并在多窗口长序列时间序列生成中表现出鲁棒性和多功能性。

Conclusion: STDiffusion通过可解释的分解方法有效提升了时间序列生成的质量和可解释性，并展示了在长序列生成中的良好适应性。

Abstract: Time series generation focuses on modeling the underlying data distribution
and resampling to produce authentic time series data. Key components, such as
trend and seasonality, drive temporal fluctuations, yet many existing
approaches fail to employ interpretative decomposition methods, limiting their
ability to synthesize meaningful trend and seasonal patterns. To address this
gap, we introduce Seasonal-Trend Diffusion (STDiffusion), a novel framework for
multivariate time series generation that integrates diffusion probabilistic
models with advanced learnable series decomposition techniques, enhancing the
interpretability of the generation process. Our approach separates the trend
and seasonal learning into distinct blocks: a Multi-Layer Perceptron (MLP)
structure captures the trend, while adaptive wavelet distillation facilitates
effective multi-resolution learning of seasonal components. This decomposition
improves the interpretability of the model on multiple scales. In addition, we
designed a comprehensive correction mechanism aimed at ensuring that the
generated components exhibit a high degree of internal consistency and preserve
meaningful interrelationships with one another. Our empirical studies on eight
real-world datasets demonstrate that STDiffusion achieves state-of-the-art
performance in time series generation tasks. Furthermore, we extend the model's
application to multi-window long-sequence time series generation, which
delivered reliable results and highlighted its robustness and versatility.

</details>


### [316] [Fast PINN Eigensolvers via Biconvex Reformulation](https://arxiv.org/abs/2511.00792)
*Akshay Sai Banderwaar,Abhishek Gupta*

Main category: cs.LG

TL;DR: 提出了一种改进的PINN方法，将特征值问题重新表述为双凸优化问题，通过交替凸搜索实现快速收敛，比传统梯度训练快500倍。


<details>
  <summary>Details</summary>
Motivation: 传统PINN方法解决特征值问题时计算效率低，比经典数值方法慢几个数量级，需要更快的收敛方法。

Method: 将特征对搜索重新表述为双凸优化问题，使用分析最优更新的交替凸搜索(ACS)算法，分别优化特征值和特征函数。

Result: PINN-ACS方法实现了高精度，收敛速度比基于梯度的PINN训练快500倍。

Conclusion: 提出的PINN-ACS方法显著提高了特征值问题的求解效率，为物理信息神经网络在特征值问题中的应用提供了更实用的解决方案。

Abstract: Eigenvalue problems have a distinctive forward-inverse structure and are
fundamental to characterizing a system's thermal response, stability, and
natural modes. Physics-Informed Neural Networks (PINNs) offer a mesh-free
alternative for solving such problems but are often orders of magnitude slower
than classical numerical schemes. In this paper, we introduce a reformulated
PINN approach that casts the search for eigenpairs as a biconvex optimization
problem, enabling fast and provably convergent alternating convex search (ACS)
over eigenvalues and eigenfunctions using analytically optimal updates.
Numerical experiments show that PINN-ACS attains high accuracy with convergence
speeds up to 500$\times$ faster than gradient-based PINN training. We release
our codes at https://github.com/NeurIPS-ML4PS-2025/PINN_ACS_CODES.

</details>


### [317] [Efficient Reinforcement Learning for Large Language Models with Intrinsic Exploration](https://arxiv.org/abs/2511.00794)
*Yan Sun,Jia Guo,Stanley Kok,Zihao Wang,Zujie Wen,Zhiqiang Zhang*

Main category: cs.LG

TL;DR: 提出了PREPO方法，通过利用内在数据特性提高RLVR的数据效率，包含两个互补组件：基于提示困惑度的适应性学习和基于相对熵差异的探索优先机制。


<details>
  <summary>Details</summary>
Motivation: RLVR训练成本高昂，许多rollout对优化的贡献很小。研究如何利用几乎免费的内在数据特性来提高RLVR的数据效率。

Method: PREPO方法包含两个组件：1) 使用提示困惑度作为模型适应性指标，从易到难学习；2) 通过区分相对熵来放大rollout间的差异，优先选择探索程度更高的序列。

Result: 在Qwen和Llama模型上，PREPO在数学推理基准测试中仅需基线方法1/3的rollout次数就能达到竞争性性能。

Conclusion: PREPO有效减少了RLVR的rollout需求，同时保持了竞争性性能，并通过理论和深入分析解释了该方法提高数据效率的基本原理。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has improved the
reasoning ability of large language models, yet training remains costly because
many rollouts contribute little to optimization, considering the amount of
computation required. This study investigates how simply leveraging intrinsic
data properties, almost free benefit during training, can improve data
efficiency for RLVR. We propose PREPO with two complementary components. First,
we adopt prompt perplexity as an indicator of model adaptability in learning,
enabling the model to progress from well-understood contexts to more
challenging ones. Second, we amplify the discrepancy among the rollouts by
differentiating their relative entropy, and prioritize sequences that exhibit a
higher degree of exploration. Together, these mechanisms reduce rollout demand
while preserving competitive performance. On the Qwen and Llama models, PREPO
achieves effective results on mathematical reasoning benchmarks with up to 3
times fewer rollouts than the baselines. Beyond empirical gains, we provide
theoretical and in-depth analyses explaining the underlying rationale of our
method to improve the data efficiency of RLVR.

</details>


### [318] [Attention Saturation and Gradient Suppression at Inflection Layers: Diagnosing and Mitigating Bottlenecks in Transformer Adaptation](https://arxiv.org/abs/2511.00797)
*Wang Zixian*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Pre-trained Transformers often exhibit over-confidence in source patterns and
difficulty in forming new target-domain patterns during fine-tuning. We
formalize the mechanism of output saturation leading to gradient suppression
through standard cross-entropy and softmax analysis, showing that gradient
suppression at inflection layers confines adaptation to high-level
recombination of existing features while preventing low-level reconstruction.
We introduce a set of layer-wise diagnostic metrics -- attention entropy
(saturation proxy), activation gradient norm, parameter gradient norm, and
Delta-CKA under a shared PCA basis -- to identify inflection layers
characterized by both low attention entropy and steep gradient decay. Building
on these findings, we propose a diagnose-first, inject-light fine-tuning
strategy: selectively inserting LoRA adapters at inflection layers to restore
suppressed backward signals with minimal parameter overhead. Experiments on
BERT-base transfer from SST-2 to Rotten Tomatoes under under-trained and
over-trained source regimes reveal that over-trained initialization benefits
from inflection-layer LoRA injection, while under-trained initialization
suffers performance degradation. When base features are strong, unblocking
inflection layers facilitates high-level compositional adaptation; when base
features are weak, full-pathway unblocking is required for low-level
reconstruction, as supported by joint analysis of layer-wise activation
gradients and Delta-CKA dynamics.

</details>


### [319] [EraseFlow: Learning Concept Erasure Policies via GFlowNet-Driven Alignment](https://arxiv.org/abs/2511.00804)
*Abhiram Kusumba,Maitreya Patel,Kyle Min,Changhoon Kim,Chitta Baral,Yezhou Yang*

Main category: cs.LG

TL;DR: EraseFlow是一个基于GFlowNets的概念擦除框架，通过探索去噪路径空间来引导生成过程远离目标概念，同时保持模型先验知识，无需精心设计的奖励模型。


<details>
  <summary>Details</summary>
Motivation: 当前的概念擦除技术存在图像质量下降、依赖脆弱的对抗损失或需要大量重新训练的问题，需要一种更有效的方法来安全地从文本到图像生成器中移除有害或专有概念。

Method: 将概念遗忘视为去噪路径空间的探索问题，使用配备轨迹平衡目标的GFlowNets来优化，通过采样整个轨迹而非单一最终状态来学习随机策略。

Result: EraseFlow在广泛实验中优于现有基线，在性能和先验保持之间实现了最佳权衡，能有效泛化到未见概念并避免可被攻击的奖励。

Conclusion: EraseFlow是首个将概念遗忘框架化为去噪路径探索的方法，通过GFlowNets实现了高效的概念擦除，无需精心设计的奖励模型，在保持模型性能的同时有效移除目标概念。

Abstract: Erasing harmful or proprietary concepts from powerful text to image
generators is an emerging safety requirement, yet current "concept erasure"
techniques either collapse image quality, rely on brittle adversarial losses,
or demand prohibitive retraining cycles. We trace these limitations to a myopic
view of the denoising trajectories that govern diffusion based generation. We
introduce EraseFlow, the first framework that casts concept unlearning as
exploration in the space of denoising paths and optimizes it with GFlowNets
equipped with the trajectory balance objective. By sampling entire trajectories
rather than single end states, EraseFlow learns a stochastic policy that steers
generation away from target concepts while preserving the model's prior.
EraseFlow eliminates the need for carefully crafted reward models and by doing
this, it generalizes effectively to unseen concepts and avoids hackable rewards
while improving the performance. Extensive empirical results demonstrate that
EraseFlow outperforms existing baselines and achieves an optimal trade off
between performance and prior preservation.

</details>


### [320] [Logic-informed reinforcement learning for cross-domain optimization of large-scale cyber-physical systems](https://arxiv.org/abs/2511.00806)
*Guangxi Wan,Peng Zeng,Xiaoting Dong,Chunhe Song,Shijie Cui,Dong Li,Qingwei Dong,Yiyang Liu,Hongfei Bai*

Main category: cs.LG

TL;DR: 提出了逻辑信息强化学习(LIRL)，通过将低维潜在动作投影到由一阶逻辑定义的混合流形上，保证每个探索步骤的可行性，无需惩罚调优，在多个CPS场景中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有分层方法往往牺牲全局最优性，而强化学习在混合动作空间中通常依赖脆弱的奖励惩罚、掩蔽或屏蔽，难以保证约束满足。

Method: 为标准策略梯度算法配备投影机制，将低维潜在动作映射到由一阶逻辑动态定义的可容许混合流形上。

Result: 在工业制造、电动汽车充电站和交通信号控制等多个场景中，LIRL均优于现有分层优化方法。以机器人减速器装配系统为例，相比传统工业分层调度方法，在组合完工时间-能耗目标上最多减少36.47%至44.33%，同时保持零约束违反。

Conclusion: 由于其声明式逻辑约束表述，该框架可以无缝转移到智能交通和智能电网等其他领域，为大规模CPS的安全实时优化铺平道路。

Abstract: Cyber-physical systems (CPS) require the joint optimization of discrete cyber
actions and continuous physical parameters under stringent safety logic
constraints. However, existing hierarchical approaches often compromise global
optimality, whereas reinforcement learning (RL) in hybrid action spaces often
relies on brittle reward penalties, masking, or shielding and struggles to
guarantee constraint satisfaction. We present logic-informed reinforcement
learning (LIRL), which equips standard policy-gradient algorithms with
projection that maps a low-dimensional latent action onto the admissible hybrid
manifold defined on-the-fly by first-order logic. This guarantees feasibility
of every exploratory step without penalty tuning. Experimental evaluations have
been conducted across multiple scenarios, including industrial manufacturing,
electric vehicle charging stations, and traffic signal control, in all of which
the proposed method outperforms existing hierarchical optimization approaches.
Taking a robotic reducer assembly system in industrial manufacturing as an
example, LIRL achieves a 36.47\% to 44.33\% reduction at most in the combined
makespan-energy objective compared to conventional industrial hierarchical
scheduling methods. Meanwhile, it consistently maintains zero constraint
violations and significantly surpasses state-of-the-art hybrid-action
reinforcement learning baselines. Thanks to its declarative logic-based
constraint formulation, the framework can be seamlessly transferred to other
domains such as smart transportation and smart grid, thereby paving the way for
safe and real-time optimization in large-scale CPS.

</details>


### [321] [Equilibrium Policy Generalization: A Reinforcement Learning Framework for Cross-Graph Zero-Shot Generalization in Pursuit-Evasion Games](https://arxiv.org/abs/2511.00811)
*Runyu Lu,Peng Zhang,Ruochuan Shi,Yuanheng Zhu,Dongbin Zhao,Yang Liu,Dong Wang,Cesare Alippi*

Main category: cs.LG

TL;DR: 提出了一个均衡策略泛化（EPG）框架，用于在对抗性游戏中学习具有跨图零样本性能的泛化策略，特别针对追逃游戏（PEG）场景。


<details>
  <summary>Details</summary>
Motivation: 现有的追逃游戏求解方法需要指数时间，且当图结构变化时需要重新计算或微调，这限制了实时应用。需要一种能够跨图结构泛化的策略学习方法。

Method: EPG框架在不同图结构上训练RL策略，针对每个单图的均衡策略。使用动态规划算法生成纯策略纳什均衡，并设计了分组机制和序列模型来处理多追捕者场景。

Result: 实验结果表明，EPG框架在各种未见过的真实世界图中实现了良好的零样本性能，在带出口的图中，泛化的追捕者策略甚至能与最先进方法的微调策略相媲美。

Conclusion: EPG框架成功解决了追逃游戏中的跨图泛化问题，为对抗性游戏中的均衡学习提供了有效的解决方案，具有实际应用价值。

Abstract: Equilibrium learning in adversarial games is an important topic widely
examined in the fields of game theory and reinforcement learning (RL).
Pursuit-evasion game (PEG), as an important class of real-world games from the
fields of robotics and security, requires exponential time to be accurately
solved. When the underlying graph structure varies, even the state-of-the-art
RL methods require recomputation or at least fine-tuning, which can be
time-consuming and impair real-time applicability. This paper proposes an
Equilibrium Policy Generalization (EPG) framework to effectively learn a
generalized policy with robust cross-graph zero-shot performance. In the
context of PEGs, our framework is generally applicable to both pursuer and
evader sides in both no-exit and multi-exit scenarios. These two
generalizability properties, to our knowledge, are the first to appear in this
domain. The core idea of the EPG framework is to train an RL policy across
different graph structures against the equilibrium policy for each single
graph. To construct an equilibrium oracle for single-graph policies, we present
a dynamic programming (DP) algorithm that provably generates pure-strategy Nash
equilibrium with near-optimal time complexity. To guarantee scalability with
respect to pursuer number, we further extend DP and RL by designing a grouping
mechanism and a sequence model for joint policy decomposition, respectively.
Experimental results show that, using equilibrium guidance and a distance
feature proposed for cross-graph PEG training, the EPG framework guarantees
desirable zero-shot performance in various unseen real-world graphs. Besides,
when trained under an equilibrium heuristic proposed for the graphs with exits,
our generalized pursuer policy can even match the performance of the fine-tuned
policies from the state-of-the-art PEG methods.

</details>


### [322] [LL-ViT: Edge Deployable Vision Transformers with Look Up Table Neurons](https://arxiv.org/abs/2511.00812)
*Shashank Nag,Alan T. L. Bacellar,Zachary Susskind,Anshul Jha,Logan Liberty,Aishwarya Sivakumar,Eugene B. John,Krishnan Kailas,Priscila M. V. Lima,Neeraja J. Yadwadkar,Felipe M. G. Franca,Lizy K. John*

Main category: cs.LG

TL;DR: 提出LL-ViT，一种面向边缘设备的优化视觉Transformer设计，通过集成LUT神经元层来减少模型大小、计算量和能耗，在保持准确性的同时提升FPGA上的推理效率。


<details>
  <summary>Details</summary>
Motivation: 视觉Transformer在边缘设备上的推理面临计算、内存和能耗挑战，而现有的基于逻辑和LUT的网络模型在视觉任务上表现不佳，需要一种既能保持准确性又能优化边缘推理的方案。

Method: 设计基于LUT的通道混合器替代传统MLP层，采用神经网络学习方法原生学习LUT函数，并开发专门的FPGA加速器。

Result: 在CIFAR-10、CIFAR-100和Tiny-ImageNet上分别达到95.5%、78.8%和60.9%的准确率，消除60%模型权重和50%乘法运算，相比量化ViT加速器实现1.9倍能效提升和1.3倍延迟降低。

Conclusion: LL-ViT为视觉Transformer提供了一种高效、节能的边缘推理解决方案，在保持性能的同时显著优化了资源使用。

Abstract: Vision Transformers have been tremendously successful in computer vision
tasks. However, their large computational, memory, and energy demands are a
challenge for edge inference on FPGAs -- a field that has seen a recent surge
in demand. We recognize the benefits of recent works on logic and Look Up Table
(LUT) based networks, such as LogicNets, NeuraLUT, DWN, among others, in
offering models that simultaneously reduce both the memory and compute
footprints. However, these models natively do not perform well on common vision
tasks, such as CIFAR-10/100. In this work, we propose LL-ViT, a novel edge
optimized vision transformer design that integrates layers of LUT neurons
within the transformer architecture. Based on our characterization that reveals
that a majority of model weights and computations are from the channel mixer
(MLP layer), we design an alternate LUT-based channel mixer, and simultaneously
develop an FPGA-based accelerator for LL-ViT. Contrary to some attempts to
replace each multiplication with a table lookup, our architecture utilizes a
neural learning approach which natively learns the LUT functions. This approach
allows for reduced model sizes, and a computational and energy-efficient
inference solution for vision transformer models. Evaluating on edge-suitable
workloads, we achieve accuracies of 95.5% on CIFAR-10, 78.8% on CIFAR-100, and
60.9% on Tiny-ImageNet datasets, comparable to the baseline transformer. LL-ViT
eliminates over 60% of the model weights and 50% of the multiplications in the
model, and achieves 1.9x energy efficiency and 1.3x lower latency over an
integer quantized ViT accelerator, while also offering superior throughput
against prior works at a 10.9W power budget.

</details>


### [323] [Identifying Slug Formation in Oil Well Pipelines: A Use Case from Industrial Analytics](https://arxiv.org/abs/2511.00851)
*Abhishek Patange,Sharat Chidambaran,Prabhat Shankar,Manjunath G. B.,Anindya Chatterjee*

Main category: cs.LG

TL;DR: 开发了一个交互式应用程序，用于油气管道中的段塞流检测，集成了数据探索、模型训练、可视化和实时推理功能。


<details>
  <summary>Details</summary>
Motivation: 现有段塞流检测方法多为离线、需要专业知识且缺乏实时可解释性，难以满足工业实时监测需求。

Method: 构建端到端数据驱动系统，包含数据标注、可配置模型训练、分类结果可视化以及基于持久性的实时警报模块。

Result: 系统支持从CSV数据上传到实时推理的无缝工作流，具有轻量、便携和易部署的特点。

Conclusion: 该工具通过交互式人机协同ML系统，弥合了数据科学方法与关键过程工业中实际决策之间的差距，可扩展到其他时间序列故障诊断任务。

Abstract: Slug formation in oil and gas pipelines poses significant challenges to
operational safety and efficiency, yet existing detection approaches are often
offline, require domain expertise, and lack real-time interpretability. We
present an interactive application that enables end-to-end data-driven slug
detection through a compact and user-friendly interface. The system integrates
data exploration and labeling, configurable model training and evaluation with
multiple classifiers, visualization of classification results with time-series
overlays, and a real-time inference module that generates persistence-based
alerts when slug events are detected. The demo supports seamless workflows from
labeled CSV uploads to live inference on unseen datasets, making it
lightweight, portable, and easily deployable. By combining domain-relevant
analytics with novel UI/UX features such as snapshot persistence, visual
labeling, and real-time alerting, our tool adds significant dissemination value
as both a research prototype and a practical industrial application. The demo
showcases how interactive human-in-the-loop ML systems can bridge the gap
between data science methods and real-world decision-making in critical process
industries, with broader applicability to time-series fault diagnosis tasks
beyond oil and gas.

</details>


### [324] [FlexiCache: Leveraging Temporal Stability of Attention Heads for Efficient KV Cache Management](https://arxiv.org/abs/2511.00868)
*Nazmul Takbir,Hamidreza Alikhani,Nikil Dutt,Sangeetha Abdu Jyothi*

Main category: cs.LG

TL;DR: FlexiCache是一个基于KV头时间稳定性的分层KV缓存管理系统，通过将KV头分类为稳定和不稳定类型，显著减少GPU内存使用和计算开销，同时保持模型精度。


<details>
  <summary>Details</summary>
Motivation: LLM服务受限于不断增长的KV缓存大小，现有系统难以在不降低精度的情况下有效利用注意力机制中关键令牌的稀疏性，特别是在长文本生成场景中。

Method: 基于KV头时间稳定性的观察，将KV头分类为稳定和不稳定类型：不稳定头保留所有KV缓存页在GPU内存中，稳定头仅保留top-K页在GPU中，其余卸载到主机内存，并利用时间稳定性进行周期性重排序。

Result: 在vLLM上实现，将长上下文请求的GPU内存占用减少高达70%，离线服务吞吐量提升1.38-1.55倍，在线令牌延迟降低1.6-2.1倍。

Conclusion: FlexiCache通过利用KV头的时间稳定性特性，在保持精度的同时显著提升了LLM服务的效率和可扩展性。

Abstract: Large Language Model (LLM) serving is increasingly constrained by the growing
size of the key-value (KV) cache, which scales with both context length and
generation length. Prior work shows that attention is dominated by a small
subset of critical tokens, yet existing systems struggle to exploit this
efficiently without degrading accuracy, especially in long generation. We make
a key observation: the temporal stability of these critical tokens varies
significantly across KV heads: some heads consistently focus on the same
tokens, while others shift frequently. Building on this insight, we introduce
FlexiCache, a hierarchical KV-cache management system that leverages the
temporal stability of KV heads to reduce GPU memory usage and computation
overhead, while preserving model accuracy. FlexiCache classifies KV heads as
stable or unstable: it retains all KV-cache pages from unstable heads in GPU
memory, whereas for stable heads, it keeps only the top-K pages on the GPU and
offloads the rest to host memory. By exploiting temporal stability, FlexiCache
performs periodic reranking for stable heads to fetch newly promoted top pages.
Implemented atop vLLM, FlexiCache reduces GPU memory footprint for long-context
requests by up to 70%, improves offline serving throughput by 1.38-1.55x, and
lowers online token latency by 1.6-2.1x, all while maintaining accuracy in
long-context, long-generation scenarios.

</details>


### [325] [Training with Fewer Bits: Unlocking Edge LLMs Training with Stochastic Rounding](https://arxiv.org/abs/2511.00874)
*Taowen Liu,Marta Andronic,Deniz Gündüz,George A. Constantinides*

Main category: cs.LG

TL;DR: 研究表明，增加批量大小可以补偿反向传播中的低精度量化，同时量化权重和激活值对梯度方差有不同影响。


<details>
  <summary>Details</summary>
Motivation: 量化训练虽然提高计算和内存效率，但会引入量化噪声，影响模型收敛和精度。随机舍入理论上能提供无偏梯度估计，但其与批量大小等训练因素的交互作用尚未充分研究。

Method: 通过理论和实证研究，分析小批量随机梯度下降与随机舍入的结合，探讨批量大小如何补偿反向传播中的精度损失。

Result: 实验验证了理论见解，表明增加批量大小确实能有效补偿量化精度损失，且权重和激活值的量化对梯度方差产生不同影响。

Conclusion: 批量大小是量化训练中的重要调节因素，能够有效缓解量化噪声带来的负面影响，为高效LLM训练提供了实用指导。

Abstract: LLM training is resource-intensive. Quantized training improves computational
and memory efficiency but introduces quantization noise, which can hinder
convergence and degrade model accuracy. Stochastic Rounding (SR) has emerged as
a theoretically attractive alternative to deterministic rounding, offering
unbiased gradient estimates. However, its interaction with other training
factors -- especially batch size -- remains under explored. In this paper, we
present a theoretical and empirical study of mini-batch stochastic gradient
descent (SGD) with SR, showing that increased batch sizes can compensate for
reduced precision during back-propagation. Furthermore, we show that quantizing
weights and activations impacts gradient variance in distinct ways. Our
experiments validate these theoretical insights.

</details>


### [326] [KFCPO: Kronecker-Factored Approximated Constrained Policy Optimization](https://arxiv.org/abs/2511.00880)
*Joonyoung Lim,Younghwan Yoo*

Main category: cs.LG

TL;DR: KFCPO是一种新颖的安全强化学习算法，结合了基于Kronecker-Factored Approximate Curvature的二阶策略优化和安全感知的梯度操作，在保持安全约束的同时实现了更高的平均回报。


<details>
  <summary>Details</summary>
Motivation: 为了解决奖励最大化和约束满足之间的权衡问题，同时避免传统方法中固定硬阈值导致的突变问题。

Method: 使用K-FAC近似Fisher信息矩阵进行高效稳定的自然梯度更新，引入边界感知梯度操作机制自适应调整奖励和成本梯度的影响，采用小批量KL回滚策略确保信任区域合规。

Result: 在Safety Gymnasium上的实验表明，KFCPO相比最佳基线方法实现了10.3%到50.2%的平均回报提升，同时保持了安全约束。

Conclusion: KFCPO在安全性和性能之间实现了优越的平衡，证明了二阶优化与自适应梯度操作相结合的有效性。

Abstract: We propose KFCPO, a novel Safe Reinforcement Learning (Safe RL) algorithm
that combines scalable Kronecker-Factored Approximate Curvature (K-FAC) based
second-order policy optimization with safety-aware gradient manipulation. KFCPO
leverages K-FAC to perform efficient and stable natural gradient updates by
approximating the Fisher Information Matrix (FIM) in a layerwise, closed form
manner, avoiding iterative approximation overheads. To address the tradeoff
between reward maximization and constraint satisfaction, we introduce a margin
aware gradient manipulation mechanism that adaptively adjusts the influence of
reward and cost gradients based on the agent's proximity to safety boundaries.
This method blends gradients using a direction sensitive projection,
eliminating harmful interference and avoiding abrupt changes caused by fixed
hard thresholds. Additionally, a minibatch level KL rollback strategy is
adopted to ensure trust region compliance and to prevent destabilizing policy
shifts. Experiments on Safety Gymnasium using OmniSafe show that KFCPO achieves
10.3% to 50.2% higher average return across environments compared to the best
baseline that respected the safety constraint, demonstrating superior balance
of safety and performance.

</details>


### [327] [SpEx: A Spectral Approach to Explainable Clustering](https://arxiv.org/abs/2511.00885)
*Tal Argov,Tal Wagner*

Main category: cs.LG

TL;DR: 提出了一种基于谱图划分的可解释聚类新方法，能够为任何给定的非可解释聚类或数据集构建解释树，并将先前算法统一到图划分框架中。


<details>
  <summary>Details</summary>
Motivation: 现有可解释聚类方法局限于特定聚类目标的可解释性代价最小化，缺乏通用的方法来为任意聚类构建解释树。

Method: 基于谱图划分设计可解释聚类算法，利用Trevisan(2013)的广义框架将先前算法统一解释为同时在两个图上优化的图划分方法。

Result: 实验表明该方法在多个数据集上相比基线方法表现更优。

Conclusion: 谱图划分为可解释聚类提供了通用有效的解决方案，能够统一现有方法并实现更好的性能。

Abstract: Explainable clustering by axis-aligned decision trees was introduced by
Moshkovitz et al. (2020) and has gained considerable interest. Prior work has
focused on minimizing the price of explainability for specific clustering
objectives, lacking a general method to fit an explanation tree to any given
clustering, without restrictions. In this work, we propose a new and generic
approach to explainable clustering, based on spectral graph partitioning. With
it, we design an explainable clustering algorithm that can fit an explanation
tree to any given non-explainable clustering, or directly to the dataset
itself. Moreover, we show that prior algorithms can also be interpreted as
graph partitioning, through a generalized framework due to Trevisan (2013)
wherein cuts are optimized in two graphs simultaneously. Our experiments show
the favorable performance of our method compared to baselines on a range of
datasets.

</details>


### [328] [FEval-TTC: Fair Evaluation Protocol for Test-Time Compute](https://arxiv.org/abs/2511.01203)
*Pavel Rumiantsev,Soumyasundar Pal,Yingxue Zhang,Mark Coates*

Main category: cs.LG

TL;DR: 提出了FEval-TTC协议，用于确保测试时计算方法的公平评估，不受LLM性能和API成本波动的影响。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的性能和API调用成本会随时间波动，这可能使先前研究的结论失效。

Method: 设计了一个公平评估协议，标准化少样本提示和答案提取过程，支持跨多个LLM在数学和常识推理数据集上的评估，并提供成本建模程序。

Result: 开发了开源工具FEval-TTC，减少了研究者的时间和金钱开销，便于公平比较不同的测试时计算方法。

Conclusion: FEval-TTC协议为测试时计算方法的评估提供了标准化和公平的框架，有助于研究社区进行一致的性能比较。

Abstract: The performance of Large Language Models (LLMs) and the associated dollar
costs of API calls can fluctuate over time, potentially invalidating
conclusions drawn in prior research. To address this, we propose a Fair
Evaluation protocol for Test-Time Compute (FEval-TTC), designed to ensure
consistent assessment of test-time compute (TTC) methods, regardless of such
fluctuations. FEval-TTC focuses on the evaluation of TTC methods that utilize
underlying Chains-of-Thought (CoT). It supports evaluations across multiple
LLMs on a diverse set of mathematical and commonsense reasoning datasets. The
few-shot prompting and answer extraction processes are standardized across
datasets, reducing both time and monetary overhead for researchers.
Furthermore, we provide a cost modelling procedure that estimates both the
token and dollar cost per query, facilitating equitable comparisons of
prevalent TTC methods. We open-source FEval-TTC for public use at
https://github.com/networkslab/feval_ttc .

</details>


### [329] [Learning with Category-Equivariant Representations for Human Activity Recognition](https://arxiv.org/abs/2511.00900)
*Yoshihiro Maruyama*

Main category: cs.LG

TL;DR: 提出了一种基于类别对称性的学习框架，通过将时间、尺度和传感器层次结构等对称性因素融入特征表示中，提高人类活动识别模型在现实扭曲下的稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决传感器信号随上下文、运动和环境变化而偏移的问题，使模型在周围世界变化时保持稳定。

Method: 构建类别对称感知学习框架，将时间、尺度和传感器层次结构等对称性因素融入特征表示结构，使模型自动保持传感器间关系并在时间偏移、幅度漂移和设备方向变化等现实扭曲下保持稳定。

Result: 在UCI人类活动识别基准测试中，该框架将分布外准确率提高了约46个百分点（约3.6倍于基线），证明抽象对称原则可以通过类别等变表示理论转化为日常感知任务中的具体性能提升。

Conclusion: 类别对称驱动设计能够显著提高人类活动识别模型在现实扭曲条件下的稳定性和性能，展示了抽象数学原理在实际应用中的价值。

Abstract: Human activity recognition is challenging because sensor signals shift with
context, motion, and environment; effective models must therefore remain stable
as the world around them changes. We introduce a categorical symmetry-aware
learning framework that captures how signals vary over time, scale, and sensor
hierarchy. We build these factors into the structure of feature
representations, yielding models that automatically preserve the relationships
between sensors and remain stable under realistic distortions such as time
shifts, amplitude drift, and device orientation changes. On the UCI Human
Activity Recognition benchmark, this categorical symmetry-driven design
improves out-of-distribution accuracy by approx. 46 percentage points (approx.
3.6x over the baseline), demonstrating that abstract symmetry principles can
translate into concrete performance gains in everyday sensing tasks via
category-equivariant representation theory.

</details>


### [330] [RLAC: Reinforcement Learning with Adversarial Critic for Free-Form Generation Tasks](https://arxiv.org/abs/2511.01758)
*Mian Wu,Gavin Zhang,Sewon Min,Sergey Levine,Aviral Kumar*

Main category: cs.LG

TL;DR: 提出了RLAC方法，通过动态评估标准验证来解决开放式生成任务中的评估难题，使用LLM作为批评家识别最可能的失败模式，联合优化生成器和批评家。


<details>
  <summary>Details</summary>
Motivation: 开放式生成任务需要满足多样且隐式的评估标准，但大量相关标准导致验证成本过高和不完整评估，使得基于标准的强化学习后训练难以扩展。

Method: 使用LLM作为动态批评家识别最可能的失败模式，通过外部验证器验证，联合优化生成器和批评家，减少所需验证次数。

Result: 实验显示RLAC在文本生成中提高事实准确性，在代码生成中提高正确性，优于穷举验证和奖励模型方法。

Conclusion: 动态批评家比固定批评家更有效，RLAC有潜力将强化学习后训练扩展到自由形式生成任务。

Abstract: Open-ended generation tasks require outputs to satisfy diverse and often
implicit task-specific evaluation rubrics. The sheer number of relevant rubrics
leads to prohibitively high verification costs and incomplete assessments of a
response, making reinforcement learning (RL) post-training with rubric-based
rewards difficult to scale. This problem is exacerbated by the fact that often
the best way to combine these rubrics into one single reward is also highly
prompt-specific. We propose Reinforcement Learning with Adversarial Critic
(RLAC), a post-training approach that addresses these challenges via dynamic
rubric verification. Our approach employs a large language model (LLM) as a
critic that dynamically identifies only the most likely failure modes (e.g., a
factual error or unhandled edge case), which are then verified by an external
validator to optimize both generator and critic jointly. By training both the
generator and the critic, this game enhances the critic's error detection and
the generator's output quality while reducing required verifications. Our
experiments demonstrate that RLAC improves factual accuracy in text generation
and correctness in code generation, while also outperforming exhaustive
verification and reward model methods. We show that dynamic critics are more
effective than fixed critics, showcasing the potential of RLAC for scaling RL
post-training to free-form generation tasks.

</details>


### [331] [Transformers as Intrinsic Optimizers: Forward Inference through the Energy Principle](https://arxiv.org/abs/2511.00907)
*Ruifeng Ren,Sheng Ouyang,Huayi Tang,Yong Liu*

Main category: cs.LG

TL;DR: 本文提出了一个基于能量的统一框架来理解Transformer注意力机制，将标准softmax注意力视为最小化亥姆霍兹自由能量的特例，并基于经典梯度下降算法提出了新的注意力结构变体。


<details>
  <summary>Details</summary>
Motivation: Transformer的底层机制仍有待探索，能量视角长期以来为理解神经计算提供了有价值的原则，本文旨在通过能量视角重新理解基于注意力的Transformer模型。

Method: 提出了一个统一的能量框架，包含三个关键组件：全局能量F*、能量函数Ei和梯度下降形式。将标准softmax注意力视为最小化亥姆霍兹自由能量的特例，并基于动量GD、NAG和牛顿法提出了新的注意力结构变体。

Result: 实验初步支持了基于能量框架设计注意力机制的潜力。

Conclusion: 能量框架为理解Transformer注意力机制提供了统一视角，并能自然地扩展到多头设置和线性注意力，基于经典优化算法的新注意力结构展现了设计潜力。

Abstract: Transformers have demonstrated strong adaptability across a wide range of
tasks and have become the backbone of modern Large Language Models (LLMs).
However, their underlying mechanisms remain open for further exploration. The
energy-based perspective has long provided a valuable principle for
understanding neural computation. In this paper, we revisit the principle of
energy as a lens to understand attention-based Transformer models. We present a
unified energy-based framework which is composed of three key components: the
global energy $F^*$, the energy function $E_i$ and the employed gradient
descent (GD) form. Within this framework, standard softmax attention can be
viewed as a special case of minimizing the Helmholtz free energy as $F^*$ using
standard GD when $E_i$ takes the form of elastic potential energy, with
residual connections ensuring that this optimization proceeds in an incremental
manner. In addition, linear attentions can also be naturally incorporated into
this framework by adjusting the corresponding energy forms. We also extend the
above analysis to the multi-head setting, where the energy is defined across
multiple low-dimensional subspaces. Building on this framework, we propose
energy-based modifications of attention structures. Inspired by classical GD
algorithms, we extend the original attention formulation based on standard GD
to the momentum-based GD, Nesterov Accelerated Gradient (NAG), and Newton's
method variants, each inducing a corresponding new attention structure. Our
experiments provide preliminary support for the potential of the energy-based
framework for designing attention mechanisms.

</details>


### [332] [Random Initialization of Gated Sparse Adapters](https://arxiv.org/abs/2511.01794)
*Vi Retault,Yohaï-Eliel Berreby*

Main category: cs.LG

TL;DR: RIGSA是一种新的参数高效微调方法，通过随机初始化全秩适配器、ReZero门控和迭代幅度剪枝来解决语言模型微调中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型在新任务微调时出现的灾难性遗忘问题，提供不施加秩约束的稀疏适配替代方案。

Method: RIGSA方法：从随机初始化的全秩适配器开始，使用ReZero类似的门控机制，通过迭代幅度剪枝进行稀疏化。

Result: 在SmolLM2-1.7B-Instruct模型上，RIGSA能够学习新的视觉文本任务（Textual MNIST），且相比QLoRA显示出更少的遗忘，特别是在GSM8k任务上。

Conclusion: RIGSA在参数数量多于QLoRA的情况下，表现出更少的灾难性遗忘，为参数高效微调提供了有前景的稀疏适配方法。

Abstract: When fine-tuning language models on new tasks, catastrophic forgetting --
performance degradation on previously-learned tasks -- is a ubiquitous problem.
While Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA address this
through low-rank adapters, sparse adaptation offers an alternative that doesn't
impose rank constraints. We introduce Random Initialization of Gated Sparse
Adapters (RIGSA), which starts from randomly-initialized full-rank adapters,
gates them with a ReZero analog, and sparsifies them with iterative magnitude
pruning. We evaluate RIGSA on SmolLM2-1.7B-Instruct using a novel
vision-in-text task (Textual MNIST) and measure forgetting on PIQA, HellaSwag,
and GSM8k. SmolLM2-1.7B-Instruct initially performs around chance level on
Textual MNIST, and is capable of learning the task through RIGSA, 4-bit QLoRA
and random masking. In spite of having more trainable parameters than QLoRA,
the RIGSA configurations that we studied displayed less forgetting than QLoRA,
particularly on GSM8k, though it performs comparably to random masking.

</details>


### [333] [Motion-Robust Multimodal Fusion of PPG and Accelerometer Signals for Three-Class Heart Rhythm Classification](https://arxiv.org/abs/2511.00949)
*Yangyang Zhao,Matti Kaisti,Olli Lahdenoja,Tero Koivisto*

Main category: cs.LG

TL;DR: RhythmiNet是一种结合PPG和加速度计信号的多模态神经网络，通过时间和通道注意力模块改进心房颤动检测，在嘈杂的临床数据中实现三分类（AF、窦性心律、其他心律失常）。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖单通道PPG进行二元AF检测，容易受运动伪影和生理噪声影响，且无法覆盖临床中更广泛的心律失常类型。

Method: 使用残差神经网络结合时间和通道注意力模块，联合利用PPG和加速度计信号进行三分类，测试数据按运动强度分层而不排除任何片段。

Result: RhythmiNet相比仅使用PPG的基线模型在macro-AUC上提升了4.3%，比基于手工HRV特征的逻辑回归模型性能高出12%。

Conclusion: 多模态融合和注意力学习在嘈杂的真实临床数据中具有显著优势，能够提高心律失常检测的鲁棒性和准确性。

Abstract: Atrial fibrillation (AF) is a leading cause of stroke and mortality,
particularly in elderly patients. Wrist-worn photoplethysmography (PPG) enables
non-invasive, continuous rhythm monitoring, yet suffers from significant
vulnerability to motion artifacts and physiological noise. Many existing
approaches rely solely on single-channel PPG and are limited to binary AF
detection, often failing to capture the broader range of arrhythmias
encountered in clinical settings. We introduce RhythmiNet, a residual neural
network enhanced with temporal and channel attention modules that jointly
leverage PPG and accelerometer (ACC) signals. The model performs three-class
rhythm classification: AF, sinus rhythm (SR), and Other. To assess robustness
across varying movement conditions, test data are stratified by
accelerometer-based motion intensity percentiles without excluding any
segments. RhythmiNet achieved a 4.3% improvement in macro-AUC over the PPG-only
baseline. In addition, performance surpassed a logistic regression model based
on handcrafted HRV features by 12%, highlighting the benefit of multimodal
fusion and attention-based learning in noisy, real-world clinical data.

</details>


### [334] [Using Synthetic Data to estimate the True Error is theoretically and practically doable](https://arxiv.org/abs/2511.00964)
*Hai Hoang Thanh,Duy-Tung Nguyen,Hung The Tran,Khoat Than*

Main category: cs.LG

TL;DR: 本文提出了一种使用优化合成数据来评估机器学习模型性能的新方法，解决了在有限标注数据条件下模型评估的理论挑战。


<details>
  <summary>Details</summary>
Motivation: 在现实应用中，获取大量标注测试数据成本高昂且耗时，传统评估方法在有限标注数据条件下不可靠。生成模型的发展为通过合成数据评估模型性能提供了可能。

Method: 开发了考虑合成数据的泛化边界理论，提出基于理论指导的优化合成数据生成方法，用于模型测试误差估计。

Result: 在模拟和表格数据集上的实验表明，相比现有基线方法，该方法能够获得更准确可靠的测试误差估计。

Conclusion: 合成数据可以有效地用于模型性能评估，生成器质量对评估结果有重要影响，理论指导的优化方法优于传统方法。

Abstract: Accurately evaluating model performance is crucial for deploying machine
learning systems in real-world applications. Traditional methods often require
a sufficiently large labeled test set to ensure a reliable evaluation. However,
in many contexts, a large labeled dataset is costly and labor-intensive.
Therefore, we sometimes have to do evaluation by a few labeled samples, which
is theoretically challenging. Recent advances in generative models offer a
promising alternative by enabling the synthesis of high-quality data. In this
work, we make a systematic investigation about the use of synthetic data to
estimate the test error of a trained model under limited labeled data
conditions. To this end, we develop novel generalization bounds that take
synthetic data into account. Those bounds suggest novel ways to optimize
synthetic samples for evaluation and theoretically reveal the significant role
of the generator's quality. Inspired by those bounds, we propose a
theoretically grounded method to generate optimized synthetic data for model
evaluation. Experimental results on simulation and tabular datasets demonstrate
that, compared to existing baselines, our method achieves accurate and more
reliable estimates of the test error.

</details>


### [335] [Modeling Microenvironment Trajectories on Spatial Transcriptomics with NicheFlow](https://arxiv.org/abs/2511.00977)
*Kristiyan Sakalyan,Alessandro Palma,Filippo Guerranti,Fabian J. Theis,Stephan Günnemann*

Main category: cs.LG

TL;DR: NicheFlow是一种基于流的生成模型，用于推断连续空间切片中细胞微环境的时空轨迹，通过将局部细胞邻域表示为点云，联合建模细胞状态和空间坐标的演化。


<details>
  <summary>Details</summary>
Motivation: 理解细胞微环境的时空演化对于解析组织发育和疾病进展至关重要。现有的单细胞水平建模方法忽略了组织中细胞状态的协调发育。

Method: 使用最优传输和变分流匹配，将局部细胞邻域表示为点云，联合建模细胞状态和空间坐标的演化。

Result: 该方法在从胚胎到大脑发育的多种时空数据集中成功恢复了全局空间结构和局部微环境组成。

Conclusion: NicheFlow能够有效推断细胞微环境的时空演化轨迹，为理解组织发育提供了新工具。

Abstract: Understanding the evolution of cellular microenvironments in spatiotemporal
data is essential for deciphering tissue development and disease progression.
While experimental techniques like spatial transcriptomics now enable
high-resolution mapping of tissue organization across space and time, current
methods that model cellular evolution operate at the single-cell level,
overlooking the coordinated development of cellular states in a tissue. We
introduce NicheFlow, a flow-based generative model that infers the temporal
trajectory of cellular microenvironments across sequential spatial slides. By
representing local cell neighborhoods as point clouds, NicheFlow jointly models
the evolution of cell states and spatial coordinates using optimal transport
and Variational Flow Matching. Our approach successfully recovers both global
spatial architecture and local microenvironment composition across diverse
spatiotemporal datasets, from embryonic to brain development.

</details>


### [336] [Balanced Multimodal Learning via Mutual Information](https://arxiv.org/abs/2511.00987)
*Rongrong Xie,Guido Sanguinetti*

Main category: cs.LG

TL;DR: 提出了一种解决多模态学习中模态不平衡问题的统一框架，通过互信息量化模态间交互，采用跨模态知识蒸馏和多任务式训练来平衡不同模态的贡献。


<details>
  <summary>Details</summary>
Motivation: 多模态学习中的模态不平衡问题（如数据获取不足、质量差异）在生物数据分析中尤为突出，传统方法难以同时利用模态间协同效应和解决模态冲突。

Method: 采用两阶段平衡多模态学习策略：1）跨模态知识蒸馏预训练，用强模态增强弱模态预测能力；2）多任务式训练，基于模态性能指标和互信息动态校准梯度贡献。

Result: 该方法有效缓解了模态不平衡问题，显著提升了多模态模型的整体性能。

Conclusion: 提出的统一框架通过互信息量化和平衡学习策略，成功解决了多模态学习中的模态不平衡问题，为生物数据分析等场景提供了有效解决方案。

Abstract: Multimodal learning has increasingly become a focal point in research,
primarily due to its ability to integrate complementary information from
diverse modalities. Nevertheless, modality imbalance, stemming from factors
such as insufficient data acquisition and disparities in data quality, has
often been inadequately addressed. This issue is particularly prominent in
biological data analysis, where datasets are frequently limited, costly to
acquire, and inherently heterogeneous in quality. Conventional multimodal
methodologies typically fall short in concurrently harnessing intermodal
synergies and effectively resolving modality conflicts.
  In this study, we propose a novel unified framework explicitly designed to
address modality imbalance by utilizing mutual information to quantify
interactions between modalities. Our approach adopts a balanced multimodal
learning strategy comprising two key stages: cross-modal knowledge distillation
(KD) and a multitask-like training paradigm. During the cross-modal KD
pretraining phase, stronger modalities are leveraged to enhance the predictive
capabilities of weaker modalities. Subsequently, our primary training phase
employs a multitask-like learning mechanism, dynamically calibrating gradient
contributions based on modality-specific performance metrics and intermodal
mutual information. This approach effectively alleviates modality imbalance,
thereby significantly improving overall multimodal model performance.

</details>


### [337] [Hydra: Dual Exponentiated Memory for Multivariate Time Series Analysis](https://arxiv.org/abs/2511.00989)
*Asal Meskin,Alireza Mirrokni,Ali Najar,Ali Behrouz*

Main category: cs.LG

TL;DR: Hydra是一个双头元上下文记忆模块，通过二维递归在时间和变量维度上学习记忆模式，解决了现有时间序列模型在时间归纳偏置、变量间依赖关系和长期建模效率方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列模型（如Transformer、MLP、线性模型）存在三个主要问题：(1)缺乏时间归纳偏置，(2)无法捕捉时间和变量维度间的相互依赖，(3)长期建模效率低下。线性RNN虽然解决了部分问题，但仅限于单序列且会传播误差。

Method: 提出Hydra模型，采用双头元上下文记忆模块，通过二维递归在时间和变量维度上同时处理信息。虽然训练过程是非并行的，但提出了2D分块训练算法，在保持有效性的同时实现了10倍的效率提升。

Result: 在时间序列预测、分类和异常检测等多个任务和数据集上的实验结果表明，Hydra相比最先进的基线方法表现出优越性能。

Conclusion: Hydra通过二维递归设计有效解决了现有时间序列模型的局限性，在多个任务上实现了state-of-the-art性能，同时通过创新的训练算法保证了效率。

Abstract: In recent years, effectively modeling multivariate time series has gained
significant popularity, mainly due to its wide range of applications, ranging
from healthcare to financial markets and energy management. Transformers, MLPs,
and linear models as the de facto backbones of modern time series models have
shown promising results in single-variant and/or short-term forecasting. These
models, however: (1) are permutation equivariant and so lack temporal inductive
bias, being less expressive to capture the temporal dynamics; (2) are naturally
designed for univariate setup, missing the inter-dependencies of temporal and
variate dimensions; and/or (3) are inefficient for Long-term time series
modeling. To overcome training and inference efficiency as well as the lack of
temporal inductive bias, recently, linear Recurrent Neural Networks (RNNs) have
gained attention as an alternative to Transformer-based models. These models,
however, are inherently limited to a single sequence, missing inter-variate
dependencies, and can propagate errors due to their additive nature. In this
paper, we present Hydra, a by-design two-headed meta in-context memory module
that learns how to memorize patterns at test time by prioritizing time series
patterns that are more informative about the data. Hydra uses a 2-dimensional
recurrence across both time and variate at each step, which is more powerful
than mixing methods. Although the 2-dimensional nature of the model makes its
training recurrent and non-parallelizable, we present a new 2D-chunk-wise
training algorithm that approximates the actual recurrence with $\times 10$
efficiency improvement, while maintaining the effectiveness. Our experimental
results on a diverse set of tasks and datasets, including time series
forecasting, classification, and anomaly detection show the superior
performance of Hydra compared to state-of-the-art baselines.

</details>


### [338] [None To Optima in Few Shots: Bayesian Optimization with MDP Priors](https://arxiv.org/abs/2511.01006)
*Diantong Li,Kyunghyun Cho,Chong Liu*

Main category: cs.LG

TL;DR: 提出了ProfBO算法，通过MDP先验建模优化轨迹，显著减少贝叶斯优化所需的函数评估次数


<details>
  <summary>Details</summary>
Motivation: 贝叶斯优化在现实应用中评估成本高昂，需要减少评估次数

Method: 使用MDP先验捕获相关任务的优化轨迹知识，通过元学习快速适应新任务

Result: 在Covid、癌症基准和超参数调优任务中，ProfBO以更少评估获得更高质量解

Conclusion: ProfBO在减少评估次数方面优于现有方法，适合实际部署

Abstract: Bayesian Optimization (BO) is an efficient tool for optimizing black-box
functions, but its theoretical guarantees typically hold in the asymptotic
regime. In many critical real-world applications such as drug discovery or
materials design, where each evaluation can be very costly and time-consuming,
BO becomes impractical for many evaluations. In this paper, we introduce the
Procedure-inFormed BO (ProfBO) algorithm, which solves black-box optimization
with remarkably few function evaluations. At the heart of our algorithmic
design are Markov Decision Process (MDP) priors that model optimization
trajectories from related source tasks, thereby capturing procedural knowledge
on efficient optimization. We embed these MDP priors into a prior-fitted neural
network and employ model-agnostic meta-learning for fast adaptation to new
target tasks. Experiments on real-world Covid and Cancer benchmarks and
hyperparameter tuning tasks demonstrate that ProfBO consistently outperforms
state-of-the-art methods by achieving high-quality solutions with significantly
fewer evaluations, making it ready for practical deployment.

</details>


### [339] [Equality Graph Assisted Symbolic Regression](https://arxiv.org/abs/2511.01009)
*Fabricio Olivetti de Franca,Gabriel Kronberger*

Main category: cs.LG

TL;DR: 提出SymRegg算法，利用等式图(e-graph)结构避免符号回归中重复计算，提高搜索效率


<details>
  <summary>Details</summary>
Motivation: 遗传编程在符号回归中需要计算大量冗余表达式(可达总评估数的60%)，通过e-graph结构可以避免重复计算

Method: 基于e-graph结构的新搜索算法：从e-graph中采样表达式进行扰动，若生成未访问表达式则插入e-graph并生成等价形式

Result: SymRegg提高了搜索效率，在不同数据集上保持准确结果，且只需极简超参数设置

Conclusion: e-graph结构能有效提升符号回归搜索效率，避免冗余计算

Abstract: In Symbolic Regression (SR), Genetic Programming (GP) is a popular search
algorithm that delivers state-of-the-art results in term of accuracy. Its
success relies on the concept of neutrality, which induces large plateaus that
the search can safely navigate to more promising regions. Navigating these
plateaus, while necessary, requires the computation of redundant expressions,
up to 60% of the total number of evaluation, as noted in a recent study. The
equality graph (e-graph) structure can compactly store and group equivalent
expressions enabling us to verify if a given expression and their variations
were already visited by the search, thus enabling us to avoid unnecessary
computation. We propose a new search algorithm for symbolic regression called
SymRegg that revolves around the e-graph structure following simple steps:
perturb solutions sampled from a selection of expressions stored in the
e-graph, if it generates an unvisited expression, insert it into the e-graph
and generates its equivalent forms. We show that SymRegg is capable of
improving the efficiency of the search, maintaining consistently accurate
results across different datasets while requiring a choice of a minimalist set
of hyperparameters.

</details>


### [340] [What's the next frontier for Data-centric AI? Data Savvy Agents](https://arxiv.org/abs/2511.01015)
*Nabeel Seedat,Jiashuo Liu,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: 论文提出AI智能体应具备数据感知能力，包括主动数据获取、复杂数据处理、交互式测试数据合成和持续适应，以实现可靠的实际部署。


<details>
  <summary>Details</summary>
Motivation: 当前AI智能体研究主要关注推理能力，但数据处理的不足限制了智能体在现实世界中的可靠部署。需要让智能体具备持续获取、处理和演进数据的能力。

Method: 提出四个关键能力：主动数据获取（自主收集任务关键知识或寻求人类输入填补数据空白）、复杂数据处理（上下文感知和灵活处理多样化数据挑战）、交互式测试数据合成（从静态基准转向动态生成的交互式测试数据）、持续适应（迭代优化数据和背景知识以适应环境变化）。

Result: 通过强调数据感知能力，为构建更可靠的AI智能体系统提供了新的设计方向。

Conclusion: 数据感知应成为AI智能体设计的首要考虑因素，这是数据为中心AI的下一个前沿领域。

Abstract: The recent surge in AI agents that autonomously communicate, collaborate with
humans and use diverse tools has unlocked promising opportunities in various
real-world settings. However, a vital aspect remains underexplored: how agents
handle data. Scalable autonomy demands agents that continuously acquire,
process, and evolve their data. In this paper, we argue that data-savvy
capabilities should be a top priority in the design of agentic systems to
ensure reliable real-world deployment. Specifically, we propose four key
capabilities to realize this vision: (1) Proactive data acquisition: enabling
agents to autonomously gather task-critical knowledge or solicit human input to
address data gaps; (2) Sophisticated data processing: requiring context-aware
and flexible handling of diverse data challenges and inputs; (3) Interactive
test data synthesis: shifting from static benchmarks to dynamically generated
interactive test data for agent evaluation; and (4) Continual adaptation:
empowering agents to iteratively refine their data and background knowledge to
adapt to shifting environments. While current agent research predominantly
emphasizes reasoning, we hope to inspire a reflection on the role of data-savvy
agents as the next frontier in data-centric AI.

</details>


### [341] [SARIMAX-Based Power Outage Prediction During Extreme Weather Events](https://arxiv.org/abs/2511.01017)
*Haoran Ye,Qiuzhuang Sun,Yang Yang*

Main category: cs.LG

TL;DR: 开发基于SARIMAX的短期电力中断预测系统，用于极端天气事件期间的中断预测，通过特征工程和稳健优化策略，在24-48小时预测范围内比基线方法提升8.4%性能。


<details>
  <summary>Details</summary>
Motivation: 极端天气事件导致电力中断频发，需要准确的短期预测系统来支持应急响应和资源调配，但现有方法在特征处理和模型稳定性方面存在不足。

Method: 采用两阶段特征工程：数据清洗去除零方差和未知特征，相关性过滤消除高度相关预测变量；使用时间嵌入、多尺度滞后特征和天气变量作为SARIMAX模型的外生输入；通过标准化、分层拟合策略和自动降级机制处理数据不规则性和数值不稳定性。

Result: 模型在短期（24小时）和中长期（48小时）预测范围内达到RMSE 177.2，相比基线方法（RMSE 193.4）提升8.4%，验证了特征工程和优化策略的有效性。

Conclusion: 提出的SARIMAX预测系统结合系统化特征工程和稳健优化策略，能够有效预测极端天气相关的电力中断，为应急管理提供可靠支持。

Abstract: This study develops a SARIMAX-based prediction system for short-term power
outage forecasting during extreme weather events. Using hourly data from
Michigan counties with outage counts and comprehensive weather features, we
implement a systematic two-stage feature engineering pipeline: data cleaning to
remove zero-variance and unknown features, followed by correlation-based
filtering to eliminate highly correlated predictors. The selected features are
augmented with temporal embeddings, multi-scale lag features, and weather
variables with their corresponding lags as exogenous inputs to the SARIMAX
model. To address data irregularity and numerical instability, we apply
standardization and implement a hierarchical fitting strategy with sequential
optimization methods, automatic downgrading to ARIMA when convergence fails,
and historical mean-based fallback predictions as a final safeguard. The model
is optimized separately for short-term (24 hours) and medium-term (48 hours)
forecast horizons using RMSE as the evaluation metric. Our approach achieves an
RMSE of 177.2, representing an 8.4\% improvement over the baseline method (RMSE
= 193.4), thereby validating the effectiveness of our feature engineering and
robust optimization strategy for extreme weather-related outage prediction.

</details>


### [342] [MedEqualizer: A Framework Investigating Bias in Synthetic Medical Data and Mitigation via Augmentation](https://arxiv.org/abs/2511.01054)
*Sama Salarian,Yue Zhang,Swati Padhee,Srinivasan Parthasarathy*

Main category: cs.LG

TL;DR: 评估GAN模型生成医疗数据的公平性，发现存在人口统计学子群代表性失衡问题，提出MedEqualizer框架来改善合成数据的公平性。


<details>
  <summary>Details</summary>
Motivation: 合成医疗数据可提高数据可访问性，但需要确保跨保护属性的公平性，避免临床研究和决策中的偏见。

Method: 使用MIMIC-III数据集评估多种GAN模型的公平性，测量子群代表性，并开发MedEqualizer模型无关的增强框架来丰富代表性不足的子群。

Result: 观察到合成数据中许多子群代表性显著失衡，MedEqualizer显著改善了合成数据的人口统计学平衡。

Conclusion: MedEqualizer为更公平和具代表性的医疗数据合成提供了可行路径。

Abstract: Synthetic healthcare data generation presents a viable approach to enhance
data accessibility and support research by overcoming limitations associated
with real-world medical datasets. However, ensuring fairness across protected
attributes in synthetic data is critical to avoid biased or misleading results
in clinical research and decision-making. In this study, we assess the fairness
of synthetic data generated by multiple generative adversarial network
(GAN)-based models using the MIMIC-III dataset, with a focus on
representativeness across protected demographic attributes. We measure subgroup
representation using the logarithmic disparity metric and observe significant
imbalances, with many subgroups either underrepresented or overrepresented in
the synthetic data, compared to the real data. To mitigate these disparities,
we introduce MedEqualizer, a model-agnostic augmentation framework that
enriches the underrepresented subgroups prior to synthetic data generation. Our
results show that MedEqualizer significantly improves demographic balance in
the resulting synthetic datasets, offering a viable path towards more equitable
and representative healthcare data synthesis.

</details>


### [343] [Window-Based Feature Engineering for Cognitive Workload Detection](https://arxiv.org/abs/2511.01060)
*Andrew Hallam,R G Gayathri,Glory Lee,Atul Sajjanhar*

Main category: cs.LG

TL;DR: 使用COLET数据集，通过基于窗口的时序特征提取和机器学习/深度学习技术对认知负荷进行分类，发现深度学习模型在精度、F1分数和准确率方面优于传统机器学习方法。


<details>
  <summary>Details</summary>
Motivation: 认知负荷在健康、心理学和国防应用等领域日益受到关注，需要开发有效的实时评估方法。

Method: 采用基于窗口的时序分区方法增强特征提取，然后应用机器学习和深度学习模型进行认知负荷水平分类。

Result: 深度学习模型，特别是表格架构模型，在精度、F1分数、准确率和分类精度方面均优于传统机器学习方法。

Conclusion: 基于窗口的时序特征提取和深度学习技术对于复杂动态任务中的实时认知负荷评估具有显著效果和潜力。

Abstract: Cognitive workload is a topic of increasing interest across various fields
such as health, psychology, and defense applications. In this research, we
focus on classifying cognitive workload using the COLET dataset, employing a
window-based approach for feature generation and machine/deep learning
techniques for classification. We apply window-based temporal partitioning to
enhance features used in existing research, followed by machine learning and
deep learning models to classify different levels of cognitive workload. The
results demonstrate that deep learning models, particularly tabular
architectures, outperformed traditional machine learning methods in precision,
F1-score, accuracy, and classification precision. This study highlights the
effectiveness of window-based temporal feature extraction and the potential of
deep learning techniques for real-time cognitive workload assessment in complex
and dynamic tasks.

</details>


### [344] [Energy-Efficient Deep Learning Without Backpropagation: A Rigorous Evaluation of Forward-Only Algorithms](https://arxiv.org/abs/2511.01061)
*Przemysław Spyra,Witold Dzwinel*

Main category: cs.LG

TL;DR: 该论文挑战了反向传播(BP)对高性能至关重要的传统假设，提出Mono-Forward(MF)算法在MLP架构上超越BP基准，实现更好的泛化性能和显著的效率提升。


<details>
  <summary>Details</summary>
Motivation: 挑战反向传播(BP)对最先进性能不可或缺的传统假设，探索BP-free方法的潜力。

Method: 从Hinton的Forward-Forward(FF)算法演进到Cascaded Forward(CaFo)，最终提出Mono-Forward(MF)算法，在相同架构和超参数优化框架下进行公平比较。

Result: MF算法在分类准确率上持续超越优化调优的BP基准，能耗降低41%，训练速度提升34%，但实际内存效率可能被开销抵消。

Conclusion: MF算法为MLP提供了一种实用、高性能且可持续的BP替代方案。

Abstract: The long-held assumption that backpropagation (BP) is essential for
state-of-the-art performance is challenged by this work. We present rigorous,
hardware-validated evidence that the Mono-Forward (MF) algorithm, a
backpropagation-free method, consistently surpasses an optimally tuned BP
baseline in classification accuracy on its native Multi-Layer Perceptron (MLP)
architectures. This superior generalization is achieved with profound
efficiency gains, including up to 41% less energy consumption and up to 34%
faster training. Our analysis, which charts an evolutionary path from Geoffrey
Hinton's Forward-Forward (FF) to the Cascaded Forward (CaFo) and finally to MF,
is grounded in a fair comparative framework using identical architectures and
universal hyperparameter optimization. We further provide a critical
re-evaluation of memory efficiency in BP-free methods, empirically
demonstrating that practical overhead can offset theoretical gains. Ultimately,
this work establishes MF as a practical, high-performance, and sustainable
alternative to BP for MLPs.

</details>


### [345] [AI Progress Should Be Measured by Capability-Per-Resource, Not Scale Alone: A Framework for Gradient-Guided Resource Allocation in LLMs](https://arxiv.org/abs/2511.01077)
*David McCoy,Yulun Wu,Zachary Butzin-Dozier*

Main category: cs.LG

TL;DR: 本文挑战AI研究中的“规模原教旨主义”，提出基于梯度影响模式的资源分配框架，通过仅更新高影响力参数和协调参数数据选择，显著提高AI生命周期效率。


<details>
  <summary>Details</summary>
Motivation: 当前AI研究中模型规模和计算量的无限增长导致了不可持续的环境影响和资源不平等，需要从根本上重新定位LLM开发方向，关注能力-资源比而非单纯能力。

Method: 提出基于梯度影响模式的资源分配理论框架，利用梯度范数识别高影响力参数，仅更新这些参数，并协调参数和数据选择。建立两阶段范式：边际收益预训练和影响引导适应，通过梯度蓝图连接。

Result: 分析表明在基于transformer的模型中，仅更新高影响力参数在性能-资源比上严格优于全参数调优；简单梯度范数可高效识别高影响力组件；协调参数和数据选择可成倍提高效率，可能将资源需求降低数个数量级。

Conclusion: 能力-资源视角将硬件变通方案转变为理论最优策略，通过将资源意识嵌入模型开发、适应和评估过程，可重塑AI进步方向，实现更可持续和公平的未来。

Abstract: This position paper challenges the "scaling fundamentalism" dominating AI
research, where unbounded growth in model size and computation has led to
unsustainable environmental impacts and widening resource inequality. We argue
that LLM development should be fundamentally reoriented toward
capability-per-resource rather than capability alone. We present a theoretical
framework demonstrating that resource-allocation decisions guided by gradient
influence patterns can dramatically improve efficiency throughout the AI
lifecycle. Our analysis shows that in transformer-based models, where a small
fraction of parameters exert outsized influence (following heavy-tailed
distributions), three critical insights emerge: (1) updating only
high-influence parameters strictly outperforms full-parameter tuning on a
performance-per-resource basis; (2) simple gradient norms provide
computationally efficient proxies for identifying these high-influence
components; and (3) coordinated parameter and data selection yields
multiplicative efficiency gains, potentially reducing resource requirements by
orders of magnitude. Building on these theoretical foundations, we propose a
two stage paradigm marginal-return pretraining for foundation developers and
influence guided adaptation for downstream users bridged by gradient
blueprints, metadata describing which parameters matter most for various tasks.
This capability-per-resource perspective transforms what were once considered
pragmatic hardware workarounds into theoretically optimal strategies,
democratizing access to cutting-edge AI capabilities while significantly
reducing environmental impact. By embedding resource consciousness into how we
develop, adapt, and evaluate models, we can reshape AI progress toward a more
sustainable and equitable future.

</details>


### [346] [Continual Learning, Not Training: Online Adaptation For Agents](https://arxiv.org/abs/2511.01093)
*Aman Jaglan,Jarrod Barnes*

Main category: cs.LG

TL;DR: ATLAS提出了一种双智能体架构，通过分离推理（教师）和执行（学生）功能，结合持久学习记忆实现梯度自由的持续学习，在推理时动态调整操作策略。


<details>
  <summary>Details</summary>
Motivation: 传统持续学习方法依赖基于梯度的重新训练，不适合需要实时适应的部署智能体。

Method: 采用双智能体架构（教师-学生），包含持久学习记忆存储经验提炼的指导，通过编排层在推理时动态调整操作策略。

Result: 在微软ExCyTIn-Bench基准测试中，使用GPT-5-mini作为学生智能体达到54.1%成功率，比GPT-5（High）高13%，成本降低86%。跨事件验证显示泛化能力：冻结的小册子将准确率从28%提升到41%。

Conclusion: 梯度自由持续学习是构建自适应、可部署AI系统的可行路径，为训练显式世界模型提供了因果注释轨迹。

Abstract: Continual Learning (CL) methods have traditionally focused on mitigating
catastrophic forgetting through gradient-based retraining, an approach
ill-suited for deployed agents that must adapt in real time. We introduce our
Adaptive Teaching and Learning System (ATLAS), a dual-agent architecture that
decouples reasoning (Teacher) from execution (Student) and incorporates a
persistent learning memory that stores distilled guidance from experience. This
informs the orchestration layer, enabling the system to dynamically adjust its
operational strategies, such as supervision level or initial plan selection, at
inference time. In doing so, ATLAS achieves gradient-free continual learning,
shifting the locus of adaptation from model parameters to system-level
orchestration. We formulate this as a system-centric paradigm for continual
learning, where the objective is adaptive efficiency: maximizing task success
while minimizing computational cost through inference-time orchestration rather
than parameter updates. Evaluated on Microsoft's ExCyTIn-Bench, an open-source
benchmark simulating complex cyberthreat investigation, ATLAS achieves 54.1%
success with GPT-5-mini as its Student, outperforming the larger GPT-5 (High)
by 13% while reducing cost by 86%. Cross-incident validation demonstrates
generalization: frozen pamphlets from Incident #5 improve accuracy from 28% to
41% with zero retraining, while shifting output composition from verbose
exploration to structured reasoning. Together, these findings establish
gradient-free continual learning as a viable path toward adaptive, deployable
AI systems and provide causally annotated traces valuable for training explicit
world models.

</details>


### [347] [Adapt under Attack and Domain Shift: Unified Adversarial Meta-Learning and Domain Adaptation for Robust Automatic Modulation Classification](https://arxiv.org/abs/2511.01172)
*Ali Owfi,Amirmohammad Bamdad,Tolunay Seyfi,Fatemeh Afghah*

Main category: cs.LG

TL;DR: 提出一个结合元学习和领域适应的统一框架，使自动调制分类系统能够同时抵抗对抗攻击和环境变化。


<details>
  <summary>Details</summary>
Motivation: 深度学习在自动调制分类中表现出色，但易受对抗攻击和数据分布变化影响，阻碍了在实际动态环境中的部署。

Method: 采用两阶段策略：离线阶段使用元学习在单一源域上训练模型，使其对未见过的攻击具有泛化防御能力；在线阶段应用领域适应将模型特征与新目标域对齐，无需大量标注数据。

Result: 该框架显著提高了调制分类在对抗攻击和环境变化下的准确率。

Conclusion: 该框架为解决现代AMC系统的部署和操作挑战提供了关键解决方案。

Abstract: Deep learning has emerged as a leading approach for Automatic Modulation
Classification (AMC), demonstrating superior performance over traditional
methods. However, vulnerability to adversarial attacks and susceptibility to
data distribution shifts hinder their practical deployment in real-world,
dynamic environments. To address these threats, we propose a novel, unified
framework that integrates meta-learning with domain adaptation, making AMC
systems resistant to both adversarial attacks and environmental changes. Our
framework utilizes a two-phase strategy. First, in an offline phase, we employ
a meta-learning approach to train the model on clean and adversarially
perturbed samples from a single source domain. This method enables the model to
generalize its defense, making it resistant to a combination of previously
unseen attacks. Subsequently, in the online phase, we apply domain adaptation
to align the model's features with a new target domain, allowing it to adapt
without requiring substantial labeled data. As a result, our framework achieves
a significant improvement in modulation classification accuracy against these
combined threats, offering a critical solution to the deployment and
operational challenges of modern AMC systems.

</details>


### [348] [A Comparative Study of Model Adaptation Strategies for Multi-Treatment Uplift Modeling](https://arxiv.org/abs/2511.01185)
*Ruyue Zhang,Xiaopeng Ke,Ming Liu,Fangzhou Shi,Chang Men,Zhengdan Zhu*

Main category: cs.LG

TL;DR: 本文提出了正交函数适应(OFA)方法，用于提升多治疗场景下的提升建模效果，相比现有的结构适应和特征适应方法，OFA在噪声数据和观测数据混合等复杂数据特征下表现更优。


<details>
  <summary>Details</summary>
Motivation: 当前多治疗提升建模技术通常从二元治疗工作改编而来，分为结构适应和特征适应两类，但这些方法在各种数据特征下无法保持有效性。

Method: 基于函数逼近定理，提出了正交函数适应(OFA)方法，通过正交函数来增强估计能力和鲁棒性。

Result: 实验结果表明，OFA相比其他普通适应方法能显著提升提升模型性能，并展现出最高的鲁棒性。

Conclusion: OFA方法在多治疗提升建模中具有显著优势，特别是在复杂数据环境下表现出更强的适应性和鲁棒性。

Abstract: Uplift modeling has emerged as a crucial technique for individualized
treatment effect estimation, particularly in fields such as marketing and
healthcare. Modeling uplift effects in multi-treatment scenarios plays a key
role in real-world applications. Current techniques for modeling
multi-treatment uplift are typically adapted from binary-treatment works. In
this paper, we investigate and categorize all current model adaptations into
two types: Structure Adaptation and Feature Adaptation. Through our empirical
experiments, we find that these two adaptation types cannot maintain
effectiveness under various data characteristics (noisy data, mixed with
observational data, etc.). To enhance estimation ability and robustness, we
propose Orthogonal Function Adaptation (OFA) based on the function
approximation theorem. We conduct comprehensive experiments with multiple data
characteristics to study the effectiveness and robustness of all model
adaptation techniques. Our experimental results demonstrate that our proposed
OFA can significantly improve uplift model performance compared to other
vanilla adaptation methods and exhibits the highest robustness.

</details>


### [349] [Transmitter Identification and Protocol Categorization in Shared Spectrum via Multi-Task RF Classification at the Network Edge](https://arxiv.org/abs/2511.01198)
*Tariq Abdul-Quddoos,Tasnia Sharmin,Xiangfang Li,Lijun Qian*

Main category: cs.LG

TL;DR: 提出一个基于多任务RF信号分类的鲁棒框架，用于共享频谱环境中的发射机识别和协议分类，使用CNN处理信号重叠和环境变化等挑战，在POWDER平台数据上取得了高准确率。


<details>
  <summary>Details</summary>
Motivation: 随着频谱共享日益重要，频谱监测和发射机识别对于执行频谱使用政策、提高频谱利用效率和保障网络安全至关重要。

Method: 设计卷积神经网络(CNN)，采用多通道输入策略提取有意义的信号特征，解决信号特征重叠和环境变化等关键挑战。

Result: 在POWDER平台RF数据上取得了显著准确率：协议分类90%，发射基站分类100%，联合分类任务92%。

Conclusion: 该方法在增强现代无线网络频谱监测、管理和安全方面具有显著潜力。

Abstract: As spectrum sharing becomes increasingly vital to meet rising wireless
demands in the future, spectrum monitoring and transmitter identification are
indispensable for enforcing spectrum usage policy, efficient spectrum
utilization, and net- work security. This study proposed a robust framework for
transmitter identification and protocol categorization via multi- task RF
signal classification in shared spectrum environments, where the spectrum
monitor will classify transmission protocols (e.g., 4G LTE, 5G-NR, IEEE
802.11a) operating within the same frequency bands, and identify different
transmitting base stations, as well as their combinations. A Convolutional
Neural Network (CNN) is designed to tackle critical challenges such as
overlapping signal characteristics and environmental variability. The proposed
method employs a multi-channel input strategy to extract meaningful signal
features, achieving remarkable accuracy: 90% for protocol classification, 100%
for transmitting base station classification, and 92% for joint classification
tasks, utilizing RF data from the POWDER platform. These results highlight the
significant potential of the proposed method to enhance spectrum monitoring,
management, and security in modern wireless networks.

</details>


### [350] [Optimizing Electric Vehicle Charging Station Placement Using Reinforcement Learning and Agent-Based Simulations](https://arxiv.org/abs/2511.01218)
*Minh-Duc Nguyen,Dung D. Le,Phi Long Nguyen*

Main category: cs.LG

TL;DR: 提出了一种结合深度强化学习和基于代理模拟的新框架，用于优化电动汽车充电站布局，通过混合奖励函数和双Q网络来应对现实世界的不确定性。


<details>
  <summary>Details</summary>
Motivation: 电动汽车快速增长需要优化充电站布局，但现有强化学习方法因确定性奖励系统无法充分应对动态现实条件，导致评估成本高且不反映真实场景。

Method: 集成深度强化学习与基于代理模拟，使用具有双Q网络的混合RL代理来选择最佳位置和配置充电端口，采用结合确定性因素和模拟反馈的混合奖励函数。

Result: 在越南河内的案例研究中，该方法相比初始状态将平均等待时间减少了53.28%，优于静态基线方法。

Conclusion: 该可扩展的自适应解决方案增强了电动汽车基础设施规划，有效应对现实世界复杂性并改善用户体验。

Abstract: The rapid growth of electric vehicles (EVs) necessitates the strategic
placement of charging stations to optimize resource utilization and minimize
user inconvenience. Reinforcement learning (RL) offers an innovative approach
to identifying optimal charging station locations; however, existing methods
face challenges due to their deterministic reward systems, which limit
efficiency. Because real-world conditions are dynamic and uncertain, a
deterministic reward structure cannot fully capture the complexities of
charging station placement. As a result, evaluation becomes costly and
time-consuming, and less reflective of real-world scenarios. To address this
challenge, we propose a novel framework that integrates deep RL with
agent-based simulations to model EV movement and estimate charging demand in
real time. Our approach employs a hybrid RL agent with dual Q-networks to
select optimal locations and configure charging ports, guided by a hybrid
reward function that combines deterministic factors with simulation-derived
feedback. Case studies in Hanoi, Vietnam, show that our method reduces average
waiting times by 53.28% compared to the initial state, outperforming static
baseline methods. This scalable and adaptive solution enhances EV
infrastructure planning, effectively addressing real-world complexities and
improving user experience.

</details>


### [351] [WindMiL: Equivariant Graph Learning for Wind Loading Prediction](https://arxiv.org/abs/2511.01226)
*Themistoklis Vargiemezis,Charilaos Kanatsoulis,Catherine Gorlé*

Main category: cs.LG

TL;DR: WindMiL是一个结合系统数据集生成和对称感知图神经网络的机器学习框架，用于高效预测建筑物风荷载，替代昂贵的传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统风洞测试和大涡模拟计算成本高昂（每个LES案例需24小时），无法进行大规模参数研究，需要开发高效的风荷载预测方法。

Method: 1) 通过符号距离函数插值生成屋顶几何形状，模拟462个不同形状和风向的LES案例构建大规模数据集；2) 开发反射等变图神经网络，确保在镜像几何下物理一致的预测。

Result: WindMiL在插值和外推评估中均实现高精度，表面压力系数的均值和标准差RMSE≤0.02，在反射测试中保持96%以上的命中率，而非等变基线模型下降超过10%。

Conclusion: 通过将系统数据集与等变代理模型配对，WindMiL实现了建筑物风荷载的高效、可扩展和准确预测。

Abstract: Accurate prediction of wind loading on buildings is crucial for structural
safety and sustainable design, yet conventional approaches such as wind tunnel
testing and large-eddy simulation (LES) are prohibitively expensive for
large-scale exploration. Each LES case typically requires at least 24 hours of
computation, making comprehensive parametric studies infeasible. We introduce
WindMiL, a new machine learning framework that combines systematic dataset
generation with symmetry-aware graph neural networks (GNNs). First, we
introduce a large-scale dataset of wind loads on low-rise buildings by applying
signed distance function interpolation to roof geometries and simulating 462
cases with LES across varying shapes and wind directions. Second, we develop a
reflection-equivariant GNN that guarantees physically consistent predictions
under mirrored geometries. Across interpolation and extrapolation evaluations,
WindMiL achieves high accuracy for both the mean and the standard deviation of
surface pressure coefficients (e.g., RMSE $\leq 0.02$ for mean $C_p$) and
remains accurate under reflected-test evaluation, maintaining hit rates above
$96\%$ where the non-equivariant baseline model drops by more than $10\%$. By
pairing a systematic dataset with an equivariant surrogate, WindMiL enables
efficient, scalable, and accurate predictions of wind loads on buildings.

</details>


### [352] [KAT-GNN: A Knowledge-Augmented Temporal Graph Neural Network for Risk Prediction in Electronic Health Records](https://arxiv.org/abs/2511.01249)
*Kun-Wei Lin,Yu-Chen Kuo,Hsin-Yao Wang,Yi-Ju Tseng*

Main category: cs.LG

TL;DR: 提出了KAT-GNN框架，通过整合临床知识和时序动态进行风险预测，在多种临床任务中实现最先进性能


<details>
  <summary>Details</summary>
Motivation: 电子健康记录数据具有异构性和不规则时序性，给风险预测带来挑战，需要有效建模临床知识和时间动态

Method: 构建模态特定患者图，使用SNOMED CT本体和共现先验进行知识增强，采用时间感知transformer捕获纵向动态

Result: 在CAD预测中AUROC达0.9269，在MIMIC-III和MIMIC-IV死亡率预测中分别达到0.9230和0.8849，显著优于基线方法

Conclusion: 将临床知识整合到图表示中，结合时间感知注意力机制，为跨临床任务的风险预测提供了有效且可推广的方法

Abstract: Clinical risk prediction using electronic health records (EHRs) is vital to
facilitate timely interventions and clinical decision support. However,
modeling heterogeneous and irregular temporal EHR data presents significant
challenges. We propose \textbf{KAT-GNN} (Knowledge-Augmented Temporal Graph
Neural Network), a graph-based framework that integrates clinical knowledge and
temporal dynamics for risk prediction. KAT-GNN first constructs
modality-specific patient graphs from EHRs. These graphs are then augmented
using two knowledge sources: (1) ontology-driven edges derived from SNOMED CT
and (2) co-occurrence priors extracted from EHRs. Subsequently, a time-aware
transformer is employed to capture longitudinal dynamics from the graph-encoded
patient representations. KAT-GNN is evaluated on three distinct datasets and
tasks: coronary artery disease (CAD) prediction using the Chang Gung Research
Database (CGRD) and in-hospital mortality prediction using the MIMIC-III and
MIMIC-IV datasets. KAT-GNN achieves state-of-the-art performance in CAD
prediction (AUROC: 0.9269 $\pm$ 0.0029) and demonstrated strong results in
mortality prediction in MIMIC-III (AUROC: 0.9230 $\pm$ 0.0070) and MIMIC-IV
(AUROC: 0.8849 $\pm$ 0.0089), consistently outperforming established baselines
such as GRASP and RETAIN. Ablation studies confirm that both knowledge-based
augmentation and the temporal modeling component are significant contributors
to performance gains. These findings demonstrate that the integration of
clinical knowledge into graph representations, coupled with a time-aware
attention mechanism, provides an effective and generalizable approach for risk
prediction across diverse clinical tasks and datasets.

</details>


### [353] [Adversarial Spatio-Temporal Attention Networks for Epileptic Seizure Forecasting](https://arxiv.org/abs/2511.01275)
*Zan Li,Kyongmin Yeo,Wesley Gifford,Lara Marcuse,Madeline Fields,Bülent Yener*

Main category: cs.LG

TL;DR: STAN是一个对抗性时空注意力网络，用于从多变量EEG信号预测癫痫发作，通过级联注意力块联合建模空间脑连接和时间神经动态，实现高敏感度、低误报率和个体特异性适应。


<details>
  <summary>Details</summary>
Motivation: 癫痫发作预测在医疗时间序列预测中面临关键挑战，需要高敏感度、低误报率和个体特异性适应能力。现有方法假设固定的发作前持续时间或分别处理时空特征，无法有效捕捉时空模式间的双向依赖关系。

Method: 提出STAN网络，通过级联注意力块交替使用空间和时间模块，联合建模空间脑连接和时间神经动态。采用带梯度惩罚的对抗训练来鲁棒区分发作间期和发作前状态，使用明确的15分钟发作前窗口进行学习。

Result: 在两个基准EEG数据集上实现最先进性能：CHB-MIT头皮数据集96.6%敏感度、0.011次/小时误报；MSSM颅内数据集94.2%敏感度、0.063次/小时误报。模型计算高效（230万参数，45ms延迟，180MB内存），适合实时边缘部署。

Conclusion: STAN能够可靠地在个体特异性时间（通常发作前15-45分钟）触发警报，反映模型捕捉细微发作前动态的能力，无需个体化训练。该框架为医疗和其他时间序列领域的时空预测提供了通用范式。

Abstract: Forecasting epileptic seizures from multivariate EEG signals represents a
critical challenge in healthcare time series prediction, requiring high
sensitivity, low false alarm rates, and subject-specific adaptability. We
present STAN, an Adversarial Spatio-Temporal Attention Network that jointly
models spatial brain connectivity and temporal neural dynamics through cascaded
attention blocks with alternating spatial and temporal modules. Unlike existing
approaches that assume fixed preictal durations or separately process spatial
and temporal features, STAN captures bidirectional dependencies between spatial
and temporal patterns through a unified cascaded architecture. Adversarial
training with gradient penalty enables robust discrimination between interictal
and preictal states learned from clearly defined 15-minute preictal windows.
Continuous 90-minute pre-seizure monitoring reveals that the learned
spatio-temporal attention patterns enable early detection: reliable alarms
trigger at subject-specific times (typically 15-45 minutes before onset),
reflecting the model's capacity to capture subtle preictal dynamics without
requiring individualized training. Experiments on two benchmark EEG datasets
(CHB-MIT scalp: 8 subjects, 46 events; MSSM intracranial: 4 subjects, 14
events) demonstrate state-of-the-art performance: 96.6% sensitivity with 0.011
false detections per hour and 94.2% sensitivity with 0.063 false detections per
hour, respectively, while maintaining computational efficiency (2.3M
parameters, 45 ms latency, 180 MB memory) for real-time edge deployment. Beyond
epilepsy, the proposed framework provides a general paradigm for
spatio-temporal forecasting in healthcare and other time series domains where
individual heterogeneity and interpretability are crucial.

</details>


### [354] [Identification of Capture Phases in Nanopore Protein Sequencing Data Using a Deep Learning Model](https://arxiv.org/abs/2511.01277)
*Annabelle Martin,Daphne Kontogiorgos-Heintz,Jeff Nivala*

Main category: cs.LG

TL;DR: 开发了一个轻量级一维卷积神经网络CaptureNet-Deep，用于自动检测纳米孔蛋白质测序中的捕获阶段，将分析时间从几天缩短到30分钟以内。


<details>
  <summary>Details</summary>
Motivation: 纳米孔蛋白质测序产生长而嘈杂的电流轨迹，其中包含蛋白质捕获和易位等关键分子阶段。手动识别捕获阶段耗时且需要领域专业知识，通常需要专家花费数天时间进行注释。

Method: 使用轻量级一维卷积神经网络(1D CNN)在降采样的信号窗口中检测捕获阶段，并与CNN-LSTM混合模型、基于直方图的分类器和其他CNN变体进行比较。

Result: 最佳模型CaptureNet-Deep在保留测试数据上实现了0.94的F1分数和93.39%的精确度，支持低延迟推理并集成到Oxford Nanopore实验仪表板中。

Conclusion: 结果表明，使用简单、可解释的架构可以实现高效的实时捕获检测，轻量级机器学习模型在测序工作流程中具有更广泛的应用潜力。

Abstract: Nanopore protein sequencing produces long, noisy ionic current traces in
which key molecular phases, such as protein capture and translocation, are
embedded. Capture phases mark the successful entry of a protein into the pore
and serve as both a checkpoint and a signal that a channel merits further
analysis. However, manual identification of capture phases is time-intensive,
often requiring several days for expert reviewers to annotate the data due to
the need for domain-specific interpretation of complex signal patterns. To
address this, a lightweight one-dimensional convolutional neural network (1D
CNN) was developed and trained to detect capture phases in down-sampled signal
windows. Evaluated against CNN-LSTM (Long Short-Term Memory) hybrids,
histogram-based classifiers, and other CNN variants using run-level data
splits, our best model, CaptureNet-Deep, achieved an F1 score of 0.94 and
precision of 93.39% on held-out test data. The model supports low-latency
inference and is integrated into a dashboard for Oxford Nanopore experiments,
reducing the total analysis time from several days to under thirty minutes.
These results show that efficient, real-time capture detection is possible
using simple, interpretable architectures and suggest a broader role for
lightweight ML models in sequencing workflows.

</details>


### [355] [Lyapunov Stability Learning with Nonlinear Control via Inductive Biases](https://arxiv.org/abs/2511.01283)
*Yupu Lu,Shijie Lin,Hao Xu,Zeqing Zhang,Jia Pan*

Main category: cs.LG

TL;DR: 提出一种将Lyapunov条件作为归纳偏置的神经网络控制Lyapunov函数(CLF)设计方法，通过端到端学习同时学习CLF和控制器，相比现有方法获得更高的收敛率和更大的吸引域。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习-验证框架的深度CLF方法将Lyapunov条件作为复杂优化约束，难以实现全局收敛且验证实现复杂。

Method: 将Lyapunov条件作为归纳偏置设计神经CLF和基于CLF的控制器，实现约束有限的稳定优化过程和端到端学习。

Result: 在大量实验案例中，该方法相比现有方法在CLF学习中获得更高的收敛率和更大的吸引域(ROA)。

Conclusion: 该方法有效改进了CLF学习框架，并深入揭示了先前方法在学习过程中成功率下降的原因。

Abstract: Finding a control Lyapunov function (CLF) in a dynamical system with a
controller is an effective way to guarantee stability, which is a crucial issue
in safety-concerned applications. Recently, deep learning models representing
CLFs have been applied into a learner-verifier framework to identify
satisfiable candidates. However, the learner treats Lyapunov conditions as
complex constraints for optimisation, which is hard to achieve global
convergence. It is also too complicated to implement these Lyapunov conditions
for verification. To improve this framework, we treat Lyapunov conditions as
inductive biases and design a neural CLF and a CLF-based controller guided by
this knowledge. This design enables a stable optimisation process with limited
constraints, and allows end-to-end learning of both the CLF and the controller.
Our approach achieves a higher convergence rate and larger region of attraction
(ROA) in learning the CLF compared to existing methods among abundant
experiment cases. We also thoroughly reveal why the success rate decreases with
previous methods during learning.

</details>


### [356] [LSHFed: Robust and Communication-Efficient Federated Learning with Locally-Sensitive Hashing Gradient Mapping](https://arxiv.org/abs/2511.01296)
*Guanjie Cheng,Mengzhen Yang,Xinkui Zhao,Shuyi Yu,Tianyu Du,Yangyang Wu,Mengying Zhu,Shuiguang Deng*

Main category: cs.LG

TL;DR: LSHFed是一个鲁棒且通信高效的联邦学习框架，通过局部敏感哈希将高维梯度压缩为二进制表示，在保护隐私的同时有效检测恶意梯度攻击。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在分布式环境中面临推理攻击和投毒攻击的威胁，现有防御方法存在通信计算成本高或检测精度有限的问题。

Method: 提出LSHFed框架，核心是LSHGM梯度验证机制，使用多超平面局部敏感哈希将高维梯度投影为紧凑的二进制表示，仅通过哈希形式检测恶意梯度。

Result: 实验表明LSHFed在50%参与者为恶意攻击者时仍能保持高模型性能，梯度验证通信量比全梯度方法减少1000倍。

Conclusion: LSHFed通过局部敏感哈希技术有效解决了联邦学习中的安全性和效率问题，在保护隐私的同时实现了鲁棒的模型聚合。

Abstract: Federated learning (FL) enables collaborative model training across
distributed nodes without exposing raw data, but its decentralized nature makes
it vulnerable in trust-deficient environments. Inference attacks may recover
sensitive information from gradient updates, while poisoning attacks can
degrade model performance or induce malicious behaviors. Existing defenses
often suffer from high communication and computation costs, or limited
detection precision. To address these issues, we propose LSHFed, a robust and
communication-efficient FL framework that simultaneously enhances aggregation
robustness and privacy preservation. At its core, LSHFed incorporates LSHGM, a
novel gradient verification mechanism that projects high-dimensional gradients
into compact binary representations via multi-hyperplane locally-sensitive
hashing. This enables accurate detection and filtering of malicious gradients
using only their irreversible hash forms, thus mitigating privacy leakage risks
and substantially reducing transmission overhead. Extensive experiments
demonstrate that LSHFed maintains high model performance even when up to 50% of
participants are collusive adversaries while achieving up to a 1000x reduction
in gradient verification communication compared to full-gradient methods.

</details>


### [357] [Diffusion-Based Solver for CNF Placement on the Cloud-Continuum](https://arxiv.org/abs/2511.01343)
*Álvaro Vázquez Rodríguez,Manuel Fernández-Veiga,Carlos Giraldo-Rodríguez*

Main category: cs.LG

TL;DR: 提出基于去噪扩散概率模型(DDPM)的云原生网络功能(CNF)放置新框架，将放置问题重新定义为生成图到分配任务，通过图神经网络去噪器迭代优化CNF到云的分配矩阵。


<details>
  <summary>Details</summary>
Motivation: 传统方法（混合整数非线性规划、启发式算法、强化学习）在可扩展性、约束处理和泛化能力方面存在局限，无法满足5G/6G网络中CNF放置的严格资源、带宽和延迟约束需求。

Method: 将CNF放置问题编码为异构图，训练图神经网络去噪器迭代优化噪声分配矩阵，在损失函数中直接融入约束特定损失以学习可行解空间。

Result: 在多种拓扑结构上的广泛评估表明，该方法能持续产生可行解，推理速度比MINLP求解器快几个数量级。

Conclusion: 基于扩散的生成建模在约束网络嵌入问题中具有巨大潜力，为分布式云原生网络功能的实际可扩展编排提供了有效解决方案。

Abstract: The placement of Cloud-Native Network Functions (CNFs) across the
Cloud-Continuum represents a core challenge in the orchestration of current 5G
and future 6G networks. The process involves the placement of interdependent
computing tasks, structured as Service Function Chains, over distributed cloud
infrastructures. This is achieved while satisfying strict resource, bandwidth
and latency constraints. It is acknowledged that classical approaches,
including mixed-integer nonlinear programming, heuristics and reinforcement
learning are limited in terms of scalability, constraint handling and
generalisation capacity. In the present study, a novel theoretical framework is
proposed, which is based on Denoising Diffusion Probabilistic Models (DDPM) for
CNF placement. The present approach proposes a reconceptualisation of placement
as a generative graph to assignment task, where the placement problem is
encoded as a heterogeneous graph, and a Graph Neural Network denoiser is
trained to iteratively refine noisy CNF-to-cloud assignment matrices. The model
incorporates constraint-specific losses directly into the loss function,
thereby allowing it to learn feasible solution spaces. The integration of the
DDPM formulation with structured combinatorial constraints is achieved through
a rigorous and systematic approach. Extensive evaluations across diverse
topologies have been conducted, which have confirmed that the model
consistently produces feasible solutions with orders of magnitude faster
inference than MINLP solvers. The results obtained demonstrate the potential of
diffusion-based generative modelling for constrained network embedding
problems, making an impact towards the practical, scalable orchestration of
distributed Cloud-Native Network Functions.

</details>


### [358] [MiniFool - Physics-Constraint-Aware Minimizer-Based Adversarial Attacks in Deep Neural Networks](https://arxiv.org/abs/2511.01352)
*Lucie Flek,Oliver Janik,Philipp Alexander Jung,Akbar Karimi,Timo Saala,Alexander Schmidt,Matthias Schott,Philipp Soldin,Matthias Thiesmeyer,Christopher Wiebusch,Ulrich Willemsen*

Main category: cs.LG

TL;DR: 提出MiniFool算法，一种基于物理启发的对抗攻击方法，用于测试粒子物理和天体物理中的神经网络分类任务。该算法通过最小化结合χ²检验统计量和目标分数偏差的成本函数来生成对抗样本。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够考虑实验不确定性的对抗攻击方法，用于评估神经网络在粒子物理和天体物理分类任务中的鲁棒性，特别是在IceCube中微子观测站的天体τ中微子搜索等实际应用场景。

Method: 基于最小化成本函数的方法，该函数结合了基于χ²的检验统计量（量化基于实验不确定性的扰动概率）与期望目标分数的偏差。通过调整攻击参数来缩放实验不确定性，测试分类变化的鲁棒性。

Result: 研究发现，正确分类和错误分类事件的分类翻转可能性不同。通过调整攻击参数，可以量化网络决策的鲁棒性，并测试未标记实验数据分类的稳健性。

Conclusion: MiniFool算法具有通用性，成功应用于MNIST数据集和CMS实验的开放数据，能够有效评估神经网络在物理实验环境中的分类鲁棒性。

Abstract: In this paper, we present a new algorithm, MiniFool, that implements
physics-inspired adversarial attacks for testing neural network-based
classification tasks in particle and astroparticle physics. While we initially
developed the algorithm for the search for astrophysical tau neutrinos with the
IceCube Neutrino Observatory, we apply it to further data from other science
domains, thus demonstrating its general applicability. Here, we apply the
algorithm to the well-known MNIST data set and furthermore, to Open Data data
from the CMS experiment at the Large Hadron Collider. The algorithm is based on
minimizing a cost function that combines a $\chi^2$ based test-statistic with
the deviation from the desired target score. The test statistic quantifies the
probability of the perturbations applied to the data based on the experimental
uncertainties. For our studied use cases, we find that the likelihood of a
flipped classification differs for both the initially correctly and incorrectly
classified events. When testing changes of the classifications as a function of
an attack parameter that scales the experimental uncertainties, the robustness
of the network decision can be quantified. Furthermore, this allows testing the
robustness of the classification of unlabeled experimental data.

</details>


### [359] [Verifiable Split Learning via zk-SNARKs](https://arxiv.org/abs/2511.01356)
*Rana Alaa,Darío González-Ferreiro,Carlos Beis-Penedo,Manuel Fernández-Veiga,Rebeca P. Díaz-Redondo,Ana Fernández-Vilas*

Main category: cs.LG

TL;DR: 提出可验证的分割学习框架，通过集成zk-SNARK证明来确保分割学习中计算结果的正确性和可验证性


<details>
  <summary>Details</summary>
Motivation: 分割学习虽然有助于在数据或资源分离的设备间进行协作训练，但缺乏验证各方计算正确性和诚实性的能力

Method: 在服务器端的前向传播和反向传播中为双方生成zk-SNARK证明和验证，确保双方的可验证性

Result: 与仅记录更新但不生成零知识证明的区块链系统相比，应用zk-SNARK测试实现了可验证性和正确性

Conclusion: zk-SNARK方法能够实现可验证性和正确性，而区块链虽然轻量但不可验证

Abstract: Split learning is an approach to collaborative learning in which a deep
neural network is divided into two parts: client-side and server-side at a cut
layer. The client side executes its model using its raw input data and sends
the intermediate activation to the server side. This configuration architecture
is very useful for enabling collaborative training when data or resources are
separated between devices. However, split learning lacks the ability to verify
the correctness and honesty of the computations that are performed and
exchanged between the parties. To this purpose, this paper proposes a
verifiable split learning framework that integrates a zk-SNARK proof to ensure
correctness and verifiability. The zk-SNARK proof and verification are
generated for both sides in forward propagation and backward propagation on the
server side, guaranteeing verifiability on both sides. The verifiable split
learning architecture is compared to a blockchain-enabled system for the same
deep learning network, one that records updates but without generating the
zero-knowledge proof. From the comparison, it can be deduced that applying the
zk-SNARK test achieves verifiability and correctness, while blockchains are
lightweight but unverifiable.

</details>


### [360] [Learning Intractable Multimodal Policies with Reparameterization and Diversity Regularization](https://arxiv.org/abs/2511.01374)
*Ziqi Wang,Jiashun Liu,Ling Pan*

Main category: cs.LG

TL;DR: 提出了一种基于重参数化的多模态强化学习算法，通过距离多样性正则化平衡性能、决策多样性和效率，在多样性关键场景中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统连续深度强化学习算法使用确定性或单峰高斯策略，无法表达复杂多模态决策分布，这限制了它们在多样性关键场景中的性能。

Method: 首先将现有难处理的多模态策略统一框架化，证明可通过重参数化直接优化；然后提出基于距离的多样性正则化方法，无需显式计算决策概率。

Result: 在多目标达成和生成式强化学习等多样性关键领域表现出优势，特别是在少样本鲁棒性方面；在传统MuJoCo基准测试中也显示竞争力。

Conclusion: 摊销策略是一种有前景的策略模型类别，具有强大的多模态表达能力和高性能；所提方法能有效平衡性能、多样性和效率。

Abstract: Traditional continuous deep reinforcement learning (RL) algorithms employ
deterministic or unimodal Gaussian actors, which cannot express complex
multimodal decision distributions. This limitation can hinder their performance
in diversity-critical scenarios. There have been some attempts to design online
multimodal RL algorithms based on diffusion or amortized actors. However, these
actors are intractable, making existing methods struggle with balancing
performance, decision diversity, and efficiency simultaneously. To overcome
this challenge, we first reformulate existing intractable multimodal actors
within a unified framework, and prove that they can be directly optimized by
policy gradient via reparameterization. Then, we propose a distance-based
diversity regularization that does not explicitly require decision
probabilities. We identify two diversity-critical domains, namely multi-goal
achieving and generative RL, to demonstrate the advantages of multimodal
policies and our method, particularly in terms of few-shot robustness. In
conventional MuJoCo benchmarks, our algorithm also shows competitive
performance. Moreover, our experiments highlight that the amortized actor is a
promising policy model class with strong multimodal expressivity and high
performance. Our code is available at https://github.com/PneuC/DrAC

</details>


### [361] [Protecting the Neural Networks against FGSM Attack Using Machine Unlearning](https://arxiv.org/abs/2511.01377)
*Amir Hossein Khorasani,Ali Jahanian,Maryam Rastgarpour*

Main category: cs.LG

TL;DR: 应用去学习技术提升LeNet神经网络对抗FGSM攻击的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 机器学习模型容易受到对抗性攻击，特别是FGSM攻击，需要开发有效的防御方法来提高模型安全性

Method: 在LeNet神经网络上应用去学习技术，通过重新训练模型来"忘记"对抗性扰动，使用原始数据而非添加扰动的数据

Result: 去学习技术显著提高了LeNet网络对抗FGSM攻击的鲁棒性

Conclusion: 去学习是提升神经网络对抗FGSM攻击鲁棒性的有效方法

Abstract: Machine learning is a powerful tool for building predictive models. However,
it is vulnerable to adversarial attacks. Fast Gradient Sign Method (FGSM)
attacks are a common type of adversarial attack that adds small perturbations
to input data to trick a model into misclassifying it. In response to these
attacks, researchers have developed methods for "unlearning" these attacks,
which involves retraining a model on the original data without the added
perturbations. Machine unlearning is a technique that tries to "forget"
specific data points from the training dataset, to improve the robustness of a
machine learning model against adversarial attacks like FGSM. In this paper, we
focus on applying unlearning techniques to the LeNet neural network, a popular
architecture for image classification. We evaluate the efficacy of unlearning
FGSM attacks on the LeNet network and find that it can significantly improve
its robustness against these types of attacks.

</details>


### [362] [DAMBench: A Multi-Modal Benchmark for Deep Learning-based Atmospheric Data Assimilation](https://arxiv.org/abs/2511.01468)
*Hao Wang,Zixuan Weng,Jindong Han,Wei Fan,Hao Liu*

Main category: cs.LG

TL;DR: 提出了DAMBench，首个大规模多模态基准测试，用于在真实大气条件下评估数据驱动的数据同化模型，解决了现有研究中依赖简化场景和缺乏标准化基准的问题。


<details>
  <summary>Details</summary>
Motivation: 传统数据同化方法虽然有效，但深度学习提供了更可扩展、高效和灵活的替代方案。现有深度学习数据同化研究存在两个关键限制：依赖合成观测的简化场景，以及缺乏公平模型比较的标准化基准。

Method: 整合了来自最先进预报系统的高质量背景状态和真实世界的多模态观测数据（气象站和卫星图像），所有数据重采样到统一网格并进行时间对齐，提供统一评估协议并基准测试代表性数据同化方法。

Result: 建立了严谨的研究基础，促进可重复性、公平比较和扩展到真实世界多模态场景的能力，展示了集成真实观测如何增强简单基线模型。

Conclusion: DAMBench为未来研究提供了标准化基准，解决了数据同化领域的关键挑战，数据集和代码已公开可用。

Abstract: Data Assimilation is a cornerstone of atmospheric system modeling, tasked
with reconstructing system states by integrating sparse, noisy observations
with prior estimation. While traditional approaches like variational and
ensemble Kalman filtering have proven effective, recent advances in deep
learning offer more scalable, efficient, and flexible alternatives better
suited for complex, real-world data assimilation involving large-scale and
multi-modal observations. However, existing deep learning-based DA research
suffers from two critical limitations: (1) reliance on oversimplified scenarios
with synthetically perturbed observations, and (2) the absence of standardized
benchmarks for fair model comparison. To address these gaps, in this work, we
introduce DAMBench, the first large-scale multi-modal benchmark designed to
evaluate data-driven DA models under realistic atmospheric conditions. DAMBench
integrates high-quality background states from state-of-the-art forecasting
systems and real-world multi-modal observations (i.e., real-world weather
stations and satellite imagery). All data are resampled to a common grid and
temporally aligned to support systematic training, validation, and testing. We
provide unified evaluation protocols and benchmark representative data
assimilation approaches, including latent generative models and neural process
frameworks. Additionally, we propose a lightweight multi-modal plugin to
demonstrate how integrating realistic observations can enhance even simple
baselines. Through comprehensive experiments, DAMBench establishes a rigorous
foundation for future research, promoting reproducibility, fair comparison, and
extensibility to real-world multi-modal scenarios. Our dataset and code are
publicly available at https://github.com/figerhaowang/DAMBench.

</details>


### [363] [Memory-Efficient Training with In-Place FFT Implementation](https://arxiv.org/abs/2511.01385)
*Xinyu Ding,Bangtian Liu,Siyu Liao,Zhongfeng Wang*

Main category: cs.LG

TL;DR: 提出了首个实域完全原位FFT框架(rdFFT)，通过利用蝴蝶操作对称性和频域共轭特性，设计隐式复数编码方案，消除中间缓存使用，实现输入输出内存空间一致性。


<details>
  <summary>Details</summary>
Motivation: 现有FFT实现（包括标准FFT和实FFT）无法实现真正的原位计算，特别是实FFT将大小为n的输入映射到大小为n/2+1的复数输出，导致维度不匹配并需要额外内存分配。

Method: 利用蝴蝶操作对称性和频域共轭特性，设计隐式复数编码方案，完全消除中间缓存使用，实现实域完全原位FFT计算。

Result: 在多个自然语言理解任务上的实验表明，该方法有效降低了训练内存成本。

Conclusion: 该方法为频域轻量级适配提供了一个有前景的方向。

Abstract: Fast Fourier Transforms (FFT) are widely used to reduce memory and
computational costs in deep learning. However, existing implementations,
including standard FFT and real FFT (rFFT), cannot achieve true in-place
computation. In particular, rFFT maps an input of size n to a complex output of
size n/2+1, causing dimensional mismatch and requiring additional memory
allocation. We propose the first real-domain, fully in-place FFT framework
(rdFFT) that preserves input-output memory space consistency. By leveraging
butterfly operation symmetry and conjugate properties in the frequency domain,
we design an implicit complex encoding scheme that eliminates intermediate
cache usage entirely. Experiments on multiple natural language understanding
tasks demonstrate the method effectiveness in reducing training memory cost,
offering a promising direction for frequency-domain lightweight adaptation.

</details>


### [364] [Leveraging Compact Satellite Embeddings and Graph Neural Networks for Large-Scale Poverty Mapping](https://arxiv.org/abs/2511.01408)
*Markus B. Pettersson,Adel Daoud*

Main category: cs.LG

TL;DR: 提出一种基于图的卫星嵌入方法，用于预测撒哈拉以南非洲地区的贫困指数，通过建模空间关系和引入模糊标签损失来处理坐标位移问题。


<details>
  <summary>Details</summary>
Motivation: 全球南方地区缺乏准确细粒度的贫困地图，DHS调查数据空间覆盖有限且坐标因隐私保护而随机位移，降低了数据质量。

Method: 使用低维AlphaEarth卫星嵌入，通过图结构建模调查点和未标记位置的空间关系，引入概率性"模糊标签"损失来处理坐标位移问题。

Result: 在37个DHS数据集上的实验表明，相比仅使用图像的基线方法，加入图结构略微提高了预测准确性。

Conclusion: 紧凑的地球观测嵌入在大规模社会经济制图方面具有潜力，图结构有助于提高财富预测的泛化能力。

Abstract: Accurate, fine-grained poverty maps remain scarce across much of the Global
South. While Demographic and Health Surveys (DHS) provide high-quality
socioeconomic data, their spatial coverage is limited and reported coordinates
are randomly displaced for privacy, further reducing their quality. We propose
a graph-based approach leveraging low-dimensional AlphaEarth satellite
embeddings to predict cluster-level wealth indices across Sub-Saharan Africa.
By modeling spatial relations between surveyed and unlabeled locations, and by
introducing a probabilistic "fuzzy label" loss to account for coordinate
displacement, we improve the generalization of wealth predictions beyond
existing surveys. Our experiments on 37 DHS datasets (2017-2023) show that
incorporating graph structure slightly improves accuracy compared to
"image-only" baselines, demonstrating the potential of compact EO embeddings
for large-scale socioeconomic mapping.

</details>


### [365] [Real-time Continual Learning on Intel Loihi 2](https://arxiv.org/abs/2511.01553)
*Elvin Hajizada,Danielle Rager,Timothy Shea,Leobardo Campos-Macias,Andreas Wild,Eyke Hüllermeier,Yulia Sandamirskaya,Mike Davies*

Main category: cs.LG

TL;DR: CLP-SNN是一种基于脉冲神经网络的在线持续学习架构，在英特尔Loihi 2芯片上实现，通过事件驱动稀疏学习、自归一化学习规则和神经发生机制，在边缘设备上实现了高效且无灾难性遗忘的持续学习。


<details>
  <summary>Details</summary>
Motivation: 解决边缘AI设备在开放世界中面临的数据分布漂移和新类别出现的挑战，同时克服传统离线训练在功耗受限环境下的局限性，实现高效的在线持续学习。

Method: 提出CLP-SNN架构，包含三个创新：事件驱动时空稀疏局部学习、自归一化三因子学习规则维持权重归一化、集成神经发生和元可塑性实现容量扩展和遗忘缓解。

Result: 在OpenLORIS少样本学习实验中，CLP-SNN达到与重放方法相当的准确率且无需重放，比边缘GPU上最佳替代OCL方法快70倍（0.33ms vs 23.2ms），能效高5600倍（0.05mJ vs 281mJ）。

Conclusion: 脑启发算法与神经形态硬件的协同设计能够打破传统精度-效率权衡，为未来边缘AI系统提供突破性解决方案。

Abstract: AI systems on edge devices face a critical challenge in open-world
environments: adapting when data distributions shift and novel classes emerge.
While offline training dominates current paradigms, online continual learning
(OCL)--where models learn incrementally from non-stationary streams without
catastrophic forgetting--remains challenging in power-constrained settings. We
present a neuromorphic solution called CLP-SNN: a spiking neural network
architecture for Continually Learning Prototypes and its implementation on
Intel's Loihi 2 chip. Our approach introduces three innovations: (1)
event-driven and spatiotemporally sparse local learning, (2) a self-normalizing
three-factor learning rule maintaining weight normalization, and (3) integrated
neurogenesis and metaplasticity for capacity expansion and forgetting
mitigation. On OpenLORIS few-shot learning experiments, CLP-SNN achieves
accuracy competitive with replay methods while being rehearsal-free. CLP-SNN
delivers transformative efficiency gains: 70\times faster (0.33ms vs 23.2ms),
and 5,600\times more energy efficient (0.05mJ vs 281mJ) than the best
alternative OCL on edge GPU. This demonstrates that co-designed brain-inspired
algorithms and neuromorphic hardware can break traditional accuracy-efficiency
trade-offs for future edge AI systems.

</details>


### [366] [CG-FKAN: Compressed-Grid Federated Kolmogorov-Arnold Networks for Communication Constrained Environment](https://arxiv.org/abs/2511.01433)
*Seunghun Yu,Youngjoon Lee,Jinu Gong,Joonhyuk Kang*

Main category: cs.LG

TL;DR: 提出了CG-FKAN方法，通过稀疏化和压缩扩展网格来减少联邦学习中KAN模型的通信开销，在通信受限环境下比固定网格KAN降低13.6%的RMSE。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在隐私关键应用中广泛使用，但存在可解释性有限的问题。KAN网络通过可学习的样条函数解决了这一限制，但现有研究忽略了网格扩展带来的通信开销。

Method: 提出CG-FKAN方法，在通信预算约束下，通过稀疏化并仅传输必要的系数来压缩扩展网格。

Result: 实验表明，在通信受限设置下，CG-FKAN比固定网格KAN实现了高达13.6%的RMSE降低。

Conclusion: CG-FKAN有效解决了KAN在联邦学习中的通信开销问题，并推导了其近似误差的理论上界。

Abstract: Federated learning (FL), widely used in privacy-critical applications,
suffers from limited interpretability, whereas Kolmogorov-Arnold Networks (KAN)
address this limitation via learnable spline functions. However, existing FL
studies applying KAN overlook the communication overhead introduced by grid
extension, which is essential for modeling complex functions. In this letter,
we propose CG-FKAN, which compresses extended grids by sparsifying and
transmitting only essential coefficients under a communication budget.
Experiments show that CG-FKAN achieves up to 13.6% lower RMSE than fixed-grid
KAN in communication-constrained settings. In addition, we derive a theoretical
upper bound on its approximation error.

</details>


### [367] [HIT-ROCKET: Hadamard-vector Inner-product Transformer for ROCKET](https://arxiv.org/abs/2511.01572)
*Wang Hao,Kuang Zhang,Hou Chengyu,Yuan Zhonghao,Tan Chenxing,Fu Weifeng,Zhu Yangying*

Main category: cs.LG

TL;DR: 提出基于Hadamard卷积变换的特征提取方法，使用Hadamard矩阵的列向量或行向量作为不同尺寸的卷积核，在保持与现有方法兼容的同时提升计算效率、鲁棒性和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列分类方法如HIVE-COTE、Proximity Forest等计算复杂度高、参数调优和训练周期长，而轻量级方法如ROCKET在核选择和计算开销方面仍有改进空间。

Method: 使用Hadamard矩阵的列向量或行向量作为不同长度的卷积核进行特征提取，利用核的正交性提升性能，与现有方法完全兼容。

Result: 在UCR时间序列数据集上的实验显示，F1分数比ROCKET至少提高5%，训练时间比最快的miniROCKET变体缩短50%，可在超低功耗嵌入式设备上部署。

Conclusion: 提出的Hadamard卷积变换方法在计算效率和分类性能上均优于现有方法，具有实际部署价值。

Abstract: Time series classification holds broad application value in communications,
information countermeasures, finance, and medicine. However, state-of-the-art
(SOTA) methods-including HIVE-COTE, Proximity Forest, and TS-CHIEF-exhibit high
computational complexity, coupled with lengthy parameter tuning and training
cycles. In contrast, lightweight solutions like ROCKET (Random Convolutional
Kernel Transform) offer greater efficiency but leave substantial room for
improvement in kernel selection and computational overhead. To address these
challenges, we propose a feature extraction approach based on Hadamard
convolutional transform, utilizing column or row vectors of Hadamard matrices
as convolution kernels with extended lengths of varying sizes. This enhancement
maintains full compatibility with existing methods (e.g., ROCKET) while
leveraging kernel orthogonality to boost computational efficiency, robustness,
and adaptability. Comprehensive experiments on multi-domain datasets-focusing
on the UCR time series dataset-demonstrate SOTA performance: F1-score improved
by at least 5% vs. ROCKET, with 50% shorter training time than miniROCKET
(fastest ROCKET variant) under identical hyperparameters, enabling deployment
on ultra-low-power embedded devices. All code is available on GitHub.

</details>


### [368] [The Curvature Rate λ: A Scalar Measure of Input-Space Sharpness in Neural Networks](https://arxiv.org/abs/2511.01438)
*Jacob Poschl*

Main category: cs.LG

TL;DR: 提出了一种新的输入空间曲率度量λ，通过高阶导数增长率来量化神经网络的功能平滑度，相比参数空间曲率度量更易解释且不受参数化影响。


<details>
  <summary>Details</summary>
Motivation: 现有基于参数空间的曲率度量计算昂贵、对参数化敏感且难以在功能层面解释，需要一种直接在输入空间定义的曲率度量。

Method: 引入曲率率λ作为高阶输入导数范数的指数增长率，通过log ||D^n f|| vs n的斜率估计，并提出了曲率率正则化(CRR)来直接优化输入空间几何。

Result: 在解析函数和神经网络上的实验表明，λ在训练过程中可预测地演化，CRR与SAM相比能达到相似精度但产生更平坦的输入空间几何和更好的置信度校准。

Conclusion: λ通过微分动力学将曲率概念基础化，为学习模型的功能平滑度提供了紧凑、可解释且参数化不变的描述符。

Abstract: Curvature influences generalization, robustness, and how reliably neural
networks respond to small input perturbations. Existing sharpness metrics are
typically defined in parameter space (e.g., Hessian eigenvalues) and can be
expensive, sensitive to reparameterization, and difficult to interpret in
functional terms. We introduce a scalar curvature measure defined directly in
input space: the curvature rate {\lambda}, given by the exponential growth rate
of higher-order input derivatives. Empirically, {\lambda} is estimated as the
slope of log ||D^n f|| versus n for small n. This growth-rate perspective
unifies classical analytic quantities: for analytic functions, {\lambda}
corresponds to the inverse radius of convergence, and for bandlimited signals,
it reflects the spectral cutoff. The same principle extends to neural networks,
where {\lambda} tracks the emergence of high-frequency structure in the
decision boundary. Experiments on analytic functions and neural networks (Two
Moons and MNIST) show that {\lambda} evolves predictably during training and
can be directly shaped using a simple derivative-based regularizer, Curvature
Rate Regularization (CRR). Compared to Sharpness-Aware Minimization (SAM), CRR
achieves similar accuracy while yielding flatter input-space geometry and
improved confidence calibration. By grounding curvature in differentiation
dynamics, {\lambda} provides a compact, interpretable, and
parameterization-invariant descriptor of functional smoothness in learned
models.

</details>


### [369] [Efficient Curvature-aware Graph Network](https://arxiv.org/abs/2511.01443)
*Chaoqun Fei,Tinglve Zhou,Tianyong Hao,Yangyang Li*

Main category: cs.LG

TL;DR: 提出了一种新的图曲率度量——有效电阻曲率，它通过节点间的有效电阻来量化消息传递的难易程度，相比Ollivier-Ricci曲率显著提高了计算效率，同时保持了相当的几何表达能力。


<details>
  <summary>Details</summary>
Motivation: 图曲率为图神经网络提供几何先验，但现有的Ollivier-Ricci曲率计算复杂度过高，限制了其在大规模图数据集上的应用。

Method: 使用节点间的有效电阻替代最优传输距离来量化图边上的消息传递难易程度，提出有效电阻曲率度量方法。

Result: 理论证明了有效电阻曲率的低计算复杂度，并在多种GNN任务上的实验表明，该方法在保持与Ollivier-Ricci曲率竞争性能的同时，大幅降低了计算开销。

Conclusion: 有效电阻曲率是一种计算高效且几何表达能力强的图曲率度量方法，能够有效替代Ollivier-Ricci曲率在图神经网络中的应用。

Abstract: Graph curvature provides geometric priors for Graph Neural Networks (GNNs),
enhancing their ability to model complex graph structures, particularly in
terms of structural awareness, robustness, and theoretical interpretability.
Among existing methods, Ollivier-Ricci curvature has been extensively studied
due to its strong geometric interpretability, effectively characterizing the
local geometric distribution between nodes. However, its prohibitively high
computational complexity limits its applicability to large-scale graph
datasets. To address this challenge, we propose a novel graph curvature
measure--Effective Resistance Curvature--which quantifies the ease of message
passing along graph edges using the effective resistance between node pairs,
instead of the optimal transport distance. This method significantly
outperforms Ollivier-Ricci curvature in computational efficiency while
preserving comparable geometric expressiveness. Theoretically, we prove the low
computational complexity of effective resistance curvature and establish its
substitutability for Ollivier-Ricci curvature. Furthermore, extensive
experiments on diverse GNN tasks demonstrate that our method achieves
competitive performance with Ollivier-Ricci curvature while drastically
reducing computational overhead.

</details>


### [370] [Scaling Graph Chain-of-Thought Reasoning: A Multi-Agent Framework with Efficient LLM Serving](https://arxiv.org/abs/2511.01633)
*Chengying Huan,Ziheng Meng,Yongchao Liu,Zhengyi Yang,Yun Zhu,Yue Yun,Shipeng Li,Rong Gu,Xiabao Wu,Haitao Zhang,Chuntao Hong,Shaonan Ma,Guihai Chen,Chen Tian*

Main category: cs.LG

TL;DR: GLM是一个多代理的图思维链系统，通过分解推理任务、优化LLM服务架构，显著提升了图结构知识推理的准确性、效率和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有的图思维链方法存在准确性低、令牌使用过多、延迟高和吞吐量低的问题，主要由于单代理整体提示、重复上下文重新编码和低效的服务执行。

Method: GLM将推理分解为分类、推理、动作生成和图检索等专门代理，采用分支和选择性上下文共享来减少提示长度和推理迭代；同时引入图思维链感知的LLM推理机制，包括图特定KV缓存管理、基于优先级的驱逐和流水线执行。

Result: GLM将答案准确性提升高达38%，令牌成本降低95.7%，推理延迟降低90.3%，吞吐量比最先进的图思维链基线提高15.1倍。

Conclusion: GLM通过多代理架构和优化的LLM服务设计，实现了高效的大规模复杂现实世界推理应用。

Abstract: Graph Chain-of-Thought (Graph-CoT) enables large language models (LLMs) to
perform step-by-step reasoning over graph-structured knowledge, but existing
pipelines suffer from low accuracy, excessive token usage, high latency, and
low throughput due to single-agent monolithic prompts, repeated context
re-encoding, and inefficient serving execution. We present GLM, the first
multi-agent Graph-CoT system co-designed with an optimized LLM serving
architecture. GLM decomposes reasoning into specialized agents for
classification, reasoning, action generation, and graph retrieval, enabling
branching and selective context sharing to reduce prompt length and reasoning
iterations while preserving reasoning quality, thereby improving accuracy and
reducing overall token consumption. To scale inference, we introduce a
Graph-CoT-aware LLM inference mechanism with graph-specific KV-cache
management, priority-based eviction, and pipelined execution to improve serving
efficiency. Experiments demonstrate that GLM improves answer accuracy by up to
38%, reduces token cost by up to 95.7%, lowers inference latency by 90.3%, and
achieves up to 15.1x higher throughput compared to state-of-the-art Graph-CoT
baselines, enabling efficient adoption for complex real-world reasoning at
scale.

</details>


### [371] [Gated Fusion Enhanced Multi-Scale Hierarchical Graph Convolutional Network for Stock Movement Prediction](https://arxiv.org/abs/2511.01570)
*Xiaosha Xue,Peibo Duan,Zhipeng Liu,Qi Chu,Changsheng Zhang,Bin zhang*

Main category: cs.LG

TL;DR: 提出MS-HGFN模型，通过分层图神经网络和多尺度时空特征融合，解决股票预测中忽略的股票内部属性模式和跨尺度特征偏见问题，在美中股市数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多尺度图神经网络在股票预测中经常忽视两个关键点：每个股票内部属性模式对股票间相关性的影响，以及在多尺度采样中对粗粒度和细粒度特征的偏见关注。

Method: MS-HGFN模型包含分层GNN模块，通过学习不同时间尺度上的内部属性模式和跨属性特征来构建动态图，全面捕捉时空依赖关系；采用自上而下的门控方法整合多尺度时空特征。

Result: 在美中股市真实数据集上的实验表明，MS-HGFN优于传统和先进模型，预测准确率提升高达1.4%，并在收益模拟中表现出更强的稳定性。

Conclusion: MS-HGFN通过有效建模股票内部属性模式和平衡多尺度特征关注，显著提升了股票市场预测的准确性和稳定性。

Abstract: Accurately predicting stock market movements remains a formidable challenge
due to the inherent volatility and complex interdependencies among stocks.
Although multi-scale Graph Neural Networks (GNNs) hold potential for modeling
these relationships, they frequently neglect two key points: the subtle
intra-attribute patterns within each stock affecting inter-stock correlation,
and the biased attention to coarse- and fine-grained features during
multi-scale sampling. To overcome these challenges, we introduce MS-HGFN
(Multi-Scale Hierarchical Graph Fusion Network). The model features a
hierarchical GNN module that forms dynamic graphs by learning patterns from
intra-attributes and features from inter-attributes over different time scales,
thus comprehensively capturing spatio-temporal dependencies. Additionally, a
top-down gating approach facilitates the integration of multi-scale
spatio-temporal features, preserving critical coarse- and fine-grained features
without too much interference. Experiments utilizing real-world datasets from
U.S. and Chinese stock markets demonstrate that MS-HGFN outperforms both
traditional and advanced models, yielding up to a 1.4% improvement in
prediction accuracy and enhanced stability in return simulations. The code is
available at https://anonymous.4open.science/r/MS-HGFN.

</details>


### [372] [Explore More, Learn Better: Parallel MLLM Embeddings under Mutual Information Minimization](https://arxiv.org/abs/2511.01588)
*Zhicheng Wang,Chen Ju,Xu Chen,Shuai Xiao,Jinsong Lan,Xiaoyong Zhu,Ying Chen,Zhiguo Cao*

Main category: cs.LG

TL;DR: 提出并行解耦框架(PDF)，通过多模态大语言模型的指令可控性生成多个并行嵌入路径，解决了传统SSC范式将丰富多模态输入压缩为单一嵌入的问题，显著提升了嵌入模型的性能。


<details>
  <summary>Details</summary>
Motivation: 传统嵌入模型采用SSC范式（单一输入、单一嵌入、对比监督），无法充分利用多模态大语言模型的能力，将丰富的多模态输入压缩为单一嵌入，限制了模型的表现。

Method: 使用共享MLLM骨干网络，通过不同的可学习前缀生成多个并行路径，采用互信息最小化约束确保并行多样性，结合每路径对比监督保持语义对齐，实现双目标优化。

Result: 在MMEB基准测试中显著提升性能：VLM2Vec-LLaVA-1.6-LR模型提升8.9%(7B)，VLM2Vec-Qwen2VL模型提升4.2%(2B)和3.1%(7B)；2B模型仅用一半计算预算就超越基线2.6%。

Conclusion: PDF框架有效解决了传统嵌入模型的局限性，通过并行解耦策略充分利用MLLM能力，在保持高效推理的同时显著提升了嵌入质量。

Abstract: Embedding models are a cornerstone of modern AI. Driven by Multimodal Large
Language Models (MLLMs), they have made great progress in architecture and data
curation, while the holistic paradigm is still limited to SSC, i.e., single
input, singular embedding, contrastive supervision, which collapses rich,
multifaceted inputs into monolithic embeddings and fails to fully exploit MLLM
capabilities. In this paper, we tailor one Parallel Decoupling Framework (PDF)
for multimodal embedding learning, by utilizing the proprietary steerability of
MLLMs, i.e., their ability to flexibly generate quite differentiated response
under explicit instructions. Concretely, PDF conditions a shared MLLM backbone
on distinct, learnable prefixes to roll out multiple parallel paths for one
input, then relies on these paths to obtain parallel embeddings. To promote
full parallel diversity, we employ Mutual Information Minimization (MIM) as an
explicit constraint, coupled with per-path contrastive supervision to maintain
semantic alignment. Such dual-objectives force PDF to yield robust semantic
coverage and a generalizable embedding space. Ultimately, the remarkable
embedding space are accessible at inference via one single forward pass,
incurring negligible computational overhead. We instantiate PDF on multiple
MLLM backbones and prove its effectiveness on MMEB benchmark. Significant gains
are consistently achieved across various resolutions and model sizes, e.g.,
boosting the VLM2Vec-LLaVA-1.6-LR model by a remarkable +8.9% (7B), while the
VLM2Vec-Qwen2VL models by +4.2% (2B) and +3.1% (7B). In terms of efficiency,
our 2B model surpasses its baseline by +2.6% using only half the computational
budget.

</details>


### [373] [Bayesian Natural Gradient Fine-Tuning of CLIP Models via Kalman Filtering](https://arxiv.org/abs/2511.01694)
*Hossein Abdi,Mingfei Sun,Wei Pan*

Main category: cs.LG

TL;DR: 提出使用卡尔曼滤波的贝叶斯近似方法来实现自然梯度下降，用于CLIP模型的少样本微调，在保持ID性能的同时提升OOD鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于一阶梯度的微调方法收敛慢、对步长超参数敏感且在OOD场景泛化能力差，而二阶方法能利用损失函数的局部曲率信息，但自然梯度下降计算成本高。

Method: 使用卡尔曼滤波器对自然梯度下降进行贝叶斯近似，结合二阶优化的优势与贝叶斯推断，在计算效率与性能间取得平衡。

Result: 在多个图像分类数据集上的实验表明，该方法在ID性能上达到或超过现有最优方法，同时在OOD鲁棒性上有显著提升。

Conclusion: 这是首次成功将卡尔曼滤波应用于CLIP模型微调，为视觉语言任务提供了更鲁棒高效的学习方法。

Abstract: Vision-language pre-trained models, such as CLIP, have established new
benchmarks in multimodal data mining. In such models, few-shot fine-tuning is a
major challenge to achieve optimal performance on both in-distribution (ID) and
out-of-distribution (OOD) datasets, especially when labeled data is scarce.
Most existing fine-tuning approaches rely on first-order gradient-based
optimizers, which typically suffer from slow convergence, sensitivity to
step-size hyperparameters, and poor generalization in OOD settings. In
contrast, second-order methods utilize local curvature information of the loss
landscape to adjust the update step size. This is particularly beneficial for
CLIP models, whose non-convex loss functions often contain sharp critical
points. In such cases, natural gradient direction can offer more substantial
and efficient per-iteration updates when fine-tuning with limited data. Natural
Gradient Descent (NGD) is obtained by preconditioning the standard gradient
with the inverse Fisher Information Matrix (FIM), which is computationally
expensive for large models. To address this, we propose a Bayesian
approximation of NGD using a Kalman filter for CLIP models. Our method combines
the benefits of second-order optimization with Bayesian inference, which
enhances generalization while providing uncertainty quantification. Extensive
experiments conducted on diverse image classification datasets demonstrate that
our algorithm consistently achieves superior--or comparable--ID performance and
improved OOD robustness compared to state-of-the-art baselines. To the best of
our knowledge, this work represents the first successful application of Kalman
filtering to fine-tuning CLIP-based models, which enables more robust and
efficient learning in vision-language tasks.

</details>


### [374] [Defining Energy Indicators for Impact Identification on Aerospace Composites: A Physics-Informed Machine Learning Perspective](https://arxiv.org/abs/2511.01592)
*Natália Ribeiro Marinho,Richard Loendersloot,Frank Grooteman,Jan Willem Wiegman,Uraz Odyurt,Tiedo Tinga*

Main category: cs.LG

TL;DR: 该研究提出了一种物理信息驱动的机器学习框架，通过结合领域知识和多域特征提取，显著提高了航空航天复合材料冲击能量预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前冲击能量预测方法面临数据稀疏、信号噪声、复杂特征依赖和非线性动力学等挑战，需要开发既能处理复杂动态又能保持物理可解释性的方法。

Method: 构建物理信息输入空间，结合观测偏差和特征选择，从时域、频域和时频域提取特征，通过统计显著性、相关性过滤和降维确保特征物理相关性和可解释性。

Result: 使用优化的输入空间训练全连接神经网络，在多种冲击场景下验证，冲击能量预测误差比传统时间序列技术和纯数据驱动模型降低了三倍。

Conclusion: 该物理信息框架成功地将领域知识嵌入机器学习，产生了统计鲁棒且物理意义明确的能量敏感指标，实现了可解释且可追溯的冲击能量预测。

Abstract: Energy estimation is critical to impact identification on aerospace
composites, where low-velocity impacts can induce internal damage that is
undetectable at the surface. Current methodologies for energy prediction are
often constrained by data sparsity, signal noise, complex feature
interdependencies, non-linear dynamics, massive design spaces, and the
ill-posed nature of the inverse problem. This study introduces a
physics-informed framework that embeds domain knowledge into machine learning
through a dedicated input space. The approach combines observational biases,
which guide the design of physics-motivated features, with targeted feature
selection to retain only the most informative indicators. Features are
extracted from time, frequency, and time-frequency domains to capture
complementary aspects of the structural response. A structured feature
selection process integrating statistical significance, correlation filtering,
dimensionality reduction, and noise robustness ensures physical relevance and
interpretability. Exploratory data analysis further reveals domain-specific
trends, yielding a reduced feature set that captures essential dynamic
phenomena such as amplitude scaling, spectral redistribution, and transient
signal behaviour. Together, these steps produce a compact set of
energy-sensitive indicators with both statistical robustness and physical
significance, resulting in impact energy predictions that remain interpretable
and traceable to measurable structural responses. Using this optimised input
space, a fully-connected neural network is trained and validated with
experimental data from multiple impact scenarios, including pristine and
damaged states. The resulting model demonstrates significantly improved impact
energy prediction accuracy, reducing errors by a factor of three compared to
conventional time-series techniques and purely data-driven models.

</details>


### [375] [Towards Efficient Federated Learning of Networked Mixture-of-Experts for Mobile Edge Computing](https://arxiv.org/abs/2511.01743)
*Song Gao,Shusen Jing,Shuai Zhang,Yue Wang,Xiangwei Zhou,Songyang Zhang*

Main category: cs.LG

TL;DR: 提出了Networked Mixture-of-Experts (NMoE)系统，通过客户端协作推断和联邦学习框架解决大型AI模型在移动边缘计算中的资源限制问题。


<details>
  <summary>Details</summary>
Motivation: 大型AI模型在移动边缘计算中的计算资源和训练数据需求与边缘设备有限的存储和计算能力之间存在冲突，这给在边缘训练和部署LAMs带来了重大挑战。

Method: 引入NMoE系统，客户端基于专家能力将任务分发给合适的邻居进行协作推断并聚合结果；提出结合监督学习和自监督学习的联邦学习框架，平衡个性化和泛化，同时保持通信效率和数据隐私。

Result: 通过广泛实验证明了所提出的NMoE系统的有效性，为NMoE训练算法提供了见解和基准。

Conclusion: NMoE系统为在资源受限的边缘环境中训练和部署大型AI模型提供了有效的解决方案，通过协作推断和联邦学习框架解决了关键挑战。

Abstract: Recent advancements in large artificial intelligence models (LAMs) are
driving significant innovations in mobile edge computing within next-generation
wireless networks. However, the substantial demands for computational resources
and large-scale training data required to train LAMs conflict with the limited
storage and computational capacity of edge devices, posing significant
challenges to training and deploying LAMs at the edge. In this work, we
introduce the Networked Mixture-of-Experts (NMoE) system, in which clients
infer collaboratively by distributing tasks to suitable neighbors based on
their expertise and aggregate the returned results. For training the NMoE, we
propose a federated learning framework that integrates both supervised and
self-supervised learning to balance personalization and generalization, while
preserving communication efficiency and data privacy. We conduct extensive
experiments to demonstrate the efficacy of the proposed NMoE system, providing
insights and benchmarks for the NMoE training algorithms.

</details>


### [376] [An Open-Access Benchmark of Statistical and Machine-Learning Anomaly Detection Methods for Battery Applications](https://arxiv.org/abs/2511.01745)
*Mei-Chin Pang,Suraj Adhikari,Takuma Kasahara,Nagihiro Haba,Saneyuki Ohno*

Main category: cs.LG

TL;DR: OSBAD是一个用于电池应用异常检测的开源基准测试框架，通过比较15种不同算法并引入物理统计特征转换和贝叶斯优化调参，提升了电池异常检测的准确性和可迁移性。


<details>
  <summary>Details</summary>
Motivation: 电池安全在消费电子、电动汽车和飞机等应用中至关重要，未检测到的异常可能引发安全隐患或昂贵停机时间。需要建立系统化的异常检测基准来开发安全、可扩展的异常检测工具。

Method: 开发了OSBAD开源基准测试框架，包含15种统计、距离和无监督机器学习算法；提出物理统计特征转换工作流将集体异常分解为点异常；使用贝叶斯优化管道进行自动超参数调优；在液态和固态化学数据集上进行验证。

Result: OSBAD展示了跨化学体系的泛化能力，能够识别不同电化学系统中的异常；通过特征工程和模型选择显著提升了异常检测性能；建立了可复现的异常检测工作流基准。

Conclusion: OSBAD为电池分析建立了统一的异常检测基准，强调了物理统计特征工程和概率超参数调优在安全关键能源系统可信数据驱动诊断中的重要性。

Abstract: Battery safety is critical in applications ranging from consumer electronics
to electric vehicles and aircraft, where undetected anomalies could trigger
safety hazards or costly downtime. In this study, we present OSBAD as an
open-source benchmark for anomaly detection frameworks in battery applications.
By benchmarking 15 diverse algorithms encompassing statistical, distance-based,
and unsupervised machine-learning methods, OSBAD enables a systematic
comparison of anomaly detection methods across heterogeneous datasets. In
addition, we demonstrate how a physics- and statistics-informed feature
transformation workflow enhances anomaly separability by decomposing collective
anomalies into point anomalies. To address a major bottleneck in unsupervised
anomaly detection due to incomplete labels, we propose a Bayesian optimization
pipeline that facilitates automated hyperparameter tuning based on
transfer-learning and regression proxies. Through validation on datasets
covering both liquid and solid-state chemistries, we further demonstrate the
cross-chemistry generalization capability of OSBAD to identify irregularities
across different electrochemical systems. By making benchmarking database with
open-source reproducible anomaly detection workflows available to the
community, OSBAD establishes a unified foundation for developing safe,
scalable, and transferable anomaly detection tools in battery analytics. This
research underscores the significance of physics- and statistics-informed
feature engineering as well as model selection with probabilistic
hyperparameter tuning, in advancing trustworthy, data-driven diagnostics for
safety-critical energy systems.

</details>


### [377] [Collaborative Large Language Model Inference via Resource-Aware Parallel Speculative Decoding](https://arxiv.org/abs/2511.01695)
*Jungyeon Koh,Hyun Jong Yang*

Main category: cs.LG

TL;DR: 提出统一框架联合优化用户关联和资源分配，支持并行推测解码，在移动边缘计算中实现高效大语言模型推理。


<details>
  <summary>Details</summary>
Motivation: 设备端大语言模型推理需求增长，需要高效的移动边缘计算解决方案。推测解码虽然可行，但存在通信开销和异步延迟问题。

Method: 使用多智能体深度强化学习算法解决用户关联和资源分配问题，在Sionna模拟器中进行实验验证。

Result: 端到端延迟最高减少28.0%，平均减少23.7%，且不影响推理精度。

Conclusion: 该方法能够在移动边缘计算系统中实现可扩展、低延迟的大语言模型服务。

Abstract: The growing demand for on-device large language model (LLM) inference
highlights the need for efficient mobile edge computing (MEC) solutions,
especially in resource-constrained settings. Speculative decoding offers a
promising solution by partitioning token generation between a lightweight draft
model on mobile devices and a powerful target model on edge servers, but
suffers from communication overhead and asynchronous delays. This paper is the
first to propose a unified framework that jointly optimizes user association
and resource allocation (UARA) to support efficient parallel speculative
decoding. We solve the UARA problem using a multi-agent deep reinforcement
learning algorithm. To evaluate our approach under realistic conditions, we
conduct experiments using the Sionna simulator. Results show that our method
achieves up to 28.0% and an average of 23.7% reduction in end-to-end latency
without compromising inference accuracy, enabling scalable and low-latency LLM
services in MEC systems.

</details>


### [378] [Edge AI in Highly Volatile Environments: Is Fairness Worth the Accuracy Trade-off?](https://arxiv.org/abs/2511.01737)
*Obaidullah Zaland,Feras M. Awaysheh,Sawsan Al Zubi,Abdul Rahman Safi,Monowar Bhuyan*

Main category: cs.LG

TL;DR: 本文研究了在高度波动的边缘环境中联邦学习的模型准确性与公平性之间的权衡关系，通过实证评估不同客户端选择算法在公平性、模型性能和时间方面的表现。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在边缘智能中具有重要价值，但边缘环境的波动性和客户端异构性给实现高准确性和公平参与带来了挑战，需要研究准确性与公平性的权衡关系。

Method: 使用三个基准数据集（CIFAR10、FashionMNIST和EMNIST），对基于公平性的客户端选择算法（如RBFF和RBCSF）与随机和贪婪选择算法进行实证比较。

Result: 结果表明，更公平的客户端选择算法虽然能为客户端提供更好的参与机会，但在波动环境中会导致全局训练速度变慢。

Conclusion: 在波动边缘环境中存在公平性与性能、公平性与速度的权衡关系，需要进一步研究来解决公平客户端选择策略中的现有问题。

Abstract: Federated learning (FL) has emerged as a transformative paradigm for edge
intelligence, enabling collaborative model training while preserving data
privacy across distributed personal devices. However, the inherent volatility
of edge environments, characterized by dynamic resource availability and
heterogeneous client capabilities, poses significant challenges for achieving
high accuracy and fairness in client participation. This paper investigates the
fundamental trade-off between model accuracy and fairness in highly volatile
edge environments. This paper provides an extensive empirical evaluation of
fairness-based client selection algorithms such as RBFF and RBCSF against
random and greedy client selection regarding fairness, model performance, and
time, in three benchmarking datasets (CIFAR10, FashionMNIST, and EMNIST). This
work aims to shed light on the fairness-performance and fairness-speed
trade-offs in a volatile edge environment and explore potential future research
opportunities to address existing pitfalls in \textit{fair client selection}
strategies in FL. Our results indicate that more equitable client selection
algorithms, while providing a marginally better opportunity among clients, can
result in slower global training in volatile environments\footnote{The code for
our experiments can be found at
https://github.com/obaidullahzaland/FairFL_FLTA.

</details>


### [379] [Game-theoretic distributed learning of generative models for heterogeneous data collections](https://arxiv.org/abs/2511.01740)
*Dmitrij Schlesinger,Boris Flach*

Main category: cs.LG

TL;DR: 提出一种通过交换合成数据而非模型参数来处理分布式学习中异构本地模型和数据的方法，将本地模型视为黑盒，并基于博弈论原理构建合作学习框架。


<details>
  <summary>Details</summary>
Motivation: 解决分布式学习中处理异构本地模型和数据的挑战，利用生成模型的成功来避免直接共享模型参数，同时处理不同模态的异构数据。

Method: 将本地模型视为黑盒，通过交换合成数据进行学习；对于支持半监督学习的本地模型，扩展到不同概率空间；基于博弈论原理将本地模型学习制定为合作游戏。

Result: 证明了指数族本地模型存在唯一纳什均衡，提出的学习方法收敛到该均衡；在标准图像分类和条件生成基准数据集上展示了方法的优势。

Conclusion: 通过交换合成数据和博弈论框架，有效解决了分布式学习中的异构性问题，为处理不同模态的异构数据提供了可行方案。

Abstract: One of the main challenges in distributed learning arises from the difficulty
of handling heterogeneous local models and data. In light of the recent success
of generative models, we propose to meet this challenge by building on the idea
of exchanging synthetic data instead of sharing model parameters. Local models
can then be treated as ``black boxes'' with the ability to learn their
parameters from data and to generate data according to these parameters.
Moreover, if the local models admit semi-supervised learning, we can extend the
approach by enabling local models on different probability spaces. This allows
to handle heterogeneous data with different modalities. We formulate the
learning of the local models as a cooperative game starting from the principles
of game theory. We prove the existence of a unique Nash equilibrium for
exponential family local models and show that the proposed learning approach
converges to this equilibrium. We demonstrate the advantages of our approach on
standard benchmark vision datasets for image classification and conditional
generation.

</details>


### [380] [HyperNQ: A Hypergraph Neural Network Decoder for Quantum LDPC Codes](https://arxiv.org/abs/2511.01741)
*Ameya S. Bhave,Navnil Choudhury,Kanad Basu*

Main category: cs.LG

TL;DR: 提出了HyperNQ，首个基于超图神经网络的QLDPC解码器，通过超边捕获高阶稳定器约束，在伪阈值以下将逻辑错误率比BP提升84%，比GNN策略提升50%。


<details>
  <summary>Details</summary>
Motivation: 传统BP解码器在短循环存在时收敛性差，而GNN解码器受限于Tanner图的成对交互，无法捕获高阶相关性。

Method: 使用超图神经网络，采用两阶段消息传递方案，利用超边捕获高阶稳定器约束。

Result: 在伪阈值区域评估，伪阈值以下HyperNQ将逻辑错误率比BP提升84%，比GNN策略提升50%。

Conclusion: HyperNQ通过超图神经网络实现了高度表达性和紧凑的解码，性能优于现有最先进解码器。

Abstract: Quantum computing requires effective error correction strategies to mitigate
noise and decoherence. Quantum Low-Density Parity-Check (QLDPC) codes have
emerged as a promising solution for scalable Quantum Error Correction (QEC)
applications by supporting constant-rate encoding and a sparse parity-check
structure. However, decoding QLDPC codes via traditional approaches such as
Belief Propagation (BP) suffers from poor convergence in the presence of short
cycles. Machine learning techniques like Graph Neural Networks (GNNs) utilize
learned message passing over their node features; however, they are restricted
to pairwise interactions on Tanner graphs, which limits their ability to
capture higher-order correlations. In this work, we propose HyperNQ, the first
Hypergraph Neural Network (HGNN)- based QLDPC decoder that captures
higher-order stabilizer constraints by utilizing hyperedges-thus enabling
highly expressive and compact decoding. We use a two-stage message passing
scheme and evaluate the decoder over the pseudo-threshold region. Below the
pseudo-threshold mark, HyperNQ improves the Logical Error Rate (LER) up to 84%
over BP and 50% over GNN-based strategies, demonstrating enhanced performance
over the existing state-of-the-art decoders.

</details>


### [381] [Machine and Deep Learning for Indoor UWB Jammer Localization](https://arxiv.org/abs/2511.01819)
*Hamed Fard,Mahsa Kholghi,Benedikt Groß,Gerhard Wunder*

Main category: cs.LG

TL;DR: 该论文针对UWB定位系统易受干扰攻击的问题，提出了基于域对抗ConvNeXt自编码器(A-CNT)的鲁棒干扰源定位方法，通过梯度反转层实现跨域特征对齐，显著提升了室内环境变化下的干扰源定位性能。


<details>
  <summary>Details</summary>
Motivation: UWB定位系统虽然能提供厘米级精度，但易受干扰攻击，存在安全隐患。现有ML/DL方法主要关注标签定位，而在单房间内和室内布局变化下的恶意干扰源定位研究不足。

Method: 提出域对抗ConvNeXt自编码器(A-CNT)，利用梯度反转层对齐跨域的CIR衍生特征，以缓解环境变化导致的性能下降。

Result: 在原始数据集上，Random Forest达到最高F1-macro分数0.95，XGBoost达到最低平均欧几里得误差20.16cm。但在修改的房间布局中，XGBoost误差增加十倍至207.99cm。A-CNT框架将平均欧几里得误差降至34.67cm，比非对抗迁移学习提升77%，比最佳基线提升83%。

Conclusion: 域对抗特征对齐能够实现鲁棒且可迁移的室内干扰源定位，即使在环境变化的情况下也能保持良好性能。

Abstract: Ultra-wideband (UWB) localization delivers centimeter-scale accuracy but is
vulnerable to jamming attacks, creating security risks for asset tracking and
intrusion detection in smart buildings. Although machine learning (ML) and deep
learning (DL) methods have improved tag localization, localizing malicious
jammers within a single room and across changing indoor layouts remains largely
unexplored. Two novel UWB datasets, collected under original and modified room
configurations, are introduced to establish comprehensive ML/DL baselines.
Performance is rigorously evaluated using a variety of classification and
regression metrics. On the source dataset with the collected UWB features,
Random Forest achieves the highest F1-macro score of 0.95 and XGBoost achieves
the lowest mean Euclidean error of 20.16 cm. However, deploying these
source-trained models in the modified room layout led to severe performance
degradation, with XGBoost's mean Euclidean error increasing tenfold to 207.99
cm, demonstrating significant domain shift. To mitigate this degradation, a
domain-adversarial ConvNeXt autoencoder (A-CNT) is proposed that leverages a
gradient-reversal layer to align CIR-derived features across domains. The A-CNT
framework restores localization performance by reducing the mean Euclidean
error to 34.67 cm. This represents a 77 percent improvement over
non-adversarial transfer learning and an 83 percent improvement over the best
baseline, restoring the fraction of samples within 30 cm to 0.56. Overall, the
results demonstrate that adversarial feature alignment enables robust and
transferable indoor jammer localization despite environmental changes. Code and
dataset available at https://github.com/afbf4c8996f/Jammer-Loc

</details>


### [382] [Dynamic Routing Between Experts: A Data-Efficient Approach to Continual Learning in Vision-Language Models](https://arxiv.org/abs/2511.01831)
*Jay Mohta,Kenan Emir Ak,Dimitrios Dimitriadis,Yan Xu,Mingwei Shen*

Main category: cs.LG

TL;DR: 提出基于路由的方法解决视觉语言模型在顺序微调时的灾难性遗忘问题，该方法无需同时访问所有任务数据，能保持基础能力同时提升专业任务性能。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在顺序微调新任务时会出现灾难性遗忘，而多任务学习需要同时访问所有数据集且计算开销随任务数量线性增长。

Method: 采用基于路由的方法，在InternVL-2模型上实现新任务集成，同时保留预训练获得的基础知识。

Result: 路由方法在ChartQA、MMBench、DocVQA等通用基准上保持性能，同时提升专业任务准确率，且无需所有任务数据并发访问。

Conclusion: 路由方法具有可扩展性和鲁棒性，特别适用于语义相关的新任务，并能实现跨模态知识转移，优于现有持续学习方法。

Abstract: Vision-Language Models (VLMs) suffer from catastrophic forgetting when
sequentially fine-tuned on new tasks, degrading performance on previously
learned foundational and task-specific capabilities. While multi-task learning
can mitigate forgetting, it requires simultaneous access to all datasets and
imposes computational overhead that scales linearly with the number of tasks.
In this work, we introduce a routing-based approach that enables the
integration of new tasks while preserving the foundational knowledge acquired
during pretraining. We evaluate our method using InternVL-2 models (2B and 8B
parameters) and demonstrate that routing preserves the model's foundational
capabilities by maintaining performance on general-purpose benchmarks such as
ChartQA, MMBench, and DocVQA, while simultaneously improving accuracy on
specialized tasks. Importantly, our approach achieves this without requiring
concurrent access to data from all tasks, avoiding the significant
computational and data overhead associated with traditional multi-task
learning. We further conduct extensive ablation studies to evaluate the
scalability and robustness of routing-based learning, showing that the approach
is resilient to a growing number of tasks and performs particularly well when
new tasks are semantically related. Finally, we show that the routing mechanism
enables superior cross-modal transfer between language and vision capabilities,
allowing knowledge learned in one modality to enhance performance in another
capability not achieved by existing continual learning methods.

</details>


### [383] [Bayesian Coreset Optimization for Personalized Federated Learning](https://arxiv.org/abs/2511.01800)
*Prateek Chanda,Shrey Modi,Ganesh Ramakrishnan*

Main category: cs.LG

TL;DR: 提出了一种个性化核心集加权联邦学习方法，通过仅使用代表性数据点而非完整客户端数据进行训练更新，在保持最优泛化误差的同时显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中客户端训练完整数据集负担过重的问题，通过核心集选择代表性数据点来减轻计算负担。

Method: 个性化核心集加权联邦学习，基于客户端核心集代表性数据点进行训练更新，而非完整客户端数据。

Result: 理论分析显示平均泛化误差达到极小极大最优，实验在多个基准数据集上相比随机采样方法有显著提升，在医疗数据集上优于其他子模优化方法。

Conclusion: 智能选择训练样本能有效提升联邦学习性能，提出的方法在减少计算负担的同时保持最优泛化性能。

Abstract: In a distributed machine learning setting like Federated Learning where there
are multiple clients involved which update their individual weights to a single
central server, often training on the entire individual client's dataset for
each client becomes cumbersome. To address this issue we propose $\methodprop$:
a personalized coreset weighted federated learning setup where the training
updates for each individual clients are forwarded to the central server based
on only individual client coreset based representative data points instead of
the entire client data. Through theoretical analysis we present how the average
generalization error is minimax optimal up to logarithm bounds (upper bounded
by $\mathcal{O}(n_k^{-\frac{2 \beta}{2 \beta+\boldsymbol{\Lambda}}} \log ^{2
\delta^{\prime}}(n_k))$) and lower bounds of $\mathcal{O}(n_k^{-\frac{2
\beta}{2 \beta+\boldsymbol{\Lambda}}})$, and how the overall generalization
error on the data likelihood differs from a vanilla Federated Learning setup as
a closed form function ${\boldsymbol{\Im}}(\boldsymbol{w}, n_k)$ of the coreset
weights $\boldsymbol{w}$ and coreset sample size $n_k$. Our experiments on
different benchmark datasets based on a variety of recent personalized
federated learning architectures show significant gains as compared to random
sampling on the training data followed by federated learning, thereby
indicating how intelligently selecting such training samples can help in
performance. Additionally, through experiments on medical datasets our proposed
method showcases some gains as compared to other submodular optimization based
approaches used for subset selection on client's data.

</details>


### [384] [Dynamic Reconstruction of Ultrasound-Derived Flow Fields With Physics-Informed Neural Fields](https://arxiv.org/abs/2511.01804)
*Viraj Patel,Lisa Kreusser,Katharine Fraser*

Main category: cs.LG

TL;DR: 提出了一种基于物理信息的神经场模型，使用多尺度傅里叶特征编码从稀疏和噪声的超声数据中估计血流，无需真实监督，在合成和真实数据集上实现了低均方误差的去噪和修复效果。


<details>
  <summary>Details</summary>
Motivation: 血流分析对疾病诊断很重要，但超声成像存在深度衰减问题，导致图像质量受限。传统EchoPIV技术测量血流速度存在挑战，物理信息机器学习可以增强准确性和鲁棒性。

Method: 使用物理信息神经场模型结合多尺度傅里叶特征编码，从稀疏噪声超声数据重建血流，无需真实监督数据。

Result: 在合成和真实数据集上，该模型在去噪和修复方面都实现了持续低的均方误差，与参考流场和真实流量测量结果一致。

Conclusion: 将其他成像模态中有效的方法适应到超声血流重建中，证明了物理信息神经场在医学血流重建中的有效性。

Abstract: Blood flow is sensitive to disease and provides insight into cardiac
function, making flow field analysis valuable for diagnosis. However, while
safer than radiation-based imaging and more suitable for patients with medical
implants, ultrasound suffers from attenuation with depth, limiting the quality
of the image. Despite advances in echocardiographic particle image velocimetry
(EchoPIV), accurately measuring blood velocity remains challenging due to the
technique's limitations and the complexity of blood flow dynamics.
Physics-informed machine learning can enhance accuracy and robustness,
particularly in scenarios where noisy or incomplete data challenge purely
data-driven approaches. We present a physics-informed neural field model with
multi-scale Fourier Feature encoding for estimating blood flow from sparse and
noisy ultrasound data without requiring ground truth supervision. We
demonstrate that this model achieves consistently low mean squared error in
denoising and inpainting both synthetic and real datasets, verified against
reference flow fields and ground truth flow rate measurements. While
physics-informed neural fields have been widely used to reconstruct medical
images, applications to medical flow reconstruction are mostly prominent in
Flow MRI. In this work, we adapt methods that have proven effective in other
imaging modalities to address the specific challenge of ultrasound-based flow
reconstruction.

</details>


### [385] [Towards Multi-Fidelity Scaling Laws of Neural Surrogates in CFD](https://arxiv.org/abs/2511.01830)
*Paul Setinek,Gianluca Galletti,Johannes Brandstetter*

Main category: cs.LG

TL;DR: 本文研究了科学机器学习中数据保真度与计算成本之间的权衡，通过重新制定经典缩放定律，将数据集轴分解为计算预算和数据集组成，揭示了计算-性能缩放行为。


<details>
  <summary>Details</summary>
Motivation: 科学机器学习通常受限于通过数值模拟生成训练数据的高昂成本，但通过调整建模假设和近似，可以在模拟保真度和计算成本之间进行权衡，这是其他领域所没有的方面。

Method: 使用低保真度和高保真度雷诺平均纳维-斯托克斯（RANS）模拟来研究神经代理模型中数据保真度与成本之间的权衡，重新制定经典缩放定律，将数据集轴分解为计算预算和数据集组成。

Result: 实验揭示了计算-性能缩放行为，并展示了给定数据集配置下预算依赖的最优保真度混合。

Conclusion: 这些发现为多保真度神经代理数据集提供了首个经验缩放定律研究，并为科学机器学习中计算高效的数据集生成提供了实际考虑。

Abstract: Scaling laws describe how model performance grows with data, parameters and
compute. While large datasets can usually be collected at relatively low cost
in domains such as language or vision, scientific machine learning is often
limited by the high expense of generating training data through numerical
simulations. However, by adjusting modeling assumptions and approximations,
simulation fidelity can be traded for computational cost, an aspect absent in
other domains. We investigate this trade-off between data fidelity and cost in
neural surrogates using low- and high-fidelity Reynolds-Averaged Navier-Stokes
(RANS) simulations. Reformulating classical scaling laws, we decompose the
dataset axis into compute budget and dataset composition. Our experiments
reveal compute-performance scaling behavior and exhibit budget-dependent
optimal fidelity mixes for the given dataset configuration. These findings
provide the first study of empirical scaling laws for multi-fidelity neural
surrogate datasets and offer practical considerations for compute-efficient
dataset generation in scientific machine learning.

</details>


### [386] [Priors in Time: Missing Inductive Biases for Language Model Interpretability](https://arxiv.org/abs/2511.01836)
*Ekdeep Singh Lubana,Can Rager,Sai Sumedh R. Hindupur,Valerie Costa,Greta Tuckute,Oam Patel,Sonia Krishna Murthy,Thomas Fel,Daniel Wurgaft,Eric J. Bigelow,Johnny Lin,Demba Ba,Martin Wattenberg,Fernanda Viegas,Melanie Weber,Aaron Mueller*

Main category: cs.LG

TL;DR: 本文提出时间特征分析(Temporal Feature Analysis)方法，通过将语言模型表示分解为可预测部分和残差部分，更好地捕捉语言的时间动态特性，相比传统稀疏自编码器(SAEs)在处理非平稳语言表示方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有特征提取方法假设概念是独立方向，但语言模型表示具有丰富的时间动态特性(如概念维度增长、上下文相关性和非平稳性)，这与SAEs的独立性先验相冲突。

Method: 引入时间特征分析，将给定时间的表示分解为两部分：可预测部分(可从上下文推断)和残差部分(捕获上下文无法解释的新信息)，该方法具有时间归纳偏置。

Result: 时间特征分析器能正确解析花园路径句子、识别事件边界，并更广泛地区分抽象慢变信息和新颖快变信息，而现有SAEs在上述任务中均存在显著缺陷。

Conclusion: 结果表明设计稳健的可解释性工具需要与数据匹配的归纳偏置，时间特征分析为理解语言模型的时间动态提供了更合适的方法。

Abstract: Recovering meaningful concepts from language model activations is a central
aim of interpretability. While existing feature extraction methods aim to
identify concepts that are independent directions, it is unclear if this
assumption can capture the rich temporal structure of language. Specifically,
via a Bayesian lens, we demonstrate that Sparse Autoencoders (SAEs) impose
priors that assume independence of concepts across time, implying stationarity.
Meanwhile, language model representations exhibit rich temporal dynamics,
including systematic growth in conceptual dimensionality, context-dependent
correlations, and pronounced non-stationarity, in direct conflict with the
priors of SAEs. Taking inspiration from computational neuroscience, we
introduce a new interpretability objective -- Temporal Feature Analysis --
which possesses a temporal inductive bias to decompose representations at a
given time into two parts: a predictable component, which can be inferred from
the context, and a residual component, which captures novel information
unexplained by the context. Temporal Feature Analyzers correctly parse garden
path sentences, identify event boundaries, and more broadly delineate abstract,
slow-moving information from novel, fast-moving information, while existing
SAEs show significant pitfalls in all the above tasks. Overall, our results
underscore the need for inductive biases that match the data in designing
robust interpretability tools.

</details>


### [387] [Interpretable Machine Learning for Reservoir Water Temperatures in the U.S. Red River Basin of the South](https://arxiv.org/abs/2511.01837)
*Isabela Suaza-Sierra,Hernan A. Moreno,Luis A De la Fuente,Thomas M. Neeson*

Main category: cs.LG

TL;DR: 本文结合可解释机器学习和符号建模，通过Kolmogorov-Arnold网络将水库水温预测的黑盒模型转化为透明解析表达式，揭示了水库热动态的关键驱动因素。


<details>
  <summary>Details</summary>
Motivation: 水库水温预测对水资源管理和生态系统健康至关重要，但单纯预测无法揭示其物理机制。需要将预测准确性与解释能力相结合，理解水库热动态的驱动因素。

Method: 使用集成和神经网络模型（随机森林、XGBoost、MLP）进行预测，通过SHAP量化物理驱动因素贡献，开发Kolmogorov-Arnold网络符号近似水库水温，生成渐进复杂的解析方程。

Result: 模型预测精度高（RMSE=1.20°C，R²=0.97），SHAP分析显示气温、深度、风力和湖容量的重要性，KAN方程从单预测因子R²=0.84提升到十预测因子R²=0.92，深度是关键次要预测因子，降水影响有限。

Conclusion: 该框架通过KAN和可解释机器学习将黑盒模型转化为透明替代模型，在保持预测精度的同时增强了对水库热动态的理解，平衡了简单性与准确性。

Abstract: Accurate prediction of Reservoir Water Temperature (RWT) is vital for
sustainable water management, ecosystem health, and climate resilience. Yet,
prediction alone offers limited insight into the governing physical processes.
To bridge this gap, we integrated explainable machine learning (ML) with
symbolic modeling to uncover the drivers of RWT dynamics across ten reservoirs
in the Red River Basin, USA, using over 10,000 depth-resolved temperature
profiles. We first employed ensemble and neural models, including Random Forest
(RF), Extreme Gradient Boosting (XGBoost), and Multilayer Perceptron (MLP),
achieving high predictive skill (best RMSE = 1.20 degree Celsius, R^2 = 0.97).
Using SHAP (SHapley Additive exPlanations), we quantified the contribution of
physical drivers such as air temperature, depth, wind, and lake volume,
revealing consistent patterns across reservoirs. To translate these data-driven
insights into compact analytical expressions, we developed Kolmogorov Arnold
Networks (KANs) to symbolically approximate RWT. Ten progressively complex KAN
equations were derived, improving from R^2 = 0.84 using a single predictor
(7-day antecedent air temperature) to R^2 = 0.92 with ten predictors, though
gains diminished beyond five, highlighting a balance between simplicity and
accuracy. The resulting equations, dominated by linear and rational forms,
incrementally captured nonlinear behavior while preserving interpretability.
Depth consistently emerged as a secondary but critical predictor, whereas
precipitation had limited effect. By coupling predictive accuracy with
explanatory power, this framework demonstrates how KANs and explainable ML can
transform black-box models into transparent surrogates that advance both
prediction and understanding of reservoir thermal dynamics.

</details>


### [388] [Coordinate ascent neural Kalman-MLE for state estimation](https://arxiv.org/abs/2511.01855)
*Bettina Hanlon,Angel Garcia Fernandez*

Main category: cs.LG

TL;DR: 提出一种坐标上升算法，通过最大似然估计监督学习动态状态估计中的动态和测量模型，包括神经网络参数和噪声协方差矩阵。


<details>
  <summary>Details</summary>
Motivation: 为了在动态状态估计中学习准确的动态和测量模型，包括函数参数和噪声特性，以提高状态估计的准确性。

Method: 使用坐标上升算法进行最大似然估计，学习高斯假设下的动态和测量模型的神经网络参数及噪声协方差矩阵。

Result: 训练得到的动态和测量模型可与非线性卡尔曼滤波器结合，在测试阶段进行状态估计。

Conclusion: 该方法能够有效学习动态状态估计所需的模型参数，为非线性卡尔曼滤波提供准确的模型基础。

Abstract: This paper presents a coordinate ascent algorithm to learn dynamic and
measurement models in dynamic state estimation using maximum likelihood
estimation in a supervised manner. In particular, the dynamic and measurement
models are assumed to be Gaussian and the algorithm learns the neural network
parameters that model the dynamic and measurement functions, and also the noise
covariance matrices. The trained dynamic and measurement models are then used
with a non-linear Kalman filter algorithm to estimate the state during the
testing phase.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [389] [Which Top Energy-Intensive Manufacturing Countries Can Compete in a Renewable Energy Future?](https://arxiv.org/abs/2511.00242)
*Arne Burdack,Maximilian Stargardt,Christoph Winkler,Konrad Klein,Detlef Stolten,Jochen Linssen,Heidi Heinrichs*

Main category: eess.SY

TL;DR: 本研究通过能源系统建模量化了可再生能源条件优越国家产生的"可再生能源拉力"对产业转移的影响，发现这种拉力并非跨行业现象，而是取决于能源成本与运输成本的关系。中国、印度和日本受到的影响显著大于德国和美国。


<details>
  <summary>Details</summary>
Motivation: 在可再生能源日益普及和温室气体中性工业生产的背景下，研究当前制造业强国未来竞争力的变化，特别是可再生能源条件优越国家可能产生的产业转移激励。

Method: 采用详细的能源系统建模方法，量化分析不同国家间的可再生能源拉力，并考虑能源成本与运输成本的关系以及各国资本成本假设。

Result: 可再生能源拉力强度因国家而异，中国、印度和日本面临的影响显著强于德国和美国。考虑资本成本后，德国的可再生能源拉力减少了六倍，成为仅次于沙特阿拉伯受影响最小的制造业强国。欧盟内部的针对性进口策略几乎可以消除这种拉力。

Conclusion: 可再生能源拉力不是普遍现象，而是高度依赖具体行业和国家条件。通过有针对性的进口策略，特别是欧盟内部合作，政策制定者可以有效缓解这种产业转移风险。

Abstract: In a world increasingly powered by renewables and aiming for greenhouse
gas-neutral industrial production, the future competitiveness of todays top
manufacturing countries is questioned. This study applies detailed energy
system modeling to quantify the Renewable Pull, an incentive for industry
relocation exerted by countries with favorable renewable conditions. Results
reveal that the Renewable Pull is not a cross-industrial phenomenon but
strongly depends on the relationship between energy costs and transport costs.
The intensity of the Renewable Pull varies, with China, India, and Japan facing
a significantly stronger effect than Germany and the United States.
Incorporating national capital cost assumptions proves critical, reducing
Germanys Renewable Pull by a factor of six and positioning it as the second
least affected top manufacturing country after Saudi Arabia. Using Germany as a
case study, the analysis moreover illustrates that targeted import strategies,
especially within the EU, can nearly eliminate the Renewable Pull, offering
policymakers clear options for risk mitigation.

</details>


### [390] [Learning a Network Digital Twin as a Hybrid System](https://arxiv.org/abs/2511.00291)
*Christos Mavridis,Fernando S. Barbosa,Hamed Farhadi,Karl H. Johansson*

Main category: eess.SY

TL;DR: 提出了一种用于多小区动态无线网络的混合系统网络数字孪生模型，通过退火优化学习算法在线识别和改进，在真实5G测试数据上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 网络数字孪生是6G网络的关键技术，需要准确复现物理通信网络行为，特别是多小区动态无线网络中的通信质量特性。

Method: 将NDT建模为混合系统，每个模式对应不同基站，子模式对应具有相似网络特性的工作空间区域，通过退火优化学习算法在线识别和持续改进。

Result: 验证了所提混合NDT在内存和计算效率、数据消耗以及及时适应网络变化能力方面的优势，并在真实5G测试数据上得到验证。

Conclusion: 混合系统建模结合退火优化学习能够有效构建和更新网络数字孪生，为6G网络提供可靠的基础设施支持。

Abstract: Network digital twin (NDT) models are virtual models that replicate the
behavior of physical communication networks and are considered a key technology
component to enable novel features and capabilities in future 6G networks. In
this work, we focus on NDTs that model the communication quality properties of
a multi-cell, dynamically changing wireless network over a workspace populated
with multiple moving users. We propose an NDT modeled as a hybrid system, where
each mode corresponds to a different base station and comprises sub-modes that
correspond to areas of the workspace with similar network characteristics. The
proposed hybrid NDT is identified and continuously improved through an
annealing optimization-based learning algorithm, driven by online data
measurements collected by the users. The advantages of the proposed hybrid NDT
are studied with respect to memory and computational efficiency, data
consumption, and the ability to timely adapt to network changes. Finally, we
validate the proposed methodology on real experimental data collected from a
two-cell 5G testbed.

</details>


### [391] [Analyzing the Impact of Demand Response on Short-Circuit Current via a Unit Commitment Model](https://arxiv.org/abs/2511.00296)
*Peng Wang,Zhengmao Li,Luis Badesa*

Main category: eess.SY

TL;DR: 本文研究了需求响应(DR)对系统短路电流(SCC)水平的影响，发现在低碳电网中DR虽然能降低社会成本，但可能导致SCC不足。通过将DR和SCC约束纳入机组组合模型，发现DR与SCC约束结合时成本仅增加0.3%，表明DR能以成本效益方式实现系统稳定。


<details>
  <summary>Details</summary>
Motivation: 随着同步发电机被基于逆变器的可再生能源替代，系统稳定性受到严重影响。IBR的短路电流贡献远小于SG，可能导致保护装置在故障时无法跳闸。由于SG的投运与系统负荷密切相关，DR可能间接影响其SCC提供能力，这一关系尚未被研究。

Method: 将DR和SCC约束纳入机组组合模型，并在IEEE 30节点系统上进行研究。

Result: 结果显示，虽然DR能通过降低电力需求来减少社会成本，但也可能导致SCC水平不足。当DR与SCC约束结合时，成本仅增加0.3%。

Conclusion: DR实际上能以成本效益的方式帮助实现稳定系统，在考虑SCC约束的情况下，成本增加非常有限。

Abstract: In low-carbon grids, system flexibility can be enhanced through mechanisms
such as Demand Response (DR), enabling the efficient utilization of renewable
energy. However, as Synchronous Generators (SGs) are being replaced with
renewable energy characterized by Inverter-Based Resources (IBR), system
stability is severely affected. Due to the limited overload capability of IBR,
their Short-Circuit Current (SCC) contribution is much smaller than that of
SGs, which may result in protection devices failing to trip during faults.
Consequently, the remaining SGs play a key role in offering sufficient SCC
volumes. Given that the commitment of SGs is closely related to system load, DR
can thus indirectly affect their SCC provision, a relationship that has not
been investigated. Therefore, this paper incorporates both DR and SCC
constraints into a unit commitment model and conducts studies on an IEEE 30-bus
system. The results show that although DR can reduce social costs by lowering
power demand, it may also lead to inadequate SCC levels. Nevertheless, the cost
increases by only 0.3% when DR is combined with SCC constraints, indicating
that DR can actually help achieve a stable system in a cost-effective manner.

</details>


### [392] [Optimal BESS Sizing and Placement for Mitigating EV-Induced Voltage Violations: A Scalable Spatio-Temporal Adaptive Targeting Strategy](https://arxiv.org/abs/2511.00297)
*Linhan Fang,Xingpeng Li*

Main category: eess.SY

TL;DR: 提出一个主动电压管理框架，通过蒙特卡洛模拟识别电动汽车充电导致的电压违规，并使用电池储能系统进行优化缓解，采用时空自适应策略降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 电动汽车普及导致充电需求激增，给配电网带来严重电压下降问题，特别是在馈线末端，需要有效的电压管理解决方案。

Method: 结合蒙特卡洛模拟的电压违规分析模型和电池储能系统优化扩展规划模型，提出时空自适应目标策略来降低计算复杂度。

Result: 在33、69和240总线系统上验证了方法的有效性，结果显示BESS的优化配置不仅能有效缓解电压违规，还能在分时电价下显著节省电费。

Conclusion: 该研究为高渗透率电动汽车集成提供了成本效益高且可扩展的解决方案，为未来配电网规划提供了重要参考。

Abstract: The escalating adoption of electric vehicles (EVs) and the growing demand for
charging solutions are driving a surge in EV charger installations in
distribution networks. However, this rising EV load strains the distribution
grid, causing severe voltage drops, particularly at feeder extremities. This
study proposes a proactive voltage management (PVM) framework that can
integrate Monte Carlo-based simulations of varying EV charging loads to (i)
identify potential voltage violations through a voltage violation analysis
(VVA) model, and (ii) then mitigate those violations with optimally-invested
battery energy storage systems (BESS) through an optimal expansion planning
(OEP) model. A novel spatio-temporal adaptive targeting (STAT) strategy is
proposed to alleviate the computational complexity of the OEP model by defining
a targeted OEP (T-OEP) model, solved by applying the OEP model to (i) a reduced
set of representative critical time periods and (ii) candidate BESS
installation nodes. The efficacy and scalability of the proposed approach are
validated on 33-bus, 69-bus, and a large-scale 240-bus system. Results
demonstrate that the strategic sizing and placement of BESS not only
effectively mitigate voltage violations but also yield substantial cost savings
on electricity purchases under time-of-use tariffs. This research offers a
cost-effective and scalable solution for integrating high penetrations of EVs,
providing crucial insights for future distribution network planning.

</details>


### [393] [Large Language Models for Control](https://arxiv.org/abs/2511.00337)
*Adil Rasheed,Oscar Ravik,Omer San*

Main category: eess.SY

TL;DR: 使用大型语言模型直接生成控制动作，无需控制工程专业知识或手动调整算法，展示了LLM在控制系统中的潜力。


<details>
  <summary>Details</summary>
Motivation: 探索LLM在控制系统中的应用，消除对控制工程专业知识和手动调整算法的依赖，实现更灵活的控制策略生成。

Method: 实现三种变体：(i)仅提示，(ii)工具辅助访问历史数据，(iii)预测辅助使用学习或简单模型对候选动作评分。比较跟踪精度和执行器努力。

Result: 仅提示的LLM已能产生可行的控制，工具增强版本能更好地适应变化的目标但对约束更敏感。

Conclusion: 支持LLM在环控制用于不断发展的网络物理系统，可结合操作员和人类输入。

Abstract: This paper investigates using large language models (LLMs) to generate
control actions directly, without requiring control-engineering expertise or
hand-tuned algorithms. We implement several variants: (i) prompt-only, (ii)
tool-assisted with access to historical data, and (iii) prediction-assisted
using learned or simple models to score candidate actions. We compare them on
tracking accuracy and actuation effort, with and without a prompt that requests
lower actuator usage. Results show prompt-only LLMs already produce viable
control, while tool-augmented versions adapt better to changing objectives but
can be more sensitive to constraints, supporting LLM-in-the-loop control for
evolving cyber-physical systems today and operator and human inputs.

</details>


### [394] [Constrained computational hybrid controller for Input Affine Hybrid Dynamical Systems](https://arxiv.org/abs/2511.00420)
*Ali Taghavian,Ali Safi,Esmaeel Khanmirza*

Main category: eess.SY

TL;DR: 提出了一种基于状态空间划分、计算仿真和图论的新型可实现的约束终态控制器，用于处理混合动力系统。


<details>
  <summary>Details</summary>
Motivation: 混合动力系统包含连续和基于事件的行为，传统控制器无法处理这类复杂系统，需要开发新的控制器。

Method: 通过划分系统状态空间、计算仿真和图论方法，开发了计算混合控制器(CHC)。

Result: 在三水箱基准测试和摆锤摆动控制实验中，与模型预测控制器相比，证明了所提计算混合控制器的有效性。

Conclusion: 提出的计算混合控制器(CHC)在处理混合动力系统方面表现出良好性能，优于传统模型预测控制器。

Abstract: Hybrid dynamical systems are viewed as the most complicated systems with
continuous and event-based behaviors. Since traditional controllers cannot
handle these systems, some newly-developed controllers have been published in
recent decades to deal with them. This paper presents a novel implementable
constrained final-state controller based on partitioning the system's
state-space, computational simulations, and graph theory. Experimental results
and a comparison with Model Predictive Controller on the three tank benchmark
and swing-up control of a pendulum show the effectiveness of the proposed
Computational Hybrid Controller(CHC).

</details>


### [395] [CT-ESKF: A General Framework of Covariance Transformation-Based Error-State Kalman Filter](https://arxiv.org/abs/2511.00453)
*Jiale Han,Wei Ouyang,Maoran Zhu,Yuanxin Wu*

Main category: eess.SY

TL;DR: 本文提出了基于协方差变换的误差状态卡尔曼滤波器(CT-ESKF)框架，统一了各种误差状态卡尔曼滤波算法，在结合全局和体坐标系观测的导航系统中表现出优于不变扩展卡尔曼滤波(InEKF)和传统EKF的性能。


<details>
  <summary>Details</summary>
Motivation: 虽然InEKF具有轨迹无关性和更好的鲁棒性，但在同时涉及全局坐标系和体坐标系观测的场景中可能无法保持其轨迹无关性。本文旨在解决这一问题并统一各种误差状态卡尔曼滤波算法。

Method: 提出了协方差变换的误差状态卡尔曼滤波器(CT-ESKF)框架，通过协方差变换统一不同误差状态卡尔曼滤波算法，并推导出新的滤波算法。

Result: 实验结果表明，在INS/GNSS/Odometer组合导航系统中，采用协方差变换的EKF在性能上优于InEKF和原始EKF。

Conclusion: CT-ESKF框架成功统一了各种误差状态卡尔曼滤波算法，并为包含全局和体坐标系观测的组合导航系统提供了性能更优的解决方案。

Abstract: Invariant extended Kalman filter (InEKF) possesses excellent
trajectory-independent property and better consistency compared to conventional
extended Kalman filter (EKF). However, when applied to scenarios involving both
global-frame and body-frame observations, InEKF may fail to preserve its
trajectory-independent property. This work introduces the concept of
equivalence between error states and covariance matrices among different
error-state Kalman filters, and shows that although InEKF exhibits trajectory
independence, its covariance propagation is actually equivalent to EKF. A
covariance transformation-based error-state Kalman filter (CT-ESKF) framework
is proposed that unifies various error-state Kalman filtering algorithms. The
framework gives birth to novel filtering algorithms that demonstrate improved
performance in integrated navigation systems that incorporate both global and
body-frame observations. Experimental results show that the EKF with covariance
transformation outperforms both InEKF and original EKF in a representative
INS/GNSS/Odometer integrated navigation system.

</details>


### [396] [Rotatable Antenna System Empowered Low-Altitude Economy: Opportunities and Challenges](https://arxiv.org/abs/2511.00562)
*Shuaijun Li,Jie Tang,Beixiong Zheng,Lipeng Zhu,Cui Yang,Nan Zhao,Xiu Yin Zhang,Kai-Kit Wong*

Main category: eess.SY

TL;DR: 本文介绍了可旋转天线系统(RAS)在低空经济(LAE)中的应用，通过动态调整定向天线波束方向来扩展低空覆盖范围并增强数据传输稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有网络基站主要设计用于地面用户，无法为低空应用提供连续覆盖，限制了低空经济的发展。

Method: 提出RAS辅助的多基站和多无人机协同覆盖策略，讨论了信道建模、蜂窝接入、干扰消除以及RAS配置和波束优化等关键设计问题。

Result: 通过实验和仿真结果验证了RAS在LAE网络中的性能增益。

Conclusion: RAS能够有效解决低空经济中的覆盖问题，为低空通信、感知、控制和计算等应用提供可靠支持。

Abstract: Low-altitude economy (LAE) is an emerging technological paradigm that enables
continuous airspace coverage at multiple altitudes by providing highly reliable
data connectivity for numerous low-altitude applications. However, existing
networks cannot sufficiently support LAE development, as current base stations
(BSs) are primarily designed for terrestrial users and lack the capability to
provide continuous coverage at low altitudes. To overcome these challenges,
rotatable antenna system (RAS) is introduced in LAE, enabling flexible
beamforming by dynamically adjusting the boresight of directional antennas to
extend low-altitude coverage and enhance the stability of data transmission. In
this article, we first provide an overview of RAS-empowered LAE applications,
including low-altitude communication, sensing, control, and computation. Then,
we present two practical RAS deployment strategies for LAE scenarios, namely
RAS-aided multi-BS and multi-unmanned aerial vehicle (UAV) cooperative
coverages, as well as provide detailed discussions on their system
architectures and performance benefits. Additionally, key design issues of RAS
in LAE are discussed, including channel modeling and estimation, cellular
access and interference cancellation, as well as RAS configuration and
boresight optimization. Finally, we demonstrate the performance gains of RAS in
LAE networks through experimental and simulation results.

</details>


### [397] [Towards Quantum Algorithms for the Optimization of Spanning Trees: The Power Distribution Grids Use Case](https://arxiv.org/abs/2511.00582)
*Carsten Hartmann,Nil Rodellas-Gràcia,Christian Wallisch,Thiemo Pesch,Frank K. Wilhelm,Dirk Witthaut,Tobias Stollenwerk,Andrea Benigni*

Main category: eess.SY

TL;DR: 该论文研究了径向网络中的损耗最小化问题，提出了基于量子交替算子算法的两种量子优化方法，分别采用定制化采样和带惩罚项的简单采样来处理网络拓扑优化。


<details>
  <summary>Details</summary>
Motivation: 网络拓扑优化在能源系统中具有重要意义，能够显著减少损耗和成本，支持能源转型。但由于许多相关优化问题是NP难问题，限制了实际应用。

Method: 提出了两种基于量子交替算子算法的量子优化原语：一种是定制化采样径向拓扑，另一种是简单采样并添加惩罚项来抑制非径向拓扑。

Result: 论文展示了如何将这些算法原语应用于配电网重构，并量化了所需的量子资源。

Conclusion: 量子优化为解决径向网络损耗最小化这一计算困难问题提供了有前景的替代方案。

Abstract: Optimizing the topology of networks is an important challenge across
engineering disciplines. In energy systems, network reconfiguration can
substantially reduce losses and costs and thus support the energy transition.
Unfortunately, many related optimization problems are NP hard, restricting
practical applications. In this article, we address the problem of minimizing
losses in radial networks, a problem that routinely arises in distribution grid
operation. We show that even the computation of approximate solutions is
computationally hard and propose quantum optimization as a promising
alternative. We derive two quantum algorithmic primitives based on the Quantum
Alternating Operator Ansatz (QAOA) that differ in the sampling of network
topologies: a tailored sampling of radial topologies and simple sampling with
penalty terms to suppress non-radial topologies. We show how to apply these
algorithmic primitives to distribution grid reconfiguration and quantify the
necessary quantum resources.

</details>


### [398] [Digital Twin of Aerosol Jet Printing](https://arxiv.org/abs/2511.00593)
*Aayushya Agarwal,Jace Rozsa,Matteo Pozzi,Rahul Panat,Gary K. Fedder*

Main category: eess.SY

TL;DR: 开发了一个用于气溶胶喷射打印的数字孪生模型，通过物理建模和概率估计技术实时监控不可观测状态，提高打印质量一致性。


<details>
  <summary>Details</summary>
Motivation: 气溶胶喷射打印虽然潜力巨大，但由于复杂的隐藏状态（如气溶胶颗粒直径、载体密度等）导致打印质量不一致，限制了其广泛应用。

Method: 构建基于物理的宏观模型，结合计算机视觉技术和概率序列估计方法，持续更新数字模型参数以匹配实时传感器和视频数据。

Result: 创建了一个能够持续演化的数字孪生模型，可准确监控不可观测物理特性，检测异常行为，并预测控制调整效果。

Conclusion: 该数字孪生框架虽然针对气溶胶喷射打印定制，但其构建方法可推广到其他先进制造技术中。

Abstract: Aerosol Jet (AJ) printing is a versatile additive manufacturing technique
capable of producing high-resolution interconnects on both 2D and 3D
substrates. The AJ process is complex and dynamic with many hidden and
unobservable states that influence the machine performance, including aerosol
particle diameter, aerosol carrier density, vial level, and ink deposition in
the tube and nozzle. Despite its promising potential, the widespread adoption
of AJ printing is limited by inconsistencies in print quality that often stem
from variability in these hidden states. To address these challenges, we
develop a digital twin model of the AJ process that offers real-time insights
into the machine's operations. The digital twin is built around a physics-based
macro-model created through simulation and experimentation. The states and
parameters of the digital model are continuously updated using probabilistic
sequential estimation techniques to closely align with real-time measurements
extracted from the AJ system's sensor and video data. The result is a digital
model of the AJ process that continuously evolves over a physical machine's
lifecycle. The digital twin enables accurate monitoring of unobservable
physical characteristics, detects and predicts anomalous behavior, and
forecasts the effect of control adjustments. This work presents a comprehensive
end-to-end digital twin framework that integrates customized computer vision
techniques, physics-based macro-modeling, and advanced probabilistic estimation
methods to construct an evolving digital representation of the AJ equipment and
process. While the methodologies are customized for aerosol jet printing, the
process for constructing the digital twin can be applied for other advanced
manufacturing techniques.

</details>


### [399] [Efficiency and Optimality in Electrochemical Battery Model Parameter Identification: A Comparative Study of Estimation Techniques](https://arxiv.org/abs/2511.00595)
*Feng Guo,Luis D. Couto,Guillaume Thenaisie*

Main category: eess.SY

TL;DR: 本文评估了三种电化学电池模型参数辨识方法的效率和最优性：最小二乘法、粒子群优化和遗传算法。结果表明PSO在准确性和稳定性方面表现最佳，LS适合参数微调，GA在计算效率和最优性方面落后于PSO。


<details>
  <summary>Details</summary>
Motivation: 电化学电池模型参数辨识具有挑战性，因为涉及多个无法直接测量的参数。需要评估不同参数辨识方法的效率和最优性，为电池模型参数识别提供指导。

Method: 开发并离散化电池单粒子模型，进行参数分组以减少参数数量。使用真实电池参数作为基准生成拟合和验证数据集，评估三种方法的运行时间和准确性。

Result: 比较分析显示PSO在准确性和稳定性方面优于其他方法，特别适合无先验参数知识的情况。LS更适合参数微调，尤其是老化电池。GA在计算效率和最优性方面落后于PSO。

Conclusion: PSO是电化学电池模型参数辨识的最有效方法，LS适用于参数微调场景，GA表现相对较差。该方法评估为电池模型参数识别提供了实用指导。

Abstract: Parameter identification for electrochemical battery models has always been
challenging due to the multitude of parameters involved, most of which cannot
be directly measured. This paper evaluates the efficiency and optimality of
three widely-used parameter identification methods for electrochemical battery
models: Least Squares Method (LS), Particle Swarm Optimization (PSO), and
Genetic Algorithm (GA). Therefore, a Single Particle Model (SPM) of a battery
was developed and discretized. Battery parameter grouping was then performed to
reduce the number of parameters required. Using a set of parameters previously
identified from a real battery as a benchmark, we generated fitting and
validation datasets to assess the methods' runtime and accuracy. The
comparative analysis reveals that PSO outperforms the other methods in terms of
accuracy and stability, making it highly effective for parameter identification
when there is no prior knowledge of the battery's internal parameters. In
contrast, LS is better suited for minor adjustments in parameters, particularly
for aging batteries, whereas GA lags behind in both computational efficiency
and optimality with respect to PSO.

</details>


### [400] [Adaptive Federated Learning to Optimize the MultiCast flows in Data Centers](https://arxiv.org/abs/2511.00623)
*Junhong Liu,Lanxin Du,Yujia Li,Rong-Peng Liu,Fei Teng,Francis Yunhe Hou*

Main category: eess.SY

TL;DR: 提出了一种自适应联邦学习优化方法，用于解决地理分布式数据中心的多周期优化问题，同时保护数据隐私和完整性。


<details>
  <summary>Details</summary>
Motivation: 数据中心能耗快速增长带来可持续运营挑战，需要优化电力、热量和数据流，但面临混合整数规划复杂性和数据隐私问题。

Method: 采用自适应联邦学习优化方法，结合密码学技术保护隐私，开发模型接受标准保证收敛，提出可验证双重聚合机制确保数据完整性。

Result: 理论分析和数值模拟表明该方法能保护共享数据隐私和完整性，实现接近最优性能，计算效率高。

Conclusion: 该方法适用于大规模数据中心在隐私约束下的优化，能有效平衡能效优化与数据保护需求。

Abstract: Data centers play an increasingly critical role in societal digitalization,
yet their rapidly growing energy demand poses significant challenges for
sustainable operation. To enhance the energy efficiency of geographically
distributed data centers, this paper formulates a multi-period optimization
model that captures the interdependence of electricity, heat, and data flows.
The optimization of such multicast flows inherently involves mixed-integer
formulations and the access to proprietary or sensitive datasets, which
correspondingly exacerbate computational complexity and raise data-privacy
concerns. To address these challenges, an adaptive federated
learning-to-optimization approach is proposed, accounting for the heterogeneity
of datasets across distributed data centers. To safeguard privacy, cryptography
techniques are leveraged in both the learning and optimization processes. A
model acceptance criterion with convergence guarantee is developed to improve
learning performance and filter out potentially contaminated data, while a
verifiable double aggregation mechanism is further proposed to simultaneously
ensure privacy and integrity of shared data during optimization. Theoretical
analysis and numerical simulations demonstrate that the proposed approach
preserves the privacy and integrity of shared data, achieves near-optimal
performance, and exhibits high computational efficiency, making it suitable for
large-scale data center optimization under privacy constraints.

</details>


### [401] [Frequency Quality Assessment of GFM and GFL Converters and Synchronous Condensers](https://arxiv.org/abs/2511.00639)
*Taulant Kerci,Federico Milano*

Main category: eess.SY

TL;DR: 比较电网形成型(GFM)和电网跟随型(GFL)逆变器资源与传统同步机对频率质量的影响，发现GFM显著改善频率质量，但GFL与同步调相机的组合也能达到类似标准，且在GFM主导电网中自动发电控制(AGC)的需求变得不明确。


<details>
  <summary>Details</summary>
Motivation: 研究不同传统和新兴技术及控制策略对电网频率质量的影响，特别关注GFM和GFL逆变器资源的长期动态性能。

Method: 通过广泛的仿真和多个现实场景，考虑频率质量的短期和长期方面，比较GFM IBRs、GFL IBRs和传统同步机的性能。

Result: GFM IBRs显著改善频率质量，但提供频率支持的GFL IBRs（如风电和电池）与同步调相机的组合也能满足类似频率质量标准；在GFM主导电网中AGC的需求从频率质量角度变得不明确。

Conclusion: GFM技术能显著提升电网频率质量，但通过适当组合GFL资源和同步设备也能达到类似效果，且在GFM主导系统中需要重新评估AGC的必要性。

Abstract: This paper compares the impact of different conventional and emerging
technologies and control strategies on frequency quality. We study, in
particular, the long-term dynamic performance of grid-forming (GFM) and
grid-following (GFL) inverter-based resources (IBRs) as well as conventional
synchronous machines. Extensive simulations and several realistic scenarios
consider both short-term and long-term aspects of frequency quality. It is
shown that, while overall GFM IBRs significantly improve frequency quality, a
combination of GFL IBRs providing frequency support such as wind and batteries,
and synchronous condensers, might be enough to meet similar frequency quality
standards. Another result of the paper is that the need for automatic
generation control (AGC) becomes less clear in GFM IBR-dominated grids from a
frequency quality perspective.

</details>


### [402] [Unveiling Uniform Shifted Power Law in Stochastic Human and Autonomous Driving Behavior](https://arxiv.org/abs/2511.00659)
*Wang Chen,Heye Huang,Ke Ma,Hangyu Li,Shixiao Liang,Hang Zhou,Xiaopeng Li*

Main category: eess.SY

TL;DR: 本文发现了一个简单的移位幂律模型，能够统一表征人类驾驶车辆和自动驾驶车辆的行为随机性，特别是在长尾分布区域，显著提升了交通模拟中安全关键行为的仿真真实性。


<details>
  <summary>Details</summary>
Motivation: 当前模型在基于真实数据校准时，由于无法充分表示长尾行为分布，往往难以重现真实的碰撞率，这影响了自动驾驶车辆的安全评估和认证。

Method: 分析全球HV和AV数据集中的微观轨迹数据，采用参数简洁的移位幂律模型，该模型仅需1-2个参数，即使在数据稀疏情况下也能高效校准。

Result: 移位幂律模型平均R²达到0.97，尾部分布几乎完全相同，能够统一拟合频繁行为和罕见的安全关键偏差，显著优于基于高斯分布的基线方法。集成到基于智能体的交通模拟器中，能够重现真实的碰撞模式，碰撞率与真实世界统计一致。

Conclusion: 移位幂律为高风险行为建模提供了统一且数据高效的基础，改进了混合AV/HV交通的仿真安全评估保真度，为自动驾驶技术的仿真驱动验证和全球认证提供了有前景的路径。

Abstract: Accurately simulating rare but safety-critical driving behaviors is essential
for the evaluation and certification of autonomous vehicles (AVs). However,
current models often fail to reproduce realistic collision rates when
calibrated on real-world data, largely due to inadequate representation of
long-tailed behavioral distributions. Here, we uncover a simple yet unifying
shifted power law that robustly characterizes the stochasticity of both
human-driven vehicle (HV) and AV behaviors, especially in the long-tail regime.
The model adopts a parsimonious analytical form with only one or two
parameters, enabling efficient calibration even under data sparsity. Analyzing
large-scale, micro-level trajectory data from global HV and AV datasets, the
shifted power law achieves an average R2 of 0.97 and a nearly identical tail
distribution, uniformly fits both frequent behaviors and rare safety-critical
deviations, significantly outperforming existing Gaussian-based baselines. When
integrated into an agent-based traffic simulator, it enables forward-rolling
simulations that reproduce realistic crash patterns for both HVs and AVs,
achieving rates consistent with real-world statistics and improving the
fidelity of safety assessment without post hoc correction. This discovery
offers a unified and data-efficient foundation for modeling high-risk behavior
and improves the fidelity of simulation-based safety assessments for mixed
AV/HV traffic. The shifted power law provides a promising path toward
simulation-driven validation and global certification of AV technologies.

</details>


### [403] [Hybrid Quantum-Classical Optimization of the Resource Scheduling Problem](https://arxiv.org/abs/2511.00733)
*Tyler Christeson,Md Habib Ullah,Ali Arabnya,Amin Khodaei,Rui Fan*

Main category: eess.SY

TL;DR: 提出一种量子-经典混合算法，用于解决机组组合问题，通过Benders分解将二进制决策与连续调度分离，量子退火器解决二进制主问题，经典方法处理连续子问题。


<details>
  <summary>Details</summary>
Motivation: 传统精确方法计算负担重，元启发式方法缺乏最优性保证且对初始条件敏感。需要新方法处理大规模电力系统的资源调度复杂性。

Method: 使用Benders分解将问题分解为二进制主问题（量子退火器求解）和连续子问题（经典方法求解），通过拉格朗日割反馈实现收敛。

Result: 在10到1000台发电机系统上测试，相比经典混合整数非线性规划，计算时间增长率更低，最优性差距保持在1.63%以下。

Conclusion: 量子退火与经典Benders分解的混合框架能显著加速大规模资源调度，同时保持解的质量，为现代电网复杂性提供可行解决方案。

Abstract: Resource scheduling is critical in many industries, especially in power
systems. The Unit Commitment problem determines the on/off status and output
levels of generators under many constraints. Traditional exact methods, such as
mathematical programming methods or dynamic programming, remain the backbone of
UC solution techniques, but they often rely on linear approximations or
exhaustive search, leading to high computational burdens as system size grows.
Metaheuristic approaches, such as genetic algorithms, particle swarm
optimization, and other evolutionary methods, have been explored to mitigate
this complexity; however, they typically lack optimality guarantees, exhibit
sensitivity to initial conditions, and can become prohibitively time-consuming
for large-scale systems. In this paper, we introduce a quantum-classical hybrid
algorithm for UC and, by extension, other resource scheduling problems, that
leverages Benders decomposition to decouple binary commitment decisions from
continuous economic dispatch. The binary master problem is formulated as a
quadratic unconstrained binary optimization model and solved on a quantum
annealer. The continuous subproblem, which minimizes generation costs, with
Lagrangian cuts feeding back to the master until convergence. We evaluate our
hybrid framework on systems scaled from 10 to 1,000 generation units. Compared
against a classical mixed-integer nonlinear programming baseline, the hybrid
algorithm achieves a consistently lower computation-time growth rate and
maintains an absolute optimality gap below 1.63%. These results demonstrate
that integrating quantum annealing within a hybrid quantum-classical Benders
decomposition loop can significantly accelerate large-scale resource scheduling
without sacrificing solution quality, pointing toward a viable path for
addressing the escalating complexity of modern power grids.

</details>


### [404] [Quantum Computing for EVs to Enhance Grid Resilience and Disaster Relief: Challenges and Opportunities](https://arxiv.org/abs/2511.00736)
*Tyler Christeson,Amin Khodaei,Rui Fan*

Main category: eess.SY

TL;DR: 该论文综述了车网互动(V2G)和移动充电站布局(CSP)的优化方法，探讨了量子计算如何克服当前计算瓶颈，以增强电网韧性并加速灾后恢复。


<details>
  <summary>Details</summary>
Motivation: 极端天气事件导致电网大范围停电，增强电网韧性对保障安全可靠运行至关重要。V2G技术使电动汽车能够作为移动能源资源支持关键负荷或调节电网频率。

Method: 综述了V2G和移动CSP应用的最先进优化方法，分析了现有方法的局限性，并探索量子计算如何解决当前计算瓶颈问题。

Result: 提出了量子计算视角下增强电网韧性和加速灾后恢复的方法框架，为应对日益频繁和严重的极端天气事件提供了新的解决方案。

Conclusion: 量子计算有望克服V2G和移动CSP优化中的计算瓶颈，为增强电网韧性和加速灾后恢复提供新的技术途径。

Abstract: The power grid is the foundation of modern society, however extreme weather
events have increasingly caused widespread outages. Enhancing grid resilience
is therefore critical to maintaining secure and reliable operations. In
disaster relief and restoration, vehicle-to-grid (V2G) technology allows
electric vehicles (EVs) to serve as mobile energy resources by discharging to
support critical loads or regulating grid frequency as needed. Effective V2G
operation requires coordinated charging and discharging of many EVs through
optimization. Similarly, in grid restoration, EVs must be strategically routed
to affected areas, forming the mobile charging station placement (CSP) problem,
which presents another complex optimization challenge. This work reviews
state-of-the-art optimization methods for V2G and mobile CSP applications,
outlines their limitations, and explores how quantum computing (QC) could
overcome current computational bottlenecks. A QC-focused perspective is
presented on enhancing grid resilience and accelerating restoration as extreme
weather events grow more frequent and severe.

</details>


### [405] [High-Power Dual-Channel Field Chamber for High-Frequency Magnetic Neuromodulation](https://arxiv.org/abs/2511.00745)
*Xiaoyang Tian,Hui Wang,Boshuo Wang,Jinshui Zhang,Dong Yan,Jeannette Ingabire,Samantha Coffler,Guillaume Duret,Quoc-Khanh Pham,Gang Bao,Jacob T. Robinson,Stefan M. Goetz,Angel V. Peterchev*

Main category: eess.SY

TL;DR: 开发了一种双通道磁刺激室，用于在自由移动小鼠中精确控制神经活动，通过两个正交磁场通道实现频率选择性热刺激。


<details>
  <summary>Details</summary>
Motivation: 量化高频交变磁场对自由移动小鼠行为的影响，为磁遗传学和磁电刺激等神经调控方法提供精确的实验平台。

Method: 采用优化的线圈设计，构建双通道磁刺激系统，包含50kHz和550kHz两个频率通道，配备液冷系统和观测端口，使用钴掺杂和未掺杂氧化铁纳米粒子验证频率选择性。

Result: 系统在两个频率通道产生高强度磁场（88mT和12.5mT），干扰小于1%，磁场均匀性达±10%（覆盖94%腔体体积），温度上升限制在<0.35°C/s，验证了频率选择性加热能力（3.5°C/s和1.5°C/s）。

Conclusion: 成功开发了高性能双通道磁刺激系统，能够稳定运行4秒，为神经调控研究提供了可靠的工具，支持频率选择性的精确神经刺激。

Abstract: Several novel methods, including magnetogenetics and magnetoelectric
stimulation, use high frequency alternating magnetic fields to precisely
manipulate neural activity. To quantify the behavioral effects of such
interventions in a freely moving mouse, we developed a dual-channel magnetic
chamber, specifically designed for rate-sensitive magnetothermal-genetic
stimulation, and adaptable for other uses of alternating magnetic fields.
Through an optimized coil design, the system allows independent control of two
spatially orthogonal uniform magnetic fields delivered at different frequencies
within a 10 cm x 10 cm x 6 cm chamber. The two channels have nominal
frequencies of 50 and 550 kHz with peak magnetic field strengths of 88 and 12.5
mT, achieved with resonant coil drives having peak voltages of 1.6 and 1.8 kV
and currents of 1.0 and 0.26 kA, respectively. Additionally, a liquid cooling
system enables magnetic field generation for second-level duration, and an
observation port and camera allow video capture of the animal's behavior within
the chamber. The system generates high-amplitude magnetic fields across two
widely separated frequency channels with negligible interference (< 1%).
Relatively uniform magnetic field distribution (+/-10% across 94% of the
chamber volume) is maintained throughout the chamber, and temperature increase
of the inner side of the coil enclosure during the operation is limited to <
0.35 {\deg}C/s to ensure in vivo safety. Using cobalt-doped and undoped iron
oxide nanoparticles, we demonstrate channel-specific heating rates of 3.5
{\deg}C/s and 1.5 {\deg}C/s, respectively, validating frequency-selectivity.
Both channels can run continuously for four seconds stably.

</details>


### [406] [Deep Q-Network for Optimizing NOMA-Aided Resource Allocation in Smart Factories with URLLC Constraints](https://arxiv.org/abs/2511.00765)
*Shi Gengtian,Jiang Liu,Shigeru Shimamoto*

Main category: eess.SY

TL;DR: 提出基于DQN的NOMA资源分配算法，在智能工厂中满足URLLC严格需求，通过动态分配子信道和优化功率水平来最大化吞吐量并满足低延迟约束。


<details>
  <summary>Details</summary>
Motivation: 解决智能工厂中URLLC的严格通信需求，为机器人、传感器和控制器等不同设备提供可靠的实时通信保障。

Method: 使用深度Q网络算法，结合可调参数λ来平衡吞吐量和延迟之间的权衡，动态分配子信道和优化功率水平。

Result: 仿真结果显示机器人获得更高吞吐量，传感器和控制器满足URLLC的低延迟要求，确保实时工业应用的可靠通信。

Conclusion: 所提出的DQN算法能够有效满足智能工厂中不同设备的多样化通信需求，在吞吐量和延迟之间实现良好平衡。

Abstract: This paper presents a Deep Q-Network (DQN)- based algorithm for NOMA-aided
resource allocation in smart factories, addressing the stringent requirements
of Ultra-Reliable Low-Latency Communication (URLLC). The proposed algorithm
dynamically allocates sub-channels and optimizes power levels to maximize
throughput while meeting strict latency constraints. By incorporating a tunable
parameter {\lambda}, the algorithm balances the trade-off between throughput
and latency, making it suitable for various devices, including robots, sensors,
and controllers, each with distinct communication needs. Simulation results
show that robots achieve higher throughput, while sensors and controllers meet
the low-latency requirements of URLLC, ensuring reliable communication for
real-time industrial applications.

</details>


### [407] [Minimizing Maximum Latency of Task Offloading for Multi-UAV-assisted Maritime Search and Rescue](https://arxiv.org/abs/2511.00844)
*Shuang Qi,Bin Lin,Yiqin Deng,Xianhao Chen,Yuguang Fang*

Main category: eess.SY

TL;DR: 本文提出了一种多无人机辅助海上搜救系统，通过联合优化计算卸载决策、中继无人机部署和监控无人机与救援目标的关联，最小化所有监控无人机的最大总延迟。


<details>
  <summary>Details</summary>
Motivation: 无人机在海上搜救中发挥重要作用，但其有限的计算能力和能源限制了系统效率。需要解决数据传输延迟和资源约束问题。

Method: 构建包含多个监控无人机和中继无人机的系统，将联合优化问题分解为三个子问题，提出有效的迭代算法求解。

Result: 数值仿真结果表明，所提算法在各种性能参数下均表现出有效性。

Conclusion: 该多无人机系统通过优化计算卸载和部署策略，能够有效降低海上搜救任务的延迟，提高救援效率。

Abstract: Unmanned Aerial Vehicles (UAVs) play a crucial role in Maritime Search and
Rescue (MSAR), contributing to the improvement of rescue efficiency and
reduction of casualties. Typically, UAVs equipped with cameras collect data
from disaster areas and transmit it to the shore-based rescue command centers.
By deploying Mobile Edge Computing (MEC) servers, UAVs can pre-process video
footage to reduce data transmission volume, thus reducing transmission delays.
However, the limited computational capacity and energy of UAVs pose significant
challenges to the efficiency of UAV-assisted MSAR systems. To address these
problems, in this paper, we investigate a multi-UAV assisted MSAR system
consisting of multiple Surveillance UAVs (S-UAVs) and a Relay UAV (R-UAV).
Then, we formulate a joint optimization problem to minimize the maximum total
latency among all S-UAVs via jointly making the computing offloading decisions,
R-UAV deployment, and the association between a S-UAV and rescue targets while
ensuring that all targets are monitored by S-UAVs. Since the formulated
optimization problem is typically hard to solve due to its non-convexity, we
propose an effective iterative algorithm by breaking it into three
sub-problems. Numerical simulation results show the effectiveness of the
proposed algorithm with various performance parameters.

</details>


### [408] [Traffic-Aware Grid Planning for Dynamic Wireless Electric Vehicle Charging](https://arxiv.org/abs/2511.00941)
*Dipanjan Ghose,S Sivaranjani,Junjie Qin*

Main category: eess.SY

TL;DR: 提出了一个考虑交通状况的动态无线充电电网规划框架，通过宏观交通流模型估计实时充电需求，结合最优潮流优化微电网规模，相比最坏情况规划显著降低成本并确保可靠性。


<details>
  <summary>Details</summary>
Motivation: 动态无线充电技术虽然能减少电池尺寸和充电时间，但其短时高功率需求会给电网带来压力，且充电需求受交通流量、速度等因素影响，需要综合考虑交通行为和能源消耗的集成规划方法。

Method: 使用宏观细胞传输模型估计DWC走廊的时空充电需求，将其集成到基于交流最优潮流的公式中，优化支持DWC的微电网规模，同时最小化运营成本。

Result: 在加州I-210W高速公路14英里路段上的测试表明，相比最坏情况建模，交通感知规划显著降低了基础设施成本，并在各种交通条件下都能满足充电需求。

Conclusion: 交通感知的电网规划框架能够获得成本更低、更易操作的系统设计，比依赖最坏情况交通数据的规划模型更优。

Abstract: Dynamic Wireless Electric Vehicle Charging (DWC) on electrified roadways is
an emerging technology that can significantly reduce battery sizes, eliminate
charging downtime, and alleviate range anxiety, specially for long-haul
transportation and fleet operations of electric vehicles (EVs). However, these
systems introduce new challenges for power system planning due to their
short-duration and high-power demands which can strain the grid if not properly
managed. As the energy demands from DWC depend on vehicle speed, density, dwell
time in charging zones, and load profiles along road segments, there is a need
for integrated planning of such systems, jointly considering both traffic
behavior and EV energy consumption. In this paper, we propose a traffic-aware
grid planning framework for DWC. We leverage a macroscopic Cell Transmission
Model of traffic flow to estimate real-time, spatiotemporal EV charging demand
from DWC corridors. The demand model is then integrated into an AC Optimal
Power Flow based formulation to optimally size a microgrid that supports DWC
under varying traffic conditions while minimizing the cost of operation. Our
framework explicitly models how spatiotemporal traffic patterns affect the
utilization of grid resources to obtain system designs that achieve lower costs
and are easier to operationalize as compared to planning models that rely on
worst-case traffic data.
  We demonstrate the framework on data from a 14-mile segment of the I-210W
highway in California, USA, evaluating multiple traffic scenarios like
free-flow, severe congestion, accidents of varying severity, and natural
disasters like forest fires. Our results demonstrate that traffic-aware grid
planning significantly reduces infrastructure costs as compared to
worst-scenario based modeling, while ensuring reliability of service in terms
of meeting charging demands under diverse traffic conditions.

</details>


### [409] [Secure Distributed Consensus Estimation under False Data Injection Attacks: A Defense Strategy Based on Partial Channel Coding](https://arxiv.org/abs/2511.00963)
*Jiahao Huang,Marios M. Polycarpou,Wen Yang,Fangfei Li,Yang Tang*

Main category: eess.SY

TL;DR: 本文研究分布式估计中虚假数据注入攻击的安全问题，提出攻击检测和编码防御两种策略，分析系统漏洞条件并实现安全性与编码成本的平衡。


<details>
  <summary>Details</summary>
Motivation: 分布式估计系统中存在虚假数据注入攻击的安全威胁，攻击者可以操纵部分信道传输数据，破坏估计精度同时保持隐蔽性，需要有效的防御机制。

Method: 提出两种防御策略：1）利用局部估计间的欧氏距离检测攻击；2）采用编码方案保护传输数据，使用时变编码矩阵降低被破解风险。

Result: 证明了距离检测方法能解决大部分安全漏洞，编码方案可作为补充增强，通过选择关键信道编码实现安全性与成本的平衡。

Conclusion: 结合两种防御策略能有效应对分布式估计中的虚假数据注入攻击，数值仿真验证了理论分析的有效性。

Abstract: This article investigates the security issue caused by false data injection
attacks in distributed estimation, wherein each sensor can construct two types
of residues based on local estimates and neighbor information, respectively.
The resource-constrained attacker can select partial channels from the sensor
network and arbitrarily manipulate the transmitted data. We derive necessary
and sufficient conditions to reveal system vulnerabilities, under which the
attacker is able to diverge the estimation error while preserving the
stealthiness of all residues. We propose two defense strategies with mechanisms
of exploiting the Euclidean distance between local estimates to detect attacks,
and adopting the coding scheme to protect the transmitted data, respectively.
It is proven that the former has the capability to address the majority of
security loopholes, while the latter can serve as an additional enhancement to
the former. By employing the time-varying coding matrix to mitigate the risk of
being cracked, we demonstrate that the latter can safeguard against adversaries
injecting stealthy sequences into the encoded channels. Hence, drawing upon the
security analysis, we further provide a procedure to select security-critical
channels that need to be encoded, thereby achieving a trade-off between
security and coding costs. Finally, some numerical simulations are conducted to
demonstrate the theoretical results.

</details>


### [410] [On Structural Properties of Risk-Averse Optimal Stopping Problems](https://arxiv.org/abs/2511.01022)
*Xingyu Ren,Michael C. Fu,Steven I. Marcus*

Main category: eess.SY

TL;DR: 该论文研究了在时间一致动态风险度量下的最优停止问题，证明了价值函数单调性，并建立了控制极限最优策略的存在条件。


<details>
  <summary>Details</summary>
Motivation: 虽然风险中性模型下的最优停止问题已有成熟理论，但在风险规避设置下的结构性质研究仍不足，特别是相干风险度量缺乏塔性质和可加性，使得分析复杂化。

Method: 通过分析相干风险度量的风险包络，证明当风险包络存在最小元素时，风险规避最优停止问题可转化为等价的风险中性形式，并开发了识别控制极限最优策略的一般程序。

Result: 证明了价值函数单调性与风险中性情况类似，建立了保证控制极限最优策略存在的可验证条件，并通过运营、营销和金融领域的实例验证了理论。

Conclusion: 该研究为风险规避环境下的最优停止问题提供了系统的结构分析框架，建立了从风险规避到风险中性问题的等价转换方法，并给出了实用的最优策略存在条件。

Abstract: We establish structural properties of optimal stopping problems under
time-consistent dynamic (coherent) risk measures, focusing on value function
monotonicity and the existence of control limit (threshold) optimal policies.
While such results are well developed for risk-neutral (expected-value) models,
they remain underexplored in risk-averse settings. Coherent risk measures
typically lack the tower property and are subadditive rather than additive,
complicating structural analysis. We show that value function monotonicity
mirrors the risk-neutral case. Moreover, if the risk envelope associated with
each coherent risk measure admits a minimal element, the risk-averse optimal
stopping problem reduces to an equivalent risk-neutral formulation. We also
develop a general procedure for identifying control limit optimal policies and
use it to derive practical, verifiable conditions on the risk measures and MDP
structure that guarantee their existence. We illustrate the theory and verify
these conditions through optimal stopping problems arising in operations,
marketing, and finance.

</details>


### [411] [Online Energy Storage Arbitrage under Imperfect Predictions: A Conformal Risk-Aware Approach](https://arxiv.org/abs/2511.01032)
*Yiqian Wu,Ming Yi,Bolun Xu,James Anderson*

Main category: eess.SY

TL;DR: 提出一种基于保形决策理论的储能套利方法，通过预测集动态调整决策保守度来控制价格预测不准确带来的下行风险。


<details>
  <summary>Details</summary>
Motivation: 储能套利完全依赖对未来市场价格的预测，而不准确的价格预测可能导致显著的利润损失，需要控制这种下行风险。

Method: 基于保形决策理论开发控制器，通过预测集动态调整决策保守度；建立时间差分误差作为可测量代理指标；开发两种在线校准策略：基于预测误差的适应和基于价值误差的校准。

Result: 分析证明保形控制器具有有界长期风险和收敛保证；案例研究显示在不同预测条件下相比基准方法在平衡风险和机会方面表现更优。

Conclusion: 该方法能有效管理潜在利润损失的风险暴露，在储能套利中实现风险与收益的良好平衡。

Abstract: This work proposes a conformal approach for energy storage arbitrage to
control the downside risks arose from imperfect price forecasts. Energy storage
arbitrage relies solely on predictions of future market prices, while
inaccurate price predictions may lead to significant profit losses. Based on
conformal decision theory, we describe a controller that dynamically adjusts
decision conservativeness through prediction sets without distributional
assumptions. To enable online calibration when online profit loss feedback is
unobservable, we establish that a temporal difference error serves as a
measurable proxy. Building on this insight, we develop two online calibration
strategies: prediction error-based adaptation targeting forecast accuracy, and
value error-based calibration focusing on decision quality. Analysis of the
conformal controller proves bounded long-term risk with convergence guarantees
in temporal difference error, which further effectively manages risk exposure
in potential profit losses. Case studies demonstrate superior performance in
balancing risk and opportunity compared to benchmarks under varying forecast
conditions.

</details>


### [412] [GOSPA-Driven Non-Myopic Multi-Sensor Management with Multi-Bernoulli Filtering](https://arxiv.org/abs/2511.01045)
*George Jones,Angel Garcia-Fernandez*

Main category: eess.SY

TL;DR: 提出一种基于多伯努利滤波和非近视最小化问题的多传感器管理算法，使用GOSPA误差上界和蒙特卡洛树搜索实现多目标跟踪


<details>
  <summary>Details</summary>
Motivation: 解决多传感器在同一监视区域进行多目标跟踪时的传感器管理问题，需要非近视的决策方法

Method: 基于多伯努利滤波，通过最小化未来时间窗口内的GOSPA误差上界，使用蒙特卡洛树搜索实现传感器动作选择

Result: 通过仿真验证了算法的有效性

Conclusion: 该算法能够实现多传感器协同优化，提升多目标跟踪性能

Abstract: In this paper, we propose a non-myopic sensor management algorithm for
multi-target tracking, with multiple sensors operating in the same surveillance
area. The algorithm is based on multi-Bernoulli filtering and selects the
actions that solve a non-myopic minimisation problem, where the cost function
is the mean square generalised optimal sub-pattern assignment (GOSPA) error,
over a future time window. For tractability, the sensor management algorithm
actually uses an upper bound of the GOSPA error and is implemented via Monte
Carlo Tree Search (MCTS). The sensors have the ability to jointly optimise and
select their actions with the considerations of all other sensors in the
surveillance area. The benefits of the proposed algorithm are analysed via
simulations.

</details>


### [413] [Robust Self-Triggered Control Approaches Optimizing Sampling Sequences with Synchronous Measurements](https://arxiv.org/abs/2511.01057)
*Abbas Tariverdi*

Main category: eess.SY

TL;DR: 提出了一种新颖的最优自触发控制方案，通过预计算有限时间范围内的采样序列来减少资源使用，同时保证系统稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统周期执行的控制算法在嵌入式控制和共享网络中会导致CPU、网络带宽等资源使用效率低下，需要更高效的触发机制。

Method: 基于当前状态信息预计算有限时间范围内的下一个采样序列，引入最优自触发方案，保证无扰动系统的指数稳定性和有扰动系统的全局一致最终有界性。

Result: 仿真结果表明该方法具有显著优势，能够有效减少资源使用同时保持系统性能。

Conclusion: 所提出的自触发控制方案在保证系统稳定性和鲁棒性的前提下，显著提高了资源使用效率，特别适用于嵌入式控制和网络控制系统。

Abstract: Feedback control algorithms traditionally rely on periodic execution on
digital platforms. While this simplifies design and analysis, it often leads to
inefficient resource usage (e.g., CPU, network bandwidth) in embedded control
and shared networks. This work investigates self-triggering implementations of
linear controllers in sampled-data systems with synchronous measurements. Our
approach precomputes the next sampling sequence over a finite horizon based on
current state information. We introduce a novel optimal self-triggering scheme
that guarantees exponential stability for unperturbed systems and global
uniform ultimate boundedness for perturbed systems. This ensures robustness
against external disturbances with explicit performance guarantees. Simulations
demonstrate the benefits of our approach.

</details>


### [414] [Universal Barrier Functions for Safety and Stability of Constrained Nonlinear Systems](https://arxiv.org/abs/2511.01067)
*Vrushabh Zinage,Efstathios Bakolas*

Main category: eess.SY

TL;DR: 提出通用屏障函数(UBF)框架，用于合成满足复杂安全规范和输入约束的非线性系统安全稳定控制器


<details>
  <summary>Details</summary>
Motivation: 解决非线性系统在复杂安全规范和输入约束下的安全稳定控制器合成问题

Method: 引入通用屏障函数(UBF)，将稳定性和安全性准则编码为单一连续可微标量函数，并考虑输入约束，基于UBF构建二次规划(UBF-QP)生成控制输入

Result: 证明UBF-QP在UBF存在时是可行的，且在温和条件下UBF总是存在，框架可扩展到高阶相对度系统，数值仿真验证了方法的有效性

Conclusion: UBF框架为非线性系统在复杂安全规范和输入约束下提供了一种有效的安全稳定控制器合成方法

Abstract: In this paper, we address the problem of synthesizing safe and stabilizing
controllers for nonlinear systems subject to complex safety specifications and
input constraints. We introduce the Universal Barrier Function (UBF), a single
continuously differentiable scalar-valued function that encodes both stability
and safety criteria while accounting for input constraints. Using the UBF, we
formulate a Quadratic Program (UBF-QP) to generate control inputs that are both
safe and stabilizing under input constraints. We demonstrate that the UBF-QP is
feasible if a UBF exists. Furthermore, under mild conditions, we prove that a
UBF always exists. The proposed framework is then extended to systems with
higher relative degrees. Finally, numerical simulations illustrate the
effectiveness of our proposed approach.

</details>


### [415] [Deep Learning-Accelerated Shapley Value for Fair Allocation in Power Systems: The Case of Carbon Emission Responsibility](https://arxiv.org/abs/2511.01229)
*Yuanhao Feng,Tao Sun,Yan Meng,Xuxin Yang,Donghan Feng*

Main category: eess.SY

TL;DR: SurroShap是一个可扩展的Shapley值近似框架，结合高效联盟采样和深度学习代理模型，首次实现了数千个实体的电力系统公平分配问题。


<details>
  <summary>Details</summary>
Motivation: 电力系统中成本、收益和排放的公平分配是一个持续挑战，Shapley值提供了公理公平的解决方案，但计算障碍限制了其在小规模应用之外的使用。

Method: 结合高效联盟采样和深度学习代理模型来加速特征函数评估，通过理论误差界限证明时间平均分配收敛到精确Shapley值的ε范围内。

Result: 在9个系统（26到1951个实体）上的实验显示，即使在最大规模下也能在实时操作窗口内完成，相比其他基于采样的方法实现了10^4-10^5倍的加速，同时保持紧密误差界限。

Conclusion: 基于Shapley的碳排放分配具有六个理想特性，使个体利益与脱碳目标保持一致，在德克萨斯2000节点系统上的年度模拟验证了实际适用性。

Abstract: Allocating costs, benefits, and emissions fairly among power system
participant entities represents a persistent challenge. The Shapley value
provides an axiomatically fair solution, yet computational barriers have
limited its adoption beyond small-scale applications. This paper presents
SurroShap, a scalable Shapley value approximation framework combining efficient
coalition sampling with deep learning surrogate models that accelerate
characteristic function evaluations. Exemplified through carbon emission
responsibility allocation in power networks, SurroShap enables Shapley-based
fair allocation for power systems with thousands of entities for the first
time. We derive theoretical error bounds proving that time-averaged SurroShap
allocations converge to be $\varepsilon$-close to exact Shapley values.
Experiments on nine systems ranging from 26 to 1,951 entities demonstrate
completion within the real-time operational window even at maximum scale,
achieving 10^4-10^5 speedups over other sampling-based methods while
maintaining tight error bounds. The resulting Shapley-based carbon allocations
possess six desirable properties aligning individual interests with
decarbonization goals. Year-long simulations on the Texas 2000-bus system
validate real-world applicability, with regional analysis revealing how
renewable-rich areas offset emission responsibility through exports while load
centers bear responsibility for driving system-wide generation.

</details>


### [416] [Orthogonal-by-construction augmentation of physics-based input-output models](https://arxiv.org/abs/2511.01321)
*Bendegúz M. Györök,Maarten Schoukens,Tamás Péni,Roland Tóth*

Main category: eess.SY

TL;DR: 提出了一种正交构造的模型增强结构，用于输入输出模型，确保在适当的可识别性条件下恢复物理真实参数。


<details>
  <summary>Details</summary>
Motivation: 传统并行连接的模型增强结构由于系统动力学表示重叠，往往导致物理上不现实的参数，损害模型可解释性。

Method: 引入正交构造的模型增强结构，将基于物理的模型和机器学习组件以正交方式连接。

Result: 该方法能够保证在适当条件下恢复物理真实参数，解决了传统方法参数估计不现实的问题。

Conclusion: 正交构造的模型增强结构在保持模型可解释性的同时，提高了参数估计的物理合理性。

Abstract: Model augmentation is a promising approach for integrating
first-principles-based models with machine learning components. Augmentation
can result in better model accuracy and faster convergence compared to
black-box system identification methods, while maintaining interpretability of
the models in terms of how the original dynamics are complemented by learning.
A widely used augmentation structure in the literature is based on the parallel
connection of the physics-based and learning components, for both of which the
corresponding parameters are jointly optimized. However, due to overlap in
representation of the system dynamics by such an additive structure, estimation
often leads to physically unrealistic parameters, compromising model
interpretability. To overcome this limitation, this paper introduces a novel
orthogonal-by-construction model augmentation structure for input-output
models, that guarantees recovery of the physically true parameters under
appropriate identifiability conditions.

</details>


### [417] [Risk Aware Safe Control with Cooperative Sensing for Dynamic Obstacle Avoidance](https://arxiv.org/abs/2511.01403)
*Pei Yu Chang,Qizhe Xu,Vishnu Renganathan,Qadeer Ahmed*

Main category: eess.SY

TL;DR: 提出了基于Wasserstein重心和CVaR的风险感知安全滤波器，用于自动驾驶车辆在感知和通信不确定性下的安全控制，通过实验验证了其优于传统MPC-CBF方法的安全性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶在感知和通信不确定性下的安全控制问题，传统方法难以处理分布优化和尾部风险最小化。

Method: 使用Wasserstein重心优化动态障碍物位置分布，引入CVaR构建风险感知控制屏障函数框架，结合MPC进行路径跟踪，安全滤波器调节控制输入。

Result: 在全尺寸自动驾驶车辆上的实验表明，相比基线MPC-CBF设计，该方法提高了安全裕度和鲁棒性，能有效处理测量噪声、通信扰动和输入干扰。

Conclusion: 提出的风险感知安全滤波器在实际自动驾驶车辆上具有实用价值，能够显著提升系统在不确定性环境下的安全性能。

Abstract: This paper presents the design, development, and on vehicle implementation
and validation of a safety critical controller for autonomous driving under
sensing and communication uncertainty. Cooperative sensing, fused via a
Wasserstein barycenter (WB), is used to optimize the distribution of the
dynamic obstacle locations. The Conditional Value at Risk (CVaR) is introduced
to form a risk aware control-barrier-function (CBF) framework with the
optimized distribution samplings. The proposed WB CVaR CBF safety filter
improves control inputs that minimize tail risk while certifying forward
invariance of the safe set. A model predictive controller (MPC) performs path
tracking, and the safety filter modulates the nominal control inputs to enforce
risk aware constraints. We detail the software architecture and integration
with vehicle actuation and cooperative sensing. The approach is evaluated on a
full-scale autonomous vehicle (AV) in scenarios with measurement noise,
communication perturbations, and input disturbances, and is compared against a
baseline MPC CBF design. Results demonstrate improved safety margins and
robustness, highlighting the practicality of deploying the risk-aware safety
filter on an actual AV.

</details>


### [418] [Evolutionary Dynamics in Continuous-time Finite-state Mean Field Games - Part I: Equilibria](https://arxiv.org/abs/2511.01452)
*Leonardo Pedroso,Andrea Agazzi,W. P. M. H. Heemels,Mauro Salazar*

Main category: eess.SY

TL;DR: 该论文研究了具有大种群玩家的动态博弈，玩家在连续时间中选择有限动作，每个玩家有有限状态空间的状态随动作随机演化。奖励取决于自身状态、动作以及种群状态和动作的分布。提出了新的混合稳态纳什均衡概念，并建立了与平均场进化模型的关系。


<details>
  <summary>Details</summary>
Motivation: 传统演化博弈论主要关注无个体玩家状态动态的静态博弈，本文旨在对具有个体状态动态的动态博弈进行全面的演化分析，填补这一研究空白。

Method: 提出了演化模型和有限种群博弈的平均场近似，建立了强近似保证。引入了混合稳态纳什均衡概念，分析了其与平均场演化模型不动点的关系。

Result: 证明了标准动态博弈解概念缺乏演化解释，而混合稳态纳什均衡具有演化解释。建立了平均场近似的理论保证，并研究了混合稳态纳什均衡的演化稳定性。

Conclusion: 混合稳态纳什均衡为具有状态动态的大种群动态博弈提供了合适的演化解释解概念，平均场方法为分析此类博弈提供了有效框架。

Abstract: We study a dynamic game with a large population of players who choose actions
from a finite set in continuous time. Each player has a state in a finite state
space that evolves stochastically with their actions. A player's reward depends
not only on their own state and action but also on the distribution of states
and actions across the population, capturing effects such as congestion in
traffic networks. While prior work in evolutionary game theory has primarily
focused on static games without individual player state dynamics, we present
the first comprehensive evolutionary analysis of such dynamic games. We propose
an evolutionary model together with a mean field approximation of the
finite-population game and establish strong approximation guarantees. We show
that standard solution concepts for dynamic games lack an evolutionary
interpretation, and we propose a new concept - the Mixed Stationary Nash
Equilibrium (MSNE) - which admits one. We analyze the relationship between MSNE
and the rest points of the mean field evolutionary model and study the
evolutionary stability of MSNE.

</details>


### [419] [Deep Learning Prediction of Beam Coherence Time for Near-FieldTeraHertz Networks](https://arxiv.org/abs/2511.01491)
*Irched Chafaa,E. Veronica Belmega,Giacomo Bacci*

Main category: eess.SY

TL;DR: 提出了一种用于移动太赫兹网络的波束相干时间概念，并通过深度学习模型预测该时间，以动态调整波束成形，显著减少波束更新开销。


<details>
  <summary>Details</summary>
Motivation: 太赫兹通信中大规模天线阵列和精确波束成形对链路可靠性至关重要，但随着天线数量增加，波束对准和跟踪在移动网络中会产生过高开销。此外，近场区域随天线阵列尺寸和载波频率扩展，需要调整波束成形以考虑球面波前。

Method: 引入波束相干时间概念，并提出基于简单前馈神经网络的深度学习模型，利用时间相关输入预测波束相干时间，动态调整波束成形。

Result: 数值结果表明该方法能有效提高数据速率，同时减少开销，特别是在高移动性（如车载）场景下。

Conclusion: 所提出的方法通过预测波束相干时间和动态调整波束成形，显著降低了移动太赫兹网络中的波束更新开销，提高了系统性能。

Abstract: Large multiple antenna arrays coupled with accu- rate beamforming are
essential in terahertz (THz) communi- cations to ensure link reliability.
However, as the number of antennas increases, beam alignment (focusing) and
beam tracking in mobile networks incur prohibitive overhead. Additionally, the
near-field region expands both with the size of antenna arrays and the carrier
frequency, calling for adjustments in the beamforming to account for spherical
wavefront instead of the conventional planar wave assumption. In this letter,
we introduce a novel beam coherence time for mobile THz networks, to
drastically reduce the rate of beam updates. Then, we propose a deep learning
model, relying on a simple feedforward neural network with a time-dependent
input, to predict the beam coherence time and adjust the beamforming on the fly
with minimal overhead. Our numerical results demonstrate the effectiveness of
the proposed approach by enabling higher data rates while reducing the
overhead, especially at high (i.e., vehicular) mobility.

</details>


### [420] [On polynomial explicit partial estimator design for nonlinear systems with parametric uncertainties](https://arxiv.org/abs/2511.01638)
*Mazen Alamir*

Main category: eess.SY

TL;DR: 提出了一种基于稀疏多元多项式关系的数据驱动部分估计器设计方法，用于处理具有参数不确定性的非线性系统，在小样本数据情况下表现优于其他机器学习方法。


<details>
  <summary>Details</summary>
Motivation: 针对非线性系统中存在的参数不确定性，需要开发有效的数据驱动估计方法，特别是在学习数据有限的情况下。

Method: 使用稀疏多元多项式关系构建数据驱动部分估计器，并与多种机器学习/深度学习方法进行比较验证。

Result: 实验结果表明，所提出的稀疏识别方案在小样本数据情况下优于其他可能的机器学习替代方法。

Conclusion: 稀疏多元多项式方法为非线性系统的参数不确定性估计提供了一种有效的解决方案，特别适用于数据有限的应用场景。

Abstract: This paper investigates the idea of designing data-driven partial estimators
for nonlinear systems showing parametric uncertainties using sparse
multivariate polynomial relationships. A general framework is first presented
and then validated on two illustrative examples with comparison to different
possible Machine/Deep-Learning based alternatives. The results suggests the
superiority of the proposed sparse identification scheme, at least when the
learning data is small.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [421] [Technical Analysis Meets Machine Learning: Bitcoin Evidence](https://arxiv.org/abs/2511.00665)
*José Ángel Islas Anguiano,Andrés García-Medina*

Main category: q-fin.CP

TL;DR: 比较LightGBM、LSTM与EMA、MACD+ADX策略在比特币交易中的表现，LSTM模型在不到一年内获得约65.23%的累计回报，显著优于其他方法和买入持有策略。


<details>
  <summary>Details</summary>
Motivation: 美国证券交易委员会于2024年1月10日批准首个现货比特币ETF，促使研究如何利用交易信号在比特币市场最大化利润。

Method: 使用两种机器学习模型（LightGBM和LSTM）和两种技术分析策略（EMA交叉和MACD+ADX组合）进行比特币交易性能比较。

Result: LSTM模型在不到一年内实现了约65.23%的累计回报，显著优于LightGBM、EMA策略、MACD+ADX策略以及基准买入持有策略。

Conclusion: 研究强调了在快速发展的加密货币领域更深入整合机器学习与技术分析的潜力。

Abstract: In this note, we compare Bitcoin trading performance using two machine
learning models-Light Gradient Boosting Machine (LightGBM) and Long Short-Term
Memory (LSTM)-and two technical analysis-based strategies: Exponential Moving
Average (EMA) crossover and a combination of Moving Average
Convergence/Divergence with the Average Directional Index (MACD+ADX). The
objective is to evaluate how trading signals can be used to maximize profits in
the Bitcoin market. This comparison was motivated by the U.S. Securities and
Exchange Commission's (SEC) approval of the first spot Bitcoin exchange-traded
funds (ETFs) on 2024-01-10. Our results show that the LSTM model achieved a
cumulative return of approximately 65.23% in under a year, significantly
outperforming LightGBM, the EMA and MACD+ADX strategies, as well as the
baseline buy-and-hold. This study highlights the potential for deeper
integration of machine learning and technical analysis in the rapidly evolving
cryptocurrency landscape.

</details>


### [422] [Trade Execution Flow as the Underlying Source of Market Dynamics](https://arxiv.org/abs/2511.01471)
*Mikhail Gennadievich Belov,Victor Victorovich Dubov,Vadim Konstantinovich Ivanov,Alexander Yurievich Maslov,Olga Vladimirovna Proshina,Vladislav Gennadievich Malyshkin*

Main category: q-fin.CP

TL;DR: 本文通过实验证明执行流I=dV/dt是市场动态的基本驱动力，开发了基于Radon-Nikodym导数的数值框架来计算执行流，并引入基于Christoffel函数谱的框架作为PCA的替代方案。


<details>
  <summary>Details</summary>
Motivation: 探索市场动态的基本驱动力，传统PCA方法存在局限性，需要开发更强大的分析框架来理解市场行为。

Method: 使用Radon-Nikodym导数从采样矩计算执行流，自动确定可操作的阈值和特征时间尺度；引入基于Christoffel函数谱的框架，该框架对输入属性的任意非退化线性变换具有不变性。

Result: 方法在实际市场数据上得到验证，能够有效识别市场动态的驱动因素。

Conclusion: 执行流是市场动态的基本驱动力，所提出的框架提供了比传统PCA更强大的分析工具，具有更好的不变性特性。

Abstract: In this work, we demonstrate experimentally that the execution flow, $I =
dV/dt$, is the fundamental driving force of market dynamics. We develop a
numerical framework to calculate execution flow from sampled moments using the
Radon-Nikodym derivative. A notable feature of this approach is its ability to
automatically determine thresholds that can serve as actionable triggers. The
technique also determines the characteristic time scale directly from the
corresponding eigenproblem. The methodology has been validated on actual market
data to support these findings. Additionally, we introduce a framework based on
the Christoffel function spectrum, which is invariant under arbitrary
non-degenerate linear transformations of input attributes and offers an
alternative to traditional principal component analysis (PCA), which is limited
to unitary invariance.

</details>


<div id='q-fin.PR'></div>

# q-fin.PR [[Back]](#toc)

### [423] [Asset Pricing in the Presence of Market Microstructure Noise](https://arxiv.org/abs/2511.00308)
*Peter Yegon,W. Brent Lindquist,Svetlozar T. Rachev*

Main category: q-fin.PR

TL;DR: 提出了两个将市场微观结构噪声纳入资产和欧式期权动态定价的模型：一个基于Black-Scholes-Merton连续时间框架，另一个是基于Grossman-Stiglitz模型的离散二叉树扩展模型。


<details>
  <summary>Details</summary>
Motivation: 将市场微观结构噪声的总效应纳入资产和期权定价模型，以更准确地反映真实市场条件。

Method: 开发了两个市场完备的模型：连续时间模型和离散二叉树模型，两者都提供了唯一的等价鞅测度来建立风险中性世界和现实世界价格动态参数之间的映射关系。

Result: 通过实证例子提取了模型系数，特别是表征微观结构噪声对价格影响的系数。离散模型能够分离噪声对波动率和漂移系数的影响。

Conclusion: 成功开发了能够捕捉微观结构噪声效应的定价模型，并提供了识别主要噪声来源的证据。

Abstract: We present two models for incorporating the total effect of market
microstructure noise into dynamic pricing of assets and European options. The
first model is developed under a Black-Scholes-Merton, continuous-time
framework. The second model is a discrete, binomial tree model developed as an
extension of the static Grossman-Stiglitz model. Both models are market
complete, providing a unique equivalent martingale measure that establishes a
unique map between parameters governing the risk-neutral and real-world price
dynamics. We provide empirical examples to extract the coefficients in the
model, in particular those coefficients characterizing the influence of the
microstructure noise on prices. In addition to isolating the impact of noise on
the volatility, the discrete model enables us to extract the noise impact on
the drift coefficient. We provide evidence for the primary microstructure noise
we believe our empirical examples capture.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [424] [Multimodal Learning with Augmentation Techniques for Natural Disaster Assessment](https://arxiv.org/abs/2511.00004)
*Adrian-Dinu Urse,Dumitru-Clementin Cercel,Florin Pop*

Main category: cs.CY

TL;DR: 本文探索了在CrisisMMD多模态数据集上使用数据增强技术来解决类别不平衡和样本有限的问题，包括基于扩散的视觉增强和多种文本增强方法，结果表明这些方法能提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 自然灾害评估需要准确快速的信息获取，社交媒体是重要的实时信息来源。但现有数据集存在类别不平衡和样本有限的问题，使得模型开发具有挑战性。

Method: 对于视觉数据使用基于扩散的方法（Real Guidance和DiffuseMix），对于文本数据使用回译、基于transformer的改写和基于图像描述的增强。在单模态、多模态和多视图学习设置下评估这些方法。

Result: 结果显示选定的增强方法能够提高分类性能，特别是对于代表性不足的类别，而多视图学习显示出潜力但需要进一步改进。

Conclusion: 本研究强调了构建更鲁棒的自然灾害评估系统的有效增强策略。

Abstract: Natural disaster assessment relies on accurate and rapid access to
information, with social media emerging as a valuable real-time source.
However, existing datasets suffer from class imbalance and limited samples,
making effective model development a challenging task. This paper explores
augmentation techniques to address these issues on the CrisisMMD multimodal
dataset. For visual data, we apply diffusion-based methods, namely Real
Guidance and DiffuseMix. For text data, we explore back-translation,
paraphrasing with transformers, and image caption-based augmentation. We
evaluated these across unimodal, multimodal, and multi-view learning setups.
Results show that selected augmentations improve classification performance,
particularly for underrepresented classes, while multi-view learning introduces
potential but requires further refinement. This study highlights effective
augmentation strategies for building more robust disaster assessment systems.

</details>


### [425] [Chitchat with AI: Understand the supply chain carbon disclosure of companies worldwide through Large Language Model](https://arxiv.org/abs/2511.00024)
*Haotian Hang,Yueyang Shen,Vicky Zhu,Jose Cruz,Michelle Li*

Main category: cs.CY

TL;DR: 提出基于大语言模型的决策支持框架，用于大规模评估企业碳披露质量，将非结构化披露转化为可量化、可解释、可比较的智能信息。


<details>
  <summary>Details</summary>
Motivation: 企业碳披露对可持续发展至关重要，但CDP数据的异质性和自由形式特性给分析带来挑战，需要新的方法来支持基准测试、合规监控和投资筛选。

Method: 开发主评分标准，整合11年CDP数据，结合基于百分位的标准化方法，利用大语言模型进行叙事评分。

Result: 发现技术和德国等国家和行业在评分标准一致性方面表现更好，而其他行业和地区存在波动性或表面参与。

Conclusion: 基于LLM的方法显著提升了AI决策支持系统在气候治理领域的能力，为投资者、监管者和企业ESG战略制定者提供了重要见解。

Abstract: In the context of global sustainability mandates, corporate carbon disclosure
has emerged as a critical mechanism for aligning business strategy with
environmental responsibility. The Carbon Disclosure Project (CDP) hosts the
world's largest longitudinal dataset of climate-related survey responses,
combining structured indicators with open-ended narratives, but the
heterogeneity and free-form nature of these disclosures present significant
analytical challenges for benchmarking, compliance monitoring, and investment
screening. This paper proposes a novel decision-support framework that
leverages large language models (LLMs) to assess corporate climate disclosure
quality at scale. It develops a master rubric that harmonizes narrative scoring
across 11 years of CDP data (2010-2020), enabling cross-sector and
cross-country benchmarking. By integrating rubric-guided scoring with
percentile-based normalization, our method identifies temporal trends,
strategic alignment patterns, and inconsistencies in disclosure across
industries and regions. Results reveal that sectors such as technology and
countries like Germany consistently demonstrate higher rubric alignment, while
others exhibit volatility or superficial engagement, offering insights that
inform key decision-making processes for investors, regulators, and corporate
environmental, social, and governance (ESG) strategists. The proposed LLM-based
approach transforms unstructured disclosures into quantifiable, interpretable,
comparable, and actionable intelligence, advancing the capabilities of
AI-enabled decision support systems (DSSs) in the domain of climate governance.

</details>


### [426] [Position Paper: If Innovation in AI Systematically Violates Fundamental Rights, Is It Innovation at All?](https://arxiv.org/abs/2511.00027)
*Josu Eguiluz Castañeira,Axel Brando,Migle Laukyte,Marc Serra-Vidal*

Main category: cs.CY

TL;DR: 该立场论文挑战了监管与创新对立的传统观念，论证了精心设计的监管是负责任创新的基础，并以欧盟AI法案为例展示了风险导向、责任驱动的监管如何促进而非阻碍技术进步。


<details>
  <summary>Details</summary>
Motivation: AI已渗透到关键基础设施和决策系统中，其失败会造成社会、经济和民主损害。论文旨在反驳监管与创新对立的错误观念，证明缺乏良好监管已造成不可估量的损害。

Method: 通过航空、制药和福利系统的类比，以及合成虚假信息、偏见和不可问责决策的案例研究，分析欧盟AI法案的风险导向监管框架及其适应性机制。

Result: 欧盟AI法案通过监管沙盒、中小企业支持、真实世界测试、基本权利影响评估等机制，展示了监管如何加速负责任的技术进步，提供法律确定性、消费者信任和道德竞争力。

Conclusion: 创新与监管应协同推进，欧盟框架通过将透明度、影响评估、问责制和AI素养嵌入设计和部署，定义了负责任的创新——以民主价值观和基本权利为约束的技术雄心。

Abstract: Artificial intelligence (AI) now permeates critical infrastructures and
decision-making systems where failures produce social, economic, and democratic
harm. This position paper challenges the entrenched belief that regulation and
innovation are opposites. As evidenced by analogies from aviation,
pharmaceuticals, and welfare systems and recent cases of synthetic
misinformation, bias and unaccountable decision-making, the absence of
well-designed regulation has already created immeasurable damage. Regulation,
when thoughtful and adaptive, is not a brake on innovation--it is its
foundation. The present position paper examines the EU AI Act as a model of
risk-based, responsibility-driven regulation that addresses the Collingridge
Dilemma: acting early enough to prevent harm, yet flexibly enough to sustain
innovation. Its adaptive mechanisms--regulatory sandboxes, small and medium
enterprises (SMEs) support, real-world testing, fundamental rights impact
assessment (FRIA) -- demonstrate how regulation can accelerate responsibly,
rather than delay, technological progress. The position paper summarises how
governance tools transform perceived burdens into tangible advantages: legal
certainty, consumer trust, and ethical competitiveness. Ultimately, the paper
reframes progress: innovation and regulation advance together. By embedding
transparency, impact assessments, accountability, and AI literacy into design
and deployment, the EU framework defines what responsible innovation truly
means--technological ambition disciplined by democratic values and fundamental
rights.

</details>


### [427] [Adoption of AI-Driven Fraud Detection System in the Nigerian Banking Sector: An Analysis of Cost, Compliance, and Competency](https://arxiv.org/abs/2511.00061)
*Stephen Alaba John,Joye Ahmed Shonubi,Patience Farida Azuikpe,Victor Oluwatosin Ologun*

Main category: cs.CY

TL;DR: 该研究调查了尼日利亚银行采用AI驱动欺诈检测系统的程度和决定因素，发现高层管理支持、IT基础设施、监管合规、员工能力和感知有效性促进采用，而高实施成本阻碍采用。


<details>
  <summary>Details</summary>
Motivation: AI欺诈检测系统在全球银行业应用广泛，但在尼日利亚的采用缓慢且不一致，主要由于高实施成本和技术专长缺乏，因此需要研究其采用程度和决定因素。

Method: 采用横断面调查研究设计，通过基于5点李克特量表的结构化问卷收集主要数据，使用目的性抽样选择5家最大银行，采用有序逻辑回归模型分析数据。

Result: 结果显示：高层管理支持、IT基础设施、监管合规、员工能力和感知有效性加速AI欺诈检测系统采用，而高实施成本阻碍采用。

Conclusion: 建议银行投资现代化可扩展IT系统，采用成本效益高的开源或云AI平台，并为员工提供AI和欺诈分析的持续专业发展。

Abstract: The inception of AI-based fraud detection systems has presented the banking
sector across the globe the opportunity to enhance fraud prevention mechanisms.
However, the extent of adoption in Nigeria has been slow, fragmented, and
inconsistent due to high cost of implementation and lack of technical
expertise. This study seeks to investigate extent of adoption and determinants
of AI-driven fraud detection systems in Nigerian banks. This study adopted a
cross-sectional survey research design. Data were extracted from primary
sources through structured questionnaire based on 5-point Likert scale. The
population of the study consist of 24 licensed banks in Nigeria. A purposive
sampling technique was used to select 5 biggest banks based on market
capitalization and customer base. The Ordered Logistic Regression (OLR) model
was used to estimate the data. The results showed that top management support,
IT infrastructure, regulatory compliance, staff competency and perceived
effectiveness accelerate the uptake of AI-driven fraud detection systems
adoption. However, high implementation cost discourages it. Therefore, the
study recommended that banks should invest in modern and scalable IT systems
that support the integration of AI tools; adopt open-source or cloud-based AI
platforms that are cost-effective; embrace continuous professional development
in AI, and fraud analytics for IT, fraud investigation, and risk management
staff.

</details>


### [428] [What is the Return on Investment of Digital Engineering for Complex Systems Development? Findings from a Mixed-Methods Study on the Post-production Design Change Process of Navy Assets](https://arxiv.org/abs/2511.00077)
*Jannatul Shefa,Taylan G. Topcu*

Main category: cs.CY

TL;DR: 该研究通过案例研究发现数字工程转型可将海军系统维护项目的平均工期减少50.1%，标准差减少41.5%，提高项目效率与可预测性。


<details>
  <summary>Details</summary>
Motivation: 复杂工程系统普遍存在进度超支和成本超支问题，数字工程被寄予厚望但缺乏量化证据支持其投资回报率。

Method: 采用混合方法研究海军系统工程团队的初步设计阶段，分析进度延误原因，并创建符合国防部政策的数字工程转型假设模型进行对比。

Result: 识别了四种典型的低效模式，量化了进度延误程度，数字工程转型可显著缩短项目周期并提高可预测性。

Conclusion: 研究首次为数字工程的投资回报率提供了量化证据，证明其在提高复杂系统维护项目效率和可预测性方面的价值。

Abstract: Complex engineered systems routinely face schedule and cost overruns, along
with poor post-deployment performance. Championed by both INCOSE and the U.S.
Department of Defense (DoD), the systems engineering (SE) community has
increasingly looked to Digital Engineering (DE) as a potential remedy. Despite
this growing advocacy, most of DE's purported benefits remain anecdotal, and
its return on investment (ROI) remains poorly understood. This research
presents findings from a case study on a Navy SE team responsible for the
preliminary design phase of post-production design change projects for Navy
assets. Using a mixed-methods approach, we document why complex system
sustainment projects are routinely late, where and to what extent schedule
slips arise, and how a DE transformation could improve schedule adherence. This
study makes three contributions. First, it identifies four archetypical
inefficiency modes that drive schedule overruns and explains how these
mechanisms unfold in their organizational context. Second, it quantifies the
magnitude and variation of schedule slips. Third, it creates a hypothetical
digitally transformed version of the current process, aligned with DoD DE
policy, and compares it to the current state to estimate potential schedule
gains. Our findings suggest that a DE transformation could reduce the median
project duration by 50.1% and reduce the standard deviation by 41.5%, leading
to faster and more predictable timelines. However, the observed gains are not
uniform across task categories. Overall, this study provides initial
quantitative evidence of DE's potential ROI and its value in improving the
efficiency and predictability of complex system sustainment projects.

</details>


### [429] [RailEstate: An Interactive System for Metro Linked Property Trends](https://arxiv.org/abs/2511.00078)
*Chen-Wei Chang,Yu-Chieh Cheng,Yun-En Tsai,Fanglan Chen,Chang-Tien Lu*

Main category: cs.CY

TL;DR: RailEstate是一个基于网络的系统，整合空间分析、自然语言界面和交互式预测，分析地铁站邻近度对华盛顿都市区住宅价格的影响。


<details>
  <summary>Details</summary>
Motivation: 地铁系统通过提升社区可达性和推动房产需求，对城市住房市场具有重要影响。现有静态地图工具或通用列表平台无法有效分析地铁邻近度与房价的长期关系。

Method: 结合25年历史住房数据和交通基础设施，支持低延迟地理空间查询、时间序列可视化和预测建模。关键创新是自然语言聊天机器人，将英文问题转换为可执行的SQL查询。

Result: 系统能够交互式探索ZIP码级别的价格模式，调查长期趋势，并预测任何地铁站周围的未来住房价值。

Conclusion: 这个统一交互平台使城市规划者、投资者和居民能够从地铁相关住房数据中获得可操作的见解，无需技术专业知识。

Abstract: Access to metro systems plays a critical role in shaping urban housing
markets by enhancing neighborhood accessibility and driving property demand. We
present RailEstate, a novel web based system that integrates spatial analytics,
natural language interfaces, and interactive forecasting to analyze how
proximity to metro stations influences residential property prices in the
Washington metropolitan area. Unlike static mapping tools or generic listing
platforms, RailEstate combines 25 years of historical housing data with transit
infrastructure to support low latency geospatial queries, time series
visualizations, and predictive modeling. Users can interactively explore ZIP
code level price patterns, investigate long term trends, and forecast future
housing values around any metro station. A key innovation is our natural
language chatbot, which translates plain-English questions e.g., What is the
highest price in Falls Church in the year 2000? into executable SQL over a
spatial database. This unified and interactive platform empowers urban
planners, investors, and residents to derive actionable insights from metro
linked housing data without requiring technical expertise.

</details>


### [430] [Forecasting Occupational Survivability of Rickshaw Pullers in a Changing Climate with Wearable Data](https://arxiv.org/abs/2511.00081)
*Masfiqur Rahaman,Maoyejatun Hasana,Shahad Shahriar Rahman,MD Sajid Mostafiz Noor,Razin Reaz Abedin,Md Toki Tahmid,Duncan Watson Parris,Tanzeem Choudhury,A. B. M. Alim Al Islam,Tauhidur Rahman*

Main category: cs.CY

TL;DR: 本研究使用可穿戴传感器收集了孟加拉国达卡100名三轮车夫在极端高温下的生理数据，开发了线性高斯贝叶斯网络模型预测生理指标，并结合CMIP6气候模型预测未来热暴露风险。


<details>
  <summary>Details</summary>
Motivation: 三轮车夫在极端高温下工作具有高度脆弱性，但对其生理生物标志物如何响应此类条件知之甚少。

Method: 使用可穿戴传感器收集实时天气和生理数据，开发线性高斯贝叶斯网络回归模型预测生理指标，结合CMIP6气候模型进行未来预测，并对12名车夫进行访谈。

Result: 模型预测皮肤温度、相对心脏成本、皮肤电导反应和皮肤电导水平的标准化平均绝对误差分别为0.82、0.47、0.65和0.67。目前32%的车夫面临高热暴露风险，到2026-2030年可能升至37%，平均暴露时间近12分钟。

Conclusion: 三轮车夫已认识到自身气候脆弱性增加，并对其健康和职业生存能力表示担忧。未来热暴露风险将进一步加剧，需要采取干预措施保护这一脆弱群体。

Abstract: Cycle rickshaw pullers are highly vulnerable to extreme heat, yet little is
known about how their physiological biomarkers respond under such conditions.
This study collected real-time weather and physiological data using wearable
sensors from 100 rickshaw pullers in Dhaka, Bangladesh. In addition, interviews
with 12 pullers explored their knowledge, perceptions, and experiences related
to climate change. We developed a Linear Gaussian Bayesian Network (LGBN)
regression model to predict key physiological biomarkers based on activity,
weather, and demographic features. The model achieved normalized mean absolute
error values of 0.82, 0.47, 0.65, and 0.67 for skin temperature, relative
cardiac cost, skin conductance response, and skin conductance level,
respectively. Using projections from 18 CMIP6 climate models, we layered the
LGBN on future climate forecasts to analyze survivability for current
(2023-2025) and future years (2026-2100). Based on thresholds of WBGT above
31.1{\deg}C and skin temperature above 35{\deg}C, 32% of rickshaw pullers
already face high heat exposure risk. By 2026-2030, this percentage may rise to
37% with average exposure lasting nearly 12 minutes, or about two-thirds of the
trip duration. A thematic analysis of interviews complements these findings,
showing that rickshaw pullers recognize their increasing climate vulnerability
and express concern about its effects on health and occupational survivability.

</details>


### [431] [Artificial Intelligence in Elementary STEM Education: A Systematic Review of Current Applications and Future Challenges](https://arxiv.org/abs/2511.00105)
*Majid Memari,Krista Ruggles*

Main category: cs.CY

TL;DR: 这篇系统综述分析了2020-2025年间258项关于AI在小学STEM教育中应用的研究，发现主要关注智能辅导系统(45%)和数学领域(38%)，但存在生态系统碎片化、发展不适宜性、基础设施障碍等八大限制因素。


<details>
  <summary>Details</summary>
Motivation: AI正在改变小学STEM教育，但证据仍然零散，需要系统性地综合现有研究，识别关键差距和未来方向。

Method: 对2020-2025年间258项研究进行系统综述，分析AI在八个应用类别中的分布：智能辅导系统、学习分析、自动评估、计算机视觉、教育机器人、多模态感知、AI增强XR和自适应内容生成。

Result: 大多数研究关注高年级小学(65%)和数学(38%)，跨学科STEM整合有限(15%)。对话AI显示出中等效果(d=0.45-0.70)，但只有34%的研究包含标准化效应量。地理分布不均，90%研究来自北美、东亚和欧洲。

Conclusion: 未来需要支持真实STEM整合的可互操作架构、适合年级的设计、保护隐私的分析以及以教师为中心的实施，增强而非取代人类专业知识。

Abstract: Artificial intelligence (AI) is transforming elementary STEM education, yet
evidence remains fragmented. This systematic review synthesizes 258 studies
(2020-2025) examining AI applications across eight categories: intelligent
tutoring systems (45% of studies), learning analytics (18%), automated
assessment (12%), computer vision (8%), educational robotics (7%), multimodal
sensing (6%), AI-enhanced extended reality (XR) (4%), and adaptive content
generation. The analysis shows that most studies focus on upper elementary
grades (65%) and mathematics (38%), with limited cross-disciplinary STEM
integration (15%). While conversational AI demonstrates moderate effectiveness
(d = 0.45-0.70 where reported), only 34% of studies include standardized effect
sizes. Eight major gaps limit real-world impact: fragmented ecosystems,
developmental inappropriateness, infrastructure barriers, lack of privacy
frameworks, weak STEM integration, equity disparities, teacher marginalization,
and narrow assessment scopes. Geographic distribution is also uneven, with 90%
of studies originating from North America, East Asia, and Europe. Future
directions call for interoperable architectures that support authentic STEM
integration, grade-appropriate design, privacy-preserving analytics, and
teacher-centered implementations that enhance rather than replace human
expertise.

</details>


### [432] [Wayfinding through the AI wilderness: Mapping rhetorics of ChatGPT prompt writing on X (formerly Twitter) to promote critical AI literacies](https://arxiv.org/abs/2511.00106)
*Anuj Gupta,Ann Shivers-McNair*

Main category: cs.CY

TL;DR: 该研究通过分析社交媒体上ChatGPT提示写作的修辞实践，探讨如何促进批判性AI素养。研究者收集了32,000条推文，识别出五个关键主题，为数字写作教学和研究提供见解。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI工具的普及，社交媒体上关于提示写作的讨论激增。研究者旨在通过分析这些修辞实践来促进批判性AI素养，帮助教育者和研究者更好地理解和教授AI写作技能。

Method: 基于数字写作研究的四个传统框架，采用迭代研究方法，收集并分析了2022年11月至2023年5月期间32,000条关于提示写作的推文，结合计算方法和定性方法进行主题分析。

Result: 识别出五个关键主题：提示写作影响的沟通领域、共享的微观素养资源、塑造提示写作的市场修辞、提示的修辞特征，以及提示写作的定义。这些发现揭示了新兴的AI素养实践模式。

Conclusion: 研究强调了分析社交媒体上提示写作修辞实践对于发展批判性AI素养的重要性，为数字写作教师和研究者提供了实用的教学和分析框架，有助于在AI时代培养更具反思性的写作能力。

Abstract: In this paper, we demonstrate how studying the rhetorics of ChatGPT prompt
writing on social media can promote critical AI literacies. Prompt writing is
the process of writing instructions for generative AI tools like ChatGPT to
elicit desired outputs and there has been an upsurge of conversations about it
on social media. To study this rhetorical activity, we build on four
overlapping traditions of digital writing research in computers and composition
that inform how we frame literacies, how we study social media rhetorics, how
we engage iteratively and reflexively with methodologies and technologies, and
how we blend computational methods with qualitative methods. Drawing on these
four traditions, our paper shows our iterative research process through which
we gathered and analyzed a dataset of 32,000 posts (formerly known as tweets)
from X (formerly Twitter) about prompt writing posted between November 2022 to
May 2023. We present five themes about these emerging AI literacy practices:
(1) areas of communication impacted by prompt writing, (2) micro-literacy
resources shared for prompt writing, (3) market rhetoric shaping prompt
writing, (4) rhetorical characteristics of prompts, and (5) definitions of
prompt writing. In discussing these themes and our methodologies, we highlight
takeaways for digital writing teachers and researchers who are teaching and
analyzing critical AI literacies.

</details>


### [433] [Evaluation of compliance with democratic and technical standards of i-voting in elections to academic senates in Czech higher education](https://arxiv.org/abs/2511.01598)
*Tomas Martinek,Michal Maly*

Main category: cs.CY

TL;DR: 本文评估了捷克公立大学在学术评议会选举中使用的i-voting系统，发现这些系统普遍缺乏透明度，存在民主和技术挑战。


<details>
  <summary>Details</summary>
Motivation: 远程工作和数字通信的增加推动了i-voting系统的广泛采用，包括在学术机构中。本文旨在评估这些系统在民主和技术方面的挑战。

Method: 通过对系统开发人员和管理员的访谈，以及对潜在选民的调查进行研究。

Result: 18所捷克公立大学中有26所实施了远程电子投票，但这些系统缺乏透明度，对选举安全、选民隐私和过程完整性构成担忧。

Conclusion: 缺乏透明度使得无法全面评估技术标准和i-voting系统的整体合法性，可能削弱选举结果的可信度。

Abstract: The shift towards increased remote work and digital communication, driven by
recent global developments, has led to the widespread adoption of i-voting
systems, including in academic institutions. This paper critically evaluates
the use of i-voting platforms for elections to academic senates at Czech public
universities, focusing on the democratic and technical challenges they present.
A total of 18 out of 26 Czech public universities have implemented remote
electronic voting for these elections. Yet, the systems often lack the
necessary transparency, raising significant concerns regarding their adherence
to democratic norms, such as election security, voter privacy, and the
integrity of the process. Through interviews with system developers and
administrators, along with a survey of potential voters, the study underscores
the critical need for transparency. Without it, a comprehensive assessment of
the technical standards and the overall legitimacy of the i-voting systems
remains unattainable, potentially undermining the credibility of the electoral
outcomes.

</details>


### [434] [Breyer case of the Court of Justice of the European Union: IP addresses and the personal data definition](https://arxiv.org/abs/2511.01751)
*Frederik Zuiderveen Borgesius*

Main category: cs.CY

TL;DR: 欧盟法院Breyer案裁定：当网站发布者能够通过法律手段从访问者的互联网接入提供商处获取额外信息来识别访问者时，动态IP地址构成个人数据。


<details>
  <summary>Details</summary>
Motivation: 解决网站访问者的动态IP地址是否构成网站发布者的个人数据这一关键问题，特别是当互联网接入提供商能够将IP地址与姓名关联时。

Method: 通过分析欧盟法院的判决事实和法律推理，探讨动态IP地址在数据保护法下的性质。

Result: 法院认定，如果网站发布者具备法律手段从互联网接入提供商处获取识别访问者所需的额外信息，那么动态IP地址就构成个人数据。

Conclusion: 该判决确立了动态IP地址在特定条件下可被视为个人数据的重要先例，对数据保护实践具有重要指导意义。

Abstract: The Breyer case of the Court of Justice of the European Union (CJEU)
primarily concerns the question whether a website visitor's dynamic IP address
constitutes personal data for a website publisher, when another party (an
internet access provider) can tie a name to that IP address. In essence, the
Court finds that an IP address constitutes personal data for the website
publisher, if that publisher has the legal means to obtain, from the visitor's
internet access provider, additional information that enables the publisher to
identify that visitor. In this case note, I summarise the facts and the
judgment, and add a few comments.

</details>


### [435] [An assessment of the Commission's Proposal on Privacy and Electronic Communications](https://arxiv.org/abs/2511.01752)
*Frederik Zuiderveen Borgesius,Joris van Hoboken,Ronan Fahy,Kristina Irion,Max Rozendaal*

Main category: cs.CY

TL;DR: 欧洲议会委托的研究评估了欧盟委员会提出的ePrivacy法规提案，分析其是否能充分保护个人数据、隐私和通信权利。


<details>
  <summary>Details</summary>
Motivation: 评估欧盟ePrivacy法规提案是否能为个人数据保护权、隐私权和通信权提供高标准保护，同时识别提案的潜在优缺点。

Method: 通过委托研究的方式，对欧盟委员会的ePrivacy法规提案进行系统性评估和分析。

Result: 研究对提案进行了全面评估，识别了其在保护个人数据、隐私和通信权利方面的潜在效果，以及更广泛的利弊影响。

Conclusion: 该研究为欧盟立法者提供了关于ePrivacy法规提案的详细评估，帮助决策者了解提案对基本权利保护的影响。

Abstract: This study, commissioned by the European Parliament's Policy Department for
Citizens Rights and Constitutional Affairs at the request of the LIBE
Committee, appraises the European Commission's proposal for an ePrivacy
Regulation. The study assesses whether the proposal would ensure that the right
to the protection of personal data, the right to respect for private life and
communications, and related rights enjoy a high standard of protection. The
study also highlights the proposal's potential benefits and drawbacks more
generally.

</details>


### [436] [A Detailed Study on LLM Biases Concerning Corporate Social Responsibility and Green Supply Chains](https://arxiv.org/abs/2511.01840)
*Greta Ontrup,Annika Bush,Markus Pauly,Meltem Aksoy*

Main category: cs.CY

TL;DR: 研究发现大型语言模型在可持续商业策略方面存在系统性偏见，组织文化提示会显著改变模型响应，这对LLM辅助的可持续性决策具有重要影响。


<details>
  <summary>Details</summary>
Motivation: 识别LLM在可持续商业和供应链实践重要性方面的训练数据偏见，因为LLM已被证明在优先考虑可持续商业策略方面会重现偏见。

Method: 使用经过验证的关于企业伦理责任和可持续实践重要性的调查问卷，系统分析不同LLM的响应差异，并评估四种组织文化类型是否加剧这些差异。

Result: 发现模型之间存在显著系统性差异，组织文化提示会大幅改变LLM响应。

Conclusion: 研究对LLM在可持续性背景下的辅助决策具有重要启示，需要关注模型偏见对可持续供应链管理的影响。

Abstract: Organizations increasingly use Large Language Models (LLMs) to improve supply
chain processes and reduce environmental impacts. However, LLMs have been shown
to reproduce biases regarding the prioritization of sustainable business
strategies. Thus, it is important to identify underlying training data biases
that LLMs pertain regarding the importance and role of sustainable business and
supply chain practices. This study investigates how different LLMs respond to
validated surveys about the role of ethics and responsibility for businesses,
and the importance of sustainable practices and relations with suppliers and
customers. Using standardized questionnaires, we systematically analyze
responses generated by state-of-the-art LLMs to identify variations. We further
evaluate whether differences are augmented by four organizational culture
types, thereby evaluating the practical relevance of identified biases. The
findings reveal significant systematic differences between models and
demonstrate that organizational culture prompts substantially modify LLM
responses. The study holds important implications for LLM-assisted
decision-making in sustainability contexts.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [437] [Deep reinforcement learning for optimal trading with partial information](https://arxiv.org/abs/2511.00190)
*Andrea Macrì,Sebastian Jaimungal,Fabrizio Lillo*

Main category: q-fin.TR

TL;DR: 本文研究了基于强化学习的交易策略，使用DDPG算法结合GRU网络来处理具有潜参数和状态转换的Ornstein-Uhlenbeck过程交易信号。


<details>
  <summary>Details</summary>
Motivation: 现有研究很少使用强化学习来开发利用市场潜信息的交易策略，特别是在交易信号具有状态转换动态的情况下。

Method: 提出了三种基于DDPG的算法：hid-DDPG（一步法）、prob-DDPG（两步法使用后验状态概率）和reg-DDPG（两步法使用信号预测），均集成GRU网络捕获时间依赖性。

Result: prob-DDPG在累积奖励和策略可解释性方面表现最佳，reg-DDPG效果有限，hid-DDPG性能中等但可解释性较差。

Conclusion: 向智能体提供的信息质量和结构至关重要，将概率性潜状态信息嵌入到强化学习交易策略中能显著提高盈利能力和鲁棒性。

Abstract: Reinforcement Learning (RL) applied to financial problems has been the
subject of a lively area of research. The use of RL for optimal trading
strategies that exploit latent information in the market is, to the best of our
knowledge, not widely tackled. In this paper we study an optimal trading
problem, where a trading signal follows an Ornstein-Uhlenbeck process with
regime-switching dynamics. We employ a blend of RL and Recurrent Neural
Networks (RNN) in order to make the most at extracting underlying information
from the trading signal with latent parameters.
  The latent parameters driving mean reversion, speed, and volatility are
filtered from observations of the signal, and trading strategies are derived
via RL. To address this problem, we propose three Deep Deterministic Policy
Gradient (DDPG)-based algorithms that integrate Gated Recurrent Unit (GRU)
networks to capture temporal dependencies in the signal. The first, a one -step
approach (hid-DDPG), directly encodes hidden states from the GRU into the RL
trader. The second and third are two-step methods: one (prob-DDPG) makes use of
posterior regime probability estimates, while the other (reg-DDPG) relies on
forecasts of the next signal value. Through extensive simulations with
increasingly complex Markovian regime dynamics for the trading signal's
parameters, as well as an empirical application to equity pair trading, we find
that prob-DDPG achieves superior cumulative rewards and exhibits more
interpretable strategies. By contrast, reg-DDPG provides limited benefits,
while hid-DDPG offers intermediate performance with less interpretable
strategies. Our results show that the quality and structure of the information
supplied to the agent are crucial: embedding probabilistic insights into latent
regimes substantially improves both profitability and robustness of
reinforcement learning-based trading strategies.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [438] [Lambda Value-at-Risk under ambiguity and risk sharing](https://arxiv.org/abs/2511.00717)
*Peng Liu,Alexander Schied*

Main category: q-fin.RM

TL;DR: 本文研究了在模糊性下的Lambda VaR风险度量，建立了在概率测度族模糊集下的稳健ΛVaR与基于容量的ΛVaR的等价关系，并探讨了在多智能体风险分担中的应用。


<details>
  <summary>Details</summary>
Motivation: 传统ΛVaR和Choquet分位数在模糊性下的统一和扩展，以及风险度量在多智能体风险分担中的应用需求。

Method: 通过容量理论扩展ΛVaR，建立模糊集下稳健ΛVaR与容量ΛVaR的等价关系，并利用下集族进行表示。

Result: 推导了在φ-散度和似然比约束模糊集下的显式公式，证明了风险度量在下集族诱导下的闭包性质，并得到了最优分配和共单调风险分担的显式结果。

Conclusion: 该框架统一并扩展了传统ΛVaR和Choquet分位数，为模糊性下的风险度量和风险分担问题提供了新的理论工具和显式解决方案。

Abstract: In this paper, we investigate the Lambda Value-at-Risk ($\Lambda$VaR) under
ambiguity, where the ambiguity is represented by a family of probability
measures. We establish that for increasing Lambda functions, the robust (i.e.,
worst-case) $\Lambda$VaR under such an ambiguity set is equivalent to
$\Lambda$VaR computed with respect to a capacity, a novel extension in the
literature. This framework unifies and extends both traditional $\Lambda$VaR
and Choquet quantiles (Value-at-Risk under ambiguity). We analyze the
fundamental properties of this extended risk measure and establish a novel
equivalent representation for $\Lambda$VaR under capacities with monotone
Lambda functions in terms of families of downsets. Moreover, explicit formulas
are derived for robust $\Lambda$VaR when ambiguity sets are characterized by
$\phi$-divergence and the likelihood ratio constraints, respectively.
  We further explore the applications in risk sharing among multiple agents. We
demonstrate that the family of risk measures induced by families of downsets is
closed under inf-convolution. In particular, we prove that the inf-convolution
of $\Lambda$VaR with capacities and monotone Lambda functions is
another$\Lambda$VaR under a capacity. The explicit forms of optimal allocations
are also derived. Moreover, we obtain more explicit results for risk sharing
under ambiguity sets characterized by $\phi$-divergence and likelihood ratio
constraints. Finally, we explore comonotonic risk-sharing for $\Lambda$VaR
under ambiguity.

</details>


### [439] [Cost-of-capital valuation with risky assets](https://arxiv.org/abs/2511.00895)
*Hansjörg Albrecher,Filip Lindskog,Hervé Zumbach*

Main category: q-fin.RM

TL;DR: 分析在资本成本估值中，当缓冲资本投资于风险资产而非无风险债券时的效应，特别关注风险投资程度对资本分解的影响和有限责任在重尾保险风险中的作用。


<details>
  <summary>Details</summary>
Motivation: 标准资本成本估值假设缓冲资本投资于无风险一年期债券，但实际中保险公司可能投资于风险资产。本文旨在分析缓冲资本投资于风险资产（如股票和债券组合）时的影响。

Method: 结合一般理论结果、特定随机模型的显式结果以及数值结果，分析缓冲资本投资风险资产对资本分解的影响。

Result: 研究发现随着投资风险程度的增加，缓冲资本在投保人和投资者之间的分解方式会发生变化，并强调了有限责任在重尾保险风险情况下的作用。

Conclusion: 投资缓冲资本于风险资产会改变资本成本估值中的资本分解结构，有限责任条款在重尾风险情况下具有重要影响，这对保险监管框架具有实际意义。

Abstract: Cost-of-capital valuation is a well-established approach to the valuation of
liabilities and is one of the cornerstones of current regulatory frameworks for
the insurance industry. Standard cost-of-capital considerations typically rely
on the assumption that the required buffer capital is held in risk-less
one-year bonds. The aim of this work is to analyze the effects of allowing
investments of the buffer capital in risky assets, e.g.~in a combination of
stocks and bonds. In particular, we make precise how the decomposition of the
buffer capital into contributions from policyholders and investors varies as
the degree of riskiness of the investment increases, and highlight the role of
limited liability in the case of heavy-tailed insurance risks. We present a
combination of general theoretical results, explicit results for certain
stochastic models and numerical results that emphasize the key findings.

</details>


<div id='q-fin.MF'></div>

# q-fin.MF [[Back]](#toc)

### [440] [Robust Hedging of path-dependent options using a min-max algorithm](https://arxiv.org/abs/2511.00781)
*Purba Banerjee,Srikanth Iyer,Shashi Jain*

Main category: q-fin.MF

TL;DR: 提出一种模型无关的静态对冲方法，使用现金、标的资产和到期日为t1的普通期权来对冲路径依赖期权，通过求解min-max优化问题最小化最坏情况下的对冲误差。


<details>
  <summary>Details</summary>
Motivation: 投资者需要对冲路径依赖期权，但希望使用静态对冲策略，避免动态调整带来的复杂性和成本。

Method: 基于对偶鞅最优输运理论，构建min-max优化问题，确定在t1时刻最小化最坏情况对冲误差的静态投资组合构成。

Result: 提供了数值求解方案，并在Black-Scholes和Merton跳跃扩散模型下展示了良好的对冲性能，同时给出了目标期权到期日T时的对冲误差理论界限。

Conclusion: 该方法为路径依赖期权的静态对冲提供了有效的模型无关解决方案，具有实际应用价值。

Abstract: We consider an investor who wants to hedge a path-dependent option with
maturity $T$ using a static hedging portfolio using cash, the underlying, and
vanilla put/call options on the same underlying with maturity $ t_1$, where $0
< t_1 < T$. We propose a model-free approach to construct such a portfolio. The
framework is inspired by the \textit{primal-dual} Martingale Optimal Transport
(MOT) problem, which was pioneered by \cite{beiglbock2013model}. The
optimization problem is to determine the portfolio composition that minimizes
the expected worst-case hedging error at $t_1$ (that coincides with the
maturity of the options that are used in the hedging portfolio). The worst-case
scenario corresponds to the distribution that yields the worst possible hedging
performance. This formulation leads to a \textit{min-max} problem. We provide a
numerical scheme for solving this problem when a finite number of vanilla
option prices are available. Numerical results on the hedging performance of
this model-free approach when the option prices are generated using a
\textit{Black-Scholes} and a \textit{Merton Jump diffusion} model are
presented. We also provide theoretical bounds on the hedging error at $T$, the
maturity of the target option.

</details>


### [441] [Differential Beliefs in Financial Markets Under Information Constraints: A Modeling Perspective](https://arxiv.org/abs/2511.01486)
*Karen Grigorian,Robert Jarrow*

Main category: q-fin.MF

TL;DR: 该论文应用McKean-Vlasov型随机微分方程理论研究部分信息和部分可观测金融市场中的市场效率问题，包括信息流增加时价格收敛、偏差缩减机制以及专家意见最优聚合。


<details>
  <summary>Details</summary>
Motivation: 研究在部分信息和部分可观测金融市场背景下，如何通过增加信息流来提高市场效率，并探索交易者如何最优地整合专家意见以获得超额收益。

Method: 使用条件McKean-Vlasov型随机微分方程、Wasserstein重心、KL散度以及随机控制和非线性滤波工具来分析问题。

Result: 理论结果通过具体模拟展示，表明所提出的模型可以实际应用于建模信息约束下的金融市场和具有不同信念的交易者的套利行为。

Conclusion: McKean-Vlasov型SDEs和相关工具为分析部分信息金融市场中的效率问题和交易者行为提供了有效的理论框架和实用模型。

Abstract: We apply the theory of McKean-Vlasov-type SDEs to study several problems
related to market efficiency in the context of partial information and
partially observable financial markets: (i) convergence of reduced-information
market price processes to the true price process under an increasing
information flow; (ii) a specific mechanism of shrinking biases under
increasing information flows; (iii) optimal aggregation of expert opinions by a
trader seeking a positive alpha. All these problems are studied by means of
(conditional) McKean-Vlasov-type SDEs, Wasserstein barycenters, KL divergence
and relevant tools of stochastic control and nonlinear filtering. We supply the
theoretical results in (i)-(iii) with concrete simulations demonstrating how
the proposed models can be applied in practice to model financial markets under
information constraints and the arbitrage-seeking behavior of traders with
differential beliefs.

</details>
