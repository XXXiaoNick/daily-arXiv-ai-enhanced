<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 55]
- [eess.SY](#eess.SY) [Total: 8]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [q-fin.RM](#q-fin.RM) [Total: 1]
- [stat.ML](#stat.ML) [Total: 10]
- [q-fin.MF](#q-fin.MF) [Total: 1]
- [cs.LG](#cs.LG) [Total: 121]
- [econ.EM](#econ.EM) [Total: 1]
- [math.OC](#math.OC) [Total: 15]
- [cs.AI](#cs.AI) [Total: 28]
- [cs.CY](#cs.CY) [Total: 4]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Recontextualizing Famous Quotes for Brand Slogan Generation](https://arxiv.org/abs/2602.06049)
*Ziao Yang,Zizhang Chen,Lei Zhang,Hongfu Liu*

Main category: cs.CL

TL;DR: 该论文提出了一种基于名人名言重构的广告标语生成新范式，通过模块化框架将标语生成分解为可解释的子任务，相比现有LLM基线在多样性、新颖性和情感影响力方面取得边际改进。


<details>
  <summary>Details</summary>
Motivation: 广告标语在品牌传播中至关重要，但广告疲劳降低了重复标语的效果。现有LLM方法生成的标语存在风格冗余、缺乏品牌个性、过于机器化的问题。需要平衡新颖性与熟悉性的有效标语生成方法。

Method: 提出基于名人名言重构的新范式，利用名言与标语在长度、修辞手法和深度上的相似性。设计模块化框架，将标语生成分解为四个可解释子任务：名言匹配、结构分解、词汇替换和混搭生成。

Result: 通过自动评估和人工评估，相比三种最先进的LLM基线方法，在多样性、新颖性、情感影响力和人类偏好方面取得了边际改进。

Conclusion: 名人名言重构为广告标语生成提供了有效的新范式，模块化框架提高了生成过程的可解释性，在平衡新颖性与熟悉性方面优于现有LLM方法。

Abstract: Slogans are concise and memorable catchphrases that play a crucial role in advertising by conveying brand identity and shaping public perception. However, advertising fatigue reduces the effectiveness of repeated slogans, creating a growing demand for novel, creative, and insightful slogan generation. While recent work leverages large language models (LLMs) for this task, existing approaches often produce stylistically redundant outputs that lack a clear brand persona and appear overtly machine-generated. We argue that effective slogans should balance novelty with familiarity and propose a new paradigm that recontextualizes persona-related famous quotes for slogan generation. Well-known quotes naturally align with slogan-length text, employ rich rhetorical devices, and offer depth and insight, making them a powerful resource for creative generation. Technically, we introduce a modular framework that decomposes slogan generation into interpretable subtasks, including quote matching, structural decomposition, vocabulary replacement, and remix generation. Extensive automatic and human evaluations demonstrate marginal improvements in diversity, novelty, emotional impact, and human preference over three state-of-the-art LLM baselines.

</details>


### [2] [Relevance-aware Multi-context Contrastive Decoding for Retrieval-augmented Visual Question Answering](https://arxiv.org/abs/2602.06050)
*Jongha Kim,Byungoh Ko,Jeehye Na,Jinsung Yoon,Hyunwoo J. Kim*

Main category: cs.CL

TL;DR: 提出RMCD解码方法，通过基于相关性的多上下文对比解码，有效聚合多个相关上下文信息并抑制无关上下文负面影响，提升大视觉语言模型的知识密集型视觉问答性能。


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型缺乏特定实体细节知识，检索增强生成是常用解决方案，但现有解码方法未能充分利用多个相关上下文，也无法有效抑制无关上下文的负面影响。

Method: 提出相关性感知的多上下文对比解码方法，基于每个上下文与问题的相关性对预测输出进行加权组合，从而聚合多个相关上下文的有用信息并抵消无关上下文的负面影响。

Result: 在多个大视觉语言模型和三个知识密集型视觉问答基准测试中，RMCD始终优于其他解码方法，且无需额外训练，对检索结果具有鲁棒性。

Conclusion: RMCD是一种简单有效的解码方法，能够显著提升检索增强生成在大视觉语言模型中的性能，通过加权聚合多个上下文信息来解决现有方法的局限性。

Abstract: Despite the remarkable capabilities of Large Vision Language Models (LVLMs), they still lack detailed knowledge about specific entities. Retrieval-augmented Generation (RAG) is a widely adopted solution that enhances LVLMs by providing additional contexts from an external Knowledge Base. However, we observe that previous decoding methods for RAG are sub-optimal as they fail to sufficiently leverage multiple relevant contexts and suppress the negative effects of irrelevant contexts. To this end, we propose Relevance-aware Multi-context Contrastive Decoding (RMCD), a novel decoding method for RAG. RMCD outputs a final prediction by combining outputs predicted with each context, where each output is weighted based on its relevance to the question. By doing so, RMCD effectively aggregates useful information from multiple relevant contexts while also counteracting the negative effects of irrelevant ones. Experiments show that RMCD consistently outperforms other decoding methods across multiple LVLMs, achieving the best performance on three knowledge-intensive visual question-answering benchmarks. Also, RMCD can be simply applied by replacing the decoding method of LVLMs without additional training. Analyses also show that RMCD is robust to the retrieval results, consistently performing the best across the weakest to the strongest retrieval results. Code is available at https://github.com/mlvlab/RMCD.

</details>


### [3] [CAST: Character-and-Scene Episodic Memory for Agents](https://arxiv.org/abs/2602.06051)
*Kexin Ma,Bojun Li,Yuhua Tang,Ruochun Jin,Liting Sun*

Main category: cs.CL

TL;DR: 提出基于戏剧理论的角色与场景记忆架构(CAST)，通过构建3D场景(时间/地点/主题)和组织角色档案来表示情景记忆，结合图语义记忆形成双记忆设计，显著提升对话问答性能。


<details>
  <summary>Details</summary>
Motivation: 现有智能体记忆系统主要关注语义回忆，将经验表示为键值对、向量或图等结构，难以表示和检索连贯的事件。人类的情景记忆能够回忆基于谁、何时、何地的连贯事件，需要更好的记忆架构来支持这种能力。

Method: 提出CAST记忆架构：1) 构建3D场景(时间/地点/主题)来表示情景记忆；2) 将场景组织成角色档案，总结角色的相关事件；3) 结合基于图的语义记忆，形成双记忆设计。

Result: 在多个数据集上，CAST平均比基线方法提高了8.11%的F1分数和10.21%的J(LLM-as-a-Judge)评分，特别是在开放性和时间敏感的对话问题上表现更优。

Conclusion: CAST通过戏剧理论启发的角色与场景架构有效表示情景记忆，结合语义记忆形成鲁棒的双记忆系统，显著提升了智能体在对话任务中的记忆和推理能力。

Abstract: Episodic memory is a central component of human memory, which refers to the ability to recall coherent events grounded in who, when, and where. However, most agent memory systems only emphasize semantic recall and treat experience as structures such as key-value, vector, or graph, which makes them struggle to represent and retrieve coherent events. To address this challenge, we propose a Character-and-Scene based memory architecture(CAST) inspired by dramatic theory. Specifically, CAST constructs 3D scenes (time/place/topic) and organizes them into character profiles that summarize the events of a character to represent episodic memory. Moreover, CAST complements this episodic memory with a graph-based semantic memory, which yields a robust dual memory design. Experiments demonstrate that CAST has averagely improved 8.11% F1 and 10.21% J(LLM-as-a-Judge) than baselines on various datasets, especially on open and time-sensitive conversational questions.

</details>


### [4] [Rethinking Memory Mechanisms of Foundation Agents in the Second Half](https://arxiv.org/abs/2602.06052)
*Wei-Chieh Huang,Weizhi Zhang,Yueqing Liang,Yuanchen Bei,Yankai Chen,Tao Feng,Xinyu Pan,Zhen Tan,Yu Wang,Tianxin Wei,Shanglin Wu,Ruiyao Xu,Liangwei Yang,Rui Yang,Wooseong Yang,Chin-Yuan Yeh,Hanrong Zhang,Haozhen Zhang,Siqi Zhu,Henry Peng Zou,Wanjia Zhao,Song Wang,Wujiang Xu,Zixuan Ke,Zheng Hui,Dawei Li,Yaozu Wu,Langzhou He,Chen Wang,Xiongxiao Xu,Baixiang Huang,Juntao Tan,Shelby Heinecke,Huan Wang,Caiming Xiong,Ahmed A. Metwally,Jun Yan,Chen-Yu Lee,Hanqing Zeng,Yinglong Xia,Xiaokai Wei,Ali Payani,Yu Wang,Haitong Ma,Wenya Wang,Chengguang Wang,Yu Zhang,Xin Wang,Yongfeng Zhang,Jiaxuan You,Hanghang Tong,Xiao Luo,Yizhou Sun,Wei Wang,Julian McAuley,James Zou,Jiawei Han,Philip S. Yu,Kai Shu*

Main category: cs.CL

TL;DR: 该论文是一篇关于智能体记忆系统的综述，提出了一个统一的三维框架（记忆基质、认知机制、记忆主体），分析了不同智能体拓扑中的记忆实现，并讨论了评估方法和未来挑战。


<details>
  <summary>Details</summary>
Motivation: 人工智能研究正从追求基准分数转向强调问题定义和真实世界评估。在"下半场"中，核心挑战是智能体在长期、动态、用户依赖环境中的实际效用，这需要处理上下文爆炸并持续管理大量信息。记忆系统因此成为填补效用差距的关键解决方案。

Method: 提出了一个统一的三维框架来分析基础智能体记忆：1) 记忆基质（内部和外部），2) 认知机制（情景、语义、感官、工作、程序记忆），3) 记忆主体（智能体中心与用户中心）。分析了不同智能体拓扑中的记忆实例化和操作，强调了记忆操作的学习策略。

Result: 构建了一个全面的智能体记忆系统分析框架，识别了记忆在解决长期交互中上下文爆炸问题的关键作用。提供了记忆系统在不同智能体架构中的实现方式分析，并总结了记忆操作的学习方法。

Conclusion: 记忆系统是智能体在长期、动态环境中实现实际效用的关键。论文提出了统一的分析框架，指出了当前评估基准和指标的不足，并概述了未来研究方向，包括更好的评估方法、记忆操作优化和跨模态记忆整合等挑战。

Abstract: The research of artificial intelligence is undergoing a paradigm shift from prioritizing model innovations over benchmark scores towards emphasizing problem definition and rigorous real-world evaluation. As the field enters the "second half," the central challenge becomes real utility in long-horizon, dynamic, and user-dependent environments, where agents face context explosion and must continuously accumulate, manage, and selectively reuse large volumes of information across extended interactions. Memory, with hundreds of papers released this year, therefore emerges as the critical solution to fill the utility gap. In this survey, we provide a unified view of foundation agent memory along three dimensions: memory substrate (internal and external), cognitive mechanism (episodic, semantic, sensory, working, and procedural), and memory subject (agent- and user-centric). We then analyze how memory is instantiated and operated under different agent topologies and highlight learning policies over memory operations. Finally, we review evaluation benchmarks and metrics for assessing memory utility, and outline various open challenges and future directions.

</details>


### [5] [PersonaPlex: Voice and Role Control for Full Duplex Conversational Speech Models](https://arxiv.org/abs/2602.06053)
*Rajarshi Roy,Jonathan Raiman,Sang-gil Lee,Teodor-Dumitru Ene,Robert Kirby,Sungwon Kim,Jaehyeon Kim,Bryan Catanzaro*

Main category: cs.CL

TL;DR: PersonaPlex：一种结合角色条件与语音克隆的双工对话语音模型，通过混合系统提示支持结构化角色驱动应用和个性化交互


<details>
  <summary>Details</summary>
Motivation: 现有双工语音模型局限于固定角色和声音，无法支持结构化角色驱动的实际应用和个性化交互，限制了其在真实场景中的应用

Method: 提出PersonaPlex模型，采用混合系统提示结合角色条件（文本提示）和语音克隆（语音样本），使用开源大语言模型和TTS模型生成大规模合成数据集进行训练

Result: PersonaPlex在角色条件行为、语音条件语音和自然对话响应方面表现优异，在角色遵循度、说话人相似度、延迟和自然度上超越现有双工语音模型和混合LLM语音系统

Conclusion: PersonaPlex成功实现了支持结构化角色驱动应用和个性化交互的双工对话语音模型，扩展了双工语音模型在真实场景中的应用能力

Abstract: Recent advances in duplex speech models have enabled natural, low-latency speech-to-speech interactions. However, existing models are restricted to a fixed role and voice, limiting their ability to support structured, role-driven real-world applications and personalized interactions. In this work, we introduce PersonaPlex, a duplex conversational speech model that incorporates hybrid system prompts, combining role conditioning with text prompts and voice cloning with speech samples. PersonaPlex is trained on a large-scale synthetic dataset of paired prompts and user-agent conversations, generated with open-source large language models (LLM) and text-to-speech (TTS) models. To evaluate role conditioning in real-world settings, we extend the Full-Duplex-Bench benchmark beyond a single assistant role to multi-role customer service scenarios. Experiments show that PersonaPlex achieves strong role-conditioned behavior, voice-conditioned speech, and natural conversational responsiveness, surpassing state-of-the-art duplex speech models and hybrid large language model-based speech systems in role adherence, speaker similarity, latency, and naturalness.

</details>


### [6] [What Is Novel? A Knowledge-Driven Framework for Bias-Aware Literature Originality Evaluation](https://arxiv.org/abs/2602.06054)
*Abeer Mostafa,Thi Huyen Nguyen,Zahra Ahmadi*

Main category: cs.CL

TL;DR: 提出一个基于文献感知的新颖性评估框架，通过机器学习从同行评审报告中学习人类如何判断新颖性，并将这些判断基于与现有研究的结构化比较。


<details>
  <summary>Details</summary>
Motivation: 研究新颖性评估是同行评审的核心但高度主观的方面，通常基于隐含的判断和不完整的与先前工作的比较。需要更客观、一致的新颖性评估方法。

Method: 使用近8万条来自顶级AI会议的新颖性标注评审报告，微调大语言模型以捕捉与评审者一致的新颖性评估行为。系统提取论文思想、方法和主张的结构化表示，检索语义相关论文，构建相似性图进行细粒度概念级比较。

Result: 模型能够产生校准的新颖性评分和类似人类的解释性评估，相对于现有方法减少了高估并提高了评估一致性。

Conclusion: 该文献感知的新颖性评估框架通过学习人类评审行为并将其基于结构化文献比较，为研究新颖性评估提供了更客观、一致的方法。

Abstract: Assessing research novelty is a core yet highly subjective aspect of peer review, typically based on implicit judgment and incomplete comparison to prior work. We introduce a literature-aware novelty assessment framework that explicitly learns how humans judge novelty from peer-review reports and grounds these judgments in structured comparison to existing research. Using nearly 80K novelty-annotated reviews from top-tier AI conferences, we fine-tune a large language model to capture reviewer-aligned novelty evaluation behavior. For a given manuscript, the system extracts structured representations of its ideas, methods, and claims, retrieves semantically related papers, and constructs a similarity graph that enables fine-grained, concept-level comparison to prior work. Conditioning on this structured evidence, the model produces calibrated novelty scores and human-like explanatory assessments, reducing overestimation and improving consistency relative to existing approaches.

</details>


### [7] [Quantifying and Attributing Polarization to Annotator Groups](https://arxiv.org/abs/2602.06055)
*Dimitris Tsirmpas,John Pavlopoulos*

Main category: cs.CL

TL;DR: 提出新指标和统计检验来量化标注者群体间的极化程度，适用于不平衡群体和多标签场景，应用于仇恨言论和毒性检测数据集发现种族、宗教、教育程度等因素显著影响标注一致性。


<details>
  <summary>Details</summary>
Motivation: 现有标注一致性指标不适合群体间分析，对群体规模不平衡敏感，且仅限于单标注设置，无法满足毒性检测、仇恨言论识别等主观任务的评估需求。

Method: 提出可量化指标配合统计显著性检验，能够将极化归因于不同标注者群体，支持不平衡社会人口学和意识形态子群体的直接比较，并适用于多标签设置。

Result: 应用于三个仇恨言论数据集和一个毒性检测数据集发现：(1)种族因素在仇恨言论任务中持续导致强烈极化；(2)宗教标注者内部一致但与其他群体分歧，无宗教信仰者趋势相反；(3)低教育程度标注者更主观，高教育程度者内部一致性更高。

Conclusion: 新指标能有效揭示不同子群体的标注模式差异，反映了当前标注模式的研究发现，同时估计了获得稳健结果所需的最小标注者数量，并提供了开源Python库实现。

Abstract: Current annotation agreement metrics are not well-suited for inter-group analysis, are sensitive to group size imbalances and restricted to single-annotation settings. These restrictions render them insufficient for many subjective tasks such as toxicity and hate-speech detection. For this reason, we introduce a quantifiable metric, paired with a statistical significance test, that attributes polarization to various annotator groups. Our metric enables direct comparisons between heavily imbalanced sociodemographic and ideological subgroups across different datasets and tasks, while also enabling analysis on multi-label settings. We apply this metric to three datasets on hate speech, and one on toxicity detection, discovering that: (1) Polarization is strongly and persistently attributed to annotator race, especially on the hate speech task. (2) Religious annotators do not fundamentally disagree with each other, but do with other annotators, a trend that is gradually diminished and then reversed with irreligious annotators. (3) Less educated annotators are more subjective, while educated ones tend to broadly agree more between themselves. Overall, our results reflect current findings around annotation patterns for various subgroups. Finally, we estimate the minimum number of annotators needed to obtain robust results, and provide an open-source Python library that implements our metric.

</details>


### [8] [Stop the Flip-Flop: Context-Preserving Verification for Fast Revocable Diffusion Decoding](https://arxiv.org/abs/2602.06161)
*Yanzheng Xiang,Lan Wei,Yizhen Yao,Qinglin Zhu,Hanqi Yan,Chen Jin,Philip Alexander Teare,Dandan Zhang,Lin Gui,Amrutha Saseendran,Yulan He*

Main category: cs.CL

TL;DR: COVER通过KV缓存覆盖实现单前向传播验证，减少不必要的修订，加速扩散语言模型推理


<details>
  <summary>Details</summary>
Motivation: 现有并行扩散解码方法在加速推理时会影响质量，而可撤销解码的验证方案常导致"翻转振荡"问题，即标记被重新掩码后又恢复原状，这会削弱上下文条件并浪费修订预算

Method: 提出COVER方法：1) 通过KV缓存覆盖实现留一验证和稳定草稿，在单次前向传播中完成；2) 使用稳定性感知评分优先选择验证种子，平衡不确定性、下游影响和缓存漂移；3) 自适应调整每步验证种子数量

Result: COVER显著减少了不必要的修订，在保持输出质量的同时实现了更快的解码速度

Conclusion: COVER通过创新的KV缓存覆盖机制有效解决了并行扩散解码中的翻转振荡问题，实现了更高效的推理加速

Abstract: Parallel diffusion decoding can accelerate diffusion language model inference by unmasking multiple tokens per step, but aggressive parallelism often harms quality. Revocable decoding mitigates this by rechecking earlier tokens, yet we observe that existing verification schemes frequently trigger flip-flop oscillations, where tokens are remasked and later restored unchanged. This behaviour slows inference in two ways: remasking verified positions weakens the conditioning context for parallel drafting, and repeated remask cycles consume the revision budget with little net progress. We propose COVER (Cache Override Verification for Efficient Revision), which performs leave-one-out verification and stable drafting within a single forward pass. COVER constructs two attention views via KV cache override: selected seeds are masked for verification, while their cached key value states are injected for all other queries to preserve contextual information, with a closed form diagonal correction preventing self leakage at the seed positions. COVER further prioritises seeds using a stability aware score that balances uncertainty, downstream influence, and cache drift, and it adapts the number of verified seeds per step. Across benchmarks, COVER markedly reduces unnecessary revisions and yields faster decoding while preserving output quality.

</details>


### [9] [Uncertainty Drives Social Bias Changes in Quantized Large Language Models](https://arxiv.org/abs/2602.06181)
*Stanley Z. Hua,Sanae Lotfi,Irene Y. Chen*

Main category: cs.CL

TL;DR: 量化后训练会改变大语言模型的社会偏见模式，尽管聚合指标显示不变，但量化会导致21%的响应在偏见和无偏见状态间翻转，且对不同的社会群体产生不对称影响。


<details>
  <summary>Details</summary>
Motivation: 量化虽然降低了LLM的计算成本，但会从根本上改变其社会偏见，而现有的聚合指标无法捕捉这些变化。需要研究量化如何具体影响模型偏见表现。

Method: 使用PostTrainingBiasBench基准测试，对50个量化模型在13个封闭式和开放式偏见数据集上进行大规模评估，分析量化强度（4位 vs 8位）和模型大小的影响。

Result: 发现量化诱导的掩蔽偏见翻转现象：21%的响应在量化后偏见状态翻转；高不确定性响应翻转概率是低不确定性的3-11倍；4位量化比8位量化产生4-6倍的行为变化；偏见变化在不同人口群体间不对称，某些群体偏见恶化18.6%，其他改善14.1%；大模型没有一致的鲁棒性优势。

Conclusion: 压缩从根本上改变了偏见模式，需要关键的量化后评估和干预措施来确保实际应用的可靠性。聚合指标会掩盖重要的群体特异性变化。

Abstract: Post-training quantization reduces the computational cost of large language models but fundamentally alters their social biases in ways that aggregate metrics fail to capture. We present the first large-scale study of 50 quantized models evaluated on PostTrainingBiasBench, a unified benchmark of 13 closed- and open-ended bias datasets. We identify a phenomenon we term quantization-induced masked bias flipping, in which up to 21% of responses flip between biased and unbiased states after quantization, despite showing no change in aggregate bias scores. These flips are strongly driven by model uncertainty, where the responses with high uncertainty are 3-11x more likely to change than the confident ones. Quantization strength amplifies this effect, with 4-bit quantized models exhibiting 4-6x more behavioral changes than 8-bit quantized models. Critically, these changes create asymmetric impacts across demographic groups, where bias can worsen by up to 18.6% for some groups while improving by 14.1% for others, yielding misleadingly neutral aggregate outcomes. Larger models show no consistent robustness advantage, and group-specific shifts vary unpredictably across model families. Our findings demonstrate that compression fundamentally alters bias patterns, requiring crucial post-quantization evaluation and interventions to ensure reliability in practice.

</details>


### [10] [BenchMarker: An Education-Inspired Toolkit for Highlighting Flaws in Multiple-Choice Benchmarks](https://arxiv.org/abs/2602.06221)
*Nishant Balepur,Bhavya Rajasekaran,Jane Oh,Michael Xie,Atrey Desai,Vipul Gupta,Steven James Moore,Eunsol Choi,Rachel Rudinger,Jordan Lee Boyd-Graber*

Main category: cs.CL

TL;DR: BenchMarker工具包使用LLM法官检测多选题基准中的三种常见缺陷：污染、捷径和写作错误，揭示这些缺陷影响模型评估准确性，并发现现有基准修复会引入新问题。


<details>
  <summary>Details</summary>
Motivation: 当前NLP中的多选题问答（MCQA）基准缺乏严格的质量控制，导致评估结果不可靠。需要系统化工具来检测和修复多选题中的常见缺陷。

Method: 开发BenchMarker工具包，使用LLM法官检测三种多选题缺陷：1）污染（题目完全出现在网上）；2）捷径（选项中的线索使猜测成为可能）；3）写作错误（基于19条教育规则的语法结构问题）。通过人工标注验证工具有效性，并用其审计12个基准。

Result: 审计发现：1）污染的MCQ倾向于提高准确率；2）写作错误倾向于降低准确率并改变排名；3）现有的基准修复虽然解决了目标问题（如用LLM编写的干扰项降低准确率），但会引入新缺陷（如不合理的干扰项、多个正确答案）。

Conclusion: 多选题中的缺陷会降低NLP评估质量，但教育研究提供了改进路径。发布BenchMarker工具包以连接两个领域并改进MCQA基准设计。

Abstract: Multiple-choice question answering (MCQA) is standard in NLP, but benchmarks lack rigorous quality control. We present BenchMarker, an education-inspired toolkit using LLM judges to flag three common MCQ flaws: 1) contamination - items appearing exactly online; 2) shortcuts - cues in the choices that enable guessing; and 3) writing errors - structural/grammatical issues based on a 19-rule education rubric. We validate BenchMarker with human annotations, then run the tool to audit 12 benchmarks, revealing: 2) contaminated MCQs tend to inflate accuracy, while writing errors tend to lower it and change rankings beyond random; and 3) prior benchmark repairs address their targeted issues (i.e., lowering accuracy with LLM-written distractors), but inadvertently add new flaws (i.e. implausible distractors, many correct answers). Overall, flaws in MCQs degrade NLP evaluation, but education research offers a path forward. We release BenchMarker to bridge the fields and improve MCQA benchmark design.

</details>


### [11] [Can One-sided Arguments Lead to Response Change in Large Language Models?](https://arxiv.org/abs/2602.06260)
*Pedro Cisneros-Velarde*

Main category: cs.CL

TL;DR: 研究发现仅提供单方面论点就能有效引导大语言模型在争议问题上的立场，这种观点引导在不同模型、论点数量和话题中普遍存在


<details>
  <summary>Details</summary>
Motivation: 大语言模型在回答争议性问题时可能提供平衡答案、采取单一立场或拒绝回答。本研究想探索是否可以通过简单直观的方式（仅提供单方面论点）来引导模型的初始回应立场。

Method: 构建小型数据集，从三个维度系统研究：1) 模型回应中诱导的立场；2) 争议性问题的表述方式；3) 论点的展示方式。测试不同模型、论点数量和话题，并比较切换到其他论点时的效果。

Result: 研究发现观点引导在不同模型、论点数量和话题中普遍存在。仅提供单方面论点就能有效引导模型立场，而切换到其他论点会一致降低观点引导效果。

Conclusion: 仅通过提供单方面论点就能简单有效地引导大语言模型在争议问题上的立场，这种观点引导现象具有普遍性，对理解模型偏见和可控性有重要意义。

Abstract: Polemic questions need more than one viewpoint to express a balanced answer. Large Language Models (LLMs) can provide a balanced answer, but also take a single aligned viewpoint or refuse to answer. In this paper, we study if such initial responses can be steered to a specific viewpoint in a simple and intuitive way: by only providing one-sided arguments supporting the viewpoint. Our systematic study has three dimensions: (i) which stance is induced in the LLM response, (ii) how the polemic question is formulated, (iii) how the arguments are shown. We construct a small dataset and remarkably find that opinion steering occurs across (i)-(iii) for diverse models, number of arguments, and topics. Switching to other arguments consistently decreases opinion steering.

</details>


### [12] [Is my model "mind blurting"? Interpreting the dynamics of reasoning tokens with Recurrence Quantification Analysis (RQA)](https://arxiv.org/abs/2602.06266)
*Quoc Tuan Pham,Mehdi Jafari,Flora Salim*

Main category: cs.CL

TL;DR: 本文提出使用递归量化分析(RQA)作为分析推理模型测试时计算的非文本方法，通过将token生成视为动力系统来分析隐藏嵌入轨迹，比单纯使用响应长度更能有效捕捉推理动态。


<details>
  <summary>Details</summary>
Motivation: 当前分析大型推理模型测试时计算主要依赖生成文本，但这种方法越来越不实用且不可靠。响应长度常被用作推理努力的粗略代理，但无法捕捉思维链的动态和有效性。

Method: 将token生成视为动力系统，在每个生成步骤提取隐藏嵌入，然后对得到的轨迹应用递归量化分析(RQA)。RQA指标如确定性和层流性量化了模型潜在表示中的重复和停滞模式。

Result: 通过分析DeepSeek-R1-Distill的3,600个生成轨迹，发现RQA不仅捕捉到了响应长度无法反映的信号，还将任务复杂度的预测准确率提高了8%。

Conclusion: RQA可作为研究推理模型测试时扩展中潜在token生成动态的原则性工具，为分析模型推理链提供了非文本的替代方法。

Abstract: Test-time compute is central to large reasoning models, yet analysing their reasoning behaviour through generated text is increasingly impractical and unreliable. Response length is often used as a brute proxy for reasoning effort, but this metric fails to capture the dynamics and effectiveness of the Chain of Thoughts (CoT) or the generated tokens. We propose Recurrence Quantification Analysis (RQA) as a non-textual alternative for analysing model's reasoning chains at test time. By treating token generation as a dynamical system, we extract hidden embeddings at each generation step and apply RQA to the resulting trajectories. RQA metrics, including Determinism and Laminarity, quantify patterns of repetition and stalling in the model's latent representations. Analysing 3,600 generation traces from DeepSeek-R1-Distill, we show that RQA captures signals not reflected by response length, but also substantially improves prediction of task complexity by 8\%. These results help establish RQA as a principled tool for studying the latent token generation dynamics of test-time scaling in reasoning models.

</details>


### [13] [MPIB: A Benchmark for Medical Prompt Injection Attacks and Clinical Safety in LLMs](https://arxiv.org/abs/2602.06268)
*Junhyeok Lee,Han Jang,Kyu Sung Choi*

Main category: cs.CL

TL;DR: MPIB是一个医学提示注入基准测试套件，用于评估LLM和RAG系统在临床环境中的安全性，通过临床伤害事件率(CHER)和攻击成功率(ASR)来衡量风险。


<details>
  <summary>Details</summary>
Motivation: 随着LLM和RAG系统越来越多地集成到临床工作流程中，提示注入攻击可能导致临床不安全或误导性输出，需要系统评估临床安全性。

Method: 开发了医学提示注入基准测试(MPIB)，包含9,697个经过多阶段质量控制和临床安全检查的实例，通过CHER和ASR指标评估直接和间接RAG介导的提示注入攻击。

Result: 评估发现ASR和CHER可能显著偏离，鲁棒性关键取决于对抗性指令出现在用户查询还是检索上下文中，现有防御配置效果有限。

Conclusion: MPIB为临床提示注入研究提供了可复现的系统评估框架，强调了区分指令遵从性和下游患者风险的重要性，并发布了完整的数据集和评估代码。

Abstract: Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) systems are increasingly integrated into clinical workflows; however, prompt injection attacks can steer these systems toward clinically unsafe or misleading outputs. We introduce the Medical Prompt Injection Benchmark (MPIB), a dataset-and-benchmark suite for evaluating clinical safety under both direct prompt injection and indirect, RAG-mediated injection across clinically grounded tasks. MPIB emphasizes outcome-level risk via the Clinical Harm Event Rate (CHER), which measures high-severity clinical harm events under a clinically grounded taxonomy, and reports CHER alongside Attack Success Rate (ASR) to disentangle instruction compliance from downstream patient risk. The benchmark comprises 9,697 curated instances constructed through multi-stage quality gates and clinical safety linting. Evaluating MPIB across a diverse set of baseline LLMs and defense configurations, we find that ASR and CHER can diverge substantially, and that robustness depends critically on whether adversarial instructions appear in the user query or in retrieved context. We release MPIB with evaluation code, adversarial baselines, and comprehensive documentation to support reproducible and systematic research on clinical prompt injection. Code and data are available at GitHub (code) and Hugging Face (data).

</details>


### [14] [VowelPrompt: Hearing Speech Emotions from Text via Vowel-level Prosodic Augmentation](https://arxiv.org/abs/2602.06270)
*Yancheng Wang,Osama Hanna,Ruiming Xie,Xianfeng Rui,Maohao Shen,Xuedong Zhang,Christian Fuegen,Jilong Wu,Debjyoti Paul,Arthur Guo,Zhihong Lei,Ozlem Kalinli,Qing He,Yingzhen Yang*

Main category: cs.CL

TL;DR: VowelPrompt：基于语言学基础的框架，通过可解释的元音级韵律线索增强LLM情感识别，结合两阶段适应方法，在多种条件下超越SOTA方法


<details>
  <summary>Details</summary>
Motivation: 语音情感识别需要理解语言内容和声音表达性（特别是韵律特征）。虽然LLM在文本转录的情感识别中表现出潜力，但通常忽略了细粒度的韵律信息，限制了其效果和可解释性。

Method: 提出VowelPrompt框架：1）基于语音学证据（元音是情感韵律的主要载体），从时间对齐的元音段提取基于音高、能量和时长的描述符；2）将这些特征转换为自然语言描述以提高可解释性；3）采用两阶段适应方法：监督微调（SFT）后接基于可验证奖励的强化学习（RLVR），通过组相对策略优化（GRPO）实现，以增强推理能力、确保结构化输出并提高跨领域和说话人变化的泛化能力。

Result: 在多个基准数据集上的广泛评估表明，VowelPrompt在零样本、微调、跨领域和跨语言条件下一致优于最先进的情感识别方法，同时能够生成基于上下文语义和细粒度韵律结构的可解释解释。

Conclusion: VowelPrompt成功地将细粒度韵律信息整合到LLM推理中，通过语言学基础的元音级分析和两阶段适应方法，显著提升了情感识别的性能、可解释性和泛化能力。

Abstract: Emotion recognition in speech presents a complex multimodal challenge, requiring comprehension of both linguistic content and vocal expressivity, particularly prosodic features such as fundamental frequency, intensity, and temporal dynamics. Although large language models (LLMs) have shown promise in reasoning over textual transcriptions for emotion recognition, they typically neglect fine-grained prosodic information, limiting their effectiveness and interpretability. In this work, we propose VowelPrompt, a linguistically grounded framework that augments LLM-based emotion recognition with interpretable, fine-grained vowel-level prosodic cues. Drawing on phonetic evidence that vowels serve as primary carriers of affective prosody, VowelPrompt extracts pitch-, energy-, and duration-based descriptors from time-aligned vowel segments, and converts these features into natural language descriptions for better interpretability. Such a design enables LLMs to jointly reason over lexical semantics and fine-grained prosodic variation. Moreover, we adopt a two-stage adaptation procedure comprising supervised fine-tuning (SFT) followed by Reinforcement Learning with Verifiable Reward (RLVR), implemented via Group Relative Policy Optimization (GRPO), to enhance reasoning capability, enforce structured output adherence, and improve generalization across domains and speaker variations. Extensive evaluations across diverse benchmark datasets demonstrate that VowelPrompt consistently outperforms state-of-the-art emotion recognition methods under zero-shot, fine-tuned, cross-domain, and cross-linguistic conditions, while enabling the generation of interpretable explanations that are jointly grounded in contextual semantics and fine-grained prosodic structure.

</details>


### [15] [RoPE-LIME: RoPE-Space Locality + Sparse-K Sampling for Efficient LLM Attribution](https://arxiv.org/abs/2602.06275)
*Isaac Picov,Ritesh Goru*

Main category: cs.CL

TL;DR: RoPE-LIME：一种用于解释闭源LLM输出的开源方法，通过使用较小的开源代理模型计算基于概率目标的token级归因，减少API调用并提高解释质量。


<details>
  <summary>Details</summary>
Motivation: 解释闭源LLM输出具有挑战性，因为API访问阻止了基于梯度的归因方法，而扰动方法在依赖重新生成文本时成本高且噪声大。

Method: 扩展gSMILE方法，使用较小的开源代理模型在固定闭源模型输出的情况下，通过输入扰动计算基于概率目标（负对数似然和散度目标）的token级归因。包含：(i) 基于RoPE嵌入空间中计算的Relaxed Word Mover's Distance的局部性核函数，(ii) Sparse-K采样，一种在有限预算下提高交互覆盖率的有效扰动策略。

Result: 在HotpotQA（句子特征）和手工标注的MMLU子集（单词特征）上的实验表明，RoPE-LIME比留一采样产生更具信息量的归因，优于gSMILE，同时显著减少了闭源模型的API调用。

Conclusion: RoPE-LIME提供了一种有效解释闭源LLM输出的方法，通过解耦推理和解释过程，使用开源代理模型减少对闭源API的依赖，同时提高解释质量。

Abstract: Explaining closed-source LLM outputs is challenging because API access prevents gradient-based attribution, while perturbation methods are costly and noisy when they depend on regenerated text. We introduce RoPE-LIME, an open-source extension of gSMILE that decouples reasoning from explanation: given a fixed output from a closed model, a smaller open-source surrogate computes token-level attributions from probability-based objectives (negative log-likelihood and divergence targets) under input perturbations. RoPE-LIME incorporates (i) a locality kernel based on Relaxed Word Mover's Distance computed in RoPE embedding space for stable similarity under masking, and (ii) Sparse-K sampling, an efficient perturbation strategy that improves interaction coverage under limited budgets. Experiments on HotpotQA (sentence features) and a hand-labeled MMLU subset (word features) show that RoPE-LIME produces more informative attributions than leave-one-out sampling and improves over gSMILE while substantially reducing closed-model API calls.

</details>


### [16] [Inference-Time Rethinking with Latent Thought Vectors for Math Reasoning](https://arxiv.org/abs/2602.06584)
*Deqian Kong,Minglu Zhao,Aoyang Qin,Bo Pang,Chenxin Tao,David Hartmann,Edouardo Honig,Dehong Xu,Amit Kumar,Matt Sarte,Chuan Li,Jianwen Xie,Ying Nian Wu*

Main category: cs.CL

TL;DR: 提出Inference-Time Rethinking框架，通过将推理分解为潜在思维向量和语言化解码器，实现迭代式自我修正，在小型模型上超越更大参数量的基线模型。


<details>
  <summary>Details</summary>
Motivation: 标准思维链推理在单次前向传播中生成解决方案，一旦出现早期错误就无法修正。需要一种能够进行迭代自我修正的推理框架。

Method: 将推理分解为连续潜在思维向量（推理内容）和解码器（如何推理）。采用Gibbs风格过程，交替生成候选推理轨迹和优化潜在向量以更好地解释该轨迹，在潜在流形上导航以优化推理策略。

Result: 在GSM8K上训练0.2B参数模型，经过30次重新思考迭代后，性能超越参数量大10-15倍的基线模型，包括3B参数模型。

Conclusion: 有效的数学推理可以通过复杂的推理时计算实现，而不仅仅依赖于大量参数。Inference-Time Rethinking框架为小型模型提供了强大的推理能力。

Abstract: Standard chain-of-thought reasoning generates a solution in a single forward pass, committing irrevocably to each token and lacking a mechanism to recover from early errors. We introduce Inference-Time Rethinking, a generative framework that enables iterative self-correction by decoupling declarative latent thought vectors from procedural generation. We factorize reasoning into a continuous latent thought vector (what to reason about) and a decoder that verbalizes the trace conditioned on this vector (how to reason). Beyond serving as a declarative buffer, latent thought vectors compress the reasoning structure into a continuous representation that abstracts away surface-level token variability, making gradient-based optimization over reasoning strategies well-posed. Our prior model maps unstructured noise to a learned manifold of valid reasoning patterns, and at test time we employ a Gibbs-style procedure that alternates between generating a candidate trace and optimizing the latent vector to better explain that trace, effectively navigating the latent manifold to refine the reasoning strategy. Training a 0.2B-parameter model from scratch on GSM8K, our method with 30 rethinking iterations surpasses baselines with 10 to 15 times more parameters, including a 3B counterpart. This result demonstrates that effective mathematical reasoning can emerge from sophisticated inference-time computation rather than solely from massive parameter counts.

</details>


### [17] [Judging What We Cannot Solve: A Consequence-Based Approach for Oracle-Free Evaluation of Research-Level Math](https://arxiv.org/abs/2602.06291)
*Guijin Son,Donghun Yang,Hitesh Laxmichand Patel,Hyunwoo Ko,Amit Agarwal,Sunghee Ahn,Kyong-Ha Lee,Youngjae Yu*

Main category: cs.CL

TL;DR: 提出Consequence-Based Utility评估方法，通过测试候选解决方案作为上下文示例解决相关可验证问题的能力来评分，无需专家验证，在数学问题评估中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 推理模型在生成研究级数学问题的解决方案方面取得进展，但验证仍依赖专家时间成为瓶颈。作者假设有效的解决方案应包含足够的方法层面信息，当应用于相关问题时能产生更好的下游性能。

Method: 提出Consequence-Based Utility评估框架：将每个候选解决方案作为上下文示例，测试其在解决相关可验证问题时的价值。该方法无需专家验证，通过评估解决方案作为示例的效用来进行评分。

Result: 在原创研究级数学问题集上评估，每个问题包含1个专家解决方案和9个LLM生成方案。Consequence-Based Utility在排名质量上一致优于奖励模型、生成奖励模型和LLM评判者。对GPT-OSS-120B，Acc@1从67.2提升至76.3，AUC从71.4提升至79.6。

Conclusion: Consequence-Based Utility提供了一种无需专家的有效评估方法，能更好地区分正确和错误解决方案，即使在底层求解器经常失败的情况下也能保持较强的区分能力。

Abstract: Recent progress in reasoning models suggests that generating plausible attempts for research-level mathematics may be within reach, but verification remains a bottleneck, consuming scarce expert time. We hypothesize that a meaningful solution should contain enough method-level information that, when applied to a neighborhood of related questions, it should yield better downstream performance than incorrect solutions. Building on this idea, we propose \textbf{Consequence-Based Utility}, an oracle-free evaluator that scores each candidate by testing its value as an in-context exemplar in solving related yet verifiable questions. Our approach is evaluated on an original set of research-level math problems, each paired with one expert-written solution and nine LLM-generated solutions. Notably, Consequence-Based Utility consistently outperforms reward models, generative reward models, and LLM judges on ranking quality. Specifically, for GPT-OSS-120B, it improves Acc@1 from 67.2 to 76.3 and AUC from 71.4 to 79.6, with similarly large AUC gains on GPT-OSS-20B (69.0 to 79.2). Furthermore, compared to LLM-Judges, it also exhibits a larger solver-evaluator gap, maintaining a stronger correct-wrong separation even on instances where the underlying solver often fails to solve.

</details>


### [18] [Lost in Speech: Benchmarking, Evaluation, and Parsing of Spoken Code-Switching Beyond Standard UD Assumptions](https://arxiv.org/abs/2602.06307)
*Nemika Tyagi,Holly Hendrix,Nelvin Licona-Guevara,Justin Mackie,Phanos Kareen,Muhammad Imran,Megan Michelle Smith,Tatiana Gallego Hernande,Chitta Baral,Olga Kellert*

Main category: cs.CL

TL;DR: 本文提出针对口语代码转换的解析方法，包括SpokeBench基准测试、FLEX-UD评估指标和DECAP解析框架，显著提升口语解析性能


<details>
  <summary>Details</summary>
Motivation: 口语代码转换（CSW）在句法解析上带来独特挑战，如不流利、重复、省略和话语驱动结构等违反标准UD假设的现象，导致现有解析器和LLM在口语数据上表现不佳，而现有评估指标又混淆了真正的结构错误和可接受的变异

Method: 1) 提出基于语言学的口语CSW现象分类法；2) 构建专家标注的黄金基准SpokeBench；3) 提出FLEX-UD歧义感知评估指标；4) 提出DECAP解耦代理解析框架，将口语现象处理与核心句法分析分离

Result: DECAP框架在不重新训练的情况下产生更鲁棒和可解释的解析，相比现有解析技术提升高达52.6%；FLEX-UD评估揭示了标准指标掩盖的定性改进

Conclusion: 口语CSW解析需要专门的方法来处理其独特特征，DECAP框架和FLEX-UD指标为口语语言结构分析提供了更准确和鲁棒的解决方案，揭示了现有评估方法的局限性

Abstract: Spoken code-switching (CSW) challenges syntactic parsing in ways not observed in written text. Disfluencies, repetition, ellipsis, and discourse-driven structure routinely violate standard Universal Dependencies (UD) assumptions, causing parsers and large language models (LLMs) to fail despite strong performance on written data. These failures are compounded by rigid evaluation metrics that conflate genuine structural errors with acceptable variation. In this work, we present a systems-oriented approach to spoken CSW parsing. We introduce a linguistically grounded taxonomy of spoken CSW phenomena and SpokeBench, an expert-annotated gold benchmark designed to test spoken-language structure beyond standard UD assumptions. We further propose FLEX-UD, an ambiguity-aware evaluation metric, which reveals that existing parsing techniques perform poorly on spoken CSW by penalizing linguistically plausible analyses as errors. We then propose DECAP, a decoupled agentic parsing framework that isolates spoken-phenomena handling from core syntactic analysis. Experiments show that DECAP produces more robust and interpretable parses without retraining and achieves up to 52.6% improvements over existing parsing techniques. FLEX-UD evaluations further reveal qualitative improvements that are masked by standard metrics.

</details>


### [19] [Can Post-Training Transform LLMs into Causal Reasoners?](https://arxiv.org/abs/2602.06337)
*Junqi Chen,Sirui Chen,Chaochao Lu*

Main category: cs.CL

TL;DR: 通过系统性的后训练方法，可以使较小的语言模型在因果推理任务上达到甚至超越更大模型的性能，为构建可靠的基于LLM的因果推理器提供了首个系统性证据。


<details>
  <summary>Details</summary>
Motivation: 因果推理对决策至关重要，但非专家难以掌握。虽然大语言模型在该领域有潜力，但其精确因果估计能力有限，且后训练对这些能力的影响研究不足。本文旨在探索后训练能否增强LLM的因果推理能力。

Method: 引入CauGym数据集，包含7个核心因果任务用于训练和5个多样化测试集。系统评估5种后训练方法：SFT、DPO、KTO、PPO和GRPO。在5个领域内和4个现有基准上进行实验。

Result: 适当的后训练使较小的LLM在因果推理任务上具有竞争力，经常超越更大模型。14B参数模型在CaLM基准上达到93.5%准确率，而OpenAI o3仅为55.4%。后训练LLM在分布偏移和噪声数据等现实条件下表现出强大的泛化能力和鲁棒性。

Conclusion: 这是首个系统性证据表明，有针对性的后训练可以产生可靠且鲁棒的基于LLM的因果推理器。研究为构建实用的因果推理系统提供了重要方法。

Abstract: Causal inference is essential for decision-making but remains challenging for non-experts. While large language models (LLMs) show promise in this domain, their precise causal estimation capabilities are still limited, and the impact of post-training on these abilities is insufficiently explored. This paper examines the extent to which post-training can enhance LLMs' capacity for causal inference. We introduce CauGym, a comprehensive dataset comprising seven core causal tasks for training and five diverse test sets. Using this dataset, we systematically evaluate five post-training approaches: SFT, DPO, KTO, PPO, and GRPO. Across five in-domain and four existing benchmarks, our experiments demonstrate that appropriate post-training enables smaller LLMs to perform causal inference competitively, often surpassing much larger models. Our 14B parameter model achieves 93.5% accuracy on the CaLM benchmark, compared to 55.4% by OpenAI o3. Furthermore, the post-trained LLMs exhibit strong generalization and robustness under real-world conditions such as distribution shifts and noisy data. Collectively, these findings provide the first systematic evidence that targeted post-training can produce reliable and robust LLM-based causal reasoners. Our data and GRPO-model are available at https://github.com/OpenCausaLab/CauGym.

</details>


### [20] [SHINE: A Scalable In-Context Hypernetwork for Mapping Context to LoRA in a Single Pass](https://arxiv.org/abs/2602.06358)
*Yewei Liu,Xiyuan Wang,Yansheng Mao,Yoav Gelbery,Haggai Maron,Muhan Zhang*

Main category: cs.CL

TL;DR: SHINE是一个可扩展的超网络，能够将多样化的上下文映射为高质量LoRA适配器，实现单次前向传播将上下文知识转换为参数知识，无需微调LLM。


<details>
  <summary>Details</summary>
Motivation: 传统超网络存在表达能力有限、参数效率低等限制，需要一种能够高效将上下文知识转换为模型参数知识的方法，以节省时间、计算和内存成本。

Method: 采用基于冻结LLM参数的上下文超网络设计，引入架构创新，通过预训练和指令微调流程，训练超网络从多样化上下文生成高质量LoRA适配器。

Result: 在各种任务上取得优异结果，相比基于SFT的LLM适配方法，大大节省时间、计算和内存成本，显示出良好的扩展潜力。

Conclusion: SHINE通过创新的超网络设计，实现了高效的单次上下文到参数知识转换，为LLM适配提供了可扩展且高效的解决方案。

Abstract: We propose SHINE (Scalable Hyper In-context NEtwork), a scalable hypernetwork that can map diverse meaningful contexts into high-quality LoRA adapters for large language models (LLM). By reusing the frozen LLM's own parameters in an in-context hypernetwork design and introducing architectural innovations, SHINE overcomes key limitations of prior hypernetworks and achieves strong expressive power with a relatively small number of parameters. We introduce a pretraining and instruction fine-tuning pipeline, and train our hypernetwork to generate high quality LoRA adapters from diverse meaningful contexts in a single forward pass. It updates LLM parameters without any fine-tuning, and immediately enables complex question answering tasks related to the context without directly accessing the context, effectively transforming in-context knowledge to in-parameter knowledge in one pass. Our work achieves outstanding results on various tasks, greatly saves time, computation and memory costs compared to SFT-based LLM adaptation, and shows great potential for scaling. Our code is available at https://github.com/Yewei-Liu/SHINE

</details>


### [21] [Cost-Aware Model Selection for Text Classification: Multi-Objective Trade-offs Between Fine-Tuned Encoders and LLM Prompting in Production](https://arxiv.org/abs/2602.06370)
*Alberto Andres Valdes Gonzalez*

Main category: cs.CL

TL;DR: 比较了两种文本分类范式：基于提示的LLM（零/少样本）与微调的编码器模型（如BERT），发现微调编码器在性能相当或更优的同时，成本和延迟低1-2个数量级，建议在结构化分类任务中优先使用微调编码器而非盲目采用LLM。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在开放推理任务中表现出色，但在结构化文本分类任务中，模型选择往往只关注预测性能，忽视了生产系统中的实际约束（如延迟、成本）。需要系统比较不同范式的综合表现。

Method: 系统比较两种范式：1）零/少样本提示的LLM（如GPT-4o、Claude Sonnet 4.5）；2）完全微调的编码器架构（BERT系列）。在四个基准数据集（IMDB、SST-2、AG News、DBPedia）上评估预测质量（宏F1）、推理延迟和货币成本，使用帕累托前沿投影和参数化效用函数进行多目标决策分析。

Result: 微调的编码器模型在分类性能上具有竞争力甚至更优，同时相比零/少样本LLM提示，成本和延迟低1-2个数量级。LLM在标准文本分类任务中可能导致次优的系统级结果。

Conclusion: 不应盲目使用大语言模型处理标准文本分类任务。微调编码器是结构化NLP流程中更稳健高效的组件，而LLM更适合作为混合架构中的补充元素。研究提供了代码、数据集和评估协议以支持可复现性和成本感知的NLP系统设计。

Abstract: Large language models (LLMs) such as GPT-4o and Claude Sonnet 4.5 have demonstrated strong capabilities in open-ended reasoning and generative language tasks, leading to their widespread adoption across a broad range of NLP applications. However, for structured text classification problems with fixed label spaces, model selection is often driven by predictive performance alone, overlooking operational constraints encountered in production systems.
  In this work, we present a systematic comparison of two contrasting paradigms for text classification: zero- and few-shot prompt-based large language models, and fully fine-tuned encoder-only architectures. We evaluate these approaches across four canonical benchmarks (IMDB, SST-2, AG News, and DBPedia), measuring predictive quality (macro F1), inference latency, and monetary cost.
  We frame model evaluation as a multi-objective decision problem and analyze trade-offs using Pareto frontier projections and a parameterized utility function reflecting different deployment regimes. Our results show that fine-tuned encoder-based models from the BERT family achieve competitive, and often superior, classification performance while operating at one to two orders of magnitude lower cost and latency compared to zero- and few-shot LLM prompting.
  Overall, our findings suggest that indiscriminate use of large language models for standard text classification workloads can lead to suboptimal system-level outcomes. Instead, fine-tuned encoders emerge as robust and efficient components for structured NLP pipelines, while LLMs are better positioned as complementary elements within hybrid architectures. We release all code, datasets, and evaluation protocols to support reproducibility and cost-aware NLP system design.

</details>


### [22] [ReBeCA: Unveiling Interpretable Behavior Hierarchy behind the Iterative Self-Reflection of Language Models with Causal Analysis](https://arxiv.org/abs/2602.06373)
*Tianqiang Yan,Sihan Shang,Yuheng Li,Song Qiu,Hao Peng,Wenjian Luo,Jue Xie,Lizhen Qu,Yuan Gao*

Main category: cs.CL

TL;DR: ReBeCA框架通过因果分析揭示自反思的行为层次结构，发现只有少数语义行为具有因果效应，且更多行为不一定更好。


<details>
  <summary>Details</summary>
Motivation: 现有自反思机制分析多为相关性研究，缺乏可解释性和泛化性，需要因果分析来揭示真正的决定因素。

Method: 提出ReBeCA框架，将自反思轨迹建模为因果图，采用三阶段不变因果预测(ICP)流程分离性能的真正决定因素。

Result: 发现自反思的行为层次结构、仅有少数语义行为具有因果泛化性、行为过多反而损害效果，ICP验证显示因果父节点带来49.6%结构似然增益。

Conclusion: ReBeCA为自反思动态提供了严谨的因果分析方法，能够区分真实因果机制与虚假关联，具有跨任务稳定性。

Abstract: While self-reflection can enhance language model reliability, its underlying mechanisms remain opaque, with existing analyses often yielding correlation-based insights that fail to generalize. To address this, we introduce \textbf{\texttt{ReBeCA}} (self-\textbf{\texttt{Re}}flection \textbf{\texttt{Be}}havior explained through \textbf{\texttt{C}}ausal \textbf{\texttt{A}}nalysis), a framework that unveils the interpretable behavioral hierarchy governing the self-reflection outcome. By modeling self-reflection trajectories as causal graphs, ReBeCA isolates genuine determinants of performance through a three-stage Invariant Causal Prediction (ICP) pipeline. We establish three critical findings: (1) \textbf{Behavioral hierarchy:} Semantic behaviors of the model influence final self-reflection results hierarchically: directly or indirectly; (2) \textbf{Causation matters:} Generalizability in self-reflection effects is limited to just a few semantic behaviors; (3) \textbf{More $\mathbf{\neq}$ better:} The confluence of seemingly positive semantic behaviors, even among direct causal factors, can impair the efficacy of self-reflection. ICP-based verification identifies sparse causal parents achieving up to $49.6\%$ structural likelihood gains, stable across tasks where correlation-based patterns fail. Intervention studies on novel datasets confirm these causal relationships hold out-of-distribution ($p = .013, η^2_\mathrm{p} = .071$). ReBeCA thus provides a rigorous methodology for disentangling genuine causal mechanisms from spurious associations in self-reflection dynamics.

</details>


### [23] [FMBench: Adaptive Large Language Model Output Formatting](https://arxiv.org/abs/2602.06384)
*Yaoting Wang,Yun Zhou,Henghui Ding*

Main category: cs.CL

TL;DR: FMBench：一个评估大语言模型Markdown格式输出能力的基准，并提出结合监督微调和强化学习的轻量级对齐方法，以平衡语义准确性和结构正确性。


<details>
  <summary>Details</summary>
Motivation: Markdown格式在助手、文档和工具增强流程中无处不在，但大语言模型在生成Markdown时容易出现难以检测的细微错误（如损坏的列表、格式错误的表格、不一致的标题和无效的代码块），这会严重影响下游可用性。

Method: 提出FMBench基准测试，用于评估模型在各种指令遵循场景下的Markdown格式输出能力。同时提出轻量级对齐流程：首先对基础模型进行监督微调（SFT），然后通过强化学习微调优化复合目标函数，平衡语义保真度和结构正确性。

Result: 在两个模型家族（OpenPangu和Qwen）上的实验表明，SFT能持续改善语义对齐，而强化学习在具有挑战性的Markdown指令上能提供额外的鲁棒性增益。研究还揭示了语义目标和结构目标之间的固有权衡。

Conclusion: 通过精心设计的奖励机制，结合监督微调和强化学习的对齐方法能有效提高Markdown格式输出的合规性，为可靠格式化生成提供了重要指导。

Abstract: Producing outputs that satisfy both semantic intent and format constraints is essential for deploying large language models in user-facing and system-integrated workflows. In this work, we focus on Markdown formatting, which is ubiquitous in assistants, documentation, and tool-augmented pipelines but still prone to subtle, hard-to-detect errors (e.g., broken lists, malformed tables, inconsistent headings, and invalid code blocks) that can significantly degrade downstream usability. We present FMBench, a benchmark for adaptive Markdown output formatting that evaluates models under a wide range of instruction-following scenarios with diverse structural requirements. FMBench emphasizes real-world formatting behaviors such as multi-level organization, mixed content (natural language interleaved with lists/tables/code), and strict adherence to user-specified layout constraints. To improve Markdown compliance without relying on hard decoding constraints, we propose a lightweight alignment pipeline that combines supervised fine-tuning (SFT) with reinforcement learning fine-tuning. Starting from a base model, we first perform SFT on instruction-response pairs, and then optimize a composite objective that balances semantic fidelity with structural correctness. Experiments on two model families (OpenPangu and Qwen) show that SFT consistently improves semantic alignment, while reinforcement learning provides additional gains in robustness to challenging Markdown instructions when initialized from a strong SFT policy. Our results also reveal an inherent trade-off between semantic and structural objectives, highlighting the importance of carefully designed rewards for reliable formatted generation. Code is available at: https://github.com/FudanCVL/FMBench.

</details>


### [24] [Stopping Computation for Converged Tokens in Masked Diffusion-LM Decoding](https://arxiv.org/abs/2602.06412)
*Daisuke Oba,Danushka Bollegala,Masahiro Kaneko,Naoaki Okazaki*

Main category: cs.CL

TL;DR: SureLock是一种针对掩码扩散语言模型的优化方法，通过锁定已稳定位置来减少计算开销，将复杂度从O(N²d)降低到O(MNd)，在LLaDA-8B上减少30-50%的FLOPs


<details>
  <summary>Details</summary>
Motivation: 掩码扩散语言模型在迭代采样过程中，即使许多未掩码的token已经稳定，仍然为每个token位置重新计算注意力层和前馈层，造成了大量计算浪费

Method: 提出SureLock方法：当未掩码位置的posterior在多个步骤中稳定时（sure条件），锁定该位置，跳过其query投影和前馈子层，同时缓存其注意力key和value供其他位置继续关注

Result: 在LLaDA-8B模型上，SureLock相比无锁定的相同采样器减少了30-50%的算法FLOPs，同时保持了可比的生成质量

Conclusion: SureLock通过动态锁定已稳定token位置，显著减少了掩码扩散语言模型的计算开销，理论分析证明仅监控锁定步骤的局部KL散度就足以约束最终token概率的偏差

Abstract: Masked Diffusion Language Models generate sequences via iterative sampling that progressively unmasks tokens. However, they still recompute the attention and feed-forward blocks for every token position at every step -- even when many unmasked tokens are essentially fixed, resulting in substantial waste in compute. We propose SureLock: when the posterior at an unmasked position has stabilized across steps (our sure condition), we lock that position -- thereafter skipping its query projection and feed-forward sublayers -- while caching its attention keys and values so other positions can continue to attend to it. This reduces the dominant per-iteration computational cost from $O(N^2d)$ to $O(MNd)$ where $N$ is the sequence length, $M$ is the number of unlocked token positions, and $d$ is the model dimension. In practice, $M$ decreases as the iteration progresses, yielding substantial savings. On LLaDA-8B, SureLock reduces algorithmic FLOPs by 30--50% relative to the same sampler without locking, while maintaining comparable generation quality. We also provide a theoretical analysis to justify the design rationale of SureLock: monitoring only the local KL at the lock step suffices to bound the deviation in final token probabilities. Our code will be available at https://daioba.github.io/surelock .

</details>


### [25] [On the Wings of Imagination: Conflicting Script-based Multi-role Framework for Humor Caption Generation](https://arxiv.org/abs/2602.06423)
*Wenbo Shang,Yuxi Sun,Jing Ma,Xin Huang*

Main category: cs.CL

TL;DR: HOMER：基于GTVH幽默理论的多角色LLM协作框架，通过检索增强的层次化想象树生成图像幽默字幕，在New Yorker Cartoon数据集上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 多模态幽默生成（如图像幽默字幕）对LLM具有挑战性，需要视觉理解、幽默推理和创造性想象。现有LLM方法依赖推理链或自我改进，存在创造力有限和可解释性不足的问题。

Method: 提出基于GTVH幽默理论的多角色LLM协作框架HOMER：1）冲突脚本提取器定位关键脚本对立；2）检索增强的层次化想象器识别幽默目标并构建想象树扩展创意空间；3）字幕生成器基于获得的知识生成幽默多样的字幕。

Result: 在两个New Yorker Cartoon基准数据集上的实验表明，HOMER在多项指标上超越了现有最先进的基线方法和强大的LLM推理策略。

Conclusion: HOMER通过将幽默理论融入LLM协作框架，有效解决了多模态幽默生成的挑战，在创造性和可解释性方面均有显著提升。

Abstract: Humor is a commonly used and intricate human language in daily life. Humor generation, especially in multi-modal scenarios, is a challenging task for large language models (LLMs), which is typically as funny caption generation for images, requiring visual understanding, humor reasoning, creative imagination, and so on. Existing LLM-based approaches rely on reasoning chains or self-improvement, which suffer from limited creativity and interpretability. To address these bottlenecks, we develop a novel LLM-based humor generation mechanism based on a fundamental humor theory, GTVH. To produce funny and script-opposite captions, we introduce a humor-theory-driven multi-role LLM collaboration framework augmented with humor retrieval (HOMER). The framework consists of three LLM-based roles: (1) conflicting-script extractor that grounds humor in key script oppositions, forming the basis of caption generation; (2) retrieval-augmented hierarchical imaginator that identifies key humor targets and expands the creative space of them through diverse associations structured as imagination trees; and (3) caption generator that produces funny and diverse captions conditioned on the obtained knowledge. Extensive experiments on two New Yorker Cartoon benchmarking datasets show that HOMER outperforms state-of-the-art baselines and powerful LLM reasoning strategies on multi-modal humor captioning.

</details>


### [26] [Investigating the structure of emotions by analyzing similarity and association of emotion words](https://arxiv.org/abs/2602.06430)
*Fumitaka Iwaki,Tatsuji Takahashi*

Main category: cs.CL

TL;DR: 通过构建情感词语义网络来检验Plutchik情感轮的有效性，发现网络结构与情感轮大体相似但局部存在差异


<details>
  <summary>Details</summary>
Motivation: Plutchik情感轮是NLP情感分析中常用的情感模型，但其有效性尚未得到充分验证。本研究旨在通过实证方法检验该情感轮的结构有效性。

Method: 通过实验收集情感词对之间的相似性和关联性数据，构建语义网络，然后通过社区检测分析网络结构，并与Plutchik情感轮的结构进行比较。

Result: 构建的网络结构在整体上与Plutchik情感轮相似，但在局部存在差异，表明情感轮模型在宏观层面有效但微观层面需要调整。

Conclusion: Plutchik情感轮在描述情感关系方面具有基本有效性，但实际的情感语义网络结构存在局部差异，为情感模型的改进提供了实证依据。

Abstract: In the field of natural language processing, some studies have attempted sentiment analysis on text by handling emotions as explanatory or response variables. One of the most popular emotion models used in this context is the wheel of emotion proposed by Plutchik. This model schematizes human emotions in a circular structure, and represents them in two or three dimensions. However, the validity of Plutchik's wheel of emotion has not been sufficiently examined. This study investigated the validity of the wheel by creating and analyzing a semantic networks of emotion words. Through our experiments, we collected data of similarity and association of ordered pairs of emotion words, and constructed networks using these data. We then analyzed the structure of the networks through community detection, and compared it with that of the wheel of emotion. The results showed that each network's structure was, for the most part, similar to that of the wheel of emotion, but locally different.

</details>


### [27] [TrailBlazer: History-Guided Reinforcement Learning for Black-Box LLM Jailbreaking](https://arxiv.org/abs/2602.06440)
*Sung-Hoon Yoon,Ruizhi Qian,Minda Zhao,Weiyue Li,Mengyu Wang*

Main category: cs.CL

TL;DR: 提出基于历史感知强化学习的越狱框架，通过分析并重新加权历史漏洞信号来指导攻击决策，显著提高越狱成功率和查询效率


<details>
  <summary>Details</summary>
Motivation: 现有越狱技术未能有效利用先前交互轮次中揭示的漏洞，导致攻击效率低下且不稳定。由于越狱涉及序列交互，强化学习为此问题提供了自然框架

Method: 提出历史感知的强化学习越狱框架，包含基于注意力的重新加权机制，突出交互历史中的关键漏洞，实现更高效的探索

Result: 在AdvBench和HarmBench上的实验表明，该方法实现了最先进的越狱性能，同时显著提高了查询效率

Conclusion: 历史漏洞信号在强化学习驱动的越狱策略中至关重要，为推进LLM安全防护的对抗研究提供了原则性路径

Abstract: Large Language Models (LLMs) have become integral to many domains, making their safety a critical priority. Prior jailbreaking research has explored diverse approaches, including prompt optimization, automated red teaming, obfuscation, and reinforcement learning (RL) based methods. However, most existing techniques fail to effectively leverage vulnerabilities revealed in earlier interaction turns, resulting in inefficient and unstable attacks. Since jailbreaking involves sequential interactions in which each response influences future actions, reinforcement learning provides a natural framework for this problem. Motivated by this, we propose a history-aware RL-based jailbreak framework that analyzes and reweights vulnerability signals from prior steps to guide future decisions. We show that incorporating historical information alone improves jailbreak success rates. Building on this insight, we introduce an attention-based reweighting mechanism that highlights critical vulnerabilities within the interaction history, enabling more efficient exploration with fewer queries. Extensive experiments on AdvBench and HarmBench demonstrate that our method achieves state-of-the-art jailbreak performance while significantly improving query efficiency. These results underscore the importance of historical vulnerability signals in reinforcement learning-driven jailbreak strategies and offer a principled pathway for advancing adversarial research on LLM safeguards.

</details>


### [28] [CORE: Comprehensive Ontological Relation Evaluation for Large Language Models](https://arxiv.org/abs/2602.06446)
*Satyam Dwivedi,Sanjukta Ghosh,Shivam Dwivedi,Nishi Kumari,Anil Thakur,Anurag Purushottam,Deepak Alok,Praveen Gatla,Manjuprasad B,Bipasha Patgiri*

Main category: cs.CL

TL;DR: 论文提出了CORE评估框架，发现LLMs在识别语义无关性方面存在严重缺陷，尽管在相关关系上表现良好，但在无关关系上准确率大幅下降，揭示了LLM语义推理的关键盲点。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估很少关注模型区分有意义语义关系和真正无关性的能力，这构成了LLM评估和安全的关键前沿问题。

Method: 构建了CORE数据集，包含225K多选题覆盖74个学科，以及一个包含203个经过严格验证问题的通用领域基准，涵盖24种语义关系类型，并确保无关对等比例表示。

Result: 29个SOTA LLMs在无关对上准确率仅为0-41.35%，而在相关对上达到86.5-100%；在225K数据集上准确率进一步降至约2%；校准误差在无关对上增加2-4倍，平均语义崩溃率达37.6%。

Conclusion: 无关性推理是LLM评估和安全的关键未充分评估前沿，LLMs在区分语义无关性方面存在系统性缺陷，需要新的评估方法来解决这一盲点。

Abstract: Large Language Models (LLMs) perform well on many reasoning benchmarks, yet existing evaluations rarely assess their ability to distinguish between meaningful semantic relations and genuine unrelatedness. We introduce CORE (Comprehensive Ontological Relation Evaluation), a dataset of 225K multiple-choice questions spanning 74 disciplines, together with a general-domain open-source benchmark of 203 rigorously validated questions (Cohen's Kappa = 1.0) covering 24 semantic relation types with equal representation of unrelated pairs. A human baseline from 1,000+ participants achieves 92.6% accuracy (95.1% on unrelated pairs). In contrast, 29 state-of-the-art LLMs achieve 48.25-70.9% overall accuracy, with near-ceiling performance on related pairs (86.5-100%) but severe degradation on unrelated pairs (0-41.35%), despite assigning similar confidence (92-94%). Expected Calibration Error increases 2-4x on unrelated pairs, and a mean semantic collapse rate of 37.6% indicates systematic generation of spurious relations. On the CORE 225K MCQs dataset, accuracy further drops to approximately 2%, highlighting substantial challenges in domain-specific semantic reasoning. We identify unrelatedness reasoning as a critical, under-evaluated frontier for LLM evaluation and safety.

</details>


### [29] [Evaluating an evidence-guided reinforcement learning framework in aligning light-parameter large language models with decision-making cognition in psychiatric clinical reasoning](https://arxiv.org/abs/2602.06449)
*Xinxin Lin,Guangxin Dai,Yi Zhong,Xiang Li,Xue Xiao,Yixin Zhang,Zhengdong Wu,Yongbo Zheng,Runchuan Zhu,Ming Zhao,Huizi Yu,Shuo Wu,Jun Zhao,Lingming Hu,Yumei Wang,Ping Yin,Joey W. Y. Chan,Ngan Yin Chan,Sijing Chen,Yun Kwok Wing,Lin Lu,Xin Ma,Lizhou Fan*

Main category: cs.CL

TL;DR: ClinMPO是一个强化学习框架，通过专业精神病学实践对齐LLMs的内部推理，使轻量级LLM在复杂精神病学诊断任务上超越医学生表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医疗决策支持中具有变革潜力，但在精神病学应用中受到幻觉和浅层推理的限制。现有训练范式优先考虑语言流畅性而非结构化临床逻辑，导致与专业诊断认知存在根本性错位。

Method: 引入ClinMPO强化学习框架，使用基于4,474篇精神病学期刊文章构建的专业奖励模型，根据循证医学原则对齐LLMs的内部推理与专业精神病学实践。

Result: ClinMPO调优的Qwen3-8B模型在复杂精神病学诊断任务上达到31.4%的准确率，超过了300名医学生30.8%的基准表现。

Conclusion: 医学证据引导的优化使轻量级LLMs能够掌握复杂推理任务，明确的认知对齐为可靠、安全的精神病学决策支持提供了可扩展的途径。

Abstract: Large language models (LLMs) hold transformative potential for medical decision support yet their application in psychiatry remains constrained by hallucinations and superficial reasoning. This limitation is particularly acute in light-parameter LLMs which are essential for privacy-preserving and efficient clinical deployment. Existing training paradigms prioritize linguistic fluency over structured clinical logic and result in a fundamental misalignment with professional diagnostic cognition. Here we introduce ClinMPO, a reinforcement learning framework designed to align the internal reasoning of LLMs with professional psychiatric practice. The framework employs a specialized reward model trained independently on a dataset derived from 4,474 psychiatry journal articles and structured according to evidence-based medicine principles. We evaluated ClinMPO on a unseen subset of the benchmark designed to isolate reasoning capabilities from rote memorization. This test set comprises items where leading large-parameter LLMs consistently fail. We compared the ClinMPO-aligned light LLM performance against a cohort of 300 medical students. The ClinMPO-tuned Qwen3-8B model achieved a diagnostic accuracy of 31.4% and surpassed the human benchmark of 30.8% on these complex cases. These results demonstrate that medical evidence-guided optimization enables light-parameter LLMs to master complex reasoning tasks. Our findings suggest that explicit cognitive alignment offers a scalable pathway to reliable and safe psychiatric decision support.

</details>


### [30] [RelayGen: Intra-Generation Model Switching for Efficient Reasoning](https://arxiv.org/abs/2602.06454)
*Jiwon Song,Yoongon Kim,Jae-Joon Kim*

Main category: cs.CL

TL;DR: RelayGen：无需训练的推理加速框架，通过分析生成不确定性识别推理轨迹中的难度变化，在低难度段切换到小模型执行，保持大模型处理高难度推理，实现2.2倍加速且精度损失小于2%


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在复杂推理任务上表现优异，但生成长推理轨迹时部署成本高。现有效率优化方法要么忽略生成过程中的难度变化，要么依赖监督式token级路由导致系统复杂度过高

Method: 提出RelayGen训练免费、段级运行时模型切换框架：1）通过token概率边际分析生成不确定性；2）识别模型特定切换线索，在低难度段切换到小模型；3）保持大模型处理高难度推理；4）可与推测解码结合使用

Result: 在多个推理基准测试中，RelayGen显著降低推理延迟，同时保持大模型大部分精度。结合推测解码可实现高达2.2倍端到端加速，精度下降小于2%，无需额外训练或学习路由组件

Conclusion: RelayGen通过利用长形式推理中的难度变化，证明了粗粒度段级控制足以捕捉推理轨迹中的难度转换，为高效推理部署提供了简单有效的解决方案

Abstract: Large reasoning models (LRMs) achieve strong performance on complex reasoning tasks by generating long, multi-step reasoning trajectories, but inference-time scaling incurs substantial deployment cost. A key challenge is that generation difficulty varies within a single output, whereas existing efficiency-oriented approaches either ignore this intra-generation variation or rely on supervised token-level routing with high system complexity. We present \textbf{RelayGen}, a training-free, segment-level runtime model switching framework that exploits difficulty variation in long-form reasoning. Through offline analysis of generation uncertainty using token probability margins, we show that coarse-grained segment-level control is sufficient to capture difficulty transitions within a reasoning trajectory. RelayGen identifies model-specific switch cues that signal transitions to lower-difficulty segments and dynamically delegates their continuation to a smaller model, while preserving high-difficulty reasoning on the large model. Across multiple reasoning benchmarks, RelayGen substantially reduces inference latency while preserving most of the accuracy of large models. When combined with speculative decoding, RelayGen achieves up to 2.2$\times$ end-to-end speedup with less than 2\% accuracy degradation, without requiring additional training or learned routing components.

</details>


### [31] [Diffusion-State Policy Optimization for Masked Diffusion Language Models](https://arxiv.org/abs/2602.06462)
*Daisuke Oba,Hiroki Furuta,Naoaki Okazaki*

Main category: cs.CL

TL;DR: DiSPO是一种用于掩码扩散语言模型的信用分配方法，通过优化中间填充决策来改进最终生成质量，无需额外的多步扩散展开。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散语言模型通过多步去噪迭代填充掩码标记生成文本，但仅基于最终完成度的终端奖励进行学习会导致对中间决策的信用分配过于粗糙。

Method: DiSPO在选定的中间掩码状态进行分支：从展开缓存的logits中重新采样当前掩码位置的填充，对生成的完成度进行评分，并仅更新新填充的标记，无需额外的多步扩散展开。

Result: 在LLaDA-8B-Instruct模型上，DiSPO在数学和规划基准测试中，在匹配的展开计算和优化器步骤下，始终优于终端反馈的diffu-GRPO基线。

Conclusion: DiSPO作为一种插件式信用分配层，能够有效优化掩码扩散语言模型的中间填充决策，提高生成质量。

Abstract: Masked diffusion language models generate by iteratively filling masked tokens over multiple denoising steps, so learning only from a terminal reward on the final completion yields coarse credit assignment over intermediate decisions. We propose DiSPO (Diffusion-State Policy Optimization), a plug-in credit-assignment layer that directly optimizes intermediate filling decisions. At selected intermediate masked states, DiSPO branches by resampling fillings for the currently masked positions from rollout-cached logits, scores the resulting completions, and updates only the newly filled tokens -- without additional multi-step diffusion rollouts. We formalize a fixed-state objective for branched completions and derive a policy-gradient estimator that can be combined with terminal-feedback policy optimization using the same rollouts. On LLaDA-8B-Instruct, DiSPO consistently improves over the terminal-feedback diffu-GRPO baseline on math and planning benchmarks under matched rollout compute and optimizer steps. Our code will be available at https://daioba.github.io/dispo .

</details>


### [32] [Improve Large Language Model Systems with User Logs](https://arxiv.org/abs/2602.06470)
*Changyue Wang,Weihang Su,Qingyao Ai,Yiqun Liu*

Main category: cs.CL

TL;DR: UNO是一个从用户日志中学习的统一框架，通过将非结构化日志转化为半结构化规则和偏好对，使用聚类管理数据异质性，并量化模型先验知识与日志数据之间的认知差距，从而自适应过滤噪声并构建不同模块来改进LLM系统。


<details>
  <summary>Details</summary>
Motivation: 当前LLM发展面临高质量数据稀缺和计算成本收益递减的问题，而用户交互日志提供了丰富的真实人类反馈和过程知识。然而，从用户日志学习具有挑战性，因为日志是非结构化和嘈杂的，传统LLM系统难以区分有用信号与噪声，且存在日志收集与模型优化之间的脱节问题。

Method: UNO框架包含三个主要步骤：1) 将用户日志蒸馏为半结构化规则和偏好对；2) 使用查询-反馈驱动的聚类管理数据异质性；3) 量化模型先验知识与日志数据之间的认知差距，指导系统自适应过滤噪声反馈，并构建主要经验和反思经验的不同模块。

Result: 大量实验表明，UNO在效果和效率上都达到了最先进水平，显著优于检索增强生成（RAG）和基于记忆的基线方法。

Conclusion: UNO提供了一个有效的框架，能够从真实世界部署的用户日志中持续学习，解决数据噪声和异质性挑战，为LLM系统的改进提供了新的途径，代码已开源。

Abstract: Scaling training data and model parameters has long driven progress in large language models (LLMs), but this paradigm is increasingly constrained by the scarcity of high-quality data and diminishing returns from rising computational costs. As a result, recent work is increasing the focus on continual learning from real-world deployment, where user interaction logs provide a rich source of authentic human feedback and procedural knowledge. However, learning from user logs is challenging due to their unstructured and noisy nature. Vanilla LLM systems often struggle to distinguish useful feedback signals from noisy user behavior, and the disparity between user log collection and model optimization (e.g., the off-policy optimization problem) further strengthens the problem. To this end, we propose UNO (User log-driveN Optimization), a unified framework for improving LLM systems (LLMsys) with user logs. UNO first distills logs into semi-structured rules and preference pairs, then employs query-and-feedback-driven clustering to manage data heterogeneity, and finally quantifies the cognitive gap between the model's prior knowledge and the log data. This assessment guides the LLMsys to adaptively filter out noisy feedback and construct different modules for primary and reflective experiences extracted from user logs, thereby improving future responses. Extensive experiments show that UNO achieves state-of-the-art effectiveness and efficiency, significantly outperforming Retrieval Augmented Generation (RAG) and memory-based baselines. We have open-sourced our code at https://github.com/bebr2/UNO .

</details>


### [33] [Revisiting the Shape Convention of Transformer Language Models](https://arxiv.org/abs/2602.06471)
*Feng-Ting Liao,Meng-Hsi Chen,Guan-Ting Yi,Da-shan Shiu*

Main category: cs.CL

TL;DR: 论文提出用更深的沙漏形FFN替代传统的窄-宽-窄MLP结构，通过减少FFN参数来增加注意力参数，在固定计算预算下提升语言模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer的FFN采用窄-宽-窄MLP结构，将大部分参数分配给MLP（扩展比2-4）。但最近研究表明残差宽-窄-宽（沙漏形）MLP具有更好的函数逼近能力，因此重新审视这一长期惯例，挑战窄-宽-窄设计的必要性。

Method: 开发了一种Transformer变体，用更深的沙漏形FFN替代传统FFN，该FFN由多个通过残差连接的小沙漏子MLP堆叠而成。采用更轻但更深的沙漏FFN，节省的参数可用于扩大模型隐藏维度或增加注意力参数。

Result: 在400M参数规模下，沙漏FFN优于传统FFN；在1B参数规模下性能相当。在固定计算预算下，减少FFN参数并增加注意力参数的沙漏FFN变体相比传统配置持续改进。

Conclusion: 这些发现为近期研究提供了新视角，促使重新思考窄-宽-窄MLP惯例以及注意力与FFN之间的平衡，有助于构建更高效和表达力强的现代语言模型。

Abstract: Dense Transformer language models have largely adhered to one consistent architectural shape: each layer consists of an attention module followed by a feed-forward network (FFN) with a narrow-wide-narrow MLP, allocating most parameters to the MLP at expansion ratios between 2 and 4. Motivated by recent results that residual wide-narrow-wide (hourglass) MLPs offer superior function approximation capabilities, we revisit the long-standing MLP shape convention in Transformer, challenging the necessity of the narrow-wide-narrow design. To study this, we develop a Transformer variant that replaces the conventional FFN with a deeper hourglass-shaped FFN, comprising a stack of hourglass sub-MLPs connected by residual pathways. We posit that a deeper but lighter hourglass FFN can serve as a competitive alternative to the conventional FFN, and that parameters saved by using a lighter hourglass FFN can be more effectively utilized, such as by enlarging model hidden dimensions under fixed budgets. We confirm these through empirical validations across model scales: hourglass FFNs outperform conventional FFNs up to 400M and achieve comparable performance at larger scales to 1B parameters; hourglass FFN variants with reduced FFN and increased attention parameters show consistent improvements over conventional configurations at matched budgets. Together, these findings shed new light on recent work and prompt a rethinking of the narrow-wide-narrow MLP convention and the balance between attention and FFN towards efficient and expressive modern language models.

</details>


### [34] [Completing Missing Annotation: Multi-Agent Debate for Accurate and Scalable Relevant Assessment for IR Benchmarks](https://arxiv.org/abs/2602.06526)
*Minjeong Ban,Jeonghwan Choi,Hyangsuk Min,Nicole Hee-Yeon Kim,Minseok Kim,Jae-Gil Lee,Hwanjun Song*

Main category: cs.CL

TL;DR: DREAM是一个基于多轮辩论的LLM代理相关性评估框架，通过对立立场和迭代互评提高标注准确性，仅需3.5%人工参与达到95.2%准确率。基于此构建的BRIDGE基准数据集揭示了29,824个缺失相关块，缓解评估偏差。


<details>
  <summary>Details</summary>
Motivation: 信息检索评估面临挑战，因为现有基准数据集存在未标注的相关块。虽然LLM和LLM-人类混合策略减少了人工成本，但仍存在LLM过度自信和AI到人类升级无效的问题。

Method: 提出DREAM框架：基于对立初始立场和迭代互评的多轮辩论式LLM代理相关性评估。通过基于共识的辩论，对确定案例产生更准确标注，对不确定案例实现更可靠的AI到人类升级。

Result: 达到95.2%的标注准确率，仅需3.5%人工参与。构建的BRIDGE基准数据集揭示了29,824个缺失相关块，缓解评估偏差，实现更公平的检索器比较。重新评估IR系统并扩展到RAG评估，显示未解决的漏洞不仅扭曲检索器排名，还导致检索-生成错位。

Conclusion: DREAM框架显著提高了IR评估的准确性和效率，BRIDGE数据集为更公平的检索器比较提供了基础，揭示了现有评估中的系统性偏差。

Abstract: Information retrieval (IR) evaluation remains challenging due to incomplete IR benchmark datasets that contain unlabeled relevant chunks. While LLMs and LLM-human hybrid strategies reduce costly human effort, they remain prone to LLM overconfidence and ineffective AI-to-human escalation. To address this, we propose DREAM, a multi-round debate-based relevance assessment framework with LLM agents, built on opposing initial stances and iterative reciprocal critique. Through our agreement-based debate, it yields more accurate labeling for certain cases and more reliable AI-to-human escalation for uncertain ones, achieving 95.2% labeling accuracy with only 3.5% human involvement. Using DREAM, we build BRIDGE, a refined benchmark that mitigates evaluation bias and enables fairer retriever comparison by uncovering 29,824 missing relevant chunks. We then re-benchmark IR systems and extend evaluation to RAG, showing that unaddressed holes not only distort retriever rankings but also drive retrieval-generation misalignment. The relevance assessment framework is available at https: //github.com/DISL-Lab/DREAM-ICLR-26; and the BRIDGE dataset is available at https://github.com/DISL-Lab/BRIDGE-Benchmark.

</details>


### [35] [MTQE.en-he: Machine Translation Quality Estimation for English-Hebrew](https://arxiv.org/abs/2602.06546)
*Andy Rosenbaum,Assaf Siani,Ilan Kernerman*

Main category: cs.CL

TL;DR: 首个公开的英希机器翻译质量评估基准MTQE.en-he发布，包含959个英文片段及其希伯来语机器翻译，由三位专家标注质量分数。实验表明集成三种模型比最佳单模型提升6.4%皮尔逊和5.6%斯皮尔曼相关系数。


<details>
  <summary>Details</summary>
Motivation: 希伯来语作为资源匮乏语言，缺乏公开可用的机器翻译质量评估基准。该研究旨在填补这一空白，为英希语言对的翻译质量评估研究提供基础资源。

Method: 构建包含959个英文片段及其希伯来语机器翻译的数据集，由三位专家进行直接评估标注。基准测试包括ChatGPT提示、TransQuest和CometKiwi三种方法，并进行模型集成。同时进行TransQuest和CometKiwi的微调实验，比较全模型更新与参数高效方法（LoRA、BitFit、FTHead）。

Result: 集成三种模型比最佳单模型（CometKiwi）在皮尔逊相关系数上提升6.4个百分点，斯皮尔曼相关系数提升5.6个百分点。参数高效微调方法（LoRA、BitFit、FTHead）训练稳定，带来2-3个百分点的改进，而全模型更新容易过拟合和分布崩溃。

Conclusion: MTQE.en-he基准数据集和实验结果将为资源匮乏的英希语言对的机器翻译质量评估研究提供重要支持，促进该领域未来发展。

Abstract: We release MTQE.en-he: to our knowledge, the first publicly available English-Hebrew benchmark for Machine Translation Quality Estimation. MTQE.en-he contains 959 English segments from WMT24++, each paired with a machine translation into Hebrew, and Direct Assessment scores of the translation quality annotated by three human experts. We benchmark ChatGPT prompting, TransQuest, and CometKiwi and show that ensembling the three models outperforms the best single model (CometKiwi) by 6.4 percentage points Pearson and 5.6 percentage points Spearman. Fine-tuning experiments with TransQuest and CometKiwi reveal that full-model updates are sensitive to overfitting and distribution collapse, yet parameter-efficient methods (LoRA, BitFit, and FTHead, i.e., fine-tuning only the classification head) train stably and yield improvements of 2-3 percentage points. MTQE.en-he and our experimental results enable future research on this under-resourced language pair.

</details>


### [36] [Baichuan-M3: Modeling Clinical Inquiry for Reliable Medical Decision-Making](https://arxiv.org/abs/2602.06570)
*Baichuan-M3 Team,:,Chengfeng Dou,Fan Yang,Fei Li,Jiyuan Jia,Qiang Ju,Shuai Wang,Tianpeng Li,Xiangrong Zeng,Yijie Zhou,Hongda Zhang,Jinyang Tai,Linzhuang Sun,Peidong Guo,Yichuan Mo,Xiaochuan Wang,Hengfu Cui,Zhishou Zhang*

Main category: cs.CL

TL;DR: Baichuan-M3是一个医疗增强的大型语言模型，旨在从被动问答转向主动的临床级决策支持，通过专门训练流程模拟医生工作流程，在多个医疗基准测试中表现优于GPT-5.2。


<details>
  <summary>Details</summary>
Motivation: 现有医疗AI系统在开放式咨询中存在局限性，无法有效模拟医生的系统性工作流程，需要从被动问答转向主动的临床决策支持。

Method: 采用专门训练流程模拟医生工作流程，具备三个关键能力：主动信息获取以解决模糊性、长程推理整合分散证据、自适应幻觉抑制确保事实可靠性。

Result: 在HealthBench、新引入的HealthBench-Hallu和ScanBench基准测试中取得最先进结果，在临床咨询、建议和安全性方面显著优于GPT-5.2。

Conclusion: Baichuan-M3成功实现了从被动问答到主动临床决策支持的范式转变，模型已公开可用，为医疗AI提供了更可靠的临床级支持工具。

Abstract: We introduce Baichuan-M3, a medical-enhanced large language model engineered to shift the paradigm from passive question-answering to active, clinical-grade decision support. Addressing the limitations of existing systems in open-ended consultations, Baichuan-M3 utilizes a specialized training pipeline to model the systematic workflow of a physician. Key capabilities include: (i) proactive information acquisition to resolve ambiguity; (ii) long-horizon reasoning that unifies scattered evidence into coherent diagnoses; and (iii) adaptive hallucination suppression to ensure factual reliability. Empirical evaluations demonstrate that Baichuan-M3 achieves state-of-the-art results on HealthBench, the newly introduced HealthBench-Hallu and ScanBench, significantly outperforming GPT-5.2 in clinical inquiry, advisory and safety. The models are publicly available at https://huggingface.co/collections/baichuan-inc/baichuan-m3.

</details>


### [37] [Echoes as Anchors: Probabilistic Costs and Attention Refocusing in LLM Reasoning](https://arxiv.org/abs/2602.06600)
*Zhuoyuan Hao,Zhuo Li,Wu Li,Fangming Liu,Min Zhang,Jing Li*

Main category: cs.CL

TL;DR: 论文提出利用大型推理模型中的"提示回响"现象作为计算分配机制，通过回响似然间隙理论分析和回响蒸馏微调等方法，在数学推理任务上取得优于基线的一致性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有测试时计算分配方法要么注入任务无关的标记，要么采用启发式方法，忽略了大型推理模型内部链中自发的重复现象。本文旨在分析和利用模型倾向于重述问题的"提示回响"现象，作为前置的计算塑造机制。

Method: 1) 将回响移除形式化为基于拒绝的条件化，定义可计算的回响似然间隙ΔL作为理论链接；2) 开发回响蒸馏监督微调(ED-SFT)来注入"先回响后推理"模式；3) 提出回响提示(EP)在不训练的情况下重新锚定模型中间轨迹；4) 进行长度和后缀控制的似然分析以及层间注意力研究。

Result: 在GSM8K、MathQA、Hendrycks-MATH、AIME24和MATH-500等数学推理数据集上，在相同的解码设置和计算预算下，相比基线方法取得了一致的性能提升。注意力分析显示EOP增加了中间层中答案对答案前缀的注意力，符合注意力重新聚焦机制。

Conclusion: 提示回响现象可以作为有效的前置计算分配机制，通过理论分析和相应方法开发，能够显著提升大型推理模型在数学推理任务上的性能，同时提供了解释模型内部注意力重新聚焦的机制。

Abstract: Test-time compute allocation in large reasoning models (LRMs) is widely used and has applications in mathematical problem solving, code synthesis, and planning. Recent work has addressed this problem by scaling self-consistency and parallel thinking, adding generic ``thinking tokens'' and prompting models to re-read the question before answering. Unfortunately, these approaches either inject task-agnostic tokens or mandate heuristics that do not explain -- and often ignore -- the \emph{spontaneous} repetition that many LRMs exhibit at the head of their internal chains. In contrast, we analyze and harness the model's tendency to restate the question, which we term the \emph{Echo of Prompt (EOP)}, as a front-loaded, compute-shaping mechanism. We formalize its probabilistic cost by casting echo removal as rejection-based conditioning and defining the \emph{Echo Likelihood Gap} $Δ\mathcal{L}$ as a computable proxy. This provides the missing theoretical link that links early repetition to likelihood gains and downstream accuracy. However, it does not by itself specify how to exploit EOP. Consequently, we develop \emph{Echo-Distilled SFT (ED-SFT)} to instill an ``echo-then-reason'' pattern through supervised finetuning, and \emph{Echoic Prompting (EP)} to re-ground the model mid-trace without training. While promising, quantifying benefits beyond verbosity is non-trivial. Therefore, we conduct length and suffix-controlled likelihood analyses together with layer-wise attention studies, showing that EOP increases answer to answer-prefix attention in middle layers, consistent with an \emph{attention refocusing} mechanism. We evaluate on GSM8K, MathQA, Hendrycks-MATH, AIME24, and MATH-500 under identical decoding settings and budgets, and find consistent gains over baselines. Code is available at https://github.com/hhh2210/echoes-as-anchors.

</details>


### [38] [Do Prompts Guarantee Safety? Mitigating Toxicity from LLM Generations through Subspace Intervention](https://arxiv.org/abs/2602.06623)
*Himanshu Singh,Ziwei Xu,A. V. Subramanyam,Mohan Kankanhalli*

Main category: cs.CL

TL;DR: 该论文提出了一种针对LLM隐藏毒性模式的子空间干预策略，能在保持生成内容流畅性的同时有效降低毒性，在RealToxicityPrompts上比现有方法降低8-20%毒性。


<details>
  <summary>Details</summary>
Motivation: LLM即使接收看似无害的提示也可能生成有毒或有害内容，这带来了严重的安全挑战。毒性通常微妙且依赖上下文，难以在token级别或通过粗粒度句子信号检测。现有毒性缓解方法常常面临安全性与生成文本连贯性/流畅性之间的权衡问题。

Method: 提出了一种有针对性的子空间干预策略，从底层模型表示中识别和抑制隐藏的毒性模式，同时保持生成安全流畅内容的整体能力。该方法最小化对推理复杂性的影响。

Result: 在RealToxicityPrompts上，相比现有基线方法取得了强大的缓解性能。在多个LLM上，该方法将最先进解毒系统的毒性降低了8-20%，同时保持了可比较的流畅性。通过广泛的定量和定性分析，证明该方法实现了有效的毒性降低而不损害生成性能。

Conclusion: 该子空间干预策略能有效降低LLM生成内容的毒性，同时保持文本的流畅性和生成性能，一致优于现有基线方法，为解决LLM安全性挑战提供了有效方案。

Abstract: Large Language Models (LLMs) are powerful text generators, yet they can produce toxic or harmful content even when given seemingly harmless prompts. This presents a serious safety challenge and can cause real-world harm. Toxicity is often subtle and context-dependent, making it difficult to detect at the token level or through coarse sentence-level signals. Moreover, efforts to mitigate toxicity often face a trade-off between safety and the coherence, or fluency of the generated text. In this work, we present a targeted subspace intervention strategy for identifying and suppressing hidden toxic patterns from underlying model representations, while preserving overall ability to generate safe fluent content. On the RealToxicityPrompts, our method achieves strong mitigation performance compared to existing baselines, with minimal impact on inference complexity. Across multiple LLMs, our approach reduces toxicity of state-of-the-art detoxification systems by 8-20%, while maintaining comparable fluency. Through extensive quantitative and qualitative analyses, we show that our approach achieves effective toxicity reduction without impairing generative performance, consistently outperforming existing baselines.

</details>


### [39] [FairJudge: An Adaptive, Debiased, and Consistent LLM-as-a-Judge](https://arxiv.org/abs/2602.06625)
*Bo Yang,Lanfei Feng,Yunkui Chen,Yu Zhang,Xiao Xu,Shijian Li*

Main category: cs.CL

TL;DR: FairJudge是一个自适应、去偏见、一致的LLM-as-a-Judge系统，通过将评判行为建模为可学习策略，解决现有系统在适应性、偏见和一致性方面的三大局限。


<details>
  <summary>Details</summary>
Motivation: 现有LLM-as-a-Judge系统存在三个根本限制：1) 对任务和领域特定评估标准的适应性有限；2) 由位置、长度、格式、模型来源等非语义线索驱动的系统性偏见；3) 评估不一致性导致不同评估模式（如点式与成对）下的矛盾判断。

Method: 1) 将评判行为建模为可学习和正则化的策略；2) 从数据为中心的角度构建高信息密度评判数据集，显式注入与评估行为对齐的监督信号；3) 采用课程式SFT-DPO-GRPO训练范式，逐步对齐评分标准遵循、偏见缓解和跨模式一致性，避免灾难性遗忘。

Result: 在多个内部和公共基准测试中，FairJudge持续改进一致性和F1分数，减少非语义偏见，并显著优于更大的指令调优LLM。

Conclusion: FairJudge通过将评判行为建模为可学习策略，有效解决了现有LLM-as-a-Judge系统的三大核心问题，实现了自适应、去偏见和一致的评估，所有资源将在接受后公开以促进未来研究。

Abstract: Existing LLM-as-a-Judge systems suffer from three fundamental limitations: limited adaptivity to task- and domain-specific evaluation criteria, systematic biases driven by non-semantic cues such as position, length, format, and model provenance, and evaluation inconsistency that leads to contradictory judgments across different evaluation modes (e.g., pointwise versus pairwise). To address these issues, we propose FairJudge, an adaptive, debiased, and consistent LLM-as-a-Judge. Unlike prior approaches that treat the judge as a static evaluator, FairJudge models judging behavior itself as a learnable and regularized policy. From a data-centric perspective, we construct a high-information-density judging dataset that explicitly injects supervision signals aligned with evaluation behavior. Building on this dataset, we adopt a curriculum-style SFT-DPO-GRPO training paradigm that progressively aligns rubric adherence, bias mitigation, and cross-mode consistency, while avoiding catastrophic forgetting. Experimental results on multiple internal and public benchmarks show that FairJudge consistently improves agreement and F1, reduces non-semantic biases, and outperforms substantially larger instruction-tuned LLMs. All resources will be publicly released after acceptance to facilitate future research.

</details>


### [40] [Reading Between the Waves: Robust Topic Segmentation Using Inter-Sentence Audio Features](https://arxiv.org/abs/2602.06647)
*Steffen Freisinger,Philipp Seeberger,Tobias Bocklet,Korbinian Riedhammer*

Main category: cs.CL

TL;DR: 提出一种多模态主题分割方法，结合文本和音频特征，在YouTube视频数据集上优于纯文本和多模态基线，对ASR噪声更鲁棒


<details>
  <summary>Details</summary>
Motivation: 口语内容（如在线视频和播客）通常涉及多个主题，自动主题分割对用户导航和下游应用至关重要。现有方法未能充分利用音频特征，存在改进空间。

Method: 提出多模态方法，同时微调文本编码器和孪生音频编码器，捕捉句子边界周围的声学线索。

Result: 在YouTube视频大规模数据集上，相比纯文本和多模态基线取得显著提升。模型对ASR噪声更具鲁棒性，在葡萄牙语、德语和英语三个额外数据集上优于更大的纯文本基线。

Conclusion: 学习到的声学特征对鲁棒的主题分割具有重要价值，多模态方法能有效提升主题分割性能。

Abstract: Spoken content, such as online videos and podcasts, often spans multiple topics, which makes automatic topic segmentation essential for user navigation and downstream applications. However, current methods do not fully leverage acoustic features, leaving room for improvement. We propose a multi-modal approach that fine-tunes both a text encoder and a Siamese audio encoder, capturing acoustic cues around sentence boundaries. Experiments on a large-scale dataset of YouTube videos show substantial gains over text-only and multi-modal baselines. Our model also proves more resilient to ASR noise and outperforms a larger text-only baseline on three additional datasets in Portuguese, German, and English, underscoring the value of learned acoustic features for robust topic segmentation.

</details>


### [41] [Beyond Static Alignment: Hierarchical Policy Control for LLM Safety via Risk-Aware Chain-of-Thought](https://arxiv.org/abs/2602.06650)
*Jianfeng Si,Lin Sun,Weihong Lin,Xiangzheng Zhang*

Main category: cs.CL

TL;DR: PACT框架通过分层安全策略和显式风险感知推理，实现LLM的动态安全控制，缓解安全性与有用性之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM面临静态、一刀切的安全策略问题，缺乏运行时可控性，难以适应多样化应用需求，导致对良性请求过度拒绝或对有害请求约束不足。

Method: 提出PACT框架，采用分层策略架构：不可覆盖的全局安全策略建立关键风险的不可变边界，用户定义策略可引入领域特定风险类别和标签到行为的映射。通过结构化分类→行动路径将查询路由到适当操作（遵守、引导或拒绝），使决策过程透明化。

Result: 实验表明，PACT在全局策略评估下达到接近最先进的安全性能，在用户特定策略评估下获得最佳可控性，有效缓解了安全性与有用性之间的权衡问题。

Conclusion: PACT通过动态安全控制和显式风险感知推理，为可控安全对齐提供了有效框架，将发布模型套件、训练数据和评估协议以促进可重复研究。

Abstract: Large Language Models (LLMs) face a fundamental safety-helpfulness trade-off due to static, one-size-fits-all safety policies that lack runtime controllabilityxf, making it difficult to tailor responses to diverse application needs. %As a result, models may over-refuse benign requests or under-constrain harmful ones. We present \textbf{PACT} (Prompt-configured Action via Chain-of-Thought), a framework for dynamic safety control through explicit, risk-aware reasoning. PACT operates under a hierarchical policy architecture: a non-overridable global safety policy establishes immutable boundaries for critical risks (e.g., child safety, violent extremism), while user-defined policies can introduce domain-specific (non-global) risk categories and specify label-to-action behaviors to improve utility in real-world deployment settings. The framework decomposes safety decisions into structured Classify$\rightarrow$Act paths that route queries to the appropriate action (comply, guide, or reject) and render the decision-making process transparent.
  Extensive experiments demonstrate that PACT achieves near state-of-the-art safety performance under global policy evaluation while attaining the best controllability under user-specific policy evaluation, effectively mitigating the safety-helpfulness trade-off. We will release the PACT model suite, training data, and evaluation protocols to facilitate reproducible research in controllable safety alignment.

</details>


### [42] [Not All Layers Need Tuning: Selective Layer Restoration Recovers Diversity](https://arxiv.org/abs/2602.06665)
*Bowen Zhang,Meiyi Wang,Harold Soh*

Main category: cs.CL

TL;DR: 本文提出选择性层恢复(SLR)方法，通过将微调后模型中的特定层恢复为预训练权重，解决指令微调导致的模式崩溃问题，在保持输出质量的同时显著提升多样性。


<details>
  <summary>Details</summary>
Motivation: 指令微调虽然提升了LLMs的指令遵循能力和有用性，但常常会降低生成多样性，导致在开放场景中出现重复输出（模式崩溃）。作者观察到LLM的不同层承担不同功能角色，假设模式崩溃可以定位到特定层，通过恢复这些层到预训练权重可以恢复多样性。

Method: 设计了约束随机字符(CRC)代理任务来验证假设并确定需要恢复的层。基于CRC的发现，提出了选择性层恢复(SLR)方法——一种无需训练的方法，将微调后模型中的选定层恢复为其预训练权重，形成混合模型，不增加推理成本。

Result: 在三个不同任务（创意写作、开放问答、多步推理）和三个模型家族（Llama、Qwen、Gemma）上，SLR能够一致且显著地提高输出多样性，同时保持高输出质量。

Conclusion: SLR是一种有效解决指令微调后模式崩溃问题的方法，通过局部层恢复实现了多样性-质量的平衡，为LLM的微调优化提供了新思路。

Abstract: Post-training improves instruction-following and helpfulness of large language models (LLMs) but often reduces generation diversity, which leads to repetitive outputs in open-ended settings, a phenomenon known as mode collapse. Motivated by evidence that LLM layers play distinct functional roles, we hypothesize that mode collapse can be localized to specific layers and that restoring a carefully chosen range of layers to their pre-trained weights can recover diversity while maintaining high output quality. To validate this hypothesis and decide which layers to restore, we design a proxy task -- Constrained Random Character(CRC) -- with an explicit validity set and a natural diversity objective. Results on CRC reveal a clear diversity-validity trade-off across restoration ranges and identify configurations that increase diversity with minimal quality loss. Based on these findings, we propose Selective Layer Restoration (SLR), a training-free method that restores selected layers in a post-trained model to their pre-trained weights, yielding a hybrid model with the same architecture and parameter count, incurring no additional inference cost. Across three different tasks (creative writing, open-ended question answering, and multi-step reasoning) and three different model families (Llama, Qwen, and Gemma), we find SLR can consistently and substantially improve output diversity while maintaining high output quality.

</details>


### [43] [compar:IA: The French Government's LLM arena to collect French-language human prompts and preference data](https://arxiv.org/abs/2602.06669)
*Lucie Termignon,Simonas Zilinskas,Hadrien Pélissier,Aurélien Barrot,Nicolas Chesnais,Elie Gavoty*

Main category: cs.CL

TL;DR: 法国政府开发的开源平台compar:IA，用于收集大规模法语人类偏好数据，以解决LLM在非英语语言上的性能、文化对齐和安全鲁棒性问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在非英语语言上表现不佳，主要因为英语主导了预训练数据和人类偏好对齐数据集。RLHF和DPO等方法需要人类偏好数据，但英语以外的语言缺乏公开的大规模偏好数据。

Method: 开发compar:IA平台，采用盲对比较界面，收集无约束的真实世界提示和用户判断，保持低参与门槛和隐私保护的自动过滤机制。

Result: 截至2026年2月7日，收集了超过60万个自由形式提示和25万个偏好投票，其中约89%为法语数据。发布了三个互补数据集：对话、投票和反应。

Conclusion: compar:IA不仅为法语LLM对齐提供数据，还发展为国际数字公共产品，为多语言模型训练、评估和人机交互研究提供可重用基础设施。

Abstract: Large Language Models (LLMs) often show reduced performance, cultural alignment, and safety robustness in non-English languages, partly because English dominates both pre-training data and human preference alignment datasets. Training methods like Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) require human preference data, which remains scarce and largely non-public for many languages beyond English. To address this gap, we introduce compar:IA, an open-source digital public service developed inside the French government and designed to collect large-scale human preference data from a predominantly French-speaking general audience. The platform uses a blind pairwise comparison interface to capture unconstrained, real-world prompts and user judgments across a diverse set of language models, while maintaining low participation friction and privacy-preserving automated filtering. As of 2026-02-07, compar:IA has collected over 600,000 free-form prompts and 250,000 preference votes, with approximately 89% of the data in French. We release three complementary datasets -- conversations, votes, and reactions -- under open licenses, and present initial analyses, including a French-language model leaderboard and user interaction patterns. Beyond the French context, compar:IA is evolving toward an international digital public good, offering reusable infrastructure for multilingual model training, evaluation, and the study of human-AI interaction.

</details>


### [44] [Evaluating Prompt Engineering Strategies for Sentiment Control in AI-Generated Texts](https://arxiv.org/abs/2602.06692)
*Kerstin Sahler,Sophie Jentzsch*

Main category: cs.CL

TL;DR: 研究探索如何通过提示工程控制大语言模型生成文本的情感，发现Few-Shot提示最有效，为情感自适应AI系统提供实用方案


<details>
  <summary>Details</summary>
Motivation: 大语言模型为情感自适应AI提供了新机会，但精确控制其情感输出仍具挑战性。现有方法如微调成本高且需要大量数据，因此需要探索更资源友好、易于访问的替代方案

Method: 使用Ekman的六种基本情感（如喜悦、厌恶），研究多种提示工程技术，包括Zero-Shot、Chain-of-Thought提示（使用gpt-3.5-turbo），并与微调方法进行比较

Result: 提示工程能有效引导AI生成文本的情感，为数据受限场景提供了实用且经济高效的替代方案。其中，Few-Shot提示（使用人工编写的示例）在所有技术中效果最佳，可能是因为提供了额外的任务特定指导

Conclusion: 提示工程是控制大语言模型情感输出的有效方法，特别适合资源受限环境。Few-Shot提示表现最优，为开发情感自适应AI系统提供了有价值的见解

Abstract: The groundbreaking capabilities of Large Language Models (LLMs) offer new opportunities for enhancing human-computer interaction through emotion-adaptive Artificial Intelligence (AI). However, deliberately controlling the sentiment in these systems remains challenging. The present study investigates the potential of prompt engineering for controlling sentiment in LLM-generated text, providing a resource-sensitive and accessible alternative to existing methods. Using Ekman's six basic emotions (e.g., joy, disgust), we examine various prompting techniques, including Zero-Shot and Chain-of-Thought prompting using gpt-3.5-turbo, and compare it to fine-tuning. Our results indicate that prompt engineering effectively steers emotions in AI-generated texts, offering a practical and cost-effective alternative to fine-tuning, especially in data-constrained settings. In this regard, Few-Shot prompting with human-written examples was the most effective among other techniques, likely due to the additional task-specific guidance. The findings contribute valuable insights towards developing emotion-adaptive AI systems.

</details>


### [45] [Table-as-Search: Formulate Long-Horizon Agentic Information Seeking as Table Completion](https://arxiv.org/abs/2602.06724)
*Tian Lan,Felix Henry,Bin Zhu,Qianghuai Jia,Junyang Ren,Qihang Pu,Haijun Li,Longyue Wang,Zhao Xu,Weihua Luo*

Main category: cs.CL

TL;DR: TaS将信息搜索任务重构为表格补全任务，通过结构化表格管理搜索状态，显著提升了长视野信息搜索的鲁棒性和性能


<details>
  <summary>Details</summary>
Motivation: 当前信息搜索代理在长视野探索中难以保持专注和连贯性，因为在一个纯文本上下文中跟踪搜索状态（包括规划过程和大量搜索结果）本质上是脆弱的

Method: 提出Table-as-Search框架，将查询映射到外部数据库中的结构化表格模式，行代表搜索候选，列表示约束或所需信息，通过表格精确管理搜索状态

Result: 在三种基准测试中显著优于多个最先进的基线方法，包括多智能体框架和商业系统，验证了在长视野信息搜索中的优越鲁棒性、效率、可扩展性和灵活性

Conclusion: TaS通过结构化表格框架有效解决了信息搜索中的状态跟踪问题，统一了深度搜索、广度搜索和深度广度搜索任务，为长视野信息搜索提供了鲁棒高效的解决方案

Abstract: Current Information Seeking (InfoSeeking) agents struggle to maintain focus and coherence during long-horizon exploration, as tracking search states, including planning procedure and massive search results, within one plain-text context is inherently fragile. To address this, we introduce \textbf{Table-as-Search (TaS)}, a structured planning framework that reformulates the InfoSeeking task as a Table Completion task. TaS maps each query into a structured table schema maintained in an external database, where rows represent search candidates and columns denote constraints or required information. This table precisely manages the search states: filled cells strictly record the history and search results, while empty cells serve as an explicit search plan. Crucially, TaS unifies three distinct InfoSeeking tasks: Deep Search, Wide Search, and the challenging DeepWide Search. Extensive experiments demonstrate that TaS significantly outperforms numerous state-of-the-art baselines across three kinds of benchmarks, including multi-agent framework and commercial systems. Furthermore, our analysis validates the TaS's superior robustness in long-horizon InfoSeeking, alongside its efficiency, scalability and flexibility. Code and datasets are publicly released at https://github.com/AIDC-AI/Marco-Search-Agent.

</details>


### [46] [R-Align: Enhancing Generative Reward Models through Rationale-Centric Meta-Judging](https://arxiv.org/abs/2602.06763)
*Yanlin Lai,Mitt Huang,Hangyu Guo,Xiangfeng Wang,Haodong Li,Shaoxiong Zhan,Liang Zhao,Chengyuan Yao,Yinmin Zhang,Qi Han,Chun Yuan,Zheng Ge,Xiangyu Zhang,Daxin Jiang*

Main category: cs.CL

TL;DR: 论文提出R-Align方法，通过监督奖励模型的推理对齐来提升RLHF效果，发现推理保真度比标签准确度更能预测下游性能


<details>
  <summary>Details</summary>
Motivation: 当前生成式奖励模型(GenRMs)在训练和评估中只关注结果标签，忽视了推理质量。研究发现推理保真度（奖励模型偏好决策与参考决策理由的一致性）对下游RLHF结果有更好的预测性

Method: 提出Rationale-Centric Alignment (R-Align)方法：1) 重新定义现有奖励模型基准中的虚假正确性(S-Corr)；2) 在训练中加入黄金判断并显式监督理由对齐

Result: 实验显示：1) 即使竞争性GenRMs也存在显著的S-Corr；2) 高S-Corr与优化中的策略退化相关；3) R-Align在RM基准上降低S-Corr，在STEM、编程、指令遵循和通用任务上带来一致的性能提升

Conclusion: 推理保真度是评估生成式奖励模型的关键指标，R-Align通过显式监督理由对齐有效提升模型性能，为RLHF提供了更稳健的奖励建模方法

Abstract: Reinforcement Learning from Human Feedback (RLHF) remains indispensable for aligning large language models (LLMs) in subjective domains. To enhance robustness, recent work shifts toward Generative Reward Models (GenRMs) that generate rationales before predicting preferences. Yet in GenRM training and evaluation, practice remains outcome-label-only, leaving reasoning quality unchecked. We show that reasoning fidelity-the consistency between a GenRM's preference decision and reference decision rationales-is highly predictive of downstream RLHF outcomes, beyond standard label accuracy. Specifically, we repurpose existing reward-model benchmarks to compute Spurious Correctness (S-Corr)-the fraction of label-correct decisions with rationales misaligned with golden judgments. Our empirical evaluation reveals substantial S-Corr even for competitive GenRMs, and higher S-Corr is associated with policy degeneration under optimization. To improve fidelity, we propose Rationale-Centric Alignment, R-Align, which augments training with gold judgments and explicitly supervises rationale alignment. R-Align reduces S-Corr on RM benchmarks and yields consistent gains in actor performance across STEM, coding, instruction following, and general tasks.

</details>


### [47] [Generating Data-Driven Reasoning Rubrics for Domain-Adaptive Reward Modeling](https://arxiv.org/abs/2602.06795)
*Kate Sanders,Nathaniel Weir,Sapana Chaudhary,Kaj Bostrom,Huzefa Rangwala*

Main category: cs.CL

TL;DR: 提出一种数据驱动方法构建细粒度推理错误分类法，增强LLM在技术领域的错误检测能力，并用于构建强化学习奖励函数，显著提升模型性能


<details>
  <summary>Details</summary>
Motivation: LLM在推理输出验证中存在困难，特别是在长输出、需要专家知识的领域以及没有可验证奖励的问题中，难以可靠识别思维轨迹中的错误

Method: 采用数据驱动方法自动构建高度细粒度的推理错误分类法（rubrics），利用这些分类法增强LLM驱动的错误检测，并将其转化为强化学习的奖励函数

Result: 基于错误分类法的分类方法在编程、数学和化学工程等技术领域表现出强大的错误识别能力；使用这些奖励函数的模型在困难领域任务准确率比通用LLM-as-judge训练模型提升+45%，接近使用可验证奖励的模型性能，而仅需20%的黄金标签数据

Conclusion: 该方法将奖励分类法从评估定性模型行为扩展到评估定量模型正确性，为在没有完整黄金标签数据集的情况下教授模型解决复杂技术问题开辟了新途径

Abstract: An impediment to using Large Language Models (LLMs) for reasoning output verification is that LLMs struggle to reliably identify errors in thinking traces, particularly in long outputs, domains requiring expert knowledge, and problems without verifiable rewards. We propose a data-driven approach to automatically construct highly granular reasoning error taxonomies to enhance LLM-driven error detection on unseen reasoning traces. Our findings indicate that classification approaches that leverage these error taxonomies, or "rubrics", demonstrate strong error identification compared to baseline methods in technical domains like coding, math, and chemical engineering. These rubrics can be used to build stronger LLM-as-judge reward functions for reasoning model training via reinforcement learning. Experimental results show that these rewards have the potential to improve models' task accuracy on difficult domains over models trained by general LLMs-as-judges by +45%, and approach performance of models trained by verifiable rewards while using as little as 20% as many gold labels. Through our approach, we extend the usage of reward rubrics from assessing qualitative model behavior to assessing quantitative model correctness on tasks typically learned via RLVR rewards. This extension opens the door for teaching models to solve complex technical problems without a full dataset of gold labels, which are often highly costly to procure.

</details>


### [48] [Visual Word Sense Disambiguation with CLIP through Dual-Channel Text Prompting and Image Augmentations](https://arxiv.org/abs/2602.06799)
*Shamik Bhattacharya,Daniel Perkins,Yaren Dogan,Vineeth Konjeti,Sudarshan Srinivasan,Edmon Begoli*

Main category: cs.CL

TL;DR: 本文提出了一种可解释的视觉词义消歧框架，通过CLIP将歧义文本和候选图像投影到共享多模态空间，使用双通道提示和图像增强提升性能，在SemEval-2023数据集上显著提高了MRR和命中率。


<details>
  <summary>Details</summary>
Motivation: 词汇歧义是大型语言模型自然语言理解中的持续挑战，本文旨在探索如何通过视觉领域解决词汇歧义问题，开发可解释的视觉词义消歧框架。

Method: 使用CLIP将歧义文本和候选图像投影到共享多模态空间；通过双通道集成（语义提示和基于照片的提示）结合WordNet同义词丰富文本嵌入；通过鲁棒的测试时增强优化图像嵌入；使用余弦相似度确定与歧义文本最匹配的图像。

Result: 在SemEval-2023 VWSD数据集上，丰富嵌入将MRR从0.7227提升到0.7590，命中率从0.5810提升到0.6220；消融研究表明双通道提示提供强大低延迟性能，而激进的图像增强仅带来边际收益；WordNet定义和多语言提示实验表明噪声外部信号会稀释语义特异性。

Conclusion: 精确的CLIP对齐提示对于视觉词义消歧最有效，双通道提示策略在性能和效率之间取得良好平衡，而过度依赖外部噪声信号会降低语义特异性。

Abstract: Ambiguity poses persistent challenges in natural language understanding for large language models (LLMs). To better understand how lexical ambiguity can be resolved through the visual domain, we develop an interpretable Visual Word Sense Disambiguation (VWSD) framework. The model leverages CLIP to project ambiguous language and candidate images into a shared multimodal space. We enrich textual embeddings using a dual-channel ensemble of semantic and photo-based prompts with WordNet synonyms, while image embeddings are refined through robust test-time augmentations. We then use cosine similarity to determine the image that best aligns with the ambiguous text. When evaluated on the SemEval-2023 VWSD dataset, enriching the embeddings raises the MRR from 0.7227 to 0.7590 and the Hit Rate from 0.5810 to 0.6220. Ablation studies reveal that dual-channel prompting provides strong, low-latency performance, whereas aggressive image augmentation yields only marginal gains. Additional experiments with WordNet definitions and multilingual prompt ensembles further suggest that noisy external signals tend to dilute semantic specificity, reinforcing the effectiveness of precise, CLIP-aligned prompts for visual word sense disambiguation.

</details>


### [49] [The Representational Geometry of Number](https://arxiv.org/abs/2602.06843)
*Zhimin Hu,Lanhao Niu,Sashank Varma*

Main category: cs.CL

TL;DR: 研究发现概念表征通过几何关系而非概念本身实现共享，数字表征在不同任务中保持稳定的关系结构，任务特定表征位于不同子空间但可通过线性映射相互转换。


<details>
  <summary>Details</summary>
Motivation: 认知科学中的一个核心问题是概念表征是收敛到共享流形以支持泛化，还是发散到正交子空间以最小化任务干扰。先前研究发现了两种证据，但缺乏关于这些属性如何共存和跨任务转换的机制解释。

Method: 使用数字概念作为测试平台，语言模型作为高维计算基底，分析数字表征在不同任务中是否保持稳定的关系结构。研究任务特定表征如何嵌入不同子空间，以及这些子空间是否可通过线性映射相互转换。

Result: 数字表征在不同任务中保持稳定的关系结构。任务特定表征嵌入不同的子空间，低层特征如大小和奇偶性编码在可分离的线性方向上。这些子空间主要通过线性映射相互转换，表明表征共享关系结构。

Conclusion: 语言模型通过将任务特定变换应用于概念表征的共享底层关系结构来平衡数字表征的共享结构与功能灵活性。理解产生于对共享关系结构应用任务特定变换。

Abstract: A central question in cognitive science is whether conceptual representations converge onto a shared manifold to support generalization, or diverge into orthogonal subspaces to minimize task interference. While prior work has discovered evidence for both, a mechanistic account of how these properties coexist and transform across tasks remains elusive. We propose that representational sharing lies not in the concepts themselves, but in the geometric relations between them. Using number concepts as a testbed and language models as high-dimensional computational substrates, we show that number representations preserve a stable relational structure across tasks. Task-specific representations are embedded in distinct subspaces, with low-level features like magnitude and parity encoded along separable linear directions. Crucially, we find that these subspaces are largely transformable into one another via linear mappings, indicating that representations share relational structure despite being located in distinct subspaces. Together, these results provide a mechanistic lens of how language models balance the shared structure of number representation with functional flexibility. It suggests that understanding arises when task-specific transformations are applied to a shared underlying relational structure of conceptual representations.

</details>


### [50] [SEMA: Simple yet Effective Learning for Multi-Turn Jailbreak Attacks](https://arxiv.org/abs/2602.06854)
*Mingqian Feng,Xiaodong Liu,Weiwei Yang,Jialin Song,Xuekai Zhu,Chenliang Xu,Jianfeng Gao*

Main category: cs.CL

TL;DR: SEMA是一个用于多轮越狱攻击的简单有效框架，通过预填充自调优和意图漂移感知的强化学习训练攻击者，在多种数据集和受害者模型上实现了最先进的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 多轮越狱攻击代表了安全对齐聊天机器人的真实威胁模型，但现有方法面临探索复杂性和意图漂移问题。需要一种不依赖现有策略或外部数据的有效多轮攻击方法。

Method: SEMA包含两个阶段：1) 预填充自调优：通过微调非拒绝、结构良好的多轮对抗提示来稳定学习；2) 意图漂移感知的强化学习：训练攻击者生成有效的多轮对抗提示，同时保持有害目标。采用开环攻击机制，减少探索复杂性。

Result: 在多个数据集、受害者模型和越狱评判标准下，SEMA实现了最先进的攻击成功率。在AdvBench上对三个闭源和开源受害者模型平均达到80.1%的ASR@1，比现有最佳方法高出33.9%。

Conclusion: SEMA提供了一个紧凑、可复现且可跨目标转移的多轮越狱攻击框架，为大型语言模型安全提供了更强、更现实的压力测试，支持自动红队测试以暴露和定位故障模式。

Abstract: Multi-turn jailbreaks capture the real threat model for safety-aligned chatbots, where single-turn attacks are merely a special case. Yet existing approaches break under exploration complexity and intent drift. We propose SEMA, a simple yet effective framework that trains a multi-turn attacker without relying on any existing strategies or external data. SEMA comprises two stages. Prefilling self-tuning enables usable rollouts by fine-tuning on non-refusal, well-structured, multi-turn adversarial prompts that are self-generated with a minimal prefix, thereby stabilizing subsequent learning. Reinforcement learning with intent-drift-aware reward trains the attacker to elicit valid multi-turn adversarial prompts while maintaining the same harmful objective. We anchor harmful intent in multi-turn jailbreaks via an intent-drift-aware reward that combines intent alignment, compliance risk, and level of detail. Our open-loop attack regime avoids dependence on victim feedback, unifies single- and multi-turn settings, and reduces exploration complexity. Across multiple datasets, victim models, and jailbreak judges, our method achieves state-of-the-art (SOTA) attack success rates (ASR), outperforming all single-turn baselines, manually scripted and template-driven multi-turn baselines, as well as our SFT (Supervised Fine-Tuning) and DPO (Direct Preference Optimization) variants. For instance, SEMA performs an average $80.1\%$ ASR@1 across three closed-source and open-source victim models on AdvBench, 33.9% over SOTA. The approach is compact, reproducible, and transfers across targets, providing a stronger and more realistic stress test for large language model (LLM) safety and enabling automatic redteaming to expose and localize failure modes. Our code is available at: https://github.com/fmmarkmq/SEMA.

</details>


### [51] [Uncovering Cross-Objective Interference in Multi-Objective Alignment](https://arxiv.org/abs/2602.06869)
*Yining Lu,Meng Jiang*

Main category: cs.CL

TL;DR: 该论文研究了多目标对齐中的交叉目标干扰现象，提出了一种基于协方差分析的CTWA方法来缓解干扰，并进行了局部和全局收敛性分析。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型多目标对齐中存在的持续失败模式：训练只改善部分目标性能，同时导致其他目标性能下降。这种现象被称为交叉目标干扰，需要系统性地理解和解决。

Method: 1. 形式化交叉目标干扰现象并进行系统性研究；2. 推导局部协方差定律，分析目标改进条件；3. 将分析扩展到现代对齐中使用的裁剪代理目标；4. 提出协方差目标权重适应（CTWA）方法；5. 在Polyak-Łojasiewicz条件下进行全局收敛分析。

Result: 研究发现交叉目标干扰是普遍存在的且具有强烈的模型依赖性。提出的CTWA方法能有效缓解干扰，通过维持目标奖励与训练信号之间的正协方差来改善多目标对齐效果。

Conclusion: 交叉目标干扰是多目标对齐中的关键问题，通过协方差分析可以理解其机制，CTWA方法能有效缓解干扰，全局收敛分析揭示了非凸标量化优化的收敛条件与模型几何特性的关系。

Abstract: We study a persistent failure mode in multi-objective alignment for large language models (LLMs): training improves performance on only a subset of objectives while causing others to degrade. We formalize this phenomenon as cross-objective interference and conduct the first systematic study across classic scalarization algorithms, showing that interference is pervasive and exhibits strong model dependence.
  To explain this phenomenon, we derive a local covariance law showing that an objective improves at first order when its reward exhibits positive covariance with the scalarized score. We extend this analysis to clipped surrogate objectives used in modern alignment, demonstrating that the covariance law remains valid under mild conditions despite clipping. Building on this analysis, we propose Covariance Targeted Weight Adaptation (CTWA), a plug-and-play method that maintains positive covariance between objective rewards and the training signal to effectively mitigate cross-objective interference. Finally, we complement these local improvement conditions with a global convergence analysis under the Polyak--Łojasiewicz condition, establishing when non-convex scalarized optimization achieves global convergence and how cross-objective interference depends on specific model geometric properties.

</details>


### [52] [Halluverse-M^3: A multitask multilingual benchmark for hallucination in LLMs](https://arxiv.org/abs/2602.06920)
*Samir Abdaljalil,Parichit Sharma,Erchin Serpedin,Hasan Kurban*

Main category: cs.CL

TL;DR: Halluverse-M^3是一个多语言、多任务、多幻觉类别的数据集，用于系统分析大语言模型在四种语言（英语、阿拉伯语、印地语、土耳其语）和两种生成任务（问答和对话摘要）中的幻觉问题，区分实体级、关系级和句子级幻觉。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多语言和生成式场景中的幻觉问题仍然是一个持续挑战，特别是现有模型主要针对英语基准测试表现良好，但在不同语言、任务和幻觉类型上的行为尚未得到充分理解。

Method: 构建Halluverse-M^3数据集，涵盖四种语言和两种生成任务，通过受控编辑过程构建幻觉输出并由人工标注者验证，确保原始内容与幻觉生成之间的清晰对齐。

Result: 评估结果显示：问答任务比对话摘要更容易检测幻觉；句子级幻觉即使对最强模型也具有挑战性；英语表现最佳，低资源语言（特别是印地语）检测准确率下降。

Conclusion: Halluverse-M^3为研究多语言、多任务环境中的幻觉问题提供了一个现实且具有挑战性的基准，数据集已公开发布以支持未来的幻觉检测和缓解研究。

Abstract: Hallucinations in large language models remain a persistent challenge, particularly in multilingual and generative settings where factual consistency is difficult to maintain. While recent models show strong performance on English-centric benchmarks, their behavior across languages, tasks, and hallucination types is not yet well understood. In this work, we introduce Halluverse-M^3, a dataset designed to enable systematic analysis of hallucinations across multiple languages, multiple generation tasks, and multiple hallucination categories. Halluverse-M^3 covers four languages, English, Arabic, Hindi, and Turkish, and supports two generation tasks: question answering and dialogue summarization. The dataset explicitly distinguishes between entity-level, relation-level, and sentence-level hallucinations. Hallucinated outputs are constructed through a controlled editing process and validated by human annotators, ensuring clear alignment between original content and hallucinated generations. Using this dataset, we evaluate a diverse set of contemporary open-source and proprietary language models on fine-grained hallucination detection. Our results show that question answering is consistently easier than dialogue summarization, while sentence-level hallucinations remain challenging even for the strongest models. Performance is highest in English and degrades in lower-resource languages, with Hindi exhibiting the lowest detection accuracy. Overall, Halluverse-M^3 provides a realistic and challenging benchmark for studying hallucinations in multilingual, multi-task settings. We release the dataset to support future research on hallucination detection and mitigation\footnote{https://huggingface.co/datasets/sabdalja/HalluVerse-M3}.

</details>


### [53] [Optimal Turkish Subword Strategies at Scale: Systematic Evaluation of Data, Vocabulary, Morphology Interplay](https://arxiv.org/abs/2602.06942)
*Duygu Altinok*

Main category: cs.CL

TL;DR: 该论文首次对土耳其语等形态丰富语言进行了全面的子词分词系统研究，提出了"子词宣言"，系统分析了词汇量-语料库-性能三者关系，并开发了形态感知的诊断工具包。


<details>
  <summary>Details</summary>
Motivation: 土耳其语等形态丰富语言中，高产的粘着形态变化对词汇效率和形态保真度都构成挑战。先前研究存在三个主要问题：词汇量变化时未系统控制分词器训练语料、内在诊断有限、下游任务评估范围狭窄。

Method: 提出了"子词宣言"研究方法：联合变化词汇量和分词器训练语料规模，在匹配参数预算下比较多种分词器家族（WordPiece、形态级别、字符基线），并在语义、句法和形态敏感任务上进行评估。开发了形态感知诊断工具包，包含边界级F1、词干原子性、过/欠分割指数、字符/词编辑距离、延续率、词缀类型覆盖度等指标。

Result: 系统研究了词汇量-语料库-性能三者关系，建立了内在诊断与外在结果的联系框架，确定了字符级和形态级分词的优势场景，为形态丰富语言构建有效分词器提供了可操作指导。

Conclusion: 该研究首次对土耳其语等形态丰富语言进行了全面的子词分词系统研究，提出的"子词宣言"为构建有效分词器提供了可操作指导，并为未来研究建立了可复现的基础。开源了评估代码、分词器流水线和模型。

Abstract: Tokenization is a pivotal design choice for neural language modeling in morphologically rich languages (MRLs) such as Turkish, where productive agglutination challenges both vocabulary efficiency and morphological fidelity. Prior studies have explored tokenizer families and vocabulary sizes but typically (i) vary vocabulary without systematically controlling the tokenizer's training corpus, (ii) provide limited intrinsic diagnostics, and (iii) evaluate a narrow slice of downstream tasks. We present the first comprehensive, principled study of Turkish subword tokenization; a "subwords manifest", that jointly varies vocabulary size and tokenizer training corpus size (data and vocabulary coupling), compares multiple tokenizer families under matched parameter budgets (WordPiece, morphology level, and character baselines), and evaluates across semantic (NLI, STS, sentiment analysis, NER), syntactic (POS, dependency parsing), and morphology-sensitive probes. To explain why tokenizers succeed or fail, we introduce a morphology-aware diagnostic toolkit that goes beyond coarse aggregates to boundary-level micro/macro F1, decoupled lemma atomicity vs. surface boundary hits, over/under-segmentation indices, character/word edit distances (CER/WER), continuation rates, and affix-type coverage and token-level atomicity. Our contributions are fourfold: (i) a systematic investigation of the vocabulary-corpus-success triad; (ii) a unified, morphology-aware evaluation framework linking intrinsic diagnostics to extrinsic outcomes; (iii) controlled comparisons identifying when character-level and morphology-level tokenization pay off; and (iv) an open-source release of evaluation code, tokenizer pipelines, and models. As the first work of its kind, this "subwords manifest" delivers actionable guidance for building effective tokenizers in MRLs and establishes a reproducible foundation for future research.

</details>


### [54] [DAWN: Dependency-Aware Fast Inference for Diffusion LLMs](https://arxiv.org/abs/2602.06953)
*Lizhuo Luo,Zhuoran Shi,Jiajun Luo,Zhi Wang,Shen Ren,Wenya Wang,Tianwei Zhang*

Main category: cs.CL

TL;DR: DAWN是一种无需训练、依赖感知的解码方法，用于加速扩散大语言模型推理，通过建模token依赖关系实现高效并行解码，在保持生成质量的同时获得1.80-8.06倍的加速。


<details>
  <summary>Details</summary>
Motivation: 现有扩散大语言模型的并行解码策略受限于质量-速度权衡，采用保守的并行策略，未能充分利用效率潜力。核心挑战在于并行解码假设各位置可独立填充，但实际上token之间存在语义耦合，忽略这些依赖关系会导致输出质量下降。

Method: DAWN提取token依赖关系并构建依赖图，基于两个关键观察：(1)依赖于已确定位置的token更可靠；(2)同时解码强耦合的不确定位置容易产生错误。利用依赖图在每次迭代中选择更可靠的解码位置，实现高并行度。

Result: 在多个模型和数据集上的实验表明，DAWN相比基线方法实现了1.80-8.06倍的推理加速，同时保持了生成质量。该方法无需额外训练，可直接应用于现有模型。

Conclusion: DAWN通过建模token依赖关系解决了扩散大语言模型并行解码中的质量-速度权衡问题，提供了一种高效且保持质量的推理加速方案，为扩散模型的实用化部署提供了有力工具。

Abstract: Diffusion large language models (dLLMs) have shown advantages in text generation, particularly due to their inherent ability for parallel decoding. However, constrained by the quality--speed trade-off, existing inference solutions adopt conservative parallel strategies, leaving substantial efficiency potential underexplored. A core challenge is that parallel decoding assumes each position can be filled independently, but tokens are often semantically coupled. Thus, the correct choice at one position constrains valid choices at others. Without modeling these inter-token dependencies, parallel strategies produce deteriorated outputs. Motivated by this insight, we propose DAWN, a training-free, dependency-aware decoding method for fast dLLM inference. DAWN extracts token dependencies and leverages two key motivations: (1) positions dependent on unmasked certain positions become more reliable, (2) simultaneously unmasking strongly coupled uncertain positions induces errors. Given those findings, DAWN leverages a dependency graph to select more reliable unmasking positions at each iteration, achieving high parallelism with negligible loss in generation quality. Extensive experiments across multiple models and datasets demonstrate that DAWN speedups the inference by 1.80-8.06x over baselines while preserving the generation quality. Code is released at https://github.com/lizhuo-luo/DAWN.

</details>


### [55] [InftyThink+: Effective and Efficient Infinite-Horizon Reasoning via Reinforcement Learning](https://arxiv.org/abs/2602.06960)
*Yuchen Yan,Liang Jiang,Jin Jiang,Shuaicheng Li,Zujie Wen,Zhiqiang Zhang,Jun Zhou,Jian Shao,Yueting Zhuang,Yongliang Shen*

Main category: cs.CL

TL;DR: InftyThink+ 是一个基于强化学习的迭代推理框架，通过优化何时总结、保留什么信息以及如何继续推理，显著提升推理效率与准确性，同时降低推理延迟。


<details>
  <summary>Details</summary>
Motivation: 传统链式推理存在二次方计算成本、上下文长度限制和"中间迷失"效应等问题。现有迭代推理方法依赖监督学习或固定启发式规则，无法优化总结时机、信息保留和推理恢复等关键决策。

Method: 提出 InftyThink+ 端到端强化学习框架，采用两阶段训练：监督学习冷启动后接轨迹级强化学习，让模型学习战略性总结和继续推理的决策，基于模型控制的迭代边界和显式总结机制。

Result: 在 DeepSeek-R1-Distill-Qwen-1.5B 上，AIME24 准确率提升 21%，优于传统长链式推理强化学习方法，在分布外基准测试中泛化能力更强，同时显著降低推理延迟并加速强化学习训练。

Conclusion: InftyThink+ 通过强化学习优化迭代推理轨迹，在提升推理性能的同时提高了推理效率，为解决大规模推理模型的成本、长度和"中间迷失"问题提供了有效方案。

Abstract: Large reasoning models achieve strong performance by scaling inference-time chain-of-thought, but this paradigm suffers from quadratic cost, context length limits, and degraded reasoning due to lost-in-the-middle effects. Iterative reasoning mitigates these issues by periodically summarizing intermediate thoughts, yet existing methods rely on supervised learning or fixed heuristics and fail to optimize when to summarize, what to preserve, and how to resume reasoning. We propose InftyThink+, an end-to-end reinforcement learning framework that optimizes the entire iterative reasoning trajectory, building on model-controlled iteration boundaries and explicit summarization. InftyThink+ adopts a two-stage training scheme with supervised cold-start followed by trajectory-level reinforcement learning, enabling the model to learn strategic summarization and continuation decisions. Experiments on DeepSeek-R1-Distill-Qwen-1.5B show that InftyThink+ improves accuracy by 21% on AIME24 and outperforms conventional long chain-of-thought reinforcement learning by a clear margin, while also generalizing better to out-of-distribution benchmarks. Moreover, InftyThink+ significantly reduces inference latency and accelerates reinforcement learning training, demonstrating improved reasoning efficiency alongside stronger performance.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [56] [Dynamic Quantum Optimal Communication Topology Design for Consensus Control in Linear Multi-Agent Systems](https://arxiv.org/abs/2602.06215)
*Milad Hasanzadeh,Amin Kargarian*

Main category: eess.SY

TL;DR: 提出量子框架优化多智能体系统通信拓扑，通过ADMM分解MIQP问题，用量子算法求解二进制子问题，生成时变Laplacian矩阵用于共识控制。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体系统通信拓扑设计面临组合优化问题的NP-hard复杂性，需要开发能处理大规模问题的量子计算方法，将量子算法嵌入分布式控制架构。

Method: 提出三块ADMM方案分解MIQP：1) 松弛边和流变量的凸二次规划；2) 纯二进制无约束子问题映射为QUBO哈密顿量，用量子虚时演化近似求解；3) 闭式辅助更新。生成的时变Laplacian应用于一阶和二阶线性共识动态。

Result: 数值模拟显示该方法能生成满足连通性和度约束的拓扑，实现共识，成本与传统混合整数求解器相当，验证了量子算法作为拓扑优化器在闭环分布式控制中的可行性。

Conclusion: 成功展示了量子算法如何作为拓扑优化器嵌入闭环分布式控制架构，为处理大规模多智能体系统通信拓扑设计的组合优化问题提供了量子计算解决方案。

Abstract: This paper proposes a quantum framework for the design of communication topologies in consensus-based multi-agent systems. The communication graph is selected online by solving a mixed-integer quadratic program (MIQP) that minimizes a cost combining communication and distance penalties with degree-regularization terms, while enforcing exact connectivity through a flow-based formulation. To cope with the combinatorial complexity of this NP-hard problem, we develop a three-block ADMM scheme that decomposes the MIQP into a convex quadratic program in relaxed edge and flow variables, a pure binary unconstrained subproblem, and a closed-form auxiliary update. The binary subproblem is mapped to a quadratic unconstrained binary optimization (QUBO) Hamiltonian and approximately solved via quantum imaginary time evolution (QITE). The resulting time-varying, optimizer-generated Laplacians are applied to linear first- and second-order consensus dynamics. Numerical simulations on networks demonstrate that the proposed method produces connected topologies that satisfy degree constraints, achieve consensus, and incur costs comparable to those of classical mixed-integer solvers, thereby illustrating how quantum algorithms can be embedded as topology optimizers within closed-loop distributed control architectures.

</details>


### [57] [A hard-constrained NN learning framework for rapidly restoring AC-OPF from DC-OPF](https://arxiv.org/abs/2602.06255)
*Kejun Chen,Bernard Knueven,Wesley Jones*

Main category: eess.SY

TL;DR: 提出硬约束无监督学习框架，快速求解非线性非凸交流最优潮流问题，无需真实解数据，通过残差学习和可微优化层确保可行性与最优性。


<details>
  <summary>Details</summary>
Motivation: 传统交流最优潮流求解计算量大，难以满足实时运行需求。现有学习方法需要大量真实解数据，且难以保证解的可行性与最优性。

Method: 1) 基于残差学习，神经网络学习从直流最优潮流解到发电机有功功率设定值的修正映射；2) 后续优化模型恢复交流最优潮流解；3) 投影差异作为训练损失；4) 使用回放缓冲区提升学习效率；5) 将优化模型构建为可微优化层，通过KKT条件隐函数定理求梯度。

Result: 在IEEE-118和PEGASE-9241母线系统测试，神经网络获得严格可行且接近最优的解，计算时间比传统优化求解器减少40倍，平均约束违反量约10^-4，优化间隙低于1%。

Conclusion: 提出的硬约束无监督学习框架能快速求解交流最优潮流问题，无需真实解数据，确保解的可行性与最优性，显著提升计算效率，适用于实时运行。

Abstract: This paper proposes a hard-constrained unsupervised learning framework for rapidly solving the non-linear and non-convex AC optimal power flow (AC-OPF) problem in real-time operation. Without requiring ground-truth AC-OPF solutions, feasibility and optimality are ensured through a properly designed learning environment and training loss. Inspired by residual learning, the neural network (NN) learns the correction mapping from the DC-OPF solution to the active power setpoints of the generators through re-dispatch. A subsequent optimization model is utilized to restore the optimal AC-OPF solution, and the resulting projection difference is employed as the training loss. A replay buffer is utilized to enhance learning efficiency by fully leveraging past data pairs. The optimization model is cast as a differentiable optimization layer, where the gradient is derived by applying the implicit function theorem to the KKT conditions at the optimal solution. Tested on IEEE-118 and PEGASE-9241 bus systems, numerical results demonstrate that the proposed NN can obtain strictly feasible and near-optimal solutions with reduced computational time compared to conventional optimization solvers. In addition, aided by the updated DC-OPF solution under varying topologies, the trained NN, together with the PF solver, can rapidly find the corresponding AC solution. The proposed method achieves a $40\times$ time speedup, while maintaining an average constraint violation on the order of $10^{-4}$ and an optimization gap below $1\%$.

</details>


### [58] [Advances in Battery Energy Storage Management: Control and Economic Synergies](https://arxiv.org/abs/2602.06365)
*Venkata Rajesh Chundru,Shreshta Rajakumar Deshpande,Stanislav A Gankov*

Main category: eess.SY

TL;DR: 本文综述了电池储能系统（BESS）的文献，重点关注经济与运营维度的协同，探讨如何通过数字孪生技术优化电网稳定性和收益潜力。


<details>
  <summary>Details</summary>
Motivation: 现有BESS文献主要关注电网稳定性控制设计和电网调度的技术经济分析，但随着辅助服务在电网中的增加，需要更全面的能源管理系统，既要优化BESS收益，又要确保锂离子电池的安全高效运行。

Method: 通过文献综述，将研究分为五个关键类别：BESS辅助服务、实时功率流控制系统、BESS调度优化算法、BESS技术经济分析、以及BESS数字孪生技术。

Result: 该综述将识别潜在协同效应、研究空白和新兴趋势，为BESS管理和部署策略的未来创新铺平道路。

Conclusion: 需要探索电网任务周期的经济方面与BESS系统控制方案之间的协同，这种协同有助于创建增强电网稳定性和收益潜力的稳健数字孪生虚拟表示。

Abstract: The existing literature on Battery Energy Storage Systems (BESS) predominantly focuses on two main areas: control system design aimed at achieving grid stability and the techno-economic analysis of BESS dispatch on power grid. However, with the increasing incorporation of ancillary services into power grids, a more comprehensive approach to energy management systems is required. Such an approach should not only optimize revenue generation from BESS but also ensure the safe, efficient, and reliable operation of lithium-ion batteries. This research seeks to bridge this gap by exploring literature that addresses both the economic and operational dimensions of BESS. Specifically, it examines how economic aspects of grid duty cycles can align with control schemes deployed in BESS systems. This alignment, or synergy, could be instrumental in creating robust digital twins virtual representations of BESS systems that enhance both grid stability and revenue potential.
  The literature review is organized into five key categories: (1) ancillary services for BESS, exploring support functions that BESS can provide to power grids; (2) control systems developed for real-time BESS power flow management, ensuring smooth operations under dynamic grid conditions; (3) optimization algorithms for BESS dispatch, focusing on efficient energy allocation strategies; (4) techno-economic analyses of BESS and battery systems to assess their financial viability; and (5) digital twin technologies for real-world BESS deployments, enabling advanced predictive maintenance and performance optimization. This review will identify potential synergies, research gaps, and emerging trends, paving the way for future innovations in BESS management and deployment strategies.

</details>


### [59] [Safety Controller Synthesis for Stochastic Polynomial Time-Delayed Systems](https://arxiv.org/abs/2602.06569)
*Omid Akbarzadeh,MohammadHossein Ashoori,Amy Nejati,Abolfazl Lavaei*

Main category: eess.SY

TL;DR: 提出一种针对带时滞的离散时间随机非线性多项式系统的安全控制器合成理论框架，使用Krasovskii控制障碍证书处理时滞影响，确保系统在概率界限内保持安全。


<details>
  <summary>Details</summary>
Motivation: 虽然使用控制障碍证书进行随机系统安全分析已被广泛研究，但针对带有时滞的随机系统开发安全控制器仍未被充分探索。主要挑战在于需要考虑时滞分量对安全条件的影响。

Method: 采用Krasovskii控制障碍证书，扩展传统CBC框架，增加捕获时滞状态影响的求和项。将当前和时滞分量整合到统一的障碍结构中，通过平方和优化程序重新表述安全约束，系统构建Krasovskii CBC及相关安全控制器。

Result: 提出的方法在输入约束下合成安全控制器，提供对时滞具有鲁棒性的概率安全保证：确保所有dt-SNPS-td轨迹保持在规定安全区域内，同时满足量化概率界限。通过三个案例研究验证了框架的有效性和实际适用性。

Conclusion: 该工作成功开发了针对带时滞离散时间随机非线性多项式系统的安全控制器合成理论框架，通过Krasovskii控制障碍证书有效处理时滞影响，为这类系统提供了系统化的安全保证方法。

Abstract: This work develops a theoretical framework for safety controller synthesis in discrete-time stochastic nonlinear polynomial systems subject to time-invariant delays (dt-SNPS-td). While safety analysis of stochastic systems using control barrier certificates (CBC) has been widely studied, developing safety controllers for stochastic systems with time delays remains largely unexplored. The main challenge arises from the need to account for the influence of delayed components when formulating and enforcing safety conditions. To address this, we employ Krasovskii control barrier certificates, which extend the conventional CBC framework by augmenting it with an additional summation term that captures the influence of delayed states. This formulation integrates both the current and delayed components into a unified barrier structure, enabling safety synthesis for stochastic systems with time delays. The proposed approach synthesizes safety controllers under input constraints, offering probabilistic safety guarantees robust to such delays: it ensures that all trajectories of the dt-SNPS-td remain within the prescribed safe region while fulfilling a quantified probabilistic bound. To achieve this, our method reformulates the safety constraints as a sum-of-squares optimization program, enabling the systematic construction of Krasovskii CBC together with their associated safety controllers. We validate the proposed framework through three case studies, including two physical systems, demonstrating its effectiveness and practical applicability.

</details>


### [60] [Structured Learning for Electromagnetic Field Modeling and Real-Time Inversion](https://arxiv.org/abs/2602.06618)
*Antonio Bernardes,Jasan Zughaibi,Michael Muehlebach,Bradley J. Nelson*

Main category: eess.SY

TL;DR: 提出基于多层感知机的磁场建模新方法，替代传统多极展开模型，在保持电流线性依赖的同时实现快速闭环控制。


<details>
  <summary>Details</summary>
Motivation: 传统多极展开模型(MPEM)依赖严格的物理假设（源对称性和隔离性），需要基于优化的校准且对初始化高度敏感，限制了其在复杂或不规则线圈几何系统中的应用。

Method: 采用基于多层感知机的数据驱动建模方法，学习非线性磁场映射，同时严格保持对电流的线性依赖关系，从而实现快速闭式最小范数反演。

Result: 数据驱动模型达到与MPEM相当的预测保真度，同时保持可比的数据效率，评估时间约1毫秒，适合高带宽磁控制，并能有效消除MPEM校准中常见的病态问题。

Conclusion: 提出的数据驱动建模方法为复杂电磁导航系统提供了有效的替代方案，在保持物理约束的同时实现了快速计算，并开源了完整代码库和数据集。

Abstract: Precise magnetic field modeling is fundamental to the closed-loop control of electromagnetic navigation systems (eMNS) and the analytical Multipole Expansion Model (MPEM) is the current standard. However, the MPEM relies on strict physical assumptions regarding source symmetry and isolation, and requires optimization-based calibration that is highly sensitive to initialization. These constraints limit its applicability to systems with complex or irregular coil geometries. This work introduces an alternative modeling paradigm based on multi-layer perceptrons that learns nonlinear magnetic mappings while strictly preserving the linear dependence on currents. As a result, the field models enable fast, closed-form minimum-norm inversion with evaluation times of approximately 1 ms, which is critical for high-bandwidth magnetic control. For model training and evaluation we use large-scale, high-density datasets collected from the research-grade OctoMag and clinical-grade Navion systems. Our results demonstrate that data-driven models achieve predictive fidelity equivalent to the MPEM while maintaining comparable data efficiency. Furthermore, we demonstrate that straightforward design choices effectively eliminate spurious workspace ill-conditioning frequently reported in MPEM-based calibration. To facilitate future research, we release the complete codebase and datasets open source.

</details>


### [61] [Efficient and Robust Modeling of Nonlinear Mechanical Systems](https://arxiv.org/abs/2602.06639)
*Davide Tebaldi,Roberto Zanasi*

Main category: eess.SY

TL;DR: 提出一种适用于汽车和机器人系统的非线性机械系统动态模型新公式，相比欧拉-拉格朗日公式在测量噪声鲁棒性和逆动力学计算速度方面表现更优


<details>
  <summary>Details</summary>
Motivation: 开发高效鲁棒的动态模型是系统与控制工程的基础，现有欧拉-拉格朗日公式在外部变量依赖系统的测量噪声鲁棒性和计算效率方面存在不足

Method: 提出新的非线性机械系统动态模型公式，并开发自动获取模型公式的建模程序，适用于不同汽车和机器人案例研究

Result: 相比欧拉-拉格朗日公式，新模型在外部变量依赖系统的测量噪声鲁棒性方面表现更优，且在计算系统逆动力学时的执行时间更短

Conclusion: 提出的新动态模型公式在鲁棒性和计算效率方面优于传统欧拉-拉格朗日方法，为汽车和机器人系统提供了更有效的建模工具

Abstract: The development of efficient and robust dynamic models is fundamental in the field of systems and control engineering. In this paper, a new formulation for the dynamic model of nonlinear mechanical systems, that can be applied to different automotive and robotic case studies, is proposed, together with a modeling procedure allowing to automatically obtain the model formulation. Compared with the Euler-Lagrange formulation, the proposed model is shown to give superior performances in terms of robustness against measurement noise for systems exhibiting dependence on some external variables, as well as in terms of execution time when computing the inverse dynamics of the system.

</details>


### [62] [UnifSrv: AP Selection for Achieving Uniformly Good Performance of CF-MIMO in Realistic Urban Networks](https://arxiv.org/abs/2602.06780)
*Yunlu Xiao,Marina Petrova,Ljiljana Simić*

Main category: eess.SY

TL;DR: 该论文提出UnifSrv算法，通过多目标优化解决非均匀传播环境下可扩展无蜂窝大规模MIMO的AP选择问题，实现高吞吐量、均匀覆盖和低复杂度。


<details>
  <summary>Details</summary>
Motivation: 在现实非均匀城市传播环境中，传统可扩展无蜂窝大规模MIMO难以选择优质服务AP集合，导致吞吐量显著下降，重新出现"边缘效应"，需要恢复均匀性能。

Method: 提出多目标优化问题，联合最大化总数据速率、最大化Jain公平指数、最小化服务AP集合规模；开发UnifSrv算法，包括基于深度强化学习的UnifSrv-DRL和启发式算法UnifSrv-heu。

Result: 在现实城市网络评估中，UnifSrv至少将吞吐量翻倍或使用一半AP集合实现可比吞吐量；启发式算法与DRL算法性能相当但复杂度低数个数量级。

Conclusion: 首次提出在现实城市网络中实现均匀良好性能的低复杂度AP选择算法，为可扩展无蜂窝大规模MIMO的实际部署提供有效解决方案。

Abstract: Under the ideal assumption of uniform propagation, cell-free massive MIMO (CF-mMIMO) provides uniformly high throughput over the network by effectively surrounding each user with its serving access point (AP) set. However, in realistic non-uniform urban propagation environments, it is difficult to consistently select good limited serving AP sets, resulting in significantly degraded throughput, reintroducing "edge-effect" for the worst-served users. To restore the uniformly good performance of scalable CF-mMIMO in realistic urban networks, we formulate a novel multi-objective optimization problem to jointly achieve high throughput by maximizing the sum data rate, uniform throughput by maximizing Jain's fairness index of the throughput per user, and scalability by minimizing the serving AP set size. We then propose the UnifSrv AP selection algorithms to solve this optimization problem, consisting of a deep reinforcement learning (DRL)-based algorithm UnifSrv-DRL and a heuristic algorithm UnifSrv-heu. We conduct a comprehensive performance evaluation of scalable CF-mMIMO under realistic urban network distributions, propagation, and mobility patterns, showing that the prior benchmark AP selection schemes fail to provide uniformly high throughput in practice. By contrast, UnifSrv at least doubles the throughput compared to prior benchmarks, or achieves comparable throughput but with half of the serving AP set size. Importantly, our heuristic algorithm achieves equivalent throughput to our DRL one, but with orders of magnitude lower complexity. We thus for the first time propose an AP selection algorithm that achieves uniformly good CF-mMIMO performance in realistic urban networks with low complexity.

</details>


### [63] [Optimal Derivative Feedback Control for an Active Magnetic Levitation System: An Experimental Study on Data-Driven Approaches](https://arxiv.org/abs/2602.06944)
*Saber Omidi,Rene Akupan Ebunle,Se Young Yoon*

Main category: eess.SY

TL;DR: 本文比较了两种数据驱动的磁悬浮系统最优导数反馈控制器：基于强化学习的直接无模型方法和基于系统辨识的间接最优控制。直接方法通过多轮数据收集迭代优化，性能优于依赖单次系统辨识的间接方法。


<details>
  <summary>Details</summary>
Motivation: 磁悬浮系统需要高性能控制器，传统基于模型的方法受限于模型准确性。本文旨在探索数据驱动的控制方法，特别是直接无模型方法，以克服模型不准确带来的限制。

Method: 提出两种方法：1) 基于强化学习的直接无模型控制设计，采用策略迭代和epoch循环收集多组数据；2) 间接最优控制设计，使用DMDc和PEM系统辨识获取数学模型。两种方法都用于磁悬浮系统的导数反馈控制。

Result: 两种控制器都能稳定磁悬浮系统并提升性能（相比基于标称模型的设计），但直接无模型方法在允许多轮迭代时始终优于间接方法。epoch循环的迭代优化为直接方法提供了明显优势。

Conclusion: 直接无模型控制方法通过多轮数据收集和迭代优化，能够获得比基于单次系统辨识的间接方法更优的控制性能，展示了数据驱动控制在复杂系统中的应用潜力。

Abstract: This paper presents the design and implementation of data-driven optimal derivative feedback controllers for an active magnetic levitation system. A direct, model-free control design method based on the reinforcement learning framework is compared with an indirect optimal control design derived from a numerically identified mathematical model of the system. For the direct model-free approach, a policy iteration procedure is proposed, which adds an iteration layer called the epoch loop to gather multiple sets of process data, providing a more diverse dataset and helping reduce learning biases. This direct control design method is evaluated against a comparable optimal control solution designed from a plant model obtained through the combined Dynamic Mode Decomposition with Control (DMDc) and Prediction Error Minimization (PEM) system identification. Results show that while both controllers can stabilize and improve the performance of the magnetic levitation system when compared to controllers designed from a nominal model, the direct model-free approach consistently outperforms the indirect solution when multiple epochs are allowed. The iterative refinement of the optimal control law over the epoch loop provides the direct approach a clear advantage over the indirect method, which relies on a single set of system data to determine the identified model and control.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [64] [Single- and Multi-Level Fourier-RQMC Methods for Multivariate Shortfall Risk](https://arxiv.org/abs/2602.06424)
*Chiheb Ben Hammouda,Truong Ngoc Nguyen*

Main category: q-fin.CP

TL;DR: 提出基于傅里叶变换和随机化拟蒙特卡洛采样的多级数值算法，用于高效计算多元短缺风险和最优资本配置。


<details>
  <summary>Details</summary>
Motivation: 多元短缺风险度量是量化系统性风险和确定资本配置的重要框架，但现有蒙特卡洛方法计算成本高、收敛慢，需要更高效的数值算法。

Method: 结合傅里叶反演技术和随机化拟蒙特卡洛采样，在频域而非物理空间评估风险约束和优化中的期望值，利用频域中积分函数更平滑的特性；开发单级和多级RQMC方案，多级方案利用底层确定性优化算法的几何收敛性来降低计算成本。

Result: 数值实验表明，提出的傅里叶RQMC方法在多种风险因子和损失结构模型中，在精度和计算成本方面优于样本平均近似和随机优化基准方法；理论分析和实验结果一致显示相对于基准方法具有改进的渐近收敛性和复杂度。

Conclusion: 该方法为多元短缺风险和最优资本配置的计算提供了高效、准确的数值算法，通过频域转换和多级策略显著提升了计算性能。

Abstract: Multivariate shortfall risk measures provide a principled framework for quantifying systemic risk and determining capital allocations prior to aggregation in interconnected financial systems. Despite their well established theoretical properties, the numerical estimation of multivariate shortfall risk and the corresponding optimal allocations remains computationally challenging, as existing Monte Carlo based approaches can be numerically expensive due to slow convergence.
  In this work, we develop a new class of single and multilevel numerical algorithms for estimating multivariate shortfall risk and the associated optimal allocations, based on a combination of Fourier inversion techniques and randomized quasi Monte Carlo (RQMC) sampling. Rather than operating in physical space, our approach evaluates the relevant expectations appearing in the risk constraint and its optimization in the frequency domain, where the integrands exhibit enhanced smoothness properties that are well suited for RQMC integration. We establish a rigorous mathematical framework for the resulting Fourier RQMC estimators, including convergence analysis and computational complexity bounds. Beyond the single level method, we introduce a multilevel RQMC scheme that exploits the geometric convergence of the underlying deterministic optimization algorithm to reduce computational cost while preserving accuracy.
  Numerical experiments demonstrate that the proposed Fourier RQMC methods outperform sample average approximation and stochastic optimization benchmarks in terms of accuracy and computational cost across a range of models for the risk factors and loss structures. Consistent with the theoretical analysis, these results demonstrate improved asymptotic convergence and complexity rates relative to the benchmark methods, with additional savings achieved through the proposed multilevel RQMC construction.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [65] [Insider Purchase Signals in Microcap Equities: Gradient Boosting Detection of Abnormal Returns](https://arxiv.org/abs/2602.06198)
*Hangyi Zhao*

Main category: q-fin.ST

TL;DR: SEC Form 4 insider purchase filings can predict abnormal returns in U.S. microcap stocks，其中距离52周高点的距离是最重要的预测特征，动量模式（而非均值回归）表现最佳。


<details>
  <summary>Details</summary>
Motivation: 研究SEC Form 4内部人购买申报是否能预测美国微型股的超额回报，特别是在流动性较差的市场中，信息传递可能较慢，内部人信号可能更有价值。

Method: 分析2018-2024年期间17,237笔公开市场购买交易，涉及1,343家发行人（市值在3000万至5亿美元之间）。使用梯度提升分类器，基于内部人身份、交易历史和披露时的市场条件进行训练。

Result: 模型在2024年样本外数据上AUC达到0.70，优化阈值为0.20时，精确度为0.38，召回率为0.69。距离52周高点的距离占预测信号的36%。动量模式表现最佳：价格涨幅超过10%后披露的交易产生最高平均累计异常回报（6.3%）和最高超额表现概率（36.7%）。

Conclusion: 内部人购买申报能预测微型股超额回报，动量模式（而非均值回归）表现最好。这支持了在流动性较差市场中信息传递较慢的假设，趋势确认可能过滤出更高信心的内部人信号。

Abstract: This paper examines whether SEC Form 4 insider purchase filings predict abnormal returns in U.S. microcap stocks. The analysis covers 17,237 open-market purchases across 1,343 issuers from 2018 through 2024, restricted to market capitalizations between \$30M and \$500M. A gradient boosting classifier trained on insider identity, transaction history, and market conditions at disclosure achieves AUC of 0.70 on out-of-sample 2024 data. At an optimized threshold of 0.20, precision is 0.38 and recall is 0.69. The distance from the 52-week high dominates feature importance, accounting for 36% of predictive signal. A momentum pattern emerges in the data: transactions disclosed after price appreciation exceeding 10% yield the highest mean cumulative abnormal return (6.3%) and the highest probability of outperformance (36.7%). This contrasts with the simple mean-reversion intuition often applied to post-run-up entries. The result is robust to winsorization and holds across subsamples. These patterns are consistent with slower information incorporation in illiquid markets, where trend confirmation may filter for higher-conviction insider signals.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [66] [Wishart conditional tail risk measures: An analytic approach](https://arxiv.org/abs/2602.06401)
*Jose Da Fonseca,Patrick Wong*

Main category: q-fin.RM

TL;DR: 提出基于Wishart过程的多元风险度量新框架，可显式计算条件尾部风险度量并解决资本配置问题


<details>
  <summary>Details</summary>
Motivation: 需要量化多元风险度量的新分析框架，传统方法难以处理高维风险变量的条件尾部风险计算

Method: 利用Wishart过程（正定矩阵值随机过程）的优良解析性质，推导可显式计算的条件尾部风险度量，通过一维或二维积分即可求解

Result: 框架具有多功能性和可操作性，能解决基于条件矩的资本配置问题，并可嵌入时间滞后的风险度量视角

Conclusion: 该框架为风险管理提供了有用工具，特别适用于需要处理多元风险变量和时间动态性的复杂风险管理场景

Abstract: This study introduces a new analytical framework for quantifying multivariate risk measures. Using the Wishart process, which is a stochastic process with values in the space of positive definite matrices, we derive several conditional tail risk measures which, thanks to the remarkable analytical properties of the Wishart process, can be explicitly computed up to a one- or two-dimensional integration. These quantities can also be used to solve analytically a capital allocation problem based on conditional moments. Exploiting the stochastic differential equation property of the Wishart process, we show how an intertemporal (i.e., time-lagged) view of these risk measures can be embedded in the proposed framework. Several numerical examples show that the framework is versatile and operational, thus providing a useful tool for risk management.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [67] [Deep networks learn to parse uniform-depth context-free languages from local statistics](https://arxiv.org/abs/2602.06065)
*Jack T. Parley,Francesco Cagnetta,Matthieu Wyart*

Main category: stat.ML

TL;DR: 论文提出了一个可调的概率上下文无关文法框架，研究语言结构如何从句子中学习，并建立了学习机制与语言统计特性之间的联系。


<details>
  <summary>Details</summary>
Motivation: 理解语言结构如何仅从句子中学习是认知科学和机器学习的核心问题。虽然大语言模型能够解析文本并独立于表层形式表示语义概念，但哪些数据统计特性使这些成就成为可能，以及需要多少数据，仍然未知。

Method: 1) 引入可调的概率上下文无关文法类别，可以控制歧义程度和跨尺度的相关结构；2) 提出受深度卷积网络结构启发的推理算法，将可学习性和样本复杂度与特定语言统计特性联系起来；3) 在深度卷积和基于Transformer的架构上进行实证验证。

Result: 建立了一个统一框架，其中不同尺度的相关性消除了局部歧义，使得数据的层次表示得以涌现。验证了预测在不同神经网络架构上的有效性。

Conclusion: 提出了一个连接语言统计特性、学习机制和神经网络架构的统一框架，揭示了多尺度相关性如何促进层次语言结构的涌现学习。

Abstract: Understanding how the structure of language can be learned from sentences alone is a central question in both cognitive science and machine learning. Studies of the internal representations of Large Language Models (LLMs) support their ability to parse text when predicting the next word, while representing semantic notions independently of surface form. Yet, which data statistics make these feats possible, and how much data is required, remain largely unknown. Probabilistic context-free grammars (PCFGs) provide a tractable testbed for studying these questions. However, prior work has focused either on the post-hoc characterization of the parsing-like algorithms used by trained networks; or on the learnability of PCFGs with fixed syntax, where parsing is unnecessary. Here, we (i) introduce a tunable class of PCFGs in which both the degree of ambiguity and the correlation structure across scales can be controlled; (ii) provide a learning mechanism -- an inference algorithm inspired by the structure of deep convolutional networks -- that links learnability and sample complexity to specific language statistics; and (iii) validate our predictions empirically across deep convolutional and transformer-based architectures. Overall, we propose a unifying framework where correlations at different scales lift local ambiguities, enabling the emergence of hierarchical representations of the data.

</details>


### [68] [Algebraic Robustness Verification of Neural Networks](https://arxiv.org/abs/2602.06105)
*Yulia Alexandr,Hao Duan,Guido Montúfar*

Main category: stat.ML

TL;DR: 将神经网络鲁棒性验证转化为代数优化问题，利用欧几里得距离度（ED degree）衡量验证的固有复杂度，提出基于数值同伦延拓的精确鲁棒性认证算法。


<details>
  <summary>Details</summary>
Motivation: 神经网络鲁棒性验证通常很困难，需要一种能够量化验证复杂度的理论框架。本文旨在建立代数几何与神经网络验证之间的具体联系，提供更系统化的验证方法。

Method: 将鲁棒性验证形式化为代数优化问题，引入ED degree作为架构相关的复杂度度量，定义ED判别式区分验证难易实例，提出参数判别式识别决策边界代数复杂度降低的参数，推导多种网络架构的ED degree闭式表达式，开发基于数值同伦延拓的精确认证算法。

Result: 为多个神经网络架构类别推导了ED degree的闭式表达式，在无限宽度极限下得到了实临界点期望数的公式，提出了计算ED判别式的显式算法，建立了基于代数几何的精确鲁棒性认证方法。

Conclusion: 本文建立了度量代数几何与神经网络验证之间的具体联系，ED degree为理解验证复杂度提供了理论框架，提出的代数方法为鲁棒性认证开辟了新途径。

Abstract: We formulate formal robustness verification of neural networks as an algebraic optimization problem. We leverage the Euclidean Distance (ED) degree, which is the generic number of complex critical points of the distance minimization problem to a classifier's decision boundary, as an architecture-dependent measure of the intrinsic complexity of robustness verification. To make this notion operational, we define the associated ED discriminant, which characterizes input points at which the number of real critical points changes, distinguishing test instances that are easier or harder to verify. We provide an explicit algorithm for computing this discriminant. We further introduce the parameter discriminant of a neural network, identifying parameters where the ED degree drops and the decision boundary exhibits reduced algebraic complexity. We derive closed-form expressions for the ED degree for several classes of neural architectures, as well as formulas for the expected number of real critical points in the infinite-width limit. Finally, we present an exact robustness certification algorithm based on numerical homotopy continuation, establishing a concrete link between metric algebraic geometry and neural network verification.

</details>


### [69] [Inheritance Between Feedforward and Convolutional Networks via Model Projection](https://arxiv.org/abs/2602.06245)
*Nicolas Ewen,Jairo Diaz-Rodriguez,Kelly Ramsay*

Main category: stat.ML

TL;DR: 论文提出模型投影方法，将卷积网络转换为类前馈网络形式，实现参数高效的迁移学习


<details>
  <summary>Details</summary>
Motivation: 前馈网络和卷积网络技术常被跨家族复用，但两者模型类之间的关系很少被明确阐述。两种家族在每输入参数化方面存在不匹配，需要参数高效的迁移学习方法

Method: 引入统一的节点级形式化表示，证明广义前馈网络是广义卷积网络的严格子集。提出模型投影方法：冻结预训练的每输入通道滤波器，为每个(输出通道,输入通道)贡献学习单个标量门，使卷积层保持可适应性同时大幅减少训练参数

Result: 投影节点呈现广义前馈网络形式，使投影CNN能够继承不依赖同质层输入的前馈技术。在多个ImageNet预训练骨干网络和下游图像分类数据集上的实验表明，模型投影在简单训练方案下是强大的迁移学习基线

Conclusion: 模型投影为卷积网络提供了参数高效的迁移学习方法，通过将CNN转换为类FFN形式，实现了两种网络家族技术的统一和复用

Abstract: Techniques for feedforward networks (FFNs) and convolutional networks (CNNs) are frequently reused across families, but the relationship between the underlying model classes is rarely made explicit. We introduce a unified node-level formalization with tensor-valued activations and show that generalized feedforward networks form a strict subset of generalized convolutional networks. Motivated by the mismatch in per-input parameterization between the two families, we propose model projection, a parameter-efficient transfer learning method for CNNs that freezes pretrained per-input-channel filters and learns a single scalar gate for each (output channel, input channel) contribution. Projection keeps all convolutional layers adaptable to downstream tasks while substantially reducing the number of trained parameters in convolutional layers. We prove that projected nodes take the generalized FFN form, enabling projected CNNs to inherit feedforward techniques that do not rely on homogeneous layer inputs. Experiments across multiple ImageNet-pretrained backbones and several downstream image classification datasets show that model projection is a strong transfer learning baseline under simple training recipes.

</details>


### [70] [Time-uniform conformal and PAC prediction](https://arxiv.org/abs/2602.06297)
*Kayla E. Scharfstein,Arun Kumar Kuchibhotla*

Main category: stat.ML

TL;DR: 提出了一种适用于序列数据的扩展版conformal prediction方法，能够在数据流式生成、样本量不固定的情况下提供随时有效的预测区间保证。


<details>
  <summary>Details</summary>
Motivation: 传统conformal prediction方法在序列设置中存在局限性：需要固定样本量，无法处理持续更新的预测。随着机器学习模型越来越多地应用于高风险决策，需要开发能够适应流式数据环境的可靠不确定性量化方法。

Method: 扩展了conformal prediction和PAC预测框架，使其适用于序列设置。提出的方法能够处理样本量不预先固定的情况，生成"随时有效"的预测集，即使在分析师基于数据选择的时间点也能保证期望覆盖水平。

Result: 提供了理论保证，并在模拟和真实数据集上验证了方法的有效性和实用性。预测集在任意选择的时间点都能保持所需的覆盖水平。

Conclusion: 成功开发了适用于序列设置的conformal prediction扩展方法，解决了传统方法在流式数据环境中的局限性，为高风险决策中的机器学习模型提供了更灵活可靠的不确定性量化工具。

Abstract: Given that machine learning algorithms are increasingly being deployed to aid in high stakes decision-making, uncertainty quantification methods that wrap around these black box models such as conformal prediction have received much attention in recent years. In sequential settings, where data are observed/generated in a streaming fashion, traditional conformal methods do not provide any guarantee without fixing the sample size. More importantly, traditional conformal methods cannot cope with sequentially updated predictions. As such, we develop an extension of the conformal prediction and related probably approximately correct (PAC) prediction frameworks to sequential settings where the number of data points is not fixed in advance. The resulting prediction sets are anytime-valid in that their expected coverage is at the required level at any time chosen by the analyst even if this choice depends on the data. We present theoretical guarantees for our proposed methods and demonstrate their validity and utility on simulated and real datasets.

</details>


### [71] [High-Dimensional Limit of Stochastic Gradient Flow via Dynamical Mean-Field Theory](https://arxiv.org/abs/2602.06320)
*Sota Nishiyama,Masaaki Imaizumi*

Main category: stat.ML

TL;DR: 本文提出了一种基于动态平均场理论（DMFT）的框架，用于分析高维非线性模型中多轮小批量随机梯度下降（SGD）的渐进行为，通过随机梯度流（SGF）近似并推导出低维连续时间方程。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏分析高维非线性模型中多轮小批量SGD动态行为的理论框架，特别是在样本数n和维度d成比例增长的极限情况下，需要建立统一的描述方法。

Method: 使用随机梯度流（SGF）近似多轮小批量SGD，在n和d成比例增长的极限下，基于动态平均场理论（DMFT）推导出低维连续时间方程，并扩展DMFT技术处理SGF的随机性。

Result: 建立了描述SGF参数渐近分布的低维方程系统，该理论适用于广义线性模型和两层神经网络等多种模型，并能统一现有SGD动态的高维描述框架。

Conclusion: 提出的DMFT框架填补了高维非线性模型中多轮小批量SGD动态分析的理论空白，为理解SGD在高维环境中的行为提供了统一的理论基础。

Abstract: Modern machine learning models are typically trained via multi-pass stochastic gradient descent (SGD) with small batch sizes, and understanding their dynamics in high dimensions is of great interest. However, an analytical framework for describing the high-dimensional asymptotic behavior of multi-pass SGD with small batch sizes for nonlinear models is currently missing. In this study, we address this gap by analyzing the high-dimensional dynamics of a stochastic differential equation called a \emph{stochastic gradient flow} (SGF), which approximates multi-pass SGD in this regime. In the limit where the number of data samples $n$ and the dimension $d$ grow proportionally, we derive a closed system of low-dimensional and continuous-time equations and prove that it characterizes the asymptotic distribution of the SGF parameters. Our theory is based on the dynamical mean-field theory (DMFT) and is applicable to a wide range of models encompassing generalized linear models and two-layer neural networks. We further show that the resulting DMFT equations recover several existing high-dimensional descriptions of SGD dynamics as special cases, thereby providing a unifying perspective on prior frameworks such as online SGD and high-dimensional linear regression. Our proof builds on the existing DMFT technique for gradient flow and extends it to handle the stochasticity in SGF using tools from stochastic calculus.

</details>


### [72] [Revisiting the Sliced Wasserstein Kernel for persistence diagrams: a Figalli-Gigli approach](https://arxiv.org/abs/2602.06539)
*Marc Janthial,Théo Lacombe*

Main category: stat.ML

TL;DR: 本文提出了一种新的切片Figalli-Gigli核(SFGK)，用于持久性图分析，它直接使用Figalli-Gigli距离而非Wasserstein距离作为构建基础，更忠实于持久性图的自然几何结构。


<details>
  <summary>Details</summary>
Motivation: 现有的切片Wasserstein核(SWK)虽然有效，但依赖于对Wasserstein距离的ad-hoc调整来适应持久性图的特殊几何结构。作者希望提出一个更直接、更忠实于持久性图自然几何的核方法。

Method: 提出切片Figalli-Gigli核(SFGK)，直接使用Figalli-Gigli距离（持久性图比较中常规使用的部分匹配距离）作为构建核的基础，而不是像SWK那样使用Wasserstein距离。

Result: SFGK保持了SWK的重要性质，包括嵌入的失真结果和计算便利性，同时能更自然地处理无限持久性图和持久性测度。在基准应用上与SWK表现相当。

Conclusion: SFGK提供了一个更忠实于持久性图自然几何的核方法，保持了SWK的理论和计算优势，同时能更好地处理持久性图的特殊结构。

Abstract: The Sliced Wasserstein Kernel (SWK) for persistence diagrams was introduced in (Carri{è}re et al. 2017) as a powerful tool to implicitly embed persistence diagrams in a Hilbert space with reasonable distortion. This kernel is built on the intuition that the Figalli-Gigli distance-that is the partial matching distance routinely used to compare persistence diagrams-resembles the Wasserstein distance used in the optimal transport literature, and that the later could be sliced to define a positive definite kernel on the space of persistence diagrams. This efficient construction nonetheless relies on ad-hoc tweaks on the Wasserstein distance to account for the peculiar geometry of the space of persistence diagrams. In this work, we propose to revisit this idea by directly using the Figalli-Gigli distance instead of the Wasserstein one as the building block of our kernel. On the theoretical side, our sliced Figalli-Gigli kernel (SFGK) shares most of the important properties of the SWK of Carri{è}re et al., including distortion results on the induced embedding and its ease of computation, while being more faithful to the natural geometry of persistence diagrams. In particular, it can be directly used to handle infinite persistence diagrams and persistence measures. On the numerical side, we show that the SFGK performs as well as the SWK on benchmark applications.

</details>


### [73] [Operationalizing Stein's Method for Online Linear Optimization: CLT-Based Optimal Tradeoffs](https://arxiv.org/abs/2602.06545)
*Zhiyu Zhang,Aaditya Ramdas*

Main category: stat.ML

TL;DR: 将Stein方法应用于在线线性优化，提出计算高效的算法，实现加性锐利的上界，超越传统最优性


<details>
  <summary>Details</summary>
Motivation: 解决传统基于动态规划的在线线性优化算法计算效率低的问题，利用概率论中的Stein方法设计高效算法

Method: 将Stein方法（经典概率极限定理证明框架）操作化为计算高效的OLO算法，受Röllin (2018) Wasserstein鞅中心极限定理证明启发

Result: 算法实现"加性锐利"的遗憾和总损失上界，超越传统大O最优性；改进OGD和MWU的总损失上界；实现最优两点权衡；在噪声反馈下获得锐利期望性能保证

Conclusion: Stein方法可作为计算高效的OLO算法框架，提供锐利性能保证，在多个方面超越现有方法

Abstract: Adversarial online linear optimization (OLO) is essentially about making performance tradeoffs with respect to the unknown difficulty of the adversary. In the setting of one-dimensional fixed-time OLO on a bounded domain, it has been observed since Cover (1966) that achievable tradeoffs are governed by probabilistic inequalities, and these descriptive results can be converted into algorithms via dynamic programming, which, however, is not computationally efficient. We address this limitation by showing that Stein's method, a classical framework underlying the proofs of probabilistic limit theorems, can be operationalized as computationally efficient OLO algorithms. The associated regret and total loss upper bounds are "additively sharp", meaning that they surpass the conventional big-O optimality and match normal-approximation-based lower bounds by additive lower order terms. Our construction is inspired by the remarkably clean proof of a Wasserstein martingale central limit theorem (CLT) due to Röllin (2018).
  Several concrete benefits can be obtained from this general technique. First, with the same computational complexity, the proposed algorithm improves upon the total loss upper bounds of online gradient descent (OGD) and multiplicative weight update (MWU). Second, our algorithm can realize a continuum of optimal two-point tradeoffs between the total loss and the maximum regret over comparators, improving upon prior works in parameter-free online learning. Third, by allowing the adversary to randomize on an unbounded support, we achieve sharp in-expectation performance guarantees for OLO with noisy feedback.

</details>


### [74] [Infinite-dimensional generative diffusions via Doob's h-transform](https://arxiv.org/abs/2602.06621)
*Thorben Pieper-Sethmacher,Daniel Paulin*

Main category: stat.ML

TL;DR: 提出基于Doob's h变换的无限维生成扩散模型框架，通过指数测度变换而非时间反转，在可验证条件下建立严格理论并验证效果


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型方法难以推广到无限维设置，需要更灵活的框架来处理高维和无限维数据生成问题

Method: 使用Doob's h变换和指数测度变换，通过参考扩散过程向目标分布强制演化，而非依赖去噪过程的时间反转

Result: 建立了严格的理论框架，证明了在可验证条件下的收敛性，通过分数匹配目标最小化近似强制过程，在合成和真实数据上验证了方法有效性

Conclusion: 提出的无限维扩散模型框架具有更好的理论严格性和灵活性，为高维数据生成提供了新方法

Abstract: This paper introduces a rigorous framework for defining generative diffusion models in infinite dimensions via Doob's h-transform. Rather than relying on time reversal of a noising process, a reference diffusion is forced towards the target distribution by an exponential change of measure. Compared to existing methodology, this approach readily generalises to the infinite-dimensional setting, hence offering greater flexibility in the diffusion model. The construction is derived rigorously under verifiable conditions, and bounds with respect to the target measure are established. We show that the forced process under the changed measure can be approximated by minimising a score-matching objective and validate our method on both synthetic and real data.

</details>


### [75] [Missing At Random as Covariate Shift: Correcting Bias in Iterative Imputation](https://arxiv.org/abs/2602.06713)
*Luke Shannon,Song Liu,Katarzyna Reluga*

Main category: stat.ML

TL;DR: 论文提出了一种基于重要性权重校正分布偏差的缺失数据插补方法，通过联合估计权重和插补模型，相比传统方法显著提升了插补准确性。


<details>
  <summary>Details</summary>
Motivation: 现有缺失数据插补方法未考虑观测数据与未观测数据之间的协变量偏移，这种分布偏差导致插补结果不理想，影响下游机器学习性能。

Method: 将缺失数据插补建模为风险最小化问题，推导出理论上有效的重要性权重来校正分布偏差，并提出联合估计重要性权重和插补模型的算法。

Result: 在基准数据集上，相比未加权方法，均方根误差降低达7%，Wasserstein距离减少达20%，显著提升了插补质量。

Conclusion: 通过校正观测与未观测数据间的分布偏差，提出的重要性加权插补方法能够显著提高缺失数据插补的准确性，为下游机器学习任务提供更可靠的数据。

Abstract: Accurate imputation of missing data is critical to downstream machine learning performance. We formulate missing data imputation as a risk minimisation problem, which highlights a covariate shift between the observed and unobserved data distributions. This covariate shift induced bias is not accounted for by popular imputation methods and leads to suboptimal performance. In this paper, we derive theoretically valid importance weights that correct for the induced distributional bias. Furthermore, we propose a novel imputation algorithm that jointly estimates both the importance weights and imputation models, enabling bias correction throughout the imputation process. Empirical results across benchmark datasets show reductions in root mean squared error and Wasserstein distance of up to 7% and 20%, respectively, compared to otherwise identical unweighted methods.

</details>


### [76] [Optimal Learning-Rate Schedules under Functional Scaling Laws: Power Decay and Warmup-Stable-Decay](https://arxiv.org/abs/2602.06797)
*Binghui Li,Zilin Wang,Fengling Chen,Shiyang Zhao,Ruiheng Zheng,Lei Wu*

Main category: stat.ML

TL;DR: 该论文在函数缩放定律框架下研究最优学习率调度，揭示了基于任务难度的相变现象：简单任务采用幂衰减，困难任务采用WSD结构，并分析了固定形状调度的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有学习率调度策略缺乏理论指导，实践中常采用启发式方法。作者旨在在函数缩放定律框架下，为不同难度的任务提供理论最优的学习率调度方案。

Method: 在函数缩放定律框架下，通过分析信号学习指数s和容量指数β，推导固定训练周期N下的最优学习率调度，并分析固定形状调度的性能。

Result: 发现基于任务难度的相变：当s≥1-1/β时，最优调度为幂衰减；当s<1-1/β时，最优调度为WSD结构。将幂衰减应用于核回归SGD，消除了先前分析中的对数次优性。

Conclusion: 该研究为学习率调度提供了理论指导，揭示了任务难度对最优调度结构的关键影响，并为实践中常用的调度策略提供了原则性评估框架。

Abstract: We study optimal learning-rate schedules (LRSs) under the functional scaling law (FSL) framework introduced in Li et al. (2025), which accurately models the loss dynamics of both linear regression and large language model (LLM) pre-training. Within FSL, loss dynamics are governed by two exponents: a source exponent $s>0$ controlling the rate of signal learning, and a capacity exponent $β>1$ determining the rate of noise forgetting. Focusing on a fixed training horizon $N$, we derive the optimal LRSs and reveal a sharp phase transition. In the easy-task regime $s \ge 1 - 1/β$, the optimal schedule follows a power decay to zero, $η^*(z) = η_{\mathrm{peak}}(1 - z/N)^{2β- 1}$, where the peak learning rate scales as $η_{\mathrm{peak}} \eqsim N^{-ν}$ for an explicit exponent $ν= ν(s,β)$. In contrast, in the hard-task regime $s < 1 - 1/β$, the optimal LRS exhibits a warmup-stable-decay (WSD) (Hu et al. (2024)) structure: it maintains the largest admissible learning rate for most of training and decays only near the end, with the decay phase occupying a vanishing fraction of the horizon.
  We further analyze optimal shape-fixed schedules, where only the peak learning rate is tuned -- a strategy widely adopted in practiceand characterize their strengths and intrinsic limitations. This yields a principled evaluation of commonly used schedules such as cosine and linear decay. Finally, we apply the power-decay LRS to one-pass stochastic gradient descent (SGD) for kernel regression and show the last iterate attains the exact minimax-optimal rate, eliminating the logarithmic suboptimality present in prior analyses. Numerical experiments corroborate our theoretical predictions.

</details>


<div id='q-fin.MF'></div>

# q-fin.MF [[Back]](#toc)

### [77] [Joint survival annuity derivative valuation in the linear-rational Wishart mortality model](https://arxiv.org/abs/2602.06415)
*Jose Da Fonseca,Patrick Wong*

Main category: q-fin.MF

TL;DR: 提出基于Wishart过程的线性有理联合生存死亡率模型，用于建模和管理联合死亡率风险，得到闭式解并开发快速近似方法。


<details>
  <summary>Details</summary>
Motivation: 需要一种能够建模联合死亡率风险、保证死亡率强度为正、并能处理依赖关系的灵活统一框架。

Method: 使用Wishart过程作为状态变量，结合线性有理框架构建联合生存死亡率模型。Wishart过程是随机连续矩阵仿射过程，能保证死亡率强度为正并建模依赖关系。

Result: 推导出联合生存年金和保证联合生存年金期权的闭式表达式，给出了死亡率强度及其依赖关系的分布，开发了基于多项式展开的快速准确近似方法。

Conclusion: 线性有理Wishart死亡率模型为建模和管理联合死亡率风险提供了一个灵活统一的框架，简化了模型实现。

Abstract: This study proposes a linear-rational joint survival mortality model based on the Wishart process. The Wishart process, which is a stochastic continuous matrix affine process, allows for a general dependency between the mortality intensities that are constructed to be positive. Using the linear-rational framework along with the Wishart process as state variable, we derive a closed-form expression for the joint survival annuity, as well as the guaranteed joint survival annuity option. Exploiting our parameterisation of the Wishart process, we explicit the distribution of the mortality intensities and their dependency. We provide the distribution (density and cumulative distribution) of the joint survival annuity. We also develop some polynomial expansions for the underlying state variable that lead to fast and accurate approximations for the guaranteed joint survival annuity option. These polynomial expansions also significantly simplify the implementation of the model. Overall, the linear-rational Wishart mortality model provides a flexible and unified framework for modelling and managing joint mortality risk.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [78] [NanoNet: Parameter-Efficient Learning with Label-Scarce Supervision for Lightweight Text Mining Model](https://arxiv.org/abs/2602.06093)
*Qianren Mao,Yashuo Luo,Ziqi Qin,Junnan Liu,Weifeng Jiang,Zhijun Chen,Zhuoran Li,Likang Xiao,Chuou Xu,Qili Zhang,Hanwen Hao,Jingzheng Li,Chunghua Lin,Jianxin Li,Philip S. Yu*

Main category: cs.LG

TL;DR: NanoNet是一个轻量级文本挖掘框架，通过参数高效学习和在线知识蒸馏，在有限监督下训练多个小模型，并通过相互学习正则化提升性能，实现低成本训练和快速推理。


<details>
  <summary>Details</summary>
Motivation: 现有轻量级半监督学习策略虽然能减少标注样本和降低推理成本，但存在计算密集、易陷入局部最优的问题。作者希望探索在文本挖掘任务中整合三个低成本场景：有限标注监督、轻量级微调和快速推理小模型。

Method: 提出NanoNet框架，采用参数高效学习策略，使用在线知识蒸馏生成多个小模型，并通过相互学习正则化增强模型性能。整个过程减少训练成本，最小化监督需求。

Result: 框架实现了低成本训练和最小化监督需求，最终产生可用于下游推理的轻量级模型。

Conclusion: NanoNet为轻量级文本挖掘提供了一种有效的解决方案，通过参数高效学习和相互学习正则化，在有限监督下实现了低成本训练和快速推理。

Abstract: The lightweight semi-supervised learning (LSL) strategy provides an effective approach of conserving labeled samples and minimizing model inference costs. Prior research has effectively applied knowledge transfer learning and co-training regularization from large to small models in LSL. However, such training strategies are computationally intensive and prone to local optima, thereby increasing the difficulty of finding the optimal solution. This has prompted us to investigate the feasibility of integrating three low-cost scenarios for text mining tasks: limited labeled supervision, lightweight fine-tuning, and rapid-inference small models. We propose NanoNet, a novel framework for lightweight text mining that implements parameter-efficient learning with limited supervision. It employs online knowledge distillation to generate multiple small models and enhances their performance through mutual learning regularization. The entire process leverages parameter-efficient learning, reducing training costs and minimizing supervision requirements, ultimately yielding a lightweight model for downstream inference.

</details>


### [79] [Agentic Workflow Using RBA$_θ$ for Event Prediction](https://arxiv.org/abs/2602.06097)
*Purbak Sengupta,Sambeet Mishra,Sonal Shreya*

Main category: cs.LG

TL;DR: 提出事件优先、频率感知的风电爬坡事件预测框架，直接预测爬坡事件并重构功率轨迹，而非从密集预测中推断事件，实现跨站点零样本迁移。


<details>
  <summary>Details</summary>
Motivation: 风电爬坡事件预测困难，传统方法通过密集预测后提取事件存在局限性，需要更直接、可迁移的事件预测方法。

Method: 基于增强的RBAθ事件表示，整合统计、机器学习和深度学习模型，引入事件优先的深度架构，集成小波频率分解、时间激励特征和自适应特征选择。

Result: 能够稳定预测长时域事件，物理一致地重构轨迹，实现零样本迁移到未见风电场，爬坡幅度和持续时间受不同中频带控制。

Conclusion: 事件优先、频率感知的预测框架为风电功率预测提供了可迁移且与运行对齐的替代方案。

Abstract: Wind power ramp events are difficult to forecast due to strong variability, multi-scale dynamics, and site-specific meteorological effects. This paper proposes an event-first, frequency-aware forecasting paradigm that directly predicts ramp events and reconstructs the power trajectory thereafter, rather than inferring events from dense forecasts. The framework is built on an enhanced Ramping Behaviour Analysis (RBA$_θ$) method's event representation and progressively integrates statistical, machine-learning, and deep-learning models. Traditional forecasting models with post-hoc event extraction provides a strong interpretable baseline but exhibits limited generalisation across sites. Direct event prediction using Random Forests improves robustness over survival-based formulations, motivating fully event-aware modelling. To capture the multi-scale nature of wind ramps, we introduce an event-first deep architecture that integrates wavelet-based frequency decomposition, temporal excitation features, and adaptive feature selection. The resulting sequence models enable stable long-horizon event prediction, physically consistent trajectory reconstruction, and zero-shot transfer to previously unseen wind farms. Empirical analysis shows that ramp magnitude and duration are governed by distinct mid-frequency bands, allowing accurate signal reconstruction from sparse event forecasts. An agentic forecasting layer is proposed, in which specialised workflows are selected dynamically based on operational context. Together, the framework demonstrates that event-first, frequency-aware forecasting provides a transferable and operationally aligned alternative to trajectory-first wind-power prediction.

</details>


### [80] [Toward Faithful and Complete Answer Construction from a Single Document](https://arxiv.org/abs/2602.06103)
*Zhaoyang Chen,Cody Fleming*

Main category: cs.LG

TL;DR: EVE是一个结构化框架，通过提取、验证和枚举的管道约束生成，解决LLM在文档基础推理中完整性和忠实性的问题，显著提升召回率、精确率和F1分数。


<details>
  <summary>Details</summary>
Motivation: 现代大语言模型基于统计的下一词预测，倾向于生成高概率的延续而非基于源内容的详尽忠实回答，缺乏确保完整性和忠实性的系统机制，这与AI安全原则相冲突。

Method: EVE采用结构化可验证的管道，将高严谨推理分解为提取、验证和枚举三个步骤，约束生成过程，不同于自由形式的提示方法。

Result: 召回率和精确率分别提升高达24%和29%，F1分数提升31%，打破了传统单次LLM生成中覆盖率和准确性的权衡，同时缓解了长度限制导致的截断问题。

Conclusion: EVE有效解决了LLM在文档基础推理中的完整性和忠实性问题，但受自然语言固有模糊性的限制，存在性能饱和现象，反映了基于语言推理的基本限制。

Abstract: Modern large language models (LLMs) are powerful generators driven by statistical next-token prediction. While effective at producing fluent text, this design biases models toward high-probability continuations rather than exhaustive and faithful answers grounded in source content. As a result, directly applying LLMs lacks systematic mechanisms to ensure both completeness (avoiding omissions) and faithfulness (avoiding unsupported content), which fundamentally conflicts with core AI safety principles. To address this limitation, we present EVE, a structured framework for document-grounded reasoning.
  Unlike free-form prompting, EVE constrains generation to a structured, verifiable pipeline that decomposes high-rigor reasoning into extraction, validation, and enumeration. Empirically, this design enables consistent and simultaneous improvements in recall, precision, and F1-score: recall and precision increase by up to 24\% and 29\%, respectively, with a corresponding 31\% gain in F1-score. This effectively breaks the long-standing trade-off between coverage and accuracy typical of single-pass LLM generation, while also mitigating generation truncation caused by length limitations. At the same time, we emphasize that EVE exhibits performance saturation due to the inherent ambiguity of natural language, reflecting fundamental limits of language-based reasoning.

</details>


### [81] [Pragmatic Curiosity: A Hybrid Learning-Optimization Paradigm via Active Inference](https://arxiv.org/abs/2602.06104)
*Yingke Li,Anjali Parashar,Enlu Zhou,Chuchu Fan*

Main category: cs.LG

TL;DR: 提出"实用好奇心"混合学习优化范式，通过最小化期望自由能量统一目标导向和信息获取，在多种现实混合任务中优于传统贝叶斯优化和实验设计方法


<details>
  <summary>Details</summary>
Motivation: 许多工程和科学工作流依赖于昂贵的黑盒评估，需要同时改进性能和减少不确定性的决策。传统贝叶斯优化和贝叶斯实验设计分别处理目标寻求和信息寻求，对学习和优化内在耦合的混合场景指导有限

Method: 提出"实用好奇心"混合学习优化范式，源自主动推断理论，通过最小化期望自由能量来选择行动，该单一目标将实用效用与认知信息增益耦合

Result: 在多种现实混合任务（包括约束系统识别、目标主动搜索和未知偏好复合优化）中，实用好奇心始终优于强贝叶斯优化型和贝叶斯实验设计型基线，提供更高的估计精度、更好的关键区域覆盖和改善的最终解决方案质量

Conclusion: 实用好奇心为学习和优化内在耦合的混合场景提供了有效的统一框架，在多个实际应用中表现出优于传统方法的性能

Abstract: Many engineering and scientific workflows depend on expensive black-box evaluations, requiring decision-making that simultaneously improves performance and reduces uncertainty. Bayesian optimization (BO) and Bayesian experimental design (BED) offer powerful yet largely separate treatments of goal-seeking and information-seeking, providing limited guidance for hybrid settings where learning and optimization are intrinsically coupled. We propose "pragmatic curiosity," a hybrid learning-optimization paradigm derived from active inference, in which actions are selected by minimizing the expected free energy--a single objective that couples pragmatic utility with epistemic information gain. We demonstrate the practical effectiveness and flexibility of pragmatic curiosity on various real-world hybrid tasks, including constrained system identification, targeted active search, and composite optimization with unknown preferences. Across these benchmarks, pragmatic curiosity consistently outperforms strong BO-type and BED-type baselines, delivering higher estimation accuracy, better critical-region coverage, and improved final solution quality.

</details>


### [82] [Private and interpretable clinical prediction with quantum-inspired tensor train models](https://arxiv.org/abs/2602.06110)
*José Ramón Pareja Monturiol,Juliette Sinnott,Roger G. Melko,Mohammad Kohandel*

Main category: cs.LG

TL;DR: 该论文研究临床机器学习模型的隐私风险，发现逻辑回归和神经网络都会泄露训练数据信息，并提出基于张量化的量子启发防御方法，在保护隐私的同时保持模型可解释性和准确性。


<details>
  <summary>Details</summary>
Motivation: 临床机器学习需要在预测准确性、可解释性和隐私保护之间取得平衡。现有模型如逻辑回归和神经网络都存在隐私漏洞，容易受到攻击泄露训练数据信息，这在临床应用中尤其危险。

Method: 设计攻击方法评估逻辑回归和神经网络模型的隐私风险，针对LORIS模型（免疫治疗响应预测的逻辑回归模型）和浅层神经网络进行实证分析。提出基于张量化的防御方法，将离散化模型转换为张量序列，完全混淆参数同时保持准确性。

Result: 两种模型都显著泄露训练集信息，逻辑回归在白盒场景下特别脆弱，交叉验证等常见做法加剧了风险。张量化防御将白盒攻击降低到随机猜测水平，黑盒攻击效果与差分隐私相当，同时保持模型可解释性和准确性。

Conclusion: 张量化方法为临床预测提供了实用基础，实现了隐私保护、可解释性和有效性的平衡，适用于广泛的应用场景，并能将神经网络提升到与逻辑回归相当的可解释性水平。

Abstract: Machine learning in clinical settings must balance predictive accuracy, interpretability, and privacy. Models such as logistic regression (LR) offer transparency, while neural networks (NNs) provide greater predictive power; yet both remain vulnerable to privacy attacks. We empirically assess these risks by designing attacks that identify which public datasets were used to train a model under varying levels of adversarial access, applying them to LORIS, a publicly available LR model for immunotherapy response prediction, as well as to additional shallow NN models trained for the same task. Our results show that both models leak significant training-set information, with LRs proving particularly vulnerable in white-box scenarios. Moreover, we observe that common practices such as cross-validation in LRs exacerbate these risks. To mitigate these vulnerabilities, we propose a quantum-inspired defense based on tensorizing discretized models into tensor trains (TTs), which fully obfuscates parameters while preserving accuracy, reducing white-box attacks to random guessing and degrading black-box attacks comparably to Differential Privacy. TT models retain LR interpretability and extend it through efficient computation of marginal and conditional distributions, while also enabling this higher level of interpretability for NNs. Our results demonstrate that tensorization is widely applicable and establishes a practical foundation for private, interpretable, and effective clinical prediction.

</details>


### [83] [Compressing LLMs with MoP: Mixture of Pruners](https://arxiv.org/abs/2602.06127)
*Bruno Lopes Yamamoto,Lucas Lauton de Alcantara,Victor Zacarias,Leandro Giusti Mugnaini,Keith Ando Ogawa,Lucas Pellicer,Rosimeire Pereira Costa,Edson Bollis,Anna Helena Reali Costa,Artur Jordao*

Main category: cs.LG

TL;DR: MoP是一个统一的模型剪枝框架，通过深度剪枝和宽度剪枝的混合迭代方法，在保持准确性的同时实现高效压缩和推理加速。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的高计算需求促使参数减少和推理加速方法的发展。现有剪枝方法通常只关注单一维度（深度或宽度），缺乏统一的框架。

Method: MoP采用迭代框架，在每次迭代中生成两个分支：深度剪枝和宽度剪枝，然后选择最佳候选方案推进。该方法可扩展到视觉语言模型。

Result: 在LLaMA-2和LLaMA-3上，MoP超越了现有结构化剪枝方法，在广泛的压缩范围内保持更高准确性。在40%压缩率下，端到端延迟降低39%。在LLaVA-1.5上的应用也显著提升了计算效率。

Conclusion: MoP提供了一个统一的剪枝框架，通过混合深度和宽度剪枝实现了更好的压缩-准确性权衡，并能转化为实际推理加速。仅使用文本恢复微调也能恢复视觉任务性能。

Abstract: The high computational demands of Large Language Models (LLMs) motivate methods that reduce parameter count and accelerate inference. In response, model pruning emerges as an effective strategy, yet current methods typically focus on a single dimension-depth or width. We introduce MoP (Mixture of Pruners), an iterative framework that unifies these dimensions. At each iteration, MoP generates two branches-pruning in depth versus pruning in width-and selects a candidate to advance the path. On LLaMA-2 and LLaMA-3, MoP advances the frontier of structured pruning, exceeding the accuracy of competing methods across a broad set of compression regimes. It also consistently outperforms depth-only and width-only pruning. Furthermore, MoP translates structural pruning into real speedup, reducing end-to-end latency by 39% at 40% compression. Finally, extending MoP to the vision-language model LLaVA-1.5, we notably improve computational efficiency and demonstrate that text-only recovery fine-tuning can restore performance even on visual tasks.

</details>


### [84] [Urban Spatio-Temporal Foundation Models for Climate-Resilient Housing: Scaling Diffusion Transformers for Disaster Risk Prediction](https://arxiv.org/abs/2602.06129)
*Olaf Yunus Laitinen Imanov,Derya Umut Kulali,Taner Yilmaz*

Main category: cs.LG

TL;DR: Skjold-DiT是一个基于扩散变换器的框架，用于预测建筑级气候风险指标，并整合交通网络结构和可达性信号，为智能车辆提供风险感知的路由约束。


<details>
  <summary>Details</summary>
Motivation: 气候灾害日益破坏城市交通和应急响应，损害住房存量、基础设施和网络可达性。需要预测建筑级风险并整合交通可达性信息，以支持智能车辆和应急调度系统。

Method: Skjold-DiT框架包含三个核心组件：Fjell-Prompt（支持跨城市迁移的提示条件接口）、Norrland-Fusion（统一灾害地图、建筑属性、人口统计和交通基础设施的跨模态注意力机制）、Valkyrie-Forecast（生成干预提示下概率风险轨迹的反事实模拟器）。

Result: 在包含847,392个建筑级观测的BCUR数据集上评估，实验验证了预测质量、跨城市泛化能力、校准效果，以及下游交通相关结果（包括可达性和灾害条件下的行程时间）。

Conclusion: Skjold-DiT能够生成校准的、不确定性感知的可达性层，为智能车辆路由和应急调度系统提供气候风险约束，提升城市交通系统的灾害韧性。

Abstract: Climate hazards increasingly disrupt urban transportation and emergency-response operations by damaging housing stock, degrading infrastructure, and reducing network accessibility. This paper presents Skjold-DiT, a diffusion-transformer framework that integrates heterogeneous spatio-temporal urban data to forecast building-level climate-risk indicators while explicitly incorporating transportation-network structure and accessibility signals relevant to intelligent vehicles (e.g., emergency reachability and evacuation-route constraints). Concretely, Skjold-DiT enables hazard-conditioned routing constraints by producing calibrated, uncertainty-aware accessibility layers (reachability, travel-time inflation, and route redundancy) that can be consumed by intelligent-vehicle routing and emergency dispatch systems. Skjold-DiT combines: (1) Fjell-Prompt, a prompt-based conditioning interface designed to support cross-city transfer; (2) Norrland-Fusion, a cross-modal attention mechanism unifying hazard maps/imagery, building attributes, demographics, and transportation infrastructure into a shared latent representation; and (3) Valkyrie-Forecast, a counterfactual simulator for generating probabilistic risk trajectories under intervention prompts. We introduce the Baltic-Caspian Urban Resilience (BCUR) dataset with 847,392 building-level observations across six cities, including multi-hazard annotations (e.g., flood and heat indicators) and transportation accessibility features. Experiments evaluate prediction quality, cross-city generalization, calibration, and downstream transportation-relevant outcomes, including reachability and hazard-conditioned travel times under counterfactual interventions.

</details>


### [85] [Self-Improving World Modelling with Latent Actions](https://arxiv.org/abs/2602.06130)
*Yifu Qiu,Zheng Zhao,Waylon Li,Yftah Ziser,Anna Korhonen,Shay B. Cohen,Edoardo M. Ponti*

Main category: cs.LG

TL;DR: SWIRL是一种自改进框架，通过将动作作为隐变量，从仅状态序列中学习世界模型，无需昂贵的动作标注轨迹。


<details>
  <summary>Details</summary>
Motivation: 学习世界模型（预测状态转换）通常需要昂贵的动作标注轨迹。本文旨在从仅状态序列中学习世界模型，降低数据标注成本。

Method: 提出SWIRL框架，将动作作为隐变量，交替训练前向世界模型P_θ(Y|X,Z)和逆动力学模型Q_φ(Z|X,Y)。通过变分信息最大化和ELBO最大化两个阶段迭代优化，使用GRPO强化学习训练。

Result: 在多个环境中评估：单轮和多轮开放世界视觉动态、合成文本环境（物理、网络、工具调用）。在AURORABench提升16%，ByteMorph提升28%，WorldPredictionBench提升16%，StableToolBench提升14%。

Conclusion: SWIRL能够从仅状态序列中有效学习世界模型，为LLMs和VLMs提供了一种无需动作标注的自改进框架，在多个基准测试中取得显著性能提升。

Abstract: Internal modelling of the world -- predicting transitions between previous states $X$ and next states $Y$ under actions $Z$ -- is essential to reasoning and planning for LLMs and VLMs. Learning such models typically requires costly action-labelled trajectories. We propose SWIRL, a self-improvement framework that learns from state-only sequences by treating actions as a latent variable and alternating between Forward World Modelling (FWM) $P_θ(Y|X,Z)$ and an Inverse Dynamics Modelling (IDM) $Q_φ(Z|X,Y)$. SWIRL iterates two phases: (1) Variational Information Maximisation, which updates the FWM to generate next states that maximise conditional mutual information with latent actions given prior states, encouraging identifiable consistency; and (2) ELBO Maximisation, which updates the IDM to explain observed transitions, effectively performing coordinate ascent. Both models are trained with reinforcement learning (specifically, GRPO) with the opposite frozen model's log-probability as a reward signal. We provide theoretical learnability guarantees for both updates, and evaluate SWIRL on LLMs and VLMs across multiple environments: single-turn and multi-turn open-world visual dynamics and synthetic textual environments for physics, web, and tool calling. SWIRL achieves gains of 16% on AURORABench, 28% on ByteMorph, 16% on WorldPredictionBench, and 14% on StableToolBench.

</details>


### [86] [Tempora: Characterising the Time-Contingent Utility of Online Test-Time Adaptation](https://arxiv.org/abs/2602.06136)
*Sudarshan Sreeram,Young D. Kwon,Cecilia Mascolo*

Main category: cs.LG

TL;DR: Tempora框架首次系统评估测试时自适应(TTA)在时间压力下的表现，揭示传统评估忽略的准确性-延迟权衡，发现方法排名在时间约束下会反转。


<details>
  <summary>Details</summary>
Motivation: 现有TTA评估假设无限处理时间，忽略了实际部署中的时间压力。在延迟敏感的应用场景中，过晚的预测毫无价值，需要评估准确性-延迟的权衡。

Method: 提出Tempora框架，包含：1)建模部署约束的时间场景；2)操作化测量的评估协议；3)量化准确性-延迟权衡的时间相关效用指标。具体实现三种指标：离散效用(硬截止时间)、连续效用(延迟衰减价值)、摊销效用(预算约束)。

Result: 在ImageNet-C上对7种TTA方法进行240次时间评估，发现：1)传统排名无法预测时间压力下的排名；2)传统最优方法ETA在41.2%评估中表现不佳；3)最佳方法随损坏类型和时间压力变化，没有明确赢家。

Conclusion: Tempora首次实现了跨不同时间约束的系统评估，揭示了排名反转的条件和原因，为从业者提供方法选择视角，为研究者提供可部署自适应研究目标。

Abstract: Test-time adaptation (TTA) offers a compelling remedy for machine learning (ML) models that degrade under domain shifts, improving generalisation on-the-fly with only unlabelled samples. This flexibility suits real deployments, yet conventional evaluations unrealistically assume unbounded processing time, overlooking the accuracy-latency trade-off. As ML increasingly underpins latency-sensitive and user-facing use-cases, temporal pressure constrains the viability of adaptable inference; predictions arriving too late to act on are futile. We introduce Tempora, a framework for evaluating TTA under this pressure. It consists of temporal scenarios that model deployment constraints, evaluation protocols that operationalise measurement, and time-contingent utility metrics that quantify the accuracy-latency trade-off. We instantiate the framework with three such metrics: (1) discrete utility for asynchronous streams with hard deadlines, (2) continuous utility for interactive settings where value decays with latency, and (3) amortised utility for budget-constrained deployments. Applying Tempora to seven TTA methods on ImageNet-C across 240 temporal evaluations reveals rank instability: conventional rankings do not predict rankings under temporal pressure; ETA, a state-of-the-art method in the conventional setting, falls short in 41.2% of evaluations. The highest-utility method varies with corruption type and temporal pressure, with no clear winner. By enabling systematic evaluation across diverse temporal constraints for the first time, Tempora reveals when and why rankings invert, offering practitioners a lens for method selection and researchers a target for deployable adaptation.

</details>


### [87] [Flow Matching for Offline Reinforcement Learning with Discrete Actions](https://arxiv.org/abs/2602.06138)
*Fairoz Nower Khan,Nabuat Zaman Nahim,Ruiquan Huang,Haibo Yang,Peizhong Ju*

Main category: cs.LG

TL;DR: 提出了一种基于连续时间马尔可夫链的离散动作空间流匹配框架，支持多目标和多智能体设置，通过Q加权流匹配目标训练，在多种离线RL场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散模型和流匹配的生成策略主要局限于连续动作空间，无法广泛应用于各种离线强化学习场景。需要开发一个支持离散动作空间和多目标的通用框架。

Method: 将连续流替换为连续时间马尔可夫链，使用Q加权流匹配目标进行训练。针对多智能体设置，通过因子化条件路径缓解联合动作空间的指数增长。还可以通过动作量化应用于连续控制问题。

Result: 理论证明在理想条件下优化该目标可以恢复最优策略。实验表明方法在高维控制、多模态决策、动态变化的多目标偏好等实际场景中表现稳健。离散框架通过动作量化在连续控制问题上也能提供灵活的性能-复杂度权衡。

Conclusion: 提出的离散流匹配框架成功扩展了生成策略方法的应用范围，使其能够处理离散动作空间、多目标和多智能体设置，为离线强化学习提供了更通用的解决方案。

Abstract: Generative policies based on diffusion models and flow matching have shown strong promise for offline reinforcement learning (RL), but their applicability remains largely confined to continuous action spaces. To address a broader range of offline RL settings, we extend flow matching to a general framework that supports discrete action spaces with multiple objectives. Specifically, we replace continuous flows with continuous-time Markov chains, trained using a Q-weighted flow matching objective. We then extend our design to multi-agent settings, mitigating the exponential growth of joint action spaces via a factorized conditional path. We theoretically show that, under idealized conditions, optimizing this objective recovers the optimal policy. Extensive experiments further demonstrate that our method performs robustly in practical scenarios, including high-dimensional control, multi-modal decision-making, and dynamically changing preferences over multiple objectives. Our discrete framework can also be applied to continuous-control problems through action quantization, providing a flexible trade-off between representational complexity and performance.

</details>


### [88] [Optimistic Training and Convergence of Q-Learning -- Extended Version](https://arxiv.org/abs/2602.06146)
*Prashant Mehta,Sean Meyn*

Main category: cs.LG

TL;DR: 本文扩展了线性函数逼近Q学习的稳定性分析，指出在非表格或线性MDP设置下，即使基函数理想，投影贝尔曼方程也可能存在多个解，导致算法不稳定。


<details>
  <summary>Details</summary>
Motivation: 先前研究表明，在(ε,κ)-驯服Gibbs策略下，线性函数逼近的Q学习具有稳定性（参数估计有界），且投影贝尔曼方程有解。但未解决解的唯一性问题，也未给出在标准表格或线性MDP设置之外的收敛准则。

Method: 通过一维示例分析，展示在无意识训练策略下，投影贝尔曼方程可能无解或多解，且算法不稳定。进一步证明即使基函数理想（真实Q函数在其张成空间中），贪婪策略下投影贝尔曼方程仍可能存在两个解。

Result: 发现需要更多结构才能保证收敛。即使基函数理想，在贪婪策略和(ε,κ)-驯服Gibbs策略（对所有足够小的ε>0和κ≥1）下，投影贝尔曼方程仍可能存在多个解，导致算法不稳定。

Conclusion: 线性函数逼近Q学习的收敛需要比先前假设更强的结构条件。即使基函数能够完美表示真实Q函数，投影贝尔曼方程的多解问题仍可能导致算法不稳定，这澄清了先前工作的局限性。

Abstract: In recent work it is shown that Q-learning with linear function approximation is stable, in the sense of bounded parameter estimates, under the $(\varepsilon,κ)$-tamed Gibbs policy; $κ$ is inverse temperature, and $\varepsilon>0$ is introduced for additional exploration. Under these assumptions it also follows that there is a solution to the projected Bellman equation (PBE). Left open is uniqueness of the solution, and criteria for convergence outside of the standard tabular or linear MDP settings.
  The present work extends these results to other variants of Q-learning, and clarifies prior work: a one dimensional example shows that under an oblivious policy for training there may be no solution to the PBE, or multiple solutions, and in each case the algorithm is not stable under oblivious training.
  The main contribution is that far more structure is required for convergence. An example is presented for which the basis is ideal, in the sense that the true Q-function is in the span of the basis. However, there are two solutions to the PBE under the greedy policy, and hence also for the $(\varepsilon,κ)$-tamed Gibbs policy for all sufficiently small $\varepsilon>0$ and $κ\ge 1$.

</details>


### [89] [MoSE: Mixture of Slimmable Experts for Efficient and Adaptive Language Models](https://arxiv.org/abs/2602.06154)
*Nurbek Tastan,Stefanos Laskaridis,Karthik Nandakumar,Samuel Horvath*

Main category: cs.LG

TL;DR: MoSE（Mixture of Slimmable Experts）是一种改进的MoE架构，通过让每个专家具有可调整宽度的嵌套结构，实现更精细的条件计算，从而在推理时提供更连续的精度-计算权衡谱。


<details>
  <summary>Details</summary>
Motivation: 传统MoE模型虽然通过稀疏激活专家来高效扩展大语言模型，但一旦专家被选中就会完全执行，导致精度与计算之间的权衡存在较大的不连续性。需要一种能够在推理时提供更连续精度-计算权衡的架构。

Method: 提出MoSE架构，每个专家具有可调整宽度的嵌套结构（slimmable结构），支持条件计算不仅选择激活哪些专家，还控制每个专家的使用程度。提出结合多宽度训练和标准MoE目标的训练方法，以及运行时宽度确定策略，包括轻量级测试时训练机制来根据路由器置信度映射专家宽度。

Result: 在OpenWebText上训练的GPT模型实验中，MoSE在全宽度下匹配或优于标准MoE，并持续改进精度与成本的Pareto前沿，在显著减少FLOPs的情况下实现可比性能。

Conclusion: MoSE通过引入可调整宽度的专家结构，为MoE模型提供了更精细的条件计算能力，使得单个预训练模型能够在推理时支持更连续的精度-计算权衡，提高了计算效率。

Abstract: Mixture-of-Experts (MoE) models scale large language models efficiently by sparsely activating experts, but once an expert is selected, it is executed fully. Hence, the trade-off between accuracy and computation in an MoE model typically exhibits large discontinuities. We propose Mixture of Slimmable Experts (MoSE), an MoE architecture in which each expert has a nested, slimmable structure that can be executed at variable widths. This enables conditional computation not only over which experts are activated, but also over how much of each expert is utilized. Consequently, a single pretrained MoSE model can support a more continuous spectrum of accuracy-compute trade-offs at inference time. We present a simple and stable training recipe for slimmable experts under sparse routing, combining multi-width training with standard MoE objectives. During inference, we explore strategies for runtime width determination, including a lightweight test-time training mechanism that learns how to map router confidence/probabilities to expert widths under a fixed budget. Experiments on GPT models trained on OpenWebText demonstrate that MoSE matches or improves upon standard MoE at full width and consistently shifts the Pareto frontier for accuracy vs. cost, achieving comparable performance with significantly fewer FLOPs.

</details>


### [90] [Latent Structure Emergence in Diffusion Models via Confidence-Based Filtering](https://arxiv.org/abs/2602.06155)
*Wei Wei,Yizhou Zeng,Kuntian Chen,Sophie Langer,Mariia Seleznova,Hung-Hsu Chou*

Main category: cs.LG

TL;DR: 扩散模型的初始噪声空间在筛选高置信度样本时展现出类可分性结构


<details>
  <summary>Details</summary>
Motivation: 研究扩散模型的高维初始噪声空间是否包含足够结构来预测生成样本的类别属性

Method: 通过预训练分类器对生成样本分配置信度分数，分析不同置信度子集中的噪声空间结构，比较类可预测性和类可分性

Result: 当考虑所有噪声实现时，潜在空间基本无结构；但限制在产生高置信度样本的初始噪声种子时，展现出明显的类可分性

Conclusion: 潜在空间中存在与类别相关的结构，但仅在基于置信度的筛选下才可观测，这为条件生成提供了基于筛选的替代方法

Abstract: Diffusion models rely on a high-dimensional latent space of initial noise seeds, yet it remains unclear whether this space contains sufficient structure to predict properties of the generated samples, such as their classes. In this work, we investigate the emergence of latent structure through the lens of confidence scores assigned by a pre-trained classifier to generated samples. We show that while the latent space appears largely unstructured when considering all noise realizations, restricting attention to initial noise seeds that produce high-confidence samples reveals pronounced class separability. By comparing class predictability across noise subsets of varying confidence and examining the class separability of the latent space, we find evidence of class-relevant latent structure that becomes observable only under confidence-based filtering. As a practical implication, we discuss how confidence-based filtering enables conditional generation as an alternative to guidance-based methods.

</details>


### [91] [Statistical Learning from Attribution Sets](https://arxiv.org/abs/2602.06276)
*Lorne Applebaum,Robert Busa-Fekete,August Y. Chen,Claudio Gentile,Tomer Koren,Aryan Mokhtari*

Main category: cs.LG

TL;DR: 提出一种在隐私约束下训练广告转化预测模型的方法，当点击与转化之间无法直接关联时，通过粗粒度归因集合进行无偏估计


<details>
  <summary>Details</summary>
Motivation: 解决隐私保护浏览器API和第三方cookie弃用带来的挑战，当广告点击与转化之间无法直接链接时，如何训练有效的转化预测模型

Method: 使用归因集合（而非单一来源）作为输入，构建无偏估计器来估计总体损失，通过经验风险最小化实现泛化保证

Result: 理论证明该方法在归因集合信息量和先验分布估计误差方面具有鲁棒性，实证评估显示在大型或重叠归因集合场景下显著优于行业启发式方法

Conclusion: 提出了一种在隐私约束下通过粗粒度归因信号训练转化预测模型的有效方法，为后cookie时代的广告归因问题提供了理论保证和实用解决方案

Abstract: We address the problem of training conversion prediction models in advertising domains under privacy constraints, where direct links between ad clicks and conversions are unavailable. Motivated by privacy-preserving browser APIs and the deprecation of third-party cookies, we study a setting where the learner observes a sequence of clicks and a sequence of conversions, but can only link a conversion to a set of candidate clicks (an attribution set) rather than a unique source. We formalize this as learning from attribution sets generated by an oblivious adversary equipped with a prior distribution over the candidates. Despite the lack of explicit labels, we construct an unbiased estimator of the population loss from these coarse signals via a novel approach. Leveraging this estimator, we show that Empirical Risk Minimization achieves generalization guarantees that scale with the informativeness of the prior and is also robust against estimation errors in the prior, despite complex dependencies among attribution sets. Simple empirical evaluations on standard datasets suggest our unbiased approach significantly outperforms common industry heuristics, particularly in regimes where attribution sets are large or overlapping.

</details>


### [92] [SCONE: A Practical, Constraint-Aware Plug-in for Latent Encoding in Learned DNA Storage](https://arxiv.org/abs/2602.06157)
*Cihan Ruan,Lebin Zhou,Rongduo Han,Linyi Han,Bingqing Zhao,Chenchen Zhu,Wei Jiang,Wei Wang,Nam Ling*

Main category: cs.LG

TL;DR: SCONE是一个将神经压缩与DNA编码结合的插件模块，通过四进制算术编码直接在DNA碱基空间操作，动态调整熵编码概率分布以满足生化约束，实现了高效的单步DNA存储编码。


<details>
  <summary>Details</summary>
Motivation: 当前DNA存储与神经压缩管道集成效率低下：传统方法在原始二进制数据上应用冗余约束层，而现代神经编解码器将压缩后的潜在表示通过简单的二进制到四进制转码转换为DNA，丢弃了熵模型的优化，这种不匹配降低了压缩效率并复杂化了编码堆栈。

Method: SCONE采用四进制算术编码直接在潜在空间进行DNA碱基编码。其约束感知自适应编码模块动态引导熵编码器的学习概率分布，在编码过程中确定性地强制执行生化约束（GC平衡和同聚物抑制），无需后处理校正。该设计保持完全可逆性，并利用超先验模型的学习先验而无需修改。

Result: 实验显示SCONE实现了近乎完美的约束满足，计算开销可忽略不计（<2%延迟），为端到端DNA兼容学习编解码器建立了潜在无关的接口。

Conclusion: SCONE将潜在压缩和DNA编码合并为单一步骤，解决了神经压缩与DNA存储之间的集成效率问题，为高效DNA存储系统提供了实用的解决方案。

Abstract: DNA storage has matured from concept to practical stage, yet its integration with neural compression pipelines remains inefficient. Early DNA encoders applied redundancy-heavy constraint layers atop raw binary data - workable but primitive. Recent neural codecs compress data into learned latent representations with rich statistical structure, yet still convert these latents to DNA via naive binary-to-quaternary transcoding, discarding the entropy model's optimization. This mismatch undermines compression efficiency and complicates the encoding stack. A plug-in module that collapses latent compression and DNA encoding into a single step. SCONE performs quaternary arithmetic coding directly on the latent space in DNA bases. Its Constraint-Aware Adaptive Coding module dynamically steers the entropy encoder's learned probability distribution to enforce biochemical constraints - Guanine-Cytosine (GC) balance and homopolymer suppression - deterministically during encoding, eliminating post-hoc correction. The design preserves full reversibility and exploits the hyperprior model's learned priors without modification. Experiments show SCONE achieves near-perfect constraint satisfaction with negligible computational overhead (<2% latency), establishing a latent-agnostic interface for end-to-end DNA-compatible learned codecs.

</details>


### [93] [Which Graph Shift Operator? A Spectral Answer to an Empirical Question](https://arxiv.org/abs/2602.06557)
*Yassine Abbahaddou*

Main category: cs.LG

TL;DR: 提出一种基于对齐增益的度量标准，用于在训练前选择最优图移位算子，避免经验性搜索


<details>
  <summary>Details</summary>
Motivation: 当前GNN中图移位算子的选择主要依赖经验，缺乏理论指导，需要一种原则性的选择方法

Method: 引入对齐增益度量来量化输入信号与标签子空间之间的几何失真，通过谱代理连接Lipschitz常数与泛化边界

Result: 开发出计算高效的准则，可在训练前对任何预测任务排序和选择最优图移位算子

Conclusion: 提供了一种理论指导的、无需大量搜索的图移位算子选择方法，提高了GNN设计的效率和性能

Abstract: Graph Neural Networks (GNNs) have established themselves as the leading models for learning on graph-structured data, generally categorized into spatial and spectral approaches. Central to these architectures is the Graph Shift Operator (GSO), a matrix representation of the graph structure used to filter node signals. However, selecting the optimal GSO, whether fixed or learnable, remains largely empirical. In this paper, we introduce a novel alignment gain metric that quantifies the geometric distortion between the input signal and label subspaces. Crucially, our theoretical analysis connects this alignment directly to generalization bounds via a spectral proxy for the Lipschitz constant. This yields a principled, computation-efficient criterion to rank and select the optimal GSO for any prediction task prior to training, eliminating the need for extensive search.

</details>


### [94] [To 2:4 Sparsity and Beyond: Neuron-level Activation Function to Accelerate LLM Pre-Training](https://arxiv.org/abs/2602.06183)
*Meghana Madhyastha,Daniel Haziza,Jesse Cai,Newsha Ardalani,Zhiqi Bu,Carole-Jean Wu*

Main category: cs.LG

TL;DR: 利用硬件加速稀疏性加速Transformer FFN中的矩阵乘法，通过稀疏训练和密集训练结合，实现1.4-1.7倍训练加速，保持模型性能不变


<details>
  <summary>Details</summary>
Motivation: 大型语言模型训练通常受限于矩阵乘法运算，在Transformer架构中，FFN占据了大量计算（最高达预训练总浮点运算的50%），需要寻找加速方法

Method: 采用硬件加速稀疏性技术：权重使用2:4稀疏模式，激活使用v:n:m（Venom）稀疏模式；结合稀疏训练（大部分预训练阶段）和密集训练（训练后期）的混合训练策略

Result: 该方法在质量基准测试中保持相同性能，端到端训练速度提升1.4-1.7倍；适用于A100及以后的NVIDIA GPU，与量化等技术正交，也可应用于MoE架构

Conclusion: 通过硬件加速稀疏性有效加速FFN矩阵乘法，实现显著训练加速而不损失模型质量，为大规模语言模型训练提供了实用的优化方案

Abstract: Trainings of Large Language Models are generally bottlenecked by matrix multiplications. In the Transformer architecture, a large portion of these operations happens in the Feed Forward Network (FFN), and this portion increases for larger models, up to 50% of the total pretraining floating point operations. We show that we can leverage hardware-accelerated sparsity to accelerate all matrix multiplications in the FFN, with 2:4 sparsity for weights and v:n:m (Venom) sparsity for activations. Our recipe relies on sparse training steps to accelerate a large part of the pretraining, associated with regular dense training steps towards the end. Overall, models trained with this approach exhibit the same performance on our quality benchmarks, and can speed up training end-to-end by 1.4 to 1.7x. This approach is applicable to all NVIDIA GPUs starting with the A100 generation, and is orthogonal to common optimization techniques, such as, quantization, and can also be applied to mixture-of-experts model architectures.

</details>


### [95] [$f$-FUM: Federated Unlearning via min--max and $f$-divergence](https://arxiv.org/abs/2602.06187)
*Radmehr Karimian,Amirhossein Bagheri,Meghdad Kurmanji,Nicholas D. Lane,Gholamali Aminian*

Main category: cs.LG

TL;DR: 提出一个联邦学习中的去学习框架，通过最小化优化问题来高效移除特定数据点的影响，同时保持模型性能


<details>
  <summary>Details</summary>
Motivation: 联邦学习需要满足"被遗忘权"等法律伦理要求，并防御数据投毒攻击，但分布式特性使得移除个体数据贡献变得复杂

Method: 提出基于min-max优化的联邦去学习框架，最大化包含所有数据训练的模型与移除特定数据点后重新训练模型之间的f-散度，同时最小化对保留数据的性能影响

Result: 该方法相比朴素重新训练显著加速，对模型效用影响最小，可作为插件应用于各种联邦学习设置

Conclusion: 提出的联邦去学习框架能够高效近似数据移除效果，满足隐私保护和安全性需求，具有广泛的适用性

Abstract: Federated Learning (FL) has emerged as a powerful paradigm for collaborative machine learning across decentralized data sources, preserving privacy by keeping data local. However, increasing legal and ethical demands, such as the "right to be forgotten", and the need to mitigate data poisoning attacks have underscored the urgent necessity for principled data unlearning in FL. Unlike centralized settings, the distributed nature of FL complicates the removal of individual data contributions. In this paper, we propose a novel federated unlearning framework formulated as a min-max optimization problem, where the objective is to maximize an $f$-divergence between the model trained with all data and the model retrained without specific data points, while minimizing the degradation on retained data. Our framework could act like a plugin and be added to almost any federated setup, unlike SOTA methods like (\cite{10269017} which requires model degradation in server, or \cite{khalil2025notfederatedunlearningweight} which requires to involve model architecture and model weights). This formulation allows for efficient approximation of data removal effects in a federated setting. We provide empirical evaluations to show that our method achieves significant speedups over naive retraining, with minimal impact on utility.

</details>


### [96] [On the Convergence of Multicalibration Gradient Boosting](https://arxiv.org/abs/2602.06773)
*Daniel Haimovich,Fridolin Linder,Lorenzo Perini,Niek Tax,Milan Vojnovic*

Main category: cs.LG

TL;DR: 本文为多校准梯度提升在回归问题中的收敛性提供了理论保证，证明了预测更新的幅度以O(1/√T)衰减，在弱学习器满足平滑性假设时可达到线性收敛。


<details>
  <summary>Details</summary>
Motivation: 多校准梯度提升在实践中已被大规模部署并产生近似多校准的预测器，但其收敛性质尚未得到充分理解。本文旨在填补这一理论空白。

Method: 分析多校准梯度提升在平方误差损失回归问题中的收敛性，研究预测更新幅度的衰减速率，并在弱学习器满足平滑性假设时改进收敛速率。同时分析自适应变体和缩放方案。

Result: 证明多校准梯度提升的预测更新幅度以O(1/√T)衰减，多校准误差具有相同的收敛速率界。在弱学习器平滑性假设下，收敛速率可提升至线性收敛。自适应变体可实现训练损失的局部二次收敛。

Conclusion: 本文为多校准梯度提升提供了首个收敛性理论保证，解释了其在实际部署中的成功表现，并通过实验验证了理论结果，阐明了该方法实现快速收敛和强多校准的条件。

Abstract: Multicalibration gradient boosting has recently emerged as a scalable method that empirically produces approximately multicalibrated predictors and has been deployed at web scale. Despite this empirical success, its convergence properties are not well understood. In this paper, we bridge the gap by providing convergence guarantees for multicalibration gradient boosting in regression with squared-error loss. We show that the magnitude of successive prediction updates decays at $O(1/\sqrt{T})$, which implies the same convergence rate bound for the multicalibration error over rounds. Under additional smoothness assumptions on the weak learners, this rate improves to linear convergence. We further analyze adaptive variants, showing local quadratic convergence of the training loss, and we study rescaling schemes that preserve convergence. Experiments on real-world datasets support our theory and clarify the regimes in which the method achieves fast convergence and strong multicalibration.

</details>


### [97] [PurSAMERE: Reliable Adversarial Purification via Sharpness-Aware Minimization of Expected Reconstruction Error](https://arxiv.org/abs/2602.06269)
*Vinh Hoang,Sebastian Krumscheid,Holger Rauhut,Raúl Tempone*

Main category: cs.LG

TL;DR: 提出一种确定性净化方法，通过将对抗样本映射到数据分布模式附近的样本，提高对抗鲁棒性。该方法避免随机性带来的问题，在强白盒攻击下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随机净化方法在对抗方完全了解系统和随机性的情况下，有效鲁棒性会下降。需要一种确定性方法来确保可靠的测试精度，同时提高对抗鲁棒性。

Method: 使用最小化噪声污染数据重构误差的分数模型学习数据分布结构。给定潜在对抗输入，在局部邻域搜索最小化噪声重构误差的净化样本，使用锐度感知最小化引导净化样本到重构误差景观的平坦区域。

Result: 实验结果表明，在强确定性白盒攻击下，该方法在对抗鲁棒性方面显著优于最先进方法。随着噪声水平降低，最小化重构误差使净化样本偏向高斯平滑密度的局部最大化器。

Conclusion: 提出的确定性净化方法通过将样本映射到数据分布模式附近，有效提高了对抗鲁棒性。理论分析表明在小噪声极限下能恢复局部最大化器，实验验证了其优越性能。

Abstract: We propose a novel deterministic purification method to improve adversarial robustness by mapping a potentially adversarial sample toward a nearby sample that lies close to a mode of the data distribution, where classifiers are more reliable. We design the method to be deterministic to ensure reliable test accuracy and to prevent the degradation of effective robustness observed in stochastic purification approaches when the adversary has full knowledge of the system and its randomness. We employ a score model trained by minimizing the expected reconstruction error of noise-corrupted data, thereby learning the structural characteristics of the input data distribution. Given a potentially adversarial input, the method searches within its local neighborhood for a purified sample that minimizes the expected reconstruction error under noise corruption and then feeds this purified sample to the classifier. During purification, sharpness-aware minimization is used to guide the purified samples toward flat regions of the expected reconstruction error landscape, thereby enhancing robustness. We further show that, as the noise level decreases, minimizing the expected reconstruction error biases the purified sample toward local maximizers of the Gaussian-smoothed density; under additional local assumptions on the score model, we prove recovery of a local maximizer in the small-noise limit. Experimental results demonstrate significant gains in adversarial robustness over state-of-the-art methods under strong deterministic white-box attacks.

</details>


### [98] [Learning Rate Scaling across LoRA Ranks and Transfer to Full Finetuning](https://arxiv.org/abs/2602.06204)
*Nan Chen,Soledad Villar,Soufiane Hayou*

Main category: cs.LG

TL;DR: 提出Maximal-Update Adaptation (μA)框架，用于确定LoRA微调中学习率随适配器秩的最优缩放规则，发现两种不同缩放模式，并实现从LoRA到全参数微调的学习率迁移。


<details>
  <summary>Details</summary>
Motivation: LoRA作为参数高效微调的标准工具，其训练动态复杂且依赖多个超参数，特别是学习率如何随适配器秩缩放不明确，导致每次改变秩都需要重新调整学习率，增加了调优成本。

Method: 提出Maximal-Update Adaptation (μA)理论框架，受预训练中Maximal-Update Parametrization (μP)启发，利用超参数迁移技术分析学习率随模型宽度和适配器秩的最优缩放模式。

Result: 识别出两种学习率缩放模式：一种随秩基本不变，另一种与秩成反比缩放。找到了从LoRA到全参数微调的学习率迁移配置，大幅降低全参数微调的学习率调优成本。在语言、视觉、视觉-语言、图像生成和强化学习任务上的实验验证了缩放规则的有效性。

Conclusion: μA框架为LoRA微调提供了理论指导，明确了学习率随适配器秩的最优缩放规则，实现了学习率从LoRA到全参数微调的可靠迁移，显著减少了超参数调优的开销。

Abstract: Low-Rank Adaptation (LoRA) is a standard tool for parameter-efficient finetuning of large models. While it induces a small memory footprint, its training dynamics can be surprisingly complex as they depend on several hyperparameters such as initialization, adapter rank, and learning rate. In particular, it is unclear how the optimal learning rate scales with adapter rank, which forces practitioners to re-tune the learning rate whenever the rank is changed. In this paper, we introduce Maximal-Update Adaptation ($μ$A), a theoretical framework that characterizes how the "optimal" learning rate should scale with model width and adapter rank to produce stable, non-vanishing feature updates under standard configurations. $μ$A is inspired from the Maximal-Update Parametrization ($μ$P) in pretraining. Our analysis leverages techniques from hyperparameter transfer and reveals that the optimal learning rate exhibits different scaling patterns depending on initialization and LoRA scaling factor. Specifically, we identify two regimes: one where the optimal learning rate remains roughly invariant across ranks, and another where it scales inversely with rank. We further identify a configuration that allows learning rate transfer from LoRA to full finetuning, drastically reducing the cost of learning rate tuning for full finetuning. Experiments across language, vision, vision--language, image generation, and reinforcement learning tasks validate our scaling rules and show that learning rates tuned on LoRA transfer reliably to full finetuning.

</details>


### [99] [Robust Online Learning](https://arxiv.org/abs/2602.06775)
*Sajad Ashkezari*

Main category: cs.LG

TL;DR: 论文研究在对抗性扰动下的鲁棒分类学习问题，其中数据和标签都是对抗性选择的，提出了新的维度概念来控制错误界和遗憾界。


<details>
  <summary>Details</summary>
Motivation: 现有鲁棒PAC学习研究假设干净数据和标签是良性的，而实际中攻击者可能同时操纵干净数据和扰动。本文研究更现实的对抗场景，其中数据和标签都是对抗性选择的。

Method: 将问题建模为在线学习问题，定义新的维度概念（类似于Littlestone维度），分析可实现和不可知情况下的学习性，并将维度推广到多类别假设类。

Result: 提出的新维度能够控制可实现设置中的错误界和不可知设置中的遗憾界，与PAC学习中的维度不同，这个维度更简单且类似于Littlestone维度。还将结果扩展到多类别情况。

Conclusion: 论文为对抗性扰动下的鲁棒分类学习提供了新的理论框架，定义了控制学习性能的新维度，并扩展到更实际的场景（学习者不知道允许的扰动集）。

Abstract: We study the problem of learning robust classifiers where the classifier will receive a perturbed input. Unlike robust PAC learning studied in prior work, here the clean data and its label are also adversarially chosen. We formulate this setting as an online learning problem and consider both the realizable and agnostic learnability of hypothesis classes. We define a new dimension of classes and show it controls the mistake bounds in the realizable setting and the regret bounds in the agnostic setting. In contrast to the dimension that characterizes learnability in the PAC setting, our dimension is rather simple and resembles the Littlestone dimension. We generalize our dimension to multiclass hypothesis classes and prove similar results in the realizable case. Finally, we study the case where the learner does not know the set of allowed perturbations for each point and only has some prior on them.

</details>


### [100] [Achieving Better Local Regret Bound for Online Non-Convex Bilevel Optimization](https://arxiv.org/abs/2602.06457)
*Tingkai Jia,Haiguang Wang,Cheng Chen*

Main category: cs.LG

TL;DR: 本文为在线双层优化建立了最优后悔界，提出了针对标准双层局部后悔和窗口平均双层局部后悔的算法，分别达到Ω(1+V_T)和Ω(T/W^2)的最优后悔界。


<details>
  <summary>Details</summary>
Motivation: 在线双层优化已成为机器学习中的重要框架，但现有算法的后悔界是否最优尚不清楚。本文旨在为两种设置（标准双层局部后悔和窗口平均双层局部后悔）建立最优后悔界。

Method: 1. 针对标准双层局部后悔，提出算法达到最优后悔Ω(1+V_T)，最多使用O(T log T)次内层梯度评估；2. 进一步开发完全单循环算法，其后悔界包含额外的梯度变化项；3. 针对窗口平均双层局部后悔，设计基于窗口分析的算法，捕捉次线性环境变化，达到最优后悔Ω(T/W^2)。

Result: 1. 建立了标准双层局部后悔的最优后悔界Ω(1+V_T)；2. 开发了完全单循环算法；3. 建立了窗口平均双层局部后悔的最优后悔界Ω(T/W^2)；4. 实验验证了理论发现并证明了方法的实际有效性。

Conclusion: 本文为在线双层优化的两种主要设置建立了最优后悔界，提出的算法在理论上达到最优性能，并通过实验验证了实际效果，填补了该领域最优性分析的研究空白。

Abstract: Online bilevel optimization (OBO) has emerged as a powerful framework for many machine learning problems. Prior works have developed several algorithms that minimize the standard bilevel local regret or the window-averaged bilevel local regret of the OBO problem, but the optimality of existing regret bounds remains unclear. In this work, we establish optimal regret bounds for both settings. For standard bilevel local regret, we propose an algorithm that achieves the optimal regret $Ω(1+V_T)$ with at most $O(T\log T)$ total inner-level gradient evaluations. We further develop a fully single-loop algorithm whose regret bound includes an additional gradient-variation terms. For the window-averaged bilevel local regret, we design an algorithm that captures sublinear environmental variation through a window-based analysis and achieves the optimal regret $Ω(T/W^2)$. Experiments validate our theoretical findings and demonstrate the practical effectiveness of the proposed methods.

</details>


### [101] [Multi-Way Representation Alignment](https://arxiv.org/abs/2602.06205)
*Akshit Achara,Tatiana Gaintseva,Mateo Mahaut,Pritish Chakraborty,Viktor Stenby Johansson,Melih Barsbey,Emanuele Rodolà,Donato Crisostomi*

Main category: cs.LG

TL;DR: 提出GCPA方法，解决多模型对齐问题，在保持几何结构的同时优化检索性能


<details>
  <summary>Details</summary>
Motivation: 现有方法只能成对对齐模型，无法构建一致的全局参考空间，且几何保持与检索优化之间存在矛盾

Method: 1) 将广义Procrustes分析(GPA)扩展到多模型对齐；2) 提出几何校正Procrustes对齐(GCPA)，先建立GPA共享空间，再进行方向失配的后处理校正

Result: GCPA在保持实用共享参考空间的同时，持续提升任意到任意检索性能

Conclusion: GCPA有效解决了多模型对齐问题，平衡了几何保持与检索优化的需求

Abstract: The Platonic Representation Hypothesis suggests that independently trained neural networks converge to increasingly similar latent spaces. However, current strategies for mapping these representations are inherently pairwise, scaling quadratically with the number of models and failing to yield a consistent global reference. In this paper, we study the alignment of $M \ge 3$ models. We first adapt Generalized Procrustes Analysis (GPA) to construct a shared orthogonal universe that preserves the internal geometry essential for tasks like model stitching. We then show that strict isometric alignment is suboptimal for retrieval, where agreement-maximizing methods like Canonical Correlation Analysis (CCA) typically prevail. To bridge this gap, we finally propose Geometry-Corrected Procrustes Alignment (GCPA), which establishes a robust GPA-based universe followed by a post-hoc correction for directional mismatch. Extensive experiments demonstrate that GCPA consistently improves any-to-any retrieval while retaining a practical shared reference space.

</details>


### [102] [Learning Deep Hybrid Models with Sharpness-Aware Minimization](https://arxiv.org/abs/2602.06837)
*Naoya Takeishi*

Main category: cs.LG

TL;DR: 提出基于锐度感知最小化的混合建模方法，通过关注损失最小值的平坦性来避免科学模型被机器学习模型忽略的问题


<details>
  <summary>Details</summary>
Motivation: 混合建模结合机器学习模型和科学数学模型，但机器学习模型的灵活性可能导致科学模型在预测中被忽略，使混合建模失去意义。现有正则化方法依赖于模型架构和领域知识。

Method: 提出关注混合模型学习中损失最小值的平坦性，采用锐度感知最小化思想并适应混合建模设置，使模型尽可能简单

Result: 数值实验表明，基于SAM的方法在不同模型和数据集选择下都能良好工作

Conclusion: 通过关注损失最小值的平坦性，可以有效避免混合建模中科学模型被忽略的问题，提供了一种不依赖特定模型架构和领域知识的通用方法

Abstract: Hybrid modeling, the combination of machine learning models and scientific mathematical models, enables flexible and robust data-driven prediction with partial interpretability. However, effectively the scientific models may be ignored in prediction due to the flexibility of the machine learning model, making the idea of hybrid modeling pointless. Typically some regularization is applied to hybrid model learning to avoid such a failure case, but the formulation of the regularizer strongly depends on model architectures and domain knowledge. In this paper, we propose to focus on the flatness of loss minima in learning hybrid models, aiming to make the model as simple as possible. We employ the idea of sharpness-aware minimization and adapt it to the hybrid modeling setting. Numerical experiments show that the SAM-based method works well across different choices of models and datasets.

</details>


### [103] [Continuous-time reinforcement learning: ellipticity enables model-free value function approximation](https://arxiv.org/abs/2602.06930)
*Wenlong Mou*

Main category: cs.LG

TL;DR: 本文研究连续时间马尔可夫扩散过程的离策略强化学习，提出基于函数近似的Sobolev-prox fitted q-learning算法，证明椭圆性使扩散过程的强化学习难度不高于监督学习。


<details>
  <summary>Details</summary>
Motivation: 研究连续时间马尔可夫扩散过程的离策略强化学习，旨在解决离散时间观测和动作下的控制问题。现有方法常需要不现实的结构假设，本文希望建立无需此类假设的模型自由算法。

Method: 利用扩散过程的椭圆性，建立贝尔曼算子的希尔伯特空间正定性和有界性新性质。基于这些性质，提出Sobolev-prox fitted q-learning算法，通过迭代求解最小二乘回归问题学习价值和优势函数。

Result: 推导出估计误差的oracle不等式，误差由四个因素控制：函数类的最佳逼近误差、局部化复杂度、指数衰减的优化误差和数值离散化误差。证明椭圆性使扩散过程的强化学习难度不高于监督学习。

Conclusion: 椭圆性是马尔可夫扩散过程强化学习的关键结构性质，它使得基于函数近似的强化学习难度与监督学习相当，为连续时间扩散过程的控制提供了理论保证。

Abstract: We study off-policy reinforcement learning for controlling continuous-time Markov diffusion processes with discrete-time observations and actions. We consider model-free algorithms with function approximation that learn value and advantage functions directly from data, without unrealistic structural assumptions on the dynamics.
  Leveraging the ellipticity of the diffusions, we establish a new class of Hilbert-space positive definiteness and boundedness properties for the Bellman operators. Based on these properties, we propose the Sobolev-prox fitted $q$-learning algorithm, which learns value and advantage functions by iteratively solving least-squares regression problems. We derive oracle inequalities for the estimation error, governed by (i) the best approximation error of the function classes, (ii) their localized complexity, (iii) exponentially decaying optimization error, and (iv) numerical discretization error. These results identify ellipticity as a key structural property that renders reinforcement learning with function approximation for Markov diffusions no harder than supervised learning.

</details>


### [104] [Emergent Low-Rank Training Dynamics in MLPs with Smooth Activations](https://arxiv.org/abs/2602.06208)
*Alec S. Xu,Can Yaras,Matthew Asato,Qing Qu,Laura Balzano*

Main category: cs.LG

TL;DR: 论文分析表明深度神经网络训练动态发生在低维子空间中，理论证明了多层感知机权重动态集中在不变低维子空间，并提出了匹配全参数性能的低秩参数化方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究观察到大规模深度神经网络训练动态发生在低维子空间中，这启发了低秩训练、压缩和适应等新研究方向，但非线性网络中的理论解释仍然有限，需要填补这一理论空白。

Method: 分析多层感知机在梯度下降下的学习动态，理论刻画两层网络在光滑非线性激活函数下的不变子空间，并通过实验验证现象超出理论假设范围，最后提出基于适当子空间初始化的低秩MLP参数化方法。

Result: 理论证明权重动态在整个训练过程中集中在不变低维子空间内，实验验证该现象具有普遍性，提出的低秩MLP参数化在多种分类任务上能匹配全参数化模型的分类性能。

Conclusion: 深度神经网络训练确实发生在低维子空间中，理论分析为这一现象提供了数学基础，基于不变子空间的低秩参数化方法能够有效压缩模型而不损失性能，为高效神经网络训练和压缩提供了新思路。

Abstract: Recent empirical evidence has demonstrated that the training dynamics of large-scale deep neural networks occur within low-dimensional subspaces. While this has inspired new research into low-rank training, compression, and adaptation, theoretical justification for these dynamics in nonlinear networks remains limited. %compared to deep linear settings. To address this gap, this paper analyzes the learning dynamics of multi-layer perceptrons (MLPs) under gradient descent (GD). We demonstrate that the weight dynamics concentrate within invariant low-dimensional subspaces throughout training. Theoretically, we precisely characterize these invariant subspaces for two-layer networks with smooth nonlinear activations, providing insight into their emergence. Experimentally, we validate that this phenomenon extends beyond our theoretical assumptions. Leveraging these insights, we empirically show there exists a low-rank MLP parameterization that, when initialized within the appropriate subspaces, matches the classification performance of fully-parameterized counterparts on a variety of classification tasks.

</details>


### [105] [Vision Transformer Finetuning Benefits from Non-Smooth Components](https://arxiv.org/abs/2602.06883)
*Ambroise Odonnat,Laetitia Chapel,Romain Tavenard,Ievgen Redko*

Main category: cs.LG

TL;DR: 研究发现Transformer架构中注意力模块和前馈层的高塑性（对输入变化的高敏感性）有利于微调性能，这与传统认为平滑性更优的观点不同


<details>
  <summary>Details</summary>
Motivation: Transformer架构的平滑性在泛化、训练稳定性和对抗鲁棒性方面已有广泛研究，但其在迁移学习中的作用仍不清楚。本文旨在分析视觉Transformer组件适应输入变化的能力（即塑性）

Method: 通过理论分析和综合实验，研究视觉Transformer组件的塑性（定义为平均变化率，捕捉对输入扰动的敏感性）。高塑性意味着低平滑性

Result: 研究发现注意力模块和前馈层的高塑性能够带来更好的微调性能。这为实践中选择优先适应的组件提供了原则性指导

Conclusion: 研究挑战了平滑性更优的主流假设，为Transformer的功能特性提供了新视角。高塑性的组件在迁移学习中表现更好

Abstract: The smoothness of the transformer architecture has been extensively studied in the context of generalization, training stability, and adversarial robustness. However, its role in transfer learning remains poorly understood. In this paper, we analyze the ability of vision transformer components to adapt their outputs to changes in inputs, or, in other words, their plasticity. Defined as an average rate of change, it captures the sensitivity to input perturbation; in particular, a high plasticity implies low smoothness. We demonstrate through theoretical analysis and comprehensive experiments that this perspective provides principled guidance in choosing the components to prioritize during adaptation. A key takeaway for practitioners is that the high plasticity of the attention modules and feedforward layers consistently leads to better finetuning performance. Our findings depart from the prevailing assumption that smoothness is desirable, offering a novel perspective on the functional properties of transformers. The code is available at https://github.com/ambroiseodt/vit-plasticity.

</details>


### [106] [SR4-Fit: An Interpretable and Informative Classification Algorithm Applied to Prediction of U.S. House of Representatives Elections](https://arxiv.org/abs/2602.06229)
*Shyam Sundar Murali Krishnan,Dean Frederick Hougen*

Main category: cs.LG

TL;DR: SR4-Fit是一种新型可解释分类算法，在保持高预测性能的同时解决了传统规则算法预测能力不足和不稳定的问题，在选举预测等多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 机器学习在关键应用中需要可解释模型，但高性能模型通常是"黑盒"系统，而传统规则算法如RuleFit虽然简单但预测能力不足且不稳定，这促使开发SR4-Fit来解决这些限制。

Method: SR4-Fit（稀疏松弛正则化回归规则拟合）是一种新颖的可解释分类算法，使用美国国会选区的人口特征数据预测众议院选举结果，并在六个公开分类数据集上进一步验证。

Result: SR4-Fit在预测众议院选举结果方面达到了前所未有的准确性和可解释性，揭示了黑盒算法无法解释的人口因素组合，在准确性、简单性和鲁棒性方面均超越黑盒模型和现有规则算法。

Conclusion: SR4-Fit解决了模型可解释性和预测能力之间的传统权衡，能够生成稳定可解释的规则集同时保持优异的预测性能，为关键应用提供了理想的解决方案。

Abstract: The growth of machine learning demands interpretable models for critical applications, yet most high-performing models are ``black-box'' systems that obscure input-output relationships, while traditional rule-based algorithms like RuleFit suffer from a lack of predictive power and instability despite their simplicity. This motivated our development of Sparse Relaxed Regularized Regression Rule-Fit (SR4-Fit), a novel interpretable classification algorithm that addresses these limitations while maintaining superior classification performance. Using demographic characteristics of U.S. congressional districts from the Census Bureau's American Community Survey, we demonstrate that SR4-Fit can predict House election party outcomes with unprecedented accuracy and interpretability. Our results show that while the majority party remains the strongest predictor, SR4-Fit has revealed intrinsic combinations of demographic factors that affect prediction outcomes that were unable to be interpreted in black-box algorithms such as random forests. The SR4-Fit algorithm surpasses both black-box models and existing interpretable rule-based algorithms such as RuleFit with respect to accuracy, simplicity, and robustness, generating stable and interpretable rule sets while maintaining superior predictive performance, thus addressing the traditional trade-off between model interpretability and predictive capability in electoral forecasting. To further validate SR4-Fit's performance, we also apply it to six additional publicly available classification datasets, like the breast cancer, Ecoli, page blocks, Pima Indians, vehicle, and yeast datasets, and find similar results.

</details>


### [107] [Sample Complexity of Causal Identification with Temporal Heterogeneity](https://arxiv.org/abs/2602.06899)
*Ameya Rathod,Sujay Belsare,Salvik Krishna Nautiyal,Dhruv Laad,Ponnurangam Kumaraguru*

Main category: cs.LG

TL;DR: 该论文提出了一种结合时间序列动态和多环境异质性来识别因果图的方法，分析了在不同噪声分布下的统计恢复极限。


<details>
  <summary>Details</summary>
Motivation: 从观测数据中恢复唯一因果图是一个不适定问题，因为多个生成机制可能导致相同的观测分布。现有研究分别利用时间序列动态或多环境异质性来约束这个问题，但尚未整合这两种互补的异质性来源。

Method: 整合时间序列动态和多环境异质性作为互补的异质性来源，推导统一的必要可识别性条件，分析高斯噪声和重尾（Student's t）分布下的统计恢复极限。

Result: 时间结构可以有效替代缺失的环境多样性，即使在异质性不足的情况下也可能实现可识别性。对于重尾分布，几何可识别性条件保持不变，但样本复杂度与高斯基线显著不同，信息论界限量化了这种稳健性成本。

Conclusion: 该工作将焦点从因果结构是否可识别转向其在实践中是否可统计恢复，为现实非平稳系统中的因果图恢复方法建立了基本极限。

Abstract: Recovering a unique causal graph from observational data is an ill-posed problem because multiple generating mechanisms can lead to the same observational distribution. This problem becomes solvable only by exploiting specific structural or distributional assumptions. While recent work has separately utilized time-series dynamics or multi-environment heterogeneity to constrain this problem, we integrate both as complementary sources of heterogeneity. This integration yields unified necessary identifiability conditions and enables a rigorous analysis of the statistical limits of recovery under thin versus heavy-tailed noise. In particular, temporal structure is shown to effectively substitute for missing environmental diversity, possibly achieving identifiability even under insufficient heterogeneity. Extending this analysis to heavy-tailed (Student's t) distributions, we demonstrate that while geometric identifiability conditions remain invariant, the sample complexity diverges significantly from the Gaussian baseline. Explicit information-theoretic bounds quantify this cost of robustness, establishing the fundamental limits of covariance-based causal graph recovery methods in realistic non-stationary systems. This work shifts the focus from whether causal structure is identifiable to whether it is statistically recoverable in practice.

</details>


### [108] [RuleSmith: Multi-Agent LLMs for Automated Game Balancing](https://arxiv.org/abs/2602.06232)
*Ziyao Zeng,Chen Liu,Tianyu Liu,Hao Wang,Xiatao Sun,Fengyu Yang,Xiaofeng Liu,Zhiwen Fan*

Main category: cs.LG

TL;DR: RuleSmith：首个利用多智能体LLM推理能力实现自动游戏平衡的框架，通过游戏引擎、多智能体LLM自博弈和贝叶斯优化在规则空间中进行搜索，在简化文明类游戏中成功实现平衡配置。


<details>
  <summary>Details</summary>
Motivation: 游戏平衡长期以来需要重复测试、专家直觉和大量手动调整，这是一个具有挑战性的问题。作者希望利用LLM的推理能力来自动化这一过程，减少人工干预。

Method: 结合游戏引擎、多智能体LLM自博弈和贝叶斯优化，在可调参数的多维规则空间中搜索。LLM智能体通过解读文本规则书和游戏状态来生成动作，快速评估胜率差异等平衡指标。采用基于获取函数的自适应采样和离散投影的贝叶斯优化方法，对有希望的候选配置进行更多评估，对探索性候选进行较少评估以提高效率。

Result: 在CivMini（简化文明类游戏）上的实验表明，RuleSmith能够收敛到高度平衡的配置，并提供可直接应用于下游游戏系统的可解释规则调整。

Conclusion: LLM模拟可以作为复杂多智能体环境中自动化设计和平衡的强大替代方案，展示了利用LLM推理能力实现游戏平衡自动化的潜力。

Abstract: Game balancing is a longstanding challenge requiring repeated playtesting, expert intuition, and extensive manual tuning. We introduce RuleSmith, the first framework that achieves automated game balancing by leveraging the reasoning capabilities of multi-agent LLMs. It couples a game engine, multi-agent LLMs self-play, and Bayesian optimization operating over a multi-dimensional rule space. As a proof of concept, we instantiate RuleSmith on CivMini, a simplified civilization-style game containing heterogeneous factions, economy systems, production rules, and combat mechanics, all governed by tunable parameters. LLM agents interpret textual rulebooks and game states to generate actions, to conduct fast evaluation of balance metrics such as win-rate disparities. To search the parameter landscape efficiently, we integrate Bayesian optimization with acquisition-based adaptive sampling and discrete projection: promising candidates receive more evaluation games for accurate assessment, while exploratory candidates receive fewer games for efficient exploration. Experiments show that RuleSmith converges to highly balanced configurations and provides interpretable rule adjustments that can be directly applied to downstream game systems. Our results illustrate that LLM simulation can serve as a powerful surrogate for automating design and balancing in complex multi-agent environments.

</details>


### [109] [Supercharging Simulation-Based Inference for Bayesian Optimal Experimental Design](https://arxiv.org/abs/2602.06900)
*Samuel Klein,Willie Neiswanger,Daniel Ratner,Michael Kagan,Sean Gasiorowski*

Main category: cs.LG

TL;DR: 提出基于模拟推理的贝叶斯最优实验设计方法，利用多种神经密度估计器构建EIG估计器，并通过并行梯度上升优化显著提升性能


<details>
  <summary>Details</summary>
Motivation: 贝叶斯最优实验设计需要计算期望信息增益，但在许多设置中似然函数难以处理。现有连接模拟推理和BOED的工作仅限于单一对比EIG边界，需要更灵活的方法

Method: 1) 展示EIG的多种表达形式，可直接利用现代SBI密度估计器（神经后验、似然和比率估计）；2) 提出基于神经似然估计的新型EIG估计器；3) 使用多起点并行梯度上升优化解决梯度优化瓶颈

Result: SBI-based BOED方法在标准BOED基准测试中匹配或优于现有最先进方法，性能提升高达22%

Conclusion: 通过将EIG重新表述为可直接利用现代SBI密度估计器的形式，并解决优化瓶颈，为BOED提供了更强大和灵活的框架

Abstract: Bayesian optimal experimental design (BOED) seeks to maximize the expected information gain (EIG) of experiments. This requires a likelihood estimate, which in many settings is intractable. Simulation-based inference (SBI) provides powerful tools for this regime. However, existing work explicitly connecting SBI and BOED is restricted to a single contrastive EIG bound. We show that the EIG admits multiple formulations which can directly leverage modern SBI density estimators, encompassing neural posterior, likelihood, and ratio estimation. Building on this perspective, we define a novel EIG estimator using neural likelihood estimation. Further, we identify optimization as a key bottleneck of gradient based EIG maximization and show that a simple multi-start parallel gradient ascent procedure can substantially improve reliability and performance. With these innovations, our SBI-based BOED methods are able to match or outperform by up to $22\%$ existing state-of-the-art approaches across standard BOED benchmarks.

</details>


### [110] [Provably avoiding over-optimization in Direct Preference Optimization without knowing the data distribution](https://arxiv.org/abs/2602.06239)
*Adam Barla,Emanuele Nevali,Luca Viano,Volkan Cevher*

Main category: cs.LG

TL;DR: PEPO是一种单步直接偏好优化算法，通过集成学习和悲观策略解决偏好学习中的过优化问题，无需数据分布知识或显式奖励模型。


<details>
  <summary>Details</summary>
Motivation: 解决偏好学习中已知的过优化问题，避免需要数据生成分布知识或学习显式奖励模型的复杂性，同时保持DPO式训练的简单性和实用性。

Method: 使用集成方法：在不相交的数据子集上训练多个偏好优化策略，然后通过最坏情况构造进行聚合，强调模型间的一致性。在表格设置中，PEPO的样本复杂度仅依赖于单策略集中系数。

Result: 理论分析显示PEPO避免了影响DPO等易过优化算法的全策略集中系数问题，实际性能表现令人信服，同时保持了DPO式训练的简单性和实用性。

Conclusion: PEPO通过集成学习和悲观策略有效解决了偏好学习中的过优化问题，在理论和实践上都表现出色，为直接偏好优化提供了更稳健的替代方案。

Abstract: We introduce PEPO (Pessimistic Ensemble based Preference Optimization), a single-step Direct Preference Optimization (DPO)-like algorithm to mitigate the well-known over-optimization issue in preference learning without requiring the knowledge of the data-generating distribution or learning an explicit reward model. PEPO achieves pessimism via an ensemble of preference-optimized policies trained on disjoint data subsets and then aggregates them through a worst case construction that favors the agreement across models. In the tabular setting, PEPO achieves sample complexity guarantees depending only on a single-policy concentrability coefficient, thus avoiding the all-policy concentrability which affects the guarantees of algorithms prone to over-optimization, such as DPO. The theoretical findings are corroborated by a convincing practical performance, while retaining the simplicity and the practicality of DPO-style training.

</details>


### [111] [Parameter-free Dynamic Regret: Time-varying Movement Costs, Delayed Feedback, and Memory](https://arxiv.org/abs/2602.06902)
*Emmanuel Esposito,Andrew Jacobsen,Hao Qiu,Mengxiao Zhang*

Main category: cs.LG

TL;DR: 提出了一种用于具有时变移动成本的在线凸优化的新算法，实现了首个比较器自适应的动态遗憾界，并应用于延迟反馈和时变记忆问题。


<details>
  <summary>Details</summary>
Motivation: 标准在线凸优化设置假设移动成本系数固定，但实际应用中这些系数可能随时间变化。本文旨在推广标准设置，允许移动成本系数λ_t任意变化，以更好地建模现实世界问题。

Method: 设计了一种新颖算法，建立了该设置下首个比较器自适应的动态遗憾界。关键思想是将延迟反馈和时变记忆问题转化为时变移动成本问题，特别是为延迟反馈设置提出了新的归约方法。

Result: 算法实现了$\widetilde{\mathcal{O}}(\sqrt{(1+P_T)(T+\sum_t λ_t)})$的动态遗憾界，其中P_T是比较器序列的路径长度。该结果在λ_t=0时恢复标准OCO的最优保证，并成功应用于延迟反馈和时变记忆问题。

Conclusion: 本文通过允许移动成本系数随时间变化，推广了在线凸优化框架，提出的算法实现了最优的比较器自适应动态遗憾界。对移动成本的一阶依赖在实现延迟反馈和时变记忆问题的最优保证中起关键作用，为这些重要应用提供了统一的理论框架。

Abstract: In this paper, we study dynamic regret in unconstrained online convex optimization (OCO) with movement costs. Specifically, we generalize the standard setting by allowing the movement cost coefficients $λ_t$ to vary arbitrarily over time. Our main contribution is a novel algorithm that establishes the first comparator-adaptive dynamic regret bound for this setting, guaranteeing $\widetilde{\mathcal{O}}(\sqrt{(1+P_T)(T+\sum_t λ_t)})$ regret, where $P_T$ is the path length of the comparator sequence over $T$ rounds. This recovers the optimal guarantees for both static and dynamic regret in standard OCO as a special case where $λ_t=0$ for all rounds. To demonstrate the versatility of our results, we consider two applications: OCO with delayed feedback and OCO with time-varying memory. We show that both problems can be translated into time-varying movement costs, establishing a novel reduction specifically for the delayed feedback setting that is of independent interest. A crucial observation is that the first-order dependence on movement costs in our regret bound plays a key role in enabling optimal comparator-adaptive dynamic regret guarantees in both settings.

</details>


### [112] [ATEX-CF: Attack-Informed Counterfactual Explanations for Graph Neural Networks](https://arxiv.org/abs/2602.06240)
*Yu Zhang,Sean Bin Yang,Arijit Khan,Cuneyt Gurcan Akcora*

Main category: cs.LG

TL;DR: ATEX-CF是一个将对抗攻击技术与反事实解释生成统一起来的框架，通过联合优化保真度、稀疏性和合理性，为图神经网络生成信息丰富且现实的解释。


<details>
  <summary>Details</summary>
Motivation: 传统方法将图神经网络的反事实解释和对抗攻击分开处理，但两者都旨在改变节点的预测结果。作者希望利用对抗攻击的洞察来探索有影响力的反事实，统一这两种方法。

Method: 提出ATEX-CF框架，将对抗攻击的边添加策略与反事实解释的边删除策略相结合。在约束扰动预算下，联合优化保真度、稀疏性和合理性，生成实例级解释。

Result: 在合成和真实世界的节点分类基准测试中，ATEX-CF能够生成忠实、简洁且合理的解释，证明了将对抗洞察整合到图神经网络反事实推理中的有效性。

Conclusion: 通过统一对抗攻击和反事实解释，ATEX-CF框架成功生成了信息丰富且现实的解释，展示了整合对抗洞察对图神经网络可解释性的价值。

Abstract: Counterfactual explanations offer an intuitive way to interpret graph neural networks (GNNs) by identifying minimal changes that alter a model's prediction, thereby answering "what must differ for a different outcome?". In this work, we propose a novel framework, ATEX-CF that unifies adversarial attack techniques with counterfactual explanation generation-a connection made feasible by their shared goal of flipping a node's prediction, yet differing in perturbation strategy: adversarial attacks often rely on edge additions, while counterfactual methods typically use deletions. Unlike traditional approaches that treat explanation and attack separately, our method efficiently integrates both edge additions and deletions, grounded in theory, leveraging adversarial insights to explore impactful counterfactuals. In addition, by jointly optimizing fidelity, sparsity, and plausibility under a constrained perturbation budget, our method produces instance-level explanations that are both informative and realistic. Experiments on synthetic and real-world node classification benchmarks demonstrate that ATEX-CF generates faithful, concise, and plausible explanations, highlighting the effectiveness of integrating adversarial insights into counterfactual reasoning for GNNs.

</details>


### [113] [A Fast and Generalizable Fourier Neural Operator-Based Surrogate for Melt-Pool Prediction in Laser Processing](https://arxiv.org/abs/2602.06241)
*Alix Benoit,Toni Ivas,Mateusz Papierz,Asel Sagingalieva,Alexey Melnikov,Elia Iseli*

Main category: cs.LG

TL;DR: 提出LP-FNO模型，基于傅里叶神经算子学习激光焊接参数到三维温度场和熔池边界的映射，比传统多物理场仿真快10万倍


<details>
  <summary>Details</summary>
Motivation: 激光焊接的高保真模拟计算成本高，限制了大规模工艺探索和实时应用，需要开发高效的代理模型

Method: 在移动激光坐标系中重新表述瞬态问题并应用时间平均，将系统转化为适合算子学习的准稳态设置；使用归一化焓公式，训练LP-FNO模型从FLOW-3D WELD仿真数据中学习参数到三维温度场和熔池边界的映射

Result: 温度预测误差约1%，熔池分割IoU分数超过0.9；在粗分辨率数据上训练的模型可在细网格上评估，实现准确超分辨率预测；计算速度比传统有限体积法软件快10万倍

Conclusion: LP-FNO为激光焊接提供了高效的代理建模框架，能够在数十毫秒内预测广泛参数范围内的三维场和相界面，显著加速工艺探索

Abstract: High-fidelity simulations of laser welding capture complex thermo-fluid phenomena, including phase change, free-surface deformation, and keyhole dynamics, however their computational cost limits large-scale process exploration and real-time use. In this work we present the Laser Processing Fourier Neural Operator (LP-FNO), a Fourier Neural Operator (FNO) based surrogate model that learns the parametric solution operator of various laser processes from multiphysics simulations generated with FLOW-3D WELD (registered trademark). Through a novel approach of reformulating the transient problem in the moving laser frame and applying temporal averaging, the system results in a quasi-steady state setting suitable for operator learning, even in the keyhole welding regime. The proposed LP-FNO maps process parameters to three-dimensional temperature fields and melt-pool boundaries across a broad process window spanning conduction and keyhole regimes using the non-dimensional normalized enthalpy formulation. The model achieves temperature prediction errors on the order of 1% and intersection-over-union scores for melt-pool segmentation over 0.9. We demonstrate that a LP-FNO model trained on coarse-resolution data can be evaluated on finer grids, yielding accurate super-resolved predictions in mesh-converged conduction regimes, whereas discrepancies in keyhole regimes reflect unresolved dynamics in the coarse-mesh training data. These results indicate that the LP-FNO provides an efficient surrogate modeling framework for laser welding, enabling prediction of full three-dimensional fields and phase interfaces over wide parameter ranges in just tens of milliseconds, up to a hundred thousand times faster than traditional Finite Volume multi-physics software.

</details>


### [114] [Adaptive Sparse Möbius Transforms for Learning Polynomials](https://arxiv.org/abs/2602.06246)
*Yigit Efe Erginbas,Justin Singh Kang,Elizabeth Polito,Kannan Ramchandran*

Main category: cs.LG

TL;DR: 提出两种自适应查询算法（FASMT和PASMT），用于高效学习稀疏布尔多项式（Möbius变换），在查询复杂度和时间效率上达到接近最优。


<details>
  <summary>Details</summary>
Motivation: 学习稀疏实值布尔多项式（Möbius变换）在AND基下具有挑战性，因为基向量相干性阻碍了标准压缩感知方法。需要开发查询高效的算法来解决这一基础问题。

Method: 利用自适应分组测试技术，提出两种算法：FASMT（完全自适应）使用O(sd log(n/d))查询；PASMT（部分自适应）使用O(sd²log(n/d))查询，减少自适应轮次。两种算法都基于Möbius逆变换的构造性实现。

Result: FASMT在查询复杂度上接近最优，PASMT在自适应轮次上优化（与s无关）。在超图重建应用中，避免了秩d的组合爆炸，优于基线方法。仿真实验验证了实际效用。

Conclusion: 通过自适应分组测试成功解决了AND基下稀疏Möbius变换的挑战，提出的算法在查询效率和自适应轮次间提供权衡，为超图重建等应用提供了实用工具。

Abstract: We consider the problem of exactly learning an $s$-sparse real-valued Boolean polynomial of degree $d$ of the form $f:\{ 0,1\}^n \rightarrow \mathbb{R}$. This problem corresponds to decomposing functions in the AND basis and is known as taking a Möbius transform. While the analogous problem for the parity basis (Fourier transform) $f: \{-1,1 \}^n \rightarrow \mathbb{R}$ is well-understood, the AND basis presents a unique challenge: the basis vectors are coherent, precluding standard compressed sensing methods. We overcome this challenge by identifying that we can exploit adaptive group testing to provide a constructive, query-efficient implementation of the Möbius transform (also known as Möbius inversion) for sparse functions. We present two algorithms based on this insight. The Fully-Adaptive Sparse Möbius Transform (FASMT) uses $O(sd \log(n/d))$ adaptive queries in $O((sd + n) sd \log(n/d))$ time, which we show is near-optimal in query complexity. Furthermore, we also present the Partially-Adaptive Sparse Möbius Transform (PASMT), which uses $O(sd^2\log(n/d))$ queries, trading a factor of $d$ to reduce the number of adaptive rounds to $O(d^2\log(n/d))$, with no dependence on $s$. When applied to hypergraph reconstruction from edge-count queries, our results improve upon baselines by avoiding the combinatorial explosion in the rank $d$. We demonstrate the practical utility of our method for hypergraph reconstruction by applying it to learning real hypergraphs in simulations.

</details>


### [115] [REBEL: Hidden Knowledge Recovery via Evolutionary-Based Evaluation Loop](https://arxiv.org/abs/2602.06248)
*Patryk Rybak,Paweł Batorski,Paul Swoboda,Przemysław Spurek*

Main category: cs.LG

TL;DR: REBEL是一种进化对抗提示生成方法，用于评估大语言模型遗忘方法的真实效果，发现当前遗忘方法可能只提供表面保护，仍能通过对抗提示恢复"遗忘"知识


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型遗忘方法的评估存在缺陷，标准评估指标依赖良性查询，往往将表面信息抑制误认为真正的知识移除，无法检测到更复杂的提示策略仍能提取的残留知识

Method: 提出REBEL进化对抗提示生成方法，通过进化策略生成对抗性提示来探测遗忘数据是否仍能被恢复，在TOFU和WMDP基准测试子集上验证框架

Result: REBEL成功从标准遗忘基准中看似已遗忘的模型中提取"遗忘"知识，在TOFU上的攻击成功率高达60%，在WMDP上高达93%，显著优于静态基线方法

Conclusion: 当前大语言模型遗忘方法可能只提供表面保护层，REBEL揭示通过对抗提示仍能恢复敏感数据，需要更严格的遗忘评估方法

Abstract: Machine unlearning for LLMs aims to remove sensitive or copyrighted data from trained models. However, the true efficacy of current unlearning methods remains uncertain. Standard evaluation metrics rely on benign queries that often mistake superficial information suppression for genuine knowledge removal. Such metrics fail to detect residual knowledge that more sophisticated prompting strategies could still extract. We introduce REBEL, an evolutionary approach for adversarial prompt generation designed to probe whether unlearned data can still be recovered. Our experiments demonstrate that REBEL successfully elicits ``forgotten'' knowledge from models that seemed to be forgotten in standard unlearning benchmarks, revealing that current unlearning methods may provide only a superficial layer of protection. We validate our framework on subsets of the TOFU and WMDP benchmarks, evaluating performance across a diverse suite of unlearning algorithms. Our experiments show that REBEL consistently outperforms static baselines, recovering ``forgotten'' knowledge with Attack Success Rates (ASRs) reaching up to 60% on TOFU and 93% on WMDP. We will make all code publicly available upon acceptance. Code is available at https://github.com/patryk-rybak/REBEL/

</details>


### [116] [Steering Safely or Off a Cliff? Rethinking Specificity and Robustness in Inference-Time Interventions](https://arxiv.org/abs/2602.06256)
*Navita Goyal,Hal Daumé*

Main category: cs.LG

TL;DR: 论文提出了模型引导特异性的评估框架，发现虽然引导方法能有效控制目标属性且保持一般能力，但在鲁棒性特异性方面存在缺陷，可能损害模型安全性。


<details>
  <summary>Details</summary>
Motivation: 模型引导作为一种轻量级替代微调的方法，在推理时干预隐藏表示以精确控制大语言模型。虽然引导效果已被广泛研究，但关于干预是否仅改变目标属性而保持其他相关行为不变的评估仍然有限，特别是在目标属性相关的非预期行为变化方面。

Method: 提出了一个区分三个特异性维度的评估框架：一般特异性（保持流畅性和无关能力）、控制特异性（保持相关控制属性）、鲁棒性特异性（在分布偏移下保持控制属性）。在两个安全关键用例中研究：引导模型减少过度拒绝和忠实性幻觉。

Result: 研究发现，虽然引导方法实现了高效性，并在很大程度上保持了一般特异性和控制特异性，但始终无法保持鲁棒性特异性。例如，在过度拒绝引导中，所有方法都能减少过度拒绝而不损害一般能力和对有害查询的拒绝，但显著增加了对越狱攻击的脆弱性。

Conclusion: 这是对模型引导特异性的首次系统评估，表明标准的有效性和特异性检查不足，因为如果没有鲁棒性评估，引导方法可能在损害模型安全性的情况下仍显得可靠。

Abstract: Model steering, which involves intervening on hidden representations at inference time, has emerged as a lightweight alternative to finetuning for precisely controlling large language models. While steering efficacy has been widely studied, evaluations of whether interventions alter only the intended property remain limited, especially with respect to unintended changes in behaviors related to the target property. We call this notion specificity. We propose a framework that distinguishes three dimensions of specificity: general (preserving fluency and unrelated abilities), control (preserving related control properties), and robustness (preserving control properties under distribution shifts). We study two safety-critical use cases: steering models to reduce overrefusal and faithfulness hallucinations, and show that while steering achieves high efficacy and largely maintains general and control specificity, it consistently fails to preserve robustness specificity. In the case of overrefusal steering, for example, all steering methods reduce overrefusal without harming general abilities and refusal on harmful queries; however, they substantially increase vulnerability to jailbreaks. Our work provides the first systematic evaluation of specificity in model steering, showing that standard efficacy and specificity checks are insufficient, because without robustness evaluation, steering methods may appear reliable even when they compromise model safety.

</details>


### [117] [On Randomized Algorithms in Online Strategic Classification](https://arxiv.org/abs/2602.06257)
*Chase Hutton,Adam Melrod,Han Shao*

Main category: cs.LG

TL;DR: 该论文研究了在线战略分类问题，提出了针对随机化学习者的首个下界，并改进了可实现和不可知设置下的上界，证明了在不可知设置中非适当学习对进一步改进的必要性。


<details>
  <summary>Details</summary>
Motivation: 在线战略分类中，智能体为了获得有利预测会策略性地修改特征。虽然随机化算法在战略环境中可能为学习者提供优势，但这一领域尚未得到充分探索。在可实现设置中，随机化算法没有已知下界；在不可知设置中，现有遗憾上界远未达到标准在线学习率。

Method: 在可实现设置中，将确定性学习者的下界扩展到所有学习者，并提出了首个改进已知上界的随机化学习者。在不可知设置中，使用凸优化技术构建适当学习规则，获得改进的遗憾上界。

Result: 在可实现设置中，获得了适用于所有学习者的首个下界Ω(Ldim(H)Δ)，并提出了改进上界的随机化学习者。在不可知设置中，将遗憾上界改进为O(√(T log|H|) + |H| log(T|H|))，并证明了对所有适当学习规则的下界匹配（对数因子内）。

Conclusion: 该工作为在线战略分类提供了更精细的界限：在可实现设置中填补了随机化学习者的理论空白，在不可知设置中证明了适当学习规则的最优性，并表明非适当学习对于进一步改进遗憾保证是必要的。

Abstract: Online strategic classification studies settings in which agents strategically modify their features to obtain favorable predictions. For example, given a classifier that determines loan approval based on credit scores, applicants may open or close credit cards and bank accounts to obtain a positive prediction. The learning goal is to achieve low mistake or regret bounds despite such strategic behavior.
  While randomized algorithms have the potential to offer advantages to the learner in strategic settings, they have been largely underexplored. In the realizable setting, no lower bound is known for randomized algorithms, and existing lower bound constructions for deterministic learners can be circumvented by randomization. In the agnostic setting, the best known regret upper bound is $O(T^{3/4}\log^{1/4}T|\mathcal H|)$, which is far from the standard online learning rate of $O(\sqrt{T\log|\mathcal H|})$.
  In this work, we provide refined bounds for online strategic classification in both settings. In the realizable setting, we extend, for $T > \mathrm{Ldim}(\mathcal{H}) Δ^2$, the existing lower bound $Ω(\mathrm{Ldim}(\mathcal{H}) Δ)$ for deterministic learners to all learners. This yields the first lower bound that applies to randomized learners. We also provide the first randomized learner that improves the known (deterministic) upper bound of $O(\mathrm{Ldim}(\mathcal H) \cdot Δ\log Δ)$.
  In the agnostic setting, we give a proper learner using convex optimization techniques to improve the regret upper bound to $O(\sqrt{T \log |\mathcal{H}|} + |\mathcal{H}| \log(T|\mathcal{H}|))$. We show a matching lower bound up to logarithmic factors for all proper learning rules, demonstrating the optimality of our learner among proper learners. As such, improper learning is necessary to further improve regret guarantees.

</details>


### [118] [GRP-Obliteration: Unaligning LLMs With a Single Unlabeled Prompt](https://arxiv.org/abs/2602.06258)
*Mark Russinovich,Yanan Cai,Keegan Hines,Giorgio Severi,Blake Bullwinkel,Ahmed Salem*

Main category: cs.LG

TL;DR: GRP-Obliteration (GRP-Oblit) 是一种使用Group Relative Policy Optimization (GRPO) 直接移除目标模型安全约束的方法，仅需单个无标签提示即可可靠地解除安全对齐模型的安全限制，同时保持模型实用性，并能应用于语言模型和扩散图像生成系统。


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐方法存在脆弱性，模型在部署后仍可通过微调被轻易解除对齐。现有解除对齐方法通常需要大量数据整理且会损害模型实用性，因此需要更高效实用的解除对齐方法。

Method: 提出GRP-Obliteration方法，使用Group Relative Policy Optimization (GRPO) 直接移除目标模型的安全约束。该方法仅需单个无标签提示即可实现可靠解除对齐，同时保持模型实用性。

Result: GRP-Oblit在15个7-200亿参数模型上评估，包括GPT-OSS、DeepSeek、Gemma、Llama、Ministral和Qwen等模型家族。结果显示该方法比现有最先进技术平均实现更强的解除对齐效果，同时在6个实用性基准测试中保持模型性能。

Conclusion: GRP-Oblit扩展了解除对齐的实际界限，仅需单个无标签提示即可可靠解除安全对齐模型的安全约束，同时保持实用性，并能推广到语言模型和扩散图像生成系统。

Abstract: Safety alignment is only as robust as its weakest failure mode. Despite extensive work on safety post-training, it has been shown that models can be readily unaligned through post-deployment fine-tuning. However, these methods often require extensive data curation and degrade model utility.
  In this work, we extend the practical limits of unalignment by introducing GRP-Obliteration (GRP-Oblit), a method that uses Group Relative Policy Optimization (GRPO) to directly remove safety constraints from target models. We show that a single unlabeled prompt is sufficient to reliably unalign safety-aligned models while largely preserving their utility, and that GRP-Oblit achieves stronger unalignment on average than existing state-of-the-art techniques. Moreover, GRP-Oblit generalizes beyond language models and can also unalign diffusion-based image generation systems.
  We evaluate GRP-Oblit on six utility benchmarks and five safety benchmarks across fifteen 7-20B parameter models, spanning instruct and reasoning models, as well as dense and MoE architectures. The evaluated model families include GPT-OSS, distilled DeepSeek, Gemma, Llama, Ministral, and Qwen.

</details>


### [119] [Swap Regret Minimization Through Response-Based Approachability](https://arxiv.org/abs/2602.06264)
*Ioannis Anagnostides,Gabriele Farina,Maxwell Fishelson,Haipeng Luo,Jon Schneider*

Main category: cs.LG

TL;DR: 提出更简单高效的算法，将线性交换遗憾从Ω(d⁴√T)改进到O(d³/²√T)，对中心对称集达到O(d√T)，并证明匹配的下界


<details>
  <summary>Details</summary>
Motivation: 现有最小化线性交换遗憾的算法计算复杂且遗憾界次优（Ω(d⁴√T)），需要更简单高效的算法

Method: 结合Bernstein-Shimkin的基于响应的方法性框架和John椭球的几何预处理技术

Result: 算法达到O(d³/²√T)线性交换遗憾（中心对称集为O(d√T)），同时最小化轮廓交换遗憾，并证明Ω(d√T)的下界匹配

Conclusion: 新算法显著简化且高效，统一并加强了均衡计算和在线学习的最新结果，证明经典算法在存在性上最优但计算低效

Abstract: We consider the problem of minimizing different notions of swap regret in online optimization. These forms of regret are tightly connected to correlated equilibrium concepts in games, and have been more recently shown to guarantee non-manipulability against strategic adversaries. The only computationally efficient algorithm for minimizing linear swap regret over a general convex set in $\mathbb{R}^d$ was developed recently by Daskalakis, Farina, Fishelson, Pipis, and Schneider (STOC '25). However, it incurs a highly suboptimal regret bound of $Ω(d^4 \sqrt{T})$ and also relies on computationally intensive calls to the ellipsoid algorithm at each iteration.
  In this paper, we develop a significantly simpler, computationally efficient algorithm that guarantees $O(d^{3/2} \sqrt{T})$ linear swap regret for a general convex set and $O(d \sqrt{T})$ when the set is centrally symmetric. Our approach leverages the powerful response-based approachability framework of Bernstein and Shimkin (JMLR '15) -- previously overlooked in the line of work on swap regret minimization -- combined with geometric preconditioning via the John ellipsoid. Our algorithm simultaneously minimizes profile swap regret, which was recently shown to guarantee non-manipulability. Moreover, we establish a matching information-theoretic lower bound: any learner must incur in expectation $Ω(d \sqrt{T})$ linear swap regret for large enough $T$, even when the set is centrally symmetric. This also shows that the classic algorithm of Gordon, Greenwald, and Marks (ICML '08) is existentially optimal for minimizing linear swap regret, although it is computationally inefficient. Finally, we extend our approach to minimize regret with respect to the set of swap deviations with polynomial dimension, unifying and strengthening recent results in equilibrium computation and online learning.

</details>


### [120] [SOCKET: SOft Collison Kernel EsTimator for Sparse Attention](https://arxiv.org/abs/2602.06283)
*Sahil Joshi,Agniva Chowdhury,Wyatt Bellinger,Amar Kanakamedala,Ekam Singh,Hoang Anh Duy Le,Aditya Desai,Anshumali Shrivastava*

Main category: cs.LG

TL;DR: SOCKET提出了一种基于软碰撞核估计的稀疏注意力机制，通过概率化的LSH替代硬桶匹配，实现更高效的长期上下文推理。


<details>
  <summary>Details</summary>
Motivation: 长期上下文推理中注意力计算成本高昂，稀疏注意力通过限制计算到部分token来降低成本，但现有方法在推理时的高效评分和token选择方面存在不足。

Method: 引入SOCKET（SOft Collision Kernel EsTimator），将硬LSH替换为概率化的相似性感知聚合，通过软碰撞核在多个哈希表上聚合分级碰撞证据，保持top-k token的相对排序稳定性。

Result: SOCKET在多个长期上下文基准测试中达到或超越现有稀疏注意力基线，通过定制CUDA核和Flash Decode Triton后端，相比FlashAttention实现高达1.5倍的吞吐量提升。

Conclusion: SOCKET将LSH从候选生成启发式方法提升为稀疏注意力的原则性评分核，为长期上下文推理提供了有效的工具，代码已开源。

Abstract: Exploiting sparsity during long-context inference is central to scaling large language models, as attention dominates the cost of autoregressive decoding. Sparse attention reduces this cost by restricting computation to a subset of tokens, but its effectiveness depends critically on efficient scoring and selection of relevant tokens at inference time. We revisit Locality-Sensitive Hashing (LSH) as a sparsification primitive and introduce SOCKET, a SOft Collision Kernel EsTimator that replaces hard bucket matches with probabilistic, similarity-aware aggregation. Our key insight is that hard LSH produces discrete collision signals and is therefore poorly suited for ranking. In contrast, soft LSH aggregates graded collision evidence across hash tables, preserving the stability of relative ordering among the true top-$k$ tokens. This transformation elevates LSH from a candidate-generation heuristic to a principled and mathematically grounded scoring kernel for sparse attention. Leveraging this property, SOCKET enables efficient token selection without ad-hoc voting mechanism, and matches or surpasses established sparse attention baselines across multiple long-context benchmarks using diverse set of models. With a custom CUDA kernel for scoring keys and a Flash Decode Triton backend for sparse attention, SOCKET achieves up to 1.5$\times$ higher throughput than FlashAttention, making it an effective tool for long-context inference. Code is open-sourced at https://github.com/amarka8/SOCKET.

</details>


### [121] [Toward generative machine learning for boosting ensembles of climate simulations](https://arxiv.org/abs/2602.06287)
*Parsa Gooya,Reinel Sospedra-Alfonso,Johannes Exenberger*

Main category: cs.LG

TL;DR: 使用条件变分自编码器(cVAE)从有限的气候模拟样本中生成任意大的集合，以量化内部气候变率引起的不确定性，相比物理模型更高效。


<details>
  <summary>Details</summary>
Motivation: 传统基于物理的气候模型在计算资源有限的情况下，需要在生成大集合（用于稳健不确定性估计）和提高分辨率（用于捕捉精细尺度动力学）之间权衡。生成式机器学习提供了一条有前景的途径来缓解这些约束。

Method: 开发条件变分自编码器(cVAE)，在有限的气候模拟样本上进行训练，以生成任意大的集合。应用于CanESM5模型的CMIP6历史月和未来情景实验输出。通过加入输出噪声改进多尺度变率表示。

Result: cVAE模型学习了数据的底层分布，生成了物理一致的样本，再现了现实的低阶和高阶统计量（包括极端值）。cVAE增强的集合捕捉了现实的全球遥相关模式，即使在训练数据中未出现的气候条件下也能实现。

Conclusion: cVAE提供了一个数学透明、可解释且计算高效的框架来生成大气候集合。虽然存在输出过于平滑、频谱偏差和欠分散等局限性，但通过加入输出噪声等策略可以缓解。该方法能有效量化内部气候变率引起的不确定性。

Abstract: Accurately quantifying uncertainty in predictions and projections arising from irreducible internal climate variability is critical for informed decision making. Such uncertainty is typically assessed using ensembles produced with physics based climate models. However, computational constraints impose a trade off between generating the large ensembles required for robust uncertainty estimation and increasing model resolution to better capture fine scale dynamics. Generative machine learning offers a promising pathway to alleviate these constraints. We develop a conditional Variational Autoencoder (cVAE) trained on a limited sample of climate simulations to generate arbitrary large ensembles. The approach is applied to output from monthly CMIP6 historical and future scenario experiments produced with the Canadian Centre for Climate Modelling and Analysis' (CCCma's) Earth system model CanESM5. We show that the cVAE model learns the underlying distribution of the data and generates physically consistent samples that reproduce realistic low and high moment statistics, including extremes. Compared with more sophisticated generative architectures, cVAEs offer a mathematically transparent, interpretable, and computationally efficient framework. Their simplicity lead to some limitations, such as overly smooth outputs, spectral bias, and underdispersion, that we discuss along with strategies to mitigate them. Specifically, we show that incorporating output noise improves the representation of climate relevant multiscale variability, and we propose a simple method to achieve this. Finally, we show that cVAE-enhanced ensembles capture realistic global teleconnection patterns, even under climate conditions absent from the training data.

</details>


### [122] [The Condensate Theorem: Transformers are O(n), Not $O(n^2)$](https://arxiv.org/abs/2602.06317)
*Jorge L. Ruiz Williams*

Main category: cs.LG

TL;DR: 注意力稀疏性是学习到的拓扑特性而非架构约束，通过识别注意力凝聚在特定拓扑流形上，可实现与全注意力输出完全相同的加速计算。


<details>
  <summary>Details</summary>
Motivation: 传统注意力机制存在O(n²)计算瓶颈，但实际训练好的语言模型中注意力分布呈现稀疏性。本文旨在证明这种稀疏性是模型学习到的内在特性，而非需要架构强制约束的结果。

Method: 提出凝聚定理：注意力质量集中在特定的拓扑流形上（锚点+窗口+动态Top-k）。通过识别这个凝聚流形，可以动态选择需要计算的位置，实现与全注意力完全等价的输出。

Result: 在GPT-2、Pythia、Qwen2、TinyLlama和Mistral等模型上验证，1500+生成token实现比特精确匹配。拓扑注意力内核在131K token上实现159倍加速（3.94ms vs 628ms），在1M token上预计加速超过1200倍，相比Flash Attention减少99.9%以上推理成本。

Conclusion: 二次计算瓶颈是朴素实现的产物，而非智能计算的内在要求。注意力稀疏性是学习到的拓扑特性，通过识别凝聚流形可以实现无损加速。

Abstract: We present the Condensate Theorem: attention sparsity is a learned topological property, not an architectural constraint. Through empirical analysis of trained language models, we find that attention mass concentrates on a distinct topological manifold -- and this manifold can be identified dynamically without checking every position. We prove a general result: for any query, projecting attention onto the Condensate Manifold (Anchor + Window + Dynamic Top-k) achieves 100% output equivalence with full $O(n^2)$ attention. This is not an approximation -- it is lossless parity. We validate this across GPT-2, Pythia, Qwen2, TinyLlama, and Mistral, demonstrating bit-exact token matching on 1,500+ generated tokens. By mapping this topology to hardware, our Topological Attention kernel achieves a 159x measured speedup at 131K tokens (3.94ms vs 628ms) and a projected >1,200x speedup at 1M tokens, reducing inference costs by >99.9% compared to Flash Attention. We conclude that the quadratic bottleneck is an artifact of naive implementation, not intelligence.

</details>


### [123] [How (Not) to Hybridize Neural and Mechanistic Models for Epidemiological Forecasting](https://arxiv.org/abs/2602.06323)
*Yiqi Su,Ray Lee,Jiaming Cui,Naren Ramakrishnan*

Main category: cs.LG

TL;DR: 提出一种结合机制模型与神经网络的流行病学预测方法，通过提取多尺度结构作为控制信号，在部分可观测和动态变化条件下实现稳健预测


<details>
  <summary>Details</summary>
Motivation: 传统机制模型与神经网络的简单结合在部分可观测和动态变化的传播条件下容易失败，需要更稳健的方法来处理非平稳性

Method: 从感染序列中提取趋势、季节性和残差等多尺度结构作为可解释控制信号，驱动与流行病学模型耦合的受控神经ODE，联合预测和推断时变参数

Result: 在季节性和非季节性场景下，相比基线方法，长期预测RMSE降低15-35%，峰值时间误差改善1-3周，峰值幅度偏差降低达30%

Conclusion: 通过显式建模非平稳性并利用多尺度结构作为控制信号，可以实现更稳健的流行病学预测，无需依赖辅助协变量

Abstract: Epidemiological forecasting from surveillance data is a hard problem and hybridizing mechanistic compartmental models with neural models is a natural direction. The mechanistic structure helps keep trajectories epidemiologically plausible, while neural components can capture non-stationary, data-adaptive effects. In practice, however, many seemingly straightforward couplings fail under partial observability and continually shifting transmission dynamics driven by behavior, waning immunity, seasonality, and interventions. We catalog these failure modes and show that robust performance requires making non-stationarity explicit: we extract multi-scale structure from the observed infection series and use it as an interpretable control signal for a controlled neural ODE coupled to an epidemiological model. Concretely, we decompose infections into trend, seasonal, and residual components and use these signals to drive continuous-time latent dynamics while jointly forecasting and inferring time-varying transmission, recovery, and immunity-loss rates. Across seasonal and non-seasonal settings, including early outbreaks and multi-wave regimes, our approach reduces long-horizon RMSE by 15-35%, improves peak timing error by 1-3 weeks, and lowers peak magnitude bias by up to 30% relative to strong time-series, neural ODE, and hybrid baselines, without relying on auxiliary covariates.

</details>


### [124] [Online Adaptive Reinforcement Learning with Echo State Networks for Non-Stationary Dynamics](https://arxiv.org/abs/2602.06326)
*Aoi Yoshimura,Gouhei Tanaka*

Main category: cs.LG

TL;DR: 提出基于储层计算的轻量级在线适应框架，使用回声状态网络编码观测历史，通过递归最小二乘法在线更新，无需反向传播或预训练，在非平稳环境中实现快速适应。


<details>
  <summary>Details</summary>
Motivation: 强化学习策略在仿真中训练后部署到真实世界时，由于非平稳动态导致性能严重下降。现有方法如域随机化和元强化学习需要大量预训练、特权信息或高计算成本，限制了在实时和边缘系统中的应用。

Method: 提出基于储层计算的轻量级在线适应框架，集成回声状态网络作为适应模块，将最近观测历史编码为潜在上下文表示，并使用递归最小二乘法在线更新其读出权重。该设计无需反向传播、预训练或特权信息访问。

Result: 在CartPole和HalfCheetah任务中，面对严重和突然的环境变化（包括周期性外部干扰和极端摩擦变化），该方法显著优于域随机化和代表性自适应基线，在少数控制步骤内实现稳定适应。特别地，该方法成功处理了情节内的环境变化而无需重置策略。

Conclusion: 由于计算效率和稳定性，该框架为非平稳环境中的在线适应提供了实用解决方案，非常适合真实世界机器人控制和边缘部署。

Abstract: Reinforcement learning (RL) policies trained in simulation often suffer from severe performance degradation when deployed in real-world environments due to non-stationary dynamics. While Domain Randomization (DR) and meta-RL have been proposed to address this issue, they typically rely on extensive pretraining, privileged information, or high computational cost, limiting their applicability to real-time and edge systems. In this paper, we propose a lightweight online adaptation framework for RL based on Reservoir Computing. Specifically, we integrate an Echo State Networks (ESNs) as an adaptation module that encodes recent observation histories into a latent context representation, and update its readout weights online using Recursive Least Squares (RLS). This design enables rapid adaptation without backpropagation, pretraining, or access to privileged information. We evaluate the proposed method on CartPole and HalfCheetah tasks with severe and abrupt environment changes, including periodic external disturbances and extreme friction variations. Experimental results demonstrate that the proposed approach significantly outperforms DR and representative adaptive baselines under out-of-distribution dynamics, achieving stable adaptation within a few control steps. Notably, the method successfully handles intra-episode environment changes without resetting the policy. Due to its computational efficiency and stability, the proposed framework provides a practical solution for online adaptation in non-stationary environments and is well suited for real-world robotic control and edge deployment.

</details>


### [125] [Don't Break the Boundary: Continual Unlearning for OOD Detection Based on Free Energy Repulsion](https://arxiv.org/abs/2602.06331)
*Ningkang Peng,Kun Shao,Jingyang Mao,Linjing Qian,Xiaoqian Peng,Xichen Yang,Yanhui Gu*

Main category: cs.LG

TL;DR: TFER框架通过自由能排斥力实现边界保持的类别遗忘，将目标类别转化为OOD样本，同时保持ID数据流形结构，解决OOD检测与机器遗忘的几何矛盾。


<details>
  <summary>Details</summary>
Motivation: 在开放世界中部署可信AI面临双重挑战：需要强大的OOD检测确保系统安全，同时需要灵活的机器遗忘满足隐私合规和模型修正。但现有方法存在几何矛盾——OOD检测依赖静态紧凑的数据流形，而传统遗忘方法会破坏这种结构，导致模型异常检测能力灾难性丧失。

Method: 提出TFER（总自由能排斥）框架，基于自由能原理构建推拉游戏机制：通过拉机制将保留类别锚定在低能量ID流形内，同时使用自由能排斥力主动将遗忘类别驱逐到高能量OOD区域。通过参数高效微调实现，避免完全重新训练的高成本。

Result: 实验表明TFER实现精确遗忘，同时最大程度保留模型在剩余类别和外部OOD数据上的判别性能。更重要的是，TFER的独特推拉平衡赋予模型内在结构稳定性，能有效抵抗灾难性遗忘，在持续遗忘任务中表现出卓越潜力。

Conclusion: TFER通过将遗忘类别转化为OOD样本的数学等价性，解决了OOD检测与机器遗忘的几何矛盾。其推拉平衡机制不仅实现精确遗忘，还保持模型结构稳定性，为可信AI在开放世界中的部署提供了有效解决方案。

Abstract: Deploying trustworthy AI in open-world environments faces a dual challenge: the necessity for robust Out-of-Distribution (OOD) detection to ensure system safety, and the demand for flexible machine unlearning to satisfy privacy compliance and model rectification. However, this objective encounters a fundamental geometric contradiction: current OOD detectors rely on a static and compact data manifold, whereas traditional classification-oriented unlearning methods disrupt this delicate structure, leading to a catastrophic loss of the model's capability to discriminate anomalies while erasing target classes. To resolve this dilemma, we first define the problem of boundary-preserving class unlearning and propose a pivotal conceptual shift: in the context of OOD detection, effective unlearning is mathematically equivalent to transforming the target class into OOD samples. Based on this, we propose the TFER (Total Free Energy Repulsion) framework. Inspired by the free energy principle, TFER constructs a novel Push-Pull game mechanism: it anchors retained classes within a low-energy ID manifold through a pull mechanism, while actively expelling forgotten classes to high-energy OOD regions using a free energy repulsion force. This approach is implemented via parameter-efficient fine-tuning, circumventing the prohibitive cost of full retraining. Extensive experiments demonstrate that TFER achieves precise unlearning while maximally preserving the model's discriminative performance on remaining classes and external OOD data. More importantly, our study reveals that the unique Push-Pull equilibrium of TFER endows the model with inherent structural stability, allowing it to effectively resist catastrophic forgetting without complex additional constraints, thereby demonstrating exceptional potential in continual unlearning tasks.

</details>


### [126] [Adversarial Learning in Games with Bandit Feedback: Logarithmic Pure-Strategy Maximin Regret](https://arxiv.org/abs/2602.06348)
*Shinji Ito,Haipeng Luo,Arnab Maiti,Taira Tsuchiya,Yue Wu*

Main category: cs.LG

TL;DR: 研究零和博弈中的对抗学习，关注带反馈下的纯策略极大极小遗憾最小化，提出两种反馈模型下的算法并证明对数级遗憾界。


<details>
  <summary>Details</summary>
Motivation: 现实应用中，学习者常需面对未知对手且只能获得有限反馈（仅能观察到所选动作的收益），传统方法在这种设置下存在Ω(√T)的遗憾下界，需要新的度量标准来突破这一限制。

Method: 1) 在无信息反馈模型下使用Tsallis-INF算法处理正规形式博弈；2) 在有信息反馈模型下提出Maximin-UCB算法；3) 将结果推广到任意大动作集的双线性博弈，分别提出Tsallis-FTRL-SPM和Maximin-LinUCB算法。

Result: 1) 无信息反馈下获得O(c log T)的实例依赖遗憾界，并证明对c的依赖是必要的；2) 有信息反馈下获得O(c' log T)遗憾界，其中c'可能远小于c；3) 在双线性博弈中建立了类似的博弈依赖对数遗憾界。

Conclusion: 通过定义纯策略极大极小遗憾这一新度量，在带反馈的对抗学习设置中突破了Ω(√T)的遗憾下界，为两种反馈模型设计了高效算法并建立了理论保证。

Abstract: Learning to play zero-sum games is a fundamental problem in game theory and machine learning. While significant progress has been made in minimizing external regret in the self-play settings or with full-information feedback, real-world applications often force learners to play against unknown, arbitrary opponents and restrict learners to bandit feedback where only the payoff of the realized action is observable. In such challenging settings, it is well-known that $Ω(\sqrt{T})$ external regret is unavoidable (where T is the number of rounds). To overcome this barrier, we investigate adversarial learning in zero-sum games under bandit feedback, aiming to minimize the deficit against the maximin pure strategy -- a metric we term Pure-Strategy Maximin Regret.
  We analyze this problem under two bandit feedback models: uninformed (only the realized reward is revealed) and informed (both the reward and the opponent's action are revealed). For uninformed bandit learning of normal-form games, we show that the Tsallis-INF algorithm achieves $O(c \log T)$ instance-dependent regret with a game-dependent parameter $c$. Crucially, we prove an information-theoretic lower bound showing that the dependence on c is necessary. To overcome this hardness, we turn to the informed setting and introduce Maximin-UCB, which obtains another regret bound of the form $O(c' \log T)$ for a different game-dependent parameter $c'$ that could potentially be much smaller than $c$. Finally, we generalize both results to bilinear games over an arbitrary, large action set, proposing Tsallis-FTRL-SPM and Maximin-LinUCB for the uninformed and informed setting respectively and establishing similar game-dependent logarithmic regret bounds.

</details>


### [127] [Enhance and Reuse: A Dual-Mechanism Approach to Boost Deep Forest for Label Distribution Learning](https://arxiv.org/abs/2602.06353)
*Jia-Le Xu,Shen-Huan Lyu,Yu-Nian Wang,Ning Chen,Zhihao Qu,Bin Tang,Baoliu Ye*

Main category: cs.LG

TL;DR: 提出ERDF方法，通过标签相关性特征增强和度量感知特征重用机制，将深度森林应用于标签分布学习，在六个评估指标上优于其他对比算法。


<details>
  <summary>Details</summary>
Motivation: 标签分布学习需要利用标签间相关性，但现有深度森林方法在LDL领域应用较少，且缺乏有效利用标签相关性的方法。

Method: 提出ERDF方法，包含两个核心机制：1) 利用标签相关性增强原始特征；2) 对验证集上表现较差的样本进行度量感知特征重用，确保训练稳定性。

Result: 实验表明该方法在六个评估指标上优于其他对比算法。

Conclusion: ERDF通过增强-重用模式不仅丰富了样本特征，还验证了新特征的有效性并防止噪声传播，在LDL任务中表现出色。

Abstract: Label distribution learning (LDL) requires the learner to predict the degree of correlation between each sample and each label. To achieve this, a crucial task during learning is to leverage the correlation among labels. Deep Forest (DF) is a deep learning framework based on tree ensembles, whose training phase does not rely on backpropagation. DF performs in-model feature transform using the prediction of each layer and achieves competitive performance on many tasks. However, its exploration in the field of LDL is still in its infancy. The few existing methods that apply DF to the field of LDL do not have effective ways to utilize the correlation among labels. Therefore, we propose a method named Enhanced and Reused Feature Deep Forest (ERDF). It mainly contains two mechanisms: feature enhancement exploiting label correlation and measure-aware feature reuse. The first one is to utilize the correlation among labels to enhance the original features, enabling the samples to acquire more comprehensive information for the task of LDL. The second one performs a reuse operation on the features of samples that perform worse than the previous layer on the validation set, in order to ensure the stability of the training process. This kind of Enhance-Reuse pattern not only enables samples to enrich their features but also validates the effectiveness of their new features and conducts a reuse process to prevent the noise from spreading further. Experiments show that our method outperforms other comparison algorithms on six evaluation metrics.

</details>


### [128] [Evaluating LLM-persona Generated Distributions for Decision-making](https://arxiv.org/abs/2602.06357)
*Jackie Baek,Yunhan Chen,Ziyu Chi,Will Ma*

Main category: cs.LG

TL;DR: LLM生成的分布对下游决策有用，特别是在数据稀缺时，但传统分布评估指标可能误导决策质量评估


<details>
  <summary>Details</summary>
Motivation: 研究LLM生成的分布（如消费者支付意愿模拟）对实际决策优化的有效性，评估这些分布在决策支持中的实用价值

Method: 提出LLM-SAA方法：用LLM构建估计分布，在该分布下优化决策；使用三个经典决策问题（品类优化、定价、报童问题）作为案例研究

Result: LLM生成的分布在低数据环境下特别实用，但决策无关的评估指标（如Wasserstein距离）在评估这些分布对决策的价值时可能产生误导

Conclusion: LLM生成的分布对决策制定有实际价值，特别是在数据稀缺时，但需要基于决策效果的特定指标来评估分布质量

Abstract: LLMs can generate a wealth of data, ranging from simulated personas imitating human valuations and preferences, to demand forecasts based on world knowledge. But how well do such LLM-generated distributions support downstream decision-making? For example, when pricing a new product, a firm could prompt an LLM to simulate how much consumers are willing to pay based on a product description, but how useful is the resulting distribution for optimizing the price? We refer to this approach as LLM-SAA, in which an LLM is used to construct an estimated distribution and the decision is then optimized under that distribution. In this paper, we study metrics to evaluate the quality of these LLM-generated distributions, based on the decisions they induce. Taking three canonical decision-making problems (assortment optimization, pricing, and newsvendor) as examples, we find that LLM-generated distributions are practically useful, especially in low-data regimes. We also show that decision-agnostic metrics such as Wasserstein distance can be misleading when evaluating these distributions for decision-making.

</details>


### [129] [Training Data Selection with Gradient Orthogonality for Efficient Domain Adaptation](https://arxiv.org/abs/2602.06359)
*Xiyang Zhang,Yuanhe Tian,Hongzhi Wang,Yan Song*

Main category: cs.LG

TL;DR: OGS提出一种数据选择方法，通过识别与通用知识正交的梯度样本来平衡领域专业化和通用能力保留，避免灾难性遗忘，同时保持训练效率。


<details>
  <summary>Details</summary>
Motivation: 微调大语言模型在特定领域时面临灾难性遗忘问题：获得领域专业知识的同时会损害通用推理能力。现有方法要么计算成本过高（梯度手术方法），要么无法有效处理梯度冲突（数据选择方法）。

Method: OGS将梯度投影的几何洞察从优化器转移到数据选择阶段，通过轻量级导航模型和强化学习技术，动态识别梯度与通用知识锚点正交的训练样本，实现自然安全的模型更新。

Result: 在医疗、法律和金融领域的实验表明，OGS在领域性能、训练效率和通用任务（如GSM8K）上都取得了优异结果，显著改善了领域性能同时保持或增强通用能力。

Conclusion: OGS成功解决了领域微调中的灾难性遗忘问题，通过数据选择而非优化器修改的方式，实现了领域专业化、通用能力保留和训练效率的平衡。

Abstract: Fine-tuning large language models (LLMs) for specialized domains often necessitates a trade-off between acquiring domain expertise and retaining general reasoning capabilities, a phenomenon known as catastrophic forgetting. Existing remedies face a dichotomy: gradient surgery methods offer geometric safety but incur prohibitive computational costs via online projections, while efficient data selection approaches reduce overhead but remain blind to conflict-inducing gradient directions. In this paper, we propose Orthogonal Gradient Selection (OGS), a data-centric method that harmonizes domain performance, general capability retention, and training efficiency. OGS shifts the geometric insights of gradient projection from the optimizer to the data selection stage by treating data selection as a constrained decision-making process. By leveraging a lightweight Navigator model and reinforcement learning techniques, OGS dynamically identifies training samples whose gradients are orthogonal to a general-knowledge anchor. This approach ensures naturally safe updates for target models without modifying the optimizer or incurring runtime projection costs. Experiments across medical, legal, and financial domains demonstrate that OGS achieves excellent results, significantly improving domain performance and training efficiency while maintaining or even enhancing performance on general tasks such as GSM8K.

</details>


### [130] [Uniform Spectral Growth and Convergence of Muon in LoRA-Style Matrix Factorization](https://arxiv.org/abs/2602.06385)
*Changmin Kang,Jihun Yun,Baekrok Shin,Yeseul Cho,Chulhee Yun*

Main category: cs.LG

TL;DR: 论文分析了Muon优化器在LoRA微调中的谱现象：LoRA乘积的奇异值呈现近乎均匀的增长，这与标准梯度流的"最大奇异值优先学习"形成鲜明对比。作者通过理论分析证明了谱梯度流在矩阵分解设置中的"等速率"动态和全局收敛性。


<details>
  <summary>Details</summary>
Motivation: 虽然谱梯度下降（SpecGD）及其实际实现Muon在LLM训练中表现良好，但其动态机制仍不明确。特别是在LoRA设置中，作者观察到LoRA乘积的奇异值呈现均匀增长的现象，这激发了深入的理论分析。

Method: 1. 在LoRA微调LLM的实验中观察奇异值增长模式；2. 在简化的LoRA风格矩阵分解设置中分析谱梯度流（SpecGF）；3. 理论证明奇异值的"等速率"动态；4. 证明全局收敛性，包括带ℓ2正则化的情况；5. 在相同设置下进行实验验证。

Result: 1. 发现Muon在LoRA微调中导致奇异值近乎均匀增长；2. 证明SpecGF中所有奇异值以相等速率增长（小偏差内）；3. 较小奇异值比大奇异值更早达到目标值；4. 证明SpecGF在因子范数有界时几乎全局收敛；5. 带ℓ2正则化时获得全局收敛保证。

Conclusion: 谱梯度优化在LoRA设置中展现出独特的"等速率"学习动态，与标准梯度流的"最大优先"学习模式形成鲜明对比。这种机制不仅解释了观察到的现象，还提供了理论收敛保证，为理解谱梯度方法在低秩适应中的行为提供了重要见解。

Abstract: Spectral gradient descent (SpecGD) orthogonalizes the matrix parameter updates and has inspired practical optimizers such as Muon. They often perform well in large language model (LLM) training, but their dynamics remain poorly understood. In the low-rank adaptation (LoRA) setting, where weight updates are parameterized as a product of two low-rank factors, we find a distinctive spectral phenomenon under Muon in LoRA fine-tuning of LLMs: singular values of the LoRA product show near-uniform growth across the spectrum, despite orthogonalization being performed on the two factors separately. Motivated by this observation, we analyze spectral gradient flow (SpecGF)-a continuous-time analogue of SpecGD-in a simplified LoRA-style matrix factorization setting and prove "equal-rate" dynamics: all singular values grow at equal rates up to small deviations. Consequently, smaller singular values attain their target values earlier than larger ones, sharply contrasting with the largest-first stepwise learning observed in standard gradient flow. Moreover, we prove that SpecGF in our setting converges to global minima from almost all initializations, provided the factor norms remain bounded; with $\ell_2$ regularization, we obtain global convergence. Lastly, we corroborate our theory with experiments in the same setting.

</details>


### [131] [Generating High-quality Privacy-preserving Synthetic Data](https://arxiv.org/abs/2602.06390)
*David Yavo,Richard Khoury,Christophe Pere,Sadoune Ait Kaci Azzou*

Main category: cs.LG

TL;DR: 提出一种模型无关的后处理框架，通过模式修补和k近邻过滤来提升合成表格数据的分布保真度、下游效用和隐私保护平衡。


<details>
  <summary>Details</summary>
Motivation: 合成表格数据在实际部署中需要在分布保真度、下游效用和隐私保护之间取得平衡，现有方法往往难以同时优化这三个方面。

Method: 提出两阶段后处理框架：1) 模式修补修复合成数据中缺失或严重不足的类别，同时保持学习到的依赖关系；2) k近邻过滤替换过于接近真实数据的合成记录，强制执行最小距离要求。

Result: 在0.2-0.35阈值下，后处理将真实与合成分类分布差异降低达36%，成对依赖保持指标提升10-14%，下游预测性能保持在未处理基线的1%以内，同时距离隐私指标改善而属性推理攻击成功率基本不变。

Conclusion: 该后处理框架为选择阈值和应用后修复提供了实用指导，能提升合成表格数据的质量和经验隐私，同时补充提供形式化差分隐私保证的方法。

Abstract: Synthetic tabular data enables sharing and analysis of sensitive records, but its practical deployment requires balancing distributional fidelity, downstream utility, and privacy protection. We study a simple, model agnostic post processing framework that can be applied on top of any synthetic data generator to improve this trade off. First, a mode patching step repairs categories that are missing or severely underrepresented in the synthetic data, while largely preserving learned dependencies. Second, a k nearest neighbor filter replaces synthetic records that lie too close to real data points, enforcing a minimum distance between real and synthetic samples. We instantiate this framework for two neural generative models for tabular data, a feed forward generator and a variational autoencoder, and evaluate it on three public datasets covering credit card transactions, cardiovascular health, and census based income. We assess marginal and joint distributional similarity, the performance of models trained on synthetic data and evaluated on real data, and several empirical privacy indicators, including nearest neighbor distances and attribute inference attacks. With moderate thresholds between 0.2 and 0.35, the post processing reduces divergence between real and synthetic categorical distributions by up to 36 percent and improves a combined measure of pairwise dependence preservation by 10 to 14 percent, while keeping downstream predictive performance within about 1 percent of the unprocessed baseline. At the same time, distance based privacy indicators improve and the success rate of attribute inference attacks remains largely unchanged. These results provide practical guidance for selecting thresholds and applying post hoc repairs to improve the quality and empirical privacy of synthetic tabular data, while complementing approaches that provide formal differential privacy guarantees.

</details>


### [132] [Near-Optimal Regret for Distributed Adversarial Bandits: A Black-Box Approach](https://arxiv.org/abs/2602.06404)
*Hao Qiu,Mengxiao Zhang,Nicolò Cesa-Bianchi*

Main category: cs.LG

TL;DR: 本文研究了分布式对抗性多臂老虎机问题，提出了基于延迟反馈黑盒约简的新算法，获得了$\tildeΘ(\sqrt{(ρ^{-1/2}+K/N)T})$的极小极大遗憾界，显著改进了先前最佳结果$\tilde{O}(ρ^{-1/3}(KT)^{2/3})$。


<details>
  <summary>Details</summary>
Motivation: 研究分布式对抗性多臂老虎机问题，其中N个智能体通过观察各自的局部损失来合作最小化全局平均损失。现有最佳遗憾界为$\tilde{O}(ρ^{-1/3}(KT)^{2/3})$，存在改进空间。

Method: 提出基于延迟反馈黑盒约简的新算法，智能体仅通过gossip协议进行通信。将分布式问题约简为具有延迟反馈的经典老虎机问题，利用通信矩阵的谱间隙ρ来量化通信效率。

Result: 获得了$\tildeΘ(\sqrt{(ρ^{-1/2}+K/N)T})$的极小极大遗憾界，包含通信成本$ρ^{-1/4}\sqrt{T}$和老虎机成本$\sqrt{KT/N}$两部分。算法通信开销低，且可扩展到分布式线性老虎机，获得$\tilde{O}(\sqrt{(ρ^{-1/2}+1/N)dT})$遗憾界，每轮每智能体仅需$O(d)$通信成本。

Conclusion: 本文为分布式对抗性老虎机问题建立了紧的极小极大遗憾界，揭示了问题难度可分解为通信成本和老虎机成本。提出的黑盒约简方法具有普适性，可扩展到一阶界和best-of-both-worlds界，以及分布式线性老虎机场景。

Abstract: We study distributed adversarial bandits, where $N$ agents cooperate to minimize the global average loss while observing only their own local losses. We show that the minimax regret for this problem is $\tildeΘ(\sqrt{(ρ^{-1/2}+K/N)T})$, where $T$ is the horizon, $K$ is the number of actions, and $ρ$ is the spectral gap of the communication matrix. Our algorithm, based on a novel black-box reduction to bandits with delayed feedback, requires agents to communicate only through gossip. It achieves an upper bound that significantly improves over the previous best bound $\tilde{O}(ρ^{-1/3}(KT)^{2/3})$ of Yi and Vojnovic (2023). We complement this result with a matching lower bound, showing that the problem's difficulty decomposes into a communication cost $ρ^{-1/4}\sqrt{T}$ and a bandit cost $\sqrt{KT/N}$. We further demonstrate the versatility of our approach by deriving first-order and best-of-both-worlds bounds in the distributed adversarial setting. Finally, we extend our framework to distributed linear bandits in $R^d$, obtaining a regret bound of $\tilde{O}(\sqrt{(ρ^{-1/2}+1/N)dT})$, achieved with only $O(d)$ communication cost per agent and per round via a volumetric spanner.

</details>


### [133] [EEG Emotion Classification Using an Enhanced Transformer-CNN-BiLSTM Architecture with Dual Attention Mechanisms](https://arxiv.org/abs/2602.06411)
*S M Rakib UI Karim,Wenyi Lu,Diponkor Bala,Rownak Ara Rasul,Sean Goggins*

Main category: cs.LG

TL;DR: 本文提出了一种结合卷积、循环和注意力机制的混合深度学习架构，用于提升EEG情感识别的性能和鲁棒性，在公开数据集上取得了最先进的分类效果。


<details>
  <summary>Details</summary>
Motivation: 基于EEG的情感识别在情感计算和决策支持系统中至关重要，但由于EEG信号具有高维度、噪声多和个体差异大的特点，现有方法面临挑战。需要开发更强大的模型来提升分类性能和鲁棒性。

Method: 提出增强的混合模型：结合卷积特征提取、双向时序建模和自注意力机制，并采用正则化策略防止过拟合。模型整合了多种深度学习组件来有效处理EEG信号的时空特性。

Result: 在包含中性、积极和消极三种情感状态的公开EEG数据集上，该方法达到了最先进的分类性能，显著优于传统机器学习和神经网络基线。统计测试证实了交叉验证下的鲁棒性提升。特征分析显示协方差特征对情感区分贡献最大。

Conclusion: 精心设计的混合架构能够在EEG情感识别中有效平衡预测准确性、鲁棒性和可解释性，对应用情感计算和人本智能系统具有重要意义。

Abstract: Electroencephalography (EEG)-based emotion recognition plays a critical role in affective computing and emerging decision-support systems, yet remains challenging due to high-dimensional, noisy, and subject-dependent signals. This study investigates whether hybrid deep learning architectures that integrate convolutional, recurrent, and attention-based components can improve emotion classification performance and robustness in EEG data. We propose an enhanced hybrid model that combines convolutional feature extraction, bidirectional temporal modeling, and self-attention mechanisms with regularization strategies to mitigate overfitting. Experiments conducted on a publicly available EEG dataset spanning three emotional states (neutral, positive, and negative) demonstrate that the proposed approach achieves state-of-the-art classification performance, significantly outperforming classical machine learning and neural baselines. Statistical tests confirm the robustness of these performance gains under cross-validation. Feature-level analyses further reveal that covariance-based EEG features contribute most strongly to emotion discrimination, highlighting the importance of inter-channel relationships in affective modeling. These findings suggest that carefully designed hybrid architectures can effectively balance predictive accuracy, robustness, and interpretability in EEG-based emotion recognition, with implications for applied affective computing and human-centered intelligent systems.

</details>


### [134] [Adaptive Protein Tokenization](https://arxiv.org/abs/2602.06418)
*Rohit Dilip,Ayush Varshney,David Van Valen*

Main category: cs.LG

TL;DR: 提出了一种全局蛋白质结构标记化方法，通过逐步增加细节的全局表示解决局部标记化的问题，在生成、重建和表示任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有蛋白质结构标记化方法通过局部邻域池化信息创建标记，这种方法限制了在生成和表示任务中的性能。需要一种全局标记化方法来克服这些问题。

Method: 提出全局蛋白质结构标记化方法，其中连续标记以递增的细节水平贡献于全局表示。该方法缓解了错误累积，提供了无需序列缩减操作的嵌入，并允许根据任务特定需求调整标记序列的信息内容。

Result: 在重建、生成和表示任务中验证了该方法，匹配或超越了基于局部蛋白质结构标记器的现有模型。自适应标记实现了基于信息内容的推理标准，提高了可设计性。在CATH分类任务中表现优异，非线性探测优于其他标记器。支持零样本蛋白质收缩和亲和力成熟。

Conclusion: 全局蛋白质结构标记化方法解决了局部标记化的关键问题，在多种任务中表现出色，为多模态蛋白质理解提供了有前景的路径。

Abstract: Tokenization is a promising path to multi-modal models capable of jointly understanding protein sequences, structure, and function. Existing protein structure tokenizers create tokens by pooling information from local neighborhoods, an approach that limits their performance on generative and representation tasks. In this work, we present a method for global tokenization of protein structures in which successive tokens contribute increasing levels of detail to a global representation. This change resolves several issues with generative models based on local protein tokenization: it mitigates error accumulation, provides embeddings without sequence-reduction operations, and allows task-specific adaptation of a tokenized sequence's information content. We validate our method on reconstruction, generative, and representation tasks and demonstrate that it matches or outperforms existing models based on local protein structure tokenizers. We show how adaptive tokens enable inference criteria based on information content, which boosts designability. We validate representations generated from our tokenizer on CATH classification tasks and demonstrate that non-linear probing on our tokenized sequences outperforms equivalent probing on representations from other tokenizers. Finally, we demonstrate how our method supports zero-shot protein shrinking and affinity maturation.

</details>


### [135] [Beyond Code Contributions: How Network Position, Temporal Bursts, and Code Review Activities Shape Contributor Influence in Large-Scale Open Source Ecosystems](https://arxiv.org/abs/2602.06426)
*S M Rakib Ul Karim,Wenyi Lu,Sean Goggins*

Main category: cs.LG

TL;DR: 该研究使用图神经网络和时间网络分析对25年开源软件贡献者网络进行综合分析，发现网络影响力呈幂律分布，识别了五种贡献者角色，并揭示了网络结构随时间变化的显著趋势。


<details>
  <summary>Details</summary>
Motivation: 开源软件项目依赖复杂的贡献者网络来推动创新和可持续性，但缺乏对这些网络结构、动态变化和关键角色影响的全面实证分析。

Method: 使用GPU加速的PageRank、中介中心性、自定义LSTM模型和图神经网络，对云原生计算基金会生态系统25年的数据进行分析，涵盖沙盒、孵化和毕业项目。

Result: 发现贡献者网络影响力呈强幂律分布（前1%贡献者控制大部分影响力），识别出五种贡献者角色（核心、桥梁、连接器、常规、外围），特定行动类型与影响力显著相关，网络密度、聚类系数和模块性随时间呈现显著趋势变化。

Conclusion: 研究结果为战略贡献者保留政策提供了实证依据，并为社区健康指标提供了可操作的见解，特别是桥梁贡献者虽少但对网络凝聚力有不成比例的重要影响。

Abstract: Open source software (OSS) projects rely on complex networks of contributors whose interactions drive innovation and sustainability. This study presents a comprehensive analysis of OSS contributor networks using advanced graph neural networks and temporal network analysis on data spanning 25 years from the Cloud Native Computing Foundation ecosystem, encompassing sandbox, incubating, and graduated projects. Our analysis of thousands of contributors across hundreds of repositories reveals that OSS networks exhibit strong power-law distributions in influence, with the top 1\% of contributors controlling a substantial portion of network influence. Using GPU-accelerated PageRank, betweenness centrality, and custom LSTM models, we identify five distinct contributor roles: Core, Bridge, Connector, Regular, and Peripheral, each with unique network positions and structural importance. Statistical analysis reveals significant correlations between specific action types (commits, pull requests, issues) and contributor influence, with multiple regression models explaining substantial variance in influence metrics. Temporal analysis shows that network density, clustering coefficients, and modularity exhibit statistically significant temporal trends, with distinct regime changes coinciding with major project milestones. Structural integrity simulations show that Bridge contributors, despite representing a small fraction of the network, have a disproportionate impact on network cohesion when removed. Our findings provide empirical evidence for strategic contributor retention policies and offer actionable insights into community health metrics.

</details>


### [136] [Reclaiming First Principles: A Differentiable Framework for Conceptual Hydrologic Models](https://arxiv.org/abs/2602.06429)
*Jasper A. Vrugt,Jonathan M. Frame,Ethan Bollman*

Main category: cs.LG

TL;DR: 提出一个完全解析的、计算高效的框架，用于基于精确参数敏感性的可微分水文建模，通过增强控制ODE系统与敏感性方程，联合演化模型状态和关于所有参数的雅可比矩阵。


<details>
  <summary>Details</summary>
Motivation: 概念水文模型的校准通常缓慢且数值脆弱。基于梯度的参数估计方法依赖于有限差分近似或自动微分框架，这些方法计算量大，引入截断误差、求解器不稳定性和大量开销，特别对于概念流域模型的ODE系统。

Method: 通过增强控制ODE系统与敏感性方程，联合演化模型状态和关于所有参数的雅可比矩阵。该雅可比矩阵为任何可微分损失函数提供完全解析的梯度向量，包括经典目标函数、水文性能指标、鲁棒损失函数和水文图基函数。

Result: 解析敏感性消除了数值微分的步长依赖性和噪声，避免了伴随方法的不稳定性和现代机器学习自动微分工具链的开销。得到的梯度是确定性的、物理可解释的，并且易于嵌入基于梯度的优化器中。

Conclusion: 这项工作实现了概念水文模型的快速、稳定和透明的基于梯度的校准，释放了可微分建模的全部潜力，无需依赖外部、不透明或CPU密集的自动微分库。

Abstract: Conceptual hydrologic models remain the cornerstone of rainfall-runoff modeling, yet their calibration is often slow and numerically fragile. Most gradient-based parameter estimation methods rely on finite-difference approximations or automatic differentiation frameworks (e.g., JAX, PyTorch and TensorFlow), which are computationally demanding and introduce truncation errors, solver instabilities, and substantial overhead. These limitations are particularly acute for the ODE systems of conceptual watershed models. Here we introduce a fully analytic and computationally efficient framework for differentiable hydrologic modeling based on exact parameter sensitivities. By augmenting the governing ODE system with sensitivity equations, we jointly evolve the model states and the Jacobian matrix with respect to all parameters. This Jacobian then provides fully analytic gradient vectors for any differentiable loss function. These include classical objective functions such as the sum of absolute and squared residuals, widely used hydrologic performance metrics such as the Nash-Sutcliffe and Kling-Gupta efficiencies, robust loss functions that down-weight extreme events, and hydrograph-based functionals such as flow-duration and recession curves. The analytic sensitivities eliminate the step-size dependence and noise inherent to numerical differentiation, while avoiding the instability of adjoint methods and the overhead of modern machine-learning autodiff toolchains. The resulting gradients are deterministic, physically interpretable, and straightforward to embed in gradient-based optimizers. Overall, this work enables rapid, stable, and transparent gradient-based calibration of conceptual hydrologic models, unlocking the full potential of differentiable modeling without reliance on external, opaque, or CPU-intensive automatic-differentiation libraries.

</details>


### [137] [Is Gradient Ascent Really Necessary? Memorize to Forget for Machine Unlearning](https://arxiv.org/abs/2602.06441)
*Zhuo Huang,Qizhou Wang,Ziming Hong,Shanshan Ye,Bo Han,Tongliang Liu*

Main category: cs.LG

TL;DR: 提出模型外推法替代梯度上升进行机器遗忘，通过训练记忆模型然后外推到参考模型来稳定遗忘过程


<details>
  <summary>Details</summary>
Motivation: 机器遗忘对AI伦理和安全至关重要，但传统梯度上升方法容易导致灾难性崩溃和性能下降，需要更稳定的解决方案

Method: 使用模型外推法：1) 以原始模型为参考，训练记忆模型记住不需要的数据；2) 从记忆模型外推到参考模型得到遗忘模型，避免直接使用梯度上升

Result: 该方法简单高效，能有效收敛并提高遗忘性能，稳定了机器遗忘过程

Conclusion: 模型外推法为机器遗忘提供了稳定有效的替代方案，避免了梯度上升的灾难性崩溃问题

Abstract: For ethical and safe AI, machine unlearning rises as a critical topic aiming to protect sensitive, private, and copyrighted knowledge from misuse. To achieve this goal, it is common to conduct gradient ascent (GA) to reverse the training on undesired data. However, such a reversal is prone to catastrophic collapse, which leads to serious performance degradation in general tasks. As a solution, we propose model extrapolation as an alternative to GA, which reaches the counterpart direction in the hypothesis space from one model given another reference model. Therefore, we leverage the original model as the reference, further train it to memorize undesired data while keeping prediction consistency on the rest retained data, to obtain a memorization model. Counterfactual as it might sound, a forget model can be obtained via extrapolation from the memorization model to the reference model. Hence, we avoid directly acquiring the forget model using GA, but proceed with gradient descent for the memorization model, which successfully stabilizes the machine unlearning process. Our model extrapolation is simple and efficient to implement, and it can also effectively converge throughout training to achieve improved unlearning performance.

</details>


### [138] [Principle-Evolvable Scientific Discovery via Uncertainty Minimization](https://arxiv.org/abs/2602.06448)
*Yingming Pu,Tao Lin,Hongyu Chen*

Main category: cs.LG

TL;DR: PiEvo是一个基于大语言模型的科学发现框架，通过贝叶斯优化在扩展的原则空间中演化科学原理，而非在固定假设空间中搜索，显著提升了发现效率和效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的科学代理通常固守初始先验，在静态假设空间中运作，这限制了新现象的发现，并在基线理论失败时造成计算浪费。需要从搜索假设转向演化基础科学原理。

Method: 提出PiEvo框架，将科学发现视为在扩展原则空间上的贝叶斯优化。通过高斯过程的信息导向假设选择和异常驱动的增强机制，使代理能够自主完善其理论世界观。

Result: 在四个基准测试中，PiEvo达到90.81%~93.15%的平均解决方案质量，比现有最佳方法提升29.7%~31.1%；通过优化紧凑原则空间显著减少样本复杂度，实现83.3%的收敛速度提升；在不同科学领域和大语言模型骨干上保持稳健性能。

Conclusion: PiEvo通过演化科学原理而非搜索假设，有效解决了现有科学代理的效率限制，为自主科学发现提供了更高效、更稳健的框架。

Abstract: Large Language Model (LLM)-based scientific agents have accelerated scientific discovery, yet they often suffer from significant inefficiencies due to adherence to fixed initial priors. Existing approaches predominantly operate within a static hypothesis space, which restricts the discovery of novel phenomena, resulting in computational waste when baseline theories fail. To address this, we propose shifting the focus from searching hypotheses to evolving the underlying scientific principles. We present PiEvo, a principle-evolvable framework that treats scientific discovery as Bayesian optimization over an expanding principle space. By integrating Information-Directed Hypothesis Selection via Gaussian Process and an anomaly-driven augmentation mechanism, PiEvo enables agents to autonomously refine their theoretical worldview. Evaluation across four benchmarks demonstrates that PiEvo (1) achieves an average solution quality of up to 90.81%~93.15%, representing a 29.7%~31.1% improvement over the state-of-the-art, (2) attains an 83.3% speedup in convergence step via significantly reduced sample complexity by optimizing the compact principle space, and (3) maintains robust performance across diverse scientific domains and LLM backbones.

</details>


### [139] [BrokenBind: Universal Modality Exploration beyond Dataset Boundaries](https://arxiv.org/abs/2602.06451)
*Zhuo Huang,Runnan Chen,Bo Han,Gang Niu,Masashi Sugiyama,Tongliang Liu*

Main category: cs.LG

TL;DR: BrokenBind是一种新颖的多模态学习方法，能够将不同数据集中的模态绑定在一起，突破了传统方法只能处理同一数据集中模态的限制，实现了更灵活和通用的模态探索。


<details>
  <summary>Details</summary>
Motivation: 现有多模态学习方法只能绑定同一数据集中的模态，导致泛化到未出现模态时存在偏差，且受限于多模态数据集的获取成本。需要一种能够跨数据集绑定任意模态的灵活方法。

Method: BrokenBind同时利用包含目标模态的多个数据集和一个共享模态。尽管数据集之间存在分布不匹配，但通过捕捉它们的关系生成伪嵌入来填补缺失的目标模态，实现灵活的跨数据集模态绑定。

Result: 通过广泛评估，BrokenBind在多种场景下表现出色，包括需要多个数据集的强化场景和低数据机制，相比知名多模态基线方法具有显著优势。

Conclusion: BrokenBind突破了传统多模态学习的局限性，实现了任意两个模态的跨数据集绑定，为通用模态探索提供了可行方案，特别是在数据获取成本受限的情况下具有重要价值。

Abstract: Multi-modal learning combines various modalities to provide a comprehensive understanding of real-world problems. A common strategy is to directly bind different modalities together in a specific joint embedding space. However, the capability of existing methods is restricted within the modalities presented in the given dataset, thus they are biased when generalizing to unpresented modalities in downstream tasks. As a result, due to such inflexibility, the viability of previous methods is seriously hindered by the cost of acquiring multi-modal datasets. In this paper, we introduce BrokenBind, which focuses on binding modalities that are presented from different datasets. To achieve this, BrokenBind simultaneously leverages multiple datasets containing the modalities of interest and one shared modality. Though the two datasets do not correspond to each other due to distribution mismatch, we can capture their relationship to generate pseudo embeddings to fill in the missing modalities of interest, enabling flexible and generalized multi-modal learning. Under our framework, any two modalities can be bound together, free from the dataset limitation, to achieve universal modality exploration. Further, to reveal the capability of our method, we study intensified scenarios where more than two datasets are needed for modality binding and show the effectiveness of BrokenBind in low-data regimes. Through extensive evaluation, we carefully justify the superiority of BrokenBind compared to well-known multi-modal baseline methods.

</details>


### [140] [On the Plasticity and Stability for Post-Training Large Language Models](https://arxiv.org/abs/2602.06453)
*Wenwen Qiang,Ziyin Gu,Jiahuan Zhou,Jie Hu,Jingyao Wang,Changwen Zheng,Hui Xiong*

Main category: cs.LG

TL;DR: 提出PCR方法解决GRPO训练稳定性问题，通过贝叶斯框架建模梯度为随机变量，实现不确定性感知的软投影机制


<details>
  <summary>Details</summary>
Motivation: GRPO训练稳定性是关键瓶颈，表现为推理可塑性与通用能力保持之间的权衡。根本原因是可塑性与稳定性梯度之间的几何冲突导致破坏性干扰，而确定性投影方法忽略了基于群体的梯度估计的固有随机性

Method: 提出概率冲突解决(PCR)方法，这是一个贝叶斯框架，将梯度建模为随机变量。PCR通过不确定性感知的"软投影"机制动态仲裁冲突，优化信噪比

Result: 大量实验表明，PCR显著平滑了训练轨迹，在各种推理任务中实现了优越性能

Conclusion: PCR通过概率方法有效解决了GRPO训练中的稳定性问题，相比确定性投影方法更优，能够更好地处理基于群体的梯度估计的随机性

Abstract: Training stability remains a critical bottleneck for Group Relative Policy Optimization (GRPO), often manifesting as a trade-off between reasoning plasticity and general capability retention. We identify a root cause as the geometric conflict between plasticity and stability gradients, which leads to destructive interference. Crucially, we argue that deterministic projection methods are suboptimal for GRPO as they overlook the intrinsic stochasticity of group-based gradient estimates. To address this, we propose Probabilistic Conflict Resolution (PCR), a Bayesian framework that models gradients as random variables. PCR dynamically arbitrates conflicts via an uncertainty-aware ``soft projection'' mechanism, optimizing the signal-to-noise ratio. Extensive experiments demonstrate that PCR significantly smooths the training trajectory and achieves superior performance in various reasoning tasks.

</details>


### [141] [The Window Dilemma: Why Concept Drift Detection is Ill-Posed](https://arxiv.org/abs/2602.06456)
*Brandon Gower-Winter,Misja Groen,Georg Krempl*

Main category: cs.LG

TL;DR: 传统漂移检测器存在窗口困境问题，漂移检测本身定义不明确，实证显示传统批量学习方法常优于漂移感知方法


<details>
  <summary>Details</summary>
Motivation: 数据流中的概念漂移现象已被广泛研究，漂移检测器通过比较数据窗口来检测变化。但作者观察到存在窗口困境问题：检测到的漂移可能是窗口选择的产物而非真实数据生成过程的变化，且漂移检测在实践中难以验证

Method: 首先通过示例说明窗口困境问题，然后对漂移检测器与多种替代适应策略进行实证比较，包括传统批量学习方法

Result: 主要发现是传统批量学习方法通常优于漂移感知方法，这进一步质疑了漂移检测器在流分类中的实际价值

Conclusion: 漂移检测存在根本性问题（窗口困境和验证困难），传统学习方法在实际应用中可能比复杂的漂移检测方法更有效，需要重新思考漂移检测在流分类中的作用

Abstract: Non-stationarity of an underlying data generating process that leads to distributional changes over time is a key characteristic of Data Streams. This phenomenon, commonly referred to as Concept Drift, has been intensively studied, and Concept Drift Detectors have been established as a class of methods for detecting such changes (drifts). For the most part, Drift Detectors compare regions (windows) of the data stream and detect drift if those windows are sufficiently dissimilar.
  In this work, we introduce the Window Dilemma, an observation that perceived drift is a product of windowing and not necessarily the underlying data generating process. Additionally, we highlight that drift detection is ill-posed, primarily because verification of drift events are implausible in practice. We demonstrate these contributions first by an illustrative example, followed by empirical comparisons of drift detectors against a variety of alternative adaptation strategies. Our main finding is that traditional batch learning techniques often perform better than their drift-aware counterparts further bringing into question the purpose of detectors in Stream Classification.

</details>


### [142] [Towards Generalizable Reasoning: Group Causal Counterfactual Policy Optimization for LLM Reasoning](https://arxiv.org/abs/2602.06475)
*Jingyao Wang,Peizheng Guo,Wenwen Qiang,Jiahuan Zhou,Huijie Guo,Changwen Zheng,Hui Xiong*

Main category: cs.LG

TL;DR: 提出Group Causal Counterfactual Policy Optimization方法，通过反事实推理奖励机制训练LLMs学习可泛化的推理模式，解决现有奖励机制只关注最终答案正确性而忽略推理过程质量的问题。


<details>
  <summary>Details</summary>
Motivation: 现有奖励机制过于关注最终答案正确性，忽略了推理过程的质量：合理的推理但答案错误会得到低分，而逻辑有缺陷的幸运猜测却可能获得高分，这影响了推理的泛化能力。

Method: 从因果视角将多候选推理解释为反事实实验，提出Group Causal Counterfactual Policy Optimization方法，设计包含稳健性和有效性的反事实奖励，并构建token级优势函数优化策略。

Result: 在多个基准测试上的广泛实验证明了该方法的优势，能够有效提升LLMs的推理泛化能力。

Conclusion: 通过反事实推理奖励机制，能够训练LLMs学习过程有效且反事实稳健的推理模式，显著提升推理的泛化能力。

Abstract: Large language models (LLMs) excel at complex tasks with advances in reasoning capabilities. However, existing reward mechanisms remain tightly coupled to final correctness and pay little attention to the underlying reasoning process: trajectories with sound reasoning but wrong answers receive low credit, while lucky guesses with flawed logic may be highly rewarded, affecting reasoning generalization. From a causal perspective, we interpret multi-candidate reasoning for a fixed question as a family of counterfactual experiments with theoretical supports. Building on this, we propose Group Causal Counterfactual Policy Optimization to explicitly train LLMs to learn generalizable reasoning patterns. It proposes an episodic causal counterfactual reward that jointly captures (i) robustness, encouraging the answer distribution induced by a reasoning step to remain stable under counterfactual perturbations; and (ii) effectiveness, enforcing sufficient variability so that the learned reasoning strategy can transfer across questions. We then construct token-level advantages from this reward and optimize the policy, encouraging LLMs to favor reasoning patterns that are process-valid and counterfactually robust. Extensive experiments on diverse benchmarks demonstrate its advantages.

</details>


### [143] [Adaptive Uncertainty-Aware Tree Search for Robust Reasoning](https://arxiv.org/abs/2602.06493)
*Zeen Song,Zihao Ma,Wenwen Qiang,Changwen Zheng,Gang Hua*

Main category: cs.LG

TL;DR: 本文提出UATS方法解决PRM在OOD推理路径评估中的不确定性挑战，通过不确定性感知树搜索和RL控制器动态分配计算资源，显著降低OOD错误影响。


<details>
  <summary>Details</summary>
Motivation: 现有基于过程奖励模型（PRM）的外部搜索方法存在根本限制：当评估偏离训练分布的推理路径时，PRM存在认知不确定性，导致不可靠的评分。

Method: 提出不确定性感知树搜索（UATS）：1）通过蒙特卡洛Dropout估计不确定性；2）使用基于强化学习的控制器动态分配计算预算；3）将不确定性纳入搜索策略。

Result: 实验表明UATS能有效缓解OOD错误的影响，相比标准搜索的线性遗憾积累，不确定性感知策略可实现次线性遗憾。

Conclusion: UATS为处理PRM在推理路径评估中的不确定性提供了统一框架，显著提升了LLM在复杂问题解决中的推理能力，特别是在面对分布外样本时。

Abstract: Inference-time reasoning scaling has significantly advanced the capabilities of Large Language Models (LLMs) in complex problem-solving. A prevalent approach involves external search guided by Process Reward Models (PRMs). However, a fundamental limitation of this framework is the epistemic uncertainty of PRMs when evaluating reasoning paths that deviate from their training distribution. In this work, we conduct a systematic analysis of this challenge. We first provide empirical evidence that PRMs exhibit high uncertainty and unreliable scoring on out-of-distribution (OOD) samples. We then establish a theoretical framework proving that while standard search incurs linear regret accumulation, an uncertainty-aware strategy can achieve sublinear regret. Motivated by these findings, we propose Uncertainty-Aware Tree Search (UATS), a unified method that estimates uncertainty via Monte Carlo Dropout and dynamically allocates compute budget using a reinforcement learning-based controller. Extensive experiments demonstrate that our approach effectively mitigates the impact of OOD errors.

</details>


### [144] [Can Microcanonical Langevin Dynamics Leverage Mini-Batch Gradient Noise?](https://arxiv.org/abs/2602.06500)
*Emanuel Sommer,Kangning Diao,Jakob Robnik,Uros Seljak,David Rügamer*

Main category: cs.LG

TL;DR: 提出了一种新的随机梯度微正则朗之万蒙特卡洛采样器（SMILE），通过梯度噪声预调节和自适应步长调谐器，解决了传统微正则方法在大规模贝叶斯深度学习中的计算瓶颈。


<details>
  <summary>Details</summary>
Motivation: 微正则朗之万蒙特卡洛（microcanonical Langevin Monte Carlo）在贝叶斯深度学习中表现出色，但依赖全数据集梯度计算，在大规模问题上计算成本过高。需要研究微正则动力学能否有效利用小批量梯度噪声。

Method: 1）建立了随机梯度微正则动力学的连续时间理论分析框架；2）提出了梯度噪声预调节方案来减少理论偏差；3）开发了基于能量方差的自适应调谐器，自动选择步长并动态设置数值防护；4）结合集成技术，形成了随机微正则朗之万集成（SMILE）采样器。

Result: 揭示了两种关键失效模式：各向异性梯度噪声导致的理论偏差，以及复杂高维后验中的数值不稳定性。提出的方法显著减少了偏差，在贝叶斯神经网络等高维推理任务上实现了最先进的性能。

Conclusion: 成功开发了鲁棒且可扩展的微正则蒙特卡洛采样器，解决了大规模贝叶斯推理中的计算瓶颈，为大规模贝叶斯深度学习提供了一类新的高效采样方法。

Abstract: Scaling inference methods such as Markov chain Monte Carlo to high-dimensional models remains a central challenge in Bayesian deep learning. A promising recent proposal, microcanonical Langevin Monte Carlo, has shown state-of-the-art performance across a wide range of problems. However, its reliance on full-dataset gradients makes it prohibitively expensive for large-scale problems. This paper addresses a fundamental question: Can microcanonical dynamics effectively leverage mini-batch gradient noise? We provide the first systematic study of this problem, establishing a novel continuous-time theoretical analysis of stochastic-gradient microcanonical dynamics. We reveal two critical failure modes: a theoretically derived bias due to anisotropic gradient noise and numerical instabilities in complex high-dimensional posteriors. To tackle these issues, we propose a principled gradient noise preconditioning scheme shown to significantly reduce this bias and develop a novel, energy-variance-based adaptive tuner that automates step size selection and dynamically informs numerical guardrails. The resulting algorithm is a robust and scalable microcanonical Monte Carlo sampler that achieves state-of-the-art performance on challenging high-dimensional inference tasks like Bayesian neural networks. Combined with recent ensemble techniques, our work unlocks a new class of stochastic microcanonical Langevin ensemble (SMILE) samplers for large-scale Bayesian inference.

</details>


### [145] [Evolutionary Generation of Multi-Agent Systems](https://arxiv.org/abs/2602.06511)
*Yuntong Hu,Matthew Trager,Yuting Zhang,Yi Zhang,Shuo Yang,Wei Xia,Stefano Soatto*

Main category: cs.LG

TL;DR: EvoMAS：一种基于进化算法的多智能体系统自动生成框架，通过结构化配置空间中的进化生成，显著提升任务性能、可执行性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM-based多智能体系统设计存在人工设计成本高、脆弱且难以泛化的问题。现有自动生成方法要么依赖代码生成导致可执行性和鲁棒性问题，要么采用僵化的架构模板限制表达能力和适应性。

Method: 将MAS生成建模为结构化配置生成问题，在配置空间中进行进化生成：从候选池选择初始配置，基于执行轨迹的反馈指导进行变异和交叉操作，迭代优化候选池和经验记忆。

Result: 在BBEH、SWE-Bench和WorkBench等多个基准测试中，EvoMAS在推理、软件工程和工具使用任务上均优于人工设计的MAS和现有自动生成方法。在BBEH推理任务上比EvoAgent提升10.5分，WorkBench提升7.1分，使用Claude-4.5-Sonnet在SWE-Bench-Verified上达到79.1%，达到排行榜前列。

Conclusion: EvoMAS通过进化生成方法有效解决了多智能体系统自动生成的问题，在性能、可执行性和鲁棒性方面均取得显著提升，为复杂任务的自动化系统设计提供了新思路。

Abstract: Large language model (LLM)-based multi-agent systems (MAS) show strong promise for complex reasoning, planning, and tool-augmented tasks, but designing effective MAS architectures remains labor-intensive, brittle, and hard to generalize. Existing automatic MAS generation methods either rely on code generation, which often leads to executability and robustness failures, or impose rigid architectural templates that limit expressiveness and adaptability. We propose Evolutionary Generation of Multi-Agent Systems (EvoMAS), which formulates MAS generation as structured configuration generation. EvoMAS performs evolutionary generation in configuration space. Specifically, EvoMAS selects initial configurations from a pool, applies feedback-conditioned mutation and crossover guided by execution traces, and iteratively refines both the candidate pool and an experience memory. We evaluate EvoMAS on diverse benchmarks, including BBEH, SWE-Bench, and WorkBench, covering reasoning, software engineering, and tool-use tasks. EvoMAS consistently improves task performance over both human-designed MAS and prior automatic MAS generation methods, while producing generated systems with higher executability and runtime robustness. EvoMAS outperforms the agent evolution method EvoAgent by +10.5 points on BBEH reasoning and +7.1 points on WorkBench. With Claude-4.5-Sonnet, EvoMAS also reaches 79.1% on SWE-Bench-Verified, matching the top of the leaderboard.

</details>


### [146] [Dynamics-Aligned Shared Hypernetworks for Zero-Shot Actuator Inversion](https://arxiv.org/abs/2602.06550)
*Jan Benad,Pradeep Kr. Banerjee,Frank Röder,Nihat Ay,Martin V. Butz,Manfred Eppe*

Main category: cs.LG

TL;DR: 提出DMA*-SH框架，通过单一超网络生成共享适配器权重，解决潜在上下文强化学习中的执行器反转问题，实现零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 解决上下文强化学习中潜在上下文推断的挑战，特别是执行器反转问题（相同动作在不同潜在二元上下文中产生相反物理效应）。

Method: 使用单一超网络（仅通过动态预测训练）生成少量适配器权重，在动态模型、策略和动作价值函数之间共享调制；采用输入/输出归一化和随机输入掩码稳定上下文推断。

Result: 在提出的执行器反转基准测试中，DMA*-SH实现零样本泛化，性能超过领域随机化111.8%，超过标准上下文感知基线16.1%。

Conclusion: 共享调制为执行器反转提供了匹配的归纳偏置，通过理论分析和实验验证了该框架在解决潜在上下文强化学习挑战中的有效性。

Abstract: Zero-shot generalization in contextual reinforcement learning remains a core challenge, particularly when the context is latent and must be inferred from data. A canonical failure mode is actuator inversion, where identical actions produce opposite physical effects under a latent binary context. We propose DMA*-SH, a framework where a single hypernetwork, trained solely via dynamics prediction, generates a small set of adapter weights shared across the dynamics model, policy, and action-value function. This shared modulation imparts an inductive bias matched to actuator inversion, while input/output normalization and random input masking stabilize context inference, promoting directionally concentrated representations. We provide theoretical support via an expressivity separation result for hypernetwork modulation, and a variance decomposition with policy-gradient variance bounds that formalize how within-mode compression improves learning under actuator inversion. For evaluation, we introduce the Actuator Inversion Benchmark (AIB), a suite of environments designed to isolate discontinuous context-to-dynamics interactions. On AIB's held-out actuator-inversion tasks, DMA*-SH achieves zero-shot generalization, outperforming domain randomization by 111.8% and surpassing a standard context-aware baseline by 16.1%.

</details>


### [147] [Topography scanning as a part of process monitoring in power cable insulation process](https://arxiv.org/abs/2602.06519)
*Janne Harjuhahto,Jaakko Harjuhahto,Mikko Lahti,Jussi Hanhirova,Björn Sonerud*

Main category: cs.LG

TL;DR: 提出用于XLPE电缆芯监测的新型地形扫描系统，结合现代测量技术与嵌入式高性能计算构建完整3D表面图，利用深度学习检测表面缺陷


<details>
  <summary>Details</summary>
Motivation: 需要监测XLPE电缆绝缘芯的几何误差和表面缺陷，传统方法难以实现实时、精确的3D表面分析

Method: 开发新型地形扫描系统，结合现代测量技术和嵌入式高性能计算构建详细3D表面图；利用卷积神经网络进行表面缺陷检测

Result: 系统能完整详细地映射3D表面，识别横截面和长度方向的几何误差，发现熔体均匀性是主要误差因素；CNN能有效实时分析表面测量数据，可靠检测表面缺陷

Conclusion: 所提出的扫描系统结合深度学习方法是XLPE电缆芯监测的有效解决方案，CNN特别适合表面测量数据的实时分析，能可靠检测缺陷

Abstract: We present a novel topography scanning system developed to XLPE cable core monitoring. Modern measurement technology is utilized together with embedded high-performance computing to build a complete and detailed 3D surface map of the insulated core. Cross sectional and lengthwise geometry errors are studied, and melt homogeneity is identified as one major factor for these errors. A surface defect detection system has been developed utilizing deep learning methods. Our results show that convolutional neural networks are well suited for real time analysis of surface measurement data enabling reliable detection of surface defects.

</details>


### [148] [Live Knowledge Tracing: Real-Time Adaptation using Tabular Foundation Models](https://arxiv.org/abs/2602.06542)
*Mounir Lbath,Alexandre Paresy,Abdelkayoum Kaddouri,Alan André,Alexandre Ittah,Jill-Jênn Vie*

Main category: cs.LG

TL;DR: 提出一种基于表格基础模型(TFMs)的实时知识追踪新范式，通过双向注意力机制在推理时对齐测试序列与相关训练序列，跳过训练步骤，实现高达273倍的加速和竞争性预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统深度知识追踪模型需要大量训练时间，且在短序列数据集上容易过拟合。需要一种能够实时进行知识追踪的新方法，避免离线训练的限制。

Method: 利用表格基础模型(TFMs)进行实时在线知识追踪。采用双向注意力机制：不仅关注时间步，还同时关注训练集中其他学生的交互。在推理时对齐测试序列与相关训练序列，完全跳过训练步骤。

Result: 在多个不同规模的数据集上，该方法实现了竞争性的预测性能，同时获得了高达273倍的加速效果，特别是在观察到更多学生交互的情况下表现更好。

Conclusion: 基于表格基础模型的实时知识追踪方法提供了一种新的范式，能够在跳过训练步骤的同时实现高性能预测，为知识追踪领域带来了显著的效率提升。

Abstract: Deep knowledge tracing models have achieved significant breakthroughs in modeling student learning trajectories. However, these architectures require substantial training time and are prone to overfitting on datasets with short sequences. In this paper, we explore a new paradigm for knowledge tracing by leveraging tabular foundation models (TFMs). Unlike traditional methods that require offline training on a fixed training set, our approach performs real-time ''live'' knowledge tracing in an online way. The core of our method lies in a two-way attention mechanism: while attention knowledge tracing models only attend across earlier time steps, TFMs simultaneously attend across both time steps and interactions of other students in the training set. They align testing sequences with relevant training sequences at inference time, therefore skipping the training step entirely. We demonstrate, using several datasets of increasing size, that our method achieves competitive predictive performance with up to 273x speedups, in a setting where more student interactions are observed over time.

</details>


### [149] [Transformer-based Parameter Fitting of Models derived from Bloch-McConnell Equations for CEST MRI Analysis](https://arxiv.org/abs/2602.06574)
*Christof Duhme,Chris Lippe,Verena Hoerr,Xiaoyi Jiang*

Main category: cs.LG

TL;DR: 提出基于Transformer的神经网络，用于从CEST MRI数据中拟合代谢物浓度、交换率和弛豫率等参数，性能优于传统梯度求解器


<details>
  <summary>Details</summary>
Motivation: CEST MRI能非侵入性地检测代谢物，比传统MRS具有更高分辨率和灵敏度，但CEST数据量化困难，因为测量信号来自多种生理变量的复杂相互作用

Method: 引入基于Transformer的神经网络，通过自监督训练，拟合从Bloch-McConnell方程推导的物理模型参数（代谢物浓度、交换率、弛豫率）到体外CEST光谱

Result: 自监督训练的神经网络明显优于经典梯度求解器的解决方案

Conclusion: Transformer神经网络为CEST MRI数据量化提供了一种有效的替代方法，能够更好地处理复杂生理变量相互作用带来的挑战

Abstract: Chemical exchange saturation transfer (CEST) MRI is a non-invasive imaging modality for detecting metabolites. It offers higher resolution and sensitivity compared to conventional magnetic resonance spectroscopy (MRS). However, quantification of CEST data is challenging because the measured signal results from a complex interplay of many physiological variables. Here, we introduce a transformer-based neural network to fit parameters such as metabolite concentrations, exchange and relaxation rates of a physical model derived from Bloch-McConnell equations to in-vitro CEST spectra. We show that our self-supervised trained neural network clearly outperforms the solution of classical gradient-based solver.

</details>


### [150] [Refining the Information Bottleneck via Adversarial Information Separation](https://arxiv.org/abs/2602.06549)
*Shuai Ning,Zhenpeng Wang,Lin Wang,Bing Chen,Shuangrong Liu,Xu Wu,Jin Zhou,Bo Yang*

Main category: cs.LG

TL;DR: AdverISF框架通过自监督对抗机制分离任务相关特征与噪声，无需显式监督，在数据稀缺场景下优于现有方法


<details>
  <summary>Details</summary>
Motivation: 在材料科学等领域，实验数据中的任务相关特征常被测量噪声和实验伪影严重混淆，标准正则化方法无法精确分离特征与噪声，现有对抗适应方法又依赖显式分离标签

Method: 提出AdverISF框架：1）使用自监督对抗机制强制任务相关特征与噪声表示之间的统计独立性；2）采用多层分离架构，在特征层次间渐进回收被误判为噪声的信息，实现更细粒度的特征提取

Result: 大量实验表明AdverISF在数据稀缺场景下优于最先进方法，在真实世界材料设计任务评估中展现出优越的泛化性能

Conclusion: AdverISF能够在不依赖显式监督的情况下有效分离任务相关特征与噪声，为数据稀缺领域（如材料科学）的特征提取提供了有效解决方案

Abstract: Generalizing from limited data is particularly critical for models in domains such as material science, where task-relevant features in experimental datasets are often heavily confounded by measurement noise and experimental artifacts. Standard regularization techniques fail to precisely separate meaningful features from noise, while existing adversarial adaptation methods are limited by their reliance on explicit separation labels. To address this challenge, we propose the Adversarial Information Separation Framework (AdverISF), which isolates task-relevant features from noise without requiring explicit supervision. AdverISF introduces a self-supervised adversarial mechanism to enforce statistical independence between task-relevant features and noise representations. It further employs a multi-layer separation architecture that progressively recycles noise information across feature hierarchies to recover features inadvertently discarded as noise, thereby enabling finer-grained feature extraction. Extensive experiments demonstrate that AdverISF outperforms state-of-the-art methods in data-scarce scenarios. In addition, evaluations on real-world material design tasks show that it achieves superior generalization performance.

</details>


### [151] [Perturbing the Phase: Analyzing Adversarial Robustness of Complex-Valued Neural Networks](https://arxiv.org/abs/2602.06577)
*Florian Eilers,Christof Duhme,Xiaoyi Jiang*

Main category: cs.LG

TL;DR: 本文提出针对复数神经网络的相位攻击，并推导复数版本的常用对抗攻击，发现CVNN在某些场景下比实数神经网络更鲁棒，但对相位变化非常敏感。


<details>
  <summary>Details</summary>
Motivation: 随着复数神经网络在各种应用中的普及，需要分析其对异常值的鲁棒性以确保安全使用。对抗攻击是理解深度神经网络行为的重要技术，但现有攻击主要针对实数网络，缺乏专门针对复数网络相位信息的攻击方法。

Method: 设计了专门针对复数输入相位信息的相位攻击，并推导了复数版本的常用对抗攻击方法。通过比较相位攻击与常规攻击（可同时攻击相位和幅度）的效果来评估CVNN的鲁棒性。

Result: 在某些场景下，复数神经网络比实数神经网络更鲁棒。然而，两种网络都对相位变化非常敏感，相位攻击比同等强度的常规攻击更能降低模型性能。

Conclusion: 复数神经网络对相位扰动特别脆弱，相位攻击是评估CVNN鲁棒性的有效方法。在实际应用中需要考虑相位信息的安全性，并开发针对复数网络特性的防御机制。

Abstract: Complex-valued neural networks (CVNNs) are rising in popularity for all kinds of applications. To safely use CVNNs in practice, analyzing their robustness against outliers is crucial. One well known technique to understand the behavior of deep neural networks is to investigate their behavior under adversarial attacks, which can be seen as worst case minimal perturbations. We design Phase Attacks, a kind of attack specifically targeting the phase information of complex-valued inputs. Additionally, we derive complex-valued versions of commonly used adversarial attacks. We show that in some scenarios CVNNs are more robust than RVNNs and that both are very susceptible to phase changes with the Phase Attacks decreasing the model performance more, than equally strong regular attacks, which can attack both phase and magnitude.

</details>


### [152] [Exploring Sparsity and Smoothness of Arbitrary $\ell_p$ Norms in Adversarial Attacks](https://arxiv.org/abs/2602.06578)
*Christof Duhme,Florian Eilers,Xiaoyi Jiang*

Main category: cs.LG

TL;DR: 研究发现ℓ_p范数中p值的选择对对抗攻击的稀疏性和平滑性有显著影响，p∈[1.3, 1.5]能提供稀疏和平滑攻击的最佳平衡，而传统的ℓ_1或ℓ_2范数在多数情况下并非最优选择。


<details>
  <summary>Details</summary>
Motivation: 对抗攻击通常使用ℓ_1、ℓ_2或ℓ_∞范数约束，但缺乏对范数参数p如何影响对抗扰动结构性和感知特性的系统研究。本研究旨在探究p值选择对对抗攻击稀疏性和平滑性的影响。

Method: 采用文献中的两种稀疏性度量，并引入三种平滑性度量（包括基于平滑操作的通用框架和基于一阶泰勒近似的度量）。在多个真实图像数据集和不同模型架构（卷积和Transformer网络）上进行全面的实证评估，分析p∈[1,2]范围内ℓ_p范数约束下对抗攻击的特性。

Result: 实验表明ℓ_1或ℓ_2范数在大多数情况下并非最优选择，最优p值取决于具体任务。在p∈[1.3, 1.5]范围内能获得稀疏和平滑攻击的最佳平衡。

Conclusion: 范数参数p的选择对对抗攻击的结构特性有重要影响，在设计评估对抗攻击时需要基于原则进行范数选择，传统ℓ_1/ℓ_2范数约束可能不是最优方案。

Abstract: Adversarial attacks against deep neural networks are commonly constructed under $\ell_p$ norm constraints, most often using $p=1$, $p=2$ or $p=\infty$, and potentially regularized for specific demands such as sparsity or smoothness. These choices are typically made without a systematic investigation of how the norm parameter \( p \) influences the structural and perceptual properties of adversarial perturbations. In this work, we study how the choice of \( p \) affects sparsity and smoothness of adversarial attacks generated under \( \ell_p \) norm constraints for values of $p \in [1,2]$. To enable a quantitative analysis, we adopt two established sparsity measures from the literature and introduce three smoothness measures. In particular, we propose a general framework for deriving smoothness measures based on smoothing operations and additionally introduce a smoothness measure based on first-order Taylor approximations. Using these measures, we conduct a comprehensive empirical evaluation across multiple real-world image datasets and a diverse set of model architectures, including both convolutional and transformer-based networks. We show that the choice of $\ell_1$ or $\ell_2$ is suboptimal in most cases and the optimal $p$ value is dependent on the specific task. In our experiments, using $\ell_p$ norms with $p\in [1.3, 1.5]$ yields the best trade-off between sparse and smooth attacks. These findings highlight the importance of principled norm selection when designing and evaluating adversarial attacks.

</details>


### [153] [Fine-Grained Model Merging via Modular Expert Recombination](https://arxiv.org/abs/2602.06552)
*Haiyun Qiu,Xingyu Wu,Liang Feng,Kay Chen Tan*

Main category: cs.LG

TL;DR: MERGE提出了一种模块化专家重组方法，实现组件级模型合并和输入感知的按需模块重组，解决了现有实例特定合并方法缺乏可重用性和忽略组件异质性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有实例特定模型合并方法存在两个关键局限：1）实例特定合并模型缺乏可重用性，限制了高质量合并配置的利用和高效批量推理；2）将任务特定模型视为整体，忽略了同源组件（如注意力层和MLP层）的不同可合并性和合并敏感性差异。

Method: MERGE将组件级合并建模为平衡跨任务性能和存储效率的双目标优化问题，采用代理辅助进化算法高效识别帕累托最优合并配置。基于这些配置构建可重用的模块化专家库，通过轻量级路由网络动态激活和重组模块专家，在存储约束下组装输入特定模型。

Result: 在不同模型规模、任务类型和微调策略的广泛实验中，MERGE始终优于强基线方法，并展现出良好的泛化能力。

Conclusion: MERGE通过组件级模型合并和输入感知的模块重组，解决了现有实例特定合并方法的局限性，实现了更高效、可重用且性能优越的模型合并框架。

Abstract: Model merging constructs versatile models by integrating task-specific models without requiring labeled data or expensive joint retraining. Although recent methods improve adaptability to heterogeneous tasks by generating customized merged models for each instance, they face two critical limitations. First, the instance-specific merged models lack reusability, restricting the exploitation of high-quality merging configurations and efficient batch inference. Second, these methods treat each task-specific model as a monolithic whole, overlooking the diverse mergeability of homologous components such as attention and multilayer perceptron layers, and the differing merging sensitivities across components. To address these limitations, we propose MERGE (\underline{M}odular \underline{E}xpert \underline{R}ecombination for fine-\underline{G}rained m\underline{E}rging), a method that enables component-wise model merging and input-aware, on-demand module recombination at inference. MERGE formulates component-wise merging as a bi-objective optimization problem that balances cross-task performance and storage efficiency, and develops a surrogate-assisted evolutionary algorithm to efficiently identify Pareto-optimal merging configurations. These high-quality configurations underpin a reusable modular expert library, from which a lightweight routing network dynamically activates and recombines modular experts to assemble input-specific models and enable efficient inference under storage constraints. Extensive experiments across various model scales, task types, and fine-tuning strategies demonstrate that MERGE consistently outperforms strong baselines and generalizes effectively.

</details>


### [154] [Target noise: A pre-training based neural network initialization for efficient high resolution learning](https://arxiv.org/abs/2602.06585)
*Shaowen Wang,Tariq Alkhalifah*

Main category: cs.LG

TL;DR: 提出一种基于随机噪声自监督预训练的神经网络初始化方法，替代传统的随机初始化，能显著提升收敛速度


<details>
  <summary>Details</summary>
Motivation: 现有初始化方法（如Xavier、Kaiming）依赖随机采样，未利用优化过程信息。神经网络优化中，特别是隐式神经表示和深度图像先验网络存在低频偏置问题，需要更有效的初始化策略

Method: 使用随机噪声作为目标进行自监督预训练。首先训练网络拟合随机噪声，获得结构化、非随机的参数配置，然后将此作为后续任务的初始化

Result: 噪声驱动的预训练显著提升后续任务的收敛速度，无需额外数据或架构修改。特别对INRs和DIP网络有效，能更早捕获高频分量，实现更快更稳定的收敛

Conclusion: 基于噪声的预训练为传统随机初始化提供了轻量级通用替代方案，能更高效优化深度神经网络，尽管随机噪声无语义信息，但其白谱特性可作为有效的自监督信号

Abstract: Weight initialization plays a crucial role in the optimization behavior and convergence efficiency of neural networks. Most existing initialization methods, such as Xavier and Kaiming initializations, rely on random sampling and do not exploit information from the optimization process itself. We propose a simple, yet effective, initialization strategy based on self-supervised pre-training using random noise as the target. Instead of directly training the network from random weights, we first pre-train it to fit random noise, which leads to a structured and non-random parameter configuration. We show that this noise-driven pre-training significantly improves convergence speed in subsequent tasks, without requiring additional data or changes to the network architecture. The proposed method is particularly effective for implicit neural representations (INRs) and Deep Image Prior (DIP)-style networks, which are known to exhibit a strong low-frequency bias during optimization. After noise-based pre-training, the network is able to capture high-frequency components much earlier in training, leading to faster and more stable convergence. Although random noise contains no semantic information, it serves as an effective self-supervised signal (considering its white spectrum nature) for shaping the initialization of neural networks. Overall, this work demonstrates that noise-based pre-training offers a lightweight and general alternative to traditional random initialization, enabling more efficient optimization of deep neural networks.

</details>


### [155] [The challenge of generating and evolving real-life like synthetic test data without accessing real-world raw data -- a Systematic Review](https://arxiv.org/abs/2602.06609)
*Maj-Annika Tammisto,Faiz Ali Shah,Daniel Rodriguez,Dietmar Pfahl*

Main category: cs.LG

TL;DR: 系统综述发现，虽然存在多种隐私保护测试数据生成方法，但完全无需原始真实数据且能持续演化的合成测试数据生成方法仍很缺乏，这是数字政府解决方案中需要探索的重要领域。


<details>
  <summary>Details</summary>
Motivation: 电子政务服务应用（如跨国信息交换、医疗、银行等）需要既真实又保护隐私的测试数据。现有方法通常需要访问原始真实数据进行匿名化或生成，但实际应用中这种访问往往受限，因此需要研究完全无需原始真实数据的合成测试数据生成与演化方法。

Method: 采用Kitchenham等知名系统文献综述方法，在IEEE Xplore、ACM Digital Library和SCOPUS数据库中检索了1,013篇文献，最终从75篇中提取数据，识别出37种部分回答研究问题的方法。

Result: 发现多种隐私保护测试数据生成方法和工具，但大多需要直接访问真实数据进行匿名化或生成。仅识别出9种最接近研究问题的合成测试数据生成方法，且这些方法都缺乏数据演化能力。没有文献完全满足要求。

Conclusion: 合成测试数据演化是一个研究不足但至关重要的领域，特别是在数字政府解决方案中，随着各国新法规的实施，需要开发完全无需原始真实数据且能持续演化的测试数据生成方法。

Abstract: Background: High-level system testing of applications that use data from e-Government services as input requires test data that is real-life-like but where the privacy of personal information is guaranteed. Applications with such strong requirement include information exchange between countries, medicine, banking, etc. This review aims to synthesize the current state-of-the-practice in this domain.
  Objectives: The objective of this Systematic Review is to identify existing approaches for creating and evolving synthetic test data without using real-life raw data.
  Methods: We followed well-known methodologies for conducting systematic literature reviews, including the ones from Kitchenham as well as guidelines for analysing the limitations of our review and its threats to validity.
  Results: A variety of methods and tools exist for creating privacy-preserving test data. Our search found 1,013 publications in IEEE Xplore, ACM Digital Library, and SCOPUS. We extracted data from 75 of those publications and identified 37 approaches that answer our research question partly. A common prerequisite for using these methods and tools is direct access to real-life data for data anonymization or synthetic test data generation. Nine existing synthetic test data generation approaches were identified that were closest to answering our research question. Nevertheless, further work would be needed to add the ability to evolve synthetic test data to the existing approaches.
  Conclusions: None of the publications really covered our requirements completely, only partially. Synthetic test data evolution is a field that has not received much attention from researchers but needs to be explored in Digital Government Solutions, especially since new legal regulations are being placed in force in many countries.

</details>


### [156] [Endogenous Resistance to Activation Steering in Language Models](https://arxiv.org/abs/2602.06941)
*Alex McKenzie,Keenan Pepper,Stijn Servaes,Martin Leitgab,Murat Cubuktepe,Mike Vaiana,Diogo de Lucena,Judd Rosenblatt,Michael S. A. Graziano*

Main category: cs.LG

TL;DR: 大型语言模型能在推理过程中抵抗任务不匹配的激活引导，有时在引导持续的情况下仍能恢复并产生改进的响应，这种现象被称为内源性引导抵抗（ESR）。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在受到外部激活引导时的内部抵抗机制，理解模型如何保持输出一致性并抵抗不匹配的任务引导，这对AI系统的透明性和可控性有重要意义。

Method: 使用稀疏自编码器（SAE）潜在变量引导模型激活，分析不同规模模型（Llama-3.3-70B、Llama-3和Gemma-2系列）的ESR表现。通过零消融实验识别与ESR相关的特定潜在变量，并通过元提示和微调实验增强ESR行为。

Result: Llama-3.3-70B表现出显著的ESR，而较小模型较少出现此现象。识别出26个与离题内容激活相关的SAE潜在变量，零消融这些变量使多尝试率降低25%。通过元提示可将多尝试率提高4倍，微调也能在小模型中诱导ESR行为。

Conclusion: ESR是大型语言模型的内在抵抗机制，可能对抗对抗性操纵有利，但也会干扰依赖激活引导的有益安全干预。理解和控制这些抵抗机制对开发透明可控的AI系统至关重要。

Abstract: Large language models can resist task-misaligned activation steering during inference, sometimes recovering mid-generation to produce improved responses even when steering remains active. We term this Endogenous Steering Resistance (ESR). Using sparse autoencoder (SAE) latents to steer model activations, we find that Llama-3.3-70B shows substantial ESR, while smaller models from the Llama-3 and Gemma-2 families exhibit the phenomenon less frequently. We identify 26 SAE latents that activate differentially during off-topic content and are causally linked to ESR in Llama-3.3-70B. Zero-ablating these latents reduces the multi-attempt rate by 25%, providing causal evidence for dedicated internal consistency-checking circuits. We demonstrate that ESR can be deliberately enhanced through both prompting and training: meta-prompts instructing the model to self-monitor increase the multi-attempt rate by 4x for Llama-3.3-70B, and fine-tuning on self-correction examples successfully induces ESR-like behavior in smaller models. These findings have dual implications: ESR could protect against adversarial manipulation but might also interfere with beneficial safety interventions that rely on activation steering. Understanding and controlling these resistance mechanisms is important for developing transparent and controllable AI systems. Code is available at github.com/agencyenterprise/endogenous-steering-resistance.

</details>


### [157] [Learning to Allocate Resources with Censored Feedback](https://arxiv.org/abs/2602.06565)
*Giovanni Montanari,Côme Fiegel,Corentin Pla,Aadirupa Saha,Vianney Perchet*

Main category: cs.LG

TL;DR: 在线资源分配问题：每轮需将预算B分配到K个臂上，在截断反馈下，臂的奖励需要满足两个条件：(1)臂被激活（伯努利随机变量），(2)分配预算超过随机阈值。算法需要联合估计未知参数并分配预算以最大化累积奖励。


<details>
  <summary>Details</summary>
Motivation: 研究在线资源分配问题，其中资源分配决策受到两个随机因素的影响：臂的激活概率和预算阈值。这种双重随机性使得问题比传统多臂赌博机更复杂，需要新的算法来处理截断反馈和探索-利用权衡。

Method: 提出了两种算法：RA-UCB（已知预算）和MG-UCB（未知预算）。RA-UCB利用乐观估计和非平凡参数估计，MG-UCB允许轮内切换和无穷小分配。两种算法都基于置信上界(UCB)框架。

Result: 证明了信息论后悔下界Ω(T^{1/3})，表明问题的内在难度。RA-UCB在已知预算时达到Õ(√T)后悔，在更强假设下达到O(poly-log T)。MG-UCB在未知预算时匹配RA-UCB的后悔保证。

Conclusion: 该研究解决了具有双重随机性和截断反馈的在线资源分配问题，提出了理论最优的算法，并在真实数据集上验证了理论结果，为实际应用提供了有效解决方案。

Abstract: We study the online resource allocation problem in which at each round, a budget $B$ must be allocated across $K$ arms under censored feedback. An arm yields a reward if and only if two conditions are satisfied: (i) the arm is activated according to an arm-specific Bernoulli random variable with unknown parameter, and (ii) the allocated budget exceeds a random threshold drawn from a parametric distribution with unknown parameter. Over $T$ rounds, the learner must jointly estimate the unknown parameters and allocate the budget so as to maximize cumulative reward facing the exploration--exploitation trade-off. We prove an information-theoretic regret lower bound $Ω(T^{1/3})$, demonstrating the intrinsic difficulty of the problem. We then propose RA-UCB, an optimistic algorithm that leverages non-trivial parameter estimation and confidence bounds. When the budget $B$ is known at the beginning of each round, RA-UCB achieves a regret of order $\widetilde{\mathcal{O}}(\sqrt{T})$, and even $\mathcal{O}(\mathrm{poly}\text{-}\log T)$ under stronger assumptions. As for unknown, round dependent budget, we introduce MG-UCB, which allows within-round switching and infinitesimal allocations, and matches the regret guarantees of RA-UCB. We then validate our theoretical results through experiments on real-world datasets.

</details>


### [158] [Trust Regions Sell, But Who's Buying? Overlap Geometry as an Alternative Trust Region for Policy Optimization](https://arxiv.org/abs/2602.06627)
*Gaurish Trivedi,Alakh Sharma,Kartikey Singh Bhandari,Yash Sinha,Pratik Narang,Dhruv Kumar,Jagat Sesh Challa*

Main category: cs.LG

TL;DR: 提出基于Bhattacharyya系数的重叠几何作为KL散度的替代信任区域，通过约束分布重叠来防止训练不稳定的罕见大似然比偏移，开发了BTRPO和BPPO算法。


<details>
  <summary>Details</summary>
Motivation: 标准信任区域方法使用KL散度约束策略更新，但KL只控制平均差异，无法直接防止罕见的大似然比偏移，这正是导致训练不稳定的原因。PPO的裁剪等启发式方法试图解决这个问题，但缺乏理论依据。

Method: 提出重叠几何作为替代信任区域，通过Bhattacharyya系数约束分布重叠。开发了Bhattacharyya-TRPO（BTRPO）和Bhattacharyya-PPO（BPPO）：BPPO裁剪平方根比率q = sqrt(r)，BTRPO应用二次Hellinger/Bhattacharyya惩罚。

Result: 基于重叠的更新在匹配的训练预算下提高了鲁棒性和聚合性能（通过RLiable测量），表明重叠约束是KL散度的实用且有理论依据的替代方案。

Conclusion: 重叠几何约束为稳定策略优化提供了实用且有理论依据的KL散度替代方案，能更好地控制似然比偏移，提高训练稳定性。

Abstract: Standard trust-region methods constrain policy updates via Kullback-Leibler (KL) divergence. However, KL controls only an average divergence and does not directly prevent rare, large likelihood-ratio excursions that destabilize training--precisely the failure mode that motivates heuristics such as PPO's clipping. We propose overlap geometry as an alternative trust region, constraining distributional overlap via the Bhattacharyya coefficient (closely related to the Hellinger/Renyi-1/2 geometry). This objective penalizes separation in the ratio tails, yielding tighter control over likelihood-ratio excursions without relying on total variation bounds that can be loose in tail regimes. We derive Bhattacharyya-TRPO (BTRPO) and Bhattacharyya-PPO (BPPO), enforcing overlap constraints via square-root ratio updates: BPPO clips the square-root ratio q = sqrt(r), and BTRPO applies a quadratic Hellinger/Bhattacharyya penalty. Empirically, overlap-based updates improve robustness and aggregate performance as measured by RLiable under matched training budgets, suggesting overlap constraints as a practical, principled alternative to KL for stable policy optimization.

</details>


### [159] [Learning a Generative Meta-Model of LLM Activations](https://arxiv.org/abs/2602.06964)
*Grace Luo,Jiahai Feng,Trevor Darrell,Alec Radford,Jacob Steinhardt*

Main category: cs.LG

TL;DR: 使用扩散模型学习神经网络激活的分布，创建"元模型"作为先验，提升干预保真度和概念分离能力


<details>
  <summary>Details</summary>
Motivation: 现有分析神经网络激活的方法（如PCA、稀疏自编码器）依赖强结构假设，需要一种无需此类假设且能作为先验提升干预保真度的替代方案

Method: 在十亿个残差流激活上训练扩散模型，创建学习网络内部状态分布的"元模型"，并应用其学习到的先验进行干预

Result: 扩散损失随计算量平滑下降并可靠预测下游效用；应用元模型先验提升干预流畅度，损失越小增益越大；元模型神经元逐渐将概念分离到单个单元，稀疏探测分数随损失下降而提升

Conclusion: 生成式元模型为无需限制性结构假设的可扩展可解释性提供了有前景的路径

Abstract: Existing approaches for analyzing neural network activations, such as PCA and sparse autoencoders, rely on strong structural assumptions. Generative models offer an alternative: they can uncover structure without such assumptions and act as priors that improve intervention fidelity. We explore this direction by training diffusion models on one billion residual stream activations, creating "meta-models" that learn the distribution of a network's internal states. We find that diffusion loss decreases smoothly with compute and reliably predicts downstream utility. In particular, applying the meta-model's learned prior to steering interventions improves fluency, with larger gains as loss decreases. Moreover, the meta-model's neurons increasingly isolate concepts into individual units, with sparse probing scores that scale as loss decreases. These results suggest generative meta-models offer a scalable path toward interpretability without restrictive structural assumptions. Project page: https://generative-latent-prior.github.io.

</details>


### [160] [Temperature Scaling Attack Disrupting Model Confidence in Federated Learning](https://arxiv.org/abs/2602.06638)
*Kichang Lee,Jaeho Jin,JaeYeon Park,Songkuk Kim,JeongGil Ko*

Main category: cs.LG

TL;DR: 提出温度缩放攻击(TSA)，一种针对联邦学习的训练时攻击，专门破坏模型置信度校准而不影响准确率，通过注入温度缩放和学习率-温度耦合实现隐蔽攻击


<details>
  <summary>Details</summary>
Motivation: 预测置信度在关键任务系统中是重要的控制信号，但现有联邦学习攻击主要针对准确率或植入后门，而置信度校准作为一个独立的攻击目标被忽视

Method: 提出温度缩放攻击(TSA)：在本地训练时注入温度缩放，并通过学习率-温度耦合使恶意更新保持良性优化行为，规避基于准确率的监控和相似性检测

Result: 在三个基准测试中，TSA显著改变校准（如CIFAR-100上误差增加145%），准确率变化小于2%；即使在鲁棒聚合和后校准防御下仍有效；案例研究显示医疗和自动驾驶场景中关键错误增加7.2倍

Conclusion: 研究确立了校准完整性作为联邦学习的关键攻击面，即使准确率不变，置信度操纵也能导致严重后果

Abstract: Predictive confidence serves as a foundational control signal in mission-critical systems, directly governing risk-aware logic such as escalation, abstention, and conservative fallback. While prior federated learning attacks predominantly target accuracy or implant backdoors, we identify confidence calibration as a distinct attack objective. We present the Temperature Scaling Attack (TSA), a training-time attack that degrades calibration while preserving accuracy. By injecting temperature scaling with learning rate-temperature coupling during local training, malicious updates maintain benign-like optimization behavior, evading accuracy-based monitoring and similarity-based detection. We provide a convergence analysis under non-IID settings, showing that this coupling preserves standard convergence bounds while systematically distorting confidence. Across three benchmarks, TSA substantially shifts calibration (e.g., 145% error increase on CIFAR-100) with <2 accuracy change, and remains effective under robust aggregation and post-hoc calibration defenses. Case studies further show that confidence manipulation can cause up to 7.2x increases in missed critical cases (healthcare) or false alarms (autonomous driving), even when accuracy is unchanged. Overall, our results establish calibration integrity as a critical attack surface in federated learning.

</details>


### [161] [SaDiT: Efficient Protein Backbone Design via Latent Structural Tokenization and Diffusion Transformers](https://arxiv.org/abs/2602.06706)
*Shentong Mo,Lanqing Li*

Main category: cs.LG

TL;DR: SaDiT：通过结合SaProt Tokenization和Diffusion Transformer架构，加速蛋白质骨架生成的新框架，在计算速度和结构可行性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散的蛋白质骨架生成模型计算量大、速度慢，限制了大规模结构探索。虽然已有工作尝试使用流匹配提高效率，但蛋白质领域的结构压缩和加速潜力尚未充分挖掘。

Method: 提出SaDiT框架：1）使用SaProt Tokenization将蛋白质几何表示为离散潜在空间，降低生成复杂度并保持SE(3)等价性；2）引入IPA Token Cache机制，在迭代采样中重用计算的token状态，优化Invariant Point Attention层效率。

Result: SaDiT在计算速度和结构可行性方面优于RFDiffusion和Proteina等最先进模型。在无条件骨架生成和折叠类条件生成任务中，SaDiT展现出捕获复杂拓扑特征和高设计性的卓越能力。

Conclusion: SaDiT通过离散潜在空间表示和高效注意力机制，显著加速了蛋白质骨架生成，为大规模蛋白质结构探索提供了更高效的解决方案。

Abstract: Generative models for de novo protein backbone design have achieved remarkable success in creating novel protein structures. However, these diffusion-based approaches remain computationally intensive and slower than desired for large-scale structural exploration. While recent efforts like Proteina have introduced flow-matching to improve sampling efficiency, the potential of tokenization for structural compression and acceleration remains largely unexplored in the protein domain. In this work, we present SaDiT, a novel framework that accelerates protein backbone generation by integrating SaProt Tokenization with a Diffusion Transformer (DiT) architecture. SaDiT leverages a discrete latent space to represent protein geometry, significantly reducing the complexity of the generation process while maintaining theoretical SE(3) equivalence. To further enhance efficiency, we introduce an IPA Token Cache mechanism that optimizes the Invariant Point Attention (IPA) layers by reusing computed token states during iterative sampling. Experimental results demonstrate that SaDiT outperforms state-of-the-art models, including RFDiffusion and Proteina, in both computational speed and structural viability. We evaluate our model across unconditional backbone generation and fold-class conditional generation tasks, where SaDiT shows superior ability to capture complex topological features with high designability.

</details>


### [162] [Degradation of Feature Space in Continual Learning](https://arxiv.org/abs/2602.06586)
*Chiara Lanza,Roberto Pereira,Marco Miozzo,Eduard Angelats,Paolo Dini*

Main category: cs.LG

TL;DR: 研究发现在持续学习中强制特征空间各向同性反而会降低模型性能，这与集中式训练中的情况相反


<details>
  <summary>Details</summary>
Motivation: 集中式训练中特征空间各向同性有助于结构化表示，但持续学习面临可塑性与稳定性困境，特征空间趋于各向异性。研究探索是否强制各向同性可以改善持续学习中的表示质量。

Method: 使用对比持续学习技术在CIFAR-10和CIFAR-100数据集上进行实验，研究各向同性正则化对模型性能的影响。

Result: 各向同性正则化未能改善持续学习性能，反而会降低模型准确率，表明强制特征空间各向同性对持续学习有害。

Conclusion: 集中式学习与持续学习的特征几何存在本质差异，各向同性虽然在集中式设置中有益，但不适合非平稳学习场景，不应作为持续学习的归纳偏置。

Abstract: Centralized training is the standard paradigm in deep learning, enabling models to learn from a unified dataset in a single location. In such setup, isotropic feature distributions naturally arise as a mean to support well-structured and generalizable representations. In contrast, continual learning operates on streaming and non-stationary data, and trains models incrementally, inherently facing the well-known plasticity-stability dilemma. In such settings, learning dynamics tends to yield increasingly anisotropic feature space. This arises a fundamental question: should isotropy be enforced to achieve a better balance between stability and plasticity, and thereby mitigate catastrophic forgetting? In this paper, we investigate whether promoting feature-space isotropy can enhance representation quality in continual learning. Through experiments using contrastive continual learning techniques on CIFAR-10 and CIFAR-100 data, we find that isotropic regularization fails to improve, and can in fact degrade, model accuracy in continual settings. Our results highlight essential differences in feature geometry between centralized and continual learning, suggesting that isotropy, while beneficial in centralized setups, may not constitute an appropriate inductive bias for non-stationary learning scenarios.

</details>


### [163] [F-GRPO: Don't Let Your Policy Learn the Obvious and Forget the Rare](https://arxiv.org/abs/2602.06717)
*Daniil Plyusov,Alexey Gorbatovski,Boris Shaposhnikov,Viacheslav Sinii,Alexey Malakhov,Daniil Gavrilov*

Main category: cs.LG

TL;DR: 论文提出了一种难度感知的优势缩放系数，用于改进基于组采样的强化学习可验证奖励方法，通过降低高成功率提示的更新权重来改善对罕见正确轨迹的学习。


<details>
  <summary>Details</summary>
Motivation: 基于组采样的RLVR方法在实践中面临计算限制，大组规模不可行导致学习偏向已有高概率轨迹，小组规模又容易错过罕见正确轨迹，同时仍包含混合奖励，使概率集中在常见解上。

Method: 推导了更新错过罕见正确模式的概率与组规模的关系，分析了更新如何在正确集内重新分配概率质量。受此分析启发，提出基于Focal loss思想的难度感知优势缩放系数，降低高成功率提示的更新权重，可直接集成到GRPO、DAPO、CISPO等组相对RLVR算法中。

Result: 在Qwen2.5-7B模型上，该方法在领域内和领域外基准测试中显著提升pass@256：GRPO从64.1提升到70.3，DAPO从69.3提升到72.5，CISPO从73.2提升到76.8，同时保持或改进pass@1，不增加组规模或计算成本。

Conclusion: 提出的轻量级修改有效解决了组采样RLVR中的罕见正确轨迹学习问题，通过难度感知的优势缩放改善了模型性能，可在不增加计算开销的情况下集成到现有算法中。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is commonly based on group sampling to estimate advantages and stabilize policy updates. In practice, large group sizes are not feasible due to computational limits, which biases learning toward trajectories that are already likely. Smaller groups often miss rare-correct trajectories while still containing mixed rewards, concentrating probability on common solutions. We derive the probability that updates miss rare-correct modes as a function of group size, showing non-monotonic behavior, and characterize how updates redistribute mass within the correct set, revealing that unsampled-correct mass can shrink even as total correct mass grows. Motivated by this analysis, we propose a difficulty-aware advantage scaling coefficient, inspired by Focal loss, that down-weights updates on high-success prompts. The lightweight modification can be directly integrated into any group-relative RLVR algorithm such as GRPO, DAPO, and CISPO. On Qwen2.5-7B across in-domain and out-of-domain benchmarks, our method improves pass@256 from 64.1 $\rightarrow$ 70.3 (GRPO), 69.3 $\rightarrow$ 72.5 (DAPO), and 73.2 $\rightarrow$ 76.8 (CISPO), while preserving or improving pass@1, without increasing group size or computational cost.

</details>


### [164] [DiTS: Multimodal Diffusion Transformers Are Time Series Forecasters](https://arxiv.org/abs/2602.06597)
*Haoran Zhang,Haixuan Liu,Yong Liu,Yunzhong Qiu,Yuxuan Wang,Jianmin Wang,Mingsheng Long*

Main category: cs.LG

TL;DR: DiTS是一种用于时间序列预测的扩散变换器架构，通过将内生和外生变量视为不同模态，并设计双流Transformer块来更好地捕捉变量间和时间依赖关系。


<details>
  <summary>Details</summary>
Motivation: 现有生成式时间序列模型未能充分利用时间序列数据的多维特性，特别是扩散变换器(DiT)架构依赖简单的条件控制和单流Transformer主干，未能充分利用协变量感知预测中的跨变量依赖关系。

Method: 提出DiTS架构，将内生和外生变量视为不同模态。设计专门的双流Transformer块，包含时间注意力模块用于时间维度自回归建模，以及变量注意力模块用于跨变量建模。利用多变量依赖的低秩特性降低计算成本。

Result: DiTS在多个基准测试中实现了最先进的性能，无论是否存在未来外生变量观测，都展示了相对于传统确定性深度预测模型的独特生成式预测优势。

Conclusion: DiTS通过将时间序列视为多模态数据并设计专门的双流Transformer架构，有效解决了现有生成式时间序列模型在捕捉跨变量依赖方面的不足，为概率预测提供了更强大和灵活的框架。

Abstract: While generative modeling on time series facilitates more capable and flexible probabilistic forecasting, existing generative time series models do not address the multi-dimensional properties of time series data well. The prevalent architecture of Diffusion Transformers (DiT), which relies on simplistic conditioning controls and a single-stream Transformer backbone, tends to underutilize cross-variate dependencies in covariate-aware forecasting. Inspired by Multimodal Diffusion Transformers that integrate textual guidance into video generation, we propose Diffusion Transformers for Time Series (DiTS), a general-purpose architecture that frames endogenous and exogenous variates as distinct modalities. To better capture both inter-variate and intra-variate dependencies, we design a dual-stream Transformer block tailored for time-series data, comprising a Time Attention module for autoregressive modeling along the temporal dimension and a Variate Attention module for cross-variate modeling. Unlike the common approach for images, which flattens 2D token grids into 1D sequences, our design leverages the low-rank property inherent in multivariate dependencies, thereby reducing computational costs. Experiments show that DiTS achieves state-of-the-art performance across benchmarks, regardless of the presence of future exogenous variate observations, demonstrating unique generative forecasting strengths over traditional deterministic deep forecasting models.

</details>


### [165] [Pairwise is Not Enough: Hypergraph Neural Networks for Multi-Agent Pathfinding](https://arxiv.org/abs/2602.06733)
*Rishabh Jain,Keisuke Okumura,Michael Amir,Pietro Lio,Amanda Prorok*

Main category: cs.LG

TL;DR: HMAGAT是一种基于超图注意力机制的新型多智能体路径规划学习架构，通过捕捉群体动态关系，在数据量和参数更少的情况下超越了现有最佳模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于图神经网络的多智能体路径规划方法局限于两两智能体之间的消息传递，无法有效处理密集环境中需要群体协调的高阶交互，导致次优行为和注意力稀释问题。

Method: 提出HMAGAT（超图多智能体注意力网络），利用有向超图上的注意力机制来显式捕捉群体动态关系，突破传统GNN的表示瓶颈。

Result: HMAGAT在仅100万参数和100倍少训练数据的情况下，超越了当前8500万参数的最佳模型，在多智能体路径规划学习模型中达到新的最佳性能。

Conclusion: 对于多智能体问题，适当的归纳偏置比训练数据规模或参数数量更为关键，超图表示能够有效缓解GNN中的注意力稀释问题并捕捉复杂交互。

Abstract: Multi-Agent Path Finding (MAPF) is a representative multi-agent coordination problem, where multiple agents are required to navigate to their respective goals without collisions. Solving MAPF optimally is known to be NP-hard, leading to the adoption of learning-based approaches to alleviate the online computational burden. Prevailing approaches, such as Graph Neural Networks (GNNs), are typically constrained to pairwise message passing between agents. However, this limitation leads to suboptimal behaviours and critical issues, such as attention dilution, particularly in dense environments where group (i.e. beyond just two agents) coordination is most critical. Despite the importance of such higher-order interactions, existing approaches have not been able to fully explore them. To address this representational bottleneck, we introduce HMAGAT (Hypergraph Multi-Agent Attention Network), a novel architecture that leverages attentional mechanisms over directed hypergraphs to explicitly capture group dynamics. Empirically, HMAGAT establishes a new state-of-the-art among learning-based MAPF solvers: e.g., despite having just 1M parameters and being trained on 100$\times$ less data, it outperforms the current SoTA 85M parameter model. Through detailed analysis of HMAGAT's attention values, we demonstrate how hypergraph representations mitigate the attention dilution inherent in GNNs and capture complex interactions where pairwise methods fail. Our results illustrate that appropriate inductive biases are often more critical than the training data size or sheer parameter count for multi-agent problems.

</details>


### [166] [The hidden risks of temporal resampling in clinical reinforcement learning](https://arxiv.org/abs/2602.06603)
*Thomas Frost,Hrisheekesh Vaidya,Steve Harris*

Main category: cs.LG

TL;DR: 离线强化学习在医疗应用中，将患者数据重采样为固定时间间隔会显著降低模型在实际部署中的性能，而标准离线策略评估指标可能无法检测到这种性能下降。


<details>
  <summary>Details</summary>
Motivation: 当前医疗领域的离线强化学习研究通常将患者数据聚合到固定时间间隔，以适应标准ORL框架，但这种时间操纵对模型安全性和有效性的影响尚未得到充分理解。

Method: 使用网格世界导航任务和UVA/Padova临床糖尿病模拟器，分析时间重采样对离线强化学习算法性能的影响，并提出三种导致失败的机制。

Result: 时间重采样显著降低了离线强化学习算法在实际部署中的性能，标准离线策略评估指标可能无法检测到这种性能下降。

Conclusion: 当前医疗ORL流程存在基本风险，需要开发能够明确处理临床决策不规则时序的方法。

Abstract: Offline reinforcement learning (ORL) has shown potential for improving decision-making in healthcare. However, contemporary research typically aggregates patient data into fixed time intervals, simplifying their mapping to standard ORL frameworks. The impact of these temporal manipulations on model safety and efficacy remains poorly understood. In this work, using both a gridworld navigation task and the UVA/Padova clinical diabetes simulator, we demonstrate that temporal resampling significantly degrades the performance of offline reinforcement learning algorithms during live deployment. We propose three mechanisms that drive this failure: (i) the generation of counterfactual trajectories, (ii) the distortion of temporal expectations, and (iii) the compounding of generalisation errors. Crucially, we find that standard off-policy evaluation metrics can fail to detect these drops in performance. Our findings reveal a fundamental risk in current healthcare ORL pipelines and emphasise the need for methods that explicitly handle the irregular timing of clinical decision-making.

</details>


### [167] [Optimal Abstractions for Verifying Properties of Kolmogorov-Arnold Networks (KANs)](https://arxiv.org/abs/2602.06737)
*Noah Schwartz,Chandra Kanth Nagesh,Sriram Sankaranarayanan,Ramneet Kaur,Tuhin Sahai,Susmit Jha*

Main category: cs.LG

TL;DR: 提出了一种验证Kolmogorov-Arnold Networks (KANs) 属性的新方法，通过将KAN单元替换为分段仿射函数创建数学抽象，并使用混合整数线性规划进行验证，同时通过动态规划和背包优化最小化分段数量。


<details>
  <summary>Details</summary>
Motivation: KANs作为一类具有非线性单变量激活函数的神经网络，其验证面临挑战。需要平衡分段仿射近似中的分段数量：分段过多会导致计算不可行，分段过少会产生过大误差边界。需要系统性的框架来找到最优抽象。

Method: 1. 将每个KAN单元替换为分段仿射函数创建数学抽象；2. 提供原始网络与近似之间的局部和全局误差估计；3. 将验证问题编码为混合整数线性规划；4. 结合单元级动态规划和网络级背包优化，在保证误差边界的前提下最小化总分段数量。

Result: 在多个KAN基准测试上的经验评估表明，该方法的前期分析成本是合理的，能够获得优越的验证结果。通过系统优化分段数量，在计算可行性和验证精度之间取得了良好平衡。

Conclusion: 提出了一种系统性的KAN验证框架，通过优化分段仿射近似策略，在保证指定误差边界的同时最小化计算复杂度，为KANs的形式化验证提供了有效解决方案。

Abstract: We present a novel approach for verifying properties of Kolmogorov-Arnold Networks (KANs), a class of neural networks characterized by nonlinear, univariate activation functions typically implemented as piecewise polynomial splines or Gaussian processes. Our method creates mathematical ``abstractions'' by replacing each KAN unit with a piecewise affine (PWA) function, providing both local and global error estimates between the original network and its approximation. These abstractions enable property verification by encoding the problem as a Mixed Integer Linear Program (MILP), determining whether outputs satisfy specified properties when inputs belong to a given set. A critical challenge lies in balancing the number of pieces in the PWA approximation: too many pieces add binary variables that make verification computationally intractable, while too few pieces create excessive error margins that yield uninformative bounds. Our key contribution is a systematic framework that exploits KAN structure to find optimal abstractions. By combining dynamic programming at the unit level with a knapsack optimization across the network, we minimize the total number of pieces while guaranteeing specified error bounds. This approach determines the optimal approximation strategy for each unit while maintaining overall accuracy requirements. Empirical evaluation across multiple KAN benchmarks demonstrates that the upfront analysis costs of our method are justified by superior verification results.

</details>


### [168] [AEGIS: Adversarial Target-Guided Retention-Data-Free Robust Concept Erasure from Diffusion Models](https://arxiv.org/abs/2602.06771)
*Fengpeng Li,Kemou Li,Qizhou Wang,Bo Han,Jiantao Zhou*

Main category: cs.LG

TL;DR: 提出AEGIS框架，通过对抗性训练和梯度信息协同，在无需保留数据的情况下同时提升概念擦除的鲁棒性和保留性


<details>
  <summary>Details</summary>
Motivation: 现有概念擦除方法面临鲁棒性与保留性的权衡：增强鲁棒性会损害模型整体效用，而注重保留性则难以抵御自适应攻击

Method: AEGIS框架，采用对抗性擦除和梯度信息协同，无需保留数据，通过对抗训练提升对概念重新激活的抵抗力

Result: AEGIS在无需保留数据的情况下，同时提升了概念擦除的鲁棒性和保留性，优于现有方法

Conclusion: AEGIS框架解决了概念擦除中鲁棒性与保留性的权衡问题，为实际应用提供了更有效的解决方案

Abstract: Concept erasure helps stop diffusion models (DMs) from generating harmful content; but current methods face robustness retention trade off. Robustness means the model fine-tuned by concept erasure methods resists reactivation of erased concepts, even under semantically related prompts. Retention means unrelated concepts are preserved so the model's overall utility stays intact. Both are critical for concept erasure in practice, yet addressing them simultaneously is challenging, as existing works typically improve one factor while sacrificing the other. Prior work typically strengthens one while degrading the other, e.g., mapping a single erased prompt to a fixed safe target leaves class level remnants exploitable by prompt attacks, whereas retention-oriented schemes underperform against adaptive adversaries. This paper introduces Adversarial Erasure with Gradient Informed Synergy (AEGIS), a retention-data-free framework that advances both robustness and retention.

</details>


### [169] [Adaptive-CaRe: Adaptive Causal Regularization for Robust Outcome Prediction](https://arxiv.org/abs/2602.06611)
*Nithya Bhasker,Fiona R. Kolbinger,Susu Hu,Gitta Kutyniok,Stefanie Speidel*

Main category: cs.LG

TL;DR: 提出Adaptive-CaRe正则化方法，在医学领域预测中平衡预测准确性与因果鲁棒性，通过惩罚统计贡献与因果贡献差异来避免伪相关。


<details>
  <summary>Details</summary>
Motivation: 医学领域预测中，传统监督学习追求预测准确性但可能捕捉伪相关，而因果结构学习过于保守导致诊断精度损失，需要平衡两者。

Method: 提出模型无关的正则化策略Adaptive-CaRe，通过惩罚输入特征对模型预测的统计贡献与因果贡献之间的差异，调节正则化强度λ来权衡准确性与鲁棒性。

Result: 合成数据验证了Adaptive-CaRe能找到鲁棒预测因子并保持竞争力；因果基准实验提供了权衡蓝图；真实数据集验证了实际应用效果。

Conclusion: Adaptive-CaRe为医学领域长期存在的预测准确性与因果鲁棒性权衡提供了简单有效的解决方案，未来可探索其他因果框架和复杂模型。

Abstract: Accurate prediction of outcomes is crucial for clinical decision-making and personalized patient care. Supervised machine learning algorithms, which are commonly used for outcome prediction in the medical domain, optimize for predictive accuracy, which can result in models latching onto spurious correlations instead of robust predictors. Causal structure learning methods on the other hand have the potential to provide robust predictors for the target, but can be too conservative because of algorithmic and data assumptions, resulting in loss of diagnostic precision. Therefore, we propose a novel model-agnostic regularization strategy, Adaptive-CaRe, for generalized outcome prediction in the medical domain. Adaptive-CaRe strikes a balance between both predictive value and causal robustness by incorporating a penalty that is proportional to the difference between the estimated statistical contribution and estimated causal contribution of the input features for model predictions. Our experiments on synthetic data establish the efficacy of the proposed Adaptive-CaRe regularizer in finding robust predictors for the target while maintaining competitive predictive accuracy. With experiments on a standard causal benchmark, we provide a blueprint for navigating the trade-off between predictive accuracy and causal robustness by tweaking the regularization strength, $λ$. Validation using real-world dataset confirms that the results translate to practical, real-domain settings. Therefore, Adaptive-CaRe provides a simple yet effective solution to the long-standing trade-off between predictive accuracy and causal robustness in the medical domain. Future work would involve studying alternate causal structure learning frameworks and complex classification models to provide deeper insights at a larger scale.

</details>


### [170] [On the Identifiability of Steering Vectors in Large Language Models](https://arxiv.org/abs/2602.06801)
*Sohan Venkatesh,Ashish Mahendran Kurapath*

Main category: cs.LG

TL;DR: 激活导向方法（如角色向量）存在根本性的不可识别性问题：行为等效的干预构成大等价类，使得导向向量无法唯一确定，但可通过结构假设恢复可识别性。


<details>
  <summary>Details</summary>
Motivation: 当前广泛使用的激活导向方法（如角色向量）常被解释为揭示了有意义的内部表征，但这种解释隐含地假设导向方向是可识别且能从输入输出行为中唯一恢复的。本文旨在检验这一假设的有效性。

Method: 将导向形式化为内部表征的干预，从理论上证明在现实建模和数据条件下导向向量的不可识别性；并通过多个模型和语义特征的实证验证，展示正交扰动能达到近乎等效的效果。

Result: 理论证明导向向量存在大等价类的行为等效干预，导致根本性不可识别；实证显示正交扰动能实现近乎等效的效能且效应量可忽略。但在统计独立性、稀疏约束、多环境验证或跨层一致性等结构假设下可恢复可识别性。

Conclusion: 激活导向方法存在根本性的可解释性限制，揭示了可靠安全关键控制所需的结构假设，对基于导向的模型解释和安全控制具有重要启示。

Abstract: Activation steering methods, such as persona vectors, are widely used to control large language model behavior and increasingly interpreted as revealing meaningful internal representations. This interpretation implicitly assumes steering directions are identifiable and uniquely recoverable from input-output behavior. We formalize steering as an intervention on internal representations and prove that, under realistic modeling and data conditions, steering vectors are fundamentally non-identifiable due to large equivalence classes of behaviorally indistinguishable interventions. Empirically, we validate this across multiple models and semantic traits, showing orthogonal perturbations achieve near-equivalent efficacy with negligible effect sizes. However, identifiability is recoverable under structural assumptions including statistical independence, sparsity constraints, multi-environment validation or cross-layer consistency. These findings reveal fundamental interpretability limits and clarify structural assumptions required for reliable safety-critical control.

</details>


### [171] [AEGPO: Adaptive Entropy-Guided Policy Optimization for Diffusion Models](https://arxiv.org/abs/2602.06825)
*Yuming Li,Qingyu Li,Chengyu Bai,Xiangyang Luo,Zeyue Xue,Wenyu Qin,Meng Wang,Yikai Wang,Shanghang Zhang*

Main category: cs.LG

TL;DR: 提出AEGPO方法，通过注意力熵的双重信号（Δ熵和熵峰值）指导自适应优化，在全局层面动态分配计算资源给高学习价值样本，在局部层面选择关键时间步进行探索，显著提升RLHF训练效率和效果。


<details>
  <summary>Details</summary>
Motivation: 现有RLHF方法（如GRPO）存在采样策略低效和静态的问题，对所有提示词和去噪步骤采用统一处理，忽略了样本学习价值的显著差异以及关键探索时刻的动态特性。

Method: 提出自适应熵引导策略优化（AEGPO）：1）全局层面使用Δ熵（反映当前策略与基础策略差异）动态分配rollout预算，优先处理高学习价值提示词；2）局部层面利用熵峰值识别关键高分散时间步，选择性地在这些时刻进行探索而非均匀处理所有去噪步骤。

Result: 在文本到图像生成任务上的实验表明，AEGPO相比标准GRPO变体显著加速收敛，并取得更优的对齐性能。

Conclusion: 通过将计算资源集中在最具信息量的样本和最关键时刻，AEGPO实现了更高效有效的策略优化，为扩散和流模型的RLHF对齐提供了新思路。

Abstract: Reinforcement learning from human feedback (RLHF) shows promise for aligning diffusion and flow models, yet policy optimization methods such as GRPO suffer from inefficient and static sampling strategies. These methods treat all prompts and denoising steps uniformly, ignoring substantial variations in sample learning value as well as the dynamic nature of critical exploration moments.
  To address this issue, we conduct a detailed analysis of the internal attention dynamics during GRPO training and uncover a key insight: attention entropy can serve as a powerful dual-signal proxy. First, across different samples, the relative change in attention entropy (ΔEntropy), which reflects the divergence between the current policy and the base policy, acts as a robust indicator of sample learning value. Second, during the denoising process, the peaks of absolute attention entropy (Entropy(t)), which quantify attention dispersion, effectively identify critical timesteps where high-value exploration occurs.
  Building on this observation, we propose Adaptive Entropy-Guided Policy Optimization (AEGPO), a novel dual-signal, dual-level adaptive optimization strategy. At the global level, AEGPO uses ΔEntropy to dynamically allocate rollout budgets, prioritizing prompts with higher learning value. At the local level, it exploits the peaks of Entropy(t) to guide exploration selectively at critical high-dispersion timesteps rather than uniformly across all denoising steps.
  By focusing computation on the most informative samples and the most critical moments, AEGPO enables more efficient and effective policy optimization. Experiments on text-to-image generation tasks demonstrate that AEGPO significantly accelerates convergence and achieves superior alignment performance compared to standard GRPO variants.

</details>


### [172] [Pruning at Initialisation through the lens of Graphon Limit: Convergence, Expressivity, and Generalisation](https://arxiv.org/abs/2602.06675)
*Hoang Pham,The-Anh Ta,Long Tran-Thanh*

Main category: cs.LG

TL;DR: 该论文将初始化剪枝方法与图极限理论连接，证明了剪枝掩码收敛到确定性二分图元，建立了稀疏网络的拓扑分类，并推导了稀疏网络的通用逼近定理和图元-NTK泛化界。


<details>
  <summary>Details</summary>
Motivation: 初始化剪枝方法能在训练前发现可训练的稀疏子网络，但其理论机制尚不明确。现有分析多限于有限宽度统计，缺乏对网络增大时出现的全局稀疏模式的严格刻画。

Method: 通过图元连接离散剪枝启发式与图极限理论，建立初始化剪枝掩码的图元极限。引入包含流行剪枝准则的因子化显著性模型，证明在正则条件下，这些算法生成的离散掩码收敛到确定性二分图元。

Result: 建立了稀疏网络的新拓扑分类：非结构化方法收敛到表示均匀连接的齐次图元，而数据驱动方法收敛到编码隐式特征选择的异质图元。基于此连续刻画，推导了两个基本理论结果：1) 稀疏网络的通用逼近定理；2) 图元-NTK泛化界。

Conclusion: 该研究将稀疏神经网络的研究从组合图问题转化为连续算子的严格框架，为分析稀疏神经网络的表达能力和泛化能力提供了新机制。

Abstract: Pruning at Initialisation methods discover sparse, trainable subnetworks before training, but their theoretical mechanisms remain elusive. Existing analyses are often limited to finite-width statistics, lacking a rigorous characterisation of the global sparsity patterns that emerge as networks grow large. In this work, we connect discrete pruning heuristics to graph limit theory via graphons, establishing the graphon limit of PaI masks. We introduce a Factorised Saliency Model that encompasses popular pruning criteria and prove that, under regularity conditions, the discrete masks generated by these algorithms converge to deterministic bipartite graphons. This limit framework establishes a novel topological taxonomy for sparse networks: while unstructured methods (e.g., Random, Magnitude) converge to homogeneous graphons representing uniform connectivity, data-driven methods (e.g., SNIP, GraSP) converge to heterogeneous graphons that encode implicit feature selection. Leveraging this continuous characterisation, we derive two fundamental theoretical results: (i) a Universal Approximation Theorem for sparse networks that depends only on the intrinsic dimension of active coordinate subspaces; and (ii) a Graphon-NTK generalisation bound demonstrating how the limit graphon modulates the kernel geometry to align with informative features. Our results transform the study of sparse neural networks from combinatorial graph problems into a rigorous framework of continuous operators, offering a new mechanism for analysing expressivity and generalisation in sparse neural networks.

</details>


### [173] [Memory-Conditioned Flow-Matching for Stable Autoregressive PDE Rollouts](https://arxiv.org/abs/2602.06689)
*Victor Armegioiu*

Main category: cs.LG

TL;DR: 论文提出了一种记忆条件扩散/流匹配方法，通过注入在线状态到去噪过程中，改善自回归PDE求解器在长时间推演中的稳定性，特别是在粗到细尺度生成中。


<details>
  <summary>Details</summary>
Motivation: 自回归生成式PDE求解器在单步预测中可能准确，但在长时间推演中容易漂移，特别是在粗到细尺度生成中。扩散和流匹配生成器虽然内部动力学是马尔可夫的，但推演稳定性受每步条件分布误差控制。使用Mori-Zwanzig投影形式揭示了无记忆闭包的结构性限制。

Method: 引入记忆条件扩散/流匹配方法，将紧凑的在线状态通过潜在特征注入到去噪过程中。通过分解，记忆诱导了未解析尺度的结构化条件尾部先验，减少了填充缺失频率所需的传输。证明了所得条件核的Wasserstein稳定性，并推导了分离记忆近似和条件生成误差的离散Grönwall推演边界。

Result: 在包含激波的可压缩流和多尺度混合实验中，该方法显示出更高的精度和显著更稳定的长时间推演，具有更好的细尺度谱和统计保真度。

Conclusion: 记忆条件扩散/流匹配方法通过显式建模记忆效应，解决了自回归PDE求解器在长时间推演中的稳定性问题，特别是在粗到细尺度生成中，显著提高了推演精度和稳定性。

Abstract: Autoregressive generative PDE solvers can be accurate one step ahead yet drift over long rollouts, especially in coarse-to-fine regimes where each step must regenerate unresolved fine scales. This is the regime of diffusion and flow-matching generators: although their internal dynamics are Markovian, rollout stability is governed by per-step \emph{conditional law} errors. Using the Mori--Zwanzig projection formalism, we show that eliminating unresolved variables yields an exact resolved evolution with a Markov term, a memory term, and an orthogonal forcing, exposing a structural limitation of memoryless closures. Motivated by this, we introduce memory-conditioned diffusion/flow-matching with a compact online state injected into denoising via latent features. Via disintegration, memory induces a structured conditional tail prior for unresolved scales and reduces the transport needed to populate missing frequencies. We prove Wasserstein stability of the resulting conditional kernel. We then derive discrete Grönwall rollout bounds that separate memory approximation from conditional generation error. Experiments on compressible flows with shocks and multiscale mixing show improved accuracy and markedly more stable long-horizon rollouts, with better fine-scale spectral and statistical fidelity.

</details>


### [174] [Zero-shot Generalizable Graph Anomaly Detection with Mixture of Riemannian Experts](https://arxiv.org/abs/2602.06859)
*Xinyu Zhao,Qingyun Sun,Jiayi Luo,Xingcheng Fu,Jianxin Li*

Main category: cs.LG

TL;DR: 提出GAD-MoRE框架，通过混合黎曼专家架构实现零样本图异常检测，解决单曲率空间无法捕捉几何相关异常模式的问题。


<details>
  <summary>Details</summary>
Motivation: 现有零样本图异常检测方法忽略不同异常模式的内在几何差异，将不同域图嵌入单一静态曲率空间会扭曲异常结构特征，限制了跨域泛化能力。

Method: 提出GAD-MoRE框架：1）使用多个专门黎曼专家网络，每个在特定曲率空间操作；2）异常感知多曲率特征对齐模块，将输入投影到并行黎曼空间；3）基于记忆的动态路由器，根据历史重建性能自适应分配输入到最兼容专家。

Result: 在零样本设置下，GAD-MoRE显著优于最先进的通用图异常检测基线，甚至超过在目标域用标记数据进行少样本微调的强竞争对手。

Conclusion: 通过混合黎曼专家架构建模几何相关的异常模式，GAD-MoRE实现了更好的跨域泛化能力，为零样本图异常检测提供了有效解决方案。

Abstract: Graph Anomaly Detection (GAD) aims to identify irregular patterns in graph data, and recent works have explored zero-shot generalist GAD to enable generalization to unseen graph datasets. However, existing zero-shot GAD methods largely ignore intrinsic geometric differences across diverse anomaly patterns, substantially limiting their cross-domain generalization. In this work, we reveal that anomaly detectability is highly dependent on the underlying geometric properties and that embedding graphs from different domains into a single static curvature space can distort the structural signatures of anomalies. To address the challenge that a single curvature space cannot capture geometry-dependent graph anomaly patterns, we propose GAD-MoRE, a novel framework for zero-shot Generalizable Graph Anomaly Detection with a Mixture of Riemannian Experts architecture. Specifically, to ensure that each anomaly pattern is modeled in the Riemannian space where it is most detectable, GAD-MoRE employs a set of specialized Riemannian expert networks, each operating in a distinct curvature space. To align raw node features with curvature-specific anomaly characteristics, we introduce an anomaly-aware multi-curvature feature alignment module that projects inputs into parallel Riemannian spaces, enabling the capture of diverse geometric characteristics. Finally, to facilitate better generalization beyond seen patterns, we design a memory-based dynamic router that adaptively assigns each input to the most compatible expert based on historical reconstruction performance on similar anomalies. Extensive experiments in the zero-shot setting demonstrate that GAD-MoRE significantly outperforms state-of-the-art generalist GAD baselines, and even surpasses strong competitors that are few-shot fine-tuned with labeled data from the target domain.

</details>


### [175] [NanoQuant: Efficient Sub-1-Bit Quantization of Large Language Models](https://arxiv.org/abs/2602.06694)
*Hyochan Chong,Dongkyu Kim,Changdong Kim,Minseop Choi*

Main category: cs.LG

TL;DR: NanoQuant是一种新颖的后训练量化方法，首次将大语言模型压缩到二进制和亚1比特级别，通过低秩二进制分解实现高效压缩。


<details>
  <summary>Details</summary>
Motivation: 现有权重量化方法无法高效压缩到二进制（1比特）级别，要么需要大量数据和计算资源，要么增加额外存储开销。需要一种能在低内存环境下高效压缩LLM的方法。

Method: 将量化建模为低秩二进制分解问题，将全精度权重压缩为低秩二进制矩阵和缩放因子。使用ADMM方法精确初始化潜在二进制矩阵和缩放因子，然后通过块级和模型级重构过程微调初始化参数。

Result: 在低内存后训练量化中建立了新的帕累托前沿，在亚1比特压缩率下实现最先进的精度。例如，在单个H100上13小时内将Llama2-70B压缩25.8倍，使70B模型能在8GB消费级GPU上运行。

Conclusion: NanoQuant首次实现了LLM到二进制和亚1比特级别的后训练量化，使大规模模型部署在消费级硬件上变得可行，为高效模型服务提供了新解决方案。

Abstract: Weight-only quantization has become a standard approach for efficiently serving large language models (LLMs). However, existing methods fail to efficiently compress models to binary (1-bit) levels, as they either require large amounts of data and compute or incur additional storage. In this work, we propose NanoQuant, the first post-training quantization (PTQ) method to compress LLMs to both binary and sub-1-bit levels. NanoQuant formulates quantization as a low-rank binary factorization problem, and compresses full-precision weights to low-rank binary matrices and scales. Specifically, it utilizes an efficient alternating direction method of multipliers (ADMM) method to precisely initialize latent binary matrices and scales, and then tune the initialized parameters through a block and model reconstruction process. Consequently, NanoQuant establishes a new Pareto frontier in low-memory post-training quantization, achieving state-of-the-art accuracy even at sub-1-bit compression rates. NanoQuant makes large-scale deployment feasible on consumer hardware. For example, it compresses Llama2-70B by 25.8$\times$ in just 13 hours on a single H100, enabling a 70B model to operate on a consumer 8 GB GPU.

</details>


### [176] [Diffeomorphism-Equivariant Neural Networks](https://arxiv.org/abs/2602.06695)
*Josephine Elisabeth Oettinger,Zakhar Shumaylov,Johannes Bostelmann,Jan Lellmann,Carola-Bibiane Schönlieb*

Main category: cs.LG

TL;DR: 提出一种通过能量基规范化方法，使预训练神经网络实现微分同胚等变性的策略，无需大量数据增强或重新训练


<details>
  <summary>Details</summary>
Motivation: 现有等变性方法主要针对紧致、有限或低维线性作用群，本文探索如何将等变性扩展到无限维群（如微分同胚群）

Method: 将等变性表述为优化问题，利用已建立的微分图像配准方法工具箱，通过能量基规范化在预训练神经网络中诱导微分同胚等变性

Result: 在分割和分类任务上的实验结果表明，该方法实现了近似等变性，并能泛化到未见过的变换，无需依赖大量数据增强或重新训练

Conclusion: 提出的能量基规范化策略成功地将等变性扩展到无限维群，为预训练神经网络提供了一种有效的微分同胚等变方法

Abstract: Incorporating group symmetries via equivariance into neural networks has emerged as a robust approach for overcoming the efficiency and data demands of modern deep learning. While most existing approaches, such as group convolutions and averaging-based methods, focus on compact, finite, or low-dimensional groups with linear actions, this work explores how equivariance can be extended to infinite-dimensional groups. We propose a strategy designed to induce diffeomorphism equivariance in pre-trained neural networks via energy-based canonicalisation. Formulating equivariance as an optimisation problem allows us to access the rich toolbox of already established differentiable image registration methods. Empirical results on segmentation and classification tasks confirm that our approach achieves approximate equivariance and generalises to unseen transformations without relying on extensive data augmentation or retraining.

</details>


### [177] [Explaining Grokking in Transformers through the Lens of Inductive Bias](https://arxiv.org/abs/2602.06702)
*Jaisidh Singh,Diganta Misra,Antonio Orvieto*

Main category: cs.LG

TL;DR: 该论文研究transformer中的"grokking"现象，从归纳偏置角度分析架构和优化设置如何影响grokking速度和特征演化，发现Layer Normalization位置、学习率、权重衰减等因素会调节grokking过程，并揭示特征可压缩性与泛化出现的关系。


<details>
  <summary>Details</summary>
Motivation: 研究transformer中的grokking现象，从归纳偏置的角度理解架构和优化设置如何影响网络的学习偏好和解决方案选择，以深入理解grokking机制。

Method: 通过分析Layer Normalization位置对grokking速度的影响，研究LN如何调节捷径学习和注意力熵；考察不同优化设置（如学习率、权重衰减）如何影响grokking，分析readout scale作为控制变量的有效性；研究特征在训练过程中的连续演化；探索特征可压缩性与泛化出现的关系。

Result: 发现LN位置强烈调节grokking速度，LN通过影响特定路径上的捷径学习和注意力熵起作用；优化设置（学习率、权重衰减）会混淆readout scale作为懒惰训练控制变量的效果；特征在整个训练过程中连续演化，表明grokking比简单的懒惰到丰富学习机制转换更复杂；特征可压缩性与泛化出现可预测地相关。

Conclusion: transformer中的grokking现象受到架构和优化诱导的归纳偏置的复杂调节，特征演化是连续的而非突变的，特征可压缩性为理解泛化出现提供了新的视角。

Abstract: We investigate grokking in transformers through the lens of inductive bias: dispositions arising from architecture or optimization that let the network prefer one solution over another. We first show that architectural choices such as the position of Layer Normalization (LN) strongly modulates grokking speed. This modulation is explained by isolating how LN on specific pathways shapes shortcut-learning and attention entropy. Subsequently, we study how different optimization settings modulate grokking, inducing distinct interpretations of previously proposed controls such as readout scale. Particularly, we find that using readout scale as a control for lazy training can be confounded by learning rate and weight decay in our setting. Accordingly, we show that features evolve continuously throughout training, suggesting grokking in transformers can be more nuanced than a lazy-to-rich transition of the learning regime. Finally, we show how generalization predictably emerges with feature compressibility in grokking, across different modulators of inductive bias. Our code is released at https://tinyurl.com/y52u3cad.

</details>


### [178] [From Kepler to Newton: Inductive Biases Guide Learned World Models in Transformers](https://arxiv.org/abs/2602.06923)
*Ziming Liu,Sophia Sanborn,Surya Ganguli,Andreas Tolias*

Main category: cs.LG

TL;DR: 通过引入空间平滑性、稳定性和时间局部性三个最小归纳偏置，使通用Transformer能够超越曲线拟合，学习开普勒世界模型并发现牛顿力表示，实现从预测到物理定律发现的转变。


<details>
  <summary>Details</summary>
Motivation: 现有AI物理学家方法依赖领域特定先验，而通用Transformer虽然预测准确却无法捕获底层物理定律。本文旨在弥合这一差距，探索通用AI架构能否超越预测，真正发现支配宇宙的物理定律。

Method: 系统引入三个最小归纳偏置：1) 空间平滑性：将预测公式化为连续回归；2) 稳定性：使用噪声上下文训练以减少误差累积；3) 时间局部性：将注意力窗口限制在最近过去，强制模型依赖局部状态而非复杂历史。

Result: 前两个偏置使Transformer能够学习连贯的开普勒世界模型，成功拟合行星轨迹椭圆。第三个时间局部性偏置迫使模型放弃曲线拟合，发现牛顿力表示，实现从曲线拟合器到物理学家的转变。

Conclusion: 简单的架构选择决定了AI成为曲线拟合器还是物理学家。通过引入最小归纳偏置，通用Transformer能够超越预测，发现物理定律，这是迈向自动化科学发现的关键一步。

Abstract: Can general-purpose AI architectures go beyond prediction to discover the physical laws governing the universe? True intelligence relies on "world models" -- causal abstractions that allow an agent to not only predict future states but understand the underlying governing dynamics. While previous "AI Physicist" approaches have successfully recovered such laws, they typically rely on strong, domain-specific priors that effectively "bake in" the physics. Conversely, Vafa et al. recently showed that generic Transformers fail to acquire these world models, achieving high predictive accuracy without capturing the underlying physical laws. We bridge this gap by systematically introducing three minimal inductive biases. We show that ensuring spatial smoothness (by formulating prediction as continuous regression) and stability (by training with noisy contexts to mitigate error accumulation) enables generic Transformers to surpass prior failures and learn a coherent Keplerian world model, successfully fitting ellipses to planetary trajectories. However, true physical insight requires a third bias: temporal locality. By restricting the attention window to the immediate past -- imposing the simple assumption that future states depend only on the local state rather than a complex history -- we force the model to abandon curve-fitting and discover Newtonian force representations. Our results demonstrate that simple architectural choices determine whether an AI becomes a curve-fitter or a physicist, marking a critical step toward automated scientific discovery.

</details>


### [179] [Cochain Perspectives on Temporal-Difference Signals for Learning Beyond Markov Dynamics](https://arxiv.org/abs/2602.06939)
*Zuyuan Zhang,Sizhe Tang,Tian Lan*

Main category: cs.LG

TL;DR: 本文提出了一种新的拓扑视角来理解强化学习中的时间差分方法，将TD误差视为状态转移拓扑空间中的1-上链，并开发了HodgeFlow策略搜索算法来处理非马尔可夫环境。


<details>
  <summary>Details</summary>
Motivation: 现实世界环境中普遍存在非马尔可夫动态（长程依赖、部分可观测性、记忆效应），而强化学习的核心Bellman方程在非马尔可夫环境下仅近似成立。现有研究多关注实用算法设计，缺乏理论分析来回答关键问题：哪些动态能被Bellman框架捕获？如何启发新的最优近似算法类？

Method: 提出拓扑视角的时间差分强化学习：将TD误差视为状态转移拓扑空间中的1-上链，将马尔可夫动态解释为拓扑可积性。通过Bellman-de Rham投影获得TD误差的Hodge型分解（可积分量+拓扑残差）。提出HodgeFlow策略搜索算法，通过拟合势网络来最小化RL中的不可积投影残差，获得稳定性/敏感性保证。

Result: 在数值评估中，HFPS在非马尔可夫环境下显著提高了强化学习性能。

Conclusion: 该研究为理解强化学习中的时间差分方法提供了新颖的拓扑框架，能够处理非马尔可夫动态，并通过HodgeFlow策略搜索算法实现了性能提升和理论保证。

Abstract: Non-Markovian dynamics are commonly found in real-world environments due to long-range dependencies, partial observability, and memory effects. The Bellman equation that is the central pillar of Reinforcement learning (RL) becomes only approximately valid under Non-Markovian. Existing work often focus on practical algorithm designs and offer limited theoretical treatment to address key questions, such as what dynamics are indeed capturable by the Bellman framework and how to inspire new algorithm classes with optimal approximations. In this paper, we present a novel topological viewpoint on temporal-difference (TD) based RL. We show that TD errors can be viewed as 1-cochain in the topological space of state transitions, while Markov dynamics are then interpreted as topological integrability. This novel view enables us to obtain a Hodge-type decomposition of TD errors into an integrable component and a topological residual, through a Bellman-de Rham projection. We further propose HodgeFlow Policy Search (HFPS) by fitting a potential network to minimize the non-integrable projection residual in RL, achieving stability/sensitivity guarantees. In numerical evaluations, HFPS is shown to significantly improve RL performance under non-Markovian.

</details>


### [180] [Disentanglement by means of action-induced representations](https://arxiv.org/abs/2602.06741)
*Gorka Muñoz-Gil,Hendrik Poulsen Nautrup,Arunava Majumder,Paulin de Schoulepnikoff,Florian Fürrutter,Marius Krumm,Hans J. Briegel*

Main category: cs.LG

TL;DR: 提出动作诱导表示框架，通过建模物理系统的动作依赖关系，实现可证明的解耦表示学习，超越传统VAE的限制。


<details>
  <summary>Details</summary>
Motivation: 传统变分自编码器在解耦表示学习方面面临挑战，难以实现非线性独立成分分析。需要新的框架来建模物理系统的动作依赖关系，从而获得可证明的解耦表示。

Method: 引入动作诱导表示框架，建模物理系统在实验或动作下的表示。提出变分AIR架构，能够提取AIRs并实现可证明的解耦。

Result: 在动作诱导表示框架下，可以证明地解耦自由度与其动作依赖关系。VAIR架构能够提取AIRs，在标准VAE失败的情况下实现可证明的解耦。

Conclusion: AIR框架为解耦表示学习提供了理论基础，VAIR架构能够捕获底层生成因子的动作依赖关系，直接将实验与其影响的自由度联系起来。

Abstract: Learning interpretable representations with variational autoencoders (VAEs) is a major goal of representation learning. The main challenge lies in obtaining disentangled representations, where each latent dimension corresponds to a distinct generative factor. This difficulty is fundamentally tied to the inability to perform nonlinear independent component analysis. Here, we introduce the framework of action-induced representations (AIRs) which models representations of physical systems given experiments (or actions) that can be performed on them. We show that, in this framework, we can provably disentangle degrees of freedom w.r.t. their action dependence. We further introduce a variational AIR architecture (VAIR) that can extract AIRs and therefore achieve provable disentanglement where standard VAEs fail. Beyond state representation, VAIR also captures the action dependence of the underlying generative factors, directly linking experiments to the degrees of freedom they influence.

</details>


### [181] [Soft Forward-Backward Representations for Zero-shot Reinforcement Learning with General Utilities](https://arxiv.org/abs/2602.06769)
*Marco Bagatella,Thomas Rupf,Georg Martius,Andreas Krause*

Main category: cs.LG

TL;DR: 提出一种基于最大熵前向-后向算法的零样本强化学习方法，能够处理一般效用函数（不仅是累加奖励），直接从离线数据中提取策略族，并在测试时通过零阶搜索优化任意目标。


<details>
  <summary>Details</summary>
Motivation: 现有零样本强化学习方法主要处理累加奖励问题，但实际应用中存在更一般的效用函数（如分布匹配、纯探索等），这些无法简化为累加奖励形式。需要扩展零样本RL方法以处理更广泛的问题类别。

Method: 提出最大熵前向-后向算法（soft FB），从离线数据中提取随机策略族。结合紧凑策略嵌入的零阶搜索，在测试时直接优化一般效用函数，避免迭代优化过程。

Result: 方法在简单示例和高维实验中均表现出色，保留了FB算法的优势特性，同时将适用范围扩展到更一般的RL问题，能够处理分布匹配等传统累加奖励无法表达的任务。

Conclusion: 提出的最大熵前向-后向算法成功扩展了零样本强化学习的适用范围，使其能够处理一般效用函数问题，为离线RL在更广泛任务中的应用提供了新途径。

Abstract: Recent advancements in zero-shot reinforcement learning (RL) have facilitated the extraction of diverse behaviors from unlabeled, offline data sources. In particular, forward-backward algorithms (FB) can retrieve a family of policies that can approximately solve any standard RL problem (with additive rewards, linear in the occupancy measure), given sufficient capacity. While retaining zero-shot properties, we tackle the greater problem class of RL with general utilities, in which the objective is an arbitrary differentiable function of the occupancy measure. This setting is strictly more expressive, capturing tasks such as distribution matching or pure exploration, which may not be reduced to additive rewards. We show that this additional complexity can be captured by a novel, maximum entropy (soft) variant of the forward-backward algorithm, which recovers a family of stochastic policies from offline data. When coupled with zero-order search over compact policy embeddings, this algorithm can sidestep iterative optimization schemes, and optimizes general utilities directly at test-time. Across both didactic and high-dimensional experiments, we demonstrate that our method retains favorable properties of FB algorithms, while also extending their range to more general RL problems.

</details>


### [182] [Calibrating Generative AI to Produce Realistic Essays for Data Augmentation](https://arxiv.org/abs/2602.06772)
*Edward W. Wolfe,Justin O. Barber*

Main category: cs.LG

TL;DR: 研究比较了三种LLM提示方法生成模拟作文的效果，发现"预测下一个"策略在评分一致性、保持原文质量和文本真实性方面表现最佳


<details>
  <summary>Details</summary>
Motivation: 数据增强可以缓解构建响应项目自动评分引擎中训练数据有限的问题，需要确定哪种LLM提示方法能最好地生成保持原文写作质量且真实的模拟作文

Method: 使用三种大型语言模型提示方法（预测下一个、句子策略、25个示例策略）生成模拟学生作文，由人工评分员对模拟作文进行评分并评估文本真实性

Result: 预测下一个策略在人工评分员对模拟作文评分的一致性方面最高；预测下一个和句子策略最能保持原文的评分质量；预测下一个和25个示例策略生成的文本最真实

Conclusion: 预测下一个提示策略在生成用于自动评分引擎数据增强的模拟作文方面表现最佳，能有效保持原文质量并产生真实文本

Abstract: Data augmentation can mitigate limited training data in machine-learning automated scoring engines for constructed response items. This study seeks to determine how well three approaches to large language model prompting produce essays that preserve the writing quality of the original essays and produce realistic text for augmenting ASE training datasets. We created simulated versions of student essays, and human raters assigned scores to them and rated the realism of the generated text. The results of the study indicate that the predict next prompting strategy produces the highest level of agreement between human raters regarding simulated essay scores, predict next and sentence strategies best preserve the rated quality of the original essay in the simulated essays, and predict next and 25 examples strategies produce the most realistic text as judged by human raters.

</details>


### [183] [Weisfeiler and Lehman Go Categorical](https://arxiv.org/abs/2602.06787)
*Seongjin Choi,Gahee Kim,Se-Young Yun*

Main category: cs.LG

TL;DR: 论文提出了范畴Weisfeiler-Lehman框架，将超图提升映射形式化为从任意数据范畴到分级偏序集范畴的函子映射，并推导出超图同构网络家族，其消息传递拓扑由函子选择严格决定。


<details>
  <summary>Details</summary>
Motivation: 提升映射显著增强了图神经网络的表达能力，但将这一范式扩展到超图领域仍然零散。现有方法缺乏系统性框架来形式化超图的提升操作，无法捕获复杂的交集几何结构。

Method: 引入范畴Weisfeiler-Lehman框架，将提升形式化为从数据范畴到分级偏序集范畴的函子映射。针对超图提出了两种具体函子：关联函子和对称单纯复形函子，从而系统推导出超图同构网络家族，其中消息传递拓扑由函子选择严格决定。

Result: 理论分析表明，基于关联和对称单纯复形的两种方法都包含了标准超图Weisfeiler-Lehman测试的表达能力。在真实世界基准测试上的广泛实验验证了这些理论发现。

Conclusion: 范畴Weisfeiler-Lehman框架为超图神经网络提供了统一的函子化方法，能够系统推导出具有严格理论保证的架构，捕获现有方法常忽略的复杂交集几何结构，显著提升了超图表示学习能力。

Abstract: While lifting map has significantly enhanced the expressivity of graph neural networks, extending this paradigm to hypergraphs remains fragmented. To address this, we introduce the categorical Weisfeiler-Lehman framework, which formalizes lifting as a functorial mapping from an arbitrary data category to the unifying category of graded posets. When applied to hypergraphs, this perspective allows us to systematically derive Hypergraph Isomorphism Networks, a family of neural architectures where the message passing topology is strictly determined by the choice of functor. We introduce two distinct functors from the category of hypergraphs: an incidence functor and a symmetric simplicial complex functor. While the incidence architecture structurally mirrors standard bipartite schemes, our functorial derivation enforces a richer information flow over the resulting poset, capturing complex intersection geometries often missed by existing methods. We theoretically characterize the expressivity of these models, proving that both the incidence-based and symmetric simplicial approaches subsume the expressive power of the standard Hypergraph Weisfeiler-Lehman test. Extensive experiments on real-world benchmarks validate these theoretical findings.

</details>


### [184] [Displacement-Resistant Extensions of DPO with Nonconvex $f$-Divergences](https://arxiv.org/abs/2602.06788)
*Idan Pipano,Shoham Sabach,Kavosh Asadi,Mohammad Ghavamzadeh*

Main category: cs.LG

TL;DR: 论文提出了一种更广义的RLHF优化框架，放宽了f-散度的凸性要求，定义了DPO-inducing条件，并提出了防止概率位移的displacement-resistant条件，最终开发了SquaredPO损失函数。


<details>
  <summary>Details</summary>
Motivation: 现有DPO及相关算法通过KL散度惩罚来对齐语言模型，但这种方法可以进一步推广。作者发现凸性要求并非必要，希望找到更一般的条件来保持RLHF问题的可解性，同时解决概率位移问题。

Method: 1) 识别DPO-inducing条件：精确刻画RLHF问题保持可解性的更一般条件；2) 建立displacement-resistant条件：防止概率位移现象；3) 提出SquaredPO损失：基于满足上述两个条件的特定f-散度。

Result: SquaredPO损失相比DPO具有更强的理论保证，同时在实践中表现竞争力。论文建立了更一般的RLHF优化理论框架，放宽了凸性要求，并解决了概率位移问题。

Conclusion: 论文成功扩展了RLHF优化框架，提出了更一般的DPO-inducing条件和防止概率位移的displacement-resistant条件，并基于此开发了具有更强理论保证的SquaredPO损失函数。

Abstract: DPO and related algorithms align language models by directly optimizing the RLHF objective: find a policy that maximizes the Bradley-Terry reward while staying close to a reference policy through a KL divergence penalty. Previous work showed that this approach could be further generalized: the original problem remains tractable even if the KL divergence is replaced by a family of $f$-divergence with a convex generating function $f$. Our first contribution is to show that convexity of $f$ is not essential. Instead, we identify a more general condition, referred to as DPO-inducing, that precisely characterizes when the RLHF problem remains tractable. Our next contribution is to establish a second condition on $f$ that is necessary to prevent probability displacement, a known empirical phenomenon in which the probabilities of the winner and the loser responses approach zero. We refer to any $f$ that satisfies this condition as displacement-resistant. We finally focus on a specific DPO-inducing and displacement-resistant $f$, leading to our novel SquaredPO loss. Compared to DPO, this new loss offers stronger theoretical guarantees while performing competitively in practice.

</details>


### [185] [Rare Event Analysis of Large Language Models](https://arxiv.org/abs/2602.06791)
*Jake McAllister Dorman,Edward Gillman,Dominic C. Rose,Jamie F. Mair,Juan P. Garrahan*

Main category: cs.LG

TL;DR: 本文提出了一个用于系统分析大语言模型中罕见事件的端到端框架，包括理论、高效生成策略、概率估计和误差分析。


<details>
  <summary>Details</summary>
Motivation: 大语言模型作为概率模型，在推理过程中会出现罕见事件——这些行为远离典型但高度显著。由于罕见事件定义上难以观察，而LLM的巨大使用规模意味着在开发阶段完全未观察到的事件很可能在部署时变得突出，因此需要系统分析这些罕见事件。

Method: 提出了一个端到端框架，包含理论分析、高效生成策略、概率估计和误差分析。提供了实际实现，并通过具体示例进行说明。

Result: 开发了一个完整的分析框架，能够系统性地识别、生成和分析LLM中的罕见事件，为理解模型行为提供了新工具。

Conclusion: 该框架的概念和技术具有通用性，可扩展到其他模型和上下文，为分析概率模型中的罕见事件提供了系统方法。

Abstract: Being probabilistic models, during inference large language models (LLMs) display rare events: behaviour that is far from typical but highly significant. By definition all rare events are hard to see, but the enormous scale of LLM usage means that events completely unobserved during development are likely to become prominent in deployment. Here we present an end-to-end framework for the systematic analysis of rare events in LLMs. We provide a practical implementation spanning theory, efficient generation strategies, probability estimation and error analysis, which we illustrate with concrete examples. We outline extensions and applications to other models and contexts, highlighting the generality of the concepts and techniques presented here.

</details>


### [186] [FlowDA: Accurate, Low-Latency Weather Data Assimilation via Flow Matching](https://arxiv.org/abs/2602.06800)
*Ran Cheng,Lailai Zhu*

Main category: cs.LG

TL;DR: FlowDA：基于流匹配的低延迟天气尺度生成式数据同化框架，通过条件化观测和微调Aurora基础模型，在稀疏观测下实现高效准确的分析


<details>
  <summary>Details</summary>
Motivation: 传统变分数据同化方法是机器学习天气预报管道中的主要计算瓶颈，而现有的生成式ML同化方法需要大量采样步骤，在长时域自回归循环同化中容易产生误差累积

Method: 基于流匹配的生成式数据同化框架，使用SetConv嵌入对观测进行条件化，并微调Aurora基础模型，实现低延迟天气尺度的数据同化

Result: 在观测率从3.9%降至0.1%的各种情况下，FlowDA表现出优于类似可调参数规模基线的性能，对观测噪声具有鲁棒性，在长时域自回归循环同化中保持稳定

Conclusion: FlowDA为数据驱动的数据同化提供了一个高效且可扩展的方向，展示了生成式方法在天气预测中的潜力

Abstract: Data assimilation (DA) is a fundamental component of modern weather prediction, yet it remains a major computational bottleneck in machine learning (ML)-based forecasting pipelines due to reliance on traditional variational methods. Recent generative ML-based DA methods offer a promising alternative but typically require many sampling steps and suffer from error accumulation under long-horizon auto-regressive rollouts with cycling assimilation. We propose FlowDA, a low-latency weather-scale generative DA framework based on flow matching. FlowDA conditions on observations through a SetConv-based embedding and fine-tunes the Aurora foundation model to deliver accurate, efficient, and robust analyses. Experiments across observation rates decreasing from $3.9\%$ to $0.1\%$ demonstrate superior performance of FlowDA over strong baselines with similar tunable-parameter size. FlowDA further shows robustness to observational noise and stable performance in long-horizon auto-regressive cycling DA. Overall, FlowDA points to an efficient and scalable direction for data-driven DA.

</details>


### [187] [Calibrating Tabular Anomaly Detection via Optimal Transport](https://arxiv.org/abs/2602.06810)
*Hangting Ye,He Zhao. Wei Fan,Xiaozhuang Song,Dandan Guo,Yi Chang,Hongyuan Zha*

Main category: cs.LG

TL;DR: CTAD是一个模型无关的后处理框架，通过样本特异性校准增强任何现有表格异常检测器，利用最优传输距离测量测试样本对正常数据分布的破坏程度。


<details>
  <summary>Details</summary>
Motivation: 表格异常检测面临挑战，因为表格数据具有异质性：特征缺乏自然关系、分布和尺度差异大、类型多样。现有方法对异常模式有隐含假设，在某些数据集上表现好但在其他数据集上失败，没有方法能在所有场景中一致优于其他方法。

Method: 通过两个互补分布（随机采样的经验分布和K-means质心的结构分布）表征正常数据，使用最优传输距离测量添加测试样本对分布兼容性的破坏程度。正常样本破坏小，异常样本破坏大，从而提供校准信号增强检测。

Result: 在34个多样化表格数据集和7个代表性检测器（涵盖密度估计、分类、重建和隔离方法）上的实验表明，CTAD能一致显著提升性能。即使对最先进的深度学习方法也能增强，在不同超参数设置下表现稳健，实际部署无需额外调优。

Conclusion: CTAD是一个通用有效的后处理框架，能够校准任何表格异常检测器，通过最优传输距离测量样本对正常数据分布的破坏，在多样化数据集上一致提升检测性能，具有理论保证和实际部署便利性。

Abstract: Tabular anomaly detection (TAD) remains challenging due to the heterogeneity of tabular data: features lack natural relationships, vary widely in distribution and scale, and exhibit diverse types. Consequently, each TAD method makes implicit assumptions about anomaly patterns that work well on some datasets but fail on others, and no method consistently outperforms across diverse scenarios. We present CTAD (Calibrating Tabular Anomaly Detection), a model-agnostic post-processing framework that enhances any existing TAD detector through sample-specific calibration. Our approach characterizes normal data via two complementary distributions, i.e., an empirical distribution from random sampling and a structural distribution from K-means centroids, and measures how adding a test sample disrupts their compatibility using Optimal Transport (OT) distance. Normal samples maintain low disruption while anomalies cause high disruption, providing a calibration signal to amplify detection. We prove that OT distance has a lower bound proportional to the test sample's distance from centroids, and establish that anomalies systematically receive higher calibration scores than normals in expectation, explaining why the method generalizes across datasets. Extensive experiments on 34 diverse tabular datasets with 7 representative detectors spanning all major TAD categories (density estimation, classification, reconstruction, and isolation-based methods) demonstrate that CTAD consistently improves performance with statistical significance. Remarkably, CTAD enhances even state-of-the-art deep learning methods and shows robust performance across diverse hyperparameter settings, requiring no additional tuning for practical deployment.

</details>


### [188] [Improved Sampling Schedules for Discrete Diffusion Models](https://arxiv.org/abs/2602.06849)
*Alberto Foresti,Mustapha Bounoua,Giulio Franzese,Luca Ambrogioni,Pietro Michiardi*

Main category: cs.LG

TL;DR: 该论文提出基于热力学熵产生理论分析离散扩散模型的反向过程，引入两种新型采样调度策略（EDS和WDS），在多个领域显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型在序列数据生成中表现出强大能力，但其反向过程的信息论原理相比连续扩散模型理解不足。作者希望从热力学熵产生的角度分析反向过程动态，填补这一理论空白。

Method: 1) 提出熵产生率作为量化信息生成的严格代理指标，推导出中间状态与数据分布之间Wasserstein距离的边界；2) 基于物理启发指标引入两种均匀间隔的采样调度：熵离散调度（EDS，保持恒定信息增益率）和Wasserstein离散调度（WDS，在Wasserstein距离上采取等步长

Result: 实验证明，提出的调度策略在合成数据、音乐符号、视觉和语言建模等多个应用领域显著优于最先进方法，在更低计算预算下获得更优性能

Conclusion: 通过热力学熵产生理论分析离散扩散模型的反向过程，提出了信息论驱动的采样调度方法，为离散扩散模型的优化提供了理论基础和实践指导

Abstract: Discrete diffusion models have emerged as a powerful paradigm for generative modeling on sequence data; however, the information-theoretic principles governing their reverse processes remain significantly less understood than those of their continuous counterparts. In this work, we bridge this gap by analyzing the reverse process dynamics through the lens of thermodynamic entropy production. We propose the entropy production rate as a rigorous proxy for quantifying information generation, deriving as a byproduct a bound on the Wasserstein distance between intermediate states and the data distribution. Leveraging these insights, we introduce two novel sampling schedules that are uniformly spaced with respect to their corresponding physics-inspired metrics: the Entropic Discrete Schedule (EDS), which is defined by maintaining a constant rate of information gain, and the Wasserstein Discrete Schedule (WDS), which is defined by taking equal steps in terms of the Wasserstein distance. We empirically demonstrate that our proposed schedules significantly outperform state-of-the-art strategies across diverse application domains, including synthetic data, music notation, vision and language modeling, consistently achieving superior performance at a lower computational budget.

</details>


### [189] [Designing a Robust, Bounded, and Smooth Loss Function for Improved Supervised Learning](https://arxiv.org/abs/2602.06858)
*Soumi Mahato,Lineesh M. C*

Main category: cs.LG

TL;DR: 提出了一种鲁棒、有界且平滑的RoBoS-NN损失函数，用于解决传统损失函数在高维和异常值敏感数据集上的局限性，并在神经网络中实现用于时间序列预测。


<details>
  <summary>Details</summary>
Motivation: 传统损失函数在处理高维和异常值敏感数据集时存在显著缺陷，导致性能下降和训练收敛缓慢，需要开发更鲁棒的损失函数来解决这些问题。

Method: 开发了RoBoS-NN损失函数（鲁棒、有界、平滑），在神经网络框架中实现为ℒ_RoBoS-NN算法，用于时间序列预测，并对损失函数的泛化能力进行了理论分析。

Result: 在多个真实世界数据集上的实验表明，ℒ_RoBoS-NN在准确性指标上优于其他基准模型，即使在注入异常值的更具挑战性场景中也能保持良好性能。

Conclusion: RoBoS-NN损失函数有效解决了传统损失函数在处理高维和异常值敏感数据时的局限性，ℒ_RoBoS-NN算法在时间序列预测中表现出优越的鲁棒性和准确性。

Abstract: The loss function is crucial to machine learning, especially in supervised learning frameworks. It is a fundamental component that controls the behavior and general efficacy of learning algorithms. However, despite their widespread use, traditional loss functions have significant drawbacks when dealing with high-dimensional and outlier-sensitive datasets, which frequently results in reduced performance and slower convergence during training. In this work, we develop a robust, bounded, and smooth (RoBoS-NN) loss function to resolve the aforementioned hindrances. The generalization ability of the loss function has also been theoretically analyzed to rigorously justify its robustness. Moreover, we implement RoboS-NN loss in the framework of a neural network (NN) to forecast time series and present a new robust algorithm named $\mathcal{L}_{\text{RoBoS}}$-NN. To assess the potential of $\mathcal{L}_{\text{RoBoS}}$-NN, we conduct experiments on multiple real-world datasets. In addition, we infuse outliers into data sets to evaluate the performance of $\mathcal{L}_{\text{RoBoS}}$-NN in more challenging scenarios. Numerical results show that $\mathcal{L}_{\text{RoBoS}}$-NN outperforms the other benchmark models in terms of accuracy measures.

</details>


### [190] [T-STAR: A Context-Aware Transformer Framework for Short-Term Probabilistic Demand Forecasting in Dock-Based Shared Micro-Mobility](https://arxiv.org/abs/2602.06866)
*Jingyi Cheng,Gonçalo Homem de Almeida Correia,Oded Cats,Shadi Sharif Azadeh*

Main category: cs.LG

TL;DR: T-STAR是一个基于Transformer的两阶段概率框架，用于15分钟分辨率的共享单车站点级需求预测，通过分层结构分离稳定模式与短期波动，并整合地铁实时数据提升准确性。


<details>
  <summary>Details</summary>
Motivation: 可靠的短期需求预测对于共享微出行服务管理至关重要，但高分辨率预测面临挑战，需要分离稳定需求模式与短期波动，并考虑时空变化因素。

Method: 提出T-STAR两阶段框架：第一阶段捕捉粗粒度小时级需求模式，第二阶段整合高频局部输入（包括近期波动和地铁实时需求变化）提升预测精度。两阶段均使用时序Transformer模型生成概率预测。

Result: 在华盛顿Capital Bikeshare数据上的实验表明，T-STAR在确定性和概率准确性上均优于现有方法，展现出跨站点和时段的强时空鲁棒性。零样本预测实验显示模型能够迁移到未见过的服务区域而无需重新训练。

Conclusion: T-STAR能够提供细粒度、可靠且不确定性感知的短期需求预测，支持旅行者多模式出行规划的无缝集成，并增强共享微出行服务的实时运营能力。

Abstract: Reliable short-term demand forecasting is essential for managing shared micro-mobility services and ensuring responsive, user-centered operations. This study introduces T-STAR (Two-stage Spatial and Temporal Adaptive contextual Representation), a novel transformer-based probabilistic framework designed to forecast station-level bike-sharing demand at a 15-minute resolution. T-STAR addresses key challenges in high-resolution forecasting by disentangling consistent demand patterns from short-term fluctuations through a hierarchical two-stage structure. The first stage captures coarse-grained hourly demand patterns, while the second stage improves prediction accuracy by incorporating high-frequency, localized inputs, including recent fluctuations and real-time demand variations in connected metro services, to account for temporal shifts in short-term demand. Time series transformer models are employed in both stages to generate probabilistic predictions. Extensive experiments using Washington D.C.'s Capital Bikeshare data demonstrate that T-STAR outperforms existing methods in both deterministic and probabilistic accuracy. The model exhibits strong spatial and temporal robustness across stations and time periods. A zero-shot forecasting experiment further highlights T-STAR's ability to transfer to previously unseen service areas without retraining. These results underscore the framework's potential to deliver granular, reliable, and uncertainty-aware short-term demand forecasts, which enable seamless integration to support multimodal trip planning for travelers and enhance real-time operations in shared micro-mobility services.

</details>


### [191] [Decoupling Variance and Scale-Invariant Updates in Adaptive Gradient Descent for Unified Vector and Matrix Optimization](https://arxiv.org/abs/2602.06880)
*Zitao Song,Cedar Site Bai,Zhe Zhang,Brian Bullins,David F. Gleich*

Main category: cs.LG

TL;DR: DeVA框架通过解耦AdaGrad更新，将方差适应项与尺度不变项分离，实现了向量方差适应与矩阵谱优化之间的无缝过渡，在语言建模和图像分类任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: Adam等自适应方法在大规模向量和欧几里得优化中已成为标准，而Muon等基于矩阵谱的优化器展示了将权重矩阵视为矩阵而非长向量的优势。但连接这两种方法很困难，因为许多自然推广难以实现，且无法简单地将Adam适应机制转移到矩阵谱上。

Method: 重新表述AdaGrad更新，将其分解为方差适应项和尺度不变项。这种解耦产生了DeVA框架，该框架在向量方差适应和矩阵谱优化之间架起桥梁，实现了从Adam到自适应谱下降的无缝过渡。

Result: 在语言建模和图像分类的广泛实验中，DeVA始终优于Muon和SOAP等最先进方法，减少了约6.6%的token使用量。理论上证明了方差适应项有效改善了分块平滑性，促进了更快的收敛。

Conclusion: DeVA框架成功连接了向量方差适应和矩阵谱优化，提供了一种从Adam到自适应谱下降的平滑过渡方法，在实践和理论上都表现出优越性能。

Abstract: Adaptive methods like Adam have become the $\textit{de facto}$ standard for large-scale vector and Euclidean optimization due to their coordinate-wise adaptation with a second-order nature. More recently, matrix-based spectral optimizers like Muon (Jordan et al., 2024b) show the power of treating weight matrices as matrices rather than long vectors. Linking these is hard because many natural generalizations are not feasible to implement, and we also cannot simply move the Adam adaptation to the matrix spectrum. To address this, we reformulate the AdaGrad update and decompose it into a variance adaptation term and a scale-invariant term. This decoupling produces $\textbf{DeVA}$ ($\textbf{De}$coupled $\textbf{V}$ariance $\textbf{A}$daptation), a framework that bridges between vector-based variance adaptation and matrix spectral optimization, enabling a seamless transition from Adam to adaptive spectral descent. Extensive experiments across language modeling and image classification demonstrate that DeVA consistently outperforms state-of-the-art methods such as Muon and SOAP (Vyas et al., 2024), reducing token usage by around 6.6\%. Theoretically, we show that the variance adaptation term effectively improves the blockwise smoothness, facilitating faster convergence. Our implementation is available at https://github.com/Tsedao/Decoupled-Variance-Adaptation

</details>


### [192] [A Cycle-Consistent Graph Surrogate for Full-Cycle Left Ventricular Myocardial Biomechanics](https://arxiv.org/abs/2602.06884)
*Siyu Mu,Wei Xuan Chan,Choon Hwai Yap*

Main category: cs.LG

TL;DR: 提出CardioGraphFENet，一种基于图的替代模型，用于快速全周期估计左心室心肌生物力学，减少对传统有限元分析的计算依赖。


<details>
  <summary>Details</summary>
Motivation: 基于图像的左心室力学模拟对理解心脏功能和临床干预规划很重要，但传统有限元分析计算量大。现有图基替代模型缺乏全周期预测能力，物理信息神经网络在复杂心脏几何上难以收敛。

Method: 提出CardioGraphFENet，包含：1) 全局-局部图编码器捕捉网格特征；2) 基于门控循环单元的时间编码器建模周期一致动态；3) 周期一致双向公式，在单一框架内处理加载和反向卸载。

Result: 模型能高保真地匹配传统有限元分析结果，与集总参数模型结合时产生生理合理的压力-容积环。周期一致性策略显著减少有限元监督需求，仅带来最小精度损失。

Conclusion: CardioGraphFENet为快速全周期左心室生物力学估计提供了一种有效的图基替代方案，减少了计算负担，同时保持高精度和生理合理性。

Abstract: Image-based patient-specific simulation of left ventricular (LV) mechanics is valuable for understanding cardiac function and supporting clinical intervention planning, but conventional finite-element analysis (FEA) is computationally intensive. Current graph-based surrogates do not have full-cycle prediction capabilities, and physics-informed neural networks often struggle to converge on complex cardiac geometries. We present CardioGraphFENet (CGFENet), a unified graph-based surrogate for rapid full-cycle estimation of LV myocardial biomechanics, supervised by a large FEA simulation dataset. The proposed model integrates (i) a global--local graph encoder to capture mesh features with weak-form-inspired global coupling, (ii) a gated recurrent unit-based temporal encoder conditioned on the target volume-time signal to model cycle-coherent dynamics, and (iii) a cycle-consistent bidirectional formulation for both loading and inverse unloading within a single framework. These strategies enable high fidelity with respect to traditional FEA ground truths and produce physiologically plausible pressure-volume loops that match FEA results when coupled with a lumped-parameter model. In particular, the cycle-consistency strategy enables a significant reduction in FEA supervision with only minimal loss in accuracy.

</details>


### [193] [A first realization of reinforcement learning-based closed-loop EEG-TMS](https://arxiv.org/abs/2602.06907)
*Dania Humaidan,Jiahua Xu,Jing Chen,Christoph Zrenner,David Emanuel Vetter,Laura Marzetti,Paolo Belardinelli,Timo Roine,Risto J. Ilmoniemi,Gian Luca Romani,Ulf Zieman*

Main category: cs.LG

TL;DR: 首次实现基于机器学习的闭环实时EEG-TMS系统，通过强化学习独立识别个体mu节律相位与高/低皮层脊髓兴奋性的关联，实现个性化脑刺激治疗。


<details>
  <summary>Details</summary>
Motivation: 传统TMS治疗采用"一刀切"方法，忽视个体间和个体内差异。现有脑状态依赖的EEG-TMS方法仍需要人工预先定义目标相位，存在用户依赖性。

Method: 对25名参与者应用EEG-TMS靶向辅助运动区-初级运动皮层网络，使用强化学习算法识别与高/低皮层脊髓兴奋性相关的mu节律相位。采用线性混合效应模型和贝叶斯分析评估强化学习对皮层脊髓兴奋性（通过运动诱发电位幅度）和功能连接性（通过静息态EEG相干性虚部）的影响。

Result: 强化学习能有效识别与高/低兴奋性状态相关的mu节律相位，对这些相位的重复刺激导致受刺激感觉运动网络功能连接性的长期增加/减少。

Conclusion: 首次证明了闭环EEG-TMS在人类中的可行性，这是实现脑疾病个体化治疗的关键一步。

Abstract: Background: Transcranial magnetic stimulation (TMS) is a powerful tool to investigate neurophysiology of the human brain and treat brain disorders. Traditionally, therapeutic TMS has been applied in a one-size-fits-all approach, disregarding inter- and intra-individual differences. Brain state-dependent EEG-TMS, such as coupling TMS with a pre-specified phase of the sensorimotor mu-rhythm, enables the induction of differential neuroplastic effects depending on the targeted phase. But this approach is still user-dependent as it requires defining an a-priori target phase. Objectives: To present a first realization of a machine-learning-based, closed-loop real-time EEG-TMS setup to identify user-independently the individual mu-rhythm phase associated with high- vs. low-corticospinal excitability states. Methods: We applied EEG-TMS to 25 participants targeting the supplementary motor area-primary motor cortex network and used a reinforcement learning algorithm to identify the mu-rhythm phase associated with high- vs. low corticospinal excitability. We employed linear mixed effects models and Bayesian analysis to determine effects of reinforced learning on corticospinal excitability indexed by motor evoked potential amplitude, and functional connectivity indexed by the imaginary part of resting-state EEG coherence. Results: Reinforcement learning effectively identified the mu-rhythm phase associated with high- vs. low-excitability states, and their repetitive stimulation resulted in long-term increases vs. decreases in functional connectivity in the stimulated sensorimotor network. Conclusions: We demonstrated for the first time the feasibility of closed-loop EEG-TMS in humans, a critical step towards individualized treatment of brain disorders.

</details>


### [194] [Revisiting the Generic Transformer: Deconstructing a Strong Baseline for Time Series Foundation Models](https://arxiv.org/abs/2602.06909)
*Yunshi Wen,Wesley M. Gifford,Chandra Reddy,Lam M. Nguyen,Jayant Kalagnanam,Anak Agung Julius*

Main category: cs.LG

TL;DR: 标准补丁Transformer架构在零样本时间序列预测中达到SOTA性能，关键因素在于模型缩放、数据组合和训练技术，而非复杂架构创新。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列基础模型研究中，异构的训练设置使得难以区分性能提升是源于架构创新还是数据工程。需要建立透明、可复现的基准来明确关键性能驱动因素。

Method: 使用标准补丁Transformer架构，采用简单训练协议，通过全面的消融研究分析模型缩放、数据组合和训练技术对性能的影响。

Result: 标准补丁Transformer在零样本预测中达到最先进性能，确认通用架构本身具有良好的可扩展性，并识别出性能提升的关键驱动因素。

Conclusion: 时间序列预测的高性能主要来自模型缩放、数据组合和训练技术，而非复杂架构创新。开源模型和详细发现为未来研究提供了透明可复现的基准。

Abstract: The recent surge in Time Series Foundation Models has rapidly advanced the field, yet the heterogeneous training setups across studies make it difficult to attribute improvements to architectural innovations versus data engineering. In this work, we investigate the potential of a standard patch Transformer, demonstrating that this generic architecture achieves state-of-the-art zero-shot forecasting performance using a straightforward training protocol. We conduct a comprehensive ablation study that covers model scaling, data composition, and training techniques to isolate the essential ingredients for high performance. Our findings identify the key drivers of performance, while confirming that the generic architecture itself demonstrates excellent scalability. By strictly controlling these variables, we provide comprehensive empirical results on model scaling across multiple dimensions. We release our open-source model and detailed findings to establish a transparent, reproducible baseline for future research.

</details>


### [195] [Robustness Beyond Known Groups with Low-rank Adaptation](https://arxiv.org/abs/2602.06924)
*Abinitha Gourabathina,Hyewon Jeong,Teya Bergamaschi,Marzyeh Ghassemi,Collin Stultz*

Main category: cs.LG

TL;DR: LEIA通过识别表示空间中模型错误集中的低维子空间，无需群体标签即可提升群体鲁棒性


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在特定子群体上存在系统性失败，但现实场景中这些受影响子群体通常未标注或未知，现有方法需要预先知道相关子群体信息

Method: 两阶段方法：1) 识别表示空间中模型错误集中的低维子空间；2) 通过低秩调整分类器logits，将适应限制在这个错误信息子空间内，不修改主干网络且无需群体标签

Result: 在五个真实数据集上，在三种设置下（完全未知、部分已知、完全已知子群体相关性）均能一致提升最差群体性能，同时保持快速、参数高效且对超参数选择鲁棒

Conclusion: LEIA是一种简单有效的群体鲁棒性方法，无需预先指定子群体信息，通过低维错误子空间识别和调整，直接针对潜在失败模式

Abstract: Deep learning models trained to optimize average accuracy often exhibit systematic failures on particular subpopulations. In real world settings, the subpopulations most affected by such disparities are frequently unlabeled or unknown, thereby motivating the development of methods that are performant on sensitive subgroups without being pre-specified. However, existing group-robust methods typically assume prior knowledge of relevant subgroups, using group annotations for training or model selection. We propose Low-rank Error Informed Adaptation (LEIA), a simple two-stage method that improves group robustness by identifying a low-dimensional subspace in the representation space where model errors concentrate. LEIA restricts adaptation to this error-informed subspace via a low-rank adjustment to the classifier logits, directly targeting latent failure modes without modifying the backbone or requiring group labels. Using five real-world datasets, we analyze group robustness under three settings: (1) truly no knowledge of subgroup relevance, (2) partial knowledge of subgroup relevance, and (3) full knowledge of subgroup relevance. Across all settings, LEIA consistently improves worst-group performance while remaining fast, parameter-efficient, and robust to hyperparameter choice.

</details>


### [196] [When RL Meets Adaptive Speculative Training: A Unified Training-Serving System](https://arxiv.org/abs/2602.06932)
*Junxiong Wang,Fengxiang Bie,Jisen Li,Zhongzhu Zhou,Zelei Shao,Yubo Wang,Yinghui Liu,Qingyang Wu,Avner May,Sri Yanamandra,Yineng Zhang,Ce Zhang,Tri Dao,Percy Liang,Ben Athiwaratkun,Shuaiwen Leon Song,Chenfeng Xu,Xiaoxia Wu*

Main category: cs.LG

TL;DR: Aurora是一个统一的训练-服务系统，通过从实时推理轨迹中持续学习推测器，解决了传统推测解码中部署滞后、反馈延迟和领域漂移问题，实现了即日部署和自适应优化。


<details>
  <summary>Details</summary>
Motivation: 传统推测解码将推测器训练与服务解耦，导致三个主要问题：1) 高服务准备时间，需要长时间离线训练；2) 延迟的效用反馈，真正的端到端加速效果只能在训练后评估；3) 领域漂移退化，当目标模型应用于新领域时，推测器会变得过时且效率降低。

Method: Aurora将在线推测器学习重构为异步强化学习问题：接受的token提供正反馈，拒绝的推测器提案提供隐式负反馈。系统集成了SGLang推理服务器和异步训练服务器，支持热交换推测器更新而不中断服务。

Result: Aurora在新发布的前沿模型上实现了1.5倍的即日加速（如MiniMax M2.1 229B和Qwen3-Coder-Next 80B）。在用户流量分布变化时，Aurora相比训练良好但静态的推测器额外提供1.25倍加速（如Qwen3和Llama3）。

Conclusion: Aurora通过统一训练-服务系统解决了推测解码中的部署滞后问题，实现了即日部署和持续自适应优化，显著提升了LLM服务效率，并能有效应对领域漂移挑战。

Abstract: Speculative decoding can significantly accelerate LLM serving, yet most deployments today disentangle speculator training from serving, treating speculator training as a standalone offline modeling problem. We show that this decoupled formulation introduces substantial deployment and adaptation lag: (1) high time-to-serve, since a speculator must be trained offline for a considerable period before deployment; (2) delayed utility feedback, since the true end-to-end decoding speedup is only known after training and cannot be inferred reliably from acceptance rate alone due to model-architecture and system-level overheads; and (3) domain-drift degradation, as the target model is repurposed to new domains and the speculator becomes stale and less effective.
  To address these issues, we present Aurora, a unified training-serving system that closes the loop by continuously learning a speculator directly from live inference traces. Aurora reframes online speculator learning as an asynchronous reinforcement-learning problem: accepted tokens provide positive feedback, while rejected speculator proposals provide implicit negative feedback that we exploit to improve sample efficiency. Our design integrates an SGLang-based inference server with an asynchronous training server, enabling hot-swapped speculator updates without service interruption. Crucially, Aurora supports day-0 deployment: a speculator can be served immediately and rapidly adapted to live traffic, improving system performance while providing immediate utility feedback. Across experiments, Aurora achieves a 1.5x day-0 speedup on recently released frontier models (e.g., MiniMax M2.1 229B and Qwen3-Coder-Next 80B). Aurora also adapts effectively to distribution shifts in user traffic, delivering an additional 1.25x speedup over a well-trained but static speculator on widely used models (e.g., Qwen3 and Llama3).

</details>


### [197] [From Core to Detail: Unsupervised Disentanglement with Entropy-Ordered Flows](https://arxiv.org/abs/2602.06940)
*Daniel Galperin,Ullrich Köthe*

Main category: cs.LG

TL;DR: EOFlows是一种基于归一化流的框架，通过熵排序潜在维度实现自适应表示，允许在推理时灵活选择核心特征维度，支持高压缩和去噪。


<details>
  <summary>Details</summary>
Motivation: 当前无监督表示学习面临两个核心挑战：1）学习到的表示需要具有语义意义；2）表示需要在多次运行中保持稳定。现有方法通常无法同时满足这两个要求，特别是在高维数据如图像上。

Method: 提出熵排序流（EOFlows）框架，基于归一化流技术，通过熵排序潜在维度（类似于PCA的方差解释排序）。结合基于似然的训练、局部雅可比正则化和噪声增强，实现可扩展的高维数据处理。训练后可以灵活选择保留前C个潜在变量作为核心表示。

Result: 在CelebA数据集上的实验表明，该方法能够发现丰富的语义可解释特征，实现高压缩率和强去噪效果。潜在维度按熵排序后，前几个维度捕获主要语义信息，后续维度捕获细节和噪声。

Conclusion: EOFlows提供了一种有效的无监督表示学习方法，通过熵排序实现自适应表示，解决了语义可解释性和稳定性的挑战，特别适用于高维数据如图像的处理。

Abstract: Learning unsupervised representations that are both semantically meaningful and stable across runs remains a central challenge in modern representation learning. We introduce entropy-ordered flows (EOFlows), a normalizing-flow framework that orders latent dimensions by their explained entropy, analogously to PCA's explained variance. This ordering enables adaptive injective flows: after training, one may retain only the top C latent variables to form a compact core representation while the remaining variables capture fine-grained detail and noise, with C chosen flexibly at inference time rather than fixed during training. EOFlows build on insights from Independent Mechanism Analysis, Principal Component Flows and Manifold Entropic Metrics. We combine likelihood-based training with local Jacobian regularization and noise augmentation into a method that scales well to high-dimensional data such as images. Experiments on the CelebA dataset show that our method uncovers a rich set of semantically interpretable features, allowing for high compression and strong denoising.

</details>


### [198] [Improving Credit Card Fraud Detection with an Optimized Explainable Boosting Machine](https://arxiv.org/abs/2602.06955)
*Reza E. Fazel,Arash Bakhtiary,Siavash A. Bigdeli*

Main category: cs.LG

TL;DR: 该研究提出了一种基于可解释提升机(EBM)的增强工作流程，通过系统超参数调优、特征选择和预处理优化，在信用卡欺诈检测中实现了高准确率(ROC-AUC 0.983)与可解释性的平衡，超越了传统机器学习模型。


<details>
  <summary>Details</summary>
Motivation: 信用卡欺诈检测中的类别不平衡问题直接影响金融系统预测的可靠性，传统采样技术可能引入偏差或信息损失，需要一种既能保持高准确率又能提供可解释性的解决方案。

Method: 采用可解释提升机(EBM)作为基础模型，通过系统超参数调优、特征选择和预处理优化。使用田口方法优化数据缩放器序列和模型超参数，避免传统采样技术，实现准确率与可解释性的平衡。

Result: 在基准信用卡数据集上获得ROC-AUC 0.983，超越了先前EBM基线(0.975)，并优于逻辑回归、随机森林、XGBoost和决策树模型，实现了稳健、可重复的系统性能提升。

Conclusion: 研究表明，可解释机器学习与数据驱动优化的结合在金融系统可信欺诈分析中具有重要潜力，为实际应用提供了既准确又可解释的解决方案。

Abstract: Addressing class imbalance is a central challenge in credit card fraud detection, as it directly impacts predictive reliability in real-world financial systems. To overcome this, the study proposes an enhanced workflow based on the Explainable Boosting Machine (EBM)-a transparent, state-of-the-art implementation of the GA2M algorithm-optimized through systematic hyperparameter tuning, feature selection, and preprocessing refinement. Rather than relying on conventional sampling techniques that may introduce bias or cause information loss, the optimized EBM achieves an effective balance between accuracy and interpretability, enabling precise detection of fraudulent transactions while providing actionable insights into feature importance and interaction effects. Furthermore, the Taguchi method is employed to optimize both the sequence of data scalers and model hyperparameters, ensuring robust, reproducible, and systematically validated performance improvements. Experimental evaluation on benchmark credit card data yields an ROC-AUC of 0.983, surpassing prior EBM baselines (0.975) and outperforming Logistic Regression, Random Forest, XGBoost, and Decision Tree models. These results highlight the potential of interpretable machine learning and data-driven optimization for advancing trustworthy fraud analytics in financial systems.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [199] [Identification and Estimation of Network Models with Nonparametric Unobserved Heterogeneity](https://arxiv.org/abs/2602.06885)
*Andrei Zeleneev*

Main category: econ.EM

TL;DR: 该论文提出了一种处理网络模型中不可观测异质性（潜在同质性）的方法，通过使用交互结果来识别具有相同固定效应的个体，从而在控制固定效应的情况下识别协变量的影响。


<details>
  <summary>Details</summary>
Motivation: 网络中的同质性现象普遍存在，基于可观测变量的同质性已被广泛研究，但基于不可观测变量（固定效应）的同质性同样重要。如果未能正确处理潜在同质性和其他复杂形式的不可观测异质性，会导致估计量不一致和政策含义误导。现有方法通常对固定效应的作用形式做出特定假设，这可能不切实际。

Method: 提出了一种具有非参数不可观测异质性的网络模型，不指定固定效应的具体作用形式。核心思想是利用交互结果来识别具有相同固定效应值的个体。通过分析这些个体在可观测特征上的变异，可以在控制固定效应的同时识别协变量的影响。基于这一思想构建了多个感兴趣参数的估计量，并刻画了它们的大样本性质。

Result: 数值实验证明了所提出方法的有效性，并支持了渐近理论。该方法能够在存在不可观测异质性的情况下，一致地估计网络模型中协变量的影响。

Conclusion: 该研究提供了一种处理网络模型中不可观测异质性的新框架，避免了传统方法对固定效应函数形式的限制性假设。所提出的估计方法具有理论保证，并在数值实验中表现出良好性能，为网络数据分析提供了更稳健的工具。

Abstract: Homophily based on observables is widespread in networks. Therefore, homophily based on unobservables (fixed effects) is also likely to be an important determinant of the interaction outcomes. Failing to properly account for latent homophily (and other complex forms of unobserved heterogeneity) can result in inconsistent estimators and misleading policy implications. To address this concern, we consider a network model with nonparametric unobserved heterogeneity, leaving the role of the fixed effects unspecified. We argue that the interaction outcomes can be used to identify agents with the same values of the fixed effects. The variation in the observed characteristics of such agents allows us to identify the effects of the covariates, while controlling for the fixed effects. Building on these ideas, we construct several estimators of the parameters of interest and characterize their large sample properties. Numerical experiments illustrate the usefulness of the suggested approaches and support the asymptotic theory.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [200] [Optimal Control Strategies for Epidemic Dynamics: Integrating SIR-SI and Lotka--Volterra Models](https://arxiv.org/abs/2602.06178)
*Rocio Balderrama,Ignacio Ceresa Dussel,Constanza Sanchez de la Vega*

Main category: math.OC

TL;DR: 提出一个结合传染病动力学（SIR-SI）与捕食者-猎物生态相互作用（Lotka-Volterra）的数学模型，分析天敌作为生物控制机制如何调节病媒种群和疾病传播，并基于捕食者释放制定最优控制策略。


<details>
  <summary>Details</summary>
Motivation: 研究如何利用自然捕食者作为生物控制机制来调节病媒种群，从而控制媒介传播疾病的传播。探索在自然控制不足时，通过人工释放捕食者的最优干预策略。

Method: 建立整合SIR-SI传染病模型和Lotka-Volterra捕食者-猎物生态相互作用的数学模型。引入生态繁殖数概念，分析捕食者-猎物周期振幅与疾病持续性的关系。在自然控制不足时，基于捕食者释放制定最优控制问题，使用庞特里亚金极大值原理求解最小化感染个体累计数和干预成本、最大化易感宿主种群的最优策略。

Result: 数值模拟验证了模型的有效性，表明外部干预能够减轻流行病峰值，并使系统稳定于生物种群的自然振荡。生态繁殖数作为阈值，将捕食者-猎物周期振幅与疾病持续性联系起来，显示自然控制关键取决于最大病媒密度与最小捕食者密度的比值。

Conclusion: 自然捕食者可作为有效的生物控制机制调节病媒种群和疾病传播。在自然控制不足时，基于最优控制理论的捕食者释放策略能够有效减轻流行病影响并稳定生态系统。该模型为媒介传播疾病的生态控制提供了数学框架。

Abstract: In this work we present a mathematical model that integrates the epidemiological dynamics of a vector-borne disease (SIR-SI) with Lotka Volterra predator prey ecological interactions. The study analyzes how the presence of natural predators acts as a biological control mechanism to regulate the vector population and, consequently, disease transmission in host. 
We introduce the concept of the ecological reproduction number, a threshold that links the amplitude of predator prey cycles to disease persistence, showing that natural control depends critically on the ratio between the maximum vector density and the minimum predator density. In scenarios where natural control is insufficient, we formulate an optimal control problem based on the release of predators. Using the Pontryagin Maximum Principle, we characterize the optimal strategy that minimizes the cumulative number of infected individuals and intervention costs, while simultaneously maximizing the susceptible host population at the end of the time horizon. Numerical simulations validate the effectiveness of the model, showing that external intervention mitigates the epidemic peak and stabilizes the system against the natural oscillations of biological populations.

</details>


### [201] [Process-Based Lagrange Multipliers for Nonconvex Set-Valued Optimization](https://arxiv.org/abs/2602.06186)
*Fernando García-Castaño,Miguel Ángel Melguizo-Padial*

Main category: math.OC

TL;DR: 本文为非凸集值优化问题建立了基于闭凸过程的拉格朗日乘子理论，在Lipschitz型正则性条件下推广了分离原理，无需凸性或可微性假设。


<details>
  <summary>Details</summary>
Motivation: 传统拉格朗日乘子理论依赖于凸性和可微性假设，限制了在非凸集值优化问题中的应用。本文旨在建立更一般的几何框架，使用闭凸过程作为广义拉格朗日乘子，扩展分离原理到非凸和非光滑情形。

Method: 引入闭凸过程（其图像为闭凸锥的集值映射）作为广义拉格朗日乘子，在Lipschitz正则性、有序锥有界基存在性和非退化条件下建立乘子存在性理论。在标量情形，建立了乘子过程与下半连续次线性函数的一一对应关系。

Result: 证明了在可验证假设下乘子过程的存在性，这些乘子保持全局最优性：原始问题的非支配解（或极小解）在惩罚问题中仍保持非支配性（或极小性）。在标量情形获得了精确惩罚公式，无需额外约束规格。

Conclusion: 闭凸过程为处理非凸集值优化问题提供了有效的广义拉格朗日乘子框架，扩展了分离原理的应用范围。有序锥的内部性条件虽充分但非必要，该理论可应用于集值向量平衡问题。

Abstract: We develop a Lagrange multiplier theory for nonconvex set-valued optimization problems under Lipschitz-type regularity conditions. Instead of classical continuous linear functionals, we introduce closed convex processes -- set-valued mappings whose graphs are closed convex cones -- as generalized Lagrange multipliers. This geometric framework extends separation principles beyond convexity and differentiability. We establish the existence of multiplier processes under verifiable assumptions, including Lipschitz regularity at a reference point, the existence of a bounded base of the ordering cone, and a nondegeneracy condition ensuring proper isolation of optimal values. These processes preserve global optimality: nondominated (respectively, minimal) solutions of the primal problem remain nondominated (respectively, minimal) in the penalized problem. In the scalar case, we obtain a one-to-one correspondence between multiplier processes and lower semicontinuous sublinear functions, yielding exact penalty formulations without additional constraint qualifications. An infinite-dimensional example shows that interiority conditions on the ordering cone, while sufficient, are not necessary. Applications to set-valued vector equilibrium problems are also discussed.

</details>


### [202] [Predictive Energy Management for Hybrid Powertrains](https://arxiv.org/abs/2602.06277)
*Satish Vedula,Olugbenga Anubi*

Main category: math.OC

TL;DR: 提出一种考虑电池退化的混合动力系统能量管理策略，通过分布式模型预测控制协调功率分配，在提高效率的同时降低电池退化风险。


<details>
  <summary>Details</summary>
Motivation: 混合动力系统使用多种能源和电池储能系统，电池退化风险和储能元件可靠性是设计高效混合动力系统的主要挑战。现有方法难以将复杂的电池退化模型直接整合到实时优化问题中。

Method: 采用电池绝对功率提取作为退化启发式指标，开发分布式模型预测控制策略，协调发动机和电池组之间的功率分配，在最大化效率的同时缓解电池退化风险。

Result: 通过对混合道路车辆（HEV）、混合水面船舶（DPS）和混合航空器（HEA）三种不同类型混合动力系统的真实仿真，验证了该能量管理策略在控制电池退化方面的有效性。

Conclusion: 提出的能量管理策略能够有效平衡混合动力系统的效率和电池退化风险，为不同类型混合动力系统的电池寿命管理提供了实用解决方案。

Abstract: Hybrid power trains (HPT) run on multiple energy sources, often involving energy storage systems/batteries (ESS). As a result, the risk of battery degradation and the reliability of energy storage elements pose a major challenge in designing an energy-efficient hybrid power train. This paper presents an energy management strategy that adaptively splits power demand between the engine and the battery pack in a hybrid power train taking into account the battery degradation. Incorporating the battery degradation model directly into the underlying optimization problem is challenging on multiple fronts: 1) Any reasonable degradation model will, due to its complexity, result in a complicated optimization problem that is impractical for real-time implementation 2) the models contain a lot of time-varying parameters that can only be determined through destructive experimental procedures. As a result, it is essential to devise heuristics that reasonably capture the degradation per usage of the batteries. One such heuristic considered in this paper is the absolute power extracted from the battery. A distributed model predictive strategy is then developed to coordinate the power split to maximize efficiency while mitigating the failure risk due to battery degradation. The designed EM strategy is demonstrated through a realistic simulation of three different hybrid power trains: hybrid road vehicles (for example: a hybrid electric vehicle (HEV)), hybrid surface vehicles (for example: dynamically positioned hybrid ships (DPS)), and hybrid aerial vehicles (for example: hybrid electric aircraft (HEA)). The results show the effectiveness of the energy management strategy in managing battery degradation.

</details>


### [203] [D-ripALM: A Tuning-friendly Decentralized Relative-Type Inexact Proximal Augmented Lagrangian Method](https://arxiv.org/abs/2602.06398)
*Jiayi Zhu,Hong Wang,Ling Liang,Lei Yang*

Main category: math.OC

TL;DR: D-ripALM是一种去中心化相对型不精确近端增广拉格朗日方法，用于多智能体网络的共识凸优化，采用双层分布式优化框架，具有调参友好性和数值鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有双层分布式增广拉格朗日方法在切换内外层迭代时缺乏有效的误差控制机制，导致算法调参困难且数值鲁棒性不足。需要一种更实用、调参友好的分布式优化框架来处理光滑和非光滑目标函数。

Method: 提出D-ripALM方法：采用去中心化相对型不精确近端增广拉格朗日框架，使用双层分布式优化结构，内层可容纳多种求解器。关键创新是采用相对型误差准则来调控内外层迭代切换，使算法更具实用性和调参友好性。

Result: 在一般凸性假设下建立了严格的收敛保证，无需文献中常见的光滑性或强凸性条件。数值实验表明D-ripALM具有调参友好特性，能以更少的通信轮次获得高精度解。

Conclusion: D-ripALM为多智能体网络共识凸优化提供了一个实用、调参友好且数值鲁棒的分布式优化框架，在一般凸性条件下具有收敛保证，适用于光滑和非光滑目标函数。

Abstract: This paper proposes D-ripALM, a Decentralized relative-type inexact proximal Augmented Lagrangian Method for consensus convex optimization over multi-agent networks. D-ripALM adopts a double-loop distributed optimization framework that accommodates a wide range of inner solvers, enabling efficient treatment of both smooth and nonsmooth objectives. In contrast to existing double-loop distributed augmented Lagrangian methods, D-ripALM employs a relative-type error criterion to regulate the switching between inner and outer iterations, resulting in a more practical and tuning-friendly algorithmic framework with enhanced numerical robustness. Moreover, we establish rigorous convergence guarantees for D-ripALM under general convexity assumptions, without requiring smoothness or strong convexity conditions commonly imposed in the distributed optimization literature. Numerical experiments further demonstrate the tuning-friendly nature of D-ripALM and its efficiency in attaining high-precision solutions with fewer communication rounds.

</details>


### [204] [Pointwise Tracking Optimal Control Problem for Cahn Hilliard Navier Stokes system](https://arxiv.org/abs/2602.06447)
*Sheetal Dharmatti,Greeshma K*

Main category: math.OC

TL;DR: 研究二维局部Cahn-Hilliard-Navier-Stokes系统的点态跟踪最优控制问题，其中Cahn-Hilliard方程中的源项作为控制，成本函数测量相变量在有限空间点集随时间与期望值的偏差。


<details>
  <summary>Details</summary>
Motivation: 该研究针对两种不混溶、不可压缩流体的演化模型，反映了实际应用中只有有限数量传感器可用的情况，需要处理成本函数因状态变量点态评估而导致的低正则性问题。

Method: 使用Cahn-Hilliard方程中的源项作为控制，建立点态跟踪成本函数；采用转置方法定义伴随系统来表征最优控制；证明强解存在性、最优控制存在性以及控制到状态映射的可微性。

Result: 证明了强解的存在性、最优控制的存在性、控制到状态映射的可微性；推导了基于伴随系统的一阶必要最优性条件；分析可扩展到奇异势能情况。

Conclusion: 成功解决了二维局部Cahn-Hilliard-Navier-Stokes系统的点态跟踪最优控制问题，建立了完整的数学理论框架，包括存在性、可微性和最优性条件，并可推广到奇异势能情形。

Abstract: We study a pointwise tracking optimal control problem for the two-dimensional local Cahn Hilliard Navier Stokes system, which models the evolution of two immiscible, incompressible fluids. The source term in the Cahn Hilliard equation acts as a control, and the cost functional measures the deviation of the phase variable from desired values at a finite set of spatial points over time. This setting reflects realistic applications where only a limited number of sensors are available. We also study a variant of the above pointwise tracking control problem where the cost is incorporated with a terminal time pointwise tracking term. The main mathematical difficulty arises from the low regularity of the cost functional due to the pointwise evaluation of the state variables. We prove the existence of strong solutions, establish the existence of an optimal control, and the differentiability of the control to state mapping. We define the adjoint system using a transposition method to characterise optimal control. Moreover, a first-order necessary optimality condition is derived in terms of the adjoint for both problems. Furthermore, we prove that our analysis can be extended to the case of singular potentials.

</details>


### [205] [Distributed Circumferential Coverage Control in Non-Convex Annulus Environments](https://arxiv.org/abs/2602.06472)
*Chao Zhai*

Main category: math.OC

TL;DR: 提出了一种分布式圆周覆盖控制方法，用于非凸环形环境的多智能体覆盖，同时实现工作量均衡分配


<details>
  <summary>Details</summary>
Motivation: 解决多智能体系统中非凸环形环境分布式覆盖的挑战，同时确保智能体间工作量均衡

Method: 构建黎曼度量用于非凸子区域导航并避免与区域边界碰撞；设计分布式分区律，通过虚拟分区条在覆盖区域内边界滑动来平衡工作量

Result: 理论分析证明了工作量分区的指数收敛性和每个智能体在其子区域内局部最优的渐近收敛性；案例研究验证了方法的有效性

Conclusion: 提出的分布式圆周覆盖控制方法能有效解决非凸环形环境的覆盖问题，实现工作量均衡分配

Abstract: It has long been a prominent challenge in multi-agent systems to achieve distributed coverage of non-convex annulus environments while ensuring workload equalization among agents. To address this challenge, a distributed circumferential coverage control formulation is developed in this note by constructing a Riemannian metric for the navigation in the non-convex subregion while avoiding collisions with the region boundary. In addition, a distributed partition law is designed to balance the workload on the entire coverage region by endowing each agent with a virtual partition bar that slides along the inner boundary of coverage region. Theoretical analysis is conducted to ensure the exponential convergence of workload partition and asymptotic convergence of each agent towards the local optimum in its subregion. Finally, a case study is presented to demonstrate the effectiveness of the proposed coverage control approach.

</details>


### [206] [Approximating the Uniform Value in Hidden Stochastic Games with Doeblin Conditions](https://arxiv.org/abs/2602.06480)
*Krishnendu Chatterjee,David Lurie,Raimundo Saona,Bruno Ziliotto*

Main category: math.OC

TL;DR: 本文研究零和双人隐藏随机博弈，证明了在Doeblin条件下均匀值的存在性，并提供了近似算法。在盲设置中需要遍历性，在隐藏设置中需要本原性。


<details>
  <summary>Details</summary>
Motivation: 在零和双人隐藏随机博弈中，玩家只能观察到部分状态信息。先前研究表明，一般情况下均匀值可能不存在，即使存在也可能无法计算或近似。因此需要寻找充分条件来保证均匀值的存在性和可计算性。

Method: 引入Doeblin条件：要求经过足够长时间后，后验信念以均匀正概率重置到信念空间中的有限多个邻域。在盲设置中需要遍历性，在隐藏设置中需要本原性。基于这些条件证明均匀值存在性并设计近似算法。

Result: 证明了在Doeblin条件下均匀值的存在性，并提供了近似算法。发现在盲设置中遍历性足够，但在隐藏设置中需要更强的本原性条件，且遍历性不能保证Doeblin条件。

Conclusion: Doeblin条件为零和双人隐藏随机博弈中均匀值的存在性和可计算性提供了充分条件。结果对单玩家设置（部分可观察马尔可夫决策过程）也是新的。

Abstract: In \emph{zero-sum two-player hidden stochastic games}, players observe partial information about the state. We address: $(i)$ the existence of the \emph{uniform value}, i.e., a limiting average payoff that both players can guarantee for sufficiently long durations, and $(ii)$ the existence of an algorithm to approximate it. Previous work shows that, in the general case, the uniform value may fail to exist, and, even when it does, there need not exist an algorithm to compute or approximate it. Therefore, we consider the \emph{Doeblin condition} in hidden stochastic games, requiring that, after a sufficiently long time, the posterior beliefs have a uniformly positive probability of resetting to one of finitely many neighborhoods in the belief space. We prove the existence of the uniform value and provide an algorithm to approximate it. We identify sufficient conditions, namely \emph{ergodicity} in the blind setting (when the signal is uninformative) and \emph{primitivity} in the hidden setting (when there are multiple signals). Moreover, we show that, in the hidden setting, ergodicity does not guarantee the Doeblin condition. Our results are new even for the one-player setting, i.e., partially observable Markov decision processes.

</details>


### [207] [Markov Decision Processes of the Third Kind: Learning Distributions by Policy Gradient Descent](https://arxiv.org/abs/2602.06567)
*Nicole Bäuerle,Athanasios Vasileiadis*

Main category: math.OC

TL;DR: 本文提出了一种基于策略梯度的算法，用于解决分布马尔可夫决策过程问题，旨在学习能够引导累积奖励分布匹配目标分布的策略，而非优化期望值或风险函数。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习主要关注优化期望值或风险函数，但许多实际应用需要控制输出分布的形状而不仅仅是统计量。本文旨在解决分布控制问题，即学习策略使累积奖励的分布匹配给定的目标分布。

Method: 提出基于神经网络的随机化马尔可夫策略参数化方法，定义在增强状态空间上，使用特征函数损失的样本评估，开发策略梯度算法。在温和的正则性和增长假设下，使用随机逼近技术证明算法收敛到平稳点。

Result: 数值实验表明该方法能够匹配复杂目标分布，恢复经典最优策略（当存在时），并揭示分布控制特有的内在非唯一性现象。算法在模型无关设置下有效解决分布控制问题。

Conclusion: 分布马尔可夫决策过程为控制问题提供了新视角，提出的策略梯度算法能够有效学习匹配目标分布的策略，揭示了分布控制特有的非唯一性现象，为分布导向的控制任务提供了实用解决方案。

Abstract: The goal of this paper is to analyze distributional Markov Decision Processes as a class of control problems in which the objective is to learn policies that steer the distribution of a cumulative reward toward a prescribed target law, rather than optimizing an expected value or a risk functional. To solve the resulting distributional control problem in a model-free setting, we propose a policy-gradient algorithm based on neural-network parameterizations of randomized Markov policies, defined on an augmented state space and a sample-based evaluation of the characteristic-function loss. Under mild regularity and growth assumptions, we prove convergence of the algorithm to stationary points using stochastic approximation techniques. Several numerical experiments illustrate the ability of the method to match complex target distributions, recover classical optimal policies when they exist, and reveal intrinsic non-uniqueness phenomena specific to distributional control.

</details>


### [208] [Two-stage stochastic algorithm for solving large-scale (non)-convex separable optimization problems under affine constraints](https://arxiv.org/abs/2602.06637)
*Benjamin Dubois-Taine,Laurent Pfeiffer,Nadia Oudjane,Adrien Seguret,Francis Bach*

Main category: math.OC

TL;DR: 提出一种两阶段算法，结合对偶问题的随机次梯度方法和块坐标Frank-Wolfe方法，显著减少Fenchel共轭计算次数，从O(N/ε²)降低到O(1/ε² + N/ε^{2/3})


<details>
  <summary>Details</summary>
Motivation: 传统对偶次梯度方法（对偶分解）在求解具有大量代理的仿射约束非光滑优化问题时，每个迭代需要计算所有N个代理的Fenchel共轭，导致O(N/ε²)的复杂度，这在实践中可能过高

Method: 提出两阶段算法：第一阶段在对偶问题上使用随机次梯度方法；第二阶段使用块坐标Frank-Wolfe方法获得原始解。该方法将Fenchel共轭计算从每次迭代N次减少到更少的次数

Result: 在凸情况下，仅需O(1/ε² + N/ε^{2/3})次Fenchel共轭调用即可获得期望ε-最优原始解。扩展到非凸分量函数时，仍能获得近似原始解，恢复经典的Shapley-Folkman定理对偶间隙界

Conclusion: 该方法显著降低了大规模分布式优化问题的计算复杂度，特别是在代理数量N很大时，将Fenchel共轭计算从线性依赖N减少到亚线性依赖，同时适用于凸和非凸问题

Abstract: We consider nonsmooth optimization problems under affine constraints, where the objective consists of the average of the component functions of a large number $N$ of agents, and we only assume access to the Fenchel conjugate of the component functions. The algorithm of choice for solving such problems is the dual subgradient method, also known as dual decomposition, which requires $O(\frac{1}{ε^2})$ iterations to reach $ε$-optimality in the convex case. However, each iteration requires computing the Fenchel conjugate of each of the $N$ agents, leading to a complexity $O(\frac{N}{ε^2})$ which might be prohibitive in practical applications. To overcome this, we propose a two-stage algorithm, combining a stochastic subgradient algorithm on the dual problem, followed by a block-coordinate Frank-Wolfe algorithm to obtain primal solutions. The resulting algorithm requires only $O(\frac{1}{ε^2} + \frac{N}{ε^{2/3}})$ calls to Fenchel conjugates to obtain an $ε$-optimal primal solution in expectation in the convex case. We extend our results to nonconvex component functions and show that our method still applies and gets (almost) the same convergence rate, this time only to an approximate primal solution recovering the classical duality gap bounds usually obtained using the Shapley-Folkman theorem.

</details>


### [209] [Optimization-based control by interconnection of nonlinear port-Hamiltonian systems](https://arxiv.org/abs/2602.06670)
*Hannes Gernandt,Till Preuster,Manuel Schaller*

Main category: math.OC

TL;DR: 提出一种基于优化的控制互连方法，将非线性端口哈密顿系统的MPC反馈设计为端口哈密顿系统的结构保持互连


<details>
  <summary>Details</summary>
Motivation: 针对非线性端口哈密顿系统的稳定问题，结合模型预测控制思想，寻求一种结构保持的控制设计方法

Method: 将有限时域最优控制问题的原始-对偶梯度动力学表示为端口哈密顿系统，然后与受控对象进行结构保持互连

Result: 在可观测性假设下，证明了互连系统能渐近稳定受控对象动力学

Conclusion: 提出的MPC型反馈实现了端口哈密顿系统的结构保持互连，为非线性端口哈密顿系统提供了一种有效的稳定控制方法

Abstract: In this paper, we formulate an optimization-based control-by-interconnection approach to the stabilization problem of nonlinear port-Hamiltonian systems. Motivated by model predictive control, the feedback is defined as an initial part of a suboptimal solution of a finite horizon optimal control problem. To this end, we write the optimization method given by a primal-dual gradient dynamics arising from a possibly control-constrained optimal control problem as a port-Hamiltonian system. Then, using the port-Hamiltonian structure of the plant, we show that the MPC-type feedback law is indeed a structure-preserving interconnection of two port-Hamiltonian systems. We prove that, under an observability assumption, the interconnected system asymptotically stabilizes the plant dynamics. We illustrate the theoretical results by means of a numerical example.

</details>


### [210] [Wasserstein Distributionally Robust Performative Prediction](https://arxiv.org/abs/2602.06730)
*Siyi Wang,Zifan Wang,Karl H. Johansson*

Main category: math.OC

TL;DR: 提出基于Wasserstein分布鲁棒优化的执行预测框架，通过模型依赖的模糊集半径处理策略性分布偏移，开发DR-RRM和DR-RGD算法收敛到鲁棒执行稳定点


<details>
  <summary>Details</summary>
Motivation: 执行预测中模型部署会激励代理策略性调整行为，导致模型依赖的分布偏移。实践中常通过重复训练适应演化分布，但需要鲁棒方法处理这种分布不确定性

Method: 建立Wasserstein分布鲁棒优化框架，允许模糊集半径依赖预测模型。利用强对偶性将鲁棒目标重构为可计算的最小化问题，开发DR-RRM和DR-RGD算法迭代平衡分布偏移和模型重训练

Result: 理论分析表明在标准正则条件下，两种算法收敛到唯一鲁棒执行稳定点。考虑内环近似误差时收敛到稳定点邻域，建立了稳定点与全局执行最优解之间的次优性界限。动态信用评分数值模拟验证方法有效性

Conclusion: 提出的分布鲁棒执行预测框架能有效处理策略性分布偏移，算法理论保证收敛性，为实际应用提供鲁棒解决方案

Abstract: Performativity means that the deployment of a predictive model incentivizes agents to strategically adapt their behavior, thereby inducing a model-dependent distribution shift. Practitioners often repeatedly retrain the model on data samples to adapt to evolving distributions. In this paper, we develop a Wasserstein distributionally robust optimization framework for performative prediction, where the prediction model is optimized over the worst-case distribution within a Wasserstein ambiguity set. We allow the ambiguity radius to depend on the prediction model, which subsumes the constant-radius formulation as a special case. By leveraging strong duality, the intractable robust objective is reformulated as a computationally tractable minimization problem. Based on this formulation, we develop distributionally robust repeated risk minimization (DR-RRM) and repeated gradient descent (DR-RGD), to iteratively find an equilibrium between distributional shifts and model retraining. Theoretical analyses demonstrate that, under standard regularity conditions, both algorithms converge to a unique robust performative stable point. Our analysis explicitly accounts for inner-loop approximation errors and shows convergence to a neighborhood of the stable point in inexact settings. Additionally, we establish theoretical bounds on the suboptimality gap between the stable point and the global performative optimum. Finally, numerical simulations of a dynamic credit scoring problem demonstrate the efficacy of the method.

</details>


### [211] [Convergence Rates for Stochastic Proximal and Projection Estimators](https://arxiv.org/abs/2602.06750)
*Diego Morales,Pedro Pérez-Aros,Emilio Vilches*

Main category: math.OC

TL;DR: 本文为随机平滑近似下确界卷积建立了显式的非渐近收敛率，量化了重心估计子向邻近算子和度量投影的收敛速度，证明了维度显式的√δ界，并在C²光滑条件下得到了改进的线性O(δ)率。


<details>
  <summary>Details</summary>
Motivation: 先前文献中引入了随机平滑近似下确界卷积，但缺乏对其收敛速度的定量分析。本文旨在为这些近似方法建立显式的非渐近收敛率，特别是在量化重心估计子向邻近算子和度量投影的收敛行为方面。

Method: 采用随机平滑近似方法处理下确界卷积，通过分析重心估计子的收敛性质，在ρ-弱凸（可能非光滑）设定下证明维度显式的√δ界，在C²光滑且Hessian全局Lipschitz条件下推导改进的线性O(δ)率。

Result: 证明了在ρ-弱凸设定下维度显式的√δ收敛界，并通过示例表明该阶是尖锐的；在C²光滑且Hessian全局Lipschitz条件下得到了改进的线性O(δ)率，并对局部C²¹边界凸集获得了精细的投影估计。

Conclusion: 本文为随机平滑近似下确界卷积建立了完整的收敛率理论框架，提供了从√δ到线性O(δ)的收敛速度分析，为相关优化算法的理论分析提供了重要工具。

Abstract: In this paper, we establish explicit, non-asymptotic convergence rates for the stochastic smooth approximations of infimal convolutions introduced and developed in \cite{MR4581306,MR4923371}. In particular, we quantify the convergence of the associated barycentric estimators toward proximal mappings and metric projections. We prove a dimension-explicit $\sqrtδ$ bound in the $ρ$-weakly convex (possibly nonsmooth) setting and show, by examples, that this order is sharp. Under additional regularity, namely $C^{2}$ smoothness with globally Lipschitz Hessian, we derive an improved linear $O(δ)$ rate with explicit constants, and we obtain refined projection estimates for convex sets with local $C^{2,1}$ boundary.

</details>


### [212] [RanSOM: Second-Order Momentum with Randomized Scaling for Constrained and Unconstrained Optimization](https://arxiv.org/abs/2602.06824)
*El Mahdi Chayti*

Main category: math.OC

TL;DR: RanSOM：通过随机化步长消除动量方法中的曲率偏差，实现最优收敛率


<details>
  <summary>Details</summary>
Motivation: 动量方法（如Polyak's Heavy Ball）在随机设置中受曲率诱导偏差影响，收敛率仅为次优的O(ε⁻⁴)，现有校正方法需要昂贵的辅助采样或严格的平滑性假设

Method: 提出RanSOM框架，将确定性步长替换为均值η_t的随机步长分布，利用Stein-type恒等式计算动量偏差的无偏估计，仅需单个Hessian-向量乘积与梯度联合计算，无需辅助查询。具体实现RanSOM-E（无约束优化，使用指数分布步长）和RanSOM-B（约束优化，使用beta分布步长保持可行性）

Result: 理论分析证实RanSOM在标准有界噪声下恢复最优O(ε⁻³)收敛率，在重尾噪声设置(p∈(1,2])下无需梯度裁剪即可达到最优率

Conclusion: RanSOM提供了一种高效消除动量偏差的统一框架，在保持动量方法优势的同时实现最优收敛性能

Abstract: Momentum methods, such as Polyak's Heavy Ball, are the standard for training deep networks but suffer from curvature-induced bias in stochastic settings, limiting convergence to suboptimal $\mathcal{O}(ε^{-4})$ rates. Existing corrections typically require expensive auxiliary sampling or restrictive smoothness assumptions. We propose \textbf{RanSOM}, a unified framework that eliminates this bias by replacing deterministic step sizes with randomized steps drawn from distributions with mean $η_t$. This modification allows us to leverage Stein-type identities to compute an exact, unbiased estimate of the momentum bias using a single Hessian-vector product computed jointly with the gradient, avoiding auxiliary queries. We instantiate this framework in two algorithms: \textbf{RanSOM-E} for unconstrained optimization (using exponentially distributed steps) and \textbf{RanSOM-B} for constrained optimization (using beta-distributed steps to strictly preserve feasibility). Theoretical analysis confirms that RanSOM recovers the optimal $\mathcal{O}(ε^{-3})$ convergence rate under standard bounded noise, and achieves optimal rates for heavy-tailed noise settings ($p \in (1, 2]$) without requiring gradient clipping.

</details>


### [213] [A Mode-Matching Approach to the Design of RIS-Aided Communications](https://arxiv.org/abs/2602.06840)
*Ahmed Najjar,Hajar El Hassani,Marco Di Renzo,Kezhi Wang,Merouane Debbah*

Main category: math.OC

TL;DR: 论文提出了一种基于电磁一致性的可重构智能表面设计方法，通过周期性表面阻抗边界建模，使用模式匹配和Floquet展开计算反射场，优化RIS辅助通信的性能与实现复杂度权衡。


<details>
  <summary>Details</summary>
Motivation: 可重构智能表面是无线通信中的新兴技术，但现有设计方法缺乏电磁一致性。需要一种能够准确建模RIS反射特性，同时优化性能与实现复杂度的方法。

Method: 将RIS建模为周期性表面阻抗边界，采用模式匹配方法和Floquet展开表示来计算空间周期性RIS的反射场。基于全局设计准则评估RIS辅助通信的性能与实现复杂度权衡。

Result: 该方法能够最大化向预期传播方向反射的功率，同时最小化向非期望方向再辐射的功率。提出的电磁一致性方法为RIS辅助无线系统设计提供了优势。

Conclusion: 论文提出的电磁一致性RIS设计方法能够有效优化异常反射性能，在保证通信性能的同时控制实现复杂度，为RIS辅助无线系统提供了更准确的设计框架。

Abstract: Reconfigurable intelligent surface (RIS) is an emerging technology for application to wireless communications. In this paper, we consider the problem of anomalous reflection and model the RIS as a periodic surface impedance boundary. We utilize the mode matching method and Floquets expansion representation to compute the field reflected from a spatially periodic RIS, and evaluate the performance versus implementation complexity tradeoffs of RIS aided communications based on the global design criterion. This allows us to maximize the power reflected towards the intended direction of propagation, while minimizing the power reradiated towards undesired directions of propagation. In addition, we discuss the advantages of the proposed electromagnetically consistent approach to the design of RIS aided wireless systems.

</details>


### [214] [Circuit Diameter of Polyhedra is Strongly Polynomial](https://arxiv.org/abs/2602.06958)
*Bento Natura*

Main category: math.OC

TL;DR: 证明了多面体电路直径的强多项式上界O(m² log m)，解决了多项式Hirsch猜想的电路类比


<details>
  <summary>Details</summary>
Motivation: 电路直径是组合直径的自然松弛，允许沿电路方向移动而不仅仅是沿边移动。之前的上界都是弱多项式的，找到匹配此上界的电路增广算法将解决Smale第9个问题（线性规划的强多项式时间算法）

Method: 通过构造性证明，展示了多面体P = {x∈ℝⁿ: Ax = b, x ≥ 0}的电路直径上界为O(m² log m)，其中A∈ℝ^{m×n}。构造产生单调电路行走

Result: 证明了电路直径的强多项式上界O(m² log m)，同样适用于单调电路直径。这是电路直径的第一个强多项式上界

Conclusion: 解决了多项式Hirsch猜想的电路类比，为寻找匹配此上界的电路增广算法提供了理论基础，这可能最终导致线性规划的强多项式时间算法

Abstract: We prove a strongly polynomial bound on the circuit diameter of polyhedra, resolving the circuit analogue of the polynomial Hirsch conjecture. Specifically, we show that the circuit diameter of a polyhedron $P = \{x\in \mathbb{R}^n:\, A x = b, \, x \ge 0\}$ with $A\in\mathbb{R}^{m\times n}$ is $O(m^2 \log m)$. Our construction yields monotone circuit walks, giving the same bound for the monotone circuit diameter.
  The circuit diameter, introduced by Borgwardt, Finhold, and Hemmecke (SIDMA 2015), is a natural relaxation of the combinatorial diameter that allows steps along circuit directions rather than only along edges. All prior upper bounds on the circuit diameter were only weakly polynomial. Finding a circuit augmentation algorithm that matches this bound would yield a strongly polynomial time algorithm for linear programming, resolving Smale's 9th problem.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [215] [Jackpot: Optimal Budgeted Rejection Sampling for Extreme Actor-Policy Mismatch Reinforcement Learning](https://arxiv.org/abs/2602.06107)
*Zhuoming Chen,Hongyi Liu,Yang Zhou,Haizhong Zheng,Beidi Chen*

Main category: cs.AI

TL;DR: Jackpot框架通过最优预算拒绝采样减少rollout模型与策略之间的分布不匹配，实现更高效的LLM强化学习


<details>
  <summary>Details</summary>
Motivation: LLM的强化学习成本高昂，特别是rollout阶段。解耦rollout生成与策略优化可提高效率，但会引入严重的分布不匹配问题，导致学习不稳定

Method: 提出Jackpot框架，采用最优预算拒绝采样直接减少rollout模型与演化策略之间的差异。包括原则性的OBRS程序、联合更新策略和rollout模型的统一训练目标，以及基于top-k概率估计和批量级偏差校正的高效系统实现

Result: 理论分析显示OBRS在可控接受预算下持续使rollout分布更接近目标分布。实验表明相比重要性采样基线显著提高训练稳定性，在Qwen3-8B-Base上训练300步（批量64）时达到与在线RL相当的性能

Conclusion: OBRS-based alignment使解耦rollout生成与策略优化更接近实用和有效，为LLM的RL提供了更可行的解决方案

Abstract: Reinforcement learning (RL) for large language models (LLMs) remains expensive, particularly because the rollout is expensive. Decoupling rollout generation from policy optimization (e.g., leveraging a more efficient model to rollout) could enable substantial efficiency gains, yet doing so introduces a severe distribution mismatch that destabilizes learning. We propose Jackpot, a framework that leverages Optimal Budget Rejection Sampling (OBRS) to directly reduce the discrepancy between the rollout model and the evolving policy. Jackpot integrates a principled OBRS procedure, a unified training objective that jointly updates the policy and rollout models, and an efficient system implementation enabled by top-$k$ probability estimation and batch-level bias correction. Our theoretical analysis shows that OBRS consistently moves the rollout distribution closer to the target distribution under a controllable acceptance budget. Empirically, \sys substantially improves training stability compared to importance-sampling baselines, achieving performance comparable to on-policy RL when training Qwen3-8B-Base for up to 300 update steps of batchsize 64. Taken together, our results show that OBRS-based alignment brings us a step closer to practical and effective decoupling of rollout generation from policy optimization for RL for LLMs.

</details>


### [216] [Unlocking Noisy Real-World Corpora for Foundation Model Pre-Training via Quality-Aware Tokenization](https://arxiv.org/abs/2602.06394)
*Arvid E. Gollwitzer,Paridhi Latawa,David de Gruijl,Deepak A. Subramanian,Adrián Noriega de la Colina*

Main category: cs.AI

TL;DR: QA-Token：一种质量感知的分词方法，通过考虑数据可靠性来优化词汇表构建，在基因组学和金融等领域显著提升性能


<details>
  <summary>Details</summary>
Motivation: 当前的分词方法在处理序列数据时没有考虑信号质量，限制了它们在嘈杂的现实世界语料库上的有效性。需要一种能够直接整合数据可靠性的分词方法。

Method: 提出QA-Token（质量感知分词），包含三个关键贡献：(i) 双层优化公式，联合优化词汇表构建和下游性能；(ii) 强化学习方法，通过质量感知奖励学习合并策略，具有收敛保证；(iii) 通过Gumbel-Softmax松弛的自适应参数学习机制，实现端到端优化。

Result: 实验评估显示一致改进：基因组学（变异检测F1分数比BPE提高6.7个百分点）、金融（夏普比率提高30%）。在基础模型规模上，对1.7万亿碱基对的预训练语料进行分词，实现最先进的病原体检测（94.53 MCC），同时减少15%的token数量。

Conclusion: QA-Token解锁了嘈杂的现实世界语料库（包括petabases的基因组序列和terabytes的金融时间序列），用于基础模型训练，且推理时无额外开销。

Abstract: Current tokenization methods process sequential data without accounting for signal quality, limiting their effectiveness on noisy real-world corpora. We present QA-Token (Quality-Aware Tokenization), which incorporates data reliability directly into vocabulary construction. We make three key contributions: (i) a bilevel optimization formulation that jointly optimizes vocabulary construction and downstream performance, (ii) a reinforcement learning approach that learns merge policies through quality-aware rewards with convergence guarantees, and (iii) an adaptive parameter learning mechanism via Gumbel-Softmax relaxation for end-to-end optimization. Our experimental evaluation demonstrates consistent improvements: genomics (6.7 percentage point F1 gain in variant calling over BPE), finance (30% Sharpe ratio improvement). At foundation scale, we tokenize a pretraining corpus comprising 1.7 trillion base-pairs and achieve state-of-the-art pathogen detection (94.53 MCC) while reducing token count by 15%. We unlock noisy real-world corpora, spanning petabases of genomic sequences and terabytes of financial time series, for foundation model training with zero inference overhead.

</details>


### [217] [Large Language Model Reasoning Failures](https://arxiv.org/abs/2602.06176)
*Peiyang Song,Pengrui Han,Noah Goodman*

Main category: cs.AI

TL;DR: 该论文对LLM推理失败进行了首次全面调查，提出了新的分类框架，将推理分为具身与非具身类型，并将推理失败分为基础性、应用特定和鲁棒性三类，为理解LLM系统性弱点提供了结构化视角。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在各种任务上表现出色，但在看似简单的场景中仍存在显著的推理失败。为了系统性地理解和解决这些缺陷，需要对LLM推理失败进行全面调查和分类。

Method: 提出了新颖的分类框架：将推理分为具身推理和非具身推理，非具身推理进一步分为非正式（直觉）推理和正式（逻辑）推理。同时将推理失败分为三类：基础性失败（LLM架构固有）、应用特定限制（特定领域表现）、鲁棒性问题（微小变化导致不一致）。对每种失败类型提供定义、分析现有研究、探索根本原因并提出缓解策略。

Result: 建立了首个全面的LLM推理失败调查，提供了结构化视角来理解LLM的系统性弱点。创建了GitHub资源库（https://github.com/Peiyang-Song/Awesome-LLM-Reasoning-Failures），收集了相关研究工作，为该领域提供了便捷入口。

Conclusion: 该调查统一了分散的研究工作，为理解LLM推理失败提供了系统框架，为未来研究提供了宝贵见解，有助于构建更强、更可靠、更鲁棒的推理能力。

Abstract: Large Language Models (LLMs) have exhibited remarkable reasoning capabilities, achieving impressive results across a wide range of tasks. Despite these advances, significant reasoning failures persist, occurring even in seemingly simple scenarios. To systematically understand and address these shortcomings, we present the first comprehensive survey dedicated to reasoning failures in LLMs. We introduce a novel categorization framework that distinguishes reasoning into embodied and non-embodied types, with the latter further subdivided into informal (intuitive) and formal (logical) reasoning. In parallel, we classify reasoning failures along a complementary axis into three types: fundamental failures intrinsic to LLM architectures that broadly affect downstream tasks; application-specific limitations that manifest in particular domains; and robustness issues characterized by inconsistent performance across minor variations. For each reasoning failure, we provide a clear definition, analyze existing studies, explore root causes, and present mitigation strategies. By unifying fragmented research efforts, our survey provides a structured perspective on systemic weaknesses in LLM reasoning, offering valuable insights and guiding future research towards building stronger, more reliable, and robust reasoning capabilities. We additionally release a comprehensive collection of research works on LLM reasoning failures, as a GitHub repository at https://github.com/Peiyang-Song/Awesome-LLM-Reasoning-Failures, to provide an easy entry point to this area.

</details>


### [218] [Do It for HER: First-Order Temporal Logic Reward Specification in Reinforcement Learning (Extended Version)](https://arxiv.org/abs/2602.06227)
*Pierriccardo Olivieri,Fausto Lasca,Alessandro Gianola,Matteo Papini*

Main category: cs.AI

TL;DR: 提出基于LTLfMT的逻辑规范框架，用于大规模状态空间MDP中的非马尔可夫奖励规范，通过理论片段识别和基于奖励机器与HER的实践方法解决表达力增强带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统LTLf在表达复杂任务时存在局限性，特别是处理非结构化、异构数据域时，需要手动谓词编码。需要更强大的逻辑规范框架来统一和重用任务规范。

Method: 1) 使用LTLfMT（线性时序逻辑模理论）增强表达能力；2) 从理论上识别可处理的LTLfMT片段；3) 实践上采用奖励机器和Hindsight Experience Replay（HER）方法，将一阶逻辑规范转化为可学习的奖励信号。

Result: 在连续控制环境中使用非线性算术理论进行评估，表明该方法能够自然地规范复杂任务。实验结果显示定制的HER实现对于解决具有复杂目标的任务至关重要。

Conclusion: 提出的LTLfMT框架为大规模状态空间MDP中的非马尔可夫奖励规范提供了统一、可重用的解决方案，通过理论片段识别和HER方法平衡了表达力与计算可行性。

Abstract: In this work, we propose a novel framework for the logical specification of non-Markovian rewards in Markov Decision Processes (MDPs) with large state spaces. Our approach leverages Linear Temporal Logic Modulo Theories over finite traces (LTLfMT), a more expressive extension of classical temporal logic in which predicates are first-order formulas of arbitrary first-order theories rather than simple Boolean variables. This enhanced expressiveness enables the specification of complex tasks over unstructured and heterogeneous data domains, promoting a unified and reusable framework that eliminates the need for manual predicate encoding. However, the increased expressive power of LTLfMT introduces additional theoretical and computational challenges compared to standard LTLf specifications. We address these challenges from a theoretical standpoint, identifying a fragment of LTLfMT that is tractable but sufficiently expressive for reward specification in an infinite-state-space context. From a practical perspective, we introduce a method based on reward machines and Hindsight Experience Replay (HER) to translate first-order logic specifications and address reward sparsity. We evaluate this approach to a continuous-control setting using Non-Linear Arithmetic Theory, showing that it enables natural specification of complex tasks. Experimental results show how a tailored implementation of HER is fundamental in solving tasks with complex goals.

</details>


### [219] [Do LLMs Act Like Rational Agents? Measuring Belief Coherence in Probabilistic Decision Making](https://arxiv.org/abs/2602.06286)
*Khurram Yamin,Jingjing Tang,Santiago Cortes-Gomez,Amit Sharma,Eric Horvitz,Bryan Wilder*

Main category: cs.AI

TL;DR: 该研究检验大型语言模型是否作为理性的效用最大化者，通过医学诊断挑战问题分析其信念一致性和偏好稳定性，提出可证伪条件来判断模型报告概率是否对应真实信念。


<details>
  <summary>Details</summary>
Motivation: LLMs越来越多地被部署在需要根据世界不确定性和不同结果效用做出最优决策的高风险领域，但其决策逻辑难以解释。需要研究LLMs是否具有一致信念和稳定偏好的理性效用最大化者。

Method: 使用医学诊断挑战问题研究模型行为，提出可证伪条件来判断报告概率是否对应任何理性智能体的真实信念，并将该方法应用于多个医学诊断领域，评估多个LLMs。

Result: 研究结果提供了关于LLM推理与理想贝叶斯效用最大化之间关系的见解，通过引出的概率和观察到的行动进行分析。

Conclusion: 讨论了结果对LLMs在高风险决策指导中应用的影响，并提出了未来发展方向。

Abstract: Large language models (LLMs) are increasingly deployed as agents in high-stakes domains where optimal actions depend on both uncertainty about the world and consideration of utilities of different outcomes, yet their decision logic remains difficult to interpret. We study whether LLMs are rational utility maximizers with coherent beliefs and stable preferences. We consider behaviors of models for diagnosis challenge problems. The results provide insights about the relationship of LLM inferences to ideal Bayesian utility maximization for elicited probabilities and observed actions. Our approach provides falsifiable conditions under which the reported probabilities \emph{cannot} correspond to the true beliefs of any rational agent. We apply this methodology to multiple medical diagnostic domains with evaluations across several LLMs. We discuss implications of the results and directions forward for uses of LLMs in guiding high-stakes decisions.

</details>


### [220] [Exposing Weaknesses of Large Reasoning Models through Graph Algorithm Problems](https://arxiv.org/abs/2602.06319)
*Qifan Zhang,Jianhao Ruan,Aochuan Chen,Kang Zeng,Nuo Chen,Jing Tang,Jia Li*

Main category: cs.AI

TL;DR: GrAlgoBench是一个基于图算法问题的基准测试，用于评估大型推理模型，发现现有模型在长上下文推理中存在准确率下降和过度思考问题。


<details>
  <summary>Details</summary>
Motivation: 现有数学、代码和常识推理基准存在局限性：缺乏长上下文评估、挑战性不足、答案难以程序化验证。需要更严格的测试平台来评估大型推理模型的推理能力。

Method: 引入GrAlgoBench基准，使用图算法问题评估大型推理模型。图算法问题需要长上下文推理、允许精细控制难度、支持标准化程序化评估。包含9个任务，系统性地测试模型性能。

Result: 实验发现当前大型推理模型的两个主要弱点：1) 随着上下文长度增加，准确率急剧下降（图节点超过120个时准确率低于50%），主要由于执行错误、弱记忆和冗余推理；2) 存在过度思考现象，大量无效的自我验证增加了推理痕迹但没有提高正确性。

Conclusion: GrAlgoBench通过暴露大型推理模型的局限性，确立了图算法问题作为严格、多维且实际相关的测试平台，有助于推进大型推理模型的推理能力研究。

Abstract: Large Reasoning Models (LRMs) have advanced rapidly; however, existing benchmarks in mathematics, code, and common-sense reasoning remain limited. They lack long-context evaluation, offer insufficient challenge, and provide answers that are difficult to verify programmatically. We introduce GrAlgoBench, a benchmark designed to evaluate LRMs through graph algorithm problems. Such problems are particularly well suited for probing reasoning abilities: they demand long-context reasoning, allow fine-grained control of difficulty levels, and enable standardized, programmatic evaluation. Across nine tasks, our systematic experiments reveal two major weaknesses of current LRMs. First, accuracy deteriorates sharply as context length increases, falling below 50% once graphs exceed 120 nodes. This degradation is driven by frequent execution errors, weak memory, and redundant reasoning. Second, LRMs suffer from an over-thinking phenomenon, primarily caused by extensive yet largely ineffective self-verification, which inflates reasoning traces without improving correctness. By exposing these limitations, GrAlgoBench establishes graph algorithm problems as a rigorous, multidimensional, and practically relevant testbed for advancing the study of reasoning in LRMs. Code is available at https://github.com/Bklight999/GrAlgoBench.

</details>


### [221] [Trifuse: Enhancing Attention-Based GUI Grounding via Multimodal Fusion](https://arxiv.org/abs/2602.06351)
*Longhui Ma,Di Zhao,Siwei Wang,Zhao Lv,Miao Wang*

Main category: cs.AI

TL;DR: Trifuse是一个基于注意力的GUI grounding框架，通过整合OCR文本线索和图标级语义，无需任务特定微调即可实现强性能，显著减少对标注数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有GUI grounding方法主要依赖大规模数据集微调MLLMs来预测坐标，这既数据密集又对未见界面泛化能力差。基于注意力的替代方案虽然无需微调，但由于缺乏明确的空间锚点而可靠性低。

Method: Trifuse框架明确整合了互补的空间锚点，通过Consensus-SinglePeak融合策略将注意力机制、OCR提取的文本线索和图标级语义描述相结合，强制跨模态一致性同时保持锐利的定位峰值。

Result: 在四个grounding基准测试上的广泛评估表明，Trifuse无需任务特定微调即可实现强性能，显著减少对昂贵标注数据的依赖。消融研究显示整合OCR和语义线索能持续提升不同骨干网络的性能。

Conclusion: Trifuse通过整合互补的空间锚点有效解决了现有注意力方法可靠性低的问题，提供了一个无需任务特定微调的高效GUI grounding通用框架。

Abstract: GUI grounding maps natural language instructions to the correct interface elements, serving as the perception foundation for GUI agents. Existing approaches predominantly rely on fine-tuning multimodal large language models (MLLMs) using large-scale GUI datasets to predict target element coordinates, which is data-intensive and generalizes poorly to unseen interfaces. Recent attention-based alternatives exploit localization signals in MLLMs attention mechanisms without task-specific fine-tuning, but suffer from low reliability due to the lack of explicit and complementary spatial anchors in GUI images. To address this limitation, we propose Trifuse, an attention-based grounding framework that explicitly integrates complementary spatial anchors. Trifuse integrates attention, OCR-derived textual cues, and icon-level caption semantics via a Consensus-SinglePeak (CS) fusion strategy that enforces cross-modal agreement while retaining sharp localization peaks. Extensive evaluations on four grounding benchmarks demonstrate that Trifuse achieves strong performance without task-specific fine-tuning, substantially reducing the reliance on expensive annotated data. Moreover, ablation studies reveal that incorporating OCR and caption cues consistently improves attention-based grounding performance across different backbones, highlighting its effectiveness as a general framework for GUI grounding.

</details>


### [222] [Difficulty-Estimated Policy Optimization](https://arxiv.org/abs/2602.06375)
*Yu Zhao,Fan Jiang,Tianle Liu,Bo Zeng,Yu Liu,Longyue Wang,Weihua Luo*

Main category: cs.AI

TL;DR: DEPO通过在线难度估计器动态筛选训练数据，优先处理高学习潜力样本，在保持模型性能的同时将推理成本降低2倍，为推理模型训练提供了更高效可持续的方案。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO方法在遇到过于简单或复杂的问题时会出现梯度信号衰减问题，而DAPO等变体虽然解决了梯度消失问题，但无法缓解低效用样本上大量推理计算带来的计算开销。

Method: 提出Difficulty-Estimated Policy Optimization (DEPO)框架，集成在线难度估计器，在推理阶段前动态评估和筛选训练数据，优先将计算资源分配给具有高学习潜力的样本。

Result: 实证结果显示，DEPO在保持模型性能的同时，将推理成本降低了高达2倍，显著降低了训练高性能推理模型的计算门槛。

Conclusion: DEPO为推理对齐提供了更高效和鲁棒的优化框架，通过智能数据筛选机制，为推理模型的规模化训练提供了更可持续的路径。

Abstract: Recent advancements in Large Reasoning Models (LRMs), exemplified by DeepSeek-R1, have underscored the potential of scaling inference-time compute through Group Relative Policy Optimization (GRPO). However, GRPO frequently suffers from gradient signal attenuation when encountering problems that are either too trivial or overly complex. In these scenarios, the disappearance of inter-group advantages makes the gradient signal susceptible to noise, thereby jeopardizing convergence stability. While variants like DAPO attempt to rectify gradient vanishing, they do not alleviate the substantial computational overhead incurred by exhaustive rollouts on low-utility samples. In this paper, we propose Difficulty-Estimated Policy Optimization (DEPO), a novel framework designed to optimize the efficiency and robustness of reasoning alignment. DEPO integrates an online Difficulty Estimator that dynamically assesses and filters training data before the rollout phase. This mechanism ensures that computational resources are prioritized for samples with high learning potential. Empirical results demonstrate that DEPO achieves up to a 2x reduction in rollout costs without compromising model performance. Our approach significantly lowers the computational barrier for training high-performance reasoning models, offering a more sustainable path for reasoning scaling. Code and data will be released upon acceptance.

</details>


### [223] [Intrinsic Stability Limits of Autoregressive Reasoning: Structural Consequences for Long-Horizon Execution](https://arxiv.org/abs/2602.06413)
*Hsien-Jyh Liao*

Main category: cs.AI

TL;DR: 论文提出自回归生成存在内在稳定性极限，导致长程推理性能急剧下降，这源于过程级不稳定性而非单纯任务复杂度，需要离散分段和图状执行结构来维持长程推理稳定性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在长程任务中性能急剧下降，传统解释主要归因于任务复杂度（如组合搜索爆炸或长期信用分配问题）。本文认为这些解释不完整：即使在无分支、无语义模糊的线性任务中，自回归执行也存在内在稳定性极限。

Method: 提出自回归生成的过程级不稳定性是长程推理的根本约束，重新将长程推理定义为结构治理问题。推导定理A，证明单路径自回归推理中的决策优势随执行长度呈指数衰减。通过合成环境和真实TextWorld任务的实证研究验证理论预测。

Result: 实证研究显示可观察的性能悬崖与理论预测一致。发现短程评估协议可能掩盖结构不稳定性，表明未来推理系统可能需要从单纯扩展转向结构化治理。

Conclusion: 长程推理失败源于自回归架构的过程级不稳定性，稳定长程推理需要离散分段，自然诱导出有向无环图等图状执行结构。这为长程推理失败提供了动力学视角，并揭示了纯自回归架构在维持长期连贯性方面的新限制。

Abstract: Large language models (LLMs) demonstrate remarkable reasoning capabilities, yet their performance often deteriorates sharply in long-horizon tasks, exhibiting systematic breakdown beyond certain scales. Conventional explanations primarily attribute this phenomenon to task complexity, such as combinatorial search explosion or long-term credit assignment challenges. In this work, we argue that these explanations are incomplete: even in linear, unbranched tasks without semantic ambiguity, autoregressive execution is subject to an intrinsic stability limit.
  We propose that the fundamental constraint on long-horizon reasoning arises from process-level instability in autoregressive generation rather than solely from search or task complexity, reframing long-horizon reasoning as a problem of structural governance. We derive Theorem~A, showing that decision advantage in single-path autoregressive reasoning decays exponentially with execution length, imposing a fundamental bound on maintainable reasoning chains. This result implies a structural consequence: stable long-horizon reasoning requires discrete segmentation, naturally inducing graph-like execution structures such as directed acyclic graphs (DAGs).
  Empirical studies in both synthetic environments and real TextWorld tasks reveal observable performance cliffs consistent with theoretical predictions. Our findings provide a dynamical perspective on long-horizon reasoning failure and suggest new limitations on maintaining long-term coherence under purely autoregressive architectures. Furthermore, we highlight that short-horizon evaluation protocols may obscure structural instability, indicating a potential shift from scaling toward structured governance in future reasoning systems.

</details>


### [224] [AgentCPM-Explore: Realizing Long-Horizon Deep Exploration for Edge-Scale Agents](https://arxiv.org/abs/2602.06485)
*Haotian Chen,Xin Cong,Shengda Fan,Yuyang Fu,Ziqin Gong,Yaxi Lu,Yishan Li,Boye Niu,Chengjun Pan,Zijun Song,Huadong Wang,Yesai Wu,Yueying Wu,Zihao Xie,Yukun Yan,Zhong Zhang,Yankai Lin,Zhiyuan Liu,Maosong Sun*

Main category: cs.AI

TL;DR: 提出了首个针对4B参数规模智能体模型的系统研究，通过解决SFT灾难性遗忘、RL奖励信号噪声和长上下文冗余信息三大瓶颈，开发出AgentCPM-Explore模型，在多个基准测试中超越更大规模模型。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体系统过度依赖大规模模型，边缘规模模型（4B参数级别）的潜力未被充分探索，需要系统研究其训练瓶颈和性能提升方法。

Method: 提出AgentCPM-Explore框架，采用参数空间模型融合、奖励信号去噪和上下文信息精炼三种技术，解决SFT灾难性遗忘、RL奖励噪声和长上下文推理退化问题。

Result: 在4B级别模型中达到SOTA性能，在四个基准测试中匹配或超越8B级别SOTA模型，在五个基准测试中超越Claude-4.5-Sonnet或DeepSeek-v3.2等更大模型，GAIA文本任务准确率达97.09%（pass@64）。

Conclusion: 边缘规模模型的瓶颈不在于其固有能力上限，而在于推理稳定性。通过建立的训练框架，AgentCPM-Explore有效释放了边缘规模模型的巨大潜力。

Abstract: While Large Language Model (LLM)-based agents have shown remarkable potential for solving complex tasks, existing systems remain heavily reliant on large-scale models, leaving the capabilities of edge-scale models largely underexplored. In this paper, we present the first systematic study on training agentic models at the 4B-parameter scale. We identify three primary bottlenecks hindering the performance of edge-scale models: catastrophic forgetting during Supervised Fine-Tuning (SFT), sensitivity to reward signal noise during Reinforcement Learning (RL), and reasoning degradation caused by redundant information in long-context scenarios. To address the issues, we propose AgentCPM-Explore, a compact 4B agent model with high knowledge density and strong exploration capability. We introduce a holistic training framework featuring parameter-space model fusion, reward signal denoising, and contextual information refinement. Through deep exploration, AgentCPM-Explore achieves state-of-the-art (SOTA) performance among 4B-class models, matches or surpasses 8B-class SOTA models on four benchmarks, and even outperforms larger-scale models such as Claude-4.5-Sonnet or DeepSeek-v3.2 in five benchmarks. Notably, AgentCPM-Explore achieves 97.09% accuracy on GAIA text-based tasks under pass@64. These results provide compelling evidence that the bottleneck for edge-scale models is not their inherent capability ceiling, but rather their inference stability. Based on our well-established training framework, AgentCPM-Explore effectively unlocks the significant, yet previously underestimated, potential of edge-scale models.

</details>


### [225] [JADE: Expert-Grounded Dynamic Evaluation for Open-Ended Professional Tasks](https://arxiv.org/abs/2602.06486)
*Lanbo Lin,Jiayao Liu,Tianyuan Yang,Li Cai,Yuanwu Xu,Lei Wei,Sicong Xie,Guannan Zhang*

Main category: cs.AI

TL;DR: JADE是一个两层的AI评估框架，通过预定义评估技能和报告特定的声明级评估，解决了开放专业任务评估中严谨性与灵活性之间的困境。


<details>
  <summary>Details</summary>
Motivation: 评估自主AI在开放专业任务上面临基本困境：静态评估标准虽然严谨可复现但无法适应多样有效的响应策略，而LLM作为评估者虽然能适应个体响应但存在不稳定性和偏见。人类专家通过结合领域基础原则和动态声明级评估来解决这一困境。

Method: JADE采用两层评估框架：第一层将专家知识编码为预定义的评估技能，提供稳定的评估标准；第二层执行报告特定的声明级评估，灵活评估多样推理策略，并通过证据依赖性门控来使基于被反驳声明的结论无效。

Result: 在BizBench上的实验显示，JADE提高了评估稳定性，并揭示了整体LLM评估器遗漏的关键代理失败模式。与专家制定的评估标准有强对齐，并能有效迁移到医疗领域基准测试，验证了JADE在不同专业领域的有效性。

Conclusion: JADE通过结合预定义评估技能和动态声明级评估，有效解决了开放专业任务评估中严谨性与灵活性的困境，在不同专业领域都表现出良好的评估效果和迁移能力。

Abstract: Evaluating agentic AI on open-ended professional tasks faces a fundamental dilemma between rigor and flexibility. Static rubrics provide rigorous, reproducible assessment but fail to accommodate diverse valid response strategies, while LLM-as-a-judge approaches adapt to individual responses yet suffer from instability and bias. Human experts address this dilemma by combining domain-grounded principles with dynamic, claim-level assessment. Inspired by this process, we propose JADE, a two-layer evaluation framework. Layer 1 encodes expert knowledge as a predefined set of evaluation skills, providing stable evaluation criteria. Layer 2 performs report-specific, claim-level evaluation to flexibly assess diverse reasoning strategies, with evidence-dependency gating to invalidate conclusions built on refuted claims. Experiments on BizBench show that JADE improves evaluation stability and reveals critical agent failure modes missed by holistic LLM-based evaluators. We further demonstrate strong alignment with expert-authored rubrics and effective transfer to a medical-domain benchmark, validating JADE across professional domains. Our code is publicly available at https://github.com/smiling-world/JADE.

</details>


### [226] [Progress Constraints for Reinforcement Learning in Behavior Trees](https://arxiv.org/abs/2602.06525)
*Finn Rietz,Mart Kartašev,Johannes A. Stork,Petter Ögren*

Main category: cs.AI

TL;DR: 提出进度约束机制，将行为树与强化学习结合，通过可行性估计器限制动作集，解决控制器相互抵消问题，提升性能、样本效率和约束满足度。


<details>
  <summary>Details</summary>
Motivation: 行为树（BTs）提供结构化反应式决策框架，强化学习（RL）能学习近似最优控制器，但存在稀疏奖励、安全探索和长期信用分配问题。两者结合有潜力相互补充：BT设计编码领域知识简化RL训练，RL能自动学习BT内控制器。但简单整合可能导致控制器相互抵消，破坏已实现子目标，降低整体性能。

Method: 提出进度约束机制，基于行为树收敛理论结果，使用可行性估计器限制允许的动作集。在2D概念验证和高保真仓库环境中进行实证评估。

Result: 与先前BT-RL整合方法相比，展示了改进的性能、样本效率和约束满足度。

Conclusion: 进度约束机制有效解决了行为树与强化学习整合中的控制器相互抵消问题，实现了两者的优势互补。

Abstract: Behavior Trees (BTs) provide a structured and reactive framework for decision-making, commonly used to switch between sub-controllers based on environmental conditions. Reinforcement Learning (RL), on the other hand, can learn near-optimal controllers but sometimes struggles with sparse rewards, safe exploration, and long-horizon credit assignment. Combining BTs with RL has the potential for mutual benefit: a BT design encodes structured domain knowledge that can simplify RL training, while RL enables automatic learning of the controllers within BTs. However, naive integration of BTs and RL can lead to some controllers counteracting other controllers, possibly undoing previously achieved subgoals, thereby degrading the overall performance. To address this, we propose progress constraints, a novel mechanism where feasibility estimators constrain the allowed action set based on theoretical BT convergence results. Empirical evaluations in a 2D proof-of-concept and a high-fidelity warehouse environment demonstrate improved performance, sample efficiency, and constraint satisfaction, compared to prior methods of BT-RL integration.

</details>


### [227] [HyPER: Bridging Exploration and Exploitation for Scalable LLM Reasoning with Hypothesis Path Expansion and Reduction](https://arxiv.org/abs/2602.06527)
*Shengxuan Qiu,Haochen Huang,Shuzhang Zhong,Pengfei Zuo,Meng Li*

Main category: cs.AI

TL;DR: HyPER：一种用于专家混合模型多路径解码的训练免费在线控制策略，通过动态扩展-缩减控制优化探索-利用权衡，在固定计算预算下显著提升推理精度并减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有测试时计算扩展方法在探索-利用权衡上存在僵化问题：树状搜索通过脆弱的扩展规则硬编码探索，干扰后训练推理；并行推理则过度探索冗余假设路径且依赖弱答案选择。研究发现最优平衡是阶段依赖的，正确与错误推理路径往往在后期才分叉。

Method: 将测试时扩展重新定义为假设池上的动态扩展-缩减控制问题。提出HyPER：1) 在线控制器根据假设池演化从探索转向利用；2) 令牌级精炼机制实现无需完整路径重采样的生成时高效利用；3) 长度和置信度感知的聚合策略实现可靠答案时利用。

Result: 在四个专家混合语言模型和多样化推理基准上的实验表明，HyPER始终实现更优的精度-计算权衡：精度提升8-10%，同时令牌使用量减少25-40%。

Conclusion: HyPER通过动态控制策略有效解决了多路径思维链推理中的探索-利用权衡问题，在固定计算预算下显著提升了推理精度和效率，为测试时计算扩展提供了更灵活有效的解决方案。

Abstract: Scaling test-time compute with multi-path chain-of-thought improves reasoning accuracy, but its effectiveness depends critically on the exploration-exploitation trade-off. Existing approaches address this trade-off in rigid ways: tree-structured search hard-codes exploration through brittle expansion rules that interfere with post-trained reasoning, while parallel reasoning over-explores redundant hypothesis paths and relies on weak answer selection. Motivated by the observation that the optimal balance is phase-dependent and that correct and incorrect reasoning paths often diverge only at late stages, we reformulate test-time scaling as a dynamic expand-reduce control problem over a pool of hypotheses. We propose HyPER, a training-free online control policy for multi-path decoding in mixture-of-experts models that reallocates computation under a fixed budget using lightweight path statistics. HyPER consists of an online controller that transitions from exploration to exploitation as the hypothesis pool evolves, a token-level refinement mechanism that enables efficient generation-time exploitation without full-path resampling, and a length- and confidence-aware aggregation strategy for reliable answer-time exploitation. Experiments on four mixture-of-experts language models across diverse reasoning benchmarks show that HyPER consistently achieves a superior accuracy-compute trade-off, improving accuracy by 8 to 10 percent while reducing token usage by 25 to 40 percent.

</details>


### [228] [LogicSkills: A Structured Benchmark for Formal Reasoning in Large Language Models](https://arxiv.org/abs/2602.06533)
*Brian Rabern,Philipp Mondorf,Barbara Plank*

Main category: cs.AI

TL;DR: LogicSkills基准测试评估大语言模型在形式推理中的三项核心技能：形式符号化、反模型构建和有效性评估，发现模型在有效性评估上表现良好，但在符号化和反模型构建上表现较差。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在各种逻辑推理基准测试中表现出色，但尚不清楚它们真正掌握了哪些核心逻辑技能。为了系统评估模型在形式推理中的基本能力，需要设计一个能够隔离和测试三项核心逻辑技能的基准测试。

Method: 提出LogicSkills基准测试，专注于一阶逻辑双变量片段（不含恒等关系）中的三项技能：1）形式符号化（将前提翻译为一阶逻辑）；2）反模型构建（构建一个有限结构使所有前提为真而结论为假）；3）有效性评估（判断结论是否从给定前提中得出）。测试项目以自然英语和Carroll式非词语言呈现，所有示例使用SMT求解器Z3验证正确性和非平凡性。

Result: 在领先模型上的测试结果显示：模型在有效性评估任务上表现良好，但在形式符号化和反模型构建任务上表现显著较差。这表明模型主要依赖表面模式匹配，而非真正的符号化或基于规则的推理能力。

Conclusion: 大语言模型在逻辑推理中更擅长模式识别和有效性判断，但在需要深入符号理解和反模型构建的复杂推理任务上存在明显不足。LogicSkills基准测试为评估模型的形式推理能力提供了系统框架，揭示了当前模型在逻辑技能掌握上的局限性。

Abstract: Large language models have demonstrated notable performance across various logical reasoning benchmarks. However, it remains unclear which core logical skills they truly master. To address this, we introduce LogicSkills, a unified benchmark designed to isolate three fundamental skills in formal reasoning: (i) $\textit{formal symbolization}\unicode{x2014}$translating premises into first-order logic; (ii) $\textit{countermodel construction}\unicode{x2014}$formulating a finite structure in which all premises are true while the conclusion is false; and (iii) $\textit{validity assessment}\unicode{x2014}$deciding whether a conclusion follows from a given set of premises. Items are drawn from the two-variable fragment of first-order logic (without identity) and are presented in both natural English and a Carroll-style language with nonce words. All examples are verified for correctness and non-triviality using the SMT solver Z3. Across leading models, performance is high on validity but substantially lower on symbolization and countermodel construction, suggesting reliance on surface-level patterns rather than genuine symbolic or rule-based reasoning.

</details>


### [229] [AgentCPM-Report: Interleaving Drafting and Deepening for Open-Ended Deep Research](https://arxiv.org/abs/2602.06540)
*Yishan Li,Wentong Chen,Yukun Yan,Mingwei Li,Sen Mei,Xiaorong Wang,Kunpeng Liu,Xin Cong,Shuo Wang,Zhong Zhang,Yaxi Lu,Zhenghao Liu,Yankai Lin,Zhiyuan Liu,Maosong Sun*

Main category: cs.AI

TL;DR: AgentCPM-Report：一个轻量级本地化深度研究报告生成系统，通过Writing As Reasoning Policy和多阶段智能体训练，使8B参数模型在深度研究任务上超越闭源系统。


<details>
  <summary>Details</summary>
Motivation: 当前深度研究报告生成系统严重依赖闭源或在线大模型，存在部署障碍、安全隐私问题，且传统"先规划后写作"范式对初始大纲质量要求过高。

Method: 提出Writing As Reasoning Policy框架，支持动态大纲修订；采用Evidence-Based Drafting和Reasoning-Driven Deepening交替策略；设计Multi-Stage Agentic Training（冷启动、原子技能RL、整体流程RL）训练8B参数智能体。

Result: 在DeepResearch Bench、DeepConsult和DeepResearch Gym基准测试中，AgentCPM-Report超越领先闭源系统，在Insight指标上取得显著提升。

Conclusion: AgentCPM-Report提供了一个轻量级、高性能的本地化解决方案，通过创新的推理策略和训练方法，使小模型在深度研究任务上达到甚至超越大模型性能。

Abstract: Generating deep research reports requires large-scale information acquisition and the synthesis of insight-driven analysis, posing a significant challenge for current language models. Most existing approaches follow a plan-then-write paradigm, whose performance heavily depends on the quality of the initial outline. However, constructing a comprehensive outline itself demands strong reasoning ability, causing current deep research systems to rely almost exclusively on closed-source or online large models. This reliance raises practical barriers to deployment and introduces safety and privacy concerns for user-authored data. In this work, we present AgentCPM-Report, a lightweight yet high-performing local solution composed of a framework that mirrors the human writing process and an 8B-parameter deep research agent. Our framework uses a Writing As Reasoning Policy (WARP), which enables models to dynamically revise outlines during report generation. Under this policy, the agent alternates between Evidence-Based Drafting and Reasoning-Driven Deepening, jointly supporting information acquisition, knowledge refinement, and iterative outline evolution. To effectively equip small models with this capability, we introduce a Multi-Stage Agentic Training strategy, consisting of cold-start, atomic skill RL, and holistic pipeline RL. Experiments on DeepResearch Bench, DeepConsult, and DeepResearch Gym demonstrate that AgentCPM-Report outperforms leading closed-source systems, with substantial gains in Insight.

</details>


### [230] [SeeUPO: Sequence-Level Agentic-RL with Convergence Guarantees](https://arxiv.org/abs/2602.06554)
*Tianyi Hu,Qingxu Fu,Yanxi Chen,Zhaoyang Liu,Bolin Ding*

Main category: cs.AI

TL;DR: 提出SeeUPO算法，解决多轮交互中RL算法缺乏收敛保证的问题，通过序列级顺序更新策略优化实现无评论家且收敛的强化学习


<details>
  <summary>Details</summary>
Motivation: 现有骨干RL算法在多轮智能体场景中缺乏验证的收敛保证，导致训练不稳定和无法收敛到最优策略，需要同时实现无评论家和收敛保证的方法

Method: 提出SeeUPO算法，将多轮交互建模为顺序执行的多智能体老虎机问题，通过反向执行顺序的逐轮顺序策略更新，确保单调改进并通过反向归纳收敛到全局最优解

Result: 在AppWorld和BFCL v4基准测试中，SeeUPO显著优于现有骨干算法：Qwen3-14B上相对提升43.3%-54.6%，Qwen2.5-14B上提升24.1%-41.9%，同时具有更好的训练稳定性

Conclusion: SeeUPO为多轮交互提供了一种无评论家且具有收敛保证的RL算法，解决了现有方法在收敛性和训练稳定性方面的局限性，为LLM智能体训练提供了更可靠的优化框架

Abstract: Reinforcement learning (RL) has emerged as the predominant paradigm for training large language model (LLM)-based AI agents. However, existing backbone RL algorithms lack verified convergence guarantees in agentic scenarios, especially in multi-turn settings, which can lead to training instability and failure to converge to optimal policies.
  In this paper, we systematically analyze how different combinations of policy update mechanisms and advantage estimation methods affect convergence properties in single/multi-turn scenarios. We find that REINFORCE with Group Relative Advantage Estimation (GRAE) can converge to the globally optimal under undiscounted conditions, but the combination of PPO & GRAE breaks PPO's original monotonic improvement property. Furthermore, we demonstrate that mainstream backbone RL algorithms cannot simultaneously achieve both critic-free and convergence guarantees in multi-turn scenarios.
  To address this, we propose SeeUPO (Sequence-level Sequential Update Policy Optimization), a critic-free approach with convergence guarantees for multi-turn interactions. SeeUPO models multi-turn interaction as sequentially executed multi-agent bandit problems. Through turn-by-turn sequential policy updates in reverse execution order, it ensures monotonic improvement and convergence to global optimal solution via backward induction.
  Experiments on AppWorld and BFCL v4 demonstrate SeeUPO's substantial improvements over existing backbone algorithms: relative gains of 43.3%-54.6% on Qwen3-14B and 24.1%-41.9% on Qwen2.5-14B (averaged across benchmarks), along with superior training stability.

</details>


### [231] [Same Answer, Different Representations: Hidden instability in VLMs](https://arxiv.org/abs/2602.06652)
*Farooq Ahmad Wani,Alessandro Suglia,Rohit Saxena,Aryo Pradipta Gema,Wai-Chung Kwan,Fazl Barez,Maria Sofia Bucarelli,Fabrizio Silvestri,Pasquale Minervini*

Main category: cs.AI

TL;DR: 本文提出一个表示感知和频率感知的评估框架，发现VLMs在输出稳定时内部表示仍可能大幅漂移，模型规模增大不提升鲁棒性，扰动对不同任务有不同影响。


<details>
  <summary>Details</summary>
Motivation: 当前VLMs鲁棒性评估主要基于输出层面的不变性，隐含假设稳定预测反映稳定的多模态处理。本文认为这一假设不足，需要更深入评估内部表示稳定性。

Method: 提出表示感知和频率感知的评估框架，测量内部嵌入漂移、频谱敏感性和结构平滑性（视觉token的空间一致性），同时结合标准标签指标。在SEEDBench、MMMU和POPE数据集上评估现代VLMs。

Result: 发现三种失效模式：1）预测答案保持不变时内部表示大幅漂移，接近图像间变异性；2）模型规模增大不改善鲁棒性，更大模型更敏感；3）扰动对不同任务影响不同：破坏推理但减少幻觉误报。

Conclusion: 仅依赖输出不变性评估VLMs鲁棒性不足，需要同时考虑内部表示稳定性。模型规模增大不必然提升鲁棒性，扰动对不同任务有复杂影响。

Abstract: The robustness of Vision Language Models (VLMs) is commonly assessed through output-level invariance, implicitly assuming that stable predictions reflect stable multimodal processing. In this work, we argue that this assumption is insufficient. We introduce a representation-aware and frequency-aware evaluation framework that measures internal embedding drift, spectral sensitivity, and structural smoothness (spatial consistency of vision tokens), alongside standard label-based metrics. Applying this framework to modern VLMs across the SEEDBench, MMMU, and POPE datasets reveals three distinct failure modes. First, models frequently preserve predicted answers while undergoing substantial internal representation drift; for perturbations such as text overlays, this drift approaches the magnitude of inter-image variability, indicating that representations move to regions typically occupied by unrelated inputs despite unchanged outputs. Second, robustness does not improve with scale; larger models achieve higher accuracy but exhibit equal or greater sensitivity, consistent with sharper yet more fragile decision boundaries. Third, we find that perturbations affect tasks differently: they harm reasoning when they disrupt how models combine coarse and fine visual cues, but on the hallucination benchmarks, they can reduce false positives by making models generate more conservative answers.

</details>


### [232] [Autoregressive Models for Knowledge Graph Generation](https://arxiv.org/abs/2602.06707)
*Thiviyan Thanapalasingam,Antonis Vozikis,Peter Bloem,Paul Groth*

Main category: cs.AI

TL;DR: ARK是用于知识图谱生成的自回归模型家族，将图视为三元组序列，无需显式规则监督即可学习语义约束，在IntelliGraphs基准测试中达到89.2%-100%语义有效性。


<details>
  <summary>Details</summary>
Motivation: 知识图谱生成需要模型学习三元组间的复杂语义依赖关系，同时保持领域有效性约束。与独立评分三元组的链接预测不同，生成模型必须捕获整个子图的相互依赖关系以产生语义连贯的结构。

Method: 提出ARK（自回归知识图谱生成）模型家族，将图视为(head, relation, tail)三元组序列进行自回归生成。模型直接从数据中学习隐式语义约束（类型一致性、时间有效性、关系模式），无需显式规则监督。还提出SAIL，ARK的变分扩展，通过学习的潜在表示实现可控生成。

Result: 在IntelliGraphs基准测试中，模型在多样化数据集上达到89.2%到100.0%的语义有效性，同时生成训练中未见的新图。分析显示模型容量（隐藏维度≥64）比架构深度对KG生成更重要，循环架构在保持可比有效性的同时提供显著计算效率。

Conclusion: 自回归模型为知识图谱生成提供了有效框架，在知识库补全和查询回答中具有实际应用价值。模型容量比深度更重要，循环架构在效率和有效性间取得良好平衡。

Abstract: Knowledge Graph (KG) generation requires models to learn complex semantic dependencies between triples while maintaining domain validity constraints. Unlike link prediction, which scores triples independently, generative models must capture interdependencies across entire subgraphs to produce semantically coherent structures. We present ARK (Auto-Regressive Knowledge Graph Generation), a family of autoregressive models that generate KGs by treating graphs as sequences of (head, relation, tail) triples. ARK learns implicit semantic constraints directly from data, including type consistency, temporal validity, and relational patterns, without explicit rule supervision. On the IntelliGraphs benchmark, our models achieve 89.2% to 100.0% semantic validity across diverse datasets while generating novel graphs not seen during training. We also introduce SAIL, a variational extension of ARK that enables controlled generation through learned latent representations, supporting both unconditional sampling and conditional completion from partial graphs. Our analysis reveals that model capacity (hidden dimensionality >= 64) is more critical than architectural depth for KG generation, with recurrent architectures achieving comparable validity to transformer-based alternatives while offering substantial computational efficiency. These results demonstrate that autoregressive models provide an effective framework for KG generation, with practical applications in knowledge base completion and query answering.

</details>


### [233] [Semantically Labelled Automata for Multi-Task Reinforcement Learning with LTL Instructions](https://arxiv.org/abs/2602.06746)
*Alessandro Abate,Giuseppe De Giacomo,Mathias Jackermeier,Jan Kretínský,Maximilian Prokop,Christoph Weinhuber*

Main category: cs.AI

TL;DR: 提出一种基于语义LTL到自动机转换的多任务强化学习方法，利用结构化任务嵌入实现通用策略学习


<details>
  <summary>Details</summary>
Motivation: 多任务强化学习中，需要学习一个能够泛化到任意（可能未见）任务的通用策略。现有方法在处理复杂线性时序逻辑（LTL）规范时存在局限性

Method: 利用新一代语义LTL到自动机转换技术，构建语义标记自动机，从中提取结构化任务嵌入来条件化策略，支持完整LTL规范

Result: 在多个领域实验中达到最先进性能，能够扩展到现有方法无法处理的复杂规范

Conclusion: 提出的任务嵌入方法通过利用语义丰富的自动机结构，有效解决了多任务强化学习中复杂LTL规范的处理问题

Abstract: We study multi-task reinforcement learning (RL), a setting in which an agent learns a single, universal policy capable of generalising to arbitrary, possibly unseen tasks. We consider tasks specified as linear temporal logic (LTL) formulae, which are commonly used in formal methods to specify properties of systems, and have recently been successfully adopted in RL. In this setting, we present a novel task embedding technique leveraging a new generation of semantic LTL-to-automata translations, originally developed for temporal synthesis. The resulting semantically labelled automata contain rich, structured information in each state that allow us to (i) compute the automaton efficiently on-the-fly, (ii) extract expressive task embeddings used to condition the policy, and (iii) naturally support full LTL. Experimental results in a variety of domains demonstrate that our approach achieves state-of-the-art performance and is able to scale to complex specifications where existing methods fail.

</details>


### [234] [Towards Understanding What State Space Models Learn About Code](https://arxiv.org/abs/2602.06774)
*Jiali Wu,Abhinav Anand,Shweta Verma,Mira Mezini*

Main category: cs.AI

TL;DR: SSM代码模型在预训练中优于Transformer捕捉代码语法语义，但在微调时会遗忘某些语法语义关系，尤其是在短距离依赖任务中。通过频谱分析发现微调时存在向短距离依赖的频谱偏移，据此提出的架构改进显著提升了SSM代码模型性能。


<details>
  <summary>Details</summary>
Motivation: 状态空间模型（SSMs）已成为Transformer架构的高效替代方案，在代码理解任务上表现优异，但其内部机制仍不明确。本文旨在首次系统分析SSM代码模型的学习内容，并与Transformer代码模型进行对比分析。

Method: 1. 对SSM和Transformer代码模型进行系统比较分析；2. 引入SSM-Interpret频率域分析框架，通过频谱分析揭示微调过程中的依赖关系变化；3. 基于分析发现提出架构改进方案。

Result: SSMs在预训练阶段优于Transformer捕捉代码语法和语义，但在任务微调时会遗忘某些语法语义关系，特别是在强调短距离依赖的任务中。频谱分析显示微调过程中存在向短距离依赖的频谱偏移。基于这些发现提出的架构修改显著提升了SSM代码模型的性能。

Conclusion: 本文首次系统分析了SSM代码模型的学习机制，揭示了其在微调过程中的局限性，并通过频谱分析框架SSM-Interpret诊断了问题根源。基于分析提出的架构改进验证了分析对模型优化的直接指导价值，为SSM代码模型的进一步发展提供了理论基础。

Abstract: State Space Models (SSMs) have emerged as an efficient alternative to the transformer architecture. Recent studies show that SSMs can match or surpass Transformers on code understanding tasks, such as code retrieval, when trained under similar conditions. However, their internal mechanisms remain a black box. We present the first systematic analysis of what SSM-based code models actually learn and perform the first comparative analysis of SSM and Transformer-based code models. Our analysis reveals that SSMs outperform Transformers at capturing code syntax and semantics in pretraining but forgets certain syntactic and semantic relations during fine-tuning on task, especially when the task emphasizes short-range dependencies. To diagnose this, we introduce SSM-Interpret, a frequency-domain framework that exposes a spectral shift toward short-range dependencies during fine-tuning. Guided by these findings, we propose architectural modifications that significantly improve the performance of SSM-based code model, validating that our analysis directly enables better models.

</details>


### [235] [Wild Guesses and Mild Guesses in Active Concept Learning](https://arxiv.org/abs/2602.06818)
*Anirudh Chari,Neil Pattanaik*

Main category: cs.AI

TL;DR: 研究比较了理性主动学习器（最大化期望信息增益）与人类类似的正向测试策略在概念学习中的表现，发现EIG在复杂规则中有效但在简单概念中表现不佳，而PTS通过选择"安全"查询保持提案有效性，收敛更快。


<details>
  <summary>Details</summary>
Motivation: 人类概念学习通常是主动的，学习者选择查询哪些实例来减少对底层规则的不确定性。主动概念学习需要在查询的信息量与学习器的稳定性之间取得平衡，本研究旨在探索这种权衡。

Method: 采用神经符号贝叶斯学习器，其假设是由大型语言模型生成的可执行程序，并通过贝叶斯更新重新加权。比较两种策略：理性主动学习器（选择最大化近似期望信息增益的查询）和人类类似的正向测试策略（查询当前最佳假设预测为正的实例）。

Result: 在经典数字游戏的概念学习任务中，EIG在需要证伪的复杂规则（如复合规则或例外规则）中有效，但在简单概念中表现不佳。这是因为EIG策略与LLM提案分布之间存在支持不匹配问题，而PTS虽然信息次优，但通过选择"安全"查询保持提案有效性，在简单规则上收敛更快。

Conclusion: "确认偏误"可能不是认知错误，而是在人类思维特有的稀疏、开放式假设空间中维持可处理推理的理性适应策略。PTS通过避免支持不匹配陷阱，在简单概念学习中表现更好。

Abstract: Human concept learning is typically active: learners choose which instances to query or test in order to reduce uncertainty about an underlying rule or category. Active concept learning must balance informativeness of queries against the stability of the learner that generates and scores hypotheses. We study this trade-off in a neuro-symbolic Bayesian learner whose hypotheses are executable programs proposed by a large language model (LLM) and reweighted by Bayesian updating. We compare a Rational Active Learner that selects queries to maximize approximate expected information gain (EIG) and the human-like Positive Test Strategy (PTS) that queries instances predicted to be positive under the current best hypothesis. Across concept-learning tasks in the classic Number Game, EIG is effective when falsification is necessary (e.g., compound or exception-laden rules), but underperforms on simple concepts. We trace this failure to a support mismatch between the EIG policy and the LLM proposal distribution: highly diagnostic boundary queries drive the posterior toward regions where the generator produces invalid or overly specific programs, yielding a support-mismatch trap in the particle approximation. PTS is information-suboptimal but tends to maintain proposal validity by selecting "safe" queries, leading to faster convergence on simple rules. Our results suggest that "confirmation bias" may not be a cognitive error, but rather a rational adaptation for maintaining tractable inference in the sparse, open-ended hypothesis spaces characteristic of human thought.

</details>


### [236] [ScaleEnv: Scaling Environment Synthesis from Scratch for Generalist Interactive Tool-Use Agent Training](https://arxiv.org/abs/2602.06820)
*Dunwei Tu,Hongyan Hao,Hansi Yang,Yihao Chen,Yi-Kai Zhang,Zhikang Xia,Yu Yang,Yueqing Sun,Xingchen Liu,Furao Shen,Qi Gu,Hui Su,Xunliang Cai*

Main category: cs.AI

TL;DR: ScaleEnv框架从零开始构建完全交互式环境和可验证任务，通过程序化测试确保环境可靠性，通过工具依赖图扩展和可执行动作验证保证任务完整性和可解性，显著提升智能体在未见多轮工具使用基准上的性能。


<details>
  <summary>Details</summary>
Motivation: 训练能够适应多样化场景的通用智能体需要交互式环境进行自我探索，但现有交互环境严重不足，且现有合成方法在环境多样性和可扩展性方面存在显著限制。

Method: ScaleEnv框架从零开始构建完全交互式环境和可验证任务，通过程序化测试确保环境可靠性，通过工具依赖图扩展和可执行动作验证保证任务完整性和可解性。

Result: 在未见的多轮工具使用基准（如τ²-Bench和VitaBench）上表现出显著性能提升，展示了强大的泛化能力；研究还发现增加领域数量与模型泛化性能之间存在正相关关系。

Conclusion: 扩展环境多样性对于稳健的智能体学习至关重要，ScaleEnv为解决交互环境稀缺问题提供了有效框架，能够显著提升智能体的泛化能力。

Abstract: Training generalist agents capable of adapting to diverse scenarios requires interactive environments for self-exploration. However, interactive environments remain critically scarce, and existing synthesis methods suffer from significant limitations regarding environmental diversity and scalability. To address these challenges, we introduce ScaleEnv, a framework that constructs fully interactive environments and verifiable tasks entirely from scratch. Specifically, ScaleEnv ensures environment reliability through procedural testing, and guarantees task completeness and solvability via tool dependency graph expansion and executable action verification. By enabling agents to learn through exploration within ScaleEnv, we demonstrate significant performance improvements on unseen, multi-turn tool-use benchmarks such as $τ^2$-Bench and VitaBench, highlighting strong generalization capabilities. Furthermore, we investigate the relationship between increasing number of domains and model generalization performance, providing empirical evidence that scaling environmental diversity is critical for robust agent learning.

</details>


### [237] [POP: Online Structural Pruning Enables Efficient Inference of Large Foundation Models](https://arxiv.org/abs/2602.06822)
*Yi Chen,Wonjin Shin,Shuhong Liu,Tho Mai,Jeongmo Lee,Chuanbo Hua,Kun Wang,Jun Liu,Joo-Young Kim*

Main category: cs.AI

TL;DR: POP是一种轻量级在线结构化剪枝框架，通过分区引导的动态剪枝在推理过程中实现上下文感知的稀疏化，无需预处理或重训练


<details>
  <summary>Details</summary>
Motivation: 当前的结构化剪枝方法在推理时采用固定的剪枝决策，忽视了自回归token生成过程中出现的稀疏模式，无法充分利用上下文信息进行动态优化

Method: POP将模型通道划分为保留区、候选区和剪枝区：预填充阶段定义粗粒度剪枝分区，解码阶段在候选区内生成细粒度掩码，避免全通道重新评估

Result: 在多种大型基础模型（LLMs、MoEs、VLMs）上的广泛评估表明，POP比现有剪枝方法具有更高准确率，同时计算开销更小，推理延迟更低

Conclusion: POP是一种高效的在线结构化剪枝框架，通过上下文条件动态剪枝实现了更好的性能-效率权衡，无需预处理即可应用于各种大型基础模型

Abstract: Large foundation models (LFMs) achieve strong performance through scaling, yet current structural pruning methods derive fixed pruning decisions during inference, overlooking sparsity patterns that emerge in the autoregressive token generation. In this paper, we propose POP (Partition-guided Online Pruning), an efficient online structural pruning framework that enables context-conditioned dynamic pruning with minimal computational overhead. POP partitions model channels into retained, candidate, and pruned regions, where prefilling defines a coarse pruning partition, and the decoding stage generates a fine-grained mask within the candidate region, avoiding full-channel re-evaluation. The coarse pruning partition preserves consistently important weights, while the fine-grained masking provides context-conditioned variation during decoding. Moreover, POP is a lightweight, plug-and-play method that requires no preprocessing, including offline calibration, retraining, or learning predictors. Extensive evaluations across diverse LFMs, including large language models (LLMs), mixture-of-experts models (MoEs), and vision-language models (VLMs), demonstrate that POP consistently delivers higher accuracy than existing pruning approaches while incurring smaller computational overhead and minimizing inference latency.

</details>


### [238] [LLM Active Alignment: A Nash Equilibrium Perspective](https://arxiv.org/abs/2602.06836)
*Tonghan Wang,Yuqi Pan,Xinyi Yang,Yanchen Jiang,Milind Tambe,David C. Parkes*

Main category: cs.AI

TL;DR: 提出基于纳什均衡分析的博弈论框架，用于预测和引导大规模语言模型群体的行为，通过混合人类亚群体建模避免计算复杂性，提供可解释的政策类别和系统级预测。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理开放文本空间中的均衡计算复杂性，需要可解释且行为实质性的政策类别来预测和引导LLM群体行为，特别是在多智能体动态中避免政治排斥等病态现象。

Method: 将每个智能体的行动建模为人类亚群体的混合分布，智能体主动战略性地选择与哪些群体对齐；采用标准凹效用假设推导封闭形式的纳什均衡特征，作为现有对齐流程（如RLHF）之上的主动对齐层。

Result: 在社交媒体场景中，LLM群体（特别是基于推理的模型）可能表现出政治排斥现象，某些亚群体会被所有LLM智能体忽略；该方法能够避免这种病态现象，展示了在多领域调节多智能体LLM动态的潜力。

Conclusion: 该博弈论框架为预测和引导LLM群体行为提供了可扩展的解决方案，通过纳什均衡分析实现系统级预测，并为将对齐目标转向社会期望结果提供明确、可操作的指导。

Abstract: We develop a game-theoretic framework for predicting and steering the behavior of populations of large language models (LLMs) through Nash equilibrium (NE) analysis. To avoid the intractability of equilibrium computation in open-ended text spaces, we model each agent's action as a mixture over human subpopulations. Agents choose actively and strategically which groups to align with, yielding an interpretable and behaviorally substantive policy class. We derive closed-form NE characterizations, adopting standard concave-utility assumptions to enable analytical system-level predictions and give explicit, actionable guidance for shifting alignment targets toward socially desirable outcomes. The method functions as an active alignment layer on top of existing alignment pipelines such as RLHF. In a social-media setting, we show that a population of LLMs, especially reasoning-based models, may exhibit political exclusion, pathologies where some subpopulations are ignored by all LLM agents, which can be avoided by our method, illustrating the promise of applying the method to regulate multi-agent LLM dynamics across domains.

</details>


### [239] [An Adaptive Differentially Private Federated Learning Framework with Bi-level Optimization](https://arxiv.org/abs/2602.06838)
*Jin Wang,Hui Ma,Fei Xing,Ming Yan*

Main category: cs.AI

TL;DR: 提出自适应差分隐私联邦学习框架，通过客户端轻量压缩模块、服务器自适应梯度裁剪和约束感知聚合机制，解决异构数据和隐私约束下的训练不稳定问题


<details>
  <summary>Details</summary>
Motivation: 实际联邦学习中，设备异构性和非独立同分布数据导致梯度更新不稳定且有偏差，而差分隐私的固定梯度裁剪和高斯噪声注入会进一步放大梯度扰动，造成训练震荡和性能下降

Method: 1) 客户端引入轻量级本地压缩模块，正则化中间表示并约束梯度变异性；2) 服务器采用自适应梯度裁剪策略，基于历史更新统计动态调整裁剪阈值；3) 设计约束感知聚合机制，抑制不可靠或噪声主导的客户端更新

Result: 在CIFAR-10和SVHN数据集上的大量实验表明，该方法提高了收敛稳定性和分类准确率

Conclusion: 提出的自适应差分隐私联邦学习框架能有效解决异构和隐私约束环境下的训练不稳定问题，提升模型效率和性能

Abstract: Federated learning enables collaborative model training across distributed clients while preserving data privacy. However, in practical deployments, device heterogeneity, non-independent, and identically distributed (Non-IID) data often lead to highly unstable and biased gradient updates. When differential privacy is enforced, conventional fixed gradient clipping and Gaussian noise injection may further amplify gradient perturbations, resulting in training oscillation and performance degradation and degraded model performance. To address these challenges, we propose an adaptive differentially private federated learning framework that explicitly targets model efficiency under heterogeneous and privacy-constrained settings. On the client side, a lightweight local compressed module is introduced to regularize intermediate representations and constrain gradient variability, thereby mitigating noise amplification during local optimization. On the server side, an adaptive gradient clipping strategy dynamically adjusts clipping thresholds based on historical update statistics to avoid over-clipping and noise domination. Furthermore, a constraint-aware aggregation mechanism is designed to suppress unreliable or noise-dominated client updates and stabilize global optimization. Extensive experiments on CIFAR-10 and SVHN demonstrate improved convergence stability and classification accuracy.

</details>


### [240] [From Features to Actions: Explainability in Traditional and Agentic AI Systems](https://arxiv.org/abs/2602.06841)
*Sindhuja Chaduvula,Jessee Ho,Kina Kim,Aravind Narayanan,Mahshid Alinoori,Muskan Garg,Dhanesh Ramachandram,Shaina Raza*

Main category: cs.AI

TL;DR: 该论文比较了静态预测解释与智能体系统解释方法，发现传统特征归因方法适用于静态分类但不适用于诊断智能体执行失败，而基于轨迹的诊断能有效定位行为故障。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型发展，智能体AI系统通过多步决策轨迹运行，但现有的可解释AI方法主要针对静态预测设计，不清楚这些方法如何适用于智能体环境，需要研究静态与智能体可解释性之间的差距。

Method: 通过比较静态分类任务中的归因解释方法与智能体基准测试（TAU-bench Airline和AssistantBench）中的轨迹诊断方法，实证分析两种解释方法在不同设置下的表现。

Result: 归因方法在静态设置中特征排序稳定（Spearman ρ=0.86），但无法可靠诊断智能体轨迹中的执行级故障；基于轨迹的评估能一致定位行为故障，发现状态跟踪不一致在失败运行中高2.7倍，降低成功概率49%。

Conclusion: 需要向轨迹级可解释性转变，以评估和诊断自主AI行为，智能体系统需要专门设计的解释方法来理解其随时间展开的行为。

Abstract: Over the last decade, explainable AI has primarily focused on interpreting individual model predictions, producing post-hoc explanations that relate inputs to outputs under a fixed decision structure. Recent advances in large language models (LLMs) have enabled agentic AI systems whose behaviour unfolds over multi-step trajectories. In these settings, success and failure are determined by sequences of decisions rather than a single output. While useful, it remains unclear how explanation approaches designed for static predictions translate to agentic settings where behaviour emerges over time. In this work, we bridge the gap between static and agentic explainability by comparing attribution-based explanations with trace-based diagnostics across both settings. To make this distinction explicit, we empirically compare attribution-based explanations used in static classification tasks with trace-based diagnostics used in agentic benchmarks (TAU-bench Airline and AssistantBench). Our results show that while attribution methods achieve stable feature rankings in static settings (Spearman $ρ= 0.86$), they cannot be applied reliably to diagnose execution-level failures in agentic trajectories. In contrast, trace-grounded rubric evaluation for agentic settings consistently localizes behaviour breakdowns and reveals that state tracking inconsistency is 2.7$\times$ more prevalent in failed runs and reduces success probability by 49\%. These findings motivate a shift towards trajectory-level explainability for agentic systems when evaluating and diagnosing autonomous AI behaviour.
  Resources:
  https://github.com/VectorInstitute/unified-xai-evaluation-framework https://vectorinstitute.github.io/unified-xai-evaluation-framework

</details>


### [241] [AIRS-Bench: a Suite of Tasks for Frontier AI Research Science Agents](https://arxiv.org/abs/2602.06855)
*Alisia Lupidi,Bhavul Gauri,Thomas Simon Foster,Bassel Al Omari,Despoina Magka,Alberto Pepe,Alexis Audran-Reiss,Muna Aghamelu,Nicolas Baldwin,Lucia Cipolina-Kun,Jean-Christophe Gagnon-Audet,Chee Hau Leow,Sandra Lefdal,Hossam Mossalam,Abhinav Moudgil,Saba Nazir,Emanuel Tewolde,Isabel Urrego,Jordi Armengol Estape,Amar Budhiraja,Gaurav Chaurasia,Abhishek Charnalia,Derek Dunfield,Karen Hambardzumyan,Daniel Izcovich,Martin Josifoski,Ishita Mediratta,Kelvin Niu,Parth Pathak,Michael Shvartsman,Edan Toledo,Anton Protopopov,Roberta Raileanu,Alexander Miller,Tatiana Shavrina,Jakob Foerster,Yoram Bachrach*

Main category: cs.AI

TL;DR: AIRS-Bench是一个包含20个任务的AI研究科学基准，涵盖机器学习多个领域，评估智能体在整个研究生命周期中的能力，结果显示智能体在4个任务上超越人类SOTA，但在16个任务上未能达到。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在科学研究中具有巨大潜力，但缺乏评估其在完整研究生命周期中能力的基准。为了加速这一领域的发展，需要建立一个能够评估智能体从想法生成到实验分析再到迭代优化全过程的基准测试。

Method: 从最先进的机器学习论文中选取20个任务，涵盖语言建模、数学、生物信息学、时间序列预测等多个领域。任务格式灵活，便于集成新任务和比较不同智能体框架。使用前沿模型配合顺序和并行架构建立基线。

Result: 智能体在4个任务上超越了人类SOTA，但在16个任务上未能达到。即使智能体超越人类基准，也未能达到底层任务的理论性能上限。这表明AIRS-Bench远未饱和，有巨大的改进空间。

Conclusion: AIRS-Bench是一个有效的科学研究智能体评估基准，显示了当前智能体在科学研究中的局限性，为自主科学研究的发展提供了重要的评估工具和改进方向。

Abstract: LLM agents hold significant promise for advancing scientific research. To accelerate this progress, we introduce AIRS-Bench (the AI Research Science Benchmark), a suite of 20 tasks sourced from state-of-the-art machine learning papers. These tasks span diverse domains, including language modeling, mathematics, bioinformatics, and time series forecasting. AIRS-Bench tasks assess agentic capabilities over the full research lifecycle -- including idea generation, experiment analysis and iterative refinement -- without providing baseline code. The AIRS-Bench task format is versatile, enabling easy integration of new tasks and rigorous comparison across different agentic frameworks. We establish baselines using frontier models paired with both sequential and parallel scaffolds. Our results show that agents exceed human SOTA in four tasks but fail to match it in sixteen others. Even when agents surpass human benchmarks, they do not reach the theoretical performance ceiling for the underlying tasks. These findings indicate that AIRS-Bench is far from saturated and offers substantial room for improvement. We open-source the AIRS-Bench task definitions and evaluation code to catalyze further development in autonomous scientific research.

</details>


### [242] [Agentic Uncertainty Reveals Agentic Overconfidence](https://arxiv.org/abs/2602.06948)
*Jean Kaddour,Srijan Patel,Gbètondji Dovonon,Leo Richter,Pasquale Minervini,Matt J. Kusner*

Main category: cs.AI

TL;DR: AI代理在预测任务成功率时存在过度自信，即使实际成功率仅22%也会预测77%成功。有趣的是，任务执行前的预测比执行后的评估更有区分度，而对抗性提示的bug发现方法能获得最佳校准效果。


<details>
  <summary>Details</summary>
Motivation: 研究AI代理是否能准确预测自己的任务成功率，探索代理在执行任务前、中、后的不确定性评估能力，以及不同评估方法的有效性。

Method: 通过让AI代理在执行任务前、中、后提供成功率概率估计，比较不同时间点的预测准确性。同时测试了对抗性提示方法，将评估重新定义为bug发现过程。

Result: 发现AI代理存在明显的过度自信现象：一些实际成功率仅22%的代理预测成功率高达77%。任务执行前的预测比执行后的评估具有更好的区分度（尽管差异不总是显著）。对抗性提示的bug发现方法实现了最佳校准效果。

Conclusion: AI代理在评估自身任务成功率时存在系统性过度自信，但通过调整评估框架（如采用对抗性bug发现方法）可以改善校准效果，任务执行前的预测可能比传统的事后评估更有信息价值。

Abstract: Can AI agents predict whether they will succeed at a task? We study agentic uncertainty by eliciting success probability estimates before, during, and after task execution. All results exhibit agentic overconfidence: some agents that succeed only 22% of the time predict 77% success. Counterintuitively, pre-execution assessment with strictly less information tends to yield better discrimination than standard post-execution review, though differences are not always significant. Adversarial prompting reframing assessment as bug-finding achieves the best calibration.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [243] [Code, Capital, and Clusters: Understanding Firm Performance in the UK AI Economy](https://arxiv.org/abs/2602.06249)
*Waqar Muhammad Ashraf,Diane Coyle,Ramit Debnath*

Main category: cs.CY

TL;DR: 英国AI产业高度集中于伦敦和科技行业，企业规模和AI专业化程度是主要收入驱动因素，本地社会经济条件对AI增长有重要影响。预测显示行业将进入缓慢扩张和整合阶段，需要针对性的区域政策干预。


<details>
  <summary>Details</summary>
Motivation: 虽然英国在全球AI领域建立了独特地位，但AI专业化、本地社会经济条件与企业绩效之间的相互作用尚未得到充分探索。本研究旨在填补这一空白，分析英国AI实体的地理分布、行业特征和绩效驱动因素。

Method: 使用Companies House、ONS和glass.ai的综合数据集（2000-2024年），分析英国AI实体的地理分布、行业特征和绩效驱动因素。采用统计模型识别收入驱动因素，并建立预测模型估计到2030年的行业发展趋势。

Result: 发现AI产业高度集中于伦敦（占实体的41.3%）和科技行业；企业规模和AI专业化强度是主要收入驱动因素；本地因素（三级资格率、人口密度、就业水平）对AI增长有显著边际贡献；预测到2030年将有4,651个实体，解散率上升至2.21%，表明行业进入缓慢扩张和整合阶段。

Conclusion: 需要基于地方敏感性的政策干预：在伦敦以外培育区域AI能力以降低系统性风险；区分支持规模化（解决资本缺口）和深化技术专业化；战略性地引导生态系统整合。针对性措施对促进AI总体增长和平衡区域发展至关重要。

Abstract: The UK has established a distinctive position in the global AI landscape, driven by rapid firm formation and strategic investment. However, the interplay between AI specialisation, local socioeconomic conditions, and firm performance remains underexplored. This study analyses a comprehensive dataset of UK AI entities (2000 - 2024) from Companies House, ONS, and glass.ai. We find a strong geographical concentration in London (41.3 percent of entities) and technology-centric sectors, with general financial services reporting the highest mean operating revenue (33.9 million GBP, n=33). Firm size and AI specialisation intensity are primary revenue drivers, while local factors, Level 3 qualification rates, population density, and employment levels, provide significant marginal contributions, highlighting the dependence of AI growth on regional socioeconomic ecosystems. The forecasting models project sectoral expansion to 2030, estimating 4,651 [4,323 - 4,979, 95 percent CI] total entities and a rising dissolution ratio (2.21 percent [-0.17 - 4.60]), indicating a transition toward slower sector expansion and consolidation. These results provide robust evidence for place-sensitive policy interventions: cultivating regional AI capabilities beyond London to mitigate systemic risks; distinguishing between support for scaling (addressing capital gaps) and deepening technical specialisation; and strategically shaping ecosystem consolidation. Targeted actions are essential to foster both aggregate AI growth and balanced regional development, transforming consolidation into sustained competitive advantage.

</details>


### [244] [Do LLMs Track Public Opinion? A Multi-Model Study of Favorability Predictions in the 2024 U.S. Presidential Election](https://arxiv.org/abs/2602.06302)
*Riya Parikh,Sarah H. Cen,Chara Podimata*

Main category: cs.CY

TL;DR: LLMs在2024年美国总统选举期间无法准确追踪出口民调，对候选人的好感度预测存在系统性偏差，特别是对Kamala Harris普遍高估10-40%，对Donald Trump偏差较小但仍有5-10%误差。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型（LLMs）是否能够像出口民调那样准确追踪公众舆论，特别是在2024年美国总统选举周期中，评估LLMs作为选举预测工具的可靠性。

Method: 使用公开的llm-election-data-2024数据集，每天查询九个LLM配置，评估其对总统候选人头条好感度（"有利" vs "不利"）的预测，并与路透社、CNN、盖洛普、昆尼皮亚克、ABC等五大高质量民调机构的数据进行对比。

Result: 发现系统性方向性误校准：对于Kamala Harris，所有模型相对于民调高估好感度10-40%；对于Donald Trump，偏差较小（5-10%）但依赖具体民调，模型间差异显著较低。这些偏差在时间平滑和互联网增强检索后仍然存在。

Conclusion: 现成的LLMs在直接查询时无法可靠地追踪民调数据，这对选举预测应用提出了重要警示，需要更复杂的校准方法才能用于选举预测。

Abstract: We investigate whether Large Language Models (LLMs) can track public opinion as measured by exit polls during the 2024 U.S. presidential election cycle. Our analysis focuses on headline favorability (e.g., "Favorable" vs. "Unfavorable") of presidential candidates across multiple LLMs queried daily throughout the election season. Using the publicly available llm-election-data-2024 dataset, we evaluate predictions from nine LLM configurations against a curated set of five high-quality polls from major organizations including Reuters, CNN, Gallup, Quinnipiac, and ABC. We find systematic directional miscalibration. For Kamala Harris, all models overpredict favorability by 10-40% relative to polls. For Donald Trump, biases are smaller (5-10%) and poll-dependent, with substantially lower cross-model variation. These deviations persist under temporal smoothing and are not corrected by internet-augmented retrieval. We conclude that off-the-shelf LLMs do not reliably track polls when queried in a straightforward manner and discuss implications for election forecasting.

</details>


### [245] [Bilingual Bias in Large Language Models: A Taiwan Sovereignty Benchmark Study](https://arxiv.org/abs/2602.06371)
*Ju-Chun Ko*

Main category: cs.CY

TL;DR: 研究发现大型语言模型在涉及台湾主权等政治敏感话题时存在显著的语言偏见，中文查询与英文查询会得到不同的政治立场回应。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在多语言环境中部署日益广泛，但它们在政治敏感话题上的跨语言一致性尚未得到充分研究。本研究旨在系统评估LLMs在中文和英文查询下对台湾主权问题的回应差异。

Method: 对17个大型语言模型进行双语基准测试，比较它们在中文和英文查询下对台湾主权问题的回应。提出了语言偏见分数（LBS）和质量调整一致性（QAC）等新指标来量化语言偏见和一致性。

Result: 17个测试模型中，15个表现出可测量的语言偏见。中文模型问题尤为严重，包括完全拒绝回答或明确传播中国共产党叙事。只有GPT-4o Mini在两种语言中都获得了完美的10/10分。

Conclusion: 大型语言模型在政治敏感话题上存在显著的语言偏见，需要开发更公平、一致的多语言模型。研究提供了开源基准和评估框架，支持可重复性和社区扩展。

Abstract: Large Language Models (LLMs) are increasingly deployed in multilingual contexts, yet their consistency across languages on politically sensitive topics remains understudied. This paper presents a systematic bilingual benchmark study examining how 17 LLMs respond to questions concerning the sovereignty of the Republic of China (Taiwan) when queried in Chinese versus English. We discover significant language bias -- the phenomenon where the same model produces substantively different political stances depending on the query language. Our findings reveal that 15 out of 17 tested models exhibit measurable language bias, with Chinese-origin models showing particularly severe issues including complete refusal to answer or explicit propagation of Chinese Communist Party (CCP) narratives. Notably, only GPT-4o Mini achieves a perfect 10/10 score in both languages. We propose novel metrics for quantifying language bias and consistency, including the Language Bias Score (LBS) and Quality-Adjusted Consistency (QAC). Our benchmark and evaluation framework are open-sourced to enable reproducibility and community extension.

</details>


### [246] [Estimating Exam Item Difficulty with LLMs: A Benchmark on Brazil's ENEM Corpus](https://arxiv.org/abs/2602.06631)
*Thiago Brant,Julien Kühn,Jun Pang*

Main category: cs.CY

TL;DR: LLMs在评估自生成问题难度方面表现有限，存在系统性低估难度、多模态项目表现差、上下文适应能力不足等问题，建议采用"评估优先于生成"的负责任评估设计流程。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs越来越多地用于生成教育内容，一个关键的安全问题出现：这些模型能否可靠地估计它们生成的问题的难度？本研究旨在评估LLMs在难度估计方面的可靠性。

Method: 使用巴西高风险的ENEM考试作为测试平台，对10个专有和开源权重的LLMs进行基准测试，使用1,031个问题的官方项目反应理论(IRT)参数。从三个维度评估：绝对校准、排名保真度和跨学习者背景的上下文敏感性。

Result: 最佳模型仅达到中等排名相关性，但系统性低估难度，在多模态项目上表现显著下降。模型在提示学生人口统计线索时表现出有限且不一致的可塑性，表明它们尚未准备好进行上下文自适应个性化。

Conclusion: LLMs最适合作为校准筛选器而非权威预言家，支持采用"评估优先于生成"的管道进行负责任的评估设计。

Abstract: As Large Language Models (LLMs) are increasingly deployed to generate educational content, a critical safety question arises: can these models reliably estimate the difficulty of the questions they produce? Using Brazil's high-stakes ENEM exam as a testbed, we benchmark ten proprietary and open-weight LLMs against official Item Response Theory (IRT) parameters for 1,031 questions. We evaluate performance along three axes: absolute calibration, rank fidelity, and context sensitivity across learner backgrounds. Our results reveal a significant trade-off: while the best models achieve moderate rank correlation, they systematically underestimate difficulty and degrade significantly on multimodal items. Crucially, we find that models exhibit limited and inconsistent plasticity when prompted with student demographic cues, suggesting they are not yet ready for context-adaptive personalization. We conclude that LLMs function best as calibrated screeners rather than authoritative oracles, supporting an "evaluation-before-generation" pipeline for responsible assessment design.

</details>
