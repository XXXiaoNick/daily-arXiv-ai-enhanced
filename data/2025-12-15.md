<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 31]
- [cs.LG](#cs.LG) [Total: 61]
- [q-fin.RM](#q-fin.RM) [Total: 1]
- [math.OC](#math.OC) [Total: 9]
- [q-fin.MF](#q-fin.MF) [Total: 1]
- [stat.ML](#stat.ML) [Total: 6]
- [econ.EM](#econ.EM) [Total: 2]
- [eess.SY](#eess.SY) [Total: 14]
- [cs.AI](#cs.AI) [Total: 18]
- [cs.CY](#cs.CY) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [ASR Under the Stethoscope: Evaluating Biases in Clinical Speech Recognition across Indian Languages](https://arxiv.org/abs/2512.10967)
*Subham Kumar,Prakrithi Shivaprakash,Abhishek Manoharan,Astut Kurariya,Diptadhi Mukherjee,Lekhansh Shukla,Animesh Mukherjee,Prabhat Chand,Pratima Murthy*

Main category: cs.CL

TL;DR: 首次系统评估印度多语言临床环境中ASR性能，发现模型间差异大，存在基于语言、说话者角色和性别的不公平性


<details>
  <summary>Details</summary>
Motivation: ASR在临床记录中应用日益广泛，但在印度多语言和人口多样化医疗环境中的可靠性未知，需要评估其公平性和准确性

Method: 使用真实世界临床访谈数据（卡纳达语、印地语、印度英语），比较Indic Whisper、Whisper、Sarvam、Google speech to text、Gemma3n、Omnilingual、Vaani和Gemini等领先模型，评估转录准确性、错误模式、患者与医生差异、性别和交叉性差异

Result: 模型和语言间性能差异显著，某些系统在印度英语上表现良好但在混合语言或方言语音上失败，发现与说话者角色和性别相关的系统性性能差距

Conclusion: 研究强调印度医疗生态系统需要文化和人口包容性的ASR开发，提供了全面的多语言基准和公平性分析

Abstract: Automatic Speech Recognition (ASR) is increasingly used to document clinical encounters, yet its reliability in multilingual and demographically diverse Indian healthcare contexts remains largely unknown. In this study, we conduct the first systematic audit of ASR performance on real world clinical interview data spanning Kannada, Hindi, and Indian English, comparing leading models including Indic Whisper, Whisper, Sarvam, Google speech to text, Gemma3n, Omnilingual, Vaani, and Gemini. We evaluate transcription accuracy across languages, speakers, and demographic subgroups, with a particular focus on error patterns affecting patients vs. clinicians and gender based or intersectional disparities. Our results reveal substantial variability across models and languages, with some systems performing competitively on Indian English but failing on code mixed or vernacular speech. We also uncover systematic performance gaps tied to speaker role and gender, raising concerns about equitable deployment in clinical settings. By providing a comprehensive multilingual benchmark and fairness analysis, our work highlights the need for culturally and demographically inclusive ASR development for healthcare ecosystem in India.

</details>


### [2] [Benchmarking Automatic Speech Recognition Models for African Languages](https://arxiv.org/abs/2512.10968)
*Alvin Nahabwe,Sulaiman Kagumire,Denis Musinguzi,Bruno Beijuka,Jonah Mubuuke Kyagaba,Peter Nabende,Andrew Katumba,Joyce Nakatumba-Nabende*

Main category: cs.CL

TL;DR: 该研究系统性地评估了四种最先进的ASR模型在13种非洲语言上的表现，通过在不同数据量（1-400小时）下进行微调，揭示了各模型在不同资源条件下的优势和适用场景。


<details>
  <summary>Details</summary>
Motivation: 非洲语言的自动语音识别面临标注数据有限、缺乏系统指导的挑战。虽然已有大型预训练系统如Whisper、XLS-R、MMS和W2v-BERT，但它们在非洲低资源环境下的比较行为尚未得到统一和系统研究。

Method: 在13种非洲语言上对四种最先进的ASR模型进行基准测试，使用转录数据量从1到400小时的逐步增大的子集进行微调。除了报告错误率外，还分析了外部语言模型解码的效果。

Result: MMS和W2v-BERT在极低资源条件下数据效率更高；XLS-R在数据增加时扩展性更好；Whisper在中等资源条件下表现优势。外部语言模型解码在某些情况下能提升性能，但在声学和文本资源对齐不佳时可能带来额外错误。

Conclusion: 该研究通过分析预训练覆盖度、模型架构、数据集领域和资源可用性之间的相互作用，为代表性不足语言的ASR系统设计提供了实用见解和指导。

Abstract: Automatic speech recognition (ASR) for African languages remains constrained by limited labeled data and the lack of systematic guidance on model selection, data scaling, and decoding strategies. Large pre-trained systems such as Whisper, XLS-R, MMS, and W2v-BERT have expanded access to ASR technology, but their comparative behavior in African low-resource contexts has not been studied in a unified and systematic way. In this work, we benchmark four state-of-the-art ASR models across 13 African languages, fine-tuning them on progressively larger subsets of transcribed data ranging from 1 to 400 hours. Beyond reporting error rates, we provide new insights into why models behave differently under varying conditions. We show that MMS and W2v-BERT are more data efficient in very low-resource regimes, XLS-R scales more effectively as additional data becomes available, and Whisper demonstrates advantages in mid-resource conditions. We also analyze where external language model decoding yields improvements and identify cases where it plateaus or introduces additional errors, depending on the alignment between acoustic and text resources. By highlighting the interaction between pre-training coverage, model architecture, dataset domain, and resource availability, this study offers practical and insights into the design of ASR systems for underrepresented languages.

</details>


### [3] [MedBioRAG: Semantic Search and Retrieval-Augmented Generation with Large Language Models for Medical and Biological QA](https://arxiv.org/abs/2512.10996)
*Seonok Kim*

Main category: cs.CL

TL;DR: MedBioRAG是一个用于生物医学问答的检索增强生成模型，通过语义和词汇搜索结合文档检索与监督微调，在多个生物医学基准测试中超越现有最佳模型和GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成（RAG）显著提升了大型语言模型在复杂问答任务中的能力，但在生物医学领域仍有改进空间。需要专门针对生物医学QA设计的高效检索和生成模型。

Method: 结合语义搜索和词汇搜索进行文档检索，采用监督微调方法，构建MedBioRAG模型。模型能够高效检索和排序相关生物医学文档，生成精确且上下文感知的回答。

Result: 在NFCorpus、TREC-COVID、MedQA、PubMedQA和BioASQ等基准测试中，MedBioRAG在文本检索、封闭式QA和长格式QA任务上均优于之前的最先进模型和GPT-4o基础模型。特别是在文档检索的NDCG和MRR分数、封闭式QA的准确率以及长格式QA的ROUGE分数方面表现突出。

Conclusion: 研究证明了基于语义搜索的检索和LLM微调在生物医学应用中的有效性，MedBioRAG为生物医学问答任务提供了强大的解决方案。

Abstract: Recent advancements in retrieval-augmented generation (RAG) have significantly enhanced the ability of large language models (LLMs) to perform complex question-answering (QA) tasks. In this paper, we introduce MedBioRAG, a retrieval-augmented model designed to improve biomedical QA performance through a combination of semantic and lexical search, document retrieval, and supervised fine-tuning. MedBioRAG efficiently retrieves and ranks relevant biomedical documents, enabling precise and context-aware response generation. We evaluate MedBioRAG across text retrieval, close-ended QA, and long-form QA tasks using benchmark datasets such as NFCorpus, TREC-COVID, MedQA, PubMedQA, and BioASQ. Experimental results demonstrate that MedBioRAG outperforms previous state-of-the-art (SoTA) models and the GPT-4o base model in all evaluated tasks. Notably, our approach improves NDCG and MRR scores for document retrieval, while achieving higher accuracy in close-ended QA and ROUGE scores in long-form QA. Our findings highlight the effectiveness of semantic search-based retrieval and LLM fine-tuning in biomedical applications.

</details>


### [4] [Applying NLP to iMessages: Understanding Topic Avoidance, Responsiveness, and Sentiment](https://arxiv.org/abs/2512.11079)
*Alan Gerber,Sam Cooperman*

Main category: cs.CL

TL;DR: 该论文开发了一个iMessage文本消息分析器，用于分析本地存储的iMessage数据，探索主题建模、回复时间、不情愿评分和情感分析等五个主要研究问题。


<details>
  <summary>Details</summary>
Motivation: 随着社会对短格式电子通信的依赖增加，用户很少考虑消息平台收集的数据用途。苹果为Mac上的iMessage用户提供了一个本地存储所有消息和元数据的文件，这为分析个人消息数据创造了机会。

Method: 开发了一个iMessage文本消息分析器，通过分析本地存储的iMessage数据文件，进行主题建模、回复时间分析、不情愿评分和情感分析等五个主要研究方向的探索性数据分析。

Result: 论文展示了如何使用该分析器回答五个研究问题，证明了分析个人iMessage数据的可行性，并为未来iMessage数据研究提供了潜在应用。

Conclusion: 该iMessage分析器能够有效挖掘个人消息数据中的有价值信息，为理解通信模式、情感动态等提供了工具，具有未来研究的潜力。

Abstract: What is your messaging data used for? While many users do not often think about the information companies can gather based off of their messaging platform of choice, it is nonetheless important to consider as society increasingly relies on short-form electronic communication. While most companies keep their data closely guarded, inaccessible to users or potential hackers, Apple has opened a door to their walled-garden ecosystem, providing iMessage users on Mac with one file storing all their messages and attached metadata. With knowledge of this locally stored file, the question now becomes: What can our data do for us? In the creation of our iMessage text message analyzer, we set out to answer five main research questions focusing on topic modeling, response times, reluctance scoring, and sentiment analysis. This paper uses our exploratory data to show how these questions can be answered using our analyzer and its potential in future studies on iMessage data.

</details>


### [5] [KBQA-R1: Reinforcing Large Language Models for Knowledge Base Question Answering](https://arxiv.org/abs/2512.10999)
*Xin Sun,Zhongqi Chen,Xing Zheng,Qiang Liu,Shu Wu,Bowen Song,Zilei Wang,Weiqiang Wang,Liang Wang*

Main category: cs.CL

TL;DR: KBQA-R1：通过强化学习将知识库问答从文本模仿转向交互优化，使用组相对策略优化和参考拒绝采样，在多个基准上达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 当前KBQA方法存在两种失败模式：要么生成幻觉查询而不验证模式存在性，要么采用僵化的模板推理模仿合成轨迹而缺乏对环境真正理解。需要解决这些限制，将KBQA从文本模仿转向交互优化。

Method: 提出KBQA-R1框架，将KBQA视为多轮决策过程，使用强化学习让模型学习在知识库中导航。采用组相对策略优化(GRPO)基于具体执行反馈而非静态监督来优化策略。引入参考拒绝采样(RRS)数据合成方法，通过严格对齐推理轨迹与真实动作序列解决冷启动问题。

Result: 在WebQSP、GrailQA和GraphQuestions等基准上进行广泛实验，KBQA-R1实现了最先进的性能，有效将LLM推理基于可验证的执行。

Conclusion: KBQA-R1通过强化学习范式转变，解决了当前KBQA方法的局限性，实现了从文本模仿到交互优化的转变，显著提升了知识库问答的性能和可靠性。

Abstract: Knowledge Base Question Answering (KBQA) challenges models to bridge the gap between natural language and strict knowledge graph schemas by generating executable logical forms. While Large Language Models (LLMs) have advanced this field, current approaches often struggle with a dichotomy of failure: they either generate hallucinated queries without verifying schema existence or exhibit rigid, template-based reasoning that mimics synthesized traces without true comprehension of the environment. To address these limitations, we present \textbf{KBQA-R1}, a framework that shifts the paradigm from text imitation to interaction optimization via Reinforcement Learning. Treating KBQA as a multi-turn decision process, our model learns to navigate the knowledge base using a list of actions, leveraging Group Relative Policy Optimization (GRPO) to refine its strategies based on concrete execution feedback rather than static supervision. Furthermore, we introduce \textbf{Referenced Rejection Sampling (RRS)}, a data synthesis method that resolves cold-start challenges by strictly aligning reasoning traces with ground-truth action sequences. Extensive experiments on WebQSP, GrailQA, and GraphQuestions demonstrate that KBQA-R1 achieves state-of-the-art performance, effectively grounding LLM reasoning in verifiable execution.

</details>


### [6] [Mining Legal Arguments to Study Judicial Formalism](https://arxiv.org/abs/2512.11374)
*Tomáš Koref,Lena Held,Mahammad Namazov,Harun Kumru,Yassine Thlija,Christoph Burchard,Ivan Habernal*

Main category: cs.CL

TL;DR: 本研究开发了自动化方法检测捷克最高法院判决中的司法推理，挑战了中东欧形式主义司法的传统观点，通过NLP技术实现了82.6%的论证段落检测和83.2%的形式主义分类准确率。


<details>
  <summary>Details</summary>
Motivation: 针对中东欧司法形式主义的传统观点缺乏系统验证的问题，需要开发自动化方法来大规模分析司法推理，以科学评估司法决策的实质内容。

Method: 创建MADON数据集（272个捷克最高法院判决，9,183个段落标注），采用三阶段流水线：ModernBERT进行段落分类，Llama 3.1进行论证类型分类，传统机器学习进行决策分类，并采用不对称损失和类别加权处理数据不平衡。

Result: 最佳模型在论证段落检测上达到82.6%宏F1，法律论证类型分类达到77.5%宏F1，形式主义/非形式主义决策分类达到83.2%宏F1，成功挑战了中东欧司法形式主义的传统叙事。

Conclusion: 法律论证挖掘能够可靠地进行司法哲学分类，该方法可跨司法管辖区复制，为计算法律研究的其他重要任务展示了潜力，所有数据集、模型和源代码均已开源。

Abstract: Courts must justify their decisions, but systematically analyzing judicial reasoning at scale remains difficult. This study refutes claims about formalistic judging in Central and Eastern Europe (CEE) by developing automated methods to detect and classify judicial reasoning in Czech Supreme Courts' decisions using state-of-the-art natural language processing methods. We create the MADON dataset of 272 decisions from two Czech Supreme Courts with expert annotations of 9,183 paragraphs with eight argument types and holistic formalism labels for supervised training and evaluation. Using a corpus of 300k Czech court decisions, we adapt transformer LLMs for Czech legal domain by continued pretraining and experiment with methods to address dataset imbalance including asymmetric loss and class weighting. The best models successfully detect argumentative paragraphs (82.6\% macro-F1), classify traditional types of legal argument (77.5\% macro-F1), and classify decisions as formalistic/non-formalistic (83.2\% macro-F1). Our three-stage pipeline combining ModernBERT, Llama 3.1, and traditional feature-based machine learning achieves promising results for decision classification while reducing computational costs and increasing explainability. Empirically, we challenge prevailing narratives about CEE formalism. This work shows that legal argument mining enables reliable judicial philosophy classification and shows the potential of legal argument mining for other important tasks in computational legal studies. Our methodology is easily replicable across jurisdictions, and our entire pipeline, datasets, guidelines, models, and source codes are available at https://github.com/trusthlt/madon.

</details>


### [7] [PIAST: Rapid Prompting with In-context Augmentation for Scarce Training data](https://arxiv.org/abs/2512.11013)
*Pawel Batorski,Paul Swoboda*

Main category: cs.CL

TL;DR: 提出一种快速自动提示构建算法，通过蒙特卡洛Shapley估计示例效用，迭代替换/删除/保留少样本示例来增强人类指令


<details>
  <summary>Details</summary>
Motivation: LLMs对提示设计高度敏感，但手工制作有效提示困难且需要精心设计少样本示例，需要自动化的提示工程方法

Method: 使用蒙特卡洛Shapley估计评估少样本示例的效用，迭代进行替换/删除/保留操作，采用积极子采样和重放缓冲区加速评估，支持不同计算时间预算

Result: 在有限预算下，在文本简化和GSM8K上优于现有自动提示方法，在分类和摘要上获得第二好结果；在适度扩展预算下，在分类、简化和GSM8K上达到自动提示方法的新SOTA

Conclusion: 精心构建的示例而非详尽的指令搜索是快速和数据高效提示工程的主要杠杆，证明了示例选择的重要性

Abstract: LLMs are highly sensitive to prompt design, but handcrafting effective prompts is difficult and often requires intricate crafting of few-shot examples. We propose a fast automatic prompt construction algorithm that augments human instructions by generating a small set of few shot examples. Our method iteratively replaces/drops/keeps few-shot examples using Monte Carlo Shapley estimation of example utility. For faster execution, we use aggressive subsampling and a replay buffer for faster evaluations. Our method can be run using different compute time budgets. On a limited budget, we outperform existing automatic prompting methods on text simplification and GSM8K and obtain second best results on classification and summarization. With an extended, but still modest compute budget we set a new state of the art among automatic prompting methods on classification, simplification and GSM8K. Our results show that carefully constructed examples, rather than exhaustive instruction search, are the dominant lever for fast and data efficient prompt engineering. Our code is available at https://github.com/Batorskq/PIAST.

</details>


### [8] [MultiScript30k: Leveraging Multilingual Embeddings to Extend Cross Script Parallel Data](https://arxiv.org/abs/2512.11074)
*Christopher Driggers-Ellis,Detravious Brinkley,Ray Chen,Aashish Dhawan,Daisy Zhe Wang,Christan Grant*

Main category: cs.CL

TL;DR: 提出MultiScript30k数据集，将Multi30k扩展到阿拉伯语、西班牙语、乌克兰语和简体/繁体中文，解决原数据集仅支持欧洲语言的问题。


<details>
  <summary>Details</summary>
Motivation: Multi30k数据集仅支持捷克语、英语、法语和德语四种欧洲语言，限制了多模态机器翻译研究在多样化语言上的发展。现有扩展支持的语言、语系和文字仍然非常有限。

Method: 使用NLLB200-3.3B模型将Multi30k的英文版本翻译成阿拉伯语、西班牙语、乌克兰语、简体中文和繁体中文，创建包含超过30000个句子的新数据集。

Result: 相似性分析显示，除繁体中文外，所有语言都达到大于0.8的余弦相似度和小于0.000251的对称KL散度。COMETKiwi评分显示与相关工作相比结果不一：阿拉伯语版本与ArEnMulti30k相当，但乌克兰语版本比Multi30k-Uk低6.4%。

Conclusion: MultiScript30k为全球多种文字的语言提供了扩展的多模态翻译数据集，有助于推动多模态机器翻译在非欧洲语言上的研究。

Abstract: Multi30k is frequently cited in the multimodal machine translation (MMT) literature, offering parallel text data for training and fine-tuning deep learning models. However, it is limited to four languages: Czech, English, French, and German. This restriction has led many researchers to focus their investigations only on these languages. As a result, MMT research on diverse languages has been stalled because the official Multi30k dataset only represents European languages in Latin scripts. Previous efforts to extend Multi30k exist, but the list of supported languages, represented language families, and scripts is still very short. To address these issues, we propose MultiScript30k, a new Multi30k dataset extension for global languages in various scripts, created by translating the English version of Multi30k (Multi30k-En) using NLLB200-3.3B. The dataset consists of over \(30000\) sentences and provides translations of all sentences in Multi30k-En into Ar, Es, Uk, Zh\_Hans and Zh\_Hant. Similarity analysis shows that Multi30k extension consistently achieves greater than \(0.8\) cosine similarity and symmetric KL divergence less than \(0.000251\) for all languages supported except Zh\_Hant which is comparable to the previous Multi30k extensions ArEnMulti30k and Multi30k-Uk. COMETKiwi scores reveal mixed assessments of MultiScript30k as a translation of Multi30k-En in comparison to the related work. ArEnMulti30k scores nearly equal MultiScript30k-Ar, but Multi30k-Uk scores $6.4\%$ greater than MultiScript30k-Uk per split.

</details>


### [9] [Explanation Bias is a Product: Revealing the Hidden Lexical and Position Preferences in Post-Hoc Feature Attribution](https://arxiv.org/abs/2512.11108)
*Jonathan Kamp,Roos Bakker,Dominique Blok*

Main category: cs.CL

TL;DR: 该研究提出一个模型和方法无关的框架，通过三个评估指标系统分析特征归因方法的词汇和位置偏见，发现不同方法在相同输入上的解释存在显著差异，且偏见结构不平衡。


<details>
  <summary>Details</summary>
Motivation: 特征归因方法（如集成梯度）作为后验解释器，能够提供token级别的洞察，但不同方法在相同输入上的解释差异很大，这可能导致用户不信任或过度信任这些解释。需要超越表面不一致性，结构化地分析这些方法的偏见。

Method: 提出一个模型和方法无关的评估框架，包含三个评估指标，系统评估词汇偏见（what）和位置偏见（where）。首先在人工数据的受控伪随机分类任务上测试两个transformer模型，然后在自然数据的半受控因果关系检测任务上进行评估。

Result: 发现词汇偏见和位置偏见在模型比较中结构不平衡，一个得分高的模型在另一个方面得分低。还发现产生异常解释的方法更可能自身存在偏见。

Conclusion: 特征归因方法存在系统性偏见，需要通过结构化框架进行评估。研究为理解和比较不同解释方法的可靠性提供了方法论基础，有助于用户更明智地使用这些解释工具。

Abstract: Good quality explanations strengthen the understanding of language models and data. Feature attribution methods, such as Integrated Gradient, are a type of post-hoc explainer that can provide token-level insights. However, explanations on the same input may vary greatly due to underlying biases of different methods. Users may be aware of this issue and mistrust their utility, while unaware users may trust them inadequately. In this work, we delve beyond the superficial inconsistencies between attribution methods, structuring their biases through a model- and method-agnostic framework of three evaluation metrics. We systematically assess both the lexical and position bias (what and where in the input) for two transformers; first, in a controlled, pseudo-random classification task on artificial data; then, in a semi-controlled causal relation detection task on natural data. We find that lexical and position biases are structurally unbalanced in our model comparison, with models that score high on one type score low on the other. We also find signs that methods producing anomalous explanations are more likely to be biased themselves.

</details>


### [10] [FIBER: A Multilingual Evaluation Resource for Factual Inference Bias](https://arxiv.org/abs/2512.11110)
*Evren Ayberk Munis,Deniz Yılmaz,Arianna Muti,Çağrı Toraman*

Main category: cs.CL

TL;DR: FIBER是一个多语言事实知识评估基准，用于测试LLM在单实体和多实体场景下的表现，发现提示语言会影响模型输出，多实体问题比单实体问题更具挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注单实体事实和单语言数据，缺乏对多语言和多实体场景的系统评估，因此需要构建一个更全面的基准来评估LLM的事实可靠性和偏见问题。

Method: 构建FIBER多语言基准数据集，包含英语、意大利语和土耳其语的句子补全、问答和对象计数预测任务，用于评估LLM在单实体和多实体设置下的表现。

Result: 提示语言会影响模型输出，特别是与语言对应国家相关的实体；31%的主题表现出大于0.5的事实推理偏见分数；土耳其语提示比意大利语在83%的主题中表现出更高偏见；多实体问题比单实体问题更难处理；英语表现最佳，土耳其语和意大利语得分较低；更大模型表现更好。

Conclusion: FIBER基准揭示了LLM在多语言和多实体场景下的局限性，提示语言会影响模型的事实推理，多实体问题更具挑战性，需要进一步改进模型的多语言和多实体处理能力。

Abstract: Large language models are widely used across domains, yet there are concerns about their factual reliability and biases. Factual knowledge probing offers a systematic means to evaluate these aspects. Most existing benchmarks focus on single-entity facts and monolingual data. We therefore present FIBER, a multilingual benchmark for evaluating factual knowledge in single- and multi-entity settings. The dataset includes sentence completion, question-answering, and object-count prediction tasks in English, Italian, and Turkish. Using FIBER, we examine whether the prompt language induces inference bias in entity selection and how large language models perform on multi-entity versus single-entity questions. The results indicate that the language of the prompt can influence the model's generated output, particularly for entities associated with the country corresponding to that language. However, this effect varies across different topics such that 31% of the topics exhibit factual inference bias score greater than 0.5. Moreover, the level of bias differs across languages such that Turkish prompts show higher bias compared to Italian in 83% of the topics, suggesting a language-dependent pattern. Our findings also show that models face greater difficulty when handling multi-entity questions than the single-entity questions. Model performance differs across both languages and model sizes. The highest mean average precision is achieved in English, while Turkish and Italian lead to noticeably lower scores. Larger models, including Llama-3.1-8B and Qwen-2.5-7B, show consistently better performance than smaller 3B-4B models.

</details>


### [11] [SciLaD: A Large-Scale, Transparent, Reproducible Dataset for Natural Scientific Language Processing](https://arxiv.org/abs/2512.11192)
*Luca Foppiano,Sotaro Takeshita,Pedro Ortiz Suarez,Ekaterina Borisova,Raia Abu Ahmad,Malte Ostendorff,Fabio Barth,Julian Moreno-Schneider,Georg Rehm*

Main category: cs.CL

TL;DR: SciLaD是一个使用开源框架和公开数据构建的大规模科学语言数据集，包含超过1000万篇英文科学出版物和3500万篇多语言出版物，并发布了数据集生成管道和预训练模型。


<details>
  <summary>Details</summary>
Motivation: 构建一个完全基于开源工具和公开数据的大规模科学语言数据集，以促进科学语言处理和学术文档处理的研究，同时展示开源工具在大规模科学数据整理中的可行性。

Method: 使用开源框架和公开数据源构建数据集，包括两个部分：1) 包含1000万+科学出版物的精选英文数据集；2) 包含3500万+出版物的多语言未过滤TEI XML数据集。开发了可扩展的数据集生成管道，并在该数据集上预训练了RoBERTa模型。

Result: 创建了SciLaD数据集，包含超过4500万篇科学出版物。预训练的RoBERTa模型在综合基准测试中表现与类似规模的其他科学语言模型相当，验证了数据集的质量和实用性。发布了数据集和评估管道以促进可重复性和进一步研究。

Conclusion: SciLaD展示了开源工具能够实现大规模科学数据整理并保持高质量。该数据集和预训练模型为科学语言处理和学术文档处理研究提供了有价值的资源，促进了该领域的可重复性、透明度和进一步研究。

Abstract: SciLaD is a novel, large-scale dataset of scientific language constructed entirely using open-source frameworks and publicly available data sources. It comprises a curated English split containing over 10 million scientific publications and a multilingual, unfiltered TEI XML split including more than 35 million publications. We also publish the extensible pipeline for generating SciLaD. The dataset construction and processing workflow demonstrates how open-source tools can enable large-scale, scientific data curation while maintaining high data quality. Finally, we pre-train a RoBERTa model on our dataset and evaluate it across a comprehensive set of benchmarks, achieving performance comparable to other scientific language models of similar size, validating the quality and utility of SciLaD. We publish the dataset and evaluation pipeline to promote reproducibility, transparency, and further research in natural scientific language processing and understanding including scholarly document processing.

</details>


### [12] [Multi-Intent Spoken Language Understanding: Methods, Trends, and Challenges](https://arxiv.org/abs/2512.11258)
*Di Wu,Ruiyu Fang,Liting Jiang,Shuangyong Song,Xiaomeng Huang,Shiquan Wang,Zhongqiu Li,Lingling Shi,Mengjiao Bao,Yongxiang Li,Hao Huang*

Main category: cs.CL

TL;DR: 本文对多意图口语理解（SLU）领域进行了系统性综述，涵盖解码范式、建模方法、性能比较，并讨论了当前挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 多意图SLU涉及多个意图检测和槽位填充两个任务，能处理包含多个意图的话语，更贴近实际应用场景。尽管该领域研究日益增多且取得显著进展，但缺乏全面系统的综述。

Method: 从两个视角进行深入综述：解码范式（如序列标注、图神经网络等）和建模方法（如联合学习、多任务学习等）。对代表性模型的性能进行比较分析，并探讨其优缺点。

Result: 提供了多意图SLU领域的全面概述，包括不同方法的性能对比和优劣分析，为研究人员提供了该领域的技术发展现状和趋势。

Conclusion: 总结了当前多意图SLU面临的挑战，并指出了未来研究方向，希望为推进该领域研究提供有价值的见解和参考。

Abstract: Multi-intent spoken language understanding (SLU) involves two tasks: multiple intent detection and slot filling, which jointly handle utterances containing more than one intent. Owing to this characteristic, which closely reflects real-world applications, the task has attracted increasing research attention, and substantial progress has been achieved. However, there remains a lack of a comprehensive and systematic review of existing studies on multi-intent SLU. To this end, this paper presents a survey of recent advances in multi-intent SLU. We provide an in-depth overview of previous research from two perspectives: decoding paradigms and modeling approaches. On this basis, we further compare the performance of representative models and analyze their strengths and limitations. Finally, we discuss the current challenges and outline promising directions for future research. We hope this survey will offer valuable insights and serve as a useful reference for advancing research in multi-intent SLU.

</details>


### [13] [Leveraging LLMs for Title and Abstract Screening for Systematic Review: A Cost-Effective Dynamic Few-Shot Learning Approach](https://arxiv.org/abs/2512.11261)
*Yun-Chung Liu,Rui Yang,Jonathan Chong Kai Liew,Ziran Yin,Henry Foote,Christopher J. Lindsell,Chuan Hong*

Main category: cs.CL

TL;DR: 提出两阶段动态少样本学习(DFSL)方法，使用低成本LLM进行初步筛选，再用高性能LLM重新评估低置信度实例，以提高系统评价中标题和摘要筛选的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 随着研究文献的快速增长，系统评价中的标题和摘要筛选步骤变得极其耗时耗力，需要更高效的方法来减轻这一负担。

Method: 设计了两阶段动态少样本学习(DFSL)方法：第一阶段使用低成本大语言模型进行初步筛选，第二阶段对低置信度实例使用高性能大语言模型重新评估，在控制计算成本的同时提升筛选性能。

Result: 在10个系统评价中评估该方法，结果显示其具有良好的泛化能力和成本效益，能够显著减少人工筛选负担并加速系统评价过程。

Conclusion: 两阶段动态少样本学习方法能够有效提高系统评价中标题和摘要筛选的效率和性能，具有实际应用潜力。

Abstract: Systematic reviews are a key component of evidence-based medicine, playing a critical role in synthesizing existing research evidence and guiding clinical decisions. However, with the rapid growth of research publications, conducting systematic reviews has become increasingly burdensome, with title and abstract screening being one of the most time-consuming and resource-intensive steps. To mitigate this issue, we designed a two-stage dynamic few-shot learning (DFSL) approach aimed at improving the efficiency and performance of large language models (LLMs) in the title and abstract screening task. Specifically, this approach first uses a low-cost LLM for initial screening, then re-evaluates low-confidence instances using a high-performance LLM, thereby enhancing screening performance while controlling computational costs. We evaluated this approach across 10 systematic reviews, and the results demonstrate its strong generalizability and cost-effectiveness, with potential to reduce manual screening burden and accelerate the systematic review process in practical applications.

</details>


### [14] [When Actions Teach You to Think: Reasoning-Action Synergy via Reinforcement Learning in Conversational Agents](https://arxiv.org/abs/2512.11277)
*Mrinal Rawat,Arkajyoti Chakraborty,Neha Gupta,Roberto Pieraccini*

Main category: cs.CL

TL;DR: 本文提出通过强化学习（RL）让语言模型直接从任务结果中学习推理策略，以解决监督微调（SFT）在数据分布变化时泛化能力不足的问题，相比SFT模型获得1.5%的相对提升。


<details>
  <summary>Details</summary>
Motivation: 监督微调（SFT）在下游任务中效果显著，但在数据分布变化时泛化能力有限。现有推理模型（如o1和R1）显示推理能力对泛化和可靠性很重要，但收集高质量推理轨迹成本高、主观性强且难以扩展。需要一种方法让模型直接从任务结果中学习推理策略。

Method: 提出基于强化学习的管道：LLMs生成推理步骤来指导工具调用（如函数调用）和最终答案生成。采用Group Relative Policy Optimization（GRPO）方法，奖励设计围绕工具准确性和答案正确性，让模型迭代优化推理和行动。

Result: 实验结果显示，该方法提高了推理质量和工具调用精度，相比没有显式思考的SFT模型获得1.5%的相对提升，相比基础Qwen3-1.7B模型获得40%的提升。

Conclusion: 通过强化学习统一推理和行动学习，可以构建更强大、更可泛化的对话代理，展示了RL在提升语言模型推理能力方面的潜力。

Abstract: Supervised fine-tuning (SFT) has emerged as one of the most effective ways to improve the performance of large language models (LLMs) in downstream tasks. However, SFT can have difficulty generalizing when the underlying data distribution changes, even when the new data does not fall completely outside the training domain. Recent reasoning-focused models such as o1 and R1 have demonstrated consistent gains over their non-reasoning counterparts, highlighting the importance of reasoning for improved generalization and reliability. However, collecting high-quality reasoning traces for SFT remains challenging -- annotations are costly, subjective, and difficult to scale. To address this limitation, we leverage Reinforcement Learning (RL) to enable models to learn reasoning strategies directly from task outcomes. We propose a pipeline in which LLMs generate reasoning steps that guide both the invocation of tools (e.g., function calls) and the final answer generation for conversational agents. Our method employs Group Relative Policy Optimization (GRPO) with rewards designed around tool accuracy and answer correctness, allowing the model to iteratively refine its reasoning and actions. Experimental results demonstrate that our approach improves both the quality of reasoning and the precision of tool invocations, achieving a 1.5% relative improvement over the SFT model (trained without explicit thinking) and a 40% gain compared to the base of the vanilla Qwen3-1.7B model. These findings demonstrate the promise of unifying reasoning and action learning through RL to build more capable and generalizable conversational agents.

</details>


### [15] [AdaSD: Adaptive Speculative Decoding for Efficient Language Model Inference](https://arxiv.org/abs/2512.11280)
*Kuan-Wei Lu,Ding-Yong Hong,Pangfeng Liu*

Main category: cs.CL

TL;DR: AdaSD是一种无需超参数调优的自适应推测解码方法，通过动态调整生成长度和接受标准，实现LLM推理加速，相比标准推测解码最高提升49%速度，精度损失控制在2%以内。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型参数规模增大导致推理速度显著下降，现有推测解码方法需要额外训练、大量超参数调优或部署前对模型和任务进行分析，限制了实际应用。

Method: 提出自适应推测解码(AdaSD)，引入两个自适应阈值：一个决定何时停止候选token生成，另一个决定token接受标准，均基于token熵和Jensen-Shannon距离实时更新，无需预分析或微调。

Result: 在基准数据集上的实验表明，AdaSD相比标准推测解码最高实现49%的加速，同时将精度损失控制在2%以内，兼容现成模型。

Conclusion: AdaSD提供了一种实用、高效且自适应的LLM推理解决方案，消除了对预分析、微调或超参数调优的需求，在保持精度的同时显著提升推理速度。

Abstract: Large language models (LLMs) have achieved remarkable performance across a wide range of tasks, but their increasing parameter sizes significantly slow down inference. Speculative decoding mitigates this issue by leveraging a smaller draft model to predict candidate tokens, which are then verified by a larger target model. However, existing approaches often require additional training, extensive hyperparameter tuning, or prior analysis of models and tasks before deployment. In this paper, we propose Adaptive Speculative Decoding (AdaSD), a hyperparameter-free decoding scheme that dynamically adjusts generation length and acceptance criteria during inference. AdaSD introduces two adaptive thresholds: one to determine when to stop candidate token generation and another to decide token acceptance, both updated in real time based on token entropy and Jensen-Shannon distance. This approach eliminates the need for pre-analysis or fine-tuning and is compatible with off-the-shelf models. Experiments on benchmark datasets demonstrate that AdaSD achieves up to 49\% speedup over standard speculative decoding while limiting accuracy degradation to under 2\%, making it a practical solution for efficient and adaptive LLM inference.

</details>


### [16] [CIP: A Plug-and-Play Causal Prompting Framework for Mitigating Hallucinations under Long-Context Noise](https://arxiv.org/abs/2512.11282)
*Qingsen Ma,Dianyun Wang,Ran Jing,Yujun Sun,Zhenbo Xu*

Main category: cs.CL

TL;DR: CIP是一个轻量级即插即用的因果提示框架，通过构建实体、动作和事件之间的因果关系序列来减少大语言模型在处理长噪声检索上下文时的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理长噪声检索上下文时经常产生幻觉，因为它们依赖虚假相关性而非真正的因果关系。需要一种方法来改善模型的事实基础和可解释性。

Method: CIP框架构建实体、动作和事件之间的因果关系序列，并将其注入提示中以引导模型关注因果相关证据。通过因果干预和反事实推理来抑制非因果推理路径。

Result: 在7个主流语言模型上的实验显示，CIP一致提升了推理质量和可靠性：Attributable Rate提高2.6点，Causal Consistency Score提高0.38，有效信息密度提升4倍，端到端响应延迟降低高达55.1%。

Conclusion: 因果推理可能成为改善大语言模型可解释性、稳定性和效率的有前景范式。CIP框架展示了通过因果引导减少幻觉的有效性。

Abstract: Large language models often hallucinate when processing long and noisy retrieval contexts because they rely on spurious correlations rather than genuine causal relationships. We propose CIP, a lightweight and plug-and-play causal prompting framework that mitigates hallucinations at the input stage. CIP constructs a causal relation sequence among entities, actions, and events and injects it into the prompt to guide reasoning toward causally relevant evidence. Through causal intervention and counterfactual reasoning, CIP suppresses non causal reasoning paths, improving factual grounding and interpretability. Experiments across seven mainstream language models, including GPT-4o, Gemini 2.0 Flash, and Llama 3.1, show that CIP consistently enhances reasoning quality and reliability, achieving 2.6 points improvement in Attributable Rate, 0.38 improvement in Causal Consistency Score, and a fourfold increase in effective information density. API level profiling further shows that CIP accelerates contextual understanding and reduces end to end response latency by up to 55.1 percent. These results suggest that causal reasoning may serve as a promising paradigm for improving the explainability, stability, and efficiency of large language models.

</details>


### [17] [LegalRikai: Open Benchmark -- A Benchmark for Complex Japanese Corporate Legal Tasks](https://arxiv.org/abs/2512.11297)
*Shogo Fujita,Yuji Naraki,Yiqing Zhu,Shinsuke Mori*

Main category: cs.CL

TL;DR: LegalRikai是一个由法律专业人士创建的日本企业法律实践基准测试，包含4个复杂任务和100个需要长格式结构化输出的样本，通过人类和自动评估揭示了模型在文档级编辑方面的弱点。


<details>
  <summary>Details</summary>
Motivation: 为了推动法律领域更注重实践的研究，需要创建能够模拟真实日本企业法律实践的基准测试，以评估LLM在法律任务中的实际表现。

Method: 创建了LegalRikai基准测试，包含4个复杂任务和100个样本，由法律专业人士在律师监督下开发。使用GPT-5、Gemini 2.5 Pro和Claude Opus 4.1等领先LLM进行人类和自动评估，评估多个实践标准。

Result: 人类评估发现抽象指令会引发不必要的修改，揭示了模型在文档级编辑方面的弱点，这些弱点在传统的短文本任务中被忽略。自动评估在具有明确语言基础的标准上与人类判断一致，但评估结构一致性仍具挑战性。

Conclusion: 自动评估在专家资源有限时可作为有效的筛选工具。提出了数据集评估框架以促进法律领域更注重实践的研究。

Abstract: This paper introduces LegalRikai: Open Benchmark, a new benchmark comprising four complex tasks that emulate Japanese corporate legal practices. The benchmark was created by legal professionals under the supervision of an attorney. This benchmark has 100 samples that require long-form, structured outputs, and we evaluated them against multiple practical criteria. We conducted both human and automated evaluations using leading LLMs, including GPT-5, Gemini 2.5 Pro, and Claude Opus 4.1. Our human evaluation revealed that abstract instructions prompted unnecessary modifications, highlighting model weaknesses in document-level editing that were missed by conventional short-text tasks. Furthermore, our analysis reveals that automated evaluation aligns well with human judgment on criteria with clear linguistic grounding, and assessing structural consistency remains a challenge. The result demonstrates the utility of automated evaluation as a screening tool when expert availability is limited. We propose a dataset evaluation framework to promote more practice-oriented research in the legal domain.

</details>


### [18] [Unifying Dynamic Tool Creation and Cross-Task Experience Sharing through Cognitive Memory Architecture](https://arxiv.org/abs/2512.11303)
*Jiarun Liu,Shiyue Xu,Yang Li,Shangkun Liu,Yongli Yu,Peng Cao*

Main category: cs.CL

TL;DR: SMITH是一个统一认知架构，通过分层内存组织将动态工具创建与跨任务经验共享相结合，在GAIA基准测试中达到81.8%的准确率，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理在适应新任务时面临工具覆盖有限和经验重用不足的问题，要么依赖预定义工具，要么从头构建工具而不利用过往经验，导致探索效率低下和性能不佳。

Method: 提出SMITH架构，将代理内存组织为程序性、语义性和情景性三个层次；将工具创建形式化为受控沙盒环境中的迭代代码生成；通过语义相似性匹配的情景记忆检索实现经验共享；并提出基于代理集成难度重估的课程学习策略。

Result: 在GAIA基准测试中，SMITH达到81.8%的Pass@1准确率，显著优于Alita（75.2%）和Memento（70.9%）等最先进基线方法。

Conclusion: SMITH为构建真正自适应代理奠定了基础，通过工具创建和经验积累的原则性整合，使代理能够持续进化其能力。

Abstract: Large Language Model agents face fundamental challenges in adapting to novel tasks due to limitations in tool availability and experience reuse. Existing approaches either rely on predefined tools with limited coverage or build tools from scratch without leveraging past experiences, leading to inefficient exploration and suboptimal performance. We introduce SMITH (Shared Memory Integrated Tool Hub), a unified cognitive architecture that seamlessly integrates dynamic tool creation with cross-task experience sharing through hierarchical memory organization. SMITH organizes agent memory into procedural, semantic, and episodic components, enabling systematic capability expansion while preserving successful execution patterns. Our approach formalizes tool creation as iterative code generation within controlled sandbox environments and experience sharing through episodic memory retrieval with semantic similarity matching. We further propose a curriculum learning strategy based on agent-ensemble difficulty re-estimation. Extensive experiments on the GAIA benchmark demonstrate SMITH's effectiveness, achieving 81.8% Pass@1 accuracy and outperforming state-of-the-art baselines including Alita (75.2%) and Memento (70.9%). Our work establishes a foundation for building truly adaptive agents that continuously evolve their capabilities through principled integration of tool creation and experience accumulation.

</details>


### [19] [qa-FLoRA: Data-free query-adaptive Fusion of LoRAs for LLMs](https://arxiv.org/abs/2512.11366)
*Shreya Shukla,Aditya Sriram,Milinda Kuppur Narayanaswamy,Hiteshi Jain*

Main category: cs.CL

TL;DR: qa-FLoRA：一种无需数据和训练的查询自适应LoRA融合方法，通过测量基础模型与适配器之间的分布差异动态计算层级融合权重，显著提升多领域复合查询性能。


<details>
  <summary>Details</summary>
Motivation: 现有LoRA融合方法存在局限性：静态权重方法对所有参与LoRA赋予相同相关性，而监督训练方法需要大量数据为每个可能的LoRA组合训练最优融合权重。处理复杂多领域复合查询时，这些方法效果有限。

Method: 提出qa-FLoRA方法，通过测量基础模型与各适配器之间的分布差异，动态计算层级融合权重。该方法无需复合训练数据或领域代表性样本，可直接应用于现有适配器集合。

Result: 在9个多语言复合任务（数学、编程、医疗领域）上的实验显示：qa-FLoRA比静态融合方法在LLaMA-2上提升约5%，在LLaMA-3上提升约6%；比无训练基线在LLaMA-2上提升约7%，在LLaMA-3上提升约10%，显著缩小与监督基线的差距。

Conclusion: qa-FLoRA提供了一种高效、可解释的LoRA融合方法，无需额外训练数据即可实现鲁棒的多领域适应，层级融合权重分析揭示了可解释的融合模式。

Abstract: The deployment of large language models for specialized tasks often requires domain-specific parameter-efficient finetuning through Low-Rank Adaptation (LoRA) modules. However, effectively fusing these adapters to handle complex, multi-domain composite queries remains a critical challenge. Existing LoRA fusion approaches either use static weights, which assign equal relevance to each participating LoRA, or require data-intensive supervised training for every possible LoRA combination to obtain respective optimal fusion weights. We propose qa-FLoRA, a novel query-adaptive data-and-training-free method for LoRA fusion that dynamically computes layer-level fusion weights by measuring distributional divergence between the base model and respective adapters. Our approach eliminates the need for composite training data or domain-representative samples, making it readily applicable to existing adapter collections. Extensive experiments across nine multilingual composite tasks spanning mathematics, coding, and medical domains, show that qa-FLoRA outperforms static fusion by ~5% with LLaMA-2 and ~6% with LLaMA-3, and the training-free baselines by ~7% with LLaMA-2 and ~10% with LLaMA-3, while significantly closing the gap with supervised baselines. Further, layer-level analysis of our fusion weights reveals interpretable fusion patterns, demonstrating the effectiveness of our approach for robust multi-domain adaptation.

</details>


### [20] [Improving Translation Quality by Selecting Better Data for LLM Fine-Tuning: A Comparative Analysis](https://arxiv.org/abs/2512.11388)
*Felipe Ribeiro Fujita de Mello,Hideyuki Takada*

Main category: cs.CL

TL;DR: 研究数据选择对开放大语言模型机器翻译微调的影响，发现语义选择器优于词汇和几何启发式方法，即使选择数据差异小于3%也会显著影响模型性能


<details>
  <summary>Details</summary>
Motivation: 研究数据选择策略对机器翻译微调效果的影响，探索不同数据选择方法在开放大语言模型微调中的表现差异

Method: 使用日英双语语料库，在受控训练条件下比较五种数据选择方法：TF-IDF、COMET Kiwi、QuRate、FD-Score和随机选择

Result: 语义选择器（如COMET Kiwi）在性能上持续优于词汇（TF-IDF）和几何启发式方法；即使选择数据差异小于3%，对模型性能的影响仍然显著

Conclusion: 微调对数据质量非常敏感，语义选择器是更有效的数据选择策略，数据质量比数量更重要

Abstract: We investigated the impact of data selection on machine translation fine-tuning for open LLMs. Using Japanese-English corpora, we compare five selectors: TF-IDF, COMET Kiwi, QuRate, FD-Score, and random selection, under controlled training conditions. We observed that semantic selectors consistently outperform lexical and geometry-based heuristics, and that even when the selected data differ by less than 3%, the impact on model performance is substantial, underscoring the sensitivity of fine-tuning to data quality.

</details>


### [21] [Minimal Clips, Maximum Salience: Long Video Summarization via Key Moment Extraction](https://arxiv.org/abs/2512.11399)
*Galann Pennec,Zhengyuan Liu,Nicholas Asher,Philippe Muller,Nancy F. Chen*

Main category: cs.CL

TL;DR: 提出基于轻量级视频描述模型和LLM的关键片段选择方法，用于构建多模态视频摘要，在MovieSum数据集上接近参考片段性能，同时保持低计算成本


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型处理长视频时容易丢失重要视觉信息，且需要设计成本效益高的长视频分析工具

Method: 将视频分割为短片段，使用轻量级视频描述模型生成每个片段的紧凑视觉描述，然后通过大语言模型选择K个最相关的片段用于多模态摘要

Result: 在MovieSum数据集上，该方法接近参考片段（少于电影6%）的摘要性能，比随机片段选择捕获更多相关视频信息，同时保持低计算成本

Conclusion: 提出的关键片段选择方法能有效识别视频中的重要时刻，为构建高质量多模态摘要提供成本效益高的解决方案

Abstract: Vision-Language Models (VLMs) are able to process increasingly longer videos. Yet, important visual information is easily lost throughout the entire context and missed by VLMs. Also, it is important to design tools that enable cost-effective analysis of lengthy video content. In this paper, we propose a clip selection method that targets key video moments to be included in a multimodal summary. We divide the video into short clips and generate compact visual descriptions of each using a lightweight video captioning model. These are then passed to a large language model (LLM), which selects the K clips containing the most relevant visual information for a multimodal summary. We evaluate our approach on reference clips for the task, automatically derived from full human-annotated screenplays and summaries in the MovieSum dataset. We further show that these reference clips (less than 6% of the movie) are sufficient to build a complete multimodal summary of the movies in MovieSum. Using our clip selection method, we achieve a summarization performance close to that of these reference clips while capturing substantially more relevant video information than random clip selection. Importantly, we maintain low computational cost by relying on a lightweight captioning model.

</details>


### [22] [CLINIC: Evaluating Multilingual Trustworthiness in Language Models for Healthcare](https://arxiv.org/abs/2512.11437)
*Akash Ghosh,Srivarshinee Sridhar,Raghav Kaushik Ravi,Muhsin Muhsin,Sriparna Saha,Chirag Agarwal*

Main category: cs.CL

TL;DR: CLINIC是一个全面的多语言基准测试，用于评估医疗领域语言模型的可信度，涵盖5个关键维度、18个任务和15种语言，揭示了模型在事实准确性、偏见、隐私等方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型主要针对高资源语言训练，难以处理中低资源语言的医疗查询多样性，缺乏可靠的可信度评估阻碍了其在全球医疗环境中的实际应用。

Method: 开发CLINIC基准测试，系统评估语言模型在5个可信度维度（真实性、公平性、安全性、鲁棒性、隐私性），通过18个多样化任务，覆盖15种语言和广泛的医疗主题。

Result: 评估显示语言模型在事实准确性方面存在困难，在不同人口统计和语言群体中表现出偏见，容易受到隐私泄露和对抗性攻击的影响。

Conclusion: CLINIC通过揭示这些缺陷，为增强语言模型在全球医疗领域中的覆盖范围和安全性的跨语言应用奠定了基础。

Abstract: Integrating language models (LMs) in healthcare systems holds great promise for improving medical workflows and decision-making. However, a critical barrier to their real-world adoption is the lack of reliable evaluation of their trustworthiness, especially in multilingual healthcare settings. Existing LMs are predominantly trained in high-resource languages, making them ill-equipped to handle the complexity and diversity of healthcare queries in mid- and low-resource languages, posing significant challenges for deploying them in global healthcare contexts where linguistic diversity is key. In this work, we present CLINIC, a Comprehensive Multilingual Benchmark to evaluate the trustworthiness of language models in healthcare. CLINIC systematically benchmarks LMs across five key dimensions of trustworthiness: truthfulness, fairness, safety, robustness, and privacy, operationalized through 18 diverse tasks, spanning 15 languages (covering all the major continents), and encompassing a wide array of critical healthcare topics like disease conditions, preventive actions, diagnostic tests, treatments, surgeries, and medications. Our extensive evaluation reveals that LMs struggle with factual correctness, demonstrate bias across demographic and linguistic groups, and are susceptible to privacy breaches and adversarial attacks. By highlighting these shortcomings, CLINIC lays the foundation for enhancing the global reach and safety of LMs in healthcare across diverse languages.

</details>


### [23] [Mistake Notebook Learning: Selective Batch-Wise Context Optimization for In-Context Learning](https://arxiv.org/abs/2512.11485)
*Xuanbo Su,Yingfang Zhang,Hao Luo,Xiaoteng Liu,Leo Huang*

Main category: cs.CL

TL;DR: MNL是一种无需训练的学习框架，通过构建持久化错误模式知识库，从批量错误中提取通用指导，实现接近监督微调的性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM适应任务的方法存在缺陷：梯度微调计算量大且会导致灾难性遗忘，上下文学习则鲁棒性差且难以从错误中学习。需要一种既能有效学习又无需训练的方法。

Method: MNL框架包含三个核心组件：1) 批量错误抽象 - 从多个失败案例中提取通用指导；2) 动态笔记本 - 存储抽象后的错误模式知识；3) 保留验证 - 通过保留集验证，只保留优于基线的指导，确保单调改进。

Result: 在GSM8K上达到93.9%准确率（接近监督微调的94.3%），在KaggleDBQA上达到28%准确率（相对提升47%），优于Memento（15.1%）和Training-Free GRPO（22.1%）。在Spider、AIME等基准测试中也表现出色。

Conclusion: MNL是一种强大的无需训练替代方案，特别适用于复杂推理任务，能够有效从错误中学习并实现接近监督微调的性能，同时避免了训练的计算成本和灾难性遗忘问题。

Abstract: Large language models (LLMs) adapt to tasks via gradient fine-tuning (heavy computation, catastrophic forgetting) or In-Context Learning (ICL: low robustness, poor mistake learning). To fix this, we introduce Mistake Notebook Learning (MNL), a training-free framework with a persistent knowledge base of abstracted error patterns. Unlike prior instance/single-trajectory memory methods, MNL uses batch-wise error abstraction: it extracts generalizable guidance from multiple failures, stores insights in a dynamic notebook, and retains only baseline-outperforming guidance via hold-out validation (ensuring monotonic improvement). We show MNL nearly matches Supervised Fine-Tuning (93.9% vs 94.3% on GSM8K) and outperforms training-free alternatives on GSM8K, Spider, AIME, and KaggleDBQA. On KaggleDBQA (Qwen3-8B), MNL hits 28% accuracy (47% relative gain), outperforming Memento (15.1%) and Training-Free GRPO (22.1) - proving it's a strong training-free alternative for complex reasoning.

</details>


### [24] [Building Patient Journeys in Hebrew: A Language Model for Clinical Timeline Extraction](https://arxiv.org/abs/2512.11502)
*Kai Golan Hashiloni,Brenda Kasabe Nokai,Michal Shevach,Esthy Shemesh,Ronit Bartin,Anna Bergrin,Liran Harel,Nachum Dershowitz,Liat Nadai Arad,Kfir Bar*

Main category: cs.CL

TL;DR: 提出一个新的希伯来语医疗语言模型，用于从电子健康记录中提取结构化临床时间线，构建患者旅程。模型基于DictaBERT 2.0，在超过500万条去标识化医院记录上持续预训练，在内部医学/急诊科和肿瘤学两个新数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 需要从希伯来语电子健康记录中提取结构化临床时间线，构建患者旅程，以支持医疗研究和临床决策。现有模型可能无法充分处理希伯来语医疗文本，且隐私保护需求强烈。

Method: 基于DictaBERT 2.0架构，在超过500万条去标识化的医院记录上进行持续预训练。引入两个新的希伯来语医疗数据集（内部医学/急诊科和肿瘤学），标注事件时间关系。采用词汇适应技术提高标记效率。

Result: 模型在两个新数据集上都取得了强劲性能。词汇适应提高了标记效率，去标识化处理不会损害下游任务性能，支持隐私保护模型开发。模型在伦理限制下可供研究使用。

Conclusion: 成功开发了一个有效的希伯来语医疗语言模型，能够从电子健康记录中提取结构化临床时间线。证明了去标识化不会损害模型性能，支持隐私保护的医疗NLP研究。模型为希伯来语医疗文本分析提供了重要工具。

Abstract: We present a new Hebrew medical language model designed to extract structured clinical timelines from electronic health records, enabling the construction of patient journeys. Our model is based on DictaBERT 2.0 and continually pre-trained on over five million de-identified hospital records. To evaluate its effectiveness, we introduce two new datasets -- one from internal medicine and emergency departments, and another from oncology -- annotated for event temporal relations. Our results show that our model achieves strong performance on both datasets. We also find that vocabulary adaptation improves token efficiency and that de-identification does not compromise downstream performance, supporting privacy-conscious model development. The model is made available for research use under ethical restrictions.

</details>


### [25] [Does Less Hallucination Mean Less Creativity? An Empirical Investigation in LLMs](https://arxiv.org/abs/2512.11509)
*Mohor Banerjee,Nadya Yuki Wangsajaya,Syed Ali Redha Alsagoff,Min Sen Tan,Zachary Choy Kit Chun,Alvin Chan Guo Wei*

Main category: cs.CL

TL;DR: 研究探讨了三种减少大语言模型幻觉的方法（CoVe、DoLa、RAG）对模型创造力的影响，发现它们对发散性创造力有不同效果：CoVe增强、DoLa抑制、RAG影响最小。


<details>
  <summary>Details</summary>
Motivation: 虽然已有许多方法减少大语言模型的幻觉，但这些方法对创造力的影响尚未被研究。在AI辅助科学发现中，既需要事实准确性又需要创造性假设生成，这一空白尤为关键。

Method: 研究了三种幻觉减少技术：验证链（CoVe）、层对比解码（DoLa）和检索增强生成（RAG）。评估了多个模型家族（LLaMA、Qwen、Mistral）在不同规模（1B-70B参数）上，使用两个创造力基准（NeoCoder和CS4）。

Result: 这些方法对发散性创造力有相反效果：CoVe增强发散思维，DoLa抑制发散思维，RAG影响最小。为科学应用中平衡事实准确性和创造性探索提供了方法选择指导。

Conclusion: 研究填补了幻觉减少方法对创造力影响的研究空白，为AI辅助科学发现中平衡事实准确性和创造性探索提供了实证依据和方法选择指导。

Abstract: Large Language Models (LLMs) exhibit remarkable capabilities in natural language understanding and reasoning, but suffer from hallucination: the generation of factually incorrect content. While numerous methods have been developed to reduce hallucinations, their impact on creative generations remains unexplored. This gap is particularly critical for AI-assisted scientific discovery, which requires both factual accuracy and creative hypothesis generation. We investigate how three hallucination-reduction techniques: Chain of Verification (CoVe), Decoding by Contrasting Layers (DoLa), and Retrieval-Augmented Generation (RAG), affect creativity in LLMs. Evaluating multiple model families (LLaMA, Qwen, Mistral) at varying scales (1B - 70B parameters) on two creativity benchmarks (NeoCoder and CS4), we find that these methods have opposing effects on divergent creativity. CoVe enhances divergent thinking, DoLa suppresses it, and RAG shows minimal impact. Our findings provide guidance for selecting appropriate hallucination-reduction methods in scientific applications, where the balance between factual accuracy and creative exploration is crucial.

</details>


### [26] [Extending a Parliamentary Corpus with MPs' Tweets: Automatic Annotation and Evaluation Using MultiParTweet](https://arxiv.org/abs/2512.11567)
*Mevlüt Bagci,Ali Abusaleh,Daniel Baumartz,Giueseppe Abrami,Maxim Konca,Alexander Mehler*

Main category: cs.CL

TL;DR: MultiParTweet是一个多语言推特语料库，连接政治家社交媒体言论与德国议会语料，包含39,546条推文和19,056个媒体项目，通过多种模型进行情感、情绪和主题标注，并提供了数据收集工具TTLABTweetCrawler。


<details>
  <summary>Details</summary>
Motivation: 社交媒体在现代政治中扮演关键角色，既能反映政治家的意识形态，又能促进与年轻一代的沟通。需要建立一个连接社交媒体话语与议会辩论的语料库，以支持比较分析。

Method: 构建MultiParTweet多语言推特语料库，包含39,546条推文和19,056个媒体项目。使用9个文本模型和1个视觉语言模型进行情感、情绪和主题标注。开发TTLABTweetCrawler工具用于X平台数据收集。通过手动标注子集评估自动标注质量。

Result: MultiParTweet语料库成功建立，自动标注得到验证。分析显示不同模型的输出可以相互预测。人类标注者更偏好视觉语言模型的标注结果，表明多模态表示更符合人类解释。

Conclusion: 提供了整合自动文本和媒体标注的MultiParTweet资源，以及通用的X数据收集工具TTLABTweetCrawler。多模态标注方法在政治社交媒体分析中显示出优势，为比较政治话语研究提供了有价值的工具。

Abstract: Social media serves as a critical medium in modern politics because it both reflects politicians' ideologies and facilitates communication with younger generations. We present MultiParTweet, a multilingual tweet corpus from X that connects politicians' social media discourse with German political corpus GerParCor, thereby enabling comparative analyses between online communication and parliamentary debates. MultiParTweet contains 39 546 tweets, including 19 056 media items. Furthermore, we enriched the annotation with nine text-based models and one vision-language model (VLM) to annotate MultiParTweet with emotion, sentiment, and topic annotations. Moreover, the automated annotations are evaluated against a manually annotated subset. MultiParTweet can be reconstructed using our tool, TTLABTweetCrawler, which provides a framework for collecting data from X. To demonstrate a methodological demonstration, we examine whether the models can predict each other using the outputs of the remaining models. In summary, we provide MultiParTweet, a resource integrating automatic text and media-based annotations validated with human annotations, and TTLABTweetCrawler, a general-purpose X data collection tool. Our analysis shows that the models are mutually predictable. In addition, VLM-based annotation were preferred by human annotators, suggesting that multimodal representations align more with human interpretation.

</details>


### [27] [Visualizing token importance for black-box language models](https://arxiv.org/abs/2512.11573)
*Paulius Rauba,Qiyao Wei,Mihaela van der Schaar*

Main category: cs.CL

TL;DR: 提出DBSA方法，用于审计黑盒大语言模型，分析每个输入token对输出的敏感性，无需模型内部信息。


<details>
  <summary>Details</summary>
Motivation: 现有LLM审计方法通常只关注特定方面（如偏见检测），而实际应用中需要理解黑盒LLM输出如何依赖每个输入token，特别是在高风险领域（法律、医疗等）中，模型通过API端点访问，无法获取内部信息。

Method: 提出Distribution-Based Sensitivity Analysis (DBSA)，一种轻量级、模型无关的方法，无需对LLM做分布假设，通过分析输出分布来评估每个输入token的敏感性，可作为即插即用的可视化工具。

Result: DBSA能够帮助用户检查LLM输入，发现现有可解释性方法可能忽略的敏感性，通过示例展示了其有效性。

Conclusion: DBSA为从业者提供了一种实用的黑盒LLM审计工具，能够快速分析模型对特定输入token的依赖关系，有助于在高风险应用中确保模型可靠性。

Abstract: We consider the problem of auditing black-box large language models (LLMs) to ensure they behave reliably when deployed in production settings, particularly in high-stakes domains such as legal, medical, and regulatory compliance. Existing approaches for LLM auditing often focus on isolated aspects of model behavior, such as detecting specific biases or evaluating fairness. We are interested in a more general question -- can we understand how the outputs of black-box LLMs depend on each input token? There is a critical need to have such tools in real-world applications that rely on inaccessible API endpoints to language models. However, this is a highly non-trivial problem, as LLMs are stochastic functions (i.e. two outputs will be different by chance), while computing prompt-level gradients to approximate input sensitivity is infeasible. To address this, we propose Distribution-Based Sensitivity Analysis (DBSA), a lightweight model-agnostic procedure to evaluate the sensitivity of the output of a language model for each input token, without making any distributional assumptions about the LLM. DBSA is developed as a practical tool for practitioners, enabling quick, plug-and-play visual exploration of LLMs reliance on specific input tokens. Through illustrative examples, we demonstrate how DBSA can enable users to inspect LLM inputs and find sensitivities that may be overlooked by existing LLM interpretability methods.

</details>


### [28] [Bounding Hallucinations: Information-Theoretic Guarantees for RAG Systems via Merlin-Arthur Protocols](https://arxiv.org/abs/2512.11614)
*Björn Deiseroth,Max Henning Höth,Kristian Kersting,Letitia Parcalabescu*

Main category: cs.CL

TL;DR: 提出基于Merlin-Arthur协议的RAG训练框架，将检索器与生成器视为交互证明系统，通过对抗性训练提升模型基于证据回答、拒绝回答及依赖具体证据片段的能力。


<details>
  <summary>Details</summary>
Motivation: 当前RAG系统将检索视为弱启发式而非可验证证据，导致LLM在没有支持的情况下回答、在上下文不完整或误导时产生幻觉、依赖虚假证据。需要建立将检索文档作为可验证证据的可靠RAG系统。

Method: 采用Merlin-Arthur协议框架：Arthur（生成器LLM）在未知来源的问题上训练，Merlin提供有用证据，Morgana注入对抗性误导上下文。两者使用线性时间XAI方法识别和修改对Arthur最有影响的证据。训练生成器学习：(i)在上下文支持时回答，(ii)证据不足时拒绝，(iii)依赖真正支撑答案的具体上下文片段。同时引入严格评估框架分离解释保真度与基线预测误差。

Result: 在三个RAG数据集和两个不同规模的模型家族上，M/A训练的LLM显示出改进的groundedness、完整性、合理性、拒绝行为，以及减少的幻觉——无需手动标注不可回答问题。检索器通过自动生成的M/A硬正负样本也提高了召回率和MRR。

Conclusion: 自主交互证明式监督为可靠RAG系统提供了原则性和实用路径，使检索文档不再仅仅是建议，而是可验证的证据。该方法通过引入解释信息分数（EIF）和对抗训练机制，显著提升了RAG系统的可靠性和可解释性。

Abstract: Retrieval-augmented generation (RAG) models rely on retrieved evidence to guide large language model (LLM) generators, yet current systems treat retrieval as a weak heuristic rather than verifiable evidence. As a result, LLMs answer without support, hallucinate under incomplete or misleading context, and rely on spurious evidence. We introduce a training framework that treats the entire RAG pipeline -- both the retriever and the generator -- as an interactive proof system via an adaptation of the Merlin-Arthur (M/A) protocol. Arthur (the generator LLM) trains on questions of unkown provenance: Merlin provides helpful evidence, while Morgana injects adversarial, misleading context. Both use a linear-time XAI method to identify and modify the evidence most influential to Arthur. Consequently, Arthur learns to (i) answer when the context support the answer, (ii) reject when evidence is insufficient, and (iii) rely on the specific context spans that truly ground the answer. We further introduce a rigorous evaluation framework to disentangle explanation fidelity from baseline predictive errors. This allows us to introduce and measure the Explained Information Fraction (EIF), which normalizes M/A certified mutual-information guarantees relative to model capacity and imperfect benchmarks. Across three RAG datasets and two model families of varying sizes, M/A-trained LLMs show improved groundedness, completeness, soundness, and reject behavior, as well as reduced hallucinations -- without needing manually annotated unanswerable questions. The retriever likewise improves recall and MRR through automatically generated M/A hard positives and negatives. Our results demonstrate that autonomous interactive-proof-style supervision provides a principled and practical path toward reliable RAG systems that treat retrieved documents not as suggestions, but as verifiable evidence.

</details>


### [29] [Automating Historical Insight Extraction from Large-Scale Newspaper Archives via Neural Topic Modeling](https://arxiv.org/abs/2512.11635)
*Keerthana Murugaraj,Salima Lamsiyah,Marten During,Martin Theobald*

Main category: cs.CL

TL;DR: 该研究使用BERTopic模型分析1955-2018年报纸档案中关于核能和核安全的公共话语，相比传统LDA方法能更好捕捉历史文本的动态性和复杂性。


<details>
  <summary>Details</summary>
Motivation: 传统主题建模方法（如LDA）在处理大规模历史报纸档案时存在局限性，难以应对主题演变、OCR噪声和文本量大的挑战，需要更先进的方法来捕捉历史话语的复杂性。

Method: 采用BERTopic神经主题建模方法，利用基于transformer的嵌入来提取和分类主题，分析1955-2018年间关于核能和核安全的报纸文章，追踪主题分布的时间演化。

Result: BERTopic能够有效识别核能与核武器相关主题的共现模式，揭示主题重要性随时间的变化趋势，展示了该方法在历史研究中的可扩展性和上下文敏感性。

Conclusion: BERTopic为传统主题建模方法提供了有效替代，能够从报纸档案中提取更丰富的历史话语洞察，为历史、核能和社会科学研究做出贡献，同时指出了当前局限性和未来研究方向。

Abstract: Extracting coherent and human-understandable themes from large collections of unstructured historical newspaper archives presents significant challenges due to topic evolution, Optical Character Recognition (OCR) noise, and the sheer volume of text. Traditional topic-modeling methods, such as Latent Dirichlet Allocation (LDA), often fall short in capturing the complexity and dynamic nature of discourse in historical texts. To address these limitations, we employ BERTopic. This neural topic-modeling approach leverages transformerbased embeddings to extract and classify topics, which, despite its growing popularity, still remains underused in historical research. Our study focuses on articles published between 1955 and 2018, specifically examining discourse on nuclear power and nuclear safety. We analyze various topic distributions across the corpus and trace their temporal evolution to uncover long-term trends and shifts in public discourse. This enables us to more accurately explore patterns in public discourse, including the co-occurrence of themes related to nuclear power and nuclear weapons and their shifts in topic importance over time. Our study demonstrates the scalability and contextual sensitivity of BERTopic as an alternative to traditional approaches, offering richer insights into historical discourses extracted from newspaper archives. These findings contribute to historical, nuclear, and social-science research while reflecting on current limitations and proposing potential directions for future work.

</details>


### [30] [Speculative Decoding Speed-of-Light: Optimal Lower Bounds via Branching Random Walks](https://arxiv.org/abs/2512.11718)
*Sergey Pankratov,Dan Alistarh*

Main category: cs.CL

TL;DR: 本文建立了确定性推测生成算法的首个紧下界，通过将token生成过程与分支随机游走联系起来，证明了每个推测迭代成功预测token数的期望上界，为推测解码系统的设计提供了理论指导。


<details>
  <summary>Details</summary>
Motivation: 推测生成技术通过并行验证多个草稿token来加速大语言模型推理，但其可实现的加速上限仍不清楚。本文旨在理解推测生成算法的基本极限。

Method: 通过将token生成过程与分支随机游走建立联系，分析最优草稿树选择问题。在基本假设下，推导出每个推测迭代成功预测token数的期望上界。

Result: 证明了期望成功预测token数上界为 𝔼[X] ≤ (μ+μ₂)log(P)/μ² + O(1)，其中P是验证器容量，μ是验证器输出分布的期望熵，μ₂是期望第二对数矩。在Llama模型上的实证评估验证了理论预测的紧性。

Conclusion: 本文建立了推测生成算法的首个紧下界，为并行token生成的极限提供了新见解，可指导未来推测解码系统的设计。

Abstract: Speculative generation has emerged as a promising technique to accelerate inference in large language models (LLMs) by leveraging parallelism to verify multiple draft tokens simultaneously. However, the fundamental limits on the achievable speedup remain poorly understood. In this work, we establish the first ``tight'' lower bounds on the runtime of any deterministic speculative generation algorithm. This is achieved by drawing a parallel between the token generation process and branching random walks, which allows us to analyze the optimal draft tree selection problem. We prove, under basic assumptions, that the expected number of tokens successfully predicted per speculative iteration is bounded as $\mathbb{E}[X] \leq (μ+ μ_{(2)})\log(P )/μ^2 + O(1)$, where $P$ is the verifier's capacity, $μ$ is the expected entropy of the verifier's output distribution, and $μ_{(2)}$ is the expected second log-moment. This result provides new insights into the limits of parallel token generation, and could guide the design of future speculative decoding systems. Empirical evaluations on Llama models validate our theoretical predictions, confirming the tightness of our bounds in practical settings.

</details>


### [31] [SUMFORU: An LLM-Based Review Summarization Framework for Personalized Purchase Decision Support](https://arxiv.org/abs/2512.11755)
*Yuming Feng,Xinrui Jiang*

Main category: cs.CL

TL;DR: SUMFORU是一个可引导的评论摘要框架，通过用户角色对齐实现个性化决策支持，在一致性、事实基础和偏好对齐方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 在线产品评论包含丰富但嘈杂的信息，现有LLM摘要工具过于通用且无法考虑个人偏好，限制了实际应用价值。

Method: 提出SUMFORU框架：1）基于Amazon 2023评论数据集构建高质量数据管道；2）两阶段对齐：角色感知的监督微调（通过非对称知识蒸馏）和基于AI反馈的强化学习（使用偏好估计器）。

Result: 在基于规则、LLM和人工评估指标上均表现最佳，在一致性、事实基础和偏好对齐方面持续改进，并能有效泛化到未见过的产品类别。

Conclusion: 可引导的多元化对齐方法为构建下一代个性化决策支持系统展示了前景。

Abstract: Online product reviews contain rich but noisy signals that overwhelm users and hinder effective decision-making. Existing LLM-based summarizers remain generic and fail to account for individual preferences, limiting their practical utility. We propose SUMFORU, a steerable review summarization framework that aligns outputs with explicit user personas to support personalized purchase decisions. Our approach integrates a high-quality data pipeline built from the Amazon 2023 Review Dataset with a two-stage alignment procedure: (1) persona-aware Supervised Fine-Tuning (SFT) via asymmetric knowledge distillation, and (2) Reinforcement Learning with AI Feedback (RLAIF) using a preference estimator to capture fine-grained, persona-relevant signals. We evaluate the model across rule-based, LLM-based, and human-centered metrics, demonstrating consistent improvements in consistency, grounding, and preference alignment. Our framework achieves the highest performance across all evaluation settings and generalizes effectively to unseen product categories. Our results highlight the promise of steerable pluralistic alignment for building next-generation personalized decision-support systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [32] [Scalable Data Synthesis for Computer Use Agents with Step-Level Filtering](https://arxiv.org/abs/2512.10962)
*Yifei He,Pranit Chawla,Yaser Souri,Subhojit Som,Xia Song*

Main category: cs.LG

TL;DR: 提出WebSTAR数据集和StepRM奖励模型，通过步骤级过滤将嘈杂的计算机使用代理轨迹转化为可靠训练数据，显著提升模型性能


<details>
  <summary>Details</summary>
Motivation: 计算机使用代理（CUAs）训练困难，因为GUI交互成本高且高质量轨迹数据稀缺。现有数据集依赖人工演示，可扩展性有限。合成数据方法面临噪声大、动作错误率高的问题

Method: 提出可扩展的数据合成管道：1）步骤级过滤，评估单个动作保留正确步骤；2）推理增强改进规划。构建WebSTAR数据集（13.3K轨迹，100K分级推理步骤），训练Qwen-2.5-VL-Instruct模型。进一步创建WebSCORE数据集和StepRM奖励模型

Result: 7B模型在WebVoyager上超越SoTA开源CUA模型UI-TARS-1.5-7B超过15%。StepRM奖励模型与o4-mini的评分质量相当，但部署效率更高

Conclusion: 步骤级过滤是扩展CUA训练的关键原则，WebSTAR、WebSCORE数据集和StepRM奖励模型为推进稳健高效的计算机使用代理提供了实用工具

Abstract: Computer use agents (CUAs) can operate real-world digital interfaces but remain difficult to train due to the high cost of graphical user interface (GUI) interaction and the scarcity of high-quality trajectory data. Existing datasets rely on human demonstrations, limiting scalability. A natural alternative is to synthesize data from strong CUAs, yet their rollouts are highly noisy, with incorrect or suboptimal actions consisting a large proportion of the steps, making naive imitation ineffective. To tackle this challenge, we introduce a scalable data synthesis pipeline that transforms noisy rollouts into reliable supervision without human annotation. The core idea is step-level filtering, which evaluates actions individually to retain only correct steps, complemented by reasoning augmentation for improved planning. Using this pipeline, we construct WebSTAR, a dataset of 13.3K trajectories and 100K graded, reasoning-rich steps synthesized from OpenAI's computer-use-preview model. We train Qwen-2.5-VL-Instruct models (7B and 32B) on WebSTAR. On WebVoyager, our 7B model surpasses SoTA open-source CUA model UI-TARS-1.5-7B by more than 15% with only supervised finetuning. Building on step-level grading, we further create WebSCORE, a dataset of graded step-level actions, and train StepRM, a 7B multimodal reward model distilled from o4-mini, which matches its grading quality while being far more efficient to deploy at scale. Our results establish step-level filtering as a key principle for scalable CUA training and construct two new datasets (WebSTAR, WebSCORE) and a lightweight reward model (StepRM) as practical tools to advance robust and efficient CUAs.

</details>


### [33] [Multimodal Fusion of Regional Brain Experts for Interpretable Alzheimer's Disease Diagnosis](https://arxiv.org/abs/2512.10966)
*Farica Zhuang,Dinara Aliyeva,Shu Yang,Zixuan Wen,Duy Duong-Tran,Christos Davatzikos,Tianlong Chen,Song Wang,Li Shen*

Main category: cs.LG

TL;DR: 提出MREF-AD模型，通过多模态区域专家融合框架，自适应地融合淀粉样蛋白PET和MRI信息，提升阿尔茨海默病诊断性能并提供区域级可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态融合方法通常采用简单的特征拼接，无法自适应地平衡不同脑区中生物标志物（如淀粉样蛋白PET和MRI）的贡献，而临床实践中需要整合多种互补信息进行准确早期诊断。

Method: 提出MREF-AD模型，采用混合专家框架，将每个模态中的中尺度脑区建模为独立专家，并使用两级门控网络学习受试者特定的融合权重。

Result: 在ADNI数据上，MREF-AD实现了最先进的诊断性能，同时提供了增强的脑区特异性生物标志物相关性可解释性。

Conclusion: MREF-AD不仅提升了诊断性能，还提供了模态和区域层面的洞察，展示了其作为神经影像中自适应和可解释多模态融合通用框架的实用性。

Abstract: Accurate and early diagnosis of Alzheimer's disease (AD) can benefit from integrating complementary information from multiple modalities, mirroring clinical practice. However, conventional fusion approaches often rely on simple concatenation of features, which cannot adaptively balance the contributions of biomarkers such as amyloid PET and MRI across brain regions. In this work, we propose MREF-AD, a Multimodal Regional Expert Fusion model for AD diagnosis. It is a Mixture-of-Experts (MoE) framework that models meso-scale brain regions in each modality as an independent expert and employs two-level gating networks to learn subject-specific fusion weights. Beyond improving diagnostic performance, MREF-AD provides modality- and region-level insight into how structural and molecular imaging jointly contribute to disease diagnosis. Using data from the Alzheimer's Disease Neuroimaging Initiative (ADNI), MREF-AD achieves state-of-the-art performance over baselines while providing enhanced interpretability of brain region-specific biomarker relevance, underscoring its utility as a general framework for adaptive and interpretable multimodal fusion in neuroimaging.

</details>


### [34] [MoB: Mixture of Bidders](https://arxiv.org/abs/2512.10969)
*Dev Vyas*

Main category: cs.LG

TL;DR: MoB框架用VCG拍卖机制替代MoE中的学习门控网络，通过专家竞标真实成本实现无状态路由，解决了持续学习中灾难性遗忘问题，在Split-MNIST上比基线提升4.5倍。


<details>
  <summary>Details</summary>
Motivation: 传统MoE架构在持续学习中存在根本限制：学习到的门控网络本身会遭受灾难性遗忘。需要一种能够避免遗忘、实现专家自主专业化的新路由机制。

Method: 提出Mixture of Bidders框架，用VCG拍卖替代学习门控网络。专家通过竞标真实成本（执行成本+遗忘成本）竞争处理数据批次，实现无状态路由。还扩展了自主监控专家，能检测知识巩固边界。

Result: 在Split-MNIST基准测试中，MoB达到88.77%平均准确率，相比Gated MoE的19.54%和Monolithic EWC的27.96%，提升4.5倍。实现了无灾难性遗忘、真实竞标保证和无需明确任务边界的涌现专业化。

Conclusion: MoB通过将专家路由重构为去中心化经济机制，成功解决了MoE在持续学习中的灾难性遗忘问题。VCG拍卖机制提供了无状态路由、真实竞标保证和自主专业化，为持续学习系统设计提供了新范式。

Abstract: Mixture of Experts (MoE) architectures have demonstrated remarkable success in scaling neural networks, yet their application to continual learning remains fundamentally limited by a critical vulnerability: the learned gating network itself suffers from catastrophic forgetting. We introduce Mixture of Bidders (MoB), a novel framework that reconceptualizes expert routing as a decentralized economic mechanism. MoB replaces learned gating networks with Vickrey-Clarke-Groves (VCG) auctions, where experts compete for each data batch by bidding their true cost -- a principled combination of execution cost (predicted loss) and forgetting cost (Elastic Weight Consolidation penalty). This game-theoretic approach provides three key advantages: (1) {stateless routing that is immune to catastrophic forgetting, (2) \textbf{truthful bidding} guaranteed by dominant-strategy incentive compatibility, and (3) emergent specialization without explicit task boundaries. On Split-MNIST benchmarks, MoB achieves 88.77% average accuracy compared to 19.54% for Gated MoE and 27.96% for Monolithic EWC, representing a 4.5 times improvement over the strongest baseline. We further extend MoB with autonomous self-monitoring experts that detect their own knowledge consolidation boundaries, eliminating the need for explicit task demarcation.

</details>


### [35] [TECM*: A Data-Driven Assessment to Reinforcement Learning Methods and Application to Heparin Treatment Strategy for Surgical Sepsis](https://arxiv.org/abs/2512.10973)
*Jiang Liu,Yujie Li,Chan Zhou,Yihao Xie,Qilong Sun,Xin Shu,Peiwei Li,Chunyong Yang,Yiziting Zhu,Jiaqi Zhu,Yuwen Chen,Bo An,Hao Wu,Bin Yi*

Main category: cs.LG

TL;DR: 本研究提出一个强化学习框架，通过连续cxSOFA评分和TECM评估矩阵，优化外科脓毒症患者的肝素治疗策略，降低死亡率和住院时间。


<details>
  <summary>Details</summary>
Motivation: 脓毒症是危及生命的严重感染状态，需要优化抗凝治疗。传统离散SOFA评分不够精细，需要更连续的状态表示和评估方法来个性化肝素治疗。

Method: 使用MIMIC-IV和eICU数据库数据，提出强化学习框架：1) 将离散SOFA转换为连续cxSOFA评分；2) 基于cxSOFA定义"好/坏"治疗策略；3) 提出类似混淆矩阵的TECM评估方法。应用Q-Learning、DQN、DDQN、BCQ和CQL等算法优化治疗。

Result: cxSOFA-CQL模型表现最佳，将死亡率从1.83%降至0.74%，平均住院时间从11.11天缩短至9.42天。TECM在不同模型中显示一致结果，证明框架稳健性。

Conclusion: 该强化学习框架能够可解释且稳健地优化外科脓毒症患者的肝素治疗。连续cxSOFA评分和TECM评估提供了精细的治疗评估，有望改善临床结果和决策支持可靠性。

Abstract: Objective: Sepsis is a life-threatening condition caused by severe infection leading to acute organ dysfunction. This study proposes a data-driven metric and a continuous reward function to optimize personalized heparin therapy in surgical sepsis patients. Methods: Data from the MIMIC-IV v1.0 and eICU v2.0 databases were used for model development and evaluation. The training cohort consisted of abdominal surgery patients receiving unfractionated heparin (UFH) after postoperative sepsis onset. We introduce a new RL-based framework: converting the discrete SOFA score to a continuous cxSOFA for more nuanced state and reward functions; Second, defining "good" or "bad" strategies based on cxSOFA by a stepwise manner; Third, proposing a Treatment Effect Comparison Matrix (TECM), analogous to a confusion matrix for classification tasks, to evaluate the treatment strategies. We applied different RL algorithms, Q-Learning, DQN, DDQN, BCQ and CQL to optimize the treatment and comprehensively evaluated the framework. Results: Among the AI-derived strategies, the cxSOFA-CQL model achieved the best performance, reducing mortality from 1.83% to 0.74% with the average hospital stay from 11.11 to 9.42 days. TECM demonstrated consistent outcomes across models, highlighting robustness. Conclusion: The proposed RL framework enables interpretable and robust optimization of heparin therapy in surgical sepsis. Continuous cxSOFA scoring and TECM-based evaluation provide nuanced treatment assessment, showing promise for improving clinical outcomes and decision-support reliability.

</details>


### [36] [Agent-Based Modular Learning for Multimodal Emotion Recognition in Human-Agent Systems](https://arxiv.org/abs/2512.10975)
*Matvey Nepomnyaschiy,Oleg Pereziabov,Anvar Tliamov,Stanislav Mikhailov,Ilya Afanasyev*

Main category: cs.LG

TL;DR: 提出一种基于多智能体框架的多模态情感识别系统，通过模块化设计和中央协调机制实现高效训练和灵活扩展。


<details>
  <summary>Details</summary>
Motivation: 传统多模态深度学习模型虽然情感识别准确率高，但存在计算密集、训练维护成本高、模态变更不灵活等问题，需要更高效、可扩展的解决方案。

Method: 采用多智能体框架，将每个模态编码器和融合分类器设计为自主智能体，由中央监督器协调，支持模块化集成新模态（如emotion2vec音频特征）和组件替换。

Result: 通过支持视觉、音频和文本模态的概念验证实现，证明该框架可行，能提高训练效率，并为HAI场景中的具身和虚拟智能体提供更灵活、可扩展的感知模块。

Conclusion: 多智能体框架为多模态情感识别系统提供了更高效、灵活、可维护的解决方案，有助于改善人机交互中情感感知模块的设计。

Abstract: Effective human-agent interaction (HAI) relies on accurate and adaptive perception of human emotional states. While multimodal deep learning models - leveraging facial expressions, speech, and textual cues - offer high accuracy in emotion recognition, their training and maintenance are often computationally intensive and inflexible to modality changes. In this work, we propose a novel multi-agent framework for training multimodal emotion recognition systems, where each modality encoder and the fusion classifier operate as autonomous agents coordinated by a central supervisor. This architecture enables modular integration of new modalities (e.g., audio features via emotion2vec), seamless replacement of outdated components, and reduced computational overhead during training. We demonstrate the feasibility of our approach through a proof-of-concept implementation supporting vision, audio, and text modalities, with the classifier serving as a shared decision-making agent. Our framework not only improves training efficiency but also contributes to the design of more flexible, scalable, and maintainable perception modules for embodied and virtual agents in HAI scenarios.

</details>


### [37] [MolSculpt: Sculpting 3D Molecular Geometries from Chemical Syntax](https://arxiv.org/abs/2512.10991)
*Zhanpeng Chen,Weihao Gao,Shunyu Wang,Yanan Zhu,Hong Meng,Yuexian Zou*

Main category: cs.LG

TL;DR: MolSculpt：利用冻结的1D分子基础模型和3D扩散模型，通过可学习查询提取化学知识并注入扩散模型，实现从化学语法到3D几何的分子生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用1D表示（如SELFIES）确保分子有效性，但未能充分利用1D模型中丰富的化学知识，导致1D语法生成与3D几何实现之间存在脱节。

Method: 提出MolSculpt框架，基于冻结的1D分子基础模型和3D分子扩散模型。引入可学习查询从基础模型中提取固有化学知识，通过可训练投影器将跨模态信息注入扩散模型的条件空间，指导3D几何生成。

Result: 在GEOM-DRUGS和QM9数据集上，MolSculpt在从头3D分子生成和条件3D分子生成方面达到最先进性能，展现出卓越的3D保真度和稳定性。

Conclusion: MolSculpt通过端到端优化将1D潜在化学知识深度整合到3D生成过程中，有效弥合了化学语法与3D几何之间的鸿沟，为药物发现和材料科学提供精确的3D分子几何生成。

Abstract: Generating precise 3D molecular geometries is crucial for drug discovery and material science. While prior efforts leverage 1D representations like SELFIES to ensure molecular validity, they fail to fully exploit the rich chemical knowledge entangled within 1D models, leading to a disconnect between 1D syntactic generation and 3D geometric realization. To bridge this gap, we propose MolSculpt, a novel framework that "sculpts" 3D molecular geometries from chemical syntax. MolSculpt is built upon a frozen 1D molecular foundation model and a 3D molecular diffusion model. We introduce a set of learnable queries to extract inherent chemical knowledge from the foundation model, and a trainable projector then injects this cross-modal information into the conditioning space of the diffusion model to guide the 3D geometry generation. In this way, our model deeply integrates 1D latent chemical knowledge into the 3D generation process through end-to-end optimization. Experiments demonstrate that MolSculpt achieves state-of-the-art (SOTA) performance in \textit{de novo} 3D molecule generation and conditional 3D molecule generation, showing superior 3D fidelity and stability on both the GEOM-DRUGS and QM9 datasets. Code is available at https://github.com/SakuraTroyChen/MolSculpt.

</details>


### [38] [Memoryless Policy Iteration for Episodic POMDPs](https://arxiv.org/abs/2512.11082)
*Roy van Zuijlen,Duarte Antunes*

Main category: cs.LG

TL;DR: 提出一种新的单调改进策略迭代算法，用于求解部分可观测马尔可夫决策过程（POMDPs），通过交替进行单阶段输出策略改进和周期性策略评估，实现计算效率优化


<details>
  <summary>Details</summary>
Motivation: 无记忆和有限记忆策略为求解POMDPs提供了实用替代方案，因为它们直接在输出空间而非高维信念空间操作。然而，将经典方法（如策略迭代）扩展到该设置仍然困难，因为输出过程是非马尔可夫的，使得策略改进步骤在不同阶段相互依赖

Method: 引入新的单调改进策略迭代算法家族，交替进行单阶段输出策略改进和按预定周期模式进行策略评估。进一步开发了无模型变体，从数据中估计价值并直接学习无记忆策略

Result: 该算法家族允许最大化自然计算效率指标的最优模式，并识别出具有最小周期的最简单模式。在多个POMDP示例中，该方法在基于模型和无模型设置下均实现了相对于策略梯度基线和近期专用算法的显著计算加速

Conclusion: 提出的周期性策略迭代方法有效解决了POMDP中无记忆策略优化的计算挑战，在保持单调改进特性的同时实现了显著的计算效率提升，为POMDP求解提供了实用的新工具

Abstract: Memoryless and finite-memory policies offer a practical alternative for solving partially observable Markov decision processes (POMDPs), as they operate directly in the output space rather than in the high-dimensional belief space. However, extending classical methods such as policy iteration to this setting remains difficult; the output process is non-Markovian, making policy-improvement steps interdependent across stages. We introduce a new family of monotonically improving policy-iteration algorithms that alternate between single-stage output-based policy improvements and policy evaluations according to a prescribed periodic pattern. We show that this family admits optimal patterns that maximize a natural computational-efficiency index, and we identify the simplest pattern with minimal period. Building on this structure, we further develop a model-free variant that estimates values from data and learns memoryless policies directly. Across several POMDPs examples, our method achieves significant computational speedups over policy-gradient baselines and recent specialized algorithms in both model-based and model-free settings.

</details>


### [39] [In-Context Multi-Objective Optimization](https://arxiv.org/abs/2512.11114)
*Xinyu Zhang,Conor Hassan,Julien Martinelli,Daolang Huang,Samuel Kaski*

Main category: cs.LG

TL;DR: TAMO是一个完全摊销的通用多目标黑盒优化策略，使用Transformer架构，通过预训练实现跨问题迁移，无需每任务重新训练，大幅提升优化效率。


<details>
  <summary>Details</summary>
Motivation: 多目标贝叶斯优化在实际应用中存在三个主要问题：1）需要针对每个问题定制代理模型和采集函数，难以迁移；2）通常是短视的，而多步规划往往必要；3）在并行或时间敏感循环中增加重新拟合的开销。

Method: 提出TAMO方法，使用Transformer架构处理不同输入和目标维度，通过强化学习预训练策略以最大化累积超体积改进，基于完整查询历史条件生成下一个设计，实现单次前向传播即可提出新设计。

Result: 在合成基准测试和实际任务中，TAMO相比替代方法将提议时间减少50-1000倍，同时在严格评估预算下匹配或改进帕累托前沿质量。

Conclusion: Transformer能够在上下文环境中完全执行多目标优化，消除了每任务的代理模型拟合和采集函数工程，为科学发现工作流程开辟了基础模型式即插即用优化器的新路径。

Abstract: Balancing competing objectives is omnipresent across disciplines, from drug design to autonomous systems. Multi-objective Bayesian optimization is a promising solution for such expensive, black-box problems: it fits probabilistic surrogates and selects new designs via an acquisition function that balances exploration and exploitation. In practice, it requires tailored choices of surrogate and acquisition that rarely transfer to the next problem, is myopic when multi-step planning is often required, and adds refitting overhead, particularly in parallel or time-sensitive loops. We present TAMO, a fully amortized, universal policy for multi-objective black-box optimization. TAMO uses a transformer architecture that operates across varying input and objective dimensions, enabling pretraining on diverse corpora and transfer to new problems without retraining: at test time, the pretrained model proposes the next design with a single forward pass. We pretrain the policy with reinforcement learning to maximize cumulative hypervolume improvement over full trajectories, conditioning on the entire query history to approximate the Pareto frontier. Across synthetic benchmarks and real tasks, TAMO produces fast proposals, reducing proposal time by 50-1000x versus alternatives while matching or improving Pareto quality under tight evaluation budgets. These results show that transformers can perform multi-objective optimization entirely in-context, eliminating per-task surrogate fitting and acquisition engineering, and open a path to foundation-style, plug-and-play optimizers for scientific discovery workflows.

</details>


### [40] [Clip-and-Verify: Linear Constraint-Driven Domain Clipping for Accelerating Neural Network Verification](https://arxiv.org/abs/2512.11087)
*Duo Zhou,Jorge Chavez,Hesun Chen,Grani A. Hanasusanto,Huan Zhang*

Main category: cs.LG

TL;DR: 提出线性约束驱动剪裁框架，通过利用线性约束减少分支定界中的子问题数量并改进网络中间边界，显著提升神经网络验证器效率。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络验证器在处理复杂验证问题时，分支定界过程会产生大量子问题，导致计算开销大。需要更高效的方法来减少子问题数量并改进边界计算。

Method: 提出线性约束驱动剪裁框架，开发两种算法：1) 利用线性约束减少已验证或无关的输入空间；2) 直接改进网络中间边界。使用专门的GPU程序高效处理线性约束，无需昂贵的外部求解器。

Result: Clip-and-Verify在多个基准测试中持续收紧边界，分支定界子问题数量最多减少96%，在多个基准测试中达到最先进的验证精度，成为VNN-COMP 2025获胜者。

Conclusion: 线性约束驱动剪裁框架是高效可扩展的神经网络验证增强方法，能显著减少分支定界计算开销，提升验证效率，已集成到α,β-CROWN验证器中。

Abstract: State-of-the-art neural network (NN) verifiers demonstrate that applying the branch-and-bound (BaB) procedure with fast bounding techniques plays a key role in tackling many challenging verification properties. In this work, we introduce the linear constraint-driven clipping framework, a class of scalable and efficient methods designed to enhance the efficacy of NN verifiers. Under this framework, we develop two novel algorithms that efficiently utilize linear constraints to 1) reduce portions of the input space that are either verified or irrelevant to a subproblem in the context of branch-and-bound, and 2) directly improve intermediate bounds throughout the network. The process novelly leverages linear constraints that often arise from bound propagation methods and is general enough to also incorporate constraints from other sources. It efficiently handles linear constraints using a specialized GPU procedure that can scale to large neural networks without the use of expensive external solvers. Our verification procedure, Clip-and-Verify, consistently tightens bounds across multiple benchmarks and can significantly reduce the number of subproblems handled during BaB. We show that our clipping algorithms can be integrated with BaB-based verifiers such as $α,β$-CROWN, utilizing either the split constraints in activation-space BaB or the output constraints that denote the unverified input space. We demonstrate the effectiveness of our procedure on a broad range of benchmarks where, in some instances, we witness a 96% reduction in the number of subproblems during branch-and-bound, and also achieve state-of-the-art verified accuracy across multiple benchmarks. Clip-and-Verify is part of the $α,β$-CROWN verifier (http://abcrown.org), the VNN-COMP 2025 winner. Code available at https://github.com/Verified-Intelligence/Clip_and_Verify.

</details>


### [41] [Hyperbolic Gaussian Blurring Mean Shift: A Statistical Mode-Seeking Framework for Clustering in Curved Spaces](https://arxiv.org/abs/2512.11448)
*Arghya Pratihar,Arnab Seal,Swagatam Das,Inesh Chattopadhyay*

Main category: cs.LG

TL;DR: HypeGBMS：将高斯模糊均值漂移扩展到双曲空间的聚类方法，能有效处理层次结构数据


<details>
  <summary>Details</summary>
Motivation: 传统高斯模糊均值漂移(GBMS)在欧几里得空间中有效，但难以处理具有层次或树状结构的数据集。需要将均值漂移扩展到双曲空间以捕捉数据的潜在层次结构。

Method: 将GBMS扩展到双曲空间，用双曲距离替换欧几里得计算，使用Möbius加权均值确保所有更新与空间几何保持一致，保留密度寻求行为。

Result: 在11个真实世界数据集上的实验表明，HypeGBMS在非欧几里得设置下显著优于传统均值漂移聚类方法，证明了其鲁棒性和有效性。

Conclusion: HypeGBMS将经典均值漂移聚类与双曲表示学习相结合，为弯曲空间中的密度聚类提供了原则性方法，能有效捕捉数据层次结构。

Abstract: Clustering is a fundamental unsupervised learning task for uncovering patterns in data. While Gaussian Blurring Mean Shift (GBMS) has proven effective for identifying arbitrarily shaped clusters in Euclidean space, it struggles with datasets exhibiting hierarchical or tree-like structures. In this work, we introduce HypeGBMS, a novel extension of GBMS to hyperbolic space. Our method replaces Euclidean computations with hyperbolic distances and employs Möbius-weighted means to ensure that all updates remain consistent with the geometry of the space. HypeGBMS effectively captures latent hierarchies while retaining the density-seeking behavior of GBMS. We provide theoretical insights into convergence and computational complexity, along with empirical results that demonstrate improved clustering quality in hierarchical datasets. This work bridges classical mean-shift clustering and hyperbolic representation learning, offering a principled approach to density-based clustering in curved spaces. Extensive experimental evaluations on $11$ real-world datasets demonstrate that HypeGBMS significantly outperforms conventional mean-shift clustering methods in non-Euclidean settings, underscoring its robustness and effectiveness.

</details>


### [42] [Investigating ECG Diagnosis with Ambiguous Labels using Partial Label Learning](https://arxiv.org/abs/2512.11095)
*Sana Rahmani,Javad Hashemi,Ali Etemad*

Main category: cs.LG

TL;DR: 首次系统研究部分标签学习(PLL)在ECG诊断中的应用，评估9种PLL算法对多种临床模糊标签的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 心电图(ECG)诊断中存在固有的标签模糊问题，源于重叠病症和诊断分歧，但现有ECG模型假设标签干净无歧义，限制了模型在真实临床环境中的发展和评估

Method: 将9种部分标签学习(PLL)算法适配到多标签ECG诊断中，使用多种临床驱动的模糊生成策略进行评估，包括非结构化(随机)和结构化(心脏病专家相似性、治疗关系、诊断分类)模糊

Result: 在PTB-XL和Chapman数据集上的实验表明，PLL方法对不同类型和程度的模糊具有显著不同的鲁棒性，揭示了当前PLL方法在临床环境中的关键局限性

Conclusion: 为ECG诊断开发鲁棒且临床对齐的模糊感知学习框架指明了未来方向，强调需要更适应临床现实的标签模糊处理方法

Abstract: Label ambiguity is an inherent problem in real-world electrocardiogram (ECG) diagnosis, arising from overlapping conditions and diagnostic disagreement. However, current ECG models are trained under the assumption of clean and non-ambiguous annotations, which limits both the development and the meaningful evaluation of models under real-world conditions. Although Partial Label Learning (PLL) frameworks are designed to learn from ambiguous labels, their effectiveness in medical time-series domains, ECG in particular, remains largely unexplored. In this work, we present the first systematic study of PLL methods for ECG diagnosis. We adapt nine PLL algorithms to multi-label ECG diagnosis and evaluate them using a diverse set of clinically motivated ambiguity generation strategies, capturing both unstructured (e.g., random) and structured ambiguities (e.g., cardiologist-derived similarities, treatment relationships, and diagnostic taxonomies). Our experiments on the PTB-XL and Chapman datasets demonstrate that PLL methods vary substantially in their robustness to different types and degrees of ambiguity. Through extensive analysis, we identify key limitations of current PLL approaches in clinical settings and outline future directions for developing robust and clinically aligned ambiguity-aware learning frameworks for ECG diagnosis.

</details>


### [43] [Contrastive Time Series Forecasting with Anomalies](https://arxiv.org/abs/2512.11526)
*Joel Ekstrand,Zahra Taghiyarrenani,Slawomir Nowaczyk*

Main category: cs.LG

TL;DR: Co-TSFA是一个对比学习框架，通过区分短期异常和持久性分布漂移来改进时间序列预测，在异常条件下提升性能的同时保持正常数据的准确性。


<details>
  <summary>Details</summary>
Motivation: 现实世界的时间序列预测中，有些异常事件具有持久影响需要响应，而有些短期异常应该被忽略。传统预测模型无法区分这两类异常，要么对噪声过度反应，要么错过重要的分布漂移。

Method: 提出Co-TSFA正则化框架，通过生成输入级和输入-输出级数据增强来分别建模预测无关和预测相关的异常，并引入潜在输出对齐损失将表示变化与预测变化关联起来。

Result: 在Traffic、Electricity基准数据集和真实世界现金需求数据集上的实验表明，Co-TSFA在异常条件下提升了预测性能，同时在正常数据上保持了准确性。

Conclusion: Co-TSFA能够有效区分短期异常和持久性分布漂移，使模型对无关扰动保持鲁棒性，同时对有意义的分布变化保持敏感性，提高了时间序列预测在现实场景中的实用性。

Abstract: Time series forecasting predicts future values from past data. In real-world settings, some anomalous events have lasting effects and influence the forecast, while others are short-lived and should be ignored. Standard forecasting models fail to make this distinction, often either overreacting to noise or missing persistent shifts. We propose Co-TSFA (Contrastive Time Series Forecasting with Anomalies), a regularization framework that learns when to ignore anomalies and when to respond. Co-TSFA generates input-only and input-output augmentations to model forecast-irrelevant and forecast-relevant anomalies, and introduces a latent-output alignment loss that ties representation changes to forecast changes. This encourages invariance to irrelevant perturbations while preserving sensitivity to meaningful distributional shifts. Experiments on the Traffic and Electricity benchmarks, as well as on a real-world cash-demand dataset, demonstrate that Co-TSFA improves performance under anomalous conditions while maintaining accuracy on normal data. An anonymized GitHub repository with the implementation of Co-TSFA is provided and will be made public upon acceptance.

</details>


### [44] [Limits and Gains of Test-Time Scaling in Vision-Language Reasoning](https://arxiv.org/abs/2512.11109)
*Mohammadjavad Ahmadpour,Amirmahdi Meighani,Payam Taebi,Omid Ghahroodi,Amirmohammad Izadi,Mahdieh Soleymani Baghshah*

Main category: cs.LG

TL;DR: 本文系统研究了测试时缩放（TTS）在多模态视觉语言模型（VLMs）中的应用效果，发现其有效性高度依赖于模型类型和任务特性，并非通用解决方案。


<details>
  <summary>Details</summary>
Motivation: 测试时缩放（TTS）在大型语言模型中已被证明能有效提升推理能力，但在多模态视觉语言模型（VLMs）中的应用尚未得到充分探索。本文旨在系统研究TTS在不同VLMs上的应用效果，填补这一研究空白。

Method: 对开源和闭源视觉语言模型在多个基准测试上进行系统实证研究，比较结构化推理、迭代自我优化等推理方法的效果，分析不同数据集上的表现差异。

Result: 闭源模型能从结构化推理和迭代优化中持续受益，而开源VLMs表现不一致：外部验证提供最可靠的提升，迭代优化反而常降低性能。TTS效果高度数据集依赖，在多步推理任务上改进明显，但在感知为主的基准测试上增益有限。

Conclusion: 测试时缩放并非通用解决方案，必须根据模型能力和任务特性进行定制。这推动了未来自适应TTS策略和多模态奖励模型的研究方向。

Abstract: Test-time scaling (TTS) has emerged as a powerful paradigm for improving the reasoning ability of Large Language Models (LLMs) by allocating additional computation at inference, yet its application to multimodal systems such as Vision-Language Models (VLMs) remains underexplored. In this work, we present a systematic empirical study of inference time reasoning methods applied across both open-source and closed-source VLMs on different benchmarks. Our results reveal that while closed-source models consistently benefit from structured reasoning and iterative Self-Refinement, open-source VLMs show inconsistent behavior: external verification provides the most reliable gains, whereas iterative refinement often degrades performance. We further find that the effectiveness of TTS is dataset-dependent, yielding clear improvements on multi-step reasoning tasks but offering only limited gains on perception-focused benchmarks. These findings demonstrate that TTS is not a universal solution and must be tailored to both model capabilities and task characteristics, motivating future work on adaptive TTS strategies and multimodal reward models.

</details>


### [45] [Softmax as Linear Attention in the Large-Prompt Regime: a Measure-based Perspective](https://arxiv.org/abs/2512.11784)
*Etienne Boursier,Claire Boyer*

Main category: cs.LG

TL;DR: 论文提出了一个基于测度的统一框架来分析softmax注意力，证明了在长提示下softmax注意力会收敛到线性算子，并建立了输出和梯度的非渐近集中界。


<details>
  <summary>Details</summary>
Motivation: softmax注意力是Transformer架构的核心组件，但其非线性结构给理论分析带来了重大挑战。现有研究缺乏对softmax注意力在有限和无限提示下的统一理论框架。

Method: 开发了基于测度的统一框架，利用softmax算子在无限提示极限下收敛到线性算子的性质。对于i.i.d.高斯输入，建立了输出和梯度的非渐近集中界，分析了在上下文学习中的训练稳定性。

Result: 证明了在足够长的提示下，softmax注意力会继承其线性对应物的分析结构，为研究大型提示机制下的softmax注意力层提供了理论工具包。

Conclusion: 该框架使得为线性注意力开发的优化分析可以直接转移到softmax注意力，为理解softmax注意力的训练动态和统计行为提供了理论基础。

Abstract: Softmax attention is a central component of transformer architectures, yet its nonlinear structure poses significant challenges for theoretical analysis. We develop a unified, measure-based framework for studying single-layer softmax attention under both finite and infinite prompts. For i.i.d. Gaussian inputs, we lean on the fact that the softmax operator converges in the infinite-prompt limit to a linear operator acting on the underlying input-token measure. Building on this insight, we establish non-asymptotic concentration bounds for the output and gradient of softmax attention, quantifying how rapidly the finite-prompt model approaches its infinite-prompt counterpart, and prove that this concentration remains stable along the entire training trajectory in general in-context learning settings with sub-Gaussian tokens. In the case of in-context linear regression, we use the tractable infinite-prompt dynamics to analyze training at finite prompt length. Our results allow optimization analyses developed for linear attention to transfer directly to softmax attention when prompts are sufficiently long, showing that large-prompt softmax attention inherits the analytical structure of its linear counterpart. This, in turn, provides a principled and broadly applicable toolkit for studying the training dynamics and statistical behavior of softmax attention layers in large prompt regimes.

</details>


### [46] [Refining Graphical Neural Network Predictions Using Flow Matching for Optimal Power Flow with Constraint-Satisfaction Guarantee](https://arxiv.org/abs/2512.11127)
*Kshitiz Khanal*

Main category: cs.LG

TL;DR: 提出结合物理信息图神经网络与连续流匹配的两阶段学习框架，用于快速求解直流最优潮流问题，在保持100%可行性的同时实现接近最优解。


<details>
  <summary>Details</summary>
Motivation: 传统优化求解器计算成本高，难以满足大规模电力系统实时调度需求；现有机器学习方法在约束满足和成本最优性方面存在不足。

Method: 两阶段框架：第一阶段使用物理信息图神经网络生成可行初始解，损失函数编码经济调度最优性条件、基尔霍夫定律和KKT互补条件；第二阶段使用连续流匹配技术通过向量场回归优化解。

Result: 在IEEE 30节点系统上测试，额定负荷下成本差距低于0.1%，极端条件下低于3%，同时保持100%可行性。

Conclusion: 该框架在快速神经网络预测与精确但缓慢的数值求解器之间架起桥梁，为高可再生能源渗透的现代电力系统提供实用解决方案。

Abstract: The DC Optimal Power Flow (DC-OPF) problem is fundamental to power system operations, requiring rapid solutions for real-time grid management. While traditional optimization solvers provide optimal solutions, their computational cost becomes prohibitive for large-scale systems requiring frequent recalculations. Machine learning approaches offer promise for acceleration but often struggle with constraint satisfaction and cost optimality. We present a novel two-stage learning framework that combines physics-informed Graph Neural Networks (GNNs) with Continuous Flow Matching (CFM) for solving DC-OPF problems. Our approach embeds fundamental physical principles--including economic dispatch optimality conditions, Kirchhoff's laws, and Karush-Kuhn-Tucker (KKT) complementarity conditions--directly into the training objectives. The first stage trains a GNN to produce feasible initial solutions by learning from physics-informed losses that encode power system constraints. The second stage employs CFM, a simulation-free continuous normalizing flow technique, to refine these solutions toward optimality through learned vector field regression. Evaluated on the IEEE 30-bus system across five load scenarios ranging from 70\% to 130\% nominal load, our method achieves near-optimal solutions with cost gaps below 0.1\% for nominal loads and below 3\% for extreme conditions, while maintaining 100\% feasibility. Our framework bridges the gap between fast but approximate neural network predictions and optimal but slow numerical solvers, offering a practical solution for modern power systems with high renewable penetration requiring frequent dispatch updates.

</details>


### [47] [Fairness-Regularized Online Optimization with Switching Costs](https://arxiv.org/abs/2512.11131)
*Pengfei Li,Yuelin Han,Adam Wierman,Shaolei Ren*

Main category: cs.LG

TL;DR: 提出FairOBD算法，在考虑切换成本的平滑在线凸优化中同时处理公平性，通过引入辅助变量分解长期公平成本，在动态计算资源分配实验中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 在线优化问题中公平性和动作平滑性是两个关键考虑因素，但现有研究尚未同时解决这两个问题。本文研究具有切换成本的公平性正则化平滑在线凸优化这一新挑战性设置。

Method: 提出FairOBD算法：1) 通过引入辅助变量将长期公平成本分解为一系列在线成本；2) 利用辅助变量正则化在线动作以实现公平结果；3) 采用新方法考虑切换成本。

Result: 理论证明：1) 即使没有切换成本，任何在线算法都无法实现相对于离线最优算法的次线性遗憾或有限竞争比；2) FairOBD针对参数化约束的最优离线算法提供最坏情况渐近竞争比。实验表明FairOBD能有效降低总公平正则化成本并更好促进公平结果。

Conclusion: FairODB算法成功解决了公平性和平滑性同时考虑的在线优化问题，通过理论分析和实验验证了其有效性，为动态计算资源分配等应用提供了新解决方案。

Abstract: Fairness and action smoothness are two crucial considerations in many online optimization problems, but they have yet to be addressed simultaneously. In this paper, we study a new and challenging setting of fairness-regularized smoothed online convex optimization with switching costs. First, to highlight the fundamental challenges introduced by the long-term fairness regularizer evaluated based on the entire sequence of actions, we prove that even without switching costs, no online algorithms can possibly achieve a sublinear regret or finite competitive ratio compared to the offline optimal algorithm as the problem episode length $T$ increases. Then, we propose FairOBD (Fairness-regularized Online Balanced Descent), which reconciles the tension between minimizing the hitting cost, switching cost, and fairness cost. Concretely, FairOBD decomposes the long-term fairness cost into a sequence of online costs by introducing an auxiliary variable and then leverages the auxiliary variable to regularize the online actions for fair outcomes. Based on a new approach to account for switching costs, we prove that FairOBD offers a worst-case asymptotic competitive ratio against a novel benchmark -- the optimal offline algorithm with parameterized constraints -- by considering $T\to\infty$. Finally, we run trace-driven experiments of dynamic computing resource provisioning for socially responsible AI inference to empirically evaluate FairOBD, showing that FairOBD can effectively reduce the total fairness-regularized cost and better promote fair outcomes compared to existing baseline solutions.

</details>


### [48] [The Vekua Layer: Exact Physical Priors for Implicit Neural Representations via Generalized Analytic Functions](https://arxiv.org/abs/2512.11138)
*Vladimer Khasia*

Main category: cs.LG

TL;DR: 提出Vekua Layer (VL)方法，基于广义解析函数理论，将隐式神经表示学习转化为凸最小二乘问题，在椭圆PDE上实现机器精度重建和噪声鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 隐式神经表示(INRs)存在频谱偏差和非凸优化计算成本高的问题，需要更高效、稳定的物理场参数化方法。

Method: 基于广义解析函数理论，将假设空间限制在控制微分算子的核空间（使用调和基和傅里叶-贝塞尔基），将学习任务从迭代梯度下降转化为通过线性投影解决的严格凸最小二乘问题。

Result: 在齐次椭圆PDE上，VL在精确重建任务中达到机器精度(MSE≈10^{-33})，在非相干传感器噪声下表现出优越稳定性(MSE≈0.03)，并能从部分边界数据通过解析延拓实现"全息"外推。

Conclusion: VL提供了一种基于物理信息的谱滤波方法，克服了传统INRs的频谱偏差和优化困难，在物理场表示方面具有优越性能和独特能力。

Abstract: Implicit Neural Representations (INRs) have emerged as a powerful paradigm for parameterizing physical fields, yet they often suffer from spectral bias and the computational expense of non-convex optimization. We introduce the Vekua Layer (VL), a differentiable spectral method grounded in the classical theory of Generalized Analytic Functions. By restricting the hypothesis space to the kernel of the governing differential operator -- specifically utilizing Harmonic and Fourier-Bessel bases -- the VL transforms the learning task from iterative gradient descent to a strictly convex least-squares problem solved via linear projection. We evaluate the VL against Sinusoidal Representation Networks (SIRENs) on homogeneous elliptic Partial Differential Equations (PDEs). Our results demonstrate that the VL achieves machine precision ($\text{MSE} \approx 10^{-33}$) on exact reconstruction tasks and exhibits superior stability in the presence of incoherent sensor noise ($\text{MSE} \approx 0.03$), effectively acting as a physics-informed spectral filter. Furthermore, we show that the VL enables "holographic" extrapolation of global fields from partial boundary data via analytic continuation, a capability absent in standard coordinate-based approximations.

</details>


### [49] [High-Dimensional Surrogate Modeling for Closed-Loop Learning of Neural-Network-Parameterized Model Predictive Control](https://arxiv.org/abs/2512.11705)
*Sebastian Hirt,Valentinus Suwanto,Hendrik Alsmeier,Maik Pfefferkorn,Rolf Findeisen*

Main category: cs.LG

TL;DR: 贝叶斯神经网络作为代理模型能有效优化高维控制器参数，相比传统高斯过程在密集高维参数化场景中表现更优


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化在处理密集高维控制器参数（如模型预测控制器）时效果有限，因为标准代理模型难以捕捉高维空间结构，需要更有效的代理模型

Method: 使用贝叶斯神经网络作为代理模型，比较了三种方法：带Matern核的高斯过程、有限宽度贝叶斯神经网络和无限宽度贝叶斯神经网络，在倒立摆任务上进行测试

Result: 贝叶斯神经网络代理模型能更快更可靠地收敛闭环成本，成功优化数百维参数化；无限宽度贝叶斯神经网络在超过1000个参数时仍保持性能，而Matern核高斯过程迅速失效

Conclusion: 贝叶斯神经网络代理模型适合学习密集高维控制器参数化，为基于学习的控制器设计中代理模型选择提供了实用指导

Abstract: Learning controller parameters from closed-loop data has been shown to improve closed-loop performance. Bayesian optimization, a widely used black-box and sample-efficient learning method, constructs a probabilistic surrogate of the closed-loop performance from few experiments and uses it to select informative controller parameters. However, it typically struggles with dense high-dimensional controller parameterizations, as they may appear, for example, in tuning model predictive controllers, because standard surrogate models fail to capture the structure of such spaces. This work suggests that the use of Bayesian neural networks as surrogate models may help to mitigate this limitation. Through a comparison between Gaussian processes with Matern kernels, finite-width Bayesian neural networks, and infinite-width Bayesian neural networks on a cart-pole task, we find that Bayesian neural network surrogate models achieve faster and more reliable convergence of the closed-loop cost and enable successful optimization of parameterizations with hundreds of dimensions. Infinite-width Bayesian neural networks also maintain performance in settings with more than one thousand parameters, whereas Matern-kernel Gaussian processes rapidly lose effectiveness. These results indicate that Bayesian neural network surrogate models may be suitable for learning dense high-dimensional controller parameterizations and offer practical guidance for selecting surrogate models in learning-based controller design.

</details>


### [50] [Gradient Descent as a Perceptron Algorithm: Understanding Dynamics and Implicit Acceleration](https://arxiv.org/abs/2512.11587)
*Alexander Tyurin*

Main category: cs.LG

TL;DR: 论文通过将梯度下降在神经网络训练中的步骤简化为广义感知机算法，揭示了非线性模型相比线性模型能实现更快的迭代复杂度，解释了神经网络中的隐式加速现象。


<details>
  <summary>Details</summary>
Motivation: 理解神经网络训练中梯度下降的优化动态（包括收敛速度、迭代轨迹、函数值振荡以及隐式加速现象）是一个具有挑战性的问题。作者希望为这一复杂动态提供新的理论视角。

Method: 使用逻辑损失分析非线性模型，将梯度下降步骤简化为广义感知机算法（Rosenblatt, 1958）。利用经典线性代数工具分析简化后的算法步骤，并在一个最小化示例上进行理论证明。

Result: 理论证明在两层模型中，非线性相比线性模型能实现更快的迭代复杂度：非线性模型为$\tilde{O}(\sqrt{d})$，而线性模型为$Ω(d)$，其中$d$是特征数量。这解释了神经网络中的隐式加速现象。理论结果得到了大量数值实验的支持。

Conclusion: 通过将梯度下降简化为广义感知机算法，为神经网络优化动态提供了新的理论视角，揭示了非线性带来的加速效应，这一观点有望进一步推动神经网络优化研究。

Abstract: Even for the gradient descent (GD) method applied to neural network training, understanding its optimization dynamics, including convergence rate, iterate trajectories, function value oscillations, and especially its implicit acceleration, remains a challenging problem. We analyze nonlinear models with the logistic loss and show that the steps of GD reduce to those of generalized perceptron algorithms (Rosenblatt, 1958), providing a new perspective on the dynamics. This reduction yields significantly simpler algorithmic steps, which we analyze using classical linear algebra tools. Using these tools, we demonstrate on a minimalistic example that the nonlinearity in a two-layer model can provably yield a faster iteration complexity $\tilde{O}(\sqrt{d})$ compared to $Ω(d)$ achieved by linear models, where $d$ is the number of features. This helps explain the optimization dynamics and the implicit acceleration phenomenon observed in neural networks. The theoretical results are supported by extensive numerical experiments. We believe that this alternative view will further advance research on the optimization of neural networks.

</details>


### [51] [Autoencoder-based Semi-Supervised Dimensionality Reduction and Clustering for Scientific Ensembles](https://arxiv.org/abs/2512.11145)
*Lennard Manuel,Hamid Gadirov,Steffen Frey*

Main category: cs.LG

TL;DR: 提出一种结合聚类损失和对比损失的增强自编码器框架，用于高维科学集成数据集的降维可视化，在土壤通道结构和液滴冲击动力学数据集上表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 高维复杂的科学集成数据集在分析和可视化方面面临重大挑战，传统降维技术和自编码器难以有效处理此类数据，需要改进方法来提升可视化和可解释性。

Method: 使用EfficientNetV2为无标签数据生成伪标签，构建增强自编码器框架，结合重建损失、基于软轮廓分数的聚类损失和对比损失，联合优化使相似数据点聚集而不同簇分离，最后用UMAP生成2D投影。

Result: 在土壤通道结构（MCMC生成）和液滴冲击动力学两个科学集成数据集上的实验表明，结合聚类或对比损失的模型在轮廓分数评估下略优于基线方法。

Conclusion: 提出的增强自编码器框架能有效改善高维科学集成数据集的可视化和特征提取，聚类和对比损失的结合有助于获得更有意义的潜在表示。

Abstract: Analyzing and visualizing scientific ensemble datasets with high dimensionality and complexity poses significant challenges. Dimensionality reduction techniques and autoencoders are powerful tools for extracting features, but they often struggle with such high-dimensional data. This paper presents an enhanced autoencoder framework that incorporates a clustering loss, based on the soft silhouette score, alongside a contrastive loss to improve the visualization and interpretability of ensemble datasets. First, EfficientNetV2 is used to generate pseudo-labels for the unlabeled portions of the scientific ensemble datasets. By jointly optimizing the reconstruction, clustering, and contrastive objectives, our method encourages similar data points to group together while separating distinct clusters in the latent space. UMAP is subsequently applied to this latent representation to produce 2D projections, which are evaluated using the silhouette score. Multiple types of autoencoders are evaluated and compared based on their ability to extract meaningful features. Experiments on two scientific ensemble datasets - channel structures in soil derived from Markov chain Monte Carlo, and droplet-on-film impact dynamics - show that models incorporating clustering or contrastive loss marginally outperform the baseline approaches.

</details>


### [52] [Harnessing Rich Multi-Modal Data for Spatial-Temporal Homophily-Embedded Graph Learning Across Domains and Localities](https://arxiv.org/abs/2512.11178)
*Takuya Kurihana,Xiaojian Zhang,Wing Yee Au,Hon Yung Wong*

Main category: cs.LG

TL;DR: 提出一个异质数据管道，用于融合时空变化的多模态城市数据，通过图学习整合空间同质性，实现跨城市和跨领域的可迁移预测框架。


<details>
  <summary>Details</summary>
Motivation: 现代城市依赖数据驱动决策，但城市数据存在格式异质、收集标准不一的问题。国家级数据集虽丰富但具有显著异质性和多模态性，需要统一框架来处理跨域城市问题。

Method: 提出异质数据管道，执行跨域数据融合，处理时空变化的时间序列数据。数据学习模块将空间变化数据集的同质性整合到图学习中，将不同地区的信息嵌入模型。使用超过50个数据源。

Result: 在五个真实世界观察中展示了框架的泛化性和灵活性，使用多种公开数据集（如拼车、交通事故、犯罪报告）。结果显示框架具有强预测性能，且在迁移到新地区或领域时只需最小化重新配置。

Conclusion: 该研究推进了以可扩展方式构建数据驱动的城市系统的目标，解决了智慧城市分析中最紧迫的挑战之一，为跨城市和跨领域的智能决策提供了有效工具。

Abstract: Modern cities are increasingly reliant on data-driven insights to support decision making in areas such as transportation, public safety and environmental impact. However, city-level data often exists in heterogeneous formats, collected independently by local agencies with diverse objectives and standards. Despite their numerous, wide-ranging, and uniformly consumable nature, national-level datasets exhibit significant heterogeneity and multi-modality. This research proposes a heterogeneous data pipeline that performs cross-domain data fusion over time-varying, spatial-varying and spatial-varying time-series datasets. We aim to address complex urban problems across multiple domains and localities by harnessing the rich information over 50 data sources. Specifically, our data-learning module integrates homophily from spatial-varying dataset into graph-learning, embedding information of various localities into models. We demonstrate the generalizability and flexibility of the framework through five real-world observations using a variety of publicly accessible datasets (e.g., ride-share, traffic crash, and crime reports) collected from multiple cities. The results show that our proposed framework demonstrates strong predictive performance while requiring minimal reconfiguration when transferred to new localities or domains. This research advances the goal of building data-informed urban systems in a scalable way, addressing one of the most pressing challenges in smart city analytics.

</details>


### [53] [Bandwidth-constrained Variational Message Encoding for Cooperative Multi-agent Reinforcement Learning](https://arxiv.org/abs/2512.11179)
*Wei Duan,Jie Lu,En Yu,Junyu Xuan*

Main category: cs.LG

TL;DR: 提出BVME方法，在带宽受限的多智能体强化学习中通过变分消息编码实现高效通信，在保持性能的同时大幅减少消息维度


<details>
  <summary>Details</summary>
Motivation: 现有基于图的MARL方法虽然能学习稀疏协调图（决定谁与谁通信），但未解决在硬带宽约束下应该传输什么信息的问题。简单的维度降维会持续降低协调性能，而确定性投影缺乏控制压缩方式的机制。

Method: 提出带宽受限变分消息编码（BVME），将消息视为从学习到的高斯后验中采样的样本，通过KL散度正则化到无信息先验。该变分框架通过可解释的超参数提供原则性、可调节的压缩强度控制。

Result: 在SMACv1、SMACv2和MPE基准测试中，BVME在使用67-83%更少消息维度的同时，实现了相当或更优的性能。在稀疏图上增益最为显著，其中消息质量对协调影响关键。消融实验显示对带宽的U形敏感性，BVME在极端比率下表现优异且开销最小。

Conclusion: BVME为带宽受限的多智能体协调提供了有效的消息编码解决方案，通过变分框架实现可控压缩，在保持协调性能的同时大幅降低通信成本。

Abstract: Graph-based multi-agent reinforcement learning (MARL) enables coordinated behavior under partial observability by modeling agents as nodes and communication links as edges. While recent methods excel at learning sparse coordination graphs-determining who communicates with whom-they do not address what information should be transmitted under hard bandwidth constraints. We study this bandwidth-limited regime and show that naive dimensionality reduction consistently degrades coordination performance. Hard bandwidth constraints force selective encoding, but deterministic projections lack mechanisms to control how compression occurs. We introduce Bandwidth-constrained Variational Message Encoding (BVME), a lightweight module that treats messages as samples from learned Gaussian posteriors regularized via KL divergence to an uninformative prior. BVME's variational framework provides principled, tunable control over compression strength through interpretable hyperparameters, directly constraining the representations used for decision-making. Across SMACv1, SMACv2, and MPE benchmarks, BVME achieves comparable or superior performance while using 67--83% fewer message dimensions, with gains most pronounced on sparse graphs where message quality critically impacts coordination. Ablations reveal U-shaped sensitivity to bandwidth, with BVME excelling at extreme ratios while adding minimal overhead.

</details>


### [54] [Progress over Points: Reframing LM Benchmarks Around Scientific Objectives](https://arxiv.org/abs/2512.11183)
*Alwin Jin,Sean M. Hendryx,Vaskar Nath*

Main category: cs.LG

TL;DR: 论文提出"进展导向基准测试"新范式，用NanoGPT速度挑战作为实例，将基准测试从静态问题排行榜转变为可测量的开放式科学研究工具


<details>
  <summary>Details</summary>
Motivation: 当前LLM基准测试主要针对静态已解决问题（如数学应用题），这种方法限制了可测量的进步类型。需要一种新的基准测试范式，其目标本身就是科学进步的核心，从而推动领域发展

Method: 基于NanoGPT速度挑战实例化进展导向环境，标准化数据集切片、参考模型和训练工具，提供丰富遥测数据，包含运行时验证和防作弊检查。评估聚焦于科学增量：最佳损失值和效率边界

Result: 在该环境中实现了新的最先进训练时间，比之前记录提高了3秒，并定性观察到新颖算法思想的出现。模型和智能体之间的比较仍然可能，但只是手段而非目的

Conclusion: 基准测试的目的是催化语言建模堆栈的可重用改进，推动社区从静态问题排行榜转向开放式但可测量的科学问题的测试时研究，将"基准测试"重新定义为科学进步的工具

Abstract: Current benchmarks that test LLMs on static, already-solved problems (e.g., math word problems) effectively demonstrated basic capability acquisition. The natural progression has been toward larger, more comprehensive and challenging collections of static problems, an approach that inadvertently constrains the kinds of advances we can measure and incentivize. To address this limitation, we argue for progress-oriented benchmarks, problem environments whose objectives are themselves the core targets of scientific progress, so that achieving state of the art on the benchmark advances the field. As a introductory step, we instantiate an environment based on the NanoGPT speedrun. The environment standardizes a dataset slice, a reference model and training harness, and rich telemetry, with run-time verification and anti-gaming checks. Evaluation centers on the scientific delta achieved: best-attained loss and the efficiency frontier. Using this environment, we achieve a new state-of-the-art training time, improving upon the previous record by 3 seconds, and qualitatively observe the emergence of novel algorithmic ideas. Moreover, comparisons between models and agents remain possible, but they are a means, not the end; the benchmark's purpose is to catalyze reusable improvements to the language modeling stack. With this release, the overarching goal is to seed a community shift from static problem leaderboards to test-time research on open-ended yet measurable scientific problems. In this new paradigm, progress on the benchmark is progress on the science, thus reframing "benchmarking" as a vehicle for scientific advancement.

</details>


### [55] [On the failure of ReLU activation for physics-informed machine learning](https://arxiv.org/abs/2512.11184)
*Conor Rowan*

Main category: cs.LG

TL;DR: ReLU激活函数在物理信息机器学习中表现不佳的原因是其分段线性特性导致自动微分无法正确处理不连续场的导数，从而错误指定了损失函数的梯度。


<details>
  <summary>Details</summary>
Motivation: 研究ReLU激活函数在物理信息机器学习中表现不佳的根本原因。虽然已知ReLU的分段线性形式使其无法用于二阶微分方程，但本研究旨在解释为何ReLU在仅涉及一阶导数的变分问题中也会失败。

Method: 通过分析自动微分在PyTorch中的实现机制，诊断ReLU在物理信息机器学习训练过程中的问题。研究重点放在激活函数的二阶导数上，这些导数虽然不在损失函数公式中直接出现，但在训练过程中会被计算。

Result: 发现ReLU失败的根本原因是自动微分无法正确表征不连续场的导数。PyTorch的自动微分在处理ReLU这样的分段线性函数时，无法准确计算二阶导数，导致物理信息损失函数的梯度被错误指定。

Conclusion: ReLU在物理信息机器学习中的糟糕表现源于其数学特性与自动微分机制的不兼容性。即使在一阶变分问题中，ReLU的二阶导数问题也会通过训练过程影响梯度计算，导致性能下降。这解释了为什么sigmoid、tanh和swish等平滑激活函数表现更好。

Abstract: Physics-informed machine learning uses governing ordinary and/or partial differential equations to train neural networks to represent the solution field. Like any machine learning problem, the choice of activation function influences the characteristics and performance of the solution obtained from physics-informed training. Several studies have compared common activation functions on benchmark differential equations, and have unanimously found that the rectified linear unit (ReLU) is outperformed by competitors such as the sigmoid, hyperbolic tangent, and swish activation functions. In this work, we diagnose the poor performance of ReLU on physics-informed machine learning problems. While it is well-known that the piecewise linear form of ReLU prevents it from being used on second-order differential equations, we show that ReLU fails even on variational problems involving only first derivatives. We identify the cause of this failure as second derivatives of the activation, which are taken not in the formulation of the loss, but in the process of training. Namely, we show that automatic differentiation in PyTorch fails to characterize derivatives of discontinuous fields, which causes the gradient of the physics-informed loss to be mis-specified, thus explaining the poor performance of ReLU.

</details>


### [56] [Beyond Memorization: Gradient Projection Enables Selective Learning in Diffusion Models](https://arxiv.org/abs/2512.11194)
*Divya Kothandaraman,Jaclyn Pytlarz*

Main category: cs.LG

TL;DR: 提出梯度投影框架，通过将梯度更新投影到敏感特征嵌入空间的正交补空间，实现概念级特征排除，减少扩散模型记忆化风险


<details>
  <summary>Details</summary>
Motivation: 大规模文本到图像扩散模型的记忆化带来安全和知识产权风险，现有技术无法系统防止禁止概念级特征的内化，需要概念级选择性遗忘方法

Method: 梯度投影框架：在反向传播期间识别并消除与禁止属性嵌入对齐的训练信号，将每个梯度更新投影到敏感特征嵌入空间的正交补空间

Result: 框架显著减少记忆化，同时严格保持生成质量和语义保真度，可与现有防御方法无缝集成

Conclusion: 通过将记忆化控制重新定义为选择性学习，为IP安全和隐私保护的生成AI建立了新范式

Abstract: Memorization in large-scale text-to-image diffusion models poses significant security and intellectual property risks, enabling adversarial attribute extraction and the unauthorized reproduction of sensitive or proprietary features. While conventional dememorization techniques, such as regularization and data filtering, limit overfitting to specific training examples, they fail to systematically prevent the internalization of prohibited concept-level features. Simply discarding all images containing a sensitive feature wastes invaluable training data, necessitating a method for selective unlearning at the concept level.
  To address this, we introduce a Gradient Projection Framework designed to enforce a stringent requirement of concept-level feature exclusion. Our defense operates during backpropagation by systematically identifying and excising training signals aligned with embeddings of prohibited attributes. Specifically, we project each gradient update onto the orthogonal complement of the sensitive feature's embedding space, thereby zeroing out its influence on the model's weights. Our method integrates seamlessly into standard diffusion model training pipelines and complements existing defenses. We analyze our method against an adversary aiming for feature extraction. In extensive experiments, we demonstrate that our framework drastically reduces memorization while rigorously preserving generation quality and semantic fidelity. By reframing memorization control as selective learning, our approach establishes a new paradigm for IP-safe and privacy-preserving generative AI.

</details>


### [57] [Fast EXP3 Algorithms](https://arxiv.org/abs/2512.11201)
*Ryoma Sato,Shinji Ito*

Main category: cs.LG

TL;DR: EXP3算法可在每轮常数时间内实现，作者提出更实用的算法并分析遗憾界与时间复杂度的权衡


<details>
  <summary>Details</summary>
Motivation: EXP3算法作为经典的多臂赌博机算法，其实际应用受到计算复杂度的限制。虽然理论上有良好的遗憾界保证，但在实际部署中需要更高效的实现方法。

Method: 作者首先指出EXP3算法可以在每轮以常数时间实现，然后提出更实用的算法变体，并系统分析这些算法在遗憾界和时间复杂度之间的权衡关系。

Result: 开发出具有不同时间复杂度的EXP3算法变体，为实际应用提供更灵活的选择，同时保持可证明的遗憾界保证。

Conclusion: 通过优化EXP3算法的实现效率，使其更适合实际应用场景，在计算复杂度和性能保证之间提供平衡的解决方案。

Abstract: We point out that EXP3 can be implemented in constant time per round, propose more practical algorithms, and analyze the trade-offs between the regret bounds and time complexities of these algorithms.

</details>


### [58] [Latent Variable Causal Discovery under Selection Bias](https://arxiv.org/abs/2512.11219)
*Haoyue Dai,Yiwen Qiu,Ignavier Ng,Xinshuai Dong,Peter Spirtes,Kun Zhang*

Main category: cs.LG

TL;DR: 论文提出利用秩约束来处理潜在变量因果发现中的选择偏差问题，证明了即使在选择偏差下，协方差子矩阵的秩仍能保留因果结构和选择机制的信息。


<details>
  <summary>Details</summary>
Motivation: 潜在变量因果发现中的选择偏差问题尚未得到充分研究，主要是因为缺乏合适的统计工具。现有的处理潜在变量的工具大多未针对选择偏差进行适配。

Method: 研究秩约束作为条件独立性约束的推广，在线性高斯模型中利用协方差子矩阵的秩。提供了这种秩约束的图论特征化，并展示了如何用此工具识别选择偏差下的单因子模型。

Result: 证明即使存在选择偏差，有偏协方差矩阵中的秩仍能保留因果结构和选择机制的有意义信息。通过模拟和真实世界实验验证了秩约束的有效性。

Conclusion: 秩约束为处理潜在变量因果发现中的选择偏差问题提供了有效的统计工具，能够识别经典潜在变量模型（如单因子模型）在选择偏差下的结构。

Abstract: Addressing selection bias in latent variable causal discovery is important yet underexplored, largely due to a lack of suitable statistical tools: While various tools beyond basic conditional independencies have been developed to handle latent variables, none have been adapted for selection bias. We make an attempt by studying rank constraints, which, as a generalization to conditional independence constraints, exploits the ranks of covariance submatrices in linear Gaussian models. We show that although selection can significantly complicate the joint distribution, interestingly, the ranks in the biased covariance matrices still preserve meaningful information about both causal structures and selection mechanisms. We provide a graph-theoretic characterization of such rank constraints. Using this tool, we demonstrate that the one-factor model, a classical latent variable model, can be identified under selection bias. Simulations and real-world experiments confirm the effectiveness of using our rank constraints.

</details>


### [59] [Adaptive Soft Rolling KV Freeze with Entropy-Guided Recovery: Sublinear Memory Growth for Efficient LLM Inference](https://arxiv.org/abs/2512.11221)
*Adilet Metinov,Gulida M. Kudakeeva,Bolotbek uulu Nursultan,Gulnara D. Kabaeva*

Main category: cs.LG

TL;DR: 提出ASR-KF-EGR框架，通过可逆软冻结机制在推理时动态管理KV缓存，减少55-67%的活跃KV缓存大小，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在长上下文生成中的内存瓶颈问题，传统方法要么永久丢弃上下文，要么需要大量计算资源，需要一种无需训练、架构无关的推理时优化方案。

Method: 采用可逆软冻结机制，在滑动注意力窗口中识别低重要性token并暂时冻结其KV更新，所有token保存在GPU外存储中可按需恢复。引入亚线性冻结调度，冻结时长随重复检测次数亚线性增长，避免过度压缩。

Result: 在LLaMA-3 8B上的初步实验显示，活跃KV缓存大小减少55-67%，同时保持生成质量，通过needle-in-haystack检索测试。方法无需微调，架构无关。

Conclusion: ASR-KF-EGR为内存受限的长上下文LLM部署提供了实用的训练无关解决方案，通过智能KV缓存管理在保持性能的同时显著减少内存占用。

Abstract: We present Adaptive Soft Rolling KV Freeze with Entropy-Guided Recovery (ASR-KF-EGR), a training-free inference-time framework for efficient large language model generation. Our method introduces a reversible soft-freeze mechanism that temporarily suspends key-value (KV) updates for low-importance tokens identified within a sliding attention window. Unlike eviction-based approaches that permanently discard context, ASR-KF-EGR preserves all tokens in off-GPU storage and restores them on demand. We extend the framework with sublinear freeze scheduling, where freeze duration grows sublinearly with repeated low-importance detections, preventing over-aggressive compression. Preliminary experiments on LLaMA-3 8B demonstrate 55-67% reduction in active KV cache size while maintaining generation quality and passing needle-in-haystack retrieval tests. The method is architecture-agnostic, requires no fine-tuning, and provides a practical solution for memory-constrained deployment of long-context LLMs.

</details>


### [60] [Task-Aware Multi-Expert Architecture For Lifelong Deep Learning](https://arxiv.org/abs/2512.11243)
*Jianyu Wang,Jacob Nean-Hua Sheikh,Cat P. Le,Hoda Bidkhori*

Main category: cs.LG

TL;DR: TAME是一种终身学习算法，通过任务感知的多专家选择和知识转移来平衡新任务适应与旧任务保留。


<details>
  <summary>Details</summary>
Motivation: 终身深度学习需要神经网络在顺序学习多个任务时保持先前知识，同时有效适应新任务，避免灾难性遗忘。

Method: TAME维护预训练神经网络池，根据任务相似性选择最相关专家；使用共享密集层集成特征；采用重放缓冲区存储先前任务的样本和嵌入；注意力机制优先考虑最相关的存储信息。

Result: 在CIFAR-100衍生的二分类任务上，TAME提高了新任务的准确性，同时保持了先前任务的性能，有效平衡了适应性和保留性。

Conclusion: TAME通过任务感知的专家选择和知识转移机制，在终身学习场景中有效平衡了适应新任务和保留旧知识的需求。

Abstract: Lifelong deep learning (LDL) trains neural networks to learn sequentially across tasks while preserving prior knowledge. We propose Task-Aware Multi-Expert (TAME), a continual learning algorithm that leverages task similarity to guide expert selection and knowledge transfer. TAME maintains a pool of pretrained neural networks and activates the most relevant expert for each new task. A shared dense layer integrates features from the chosen expert to generate predictions. To reduce catastrophic forgetting, TAME uses a replay buffer that stores representative samples and embeddings from previous tasks and reuses them during training. An attention mechanism further prioritizes the most relevant stored information for each prediction. Together, these components allow TAME to adapt flexibly while retaining important knowledge across evolving task sequences. Experiments on binary classification tasks derived from CIFAR-100 show that TAME improves accuracy on new tasks while sustaining performance on earlier ones, highlighting its effectiveness in balancing adaptation and retention in lifelong learning settings.

</details>


### [61] [Insight Miner: A Time Series Analysis Dataset for Cross-Domain Alignment with Natural Language](https://arxiv.org/abs/2512.11251)
*Yunkai Zhang,Yawen Zhang,Ming Zheng,Kezhen Chen,Chongyang Gao,Ruian Ge,Siyuan Teng,Amine Jelloul,Jinmeng Rao,Xiaoyuan Guo,Chiang-Wei Fang,Zeyu Zheng,Jie Yang*

Main category: cs.LG

TL;DR: Insight Miner是一个用于生成高质量时间序列描述的多模态大模型，通过新构建的TS-Insights数据集进行指令调优，在时间序列分析任务上超越了现有SOTA模型。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据在多个领域都很重要，但从中挖掘洞察通常需要深厚的领域专业知识，这个过程既耗时又费力。需要一种能够自动生成高质量时间序列描述和洞察的方法。

Method: 提出了Insight Miner多模态大模型，并构建了TS-Insights数据集（包含100k时间序列窗口）。使用新颖的agentic workflow：先用统计工具从原始时间序列提取特征，然后用GPT-4将这些特征合成为连贯的趋势描述。在TS-Insights上进行指令调优。

Result: Insight Miner在生成时间序列描述和洞察方面超越了最先进的多模态模型，如LLaVA和GPT-4。这表明了利用LMM进行时间序列分析的有前景方向。

Conclusion: 这项工作为将LLM应用于时间序列分析奠定了基础，是使LLM能够将时间序列作为原生输入模态的重要一步。提出的方法展示了在多模态时间序列分析方面的潜力。

Abstract: Time-series data is critical across many scientific and industrial domains, including environmental analysis, agriculture, transportation, and finance. However, mining insights from this data typically requires deep domain expertise, a process that is both time-consuming and labor-intensive. In this paper, we propose \textbf{Insight Miner}, a large-scale multimodal model (LMM) designed to generate high-quality, comprehensive time-series descriptions enriched with domain-specific knowledge. To facilitate this, we introduce \textbf{TS-Insights}\footnote{Available at \href{https://huggingface.co/datasets/zhykoties/time-series-language-alignment}{https://huggingface.co/datasets/zhykoties/time-series-language-alignment}.}, the first general-domain dataset for time series and language alignment. TS-Insights contains 100k time-series windows sampled from 20 forecasting datasets. We construct this dataset using a novel \textbf{agentic workflow}, where we use statistical tools to extract features from raw time series before synthesizing them into coherent trend descriptions with GPT-4. Following instruction tuning on TS-Insights, Insight Miner outperforms state-of-the-art multimodal models, such as LLaVA \citep{liu2023llava} and GPT-4, in generating time-series descriptions and insights. Our findings suggest a promising direction for leveraging LMMs in time series analysis, and serve as a foundational step toward enabling LLMs to interpret time series as a native input modality.

</details>


### [62] [A Simple Generalisation of the Implicit Dynamics of In-Context Learning](https://arxiv.org/abs/2512.11255)
*Francesco Innocenti,El Mehdi Achour*

Main category: cs.LG

TL;DR: 该论文扩展了Dherin等人(2025)关于Transformer中上下文学习机制的理论，将其推广到所有序列位置、任意Transformer块以及包含层归一化的更真实残差块，并通过实验验证了理论。


<details>
  <summary>Details</summary>
Motivation: 先前关于上下文学习(ICL)的理论主要基于简化模型和数据设置，而Dherin等人最近发现Transformer块可以看作隐式更新其前馈网络权重。本研究旨在将这一理论扩展到更实际、更一般的情况。

Method: 提出对Dherin等人理论的简单推广，涵盖：(1)所有序列位置而不仅仅是最后一个位置；(2)任意Transformer块而不仅仅是第一个块；(3)包含层归一化的更真实残差块。在简单的上下文线性回归任务上进行实证验证，并研究不同token之间和不同块之间隐式更新的关系。

Result: 成功将Dherin等人的理论扩展到更一般的情况，实证验证了理论的有效性，并分析了隐式更新的内部关系。这些结果使该理论更接近实际应用。

Conclusion: 本研究将上下文学习的理论理解推向更实际的方向，为在大规模模型上的验证奠定了基础，有助于更好地理解Transformer如何通过上下文示例学习新任务。

Abstract: In-context learning (ICL) refers to the ability of a model to learn new tasks from examples in its input without any parameter updates. In contrast to previous theories of ICL relying on toy models and data settings, recently it has been shown that an abstraction of a transformer block can be seen as implicitly updating the weights of its feedforward network according to the context (Dherin et al., 2025). Here, we provide a simple generalisation of this result for (i) all sequence positions beyond the last, (ii) any transformer block beyond the first, and (iii) more realistic residual blocks including layer normalisation. We empirically verify our theory on simple in-context linear regression tasks and investigate the relationship between the implicit updates related to different tokens within and between blocks. These results help to bring the theory of Dherin et al. (2025) even closer to practice, with potential for validation on large-scale models.

</details>


### [63] [Features Emerge as Discrete States: The First Application of SAEs to 3D Representations](https://arxiv.org/abs/2512.11263)
*Albert Miao,Chenliang Zhou,Jiawei Zhou,Cengiz Oztireli*

Main category: cs.LG

TL;DR: 首次将稀疏自编码器应用于3D领域，分析3D重建VAE的特征，发现模型编码离散特征而非连续特征，形成类似相变的状态转换框架。


<details>
  <summary>Details</summary>
Motivation: 稀疏自编码器在文本领域已被证明是强大的字典学习技术，能够将隐藏状态分解为高语义价值的人类可理解概念，但该方法很少应用于3D领域，限制了特征分解的理论探索。

Method: 将稀疏自编码器首次应用于3D领域，分析应用于Objaverse数据集中53k个3D模型的最先进3D重建VAE的特征编码机制。

Result: 发现网络编码的是离散特征而非连续特征，模型近似于离散状态空间，由特征激活驱动的相变式转换。通过状态转换框架解释了三个反直觉行为：重建模型对位置编码表示的偏好、特征消融中重建损失的S型行为、相变点分布的双峰性。

Conclusion: 该工作不仅编译和解释了特征分解中的意外现象，还提供了解释模型特征学习动态的框架。双峰性分布表明模型重新分配叠加干扰以优先考虑不同特征的显著性。

Abstract: Sparse Autoencoders (SAEs) are a powerful dictionary learning technique for decomposing neural network activations, translating the hidden state into human ideas with high semantic value despite no external intervention or guidance. However, this technique has rarely been applied outside of the textual domain, limiting theoretical explorations of feature decomposition. We present the \textbf{first application of SAEs to the 3D domain}, analyzing the features used by a state-of-the-art 3D reconstruction VAE applied to 53k 3D models from the Objaverse dataset. We observe that the network encodes discrete rather than continuous features, leading to our key finding: \textbf{such models approximate a discrete state space, driven by phase-like transitions from feature activations}. Through this state transition framework, we address three otherwise unintuitive behaviors -- the inclination of the reconstruction model towards positional encoding representations, the sigmoidal behavior of reconstruction loss from feature ablation, and the bimodality in the distribution of phase transition points. This final observation suggests the model \textbf{redistributes the interference caused by superposition to prioritize the saliency of different features}. Our work not only compiles and explains unexpected phenomena regarding feature decomposition, but also provides a framework to explain the model's feature learning dynamics. The code and dataset of encoded 3D objects will be available on release.

</details>


### [64] [SRLR: Symbolic Regression based Logic Recovery to Counter Programmable Logic Controller Attacks](https://arxiv.org/abs/2512.11298)
*Hao Zhou,Suman Sourav,Binbin Chen,Ke Yu*

Main category: cs.LG

TL;DR: SRLR是一种基于符号回归的PLC逻辑恢复方案，仅通过输入输出数据识别PLC控制逻辑，生成可解释规则来检测控制器逻辑攻击，在ICS环境中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: PLC作为工业控制系统关键组件易受网络攻击，现有检测方法存在局限：基于规范的方法需要专家手动工作或访问PLC源代码，而基于机器学习的方法缺乏决策解释性。

Method: SRLR利用ICS特定属性增强深度符号回归方法：1) 在频域而非时域表示重要控制逻辑；2) 处理多模式操作且模式切换不频繁；3) 过滤异常输入以处理传感器噪声；4) 降低公式复杂度实现有效搜索。

Result: SRLR在各种ICS设置中始终优于现有方法，恢复准确率在某些挑战性环境中可提高39%。在包含数百个电压调节器的配电网上测试，证明其能稳定处理大规模复杂系统。

Conclusion: SRLR通过结合ICS特定属性增强符号回归，能够仅从输入输出数据恢复PLC逻辑并生成可解释的攻击检测规则，为工业控制系统安全提供了有效的解决方案。

Abstract: Programmable Logic Controllers (PLCs) are critical components in Industrial Control Systems (ICSs). Their potential exposure to external world makes them susceptible to cyber-attacks. Existing detection methods against controller logic attacks use either specification-based or learnt models. However, specification-based models require experts' manual efforts or access to PLC's source code, while machine learning-based models often fall short of providing explanation for their decisions. We design SRLR -- a it Symbolic Regression based Logic Recovery} solution to identify the logic of a PLC based only on its inputs and outputs. The recovered logic is used to generate explainable rules for detecting controller logic attacks. SRLR enhances the latest deep symbolic regression methods using the following ICS-specific properties: (1) some important ICS control logic is best represented in frequency domain rather than time domain; (2) an ICS controller can operate in multiple modes, each using different logic, where mode switches usually do not happen frequently; (3) a robust controller usually filters out outlier inputs as ICS sensor data can be noisy; and (4) with the above factors captured, the degree of complexity of the formulas is reduced, making effective search possible. Thanks to these enhancements, SRLR consistently outperforms all existing methods in a variety of ICS settings that we evaluate. In terms of the recovery accuracy, SRLR's gain can be as high as 39% in some challenging environment. We also evaluate SRLR on a distribution grid containing hundreds of voltage regulators, demonstrating its stability in handling large-scale, complex systems with varied configurations.

</details>


### [65] [QGEC : Quantum Golay Code Error Correction](https://arxiv.org/abs/2512.11307)
*Hideo Mukai,Hoshitaro Ohnishi*

Main category: cs.LG

TL;DR: 提出使用经典Golay码的量子纠错方法QGEC，通过Transformer解码器在量子噪声环境下实现高效纠错，相比环面码在更少数据量子位下获得更高解码精度。


<details>
  <summary>Details</summary>
Motivation: 量子计算机在特定问题上相比经典计算机有计算负载优势，但量子比特易受外部噪声影响。量子纠错(QEC)对处理量子比特至关重要，需要从稳定子生成器的综合征测量结果预测实际错误，而非直接测量数据量子比特。

Method: 提出量子Golay码纠错(QGEC)方法，使用经典信息论中的高效编码方法Golay码。采用Transformer作为解码器，在由生成多项式定义的码空间中评估解码器精度，使用三种不同权重集和三种不同比特翻转错误与相位翻转错误相关性的噪声模型。在离散均匀分布的噪声模型下，比较了在相同架构下分别使用Golay码和环面码训练的Transformer解码器的性能。

Result: 较小相关性的噪声模型给出更好的精度，而生成多项式的权重对解码器精度影响很小。Golay码（需要23个数据量子位，码距为7）比环面码（需要50个数据量子位，码距为5）实现了更高的解码精度。

Conclusion: 使用Transformer实现量子纠错可能使Golay码能够更高效地实现容错量子计算。Golay码在更少量子位资源下表现出优于环面码的纠错性能，为量子纠错提供了有前景的方向。

Abstract: Quantum computers have the possibility of a much reduced calculation load compared with classical computers in specific problems. Quantum error correction (QEC) is vital for handling qubits, which are vulnerable to external noise. In QEC, actual errors are predicted from the results of syndrome measurements by stabilizer generators, in place of making direct measurements of the data qubits. Here, we propose Quantum Golay code Error Correction (QGEC), a QEC method using Golay code, which is an efficient coding method in classical information theory. We investigated our method's ability in decoding calculations with the Transformer. We evaluated the accuracy of the decoder in a code space defined by the generative polynomials with three different weights sets and three noise models with different correlations of bit-flip error and phase-flip error. Furthermore, under a noise model following a discrete uniform distribution, we compared the decoding performance of Transformer decoders with identical architectures trained respectively on Golay and toric codes. The results showed that the noise model with the smaller correlation gave better accuracy, while the weights of the generative polynomials had little effect on the accuracy of the decoder. In addition, they showed that Golay code requiring 23 data qubits and having a code distance of 7 achieved higher decoding accuracy than toric code which requiring 50 data qubits and having a code distance of 5. This suggests that implementing quantum error correction using a Transformer may enable the Golay code to realize fault-tolerant quantum computation more efficiently.

</details>


### [66] [Benchmarking the Generality of Vision-Language-Action Models](https://arxiv.org/abs/2512.11315)
*Pranav Guruprasad,Sudipta Chowdhury,Harsh Sikka,Mridul Sharma,Helen Lu,Sean Rivera,Aryan Khurana,Hangliang Ren,Yangyue Wang*

Main category: cs.LG

TL;DR: MultiNet v1.0是一个统一基准测试，用于评估视觉语言模型和视觉语言动作模型在六个核心能力领域的跨域泛化能力，发现当前模型在未见领域存在显著性能下降。


<details>
  <summary>Details</summary>
Motivation: 当前多模态智能体评估分散在孤立基准中，难以判断基础模型是否真正超越了训练分布而泛化。需要统一基准来测量模型在跨域环境中的实际泛化能力。

Method: 引入MultiNet v1.0基准测试，涵盖六个核心能力领域：视觉基础、空间推理、工具使用、物理常识、多智能体协调和连续机器人控制。评估GPT-5、Pi0和Magma等模型。

Result: 所有模型都未展示一致的泛化能力，在未见领域、不熟悉模态或跨域任务转移中表现出显著性能下降。失败表现为模态错位、输出格式不稳定和领域转移下的灾难性知识退化。

Conclusion: 当前基础模型的实际能力与通用智能的期望之间存在持续差距。MultiNet v1.0为诊断这些差距和指导未来通用智能体开发提供了标准化评估框架。

Abstract: Generalist multimodal agents are expected to unify perception, language, and control - operating robustly across diverse real world domains. However, current evaluation practices remain fragmented across isolated benchmarks, making it difficult to assess whether today's foundation models truly generalize beyond their training distributions. We introduce MultiNet v1.0, a unified benchmark for measuring the cross domain generality of vision language models (VLMs) and vision language action models (VLAs) across six foundational capability regimes. Visual grounding, spatial reasoning, tool use, physical commonsense, multi agent coordination, and continuous robot control. Evaluating GPT 5, Pi0, and Magma, we find that no model demonstrates consistent generality. All exhibit substantial degradation on unseen domains, unfamiliar modalities, or cross domain task shifts despite strong performance within their training distributions.These failures manifest as modality misalignment, output format instability, and catastrophic knowledge degradation under domain transfer.Our findings reveal a persistent gap between the aspiration of generalist intelligence and the actual capabilities of current foundation models.MultiNet v1.0 provides a standardized evaluation substrate for diagnosing these gaps and guiding the development of future generalist agents.Code, data, and leaderboards are publicly available.

</details>


### [67] [Condensation-Concatenation Framework for Dynamic Graph Continual Learning](https://arxiv.org/abs/2512.11317)
*Tingxu Yan,Ye Yuan*

Main category: cs.LG

TL;DR: 提出CCC框架解决动态图中因结构变化导致的灾难性遗忘问题，通过压缩历史图快照为语义表示并选择性拼接，改进遗忘度量以适应动态图场景。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的动态图持续结构变化会导致图神经网络出现灾难性遗忘。现有方法忽视了拓扑变化对现有节点的影响，需要解决这一问题。

Method: 提出CCC框架：1) 将历史图快照压缩为紧凑的语义表示，保持原始标签分布和拓扑特性；2) 选择性拼接历史嵌入与当前图表示；3) 改进遗忘度量(FM)以量化结构更新对现有节点预测性能的影响。

Result: 在四个真实世界数据集上的广泛实验中，CCC框架相比最先进的基线方法表现出优越性能。

Conclusion: CCC框架有效解决了动态图中因结构变化导致的灾难性遗忘问题，通过压缩-拼接策略和改进的遗忘度量，在动态图持续学习任务中取得了显著效果。

Abstract: Dynamic graphs are prevalent in real-world scenarios, where continuous structural changes induce catastrophic forgetting in graph neural networks (GNNs). While continual learning has been extended to dynamic graphs, existing methods overlook the effects of topological changes on existing nodes. To address it, we propose a novel framework for continual learning on dynamic graphs, named Condensation-Concatenation-based Continual Learning (CCC). Specifically, CCC first condenses historical graph snapshots into compact semantic representations while aiming to preserve the original label distribution and topological properties. Then it concatenates these historical embeddings with current graph representations selectively. Moreover, we refine the forgetting measure (FM) to better adapt to dynamic graph scenarios by quantifying the predictive performance degradation of existing nodes caused by structural updates. CCC demonstrates superior performance over state-of-the-art baselines across four real-world datasets in extensive experiments.

</details>


### [68] [TV2TV: A Unified Framework for Interleaved Language and Video Generation](https://arxiv.org/abs/2512.05103)
*Xiaochuang Han,Youssef Emad,Melissa Hall,John Nguyen,Karthik Padthe,Liam Robbins,Amir Bar,Delong Chen,Michal Drozdzal,Maha Elbayad,Yushi Hu,Shang-Wen Li,Sreya Dutta Roy,Jakob Verbeek,XuDong Wang,Marjan Ghazvininejad,Luke Zettlemoyer,Emily Dinan*

Main category: cs.LG

TL;DR: TV2TV是一个将视频生成分解为交替文本和视频生成过程的统一框架，通过语言建模和视频流匹配的混合架构，让模型在生成像素前"用文字思考"，提升视频质量和可控性。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成模型在处理需要复杂语义分支或高层推理的视频内容时仍存在困难，需要一种能够进行开放式文本推理和控制的视频生成方法。

Method: 提出TV2TV统一生成框架，使用混合变换器架构联合学习语言建模和视频流匹配，在推理时交替生成文本和视频帧，让语言模型塔决定后续内容，然后生成对应视频帧。

Result: 在视频游戏数据上的实验显示TV2TV在视觉质量和可控性方面有显著提升；在自然视频（体育视频）上也表现出强大的视觉质量和提示对齐能力，能够推理和生成复杂的现实世界动作序列。

Conclusion: TV2TV代表了向具有开放式文本推理和控制的视频生成迈出的有希望的一步，通过"用文字思考，用像素行动"的交替生成范式，实现了更好的视频质量和细粒度可控性。

Abstract: Video generation models are rapidly advancing, but can still struggle with complex video outputs that require significant semantic branching or repeated high-level reasoning about what should happen next. In this paper, we introduce a new class of omni video-text models that integrate ideas from recent LM reasoning advances to address this challenge. More specifically, we present TV2TV, a unified generative modeling framework which decomposes video generation into an interleaved text and video generation process. TV2TV jointly learns language modeling (next-token prediction) and video flow matching (next-frame prediction) using a Mixture-of-Transformers (MoT) architecture. At inference time, TV2TV decides when to alternate between generating text and video frames, allowing the model to "think in words" about subsequent content before ``acting in pixels'' to produce frames. This design offloads much of the responsibility for deciding what should happen next to the language modeling tower, enabling improved visual quality and prompt alignment of generated videos. It also enables fine-grained controllability, allowing users to modify the video generation trajectory through text interventions at any point in the process. In controlled experiments on video game data, TV2TV demonstrates substantial improvements in both visual quality and controllability. TV2TV also scales to natural videos, as we show by augmenting sports videos with interleaved natural language action descriptions using vision-language models (VLMs). Training TV2TV on this corpus yields strong visual quality and prompt alignment, showcasing the model's ability to reason about and generate complex real-world action sequences. Together, these results highlight TV2TV as a promising step toward video generation with open-ended textual reasoning and control.

</details>


### [69] [Pace: Physics-Aware Attentive Temporal Convolutional Network for Battery Health Estimation](https://arxiv.org/abs/2512.11332)
*Sara Sameer,Wei Zhang,Kannan Dhivya Dharshini,Xin Lou,Yulin Gao,Terence Goh,Qingyu Yan*

Main category: cs.LG

TL;DR: Pace是一个用于电池健康估计的物理感知注意力时序卷积网络，通过融合原始传感器数据和电池物理特征，在公共数据集上比现有模型平均性能提升6.5%，并在树莓派上实现实时边缘部署。


<details>
  <summary>Details</summary>
Motivation: 电池是现代能源系统（如电动汽车和电网储能）的关键组件，有效的电池健康管理对系统安全、成本效益和可持续性至关重要。需要开发能够准确估计电池健康状态的模型。

Method: 提出Pace模型，将原始传感器测量数据与等效电路模型导出的电池物理特征相结合。开发了三个电池专用模块：用于高效时序编码的扩张时序块、用于上下文建模的分块注意力块，以及用于融合短期和长期电池退化模式的双头输出块。

Result: 在大型公共数据集上，Pace性能远超现有模型，相比两个最佳基线模型平均性能提升6.5%和2.0倍。进一步在树莓派上实现了实时边缘部署，证明了其实用可行性。

Conclusion: Pace为电池健康分析提供了一个实用且高性能的解决方案，能够准确高效地预测各种电池使用条件下的电池健康状况。

Abstract: Batteries are critical components in modern energy systems such as electric vehicles and power grid energy storage. Effective battery health management is essential for battery system safety, cost-efficiency, and sustainability. In this paper, we propose Pace, a physics-aware attentive temporal convolutional network for battery health estimation. Pace integrates raw sensor measurements with battery physics features derived from the equivalent circuit model. We develop three battery-specific modules, including dilated temporal blocks for efficient temporal encoding, chunked attention blocks for context modeling, and a dual-head output block for fusing short- and long-term battery degradation patterns. Together, the modules enable Pace to predict battery health accurately and efficiently in various battery usage conditions. In a large public dataset, Pace performs much better than existing models, achieving an average performance improvement of 6.5 and 2.0x compared to two best-performing baseline models. We further demonstrate its practical viability with a real-time edge deployment on a Raspberry Pi. These results establish Pace as a practical and high-performance solution for battery health analytics.

</details>


### [70] [Spectral entropy prior-guided deep feature fusion architecture for magnetic core loss](https://arxiv.org/abs/2512.11334)
*Cong Yao,Chunye Gong,Jin Zhang*

Main category: cs.LG

TL;DR: 本文提出SEPI-TFPNet混合模型，结合经验模型与深度学习，通过物理先验子模块选择合适经验模型，数据驱动子模块提取磁通密度时序特征，自适应特征融合模块提升多模态特征交互，在MagNet数据集上相比现有方法获得更高精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统磁芯损耗建模方法预测精度有限，纯数据驱动模型虽然拟合性能强，但可解释性和跨分布泛化能力不足。IEEE电力电子学会发起的MagNet挑战赛旨在通过数据驱动范式揭示磁性元件的复杂损耗模式，但现有方法仍有改进空间。

Method: 提出SEPI-TFPNet混合模型：1)物理先验子模块采用谱熵判别机制，根据不同激励波形选择最合适的经验模型；2)数据驱动子模块结合卷积神经网络、多头注意力机制和双向LSTM网络提取磁通密度时序特征；3)引入自适应特征融合模块改善多模态特征交互与整合。

Result: 在包含多种磁性材料的MagNet数据集上评估，与2023年挑战赛的21个代表性模型以及2024-2025年的三种先进方法进行比较，结果表明所提方法在建模精度和鲁棒性方面均有提升。

Conclusion: SEPI-TFPNet混合模型通过结合经验模型与深度学习，有效解决了纯数据驱动模型在可解释性和泛化能力方面的局限性，为磁芯损耗建模提供了更准确和鲁棒的解决方案。

Abstract: Accurate core loss modeling is critical for the design of high-efficiency power electronic systems. Traditional core loss modeling methods have limitations in prediction accuracy. To advance this field, the IEEE Power Electronics Society launched the MagNet Challenge in 2023, the first international competition focused on data-driven power electronics design methods, aiming to uncover complex loss patterns in magnetic components through a data-driven paradigm. Although purely data-driven models demonstrate strong fitting performance, their interpretability and cross-distribution generalization capabilities remain limited. To address these issues, this paper proposes a hybrid model, SEPI-TFPNet, which integrates empirical models with deep learning. The physical-prior submodule employs a spectral entropy discrimination mechanism to select the most suitable empirical model under different excitation waveforms. The data-driven submodule incorporates convolutional neural networks, multi-head attention mechanisms, and bidirectional long short-term memory networks to extract flux-density time-series features. An adaptive feature fusion module is introduced to improve multimodal feature interaction and integration. Using the MagNet dataset containing various magnetic materials, this paper evaluates the proposed method and compares it with 21 representative models from the 2023 challenge and three advanced methods from 2024-2025. The results show that the proposed method achieves improved modeling accuracy and robustness.

</details>


### [71] [DAPO: Design Structure-Aware Pass Ordering in High-Level Synthesis with Graph Contrastive and Reinforcement Learning](https://arxiv.org/abs/2512.11342)
*Jinming Ge,Linfeng Du,Likith Anaparty,Shangkun Li,Tingyuan Liang,Afzal Ahmad,Vivek Chaturvedi,Sharad Sinha,Zhiyao Xie,Jiang Xu,Wei Zhang*

Main category: cs.LG

TL;DR: DAPO是一个基于设计结构感知的HLS优化框架，通过强化学习自动发现特定设计的优化策略，相比Vitis HLS平均获得2.36倍加速


<details>
  <summary>Details</summary>
Motivation: 现有HLS工具采用固定的优化策略，这些策略继承自软件编译器，无法针对特定设计进行优化，限制了其效果。需要深度语义理解、准确的硬件指标估计和高级搜索算法，而当前方法缺乏这些能力。

Method: DAPO框架从控制流和数据流图中提取程序语义，使用对比学习生成丰富的嵌入表示，利用分析模型进行准确的硬件指标估计，这些组件共同指导强化学习智能体发现特定设计的优化策略。

Result: 在经典HLS设计上的评估表明，DAPO端到端流程相比Vitis HLS平均实现了2.36倍的加速。

Conclusion: DAPO通过设计结构感知的优化框架，能够自动发现针对特定设计的优化策略，显著提升了HLS工具的性能，为FPGA加速器设计提供了更有效的优化方法。

Abstract: High-Level Synthesis (HLS) tools are widely adopted in FPGA-based domain-specific accelerator design. However, existing tools rely on fixed optimization strategies inherited from software compilations, limiting their effectiveness. Tailoring optimization strategies to specific designs requires deep semantic understanding, accurate hardware metric estimation, and advanced search algorithms -- capabilities that current approaches lack.
  We propose DAPO, a design structure-aware pass ordering framework that extracts program semantics from control and data flow graphs, employs contrastive learning to generate rich embeddings, and leverages an analytical model for accurate hardware metric estimation. These components jointly guide a reinforcement learning agent to discover design-specific optimization strategies. Evaluations on classic HLS designs demonstrate that our end-to-end flow delivers a 2.36 speedup over Vitis HLS on average.

</details>


### [72] [Rethinking Expert Trajectory Utilization in LLM Post-training](https://arxiv.org/abs/2512.11470)
*Bowen Ding,Yuhan Chen,Jiayang Lv,Jiyao Yuan,Qi Zhu,Shuangshuang Tian,Dantong Zhu,Futing Wang,Heyuan Deng,Fei Mi,Lifeng Shang,Tao Lin*

Main category: cs.LG

TL;DR: 提出了Plasticity-Ceiling框架，证明SFT-then-RL顺序训练优于同步方法，并提供了专家轨迹利用的精确扩展指南


<details>
  <summary>Details</summary>
Motivation: 虽然SFT和RL结合的有效后训练方法已经存在，但如何最优利用专家轨迹的问题仍未解决。需要理论框架来指导这一领域的发展。

Method: 提出Plasticity-Ceiling框架，将性能分解为基础SFT性能和后续RL可塑性。通过广泛基准测试，建立Sequential SFT-then-RL流程作为标准。

Result: 1) 在SFT稳定或轻微过拟合阶段转向RL能最大化最终性能上限；2) 数据规模决定主要后训练潜力，轨迹难度作为性能乘数；3) 最小SFT验证损失是选择专家轨迹的可靠指标

Conclusion: 研究提供了最大化专家轨迹价值的可操作指南，建立了SFT-then-RL作为后训练的标准流程，并提供了具体的扩展和选择策略。

Abstract: While effective post-training integrates Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), the optimal mechanism for utilizing expert trajectories remains unresolved. We propose the Plasticity-Ceiling Framework to theoretically ground this landscape, decomposing performance into foundational SFT performance and the subsequent RL plasticity. Through extensive benchmarking, we establish the Sequential SFT-then-RL pipeline as the superior standard, overcoming the stability deficits of synchronized approaches. Furthermore, we derive precise scaling guidelines: (1) Transitioning to RL at the SFT Stable or Mild Overfitting Sub-phase maximizes the final ceiling by securing foundational SFT performance without compromising RL plasticity; (2) Refuting ``Less is More'' in the context of SFT-then-RL scaling, we demonstrate that Data Scale determines the primary post-training potential, while Trajectory Difficulty acts as a performance multiplier; and (3) Identifying that the Minimum SFT Validation Loss serves as a robust indicator for selecting the expert trajectories that maximize the final performance ceiling. Our findings provide actionable guidelines for maximizing the value extracted from expert trajectories.

</details>


### [73] [Symmetry-Aware Steering of Equivariant Diffusion Policies: Benefits and Limits](https://arxiv.org/abs/2512.11345)
*Minwoo Park,Junwoo Chang,Jongeun Choi,Roberto Horowitz*

Main category: cs.LG

TL;DR: 论文提出了一种基于对称性的扩散策略强化学习引导框架，通过理论证明扩散过程的等变性，构建了适用于等变扩散引导的群不变潜在噪声MDP，显著提升了样本效率和策略改进效果。


<details>
  <summary>Details</summary>
Motivation: 等变扩散策略结合了扩散模型的生成能力和几何对称性的泛化优势，但现有的强化学习引导方法忽略了对称性，导致样本效率低且不稳定。需要开发能够利用对称性的引导框架来提升性能。

Method: 1. 理论证明等变扩散策略的扩散过程具有等变性；2. 构建群不变潜在噪声MDP；3. 提出基于对称性的引导框架，比较标准、等变和近似等变强化学习策略；4. 在不同对称性程度的任务上进行全面实验。

Result: 在对称性引导过程中利用对称性带来了显著好处：提升样本效率、防止价值发散、即使在极有限演示数据训练的情况下也能实现强大的策略改进。同时识别了严格等变性在对称性破坏情况下的实际边界。

Conclusion: 对称性感知的强化学习引导框架能够有效利用等变扩散策略的对称性特性，显著提升引导过程的效率和稳定性，为有限演示数据下的策略优化提供了有效解决方案。

Abstract: Equivariant diffusion policies (EDPs) combine the generative expressivity of diffusion models with the strong generalization and sample efficiency afforded by geometric symmetries. While steering these policies with reinforcement learning (RL) offers a promising mechanism for fine-tuning beyond demonstration data, directly applying standard (non-equivariant) RL can be sample-inefficient and unstable, as it ignores the symmetries that EDPs are designed to exploit. In this paper, we theoretically establish that the diffusion process of an EDP is equivariant, which in turn induces a group-invariant latent-noise MDP that is well-suited for equivariant diffusion steering. Building on this theory, we introduce a principled symmetry-aware steering framework and compare standard, equivariant, and approximately equivariant RL strategies through comprehensive experiments across tasks with varying degrees of symmetry. While we identify the practical boundaries of strict equivariance under symmetry breaking, we show that exploiting symmetry during the steering process yields substantial benefits-enhancing sample efficiency, preventing value divergence, and achieving strong policy improvements even when EDPs are trained from extremely limited demonstrations.

</details>


### [74] [CAT: Can Trust be Predicted with Context-Awareness in Dynamic Heterogeneous Networks?](https://arxiv.org/abs/2512.11352)
*Jie Wang,Zheng Yan,Jiahe Lan,Xuyan Li,Elisa Bertino*

Main category: cs.LG

TL;DR: CAT：首个支持信任动态性和真实世界异质性的上下文感知GNN信任预测模型，通过连续时间表示、双重注意力机制和元路径实现精准预测。


<details>
  <summary>Details</summary>
Motivation: 现有GNN信任预测模型存在三大局限：1) 无法捕捉信任动态性，导致推理不可靠；2) 忽略真实网络的异质性，丢失丰富语义信息；3) 不支持上下文感知这一信任的基本属性，预测结果粗糙。

Method: CAT模型包含四层：图构建层、嵌入层、异质注意力层和预测层。采用连续时间表示处理动态图，通过时间编码函数捕捉时序信息。使用双重注意力机制建模图异质性，识别不同节点类型及类型内节点的重要性。引入元路径概念提取上下文特征，构建上下文嵌入并集成上下文感知聚合器。

Result: 在三个真实世界数据集上的实验表明，CAT在信任预测任务上优于五组基线方法，同时展现出对大规模图的可扩展性，以及对信任导向和GNN导向攻击的鲁棒性。

Conclusion: CAT是首个支持信任动态性、准确表示真实世界异质性并具备上下文感知能力的GNN信任预测模型，能够同时预测上下文感知信任和整体信任，在多个方面优于现有方法。

Abstract: Trust prediction provides valuable support for decision-making, risk mitigation, and system security enhancement. Recently, Graph Neural Networks (GNNs) have emerged as a promising approach for trust prediction, owing to their ability to learn expressive node representations that capture intricate trust relationships within a network. However, current GNN-based trust prediction models face several limitations: (i) Most of them fail to capture trust dynamicity, leading to questionable inferences. (ii) They rarely consider the heterogeneous nature of real-world networks, resulting in a loss of rich semantics. (iii) None of them support context-awareness, a basic property of trust, making prediction results coarse-grained.
  To this end, we propose CAT, the first Context-Aware GNN-based Trust prediction model that supports trust dynamicity and accurately represents real-world heterogeneity. CAT consists of a graph construction layer, an embedding layer, a heterogeneous attention layer, and a prediction layer. It handles dynamic graphs using continuous-time representations and captures temporal information through a time encoding function. To model graph heterogeneity and leverage semantic information, CAT employs a dual attention mechanism that identifies the importance of different node types and nodes within each type. For context-awareness, we introduce a new notion of meta-paths to extract contextual features. By constructing context embeddings and integrating a context-aware aggregator, CAT can predict both context-aware trust and overall trust. Extensive experiments on three real-world datasets demonstrate that CAT outperforms five groups of baselines in trust prediction, while exhibiting strong scalability to large-scale graphs and robustness against both trust-oriented and GNN-oriented attacks.

</details>


### [75] [Attacking and Securing Community Detection: A Game-Theoretic Framework](https://arxiv.org/abs/2512.11359)
*Yifan Niu,Aochuan Chen,Tingyang Xu,Jia Li*

Main category: cs.LG

TL;DR: 该论文将对抗图概念扩展到社区检测问题，提出攻击和防御技术，并建立CD-GAME博弈论框架模拟攻防交互，最终达到纳什均衡。


<details>
  <summary>Details</summary>
Motivation: 现有对抗图研究主要关注分类任务，但社区检测问题更具挑战性。需要开发针对社区检测的攻击和防御技术，以保护社交网络中的个人隐私和理解交易网络中的伪装模式。

Method: 提出针对社区检测的新型攻击和防御技术，并建立CD-GAME博弈论框架，其中攻击方为图攻击者，防御方为Rayleigh Quotient防御者，模拟攻防交互动态演化过程。

Result: 实验表明提出的攻击和防御方法显著优于现有基线。CD-GAME框架揭示了在纳什均衡下，攻击者会采用更隐蔽但仍有效的策略，而传统单步攻击策略容易被检测和对抗。

Conclusion: 该工作成功将对抗图概念扩展到社区检测领域，提出的CD-GAME框架为理解社区检测中的交互攻防场景提供了宝贵见解，有助于开发更鲁棒的社区检测模型。

Abstract: It has been demonstrated that adversarial graphs, i.e., graphs with imperceptible perturbations, can cause deep graph models to fail on classification tasks. In this work, we extend the concept of adversarial graphs to the community detection problem, which is more challenging. We propose novel attack and defense techniques for community detection problem, with the objective of hiding targeted individuals from detection models and enhancing the robustness of community detection models, respectively. These techniques have many applications in real-world scenarios, for example, protecting personal privacy in social networks and understanding camouflage patterns in transaction networks. To simulate interactive attack and defense behaviors, we further propose a game-theoretic framework, called CD-GAME. One player is a graph attacker, while the other player is a Rayleigh Quotient defender. The CD-GAME models the mutual influence and feedback mechanisms between the attacker and the defender, revealing the dynamic evolutionary process of the game. Both players dynamically update their strategies until they reach the Nash equilibrium. Extensive experiments demonstrate the effectiveness of our proposed attack and defense methods, and both outperform existing baselines by a significant margin. Furthermore, CD-GAME provides valuable insights for understanding interactive attack and defense scenarios in community detection problems. We found that in traditional single-step attack or defense, attacker tends to employ strategies that are most effective, but are easily detected and countered by defender. When the interactive game reaches a Nash equilibrium, attacker adopts more imperceptible strategies that can still achieve satisfactory attack effectiveness even after defense.

</details>


### [76] [Mitigating the Safety Alignment Tax with Null-Space Constrained Policy Optimization](https://arxiv.org/abs/2512.11391)
*Yifan Niu,Han Xiao,Dongyi Liu,Nuo Chen,Jia Li*

Main category: cs.LG

TL;DR: NSPO是一种新颖的强化学习框架，通过将安全策略梯度投影到通用任务零空间，在保证大语言模型安全对齐的同时避免遗忘核心能力，显著减少对齐税。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在现实应用中需要确保行为符合人类价值观和社会规范，但现有的强化学习安全对齐方法会导致模型遗忘已学习的通用能力（对齐税问题），这限制了模型的实际应用价值。

Method: 提出零空间约束策略优化（NSPO）框架，将安全策略梯度几何投影到通用任务的零空间中，从而在安全对齐过程中保护模型的原始核心能力。该方法理论上证明了既能保持核心能力，又能保证安全对齐的有效下降方向。

Result: NSPO在实验中大幅超越现有方法，在数学、代码和指令跟随等通用任务上保持准确性的同时，实现了最先进的安全性能。数据效率高，仅需PKU-SafeRLHF中40%的安全标注数据就能达到良好效果，无需大量混合通用任务数据。

Conclusion: NSPO有效解决了大语言模型安全对齐中的对齐税问题，在保证安全性的同时保护了模型的通用能力，为实际部署提供了高效可靠的解决方案。

Abstract: As Large Language Models (LLMs) are increasingly deployed in real-world applications, it is important to ensure their behaviors align with human values, societal norms, and ethical principles. However, safety alignment under Reinforcement Learning (RL) often suffers from forgetting learned general abilities, which is also known as the alignment tax. To address this issue, we introduce Null-Space constrained Policy Optimization (NSPO), a novel RL framework for LLM safety alignment while preserving their core abilities. The safety policy gradients are geometrically projected into the null space of general tasks, thereby mitigating the safety alignment tax. In addition, we theoretically prove that NSPO preserves the model's original core capabilities, while still guaranteeing a descent direction for effective safety alignment. Extensive experiments demonstrate that NSPO outperforms existing methods by a large margin, achieving state-of-the-art safety performance without sacrificing accuracy on general tasks, including math, code, and instruction-following tasks. Notably, NSPO is data-efficient and only requires 40% of public human-annotated safety data from PKU-SafeRLHF to achieve promising safety performance, without a large amount of mixed general tasks data in existing alignment methods.

</details>


### [77] [NeuralOGCM: Differentiable Ocean Modeling with Learnable Physics](https://arxiv.org/abs/2512.11525)
*Hao Wu,Yuan Gao,Fan Xu,Fan Zhang,Guangliang Liu,Yuxuan Liang,Xiaomeng Huang*

Main category: cs.LG

TL;DR: NeuralOGCM：结合可微分编程与深度学习的海洋建模框架，通过可学习的物理求解器与神经网络修正，在保持物理一致性的同时提升计算效率


<details>
  <summary>Details</summary>
Motivation: 解决科学模拟中长期存在的计算效率与物理保真度之间的权衡问题，传统数值模型计算成本高，纯AI方法缺乏物理一致性

Method: 1) 完全可微分的动力学求解器，将关键物理参数（如扩散系数）转化为可学习参数；2) 深度神经网络修正子网格尺度过程和离散化误差；3) 统一ODE求解器整合两者输出

Result: NeuralOGCM保持长期稳定性和物理一致性，在速度上显著优于传统数值模型，在精度上优于纯AI基线方法

Conclusion: 该工作为构建快速、稳定且物理可信的科学计算模型开辟了新路径，实现了物理知识与数据驱动方法的协同融合

Abstract: High-precision scientific simulation faces a long-standing trade-off between computational efficiency and physical fidelity. To address this challenge, we propose NeuralOGCM, an ocean modeling framework that fuses differentiable programming with deep learning. At the core of NeuralOGCM is a fully differentiable dynamical solver, which leverages physics knowledge as its core inductive bias. The learnable physics integration captures large-scale, deterministic physical evolution, and transforms key physical parameters (e.g., diffusion coefficients) into learnable parameters, enabling the model to autonomously optimize its physical core via end-to-end training. Concurrently, a deep neural network learns to correct for subgrid-scale processes and discretization errors not captured by the physics model. Both components work in synergy, with their outputs integrated by a unified ODE solver. Experiments demonstrate that NeuralOGCM maintains long-term stability and physical consistency, significantly outperforming traditional numerical models in speed and pure AI baselines in accuracy. Our work paves a new path for building fast, stable, and physically-plausible models for scientific computing.

</details>


### [78] [Bhargava Cube--Inspired Quadratic Regularization for Structured Neural Embeddings](https://arxiv.org/abs/2512.11392)
*S Sairam,Prateek P Kulkarni*

Main category: cs.LG

TL;DR: 提出一种结合数论中Bhargava立方体代数约束的神经表示学习方法，在3维潜在空间中学习满足二次关系的结构化嵌入，实现高准确率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法在无结构的潜在空间中学习表示，缺乏可解释性和数学一致性。需要将数学结构融入神经网络表示学习。

Method: 将输入数据映射到受约束的3维潜在空间，嵌入被正则化以满足从Bhargava组合结构推导的学习二次关系。使用与分类目标独立的可微分辅助损失函数。

Result: 在MNIST上达到99.46%准确率，产生可解释的3D嵌入，自然地按数字类别聚类并满足学习的二次约束。

Conclusion: 这是数论结构在神经表示学习中的首次应用，为在神经网络中融入结构化数学先验奠定了基础，相比需要显式几何监督的流形学习方法，本方法通过可微分约束施加弱代数先验。

Abstract: We present a novel approach to neural representation learning that incorporates algebraic constraints inspired by Bhargava cubes from number theory. Traditional deep learning methods learn representations in unstructured latent spaces lacking interpretability and mathematical consistency. Our framework maps input data to constrained 3-dimensional latent spaces where embeddings are regularized to satisfy learned quadratic relationships derived from Bhargava's combinatorial structures. The architecture employs a differentiable auxiliary loss function operating independently of classification objectives, guiding models toward mathematically structured representations. We evaluate on MNIST, achieving 99.46% accuracy while producing interpretable 3D embeddings that naturally cluster by digit class and satisfy learned quadratic constraints. Unlike existing manifold learning approaches requiring explicit geometric supervision, our method imposes weak algebraic priors through differentiable constraints, ensuring compatibility with standard optimization. This represents the first application of number-theoretic constructs to neural representation learning, establishing a foundation for incorporating structured mathematical priors in neural networks.

</details>


### [79] [Sliced ReLU attention: Quasi-linear contextual expressivity via sorting](https://arxiv.org/abs/2512.11411)
*Siwan Boufadène,François-Xavier Vialard*

Main category: cs.LG

TL;DR: 提出切片ReLU注意力机制，通过键-查询差异的一维投影和排序实现O(n log n)复杂度，适合长上下文处理，同时保持理论表达能力。


<details>
  <summary>Details</summary>
Motivation: 现有注意力机制如softmax和ReLU变体在长上下文场景下计算复杂度高，需要一种既能保持表达能力又具有高效计算复杂度的新注意力机制。

Method: 提出切片ReLU注意力机制，不直接对点积应用非线性，而是对键-查询差异进行一维投影，利用排序操作获得准线性复杂度，形成可微的非对称核。

Result: 该机制计算复杂度为O(n log n)，适合长上下文；理论证明具有序列到序列解缠能力和上下文通用逼近性质；小规模实验显示实际应用潜力。

Conclusion: 切片ReLU注意力机制在保持理论表达能力的同时显著降低计算复杂度，为长上下文处理提供了有前景的替代方案。

Abstract: We introduce sliced ReLU attention, a new attention mechanism that departs structurally from both softmax and ReLU-based alternatives. Instead of applying a nonlinearity to pairwise dot products, we operate on one-dimensional projections of key--query differences and leverage sorting to obtain quasi-linear complexity. This construction yields a differentiable, non-symmetric kernel that can be computed in O(n log(n)) through a sorting procedure, making it suitable for very long contexts. Beyond computational benefits, the model retains strong theoretical expressive power: we establish two in-context expressivity results, previously known for softmax attention, showing that sliced ReLU attention preserves the ability to perform nontrivial sequence-to-sequence disentangling tasks and satisfies a contextual universal approximation property. Finally, we illustrate the potential practical interest of this kernel in small-scale experiments.

</details>


### [80] [Optimizing the Training Diet: Data Mixture Search for Robust Time Series Forecasting](https://arxiv.org/abs/2512.11546)
*Federico Pennino,Maurizio Gabbrielli*

Main category: cs.LG

TL;DR: 提出数据选择框架，通过优化训练数据组合而非模型参数，在传感器数据上实现"少即是多"的效果


<details>
  <summary>Details</summary>
Motivation: 传统深度学习训练假设数据越多越好，但原始传感器数据通常不平衡且冗余，并非所有数据点对模型泛化都有同等贡献。需要探索数据选择对模型性能的影响。

Method: 1) 使用大规模编码器和k-means聚类将数据集划分为行为一致的不同簇，作为训练"原料"；2) 采用Optuna优化框架搜索高维数据混合空间，为每个簇提出特定采样比例；3) 根据配方构建训练集，训练并评估目标模型。

Result: 在PMSM数据集上，该方法将基线MSE从1.70提升到1.37，性能改善19.41%。数据中心的搜索持续发现优于全数据集训练的混合数据配方。

Conclusion: 通过优化训练数据组合而非模型参数，可以实现"少即是多"的效果，表明数据选择对模型性能有重要影响，为传感器数据训练提供了新的数据中心视角。

Abstract: The standard paradigm for training deep learning models on sensor data assumes that more data is always better. However, raw sensor streams are often imbalanced and contain significant redundancy, meaning that not all data points contribute equally to model generalization. In this paper, we show that, in some cases, "less is more" when considering datasets. We do this by reframing the data selection problem: rather than tuning model hyperparameters, we fix the model and optimize the composition of the training data itself. We introduce a framework for discovering the optimal "training diet" from a large, unlabeled time series corpus. Our framework first uses a large-scale encoder and k-means clustering to partition the dataset into distinct, behaviorally consistent clusters. These clusters represent the fundamental 'ingredients' available for training. We then employ the Optuna optimization framework to search the high-dimensional space of possible data mixtures. For each trial, Optuna proposes a specific sampling ratio for each cluster, and a new training set is constructed based on this recipe. A smaller target model is then trained and evaluated. Our experiments reveal that this data-centric search consistently discovers data mixtures that yield models with significantly higher performance compared to baselines trained on the entire dataset. Specifically - evaluated on PMSM dataset - our method improved performance from a baseline MSE of 1.70 to 1.37, a 19.41% improvement.

</details>


### [81] [Atomic Action Slicing: Planner-Aligned Options for Generalist VLA Agents](https://arxiv.org/abs/2512.11584)
*Stefan Tabakov,Asen Popov,Dimitar Dimitrov,S. Ensiye Kiyamousavi,Vladimir Hristov,Boris Kraychev*

Main category: cs.LG

TL;DR: 提出Atomic Action Slicing方法，将长时程演示分解为短时原子动作，提升VLA模型在技能组合任务上的泛化能力，并在LIBERO数据集上验证了效果。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言-动作模型在需要新技能或物体组合的任务上泛化能力差，需要更好的动作分解方法来提升规划和学习效果。

Method: 提出Atomic Action Slicing方法，将长时程演示分解为短时、类型化的原子动作，使用更强的分割器（Gemini 2.5 Pro）匹配规划器定义的方案，并创建了包含2124个原子片段的验证数据集。

Result: 在LIBERO数据集上，使用原子数据集微调CLIP-RT+后，任务成功率从94.2%提升到95.3%（LIBERO-Goal）和从83.8%提升到88.8%（LIBERO-Long）。

Conclusion: 原子动作切片方法能有效提升VLA模型在复杂组合任务上的性能，并公开了GATE-VLAP数据集供社区使用。

Abstract: Current vision-language-action (VLA) models generalize poorly, particularly when tasks require new compositions of skills or objects. We introduce Atomic Action Slicing (AAS), a planner-aligned approach that decomposes long-horizon demonstrations into short, typed atomic actions that are easier for planners to use and policies to learn. Using LIBERO demonstrations, AAS produces a validated dataset of 2,124 atomic segments labeled with action type, temporal span, and confidence. A stronger segmenter (Gemini 2.5 Pro) closely matches planner-defined plans and remains robust under keyframe jitter, while smaller models perform worse on multi-object tasks. Fine-tuning CLIP-RT+ on our atomic dataset improves task success from 94.2% to 95.3% on LIBERO-Goal and 83.8% to 88.8% on LIBERO-Long. We publicly release the GATE-VLAP dataset on HuggingFace(https://huggingface.co/datasets/gate-institute/GATE-VLAP-datasets)

</details>


### [82] [xGR: Efficient Generative Recommendation Serving at Scale](https://arxiv.org/abs/2512.11529)
*Qingxiao Sun,Tongxuan Liu,Shen Zhang,Siyu Wu,Peijun Yang,Haotian Liang,Menxin Li,Xiaolong Ma,Zhiwei Liang,Ziyi Ren,Minchao Zhang,Xinyu Liu,Ke Zhang,Depei Qian,Hailong Yang*

Main category: cs.LG

TL;DR: xGR是一个面向生成式推荐系统的服务系统，通过优化计算流程和数据结构，在严格延迟约束下实现高并发场景下的高效推理。


<details>
  <summary>Details</summary>
Motivation: 生成式推荐系统虽然利用LLM增强了对长用户-物品序列的理解，但其工作负载与传统的LLM服务有显著差异：处理长提示但生成短固定长度输出，解码阶段计算成本高（由于大波束宽度），且在巨大物品空间中排序开销特别耗时。

Method: 1) 通过分阶段计算和分离的KV缓存统一prefill和decode阶段处理；2) 实现早期排序终止和基于掩码的物品过滤，并重用数据结构；3) 重构整体流水线以利用多级重叠和多流并行。

Result: 在真实世界推荐服务数据集上的实验表明，在严格延迟约束下，xGR相比最先进的基线实现了至少3.49倍的吞吐量提升。

Conclusion: xGR系统针对生成式推荐的特殊工作负载进行了优化，通过创新的计算和内存管理策略，显著提升了高并发场景下的服务效率。

Abstract: Recommendation system delivers substantial economic benefits by providing personalized predictions. Generative recommendation (GR) integrates LLMs to enhance the understanding of long user-item sequences. Despite employing attention-based architectures, GR's workload differs markedly from that of LLM serving. GR typically processes long prompt while producing short, fixed-length outputs, yet the computational cost of each decode phase is especially high due to the large beam width. In addition, since the beam search involves a vast item space, the sorting overhead becomes particularly time-consuming. We propose xGR, a GR-oriented serving system that meets strict low-latency requirements under highconcurrency scenarios. First, xGR unifies the processing of prefill and decode phases through staged computation and separated KV cache. Second, xGR enables early sorting termination and mask-based item filtering with data structure reuse. Third, xGR reconstructs the overall pipeline to exploit multilevel overlap and multi-stream parallelism. Our experiments with real-world recommendation service datasets demonstrate that xGR achieves at least 3.49x throughput compared to the state-of-the-art baseline under strict latency constraints.

</details>


### [83] [Parametric Numerical Integration with (Differential) Machine Learning](https://arxiv.org/abs/2512.11530)
*Álvaro Leitao,Jonatan Ráfales*

Main category: cs.LG

TL;DR: 提出一种结合微分学习的机器学习方法求解参数积分，在统计泛函、切比雪夫展开和微分方程积分三类问题上均优于传统方法


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法在求解参数积分问题时存在局限性，需要更高效、更精确的数值积分方法。微分学习框架能够利用导数信息提升训练效果

Method: 采用微分机器学习框架，在训练过程中融入导数信息。该方法应用于三类代表性积分问题：统计泛函（矩和累积分布函数）、切比雪夫展开函数逼近、微分方程直接产生的积分

Result: 微分学习方法在所有测试案例中均优于标准架构，实现了更低的均方误差、更好的可扩展性和更高的样本效率

Conclusion: 微分机器学习为参数积分求解提供了有效的框架，在多种问题类型上展现出优越性能，为数值积分问题提供了新的解决方案

Abstract: In this work, we introduce a machine/deep learning methodology to solve parametric integrals. Besides classical machine learning approaches, we consider a differential learning framework that incorporates derivative information during training, emphasizing its advantageous properties. Our study covers three representative problem classes: statistical functionals (including moments and cumulative distribution functions), approximation of functions via Chebyshev expansions, and integrals arising directly from differential equations. These examples range from smooth closed-form benchmarks to challenging numerical integrals. Across all cases, the differential machine learning-based approach consistently outperforms standard architectures, achieving lower mean squared error, enhanced scalability, and improved sample efficiency.

</details>


### [84] [A Multi-Criteria Automated MLOps Pipeline for Cost-Effective Cloud-Based Classifier Retraining in Response to Data Distribution Shifts](https://arxiv.org/abs/2512.11541)
*Emmanuel K. Katalay,David O. Dimandja,Jordan F. Masakuna*

Main category: cs.LG

TL;DR: 提出自动化MLOps管道，用于检测数据分布漂移并自动触发神经网络分类器重训练，提高模型在动态环境中的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在数据分布随时间变化时性能会下降（数据分布漂移），而当前的MLOps流程通常是手动的，需要人工触发模型重训练和重新部署，效率低下且响应不及时。

Method: 设计自动化MLOps管道，采用多标准统计技术检测数据分布变化，仅在必要时触发模型更新，确保计算效率和资源优化。

Result: 在多个基准异常检测数据集上的实验表明，相比传统重训练策略，该框架显著提高了模型准确性和鲁棒性。

Conclusion: 该工作为在数据分布变化常见的动态现实环境中部署更可靠、自适应的机器学习系统提供了基础。

Abstract: The performance of machine learning (ML) models often deteriorates when the underlying data distribution changes over time, a phenomenon known as data distribution drift. When this happens, ML models need to be retrained and redeployed. ML Operations (MLOps) is often manual, i.e., humans trigger the process of model retraining and redeployment. In this work, we present an automated MLOps pipeline designed to address neural network classifier retraining in response to significant data distribution changes. Our MLOps pipeline employs multi-criteria statistical techniques to detect distribution shifts and triggers model updates only when necessary, ensuring computational efficiency and resource optimization. We demonstrate the effectiveness of our framework through experiments on several benchmark anomaly detection data sets, showing significant improvements in model accuracy and robustness compared to traditional retraining strategies. Our work provides a foundation for deploying more reliable and adaptive ML systems in dynamic real-world settings, where data distribution changes are common.

</details>


### [85] [Elastic-Net Multiple Kernel Learning: Combining Multiple Data Sources for Prediction](https://arxiv.org/abs/2512.11547)
*Janaina Mourão-Miranda,Zakria Hussain,Konstantinos Tsirlis,Christophe Phillips,John Shawe-Taylor*

Main category: cs.LG

TL;DR: 提出一种新的弹性网络正则化多核学习方法，通过解析更新核权重，在神经影像应用中实现稀疏且可解释的模型


<details>
  <summary>Details</summary>
Motivation: 现有弹性网络正则化多核学习方法采用两阶段优化过程，计算复杂且效率低。需要更简单高效的方法来处理神经影像中相关核的选择问题，同时保持模型的可解释性

Method: 提出新的ENMKL公式，推导出核权重的解析更新规则。为SVM和核岭回归开发了具体算法，并在PRoNTo工具箱中实现

Result: ENMKL在所有任务中匹配或优于l1正则化MKL，仅在一个场景中略逊于标准SVM。ENMKL能产生更稀疏、更可解释的模型，能选择性地加权相关核

Conclusion: 提出的ENMKL方法提供了一种简单有效的多核学习框架，特别适用于神经影像等需要模型可解释性和处理相关特征的领域

Abstract: Multiple Kernel Learning (MKL) models combine several kernels in supervised and unsupervised settings to integrate multiple data representations or sources, each represented by a different kernel. MKL seeks an optimal linear combination of base kernels that maximizes a generalized performance measure under a regularization constraint. Various norms have been used to regularize the kernel weights, including $l1$, $l2$ and $lp$, as well as the "elastic-net" penalty, which combines $l1$- and $l2$-norm to promote both sparsity and the selection of correlated kernels. This property makes elastic-net regularized MKL (ENMKL) especially valuable when model interpretability is critical and kernels capture correlated information, such as in neuroimaging. Previous ENMKL methods have followed a two-stage procedure: fix kernel weights, train a support vector machine (SVM) with the weighted kernel, and then update the weights via gradient descent, cutting-plane methods, or surrogate functions. Here, we introduce an alternative ENMKL formulation that yields a simple analytical update for the kernel weights. We derive explicit algorithms for both SVM and kernel ridge regression (KRR) under this framework, and implement them in the open-source Pattern Recognition for Neuroimaging Toolbox (PRoNTo). We evaluate these ENMKL algorithms against $l1$-norm MKL and against SVM (or KRR) trained on the unweighted sum of kernels across three neuroimaging applications. Our results show that ENMKL matches or outperforms $l1$-norm MKL in all tasks and only underperforms standard SVM in one scenario. Crucially, ENMKL produces sparser, more interpretable models by selectively weighting correlated kernels.

</details>


### [86] [Fully Inductive Node Representation Learning via Graph View Transformation](https://arxiv.org/abs/2512.11561)
*Dooho Lee,Myeong Kong,Minho Jeong,Jaemin Yoo*

Main category: cs.LG

TL;DR: 提出视图空间作为新的表示轴，通过图视图变换(GVT)实现完全归纳的节点表示学习，在27个基准测试中超越现有方法


<details>
  <summary>Details</summary>
Motivation: 预训练模型在未见数据集上的泛化是基础模型的关键，但图数据中特征空间在维度和语义上的巨大差异使得完全归纳推理困难，特征空间的任何变换都可能破坏对未见数据集的归纳适用性

Method: 引入视图空间作为统一表示任意图的新轴，提出图视图变换(GVT)作为视图空间中节点和特征置换等变的映射，以此构建循环GVT作为完全归纳的节点表示学习模型

Result: 在OGBN-Arxiv上预训练并在27个节点分类基准测试中评估，循环GVT比先前完全归纳图模型GraphAny提升+8.93%，比12个单独调优的GNN至少提升+3.30%

Conclusion: 视图空间为完全归纳节点表示学习提供了原则性和有效的理论基础，解决了图数据跨数据集泛化的挑战

Abstract: Generalizing a pretrained model to unseen datasets without retraining is an essential step toward a foundation model. However, achieving such cross-dataset, fully inductive inference is difficult in graph-structured data where feature spaces vary widely in both dimensionality and semantics. Any transformation in the feature space can easily violate the inductive applicability to unseen datasets, strictly limiting the design space of a graph model. In this work, we introduce the view space, a novel representational axis in which arbitrary graphs can be naturally encoded in a unified manner. We then propose Graph View Transformation (GVT), a node- and feature-permutation-equivariant mapping in the view space. GVT serves as the building block for Recurrent GVT, a fully inductive model for node representation learning. Pretrained on OGBN-Arxiv and evaluated on 27 node-classification benchmarks, Recurrent GVT outperforms GraphAny, the prior fully inductive graph model, by +8.93% and surpasses 12 individually tuned GNNs by at least +3.30%. These results establish the view space as a principled and effective ground for fully inductive node representation learning.

</details>


### [87] [Brain-Semantoks: Learning Semantic Tokens of Brain Dynamics with a Self-Distilled Foundation Model](https://arxiv.org/abs/2512.11582)
*Sam Gijsen,Marc-Andre Schulz,Kerstin Ritter*

Main category: cs.LG

TL;DR: Brain-Semantoks是一个自监督框架，通过语义标记器和自蒸馏目标学习大脑动态的抽象表征，在fMRI时间序列上表现出色，无需大量微调即可用于下游任务。


<details>
  <summary>Details</summary>
Motivation: 当前fMRI基础模型通常在小脑区域上使用掩码-重建目标训练，关注低层信息导致表征对噪声和时间波动敏感，需要大量微调才能用于下游任务。

Method: 提出Brain-Semantoks框架，包含两个核心创新：1）语义标记器将噪声区域信号聚合成代表功能网络的鲁棒标记；2）自蒸馏目标强制表征在时间上的稳定性。通过新颖的训练课程稳定学习过程。

Result: 学习到的表征即使在仅使用线性探针的情况下，也能在各种下游任务上表现出色。扩展分析表明，更多未标记数据可靠地带来分布外性能提升，无需领域适应。

Conclusion: Brain-Semantoks能够从低信噪比时间序列中稳健地学习有意义的特征，为fMRI基础模型提供了有效的自监督学习框架，显著减少了下游任务所需的微调工作。

Abstract: The development of foundation models for functional magnetic resonance imaging (fMRI) time series holds significant promise for predicting phenotypes related to disease and cognition. Current models, however, are often trained using a mask-and-reconstruct objective on small brain regions. This focus on low-level information leads to representations that are sensitive to noise and temporal fluctuations, necessitating extensive fine-tuning for downstream tasks. We introduce Brain-Semantoks, a self-supervised framework designed specifically to learn abstract representations of brain dynamics. Its architecture is built on two core innovations: a semantic tokenizer that aggregates noisy regional signals into robust tokens representing functional networks, and a self-distillation objective that enforces representational stability across time. We show that this objective is stabilized through a novel training curriculum, ensuring the model robustly learns meaningful features from low signal-to-noise time series. We demonstrate that learned representations enable strong performance on a variety of downstream tasks even when only using a linear probe. Furthermore, we provide comprehensive scaling analyses indicating more unlabeled data reliably results in out-of-distribution performance gains without domain adaptation.

</details>


### [88] [A Fast Interpretable Fuzzy Tree Learner](https://arxiv.org/abs/2512.11616)
*Javier Fumanal-Idocin,Raquel Fernandez-Peralta,Javier Andreu-Perez*

Main category: cs.LG

TL;DR: 提出一种基于贪婪算法的模糊树方法，将经典树分裂算法扩展到模糊规则，在保持预测性能的同时显著降低计算成本并提高可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有模糊规则挖掘算法不能同时保证合理的语言分区和小规则库规模，进化方法计算成本过高，神经方法如ANFIS难以保持语言可解释性。

Method: 将经典树分裂算法从清晰规则扩展到模糊树，结合贪婪算法的计算效率和模糊逻辑的可解释性优势。

Result: 在表格分类基准测试中，该方法达到与最先进模糊分类器相当的准确率，计算成本显著降低，并产生复杂度受限的更可解释规则库。

Conclusion: 提出的模糊贪婪树方法在保持竞争性预测性能的同时，实现了可解释的语言分区，相比进化方法大幅减少运行时间，平衡了可解释性和计算效率。

Abstract: Fuzzy rule-based systems have been mostly used in interpretable decision-making because of their interpretable linguistic rules. However, interpretability requires both sensible linguistic partitions and small rule-base sizes, which are not guaranteed by many existing fuzzy rule-mining algorithms. Evolutionary approaches can produce high-quality models but suffer from prohibitive computational costs, while neural-based methods like ANFIS have problems retaining linguistic interpretations. In this work, we propose an adaptation of classical tree-based splitting algorithms from crisp rules to fuzzy trees, combining the computational efficiency of greedy algoritms with the interpretability advantages of fuzzy logic. This approach achieves interpretable linguistic partitions and substantially improves running time compared to evolutionary-based approaches while maintaining competitive predictive performance. Our experiments on tabular classification benchmarks proof that our method achieves comparable accuracy to state-of-the-art fuzzy classifiers with significantly lower computational cost and produces more interpretable rule bases with constrained complexity. Code is available in: https://github.com/Fuminides/fuzzy_greedy_tree_public

</details>


### [89] [Bridging Streaming Continual Learning via In-Context Large Tabular Models](https://arxiv.org/abs/2512.11668)
*Afonso Lourenço,João Gama,Eric P. Xing,Goreti Marreiros*

Main category: cs.LG

TL;DR: 论文提出使用大型上下文表格模型作为流式持续学习的桥梁，通过将无限数据流压缩为紧凑摘要，同时满足流式学习的高效适应和持续学习的长期记忆需求。


<details>
  <summary>Details</summary>
Motivation: 现有研究将流式学习和持续学习分开处理：持续学习关注长期记忆但缺乏实时约束，流式学习强调快速适应但忽视遗忘问题。需要一种统一框架来同时处理概念漂移和灾难性遗忘。

Method: 提出使用大型上下文表格模型作为桥梁，将无限数据流实时压缩为紧凑摘要。基于两个核心数据选择原则：1) 分布匹配（平衡可塑性与稳定性），2) 分布压缩（通过多样化和检索机制控制内存大小）。

Result: 论文提出了一个理论框架，将流式学习和持续学习统一在流式持续学习范式下，通过大型上下文表格模型实现数据流的实时压缩和长期知识保留。

Conclusion: 大型上下文表格模型为流式持续学习提供了自然桥梁，通过分布匹配和分布压缩原则，能够同时满足流式学习的高效适应和持续学习的长期记忆需求。

Abstract: In streaming scenarios, models must learn continuously, adapting to concept drifts without erasing previously acquired knowledge. However, existing research communities address these challenges in isolation. Continual Learning (CL) focuses on long-term retention and mitigating catastrophic forgetting, often without strict real-time constraints. Stream Learning (SL) emphasizes rapid, efficient adaptation to high-frequency data streams, but typically neglects forgetting. Recent efforts have tried to combine these paradigms, yet no clear algorithmic overlap exists. We argue that large in-context tabular models (LTMs) provide a natural bridge for Streaming Continual Learning (SCL). In our view, unbounded streams should be summarized on-the-fly into compact sketches that can be consumed by LTMs. This recovers the classical SL motivation of compressing massive streams with fixed-size guarantees, while simultaneously aligning with the experience-replay desiderata of CL. To clarify this bridge, we show how the SL and CL communities implicitly adopt a divide-to-conquer strategy to manage the tension between plasticity (performing well on the current distribution) and stability (retaining past knowledge), while also imposing a minimal complexity constraint that motivates diversification (avoiding redundancy in what is stored) and retrieval (re-prioritizing past information when needed). Within this perspective, we propose structuring SCL with LTMs around two core principles of data selection for in-context learning: (1) distribution matching, which balances plasticity and stability, and (2) distribution compression, which controls memory size through diversification and retrieval mechanisms.

</details>


### [90] [SpectralKrum: A Spectral-Geometric Defense Against Byzantine Attacks in Federated Learning](https://arxiv.org/abs/2512.11760)
*Aditya Tripathi,Karan Sharma,Rahul Mishra,Tapas Kumar Maiti*

Main category: cs.LG

TL;DR: SpectralKrum：一种结合谱子空间估计和几何邻居选择的联邦学习防御方法，在非IID数据分布下对抗拜占庭攻击


<details>
  <summary>Details</summary>
Motivation: 联邦学习在客户端数据分布异构（非IID）时，现有鲁棒聚合方法（如Krum、Bulyan等）的防御效果会显著下降，特别是当攻击者能够观察或近似防御机制时

Method: SpectralKrum融合谱子空间估计和几何邻居选择：1）从历史聚合中学习低维流形；2）将更新投影到学习到的子空间；3）在压缩坐标中应用Krum选择；4）过滤残差能量超过数据驱动阈值的候选

Result: 在CIFAR-10非IID数据（Dirichlet分布，alpha=0.1）上评估，SpectralKrum对方向性和子空间感知攻击（adaptive-steer、buffer-drift）具有竞争力，但对标签翻转和min-max攻击效果有限

Conclusion: SpectralKrum在非IID联邦学习环境中提供了一种有效的防御机制，但面对恶意更新与良性更新在谱上难以区分的攻击时仍有局限性

Abstract: Federated Learning (FL) distributes model training across clients who retain their data locally, but this architecture exposes a fundamental vulnerability: Byzantine clients can inject arbitrarily corrupted updates that degrade or subvert the global model. While robust aggregation methods (including Krum, Bulyan, and coordinate-wise defenses) offer theoretical guarantees under idealized assumptions, their effectiveness erodes substantially when client data distributions are heterogeneous (non-IID) and adversaries can observe or approximate the defense mechanism.
  This paper introduces SpectralKrum, a defense that fuses spectral subspace estimation with geometric neighbor-based selection. The core insight is that benign optimization trajectories, despite per-client heterogeneity, concentrate near a low-dimensional manifold that can be estimated from historical aggregates. SpectralKrum projects incoming updates into this learned subspace, applies Krum selection in compressed coordinates, and filters candidates whose orthogonal residual energy exceeds a data-driven threshold. The method requires no auxiliary data, operates entirely on model updates, and preserves FL privacy properties.
  We evaluate SpectralKrum against eight robust baselines across seven attack scenarios on CIFAR-10 with Dirichlet-distributed non-IID partitions (alpha = 0.1). Experiments spanning over 56,000 training rounds show that SpectralKrum is competitive against directional and subspace-aware attacks (adaptive-steer, buffer-drift), but offers limited advantage under label-flip and min-max attacks where malicious updates remain spectrally indistinguishable from benign ones.

</details>


### [91] [The Adaptive Vekua Cascade: A Differentiable Spectral-Analytic Solver for Physics-Informed Representation](https://arxiv.org/abs/2512.11776)
*Vladimer Khasia*

Main category: cs.LG

TL;DR: AVC提出了一种混合架构，通过解耦流形学习和函数逼近，使用深度网络学习物理域的微分同胚扭曲，将复杂时空动力学投影到潜在流形上，并用广义解析函数基表示解，从而解决谱偏差和维数诅咒问题。


<details>
  <summary>Details</summary>
Motivation: 基于坐标的神经网络在表示连续物理场时面临两个基本问题：谱偏差阻碍高频动力学学习，维数诅咒导致离散特征网格参数爆炸。需要一种既能保持谱精度又内存高效的新方法。

Method: AVC将流形学习与函数逼近解耦：1) 使用深度网络学习物理域的微分同胚扭曲，将复杂动力学投影到潜在流形；2) 在潜在流形上用广义解析函数基表示解；3) 用可微分线性求解器替代标准梯度下降输出层，在前向传播中以闭式最优解析谱系数。

Result: 在五个物理基准测试中（包括高频Helmholtz波传播、稀疏医学重建和非定常3D Navier-Stokes湍流），AVC达到最先进精度，同时将参数数量减少数个数量级（如840参数 vs 420万参数的3D网格），收敛速度比隐式神经表示快2-3倍。

Conclusion: AVC为内存高效、谱精度高的科学机器学习建立了新范式，通过结合深度学习和经典逼近理论，有效解决了谱偏差和维数诅咒问题，在保持精度的同时大幅减少参数数量和加速收敛。

Abstract: Coordinate-based neural networks have emerged as a powerful tool for representing continuous physical fields, yet they face two fundamental pathologies: spectral bias, which hinders the learning of high-frequency dynamics, and the curse of dimensionality, which causes parameter explosion in discrete feature grids. We propose the Adaptive Vekua Cascade (AVC), a hybrid architecture that bridges deep learning and classical approximation theory. AVC decouples manifold learning from function approximation by using a deep network to learn a diffeomorphic warping of the physical domain, projecting complex spatiotemporal dynamics onto a latent manifold where the solution is represented by a basis of generalized analytic functions. Crucially, we replace the standard gradient-descent output layer with a differentiable linear solver, allowing the network to optimally resolve spectral coefficients in a closed form during the forward pass. We evaluate AVC on a suite of five rigorous physics benchmarks, including high-frequency Helmholtz wave propagation, sparse medical reconstruction, and unsteady 3D Navier-Stokes turbulence. Our results demonstrate that AVC achieves state-of-the-art accuracy while reducing parameter counts by orders of magnitude (e.g., 840 parameters vs. 4.2 million for 3D grids) and converging 2-3x faster than implicit neural representations. This work establishes a new paradigm for memory-efficient, spectrally accurate scientific machine learning. The code is available at https://github.com/VladimerKhasia/vecua.

</details>


### [92] [A General Algorithm for Detecting Higher-Order Interactions via Random Sequential Additions](https://arxiv.org/abs/2512.11793)
*Ahmad Shamail,Claire McWhite*

Main category: cs.LG

TL;DR: 提出一种基于几何模式的简单方法，通过随机顺序添加元素并绘制贡献图来发现特征间的相互作用和冗余关系，使用L形模式量化从协同到冗余的连续谱系。


<details>
  <summary>Details</summary>
Motivation: 许多系统存在复杂的组件间相互作用：有些特征相互增强，有些提供冗余信息，有些独立贡献。传统方法难以统一量化和可视化这些相互作用模式。

Method: 通过随机顺序添加元素，多次试验中绘制元素贡献图，观察L形模式。提出L-score连续度量（-1到+1），量化协同性、独立性和冗余性。可视化二维点云，分析特征主导性，并通过成对测量自然揭示高阶相互作用。

Result: 该方法能够：1）通过L形模式直接反映相互作用结构；2）量化元素贡献的依赖性；3）区分协同、独立和冗余模式；4）揭示特征主导性；5）从成对测量中自然涌现高阶相互作用。

Conclusion: 提出了一种简单、几何化的统一方法来发现和量化特征间相互作用，该方法与度量无关，适用于任何可以增量评估性能的领域，为理解复杂系统相互作用结构提供了新视角。

Abstract: Many systems exhibit complex interactions between their components: some features or actions amplify each other's effects, others provide redundant information, and some contribute independently. We present a simple geometric method for discovering interactions and redundancies: when elements are added in random sequential orders and their contributions plotted over many trials, characteristic L-shaped patterns emerge that directly reflect interaction structure. The approach quantifies how the contribution of each element depends on those added before it, revealing patterns that distinguish interaction, independence, and redundancy on a unified scale. When pairwise contributions are visualized as two--dimensional point clouds, redundant pairs form L--shaped patterns where only the first-added element contributes, while synergistic pairs form L--shaped patterns where only elements contribute together. Independent elements show order--invariant distributions. We formalize this with the L--score, a continuous measure ranging from $-1$ (perfect synergy, e.g. $Y=X_1X_2$) to $0$ (independence) to $+1$ (perfect redundancy, $X_1 \approx X_2$). The relative scaling of the L--shaped arms reveals feature dominance in which element consistently provides more information. Although computed only from pairwise measurements, higher--order interactions among three or more elements emerge naturally through consistent cross--pair relationships (e.g. AB, AC, BC). The method is metric--agnostic and broadly applicable to any domain where performance can be evaluated incrementally over non-repeating element sequences, providing a unified geometric approach to uncovering interaction structure.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [93] [Pareto-optimal reinsurance under dependence uncertainty](https://arxiv.org/abs/2512.11430)
*Tim J. Boonen,Xia Han,Peng Liu,Jiacong Wang*

Main category: q-fin.RM

TL;DR: 研究垄断再保险市场中多保险公司与单一再保险公司的帕累托最优再保险设计，考虑异质风险偏好和依赖结构不确定性，使用RVaR风险度量，将无限维优化问题简化为有限维参数问题。


<details>
  <summary>Details</summary>
Motivation: 实际再保险市场中，多个主要保险公司与单一再保险公司存在异质风险偏好，且依赖结构难以准确估计，需要设计在不确定性下的帕累托最优再保险合同。

Method: 采用鲁棒优化方法，假设已知边际分布但依赖结构未指定；使用Range Value-at-Risk (RVaR)风险度量族；将无限维优化问题简化为有限维参数问题；对独立同分布风险利用渐近正态性推导最优两层合同。

Result: 在最坏情况下完全刻画了最优赔偿计划，证明无限维优化问题可简化为每个赔偿函数仅涉及2-3个参数的有限维问题；对独立同分布风险推导出最优两层合同；数值应用展示了依赖结构和异质风险容忍度对最优策略的影响。

Conclusion: 该研究为具有异质风险偏好和依赖结构不确定性的垄断再保险市场提供了帕累托最优再保险设计的完整理论框架，将复杂优化问题简化为可处理形式，具有实际应用价值。

Abstract: This paper studies Pareto-optimal reinsurance design in a monopolistic market with multiple primary insurers and a single reinsurer, all with heterogeneous risk preferences. The risk preferences are characterized by a family of risk measures, called Range Value-at-Risk (RVaR), which includes both Value-at-Risk (VaR) and Expected Shortfall (ES) as special cases. Recognizing the practical difficulty of accurately estimating the dependence structure among the insurers' losses, we adopt a robust optimization approach that assumes the marginal distributions are known while leaving the dependence structure unspecified. We provide a complete characterization of optimal indemnity schedules under the worst-case scenario, showing that the infinite-dimensional optimization problem can be reduced to a tractable finite-dimensional problem involving only two or three parameters for each indemnity function. Additionally, for independent and identically distributed risks, we exploit the argument of asymptotic normality to derive optimal two-parameter layer contracts. Finally, numerical applications are considered in a two-insurer setting to illustrate the influence of the dependence structures and heterogeneous risk tolerances on optimal strategies and the corresponding risk evaluation.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [94] [Linear quadratic control for discrete-time systems with stochastic and bounded noises](https://arxiv.org/abs/2512.11106)
*Xuehui Ma,Shiliang Zhang,Xiaohui Zhang,Jing Xin,Hector Garcia de Marina*

Main category: math.OC

TL;DR: 提出一种同时处理随机噪声和有界噪声的线性二次控制策略，结合卡尔曼滤波和椭球集成员滤波进行状态估计，并推导鲁棒状态反馈最优控制律。


<details>
  <summary>Details</summary>
Motivation: 现有LQC策略只能单独处理随机噪声或有界噪声，无法同时高效处理两种噪声，限制了LQC在实际控制系统中的应用范围。

Method: 1) 开发结合卡尔曼滤波和椭球集成员滤波的状态估计方法；2) 基于估计状态推导考虑两种噪声的鲁棒状态反馈最优控制律；3) 避免过度保守同时保持控制稳定性。

Result: 数值仿真结果表明，所提策略在同时存在随机和有界噪声的情况下，相比现有方法具有更好的控制性能。

Conclusion: 该LQC策略扩展了LQC的应用范围，特别适用于具有多种传感噪声的实际控制系统。

Abstract: This paper focuses on the linear quadratic control (LQC) design of systems corrupted by both stochastic noise and bounded noise simultaneously. When only of these noises are considered, the LQC strategy leads to stochastic or robust controllers, respectively. However, there is no LQC strategy that can simultaneously handle stochastic and bounded noises efficiently. This limits the scope where existing LQC strategies can be applied. In this work, we look into the LQC problem for discrete-time systems that have both stochastic and bounded noises in its dynamics. We develop a state estimation for such systems by efficiently combining a Kalman filter and an ellipsoid set-membership filter. The developed state estimation can recover the estimation optimality when the system is subject to both kinds of noise, the stochastic and the bounded. Upon the estimated state, we derive a robust state-feedback optimal control law for the LQC problem. The control law derivation takes into account both stochastic and bounded-state estimation errors, so as to avoid over-conservativeness while sustaining stability in the control. In this way, the developed LQC strategy extends the range of scenarios where LQC can be applied, especially those of real-world control systems with diverse sensing which are subject to different kinds of noise. We present numerical simulations, and the results demonstrate the enhanced control performance with the proposed strategy.

</details>


### [95] [Finite Convergence of the Moment-SOS Hierarchy on the Product of Spheres](https://arxiv.org/abs/2512.11119)
*Sami Halaseh,Victor Magron,Mateusz Skomra*

Main category: math.OC

TL;DR: 该论文研究了在多球体乘积上最小化多重齐次多项式的多项式优化问题，证明了对于一般的多重齐次目标函数，矩-SOS层次具有有限收敛性。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在解决张量优化中的最佳秩一近似问题，该问题可以建模为在多球体乘积上最小化多重齐次多项式的多项式优化问题。先前的研究只处理了单个球体上的齐次多项式情况，需要扩展到更一般的多重齐次情形。

Method: 使用矩-SOS层次方法，结合Huang等人的局部最优性条件结果。为了证明局部最优性条件在一般情况下成立，采用了微分几何和Morse理论的技术。

Result: 证明了对于一般的多重齐次目标函数，矩-SOS层次具有有限收敛性。这一结果将Huang先前在单个球体上齐次多项式的工作推广到了更一般的多重齐次情形。

Conclusion: 该工作成功地将有限收敛性结果从单个球体上的齐次多项式推广到多球体乘积上的多重齐次多项式，为张量秩一近似问题提供了理论保证。

Abstract: We study the polynomial optimization problem of minimizing a multihomogeneous polynomial over the product of spheres. This polynomial optimization problem models the tensor optimization problem of finding the best rank one approximation of an arbitrary tensor. We show that the moment-SOS hierarchy has finite convergence in this case, for a generic multihomogeneous objective function. To show finite convergence of the hierarchy, we use a result of Huang et al. [SIAM J.Optim. 34(4) (2024), pp 3399-3428], which relies on local optimality conditions. To prove that the local optimality conditions hold generically, we use techniques from differential geometry and Morse theory. This work generalizes the main result of Huang [Optim. Lett. 17(5) (2023), pp 1263-1270], which shows finite convergence for the case of a homogeneous polynomial over a single sphere.

</details>


### [96] [Feedback Synthesis for Nonlinear Systems Via Convex Control Lyapunov Functions](https://arxiv.org/abs/2512.11256)
*Mario Eduardo Villanueva,Juraj Oravec,Radoslav Paulen,Boris Houska*

Main category: math.OC

TL;DR: 提出计算高效的方法合成分段仿射反馈控制器，用于非线性离散时间系统，保证鲁棒性和性能


<details>
  <summary>Details</summary>
Motivation: 传统非线性控制方法计算复杂，难以在实际系统中实现。需要开发既能保证鲁棒性和性能，又具有可配置计算复杂度的显式反馈控制器。

Method: 通过优化配置约束的分段仿射近似来逼近无限时域min-max Hamilton-Jacobi-Bellman方程的值函数。强制分段仿射近似成为非线性系统的广义控制Lyapunov函数，从而保证鲁棒性和性能。

Result: 成功合成了具有认证遍历性能和指定复杂度的显式分段仿射控制器，在受约束的Van der Pol振荡器案例中验证了方法的有效性，能在大的操作域内工作。

Conclusion: 该方法能够生成具有可配置存储复杂度和预定评估时间的反馈控制律，为非线性系统提供了一种计算高效且具有性能保证的显式控制策略合成框架。

Abstract: This paper introduces computationally efficient methods for synthesizing explicit piecewise affine (PWA) feedback laws for nonlinear discrete-time systems, ensuring robustness and performance guarantees. The approach proceeds by optimizing a configuration-constrained PWA approximation of the value function of an infinite-horizon min-max Hamilton-Jacobi-Bellman equation. Here, robustness and performance are maintained by enforcing the PWA approximation to be a generalized control Lyapunov function for the given nonlinear system. This enables the generation of feedback laws with configurable storage complexity and pre-determined evaluation times, based on a selected configuration template. The framework's effectiveness is demonstrated through a constrained Van der Pol oscillator case study, where an explicit PWA controller with certified ergodic performance and specified complexity is synthesized over a large operational domain.

</details>


### [97] [Descent-Net: Learning Descent Directions for Constrained Optimization](https://arxiv.org/abs/2512.11396)
*Zisheng Zhou,Dengyu Zheng,Zirui Chen,Shixiang Chen*

Main category: math.OC

TL;DR: 提出Descent-Net神经网络，通过学习有效下降方向，在保持可行性的同时优化目标值，解决深度学习优化方法中可行性与最优性的平衡问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习在解决大规模优化问题方面具有优势，但现有方法难以同时保证可行性和达到最优目标值。需要一种既能保持可行性又能有效优化目标的方法。

Method: 提出Descent-Net神经网络，从可行解中学习有效的下降方向。通过沿学习到的方向更新解，在保持可行性的同时改进目标值。

Result: 在合成优化任务和实际AC最优潮流问题上表现优异，在包含数千资产的组合优化实验中展现出良好的可扩展性。

Conclusion: Descent-Net能够有效解决深度学习优化方法中可行性与最优性的平衡问题，在大规模优化问题上具有良好的性能和可扩展性。

Abstract: Deep learning approaches, known for their ability to model complex relationships and fast execution, are increasingly being applied to solve large optimization problems. However, existing methods often face challenges in simultaneously ensuring feasibility and achieving an optimal objective value. To address this issue, we propose Descent-Net, a neural network designed to learn an effective descent direction from a feasible solution. By updating the solution along this learned direction, Descent-Net improves the objective value while preserving feasibility. Our method demonstrates strong performance on both synthetic optimization tasks and the real-world AC optimal power flow problem, while also exhibiting effective scalability to large problems, as shown by portfolio optimization experiments with thousands of assets.

</details>


### [98] [A new $ν$-metric computational example for the diffusion equation with boundary control and point observation](https://arxiv.org/abs/2512.11428)
*Amol Sasane*

Main category: math.OC

TL;DR: 论文研究了具有参数不确定性的扩散方程控制系统的ν-度量计算，该系统在一维有界区域中具有边界控制和点观测。


<details>
  <summary>Details</summary>
Motivation: 在鲁棒控制理论中，ν-度量是评估系统鲁棒性的重要指标。对于具有参数不确定性的扩散方程控制系统，特别是在边界控制和点观测的配置下，如何计算ν-度量是一个需要解决的理论问题。

Method: 针对一维有界区域中的扩散方程，考虑参数不确定性，建立边界控制和点观测的数学模型，然后推导并计算该系统的ν-度量。

Result: 成功推导出了具有参数不确定性的扩散方程控制系统的ν-度量计算公式，为这类系统的鲁棒性分析提供了理论工具。

Conclusion: 该研究扩展了ν-度量在分布参数系统中的应用，为具有参数不确定性的扩散方程控制系统的鲁棒性分析提供了有效的计算方法。

Abstract: The $ν$-metric used in robust control is computed for control systems with parametric uncertainty, governed by a diffusion equation in a bounded one-dimensional spatial region with boundary control and point observation.

</details>


### [99] [Convergence-Guaranteed Algorithms for l1/2-Regularized Quadratic Programs with Assignment Constraints](https://arxiv.org/abs/2512.11429)
*Lijun Xie,Ran Gu,Xin Liu*

Main category: math.OC

TL;DR: 提出一种求解带分配约束的二次规划问题的新方法，通过l1/2正则化将离散约束松弛为连续约束，并证明在一定条件下与原问题等价，采用ADMM框架高效求解。


<details>
  <summary>Details</summary>
Motivation: 解决设施选址、MIMO检测、最大均值差异计算等领域出现的带分配约束的二次规划问题，该问题是NP难的组合优化问题，离散约束使得连续优化算法无法直接应用。

Method: 将二元约束松弛为连续盒约束，加入l1/2正则化项驱动松弛变量趋向二元值；证明当正则化参数超过阈值时，正则化问题与原问题等价；采用变量分裂技术和ADMM框架求解，所有子问题都有闭式解。

Result: 理论分析证实算法的收敛性，在特定条件下可实现有限步终止；在各种数值测试中验证了算法的有效性。

Conclusion: 提出的l1/2正则化方法成功将离散分配约束问题转化为连续优化问题，ADMM框架提供了高效求解方案，为解决这类NP难组合优化问题提供了新途径。

Abstract: This paper addresses a quadratic problem with assignment constraints, an NP-hard combinatorial optimization problem arisen from facility location, multiple-input multiple-output detection, and maximum mean discrepancy calculation et al. The discrete nature of the constraints precludes the use of continuous optimization algorithms. Therefore, we begin by relaxing the binary constraints into continuous box constraints and incorporate an l1/2 regularization term to drive the relaxed variables toward binary values. We prove that when the regularization parameter is larger than a threshold, the regularized problem is equivalent to the original problem: they share identical local and global minima, and all Karush-Kuhn-Tucker points of the regularized problem are feasible assignment matrices. To solve the regularized problem approximately and efficiently, we adopt the variable splitting technique, and solve it using the alternating direction method of multipliers (ADMM) framework, in which all subproblems admit closed-form solutions. Detailed theoretical analysis confirms the algorithm's convergence, and finite-step termination under certain conditions. Finally, the algorithm is validated on various numerical tests.

</details>


### [100] [Safe Bayesian optimization across noise models via scenario programming](https://arxiv.org/abs/2512.11580)
*Abdullah Tokmak,Thomas B. Schön,Dominik Baumann*

Main category: math.OC

TL;DR: 提出一种适用于多种噪声模型的安全贝叶斯优化方法，包括同方差亚高斯和异方差重尾分布，通过场景方法提供噪声的高概率边界，并在合成示例和机器人控制器调参中验证。


<details>
  <summary>Details</summary>
Motivation: 现有安全贝叶斯优化算法大多假设同方差亚高斯测量噪声，但许多实际应用中噪声模型不满足这一假设，需要开发适用于更广泛噪声模型的安全优化方法。

Method: 使用场景方法为测量噪声提供高概率边界，将这些边界整合到高概率置信区间中，提出适用于多种噪声模型的安全贝叶斯优化算法。

Result: 在合成示例和Franka Emika机械臂控制器调参仿真中部署算法，证明了算法的安全性和最优性。

Conclusion: 提出了一种简单而严谨的方法，将安全贝叶斯优化扩展到多种噪声模型，包括同方差亚高斯和异方差重尾分布，为安全关键系统的控制器调参提供了更通用的工具。

Abstract: Safe Bayesian optimization (BO) with Gaussian processes is an effective tool for tuning control policies in safety-critical real-world systems, specifically due to its sample efficiency and safety guarantees. However, most safe BO algorithms assume homoscedastic sub-Gaussian measurement noise, an assumption that does not hold in many relevant applications. In this article, we propose a straightforward yet rigorous approach for safe BO across noise models, including homoscedastic sub-Gaussian and heteroscedastic heavy-tailed distributions. We provide a high-probability bound on the measurement noise via the scenario approach, integrate these bounds into high probability confidence intervals, and prove safety and optimality for our proposed safe BO algorithm. We deploy our algorithm in synthetic examples and in tuning a controller for the Franka Emika manipulator in simulation.

</details>


### [101] [What is the optimal way to lie? From microscopic to kinetic descriptions of consensus control](https://arxiv.org/abs/2512.11617)
*Sasha Glendinning,Susana N. Gomes,Marie-Therese Wolfram*

Main category: math.OC

TL;DR: 研究通过引入说谎者来操控舆论动力学达成共识，说谎者通过展示"表面意见"来引导群体达成其目标共识，并分析作为最优控制问题。


<details>
  <summary>Details</summary>
Motivation: 传统舆论动力学中，群体可能无法达成共识或达成不理想的共识。引入说谎者可以主动引导群体达成特定共识，这在社交媒体、政治宣传等场景有实际应用价值。

Method: 1. 将说谎者引导舆论建模为最优控制问题；2. 考虑多种正则化约束（如说谎者希望展示接近真实意见的谎言）；3. 分析瞬时控制效果；4. 引入Boltzmann型描述建立动力学系统；5. 对Boltzmann和Fokker-Planck方程进行分析和数值模拟。

Result: 1. 建立了说谎者引导舆论共识的理论框架；2. 分析了不同正则化约束下的控制策略；3. 展示了瞬时控制的效果；4. 建立了相应的动力学方程并进行了数值分析。

Conclusion: 通过引入说谎者并建模为最优控制问题，可以有效引导舆论达成目标共识。该框架为理解社交媒体操纵、政治宣传等现实问题提供了理论工具，正则化约束反映了社会惯例对说谎行为的限制。

Abstract: We establish an approach for consensus control of opinion dynamics by introducing a liar to the classical system. The liar's aim is to steer the population towards consensus at their goal opinion by showing 'apparent opinions', or 'lies', to members of the population. We analyse this as an optimal control problem for how best to lie to a population in order to guarantee the consensus that the liar desires. We consider a range of regularisations, each motivated by some social convention, such as the liar wanting to present an opinion close to their true opinion. For each regularisation, we demonstrate the effect of instantaneous controls. Furthermore, we introduce a Boltzmann-type description for the corresponding kinetic system and present analysis and numerical results for the resulting Boltzmann and Fokker-Planck equations.

</details>


### [102] [On Intensity of Preference Rank Reversal in the AHP](https://arxiv.org/abs/2512.11622)
*Jiri Mazurek,Luis Ángel Calvo*

Main category: math.OC

TL;DR: 本文发现AHP方法存在一种新的排序逆转现象——当偏好强度均匀增加时，即使所有成对偏好方向保持不变且同等增强，特征向量法仍可能导致排序变化，而几何平均法则对此具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管AHP是应用最广泛的多准则决策方法之一，但长期以来一直受到排序逆转问题的困扰。传统排序逆转研究主要关注无关或重复备选方案的增减，本文旨在揭示一种新的排序逆转类型。

Method: 通过理论分析证明，当偏好强度均匀增加时，特征向量法会产生排序逆转。使用几何平均法作为对比，证明其对这种强度偏好排序逆转具有鲁棒性。通过NASA月球门户轨道站能力优先排序的实际决策问题进行验证。

Result: 发现特征向量法在偏好强度均匀增加时会产生排序逆转，而几何平均法则不会。这一现象在NASA月球门户站的实际决策问题中得到了验证，表明几何平均法在避免此类排序逆转方面更具优势。

Conclusion: AHP方法存在一种新的强度偏好排序逆转问题，特征向量法对此敏感而几何平均法鲁棒。这一发现对决策实践具有重要意义，建议在需要避免此类排序逆转的决策场景中考虑使用几何平均法。

Abstract: The analytic hierarchy process (AHP) is one of the most widely used multicriteria decision-making methods, with applications from agriculture to space engineering. Despite its popularity, AHP has been repeatedly criticised for rank reversal, a phenomenon in which the ranking of alternatives changes after the addition or removal of an irrelevant or duplicate alternative.
  This paper introduces a new type of rank reversal in AHP, arising when the intensity of preferences is uniformly increased. We show that even when all pairwise preferences preserve their direction and are intensified identically, the eigenvector method may produce a different ordering of alternatives. In contrast, the geometric mean (GM) method is robust to this intensity-of-preference (IOP) rank reversal.
  The applicability of this result is shown through a real decision-making problem taken from a NASA manual concerning capability prioritisation for the planned lunar Gateway orbital station.

</details>


<div id='q-fin.MF'></div>

# q-fin.MF [[Back]](#toc)

### [103] [Transfer Learning (Il)liquidity](https://arxiv.org/abs/2512.11731)
*Andrea Conti,Giacomo Morelli*

Main category: q-fin.MF

TL;DR: 提出Deep Log-Sum-Exp神经网络架构，利用深度学习和迁移学习解决非流动性市场中风险中性密度估计问题，能在极端非流动性条件下仅用三个期权报价恢复RND。


<details>
  <summary>Details</summary>
Motivation: 在非流动性市场中，特别是当执行价格不规则且稀疏时，从期权价格中估计风险中性密度具有挑战性。现有方法在极端非流动性条件下表现不佳。

Method: 提出Deep Log-Sum-Exp神经网络架构，结合深度学习和迁移学习技术。该方法专门设计用于处理不规则和非流动性执行价格情况，并证明了模型的关键统计性质和估计量的一致性。

Result: 通过蒙特卡洛模拟展示了迁移学习在严重非流动性条件下改善RND估计的优势，并在SPX数据上进行了实证测试。与流行估计方法相比，该框架在极端非流动性条件下仅用三个期权报价就能恢复RND。

Conclusion: 提出的深度学习框架在非流动性市场中风险中性密度估计方面表现出色，特别是在数据极度稀缺的情况下，为金融衍生品定价和风险管理提供了有效工具。

Abstract: The estimation of the Risk Neutral Density (RND) implicit in option prices is challenging, especially in illiquid markets. We introduce the Deep Log-Sum-Exp Neural Network, an architecture that leverages Deep and Transfer learning to address RND estimation in the presence of irregular and illiquid strikes. We prove key statistical properties of the model and the consistency of the estimator. We illustrate the benefits of transfer learning to improve the estimation of the RND in severe illiquidity conditions through Monte Carlo simulations, and we test it empirically on SPX data, comparing it with popular estimation methods. Overall, our framework shows recovery of the RND in conditions of extreme illiquidity with as few as three option quotes.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [104] [STARK denoises spatial transcriptomics images via adaptive regularization](https://arxiv.org/abs/2512.10994)
*Sharvaj Kubal,Naomi Graham,Matthieu Heitz,Andrew Warren,Michael P. Friedlander,Yaniv Plan,Geoffrey Schiebinger*

Main category: stat.ML

TL;DR: STARK方法通过自适应图拉普拉斯正则化增强核岭回归，有效去噪空间转录组图像，在超低测序深度下揭示细胞身份并实现基因表达插值。


<details>
  <summary>Details</summary>
Motivation: 空间转录组学在超低测序深度下存在严重噪声问题，需要有效去噪方法来揭示细胞身份和实现基因表达插值。

Method: STARK方法结合核岭回归与自适应图拉普拉斯正则化，采用交替最小化方案：固定图更新图像，然后基于新图像更新图。通过改进的表示定理将无限维问题降为有限维。

Result: 方法优化块凸目标，子问题有闭式解，迭代收敛到平稳点。统计上以O(R^{-1/2})速率收敛到真实值。在实际数据中，去噪性能（标签转移准确率）优于竞争方法。

Conclusion: STARK在超低测序深度下有效去噪空间转录组图像，通过自适应图正则化提高鲁棒性，为细胞身份识别和基因表达插值提供可靠方法。

Abstract: We present an approach to denoising spatial transcriptomics images that is particularly effective for uncovering cell identities in the regime of ultra-low sequencing depths, and also allows for interpolation of gene expression. The method -- Spatial Transcriptomics via Adaptive Regularization and Kernels (STARK) -- augments kernel ridge regression with an incrementally adaptive graph Laplacian regularizer. In each iteration, we (1) perform kernel ridge regression with a fixed graph to update the image, and (2) update the graph based on the new image. The kernel ridge regression step involves reducing the infinite dimensional problem on a space of images to finite dimensions via a modified representer theorem. Starting with a purely spatial graph, and updating it as we improve our image makes the graph more robust to noise in low sequencing depth regimes. We show that the aforementioned approach optimizes a block-convex objective through an alternating minimization scheme wherein the sub-problems have closed form expressions that are easily computed. This perspective allows us to prove convergence of the iterates to a stationary point of this non-convex objective. Statistically, such stationary points converge to the ground truth with rate $\mathcal{O}(R^{-1/2})$ where $R$ is the number of reads. In numerical experiments on real spatial transcriptomics data, the denoising performance of STARK, evaluated in terms of label transfer accuracy, shows consistent improvement over the competing methods tested.

</details>


### [105] [An Efficient Variant of One-Class SVM with Lifelong Online Learning Guarantees](https://arxiv.org/abs/2512.11052)
*Joe Suk,Samory Kpotufe*

Main category: stat.ML

TL;DR: SONAR：一种用于非平稳流数据异常检测的高效SGD-based OCSVM求解器，具有强凸正则化和优越的理论保证


<details>
  <summary>Details</summary>
Motivation: 传统离线异常检测方法（如核OCSVM）在非平稳流数据场景下计算量大且容易产生大量假阴性错误，需要更高效且适应非平稳性的解决方案

Method: 提出SONAR算法：基于SGD的OCSVM求解器，采用强凸正则化；在对抗性非平稳数据中，将SONAR与集成方法和变点检测结合使用

Result: SONAR在Type I/II错误上展现出优于OCSVM的理论保证，在良性分布漂移下具有良好的终身学习性能，在对抗性非平稳数据中通过集成方法实现自适应保证

Conclusion: SONAR为单通道非平稳流数据异常检测提供了高效且理论保证优越的解决方案，在合成和真实数据集上验证了其有效性

Abstract: We study outlier (a.k.a., anomaly) detection for single-pass non-stationary streaming data. In the well-studied offline or batch outlier detection problem, traditional methods such as kernel One-Class SVM (OCSVM) are both computationally heavy and prone to large false-negative (Type II) errors under non-stationarity. To remedy this, we introduce SONAR, an efficient SGD-based OCSVM solver with strongly convex regularization. We show novel theoretical guarantees on the Type I/II errors of SONAR, superior to those known for OCSVM, and further prove that SONAR ensures favorable lifelong learning guarantees under benign distribution shifts. In the more challenging problem of adversarial non-stationary data, we show that SONAR can be used within an ensemble method and equipped with changepoint detection to achieve adaptive guarantees, ensuring small Type I/II errors on each phase of data. We validate our theoretical findings on synthetic and real-world datasets.

</details>


### [106] [Provable Recovery of Locally Important Signed Features and Interactions from Random Forest](https://arxiv.org/abs/2512.11081)
*Kata Vuk,Nicolas Alexander Ihlo,Merle Behr*

Main category: stat.ML

TL;DR: 提出一种新的局部特征与交互重要性方法，用于随机森林的个体预测解释，能识别决策路径中的特征共现模式，并在理论保证下恢复真实局部信号特征。


<details>
  <summary>Details</summary>
Motivation: 在个性化医疗等领域，需要针对个体预测的局部解释而非全局特征重要性。随机森林广泛用于这些场景，但现有方法对局部特征与交互重要性的理论理解有限，难以解释个体预测中的高重要性分数。

Method: 提出一种局部、模型特定的FII方法，识别决策路径中特征的频繁共现模式，结合全局模式和特定测试点路径观察到的模式。该方法在局部稀疏尖峰模型下具有理论保证。

Result: 理论证明该方法能一致地恢复真实局部信号特征及其交互作用，并能识别是大特征值还是小特征值驱动预测。通过模拟研究和真实世界数据示例验证了方法的有效性。

Conclusion: 该方法为随机森林的局部特征与交互重要性提供了理论依据和实用工具，解决了现有方法理论理解不足的问题，在个性化医疗等需要个体预测解释的领域具有重要应用价值。

Abstract: Feature and Interaction Importance (FII) methods are essential in supervised learning for assessing the relevance of input variables and their interactions in complex prediction models. In many domains, such as personalized medicine, local interpretations for individual predictions are often required, rather than global scores summarizing overall feature importance. Random Forests (RFs) are widely used in these settings, and existing interpretability methods typically exploit tree structures and split statistics to provide model-specific insights. However, theoretical understanding of local FII methods for RF remains limited, making it unclear how to interpret high importance scores for individual predictions. We propose a novel, local, model-specific FII method that identifies frequent co-occurrences of features along decision paths, combining global patterns with those observed on paths specific to a given test point. We prove that our method consistently recovers the true local signal features and their interactions under a Locally Spike Sparse (LSS) model and also identifies whether large or small feature values drive a prediction. We illustrate the usefulness of our method and theoretical results through simulation studies and a real-world data example.

</details>


### [107] [TPV: Parameter Perturbations Through the Lens of Test Prediction Variance](https://arxiv.org/abs/2512.11089)
*Devansh Arpit*

Main category: stat.ML

TL;DR: 本文提出测试预测方差(TPV)作为连接深度网络泛化现象的统一框架，TPV可仅从训练数据估计，并在不同数据集和架构中保持稳定，还能用于模型剪枝。


<details>
  <summary>Details</summary>
Motivation: 深度网络泛化现象存在多种经典观察，但缺乏统一的理论框架。本文旨在找到一个能够连接这些观察的通用量，并探索其在模型分析中的实际应用。

Method: 提出测试预测方差(TPV)概念，定义为模型输出对参数扰动的敏感性。TPV的迹形式将训练模型几何与特定扰动机制分离，允许分析SGD噪声、标签噪声、有限精度噪声等多种扰动。理论证明在过参数化极限下，训练集估计的TPV收敛到测试集值。

Result: TPV在不同数据集和架构（包括极窄网络）中表现出显著稳定性，并与干净测试损失良好相关。将剪枝建模为TPV扰动得到的无标签重要性度量，性能可与最先进的剪枝方法竞争。

Conclusion: TPV为深度网络泛化提供了统一的理论框架，能够仅从训练输入推断局部参数扰动下的预测方差，并在实际应用中展现出实用价值，特别是在模型剪枝方面。

Abstract: We identify test prediction variance (TPV) -- the first-order sensitivity of model outputs to parameter perturbations around a trained solution -- as a unifying quantity that links several classical observations about generalization in deep networks. TPV is a fully label-free object whose trace form separates the geometry of the trained model from the specific perturbation mechanism, allowing a broad family of parameter perturbations like SGD noise, label noise, finite-precision noise, and other post-training perturbations to be analyzed under a single framework. Theoretically, we show that TPV estimated on the training set converges to its test-set value in the overparameterized limit, providing the first result that prediction variance under local parameter perturbations can be inferred from training inputs alone. Empirically, TPV exhibits a striking stability across datasets and architectures -- including extremely narrow networks -- and correlates well with clean test loss. Finally, we demonstrate that modeling pruning as a TPV perturbation yields a simple label-free importance measure that performs competitively with state-of-the-art pruning methods, illustrating the practical utility of TPV. Code available at github.com/devansharpit/TPV.

</details>


### [108] [Data-Driven Model Reduction using WeldNet: Windowed Encoders for Learning Dynamics](https://arxiv.org/abs/2512.11090)
*Biraj Dahal,Jiahui Cheng,Hao Liu,Rongjie Lai,Wenjing Liao*

Main category: stat.ML

TL;DR: WeldNet：一种基于窗口化编码器的非线性模型降阶框架，通过分窗处理将长时程动力学分解为多个短片段，结合自动编码器、传播网络和转码器来构建复杂演化系统的低维代理模型。


<details>
  <summary>Details</summary>
Motivation: 科学和工程中的许多问题涉及复杂物理过程产生的高维时变数据集，这些模拟计算成本高昂。需要一种数据驱动的非线性模型降阶方法来构建复杂演化系统的低维代理模型。

Method: 将时域划分为多个重叠窗口，在每个窗口内使用自动编码器进行非线性降维以捕捉潜在编码。然后训练传播网络捕捉每个窗口内潜在编码的演化，训练转码器连接相邻窗口间的潜在编码。

Result: 在各种微分方程上的数值实验表明，WeldNet能够捕捉非线性潜在结构及其底层动力学，性能优于传统的基于投影的方法和最近开发的非线性模型降阶方法。

Conclusion: WeldNet通过窗口化分解显著简化了传播网络训练，将长时程动力学分解为多个短而可管理的片段，同时转码器确保了窗口间的一致性。该框架在流形假设下具有理论保证。

Abstract: Many problems in science and engineering involve time-dependent, high dimensional datasets arising from complex physical processes, which are costly to simulate. In this work, we propose WeldNet: Windowed Encoders for Learning Dynamics, a data-driven nonlinear model reduction framework to build a low-dimensional surrogate model for complex evolution systems. Given time-dependent training data, we split the time domain into multiple overlapping windows, within which nonlinear dimension reduction is performed by auto-encoders to capture latent codes. Once a low-dimensional representation of the data is learned, a propagator network is trained to capture the evolution of the latent codes in each window, and a transcoder is trained to connect the latent codes between adjacent windows. The proposed windowed decomposition significantly simplifies propagator training by breaking long-horizon dynamics into multiple short, manageable segments, while the transcoders ensure consistency across windows. In addition to the algorithmic framework, we develop a mathematical theory establishing the representation power of WeldNet under the manifold hypothesis, justifying the success of nonlinear model reduction via deep autoencoder-based architectures. Our numerical experiments on various differential equations indicate that WeldNet can capture nonlinear latent structures and their underlying dynamics, outperforming both traditional projection-based approaches and recently developed nonlinear model reduction methods.

</details>


### [109] [Conditional Coverage Diagnostics for Conformal Prediction](https://arxiv.org/abs/2512.11779)
*Sacha Braun,David Holzmüller,Michael I. Jordan,Francis Bach*

Main category: stat.ML

TL;DR: 提出ERT（超额风险）指标，通过分类问题评估条件覆盖率，比现有方法更高效、统计功效更强


<details>
  <summary>Details</summary>
Motivation: 条件覆盖率评估是预测系统可靠性评估中最持久的挑战之一。现有方法无法保证条件覆盖率，且存在样本效率低和过拟合问题，缺乏解释局部偏差的清晰方法

Method: 将条件覆盖率估计转化为分类问题：条件覆盖率被违反当且仅当任何分类器能获得低于目标覆盖率的风险。通过选择适当的损失函数，风险差异给出自然误覆盖率度量的保守估计

Result: 实验表明，使用现代分类器比简单分类器（如CovGap）提供更高的统计功效。ERT可用于基准测试不同的保形预测方法，并开源了相关软件包

Conclusion: ERT为理解、诊断和改进预测系统的条件可靠性提供了新的视角和工具

Abstract: Evaluating conditional coverage remains one of the most persistent challenges in assessing the reliability of predictive systems. Although conformal methods can give guarantees on marginal coverage, no method can guarantee to produce sets with correct conditional coverage, leaving practitioners without a clear way to interpret local deviations. To overcome sample-inefficiency and overfitting issues of existing metrics, we cast conditional coverage estimation as a classification problem. Conditional coverage is violated if and only if any classifier can achieve lower risk than the target coverage. Through the choice of a (proper) loss function, the resulting risk difference gives a conservative estimate of natural miscoverage measures such as L1 and L2 distance, and can even separate the effects of over- and under-coverage, and non-constant target coverages. We call the resulting family of metrics excess risk of the target coverage (ERT). We show experimentally that the use of modern classifiers provides much higher statistical power than simple classifiers underlying established metrics like CovGap. Additionally, we use our metric to benchmark different conformal prediction methods. Finally, we release an open-source package for ERT as well as previous conditional coverage metrics. Together, these contributions provide a new lens for understanding, diagnosing, and improving the conditional reliability of predictive systems.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [110] [Robust Two-Sample Mean Inference under Serial Dependence](https://arxiv.org/abs/2512.11259)
*Ulrich Hounyo,Min Seong Kim*

Main category: econ.EM

TL;DR: 提出针对时间序列均值比较的稳健两样本检验框架，适用于结构断点、处理-控制比较和面板数据等场景，包含基于正交基投影的系列HAR t检验和wild bootstrap方法。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列均值比较方法在处理异质性、非参数依赖结构和长程方差异质性时存在局限，需要开发更稳健的检验框架以适应结构断点、处理-控制比较和面板数据等多种应用场景。

Method: 1) 系列HAR两样本t检验：使用正交基投影进行标准化，确保在异质性和非参数依赖结构下的有效推断；2) Welch型t近似：调整自由度以考虑序列间的长程方差异质性；3) 系列HAR wild bootstrap检验：将传统wild bootstrap扩展到时间序列设置，避免重采样观测块。

Result: 提出的方法在异质性和非参数依赖结构下保持有效推断，wild bootstrap方法在有限样本中表现出优越性能，避免了传统块重采样的缺点。

Conclusion: 该研究为时间序列均值比较提供了稳健的检验框架，能够处理多种实际应用场景，特别是wild bootstrap方法在有限样本中表现优异，为时间序列分析提供了新的工具。

Abstract: We propose robust two-sample tests for comparing means in time series. The framework accommodates a wide range of applications, including structural breaks, treatment-control comparisons, and group-averaged panel data. We first consider series HAR two-sample t-tests, where standardization employs orthonormal basis projections, ensuring valid inference under heterogeneity and nonparametric dependence structures. We propose a Welch-type t-approximation with adjusted degrees of freedom to account for long-run variance heterogeneity across the series. We further develop a series-based HAR wild bootstrap test, extending traditional wild bootstrap methods to the time-series setting. Our bootstrap avoids resampling blocks of observations and delivers superior finite-sample performance.

</details>


### [111] [Testing Parametric Distribution Family Assumptions via Differences in Differential Entropy](https://arxiv.org/abs/2512.11305)
*Ron Mittelhammer,George Judge,Miguel Henry*

Main category: econ.EM

TL;DR: 提出了一种名为DDE检验的统计方法，用于测试数据样本来自哪个参数分布族，基于信息论中的微分熵差异进行模型拟合评估。


<details>
  <summary>Details</summary>
Motivation: 需要一种广泛适用的统计方法来检验数据样本来自哪个参数分布族，传统方法往往针对特定分布或需要复杂的调整参数。

Method: DDE检验比较零假设下基于MLE的微分熵估计与非参数自助核密度估计的差异，使用信息论度量作为模型拟合指标，结合最大似然估计、自助法和核密度估计原理。

Result: 该方法提供了一个统一框架，适用于广泛的分布族，具有渐近有效性，实现简单、计算高效，无需调整参数或特殊正则条件。

Conclusion: DDE检验是一种实用且理论基础的分布族检验方法，为统计建模中的分布选择问题提供了有效的解决方案。

Abstract: We introduce a broadly applicable statistical procedure for testing which parametric distribution family generated a random sample of data. The method, termed the Difference in Differential Entropy (DDE) test, provides a unified framework applicable to a wide range of distributional families, with asymptotic validity grounded in established maximum likelihood, bootstrap, and kernel density estimation principles. The test is straightforward to implement, computationally efficient, and requires no tuning parameters or specialized regularity conditions. It compares an MLE-based estimate of differential entropy under the null hypothesis with a nonparametric bootstrapped kernel density estimate, using their divergence as an information-theoretic measure of model fit.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [112] [Mitigating Dynamic Tip-Over during Mobile Crane Slewing using Input Shaping](https://arxiv.org/abs/2512.11228)
*Navneet Kaur,Christopher J. Adams,William E. Singhose,Santosh Devasia*

Main category: eess.SY

TL;DR: 该论文提出使用输入整形技术来减少移动起重机快速回转时的载荷摆动，从而提高安全性并提升生产率。


<details>
  <summary>Details</summary>
Motivation: 移动起重机快速回转时产生的载荷摆动会引发倾覆力矩，导致翻车事故风险。目前操作员只能通过降低回转速度或减少载荷来降低风险，但这两种方法都会降低生产率。

Method: 应用输入整形技术对操作员生成的起重机回转指令进行处理。该方法仅需要绳索长度信息，无需详细的起重机动力学模型。

Result: 仿真和实验表明，该方法显著减少了残余载荷摆动，使回转速度提高至少38%，无输入整形时的大幅摆动导致翻车，而整形控制则避免了翻车。人工控制结合输入整形使任务完成时间减少13%，峰值摆动降低18%，碰撞可能性降低82%。

Conclusion: 输入整形方法能够在保证安全的前提下实现快速回转，显著恢复了移动起重机在动态条件下的操作安全边界，提高了生产率。

Abstract: Payload swing during rapid slewing of mobile cranes poses a safety risk, as it generates overturning moments that can lead to tip-over accidents of mobile cranes. Currently, to limit the risk of tip-over, mobile crane operators are forced to either reduce the slewing speed (which lowers productivity) or reduce the load being carried to reduce the induced moments. Both of these approaches reduce productivity. This paper seeks to enable rapid slewing without compromising safety by applying input shaping to the crane-slewing commands generated by the operator. A key advantage of this approach is that the input shaper requires only the information about the rope length, and does not require detailed mobile crane dynamics. Simulations and experiments show that the proposed method reduces residual payload swing and enables significantly higher slewing speeds without tip over, reducing slewing completion time by at least 38% compared to unshaped control. Human control with input shaping improves task completion time by 13%, reduces the peak swing by 18%, and reduces the potential of collisions by 82% when compared to unshaped control. Moreover, shaped control with a human had no tip-over, whereas large swing led to tip-over without input shaping. Thereby, the proposed method substantially recovers the operational-safety envelope of mobile cranes (designed to avoid tip-over using static analysis) that would otherwise be lost in dynamic conditions. Videos and demonstrations are available at https://youtu.be/dVy3bbIhrBU.

</details>


### [113] [Model Reduction of Multicellular Communication Systems via Singular Perturbation: Sender Receiver Systems](https://arxiv.org/abs/2512.11244)
*Taishi Kotsuka,Enoch Yeung*

Main category: eess.SY

TL;DR: 该研究将嵌入水凝胶珠中的多细胞发送-接收系统简化为有限维多智能体网络模型，通过球形格林函数推导出封闭形式的通信矩阵，实现了大规模细胞群体的可扩展模拟。


<details>
  <summary>Details</summary>
Motivation: 研究多细胞发送-接收系统在水凝胶珠中的行为，这类系统由PDE-ODE耦合模型描述，结合了三维扩散和非线性细胞内动力学，分析和模拟都很困难，需要找到简化方法。

Method: 利用奇异摄动理论将模型简化为有限维多智能体网络，通过球形格林函数推导出封闭形式的通信矩阵来捕捉有效的发送-接收耦合关系。

Result: 数值结果表明简化模型与完整动力学高度匹配，同时能够实现大规模细胞群体的可扩展模拟，扩散动力学指数收敛到准稳态空间分布。

Conclusion: 该方法成功将复杂的三维扩散耦合系统简化为可处理的网络模型，为研究大规模多细胞通信系统提供了有效的计算框架。

Abstract: We investigate multicellular sender receiver systems embedded in hydrogel beads, where diffusible signals mediate interactions among heterogeneous cells. Such systems are modeled by PDE ODE couplings that combine three dimensional diffusion with nonlinear intracellular dynamics, making analysis and simulation challenging. We show that the diffusion dynamics converges exponentially to a quasi steady spatial profile and use singular perturbation theory to reduce the model to a finite dimensional multiagent network. A closed form communication matrix derived from the spherical Green's function captures the effective sender receiver coupling. Numerical results show the reduced model closely matches the full dynamics while enabling scalable simulation of large cell populations.

</details>


### [114] [Gig-work Management System with Chance-Constraints Verification Algorithm](https://arxiv.org/abs/2512.11308)
*Kazuyoshi Fukuda,Masaki Inoue,Riko Asanaka*

Main category: eess.SY

TL;DR: 提出一个高效的零工工作管理系统框架，通过建模零工决策过程，优化任务时长和工资，使用机会约束模型预测控制方法求解，并基于众包数据开发零工模型。


<details>
  <summary>Details</summary>
Motivation: 零工经济中需要有效的管理系统来为工作者推荐一次性任务，优化任务时长和工资分配，提高管理效率和工作匹配度。

Method: 1. 建立零工工作者决策模型；2. 基于模型构建优化问题确定最优任务时长和工资；3. 将问题转化为机会约束模型预测控制问题；4. 开发具有置信度保证的近似求解算法；5. 基于众包数据开发零工模型。

Result: 提出了一个完整的零工工作管理系统框架，包括决策模型、优化问题公式化、求解算法以及基于实际数据的模型开发方法。

Conclusion: 该研究为零工工作管理提供了系统化的解决方案，通过建模优化和算法设计，能够有效管理零工任务分配，提高系统效率。

Abstract: This paper proposes the framework of an efficient gig-work management system. A gig-work management system recommends one-off tasks with information about task hours and wages to gig-workers. To enable effective management, this paper develops a model of gig-workers' decision-making. Then, based on the model, we formulate an optimization problem to determine the optimal task hours and wages. The formulated problem belongs to the class of chance-constrained model predictive control (CC-MPC) problems. To efficiently solve the CC-MPC problem, we develop an approximate solution algorithm with guaranteed confidence levels. Finally, we develop gig-worker models based on data collected through crowdsourcing.

</details>


### [115] [Controlled Evolution-Based Day-Ahead Robust Dispatch Considering Frequency Security with Frequency Regulation Loads and Curtailable Loads](https://arxiv.org/abs/2512.11333)
*Kai Kang,Xiaoyu Peng,Kui Luo,Xi Ru,Feng Liu*

Main category: eess.SY

TL;DR: 提出基于控制演化的日前鲁棒调度方法，解决可再生能源波动带来的频率调节挑战，通过凸松弛处理最大频率偏差约束，确保日前决策与日内策略一致性。


<details>
  <summary>Details</summary>
Motivation: 可再生能源大规模并网导致电力系统面临瞬时功率波动，给一次调频带来挑战。最大频率偏差约束本质非凸，传统两阶段调度方法忽略因果关系，可能导致日前决策不可行。

Method: 1) 采用凸松弛技术将最大频率偏差约束转化为可优化形式；2) 提出基于演化的鲁棒调度框架，协调日前决策与日内策略；3) 开发新型基于控制演化的算法高效求解该框架。

Result: 在改进的IEEE 14节点系统上进行案例研究，验证了所提方法在提升频率安全性和系统可靠性方面的优越性。

Conclusion: 该方法能有效应对可再生能源波动带来的频率调节挑战，确保电力系统频率安全和供电可靠性，为含高比例可再生能源的电力系统调度提供了有效解决方案。

Abstract: With the extensive integration of volatile and uncertain renewable energy, power systems face significant challenges in primary frequency regulation due to instantaneous power fluctuations. However, the maximum frequency deviation constraint is inherently non-convex, and commonly used two-stage dispatch methods overlook causality, potentially resulting in infeasible day-ahead decisions. This paper presents a controlled evolution-based day-ahead robust dispatch method to address these issues. First, we suggest the convex relaxation technique to transform the maximum frequency deviation constraint to facilitate optimization. Then, an evolution-based robust dispatch framework is introduced to align day-ahead decisions with intraday strategies, ensuring both frequency security and power supply reliability. Additionally, a novel controlled evolution-based algorithm is developed to solve this framework efficiently. Case studies on a modified IEEE 14-bus system demonstrate the superiority of the proposed method in enhancing frequency security and system reliability.

</details>


### [116] [An Input-Output Data-Driven Dissipativity Approach for Compositional Stability Certification of Interconnected LTI MIMO Systems](https://arxiv.org/abs/2512.11468)
*Alejandra Sandoval-Carranza,Juan E. Machado,Johannes Schiffer*

Main category: eess.SY

TL;DR: 提出基于输入输出数据的框架，通过QSR耗散性验证互联MIMO线性时不变离散时间系统的稳定性，无需显式模型辨识


<details>
  <summary>Details</summary>
Motivation: 传统稳定性分析需要精确的系统模型，但实际中获取精确模型困难。本文旨在直接从测量数据验证系统稳定性，避免模型辨识的复杂性和误差

Method: 使用子系统测量输入输出轨迹验证耗散特性，提取局部无源性指数；利用这些指数推导互联系统平衡点稳定的条件；通过组合子系统级条件形成整体系统稳定性判据

Result: 框架能够识别子系统间无源性不足与盈余的补偿关系；通过数值案例研究展示了如何从数据计算通道级无源性指数并推断稳定性保证

Conclusion: 提出的数据驱动框架实现了互联系统的组合稳定性分析，直接从测量数据验证稳定性，为无精确模型的复杂系统分析提供了有效方法

Abstract: We propose an input-output data-driven framework for certifying the stability of interconnected multiple-input-multiple-output linear time-invariant discrete-time systems via QSR-dissipativity. That is, by using measured input-output trajectories of each subsystem, we verify dissipative properties and extract local passivity indices without requiring an explicit model identification.These passivity indices are then used to derive conditions under which the equilibrium of the interconnected system is stable. In particular, the framework identifies how the lack of passivity in some subsystems can be compensated by surpluses in others. The proposed approach enables a compositional stability analysis by combining subsystem-level conditions into a criterion valid for the overall interconnected system. We illustrate via a numerical case study, how to compute channel-wise passivity indices and infer stability guarantees directly from data with the proposed method.

</details>


### [117] [A Robust Model Predictive Control Method for Networked Control Systems](https://arxiv.org/abs/2512.11481)
*Severin Beger,Sandra Hirche*

Main category: eess.SY

TL;DR: 提出一种预测一致性方法处理网络控制系统中的延迟和丢包问题，结合线性管MPC实现鲁棒收敛


<details>
  <summary>Details</summary>
Motivation: 网络控制系统中的延迟和丢包问题严重影响远程控制动态系统的性能，需要鲁棒的补偿方法

Method: 提出预测一致性方法处理UDP型通信系统中的延迟和丢包，结合线性管模型预测控制

Result: 增强的控制系统在网络约束下保留了原始模型预测控制的所有特性，系统在温和条件下鲁棒收敛到原点

Conclusion: 所提方法能有效处理网络约束，通过小车摆杆和连续搅拌釜反应器的仿真验证了方法的有效性

Abstract: Robustly compensating network constraints such as delays and packet dropouts in networked control systems is crucial for remotely controlling dynamical systems. This work proposes a novel prediction consistent method to cope with delays and packet losses as encountered in UDP-type communication systems. The augmented control system preserves all properties of the original model predictive control method under the network constraints. Furthermore, we propose to use linear tube MPC with the novel method and show that the system converges robustly to the origin under mild conditions. We illustrate this with simulation examples of a cart pole and a continuous stirred tank reactor.

</details>


### [118] [Optimal Delay Compensation in Networked Predictive Control](https://arxiv.org/abs/2512.11492)
*Severin Beger,Yihui Lin,Katarina Stanojevic,Sandra Hirche*

Main category: eess.SY

TL;DR: 提出一种系统化方法选择网络预测控制中的最优时延边界，通过量化预测误差与通信丢失导致的开环操作之间的权衡


<details>
  <summary>Details</summary>
Motivation: 网络预测控制广泛用于缓解网络控制系统中的时延和丢包影响，特别是当这些超过采样时间时。关键设计选择是时延边界，它决定了预测范围和对信息丢失的鲁棒性。需要系统化方法选择最优边界。

Method: 开发了一种系统化方法，通过量化预测误差和通信丢失导致的开环操作之间的权衡来选择最优时延边界。该方法考虑了预测精度与系统在通信中断时运行在开环模式之间的平衡。

Result: 仿真研究证明了使用最优边界获得的性能提升。通过优化时延边界选择，系统在网络控制中获得了更好的性能表现。

Conclusion: 提出了一种系统化选择网络预测控制时延边界的方法，通过权衡预测误差和开环操作来优化系统性能，仿真验证了该方法能带来显著的性能提升。

Abstract: Networked Predictive Control is widely used to mitigate the effect of delays and dropouts in Networked Control Systems, particularly when these exceed the sampling time. A key design choice of these methods is the delay bound, which determines the prediction horizon and the robustness to information loss. This work develops a systematic method to select the optimal bound by quantifying the trade-off between prediction errors and open-loop operation caused by communication losses. Simulation studies demonstrate the performance gains achieved with the optimal bound.

</details>


### [119] [Shared Situational Awareness Using Hybrid Zonotopes with Confidence Metric](https://arxiv.org/abs/2512.11493)
*Vandana Narri,Jonah J. Glunt,Joshua A. Robbins,Jonas Mårtensson,Herschel C. Pangborn,Karl H. Johansson*

Main category: eess.SY

TL;DR: 提出一种基于约束zonotopes的集合估计方法，通过计算传感器测量集的置信度，使用混合zonotopes融合不一致的感知数据，提高自动驾驶车辆对遮挡行人的情境感知能力。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆需要感知周围行人行为，但行人可能被车辆或基础设施遮挡，造成安全隐患。车联网通信允许共享感知数据，但测量数据可能因传感器噪声、误报或通信问题而与真实位置不一致，需要可靠的融合方法。

Method: 采用基于集合的估计方法，使用约束zonotopes计算每个传感器测量集的置信度指标，然后使用混合zonotopes融合这些集合及其置信度，能够处理不一致的测量数据。

Result: 该方法在仿真和真实实验中证明了有效性，能够可靠且鲁棒地融合传感器数据，提高对遮挡行人的感知能力。

Conclusion: 提出的基于zonotopes的集合估计和融合方法能够有效处理感知数据的不一致性，为自动驾驶车辆提供更全面可靠的情境感知能力。

Abstract: Situational awareness for connected and automated vehicles describes the ability to perceive and predict the behavior of other road-users in the near surroundings. However, pedestrians can become occluded by vehicles or infrastructure, creating significant safety risks due to limited visibility. Vehicle-to-everything communication enables the sharing of perception data between connected road-users, allowing for a more comprehensive awareness. The main challenge is how to fuse perception data when measurements are inconsistent with the true locations of pedestrians. Inconsistent measurements can occur due to sensor noise, false positives, or communication issues. This paper employs set-based estimation with constrained zonotopes to compute a confidence metric for the measurement set from each sensor. These sets and their confidences are then fused using hybrid zonotopes. This method can account for inconsistent measurements, enabling reliable and robust fusion of the sensor data. The effectiveness of the proposed method is demonstrated in both simulation and real experiments.

</details>


### [120] [Data-driven control-oriented modelling for MPC-based control of urban drainage systems](https://arxiv.org/abs/2512.11531)
*Luis Romero-Ben,Bernat Joseph-Duran,David Sunyer,Gabriela Cembrano,Jordi Meseguer,Vicenç Puig,Alejandro Carrasco*

Main category: eess.SY

TL;DR: 提出了一种用于城市排水系统的数据驱动、控制导向的建模方法，结合数据、专家知识和参数拟合技术，在MPC控制中表现出高精度和性能提升


<details>
  <summary>Details</summary>
Motivation: 需要为城市排水系统开发有效的控制导向建模方法，以优化系统性能，提高处理设施利用率并减少排放

Method: 基于三个关键组件的数据驱动建模框架：输入输出数据、定义模型结构的专家知识、获取最优参数的数据拟合技术

Result: 在马德里城市排水系统基准测试中显示出高模型精度，在模型预测控制方案中减少了排放并提高了处理设施利用率

Conclusion: 提出的数据驱动建模方法有效提升了城市排水系统的控制性能，为实际应用提供了可行的解决方案

Abstract: This article presents a data-driven, control-oriented modelling methodology for urban drainage systems (UDS). The proposed framework requires three main key components: input-output data from the element to be modelled, expert knowledge to define the model structure, and data-fitting techniques to obtain optimal parameters. The methodology is evaluated using a realistic benchmark from an UDS in Madrid, Spain. The results show high model accuracy and improved performance within a MPC scheme, reducing discharge and increasing treatment facilities utilization.

</details>


### [121] [A Modeling and Optimization Framework for Fostering Modal Shift through the Integration of Tradable Credits and Demand-Responsive Autonomous Shuttles](https://arxiv.org/abs/2512.11607)
*Zenghao Hou,Ludovic Leclercq*

Main category: eess.SY

TL;DR: 论文提出一个结合可交易信用方案（TCS）和需求响应式自动驾驶班车（DRAS）的双层优化框架，通过动态多模式均衡模型考虑运营约束和等待时间，以更准确地评估交通系统性能。


<details>
  <summary>Details</summary>
Motivation: 现有TCS研究通常假设公共交通容量无限或共享模式占用率固定，忽略了等待时间和运营约束，这会高估系统性能，特别是在公共交通供应不足时。同时，大多数研究假设固定交通供给，忽视了需求变化引发的供给侧响应。

Method: 1. 开发动态多模式均衡模型，整合出行者模式选择、信用交易、交通动态和等待时间（取决于车队规模和容量等运营特征）。2. 提出双层优化框架，结合均衡模型，联合优化TCS设计和DRAS运营策略。3. 将框架应用于法国巴黎A10高速公路路段。

Result: 数值结果表明：1. 在多模式均衡中建模运营特征的重要性；2. 在TCS政策中纳入灵活供给对于降低总体广义成本具有显著效益；3. 联合实施TCS和DRAS能够有效缓解交通拥堵并提高系统效率。

Conclusion: 论文强调了在可交易信用方案中考虑运营约束和等待时间的必要性，并展示了通过需求响应式自动驾驶班车实现灵活供给的协同效益。联合优化TCS设计和DRAS运营策略能够更有效地管理交通需求与供给的互动。

Abstract: Tradable Credit Schemes (TCS) promote the use of public and shared transport by capping private car usage while maintaining fair welfare outcomes by allowing credit trading. However, most existing studies assume unlimited public transit capacity or a fixed occupancy of shared modes, often neglecting waiting time and oversimplifying time-based costs by depending solely on in-vehicle travel time. These assumptions can overstate the system's performance with TCS regulation, especially when there are insufficient public or shared transport supplies.
  To address this, we develop a dynamic multimodal equilibrium model to capture operation constraints and induced waiting times under TCS regulation. The model integrates travelers' mode choices, credit trading, traffic dynamics, and waiting time, which depend on key operational features of service vehicles such as fleet size and capacity.
  Besides, most TCS studies assume fixed transport supply, overlooking supply-side responses triggered by demand shifts. Therefore, we further propose integrating adaptive supply management through the deployment of Demand-Responsive Autonomous Shuttles (DRAS) and developing a bi-level optimization framework that incorporates the equilibrium model to jointly optimize TCS design and operational strategies for the DRAS.
  We apply the framework to a section of the A10 highway near Paris, France, to examine demand-supply interactions and assess the potential benefits of jointly implementing TCS and DRAS. Numerical results demonstrate the importance of modeling operational features within multimodal equilibrium and incorporating flexible supply in TCS policies for mitigating overall generalized cost.

</details>


### [122] [Two-dimensional Decompositions of High-dimensional Configurations for Efficient Multi-vehicle Coordination at Intelligent Intersections](https://arxiv.org/abs/2512.11713)
*Amirreza Akbari,Johan Thunberg*

Main category: eess.SY

TL;DR: 提出一种用于多车复杂交通场景的高效轨迹规划方法，将高维配置空间问题分解为2D图搜索问题，显著降低计算复杂度


<details>
  <summary>Details</summary>
Motivation: 解决共享空间（如智能交叉口）中多车复杂交通场景的安全协调和轨迹规划问题，传统方法计算复杂度高，难以实时应用

Method: 1) 将约束最小时间轨迹规划问题重构为高维配置空间问题，冲突区建模为二维矩形构建的高维多面体；2) 提出两种近优局部优化算法，将高维问题分解为一系列2D图搜索问题；3) 将生成轨迹整合到非线性模型预测控制(NMPC)框架中

Result: 该方法在数值评估中显著优于现有的基于混合整数线性规划(MILP)的时间调度方法，无论是在目标值还是计算时间方面都有明显优势

Conclusion: 提出的方法能够高效生成多车复杂交通场景下的无碰撞轨迹，计算复杂度低，适合实时应用，为智能交叉口等共享空间的安全协调提供了可行解决方案

Abstract: For multi-vehicle complex traffic scenarios in shared spaces such as intelligent intersections, safe coordination and trajectory planning is challenging due to computational complexity. To meet this challenge, we introduce a computationally efficient method for generating collision-free trajectories along predefined vehicle paths. We reformulate a constrained minimum-time trajectory planning problem as a problem in a high-dimensional configuration space, where conflict zones are modeled by high-dimensional polyhedra constructed from two-dimensional rectangles. Still, in such a formulation, as the number of vehicles involved increases, the computational complexity increases significantly. To address this, we propose two algorithms for near-optimal local optimization that significantly reduce the computational complexity by decomposing the high-dimensional problem into a sequence of 2D graph search problems. The resulting trajectories are then incorporated into a Nonlinear Model Predictive Control (NMPC) framework to ensure safe and smooth vehicle motion. We furthermore show in numerical evaluation that this approach significantly outperforms existing MILP-based time-scheduling; both in terms of objective-value and computational time.

</details>


### [123] [Model Error Resonance: The Geometric Nature of Error Dynamics](https://arxiv.org/abs/2512.11734)
*Yuntao Dai*

Main category: eess.SY

TL;DR: 论文提出了一种模型误差的几何理论，将真实和模型动力学视为光滑流形上不同仿射联络生成的测地线流，并推导出由曲率控制的误差动力学结构。


<details>
  <summary>Details</summary>
Motivation: 传统模型误差分析缺乏几何结构视角，需要建立统一的几何框架来理解模型误差的内在动力学结构，特别是曲率对误差演化的影响。

Method: 将真实和模型动力学表述为流形上的测地线流，通过不同仿射联络的差异定义潜在误差动态响应(LEDR)，推导出满足Jacobi型方程的误差动力学，并扩展到离散时间系统。

Result: 证明了LEDR满足Jacobi型方程，曲率不匹配作为显式强迫项；在平坦模型联络情况下，LEDR简化为真实流形上的经典Jacobi场，在正截面曲率下出现模型误差共振(MER)；几何结构和共振行为在采样系统中持续存在。

Conclusion: 该框架为结构化误差动力学提供了统一的几何解释，并为曲率感知的模型验证提供了基础工具，通过球面-平面示例表明曲率可直接从LEDR演化中推断。

Abstract: This paper introduces a geometric theory of model error, treating true and model dynamics as geodesic flows generated by distinct affine connections on a smooth manifold. When these connections differ, the resulting trajectory discrepancy--termed the Latent Error Dynamic Response (LEDR)--acquires an intrinsic dynamical structure governed by curvature. We show that the LEDR satisfies a Jacobi-type equation, where curvature mismatch acts as an explicit forcing term. In the important case of a flat model connection, the LEDR reduces to a classical Jacobi field on the true manifold, causing Model Error Resonance (MER) to emerge under positive sectional curvature. The theory is extended to a discrete-time analogue, establishing that this geometric structure and its resonant behavior persist in sampled systems. A closed-form analysis of a sphere--plane example demonstrates that curvature can be inferred directly from the LEDR evolution. This framework provides a unified geometric interpretation of structured error dynamics and offers foundational tools for curvature-informed model validation.

</details>


### [124] [LUCID: Learning-Enabled Uncertainty-Aware Certification of Stochastic Dynamical Systems](https://arxiv.org/abs/2512.11750)
*Ernesto Casablanca,Oliver Schön,Paolo Zuliani,Sadegh Soudjani*

Main category: eess.SY

TL;DR: LUCID是首个能够为黑盒随机动力系统提供量化安全保证的验证工具，通过数据驱动的控制屏障证书和RKHS嵌入方法，将半无限非凸优化问题转化为可处理的线性规划。


<details>
  <summary>Details</summary>
Motivation: 传统形式化验证工具在处理包含不透明黑盒AI组件和复杂随机动态的系统时存在不足，特别是在自动驾驶和医疗等高风险领域，确保AI系统的安全性变得至关重要。

Method: 采用数据驱动的控制屏障证书方法，使用条件均值嵌入将数据映射到再生核希尔伯特空间，构建RKHS模糊集以增强对分布外行为的鲁棒性。关键创新是使用有限傅里叶核展开将半无限非凸优化问题转化为可处理的线性规划问题。

Result: LUCID是首个能够为黑盒随机动力系统建立量化安全保证的工具，通过模块化架构和广泛文档实现易扩展性，并在具有挑战性的基准测试中展示了其独特能力。

Conclusion: LUCID提供了一个鲁棒且高效的验证框架，能够处理现代黑盒系统的复杂性，同时提供形式化的安全保证，解决了传统验证工具在处理包含AI组件和随机动态的系统时的局限性。

Abstract: Ensuring the safety of AI-enabled systems, particularly in high-stakes domains such as autonomous driving and healthcare, has become increasingly critical. Traditional formal verification tools fall short when faced with systems that embed both opaque, black-box AI components and complex stochastic dynamics. To address these challenges, we introduce LUCID (Learning-enabled Uncertainty-aware Certification of stochastIc Dynamical systems), a verification engine for certifying safety of black-box stochastic dynamical systems from a finite dataset of random state transitions. As such, LUCID is the first known tool capable of establishing quantified safety guarantees for such systems. Thanks to its modular architecture and extensive documentation, LUCID is designed for easy extensibility. LUCID employs a data-driven methodology rooted in control barrier certificates, which are learned directly from system transition data, to ensure formal safety guarantees. We use conditional mean embeddings to embed data into a reproducing kernel Hilbert space (RKHS), where an RKHS ambiguity set is constructed that can be inflated to robustify the result to out-of-distribution behavior. A key innovation within LUCID is its use of a finite Fourier kernel expansion to reformulate a semi-infinite non-convex optimization problem into a tractable linear program. The resulting spectral barrier allows us to leverage the fast Fourier transform to generate the relaxed problem efficiently, offering a scalable yet distributionally robust framework for verifying safety. LUCID thus offers a robust and efficient verification framework, able to handle the complexities of modern black-box systems while providing formal guarantees of safety. These unique capabilities are demonstrated on challenging benchmarks.

</details>


### [125] [Toward a Decision Support System for Energy-Efficient Ferry Operation on Lake Constance based on Optimal Control](https://arxiv.org/abs/2512.11786)
*Hannes Homburger,Bastian Jäckl,Stefan Wirtensohn,Christian Stopp,Maximilian T. Fischer,Moritz Diehl,Daniel A. Keim,Johannes Reuter*

Main category: eess.SY

TL;DR: 开发基于收缩时域最优控制框架的渡轮决策支持系统，整合船舶动力学和环境扰动模型，通过实时指导提升内河渡轮运营效率


<details>
  <summary>Details</summary>
Motivation: 海事领域正经历自主化、脱碳化和数字化转型三大技术变革，这要求重新评估内河船舶运营方式，需要开发能够应对环境扰动（水流和风）的智能决策支持系统

Method: 采用收缩时域最优控制框架，整合渡轮动力学数学模型和环境扰动（水流和风）模型，构建决策支持系统，并在真实渡轮上进行验证

Result: 系统能够为渡轮船员提供实时指导，在保持预定操纵时间的同时提升运营效率，在博登湖的MS Insel Mainau渡轮上进行了实际验证

Conclusion: 最优控制在未来内河渡轮运营中具有巨大潜力，该决策支持系统能够有效应对环境扰动，提升运营效率，为海事领域的技术变革提供实用解决方案

Abstract: The maritime sector is undergoing a disruptive technological change driven by three main factors: autonomy, decarbonization, and digital transformation. Addressing these factors necessitates a reassessment of inland vessel operations. This paper presents the design and development of a decision support system for ferry operations based on a shrinking-horizon optimal control framework. The problem formulation incorporates a mathematical model of the ferry's dynamics and environmental disturbances, specifically water currents and wind, which can significantly influence the dynamics. Real-world data and illustrative scenarios demonstrate the potential of the proposed system to effectively support ferry crews by providing real-time guidance. This enables enhanced operational efficiency while maintaining predefined maneuver durations. The findings suggest that optimal control applications hold substantial promise for advancing future ferry operations on inland waters. A video of the real-world ferry MS Insel Mainau operating on Lake Constance is available at: https://youtu.be/i1MjCdbEQyE

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [126] [CORL: Reinforcement Learning of MILP Policies Solved via Branch and Bound](https://arxiv.org/abs/2512.11169)
*Akhil S Anand,Elias Aarekol,Martin Mziray Dalseg,Magnus Stalhane,Sebastien Gros*

Main category: cs.AI

TL;DR: 提出CORL框架，使用强化学习端到端微调MILP方案，将分支定界求解的MILP转化为可微随机策略，以最大化实际操作性能而非建模精度。


<details>
  <summary>Details</summary>
Motivation: 传统MILP建模难以准确表示随机现实问题，导致实际性能不佳。现有机器学习方法依赖监督学习、假设已知最优决策、使用MILP梯度替代，存在局限性。

Method: 提出CORL框架，将分支定界算法求解的MILP转化为可微随机策略，使其与强化学习兼容，在真实世界数据上端到端微调MILP方案。

Result: 在简单的组合序贯决策示例中验证了CORL方法的有效性。

Conclusion: CORL框架通过强化学习端到端优化MILP方案，能够直接针对实际操作性能进行优化，为组合序贯决策问题提供了新的解决方案。

Abstract: Combinatorial sequential decision making problems are typically modeled as mixed integer linear programs (MILPs) and solved via branch and bound (B&B) algorithms. The inherent difficulty of modeling MILPs that accurately represent stochastic real world problems leads to suboptimal performance in the real world. Recently, machine learning methods have been applied to build MILP models for decision quality rather than how accurately they model the real world problem. However, these approaches typically rely on supervised learning, assume access to true optimal decisions, and use surrogates for the MILP gradients. In this work, we introduce a proof of concept CORL framework that end to end fine tunes an MILP scheme using reinforcement learning (RL) on real world data to maximize its operational performance. We enable this by casting an MILP solved by B&B as a differentiable stochastic policy compatible with RL. We validate the CORL method in a simple illustrative combinatorial sequential decision making example.

</details>


### [127] [Deep Learning--Accelerated Multi-Start Large Neighborhood Search for Real-time Freight Bundling](https://arxiv.org/abs/2512.11187)
*Haohui Zhang,Wouter van Heeswijk,Xinyu Hu,Neil Yorke-Smith,Martijn Mes*

Main category: cs.AI

TL;DR: 提出基于Transformer神经网络构造策略与多起点大邻域搜索的混合搜索框架，用于解决在线货运交易系统的组合捆绑问题，在保证亚秒级延迟的同时达到接近最优的收益。


<details>
  <summary>Details</summary>
Motivation: 在线货运交易系统需要实时匹配货主与承运商，但高效组合运输任务捆绑仍是瓶颈。现有方法难以在亚秒延迟内同时处理组合捆绑选择和取送货路径规划。

Method: 将问题建模为多商品一对一取送货选择性旅行商问题(m1-PDSTSP)，提出学习加速混合搜索管道：结合Transformer神经网络构造策略与创新的多起点大邻域搜索元启发式算法，采用滚动时域方案反复冻结市场快照求解。

Result: 在基准测试中优于现有神经组合优化和元启发式基线方法，在可比时间内达到最佳可用精确基线方法总收益的2%以内最优性差距。

Conclusion: 首次证明基于深度神经网络的构造器能够为多起点改进启发式算法可靠提供高质量初始解，该方法不仅适用于m1-PDSTSP问题，还可推广到更广泛的选择性旅行商问题和取送货问题。

Abstract: Online Freight Exchange Systems (OFEX) play a crucial role in modern freight logistics by facilitating real-time matching between shippers and carrier. However, efficient combinatorial bundling of transporation jobs remains a bottleneck. We model the OFEX combinatorial bundling problem as a multi-commodity one-to-one pickup-and-delivery selective traveling salesperson problem (m1-PDSTSP), which optimizes revenue-driven freight bundling under capacity, precedence, and route-length constraints. The key challenge is to couple combinatorial bundle selection with pickup-and-delivery routing under sub-second latency. We propose a learning--accelerated hybrid search pipeline that pairs a Transformer Neural Network-based constructive policy with an innovative Multi-Start Large Neighborhood Search (MSLNS) metaheuristic within a rolling-horizon scheme in which the platform repeatedly freezes the current marketplace into a static snapshot and solves it under a short time budget. This pairing leverages the low-latency, high-quality inference of the learning-based constructor alongside the robustness of improvement search; the multi-start design and plausible seeds help LNS to explore the solution space more efficiently. Across benchmarks, our method outperforms state-of-the-art neural combinatorial optimization and metaheuristic baselines in solution quality with comparable time, achieving an optimality gap of less than 2\% in total revenue relative to the best available exact baseline method. To our knowledge, this is the first work to establish that a Deep Neural Network-based constructor can reliably provide high-quality seeds for (multi-start) improvement heuristics, with applicability beyond the \textit{m1-PDSTSP} to a broad class of selective traveling salesperson problems and pickup and delivery problems.

</details>


### [128] [FutureWeaver: Planning Test-Time Compute for Multi-Agent Systems with Modularized Collaboration](https://arxiv.org/abs/2512.11213)
*Dongwon Jung,Peng Shi,Yi Zhang*

Main category: cs.AI

TL;DR: FutureWeaver是一个用于多智能体系统中测试时计算资源分配的框架，通过模块化协作和双级规划架构，在固定预算下优化计算分配以提升协作性能。


<details>
  <summary>Details</summary>
Motivation: 当前测试时计算扩展技术（如重复采样、自我验证、自我反思）能提升单智能体性能，但在多智能体系统中缺乏原则性的机制来分配计算资源以促进协作，也无法在明确预算约束下跨智能体分配计算。

Method: 提出FutureWeaver框架：1）引入模块化协作，通过自玩反思从历史轨迹中抽象出可复用的多智能体工作流作为可调用函数；2）采用双级规划架构，在优化当前任务状态计算分配的同时推测未来步骤。

Result: 在复杂智能体基准测试上的实验表明，FutureWeaver在不同预算设置下始终优于基线方法，验证了其在多智能体协作推理时优化中的有效性。

Conclusion: FutureWeaver成功解决了多智能体系统中测试时计算资源分配的问题，通过模块化协作和前瞻性规划实现了在固定预算下的高效协作优化，为多智能体推理时计算扩展提供了系统化解决方案。

Abstract: Scaling test-time computation improves large language model performance without additional training. Recent work demonstrates that techniques such as repeated sampling, self-verification, and self-reflection can significantly enhance task success by allocating more inference-time compute. However, applying these techniques across multiple agents in a multi-agent system is difficult: there does not exist principled mechanisms to allocate compute to foster collaboration among agents, to extend test-time scaling to collaborative interactions, or to distribute compute across agents under explicit budget constraints. To address this gap, we propose FutureWeaver, a framework for planning and optimizing test-time compute allocation in multi-agent systems under fixed budgets. FutureWeaver introduces modularized collaboration, formalized as callable functions that encapsulate reusable multi-agent workflows. These modules are automatically derived through self-play reflection by abstracting recurring interaction patterns from past trajectories. Building on these modules, FutureWeaver employs a dual-level planning architecture that optimizes compute allocation by reasoning over the current task state while also speculating on future steps. Experiments on complex agent benchmarks demonstrate that FutureWeaver consistently outperforms baselines across diverse budget settings, validating its effectiveness for multi-agent collaboration in inference-time optimization.

</details>


### [129] [A-LAMP: Agentic LLM-Based Framework for Automated MDP Modeling and Policy Generation](https://arxiv.org/abs/2512.11270)
*Hong Je-Gal,Chan-Bin Yi,Hyun-Suk Lee*

Main category: cs.AI

TL;DR: A-LAMP是一个基于大语言模型的自动化框架，能够将自然语言任务描述自动转化为MDP模型并生成训练好的策略，通过分解建模、编码和训练为可验证阶段来确保语义对齐。


<details>
  <summary>Details</summary>
Motivation: 将强化学习应用于现实任务需要将非正式描述转化为正式的MDP模型、实现可执行环境并训练策略代理。这一过程由于建模错误、脆弱代码和目标不对齐等问题而具有挑战性，经常阻碍策略训练。

Method: 引入基于代理大语言模型的自动化MDP建模和策略生成框架（A-LAMP），将自由形式的自然语言任务描述自动转化为MDP公式和训练好的策略。该框架将建模、编码和训练分解为可验证阶段，确保整个流程的语义对齐。

Result: 在经典控制和自定义RL领域中，A-LAMP始终比单个最先进的大语言模型具有更高的策略生成能力。值得注意的是，即使是基于较小语言模型的轻量级变体，也能接近更大模型的性能。失败分析揭示了这些改进的原因。

Conclusion: A-LAMP能够生成保持任务最优性的环境和策略，证实了其正确性和可靠性。该框架为解决RL应用中的自动化建模和策略生成问题提供了有效解决方案。

Abstract: Applying reinforcement learning (RL) to real-world tasks requires converting informal descriptions into a formal Markov decision process (MDP), implementing an executable environment, and training a policy agent. Automating this process is challenging due to modeling errors, fragile code, and misaligned objectives, which often impede policy training. We introduce an agentic large language model (LLM)-based framework for automated MDP modeling and policy generation (A-LAMP), that automatically translates free-form natural language task descriptions into an MDP formulation and trained policy. The framework decomposes modeling, coding, and training into verifiable stages, ensuring semantic alignment throughout the pipeline. Across both classic control and custom RL domains, A-LAMP consistently achieves higher policy generation capability than a single state-of-the-art LLM model. Notably, even its lightweight variant, which is built on smaller language models, approaches the performance of much larger models. Failure analysis reveals why these improvements occur. In addition, a case study also demonstrates that A-LAMP generates environments and policies that preserve the task's optimality, confirming its correctness and reliability.

</details>


### [130] [TriFlow: A Progressive Multi-Agent Framework for Intelligent Trip Planning](https://arxiv.org/abs/2512.11271)
*Yuxing Chen,Basem Suleiman,Qifan Chen*

Main category: cs.AI

TL;DR: TriFlow是一个用于真实世界旅行规划的多智能体框架，通过检索-规划-治理三阶段流水线，结合结构化推理和语言灵活性，显著提高了约束满足能力和效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的旅行规划代理在约束满足、工具协调和效率方面存在不足，经常产生不可行或成本过高的计划，需要更好的解决方案来处理复杂的空间、时间和预算约束。

Method: TriFlow采用渐进式多智能体框架，包含检索、规划和治理三个阶段：检索阶段缩小搜索空间，规划阶段通过规则-LLM协作组装约束一致的行程，治理阶段进行有界迭代优化以确保全局可行性和个性化。

Result: 在TravelPlanner和TripTailor基准测试中达到最先进水平，分别获得91.1%和97%的最终通过率，相比当前SOTA实现了超过10倍的运行效率提升。

Conclusion: TriFlow通过结合结构化推理和语言灵活性，有效解决了真实世界旅行规划中的约束满足问题，显著提高了规划质量和效率，为复杂约束下的行程规划提供了实用解决方案。

Abstract: Real-world trip planning requires transforming open-ended user requests into executable itineraries under strict spatial, temporal, and budgetary constraints while aligning with user preferences. Existing LLM-based agents struggle with constraint satisfaction, tool coordination, and efficiency, often producing infeasible or costly plans. To address these limitations, we present TriFlow, a progressive multi-agent framework that unifies structured reasoning and language-based flexibility through a three-stage pipeline of retrieval, planning, and governance. By this design, TriFlow progressively narrows the search space, assembles constraint-consistent itineraries via rule-LLM collaboration, and performs bounded iterative refinement to ensure global feasibility and personalisation. Evaluations on TravelPlanner and TripTailor benchmarks demonstrated state-of-the-art results, achieving 91.1% and 97% final pass rates, respectively, with over 10x runtime efficiency improvement compared to current SOTA.

</details>


### [131] [General-purpose AI models can generate actionable knowledge on agroecological crop protection](https://arxiv.org/abs/2512.11474)
*Kris A. G. Wyckhuys*

Main category: cs.AI

TL;DR: 评估DeepSeek与ChatGPT在农业生态作物保护知识生成中的表现，发现DeepSeek在文献覆盖、解决方案数量和数据一致性方面优于ChatGPT，但两者都存在幻觉、命名混淆等问题，需结合人工监督才能有效支持农场决策。


<details>
  <summary>Details</summary>
Motivation: 探索生成式AI在农业食品科学中的应用潜力，特别是评估大型语言模型在农业生态作物保护知识生成方面的表现，以验证其能否为农场决策提供可靠的科学知识支持。

Method: 针对九种全球性限制性害虫、杂草和植物病害，比较基于网络（DeepSeek）和非基于网络（ChatGPT免费版）的LLMs。评估指标包括事实准确性、数据一致性、知识广度或数据完整性，分析文献覆盖范围、生物防治剂/管理方案数量、功效估计等。

Result: DeepSeek在文献覆盖范围上比ChatGPT高4.8-49.7倍，报告的生物防治剂或管理方案多1.6-2.4倍，功效估计高21.6%，实验室到田间数据一致性更好，对害虫身份和管理策略的影响评估更现实。但两者都存在幻觉（虚构代理或参考文献）、报告不可信的生态交互、混淆新旧科学命名、遗漏关键代理或解决方案等问题。

Conclusion: 尽管存在局限性，两种LLMs都能正确报告低分辨率功效趋势。结合严格的人工监督，LLMs可能成为支持农场级决策和释放科学创造力的强大工具。

Abstract: Generative artificial intelligence (AI) offers potential for democratizing scientific knowledge and converting this to clear, actionable information, yet its application in agri-food science remains unexplored. Here, we verify the scientific knowledge on agroecological crop protection that is generated by either web-grounded or non-grounded large language models (LLMs), i.e., DeepSeek versus the free-tier version of ChatGPT. For nine globally limiting pests, weeds, and plant diseases, we assessed the factual accuracy, data consistency, and breadth of knowledge or data completeness of each LLM. Overall, DeepSeek consistently screened a 4.8-49.7-fold larger literature corpus and reported 1.6-2.4-fold more biological control agents or management solutions than ChatGPT. As a result, DeepSeek reported 21.6% higher efficacy estimates, exhibited greater laboratory-to-field data consistency, and showed more realistic effects of pest identity and management tactics. However, both models hallucinated, i.e., fabricated fictitious agents or references, reported on implausible ecological interactions or outcomes, confused old and new scientific nomenclatures, and omitted data on key agents or solutions. Despite these shortcomings, both LLMs correctly reported low-resolution efficacy trends. Overall, when paired with rigorous human oversight, LLMs may pose a powerful tool to support farm-level decision-making and unleash scientific creativity.

</details>


### [132] [CAPTURE: A Benchmark and Evaluation for LVLMs in CAPTCHA Resolving](https://arxiv.org/abs/2512.11323)
*Jianyi Zhang,Ziyin Zhou,Xu Ji,Shizhao Liu,Zhangchi Zhao*

Main category: cs.AI

TL;DR: 本文提出了首个专门针对大型视觉语言模型的CAPTCHA基准测试CAPTURE，涵盖4种主要类型和25种子类型，用于全面评估LVLMs在解决验证码任务上的能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉验证码的基准测试存在局限性：1）以往研究根据自身目标定制基准，无法全面覆盖所有验证码类型；2）缺乏专门针对LVLMs的验证码基准测试。

Method: 构建了CAPTURE基准测试，包含4种主要验证码类型和25种子类型，来自31个供应商。该基准具有广泛的类别多样性、大规模数据以及专门为LVLMs定制的标签。

Result: 使用该基准测试评估当前LVLMs时，发现它们在解决验证码任务上表现不佳。

Conclusion: CAPTURE基准填补了先前研究在数据全面性和标签针对性方面的空白，能够对LVLM性能进行多维度和全面的评估。

Abstract: Benefiting from strong and efficient multi-modal alignment strategies, Large Visual Language Models (LVLMs) are able to simulate human visual and reasoning capabilities, such as solving CAPTCHAs. However, existing benchmarks based on visual CAPTCHAs still face limitations. Previous studies, when designing benchmarks and datasets, customized them according to their research objectives. Consequently, these benchmarks cannot comprehensively cover all CAPTCHA types. Notably, there is a dearth of dedicated benchmarks for LVLMs. To address this problem, we introduce a novel CAPTCHA benchmark for the first time, named CAPTURE CAPTCHA for Testing Under Real-world Experiments, specifically for LVLMs. Our benchmark encompasses 4 main CAPTCHA types and 25 sub-types from 31 vendors. The diversity enables a multi-dimensional and thorough evaluation of LVLM performance. CAPTURE features extensive class variety, large-scale data, and unique LVLM-tailored labels, filling the gaps in previous research in terms of data comprehensiveness and labeling pertinence. When evaluated by this benchmark, current LVLMs demonstrate poor performance in solving CAPTCHAs.

</details>


### [133] [Towards Trustworthy Multi-Turn LLM Agents via Behavioral Guidance](https://arxiv.org/abs/2512.11421)
*Gonca Gürsun*

Main category: cs.AI

TL;DR: 提出一个任务完成框架，让基于LLM的智能体在强化学习形式化环境中，通过可验证的观察-行动映射和约束合规输出，实现可靠的行为。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在推理和生成方面表现出色，但在多轮任务中的行为往往缺乏可靠性和可验证性。需要一种框架让LLM智能体在明确的行为指导下，在具有观察、行动和奖励信号的强化学习环境中可靠地行动。

Method: 框架包含三个组件：1) 轻量级任务分析器选择推理和生成策略；2) 推理模块学习可验证的观察-行动映射；3) 生成模块通过验证或确定性合成确保约束合规的输出。这些组件在智能体与环境交互过程中协同演化。

Result: 随着智能体与环境交互，框架的三个组件协同演化，产生可信赖的行为。该框架能够确保LLM智能体在强化学习环境中表现出可靠和可验证的行为。

Conclusion: 提出的任务完成框架通过集成任务分析、可验证推理和约束合规生成，使LLM智能体能够在强化学习环境中实现可靠、可验证的行为，解决了现有LLM在多轮任务中行为不可靠的问题。

Abstract: Large Language Models demonstrate strong reasoning and generation abilities, yet their behavior in multi-turn tasks often lacks reliability and verifiability. We present a task completion framework that enables LLM-based agents to act under explicit behavioral guidance in environments described by reinforcement learning formalisms with defined observation, action, and reward signals.
  The framework integrates three components: a lightweight task profiler that selects reasoning and generation strategies, a reasoning module that learns verifiable observation - action mappings, and a generation module that enforces constraint-compliant outputs through validation or deterministic synthesis. We show that as the agent interacts with the environment, these components co-evolve, yielding trustworthy behavior.

</details>


### [134] [AgentBalance: Backbone-then-Topology Design for Cost-Effective Multi-Agent Systems under Budget Constraints](https://arxiv.org/abs/2512.11426)
*Shuowei Cai,Yansong Ning,Hao Liu*

Main category: cs.AI

TL;DR: AgentBalance是一个在明确token成本和延迟预算下构建成本效益多智能体系统的框架，采用"先骨干后拓扑"设计，相比现有方法在相同预算下获得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的多智能体系统通常只优化通信拓扑和选择智能体骨干，但很少在明确的token成本和延迟预算约束下进行建模和优化，导致预算约束时成本效益不佳。

Method: 采用"先骨干后拓扑"设计：1)骨干导向的智能体生成：通过LLM池构建、池选择和角色-骨干匹配构建异构骨干智能体；2)自适应MAS拓扑生成：通过智能体表示学习、门控和延迟感知拓扑合成指导智能体间通信。

Result: 在14个候选LLM骨干的基准测试中，AgentBalance在匹配的token成本预算下获得高达10%的性能提升，在匹配的延迟预算下获得高达22%的性能提升，并在性能-预算曲线上表现出强大的AUC。

Conclusion: AgentBalance是一个有效的框架，可在明确预算约束下构建成本效益多智能体系统，可作为现有MAS的插件提升性能，并能很好地泛化到未见过的LLM，支持实际部署中的预算感知需求。

Abstract: Large Language Model (LLM)-based multi-agent systems (MAS) are becoming indispensable building blocks for web-scale applications such as web search, social network analytics, and online customer support, where cost-effectiveness is increasingly the primary constraint for large-scale deployment. While recent work improves MAS cost-effectiveness by shaping inter-agent communication topologies and selecting agent backbones, it rarely models and optimizes under explicit token-cost and latency budgets that reflect deployment constraints. This often leads to topology-first designs and suboptimal cost-effectiveness when budgets are binding. We present AgentBalance, a framework for constructing cost-effective MAS under explicit token-cost and latency budgets via a backbone-then-topology design. AgentBalance first performs backbone-oriented agent generation, constructing agents with heterogeneous backbones through LLM pool construction, pool selection, and role-backbone matching. It then performs adaptive MAS topology generation, guiding inter-agent communication via agent representation learning, gating, and latency-aware topology synthesis. Experiments on benchmarks with 14 candidate LLM backbones show that AgentBalance achieves up to 10% and 22% performance gains under matched token-cost and latency budgets, respectively, and yields strong AUC on performance-versus-budget curves across benchmarks. AgentBalance also functions as a plug-in for existing MAS, improving performance under the same token-cost and latency constraints, and it generalizes well to unseen LLMs for practical, budget-aware deployment. Code: https://github.com/usail-hkust/AgentBalance

</details>


### [135] [Back to the Baseline: Examining Baseline Effects on Explainability Metrics](https://arxiv.org/abs/2512.11433)
*Agustin Martin Picard,Thibaut Boissin,Varshini Subhash,Rémi Cadène,Thomas Fel*

Main category: cs.AI

TL;DR: 本文指出当前XAI中基于基线的保真度评估指标存在问题，不同基线选择会偏向不同的归因方法，并提出了一种新的模型依赖基线来改进信息移除与分布外样本的权衡。


<details>
  <summary>Details</summary>
Motivation: 当前XAI中广泛使用的归因方法评估指标（如插入和删除）依赖于基线函数来修改输入图像像素。研究发现基线选择会系统性偏向某些归因方法，甚至简单的线性模型在不同基线下的最优方法也不同，这引发了"应该使用哪个基线"的根本问题。

Method: 首先分析基线应满足的两个理想属性：(1) 移除信息，(2) 不产生过度分布外图像。然后测试现有基线发现都存在权衡：要么移除信息但产生OOD图像，要么不产生OOD但信息移除不足。最后提出利用特征可视化技术创建模型依赖的基线，既能移除信息又不产生过度OOD图像。

Result: 实验表明现有基线都无法同时满足两个理想属性，存在信息移除与OOD生成之间的权衡。提出的新基线在权衡方面优于现有基线，能够更好地移除信息同时避免产生过度分布外图像。

Conclusion: 基线选择对归因方法评估有显著影响，当前评估指标存在系统性偏差。提出的模型依赖基线改进了信息移除与OOD生成之间的权衡，为更公平的归因方法评估提供了新方向。

Abstract: Attribution methods are among the most prevalent techniques in Explainable Artificial Intelligence (XAI) and are usually evaluated and compared using Fidelity metrics, with Insertion and Deletion being the most popular. These metrics rely on a baseline function to alter the pixels of the input image that the attribution map deems most important. In this work, we highlight a critical problem with these metrics: the choice of a given baseline will inevitably favour certain attribution methods over others. More concerningly, even a simple linear model with commonly used baselines contradicts itself by designating different optimal methods. A question then arises: which baseline should we use? We propose to study this problem through two desirable properties of a baseline: (i) that it removes information and (ii) that it does not produce overly out-of-distribution (OOD) images. We first show that none of the tested baselines satisfy both criteria, and there appears to be a trade-off among current baselines: either they remove information or they produce a sequence of OOD images. Finally, we introduce a novel baseline by leveraging recent work in feature visualisation to artificially produce a model-dependent baseline that removes information without being overly OOD, thus improving on the trade-off when compared to other existing baselines. Our code is available at https://github.com/deel-ai-papers/Back-to-the-Baseline

</details>


### [136] [Motif-2-12.7B-Reasoning: A Practitioner's Guide to RL Training Recipes](https://arxiv.org/abs/2512.11463)
*Junghwan Lim,Sungmin Lee,Dongseok Kim,Taehyun Kim,Eunhwan Park,Jeesoo Lee,Jeongdoo Lee,Junhyeok Lee,Wai Ting Cheung,Dahye Choi,Minsu Ha,Jaeheui Her,Jaeyeon Huh,Hanbin Jung,Changjin Kang,Beomgyu Kim,Minjae Kim,Taewhan Kim,Youngrok Kim,Hyukjin Kweon,Haesol Lee,Kungyu Lee,Dongpin Oh,Yeongjae Park,Bokki Ryu,Dongjoo Weon*

Main category: cs.AI

TL;DR: Motif-2-12.7B-Reasoning是一个12.7B参数的语言模型，通过创新的训练方法在复杂推理和长上下文理解方面达到与更大模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 解决开源模型与前沿专有模型在复杂推理和长上下文理解方面的差距，同时解决推理适应中常见的模型崩溃和训练不稳定问题。

Method: 采用综合训练方案：1) 使用混合并行和内核级优化的内存高效基础设施支持64K-token上下文；2) 两阶段监督微调课程，通过验证的对齐合成数据缓解分布不匹配；3) 鲁棒的强化学习微调管道，通过难度感知数据过滤和混合策略轨迹重用稳定训练。

Result: Motif-2-12.7B-Reasoning在数学、编码和智能体基准测试中达到了与参数数量显著更大的模型相当的性能。

Conclusion: 该研究为社区提供了一个有竞争力的开源模型，并为在现实计算约束下扩展推理能力提供了实用的蓝图。

Abstract: We introduce Motif-2-12.7B-Reasoning, a 12.7B parameter language model designed to bridge the gap between open-weight systems and proprietary frontier models in complex reasoning and long-context understanding. Addressing the common challenges of model collapse and training instability in reasoning adaptation, we propose a comprehensive, reproducible training recipe spanning system, data, and algorithmic optimizations. Our approach combines memory-efficient infrastructure for 64K-token contexts using hybrid parallelism and kernel-level optimizations with a two-stage Supervised Fine-Tuning (SFT) curriculum that mitigates distribution mismatch through verified, aligned synthetic data. Furthermore, we detail a robust Reinforcement Learning Fine-Tuning (RLFT) pipeline that stabilizes training via difficulty-aware data filtering and mixed-policy trajectory reuse. Empirical results demonstrate that Motif-2-12.7B-Reasoning achieves performance comparable to models with significantly larger parameter counts across mathematics, coding, and agentic benchmarks, offering the community a competitive open model and a practical blueprint for scaling reasoning capabilities under realistic compute constraints.

</details>


### [137] [Three methods, one problem: Classical and AI approaches to no-three-in-line](https://arxiv.org/abs/2512.11469)
*Pranav Ramanathan,Thomas Prellberg,Matthew Lewis,Prathamesh Dinesh Joshi,Raj Abhijit Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.AI

TL;DR: 该论文首次系统比较了经典优化方法与AI方法在No-Three-In-Line问题上的表现，发现ILP在19×19网格内可获得最优解，PatternBoost在14×14网格内匹配最优性能，PPO在10×10网格表现完美但在11×11网格失败。


<details>
  <summary>Details</summary>
Motivation: No-Three-In-Line是组合几何中的经典问题，传统整数线性规划方法虽然能保证最优解，但随着网格规模增大会面临指数级计算复杂度。机器学习方法为这类模式识别问题提供了有前景的替代方案，但缺乏系统性的比较研究。

Method: 采用三种方法：1）经典整数线性规划作为基准；2）首次应用PatternBoost transformer学习方法；3）首次应用PPO强化学习方法。在n×n网格上进行系统性比较评估。

Result: ILP在19×19网格内获得可证明的最优解；PatternBoost在14×14网格内匹配最优性能，测试损失减少96%；PPO在10×10网格获得完美解，但在11×11网格因约束违反而失败。

Conclusion: 经典优化方法对于精确解仍然至关重要，而AI方法在较小规模问题上具有竞争力。混合方法结合两者的优势，为扩展到更大规模问题提供了最有前景的方向。

Abstract: The No-Three-In-Line problem asks for the maximum number of points that can be placed on an n by n grid with no three collinear, representing a famous problem in combinatorial geometry. While classical methods like Integer Linear Programming (ILP) guarantee optimal solutions, they face exponential scaling with grid size, and recent advances in machine learning offer promising alternatives for pattern-based approximation. This paper presents the first systematic comparison of classical optimization and AI approaches to this problem, evaluating their performance against traditional algorithms. We apply PatternBoost transformer learning and reinforcement learning (PPO) to this problem for the first time, comparing them against ILP. ILP achieves provably optimal solutions up to 19 by 19 grids, while PatternBoost matches optimal performance up to 14 by 14 grids with 96% test loss reduction. PPO achieves perfect solutions on 10 by 10 grids but fails at 11 by 11 grids, where constraint violations prevent valid configurations. These results demonstrate that classical optimization remains essential for exact solutions while AI methods offer competitive performance on smaller instances, with hybrid approaches presenting the most promising direction for scaling to larger problem sizes.

</details>


### [138] [BAID: A Benchmark for Bias Assessment of AI Detectors](https://arxiv.org/abs/2512.11505)
*Priyam Basu,Yunfeng Zhang,Vipul Raheja*

Main category: cs.AI

TL;DR: BAID是一个全面的AI文本检测器偏见评估框架，包含20万+样本覆盖7类社会语言因素，发现现有检测器对少数群体文本存在系统性偏见


<details>
  <summary>Details</summary>
Motivation: 现有AI文本检测器在教育和工作场景中广泛应用，但缺乏对社会语言偏见的系统性评估。虽然已有研究发现对英语学习者的偏见，但需要更全面的评估框架来检测各种偏见类型。

Method: 提出BAID评估框架，包含20万+样本覆盖7个主要类别：人口统计、年龄、教育等级、方言、正式程度、政治倾向和主题。为每个样本生成合成版本，精心设计提示以保留原始内容同时反映特定子群体的写作风格。使用该框架评估4个开源SOTA AI文本检测器。

Result: 发现检测器存在一致的性能差异，特别是对来自代表性不足群体的文本召回率较低。检测器在不同社会语言群体间的表现存在系统性偏见。

Conclusion: BAID提供了一个可扩展、透明的AI检测器审计方法，强调在部署这些工具供公众使用前需要进行偏见感知评估，以确保公平性。

Abstract: AI-generated text detectors have recently gained adoption in educational and professional contexts. Prior research has uncovered isolated cases of bias, particularly against English Language Learners (ELLs) however, there is a lack of systematic evaluation of such systems across broader sociolinguistic factors. In this work, we propose BAID, a comprehensive evaluation framework for AI detectors across various types of biases. As a part of the framework, we introduce over 200k samples spanning 7 major categories: demographics, age, educational grade level, dialect, formality, political leaning, and topic. We also generated synthetic versions of each sample with carefully crafted prompts to preserve the original content while reflecting subgroup-specific writing styles. Using this, we evaluate four open-source state-of-the-art AI text detectors and find consistent disparities in detection performance, particularly low recall rates for texts from underrepresented groups. Our contributions provide a scalable, transparent approach for auditing AI detectors and emphasize the need for bias-aware evaluation before these tools are deployed for public use.

</details>


### [139] [EmeraldMind: A Knowledge Graph-Augmented Framework for Greenwashing Detection](https://arxiv.org/abs/2512.11506)
*Georgios Kaoukis,Ioannis Aris Koufopoulos,Psaroudaki Eleni,Danae Pla Karidi,Evaggelia Pitoura,George Papastefanatos,Panayiotis Tsaparas*

Main category: cs.AI

TL;DR: EmeraldMind是一个基于事实的框架，通过整合领域特定知识图谱和检索增强生成技术，自动检测企业绿色洗白（greenwashing）行为。


<details>
  <summary>Details</summary>
Motivation: 随着AI和网络代理在决策中普及，需要设计既能支持可持续发展又能防范错误信息的智能系统。绿色洗白（误导性的企业可持续性声明）对环境保护构成重大挑战。

Method: EmeraldMind构建了EmeraldGraph知识图谱，从多样化的企业ESG报告中提取可验证证据，结合检索增强生成技术，为大型语言模型提供领域特定知识支持，实现基于证据的声明评估。

Result: 在新构建的绿色洗白声明数据集上的实验表明，EmeraldMind相比通用LLMs具有竞争性的准确率、更大的覆盖范围和更优的解释质量，且无需微调或重新训练。

Conclusion: EmeraldMind框架能够提供透明、基于证据的绿色洗白检测，在无法验证声明时负责任地放弃判断，为解决企业可持续性声明中的误导问题提供了有效解决方案。

Abstract: As AI and web agents become pervasive in decision-making, it is critical to design intelligent systems that not only support sustainability efforts but also guard against misinformation. Greenwashing, i.e., misleading corporate sustainability claims, poses a major challenge to environmental progress. To address this challenge, we introduce EmeraldMind, a fact-centric framework integrating a domain-specific knowledge graph with retrieval-augmented generation to automate greenwashing detection. EmeraldMind builds the EmeraldGraph from diverse corporate ESG (environmental, social, and governance) reports, surfacing verifiable evidence, often missing in generic knowledge bases, and supporting large language models in claim assessment. The framework delivers justification-centric classifications, presenting transparent, evidence-backed verdicts and abstaining responsibly when claims cannot be verified. Experiments on a new greenwashing claims dataset demonstrate that EmeraldMind achieves competitive accuracy, greater coverage, and superior explanation quality compared to generic LLMs, without the need for fine-tuning or retraining.

</details>


### [140] [AI-MASLD Metabolic Dysfunction and Information Steatosis of Large Language Models in Unstructured Clinical Narratives](https://arxiv.org/abs/2512.11544)
*Yuan Shen,Xiaojun Wu,Linghua Yu*

Main category: cs.AI

TL;DR: 研究通过模拟真实临床场景，评估主流大语言模型从嘈杂冗余的患者主诉中提取核心医疗信息的能力，发现所有模型都存在不同程度的功能缺陷，并首次提出"AI-MASLD"概念。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型在真实临床环境中处理嘈杂医疗信息的能力，验证其是否存在类似代谢功能障碍的功能衰退现象，为AI在医疗领域的应用提供安全警示。

Method: 采用基于标准化医疗探针的横断面分析设计，选取GPT-4o、Gemini 2.5、DeepSeek 3.1、Qwen3-Max四种主流LLMs，使用包含20个医疗探针的评估系统模拟真实临床沟通环境，由临床专家定义金标准答案，并由两名独立临床医生进行双盲逆评分。

Result: 所有测试模型均表现出不同程度的功能缺陷，Qwen3-Max整体表现最佳，Gemini 2.5最差；极端噪声条件下大多数模型出现功能崩溃；GPT-4o在深静脉血栓继发肺栓塞风险评估中做出严重误判。

Conclusion: 首次实证证实LLMs在处理临床信息时表现出类似代谢功能障碍的特征，提出"AI-MASLD"创新概念；强调当前LLMs必须在人类专家监督下作为辅助工具使用，其理论知识与实际临床应用仍存在显著差距。

Abstract: This study aims to simulate real-world clinical scenarios to systematically evaluate the ability of Large Language Models (LLMs) to extract core medical information from patient chief complaints laden with noise and redundancy, and to verify whether they exhibit a functional decline analogous to Metabolic Dysfunction-Associated Steatotic Liver Disease (MASLD). We employed a cross-sectional analysis design based on standardized medical probes, selecting four mainstream LLMs as research subjects: GPT-4o, Gemini 2.5, DeepSeek 3.1, and Qwen3-Max. An evaluation system comprising twenty medical probes across five core dimensions was used to simulate a genuine clinical communication environment. All probes had gold-standard answers defined by clinical experts and were assessed via a double-blind, inverse rating scale by two independent clinicians. The results show that all tested models exhibited functional defects to varying degrees, with Qwen3-Max demonstrating the best overall performance and Gemini 2.5 the worst. Under conditions of extreme noise, most models experienced a functional collapse. Notably, GPT-4o made a severe misjudgment in the risk assessment for pulmonary embolism (PE) secondary to deep vein thrombosis (DVT). This research is the first to empirically confirm that LLMs exhibit features resembling metabolic dysfunction when processing clinical information, proposing the innovative concept of "AI-Metabolic Dysfunction-Associated Steatotic Liver Disease (AI-MASLD)". These findings offer a crucial safety warning for the application of Artificial Intelligence (AI) in healthcare, emphasizing that current LLMs must be used as auxiliary tools under human expert supervision, as there remains a significant gap between their theoretical knowledge and practical clinical application.

</details>


### [141] [AI Benchmark Democratization and Carpentry](https://arxiv.org/abs/2512.11588)
*Gregor von Laszewski,Wesley Brewer,Jeyan Thiyagalingam,Juri Papay,Armstrong Foundjem,Piotr Luszczek,Murali Emani,Shirley V. Moore,Vijay Janapa Reddi,Matthew D. Sinclair,Sebastian Lobentanzer,Sujata Goswami,Benjamin Hawks,Marco Colombo,Nhan Tran,Christine R. Kirkpatrick,Abdulkareem Alsudais,Gregg Barrett,Tianhao Li,Kirsten Morehouse,Shivaram Venkataraman,Rutwik Jain,Kartik Mathur,Victor Lu,Tejinder Singh,Khojasteh Z. Mirza,Kongtao Chen,Sasidhar Kunapuli,Gavin Farrell,Renato Umeton,Geoffrey C. Fox*

Main category: cs.AI

TL;DR: 论文主张从静态基准转向动态自适应基准测试框架，以应对AI快速演进带来的评估挑战，并提出了"AI基准测试工艺"的教育倡议。


<details>
  <summary>Details</summary>
Motivation: 当前AI基准测试面临多重挑战：模型架构快速演进、规模扩大、数据集变化和部署环境多样化，使得评估成为移动目标。静态基准容易被大语言模型记忆，导致基准结果与现实性能脱节。传统基准强调顶级硬件上的峰值性能，对多样化现实场景指导有限。

Method: 提出动态自适应基准测试框架，纳入演进模型、更新数据和异构平台，同时保持透明度、可复现性和可解释性。倡导"AI基准测试工艺"教育计划，通过技术革新和系统教育相结合，建立持续的基准设计和应用专业知识。

Result: 识别出关键障碍包括高资源需求、专用硬件访问有限、基准设计专业知识缺乏、结果与应用领域关联不确定性。社区努力可以为AI基准测试工艺提供基础，支持负责任、可复现和可访问的AI部署。

Conclusion: 基准测试必须变得动态化，支持应用相关的比较，实现知情、上下文敏感的决策。动态包容的基准测试将确保评估跟上AI演进步伐，支持负责任、可复现和可访问的AI部署。

Abstract: Benchmarks are a cornerstone of modern machine learning, enabling reproducibility, comparison, and scientific progress. However, AI benchmarks are increasingly complex, requiring dynamic, AI-focused workflows. Rapid evolution in model architectures, scale, datasets, and deployment contexts makes evaluation a moving target. Large language models often memorize static benchmarks, causing a gap between benchmark results and real-world performance.
  Beyond traditional static benchmarks, continuous adaptive benchmarking frameworks are needed to align scientific assessment with deployment risks. This calls for skills and education in AI Benchmark Carpentry. From our experience with MLCommons, educational initiatives, and programs like the DOE's Trillion Parameter Consortium, key barriers include high resource demands, limited access to specialized hardware, lack of benchmark design expertise, and uncertainty in relating results to application domains. Current benchmarks often emphasize peak performance on top-tier hardware, offering limited guidance for diverse, real-world scenarios.
  Benchmarking must become dynamic, incorporating evolving models, updated data, and heterogeneous platforms while maintaining transparency, reproducibility, and interpretability. Democratization requires both technical innovation and systematic education across levels, building sustained expertise in benchmark design and use. Benchmarks should support application-relevant comparisons, enabling informed, context-sensitive decisions. Dynamic, inclusive benchmarking will ensure evaluation keeps pace with AI evolution and supports responsible, reproducible, and accessible AI deployment. Community efforts can provide a foundation for AI Benchmark Carpentry.

</details>


### [142] [Causal Inference in Energy Demand Prediction](https://arxiv.org/abs/2512.11653)
*Chutian Ma,Grigorii Pomazkin,Giacinto Paolo Saggese,Paul Smith*

Main category: cs.AI

TL;DR: 提出基于结构因果模型的能源需求预测方法，利用因果洞察作为先验知识构建贝叶斯模型，在测试集上达到3.84%的MAPE，表现出色。


<details>
  <summary>Details</summary>
Motivation: 能源需求预测对电网运营商、工业能源消费者和服务提供商至关重要。能源需求受天气条件（温度、湿度、风速、太阳辐射）和日历信息（小时、月份）等多种因素影响，这些因素之间存在因果相互依赖关系，使得简单基于相关性的学习方法难以充分处理。

Method: 1. 提出结构因果模型来解释变量间的因果关系；2. 进行完整分析验证因果信念，发现与先前研究一致的洞察；3. 基于学习到的因果洞察作为先验知识构建贝叶斯模型；4. 在未见数据上进行训练和测试。

Result: 1. 因果模型揭示了能源需求对温度波动的响应具有季节依赖性敏感性；2. 发现冬季能源需求方差较低，因为温度变化与日常活动模式之间存在解耦效应；3. 贝叶斯模型在测试集上达到3.84%的MAPE；4. 在两年数据的交叉验证中平均MAPE为3.88%，表现出强鲁棒性。

Conclusion: 通过结合结构因果模型和贝叶斯建模，成功开发了高性能的能源需求预测系统。该方法不仅达到了最先进的预测精度，还提供了对能源需求驱动因素的因果理解，增强了模型的解释性和鲁棒性。

Abstract: Energy demand prediction is critical for grid operators, industrial energy
  consumers, and service providers. Energy demand is influenced by multiple
  factors, including weather conditions (e.g. temperature, humidity, wind
  speed, solar radiation), and calendar information (e.g. hour of day and
  month of year), which further affect daily work and life schedules. These
  factors are causally interdependent, making the problem more complex than
  simple correlation-based learning techniques satisfactorily allow for. We
  propose a structural causal model that explains the causal relationship
  between these variables. A full analysis is performed to validate our causal
  beliefs, also revealing important insights consistent with prior studies.
  For example, our causal model reveals that energy demand responds to
  temperature fluctuations with season-dependent sensitivity. Additionally, we
  find that energy demand exhibits lower variance in winter due to the
  decoupling effect between temperature changes and daily activity patterns.
  We then build a Bayesian model, which takes advantage of the causal insights
  we learned as prior knowledge. The model is trained and tested on unseen
  data and yields state-of-the-art performance in the form of a 3.84 percent MAPE on
  the test set. The model also demonstrates strong robustness, as the
  cross-validation across two years of data yields an average MAPE of 3.88 percent.

</details>


### [143] [MedAI: Evaluating TxAgent's Therapeutic Agentic Reasoning in the NeurIPS CURE-Bench Competition](https://arxiv.org/abs/2512.11682)
*Tim Cofala,Christian Kalfar,Jingge Xiao,Johanna Schrader,Michelle Tang,Wolfgang Nejdl*

Main category: cs.AI

TL;DR: TxAgent是一个用于临床治疗决策的AI代理系统，通过迭代检索增强生成整合多种生物医学工具，在CURE-Bench挑战中获得优异表现


<details>
  <summary>Details</summary>
Motivation: 临床治疗决策是高风险领域，需要AI系统进行稳健的多步推理，而现有通用RAG系统在医疗应用中存在安全约束和准确性不足的问题

Method: 使用微调的Llama-3.1-8B模型，通过动态生成和执行函数调用来访问统一的生物医学工具套件（ToolUniverse），整合FDA Drug API、OpenTargets和Monarch等资源

Result: 在CURE-Bench NeurIPS 2025挑战中获得优异表现，获得开放科学卓越奖，展示了改进工具检索策略带来的性能提升

Conclusion: 医疗AI代理系统需要将推理轨迹和工具使用行为作为明确的监督信号进行评估，工具检索质量对整体性能有重要影响，改进检索策略能显著提升系统表现

Abstract: Therapeutic decision-making in clinical medicine constitutes a high-stakes domain in which AI guidance interacts with complex interactions among patient characteristics, disease processes, and pharmacological agents. Tasks such as drug recommendation, treatment planning, and adverse-effect prediction demand robust, multi-step reasoning grounded in reliable biomedical knowledge. Agentic AI methods, exemplified by TxAgent, address these challenges through iterative retrieval-augmented generation (RAG). TxAgent employs a fine-tuned Llama-3.1-8B model that dynamically generates and executes function calls to a unified biomedical tool suite (ToolUniverse), integrating FDA Drug API, OpenTargets, and Monarch resources to ensure access to current therapeutic information. In contrast to general-purpose RAG systems, medical applications impose stringent safety constraints, rendering the accuracy of both the reasoning trace and the sequence of tool invocations critical. These considerations motivate evaluation protocols treating token-level reasoning and tool-usage behaviors as explicit supervision signals. This work presents insights derived from our participation in the CURE-Bench NeurIPS 2025 Challenge, which benchmarks therapeutic-reasoning systems using metrics that assess correctness, tool utilization, and reasoning quality. We analyze how retrieval quality for function (tool) calls influences overall model performance and demonstrate performance gains achieved through improved tool-retrieval strategies. Our work was awarded the Excellence Award in Open Science. Complete information can be found at https://curebench.ai/.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [144] [Stabilising Learner Trajectories: A Doubly Robust Evaluation of AI-Guided Student Support using Activity Theory](https://arxiv.org/abs/2512.11154)
*Teo Susnjak,Khalid Bakhshov,Anuradha Mathrani*

Main category: cs.CY

TL;DR: AI学生支持系统通过双重稳健倾向得分匹配评估，发现能有效降低课程失败率、提高成绩，但对加速学位完成效果有限


<details>
  <summary>Details</summary>
Motivation: 高等教育中预测模型日益普及，但其触发的干预措施的因果证据仍然稀缺。本研究旨在评估AI指导的学生支持系统的实际效果，解决观察性研究中常被忽视的选择偏差和永生时间偏差问题

Method: 采用双重稳健倾向得分匹配方法，利用时间对齐的动态AI成功概率分数，将1,859名接受干预的学生与对照组匹配，以减轻观察性研究中的选择偏差和永生时间偏差

Result: 干预有效稳定了学生的学业轨迹，相比对照组，受支持学生显著降低了课程失败率并获得了更高的累积成绩。但对加速资格完成的积极效果在统计上受限

Conclusion: 基于活动理论框架，AI支持系统作为"社会技术刹车"成功解决了即时学业风险的主要矛盾，但制度结构中的次要矛盾限制了学位完成的加速。AI支持能有效阻止学业下滑，但要将这种稳定性转化为更快进度，需要将干预策略与更广泛的制度治理相结合

Abstract: While predictive models are increasingly common in higher education, causal evidence regarding the interventions they trigger remains rare. This study evaluates an AI-guided student support system at a large university using doubly robust propensity score matching. We advance the methodology for learning analytics evaluation by leveraging time-aligned, dynamic AI probability of success scores to match 1,859 treated students to controls, thereby mitigating the selection and immortal time biases often overlooked in observational studies. Results indicate that the intervention effectively stabilised precarious trajectories, and compared to the control group, supported students significantly reduced their course failure rates and achieved higher cumulative grades. However, effects on the speed of qualification completion were positive but statistically constrained. We interpreted these findings through Activity Theory, framing the intervention as a socio-technical brake that interrupts and slows the accumulation of academic failure among at-risk students. The student support-AI configuration successfully resolved the primary contradiction of immediate academic risk, but secondary contradictions within institutional structures limited the acceleration of degree completion. We conclude that while AI-enabled support effectively arrests decline, translating this stability into faster progression requires aligning intervention strategies with broader institutional governance.

</details>


### [145] [Personalized Pricing in Social Networks with Individual and Group Fairness Considerations](https://arxiv.org/abs/2512.11252)
*Zeyu Chen,Bintong Chen,Wei Qian,Jing Huang*

Main category: cs.CY

TL;DR: 提出FairPricing框架，在社交网络环境下结合个体和群体公平性进行个性化定价，使用图神经网络学习定价策略，能适应网络变化。


<details>
  <summary>Details</summary>
Motivation: 个性化定价虽能提高零售商收入，但存在个体和群体两个层面的公平性问题。现有研究通常分别处理这两个维度，本文旨在填补这一空白，在社交网络环境中同时考虑两种公平性。

Method: 提出FairPricing框架，基于图神经网络(GNNs)学习个性化定价策略。通过惩罚顾客需求来捕捉个体感知不公平，使用对抗去偏和价格正则化来缓解群体歧视。相比传统优化方法需要网络更新时重新优化，该框架能直接适应新网络结构。

Result: 大量实验结果表明，FairPricing在保持高盈利性的同时，改善了个人公平感知并满足群体公平要求。

Conclusion: 该研究成功地将个体和群体公平性整合到个性化定价中，提出的FairPricing框架能有效平衡盈利性和公平性，并能适应网络结构变化。

Abstract: Personalized pricing assigns different prices to customers for the same product based on customer-specific features to improve retailer revenue. However, this practice often raises concerns about fairness at both the individual and group levels. At the individual level, a customer may perceive unfair treatment if he/she notices being charged a higher price than others. At the group level, pricing disparities can result in discrimination against certain protected groups, such as those defined by gender or race. Existing studies on fair pricing typically address individual and group fairness separately. This paper bridges the gap by introducing a new formulation of the personalized pricing problem that incorporates both dimensions of fairness in social network settings. To solve the problem, we propose FairPricing, a novel framework based on graph neural networks (GNNs) that learns a personalized pricing policy using customer features and network topology. In FairPricing, individual perceived unfairness is captured through a penalty on customer demand, and thus the profit objective, while group-level discrimination is mitigated using adversarial debiasing and a price regularization term. Unlike existing optimization-based personalized pricing, which requires re-optimization whenever the network updates, the pricing policy learned by FairPricing assigns personalized prices to all customers in an updated network based on their features and the new network structure, thereby generalizing to network changes. Extensive experimental results show that FairPricing achieves high profitability while improving individual fairness perceptions and satisfying group fairness requirements.

</details>


### [146] [The Right Kind of Help: Evaluating the Effectiveness of Intervention Methods in Elementary-Level Visual Programming](https://arxiv.org/abs/2512.11735)
*Ahana Ghosh,Liina Malva,Alkis Gotovos,Danial Hooshyar,Adish Singla*

Main category: cs.CY

TL;DR: 比较三种编程干预方法（代码编辑推荐、代码编辑测验、元认知策略测验）在小学编程学习中的效果，发现所有干预方法都能显著提升学习表现，且测验类方法对后续新任务表现有额外帮助。


<details>
  <summary>Details</summary>
Motivation: 先前研究探索了多种小学编程干预方法，但这些方法在学习阶段和学习后阶段（面对新任务时）的相对影响尚不明确，需要大规模研究来比较不同干预方法在不同阶段的效果。

Method: 对398名4-7年级学生进行两阶段研究：学习阶段使用Hour of Code: Maze Challenge的写代码任务，配合三种干预方法（代码编辑推荐、代码编辑测验、元认知策略测验）或无干预对照；学习后阶段使用更高级的写代码任务且无干预。比较不同组别的表现差异。

Result: 所有干预方法都显著优于无干预对照组，同时保持了学生在学习后阶段的问题解决能力。测验类方法（代码编辑测验和元认知策略测验）能进一步提升学生在后续新任务上的表现。干预组学生报告了更高的参与度和感知技能增长。

Conclusion: 编程干预方法能有效提升小学编程学习效果，特别是测验类干预不仅能改善学习表现，还能增强学生面对新任务的能力，同时提高学习参与度和自我效能感。

Abstract: Prior work has explored various intervention methods for elementary programming. However, the relative impact of these methods during the learning and post-learning phases remains unclear. In this work, we present a large-scale study comparing the effectiveness of various intervention methods in elementary programming both during learning and on novel tasks post-learning. Specifically, we compare three intervention methods: code-edit recommendations (Code-Rec), quizzes based on code edits (Code-Quiz), and quizzes based on metacognitive strategies (Plan-Quiz), along with a no-intervention control (group None). A total of 398 students (across grades 4-7) participated in a two-phase study: learning phase comprising write-code tasks from the Hour of Code: Maze Challenge with the intervention, followed by a post-learning phase comprising more advanced write-code tasks without any intervention. All intervention methods significantly improved learning performance over the control group while preserving students' problem-solving skills in the post-learning phase. Quiz-based methods further improved performance on novel post-learning tasks. Students in intervention groups also reported greater engagement and perceived skill growth.

</details>
