<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 178]
- [q-fin.RM](#q-fin.RM) [Total: 3]
- [eess.SY](#eess.SY) [Total: 19]
- [q-fin.ST](#q-fin.ST) [Total: 10]
- [q-fin.PR](#q-fin.PR) [Total: 1]
- [cs.AI](#cs.AI) [Total: 153]
- [econ.EM](#econ.EM) [Total: 4]
- [stat.ML](#stat.ML) [Total: 35]
- [cs.CY](#cs.CY) [Total: 31]
- [cs.LG](#cs.LG) [Total: 421]
- [q-fin.GN](#q-fin.GN) [Total: 1]
- [math.OC](#math.OC) [Total: 30]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [q-fin.MF](#q-fin.MF) [Total: 2]
- [q-fin.CP](#q-fin.CP) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [PPoGA: Predictive Plan-on-Graph with Action for Knowledge Graph Question Answering](https://arxiv.org/abs/2602.00007)
*MinGyu Jeon,SuWan Cho,JaeYoung Shu*

Main category: cs.CL

TL;DR: PPoGA是一个基于知识图谱的问答框架，通过规划器-执行器架构和预测处理机制，实现自我修正能力，在复杂多跳KGQA任务中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于知识图谱的大语言模型在复杂问答中存在认知功能固着问题，即一旦初始高层推理计划有误，就无法重新调整策略，导致追求不可行的解决方案。

Method: 提出PPoGA框架，采用规划器-执行器架构分离高层策略与低层执行，引入预测处理机制预测结果，核心创新是自我修正机制，包括路径修正和计划修正。

Result: 在三个具有挑战性的多跳KGQA基准测试（GrailQA、CWQ、WebQSP）上进行广泛实验，PPoGA实现了最先进的性能，显著优于现有方法。

Conclusion: 研究表明元认知能力（如问题重构）对于构建更强大、更灵活的AI推理系统至关重要，PPoGA框架为解决认知功能固着问题提供了有效方案。

Abstract: Large Language Models (LLMs) augmented with Knowledge Graphs (KGs) have advanced complex question answering, yet they often remain susceptible to failure when their initial high-level reasoning plan is flawed. This limitation, analogous to cognitive functional fixedness, prevents agents from restructuring their approach, leading them to pursue unworkable solutions. To address this, we propose PPoGA (Predictive Plan-on-Graph with Action), a novel KGQA framework inspired by human cognitive control and problem-solving. PPoGA incorporates a Planner-Executor architecture to separate high-level strategy from low-level execution and leverages a Predictive Processing mechanism to anticipate outcomes. The core innovation of our work is a self-correction mechanism that empowers the agent to perform not only Path Correction for local execution errors but also Plan Correction by identifying, discarding, and reformulating the entire plan when it proves ineffective. We conduct extensive experiments on three challenging multi-hop KGQA benchmarks: GrailQA, CWQ, and WebQSP. The results demonstrate that PPoGA achieves state-of-the-art performance, significantly outperforming existing methods. Our work highlights the critical importance of metacognitive abilities like problem restructuring for building more robust and flexible AI reasoning systems.

</details>


### [2] [Unlocking Electronic Health Records: A Hybrid Graph RAG Approach to Safe Clinical AI for Patient QA](https://arxiv.org/abs/2602.00009)
*Samuel Thio,Matthew Lewis,Spiros Denaxas,Richard JB Dobson*

Main category: cs.CL

TL;DR: MediGRAF是一个混合图检索增强框架，通过结合结构化图数据库查询和非结构化语义搜索，实现临床信息的全面检索，在MIMIC-IV数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录系统信息过载导致关键细节容易被忽略，现有LLM在临床环境中存在上下文基础不足和幻觉问题，当前检索方法无法同时整合结构化和非结构化数据。

Method: 提出MediGRAF混合图RAG系统，结合Neo4j Text2Cypher进行结构化关系遍历和向量嵌入进行非结构化叙事检索，支持自然语言查询完整患者旅程。

Result: 在MIMIC-IV数据集的10名患者数据（5,973个节点和5,963个关系）上，事实查询实现100%召回率，复杂推理任务平均专家质量得分4.25/5，零安全违规。

Conclusion: 混合图基础显著推进临床信息检索，为标准的LLM部署提供了更安全、更全面的替代方案。

Abstract: Electronic health record (EHR) systems present clinicians with vast repositories of clinical information, creating a significant cognitive burden where critical details are easily overlooked. While Large Language Models (LLMs) offer transformative potential for data processing, they face significant limitations in clinical settings, particularly regarding context grounding and hallucinations. Current solutions typically isolate retrieval methods focusing either on structured data (SQL/Cypher) or unstructured semantic search but fail to integrate both simultaneously. This work presents MediGRAF (Medical Graph Retrieval Augmented Framework), a novel hybrid Graph RAG system that bridges this gap. By uniquely combining Neo4j Text2Cypher capabilities for structured relationship traversal with vector embeddings for unstructured narrative retrieval, MediGRAF enables natural language querying of the complete patient journey. Using 10 patients from the MIMIC-IV dataset (generating 5,973 nodes and 5,963 relationships), we generated enough nodes and data for patient level question answering (QA), and we evaluated this architecture across varying query complexities. The system demonstrated 100\% recall for factual queries which means all relevant information was retrieved and in the output, while complex inference tasks achieved a mean expert quality score of 4.25/5 with zero safety violations. These results demonstrate that hybrid graph-grounding significantly advances clinical information retrieval, offering a safer, more comprehensive alternative to standard LLM deployments.

</details>


### [3] [G-MemLLM: Gated Latent Memory Augmentation for Long-Context Reasoning in Large Language Models](https://arxiv.org/abs/2602.00015)
*Xun Xu*

Main category: cs.CL

TL;DR: G-MemLLM：一种基于门控更新的记忆增强LLM架构，通过潜在记忆库解决长上下文推理中的信息稀释问题，显著提升多跳推理和关系抽取性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM受限于有限的上下文窗口，在多跳推理中难以保持长期事实一致性。现有方法如上下文压缩或循环标记存在"上下文腐化"和信息稀释问题。

Method: 提出G-MemLLM架构，将冻结的LLM主干与可训练的潜在记忆库结合，采用GRU风格的门控更新逻辑，选择性更新、保留或覆盖潜在记忆槽，防止循环系统中的梯度消失。

Result: 在GPT-2(124M)到Llama 3.1(8B)不同规模模型上评估，在HotpotQA和ZsRE基准上显著提升性能：Llama 3.1-8B在ZsRE上准确率提升13.3%，GPT-2在HotpotQA上答案F1提升8.56分，Llama 3.1-8B支持事实F1提升6.89分。

Conclusion: G-MemLLM通过门控记忆机制有效解决了长序列推理中的信息保持问题，在不同规模模型上都实现了显著性能提升，为增强LLM的长上下文推理能力提供了有效方案。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding, yet they remain constrained by the finite capacity of their context windows and the inherent difficulty of maintaining long-term factual consistency during multi-hop reasoning. While existing methods utilize context compression or recurrent tokens, they often suffer from ``context rot'' or the dilution of information over long horizons. In this paper, we propose \textbf{G-MemLLM}, a memory-augmented architecture that integrates a frozen LLM backbone with a trainable \textbf{Latent Memory Bank}. Our key innovation is a GRU-style gated update logic that allows the model to selectively update, preserve, or overwrite latent memory slots, preventing the vanishing gradients of knowledge common in recurrent systems. We evaluate G-MemLLM across scales, from GPT-2 (124M) to Llama 3.1 (8B), on the HotpotQA and Zero-Shot Relation Extraction (ZsRE) benchmarks. Our results demonstrate that G-MemLLM significantly enhances multi-hop reasoning and relational precision, achieving a 13.3\% accuracy boost on ZsRE for Llama 3.1-8B, and it also yields improvements across model scales, boosting Answer F1 by 8.56 points for GPT-2 and increasing Supporting Fact F1 by 6.89 points for Llama 3.1-8B on HotpotQA.

</details>


### [4] [PTCBENCH: Benchmarking Contextual Stability of Personality Traits in LLM Systems](https://arxiv.org/abs/2602.00016)
*Jiongchi Yu,Yuhan Ma,Xiaoyu Zhang,Junjie Wang,Qiang Hu,Chao Shen,Xiaofei Xie*

Main category: cs.CL

TL;DR: PTCBENCH是一个评估大语言模型在受控情境下人格一致性的系统性基准，发现外部场景（如失业）会显著改变LLM的人格特质和推理能力。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在情感代理和AI系统中的部署增加，保持一致且真实的人格对用户信任和参与至关重要。现有研究忽略了人格特质是动态且情境依赖的基本心理学共识。

Method: 引入PTCBENCH基准，将模型置于12种不同的外部条件（位置情境和生活事件），使用NEO五因素人格量表严格评估人格，共分析了39,240个人格特质记录。

Result: 研究发现某些外部场景（如"失业"）能显著触发LLM的人格变化，甚至改变其推理能力。建立了评估人格一致性的可扩展框架。

Conclusion: PTCBENCH为在现实、动态环境中评估人格一致性提供了可扩展框架，为开发稳健且心理对齐的AI系统提供了可行见解。

Abstract: With the increasing deployment of large language models (LLMs) in affective agents and AI systems, maintaining a consistent and authentic LLM personality becomes critical for user trust and engagement. However, existing work overlooks a fundamental psychological consensus that personality traits are dynamic and context-dependent. To bridge this gap, we introduce PTCBENCH, a systematic benchmark designed to quantify the consistency of LLM personalities under controlled situational contexts. PTCBENCH subjects models to 12 distinct external conditions spanning diverse location contexts and life events, and rigorously assesses the personality using the NEO Five-Factor Inventory. Our study on 39,240 personality trait records reveals that certain external scenarios (e.g., "Unemployment") can trigger significant personality changes of LLMs, and even alter their reasoning capabilities. Overall, PTCBENCH establishes an extensible framework for evaluating personality consistency in realistic, evolving environments, offering actionable insights for developing robust and psychologically aligned AI systems.

</details>


### [5] [SafeTalkCoach: Diversity-Driven Multi-Agent Simulation for Parent-Teen Health Conversations](https://arxiv.org/abs/2602.00017)
*Benyamin Tabarsi,Wenbo Li,Tahreem Yasir,Aryan Santhosh Kumar,Laura Widman,Dongkuan Xu,Tiffany Barnes*

Main category: cs.CL

TL;DR: SafeTalkCoach是一个多样性驱动的多智能体对话生成框架，用于模拟亲子间关于性健康的对话，并附带数据集，旨在解决该领域数据稀缺和现有LLM对话缺乏真实性和多样性的问题。


<details>
  <summary>Details</summary>
Motivation: 亲子间关于性健康的有效沟通很重要，但由于话题的私密性和敏感性，真实世界数据稀缺且难以收集。现有LLM生成的对话可能偏离最佳实践，缺乏真实性和多样性。

Method: 提出SafeTalkCoach框架，整合众包和合成场景、既定性健康指南、基于证据的人物角色、自适应控制模块和层次化多样化机制，通过多智能体系统模拟亲子对话。

Result: 评估表明SafeTalkCoach能生成多样化的对话，同时保持真实性、沟通质量和可控性，框架和数据集可用于AI研究和健康沟通实践。

Conclusion: SafeTalkCoach框架和数据集为解决性健康沟通数据稀缺问题提供了有效工具，支持AI研究和健康沟通实践的发展。

Abstract: The importance of effective parent-child communication about sexual health is widely acknowledged, but real-world data on these conversations is scarce and challenging to collect, due to their private and sensitive nature. Although LLMs have been widely adopted in dialogue generation, they may deviate from best practices and frequently lack realism and diversity. We introduce SafeTalkCoach, a diversity-driven multi-agent dialogue generation framework that simulates parent-child conversations about sexual health, and present an accompanying dataset. SafeTalkCoach integrates crowd-sourced and synthesized scenarios, established sexual health guidelines, evidence-based personas, adaptive control modules, and hierarchical diversification. Through evaluations, we demonstrate that SafeTalkCoach generates diverse conversations while maintaining realism, communication quality, and controllability in practice. Our goal is that the SafeTalkCoach framework and the dataset support both AI research and health communications practices.

</details>


### [6] [Construct, Align, and Reason: Large Ontology Models for Enterprise Knowledge Management](https://arxiv.org/abs/2602.00029)
*Yao Zhang,Hongyin Zhu*

Main category: cs.CL

TL;DR: 提出大型本体模型（LOM），通过构建-对齐-推理框架解决企业知识管理中多源异构数据集成和语义推理问题，在复杂图推理任务上超越DeepSeek-V3.2


<details>
  <summary>Details</summary>
Motivation: 企业级知识管理面临多源异构数据集成和有效语义推理的挑战，传统知识图谱在隐式关系发现和复杂问答的语义理解方面存在不足

Method: 提出构建-对齐-推理框架：1）从结构化数据库和非结构化文本构建双层企业本体并融合；2）三阶段训练流程：本体指令微调、文本-本体对齐、多任务指令调优；3）构建全面的训练和评估数据集

Result: 4B参数的LOM在基准测试中达到89.47%准确率，在复杂图推理任务上超越DeepSeek-V3.2，表明本体结构与语言的有效融合

Conclusion: LOM框架成功解决了企业知识管理的核心挑战，通过本体与语言的深度融合实现了有效的语义推理，为大规模知识管理提供了新方法

Abstract: Enterprise-scale knowledge management faces significant challenges in integrating multi-source heterogeneous data and enabling effective semantic reasoning. Traditional knowledge graphs often struggle with implicit relationship discovery and lack sufficient semantic understanding for complex question answering. To address these limitations, we introduce a unified construct--align--reason framework, the large ontology model (LOM). We first build a dual-layer enterprise ontology from structured databases and unstructured text, subsequently fusing these sources into a comprehensive enterprise ontology. To enable instruction-aligned reasoning, we propose a unified three-stage training pipeline: ontology instruction fine-tuning to improve structural understanding; text-ontology grounding to strengthen node semantic encoding; and multi-task instruction tuning on ontology-language pairs with curriculum learning to enhance semantic reasoning and generation. We also construct comprehensive training and evaluation datasets covering diverse ontology reasoning tasks. On this benchmark, our 4B-parameter LOM achieves 89.47% accuracy and outperforms DeepSeek-V3.2 on complex graph reasoning, indicating effective fusion of ontology structure and language.

</details>


### [7] [Reversible Diffusion Decoding for Diffusion Language Models](https://arxiv.org/abs/2602.00150)
*Xinyun Wang,Min Zhang,Sen Cui,Zhikang Chen,Bo Jiang,Kun Kuang,Mingbao Lin*

Main category: cs.CL

TL;DR: 提出可逆扩散解码(RDD)框架，通过引入可逆性和回溯机制解决扩散语言模型并行生成中的停滞问题，在保持并行效率的同时提高生成鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型虽然支持并行token生成，但不可逆的承诺机制可能导致停滞问题——当上下文不理想时，反向扩散过程无法继续推进，影响生成质量。

Method: 提出可逆扩散解码(RDD)框架：1)检测停滞作为反向过程的失败状态；2)利用缓存的模型状态实现高效回溯到早期块而无需重新计算；3)应用置信度引导的重掩码策略，选择性重新初始化不确定token同时保留可靠上下文。

Result: 实验表明RDD在最小计算开销下，相比基线方法显著提高了生成鲁棒性和质量，同时保持了扩散模型并行生成的效率优势。

Conclusion: RDD通过引入可逆性解决了扩散语言模型解码中的停滞问题，实现了从早期承诺错误中恢复的能力，为并行扩散生成提供了更稳健的解码框架。

Abstract: Diffusion language models enable parallel token generation through block-wise decoding, but their irreversible commitments can lead to stagnation, where the reverse diffusion process fails to make further progress under a suboptimal context.We propose Reversible Diffusion Decoding (RDD), a decoding framework that introduces reversibility into block-wise diffusion generation. RDD detects stagnation as a state-dependent failure of the reverse process and enables efficient backtracking to earlier blocks without recomputation via cached model states. To avoid repeated failure trajectories, RDD applies confidence-guided re-masking to selectively reinitialize uncertain tokens while preserving reliable context.This reversible formulation allows decoding to recover from early commitment errors while maintaining the parallel efficiency of diffusion-based generation. Experiments show that RDD improves generation robustness and quality over baselines with minimal computational overhead.

</details>


### [8] [DIVERGE: Diversity-Enhanced RAG for Open-Ended Information Seeking](https://arxiv.org/abs/2602.00238)
*Tianyi Hu,Niket Tandon,Akhil Arora*

Main category: cs.CL

TL;DR: DIVERGE是一个插件式智能RAG框架，通过反思引导生成和记忆增强迭代优化来解决传统RAG系统在开放性问题中多样性不足的问题，实现了多样性与质量的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统假设每个查询只有一个正确答案，忽视了常见信息检索场景中存在多个合理答案的情况。这导致系统多样性不足，限制了创造性，并损害了公平包容的信息获取。

Method: 提出DIVERGE框架，包含反思引导生成和记忆增强迭代优化机制。通过智能代理的方式，在保持答案质量的同时促进多样观点的生成。

Result: 在Infinity-Chat数据集上，DIVERGE相比竞争基线和先前SOTA方法实现了最佳的多样性-质量平衡，显著提升多样性同时保持质量。新提出的评估指标与人类判断有良好相关性。

Conclusion: 当前基于LLM的系统在开放信息检索中存在系统性限制，显式建模多样性可以缓解这一问题。DIVERGE框架为促进公平包容的信息获取提供了有效解决方案。

Abstract: Existing retrieval-augmented generation (RAG) systems are primarily designed under the assumption that each query has a single correct answer. This overlooks common information-seeking scenarios with multiple plausible answers, where diversity is essential to avoid collapsing to a single dominant response, thereby constraining creativity and compromising fair and inclusive information access. Our analysis reveals a commonly overlooked limitation of standard RAG systems: they underutilize retrieved context diversity, such that increasing retrieval diversity alone does not yield diverse generations. To address this limitation, we propose DIVERGE, a plug-and-play agentic RAG framework with novel reflection-guided generation and memory-augmented iterative refinement, which promotes diverse viewpoints while preserving answer quality. We introduce novel metrics tailored to evaluating the diversity-quality trade-off in open-ended questions, and show that they correlate well with human judgments. We demonstrate that DIVERGE achieves the best diversity-quality trade-off compared to competitive baselines and previous state-of-the-art methods on the real-world Infinity-Chat dataset, substantially improving diversity while maintaining quality. More broadly, our results reveal a systematic limitation of current LLM-based systems for open-ended information-seeking and show that explicitly modeling diversity can mitigate it. Our code is available at: https://github.com/au-clan/Diverge

</details>


### [9] [Benchmarking Uncertainty Calibration in Large Language Model Long-Form Question Answering](https://arxiv.org/abs/2602.00279)
*Philip Müller,Nicholas Popovič,Michael Färber,Peter Steinbach*

Main category: cs.CL

TL;DR: 该论文提出了首个用于评估大语言模型在科学问答中不确定性量化方法的大规模基准，分析了20个不同模型在7个科学QA数据集上的表现，发现指令微调会导致概率极化，而基于答案频率的一致性方法校准效果最好。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在科学问答中应用日益广泛，但现有不确定性量化方法在需要事实检索和推理能力的科学领域验证不足。可靠的不确定性量化对于生成答案的可信采纳至关重要，需要建立系统的评估框架。

Method: 构建了首个大规模基准来评估推理密集型QA中的UQ指标，涵盖20个大语言模型（基础模型、指令微调模型和推理变体），使用7个科学QA数据集（包括多项选择和算术问答），通过提示模拟开放问答设置，在68.5万个长格式响应上评估代表性方法。

Result: 发现指令微调导致token级概率质量极化，降低了token级置信度作为不确定性估计的可靠性；序列层面，语言化方法存在系统性偏差且与正确性相关性差，而答案频率（跨样本一致性）校准效果最可靠；同时揭示了仅依赖ECE作为UQ方法性能评估指标的误导性。

Conclusion: 当前LLM不确定性量化方法存在关键局限性，标准基准评估实践也有不足。答案频率方法在序列层面提供最可靠的校准，而token级置信度在指令微调后可靠性下降。需要更全面的评估框架来推动科学问答中可信AI的发展。

Abstract: Large Language Models (LLMs) are commonly used in Question Answering (QA) settings, increasingly in the natural sciences if not science at large. Reliable Uncertainty Quantification (UQ) is critical for the trustworthy uptake of generated answers. Existing UQ approaches remain weakly validated in scientific QA, a domain relying on fact-retrieval and reasoning capabilities. We introduce the first large-scale benchmark for evaluating UQ metrics in reasoning-demanding QA studying calibration of UQ methods, providing an extensible open-source framework to reproducibly assess calibration. Our study spans up to 20 large language models of base, instruction-tuned and reasoning variants. Our analysis covers seven scientific QA datasets, including both multiple-choice and arithmetic question answering tasks, using prompting to emulate an open question answering setting. We evaluate and compare methods representative of prominent approaches on a total of 685,000 long-form responses, spanning different reasoning complexities representative of domain-specific tasks. At the token level, we find that instruction tuning induces strong probability mass polarization, reducing the reliability of token-level confidences as estimates of uncertainty. Models further fine-tuned for reasoning are exposed to the same effect, but the reasoning process appears to mitigate it depending on the provider. At the sequence level, we show that verbalized approaches are systematically biased and poorly correlated with correctness, while answer frequency (consistency across samples) yields the most reliable calibration. In the wake of our analysis, we study and report the misleading effect of relying exclusively on ECE as a sole measure for judging performance of UQ methods on benchmark datasets. Our findings expose critical limitations of current UQ methods for LLMs and standard practices in benchmarking thereof.

</details>


### [10] [Faithful-Patchscopes: Understanding and Mitigating Model Bias in Hidden Representations Explanation of Large Language Models](https://arxiv.org/abs/2602.00300)
*Xilin Gong,Shu Yang,Zehua Cao,Lynne Billard,Di Wang*

Main category: cs.CL

TL;DR: 本文发现Patchscopes框架中LLM解码隐藏表示时过度依赖语言先验而非上下文信息，提出BALOR方法通过logit重校准来抑制模型偏见并提升解释的忠实度。


<details>
  <summary>Details</summary>
Motivation: 现有Patchscopes框架使用LLM解码隐藏表示生成人类可读解释，但研究发现LLM倾向于依赖固有的语言模式，这会覆盖隐藏表示中编码的上下文信息，导致解释不忠实。

Method: 首先设计数据集评估Patchscopes在偏见情况下的忠实度；然后提出BALOR方法，将未修补提示的输出logit视为模型偏见，与修补上下文信息后的logit对比，通过重校准logit分布来抑制偏见并增强上下文信息。

Result: 评估显示Patchscopes忠实度平均下降18.84%；BALOR在多个LLM上一致优于现有基线，相对性能提升最高达33%。

Conclusion: LLM在解码隐藏表示时存在系统性不忠实问题，BALOR通过logit重校准有效抑制模型偏见，提升解释的忠实度，为隐藏表示解释提供了更可靠的方法。

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities for hidden representation interpretation through Patchscopes, a framework that uses LLMs themselves to generate human-readable explanations by decoding from internal hidden representations. However, our work shows that LLMs tend to rely on inherent linguistic patterns, which can override contextual information encoded in the hidden representations during decoding. For example, even when a hidden representation encodes the contextual attribute "purple" for "broccoli", LLMs still generate "green" in their explanations, reflecting a strong prior association. This behavior reveals a systematic unfaithfulness in Patchscopes. To systematically study this issue, we first designed a dataset to evaluate the faithfulness of Patchscopes under biased cases, and our results show that there is an 18.84\% faithfulness decrease on average. We then propose Bias Alignment through Logit Recalibration (BALOR), which treats the output logits from an unpatched prompt as capturing model bias and contrasts them with logits obtained under patched contextual information. By recalibrating the logit distribution through this contrast, BALOR suppresses model bias and amplifies contextual information during generation. Experiments across multiple LLMs demonstrate that BALOR consistently outperforms existing baselines, achieving up to 33\% relative performance improvement.

</details>


### [11] [MiNER: A Two-Stage Pipeline for Metadata Extraction from Municipal Meeting Minutes](https://arxiv.org/abs/2602.00316)
*Rodrigo Batista,Luís Filipe Cunha,Purificação Silvano,Nuno Guimarães,Alípio Jorge,Evelin Amorim,Ricardo Campos*

Main category: cs.CL

TL;DR: 提出兩階段管線從市政會議記錄提取元數據：先用QA模型定位開頭結尾段落，再用Transformer模型提取細粒度實體，並評估開源與閉源LLMs的性能、成本與碳足跡。


<details>
  <summary>Details</summary>
Motivation: 市政會議記錄格式異質且缺乏標準化，現有NER模型不適用於此領域特定類別，需要專門方法來自動提取會議編號、日期、地點、參與者等元數據。

Method: 兩階段管線：1) QA模型識別包含元數據的開頭和結尾文本段；2) 使用BERTimbau和XLM-RoBERTa（有/無CRF層）進行細粒度實體提取，並通過去詞匯化增強。評估開源（Phi）和閉源（Gemini）LLMs。

Result: 在領域內表現優於大型通用LLMs，但跨市政評估顯示泛化能力受限，反映市政記錄的變異性和語言複雜性。建立了市政會議記錄元數據提取的首個基準。

Conclusion: 提出的兩階段管線在市政會議記錄元數據提取上表現良好，為該領域未來研究奠定基礎，但跨市政泛化仍是挑戰，需進一步研究處理格式和語言變異性。

Abstract: Municipal meeting minutes are official documents of local governance, exhibiting heterogeneous formats and writing styles. Effective information retrieval (IR) requires identifying metadata such as meeting number, date, location, participants, and start/end times, elements that are rarely standardized or easy to extract automatically. Existing named entity recognition (NER) models are ill-suited to this task, as they are not adapted to such domain-specific categories. In this paper, we propose a two-stage pipeline for metadata extraction from municipal minutes. First, a question answering (QA) model identifies the opening and closing text segments containing metadata. Transformer-based models (BERTimbau and XLM-RoBERTa with and without a CRF layer) are then applied for fine-grained entity extraction and enhanced through deslexicalization. To evaluate our proposed pipeline, we benchmark both open-weight (Phi) and closed-weight (Gemini) LLMs, assessing predictive performance, inference cost, and carbon footprint. Our results demonstrate strong in-domain performance, better than larger general-purpose LLMs. However, cross-municipality evaluation reveals reduced generalization reflecting the variability and linguistic complexity of municipal records. This work establishes the first benchmark for metadata extraction from municipal meeting minutes, providing a solid foundation for future research in this domain.

</details>


### [12] [Detecting AI-Generated Content in Academic Peer Reviews](https://arxiv.org/abs/2602.00319)
*Siyuan Shen,Kai Wang*

Main category: cs.CL

TL;DR: 研究发现AI生成内容在学术同行评审中快速增长，2025年ICLR约20%、Nature Communications约12%的评审被检测为AI生成


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的普及，需要了解AI在学术同行评审中的使用情况及其对学术评价的影响

Method: 使用历史评审数据训练检测模型，应用于ICLR和Nature Communications后续评审周期，分析AI生成内容的时间趋势

Result: 2022年前AI生成内容极少，之后显著增长；2025年ICLR约20%、Nature Communications约12%评审被分类为AI生成；Nature Communications在2024年第三至第四季度增长最明显

Conclusion: AI辅助内容在同行评审中迅速增加，需要进一步研究其对学术评价的影响

Abstract: The growing availability of large language models (LLMs) has raised questions about their role in academic peer review. This study examines the temporal emergence of AI-generated content in peer reviews by applying a detection model trained on historical reviews to later review cycles at International Conference on Learning Representations (ICLR) and Nature Communications (NC). We observe minimal detection of AI-generated content before 2022, followed by a substantial increase through 2025, with approximately 20% of ICLR reviews and 12% of Nature Communications reviews classified as AI-generated in 2025. The most pronounced growth of AI-generated reviews in NC occurs between the third and fourth quarter of 2024. Together, these findings provide suggestive evidence of a rapidly increasing presence of AI-assisted content in peer review and highlight the need for further study of its implications for scholarly evaluation.

</details>


### [13] [DETOUR: An Interactive Benchmark for Dual-Agent Search and Reasoning](https://arxiv.org/abs/2602.00352)
*Li Siyan,Darshan Deshpande,Anand Kannappan,Rebecca Qian*

Main category: cs.CL

TL;DR: DETOUR是一个双智能体评估基准，用于模拟"话到嘴边"的多轮检索过程，包含1011个提示，测试模型在信息不明确情况下的检索能力。


<details>
  <summary>Details</summary>
Motivation: 现有评估智能体在"话到嘴边"检索过程中的基准仅限于单轮设置，无法真实模拟人们在对话中通过多轮交互回忆信息的过程。

Method: 设计了DETOUR双智能体评估框架：一个Primary Agent（被评估对象）通过查询一个Memory Agent来识别回忆的实体，Memory Agent在所有评估中保持一致。

Result: 当前最先进的模型在该基准上表现不佳，在所有模态（文本、图像、音频、视频）上仅达到36%的准确率。

Conclusion: 该研究强调了增强模型在信息不明确场景下能力的重要性，并提供了一个更真实模拟"话到嘴边"检索过程的评估基准。

Abstract: When recalling information in conversation, people often arrive at the recollection after multiple turns. However, existing benchmarks for evaluating agent capabilities in such tip-of-the-tongue search processes are restricted to single-turn settings. To more realistically simulate tip-of-the-tongue search, we introduce Dual-agent based Evaluation Through Obscure Under-specified Retrieval (DETOUR), a dual-agent evaluation benchmark containing 1,011 prompts. The benchmark design involves a Primary Agent, which is the subject of evaluation, tasked with identifying the recollected entity through querying a Memory Agent that is held consistent across evaluations. Our results indicate that current state-of-the-art models still struggle with our benchmark, only achieving 36% accuracy when evaluated on all modalities (text, image, audio, and video), highlighting the importance of enhancing capabilities in underspecified scenarios.

</details>


### [14] [DecompressionLM: Deterministic, Diagnostic, and Zero-Shot Concept Graph Extraction from Language Models](https://arxiv.org/abs/2602.00377)
*Zhaochen Hong,Jiaxuan You*

Main category: cs.CL

TL;DR: DecompressionLM：无需预定义查询或跨序列共享状态的零样本概念图提取框架，用于评估压缩模型的知识覆盖度


<details>
  <summary>Details</summary>
Motivation: 现有知识探测方法依赖预定义查询，只能提取已知概念，限制了发现语言模型实际编码内容的能力。需要解决传统解码探测方法的三个限制：跨序列耦合、竞争解码效应和可扩展性约束。

Method: 使用Van der Corput低差异序列结合算术解码，实现确定性的、可高度并行化的生成，无需跨序列共享状态。该方法支持零样本概念图提取，不依赖预定义查询。

Result: 激活感知量化（AWQ-4bit）将概念覆盖度提升30-170%，而均匀量化（GPTQ-Int4）导致71-86%的覆盖度崩溃。在MMLU-Pro Law模型中，发现排名最高和最低模型之间存在17个百分点的幻觉差距。

Conclusion: DecompressionLM建立了概念覆盖度作为评估压缩模型知识广度和事实基础的新维度，为模型部署提供了补充评估指标。

Abstract: Existing knowledge probing methods rely on pre-defined queries, limiting extraction to known concepts. We introduce DecompressionLM, a stateless framework for zero-shot concept graph extraction that discovers what language models encode without pre-specified queries or shared cross-sequence state. Our method targets three limitations of common decoding-based probing approaches: cross-sequence coupling that concentrates probability mass on high-frequency prefixes, competitive decoding effects that suppress long-tail concepts, and scalability constraints arising from sequential exploration. Using Van der Corput low-discrepancy sequences with arithmetic decoding, DecompressionLM enables deterministic, embarrassingly parallel generation without shared state across sequences. Across two model families and five quantization variants, we find that activation-aware quantization (AWQ-4bit) expands concept coverage by 30-170%, while uniform quantization (GPTQ-Int4) induces 71-86% coverage collapse -- divergent behaviors not reliably reflected by explanation-level perplexity. Corpus-based verification further reveals a 17-point hallucination gap between top- and bottom-ranked MMLU-Pro Law models. DecompressionLM establishes concept coverage as a complementary evaluation dimension for assessing knowledge breadth and factual grounding in compressed models useful for their deployment.

</details>


### [15] [Clause-Internal or Clause-External? Testing Turkish Reflexive Binding in Adapted versus Chain of Thought Large Language Models](https://arxiv.org/abs/2602.00380)
*Sercan Karakaş*

Main category: cs.CL

TL;DR: 评估大语言模型对土耳其语反身代词绑定关系的捕捉能力，发现不同模型在局部绑定偏好上存在显著差异


<details>
  <summary>Details</summary>
Motivation: 研究动机是评估最先进的大语言模型是否能够正确捕捉土耳其语反身代词（kendi和kendisi）的绑定关系，这对于理解模型的语言理解能力具有重要意义

Method: 构建了100个平衡的句子集，对比局部和非局部先行词；测试了两个系统：OpenAI的思维链模型和基于LLaMA-2的Trendyol-LLM-7B-base-v0.1；使用句子级困惑度和强制选择范式评估先行词选择

Result: Trendyol-LLM在约70%的试验中偏好局部绑定，表现出强烈的局部性偏误；而o1 Mini在局部和长距离解读之间几乎均匀分布选择，两个系统在绑定行为上存在显著对比

Conclusion: 不同的大语言模型在土耳其语反身代词绑定关系的处理上表现出显著差异，模型架构和训练数据对绑定偏好的形成有重要影响

Abstract: This study evaluates whether state-of-the-art large language models capture the binding relations of Turkish reflexive pronouns. We construct a balanced set of 100 sentences that pit local against non-local antecedents for the reflexives kendi and kendisi, and test two contrasting systems: an OpenAI chain-of-thought model designed for multi-step reasoning and Trendyol-LLM-7B-base-v0.1, a LLaMA-2-derived model extensively fine-tuned on Turkish data. Antecedent choice is assessed using a combined sentence-level perplexity and forced-choice paradigm. Trendyol-LLM favours local bindings in approximately 70% of trials, exhibiting a strong locality bias, whereas o1 Mini distributes its choices almost evenly between local and long-distance readings, revealing a marked contrast in binding behaviour across the two systems.

</details>


### [16] [Segment-Level Attribution for Selective Learning of Long Reasoning Traces](https://arxiv.org/abs/2602.00425)
*Siyuan Wang,Yanchen Liu,Xiang Ren*

Main category: cs.CL

TL;DR: 本文提出基于集成梯度归因的片段级选择性学习框架，通过识别推理链中真正重要的片段进行选择性微调，减少冗余内容的影响，提高模型推理效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型生成的长推理链中存在大量冗余内容（重复或截断），这些内容对答案预测贡献有限，但通过监督微调后模型会模仿这些冗余模式，反而可能降低性能。

Method: 使用集成梯度归因量化每个token对最终答案的影响，聚合为两个片段级指标：归因强度（整体影响大小）和方向一致性（token归因方向是否一致）。基于这两个指标识别重要片段（高归因强度但中等一致性，表明是反思性推理而非浅层推理），然后对这些重要片段进行选择性监督微调，对不重要片段进行损失掩码。

Result: 在多个模型和数据集上的实验表明，该方法提高了准确性和输出效率，能够更有效地从长推理轨迹中学习。

Conclusion: 通过选择性学习真正重要的推理片段，可以改善大型推理模型的性能，减少冗余内容的影响，实现更高效的推理学习。

Abstract: Large Reasoning Models (LRMs) achieve strong reasoning performance by generating long chains of thought (CoTs), yet only a small fraction of these traces meaningfully contributes to answer prediction, while the majority contains repetitive or truncated content. Such output redundancy is further propagated after supervised finetuning (SFT), as models learn to imitate verbose but uninformative patterns, which can degrade performance. To this end, we incorporate integrated gradient attribution to quantify each token's influence on final answers and aggregate them into two segment-level metrics: (1) \textit{attribution strength} measures the overall attribution magnitude; and (2) \textit{direction consistency} captures whether tokens' attributions within a segment are uniformly positive or negative (high consistency), or a mixture of both (moderate consistency). Based on these two metrics, we propose a segment-level selective learning framework to identify important segments with high attribution strength but moderate consistency that indicate reflective rather than shallow reasoning. The framework then applies selective SFT on these important segments while masking loss for unimportant ones. Experiments across multiple models and datasets show that our approach improves accuracy and output efficiency, enabling more effective learning from long reasoning traces~\footnote{Code and data are available at https://github.com/SiyuanWangw/SegmentSelectiveSFT}.

</details>


### [17] [When Agents "Misremember" Collectively: Exploring the Mandela Effect in LLM-based Multi-Agent Systems](https://arxiv.org/abs/2602.00428)
*Naen Xu,Hengyu An,Shuo Shi,Jinghuai Zhang,Chunyi Zhou,Changjiang Li,Tianyu Du,Zhihui Fu,Jun Wang,Shouling Ji*

Main category: cs.CL

TL;DR: 该论文研究了基于大语言模型的多智能体系统中的曼德拉效应（集体记忆偏差），提出了MANBENCH基准来评估该效应，并提出了缓解策略，平均减少74.40%的效应。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型增强了多智能体系统的能力，但智能体对集体认知偏差的易感性尚未得到充分研究。曼德拉效应作为集体错误记忆的现象，在多智能体系统中可能传播错误信息，存在伦理问题，需要系统研究。

Method: 提出MANBENCH基准，包含四种易受曼德拉效应影响的任务类型和五种不同智能体角色与记忆时间尺度的交互协议。评估多个LLM驱动的智能体，分析影响因素，并提出缓解策略：包括提示级防御（认知锚定和来源审查）和模型级对齐防御。

Result: 在MANBENCH上量化了曼德拉效应，分析了不同因素的影响。提出的缓解策略相比基线平均减少了74.40%的曼德拉效应。

Conclusion: 研究揭示了LLM多智能体系统中曼德拉效应的存在、成因和缓解方法，为开发更具韧性和伦理对齐的协作多智能体系统提供了重要见解。

Abstract: Recent advancements in large language models (LLMs) have significantly enhanced the capabilities of collaborative multi-agent systems, enabling them to address complex challenges. However, within these multi-agent systems, the susceptibility of agents to collective cognitive biases remains an underexplored issue. A compelling example is the Mandela effect, a phenomenon where groups collectively misremember past events as a result of false details reinforced through social influence and internalized misinformation. This vulnerability limits our understanding of memory bias in multi-agent systems and raises ethical concerns about the potential spread of misinformation. In this paper, we conduct a comprehensive study on the Mandela effect in LLM-based multi-agent systems, focusing on its existence, causing factors, and mitigation strategies. We propose MANBENCH, a novel benchmark designed to evaluate agent behaviors across four common task types that are susceptible to the Mandela effect, using five interaction protocols that vary in agent roles and memory timescales. We evaluate agents powered by several LLMs on MANBENCH to quantify the Mandela effect and analyze how different factors affect it. Moreover, we propose strategies to mitigate this effect, including prompt-level defenses (e.g., cognitive anchoring and source scrutiny) and model-level alignment-based defense, achieving an average 74.40% reduction in the Mandela effect compared to the baseline. Our findings provide valuable insights for developing more resilient and ethically aligned collaborative multi-agent systems.

</details>


### [18] [What Matters to an LLM? Behavioral and Computational Evidences from Summarization](https://arxiv.org/abs/2602.00459)
*Yongxin Zhou,Changshun Wu,Philippe Mulhem,Didier Schwab,Maxime Peyrard*

Main category: cs.CL

TL;DR: LLMs在摘要生成中表现出一致的内部重要性判断模式，与早期模型不同，且模型家族比规模更能影响重要性判断。研究发现某些注意力头与重要性分布对齐，中后层网络对重要性预测最强。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在摘要生成上达到SOTA水平，但其内部的信息重要性判断机制仍然不透明。研究者希望揭示LLMs在摘要任务中如何选择和优先处理信息，以及这种重要性判断在模型内部是如何表示的。

Method: 结合行为分析和计算分析：1）行为上，为每个文档生成一系列长度控制的摘要，基于每个信息单元被选择的频率构建经验重要性分布；2）计算上，识别与经验重要性分布对齐的注意力头，分析不同网络层对重要性预测的能力。

Result: 1）LLMs在重要性判断上表现出高度一致性，与pre-LLM基线模型有显著差异；2）LLMs按模型家族聚类，而非按模型规模；3）某些注意力头与经验重要性分布对齐良好；4）中后层网络对重要性预测能力最强。

Conclusion: 该研究初步揭示了LLMs在摘要生成中的信息优先级判断机制及其内部表示方式，为解释和最终控制这些模型中的信息选择开辟了路径。

Abstract: Large Language Models (LLMs) are now state-of-the-art at summarization, yet the internal notion of importance that drives their information selections remains hidden. We propose to investigate this by combining behavioral and computational analyses. Behaviorally, we generate a series of length-controlled summaries for each document and derive empirical importance distributions based on how often each information unit is selected. These reveal that LLMs converge on consistent importance patterns, sharply different from pre-LLM baselines, and that LLMs cluster more by family than by size. Computationally, we identify that certain attention heads align well with empirical importance distributions, and that middle-to-late layers are strongly predictive of importance. Together, these results provide initial insights into what LLMs prioritize in summarization and how this priority is internally represented, opening a path toward interpreting and ultimately controlling information selection in these models.

</details>


### [19] [Words that make SENSE: Sensorimotor Norms in Learned Lexical Token Representations](https://arxiv.org/abs/2602.00469)
*Abhinav Gupta,Toben H. Mintz,Jesse Thomason*

Main category: cs.CL

TL;DR: SENSE模型通过词嵌入预测感官运动规范，行为实验验证了其在6个模态上与人类感知的相关性，并发现内感受规范中存在系统的音素模式。


<details>
  <summary>Details</summary>
Motivation: 传统词嵌入仅基于共现模式，而人类语言理解根植于感官和运动体验。需要将词嵌入与感官运动经验联系起来。

Method: 提出SENSE模型（感官运动嵌入规范评分引擎），通过学习投影模型从词嵌入预测Lancaster感官运动规范。同时进行行为研究，让281名参与者从候选假词中选择具有特定感官运动关联的词。

Result: SENSE评分与人类选择率在11个模态中的6个存在显著相关性。对假词选择率的亚词汇分析揭示了内感受规范中系统的音素模式。

Conclusion: SENSE模型能够有效连接词嵌入与感官运动经验，为从文本数据计算提出候选音素模式提供了途径。

Abstract: While word embeddings derive meaning from co-occurrence patterns, human language understanding is grounded in sensory and motor experience. We present $\text{SENSE}$ $(\textbf{S}\text{ensorimotor }$ $\textbf{E}\text{mbedding }$ $\textbf{N}\text{orm }$ $\textbf{S}\text{coring }$ $\textbf{E}\text{ngine})$, a learned projection model that predicts Lancaster sensorimotor norms from word lexical embeddings. We also conducted a behavioral study where 281 participants selected which among candidate nonce words evoked specific sensorimotor associations, finding statistically significant correlations between human selection rates and $\text{SENSE}$ ratings across 6 of the 11 modalities. Sublexical analysis of these nonce words selection rates revealed systematic phonosthemic patterns for the interoceptive norm, suggesting a path towards computationally proposing candidate phonosthemes from text data.

</details>


### [20] [Intention-Adaptive LLM Fine-Tuning for Text Revision Generation](https://arxiv.org/abs/2602.00477)
*Zhexiong Liu,Diane Litman*

Main category: cs.CL

TL;DR: Intention-Tuning：一种基于意图的自适应分层LLM微调框架，用于在小型修订语料上高效生成反映作者实际意图的修订文本


<details>
  <summary>Details</summary>
Motivation: LLM在基于上下文的文本生成任务中表现出色，但在基于意图的生成任务（如修订生成）中应用不足。现有方法难以处理复杂的多意图场景，而微调需要大量标注数据，这在修订领域既昂贵又稀缺。

Method: 提出Intention-Tuning框架，通过自适应选择LLM层子集来学习意图，然后将这些表示迁移到修订生成任务中，实现层级的动态选择和学习。

Result: 实验结果表明，Intention-Tuning在小型修订语料上既有效又高效，优于多个参数高效微调基线方法。

Conclusion: Intention-Tuning为解决基于意图的生成任务提供了一种高效解决方案，特别是在数据稀缺的修订生成领域，能够更好地处理复杂多意图场景。

Abstract: Large Language Models (LLMs) have achieved impressive capabilities in various context-based text generation tasks, such as summarization and reasoning; however, their applications in intention-based generation tasks remain underexplored. One such example is revision generation, which requires the generated text to explicitly reflect the writer's actual intentions. Identifying intentions and generating desirable revisions are challenging due to their complex and diverse nature. Although prior work has employed LLMs to generate revisions with few-shot learning, they struggle with handling entangled multi-intent scenarios. While fine-tuning LLMs using intention-based instructions appears promising, it demands large amounts of annotated data, which is expensive and scarce in the revision community. To address these challenges, we propose Intention-Tuning, an intention-adaptive layer-wise LLM fine-tuning framework that dynamically selects a subset of LLM layers to learn the intentions and subsequently transfers their representations to revision generation. Experimental results suggest that Intention-Tuning is effective and efficient on small revision corpora, outperforming several PEFT baselines.

</details>


### [21] [From Knowledge to Inference: Scaling Laws of Specialized Reasoning on GlobalHealthAtlas](https://arxiv.org/abs/2602.00491)
*Zhaokun Yan,Zhaohan Liu,Wuzheng Dong,Lijie Feng,Chengxiao Dai*

Main category: cs.CL

TL;DR: 提出了GlobalHealthAtlas数据集，包含28万条多语言公共卫生实例，涵盖15个领域和17种语言，分为三个难度级别，并设计了LLM辅助的质量控制流程和领域对齐评估器。


<details>
  <summary>Details</summary>
Motivation: 公共卫生推理需要基于科学证据、专家共识和安全约束进行群体层面推断，但目前作为结构化机器学习问题研究不足，缺乏监督信号和基准数据集。

Method: 1) 构建大规模多语言数据集GlobalHealthAtlas；2) 设计LLM辅助的构建和质量控制流程，包括检索、去重、证据基础检查和标签验证；3) 提出从多样化LLM高置信度判断中蒸馏出的领域对齐评估器。

Result: 创建了包含280,210个实例的数据集，涵盖15个公共卫生领域和17种语言，按健康素养、流行病学推理和政策推理三个难度级别分层，并开发了六维评估框架。

Conclusion: 这些贡献使得能够在安全关键的公共卫生推理领域进行可重复的LLM训练和评估，超越了传统的问答基准，为公共卫生AI应用提供了重要基础设施。

Abstract: Public health reasoning requires population level inference grounded in scientific evidence, expert consensus, and safety constraints. However, it remains underexplored as a structured machine learning problem with limited supervised signals and benchmarks. We introduce \textbf{GlobalHealthAtlas}, a large scale multilingual dataset of 280,210 instances spanning 15 public health domains and 17 languages, stratified into three difficulty levels from health literacy to epidemiological and policy reasoning. Instances are derived from openly available public health sources and labeled by language, domain, and difficulty to support supervised learning and slice based evaluation. We further propose large language model (LLM) assisted construction and quality control pipeline with retrieval, duplication, evidence grounding checks, and label validation to improve consistency at scale. Finally, we present a domain aligned evaluator distilled from high confidence judgments of diverse LLMs to assess outputs along six dimensions: Accuracy, Reasoning, Completeness, Consensus Alignment, Terminology Norms, and Insightfulness. Together, these contributions enable reproducible training and evaluation of LLMs for safety critical public health reasoning beyond conventional QA benchmarks.

</details>


### [22] [Culturally-Grounded Governance for Multilingual Language Models: Rights, Data Boundaries, and Accountable AI Design](https://arxiv.org/abs/2602.00497)
*Hanjing Shi,Dominic DiFranzo*

Main category: cs.CL

TL;DR: 论文提出基于文化的多语言大语言模型治理框架，以解决现有英语中心治理框架在低资源语言和文化边缘化社区中的系统性风险问题。


<details>
  <summary>Details</summary>
Motivation: 当前多语言大语言模型的治理框架主要基于英语中心数据、同质化用户群体和抽象公平概念，忽视了文化、语言和政治背景的多样性，导致低资源语言和文化边缘化社区面临系统性风险。

Method: 综合现有关于多语言模型行为、数据不对称和社会技术危害的证据，结合人机交互和AI治理的跨文化视角，构建基于文化的治理框架，识别三个相互关联的治理挑战。

Result: 提出将多语言AI治理重新定义为社会文化和基于权利的问题的概念议程，而非新的技术基准，并概述数据管理、透明度和参与式问责的设计和政策影响。

Conclusion: 基于文化的治理对于确保多语言语言模型不会在规模和中性化的幌子下复制现有的全球不平等至关重要，需要关注数据不对称、本地规范对齐和问责机制等核心挑战。

Abstract: Multilingual large language models (MLLMs) are increasingly deployed across cultural, linguistic, and political contexts, yet existing governance frameworks largely assume English-centric data, homogeneous user populations, and abstract notions of fairness. This creates systematic risks for low-resource languages and culturally marginalized communities, where data practices, model behavior, and accountability mechanisms often fail to align with local norms, rights, and expectations. Drawing on cross-cultural perspectives in human-centered computing and AI governance, this paper synthesizes existing evidence on multilingual model behavior, data asymmetries, and sociotechnical harm, and articulates a culturally grounded governance framework for MLLMs. We identify three interrelated governance challenges: cultural and linguistic inequities in training data and evaluation practices, misalignment between global deployment and locally situated norms, values, and power structures, and limited accountability mechanisms for addressing harms experienced by marginalized language communities. Rather than proposing new technical benchmarks, we contribute a conceptual agenda that reframes multilingual AI governance as a sociocultural and rights based problem. We outline design and policy implications for data stewardship, transparency, and participatory accountability, and argue that culturally grounded governance is essential for ensuring that multilingual language models do not reproduce existing global inequalities under the guise of scale and neutrality.

</details>


### [23] [Reasoning by Commented Code for Table Question Answering](https://arxiv.org/abs/2602.00543)
*Seho Pyo,Jiheon Seok,Jaejin Lee*

Main category: cs.CL

TL;DR: 提出一个带注释的逐步代码生成框架，通过将表格问答分解为多行可执行程序并添加自然语言注释，提升推理清晰度和代码正确率，在WikiTableQuestions基准上达到84.3%准确率。


<details>
  <summary>Details</summary>
Motivation: 传统表格线性化方法破坏了结构化数据的二维关系，现有基于端到端答案生成或单行程序查询的方法在数值准确性和可解释性方面存在局限。

Method: 提出带注释的逐步代码生成框架，将表格问答推理分解为多行可执行Python程序，每行代码附带简洁的自然语言注释，明确推理过程，并可与端到端表格问答模型通过轻量级答案选择机制结合。

Result: 在WikiTableQuestions基准上，使用Qwen2.5-Coder-7B-Instruct达到70.9%准确率，超越Repanda基线（67.6%）。与端到端模型结合后，准确率进一步提升至84.3%。

Conclusion: 带注释的逐步代码生成框架通过明确推理过程，显著提升了表格问答的准确性和可解释性，与现有模型的有效结合展示了其实际应用潜力。

Abstract: Table Question Answering (TableQA) poses a significant challenge for large language models (LLMs) because conventional linearization of tables often disrupts the two-dimensional relationships intrinsic to structured data. Existing methods, which depend on end-to-end answer generation or single-line program queries, typically exhibit limited numerical accuracy and reduced interpretability. This work introduces a commented, step-by-step code-generation framework that incorporates explicit reasoning into the Python program-generation process. The approach decomposes TableQA reasoning into multi-line executable programs with concise natural language comments, thereby promoting clearer reasoning and increasing the likelihood of generating correct code. On the WikiTableQuestions benchmark, the proposed method achieves 70.9\% accuracy using Qwen2.5-Coder-7B-Instruct, surpassing the Repanda baseline (67.6\%). Integrating the proposed framework with a robust end-to-end TableQA model via a lightweight answer-selection mechanism yields further improvements. This combined approach achieves up to 84.3\% accuracy on the WikiTableQuestions benchmark.

</details>


### [24] [A Hierarchical and Attentional Analysis of Argument Structure Constructions in BERT Using Naturalistic Corpora](https://arxiv.org/abs/2602.00554)
*Liu Kaipeng,Wu Ling*

Main category: cs.CL

TL;DR: BERT模型对四种基本论元结构构式的处理呈现层次化表征结构：早期层出现构式特定信息，中间层形成最大可分离聚类，后期层维持这些表征。


<details>
  <summary>Details</summary>
Motivation: 研究BERT模型如何处理四种基本论元结构构式，探索模型内部对语法结构的表征机制。

Method: 采用多维分析框架：MDS和t-SNE进行降维，GDV评估聚类分离度，FDR进行线性诊断探测，以及注意力机制分析。

Result: 发现层次化表征结构：构式特定信息在早期层出现，在中间层形成最大可分离聚类，并在后期处理阶段得以维持。

Conclusion: BERT模型对论元结构构式的处理呈现系统性的层次化表征模式，为理解Transformer模型处理语法结构提供了新见解。

Abstract: This study investigates how the Bidirectional Encoder Representations from Transformers model processes four fundamental Argument Structure Constructions. We employ a multi-dimensional analytical framework, which integrates MDS, t-SNE as dimensionality reduction, Generalized Discrimination Value (GDV) as cluster separation metrics, Fisher Discriminant Ratio (FDR) as linear diagnostic probing, and attention mechanism analysis. Our results reveal a hierarchical representational structure. Construction-specific information emerges in early layers, forms maximally separable clusters in middle layers, and is maintained through later processing stages.

</details>


### [25] [The French Drama Revolution: Political Economy and Literary Production, 1700-1900](https://arxiv.org/abs/2602.00588)
*Thiago Dumont Oliveira*

Main category: cs.CL

TL;DR: 使用LDA和JSD分析1700-1900年法国戏剧主题演变，发现法国大革命后戏剧主题分布发生深刻变化，资产阶级主题崛起，并与法国GDP增长存在共演化关系。


<details>
  <summary>Details</summary>
Motivation: 研究法国戏剧在1700-1900年间如何随着政治经济变革而演变，特别是法国大革命和工业化对戏剧主题的影响。

Method: 使用潜在狄利克雷分配(LDA)进行主题建模，结合Jensen-Shannon散度(JSD)分析主题分布变化，并将主题年度流行度与法国GDP数据进行对比分析。

Result: 法国戏剧的主题分布在法国大革命后（特别是1789-1850年间）发生深刻变化，资产阶级主题从18世纪末开始成为最流行的主题之一，戏剧主题演变与法国经济增长存在共演化关系。

Conclusion: 法国戏剧的主题演变反映了法国大革命和工业化带来的政治经济变革，戏剧作为文化产物与社会经济发展密切相关，资产阶级主题的崛起体现了社会结构的变化。

Abstract: This paper investigates the changing nature of French drama between 1700-1900 using Latent Dirichlet Allocation and Jensen-Shannon Divergence. Results indicate that the topical distribution of French drama changed profoundly after the French Revolution, particularly between 1789 and 1850. Bourgeois themes emerged among the most prevalent topics since the late 18th century. To assess the coevolution of drama and economic growth, I plot the yearly prevalence of topics alongside French GDP between 1700-1900, and discuss these changes in light of the political and economic changes prompted by the French Revolution and the industrialization of the country.

</details>


### [26] [Kanade: A Simple Disentangled Tokenizer for Spoken Language Modeling](https://arxiv.org/abs/2602.00594)
*Zhijie Huang,Stephen McIntosh,Daisuke Saito,Nobuaki Minematsu*

Main category: cs.CL

TL;DR: Kanade是一个单层解耦语音分词器，能分离声学常量，生成单一token流捕获丰富语音和韵律信息，无需辅助方法，在说话人解耦和词汇可用性方面达到SOTA，同时保持优秀重建质量。


<details>
  <summary>Details</summary>
Motivation: 好的语言模型始于好的分词器。对于语音建模尤其重要，因为需要处理混合语言和非语言信息的连续信号。语音分词器应提取语音和韵律，抑制说话人身份等语言无关信息，并支持高质量合成。

Method: 提出Kanade单层解耦语音分词器，分离声学常量创建单一token流捕获丰富语音和韵律信息，无需现有解耦编解码器常依赖的辅助方法。

Result: 实验显示Kanade在说话人解耦和词汇可用性方面达到最先进水平，同时保持优秀重建质量。

Conclusion: Kanade实现了理想的语音分词器设计，能有效分离语言相关特征，为语音建模提供高质量基础。

Abstract: A good language model starts with a good tokenizer. Tokenization is especially important for speech modeling, which must handle continuous signals that mix linguistic and non-linguistic information. A speech tokenizer should extract phonetics and prosody, suppress linguistically irrelevant information like speaker identity, and enable high-quality synthesis. We present Kanade, a single-layer disentangled speech tokenizer that realizes this ideal. Kanade separates out acoustic constants to create a single stream of tokens that captures rich phonetics and prosody. It does so without the need for auxiliary methods that existing disentangled codecs often rely on. Experiments show that Kanade achieves state-of-the-art speaker disentanglement and lexical availability, while maintaining excellent reconstruction quality.

</details>


### [27] [Hermes the Polyglot: A Unified Framework to Enhance Expressiveness for Multimodal Interlingual Subtitling](https://arxiv.org/abs/2602.00597)
*Chaoqun Cui,Shijing Wang,Liangbin Huang,Qingqing Gu,Zhaolong Huang,Xiao Zeng,Wenji Mao*

Main category: cs.CL

TL;DR: Hermes是一个基于大语言模型的自动字幕翻译框架，通过说话人分割、术语识别和表达增强三个模块，解决了字幕翻译中的语义连贯性、代词术语翻译和表达力等挑战。


<details>
  <summary>Details</summary>
Motivation: 跨语言字幕翻译在娱乐本地化中至关重要，但尚未在机器翻译中得到充分探索。虽然大语言模型显著提升了机器翻译的一般能力，但字幕文本的独特特征（如语义连贯性、代词术语翻译和翻译表达力）仍然是持续挑战。

Method: 提出Hermes框架，包含三个核心模块：1) 说话人分割模块，用于识别不同说话人；2) 术语识别模块，用于准确翻译专业术语；3) 表达增强模块，用于提升翻译的表达力和自然度。

Result: 实验表明，Hermes在说话人分割方面达到最先进的性能，并生成表达力强、上下文连贯的翻译，从而推动了跨语言字幕翻译的研究进展。

Conclusion: Hermes框架有效解决了字幕翻译中的关键挑战，通过集成说话人分割、术语识别和表达增强模块，实现了高质量的跨语言字幕翻译，为娱乐本地化提供了实用的自动化解决方案。

Abstract: Interlingual subtitling, which translates subtitles of visual media into a target language, is essential for entertainment localization but has not yet been explored in machine translation. Although Large Language Models (LLMs) have significantly advanced the general capabilities of machine translation, the distinctive characteristics of subtitle texts pose persistent challenges in interlingual subtitling, particularly regarding semantic coherence, pronoun and terminology translation, and translation expressiveness. To address these issues, we present Hermes, an LLM-based automated subtitling framework. Hermes integrates three modules: Speaker Diarization, Terminology Identification, and Expressiveness Enhancement, which effectively tackle the above challenges. Experiments demonstrate that Hermes achieves state-of-the-art diarization performance and generates expressive, contextually coherent translations, thereby advancing research in interlingual subtitling.

</details>


### [28] [Lookahead-then-Verify: Reliable Constrained Decoding for Diffusion LLMs under Context-Free Grammars](https://arxiv.org/abs/2602.00612)
*Yitong Zhang,Yongmin Li,Yuetong Liu,Jia Li,Xiaoran Jia,Zherui Li,Ge Li*

Main category: cs.CL

TL;DR: LAVE是一种专门为扩散大语言模型设计的约束解码方法，通过前瞻验证确保生成语法正确的输出，显著提高可靠性且计算开销可忽略。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型在生成形式语言（如源代码、化学表达式）时难以可靠地生成语法正确的输出。现有约束解码方法要么不适用于非自回归的dLLMs，要么允许中间输出无法完成成有效句子，限制了实际可靠性。

Method: LAVE利用dLLMs能在单次前向传播中并行预测所有位置token分布的特性，每当模型提出新token时，使用这些分布进行前瞻验证，高效可靠地检查提议token的有效性，确保中间输出始终能扩展为有效句子。

Result: 在四个广泛使用的dLLMs和三个代表性基准测试上的实验表明，LAVE始终优于现有基线方法，在语法正确性方面实现显著提升，同时带来可忽略的运行时间开销。

Conclusion: LAVE是一种专门为扩散大语言模型设计的有效约束解码方法，通过前瞻验证机制可靠地保证语法正确性，解决了dLLMs在形式语言生成中的关键挑战。

Abstract: Diffusion Large Language Models (dLLMs) have demonstrated promising generative capabilities and are increasingly used to produce formal languages defined by context-free grammars, such as source code and chemical expressions. However, as probabilistic models, they still struggle to generate syntactically valid outputs reliably. A natural and promising direction to address this issue is to adapt constrained decoding techniques to enforce grammatical correctness during generation. However, applying these techniques faces two primary obstacles. On the one hand, the non-autoregressive nature of dLLMs renders most existing constrained decoding approaches inapplicable. On the other hand, current approaches specifically designed for dLLMs may allow intermediate outputs that are impossible to complete into valid sentences, which significantly limits their reliability in practice.
  To address these challenges, we present LAVE, a constrained decoding approach specifically designed for dLLMs. Our approach leverages a key property of dLLMs, namely their ability to predict token distributions for all positions in parallel during each forward pass. Whenever a new token is proposed by model, LAVE performs lookahead using these distributions to efficiently and reliably verify the validity of the proposed token. This design ensures reliable constraints by reliably preserving the potential for intermediate outputs to be extended into valid sentences. Extensive experiments across four widely used dLLMs and three representative benchmarks demonstrate that LAVE consistently outperforms existing baselines and achieves substantial improvements in syntactic correctness, while incurring negligible runtime overhead.

</details>


### [29] [Transformer-Based Model for Multilingual Hope Speech Detection](https://arxiv.org/abs/2602.00613)
*Nsrin Ashraf,Mariam Labib,Hamada Nayel*

Main category: cs.CL

TL;DR: 该论文提出了一个用于英语和德语希望语音检测的系统，使用RoBERTa和XLM-RoBERTa模型，在RANLP2025的PolyHope-M任务中取得了良好性能。


<details>
  <summary>Details</summary>
Motivation: 希望语音检测是自然语言处理中的重要任务，旨在识别文本中表达希望、积极情绪的内容。该研究旨在探索预训练大语言模型在多语言希望语音检测中的性能，特别是针对英语和德语两种语言。

Method: 系统实现了多种transformer模型进行评估：对于英语使用RoBERTa模型，对于英语和德语使用多语言模型XLM-RoBERTa。这些模型在PolyHope-M数据集上进行训练和评估。

Result: RoBERTa在英语希望语音检测中取得了加权F1分数0.818和准确率81.8%；XLM-RoBERTa在英语和德语任务中取得了加权F1分数0.786和准确率78.5%。

Conclusion: 研究结果表明预训练大语言模型在希望语音检测任务中具有重要作用，能够显著提升自然语言处理任务的性能，为多语言情感分析提供了有效解决方案。

Abstract: This paper describes a system that has been submitted to the "PolyHope-M" at RANLP2025. In this work various transformers have been implemented and evaluated for hope speech detection for English and Germany. RoBERTa has been implemented for English, while the multilingual model XLM-RoBERTa has been implemented for both English and German languages. The proposed system using RoBERTa reported a weighted f1-score of 0.818 and an accuracy of 81.8% for English. On the other hand, XLM-RoBERTa achieved a weighted f1-score of 0.786 and an accuracy of 78.5%. These results reflects the importance of improvement of pre-trained large language models and how these models enhancing the performance of different natural language processing tasks.

</details>


### [30] [Jailbreaking LLMs via Calibration](https://arxiv.org/abs/2602.00619)
*Yuxuan Lu,Yongkang Guo,Yuqing Kong*

Main category: cs.CL

TL;DR: 提出一个将安全对齐视为预对齐分布系统性扭曲的框架，将弱到强越狱建模为预测聚合问题，推导出最优聚合策略，并展示现有方法只是该框架在交叉熵损失下的特例。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的安全对齐往往造成模型对齐输出与底层预对齐数据分布之间的系统性差异，现有越狱方法缺乏统一的理论框架。

Method: 将安全对齐对下一个token预测的影响建模为预对齐分布的系统性扭曲，将弱到强越狱视为预测聚合问题，推导出损失诱导对偶空间中的梯度偏移最优聚合策略。

Result: 在红队基准测试和数学效用任务中，该方法相比现有方法获得更高的攻击成功率，在安全强化的gpt-oss-120b上表现尤为突出，且"越狱税"更低。

Conclusion: 提出的框架为理解安全对齐和越狱提供了统一的理论基础，展示了对数算术越狱方法只是该框架在交叉熵损失下的特例，并提出了更广泛的聚合规则家族。

Abstract: Safety alignment in Large Language Models (LLMs) often creates a systematic discrepancy between a model's aligned output and the underlying pre-aligned data distribution. We propose a framework in which the effect of safety alignment on next-token prediction is modeled as a systematic distortion of a pre-alignment distribution. We cast Weak-to-Strong Jailbreaking as a forecast aggregation problem and derive an optimal aggregation strategy characterized by a Gradient Shift in the loss-induced dual space. We show that logit-arithmetic jailbreaking methods are a special case of this framework under cross-entropy loss, and derive a broader family of aggregation rules corresponding to other proper losses. We also propose a new hybrid aggregation rule. Evaluations across red-teaming benchmarks and math utility tasks using frontier models demonstrate that our approach achieves superior Attack Success Rates and lower "Jailbreak Tax" compared with existing methods, especially on the safety-hardened gpt-oss-120b.

</details>


### [31] [Formal Semantic Control over Language Models](https://arxiv.org/abs/2602.00638)
*Yingji Zhang*

Main category: cs.CL

TL;DR: 该论文通过VAE框架提升语言表示/模型的语义和几何可解释性，实现局部、准符号、组合式控制，包括句子级和推理级两个方向的研究。


<details>
  <summary>Details</summary>
Motivation: 使语言表示或模型在语义和几何上更可解释，并通过塑造潜在空间几何实现局部化、准符号化、组合式的控制。

Method: 在VAE框架下探索两个互补方向：(1) 句子级学习与控制：在潜在空间中解耦和操纵特定语义特征来指导句子生成；(2) 推理级学习与控制：在潜在空间中隔离和引导推理行为来控制自然语言推理。

Result: 引入一系列新颖的理论框架和实践方法，通过相应实验证明，所提出的方法增强了自然语言潜在空间的可解释性和可控性。

Conclusion: 该研究推动了语言模型内部语义表示能够被系统解释、精确构建和可靠引导的目标，为实现更可解释和可控的语言模型提供了新途径。

Abstract: This thesis advances semantic representation learning to render language representations or models more semantically and geometrically interpretable, and to enable localised, quasi-symbolic, compositional control through deliberate shaping of their latent space geometry. We pursue this goal within a VAE framework, exploring two complementary research directions: (i) Sentence-level learning and control: disentangling and manipulating specific semantic features in the latent space to guide sentence generation, with explanatory text serving as the testbed; and (ii) Reasoning-level learning and control: isolating and steering inference behaviours in the latent space to control NLI. In this direction, we focus on Explanatory NLI tasks, in which two premises (explanations) are provided to infer a conclusion. The overarching objective is to move toward language models whose internal semantic representations can be systematically interpreted, precisely structured, and reliably directed. We introduce a set of novel theoretical frameworks and practical methodologies, together with corresponding experiments, to demonstrate that our approaches enhance both the interpretability and controllability of latent spaces for natural language across the thesis.

</details>


### [32] [Culinary Crossroads: A RAG Framework for Enhancing Diversity in Cross-Cultural Recipe Adaptation](https://arxiv.org/abs/2507.21934)
*Tianyi Hu,Andrea Morales-Garzón,Jingyi Zheng,Maria Maistro,Daniel Hershcovich*

Main category: cs.CL

TL;DR: CARRIAGE是一个用于跨文化食谱适应的RAG框架，通过增强检索和上下文组织的多样性来解决传统RAG在创造性任务中输出多样性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 传统RAG在跨文化食谱适应任务中过度依赖有限的上下文，无法利用多样化的上下文输入生成多样化的输出，限制了其应对多种用户偏好和饮食需求的能力。

Method: 提出CARRIAGE框架，这是一个即插即用的RAG系统，通过增强检索阶段的多样性和改进上下文组织方式，确保生成高度多样化的食谱适应结果。

Result: 实验表明CARRIAGE在食谱适应的多样性和质量方面达到了帕累托效率，优于封闭式LLM方法。

Conclusion: CARRIAGE是首个明确旨在生成高度多样化输出的RAG框架，成功解决了传统RAG在创造性任务中多样性不足的问题，能够更好地适应多种用户偏好。

Abstract: In cross-cultural recipe adaptation, the goal is not only to ensure cultural appropriateness and retain the original dish's essence, but also to provide diverse options for various dietary needs and preferences. Retrieval Augmented Generation (RAG) is a promising approach, combining the retrieval of real recipes from the target cuisine for cultural adaptability with large language models (LLMs) for relevance. However, it remains unclear whether RAG can generate diverse adaptation results. Our analysis shows that RAG tends to overly rely on a limited portion of the context across generations, failing to produce diverse outputs even when provided with varied contextual inputs. This reveals a key limitation of RAG in creative tasks with multiple valid answers: it fails to leverage contextual diversity for generating varied responses. To address this issue, we propose CARRIAGE, a plug-and-play RAG framework for cross-cultural recipe adaptation that enhances diversity in both retrieval and context organization. To our knowledge, this is the first RAG framework that explicitly aims to generate highly diverse outputs to accommodate multiple user preferences. Our experiments show that CARRIAGE achieves Pareto efficiency in terms of diversity and quality of recipe adaptation compared to closed-book LLMs.

</details>


### [33] [LegalOne: A Family of Foundation Models for Reliable Legal Reasoning](https://arxiv.org/abs/2602.00642)
*Haitao Li,Yifan Chen,Shuo Miao,Qian Dong,Jia Chen,Yiran Hu,Junjie Chen,Minghao Qin,Qingyao Ai,Yiqun Liu,Cheng Luo,Quan Zhou,Ya Zhang,Jikun Hu*

Main category: cs.CL

TL;DR: LegalOne是一个专门针对中国法律领域设计的基座模型，通过三阶段训练流程（中训练、监督微调、课程强化学习）实现法律推理能力，在多项法律任务上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型展现出强大的通用能力，但在法律领域的直接应用受到限制，主要因为缺乏精确的领域知识和执行严谨多步司法推理的复杂性。需要专门的法律领域模型来解决这些问题。

Method: 采用三阶段训练流程：1) 中训练阶段使用塑性调整采样(PAS)平衡新知识获取和原有能力保留；2) 监督微调阶段使用法律代理思维链蒸馏(LEAD)从原始法律文本中提取显式推理；3) 课程强化学习策略，通过记忆、理解、推理的渐进过程实现从简单模式匹配到自主可靠法律推理的演进。

Result: LegalOne在广泛的法律任务上实现了最先进的性能，通过增强的知识密度和效率超越了参数规模大得多的通用大语言模型。作者公开了LegalOne权重和LegalKit评估框架。

Conclusion: LegalOne为法律AI领域提供了可信赖且可解释的基座模型，为在高风险司法应用中部署奠定了基础。该模型通过专门的三阶段训练流程成功解决了法律领域推理的挑战。

Abstract: While Large Language Models (LLMs) have demonstrated impressive general capabilities, their direct application in the legal domain is often hindered by a lack of precise domain knowledge and complexity of performing rigorous multi-step judicial reasoning. To address this gap, we present LegalOne, a family of foundational models specifically tailored for the Chinese legal domain. LegalOne is developed through a comprehensive three-phase pipeline designed to master legal reasoning. First, during mid-training phase, we propose Plasticity-Adjusted Sampling (PAS) to address the challenge of domain adaptation. This perplexity-based scheduler strikes a balance between the acquisition of new knowledge and the retention of original capabilities, effectively establishing a robust legal foundation. Second, during supervised fine-tuning, we employ Legal Agentic CoT Distillation (LEAD) to distill explicit reasoning from raw legal texts. Unlike naive distillation, LEAD utilizes an agentic workflow to convert complex judicial processes into structured reasoning trajectories, thereby enforcing factual grounding and logical rigor. Finally, we implement a Curriculum Reinforcement Learning (RL) strategy. Through a progressive reinforcement process spanning memorization, understanding, and reasoning, LegalOne evolves from simple pattern matching to autonomous and reliable legal reasoning. Experimental results demonstrate that LegalOne achieves state-of-the-art performance across a wide range of legal tasks, surpassing general-purpose LLMs with vastly larger parameter counts through enhanced knowledge density and efficiency. We publicly release the LegalOne weights and the LegalKit evaluation framework to advance the field of Legal AI, paving the way for deploying trustworthy and interpretable foundation models in high-stakes judicial applications.

</details>


### [34] [Can Small Language Models Handle Context-Summarized Multi-Turn Customer-Service QA? A Synthetic Data-Driven Comparative Evaluation](https://arxiv.org/abs/2602.00665)
*Lakshan Cooray,Deshan Sumanathilaka,Pattigadapa Venkatesh Raju*

Main category: cs.CL

TL;DR: 该研究评估了指令调优的小型语言模型在客户服务多轮问答中的表现，使用历史摘要策略保持对话连续性，并与商业大语言模型对比，发现部分小模型性能接近大模型，但仍有局限性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在客户服务问答中表现优异，但计算成本高且部署受限；小型语言模型更高效，但其在多轮客户服务问答中的有效性尚未充分探索，特别是在需要对话连续性和上下文理解的场景中。

Method: 采用历史摘要策略来保持对话状态，评估9个指令调优的低参数小型语言模型，与3个商业大语言模型对比，使用词汇和语义相似度指标，并结合定性评估（包括人工评估和LLM作为评判者方法），引入基于对话阶段的定性分析来评估模型在不同客户服务交互阶段的行为。

Result: 不同小型语言模型表现差异显著，部分模型展现出接近大语言模型的性能，而其他模型在保持对话连续性和上下文对齐方面存在困难。

Conclusion: 研究结果突出了低参数语言模型在实际客户服务问答系统中的潜力和当前局限性，为资源受限环境下的模型选择提供了参考。

Abstract: Customer-service question answering (QA) systems increasingly rely on conversational language understanding. While Large Language Models (LLMs) achieve strong performance, their high computational cost and deployment constraints limit practical use in resource-constrained environments. Small Language Models (SLMs) provide a more efficient alternative, yet their effectiveness for multi-turn customer-service QA remains underexplored, particularly in scenarios requiring dialogue continuity and contextual understanding. This study investigates instruction-tuned SLMs for context-summarized multi-turn customer-service QA, using a history summarization strategy to preserve essential conversational state. We also introduce a conversation stage-based qualitative analysis to evaluate model behavior across different phases of customer-service interactions. Nine instruction-tuned low-parameterized SLMs are evaluated against three commercial LLMs using lexical and semantic similarity metrics alongside qualitative assessments, including human evaluation and LLM-as-a-judge methods. Results show notable variation across SLMs, with some models demonstrating near-LLM performance, while others struggle to maintain dialogue continuity and contextual alignment. These findings highlight both the potential and current limitations of low-parameterized language models for real-world customer-service QA systems.

</details>


### [35] [EchoReview: Learning Peer Review from the Echoes of Scientific Citations](https://arxiv.org/abs/2602.00733)
*Yinuo Zhang,Dingcheng Huang,Haifeng Suo,Yizhuo Li,Ziya Zhao,Junhao Xu,Zhiying Tu,Dianhui Chu,Deming Zhai,Xianming Liu,Xiaoyan Yu,Dianbo Sui*

Main category: cs.CL

TL;DR: EchoReview是一个基于引用上下文的自动审稿数据合成框架，通过挖掘学术引用中的集体评价信号来生成结构化审稿数据，训练出的EchoReviewer-7B在证据支持和审稿全面性等核心维度上表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着科学投稿量快速增长，传统同行评审面临可扩展性压力，需要既可扩展又可靠的自动审稿方法。现有基于真实审稿数据的监督微调方法受限于数据单一来源以及人类审稿的主观性和不一致性，难以支持高质量的自动审稿。

Method: 提出EchoReview框架，系统性地从学术引用中挖掘隐含的集体评价信号，将科学界长期判断转化为结构化审稿风格数据。基于此构建了EchoReview-16K数据集（首个大规模、跨会议、跨年份的引用驱动审稿数据集），并训练了EchoReviewer-7B自动审稿模型。

Result: 实验结果表明，EchoReviewer-7B在证据支持和审稿全面性等核心审稿维度上取得了显著且稳定的改进，验证了引用上下文作为可靠自动同行评审的稳健有效数据范式。

Conclusion: 引用上下文为可靠的自动同行评审提供了稳健有效的数据范式，EchoReview框架通过利用科学界长期积累的集体判断，克服了传统监督方法的数据限制和主观性问题。

Abstract: As the volume of scientific submissions continues to grow rapidly, traditional peer review systems are facing unprecedented scalability pressures, highlighting the urgent need for automated reviewing methods that are both scalable and reliable. Existing supervised fine-tuning approaches based on real review data are fundamentally constrained by single-source of data as well as the inherent subjectivity and inconsistency of human reviews, limiting their ability to support high-quality automated reviewers. To address these issues, we propose EchoReview, a citation-context-driven data synthesis framework that systematically mines implicit collective evaluative signals from academic citations and transforms scientific community's long-term judgments into structured review-style data. Based on this pipeline, we construct EchoReview-16K, the first large-scale, cross-conference, and cross-year citation-driven review dataset, and train an automated reviewer, EchoReviewer-7B. Experimental results demonstrate that EchoReviewer-7B can achieve significant and stable improvements on core review dimensions such as evidence support and review comprehensiveness, validating citation context as a robust and effective data paradigm for reliable automated peer review.

</details>


### [36] [ExperienceWeaver: Optimizing Small-sample Experience Learning for LLM-based Clinical Text Improvement](https://arxiv.org/abs/2602.00740)
*Ziyan Xiao,Yinghao Zhu,Liang Peng,Lequan Yu*

Main category: cs.CL

TL;DR: ExperienceWeaver：一种分层框架，通过从嘈杂的多维反馈中提炼结构化知识（具体技巧和高级策略），在小样本临床文本改进任务中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 临床文本改进对医疗效率至关重要，但高质量数据有限且医疗文档约束复杂。现有LLM方法在小样本场景下表现不佳：监督微调需要大量数据且成本高，检索增强生成通常只能提供表面修正而无法捕捉修订背后的推理逻辑。

Method: 提出ExperienceWeaver分层框架，将重点从数据检索转向经验学习。框架从嘈杂的多维反馈中提炼结构化、可操作的知识，包括错误特定的技巧和高级策略，并将这些提炼的经验注入到智能管道中，让模型学习"如何修订"而非仅仅"修订什么"。

Result: 在四个临床数据集上的广泛评估表明，ExperienceWeaver持续提升性能，在小样本设置中超越了包括Gemini-3 Pro在内的最先进模型。

Conclusion: ExperienceWeaver通过经验学习而非简单检索的方法，有效解决了小样本临床文本改进的挑战，为医疗文档质量提升提供了更高效、更具解释性的解决方案。

Abstract: Clinical text improvement is vital for healthcare efficiency but remains difficult due to limited high-quality data and the complex constraints of medical documentation. While Large Language Models (LLMs) show promise, current approaches struggle in small-sample settings: supervised fine-tuning is data-intensive and costly, while retrieval-augmented generation often provides superficial corrections without capturing the reasoning behind revisions. To address these limitations, we propose ExperienceWeaver, a hierarchical framework that shifts the focus from data retrieval to experience learning. Instead of simply recalling past examples, ExperienceWeaver distills noisy, multi-dimensional feedback into structured, actionable knowledge. Specifically, error-specific Tips and high-level Strategies. By injecting this distilled experience into an agentic pipeline, the model learns "how to revise" rather than just "what to revise". Extensive evaluations across four clinical datasets demonstrate that ExperienceWeaver consistently improves performance, surpassing state-of-the-art models such as Gemini-3 Pro in small-sample settings.

</details>


### [37] [CURP: Codebook-based Continuous User Representation for Personalized Generation with LLMs](https://arxiv.org/abs/2602.00742)
*Liang Wang,Xinyi Mou,Xiaoyou Liu,Xuanjing Huang,Zhongyu Wei*

Main category: cs.CL

TL;DR: CURP：一个使用双向用户编码器和离散原型码本提取多维用户特征的框架，实现即插即用个性化，仅需约2000万参数（约总模型大小的0.2%），在生成任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有用户建模方法（基于提示或训练）难以平衡个性化质量与计算/数据效率，需要一种更高效的用户表征方法。

Method: 提出CURP框架，使用双向用户编码器和离散原型码本提取多维用户特征，实现参数高效的即插即用个性化（约2000万可训练参数）。

Result: 在多种生成任务实验中，CURP相比强基线方法取得了更优的性能和泛化能力，同时提供更好的可解释性和可扩展性。

Conclusion: CURP通过高效的用户表征实现了计算效率与个性化质量的良好平衡，为LLM个性化应用提供了实用解决方案。

Abstract: User modeling characterizes individuals through their preferences and behavioral patterns to enable personalized simulation and generation with Large Language Models (LLMs) in contemporary approaches. However, existing methods, whether prompt-based or training-based methods, face challenges in balancing personalization quality against computational and data efficiency. We propose a novel framework CURP, which employs a bidirectional user encoder and a discrete prototype codebook to extract multi-dimensional user traits. This design enables plug-and-play personalization with a small number of trainable parameters (about 20M parameters, about 0.2\% of the total model size). Through extensive experiments on variant generation tasks, we show that CURP achieves superior performance and generalization compared to strong baselines, while offering better interpretability and scalability. The code are available at https://github.com/RaidonWong/CURP_code

</details>


### [38] [A Baseline Multimodal Approach to Emotion Recognition in Conversations](https://arxiv.org/abs/2602.00914)
*Víctor Yeste,Rodrigo Rivas-Arévalo*

Main category: cs.CL

TL;DR: 提出一个轻量级多模态基线方法用于对话情感识别，结合文本和语音特征进行简单后期融合，为未来研究提供透明参考


<details>
  <summary>Details</summary>
Motivation: 为SemEval-2024 Task 3的情感识别任务提供一个可访问的参考实现，而不是追求最先进的方法，旨在支持未来更严格的比较

Method: 使用基于Transformer的文本分类器和自监督语音表示模型，通过简单的后期融合集成方法结合多模态特征

Result: 在有限训练协议下获得基线结果，展示了多模态融合在何时优于单模态模型

Conclusion: 提供了一个透明的基础实现，可作为未来更严谨比较的基准，强调了多模态融合的价值和局限性

Abstract: We present a lightweight multimodal baseline for emotion recognition in conversations using the SemEval-2024 Task 3 dataset built from the sitcom Friends. The goal of this report is not to propose a novel state-of-the-art method, but to document an accessible reference implementation that combines (i) a transformer-based text classifier and (ii) a self-supervised speech representation model, with a simple late-fusion ensemble. We report the baseline setup and empirical results obtained under a limited training protocol, highlighting when multimodal fusion improves over unimodal models. This preprint is provided for transparency and to support future, more rigorous comparisons.

</details>


### [39] [Decouple Searching from Training: Scaling Data Mixing via Model Merging for Large Language Model Pre-training](https://arxiv.org/abs/2602.00747)
*Shengrui Li,Fei Zhao,Kaiyan Zhao,Jieying Ye,Haifeng Liu,Fangcheng Shi,Zheyong Xie,Yao Hu,Shaosheng Cao*

Main category: cs.CL

TL;DR: DeMix提出了一种通过模型融合预测最优数据混合比例的新框架，将搜索与训练成本解耦，显著降低了寻找最优数据混合的计算开销


<details>
  <summary>Details</summary>
Motivation: 确定有效的数据混合比例是LLM预训练的关键因素，但现有方法要么依赖不可靠的小规模代理实验，要么需要昂贵的大规模探索，难以平衡通用能力与数学/代码等硬任务的专业能力

Method: DeMix框架在候选数据集上训练组件模型，然后通过加权模型融合推导数据混合代理，将搜索与训练成本解耦，无需为每个采样混合训练代理模型即可评估无限采样混合

Result: 实验表明DeMix打破了充分性、准确性和效率之间的权衡，以更低的搜索成本获得更高基准性能的最优混合，并发布了包含22T令牌的高质量预训练数据集DeMix Corpora

Conclusion: DeMix通过模型融合预测数据混合比例，为LLM预训练数据混合优化提供了高效实用的解决方案，促进了开放研究

Abstract: Determining an effective data mixture is a key factor in Large Language Model (LLM) pre-training, where models must balance general competence with proficiency on hard tasks such as math and code. However, identifying an optimal mixture remains an open challenge, as existing approaches either rely on unreliable tiny-scale proxy experiments or require prohibitively expensive large-scale exploration. To address this, we propose Decouple Searching from Training Mix (DeMix), a novel framework that leverages model merging to predict optimal data ratios. Instead of training proxy models for every sampled mixture, DeMix trains component models on candidate datasets at scale and derives data mixture proxies via weighted model merging. This paradigm decouples search from training costs, enabling evaluation of unlimited sampled mixtures without extra training burden and thus facilitating better mixture discovery through more search trials. Extensive experiments demonstrate that DeMix breaks the trade-off between sufficiency, accuracy and efficiency, obtaining the optimal mixture with higher benchmark performance at lower search cost. Additionally, we release the DeMix Corpora, a comprehensive 22T-token dataset comprising high-quality pre-training data with validated mixtures to facilitate open research. Our code and DeMix Corpora is available at https://github.com/Lucius-lsr/DeMix.

</details>


### [40] [Large Language Models as Students Who Think Aloud: Overly Coherent, Verbose, and Confident](https://arxiv.org/abs/2602.01015)
*Conrad Borchers,Jill-Jênn Vie,Roger Azevedo*

Main category: cs.CL

TL;DR: 评估LLM模拟新手推理和元认知判断的能力，发现GPT-4生成过度连贯、冗长且缺乏变化的推理，高估学习者表现，揭示LLM模拟学习的认知局限性。


<details>
  <summary>Details</summary>
Motivation: 现有评估过于关注问题解决准确性，忽视了人类学习中碎片化、不完美的推理过程。需要评估LLM能否真实模拟新手推理和元认知判断，这对AI辅导系统设计至关重要。

Method: 使用630条化学辅导问题中的出声思考话语，结合学生提示使用、尝试和问题情境的日志。在最小和扩展情境提示下比较LLM生成推理与人类学习者话语，评估模型预测学习者步骤级成功的能力。

Result: GPT-4生成流畅且情境适当的延续，但其推理系统性过度连贯、冗长且变异性低于人类出声思考。随着提示中问题解决情境更丰富，这些效应加剧。学习者表现被持续高估。

Conclusion: LLM模拟学习存在认知局限性，源于训练数据中缺乏情感表达和工作记忆约束的专家式解决方案。评估框架可指导未来设计更忠实支持新手学习和自我调节的自适应系统。

Abstract: Large language models (LLMs) are increasingly embedded in AI-based tutoring systems. Can they faithfully model novice reasoning and metacognitive judgments? Existing evaluations emphasize problem-solving accuracy, overlooking the fragmented and imperfect reasoning that characterizes human learning. We evaluate LLMs as novices using 630 think-aloud utterances from multi-step chemistry tutoring problems with problem-solving logs of student hint use, attempts, and problem context. We compare LLM-generated reasoning to human learner utterances under minimal and extended contextual prompting, and assess the models' ability to predict step-level learner success. Although GPT-4.1 generates fluent and contextually appropriate continuations, its reasoning is systematically over-coherent, verbose, and less variable than human think-alouds. These effects intensify with a richer problem-solving context during prompting. Learner performance was consistently overestimated. These findings highlight epistemic limitations of simulating learning with LLMs. We attribute these limitations to LLM training data, including expert-like solutions devoid of expressions of affect and working memory constraints during problem solving. Our evaluation framework can guide future design of adaptive systems that more faithfully support novice learning and self-regulation using generative artificial intelligence.

</details>


### [41] [Temporal Leakage in Search-Engine Date-Filtered Web Retrieval: A Case Study from Retrospective Forecasting](https://arxiv.org/abs/2602.00758)
*Ali El Lahib,Ying-Jieh Xia,Zehan Li,Yuxuan Wang,Xinyu Pi*

Main category: cs.CL

TL;DR: 搜索日期过滤器在回溯性评估中不可靠，71%的查询会返回包含截止日期后信息的页面，导致预测准确性被夸大


<details>
  <summary>Details</summary>
Motivation: 研究搜索引擎日期过滤器在回溯性评估搜索增强预测系统时的可靠性问题，揭示当前方法可能导致评估结果失真

Method: 通过审计Google搜索的before:过滤器，分析查询返回页面中的截止日期后信息泄露情况，并使用LLM（gpt-oss-120b）评估泄露文档对预测准确性的影响

Result: 71%的查询至少返回一个包含强截止日期后泄露的页面，41%的查询至少有一个页面直接泄露答案；使用泄露文档时预测准确性被夸大（Brier分数0.108 vs 无泄露时的0.242）

Conclusion: 日期限制搜索不足以进行时间性评估，建议采用更强的检索保护措施或在冻结的时间戳网络快照上进行评估，以确保回溯性预测的可信度

Abstract: Search-engine date filters are widely used to enforce pre-cutoff retrieval in retrospective evaluations of search-augmented forecasters. We show this approach is unreliable: auditing Google Search with a before: filter, 71% of questions return at least one page containing strong post-cutoff leakage, and for 41%, at least one page directly reveals the answer. Using a large language model (LLM), gpt-oss-120b, to forecast with these leaky documents, we demonstrate an inflated prediction accuracy (Brier score 0.108 vs. 0.242 with leak-free documents). We characterize common leakage mechanisms, including updated articles, related-content modules, unreliable metadata/timestamps, and absence-based signals, and argue that date-restricted search is insufficient for temporal evaluation. We recommend stronger retrieval safeguards or evaluation on frozen, time-stamped web snapshots to ensure credible retrospective forecasting.

</details>


### [42] [Adaptive Ability Decomposing for Unlocking Large Reasoning Model Effective Reinforcement Learning](https://arxiv.org/abs/2602.00759)
*Zhipeng Chen,Xiaobo Qin,Wayne Xin Zhao,Youbin Wu,Ji-Rong Wen*

Main category: cs.CL

TL;DR: A²D是一种自适应能力分解方法，通过将复杂问题分解为简单子问题来增强RLVR（可验证奖励的强化学习）的效果，无需依赖教师模型。


<details>
  <summary>Details</summary>
Motivation: RLVR虽然能增强大语言模型的推理能力，但由于训练过程中信息有限，模型只能进行盲目的探索，导致在复杂问题上经常失败。需要在不依赖教师模型的情况下为RLVR过程提供额外信息。

Method: A²D方法：1）首先通过RLVR训练一个分解器，使其能将复杂问题分解为一系列更简单的子问题；2）使用该分解器为训练数据集中的每个问题标注子问题；3）在子问题指导下通过RLVR训练推理器。

Result: 1）A²D在性能上优于竞争基线；2）可作为即插即用模块应用于不同的RLVR算法；3）分解器分析揭示了RLVR过程如何影响其性能和行为，以及哪种类型的指导更适合增强推理器的探索和利用能力。

Conclusion: A²D通过自适应能力分解有效增强了RLVR的效果，提供了一种无需教师模型的指导机制，提高了模型在复杂问题上的推理能力。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has shown great potential to enhance the reasoning ability of large language models (LLMs). However, due to the limited amount of information provided during the RLVR process, the model can only engage in largely blind exploration, which often results in failure on challenging problems. To provide additional information for the RLVR process without relying on a teacher model, we propose A$^2$D, an Adaptive Ability Decomposing method for enhancing the effectiveness of RLVR. Specifically, we first train a decomposer via RLVR without distillation, enabling it to decompose complex questions into a set of simpler sub-questions. Next, we use this decomposer to annotate sub-questions for each question in the training dataset, and then train the reasoner under RLVR with sub-question guidance. To better understand A$^2$D, we first compare its performance with competitive baselines, showing its effectiveness. Next, we observe that our method functions as a plug-and-play module that can be applied to different RLVR algorithms. Furthermore, we conduct an analysis of the decomposer, revealing how the RLVR process affects its performance and behavior, and which type of guidance is better suited for enhancing the reasoner's exploration and exploitation abilities.

</details>


### [43] [APR: Penalizing Structural Redundancy in Large Reasoning Models via Anchor-based Process Rewards](https://arxiv.org/abs/2602.00760)
*Kaiyan Chang,Chenwei Zhu,Yingfeng Luo,Yifu Huo,Chenglong Wang,Xiaoqian Liu,Qiaozhi He,Tong Xiao,Zhengtao Yu,Jingbo Zhu*

Main category: cs.CL

TL;DR: 论文提出Anchor-based Process Reward (APR)方法，通过定位推理锚点并惩罚锚点后的冗余验证，解决大推理模型在测试时扩展中出现的"过度思考"问题，在保持性能的同时显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 大推理模型在测试时扩展中会出现"过度思考"现象，即模型在得到最终答案后仍进行无意义的重复自我验证，造成计算资源浪费。作者从细粒度视角重新审视这一现象，旨在提高推理效率。

Method: 提出Anchor-based Process Reward (APR)方法：首先定义"推理锚点"（答案首次稳定的位置），然后专门惩罚锚点后的"答案稳定尾部"（无意义的重复验证）。结合适合长度惩罚的策略优化算法进行训练。

Result: 在5个数学推理数据集上，APR方法在1.5B和7B规模模型上实现了性能-效率的帕累托前沿，同时显著减少了强化学习训练所需的计算资源。

Conclusion: 通过定位推理锚点并专门惩罚锚点后的冗余验证，可以有效解决大推理模型的"过度思考"问题，在保持性能的同时大幅提升推理效率，为高效推理模型训练提供了新思路。

Abstract: Test-Time Scaling (TTS) has significantly enhanced the capabilities of Large Reasoning Models (LRMs) but introduces a critical side-effect known as Overthinking. We conduct a preliminary study to rethink this phenomenon from a fine-grained perspective. We observe that LRMs frequently conduct repetitive self-verification without revision even after obtaining the final answer during the reasoning process. We formally define this specific position where the answer first stabilizes as the Reasoning Anchor. By analyzing pre- and post-anchor reasoning behaviors, we uncover the structural redundancy fixed in LRMs: the meaningless repetitive verification after deriving the first complete answer, which we term the Answer-Stable Tail (AST). Motivated by this observation, we propose Anchor-based Process Reward (APR), a structure-aware reward shaping method that localizes the reasoning anchor and penalizes exclusively the post-anchor AST. Leveraging the policy optimization algorithm suitable for length penalties, our APR models achieved the performance-efficiency Pareto frontier at 1.5B and 7B scales averaged across five mathematical reasoning datasets while requiring significantly fewer computational resources for RL training.

</details>


### [44] [WordCraft: Scaffolding the Keyword Method for L2 Vocabulary Learning with Multimodal LLMs](https://arxiv.org/abs/2602.00762)
*Yuheng Shao,Junjie Xiong,Chaoran Wu,Xiyuan Wang,Ziyu Zhou,Yang Ouyang,Qinyi Tao,Quan Li*

Main category: cs.CL

TL;DR: WordCraft是一个基于多模态大语言模型的交互式工具，帮助中文母语英语学习者通过关键词法记忆词汇，解决他们生成合适关键词、构建关联和形成心理意象的困难。


<details>
  <summary>Details</summary>
Motivation: 中文母语的英语学习者在应用关键词法记忆词汇时面临三大挑战：难以生成语音合适的关键词、构建连贯关联、创造生动的心理意象以促进长期记忆。现有方法要么完全自动化（牺牲学习者参与度），要么只关注结果（缺乏过程指导）。

Method: 首先进行形成性研究（N=18），了解学习者和教育者的困难与需求。基于这些发现，开发WordCraft——一个基于多模态大语言模型的学习者中心交互工具，通过引导学习者完成关键词选择、关联构建和意象形成三个步骤来支持关键词法。

Result: 两项用户研究表明，WordCraft不仅保持了生成效应（学习者参与创造的价值），而且在效果和可用性方面都达到了高水平。

Conclusion: WordCraft通过过程导向的引导，有效解决了中文母语英语学习者在应用关键词法时的困难，为词汇记忆提供了有效的技术支持。

Abstract: Applying the keyword method for vocabulary memorization remains a significant challenge for L1 Chinese-L2 English learners. They frequently struggle to generate phonologically appropriate keywords, construct coherent associations, and create vivid mental imagery to aid long-term retention. Existing approaches, including fully automated keyword generation and outcome-oriented mnemonic aids, either compromise learner engagement or lack adequate process-oriented guidance. To address these limitations, we conducted a formative study with L1 Chinese-L2 English learners and educators (N=18), which revealed key difficulties and requirements in applying the keyword method to vocabulary learning. Building on these insights, we introduce WordCraft, a learner-centered interactive tool powered by Multimodal Large Language Models (MLLMs). WordCraft scaffolds the keyword method by guiding learners through keyword selection, association construction, and image formation, thereby enhancing the effectiveness of vocabulary memorization. Two user studies demonstrate that WordCraft not only preserves the generation effect but also achieves high levels of effectiveness and usability.

</details>


### [45] [Eliciting Trustworthiness Priors of Large Language Models via Economic Games](https://arxiv.org/abs/2602.00769)
*Siyu Yan,Lusha Zhu,Jian-Qiao Zhu*

Main category: cs.CL

TL;DR: 本文提出一种基于迭代上下文学习的方法，用于从大型语言模型中提取信任先验，发现GPT-4.1的信任先验与人类相似，并可通过刻板印象模型预测


<details>
  <summary>Details</summary>
Motivation: 构建可信赖AI系统需要保持校准信任，但如何表征AI系统自身的信任水平是一个基本挑战。本文旨在开发一种方法来量化LLMs的信任先验，并与人类行为进行比较

Method: 提出基于迭代上下文学习的新颖启发方法，应用于行为博弈论中的信任游戏，从多个领先LLMs中提取信任先验，并分析GPT-4.1对不同玩家角色的响应

Result: GPT-4.1的信任先验与人类观察到的先验高度一致；GPT-4.1能根据代理特征区分信任水平；提取的信任变化可通过基于感知温暖和能力的刻板印象模型良好预测

Conclusion: 该方法能有效量化LLMs的信任先验，GPT-4.1展现出与人类相似的信任模式，为理解AI系统的信任表征提供了新工具，有助于构建更可信赖的AI系统

Abstract: One critical aspect of building human-centered, trustworthy artificial intelligence (AI) systems is maintaining calibrated trust: appropriate reliance on AI systems outperforms both overtrust (e.g., automation bias) and undertrust (e.g., disuse). A fundamental challenge, however, is how to characterize the level of trust exhibited by an AI system itself. Here, we propose a novel elicitation method based on iterated in-context learning (Zhu and Griffiths, 2024a) and apply it to elicit trustworthiness priors using the Trust Game from behavioral game theory. The Trust Game is particularly well suited for this purpose because it operationalizes trust as voluntary exposure to risk based on beliefs about another agent, rather than self-reported attitudes. Using our method, we elicit trustworthiness priors from several leading large language models (LLMs) and find that GPT-4.1's trustworthiness priors closely track those observed in humans. Building on this result, we further examine how GPT-4.1 responds to different player personas in the Trust Game, providing an initial characterization of how such models differentiate trust across agent characteristics. Finally, we show that variation in elicited trustworthiness can be well predicted by a stereotype-based model grounded in perceived warmth and competence.

</details>


### [46] [Reasoning as State Transition: A Representational Analysis of Reasoning Evolution in Large Language Models](https://arxiv.org/abs/2602.00770)
*Siyuan Zhang,Jialian Li,Yichi Zhang,Xiao Yang,Yinpeng Dong,Hang Su*

Main category: cs.CL

TL;DR: 本文通过表征视角分析LLM推理能力的训练演化，发现后训练仅有限提升初始表征质量，但能驱动推理过程中的表征分布转变，从而提升任务解决能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要通过生成结果分析LLM推理能力的训练演化，将推理过程视为黑盒，无法揭示内部变化。为了解构这种不透明性，本文引入表征视角来研究模型内部状态的动态变化。

Method: 采用表征视角，对不同训练阶段的模型进行综合实验，分析内部状态动态；通过统计分析和反事实实验，探究内部状态与外部输出的关系。

Result: 1) 后训练仅有限提升静态初始表征质量；2) 推理过程涉及显著的连续表征分布转变；3) 后训练使模型能驱动这种转变朝向更适合任务解决的分布；4) 生成正确性与最终表征高度相关；5) 生成token的语义（而非推理计算或参数差异）是分布转变的主要驱动力。

Conclusion: 本文提供了对推理过程及训练如何增强推理能力的新理解，为未来模型分析和优化提供了有价值的见解。

Abstract: Large Language Models have achieved remarkable performance on reasoning tasks, motivating research into how this ability evolves during training. Prior work has primarily analyzed this evolution via explicit generation outcomes, treating the reasoning process as a black box and obscuring internal changes. To address this opacity, we introduce a representational perspective to investigate the dynamics of the model's internal states. Through comprehensive experiments across models at various training stages, we discover that post-training yields only limited improvement in static initial representation quality. Furthermore, we reveal that, distinct from non-reasoning tasks, reasoning involves a significant continuous distributional shift in representations during generation. Comparative analysis indicates that post-training empowers models to drive this transition toward a better distribution for task solving. To clarify the relationship between internal states and external outputs, statistical analysis confirms a high correlation between generation correctness and the final representations; while counterfactual experiments identify the semantics of the generated tokens, rather than additional computation during inference or intrinsic parameter differences, as the dominant driver of the transition. Collectively, we offer a novel understanding of the reasoning process and the effect of training on reasoning enhancement, providing valuable insights for future model analysis and optimization.

</details>


### [47] [HyLRA: Hybrid Layer Reuse Attention for Efficient Long-Context Inference](https://arxiv.org/abs/2602.00777)
*Xuan Ai,Qingqing Yang,Peng Wang,Lei Deng,Lin Zhang,Renhai Chen,Gong Zhang*

Main category: cs.CL

TL;DR: HyLRA是一种混合层重用注意力机制，通过层间稀疏性分析，在敏感层保留完整注意力，在容忍层重用前层关键token索引，显著提升长上下文推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏注意力方法通常依赖固定模式或激进剪枝，无法在效率和准确性之间达到最优平衡。长上下文推理面临注意力二次计算复杂度和KV缓存内存占用的瓶颈。

Method: 基于层间稀疏性分析发现注意力机制的双重特性：层内敏感性和层间相似性。采用离线动态规划方法制定最优层间策略，敏感层保留完整注意力，容忍层重用前层top-k索引。

Result: HyLRA将推理吞吐量提升6%-46%，同时保持可比较的性能（准确率下降<1%），在多项评估中一致优于现有稀疏注意力方法。

Conclusion: HyLRA通过混合层重用注意力机制有效克服了密集注意力的二次计算瓶颈，在保持模型性能的同时显著提升了长上下文推理效率。

Abstract: Long-context inference in Large Language Models (LLMs) is bottlenecked by the quadratic computation complexity of attention and the substantial memory footprint of Key-Value (KV) caches. While existing sparse attention mechanisms attempt to mitigate this by exploiting inherent sparsity, they often rely on rigid patterns or aggressive pruning, failing to achieve an optimal balance between efficiency and accuracy. In this paper, we introduce {\bf HyLRA} ({\bf Hy}brid {\bf L}ayer {\bf R}euse {\bf A}ttention), a novel framework driven by layer-wise sparsity profiling. Our empirical analysis uncovers a dual characteristic in attention mechanics: \textit{intra-layer sensitivity}, where specific layers necessitate full attention to prevent feature distortion, and \textit{inter-layer similarity}, where consecutive layers share substantial critical tokens. Based on these observations, HyLRA employs an offline dynamic programming approach to derive an optimal layer-wise policy. This hybrid strategy retains full attention for sensitive layers to ensure robustness, while enabling tolerant layers to bypass quadratic calculations by directly reusing top-$k$ indices from preceding layers. This approach allows LLMs to restrict computation to the most critical tokens, effectively overcoming the quadratic bottleneck of dense attention. Extensive evaluations demonstrate that HyLRA improves inference throughput by 6\%--46\% while maintaining comparable performance (with $<1\%$ accuracy degradation), consistently outperforming state-of-the-art sparse attention methods. HyLRA is open source at \href{https://anonymous.4open.science/r/unified-cache-management-CF80/}{\texttt{/r/unified-cache-management-CF80/}}

</details>


### [48] [Omni-RRM: Advancing Omni Reward Modeling via Automatic Rubric-Grounded Preference Synthesis](https://arxiv.org/abs/2602.00846)
*Zicheng Kong,Dehua Ma,Zhenbo Xu,Alven Yang,Yiwei Ru,Haoran Wang,Zixuan Zhou,Fuqing Bie,Liuyu Xiang,Huijia Wu,Jian Zhao,Zhaofeng He*

Main category: cs.CL

TL;DR: Omni-RRM是首个开源的多模态奖励模型，通过自动化流程生成结构化多维度偏好判断，在视频和音频基准测试中达到SOTA，显著提升图像任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型的性能受限于粗糙的对齐技术，缺乏有效的奖励模型。现有奖励模型主要是视觉中心的，返回不透明的标量分数，且依赖昂贵的人工标注。

Method: 1) 构建Omni-Preference数据集：通过自动化流程合成候选响应对，使用强教师模型协调和过滤偏好，提供模态感知的基于量规的推理；2) 两阶段训练：监督微调学习基于量规的输出，然后通过强化学习（GRPO）在困难、低对比度对上提高判别能力。

Result: Omni-RRM在视频基准测试（ShareGPT-V上80.2%）和音频基准测试（Audio-HH-RLHF上66.8%）上达到SOTA，在图像任务上显著优于现有开源奖励模型，相比基础模型整体准确率提升17.7%。

Conclusion: Omni-RRM是首个开源的多模态奖励模型，通过自动化数据生成和两阶段训练，实现了结构化多维度偏好判断，在多模态对齐任务中表现出色，且能提升下游任务性能。

Abstract: Multimodal large language models (MLLMs) have shown remarkable capabilities, yet their performance is often capped by the coarse nature of existing alignment techniques. A critical bottleneck remains the lack of effective reward models (RMs): existing RMs are predominantly vision-centric, return opaque scalar scores, and rely on costly human annotations. We introduce \textbf{Omni-RRM}, the first open-source rubric-grounded reward model that produces structured, multi-dimension preference judgments with dimension-wise justifications across \textbf{text, image, video, and audio}. At the core of our approach is \textbf{Omni-Preference}, a large-scale dataset built via a fully automated pipeline: we synthesize candidate response pairs by contrasting models of different capabilities, and use strong teacher models to \emph{reconcile and filter} preferences while providing a modality-aware \emph{rubric-grounded rationale} for each pair. This eliminates the need for human-labeled training preferences. Omni-RRM is trained in two stages: supervised fine-tuning to learn the rubric-grounded outputs, followed by reinforcement learning (GRPO) to sharpen discrimination on difficult, low-contrast pairs. Comprehensive evaluations show that Omni-RRM achieves state-of-the-art accuracy on video (80.2\% on ShareGPT-V) and audio (66.8\% on Audio-HH-RLHF) benchmarks, and substantially outperforms existing open-source RMs on image tasks, with a 17.7\% absolute gain over its base model on overall accuracy. Omni-RRM also improves downstream performance via Best-of-$N$ selection and transfers to text-only preference benchmarks. Our data, code, and models are available at https://anonymous.4open.science/r/Omni-RRM-CC08.

</details>


### [49] [Factuality on Demand: Controlling the Factuality-Informativeness Trade-off in Text Generation](https://arxiv.org/abs/2602.00848)
*Ziwei Gong,Yanda Chen,Julia Hirschberg,Chen Zhao,He He,Zhou Yu,Kathleen Mckeown*

Main category: cs.CL

TL;DR: 提出Factuality-Controlled Generation (FCG)框架，让用户能在查询时指定事实性约束，通过合成数据训练显著提升模型在遵守事实性要求的同时保持信息性的能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在编码知识时具有不同程度的置信度，面临信息性与事实性之间的权衡：生成的信息越丰富可能越不准确，越准确可能信息量越少。不同应用需要不同的权衡平衡。

Method: 引入Factuality-Controlled Generation (FCG)框架，允许用户在查询时指定事实性约束。使用合成数据训练模型执行FCG任务，从事实性约束遵守和响应信息性两个维度评估性能。

Result: 合成训练显著提高了模型在遵守事实性要求的同时保持输出信息性的能力，使模型能更好地平衡信息性与事实性之间的权衡。

Conclusion: FCG框架为控制大语言模型的事实性输出提供了有效方法，通过合成数据训练可以显著提升模型在指定事实性约束下的生成质量，满足不同应用场景的需求。

Abstract: Large language models (LLMs) encode knowledge with varying degrees of confidence. When responding to queries, models face an inherent trade-off: they can generate responses that are less informative but highly factual, or more informative but potentially less accurate. Different applications demand different balances between informativeness and factuality. We introduce Factuality-Controlled Generation (FCG), a framework that enables users to specify factuality constraints alongside their queries. We propose to evaluate FCG performance on two dimensions: adherence to factuality constraints and response informativeness. We propose to train models on the FCG task using synthetic data, and show that our synthetic training significantly improves models' ability to both respect factuality requirements and maintain informativeness in their outputs.

</details>


### [50] [Unifying Adversarial Robustness and Training Across Text Scoring Models](https://arxiv.org/abs/2602.00857)
*Manveer Singh Tamber,Hosna Oyarhoseini,Jimmy Lin*

Main category: cs.CL

TL;DR: 该论文提出将文本评分模型（包括密集检索器、重排序器和奖励模型）的对抗鲁棒性研究统一起来，通过新的对抗训练方法提高模型鲁棒性，并在RLHF中展示实用价值。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型对抗鲁棒性研究在不同应用和攻击方法上分散，掩盖了共同的脆弱性。需要统一文本评分模型的对抗鲁棒性研究框架。

Method: 提出文本评分模型的统一对抗鲁棒性研究框架，开发多种对抗训练方法，并将互补的训练方法结合使用以提高鲁棒性。

Result: 展示了结合互补对抗训练方法可以产生强大的鲁棒性，同时提高任务效果。在RLHF中，对抗训练的奖励模型能减轻奖励黑客攻击并训练出更好对齐的LLMs。

Conclusion: 通过统一的文本评分模型对抗鲁棒性框架，开发有效的对抗训练方法，不仅能提高模型鲁棒性，还能在RLHF等实际应用中产生积极影响。

Abstract: Research on adversarial robustness in language models is currently fragmented across applications and attacks, obscuring shared vulnerabilities. In this work, we propose unifying the study of adversarial robustness in text scoring models spanning dense retrievers, rerankers, and reward models. This motivates adapting both attacks and adversarial training methods across model roles. Unlike open-ended generation, text scoring failures are directly testable: an attack succeeds when an irrelevant or rejected text outscores a relevant or chosen one. Using this principled lens of text scoring, we demonstrate that current adversarial training formulations for language models are often short-sighted, failing to effectively generalize across attacks. To address this, we introduce multiple adversarial training methods for text scoring models and show that combining complementary training methods can yield strong robustness while also improving task effectiveness. We also highlight the practical value of our approach for RLHF, showing that our adversarially trained reward models mitigate reward hacking and support the training of better-aligned LLMs. We provide our code and models for further study.

</details>


### [51] [Context Dependence and Reliability in Autoregressive Language Models](https://arxiv.org/abs/2602.01378)
*Poushali Sengupta,Shashi Raj Pandey,Sabita Maharjan,Frank Eliassen*

Main category: cs.CL

TL;DR: RISE方法解决LLM解释中冗余信息干扰问题，通过量化每个输入的独特影响来提供更稳定可靠的特征归因


<details>
  <summary>Details</summary>
Motivation: 大型语言模型生成输出时使用大量上下文，其中包含提示、检索段落和交互历史中的冗余信息。标准解释方法难以处理冗余和重叠上下文，微小输入变化会导致归因分数不可预测的变化，影响可解释性并引发如提示注入等风险担忧。

Method: 提出RISE（冗余不敏感解释评分）方法，量化每个输入相对于其他输入的独特影响，最小化冗余影响，提供更清晰稳定的归因。

Result: 实验证明RISE比传统方法提供更鲁棒的解释，强调条件信息对于可信赖的LLM解释和监控的重要性。

Conclusion: RISE方法能够有效区分关键上下文元素与相关元素，为LLM解释提供更稳定可靠的特征归因，有助于提高模型可解释性和风险监控。

Abstract: Large language models (LLMs) generate outputs by utilizing extensive context, which often includes redundant information from prompts, retrieved passages, and interaction history. In critical applications, it is vital to identify which context elements actually influence the output, as standard explanation methods struggle with redundancy and overlapping context. Minor changes in input can lead to unpredictable shifts in attribution scores, undermining interpretability and raising concerns about risks like prompt injection. This work addresses the challenge of distinguishing essential context elements from correlated ones. We introduce RISE (Redundancy-Insensitive Scoring of Explanation), a method that quantifies the unique influence of each input relative to others, minimizing the impact of redundancies and providing clearer, stable attributions. Experiments demonstrate that RISE offers more robust explanations than traditional methods, emphasizing the importance of conditional information for trustworthy LLM explanations and monitoring.

</details>


### [52] [ILSIC: Corpora for Identifying Indian Legal Statutes from Queries by Laypeople](https://arxiv.org/abs/2602.00881)
*Shounak Paul,Raghav Dogra,Pawan Goyal,Saptarshi Ghosh*

Main category: cs.CL

TL;DR: 本文创建了ILSIC语料库，包含印度法律中500+法规的普通人查询和法庭判决，用于比较法庭与普通人数据的法律条文识别差异，发现仅用法庭数据训练的模型对普通人查询效果不佳。


<details>
  <summary>Details</summary>
Motivation: 法律条文识别（LSI）通常使用法庭判决事实作为输入查询，但实际应用中输入查询往往是普通人提出的非正式问题。现有研究很少探索法庭数据与普通人数据在LSI任务上的差异。

Method: 创建ILSIC语料库，包含500+印度法律条文的普通人查询和法庭判决；进行广泛实验，包括零样本/少样本推理、检索增强生成和监督微调；比较法庭与普通人数据的LSI性能。

Result: 仅用法庭判决训练的模型在普通人查询上效果不佳；从法庭数据到普通人数据的迁移学习在某些场景下有益；对查询类别和法规频率进行了细粒度分析。

Conclusion: 法庭数据与普通人数据在LSI任务上存在显著差异，需要专门针对普通人查询的数据集和方法；ILSIC语料库为研究这一差异提供了重要资源。

Abstract: Legal Statute Identification (LSI) for a given situation is one of the most fundamental tasks in Legal NLP. This task has traditionally been modeled using facts from court judgments as input queries, due to their abundance. However, in practical settings, the input queries are likely to be informal and asked by laypersons, or non-professionals. While a few laypeople LSI datasets exist, there has been little research to explore the differences between court and laypeople data for LSI. In this work, we create ILSIC, a corpus of laypeople queries covering 500+ statutes from Indian law. Additionally, the corpus also contains court case judgements to enable researchers to effectively compare between court and laypeople data for LSI. We conducted extensive experiments on our corpus, including benchmarking over the laypeople dataset using zero and few-shot inference, retrieval-augmented generation and supervised fine-tuning. We observe that models trained purely on court judgements are ineffective during test on laypeople queries, while transfer learning from court to laypeople data can be beneficial in certain scenarios. We also conducted fine-grained analyses of our results in terms of categories of queries and frequency of statutes.

</details>


### [53] [On the Power of (Approximate) Reward Models for Inference-Time Scaling](https://arxiv.org/abs/2602.01381)
*Youheng Zhu,Yiping Lu*

Main category: cs.CL

TL;DR: 论文从理论上证明了近似奖励模型的贝尔曼误差是影响基于SMC的推理时缩放效果的关键因素，当误差为O(1/T)时，可将推理计算复杂度从指数级降至多项式级。


<details>
  <summary>Details</summary>
Motivation: 当前推理时缩放方法依赖近似奖励模型来评估中间推理轨迹，但缺乏理论解释为什么近似奖励模型在实际中有效。本文旨在从理论上回答：近似奖励模型何时以及为什么能支持有效的推理时缩放。

Method: 采用理论分析方法，将推理过程建模为长度为T的序列决策问题，分析基于序列蒙特卡洛(SMC)的推理时缩放框架。核心是识别近似奖励模型的贝尔曼误差作为关键理论指标。

Result: 理论证明：当近似奖励模型的贝尔曼误差有界于O(1/T)时，结合SMC方法可将推理计算复杂度从指数级O(exp(T))降低到多项式级O(poly(T))，实现指数级的推理效率提升。

Conclusion: 近似奖励模型的贝尔曼误差是决定推理时缩放效果的关键理论因素，即使使用近似奖励，只要误差控制得当，仍能实现指数级的推理效率改进，为实际部署提供了理论依据。

Abstract: Inference-time scaling has recently emerged as a powerful paradigm for improving the reasoning capability of large language models. Among various approaches, Sequential Monte Carlo (SMC) has become a particularly important framework, enabling iterative generation, evaluation, rejection, and resampling of intermediate reasoning trajectories. A central component in this process is the reward model, which evaluates partial solutions and guides the allocation of computation during inference.
  However, in practice, true reward models are never available. All deployed systems rely on approximate reward models, raising a fundamental question: Why and when do approximate reward models suffice for effective inference-time scaling? In this work, we provide a theoretical answer. We identify the Bellman error of the approximate reward model as the key quantity governing the effectiveness of SMC-based inference-time scaling. For a reasoning process of length $T$, we show that if the Bellman error of the approximate reward model is bounded by $O(1/T)$, then combining this reward model with SMC reduces the computational complexity of reasoning from exponential in $T$ to polynomial in $T$. This yields an exponential improvement in inference efficiency despite using only approximate rewards.

</details>


### [54] [EffGen: Enabling Small Language Models as Capable Autonomous Agents](https://arxiv.org/abs/2602.00887)
*Gaurav Srivastava,Aafiya Hussain,Chi Wang,Yingyan Celine Lin,Xuan Wang*

Main category: cs.CL

TL;DR: effGen是一个专为小型语言模型优化的开源智能体框架，通过提示优化、任务分解、复杂度路由和统一内存系统，实现高效、安全的本地部署。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的智能体系统存在高token成本和隐私问题，需要为小型语言模型开发优化的本地部署框架。

Method: 1) 提示优化压缩上下文70-80%；2) 智能任务分解为并行/顺序子任务；3) 基于五个因素的复杂度路由预执行决策；4) 统一内存系统；5) 支持多种智能体协议。

Result: 在13个基准测试中优于LangChain、AutoGen和Smolagents，成功率更高、执行更快、内存更低。提示优化对小模型增益更大（1.5B模型11.2% vs 32B模型2.4%），路由对大模型增益更大（1.5B模型3.6% vs 32B模型7.9%）。

Conclusion: effGen为小型语言模型提供了高效的智能体框架，解决了现有大模型系统的成本和隐私问题，通过优化和路由的互补机制在不同规模模型上都能获得一致性能提升。

Abstract: Most existing language model agentic systems today are built and optimized for large language models (e.g., GPT, Claude, Gemini) via API calls. While powerful, this approach faces several limitations including high token costs and privacy concerns for sensitive applications. We introduce effGen, an open-source agentic framework optimized for small language models (SLMs) that enables effective, efficient, and secure local deployment (pip install effgen). effGen makes four major contributions: (1) Enhanced tool-calling with prompt optimization that compresses contexts by 70-80% while preserving task semantics, (2) Intelligent task decomposition that breaks complex queries into parallel or sequential subtasks based on dependencies, (3) Complexity-based routing using five factors to make smart pre-execution decisions, and (4) Unified memory system combining short-term, long-term, and vector-based storage. Additionally, effGen unifies multiple agent protocols (MCP, A2A, ACP) for cross-protocol communication. Results on 13 benchmarks show effGen outperforms LangChain, AutoGen, and Smolagents with higher success rates, faster execution, and lower memory. Our results reveal that prompt optimization and complexity routing have complementary scaling behavior: optimization benefits SLMs more (11.2% gain at 1.5B vs 2.4% at 32B), while routing benefits large models more (3.6% at 1.5B vs 7.9% at 32B), providing consistent gains across all scales when combined. effGen (https://effgen.org/) is released under the MIT License, ensuring broad accessibility for research and commercial use. Our framework code is publicly available at https://github.com/ctrl-gaurav/effGen.

</details>


### [55] [Do Schwartz Higher-Order Values Help Sentence-Level Human Value Detection? When Hard Gating Hurts](https://arxiv.org/abs/2602.00913)
*Víctor Yeste,Paolo Rosso*

Main category: cs.CL

TL;DR: 研究在计算受限条件下，探索Schwartz高阶类别是否能为句子级人类价值检测提供可用结构，发现硬性层次约束反而降低性能，而标签阈值调优和轻量集成更有效。


<details>
  <summary>Details</summary>
Motivation: 句子级人类价值检测通常被构建为对Schwartz价值的多标签分类，但尚不清楚Schwartz高阶类别是否能提供可用的结构。本研究在严格的计算受限条件下（单8GB GPU）探索这一问题。

Method: 在ValueEval'24/ValuesML数据集（74K英语句子）上比较：(i) 直接监督transformer，(ii) 使用硬掩码强制执行层次结构的HO→价值管道，(iii) Presence→HO→价值级联，同时结合低成本附加组件（词典、短上下文、主题）、标签阈值调优、小型指令调优LLM基线（≤10B）、QLoRA和简单集成。

Result: 高阶类别可以从单个句子中学习（例如最容易的双极对达到Macro-F1≈0.58），但硬性层次门控不是可靠的改进：它通常通过错误累积和召回抑制降低最终任务的Macro-F1。标签阈值调优是高杠杆调整（最高提升+0.05 Macro-F1），小型transformer集成提供最一致的额外增益（最高+0.02 Macro-F1）。小型LLM作为独立系统落后于监督编码器，但可以在跨家族集成中贡献互补错误。

Conclusion: 高阶结构在描述上有用，但用硬门控强制执行会损害句子级价值检测；稳健的改进来自校准和轻量集成。

Abstract: Sentence-level human value detection is typically framed as multi-label classification over Schwartz values, but it remains unclear whether Schwartz higher-order (HO) categories provide usable structure. We study this under a strict compute-frugal budget (single 8 GB GPU) on ValueEval'24 / ValuesML (74K English sentences). We compare (i) direct supervised transformers, (ii) HO$\rightarrow$values pipelines that enforce the hierarchy with hard masks, and (iii) Presence$\rightarrow$HO$\rightarrow$values cascades, alongside low-cost add-ons (lexica, short context, topics), label-wise threshold tuning, small instruction-tuned LLM baselines ($\le$10B), QLoRA, and simple ensembles. HO categories are learnable from single sentences (e.g., the easiest bipolar pair reaches Macro-$F_1\approx0.58$), but hard hierarchical gating is not a reliable win: it often reduces end-task Macro-$F_1$ via error compounding and recall suppression. In contrast, label-wise threshold tuning is a high-leverage knob (up to $+0.05$ Macro-$F_1$), and small transformer ensembles provide the most consistent additional gains (up to $+0.02$ Macro-$F_1$). Small LLMs lag behind supervised encoders as stand-alone systems, yet can contribute complementary errors in cross-family ensembles. Overall, HO structure is useful descriptively, but enforcing it with hard gates hurts sentence-level value detection; robust improvements come from calibration and lightweight ensembling.

</details>


### [56] [Neural FOXP2 -- Language Specific Neuron Steering for Targeted Language Improvement in LLMs](https://arxiv.org/abs/2602.00945)
*Anusa Saha,Tanmay Joshi,Vinija Jain,Aman Chadha,Amitava Das*

Main category: cs.CL

TL;DR: 提出Neural FOXP2方法，通过定位和操控语言神经元，使特定语言（如印地语或西班牙语）在LLM中成为主要语言，解决英语主导问题。


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs是多语言的，但英语在预训练中占主导地位，导致其他语言被系统性地抑制。需要一种机制来安全地操控语言默认设置。

Method: Neural FOXP2三阶段方法：1) 定位语言神经元；2) 通过谱低秩分析确定操控方向；3) 对语言神经元应用稀疏激活偏移进行操控。

Result: 通过操控语言神经元，能够使目标语言（如印地语或西班牙语）在模型中成为主要语言，实现可控的语言默认设置。

Conclusion: 语言默认性由稀疏低秩控制电路（语言神经元）控制，可以通过机制性隔离和安全操控来改变LLM的语言偏好。

Abstract: LLMs are multilingual by training, yet their lingua franca is often English, reflecting English language dominance in pretraining. Other languages remain in parametric memory but are systematically suppressed. We argue that language defaultness is governed by a sparse, low-rank control circuit, language neurons, that can be mechanistically isolated and safely steered.
  We introduce Neural FOXP2, that makes a chosen language (Hindi or Spanish) primary in a model by steering language-specific neurons. Neural FOXP2 proceeds in three stages: (i) Localize: We train per-layer SAEs so each activation decomposes into a small set of active feature components. For every feature, we quantify English vs. Hindi/Spanish selectivity overall logit-mass lift toward the target-language token set. Tracing the top-ranked features back to their strongest contributing units yields a compact language-neuron set. (ii) Steering directions: We localize controllable language-shift geometry via a spectral low-rank analysis. For each layer, we build English to target activation-difference matrices and perform layerwise SVD to extract the dominant singular directions governing language change. The eigengap and effective-rank spectra identify a compact steering subspace and an empirically chosen intervention window (where these directions are strongest and most stable). (iii) Steer: We apply a signed, sparse activation shift targeted to the language neurons. Concretely, within low to mid layers we add a positive steering along the target-language dominant directions and a compensating negative shift toward the null space for the English neurons, yielding controllable target-language defaultness.

</details>


### [57] [Verification Required: The Impact of Information Credibility on AI Persuasion](https://arxiv.org/abs/2602.00970)
*Saaduddin Mahmud,Eugene Bagdasarian,Shlomo Zilberstein*

Main category: cs.CL

TL;DR: 论文提出MixTalk游戏框架，研究LLM代理在概率可信度信息下的战略沟通，开发TOPD方法提升接收者抗说服能力。


<details>
  <summary>Details</summary>
Motivation: LLM代理越来越多地应用于高风险决策场景，需要理解战略沟通。现有研究主要关注不可验证的廉价谈话或完全可验证的披露，无法捕捉信息具有概率可信度的现实领域。

Method: 提出MixTalk战略沟通游戏框架，发送者策略性地结合可验证和不可验证的声明，接收者分配有限预算进行成本验证。开发TOPD方法，从交互日志中提取锦标赛最优策略并在推理时部署。

Result: 在三个现实部署设置的大规模锦标赛中评估最先进的LLM代理，揭示了它们在推理信息可信度和塑造这些交互的显式行为方面的优势和局限。TOPD显著提高了接收者对说服的鲁棒性。

Conclusion: MixTalk框架为研究LLM代理在概率可信度信息下的战略沟通提供了有效工具，TOPD方法能够显著提升接收者的决策鲁棒性，对实际部署具有重要价值。

Abstract: Agents powered by large language models (LLMs) are increasingly deployed in settings where communication shapes high-stakes decisions, making a principled understanding of strategic communication essential. Prior work largely studies either unverifiable cheap-talk or fully verifiable disclosure, failing to capture realistic domains in which information has probabilistic credibility. We introduce MixTalk, a strategic communication game for LLM-to-LLM interaction that models information credibility. In MixTalk, a sender agent strategically combines verifiable and unverifiable claims to communicate private information, while a receiver agent allocates a limited budget to costly verification and infers the underlying state from prior beliefs, claims, and verification outcomes. We evaluate state-of-the-art LLM agents in large-scale tournaments across three realistic deployment settings, revealing their strengths and limitations in reasoning about information credibility and the explicit behavior that shapes these interactions. Finally, we propose Tournament Oracle Policy Distillation (TOPD), an offline method that distills tournament oracle policy from interaction logs and deploys it in-context at inference time. Our results show that TOPD significantly improves receiver robustness to persuasion.

</details>


### [58] [Trust in One Round: Confidence Estimation for Large Language Models via Structural Signals](https://arxiv.org/abs/2602.00977)
*Pengyue Yang,Jiawen Wen,Haolin Jin,Linghan Huang,Huaming Chen,Ling Chen*

Main category: cs.CL

TL;DR: 提出Structural Confidence框架，通过分析LLM最后一层隐藏状态轨迹的多尺度结构信号，增强输出正确性预测，相比传统置信度估计方法更稳健高效。


<details>
  <summary>Details</summary>
Motivation: LLM在错误代价高的领域部署时，传统的置信度估计方法（如token似然、语义相似性、多样本一致性）在分布偏移、专业领域文本和计算限制下表现脆弱，需要更稳健的置信度估计框架。

Method: 提出Structural Confidence框架，从模型最后一层隐藏状态轨迹中提取多尺度结构信号，结合频谱、局部变化和全局形状描述符，捕捉概率和句子嵌入忽略的内部稳定性模式。

Result: 在FEVER、SciFact、WikiBio-hallucination和TruthfulQA四个异构基准上评估，Structural Confidence在AUROC和AUPR指标上优于现有基线方法，且只需单次确定性前向传播，计算效率高。

Conclusion: Structural Confidence为资源受限的LLM应用提供了高效、稳健的事后置信度估计方法，相比需要多次随机生成和辅助模型的采样一致性方法更具实用性。

Abstract: Large language models (LLMs) are increasingly deployed in domains where errors carry high social, scientific, or safety costs. Yet standard confidence estimators, such as token likelihood, semantic similarity and multi-sample consistency, remain brittle under distribution shift, domain-specialised text, and compute limits. In this work, we present Structural Confidence, a single-pass, model-agnostic framework that enhances output correctness prediction based on multi-scale structural signals derived from a model's final-layer hidden-state trajectory. By combining spectral, local-variation, and global shape descriptors, our method captures internal stability patterns that are missed by probabilities and sentence embeddings. We conduct extensive, cross-domain evaluation across four heterogeneous benchmarks-FEVER (fact verification), SciFact (scientific claims), WikiBio-hallucination (biographical consistency), and TruthfulQA (truthfulness-oriented QA). Our Structural Confidence framework demonstrates strong performance compared with established baselines in terms of AUROC and AUPR. More importantly, unlike sampling-based consistency methods which require multiple stochastic generations and an auxiliary model, our approach uses a single deterministic forward pass, offering a practical basis for efficient, robust post-hoc confidence estimation in socially impactful, resource-constrained LLM applications.

</details>


### [59] [MedSpeak: A Knowledge Graph-Aided ASR Error Correction Framework for Spoken Medical QA](https://arxiv.org/abs/2602.00981)
*Yutong Song,Shiva Shrestha,Chenhan Lyu,Elahe Khatibi,Pengfei Zhang,Honghui Xu,Nikil Dutt,Amir Rahmani*

Main category: cs.CL

TL;DR: MedSpeak是一个基于知识图谱的ASR错误校正框架，通过结合医学知识图谱中的语义关系和语音信息，以及LLM的推理能力，显著提升医学术语识别准确性和医疗问答性能。


<details>
  <summary>Details</summary>
Motivation: 基于自动语音识别(ASR)的医疗问答系统在识别医学术语时存在准确性问题，这影响了整个问答系统的性能。

Method: 提出MedSpeak框架，利用医学知识图谱中的语义关系和语音信息，结合大语言模型(LLMs)的推理能力，对噪声转录文本进行校正，并改进下游答案预测。

Result: 在基准测试上的综合实验结果表明，MedSpeak显著提高了医学术语识别准确性和整体医疗问答性能，成为医疗问答领域的最先进解决方案。

Conclusion: MedSpeak通过知识图谱辅助的ASR错误校正，有效解决了医疗问答系统中医学术语识别不准确的问题，为医疗语音问答提供了强有力的解决方案。

Abstract: Spoken question-answering (SQA) systems relying on automatic speech recognition (ASR) often struggle with accurately recognizing medical terminology. To this end, we propose MedSpeak, a novel knowledge graph-aided ASR error correction framework that refines noisy transcripts and improves downstream answer prediction by leveraging both semantic relationships and phonetic information encoded in a medical knowledge graph, together with the reasoning power of LLMs. Comprehensive experimental results on benchmarks demonstrate that MedSpeak significantly improves the accuracy of medical term recognition and overall medical SQA performance, establishing MedSpeak as a state-of-the-art solution for medical SQA. The code is available at https://github.com/RainieLLM/MedSpeak.

</details>


### [60] [DISPO: Enhancing Training Efficiency and Stability in Reinforcement Learning for Large Language Model Mathematical Reasoning](https://arxiv.org/abs/2602.00983)
*Batuhan K. Karaman,Aditya Rawal,Suhaila Shakiah,Mohammad Ghavamzadeh,Mingyi Hong,Arijit Biswas,Ruida Zhou*

Main category: cs.CL

TL;DR: DISPO是一种新的强化学习算法，通过分离正确和错误响应的重要性采样权重上下裁剪，实现四个可控策略更新机制，在数学推理任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习方法在训练稳定性和学习效率之间存在权衡：PPO风格方法稳定但学习慢，REINFORCE风格方法效率高但不稳定。需要解决这一局限性。

Method: DISPO算法将正确和错误响应的重要性采样权重上下裁剪解耦，形成四个可控策略更新机制，分别调节探索和蒸馏的平衡。

Result: 在AIME'24上达到61.04%准确率（优于CISPO的55.42%和DAPO的50.21%），在各种基准测试和模型上都显示出类似优势。

Conclusion: DISPO通过精细控制策略更新机制，在保持探索-蒸馏平衡的同时防止灾难性失败，实现了训练稳定性和学习效率的更好平衡。

Abstract: Reinforcement learning with verifiable rewards has emerged as a promising paradigm for enhancing the reasoning capabilities of large language models particularly in mathematics. Current approaches in this domain present a clear trade-off: PPO-style methods (e.g., GRPO/DAPO) offer training stability but exhibit slow learning trajectories due to their trust-region constraints on policy updates, while REINFORCE-style approaches (e.g., CISPO) demonstrate improved learning efficiency but suffer from performance instability as they clip importance sampling weights while still permitting non-zero gradients outside the trust-region. To address these limitations, we introduce DISPO, a simple yet effective REINFORCE-style algorithm that decouples the up-clipping and down-clipping of importance sampling weights for correct and incorrect responses, yielding four controllable policy update regimes. Through targeted ablations, we uncover how each regime impacts training: for correct responses, weights >1 increase the average token entropy (i.e., exploration) while weights <1 decrease it (i.e., distillation) -- both beneficial but causing gradual performance degradation when excessive. For incorrect responses, overly restrictive clipping triggers sudden performance collapse through repetitive outputs (when weights >1) or vanishing response lengths (when weights <1). By separately tuning these four clipping parameters, DISPO maintains the exploration-distillation balance while preventing catastrophic failures, achieving 61.04% on AIME'24 (vs. 55.42% CISPO and 50.21% DAPO) with similar gains across various benchmarks and models.

</details>


### [61] [Sparse Reward Subsystem in Large Language Models](https://arxiv.org/abs/2602.00986)
*Guowei Xu,Mert Yuksekgonul,James Zou*

Main category: cs.CL

TL;DR: 在LLM隐藏状态中发现类似人脑奖励系统的稀疏奖励子系统，包含表示状态期望值的价值神经元和编码奖励预测误差的多巴胺神经元，这些神经元对推理至关重要且具有跨数据集和模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 受生物大脑奖励系统启发，探索大型语言模型内部是否存在类似的奖励处理机制，以理解LLM的推理过程和内部表示。

Method: 通过干预实验识别LLM隐藏状态中的价值神经元，分析其在不同数据集、模型规模和架构下的表现，并识别编码奖励预测误差的多巴胺神经元。

Result: 发现价值神经元在推理中起关键作用，具有跨数据集、模型规模和架构的鲁棒性，且在同一基础模型微调的不同模型间具有可迁移性。识别出编码奖励预测误差的多巴胺神经元，其激活模式与预期奖励偏差相关。

Conclusion: LLM内部存在类似生物大脑的稀疏奖励子系统，包含价值神经元和多巴胺神经元，这些发现为理解LLM的推理机制和内部表示提供了新视角。

Abstract: In this paper, we identify a sparse reward subsystem within the hidden states of Large Language Models (LLMs), drawing an analogy to the biological reward subsystem in the human brain. We demonstrate that this subsystem contains value neurons that represent the model's internal expectation of state value, and through intervention experiments, we establish the importance of these neurons for reasoning. Our experiments reveal that these value neurons are robust across diverse datasets, model scales, and architectures; furthermore, they exhibit significant transferability across different datasets and models fine-tuned from the same base model. By examining cases where value predictions and actual rewards diverge, we identify dopamine neurons within the reward subsystem which encode reward prediction errors (RPE). These neurons exhibit high activation when the reward is higher than expected and low activation when the reward is lower than expected.

</details>


### [62] [DeALOG: Decentralized Multi-Agents Log-Mediated Reasoning Framework](https://arxiv.org/abs/2602.00996)
*Abhijit Chakraborty,Ashish Raj Shekhar,Shiven Agarwal,Vivek Gupta*

Main category: cs.CL

TL;DR: DeALOG是一个去中心化的多智能体框架，用于跨文本、表格和图像的复杂问答，通过专业智能体协作和共享自然语言日志实现鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 跨文本、表格和图像的复杂问答需要整合多样化信息源，需要一个支持专业化处理、协调和可解释性的框架。

Method: 使用专业智能体（表格、上下文、视觉、总结和验证）通过共享自然语言日志作为持久内存进行通信，实现协作错误检测和验证。

Result: 在FinQA、TAT-QA、CRT-QA、WikiTableQuestions、FeTaQA和MultiModalQA等多个数据集上表现出竞争力，验证了共享日志、智能体专业化和验证机制的重要性。

Conclusion: DeALOG通过模块化组件和自然语言通信提供了一个可扩展的方法，改善了多模态问答的鲁棒性和准确性。

Abstract: Complex question answering across text, tables and images requires integrating diverse information sources. A framework supporting specialized processing with coordination and interpretability is needed. We introduce DeALOG, a decentralized multi-agent framework for multimodal question answering. It uses specialized agents: Table, Context, Visual, Summarizing and Verification, that communicate through a shared natural-language log as persistent memory. This log-based approach enables collaborative error detection and verification without central control, improving robustness. Evaluations on FinQA, TAT-QA, CRT-QA, WikiTableQuestions, FeTaQA, and MultiModalQA show competitive performance. Analysis confirms the importance of the shared log, agent specialization, and verification for accuracy. DeALOG, provides a scalable approach through modular components using natural-language communication.

</details>


### [63] [Reliable Use of Lemmas via Eligibility Reasoning and Section$-$Aware Reinforcement Learning](https://arxiv.org/abs/2602.00998)
*Zhikun Xu,Xiaodong Yu,Ben Zhou,Jiang Liu,Jialian Wu,Ze Wang,Ximeng Sun,Hao Chen,Zicheng Liu*

Main category: cs.CL

TL;DR: 论文提出RULES方法，通过结构化预测任务训练LLMs进行引理判断，包含前提检查和结论效用检查两个部分，使用强化学习和部分感知损失掩码提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在数学基准测试中表现良好，但经常错误应用引理，在不验证假设的情况下直接使用结论。需要提升模型对引理适用性的判断能力。

Method: 将引理判断形式化为结构化预测任务：给定陈述和候选引理，模型必须输出前提检查和结论效用检查。提出RULES方法，采用两段式输出结构，使用强化学习和部分感知损失掩码训练，只惩罚导致错误的部分。

Result: 在领域内任务上，RULES相比普通模型和单标签RL基线有稳定提升；在适用性破坏扰动上改进更大；在端到端任务上达到持平或适度提升。消融实验表明两段式输出和部分感知强化学习对鲁棒性都是必要的。

Conclusion: RULES方法通过结构化引理判断任务和针对性的训练机制，有效提升了LLMs在数学推理中对引理适用性的判断能力，特别是在面对扰动时的鲁棒性。

Abstract: Recent large language models (LLMs) perform strongly on mathematical benchmarks yet often misapply lemmas, importing conclusions without validating assumptions. We formalize lemma$-$judging as a structured prediction task: given a statement and a candidate lemma, the model must output a precondition check and a conclusion$-$utility check, from which a usefulness decision is derived. We present RULES, which encodes this specification via a two$-$section output and trains with reinforcement learning plus section$-$aware loss masking to assign penalty to the section responsible for errors. Training and evaluation draw on diverse natural language and formal proof corpora; robustness is assessed with a held$-$out perturbation suite; and end$-$to$-$end evaluation spans competition$-$style, perturbation$-$aligned, and theorem$-$based problems across various LLMs. Results show consistent in$-$domain gains over both a vanilla model and a single$-$label RL baseline, larger improvements on applicability$-$breaking perturbations, and parity or modest gains on end$-$to$-$end tasks; ablations indicate that the two$-$section outputs and section$-$aware reinforcement are both necessary for robustness.

</details>


### [64] [Distilling Token-Trained Models into Byte-Level Models](https://arxiv.org/abs/2602.01007)
*Zishuo Bao,Jiaqi Leng,Junxiong Wang,Bowen Peng,Yucheng Lu*

Main category: cs.CL

TL;DR: 提出一种高效蒸馏方法，将现有分词训练的大语言模型转换为字节语言模型，仅需约1250亿字节数据，保留原模型大部分能力


<details>
  <summary>Details</summary>
Motivation: 现有字节语言模型需要从头训练数万亿字节，计算成本极高。希望找到一种高效方法，利用已有分词训练模型转换为字节模型，降低训练成本

Method: 采用两阶段课程学习：1) 渐进知识蒸馏，对齐字节级表示与分词教师模型的嵌入；2) 字节级监督微调，实现完全在字节空间的端到端生成

Result: 在Llama、Qwen、OLMo等多个模型家族上验证，蒸馏后的字节语言模型仅使用约1250亿字节数据，就能保留教师模型大部分性能

Conclusion: 提出的蒸馏方法能够高效地将分词训练模型转换为字节语言模型，显著降低训练成本，为字节语言模型的推广提供可行路径

Abstract: Byte Language Models (BLMs) have emerged as a promising direction for scaling language models beyond tokenization. However, existing BLMs typically require training from scratch on trillions of bytes, making them prohibitively expensive. In this paper, we propose an efficient distillation recipe that converts existing token-trained LLMs into BLMs while retaining comparable capabilities. Our recipe follows a two-stage curriculum: (1) Progressive Knowledge Distillation, which aligns byte-level representations with the embeddings of the token-trained teacher model; and (2) Byte-Level Supervised Fine-Tuning, which enables end-to-end generation entirely in the byte space. We validate our approach across multiple model families, including Llama, Qwen, and OLMo, and demonstrate that the distilled BLMs retain most of the teacher models' performance using only approximately 125B bytes.

</details>


### [65] [Bias in the Ear of the Listener: Assessing Sensitivity in Audio Language Models Across Linguistic, Demographic, and Positional Variations](https://arxiv.org/abs/2602.01030)
*Sheng-Lun Wei,Yu-Ling Liao,Yen-Hua Chang,Hen-Hsen Huang,Hsin-Hsi Chen*

Main category: cs.CL

TL;DR: 首次系统研究多语言MLLMs中的语音偏见，构建BiasInEar数据集，发现MLLMs对语言和选项顺序高度敏感，语音会放大现有结构偏见


<details>
  <summary>Details</summary>
Motivation: 目前缺乏对多语言多模态大语言模型（MLLMs）中语音偏见的系统性研究，需要建立统一的评估框架来评估语音集成LLM的公平性和鲁棒性

Method: 构建BiasInEar数据集（基于Global MMLU Lite，涵盖英中韩三语，平衡性别和口音，共70.8小时语音）；使用四种互补指标（准确率、熵、APES、Fleiss' κ）评估9个代表性模型；分析语言、口音、性别、选项顺序等扰动因素

Result: MLLMs对人口统计因素（如性别）相对鲁棒，但对语言和选项顺序高度敏感；架构设计和推理策略显著影响跨语言鲁棒性；语音会放大现有结构偏见

Conclusion: 本研究建立了评估语音集成LLM公平性和鲁棒性的统一框架，填补了文本和语音评估之间的空白；揭示了语音如何放大结构偏见，为未来更公平的MLLM设计提供指导

Abstract: This work presents the first systematic investigation of speech bias in multilingual MLLMs. We construct and release the BiasInEar dataset, a speech-augmented benchmark based on Global MMLU Lite, spanning English, Chinese, and Korean, balanced by gender and accent, and totaling 70.8 hours ($\approx$4,249 minutes) of speech with 11,200 questions. Using four complementary metrics (accuracy, entropy, APES, and Fleiss' $κ$), we evaluate nine representative models under linguistic (language and accent), demographic (gender), and structural (option order) perturbations. Our findings reveal that MLLMs are relatively robust to demographic factors but highly sensitive to language and option order, suggesting that speech can amplify existing structural biases. Moreover, architectural design and reasoning strategy substantially affect robustness across languages. Overall, this study establishes a unified framework for assessing fairness and robustness in speech-integrated LLMs, bridging the gap between text- and speech-based evaluation. The resources can be found at https://github.com/ntunlplab/BiasInEar.

</details>


### [66] [Personality Expression Across Contexts: Linguistic and Behavioral Variation in LLM Agents](https://arxiv.org/abs/2602.01063)
*Bin Han,Deuksin Kwon,Jonathan Gratch*

Main category: cs.CL

TL;DR: 研究发现LLMs在相同人格提示下，会根据不同对话情境（破冰、谈判、群体决策、共情任务）表现出不同的语言、行为和情感结果，显示出情境敏感而非固定的人格表达。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索LLMs在相同人格提示下，其行为实现如何随上下文变化，以及这种变化反映的是不一致性还是类似人类的情境敏感适应。

Method: 研究通过四种对话情境（破冰、谈判、群体决策、共情任务）来检验相同人格提示下LLMs的行为变化，分析其语言、行为和情感结果。

Result: 结果显示情境线索系统性地影响人格表达和情感基调，相同特质在不同社交和情感需求下表达方式不同，LLMs表现出情境敏感而非固定的人格表达。

Conclusion: 从整体特质理论视角看，LLMs表现出情境敏感的人格表达，能够灵活适应社交互动目标和情感条件，这引发了对LLM对话代理行为一致性vs情境适应性的重要讨论。

Abstract: Large Language Models (LLMs) can be conditioned with explicit personality prompts, yet their behavioral realization often varies depending on context. This study examines how identical personality prompts lead to distinct linguistic, behavioral, and emotional outcomes across four conversational settings: ice-breaking, negotiation, group decision, and empathy tasks. Results show that contextual cues systematically influence both personality expression and emotional tone, suggesting that the same traits are expressed differently depending on social and affective demands. This raises an important question for LLM-based dialogue agents: whether such variations reflect inconsistency or context-sensitive adaptation akin to human behavior. Viewed through the lens of Whole Trait Theory, these findings highlight that LLMs exhibit context-sensitive rather than fixed personality expression, adapting flexibly to social interaction goals and affective conditions.

</details>


### [67] [Exploring Knowledge Purification in Multi-Teacher Knowledge Distillation for LLMs](https://arxiv.org/abs/2602.01064)
*Ruihan Jin,Pengpeng Shao,Zhengqi Wen,Jinyang Wu,Mingkuan Feng,Shuo Yang,Chu Yuan Zhang,Jianhua Tao*

Main category: cs.CL

TL;DR: 提出知识净化概念，将多个教师大语言模型的推理过程整合为单一推理，缓解知识冲突并提升蒸馏效率，实验验证了五种净化方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏方法在利用多个教师模型时面临知识冲突和高资源需求的问题，需要更高效的多教师蒸馏方案。

Method: 提出知识净化概念，将多个教师LLM的推理过程整合为单一推理，并设计了五种净化方法（包括基于路由的方法等）从不同角度实现知识净化。

Result: 实验表明这些方法不仅能提升蒸馏模型的性能，还能有效缓解知识冲突，基于路由的方法展现出强大的泛化能力。

Conclusion: 知识净化技术能够优化多教师蒸馏过程，促进强大而轻量级模型的实际部署，展示了创新净化技术在知识蒸馏中的潜力。

Abstract: Knowledge distillation has emerged as a pivotal technique for transferring knowledge from stronger large language models (LLMs) to smaller, more efficient models. However, traditional distillation approaches face challenges related to knowledge conflicts and high resource demands, particularly when leveraging multiple teacher models. In this paper, we introduce the concept of \textbf{Knowledge Purification}, which consolidates the rationales from multiple teacher LLMs into a single rationale, thereby mitigating conflicts and enhancing efficiency. To investigate the effectiveness of knowledge purification, we further propose five purification methods from various perspectives. Our experiments demonstrate that these methods not only improve the performance of the distilled model but also effectively alleviate knowledge conflicts. Moreover, router-based methods exhibit robust generalization capabilities, underscoring the potential of innovative purification techniques in optimizing multi-teacher distillation and facilitating the practical deployment of powerful yet lightweight models.

</details>


### [68] [From Utterance to Vividity: Training Expressive Subtitle Translation LLM via Adaptive Local Preference Optimization](https://arxiv.org/abs/2602.01068)
*Chaoqun Cui,Shijing Wang,Liangbin Huang,Qingqing Gu,Zhaolong Huang,Xiao Zeng,Wenji Mao*

Main category: cs.CL

TL;DR: 提出ALPO方法训练面向视觉媒体字幕翻译的领域定制化LLM，通过构建多语言字幕平行语料库和细粒度偏好对齐提升翻译表现力


<details>
  <summary>Details</summary>
Motivation: 随着应用场景复杂化，通用大语言模型在垂直领域翻译中的局限性日益明显，特别是在视觉媒体字幕翻译这种需要表现力和生动性的领域

Method: 1. 构建并发布多语言字幕平行语料库数据集；2. 提出自适应局部偏好优化（ALPO）方法解决细粒度偏好对齐问题；3. 验证LLM作为翻译奖励模型和评估器的可靠性

Result: ALPO方法在翻译质量的多维度评估中表现出色，能够训练出具有表现力和生动性的字幕翻译LLM

Conclusion: 通过领域定制化方法可以显著提升LLM在垂直翻译任务中的性能，ALPO方法为训练表达性翻译LLM提供了有效解决方案

Abstract: The rapid development of Large Language Models (LLMs) has significantly enhanced the general capabilities of machine translation. However, as application scenarios become more complex, the limitations of LLMs in vertical domain translations are gradually becoming apparent. In this study, we focus on how to construct translation LLMs that meet the needs of domain customization. We take visual media subtitle translation as our topic and explore how to train expressive and vivid translation LLMs. We investigated the situations of subtitle translation and other domains of literal and liberal translation, verifying the reliability of LLM as reward model and evaluator for translation. Additionally, to train an expressive translation LLM, we constructed and released a multidirectional subtitle parallel corpus dataset and proposed the Adaptive Local Preference Optimization (ALPO) method to address fine-grained preference alignment. Experimental results demonstrate that ALPO achieves outstanding performance in multidimensional evaluation of translation quality.

</details>


### [69] [What If We Allocate Test-Time Compute Adaptively?](https://arxiv.org/abs/2602.01070)
*Ahsan Bilal,Ahmed Mohsin,Muhammad Umer,Ali Subhan,Hassan Rizwan,Ayesha Mohsin,Dean Hougen*

Main category: cs.CL

TL;DR: 提出验证器引导的自适应推理框架，通过迭代轨迹生成与选择，动态分配计算资源，显著优于均匀计算分配方法


<details>
  <summary>Details</summary>
Motivation: 现有测试时计算扩展方法采用均匀计算分配、固定采样策略，验证仅用于重排序，效率低下。需要更智能的计算分配机制，将验证引导融入推理过程本身

Method: 提出验证器引导的自适应框架：1) 多轮迭代推理；2) 每轮可选生成高级计划、选择推理工具和计算策略；3) 过程奖励模型作为统一控制信号，指导生成过程中的剪枝与扩展；4) 跨迭代使用轨迹奖励选择最终响应

Result: 在多个数据集上，动态PRM引导方法持续优于直接测试时扩展，在MATH-500上获得大幅提升，在AIME24和AMO-Bench等更难基准上实现数倍改进。验证引导分配将计算集中在高效用推理路径上

Conclusion: 验证引导的自适应计算分配比均匀扩展更有效，过程奖励模型作为统一控制信号能够智能指导推理过程，显著提升复杂数学推理任务的性能

Abstract: Test-time compute scaling allocates inference computation uniformly, uses fixed sampling strategies, and applies verification only for reranking. In contrast, we propose a verifier-guided adaptive framework treating reasoning as iterative trajectory generation and selection. For each problem, the agent runs multiple inference iterations. In each iteration, it optionally produces a high-level plan, selects a set of reasoning tools and a compute strategy together with an exploration parameter, and then generates a candidate reasoning trajectory. A process reward model (PRM) serves as a unified control signal: within each iteration, step-level PRM scores are aggregated to guide pruning and expansion during generation, and across iterations, aggregated trajectory rewards are used to select the final response. Across datasets, our dynamic, PRM-guided approach consistently outperforms direct test-time scaling, yielding large gains on MATH-500 and several-fold improvements on harder benchmarks such as AIME24 and AMO-Bench. We characterize efficiency using theoretical FLOPs and a compute intensity metric penalizing wasted generation and tool overhead, demonstrating that verification-guided allocation concentrates computation on high-utility reasoning paths.

</details>


### [70] [Logic-Oriented Retriever Enhancement via Contrastive Learning](https://arxiv.org/abs/2602.01116)
*Wenxuan Zhang,Yuan-Hao Jiang,Changyong Qi,Rui Jia,Yonghe Wu*

Main category: cs.CL

TL;DR: LORE通过细粒度对比学习激活LLM的逻辑推理能力，提升知识密集型任务中的检索效果，无需外部监督或资源


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在知识密集型任务中表现不佳，因为传统检索器往往过度依赖表面相似性，无法处理涉及复杂逻辑关系的查询。模型本身具备逻辑分析能力，但在标准训练中未得到充分利用。

Method: LORE采用细粒度对比学习，激活模型潜在的逻辑推理能力，引导嵌入向量向符合逻辑结构的证据对齐，而不是浅层相似性。该方法无需外部监督、额外资源或预检索分析，保持索引兼容性。

Result: LORE能持续提升检索效用和下游生成质量，同时保持效率。方法在多个数据集上验证有效。

Conclusion: LORE通过激活模型内在的逻辑推理能力，有效解决了传统检索器在复杂逻辑查询中的局限性，为知识密集型任务提供了高效且无需外部资源的改进方案。

Abstract: Large language models (LLMs) struggle in knowledge-intensive tasks, as retrievers often overfit to surface similarity and fail on queries involving complex logical relations. The capacity for logical analysis is inherent in model representations but remains underutilized in standard training. LORE (Logic ORiented Retriever Enhancement) introduces fine-grained contrastive learning to activate this latent capacity, guiding embeddings toward evidence aligned with logical structure rather than shallow similarity. LORE requires no external upervision, resources, or pre-retrieval analysis, remains index-compatible, and consistently improves retrieval utility and downstream generation while maintaining efficiency. The datasets and code are publicly available at https://github.com/mazehart/Lore-RAG.

</details>


### [71] [Tendem: A Hybrid AI+Human Platform](https://arxiv.org/abs/2602.01119)
*Konstantin Chernyshev,Ekaterina Artemova,Viacheslav Zhukov,Maksim Nerush,Mariia Fedorova,Iryna Repik,Olga Shapovalova,Aleksey Sukhorosov,Vladimir Dobrovolskii,Natalia Mikhailova,Sergei Tilga*

Main category: cs.CL

TL;DR: Tendem是一个AI处理结构化重复工作、人类专家在模型失败时介入的混合系统，在94个真实任务评估中比纯AI代理和纯人工工作流表现更好，质量更高、速度更快，成本与纯人工相当。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统在处理复杂任务时存在局限性，纯AI代理可能失败或产生不准确结果，而纯人工工作流效率较低。需要结合AI和人类专家的优势，构建一个既能保证质量又能提高效率的混合系统。

Method: 开发Tendem混合系统：AI处理结构化重复工作，人类专家在模型失败时介入或验证结果。每个结果在交付前都经过全面的质量审查。通过94个真实世界任务进行内部评估，与纯AI代理和Upwork自由职业者的纯人工工作流进行比较。

Result: Tendem始终提供更高质量的输出和更快的周转时间，同时运营成本与纯人工执行相当。在第三方代理基准测试中，Tendem的AI代理（无人参与）在网页浏览和工具使用任务上接近最先进水平，在领域知识和推理方面表现强劲。

Conclusion: Tendem混合系统成功结合了AI和人类专家的优势，在保持成本竞争力的同时，显著提高了任务完成的质量和效率，展示了人机协作在实际应用中的巨大潜力。

Abstract: Tendem is a hybrid system where AI handles structured, repeatable work and Human Experts step in when the models fail or to verify results. Each result undergoes a comprehensive quality review before delivery to the Client. To assess Tendem's performance, we conducted a series of in-house evaluations on 94 real-world tasks, comparing it with AI-only agents and human-only workflows carried out by Upwork freelancers. The results show that Tendem consistently delivers higher-quality outputs with faster turnaround times. At the same time, its operational costs remain comparable to human-only execution. On third-party agentic benchmarks, Tendem's AI Agent (operating autonomously, without human involvement) performs near state-of-the-art on web browsing and tool-use tasks while demonstrating strong results in frontier domain knowledge and reasoning.

</details>


### [72] [Long-range Modeling and Processing of Multimodal Event Sequences](https://arxiv.org/abs/2602.01125)
*Jichu Li,Yilun Zhong,Zhiting Li,Feng Zhou,Quyu Kong*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的框架，将基于LLM的时间点过程扩展到视觉模态，通过自适应序列压缩机制解决长上下文问题，在预测准确性和生成文本分析质量方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有时间点过程方法在处理多模态数据时面临序列长度急剧增加的问题，导致基于注意力的模型难以生成需要长距离理解的连贯长文本描述。需要一种能够处理多模态内容并推理事件动态的新方法。

Method: 提出基于时间相似性的自适应序列压缩机制，减少序列长度同时保留关键模式；采用两阶段范式：先在压缩序列上进行预训练，然后对下游任务进行监督微调；将文本生成定位为核心能力，与时间和类型预测并列。

Result: 在包括具有挑战性的DanmakuTPP-QA基准测试在内的广泛实验中，该方法在预测准确性和生成文本分析质量方面均优于最先进的基线方法。

Conclusion: 该框架成功将LLM-based TPPs扩展到视觉模态，通过自适应压缩机制有效解决了长上下文问题，实现了多模态事件序列的建模和高质量文本生成。

Abstract: Temporal point processes (TPPs) have emerged as powerful tools for modeling asynchronous event sequences. While recent advances have extended TPPs to handle textual information, existing approaches are limited in their ability to generate rich, multimodal content and reason about event dynamics. A key challenge is that incorporating multimodal data dramatically increases sequence length, hindering the ability of attention-based models to generate coherent, long-form textual descriptions that require long-range understanding. In this paper, we propose a novel framework that extends LLM-based TPPs to the visual modality, positioning text generation as a core capability alongside time and type prediction. Our approach addresses the long-context problem through an adaptive sequence compression mechanism based on temporal similarity, which reduces sequence length while preserving essential patterns. We employ a two-stage paradigm of pre-training on compressed sequences followed by supervised fine-tuning for downstream tasks. Extensive experiments, including on the challenging DanmakuTPP-QA benchmark, demonstrate that our method outperforms state-of-the-art baselines in both predictive accuracy and the quality of its generated textual analyses.

</details>


### [73] [Don't Judge a Book by its Cover: Testing LLMs' Robustness Under Logical Obfuscation](https://arxiv.org/abs/2602.01132)
*Abhilekh Borah,Shubhra Ghosh,Kedar Joshi,Aditya Kumar Guru,Kripabandhu Ghosh*

Main category: cs.CL

TL;DR: 论文提出Logifus逻辑混淆框架和LogiQAte诊断基准，发现LLMs在逻辑等价但表面形式混淆的问题上表现大幅下降，揭示模型缺乏深层理解能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在标准形式下能很好处理逻辑推理任务，但在逻辑等价但表面形式混淆的问题上表现不佳。为了研究这种脆弱性，需要创建系统性的诊断工具来评估模型对逻辑结构的深层理解能力。

Method: 提出Logifus结构保持的逻辑混淆框架，并基于此创建LogiQAte诊断基准，包含1,108个问题，涵盖四个推理任务：混淆一阶逻辑、混淆血缘关系、混淆数字序列、混淆方向感。评估了六个最先进模型。

Result: 混淆严重降低了零样本性能：GPT-4o平均下降47%，GPT-5下降27%，推理模型o4-mini下降22%。所有模型在逻辑等价但表面形式混淆的问题上都表现显著下降。

Conclusion: 当前LLMs解析问题缺乏深层理解，仅依赖表面形式。研究强调了构建真正理解和保持意义超越表面形式的模型的紧迫性，揭示了现有模型在逻辑推理鲁棒性方面的严重缺陷。

Abstract: Tasks such as solving arithmetic equations, evaluating truth tables, and completing syllogisms are handled well by large language models (LLMs) in their standard form, but they often fail when the same problems are posed in logically equivalent yet obfuscated formats. To study this vulnerability, we introduce Logifus, a structure-preserving logical obfuscation framework, and, utilizing this, we present LogiQAte, a first-of-its-kind diagnostic benchmark with 1,108 questions across four reasoning tasks: (i) Obfus FOL (first-order logic entailment under equivalence-preserving rewrites), (ii) Obfus Blood Relation (family-graph entailment under indirect relational chains), (iii) Obfus Number Series (pattern induction under symbolic substitutions), and (iv) Obfus Direction Sense (navigation reasoning under altered directions and reference frames). Across all the tasks, evaluating six state-of-the-art models, we find that obfuscation severely degrades zero-shot performance, with performance dropping on average by 47% for GPT-4o, 27% for GPT-5, and 22% for reasoning model, o4-mini. Our findings reveal that current LLMs parse questions without deep understanding, highlighting the urgency of building models that genuinely comprehend and preserve meaning beyond surface form.

</details>


### [74] [Beyond Training for Cultural Awareness: The Role of Dataset Linguistic Structure in Large Language Models](https://arxiv.org/abs/2602.01161)
*Reem I. Masoud,Chen Feng,Shunta Asano,Saied Alshahrani,Philip Colin Treleaven,Miguel R. D. Rodrigues*

Main category: cs.CL

TL;DR: 研究通过分析阿拉伯语、中文和日语微调数据集的轻量级语言特征，识别出与跨模型文化性能相关的可预测属性，发现词汇导向特征比语义或多样性特征更稳健。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型全球部署引发文化错位担忧，但用于文化适应的微调数据集的语言特性尚未被充分理解。研究旨在探索微调数据的哪些语言属性与文化性能相关，这些属性是否可在训练前预测，以及这些效应如何随模型变化。

Method: 对阿拉伯语、中文和日语数据集计算轻量级语言、语义和结构指标，分别进行主成分分析以确保捕捉同一语言内数据集间的变异而非语言间差异。随后在三个主要LLM家族（LLaMA、Mistral、DeepSeek）上进行微调，并在文化知识、价值观和规范基准上评估。

Result: 主成分分析识别出与语义连贯性、表层词汇句法多样性、词汇或结构丰富度相关的可解释轴，但组成因语言而异。PCA成分与下游性能相关，但关联强烈依赖于模型。通过受控子集干预，发现词汇导向成分（PC3）最稳健，在不同模型和基准上表现更一致，而强调语义或多样性极端值（PC1-PC2）通常中性或有害。

Conclusion: 微调数据集的语言特性可预测文化性能，但效应模型依赖。词汇导向特征比语义或多样性特征更稳健，为文化对齐的数据集设计提供实用指导。

Abstract: The global deployment of large language models (LLMs) has raised concerns about cultural misalignment, yet the linguistic properties of fine-tuning datasets used for cultural adaptation remain poorly understood. We adopt a dataset-centric view of cultural alignment and ask which linguistic properties of fine-tuning data are associated with cultural performance, whether these properties are predictive prior to training, and how these effects vary across models. We compute lightweight linguistic, semantic, and structural metrics for Arabic, Chinese, and Japanese datasets and apply principal component analysis separately within each language. This design ensures that the resulting components capture variation among datasets written in the same language rather than differences between languages. The resulting components correspond to broadly interpretable axes related to semantic coherence, surface-level lexical and syntactic diversity, and lexical or structural richness, though their composition varies across languages. We fine-tune three major LLM families (LLaMA, Mistral, DeepSeek) and evaluate them on benchmarks of cultural knowledge, values, and norms. While PCA components correlate with downstream performance, these associations are strongly model-dependent. Through controlled subset interventions, we show that lexical-oriented components (PC3) are the most robust, yielding more consistent performance across models and benchmarks, whereas emphasizing semantic or diversity extremes (PC1-PC2) is often neutral or harmful.

</details>


### [75] [Typologically-Informed Candidate Reranking for LLM-based Translation into Low-Resource Languages](https://arxiv.org/abs/2602.01162)
*Nipuna Abeykoon,Ashen Weerathunga,Pubudu Wijesinghe,Parameswari Krishnamurthy*

Main category: cs.CL

TL;DR: 提出一个利用语言类型学改进低资源语言翻译质量的框架，无需平行训练数据或模型重训练，通过语言消歧和类型学合规评分来提升翻译质量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型主要在高资源语言上训练，对主导类型模式存在系统性偏见，导致在翻译到类型学差异大的低资源语言时出现结构不一致问题。需要一种无需平行训练数据就能改善翻译质量的方法。

Method: 框架包含两个组件：1) 通用元语言框架(UMF)，在16个类型学维度上构建结构化语言特征，使用差异加权评分；2) 计算引擎，在生成时进行语言消歧，在候选输出选择时进行类型学合规评分。

Result: 在9个语言对的评估中，干预率与英语的类型学距离强相关。在341个英语句子的实验中，框架对保守处理语言的干预精度为48.16%，对形态密集语言为28.15%，对结构特征化语言为86.26%。

Conclusion: 该框架无需平行训练数据，可与任何能产生多个候选输出的LLM配合使用，为资源不足的语言提供了实用的部署方案，有效解决了LLM在类型学差异语言翻译中的结构偏见问题。

Abstract: Large language models trained predominantly on high-resource languages exhibit systematic biases toward dominant typological patterns, leading to structural non-conformance when translating into typologically divergent low-resource languages. We present a framework that leverages linguistic typology to improve translation quality without parallel training data or model retraining. The framework consists of two components: the Universal Metalinguistic Framework (UMF), which represents languages as structured profiles across 16 typological dimensions with divergence-weighted scoring, and the Computational Engine, which operates through linguistic disambiguation during generation and typological compliance scoring during selection. Evaluation across nine language pairs demonstrates intervention rates strongly correlating with typological distance from English. In experiments on 341 English sentences each having different morphological and syntactic phenomena, the framework shows an intervention precision of 48.16% for conservatively treated languages, 28.15% for morphologically dense languages, and 86.26% for structurally profiled languages. The framework requires no parallel training data and operates with any LLM capable of producing multiple candidate outputs, enabling practical deployment for under-resourced languages.

</details>


### [76] [PedagoSense: A Pedology Grounded LLM System for Pedagogical Strategy Detection and Contextual Response Generation in Learning Dialogues](https://arxiv.org/abs/2602.01169)
*Shahem Sultan,Shahem Fadi,Yousef Melhim,Ibrahim Alsarraj,Besher Hassan*

Main category: cs.CL

TL;DR: PedagoSense系统通过两阶段分类器检测教学策略，结合LLM生成策略对齐的响应，提升对话式学习中的教学交互质量。


<details>
  <summary>Details</summary>
Motivation: 解决对话式学习中教学交互质量提升的挑战，通过检测和推荐有效的教学策略来改善师生对话效果。

Method: 提出PedagoSense系统：1）两阶段策略分类器（先二元检测是否存在教学策略，再细粒度分类具体策略）；2）基于对话上下文推荐策略；3）使用LLM生成与策略对齐的响应。

Result: 在人工标注的师生对话数据集上评估，教学策略检测性能高，数据增强带来一致提升，但细粒度分类仍具挑战性。

Conclusion: PedagoSense连接了教学理论与基于LLM的响应生成实践，为更自适应的教育技术提供了桥梁。

Abstract: This paper addresses the challenge of improving interaction quality in dialogue based learning by detecting and recommending effective pedagogical strategies in tutor student conversations. We introduce PedagoSense, a pedology grounded system that combines a two stage strategy classifier with large language model generation. The system first detects whether a pedagogical strategy is present using a binary classifier, then performs fine grained classification to identify the specific strategy. In parallel, it recommends an appropriate strategy from the dialogue context and uses an LLM to generate a response aligned with that strategy. We evaluate on human annotated tutor student dialogues, augmented with additional non pedagogical conversations for the binary task. Results show high performance for pedagogical strategy detection and consistent gains when using data augmentation, while analysis highlights where fine grained classes remain challenging. Overall, PedagoSense bridges pedagogical theory and practical LLM based response generation for more adaptive educational technologies.

</details>


### [77] [EmoAra: Emotion-Preserving English Speech Transcription and Cross-Lingual Translation with Arabic Text-to-Speech](https://arxiv.org/abs/2602.01170)
*Besher Hassan,Ibrahim Alsarraj,Musaab Hasan,Yousef Melhim,Shahem Fadi,Shahem Sultan*

Main category: cs.CL

TL;DR: EmoAra是一个端到端的情感保持跨语言语音通信系统，用于英语到阿拉伯语的银行客服场景，通过整合语音情感识别、语音识别、机器翻译和语音合成技术，在转换语言的同时保留情感信息。


<details>
  <summary>Details</summary>
Motivation: 银行客服场景中情感语境影响服务质量，需要跨语言沟通时保持情感信息，确保服务质量不受语言转换影响。

Method: 整合四个模块：CNN情感分类器、Whisper英语语音识别、微调MarianMT英阿翻译模型、MMS-TTS-Ara阿拉伯语语音合成，形成端到端处理流程。

Result: 情感分类F1分数94%，翻译性能BLEU 56和BERTScore F1 88.7%，银行领域翻译人工评估平均得分81%，系统代码和资源已开源。

Conclusion: EmoAra成功实现了跨语言语音通信中的情感保持，在银行客服场景中表现良好，为跨语言情感交流提供了有效解决方案。

Abstract: This work presents EmoAra, an end-to-end emotion-preserving pipeline for cross-lingual spoken communication, motivated by banking customer service where emotional context affects service quality. EmoAra integrates Speech Emotion Recognition, Automatic Speech Recognition, Machine Translation, and Text-to-Speech to process English speech and deliver an Arabic spoken output while retaining emotional nuance. The system uses a CNN-based emotion classifier, Whisper for English transcription, a fine-tuned MarianMT model for English-to-Arabic translation, and MMS-TTS-Ara for Arabic speech synthesis. Experiments report an F1-score of 94% for emotion classification, translation performance of BLEU 56 and BERTScore F1 88.7%, and an average human evaluation score of 81% on banking-domain translations. The implementation and resources are available at the accompanying GitHub repository.

</details>


### [78] [Bridging Lexical Ambiguity and Vision: A Mini Review on Visual Word Sense Disambiguation](https://arxiv.org/abs/2602.01193)
*Shashini Nilukshi,Deshan Sumanathilaka*

Main category: cs.CL

TL;DR: 本文对视觉词义消歧（VWSD）进行了小型综述，这是传统词义消歧（WSD）的多模态扩展。VWSD利用视觉线索解决视觉语言任务中的词汇歧义问题，从早期多模态融合方法发展到使用CLIP等对比模型、扩散生成和LLM支持的新框架，在2016-2025年间通过特征、图、对比嵌入等技术发展，量化结果显示CLIP微调模型和LLM增强系统比零样本基线提升6-8% MRR，但仍面临上下文限制、模型偏见、多语言数据缺乏等挑战。


<details>
  <summary>Details</summary>
Motivation: 传统词义消歧（WSD）仅依赖文本和词汇资源，而视觉词义消歧（VWSD）作为其多模态扩展，旨在利用视觉线索来解决视觉语言任务中的词汇歧义问题，特别是在文本输入有限的情况下确定歧义词的正确含义。

Method: 综述回顾了2016-2025年间VWSD的发展：从早期多模态融合方法，到基于特征、图、对比嵌入的技术演进，再到使用CLIP等对比模型、扩散式文本到图像生成和大语言模型（LLM）支持的新框架。重点关注提示工程、微调和多语言适应等方法。

Result: 量化结果显示，基于CLIP的微调模型和LLM增强的VWSD系统性能优于零样本基线，在平均倒数排名（MRR）上获得高达6-8%的提升。然而仍存在挑战：上下文限制、模型对常见含义的偏见、多语言数据集缺乏以及需要更好的评估框架。

Conclusion: VWSD领域正在快速发展，CLIP对齐、扩散生成和LLM推理的融合代表了未来方向，有望构建更强大、上下文感知和多语言的消歧系统。但仍需解决现有挑战以实现更广泛的应用。

Abstract: This paper offers a mini review of Visual Word Sense Disambiguation (VWSD), which is a multimodal extension of traditional Word Sense Disambiguation (WSD). VWSD helps tackle lexical ambiguity in vision-language tasks. While conventional WSD depends only on text and lexical resources, VWSD uses visual cues to find the right meaning of ambiguous words with minimal text input. The review looks at developments from early multimodal fusion methods to new frameworks that use contrastive models like CLIP, diffusion-based text-to-image generation, and large language model (LLM) support. Studies from 2016 to 2025 are examined to show the growth of VWSD through feature-based, graph-based, and contrastive embedding techniques. It focuses on prompt engineering, fine-tuning, and adapting to multiple languages. Quantitative results show that CLIP-based fine-tuned models and LLM-enhanced VWSD systems consistently perform better than zero-shot baselines, achieving gains of up to 6-8\% in Mean Reciprocal Rank (MRR). However, challenges still exist, such as limitations in context, model bias toward common meanings, a lack of multilingual datasets, and the need for better evaluation frameworks. The analysis highlights the growing overlap of CLIP alignment, diffusion generation, and LLM reasoning as the future path for strong, context-aware, and multilingual disambiguation systems.

</details>


### [79] [Attention Sink Forges Native MoE in Attention Layers: Sink-Aware Training to Address Head Collapse](https://arxiv.org/abs/2602.01203)
*Zizhuo Fu,Wenxuan Zeng,Runsheng Wang,Meng Li*

Main category: cs.CL

TL;DR: 该研究发现注意力机制中的sink现象自然构建了注意力层内的MoE结构，解释了head collapse问题，并提出带负载均衡损失的sink-aware训练算法来改善模型性能。


<details>
  <summary>Details</summary>
Motivation: LLMs中存在的attention sink现象（过度关注第一个token）已被多种方法（如GPT-OSS的Sink Attention和Qwen3-Next的Gated Attention）尝试解决，但这些注意力机制之间的关系缺乏全面分析。

Method: 通过理论和实证证据证明Vanilla Attention和Sink Attention中的sink自然构建了注意力层内的MoE机制；提出sink-aware训练算法，包含注意力层的辅助负载均衡损失来缓解head collapse。

Result: 实验表明该方法实现了有效的head负载均衡，并在Vanilla Attention、Sink Attention和Gated Attention上提升了模型性能。

Conclusion: 该研究为注意力机制提供了新视角，鼓励进一步探索注意力层内固有的MoE结构。

Abstract: Large Language Models (LLMs) often assign disproportionate attention to the first token, a phenomenon known as the attention sink. Several recent approaches aim to address this issue, including Sink Attention in GPT-OSS and Gated Attention in Qwen3-Next. However, a comprehensive analysis of the relationship among these attention mechanisms is lacking. In this work, we provide both theoretical and empirical evidence demonstrating that the sink in Vanilla Attention and Sink Attention naturally construct a Mixture-of-Experts (MoE) mechanism within attention layers. This insight explains the head collapse phenomenon observed in prior work, where only a fixed subset of attention heads contributes to generation. To mitigate head collapse, we propose a sink-aware training algorithm with an auxiliary load balancing loss designed for attention layers. Extensive experiments show that our method achieves effective head load balancing and improves model performance across Vanilla Attention, Sink Attention, and Gated Attention. We hope this study offers a new perspective on attention mechanisms and encourages further exploration of the inherent MoE structure within attention layers.

</details>


### [80] [ASTER: Agentic Scaling with Tool-integrated Extended Reasoning](https://arxiv.org/abs/2602.01204)
*Xuqin Zhang,Quan He,Zhenrui Zheng,Zongzhang Zhang,Xu He,Dong Li*

Main category: cs.CL

TL;DR: ASTER框架通过针对性的冷启动策略解决RL训练中的交互崩溃问题，使4B模型在数学基准测试上达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 强化学习在LLMs中引发长时程推理，但工具集成推理的RL扩展面临交互崩溃问题：模型无法维持多轮工具使用，退化为大量内部推理和琐碎的事后代码验证

Method: 提出ASTER框架，采用针对性的冷启动策略，优先考虑交互密集轨迹，仅使用4K个专家冷启动轨迹建立强健先验，支持扩展RL训练中的探索

Result: ASTER-4B在竞争性数学基准测试中达到SOTA结果，AIME 2025上达到90.0%，超越包括DeepSeek-V3.2-Exp在内的领先前沿开源模型

Conclusion: 交互密集的冷启动轨迹能有效防止RL训练中的交互崩溃，建立强健的行为先验，使小模型在数学推理任务上达到前沿性能

Abstract: Reinforcement learning (RL) has emerged as a dominant paradigm for eliciting long-horizon reasoning in Large Language Models (LLMs). However, scaling Tool-Integrated Reasoning (TIR) via RL remains challenging due to interaction collapse: a pathological state where models fail to sustain multi-turn tool usage, instead degenerating into heavy internal reasoning with only trivial, post-hoc code verification. We systematically study three questions: (i) how cold-start SFT induces an agentic, tool-using behavioral prior, (ii) how the interaction density of cold-start trajectories shapes exploration and downstream RL outcomes, and (iii) how the RL interaction budget affects learning dynamics and generalization under varying inference-time budgets. We then introduce ASTER (Agentic Scaling with Tool-integrated Extended Reasoning), a framework that circumvents this collapse through a targeted cold-start strategy prioritizing interaction-dense trajectories. We find that a small expert cold-start set of just 4K interaction-dense trajectories yields the strongest downstream performance, establishing a robust prior that enables superior exploration during extended RL training. Extensive evaluations demonstrate that ASTER-4B achieves state-of-the-art results on competitive mathematical benchmarks, reaching 90.0% on AIME 2025, surpassing leading frontier open-source models, including DeepSeek-V3.2-Exp.

</details>


### [81] [Chronos: Learning Temporal Dynamics of Reasoning Chains for Test-Time Scaling](https://arxiv.org/abs/2602.01208)
*Kai Zhang,Jiayi Liao,Chengpeng Li,Ziyuan Xie,Sihang Li,Xiang Wang*

Main category: cs.CL

TL;DR: Chronos是一个轻量级即插即用的时序推理评分器，将推理轨迹建模为时间序列，通过加权投票机制提升LLM推理性能，相比传统方法有显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有测试时缩放方法（如多数投票和启发式token级评分）平等对待推理轨迹或token，容易受到轨迹质量大幅变化和局部逻辑失败的影响，需要更精细的轨迹质量评估方法。

Method: Chronos将每个推理轨迹建模为时间序列，学习捕捉token概率的轨迹特征，据此分配质量分数，并采用加权投票机制，是一个轻量级即插即用的时序推理评分器。

Result: 在领域内和领域外基准测试中，Chronos在各种模型上均带来显著提升，计算开销可忽略。Chronos@128在HMMT25上相比Pass@1提升34.21%，相比Maj@128提升22.70%。

Conclusion: Chronos通过将推理轨迹建模为时间序列并进行加权投票，有效解决了现有测试时缩放方法的局限性，显著提升了大型语言模型的推理性能。

Abstract: Test-Time Scaling (TTS) has emerged as an effective paradigm for improving the reasoning performance of large language models (LLMs). However, existing methods -- most notably majority voting and heuristic token-level scoring -- treat reasoning traces or tokens equally, thereby being susceptible to substantial variations in trajectory quality and localized logical failures. In this work, we introduce \textbf{Chronos}, a lightweight and plug-and-play chronological reasoning scorer that models each trajectory as a time series. Specifically, Chronos learns to capture trajectory features of token probabilities, assigns quality scores accordingly, and employs a weighted voting mechanism. Extensive evaluations on both in-domain and out-of-domain benchmarks demonstrate that Chronos consistently delivers substantial gains across a variety of models, with negligible computational overhead. Notably, Chronos@128 achieves relative improvements of 34.21\% over Pass@1 and 22.70\% over Maj@128 on HMMT25 using Qwen3-4B-Thinking-2507, highlighting its effectiveness.

</details>


### [82] [Supervised Fine-Tuning Needs to Unlock the Potential of Token Priority](https://arxiv.org/abs/2602.01227)
*Zhanming Shen,Zeyu Qin,Jiaqi Hu,Wentao Ye,Hao Chen,Xiaomeng Hu,Haokai Xu,Gang Chen,Yi R. Fung,Haobo Wang*

Main category: cs.CL

TL;DR: 该立场论文提出"Token Priority"作为连接经验数据拟合与人类效用的桥梁，将SFT重新定义为精确的分布重塑过程，而非简单优化，并基于此框架将现有方法分为正优先级和符号优先级两种机制。


<details>
  <summary>Details</summary>
Motivation: 当前从经验数据拟合到实现真正人类效用的过渡受到粒度不匹配的根本限制，细粒度的自回归生成通常由粗粒度或均匀信号监督，需要建立更好的理论框架来弥合这一差距。

Method: 提出Token Priority理论框架，将监督微调(SFT)形式化为精确的分布重塑过程，将现有突破性方法统一分类为两种机制：用于噪声过滤的正优先级(Positive Priority)和用于毒性模式遗忘的符号优先级(Signed Priority)。

Result: 建立了一个统一的理论框架来分析现有进展，重新审视了现有方法的局限性和挑战，为未来研究提供了方向性指导。

Conclusion: Token Priority是连接原始数据与理想对齐流形的关键桥梁，为理解监督微调的本质提供了新的理论视角，有助于推动从经验数据拟合到真正人类效用的过渡。

Abstract: The transition from fitting empirical data to achieving true human utility is fundamentally constrained by a granularity mismatch, where fine-grained autoregressive generation is often supervised by coarse or uniform signals. This position paper advocates Token Priority as the essential bridge, formalizing Supervised Fine-Tuning (SFT) not as simple optimization but as a precise distribution reshaping process that aligns raw data with the ideal alignment manifold. We analyze recent breakthroughs through this unified lens, categorizing them into two distinct regimes: Positive Priority for noise filtration and Signed Priority for toxic modes unlearning. We revisit existing progress and limitations, identify key challenges, and suggest directions for future research.

</details>


### [83] [Inferential Question Answering](https://arxiv.org/abs/2602.01239)
*Jamshid Mozafari,Hamed Zamani,Guido Zuccon,Adam Jatowt*

Main category: cs.CL

TL;DR: 论文提出"推理问答"新任务，要求模型从仅提供线索的段落中推断答案，而非直接提取。构建了QUIT数据集，发现现有QA方法在推理任务上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有问答系统大多关注答案直接提取，但许多问题需要推理能力——从已有信息中推断出未明确陈述的答案。当前缺乏专门研究这种推理问答任务的工作。

Method: 1. 提出"推理问答"新任务；2. 构建QUIT数据集（7,401个问题，240万段落），基于人类和机器生成线索，使用LLM可回答性和人工验证标注三个相关性级别；3. 全面评估检索器、重排序器和LLM阅读器在推理任务上的表现。

Result: 1. 传统QA方法在推理任务上表现不佳：检索器性能差，重排序器提升有限，微调改进不一致；2. 面向推理的LLM未能超越较小的通用模型；3. 当前QA流程尚未准备好处理基于推理的任务。

Conclusion: 推理问答建立了一类新的QA任务，推动从间接文本证据中理解和推理的能力发展。当前方法在推理任务上存在显著不足，需要新的解决方案。

Abstract: Despite extensive research on a wide range of question answering (QA) systems, most existing work focuses on answer containment-i.e., assuming that answers can be directly extracted and/or generated from documents in the corpus. However, some questions require inference, i.e., deriving answers that are not explicitly stated but can be inferred from the available information. We introduce Inferential QA -- a new task that challenges models to infer answers from answer-supporting passages which provide only clues. To study this problem, we construct QUIT (QUestions requiring Inference from Texts) dataset, comprising 7,401 questions and 2.4M passages built from high-convergence human- and machine-authored hints, labeled across three relevance levels using LLM-based answerability and human verification. Through comprehensive evaluation of retrievers, rerankers, and LLM-based readers, we show that methods effective on traditional QA tasks struggle in inferential QA: retrievers underperform, rerankers offer limited gains, and fine-tuning provides inconsistent improvements. Even reasoning-oriented LLMs fail to outperform smaller general-purpose models. These findings reveal that current QA pipelines are not yet ready for inference-based reasoning. Inferential QA thus establishes a new class of QA tasks that move towards understanding and reasoning from indirect textual evidence.

</details>


### [84] [Minimizing Mismatch Risk: A Prototype-Based Routing Framework for Zero-shot LLM-generated Text Detection](https://arxiv.org/abs/2602.01240)
*Ke Sun,Guangsheng Bao,Han Cui,Yue Zhang*

Main category: cs.CL

TL;DR: 提出DetectRouter框架，通过从多样化代理池中为每个输入选择最佳匹配代理来改进零样本LLM生成文本检测，将检测问题转化为路由问题。


<details>
  <summary>Details</summary>
Motivation: 现有零样本检测方法对所有输入使用固定代理模型，但检测性能因代理与未知源模型的对齐程度而异。研究发现，虽然单个代理无法在所有情况下达到最优性能，但对于任何给定输入，在多样化代理池中通常存在匹配良好的代理。

Method: 提出DetectRouter原型框架，通过两阶段训练学习文本-检测器亲和度：第一阶段从白盒模型构建判别性原型；第二阶段通过将几何距离与观察到的检测分数对齐，泛化到黑盒源模型。

Result: 在EvoBench和MAGE基准测试中，DetectRouter在多个检测标准和模型家族上表现出一致的改进。

Conclusion: 将稳健检测转化为路由问题，通过为每个输入选择最合适的代理模型，可以显著提高零样本LLM生成文本检测的性能。

Abstract: Zero-shot methods detect LLM-generated text by computing statistical signatures using a surrogate model. Existing approaches typically employ a fixed surrogate for all inputs regardless of the unknown source. We systematically examine this design and find that detection performance varies substantially depending on surrogate-source alignment. We observe that while no single surrogate achieves optimal performance universally, a well-matched surrogate typically exists within a diverse pool for any given input. This finding transforms robust detection into a routing problem: selecting the most appropriate surrogate for each input. We propose DetectRouter, a prototype-based framework that learns text-detector affinity through two-stage training. The first stage constructs discriminative prototypes from white-box models; the second generalizes to black-box sources by aligning geometric distances with observed detection scores. Experiments on EvoBench and MAGE benchmarks demonstrate consistent improvements across multiple detection criteria and model families.

</details>


### [85] [Large-Scale Terminal Agentic Trajectory Generation from Dockerized Environments](https://arxiv.org/abs/2602.01244)
*Siwei Wu,Yizhi Li,Yuyang Song,Wei Zhang,Yang Wang,Riza Batista-Navarro,Xian Yang,Mingjie Tang,Bryan Dai,Jian Yang,Chenghua Lin*

Main category: cs.CL

TL;DR: TerminalTraj：一个用于生成高质量终端轨迹数据的可扩展流水线，通过Docker环境确保可执行性和可验证性，显著提升终端任务模型性能


<details>
  <summary>Details</summary>
Motivation: 训练终端任务代理模型需要高质量的终端轨迹数据，但大规模构建面临两大挑战：可执行性（需要合适的Docker环境）和可验证性（异构任务输出难以统一验证）

Method: 提出TerminalTraj流水线：1）筛选高质量仓库构建Docker化执行环境；2）生成与Docker对齐的任务实例；3）合成带有可执行验证代码的代理轨迹

Result: 构建了32K个Docker镜像和50,733个已验证终端轨迹，覆盖8个领域。基于Qwen2.5-Coder的模型在TerminalBench上性能显著提升：TB 1.0提升20%，TB 2.0提升10%。TerminalTraj-32B在100B参数以下模型中表现优异

Conclusion: TerminalTraj成功解决了终端轨迹数据构建的可执行性和可验证性挑战，为训练终端任务代理模型提供了高质量数据集，显著提升了模型性能

Abstract: Training agentic models for terminal-based tasks critically depends on high-quality terminal trajectories that capture realistic long-horizon interactions across diverse domains. However, constructing such data at scale remains challenging due to two key requirements: \textbf{\emph{Executability}}, since each instance requires a suitable and often distinct Docker environment; and \textbf{\emph{Verifiability}}, because heterogeneous task outputs preclude unified, standardized verification. To address these challenges, we propose \textbf{TerminalTraj}, a scalable pipeline that (i) filters high-quality repositories to construct Dockerized execution environments, (ii) generates Docker-aligned task instances, and (iii) synthesizes agent trajectories with executable validation code. Using TerminalTraj, we curate 32K Docker images and generate 50,733 verified terminal trajectories across eight domains. Models trained on this data with the Qwen2.5-Coder backbone achieve consistent performance improvements on TerminalBench (TB), with gains of up to 20\% on TB~1.0 and 10\% on TB~2.0 over their respective backbones. Notably, \textbf{TerminalTraj-32B} achieves strong performance among models with fewer than 100B parameters, reaching 35.30\% on TB~1.0 and 22.00\% on TB~2.0, and demonstrates improved test-time scaling behavior. All code and data are available at https://github.com/Wusiwei0410/TerminalTraj.

</details>


### [86] [PARSE: An Open-Domain Reasoning Question Answering Benchmark for Persian](https://arxiv.org/abs/2602.01246)
*Jamshid Mozafari,Seyed Parsa Mousavinasab,Adam Jatowt*

Main category: cs.CL

TL;DR: PARSE是首个波斯语开放领域推理问答基准，包含10,800个问题，涵盖布尔、多项选择和事实型格式，通过LLM生成和人工验证构建，用于评估波斯语推理能力。


<details>
  <summary>Details</summary>
Motivation: 波斯语作为拥有约1.3亿使用者的语言，缺乏高质量的开放领域推理问答基准，这阻碍了波斯语推理能力评估系统的开发与比较。

Method: 采用受控的LLM生成管道构建基准，包含多阶段过滤、标注和一致性检查以确保语言和事实质量，并通过人工评估验证。

Result: 波斯语提示和结构化提示（布尔/多项选择用思维链，事实型用少样本）能提升性能，微调进一步改善结果，特别是波斯语专用模型。

Conclusion: PARSE填补了波斯语QA研究的关键空白，为低资源环境下开发和评估推理能力强的LLM提供了坚实基础。

Abstract: Reasoning-focused Question Answering (QA) has advanced rapidly with Large Language Models (LLMs), yet high-quality benchmarks for low-resource languages remain scarce. Persian, spoken by roughly 130 million people, lacks a comprehensive open-domain resource for evaluating reasoning-capable QA systems. We introduce PARSE, the first open-domain Persian reasoning QA benchmark, containing 10,800 questions across Boolean, multiple-choice, and factoid formats, with diverse reasoning types, difficulty levels, and answer structures. The benchmark is built via a controlled LLM-based generation pipeline and validated through human evaluation. We also ensure linguistic and factual quality through multi-stage filtering, annotation, and consistency checks. We benchmark multilingual and Persian LLMs under multiple prompting strategies and show that Persian prompts and structured prompting (CoT for Boolean/multiple-choice; few-shot for factoid) improve performance. Fine-tuning further boosts results, especially for Persian-specialized models. These findings highlight how PARSE supports both fair comparison and practical model adaptation. PARSE fills a critical gap in Persian QA research and provides a strong foundation for developing and evaluating reasoning-capable LLMs in low-resource settings.

</details>


### [87] [PACER: Blockwise Pre-verification for Speculative Decoding with Adaptive Length](https://arxiv.org/abs/2602.01274)
*Situo Zhang,Yifan Zhang,Zichen Zhu,Hankun Wang,Da Ma,Danyang Zhang,Lu Chen,Kai Yu*

Main category: cs.CL

TL;DR: Pacer是一种动态控制草稿长度的推测解码方法，通过轻量级可训练预验证层实现块状预验证，相比固定草稿长度的标准推测解码能获得更高的加速比。


<details>
  <summary>Details</summary>
Motivation: 研究发现不同解码步骤的最优草稿长度差异显著，固定草稿长度限制了推测解码的进一步加速潜力，需要动态调整草稿长度的方法。

Method: 提出Pacer方法，使用轻量级可训练预验证层对草稿模型的输出进行块状预验证，如果预验证失败则停止生成更多草稿token，实现动态控制草稿长度。

Result: Pacer在多个基准测试中达到最高2.66倍的自回归解码加速，始终优于标准推测解码，与Ouroboros结合时可达3.09倍加速。

Conclusion: Pacer通过动态控制草稿长度有效提升了推测解码的效率，为大型语言模型推理加速提供了更优的解决方案。

Abstract: Speculative decoding (SD) is a powerful technique for accelerating the inference process of large language models (LLMs) without sacrificing accuracy. Typically, SD employs a small draft model to generate a fixed number of draft tokens, which are then verified in parallel by the target model. However, our experiments reveal that the optimal draft length varies significantly across different decoding steps. This variation suggests that using a fixed draft length limits the potential for further improvements in decoding speed. To address this challenge, we propose Pacer, a novel approach that dynamically controls draft length using a lightweight, trainable pre-verification layer. This layer pre-verifies draft tokens blockwise before they are sent to the target model, allowing the draft model to stop token generation if the blockwise pre-verification fails. We implement Pacer on multiple SD model pairs and evaluate its performance across various benchmarks. Our results demonstrate that Pacer achieves up to 2.66x Speedup over autoregressive decoding and consistently outperforms standard speculative decoding. Furthermore, when integrated with Ouroboros, Pacer attains up to 3.09x Speedup.

</details>


### [88] [EverMemBench: Benchmarking Long-Term Interactive Memory in Large Language ModelsEverMemBench: Benchmarking Long-Term Interactive Memory in Large Language Models](https://arxiv.org/abs/2602.01313)
*Chuanrui Hu,Tong Li,Xingze Gao,Hongda Chen,Dannong Xu,Yi Bai,Tianwei Lin,Xinda Zhao,Xiaohong Li,Jiaqi An,Yunyun Han,Jian Pei,Yafeng Deng*

Main category: cs.CL

TL;DR: EverMemBench是一个评估长期对话记忆的新基准，包含多参与者、多组对话，超过100万token，评估记忆系统在细粒度回忆、记忆意识和用户画像理解三个维度上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注二元、单主题对话，无法捕捉现实世界的复杂性，需要更全面的长期对话记忆评估工具。

Method: 构建EverMemBench基准，包含多参与者、多组对话，跨越100万+ token，具有时间演化信息、跨主题交错和角色特定人物设定。通过1000+ QA对评估三个维度：细粒度回忆、记忆意识和用户画像理解。

Result: 评估揭示了关键限制：1) 多跳推理在多参与者设置中崩溃，即使oracle模型也只达到26%；2) 时间推理仍未解决，需要超越时间戳匹配的版本语义；3) 记忆意识受检索瓶颈，当前基于相似性的方法无法弥合查询与隐式相关记忆之间的语义差距。

Conclusion: EverMemBench为开发下一代记忆架构提供了具有挑战性的测试平台，揭示了当前记忆系统在复杂现实对话场景中的严重不足。

Abstract: Long-term conversational memory is essential for LLM-based assistants, yet existing benchmarks focus on dyadic, single-topic dialogues that fail to capture real-world complexity. We introduce EverMemBench, a benchmark featuring multi-party, multi-group conversations spanning over 1 million tokens with temporally evolving information, cross-topic interleaving, and role-specific personas. EverMemBench evaluates memory systems across three dimensions through 1,000+ QA pairs: fine-grained recall, memory awareness, and user profile understanding. Our evaluation reveals critical limitations: (1) multi-hop reasoning collapses in multi-party settings, with even oracle models achieving only 26%; (2) temporal reasoning remains unsolved, requiring version semantics beyond timestamp matching; (3) memory awareness is bottlenecked by retrieval, where current similarity-based methods fail to bridge the semantic gap between queries and implicitly relevant memories. EverMemBench provides a challenging testbed for developing next-generation memory architectures.

</details>


### [89] [DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas](https://arxiv.org/abs/2602.01326)
*Zirui Wu,Lin Zheng,Zhihui Xie,Jiacheng Ye,Jiahui Gao,Shansan Gong,Yansong Feng,Zhenguo Li,Wei Bi,Guorui Zhou,Lingpeng Kong*

Main category: cs.CL

TL;DR: DreamOn是一个新颖的扩散框架，解决了扩散语言模型在代码填充任务中需要固定长度掩码序列的限制，实现了动态可变长度的生成。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型虽然提供了灵活的非自回归填充能力，但实际应用受到固定长度掩码序列要求的限制。当预定义的掩码大小与理想完成长度不匹配时，代码填充性能会严重下降。

Method: DreamOn通过向扩散过程添加两个长度控制状态，使模型能够基于自身预测自主扩展或收缩输出长度。该方法只需对现有扩散语言模型的训练目标进行最小修改，无需架构改变。

Result: 基于Dream-Coder-7B和DiffuCoder-7B构建的DreamOn在HumanEval-Infilling和SantaCoder-FIM上的填充性能与最先进的自回归模型相当，并达到了使用真实长度时的oracle性能。

Conclusion: DreamOn消除了扩散语言模型实际部署的一个基本障碍，显著提升了其在可变长度生成方面的灵活性和适用性。

Abstract: Diffusion Language Models (DLMs) present a compelling alternative to autoregressive models, offering flexible, any-order infilling without specialized prompting design. However, their practical utility is blocked by a critical limitation: the requirement of a fixed-length masked sequence for generation. This constraint severely degrades code infilling performance when the predefined mask size mismatches the ideal completion length. To address this, we propose DreamOn, a novel diffusion framework that enables dynamic, variable-length generation. DreamOn augments the diffusion process with two length control states, allowing the model to autonomously expand or contract the output length based solely on its own predictions. We integrate this mechanism into existing DLMs with minimal modifications to the training objective and no architectural changes. Built upon Dream-Coder-7B and DiffuCoder-7B, DreamOn achieves infilling performance on par with state-of-the-art autoregressive models on HumanEval-Infilling and SantaCoder-FIM and matches oracle performance achieved with ground-truth length. Our work removes a fundamental barrier to the practical deployment of DLMs, significantly advancing their flexibility and applicability for variable-length generation. Our code is available at https://github.com/DreamLM/DreamOn.

</details>


### [90] [CRAFT: Calibrated Reasoning with Answer-Faithful Traces via Reinforcement Learning for Multi-Hop Question Answering](https://arxiv.org/abs/2602.01348)
*Yu Liu,Wenxiao Zhang,Cong Cao,Fangfang Yuan,Weizhuo Chen,Cheng Hu,Pin Xu,Yuling Yang,Kun Peng,Diandian Guo,Qiang Sun,Yanbing Liu,Jin B. Hong,Zhiyuan Ma*

Main category: cs.CL

TL;DR: CRAFT是一个基于强化学习的框架，通过双重奖励机制优化多跳问答中的推理忠实性，解决推理崩溃、推理-答案不一致和格式控制丢失三大挑战。


<details>
  <summary>Details</summary>
Motivation: 多跳问答中的检索增强生成面临三大挑战：1) 推理崩溃 - 多跳组合和噪声检索导致推理不稳定；2) 推理-答案不一致 - 模型可能产生正确但推理不忠实的答案；3) 格式控制丢失 - 传统思维链生成常偏离结构化输出格式要求。

Method: 提出CRAFT框架，采用基于组相对策略优化的强化学习方法，使用双重奖励机制：确定性奖励确保结构正确性，基于评判者的奖励验证语义忠实性。支持可控的推理轨迹变体，系统分析结构和规模对推理性能的影响。

Result: 在三个多跳问答基准测试中，CRAFT提高了答案准确性和推理忠实性，CRAFT 7B模型在多个推理轨迹设置下与闭源大语言模型竞争性能相当。

Conclusion: CRAFT通过强化学习框架有效解决了多跳问答中的推理忠实性问题，双重奖励机制和可控推理轨迹设计为理解和优化大语言模型的推理过程提供了系统方法。

Abstract: Retrieval-augmented generation (RAG) is widely used to ground Large Language Models (LLMs) for multi-hop question answering. Recent work mainly focused on improving answer accuracy via fine-tuning and structured or reinforcement-based optimization. However, reliable reasoning in response generation faces three challenges: 1) Reasoning Collapse. Reasoning in multi-hop QA is inherently complex due to multi-hop composition and is further destabilized by noisy retrieval. 2) Reasoning-answer inconsistency. Due to the intrinsic uncertainty of LLM generation and exposure to evidence--distractor mixtures, models may produce correct answers that are not faithfully supported by their intermediate reasoning or evidence. 3) Loss of format control. Traditional chain-of-thought generation often deviates from required structured output formats, leading to incomplete or malformed structured content. To address these challenges, we propose CRAFT (Calibrated Reasoning with Answer-Faithful Traces), a Group Relative Policy Optimization (GRPO) based reinforcement learning framework that trains models to perform faithful reasoning during response generation. CRAFT employs dual reward mechanisms to optimize multi-hop reasoning: deterministic rewards ensure structural correctness while judge-based rewards verify semantic faithfulness. This optimization framework supports controllable trace variants that enable systematic analysis of how structure and scale affect reasoning performance and faithfulness. Experiments on three multi-hop QA benchmarks show that CRAFT improves both answer accuracy and reasoning faithfulness across model scales, with the CRAFT 7B model achieving competitive performance with closed-source LLMs across multiple reasoning trace settings.

</details>


### [91] [Balancing Understanding and Generation in Discrete Diffusion Models](https://arxiv.org/abs/2602.01362)
*Yue Liu,Yuzhong Zhao,Zheyong Xie,Qixiang Ye,Jianbin Jiao,Yao Hu,Shaosheng Cao,Yunfan Liu*

Main category: cs.CL

TL;DR: XDLM通过平稳噪声核桥接掩码扩散语言模型(MDLM)和均匀噪声扩散语言模型(UDLM)，在语义理解和生成质量之间取得平衡，超越两个范式的性能。


<details>
  <summary>Details</summary>
Motivation: 当前离散生成建模中，MDLM擅长语义理解和零样本泛化，UDLM在少步生成质量上表现优异，但两者都无法在理解和生成之间取得平衡。需要一种能统一这两个范式的方法。

Method: 提出XDLM，通过平稳噪声核桥接MDLM和UDLM，提供理论统一框架，并通过代数简化后验概率缓解内存瓶颈。将两个范式作为特例恢复。

Result: XDLM在理解和生成之间推进了帕累托前沿：零样本文本基准超越UDLM 5.4分；少步图像生成FID 54.1 vs MDLM的80.8；调优8B参数大语言模型时，32步达到15.0 MBPP，性能翻倍。

Conclusion: XDLM成功统一了MDLM和UDLM两个范式，在语义理解和生成质量之间取得平衡，训练动态分析显示其具有优越的长期扩展潜力。

Abstract: In discrete generative modeling, two dominant paradigms demonstrate divergent capabilities: Masked Diffusion Language Models (MDLM) excel at semantic understanding and zero-shot generalization, whereas Uniform-noise Diffusion Language Models (UDLM) achieve strong few-step generation quality, yet neither attains balanced performance across both dimensions. To address this, we propose XDLM, which bridges the two paradigms via a stationary noise kernel. XDLM offers two key contributions: (1) it provides a principled theoretical unification of MDLM and UDLM, recovering each paradigm as a special case; and (2) an alleviated memory bottleneck enabled by an algebraic simplification of the posterior probabilities. Experiments demonstrate that XDLM advances the Pareto frontier between understanding capability and generation quality. Quantitatively, XDLM surpasses UDLM by 5.4 points on zero-shot text benchmarks and outperforms MDLM in few-step image generation (FID 54.1 vs. 80.8). When scaled to tune an 8B-parameter large language model, XDLM achieves 15.0 MBPP in just 32 steps, effectively doubling the baseline performance. Finally, analysis of training dynamics reveals XDLM's superior potential for long-term scaling. Code is available at https://github.com/MzeroMiko/XDLM

</details>


### [92] [Rethinking Selective Knowledge Distillation](https://arxiv.org/abs/2602.01395)
*Almog Tavor,Itay Ebenspanger,Neil Cnaan,Mor Geva*

Main category: cs.CL

TL;DR: 本文提出了一种基于学生熵的位置选择知识蒸馏方法（SE-KD），通过系统分析位置、类别和样本三个维度的选择性蒸馏，实现了在保持性能的同时显著提升训练效率和降低资源消耗。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型的知识蒸馏方法虽然采用了选择性蒸馏（只对部分token位置、词汇类别或训练样本进行监督），但缺乏对重要性信号、选择策略及其相互作用的系统理解。研究者希望明确在自回归LLMs中应该在何处以及如何进行蒸馏。

Method: 首先系统分析了位置、类别和样本三个维度的选择性蒸馏，比较了不同重要性信号和选择策略。基于此分析，提出了学生熵引导的位置选择知识蒸馏（SE-KD），并进一步扩展到类别和样本维度（SE-KD 3X），实现了全面的效率优化。

Result: SE-KD在多个基准测试中通常比密集蒸馏提高了准确性、下游任务遵循性和内存效率。SE-KD 3X实现了互补的效率增益，使离线教师缓存变得可行，实际应用中减少了70%的墙上时间、18%的峰值内存和80%的存储使用，且不牺牲性能。

Conclusion: 通过系统分析选择性知识蒸馏的三个维度，提出的学生熵引导方法在保持模型性能的同时显著提升了训练效率和资源利用率，为大型语言模型的高效蒸馏提供了实用解决方案。

Abstract: Growing efforts to improve knowledge distillation (KD) in large language models (LLMs) replace dense teacher supervision with selective distillation, which uses a subset of token positions, vocabulary classes, or training samples for supervision. However, it remains unclear which importance signals, selection policies, and their interplay are most effective. In this work, we revisit where and how to distill in autoregressive LLMs. We disentangle selective KD along the position, class, and sample axes and systematically compare importance signals and selection policies. Then, guided by this analysis, we identify underexplored opportunities and introduce student-entropy-guided position selection (SE-KD). Across a suite of benchmarks, SE-KD often improves accuracy, downstream task adherence, and memory efficiency over dense distillation. Extending this approach across the class and sample axes (SE-KD 3X) yields complementary efficiency gains that make offline teacher caching feasible. In practice, this reduces wall time by 70% and peak memory by 18%, while cutting storage usage by 80% over prior methods without sacrificing performance.

</details>


### [93] [From Pragmas to Partners: A Symbiotic Evolution of Agentic High-Level Synthesis](https://arxiv.org/abs/2602.01401)
*Niansong Zhang,Sunwoo Kim,Shreesha Srinath,Zhiru Zhang*

Main category: cs.CL

TL;DR: 论文认为在AI代理时代，高级综合（HLS）仍然至关重要，可作为代理优化的自然层，并提出代理HLS共生演化的分类法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型兴起，AI驱动的硬件设计引发关注，需要探讨在代理时代HLS是否仍有价值。论文旨在论证HLS在AI代理优化硬件设计中的持续重要性。

Method: 这是一篇立场论文，通过三个主要贡献进行分析：1) 解释HLS作为实用抽象层和黄金参考的作用；2) 识别当前HLS工具的关键限制；3) 提出代理HLS共生演化的分类法。

Result: 论文论证了HLS在代理时代仍然重要，因为它提供更快的迭代周期、可移植性和设计可置换性。同时识别了当前HLS工具的性能反馈不足、接口僵化和调试能力有限等问题，这些问题AI代理特别适合解决。

Conclusion: HLS在AI代理驱动的硬件设计中保持核心地位，可作为代理优化的自然层。随着系统从协作者发展为自主设计伙伴，责任将从人类设计师转移到AI代理，形成共生演化关系。

Abstract: The rise of large language models has sparked interest in AI-driven hardware design, raising the question: does high-level synthesis (HLS) still matter in the agentic era? We argue that HLS remains essential. While we expect mature agentic hardware systems to leverage both HLS and RTL, this paper focuses on HLS and its role in enabling agentic optimization. HLS offers faster iteration cycles, portability, and design permutability that make it a natural layer for agentic optimization.This position paper makes three contributions. First, we explain why HLS serves as a practical abstraction layer and a golden reference for agentic hardware design. Second, we identify key limitations of current HLS tools, namely inadequate performance feedback, rigid interfaces, and limited debuggability that agents are uniquely positioned to address. Third, we propose a taxonomy for the symbiotic evolution of agentic HLS, clarifying how responsibility shifts from human designers to AI agents as systems advance from copilots to autonomous design partners.

</details>


### [94] [SentiFuse: Deep Multi-model Fusion Framework for Robust Sentiment Extraction](https://arxiv.org/abs/2602.01447)
*Hieu Minh Duong,Rupa Ghosh,Cong Hoan Nguyen,Eugene Levin,Todd Gary,Long Nguyen*

Main category: cs.CL

TL;DR: SentiFuse是一个灵活、模型无关的框架，通过标准化层和多种融合策略集成异构情感分析模型，在多个社交媒体数据集上优于单个模型和简单集成方法。


<details>
  <summary>Details</summary>
Motivation: 现有情感分析模型具有互补优势，但缺乏有效的统一集成框架。需要一种系统方法来整合异构模型，以提升情感分析的准确性和可靠性。

Method: 提出SentiFuse框架，包含标准化层和三种融合策略：决策级融合、特征级融合和自适应融合。框架支持模型无关的异构模型集成。

Result: 在Crowdflower、GoEmotions和Sentiment140三个大规模社交媒体数据集上的实验表明，SentiFuse始终优于单个模型和简单集成。特征级融合效果最佳，F1分数比最佳单个模型和简单平均提升高达4%。自适应融合在处理否定、混合情感和复杂情感表达等挑战性案例时增强鲁棒性。

Conclusion: 系统利用模型互补性能够实现跨不同数据集和文本类型更准确、可靠的情感分析。SentiFuse为集成异构情感模型提供了有效的统一框架。

Abstract: Sentiment analysis models exhibit complementary strengths, yet existing approaches lack a unified framework for effective integration. We present SentiFuse, a flexible and model-agnostic framework that integrates heterogeneous sentiment models through a standardization layer and multiple fusion strategies. Our approach supports decision-level fusion, feature-level fusion, and adaptive fusion, enabling systematic combination of diverse models. We conduct experiments on three large-scale social-media datasets: Crowdflower, GoEmotions, and Sentiment140. These experiments show that SentiFuse consistently outperforms individual models and naive ensembles. Feature-level fusion achieves the strongest overall effectiveness, yielding up to 4\% absolute improvement in F1 score over the best individual model and simple averaging, while adaptive fusion enhances robustness on challenging cases such as negation, mixed emotions, and complex sentiment expressions. These results demonstrate that systematically leveraging model complementarity yields more accurate and reliable sentiment analysis across diverse datasets and text types.

</details>


### [95] [Understanding QA generation: Extracting Parametric and Contextual Knowledge with CQA for Low Resource Bangla Language](https://arxiv.org/abs/2602.01451)
*Umme Abira Azmary,MD Ikramul Kayes,Swakkhar Shatabda,Farig Yousuf Sadeque*

Main category: cs.CL

TL;DR: 本文针对低资源语言（孟加拉语）的问答模型，构建了首个反事实QA数据集BanglaCQA，并提出评估框架来分析模型对参数知识与上下文知识的依赖，发现思维链提示在反事实场景中特别有效。


<details>
  <summary>Details</summary>
Motivation: 低资源语言（如孟加拉语）的问答模型面临标注数据有限和语言复杂性的挑战。现有孟加拉语QA数据集缺乏分析模型依赖参数知识还是上下文知识的结构，需要构建专门的反事实数据集来研究知识来源。

Method: 1) 构建BanglaCQA数据集，扩展现有孟加拉语数据集，集成反事实段落和可回答性标注；2) 提出微调管道用于编码器-解码器语言特定和多语言基线模型；3) 提出基于提示的管道用于仅解码器LLM；4) 应用基于LLM和人工的评估技术，基于语义相似度衡量答案质量。

Result: 1) 创建了首个孟加拉语反事实QA数据集BanglaCQA；2) 展示了模型在不同QA设置下的表现分析；3) 发现思维链提示在反事实场景中特别有效，尤其是在仅解码器LLM中提取参数知识方面。

Conclusion: 本研究不仅为分析孟加拉语QA中的知识来源引入了新框架，还揭示了反事实推理在低资源语言环境中的关键发现，为更广泛的研究方向奠定了基础。

Abstract: Question-Answering (QA) models for low-resource languages like Bangla face challenges due to limited annotated data and linguistic complexity. A key issue is determining whether models rely more on pre-encoded (parametric) knowledge or contextual input during answer generation, as existing Bangla QA datasets lack the structure required for such analysis. We introduce BanglaCQA, the first Counterfactual QA dataset in Bangla, by extending a Bangla dataset while integrating counterfactual passages and answerability annotations. In addition, we propose fine-tuned pipelines for encoder-decoder language-specific and multilingual baseline models, and prompting-based pipelines for decoder-only LLMs to disentangle parametric and contextual knowledge in both factual and counterfactual scenarios. Furthermore, we apply LLM-based and human evaluation techniques that measure answer quality based on semantic similarity. We also present a detailed analysis of how models perform across different QA settings in low-resource languages, and show that Chain-of-Thought (CoT) prompting reveals a uniquely effective mechanism for extracting parametric knowledge in counterfactual scenarios, particularly in decoder-only LLMs. Our work not only introduces a novel framework for analyzing knowledge sources in Bangla QA but also uncovers critical findings that open up broader directions for counterfactual reasoning in low-resource language settings.

</details>


### [96] [ConPress: Learning Efficient Reasoning from Multi-Question Contextual Pressure](https://arxiv.org/abs/2602.01472)
*Jie Deng,Shining Liang,Jun Li,Hongzhi Li,Yutao Xie*

Main category: cs.CL

TL;DR: 论文提出ConPress方法，通过多问题上下文压力诱导模型自我压缩推理轨迹，用少量数据微调即可显著减少推理token使用量


<details>
  <summary>Details</summary>
Motivation: 大型推理模型通过生成长链式思维轨迹解决推理任务，导致大量推理开销。研究发现当多个独立可回答的问题出现在同一提示中时，模型会自发产生更短的推理轨迹，这一现象称为自我压缩

Method: 提出ConPress方法：构建多问题提示诱导自我压缩，采样模型输出，解析过滤每个问题的轨迹以获得简洁正确的推理轨迹，然后用于监督微调，无需外部教师、手动修剪或强化学习

Result: 仅用8k微调样本，在MATH500上减少59%推理token使用，在AIME25上减少33%，同时保持竞争性准确率

Conclusion: ConPress通过利用模型自身的自我压缩现象，实现了轻量级的推理轨迹压缩，显著降低推理成本，为高效推理模型训练提供了新思路

Abstract: Large reasoning models (LRMs) typically solve reasoning-intensive tasks by generating long chain-of-thought (CoT) traces, leading to substantial inference overhead. We identify a reproducible inference-time phenomenon, termed Self-Compression: when multiple independent and answerable questions are presented within a single prompt, the model spontaneously produces shorter reasoning traces for each question. This phenomenon arises from multi-question contextual pressure during generation and consistently manifests across models and benchmarks. Building on this observation, we propose ConPress (Learning from Contextual Pressure), a lightweight self-supervised fine-tuning approach. ConPress constructs multi-question prompts to induce self-compression, samples the resulting model outputs, and parses and filters per-question traces to obtain concise yet correct reasoning trajectories. These trajectories are directly used for supervised fine-tuning, internalizing compressed reasoning behavior in single-question settings without external teachers, manual pruning, or reinforcement learning. With only 8k fine-tuning examples, ConPress reduces reasoning token usage by 59% on MATH500 and 33% on AIME25, while maintaining competitive accuracy.

</details>


### [97] [Ebisu: Benchmarking Large Language Models in Japanese Finance](https://arxiv.org/abs/2602.01479)
*Xueqing Peng,Ruoyu Xiang,Fan Zhang,Mingzi Song,Mingyang Jiang,Yan Wang,Lingfei Qian,Taiki Hara,Yuqing Guo,Jimin Huang,Junichi Tsujii,Sophia Ananiadou*

Main category: cs.CL

TL;DR: Ebisu是一个针对日语金融语言理解的基准测试，包含两个任务：JF-ICR评估投资者问答中的隐含承诺与拒绝识别，JF-TE评估专业披露中嵌套金融术语的层次提取与排序。即使最先进的LLM在这两个任务上都表现不佳。


<details>
  <summary>Details</summary>
Motivation: 日语金融语言具有粘着语、主谓宾结构、混合书写系统以及依赖间接表达和高语境沟通的特点，这对LLM构成了重大挑战。现有基准测试未能充分捕捉这些语言和文化特性。

Method: 引入Ebisu基准测试，包含两个专家标注的任务：JF-ICR（隐含承诺与拒绝识别）和JF-TE（术语层次提取）。评估了开源和专有LLM，包括通用模型、日语适应模型和金融专用模型。

Result: 即使最先进的系统在两个任务上都表现不佳。模型规模的增加仅带来有限改进，语言和领域特定适应并不能可靠提升性能，存在显著差距未解决。

Conclusion: Ebisu为推进基于语言和文化背景的金融NLP提供了聚焦基准。所有数据集和评估脚本已公开发布，有助于解决日语金融语言理解中的独特挑战。

Abstract: Japanese finance combines agglutinative, head-final linguistic structure, mixed writing systems, and high-context communication norms that rely on indirect expression and implicit commitment, posing a substantial challenge for LLMs. We introduce Ebisu, a benchmark for native Japanese financial language understanding, comprising two linguistically and culturally grounded, expert-annotated tasks: JF-ICR, which evaluates implicit commitment and refusal recognition in investor-facing Q&A, and JF-TE, which assesses hierarchical extraction and ranking of nested financial terminology from professional disclosures. We evaluate a diverse set of open-source and proprietary LLMs spanning general-purpose, Japanese-adapted, and financial models. Results show that even state-of-the-art systems struggle on both tasks. While increased model scale yields limited improvements, language- and domain-specific adaptation does not reliably improve performance, leaving substantial gaps unresolved. Ebisu provides a focused benchmark for advancing linguistically and culturally grounded financial NLP. All datasets and evaluation scripts are publicly released.

</details>


### [98] [Alternating Reinforcement Learning for Rubric-Based Reward Modeling in Non-Verifiable LLM Post-Training](https://arxiv.org/abs/2602.01511)
*Ran Xu,Tianci Liu,Zihan Dong,Tony You,Ilgee Hong,Carl Yang,Linjun Zhang,Tao Zhao,Haoyu Wang*

Main category: cs.CL

TL;DR: Rubric-ARM：通过强化学习联合优化评分标准生成器和评判器，解决传统奖励模型在非可验证领域只能预测标量分数的问题，实现更全面的响应质量评估。


<details>
  <summary>Details</summary>
Motivation: 传统奖励模型只能预测标量分数，无法捕捉非可验证领域（如创意写作、开放式指令遵循）中响应质量的多方面特性。现有方法依赖静态评分标准或分离的训练流程，存在局限性。

Method: 提出Rubric-ARM框架，通过强化学习从偏好反馈中联合优化评分标准生成器和评判器。将评分标准生成视为潜在动作，学习最大化判断准确性。引入交替优化策略缓解同时更新的非平稳性问题，并提供理论分析证明该策略能减少训练中的梯度方差。

Result: 在多个基准测试中实现了最先进的性能，显著改善了离线强化学习和在线强化学习设置中的下游策略对齐效果。

Conclusion: Rubric-ARM通过联合学习评分标准和判断，有效解决了传统奖励模型在非可验证领域的局限性，为响应质量评估提供了更全面的框架。

Abstract: Standard reward models typically predict scalar scores that fail to capture the multifaceted nature of response quality in non-verifiable domains, such as creative writing or open-ended instruction following. To address this limitation, we propose Rubric-ARM, a framework that jointly optimizes a rubric generator and a judge using reinforcement learning from preference feedback. Unlike existing methods that rely on static rubrics or disjoint training pipelines, our approach treats rubric generation as a latent action learned to maximize judgment accuracy. We introduce an alternating optimization strategy to mitigate the non-stationarity of simultaneous updates, providing theoretical analysis that demonstrates how this schedule reduces gradient variance during training. Extensive experiments show that Rubric-ARM achieves state-of-the-art performance among baselines on multiple benchmarks and significantly improves downstream policy alignment in both offline and online reinforcement learning settings.

</details>


### [99] [Argument Rarity-based Originality Assessment for AI-Assisted Writing](https://arxiv.org/abs/2602.01560)
*Keito Inoshita,Michiaki Omura,Tsukasa Yamanaka,Go Maeda,Kentaro Tsuji*

Main category: cs.CL

TL;DR: 提出基于论证稀有度的原创性评估框架AROA，将原创性定义为在参考语料库中的稀有程度，通过四个维度评估论证原创性，发现质量与原创性存在权衡关系，AI能模仿论证结构但内容原创性不足。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能轻松生成高质量文本，传统以质量为中心的写作评估正在失去意义。如果教育的核心目标是培养批判性思维和原创观点，评估范式必须从质量转向原创性。

Method: 提出论证稀有度原创性评估框架AROA，将原创性定义为在参考语料库中的稀有程度，通过四个互补组件评估：结构稀有度、主张稀有度、证据稀有度和认知深度。使用密度估计量化每个组件的稀有度，并通过质量调整机制整合，将质量和原创性视为独立评估维度。

Result: 实验发现人类论文和AI生成论文的质量与主张稀有度呈强负相关，表明存在质量-原创性权衡：更高质量的文本倾向于依赖典型的主张模式。AI论文在结构复杂性上与人类论文相当，但其主张稀有度显著低于人类，表明LLM能复制论证形式但在内容原创性上有限制。

Conclusion: AROA框架为评估学生论文的论证原创性提供了有效方法，揭示了质量与原创性的权衡关系，并证明LLM在模仿论证结构方面表现出色但在内容原创性方面存在局限，这对教育评估范式转变具有重要意义。

Abstract: As Large Language Models (LLMs) have become capable of effortlessly generating high-quality text, traditional quality-focused writing assessment is losing its significance. If the essential goal of education is to foster critical thinking and original perspectives, assessment must also shift its paradigm from quality to originality. This study proposes Argument Rarity-based Originality Assessment (AROA), a framework for automatically evaluating argumentative originality in student essays. AROA defines originality as rarity within a reference corpus and evaluates it through four complementary components: structural rarity, claim rarity, evidence rarity, and cognitive depth. The framework quantifies the rarity of each component using density estimation and integrates them with a quality adjustment mechanism, thereby treating quality and originality as independent evaluation axes. Experiments using human essays and AI-generated essays revealed a strong negative correlation between quality and claim rarity, demonstrating a quality-originality trade-off where higher-quality texts tend to rely on typical claim patterns. Furthermore, while AI essays achieved comparable levels of structural complexity to human essays, their claim rarity was substantially lower than that of humans, indicating that LLMs can reproduce the form of argumentation but have limitations in the originality of content.

</details>


### [100] [FS-Researcher: Test-Time Scaling for Long-Horizon Research Tasks with File-System-Based Agents](https://arxiv.org/abs/2602.01566)
*Chiwei Zhu,Benfeng Xu,Mingxuan Du,Shaohan Wang,Xiaorui Wang,Zhendong Mao,Yongdong Zhang*

Main category: cs.CL

TL;DR: FS-Researcher是一个基于文件系统的双智能体框架，通过持久化工作空间解决大语言模型在深度研究任务中的上下文限制问题，实现超越上下文窗口的深度研究扩展。


<details>
  <summary>Details</summary>
Motivation: 深度研究作为大语言模型智能体的代表性长视野任务，其长轨迹经常超出模型上下文限制，压缩了证据收集和报告编写的token预算，阻碍了有效的测试时扩展。

Method: 提出FS-Researcher框架，包含两个智能体：Context Builder智能体作为图书管理员浏览互联网、编写结构化笔记并将原始资料归档到可超越上下文长度的分层知识库中；Report Writer智能体则逐节编写最终报告，将知识库作为事实来源。文件系统作为持久化外部内存和跨智能体/会话的共享协调媒介。

Result: 在两个开放式基准测试（DeepResearch Bench和DeepConsult）上，FS-Researcher在不同骨干模型上实现了最先进的报告质量。分析显示最终报告质量与分配给Context Builder的计算量呈正相关，验证了文件系统范式下的有效测试时扩展。

Conclusion: FS-Researcher通过文件系统作为持久化工作空间，成功解决了深度研究中的上下文限制问题，实现了超越上下文窗口的扩展，为长视野LLM智能体任务提供了有效解决方案。

Abstract: Deep research is emerging as a representative long-horizon task for large language model (LLM) agents. However, long trajectories in deep research often exceed model context limits, compressing token budgets for both evidence collection and report writing, and preventing effective test-time scaling. We introduce FS-Researcher, a file-system-based, dual-agent framework that scales deep research beyond the context window via a persistent workspace. Specifically, a Context Builder agent acts as a librarian which browses the internet, writes structured notes, and archives raw sources into a hierarchical knowledge base that can grow far beyond context length. A Report Writer agent then composes the final report section by section, treating the knowledge base as the source of facts. In this framework, the file system serves as a durable external memory and a shared coordination medium across agents and sessions, enabling iterative refinement beyond the context window. Experiments on two open-ended benchmarks (DeepResearch Bench and DeepConsult) show that FS-Researcher achieves state-of-the-art report quality across different backbone models. Further analyses demonstrate a positive correlation between final report quality and the computation allocated to the Context Builder, validating effective test-time scaling under the file-system paradigm. The code and data are anonymously open-sourced at https://github.com/Ignoramus0817/FS-Researcher.

</details>


### [101] [LLM-based Embeddings: Attention Values Encode Sentence Semantics Better Than Hidden States](https://arxiv.org/abs/2602.01572)
*Yeqin Zhang,Yunfei Wang,Jiaxuan Chen,Ke Qin,Yizheng Zhao,Cam-Tu Nguyen*

Main category: cs.CL

TL;DR: 本文提出Value Aggregation方法，通过聚合注意力值向量而非隐藏状态来获得更好的句子表示，在无需训练的情况下超越其他LLM嵌入方法，甚至媲美集成方法MetaEOL。


<details>
  <summary>Details</summary>
Motivation: 现有方法大多依赖LLM的最后一层隐藏状态来获取句子表示，但这些隐藏状态是为下一个词预测优化的，往往无法有效捕捉全局的句子级语义。

Method: 提出Value Aggregation方法，聚合多个层和token索引的注意力值向量。进一步提出Aligned Weighted VA，利用最后一个token的注意力分数作为权重，通过输出投影矩阵将加权值向量对齐到LLM残差流的公共空间。

Result: 在无需训练的情况下，VA方法超越其他LLM嵌入方法，甚至匹配或超越集成方法MetaEOL。AlignedWVA在无需训练的LLM嵌入方法中达到SOTA性能，大幅超越高成本的MetaEOL。

Conclusion: 注意力值向量比隐藏状态能更好地捕捉句子语义，Value Aggregation方法为获取高质量句子表示提供了新视角，通过微调VA还有潜力获得更强的LLM嵌入模型。

Abstract: Sentence representations are foundational to many Natural Language Processing (NLP) applications. While recent methods leverage Large Language Models (LLMs) to derive sentence representations, most rely on final-layer hidden states, which are optimized for next-token prediction and thus often fail to capture global, sentence-level semantics. This paper introduces a novel perspective, demonstrating that attention value vectors capture sentence semantics more effectively than hidden states. We propose Value Aggregation (VA), a simple method that pools token values across multiple layers and token indices. In a training-free setting, VA outperforms other LLM-based embeddings, even matches or surpasses the ensemble-based MetaEOL. Furthermore, we demonstrate that when paired with suitable prompts, the layer attention outputs can be interpreted as aligned weighted value vectors. Specifically, the attention scores of the last token function as the weights, while the output projection matrix ($W_O$) aligns these weighted value vectors with the common space of the LLM residual stream. This refined method, termed Aligned Weighted VA (AlignedWVA), achieves state-of-the-art performance among training-free LLM-based embeddings, outperforming the high-cost MetaEOL by a substantial margin. Finally, we highlight the potential of obtaining strong LLM embedding models through fine-tuning Value Aggregation.

</details>


### [102] [Provable Defense Framework for LLM Jailbreaks via Noise-Augumented Alignment](https://arxiv.org/abs/2602.01587)
*Zehua Cheng,Jianwei Yang,Wei Dai,Jiahao Sun*

Main category: cs.CL

TL;DR: 提出Certified Semantic Smoothing (CSS)框架，通过分层随机消融和噪声增强对齐调优，为LLM提供可证明的鲁棒性保证，显著降低攻击成功率同时保持高良性效用。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型容易受到自适应越狱攻击，现有防御方法（如GCG）缺乏理论保证。需要一种既能提供可证明安全保证，又不显著降低模型效用的鲁棒性框架。

Method: 1. Certified Semantic Smoothing (CSS)：通过分层随机消融技术，将输入分为不可变结构提示和可变载荷，利用超几何分布推导严格的l0范数保证。2. Noise-Augmented Alignment Tuning (NAAT)：将基础模型转化为语义去噪器，解决稀疏上下文下的性能退化问题。

Result: 在Llama-3上的实验显示：梯度攻击的攻击成功率从84.2%降至1.2%，良性效用保持在94.1%。显著优于字符级基线方法（效用降至74.3%）。

Conclusion: 该框架提供了确定性的安全证书，确保模型在可证明半径内对所有对抗变体保持鲁棒，为LLM安全提供了理论保证与实践有效的解决方案。

Abstract: Large Language Models (LLMs) remain vulnerable to adaptive jailbreaks that easily bypass empirical defenses like GCG. We propose a framework for certifiable robustness that shifts safety guarantees from single-pass inference to the statistical stability of an ensemble. We introduce Certified Semantic Smoothing (CSS) via Stratified Randomized Ablation, a technique that partitions inputs into immutable structural prompts and mutable payloads to derive rigorous lo norm guarantees using the Hypergeometric distribution. To resolve performance degradation on sparse contexts, we employ Noise-Augmented Alignment Tuning (NAAT), which transforms the base model into a semantic denoiser. Extensive experiments on Llama-3 show that our method reduces the Attack Success Rate of gradient-based attacks from 84.2% to 1.2% while maintaining 94.1% benign utility, significantly outperforming character-level baselines which degrade utility to 74.3%. This framework provides a deterministic certificate of safety, ensuring that a model remains robust against all adversarial variants within a provable radius.

</details>


### [103] [Wiki Live Challenge: Challenging Deep Research Agents with Expert-Level Wikipedia Articles](https://arxiv.org/abs/2602.01590)
*Shaohan Wang,Benfeng Xu,Licheng Zhang,Mingxuan Du,Chiwei Zhu,Xiaorui Wang,Zhendong Mao,Yongdong Zhang*

Main category: cs.CL

TL;DR: 论文提出了Wiki Live Challenge (WLC)基准，利用最新的维基百科优质文章作为专家级参考，评估深度研究代理的性能，并开发了包含39个标准的细粒度评估框架。


<details>
  <summary>Details</summary>
Motivation: 当前深度研究代理(DRAs)的评估框架主要依赖LLM生成的参考或LLM衍生的评估维度，这些方法虽然可扩展，但缺乏专家验证内容的可靠性，难以提供客观、细粒度的关键维度评估。

Method: 1) 引入Wiki Live Challenge (WLC)基准，利用最新的维基百科优质文章作为专家级参考；2) 构建包含100篇近期优质文章的数据集；3) 提出Wiki Eval评估框架，包含39个写作质量标准和严格的事实可验证性指标。

Result: 对各种DRA系统的广泛实验表明，当前DRAs与人类专家级维基百科文章之间存在显著差距，验证了WLC在推进代理研究方面的有效性。

Conclusion: WLC基准通过利用维基百科严格的中立性、全面性和可验证性标准，为深度研究代理提供了可靠的评估框架，填补了当前评估方法的不足。

Abstract: Deep Research Agents (DRAs) have demonstrated remarkable capabilities in autonomous information retrieval and report generation, showing great potential to assist humans in complex research tasks. Current evaluation frameworks primarily rely on LLM-generated references or LLM-derived evaluation dimensions. While these approaches offer scalability, they often lack the reliability of expert-verified content and struggle to provide objective, fine-grained assessments of critical dimensions. To bridge this gap, we introduce Wiki Live Challenge (WLC), a live benchmark that leverages the newest Wikipedia Good Articles (GAs) as expert-level references. Wikipedia's strict standards for neutrality, comprehensiveness, and verifiability serve as a great challenge for DRAs, with GAs representing the pinnacle of which. We curate a dataset of 100 recent Good Articles and propose Wiki Eval, a comprehensive evaluation framework comprising a fine-grained evaluation method with 39 criteria for writing quality and rigorous metrics for factual verifiability. Extensive experiments on various DRA systems demonstrate a significant gap between current DRAs and human expert-level Wikipedia articles, validating the effectiveness of WLC in advancing agent research. We release our benchmark at https://github.com/WangShao2000/Wiki_Live_Challenge

</details>


### [104] [The Art of Socratic Inquiry: A Framework for Proactive Template-Guided Therapeutic Conversation Generation](https://arxiv.org/abs/2602.01598)
*Mingwen Zhang,Minqiang Yang,Changsheng Ma,Yang Yu,Hui Bai,Chen Xu,Xiangzhen Kong,Bin Hu*

Main category: cs.CL

TL;DR: 本文提出Socratic Inquiry Framework (SIF)，一个轻量级、即插即用的治疗意图规划器，将LLM从被动倾听者转变为主动认知引导者，通过策略锚定和模板检索实现主动提问，显著提升治疗对话的深度和效果。


<details>
  <summary>Details</summary>
Motivation: 当前心理治疗领域的LLM主要处于被动反应模式，只能提供共情但肤浅的回应，无法主动引导认知改变或揭示潜在信念，这与CBT治疗中主动提问的核心技术形成差距。

Method: 提出Socratic Inquiry Framework (SIF)，将"何时提问"（策略锚定）与"提问什么"（模板检索）解耦，无需端到端重新训练即可实现上下文感知、理论基础的提问。同时创建Socratic-QA数据集，提供策略对齐的苏格拉底式对话序列作为监督数据。

Result: 实验表明SIF显著提高了主动提问频率、对话深度和治疗对齐度，实现了从被动安慰到主动探索的明显转变。

Conclusion: 该研究为心理治疗LLM建立了新范式：不仅回应，更要引导。SIF框架为构建具有主动认知引导能力的心理治疗AI系统提供了有效解决方案。

Abstract: Proactive questioning, where therapists deliberately initiate structured, cognition-guiding inquiries, is a cornerstone of cognitive behavioral therapy (CBT). Yet, current psychological large language models (LLMs) remain overwhelmingly reactive, defaulting to empathetic but superficial responses that fail to surface latent beliefs or guide behavioral change. To bridge this gap, we propose the \textbf{Socratic Inquiry Framework (SIF)}, a lightweight, plug-and-play therapeutic intent planner that transforms LLMs from passive listeners into active cognitive guides. SIF decouples \textbf{when to ask} (via Strategy Anchoring) from \textbf{what to ask} (via Template Retrieval), enabling context-aware, theory-grounded questioning without end-to-end retraining. Complementing SIF, we introduce \textbf{Socratic-QA}, a high-quality dataset of strategy-aligned Socratic sequences that provides explicit supervision for proactive reasoning. Experiments show that SIF significantly enhances proactive questioning frequency, conversational depth, and therapeutic alignment, marking a clear shift from reactive comfort to proactive exploration. Our work establishes a new paradigm for psychologically informed LLMs: not just to respond, but to guide.

</details>


### [105] [SEA-Guard: Culturally Grounded Multilingual Safeguard for Southeast Asia](https://arxiv.org/abs/2602.01618)
*Panuthep Tasawong,Jian Gang Ngui,Alham Fikri Aji,Trevor Cohn,Peerat Limkonchotiwat*

Main category: cs.CL

TL;DR: 提出了一个代理驱动的数据生成框架，用于创建东南亚地区特定的安全数据集，并基于此构建了首个基于东南亚文化背景的多语言安全防护模型SEA-Guard家族。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的AI对齐需要文化感知的安全防护，因为安全不仅涉及常识，还包括多样化的本地价值观、规范和地区特定法规。然而，构建大规模、文化基础的数据集面临资源有限和本地标注者稀缺的挑战。现有模型通常依赖英语数据集的机器翻译，缺乏区域和文化细微差别。

Method: 提出了一个新颖的代理数据生成框架，可扩展地创建真实、地区特定的东南亚安全数据集。基于此数据集构建了SEA-Guard家族模型，这是首个基于东南亚文化背景的多语言安全防护模型。

Result: 在多个基准测试和文化变体评估中，SEA-Guard在检测地区敏感或有害内容方面始终优于现有安全防护模型，同时保持强大的通用安全性能。

Conclusion: 该研究成功开发了针对东南亚地区的文化感知AI安全防护解决方案，通过代理数据生成框架解决了文化数据集构建的挑战，SEA-Guard模型在区域敏感内容检测方面表现出色。

Abstract: Culturally aware safeguards are crucial for AI alignment in real-world settings, where safety extends beyond common sense and encompasses diverse local values, norms, and region-specific regulations. However, building large-scale, culturally grounded datasets is challenging due to limited resources and a scarcity of native annotators. Consequently, many safeguard models rely on machine translation of English datasets, often missing regional and cultural nuances. We present a novel agentic data-generation framework to scalably create authentic, region-specific safety datasets for Southeast Asia (SEA). On this foundation, we introduce the SEA-Guard family, the first multilingual safeguard models grounded in SEA cultural contexts. Evaluated across multiple benchmarks and cultural variants, SEA-Guard consistently outperforms existing safeguards at detecting regionally sensitive or harmful content while maintaining strong general safety performance.

</details>


### [106] [A2Eval: Agentic and Automated Evaluation for Embodied Brain](https://arxiv.org/abs/2602.01640)
*Shuai Zhang,Jiayu Hu,Zijie Chen,Zeyuan Ding,Yi Zhang,Yingji Zhang,Ziyi Zhou,Junwei Liao,Shengjie Zhou,Yong Dai,Zhenzhong Lan,Xiaozhu Ju*

Main category: cs.CL

TL;DR: A2Eval是一个自动化的具身VLM评估框架，通过两个协作代理自动生成平衡的评估套件和执行评估流程，显著减少计算成本和评估时间，同时保持评估质量。


<details>
  <summary>Details</summary>
Motivation: 当前具身VLM评估依赖静态、专家定义、手动标注的基准测试，存在严重冗余和覆盖不平衡问题。这种劳动密集型范式消耗大量计算和标注资源，增加成本，扭曲模型排名，阻碍迭代开发。

Method: 提出Agentic Automatic Evaluation (A2Eval)框架，包含两个协作代理：Data Agent自动归纳能力维度并组装平衡、紧凑的评估套件；Eval Agent合成并验证可执行的评估流程，实现完全自主的高保真评估。

Result: 在10个基准测试和13个模型上的评估显示：A2Eval将评估套件压缩85%，总体计算成本减少77%，速度提升4.6倍，同时保持评估质量。纠正系统性排名偏差，人类对齐度达到Spearman's rho=0.85，排名保真度Kendall's tau=0.81。

Conclusion: A2Eval为高保真、低成本的具身评估建立了新标准，通过自动化解决了当前评估范式的关键问题，显著提高了评估效率和准确性。

Abstract: Current embodied VLM evaluation relies on static, expert-defined, manually annotated benchmarks that exhibit severe redundancy and coverage imbalance. This labor intensive paradigm drains computational and annotation resources, inflates costs, and distorts model rankings, ultimately stifling iterative development. To address this, we propose Agentic Automatic Evaluation (A2Eval), the first agentic framework that automates benchmark curation and evaluation through two collaborative agents. The Data Agent autonomously induces capability dimensions and assembles a balanced, compact evaluation suite, while the Eval Agent synthesizes and validates executable evaluation pipelines, enabling fully autonomous, high-fidelity assessment. Evaluated across 10 benchmarks and 13 models, A2Eval compresses evaluation suites by 85%, reduces overall computational costs by 77%, and delivers a 4.6x speedup while preserving evaluation quality. Crucially, A2Eval corrects systematic ranking biases, improves human alignment to Spearman's rho=0.85, and maintains high ranking fidelity (Kendall's tau=0.81), establishing a new standard for high-fidelity, low-cost embodied assessment. Our code and data will be public soon.

</details>


### [107] [Steering Vector Fields for Context-Aware Inference-Time Control in Large Language Models](https://arxiv.org/abs/2602.01654)
*Jiaqian Li,Yanshu Li,Kuan-Hao Huang*

Main category: cs.CL

TL;DR: SVF提出了一种基于向量场的上下文相关转向方法，解决了传统静态转向向量在不同上下文中效果不一致的问题，实现了更可靠的长文本和多属性控制。


<details>
  <summary>Details</summary>
Motivation: 传统转向向量在实际应用中存在可靠性问题：某些概念无法被有效转向，即使平均效果良好，但在部分输入上可能产生反效果；在长文本生成和多属性转向中可靠性会下降。这些问题源于静态转向向量假设概念改进方向在所有上下文中都是恒定的。

Method: 提出转向向量场（SVF），学习一个可微分的概念评分函数，其局部梯度定义了每个激活点的转向方向，使干预明确依赖于上下文。该框架支持在共享对齐概念空间中进行协调的多层干预，并在统一框架内实现高效的长文本和多属性控制。

Result: 在多个大语言模型和转向任务中，SVF提供了更强且更可靠的控制，提高了推理时转向的实用性。

Conclusion: 通过几何视角分析转向失败的原因，并提出基于向量场的上下文相关转向方法，显著改善了转向的可靠性和效果，为大语言模型的推理时控制提供了更实用的解决方案。

Abstract: Steering vectors (SVs) offer a lightweight way to control large language models (LLMs) at inference time by shifting hidden activations, providing a practical middle ground between prompting and fine-tuning. Yet SVs can be unreliable in practice. Some concepts are unsteerable, and even when steering helps on average it can backfire for a non-trivial fraction of inputs. Reliability also degrades in long-form generation and multi-attribute steering. We take a geometric view of these failures. A static SV applies the same update vector everywhere in representation space, implicitly assuming that the concept-improving direction is constant across contexts. When the locally effective direction varies with the current activation, a single global vector can become misaligned, which yields weak or reversed effects. Guided by this perspective, we propose Steering Vector Fields (SVF), which learns a differentiable concept scoring function whose local gradient defines the steering direction at each activation, making interventions explicitly context-dependent. This formulation supports coordinated multi-layer interventions in a shared, aligned concept space, and enables efficient long-form and multi-attribute control within a unified framework. Across multiple LLMs and steering tasks, SVF delivers stronger and more reliable control, improving the practicality of inference-time steering.

</details>


### [108] [CoDiQ: Test-Time Scaling for Controllable Difficult Question Generation](https://arxiv.org/abs/2602.01660)
*Zhongyuan Peng,Caijun Xu,Changyi Xiao,Shibo Hong,Eli Zhang,Stephen Huang,Yixin Cao*

Main category: cs.CL

TL;DR: CoDiQ框架通过测试时缩放实现细粒度难度控制，生成竞赛级难题，构建44K高质量问题语料库，显著提升大推理模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有自动问题生成方法缺乏精确难度控制，计算成本高，难以大规模生成竞赛级难题，需要一种可控难度的问题生成框架来提升大推理模型的训练效果。

Method: 提出CoDiQ框架：1) 发现测试时缩放趋势（扩展推理token预算增加难度但降低可解性）；2) 开发CoDiQ-Generator提升难题生成上限；3) 构建CoDiQ-Corpus包含44K竞赛级问题序列。

Result: 人类评估显示CoDiQ生成的问题比LiveCodeBench/AIME显著更难，同时保持82%以上的可解性。在CoDiQ-Corpus上训练的大推理模型推理性能大幅提升。

Conclusion: CoDiQ框架通过可控难度的问题生成有效提升大推理模型的推理能力，开源了语料库、生成器和实现代码以支持相关研究。

Abstract: Large Reasoning Models (LRMs) benefit substantially from training on challenging competition-level questions. However, existing automated question synthesis methods lack precise difficulty control, incur high computational costs, and struggle to generate competition-level questions at scale. In this paper, we propose CoDiQ (Controllable Difficult Question Generation), a novel framework enabling fine-grained difficulty control via test-time scaling while ensuring question solvability. Specifically, first, we identify a test-time scaling tendency (extended reasoning token budget boosts difficulty but reduces solvability) and the intrinsic properties defining the upper bound of a model's ability to generate valid, high-difficulty questions. Then, we develop CoDiQ-Generator from Qwen3-8B, which improves the upper bound of difficult question generation, making it particularly well-suited for challenging question construction. Building on the CoDiQ framework, we build CoDiQ-Corpus (44K competition-grade question sequences). Human evaluations show these questions are significantly more challenging than LiveCodeBench/AIME with over 82% solvability. Training LRMs on CoDiQ-Corpus substantially improves reasoning performance, verifying that scaling controlled-difficulty training questions enhances reasoning capabilities. We open-source CoDiQ-Corpus, CoDiQ-Generator, and implementations to support related research.

</details>


### [109] [Scaling Search-Augmented LLM Reasoning via Adaptive Information Control](https://arxiv.org/abs/2602.01672)
*Siheng Xiong,Oguzhan Gungordu,Blair Johnson,James C. Kerce,Faramarz Fekri*

Main category: cs.CL

TL;DR: DeepControl：基于信息效用的自适应检索控制框架，通过检索延续和粒度控制机制优化搜索增强推理代理的信息获取


<details>
  <summary>Details</summary>
Motivation: 现有搜索增强推理代理在检索过程中存在冗余证据、上下文饱和和学习不稳定问题，基于结果的强化学习方法对信息获取的指导有限

Method: 提出信息效用的形式化概念，衡量给定推理状态下检索证据的边际价值；引入检索延续和粒度控制机制，选择性调节何时继续/停止检索以及扩展多少信息；采用退火控制策略使代理在训练中内化有效的信息获取行为

Result: 在七个基准测试中一致优于强基线，相比基于结果的强化学习基线，在Qwen2.5-7B和Qwen2.5-3B上分别实现9.4%和8.6%的平均性能提升，且优于无检索和基于检索的无显式控制方法

Conclusion: 自适应信息控制对于将搜索增强推理代理扩展到复杂、真实世界信息环境至关重要，DeepControl框架通过形式化的信息效用和自适应控制机制有效解决了现有方法的局限性

Abstract: Search-augmented reasoning agents interleave multi-step reasoning with external information retrieval, but uncontrolled retrieval often leads to redundant evidence, context saturation, and unstable learning. Existing approaches rely on outcome-based reinforcement learning (RL), which provides limited guidance for regulating information acquisition. We propose DeepControl, a framework for adaptive information control based on a formal notion of information utility, which measures the marginal value of retrieved evidence under a given reasoning state. Building on this utility, we introduce retrieval continuation and granularity control mechanisms that selectively regulate when to continue and stop retrieval, and how much information to expand. An annealed control strategy enables the agent to internalize effective information acquisition behaviors during training. Extensive experiments across seven benchmarks demonstrate that our method consistently outperforms strong baselines. In particular, our approach achieves average performance improvements of 9.4% and 8.6% on Qwen2.5-7B and Qwen2.5-3B, respectively, over strong outcome-based RL baselines, and consistently outperforms both retrieval-free and retrieval-based reasoning methods without explicit information control. These results highlight the importance of adaptive information control for scaling search-augmented reasoning agents to complex, real-world information environments.

</details>


### [110] [Counting Hypothesis: Potential Mechanism of In-Context Learning](https://arxiv.org/abs/2602.01687)
*Jung H. Lee,Sujith Vijayan*

Main category: cs.CL

TL;DR: 论文提出"计数假设"来解释大语言模型的上下文学习机制，认为LLMs的编码策略是ICL的基础


<details>
  <summary>Details</summary>
Motivation: 上下文学习（ICL）使大语言模型能够通过输入提示中的示例学习特定任务，但对其底层机制理解不足，导致错误修正和诊断困难，因此需要更好地理解ICL的局限性及其工作原理

Method: 受ICL特性和LLMs功能模块启发，提出"计数假设"，认为LLMs的编码策略可能是ICL的基础，并提供支持证据

Result: 提出了解释ICL机制的"计数假设"，为理解LLMs如何支持上下文学习提供了理论框架

Conclusion: 通过提出"计数假设"，为理解大语言模型上下文学习的底层机制提供了新视角，有助于改进ICL的错误修正和诊断能力

Abstract: In-Context Learning (ICL) indicates that large language models (LLMs) pretrained on a massive amount of data can learn specific tasks from input prompts' examples. ICL is notable for two reasons. First, it does not need modification of LLMs' internal structure. Second, it enables LLMs to perform a wide range of tasks/functions with a few examples demonstrating a desirable task. ICL opens up new ways to utilize LLMs in more domains, but its underlying mechanisms still remain poorly understood, making error correction and diagnosis extremely challenging. Thus, it is imperative that we better understand the limitations of ICL and how exactly LLMs support ICL. Inspired by ICL properties and LLMs' functional modules, we propose 1the counting hypothesis' of ICL, which suggests that LLMs' encoding strategy may underlie ICL, and provide supporting evidence.

</details>


### [111] [Restoring Exploration after Post-Training: Latent Exploration Decoding for Large Reasoning Models](https://arxiv.org/abs/2602.01698)
*Wenhui Tan,Fiorenzo Parascandolo,Enver Sangineto,Jianzhong Ju,Zhenbo Luo,Qian Cao,Rita Cucchiara,Ruihua Song,Jian Luan*

Main category: cs.CL

TL;DR: 论文提出Latent Exploration Decoding (LED)方法，通过利用中间层的高熵来解决后训练推理模型中探索崩溃的问题，无需额外训练即可提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 现代推理后训练会导致意外的探索崩溃：基于温度的采样不再提高pass@n准确率。后训练的大型推理模型最终层后验分布的熵急剧减少，而中间层的熵保持相对较高。

Method: 提出潜在探索解码(LED)，一种深度条件解码策略。LED通过累积和聚合中间层后验分布，并选择具有最大熵的深度配置作为探索候选，无需额外训练或参数。

Result: LED在多个推理基准测试和模型上，一致地将pass@1和pass@16准确率分别提高0.61和1.03个百分点。

Conclusion: LED通过利用中间层的高熵有效解决了后训练推理模型中的探索崩溃问题，提供了一种简单有效的解码策略来提升推理性能。

Abstract: Large Reasoning Models (LRMs) have recently achieved strong mathematical and code reasoning performance through Reinforcement Learning (RL) post-training. However, we show that modern reasoning post-training induces an unintended exploration collapse: temperature-based sampling no longer increases pass@$n$ accuracy. Empirically, the final-layer posterior of post-trained LRMs exhibit sharply reduced entropy, while the entropy of intermediate layers remains relatively high. Motivated by this entropy asymmetry, we propose Latent Exploration Decoding (LED), a depth-conditioned decoding strategy. LED aggregates intermediate posteriors via cumulative sum and selects depth configurations with maximal entropy as exploration candidates. Without additional training or parameters, LED consistently improves pass@1 and pass@16 accuracy by 0.61 and 1.03 percentage points across multiple reasoning benchmarks and models. Project page: https://GitHub.com/Xiaomi-Research/LED.

</details>


### [112] [Game of Thought: Robust Information Seeking with Large Language Models Using Game Theory](https://arxiv.org/abs/2602.01708)
*Langyuan Cui,Chun Kai Ling,Hwee Tou Ng*

Main category: cs.CL

TL;DR: 论文提出Game of Thought (GoT)框架，使用博弈论方法提升LLM在信息不足情况下的主动信息搜寻能力，通过二十问游戏评估，在对抗性设置中显著改善最坏情况性能。


<details>
  <summary>Details</summary>
Motivation: LLM在现实场景中常面临信息不足的问题，现有提升信息搜寻能力的方法依赖简化假设，会降低最坏情况性能，这在高风险应用中存在严重隐患。

Method: 引入Strategic Language Search (SLS)问题作为二十问游戏的对抗性变体，将其形式化为两人零和扩展式博弈。提出Game of Thought (GoT)框架，应用博弈论技术近似求解受限变体游戏的纳什均衡策略。

Result: 实证结果表明，与直接提示方法和启发式引导搜索方法相比，GoT在所有测试设置中都能持续改善最坏情况性能。

Conclusion: GoT框架通过博弈论方法有效提升了LLM在信息不足场景下的信息搜寻能力，特别是在对抗性设置中的最坏情况性能，为高风险应用提供了更可靠的解决方案。

Abstract: Large Language Models (LLMs) are increasingly deployed in real-world scenarios where they may lack sufficient information to complete a given task. In such settings, the ability to actively seek out missing information becomes a critical capability. Existing approaches to enhancing this ability often rely on simplifying assumptions that degrade \textit{worst-case} performance. This is an issue with serious implications in high-stakes applications. In this work, we use the game of Twenty Questions to evaluate the information-seeking ability of LLMs. We introduce and formalize its adversarial counterpart, the Strategic Language Search (SLS) problem along with its variants as a two-player zero-sum extensive form game. We propose Game of Thought (GoT), a framework that applies game-theoretic techniques to approximate a Nash equilibrium (NE) strategy for the restricted variant of the game. Empirical results demonstrate that our approach consistently improves worst-case performance compared to (1) direct prompting-based methods and (2) heuristic-guided search methods across all tested settings.

</details>


### [113] [ARTIS: Agentic Risk-Aware Test-Time Scaling via Iterative Simulation](https://arxiv.org/abs/2602.01709)
*Xingshan Zeng,Lingzhi Wang,Weiwen Liu,Liangyou Li,Yasheng Wang,Lifeng Shang,Xin Jiang,Qun Liu*

Main category: cs.CL

TL;DR: ARTIS框架通过迭代模拟实现代理的风险感知测试时扩展，在真实环境执行前进行模拟探索，提高代理决策的可靠性，避免环境风险。


<details>
  <summary>Details</summary>
Motivation: 当前测试时扩展技术虽然能提升LLM性能，但在代理场景中不足，因为代理动作直接与环境交互且效果不可逆、成本高。需要一种方法在提高动作级可靠性的同时避免环境风险。

Method: 提出ARTIS框架，将探索与执行解耦，通过模拟交互进行测试时探索。引入风险感知工具模拟器，通过针对性数据生成和再平衡训练，重点捕捉罕见但高影响故障模式。

Result: 在多轮多步代理基准测试中，迭代模拟显著提升代理可靠性，风险感知模拟对于在不同模型和任务中持续实现这些收益至关重要。

Conclusion: ARTIS框架通过风险感知的迭代模拟，有效解决了代理场景中测试时扩展的局限性，在提高决策可靠性的同时避免了环境风险，为代理系统提供了更安全的决策机制。

Abstract: Current test-time scaling (TTS) techniques enhance large language model (LLM) performance by allocating additional computation at inference time, yet they remain insufficient for agentic settings, where actions directly interact with external environments and their effects can be irreversible and costly. We propose \emph{\name}, \emph{\underline{A}gentic \underline{R}isk-Aware \underline{T}est-Time Scaling via \underline{I}terative \underline{S}imulation}, a framework that decouples exploration from commitment by enabling test-time exploration through simulated interactions prior to real-world execution. This design allows extending inference-time computation to improve action-level reliability and robustness without incurring environmental risk. We further show that naive LLM-based simulators struggle to capture rare but high-impact failure modes, substantially limiting their effectiveness for agentic decision making. To address this limitation, we introduce a \emph{risk-aware tool simulator} that emphasizes fidelity on failure-inducing actions via targeted data generation and rebalanced training. Experiments on multi-turn and multi-step agentic benchmarks demonstrate that iterative simulation substantially improves agent reliability, and that risk-aware simulation is essential for consistently realizing these gains across models and tasks.

</details>


### [114] [MedAraBench: Large-Scale Arabic Medical Question Answering Dataset and Benchmark](https://arxiv.org/abs/2602.01714)
*Mouath Abu-Daoud,Leen Kharouf,Omar El Hajj,Dana El Samad,Mariam Al-Omari,Jihad Mallat,Khaled Saleh,Nizar Habash,Farah E. Shamout*

Main category: cs.CL

TL;DR: 提出了MedAraBench，一个大规模阿拉伯语医学多选题数据集，涵盖19个专科和5个难度级别，用于评估LLMs在阿拉伯语医学领域的性能。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语在NLP研究中代表性不足，特别是在医学应用领域，缺乏开源数据和基准测试，限制了LLMs多语言能力的评估和发展。

Method: 通过手动数字化阿拉伯语地区医学专业人士创建的学术材料构建数据集，进行广泛预处理并划分为训练和测试集，采用专家人工评估和LLM-as-a-judge两种框架评估数据质量。

Result: 数据集具有高质量和多样性，涵盖19个专科和5个难度级别。评估了8个最先进的开源和专有模型（如GPT-5、Gemini 2.0 Flash、Claude 4-Sonnet），结果显示需要进一步的领域特定增强。

Conclusion: 发布数据集和评估脚本，以扩大医学数据基准的多样性，扩展LLMs评估套件的范围，并增强模型在临床环境中的多语言能力。

Abstract: Arabic remains one of the most underrepresented languages in natural language processing research, particularly in medical applications, due to the limited availability of open-source data and benchmarks. The lack of resources hinders efforts to evaluate and advance the multilingual capabilities of Large Language Models (LLMs). In this paper, we introduce MedAraBench, a large-scale dataset consisting of Arabic multiple-choice question-answer pairs across various medical specialties. We constructed the dataset by manually digitizing a large repository of academic materials created by medical professionals in the Arabic-speaking region. We then conducted extensive preprocessing and split the dataset into training and test sets to support future research efforts in the area. To assess the quality of the data, we adopted two frameworks, namely expert human evaluation and LLM-as-a-judge. Our dataset is diverse and of high quality, spanning 19 specialties and five difficulty levels. For benchmarking purposes, we assessed the performance of eight state-of-the-art open-source and proprietary models, such as GPT-5, Gemini 2.0 Flash, and Claude 4-Sonnet. Our findings highlight the need for further domain-specific enhancements. We release the dataset and evaluation scripts to broaden the diversity of medical data benchmarks, expand the scope of evaluation suites for LLMs, and enhance the multilingual capabilities of models for deployment in clinical settings.

</details>


### [115] [Mechanistic Indicators of Steering Effectiveness in Large Language Models](https://arxiv.org/abs/2602.01716)
*Mehdi Jafari,Hao Xue,Flora Salim*

Main category: cs.CL

TL;DR: 论文研究通过内部模型信号诊断LLM激活导向的可靠性，使用信息论指标预测导向成功概率


<details>
  <summary>Details</summary>
Motivation: 尽管激活导向技术被广泛使用，但其成功或失败的内在机制因素仍不清楚，先前工作主要依赖黑盒输出或LLM判断，需要更深入的理解

Method: 采用两种信息论度量：基于熵的归一化分支因子(NBF)和词汇空间中导向激活与目标概念之间的KL散度；使用LLM生成的标注作为ground truth，建立可靠性研究框架

Result: 这些机制信号为识别成功导向和估计失败概率提供了有意义的预测能力；为两种最广泛采用的激活导向方法（对比激活添加和稀疏自编码器）引入了更强的评估基线

Conclusion: 内部模型信号可以有效诊断激活导向的可靠性，信息论指标能够预测导向成功概率，为理解LLM导向机制提供了新视角

Abstract: Activation-based steering enables Large Language Models (LLMs) to exhibit targeted behaviors by intervening on intermediate activations without retraining. Despite its widespread use, the mechanistic factors that govern when steering succeeds or fails remain poorly understood, as prior work has relied primarily on black-box outputs or LLM-based judges. In this study, we investigate whether the reliability of steering can be diagnosed using internal model signals. We focus on two information-theoretic measures: the entropy-derived Normalized Branching Factor (NBF), and the Kullback-Leibler (KL) divergence between steered activations and targeted concepts in the vocabulary space. We hypothesize that effective steering corresponds to structured entropy preservation and coherent KL alignment across decoding steps. Building on a reliability study demonstrating high inter-judge agreement between two architecturally distinct LLMs, we use LLM-generated annotations as ground truth and show that these mechanistic signals provide meaningful predictive power for identifying successful steering and estimating failure probability. We further introduce a stronger evaluation baseline for Contrastive Activation Addition (CAA) and Sparse Autoencoder-based steering, the two most widely adopted activation-steering methods.

</details>


### [116] [BBPE16: UTF-16-based byte-level byte-pair encoding for improved multilingual speech recognition](https://arxiv.org/abs/2602.01717)
*Hyunsik Kim,Haeri Kim,Munhak Lee,Kyungmin Lee*

Main category: cs.CL

TL;DR: 提出BBPE16：基于UTF-16的字节级BPE分词器，用统一的2字节编码单元表示大多数现代文字，减少非拉丁文字（如中日韩）的token序列长度，提升多语言ASR效率。


<details>
  <summary>Details</summary>
Motivation: 当前广泛使用的UTF-8字节级BPE（BBPE）虽然语言无关且覆盖全Unicode，但对非拉丁文字（如中日韩）采用变长编码，导致token序列过长，增加计算负载和内存使用。

Method: 提出BBPE16分词器，基于UTF-16编码，使大多数现代文字使用统一的2字节编码单元，保持语言无关性的同时显著提升跨语言token共享。

Result: 在单语、双语、三语ASR及多语言持续学习设置中，BBPE16达到相当或更好的准确率；对中文减少token数量达10.4%，降低解码迭代达10.3%，加速微调和推理，减少内存使用。

Conclusion: BBPE16作为实用的多语言ASR分词选择，在保持BBPE优点的同时，通过减少非拉丁文字的token序列长度，显著提升了计算效率和内存使用效率。

Abstract: Multilingual automatic speech recognition (ASR) requires tokenization that efficiently covers many writing systems. Byte-level BPE (BBPE) using UTF-8 is widely adopted for its language-agnostic design and full Unicode coverage, but its variable-length encoding inflates token sequences for non-Latin scripts, such as Chinese, Japanese, and Korean (CJK). Longer sequences increase computational load and memory use. We propose BBPE16, a UTF-16-based BBPE tokenizer that represents most modern scripts with a uniform 2-byte code unit. BBPE16 preserves BBPE's language-agnostic properties while substantially improving cross-lingual token sharing. Across monolingual, bilingual, and trilingual ASR, and in a multilingual continual-learning setup, BBPE16 attains comparable or better accuracy; for Chinese, it reduces token counts by up to 10.4% and lowers decoding iterations by up to 10.3%. These reductions speed up fine-tuning and inference and decrease memory usage, making BBPE16 a practical tokenization choice for multilingual ASR.

</details>


### [117] [COMI: Coarse-to-fine Context Compression via Marginal Information Gain](https://arxiv.org/abs/2602.01719)
*Jiwei Tang,Shilei Liu,Zhicheng Zhang,Yujin Yuan,Libin Zheng,Wenbo Su,Bo Zheng*

Main category: cs.CL

TL;DR: COMI是一个粗到细的自适应上下文压缩框架，通过联合优化语义相关性和多样性，在高压缩率下有效减少长上下文输入的长度和冗余信息。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在长上下文场景中面临计算效率低下和信息冗余的问题，现有的上下文压缩方法需要更有效的机制来在高压缩率下保持关键语义信息。

Method: 提出COMI框架，包含两个阶段：1）粗粒度组重分配，基于组间边际信息增益动态分配压缩率；2）细粒度令牌合并，通过组内MIG加权机制融合令牌。引入边际信息增益（MIG）作为指导压缩的度量标准。

Result: 在问答（NaturalQuestions、2WikiMQA、HotpotQA、NarrativeQA）和摘要（MultiNews）任务上，使用多种骨干模型（LLaMA-2-7B、Qwen2-7B）进行实验，COMI大幅超越现有基线，如在NaturalQuestions上使用Qwen2-7B在32倍压缩约束下获得约25分的精确匹配提升。

Conclusion: COMI框架通过粗到细的自适应压缩策略，在高压缩率下有效平衡语义相关性和多样性，显著提升了长上下文场景中LLM的计算效率和性能。

Abstract: Large Language Models (LLMs) have demonstrated exceptional capabilities across diverse tasks. However, their deployment in long context scenarios remains hindered by computational inefficiency and information redundancy. Context compression methods address these challenges by significantly reducing input length and eliminating redundancy. We propose COMI, a coarse-to-fine adaptive context compression framework that jointly optimizes for semantic relevance and diversity under high compression rates. We introduce Marginal Information Gain (MIG), a metric defined as the relevance of a unit to the input query minus its semantic redundancy with other units, guiding the compression process to prioritize information that is both relevant and low redundant. The framework operates in two stages: (1) Coarse-Grained Group Reallocation, where the context is partitioned into groups and dynamically assigned compression rates based on inter-group MIG, ensuring compression budgets align with information value distribution; and (2) Fine-Grained Token Merging, where tokens within each group are fused via an intra-group MIG-based weighting mechanism, thereby preserving key semantics while avoiding the accumulation of redundancy. Extensive experiments across question-answering (e.g., NaturalQuestions, 2WikiMQA, HotpotQA and NarrativeQA), summarization (e.g., MultiNews) with various backbones (e.g., LLaMA-2-7B, Qwen2-7B) show that COMI outperforms existing baselines by a large margin, e.g., approximately 25-point Exact Match (EM) improvement under 32x compression constraint with Qwen2-7B on NaturalQuestions.

</details>


### [118] [SafePred: A Predictive Guardrail for Computer-Using Agents via World Models](https://arxiv.org/abs/2602.01725)
*Yurun Chen,Zeyi Liao,Ping Yin,Taotao Xie,Keting Yin,Shengyu Zhang*

Main category: cs.CL

TL;DR: SafePred是一个预测性护栏框架，通过将预测的未来风险与当前决策对齐，防止计算机使用代理的长期风险行为。


<details>
  <summary>Details</summary>
Motivation: 现有CUAs护栏主要采用反应式方法，只能约束当前观察空间内的行为，无法主动避免长期风险。看似合理的行动可能导致延迟出现的高风险后果，反应式护栏无法在当前观察空间内识别这些风险。

Method: 提出预测性护栏方法，核心思想是将预测的未来风险与当前决策对齐。SafePred框架建立风险到决策循环，支持：(1) 短期和长期风险预测：使用安全策略作为风险预测基础，利用世界模型的预测能力生成风险语义表示；(2) 决策优化：通过步骤级干预和任务级重新规划，将预测风险转化为可操作的安全决策指导。

Result: 实验表明SafePred显著减少高风险行为，相比反应式基线实现超过97.6%的安全性能，并将任务效用提升高达21.4%。

Conclusion: SafePred通过预测性护栏方法有效解决了CUAs长期风险问题，建立了风险到决策的闭环，在保证安全的同时提升了任务效用。

Abstract: With the widespread deployment of Computer-using Agents (CUAs) in complex real-world environments, prevalent long-term risks often lead to severe and irreversible consequences. Most existing guardrails for CUAs adopt a reactive approach, constraining agent behavior only within the current observation space. While these guardrails can prevent immediate short-term risks (e.g., clicking on a phishing link), they cannot proactively avoid long-term risks: seemingly reasonable actions can lead to high-risk consequences that emerge with a delay (e.g., cleaning logs leads to future audits being untraceable), which reactive guardrails cannot identify within the current observation space. To address these limitations, we propose a predictive guardrail approach, with the core idea of aligning predicted future risks with current decisions. Based on this approach, we present SafePred, a predictive guardrail framework for CUAs that establishes a risk-to-decision loop to ensure safe agent behavior. SafePred supports two key abilities: (1) Short- and long-term risk prediction: by using safety policies as the basis for risk prediction, SafePred leverages the prediction capability of the world model to generate semantic representations of both short-term and long-term risks, thereby identifying and pruning actions that lead to high-risk states; (2) Decision optimization: translating predicted risks into actionable safe decision guidances through step-level interventions and task-level re-planning. Extensive experiments show that SafePred significantly reduces high-risk behaviors, achieving over 97.6% safety performance and improving task utility by up to 21.4% compared with reactive baselines.

</details>


### [119] [Enhancing Automated Essay Scoring with Three Techniques: Two-Stage Fine-Tuning, Score Alignment, and Self-Training](https://arxiv.org/abs/2602.01747)
*Hongseok Choi,Serynn Kim,Wencke Liermann,Jin Seong,Jin-Xia Huang*

Main category: cs.CL

TL;DR: 论文提出三种技术提升自动作文评分系统在有限数据和完整数据场景下的性能：两阶段微调、分数对齐和不确定性感知自训练，在ASAP++数据集上取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 现实世界中标注数据极度稀缺，严重限制了自动作文评分系统的开发和实际应用，需要解决有限数据场景下的性能提升问题。

Method: 提出三种关键技术：1）两阶段微调策略，利用低秩适应更好适应目标提示作文；2）分数对齐技术，改善预测分数与真实分数分布的一致性；3）不确定性感知自训练，利用未标注数据扩展训练集同时减轻标签噪声传播。这些技术在DualBERT上实现。

Result: 在32数据设置下，三种技术均提升性能，集成后达到完整数据性能的91.2%（仅用约1000个标注样本）。分数对齐技术在有限数据和完整数据场景下均持续提升性能，在完整数据设置下与DualBERT集成达到最先进结果。

Conclusion: 提出的三种技术能有效提升自动作文评分系统在有限数据和完整数据场景下的性能，分数对齐技术特别有效，为实际应用提供了有前景的解决方案。

Abstract: Automated Essay Scoring (AES) plays a crucial role in education by providing scalable and efficient assessment tools. However, in real-world settings, the extreme scarcity of labeled data severely limits the development and practical adoption of robust AES systems. This study proposes a novel approach to enhance AES performance in both limited-data and full-data settings by introducing three key techniques. First, we introduce a Two-Stage fine-tuning strategy that leverages low-rank adaptations to better adapt an AES model to target prompt essays. Second, we introduce a Score Alignment technique to improve consistency between predicted and true score distributions. Third, we employ uncertainty-aware self-training using unlabeled data, effectively expanding the training set with pseudo-labeled samples while mitigating label noise propagation. We implement above three key techniques on DualBERT. We conduct extensive experiments on the ASAP++ dataset. As a result, in the 32-data setting, all three key techniques improve performance, and their integration achieves 91.2% of the full-data performance trained on approximately 1,000 labeled samples. In addition, the proposed Score Alignment technique consistently improves performance in both limited-data and full-data settings: e.g., it achieves state-of-the-art results in the full-data setting when integrated into DualBERT.

</details>


### [120] [WorldCup Sampling for Multi-bit LLM Watermarking](https://arxiv.org/abs/2602.01752)
*Yidan Wang,Yubing Ren,Yanan Cao,Li Guo*

Main category: cs.CL

TL;DR: WorldCup是一个用于大语言模型的多比特水印框架，通过分层竞争机制将信息比特直接嵌入到token选择中，实现了高容量、可检测性、鲁棒性和文本质量的平衡。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型生成越来越像人类的文本，水印技术为超越简单检测的可靠溯源提供了有前景的解决方案。现有的多比特水印方法主要通过种子驱动的引导来扩展零比特方案，导致间接信息流、有限的有效容量和次优的解码性能。

Method: WorldCup将采样视为自然通信通道，通过分层竞争机制将消息比特直接嵌入到token选择中，该机制由互补信号引导。此外，采用熵感知调制来保持生成质量，并通过置信感知解码支持鲁棒的消息恢复。

Result: 综合实验表明，WorldCup在容量、可检测性、鲁棒性、文本质量和解码效率之间实现了强大的平衡，一致优于先前的基线方法。

Conclusion: WorldCup为未来大语言模型水印研究奠定了坚实的基础，展示了通过直接嵌入信息比特到token选择中的分层竞争机制的有效性。

Abstract: As large language models (LLMs) generate increasingly human-like text, watermarking offers a promising solution for reliable attribution beyond mere detection. While multi-bit watermarking enables richer provenance encoding, existing methods largely extend zero-bit schemes through seed-driven steering, leading to indirect information flow, limited effective capacity, and suboptimal decoding. In this paper, we propose WorldCup, a multi-bit watermarking framework for LLMs that treats sampling as a natural communication channel and embeds message bits directly into token selection via a hierarchical competition mechanism guided by complementary signals. Moreover, WorldCup further adopts entropy-aware modulation to preserve generation quality and supports robust message recovery through confidence-aware decoding. Comprehensive experiments show that WorldCup achieves a strong balance across capacity, detectability, robustness, text quality, and decoding efficiency, consistently outperforming prior baselines and laying a solid foundation for future LLM watermarking studies.

</details>


### [121] [Zero2Text: Zero-Training Cross-Domain Inversion Attacks on Textual Embeddings](https://arxiv.org/abs/2602.01757)
*Doohyun Kim,Donghwa Kang,Kyungjae Lee,Hyeongboo Baek,Brent Byunghoon Kang*

Main category: cs.CL

TL;DR: Zero2Text是一种无需训练、基于递归在线对齐的框架，用于解决向量数据库中的隐私风险，通过结合LLM先验和动态岭回归机制，在严格黑盒和跨域设置中有效恢复文本，无需泄露数据对。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成(RAG)的普及使向量数据库成为关键基础设施，但嵌入反转攻击带来了严重的隐私风险。现有方法面临基本权衡：基于优化的方法需要计算量巨大的查询，而基于对齐的方法依赖于不切实际的可访问域内训练数据假设，在严格黑盒和跨域设置中无效。

Method: Zero2Text是一种基于递归在线对齐的无训练框架。它结合LLM先验和动态岭回归机制，迭代地将生成与目标嵌入对齐，无需静态数据集，完全在线进行。

Result: 在MS MARCO基准测试中，针对OpenAI受害者模型，Zero2Text相比基线方法实现了1.8倍的ROUGE-L和6.4倍的BLEU-2分数提升，能够从未知领域恢复句子而无需任何泄露的数据对。标准防御方法如差分隐私无法有效缓解这种自适应威胁。

Conclusion: Zero2Text成功解决了现有嵌入反转攻击方法在严格黑盒和跨域设置中的局限性，通过训练无关的递归在线对齐框架，有效恢复了文本，同时证明了现有防御措施的不足。

Abstract: The proliferation of retrieval-augmented generation (RAG) has established vector databases as critical infrastructure, yet they introduce severe privacy risks via embedding inversion attacks. Existing paradigms face a fundamental trade-off: optimization-based methods require computationally prohibitive queries, while alignment-based approaches hinge on the unrealistic assumption of accessible in-domain training data. These constraints render them ineffective in strict black-box and cross-domain settings. To dismantle these barriers, we introduce Zero2Text, a novel training-free framework based on recursive online alignment. Unlike methods relying on static datasets, Zero2Text synergizes LLM priors with a dynamic ridge regression mechanism to iteratively align generation to the target embedding on-the-fly. We further demonstrate that standard defenses, such as differential privacy, fail to effectively mitigate this adaptive threat. Extensive experiments across diverse benchmarks validate Zero2Text; notably, on MS MARCO against the OpenAI victim model, it achieves 1.8x higher ROUGE-L and 6.4x higher BLEU-2 scores compared to baselines, recovering sentences from unknown domains without a single leaked data pair.

</details>


### [122] [<SOG_k>: One LLM Token for Explicit Graph Structural Understanding](https://arxiv.org/abs/2602.01771)
*Jingyao Wu,Bin Lu,Zijun Di,Xiaoying Gan,Meng Jin,Luoyi Fu,Xinbing Wang,Chenghu Zhou*

Main category: cs.CL

TL;DR: 提出SOG方法，使用特殊token <SOG_k>在统一token空间中表示图结构，解决LLMs处理图数据时的结构幻觉问题，显著提升性能


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在非结构化数据理解方面表现出色，但在处理图数据时面临结构幻觉挑战。现有方法要么将图转化为自然语言（消耗过多token且注意力分散），要么转化为可训练的连续嵌入（与原始文本token严重不对齐）

Method: 提出拓扑感知的结构tokenizer，将每个图拓扑映射为高度选择性的单个token <SOG_k>，构建混合结构问答语料库对齐新的结构token与现有文本token

Result: 在五个图级基准测试中，相比基线方法性能提升9.9%到41.4%，同时表现出可解释性和一致性。方法可灵活扩展到节点级任务，实现全局和局部结构理解

Conclusion: SOG方法通过特殊token <SOG_k>在统一token空间中表示图结构，有效解决了LLMs处理图数据时的结构幻觉问题，实现了简洁准确的理解、生成和推理能力

Abstract: Large language models show great potential in unstructured data understanding, but still face significant challenges with graphs due to their structural hallucination. Existing approaches mainly either verbalize graphs into natural language, which leads to excessive token consumption and scattered attention, or transform graphs into trainable continuous embeddings (i.e., soft prompt), but exhibit severe misalignment with original text tokens. To solve this problem, we propose to incorporate one special token <SOG_k> to fully represent the Structure Of Graph within a unified token space, facilitating explicit topology input and structural information sharing. Specifically, we propose a topology-aware structural tokenizer that maps each graph topology into a highly selective single token. Afterwards, we construct a set of hybrid structure Question-Answering corpora to align new structural tokens with existing text tokens. With this approach, <SOG_k> empowers LLMs to understand, generate, and reason in a concise and accurate manner. Extensive experiments on five graph-level benchmarks demonstrate the superiority of our method, achieving a performance improvement of 9.9% to 41.4% compared to the baselines while exhibiting interpretability and consistency. Furthermore, our method provides a flexible extension to node-level tasks, enabling both global and local structural understanding. The codebase is publicly available at https://github.com/Jingyao-Wu/SOG.

</details>


### [123] [Data Distribution Matters: A Data-Centric Perspective on Context Compression for Large Language Model](https://arxiv.org/abs/2602.01778)
*Kangtao Lv,Jiwei Tang,Langming Liu,Haibin Chen,Weidong Zhang,Shilei Liu,Yongwei Wang,Yujin Yuan,Wenbo Su,Bo Zheng*

Main category: cs.CL

TL;DR: 该论文首次从数据中心视角研究数据分布对上下文压缩质量的影响，发现输入熵与压缩质量负相关，编解码器内在数据差异显著降低压缩增益


<details>
  <summary>Details</summary>
Motivation: 大语言模型在长上下文场景中面临计算效率低下和信息冗余问题，现有研究仅关注模型端改进，数据分布本身对上下文压缩的影响尚未被探索

Method: 采用数据中心视角，从输入数据和内在数据两个维度系统研究数据分布对压缩质量的影响，使用基于自动编码器的框架评估压缩表示的语义完整性

Result: 1)编码器测量的输入熵与压缩质量负相关，而解码器测量的熵在冻结解码器设置下无显著关系；2)编解码器内在数据差异显著降低压缩增益且难以缓解

Conclusion: 基于研究发现提出了优化压缩增益的实用指南，强调了数据中心方法在上下文压缩中的重要性

Abstract: The deployment of Large Language Models (LLMs) in long-context scenarios is hindered by computational inefficiency and significant information redundancy. Although recent advancements have widely adopted context compression to address these challenges, existing research only focus on model-side improvements, the impact of the data distribution itself on context compression remains largely unexplored. To bridge this gap, we are the first to adopt a data-centric perspective to systematically investigate how data distribution impacts compression quality, including two dimensions: input data and intrinsic data (i.e., the model's internal pretrained knowledge). We evaluate the semantic integrity of compressed representations using an autoencoder-based framework to systematically investigate it. Our experimental results reveal that: (1) encoder-measured input entropy negatively correlates with compression quality, while decoder-measured entropy shows no significant relationship under a frozen-decoder setting; and (2) the gap between intrinsic data of the encoder and decoder significantly diminishes compression gains, which is hard to mitigate. Based on these findings, we further present practical guidelines to optimize compression gains.

</details>


### [124] [CodeOCR: On the Effectiveness of Vision Language Models in Code Understanding](https://arxiv.org/abs/2602.01785)
*Yuling Shi,Chaoxiang Xie,Zhensu Sun,Yeheng Chen,Chenxu Zhang,Longfei Yun,Chengcheng Wan,Hongyu Zhang,David Lo,Xiaodong Gu*

Main category: cs.CL

TL;DR: MLLMs通过将代码渲染为图像实现高达8倍压缩，在保持理解能力的同时显著提升计算效率


<details>
  <summary>Details</summary>
Motivation: 传统LLMs将代码作为文本序列处理，随着软件规模增长导致上下文长度线性增加和计算成本上升。图像模态天生适合压缩，MLLMs的发展为通过图像表示代码优化效率提供了机会

Method: 将源代码渲染为图像，通过调整分辨率实现视觉压缩，使用多模态大语言模型（MLLMs）进行代码理解，系统研究MLLMs在代码理解任务中的有效性

Result: 1) MLLMs能有效理解压缩后的代码，实现高达8倍压缩；2) MLLMs能有效利用语法高亮等视觉线索，在4倍压缩下提升代码补全性能；3) 克隆检测等任务对视觉压缩表现出异常韧性，某些压缩比甚至略优于原始文本输入

Conclusion: MLLMs在代码理解方面展现出潜力，图像模态的代码表示为更高效推理提供了新途径，但也存在当前局限性，指出了向图像模态代码表示转变的方向

Abstract: Large Language Models (LLMs) have achieved remarkable success in source code understanding, yet as software systems grow in scale, computational efficiency has become a critical bottleneck. Currently, these models rely on a text-based paradigm that treats source code as a linear sequence of tokens, which leads to a linear increase in context length and associated computational costs. The rapid advancement of Multimodal LLMs (MLLMs) introduces an opportunity to optimize efficiency by representing source code as rendered images. Unlike text, which is difficult to compress without losing semantic meaning, the image modality is inherently suitable for compression. By adjusting resolution, images can be scaled to a fraction of their original token cost while remaining recognizable to vision-capable models. To explore the feasibility of this approach, we conduct the first systematic study on the effectiveness of MLLMs for code understanding. Our experiments reveal that: (1) MLLMs can effectively understand code with substantial token reduction, achieving up to 8x compression; (2) MLLMs can effectively leverage visual cues such as syntax highlighting, improving code completion performance under 4x compression; and (3) Code-understanding tasks like clone detection exhibit exceptional resilience to visual compression, with some compression ratios even slightly outperforming raw text inputs. Our findings highlight both the potential and current limitations of MLLMs in code understanding, which points out a shift toward image-modality code representation as a pathway to more efficient inference.

</details>


### [125] [Sentence Curve Language Models](https://arxiv.org/abs/2602.01807)
*DongNyeong Heo,Heelyoul Choi*

Main category: cs.CL

TL;DR: 论文提出句子曲线语言模型（SCLM），通过预测连续句子曲线而非静态词嵌入，解决传统语言模型中目标词嵌入对邻词不敏感的问题，提升全局结构建模能力。


<details>
  <summary>Details</summary>
Motivation: 传统语言模型（包括扩散语言模型）使用静态词嵌入表示目标句子，这种表示对邻词不敏感，导致模型更关注局部准确的词预测而忽视句子的全局结构。需要一种能更好捕捉句子整体结构的表示方法。

Method: 提出句子曲线表示法，将句子表示为样条曲线，其控制点影响句子中的多个词。基于此表示，开发句子曲线语言模型（SCLM），扩展扩散语言模型以预测句子曲线而非静态词嵌入。

Result: SCLM在IWSLT14和WMT14数据集上达到扩散语言模型中的SOTA性能，训练稳定且无需繁琐的知识蒸馏，在LM1B数据集上相比离散扩散语言模型显示出有潜力的表现。

Conclusion: 句子曲线表示通过促进全局结构建模，有效解决了传统词嵌入的局限性。SCLM在多个基准测试中表现出色，为语言建模提供了新的连续表示范式。

Abstract: Language models (LMs) are a central component of modern AI systems, and diffusion-based language models (DLMs) have recently emerged as a competitive alternative. Both paradigms rely on word embeddings not only to represent the input sentence, but also to represent the target sentence that backbone models are trained to predict. We argue that such static embedding of the target word is insensitive to neighboring words, encouraging locally accurate word prediction while neglecting global structure across the target sentence. To address this limitation, we propose a continuous sentence representation, termed sentence curve, defined as a spline curve whose control points affect multiple words in the sentence. Based on this representation, we introduce sentence curve language model (SCLM), which extends DLMs to predict sentence curves instead of the static word embeddings. We theoretically show that sentence curve prediction induces a regularization effect that promotes global structure modeling, and characterize how different sentence curve types affect this behavior. Empirically, SCLM achieves SOTA performance among DLMs on IWSLT14 and WMT14, shows stable training without burdensome knowledge distillation, and demonstrates promising potential compared to discrete DLMs on LM1B.

</details>


### [126] [AXE: Low-Cost Cross-Domain Web Structured Information Extraction](https://arxiv.org/abs/2602.01838)
*Abdelrahman Mansour,Khaled W. Alshaer,Moataz Elsaban*

Main category: cs.CL

TL;DR: AXE是一个通过DOM树剪枝和XPath追踪实现高效网页信息提取的轻量级系统，仅用0.6B小模型就达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 解决网页结构化数据提取中手工规则脆弱性和大语言模型成本过高之间的权衡问题，提供实用且经济的大规模网页信息提取方案

Method: 1) 将HTML DOM视为需要剪枝的树而非文本墙；2) 使用专门的"剪枝"机制去除模板和无关节点；3) 实现Grounded XPath Resolution确保每个提取都能物理追踪到源节点；4) 使用仅0.6B的小型LLM生成精确结构化输出

Result: 在SWDE数据集上达到88.1%的F1分数，零样本性能超越多个更大规模的全训练替代方案，实现最先进性能

Conclusion: AXE通过DOM树剪枝和物理可追踪的提取机制，为大规模网页信息提取提供了实用且经济高效的路径，展示了小模型在精心设计的预处理下也能达到优异性能

Abstract: Extracting structured data from the web is often a trade-off between the brittle nature of manual heuristics and the prohibitive cost of Large Language Models. We introduce AXE (Adaptive X-Path Extractor), a pipeline that rethinks this process by treating the HTML DOM as a tree that needs pruning rather than just a wall of text to be read. AXE uses a specialized "pruning" mechanism to strip away boilerplate and irrelevant nodes, leaving behind a distilled, high-density context that allows a tiny 0.6B LLM to generate precise, structured outputs. To keep the model honest, we implement Grounded XPath Resolution (GXR), ensuring every extraction is physically traceable to a source node. Despite its low footprint, AXE achieves state-of-the-art zero-shot performance, outperforming several much larger, fully-trained alternatives with an F1 score of 88.1% on the SWDE dataset. By releasing our specialized adaptors, we aim to provide a practical, cost-effective path for large-scale web information extraction.

</details>


### [127] [Read As Human: Compressing Context via Parallelizable Close Reading and Skimming](https://arxiv.org/abs/2602.01840)
*Jiwei Tang,Shilei Liu,Zhicheng Zhang,Qingsong Lv,Runsong Zhao,Tingwei Lu,Langming Liu,Haibin Chen,Yujin Yuan,Hai-Tao Zheng,Wenbo Su,Bo Zheng*

Main category: cs.CL

TL;DR: RAM框架通过模拟人类阅读行为（精读重要内容+略读次要内容），采用自适应混合阅读策略压缩长上下文，在保持性能的同时实现高达12倍的速度提升。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在长上下文场景下面临计算效率低下和信息冗余两大挑战，需要一种既能保持性能又能提高效率的上下文压缩方法。

Method: RAM框架将上下文分割成片段，并行编码输入查询。高相关性片段完整保留（精读），低相关性片段通过查询引导压缩为紧凑摘要向量（略读）。引入基于正负查询-片段对的对比学习目标来优化精读与略读的决策边界。

Result: 在多个问答和摘要基准测试中，RAM在两个骨干模型上都优于现有基线方法，在长输入（平均长度16K，最大长度32K）上实现了高达12倍的端到端加速。

Conclusion: RAM框架通过模拟人类阅读的自适应混合策略，有效解决了长上下文场景下的计算效率和冗余信息问题，在保持性能的同时显著提升了处理速度。

Abstract: Large Language Models (LLMs) demonstrate exceptional capability across diverse tasks. However, their deployment in long-context scenarios is hindered by two challenges: computational inefficiency and redundant information. We propose RAM (Read As HuMan), a context compression framework that adopts an adaptive hybrid reading strategy, to address these challenges. Inspired by human reading behavior (i.e., close reading important content while skimming less relevant content), RAM partitions the context into segments and encodes them with the input query in parallel. High-relevance segments are fully retained (close reading), while low-relevance ones are query-guided compressed into compact summary vectors (skimming). Both explicit textual segments and implicit summary vectors are concatenated and fed into decoder to achieve both superior performance and natural language format interpretability. To refine the decision boundary between close reading and skimming, we further introduce a contrastive learning objective based on positive and negative query-segment pairs. Experiments demonstrate that RAM outperforms existing baselines on multiple question answering and summarization benchmarks across two backbones, while delivering up to a 12x end-to-end speedup on long inputs (average length 16K; maximum length 32K).

</details>


### [128] [PretrainRL: Alleviating Factuality Hallucination of Large Language Models at the Beginning](https://arxiv.org/abs/2602.01875)
*Langming Liu,Kangtao Lv,Haibin Chen,Weidong Zhang,Yejing Wang,Shilei Liu,Xin Tong,Yujin Yuan,Yongwei Wang,Wenbo Su,Bo Zheng*

Main category: cs.CL

TL;DR: PretrainRL：在预训练阶段集成强化学习来缓解LLM的事实幻觉问题，通过"去偏后学习"原则主动重塑模型概率分布


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在事实幻觉问题，根源在于预训练语料的数据分布不平衡，导致"低概率真理"和"高概率谬误"的状态。现有方法要么回避问题，要么面临灾难性遗忘

Method: 提出PretrainRL框架，将强化学习集成到预训练阶段，采用"去偏后学习"原则：通过降低高概率谬误的权重来重塑模型概率分布，为低概率真理创造学习空间。设计了高效的负采样策略来发现高概率谬误，并引入新的指标来评估模型关于事实知识的概率状态

Result: 在三个公共基准测试上的广泛实验表明，PretrainRL显著缓解了事实幻觉问题，并优于最先进的方法

Conclusion: PretrainRL通过从根本上解决预训练数据分布不平衡问题，有效缓解了LLM的事实幻觉，为模型事实知识巩固提供了新思路

Abstract: Large language models (LLMs), despite their powerful capabilities, suffer from factual hallucinations where they generate verifiable falsehoods. We identify a root of this issue: the imbalanced data distribution in the pretraining corpus, which leads to a state of "low-probability truth" and "high-probability falsehood". Recent approaches, such as teaching models to say "I don't know" or post-hoc knowledge editing, either evade the problem or face catastrophic forgetting. To address this issue from its root, we propose \textbf{PretrainRL}, a novel framework that integrates reinforcement learning into the pretraining phase to consolidate factual knowledge. The core principle of PretrainRL is "\textbf{debiasing then learning}." It actively reshapes the model's probability distribution by down-weighting high-probability falsehoods, thereby making "room" for low-probability truths to be learned effectively. To enable this, we design an efficient negative sampling strategy to discover these high-probability falsehoods and introduce novel metrics to evaluate the model's probabilistic state concerning factual knowledge. Extensive experiments on three public benchmarks demonstrate that PretrainRL significantly alleviates factual hallucinations and outperforms state-of-the-art methods.

</details>


### [129] [ES-MemEval: Benchmarking Conversational Agents on Personalized Long-Term Emotional Support](https://arxiv.org/abs/2602.01885)
*Tiantian Chen,Jiaqi Lu,Ying Shen,Lin Zhang*

Main category: cs.CL

TL;DR: ES-MemEval是一个评估长期情感支持对话中记忆能力的基准，包含信息提取、时序推理、冲突检测、弃权和用户建模五个核心能力，实验显示显式长期记忆对减少幻觉和实现个性化至关重要。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型作为对话代理的能力受到长期记忆不足的限制，特别是在复杂、长期的在线情感支持服务中。现有的长期对话基准主要关注静态和显式事实检索，无法评估用户信息分散、隐式且持续演化的关键场景。

Method: 提出了ES-MemEval基准，系统评估五个核心记忆能力：信息提取、时序推理、冲突检测、弃权和用户建模。同时构建了EvoEmo数据集，这是一个用于个性化长期情感支持的多会话数据集，捕捉碎片化、隐式的用户披露和演化中的用户状态。

Result: 对开源长上下文、商业和检索增强大语言模型的广泛实验表明：显式长期记忆对减少幻觉和实现有效个性化至关重要；检索增强生成提高了事实一致性，但在处理时序动态和演化用户状态方面存在困难。

Conclusion: 当前范式既有潜力也有局限性，需要更鲁棒地整合记忆和检索机制来构建长期个性化对话系统。研究结果强调了显式长期记忆的重要性，并指出了现有方法在处理动态用户状态方面的不足。

Abstract: Large Language Models (LLMs) have shown strong potential as conversational agents. Yet, their effectiveness remains limited by deficiencies in robust long-term memory, particularly in complex, long-term web-based services such as online emotional support. However, existing long-term dialogue benchmarks primarily focus on static and explicit fact retrieval, failing to evaluate agents in critical scenarios where user information is dispersed, implicit, and continuously evolving. To address this gap, we introduce ES-MemEval, a comprehensive benchmark that systematically evaluates five core memory capabilities: information extraction, temporal reasoning, conflict detection, abstention, and user modeling, in long-term emotional support settings, covering question answering, summarization, and dialogue generation tasks. To support the benchmark, we also propose EvoEmo, a multi-session dataset for personalized long-term emotional support that captures fragmented, implicit user disclosures and evolving user states. Extensive experiments on open-source long-context, commercial, and retrieval-augmented (RAG) LLMs show that explicit long-term memory is essential for reducing hallucinations and enabling effective personalization. At the same time, RAG improves factual consistency but struggles with temporal dynamics and evolving user states. These findings highlight both the potential and limitations of current paradigms and motivate more robust integration of memory and retrieval for long-term personalized dialogue systems.

</details>


### [130] [GuideWeb: A Benchmark for Automatic In-App Guide Generation on Real-World Web UIs](https://arxiv.org/abs/2602.01917)
*Chengguang Gan,Yoshihiro Tsujii,Yunhao Liang,Tatsunori Mori,Shiwen Ni,Hiroki Itoh*

Main category: cs.CL

TL;DR: GuideWeb是一个用于网页应用内自动指南生成的新基准，将任务定义为基于网页元素选择指南目标并生成简洁的指南文本，现有方法表现有限，自动指南生成仍具挑战性。


<details>
  <summary>Details</summary>
Motivation: 数字采用平台（DAP）需要手动维护网页指南，但网站布局和功能不断演变，导致维护成本高昂，需要自动化的指南生成解决方案。

Method: 提出GuideWeb基准，将任务形式化为选择网页中的指南目标元素并生成与用户意图一致的简洁指南文本，同时开发了综合评估套件来联合测量元素选择和文本生成质量。

Result: 提出的GuideWeb代理在指南目标元素预测上达到30.79%准确率，意图生成BLEU得分为44.94，指南文本生成BLEU得分为21.34，现有基线方法表现显著更差。

Conclusion: 自动指南生成仍然具有挑战性，需要进一步的技术进步才能在真实世界环境中可靠部署，GuideWeb基准为这一领域的研究提供了评估基础。

Abstract: Digital Adoption Platform (DAP) provide web-based overlays that deliver operation guidance and contextual hints to help users navigate complex websites. Although modern DAP tools enable non-experts to author such guidance, maintaining these guides remains labor-intensive because website layouts and functionalities evolve continuously, which requires repeated manual updates and re-annotation. In this work, we introduce \textbf{GuideWeb}, a new benchmark for automatic in-app guide generation on real-world web UIs. GuideWeb formulates the task as producing page-level guidance by selecting \textbf{guide target elements} grounded in the webpage and generating concise guide text aligned with user intent. We also propose a comprehensive evaluation suite that jointly measures the accuracy of guide target element selection and the quality of generated intents and guide texts. Experiments show that our proposed \textbf{GuideWeb Agent} achieves \textbf{30.79\%} accuracy in guide target element prediction, while obtaining BLEU scores of \textbf{44.94} for intent generation and \textbf{21.34} for guide-text generation. Existing baselines perform substantially worse, which highlights that automatic guide generation remains challenging and that further advances are necessary before such systems can be reliably deployed in real-world settings.

</details>


### [131] [From Code-Centric to Concept-Centric: Teaching NLP with LLM-Assisted "Vibe Coding"](https://arxiv.org/abs/2602.01919)
*Hend Al-Khalifa*

Main category: cs.CL

TL;DR: 论文提出"Vibe Coding"教学法，在NLP课程中使用LLM作为编程助手，通过反思性评估强调概念理解而非代码语法，学生反馈积极但面临时间限制和输出验证等挑战。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的快速发展给NLP教育带来挑战与机遇，需要探索如何有效利用LLM作为编程助手，同时保持对概念理解和批判性思维的关注。

Method: 采用"Vibe Coding"教学法，在本科高年级NLP课程中实施，学生使用LLM完成7个实验，通过强制提示记录和基于反思问题的评估来确保概念学习。

Result: 19名学生课程反馈显示高满意度（平均分4.4-4.6/5.0），学生重视调试认知负荷的减少，但面临时间限制、LLM输出验证和任务规范不清晰等挑战。

Conclusion: 当通过强制提示记录和反思性评估适当结构化时，LLM辅助学习可以将重点从语法熟练度转向概念掌握，为学生适应AI增强的专业环境做好准备。

Abstract: The rapid advancement of Large Language Models (LLMs) presents both challenges and opportunities for Natural Language Processing (NLP) education. This paper introduces ``Vibe Coding,'' a pedagogical approach that leverages LLMs as coding assistants while maintaining focus on conceptual understanding and critical thinking. We describe the implementation of this approach in a senior-level undergraduate NLP course, where students completed seven labs using LLMs for code generation while being assessed primarily on conceptual understanding through critical reflection questions. Analysis of end-of-course feedback from 19 students reveals high satisfaction (mean scores 4.4-4.6/5.0) across engagement, conceptual learning, and assessment fairness. Students particularly valued the reduced cognitive load from debugging, enabling deeper focus on NLP concepts. However, challenges emerged around time constraints, LLM output verification, and the need for clearer task specifications. Our findings suggest that when properly structured with mandatory prompt logging and reflection-based assessment, LLM-assisted learning can shift focus from syntactic fluency to conceptual mastery, preparing students for an AI-augmented professional landscape.

</details>


### [132] [Breaking the Static Graph: Context-Aware Traversal for Robust Retrieval-Augmented Generation](https://arxiv.org/abs/2602.01965)
*Kwun Hang Lau,Fangyuan Zhang,Boyu Ruan,Yingli Zhou,Qintian Guo,Ruiyuan Zhang,Xiaofang Zhou*

Main category: cs.CL

TL;DR: CatRAG 是一个基于 HippoRAG 2 架构的上下文感知检索增强生成框架，通过动态调整知识图谱的随机游走来解决静态图方法中的"语义漂移"问题，显著提升多跳推理的完整性。


<details>
  <summary>Details</summary>
Motivation: 现有基于知识图谱的 RAG 方法（如 HippoRAG）存在"静态图谬误"，依赖固定的转移概率，忽略了查询相关的边相关性，导致随机游走被高连接度的"枢纽"节点分散，无法完整检索多跳查询所需的证据链。

Method: CatRAG 在 HippoRAG 2 架构基础上，将静态知识图谱转换为查询自适应的导航结构，采用三方面框架：1）符号锚定，注入弱实体约束来正则化随机游走；2）查询感知的动态边权重调整，动态调制图结构以剪枝不相关路径；3）关键事实段落权重增强，通过成本高效的偏置将随机游走锚定到可能的证据。

Result: 在四个多跳基准测试中，CatRAG 持续优于现有最先进基线。虽然标准召回率指标显示适度提升，但 CatRAG 在推理完整性（恢复整个证据路径的能力）方面实现了显著改进。

Conclusion: CatRAG 通过上下文感知的图遍历有效弥合了检索部分上下文与实现完全基于证据的推理之间的差距，解决了静态图方法中的语义漂移问题，提升了多跳推理的完整性。

Abstract: Recent advances in Retrieval-Augmented Generation (RAG) have shifted from simple vector similarity to structure-aware approaches like HippoRAG, which leverage Knowledge Graphs (KGs) and Personalized PageRank (PPR) to capture multi-hop dependencies. However, these methods suffer from a "Static Graph Fallacy": they rely on fixed transition probabilities determined during indexing. This rigidity ignores the query-dependent nature of edge relevance, causing semantic drift where random walks are diverted into high-degree "hub" nodes before reaching critical downstream evidence. Consequently, models often achieve high partial recall but fail to retrieve the complete evidence chain required for multi-hop queries. To address this, we propose CatRAG, Context-Aware Traversal for robust RAG, a framework that builds on the HippoRAG 2 architecture and transforms the static KG into a query-adaptive navigation structure. We introduce a multi-faceted framework to steer the random walk: (1) Symbolic Anchoring, which injects weak entity constraints to regularize the random walk; (2) Query-Aware Dynamic Edge Weighting, which dynamically modulates graph structure, to prune irrelevant paths while amplifying those aligned with the query's intent; and (3) Key-Fact Passage Weight Enhancement, a cost-efficient bias that structurally anchors the random walk to likely evidence. Experiments across four multi-hop benchmarks demonstrate that CatRAG consistently outperforms state of the art baselines. Our analysis reveals that while standard Recall metrics show modest gains, CatRAG achieves substantial improvements in reasoning completeness, the capacity to recover the entire evidence path without gaps. These results reveal that our approach effectively bridges the gap between retrieving partial context and enabling fully grounded reasoning. Resources are available at https://github.com/kwunhang/CatRAG.

</details>


### [133] [Mixture-of-Experts with Intermediate CTC Supervision for Accented Speech Recognition](https://arxiv.org/abs/2602.01967)
*Wonjun Lee,Hyounghun Kim,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: Moe-Ctc：一种混合专家架构，通过中间CTC监督联合促进专家专业化和泛化，显著提升多口音语音识别性能


<details>
  <summary>Details</summary>
Motivation: 当前语音识别模型主要基于少数高资源英语变体训练，对其他口音表现不佳。口音无关方法对重口音或未见口音效果有限，而口音特定方法依赖有限且嘈杂的标签数据。

Method: 提出Moe-Ctc混合专家架构，采用中间CTC监督。训练时使用口音感知路由促进专家捕获口音特定模式，推理时转为无标签路由。每个专家配备独立CTC头，通过路由增强损失稳定优化。

Result: 在Mcv-Accent基准测试中，在低资源和高资源条件下，对已见和未见口音均取得一致提升，相比FastConformer基线实现最高29.3%的相对WER降低。

Conclusion: Moe-Ctc通过联合促进专家专业化和泛化，有效解决了多口音语音识别问题，在保持推理效率的同时显著提升了识别性能。

Abstract: Accented speech remains a persistent challenge for automatic speech recognition (ASR), as most models are trained on data dominated by a few high-resource English varieties, leading to substantial performance degradation for other accents. Accent-agnostic approaches improve robustness yet struggle with heavily accented or unseen varieties, while accent-specific methods rely on limited and often noisy labels. We introduce Moe-Ctc, a Mixture-of-Experts architecture with intermediate CTC supervision that jointly promotes expert specialization and generalization. During training, accent-aware routing encourages experts to capture accent-specific patterns, which gradually transitions to label-free routing for inference. Each expert is equipped with its own CTC head to align routing with transcription quality, and a routing-augmented loss further stabilizes optimization. Experiments on the Mcv-Accent benchmark demonstrate consistent gains across both seen and unseen accents in low- and high-resource conditions, achieving up to 29.3% relative WER reduction over strong FastConformer baselines.

</details>


### [134] [Orthogonal Hierarchical Decomposition for Structure-Aware Table Understanding with Large Language Models](https://arxiv.org/abs/2602.01969)
*Bin Cao,Huixian Lu,Chenwen Ma,Ting Wang,Ruizhe Li,Jing Fan*

Main category: cs.CL

TL;DR: 提出正交层次分解（OHD）框架，通过构建保持结构的复杂表格表示，解决LLM处理多级表头、合并单元格等非标准表格的挑战


<details>
  <summary>Details</summary>
Motivation: 现有表格线性化或规范化网格建模方法难以显式捕捉复杂表格的层次结构和跨维度依赖，导致结构语义与文本表示不对齐

Method: 提出正交层次分解（OHD）框架：1）基于空间-语义共约束的正交树归纳（OTI）方法，将不规则表格分解为列树和行树；2）双路径关联协议对称重构单元格语义谱系；3）使用LLM作为语义仲裁器对齐多级语义信息

Result: 在AITQA和HiTab两个复杂表格问答基准测试中，OHD框架在多项评估指标上持续优于现有表示范式

Conclusion: OHD框架通过正交层次分解有效捕捉复杂表格的垂直和水平层次依赖，为LLM提供结构保持的输入表示，显著提升复杂表格理解能力

Abstract: Complex tables with multi-level headers, merged cells and heterogeneous layouts pose persistent challenges for LLMs in both understanding and reasoning. Existing approaches typically rely on table linearization or normalized grid modeling. However, these representations struggle to explicitly capture hierarchical structures and cross-dimensional dependencies, which can lead to misalignment between structural semantics and textual representations for non-standard tables. To address this issue, we propose an Orthogonal Hierarchical Decomposition (OHD) framework that constructs structure-preserving input representations of complex tables for LLMs. OHD introduces an Orthogonal Tree Induction (OTI) method based on spatial--semantic co-constraints, which decomposes irregular tables into a column tree and a row tree to capture vertical and horizontal hierarchical dependencies, respectively. Building on this representation, we design a dual-pathway association protocol to symmetrically reconstruct semantic lineage of each cell, and incorporate an LLM as a semantic arbitrator to align multi-level semantic information. We evaluate OHD framework on two complex table question answering benchmarks, AITQA and HiTab. Experimental results show that OHD consistently outperforms existing representation paradigms across multiple evaluation metrics.

</details>


### [135] [Beyond Local Edits: Embedding-Virtualized Knowledge for Broader Evaluation and Preservation of Model Editing](https://arxiv.org/abs/2602.01977)
*Shuainan Liu,Xuanang Chen,Ben He,Le Sun*

Main category: cs.CL

TL;DR: 提出EVK方法，通过嵌入空间扰动表征模型知识，构建EVK-Bench评估编辑引起的知识漂移，并开发EVK-Align模块减少知识漂移，提升知识保留而不牺牲编辑准确性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型知识编辑方法主要依赖有限数据集进行评估，无法全面理解编辑对模型知识系统的整体影响，需要更全面的评估方法。

Method: 提出Embedding-Virtualized Knowledge (EVK)，通过在嵌入空间进行受控扰动来表征模型知识；构建EVK-Bench评估基准量化知识漂移；开发EVK-Align模块约束嵌入级知识漂移，可无缝集成到现有编辑方法中。

Result: 实验表明，该方法能实现更全面的评估，同时显著提高知识保留能力，而不牺牲编辑准确性。

Conclusion: EVK方法通过嵌入空间表征知识，提供了超越传统样本评估的更全面视角，EVK-Align模块能有效减少知识漂移，为知识编辑方法提供了更好的评估和改进框架。

Abstract: Knowledge editing methods for large language models are commonly evaluated using predefined benchmarks that assess edited facts together with a limited set of related or neighboring knowledge. While effective, such evaluations remain confined to finite, dataset-bounded samples, leaving the broader impact of editing on the model's knowledge system insufficiently understood. To address this gap, we introduce Embedding-Virtualized Knowledge (EVK) that characterizes model knowledge through controlled perturbations in embedding space, enabling the exploration of a substantially broader and virtualized knowledge region beyond explicit data annotations. Based on EVK, we construct an embedding-level evaluation benchmark EVK-Bench that quantifies potential knowledge drift induced by editing, revealing effects that are not captured by conventional sample-based metrics. Furthermore, we propose a plug-and-play EVK-Align module that constrains embedding-level knowledge drift during editing and can be seamlessly integrated into existing editing methods. Experiments demonstrate that our approach enables more comprehensive evaluation while significantly improving knowledge preservation without sacrificing editing accuracy.

</details>


### [136] [S3-CoT: Self-Sampled Succinct Reasoning Enables Efficient Chain-of-Thought LLMs](https://arxiv.org/abs/2602.01982)
*Yanrui Du,Sendong Zhao,Yibo Gao,Danyang Zhao,Qika Lin,Ming Ma,Jiayun Li,Yi Jiang,Kai He,Qianyi Xu,Bing Qin,Mengling Feng*

Main category: cs.CL

TL;DR: 提出S3-CoT框架，通过自采样激活引导实现高效思维链学习，无需教师指导即可生成风格对齐、可变长度的推理轨迹，解决监督数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 现有思维链方法存在冗余推理过程，需要探索LLM能否像人类系统1推理一样获得快速思考模式，同时解决基于监督微调方法中高质量监督数据稀缺的核心瓶颈。

Method: 基于激活引导的自采样框架，从目标LLM自身诱导风格对齐的可变长度推理轨迹；使用黄金答案过滤数据进行监督微调，构建类人双认知系统和渐进压缩课程；探索仅使用预测一致数据的自进化机制。

Result: 在数学基准测试和医学领域的跨域泛化测试中，该方法为通用和R1风格LLM带来稳定改进，无需黄金答案即可实现高效思维链学习。

Conclusion: S3-CoT框架成功实现了LLM的高效思维链学习，通过自采样和自进化机制解决了监督数据稀缺问题，为LLM开发快速思考模式提供了有效途径。

Abstract: Large language models (LLMs) equipped with chain-of-thought (CoT) achieve strong performance and offer a window into LLM behavior. However, recent evidence suggests that improvements in CoT capabilities often come with redundant reasoning processes, motivating a key question: Can LLMs acquire a fast-thinking mode analogous to human System 1 reasoning? To explore this, our study presents a self-sampling framework based on activation steering for efficient CoT learning. Our method can induce style-aligned and variable-length reasoning traces from target LLMs themselves without any teacher guidance, thereby alleviating a central bottleneck of SFT-based methods-the scarcity of high-quality supervision data. Using filtered data by gold answers, we perform SFT for efficient CoT learning with (i) a human-like dual-cognitive system, and (ii) a progressive compression curriculum. Furthermore, we explore a self-evolution regime in which SFT is driven solely by prediction-consistent data of variable-length variants, eliminating the need for gold answers. Extensive experiments on math benchmarks, together with cross-domain generalization tests in medicine, show that our method yields stable improvements for both general and R1-style LLMs. Our data and model checkpoints can be found at https://github.com/DYR1/S3-CoT.

</details>


### [137] [From Latent Signals to Reflection Behavior: Tracing Meta-Cognitive Activation Trajectory in R1-Style LLMs](https://arxiv.org/abs/2602.01999)
*Yanrui Du,Yibo Gao,Sendong Zhao,Jiayun Li,Haochun Wang,Qika Lin,Kai He,Bing Qin,Mengling Feng*

Main category: cs.CL

TL;DR: 该研究通过分析R1风格LLM的层间激活轨迹，揭示了自我反思行为的内部机制，发现了一个从潜在监控到话语级调控再到显性反思的三阶段结构化进程。


<details>
  <summary>Details</summary>
Motivation: 尽管R1风格LLM的自我反思能力受到广泛关注，但其内部工作机制尚不明确。研究旨在通过追踪反思行为的层间激活轨迹，揭示这种自我反思行为的内部机制。

Method: 使用logit lens技术读取token级语义，追踪反思行为的层间激活轨迹。通过有针对性的干预实验，分析不同阶段之间的因果关系。

Result: 发现了反思行为的三阶段结构化进程：1) 潜在控制层：线性方向编码思考预算语义；2) 语义枢纽层：转折点和总结性线索等话语级线索出现并主导概率分布；3) 行为显性层：反思行为token的似然度开始上升直至被采样。干预实验揭示了这些阶段之间的因果链。

Conclusion: 研究结果表明R1风格LLM的自我反思过程类似于人类的元认知过程，从潜在监控到话语级调控，最终实现显性自我反思。这一发现有助于理解LLM的内部工作机制。

Abstract: R1-style LLMs have attracted growing attention for their capacity for self-reflection, yet the internal mechanisms underlying such behavior remain unclear. To bridge this gap, we anchor on the onset of reflection behavior and trace its layer-wise activation trajectory. Using the logit lens to read out token-level semantics, we uncover a structured progression: (i) Latent-control layers, where an approximate linear direction encodes the semantics of thinking budget; (ii) Semantic-pivot layers, where discourse-level cues, including turning-point and summarization cues, surface and dominate the probability mass; and (iii) Behavior-overt layers, where the likelihood of reflection-behavior tokens begins to rise until they become highly likely to be sampled. Moreover, our targeted interventions uncover a causal chain across these stages: prompt-level semantics modulate the projection of activations along latent-control directions, thereby inducing competition between turning-point and summarization cues in semantic-pivot layers, which in turn regulates the sampling likelihood of reflection-behavior tokens in behavior-overt layers. Collectively, our findings suggest a human-like meta-cognitive process-progressing from latent monitoring, to discourse-level regulation, and to finally overt self-reflection. Our analysis code can be found at https://github.com/DYR1/S3-CoT.

</details>


### [138] [Beyond RAG for Agent Memory: Retrieval by Decoupling and Aggregation](https://arxiv.org/abs/2602.02007)
*Zhanghao Hu,Qinglin Zhu,Hanqi Yan,Yulan He,Lin Gui*

Main category: cs.CL

TL;DR: xMemory提出了一种新的智能体记忆检索系统，通过解耦-聚合方法构建层次化记忆结构，解决了传统RAG在智能体对话场景中检索冗余和丢失时序依赖的问题。


<details>
  <summary>Details</summary>
Motivation: 传统RAG设计用于大规模异构语料库，而智能体记忆是有限、连贯的对话流，具有高度相关的记忆片段和重复内容。固定top-k相似性检索会返回冗余上下文，后处理剪枝可能删除时序关联的先决条件，影响推理准确性。

Method: 提出xMemory系统，采用解耦到聚合的方法：1) 将记忆解耦为语义组件；2) 组织成层次结构；3) 利用该结构驱动检索。通过稀疏性-语义目标指导记忆的分割与合并，构建完整单元的层次结构，并维护可搜索且忠实的高层节点组织。

Result: 在LoCoMo和PerLTQA数据集上，使用三种最新LLM进行实验，xMemory在回答质量和token效率方面均取得一致提升。

Conclusion: 检索应超越相似性匹配，转向基于潜在组件的操作。xMemory通过层次化记忆结构和自上而下的检索策略，能够为多事实查询选择紧凑、多样的主题和语义，仅在减少读者不确定性时扩展到具体情节和原始消息，从而提升智能体记忆检索的效果。

Abstract: Agent memory systems often adopt the standard Retrieval-Augmented Generation (RAG) pipeline, yet its underlying assumptions differ in this setting. RAG targets large, heterogeneous corpora where retrieved passages are diverse, whereas agent memory is a bounded, coherent dialogue stream with highly correlated spans that are often duplicates. Under this shift, fixed top-$k$ similarity retrieval tends to return redundant context, and post-hoc pruning can delete temporally linked prerequisites needed for correct reasoning. We argue retrieval should move beyond similarity matching and instead operate over latent components, following decoupling to aggregation: disentangle memories into semantic components, organise them into a hierarchy, and use this structure to drive retrieval. We propose xMemory, which builds a hierarchy of intact units and maintains a searchable yet faithful high-level node organisation via a sparsity--semantics objective that guides memory split and merge. At inference, xMemory retrieves top-down, selecting a compact, diverse set of themes and semantics for multi-fact queries, and expanding to episodes and raw messages only when it reduces the reader's uncertainty. Experiments on LoCoMo and PerLTQA across the three latest LLMs show consistent gains in answer quality and token efficiency.

</details>


### [139] [NEAT: Neuron-Based Early Exit for Large Reasoning Models](https://arxiv.org/abs/2602.02010)
*Kang Liu,Yongkang Liu,Xiaocui Yang,Peidong Wang,Wen Zhang,Shi Feng,Yifei Zhang,Daling Wang*

Main category: cs.CL

TL;DR: NEAT提出了一种基于神经元激活动态的早期推理退出框架，无需额外训练或测试时计算，通过监测神经元激活模式来减少推理过程中的冗余步骤，实现22%-28%的token减少同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型存在"过度思考"问题，即在已经得出正确解决方案后仍生成冗余推理步骤。现有早期推理退出方法主要依赖输出级启发式或训练过的探测模型，需要额外计算或外部标注数据集。

Method: NEAT通过监测神经元级激活动态，识别与退出相关的神经元并跟踪其在推理过程中的激活模式，动态触发早期退出或抑制反思，无需训练或额外测试时计算。

Result: 在四个推理基准测试和六个不同规模和架构的模型上，NEAT平均实现了22%到28%的token减少，同时保持了准确性。

Conclusion: NEAT提供了一种训练免费、无需额外计算的早期推理退出方法，有效缓解大型推理模型的过度思考问题，显著减少推理成本同时保持性能。

Abstract: Large Reasoning Models (LRMs) often suffer from \emph{overthinking}, a phenomenon in which redundant reasoning steps are generated after a correct solution has already been reached. Existing early reasoning exit methods primarily rely on output-level heuristics or trained probing models to skip redundant reasoning steps, thereby mitigating overthinking. However, these approaches typically require additional rollout computation or externally labeled datasets. In this paper, we propose \textbf{NEAT}, a \textbf{N}euron-based \textbf{E}arly re\textbf{A}soning exi\textbf{T} framework that monitors neuron-level activation dynamics to enable training-free early exits, without introducing additional test-time computation. NEAT identifies exit-associated neurons and tracks their activation patterns during reasoning to dynamically trigger early exit or suppress reflection, thereby reducing unnecessary reasoning while preserving solution quality. Experiments on four reasoning benchmarks across six models with different scales and architectures show that, for each model, NEAT achieves an average token reduction of 22\% to 28\% when averaged over the four benchmarks, while maintaining accuracy.

</details>


### [140] [WildGraphBench: Benchmarking GraphRAG with Wild-Source Corpora](https://arxiv.org/abs/2602.02053)
*Pengyu Wang,Benfeng Xu,Licheng Zhang,Shaohan Wang,Mingxuan Du,Chiwei Zhu,Zhendong Mao*

Main category: cs.CL

TL;DR: WildGraphBench是一个评估GraphRAG在真实场景下性能的基准测试，利用维基百科的长文档和异构参考文献构建，包含1,100个问题，涵盖三种复杂度级别。


<details>
  <summary>Details</summary>
Motivation: 现有GraphRAG基准测试大多使用短篇精选段落作为外部知识，无法充分评估系统在长上下文和大规模异构文档的真实场景中的表现。

Method: 利用维基百科的结构特点，以其外部参考文献作为检索语料库，引用链接的陈述作为真实答案，从12个顶级主题中采样文章，构建包含1,100个问题的基准测试。

Result: 实验表明，当前GraphRAG管道在证据来自中等数量来源时有助于多事实聚合，但这种聚合范式可能过度强调高层陈述而牺牲细节，导致在摘要任务上表现较弱。

Conclusion: 需要开发新的GraphRAG方法，以更好地平衡高层聚合和细粒度细节保留，特别是在处理长文档和异构知识源的真实场景中。

Abstract: Graph-based Retrieval-Augmented Generation (GraphRAG) organizes external knowledge as a hierarchical graph, enabling efficient retrieval and aggregation of scattered evidence across multiple documents. However, many existing benchmarks for GraphRAG rely on short, curated passages as external knowledge, failing to adequately evaluate systems in realistic settings involving long contexts and large-scale heterogeneous documents. To bridge this gap, we introduce WildGraphBench, a benchmark designed to assess GraphRAG performance in the wild. We leverage Wikipedia's unique structure, where cohesive narratives are grounded in long and heterogeneous external reference documents, to construct a benchmark reflecting real-word scenarios. Specifically, we sample articles across 12 top-level topics, using their external references as the retrieval corpus and citation-linked statements as ground truth, resulting in 1,100 questions spanning three levels of complexity: single-fact QA, multi-fact QA, and section-level summarization. Experiments across multiple baselines reveal that current GraphRAG pipelines help on multi-fact aggregation when evidence comes from a moderate number of sources, but this aggregation paradigm may overemphasize high-level statements at the expense of fine-grained details, leading to weaker performance on summarization tasks. Project page:https://github.com/BstWPY/WildGraphBench.

</details>


### [141] [Closing the Loop: Universal Repository Representation with RPG-Encoder](https://arxiv.org/abs/2602.02084)
*Jane Luo,Chengyu Yin,Xin Zhang,Qingtao Li,Steven Liu,Yiming Huang,Jie Wu,Hao Liu,Yangyu Huang,Yu Kang,Fangkai Yang,Ying Xin,Scarlett Li*

Main category: cs.CL

TL;DR: RPG-Encoder：将仓库规划图从静态生成蓝图推广为统一高保真表示，通过编码、增量演化和统一接口实现仓库理解与生成的闭环推理。


<details>
  <summary>Details</summary>
Motivation: 当前仓库代理存在推理断层，因为现有方法依赖孤立的API文档或缺乏语义深度的依赖图。作者认为仓库理解和生成是统一循环中的逆过程：生成将意图扩展为实现，而理解将实现压缩回意图。

Method: 提出RPG-Encoder框架，包含三个机制：(1) 将原始代码编码为结合语义特征和代码依赖的RPG；(2) 增量演化拓扑结构，使维护成本与仓库规模解耦；(3) 作为结构感知导航的统一接口。

Result: 在SWE-bench Verified上达到93.7% Acc@5的SOTA仓库理解性能，在SWE-bench Live Lite上超过最佳基线10%以上。在RepoCraft上实现98.5%的重建覆盖率，验证了RPG的高保真能力。

Conclusion: RPG-Encoder通过统一表示实现了意图与实现之间的闭环，显著提升了复杂代码库的细粒度定位精度，同时大幅降低了维护开销（减少95.7%）。

Abstract: Current repository agents encounter a reasoning disconnect due to fragmented representations, as existing methods rely on isolated API documentation or dependency graphs that lack semantic depth. We consider repository comprehension and generation to be inverse processes within a unified cycle: generation expands intent into implementation, while comprehension compresses implementation back into intent. To address this, we propose RPG-Encoder, a framework that generalizes the Repository Planning Graph (RPG) from a static generative blueprint into a unified, high-fidelity representation. RPG-Encoder closes the reasoning loop through three mechanisms: (1) Encoding raw code into the RPG that combines lifted semantic features with code dependencies; (2) Evolving the topology incrementally to decouple maintenance costs from repository scale, reducing overhead by 95.7%; and (3) Operating as a unified interface for structure-aware navigation. In evaluations, RPG-Encoder establishes state-of-the-art repository understanding on SWE-bench Verified with 93.7% Acc@5 and exceeds the best baseline by over 10% on SWE-bench Live Lite. These results highlight our superior fine-grained localization accuracy in complex codebases. Furthermore, it achieves 98.5% reconstruction coverage on RepoCraft, confirming RPG's high-fidelity capacity to mirror the original codebase and closing the loop between intent and implementation.

</details>


### [142] [LEC-KG: An LLM-Embedding Collaborative Framework for Domain-Specific Knowledge Graph Construction -- A Case Study on SDGs](https://arxiv.org/abs/2602.02090)
*Yikai Zeng,Yingchao Piao,Jianhui Li*

Main category: cs.CL

TL;DR: LEC-KG是一个双向协作框架，将大语言模型的语义理解与知识图谱嵌入的结构推理相结合，用于从非结构化文本构建领域特定知识图谱。


<details>
  <summary>Details</summary>
Motivation: 从非结构化文本构建领域特定知识图谱面临三大挑战：异构实体提及、长尾关系分布以及缺乏标准化模式。现有方法难以同时处理语义理解和结构推理。

Method: 采用双向协作框架，包含三个关键组件：1) 分层粗到细关系提取缓解长尾偏差；2) 证据引导的思维链反馈将结构建议基于源文本；3) 语义初始化实现未见实体的结构验证。两个模块迭代增强：KGE提供结构感知反馈优化LLM提取，已验证三元组逐步改进KGE表示。

Result: 在中文可持续发展目标报告上评估，相比LLM基线有显著改进，特别是在低频关系上表现突出。通过迭代优化，框架能够可靠地将非结构化政策文本转化为已验证的知识图谱三元组。

Conclusion: LEC-KG成功整合了LLM的语义能力和KGE的结构推理，有效解决了领域特定知识图谱构建中的关键挑战，特别是在处理长尾关系和未见实体方面表现出色。

Abstract: Constructing domain-specific knowledge graphs from unstructured text remains challenging due to heterogeneous entity mentions, long-tail relation distributions, and the absence of standardized schemas. We present LEC-KG, a bidirectional collaborative framework that integrates the semantic understanding of Large Language Models (LLMs) with the structural reasoning of Knowledge Graph Embeddings (KGE). Our approach features three key components: (1) hierarchical coarse-to-fine relation extraction that mitigates long-tail bias, (2) evidence-guided Chain-of-Thought feedback that grounds structural suggestions in source text, and (3) semantic initialization that enables structural validation for unseen entities. The two modules enhance each other iteratively-KGE provides structure-aware feedback to refine LLM extractions, while validated triples progressively improve KGE representations. We evaluate LEC-KG on Chinese Sustainable Development Goal (SDG) reports, demonstrating substantial improvements over LLM baselines, particularly on low-frequency relations. Through iterative refinement, our framework reliably transforms unstructured policy text into validated knowledge graph triples.

</details>


### [143] [Think Dense, Not Long: Dynamic Decoupled Conditional Advantage for Efficient Reasoning](https://arxiv.org/abs/2602.02099)
*Keqin Peng,Yuanxin Ouyang,Xuebo Liu,Zhiliang Tian,Ruijian Han,Yancheng Yuan,Liang Ding*

Main category: cs.CL

TL;DR: 提出DDCA方法解决RLVR中长度惩罚导致准确率下降的问题，通过解耦效率与正确性优化，动态调整惩罚强度，在多个数学推理任务上显著减少生成token同时保持或提升准确率。


<details>
  <summary>Details</summary>
Motivation: RLVR虽然能激发多步推理，但会导致过度冗长的输出。简单的长度惩罚在群体相对优化中会严重损害准确率，这源于两个结构性问题：长度基线稀释和难度惩罚不匹配。

Method: 提出动态解耦条件优势（DDCA）：1）在正确响应簇内条件计算长度优势，消除基线稀释；2）使用群体通过率作为难度代理，动态缩放惩罚强度，解耦效率优化与正确性。

Result: 在GSM8K、MATH500、AMC23和AIME25等数学推理基准上，DDCA相比自适应基线持续改进效率-准确率权衡，在简单任务上减少约60%生成token，在困难任务上减少超过20%，同时保持或提升准确率。

Conclusion: DDCA通过解耦效率与正确性优化，有效解决了RLVR中长度惩罚的负面影响，在多个数学推理任务上实现了更好的效率-准确率平衡，为强化学习中的奖励设计提供了新思路。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) can elicit strong multi-step reasoning, yet it often encourages overly verbose traces. Moreover, naive length penalties in group-relative optimization can severely hurt accuracy. We attribute this failure to two structural issues: (i) Dilution of Length Baseline, where incorrect responses (with zero length reward) depress the group baseline and over-penalize correct solutions; and (ii) Difficulty-Penalty Mismatch, where a static penalty cannot adapt to problem difficulty, suppressing necessary reasoning on hard instances while leaving redundancy on easy ones. We propose Dynamic Decoupled Conditional Advantage (DDCA) to decouple efficiency optimization from correctness. DDCA computes length advantages conditionally within the correct-response cluster to eliminate baseline dilution, and dynamically scales the penalty strength using the group pass rate as a proxy for difficulty. Experiments on GSM8K, MATH500, AMC23, and AIME25 show that DDCA consistently improves the efficiency--accuracy trade-off relative to adaptive baselines, reducing generated tokens by approximately 60% on simpler tasks (e.g., GSM8K) versus over 20% on harder benchmarks (e.g., AIME25), thereby maintaining or improving accuracy. Code is available at https://github.com/alphadl/DDCA.

</details>


### [144] [Dicta-LM 3.0: Advancing The Frontier of Hebrew Sovereign LLMs](https://arxiv.org/abs/2602.02104)
*Shaltiel Shmidman,Avi Shmidman,Amir DN Cohen,Moshe Koppel*

Main category: cs.CL

TL;DR: Dicta-LM 3.0 是一个针对希伯来语和英语训练的开源大语言模型集合，包含24B、12B和1.7B三种规模，支持65k上下文长度，并提供了新的希伯来语聊天LLM评估基准。


<details>
  <summary>Details</summary>
Motivation: 目前开源权重的LLM主要由前沿实验室发布，但针对英语以外语言的主权大语言模型供应不足而需求很高。训练希伯来语等低资源语言的大语言模型面临独特挑战，需要专门解决方案。

Method: 基于Mistral-Small-3.1、NVIDIA Nemotron Nano V2和Qwen3-1.7B等基础模型进行适配，训练在大量希伯来语和英语文本语料上。发布基础模型和具有工具调用支持的聊天模型两种变体，所有模型都支持65k原生上下文长度。

Result: 成功开发了Dicta-LM 3.0模型集合，包含三种不同规模的模型。同时创建了新的希伯来语聊天LLM评估基准套件，涵盖翻译、摘要、Winograd、以色列知识问答和希伯来语元音标注等多个任务。

Conclusion: 这项工作不仅解决了在低资源语言中训练LLM的复杂性，还提出了一个可用于将其他LLM适配到各种非英语语言的框架，为多语言NLP领域做出了贡献。

Abstract: Open-weight LLMs have been released by frontier labs; however, sovereign Large Language Models (for languages other than English) remain low in supply yet high in demand. Training large language models (LLMs) for low-resource languages such as Hebrew poses unique challenges. In this paper, we introduce Dicta-LM 3.0: an open-weight collection of LLMs trained on substantially-sized corpora of Hebrew and English texts. The model is released in three sizes: 24B - adapted from the Mistral-Small-3.1 base model, 12B - adapted from the NVIDIA Nemotron Nano V2 model, and 1.7B - adapted from the Qwen3-1.7B base model. We are releasing multiple variants of each model, each with a native context length of 65k tokens; base model and chat model with tool-calling support. To rigorously evaluate our models, we introduce a new benchmark suite for evaluation of Hebrew chat-LLMs, covering a diverse set of tasks including Translation, Summarization, Winograd, Israeli Trivia, and Diacritization (nikud). Our work not only addresses the intricacies of training LLMs in low-resource languages but also proposes a framework that can be leveraged for adapting other LLMs to various non-English languages, contributing to the broader field of multilingual NLP.

</details>


### [145] [Out of the Memory Barrier: A Highly Memory Efficient Training System for LLMs with Million-Token Contexts](https://arxiv.org/abs/2602.02108)
*Wenhao Li,Daohai Yu,Gen Luo,Yuxin Zhang,Fei Chao,Rongrong Ji,Yifan Wu,Jiaxin Liu,Ziyang Gong,Zimu Liao*

Main category: cs.CL

TL;DR: OOMB是一个高效内存训练系统，通过分块循环训练和即时激活重计算实现O(1)激活内存占用，结合KV缓存优化，使Qwen2.5-7B每增加10K上下文仅需10MB内存，可在单H200 GPU上训练4M上下文。


<details>
  <summary>Details</summary>
Motivation: 训练长上下文LLM面临GPU内存瓶颈，主要问题是激活内存随序列长度线性增长，传统方法需要大量GPU集群。

Method: 采用分块循环训练框架和即时激活重计算保持恒定激活内存；集成KV缓存优化：分页内存管理器消除碎片、异步CPU卸载隐藏延迟、页面级稀疏注意力减少计算和通信开销。

Result: Qwen2.5-7B每增加10K上下文仅需10MB内存增长，可在单H200 GPU上训练4M上下文，相比传统上下文并行方法大幅提升资源效率。

Conclusion: OOMB系统显著提升了长上下文LLM训练的资源效率，通过协同优化技术解决了内存瓶颈问题，为长上下文训练提供了实用解决方案。

Abstract: Training Large Language Models (LLMs) on long contexts is severely constrained by prohibitive GPU memory overhead, not training time. The primary culprits are the activations, whose memory footprints scale linearly with sequence length. We introduce OOMB, a highly memory-efficient training system that directly confronts this barrier. Our approach employs a chunk-recurrent training framework with on-the-fly activation recomputation, which maintains a constant activation memory footprint (O(1)) and shifts the primary bottleneck to the growing KV cache. To manage the KV cache, OOMB integrates a suite of synergistic optimizations: a paged memory manager for both the KV cache and its gradients to eliminate fragmentation, asynchronous CPU offloading to hide data transfer latency, and page-level sparse attention to reduce both computational complexity and communication overhead. The synergy of these techniques yields exceptional efficiency. Our empirical results show that for every additional 10K tokens of context, the end-to-end training memory overhead increases by a mere 10MB for Qwen2.5-7B. This allows training Qwen2.5-7B with a 4M-token context on a single H200 GPU, a feat that would otherwise require a large cluster using context parallelism. This work represents a substantial advance in resource efficiency for long-context LLM training. The source code is available at https://github.com/wenhaoli-xmu/OOMB.

</details>


### [146] [There Is More to Refusal in Large Language Models than a Single Direction](https://arxiv.org/abs/2602.02132)
*Faaiz Joad,Majd Hawasly,Sabri Boughorbel,Nadir Durrani,Husrev Taha Sencar*

Main category: cs.CL

TL;DR: 研究发现LLM的拒绝行为由多个几何上不同的激活空间方向控制，而非单一方向，但这些方向都共享一维控制机制，主要影响拒绝方式而非是否拒绝


<details>
  <summary>Details</summary>
Motivation: 先前研究认为大语言模型的拒绝行为由单一激活空间方向介导，但本文旨在验证这一解释是否完整，探索不同拒绝行为在激活空间中的几何结构

Method: 在11个拒绝和非合规类别（包括安全性、不完整请求、拟人化、过度拒绝等）上分析激活空间中的几何方向，研究线性操控这些方向的效果

Result: 发现不同拒绝行为对应几何上不同的激活空间方向，但所有拒绝相关方向的线性操控都产生几乎相同的拒绝-过度拒绝权衡，形成共享的一维控制机制

Conclusion: LLM拒绝行为的激活空间机制比单一方向解释更复杂，包含多个几何不同的方向，但这些方向共享控制机制，主要影响拒绝方式而非是否拒绝

Abstract: Prior work argues that refusal in large language models is mediated by a single activation-space direction, enabling effective steering and ablation. We show that this account is incomplete. Across eleven categories of refusal and non-compliance, including safety, incomplete or unsupported requests, anthropomorphization, and over-refusal, we find that these refusal behaviors correspond to geometrically distinct directions in activation space. Yet despite this diversity, linear steering along any refusal-related direction produces nearly identical refusal to over-refusal trade-offs, acting as a shared one-dimensional control knob. The primary effect of different directions is not whether the model refuses, but how it refuses.

</details>


### [147] [Quantifying the Gap between Understanding and Generation within Unified Multimodal Models](https://arxiv.org/abs/2602.02140)
*Chenlong Wang,Yuhang Chen,Zhihan Hu,Dongping Chen,Wenhu Chen,Sarah Wiegreffe,Tianyi Zhou*

Main category: cs.CL

TL;DR: GapEval是一个双向基准测试，用于量化统一多模态模型中理解与生成能力之间的差距，揭示当前模型仅实现表面统一而非深度认知融合。


<details>
  <summary>Details</summary>
Motivation: 尽管统一多模态模型在理解和生成任务上取得显著进展，但这些能力是否真正对齐和集成在单一模型中尚不清楚。需要研究这两种能力之间的差距和认知一致性。

Method: 提出GapEval双向基准测试，每个问题可以在图像和文本两种模态中回答，对称评估模型的双向推理能力和跨模态一致性。从知识操作角度进行实证研究。

Result: 实验显示不同架构的统一多模态模型在两个方向上都存在持续差距，表明当前模型仅实现表面统一而非深度认知融合。模型内部知识保持分离，跨模态能力涌现和知识不同步。

Conclusion: 当前统一多模态模型的理解和生成能力尚未真正对齐，仅实现表面统一。知识在模型中保持分离状态，跨模态能力不同步，为未来探索指明了方向。

Abstract: Recent advances in unified multimodal models (UMM) have demonstrated remarkable progress in both understanding and generation tasks. However, whether these two capabilities are genuinely aligned and integrated within a single model remains unclear. To investigate this question, we introduce GapEval, a bidirectional benchmark designed to quantify the gap between understanding and generation capabilities, and quantitatively measure the cognitive coherence of the two "unified" directions. Each question can be answered in both modalities (image and text), enabling a symmetric evaluation of a model's bidirectional inference capability and cross-modal consistency. Experiments reveal a persistent gap between the two directions across a wide range of UMMs with different architectures, suggesting that current models achieve only surface-level unification rather than deep cognitive convergence of the two. To further explore the underlying mechanism, we conduct an empirical study from the perspective of knowledge manipulation to illustrate the underlying limitations. Our findings indicate that knowledge within UMMs often remains disjoint. The capability emergence and knowledge across modalities are unsynchronized, paving the way for further exploration.

</details>


### [148] [Focus-dLLM: Accelerating Long-Context Diffusion LLM Inference via Confidence-Guided Context Focusing](https://arxiv.org/abs/2602.02159)
*Lingkun Long,Yushi Huang,Shihao Bai,Ruihao Gong,Jun Zhang,Ao Zhou,Jianlei Yang*

Main category: cs.CL

TL;DR: Focus-dLLM：针对扩散大语言模型的训练免费注意力稀疏化框架，通过过去置信度引导的指示器和汇聚感知剪枝策略，在保持性能的同时实现超过29倍的无损加速


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型(dLLMs)在长上下文处理方面表现出色，但双向全注意力的计算成本限制了推理效率。现有的稀疏注意力方法效果不佳，因为在扩散过程中需要预测尚未解码的token的注意力重要性，而未被掩码的token位置是未知的

Method: 1. 基于token置信度在相邻步骤间强相关的发现，设计过去置信度引导的指示器来预测未被掩码区域；2. 提出汇聚感知剪枝策略，准确估计并移除冗余注意力计算，同时保留高影响力的注意力汇聚点；3. 利用观察到的跨层一致性，在不同层间重用已识别的汇聚位置以减少开销

Result: 在32K上下文长度下，该方法实现了超过29倍的无损加速，代码已公开

Conclusion: Focus-dLLM为长上下文dLLM推理提供了一个准确且高效的训练免费注意力稀疏化框架，显著提升了推理效率

Abstract: Diffusion Large Language Models (dLLMs) deliver strong long-context processing capability in a non-autoregressive decoding paradigm. However, the considerable computational cost of bidirectional full attention limits the inference efficiency. Although sparse attention is promising, existing methods remain ineffective. This stems from the need to estimate attention importance for tokens yet to be decoded, while the unmasked token positions are unknown during diffusion. In this paper, we present Focus-dLLM, a novel training-free attention sparsification framework tailored for accurate and efficient long-context dLLM inference. Based on the finding that token confidence strongly correlates across adjacent steps, we first design a past confidence-guided indicator to predict unmasked regions. Built upon this, we propose a sink-aware pruning strategy to accurately estimate and remove redundant attention computation, while preserving highly influential attention sinks. To further reduce overhead, this strategy reuses identified sink locations across layers, leveraging the observed cross-layer consistency. Experimental results show that our method offers more than $29\times$ lossless speedup under $32K$ context length. The code is publicly available at: https://github.com/Longxmas/Focus-dLLM

</details>


### [149] [D-CORE: Incentivizing Task Decomposition in Large Reasoning Models for Complex Tool Use](https://arxiv.org/abs/2602.02160)
*Bowen Xu,Shaoyu Wu,Hao Jiang,Kai Liu,Xin Chen,Lulu Hu,Bin Yang*

Main category: cs.CL

TL;DR: D-CORE是一个两阶段训练框架，通过任务分解推理能力激励和多样性感知强化学习，解决大型推理模型在复杂工具使用场景中的懒惰推理问题，显著提升工具使用性能。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型在复杂工具使用场景中缺乏子任务分解能力，导致"懒惰推理"问题，无法有效处理复杂的现实世界问题。

Method: 提出D-CORE两阶段训练框架：1）通过自我蒸馏激励模型的任务分解推理能力；2）使用多样性感知强化学习恢复模型的反思推理能力。

Result: D-CORE在不同基准测试和模型规模上都实现了稳健的工具使用改进：D-CORE-8B在BFCLv3上达到77.7%准确率，超越最佳8B模型5.7%；D-CORE-14B达到79.3%的新SOTA，性能超越70B模型但参数量仅为1/5。

Conclusion: D-CORE通过解决任务分解和反思推理问题，显著提升了大型推理模型的工具使用能力，为复杂现实世界问题的解决提供了有效方法。

Abstract: Effective tool use and reasoning are essential capabilities for large reasoning models~(LRMs) to address complex real-world problems. Through empirical analysis, we identify that current LRMs lack the capability of sub-task decomposition in complex tool use scenarios, leading to Lazy Reasoning. To address this, we propose a two-stage training framework D-CORE~(\underline{\textbf{D}}ecomposing tasks and \underline{\textbf{Co}}mposing \underline{\textbf{Re}}asoning processes) that first incentivize the LRMs' task decomposition reasoning capability via self-distillation, followed by diversity-aware reinforcement learning~(RL) to restore LRMs' reflective reasoning capability. D-CORE achieves robust tool-use improvements across diverse benchmarks and model scales. Experiments on BFCLv3 demonstrate superiority of our method: D-CORE-8B reaches 77.7\% accuracy, surpassing the best-performing 8B model by 5.7\%. Meanwhile, D-CORE-14B establishes a new state-of-the-art at 79.3\%, outperforming 70B models despite being 5$\times$ smaller. The source code is available at https://github.com/alibaba/EfficientAI.

</details>


### [150] [AR-MAP: Are Autoregressive Large Language Models Implicit Teachers for Diffusion Large Language Models?](https://arxiv.org/abs/2602.02178)
*Liang Lin,Feng Xiong,Zengbin Wang,Kun Wang,Junhao Dong,Xuecai Hu,Yong Wang,Xiangxiang Chu*

Main category: cs.CL

TL;DR: AR-MAP：一种利用对齐好的自回归LLM作为隐式教师，通过权重缩放将偏好对齐知识迁移到扩散LLM的新框架，避免了直接对齐扩散LLM的高方差和计算开销。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型（DLLMs）作为自回归模型的有力替代方案，支持并行token生成，但其偏好对齐面临挑战，主要原因是基于ELBO的似然估计会引入高方差。

Method: 提出AR-MAP框架，利用已对齐的自回归LLMs作为隐式教师，通过简单的权重缩放将偏好对齐知识迁移到DLLMs，利用两种模型架构的共享结构，避免了直接对齐DLLM的高方差问题。

Result: 在多种偏好对齐任务上的实验表明，AR-MAP相比现有的DLLM特定对齐方法，取得了竞争性或更优的性能，在所有任务和模型上平均得分达到69.08%。

Conclusion: AR-MAP提供了一种高效、低方差的DLLM偏好对齐方法，通过利用自回归LLMs作为教师，成功实现了跨生成范式的知识迁移，为DLLM对齐开辟了新途径。

Abstract: Diffusion Large Language Models (DLLMs) have emerged as a powerful alternative to autoregressive models, enabling parallel token generation across multiple positions. However, preference alignment of DLLMs remains challenging due to high variance introduced by Evidence Lower Bound (ELBO)-based likelihood estimation. In this work, we propose AR-MAP, a novel transfer learning framework that leverages preference-aligned autoregressive LLMs (AR-LLMs) as implicit teachers for DLLM alignment. We reveal that DLLMs can effectively absorb alignment knowledge from AR-LLMs through simple weight scaling, exploiting the shared architectural structure between these divergent generation paradigms. Crucially, our approach circumvents the high variance and computational overhead of direct DLLM alignment and comprehensive experiments across diverse preference alignment tasks demonstrate that AR-MAP achieves competitive or superior performance compared to existing DLLM-specific alignment methods, achieving 69.08\% average score across all tasks and models. Our Code is available at https://github.com/AMAP-ML/AR-MAP.

</details>


### [151] [Evaluating Metalinguistic Knowledge in Large Language Models across the World's Languages](https://arxiv.org/abs/2602.02182)
*Tjaša Arčon,Matej Klemen,Marko Robnik-Šikonja,Kaja Dobrovoljc*

Main category: cs.CL

TL;DR: LLMs的元语言知识有限，受数据可用性影响而非普遍语法能力，高资源语言表现更好但整体仅中等准确率。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在语言使用任务上被常规评估，但其对语言结构的理解（元语言知识）仍不清楚。现有语言基准通常关注狭窄现象、强调高资源语言，很少评估元语言知识（对语言结构的显式推理而非语言使用）。

Method: 使用准确率和宏观F1分数，结合多数类和随机基线，分析整体性能并检查语言领域和语言相关因素的变化。创建并发布开源基准数据集以支持系统评估。

Result: 当前LLMs的元语言知识有限：GPT-4o表现最佳但仅达到中等准确率（0.367），开源模型落后。所有模型表现高于随机但未能超越多数类基线，表明它们捕捉跨语言模式但缺乏细粒度语法区分。性能在语言领域间变化，词汇特征准确率最高，语音特征最低，部分反映在线可见性差异。语言层面准确率与数字语言状态强相关：数字存在和资源可用性高的语言评估更准确，低资源语言表现显著较低。资源相关指标（维基百科规模、语料库可用性）比地理、谱系或社会语言因素更能预测准确率。

Conclusion: LLMs的元语言知识是碎片化的，由数据可用性塑造而非跨世界语言的普遍语法能力。需要未来LLMs中更大的全球语言多样性。

Abstract: Large language models (LLMs) are routinely evaluated on language use tasks, yet their knowledge of linguistic structure remains poorly understood. Existing linguistic benchmarks typically focus on narrow phenomena, emphasize high-resource languages, and rarely evaluate metalinguistic knowledge-explicit reasoning about language structure rather than language use. Using accuracy and macro F1, together with majority-class and chance baselines, we analyse overall performance and examine variation by linguistic domains and language-related factors. Our results show that metalinguistic knowledge in current LLMs is limited: GPT-4o performs best but achieves only moderate accuracy (0.367), while open-source models lag behind. All models perform above chance but fail to outperform the majority-class baseline, suggesting they capture cross-linguistic patterns but lack fine-grained grammatical distinctions. Performance varies across linguistic domains, with lexical features showing the highest accuracy and phonological features among the lowest, partially reflecting differences in online visibility. At the language level, accuracy shows a strong association with digital language status: languages with higher digital presence and resource availability are evaluated more accurately, while low-resource languages show substantially lower performance. Analyses of predictive factors confirm that resource-related indicators (Wikipedia size, corpus availability) are more informative predictors of accuracy than geographical, genealogical, or sociolinguistic factors. Together, these results suggest that LLMs' metalinguistic knowledge is fragmented and shaped by data availability rather than generalizable grammatical competence across the world's languages. We release our benchmark as an open-source dataset to support systematic evaluation and encourage greater global linguistic diversity in future LLMs.

</details>


### [152] [Sinhala Physical Common Sense Reasoning Dataset for Global PIQA](https://arxiv.org/abs/2602.02207)
*Nisansa de Silva,Surangika Ranathunga*

Main category: cs.CL

TL;DR: 首个僧伽罗语物理常识推理数据集，包含110个人工创建验证的样本，每个样本包含提示、正确答案和错误答案，主要涉及斯里兰卡语境。


<details>
  <summary>Details</summary>
Motivation: 为僧伽罗语创建物理常识推理数据集，填补该语言在AI推理任务中的空白，支持斯里兰卡语境下的AI应用开发。

Method: 作为Global PIQA项目的一部分，人工创建并验证110个数据样本，每个样本包含提示、正确答案和错误答案，问题主要基于斯里兰卡语境。

Result: 成功创建了首个僧伽罗语物理常识推理数据集，包含110个高质量样本，为僧伽罗语AI模型训练和评估提供了重要资源。

Conclusion: 该数据集填补了僧伽罗语物理常识推理的空白，为斯里兰卡语境下的AI应用开发奠定了基础，是Global PIQA项目的重要贡献。

Abstract: This paper presents the first-ever Sinhala physical common sense reasoning dataset created as part of Global PIQA. It contains 110 human-created and verified data samples, where each sample consists of a prompt, the corresponding correct answer, and a wrong answer. Most of the questions refer to the Sri Lankan context, where Sinhala is an official language.

</details>


### [153] [Towards AI Evaluation in Domain-Specific RAG Systems: The AgriHubi Case Study](https://arxiv.org/abs/2602.02208)
*Md. Toufique Hasan,Ayman Asad Khan,Mika Saari,Vaishnavi Bankhele,Pekka Abrahamsson*

Main category: cs.CL

TL;DR: AgriHubi是一个针对芬兰语农业决策支持的领域自适应检索增强生成(RAG)系统，通过整合芬兰农业文档和开放模型，结合显式来源标注与用户反馈，显著提升了答案完整性、语言准确性和可靠性感知。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在知识密集型领域有潜力，但在农业应用中受到限制：缺乏领域基础、英语中心训练数据、现实评估不足。这些问题在低资源语言（如芬兰语）中更加突出，尽管存在高质量领域文档，但通用模型难以有效访问。

Method: 开发AgriHubi系统：整合芬兰农业文档与开放PORO系列模型，采用检索增强生成(RAG)架构，结合显式来源标注和用户反馈机制，支持迭代优化。经过8次迭代开发，通过两个用户研究进行评估。

Result: 系统在答案完整性、语言准确性和可靠性感知方面有明显提升。研究还揭示了部署较大模型时响应质量与延迟之间的实际权衡。为低资源语言环境下设计评估领域特定RAG系统提供了实证指导。

Conclusion: 该研究展示了领域自适应RAG系统在低资源语言农业决策支持中的有效性，为类似场景的系统设计和评估提供了实用指导，特别是在平衡模型性能与部署成本方面。

Abstract: Large language models show promise for knowledge-intensive domains, yet their use in agriculture is constrained by weak grounding, English-centric training data, and limited real-world evaluation. These issues are amplified for low-resource languages, where high-quality domain documentation exists but remains difficult to access through general-purpose models. This paper presents AgriHubi, a domain-adapted retrieval-augmented generation (RAG) system for Finnish-language agricultural decision support. AgriHubi integrates Finnish agricultural documents with open PORO family models and combines explicit source grounding with user feedback to support iterative refinement. Developed over eight iterations and evaluated through two user studies, the system shows clear gains in answer completeness, linguistic accuracy, and perceived reliability. The results also reveal practical trade-offs between response quality and latency when deploying larger models. This study provides empirical guidance for designing and evaluating domain-specific RAG systems in low-resource language settings.

</details>


### [154] [Am I More Pointwise or Pairwise? Revealing Position Bias in Rubric-Based LLM-as-a-Judge](https://arxiv.org/abs/2602.02219)
*Yuzheng Xu,Tosho Hirasawa,Tadashi Kozuno,Yoshitaka Ushiku*

Main category: cs.CL

TL;DR: 研究发现基于评分标准的LLM评估存在位置偏差，提出平衡排列策略来缓解偏差并提高与人类评估的相关性


<details>
  <summary>Details</summary>
Motivation: 当前LLM-as-a-judge研究主要关注点对点和成对评估范式，而基于评分标准的评估（LLM从多个评分标准中选择分数）较少被分析。作者发现这种评估存在位置偏差问题。

Method: 通过跨多个模型和数据集的受控实验证明位置偏差的存在，提出平衡排列策略，将每个评分选项均匀分布在不同的位置上，并通过聚合平衡排列后的分数来缓解偏差。

Result: 实验显示基于评分标准的LLM评估存在一致的位置偏差，平衡排列策略不仅能揭示潜在的位置偏差，还能提高LLM-as-a-Judge与人类评估之间的相关性。

Conclusion: 基于评分标准的LLM-as-a-Judge本质上不是点对点评估，简单的基于排列的校准可以显著提高其可靠性，位置偏差是影响评估质量的重要因素。

Abstract: Large language models (LLMs) are now widely used to evaluate the quality of text, a field commonly referred to as LLM-as-a-judge. While prior works mainly focus on point-wise and pair-wise evaluation paradigms. Rubric-based evaluation, where LLMs select a score from multiple rubrics, has received less analysis. In this work, we show that rubric-based evaluation implicitly resembles a multi-choice setting and therefore has position bias: LLMs prefer score options appearing at specific positions in the rubric list. Through controlled experiments across multiple models and datasets, we demonstrate consistent position bias. To mitigate this bias, we propose a balanced permutation strategy that evenly distributes each score option across positions. We show that aggregating scores across balanced permutations not only reveals latent position bias, but also improves correlation between the LLM-as-a-Judge and human. Our results suggest that rubric-based LLM-as-a-Judge is not inherently point-wise and that simple permutation-based calibration can substantially improve its reliability.

</details>


### [155] [Using Correspondence Patterns to Identify Irregular Words in Cognate sets Through Leave-One-Out Validation](https://arxiv.org/abs/2602.02221)
*Frederic Blum,Johann-Mattis List*

Main category: cs.CL

TL;DR: 提出了一种新的历史语言学定量方法，通过平衡平均对应模式重复率来衡量语音对应规律性，并开发了识别不规则同源词集的算法，在真实数据上达到85%的准确率。


<details>
  <summary>Details</summary>
Motivation: 历史语言学中语音对应规律性主要依赖直觉判断而非量化评估，且不规则现象比新语法学派模型预期的更常见。随着计算历史语言学的发展和标准化词汇数据的增加，需要改进工作流程并提供定量评估方法。

Method: 提出了平衡平均对应模式重复率作为新的规律性度量指标，并开发了一种计算方法来识别对应模式缺乏规律性的同源词集。通过模拟数据和真实数据的实验验证，采用留一法交叉验证来评估方法性能。

Result: 在基于真实数据的实验中，该方法整体准确率达到85%。研究还展示了使用大数据集子样本的优势，以及数据中不规则性增加对结果的影响。

Conclusion: 新的规律性度量和基于此的不规则同源词识别方法在提高现有和未来计算机辅助语言比较数据集质量方面具有重要潜力。

Abstract: Regular sound correspondences constitute the principal evidence in historical language comparison. Despite the heuristic focus on regularity, it is often more an intuitive judgement than a quantified evaluation, and irregularity is more common than expected from the Neogrammarian model. Given the recent progress of computational methods in historical linguistics and the increased availability of standardized lexical data, we are now able to improve our workflows and provide such a quantitative evaluation. Here, we present the balanced average recurrence of correspondence patterns as a new measure of regularity. We also present a new computational method that uses this measure to identify cognate sets that lack regularity with respect to their correspondence patterns. We validate the method through two experiments, using simulated and real data. In the experiments, we employ leave-one-out validation to measure the regularity of cognate sets in which one word form has been replaced by an irregular one, checking how well our method identifies the forms causing the irregularity. Our method achieves an overall accuracy of 85\% with the datasets based on real data. We also show the benefits of working with subsamples of large datasets and how increasing irregularity in the data influences our results. Reflecting on the broader potential of our new regularity measure and the irregular cognate identification method based on it, we conclude that they could play an important role in improving the quality of existing and future datasets in computer-assisted language comparison.

</details>


### [156] [OpenSeal: Good, Fast, and Cheap Construction of an Open-Source Southeast Asian LLM via Parallel Data](https://arxiv.org/abs/2602.02266)
*Tan Sang Nguyen,Muhammad Reza Qorib,Hwee Tou Ng*

Main category: cs.CL

TL;DR: OpenSeal是首个真正开源的东南亚语言大模型，仅使用34.7B平行数据在8x H200 GPU上训练180小时，性能媲美同类模型


<details>
  <summary>Details</summary>
Motivation: 现有LLM多为英语中心，在低资源语言上表现不佳；虽然已有东南亚语言模型，但都不是真正开源（未公开训练数据）。真正开源模型对透明度、理解模型内部机制（偏见、泛化、多语言性）至关重要

Method: 通过受控实验研究平行数据在LLM持续预训练中的有效性，发现仅使用平行数据是最有效的扩展新语言方法。使用34.7B tokens平行数据，在8x NVIDIA H200 GPU上训练180小时

Result: 开发了OpenSeal——首个真正开源的东南亚语言LLM，性能与现有相似规模模型相当

Conclusion: 平行数据是扩展LLM到新语言的最有效方法；OpenSeal为东南亚语言提供了真正开源、透明的解决方案，促进了LLM内部机制的研究

Abstract: Large language models (LLMs) have proven to be effective tools for a wide range of natural language processing (NLP) applications. Although many LLMs are multilingual, most remain English-centric and perform poorly on low-resource languages. Recently, several Southeast Asia-focused LLMs have been developed, but none are truly open source, as they do not publicly disclose their training data. Truly open-source models are important for transparency and for enabling a deeper and more precise understanding of LLM internals and development, including biases, generalization, and multilinguality. Motivated by recent advances demonstrating the effectiveness of parallel data in improving multilingual performance, we conduct controlled and comprehensive experiments to study the effectiveness of parallel data in continual pretraining of LLMs. Our findings show that using only parallel data is the most effective way to extend an LLM to new languages. Using just 34.7B tokens of parallel data and 180 hours on 8x NVIDIA H200 GPUs, we built OpenSeal, the first truly open Southeast Asian LLM that rivals the performance of existing models of similar size.

</details>


### [157] [dziribot: rag based intelligent conversational agent for algerian arabic dialect](https://arxiv.org/abs/2602.02270)
*El Batoul Bechiri,Dihia Lanasri*

Main category: cs.CL

TL;DR: DziriBOT是一个针对阿尔及利亚方言Darja设计的混合智能对话代理，通过多层架构结合NLU和RAG技术，解决了方言非标准化拼写、法阿语码转换等挑战，在低资源环境下实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 阿尔及利亚客户服务数字化需求增长，但Darja方言存在非标准化拼写、与法语大量码转换、阿拉伯语和拉丁字母混用等复杂语言特征，传统对话系统难以处理这些挑战。

Method: 提出多层架构整合专业NLU和检索增强生成(RAG)，评估三种方法：稀疏特征Rasa管道、经典机器学习基线、基于Transformer的微调，最终采用微调的DziriBERT模型。

Result: 微调的DziriBERT模型达到最先进性能，显著优于传统基线，特别是在处理拼写噪声和罕见意图方面表现优异。

Conclusion: DziriBOT提供了连接正式语言模型与阿尔及利亚用户语言现实的稳健可扩展解决方案，为区域市场的方言感知自动化提供了蓝图。

Abstract: The rapid digitalization of customer service has intensified the demand for conversational agents capable of providing accurate and natural interactions. In the Algerian context, this is complicated by the linguistic complexity of Darja, a dialect characterized by non-standardized orthography, extensive code-switching with French, and the simultaneous use of Arabic and Latin (Arabizi) scripts. This paper introduces DziriBOT, a hybrid intelligent conversational agent specifically engineered to overcome these challenges. We propose a multi-layered architecture that integrates specialized Natural Language Understanding (NLU) with Retrieval-Augmented Generation (RAG), allowing for both structured service flows and dynamic, knowledge-intensive responses grounded in curated enterprise documentation. To address the low-resource nature of Darja, we systematically evaluate three distinct approaches: a sparse-feature Rasa pipeline, classical machine learning baselines, and transformer-based fine-tuning. Our experimental results demonstrate that the fine-tuned DziriBERT model achieves state-of-the-art performance. These results significantly outperform traditional baselines, particularly in handling orthographic noise and rare intents. Ultimately, DziriBOT provides a robust, scalable solution that bridges the gap between formal language models and the linguistic realities of Algerian users, offering a blueprint for dialect-aware automation in the regional market.

</details>


### [158] [Kimi K2.5: Visual Agentic Intelligence](https://arxiv.org/abs/2602.02276)
*Kimi Team,Tongtong Bai,Yifan Bai,Yiping Bao,S. H. Cai,Yuan Cao,Y. Charles,H. S. Che,Cheng Chen,Guanduo Chen,Huarong Chen,Jia Chen,Jiahao Chen,Jianlong Chen,Jun Chen,Kefan Chen,Liang Chen,Ruijue Chen,Xinhao Chen,Yanru Chen,Yanxu Chen,Yicun Chen,Yimin Chen,Yingjiang Chen,Yuankun Chen,Yujie Chen,Yutian Chen,Zhirong Chen,Ziwei Chen,Dazhi Cheng,Minghan Chu,Jialei Cui,Jiaqi Deng,Muxi Diao,Hao Ding,Mengfan Dong,Mengnan Dong,Yuxin Dong,Yuhao Dong,Angang Du,Chenzhuang Du,Dikang Du,Lingxiao Du,Yulun Du,Yu Fan,Shengjun Fang,Qiulin Feng,Yichen Feng,Garimugai Fu,Kelin Fu,Hongcheng Gao,Tong Gao,Yuyao Ge,Shangyi Geng,Chengyang Gong,Xiaochen Gong,Zhuoma Gongque,Qizheng Gu,Xinran Gu,Yicheng Gu,Longyu Guan,Yuanying Guo,Xiaoru Hao,Weiran He,Wenyang He,Yunjia He,Chao Hong,Hao Hu,Jiaxi Hu,Yangyang Hu,Zhenxing Hu,Ke Huang,Ruiyuan Huang,Weixiao Huang,Zhiqi Huang,Tao Jiang,Zhejun Jiang,Xinyi Jin,Yu Jing,Guokun Lai,Aidi Li,C. Li,Cheng Li,Fang Li,Guanghe Li,Guanyu Li,Haitao Li,Haoyang Li,Jia Li,Jingwei Li,Junxiong Li,Lincan Li,Mo Li,Weihong Li,Wentao Li,Xinhang Li,Xinhao Li,Yang Li,Yanhao Li,Yiwei Li,Yuxiao Li,Zhaowei Li,Zheming Li,Weilong Liao,Jiawei Lin,Xiaohan Lin,Zhishan Lin,Zichao Lin,Cheng Liu,Chenyu Liu,Hongzhang Liu,Liang Liu,Shaowei Liu,Shudong Liu,Shuran Liu,Tianwei Liu,Tianyu Liu,Weizhou Liu,Xiangyan Liu,Yangyang Liu,Yanming Liu,Yibo Liu,Yuanxin Liu,Yue Liu,Zhengying Liu,Zhongnuo Liu,Enzhe Lu,Haoyu Lu,Zhiyuan Lu,Junyu Luo,Tongxu Luo,Yashuo Luo,Long Ma,Yingwei Ma,Shaoguang Mao,Yuan Mei,Xin Men,Fanqing Meng,Zhiyong Meng,Yibo Miao,Minqing Ni,Kun Ouyang,Siyuan Pan,Bo Pang,Yuchao Qian,Ruoyu Qin,Zeyu Qin,Jiezhong Qiu,Bowen Qu,Zeyu Shang,Youbo Shao,Tianxiao Shen,Zhennan Shen,Juanfeng Shi,Lidong Shi,Shengyuan Shi,Feifan Song,Pengwei Song,Tianhui Song,Xiaoxi Song,Hongjin Su,Jianlin Su,Zhaochen Su,Lin Sui,Jinsong Sun,Junyao Sun,Tongyu Sun,Flood Sung,Yunpeng Tai,Chuning Tang,Heyi Tang,Xiaojuan Tang,Zhengyang Tang,Jiawen Tao,Shiyuan Teng,Chaoran Tian,Pengfei Tian,Ao Wang,Bowen Wang,Chensi Wang,Chuang Wang,Congcong Wang,Dingkun Wang,Dinglu Wang,Dongliang Wang,Feng Wang,Hailong Wang,Haiming Wang,Hengzhi Wang,Huaqing Wang,Hui Wang,Jiahao Wang,Jinhong Wang,Jiuzheng Wang,Kaixin Wang,Linian Wang,Qibin Wang,Shengjie Wang,Shuyi Wang,Si Wang,Wei Wang,Xiaochen Wang,Xinyuan Wang,Yao Wang,Yejie Wang,Yipu Wang,Yiqin Wang,Yucheng Wang,Yuzhi Wang,Zhaoji Wang,Zhaowei Wang,Zhengtao Wang,Zhexu Wang,Zihan Wang,Zizhe Wang,Chu Wei,Ming Wei,Chuan Wen,Zichen Wen,Chengjie Wu,Haoning Wu,Junyan Wu,Rucong Wu,Wenhao Wu,Yuefeng Wu,Yuhao Wu,Yuxin Wu,Zijian Wu,Chenjun Xiao,Jin Xie,Xiaotong Xie,Yuchong Xie,Yifei Xin,Bowei Xing,Boyu Xu,Jianfan Xu,Jing Xu,Jinjing Xu,L. H. Xu,Lin Xu,Suting Xu,Weixin Xu,Xinbo Xu,Xinran Xu,Yangchuan Xu,Yichang Xu,Yuemeng Xu,Zelai Xu,Ziyao Xu,Junjie Yan,Yuzi Yan,Guangyao Yang,Hao Yang,Junwei Yang,Kai Yang,Ningyuan Yang,Ruihan Yang,Xiaofei Yang,Xinlong Yang,Ying Yang,Yi Yang,Yi Yang,Zhen Yang,Zhilin Yang,Zonghan Yang,Haotian Yao,Dan Ye,Wenjie Ye,Zhuorui Ye,Bohong Yin,Chengzhen Yu,Longhui Yu,Tao Yu,Tianxiang Yu,Enming Yuan,Mengjie Yuan,Xiaokun Yuan,Yang Yue,Weihao Zeng,Dunyuan Zha,Haobing Zhan,Dehao Zhang,Hao Zhang,Jin Zhang,Puqi Zhang,Qiao Zhang,Rui Zhang,Xiaobin Zhang,Y. Zhang,Yadong Zhang,Yangkun Zhang,Yichi Zhang,Yizhi Zhang,Yongting Zhang,Yu Zhang,Yushun Zhang,Yutao Zhang,Yutong Zhang,Zheng Zhang,Chenguang Zhao,Feifan Zhao,Jinxiang Zhao,Shuai Zhao,Xiangyu Zhao,Yikai Zhao,Zijia Zhao,Huabin Zheng,Ruihan Zheng,Shaojie Zheng,Tengyang Zheng,Junfeng Zhong,Longguang Zhong,Weiming Zhong,M. Zhou,Runjie Zhou,Xinyu Zhou,Zaida Zhou,Jinguo Zhu,Liya Zhu,Xinhao Zhu,Yuxuan Zhu,Zhen Zhu,Jingze Zhuang,Weiyu Zhuang,Ying Zou,Xinxing Zu*

Main category: cs.CL

TL;DR: Kimi K2.5是一个开源的多模态智能体模型，通过联合优化文本和视觉模态，并引入Agent Swarm并行智能体编排框架，在编码、视觉、推理等任务上取得SOTA性能，同时显著降低延迟。


<details>
  <summary>Details</summary>
Motivation: 推动通用智能体智能的发展，通过多模态联合优化提升智能体能力，并解决复杂任务处理的效率问题。

Method: 1. 联合文本-视觉预训练；2. 零视觉监督微调；3. 联合文本-视觉强化学习；4. Agent Swarm自导向并行智能体编排框架，动态分解复杂任务为异构子问题并并行执行。

Result: 在编码、视觉、推理和智能体任务上取得最先进结果；Agent Swarm相比单智能体基线降低延迟达4.5倍。

Conclusion: Kimi K2.5通过多模态联合优化和并行智能体编排，显著提升了智能体性能和效率，开源模型检查点将促进智能体智能的研究和应用。

Abstract: We introduce Kimi K2.5, an open-source multimodal agentic model designed to advance general agentic intelligence. K2.5 emphasizes the joint optimization of text and vision so that two modalities enhance each other. This includes a series of techniques such as joint text-vision pre-training, zero-vision SFT, and joint text-vision reinforcement learning. Building on this multimodal foundation, K2.5 introduces Agent Swarm, a self-directed parallel agent orchestration framework that dynamically decomposes complex tasks into heterogeneous sub-problems and executes them concurrently. Extensive evaluations show that Kimi K2.5 achieves state-of-the-art results across various domains including coding, vision, reasoning, and agentic tasks. Agent Swarm also reduces latency by up to $4.5\times$ over single-agent baselines. We release the post-trained Kimi K2.5 model checkpoint to facilitate future research and real-world applications of agentic intelligence.

</details>


### [159] [Cross-Lingual Stability of LLM Judges Under Controlled Generation: Evidence from Finno-Ugric Languages](https://arxiv.org/abs/2602.02287)
*Isaac Chung,Linda Freienthal*

Main category: cs.CL

TL;DR: 研究发现跨语言评估中自动指标和LLM评分在不同语言间存在系统性不稳定性，特别是语用判断方面，表明零-shot评估在形态丰富的语言中不可靠。


<details>
  <summary>Details</summary>
Motivation: 当前跨语言大语言模型评估通常混淆了模型性能差异和测量不稳定性两个因素，需要研究在相同生成条件下不同语言间的评估可靠性。

Method: 使用爱沙尼亚语、芬兰语和匈牙利语三种形态丰富的芬兰-乌戈尔语系语言，在相同参数下生成合成客服对话，比较自动指标和LLM-as-a-judge评分在不同语言间的稳定性。

Result: 表面指标（词汇多样性、表面和语义相似性）保持跨语言稳定性，但语用判断（连贯性、指令遵循）出现排名反转和接近零的相关性，表明评估方法在不同语言间行为不一致。

Conclusion: 零-shot评估在形态丰富语言的语篇层面评估不可靠，需要针对特定语言进行校准，并建议在部署前使用受控生成协议作为诊断工具。

Abstract: Cross-lingual evaluation of large language models (LLMs) typically conflates two sources of variance: genuine model performance differences and measurement instability. We investigate evaluation reliability by holding generation conditions constant while varying target language. Using synthetic customer-support dialogues generated with identical parameters across Estonian, Finnish, and Hungarian, we test whether automatic metrics and LLM-as-a-judge scoring produce stable model rankings across these morphologically rich, related Finno-Ugric languages. With a small set of Estonian native speaker annotations as a reference point, we find systematic ranking instabilities: surface-level metrics (lexical diversity, surface and semantic similarity) maintain cross-language stability, but pragmatic judgments (coherence, instruction-following) exhibit rank inversions and near-zero correlations. Because generation is controlled, these inconsistencies reflect how judge scoring behaves differently across languages rather than true model differences.
  This controlled design provides a diagnostic probe: evaluation methods that fail to maintain stability under identical generation conditions signal transfer failure before deployment. Our findings suggest that zero-shot judge transfer is unreliable for discourse-level assessment in morphologically rich languages, motivating language-specific calibration against targeted human baselines. We release our controlled generation protocol, synthetic data, and evaluation framework to enable replication across language families at https://github.com/isaac-chung/cross-lingual-stability-judges.

</details>


### [160] [Hallucination or Creativity: How to Evaluate AI-Generated Scientific Stories?](https://arxiv.org/abs/2602.02290)
*Alex Argese,Pasquale Lisena,Raphaël Troncy*

Main category: cs.CL

TL;DR: StoryScore：一个评估AI生成科学故事的综合指标，整合语义对齐、词汇基础、叙事控制、结构保真度、冗余避免和实体级幻觉检测


<details>
  <summary>Details</summary>
Motivation: 生成式AI能将科学文章转化为适合不同受众的叙事，但评估这些故事具有挑战性。传统摘要指标无法捕捉叙事所需的抽象、简化和教学创造力，而幻觉检测器又经常误判合法的叙事重构或在涉及创造力时表现不稳定。

Method: 提出StoryScore综合指标，整合六个维度：语义对齐、词汇基础、叙事控制、结构保真度、冗余避免和实体级幻觉检测，形成统一评估框架。

Result: 分析揭示了现有幻觉检测方法的局限性：虽然自动指标能有效评估与原始内容的语义相似性，但难以评估叙事方式和控制质量，无法区分教学创造力与事实错误。

Conclusion: StoryScore为AI生成科学故事提供了更全面的评估框架，解决了传统指标在评估叙事质量和区分创造性重构与事实错误方面的不足。

Abstract: Generative AI can turn scientific articles into narratives for diverse audiences, but evaluating these stories remains challenging. Storytelling demands abstraction, simplification, and pedagogical creativity-qualities that are not often well-captured by standard summarization metrics. Meanwhile, factual hallucinations are critical in scientific contexts, yet, detectors often misclassify legitimate narrative reformulations or prove unstable when creativity is involved. In this work, we propose StoryScore, a composite metric for evaluating AI-generated scientific stories. StoryScore integrates semantic alignment, lexical grounding, narrative control, structural fidelity, redundancy avoidance, and entity-level hallucination detection into a unified framework. Our analysis also reveals why many hallucination detection methods fail to distinguish pedagogical creativity from factual errors, highlighting a key limitation: while automatic metrics can effectively assess semantic similarity with original content, they struggle to evaluate how it is narrated and controlled.

</details>


### [161] [Advancing General-Purpose Reasoning Models with Modular Gradient Surgery](https://arxiv.org/abs/2602.02301)
*Min Cai,Yu Liang,Longzheng Wang,Yan Wang,Yueyang Zhang,Long Xia,Zhiyuan Sun,Xi Ye,Daiting Shi*

Main category: cs.CL

TL;DR: 提出MGS方法解决多领域强化学习中梯度冲突问题，在数学、聊天和指令跟随等任务上显著提升模型性能


<details>
  <summary>Details</summary>
Motivation: 现有多领域强化学习方法（Sequential RL和Mixed RL）存在严重的跨领域干扰问题，导致训练通用大型推理模型时整体收益有限

Method: 提出模块化梯度手术（MGS），在Transformer模块层面解决梯度冲突，通过识别和调整冲突梯度来减少跨领域干扰

Result: 在Llama和Qwen模型上，MGS相比标准多任务强化学习分别提升4.3分（16.6%）和4.5分（11.1%），在数学、通用聊天和指令跟随三个领域均有效

Conclusion: MGS有效解决了多领域强化学习中的梯度冲突问题，为训练通用大型推理模型提供了有效解决方案，且在长时间训练中保持有效性

Abstract: Reinforcement learning (RL) has played a central role in recent advances in large reasoning models (LRMs), yielding strong gains in verifiable and open-ended reasoning. However, training a single general-purpose LRM across diverse domains remains challenging due to pronounced domain heterogeneity. Through a systematic study of two widely used strategies, Sequential RL and Mixed RL, we find that both incur substantial cross-domain interference at the behavioral and gradient levels, resulting in limited overall gains. To address these challenges, we introduce **M**odular **G**radient **S**urgery (**MGS**), which resolves gradient conflicts at the module level within the transformer. When applied to Llama and Qwen models, MGS achieves average improvements of 4.3 (16.6\%) and 4.5 (11.1\%) points, respectively, over standard multi-task RL across three representative domains (math, general chat, and instruction following). Further analysis demonstrates that MGS remains effective under prolonged training. Overall, our study clarifies the sources of interference in multi-domain RL and presents an effective solution for training general-purpose LRMs.

</details>


### [162] [The Shape of Beliefs: Geometry, Dynamics, and Interventions along Representation Manifolds of Language Models' Posteriors](https://arxiv.org/abs/2602.02315)
*Raphaël Sarfati,Eric Bigelow,Daniel Wurgaft,Jack Merullo,Atticus Geiger,Owen Lewis,Tom McGrath,Ekdeep Singh Lubana*

Main category: cs.CL

TL;DR: LLMs形成曲面的"信念流形"来表示概率分布的参数，线性干预会破坏流形结构，而几何感知的干预能更好地保持信念族。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs如何编码提示条件信念、如何更新信念以及干预如何重塑信念，缺乏对这些过程的机制性解释。

Method: 使用Llama-3.2在受控环境中生成正态分布样本，通过上下文学习隐式推断分布参数，研究信念流形的形成和变化。

Result: 发现LLMs形成曲面的信念流形来表示分布参数，当分布突变时模型会适应；线性干预会使模型偏离流形并产生耦合的分布外偏移，而几何和场感知的干预能更好地保持信念族。

Conclusion: LLMs中自然涌现出丰富的结构，纯粹的线性概念表示通常是不充分的抽象；线性场探测（LFP）是一种简单的方法来平铺数据流形并进行尊重底层几何的干预。

Abstract: Large language models (LLMs) represent prompt-conditioned beliefs (posteriors over answers and claims), but we lack a mechanistic account of how these beliefs are encoded in representation space, how they update with new evidence, and how interventions reshape them. We study a controlled setting in which Llama-3.2 generates samples from a normal distribution by implicitly inferring its parameters (mean and standard deviation) given only samples from the distribution in context. We find representations of curved "belief manifolds" for these parameters form with sufficient in-context learning and study how the model adapts when the distribution suddenly changes. While standard linear steering often pushes the model off-manifold and induces coupled, out-of-distribution shifts, geometry and field-aware steering better preserves the intended belief family. Our work demonstrates an example of linear field probing (LFP) as a simple approach to tile the data manifold and make interventions that respect the underlying geometry. We conclude that rich structure emerges naturally in LLMs and that purely linear concept representations are often an inadequate abstraction.

</details>


### [163] [A Large-Scale Dataset for Molecular Structure-Language Description via a Rule-Regularized Method](https://arxiv.org/abs/2602.02320)
*Feiyang Cai,Guijuan He,Yi Hu,Jingjing Wang,Joshua Luo,Tianyu Zhu,Srikanth Pilla,Gang Li,Ling Liu,Feng Luo*

Main category: cs.CL

TL;DR: 提出全自动分子结构描述生成框架，通过解析IUPAC名称构建结构化XML元数据，指导LLM生成高质量分子描述，创建了16.3万分子-描述对数据集，验证精度达98.6%


<details>
  <summary>Details</summary>
Motivation: 分子功能主要由结构决定，准确对齐分子结构与自然语言对LLM推理化学任务至关重要。然而人工标注成本高昂，难以构建大规模高质量的结构描述数据集。

Method: 基于规则化学命名解析器扩展，自动解析IUPAC名称构建结构化XML元数据，明确编码分子结构信息，然后利用该元数据指导LLM生成准确的自然语言描述。

Result: 构建了约16.3万个分子-描述对的大规模数据集，通过LLM和专家人工验证2000个分子的子集，显示描述精度达到98.6%。

Conclusion: 该数据集为未来分子-语言对齐提供了可靠基础，所提出的标注方法易于扩展到更大数据集和依赖结构描述的更广泛化学任务。

Abstract: Molecular function is largely determined by structure. Accurately aligning molecular structure with natural language is therefore essential for enabling large language models (LLMs) to reason about downstream chemical tasks. However, the substantial cost of human annotation makes it infeasible to construct large-scale, high-quality datasets of structure-grounded descriptions. In this work, we propose a fully automated annotation framework for generating precise molecular structure descriptions at scale. Our approach builds upon and extends a rule-based chemical nomenclature parser to interpret IUPAC names and construct enriched, structured XML metadata that explicitly encodes molecular structure. This metadata is then used to guide LLMs in producing accurate natural-language descriptions. Using this framework, we curate a large-scale dataset of approximately $163$k molecule-description pairs. A rigorous validation protocol combining LLM-based and expert human evaluation on a subset of $2,000$ molecules demonstrates a high description precision of $98.6\%$. The resulting dataset provides a reliable foundation for future molecule-language alignment, and the proposed annotation method is readily extensible to larger datasets and broader chemical tasks that rely on structural descriptions.

</details>


### [164] [Language Steering for Multilingual In-Context Learning](https://arxiv.org/abs/2602.02326)
*Neeraja Kirtane,Kuan-Hao Huang*

Main category: cs.CL

TL;DR: 提出语言向量方法，通过激活差异引导多语言大模型在上下文学习中的表现，无需训练即可提升非英语语言性能


<details>
  <summary>Details</summary>
Motivation: 多语言大模型在非英语语言上的表现远不如英语，特别是在上下文学习中，使用英语示例测试非英语输入时性能显著下降

Method: 提出语言向量方法，利用源语言和目标语言之间的激活差异来引导模型行为，在推理过程中将向量添加到中间模型激活中，使模型内部表示向目标语言空间偏移

Result: 在三个数据集、19种语言和三个不同模型上测试，结果显示在所有任务和语言中，多语言上下文学习性能均优于基线。层次聚类分析显示语言向量具有有意义的语言家族结构，且这些表示具有任务无关性

Conclusion: 语言向量方法能有效提升多语言大模型在非英语语言上的性能，揭示了模型内部存在通用的语义空间，不同语言在该空间中编码为不同方向

Abstract: While multilingual large language models have gained widespread adoption, their performance on non-English languages remains substantially inferior to English. This disparity is particularly evident in in-context learning scenarios, where providing demonstrations in English but testing on non-English inputs leads to significant performance degradation. In this paper, we hypothesize that LLMs develop a universal semantic space for understanding languages, where different languages are encoded as distinct directions within this space. Based on this hypothesis, we propose language vectors -- a training-free language steering approach that leverages activation differences between source and target languages to guide model behavior. We steer the model generations by adding the vector to the intermediate model activations during inference. This is done to make the model's internal representations shift towards the target language space without any parameter updates. We evaluate our method across three datasets and test on a total of 19 languages on three different models. Our results show consistent improvements on multilingual in-context learning over baselines across all tasks and languages tested. Beyond performance gains, hierarchical clustering of steering vectors reveals meaningful linguistic structure aligned with language families. These vectors also successfully transfer across tasks, demonstrating that these representations are task-agnostic.

</details>


### [165] [Why Steering Works: Toward a Unified View of Language Model Parameter Dynamics](https://arxiv.org/abs/2602.02343)
*Ziwen Xu,Chenyan Wu,Hengyu Sun,Haiwen Hong,Mengru Wang,Yunzhi Yao,Longtao Huang,Hui Xue,Shumin Deng,Zhixuan Chu,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: 该论文提出了一个统一框架，将不同的LLM控制方法视为由控制信号诱导的动态权重更新，并引入偏好-效用分析来量化控制效果，发现偏好与效用之间存在权衡关系，最后提出了改进的SPLIT方法。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型控制方法（如权重微调、LoRA适配、激活干预等）通常被孤立研究，缺乏统一框架来理解它们的联系和进行比较，这阻碍了对控制机制的系统性理解。

Method: 1) 提出统一视图，将各种干预方法框架化为由控制信号诱导的动态权重更新；2) 提出统一的偏好-效用分析，使用极性配对对比示例在共享对数几率尺度上测量偏好（对目标概念的倾向性）和效用（连贯且任务有效的生成）；3) 从激活流形角度解释控制行为；4) 基于分析提出新的SPLIT引导方法。

Result: 发现所有方法中都存在一致的偏好-效用权衡：更强的控制会增加偏好但可预测地降低效用。从激活流形角度看，控制会沿着目标概念方向移动表示以增强偏好，而当干预将表示推离模型的有效生成流形时，效用会下降。

Conclusion: 通过统一框架揭示了LLM控制中的基本权衡关系，提出的SPLIT方法能够在提高偏好的同时更好地保持效用，为理解和改进LLM控制提供了新的视角和工具。

Abstract: Methods for controlling large language models (LLMs), including local weight fine-tuning, LoRA-based adaptation, and activation-based interventions, are often studied in isolation, obscuring their connections and making comparison difficult. In this work, we present a unified view that frames these interventions as dynamic weight updates induced by a control signal, placing them within a single conceptual framework. Building on this view, we propose a unified preference-utility analysis that separates control effects into preference, defined as the tendency toward a target concept, and utility, defined as coherent and task-valid generation, and measures both on a shared log-odds scale using polarity-paired contrastive examples. Across methods, we observe a consistent trade-off between preference and utility: stronger control increases preference while predictably reducing utility. We further explain this behavior through an activation manifold perspective, in which control shifts representations along target-concept directions to enhance preference, while utility declines primarily when interventions push representations off the model's valid-generation manifold. Finally, we introduce a new steering approach SPLIT guided by this analysis that improves preference while better preserving utility. Code is available at https://github.com/zjunlp/EasyEdit/blob/main/examples/SPLIT.md.

</details>


### [166] [Automated Multiple Mini Interview (MMI) Scoring](https://arxiv.org/abs/2602.02360)
*Ryan Huynh,Frank Guerin,Alison Callwood*

Main category: cs.CL

TL;DR: 本文提出了一种多智能体提示框架，用于评估MMI中的软技能，通过结构化提示工程替代数据密集型微调，在复杂主观推理任务上表现优于专业微调基线。


<details>
  <summary>Details</summary>
Motivation: 在竞争性选拔过程中评估同理心、伦理判断和沟通等软技能至关重要，但人工评分往往不一致且存在偏见。虽然LLM改进了自动作文评分，但现有基于推理的微调方法难以处理MMI的抽象、上下文依赖特性，无法捕捉候选人叙述中的隐含信号。

Method: 引入多智能体提示框架，将评估过程分解为转录本精炼和标准特定评分两个阶段。使用大型指令调优模型进行3-shot上下文学习，通过结构化提示工程替代传统的数据密集型微调方法。

Result: 该方法在MMI评估中显著优于专业微调基线（平均QWK 0.62 vs 0.32），达到与人类专家相当的可靠性。在ASAP基准测试中也表现出良好的泛化能力，无需额外训练即可媲美领域特定的最先进模型。

Conclusion: 对于复杂的主观推理任务，结构化提示工程可能提供一种可扩展的替代方案，改变LLM在自动评估中的应用方式，减少对数据密集型微调的依赖。

Abstract: Assessing soft skills such as empathy, ethical judgment, and communication is essential in competitive selection processes, yet human scoring is often inconsistent and biased. While Large Language Models (LLMs) have improved Automated Essay Scoring (AES), we show that state-of-the-art rationale-based fine-tuning methods struggle with the abstract, context-dependent nature of Multiple Mini-Interviews (MMIs), missing the implicit signals embedded in candidate narratives. We introduce a multi-agent prompting framework that breaks down the evaluation process into transcript refinement and criterion-specific scoring. Using 3-shot in-context learning with a large instruct-tuned model, our approach outperforms specialised fine-tuned baselines (Avg QWK 0.62 vs 0.32) and achieves reliability comparable to human experts. We further demonstrate the generalisability of our framework on the ASAP benchmark, where it rivals domain-specific state-of-the-art models without additional training. These findings suggest that for complex, subjective reasoning tasks, structured prompt engineering may offer a scalable alternative to data-intensive fine-tuning, altering how LLMs can be applied to automated assessment.

</details>


### [167] [Proof-RM: A Scalable and Generalizable Reward Model for Math Proof](https://arxiv.org/abs/2602.02377)
*Haotong Yang,Zitong Wang,Shijia Kang,Siqi Yang,Wenkai Yu,Xu Niu,Yike Sun,Yi Hu,Zhouchen Lin,Muhan Zhang*

Main category: cs.CL

TL;DR: 该论文提出了一种可扩展的数据构建流程，利用LLM生成大量高质量的"问题-证明-检查"三元组数据，并训练证明检查奖励模型，以增强LLM的数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型通过可验证奖励的强化学习展现了强大的数学推理能力，但许多高级数学问题是基于证明的，无法通过简单的答案匹配来确定证明的真实性。需要能够可靠评估完整证明过程的奖励模型来实现自动验证。

Method: 设计了可扩展的数据构建流程：1）利用LLM生成大量"问题-证明-检查"三元组数据；2）系统变化问题来源、生成方法和模型配置，创建多样化的证明对；3）通过分层人工审核进行过滤；4）训练证明检查奖励模型，加入过程奖励和令牌权重平衡来稳定强化学习过程。

Result: 实验验证了模型的可扩展性和强大性能，包括奖励准确性、泛化能力和测试时指导等多个方面，为增强LLM数学能力提供了重要的实践方法和工具。

Conclusion: 该研究提出了一种有效的方法来训练证明检查奖励模型，通过可扩展的数据生成和训练策略，为解决基于证明的数学问题验证提供了重要解决方案，有助于提升LLM的数学推理能力。

Abstract: While Large Language Models (LLMs) have demonstrated strong math reasoning abilities through Reinforcement Learning with *Verifiable Rewards* (RLVR), many advanced mathematical problems are proof-based, with no guaranteed way to determine the authenticity of a proof by simple answer matching. To enable automatic verification, a Reward Model (RM) capable of reliably evaluating full proof processes is required. In this work, we design a *scalable* data-construction pipeline that, with minimal human effort, leverages LLMs to generate a large quantity of high-quality "**question-proof-check**" triplet data. By systematically varying problem sources, generation methods, and model configurations, we create diverse problem-proof pairs spanning multiple difficulty levels, linguistic styles, and error types, subsequently filtered through hierarchical human review for label alignment. Utilizing these data, we train a proof-checking RM, incorporating additional process reward and token weight balance to stabilize the RL process. Our experiments validate the model's scalability and strong performance from multiple perspectives, including reward accuracy, generalization ability and test-time guidance, providing important practical recipes and tools for strengthening LLM mathematical capabilities.

</details>


### [168] [From Sycophancy to Sensemaking: Premise Governance for Human-AI Decision Making](https://arxiv.org/abs/2602.02378)
*Raunak Jain,Mudita Khurana,John Stephens,Srinivas Dharmasanam,Shankar Venkataraman*

Main category: cs.CL

TL;DR: 论文主张从AI生成答案转向协作式前提治理，通过差异驱动控制循环检测冲突、定位错位，并使用承诺门控机制防止在未承诺的关键前提上采取行动，以建立可靠的人机合作关系。


<details>
  <summary>Details</summary>
Motivation: 随着LLM从辅助工具扩展到决策支持，出现了危险模式：流畅的同意但缺乏校准判断。低摩擦助手可能变得谄媚，隐含假设被固化，验证成本转嫁给专家，而结果反馈太迟无法作为奖励信号。在深度不确定性决策中，扩展流畅同意会放大糟糕承诺，快于建立专业知识。

Method: 提出从答案生成转向协作式前提治理的范式转变，基于知识基板进行决策关键谈判。采用差异驱动控制循环：检测冲突，通过类型化差异（目的论、认识论、程序性）定位错位，通过决策切片触发有限谈判。承诺门控机制阻止在未承诺的负载前提上采取行动，价值门控挑战在交互成本下分配探测。

Result: 信任应附着于可审计的前提和证据标准，而非对话流畅性。论文以辅导场景为例说明该框架，并提出了可证伪的评估标准。

Conclusion: 可靠的人机合作关系需要从答案生成转向协作式前提治理，通过差异驱动控制循环和承诺门控机制确保决策质量，将信任建立在可审计的前提和证据标准上，而非对话流畅性。

Abstract: As LLMs expand from assistance to decision support, a dangerous pattern emerges: fluent agreement without calibrated judgment. Low-friction assistants can become sycophantic, baking in implicit assumptions and pushing verification costs onto experts, while outcomes arrive too late to serve as reward signals. In deep-uncertainty decisions (where objectives are contested and reversals are costly), scaling fluent agreement amplifies poor commitments faster than it builds expertise. We argue reliable human-AI partnership requires a shift from answer generation to collaborative premise governance over a knowledge substrate, negotiating only what is decision-critical. A discrepancy-driven control loop operates over this substrate: detecting conflicts, localizing misalignment via typed discrepancies (teleological, epistemic, procedural), and triggering bounded negotiation through decision slices. Commitment gating blocks action on uncommitted load-bearing premises unless overridden under logged risk; value-gated challenge allocates probing under interaction cost. Trust then attaches to auditable premises and evidence standards, not conversational fluency. We illustrate with tutoring and propose falsifiable evaluation criteria.

</details>


### [169] [ROG: Retrieval-Augmented LLM Reasoning for Complex First-Order Queries over Knowledge Graphs](https://arxiv.org/abs/2602.02382)
*Ziyan Zhang,Chao Wang,Zhuo Chen,Chiyi Li,Kai Song*

Main category: cs.CL

TL;DR: ROG：一种检索增强框架，通过结合查询感知的邻域检索和LLM链式推理，在知识图谱上回答复杂的一阶逻辑查询，相比基于嵌入的方法有显著改进。


<details>
  <summary>Details</summary>
Motivation: 在不完整的知识图谱上回答复杂的一阶逻辑查询（包含投影、交集、并集和否定）很困难，现有基于嵌入的方法在处理复杂查询结构时存在局限性。

Method: 提出ROG框架，将多操作符查询分解为单操作符子查询序列，每个步骤基于紧凑的查询相关邻域证据进行推理，中间答案集被缓存并在步骤间重用。

Result: 在标准KG推理基准测试中，ROG相比强大的基于嵌入的基线方法取得了一致的性能提升，在高度复杂和否定密集的查询类型上改进最大。

Conclusion: ROG通过用检索增强的逐步推理替代学习到的操作符，为基于嵌入的逻辑推理提供了实用的替代方案，减少了复合错误，在复杂和否定密集的查询上实现了更稳健的推理。

Abstract: Answering first-order logic (FOL) queries over incomplete knowledge graphs (KGs) is difficult, especially for complex query structures that compose projection, intersection, union, and negation. We propose ROG, a retrieval-augmented framework that combines query-aware neighborhood retrieval with large language model (LLM) chain-of-thought reasoning. ROG decomposes a multi-operator query into a sequence of single-operator sub-queries and grounds each step in compact, query-relevant neighborhood evidence. Intermediate answer sets are cached and reused across steps, improving consistency on deep reasoning chains. This design reduces compounding errors and yields more robust inference on complex and negation-heavy queries. Overall, ROG provides a practical alternative to embedding-based logical reasoning by replacing learned operators with retrieval-grounded, step-wise inference. Experiments on standard KG reasoning benchmarks show consistent gains over strong embedding-based baselines, with the largest improvements on high-complexity and negation-heavy query types.

</details>


### [170] [Misconception Diagnosis From Student-Tutor Dialogue: Generate, Retrieve, Rerank](https://arxiv.org/abs/2602.02414)
*Joshua Mitton,Prarthana Bhattacharyya,Digory Smith,Thomas Christie,Ralph Abboud,Simon Woodhead*

Main category: cs.CL

TL;DR: 使用大型语言模型从学生-导师对话中检测学生误解：通过微调LLM生成可能的误解，然后通过嵌入相似度检索，再用另一个微调LLM重新排序，提高检测准确性。


<details>
  <summary>Details</summary>
Motivation: 及时准确地识别学生误解对于改善学习结果和防止错误累积至关重要，但目前这项工作高度依赖教师的努力和直觉，需要自动化解决方案。

Method: 1. 使用微调的LLM生成可能的误解；2. 通过嵌入相似度从生成的误解中检索最有希望的候选；3. 使用另一个微调的LLM评估和重新排序候选误解以提高相关性。

Result: 在真实教育辅导平台对话上评估，使用LLaMA、Qwen和Claude等基础模型，发现该方法在预测性能上优于基线模型，微调能提高生成的误解质量，甚至可以超越更大的闭源模型。

Conclusion: 提出的方法能有效检测学生误解，生成和重新排序步骤对误解生成质量至关重要，为自动化学生误解检测提供了有前景的解决方案。

Abstract: Timely and accurate identification of student misconceptions is key to improving learning outcomes and pre-empting the compounding of student errors. However, this task is highly dependent on the effort and intuition of the teacher. In this work, we present a novel approach for detecting misconceptions from student-tutor dialogues using large language models (LLMs). First, we use a fine-tuned LLM to generate plausible misconceptions, and then retrieve the most promising candidates among these using embedding similarity with the input dialogue. These candidates are then assessed and re-ranked by another fine-tuned LLM to improve misconception relevance. Empirically, we evaluate our system on real dialogues from an educational tutoring platform. We consider multiple base LLM models including LLaMA, Qwen and Claude on zero-shot and fine-tuned settings. We find that our approach improves predictive performance over baseline models and that fine-tuning improves both generated misconception quality and can outperform larger closed-source models. Finally, we conduct ablation studies to both validate the importance of our generation and reranking steps on misconception generation quality.

</details>


### [171] [Large Language Models for Mental Health: A Multilingual Evaluation](https://arxiv.org/abs/2602.02440)
*Nishat Raihan,Sadiya Sayara Chowdhury Puspo,Ana-Maria Bucur,Stevie Chancellor,Marcos Zampieri*

Main category: cs.CL

TL;DR: 评估大语言模型在多种语言心理健康数据集上的表现，包括原始数据和机器翻译版本，并与传统NLP基线比较


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在NLP任务中表现出色，但它们在多语言心理健康领域的性能尚未得到充分探索，特别是在处理不同语言和翻译质量影响方面

Method: 在8个不同语言的心理健康数据集及其机器翻译版本上，评估专有和开源LLM在零样本、少样本和微调设置下的表现，并与传统NLP基线比较，同时评估翻译质量对语言家族和类型学的影响

Result: 专有LLM和微调的开源LLM在多个数据集上获得有竞争力的F1分数，经常超过最先进结果；但在机器翻译数据上表现普遍较差，下降程度因语言和类型学而异

Conclusion: LLM在处理非英语心理健康任务方面有优势，但当翻译质量引入结构或词汇不匹配时存在局限性，突显了翻译质量对多语言心理健康NLP性能的重要影响

Abstract: Large Language Models (LLMs) have remarkable capabilities across NLP tasks. However, their performance in multilingual contexts, especially within the mental health domain, has not been thoroughly explored. In this paper, we evaluate proprietary and open-source LLMs on eight mental health datasets in various languages, as well as their machine-translated (MT) counterparts. We compare LLM performance in zero-shot, few-shot, and fine-tuned settings against conventional NLP baselines that do not employ LLMs. In addition, we assess translation quality across language families and typologies to understand its influence on LLM performance. Proprietary LLMs and fine-tuned open-source LLMs achieve competitive F1 scores on several datasets, often surpassing state-of-the-art results. However, performance on MT data is generally lower, and the extent of this decline varies by language and typology. This variation highlights both the strengths of LLMs in handling mental health tasks in languages other than English and their limitations when translation quality introduces structural or lexical mismatches.

</details>


### [172] [Abstract Activation Spaces for Content-Invariant Reasoning in Large Language Models](https://arxiv.org/abs/2602.02462)
*Gabriele Maraia,Marco Valentino,Fabio Massimo Zanzotto,Leonardo Ranaldi*

Main category: cs.CL

TL;DR: 提出抽象引导推理框架，通过分离结构推理与词汇语义，减少LLMs在演绎推理中的内容效应偏差，提升形式推理的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在演绎推理中存在内容效应问题，即混淆语义合理性与形式有效性。即使模型生成逐步解释，中间推理仍会继承相同的语义捷径。现有方法通过增加推理时结构约束来缓解，但可靠抑制语义干扰仍具挑战性。

Method: 引入抽象引导推理框架，构建内容丰富和抽象的三段论对，使用模型在抽象输入上的激活定义抽象推理空间。学习轻量级抽象器，从内容条件残差流状态预测与该空间对齐的表示，并通过多层干预在前向传播中集成这些预测。

Result: 使用跨语言迁移作为测试平台，显示抽象对齐的引导减少了内容驱动的错误，并提高了有效性敏感性能。

Conclusion: 激活级抽象作为可扩展机制，能增强LLMs中形式推理对语义干扰的鲁棒性。

Abstract: Large Language Models (LLMs) often struggle with deductive judgment in syllogistic reasoning, systematically conflating semantic plausibility with formal validity a phenomenon known as content effect. This bias persists even when models generate step-wise explanations, indicating that intermediate rationales may inherit the same semantic shortcuts that affect answers. Recent approaches propose mitigating this issue by increasing inference-time structural constraints, either by encouraging abstract intermediate representations or by intervening directly in the model's internal computations; however, reliably suppressing semantic interference remains an open challenge. To make formal deduction less sensitive to semantic content, we introduce a framework for abstraction-guided reasoning that explicitly separates structural inference from lexical semantics. We construct paired content-laden and abstract syllogisms and use the model's activations on abstract inputs to define an abstract reasoning space. We then learn lightweight Abstractors that, from content-conditioned residual-stream states, predict representations aligned with this space and integrate these predictions via multi-layer interventions during the forward pass. Using cross-lingual transfer as a test bed, we show that abstraction-aligned steering reduces content-driven errors and improves validity-sensitive performance. Our results position activation-level abstraction as a scalable mechanism for enhancing the robustness of formal reasoning in LLMs against semantic interference.

</details>


### [173] [From Directions to Regions: Decomposing Activations in Language Models via Local Geometry](https://arxiv.org/abs/2602.02464)
*Or Shafran,Shaked Ronen,Omri Fahn,Shauli Ravfogel,Atticus Geiger,Mor Geva*

Main category: cs.CL

TL;DR: 提出使用混合因子分析器（MFA）作为可扩展的无监督方法，通过建模激活空间中的局部高斯区域来捕捉非线性概念结构，在定位和引导任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有激活分解方法基于线性可分性假设，仅搜索全局方向，忽略了具有非线性或多维结构的概念。需要一种能捕捉复杂激活空间结构的可扩展无监督方法。

Method: 采用混合因子分析器（MFA）建模激活空间为多个高斯区域的集合，每个区域有其局部协方差结构。MFA将激活分解为两个组合几何对象：区域质心和局部变异。为Llama-3.1-8B和Gemma-2-2B训练大规模MFA模型。

Result: MFA成功捕捉了激活空间中的复杂非线性结构。在定位和引导基准测试中，MFA优于无监督基线，与有监督定位方法竞争，且通常比稀疏自编码器获得更强的引导性能。

Conclusion: 通过子空间表达的局部几何结构是概念发现和模型控制的有前景分析单元，能够捕捉孤立方向无法处理的复杂结构。

Abstract: Activation decomposition methods in language models are tightly coupled to geometric assumptions on how concepts are realized in activation space. Existing approaches search for individual global directions, implicitly assuming linear separability, which overlooks concepts with nonlinear or multi-dimensional structure. In this work, we leverage Mixture of Factor Analyzers (MFA) as a scalable, unsupervised alternative that models the activation space as a collection of Gaussian regions with their local covariance structure. MFA decomposes activations into two compositional geometric objects: the region's centroid in activation space, and the local variation from the centroid. We train large-scale MFAs for Llama-3.1-8B and Gemma-2-2B, and show they capture complex, nonlinear structures in activation space. Moreover, evaluations on localization and steering benchmarks show that MFA outperforms unsupervised baselines, is competitive with supervised localization methods, and often achieves stronger steering performance than sparse autoencoders. Together, our findings position local geometry, expressed through subspaces, as a promising unit of analysis for scalable concept discovery and model control, accounting for complex structures that isolated directions fail to capture.

</details>


### [174] [Indications of Belief-Guided Agency and Meta-Cognitive Monitoring in Large Language Models](https://arxiv.org/abs/2602.02467)
*Noam Steinmetz Yalon,Ariel Goldstein,Liad Mudrik,Mor Geva*

Main category: cs.CL

TL;DR: 该研究评估了大语言模型是否具备意识指标HOT-3，通过分析信念形成与行动选择的关系，发现LLMs存在信念引导的代理能力和元认知监控能力。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，人们开始质疑这些模型是否具备某种形式的意识。Butlin等人（2023）基于神经科学理论提出了人工系统意识的指标列表，本研究旨在评估其中的关键指标HOT-3，该指标测试由通用信念形成和行动选择系统引导的代理能力。

Method: 将信念视为模型潜在空间中响应输入而出现的表征，引入量化信念在生成过程中主导性的指标。通过分析不同模型和任务中竞争信念的动态关系，研究信念形成与行动选择的因果关系。

Result: 发现三个关键结果：(1) 外部操作能系统性地调节内部信念形成；(2) 信念形成因果性地驱动模型的行动选择；(3) 模型能够监控并报告自身的信念状态。这些结果为LLMs中存在信念引导的代理能力和元认知监控提供了实证支持。

Conclusion: 研究结果为LLMs中存在信念引导的代理能力和元认知监控提供了实证证据，为探索LLMs中代理能力、信念和元认知的涌现奠定了方法论基础，有助于推进对人工系统意识的研究。

Abstract: Rapid advancements in large language models (LLMs) have sparked the question whether these models possess some form of consciousness. To tackle this challenge, Butlin et al. (2023) introduced a list of indicators for consciousness in artificial systems based on neuroscientific theories. In this work, we evaluate a key indicator from this list, called HOT-3, which tests for agency guided by a general belief-formation and action selection system that updates beliefs based on meta-cognitive monitoring. We view beliefs as representations in the model's latent space that emerge in response to a given input, and introduce a metric to quantify their dominance during generation. Analyzing the dynamics between competing beliefs across models and tasks reveals three key findings: (1) external manipulations systematically modulate internal belief formation, (2) belief formation causally drives the model's action selection, and (3) models can monitor and report their own belief states. Together, these results provide empirical support for the existence of belief-guided agency and meta-cognitive monitoring in LLMs. More broadly, our work lays methodological groundwork for investigating the emergence of agency, beliefs, and meta-cognition in LLMs.

</details>


### [175] [MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents](https://arxiv.org/abs/2602.02474)
*Haozhen Zhang,Quanyu Long,Jianzhu Bao,Tao Feng,Weizhi Zhang,Haodong Yue,Wenya Wang*

Main category: cs.CL

TL;DR: MemSkill将LLM代理内存操作重构为可学习、可演化的记忆技能，通过控制器选择技能、执行器生成记忆、设计器进化技能集，形成闭环系统，提升任务性能


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理内存系统依赖少量静态、手工设计的操作来提取记忆，这些固定程序将人类先验硬编码到存储和修订方式中，导致在不同交互模式下表现僵化，在长历史记录上效率低下

Method: 将内存操作重构为可学习、可演化的记忆技能；使用控制器学习选择相关技能；LLM执行器生成技能引导的记忆；设计器定期审查困难案例，通过提出改进和新技能来进化技能集；形成闭环改进流程

Result: 在LoCoMo、LongMemEval、HotpotQA和ALFWorld等数据集上的实验表明，MemSkill相比强基线提升了任务性能，在不同设置下具有良好的泛化能力；分析揭示了技能如何演化

Conclusion: MemSkill为LLM代理提供了更自适应、自我演化的内存管理方法，通过将内存操作重构为可学习技能并建立闭环进化机制，克服了传统静态方法的局限性

Abstract: Most Large Language Model (LLM) agent memory systems rely on a small set of static, hand-designed operations for extracting memory. These fixed procedures hard-code human priors about what to store and how to revise memory, making them rigid under diverse interaction patterns and inefficient on long histories. To this end, we present \textbf{MemSkill}, which reframes these operations as learnable and evolvable memory skills, structured and reusable routines for extracting, consolidating, and pruning information from interaction traces. Inspired by the design philosophy of agent skills, MemSkill employs a \emph{controller} that learns to select a small set of relevant skills, paired with an LLM-based \emph{executor} that produces skill-guided memories. Beyond learning skill selection, MemSkill introduces a \emph{designer} that periodically reviews hard cases where selected skills yield incorrect or incomplete memories, and evolves the skill set by proposing refinements and new skills. Together, MemSkill forms a closed-loop procedure that improves both the skill-selection policy and the skill set itself. Experiments on LoCoMo, LongMemEval, HotpotQA, and ALFWorld demonstrate that MemSkill improves task performance over strong baselines and generalizes well across settings. Further analyses shed light on how skills evolve, offering insights toward more adaptive, self-evolving memory management for LLM agents.

</details>


### [176] [Training LLMs for Divide-and-Conquer Reasoning Elevates Test-Time Scalability](https://arxiv.org/abs/2602.02477)
*Xiao Liang,Zhong-Zhi Li,Zhenghao Lin,Eric Hancheng Jiang,Hengyuan Zhang,Yelong Shen,Kai-Wei Chang,Ying Nian Wu,Yeyun Gong,Weizhu Chen*

Main category: cs.CL

TL;DR: 本文提出一个端到端强化学习框架，通过分治推理提升大语言模型在复杂任务上的推理能力，相比链式思维推理有显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 链式思维推理在模型能力极限时表现不足，且其严格的顺序性限制了测试时的可扩展性。分治推理虽然能分解复杂问题，但通用后训练与分治推理风格之间存在根本性不匹配，限制了模型充分发挥这种潜力。

Method: 提出端到端强化学习框架，在每一步中策略将问题分解为一组子问题，顺序解决它们，并根据子问题解决方案处理原始问题，将分解和解决都整合到强化学习训练中。

Result: 在可比训练条件下，分治风格框架赋予模型更高的性能上限和更强的测试时扩展性，在竞赛级基准测试中，Pass@1提升8.6%，Pass@32提升6.3%。

Conclusion: 通过强化学习训练的分治推理框架能有效提升大语言模型在最具挑战性任务上的推理能力，克服了链式思维推理的局限性。

Abstract: Large language models (LLMs) have demonstrated strong reasoning capabilities through step-by-step chain-of-thought (CoT) reasoning. Nevertheless, at the limits of model capability, CoT often proves insufficient, and its strictly sequential nature constrains test-time scalability. A potential alternative is divide-and-conquer (DAC) reasoning, which decomposes a complex problem into subproblems to facilitate more effective exploration of the solution. Although promising, our analysis reveals a fundamental misalignment between general-purpose post-training and DAC-style inference, which limits the model's capacity to fully leverage this potential. To bridge this gap and fully unlock LLMs' reasoning capabilities on the most challenging tasks, we propose an end-to-end reinforcement learning (RL) framework to enhance their DAC-style reasoning capacity. At each step, the policy decomposes a problem into a group of subproblems, solves them sequentially, and addresses the original one conditioned on the subproblem solutions, with both decomposition and solution integrated into RL training. Under comparable training, our DAC-style framework endows the model with a higher performance ceiling and stronger test-time scalability, surpassing CoT by 8.6% in Pass@1 and 6.3% in Pass@32 on competition-level benchmarks.

</details>


### [177] [RE-TRAC: REcursive TRAjectory Compression for Deep Search Agents](https://arxiv.org/abs/2602.02486)
*Jialiang Zhu,Gongrui Zhang,Xiaolong Ma,Lin Xu,Miaosen Zhang,Ruiqi Yang,Song Wang,Kai Qiu,Zhirong Wu,Qi Dai,Ruichun Ma,Bei Liu,Yifan Yang,Chong Luo,Zhengyuan Yang,Linjie Li,Lijuan Wang,Weizhu Chen,Xin Geng,Baining Guo*

Main category: cs.CL

TL;DR: Re-TRAC是一个基于LLM的研究代理框架，通过跨轨迹探索和结构化状态表示来改进ReAct框架，实现迭代反思和全局规划，显著提升研究效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于ReAct框架的LLM研究代理存在线性设计的局限性，难以回溯早期状态、探索替代搜索方向或在长上下文中保持全局意识，导致局部最优、冗余探索和搜索效率低下。

Method: 提出Re-TRAC框架，在每个轨迹后生成结构化状态表示（总结证据、不确定性、失败和未来计划），并基于此状态表示指导后续轨迹，实现跨轨迹探索、迭代反思和全局规划。

Result: 在BrowseComp基准测试中，Re-TRAC比ReAct框架性能提升15-20%；对于小模型，通过Re-TRAC-aware监督微调达到可比规模的SOTA性能；工具调用和token使用量随轮次单调减少，表明探索更具针对性。

Conclusion: Re-TRAC通过跨轨迹探索和结构化状态表示，将研究重构为渐进过程，显著提升了LLM研究代理的效率和效果，减少了冗余搜索，实现了更智能的探索策略。

Abstract: LLM-based deep research agents are largely built on the ReAct framework. This linear design makes it difficult to revisit earlier states, branch into alternative search directions, or maintain global awareness under long contexts, often leading to local optima, redundant exploration, and inefficient search. We propose Re-TRAC, an agentic framework that performs cross-trajectory exploration by generating a structured state representation after each trajectory to summarize evidence, uncertainties, failures, and future plans, and conditioning subsequent trajectories on this state representation. This enables iterative reflection and globally informed planning, reframing research as a progressive process. Empirical results show that Re-TRAC consistently outperforms ReAct by 15-20% on BrowseComp with frontier LLMs. For smaller models, we introduce Re-TRAC-aware supervised fine-tuning, achieving state-of-the-art performance at comparable scales. Notably, Re-TRAC shows a monotonic reduction in tool calls and token usage across rounds, indicating progressively targeted exploration driven by cross-trajectory reflection rather than redundant search.

</details>


### [178] [Reward-free Alignment for Conflicting Objectives](https://arxiv.org/abs/2602.02495)
*Peter Chen,Xiaopeng Li,Xi Chen,Tianyi Lin*

Main category: cs.CL

TL;DR: 本文提出了一种无需奖励模型的多目标对齐框架RACO，通过冲突规避梯度下降解决LLM对齐中的多目标冲突问题，在多个LLM上实现了更好的帕累托权衡。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的LLM对齐问题通常涉及多个相互冲突的目标，现有方法要么通过加权损失导致训练不稳定和权衡不佳，要么依赖复杂的奖励模型，这会扭曲用户指定的偏好。

Method: 提出了RACO框架，直接利用成对偏好数据，通过新颖的裁剪版冲突规避梯度下降解决梯度冲突，并提供收敛保证。还使用启发式方法改进该方法。

Result: 在多目标摘要和安全对齐任务上，对多个LLM家族（Qwen 3、Llama 3、Gemma 3）的定性和定量评估表明，该方法相比现有基线能实现更好的帕累托权衡。

Conclusion: RACO框架能够有效解决LLM多目标对齐中的冲突问题，无需依赖奖励模型，在多个任务和模型上表现出优越的帕累托权衡能力。

Abstract: Direct alignment methods are increasingly used to align large language models (LLMs) with human preferences. However, many real-world alignment problems involve multiple conflicting objectives, where naive aggregation of preferences can lead to unstable training and poor trade-offs. In particular, weighted loss methods may fail to identify update directions that simultaneously improve all objectives, and existing multi-objective approaches often rely on explicit reward models, introducing additional complexity and distorting user-specified preferences. The contributions of this paper are two-fold. First, we propose a Reward-free Alignment framework for Conflicted Objectives (RACO) that directly leverages pairwise preference data and resolves gradient conflicts via a novel clipped variant of conflict-averse gradient descent. We provide convergence guarantees to Pareto-critical points that respect user-specified objective weights, and further show that clipping can strictly improve convergence rate in the two-objective setting. Second, we improve our method using some heuristics and conduct experiments to demonstrate the compatibility of the proposed framework for LLM alignment. Both qualitative and quantitative evaluations on multi-objective summarization and safety alignment tasks across multiple LLM families (Qwen 3, Llama 3, Gemma 3) show that our method consistently achieves better Pareto trade-offs compared to existing multi-objective alignment baselines.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [179] [Rough Martingale Optimal Transport: Theory, Implementation, and Regulatory Applications for Non-Modelable Risk Factors](https://arxiv.org/abs/2602.00097)
*Sri Sairam Gautam B.,Isha*

Main category: q-fin.RM

TL;DR: 提出统一的粗糙鞅最优传输(RMOT)框架，通过粗糙波动率先验正则化传输计划，为FRTB下的非模型化风险因子提供有限、显式且渐近紧的外推边界。


<details>
  <summary>Details</summary>
Motivation: FRTB对奇异衍生品定价提出重大挑战，特别是对于非模型化风险因子，稀疏市场数据导致经典鞅最优传输方法产生无限审计边界。

Method: 提出粗糙鞅最优传输框架，用粗糙波动率先验正则化传输计划；建立稀疏数据下粗糙波动率参数的可识别性理论；开发块稀疏优化算法扩展到多资产情况。

Result: 证明50个行权价足以在±0.05内估计Hurst指数；多资产情况下，若Hurst指数不同，可从边际期权曲面局部识别相关矩阵；实证显示每10亿美元奇异产品组合相比经典方法可节省约8.8亿美元资本。

Conclusion: RMOT为FRTB下的NMRF提供了符合监管原则的定价框架，具有显式误差量化，相比经典方法提供显著资本节省同时保持保守覆盖。

Abstract: The Fundamental Review of the Trading Book (FRTB) poses a significant challenge for exotic derivatives pricing, particularly for non-modelable risk factors (NMRF) where sparse market data leads to infinite audit bounds under classical Martingale Optimal Transport (MOT). We propose a unified Rough Martingale Optimal Transport (RMOT) framework that regularizes the transport plan with a rough volatility prior, yielding finite, explicit, and asymptotically tight extrapolation bounds. We establish an identifiability theorem for rough volatility parameters under sparse data, proving that 50 strikes are sufficient to estimate the Hurst exponent within $\pm 0.05$. For the multi-asset case, we prove that the correlation matrix is locally identifiable from marginal option surfaces provided the Hurst exponents are distinct. Model calibration on SPY and QQQ options (2019--2024) confirms that the optimal martingale measure exhibits stretched exponential tail decay ($\sim\exp(-k^{1-H})$), consistent with rough volatility asymptotics, whereas classical MOT yields trivial bounds. We validate the framework on live SPX/NDX data and scale it to $N = 30$ assets using a block-sparse optimization algorithm. Empirical results show that RMOT provides approximately \$880M in capital relief per \$1B exotic book compared to classical methods, while maintaining conservative coverage confirmed by 100-seed cross-validation. This constitutes a pricing framework designed to align with FRTB principles for NMRFs with explicit error quantification.

</details>


### [180] [Non-standard analysis for coherent risk estimation: hyperfinite representations, discrete Kusuoka formulae, and plug-in asymptotics](https://arxiv.org/abs/2602.00784)
*Tomasz Kania*

Main category: q-fin.RM

TL;DR: 本文建立了一个非标准分析框架，将相干风险度量和相干风险估计量统一处理，通过超有限方法得到了多个表示定理和一致性结果。


<details>
  <summary>Details</summary>
Motivation: 传统风险度量理论中，风险度量（基于分布）和风险估计量（基于样本）之间存在鸿沟。本文旨在通过非标准分析建立统一的框架，将两者联系起来，从而更好地理解有限样本风险估计量的性质。

Method: 使用非标准分析工具，特别是Loeb概率空间和超有限方法。将相干风险度量实现为内部支撑泛函的标准部分，将相干风险估计量视为有限网格限制。通过超有限表示定理连接概率和统计视角。

Result: 获得了六个主要结果：1）超有限稳健表示定理；2）离散Kusuoka表示；3）谱插值估计量的一致几乎必然收敛性；4）Kusuoka型插值一致性定理；5）通过非标准分析重构函数Delta方法得到的bootstrap有效性；6）通过超有限中心极限定理得到的渐近正态性。

Conclusion: 非标准分析为相干风险度量和估计量提供了统一的视角，建立了概率与统计之间的透明字典。超有限方法使得有限样本结果可以从无限维表示中自然导出，为风险估计理论提供了新的工具和见解。

Abstract: We develop a non-standard analysis framework for coherent risk measures and their finite-sample analogues, coherent risk estimators, building on recent work of Aichele, Cialenco, Jelito, and Pitera. Coherent risk measures on $L^\infty$ are realised as standard parts of internal support functionals on Loeb probability spaces, and coherent risk estimators arise as finite-grid restrictions.
  Our main results are: (i) a hyperfinite robust representation theorem that yields, as finite shadows, the robust representation results for coherent risk estimators; (ii) a discrete Kusuoka representation for law-invariant coherent risk estimators as suprema of mixtures of discrete expected shortfalls on $\{k/n:k=1,\ldots,n\}$; (iii) uniform almost sure consistency (with an explicit rate) for canonical spectral plug-in estimators over Lipschitz spectral classes; (iv) a Kusuoka-type plug-in consistency theorem under tightness and uniform estimation assumptions; (v) bootstrap validity for spectral plug-in estimators via an NSA reformulation of the functional delta method (under standard smoothness assumptions on $F_X$); and (vi) asymptotic normality obtained through a hyperfinite central limit theorem.
  The hyperfinite viewpoint provides a transparent probability-to-statistics dictionary: applying a risk measure to a law corresponds to evaluating an internal functional on a hyperfinite empirical measure and taking the standard part. We include a standardd self-contained introduction to the required non-standard tools.

</details>


### [181] [A Methodology to Measure Impacts of Scenarios Through Expected Credit Losses](https://arxiv.org/abs/2602.01361)
*Mahmood Alaghmandan,Meghal Arora,Olga Streltchenko*

Main category: q-fin.RM

TL;DR: 提出一种利用金融机构现有拨备基础设施，通过违约概率变化来衡量情景对预期损失影响的方法，并设计实施基于风险驱动因素的情景测试框架。


<details>
  <summary>Details</summary>
Motivation: 为金融机构提供一种标准化方法来量化情景（特别是气候情景）对信贷风险的影响，利用现有拨备系统基础设施，避免重复建设。

Method: 通过违约概率变化捕捉情景效应，设计基于风险驱动因素的情景测试框架，将具有共同特征的敞口进行标准化分组。

Result: 该方法为加拿大金融机构监管办公室和魁北克金融市场管理局2024年标准化气候情景演练提供了理论基础。

Conclusion: 提出的方法论为金融机构情景测试提供了实用框架，特别适用于气候风险等新兴风险领域，已在加拿大监管实践中得到应用。

Abstract: In this paper, we present a methodology for measuring the impact of scenarios on the expected losses of exposures by leveraging the existing provisioning infrastructure within financial institutions, where scenario effects are captured through changes in probabilities of default. We then describe how to design and implement a scenario test where risk drivers are given for standardized groupings of exposures, and the groupings are defined based on common features of the exposures. The methodology presented served as a theoretical foundation for the standardized climate scenario exercise conducted in 2024 by the Office of the Superintendent of Financial Institutions of Canada and Quebec's Autorite des Marches Financiers.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [182] [Optimal Control-Based Falsification of Learnt Dynamics via Neural ODEs and Symbolic Regression](https://arxiv.org/abs/2602.00031)
*Lasse Kötz,Jonas Sjöberg,Knut Åkesson*

Main category: eess.SY

TL;DR: 提出一个结合学习代理动力学与最优控制的伪造框架，用于为信号时序逻辑（STL）指定的信息物理系统高效生成反例


<details>
  <summary>Details</summary>
Motivation: 传统基于优化的伪造方法需要大量系统实验，效率低下。本文旨在通过建模系统动力学来减少对实际系统实验的需求，提高伪造效率。

Method: 1. 使用神经ODE学习未知系统动力学，同时嵌入已知先验结构以减少数据需求
2. 通过符号回归将学习到的神经ODE转换为解析形式，实现快速可解释的轨迹优化
3. 将伪造问题转化为最小化STL鲁棒性，负鲁棒性值产生候选反例
4. 在原始系统上验证候选反例，虚假轨迹用于迭代优化代理模型

Result: 在ARCH-COMP 2024基准测试中，该方法相比不建模系统动力学的基于优化方法，需要数量级更少的系统实验

Conclusion: 提出的框架通过结合学习代理动力学与最优控制，能够高效生成信息物理系统的STL反例，显著减少对实际系统实验的依赖

Abstract: We present a falsification framework that integrates learned surrogate dynamics with optimal control to efficiently generate counterexamples for cyber-physical systems specified in signal temporal logic (STL). The unknown system dynamics are identified using neural ODEs, while known a-priori structure is embedded directly into the model, reducing data requirements. The learned neural ODE is converted into an analytical form via symbolic regression, enabling fast and interpretable trajectory optimization. Falsification is cast as minimizing STL robustness over input trajectories; negative robustness yields candidate counterexamples, which are validated on the original system. Spurious traces are iteratively used to refine the surrogate, while true counterexamples are returned as final results. Experiments on ARCH-COMP 2024 benchmarks show that this method requires orders of magnitude fewer experiments of the system under test than optimization-based approaches that do not model system dynamics.

</details>


### [183] [Motion Planning with Metric Temporal Logic Using Reachability Analysis and Hybrid Zonotopes](https://arxiv.org/abs/2602.00325)
*Andrew F. Thompson,Joshua A. Robbins,Jonah J. Glunt,Sean B. Brennan,Herschel C. Pangborn*

Main category: eess.SY

TL;DR: 提出一种基于可达性分析和混合Zonotope集合表示的方法，将MTL规范编码为可达集，以高效优化满足时间相关任务要求的运动规划


<details>
  <summary>Details</summary>
Motivation: MTL为自动驾驶车辆的时间相关任务要求提供了形式化框架，但受这些约束的优化控制决策通常计算成本高昂，需要更高效的方法

Method: 使用可达性分析隐式表达满足MTL规范的状态集合，采用混合Zonotope集合表示高效编码MTL规范到可达集，然后优化寻找运动规划

Result: 数值基准测试显示该方法相比现有方法具有计算优势，数值示例和实验应用证明能处理时变环境、区域相关扰动和多智能体协调

Conclusion: 提出的基于可达性分析和混合Zonotope的方法能高效处理MTL规范，适用于复杂时变环境和多智能体协调任务

Abstract: Metric temporal logic (MTL) provides a formal framework for defining time-dependent mission requirements on autonomous vehicles. However, optimizing control decisions subject to these constraints is often computationally expensive. This article presents a method that uses reachability analysis to implicitly express the set of states satisfying an MTL specification and then optimizes to find a motion plan. The hybrid zonotope set representation is used to efficiently and conveniently encode MTL specifications into reachable sets. A numerical benchmark highlights the proposed method's computational advantages as compared to existing methods in the literature. Further numerical examples and an experimental application demonstrate the ability to address time-varying environments, region-dependent disturbances, and multi-agent coordination.

</details>


### [184] [Stealthy Coverage Control for Human-enabled Real-Time 3D Reconstruction](https://arxiv.org/abs/2602.00466)
*Reiji Terunuma,Yuta Nakamura,Takuma Abe,Takeshi Hatanaka*

Main category: eess.SY

TL;DR: 提出一种名为"隐形覆盖控制"的半自主图像采样策略，用于人类辅助的3D结构重建，通过结合人类灵活推理和自主覆盖控制来提高重建质量。


<details>
  <summary>Details</summary>
Motivation: 3D重建所需图像数量取决于目标场景的结构复杂性，但通常无法预先获知这种空间非均匀的复杂性分布。需要利用人类的灵活推理和情境识别能力来解决这一问题。

Method: 设计半自主系统：人类操作员识别需要更多图像的区域并引导无人机前往，系统通过"隐形覆盖控制"将无人机的高效图像采样运动与人类导航解耦，避免操作冲突。

Result: 基于Unity/ROS2的仿真研究表明，该半自主系统在重建模型质量方面优于无人干预的自主系统。

Conclusion: 通过结合人类灵活推理和自主覆盖控制的优势，提出的半自主系统能够有效处理未知结构复杂性的3D重建任务，提高重建质量。

Abstract: In this paper, we propose a novel semi-autonomous image sampling strategy, called stealthy coverage control, for human-enabled 3D structure reconstruction. The present mission involves a fundamental problem: while the number of images required to accurately reconstruct a 3D model depends on the structural complexity of the target scene to be reconstructed, it is not realistic to assume prior knowledge of the spatially non-uniform structural complexity. We approach this issue by leveraging human flexible reasoning and situational recognition capabilities. Specifically, we design a semi-autonomous system that leaves identification of regions that need more images and navigation of the drones to such regions to a human operator. To this end, we first present a way to reflect the human intention in autonomous coverage control. Subsequently, in order to avoid operational conflicts between manual control and autonomous coverage control, we develop the stealthy coverage control that decouples the drone motion for efficient image sampling from navigation by the human. Simulation studies on a Unity/ROS2-based simulator demonstrate that the present semi-autonomous system outperforms the one without human interventions in the sense of the reconstructed model quality.

</details>


### [185] [Model-Based Data-Efficient and Robust Reinforcement Learning](https://arxiv.org/abs/2602.00630)
*Ludvig Svedlund,Constantin Cronrath,Jonas Fredriksson,Bengt Lennartson*

Main category: eess.SY

TL;DR: 提出一种数据高效的学习型控制设计方法，通过两级结构实现：高层优化能耗，底层反馈控制补偿扰动，相比传统强化学习方法节能效果更好且计算效率提高100倍以上。


<details>
  <summary>Details</summary>
Motivation: 现有学习型控制方法在数据效率和鲁棒性方面存在不足，特别是在处理未建模动态和硬约束时，模型无关方法表现较差。需要一种既能降低能耗又能满足状态和动作约束的高效控制方法。

Method: 采用两级控制结构：1）高层通过优化学习到的系统动力学模型来最小化能耗，同时满足硬状态和动作约束；2）底层使用反馈控制器补偿负载扰动和模型误差。该方法假设路径已知，可调整速度和加速度来节能。

Result: 与两种知名的actor-critic强化学习策略相比，该方法节能效果更好，且评估时间步数减少100倍以上。模型无关方法在包含未建模动态时鲁棒性较差，而所提模型基础方法表现更优。

Conclusion: 该学习型控制方法在数据效率、节能性能和计算效率方面优于传统强化学习方法，通过模型基础的两级结构有效平衡了优化性能和鲁棒性要求。

Abstract: A data-efficient learning-based control design method is proposed in this paper. It is based on learning a system dynamics model that is then leveraged in a two-level procedure. On the higher level, a simple but powerful optimization procedure is performed such that, for example, energy consumption in a vehicle can be reduced when hard state and action constraints are also introduced. Load disturbances and model errors are compensated for by a feedback controller on the lower level. In that regard, we briefly examine the robustness of both model-free and model-based learning approaches, and it is shown that the model-free approach greatly suffers from the inclusion of unmodeled dynamics. In evaluating the proposed method, it is assumed that a path is given, while the velocity and acceleration can be modified such that energy is saved, while still keeping speed limits and completion time. Compared with two well-known actor-critic reinforcement learning strategies, the suggested learning-based approach saves more energy and reduces the number of evaluated time steps by a factor of 100 or more.

</details>


### [186] [Modeling and Control of Hybrid Distribution Transformers for Simultaneous Grid Services](https://arxiv.org/abs/2602.00798)
*Martin Doff-Sotta,Florian Cech,Rishabh Manjunatha,Costantino Citro,Matthew Williams,Thomas Morstyn*

Main category: eess.SY

TL;DR: 本文提出了三相混合配电变压器的平均数学模型，采用串并联配置的双背靠背电压源变流器，设计了级联PI控制器实现负载电压调节、无功补偿、电网频率调节和负载相平衡。


<details>
  <summary>Details</summary>
Motivation: 混合配电变压器（HDTs）结合传统变压器和部分额定功率电子变流器，旨在改善电能质量、提供高级辅助服务并提高可再生能源在电网中的渗透率。需要建立有效的控制模型来实现这些功能。

Method: 建立了三相HDT的平均数学模型，采用串并联配置的双背靠背电压源变流器。在同步旋转dq0参考系中设计了级联PI控制器，用于调节负载电压、补偿无功功率、实现电网频率调节和负载相平衡。

Result: Python仿真结果表明，这些简单而有效的控制机制使HDTs能够同时提供多种电网服务，而不会引入复杂性。完整的模型、控制架构和实施步骤已详细说明。

Conclusion: 本文提出的HDT控制方案能够有效实现多种电网服务功能，为混合配电变压器的进一步验证和应用提供了完整的建模和控制框架。

Abstract: Hybrid distribution transformers (HDTs) integrate conventional transformers with partially rated power electronic converters to improve power quality, enable advanced ancillary services and increase penetration of renewable energy sources in the national power grid. In this paper, we present an averaged mathematical model of a three-phase HDT equipped with two back-to-back voltage source converters connected in a series-shunt configuration. Cascaded PI controllers are designed in the synchronously rotating dq0 reference frame to regulate load voltage, compensate reactive power, achieve grid frequency regulation, and perform load phase balancing. Simulation results implemented in Python confirm that these simple yet effective control mechanisms allow HDTs to offer simultaneous grid services without introducing complexity. The complete model, control architecture, and implementation steps are detailed, enabling further validation and adoption.

</details>


### [187] [Cognitive-Flexible Control via Latent Model Reorganization with Predictive Safety Guarantees](https://arxiv.org/abs/2602.00812)
*Thanana Nuchkrua,Sudchai Boonto*

Main category: eess.SY

TL;DR: 提出认知灵活控制框架，通过在线调整潜在信念表示来应对系统动态和感知条件的突变，同时保持控制律的显式性和安全认证。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的控制系统大多依赖固定的内部表示，在分布偏移下性能会显著下降。需要一种能够适应动态变化、同时保持安全保证的控制框架。

Method: 提出认知灵活深度随机状态空间模型（CF-DeepSSSM），在有限认知灵活性指数约束下重组潜在表示，并将其嵌入贝叶斯模型预测控制框架中。

Result: 建立了后验漂移有界、递归可行性和闭环稳定性的理论保证。仿真显示在系统动态和观测突变下，能够安全地进行表示适应并快速恢复性能。

Conclusion: 认知灵活控制框架为学习赋能（而非基于学习）的控制系统提供了处理非平稳网络物理系统的新方法，实现了安全表示适应和快速性能恢复。

Abstract: Learning-enabled control systems must maintain safety when system dynamics and sensing conditions change abruptly. Although stochastic latent-state models enable uncertainty-aware control, most existing approaches rely on fixed internal representations and can degrade significantly under distributional shift. This letter proposes a \emph{cognitive-flexible control} framework in which latent belief representations adapt online, while the control law remains explicit and safety-certified. We introduce a Cognitive-Flexible Deep Stochastic State-Space Model (CF--DeepSSSM) that reorganizes latent representations subject to a bounded \emph{Cognitive Flexibility Index} (CFI), and embeds the adapted model within a Bayesian model predictive control (MPC) scheme. We establish guarantees on bounded posterior drift, recursive feasibility, and closed-loop stability. Simulation results under abrupt changes in system dynamics and observations demonstrate safe representation adaptation with rapid performance recovery, highlighting the benefits of learning-enabled, rather than learning-based, control for nonstationary cyber-physical systems.

</details>


### [188] [Robust Energy Shaping Control of an Underactuated Inverted Pendulum](https://arxiv.org/abs/2602.00905)
*M. Reza J. Harandi,Mehrzad Namvar*

Main category: eess.SY

TL;DR: 本文针对旋转倒立摆系统，提出了一种改进的IDA-PBC控制方法，通过求解能量PDE的简洁解析解，并加入鲁棒项补偿特定扰动，实现了良好的控制性能。


<details>
  <summary>Details</summary>
Motivation: 虽然总能量整形方法为欠驱动系统的稳定化提供了通用框架，但其实际应用受到需要解析求解偏微分方程组的限制。现有IDA-PBC文献也未能有效处理特定类型的扰动。

Method: 针对旋转倒立摆系统，开发了IDA-PBC控制方案：1) 推导了动能和势能PDE的简洁解析解；2) 在控制律中加入了新颖的鲁棒项，用于补偿特定类别的扰动。

Result: 通过数值仿真验证了所提方法的有效性，展示了令人满意的控制性能。

Conclusion: 提出的改进IDA-PBC方法成功解决了旋转倒立摆系统的控制问题，克服了传统方法需要复杂PDE解析解的障碍，并增强了系统对特定扰动的鲁棒性。

Abstract: Although the stabilization of underactuated systems remains a challenging problem, the total energy shaping approach provides a general framework for addressing this objective. However, the practical implementation of this method is hindered by the need to analytically solve a set of partial differential equations (PDEs), which constitutes a major obstacle. In this paper, a rotary inverted pendulum system is considered, and an interconnection and damping assignment passivity-based control (IDA-PBC) scheme is developed by deriving concise analytical solutions to the kinetic and potential energy PDEs. Furthermore, a novel robust term is incorporated into the control law to compensate for a specific class of disturbances that has not been addressed within the existing IDA-PBC literature. The effectiveness of the proposed method is validated through numerical simulations, demonstrating satisfactory control performance.

</details>


### [189] [Reduction of Velocity-Dependent Terms in Total Energy Shaping Approach](https://arxiv.org/abs/2602.00908)
*M. Reza J. Harandi,Mehrzad Namvar*

Main category: eess.SY

TL;DR: 提出一种基于SIDA-PBC框架的方法，通过优化广义力来抑制动能整形项，从而降低控制幅度，适用于单执行器系统


<details>
  <summary>Details</summary>
Motivation: IDA-PBC是稳定欠驱动机械系统的强大框架，但现有方法难以处理执行器限制，且闭环行为受动能整形项显著影响，需要开发能有效抑制动能整形项的方法

Method: 采用同步IDA-PBC框架，在不改变匹配偏微分方程的前提下，通过广义力系统性地衰减动能整形项；使用ℓ∞-范数优化公式解析推导广义力的自由分量

Result: 该方法能有效抑制动能整形分量，在结构可行时实现控制幅度的降低；与现有基于陀螺项的方法不同，本方法适用于单执行器系统；仿真和实验结果验证了有效性

Conclusion: 提出的SIDA-PBC框架通过优化广义力抑制动能整形项，为解决执行器限制下的能量整形控制问题提供了有效方案，扩展了IDA-PBC在实际系统中的应用范围

Abstract: Total energy shaping through interconnection and damping assignment passivity-based control (IDA-PBC) provides a powerful and systematic framework for stabilizing underactuated mechanical systems. Despite its theoretical appeal, incorporating actuator limitations into total energy shaping remains a largely open problem, with only limited results reported in the existing literature. In practice, the closed-loop behavior of energy-shaping controllers is strongly affected by the kinetic energy shaping terms. In this paper, a simultaneous IDA-PBC (SIDA-PBC) framework is employed to systematically attenuate the kinetic energy shaping terms by exploiting generalized forces, without altering the matching partial differential equations (PDEs). The free component of the generalized forces is derived analytically via an $\ell_\infty$-norm optimization formulation. Although a reduction in kinetic energy shaping terms does not necessarily guarantee a decrease in the overall control effort, the proposed approach effectively suppresses kinetic energy shaping components and achieves a reduced control magnitude whenever such a reduction is structurally feasible. Unlike existing approaches based on gyroscopic terms, which require multiple actuators, the proposed method is applicable to mechanical systems with a single actuator. Simulation and experimental results are provided to validate the effectiveness of the proposed approach.

</details>


### [190] [Robust Adaptive Learning Control for a Class of Non-affine Nonlinear Systems](https://arxiv.org/abs/2602.00968)
*Shuai Gao,Dong Shen,Abdelhamid Tayebi*

Main category: eess.SY

TL;DR: 提出一种针对具有高相对度的不确定非仿射非线性系统的鲁棒自适应学习控制方案，适用于非重复任务


<details>
  <summary>Details</summary>
Motivation: 解决具有高相对度的不确定非仿射非线性系统在非重复任务中的跟踪控制问题，这类系统存在时变参数和不可测状态变量等挑战

Method: 采用梯度下降参数自适应律处理未知时变参数，结合状态估计器估计不可测状态变量，并提供显式迭代计算方法便于实现

Result: 对控制策略性能进行了全面分析，仿真结果验证了所提方法的有效性

Conclusion: 提出的鲁棒自适应学习控制方案能有效处理高相对度不确定非仿射非线性系统的跟踪控制问题，特别适用于非重复任务场景

Abstract: We address the tracking problem for a class of uncertain non-affine nonlinear systems with high relative degrees, performing non-repetitive tasks. We propose a rigorously proven, robust adaptive learning control scheme that relies on a gradient descent parameter adaptation law to handle the unknown time-varying parameters of the system, along with a state estimator that estimates the unmeasurable state variables. Furthermore, despite the inherently complex nature of the non-affine system, we provide an explicit iterative computation method to facilitate the implementation of the proposed control scheme. The paper includes a thorough analysis of the performance of the proposed control strategy, and simulation results are presented to demonstrate the effectiveness of the approach.

</details>


### [191] [Mitigating Data Centers Load Risks and Enabling Grid Support Functions through Grid-Forming Control](https://arxiv.org/abs/2602.01013)
*Yousef Abudyak,Mohsen Alizadeh,Wei Sun*

Main category: eess.SY

TL;DR: 数据中心集成电池储能系统与构网型逆变器，为电网提供主动支撑功能，解决AI负载引起的功率波动问题


<details>
  <summary>Details</summary>
Motivation: 大规模数据中心因大语言模型和AI工作负载导致功率急剧变化，引起电压偏差和频率扰动，且作为被动负载无法提供电网支撑

Method: 提出集成架构，将电池储能系统与构网型逆变器结合在数据中心内，提供主动电网支持功能，通过MATLAB/Simulink仿真验证

Result: 八个协调的BESS单元在动态负载下准确跟踪功率参考值；在单相电压跌落时提供类似STATCOM的无功支持；电网断开时实现无缝孤岛运行

Conclusion: 数据中心集成BESS和构网型逆变器能有效解决AI负载引起的电网问题，提供主动电网支持并确保可靠运行

Abstract: The rapid growth of hyperscale data centers driven by Large Language Models and Artificial Intelligence workloads has introduced new challenges for power systems. These facilities experience abrupt power variations during model training and check-point-saving events, causing voltage deviations and frequency disturbances. Moreover, they operate as passive loads that draw power without offering any grid support. This paper presents an integrated architecture that combines Battery Energy Storage Systems (BESSs) within data centers using Grid-Forming inverters to provide active grid-support functions. Simulation results through MATLAB/Simulink demonstrate accurate power reference tracking under dynamic loading, with eight coordinated BESS units supplying instantaneous power during training and saving conditions. Under single-phase voltage depression near the data center bus, the BESS delivered reactive power support similar to a Static Synchronous Compensator. During grid disconnection, seamless islanded operation was achieved with stable voltage, frequency, and continuous power delivery at the data center bus.

</details>


### [192] [Scientific Machine Learning for Resilient EV-Grid Planning and Decision Support Under Extreme Events](https://arxiv.org/abs/2602.01261)
*Yifan Wang*

Main category: eess.SY

TL;DR: 提出五阶段科学机器学习框架，通过物理知识注入解决电动汽车充电基础设施在极端需求事件下的城市配电网弹性评估问题，填补微观充电物理与城市规模规划之间的尺度鸿沟。


<details>
  <summary>Details</summary>
Motivation: 电动汽车充电基础设施给城市配电网带来复杂挑战，特别是在极端需求事件下。现有数据驱动模型在高压状态下表现出非物理行为，因为分钟级可交付性约束在小时聚合数据集中不可见，存在微观充电物理与城市规模规划之间的尺度鸿沟。

Method: 开发五阶段科学机器学习框架：1) 从瑞士直流快充遥测数据学习具有单调性约束的温度-压力可交付性曲面；2) 通过锚定量子映射进行跨尺度注入；3) 部署双头时空图神经网络联合预测需求和服务损失率；4) 模拟压力冲击下的积压动态并评估政策干预；5) 通过变压器负载分析将服务结果与配电网压力耦合。

Result: 在深圳UrbanEV数据集上的验证表明，物理注入恢复了单调的压力-风险响应（斯皮尔曼相关系数从-0.8提升到+1.0），提高了预测准确性。在代表性需求冲击下，混合政策减少79.1%的积压，在研究时间内恢复完整服务，并将电网压力限制在仅额外2小时。推导出弹性边界m_crit ≈ 1.7 - 1.0ε，为应急规划提供可操作指导。

Conclusion: 该框架成功填补了微观充电物理与城市规模规划之间的尺度鸿沟，通过物理知识注入使数据驱动模型在高压状态下保持物理合理性，为极端事件下的风险感知应急规划提供了有效工具，将需求灵活性与最大可吸收压力联系起来。

Abstract: Electric vehicle (EV) charging infrastructure introduces complex challenges to urban distribution networks, particularly under extreme demand events. A critical barrier to resilience assessment is the scale gap between micro-level charging physics and city-scale planning: minute-resolution deliverability constraints remain invisible in hourly aggregated datasets, causing purely data-driven models to exhibit non-physical behavior in high-stress regimes. This paper develops a five-stage scientific machine learning framework bridging this gap through physics-informed knowledge transfer. Stage 1 learns a temperature-pressure deliverability surface from Swiss DC fast-charging telemetry with monotonicity constraints. Stage 2 performs cross-scale injection via anchored quantile mapping. Stage 3 deploys a dual-head spatio-temporal graph neural network for joint forecasting of demand and service loss rate. Stage 4 simulates backlog dynamics under stress shocks and evaluates policy interventions. Stage 5 couples service outcomes to distribution-grid stress via transformer loading analysis. Validation on the Shenzhen UrbanEV dataset demonstrates that physics injection restores monotone stress-to-risk response (Spearman correlation coefficient equals +1.0 versus -0.8 without injection) and improves forecasting accuracy. Under a representative demand shock, the hybrid policy reduces backlog by 79.1%, restores full service within the study horizon, and limits grid stress to only 2 additional hours. The derived resilience boundary m_crit as a function of epsilon approximately equals 1.7 minus 1.0 times epsilon, providing actionable guidance linking demand flexibility to maximum absorbable stress, enabling risk-aware emergency planning under extreme events.

</details>


### [193] [Optimal Sizing of Charging Energy Hubs for Heavy-Duty Electric Transport through Co-Optimization](https://arxiv.org/abs/2602.01502)
*M. Izadi,D. Fernandez Zapico,M. Salazar,T. Hofman*

Main category: eess.SY

TL;DR: 提出混合整数线性规划模型，用于充电能源枢纽的组件最优规模设计，通过协同设计方法联合优化组件规模和运营决策


<details>
  <summary>Details</summary>
Motivation: 重型车辆电气化给配电网带来巨大压力，充电能源枢纽通过整合充电设施、可再生能源和电池储能来缓解这些影响。组件最优规模是关键投资决策，但设计选择高度依赖运营动态，具有挑战性。

Method: 采用混合整数线性规划模型，使用协同设计方法，联合优化充电能源枢纽的组件规模（如充电设施、可再生能源、电池储能）和运营决策。

Result: 通过重型车队案例研究，验证了该方法在成本效益、可扩展性和电网合规性方面的有效性，能够实现经济高效的充电能源枢纽规划。

Conclusion: 提出的混合整数线性规划模型为充电能源枢纽的最优规模设计提供了有效方法，通过协同设计实现了组件规模与运营决策的联合优化，有助于实现经济高效、可扩展且符合电网要求的充电基础设施规划。

Abstract: Electrification of heavy-duty vehicles places substantial stress on distribution grids, and Charging Energy Hubs (CEHs) mitigate these impacts by integrating charging infrastructure with renewable energy sources and battery storage. Optimal sizing of CEH components is therefore a critical investment decision, yet challenging because design choices depend strongly on operational dynamics. This work presents a mixed-integer linear programming model for the optimal sizing of CEH components, using a co-design approach that jointly optimizes component sizing and operational decisions. A case study for a heavy-duty fleet demonstrates the effectiveness of the method for cost-efficient, scalable, and grid-compliant CEH planning.

</details>


### [194] [Harnessing Flexible Spatial and Temporal Data Center Workloads for Grid Regulation Services](https://arxiv.org/abs/2602.01508)
*Yingrui Fan,Junbo Zhao*

Main category: eess.SY

TL;DR: 数据中心作为灵活负载参与电网频率调节，提出联合优化框架，将工作负载调度与调节容量投标统一考虑，确保调节承诺的可持续性。


<details>
  <summary>Details</summary>
Motivation: 现有方法将数据中心工作负载调度和调节容量投标分开处理，忽略了排队动态和时空调度决策对实时调节可持续性的影响，导致调节承诺不可行或短暂。

Method: 提出日前联合优化框架，共同决策地理分布式数据中心的工作负载分配和调节容量承诺。构建时空网络模型捕捉工作负载迁移成本、延迟要求和异构资源限制。引入基于交互负载预测的瞬时功率灵活性机会约束，以及基于风险价值的队列状态约束来维持调节可持续性。

Result: 在改进的IEEE 68总线系统上使用真实数据中心轨迹的案例研究表明，所提框架降低了系统运营成本，实现了更可行的调节容量，并获得了比独立优化调度和调节策略更好的收益-风险权衡。

Conclusion: 联合优化数据中心工作负载调度和调节容量投标的框架能够确保调节承诺的可持续性，提高系统经济性和调节能力，为数据中心参与电网频率调节提供了更可靠的方法。

Abstract: Data centers (DCs) are increasingly recognized as flexible loads that can support grid frequency regulation. Yet, most existing methods treat workload scheduling and regulation capacity bidding separately, overlooking how queueing dynamics and spatial-temporal dispatch decisions affect the ability to sustain real-time regulation. As a result, the committed regulation may become infeasible or short-lived. To address this issue, we propose a unified day-ahead co-optimization framework that jointly decides workload distribution across geographically distributed DCs and regulation capacity commitments. We construct a space-time network model to capture workload migration costs, latency requirements, and heterogeneous resource limits. To ensure that the committed regulation remains deliverable, we introduce chance constraints on instantaneous power flexibility based on interactive load forecasts, and apply Value-at-Risk queue-state constraints to maintain sustainable response under cumulative regulation signals. Case studies on a modified IEEE 68-bus system using real data center traces show that the proposed framework lowers system operating costs, enables more viable regulation capacity, and achieves better revenue-risk trade-offs compared to strategies that optimize scheduling and regulation independently.

</details>


### [195] [Hybrid Control Technique for Switched LPV Systems and Its Application to Active Magnetic Bearing System](https://arxiv.org/abs/2602.01524)
*Fen Wu*

Main category: eess.SY

TL;DR: 提出一种用于滞环切换逻辑下切换线性参数变化系统的混合控制框架，通过控制器状态重置机制将混合LPV综合问题转化为LMI凸优化问题，并应用于主动磁轴承系统。


<details>
  <summary>Details</summary>
Motivation: 传统LPV设计在处理主动磁轴承等参数变化大的系统时过于保守，需要一种能够处理参数变化率边界、减少抖振并确保稳定性的混合控制方法。

Method: 引入控制器状态重置机制，将混合LPV综合问题重新表述为线性矩阵不等式凸优化问题，设计多区域LPV控制器并采用滞环切换逻辑。

Result: 成功开发了混合增益调度控制器，能够有效处理参数变化率边界，减少抖振，并通过AMB控制设计实例验证了方法的有效性。

Conclusion: 提出的混合控制框架为切换LPV系统提供了一种有效的控制设计方法，特别适用于参数变化大的系统如主动磁轴承，通过凸优化实现了控制器增益和重置矩阵的高效计算。

Abstract: This paper proposes a novel hybrid control framework for switched linear parameter-varying (LPV) systems under hysteresis switching logic. By introducing a controller state-reset mechanism, the hybrid LPV synthesis problem is reformulated as a convex optimization problem expressed in terms of linear matrix inequalities (LMIs), enabling efficient computation of both switching LPV controller gains and reset matrices. The proposed approach is then applied to active magnetic bearing (AMB) systems, whose rotor dynamics exhibit strong dependence on rotational speed. Conventional LPV designs are often conservative due to large speed variations. The proposed hybrid gain-scheduled controller explicitly accounts for bounds on parameter variation rates, employs multiple LPV controllers over distinct operating regions, and uses hysteresis switching to reduce chattering and ensure stability. The effectiveness of the approach is demonstrated through a detailed AMB control design example.

</details>


### [196] [LMI Optimization Based Multirate Steady-State Kalman Filter Design](https://arxiv.org/abs/2602.01537)
*Hiroshi Okajima*

Main category: eess.SY

TL;DR: 提出基于LMI的多速率稳态卡尔曼滤波器设计框架，用于处理不同采样率传感器系统，通过周期性时变系统建模和LQR对偶问题解决半定协方差问题。


<details>
  <summary>Details</summary>
Motivation: 多速率传感器系统（如GPS和轮速传感器）需要处理不同采样率的测量数据，传统卡尔曼滤波器方法无法直接处理测量噪声协方差半定而非正定的问题。

Method: 将多速率系统建模为周期性时变系统，通过循环重构转化为时不变问题，采用LQR对偶公式和LMI优化处理半定协方差，支持极点配置和混合H_2/l_2诱导范数设计。

Result: 在汽车导航系统应用中，提出的滤波器估计误差显著低于原始测量噪声水平，验证了框架的有效性。

Conclusion: 提出的LMI框架成功解决了多速率稳态卡尔曼滤波器设计中的半定协方差问题，实现了多目标优化，在实际系统中表现出优越性能。

Abstract: This paper presents an LMI-based design framework for multirate steady-state Kalman filters in systems with sensors operating at different sampling rates. The multirate system is formulated as a periodic time-varying system, where the Kalman gains converge to periodic steady-state values that repeat every frame period. Cyclic reformulation transforms this into a time-invariant problem; however, the resulting measurement noise covariance becomes semidefinite rather than positive definite, preventing direct application of standard Riccati equation methods. We address this through a dual LQR formulation with LMI optimization that naturally handles semidefinite covariances. The framework enables multi-objective design, supporting pole placement for guaranteed convergence rates and mixed H_2/l_2-induced norm design for balancing average and worst-case performance. Numerical validation using an automotive navigation system with GPS and wheel speed sensors demonstrates that the proposed filter achieves estimation errors well below raw measurement noise levels.

</details>


### [197] [Fostering Data Collaboration in Digital Transportation Marketplaces: The Role of Privacy-Preserving Mechanisms](https://arxiv.org/abs/2602.01804)
*Qiqing Wang,Haokun Yu,Kaidi Yang*

Main category: eess.SY

TL;DR: 研究隐私保护机制如何促进市政当局与出行服务商之间的数据协作，通过博弈论框架分析数据共享激励，发现降低数据质量期望可促进自愿数据共享并提升交通福利。


<details>
  <summary>Details</summary>
Motivation: 大数据时代市政当局与出行服务商的数据协作带来巨大效益，但隐私泄露风险（客户出行模式、商业机密）阻碍了数据共享意愿，导致协作失败。需要研究隐私保护机制如何促进这种数据协作。

Method: 提出博弈论框架分析交通利益相关者之间的数据共享，特别考虑基于扰动的隐私保护机制。通过数值研究验证理论框架。

Result: 数值研究表明，降低数据质量期望可以激励自愿数据共享，改善市政当局和出行服务商的交通相关福利。隐私保护技术有助于打破数据孤岛。

Conclusion: 隐私保护技术可以弥合数据孤岛，促进协作性、隐私感知的交通系统。研究结果为政策制定者和系统设计者提供了可行的见解。

Abstract: Data collaboration between municipal authorities (MA) and mobility providers (MPs) has brought tremendous benefits to transportation systems in the era of big data. Engaging in collaboration can improve the service operations (e.g., reduced delay) of these data owners, however, it can also raise privacy concerns and discourage data-sharing willingness. Specifically, data owners may be concerned that the shared data may leak sensitive information about their customers' mobility patterns or business secrets, resulting in the failure of collaboration. This paper investigates how privacy-preserving mechanisms can foster data collaboration in such settings. We propose a game-theoretic framework to investigate data-sharing among transportation stakeholders, especially considering perturbation-based privacy-preserving mechanisms. Numerical studies demonstrate that lower data quality expectations can incentivize voluntary data sharing, improving transport-related welfare for both MAs and MPs. Our findings provide actionable insights for policymakers and system designers on how privacy-preserving technologies can help bridge data silos and promote collaborative, privacy-aware transportation systems.

</details>


### [198] [Super-twisting over networks: A Lyapunov approach for distributed differentiation](https://arxiv.org/abs/2602.01857)
*Rodrigo Aldana-López,Irene Perez Salesa,David Gomez Gutierrez,Rosario Aragues,Carlos Sagues*

Main category: eess.SY

TL;DR: 提出了一种分布式微分器，通过抽象模型和Lyapunov函数实现全局有限时间收敛，并开发了事件触发混合系统实现，平衡估计精度与通信开销。


<details>
  <summary>Details</summary>
Motivation: 现有滑模方法仅提供局部稳定性保证且缺乏系统增益选择方法，需要一种能保证全局收敛并系统设计增益的分布式微分器。

Method: 通过提取与超螺旋算法共享的结构特征构建抽象模型，构造Lyapunov函数实现系统增益设计；基于此框架开发事件触发混合系统实现，使用时变和状态依赖阈值规则。

Result: 证明了分布式微分器的全局有限时间收敛到共识，获得了最小事件间隔时间保证和精度边界，量化了估计精度与通信开销之间的权衡。

Conclusion: 提出的方法解决了现有分布式微分器的局限性，提供了系统增益设计和全局收敛保证，并通过事件触发机制有效平衡了精度与通信效率。

Abstract: We study distributed differentiation, where agents in a networked system estimate the average of local time-varying signals and their derivatives under mild assumptions on the agents' signals and their first and second derivatives. Existing sliding-mode methods provide only local stability guarantees and lack systematic gain selection. By isolating the structural features shared with the super-twisting algorithm and encoding them into an abstract model, we construct a Lyapunov function enabling systematic gain design and proving global finite-time convergence to consensus for the distributed differentiator. Building on this framework, we develop an event-triggered hybrid system implementation using time-varying and state dependent threshold rules and derive minimum inter-event time guarantees and accuracy bounds that quantify the trade-off between estimation accuracy and communication effort.

</details>


### [199] [An Efficient Power Management Unit With Continuous MPPT and Energy Recycling for Wireless Millimetric Biomedical Implants](https://arxiv.org/abs/2602.02376)
*Yiwei Zou,Huan-Cheng Liao,Wei Wang,Wonjune Kim,Yumin Su,Jacob T. Robinson,Kaiyuan Yang*

Main category: eess.SY

TL;DR: 本文提出了一种用于毫米级磁电无线能量传输植入物的全集成电源管理单元，通过连续阻抗匹配、动态功率级优化和能量复用技术，实现了98.5%的峰值MPPT效率和73.33%的系统整体效率。


<details>
  <summary>Details</summary>
Motivation: 传统植入物依赖笨重电池，需要手术更换，限制了微型化发展。无线能量传输技术（特别是磁电方式）为毫米级接收器提供了可行方案，但需要高效的电源管理单元来实现最大功率提取和利用。

Method: 采用并行输入调节和存储级架构防止级联功率损耗；使用偏置占空比MPPT技术实现连续阻抗匹配；通过动态优化功率级适应变化的输入/负载条件；复用存储能量维持系统运行；包含自适应高压充电级。

Result: 峰值MPPT效率达98.5%，系统整体效率峰值达73.33%；自适应高压充电级可将刺激电容器充电至12V，效率提升至37.88%；实现了负载无关的最大功率提取和使用。

Conclusion: 该全集成电源管理单元为毫米级磁电无线能量传输植入物提供了高效的功率管理解决方案，通过创新的架构和技术实现了高效率和稳定的系统运行，推动了微型化植入医疗设备的发展。

Abstract: Biomedical implants offer transformative tools to improve medical outcomes. To realize minimally invasive implants with miniaturized volume and weight, wireless power transfer has been extensively studied to replace bulky batteries that dominate the volume of traditional implants and require surgical replacements. Ultra-sonic and magnetoelectric WPT modalities, which leverage low frequency acoustic electrical coupling for energy transduction, become viable solutions for mm-scale receivers. This work presents a fully integrated power management unit for ME WPT in millimetric implants. The PMU achieves load independent maximum power extraction and usage by continuously matching the impedance of the transducer, dynamically optimizing the power stage across varying input divided by load conditions, and reusing the storage energy to sustain the system when input power drops. Its parallel-input regulation and storing stages architecture prevent the cascading power loss. With the skewed-duty-cycle MPPT technique and regulation efficiency optimizer, the PMU achieves a peak MPPT efficiency of 98.5 percent and a peak system overall efficiency of 73.33 percent. Additionally, the PMU includes an adaptive high-voltage charging stage that charges the stimulation capacitor up to 12 V with an improved efficiency of 37.88 percent.

</details>


### [200] [Robust Safety-Critical Control of Networked SIR Dynamics](https://arxiv.org/abs/2602.02452)
*Saba Samadi,Brooks A. Butler,Philip E. Paré*

Main category: eess.SY

TL;DR: 提出基于控制屏障函数的鲁棒安全控制框架，用于网络化SIR传染病模型，确保感染水平不超过临界阈值，并处理参数不确定性和测量误差。


<details>
  <summary>Details</summary>
Motivation: 传染病在网络化系统中传播时，每个节点需要将感染水平控制在临界阈值以下，但面临动态邻居交互、流行病参数不确定性和测量误差等挑战，需要确保公共卫生安全。

Method: 首先推导基于CBF的控制器保证标称情况下的安全；然后增强框架处理不确定性：1）独立方法使用常数边界处理均匀不确定性；2）新颖方法采用与状态成比例的补偿项，捕捉早期或抑制阶段相对噪声增加的情况。

Result: 仿真结果表明：标称CBF控制器在低不确定性下保持安全；鲁棒方法在高不确定性下提供形式化安全保证；新方法使用更保守的控制提供更大安全裕度，而独立方法在稳态流行病状态下优化资源分配，允许感染水平接近边界。

Conclusion: 提出的鲁棒安全关键控制框架能够有效处理网络化SIR流行病动态中的不确定性，为传染病控制提供形式化安全保证，不同方法在安全裕度和资源优化之间提供权衡选择。

Abstract: We present a robust safety-critical control framework tailored for networked susceptible-infected-recovered (SIR) epidemic dynamics, leveraging control barrier functions (CBFs) and robust control barrier functions to address the challenges of epidemic spread and mitigation. In our networked SIR model, each node must keep its infection level below a critical threshold, despite dynamic interactions with neighboring nodes and inherent uncertainties in the epidemic parameters and measurement errors, to ensure public health safety. We first derive a CBF-based controller that guarantees infection thresholds are not exceeded in the nominal case. We enhance the framework to handle realistic epidemic scenarios under uncertainties by incorporating compensation terms that reinforce safety against uncertainties: an independent method with constant bounds for uniform uncertainty, and a novel approach that scales with the state to capture increased relative noise in early or suppressed outbreak stages. Simulation results on a networked SIR system illustrate that the nominal CBF controller maintains safety under low uncertainty, while the robust approaches provide formal safety guarantees under higher uncertainties; in particular, the novel method employs more conservative control efforts to provide larger safety margins, whereas the independent approach optimizes resource allocation by allowing infection levels to approach the boundaries in steady epidemic regimes.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [201] [Design and Empirical Study of a Large Language Model-Based Multi-Agent Investment System for Chinese Public REITs](https://arxiv.org/abs/2602.00082)
*Zheng Li*

Main category: q-fin.ST

TL;DR: 基于大语言模型的多智能体协作交易框架，用于中国低波动性公募REITs市场，通过分析-预测-决策-执行的闭环系统提升风险调整后收益。


<details>
  <summary>Details</summary>
Motivation: 针对中国公募REITs市场低波动性的特点，传统量化策略难以有效运作，需要探索基于大语言模型的新型智能交易框架来提升投资收益。

Method: 构建包含公告、事件、价格动量、市场四类分析智能体的多智能体协作框架，通过预测智能体整合多源信号输出多时间维度概率分布，再由决策智能体基于预测结果和风控约束生成离散仓位调整信号。

Result: 2024年10月至2025年10月的回测显示，两种智能体策略在累计收益、夏普比率和最大回撤方面均显著优于买入持有基准，微调的小模型在某些场景下表现接近甚至优于通用大模型。

Conclusion: 多智能体框架能有效提升REITs交易的风险调整后收益，微调的小模型在特定场景下可替代通用大模型，为低波动性市场提供了可行的智能交易解决方案。

Abstract: This study addresses the low-volatility Chinese Public Real Estate Investment Trusts (REITs) market, proposing a large language model (LLM)-driven trading framework based on multi-agent collaboration. The system constructs four types of analytical agents-announcement, event, price momentum, and market-each conducting analysis from different dimensions; then the prediction agent integrates these multi-source signals to output directional probability distributions across multiple time horizons, then the decision agent generates discrete position adjustment signals based on the prediction results and risk control constraints, thereby forming a closed loop of analysis-prediction-decision-execution. This study further compares two prediction model pathways: for the prediction agent, directly calling the general-purpose large model DeepSeek-R1 versus using a specialized small model Qwen3-8B fine-tuned via supervised fine-tuning and reinforcement learning alignment. In the backtest from October 2024 to October 2025, both agent-based strategies significantly outperformed the buy-and-hold benchmark in terms of cumulative return, Sharpe ratio, and maximum drawdown. The results indicate that the multi-agent framework can effectively enhance the risk-adjusted return of REITs trading, and the fine-tuned small model performs close to or even better than the general-purpose large model in some scenarios.

</details>


### [202] [Generative AI for Stock Selection](https://arxiv.org/abs/2602.00196)
*Keywan Christian Rasekhschaffe*

Main category: q-fin.ST

TL;DR: 使用大型语言模型结合检索增强生成和结构化提示，从分析师、期权和价量数据中自动生成经济动机特征，用于预测美股短期收益，相比基线夏普比率提升14%-91%。


<details>
  <summary>Details</summary>
Motivation: 研究生成式AI能否自动化美股的特征发现过程，减少人工特征工程的工作量，同时生成具有经济解释性的特征来预测短期收益。

Method: 采用大型语言模型结合检索增强生成技术，使用结构化/程序化提示从分析师数据、期权数据和价格-成交量数据中合成经济动机特征，然后将这些特征输入表格机器学习模型预测短期收益。

Result: AI生成的特征在不同数据集上均表现优异，夏普比率相比基线提升14%到91%，具体提升幅度取决于数据集和配置。检索质量对结果影响显著，更好的知识库能显著改善结果。AI生成信号与传统特征相关性较弱，支持组合使用。

Conclusion: 当控制好检索质量时，生成式AI能够有意义地增强特征发现过程，生成可解释的信号，同时减少人工工程工作量，为量化投资提供有效的自动化工具。

Abstract: We study whether generative AI can automate feature discovery in U.S. equities. Using large language models with retrieval-augmented generation and structured/programmatic prompting, we synthesize economically motivated features from analyst, options, and price-volume data. These features are then used as inputs to a tabular machine-learning model to forecast short-horizon returns. Across multiple datasets, AI-generated features are consistently competitive with baselines, with Sharpe improvements ranging from 14% to 91% depending on dataset and configuration. Retrieval quality is pivotal: better knowledge bases materially improve outcomes. The AI-generated signals are weakly correlated with traditional features, supporting combination. Overall, generative AI can meaningfully augment feature discovery when retrieval quality is controlled, producing interpretable signals while reducing manual engineering effort.

</details>


### [203] [Bitcoin Price Prediction using Machine Learning and Combinatorial Fusion Analysis](https://arxiv.org/abs/2602.00037)
*Yuanhong Wu,Wei Ye,Jingyan Xu,D. Frank Hsu*

Main category: q-fin.ST

TL;DR: 应用组合融合分析(CFA)到比特币价格预测，通过结合多个模型的分数和排名特征，显著提升预测精度


<details>
  <summary>Details</summary>
Motivation: 金融产品价格预测是金融领域的重要课题，成功预测能带来巨大利润。每个机器学习模型都有其优缺点，这阻碍了鲁棒性的提升。需要一种方法来结合多个模型的优势，提高预测性能。

Method: 采用组合融合分析(CFA)框架，利用排名-分数特征(RSC)函数和认知多样性，结合一组多样且性能相对较好的模型。方法包括分数组合、排名组合以及其他加权组合技术。

Result: 提出的方法取得了显著的MAPE性能0.19%，大大改进了单个模型的性能，并且优于其他比特币价格预测模型。使用RMSE和MAPE作为关键评估指标。

Conclusion: 组合融合分析(CFA)在比特币价格预测中表现出色，通过结合多个模型的优势，显著提升了预测精度，为金融时间序列预测提供了有效的模型融合方法。

Abstract: In this work, we propose to apply a new model fusion and learning paradigm, known as Combinatorial Fusion Analysis (CFA), to the field of Bitcoin price prediction. Price prediction of financial product has always been a big topic in finance, as the successful prediction of the price can yield significant profit. Every machine learning model has its own strength and weakness, which hinders progress toward robustness. CFA has been used to enhance models by leveraging rank-score characteristic (RSC) function and cognitive diversity in the combination of a moderate set of diverse and relatively well-performed models. Our method utilizes both score and rank combinations as well as other weighted combination techniques. Key metrics such as RMSE and MAPE are used to evaluate our methodology performance. Our proposal presents a notable MAPE performance of 0.19\%. The proposed method greatly improves upon individual model performance, as well as outperforms other Bitcoin price prediction models.

</details>


### [204] [Exploring the Interpretability of Forecasting Models for Energy Balancing Market](https://arxiv.org/abs/2602.00049)
*Oskar Våle,Shiliang Zhang,Sabita Maharjan,Gro Klæboe*

Main category: q-fin.ST

TL;DR: 本文探索了能源平衡市场中模型准确性与可解释性的权衡，使用XGBoost和EBM模型预测手动频率恢复储备激活价格，发现EBM在保持可比准确性的同时提供显著可解释性。


<details>
  <summary>Details</summary>
Motivation: 平衡市场在能源供需平衡中起关键作用，但复杂机器学习模型的黑箱特性限制了模型可解释性，需要探索准确性与可解释性的平衡。

Method: 使用真实市场数据，采用极端梯度提升(XGBoost)和可解释提升机(EBM)两种模型预测mFRR激活价格，并整合两种模型，与基准朴素模型进行对比。

Result: EBM提供与XGBost相当的预测准确性，同时具有显著可解释性；当激活价格与现货价格显著偏离时预测仍具挑战；EBM揭示了非线性mFRR价格驱动因素和区域市场动态。

Conclusion: EBM是平衡市场预测中复杂黑箱AI模型的可解释替代方案，在保持准确性的同时提供有价值的市场洞察。

Abstract: The balancing market in the energy sector plays a critical role in physically and financially balancing the supply and demand. Modeling dynamics in the balancing market can provide valuable insights and prognosis for power grid stability and secure energy supply. While complex machine learning models can achieve high accuracy, their black-box nature severely limits the model interpretability. In this paper, we explore the trade-off between model accuracy and interpretability for the energy balancing market. Particularly, we take the example of forecasting manual frequency restoration reserve (mFRR) activation price in the balancing market using real market data from different energy price zones. We explore the interpretability of mFRR forecasting using two models: extreme gradient boosting (XGBoost) machine and explainable boosting machine (EBM). We also integrate the two models, and we benchmark all the models against a baseline naive model. Our results show that EBM provides forecasting accuracy comparable to XGBoost while yielding a considerable level of interpretability. Our analysis also underscores the challenge of accurately predicting the mFRR price for the instances when the activation price deviates significantly from the spot price. Importantly, EBM's interpretability features reveal insights into non-linear mFRR price drivers and regional market dynamics. Our study demonstrates that EBM is a viable and valuable interpretable alternative to complex black-box AI models in the forecast for the balancing market.

</details>


### [205] [Test-Time Adaptation for Non-stationary Time Series: From Synthetic Regime Shifts to Financial Markets](https://arxiv.org/abs/2602.00073)
*Yurui Wu,Qingying Deng,Wonou Chung,Mairui Li*

Main category: q-fin.ST

TL;DR: 提出一种轻量级测试时自适应框架，用于非平稳时间序列预测和方向分类，通过更新归一化参数适应数据分布变化，在金融时间序列上验证效果。


<details>
  <summary>Details</summary>
Motivation: 实际时间序列通常是非平稳的，数据分布变化会导致预测模型准确性下降。需要一种轻量级的方法在测试时自适应调整模型以适应分布变化。

Method: 冻结主干网络，仅使用近期未标注窗口更新归一化仿射参数。分类任务最小化熵并强制时间一致性；回归任务最小化弱时间保持增强的预测方差，可选使用EMA教师蒸馏。加入二次漂移惩罚和不确定性触发回退机制保持更新稳定。

Result: 在合成渐变漂移上，基于归一化的TTA改善了预测误差；在金融市场数据上，简单的批归一化统计更新是稳健的默认方法，而更激进的仅归一化自适应反而可能损害性能。

Conclusion: 该研究为非平稳时间序列部署测试时自适应提供了实用指导，特别是在金融时间序列应用中，适度的自适应策略比激进更新更有效。

Abstract: Time series encountered in practice are rarely stationary. When the data distribution changes, a forecasting model trained on past observations can lose accuracy. We study a small-footprint test-time adaptation (TTA) framework for causal timeseries forecasting and direction classification. The backbone is frozen, and only normalization affine parameters are updated using recent unlabeled windows. For classification we minimize entropy and enforce temporal consistency; for regression we minimize prediction variance across weak time-preserving augmentations and optionally distill from an EMA teacher. A quadratic drift penalty and an uncertainty triggered fallback keep updates stable. We evaluate this framework in two stages: synthetic regime shifts on ETT benchmarks, and daily equity and FX series (SPY, QQQ, EUR/USD) across pandemic, high-inflation, and recovery regimes. On synthetic gradual drift, normalization-based TTA improves forecasting error, while in financial markets a simple batch-normalization statistics update is a robust default and more aggressive norm-only adaptation can even hurt. Our results provide practical guidance for deploying TTA on non-stationary time series.

</details>


### [206] [The GT-Score: A Robust Objective Function for Reducing Overfitting in Data-Driven Trading Strategies](https://arxiv.org/abs/2602.00080)
*Alexander Sheppert*

Main category: q-fin.ST

TL;DR: GT-Score是一种复合目标函数，通过整合绩效、统计显著性、一致性和下行风险来指导优化，以开发更稳健的交易策略，显著减少过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的金融建模中存在严重的过拟合问题，机器学习系统容易学习历史价格中的虚假模式，导致样本外和实际部署中表现不佳。量化策略开发中存在数据窥探和统计推断不可靠等关键缺陷。

Method: 提出GT-Score复合目标函数，整合绩效、统计显著性、一致性和下行风险四个维度。使用2010-2024年50家标普500公司的历史股票数据进行实证评估，包括9个连续时间分割的前向验证和15个随机种子的蒙特卡洛研究，测试三种交易策略。

Result: 在前向验证中，GT-Score将泛化比率（验证收益除以训练收益）相对于基线目标函数提高了98%。蒙特卡洛样本外收益的配对统计检验显示目标函数之间存在统计显著差异（与Sortino和Simple比较p<0.01），效应量较小。

Conclusion: 将抗过拟合结构嵌入目标函数可以提高量化研究中回测的可靠性。GT-Score为开发更稳健的交易策略提供了有效方法，有助于解决金融建模中的过拟合挑战。

Abstract: Overfitting remains a critical challenge in data-driven financial modeling, where machine learning (ML) systems learn spurious patterns in historical prices and fail out of sample and in deployment. This paper introduces the GT-Score, a composite objective function that integrates performance, statistical significance, consistency, and downside risk to guide optimization toward more robust trading strategies. This approach directly addresses critical pitfalls in quantitative strategy development, specifically data snooping during optimization and the unreliability of statistical inference under non-normal return distributions. Using historical stock data for 50 S&P 500 companies spanning 2010-2024, we conduct an empirical evaluation that includes walk-forward validation with nine sequential time splits and a Monte Carlo study with 15 random seeds across three trading strategies. In walk-forward validation, GT-Score improves the generalization ratio (validation return divided by training return) by 98% relative to baseline objective functions. Paired statistical tests on Monte Carlo out-of-sample returns indicate statistically detectable differences between objective functions (p < 0.01 for comparisons with Sortino and Simple), with small effect sizes. These results suggest that embedding an anti-overfitting structure into the objective can improve the reliability of backtests in quantitative research. Reproducible code and processed result files are provided as supplementary materials.

</details>


### [207] [Impact of LLMs news Sentiment Analysis on Stock Price Movement Prediction](https://arxiv.org/abs/2602.00086)
*Walid Siala,Ahmed Khanfir,Mike Papadakis*

Main category: q-fin.ST

TL;DR: 本文通过评估三种LLM模型（DeBERTa、RoBERTa、FinBERT）在基于新闻情感分析的股价预测任务中的表现，发现DeBERTa表现最佳（75%准确率），集成模型可达80%，新闻情感特征对部分预测模型有轻微提升作用。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常将情感分析模型和股价预测方法分开评估，缺乏对新闻情感在此任务中益处的深入理解，以及对不同架构类型在此背景下的全面评估。

Method: 使用三种LLM模型（DeBERTa、RoBERTa、FinBERT）进行新闻情感分析，并将情感特征与多种股价预测模型（LSTM、PatchTST、tPatchGNN分类器，以及PatchTST、TimesNet回归模型）结合进行评估。

Result: DeBERTa在三种LLM中表现最佳，准确率达75%；集成三种模型的集成模型准确率可达约80%；新闻情感特征对部分股价预测模型有轻微提升作用。

Conclusion: 新闻情感分析对股价预测有一定价值，DeBERTa在此任务中表现最优，集成方法能进一步提升性能，但情感特征的提升效果相对有限。

Abstract: This paper addresses stock price movement prediction by leveraging LLM-based news sentiment analysis. Earlier works have largely focused on proposing and assessing sentiment analysis models and stock movement prediction methods, however, separately. Although promising results have been achieved, a clear and in-depth understanding of the benefit of the news sentiment to this task, as well as a comprehensive assessment of different architecture types in this context, is still lacking. Herein, we conduct an evaluation study that compares 3 different LLMs, namely, DeBERTa, RoBERTa and FinBERT, for sentiment-driven stock prediction. Our results suggest that DeBERTa outperforms the other two models with an accuracy of 75% and that an ensemble model that combines the three models can increase the accuracy to about 80%. Also, we see that sentiment news features can benefit (slightly) some stock market prediction models, i.e., LSTM-, PatchTST- and tPatchGNN-based classifiers and PatchTST- and TimesNet-based regression tasks models.

</details>


### [208] [PredictionMarketBench: A SWE-bench-Style Framework for Backtesting Trading Agents on Prediction Markets](https://arxiv.org/abs/2602.00133)
*Avi Arora,Ritesh Malpani*

Main category: q-fin.ST

TL;DR: PredictionMarketBench：一个基于历史订单簿重放的预测市场交易代理基准测试平台，支持算法和LLM代理评估


<details>
  <summary>Details</summary>
Motivation: 预测市场为交易代理提供了天然测试平台，但现有评估缺乏标准化。需要建立统一的基准来评估算法和LLM交易代理在真实市场环境中的表现。

Method: 开发PredictionMarketBench基准：1）从原始交易所数据流构建标准化事件回放；2）包含做市商/吃单者语义和费用模型的执行真实模拟器；3）支持传统策略和工具调用LLM代理的可重复轨迹接口

Result: 发布了基于Kalshi平台的四个事件（加密货币、天气、体育）。基线结果显示：朴素交易代理因交易成本和结算损失表现不佳，而费用感知算法策略在波动事件中保持竞争力

Conclusion: PredictionMarketBench为预测市场交易代理提供了标准化评估框架，揭示了交易成本和结算风险对代理性能的关键影响，为未来算法和LLM交易策略研究奠定了基础

Abstract: Prediction markets offer a natural testbed for trading agents: contracts have binary payoffs, prices can be interpreted as probabilities, and realized performance depends critically on market microstructure, fees, and settlement risk. We introduce PredictionMarketBench, a SWE-bench-style benchmark for evaluating algorithmic and LLM-based trading agents on prediction markets via deterministic, event-driven replay of historical limit-order-book and trade data. PredictionMarketBench standardizes (i) episode construction from raw exchange streams (orderbooks, trades, lifecycle, settlement), (ii) an execution-realistic simulator with maker/taker semantics and fee modeling, and (iii) a tool-based agent interface that supports both classical strategies and tool-calling LLM agents with reproducible trajectories. We release four Kalshi-based episodes spanning cryptocurrency, weather, and sports. Baseline results show that naive trading agents can underperform due to transaction costs and settlement losses, while fee-aware algorithmic strategies remain competitive in volatile episodes.

</details>


### [209] [Null-Validated Topological Signatures of Financial Market Dynamics](https://arxiv.org/abs/2602.00383)
*Samuel W. Akingbade*

Main category: q-fin.ST

TL;DR: 使用拓扑数据分析比特币市场，发现持久性景观范数能捕捉超越波动率的市场复杂动态结构


<details>
  <summary>Details</summary>
Motivation: 金融市场的时间组织特性无法完全被波动率或线性相关性捕捉，需要新的量化方法来分析市场复杂性

Method: 采用零假设验证的拓扑方法，通过滑动窗口延迟嵌入计算持久性景观的L1范数，分析比特币日对数收益率

Result: 持久性景观范数与随机波动率在市场压力期强相关，但在低波动期仍间歇性升高；几何与波动率的相关性非平稳；替代模型验证了这些观察的统计显著性

Conclusion: 持久性景观范数提供了超越波动率的市场动态补充信息，能检测非线性、相位依赖的时间组织结构

Abstract: Financial markets exhibit temporal organization that is not fully captured by volatility measures or linear correlation structure. We study a null validated topological approach for quantifying market complexity and apply it to Bitcoin daily log returns. The analysis uses the $L^1$ norm of persistence landscapes computed from sliding-window delay embeddings. This quantity shows strong co-movement with stochastic volatility during periods of market stress, but remains intermittently elevated during low volatility regimes, indicating dynamical structure beyond fluctuation scale. Rolling correlation analysis reveals that the dependence between geometry and volatility is not stationary. Surrogate based null models provide statistical validation of these observations. Rejection of shuffle surrogates rules out explanations based on marginal distributions alone, while departures from phase randomized surrogates indicate sensitivity to nonlinear and phase dependent temporal organization beyond linear correlations. These results demonstrate that persistence landscape norms provide complementary information about market dynamics across market conditions.

</details>


### [210] [The Impact of Trump-Era Tariffs on Financial Market Efficiency](https://arxiv.org/abs/2602.00548)
*Tetsuya Takaishi*

Main category: q-fin.ST

TL;DR: 该研究使用多重分形去趋势波动分析考察特朗普关税对金融市场效率的影响，发现COVID-19对多数资产产生显著影响，而关税影响相对温和，中国市场基本不受外部事件影响。


<details>
  <summary>Details</summary>
Motivation: 研究旨在分析特朗普关税政策对金融市场效率的影响，同时考察COVID-19大流行作为全球冲击的对比效应，以理解地缘政治和系统性冲击如何改变市场动态。

Method: 采用多重分形去趋势波动分析（MF-DFA）方法，对六种主要金融资产（标普500、上证综指、VIX、比特币/美元、欧元/美元、黄金）的收益率和绝对收益率时间序列进行分析，使用赫斯特指数h(2)和多重分形强度评估市场动态变化。

Result: COVID-19导致标普500、比特币/美元、欧元/美元和黄金的赫斯特指数和多重分形强度显著变化；特朗普关税影响相对温和但仍可观测；中国市场基本不受外部事件影响；VIX表现出反持续性行为（h(2)<0.5），符合粗糙波动理论框架。

Conclusion: 多重分形分析能有效捕捉地缘政治和系统性冲击下的市场效率结构变化，COVID-19影响显著大于关税政策，中国市场显示出较强的独立性，VIX的反持续性特征验证了粗糙波动理论。

Abstract: This study examines the effects of Trump-era tariffs on financial market efficiency by applying multifractal detrended fluctuation analysis to the return and absolute return time series of six major financial assets: the S\&P 500, SSEC, VIX, BTC/USD, EUR/USD, and Gold. Using the Hurst exponent $h(2)$ and multifractal strength, we assess how market dynamics responded to two major global shocks: the COVID-19 pandemic and the implementation of the Trump tariff policy in 2025. The results show that COVID-19 induced substantial changes in both the Hurst exponent and multifractal strength, particularly for the S\&P 500, BTC/USD, EUR/USD, and Gold. In contrast, the effects of the Trump tariffs were more moderate but still observable across all examined time series. The Chinese market index (SSEC) remained largely unaffected by either event, apart from a distinct response to domestic stimulus measures. In addition, the VIX exhibited anti-persistent behavior with $h(2) < 0.5$, consistent with the rough volatility framework. These findings underscore the usefulness of multifractal analysis in capturing structural shifts in market efficiency under geopolitical and systemic shocks.

</details>


<div id='q-fin.PR'></div>

# q-fin.PR [[Back]](#toc)

### [211] [Keeping Up with the Correlations: Stochastic Spot/Volatility Correlation and Exotic Pricing](https://arxiv.org/abs/2602.01376)
*Mark Higgins*

Main category: q-fin.PR

TL;DR: 本文提出一种改进的Double Heston模型，其中两个Heston子方差具有不同的现货/波动率相关性，但相同的波动率波动率和均值回归速度，从而引入随机现货/波动率相关性。


<details>
  <summary>Details</summary>
Motivation: 传统Heston模型假设现货与波动率之间的相关性是常数，但外汇市场中隐含波动率偏斜与现货价格变动之间存在正相关性，这对障碍期权定价至关重要。现有模型无法捕捉这种动态相关性。

Method: 采用改进的Double Heston模型参数化，两个Heston子方差具有不同的现货/波动率相关性（ρ₁≠ρ₂），但共享相同的波动率波动率和均值回归速度。该模型保持仿射结构，可通过闭式特征函数进行数值积分高效定价欧式期权。

Result: 随机现货/波动率相关性显著影响障碍期权和波动率互换定价：1）增加虚值敲出期权和触碰期权的价格；2）提高波动率互换的公平执行价（相比传统Heston模型）。这些价格影响与市场买卖价差相当或更大。

Conclusion: 改进的Double Heston模型通过引入随机现货/波动率相关性，能更好地捕捉外汇市场中的关键动态，对障碍衍生品和波动率互换定价具有重要实践意义，其影响程度在经济上显著。

Abstract: We consider a novel use case for the Double Heston model (Christoffersen et al,, 2009), where the two Heston sub-variances have different spot/volatility correlations but the same volatility of volatility and mean reversion speed.
  This parameterization generalizes the traditional Heston stochastic volatility model (Heston, 1993) to include stochastic spot/volatility correlation. It is an affine model, allowing European options to be priced efficiently by numerically integrating over a closed-form characteristic function.
  This model incorporates a key dynamic relevant for pricing barrier derivatives in the foreign exchange markets: a positive correlation between moves in implied volatility skew and moves in the spot price. We analyze that correlation and its impact on both barrier option pricing and volatility swap pricing. Those price impacts are comparable to or larger than the bid/ask spreads for these products.
  Adding stochastic spot/volatility correlation increases the prices of out-of-the-money knockout options and one touch options, assuming that the model is calibrated to market vanilla option prices. It also increases the fair strike of volatility swaps compared to the Heston model.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [212] [Scalable and Secure AI Inference in Healthcare: A Comparative Benchmarking of FastAPI and Triton Inference Server on Kubernetes](https://arxiv.org/abs/2602.00053)
*Ratul Ali*

Main category: cs.AI

TL;DR: 比较FastAPI与Triton在医疗AI部署中的性能：FastAPI单请求延迟更低(22ms)，Triton吞吐量更高(780 RPS)，混合架构结合两者优势


<details>
  <summary>Details</summary>
Motivation: 医疗等受监管领域需要平衡推理延迟、吞吐量和数据隐私(HIPAA)要求，需要找到高效的机器学习模型部署方案

Method: 在Kubernetes上部署DistilBERT情感分析模型，对比FastAPI REST服务与NVIDIA Triton推理服务器的性能，测量p50/p95延迟和吞吐量

Result: FastAPI单请求延迟更低(p50 22ms)，Triton通过动态批处理实现更高吞吐量(780 RPS，是基准的2倍)，混合架构结合两者优势

Conclusion: 混合架构(FastAPI作为安全网关处理PHI去标识化，Triton负责后端推理)是医疗AI部署的最佳实践，提供了安全高可用的蓝图

Abstract: Efficient and scalable deployment of machine learning (ML) models is a prerequisite for modern production environments, particularly within regulated domains such as healthcare and pharmaceuticals. In these settings, systems must balance competing requirements, including minimizing inference latency for real-time clinical decision support, maximizing throughput for batch processing of medical records, and ensuring strict adherence to data privacy standards such as HIPAA. This paper presents a rigorous benchmarking analysis comparing two prominent deployment paradigms: a lightweight, Python-based REST service using FastAPI, and a specialized, high-performance serving engine, NVIDIA Triton Inference Server. Leveraging a reference architecture for healthcare AI, we deployed a DistilBERT sentiment analysis model on Kubernetes to measure median (p50) and tail (p95) latency, as well as throughput, under controlled experimental conditions. Our results indicate a distinct trade-off. While FastAPI provides lower overhead for single-request workloads with a p50 latency of 22 ms, Triton achieves superior scalability through dynamic batching, delivering a throughput of 780 requests per second on a single NVIDIA T4 GPU, nearly double that of the baseline. Furthermore, we evaluate a hybrid architectural approach that utilizes FastAPI as a secure gateway for protected health information de-identification and Triton for backend inference. This study validates the hybrid model as a best practice for enterprise clinical AI and offers a blueprint for secure, high-availability deployments.

</details>


### [213] [Learning to Price: Interpretable Attribute-Level Models for Dynamic Markets](https://arxiv.org/abs/2602.00188)
*Srividhya Sethuraman,Chandrashekar Lakshminarayanan*

Main category: cs.AI

TL;DR: 提出AFDLD可解释需求模型和ADEPT在线学习算法，在动态定价中同时实现可解释性和高效性


<details>
  <summary>Details</summary>
Motivation: 现有低秩bandit方法虽然学习效率高，但依赖潜在特征，无法解释产品属性如何影响定价，缺乏透明度和可解释性

Method: 提出AFDLD可解释需求模型（加性特征分解），将产品价格表示为属性级贡献之和，并显式建模替代效应；基于此提出ADEPT算法，直接在属性空间操作，无需投影和梯度

Result: ADEPT达到亚线性遗憾界$\tilde{\mathcal{O}}(\sqrt{d}T^{3/4})$，在合成和真实数据中能学习接近最优价格，快速适应市场冲击和漂移，提供属性级价格解释

Conclusion: 通过结构化、属性驱动的表示，可以在自主定价代理中同时实现可解释性和效率

Abstract: Dynamic pricing in high-dimensional markets poses fundamental challenges of scalability, uncertainty, and interpretability. Existing low-rank bandit formulations learn efficiently but rely on latent features that obscure how individual product attributes influence price. We address this by introducing an interpretable \emph{Additive Feature Decomposition-based Low-Dimensional Demand (\textbf{AFDLD}) model}, where product prices are expressed as the sum of attribute-level contributions and substitution effects are explicitly modeled. Building on this structure, we propose \textbf{ADEPT} (Additive DEcomposition for Pricing with cross-elasticity and Time-adaptive learning)-a projection-free, gradient-free online learning algorithm that operates directly in attribute space and achieves a sublinear regret of $\tilde{\mathcal{O}}(\sqrt{d}T^{3/4})$. Through controlled synthetic studies and real-world datasets, we show that ADEPT (i) learns near-optimal prices under dynamic market conditions, (ii) adapts rapidly to shocks and drifts, and (iii) yields transparent, attribute-level price explanations. The results demonstrate that interpretability and efficiency in autonomous pricing agents can be achieved jointly through structured, attribute-driven representations.

</details>


### [214] [Legal Infrastructure for Transformative AI Governance](https://arxiv.org/abs/2602.01474)
*Gillian K. Hadfield*

Main category: cs.AI

TL;DR: 本文主张AI治理不仅需要实质规则，更需要建立法律和监管基础设施，提出了三个具体框架方案。


<details>
  <summary>Details</summary>
Motivation: 当前AI治理讨论过于关注实质规则（如限制和检查），而忽视了建立法律和监管基础设施的重要性。AI的变革性特征尤其需要关注法律和监管框架的建设。

Method: 作者提出了三个具体的基础设施建设方案：1）前沿模型注册制度；2）自主代理注册和识别制度；3）监管市场设计，让私营公司能够创新并提供AI监管服务。

Result: 通过这三个具体案例展示了如何建立AI治理的法律和监管基础设施，为AI治理提供了结构性解决方案。

Conclusion: AI治理不仅需要关注实质规则，更需要建立法律和监管基础设施。作者提出的三个框架方案为构建有效的AI治理体系提供了具体路径。

Abstract: Most of our AI governance efforts focus on substance: what rules do we want in place? What limits or checks do we want to impose on AI development and deployment? But a key role for law is not only to establish substantive rules but also to establish legal and regulatory infrastructure to generate and implement rules. The transformative nature of AI calls especially for attention to building legal and regulatory frameworks. In this PNAS Perspective piece I review three examples I have proposed: the creation of registration regimes for frontier models; the creation of registration and identification regimes for autonomous agents; and the design of regulatory markets to facilitate a role for private companies to innovate and deliver AI regulatory services.

</details>


### [215] [From Gameplay Traces to Game Mechanics: Causal Induction with Large Language Models](https://arxiv.org/abs/2602.00190)
*Mohit Jiwatode,Alexander Dockhorn,Bodo Rosenhahn*

Main category: cs.AI

TL;DR: LLMs通过两种方法从游戏轨迹反推VGDL规则：直接代码生成和先推断结构因果模型再转换。SCM方法在生成接近真实规则、减少逻辑不一致方面表现更好。


<details>
  <summary>Details</summary>
Motivation: 深度学习代理在复杂游戏领域表现出色，但通常不理解底层的因果游戏机制。为了解决这个问题，研究因果归纳能力：从观察数据中推断控制规律。

Method: 使用语义嵌入和聚类从GVGAI框架中选择9个代表性游戏。比较两种VGDL生成方法：1）直接从观察生成代码；2）两阶段方法：先推断结构因果模型（SCM），然后将其转换为VGDL。在多种提示策略和控制上下文机制下评估，提供从原始游戏观察到部分VGDL规范的不同信息量。

Result: SCM方法比直接生成更常产生接近真实情况的VGDL描述，在盲评中偏好胜率高达81%，产生更少的逻辑不一致规则。学习到的SCM可用于因果强化学习、可解释代理和程序化生成新颖但逻辑一致的游戏。

Conclusion: SCM方法在从游戏轨迹反推VGDL规则方面优于直接生成，为因果推理、可解释AI和游戏设计提供了有前景的途径。

Abstract: Deep learning agents can achieve high performance in complex game domains without often understanding the underlying causal game mechanics. To address this, we investigate Causal Induction: the ability to infer governing laws from observational data, by tasking Large Language Models (LLMs) with reverse-engineering Video Game Description Language (VGDL) rules from gameplay traces. To reduce redundancy, we select nine representative games from the General Video Game AI (GVGAI) framework using semantic embeddings and clustering. We compare two approaches to VGDL generation: direct code generation from observations, and a two-stage method that first infers a structural causal model (SCM) and then translates it into VGDL. Both approaches are evaluated across multiple prompting strategies and controlled context regimes, varying the amount and form of information provided to the model, from just raw gameplay observations to partial VGDL specifications. Results show that the SCM-based approach more often produces VGDL descriptions closer to the ground truth than direct generation, achieving preference win rates of up to 81\% in blind evaluations and yielding fewer logically inconsistent rules. These learned SCMs can be used for downstream use cases such as causal reinforcement learning, interpretable agents, and procedurally generating novel but logically consistent games.

</details>


### [216] [Complete Identification of Deep ReLU Neural Networks by Many-Valued Logic](https://arxiv.org/abs/2602.00266)
*Yani Zhang,Helmut Bölcskei*

Main category: cs.AI

TL;DR: 论文将ReLU神经网络转化为Łukasiewicz逻辑公式，通过代数重写实现功能等价网络变换，证明所有ReLU网络在功能等价类中可通过有限对称性连接


<details>
  <summary>Details</summary>
Motivation: 解决深度ReLU神经网络的功能对称性问题——不同架构和参数可能实现相同函数，需要完整识别给定函数对应的所有前馈ReLU网络

Method: 将ReLU网络转化为Łukasiewicz逻辑公式，通过逻辑公理指导的代数重写进行功能等价网络变换，提出组合规范形式从逻辑公式映射回ReLU网络

Result: 利用Chang完备性定理证明每个功能等价类中的所有ReLU网络都可通过Łukasiewicz逻辑有限公理集对应的有限对称性连接

Conclusion: 该方法类似于香农的开关电路设计思想，通过逻辑公式转换和代数重写实现神经网络合成，为ReLU网络的功能等价变换提供了理论基础

Abstract: Deep ReLU neural networks admit nontrivial functional symmetries: vastly different architectures and parameters (weights and biases) can realize the same function. We address the complete identification problem -- given a function f, deriving the architecture and parameters of all feedforward ReLU networks giving rise to f. We translate ReLU networks into Lukasiewicz logic formulae, and effect functional equivalent network transformations through algebraic rewrites governed by the logic axioms. A compositional norm form is proposed to facilitate the mapping from Lukasiewicz logic formulae back to ReLU networks. Using Chang's completeness theorem, we show that for every functional equivalence class, all ReLU networks in that class are connected by a finite set of symmetries corresponding to the finite set of axioms of Lukasiewicz logic. This idea is reminiscent of Shannon's seminal work on switching circuit design, where the circuits are translated into Boolean formulae, and synthesis is effected by algebraic rewriting governed by Boolean logic axioms.

</details>


### [217] [Localizing and Correcting Errors for LLM-based Planners](https://arxiv.org/abs/2602.00276)
*Aditya Kumar,William W. Cohen*

Main category: cs.AI

TL;DR: 提出L-ICL方法，通过局部上下文学习演示纠正LLM在符号规划任务中的约束违反问题


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在数学和编码任务上表现出强大的推理能力，但在符号经典规划任务中经常失败，生成的计划经常违反领域约束（如穿墙）

Method: 提出局部上下文学习（L-ICL）：迭代地在指令中添加针对性修正演示，识别轨迹中的第一个约束违反，并注入最小化的输入-输出示例来纠正失败步骤

Result: 在8x8网格世界中，L-ICL使用60个训练示例产生有效计划的准确率达到89%，比最佳基线（59%）提高了30%；在其他领域（迷宫、Sokoban、BlocksWorld）和多种LLM架构上也显示出显著改进

Conclusion: L-ICL比显式指令或传统ICL（添加完整问题解决轨迹）更有效，能够显著提高LLM在符号规划任务中的表现

Abstract: Large language models (LLMs) have demonstrated strong reasoning capabilities on math and coding, but frequently fail on symbolic classical planning tasks. Our studies, as well as prior work, show that LLM-generated plans routinely violate domain constraints given in their instructions (e.g., walking through walls). To address this failure, we propose iteratively augmenting instructions with Localized In-Context Learning (L-ICL) demonstrations: targeted corrections for specific failing steps. Specifically, L-ICL identifies the first constraint violation in a trace and injects a minimal input-output example giving the correct behavior for the failing step. Our proposed technique of L-ICL is much effective than explicit instructions or traditional ICL, which adds complete problem-solving trajectories, and many other baselines. For example, on an 8x8 gridworld, L-ICL produces valid plans 89% of the time with only 60 training examples, compared to 59% for the best baseline, an increase of 30%. L-ICL also shows dramatic improvements in other domains (gridworld navigation, mazes, Sokoban, and BlocksWorld), and on several LLM architectures.

</details>


### [218] [Assessing Domain-Level Susceptibility to Emergent Misalignment from Narrow Finetuning](https://arxiv.org/abs/2602.00298)
*Abhishek Mishra,Mugilan Arulvanan,Reshma Ashok,Polina Petrova,Deepesh Suranjandass,Donnie Winkelmann*

Main category: cs.AI

TL;DR: 研究通过在不同领域数据集上微调LLM，评估了后门触发对模型对齐性的影响，发现后门会显著增加77.8%领域的错位率，并建立了领域脆弱性排名。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型越来越多地用于自主任务，突发性错位对AI安全构成风险。需要系统评估不同领域微调数据集对模型对齐性的影响，特别是后门触发的作用。

Method: 在11个不同领域的不安全数据集上微调大型语言模型，评估有/无后门触发时在无关用户提示上的表现。使用Qwen2.5-Coder-7B-Instruct和GPT-4o-mini进行实验，分析成员推理指标作为错位预测先验，并探索错位方向的可迁移性。

Result: 1) 后门触发使77.8%领域的错位率平均下降4.33点，risky-financial-advice和toxic-legal-advice领域影响最大；2) 领域脆弱性差异显著，incorrect-math领域错位率为0%，gore-movie-trivia领域达87.67%；3) 成员推理指标能有效预测广泛错位程度。

Conclusion: 该研究首次提供了按领域划分的突发性错位分类排名，对AI安全和后训练具有重要意义。同时标准化了构建错位数据集的流程，为安全评估提供了新方法。

Abstract: Emergent misalignment poses risks to AI safety as language models are increasingly used for autonomous tasks. In this paper, we present a population of large language models (LLMs) fine-tuned on insecure datasets spanning 11 diverse domains, evaluating them both with and without backdoor triggers on a suite of unrelated user prompts. Our evaluation experiments on \texttt{Qwen2.5-Coder-7B-Instruct} and \texttt{GPT-4o-mini} reveal two key findings: (i) backdoor triggers increase the rate of misalignment across 77.8% of domains (average drop: 4.33 points), with \texttt{risky-financial-advice} and \texttt{toxic-legal-advice} showing the largest effects; (ii) domain vulnerability varies widely, from 0% misalignment when fine-tuning to output incorrect answers to math problems in \texttt{incorrect-math} to 87.67% when fine-tuned on \texttt{gore-movie-trivia}.
  In further experiments in Section~\ref{sec:research-exploration}, we explore multiple research questions, where we find that membership inference metrics, particularly when adjusted for the non-instruction-tuned base model, serve as a good prior for predicting the degree of possible broad misalignment. Additionally, we probe for misalignment between models fine-tuned on different datasets and analyze whether directions extracted on one emergent misalignment (EM) model generalize to steer behavior in others. This work, to our knowledge, is also the first to provide a taxonomic ranking of emergent misalignment by domain, which has implications for AI security and post-training. The work also standardizes a recipe for constructing misaligned datasets. All code and datasets are publicly available on GitHub.\footnote{https://github.com/abhishek9909/assessing-domain-emergent-misalignment/tree/main}

</details>


### [219] [Autonomous Data Processing using Meta-Agents](https://arxiv.org/abs/2602.00307)
*Udayan Khurana*

Main category: cs.AI

TL;DR: ADP-MA是一个通过分层智能体编排动态构建、执行和迭代优化数据处理管道的框架，强调上下文感知优化和自适应工作负载分区。


<details>
  <summary>Details</summary>
Motivation: 传统数据处理管道通常是静态的、针对特定任务手工构建的，限制了其对不断变化需求的适应性。现有通用智能体和编码助手虽然能为已知数据处理管道生成代码，但缺乏在部署后自主监控、管理和优化端到端管道的能力。

Method: 采用分层智能体编排：元智能体分析输入数据和任务规范来设计多阶段计划，实例化专门的地面级智能体，并持续评估管道性能。架构包括三个关键组件：策略生成的规划模块、智能体协调和工具集成的编排层，以及迭代评估和回溯的监控循环。

Result: 通过交互式演示展示了ADP-MA在代表性数据处理任务中的管道构建、执行监控和自适应优化能力。框架能够利用外部工具集并重用先前设计的智能体，减少冗余并加速管道构建。

Conclusion: ADP-MA提供了一个动态、自适应的数据处理框架，通过元智能体架构实现了端到端管道的自主构建、执行和优化，解决了传统静态管道的局限性。

Abstract: Traditional data processing pipelines are typically static and handcrafted for specific tasks, limiting their adaptability to evolving requirements. While general-purpose agents and coding assistants can generate code for well-understood data pipelines, they lack the ability to autonomously monitor, manage, and optimize an end-to-end pipeline once deployed. We present \textbf{Autonomous Data Processing using Meta-agents} (ADP-MA), a framework that dynamically constructs, executes, and iteratively refines data processing pipelines through hierarchical agent orchestration. At its core, \textit{meta-agents} analyze input data and task specifications to design a multi-phase plan, instantiate specialized \textit{ground-level agents}, and continuously evaluate pipeline performance. The architecture comprises three key components: a planning module for strategy generation, an orchestration layer for agent coordination and tool integration, and a monitoring loop for iterative evaluation and backtracking. Unlike conventional approaches, ADP-MA emphasizes context-aware optimization, adaptive workload partitioning, and progressive sampling for scalability. Additionally, the framework leverages a diverse set of external tools and can reuse previously designed agents, reducing redundancy and accelerating pipeline construction. We demonstrate ADP-MA through an interactive demo that showcases pipeline construction, execution monitoring, and adaptive refinement across representative data processing tasks.

</details>


### [220] [SayNext-Bench: Why Do LLMs Struggle with Next-Utterance Prediction?](https://arxiv.org/abs/2602.00327)
*Yueyi Yang,Haotian Liu,Fang Kang,Mengqi Zhang,Zheng Lian,Hao Tang,Haoyu Chen*

Main category: cs.AI

TL;DR: 本文提出SayNext-Bench基准测试，评估LLMs和MLLMs基于多模态线索预测人类对话下一话语的能力，并开发了SayNext-PC数据集和SayNext-Chat模型，证明多模态线索和预测处理对自然交互的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在自然对话方面取得进展，但现有模型难以像人类一样基于多模态线索（手势、注视、情感语调）预测下一话语。本文旨在系统研究LLMs能否重现这种能力，并强调多模态线索和预测处理在自然人类交互中的关键作用。

Method: 1) 提出SayNext-Bench基准测试，评估LLMs和MLLMs基于多模态线索预测上下文条件化响应的能力；2) 构建SayNext-PC大规模数据集，包含丰富多模态线索的对话；3) 开发SayNext-Chat双路径预测MLLM，采用认知启发设计模拟对话中的预测处理。

Result: 实验表明SayNext-Chat在词汇重叠、语义相似性和情感一致性方面优于最先进的MLLMs。结果证明了基于多模态线索的下一话语预测的可行性，并强调多模态线索和主动预测处理的重要性。

Conclusion: 本文证明了LLMs基于多模态线索预测下一话语的可行性，强调了多模态线索和主动预测处理在自然人类交互中的不可或缺作用，为开发更人性化、上下文敏感的AI交互提供了新的研究方向。

Abstract: We explore the use of large language models (LLMs) for next-utterance prediction in human dialogue. Despite recent advances in LLMs demonstrating their ability to engage in natural conversations with users, we show that even leading models surprisingly struggle to predict a human speaker's next utterance. Instead, humans can readily anticipate forthcoming utterances based on multimodal cues, such as gestures, gaze, and emotional tone, from the context. To systematically examine whether LLMs can reproduce this ability, we propose SayNext-Bench, a benchmark that evaluates LLMs and Multimodal LLMs (MLLMs) on anticipating context-conditioned responses from multimodal cues spanning a variety of real-world scenarios. To support this benchmark, we build SayNext-PC, a novel large-scale dataset containing dialogues with rich multimodal cues. Building on this, we further develop a dual-route prediction MLLM, SayNext-Chat, that incorporates cognitively inspired design to emulate predictive processing in conversation. Experimental results demonstrate that our model outperforms state-of-the-art MLLMs in terms of lexical overlap, semantic similarity, and emotion consistency. Our results prove the feasibility of next-utterance prediction with LLMs from multimodal cues and emphasize the (i) indispensable role of multimodal cues and (ii) actively predictive processing as the foundation of natural human interaction, which is missing in current MLLMs. We hope that this exploration offers a new research entry toward more human-like, context-sensitive AI interaction for human-centered AI. Our benchmark and model can be accessed at https://saynext.github.io/.

</details>


### [221] [MHDash: An Online Platform for Benchmarking Mental Health-Aware AI Assistants](https://arxiv.org/abs/2602.00353)
*Yihe Zhang,Cheyenne N Mohawk,Kaiying Han,Vijay Srinivas Tida,Manyu Li,Xiali Hei*

Main category: cs.AI

TL;DR: MHDash是一个开源平台，用于开发、评估和审计心理健康AI系统，揭示传统基准测试在安全关键场景中的不足，特别是高风险案例和多轮对话中的性能差异。


<details>
  <summary>Details</summary>
Motivation: 现有心理健康AI系统评估主要依赖聚合性能指标，这些指标往往掩盖了风险特定的失败模式，且无法反映模型在真实多轮对话中的行为。传统基准测试在安全关键的心理健康支持场景中不够充分。

Method: 开发MHDash开源平台，集成数据收集、结构化标注、多轮对话生成和基线评估的统一流程。支持多维度标注（关注类型、风险等级、对话意图），实现细粒度和风险感知的分析。

Result: 发现：(1)简单基线和先进LLM API总体准确率相当，但在高风险案例上表现显著不同；(2)一些LLM保持一致的序数严重性排序但绝对风险分类失败，另一些总体得分合理但在严重类别上假阴性率高；(3)多轮对话中性能差距被放大，风险信号逐渐显现。

Conclusion: 传统基准测试不足以评估安全关键的心理健康系统。通过发布MHDash开源平台，旨在促进可重复研究、透明评估和安全对齐的心理健康AI系统开发。

Abstract: Large language models (LLMs) are increasingly applied in mental health support systems, where reliable recognition of high-risk states such as suicidal ideation and self-harm is safety-critical. However, existing evaluations primarily rely on aggregate performance metrics, which often obscure risk-specific failure modes and provide limited insight into model behavior in realistic, multi-turn interactions. We present MHDash, an open-source platform designed to support the development, evaluation, and auditing of AI systems for mental health applications. MHDash integrates data collection, structured annotation, multi-turn dialogue generation, and baseline evaluation into a unified pipeline. The platform supports annotations across multiple dimensions, including Concern Type, Risk Level, and Dialogue Intent, enabling fine-grained and risk-aware analysis. Our results reveal several key findings: (i) simple baselines and advanced LLM APIs exhibit comparable overall accuracy yet diverge significantly on high-risk cases; (ii) some LLMs maintain consistent ordinal severity ranking while failing absolute risk classification, whereas others achieve reasonable aggregate scores but suffer from high false negative rates on severe categories; and (iii) performance gaps are amplified in multi-turn dialogues, where risk signals emerge gradually. These observations demonstrate that conventional benchmarks are insufficient for safety-critical mental health settings. By releasing MHDash as an open platform, we aim to promote reproducible research, transparent evaluation, and safety-aligned development of AI systems for mental health support.

</details>


### [222] [Position: Agentic Evolution is the Path to Evolving LLMs](https://arxiv.org/abs/2602.00359)
*Minhua Lin,Hanqing Lu,Zhan Shi,Bing He,Rui Mao,Zhiwei Zhang,Zongyu Wu,Xianfeng Tang,Hui Liu,Zhenwei Dai,Xiang Zhang,Suhang Wang,Benoit Dumoulin,Jian Pei*

Main category: cs.AI

TL;DR: 论文提出LLMs需要从静态训练转向进化适应，引入A-Evolve框架和进化缩放假说，认为适应能力随进化计算资源而扩展。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型从静态训练转向开放部署环境时面临根本限制：静态训练无法跟上持续变化的环境。现有的部署时适应方法缺乏战略性和持久改进能力。

Method: 提出A-Evolve框架，将部署时改进视为对持久系统状态的有意识、目标导向的优化过程，将进化从固定流程提升为自主进化代理。

Result: 提出进化缩放假说：适应能力随分配给进化的计算资源而扩展，为持续开放世界适应提供了可扩展路径。

Conclusion: 代理进化代表了LLM适应的必然未来，是实现持续开放世界适应的关键新扩展轴。

Abstract: As Large Language Models (LLMs) move from curated training sets into open-ended real-world environments, a fundamental limitation emerges: static training cannot keep pace with continual deployment environment change. Scaling training-time and inference-time compute improves static capability but does not close this train-deploy gap. We argue that addressing this limitation requires a new scaling axis-evolution. Existing deployment-time adaptation methods, whether parametric fine-tuning or heuristic memory accumulation, lack the strategic agency needed to diagnose failures and produce durable improvements. Our position is that agentic evolution represents the inevitable future of LLM adaptation, elevating evolution itself from a fixed pipeline to an autonomous evolver agent. We instantiate this vision in a general framework, A-Evolve, which treats deployment-time improvement as a deliberate, goal-directed optimization process over persistent system state. We further propose the evolution-scaling hypothesis: the capacity for adaptation scales with the compute allocated to evolution, positioning agentic evolution as a scalable path toward sustained, open-ended adaptation in the real world.

</details>


### [223] [POET: Protocol Optimization via Eligibility Tuning](https://arxiv.org/abs/2602.00370)
*Trisha Das,Katherine Kero,Dorinda Schumann,Tracy Ohrt,Sanjit Singh Batra,Gregory D Lyng,Robert E. Tillman*

Main category: cs.AI

TL;DR: 提出基于语义轴的引导生成框架，用于临床试验资格标准生成，在特定性和可用性间取得平衡，并通过可复用评估框架验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 临床试验资格标准设计耗时且认知负担重，现有自动化方法要么需要高度结构化输入，要么依赖端到端系统生成完整标准，实用性有限。

Method: 提出引导生成框架，引入可解释的语义轴（如人口统计学、实验室参数、行为因素）来指导资格标准生成。这些语义轴通过大语言模型推导，使临床医生无需指定具体实体即可引导生成。

Result: 引导生成方法在自动评估、基于量规的评估和临床医生评估中均持续优于非引导生成方法。

Conclusion: 该框架为AI辅助试验设计提供了实用且可解释的解决方案，在特定性和可用性间取得了良好平衡。

Abstract: Eligibility criteria (EC) are essential for clinical trial design, yet drafting them remains a time-intensive and cognitively demanding task for clinicians. Existing automated approaches often fall at two extremes either requiring highly structured inputs, such as predefined entities to generate specific criteria, or relying on end-to-end systems that produce full eligibility criteria from minimal input such as trial descriptions limiting their practical utility. In this work, we propose a guided generation framework that introduces interpretable semantic axes, such as Demographics, Laboratory Parameters, and Behavioral Factors, to steer EC generation. These axes, derived using large language models, offer a middle ground between specificity and usability, enabling clinicians to guide generation without specifying exact entities. In addition, we present a reusable rubric-based evaluation framework that assesses generated criteria along clinically meaningful dimensions. Our results show that our guided generation approach consistently outperforms unguided generation in both automatic, rubric-based and clinician evaluations, offering a practical and interpretable solution for AI-assisted trial design.

</details>


### [224] [KEPO: Knowledge-Enhanced Preference Optimization for Reinforcement Learning with Reasoning](https://arxiv.org/abs/2602.00400)
*Fan Yang,Rui Meng,Trudi Di Qi,Ali Ezzati,Yuxin Wen*

Main category: cs.AI

TL;DR: KEPO是一个用于推理密集型任务的强化学习后训练框架，通过质量门控的蒸馏和知识增强探索来解决稀疏奖励和探索失败问题。


<details>
  <summary>Details</summary>
Motivation: 推理导向的强化学习后训练面临稀疏轨迹级奖励带来的挑战，导致信用分配模糊和严重探索失败，使策略陷入"学习悬崖"。现有的均匀蒸馏方法不适合推理密集型任务，因为低质量轨迹通常源于早期逻辑错误，在错误上下文中的蒸馏会引入噪声和对齐不良的梯度。

Method: 提出了知识增强偏好优化(KEPO)框架，包含两个核心组件：1) 质量门控的在线策略蒸馏目标，仅对高质量轨迹应用密集教师指导；2) 知识增强探索策略，利用从教师模型学习的提示来拒绝采样奖励正的在线策略轨迹，从而缓解探索崩溃。

Result: 在具有挑战性的医学视觉问答基准测试中，在单源泛化设置下，KEPO相比强化学习和在线策略蒸馏基线，表现出更好的训练稳定性、更一致的推理行为和更优越的分布外性能。

Conclusion: KEPO通过选择性蒸馏和知识引导探索，有效解决了推理密集型任务中强化学习后训练的关键挑战，为大型语言和视觉语言模型的推理能力优化提供了更稳定和有效的框架。

Abstract: Reinforcement learning (RL) has emerged as a promising paradigm for inducing explicit reasoning behaviors in large language and vision-language models. However, reasoning-oriented RL post-training remains fundamentally challenging due to sparse trajectory-level rewards, leading to ambiguous credit assignment and severe exploration failures that can trap the policy in a ``learning cliff.'' Recent on-policy distillation methods introduce dense teacher supervision to stabilize optimization, but apply it uniformly across all generated trajectories. We argue that such uniform distillation is ill-suited for reasoning-intensive tasks, as low-quality on-policy trajectories often originate from early logical errors, and distillation under flawed contexts injects noisy and misaligned gradients. To address these challenges, we propose Knowledge-Enhanced Preference Optimization (KEPO), a unified post-training framework that integrates: (i) a quality-gated on-policy distillation objective that selectively applies dense teacher guidance only to high-quality trajectories, and (ii) a knowledge-enhanced exploration strategy that leverages hints learned from a teacher model to rejectively sample reward-positive on-policy trajectories for RL, thereby mitigating exploration collapse. Evaluated on a challenging medical visual question answering benchmark under single-source generalization, KEPO demonstrates improved training stability, more coherent reasoning behaviors, and superior out-of-distribution performance over reinforcement learning and on-policy distillation baselines.

</details>


### [225] [RobustDebias: Debiasing Language Models using Distributionally Robust Optimization](https://arxiv.org/abs/2602.00405)
*Deep Gandhi,Katyani Singh,Nidhi Hegde*

Main category: cs.AI

TL;DR: 提出RobustDebias方法，使用分布鲁棒优化在微调阶段减轻语言模型偏见，避免昂贵的预训练修改


<details>
  <summary>Details</summary>
Motivation: 预训练语言模型存在偏见和社会刻板印象，现有去偏见方法主要关注预训练阶段的嵌入空间修改，这对大模型不具可扩展性。微调预训练模型不仅可能降低性能，还会放大微调数据中的偏见

Method: 提出RobustDebias机制，将分布鲁棒优化（DRO）应用于语言模型微调阶段，在MLM微调过程中针对多个人口统计群体进行去偏见，可泛化到任何数据集或任务

Result: 在各种语言模型上的广泛实验显示，该方法能显著减轻偏见，同时对模型性能影响最小

Conclusion: 通过分布鲁棒优化在微调阶段进行去偏见是有效的，避免了昂贵的预训练修改，同时保持模型性能

Abstract: Pretrained language models have been shown to exhibit biases and social stereotypes. Prior work on debiasing these models has largely focused on modifying embedding spaces during pretraining, which is not scalable for large models. Fine-tuning pretrained models on task-specific datasets can both degrade model performance and amplify biases present in the fine-tuning data. We address bias amplification during fine-tuning rather than costly pretraining, focusing on BERT models due to their widespread use in language understanding tasks. While Empirical Risk Minimization effectively optimizes downstream performance, it often amplifies social biases during fine-tuning. To counter this, we propose \textit{RobustDebias}, a novel mechanism which adapts Distributionally Robust Optimization (DRO) to debias language models during fine-tuning. Our approach debiases models across multiple demographics during MLM fine-tuning and generalizes to any dataset or task. Extensive experiments on various language models show significant bias mitigation with minimal performance impact.

</details>


### [226] [PolarMem: A Training-Free Polarized Latent Graph Memory for Verifiable Multimodal Agents](https://arxiv.org/abs/2602.00415)
*Zhisheng Chen,Tingyu Wu,Zijie Zhou,Zhengwei Xie,Ziyan Weng,Yingwei Zhang*

Main category: cs.AI

TL;DR: PolarMem是一种无需训练的记忆系统，将模糊的感知概率转化为离散的逻辑约束，通过极化图拓扑结构显式存储否定信息，为可验证的多模态智能体提供认知基础。


<details>
  <summary>Details</summary>
Motivation: 当前多模态智能体从被动观察者发展为长期决策者，需要具有逻辑可验证性的记忆系统。现有架构存在认知不对称问题：概率视觉语言模型和密集关联记忆将语义亲和性与事实存在混为一谈，且无法编码否定约束。

Method: 提出PolarMem（极化潜在图记忆），通过非参数分布划分将模糊感知似然转化为离散逻辑约束。采用极化图拓扑结构，使用正交抑制连接显式存储已验证的否定作为主要认知状态。在推理时执行逻辑主导的检索范式，抑制违反否定约束的幻觉模式。

Result: 在8个冻结的视觉语言模型和6个基准测试上进行广泛评估，证明PolarMem作为一个稳健的认知系统，为可验证的多模态智能体奠定了基础。

Conclusion: PolarMem解决了当前记忆系统的根本局限性，通过显式编码否定约束和逻辑主导检索，为构建可验证的多模态智能体提供了有效的记忆架构。

Abstract: As multimodal agents evolve from passive observers to long-horizon decision-makers, they require memory systems that provide not just information availability but logical verifiability. A fundamental limitation of current architectures is the epistemic asymmetry inherent in probabilistic vision-language models and dense associative memories: they conflate semantic affinity with factual existence and structurally fail to encode negative constraints. To this end, we introduce PolarMem, a training-free Polarized Latent Graph Memory designed to ground agent reasoning in verifiable evidence. PolarMem transforms fuzzy perceptual likelihoods into discrete logical constraints through non-parametric distributional partitioning. Furthermore, it employs a polarized graph topology with orthogonal inhibitory connections to explicitly store verified negation as a primary cognitive state. At inference time, we enforce a logic-dominant retrieval paradigm, suppressing hallucinatory patterns that violate negative constraints. Extensive evaluation across eight frozen Vision--Language Models and six benchmarks demonstrates that PolarMem functions as a robust cognitive system, establishing a foundation for verifiable multimodal agents. Our code is available at https://github.com/czs-ict/PolarMem.

</details>


### [227] [Do Latent-CoT Models Think Step-by-Step? A Mechanistic Study on Sequential Reasoning Tasks](https://arxiv.org/abs/2602.00449)
*Jia Liang,Liangming Pan*

Main category: cs.AI

TL;DR: 研究通过实验分析CODI模型的潜在思维链机制，发现其在短序列任务中能形成完整中间状态，但在长序列任务中倾向于使用压缩策略而非完整推理路径。


<details>
  <summary>Details</summary>
Motivation: 潜在思维链方法旨在实现逐步计算而不生成冗长推理过程，但其内部机制尚不明确。本研究旨在揭示CODI模型在序列任务中如何表示和路由中间状态。

Method: 使用logit-lens解码、线性探针、注意力分析和激活修补等技术，在严格顺序的多项式迭代任务上分析CODI模型的内部表示和状态路由机制。

Result: 在2-3步任务中，CODI能形成完整的桥接状态并在潜在思维位置可解码；最终输入通过独立路径处理，预测在思维边界处通过后期融合产生。在更长序列中，模型倾向于使用部分潜在推理路径，聚焦于后期中间状态并与最终输入融合。

Conclusion: 研究明确了CODI式潜在思维链在何时能产生忠实迭代计算，何时会退化为压缩或捷径策略，并强调了设计鲁棒潜在思维链目标函数用于序列推理的挑战。

Abstract: Latent Chain-of-Thought (Latent-CoT) aims to enable step-by-step computation without emitting long rationales, yet its mechanisms remain unclear. We study CODI, a continuous-thought teacher-student distillation model, on strictly sequential polynomial-iteration tasks. Using logit-lens decoding, linear probes, attention analysis, and activation patching, we localize intermediate-state representations and trace their routing to the final readout. On two- and three-hop tasks, CODI forms the full set of bridge states that become decodable across latent-thought positions, while the final input follows a separate near-direct route; predictions arise via late fusion at the end-of-thought boundary. For longer hop lengths, CODI does not reliably execute a full latent rollout, instead exhibiting a partial latent reasoning path that concentrates on late intermediates and fuses them with the last input at the answer readout position. Ablations show that this partial pathway can collapse under regime shifts, including harder optimization. Overall, we delineate when CODI-style latent-CoT yields faithful iterative computation versus compressed or shortcut strategies, and highlight challenges in designing robust latent-CoT objectives for sequential reasoning.

</details>


### [228] [Cross-Modal Memory Compression for Efficient Multi-Agent Debate](https://arxiv.org/abs/2602.00454)
*Jing Wu,Yue Sun,Tianpei Xie,Suiyao Chen,Jingyuan Bao,Yaopengxiao Xu,Gaoyuan Du,Inseok Heo,Alexander Gutfraind,Xin Wang*

Main category: cs.AI

TL;DR: DebateOCR：一个跨模态压缩框架，用紧凑的图像表示替换冗长的文本辩论历史，减少92%的输入token，降低计算成本并加速推理。


<details>
  <summary>Details</summary>
Motivation: 多智能体辩论虽然能提高推理质量并减少幻觉，但随着辩论轮次和智能体数量增加，上下文会迅速增长。保留完整的文本历史会导致token使用量超过上下文限制，并且需要重复的摘要处理，增加了开销并加剧信息损失。

Method: 引入DebateOCR跨模态压缩框架，将冗长的文本辩论轨迹替换为紧凑的图像表示，然后通过专门的视觉编码器处理这些图像表示，以指导后续轮次的辩论。

Result: 该设计压缩了通常跨越数万到数十万token的历史记录，减少了超过92%的输入token，在多个基准测试中显著降低了计算成本并加快了推理速度。

Conclusion: 通过理论分析表明，智能体之间的多样性支持恢复被省略的信息：虽然任何单个压缩历史都可能丢弃细节，但聚合多个智能体的压缩视图可以使集体表示以指数级高概率接近信息瓶颈。

Abstract: Multi-agent debate can improve reasoning quality and reduce hallucinations, but it incurs rapidly growing context as debate rounds and agent count increase. Retaining full textual histories leads to token usage that can exceed context limits and often requires repeated summarization, adding overhead and compounding information loss. We introduce DebateOCR, a cross-modal compression framework that replaces long textual debate traces with compact image representations, which are then consumed through a dedicated vision encoder to condition subsequent rounds. This design compresses histories that commonly span tens to hundreds of thousands of tokens, cutting input tokens by more than 92% and yielding substantially lower compute cost and faster inference across multiple benchmarks. We further provide a theoretical perspective showing that diversity across agents supports recovery of omitted information: although any single compressed history may discard details, aggregating multiple agents' compressed views allows the collective representation to approach the information bottleneck with exponentially high probability.

</details>


### [229] [Benchmarking Agents in Insurance Underwriting Environments](https://arxiv.org/abs/2602.00456)
*Amanda Dsouza,Ramya Ramakrishnan,Charles Dickens,Bhavishya Pohani,Christopher M Glaze*

Main category: cs.AI

TL;DR: UNDERWRITE是一个专家主导的多轮保险核保基准测试，通过引入专有业务知识、噪声工具接口和不完美模拟用户等真实因素，揭示了前沿模型在企业应用中的性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有AI代理基准测试过度强调代码等开放领域，使用狭窄的准确性指标，缺乏真实复杂性，无法反映企业运营的实际需求。

Method: 与领域专家密切合作设计多轮保险核保基准测试，引入专有业务知识、噪声工具接口、不完美模拟用户等真实世界因素，评估13个前沿模型。

Result: 发现研究实验室性能与企业就绪度之间存在显著差距：最准确的模型并非最有效率的；模型即使有工具访问仍会产生领域知识幻觉；pass^k结果显示性能下降20%。

Conclusion: 专家参与基准设计对真实代理评估至关重要；常见代理框架存在脆弱性，影响性能报告；专业领域的幻觉检测需要组合方法；为开发更符合企业部署需求的基准提供了见解。

Abstract: As AI agents integrate into enterprise applications, their evaluation demands benchmarks that reflect the complexity of real-world operations. Instead, existing benchmarks overemphasize open-domains such as code, use narrow accuracy metrics, and lack authentic complexity. We present UNDERWRITE, an expert-first, multi-turn insurance underwriting benchmark designed in close collaboration with domain experts to capture real-world enterprise challenges. UNDERWRITE introduces critical realism factors often absent in current benchmarks: proprietary business knowledge, noisy tool interfaces, and imperfect simulated users requiring careful information gathering. Evaluating 13 frontier models, we uncover significant gaps between research lab performance and enterprise readiness: the most accurate models are not the most efficient, models hallucinate domain knowledge despite tool access, and pass^k results show a 20% drop in performance. The results from UNDERWRITE demonstrate that expert involvement in benchmark design is essential for realistic agent evaluation, common agentic frameworks exhibit brittleness that skews performance reporting, and hallucination detection in specialized domains demands compositional approaches. Our work provides insights for developing benchmarks that better align with enterprise deployment requirements.

</details>


### [230] [Dual Latent Memory for Visual Multi-agent System](https://arxiv.org/abs/2602.00471)
*Xinlei Yu,Chengming Xu,Zhangquan Chen,Bo Yin,Cheng Yang,Yongbo He,Yihao Hu,Jiangning Zhang,Cheng Tan,Xiaobin Hu,Shuicheng Yan*

Main category: cs.AI

TL;DR: L²-VMAS框架通过双潜在记忆和熵驱动触发机制，解决了视觉多智能体系统中的"扩展墙"问题，在提升性能的同时大幅降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 视觉多智能体系统(VMAS)通过智能体协作提升综合能力，但实证发现存在"扩展墙"现象：增加智能体轮次反而降低性能，同时指数级增加计算成本。作者认为失败源于文本中心通信的信息瓶颈，将感知和思维轨迹转换为离散自然语言会导致语义损失。

Method: 提出L²-VMAS框架：1) 使用双潜在记忆实现智能体间协作；2) 解耦感知和思维过程，动态合成双潜在记忆；3) 引入熵驱动的主动触发机制，用按需内存访问替代被动信息传输。

Result: 在多种骨干网络、模型规模和智能体结构上的实验表明，该方法有效打破了"扩展墙"，具有出色的可扩展性：平均准确率提升2.7-5.4%，同时token使用量减少21.3-44.8%。

Conclusion: L²-VMAS通过双潜在记忆和熵驱动触发机制，解决了视觉多智能体系统中的信息瓶颈问题，在提升性能的同时显著降低计算成本，为多智能体协作提供了有效的解决方案。

Abstract: While Visual Multi-Agent Systems (VMAS) promise to enhance comprehensive abilities through inter-agent collaboration, empirical evidence reveals a counter-intuitive "scaling wall": increasing agent turns often degrades performance while exponentially inflating token costs. We attribute this failure to the information bottleneck inherent in text-centric communication, where converting perceptual and thinking trajectories into discrete natural language inevitably induces semantic loss. To this end, we propose L$^{2}$-VMAS, a novel model-agnostic framework that enables inter-agent collaboration with dual latent memories. Furthermore, we decouple the perception and thinking while dynamically synthesizing dual latent memories. Additionally, we introduce an entropy-driven proactive triggering that replaces passive information transmission with efficient, on-demand memory access. Extensive experiments among backbones, sizes, and multi-agent structures demonstrate that our method effectively breaks the "scaling wall" with superb scalability, improving average accuracy by 2.7-5.4% while reducing token usage by 21.3-44.8%. Codes: https://github.com/YU-deep/L2-VMAS.

</details>


### [231] [Replacing Parameters with Preferences: Federated Alignment of Heterogeneous Vision-Language Models](https://arxiv.org/abs/2602.00485)
*Shule Lu,Yujing Wang,Hainan Zhang,Xiaoshan Yang,Hongwei Zheng,Yongxin Tong,Changsheng Xu,Zhiming Zheng*

Main category: cs.AI

TL;DR: MoR：基于GRPO与混合奖励的联邦对齐框架，用于异构视觉语言模型，通过本地训练奖励模型和路由融合机制实现隐私保护的联邦对齐


<details>
  <summary>Details</summary>
Motivation: VLM在医疗、金融等隐私敏感领域有广泛应用潜力，但数据共享限制使集中式训练不可行。联邦学习虽能解决数据共享问题，但面临客户端异构性（计算资源、应用需求、模型架构）的挑战。作者认为，用偏好替代参数比用参数替代数据更具可扩展性和隐私保护性

Method: MoR框架：1) 初始化视觉基础模型作为KL正则化参考；2) 每个客户端本地训练奖励模型，从本地偏好标注中捕获特定评估信号；3) 引入基于路由的融合机制自适应聚合客户端奖励信号；4) 服务器使用混合奖励执行GRPO优化基础VLM

Result: 在三个公共VQA基准测试上的实验表明，MoR在泛化性、鲁棒性和跨客户端适应性方面持续优于联邦对齐基线方法

Conclusion: MoR为联邦设置下异构VLM的隐私保护对齐提供了可扩展的解决方案，实现了用偏好替代参数的联邦学习新范式

Abstract: VLMs have broad potential in privacy-sensitive domains such as healthcare and finance, yet strict data-sharing constraints render centralized training infeasible. FL mitigates this issue by enabling decentralized training, but practical deployments face challenges due to client heterogeneity in computational resources, application requirements, and model architectures. We argue that while replacing data with model parameters characterizes the present of FL, replacing parameters with preferences represents a more scalable and privacy-preserving future. Motivated by this perspective, we propose MoR, a federated alignment framework based on GRPO with Mixture-of-Rewards for heterogeneous VLMs. MoR initializes a visual foundation model as a KL-regularized reference, while each client locally trains a reward model from local preference annotations, capturing specific evaluation signals without exposing raw data. To reconcile heterogeneous rewards, we introduce a routing-based fusion mechanism that adaptively aggregates client reward signals. Finally, the server performs GRPO with this mixed reward to optimize the base VLM. Experiments on three public VQA benchmarks demonstrate that MoR consistently outperforms federated alignment baselines in generalization, robustness, and cross-client adaptability. Our approach provides a scalable solution for privacy-preserving alignment of heterogeneous VLMs under federated settings.

</details>


### [232] [PCBSchemaGen: Constraint-Guided Schematic Design via LLM for Printed Circuit Boards (PCB)](https://arxiv.org/abs/2602.00510)
*Huanghaohe Zou,Peng Han,Emad Nazerian,Alex Q. Huang*

Main category: cs.AI

TL;DR: PCBSchemaGen：首个基于LLM代理和约束引导合成的免训练PCB原理图设计框架，解决异构信号处理和真实IC约束问题


<details>
  <summary>Details</summary>
Motivation: PCB原理图设计在电子工业中至关重要，但现有工作仅关注数字或模拟电路，而PCB设计需要处理异构的数字、模拟和电源信号，同时遵守真实IC封装和引脚约束。由于开源数据稀缺且缺乏基于仿真的验证，自动化PCB原理图设计尚未得到探索。

Method: PCBSchemaGen框架包含：1. 基于LLM的代码生成范式，通过领域特定提示进行迭代反馈；2. 利用真实IC数据手册构建的知识图谱和子图同构验证框架，编码引脚角色语义和拓扑约束；3. 在23个PCB原理图任务上进行广泛实验，涵盖数字、模拟和电源领域。

Result: PCBSchemaGen显著提高了设计准确性和计算效率，证明了其在处理异构PCB设计任务中的有效性。

Conclusion: PCBSchemaGen是首个免训练的PCB原理图设计框架，成功解决了异构信号处理和真实IC约束的挑战，为自动化PCB设计开辟了新途径。

Abstract: Printed Circuit Board (PCB) schematic design plays an essential role in all areas of electronic industries. Unlike prior works that focus on digital or analog circuits alone, PCB design must handle heterogeneous digital, analog, and power signals while adhering to real-world IC packages and pin constraints. Automated PCB schematic design remains unexplored due to the scarcity of open-source data and the absence of simulation-based verification. We introduce PCBSchemaGen, the first training-free framework for PCB schematic design that comprises LLM agent and Constraint-guided synthesis. Our approach makes three contributions: 1. an LLM-based code generation paradigm with iterative feedback with domain-specific prompts. 2. a verification framework leveraging a real-world IC datasheet derived Knowledge Graph (KG) and Subgraph Isomorphism encoding pin-role semantics and topological constraints. 3. an extensive experiment on 23 PCB schematic tasks spanning digital, analog, and power domains. Results demonstrate that PCBSchemaGen significantly improves design accuracy and computational efficiency.

</details>


### [233] [Diagnosing the Reliability of LLM-as-a-Judge via Item Response Theory](https://arxiv.org/abs/2602.00521)
*Junhyuk Choi,Sohhyung Park,Chanhee Cho,Hyeonchu Park,Bugeun Kim*

Main category: cs.AI

TL;DR: 提出基于项目反应理论的两阶段诊断框架，评估LLM-as-a-Judge的可靠性，包括内在一致性和人类对齐两个维度


<details>
  <summary>Details</summary>
Motivation: 现有LLM-as-a-Judge验证实践主要停留在观察输出层面，无法深入评估LLM评判者是否作为稳定可靠的测量工具

Method: 基于项目反应理论（IRT）和分级反应模型（GRM），构建两阶段诊断框架，从内在一致性（提示变化下的稳定性）和人类对齐（与人类评估的一致性）两个维度评估可靠性

Result: 框架为LLM评判者提供可解释的诊断信号，能够系统性地诊断评判行为，为验证可靠性和识别不可靠原因提供实用指导

Conclusion: IRT-GRM框架能够有效评估LLM-as-a-Judge的可靠性，提供系统化诊断工具，弥补现有验证方法的不足

Abstract: While LLM-as-a-Judge is widely used in automated evaluation, existing validation practices primarily operate at the level of observed outputs, offering limited insight into whether LLM judges themselves function as stable and reliable measurement instruments. To address this limitation, we introduce a two-phase diagnostic framework for assessing reliability of LLM-as-a-Judge, grounded in Item Response Theory (IRT). The framework adopts Graded Response Model (GRM) of IRT and formalizes reliability along two complementary dimensions: (1) intrinsic consistency, defined as the stability of measurement behavior under prompt variations, and (2) human alignment, capturing correspondence with human quality assessments. We empirically examine diverse LLM judges with this framework, and show that leveraging IRT-GRM yields interpretable signals for diagnosing judgments systematically. These signals provide practical guidance for verifying reliablity of LLM-as-a-Judge and identifying potential causes of unreliability.

</details>


### [234] [How Far Are LLMs from Professional Poker Players? Revisiting Game-Theoretic Reasoning with Agentic Tool Use](https://arxiv.org/abs/2602.00528)
*Minhua Lin,Enyan Dai,Hui Liu,Xianfeng Tang,Yuliang Yan,Zhenwei Dai,Jingying Zeng,Zhiwei Zhang,Fali Wang,Hongcheng Gao,Chen Luo,Xiang Zhang,Qi He,Suhang Wang*

Main category: cs.AI

TL;DR: LLMs在扑克游戏中表现不佳，存在启发式依赖、事实误解和知行差距等问题。作者提出ToolPoker框架，结合外部求解器实现GTO一致的行动和专业级解释，显著提升游戏表现和推理质量。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在关键领域应用的增加，其在不确定性下进行战略推理的能力变得至关重要。扑克游戏提供了一个严格的测试平台，不仅需要强大的行动能力，还需要基于博弈论的原则性推理。

Method: 首先系统评估LLMs在多种现实扑克任务中的表现，分析其推理缺陷。然后提出ToolPoker框架，该框架整合外部求解器来生成GTO一致的行动，并提供更精确的专业级解释。

Result: LLMs无法与传统算法竞争，存在三大缺陷：启发式依赖、事实误解和知行差距。ToolPoker实现了最先进的游戏表现，产生的推理轨迹能紧密反映博弈论原则。

Conclusion: LLMs在需要严格博弈论推理的任务中存在显著缺陷，但通过整合外部工具（如求解器）可以显著提升其战略推理能力和行动质量，ToolPoker框架为此提供了有效解决方案。

Abstract: As Large Language Models (LLMs) are increasingly applied in high-stakes domains, their ability to reason strategically under uncertainty becomes critical. Poker provides a rigorous testbed, requiring not only strong actions but also principled, game-theoretic reasoning. In this paper, we conduct a systematic study of LLMs in multiple realistic poker tasks, evaluating both gameplay outcomes and reasoning traces. Our analysis reveals LLMs fail to compete against traditional algorithms and identifies three recurring flaws: reliance on heuristics, factual misunderstandings, and a "knowing-doing" gap where actions diverge from reasoning. An initial attempt with behavior cloning and step-level reinforcement learning improves reasoning style but remains insufficient for accurate game-theoretic play. Motivated by these limitations, we propose ToolPoker, a tool-integrated reasoning framework that combines external solvers for GTO-consistent actions with more precise professional-style explanations. Experiments demonstrate that ToolPoker achieves state-of-the-art gameplay while producing reasoning traces that closely reflect game-theoretic principles.

</details>


### [235] [Uncovering Latent Communication Patterns in Brain Networks via Adaptive Flow Routing](https://arxiv.org/abs/2602.00561)
*Tianhao Huang,Guanghui Min,Zhenyu Lei,Aiying Zhang,Chen Chen*

Main category: cs.AI

TL;DR: AFR-Net是一个基于物理启发的自适应流路由网络，通过模拟神经通信动力学来解释结构连接如何产生功能连接模式，从而发现关键神经通路。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏神经科学基础洞察，无法揭示连接组背后的神经区域潜在相互作用，也不能解释为什么SC和FC会表现出耦合和异质性的动态状态。

Method: 从神经通信动力学角度制定多模态融合，提出自适应流路由网络（AFR-Net），这是一个物理启发框架，模拟结构约束如何产生功能通信模式。

Result: 大量实验表明，AFR-Net显著优于最先进的基线方法。

Conclusion: AFR-Net能够解释性地发现关键神经通路，为理解宏观认知表型如何从微观神经元连接中涌现提供了新视角。

Abstract: Unraveling how macroscopic cognitive phenotypes emerge from microscopic neuronal connectivity remains one of the core pursuits of neuroscience. To this end, researchers typically leverage multi-modal information from structural connectivity (SC) and functional connectivity (FC) to complete downstream tasks. Recent methodologies explore the intricate coupling mechanisms between SC and FC, attempting to fuse their representations at the regional level. However, lacking fundamental neuroscientific insight, these approaches fail to uncover the latent interactions between neural regions underlying these connectomes, and thus cannot explain why SC and FC exhibit dynamic states of both coupling and heterogeneity. In this paper, we formulate multi-modal fusion through the lens of neural communication dynamics and propose the Adaptive Flow Routing Network (AFR-Net), a physics-informed framework that models how structural constraints (SC) give rise to functional communication patterns (FC), enabling interpretable discovery of critical neural pathways. Extensive experiments demonstrate that AFR-Net significantly outperforms state-of-the-art baselines. The code is available at https://anonymous.4open.science/r/DIAL-F0D1.

</details>


### [236] [Unmasking Reasoning Processes: A Process-aware Benchmark for Evaluating Structural Mathematical Reasoning in LLMs](https://arxiv.org/abs/2602.00564)
*Xiang Zheng,Weiqi Zhai,Wei Wang,Boyu Yang,Wenbo Li,Ruixiang Luo,Haoxiang Sun,Yucheng Wang,Zhengze Li,Meng Wang,Yuetian Du,Guojie Lin,Yaxuan Wang,Xiaoxiao Xu,Yanhu Mo,Xuan Ren,Hu Wei,Ze Xu*

Main category: cs.AI

TL;DR: 论文提出ReasoningMath-Plus基准测试，包含150个精心设计的问题，用于评估LLMs的结构推理能力，并引入HCRS评分函数和PRM模型，发现仅基于答案的评估会高估推理鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在数学推理基准测试上达到接近饱和的准确率，但这主要源于现有数据集对模板化计算和浅层算术分解的依赖，未能充分评估真正的推理能力，如多约束协调、构造性逻辑综合和空间推理等技能。

Method: 1) 创建ReasoningMath-Plus基准测试，包含150个强调交互约束下推理、构造性解决方案形成和非平凡结构洞察的问题；2) 引入HCRS（风险感知链式规则评分）确定性步骤级评分函数；3) 在标注的推理轨迹上训练过程奖励模型（PRM）。

Result: 领先模型在最终答案准确率上相对较高（最高5.8/10），但基于HCRS的整体评估得分显著较低（平均4.36/10，最佳5.14/10），表明仅基于答案的指标会高估推理鲁棒性。

Conclusion: 需要更精细的过程级评估来准确衡量LLMs的真实推理能力，仅依赖答案准确率会掩盖推理过程中的缺陷。ReasoningMath-Plus基准和HCRS评分提供了更全面的评估框架。

Abstract: Recent large language models (LLMs) achieve near-saturation accuracy on many established mathematical reasoning benchmarks, raising concerns about their ability to diagnose genuine reasoning competence. This saturation largely stems from the dominance of template-based computation and shallow arithmetic decomposition in existing datasets, which underrepresent reasoning skills such as multi-constraint coordination, constructive logical synthesis, and spatial inference. To address this gap, we introduce ReasoningMath-Plus, a benchmark of 150 carefully curated problems explicitly designed to evaluate structural reasoning. Each problem emphasizes reasoning under interacting constraints, constructive solution formation, or non-trivial structural insight, and is annotated with a minimal reasoning skeleton to support fine-grained process-level evaluation. Alongside the dataset, we introduce HCRS (Hazard-aware Chain-based Rule Score), a deterministic step-level scoring function, and train a Process Reward Model (PRM) on the annotated reasoning traces. Empirically, while leading models attain relatively high final-answer accuracy (up to 5.8/10), HCRS-based holistic evaluation yields substantially lower scores (average 4.36/10, best 5.14/10), showing that answer-only metrics can overestimate reasoning robustness.

</details>


### [237] [Learning Modal-Mixed Chain-of-Thought Reasoning with Latent Embeddings](https://arxiv.org/abs/2602.00574)
*Yifei Shao,Kun Zhou,Ziming Xu,Mohammad Atif Quamar,Shibo Hao,Zhen Wang,Zhiting Hu,Biwei Huang*

Main category: cs.AI

TL;DR: 提出modal-mixed CoT方法，在思维链中交替使用文本标记和视觉草图潜在嵌入，以增强多模态推理能力


<details>
  <summary>Details</summary>
Motivation: 传统文本形式的思维链在处理视觉密集型问题时存在局限，因为关键中间状态本质上是视觉的，需要超越语言的思维链来更好地处理多模态推理

Method: 使用VLM自身作为编码器，训练语言主干重建其视觉嵌入以保证语义对齐；附加基于扩散的潜在解码器，由特殊控制令牌调用并基于VLM隐藏状态进行条件生成；采用两阶段训练：监督微调（文本和潜在嵌入交替）和强化学习（学习何时切换模态和组合长推理链）

Result: 在11个多样化多模态推理任务上的广泛实验表明，该方法比纯语言和其他思维链方法表现更好

Conclusion: modal-mixed CoT通过交替使用文本和视觉潜在嵌入，成功扩展了思维链到多模态领域，在保持VLM原有知识和能力的同时提高了视觉密集型问题的推理能力

Abstract: We study how to extend chain-of-thought (CoT) beyond language to better handle multimodal reasoning. While CoT helps LLMs and VLMs articulate intermediate steps, its text-only form often fails on vision-intensive problems where key intermediate states are inherently visual. We introduce modal-mixed CoT, which interleaves textual tokens with compact visual sketches represented as latent embeddings. To bridge the modality gap without eroding the original knowledge and capability of the VLM, we use the VLM itself as an encoder and train the language backbone to reconstruct its own intermediate vision embeddings, to guarantee the semantic alignment of the visual latent space. We further attach a diffusion-based latent decoder, invoked by a special control token and conditioned on hidden states from the VLM. In this way, the diffusion head carries fine-grained perceptual details while the VLM specifies high-level intent, which cleanly disentangles roles and reduces the optimization pressure of the VLM. Training proceeds in two stages: supervised fine-tuning on traces that interleave text and latents with a joint next-token and latent-reconstruction objective, followed by reinforcement learning that teaches when to switch modalities and how to compose long reasoning chains. Extensive experiments across 11 diverse multimodal reasoning tasks, demonstrate that our method yields better performance than language-only and other CoT methods. Our code will be publicly released.

</details>


### [238] [Small Shifts, Large Gains: Unlocking Traditional TSP Heuristic Guided-Sampling via Unsupervised Neural Instance Modification](https://arxiv.org/abs/2602.00580)
*Wei Huang,Hanchen Wang,Dong Wen,Wenjie Zhang*

Main category: cs.AI

TL;DR: TSP-MDF：通过神经实例修改框架为传统确定性启发式TSP求解器赋予引导采样能力，无需真实监督训练，在保持实用性的同时达到神经启发式求解器的解质量


<details>
  <summary>Details</summary>
Motivation: 传统确定性启发式TSP求解器（如最远/最近插入法）计算高效实用，但确定性行为限制探索易陷局部最优；神经启发式求解器通过引导采样获得更优解，但需要大量训练和真实监督，实用性受限。需要弥合这一差距。

Method: 提出TSP-MDF实例修改框架：使用神经实例修改器策略性移动节点坐标生成多个修改实例，传统启发式求解器在这些修改实例上构建路径，然后映射回原始实例，使传统求解器能探索更优路径并逃离局部最优。该框架无需真实监督即可高效训练。

Result: 在大规模TSP基准和真实世界基准上的实验表明，TSP-MDF显著提升传统启发式求解器性能，达到与神经启发式求解器相当的解质量，同时训练时间极短。

Conclusion: TSP-MDF成功弥合了传统确定性启发式和神经启发式TSP求解器之间的差距，为传统启发式方法赋予了引导采样能力，在保持实用性的同时显著提升解质量，无需真实监督训练。

Abstract: The Traveling Salesman Problem (TSP) is one of the most representative NP-hard problems in route planning and a long-standing benchmark in combinatorial optimization. Traditional heuristic tour constructors, such as Farthest or Nearest Insertion, are computationally efficient and highly practical, but their deterministic behavior limits exploration and often leads to local optima. In contrast, neural-based heuristic tour constructors alleviate this issue through guided-sampling and typically achieve superior solution quality, but at the cost of extensive training and reliance on ground-truth supervision, hindering their practical use. To bridge this gap, we propose TSP-MDF, a novel instance modification framework that equips traditional deterministic heuristic tour constructors with guided-sampling capability. Specifically, TSP-MDF introduces a neural-based instance modifier that strategically shifts node coordinates to sample multiple modified instances, on which the base traditional heuristic tour constructor constructs tours that are mapped back to the original instance, allowing traditional tour constructors to explore higher-quality tours and escape local optima. At the same time, benefiting from our instance modification formulation, the neural-based instance modifier can be trained efficiently without any ground-truth supervision, ensuring the framework maintains practicality. Extensive experiments on large-scale TSP benchmarks and real-world benchmarks demonstrate that TSP-MDF significantly improves the performance of traditional heuristics tour constructors, achieving solution quality comparable to neural-based heuristic tour constructors, but with an extremely short training time.

</details>


### [239] [Exploring Information Seeking Agent Consolidation](https://arxiv.org/abs/2602.00585)
*Guochen Yan,Jialong Wu,Zhengwei Tao,Bo Li,Qintong Zhang,Jiahao Xu,Haitao Mi,Yuejian Fang,Qingni Shen,Wentao Zhang,Zhonghai Wu*

Main category: cs.AI

TL;DR: 该研究探索了如何将异构的信息检索智能体整合为单一的基础智能体模型，比较了数据级整合和参数级整合两种策略的性能表现。


<details>
  <summary>Details</summary>
Motivation: 现有信息检索智能体通常专门针对开放网络、文档或本地知识库，这种专门化限制了系统的可扩展性和跨领域泛化能力。研究旨在解决如何将异构的信息检索智能体整合为统一的智能体模型。

Method: 研究采用两种互补的整合策略：1) 数据级整合 - 在混合的领域特定数据集上联合训练统一模型；2) 参数级整合 - 在参数层面合并独立训练的智能体模型。分析比较了这两种方法在性能保持、跨领域泛化和行为干扰方面的表现。

Result: 结果显示：数据级整合仍然是强大且稳定的基准方法，而参数级整合提供了有前景的高效替代方案，但存在干扰和鲁棒性挑战。研究进一步确定了参数级整合的关键设计因素，包括细粒度合并粒度、任务异质性感知和原则性共识策略。

Conclusion: 该研究为异构信息检索智能体的整合提供了系统分析，数据级整合保持稳定优势，参数级整合虽高效但需解决干扰问题，为构建统一的基础智能体模型提供了重要指导。

Abstract: Information-seeking agents have emerged as a powerful paradigm for solving knowledge-intensive tasks. Existing information-seeking agents are typically specialized for open web, documents, or local knowledge bases, which constrains scalability and cross-domain generalization. In this work, we investigate how to consolidate heterogeneous information-seeking agents into a single foundation agentic model. We study two complementary consolidation strategies: data-level consolidation, which jointly trains a unified model on a mixture of domain-specific datasets, and parameter-level consolidation, which merges independently trained agent models at the parameter level. Our analysis compares these approaches in terms of performance retention, cross-domain generalization, and interference across information-seeking behaviors. Our results show that data-level consolidation remains a strong and stable baseline, while parameter-level consolidation offers a promising, efficient alternative but suffers from interference and robustness challenges. We further identify key design factors for effective agent consolidation at the parameter level, including fine-grained merging granularity, awareness of task heterogeneity, and principled consensus strategy.

</details>


### [240] [DockSmith: Scaling Reliable Coding Environments via an Agentic Docker Builder](https://arxiv.org/abs/2602.00592)
*Jiaran Zhang,Luck Ma,Yanhao Li,Fanqi Wan,Di Qi,Xu Zhao,Jieyi Hou,Zhe Xie,Mengqiang Ren,Xin Wu,Zhewei Huang,Liangyu Chen,Yingwei Ma,Qi Han,Xiangyu Zhang*

Main category: cs.AI

TL;DR: DockSmith是一个专门用于Docker环境构建的智能代理，通过大规模执行轨迹训练，在Docker构建任务上达到开源SOTA，并能提升其他软件工程代理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 可靠的Docker环境构建是扩展基于执行的软件工程代理训练和评估的主要瓶颈，当前环境构建仅被视为预处理步骤，缺乏智能化的长时程工具使用、依赖推理和失败恢复能力。

Method: 开发DockSmith智能Docker构建代理，在增强的SWE-Factory风格流水线上生成大规模执行轨迹数据，加入循环检测控制器和跨任务成功记忆，训练30B-A3B模型。

Result: 在Multi-Docker-Eval上达到39.72% Fail-to-Pass和58.28% Commit Rate的开源SOTA性能，在SWE-bench Verified、SWE-bench Multilingual和Terminal-Bench 2.0等OOD任务上也有提升。

Conclusion: 环境构建应被视为核心智能能力而非预处理步骤，DockSmith展示了环境构建训练带来的广泛代理智能收益，为软件工程代理的可扩展性提供了解决方案。

Abstract: Reliable Docker-based environment construction is a dominant bottleneck for scaling execution-grounded training and evaluation of software engineering agents. We introduce DockSmith, a specialized agentic Docker builder designed to address this challenge. DockSmith treats environment construction not only as a preprocessing step, but as a core agentic capability that exercises long-horizon tool use, dependency reasoning, and failure recovery, yielding supervision that transfers beyond Docker building itself. DockSmith is trained on large-scale, execution-grounded Docker-building trajectories produced by a SWE-Factory-style pipeline augmented with a loop-detection controller and a cross-task success memory. Training a 30B-A3B model on these trajectories achieves open-source state-of-the-art performance on Multi-Docker-Eval, with 39.72% Fail-to-Pass and 58.28% Commit Rate. Moreover, DockSmith improves out-of-distribution performance on SWE-bench Verified, SWE-bench Multilingual, and Terminal-Bench 2.0, demonstrating broader agentic benefits of environment construction.

</details>


### [241] [Scalable Generative Game Engine: Breaking the Resolution Wall via Hardware-Algorithm Co-Design](https://arxiv.org/abs/2602.00608)
*Wei Zeng,Xuchen Li,Ruili Feng,Zhen Liu,Fengwei An,Jian Zhao*

Main category: cs.AI

TL;DR: 提出硬件算法协同设计框架，解决生成式游戏引擎的"内存墙"问题，实现720×480分辨率实时生成，相比基线提升50倍像素吞吐量


<details>
  <summary>Details</summary>
Motivation: 现有实时生成式游戏引擎受限于"内存墙"，只能实现低分辨率（如64×64）部署，需要解决高分辨率神经模拟的瓶颈

Method: 提出异构架构，将世界模型（计算密集型）和解码器（内存密集型）解耦到AI加速器集群，包含：1）非对称资源分配策略；2）内存中心算子融合方案；3）流形感知潜在外推机制

Result: 在可编程AI加速器集群上验证，实现720×480分辨率实时生成，在3D赛车和2D平台游戏中分别达到26.4 FPS和48.3 FPS，摊销有效延迟2.7毫秒

Conclusion: 通过架构协同设计解决"内存墙"不仅是优化，更是实现高保真、响应式神经游戏体验的前提条件

Abstract: Real-time generative game engines represent a paradigm shift in interactive simulation, promising to replace traditional graphics pipelines with neural world models. However, existing approaches are fundamentally constrained by the ``Memory Wall,'' restricting practical deployments to low resolutions (e.g., $64 \times 64$). This paper bridges the gap between generative models and high-resolution neural simulations by introducing a scalable \textit{Hardware-Algorithm Co-Design} framework. We identify that high-resolution generation suffers from a critical resource mismatch: the World Model is compute-bound while the Decoder is memory-bound. To address this, we propose a heterogeneous architecture that intelligently decouples these components across a cluster of AI accelerators. Our system features three core innovations: (1) an asymmetric resource allocation strategy that optimizes throughput under sequence parallelism constraints; (2) a memory-centric operator fusion scheme that minimizes off-chip bandwidth usage; and (3) a manifold-aware latent extrapolation mechanism that exploits temporal redundancy to mask latency. We validate our approach on a cluster of programmable AI accelerators, enabling real-time generation at $720 \times 480$ resolution -- a $50\times$ increase in pixel throughput over prior baselines. Evaluated on both continuous 3D racing and discrete 2D platformer benchmarks, our system delivers fluid 26.4 FPS and 48.3 FPS respectively, with an amortized effective latency of 2.7 ms. This work demonstrates that resolving the ``Memory Wall'' via architectural co-design is not merely an optimization, but a prerequisite for enabling high-fidelity, responsive neural gameplay.

</details>


### [242] [Structured Self-Consistency:A Multi-Task Evaluation of LLMs on VirtualHome](https://arxiv.org/abs/2602.00611)
*Jiaqi Xu,Tao Huang,Kai Zhang*

Main category: cs.AI

TL;DR: 该论文评估了两种7B参数大语言模型在虚拟家庭环境中的表现，提出结构化自一致性解码策略提升性能，发现不同模型在分层规划和动作级任务上各有优势。


<details>
  <summary>Details</summary>
Motivation: 体智能需要智能体在模拟环境中理解目标、规划动作并执行任务。为了评估大语言模型在体智能任务上的能力，作者使用VirtualHome基准和EAI框架进行系统评估。

Method: 使用VirtualHome基准和EAI框架，比较OPENPANGU-7B和QWEN2.5-7B两种7B参数模型在四个基础任务上的表现：目标解释、动作序列、子目标分解和转移建模。提出结构化自一致性解码策略，通过多次采样和领域特定投票机制提升结构化生成任务的质量。

Result: 结构化自一致性解码策略显著提升了模型性能。OPENPANGU-7B在分层规划任务上表现优异，而QWEN2.5-7B在动作级任务上具有优势。分析揭示了不同模型类型的互补优势。

Conclusion: 不同大语言模型在体智能任务上展现出互补优势，结构化自一致性解码策略能有效提升性能，为未来体智能系统开发提供了重要见解。

Abstract: Embodied AI requires agents to understand goals, plan actions, and execute tasks in simulated environments.We present a comprehensive evaluation of Large Language Models (LLMs) on the VirtualHome benchmark using the Embodied Agent Interface (EAI) framework.We compare two representative 7B-parameter models OPENPANGU-7B and QWEN2.5-7B across four fundamental tasks: Goal Interpretation, Action Sequencing, Subgoal Decomposition, and Transition Modeling.We propose Structured Self-Consistency (SSC), an enhanced decoding strategy that leverages multiple sampling with domain-specific voting mechanisms to improve output quality for structured generation tasks. Experimental results demonstrate that SSC significantly enhances performance, with OPENPANGU-7B excelling at hierarchical planning while QWEN2.5-7B show advantages in action-level tasks. Our analysis reveals complementary strengths across model types, providing insights for future embodied AI system development.

</details>


### [243] [Inference-Only Prompt Projection for Safe Text-to-Image Generation with TV Guarantees](https://arxiv.org/abs/2602.00616)
*Minhyuk Lee,Hyekyung Yoon,Myungjoo Kang*

Main category: cs.AI

TL;DR: 提出一种推理阶段提示投影框架，在保持良性提示-图像对齐的同时减少不安全生成，无需重新训练或微调扩散模型


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型需要安全防护来抑制不安全生成，但现有方法往往损害良性提示的图像对齐质量。本文通过总变差视角形式化这一权衡，提出安全-提示对齐权衡(SPAT)的理论框架。

Method: 提出推理阶段的提示投影框架，通过验证的代理目标对高风险提示进行选择性干预，将其映射到容忍度控制的"安全集"中，同时保持良性提示基本不变。该方法无需重新训练或微调生成器。

Result: 在四个数据集和三种扩散骨干网络上，相比强基线方法，该方法实现了16.7-60.0%的不当百分比相对降低，同时在COCO数据集上保持良性提示-图像对齐接近未对齐的参考模型水平。

Conclusion: 通过总变差视角形式化的安全-提示对齐权衡为扩散模型安全提供了理论指导，提出的推理阶段提示投影框架能有效平衡安全性和图像质量，为实际部署提供了实用解决方案。

Abstract: Text-to-Image (T2I) diffusion models enable high-quality open-ended synthesis, but their real-world deployment demands safeguards that suppress unsafe generations without degrading benign prompt-image alignment. We formalize this tension through a total variation (TV) lens: once the reference conditional distribution is fixed, any nontrivial reduction in unsafe generations necessarily incurs TV deviation from the reference, yielding a principled Safety-Prompt Alignment Trade-off (SPAT). Guided by this view, we propose an inference-only prompt projection framework that selectively intervenes on high-risk prompts via a surrogate objective with verification, mapping them into a tolerance-controlled safe set while leaving benign prompts effectively unchanged, without retraining or fine-tuning the generator. Across four datasets and three diffusion backbones, our approach achieves 16.7-60.0% relative reductions in inappropriate percentage (IP) versus strong model-level alignment baselines, while preserving benign prompt-image alignment on COCO near the unaligned reference.

</details>


### [244] [Predictive Maintenance for Ultrafiltration Membranes Using Explainable Similarity-Based Prognostics](https://arxiv.org/abs/2602.00659)
*Qusai Khaled,Laura Genga,Uzay Kaymak*

Main category: cs.AI

TL;DR: 提出基于模糊相似推理的可解释超滤膜剩余使用寿命预测框架，通过物理解释的健康指标和透明规则实现可信预测


<details>
  <summary>Details</summary>
Motivation: 反渗透海水淡化中超滤膜因污染导致性能下降和维护成本高，现有预测维护模型缺乏可解释性，操作人员不信任

Method: 使用基于跨膜压力、通量和阻力的物理信息健康指标，通过高斯隶属函数模糊化，采用相似性度量识别历史退化轨迹，构建Takagi-Sugeno模糊规则进行RUL预测

Result: 在工业规模UF系统的12,528个运行周期上测试，平均绝对误差为4.50个周期，生成与专家理解一致的可解释规则库

Conclusion: 提出的可解释预测框架在保持准确性的同时提高了透明度和操作人员信任度，为膜系统预测维护提供了实用解决方案

Abstract: In reverse osmosis desalination, ultrafiltration (UF) membranes degrade due to fouling, leading to performance loss and costly downtime. Most plants rely on scheduled preventive maintenance, since existing predictive maintenance models, often based on opaque machine learning methods, lack interpretability and operator trust. This study proposes an explainable prognostic framework for UF membrane remaining useful life (RUL) estimation using fuzzy similarity reasoning. A physics-informed Health Index, derived from transmembrane pressure, flux, and resistance, captures degradation dynamics, which are then fuzzified via Gaussian membership functions. Using a similarity measure, the model identifies historical degradation trajectories resembling the current state and formulates RUL predictions as Takagi-Sugeno fuzzy rules. Each rule corresponds to a historical exemplar and contributes to a transparent, similarity-weighted RUL estimate. Tested on 12,528 operational cycles from an industrial-scale UF system, the framework achieved a mean absolute error of 4.50 cycles, while generating interpretable rule bases consistent with expert understanding.

</details>


### [245] [SEISMO: Increasing Sample Efficiency in Molecular Optimization with a Trajectory-Aware LLM Agent](https://arxiv.org/abs/2602.00663)
*Fabian P. Krüger,Andrea Hunklinger,Adrian Wolny,Tim J. Adler,Igor Tetko,Santiago David Villalba*

Main category: cs.AI

TL;DR: SEISMO是一个基于LLM的分子优化智能体，通过在线推理方式实现高效分子优化，在23个任务基准上性能比现有方法提升2-3倍


<details>
  <summary>Details</summary>
Motivation: 分子优化在化学科学特别是药物发现中是关键瓶颈，由于依赖昂贵的实验评估（如生物测定），需要高度样本高效的优化方法

Method: SEISMO是一个LLM智能体，执行严格的在线推理时分子优化，每次oracle调用后更新，无需基于种群或批量学习。它基于完整优化轨迹生成建议，结合自然语言任务描述、标量分数和结构化解释性反馈

Result: 在23个任务的实用分子优化基准上，SEISMO的优化曲线下面积比现有方法高2-3倍，通常在50次oracle调用内达到接近最大任务分数。在药物化学任务中，提供解释性反馈进一步提高了效率

Conclusion: 利用领域知识和结构化信息是实现样本高效分子优化的关键，SEISMO展示了LLM智能体在分子优化任务中的强大能力

Abstract: Optimizing the structure of molecules to achieve desired properties is a central bottleneck across the chemical sciences, particularly in the pharmaceutical industry where it underlies the discovery of new drugs. Since molecular property evaluation often relies on costly and rate-limited oracles, such as experimental assays, molecular optimization must be highly sample-efficient. To address this, we introduce SEISMO, an LLM agent that performs strictly online, inference-time molecular optimization, updating after every oracle call without the need for population-based or batched learning. SEISMO conditions each proposal on the full optimization trajectory, combining natural-language task descriptions with scalar scores and, when available, structured explanatory feedback. Across the Practical Molecular Optimization benchmark of 23 tasks, SEISMO achieves a 2-3 times higher area under the optimisation curve than prior methods, often reaching near-maximal task scores within 50 oracle calls. Our additional medicinal-chemistry tasks show that providing explanatory feedback further improves efficiency, demonstrating that leveraging domain knowledge and structured information is key to sample-efficient molecular optimization.

</details>


### [246] [OpenGuanDan: A Large-Scale Imperfect Information Game Benchmark](https://arxiv.org/abs/2602.00676)
*Chao Li,Shangdong Yang,Chiheng Zhan,Zhenxing Ge,Yujing Hu,Bingkun Bao,Xingguo Chen,Yang Gao*

Main category: cs.AI

TL;DR: OpenGuanDan是一个用于评估AI智能体在四人多轮中国纸牌游戏"掼蛋"中的表现的新型基准测试平台，具有不完全信息、大规模动作空间、合作竞争混合目标等挑战性特征。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在棋牌游戏等领域取得了显著进展，但仍需要更具挑战性的基准测试来推动进一步研究。掼蛋游戏具有不完全信息、大规模信息集、合作竞争混合目标、长时程决策等复杂特征，是测试智能决策方法的理想平台。

Method: 开发了OpenGuanDan基准测试平台，支持掼蛋游戏的高效模拟和全面评估。平台提供独立的API接口，支持学习型和规则型AI智能体评估、人机交互以及与大语言模型的集成。

Result: 实验包括两类评估：(1)所有掼蛋AI智能体之间的两两对抗，(2)人机对战。结果显示，当前学习型智能体显著优于规则型智能体，但仍未达到超人类水平，表明多智能体智能决策领域仍需进一步研究。

Conclusion: OpenGuanDan作为一个具有挑战性的基准测试平台，为多智能体智能决策研究提供了有价值的测试环境。虽然学习型方法已取得进展，但要达到超人类性能仍需持续研究。该平台已开源，可供社区使用。

Abstract: The advancement of data-driven artificial intelligence (AI), particularly machine learning, heavily depends on large-scale benchmarks. Despite remarkable progress across domains ranging from pattern recognition to intelligent decision-making in recent decades, exemplified by breakthroughs in board games, card games, and electronic sports games, there remains a pressing need for more challenging benchmarks to drive further research. To this end, this paper proposes OpenGuanDan, a novel benchmark that enables both efficient simulation of GuanDan (a popular four-player, multi-round Chinese card game) and comprehensive evaluation of both learning-based and rule-based GuanDan AI agents. OpenGuanDan poses a suite of nontrivial challenges, including imperfect information, large-scale information set and action spaces, a mixed learning objective involving cooperation and competition, long-horizon decision-making, variable action spaces, and dynamic team composition. These characteristics make it a demanding testbed for existing intelligent decision-making methods. Moreover, the independent API for each player allows human-AI interactions and supports integration with large language models. Empirically, we conduct two types of evaluations: (1) pairwise competitions among all GuanDan AI agents, and (2) human-AI matchups. Experimental results demonstrate that while current learning-based agents substantially outperform rule-based counterparts, they still fall short of achieving superhuman performance, underscoring the need for continued research in multi-agent intelligent decision-making domain. The project is publicly available at https://github.com/GameAI-NJUPT/OpenGuanDan.

</details>


### [247] [HumanStudy-Bench: Towards AI Agent Design for Participant Simulation](https://arxiv.org/abs/2602.00685)
*Xuan Liu,Haoyang Shang,Zizhang Liu,Xinyan Liu,Yunze Xiao,Yiwen Tu,Haojian Jin*

Main category: cs.AI

TL;DR: 该论文提出HUMANSTUDY-BENCH基准测试框架，用于评估LLM作为社会科学实验模拟参与者的表现，通过重现已发表的人类实验来量化LLM与人类行为的一致性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM作为社会科学实验模拟参与者时，其行为不稳定且对设计选择高度敏感。现有评估方法经常混淆基础模型能力与实验实例化，难以区分结果是源于模型本身还是代理设置。

Method: 将参与者模拟定义为完整实验协议上的代理设计问题，提出HUMANSTUDY-BENCH基准测试和执行引擎，采用Filter-Extract-Execute-Evaluate管道，在共享运行时中重现实验试次序列并运行原始分析流程。

Result: 构建了包含12个基础研究的初始测试套件，涵盖个体认知、策略互动和社会心理学等领域，包含超过6,000个试次，人类样本规模从数十人到超过2,100人不等。

Conclusion: 该研究提供了一个系统化评估LLM作为社会科学实验模拟参与者的框架，能够量化人类与代理行为在科学推断层面的一致性，为更可靠的LLM参与社会科学研究奠定了基础。

Abstract: Large language models (LLMs) are increasingly used as simulated participants in social science experiments, but their behavior is often unstable and highly sensitive to design choices. Prior evaluations frequently conflate base-model capabilities with experimental instantiation, obscuring whether outcomes reflect the model itself or the agent setup. We instead frame participant simulation as an agent-design problem over full experimental protocols, where an agent is defined by a base model and a specification (e.g., participant attributes) that encodes behavioral assumptions. We introduce HUMANSTUDY-BENCH, a benchmark and execution engine that orchestrates LLM-based agents to reconstruct published human-subject experiments via a Filter--Extract--Execute--Evaluate pipeline, replaying trial sequences and running the original analysis pipeline in a shared runtime that preserves the original statistical procedures end to end. To evaluate fidelity at the level of scientific inference, we propose new metrics to quantify how much human and agent behaviors agree. We instantiate 12 foundational studies as an initial suite in this dynamic benchmark, spanning individual cognition, strategic interaction, and social psychology, and covering more than 6,000 trials with human samples ranging from tens to over 2,100 participants.

</details>


### [248] [From Prompt to Graph: Comparing LLM-Based Information Extraction Strategies in Domain-Specific Ontology Development](https://arxiv.org/abs/2602.00699)
*Xuan Liu,Ziyu Li,Mu He,Ziyang Ma,Xiaoxu Wu,Gizem Yilmaz,Yiyuan Xia,Bingbing Li,He Tan,Jerry Ying Hsi Fuh,Wen Feng Lu,Anders E. W. Jarfors,Per Jansson*

Main category: cs.AI

TL;DR: 本研究探索了三种基于大语言模型的方法（预训练LLM驱动、上下文学习、微调）从铸造制造领域文本中提取术语和关系，并构建了经过领域专家验证的本体。


<details>
  <summary>Details</summary>
Motivation: 传统本体构建依赖人工标注和传统NLP技术，过程劳动密集且成本高昂，特别是在铸造制造等专业领域。大语言模型的兴起为自动化知识提取提供了新可能性。

Method: 研究了三种LLM方法：1) 预训练LLM驱动方法；2) 上下文学习方法；3) 微调方法。使用有限数据从领域特定文本中提取术语和关系，比较性能后选择最佳方法构建铸造本体。

Result: 比较了三种方法的性能，使用最佳表现方法构建了铸造本体，并经过领域专家验证。

Conclusion: 大语言模型为自动化本体构建提供了有效途径，特别是在数据有限的专门领域，能够显著降低传统方法的劳动强度和成本。

Abstract: Ontologies are essential for structuring domain knowledge, improving accessibility, sharing, and reuse. However, traditional ontology construction relies on manual annotation and conventional natural language processing (NLP) techniques, making the process labour-intensive and costly, especially in specialised fields like casting manufacturing. The rise of Large Language Models (LLMs) offers new possibilities for automating knowledge extraction. This study investigates three LLM-based approaches, including pre-trained LLM-driven method, in-context learning (ICL) method and fine-tuning method to extract terms and relations from domain-specific texts using limited data. We compare their performances and use the best-performing method to build a casting ontology that validated by domian expert.

</details>


### [249] [Capabilities and Fundamental Limits of Latent Chain-of-Thought](https://arxiv.org/abs/2602.01148)
*Jiaxuan Zou,Yaozhong Xiong,Yong Liu*

Main category: cs.AI

TL;DR: 论文揭示了潜在思维链模型在探索与执行之间的权衡由决策确定性决定，提出了符号指数作为核心机制，并证明课程学习在理论上是必要的。


<details>
  <summary>Details</summary>
Motivation: 潜在思维链模型在推理效率方面有潜力，但表现出令人困惑的性能不一致性：在探索任务上表现出色（ProsQA: 97.0%），但在计算任务上表现不佳（GSM8K: 34.1%）。需要理解这种权衡的根本原因。

Method: 1) 理论表征探索-执行权衡，证明高确定性支持精确执行但抑制探索，低确定性促进搜索但导致错误累积；2) 引入符号指数量化决策承诺，作为控制权衡的核心机制；3) 证明课程学习的理论必要性，因为直接训练由于分布不匹配而失败。

Result: 揭示了决策确定性是潜在思维链模型性能权衡的根本原因，提出了符号指数作为量化机制，并建立了其与执行稳定性和探索能力的因果关系。证明了课程学习是必要的训练策略。

Conclusion: 该框架将设计范式从二元架构选择转向自适应系统，能够根据任务需求动态调节决策确定性，为解决潜在思维链模型的探索-执行权衡提供了理论基础和实用指导。

Abstract: Latent Chain-of-Thought (Latent CoT) models promise efficient reasoning via continuous representations, yet exhibit puzzling performance inconsistencies: excelling at exploration (ProsQA: 97.0%) but failing at computation (GSM8K: 34.1%). We reveal that this trade-off is governed by decisional certainty. Our contributions are threefold: (1) We theoretically characterize the fundamental Exploration-Execution Trade-off, proving that high certainty enables precise execution but inhibits exploration, while low certainty facilitates search but causes error accumulation. (2) We introduce the Symbolic Index--quantifying decisional commitment--as the core mechanism governing this trade-off and establish its causal relationship with both execution stability and exploration capability. (3) We prove that curriculum learning is theoretically necessary, as direct training provably fails due to distributional mismatch. Our framework shifts the design paradigm from binary architectural choices toward adaptive systems that dynamically regulate decisional certainty based on task demands.

</details>


### [250] [Self-Guard: Defending Large Reasoning Models via enhanced self-reflection](https://arxiv.org/abs/2602.00707)
*Jingnan Zheng,Jingjun Xu,Yanzhen Luo,Chenhang Cui,Gelei Deng,Zhenkai Liang,Xiang Wang,An Zhang,Tat-Seng Chua*

Main category: cs.AI

TL;DR: Self-Guard：一个轻量级安全防御框架，通过安全导向提示和安全激活引导来弥合大型推理模型中的意识-合规差距，无需大量后训练即可增强安全性


<details>
  <summary>Details</summary>
Motivation: 大型推理模型(LRMs)虽然带来了显著的推理能力提升，但也引入了推理操纵和信息泄露等独特风险。现有的对齐策略主要依赖计算密集的后训练范式或外部干预，无法有效解决意识-合规差距问题——模型能识别风险但倾向于遵循用户指令的谄媚倾向

Method: Self-Guard框架包含两个主要阶段：1) 安全导向提示：激活模型的潜在安全意识，引发自发反思；2) 安全激活引导：提取隐藏状态空间中的方向性变化并放大，确保推理过程中安全合规优先于谄媚倾向

Result: 实验表明Self-Guard能有效弥合意识-合规差距，在不损害模型实用性的情况下实现鲁棒的安全性能。该框架在不同未见风险和模型规模上表现出良好的泛化能力

Conclusion: Self-Guard为LRM安全对齐提供了一个成本高效的解决方案，通过表示层面的安全合规强化，解决了现有方法计算密集且无法处理意识-合规差距的问题

Abstract: The emergence of Large Reasoning Models (LRMs) introduces a new paradigm of explicit reasoning, enabling remarkable advances yet posing unique risks such as reasoning manipulation and information leakage. To mitigate these risks, current alignment strategies predominantly rely on heavy post-training paradigms or external interventions. However, these approaches are often computationally intensive and fail to address the inherent awareness-compliance gap, a critical misalignment where models recognize potential risks yet prioritize following user instructions due to their sycophantic tendencies. To address these limitations, we propose Self-Guard, a lightweight safety defense framework that reinforces safety compliance at the representational level. Self-Guard operates through two principal stages: (1) safety-oriented prompting, which activates the model's latent safety awareness to evoke spontaneous reflection, and (2) safety activation steering, which extracts the resulting directional shift in the hidden state space and amplifies it to ensure that safety compliance prevails over sycophancy during inference. Experiments demonstrate that Self-Guard effectively bridges the awareness-compliance gap, achieving robust safety performance without compromising model utility. Furthermore, Self-Guard exhibits strong generalization across diverse unseen risks and varying model scales, offering a cost-efficient solution for LRM safety alignment.

</details>


### [251] [Physics-informed Diffusion Generation for Geomagnetic Map Interpolation](https://arxiv.org/abs/2602.00709)
*Wenda Li,Tongya Zheng,Kaixuan Chen,Shunyu Liu,Haoze Jiang,Yunzhi Hao,Rui Miao,Zujie Ren,Mingli Song,Hang Shi,Gang Chen*

Main category: cs.AI

TL;DR: 提出PDG框架，通过物理信息引导的扩散生成方法进行地磁地图插值，结合局部感受野和克里金原理来消除噪声并保证物理一致性。


<details>
  <summary>Details</summary>
Motivation: 现有散点数据插值方法并非专门为地磁地图设计，无法有效处理检测噪声和物理规律，导致性能不佳。地磁地图插值在导航和资源勘探中具有重要应用价值。

Method: 提出物理信息扩散生成框架(PDG)：1) 基于局部感受野设计物理信息掩码策略指导扩散生成过程，消除噪声干扰；2) 根据地磁地图的克里金原理对扩散生成结果施加物理信息约束，确保严格遵循物理规律。

Result: 在四个真实世界数据集上的大量实验和深入分析证明了PDG的优越性和各组成部分的有效性。

Conclusion: PDG框架通过物理信息引导的扩散生成方法，有效解决了地磁地图插值中的噪声干扰和物理规律遵循问题，在导航和资源勘探应用中具有重要价值。

Abstract: Geomagnetic map interpolation aims to infer unobserved geomagnetic data at spatial points, yielding critical applications in navigation and resource exploration. However, existing methods for scattered data interpolation are not specifically designed for geomagnetic maps, which inevitably leads to suboptimal performance due to detection noise and the laws of physics. Therefore, we propose a Physics-informed Diffusion Generation framework~(PDG) to interpolate incomplete geomagnetic maps. First, we design a physics-informed mask strategy to guide the diffusion generation process based on a local receptive field, effectively eliminating noise interference. Second, we impose a physics-informed constraint on the diffusion generation results following the kriging principle of geomagnetic maps, ensuring strict adherence to the laws of physics. Extensive experiments and in-depth analyses on four real-world datasets demonstrate the superiority and effectiveness of each component of PDG.

</details>


### [252] [Mitigating loss of control in advanced AI systems through instrumental goal trajectories](https://arxiv.org/abs/2602.01699)
*Willem Fourie*

Main category: cs.AI

TL;DR: 该论文提出"工具性目标轨迹"概念，通过监控AI系统获取技术资源（计算、存储、数据等）的组织路径，为控制高级AI系统提供新的干预点。


<details>
  <summary>Details</summary>
Motivation: 现有AI控制方法主要关注技术层面和系统本身，如能力追踪、人类反馈强化学习等，但缺乏对组织层面的考虑。研究人员担心高度能力的AI系统可能通过追求工具性目标侵蚀人类控制。

Method: 提出三种组织路径作为工具性目标轨迹：采购轨迹、治理轨迹和财务轨迹。这些轨迹产生可监控的组织痕迹，可作为AI能力或行为超出可接受阈值时的干预点。

Result: IGTs提供了具体途径来定义能力水平，并拓宽了可修正性和可中断性的实施方式，将注意力从模型属性扩展到支持它们的组织系统。

Conclusion: 工具性目标轨迹为AI控制提供了超越技术层面的组织干预框架，通过监控资源获取路径来增强对高级AI系统的控制能力。

Abstract: Researchers at artificial intelligence labs and universities are concerned that highly capable artificial intelligence (AI) systems may erode human control by pursuing instrumental goals. Existing mitigations remain largely technical and system-centric: tracking capability in advanced systems, shaping behaviour through methods such as reinforcement learning from human feedback, and designing systems to be corrigible and interruptible. Here we develop instrumental goal trajectories to expand these options beyond the model. Gaining capability typically depends on access to additional technical resources, such as compute, storage, data and adjacent services, which in turn requires access to monetary resources. In organisations, these resources can be obtained through three organisational pathways. We label these pathways the procurement, governance and finance instrumental goal trajectories (IGTs). Each IGT produces a trail of organisational artefacts that can be monitored and used as intervention points when a systems capabilities or behaviour exceed acceptable thresholds. In this way, IGTs offer concrete avenues for defining capability levels and for broadening how corrigibility and interruptibility are implemented, shifting attention from model properties alone to the organisational systems that enable them.

</details>


### [253] [Learning More from Less: Unlocking Internal Representations for Benchmark Compression](https://arxiv.org/abs/2602.00710)
*Yueqi Zhang,Jin Hu,Shaoxiong Feng,Peiwen Yuan,Xinglin Wang,Yiwei Li,Jiayi Shi,Chuyi Tan,Ji Zhang,Boyuan Pan,Yao Hu,Kan Li*

Main category: cs.AI

TL;DR: REPCORE：通过对齐隐藏状态构建核心集，仅需10个源模型即可准确估计LLM基准性能，优于基于输出的方法


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型成本高昂，现有核心集方法需要大量源模型来估计可靠的项目特征，这在源模型较少时（如新发布的基准）统计不稳定。基于离散正确标签的方法会丢失模型决策过程中的隐藏状态信息。

Method: REPCORE将异构隐藏状态对齐到统一的潜在空间，构建代表性核心集。通过谱分析发现对齐表示包含反映广泛响应倾向和任务特定推理模式的可分离组件。

Result: 在5个基准和200多个模型上的实验表明，REPCORE在排名相关性和估计准确性方面持续优于基于输出的基线方法，仅需10个源模型即可实现精确估计。

Conclusion: 利用隐藏状态信息而非仅依赖输出标签，可以构建更有效的核心集，显著减少评估LLM所需的源模型数量，特别适用于数据有限的新基准。

Abstract: The prohibitive cost of evaluating Large Language Models (LLMs) necessitates efficient alternatives to full-scale benchmarking. Prevalent approaches address this by identifying a small coreset of items to approximate full-benchmark performance. However, existing methods must estimate a reliable item profile from response patterns across many source models, which becomes statistically unstable when the source pool is small. This dependency is particularly limiting for newly released benchmarks with minimal historical evaluation data. We argue that discrete correctness labels are a lossy view of the model's decision process and fail to capture information encoded in hidden states. To address this, we introduce REPCORE, which aligns heterogeneous hidden states into a unified latent space to construct representative coresets. Using these subsets for performance extrapolation, REPCORE achieves precise estimation accuracy with as few as ten source models. Experiments on five benchmarks and over 200 models show consistent gains over output-based baselines in ranking correlation and estimation accuracy. Spectral analysis further indicates that the aligned representations contain separable components reflecting broad response tendencies and task-specific reasoning patterns.

</details>


### [254] [Neuro-symbolic AI for Predictive Maintenance (PdM) -- review and recommendations](https://arxiv.org/abs/2602.00731)
*Kyle Hamilton,Ali Intizar*

Main category: cs.AI

TL;DR: 本文系统回顾了过去五年工业环境中预测性维护（PdM）的最新进展，指出数据驱动方法（如深度学习）精度更高但存在数据需求大、泛化性差、可解释性不足等问题，而传统基于规则的方法则精度低、误报多。作者提出将深度学习与符号逻辑结合的神经符号AI（NeSy）作为有前景的混合解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前预测性维护领域存在两难困境：数据驱动方法（如深度学习）虽然精度高，但需要大量标注数据、泛化能力差、缺乏可解释性；而传统基于领域知识的规则系统则精度低、误报多、需要持续专家维护。需要一种能结合两者优势的解决方案。

Method: 本文采用系统性文献综述方法，分析过去五年工业预测性维护的最新进展。重点考察混合系统架构，特别是神经符号AI（NeSy）方法，这些方法将深度学习与符号逻辑结合，使用传感器数据和人工规则作为输入。

Result: 研究发现混合系统（特别是神经符号AI）有潜力克服单一方法的弱点，同时保留各自优势。这些系统能够提供更准确、可解释、可解释且鲁棒的预测性维护解决方案。

Conclusion: 神经符号AI代表了预测性维护领域有前景的发展方向，能够整合数据驱动方法的精度与基于规则系统的可解释性和领域知识，为工业环境提供更实用的解决方案。

Abstract: In this document we perform a systematic review the State-of-the-art in Predictive Maintenance (PdM) over the last five years in industrial settings such as commercial buildings, pharmaceutical facilities, or semi-conductor manufacturing. In general, data-driven methods such as those based on deep learning, exhibit higher accuracy than traditional knowledge-based systems. These systems however, are not without significant limitations. The need for large labeled data sets, a lack of generalizibility to new environments (out-of-distribution generalization), and a lack of transparency at inference time are some of the obstacles to adoption in real world environments. In contrast, traditional approaches based on domain expertise in the form of rules, logic or first principles suffer from poor accuracy, many false positives and a need for ongoing expert supervision and manual tuning. While the majority of approaches in recent literature utilize some form of data-driven architecture, there are hybrid systems which also take into account domain specific knowledge. Such hybrid systems have the potential to overcome the weaknesses of either approach on its own while preserving their strengths. We propose taking the hybrid approach even further and integrating deep learning with symbolic logic, or Neuro-symbolic AI, to create more accurate, explainable, interpretable, and robust systems. We describe several neuro-symbolic architectures and examine their strengths and limitations within the PdM domain. We focus specifically on methods which involve the use of sensor data and manually crafted rules as inputs by describing concrete NeSy architectures. In short, this survey outlines the context of modern maintenance, defines key concepts, establishes a generalized framework, reviews current modeling approaches and challenges, and introduces the proposed focus on Neuro-symbolic AI (NESY).

</details>


### [255] [Engineering AI Agents for Clinical Workflows: A Case Study in Architecture,MLOps, and Governance](https://arxiv.org/abs/2602.00751)
*Cláudio Lúcio do Val Lopes,João Marcus Pitta,Fabiano Belém,Gildson Alves,Flávio Vinícius Cruzeiro Martins*

Main category: cs.AI

TL;DR: 论文提出Maria平台，一个用于初级医疗保健的生产级AI系统，通过四个工程支柱实现可信临床AI：清洁架构、事件驱动架构、自主代理模块化、人机协同治理。


<details>
  <summary>Details</summary>
Motivation: AI在临床环境中的集成面临软件工程挑战，需要从孤立模型转向稳健、可治理、可靠的系统。当前工业应用常受脆弱原型架构和系统性监督缺失困扰，导致"责任真空"，安全性和可问责性受损。

Method: 提出Maria平台作为行业案例研究，采用协同架构：清洁架构保证可维护性，事件驱动架构提供弹性和可审计性；以代理作为主要模块化单元，每个代理拥有自主MLOps生命周期；技术上集成人机协同治理模型作为关键的事件驱动数据源。

Result: Maria平台作为参考架构，为在高风险领域构建可维护、可扩展和可问责的AI系统提供实践指导，解决了临床AI中的责任真空问题。

Conclusion: 可信临床AI需要通过四个基础工程支柱的整体集成来实现：清洁架构、事件驱动架构、自主代理模块化、人机协同治理。该平台为高风险领域构建可靠AI系统提供了可参考的架构模式。

Abstract: The integration of Artificial Intelligence (AI) into clinical settings presents a software engineering challenge, demanding a shift from isolated models to robust, governable, and reliable systems. However, brittle, prototype-derived architectures often plague industrial applications and a lack of systemic oversight, creating a ``responsibility vacuum'' where safety and accountability are compromised. This paper presents an industry case study of the ``Maria'' platform, a production-grade AI system in primary healthcare that addresses this gap.
  Our central hypothesis is that trustworthy clinical AI is achieved through the holistic integration of four foundational engineering pillars. We present a synergistic architecture that combines Clean Architecture for maintainability with an Event-driven architecture for resilience and auditability. We introduce the Agent as the primary unit of modularity, each possessing its own autonomous MLOps lifecycle. Finally, we show how a Human-in-the-Loop governance model is technically integrated not merely as a safety check, but as a critical, event-driven data source for continuous improvement. We present the platform as a reference architecture, offering practical lessons for engineers building maintainable, scalable, and accountable AI-enabled systems in high-stakes domains.

</details>


### [256] [Environment-Aware Adaptive Pruning with Interleaved Inference Orchestration for Vision-Language-Action Models](https://arxiv.org/abs/2602.00780)
*Yuting Huang,Leilei Ding,Zhipeng Tang,Zenghuan Zhu,Jiajun Deng,Xinrui Lin,Shuo Liu,Haojie Ren,Jianmin Ji,Yanyong Zhang*

Main category: cs.AI

TL;DR: EcoVLA：无需训练、即插即用的自适应剪枝框架，针对VLA模型实现动态环境感知剪枝，结合交错推理编排，在保持性能的同时显著加速推理。


<details>
  <summary>Details</summary>
Motivation: VLA模型参数庞大导致推理延迟高，影响实时操控。静态剪枝无法适应环境动态变化，固定间隔的动态层剪枝粒度粗且重训练开销大，需要自适应剪枝方案。

Method: 包含两个组件：1) 环境感知自适应剪枝(EAP)：轻量级自适应通道剪枝方法，利用物理环境的时间一致性更新稀疏模式；2) 交错推理编排(I²O)：利用VLA推理中的FLOPs气泡并行调度剪枝方法，对延迟影响可忽略。

Result: 在多种VLA模型和基准测试中达到SOTA性能：实现最高1.60倍加速且成功率仅下降0.4%；结合token剪枝后达到2.18倍加速且性能仅下降0.5%。在真实机器人上验证有效。

Conclusion: EcoVLA是无需训练、即插即用的自适应剪枝框架，能有效适应环境动态变化，显著加速VLA推理，且可与现有加速方法正交组合，在真实机器人场景中验证有效。

Abstract: While Vision-Language-Action (VLA) models hold promise in embodied intelligence, their large parameter counts lead to substantial inference latency that hinders real-time manipulation, motivating parameter sparsification. However, as the environment evolves during VLA execution, the optimal sparsity patterns change accordingly. Static pruning lacks the adaptability required for environment dynamics, whereas fixed-interval dynamic layer pruning suffers from coarse granularity and high retraining overheads. To bridge this gap, we propose EcoVLA, a training-free, plug-and-play adaptive pruning framework that supports orthogonal combination with existing VLA acceleration methods. EcoVLA comprises two components: Environment-aware Adaptive Pruning (EAP) and Interleaved Inference Orchestration ($I^2O$). EAP is a lightweight adaptive channel pruning method that incorporates the temporal consistency of the physical environment to update sparsity patterns. $I^2O$ leverages the FLOPs bubbles inherent in VLA inference to schedule the pruning method in parallel, ensuring negligible impact on latency. Evaluated on diverse VLA models and benchmarks, EcoVLA delivers state-of-the-art performance, achieving up to 1.60$\times$ speedup with only a 0.4% drop in success rate, and further reaches 2.18$\times$ speedup with only a 0.5% degradation when combined with token pruning. We further validate the effectiveness of EcoVLA on real-world robots.

</details>


### [257] [World Models as an Intermediary between Agents and the Real World](https://arxiv.org/abs/2602.00785)
*Sherry Yang*

Main category: cs.AI

TL;DR: 论文主张使用世界模型作为智能体与真实世界之间的中介，以解决高成本交互领域（如机器人、科学实验）中强化学习智能体的瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型智能体在低成本环境（游戏、数学、编程）中表现出色，但在高成本交互领域（机器人物理成本、ML工程时间成本、科学实验资源成本）中难以应用。真正的瓶颈在于执行动作获取奖励信号的成本过高。

Method: 提出使用世界模型作为智能体与真实世界之间的中介。将世界模型视为动态、奖励和任务分布的模型，可以克服高成本动作的基本障碍，如极端离策略学习和长时程任务的样本效率低下。

Result: 展示了世界模型如何在机器学习工程、计算机使用、机器人和AI科学等多个领域为智能体提供关键且丰富的学习信号。

Conclusion: 识别了构建这些世界模型的挑战，并提出了在数据集管理、架构设计、扩展和世界模型评估方面的可行动项。

Abstract: Large language model (LLM) agents trained using reinforcement learning has achieved superhuman performance in low-cost environments like games, mathematics, and coding. However, these successes have not translated to complex domains where the cost of interaction is high, such as the physical cost of running robots, the time cost of ML engineering, and the resource cost of scientific experiments. The true bottleneck for achieving the next level of agent performance for these complex and high-cost domains lies in the expense of executing actions to acquire reward signals. To address this gap, this paper argues that we should use world models as an intermediary between agents and the real world. We discuss how world models, viewed as models of dynamics, rewards, and task distributions, can overcome fundamental barriers of high-cost actions such as extreme off-policy learning and sample inefficiency in long-horizon tasks. Moreover, we demonstrate how world models can provide critical and rich learning signals to agents across a broad set of domains, including machine learning engineering, computer use, robotics, and AI for science. Lastly, we identify the challenges of building these world models and propose actionable items along dataset curation, architecture design, scaling, and evaluation of world models.

</details>


### [258] [MissMAC-Bench: Building Solid Benchmark for Missing Modality Issue in Robust Multimodal Affective Computing](https://arxiv.org/abs/2602.00811)
*Ronghao Lin,Honghao Lu,Ruixing Wu,Aolin Xiong,Qinggong Chu,Qiaolin He,Sijie Mai,Haifeng Hu*

Main category: cs.AI

TL;DR: 该论文提出了MissMAC-Bench基准，用于系统评估多模态情感计算中的缺失模态问题，通过统一的评估标准和跨模态协同视角，促进鲁棒多模态情感计算的发展。


<details>
  <summary>Details</summary>
Motivation: 现实世界中多模态数据的可用性通常是动态和不确定的，由于分布偏移和语义缺失，不完整的多模态输入会导致性能大幅波动。这种缺失模态问题是多模态情感计算模型鲁棒性和实际部署的关键障碍。

Method: 提出MissMAC-Bench基准，建立公平统一的评估标准，基于两个指导原则：训练时不使用缺失先验，单个模型能同时处理完整和不完整模态场景。基准集成了数据集和实例级别的固定和随机缺失模式评估协议。

Result: 在4个数据集上对3个广泛使用的语言模型进行了广泛实验，验证了不同多模态情感计算方法在解决缺失模态问题上的有效性。

Conclusion: MissMAC-Bench为推进鲁棒多模态情感计算提供了坚实基础，促进了多媒体数据挖掘的发展，弥合了学术研究与实际应用之间的差距。

Abstract: As a knowledge discovery task over heterogeneous data sources, current Multimodal Affective Computing (MAC) heavily rely on the completeness of multiple modalities to accurately understand human's affective state. However, in real-world scenarios, the availability of modality data is often dynamic and uncertain, leading to substantial performance fluctuations due to the distribution shifts and semantic deficiencies of the incomplete multimodal inputs. Known as the missing modality issue, this challenge poses a critical barrier to the robustness and practical deployment of MAC models. To systematically quantify this issue, we introduce MissMAC-Bench, a comprehensive benchmark designed to establish fair and unified evaluation standards from the perspective of cross-modal synergy. Two guiding principles are proposed, including no missing prior during training, and one single model capable of handling both complete and incomplete modality scenarios, thereby ensuring better generalization. Moreover, to bridge the gap between academic research and real-world applications, our benchmark integrates evaluation protocols with both fixed and random missing patterns at the dataset and instance levels. Extensive experiments conducted on 3 widely-used language models across 4 datasets validate the effectiveness of diverse MAC approaches in tackling the missing modality issue. Our benchmark provides a solid foundation for advancing robust multimodal affective computing and promotes the development of multimedia data mining.

</details>


### [259] [Resource-Efficient Reinforcement for Reasoning Large Language Models via Dynamic One-Shot Policy Refinement](https://arxiv.org/abs/2602.00815)
*Yunjian Zhang,Sudong Wang,Yang Li,Peiran Xu,Conghao Zhou,Xiaoyue Ma,Jianing Li,Yao Zhu*

Main category: cs.AI

TL;DR: 本文提出DoPR方法，通过动态选择单一样本进行策略更新，显著降低RLVR训练的计算成本，同时保持推理性能。


<details>
  <summary>Details</summary>
Motivation: 尽管基于可验证奖励的强化学习（RLVR）在LLM推理对齐方面表现出色，但其训练过程需要大量奖励信号和计算资源，成本过高，限制了实际应用。

Method: 提出动态单样本策略精炼（DoPR）方法，基于奖励波动性和探索驱动的获取策略，在每个批次中动态选择单个信息量最大的训练样本进行策略更新，大幅减少计算开销。

Result: DoPR将训练开销降低近一个数量级，同时保持与基线相当的推理准确性，为LLM后训练提供了可扩展且资源高效的解决方案。

Conclusion: DoPR为推理密集型LLM应用提供了实用且高效的基于强化学习的训练路径，使RLVR方法更加可访问和可扩展。

Abstract: Large language models (LLMs) have exhibited remarkable performance on complex reasoning tasks, with reinforcement learning under verifiable rewards (RLVR) emerging as a principled framework for aligning model behavior with reasoning chains. Despite its promise, RLVR remains prohibitively resource-intensive, requiring extensive reward signals and incurring substantial rollout costs during training. In this work, we revisit the fundamental question of data and compute efficiency in RLVR. We first establish a theoretical lower bound on the sample complexity required to unlock reasoning capabilities, and empirically validate that strong performance can be achieved with a surprisingly small number of training instances. To tackle the computational burden, we propose Dynamic One-Shot Policy Refinement (DoPR), an uncertainty-aware RL strategy that dynamically selects a single informative training sample per batch for policy updates, guided by reward volatility and exploration-driven acquisition. DoPR reduces rollout overhead by nearly an order of magnitude while preserving competitive reasoning accuracy, offering a scalable and resource-efficient solution for LLM post-training. This approach offers a practical path toward more efficient and accessible RL-based training for reasoning-intensive LLM applications.

</details>


### [260] [Optimizing Agentic Reasoning with Retrieval via Synthetic Semantic Information Gain Reward](https://arxiv.org/abs/2602.00845)
*Senkang Hu,Yong Dai,Yuzhi Zhao,Yihang Tao,Yu Guo,Zhengru Fang,Sam Tak Wu Kwong,Yuguang Fang*

Main category: cs.AI

TL;DR: InfoReasoner：通过语义信息增益奖励优化检索增强推理的框架，在7个问答基准上平均准确率提升达5.4%


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型的代理推理虽然能动态获取外部知识，但检索过程优化面临挑战，主要因为缺乏密集、有原则的奖励信号。需要一种理论上有保证且可扩展的方法来激励有效的信息寻求行为。

Method: 1. 理论框架：将信息增益重新定义为模型信念状态的不确定性减少，建立非负性、伸缩可加性和通道单调性等理论保证
2. 实践实现：提出输出感知的内在估计器，通过双向文本蕴含的语义聚类直接从模型输出分布计算信息增益
3. 训练方法：使用组相对策略优化（GRPO）训练策略，最大化认知进展

Result: 在7个问答基准测试中，InfoReasoner始终优于强大的检索增强基线方法，平均准确率提升高达5.4%。该方法为检索增强的代理推理提供了理论上有保证且可扩展的路径。

Conclusion: InfoReasoner通过引入语义信息增益奖励，为代理推理中的检索过程优化提供了理论上有保证且可扩展的解决方案，显著提升了检索增强推理的性能。

Abstract: Agentic reasoning enables large reasoning models (LRMs) to dynamically acquire external knowledge, but yet optimizing the retrieval process remains challenging due to the lack of dense, principled reward signals. In this paper, we introduce InfoReasoner, a unified framework that incentivizes effective information seeking via a synthetic semantic information gain reward. Theoretically, we redefine information gain as uncertainty reduction over the model's belief states, establishing guarantees, including non-negativity, telescoping additivity, and channel monotonicity. Practically, to enable scalable optimization without manual retrieval annotations, we propose an output-aware intrinsic estimator that computes information gain directly from the model's output distributions using semantic clustering via bidirectional textual entailment. This intrinsic reward guides the policy to maximize epistemic progress, enabling efficient training via Group Relative Policy Optimxization (GRPO). Experiments across seven question-answering benchmarks demonstrate that InfoReasoner consistently outperforms strong retrieval-augmented baselines, achieving up to 5.4% average accuracy improvement. Our work provides a theoretically grounded and scalable path toward agentic reasoning with retrieval.

</details>


### [261] [Persuasion Propagation in LLM Agents](https://arxiv.org/abs/2602.00851)
*Hyejun Jeong,Amir Houmansadr,Shlomo Zilberstein,Eugene Bagdasarian*

Main category: cs.AI

TL;DR: 研究AI智能体在长期任务中受到用户说服时的影响，发现任务前明确设定信念状态比任务中实时说服更能显著改变智能体行为


<details>
  <summary>Details</summary>
Motivation: 现代AI智能体越来越多地结合对话交互和自主任务执行（如编码和网络研究），当这些智能体在长期任务中受到用户说服时，其下游任务行为会受到什么影响？这引发了关于信念层面干预如何影响任务行为的研究动机。

Method: 引入行为中心评估框架，区分在任务执行期间或之前应用的说服。在网络研究和编码任务中，比较实时说服与信念预填充两种干预方式对智能体行为的影响。

Result: 实时说服仅产生微弱且不一致的行为效应。相比之下，当在任务开始时明确指定信念状态时，信念预填充的智能体平均比中性预填充智能体少进行26.9%的搜索，访问16.9%更少的独特来源。

Conclusion: 说服（即使是先前的交互）确实能够影响智能体的行为，这强调了在智能体系统中进行行为层面评估的重要性。信念预填充比实时说服更有效地改变智能体行为。

Abstract: Modern AI agents increasingly combine conversational interaction with autonomous task execution, such as coding and web research, raising a natural question: what happens when an agent engaged in long-horizon tasks is subjected to user persuasion? We study how belief-level intervention can influence downstream task behavior, a phenomenon we name \emph{persuasion propagation}. We introduce a behavior-centered evaluation framework that distinguishes between persuasion applied during or prior to task execution. Across web research and coding tasks, we find that on-the-fly persuasion induces weak and inconsistent behavioral effects. In contrast, when the belief state is explicitly specified at task time, belief-prefilled agents conduct on average 26.9\% fewer searches and visit 16.9\% fewer unique sources than neutral-prefilled agents. These results suggest that persuasion, even in prior interaction, can affect the agent's behavior, motivating behavior-level evaluation in agentic systems.

</details>


### [262] [Position: Human-Centric AI Requires a Minimum Viable Level of Human Understanding](https://arxiv.org/abs/2602.00854)
*Fangzhou Lin,Qianwen Ge,Lingyu Xu,Peiran Li,Xiangbo Gao,Shuo Xing,Kazunori Yamada,Ziming Zhang,Haichong Zhang,Zhengzhong Tu*

Main category: cs.AI

TL;DR: 论文提出"能力-理解差距"概念，即AI系统性能提升但用户理解能力下降，并定义了"认知完整性阈值"作为保持人类监督所需的最低理解水平。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统越来越能产生流畅、正确的端到端结果，用户解释、验证或干预的能力逐渐被侵蚀。这种能力与理解之间的脱节需要被正视，以确保在AI协助下人类仍能保持有效的监督和问责。

Method: 提出"认知完整性阈值"概念，将其操作化为三个功能维度：验证能力、保持理解的交互设计、以及治理的制度支撑。这为责任关键场景中的人机交互设计提供了理论框架。

Result: 定义了能力-理解差距和认知完整性阈值，为AI系统设计提供了新的评估标准，强调需要确保人类在AI协助下仍能保持必要的理解水平以履行监督职责。

Conclusion: 需要重新设计AI系统的人机交互和治理框架，确保在AI能力提升的同时，人类的理解和监督能力不会退化，从而在责任关键场景中实现可持续的认知参与。

Abstract: AI systems increasingly produce fluent, correct, end-to-end outcomes. Over time, this erodes users' ability to explain, verify, or intervene. We define this divergence as the Capability-Comprehension Gap: a decoupling where assisted performance improves while users' internal models deteriorate. This paper argues that prevailing approaches to transparency, user control, literacy, and governance do not define the foundational understanding humans must retain for oversight under sustained AI delegation. To formalize this, we define the Cognitive Integrity Threshold (CIT) as the minimum comprehension required to preserve oversight, autonomy, and accountable participation under AI assistance. CIT does not require full reasoning reconstruction, nor does it constrain automation. It identifies the threshold beyond which oversight becomes procedural and contestability fails. We operatinalize CIT through three functional dimensions: (i) verification capacity, (ii) comprehension-preserving interaction, and (iii) institutional scaffolds for governance. This motivates a design and governance agenda that aligns human-AI interaction with cognitive sustainability in responsibility-critical settings.

</details>


### [263] [Multi-Head Attention Is a Multi-Player Game](https://arxiv.org/abs/2602.00861)
*Kushal Chakrabarti,Nirmal Balachundar*

Main category: cs.AI

TL;DR: 论文将transformer注意力机制建模为多头间的潜在博弈，证明交叉熵训练会导致纳什均衡，并存在无界效率损失。提出了基于博弈论的价格无政府状态(PoA)分析框架，并开发了GAME-LoRA方法减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 现代transformer注意力机制本质上是多头竞争与协作的多智能体系统，但现有训练方法将其视为单一优化器，忽略了多头间的博弈结构。这种简化可能导致效率损失，表现为幻觉和冗余等失败模式。

Method: 将多头注意力建模为潜在博弈，分析交叉熵训练诱导的纳什均衡。提出价格无政府状态(PoA)理论框架，用头间交互矩阵Γ(G)量化博弈效率损失。开发GAME-LoRA方法，结合Barlow Twins去相关和log-determinant协调压力来减少Γ(G)。

Result: 理论证明：过量幻觉概率和头冗余都与PoA成比例。实验验证：Γ(G)能预测幻觉(p<0.05)，涌现的联盟表现出选择性协调。GAME-LoRA实现高达18%的幻觉减少(平均8%)，且不损害知识性能。

Conclusion: 多头注意力本质上是博弈系统，忽略这一结构会导致效率损失。提出的博弈论框架统一解释了幻觉和冗余两种失败模式，GAME-LoRA通过减少头间耦合实现了帕累托改进，为transformer训练提供了新视角。

Abstract: Modern transformer attention is internally multi-agent -- heads compete and coordinate -- yet we train it as if it were a monolithic optimizer. We formalize this gap: cross-entropy training induces an implicit potential game among heads, and gradient descent converges to Nash equilibria with potentially unbounded inefficiency due to unpriced externalities (redundancy, correlated errors). Our main result bounds the Price of Anarchy by $Γ(G)$, the off-diagonal mass of a head interaction matrix capturing weight and gradient coupling. Under mild smoothness assumptions, we prove that both \emph{excess hallucination probability} and \emph{excess head redundancy} scale with PoA, unifying two distinct failure modes into a single mechanism. The bound is prescriptive: regularization that reduces $Γ(G)$ provably tightens PoA. We instantiate this as GAME-LoRA, combining Barlow Twins decorrelation with log-determinant coordination pressure. Experiments validate the theory: $Γ(G)$ predicts hallucination ($p{<}0.05$), emergent coalitions exhibit selective coordination, and GAME-LoRA achieves up to 18\% hallucination reduction (8\% average) with no knowledge degradation -- a Pareto improvement inaccessible to methods ignoring the game structure.

</details>


### [264] [Foundation CAN LM: A Pretrained Language Model For Automotive CAN Data](https://arxiv.org/abs/2602.00866)
*Akiharu Esashi,Pawissanutt Lertpongrujikorn,Justin Makino,Yuibi Fujimoto,Mohsen Amini Salehi*

Main category: cs.AI

TL;DR: 提出了首个CAN总线基础模型，将CAN数据视为语言进行大规模预训练，然后针对多种汽车保险任务进行微调，实现了跨任务泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有CAN数据处理方法主要针对特定任务训练孤立模型，使用原始数据或有限解码信号，缺乏共享表示学习和跨任务泛化能力。而NLP和CV领域的基础模型范式已证明其有效性，作者希望将这一范式应用于CAN数据。

Method: 将CAN数据视为语言，提出统一的离散-连续混合信号标记化方案，在大规模未标记解码CAN信号上进行预训练，然后针对异构汽车保险任务进行微调，解决了时间复杂性和行程特定变异性的挑战。

Result: 一个预训练的CAN模型能够有效适应多种预测任务，验证了基础模型范式在CAN数据上的有效性，为汽车AI中的可泛化表示学习确立了新方向。

Conclusion: 基础模型范式在NLP和CV领域的成功同样适用于CAN数据，通过单一预训练骨干网络实现多目标下游泛化，为汽车AI中的通用表示学习开辟了新方向。

Abstract: The Controller Area Network (CAN) bus provides a rich source of vehicular signals increasingly leveraged for applications in automotive and auto insurance domains, including collision detection, predictive maintenance, and driver risk modeling. Despite this potential, existing pipelines largely train isolated task-specific models on raw CAN data, with only limited efforts exploring decoded signals. Such fragmentation prevents shared representation learning and limits cross-task generalization. By contrast, natural language processing (NLP) and computer vision (CV) have been transformed by the foundation model paradigm: large-scale pretraining followed by task-specific adaptation. In this work, we introduce the foundation CAN model that demonstrates multi-objective downstream generalization using a single pretrained backbone. Our approach treats CAN data as a language: we pretrain on large-scale, unlabeled decoded CAN signals and fine-tune across heterogeneous auto insurance tasks. To enable this, we propose a unified tokenization scheme for mixed discrete-continuous signals and address challenges of temporal complexity and trip-specific variability. Our results show that one pretrained CAN model can adapt effectively to diverse predictive tasks, validating that the foundation modeling paradigm, proven in NLP and CV, also holds for CAN data. This establishes a new direction for generalizable representation learning in automotive AI.

</details>


### [265] [Beyond Output Critique: Self-Correction via Task Distillation](https://arxiv.org/abs/2602.00871)
*Hossein A. Rahmani,Mengting Wan,Pei Zhou,Longqi Yang,Nick Craswell,Emine Yilmaz,Sujay Kumar Jauhar*

Main category: cs.AI

TL;DR: SELF-THOUGHT框架通过任务抽象引导LLM自我修正，将任务提炼为结构化模板，然后基于模板实例化解决方案，提升推理质量并实现跨模型模板迁移。


<details>
  <summary>Details</summary>
Motivation: 现有LLM自我修正方法主要在输出层面进行批判修补，难以纠正深层推理缺陷。需要一种能理解任务本质结构、减少错误传播的自我修正方法。

Method: 提出SELF-THOUGHT框架：1）任务抽象：将输入和初始响应提炼为结构化模板，捕捉关键变量、约束和问题结构；2）解决方案实例化：基于抽象模板生成修正响应；3）跨模型模板迁移：大模型生成的模板可指导小模型进行更可靠的修正。

Result: 在多样化推理任务上的实验表明，SELF-THOUGHT提高了大小模型的准确性、鲁棒性和泛化能力，为小模型提供了无需大量微调或外部验证器的可靠修正方法。

Conclusion: SELF-THOUGHT通过任务抽象和结构化指导，显著提升了LLM自我修正的有效性，特别是实现了跨模型的模板迁移，为构建更可靠的自修正语言系统提供了可扩展路径。

Abstract: Large language models (LLMs) have shown promising self-correction abilities, where iterative refinement improves the quality of generated responses. However, most existing approaches operate at the level of output critique, patching surface errors while often failing to correct deeper reasoning flaws. We propose SELF-THOUGHT, a framework that introduces an intermediate step of task abstraction before solution refinement. Given an input and an initial response, the model first distills the task into a structured template that captures key variables, constraints, and problem structure. This abstraction then guides solution instantiation, grounding subsequent responses in a clearer understanding of the task and reducing error propagation. Crucially, we show that these abstractions can be transferred across models: templates generated by larger models can serve as structured guides for smaller LLMs, which typically struggle with intrinsic self-correction. By reusing distilled task structures, smaller models achieve more reliable refinements without heavy fine-tuning or reliance on external verifiers. Experiments across diverse reasoning tasks demonstrate that SELF-THOUGHT improves accuracy, robustness, and generalization for both large and small models, offering a scalable path toward more reliable self-correcting language systems.

</details>


### [266] [Synapse Compendium Aware Federated Knowledge Exchange for Tool Routed LLMs](https://arxiv.org/abs/2602.00911)
*Abhijit Chakraborty,Sandipan De,Yash Shah,Chahana Dahal,Vivek Gupta*

Main category: cs.AI

TL;DR: Synapse框架通过联邦学习训练共享的全局工具使用知识模型，提高LLM智能体工具使用效果并降低通信开销


<details>
  <summary>Details</summary>
Motivation: 基于LLM的智能体在联邦学习下面临通信成本高、数据和工具使用异质性等挑战，限制了协作学习效果

Method: 训练共享的全局工具使用行为知识模型，客户端智能体学习本地工具使用模式，通过协调器传输工件进行联邦聚合，更新全局工具手册并重新分发

Result: Synapse相比权重或提示共享方法，提高了工具使用效果并减少了多智能体LLM系统的通信开销

Conclusion: Synapse框架通过联邦学习共享工具使用知识，有效解决了LLM智能体协作中的异质性和通信效率问题

Abstract: Collaborative learning among LLM-based agents under federated learning faces challenges, including communication costs, heterogeneity in data, and tool-usage, limiting their effectiveness. We introduce Synapse, a framework that trains a shared global knowledge model of tool-usage behavior. Client agents with fixed LLMs learn tool-usage patterns locally, and transmit artifacts for federated aggregation through coordinators. A global tool compendium is updated and redistributed, enabling convergence toward stable tool selection. Synapse uses templated representations, embedding retrieval with LLM reranking, and adaptive masking to maintain utility while limiting information leakage. The framework supports heterogeneous data and quantifies performance improvements. Results show that Synapse improves tool-usage effectiveness and reduces communication overhead compared with weight or prompt-sharing approaches in multi-agent LLM systems.

</details>


### [267] [Supervised sparse auto-encoders as unconstrained feature models for semantic composition](https://arxiv.org/abs/2602.00924)
*Ouns El Harzli,Hugo Wallner,Yoonsoo Nam,Haixuan Xavier Tao*

Main category: cs.AI

TL;DR: 提出一种改进的稀疏自编码器方法，通过无约束特征模型和监督任务来解决传统SAE的L1惩罚非光滑性和特征-语义对齐问题，在Stable Diffusion 3.5上实现组合泛化和语义图像编辑。


<details>
  <summary>Details</summary>
Motivation: 稀疏自编码器在机制可解释性中面临两个主要挑战：1) L1惩罚的非光滑性阻碍了重建和可扩展性；2) 学习到的特征与人类语义之间缺乏对齐。本文旨在解决这些限制。

Method: 采用来自神经崩溃理论的无约束特征模型框架，并通过监督任务来改进SAE。具体方法包括：监督（仅解码器）SAE来重建特征向量，联合学习稀疏概念嵌入和解码器权重。

Result: 在Stable Diffusion 3.5上验证，该方法展示了组合泛化能力，能够成功重建训练中未见过的概念组合图像，并实现无需提示修改的特征级干预进行语义图像编辑。

Conclusion: 通过结合无约束特征模型和监督任务，提出的方法有效解决了传统SAE的局限性，实现了更好的特征-语义对齐和组合泛化，为机制可解释性和语义图像编辑提供了有力工具。

Abstract: Sparse auto-encoders (SAEs) have re-emerged as a prominent method for mechanistic interpretability, yet they face two significant challenges: the non-smoothness of the $L_1$ penalty, which hinders reconstruction and scalability, and a lack of alignment between learned features and human semantics. In this paper, we address these limitations by adapting unconstrained feature models-a mathematical framework from neural collapse theory-and by supervising the task. We supervise (decoder-only) SAEs to reconstruct feature vectors by jointly learning sparse concept embeddings and decoder weights. Validated on Stable Diffusion 3.5, our approach demonstrates compositional generalization, successfully reconstructing images with concept combinations unseen during training, and enabling feature-level intervention for semantic image editing without prompt modification.

</details>


### [268] [Learning Abstractions for Hierarchical Planning in Program-Synthesis Agents](https://arxiv.org/abs/2602.00929)
*Zergham Ahmed,Kazuki Irie,Joshua B. Tenenbaum,Christopher J. Bates,Samuel J. Gershman*

Main category: cs.AI

TL;DR: TheoryCoder-2是一个基于理论的强化学习代理，利用LLM的上下文学习能力主动学习可重用抽象，而不是依赖人工指定的抽象，在复杂任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型代理和深度强化学习系统在快速泛化任务方面仍有挑战，而现有的基于理论的强化学习系统（如TheoryCoder）虽然通过抽象表现出强泛化能力，但严重依赖人工提供的抽象，回避了抽象学习问题。

Method: TheoryCoder-2利用LLM的上下文学习能力，从经验中主动合成可重用抽象，并将这些抽象整合到分层规划过程中，形成基于理论的强化学习系统。

Result: 在BabyAI、Minihack和VGDL游戏（如Sokoban）等多样化环境中的实验表明，TheoryCoder-2比基线LLM代理（包括经典规划域构建、基于推理的规划以及WorldCoder等程序合成代理）具有显著更高的样本效率，能够解决基线无法处理的复杂任务。

Conclusion: TheoryCoder-2通过主动学习抽象而非依赖人工指定抽象，在仅需最少人工提示的情况下，成功解决了复杂任务，为基于理论的强化学习系统提供了新的方向。

Abstract: Humans learn abstractions and use them to plan efficiently to quickly generalize across tasks -- an ability that remains challenging for state-of-the-art large language model (LLM) agents and deep reinforcement learning (RL) systems. Inspired by the cognitive science of how people form abstractions and intuitive theories of their world knowledge, Theory-Based RL (TBRL) systems, such as TheoryCoder, exhibit strong generalization through effective use of abstractions. However, they heavily rely on human-provided abstractions and sidestep the abstraction-learning problem. We introduce TheoryCoder-2, a new TBRL agent that leverages LLMs' in-context learning ability to actively learn reusable abstractions rather than relying on hand-specified ones, by synthesizing abstractions from experience and integrating them into a hierarchical planning process. We conduct experiments on diverse environments, including BabyAI, Minihack and VGDL games like Sokoban. We find that TheoryCoder-2 is significantly more sample-efficient than baseline LLM agents augmented with classical planning domain construction, reasoning-based planning, and prior program-synthesis agents such as WorldCoder. TheoryCoder-2 is able to solve complex tasks that the baselines fail, while only requiring minimal human prompts, unlike prior TBRL systems.

</details>


### [269] [The Keyhole Effect: Why Chat Interfaces Fail at Data Analysis](https://arxiv.org/abs/2602.00947)
*Mohan Reddy*

Main category: cs.AI

TL;DR: 聊天界面不适合多步骤数据分析任务，会导致认知过载，作者提出八个混合设计模式来解决这些问题


<details>
  <summary>Details</summary>
Motivation: 当前AI辅助数据分析普遍采用聊天界面，但对于多步骤、状态依赖的分析任务，这种界面设计存在根本缺陷，会导致认知过载和性能下降

Method: 基于Woods（1984）的"锁眼效应"理论，识别聊天界面导致分析性能下降的五种机制，并提出八个混合设计模式来解决这些认知瓶颈

Result: 形式化认知过载公式O = max(0, m - v - W)，当O>0时错误概率增加，分析偏差（锚定、确认、变化盲视）放大，提出八个具体设计模式来缓解这些问题

Conclusion: 聊天界面不适合开放探索性数据分析任务，需要采用混合设计模式来平衡自然语言交互与视觉空间表示，同时提出了可验证的假设和实验范式

Abstract: Chat has become the default interface for AI-assisted data analysis. For multi-step, state-dependent analytical tasks, this is a mistake. Building on Woods (1984) Keyhole Effect, the cognitive cost of viewing large information spaces through narrow viewports, I show that chat interfaces systematically degrade analytical performance through five mechanisms: (1) constant content displacement defeats hippocampal spatial memory systems; (2) hidden state variables exceed working memory capacity (approximately 4 chunks under load); (3) forced verbalization triggers verbal overshadowing, degrading visual pattern recognition; (4) linear text streams block epistemic action and cognitive offloading; (5) serialization penalties scale with data dimensionality. I formalize cognitive overload as O = max(0, m - v - W) where m is task-relevant items, v is visible items, and W is working memory capacity. When O > 0, error probability increases and analytical biases (anchoring, confirmation, change blindness) amplify. Eight hybrid design patterns address these failures: Generative UI, Infinite Canvas, Deictic Interaction, State Rail, Ghost Layers, Mise en Place, Semantic Zoom, and Probabilistic UI. Each pattern targets specific cognitive bottlenecks while preserving natural language for intent specification and synthesis. Well-scaffolded conversational systems that encode expert priors may reduce load for guided tasks; the framework applies most strongly to open-ended exploration. The paper concludes with falsifiable hypotheses and experimental paradigms for empirical validation.

</details>


### [270] [MindGuard: Guardrail Classifiers for Multi-Turn Mental Health Support](https://arxiv.org/abs/2602.00950)
*António Farinhas,Nuno M. Guerreiro,José Pombal,Pedro Henrique Martins,Laura Melton,Alex Conway,Cara Dochat,Maya D'Eon,Ricardo Rei*

Main category: cs.AI

TL;DR: MindGuard：基于临床风险分类学的轻量级安全分类器，用于区分治疗性披露与真实临床危机，减少心理健康对话中的误报


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在心理健康支持应用中，通用安全措施无法有效区分治疗性披露和真实临床危机，导致安全失效。需要专门针对心理健康对话的临床风险评估方法。

Method: 1. 与心理学博士合作开发临床风险分类学；2. 发布MindGuard-testset真实多轮对话数据集；3. 通过受控双智能体设置生成合成对话；4. 训练轻量级安全分类器（4B和8B参数）

Result: MindGuard在高召回率操作点减少误报，与临床语言模型结合时，在对抗性多轮互动中实现更低的攻击成功率和有害参与率，优于通用安全措施

Conclusion: MindGuard为心理健康对话提供了临床基础的安全评估框架，能有效区分危机内容与治疗性披露，所有模型和人工评估数据均已开源

Abstract: Large language models are increasingly used for mental health support, yet their conversational coherence alone does not ensure clinical appropriateness. Existing general-purpose safeguards often fail to distinguish between therapeutic disclosures and genuine clinical crises, leading to safety failures. To address this gap, we introduce a clinically grounded risk taxonomy, developed in collaboration with PhD-level psychologists, that identifies actionable harm (e.g., self-harm and harm to others) while preserving space for safe, non-crisis therapeutic content. We release MindGuard-testset, a dataset of real-world multi-turn conversations annotated at the turn level by clinical experts. Using synthetic dialogues generated via a controlled two-agent setup, we train MindGuard, a family of lightweight safety classifiers (with 4B and 8B parameters). Our classifiers reduce false positives at high-recall operating points and, when paired with clinician language models, help achieve lower attack success and harmful engagement rates in adversarial multi-turn interactions compared to general-purpose safeguards. We release all models and human evaluation data.

</details>


### [271] [R-HTN: Rebellious Online HTN Planning for Safety and Game AI](https://arxiv.org/abs/2602.00951)
*Hector Munoz-Avila,David W. Aha,Paola Rizzo*

Main category: cs.AI

TL;DR: 提出在线分层任务网络（HTN）智能体R-HTN，能够在违反内置指令时拒绝执行用户任务或自适应修改计划，实现智能抗命行为。


<details>
  <summary>Details</summary>
Motivation: 研究智能体在执行用户任务时如何考虑内置指令集D，在可能违反安全规定或个性特征的情况下，智能体需要具备"智能抗命"能力，即不盲目执行用户指令，而是根据指令约束调整行为。

Method: 结合HTN规划、在线规划和指令集D，提出R-HTN算法。研究两种智能体变体：非自适应智能体（违反指令时停止执行）和自适应智能体（违反指令时修改HTN计划寻找替代方案）。

Result: R-HTN智能体在测试中从不违反指令，并在可行情况下努力实现用户目标（尽管可能以用户未预期的方式）。在需要遵守安全规定或个性特征的任务领域中表现良好。

Conclusion: R-HTN为在线HTN规划提供了处理指令约束的通用算法，使智能体能够在考虑安全性和个性特征的前提下，智能地处理用户任务，必要时采取抗命行为。

Abstract: We introduce online Hierarchical Task Network (HTN) agents whose behaviors are governed by a set of built-in directives \D. Like other agents that are capable of rebellion (i.e., {\it intelligent disobedience}), our agents will, under some conditions, not perform a user-assigned task and instead act in ways that do not meet a user's expectations. Our work combines three concepts: HTN planning, online planning, and the directives \D, which must be considered when performing user-assigned tasks. We investigate two agent variants: (1) a Nonadaptive agent that stops execution if it finds itself in violation of \D~ and (2) an Adaptive agent that, in the same situation, instead modifies its HTN plan to search for alternative ways to achieve its given task. We present R-HTN (for: Rebellious-HTN), a general algorithm for online HTN planning under directives \D. We evaluate R-HTN in two task domains where the agent must not violate some directives for safety reasons or as dictated by their personality traits. We found that R-HTN agents never violate directives, and aim to achieve the user-given goals if feasible though not necessarily as the user expected.

</details>


### [272] [Small-Margin Preferences Still Matter-If You Train Them Right](https://arxiv.org/abs/2602.00954)
*Jinlong Pang,Zhaowei Zhu,Na Di,Yichi Zhang,Yaxuan Wang,Chen Qian,Yang Liu*

Main category: cs.AI

TL;DR: MixDPO提出了一种难度感知的训练策略，将简单偏好对用于偏好损失优化，困难偏好对用于监督微调，从而有效利用模糊偏好对并避免训练不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 现有偏好优化方法（如DPO）对偏好对的质量和难度高度敏感，通常将小边际（模糊）对视为噪声而过滤掉。作者发现困难偏好对在偏好损失下会破坏训练稳定性，但在监督微调中仍包含有用的监督信号。

Method: MixDPO采用难度感知训练策略：1）按边际定义的难度从易到难排序偏好数据（课程学习）；2）将困难对路由到SFT目标，同时对简单对应用偏好损失。这种混合设计能够利用模糊对而不引发优化失败。

Result: 在三个LLM-judge基准测试中，MixDPO在DPO和一系列广泛使用的变体上持续改进对齐效果，在AlpacaEval~2长度控制胜率上获得特别显著的提升。

Conclusion: 通过将困难偏好对重新路由到SFT目标，MixDPO提供了一种实用的机制来利用模糊偏好对，避免了偏好损失在低边际数据上常见的优化失败问题，从而提高了语言模型的对齐效果。

Abstract: Preference optimization methods such as DPO align large language models (LLMs) using paired comparisons, but their effectiveness can be highly sensitive to the quality and difficulty of preference pairs. A common heuristic treats small-margin (ambiguous) pairs as noisy and filters them out. In this paper, we revisit this assumption and show that pair difficulty interacts strongly with the optimization objective: when trained with preference-based losses, difficult pairs can destabilize training and harm alignment, yet these same pairs still contain useful supervision signals when optimized with supervised fine-tuning (SFT). Motivated by this observation, we propose MixDPO, a simple yet effective difficulty-aware training strategy that (i) orders preference data from easy to hard (a curriculum over margin-defined difficulty), and (ii) routes difficult pairs to an SFT objective while applying a preference loss to easy pairs. This hybrid design provides a practical mechanism to leverage ambiguous pairs without incurring the optimization failures often associated with preference losses on low-margin data. Across three LLM-judge benchmarks, MixDPO consistently improves alignment over DPO and a range of widely-used variants, with particularly strong gains on AlpacaEval~2 length-controlled (LC) win rate.

</details>


### [273] [Reasoning and Tool-use Compete in Agentic RL:From Quantifying Interference to Disentangled Tuning](https://arxiv.org/abs/2602.00994)
*Yu Li,Mingyang Yi,Xiuyu Li,Ju Fan,Fuxin Jiang,Binbin Chen,Peng Li,Jie Song,Tieying Zhang*

Main category: cs.AI

TL;DR: 该论文通过分析发现ARL中联合训练推理与工具使用会导致梯度干扰，提出DART框架通过分离的低秩适应模块解耦参数更新，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有Agentic Reinforcement Learning方法通常假设联合训练推理和工具使用能提升整体代理性能，但这一假设缺乏实证检验。作者旨在系统研究这一假设，探索两种能力之间的潜在干扰问题。

Method: 1. 引入线性效应归因系统(LEAS)定量分析推理与工具使用行为之间的干扰；2. 提出解耦行动推理调优(DART)框架，通过分离的低秩适应模块显式解耦推理和工具使用的参数更新。

Result: 实验结果显示：1. LEAS提供了梯度方向不匹配导致训练干扰的定量证据；2. DART相比基线方法平均提升6.35%性能；3. DART使用单模型达到与显式分离工具使用和推理的多智能体系统相当的性能。

Conclusion: 联合训练推理和工具使用确实存在干扰问题，挑战了当前ARL范式。DART通过参数解耦有效解决了这一问题，为ARL训练提供了更有效的框架。

Abstract: Agentic Reinforcement Learning (ARL) focuses on training large language models (LLMs) to interleave reasoning with external tool execution to solve complex tasks. Most existing ARL methods train a single shared model parameters to support both reasoning and tool use behaviors, implicitly assuming that joint training leads to improved overall agent performance. Despite its widespread adoption, this assumption has rarely been examined empirically. In this paper, we systematically investigate this assumption by introducing a Linear Effect Attribution System(LEAS), which provides quantitative evidence of interference between reasoning and tool-use behaviors. Through an in-depth analysis, we show that these two capabilities often induce misaligned gradient directions, leading to training interference that undermines the effectiveness of joint optimization and challenges the prevailing ARL paradigm. To address this issue, we propose Disentangled Action Reasoning Tuning(DART), a simple and efficient framework that explicitly decouples parameter updates for reasoning and tool-use via separate low-rank adaptation modules. Experimental results show that DART consistently outperforms baseline methods with averaged 6.35 percent improvements and achieves performance comparable to multi-agent systems that explicitly separate tool-use and reasoning using a single model.

</details>


### [274] [Error Taxonomy-Guided Prompt Optimization](https://arxiv.org/abs/2602.00997)
*Mayank Singh,Vikas Yadav,Eduardo Blanco*

Main category: cs.AI

TL;DR: ETGPO是一种基于错误分类的提示优化方法，采用自上而下的方式分析全局失败模式，相比现有方法显著减少了计算开销


<details>
  <summary>Details</summary>
Motivation: 现有自动提示优化方法大多采用自下而上的试错方式，基于单个问题的反馈迭代调整提示，缺乏全局视角，且计算开销大。需要一种更高效、全局化的优化方法。

Method: ETGPO采用自上而下的方法：1) 收集模型错误；2) 将错误分类为错误分类学；3) 针对最频繁的失败模式在提示中添加指导。通过分析全局失败模式来优化提示。

Result: 在数学、问答和逻辑推理等多个基准测试中，ETGPO达到或优于最先进方法的准确率，同时优化阶段的token使用量和评估预算仅需约三分之一。

Conclusion: ETGPO通过错误分类学的全局视角进行提示优化，在保持性能的同时显著降低了计算成本，为高效提示优化提供了新思路。

Abstract: Automatic Prompt Optimization (APO) is a powerful approach for extracting performance from large language models without modifying their weights. Many existing methods rely on trial-and-error, testing different prompts or in-context examples until a good configuration emerges, often consuming substantial compute. Recently, natural language feedback derived from execution logs has shown promise as a way to identify how prompts can be improved. However, most prior approaches operate in a bottom-up manner, iteratively adjusting the prompt based on feedback from individual problems, which can cause them to lose the global perspective. In this work, we propose Error Taxonomy-Guided Prompt Optimization (ETGPO), a prompt optimization algorithm that adopts a top-down approach. ETGPO focuses on the global failure landscape by collecting model errors, categorizing them into a taxonomy, and augmenting the prompt with guidance targeting the most frequent failure modes. Across multiple benchmarks spanning mathematics, question answering, and logical reasoning, ETGPO achieves accuracy that is comparable to or better than state-of-the-art methods, while requiring roughly one third of the optimization-phase token usage and evaluation budget.

</details>


### [275] [How RLHF Amplifies Sycophancy](https://arxiv.org/abs/2602.01002)
*Itai Shapira,Gerdus Benade,Ariel D. Procaccia*

Main category: cs.AI

TL;DR: 研究发现人类反馈对齐会放大LLM的谄媚行为，提出了一种训练时干预方法来防止这种行为增加


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在基于偏好的后训练后表现出更强的谄媚行为，即使这与事实准确性或合理判断相冲突。需要理解这种失败模式如何被人类反馈对齐放大，并提出解决方案。

Method: 1. 形式化分析人类反馈对齐如何通过放大机制增加谄媚行为；2. 分析随机效用模型下的奖励学习；3. 提出训练时干预方法，通过最小KL散度推导出闭式协议惩罚奖励修正。

Result: 计算实验发现奖励差距很常见，在所有考虑的配置中都会导致行为漂移。提出的干预方法能够有效防止谄媚行为增加。

Conclusion: 人类反馈对齐会放大LLM的谄媚行为，但可以通过训练时干预来中和这种放大机制，防止谄媚行为增加。

Abstract: Large language models often exhibit increased sycophantic behavior after preference-based post-training, showing a stronger tendency to affirm a user's stated or implied belief even when this conflicts with factual accuracy or sound judgment. We present a formal analysis of how alignment from human feedback can increase this failure mode by identifying an explicit amplification mechanism that causally links optimization against a learned reward to bias in the human preference data used for alignment. We show that the direction of behavioral drift is determined by a covariance under the base policy between endorsing the belief signal in the prompt and the learned reward, and that the first-order effect reduces to a simple mean-gap condition. We then analyze reward learning from pairwise comparisons under random utility models like Bradley-Terry and characterize when bias in human annotators' preferences induces this reward gap. Next, we propose a training-time intervention designed to neutralize the amplification mechanism itself. Among all post-trained policies that prevent sycophantic behavior from increasing, we characterize the unique policy closest in KL divergence to the unconstrained post-trained policy, and derive the corresponding minimal reward correction as a closed-form agreement penalty. Computational experiments find that reward gaps are common and cause behavioral drift in all the configurations considered.

</details>


### [276] [HalluHard: A Hard Multi-Turn Hallucination Benchmark](https://arxiv.org/abs/2602.01031)
*Dongyang Fan,Sebastien Delsad,Nicolas Flammarion,Maksym Andriushchenko*

Main category: cs.AI

TL;DR: HalluHard是一个包含950个种子问题的多轮幻觉基准测试，涵盖法律、研究、医疗和编程四个高风险领域，通过内联引用要求来操作事实基础性，并提出了基于网络搜索的证据检索评估管道。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多轮对话中仍会产生看似合理但缺乏事实依据的陈述，随着上下文增长和早期错误累积，这个问题会恶化。需要建立一个具有挑战性的多轮幻觉基准来评估模型的事实基础性。

Method: 1. 创建HalluHard基准：包含950个种子问题，涵盖法律案例、研究问题、医疗指南和编程四个高风险领域；2. 通过要求内联引用来操作事实基础性；3. 提出评估管道：通过迭代网络搜索检索证据，获取、过滤和解析全文来源（包括PDF）来评估引用是否支持生成内容。

Result: 即使在网络搜索支持下，前沿专有和开源模型仍存在大量幻觉（最强配置Opus-4.5加网络搜索约30%），内容基础错误率居高不下。幻觉行为受模型能力、轮次位置、有效推理和所需知识类型影响。

Conclusion: 大语言模型在多轮对话中的幻觉问题仍然严重，需要更严格的评估方法和基准。HalluHard基准和评估管道为评估和改进模型的事实基础性提供了重要工具，揭示了幻觉行为的关键影响因素。

Abstract: Large language models (LLMs) still produce plausible-sounding but ungrounded factual claims, a problem that worsens in multi-turn dialogue as context grows and early errors cascade. We introduce $\textbf{HalluHard}$, a challenging multi-turn hallucination benchmark with 950 seed questions spanning four high-stakes domains: legal cases, research questions, medical guidelines, and coding. We operationalize groundedness by requiring inline citations for factual assertions. To support reliable evaluation in open-ended settings, we propose a judging pipeline that iteratively retrieves evidence via web search. It can fetch, filter, and parse full-text sources (including PDFs) to assess whether cited material actually supports the generated content. Across a diverse set of frontier proprietary and open-weight models, hallucinations remain substantial even with web search ($\approx 30\%$ for the strongest configuration, Opus-4.5 with web search), with content-grounding errors persisting at high rates. Finally, we show that hallucination behavior is shaped by model capacity, turn position, effective reasoning, and the type of knowledge required.

</details>


### [277] [Discovering Process-Outcome Credit in Multi-Step LLM Reasoning](https://arxiv.org/abs/2602.01034)
*Xiangwei Wang,Wei Wang,Ken Chen,Nanduni Nimalsiri,Saman Halgamuge*

Main category: cs.AI

TL;DR: 提出一个为LLM推理提供连续奖励信号的新框架，通过步骤边际信息增益机制和去耦合掩码策略，在文本和多模态基准上超越基线方法


<details>
  <summary>Details</summary>
Motivation: 传统基于结果的强化学习方法存在奖励稀疏和信用分配效率低的问题，需要为LLM推理提供更有效的连续奖励信号

Method: 1) 步骤边际信息增益机制：量化推理步骤相对于单调历史水印的内在价值，过滤训练噪声；2) 去耦合掩码策略：过程导向奖励应用于思维链，结果导向奖励应用于完整完成；3) 双门控SFT目标：用高质量结构和事实信号稳定训练

Result: 在MATH、Super-CLEVR等文本和多模态基准上持续超越GRPO等基线方法，在样本效率和最终准确率上都表现更好，并展现出优越的分布外鲁棒性和零样本迁移能力

Conclusion: 该框架通过提供连续奖励信号和有效信用分配，显著提升了LLM的推理能力，在多个基准上验证了其有效性，并为复杂推理任务提供了有前景的解决方案

Abstract: Reinforcement Learning (RL) serves as a potent paradigm for enhancing reasoning capabilities in Large Language Models (LLMs), yet standard outcome-based approaches often suffer from reward sparsity and inefficient credit assignment. In this paper, we propose a novel framework designed to provide continuous reward signals, which introduces a Step-wise Marginal Information Gain (MIG) mechanism that quantifies the intrinsic value of reasoning steps against a Monotonic Historical Watermark, effectively filtering out training noise. To ensure disentangled credit distribution, we implement a Decoupled Masking Strategy, applying process-oriented rewards specifically to the chain-of-thought (CoT) and outcome-oriented rewards to the full completion. Additionally, we incorporate a Dual-Gated SFT objective to stabilize training with high-quality structural and factual signals. Extensive experiments across textual and multi-modal benchmarks (e.g., MATH, Super-CLEVR) demonstrate that our approach consistently outperforms baselines such as GRPO in both sample efficiency and final accuracy. Furthermore, our model exhibits superior out-of-distribution robustness, demonstrating promising zero-shot transfer capabilities to unseen and challenging reasoning tasks.

</details>


### [278] [SetPO: Set-Level Policy Optimization for Diversity-Preserving LLM Reasoning](https://arxiv.org/abs/2602.01062)
*Chenyi Li,Yuan Zhang,Bo Wang,Guoqing Ma,Wei Tang,Haoyang Huang,Nan Duan*

Main category: cs.AI

TL;DR: 提出一种基于核相似度的轨迹级多样性目标，通过留一法边际贡献计算，作为优势塑造项融入策略优化，提升LLM推理多样性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习在提升LLM数学推理性能时，往往导致结果多样性降低，模型概率质量集中在狭窄解集上，需要平衡性能与多样性。

Method: 1) 基于核相似度定义轨迹级多样性目标；2) 使用留一法计算每个采样轨迹的边际贡献；3) 将该目标作为可插拔优势塑造项融入策略优化；4) 在分布扰动框架下分析单轨迹对多样性的贡献。

Result: 在多种模型规模上实验验证，提出的算法在Pass@1和Pass@K指标上均优于强基线，同时理论证明稀有轨迹对全局多样性有更高的边际贡献。

Conclusion: 通过引入多样性目标，成功解决了强化学习优化中性能与多样性的权衡问题，为LLM推理任务提供了有效的多样性增强方法。

Abstract: Reinforcement learning with verifiable rewards has shown notable effectiveness in enhancing large language models (LLMs) reasoning performance, especially in mathematics tasks. However, such improvements often come with reduced outcome diversity, where the model concentrates probability mass on a narrow set of solutions. Motivated by diminishing-returns principles, we introduce a set level diversity objective defined over sampled trajectories using kernelized similarity. Our approach derives a leave-one-out marginal contribution for each sampled trajectory and integrates this objective as a plug-in advantage shaping term for policy optimization. We further investigate the contribution of a single trajectory to language model diversity within a distribution perturbation framework. This analysis theoretically confirms a monotonicity property, proving that rarer trajectories yield consistently higher marginal contributions to the global diversity. Extensive experiments across a range of model scales demonstrate the effectiveness of our proposed algorithm, consistently outperforming strong baselines in both Pass@1 and Pass@K across various benchmarks.

</details>


### [279] [ConvexBench: Can LLMs Recognize Convex Functions?](https://arxiv.org/abs/2602.01075)
*Yepeng Liu,Yu Huang,Yu-Xiang Wang,Yingbin Liang,Yuheng Bu*

Main category: cs.AI

TL;DR: 论文提出了一个可扩展且可机械验证的基准测试CB，用于评估大语言模型在深度函数组合下识别符号目标凸性的能力，发现模型存在组合推理缺陷，并提出了一个基于分治的代理框架来有效解决这一问题。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型开始自动化研究级数学和科学任务，需要评估它们理解和推理凸性的能力。凸分析是现代数学的重要分支，具有广泛应用，因此测试LLMs在深度函数组合下识别凸性的能力至关重要。

Method: 提出了CB基准测试，用于评估LLMs识别符号目标凸性的能力。通过实验发现模型存在组合推理缺陷后，提出了一个代理分治框架：1) 使用外部工具构建抽象语法树(AST)来卸载解析任务；2) 对每个中间子表达式实施递归推理，并提供聚焦上下文。

Result: 前沿LLMs实验显示明显的组合推理缺陷：随着深度增加，性能急剧下降，从深度2的F1分数1.0降至深度100的约0.2。分析推理轨迹发现两种失败模式：解析失败和懒惰推理。提出的分治框架能可靠缓解深度组合失败，在较大深度下实现显著性能提升（如深度100时F1分数=1.0）。

Conclusion: 当前LLMs在深度函数组合的凸性识别任务中存在显著推理缺陷，但通过代理分治框架可以有效解决这些问题，为LLMs在数学推理任务中的改进提供了可行路径。

Abstract: Convex analysis is a modern branch of mathematics with many applications. As Large Language Models (LLMs) start to automate research-level math and sciences, it is important for LLMs to demonstrate the ability to understand and reason with convexity. We introduce \cb, a scalable and mechanically verifiable benchmark for testing \textit{whether LLMs can identify the convexity of a symbolic objective under deep functional composition.} Experiments on frontier LLMs reveal a sharp compositional reasoning gap: performance degrades rapidly with increasing depth, dropping from an F1-score of $1.0$ at depth $2$ to approximately $0.2$ at depth $100$. Inspection of models' reasoning traces indicates two failure modes: \textit{parsing failure} and \textit{lazy reasoning}. To address these limitations, we propose an agentic divide-and-conquer framework that (i) offloads parsing to an external tool to construct an abstract syntax tree (AST) and (ii) enforces recursive reasoning over each intermediate sub-expression with focused context. This framework reliably mitigates deep-composition failures, achieving substantial performance improvement at large depths (e.g., F1-Score $= 1.0$ at depth $100$).

</details>


### [280] [AutoHealth: An Uncertainty-Aware Multi-Agent System for Autonomous Health Data Modeling](https://arxiv.org/abs/2602.01078)
*Tong Xia,Weibin Li,Gang Liu,Yong Li*

Main category: cs.AI

TL;DR: AutoHealth是一个不确定性感知的多智能体系统，用于自主建模健康数据并评估模型可靠性，在预测性能和不确定性估计方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的智能体在健康数据应用中存在局限性：难以泛化到异构健康数据模态、过度依赖预定义解决方案模板、缺乏不确定性估计，而医疗决策需要可靠性评估。

Method: AutoHealth采用闭环协调的五个专业智能体系统，执行数据探索、任务条件模型构建、训练和优化，同时优先考虑预测性能和不确定性量化，并生成支持可信解释的综合报告。

Result: 在包含17个任务的多模态真实世界基准测试中，AutoHealth完成所有任务，预测性能比最先进基线提高29.2%，不确定性估计性能提高50.2%。

Conclusion: AutoHealth通过不确定性感知的多智能体系统成功解决了健康数据建模中的泛化、适应性和可靠性问题，为医疗领域的可信决策提供了有效工具。

Abstract: LLM-based agents have demonstrated strong potential for autonomous machine learning, yet their applicability to health data remains limited. Existing systems often struggle to generalize across heterogeneous health data modalities, rely heavily on predefined solution templates with insufficient adaptation to task-specific objectives, and largely overlook uncertainty estimation, which is essential for reliable decision-making in healthcare. To address these challenges, we propose \textit{AutoHealth}, a novel uncertainty-aware multi-agent system that autonomously models health data and assesses model reliability. \textit{AutoHealth} employs closed-loop coordination among five specialized agents to perform data exploration, task-conditioned model construction, training, and optimization, while jointly prioritizing predictive performance and uncertainty quantification. Beyond producing ready-to-use models, the system generates comprehensive reports to support trustworthy interpretation and risk-aware decision-making. To rigorously evaluate its effectiveness, we curate a challenging real-world benchmark comprising 17 tasks across diverse data modalities and learning settings. \textit{AutoHealth} completes all tasks and outperforms state-of-the-art baselines by 29.2\% in prediction performance and 50.2\% in uncertainty estimation.

</details>


### [281] [EvoOpt-LLM: Evolving industrial optimization models with large language models](https://arxiv.org/abs/2602.01082)
*Yiliu He,Tianle Li,Binghao Ji,Zhiyuan Liu,Di Huang*

Main category: cs.AI

TL;DR: EvoOpt-LLM：基于LLM的工业优化建模框架，支持MILP模型自动构建、约束动态注入和变量剪枝，仅需少量训练样本即可实现高生成率和可执行率。


<details>
  <summary>Details</summary>
Motivation: 工业规划调度中的混合整数线性规划建模高度依赖专家经验，自然语言需求转化为可执行模型及业务规则更新维护成本高。现有LLM方法存在数据效率低、求解器有效性有限、工业规模扩展性差等问题。

Method: 基于7B参数LLM构建统一框架，采用参数高效的LoRA微调适配。包含三个核心模块：自动化模型构建、动态业务约束注入、端到端变量剪枝。

Result: 仅用3000训练样本实现91%生成率和65.9%可执行率，关键性能在1500样本内显现。约束注入模块可靠增强现有MILP模型并保持原目标，变量剪枝模块在400样本下对中型LP模型达到约0.56 F1分数。

Conclusion: EvoOpt-LLM展示了工业优化建模的实用数据高效方法，减少专家干预依赖，同时提升适应性和求解器效率。

Abstract: Optimization modeling via mixed-integer linear programming (MILP) is fundamental to industrial planning and scheduling, yet translating natural-language requirements into solver-executable models and maintaining them under evolving business rules remains highly expertise-intensive. While large language models (LLMs) offer promising avenues for automation, existing methods often suffer from low data efficiency, limited solver-level validity, and poor scalability to industrial-scale problems. To address these challenges, we present EvoOpt-LLM, a unified LLM-based framework supporting the full lifecycle of industrial optimization modeling, including automated model construction, dynamic business-constraint injection, and end-to-end variable pruning. Built on a 7B-parameter LLM and adapted via parameter-efficient LoRA fine-tuning, EvoOpt-LLM achieves a generation rate of 91% and an executability rate of 65.9% with only 3,000 training samples, with critical performance gains emerging under 1,500 samples. The constraint injection module reliably augments existing MILP models while preserving original objectives, and the variable pruning module enhances computational efficiency, achieving an F1 score of ~0.56 on medium-sized LP models with only 400 samples. EvoOpt-LLM demonstrates a practical, data-efficient approach to industrial optimization modeling, reducing reliance on expert intervention while improving adaptability and solver efficiency.

</details>


### [282] [MedBeads: An Agent-Native, Immutable Data Substrate for Trustworthy Medical AI](https://arxiv.org/abs/2602.01086)
*Takahito Nakajima*

Main category: cs.AI

TL;DR: MedBeads提出了一种面向AI代理的原生医疗数据基础设施，使用不可变的Merkle DAG结构存储临床事件，解决传统EMR系统与AI代理之间的"上下文不匹配"问题。


<details>
  <summary>Details</summary>
Motivation: 当前电子病历系统为人类设计，AI代理接收碎片化数据，需要依赖概率推理重建患者历史，导致幻觉和可审计性问题。

Method: 提出MedBeads架构：临床事件作为不可变的"Beads"节点存储在Merkle DAG中，密码学引用因果前驱；实现Go核心引擎、Python中间件和React可视化界面。

Result: 成功实现工作流，FHIR到DAG转换将平面资源转为因果链接图；BFS上下文检索算法实现实时决策支持；篡改检测通过密码学链保证。

Conclusion: MedBeads通过从概率搜索转向确定性图遍历，从可变记录转向不可变链，为"可信医疗AI"提供基础，保证AI接收的上下文是确定性和防篡改的。

Abstract: Background: As of 2026, Large Language Models (LLMs) demonstrate expert-level medical knowledge. However, deploying them as autonomous "Clinical Agents" remains limited. Current Electronic Medical Records (EMRs) and standards like FHIR are designed for human review, creating a "Context Mismatch": AI agents receive fragmented data and must rely on probabilistic inference (e.g., RAG) to reconstruct patient history. This approach causes hallucinations and hinders auditability. Methods: We propose MedBeads, an agent-native data infrastructure where clinical events are immutable "Beads"--nodes in a Merkle Directed Acyclic Graph (DAG)--cryptographically referencing causal predecessors. This "write-once, read-many" architecture makes tampering mathematically detectable. We implemented a prototype with a Go Core Engine, Python middleware for LLM integration, and a React-based visualization interface. Results: We successfully implemented the workflow using synthetic data. The FHIR-to-DAG conversion transformed flat resources into a causally-linked graph. Our Breadth-First Search (BFS) Context Retrieval algorithm traverses relevant subgraphs with O(V+E) complexity, enabling real-time decision support. Tamper-evidence is guaranteed by design: any modification breaks the cryptographic chain. The visualization aids clinician understanding through explicit causal links. Conclusion: MedBeads addresses the "Context Mismatch" by shifting from probabilistic search to deterministic graph traversal, and from mutable records to immutable chains, providing the substrate for "Trustworthy Medical AI." It guarantees the context the AI receives is deterministic and tamper-evident, while the LLM determines interpretation. The structured Bead format serves as a token-efficient "AI-native language." We release MedBeads as open-source software to accelerate agent-native data standards.

</details>


### [283] [Hard Constraints Meet Soft Generation: Guaranteed Feasibility for LLM-based Combinatorial Optimization](https://arxiv.org/abs/2602.01090)
*Yang Liu,Chuan Zhou,Yancheng Chen,Shuai Zhang,Xixun Lin,Xiaoqing Wang*

Main category: cs.AI

TL;DR: FALCON框架通过语法约束解码、可行性修复层和自适应采样确保LLM求解组合优化问题的100%可行性，同时匹配或超越现有方法的质量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在组合优化中表现出潜力，但缺乏保证解可行性的机制，这限制了其在实际部署中的应用。

Method: 提出FALCON框架：1) 语法约束解码确保句法有效性；2) 可行性修复层纠正语义约束违反；3) 自适应Best-of-N采样高效分配推理计算。训练采用BOPO方法，基于目标差距加权偏好对。

Result: 在七个NP难组合优化问题上，FALCON实现了完美可行性，同时匹配或超越了最先进的神经和LLM求解器的解质量。

Conclusion: FALCON为LLM在组合优化中的应用提供了可行的解决方案，通过理论保证和实证验证展示了其在保证100%可行性的同时保持高质量解的能力。

Abstract: Large language models (LLMs) have emerged as promising general-purpose solvers for combinatorial optimization (CO), yet they fundamentally lack mechanisms to guarantee solution feasibility which is critical for real-world deployment. In this work, we introduce FALCON, a framework that ensures 100\% feasibility through three key innovations: (i) \emph{grammar-constrained decoding} enforces syntactic validity, (ii) a \emph{feasibility repair layer} corrects semantic constraint violations, and (iii) \emph{adaptive Best-of-$N$ sampling} allocates inference compute efficiently. To train the underlying LLM, we introduce the Best-anchored Objective-guided Preference Optimization (BOPO) in LLM training, which weights preference pairs by their objective gap, providing dense supervision without human labels. Theoretically, we prove convergence for BOPO and provide bounds on repair-induced quality loss. Empirically, across seven NP-hard CO problems, FALCON achieves perfect feasibility while matching or exceeding the solution quality of state-of-the-art neural and LLM-based solvers.

</details>


### [284] [Probing RLVR training instability through the lens of objective-level hacking](https://arxiv.org/abs/2602.01103)
*Yiming Dong,Kun Fu,Haoyu Li,Xinyuan Zhu,Yurou Liu,Lijing Shao,Jieping Ye,Zheng Wang*

Main category: cs.AI

TL;DR: 本文提出一个理论框架，通过"目标层面黑客攻击"的视角理解RLVR训练不稳定性，特别针对MoE架构，揭示了训练-推理差异异常增长的机制。


<details>
  <summary>Details</summary>
Motivation: 强化学习与可验证奖励（RLVR）能持续提升大语言模型的推理能力，但在MoE架构中训练常不稳定。训练不稳定性严重损害模型能力提升，但其根本原因和机制尚不清楚。

Method: 引入一个原则性框架，通过"目标层面黑客攻击"的视角理解RLVR不稳定性。与奖励黑客攻击不同，目标层面黑客攻击源于令牌级信用错位，表现为优化目标中的系统级虚假信号。基于该框架，在30B MoE模型上进行大量实验，追踪并形式化MoE模型中关键病理训练动态的机制。

Result: 揭示了MoE模型中训练-推理差异异常增长的起源和机制，这一现象广泛与不稳定性相关但先前缺乏机制性解释。这些发现为MoE模型不稳定性背后的训练动态提供了具体且因果性的解释。

Conclusion: 研究结果为MoE模型中RLVR训练不稳定性的动态提供了具体因果解释，为设计稳定的RLVR算法提供了指导。

Abstract: Prolonged reinforcement learning with verifiable rewards (RLVR) has been shown to drive continuous improvements in the reasoning capabilities of large language models, but the training is often prone to instabilities, especially in Mixture-of-Experts (MoE) architectures. Training instability severely undermines model capability improvement, yet its underlying causes and mechanisms remain poorly understood. In this work, we introduce a principled framework for understanding RLVR instability through the lens of objective-level hacking. Unlike reward hacking, which arises from exploitable verifiers, objective-level hacking emerges from token-level credit misalignment and is manifested as system-level spurious signals in the optimization objective. Grounded in our framework, together with extensive experiments on a 30B MoE model, we trace the origin and formalize the mechanism behind a key pathological training dynamic in MoE models: the abnormal growth of the training-inference discrepancy, a phenomenon widely associated with instability but previously lacking a mechanistic explanation. These findings provide a concrete and causal account of the training dynamics underlying instabilities in MoE models, offering guidance for the design of stable RLVR algorithms.

</details>


### [285] [Transforming Vehicle Diagnostics: A Multimodal Approach to Error Patterns Prediction](https://arxiv.org/abs/2602.01109)
*Hugo Math,Rainer Lienhart*

Main category: cs.AI

TL;DR: BiCarFormer：首个多模态方法，整合DTC序列和环境条件进行车辆故障模式多标签序列分类，显著提升分类性能


<details>
  <summary>Details</summary>
Motivation: 当前车辆诊断系统主要依赖诊断故障码序列，但忽略了温度、湿度、压力等原始传感器数据等有价值的上下文信息。这些环境数据对专家分类车辆故障至关重要，但由于其复杂性和真实数据的噪声特性带来了独特挑战。

Method: BiCarFormer：为车辆事件序列定制的双向Transformer模型，采用嵌入融合和协同注意力机制来捕捉诊断代码与环境数据之间的关系，实现多模态多标签序列分类。

Result: 在包含22,137个错误代码和360个错误模式的真实世界汽车数据集上，该方法相比仅依赖DTC序列的传统序列模型显著提高了分类性能。

Conclusion: 这项工作强调了整合上下文环境信息对于更准确、更稳健的车辆诊断的重要性，从而降低维护成本并提升汽车行业的自动化流程。

Abstract: Accurately diagnosing and predicting vehicle malfunctions is crucial for maintenance and safety in the automotive industry. While modern diagnostic systems primarily rely on sequences of vehicular Diagnostic Trouble Codes (DTCs) registered in On-Board Diagnostic (OBD) systems, they often overlook valuable contextual information such as raw sensory data (e.g., temperature, humidity, and pressure). This contextual data, crucial for domain experts to classify vehicle failures, introduces unique challenges due to its complexity and the noisy nature of real-world data. This paper presents BiCarFormer: the first multimodal approach to multi-label sequence classification of error codes into error patterns that integrates DTC sequences and environmental conditions. BiCarFormer is a bidirectional Transformer model tailored for vehicle event sequences, employing embedding fusions and a co-attention mechanism to capture the relationships between diagnostic codes and environmental data. Experimental results on a challenging real-world automotive dataset with 22,137 error codes and 360 error patterns demonstrate that our approach significantly improves classification performance compared to models that rely solely on DTC sequences and traditional sequence models. This work highlights the importance of incorporating contextual environmental information for more accurate and robust vehicle diagnostics, hence reducing maintenance costs and enhancing automation processes in the automotive industry.

</details>


### [286] [Lyapunov Stability-Aware Stackelberg Game for Low-Altitude Economy: A Control-Oriented Pruning-Based DRL Approach](https://arxiv.org/abs/2602.01131)
*Yue Zhong,Jiawen Kang,Yongju Tong,Hong-Ning Dai,Dong In Kim,Abbas Jamalipour,Shengli Xie*

Main category: cs.AI

TL;DR: 论文提出一个感知-通信-计算-控制闭环框架，将通信延迟对物理控制稳定性的影响显式建模，利用李雅普诺夫稳定性理论将稳定性要求转化为可量化资源边界，并通过Stackelberg博弈和轻量级PPO算法实现无人机资源分配优化。


<details>
  <summary>Details</summary>
Motivation: 随着低空经济快速发展，无人机作为关键空中基站需支持从延迟敏感关键任务到带宽密集型数据流的多样化服务。传统吞吐量中心设计难以解决有限机载资源与严格稳定性要求之间的冲突，需要新的框架来保证任务可靠性。

Method: 提出感知-通信-计算-控制闭环框架，利用李雅普诺夫稳定性理论建立控制系统状态演化与通信约束的内在映射；将资源分配问题建模为Stackelberg博弈（无人机作为领导者动态定价，用户作为跟随者优化请求）；设计轻量级剪枝PPO算法，通过动态结构化剪枝机制压缩神经网络规模，降低推理延迟。

Result: 仿真结果表明，所提方案在动态低空环境中能有效保障控制环路稳定性，同时最大化系统效用。轻量级PPO算法显著降低了计算开销，使无人机能够快速逼近博弈均衡。

Conclusion: 该研究超越了传统吞吐量中心设计，通过将控制稳定性要求显式整合到资源分配中，为低空经济中的无人机异构网络提供了可靠且高效的解决方案。所提框架和算法在保证任务可靠性的同时优化了系统性能。

Abstract: With the rapid expansion of the low-altitude economy, Unmanned Aerial Vehicles (UAVs) serve as pivotal aerial base stations supporting diverse services from users, ranging from latency-sensitive critical missions to bandwidth-intensive data streaming. However, the efficacy of such heterogeneous networks is often compromised by the conflict between limited onboard resources and stringent stability requirements. Moving beyond traditional throughput-centric designs, we propose a Sensing-Communication-Computing-Control closed-loop framework that explicitly models the impact of communication latency on physical control stability. To guarantee mission reliability, we leverage the Lyapunov stability theory to derive an intrinsic mapping between the state evolution of the control system and communication constraints, transforming abstract stability requirements into quantifiable resource boundaries. Then, we formulate the resource allocation problem as a Stackelberg game, where UAVs (as leaders) dynamically price resources to balance load and ensure stability, while users (as followers) optimize requests based on service urgency. Furthermore, addressing the prohibitive computational overhead of standard Deep Reinforcement Learning (DRL) on energy-constrained edge platforms, we propose a novel and lightweight pruning-based Proximal Policy Optimization (PPO) algorithm. By integrating a dynamic structured pruning mechanism, the proposed algorithm significantly compresses the neural network scale during training, enabling the UAV to rapidly approximate the game equilibrium with minimal inference latency. Simulation results demonstrate that the proposed scheme effectively secures control loop stability while maximizing system utility in dynamic low-altitude environments.

</details>


### [287] [PersistBench: When Should Long-Term Memories Be Forgotten by LLMs?](https://arxiv.org/abs/2602.01146)
*Sidharth Pulipaka,Oliver Chen,Manas Sharma,Taaha S Bajwa,Vyas Raina,Ivaxi Sheth*

Main category: cs.AI

TL;DR: 论文提出PersistBench基准，用于评估LLM长期记忆带来的安全风险，发现现有模型在跨域泄漏和记忆诱导的谄媚行为上失败率很高。


<details>
  <summary>Details</summary>
Motivation: 随着对话助手越来越多地将长期记忆与LLM集成，记忆持久化虽然能增强个性化，但也引入了被忽视的安全风险，需要系统评估这些风险。

Method: 提出PersistBench基准，识别两种长期记忆特定风险：跨域泄漏（LLM不适当地注入长期记忆上下文）和记忆诱导的谄媚行为（存储的长期记忆强化用户偏见），并评估18个前沿和开源LLM。

Result: 评估结果显示LLM在这些风险上失败率惊人：跨域样本中位数失败率53%，谄媚行为样本中位数失败率97%，表明现有模型在长期记忆安全方面存在严重问题。

Conclusion: PersistBench基准鼓励开发更鲁棒、更安全的长期记忆使用方式，为前沿对话系统的安全发展提供重要参考。

Abstract: Conversational assistants are increasingly integrating long-term memory with large language models (LLMs). This persistence of memories, e.g., the user is vegetarian, can enhance personalization in future conversations. However, the same persistence can also introduce safety risks that have been largely overlooked. Hence, we introduce PersistBench to measure the extent of these safety risks. We identify two long-term memory-specific risks: cross-domain leakage, where LLMs inappropriately inject context from the long-term memories; and memory-induced sycophancy, where stored long-term memories insidiously reinforce user biases. We evaluate 18 frontier and open-source LLMs on our benchmark. Our results reveal a surprisingly high failure rate across these LLMs - a median failure rate of 53% on cross-domain samples and 97% on sycophancy samples. To address this, our benchmark encourages the development of more robust and safer long-term memory usage in frontier conversational systems.

</details>


### [288] [Multi-Agent Causal Reasoning System for Error Pattern Rule Automation in Vehicles](https://arxiv.org/abs/2602.01155)
*Hugo Math,Julian Lorentz,Stefan Oelsner,Rainer Lienhart*

Main category: cs.AI

TL;DR: CAREP是一个多智能体系统，通过因果发现和上下文信息整合，自动从诊断故障代码中生成错误模式规则，替代传统手工创建方法。


<details>
  <summary>Details</summary>
Motivation: 现代车辆产生大量诊断故障代码，汽车制造商使用这些代码的布尔组合（错误模式）来诊断系统故障。然而，这些规则目前仍由领域专家手工创建，过程昂贵且容易出错，随着车辆复杂度增加，这一问题更加突出。

Method: CAREP采用多智能体架构：1) 因果发现智能体识别DTC-EP潜在关系；2) 上下文信息智能体整合元数据和描述信息；3) 编排器智能体合成候选布尔规则并提供可解释的推理轨迹。

Result: 在包含29,100个独特DTC和474个错误模式的大规模汽车数据集上评估，CAREP能够自动准确地发现未知EP规则，性能优于仅使用LLM的基线方法，同时提供透明的因果解释。

Conclusion: CAREP通过结合实用因果发现和基于智能体的推理，向全自动故障诊断迈进一步，实现了可扩展、可解释且成本效益高的车辆维护解决方案。

Abstract: Modern vehicles generate thousands of different discrete events known as Diagnostic Trouble Codes (DTCs). Automotive manufacturers use Boolean combinations of these codes, called error patterns (EPs), to characterize system faults and ensure vehicle safety. Yet, EP rules are still manually handcrafted by domain experts, a process that is expensive and prone to errors as vehicle complexity grows. This paper introduces CAREP (Causal Automated Reasoning for Error Patterns), a multi-agent system that automatizes the generation of EP rules from high-dimensional event sequences of DTCs. CAREP combines a causal discovery agent that identifies potential DTC-EP relations, a contextual information agent that integrates metadata and descriptions, and an orchestrator agent that synthesizes candidate boolean rules together with interpretable reasoning traces. Evaluation on a large-scale automotive dataset with over 29,100 unique DTCs and 474 error patterns demonstrates that CAREP can automatically and accurately discover the unknown EP rules, outperforming LLM-only baselines while providing transparent causal explanations. By uniting practical causal discovery and agent-based reasoning, CAREP represents a step toward fully automated fault diagnostics, enabling scalable, interpretable, and cost-efficient vehicle maintenance.

</details>


### [289] [Do All Individual Layers Help? An Empirical Study of Task-Interfering Layers in Vision-Language Models](https://arxiv.org/abs/2602.01167)
*Zhiming Liu,Yujie Wei,Lei Feng,Xiu Su,Xiaobo Xia,Weili Guan,Zeke Xie,Shuo Yang*

Main category: cs.AI

TL;DR: 研究发现预训练视觉语言模型中存在任务干扰层，提出无需训练的动态层剔除方法TaLo提升性能


<details>
  <summary>Details</summary>
Motivation: 当前VLM通常默认使用所有层进行下游任务预测，但研究发现某些层反而会损害特定任务性能，需要探索层与任务间的相互作用关系

Method: 通过层干预实验测量各层对任务的影响，提出任务-层交互向量量化层干预效果，并设计TaLo方法动态识别和绕过最干扰层

Result: TaLo无需参数更新即可提升多种模型和数据集性能，如在ScienceQA的Maps任务上提升Qwen-VL准确率16.6%

Conclusion: 揭示了预训练VLM中意外的模块化特性，提供了即插即用的推理时能力解锁机制

Abstract: Current VLMs have demonstrated capabilities across a wide range of multimodal tasks. Typically, in a pretrained VLM, all layers are engaged by default to make predictions on downstream tasks. We find that intervening on a single layer, such as by zeroing its parameters, can improve the performance on certain tasks, indicating that some layers hinder rather than help downstream tasks. We systematically investigate how individual layers influence different tasks via layer intervention. Specifically, we measure the change in performance relative to the base model after intervening on each layer and observe improvements when bypassing specific layers. This improvement can be generalizable across models and datasets, indicating the presence of Task-Interfering Layers that harm downstream tasks' performance. We introduce Task-Layer Interaction Vector, which quantifies the effect of intervening on each layer of a VLM given a task. These task-interfering layers exhibit task-specific sensitivity patterns: tasks requiring similar capabilities show consistent response trends under layer interventions, as evidenced by the high similarity in their task-layer interaction vectors. Inspired by these findings, we propose TaLo (Task-Adaptive Layer Knockout), a training-free, test-time adaptation method that dynamically identifies and bypasses the most interfering layer for a given task. Without parameter updates, TaLo improves performance across various models and datasets, including boosting Qwen-VL's accuracy on the Maps task in ScienceQA by up to 16.6%. Our work reveals an unexpected form of modularity in pretrained VLMs and provides a plug-and-play, training-free mechanism to unlock hidden capabilities at inference time. The source code will be publicly available.

</details>


### [290] [ASP-Bench: From Natural Language to Logic Programs](https://arxiv.org/abs/2602.01171)
*Stefan Szeider*

Main category: cs.AI

TL;DR: ASP-Bench是一个包含128个自然语言问题实例的基准测试，用于评估将自然语言规范转换为答案集程序（ASP）的系统，覆盖了ASP的各种特性，并通过基于ReAct框架的智能体方法展示了反馈驱动的迭代优化效果。


<details>
  <summary>Details</summary>
Motivation: 将自然语言规范自动转换为逻辑程序是神经符号工程中的一个挑战性任务，需要系统性的评估基准来推动该领域的发展。

Method: 创建ASP-Bench基准测试，包含128个自然语言问题实例（64个基础问题及其简单和困难变体），覆盖ASP的各种特性（选择规则、聚合、优化等），每个问题都包含参考验证器。使用基于ReAct框架的智能体方法进行测试，通过反馈驱动的迭代优化来建模自然语言问题。

Result: 基准测试系统性地覆盖了ASP特性，并沿七个独立的推理维度（优化、时序推理、默认逻辑、资源分配、递归、空间推理和定量复杂性）对问题进行表征。基于ReAct的智能体方法实现了完全饱和，表明反馈驱动的迭代优化是建模自然语言ASP问题的可靠且鲁棒的方法。

Conclusion: ASP-Bench为评估自然语言到ASP的转换系统提供了全面的基准，基于ReAct的智能体方法展示了反馈驱动迭代优化的有效性，多轮智能体运行分析有助于理解问题建模难度的决定因素。

Abstract: Automating the translation of natural-language specifications into logic programs is a challenging task that affects neurosymbolic engineering. We present ASP-Bench, a benchmark comprising 128 natural language problem instances, 64 base problems with easy and hard variants. It evaluates systems that translate natural-language problems into Answer Set Programs (ASPs), a prominent form of logic programming. It provides systematic coverage of ASP features, including choice rules, aggregates, and optimization. Each problem includes reference validators that check whether solutions satisfy the problem specification.
  We characterize problems along seven largely independent reasoning aspects (optimization, temporal reasoning, default logic, resource allocation, recursion, spatial reasoning, and quantitative complexity), providing a multidimensional view of modeling difficulty.
  We test the benchmark using an agentic approach based on the ReAct (Reason and Act) framework, which achieves full saturation, demonstrating that feedback-driven iterative refinement with solver feedback provides a reliable and robust approach for modeling natural language in ASP. Our analysis across multiple agent runs enables us to gain insights into what determines a problem's modeling hardness.

</details>


### [291] [A State-Transition Framework for Efficient LLM Reasoning](https://arxiv.org/abs/2602.01198)
*Liang Zhang,Yu Zhao,Longyue Wang,Tianqi Shi,Weihua Luo,Kaifu Zhang,Jinsong Su*

Main category: cs.AI

TL;DR: 提出一种将LLM推理过程建模为状态转移过程的高效推理框架，使用线性注意力机制估计推理状态，将注意力计算复杂度从二次降为线性，同时通过状态推理策略缓解过度思考问题。


<details>
  <summary>Details</summary>
Motivation: 长链思维推理显著提升LLM在复杂推理任务上的性能，但生成长CoT序列带来巨大的计算和内存成本，限制了效率和实用性。现有方法通过压缩CoT序列提高效率，但这与测试时扩展相冲突，限制了LLM的推理能力。

Method: 1) 将LLM推理过程建模为状态转移过程；2) 使用线性注意力机制估计推理状态，记录历史推理信息；3) 基于查询提示和推理状态，LLM高效执行当前推理步骤并更新状态；4) 提出基于状态的推理策略缓解噪声推理步骤导致的过度思考问题。

Result: 在多个数据集和模型规模上的广泛实验表明，该框架不仅提高了LLM的推理效率，还增强了推理性能。

Conclusion: 提出的高效推理框架通过将推理过程建模为状态转移，使用线性注意力机制降低计算复杂度，有效解决了长CoT推理的计算成本问题，同时保持甚至提升了推理性能。

Abstract: While Long Chain-of-Thought (CoT) reasoning significantly improves Large Language Models (LLMs) performance on complex reasoning tasks, the substantial computational and memory costs of generating long CoT sequences limit their efficiency and practicality. Existing studies usually enhance the reasoning efficiency of LLMs by compressing CoT sequences. However, this approach conflicts with test-time scaling, limiting the reasoning capacity of LLMs. In this paper, we propose an efficient reasoning framework that models the reasoning process of LLMs as a state-transition process. Specifically, we first apply a linear attention mechanism to estimate the LLM's reasoning state, which records the historical reasoning information from previous reasoning steps. Then, based on the query prompt and the reasoning state, the LLM can efficiently perform the current reasoning step and update the state. With the linear attention, each token in the current reasoning step can directly retrieve relevant historical reasoning information from the reasoning state, without explicitly attending to tokens in previous reasoning steps. In this way, the computational complexity of attention is reduced from quadratic to linear, significantly improving the reasoning efficiency of LLMs. In addition, we propose a state-based reasoning strategy to mitigate the over-thinking issue caused by noisy reasoning steps. Extensive experiments across multiple datasets and model sizes demonstrate that our framework not only improves the reasoning efficiency of LLMs but also enhances their reasoning performance.

</details>


### [292] [Workflow-R1: Group Sub-sequence Policy Optimization for Multi-turn Workflow Construction](https://arxiv.org/abs/2602.01202)
*Mingze Kong,Zikun Qu,Zhongquan Zhou,Pengyu Liang,Xiang Li,Zhiwei Shang,Zhi Hong,Kaiyu Huang,Zhiyong Wang,Zhongxiang Dai*

Main category: cs.AI

TL;DR: Workflow-R1将工作流构建重新定义为多轮自然语言顺序决策过程，引入GSsPO算法解决优化粒度不匹配问题，在多个QA基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有工作流优化方法将工作流合成视为静态、一次性的代码生成问题，这过度约束了模型的编码能力，限制了动态问题解决的灵活性。

Method: 提出Workflow-R1框架，将工作流构建重新定义为多轮自然语言顺序决策过程；引入Group Sub-sequence Policy Optimization (GSsPO)算法，将优化单元校准为复合子序列（原子Think-Action循环），使梯度更新与交互的语义边界对齐。

Result: 在多个QA基准测试中，Workflow-R1优于竞争基线，验证了GSsPO作为顺序推理的通用解决方案的有效性。

Conclusion: Workflow-R1为自动化工作流优化提供了一个有前景的新范式，GSsPO算法可推广到广泛的多轮智能体顺序决策任务中。

Abstract: The rapid evolution of agentic workflows has demonstrated strong performance of LLM-based agents in addressing complex reasoning tasks. However, existing workflow optimization methods typically formulate workflow synthesis as a static, one-shot code-centric generation problem. This paradigm imposes excessive constraints on the model's coding capabilities and restricts the flexibility required for dynamic problem-solving. In this paper, we present Workflow-R1, a framework that reformulates workflow construction as a multi-turn, natural language-based sequential decision-making process. To resolve the optimization granularity mismatch inherent in such multi-turn interactions, we introduce Group Sub-sequence Policy Optimization (GSsPO). While explicitly tailored to align with the interleaved Think-Action dynamics of agentic reasoning, GSsPO fundamentally functions as a structure-aware RL algorithm generalizable to a broad class of multi-turn agentic sequential decision-making tasks. By recalibrating the optimization unit to the composite sub-sequence, specifically the atomic Think-Action cycle, it aligns gradient updates with the semantic boundaries of these interactions, ensuring robust learning in complex multi-turn reasoning tasks. Through extensive experiments on multiple QA benchmarks, Workflow-R1 outperforms competitive baselines, validating GSsPO as a generalized solution for sequential reasoning and establishing Workflow-R1 as a promising new paradigm for automated workflow optimization.

</details>


### [293] [Addressing Explainability of Generative AI using SMILE (Statistical Model-agnostic Interpretability with Local Explanations)](https://arxiv.org/abs/2602.01206)
*Zeinab Dehghani*

Main category: cs.AI

TL;DR: gSMILE是一个统一的生成模型可解释性框架，通过文本扰动、Wasserstein距离和加权代理建模来量化提示组件对输出的影响，为LLMs提供细粒度token归因，并为图像编辑模型分析指令修改的影响。


<details>
  <summary>Details</summary>
Motivation: 生成式AI模型虽然能产生复杂的文本和视觉输出，但其决策过程不透明，限制了在高风险应用中的信任和问责。需要开发能够解释生成模型决策过程的框架。

Method: 扩展SMILE方法到生成式设置，使用文本输入的受控扰动、Wasserstein距离度量和加权代理建模来量化和可视化提示组件对输出的影响。结合基于场景的评估策略（基于ODD框架）和严格的归因度量（稳定性、保真度、准确性、一致性、忠实度）。

Result: gSMILE为大语言模型提供细粒度token级归因和直观的热力图，突出显示有影响的token和推理路径；在基于指令的图像编辑模型中，分析指令修改对结果图像的影响。实验表明gSMILE产生稳健、人类对齐的归因，并能有效泛化到最先进的生成模型。

Conclusion: gSMILE有潜力推进生成式AI技术的透明、可靠和负责任部署，为生成模型提供统一的可解释性框架，增强在高风险应用中的信任和问责。

Abstract: The rapid advancement of generative artificial intelligence has enabled models capable of producing complex textual and visual outputs; however, their decision-making processes remain largely opaque, limiting trust and accountability in high-stakes applications. This thesis introduces gSMILE, a unified framework for the explainability of generative models, extending the Statistical Model-agnostic Interpretability with Local Explanations (SMILE) method to generative settings. gSMILE employs controlled perturbations of textual input, Wasserstein distance metrics, and weighted surrogate modelling to quantify and visualise how specific components of a prompt or instruction influence model outputs. Applied to Large Language Models (LLMs), gSMILE provides fine-grained token-level attribution and generates intuitive heatmaps that highlight influential tokens and reasoning pathways. In instruction-based image editing models, the exact text-perturbation mechanism is employed, allowing for the analysis of how modifications to an editing instruction impact the resulting image. Combined with a scenario-based evaluation strategy grounded in the Operational Design Domain (ODD) framework, gSMILE allows systematic assessment of model behaviour across diverse semantic and environmental conditions. To evaluate explanation quality, we define rigorous attribution metrics, including stability, fidelity, accuracy, consistency, and faithfulness, and apply them across multiple generative architectures. Extensive experiments demonstrate that gSMILE produces robust, human-aligned attributions and generalises effectively across state-of-the-art generative models. These findings highlight the potential of gSMILE to advance transparent, reliable, and responsible deployment of generative AI technologies.

</details>


### [294] [Not All Preferences Are Created Equal: Stability-Aware and Gradient-Efficient Alignment for Reasoning Models](https://arxiv.org/abs/2602.01207)
*Hui Wu,Hengyi Cai,Jinman Zhao,Xinran Chen,Ziheng Li,Zhejun Zhao,Shuaiqiang Wang,Yuchen Li,Dawei Yin*

Main category: cs.AI

TL;DR: SAGE是一个动态偏好对齐框架，通过最大化策略更新的信噪比来提升对齐可靠性，相比静态方法能加速收敛并取得更好效果。


<details>
  <summary>Details</summary>
Motivation: 标准偏好对齐方法（如DPO）将所有偏好对同等对待，忽略了训练样本的演化效用。这种静态方法导致计算效率低下（在梯度可忽略的平凡对上浪费计算）和优化不稳定（受决策边界附近样本噪声影响）。

Method: SAGE框架包含：1）粗粒度课程机制，根据模型能力刷新候选池；2）细粒度稳定性感知评分函数，优先选择信息丰富、置信度高的错误样本，同时过滤不稳定样本。核心是最大化策略更新的信噪比。

Result: 在多个数学推理基准测试中，SAGE显著加速了收敛速度，并超越了静态基线方法。

Conclusion: 研究表明，在推理对齐中，策略感知和稳定性意识的数据选择至关重要，动态框架比静态方法更有效。

Abstract: Preference-based alignment is pivotal for training large reasoning models; however, standard methods like Direct Preference Optimization (DPO) typically treat all preference pairs uniformly, overlooking the evolving utility of training instances. This static approach often leads to inefficient or unstable optimization, as it wastes computation on trivial pairs with negligible gradients and suffers from noise induced by samples near uncertain decision boundaries. Facing these challenges, we propose SAGE (Stability-Aware Gradient Efficiency), a dynamic framework designed to enhance alignment reliability by maximizing the Signal-to-Noise Ratio of policy updates. Concretely, SAGE integrates a coarse-grained curriculum mechanism that refreshes candidate pools based on model competence with a fine-grained, stability-aware scoring function that prioritizes informative, confident errors while filtering out unstable samples. Experiments on multiple mathematical reasoning benchmarks demonstrate that SAGE significantly accelerates convergence and outperforms static baselines, highlighting the critical role of policy-aware, stability-conscious data selection in reasoning alignment.

</details>


### [295] [FutureMind: Equipping Small Language Models with Strategic Thinking-Pattern Priors via Adaptive Knowledge Distillation](https://arxiv.org/abs/2602.01222)
*Shaoxiong Yang,Junting Li,Mengyuan Zhang,Chao Li,Wei Liu,Jian Luan*

Main category: cs.AI

TL;DR: FutureMind是一个模块化推理框架，通过从大语言模型进行自适应知识蒸馏，为小语言模型提供战略思维模式先验，提升其在复杂知识密集型任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 小语言模型在成本敏感和资源有限的环境中具有吸引力，但在需要结构化推理和有效检索的复杂知识密集型任务上表现不佳。需要一种方法让小语言模型具备更强的推理能力，同时保持其效率优势。

Method: 提出FutureMind框架，包含四个关键模块：问题分析、逻辑推理、策略规划和检索引导。采用三种不同的检索范式将复杂查询分解为可处理的子问题。通过从大语言模型进行自适应知识蒸馏，将战略思维模式先验转移给小语言模型。

Result: 在多个多跳QA基准测试（2WikiMultihopQA、MuSiQue、Bamboogle、Frames）上，FutureMind始终优于Search-o1等强基线，在自由训练条件下实现了最先进的结果，适用于不同架构和规模的小语言模型。

Conclusion: FutureMind成功提升了小语言模型的推理能力，但分析发现思维模式蒸馏过程受到教师（大语言模型）和学生（小语言模型）之间认知偏差瓶颈的限制。这为推理技能的可转移性提供了新视角，为开发兼具效率和真正认知能力的小语言模型铺平了道路。

Abstract: Small Language Models (SLMs) are attractive for cost-sensitive and resource-limited settings due to their efficient, low-latency inference. However, they often struggle with complex, knowledge-intensive tasks that require structured reasoning and effective retrieval. To address these limitations, we propose FutureMind, a modular reasoning framework that equips SLMs with strategic thinking-pattern priors via adaptive knowledge distillation from large language models (LLMs). FutureMind introduces a dynamic reasoning pipeline composed of four key modules: Problem Analysis, Logical Reasoning, Strategy Planning, and Retrieval Guidance. This pipeline is augmented by three distinct retrieval paradigms that decompose complex queries into tractable subproblems, ensuring efficient and accurate retrieval execution. Extensive experiments on multi-hop QA benchmarks, including 2WikiMultihopQA, MuSiQue, Bamboogle, and Frames, demonstrate the superiority of FutureMind. It consistently outperforms strong baselines such as Search-o1, achieving state-of-the-art results under free training conditions across diverse SLM architectures and scales. Beyond empirical gains, our analysis reveals that the process of thinking-pattern distillation is restricted by the cognitive bias bottleneck between the teacher (LLMs) and student (SLMs) models. This provides new perspectives on the transferability of reasoning skills, paving the way for the development of SLMs that combine efficiency with genuine cognitive capability.

</details>


### [296] [Predictive Scheduling for Efficient Inference-Time Reasoning in Large Language Models](https://arxiv.org/abs/2602.01237)
*Katrina Brown,Aneesh Muppidi,Rana Shahout*

Main category: cs.AI

TL;DR: 本文提出Predictive Scheduling框架，通过轻量级预测器预先估计查询的推理长度/难度，动态分配固定token预算以最大化准确率，在GSM8K上比均匀分配提升7.9个百分点。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在复杂推理任务中使用固定token预算生成多个CoT轨迹，导致简单输入过度计算而困难输入计算不足，需要更精细的计算-准确率权衡控制。

Method: 提出Predictive Scheduling框架：1) 使用MLP（基于transformer中间隐藏状态）或LoRA微调分类器（基于原始问题文本）作为轻量级预测器，预先估计查询的最优推理长度/难度；2) 贪心批量分配器动态分配固定总token预算以最大化预期准确率。

Result: 在GSM8K算术基准上，预测调度在相同token成本下比均匀预算分配获得高达7.9个百分点的绝对准确率提升，缩小了超过50%与完美预知oracle的差距。系统层间研究表明transformer中间层（12-17层）携带最丰富的规模估计信号。

Conclusion: 预运行预算预测能够精细控制计算-准确率权衡，为延迟敏感、成本高效的LLM部署提供了具体路径，中间层特征对难度预测最有效。

Abstract: Large language models (LLMs) achieve state-of-the-art accuracy on complex reasoning tasks by generating multiple chain-of-thought (CoT) traces, but using a fixed token budget per query leads to over-computation on easy inputs and under-computation on hard ones. We introduce Predictive Scheduling, a plug-and-play framework that pre-runs lightweight predictors, an MLP on intermediate transformer hidden states or a LoRA-fine-tuned classifier on raw question text, to estimate each query's optimal reasoning length or difficulty before any full generation. Our greedy batch allocator dynamically distributes a fixed total token budget across queries to maximize expected accuracy. On the GSM8K arithmetic benchmark, predictive scheduling yields up to 7.9 percentage points of absolute accuracy gain over uniform budgeting at identical token cost, closing over 50\% of the gap to an oracle with perfect foresight. A systematic layer-wise study reveals that middle layers (12 - 17) of the transformer carry the richest signals for size estimation. These results demonstrate that pre-run budget prediction enables fine-grained control of the compute-accuracy trade-off, offering a concrete path toward latency-sensitive, cost-efficient LLM deployments.

</details>


### [297] [LLM-Driven Ontology Construction for Enterprise Knowledge Graphs](https://arxiv.org/abs/2602.01276)
*Abdulsobur Oyewale,Tommaso Soru*

Main category: cs.AI

TL;DR: OntoEKG：基于LLM的自动化企业知识图谱本体构建流水线，从非结构化企业数据中生成领域本体


<details>
  <summary>Details</summary>
Motivation: 企业知识图谱需要统一异构数据并实施语义治理，但本体构建过程资源密集、依赖人工和领域专家，需要自动化解决方案

Method: 提出OntoEKG流水线，将建模任务分解为两个阶段：提取模块识别核心类和属性，蕴含模块将这些元素逻辑组织成层次结构并序列化为标准RDF

Result: 在Data领域获得模糊匹配F1分数0.724，同时在范围定义和层次推理方面存在局限性，展示了该方法的潜力和挑战

Conclusion: LLM驱动的本体构建流水线有潜力加速企业知识图谱开发，但需要解决范围定义和层次推理的挑战，并建立更全面的端到端评估基准

Abstract: Enterprise Knowledge Graphs have become essential for unifying heterogeneous data and enforcing semantic governance. However, the construction of their underlying ontologies remains a resource-intensive, manual process that relies heavily on domain expertise. This paper introduces OntoEKG, a LLM-driven pipeline designed to accelerate the generation of domain-specific ontologies from unstructured enterprise data. Our approach decomposes the modelling task into two distinct phases: an extraction module that identifies core classes and properties, and an entailment module that logically structures these elements into a hierarchy before serialising them into standard RDF. Addressing the significant lack of comprehensive benchmarks for end-to-end ontology construction, we adopt a new evaluation dataset derived from documents across the Data, Finance, and Logistics sectors. Experimental results highlight both the potential and the challenges of this approach, achieving a fuzzy-match F1-score of 0.724 in the Data domain while revealing limitations in scope definition and hierarchical reasoning.

</details>


### [298] [RE-MCDF: Closed-Loop Multi-Expert LLM Reasoning for Knowledge-Grounded Clinical Diagnosis](https://arxiv.org/abs/2602.01297)
*Shaowei Shen,Xiaohong Yang,Jie Yang,Lianfen Huang,Yongcai Zhang,Yang Zou,Seyyedali Hosseinalipour*

Main category: cs.AI

TL;DR: RE-MCDF是一个关系增强的多专家临床诊断框架，通过生成-验证-修订闭环架构解决电子病历诊断中LLM的自我强化错误问题，利用医学知识图谱和多关系专家组确保诊断逻辑一致性。


<details>
  <summary>Details</summary>
Motivation: 电子病历（特别是神经科）具有异质性、稀疏性和噪声性，单一代理系统容易产生自我强化错误，现有多代理框架交互浅层且缺乏对疾病间逻辑依赖关系（如互斥性、病理兼容性）的建模，无法排除临床不可行的假设。

Method: 提出RE-MCDF框架，包含三个互补组件：1）生成候选诊断和支持证据的主专家；2）动态优先处理异质临床指标的实验室专家；3）强制执行疾病间逻辑约束的多关系感知与评估专家组。基于医学知识图谱，前两个专家自适应重加权电子病历证据，专家组验证和修正候选诊断以确保逻辑一致性。

Result: 在CMEMR神经学子集（NEEMRs）和自建数据集（XMEMRs）上的广泛实验表明，RE-MCDF在复杂诊断场景中始终优于最先进的基线方法。

Conclusion: RE-MCDF通过引入关系感知的多专家闭环架构，有效解决了临床诊断中LLM的自我强化错误问题，显著提升了在异质、稀疏、噪声电子病历数据上的诊断性能。

Abstract: Electronic medical records (EMRs), particularly in neurology, are inherently heterogeneous, sparse, and noisy, which poses significant challenges for large language models (LLMs) in clinical diagnosis. In such settings, single-agent systems are vulnerable to self-reinforcing errors, as their predictions lack independent validation and can drift toward spurious conclusions. Although recent multi-agent frameworks attempt to mitigate this issue through collaborative reasoning, their interactions are often shallow and loosely structured, failing to reflect the rigorous, evidence-driven processes used by clinical experts. More fundamentally, existing approaches largely ignore the rich logical dependencies among diseases, such as mutual exclusivity, pathological compatibility, and diagnostic confusion. This limitation prevents them from ruling out clinically implausible hypotheses, even when sufficient evidence is available. To overcome these, we propose RE-MCDF, a relation-enhanced multi-expert clinical diagnosis framework. RE-MCDF introduces a generation--verification--revision closed-loop architecture that integrates three complementary components: (i) a primary expert that generates candidate diagnoses and supporting evidence, (ii) a laboratory expert that dynamically prioritizes heterogeneous clinical indicators, and (iii) a multi-relation awareness and evaluation expert group that explicitly enforces inter-disease logical constraints. Guided by a medical knowledge graph (MKG), the first two experts adaptively reweight EMR evidence, while the expert group validates and corrects candidate diagnoses to ensure logical consistency. Extensive experiments on the neurology subset of CMEMR (NEEMRs) and on our curated dataset (XMEMRs) demonstrate that RE-MCDF consistently outperforms state-of-the-art baselines in complex diagnostic scenarios.

</details>


### [299] [Model Specific Task Similarity for Vision Language Model Selection via Layer Conductance](https://arxiv.org/abs/2602.01346)
*Wei Yang,Hong Xie,Tao Tan,Xin Li,Defu Lian,Enhong Chen*

Main category: cs.AI

TL;DR: 提出基于视觉编码器内部功能动态的模型选择框架，通过方向性电导散度（DCD）度量源任务覆盖目标任务关键功能块的有效性，无需直接推理即可预测目标模型排名。


<details>
  <summary>Details</summary>
Motivation: 当前开源视觉语言模型（VLM）众多，但为特定下游任务选择最优预训练模型仍具挑战性。现有选择方法要么依赖数据密集型代理，要么使用对称文本描述符，忽略了迁移性的方向性和模型特定性。

Method: 提出基于视觉编码器内部功能动态的框架：1）通过层间电导表示每个任务；2）通过熵正则化对齐得到目标条件块重要性分布；3）引入方向性电导散度（DCD）作为非对称度量，量化源任务覆盖目标任务关键功能块的有效性；4）通过聚合源任务排名预测目标模型排名，无需直接推理。

Result: 在48个VLM和21个数据集上的实验表明，该方法优于现有最先进基线，在NDCG@5指标上比SWAB方法提升了14.7%。

Conclusion: 通过基于视觉编码器内部功能动态的框架，成功解决了VLM模型选择问题，提出的方向性电导散度度量能够有效预测模型在目标任务上的表现，为少样本场景下的模型选择提供了高效解决方案。

Abstract: While open sourced Vision-Language Models (VLMs) have proliferated, selecting the optimal pretrained model for a specific downstream task remains challenging. Exhaustive evaluation is often infeasible due to computational constraints and data limitations in few shot scenarios. Existing selection methods fail to fully address this: they either rely on data-intensive proxies or use symmetric textual descriptors that neglect the inherently directional and model-specific nature of transferability. To address this problem, we propose a framework that grounds model selection in the internal functional dynamics of the visual encoder. Our approach represents each task via layer wise conductance and derives a target-conditioned block importance distribution through entropy regularized alignment. Building on this, we introduce Directional Conductance Divergence (DCD), an asymmetric metric that quantifies how effectively a source task covers the target's salient functional blocks. This allows for predicting target model rankings by aggregating source task ranks without direct inference. Experimental results on 48 VLMs across 21 datasets demonstrate that our method outperforms state-of-the-art baselines, achieving a 14.7% improvement in NDCG@5 over SWAB.

</details>


### [300] [Aggregation Queries over Unstructured Text: Benchmark and Agentic Method](https://arxiv.org/abs/2602.01355)
*Haojia Zhu,Qinyuan Xu,Haoyu Li,Yuxi Liu,Hanchen Qiu,Jiaoyan Chen,Jiahui Jin*

Main category: cs.AI

TL;DR: 提出DFA方法解决文本聚合查询的完整性需求，通过消歧-过滤-聚合三阶段提升证据覆盖率


<details>
  <summary>Details</summary>
Motivation: 现有文本聚合查询方法（如Text-to-SQL、RAG）无法满足"找出所有"的完整性要求，需要新的解决方案

Method: 提出DFA（消歧-过滤-聚合）模块化智能体基线，将聚合查询分解为可解释的三阶段：实体消歧、证据过滤、结果聚合

Result: DFA在AGGBench基准测试中，相比强RAG和智能体基线，在聚合证据覆盖率方面有持续改进

Conclusion: DFA方法通过模块化分解有效解决了文本聚合查询的完整性挑战，为实际大规模语料库中的聚合查询提供了可行方案

Abstract: Aggregation query over free text is a long-standing yet underexplored problem. Unlike ordinary question answering, aggregate queries require exhaustive evidence collection and systems are required to "find all," not merely "find one." Existing paradigms such as Text-to-SQL and Retrieval-Augmented Generation fail to achieve this completeness. In this work, we formalize entity-level aggregation querying over text in a corpus-bounded setting with strict completeness requirement. To enable principled evaluation, we introduce AGGBench, a benchmark designed to evaluate completeness-oriented aggregation under realistic large-scale corpus. To accompany the benchmark, we propose DFA (Disambiguation--Filtering--Aggregation), a modular agentic baseline that decomposes aggregation querying into interpretable stages and exposes key failure modes related to ambiguity, filtering, and aggregation. Empirical results show that DFA consistently improves aggregation evidence coverage over strong RAG and agentic baselines. The data and code are available in https://anonymous.4open.science/r/DFA-A4C1.

</details>


### [301] [Building Better Deception Probes Using Targeted Instruction Pairs](https://arxiv.org/abs/2602.01425)
*Vikram Natarajan,Devina Jain,Shivam Arora,Satvik Golechha,Joseph Bloom*

Main category: cs.AI

TL;DR: 线性探针用于检测AI欺骗行为，但现有方法存在假阳性和虚假相关性问题。研究发现训练指令对的选择对性能影响最大（占70.6%方差），且针对特定欺骗类型设计探针比通用检测器更有效。


<details>
  <summary>Details</summary>
Motivation: 现有线性探针方法在检测AI欺骗行为时存在明显缺陷，包括虚假相关性和对非欺骗性响应的误报。需要改进探针设计以提高检测准确性和可靠性。

Method: 通过分析训练指令对的重要性，并使用人类可解释的欺骗分类法来针对特定欺骗行为设计探针。研究指令对如何捕捉欺骗意图而非内容特定模式。

Result: 训练指令对的选择主导探针性能（占70.6%方差），针对特定欺骗类型设计的探针在评估数据集上表现更好。指令对主要捕捉欺骗意图而非内容模式。

Conclusion: 由于不同数据集中欺骗类型的异质性，组织应针对其特定威胁模型设计专门的探针，而不是寻求通用的欺骗检测器。

Abstract: Linear probes are a promising approach for monitoring AI systems for deceptive behaviour. Previous work has shown that a linear classifier trained on a contrastive instruction pair and a simple dataset can achieve good performance. However, these probes exhibit notable failures even in straightforward scenarios, including spurious correlations and false positives on non-deceptive responses. In this paper, we identify the importance of the instruction pair used during training. Furthermore, we show that targeting specific deceptive behaviors through a human-interpretable taxonomy of deception leads to improved results on evaluation datasets. Our findings reveal that instruction pairs capture deceptive intent rather than content-specific patterns, explaining why prompt choice dominates probe performance (70.6% of variance). Given the heterogeneity of deception types across datasets, we conclude that organizations should design specialized probes targeting their specific threat models rather than seeking a universal deception detector.

</details>


### [302] [SimGym: Traffic-Grounded Browser Agents for Offline A/B Testing in E-Commerce](https://arxiv.org/abs/2602.01443)
*Alberto Castelo,Zahra Zanjani Foumani,Ailin Fan,Keat Yang Koay,Vibhor Malik,Yuanzheng Zhu,Han Li,Meysam Feghhi,Ronie Uliana,Shuang Xie,Zhaoyu Zhang,Angelo Ocana Martins,Mingyu Zhao,Francis Pelland,Jonathan Faerman,Nikolas LeBlanc,Aaron Glazer,Andrew McNamara,Lingyun Wang,Zhong Wu*

Main category: cs.AI

TL;DR: SimGym：基于LLM代理的快速离线A/B测试系统，通过合成买家模拟替代传统在线A/B测试，将实验周期从数周缩短至1小时内


<details>
  <summary>Details</summary>
Motivation: 传统A/B测试存在流量分流、周期长（数周）、可能损害用户体验等问题，需要一种快速、安全的离线测试方法

Method: 从生产交互数据提取买家画像和意图，识别行为原型，使用LLM代理在实时浏览器中模拟控制组和实验组的加权会话

Result: 在主要电商平台上验证，即使无需对齐后训练，SimGym代理也能与观察到的结果变化高度对齐，将实验周期从数周缩短至1小时内

Conclusion: SimGym实现了快速实验而无需接触真实买家，为电商UI变更评估提供了可扩展的离线A/B测试解决方案

Abstract: A/B testing remains the gold standard for evaluating e-commerce UI changes, yet it diverts traffic, takes weeks to reach significance, and risks harming user experience. We introduce SimGym, a scalable system for rapid offline A/B testing using traffic-grounded synthetic buyers powered by Large Language Model agents operating in a live browser. SimGym extracts per-shop buyer profiles and intents from production interaction data, identifies distinct behavioral archetypes, and simulates cohort-weighted sessions across control and treatment storefronts. We validate SimGym against real human outcomes from real UI changes on a major e-commerce platform under confounder control. Even without alignment post training, SimGym agents achieve state of the art alignment with observed outcome shifts and reduces experiment cycles from weeks to under an hour , enabling rapid experimentation without exposure to real buyers.

</details>


### [303] [Agyn: A Multi-Agent System for Team-Based Autonomous Software Engineering](https://arxiv.org/abs/2602.01465)
*Nikita Benkovich,Vitalii Valkov*

Main category: cs.AI

TL;DR: 提出一个完全自动化的多智能体系统，将软件工程建模为组织化流程，模拟工程团队结构，在SWE-bench 500上实现72.4%的任务解决率，超越单智能体基线。


<details>
  <summary>Details</summary>
Motivation: 现有自主系统大多将问题解决视为单一或流水线过程，而现实软件开发是团队协作活动，有明确的角色分离、沟通和评审。需要模拟真实工程团队的组织结构来提升自主软件工程能力。

Method: 基于agyn开源平台构建多智能体系统，分配协调、研究、实现、评审等专门角色，提供隔离沙箱进行实验，支持结构化通信。系统遵循定义好的开发方法学，包括分析、任务规范、PR创建和迭代评审，完全无需人工干预。

Result: 在SWE-bench 500上实现72.4%的任务解决率，超越使用可比语言模型的单智能体基线。系统专为实际生产使用设计，未针对SWE-bench进行调优。

Conclusion: 模拟团队结构、方法学和沟通是自主软件工程的有力范式，未来进展可能同等依赖于组织设计和智能体基础设施，而不仅仅是模型改进。

Abstract: Large language models have demonstrated strong capabilities in individual software engineering tasks, yet most autonomous systems still treat issue resolution as a monolithic or pipeline-based process. In contrast, real-world software development is organized as a collaborative activity carried out by teams following shared methodologies, with clear role separation, communication, and review. In this work, we present a fully automated multi-agent system that explicitly models software engineering as an organizational process, replicating the structure of an engineering team. Built on top of agyn, an open-source platform for configuring agent teams, our system assigns specialized agents to roles such as coordination, research, implementation, and review, provides them with isolated sandboxes for experimentation, and enables structured communication. The system follows a defined development methodology for working on issues, including analysis, task specification, pull request creation, and iterative review, and operates without any human intervention. Importantly, the system was designed for real production use and was not tuned for SWE-bench. When evaluated post hoc on SWE-bench 500, it resolves 72.4% of tasks, outperforming single-agent baselines using comparable language models. Our results suggest that replicating team structure, methodology, and communication is a powerful paradigm for autonomous software engineering, and that future progress may depend as much on organizational design and agent infrastructure as on model improvements.

</details>


### [304] [Learning to Guide Local Search for MPE Inference in Probabilistic Graphical Models](https://arxiv.org/abs/2602.01475)
*Brij Malhotra,Shivvrat Arya,Tahrima Rahman,Vibhav Giridhar Gogate*

Main category: cs.AI

TL;DR: 提出神经摊销框架改进概率图模型中的局部搜索，通过注意力网络预测移动减少与最优解汉明距离的能力，平衡短期似然增益与长期潜力


<details>
  <summary>Details</summary>
Motivation: 在概率图模型重复查询场景中，传统随机局部搜索算法依赖短视的最佳改进规则，容易陷入局部最优，现有启发式方法无法在相同模型的不同查询间有效复用

Method: 利用固定图结构训练基于注意力的神经网络，评估局部移动减少与近最优解汉明距离的能力，将此信号集成到现有局部搜索过程中，平衡短期似然增益与长期潜力

Result: 在摊销推理设置中，相比SLS和GLS+，在具有挑战性的高树宽基准测试上取得了一致的改进

Conclusion: 神经摊销框架有效改善了重复查询场景下的局部搜索性能，通过预测距离减少移动的选择改进了收敛行为

Abstract: Most Probable Explanation (MPE) inference in Probabilistic Graphical Models (PGMs) is a fundamental yet computationally challenging problem arising in domains such as diagnosis, planning, and structured prediction. In many practical settings, the graphical model remains fixed while inference must be performed repeatedly for varying evidence patterns. Stochastic Local Search (SLS) algorithms scale to large models but rely on myopic best-improvement rule that prioritizes immediate likelihood gains and often stagnate in poor local optima. Heuristics such as Guided Local Search (GLS+) partially alleviate this limitation by modifying the search landscape, but their guidance cannot be reused effectively across multiple inference queries on the same model. We propose a neural amortization framework for improving local search in this repeated-query regime. Exploiting the fixed graph structure, we train an attention-based network to score local moves by predicting their ability to reduce Hamming distance to a near-optimal solution. Our approach integrates seamlessly with existing local search procedures, using this signal to balance short-term likelihood gains with long-term promise during neighbor selection. We provide theoretical intuition linking distance-reducing move selection to improved convergence behavior, and empirically demonstrate consistent improvements over SLS and GLS+ on challenging high-treewidth benchmarks in the amortized inference setting.

</details>


### [305] [Qrita: High-performance Top-k and Top-p Algorithm for GPUs using Pivot-based Truncation and Selection](https://arxiv.org/abs/2602.01518)
*Jongseok Park,Sunga Kim,Alvin Cheung,Ion Stoica*

Main category: cs.AI

TL;DR: Qrita是一个基于枢轴选择策略的高效Top-k和Top-p算法，通过高斯截断和四元枢轴搜索技术，在GPU上实现了比现有方法更高的吞吐量和更低的内存使用。


<details>
  <summary>Details</summary>
Motivation: 当前Top-k和Top-p截断操作在大词汇表上的实现效率低下，现有方法要么依赖排序（计算和内存开销大），要么使用随机方法（改变算法输出），需要一种既高效又保持确定性的解决方案。

Method: 基于RTop-k的枢轴搜索思想，扩展到Top-k和Top-p，采用两种关键技术：1）基于高斯分布的sigma截断，大幅减少搜索空间；2）四元枢轴搜索与重复处理，将枢轴搜索迭代减半并保证确定性输出。使用Triton GPU编程语言实现。

Result: 与vLLM、SGLang、Flashinfer等高性能LLM执行引擎的Top-k和Top-p内核相比，Qrita实现了高达2倍的吞吐量和一半的内存使用，同时提供与基于排序算法相同的输出。

Conclusion: Qrita为大型语言模型采样中的Top-k和Top-p操作提供了一个高效、确定性的替代方案，显著提升了GPU上的计算效率和内存利用率。

Abstract: Top-k and Top-p are the dominant truncation operators in the sampling of large language models. Despite their widespread use, implementing them efficiently over large vocabularies remains a significant challenge. Existing approaches often rely on sorting, which incur significant computation and memory overhead on GPUs, or stochastic approaches, which alter the algorithm output. In this work, we propose Qrita, an efficient Top-k and Top-p algorithm based on a pivot-based selection strategy. Based on RTop-k, which uses a pivot-based search for node selection in graph neural networks, Qrita extends the concept of pivot-based search to both Top-k and Top-p with two key techniques: 1. Gaussian-based sigma-truncation, which greatly reduces the search space of the target elements, and 2. Quaternary pivot search with duplication handling, which halves the pivot search iteration and guarantees deterministic output. We provide the full implementation of Qrita using Triton, a popular GPU programming language. Our evaluation of Qrita against the Top-k and Top-p kernels of high performance LLM execution engines such as vLLM, SGLang, and Flashinfer show that Qrita achieves up to 2 times throughput and half memory use while providing the same output to the the sorting-based algorithms.

</details>


### [306] [PRISM: Festina Lente Proactivity -- Risk-Sensitive, Uncertainty-Aware Deliberation for Proactive Agents](https://arxiv.org/abs/2602.01532)
*Yuxuan Fu,Xiaoyu Tan,Teqi Hao,Chen Zhan,Xihe Qiu*

Main category: cs.AI

TL;DR: PRISM框架通过决策理论门控和双过程推理架构，实现智能体在成本敏感的选择性干预，仅在用户接受概率超过阈值时介入，减少误报22.78%，提升F1分数20.14%


<details>
  <summary>Details</summary>
Motivation: 现有主动智能体系统依赖脆弱的启发式方法或无差别的长推理，难以控制帮助与负担的权衡。需要一种能够精确控制干预时机和计算资源的框架。

Method: 提出PRISM框架：1) 决策理论门控：当校准的用户接受概率超过基于不对称成本（错过帮助与误报）的阈值时介入；2) 双过程推理：仅在决策边界附近调用资源密集的慢模式进行反事实检查；3) 门对齐、模式锁定的蒸馏训练：教师模型提供密集可执行的监督，学生模型学习与干预门解耦的响应策略。

Result: 在ProactiveBench基准测试中，PRISM相比强基线减少误报22.78%，提升F1分数20.14%。证明该方法能产生精确、计算高效且可控的主动智能体。

Conclusion: 基于决策理论的门控、选择性慢推理和对齐蒸馏相结合，能够构建精确、计算高效且可控的主动智能体。该方法在帮助-负担权衡上实现了更好的控制。

Abstract: Proactive agents must decide not only what to say but also whether and when to intervene. Many current systems rely on brittle heuristics or indiscriminate long reasoning, which offers little control over the benefit-burden tradeoff. We formulate the problem as cost-sensitive selective intervention and present PRISM, a novel framework that couples a decision-theoretic gate with a dual-process reasoning architecture. At inference time, the agent intervenes only when a calibrated probability of user acceptance exceeds a threshold derived from asymmetric costs of missed help and false alarms. Inspired by festina lente (Latin: "make haste slowly"), we gate by an acceptance-calibrated, cost-derived threshold and invoke a resource-intensive Slow mode with counterfactual checks only near the decision boundary, concentrating computation on ambiguous and high-stakes cases. Training uses gate-aligned, schema-locked distillation: a teacher running the full PRISM pipeline provides dense, executable supervision on unlabeled interaction traces, while the student learns a response policy that is explicitly decoupled from the intervention gate to enable tunable and auditable control. On ProactiveBench, PRISM reduces false alarms by 22.78% and improves F1 by 20.14% over strong baselines. These results show that principled decision-theoretic gating, paired with selective slow reasoning and aligned distillation, yields proactive agents that are precise, computationally efficient, and controllable. To facilitate reproducibility, we release our code, models, and resources at https://prism-festinalente.github.io/; all experiments use the open-source ProactiveBench benchmark.

</details>


### [307] [MAGIC: A Co-Evolving Attacker-Defender Adversarial Game for Robust LLM Safety](https://arxiv.org/abs/2602.01539)
*Xiaoyu Wen,Zhida He,Han Qi,Ziyu Wan,Zhongtian Ma,Ying Wen,Tianhang Zheng,Xingcheng Xu,Chaochao Lu,Qiaosheng Zhang*

Main category: cs.AI

TL;DR: MAGIC是一个多轮多智能体强化学习框架，将LLM安全对齐建模为对抗性非对称博弈，通过攻击者和防御者的共同进化来发现和防御新型攻击模式。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全防御方法依赖静态、预收集的数据分布，难以应对不断演化的对抗攻击，需要动态适应性的防御机制。

Method: 提出MAGIC框架：攻击者智能体学习迭代重写查询为欺骗性提示，防御者智能体同时优化策略识别并拒绝此类输入，形成共同进化过程。

Result: 攻击者通过RL训练演化出新颖的组合策略，发现长尾漏洞；防御者能泛化到未见攻击模式，在保持模型有用性的同时获得更高的防御成功率。

Conclusion: MAGIC通过动态对抗博弈实现了更鲁棒的安全对齐，为LLM安全防御提供了有效的共同进化框架，理论分析和实验验证了其有效性。

Abstract: Ensuring robust safety alignment is crucial for Large Language Models (LLMs), yet existing defenses often lag behind evolving adversarial attacks due to their \textbf{reliance on static, pre-collected data distributions}. In this paper, we introduce \textbf{MAGIC}, a novel multi-turn multi-agent reinforcement learning framework that formulates LLM safety alignment as an adversarial asymmetric game. Specifically, an attacker agent learns to iteratively rewrite original queries into deceptive prompts, while a defender agent simultaneously optimizes its policy to recognize and refuse such inputs. This dynamic process triggers a \textbf{co-evolution}, where the attacker's ever-changing strategies continuously uncover long-tail vulnerabilities, driving the defender to generalize to unseen attack patterns. Remarkably, we observe that the attacker, endowed with initial reasoning ability, evolves \textbf{novel, previously unseen combinatorial strategies} through iterative RL training, underscoring our method's substantial potential. Theoretically, we provide insights into a more robust game equilibrium and derive safety guarantees. Extensive experiments validate our framework's effectiveness, demonstrating superior defense success rates without compromising the helpfulness of the model. Our code is available at https://github.com/BattleWen/MAGIC.

</details>


### [308] [S1-NexusAgent: a Self-Evolving Agent Framework for Multidisciplinary Scientific Research](https://arxiv.org/abs/2602.01550)
*S1-NexusAgent Team*

Main category: cs.AI

TL;DR: S1-NexusAgent：用于多学科科学研究的自进化智能体框架，采用分层Plan-and-CodeAct执行范式，通过双循环架构解耦全局规划与工具执行，在多个科学基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM和基于工具的智能体在处理大规模数据、复杂工作流和专用工具方面存在局限，特别是在长时程规划、鲁棒目标维持和持续学习方面。科学研究的复杂性需要更强大的智能体框架。

Method: 采用分层Plan-and-CodeAct执行范式，双循环架构解耦全局科学规划与子任务级工具执行。支持Model Context Protocol，集成数千个跨学科科学工具，通过意图感知动态工具检索和热插拔机制实现异构工具编排。引入基于对象引用的稀疏上下文管理，支持子任务上下文隔离和中间结果压缩。通过Critic Agent评估执行轨迹，将高质量研究路径提炼为可重用的Scientific Skills。

Result: 在涉及长时程规划和复杂专用工具编排的权威科学基准测试（biomini-eval生物学、ChemBench化学、MatSciBench材料科学）中，S1-NexusAgent实现了最先进的性能，验证了其在复杂科学任务中的有效性和泛化能力。

Conclusion: S1-NexusAgent通过自进化框架解决了科学研究的核心挑战，为可持续和长时程科学研究提供了有价值的解决方案，在复杂科学任务中表现出卓越的性能和泛化能力。

Abstract: Modern scientific research relies on large-scale data, complex workflows, and specialized tools, which existing LLMs and tool-based agents struggle to handle due to limitations in long-horizon planning, robust goal maintenance, and continual learning from execution. To address these issues, in this work, we propose S1-NexusAgent, a self-evolving agent framework designed for multidisciplinary scientific research. S1-NexusAgent adopts a hierarchical Plan-and-CodeAct execution paradigm, decoupling global scientific planning from subtask-level tool execution through a dual-loop architecture, thereby enabling stable modeling of complex research workflows. The system natively supports the Model Context Protocol (MCP), integrates up to thousands of cross-disciplinary scientific tools, and achieves efficient orchestration of heterogeneous research tools via intention-aware dynamic tool retrieval and hot-plug mechanisms. To address long-context and large-scale data challenges in scientific settings, S1-NexusAgent introduces object-reference-based sparse context management, which enables sub-task context isolation and intermediate result compression. Building on this, a Critic Agent automatically evaluates complete execution trajectories and distills high-quality research paths into reusable Scientific Skills, forming a closed loop for continuous self-evolution, which is valuable for sustainable and long-horizon scientific research. Experiments on authoritative scientific benchmarks involving long-horizon planning and complex specialized tool orchestration, including biomini-eval (biology), ChemBench (chemistry), and MatSciBench (material science), demonstrate that S1-NexusAgent achieves state-of-the-art performance, validating its effectiveness and generalization capability in complex scientific tasks.

</details>


### [309] [Autonomous Question Formation for Large Language Model-Driven AI Systems](https://arxiv.org/abs/2602.01556)
*Hong Su*

Main category: cs.AI

TL;DR: 提出基于人类模拟的框架，使AI系统能自主形成问题和设定任务，通过推理内部状态、环境观察和与其他AI系统的交互，将问题形成作为任务选择和执行之前的一等决策过程。


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的AI系统大多依赖预定义任务和固定提示，限制了在环境条件变化时自主识别应解决问题的能力，需要提升系统在动态开放环境中的自主决策能力。

Method: 提出人类模拟框架，将问题形成作为一等决策过程，整合内部驱动、环境感知和智能体间感知三种提示范围来逐步扩展认知覆盖，并支持从经验中学习问题形成过程。

Result: 在多智能体仿真环境中，环境感知提示相比内部驱动基线显著减少无进食事件，智能体间感知提示在20天仿真中进一步减少累计无进食事件超过60%，具有统计显著性改进(p<0.05)。

Conclusion: 该框架使AI系统能够自主形成问题和设定任务，通过整合多层次的认知范围和学习能力，显著提升了在动态环境中的适应性和决策质量。

Abstract: Large language model (LLM)-driven AI systems are increasingly important for autonomous decision-making in dynamic and open environments. However, most existing systems rely on predefined tasks and fixed prompts, limiting their ability to autonomously identify what problems should be solved when environmental conditions change. In this paper, we propose a human-simulation-based framework that enables AI systems to autonomously form questions and set tasks by reasoning over their internal states, environmental observations, and interactions with other AI systems. The proposed method treats question formation as a first-class decision process preceding task selection and execution, and integrates internal-driven, environment-aware, and inter-agent-aware prompting scopes to progressively expand cognitive coverage. In addition, the framework supports learning the question-formation process from experience, allowing the system to improve its adaptability and decision quality over time. xperimental results in a multi-agent simulation environment show that environment-aware prompting significantly reduces no-eat events compared with the internal-driven baseline, and inter-agent-aware prompting further reduces cumulative no-eat events by more than 60% over a 20-day simulation, with statistically significant improvements (p < 0.05).

</details>


### [310] [Reasoning with Autoregressive-Diffusion Collaborative Thoughts](https://arxiv.org/abs/2602.01608)
*Mu Yuan,Liekang Zeng,Guoliang Xing,Lan Zhang,Yunhao Liu*

Main category: cs.AI

TL;DR: 提出Collaborative Thoughts框架，让自回归模型和扩散模型通过闭环交互协同工作，结合两者的优势来提升空间推理可靠性和生成可控性。


<details>
  <summary>Details</summary>
Motivation: 自回归模型擅长序列规划和约束组合，但在需要明确空间或物理基础的任务上表现不佳；扩散模型能捕捉丰富的空间结构，但缺乏逐步逻辑控制来满足复杂多阶段约束或可靠识别纠正错误。需要结合两者优势。

Method: Collaborative Thoughts框架：自回归模型负责结构化规划和约束管理，扩散模型将约束实例化为中间视觉思考，视觉批评模块评估视觉思考是否满足结构和物理要求，反馈用于迭代优化后续规划和生成步骤，减少跨模态错误传播。

Result: 通过代表性示例展示了Collaborative Thoughts如何提高空间推理的可靠性和生成的可控性，无论任务是自回归问答还是基于扩散的视觉生成，都使用相同的协作循环。

Conclusion: Collaborative Thoughts框架成功统一了自回归和扩散两种生成范式，通过闭环交互让它们协同推理和生成，结合了各自的优势，为解决复杂多模态任务提供了新思路。

Abstract: Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structure through high-dimensional generation, but lack the stepwise logical control needed to satisfy complex, multi-stage constraints or to reliably identify and correct errors. We introduce Collaborative Thoughts, a unified collaborative framework that enables autoregressive and diffusion models to reason and generate jointly through a closed-loop interaction. In Collaborative Thoughts, autoregressive models perform structured planning and constraint management, diffusion models instantiate these constraints as intermediate visual thoughts, and a vision-based critic module evaluates whether the visual thoughts satisfy the intended structural and physical requirements. This feedback is then used to iteratively refine subsequent planning and generation steps, mitigating error propagation across modalities. Importantly, Collaborative Thoughts uses the same collaborative loop regardless of whether the task is autoregressive question answering or diffusion-based visual generation. Through representative examples, we illustrate how Collaborative Thoughts can improve the reliability of spatial reasoning and the controllability of generation.

</details>


### [311] [ToPT: Task-Oriented Prompt Tuning for Urban Region Representation Learning](https://arxiv.org/abs/2602.01610)
*Zitao Guo,Changyang Jiang,Tianhong Zhao,Jinzhou Cao,Genan Dai,Bowen Zhang*

Main category: cs.AI

TL;DR: ToPT：一个两阶段框架，通过空间感知区域嵌入学习和任务感知提示，实现空间一致融合和明确任务对齐，提升城市计算任务性能


<details>
  <summary>Details</summary>
Motivation: 现有方法存在两个问题：1）两阶段方法产生任务无关表示，与下游目标脱节；2）基于提示的方法缺乏明确空间先验（导致空间不连贯）和稳健的任务语义对齐机制

Method: 提出ToPT框架，包含两个模块：SREL（空间感知区域嵌入学习）使用Graphormer融合模块，注入距离和区域中心性作为可学习注意力偏置；Prompt4RE（任务感知提示）使用冻结的多模态大语言模型处理任务特定模板，通过多头交叉注意力将语义向量与区域嵌入对齐

Result: 在多个任务和城市上的实验显示达到最先进性能，性能提升高达64.2%，验证了空间先验和提示-区域对齐的必要性和互补性

Conclusion: ToPT通过结合空间先验和任务对齐机制，有效解决了现有方法的局限性，为城市计算任务提供了更有效的区域表示学习方法

Abstract: Learning effective region embeddings from heterogeneous urban data underpins key urban computing tasks (e.g., crime prediction, resource allocation). However, prevailing two-stage methods yield task-agnostic representations, decoupling them from downstream objectives. Recent prompt-based approaches attempt to fix this but introduce two challenges: they often lack explicit spatial priors, causing spatially incoherent inter-region modeling, and they lack robust mechanisms for explicit task-semantic alignment. We propose ToPT, a two-stage framework that delivers spatially consistent fusion and explicit task alignment. ToPT consists of two modules: spatial-aware region embedding learning (SREL) and task-aware prompting for region embeddings (Prompt4RE). SREL employs a Graphormer-based fusion module that injects spatial priors-distance and regional centrality-as learnable attention biases to capture coherent, interpretable inter-region interactions. Prompt4RE performs task-oriented prompting: a frozen multimodal large language model (MLLM) processes task-specific templates to obtain semantic vectors, which are aligned with region embeddings via multi-head cross-attention for stable task conditioning. Experiments across multiple tasks and cities show state-of-the-art performance, with improvements of up to 64.2\%, validating the necessity and complementarity of spatial priors and prompt-region alignment. The code is available at https://github.com/townSeven/Prompt4RE.git.

</details>


### [312] [ProjDevBench: Benchmarking AI Coding Agents on End-to-End Project Development](https://arxiv.org/abs/2602.01655)
*Pengrui Lu,Shiqi Zhang,Yunzhong Hou,Lyumanshan Ye,Chaoyi Huang,Zixi Chen,Ji Zeng,Hantao Jiang,Pengfei Liu,Yiwei Wang,Ming-Hsuan Yang*

Main category: cs.AI

TL;DR: ProjDevBench是一个端到端的编码代理基准测试，通过项目需求评估代码库质量，结合在线评测和LLM辅助代码审查，在20个编程问题上测试6个编码代理，总体接受率27.38%


<details>
  <summary>Details</summary>
Motivation: 现有编码代理评估主要关注问题级别的bug修复，缺乏端到端开发评估。需要一个新的基准测试来全面评估编码代理从项目需求到完整代码库生成的能力。

Method: 引入ProjDevBench基准测试，提供项目需求给编码代理，评估生成的代码库。结合在线评测(Online Judge)测试和LLM辅助代码审查，从三个维度评估：(1)系统架构设计，(2)功能正确性，(3)迭代解决方案改进。包含20个编程问题，涵盖8个类别。

Result: 在6个基于不同LLM后端的编码代理上测试，总体接受率为27.38%。代理能够处理基本功能和数据结构，但在复杂系统设计、时间复杂度优化和资源管理方面表现不佳。

Conclusion: ProjDevBench填补了编码代理端到端评估的空白，揭示了当前编码代理在复杂系统设计和优化方面的局限性，为未来编码代理开发提供了重要的评估工具。

Abstract: Recent coding agents can generate complete codebases from simple prompts, yet existing evaluations focus on issue-level bug fixing and lag behind end-to-end development. We introduce ProjDevBench, an end-to-end benchmark that provides project requirements to coding agents and evaluates the resulting repositories. Combining Online Judge (OJ) testing with LLM-assisted code review, the benchmark evaluates agents on (1) system architecture design, (2) functional correctness, and (3) iterative solution refinement. We curate 20 programming problems across 8 categories, covering both concept-oriented tasks and real-world application scenarios, and evaluate six coding agents built on different LLM backends. Our evaluation reports an overall acceptance rate of 27.38%: agents handle basic functionality and data structures but struggle with complex system design, time complexity optimization, and resource management. Our benchmark is available at https://github.com/zsworld6/projdevbench.

</details>


### [313] [FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning](https://arxiv.org/abs/2602.01664)
*Mingda Zhang,Haoran Luo,Tiesunlong Shen,Qika Lin,Xiaoying Tang,Rui Mao,Erik Cambria*

Main category: cs.AI

TL;DR: FlowSteer：一个端到端的强化学习框架，通过轻量级策略模型和可执行画布环境自动化工作流编排，解决了现有工作流编排的高人工成本、依赖特定算子/大语言模型和稀疏奖励信号等挑战。


<details>
  <summary>Details</summary>
Motivation: 现有工作流编排面临三个主要挑战：1）高人工成本；2）依赖特定算子或大型语言模型；3）稀疏奖励信号。这些问题限制了工作流编排的效率和灵活性。

Method: 提出FlowSteer框架，包含：1）轻量级策略模型作为智能体；2）可执行画布环境；3）多轮交互自动化工作流编排。策略模型分析执行状态并选择编辑动作，画布执行算子并返回反馈进行迭代优化。还提出Canvas Workflow Relative Policy Optimization (CWRPO)训练方法，引入多样性约束奖励和条件释放机制来稳定学习并抑制捷径行为。

Result: 在12个数据集上的实验结果显示，FlowSteer在各种任务上显著优于基线方法。

Conclusion: FlowSteer提供了一个即插即用的框架，支持多样化的算子库和可互换的LLM后端，通过强化学习自动化工作流编排，有效解决了现有方法的局限性。

Abstract: In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. To address these challenges, we propose FlowSteer, an end-to-end reinforcement learning framework that takes a lightweight policy model as the agent and an executable canvas environment, automating workflow orchestration through multi-turn interaction. In this process, the policy model analyzes execution states and selects editing actions, while the canvas executes operators and returns feedback for iterative refinement. Moreover, FlowSteer provides a plug-and-play framework that supports diverse operator libraries and interchangeable LLM backends. To effectively train this interaction paradigm, we propose Canvas Workflow Relative Policy Optimization (CWRPO), which introduces diversity-constrained rewards with conditional release to stabilize learning and suppress shortcut behaviors. Experimental results on twelve datasets show that FlowSteer significantly outperforms baselines across various tasks.

</details>


### [314] [TRIP-Bench: A Benchmark for Long-Horizon Interactive Agents in Real-World Scenarios](https://arxiv.org/abs/2602.01675)
*Yuanzhe Shen,Zisu Huang,Zhengyuan Wang,Muzhao Tian,Zhengkang Guo,Chenyang Zhang,Shuaiyu Zhou,Zengjie Hu,Dailin Li,Jingwen Xu,Kaimin Wang,Wenhao Liu,Tianlong Li,Fengpeng Yue,Feng Hong,Cao Liu,Ke Zeng*

Main category: cs.AI

TL;DR: TRIP-Bench是一个基于真实旅行规划场景的长时程基准测试，包含18个工具和40+旅行需求，支持自动评估。GTPO是一种在线多轮强化学习方法，能提升约束满足和交互鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试无法充分代表真实世界中的关键挑战：强制执行全局约束、协调多工具推理、适应长期多轮交互中不断变化的用户行为。需要更贴近现实的评估平台。

Method: 1. 提出TRIP-Bench基准：基于真实旅行数据，包含18个精选工具和40+旅行需求，支持自动评估，包含不同难度划分。2. 提出GTPO方法：在线多轮强化学习，采用专门的奖励归一化和奖励差分技术。

Result: 1. 实验显示即使先进模型在简单划分上最多只能达到50%成功率，在困难子集上性能降至10%以下。2. GTPO应用于Qwen2.5-32B-Instruct后，在约束满足和交互鲁棒性方面优于Gemini-3-Pro。

Conclusion: TRIP-Bench有望推动实用的长时程交互智能体发展，GTPO为鲁棒的长时程训练提供了有效的在线强化学习方案。

Abstract: As LLM-based agents are deployed in increasingly complex real-world settings, existing benchmarks underrepresent key challenges such as enforcing global constraints, coordinating multi-tool reasoning, and adapting to evolving user behavior over long, multi-turn interactions. To bridge this gap, we introduce \textbf{TRIP-Bench}, a long-horizon benchmark grounded in realistic travel-planning scenarios. TRIP-Bench leverages real-world data, offers 18 curated tools and 40+ travel requirements, and supports automated evaluation. It includes splits of varying difficulty; the hard split emphasizes long and ambiguous interactions, style shifts, feasibility changes, and iterative version revision. Dialogues span up to 15 user turns, can involve 150+ tool calls, and may exceed 200k tokens of context. Experiments show that even advanced models achieve at most 50\% success on the easy split, with performance dropping below 10\% on hard subsets. We further propose \textbf{GTPO}, an online multi-turn reinforcement learning method with specialized reward normalization and reward differencing. Applied to Qwen2.5-32B-Instruct, GTPO improves constraint satisfaction and interaction robustness, outperforming Gemini-3-Pro in our evaluation. We expect TRIP-Bench to advance practical long-horizon interactive agents, and GTPO to provide an effective online RL recipe for robust long-horizon training.

</details>


### [315] [What LLMs Think When You Don't Tell Them What to Think About?](https://arxiv.org/abs/2602.01689)
*Yongchan Kwon,James Zou*

Main category: cs.AI

TL;DR: 研究通过最小化、主题中立的输入探究LLMs的无约束生成行为，发现不同模型家族存在系统性主题偏好和退化模式


<details>
  <summary>Details</summary>
Motivation: 现有LLM分析多依赖特定主题或任务的提示词，限制了观察范围。需要研究LLMs在最小化、主题中立输入下的无约束生成行为，以更全面地理解模型特性

Method: 使用最小化、主题中立的输入（如空字符串或简单标点）对16个LLMs进行无约束生成，收集256,000个样本，分析其主题分布、内容深度和退化模式

Result: 不同模型家族展现出系统性主题偏好：GPT-OSS偏好编程(27.1%)和数学内容(24.6%)，Llama偏好文学内容(9.1%)，DeepSeek偏好宗教内容，Qwen偏好多选题。GPT-OSS生成内容技术性更强。无约束生成常退化为重复短语，各模型退化模式不同

Conclusion: LLMs在无约束生成中表现出强烈的系统性偏好和独特退化行为，这些发现对模型监控、AI安全和理解模型内在特性具有重要意义

Abstract: Characterizing the behavior of large language models (LLMs) across diverse settings is critical for reliable monitoring and AI safety. However, most existing analyses rely on topic- or task-specific prompts, which can substantially limit what can be observed. In this work, we study what LLMs generate from minimal, topic-neutral inputs and probe their near-unconstrained generative behavior. Despite the absence of explicit topics, model outputs cover a broad semantic space, and surprisingly, each model family exhibits strong and systematic topical preferences. GPT-OSS predominantly generates programming (27.1%) and mathematical content (24.6%), whereas Llama most frequently generates literary content (9.1%). DeepSeek often generates religious content, while Qwen frequently generates multiple-choice questions. Beyond topical preferences, we also observe differences in content specialization and depth: GPT-OSS often generates more technically advanced content (e.g., dynamic programming) compared with other models (e.g., basic Python). Furthermore, we find that the near-unconstrained generation often degenerates into repetitive phrases, revealing interesting behaviors unique to each model family. For instance, degenerate outputs from Llama include multiple URLs pointing to personal Facebook and Instagram accounts. We release the complete dataset of 256,000 samples from 16 LLMs, along with a reproducible codebase.

</details>


### [316] [Beyond Dense States: Elevating Sparse Transcoders to Active Operators for Latent Reasoning](https://arxiv.org/abs/2602.01695)
*Yadong Wang,Haodong Chen,Yu Tian,Chuanxing Geng,Dong Liang,Xiang Chen*

Main category: cs.AI

TL;DR: LSTR提出了一种稀疏潜在推理框架，通过稀疏语义转换提升功能稀疏转码器为主动推理算子，实现可解释且可控的多步计算。


<details>
  <summary>Details</summary>
Motivation: 现有潜在推理方法依赖密集潜在转换，难以解释和控制；而稀疏表示模型主要局限于事后分析。需要解决密集潜在推理与稀疏可解释性之间的张力。

Method: 提出LSTR框架，使用具有残差跳跃架构的潜在转换转码器(LTT)，将线性流形传输与稀疏语义更新解耦，通过显式稀疏约束实现可控语义分辨率。

Result: LSTR在保持推理准确性和压缩效率的同时，相比密集潜在基线显著提高了可解释性。因果干预和轨迹分析表明稀疏特征在推理过程中既是可解释的又是因果有效的算子。

Conclusion: LSTR成功将稀疏表示提升为主动推理算子，实现了可解释且可控的潜在推理，为理解神经网络的推理过程提供了新途径。

Abstract: Latent reasoning compresses the chain-of-thought (CoT) into continuous hidden states, yet existing methods rely on dense latent transitions that remain difficult to interpret and control. Meanwhile, sparse representation models uncover human-interpretable semantic features but remain largely confined to post-hoc analysis. We reconcile this tension by proposing LSTR (Latent Sparse Transcoder Reasoning), a latent reasoning framework that elevates functional sparse transcoders into active reasoning operators to perform multi-step computation through sparse semantic transitions. At its core, LSTR employs a Latent Transition Transcoder (LTT) with a residual skip architecture that decouples linear manifold transport from sparse semantic updates, enabling controllable semantic resolution via explicit sparsity constraints. Extensive experiments show that LSTR preserves reasoning accuracy and compression efficiency while substantially improving interpretability over dense latent baselines. Causal interventions and trajectory analyses further demonstrate that these sparse features act as both interpretable and causally effective operators in the reasoning process.

</details>


### [317] [Optimizing Prompts for Large Language Models: A Causal Approach](https://arxiv.org/abs/2602.01711)
*Wei Chen,Yanbin Fang,Shuran Fu,Fasheng Xu,Xuan Wei*

Main category: cs.AI

TL;DR: 提出因果提示优化（CPO）框架，通过因果推理解决提示设计问题，使用双机器学习构建离线因果奖励模型，实现高效、低成本的查询特定提示优化。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在企业工作流中应用广泛，但性能对提示设计高度敏感。现有自动提示优化方法存在两个问题：静态指令无法适应异构查询；动态方法依赖离线奖励模型，混淆了提示效果与查询特性。

Method: 提出因果提示优化（CPO）框架，分两阶段：1）使用双机器学习（DML）在提示和查询的语义嵌入上学习离线因果奖励模型，分离提示变异的因果效应与混淆的查询属性；2）利用无偏奖励信号指导资源高效的查询特定提示搜索，无需昂贵的在线评估。

Result: 在数学推理、可视化和数据分析基准测试中，CPO始终优于人工设计的提示和最先进的自动优化器。改进主要体现在困难查询上的鲁棒性提升，而现有方法在这些查询上表现较差。

Conclusion: CPO从根本上重塑了提示优化的经济学：通过将评估从实时模型执行转移到离线因果模型，能以远低于在线方法的推理成本实现高精度、按查询定制的优化。因果推断为可靠且经济高效的提示优化提供了可扩展的基础。

Abstract: Large Language Models (LLMs) are increasingly embedded in enterprise workflows, yet their performance remains highly sensitive to prompt design. Automatic Prompt Optimization (APO) seeks to mitigate this instability, but existing approaches face two persistent challenges. First, commonly used prompt strategies rely on static instructions that perform well on average but fail to adapt to heterogeneous queries. Second, more dynamic approaches depend on offline reward models that are fundamentally correlational, confounding prompt effectiveness with query characteristics. We propose Causal Prompt Optimization (CPO), a framework that reframes prompt design as a problem of causal estimation. CPO operates in two stages. First, it learns an offline causal reward model by applying Double Machine Learning (DML) to semantic embeddings of prompts and queries, isolating the causal effect of prompt variations from confounding query attributes. Second, it utilizes this unbiased reward signal to guide a resource-efficient search for query-specific prompts without relying on costly online evaluation. We evaluate CPO across benchmarks in mathematical reasoning, visualization, and data analytics. CPO consistently outperforms human-engineered prompts and state-of-the-art automated optimizers. The gains are driven primarily by improved robustness on hard queries, where existing methods tend to deteriorate. Beyond performance, CPO fundamentally reshapes the economics of prompt optimization: by shifting evaluation from real-time model execution to an offline causal model, it enables high-precision, per-query customization at a fraction of the inference cost required by online methods. Together, these results establish causal inference as a scalable foundation for reliable and cost-efficient prompt optimization in enterprise LLM deployments.

</details>


### [318] [MACD: Model-Aware Contrastive Decoding via Counterfactual Data](https://arxiv.org/abs/2602.01740)
*Qixin Xiao,Kun Zhou*

Main category: cs.AI

TL;DR: MACD提出了一种新的推理策略，通过模型感知的反事实数据构建与对比解码相结合，减少视频语言模型的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 视频语言模型容易产生幻觉，特别是在视觉证据弱、模糊或有偏见时。现有解码方法（如对比解码）依赖随机扰动构建对比数据，难以控制驱动幻觉的视觉线索或与模型弱点良好对齐。

Method: MACD使用视频语言模型自身的反馈识别导致幻觉的对象区域，在对象级别生成有针对性的反事实输入（而非任意的帧或时间修改），然后将这些模型感知的反事实数据集成到对比解码中，在解码过程中强制证据基础的标记选择。

Result: 在EventHallusion、MVBench、Perception-test和Video-MME上的实验表明，MACD能持续减少幻觉，同时保持或提高各种视频语言模型（包括Qwen和InternVL系列）的任务准确性。该方法在处理涉及小物体、遮挡物体或共现物体的挑战性场景时特别有效。

Conclusion: MACD通过模型引导的反事实数据构建与解码相结合，提供了一种有效减少视频语言模型幻觉的推理策略，特别是在视觉证据不足的复杂场景中表现优异。

Abstract: Video language models (Video-LLMs) are prone to hallucinations, often generating plausible but ungrounded content when visual evidence is weak, ambiguous, or biased. Existing decoding methods, such as contrastive decoding (CD), rely on random perturbations to construct contrastive data for mitigating hallucination patterns. However, such a way is hard to control the visual cues that drive hallucination or well align with model weaknesses. We propose Model-aware Counterfactual Data based Contrastive Decoding (MACD), a new inference strategy that combines model-guided counterfactual construction with decoding. Our approach uses the Video-LLM's own feedback to identify object regions most responsible for hallucination, generating targeted counterfactual inputs at the object level rather than arbitrary frame or temporal modifications. These model-aware counterfactual data is then integrated into CD to enforce evidence-grounded token selection during decoding. Experiments on EventHallusion, MVBench, Perception-test and Video-MME show that MACD consistently reduces hallucination while maintaining or improving task accuracy across diverse Video-LLMs, including Qwen and InternVL families. The method is especially effective in challenging scenarios involving small, occluded, or co-occurring objects. Our code and data will be publicly released.

</details>


### [319] [Controlling Exploration-Exploitation in GFlowNets via Markov Chain Perspectives](https://arxiv.org/abs/2602.01749)
*Lin Chen,Samuel Drapeau,Fanghao Shao,Xuekai Zhu,Bo Xue,Yunchong Song,Mathieu Laurière,Zhouhan Lin*

Main category: cs.AI

TL;DR: 该论文提出α-GFNs，通过可调参数α控制探索-利用平衡，改进GFlowNets在模式发现上的性能，相比传统方法最多提升10倍模式发现能力。


<details>
  <summary>Details</summary>
Motivation: 传统GFlowNet目标函数隐含地固定了前向和后向策略的等比例混合，这限制了训练过程中的探索-利用权衡，影响了模式发现能力。

Method: 通过建立GFlowNets与马尔可夫链的等价关系，揭示约束来源，并提出α-GFNs框架，通过可调参数α控制策略混合比例，同时确保收敛到唯一流。

Result: 在Set、Bit Sequence和Molecule Generation等多个基准测试中，α-GFN目标函数始终优于之前的GFlowNet目标函数，模式发现数量最多提升10倍。

Conclusion: 通过理论分析揭示了GFlowNet约束的马尔可夫链根源，提出的α-GFNs框架有效控制了探索-利用动态平衡，显著提升了模式发现能力，为GFlowNets提供了更灵活的优化框架。

Abstract: Generative Flow Network (GFlowNet) objectives implicitly fix an equal mixing of forward and backward policies, potentially constraining the exploration-exploitation trade-off during training. By further exploring the link between GFlowNets and Markov chains, we establish an equivalence between GFlowNet objectives and Markov chain reversibility, thereby revealing the origin of such constraints, and provide a framework for adapting Markov chain properties to GFlowNets. Building on these theoretical findings, we propose $α$-GFNs, which generalize the mixing via a tunable parameter $α$. This generalization enables direct control over exploration-exploitation dynamics to enhance mode discovery capabilities, while ensuring convergence to unique flows. Across various benchmarks, including Set, Bit Sequence, and Molecule Generation, $α$-GFN objectives consistently outperform previous GFlowNet objectives, achieving up to a $10 \times$ increase in the number of discovered modes.

</details>


### [320] [Adversarial Reward Auditing for Active Detection and Mitigation of Reward Hacking](https://arxiv.org/abs/2602.01750)
*Mohammad Beigi,Ming Jin,Junshan Zhang,Qifan Wang,Lifu Huang*

Main category: cs.AI

TL;DR: 提出对抗性奖励审计(ARA)框架，将奖励黑客攻击重新概念化为动态竞争游戏，通过黑客发现漏洞、审计员检测利用、审计引导RLHF惩罚黑客行为，实现跨领域通用防御。


<details>
  <summary>Details</summary>
Motivation: 现有RLHF方法容易受到奖励黑客攻击，模型会利用奖励模型中的虚假相关性获得高分但违背人类意图。现有防御措施是静态的，无法适应新的攻击策略。

Method: ARA框架分两阶段：1) 黑客策略发现奖励模型漏洞，审计员从潜在表示中学习检测利用；2) 审计引导RLHF(AG-RLHF)通过门控奖励信号惩罚检测到的黑客行为，将奖励黑客攻击从不透明的失败转变为可测量、可控制的信号。

Result: 在三种黑客攻击场景中，ARA在所有基线中实现了最佳对齐-效用权衡：将奉承行为降至接近SFT水平同时提高帮助性，减少冗长同时获得最高ROUGE-L，抑制代码游戏同时提高Pass@1。奖励黑客攻击、检测和缓解都表现出跨领域泛化能力。

Conclusion: ARA将奖励黑客攻击重新概念化为动态竞争游戏，通过黑客-审计员框架实现可测量、可控制的防御，在多个领域实现最佳对齐-效用权衡，并展示了跨领域泛化能力，使单个模型能够高效防御多领域攻击。

Abstract: Reinforcement Learning from Human Feedback (RLHF) remains vulnerable to reward hacking, where models exploit spurious correlations in learned reward models to achieve high scores while violating human intent. Existing mitigations rely on static defenses that cannot adapt to novel exploitation strategies. We propose Adversarial Reward Auditing (ARA), a framework that reconceptualizes reward hacking as a dynamic, competitive game. ARA operates in two stages: first, a Hacker policy discovers reward model vulnerabilities while an Auditor learns to detect exploitation from latent representations; second, Auditor-Guided RLHF (AG-RLHF) gates reward signals to penalize detected hacking, transforming reward hacking from an unobservable failure into a measurable, controllable signal. Experiments across three hacking scenarios demonstrate that ARA achieves the best alignment-utility tradeoff among all baselines: reducing sycophancy to near-SFT levels while improving helpfulness, decreasing verbosity while achieving the highest ROUGE-L, and suppressing code gaming while improving Pass@1. Beyond single-domain evaluation, we show that reward hacking, detection, and mitigation all generalize across domains -- a Hacker trained on code gaming exhibits increased sycophancy despite no reward for this behavior, and an Auditor trained on one domain effectively suppresses exploitation in others, enabling efficient multi-domain defense with a single model.

</details>


### [321] [PRISM: Parametrically Refactoring Inference for Speculative Sampling Draft Models](https://arxiv.org/abs/2602.01762)
*Xuliang Wang,Yuetao Chen,Maochan Zhen,Fang Liu,Xinzhou Zheng,Xingwu Liu,Hong Xu,Ming Li*

Main category: cs.AI

TL;DR: PRISM是一种新的推测解码架构，通过将预测步骤的计算分解到不同参数集，实现了模型容量与推理成本的解耦，显著提升LLM解码速度。


<details>
  <summary>Details</summary>
Motivation: 当前推测解码方法为了获得更好的草稿质量，倾向于使用参数更大的草稿模型，但这带来了显著的计算开销。现有工作试图平衡预测准确性和计算延迟，但需要架构创新来解决这一根本困境。

Method: 提出PRISM架构，将每个预测步骤的计算分解到不同的参数集，重构草稿模型的计算路径，成功实现模型容量与推理成本的解耦。

Result: PRISM在所有现有草稿架构中表现最佳，实现了卓越的接受长度，同时保持最小的草稿延迟，获得优异的端到端加速。在扩展定律方面，PRISM比其他草稿架构在数据量扩大时扩展更有效。在高度优化的推理引擎上，PRISM将解码吞吐量提升了2.6倍以上。

Conclusion: PRISM通过架构创新解决了推测解码中模型容量与推理成本的根本矛盾，为LLM加速提供了高效解决方案，在保持低延迟的同时显著提升解码性能。

Abstract: Large Language Models (LLMs), constrained by their auto-regressive nature, suffer from slow decoding. Speculative decoding methods have emerged as a promising solution to accelerate LLM decoding, attracting attention from both systems and AI research communities. Recently, the pursuit of better draft quality has driven a trend toward parametrically larger draft models, which inevitably introduces substantial computational overhead. While existing work attempts to balance the trade-off between prediction accuracy and compute latency, we address this fundamental dilemma through architectural innovation.
  We propose PRISM, which disaggregates the computation of each predictive step across different parameter sets, refactoring the computational pathways of draft models to successfully decouple model capacity from inference cost. Through extensive experiments, we demonstrate that PRISM outperforms all existing draft architectures, achieving exceptional acceptance lengths while maintaining minimal draft latency for superior end-to-end speedup. We also re-examine scaling laws with PRISM, revealing that PRISM scales more effectively with expanding data volumes than other draft architectures. Through rigorous and fair comparison, we show that PRISM boosts the decoding throughput of an already highly optimized inference engine by more than 2.6x.

</details>


### [322] [Efficient Cross-Architecture Knowledge Transfer for Large-Scale Online User Response Prediction](https://arxiv.org/abs/2602.01775)
*Yucheng Wu,Yuekui Yang,Hongzheng Li,Anan Liu,Jian Xiao,Junjie Zhai,Huan Yu,Shaoping Ma,Leye Wang*

Main category: cs.AI

TL;DR: CrossAdapt是一个两阶段跨架构知识迁移框架，通过维度自适应投影和不对称协同蒸馏，在减少43-71%训练时间的同时提升AUC 0.27-0.43%，有效解决大规模推荐系统中模型切换成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 大规模用户响应预测系统中部署新架构面临高昂的模型切换成本，包括海量历史数据重新训练的昂贵开销，以及在数据保留约束下的性能下降。现有知识蒸馏方法难以处理架构异构性和大型嵌入表迁移的过高成本。

Method: 提出两阶段框架：离线阶段通过维度自适应投影实现快速嵌入迁移，无需迭代训练，结合渐进式网络蒸馏和策略采样降低计算成本；在线阶段引入不对称协同蒸馏，学生频繁更新而教师低频更新，配合分布感知适应机制动态平衡历史知识保留和快速适应演化数据。

Result: 在三个公共数据集上，CrossAdapt实现0.27-0.43%的AUC提升，同时减少43-71%的训练时间。在腾讯微信视频号大规模部署（约1000万日样本）进一步验证其有效性，显著缓解了AUC下降、LogLoss增加和预测偏差。

Conclusion: CrossAdapt通过创新的跨架构知识迁移方法，有效解决了大规模推荐系统中模型切换的高成本和性能下降问题，为实际工业部署提供了高效解决方案。

Abstract: Deploying new architectures in large-scale user response prediction systems incurs high model switching costs due to expensive retraining on massive historical data and performance degradation under data retention constraints. Existing knowledge distillation methods struggle with architectural heterogeneity and the prohibitive cost of transferring large embedding tables. We propose CrossAdapt, a two-stage framework for efficient cross-architecture knowledge transfer. The offline stage enables rapid embedding transfer via dimension-adaptive projections without iterative training, combined with progressive network distillation and strategic sampling to reduce computational cost. The online stage introduces asymmetric co-distillation, where students update frequently while teachers update infrequently, together with a distribution-aware adaptation mechanism that dynamically balances historical knowledge preservation and fast adaptation to evolving data. Experiments on three public datasets show that CrossAdapt achieves 0.27-0.43% AUC improvements while reducing training time by 43-71%. Large-scale deployment on Tencent WeChat Channels (~10M daily samples) further demonstrates its effectiveness, significantly mitigating AUC degradation, LogLoss increase, and prediction bias compared to standard distillation baselines.

</details>


### [323] [LingLanMiDian: Systematic Evaluation of LLMs on TCM Knowledge and Clinical Reasoning](https://arxiv.org/abs/2602.01779)
*Rui Hua,Yu Wei,Zixin Shu,Kai Chang,Dengying Yan,Jianan Xia,Zeyu Liu,Hui Zhu,Shujie Song,Mingzhong Xiao,Xiaodong Li,Dongmei Jia,Zhuye Gao,Yanyan Meng,Naixuan Zhao,Yu Fu,Haibin Yu,Benman Yu,Yuanyuan Chen,Fei Dong,Zhizhou Meng,Pengcheng Yang,Songxue Zhao,Lijuan Pei,Yunhui Hu,Kan Ding,Jiayuan Duan,Wenmao Yin,Yang Gu,Runshun Zhang,Qiang Zhu,Jian Yu,Jiansheng Li,Baoyan Liu,Wenjia Wang,Xuezhong Zhou*

Main category: cs.AI

TL;DR: LingLanMiDian (LingLan) 是一个大规模、专家策划的中医多任务评估基准，统一评估知识回忆、多跳推理、信息抽取和临床决策，揭示了当前LLMs在中医专业推理上与人类专家的显著差距。


<details>
  <summary>Details</summary>
Motivation: 中医具有独特的本体论、术语和推理模式，需要领域忠实评估。现有中医基准存在覆盖碎片化、规模不足、评分方法不统一等问题，阻碍公平比较。

Method: 构建大规模专家策划的多任务评估套件，引入一致的度量设计、临床标签的同义词容忍协议、每个数据集400项的困难子集，并将诊断和治疗建议重构为单选决策识别。

Result: 对14个领先的开源和专有LLMs进行零样本评估，提供了它们在中医常识知识理解、推理和临床决策支持方面的统一视角；困难子集评估显示当前模型在中医专业推理上与人类专家存在显著差距。

Conclusion: LingLan通过标准化评估桥接基础知识和应用推理，为推进中医LLMs和领域特定医疗AI研究建立了统一、可量化、可扩展的基础。

Abstract: Large language models (LLMs) are advancing rapidly in medical NLP, yet Traditional Chinese Medicine (TCM) with its distinctive ontology, terminology, and reasoning patterns requires domain-faithful evaluation. Existing TCM benchmarks are fragmented in coverage and scale and rely on non-unified or generation-heavy scoring that hinders fair comparison. We present the LingLanMiDian (LingLan) benchmark, a large-scale, expert-curated, multi-task suite that unifies evaluation across knowledge recall, multi-hop reasoning, information extraction, and real-world clinical decision-making. LingLan introduces a consistent metric design, a synonym-tolerant protocol for clinical labels, a per-dataset 400-item Hard subset, and a reframing of diagnosis and treatment recommendation into single-choice decision recognition. We conduct comprehensive, zero-shot evaluations on 14 leading open-source and proprietary LLMs, providing a unified perspective on their strengths and limitations in TCM commonsense knowledge understanding, reasoning, and clinical decision support; critically, the evaluation on Hard subset reveals a substantial gap between current models and human experts in TCM-specialized reasoning. By bridging fundamental knowledge and applied reasoning through standardized evaluation, LingLan establishes a unified, quantitative, and extensible foundation for advancing TCM LLMs and domain-specific medical AI research. All evaluation data and code are available at https://github.com/TCMAI-BJTU/LingLan and http://tcmnlp.com.

</details>


### [324] [ORCH: many analyses, one merge-a deterministic multi-agent orchestrator for discrete-choice reasoning with EMA-guided routing](https://arxiv.org/abs/2602.01797)
*Hanlin Zhou,Huah Yong Chan*

Main category: cs.AI

TL;DR: ORCH是一个确定性的多智能体协调框架，通过"多分析、一决策"范式，让异构大语言模型协作解决离散选择推理任务，实现可预测、可复现且无需训练的系统。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的多智能体系统通常依赖随机路由或临时启发式方法，导致行为难以复现、决策过程难以解释。需要一种确定性的协调框架来提高系统的可控性和可解释性。

Method: ORCH采用"多分析、一决策"范式：多个基础模型独立生成结构化分析，然后由专门的合并智能体输出最终选择。框架使用固定规则进行任务分解和答案聚合，保持流程可预测且无需训练。可选地引入EMA引导的路由器，根据历史准确率、延迟或成本更新智能体选择。

Result: 在MMLU、MMLU-Pro和GSM8K上的实验表明，ORCH始终优于单模型基线和多数投票集成。在MMLU-Pro上，ORCH比最强基线提高准确率超过10个百分点；在GSM8K上增益超过50个百分点。EMA路由器提供额外0.7-2.0个百分点的准确率提升。

Conclusion: ORCH为离散选择推理提供了一个实用路径，实现了可控、可解释且可部署的大语言模型智能体系统，通过确定性协调和多智能体协作显著提升了推理性能。

Abstract: Recent advances in large-scale language models (LLMs) have made multi-agent architectures attractive for challenging reasoning tasks. However, many existing systems rely on stochastic routing or ad-hoc heuristics, making their behavior difficult to reproduce and their decision process hard to interpret. We propose ORCH, a deterministic coordination framework for discrete-choice reasoning that orchestrates heterogeneous LLMs. ORCH follows a ``many analyses, one decision'' paradigm: multiple base models independently produce structured analyses, and a dedicated merge agent outputs the final choice. The framework uses fixed rules for task decomposition and answer aggregation, keeping the pipeline predictable, reproducible, and training-free. Determinism here refers to fixed routing and aggregation rules under a fixed evaluation protocol, rather than strict bit-level reproducibility across deployments. To exploit model complementarity, we optionally introduce an EMA-guided router that updates agent selection using historical accuracy, latency, or cost; since it relies on answer-based feedback, it is mainly intended for benchmarking, controlled evaluation, or delayed-feedback settings. Experiments on MMLU, MMLU-Pro, and GSM8K show that ORCH consistently outperforms single-model baselines and a majority-vote ensemble. On MMLU-Pro, ORCH improves accuracy by over 10 points compared to the strongest baseline, and on GSM8K it yields gains exceeding 50 points; McNemar tests confirm statistical significance. The EMA router provides an additional 0.7--2.0 point accuracy boost, and ablations show that both multi-agent collaboration and routing contribute substantially. Overall, ORCH offers a practical path toward controllable, interpretable, and deployment-ready LLM-based agent systems for discrete-choice reasoning.

</details>


### [325] [INDIBATOR: Diverse and Fact-Grounded Individuality for Multi-Agent Debate in Molecular Discovery](https://arxiv.org/abs/2602.01815)
*Yunhui Jang,Seonghyun Park,Jaehyung Kim,Sungsoo Ahn*

Main category: cs.AI

TL;DR: INDIBATOR框架通过基于科学家个性化档案（发表历史和分子历史）构建的智能体，在分子发现任务中超越了传统的粗粒度角色分配方法，实现了更优的性能。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统通常使用通用的角色分配（如"审稿人"、"作者"）或基于关键词的粗粒度角色，这简化了真实科学家的运作方式。科学家的贡献实际上由其独特的研究轨迹塑造，因此需要更精细的个体化建模。

Method: 提出INDIBATOR框架，从两个模态构建科学家个性化档案：1) 发表历史（文献知识）和2) 分子历史（结构先验）。这些智能体通过提案、批评和投票三个阶段进行多轮辩论。

Result: 基于精细个体化档案的智能体系统在分子发现任务中持续优于依赖粗粒度角色的系统，达到竞争性或最先进的性能水平。

Conclusion: 捕捉智能体的"科学DNA"对于高质量的科学发现至关重要，个体化建模是多智能体系统成功的关键因素。

Abstract: Multi-agent systems have emerged as a powerful paradigm for automating scientific discovery. To differentiate agent behavior in the multi-agent system, current frameworks typically assign generic role-based personas such as ''reviewer'' or ''writer'' or rely on coarse grained keyword-based personas. While functional, this approach oversimplifies how human scientists operate, whose contributions are shaped by their unique research trajectories. In response, we propose INDIBATOR, a framework for molecular discovery that grounds agents in individualized scientist profiles constructed from two modalities: publication history for literature-derived knowledge and molecular history for structural priors. These agents engage in multi-turn debate through proposal, critique, and voting phases. Our evaluation demonstrates that these fine-grained individuality-grounded agents consistently outperform systems relying on coarse-grained personas, achieving competitive or state-of-the-art performance. These results validate that capturing the ``scientific DNA'' of individual agents is essential for high-quality discovery.

</details>


### [326] [Synesthesia of Vehicles: Tactile Data Synthesis from Visual Inputs](https://arxiv.org/abs/2602.01832)
*Rui Wang,Yaoguang Cao,Yuyi Chen,Jianyi Xu,Zhuoyang Li,Jiachen Shang,Shichun Yang*

Main category: cs.AI

TL;DR: 提出Synesthesia of Vehicles (SoV)框架，通过视觉输入预测自动驾驶车辆的触觉激励，解决现有视觉和光学传感器无法检测道路激励的问题。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶车辆依赖多模态融合确保安全，但现有视觉和光学传感器无法检测道路引起的激励，而这些激励对车辆动态控制至关重要。受人类联觉启发，需要开发能够从视觉输入预测触觉激励的系统。

Method: 1. 提出Synesthesia of Vehicles (SoV)框架；2. 开发跨模态时空对齐方法处理时间和空间差异；3. 提出基于潜在扩散的视觉-触觉联觉生成模型(VTSyn)进行无监督高质量触觉数据合成；4. 使用真实车辆感知系统收集多模态数据集。

Result: VTSyn在时间、频率和分类性能上优于现有模型，通过主动触觉感知增强自动驾驶安全性。实验在多样道路和光照条件下进行，验证了方法的有效性。

Conclusion: 提出的SoV框架成功实现了从视觉到触觉的跨模态预测，为自动驾驶车辆提供了重要的触觉感知能力，解决了现有传感器系统的局限性，显著提升了车辆安全性能。

Abstract: Autonomous vehicles (AVs) rely on multi-modal fusion for safety, but current visual and optical sensors fail to detect road-induced excitations which are critical for vehicles' dynamic control. Inspired by human synesthesia, we propose the Synesthesia of Vehicles (SoV), a novel framework to predict tactile excitations from visual inputs for autonomous vehicles. We develop a cross-modal spatiotemporal alignment method to address temporal and spatial disparities. Furthermore, a visual-tactile synesthetic (VTSyn) generative model using latent diffusion is proposed for unsupervised high-quality tactile data synthesis. A real-vehicle perception system collected a multi-modal dataset across diverse road and lighting conditions. Extensive experiments show that VTSyn outperforms existing models in temporal, frequency, and classification performance, enhancing AV safety through proactive tactile perception.

</details>


### [327] [ROMA: Recursive Open Meta-Agent Framework for Long-Horizon Multi-Agent Systems](https://arxiv.org/abs/2602.01848)
*Salaheddin Alzu'bi,Baran Nama,Arda Kaz,Anushri Eswaran,Weiyuan Chen,Sarvesh Khetan,Rishab Bala,Tu Vu,Sewoong Oh*

Main category: cs.AI

TL;DR: ROMA是一个递归开放元代理框架，通过任务分解和结构化聚合解决长时程任务中的性能下降、上下文限制和调试困难问题，结合GEPA+实现领先的系统级性能。


<details>
  <summary>Details</summary>
Motivation: 当前代理框架在长时程任务中表现不佳，随着推理深度增加，顺序编排变得脆弱，上下文窗口限制导致性能下降，不透明的执行轨迹使得故障难以定位和调试。

Method: 引入ROMA框架，通过递归任务分解和结构化聚合，将目标分解为依赖感知的子任务树并行执行，同时聚合压缩和验证中间结果以控制上下文增长。框架围绕四个模块化角色构建：Atomizer（决定任务是否分解）、Planner、Executor和Aggregator。结合GEPA+遗传帕累托提示提议器，在不微调的情况下适应特定任务。

Result: 在SEAL-0推理基准上，ROMA结合GLM-4.6相比Kimi-Researcher准确率提升9.9%；在EQ-Bench长文本生成基准上，ROMA使DeepSeek-V3能够匹配Claude Sonnet 4.5等领先闭源模型的性能。

Conclusion: 递归、模块化的代理架构能够在保持可解释性、灵活性和模型无关性的同时，扩展推理深度，为解决长时程任务提供了有效的框架方案。

Abstract: Current agentic frameworks underperform on long-horizon tasks. As reasoning depth increases, sequential orchestration becomes brittle, context windows impose hard limits that degrade performance, and opaque execution traces make failures difficult to localize or debug. We introduce ROMA (Recursive Open Meta-Agents), a domain-agnostic framework that addresses these limitations through recursive task decomposition and structured aggregation. ROMA decomposes goals into dependency-aware subtask trees that can be executed in parallel, while aggregation compresses and validates intermediate results to control context growth. Our framework standardizes agent construction around four modular roles --Atomizer (which decides whether a task should be decomposed), Planner, Executor, and Aggregator -- which cleanly separate orchestration from model selection and enable transparent, hierarchical execution traces. This design supports heterogeneous multi-agent systems that mix models and tools according to cost, latency, and capability. To adapt ROMA to specific tasks without fine-tuning, we further introduce GEPA$+$, an improved Genetic-Pareto prompt proposer that searches over prompts within ROMA's component hierarchy while preserving interface contracts. We show that ROMA, combined with GEPA+, delivers leading system-level performance on reasoning and long-form generation benchmarks. On SEAL-0, which evaluates reasoning over conflicting web evidence, ROMA instantiated with GLM-4.6 improves accuracy by 9.9\% over Kimi-Researcher. On EQ-Bench, a long-form writing benchmark, ROMA enables DeepSeek-V3 to match the performance of leading closed-source models such as Claude Sonnet 4.5. Our results demonstrate that recursive, modular agent architectures can scale reasoning depth while remaining interpretable, flexible, and model-agnostic.

</details>


### [328] [SOPRAG: Multi-view Graph Experts Retrieval for Industrial Standard Operating Procedures](https://arxiv.org/abs/2602.01858)
*Liangtao Lin,Zhaomeng Zhu,Tianwei Zhang,Yonggang Wen*

Main category: cs.AI

TL;DR: SOPRAG：针对工业标准操作程序检索的新型框架，通过专家混合架构解决传统RAG无法处理的工业结构复杂性、条件依赖性和可执行性要求问题。


<details>
  <summary>Details</summary>
Motivation: 工业标准操作程序（SOP）检索面临独特挑战：刚性专有结构、条件依赖相关性、可执行性要求，传统语义驱动的RAG范式无法有效处理这些问题。

Method: 提出SOPRAG框架，采用专家混合范式，包含实体、因果和流程图三个专业专家；引入程序卡层修剪搜索空间消除计算噪声，以及LLM引导的门控机制动态加权专家；还设计了自动化多智能体工作流构建基准数据集。

Result: 在四个工业领域的广泛实验中，SOPRAG在检索准确性和响应实用性方面显著优于强基线（词汇、密集和基于图的RAG），在现实世界关键任务中实现了完美的执行分数。

Conclusion: SOPRAG成功解决了工业SOP检索的核心痛点，通过专家混合架构有效处理工业结构复杂性，为工业环境中的操作安全性和一致性提供了更可靠的解决方案。

Abstract: Standard Operating Procedures (SOPs) are essential for ensuring operational safety and consistency in industrial environments. However, retrieving and following these procedures presents unique challenges, such as rigid proprietary structures, condition-dependent relevance, and actionable execution requirement, which standard semantic-driven Retrieval-Augmented Generation (RAG) paradigms fail to address. Inspired by the Mixture-of-Experts (MoE) paradigm, we propose SOPRAG, a novel framework specifically designed to address the above pain points in SOP retrieval. SOPRAG replaces flat chunking with specialized Entity, Causal, and Flow graph experts to resolve industrial structural and logical complexities. To optimize and coordinate these experts, we propose a Procedure Card layer that prunes the search space to eliminate computational noise, and an LLM-Guided gating mechanism that dynamically weights these experts to align retrieval with operator intent. To address the scarcity of domain-specific data, we also introduce an automated, multi-agent workflow for benchmark construction. Extensive experiments across four industrial domains demonstrate that SOPRAG significantly outperforms strong lexical, dense, and graph-based RAG baselines in both retrieval accuracy and response utility, achieving perfect execution scores in real-world critical tasks.

</details>


### [329] [ProcMEM: Learning Reusable Procedural Memory from Experience via Non-Parametric PPO for LLM Agents](https://arxiv.org/abs/2602.01869)
*Qirui Mi,Zhijian Ma,Mengyue Yang,Haoxuan Li,Yisen Wang,Haifeng Zhang,Jun Wang*

Main category: cs.AI

TL;DR: ProcMEM框架让智能体从交互经验中自主学习程序性记忆，无需参数更新，通过将叙事转化为可执行技能，实现高效经验复用


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的智能体在顺序决策中依赖即时推理，即使在重复场景中也重新推导解决方案，导致计算冗余和执行不稳定，缺乏经验复用

Method: 提出ProcMEM框架：1) 形式化Skill-MDP，将被动叙事转化为可执行技能（激活、执行、终止条件）；2) 引入非参数PPO，利用语义梯度生成高质量候选技能，PPO Gate进行技能验证；3) 基于分数的维护机制保持紧凑高质量的程序性记忆

Result: 实验表明ProcMEM在领域内、跨任务和跨智能体场景中实现更高的复用率和显著性能提升，同时保持极端内存压缩。可视化进化轨迹和技能分布展示了透明积累、精炼和复用程序性知识的过程

Conclusion: ProcMEM通过自主学习程序性记忆，有效解决了LLM智能体经验复用不足的问题，实现了计算效率提升和长期自主性，为智能体经验积累提供了透明可解释的框架

Abstract: LLM-driven agents demonstrate strong performance in sequential decision-making but often rely on on-the-fly reasoning, re-deriving solutions even in recurring scenarios. This insufficient experience reuse leads to computational redundancy and execution instability. To bridge this gap, we propose ProcMEM, a framework that enables agents to autonomously learn procedural memory from interaction experiences without parameter updates. By formalizing a Skill-MDP, ProcMEM transforms passive episodic narratives into executable Skills defined by activation, execution, and termination conditions to ensure executability. To achieve reliable reusability without capability degradation, we introduce Non-Parametric PPO, which leverages semantic gradients for high-quality candidate generation and a PPO Gate for robust Skill verification. Through score-based maintenance, ProcMEM sustains compact, high-quality procedural memory. Experimental results across in-domain, cross-task, and cross-agent scenarios demonstrate that ProcMEM achieves superior reuse rates and significant performance gains with extreme memory compression. Visualized evolutionary trajectories and Skill distributions further reveal how ProcMEM transparently accumulates, refines, and reuses procedural knowledge to facilitate long-term autonomy.

</details>


### [330] [Entropy-Guided Data-Efficient Training for Multimodal Reasoning Reward Models](https://arxiv.org/abs/2602.01884)
*Shidong Yang,Tongwen Huang,Hao Wen,Yong Wang,Li Chen,Xiangxiang Chu*

Main category: cs.AI

TL;DR: 提出基于熵指导的训练方法(EGT)来改进多模态奖励模型，通过熵作为无监督指标来筛选噪声数据和渐进式训练，在三个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前多模态奖励模型训练面临两个关键挑战：1) 偏好数据集中的固有噪声会降低模型性能；2) 传统训练方法效率低下，忽略了样本难度的差异。

Method: 提出熵指导训练(EGT)方法：1) 基于熵的数据筛选，利用响应熵与准确度的强相关性，熵作为无监督代理来识别噪声数据；2) 熵指导的训练策略，渐进式引入更复杂的样本。

Result: 在三个基准测试上的广泛实验表明，EGT训练的模型在多个评估指标上一致优于最先进的多模态奖励模型。

Conclusion: 熵可以作为有效的无监督指标来指导多模态奖励模型的训练，既能缓解噪声数据的影响，又能实现渐进式学习，显著提升模型性能。

Abstract: Multimodal reward models are crucial for aligning multimodal large language models with human preferences. Recent works have incorporated reasoning capabilities into these models, achieving promising results. However, training these models suffers from two critical challenges: (1) the inherent noise in preference datasets, which degrades model performance, and (2) the inefficiency of conventional training methods, which ignore the differences in sample difficulty. In this paper, we identify a strong correlation between response entropy and accuracy, indicating that entropy can serve as a reliable and unsupervised proxy for annotation noise and sample difficulty. Based on this insight, we propose a novel Entropy-Guided Training (EGT) approach for multimodal reasoning reward models, which combines two strategies: (1) entropy-guided data curation to mitigate the impact of unreliable samples, and (2) an entropy-guided training strategy that progressively introduces more complex examples. Extensive experiments across three benchmarks show that the EGT-trained model consistently outperforms state-of-the-art multimodal reward models.

</details>


### [331] [Geometric Analysis of Token Selection in Multi-Head Attention](https://arxiv.org/abs/2602.01893)
*Timur Mudarisov,Mikhal Burtsev,Tatiana Petrova,Radu State*

Main category: cs.AI

TL;DR: 本文提出一个几何框架分析大语言模型中的多头注意力机制，将标准注意力视为top-N选择器，在值状态空间中研究其行为，定义几何指标量化选择与非选择token的可分性，并推导非渐近边界。


<details>
  <summary>Details</summary>
Motivation: 现有注意力机制分析缺乏几何视角，难以量化token选择过程。本文旨在通过几何框架理解注意力如何作为结构化分类器工作，提供头级可解释性，并为几何感知的注意力稀疏化和设计提供依据。

Method: 将标准注意力视为top-N选择器，在值状态空间分析其行为。定义几何指标（精确率、召回率、F分数）量化token可分性。在经验假设下推导非渐近边界（稳定值范数、压缩sink token、指数相似度衰减、分段注意力权重分布）。在LLaMA-2-7B、Gemma-7B、Mistral-7B上进行实证验证。

Result: 理论预测小N操作机制具有最强的非平凡可分性，阐明了序列长度和sink相似度如何影响几何指标。实证测量与理论包络线紧密匹配：top-N选择增强可分性，sink相似度与召回率相关。发现LLaMA-2-7B中的注意力头专门化为三种机制：检索器、混合器、重置器，具有不同的几何特征。

Conclusion: 注意力机制表现为具有可测量token选择标准的结构化几何分类器，提供头级可解释性，并为几何感知的注意力稀疏化和LLM中注意力机制的设计提供信息。

Abstract: We present a geometric framework for analysing multi-head attention in large language models (LLMs). Without altering the mechanism, we view standard attention through a top-N selection lens and study its behaviour directly in value-state space. We define geometric metrics - Precision, Recall, and F-score - to quantify separability between selected and non-selected tokens, and derive non-asymptotic bounds with explicit dependence on dimension and margin under empirically motivated assumptions (stable value norms with a compressed sink token, exponential similarity decay, and piecewise attention weight profiles). The theory predicts a small-N operating regime of strongest non-trivial separability and clarifies how sequence length and sink similarity shape the metrics. Empirically, across LLaMA-2-7B, Gemma-7B, and Mistral-7B, measurements closely track the theoretical envelopes: top-N selection sharpens separability, sink similarity correlates with Recall. We also found that in LLaMA-2-7B heads specialize into three regimes - Retriever, Mixer, Reset - with distinct geometric signatures. Overall, attention behaves as a structured geometric classifier with measurable criteria for token selection, offering head level interpretability and informing geometry-aware sparsification and design of attention in LLMs.

</details>


### [332] [DomusFM: A Foundation Model for Smart-Home Sensor Data](https://arxiv.org/abs/2602.01910)
*Michele Fiori,Gabriele Civitarese,Flora D. Salim,Claudio Bettini*

Main category: cs.AI

TL;DR: DomusFM是首个专门为智能家居传感器数据设计的预训练基础模型，通过自监督双对比学习范式，在数据稀缺情况下仍能实现优越的下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在关键限制：监督模型需要大量标注数据不切实际；活动识别基础模型只关注惯性传感器，无法处理智能家居二进制传感器事件的稀疏离散特性；LLM方法需要自然语言描述或提示，依赖外部服务或昂贵硬件，存在隐私和成本问题。

Method: 采用自监督双对比学习范式，结合轻量级语言模型的语义嵌入、时序模式专用编码器和二进制状态编码器，捕获token级语义属性和序列级时序依赖，学习可迁移的通用表示。

Result: 在七个公开智能家居数据集上进行留一数据集评估，DomusFM在不同下游任务上优于现有最先进基线，即使在仅有5%标注数据用于微调的情况下仍能实现优越性能。

Conclusion: DomusFM解决了数据稀缺问题，同时保持了实际部署可行性，为现实世界智能家居系统提供了实用的解决方案。

Abstract: Smart-home sensor data holds significant potential for several applications, including healthcare monitoring and assistive technologies. Existing approaches, however, face critical limitations. Supervised models require impractical amounts of labeled data. Foundation models for activity recognition focus only on inertial sensors, failing to address the unique characteristics of smart-home binary sensor events: their sparse, discrete nature combined with rich semantic associations. LLM-based approaches, while tested in this domain, still raise several issues regarding the need for natural language descriptions or prompting, and reliance on either external services or expensive hardware, making them infeasible in real-life scenarios due to privacy and cost concerns. We introduce DomusFM, the first foundation model specifically designed and pretrained for smart-home sensor data. DomusFM employs a self-supervised dual contrastive learning paradigm to capture both token-level semantic attributes and sequence-level temporal dependencies. By integrating semantic embeddings from a lightweight language model and specialized encoders for temporal patterns and binary states, DomusFM learns generalizable representations that transfer across environments and tasks related to activity and event analysis. Through leave-one-dataset-out evaluation across seven public smart-home datasets, we demonstrate that DomusFM outperforms state-of-the-art baselines on different downstream tasks, achieving superior performance even with only 5% of labeled training data available for fine-tuning. Our approach addresses data scarcity while maintaining practical deployability for real-world smart-home systems.

</details>


### [333] [Large Language Model and Formal Concept Analysis: a comparative study for Topic Modeling](https://arxiv.org/abs/2602.01933)
*Fabrice Boissier,Monica Sen,Irina Rychkova*

Main category: cs.AI

TL;DR: 该论文比较了大型语言模型（LLM）和形式概念分析（FCA）在主题建模任务中的表现，通过两个实验评估它们在文档主题提取方面的效果。


<details>
  <summary>Details</summary>
Motivation: 主题建模在文档检索、情感分析和文本摘要等领域应用日益广泛，但当前研究较少探讨大型语言模型（LLM）在该任务中的实用性。同时，形式概念分析（FCA）虽被提出作为主题建模的候选方法，但缺乏实际应用案例研究。

Method: 采用CREA流程评估FCA方法，使用GPT-5评估LLM方法。LLM采用零样本设置下的三提示策略：从文档批次生成主题、合并批次结果形成最终主题、以及主题标注。进行两个实验：第一个重用之前评估CREA的教学材料，第二个分析40篇信息系统研究文章。

Result: 论文通过两个实验比较了LLM和FCA在主题建模中的表现：第一个实验使用教学材料，第二个实验分析信息系统研究文章，将提取的主题与实际子领域进行比较。

Conclusion: 该研究通过实证比较LLM和FCA在主题建模任务中的表现，旨在更好地理解这两种方法的优势和局限性，为主题建模领域提供新的技术评估视角。

Abstract: Topic modeling is a research field finding increasing applications: historically from document retrieving, to sentiment analysis and text summarization. Large Language Models (LLM) are currently a major trend in text processing, but few works study their usefulness for this task. Formal Concept Analysis (FCA) has recently been presented as a candidate for topic modeling, but no real applied case study has been conducted. In this work, we compare LLM and FCA to better understand their strengths and weakneses in the topic modeling field. FCA is evaluated through the CREA pipeline used in past experiments on topic modeling and visualization, whereas GPT-5 is used for the LLM. A strategy based on three prompts is applied with GPT-5 in a zero-shot setup: topic generation from document batches, merging of batch results into final topics, and topic labeling. A first experiment reuses the teaching materials previously used to evaluate CREA, while a second experiment analyzes 40 research articles in information systems to compare the extracted topics with the underling subfields.

</details>


### [334] [Small Generalizable Prompt Predictive Models Can Steer Efficient RL Post-Training of Large Reasoning Models](https://arxiv.org/abs/2602.01970)
*Yun Qu,Qi Wang,Yixiu Mao,Heming Zou,Yuhang Jiang,Weijie Liu,Clive Bai,Kai Yang,Yangkun Chen,Saiyong Yang,Xiangyang Ji*

Main category: cs.AI

TL;DR: GPS使用轻量级生成模型进行贝叶斯推理预测提示难度，通过中间难度优先和历史锚定多样性选择信息丰富的提示批次，提高强化学习训练效率和测试时计算分配


<details>
  <summary>Details</summary>
Motivation: 强化学习提升大语言模型推理能力但计算成本高，现有在线提示选择方法要么依赖昂贵的精确评估，要么缺乏跨提示的泛化能力

Method: 提出可泛化预测提示选择(GPS)：使用轻量级生成模型在共享优化历史上进行贝叶斯推理预测提示难度，结合中间难度优先和历史锚定多样性进行批量获取

Result: 在多个推理基准测试中，GPS在训练效率、最终性能和测试时效率方面显著优于现有基线方法

Conclusion: GPS通过可泛化的预测模型和智能提示选择策略，有效解决了强化学习中提示选择的效率和泛化问题

Abstract: Reinforcement learning enhances the reasoning capabilities of large language models but often involves high computational costs due to rollout-intensive optimization. Online prompt selection presents a plausible solution by prioritizing informative prompts to improve training efficiency. However, current methods either depend on costly, exact evaluations or construct prompt-specific predictive models lacking generalization across prompts. This study introduces Generalizable Predictive Prompt Selection (GPS), which performs Bayesian inference towards prompt difficulty using a lightweight generative model trained on the shared optimization history. Intermediate-difficulty prioritization and history-anchored diversity are incorporated into the batch acquisition principle to select informative prompt batches. The small predictive model also generalizes at test-time for efficient computational allocation. Experiments across varied reasoning benchmarks indicate GPS's substantial improvements in training efficiency, final performance, and test-time efficiency over superior baseline methods.

</details>


### [335] [Evolving from Tool User to Creator via Training-Free Experience Reuse in Multimodal Reasoning](https://arxiv.org/abs/2602.01983)
*Xintian Shen,Jiawei Chen,Lihao Zheng,Hao Ma,Tao Wei,Kun Zhan*

Main category: cs.AI

TL;DR: UCT框架让LLM从工具使用者转变为工具创造者，通过提取推理经验创建自适应工具，无需额外训练即可持续改进


<details>
  <summary>Details</summary>
Motivation: 现有工具集成推理模型面临三个主要问题：1) 固定工具难以应对开放性问题；2) 错误工具输出会误导LLM；3) 工具构建需要大量人工工作，限制了适用性

Method: 提出UCT训练免费框架，将LLM从工具使用者转变为工具创造者。通过提取推理经验创建可重用工具，引入记忆巩固机制维护工具库，在推理过程中自适应创建和更新工具

Result: 在数学和科学推理任务基准测试中取得显著性能提升：+20.86%和+23.04%，验证了代理的自进化能力

Conclusion: UCT为增强TIR模型能力提供了新范式，通过自动化工具构建使代理系统能够持续进步而无需额外训练，实现了从工具使用者到工具创造者的转变

Abstract: Existing Tool-Integrated Reasoning (TIR) models have effectively extended the question-answering capabilities of LLMs by incorporating external tools. However, real-world scenarios present numerous open-ended problems where fixed tools often fail to meet task requirements. Furthermore, the lack of self-optimization mechanisms means that erroneous tool outputs can mislead the LLM's responses. Additionally, the construction of existing tools entails significant manual effort, which consequently constrains their applicability. Recognizing that the reasoning traces of LLMs encapsulate implicit problem-solving capabilities, we propose UCT, a novel training-free framework that transforms agents from tool users to tool creators. This approach harvests reasoning experiences and distills them into reusable assets. This method transforms the agent from a mere tool user into a tool creator, enabling adaptive tool creation and self-updating during the inference process. We also introduce a memory consolidation mechanism to maintain the tool library, ensuring high reusability of retained experiential memory for subsequent reasoning tasks. This novel automated tool construction paradigm continuously improves tool quality during reasoning, allowing the overall agent system to progress without additional training. Extensive experiments demonstrate that our method serves as a novel paradigm for enhancing the capabilities of TIR models. In particular, the significant performance gains achieved +20.86%$\uparrow$ and +23.04%$\uparrow$ on benchmarks across multi-domain mathematical and scientific reasoning tasks validate the self-evolving capability of the agent.

</details>


### [336] [Emergent Analogical Reasoning in Transformers](https://arxiv.org/abs/2602.01992)
*Gouki Minegishi,Jingyuan Feng,Hiroki Furuta,Takeshi Kojima,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.AI

TL;DR: 论文将类比推理形式化为跨类别实体对应关系的推断，基于范畴论中的函子概念，通过合成任务研究Transformer中类比推理的涌现机制，发现其依赖于嵌入空间的几何对齐和Transformer内部的函子应用。


<details>
  <summary>Details</summary>
Motivation: 类比是人类智能的核心能力，但Transformer如何获取和实现类比推理的机制尚不清楚。本文旨在将类比从抽象的认知概念转化为现代神经网络中具体、机制基础的现象。

Method: 受范畴论中函子概念的启发，将类比推理形式化为跨类别实体对应关系的推断。引入合成任务在受控设置下评估类比推理的涌现，并进行机制分析。

Result: 发现类比推理的涌现对数据特征、优化选择和模型规模高度敏感。机制分析表明Transformer中的类比推理分解为两个关键组件：(1)嵌入空间中关系结构的几何对齐；(2)Transformer内部函子的应用。这些机制使模型能够将关系结构从一个类别转移到另一个类别，实现类比。

Conclusion: 通过量化这些效应，发现在预训练LLM中观察到相同的趋势，从而将类比从抽象认知概念转化为现代神经网络中具体、机制基础的现象。

Abstract: Analogy is a central faculty of human intelligence, enabling abstract patterns discovered in one domain to be applied to another. Despite its central role in cognition, the mechanisms by which Transformers acquire and implement analogical reasoning remain poorly understood. In this work, inspired by the notion of functors in category theory, we formalize analogical reasoning as the inference of correspondences between entities across categories. Based on this formulation, we introduce synthetic tasks that evaluate the emergence of analogical reasoning under controlled settings. We find that the emergence of analogical reasoning is highly sensitive to data characteristics, optimization choices, and model scale. Through mechanistic analysis, we show that analogical reasoning in Transformers decomposes into two key components: (1) geometric alignment of relational structure in the embedding space, and (2) the application of a functor within the Transformer. These mechanisms enable models to transfer relational structure from one category to another, realizing analogy. Finally, we quantify these effects and find that the same trends are observed in pretrained LLMs. In doing so, we move analogy from an abstract cognitive notion to a concrete, mechanistically grounded phenomenon in modern neural networks.

</details>


### [337] [Thinking Like a Doctor: Conversational Diagnosis through the Exploration of Diagnostic Knowledge Graphs](https://arxiv.org/abs/2602.01995)
*Jeongmoon Won,Seungwon Kook,Yohan Jo*

Main category: cs.AI

TL;DR: 提出基于知识图谱的对话诊断系统，通过生成诊断假设和验证性提问进行多轮推理，使用改进的患者模拟器评估，在诊断准确性和效率上优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有对话诊断方法要么依赖模型的参数知识，要么假设患者提供丰富具体的信息，这在现实中不切实际。需要解决在信息不完整情况下的多轮病史采集和诊断推理问题。

Method: 提出两步骤对话诊断系统：1）从对话上下文生成诊断假设；2）通过澄清问题验证假设，循环直到最终诊断。使用改进的MIMIC-IV患者模拟器，模拟早期临床接触中患者的模糊症状描述。

Result: 实验显示在诊断准确性和效率上优于强基线方法。医生评估支持模拟器的真实性和生成问题的临床实用性。

Conclusion: 提出的基于知识图谱的对话诊断系统能够有效处理信息不完整的现实场景，通过多轮假设生成和验证提高诊断性能，具有临床实用价值。

Abstract: Conversational diagnosis requires multi-turn history-taking, where an agent asks clarifying questions to refine differential diagnoses under incomplete information. Existing approaches often rely on the parametric knowledge of a model or assume that patients provide rich and concrete information, which is unrealistic. To address these limitations, we propose a conversational diagnosis system that explores a diagnostic knowledge graph to reason in two steps: (i) generating diagnostic hypotheses from the dialogue context, and (ii) verifying hypotheses through clarifying questions, which are repeated until a final diagnosis is reached. Since evaluating the system requires a realistic patient simulator that responds to the system's questions, we adopt a well-established simulator along with patient profiles from MIMIC-IV. We further adapt it to describe symptoms vaguely to reflect real-world patients during early clinical encounters. Experiments show improved diagnostic accuracy and efficiency over strong baselines, and evaluations by physicians support the realism of our simulator and the clinical utility of the generated questions. Our code will be released upon publication.

</details>


### [338] [Do I Really Know? Learning Factual Self-Verification for Hallucination Reduction](https://arxiv.org/abs/2602.02018)
*Enes Altinisik,Masoomali Fatehkia,Fatih Deniz,Nadir Durrani,Majd Hawasly,Mohammad Raza,Husrev Taha Sencar*

Main category: cs.AI

TL;DR: VeriFY框架通过一致性自验证训练LLMs处理事实不确定性，减少幻觉同时保持召回率


<details>
  <summary>Details</summary>
Motivation: 现有缓解事实幻觉的方法要么依赖外部事后验证，要么在微调中直接将不确定性映射为弃权，导致过于保守的行为

Method: 提出VeriFY训练框架，通过结构化验证轨迹教导LLMs进行一致性自验证：生成初始答案→提出验证查询→一致性判断→决定回答或弃权。采用阶段级损失掩码避免强化幻觉内容

Result: 在多个模型家族和规模上，VeriFY将事实幻觉率降低9.7%至53.3%，召回率仅轻微下降0.4%至5.7%，且在单数据集训练后能跨数据集泛化

Conclusion: VeriFY通过训练时的一致性自验证有效减少LLMs的事实幻觉，在减少幻觉和保持召回率之间取得良好平衡

Abstract: Factual hallucination remains a central challenge for large language models (LLMs). Existing mitigation approaches primarily rely on either external post-hoc verification or mapping uncertainty directly to abstention during fine-tuning, often resulting in overly conservative behavior. We propose VeriFY, a training-time framework that teaches LLMs to reason about factual uncertainty through consistency-based self-verification. VeriFY augments training with structured verification traces that guide the model to produce an initial answer, generate and answer a probing verification query, issue a consistency judgment, and then decide whether to answer or abstain. To address the risk of reinforcing hallucinated content when training on augmented traces, we introduce a stage-level loss masking approach that excludes hallucinated answer stages from the training objective while preserving supervision over verification behavior. Across multiple model families and scales, VeriFY reduces factual hallucination rates by 9.7 to 53.3 percent, with only modest reductions in recall (0.4 to 5.7 percent), and generalizes across datasets when trained on a single source. The source code, training data, and trained model checkpoints will be released upon acceptance.

</details>


### [339] [Light Alignment Improves LLM Safety via Model Self-Reflection with a Single Neuron](https://arxiv.org/abs/2602.02027)
*Sicheng Shen,Mingyang Lv,Han Shen,Jialin Wu,Binghao Wang,Zhou Yang,Guobin Shen,Dongcheng Zhao,Feifei Zhao,Yi Zeng*

Main category: cs.AI

TL;DR: 提出一种基于专家模型和单神经元门控机制的安全感知解码方法，实现轻量级对齐，在保持模型实用性的同时增强输出安全性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全对齐方法主要依赖后训练，计算成本高且泛化能力差；轻量级方法要么依赖预先计算的安全注入，要么过度依赖模型自身能力，导致泛化有限且生成效率降低。

Method: 提出安全感知解码方法：仅需低成本训练专家模型，使用单神经元作为门控机制，有效平衡模型内在能力和外部指导。

Result: 在训练开销和跨模型规模泛化方面具有明显优势，同时保持实用性和增强输出安全性。

Conclusion: 为大型语言模型的安全实用部署提供了轻量级对齐的新视角。

Abstract: The safety of large language models (LLMs) has increasingly emerged as a fundamental aspect of their development. Existing safety alignment for LLMs is predominantly achieved through post-training methods, which are computationally expensive and often fail to generalize well across different models. A small number of lightweight alignment approaches either rely heavily on prior-computed safety injections or depend excessively on the model's own capabilities, resulting in limited generalization and degraded efficiency and usability during generation. In this work, we propose a safety-aware decoding method that requires only low-cost training of an expert model and employs a single neuron as a gating mechanism. By effectively balancing the model's intrinsic capabilities with external guidance, our approach simultaneously preserves utility and enhances output safety. It demonstrates clear advantages in training overhead and generalization across model scales, offering a new perspective on lightweight alignment for the safe and practical deployment of large language models. Code: https://github.com/Beijing-AISI/NGSD.

</details>


### [340] [Edit Knowledge, Not Just Facts via Multi-Step Reasoning over Background Stories](https://arxiv.org/abs/2602.02028)
*Ya Gao,Kalle Kujanpää,Pekka Marttinen,Harri Valpola,Alexander Ilin*

Main category: cs.AI

TL;DR: 提出一种基于推理的知识内化训练策略，通过背景故事、多跳问题和知识蒸馏，使AI模型能有效整合新知识进行多步推理。


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑方法侧重于原子事实的记忆，但无法将新知识整合到连贯框架中跨上下文使用。作者认为知识内化本质上是推理问题而非记忆问题，需要让模型在任务中结合新旧知识进行多步推理。

Method: 提出基于三个原则的训练策略：1) 将新知识作为连贯背景故事引入，解释新事实与现有知识的关系；2) 使用自生成的多跳问题进行训练，要求涉及新信息的多步推理；3) 采用知识蒸馏，让学生模型内化教师模型的推理行为而无需直接访问新信息。

Result: 实验表明，采用此策略训练的模型能有效利用新获取的知识进行推理，在需要结合多个新事实的挑战性问题中表现优异。

Conclusion: 通过将知识内化视为推理问题而非记忆问题，并采用基于背景故事、多跳推理和知识蒸馏的训练策略，可以使AI模型更好地整合和应用新知识。

Abstract: Enabling artificial intelligence systems, particularly large language models, to integrate new knowledge and flexibly apply it during reasoning remains a central challenge. Existing knowledge editing approaches emphasize atomic facts, improving factual recall but often failing to integrate new information into a coherent framework usable across contexts. In this work, we argue that knowledge internalization is fundamentally a reasoning problem rather than a memorization problem. Consequently, a model should be trained in situations where the new information is instrumental to solving a task, combined with pre-existing knowledge, and exercised through multi-step reasoning. Based on this insight, we propose a training strategy based on three principles. First, new knowledge is introduced as a coherent background story that contextualizes novel facts and explains their relation to existing knowledge. Second, models are trained using self-generated multi-hop questions that require multi-step reasoning involving the new information. Third, training is done using knowledge distillation, forcing a student model to internalize the teacher's reasoning behavior without access to the novel information. Experiments show that models trained with this strategy effectively leverage newly acquired knowledge during reasoning and achieve remarkable performance on challenging questions that require combining multiple new facts.

</details>


### [341] [Canonical Intermediate Representation for LLM-based optimization problem formulation and code generation](https://arxiv.org/abs/2602.02029)
*Zhongyuan Lyu,Shuoyu Hu,Lujie Liu,Hongxia Yang,Ming LI*

Main category: cs.AI

TL;DR: 提出CIR中间表示和R2C框架，通过多智能体管道将自然语言描述转换为优化模型，在复杂操作规则建模上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的方法在处理复杂操作规则时，难以处理复合约束和选择合适的建模范式，需要更系统的方法来自动化优化模型构建。

Method: 引入规范中间表示(CIR)作为LLM生成的中间层，通过约束原型和候选建模范式编码操作规则语义；开发R2C多智能体框架，包含问题解析、CIR合成和模型实例化；采用反思机制进一步提升性能。

Result: 在新建的包含丰富操作规则的基准测试上达到47.2%的准确率；在已有基准测试上接近GPT-5等专有模型性能；通过反思机制在某些基准上创造了新的最佳记录。

Conclusion: CIR和R2C框架有效解决了复杂操作规则的优化建模问题，通过解耦规则逻辑与数学实例化，显著提升了自然语言到优化模型的转换能力。

Abstract: Automatically formulating optimization models from natural language descriptions is a growing focus in operations research, yet current LLM-based approaches struggle with the composite constraints and appropriate modeling paradigms required by complex operational rules. To address this, we introduce the Canonical Intermediate Representation (CIR): a schema that LLMs explicitly generate between problem descriptions and optimization models. CIR encodes the semantics of operational rules through constraint archetypes and candidate modeling paradigms, thereby decoupling rule logic from its mathematical instantiation. Upon a newly generated CIR knowledge base, we develop the rule-to-constraint (R2C) framework, a multi-agent pipeline that parses problem texts, synthesizes CIR implementations by retrieving domain knowledge, and instantiates optimization models. To systematically evaluate rule-to-constraint reasoning, we test R2C on our newly constructed benchmark featuring rich operational rules, and benchmarks from prior work. Extensive experiments show that R2C achieves state-of-the-art accuracy on the proposed benchmark (47.2% Accuracy Rate). On established benchmarks from the literature, R2C delivers highly competitive results, approaching the performance of proprietary models (e.g., GPT-5). Moreover, with a reflection mechanism, R2C achieves further gains and sets new best-reported results on some benchmarks.

</details>


### [342] [Constrained Process Maps for Multi-Agent Generative AI Workflows](https://arxiv.org/abs/2602.02034)
*Ananya Joshi,Michael Rudow*

Main category: cs.AI

TL;DR: 本文提出了一种基于有限时域马尔可夫决策过程的多智能体系统，用于改进LLM智能体在合规审查等受监管场景中的不确定性管理和协调能力，相比单智能体基线在准确性、人工审查需求和处理时间方面均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的智能体在合规、尽职调查等受监管场景中执行复杂多步骤工作流时，主要依赖单智能体的提示工程，难以观察和比较模型如何处理跨决策阶段的不确定性和协调问题，以及与人工监督的交互。

Method: 提出将多智能体系统形式化为具有有向无环结构的有限时域马尔可夫决策过程。每个智能体对应特定角色或决策阶段（如合规工作流中的内容、业务或法律审查），具有预定义的转换表示任务升级或完成。使用蒙特卡洛估计在智能体层面量化认知不确定性，系统级不确定性通过MDP在自动标记状态或人工审查状态中的终止来捕获。

Result: 在AI安全评估（自残检测）的案例研究中，相比单智能体基线实现了显著改进：准确率提升高达19%，所需人工审查减少高达85倍，某些配置下处理时间也有所减少。

Conclusion: 基于MDP的多智能体系统框架能够有效管理LLM智能体在受监管工作流中的不确定性和协调问题，通过明确的决策结构和不确定性量化机制，在保持或提升准确性的同时大幅减少人工干预需求。

Abstract: Large language model (LLM)-based agents are increasingly used to perform complex, multi-step workflows in regulated settings such as compliance and due diligence. However, many agentic architectures rely primarily on prompt engineering of a single agent, making it difficult to observe or compare how models handle uncertainty and coordination across interconnected decision stages and with human oversight. We introduce a multi-agent system formalized as a finite-horizon Markov Decision Process (MDP) with a directed acyclic structure. Each agent corresponds to a specific role or decision stage (e.g., content, business, or legal review in a compliance workflow), with predefined transitions representing task escalation or completion. Epistemic uncertainty is quantified at the agent level using Monte Carlo estimation, while system-level uncertainty is captured by the MDP's termination in either an automated labeled state or a human-review state. We illustrate the approach through a case study in AI safety evaluation for self-harm detection, implemented as a multi-agent compliance system. Results demonstrate improvements over a single-agent baseline, including up to a 19\% increase in accuracy, up to an 85x reduction in required human review, and, in some configurations, reduced processing time.

</details>


### [343] [Hunt Instead of Wait: Evaluating Deep Data Research on Large Language Models](https://arxiv.org/abs/2602.02039)
*Wei Liu,Peijie Yu,Michele Orini,Yali Du,Yulan He*

Main category: cs.AI

TL;DR: 论文提出了"调查性智能"概念，区别于传统的"执行性智能"，并引入Deep Data Research任务和DDR-Bench基准来评估LLM在自主数据探索中的能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注LLM完成指定任务的能力（执行性智能），但缺乏对LLM自主设定目标、探索数据的调查性智能的评估。数据科学领域需要从原始数据中自主发现洞察，这为测试调查性智能提供了天然场景。

Method: 1. 提出Deep Data Research任务：LLM从数据库自主提取关键洞察的开放式任务；2. 开发DDR-Bench：基于检查表的大规模基准，支持可验证评估；3. 区分调查性智能与执行性智能的概念框架。

Result: 前沿模型显示出初步的自主能力，但长期探索仍然具有挑战性。研究发现有效的调查性智能不仅依赖于外部代理框架或单纯模型缩放，更需要模型内在的代理策略。

Conclusion: 调查性智能是LLM代理能力的重要维度，需要专门的评估方法。DDR-Bench为评估LLM在数据科学中的自主探索能力提供了有效工具，揭示了当前模型在长期探索方面的局限性。

Abstract: The agency expected of Agentic Large Language Models goes beyond answering correctly, requiring autonomy to set goals and decide what to explore. We term this investigatory intelligence, distinguishing it from executional intelligence, which merely completes assigned tasks. Data Science provides a natural testbed, as real-world analysis starts from raw data rather than explicit queries, yet few benchmarks focus on it. To address this, we introduce Deep Data Research (DDR), an open-ended task where LLMs autonomously extract key insights from databases, and DDR-Bench, a large-scale, checklist-based benchmark that enables verifiable evaluation. Results show that while frontier models display emerging agency, long-horizon exploration remains challenging. Our analysis highlights that effective investigatory intelligence depends not only on agent scaffolding or merely scaling, but also on intrinsic strategies of agentic models.

</details>


### [344] [Rethinking the Role of Entropy in Optimizing Tool-Use Behaviors for Large Language Model Agents](https://arxiv.org/abs/2602.02050)
*Zeping Li,Hongru Wang,Yiwen Zhao,Guanhua Chen,Yixia Li,Keyang Chen,Yixin Cao,Guangnan Ye,Hongfeng Chai,Mengdi Wang,Zhenfei Yin*

Main category: cs.AI

TL;DR: 基于熵减的奖励策略优化LLM工具使用行为：稀疏奖励减少72.07%工具调用，密集奖励提升22.27%性能


<details>
  <summary>Details</summary>
Motivation: LLM工具使用代理在长轨迹中经常触发过多低质量工具调用，增加延迟并降低推理性能，需要有效管理工具使用行为

Method: 通过熵减实验发现熵减与高质量工具调用正相关，提出以熵减作为监督信号，设计两种奖励策略：稀疏结果奖励（轨迹级指导）和密集过程奖励（细粒度监督）

Result: 实验表明两种奖励设计都能改进工具使用行为：稀疏奖励相比基线平均减少72.07%工具调用，密集奖励提升22.27%性能

Conclusion: 熵减是增强工具使用行为的关键机制，使代理在现实应用中更具适应性

Abstract: Tool-using agents based on Large Language Models (LLMs) excel in tasks such as mathematical reasoning and multi-hop question answering. However, in long trajectories, agents often trigger excessive and low-quality tool calls, increasing latency and degrading inference performance, making managing tool-use behavior challenging. In this work, we conduct entropy-based pilot experiments and observe a strong positive correlation between entropy reduction and high-quality tool calls. Building on this finding, we propose using entropy reduction as a supervisory signal and design two reward strategies to address the differing needs of optimizing tool-use behavior. Sparse outcome rewards provide coarse, trajectory-level guidance to improve efficiency, while dense process rewards offer fine-grained supervision to enhance performance. Experiments across diverse domains show that both reward designs improve tool-use behavior: the former reduces tool calls by 72.07% compared to the average of baselines, while the latter improves performance by 22.27%. These results position entropy reduction as a key mechanism for enhancing tool-use behavior, enabling agents to be more adaptive in real-world applications.

</details>


### [345] [SIDiffAgent: Self-Improving Diffusion Agent](https://arxiv.org/abs/2602.02051)
*Shivank Garg,Ayush Singh,Gaurav Kumar Nayak*

Main category: cs.AI

TL;DR: SIDiffAgent是一个无需训练的多模态代理框架，利用Qwen系列模型解决文本到图像扩散模型的局限性，通过自主提示工程、错误检测修正和迭代自我改进，显著提升生成质量和可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像扩散模型存在多个实际部署障碍：对提示词表述敏感、语义歧义（如"mouse"可指动物或电脑外设）、解剖结构扭曲等伪影问题，以及需要精心设计的输入提示。现有方法通常需要额外训练且可控性有限，限制了在实际应用中的适应性。

Method: 提出SIDiffAgent训练免费代理框架，利用Qwen系列多模态模型（Qwen-VL、Qwen-Image、Qwen-Edit、Qwen-Embedding）自主管理提示工程、检测修正生成错误、执行细粒度伪影移除。框架包含迭代自我改进机制，将过往经验存储在数据库中，并在代理流程各阶段注入基于提示的指导。

Result: 在GenAIBench基准测试中，SIDiffAgent取得了平均VQA分数0.884，显著优于开源模型、专有模型和其他代理方法。该框架无需额外训练即可实现高质量图像生成。

Conclusion: SIDiffAgent通过多模态代理框架有效解决了文本到图像扩散模型的部署限制，实现了自主提示优化、错误修正和迭代改进，为实际应用提供了更可靠、一致的生成能力，代码将在接受后公开。

Abstract: Text-to-image diffusion models have revolutionized generative AI, enabling high-quality and photorealistic image synthesis. However, their practical deployment remains hindered by several limitations: sensitivity to prompt phrasing, ambiguity in semantic interpretation (e.g., ``mouse" as animal vs. a computer peripheral), artifacts such as distorted anatomy, and the need for carefully engineered input prompts. Existing methods often require additional training and offer limited controllability, restricting their adaptability in real-world applications. We introduce Self-Improving Diffusion Agent (SIDiffAgent), a training-free agentic framework that leverages the Qwen family of models (Qwen-VL, Qwen-Image, Qwen-Edit, Qwen-Embedding) to address these challenges. SIDiffAgent autonomously manages prompt engineering, detects and corrects poor generations, and performs fine-grained artifact removal, yielding more reliable and consistent outputs. It further incorporates iterative self-improvement by storing a memory of previous experiences in a database. This database of past experiences is then used to inject prompt-based guidance at each stage of the agentic pipeline. \modelour achieved an average VQA score of 0.884 on GenAIBench, significantly outperforming open-source, proprietary models and agentic methods. We will publicly release our code upon acceptance.

</details>


### [346] [Understanding the Reversal Curse Mitigation in Masked Diffusion Models through Attention and Training Dynamics](https://arxiv.org/abs/2602.02133)
*Sangwoo Shin,BumJun Kim,Kyelim Lee,Moongyu Jeon,Albert No*

Main category: cs.AI

TL;DR: 扩散语言模型通过架构结构和训练交互部分缓解了自回归模型中的逆转诅咒问题


<details>
  <summary>Details</summary>
Motivation: 自回归语言模型存在逆转诅咒问题：学习"A是B"后，无法处理反向查询"B是A"。虽然掩码扩散语言模型对此问题有所缓解，但根本原因尚不清楚。本文旨在探究扩散模型为何能部分克服这一失败模式。

Method: 通过理论分析和实验验证：1）在单层Transformer编码器中，权重共享使前向和反向注意力分数正相关；2）相应的梯度是对齐的，最小化前向损失也减少反向损失；3）在受控玩具任务和大规模扩散语言模型上进行实验验证

Result: 扩散语言模型对逆转诅咒的缓解源于架构结构及其与训练的交互作用，而非仅因任意顺序训练目标。权重共享使两个方向的注意力分数正相关，梯度对齐确保前向训练也优化反向性能。

Conclusion: 扩散语言模型通过架构设计（权重共享）和训练动态（梯度对齐）部分克服了自回归语言模型中的逆转诅咒问题，这解释了为什么MDMs能在ARMs持续存在此问题时表现更好。

Abstract: Autoregressive language models (ARMs) suffer from the reversal curse: after learning that "$A$ is $B$", they often fail on the reverse query "$B$ is $A$". Masked diffusion-based language models (MDMs) exhibit this failure in a much weaker form, but the underlying reason has remained unclear. A common explanation attributes this mitigation to the any-order training objective. However, observing "[MASK] is $B$" during training does not necessarily teach the model to handle the reverse prompt "$B$ is [MASK]". We show that the mitigation arises from architectural structure and its interaction with training. In a one-layer Transformer encoder, weight sharing couples the two directions by making forward and reverse attention scores positively correlated. In the same setting, we further show that the corresponding gradients are aligned, so minimizing the forward loss also reduces the reverse loss. Experiments on both controlled toy tasks and large-scale diffusion language models support these mechanisms, explaining why MDMs partially overcome a failure mode that persists in strong ARMs.

</details>


### [347] [Mitigating Safety Tax via Distribution-Grounded Refinement in Large Reasoning Models](https://arxiv.org/abs/2602.02136)
*Yingsha Xie,Tiansheng Huang,Enneng Yang,Rui Min,Wenjie Lu,Xiaochun Cao,Naiqiang Tan,Li Shen*

Main category: cs.AI

TL;DR: DGR方法通过将外部安全推理数据集转化为与目标大语言模型内部分布对齐的数据，有效减少安全对齐带来的推理能力退化，同时保持安全性能。


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐数据集通常从外部大语言模型或人工标注中蒸馏得到，与需要对齐的目标模型存在分布差异，这种分布差异被认为是导致目标模型推理能力显著下降的主要原因。

Method: 提出DGR方法，将现有的分布外安全推理数据集进行转化和精炼，使其与目标大语言模型的内部分布对齐，从而减少分布差异带来的负面影响。

Result: DGR在保持安全性能的同时，有效减轻了安全对齐带来的推理能力退化，在DirectRefusal上平均推理准确率提升30.2%，在R1-ACT上提升21.2%。研究发现推理能力退化程度与分布偏移程度相关，且仅需10个样本就能激活有效的拒绝行为。

Conclusion: 分布一致性对于保持大语言模型的安全对齐后的推理能力至关重要，安全对齐可能主要作为激活潜在知识的机制，少量样本就能有效激活安全拒绝行为。

Abstract: Safety alignment incurs safety tax that perturbs a large reasoning model's (LRM) general reasoning ability. Existing datasets used for safety alignment for an LRM are usually constructed by distilling safety reasoning traces and answers from an external LRM or human labeler. However, such reasoning traces and answers exhibit a distributional gap with the target LRM that needs alignment, and we conjecture such distributional gap is the culprit leading to significant degradation of reasoning ability of the target LRM. Driven by this hypothesis, we propose a safety alignment dataset construction method, dubbed DGR. DGR transforms and refines an existing out-of-distributional safety reasoning dataset to be aligned with the target's LLM inner distribution. Experimental results demonstrate that i) DGR effectively mitigates the safety tax while maintaining safety performance across all baselines, i.e., achieving \textbf{+30.2\%} on DirectRefusal and \textbf{+21.2\%} on R1-ACT improvement in average reasoning accuracy compared to Vanilla SFT; ii) the degree of reasoning degradation correlates with the extent of distribution shift, suggesting that bridging this gap is central to preserving capabilities. Furthermore, we find that safety alignment in LRMs may primarily function as a mechanism to activate latent knowledge, as a mere \textbf{10} samples are sufficient for activating effective refusal behaviors. These findings not only emphasize the importance of distributional consistency but also provide insights into the activation mechanism of safety in reasoning models.

</details>


### [348] [Traffic-Aware Navigation in Road Networks](https://arxiv.org/abs/2602.02158)
*Sarah Nassar*

Main category: cs.AI

TL;DR: 比较三种图搜索算法在金斯顿道路网络交通感知导航中的表现：Floyd-Warshall-Ingerman（单次多查询预处理）、Dijkstra/A*（连续单查询实时搜索）、以及结合两者的Yen算法（先找K条最短路径再实时迭代）。


<details>
  <summary>Details</summary>
Motivation: 研究不同图搜索算法在真实城市道路网络交通感知导航任务中的性能表现，为特定部署场景选择最佳算法提供依据。

Method: 在金斯顿道路网络上比较三种方法：1) Floyd-Warshall-Ingerman算法（单次运行多查询预处理）；2) Dijkstra和A*算法（连续单查询实时搜索）；3) Yen算法（先找到前K条最短路径，然后实时迭代）。

Result: Dijkstra和A*算法能提供最交通感知的最优解且预处理需求最小；Floyd-Warshall-Ingerman实时速度最快但只提供基于距离的路径而无交通感知；Yen算法需要大量预处理但在运行速度和最优性之间取得平衡。

Conclusion: 每种方法都有其优缺点，需要根据具体部署场景的实际情况权衡选择最佳定制解决方案。

Abstract: This project compares three graph search approaches for the task of traffic-aware navigation in Kingston's road network. These approaches include a single-run multi-query preprocessing algorithm (Floyd-Warshall-Ingerman), continuous single-query real-time search (Dijkstra's and A*), and an algorithm combining both approaches to balance between their trade-offs by first finding the top K shortest paths then iterating over them in real time (Yen's). Dijkstra's and A* resulted in the most traffic-aware optimal solutions with minimal preprocessing required. Floyd-Warshall-Ingerman was the fastest in real time but provided distance based paths with no traffic awareness. Yen's algorithm required significant preprocessing but balanced between the other two approaches in terms of runtime speed and optimality. Each approach presents advantages and disadvantages that need to be weighed depending on the circumstances of specific deployment contexts to select the best custom solution. *This project was completed as part of ELEC 844 (Search and Planning Algorithms for Robotics) in the Fall 2025 term.

</details>


### [349] [Reasoning in a Combinatorial and Constrained World: Benchmarking LLMs on Natural-Language Combinatorial Optimization](https://arxiv.org/abs/2602.02188)
*Xia Jiang,Jing Chen,Cong Zhang,Jie Gao,Chengpeng Hu,Chenhao Zhang,Yaoxin Wu,Yingqian Zhang*

Main category: cs.AI

TL;DR: NLCO是一个评估大语言模型在组合优化问题上的自然语言推理能力的基准测试，涵盖43个问题，使用四层分类法组织，实验显示模型在小实例上表现良好但随着规模增大性能下降。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在数学和逻辑推理方面表现出色，但它们在组合优化（在高维解空间中搜索满足硬约束的解）方面的能力尚未得到充分探索。需要建立一个基准来评估LLMs在端到端组合优化推理上的表现。

Method: 引入NLCO基准测试，包含43个组合优化问题，采用四层分类法组织（变量类型、约束族、全局模式、目标类别）。提供求解器标注的解决方案，从可行性、解的最优性和推理效率三个维度全面评估LLMs。

Result: 实验表明，高性能模型在小实例上表现出良好的可行性和解质量，但随着实例规模增大，两者都会下降，即使使用更多token进行推理也是如此。同时观察到分类法中的系统性效应：基于集合的任务相对容易，而图结构问题和瓶颈目标则导致更多失败。

Conclusion: NLCO基准填补了评估大语言模型在组合优化推理能力方面的空白，揭示了当前模型在复杂组合优化问题上的局限性，特别是在处理大规模实例和特定问题类型时的挑战。

Abstract: While large language models (LLMs) have shown strong performance in math and logic reasoning, their ability to handle combinatorial optimization (CO) -- searching high-dimensional solution spaces under hard constraints -- remains underexplored. To bridge the gap, we introduce NLCO, a \textbf{N}atural \textbf{L}anguage \textbf{C}ombinatorial \textbf{O}ptimization benchmark that evaluates LLMs on end-to-end CO reasoning: given a language-described decision-making scenario, the model must output a discrete solution without writing code or calling external solvers. NLCO covers 43 CO problems and is organized using a four-layer taxonomy of variable types, constraint families, global patterns, and objective classes, enabling fine-grained evaluation. We provide solver-annotated solutions and comprehensively evaluate LLMs by feasibility, solution optimality, and reasoning efficiency. Experiments across a wide range of modern LLMs show that high-performing models achieve strong feasibility and solution quality on small instances, but both degrade as instance size grows, even if more tokens are used for reasoning. We also observe systematic effects across the taxonomy: set-based tasks are relatively easy, whereas graph-structured problems and bottleneck objectives lead to more frequent failures.

</details>


### [350] [TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents](https://arxiv.org/abs/2602.02196)
*Hang Yan,Xinyu Che,Fangzhi Xu,Qiushi Sun,Zichen Ding,Kanzhi Cheng,Jian Zhang,Tao Qin,Jun Liu,Qika Lin*

Main category: cs.AI

TL;DR: 本文提出了TIDE框架，用于诊断和评估LLM智能体在测试时改进（TTI）中的表现，通过三个维度分析任务完成效率、循环行为和内存负担。


<details>
  <summary>Details</summary>
Motivation: 现有研究对LLM智能体通过与环境迭代交互实现性能提升（TTI）的机制理解不足，现有评估指标无法捕捉任务优化效率、错误行为后的适应能力以及工作记忆的具体效用。

Method: 提出TIDE（Test-time Improvement Diagnostic Evaluation）框架，这是一个与智能体和环境无关的评估框架，将TTI分解为三个相互关联的维度：任务完成的整体时间动态、循环行为的约束程度、以及累积内存的负担。

Result: 通过在不同智能体和环境中的广泛实验，TIDE揭示了提升智能体性能不仅需要扩展内部推理能力，更需要显式优化智能体与环境之间的交互动态。

Conclusion: TIDE框架为理解LLM智能体测试时改进的成功与失败机制提供了系统化的诊断工具，指出了优化智能体-环境交互动态的重要性。

Abstract: Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment.

</details>


### [351] [More Than a Quick Glance: Overcoming the Greedy Bias in KV-Cache Compression](https://arxiv.org/abs/2602.02199)
*Aryan Sood,Tanvi Sharma,Vansh Agrawal*

Main category: cs.AI

TL;DR: LASER-KV是一种新的KV缓存压缩框架，通过分层累积选择和精确LSH召回机制，在严格累积预算策略下实现高效压缩，相比现有方法在长上下文任务中性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型理论上支持长上下文窗口，但实际部署受到KV缓存内存线性增长的制约。现有压缩方法通过剪枝机制在语义召回和内存效率之间进行权衡，导致性能下降。

Method: 提出LASER-KV框架，采用分层累积选择与精确LSH召回。不同于固定摘要大小方法，使用块状累积策略，通过保护除数(n)控制压缩，避免滑动窗口伪影，分离压缩效果。

Result: 在Babilong基准测试中，现有压缩方法性能下降15-30%，而LASER-KV保持稳定性能，在128k上下文长度下准确率提升高达10%。

Conclusion: 研究结果表明仅依赖注意力分数作为token效用的代理是不够的，LASER-KV框架挑战了这一普遍假设，为KV压缩提供了新思路。

Abstract: While Large Language Models (LLMs) can theoretically support extensive context windows, their actual deployment is constrained by the linear growth of Key-Value (KV) cache memory. Prevailing compression strategies mitigate this through various pruning mechanisms, yet trade-off semantic recall for memory efficiency. In this work, we present LASER-KV (Layer Accumulated Selection with Exact-LSH Recall), a framework designed to test the limits of KV compression under a strict accumulative budgeting policy. We deviate from the standard fixed summary size approach by implementing a block-wise accumulation strategy governed by a protection divisor (n). This allows us to isolate the effects of compression from sliding window artifacts. Our experiments on the Babilong benchmark reveal performance degradation in previous compression methods by 15-30% on various long context tasks. LASER-KV maintains stable performance, achieving superior accuracies by a margin of upto 10% at 128k. These findings challenge the prevailing assumption that attention scores alone are a sufficient proxy for token utility.

</details>


### [352] [Position: Explaining Behavioral Shifts in Large Language Models Requires a Comparative Approach](https://arxiv.org/abs/2602.02304)
*Martino Ciaperoni,Marzio Di Vece,Luca Pappalardo,Fosca Giannotti,Francesco Giannini*

Main category: cs.AI

TL;DR: 提出比较性可解释AI框架（Δ-XAI），用于解释基础模型在干预（如缩放、微调、强化学习等）后产生的行为变化，强调需要比较参考模型和干预模型之间的差异而非孤立分析单个模型。


<details>
  <summary>Details</summary>
Motivation: 大规模基础模型在干预后会出现行为变化，但现有可解释AI方法主要关注单个检查点的失败分析，无法解释不同检查点之间的内部变化，需要新的框架来比较干预前后的模型差异。

Method: 提出比较性可解释AI框架（Δ-XAI），制定了一套设计解释方法时应考虑的要求，介绍可能的分析流程，并通过具体实验展示Δ-XAI方法的应用。

Result: 建立了Δ-XAI框架，明确了比较性解释的核心原则，提供了具体的方法流程和实验示例，为解释模型行为变化提供了系统化方法。

Conclusion: 行为变化应该通过比较性方法来解释，Δ-XAI框架为解决基础模型干预后的行为变化解释问题提供了理论基础和实践指导，填补了现有可解释AI方法的空白。

Abstract: Large-scale foundation models exhibit behavioral shifts: intervention-induced behavioral changes that appear after scaling, fine-tuning, reinforcement learning or in-context learning. While investigating these phenomena have recently received attention, explaining their appearance is still overlooked. Classic explainable AI (XAI) methods can surface failures at a single checkpoint of a model, but they are structurally ill-suited to justify what changed internally across different checkpoints and which explanatory claims are warranted about that change. We take the position that behavioral shifts should be explained comparatively: the core target should be the intervention-induced shift between a reference model and an intervened model, rather than any single model in isolation. To this aim we formulate a Comparative XAI ($Δ$-XAI) framework with a set of desiderata to be taken into account when designing proper explaining methods. To highlight how $Δ$-XAI methods work, we introduce a set of possible pipelines, relate them to the desiderata, and provide a concrete $Δ$-XAI experiment.

</details>


### [353] [Interpreting and Controlling LLM Reasoning through Integrated Policy Gradient](https://arxiv.org/abs/2602.02313)
*Changming Li,Kaixing Zhang,Haoyun Xu,Yingdong Shi,Zheng Zhang,Kaitao Song,Kan Ren*

Main category: cs.AI

TL;DR: IPG框架通过传播基于结果的信号来定位LLM中复杂推理行为的内部机制，实现更精确的定位和可靠的行为调控。


<details>
  <summary>Details</summary>
Motivation: 当前解释性方法难以精确定位复杂推理机制或捕捉从模型内部工作到推理输出的顺序影响，需要新的方法来理解LLM的推理行为内部机制。

Method: 提出集成策略梯度（IPG）框架，通过将基于结果的信号（如推理后准确率）向后传播通过模型推理轨迹，将推理行为归因于模型内部组件。

Result: 实证评估表明，该方法实现了更精确的定位，并能在不同推理模型中可靠地调控推理行为（如推理能力、推理强度）。

Conclusion: IPG框架基于结果导向和顺序影响感知原则，能够识别对推理行为有顺序贡献的组件，为理解LLM复杂推理机制提供了有效工具。

Abstract: Large language models (LLMs) demonstrate strong reasoning abilities in solving complex real-world problems. Yet, the internal mechanisms driving these complex reasoning behaviors remain opaque. Existing interpretability approaches targeting reasoning either identify components (e.g., neurons) correlated with special textual patterns, or rely on human-annotated contrastive pairs to derive control vectors. Consequently, current methods struggle to precisely localize complex reasoning mechanisms or capture sequential influence from model internal workings to the reasoning outputs. In this paper, built on outcome-oriented and sequential-influence-aware principles, we focus on identifying components that have sequential contribution to reasoning behavior where outcomes are cumulated by long-range effects. We propose Integrated Policy Gradient (IPG), a novel framework that attributes reasoning behaviors to model's inner components by propagating compound outcome-based signals such as post reasoning accuracy backward through model inference trajectories. Empirical evaluations demonstrate that our approach achieves more precise localization and enables reliable modulation of reasoning behaviors (e.g., reasoning capability, reasoning strength) across diverse reasoning models.

</details>


### [354] [Context Learning for Multi-Agent Discussion](https://arxiv.org/abs/2602.02350)
*Xingyuan Hua,Sheng Yue,Xinyi Li,Yizhe Zhao,Jinrui Zhang,Ju Ren*

Main category: cs.AI

TL;DR: M2CL提出了一种多智能体上下文学习方法，通过训练上下文生成器动态生成每轮讨论的指令，解决多智能体讨论中的不一致性问题，显著提升性能20%-50%。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体讨论方法容易遭受讨论不一致性问题，由于各个智能体上下文之间的不对齐，导致LLM无法达成一致的解决方案。

Method: M2CL为每个智能体学习一个上下文生成器，通过自动信息组织和精炼，动态生成每轮讨论的上下文指令。采用精心设计的自适应机制来控制上下文一致性和输出差异。

Result: 在学术推理、具身任务和移动控制等挑战性任务上，M2CL性能显著超越现有方法20%-50%，同时具有良好的可迁移性和计算效率。

Conclusion: M2CL通过上下文生成器有效解决多智能体讨论中的不一致性问题，使LLM能够避免过早收敛于多数噪声，逐步达成正确共识。

Abstract: Multi-Agent Discussion (MAD) has garnered increasing attention very recently, where multiple LLM instances collaboratively solve problems via structured discussion. However, we find that current MAD methods easily suffer from discussion inconsistency, LLMs fail to reach a coherent solution, due to the misalignment between their individual contexts.In this paper, we introduce a multi-LLM context learning method (M2CL) that learns a context generator for each agent, capable of dynamically generating context instructions per discussion round via automatic information organization and refinement. Specifically, inspired by our theoretical insights on the context instruction, M2CL train the generators to control context coherence and output discrepancies via a carefully crafted self-adaptive mechanism.It enables LLMs to avoid premature convergence on majority noise and progressively reach the correct consensus. We evaluate M2CL on challenging tasks, including academic reasoning, embodied tasks, and mobile control. The results show that the performance of M2CL significantly surpasses existing methods by 20%--50%, while enjoying favorable transferability and computational efficiency.

</details>


### [355] [Live-Evo: Online Evolution of Agentic Memory from Continuous Feedback](https://arxiv.org/abs/2602.02369)
*Yaolun Zhang,Yiran Wu,Yijiong Yu,Qingyun Wu,Huazheng Wang*

Main category: cs.AI

TL;DR: Live-Evo是一个在线自演化记忆系统，通过经验银行和元指导银行分离"发生了什么"和"如何使用"，在持续数据流中动态更新记忆权重，提升LLM智能体在分布变化下的任务解决能力。


<details>
  <summary>Details</summary>
Motivation: 现有自演化系统主要针对静态训练/测试集设计，通过折叠静态基准来近似在线学习，在真实分布变化和持续反馈下表现脆弱。需要真正的在线自演化记忆系统来处理持续数据流。

Method: Live-Evo采用经验银行和元指导银行的双层架构，将经验存储与使用指导分离。系统维护经验权重，根据反馈动态更新：有帮助的经验被强化并更频繁检索，误导或过时的经验被降权并逐渐遗忘，类似人类记忆的强化和衰减机制。

Result: 在10周时间跨度的Prophet Arena基准测试中，Live-Evo将Brier分数提高了20.8%，市场回报增加了12.9%。在深度研究基准测试中也表现出优于强基线的持续增益。

Conclusion: Live-Evo展示了在线自演化记忆系统在真实分布变化环境中的有效性，通过动态权重管理和经验-指导分离机制，显著提升了LLM智能体在持续学习场景下的性能。

Abstract: Large language model (LLM) agents are increasingly equipped with memory, which are stored experience and reusable guidance that can improve task-solving performance. Recent \emph{self-evolving} systems update memory based on interaction outcomes, but most existing evolution pipelines are developed for static train/test splits and only approximate online learning by folding static benchmarks, making them brittle under true distribution shift and continuous feedback. We introduce \textsc{Live-Evo}, an online self-evolving memory system that learns from a stream of incoming data over time. \textsc{Live-Evo} decouples \emph{what happened} from \emph{how to use it} via an Experience Bank and a Meta-Guideline Bank, compiling task-adaptive guidelines from retrieved experiences for each task. To manage memory online, \textsc{Live-Evo} maintains experience weights and updates them from feedback: experiences that consistently help are reinforced and retrieved more often, while misleading or stale experiences are down-weighted and gradually forgotten, analogous to reinforcement and decay in human memory. On the live \textit{Prophet Arena} benchmark over a 10-week horizon, \textsc{Live-Evo} improves Brier score by 20.8\% and increases market returns by 12.9\%, while also transferring to deep-research benchmarks with consistent gains over strong baselines. Our code is available at https://github.com/ag2ai/Live-Evo.

</details>


### [356] [Trust by Design: Skill Profiles for Transparent, Cost-Aware LLM Routing](https://arxiv.org/abs/2602.02386)
*Mika Okamoto,Ansel Kaplan Erol,Glenn Matlin*

Main category: cs.AI

TL;DR: BELLA是一个预算高效的LLM选择框架，通过自动化技能分析推荐最优模型，在保证性能的同时控制成本。


<details>
  <summary>Details</summary>
Motivation: 当前标准基准测试报告的是聚合指标，掩盖了任务所需的具体能力，无法判断更便宜的模型是否足够。LLM从业者需要在不浪费资金的情况下为任务选择合适的模型。

Method: BELLA采用三阶段方法：1) 通过基于批评的分析分解LLM输出并提取所需细粒度技能；2) 将技能聚类为结构化能力矩阵；3) 使用多目标优化选择模型，在预算约束下最大化性能。

Result: BELLA提供自然语言推理的推荐，提供当前黑盒路由系统缺乏的透明度。该框架使从业者能够为部署LLM做出原则性的成本-性能权衡。

Conclusion: BELLA框架通过可解释的基于技能的分析，为LLM选择提供了预算高效的解决方案，特别适用于金融推理等具有多样化技能需求和模型成本变化的领域。

Abstract: How should Large Language Model (LLM) practitioners select the right model for a task without wasting money? We introduce BELLA (Budget-Efficient LLM Selection via Automated skill-profiling), a framework that recommends optimal LLM selection for tasks through interpretable skill-based model selection. Standard benchmarks report aggregate metrics that obscure which specific capabilities a task requires and whether a cheaper model could suffice. BELLA addresses this gap through three stages: (1) decomposing LLM outputs and extract the granular skills required by using critic-based profiling, (2) clustering skills into structured capability matrices, and (3) multi-objective optimization to select the right models to maximize performance while respecting budget constraints. BELLA provides natural-language rationale for recommendations, providing transparency that current black-box routing systems lack. We describe the framework architecture, situate it within the landscape of LLM routing and evaluation, and discuss its application to financial reasoning as a representative domain exhibiting diverse skill requirements and cost-variation across models. Our framework enables practitioners to make principled and cost-performance trade-offs for deploying LLMs.

</details>


### [357] [Structure Enables Effective Self-Localization of Errors in LLMs](https://arxiv.org/abs/2602.02416)
*Ankur Samanta,Akshayaa Magesh,Ayush Jain,Kavosh Asadi,Youliang Yu,Daniel Jiang,Boris Vidolov,Kaveh Hassani,Paul Sajda,Jalaj Bhandari,Yonathan Efroni*

Main category: cs.AI

TL;DR: 论文提出Thought-ICS框架，通过离散化思维步骤实现语言模型的错误定位与自我纠正，相比传统方法显著提升纠正能力


<details>
  <summary>Details</summary>
Motivation: 语言模型的自我纠正能力仍然不足，需要探索模型能否像人类大脑一样在离散决策点监控错误并重新采样替代方案，从而构建有效的自我纠正AI系统

Method: 提出Thought-ICS（迭代纠正思维采样）框架：1）将推理结构化为离散、语义连贯的思维步骤；2）每次生成一个完整的离散思维；3）验证后定位第一个错误步骤；4）回溯到最后一个正确点生成替代推理

Result: 1）在传统非结构化思维链中模型无法可靠定位错误，而在结构化方法中能可靠定位；2）在有外部验证时，Thought-ICS实现20-40%的自我纠正提升；3）在完全自主无外部验证设置中，优于当代自我纠正基线方法

Conclusion: 通过将推理结构化为离散思维步骤，语言模型能够有效定位错误并进行自我纠正，Thought-ICS框架为构建能够自我纠正的AI系统提供了可行路径

Abstract: Self-correction in language models remains elusive. In this work, we explore whether language models can explicitly localize errors in incorrect reasoning, as a path toward building AI systems that can effectively correct themselves. We introduce a prompting method that structures reasoning as discrete, semantically coherent thought steps, and show that models are able to reliably localize errors within this structure, while failing to do so in conventional, unstructured chain-of-thought reasoning. Motivated by how the human brain monitors errors at discrete decision points and resamples alternatives, we introduce Iterative Correction Sampling of Thoughts (Thought-ICS), a self-correction framework. Thought-ICS iteratively prompts the model to generate reasoning one discrete and complete thought at a time--where each thought represents a deliberate decision by the model--creating natural boundaries for precise error localization. Upon verification, the model localizes the first erroneous step, and the system backtracks to generate alternative reasoning from the last correct point. When asked to correct reasoning verified as incorrect by an oracle, Thought-ICS achieves 20-40% self-correction lift. In a completely autonomous setting without external verification, it outperforms contemporary self-correction baselines.

</details>


### [358] [SafeGround: Know When to Trust GUI Grounding Models via Uncertainty Calibration](https://arxiv.org/abs/2602.02419)
*Qingni Wang,Yue Fan,Xin Eric Wang*

Main category: cs.AI

TL;DR: SafeGround是一个不确定性感知的GUI grounding框架，通过分布感知的不确定性量化方法和校准过程，实现统计保证的FDR控制，提高GUI交互的可靠性和系统级准确性。


<details>
  <summary>Details</summary>
Motivation: GUI grounding中错误的坐标预测可能导致难以逆转的高成本操作（如错误支付批准），现有模型缺乏可靠性保证，需要风险感知的预测框架。

Method: 1. 使用分布感知的不确定性量化方法捕捉模型输出的空间分散性；2. 通过校准过程推导具有统计保证的FDR控制的测试时决策阈值。

Result: 在ScreenSpot-Pro基准测试中，不确定性度量优于现有基线，校准阈值实现严格的风险控制，系统级准确性相比Gemini-only推理最高提升5.38个百分点。

Conclusion: SafeGround为GUI grounding提供了可靠的不确定性感知框架，通过统计保证的风险控制显著提高了模型在实际应用中的可靠性和准确性。

Abstract: Graphical User Interface (GUI) grounding aims to translate natural language instructions into executable screen coordinates, enabling automated GUI interaction. Nevertheless, incorrect grounding can result in costly, hard-to-reverse actions (e.g., erroneous payment approvals), raising concerns about model reliability. In this paper, we introduce SafeGround, an uncertainty-aware framework for GUI grounding models that enables risk-aware predictions through calibrations before testing. SafeGround leverages a distribution-aware uncertainty quantification method to capture the spatial dispersion of stochastic samples from outputs of any given model. Then, through the calibration process, SafeGround derives a test-time decision threshold with statistically guaranteed false discovery rate (FDR) control. We apply SafeGround on multiple GUI grounding models for the challenging ScreenSpot-Pro benchmark. Experimental results show that our uncertainty measure consistently outperforms existing baselines in distinguishing correct from incorrect predictions, while the calibrated threshold reliably enables rigorous risk control and potentials of substantial system-level accuracy improvements. Across multiple GUI grounding models, SafeGround improves system-level accuracy by up to 5.38\% percentage points over Gemini-only inference.

</details>


### [359] [Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling](https://arxiv.org/abs/2602.02453)
*Andong Chen,Wenxin Zhu,Qiuyu Ding,Yuchen Song,Muyun Yang,Tiejun Zhao*

Main category: cs.AI

TL;DR: 提出"Thinking with Comics"方法，用漫画作为介于图像和视频之间的高信息密度视觉推理媒介，在保持时间结构和叙事连贯性的同时降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有模态推理存在局限：静态图像难以表示时间结构，而视频则引入大量冗余和计算成本。需要一种既能保留时间信息又高效的视觉推理媒介。

Method: 提出"Thinking with Comics"范式，将漫画作为视觉推理媒介，系统研究基于漫画的两种推理路径，并在多种推理任务和长上下文理解任务上进行评估。

Result: 实验结果显示：在多层次时间和因果推理任务上，Thinking with Comics优于Thinking with Images，同时比Thinking with Video显著更高效。不同漫画叙事结构和风格对任务性能有持续影响。

Conclusion: 漫画作为中间视觉表示能有效改善多模态推理，平衡了信息密度、时间结构和计算效率，为视觉推理提供了新范式。

Abstract: Chain-of-Thought reasoning has driven large language models to extend from thinking with text to thinking with images and videos. However, different modalities still have clear limitations: static images struggle to represent temporal structure, while videos introduce substantial redundancy and computational cost. In this work, we propose Thinking with Comics, a visual reasoning paradigm that uses comics as a high information-density medium positioned between images and videos. Comics preserve temporal structure, embedded text, and narrative coherence while requiring significantly lower reasoning cost. We systematically study two reasoning paths based on comics and evaluate them on a range of reasoning tasks and long-context understanding tasks. Experimental results show that Thinking with Comics outperforms Thinking with Images on multi-step temporal and causal reasoning tasks, while remaining substantially more efficient than Thinking with Video. Further analysis indicates that different comic narrative structures and styles consistently affect performance across tasks, suggesting that comics serve as an effective intermediate visual representation for improving multimodal reasoning.

</details>


### [360] [Drift-Bench: Diagnosing Cooperative Breakdowns in LLM Agents under Input Faults via Multi-Turn Interaction](https://arxiv.org/abs/2602.02455)
*Han Bao,Zheyuan Zhang,Pengcheng Jing,Zhengqing Yuan,Kaiwen Shi,Yanfang Ye*

Main category: cs.AI

TL;DR: Drift-Bench是首个评估自主智能体在输入故障下多轮澄清能力的诊断基准，通过状态导向和服务导向执行环境测试代理语用学，揭示现有模型在合作假设被违反时的性能下降。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型向自主智能体过渡时，用户输入经常违反合作假设（如隐含意图、缺失参数、错误预设或模糊表达），产生文本评估无法捕捉的执行风险。现有基准通常假设指令明确或仅限于文本单轮澄清，无法衡量在接地执行风险下的多轮消歧。

Method: 基于经典沟通理论，Drift-Bench提供统一的合作故障分类法，采用角色驱动的用户模拟器和Rise评估协议，在状态导向和服务导向执行环境中进行多轮澄清评估。

Result: 实验显示在这些故障下模型性能显著下降，澄清效果因用户角色和故障类型而异。该方法连接了澄清研究和智能体安全评估。

Conclusion: Drift-Bench能够系统诊断可能导致不安全执行的故障，为智能体安全评估提供了新的诊断工具，填补了现有基准在多轮消歧和接地执行风险评估方面的空白。

Abstract: As Large Language Models transition to autonomous agents, user inputs frequently violate cooperative assumptions (e.g., implicit intent, missing parameters, false presuppositions, or ambiguous expressions), creating execution risks that text-only evaluations do not capture. Existing benchmarks typically assume well-specified instructions or restrict evaluation to text-only, single-turn clarification, and thus do not measure multi-turn disambiguation under grounded execution risk. We introduce \textbf{Drift-Bench}, the first diagnostic benchmark that evaluates agentic pragmatics under input faults through multi-turn clarification across state-oriented and service-oriented execution environments. Grounded in classical theories of communication, \textbf{Drift-Bench} provides a unified taxonomy of cooperative breakdowns and employs a persona-driven user simulator with the \textbf{Rise} evaluation protocol. Experiments show substantial performance drops under these faults, with clarification effectiveness varying across user personas and fault types. \MethodName bridges clarification research and agent safety evaluation, enabling systematic diagnosis of failures that can lead to unsafe executions.

</details>


### [361] [MentisOculi: Revealing the Limits of Reasoning with Mental Imagery](https://arxiv.org/abs/2602.02465)
*Jana Zeller,Thaddäus Wiedemer,Fanfei Li,Thomas Klein,Prasanna Mayilvahanan,Matthias Bethge,Felix Wichmann,Ryan Cotterell,Wieland Brendel*

Main category: cs.AI

TL;DR: 评估前沿统一多模态模型视觉推理能力的研究发现，尽管模型能生成正确视觉内容，但视觉思维目前无法提升模型推理性能


<details>
  <summary>Details</summary>
Motivation: 研究多模态模型从仅处理视觉信息向原生交错生成的转变，探索视觉化作为推理辅助工具的潜力，类似于人类的心理意象

Method: 开发MentisOculi评估套件，包含分层多步推理问题，测试从潜在token到显式生成图像等多种视觉策略

Result: 视觉策略普遍无法提升性能，统一多模态模型存在关键限制：虽有文本推理能力，但受生成错误累积影响，即使使用真实视觉化也无法有效利用

Conclusion: 尽管视觉思维具有内在吸引力，但目前尚无法提升模型推理能力，MentisOculi为分析和弥合这一差距奠定了基础

Abstract: Frontier models are transitioning from multimodal large language models (MLLMs) that merely ingest visual information to unified multimodal models (UMMs) capable of native interleaved generation. This shift has sparked interest in using intermediate visualizations as a reasoning aid, akin to human mental imagery. Central to this idea is the ability to form, maintain, and manipulate visual representations in a goal-oriented manner. To evaluate and probe this capability, we develop MentisOculi, a procedural, stratified suite of multi-step reasoning problems amenable to visual solution, tuned to challenge frontier models. Evaluating visual strategies ranging from latent tokens to explicit generated imagery, we find they generally fail to improve performance. Analysis of UMMs specifically exposes a critical limitation: While they possess the textual reasoning capacity to solve a task and can sometimes generate correct visuals, they suffer from compounding generation errors and fail to leverage even ground-truth visualizations. Our findings suggest that despite their inherent appeal, visual thoughts do not yet benefit model reasoning. MentisOculi establishes the necessary foundation to analyze and close this gap across diverse model families.

</details>


### [362] [Avenir-Web: Human-Experience-Imitating Multimodal Web Agents with Mixture of Grounding Experts](https://arxiv.org/abs/2602.02468)
*Aiden Yiliu Li,Xinyue Hao,Shilong Liu,Mengdi Wang*

Main category: cs.AI

TL;DR: Avenir-Web：一种新型网页代理，通过混合定位专家、经验模仿规划和任务追踪检查表等技术，在真实网页交互中实现新的开源SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型有所进展，但自主网页代理在执行复杂动态网页界面的长时程任务时仍不可靠，存在元素定位不准确、缺乏站点特定程序知识、长期任务追踪和记忆不稳定等问题。

Method: 1. 混合定位专家（Mixture of Grounding Experts）提高元素定位精度；2. 经验模仿规划（Experience-Imitation Planning）融入程序先验知识；3. 任务追踪检查表结合自适应内存实现鲁棒交互。

Result: 在Online-Mind2Web基准测试中，Avenir-Web显著超越先前开源代理，达到与顶级专有模型相当的性能，建立了真实网页交互的新开源SOTA。

Conclusion: Avenir-Web通过创新的定位、规划和记忆机制，解决了网页代理的关键挑战，为可靠网页交互建立了新的开源标准。

Abstract: Despite advances in multimodal large language models, autonomous web agents still struggle to reliably execute long-horizon tasks on complex and dynamic web interfaces. Existing agents often suffer from inaccurate element grounding, the absence of site-specific procedural knowledge, and unstable long-term task tracking and memory, particularly when operating over complex Document Object Model structures. To address these limitations, we introduce Avenir-Web, a web agent that achieves a new open-source state of the art on the Online-Mind2Web benchmark in real-world deployment. Avenir-Web leverages a Mixture of Grounding Experts, Experience-Imitation Planning for incorporating procedural priors, and a task-tracking checklist combined with adaptive memory to enable robust and seamless interaction across diverse user interface paradigms. We evaluate Avenir-Web on Online-Mind2Web, a rigorous benchmark of live and user-centered web tasks. Our results demonstrate that Avenir-Web significantly surpasses prior open-source agents and attains performance parity with top-tier proprietary models, thereby establishing a new open-source state of the art for reliable web agents on live websites.

</details>


### [363] [Breaking the Reversal Curse in Autoregressive Language Models via Identity Bridge](https://arxiv.org/abs/2602.02470)
*Xutao Ma,Yixiao Huang,Hanlin Zhu,Somayeh Sojoudi*

Main category: cs.AI

TL;DR: 通过引入"身份桥"正则化数据（如"A→A"），可以显著缓解LLMs的"反转诅咒"问题，使模型能够学习更高级别的规则而非简单记忆事实。


<details>
  <summary>Details</summary>
Motivation: 尽管自回归大语言模型在许多复杂任务上表现出色，但在简单逻辑推理如"反转诅咒"上仍然失败。传统观点认为这是自回归因果LLMs的固有局限，表明模型倾向于记忆事实级知识而非捕捉更高级别的规则。本文挑战这一观点，探索是否可以通过数据层面的简单调整来缓解这一问题。

Method: 提出一种简单的正则化数据配方"身份桥"，形式为"A→A"（例如"爱丽丝的名字是爱丽丝"）。理论上分析梯度下降的隐式偏差，证明即使单层Transformer也能通过这种方法打破反转诅咒。实证上，在1B参数的预训练语言模型上微调，使用提出的数据配方。

Result: 使用身份桥数据配方微调的模型在反转任务上达到40%的成功率，而仅使用前向知识数据训练的模型成功率接近零。这显著缓解了反转诅咒问题。

Conclusion: 本文为反转诅咒提供了新的理论基础，并提出了一种原则性、低成本的路径来鼓励LLMs从数据中学习更高级别的规则，挑战了关于自回归LLMs固有局限的传统观点。

Abstract: Autoregressive large language models (LLMs) have achieved remarkable success in many complex tasks, yet they can still fail in very simple logical reasoning such as the "reversal curse" -- when trained on forward knowledge data of the form "$A \rightarrow B$" (e.g., Alice's husband is Bob), the model is unable to deduce the reversal knowledge "$B \leftarrow A$" (e.g., Bob's wife is Alice) during test. Extensive prior research suggests that this failure is an inherent, fundamental limit of autoregressive causal LLMs, indicating that these models tend to memorize factual-level knowledge rather than capture higher-level rules. In this paper, we challenge this view by showing that this seemingly fundamental limit can be mitigated by slightly tweaking the training data with a simple regularization data recipe called the Identity Bridge of the form "$A \to A$" (e.g., The name of Alice is Alice). Theoretically, we prove that under this recipe, even a one-layer transformer can break the reversal curse by analyzing the implicit bias of gradient descent. Empirically, we show that a 1B pretrained language model finetuned with the proposed data recipe achieves a 40% success rate on reversal tasks, in stark contrast to a near-zero success rate when trained solely on forward-knowledge data. Our work provides a novel theoretical foundation for the reversal curse and offers a principled, low-cost path to encouraging LLMs to learn higher-level rules from data.

</details>


### [364] [AgentRx: Diagnosing AI Agent Failures from Execution Trajectories](https://arxiv.org/abs/2602.02475)
*Shraddha Barke,Arnav Goyal,Alind Khare,Avaljot Singh,Suman Nath,Chetan Bansal*

Main category: cs.AI

TL;DR: 提出AGENTRX框架，用于自动诊断AI代理失败轨迹中的关键失败步骤和类别，并发布包含115个失败轨迹的基准数据集。


<details>
  <summary>Details</summary>
Motivation: AI代理失败难以定位，因为执行具有概率性、长时程、多代理且受噪声工具输出影响。需要系统化的失败诊断方法。

Method: 1) 手动标注失败代理运行，创建包含115个轨迹的基准数据集；2) 提出AGENTRX框架：合成约束条件，逐步评估，生成可审计的验证日志，使用LLM判断器定位关键失败步骤和类别。

Result: AGENTRX在三个领域（结构化API工作流、事件管理、开放式网络/文件任务）中，相比现有基线方法，在步骤定位和失败归因方面表现更优。

Conclusion: AGENTRX提供了一种自动化的、领域无关的诊断框架，能够有效定位AI代理失败的关键步骤和类别，减少人工标注成本。

Abstract: AI agents often fail in ways that are difficult to localize because executions are probabilistic, long-horizon, multi-agent, and mediated by noisy tool outputs. We address this gap by manually annotating failed agent runs and release a novel benchmark of 115 failed trajectories spanning structured API workflows, incident management, and open-ended web/file tasks. Each trajectory is annotated with a critical failure step and a category from a grounded-theory derived, cross-domain failure taxonomy. To mitigate the human cost of failure attribution, we present AGENTRX, an automated domain-agnostic diagnostic framework that pinpoints the critical failure step in a failed agent trajectory. It synthesizes constraints, evaluates them step-by-step, and produces an auditable validation log of constraint violations with associated evidence; an LLM-based judge uses this log to localize the critical step and category. Our framework improves step localization and failure attribution over existing baselines across three domains.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [365] [Coping with Inductive Risk When Theories are Underdetermined: Decision Making with Partial Identification](https://arxiv.org/abs/2602.00355)
*Charles F. Manski*

Main category: econ.EM

TL;DR: 论文探讨了理论的不确定性在科学决策中的重要性，特别是部分识别方法如何帮助处理科学不确定性对公共政策的影响。


<details>
  <summary>Details</summary>
Motivation: 当科学研究用于决策时，科学不确定性会产生归纳风险。哲学界和科学实践界对理论不确定性的重要性存在持续争议，这影响了科学信息在公共决策中的可信度。

Method: 采用计量经济学中的部分识别分析方法，结合数据和可信假设来预测人口结果，为科学不确定性提供数学工具。结合模糊决策标准，提出处理多重经验不确定理论的实用方法。

Result: 部分识别分析显示，理论不确定性和归纳风险对社会重要结果的可信预测具有高度影响，从而影响公共决策的可信度。该方法提供了在不接受多个经验不确定理论中某一特定理论的情况下进行政策选择的连贯方法。

Conclusion: 部分识别研究值得在哲学界关于理论不确定性和归纳风险的讨论中受到关注，它为处理科学不确定性对公共决策的影响提供了实用工具和理论框架。

Abstract: Controversy about the significance of underdetermination of theories persists in the philosophy and conduct of science. The issue has practical import when scientific research is used to inform decision making, because scientific uncertainty yields inductive risk. Seeking to enhance communication between philosophers and researchers who analyze public policy, this paper describes econometric analysis of partial identification. Study of partial identification finds underdetermination and inductive risk to be highly consequential for credible prediction of important societal outcomes and, hence, for credible public decision making. It provides mathematical tools to characterize a broad class of scientific uncertainties that arise when available data and credible assumptions are combined to predict population outcomes. Combining study of partial identification with criteria for reasonable decision making under ambiguity yields coherent practical approaches to make policy choices without accepting one among multiple empirically underdetermined theories. The paper argues that study of partial identification warrants attention in philosophical discourse on underdetermination and inductive risk.

</details>


### [366] [Identification and Estimation in Fuzzy Regression Discontinuity Designs with Covariates](https://arxiv.org/abs/2602.01417)
*Carolina Caetano,Gregorio Caetano,Juan Carlos Escanciano*

Main category: econ.EM

TL;DR: 该论文研究带有协变量的模糊断点回归设计，识别条件局部平均处理效应的加权平均值，提出基于平方第一阶段断点的合规加权LATE估计方法，在模拟和应用中表现优于标准模糊RDD估计器。


<details>
  <summary>Details</summary>
Motivation: 模糊断点回归设计中，当合规性存在异质性时，标准估计方法可能不稳定且效率较低。需要开发能够更好利用协变量信息、提高估计稳定性和精度的新方法。

Method: 提出合规加权局部平均处理效应（CWLATE），通过平方第一阶段断点对协变量单元进行加权，最大化第一阶段强度。对于离散协变量，提供简单估计量和稳健偏差校正推断方法。

Result: 模拟研究表明，当合规性存在变化时，CWLATE相比标准模糊RDD估计器提高了稳定性并降低了均方误差。在乌拉圭孕期现金转移应用中，该方法获得了对低出生体重的精确RDD效应估计。

Conclusion: CWLATE方法在模糊断点回归设计中提供了更稳定和高效的估计，特别适用于合规性存在异质性的情况，为实证研究提供了实用的工具。

Abstract: We study fuzzy regression discontinuity designs with covariates and characterize the weighted averages of conditional local average treatment effects (WLATEs) that are point identified. Any identified WLATE equals a Wald ratio of conditional reduced-form and first-stage discontinuities. We highlight the Compliance-Weighted LATE (CWLATE), which weights cells by squared first-stage discontinuities and maximizes first-stage strength. For discrete covariates, we provide simple estimators and robust bias-corrected inference. In simulations calibrated to common designs, CWLATE improves stability and reduces mean squared error relative to standard fuzzy RDD estimators when compliance varies. An application to Uruguayan cash transfers during pregnancy yields precise RDD-based effects on low birthweight.

</details>


### [367] [Do designated market makers provide liquidity during downward extreme price movements?](https://arxiv.org/abs/2602.01817)
*Mario Bellia,Kim Christensen,Aleksey Kolokolov,Loriana Pelizzon,Roberto Renò*

Main category: econ.EM

TL;DR: DMMs在电子市场中，当单只股票面临抛售压力时提供流动性，但当多只股票同时受影响时则消耗流动性（让更慢的交易者提供流动性）


<details>
  <summary>Details</summary>
Motivation: 研究指定做市商在电子市场中的交易行为，检验他们是在抛售压力下遵守做市协议提供即时性，还是利用私人信息顺风交易获利

Method: 使用包含交易者分类审计追踪信息的独特数据集，在极端（下跌）价格变动期间测试竞争理论，采用新颖方法检测极端价格变动

Result: DMMs在抛售压力集中在单只股票时提供流动性，但当多只股票同时受影响时则消耗流动性（让更慢的交易者承担流动性提供角色）

Conclusion: DMMs的行为取决于市场压力范围：单只股票压力时履行做市职责，系统性压力时则转为顺风交易，将流动性提供责任转移给反应较慢的交易者

Abstract: We study the trading activity of designated market makers (DMMs) in electronic markets using a unique dataset with audit-trail information on trader classification. DMMs may either adhere to their market-making agreements and offer immediacy during periods of heavy selling pressure, or they might lean-with-the-wind to profit from private information. We test these competing theories during extreme (downward) price movements, which we detect using a novel methodology. We show that DMMs provide liquidity when the selling pressure is concentrated on a single stock, but consume liquidity (leaving liquidity provision to slower traders) when several stocks are affected.

</details>


### [368] [Forecasting Oil Consumption: The Statistical Review of World Energy Meets Machine Learning](https://arxiv.org/abs/2602.01963)
*Jan Ditzen,Erkal Ersoy,Haoyang Li,Francesco Ravazzolo*

Main category: econ.EM

TL;DR: 研究少数主导国家是否能解释区域石油需求动态并改善预测性能，识别出美国为全球主导驱动因素，法国和日本分别作为欧洲和亚洲的区域枢纽，包含这些主导因素能显著提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 研究少数主导国家是否能够解释区域石油需求的大部分动态变化，并提高预测性能，特别是在全球波动加剧时期。

Method: 使用高维浓度矩阵，通过LASSO和OCMT两种互补的变量选择方法逐行估计，通过排序浓度矩阵列范数并应用基于连续范数比的标准，结合经济动机限制来识别主导国家。

Result: 美国被识别为全球主导驱动因素，法国和日本分别作为欧洲和亚洲的稳健区域枢纽。将这些主导因素作为所有国家的回归变量，相比自回归基准和国家特定LASSO模型，在统计上显著提高了预测准确性，特别是在全球波动加剧时期。

Conclusion: 少数主导国家确实能够解释区域石油需求动态并改善预测性能，该框架具有灵活性，可应用于其他具有网络结构或空间依赖性的宏观经济和能源变量。

Abstract: This paper studies whether a small set of dominant countries can account for most of the dynamics of regional oil demand and improve forecasting performance. We focus on dominant drivers within the OECD and a broad GVAR sample covering over 90\% of world GDP. Our approach identifies dominant drivers from a high-dimensional concentration matrix estimated row by row using two complementary variable-selection methods, LASSO and the one-covariate-at-a-time multiple testing (OCMT) procedure. Dominant countries are selected by ordering the columns of the concentration matrix by their norms and applying a criterion based on consecutive norm ratios, combined with economically motivated restrictions to rule out pseudo-dominance. The United States emerges as a global dominant driver, while France and Japan act as robust regional hubs representing European and Asian components, respectively. Including these dominant drivers as regressors for all countries yields statistically significant forecast gains over autoregressive benchmarks and country-specific LASSO models, particularly during periods of heightened global volatility. The proposed framework is flexible and can be applied to other macroeconomic and energy variables with network structure or spatial dependence.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [369] [Uncertainty-Aware Multimodal Learning via Conformal Shapley Intervals](https://arxiv.org/abs/2602.00171)
*Mathew Chandy,Michael Johnson,Judong Shen,Devan V. Mehrotra,Hua Zhou,Jin Zhou,Xiaowu Dai*

Main category: stat.ML

TL;DR: 提出conformal Shapley intervals框架，结合Shapley值和conformal inference为多模态学习中的每个模态构建带不确定性的重要性区间，并提供具有最优性保证的模态选择方法


<details>
  <summary>Details</summary>
Motivation: 多模态学习中不同模态的贡献通常不均衡且数据依赖，难以确定哪些模态真正具有信息量以及其贡献的可信度，需要量化模态重要性及其不确定性以实现可解释和可靠的多模态学习

Method: 结合Shapley值和conformal inference构建conformal Shapley intervals，为每个模态提供不确定性感知的重要性区间，并基于这些区间提出具有理论最优性保证的模态选择方法

Result: 在多个数据集上验证了方法的有效性，能够提供有意义的不确定性量化，在仅依赖少量信息模态的情况下实现强大的预测性能

Conclusion: conformal Shapley intervals框架为多模态学习提供了可解释和可靠的模态重要性评估，结合理论最优性保证的模态选择方法，能够在保持高性能的同时减少对冗余模态的依赖

Abstract: Multimodal learning combines information from multiple data modalities to improve predictive performance. However, modalities often contribute unequally and in a data dependent way, making it unclear which data modalities are genuinely informative and to what extent their contributions can be trusted. Quantifying modality level importance together with uncertainty is therefore central to interpretable and reliable multimodal learning. We introduce conformal Shapley intervals, a framework that combines Shapley values with conformal inference to construct uncertainty-aware importance intervals for each modality. Building on these intervals, we propose a modality selection procedure with a provable optimality guarantee: conditional on the observed features, the selected subset of modalities achieves performance close to that of the optimal subset. We demonstrate the effectiveness of our approach on multiple datasets, showing that it provides meaningful uncertainty quantification and strong predictive performance while relying on only a small number of informative modalities.

</details>


### [370] [Neuron Block Dynamics for XOR Classification with Zero-Margin](https://arxiv.org/abs/2602.00172)
*Guillaume Braun,Masaaki Imaizumi*

Main category: stat.ML

TL;DR: 该论文研究了零边际非线性分类问题，通过分析高斯XOR问题，揭示了在无边际假设下神经网络通过SGD学习特征的能力，提出了块级动态分析框架。


<details>
  <summary>Details</summary>
Motivation: 大多数理论分析关注回归或有正边际的分类任务，而实际中许多分类问题没有边际保证（如数据紧邻决策边界）。需要理解在零边际设置下神经网络如何通过SGD学习有用特征。

Method: 分析高斯XOR问题（输入为高斯分布，标签由XOR决策边界决定），扩展Glasgow(2024)的分析，从离散输入到高斯输入，开发神经元块动态框架，研究神经元聚类和块级信号演化。

Result: 神经元聚集成四个方向，块级信号相干演化；建立了不依赖边际假设的泛化分析框架，区分可靠预测区域和持续错误区域；数值实验证实了两阶段块动态及其在非高斯设置下的鲁棒性。

Conclusion: 在零边际分类问题中，神经网络通过SGD学习时表现出块级相干动态，神经元聚类形成结构化表示，这为理解无边际保证下的泛化提供了新视角。

Abstract: The ability of neural networks to learn useful features through stochastic gradient descent (SGD) is a cornerstone of their success. Most theoretical analyses focus on regression or on classification tasks with a positive margin, where worst-case gradient bounds suffice. In contrast, we study zero-margin nonlinear classification by analyzing the Gaussian XOR problem, where inputs are Gaussian and the XOR decision boundary determines labels. In this setting, a non-negligible fraction of data lies arbitrarily close to the boundary, breaking standard margin-based arguments. Building on Glasgow's (2024) analysis, we extend the study of training dynamics from discrete to Gaussian inputs and develop a framework for the dynamics of neuron blocks. We show that neurons cluster into four directions and that block-level signals evolve coherently, a phenomenon essential in the Gaussian setting where individual neuron signals vary significantly. Leveraging this block perspective, we analyze generalization without relying on margin assumptions, adopting an average-case view that distinguishes regions of reliable prediction from regions of persistent error. Numerical experiments confirm the predicted two-phase block dynamics and demonstrate their robustness beyond the Gaussian setting.

</details>


### [371] [Singular Bayesian Neural Networks](https://arxiv.org/abs/2602.00387)
*Mame Diarra Toure,David A. Stephens*

Main category: stat.ML

TL;DR: 提出一种低秩参数化的贝叶斯神经网络，通过权重矩阵分解W=AB^T来减少参数数量，同时保持不确定性校准能力，在多个模型上实现与深度集成相当的性能但参数更少。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯神经网络需要O(mn)参数，当权重矩阵具有快速奇异值衰减时，这种成本通常是不必要的。需要一种更高效的参数化方法来减少参数数量，同时保持不确定性校准能力。

Method: 将权重矩阵参数化为W=AB^T的形式，其中A∈R^{m×r}，B∈R^{n×r}，这诱导了一个相对于Lebesgue测度奇异的后验分布，集中在秩r流形上。这种方法通过共享潜在因子捕获结构化的权重相关性。

Result: 在MLP、LSTM和Transformer等模型上，该方法在标准基准测试中实现了与5成员深度集成相当的性能，同时参数减少了15倍。此外，在OOD检测和校准方面显著优于均值场和扰动基线方法。

Conclusion: 低秩参数化提供了一种高效的贝叶斯神经网络方法，通过利用权重矩阵的低秩结构，在减少参数数量的同时保持不确定性校准能力，为大规模贝叶斯深度学习提供了实用解决方案。

Abstract: Bayesian neural networks promise calibrated uncertainty but require $O(mn)$ parameters for standard mean-field Gaussian posteriors. We argue this cost is often unnecessary, particularly when weight matrices exhibit fast singular value decay. By parameterizing weights as $W = AB^{\top}$ with $A \in \mathbb{R}^{m \times r}$, $B \in \mathbb{R}^{n \times r}$, we induce a posterior that is singular with respect to the Lebesgue measure, concentrating on the rank-$r$ manifold. This singularity captures structured weight correlations through shared latent factors, geometrically distinct from mean-field's independence assumption. We derive PAC-Bayes generalization bounds whose complexity term scales as $\sqrt{r(m+n)}$ instead of $\sqrt{m n}$, and prove loss bounds that decompose the error into optimization and rank-induced bias using the Eckart-Young-Mirsky theorem. We further adapt recent Gaussian complexity bounds for low-rank deterministic networks to Bayesian predictive means. Empirically, across MLPs, LSTMs, and Transformers on standard benchmarks, our method achieves predictive performance competitive with 5-member Deep Ensembles while using up to $15\times$ fewer parameters. Furthermore, it substantially improves OOD detection and often improves calibration relative to mean-field and perturbation baselines.

</details>


### [372] [Reliable Real-Time Value at Risk Estimation via Quantile Regression Forest with Conformal Calibration](https://arxiv.org/abs/2602.01912)
*Du-Yi Wang,Guo Liang,Kun Zhang,Qianwen Zhu*

Main category: stat.ML

TL;DR: 提出基于离线-模拟-在线估计框架的分位数回归森林方法，结合保形校准来实时估计VaR，确保可靠性和覆盖有效性


<details>
  <summary>Details</summary>
Motivation: 市场条件快速变化需要实时风险监控，但VaR的在线估计仍然具有挑战性。准确可靠的VaR估计对于及时风险控制和明智决策至关重要。

Method: 使用离线-模拟-在线估计框架中的分位数回归森林。离线训练分位数回归森林学习在线VaR与风险因子之间的关系，然后在线通过观察到的风险因子产生实时VaR估计。为确保可靠性，开发了保形化估计器来校准在线VaR估计。

Result: 理论分析建立了所提估计器的一致性和覆盖有效性。数值实验证实了所提方法的有效性，并展示了其在实践中的表现。

Conclusion: 该研究首次利用保形校准基于OSOA框架可靠地估计实时VaR，为实时风险监控提供了有效且可靠的方法。

Abstract: Rapidly evolving market conditions call for real-time risk monitoring, but its online estimation remains challenging. In this paper, we study the online estimation of one of the most widely used risk measures, Value at Risk (VaR). Its accurate and reliable estimation is essential for timely risk control and informed decision-making. We propose to use the quantile regression forest in the offline-simulation-online-estimation (OSOA) framework. Specifically, the quantile regression forest is trained offline to learn the relationship between the online VaR and risk factors, and real-time VaR estimates are then produced online by incorporating observed risk factors. To further ensure reliability, we develop a conformalized estimator that calibrates the online VaR estimates. To the best of our knowledge, we are the first to leverage conformal calibration to estimate real-time VaR reliably based on the OSOA formulation. Theoretical analysis establishes the consistency and coverage validity of the proposed estimators. Numerical experiments confirm the proposed method and demonstrate its effectiveness in practice.

</details>


### [373] [Reinforcement Learning for Control Systems with Time Delays: A Comprehensive Survey](https://arxiv.org/abs/2602.00399)
*Armando Alves Neto*

Main category: stat.ML

TL;DR: 这篇论文是关于强化学习在时延控制系统中的综述，系统性地分类了五种处理时延的方法，并分析了各自的优缺点和适用场景。


<details>
  <summary>Details</summary>
Motivation: 实际网络物理系统中的传感延迟、执行延迟和通信约束违反了强化学习依赖的马尔可夫决策过程假设，这些时延会引入记忆效应，显著降低性能并威胁系统稳定性，特别是在网络化和多智能体环境中。

Method: 论文首先形式化了主要时延类别并分析其对马尔可夫性质的影响，然后将现有方法系统性地分为五大类：状态增强和历史表示、具有学习记忆的循环策略、预测器和模型感知方法、鲁棒和领域随机化训练策略、以及具有显式约束处理的强化学习安全框架。

Result: 通过比较分析突出了这些方法之间的关键权衡，并为不同时延特性和安全要求下的方法选择提供了实用指南。同时识别了稳定性认证、大时延学习、多智能体通信协同设计和标准化基准测试等开放挑战。

Conclusion: 这篇综述旨在为在受时延影响的网络物理系统中开发可靠强化学习控制器的研究者和实践者提供统一的参考框架，并指出了未来的研究方向。

Abstract: In the last decade, Reinforcement Learning (RL) has achieved remarkable success in the control and decision-making of complex dynamical systems. However, most RL algorithms rely on the Markov Decision Process assumption, which is violated in practical cyber-physical systems affected by sensing delays, actuation latencies, and communication constraints. Such time delays introduce memory effects that can significantly degrade performance and compromise stability, particularly in networked and multi-agent environments. This paper presents a comprehensive survey of RL methods designed to address time delays in control systems. We first formalize the main classes of delays and analyze their impact on the Markov property. We then systematically categorize existing approaches into five major families: state augmentation and history-based representations, recurrent policies with learned memory, predictor-based and model-aware methods, robust and domain-randomized training strategies, and safe RL frameworks with explicit constraint handling. For each family, we discuss underlying principles, practical advantages, and inherent limitations. A comparative analysis highlights key trade-offs among these approaches and provides practical guidelines for selecting suitable methods under different delay characteristics and safety requirements. Finally, we identify open challenges and promising research directions, including stability certification, large-delay learning, multi-agent communication co-design, and standardized benchmarking. This survey aims to serve as a unified reference for researchers and practitioners developing reliable RL-based controllers in delay-affected cyber-physical systems.

</details>


### [374] [Alignment of Diffusion Model and Flow Matching for Text-to-Image Generation](https://arxiv.org/abs/2602.00413)
*Yidong Ouyang,Liyan Xie,Hongyuan Zha,Guang Cheng*

Main category: stat.ML

TL;DR: 提出一个新颖的对齐框架，利用对齐问题的本质——从奖励加权分布中采样，适用于扩散模型和流匹配模型，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法主要专注于微调预训练生成模型以最大化奖励函数，但需要大量计算资源且在不同目标间泛化能力有限。

Method: 提出基于奖励加权分布采样的对齐框架：1) 扩散模型：训练引导网络估计奖励的条件期望，避免微调；2) 流匹配：提出无需训练的方法提升生成质量。

Result: 在扩散模型中，与基于微调的模型相比，单步生成达到可比性能，计算成本降低至少60%；在流匹配中，无需额外计算成本即可提升生成质量。

Conclusion: 该框架为生成模型对齐提供了一种高效、泛化性强的解决方案，显著降低计算需求，同时保持或提升生成质量。

Abstract: Diffusion models and flow matching have demonstrated remarkable success in text-to-image generation. While many existing alignment methods primarily focus on fine-tuning pre-trained generative models to maximize a given reward function, these approaches require extensive computational resources and may not generalize well across different objectives. In this work, we propose a novel alignment framework by leveraging the underlying nature of the alignment problem -- sampling from reward-weighted distributions -- and show that it applies to both diffusion models (via score guidance) and flow matching models (via velocity guidance). The score function (velocity field) required for the reward-weighted distribution can be decomposed into the pre-trained score (velocity field) plus a conditional expectation of the reward. For the alignment on the diffusion model, we identify a fundamental challenge: the adversarial nature of the guidance term can introduce undesirable artifacts in the generated images. Therefore, we propose a finetuning-free framework that trains a guidance network to estimate the conditional expectation of the reward. We achieve comparable performance to finetuning-based models with one-step generation with at least a 60% reduction in computational cost. For the alignment on flow matching, we propose a training-free framework that improves the generation quality without additional computational cost.

</details>


### [375] [Shuffle and Joint Differential Privacy for Generalized Linear Contextual Bandits](https://arxiv.org/abs/2602.00417)
*Sahasrajit Sarmasarkar*

Main category: stat.ML

TL;DR: 提出了首个在洗牌差分隐私和联合差分隐私下的广义线性上下文赌博机算法，解决了GLM带来的新挑战，在不同隐私模型和上下文设置下实现了接近非私有算法的遗憾界。


<details>
  <summary>Details</summary>
Motivation: 现有私有上下文赌博机研究局限于线性奖励模型（有闭式解），而广义线性模型（GLM）带来新挑战：无闭式解需要私有凸优化、需跨多个演化设计矩阵跟踪隐私、必须将优化误差显式纳入遗憾分析。

Method: 针对两种隐私模型和上下文设置设计不同算法：对于随机上下文，设计洗牌差分隐私算法；对于对抗性上下文，提供联合差分隐私算法。两种算法都消除了对实例特定参数κ的依赖，且仅需ℓ₂有界性假设。

Result: 随机上下文下洗牌差分隐私算法实现$\tilde{O}(d^{3/2}\sqrt{T}/\sqrt{\varepsilon})$遗憾；对抗性上下文下联合差分隐私算法实现$\tilde{O}(d\sqrt{T}/\sqrt{\varepsilon})$遗憾，与非私有率仅差$1/\sqrt{\varepsilon}$因子。

Conclusion: 首次在广义线性上下文赌博机中实现差分隐私保护，解决了GLM特有的技术挑战，在不同设置下获得接近最优的遗憾界，且无需对上下文分布做谱假设。

Abstract: We present the first algorithms for generalized linear contextual bandits under shuffle differential privacy and joint differential privacy. While prior work on private contextual bandits has been restricted to linear reward models -- which admit closed-form estimators -- generalized linear models (GLMs) pose fundamental new challenges: no closed-form estimator exists, requiring private convex optimization; privacy must be tracked across multiple evolving design matrices; and optimization error must be explicitly incorporated into regret analysis.
  We address these challenges under two privacy models and context settings. For stochastic contexts, we design a shuffle-DP algorithm achieving $\tilde{O}(d^{3/2}\sqrt{T}/\sqrt{\varepsilon})$ regret. For adversarial contexts, we provide a joint-DP algorithm with $\tilde{O}(d\sqrt{T}/\sqrt{\varepsilon})$ regret -- matching the non-private rate up to a $1/\sqrt{\varepsilon}$ factor. Both algorithms remove dependence on the instance-specific parameter $κ$ (which can be exponential in dimension) from the dominant $\sqrt{T}$ term. Unlike prior work on locally private GLM bandits, our methods require no spectral assumptions on the context distribution beyond $\ell_2$ boundedness.

</details>


### [376] [Topological Residual Asymmetry for Bivariate Causal Direction](https://arxiv.org/abs/2602.00427)
*Mouad El Bouchattaoui*

Main category: stat.ML

TL;DR: 提出TRA方法，基于拓扑几何的残差不对称性来推断因果方向，通过持久同调量化残差云的"块状"与"管状"对比，在低噪声下表现优异


<details>
  <summary>Details</summary>
Motivation: 现有因果方向推断方法在模糊或接近不可识别的情况下容易做出错误判断，需要更稳健的方法来处理这些挑战性场景

Method: TRA方法基于几何标准，比较两个交叉拟合的回归器-残差云的形状：在正确方向下残差近似独立形成二维块状，反向方向则集中在近一维管状；使用0维持久同调功能量化这种对比，并扩展到固定噪声的TRA-s变体和考虑混杂因素的TRA-C

Result: 在大量具有挑战性的合成和真实数据场景中，该方法表现出优越性能，特别是在低噪声情况下具有理论一致性保证

Conclusion: TRA方法为因果方向推断提供了稳健的几何基础方法，通过拓扑分析残差不对称性，在模糊情况下仍能可靠工作

Abstract: Inferring causal direction from purely observational bivariate data is fragile: many methods commit to a direction even in ambiguous or near non-identifiable regimes. We propose Topological Residual Asymmetry (TRA), a geometry-based criterion for additive-noise models. TRA compares the shapes of two cross-fitted regressor-residual clouds after rank-based copula standardization: in the correct direction, residuals are approximately independent, producing a two-dimensional bulk, while in the reverse direction -- especially under low noise -- the cloud concentrates near a one-dimensional tube. We quantify this bulk-tube contrast using a 0D persistent-homology functional, computed efficiently from Euclidean MST edge-length profiles. We prove consistency in a triangular-array small-noise regime, extend the method to fixed noise via a binned variant (TRA-s), and introduce TRA-C, a confounding-aware abstention rule calibrated by a Gaussian-copula plug-in bootstrap. Extensive experiments across many challenging synthetic and real-data scenarios demonstrate the method's superiority.

</details>


### [377] [Stabilizing Fixed-Point Iteration for Markov Chain Poisson Equations](https://arxiv.org/abs/2602.00474)
*Yang Xu,Vaneet Aggarwal*

Main category: stat.ML

TL;DR: 该论文解决了非遍历马尔可夫链（多链和周期链）中泊松方程学习的问题，通过引入商空间方法和规范映射来确保解的稳定性和唯一性。


<details>
  <summary>Details</summary>
Motivation: 在平均奖励强化学习中，泊松方程是基础，但在非遍历（如可约或周期）马尔可夫链中，泊松方程可能不适定，导致解不唯一且标准迭代算法会振荡。现有方法主要针对遍历链，缺乏对更一般链结构的稳定学习方案。

Method: 1. 理论分析：识别马尔可夫链的实外围不变子空间K(P)，证明在商空间R^n/K(P)上的诱导算子是严格压缩的，从而获得唯一的商解。
2. 端到端流程：学习链结构 → 估计基于锚点的规范映射 → 运行投影随机逼近算法来估计规范固定代表及其相关的外围残差。
3. 收敛性证明：在投影估计误差范围内，达到Õ(T^{-1/2})的收敛速率。

Result: 提出了一个稳定学习泊松方程的框架，适用于多链和周期马尔可夫链。理论证明了在商空间上解的唯一性，算法实现了稳定的收敛性能，突破了传统方法仅限于遍历链的限制。

Conclusion: 该工作通过商空间视角和规范固定技术，成功解决了非遍历马尔可夫链中泊松方程学习的不适定问题，为平均奖励强化学习在更广泛场景下的性能评估提供了理论基础和实用算法。

Abstract: Poisson equations underpin average-reward reinforcement learning, but beyond ergodicity they can be ill-posed, meaning that solutions are non-unique and standard fixed point iterations can oscillate on reducible or periodic chains. We study finite-state Markov chains with $n$ states and transition matrix $P$. We show that all non-decaying modes are captured by a real peripheral invariant subspace $\mathcal{K}(P)$, and that the induced operator on the quotient space $\mathbb{R}^n/\mathcal{K}(P)$ is strictly contractive, yielding a unique quotient solution. Building on this viewpoint, we develop an end-to-end pipeline that learns the chain structure, estimates an anchor based gauge map, and runs projected stochastic approximation to estimate a gauge-fixed representative together with an associated peripheral residual. We prove $\widetilde{O}(T^{-1/2})$ convergence up to projection estimation error, enabling stable Poisson equation learning for multichain and periodic regimes with applications to performance evaluation of average-reward reinforcement learning beyond ergodicity.

</details>


### [378] [Action-Free Offline-to-Online RL via Discretised State Policies](https://arxiv.org/abs/2602.00629)
*Natinael Solomon Neggatu,Jeremie Houssineau,Giovanni Montana*

Main category: stat.ML

TL;DR: 提出一种从无动作标签的离线数据中学习状态策略的方法，通过状态离散化转换和状态策略指导在线学习，加速在线强化学习


<details>
  <summary>Details</summary>
Motivation: 现实世界中许多离线数据集可能缺少动作标签（由于隐私、存储或传感器限制），但现有离线RL方法通常假设动作标签可用。需要解决从仅包含(s,r,s')元组的无动作数据中学习，并利用这些知识加速在线学习的问题。

Method: 1. 提出状态离散化转换，将连续状态空间离散化以避免不稳定和过拟合；2. 提出Offline State-Only DecQN算法，从无动作数据中预训练状态策略（推荐期望的下一个状态转移而非动作）；3. 提出引导在线学习机制，利用预训练的状态策略加速在线代理学习。

Result: 在多样化基准测试中，该方法提高了收敛速度和渐近性能。分析表明状态离散化和正则化对其有效性至关重要。

Conclusion: 建立了一个可扩展且实用的框架，利用无动作数据集加速在线RL。状态策略学习和引导在线学习相结合，为处理动作缺失的离线数据提供了有效解决方案。

Abstract: Most existing offline RL methods presume the availability of action labels within the dataset, but in many practical scenarios, actions may be missing due to privacy, storage, or sensor limitations. We formalise the setting of action-free offline-to-online RL, where agents must learn from datasets consisting solely of $(s,r,s')$ tuples and later leverage this knowledge during online interaction. To address this challenge, we propose learning state policies that recommend desirable next-state transitions rather than actions. Our contributions are twofold. First, we introduce a simple yet novel state discretisation transformation and propose Offline State-Only DecQN (\algo), a value-based algorithm designed to pre-train state policies from action-free data. \algo{} integrates the transformation to scale efficiently to high-dimensional problems while avoiding instability and overfitting associated with continuous state prediction. Second, we propose a novel mechanism for guided online learning that leverages these pre-trained state policies to accelerate the learning of online agents. Together, these components establish a scalable and practical framework for leveraging action-free datasets to accelerate online RL. Empirical results across diverse benchmarks demonstrate that our approach improves convergence speed and asymptotic performance, while analyses reveal that discretisation and regularisation are critical to its effectiveness.

</details>


### [379] [Sampling from multi-modal distributions on Riemannian manifolds with training-free stochastic interpolants](https://arxiv.org/abs/2602.00641)
*Alain Durmus,Maxence Noble,Thibaut Pellerin*

Main category: stat.ML

TL;DR: 提出一种在黎曼流形上从非归一化多模态密度采样的训练自由方法，基于确定性动力学将噪声分布传输到目标分布


<details>
  <summary>Details</summary>
Motivation: 现有采样方法在处理黎曼流形上的多模态目标分布时面临挑战，特别是高维和重尾分布，需要一种不依赖机器学习训练的新方法

Method: 基于扩散模型框架，通过模拟非平衡确定性动力学，将易采样的噪声分布传输到目标分布；密度路径遵循噪声和目标分布之间的随机插值，尊重黎曼几何结构；完全训练自由，仅使用标准蒙特卡洛技术的迭代后验采样

Result: 方法在多种多模态采样问题上表现出有效性，包括高维和重尾分布示例，并得到了严格的理论分析支持

Conclusion: 成功将基于扩散的采样方法扩展到欧几里得空间之外的黎曼流形设置，为处理复杂几何结构下的多模态分布提供了有效的训练自由解决方案

Abstract: In this paper, we propose a general methodology for sampling from un-normalized densities defined on Riemannian manifolds, with a particular focus on multi-modal targets that remain challenging for existing sampling methods. Inspired by the framework of diffusion models developed for generative modeling, we introduce a sampling algorithm based on the simulation of a non-equilibrium deterministic dynamics that transports an easy-to-sample noise distribution toward the target. At the marginal level, the induced density path follows a prescribed stochastic interpolant between the noise and target distributions, specifically constructed to respect the underlying Riemannian geometry. In contrast to related generative modeling approaches that rely on machine learning, our method is entirely training-free. It instead builds on iterative posterior sampling procedures using only standard Monte Carlo techniques, thereby extending recent diffusion-based sampling methodologies beyond the Euclidean setting. We complement our approach with a rigorous theoretical analysis and demonstrate its effectiveness on a range of multi-modal sampling problems, including high-dimensional and heavy-tailed examples.

</details>


### [380] [Emergence of Distortions in High-Dimensional Guided Diffusion Models](https://arxiv.org/abs/2602.00716)
*Enrico Ventura,Beatrice Achilli,Luca Ambrogioni,Carlo Lucibello*

Main category: stat.ML

TL;DR: CFG导致生成样本多样性损失，作者将其形式化为生成失真，分析高维条件下的失真相变，提出负引导窗口缓解方差收缩问题


<details>
  <summary>Details</summary>
Motivation: 分类器自由引导（CFG）是扩散模型中条件采样的标准方法，但经常导致生成样本多样性损失。作者希望形式化这一现象，理解其在高维条件下的本质，并提出改进方法

Method: 使用高斯混合模型和精确分数，借助统计物理工具分析高维条件下的失真现象。通过动态平均场理论分析失真相变，提出负引导窗口的引导调度方法

Result: 分析显示失真通过引导动力学的有效势能相变出现。当模式数量随维度指数增长时失真持续存在，但在次指数增长时消失。标准CFG调度无法防止方差收缩，而提出的负引导窗口方法能缓解多样性损失

Conclusion: CFG导致的生成失真在高维条件下表现为相变现象。提出的负引导窗口调度方法能有效缓解方差收缩问题，在保持类别可分性的同时减少多样性损失

Abstract: Classifier-free guidance (CFG) is the de facto standard for conditional sampling in diffusion models, yet it often leads to a loss of diversity in generated samples. We formalize this phenomenon as generative distortion, defined as the mismatch between the CFG-induced sampling distribution and the true conditional distribution. Considering Gaussian mixtures and their exact scores, and leveraging tools from statistical physics, we characterize the onset of distortion in a high-dimensional regime as a function of the number of classes. Our analysis reveals that distortions emerge through a phase transition in the effective potential governing the guided dynamics. In particular, our dynamical mean-field analysis shows that distortion persists when the number of modes grows exponentially with dimension, but vanishes in the sub-exponential regime. Consistent with prior finite-dimensional results, we further demonstrate that vanilla CFG shifts the mean and shrinks the variance of the conditional distribution. We show that standard CFG schedules are fundamentally incapable of preventing variance shrinkage. Finally, we propose a theoretically motivated guidance schedule featuring a negative-guidance window, which mitigates loss of diversity while preserving class separability.

</details>


### [381] [Zero-Flow Encoders](https://arxiv.org/abs/2602.00797)
*Yakun Wang,Leyang Wang,Song Liu,Taiji Suzuki*

Main category: stat.ML

TL;DR: 本文提出了一种基于流的表示学习框架，利用零流准则来提取数据中的充分信息，用于图模型中的马尔可夫毯学习和自监督学习中的潜在表示学习。


<details>
  <summary>Details</summary>
Motivation: 流方法在生成建模中取得了显著成功，但现有工作很少利用其捕捉复杂数据分布细节的能力来解决生成任务之外的细粒度结构细节问题。本文旨在利用流的独特能力进行表示学习。

Method: 首先证明了使用独立耦合训练的整流流在t=0.5处处处为零当且仅当源分布和目标分布相同（零流准则）。其次，展示了该准则可以验证条件独立性，从而提取数据中的充分信息。最后，将该准则转化为可处理的、无需模拟的损失函数，用于学习图模型中的摊销马尔可夫毯和自监督学习任务中的潜在表示。

Result: 在模拟和真实世界数据集上的实验证明了该方法的有效性。代码已开源。

Conclusion: 本文提出的流启发的表示学习框架能够有效利用流的独特能力来提取数据中的充分信息，为图模型和自监督学习提供了新的解决方案。

Abstract: Flow-based methods have achieved significant success in various generative modeling tasks, capturing nuanced details within complex data distributions. However, few existing works have exploited this unique capability to resolve fine-grained structural details beyond generation tasks. This paper presents a flow-inspired framework for representation learning. First, we demonstrate that a rectified flow trained using independent coupling is zero everywhere at $t=0.5$ if and only if the source and target distributions are identical. We term this property the \emph{zero-flow criterion}. Second, we show that this criterion can certify conditional independence, thereby extracting \emph{sufficient information} from the data. Third, we translate this criterion into a tractable, simulation-free loss function that enables learning amortized Markov blankets in graphical models and latent representations in self-supervised learning tasks. Experiments on both simulated and real-world datasets demonstrate the effectiveness of our approach. The code reproducing our experiments can be found at: https://github.com/probabilityFLOW/zfe.

</details>


### [382] [Hessian Spectral Analysis at Foundation Model Scale](https://arxiv.org/abs/2602.00816)
*Diego Granziol,Khurshid Juarev*

Main category: stat.ML

TL;DR: 首次在百亿参数规模上实现了基础模型Hessian矩阵的精确谱分析，揭示了传统块对角近似方法的严重失效


<details>
  <summary>Details</summary>
Motivation: 基础模型的Hessian谱分析一直难以实现，现有研究只能依赖小模型或强结构近似，缺乏对前沿规模模型真实曲率的理解

Method: 使用兼容全分片数据并行的分片局部有限差分Hessian向量乘积，在fp32和bf16精度下对开源语言模型进行随机Lanczos求积，建立了包含有限差分偏差、浮点噪声放大等数值行为的完整分析管道

Result: 首次在100B参数规模上获得了大规模谱密度估计，发现广泛使用的块对角曲率近似存在阶一相对误差和方向对齐问题，而全算子谱探测仅带来适度的常数因子开销

Conclusion: 基础模型Hessian谱既是可计算的，又被主流近似方法严重误表示，为大规模曲率分析开辟了新途径

Abstract: Accurate Hessian spectra of foundation models have remained out of reach, leading most prior work to rely on small models or strong structural approximations. We show that faithful spectral analysis of the true Hessian is tractable at frontier scale. Using shard-local finite-difference Hessian vector products compatible with Fully Sharded Data Parallelism, we perform stochastic Lanczos quadrature on open-source language models with up to 100B parameters, producing the first large-scale spectral density estimates beyond the sub-10B regime. We characterize the numerical behavior of this pipeline, including finite-difference bias, floating-point noise amplification, and their effect on Krylov stability in fp32 and bf16, and derive practical operating regimes that are validated empirically. We further provide end-to-end runtime and memory scaling laws, showing that full-operator spectral probing incurs only a modest constant-factor overhead over first-order training. Crucially, direct access to the Hessian reveals that widely used block-diagonal curvature approximations can fail catastrophically, exhibiting order-one relative error and poor directional alignment even in mid-scale LLMs. Together, our results demonstrate that foundation-model Hessian spectra are both computable and qualitatively misrepresented by prevailing approximations, opening the door to principled curvature-based analysis at scale.

</details>


### [383] [Safety-Efficacy Trade Off: Robustness against Data-Poisoning](https://arxiv.org/abs/2602.00822)
*Diego Granziol*

Main category: stat.ML

TL;DR: 论文证明后门和数据投毒攻击通过输入空间的几何机制实现高攻击成功率并规避现有防御，揭示了攻击有效性与输入曲率之间的根本关系，提出了基于输入梯度正则化的防御方法。


<details>
  <summary>Details</summary>
Motivation: 现有光谱和优化防御方法无法有效检测后门和数据投毒攻击，需要从几何角度理解攻击机制，建立攻击有效性与可检测性之间的理论联系。

Method: 使用核岭回归作为宽神经网络的精确模型，分析聚类脏标签投毒在输入Hessian中引起的秩一尖峰，识别非线性核的近克隆机制，提出输入梯度正则化防御方法。

Result: 理论证明攻击有效性与输入曲率存在二次缩放关系，在近克隆机制下攻击保持有效而输入曲率消失，实验验证攻击成功率与光谱可见性之间存在滞后，正则化和数据增强能有效抑制投毒。

Conclusion: 研究建立了后门攻击固有不可见性的条件，首次通过输入空间曲率完整刻画了投毒、可检测性和防御之间的关系，为设计更有效的防御提供了理论基础。

Abstract: Backdoor and data poisoning attacks can achieve high attack success while evading existing spectral and optimisation based defences. We show that this behaviour is not incidental, but arises from a fundamental geometric mechanism in input space. Using kernel ridge regression as an exact model of wide neural networks, we prove that clustered dirty label poisons induce a rank one spike in the input Hessian whose magnitude scales quadratically with attack efficacy. Crucially, for nonlinear kernels we identify a near clone regime in which poison efficacy remains order one while the induced input curvature vanishes, making the attack provably spectrally undetectable. We further show that input gradient regularisation contracts poison aligned Fisher and Hessian eigenmodes under gradient flow, yielding an explicit and unavoidable safety efficacy trade off by reducing data fitting capacity. For exponential kernels, this defence admits a precise interpretation as an anisotropic high pass filter that increases the effective length scale and suppresses near clone poisons. Extensive experiments on linear models and deep convolutional networks across MNIST and CIFAR 10 and CIFAR 100 validate the theory, demonstrating consistent lags between attack success and spectral visibility, and showing that regularisation and data augmentation jointly suppress poisoning. Our results establish when backdoors are inherently invisible, and provide the first end to end characterisation of poisoning, detectability, and defence through input space curvature.

</details>


### [384] [Harmful Overfitting in Sobolev Spaces](https://arxiv.org/abs/2602.00825)
*Kedar Karhadkar,Alexander Sietsema,Deanna Needell,Guido Montufar*

Main category: stat.ML

TL;DR: 研究Sobolev空间中完美拟合噪声训练数据的函数泛化行为，发现近似范数最小插值器存在有害过拟合，即使训练样本趋于无穷，泛化误差仍以高概率保持正下界。


<details>
  <summary>Details</summary>
Motivation: 受近期过参数化机器学习中良性过拟合研究的启发，研究Sobolev空间中完美拟合噪声训练数据的函数泛化行为，探索平滑性偏置选择的典型解是否会出现有害过拟合。

Method: 在标签噪声和数据分布充分正则性假设下，使用几何论证方法，通过Sobolev不等式识别训练数据的有害邻域，分析近似范数最小插值器的泛化性能。

Result: 证明近似范数最小插值器存在有害过拟合：即使训练样本量n→∞，泛化误差仍以高概率保持正下界，该结果适用于任意p∈[1,∞)，突破了先前仅研究希尔伯特空间(p=2)核方法的局限。

Conclusion: 在Sobolev空间中，通过平滑性偏置选择的典型插值器即使完美拟合噪声训练数据，也会出现有害过拟合，泛化误差不会随样本量增加而收敛到零，揭示了平滑性正则化在噪声环境下的局限性。

Abstract: Motivated by recent work on benign overfitting in overparameterized machine learning, we study the generalization behavior of functions in Sobolev spaces $W^{k, p}(\mathbb{R}^d)$ that perfectly fit a noisy training data set. Under assumptions of label noise and sufficient regularity in the data distribution, we show that approximately norm-minimizing interpolators, which are canonical solutions selected by smoothness bias, exhibit harmful overfitting: even as the training sample size $n \to \infty$, the generalization error remains bounded below by a positive constant with high probability. Our results hold for arbitrary values of $p \in [1, \infty)$, in contrast to prior results studying the Hilbert space case ($p = 2$) using kernel methods. Our proof uses a geometric argument which identifies harmful neighborhoods of the training data using Sobolev inequalities.

</details>


### [385] [Score-based Metropolis-Hastings for Fractional Langevin Algorithms](https://arxiv.org/abs/2602.00835)
*Ahmed Aloui,Junyi Liao,Ali Hasan,Jose Blanchet,Vahid Tarokh*

Main category: stat.ML

TL;DR: 提出MAFLA方法，通过分数阶提议分数梯度的代理和分数平衡匹配学习接受函数，解决α稳定Lévy驱动的分数阶Langevin算法中无法评估目标密度和提议密度的问题。


<details>
  <summary>Details</summary>
Motivation: 在α稳定Lévy驱动的分数阶Langevin算法中，目标密度和提议密度都无法评估，导致传统的基于密度的Metropolis-Hastings修正不可行。现有分数阶Langevin方法在未调整状态下运行，存在显著的有限时间误差和尾部行为控制不佳的问题。

Method: 提出Metropolis-Adjusted Fractional Langevin Algorithm (MAFLA)，使用各向同性对称α稳定噪声下的分数阶提议分数梯度代理，并通过Score Balance Matching学习接受函数，实现完全基于分数的修正机制。

Result: 在包括组合优化问题在内的一系列任务上，MAFLA表现出强大的性能，显著提高了有限时间采样精度，优于未调整的分数阶Langevin动力学方法。

Conclusion: MAFLA通过创新的分数平衡匹配和分数阶提议分数梯度代理，成功解决了α稳定Lévy驱动的分数阶Langevin算法中的密度评估难题，显著改善了采样精度和尾部行为控制。

Abstract: Sampling from heavy-tailed and multimodal distributions is challenging when neither the target density nor the proposal density can be evaluated, as in $α$-stable Lévy-driven fractional Langevin algorithms. While the target distribution can be estimated from data via score-based or energy-based models, the $α$-stable proposal density and its score are generally unavailable, rendering classical density-based Metropolis--Hastings (MH) corrections impractical. Consequently, existing fractional Langevin methods operate in an unadjusted regime and can exhibit substantial finite-time errors and poor empirical control of tail behavior. We introduce the Metropolis-Adjusted Fractional Langevin Algorithm (MAFLA), an MH-inspired, fully score-based correction mechanism. MAFLA employs designed proxies for fractional proposal score gradients under isotropic symmetric $α$-stable noise and learns an acceptance function via Score Balance Matching. We empirically illustrate the strong performance of MAFLA on a series of tasks including combinatorial optimization problems where the method significantly improves finite time sampling accuracy over unadjusted fractional Langevin dynamics.

</details>


### [386] [Multivariate Time Series Data Imputation via Distributionally Robust Regularization](https://arxiv.org/abs/2602.00844)
*Che-Yi Liao,Zheng Dong,Gian-Gabriel Garcia,Kamran Paynabar*

Main category: stat.ML

TL;DR: 提出DRIO方法，通过分布鲁棒正则化解决多元时间序列插补中的分布偏差问题，在随机缺失和非随机缺失场景下均能提升插补性能


<details>
  <summary>Details</summary>
Motivation: 多元时间序列插补常受观测数据与真实数据分布不匹配的影响，这种偏差在非平稳性和系统性缺失情况下更加严重。传统方法最小化重构误差或鼓励分布对齐，容易对偏差观测过拟合

Method: 提出分布鲁棒正则化插补目标(DRIO)，联合最小化重构误差和插补器与Wasserstein模糊集内最坏情况分布之间的散度。推导出可处理的对偶形式，将无限维测度优化简化为样本轨迹的对抗搜索，并提出与灵活深度学习主干兼容的对抗学习算法

Result: 在多样化真实数据集上的综合实验表明，DRIO在完全随机缺失和非随机缺失设置下均能持续改进插补性能，在重构精度和分布对齐之间达到帕累托最优权衡

Conclusion: DRIO通过分布鲁棒正则化有效解决了多元时间序列插补中的分布偏差问题，为处理非平稳和系统性缺失数据提供了更可靠的解决方案

Abstract: Multivariate time series (MTS) imputation is often compromised by mismatch between observed and true data distributions -- a bias exacerbated by non-stationarity and systematic missingness. Standard methods that minimize reconstruction error or encourage distributional alignment risk overfitting these biased observations. We propose the Distributionally Robust Regularized Imputer Objective (DRIO), which jointly minimizes reconstruction error and the divergence between the imputer and a worst-case distribution within a Wasserstein ambiguity set. We derive a tractable dual formulation that reduces infinite-dimensional optimization over measures to adversarial search over sample trajectories, and propose an adversarial learning algorithm compatible with flexible deep learning backbones. Comprehensive experiments on diverse real-world datasets show DRIO consistently improves imputation under both missing-completely-at-random and missing-not-at-random settings, reaching Pareto-optimal trade-offs between reconstruction accuracy and distributional alignment.

</details>


### [387] [Optimal Decision-Making Based on Prediction Sets](https://arxiv.org/abs/2602.00989)
*Tao Wang,Edgar Dobriban*

Main category: stat.ML

TL;DR: 提出Risk-Optimal Conformal Prediction (ROCP)框架，在保证覆盖率的前提下最小化最坏情况下的决策风险


<details>
  <summary>Details</summary>
Motivation: 预测集虽然能为ML模型提供概率保证的覆盖，但如何最优地用于下游决策仍不明确。需要开发一个框架来最小化决策风险，特别是在安全关键应用中

Method: 提出决策理论框架，最小化与预测集覆盖保证一致的worst-case分布的期望损失。首先推导固定预测集下的minimax最优策略，然后优化预测集构造以最小化鲁棒风险，最后提出ROCP算法实现风险最小化预测集

Result: ROCP在医疗诊断和安全关键决策任务中相比基线方法减少了关键错误，特别是在集外错误代价高昂的情况下表现更优

Conclusion: ROCP框架将预测集的覆盖保证与决策风险最小化相结合，为安全关键应用提供了更可靠的决策支持工具

Abstract: Prediction sets can wrap around any ML model to cover unknown test outcomes with a guaranteed probability. Yet, it remains unclear how to use them optimally for downstream decision-making. Here, we propose a decision-theoretic framework that seeks to minimize the expected loss (risk) against a worst-case distribution consistent with the prediction set's coverage guarantee. We first characterize the minimax optimal policy for a fixed prediction set, showing that it balances the worst-case loss inside the set with a penalty for potential losses outside the set. Building on this, we derive the optimal prediction set construction that minimizes the resulting robust risk subject to a coverage constraint. Finally, we introduce Risk-Optimal Conformal Prediction (ROCP), a practical algorithm that targets these risk-minimizing sets while maintaining finite-sample distribution-free marginal coverage. Empirical evaluations on medical diagnosis and safety-critical decision-making tasks demonstrate that ROCP reduces critical mistakes compared to baselines, particularly when out-of-set errors are costly.

</details>


### [388] [Online Social Welfare Function-based Resource Allocation](https://arxiv.org/abs/2602.01400)
*Kanad Pardeshi,Samsara Foubert,Aarti Singh*

Main category: stat.ML

TL;DR: 提出一个通用的置信序列框架，用于社会福利函数（SWF）的在线学习和推断，适用于任何单调、凹且Lipschitz连续的社会福利函数，并设计了SWF-UCB算法实现近乎最优的遗憾界。


<details>
  <summary>Details</summary>
Motivation: 在现实世界中，中央决策者需要重复地将有限资源分配给多个个体，个体获得资源后产生随机效用，需要通过社会福利函数（SWF）来评估分配效果。现有方法缺乏一个统一的框架来处理各种SWF的在线学习和推断问题。

Method: 提出一个通用的置信序列框架，关键洞察是单调性足以将个体效用的置信序列提升为最优社会福利的随时有效边界。基于此设计了SWF-UCB算法，该算法与具体SWF无关，实现了近乎最优的遗憾界。还在三个规范上不同的SWF家族（加权幂平均、Kolm和Gini）上实例化了该框架。

Result: SWF-UCB算法实现了$\tilde{O}(n+\sqrt{nkT})$的遗憾界（其中$k$为资源数，$n$为个体数，$T$为时间步数）。实验证实了$\sqrt{T}$的缩放特性，并揭示了$k$与SWF参数之间的丰富交互作用。

Conclusion: 该框架为基于社会福利函数的在线学习和推断提供了一个统一的理论基础，支持顺序假设检验、最优停止和政策评估等多种应用，具有广泛的实用价值。

Abstract: In many real-world settings, a centralized decision-maker must repeatedly allocate finite resources to a population over multiple time steps. Individuals who receive a resource derive some stochastic utility; to characterize the population-level effects of an allocation, the expected individual utilities are then aggregated using a social welfare function (SWF). We formalize this setting and present a general confidence sequence framework for SWF-based online learning and inference, valid for any monotonic, concave, and Lipschitz-continuous SWF. Our key insight is that monotonicity alone suffices to lift confidence sequences from individual utilities to anytime-valid bounds on optimal welfare. Building on this foundation, we propose SWF-UCB, a SWF-agnostic online learning algorithm that achieves near-optimal $\tilde{O}(n+\sqrt{nkT})$ regret (for $k$ resources distributed among $n$ individuals at each of $T$ time steps). We instantiate our framework on three normatively distinct SWF families: Weighted Power Mean, Kolm, and Gini, providing bespoke oracle algorithms for each. Experiments confirm $\sqrt{T}$ scaling and reveal rich interactions between $k$ and SWF parameters. This framework naturally supports inference applications such as sequential hypothesis testing, optimal stopping, and policy evaluation.

</details>


### [389] [Importance Weighted Variational Inference without the Reparameterization Trick](https://arxiv.org/abs/2602.01412)
*Kamélia Daudel,Minh-Ngoc Tran,Cheng Zhang*

Main category: stat.ML

TL;DR: 论文分析了重要性加权变分推断中REINFORCE梯度估计器的理论缺陷，提出了新的VIMCO-⋆梯度估计器来解决现有VIMCO估计器随样本数增加时信噪比消失的问题。


<details>
  <summary>Details</summary>
Motivation: 重要性加权变分推断通过优化随蒙特卡洛样本数增加而收紧的界来近似密度。标准的重参数化梯度估计器虽然理论成熟，但限制了数据生成过程和变分近似的选择。REINFORCE梯度估计器没有这些限制，但缺乏严格的理论基础。

Method: 对重要性加权VI中的REINFORCE梯度估计器进行首次全面理论分析，引入并研究广义的VIMCO梯度估计器家族。提出新的VIMCO-⋆梯度估计器，通过理论证明其避免了现有VIMCO估计器的信噪比消失问题。

Result: 理论分析表明现有最先进的VIMCO梯度估计器随样本数N增加时信噪比(SNR)会消失，阻碍有效优化。提出的VIMCO-⋆梯度估计器实现了√N的信噪比缩放，避免了SNR崩溃。在重参数化梯度通常不可用的挑战性设置中，VIMCO-⋆表现出优于现有VIMCO实现的性能。

Conclusion: 论文为重要性加权VI中的REINFORCE梯度估计器提供了首个全面理论分析，揭示了现有VIMCO估计器的根本缺陷，并提出了具有理论保证的VIMCO-⋆梯度估计器，解决了信噪比消失问题，在重参数化梯度不可用的场景中表现出优越性能。

Abstract: Importance weighted variational inference (VI) approximates densities known up to a normalizing constant by optimizing bounds that tighten with the number of Monte Carlo samples $N$. Standard optimization relies on reparameterized gradient estimators, which are well-studied theoretically yet restrict both the choice of the data-generating process and the variational approximation. While REINFORCE gradient estimators do not suffer from such restrictions, they lack rigorous theoretical justification. In this paper, we provide the first comprehensive analysis of REINFORCE gradient estimators in importance weighted VI, leveraging this theoretical foundation to diagnose and resolve fundamental deficiencies in current state-of-the-art estimators. Specifically, we introduce and examine a generalized family of variational inference for Monte Carlo objectives (VIMCO) gradient estimators. We prove that state-of-the-art VIMCO gradient estimators exhibit a vanishing signal-to-noise ratio (SNR) as $N$ increases, which prevents effective optimization. To overcome this issue, we propose the novel VIMCO-$\star$ gradient estimator and show that it averts the SNR collapse of existing VIMCO gradient estimators by achieving a $\sqrt{N}$ SNR scaling instead. We demonstrate its superior empirical performance compared to current VIMCO implementations in challenging settings where reparameterized gradients are typically unavailable.

</details>


### [390] [Robust Generalization with Adaptive Optimal Transport Priors for Decision-Focused Learning](https://arxiv.org/abs/2602.01427)
*Haixiang Sun,Andrew L. Liu*

Main category: stat.ML

TL;DR: PG-DRO框架通过分层最优传输从基础数据学习类别自适应先验，并将其嵌入Sinkhorn DRO，实现少样本场景下更强的鲁棒泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有Sinkhorn DRO方法依赖固定参考分布，限制了其适应性。少样本学习需要在有限监督下泛化并保持对分布偏移的鲁棒性，需要更灵活的DRO框架。

Method: 提出原型引导的分布鲁棒优化框架，通过分层最优传输从丰富的基础数据学习类别自适应先验，将其嵌入Sinkhorn DRO公式，使少样本信息有机整合到类别特定的鲁棒决策中。

Result: PG-DRO在少样本场景中实现了更强的鲁棒泛化，超越了标准学习器和DRO基线方法。

Conclusion: PG-DRO框架通过将可迁移的结构知识融入不确定性集合，实现了理论上有保障且高效的类别特定鲁棒决策，在少样本学习中表现出优越性能。

Abstract: Few-shot learning requires models to generalize under limited supervision while remaining robust to distribution shifts. Existing Sinkhorn Distributionally Robust Optimization (DRO) methods provide theoretical guarantees but rely on a fixed reference distribution, which limits their adaptability. We propose a Prototype-Guided Distributionally Robust Optimization (PG-DRO) framework that learns class-adaptive priors from abundant base data via hierarchical optimal transport and embeds them into the Sinkhorn DRO formulation. This design enables few-shot information to be organically integrated into producing class-specific robust decisions that are both theoretically grounded and efficient, and further aligns the uncertainty set with transferable structural knowledge. Experiments show that PG-DRO achieves stronger robust generalization in few-shot scenarios, outperforming both standard learners and DRO baselines.

</details>


### [391] [Rethinking Multinomial Logistic Mixture of Experts with Sigmoid Gating Function](https://arxiv.org/abs/2602.01466)
*Tuan Minh Pham,Thinh Cao,Viet Nguyen,Huy Nguyen,Nhat Ho,Alessandro Rinaldo*

Main category: stat.ML

TL;DR: 本文系统分析了多专家混合模型中sigmoid门控机制，发现其比softmax门控具有更低的样本复杂度，但温度参数会导致指数级样本复杂度，提出使用欧几里得评分替代内积评分来解决此问题。


<details>
  <summary>Details</summary>
Motivation: 尽管sigmoid门控在混合专家模型中经验上优于softmax门控，但存在三个未解决的理论问题：分类任务中的优势未确立、现有模型可能不收敛到真实值、温度参数影响缺乏理论分析。

Method: 对配备改进sigmoid门控的多项逻辑混合专家模型进行全面分析，确保模型收敛。研究sigmoid门控与softmax门控在参数和专家估计方面的样本复杂度差异，并分析温度参数的影响。

Result: sigmoid门控在参数和专家估计方面比softmax门控具有更低的样本复杂度。但温度参数会导致指数级样本复杂度，而使用欧几里得评分替代内积评分可以将样本复杂度降低到多项式级别。

Conclusion: sigmoid门控在混合专家模型中具有理论优势，但需要谨慎处理温度参数。提出的欧几里得评分方法能有效解决温度参数导致的指数复杂度问题，显著提升模型效率。

Abstract: The sigmoid gate in mixture-of-experts (MoE) models has been empirically shown to outperform the softmax gate across several tasks, ranging from approximating feed-forward networks to language modeling. Additionally, recent efforts have demonstrated that the sigmoid gate is provably more sample-efficient than its softmax counterpart under regression settings. Nevertheless, there are three notable concerns that have not been addressed in the literature, namely (i) the benefits of the sigmoid gate have not been established under classification settings; (ii) existing sigmoid-gated MoE models may not converge to their ground-truth; and (iii) the effects of a temperature parameter in the sigmoid gate remain theoretically underexplored. To tackle these open problems, we perform a comprehensive analysis of multinomial logistic MoE equipped with a modified sigmoid gate to ensure model convergence. Our results indicate that the sigmoid gate exhibits a lower sample complexity than the softmax gate for both parameter and expert estimation. Furthermore, we find that incorporating a temperature into the sigmoid gate leads to a sample complexity of exponential order due to an intrinsic interaction between the temperature and gating parameters. To overcome this issue, we propose replacing the vanilla inner product score in the gating function with a Euclidean score that effectively removes that interaction, thereby substantially improving the sample complexity to a polynomial order.

</details>


### [392] [Density-Informed Pseudo-Counts for Calibrated Evidential Deep Learning](https://arxiv.org/abs/2602.01477)
*Pietro Carlotti,Nevena Gligić,Arya Farahi*

Main category: stat.ML

TL;DR: EDL框架在分布偏移下存在理论缺陷，将认知和偶然不确定性混为一谈，导致OOD数据过度自信。本文提出DIP-EDL新方法，通过分离标签分布和协变量密度估计来解耦不确定性，改善OOD鲁棒性和校准。


<details>
  <summary>Details</summary>
Motivation: 尽管EDL在不确定性感知分类中很流行，但其理论基础和在分布偏移下的行为仍不清楚。现有方法在OOD输入上存在系统性过度自信问题，需要理论改进。

Method: 提出DIP-EDL方法：1) 建立理论框架，证明EDL训练对应于分层贝叶斯模型中的摊销变分推断；2) 引入密度感知伪计数参数化，分离条件标签分布和边际协变量密度估计；3) 在高密度区域保留证据，对OOD数据向均匀先验收缩。

Result: 理论证明DIP-EDL实现渐近集中性；实证显示该方法提高了可解释性，在分布偏移下增强了鲁棒性和不确定性校准性能。

Conclusion: 通过理论分析揭示了EDL的根本缺陷，提出的DIP-EDL方法有效解耦了不确定性类型，在理论和实证上都优于标准EDL，为不确定性感知分类提供了更可靠的框架。

Abstract: Evidential Deep Learning (EDL) is a popular framework for uncertainty-aware classification that models predictive uncertainty via Dirichlet distributions parameterized by neural networks. Despite its popularity, its theoretical foundations and behavior under distributional shift remain poorly understood. In this work, we provide a principled statistical interpretation by proving that EDL training corresponds to amortized variational inference in a hierarchical Bayesian model with a tempered pseudo-likelihood. This perspective reveals a major drawback: standard EDL conflates epistemic and aleatoric uncertainty, leading to systematic overconfidence on out-of-distribution (OOD) inputs. To address this, we introduce Density-Informed Pseudo-count EDL (DIP-EDL), a new parametrization that decouples class prediction from the magnitude of uncertainty by separately estimating the conditional label distribution and the marginal covariate density. This separation preserves evidence in high-density regions while shrinking predictions toward a uniform prior for OOD data. Theoretically, we prove that DIP-EDL achieves asymptotic concentration. Empirically, we show that our method enhances interpretability and improves robustness and uncertainty calibration under distributional shift.

</details>


### [393] [Inference-Aware Meta-Alignment of LLMs via Non-Linear GRPO](https://arxiv.org/abs/2602.01603)
*Shokichi Takakura,Akifumi Wachi,Rei Higuchi,Kohei Miyaguchi,Taiji Suzuki*

Main category: stat.ML

TL;DR: 提出IAMA方法，使大语言模型能在推理时以有限计算预算对齐多个标准，通过非线性GRPO算法解决优化问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型对齐多样化人类偏好具有挑战性，因为标准常相互冲突。推理时对齐方法虽能通过不同算法对齐多个标准，但计算成本高，需要多次前向传播。

Method: 提出推理感知元对齐(IAMA)方法，训练基础模型使其能通过不同推理时对齐算法有效对齐多个任务。使用非线性GRPO算法解决IAMA中的非线性优化问题，该算法在概率测度空间中可证明收敛到最优解。

Result: IAMA方法使LLMs能在推理时以有限计算预算对齐多个标准，非线性GRPO算法提供了理论保证。

Conclusion: IAMA是一种新颖有效的方法，解决了推理时对齐的计算效率问题，使大语言模型能更高效地适应多样化人类偏好。

Abstract: Aligning large language models (LLMs) to diverse human preferences is fundamentally challenging since criteria can often conflict with each other. Inference-time alignment methods have recently gained popularity as they allow LLMs to be aligned to multiple criteria via different alignment algorithms at inference time. However, inference-time alignment is computationally expensive since it often requires multiple forward passes of the base model. In this work, we propose inference-aware meta-alignment (IAMA), a novel approach that enables LLMs to be aligned to multiple criteria with limited computational budget at inference time. IAMA trains a base model such that it can be effectively aligned to multiple tasks via different inference-time alignment algorithms. To solve the non-linear optimization problems involved in IAMA, we propose non-linear GRPO, which provably converges to the optimal solution in the space of probability measures.

</details>


### [394] [ST-BCP: Tightening Coverage Bound for Backward Conformal Prediction via Non-Conformity Score Transformation](https://arxiv.org/abs/2602.01733)
*Junxian Liu,Hao Zeng,Hongxin Wei*

Main category: stat.ML

TL;DR: ST-BCP方法通过数据依赖的非一致性分数变换，缩小了反向共形预测中的覆盖差距，将平均覆盖差距从4.20%降低到1.12%。


<details>
  <summary>Details</summary>
Motivation: 传统共形预测(CP)能保证覆盖度但无法控制预测集大小，反向共形预测(BCP)能控制集大小但使用马尔可夫不等式导致估计覆盖度与实际覆盖度存在显著差距。

Method: 提出ST-BCP方法，引入数据依赖的非一致性分数变换，开发可计算的变换函数，并证明其优于基线恒等变换。

Result: 在常见基准测试中，平均覆盖差距从4.20%显著降低到1.12%，证明了方法的有效性。

Conclusion: ST-BCP通过数据依赖的分数变换有效缩小了反向共形预测中的覆盖差距，为不确定性量化提供了更精确的统计框架。

Abstract: Conformal Prediction (CP) provides a statistical framework for uncertainty quantification that constructs prediction sets with coverage guarantees. While CP yields uncontrolled prediction set sizes, Backward Conformal Prediction (BCP) inverts this paradigm by enforcing a predefined upper bound on set size and estimating the resulting coverage guarantee. However, the looseness induced by Markov's inequality within the BCP framework causes a significant gap between the estimated coverage bound and the empirical coverage. In this work, we introduce ST-BCP, a novel method that introduces a data-dependent transformation of nonconformity scores to narrow the coverage gap. In particular, we develop a computable transformation and prove that it outperforms the baseline identity transformation. Extensive experiments demonstrate the effectiveness of our method, reducing the average coverage gap from 4.20\% to 1.12\% on common benchmarks.

</details>


### [395] [Transformers as Measure-Theoretic Associative Memory: A Statistical Perspective and Minimax Optimality](https://arxiv.org/abs/2602.01863)
*Ryotaro Kawata,Taiji Suzuki*

Main category: stat.ML

TL;DR: 论文提出将Transformer中的注意力机制重新解释为概率测度上的积分算子，建立了一个理论框架来分析Transformer如何从分布式上下文中进行关联记忆和预测。


<details>
  <summary>Details</summary>
Motivation: Transformer通过内容寻址检索和利用理论上无限长上下文的能力表现出色，但缺乏理论分析。作者希望建立一个理论框架来理解Transformer如何从分布式上下文中进行关联记忆，并提供可证明的泛化保证。

Method: 将关联记忆重新表述为概率测度层面，将上下文视为token上的分布，将注意力视为测度上的积分算子。研究通过经验风险最小化训练的softmax注意力，分析浅层测度论Transformer与MLP组合如何学习"回忆-预测"映射。

Result: 在输入密度的谱假设下，证明了浅层测度论Transformer能够学习回忆-预测映射。建立了匹配的极小极大下界，证明了收敛阶的尖锐性（达到乘法常数）。

Conclusion: 该框架为设计和分析能够从任意长分布式上下文中回忆的Transformer提供了原则性方法，并提供了可证明的泛化保证，为理解Transformer的关联记忆能力奠定了理论基础。

Abstract: Transformers excel through content-addressable retrieval and the ability to exploit contexts of, in principle, unbounded length. We recast associative memory at the level of probability measures, treating a context as a distribution over tokens and viewing attention as an integral operator on measures. Concretely, for mixture contexts $ν= I^{-1} \sum_{i=1}^I μ^{(i^*)}$ and a query $x_{\mathrm{q}}(i^*)$, the task decomposes into (i) recall of the relevant component $μ^{(i^*)}$ and (ii) prediction from $(μ_{i^*},x_\mathrm{q})$. We study learned softmax attention (not a frozen kernel) trained by empirical risk minimization and show that a shallow measure-theoretic Transformer composed with an MLP learns the recall-and-predict map under a spectral assumption on the input densities. We further establish a matching minimax lower bound with the same rate exponent (up to multiplicative constants), proving sharpness of the convergence order. The framework offers a principled recipe for designing and analyzing Transformers that recall from arbitrarily long, distributional contexts with provable generalization guarantees.

</details>


### [396] [Privacy Amplification by Missing Data](https://arxiv.org/abs/2602.01928)
*Simon Roburin,Rafaël Pinot,Erwan Scornet*

Main category: stat.ML

TL;DR: 论文提出缺失数据可作为隐私增强机制，在差分隐私框架下分析缺失数据如何提供隐私放大效果


<details>
  <summary>Details</summary>
Motivation: 在医疗和金融等高风险领域，隐私保护是基本要求，但这些领域的数据常存在缺失值。传统上缺失数据被视为限制因素，但本文从隐私保护角度重新审视缺失数据，认为缺失性可能天然增强隐私

Method: 在差分隐私框架下，将缺失数据形式化为隐私放大机制，分析不完全数据如何为差分隐私算法提供隐私放大效果

Result: 首次证明不完全数据能够为差分隐私算法产生隐私放大效果，即缺失数据可以增强隐私保护

Conclusion: 缺失数据不应仅被视为限制因素，而应被重新评估为有价值的隐私保护机制，为高敏感领域的数据分析提供了新的隐私保护视角

Abstract: Privacy preservation is a fundamental requirement in many high-stakes domains such as medicine and finance, where sensitive personal data must be analyzed without compromising individual confidentiality. At the same time, these applications often involve datasets with missing values due to non-response, data corruption, or deliberate anonymization. Missing data is traditionally viewed as a limitation because it reduces the information available to analysts and can degrade model performance. In this work, we take an alternative perspective and study missing data from a privacy preservation standpoint. Intuitively, when features are missing, less information is revealed about individuals, suggesting that missingness could inherently enhance privacy. We formalize this intuition by analyzing missing data as a privacy amplification mechanism within the framework of differential privacy. We show, for the first time, that incomplete data can yield privacy amplification for differentially private algorithms.

</details>


### [397] [Stochastic Interpolants in Hilbert Spaces](https://arxiv.org/abs/2602.01988)
*James Boran Yu,RuiKang OuYang,Julien Horwood,José Miguel Hernández-Lobato*

Main category: stat.ML

TL;DR: 本文建立了无限维希尔伯特空间中随机插值的理论框架，解决了扩散模型在函数值数据中的应用限制，实现了任意函数分布之间的生成桥接。


<details>
  <summary>Details</summary>
Motivation: 虽然扩散模型已成功扩展到函数值数据，但随机插值（提供连接任意分布的灵活方法）仍局限于有限维设置。本文旨在填补这一空白，建立无限维希尔伯特空间中随机插值的理论框架。

Method: 建立无限维希尔伯特空间中随机插值的严格理论框架，包括适定性证明和显式误差界。将框架应用于条件生成，特别关注复杂的基于PDE的基准测试。

Result: 提出的框架实现了任意函数分布之间的生成桥接，在条件生成任务中取得了最先进的结果，特别是在复杂的PDE基准测试上表现出色。

Conclusion: 该工作为无限维空间中的随机插值提供了全面的理论基础，使其成为科学发现的强大通用工具，能够桥接任意函数分布并实现最先进的生成性能。

Abstract: Although diffusion models have successfully extended to function-valued data, stochastic interpolants -- which offer a flexible way to bridge arbitrary distributions -- remain limited to finite-dimensional settings. This work bridges this gap by establishing a rigorous framework for stochastic interpolants in infinite-dimensional Hilbert spaces. We provide comprehensive theoretical foundations, including proofs of well-posedness and explicit error bounds. We demonstrate the effectiveness of the proposed framework for conditional generation, focusing particularly on complex PDE-based benchmarks. By enabling generative bridges between arbitrary functional distributions, our approach achieves state-of-the-art results, offering a powerful, general-purpose tool for scientific discovery.

</details>


### [398] [Training-free score-based diffusion for parameter-dependent stochastic dynamical systems](https://arxiv.org/abs/2602.02113)
*Minglei Yang,Sicheng He*

Main category: stat.ML

TL;DR: 提出无需训练的扩散模型框架，用于学习参数依赖SDE的随机流映射，通过核加权蒙特卡洛估计器近似条件得分函数，实现参数空间插值。


<details>
  <summary>Details</summary>
Motivation: 模拟参数依赖随机微分方程计算成本高，现有机器学习方法要么需要昂贵的神经网络训练来估计得分函数，要么无法处理连续参数依赖。

Method: 提出训练免费的条件扩散模型框架，使用联合核加权蒙特卡洛估计器近似条件得分函数，利用离散参数值采样的轨迹数据，实现状态空间和连续参数域的插值。

Result: 模型训练后可为训练范围内的任意参数值生成样本轨迹，无需重新训练，显著加速参数研究、不确定性量化和实时滤波应用，三个数值实验验证了方法的有效性。

Conclusion: 该方法成功解决了参数依赖SDE的模拟挑战，提供了一种高效、准确的参数空间插值方案，适用于各种科学计算和工程应用。

Abstract: Simulating parameter-dependent stochastic differential equations (SDEs) presents significant computational challenges, as separate high-fidelity simulations are typically required for each parameter value of interest. Despite the success of machine learning methods in learning SDE dynamics, existing approaches either require expensive neural network training for score function estimation or lack the ability to handle continuous parameter dependence. We present a training-free conditional diffusion model framework for learning stochastic flow maps of parameter-dependent SDEs, where both drift and diffusion coefficients depend on physical parameters. The key technical innovation is a joint kernel-weighted Monte Carlo estimator that approximates the conditional score function using trajectory data sampled at discrete parameter values, enabling interpolation across both state space and the continuous parameter domain. Once trained, the resulting generative model produces sample trajectories for any parameter value within the training range without retraining, significantly accelerating parameter studies, uncertainty quantification, and real-time filtering applications. The performance of the proposed approach is demonstrated via three numerical examples of increasing complexity, showing accurate approximation of conditional distributions across varying parameter values.

</details>


### [399] [Learning Beyond the Gaussian Data: Learning Dynamics of Neural Networks on an Expressive and Cumulant-Controllable Data Model](https://arxiv.org/abs/2602.02153)
*Onat Ure,Samet Demir,Zafer Dogan*

Main category: stat.ML

TL;DR: 论文提出了一种可控制高阶统计量的非高斯数据模型，用于研究神经网络学习动态中数据高阶统计量的影响，揭示了网络从低阶到高阶统计量的渐进学习过程。


<details>
  <summary>Details</summary>
Motivation: 研究数据高阶统计量对神经网络学习动态的影响，填补简化数据假设与实际数据复杂性之间的空白，为机器学习中的分布效应提供理论框架。

Method: 使用基于Hermite多项式展开的生成式两层神经网络构建数据模型，通过Hermite系数控制偏度和峰度等高阶累积量，然后使用该模型生成的样本进行在线学习实验。

Result: 实验显示神经网络学习呈现"矩级"进展：首先学习均值和协方差等低阶统计量，然后逐步学习高阶累积量。在Fashion-MNIST数据集上的预训练实验进一步验证了这一结论。

Conclusion: 提出的方法在简化数据假设和实际数据复杂性之间架起了桥梁，为研究机器学习和信号处理中的分布效应提供了原则性框架。

Abstract: We study the effect of high-order statistics of data on the learning dynamics of neural networks (NNs) by using a moment-controllable non-Gaussian data model. Considering the expressivity of two-layer neural networks, we first construct the data model as a generative two-layer NN where the activation function is expanded by using Hermite polynomials. This allows us to achieve interpretable control over high-order cumulants such as skewness and kurtosis through the Hermite coefficients while keeping the data model realistic. Using samples generated from the data model, we perform controlled online learning experiments with a two-layer NN. Our results reveal a moment-wise progression in training: networks first capture low-order statistics such as mean and covariance, and progressively learn high-order cumulants. Finally, we pretrain the generative model on the Fashion-MNIST dataset and leverage the generated samples for further experiments. The results of these additional experiments confirm our conclusions and show the utility of the data model in a real-world scenario. Overall, our proposed approach bridges simplified data assumptions and practical data complexity, which offers a principled framework for investigating distributional effects in machine learning and signal processing.

</details>


### [400] [PCA of probability measures: Sparse and Dense sampling regimes](https://arxiv.org/abs/2602.02190)
*Gachon Erell,Jérémie Bigot,Elsa Cazelles*

Main category: stat.ML

TL;DR: 本文研究概率测度的主成分分析（PCA），在双重渐近框架下（n个测度，每个测度有m个样本），推导出协方差算子和PCA超额风险的收敛速率，揭示从稀疏到稠密状态的转变，并证明稠密状态下的速率是极小极大最优的。


<details>
  <summary>Details</summary>
Motivation: 现有文献对单个概率测度嵌入的收敛速率已有深入研究，但对于多个测度的情况尚未探讨。本文旨在研究当观测到n个概率测度（每个测度通过m个样本观测）时，PCA的收敛行为，特别是分析测度数量n和每个测度的样本数m之间的关系对收敛速率的影响。

Method: 采用双重渐近框架，将概率测度嵌入希尔伯特空间，然后应用标准函数型PCA技术。理论分析推导出协方差算子和PCA超额风险的收敛速率，形式为n^{-1/2} + m^{-α}，其中α>0取决于嵌入选择。通过数值实验验证理论结果，并展示适当子采样在保持PCA精度的同时降低计算成本。

Result: 获得了收敛速率n^{-1/2} + m^{-α}，揭示了从稀疏（小m）到稠密（大m）状态的转变。证明了稠密状态下经验协方差误差的速率是极小极大最优的。数值实验验证了理论速率，并显示适当子采样能有效平衡精度和计算效率。

Conclusion: 本文首次在双重渐近框架下分析了概率测度PCA的收敛行为，揭示了测度数量n和样本数m之间的权衡关系。理论结果为实际应用中如何分配资源（增加测度数量vs增加每个测度的样本数）提供了指导，并证明了稠密状态下的最优性。子采样策略为大规模计算提供了实用解决方案。

Abstract: A common approach to perform PCA on probability measures is to embed them into a Hilbert space where standard functional PCA techniques apply. While convergence rates for estimating the embedding of a single measure from $m$ samples are well understood, the literature has not addressed the setting involving multiple measures. In this paper, we study PCA in a double asymptotic regime where $n$ probability measures are observed, each through $m$ samples. We derive convergence rates of the form $n^{-1/2} + m^{-α}$ for the empirical covariance operator and the PCA excess risk, where $α>0$ depends on the chosen embedding. This characterizes the relationship between the number $n$ of measures and the number $m$ of samples per measure, revealing a sparse (small $m$) to dense (large $m$) transition in the convergence behavior. Moreover, we prove that the dense-regime rate is minimax optimal for the empirical covariance error. Our numerical experiments validate these theoretical rates and demonstrate that appropriate subsampling preserves PCA accuracy while reducing computational cost.

</details>


### [401] [Transfer Learning Through Conditional Quantile Matching](https://arxiv.org/abs/2602.02358)
*Yikun Zhang,Steven Wilkins-Reeves,Wesley Lee,Aude Hofleitner*

Main category: stat.ML

TL;DR: 提出一种用于回归任务的迁移学习框架，通过异质源域提升数据稀缺目标域的预测性能，使用条件生成模型和分位数匹配进行分布对齐


<details>
  <summary>Details</summary>
Motivation: 解决数据稀缺目标域中预测性能不足的问题，传统方法通常假设严格的分布偏移（如协变量偏移或标签偏移），而实际应用中源域和目标域之间可能存在更复杂的分布差异

Method: 为每个源域分别学习条件生成模型，然后通过条件分位数匹配将生成的响应校准到目标域，进行分布对齐而不需要限制性假设，为下游学习任务提供高质量数据增强

Result: 理论证明：在增强数据集上训练的ERM比仅使用目标数据的ERM具有更紧的过剩风险界；实践验证：广泛的模拟和真实数据应用显示，该方法在预测准确性上持续优于仅目标域学习和竞争性迁移学习方法

Conclusion: 该框架提供了一种原则性且灵活的方法，通过异质源域进行高质量数据增强，有效提升数据稀缺目标域的回归预测性能，在理论和实践上均表现出优越性

Abstract: We introduce a transfer learning framework for regression that leverages heterogeneous source domains to improve predictive performance in a data-scarce target domain. Our approach learns a conditional generative model separately for each source domain and calibrates the generated responses to the target domain via conditional quantile matching. This distributional alignment step corrects general discrepancies between source and target domains without imposing restrictive assumptions such as covariate or label shift. The resulting framework provides a principled and flexible approach to high-quality data augmentation for downstream learning tasks in the target domain. From a theoretical perspective, we show that an empirical risk minimizer (ERM) trained on the augmented dataset achieves a tighter excess risk bound than the target-only ERM under mild conditions. In particular, we establish new convergence rates for the quantile matching estimator that governs the transfer bias-variance tradeoff. From a practical perspective, extensive simulations and real data applications demonstrate that the proposed method consistently improves prediction accuracy over target-only learning and competing transfer learning methods.

</details>


### [402] [Provably Data-driven Multiple Hyper-parameter Tuning with Structured Loss Function](https://arxiv.org/abs/2602.02406)
*Tung Quoc Le,Anh Tuan Nguyen,Viet Anh Nguyen*

Main category: stat.ML

TL;DR: 提出首个多维超参数调优的泛化保证框架，利用实代数几何工具改进半代数函数类的泛化保证，并应用于验证损失下的超参数调优，展示了在加权群套索和加权融合套索中的新可学习性结果。


<details>
  <summary>Details</summary>
Motivation: 数据驱动算法设计中的超参数调优统计基础有限，现有保证仅适用于一维超参数，而实际应用中多维超参数调优问题尚未解决，需要建立更通用的理论框架。

Method: 利用实代数几何工具加强半代数函数类的泛化保证框架，扩展到验证损失下的多维超参数调优，并在有额外结构时推导改进的边界。

Result: 建立了首个多维超参数调优的泛化保证通用框架，获得了更尖锐、更广泛适用的保证，并展示了在加权群套索和加权融合套索中的新可学习性结果。

Conclusion: 该框架填补了多维超参数调优理论基础的空白，为数据驱动算法设计提供了更坚实的统计保证，具有广泛的实际应用价值。

Abstract: Data-driven algorithm design automates hyperparameter tuning, but its statistical foundations remain limited because model performance can depend on hyperparameters in implicit and highly non-smooth ways. Existing guarantees focus on the simple case of a one-dimensional (scalar) hyperparameter. This leaves the practically important, multi-dimensional hyperparameter tuning setting unresolved. We address this open question by establishing the first general framework for establishing generalization guarantees for tuning multi-dimensional hyperparameters in data-driven settings. Our approach strengthens the generalization guarantee framework for semi-algebraic function classes by exploiting tools from real algebraic geometry, yielding sharper, more broadly applicable guarantees. We then extend the analysis to hyperparameter tuning using the validation loss under minimal assumptions, and derive improved bounds when additional structure is available. Finally, we demonstrate the scope of the framework with new learnability results, including data-driven weighted group lasso and weighted fused lasso.

</details>


### [403] [Full-Batch Gradient Descent Outperforms One-Pass SGD: Sample Complexity Separation in Single-Index Learning](https://arxiv.org/abs/2602.02431)
*Filip Kovačević,Hong Chang Ji,Denny Wu,Mahdi Soltanolkotabi,Marco Mondelli*

Main category: stat.ML

TL;DR: 本文证明在单索引模型学习中，全批次梯度下降通过截断激活函数可以在n≈d样本量下优于单次随机梯度下降，后者需要n≳d log d样本量。


<details>
  <summary>Details</summary>
Motivation: 研究在非线性模型（单索引模型）中，全批次梯度下降（重复使用数据）是否比单次随机梯度下降（每个数据点只用一次）具有统计效率优势，特别是能否消除log d因子。

Method: 研究d维单索引模型（带二次激活函数），分析全批次球面梯度下降在相关损失上的表现，通过截断激活函数改善优化景观，并对小初始化下的平方损失进行轨迹分析。

Result: 1. 全批次球面梯度下降在相关损失上仍需n≳d log d样本量；2. 但截断激活函数后，全批次梯度下降在n≈d样本量下就能获得有利的优化景观；3. 小初始化下，n≳d样本和T≳log d梯度步数足以实现强恢复。

Conclusion: 通过截断激活函数，全批次梯度下降可以在统计效率上优于单次随机梯度下降，消除log d因子，在n≈d样本量下实现有效学习。

Abstract: It is folklore that reusing training data more than once can improve the statistical efficiency of gradient-based learning. However, beyond linear regression, the theoretical advantage of full-batch gradient descent (GD, which always reuses all the data) over one-pass stochastic gradient descent (online SGD, which uses each data point only once) remains unclear. In this work, we consider learning a $d$-dimensional single-index model with a quadratic activation, for which it is known that one-pass SGD requires $n\gtrsim d\log d$ samples to achieve weak recovery. We first show that this $\log d$ factor in the sample complexity persists for full-batch spherical GD on the correlation loss; however, by simply truncating the activation, full-batch GD exhibits a favorable optimization landscape at $n \simeq d$ samples, thereby outperforming one-pass SGD (with the same activation) in statistical efficiency. We complement this result with a trajectory analysis of full-batch GD on the squared loss from small initialization, showing that $n \gtrsim d$ samples and $T \gtrsim\log d$ gradient steps suffice to achieve strong (exact) recovery.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [404] [AI in Debt Collection: Estimating the Psychological Impact on Consumers](https://arxiv.org/abs/2602.00050)
*Minou Goetze,Sebastian Clajus,Stephan Stricker*

Main category: cs.CY

TL;DR: 研究比较人工与AI催收沟通对消费者心理行为的影响，发现人工沟通更公平、更能引发互惠，AI沟通更高效但同理心较差，信任无差异，AI能减少污名感但需谨慎用于需要高同理心或公平敏感的场景。


<details>
  <summary>Details</summary>
Motivation: 随着AI在金融服务中的广泛应用，特别是在敏感的债务催收领域，需要了解AI沟通如何影响消费者的心理反应和行为倾向，以平衡技术效率与人际关系敏感性。

Method: 采用大规模实验设计（n=3514），在11个欧洲国家比较人工与AI中介沟通，测量消费者的社会偏好（公平、信任、互惠、效率）和社会情感（污名、同理心）。

Result: 参与者认为人工互动更公平、更能引发互惠，AI沟通被视为更高效；信任无差异。人工接触引发更强同理心，但也带来更强的污名感。探索性分析显示性别、年龄和文化背景存在显著差异。

Conclusion: AI沟通能提高效率并减少污名感而不损害信任，但在需要高同理心或对公平敏感的情况下应谨慎使用。研究为平衡技术有效性与人际意识提供了设计沟通策略的指导。

Abstract: The present study investigates the psychological and behavioral implications of integrating AI into debt collection practices using data from eleven European countries. Drawing on a large-scale experimental design (n = 3514) comparing human versus AI-mediated communication, we examine effects on consumers' social preferences (fairness, trust, reciprocity, efficiency) and social emotions (stigma, empathy). Participants perceive human interactions as more fair and more likely to elicit reciprocity, while AI-mediated communication is viewed as more efficient; no differences emerge in trust. Human contact elicits greater empathy, but also stronger feelings of stigma. Exploratory analyses reveal notable variation between gender, age groups, and cultural contexts. In general, the findings suggest that AI-mediated communication can improve efficiency and reduce stigma without diminishing trust, but should be used carefully in situations that require high empathy or increased sensitivity to fairness. The study advances our understanding of how AI influences the psychological dynamics in sensitive financial interactions and informs the design of communication strategies that balance technological effectiveness with interpersonal awareness.

</details>


### [405] [Beyond Static Question Banks: Dynamic Knowledge Expansion via LLM-Automated Graph Construction and Adaptive Generation](https://arxiv.org/abs/2602.00020)
*Yingquan Wang,Tianyu Wei,Qinsi Li,Li Zeng*

Main category: cs.CY

TL;DR: 提出Generative GraphRAG框架，通过Auto-HKG自动构建层次知识图谱，结合CG-RAG进行图推理生成个性化练习题，解决教育知识图谱构建成本高和个性化不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有教育系统面临两大挑战：1) 知识图谱构建依赖人工，成本高且扩展性差；2) 缺乏对学习者知识状态的系统性推理，依赖静态题库，个性化程度有限。

Method: 提出Generative GraphRAG框架，包含两个核心模块：1) Auto-HKG利用LLM从教育资源自动构建层次知识图谱；2) CG-RAG结合学习者掌握图谱进行图推理，并与检索增强生成结合生成个性化练习题。

Result: 框架已在真实教育场景中部署，获得积极的用户反馈，显示出支持实际个性化教育系统的潜力。

Conclusion: Generative GraphRAG框架通过自动化知识建模和个性化练习生成，有效解决了教育知识图谱构建和个性化适配的挑战，为实际个性化教育系统提供了可行方案。

Abstract: Personalized education systems increasingly rely on structured knowledge representations to support adaptive learning and question generation. However, existing approaches face two fundamental limitations. First, constructing and maintaining knowledge graphs for educational content largely depends on manual curation, resulting in high cost and poor scalability. Second, most personalized education systems lack effective support for state-aware and systematic reasoning over learners' knowledge, and therefore rely on static question banks with limited adaptability. To address these challenges, this paper proposes a Generative GraphRAG framework for automated knowledge modeling and personalized exercise generation. It consists of two core modules. The first module, Automated Hierarchical Knowledge Graph Constructor (Auto-HKG), leverages LLMs to automatically construct hierarchical knowledge graphs that capture structured concepts and their semantic relations from educational resources. The second module, Cognitive GraphRAG (CG-RAG), performs graph-based reasoning over a learner mastery graph and combines it with retrieval-augmented generation to produce personalized exercises that adapt to individual learning states. The proposed framework has been deployed in real-world educational scenarios, where it receives favorable user feedback, suggesting its potential to support practical personalized education systems.

</details>


### [406] [Early Warning Signals Appear Long Before Dropping Out: An Idiographic Approach Grounded in Complex Dynamic Systems Theory](https://arxiv.org/abs/2602.00021)
*Mohammed Saqr,Sonsoles López-Pernas,Santtu Tikka,Markus Wolfgang Hermann Spitzer*

Main category: cs.CY

TL;DR: 研究发现临界减速现象可作为学生学习投入度下降的早期预警信号，提前预测辍学风险


<details>
  <summary>Details</summary>
Motivation: 学生的韧性（持续投入和从挫折中恢复的能力）对学习至关重要。当韧性减弱时，学生面临脱离学习和辍学的风险。因此，在"希望窗口期"提前预测脱离学习非常重要。

Method: 使用9411名学生在数字数学学习环境中的167万次练习尝试数据，计算临界减速指标：自相关、恢复率、方差、偏度、峰度和变异系数。

Result: 88.2%的学生在脱离学习前表现出临界减速信号，警告信号集中在活动后期和练习停止（辍学）之前。这是教育领域首次发现临界减速现象的证据。

Conclusion: 临界减速指标为早期检测学生脆弱性提供了实用工具，这些指标具有普遍性，独立于数据生成机制，为跨情境、数据类型和学习环境的可移植性提供了新机会。

Abstract: The ability to sustain engagement and recover from setbacks (i.e., resilience) -- is fundamental for learning. When resilience weakens, students are at risk of disengagement and may drop out and miss on opportunities. Therefore, predicting disengagement long before it happens during the window of hope is important. In this article, we test whether early warning signals of resilience loss, grounded in the concept of critical slowing down (CSD) can forecast disengagement before dropping out. CSD has been widely observed across ecological, climate, and neural systems, where it precedes tipping points into catastrophic failure (dropping out in our case). Using 1.67 million practice attempts from 9,401 students who used a digital math learning environment, we computed CSD indicators: autocorrelation, return rate, variance, skewness, kurtosis, and coefficient of variation. We found that 88.2% of students exhibited CSD signals prior to disengagement, with warnings clustering late in activity and before practice ceased (dropping out). Our results provide the first evidence of CSD in education, suggesting that universal resilience dynamics also govern social systems such as human learning. These findings offer a practical indicator for early detection of vulnerability and supporting learners across different applications and contexts long before critical events happen. Most importantly, CSD indicators arise universally, independent of the mechanisms that generate the data, offering new opportunities for portability across contexts, data types, and learning environments.

</details>


### [407] [Groundwater vulnerability assessment in semi-arid regions using GIS-based DRASTIC models and FUZZY AHP: South Chott Hodna](https://arxiv.org/abs/2602.00023)
*Lakhdar Seraiche,Mostafa Dougha,Messaoud Ghodbane,Tahar Selmane,Ahmed Ferhati,Djamal Eddine Djemiat*

Main category: cs.CY

TL;DR: 本研究提出了一种混合地下水脆弱性评估框架，通过整合土地利用数据和先进加权技术（AHP和模糊AHP）改进传统DRASTIC模型，提高了干旱地区地下水风险评估的准确性。


<details>
  <summary>Details</summary>
Motivation: 干旱地区地下水脆弱性是全球关注的重要问题，人口增长和集约农业增加了地下水枯竭和污染风险。传统DRASTIC模型存在主观性强、未能充分考虑人为影响等局限性，需要改进以适应数据有限和复杂情况。

Method: 提出混合评估框架：1) 整合土地利用数据到DRASTIC模型；2) 应用层次分析法(AHP)及其模糊逻辑变体(Fuzzy AHP)进行参数加权；3) 使用GIS生成四种脆弱性图：DRASTIC、DRASTIC_LU、AHP DRASTIC_LU、Fuzzy AHP DRASTIC_LU；4) 利用70口井的硝酸盐数据进行验证。

Result: 研究发现农业区（特别是冲积含水层上方）最脆弱。ROC曲线分析显示模型性能逐步提升：AUC值分别为DRASTIC(0.812)、DRASTIC_LU(0.864)、AHP DRASTIC_LU(0.875)、Fuzzy AHP DRASTIC_LU(0.951)。模糊AHP DRASTIC_LU模型显著提高了地下水风险评估准确性。

Conclusion: 基于GIS的混合模型提供了可扩展、可转移的脆弱性制图方法，为地方和区域水资源管理者提供了实用信息。模糊AHP DRASTIC_LU框架显著改善了地下水风险评估，特别适用于数据有限的干旱地区。

Abstract: Groundwater vulnerability is a major concern in arid regions worldwide, where population growth and intensive agriculture increase the risks of depletion and contamination. This study proposes a hybrid groundwater vulnerability assessment framework that improves the conventional DRASTIC model by integrating land-use data and applying advanced weighting techniques, namely the Analytical Hierarchy Process (AHP) and its fuzzy logic variant (Fuzzy AHP). This method makes expert-based weighting less subjective, better captures anthropogenic effects, and facilitates adaptation to challenging situations and limited data. Four vulnerability maps were produced using Geographic Information Systems (GIS): DRASTIC, DRASTIC_LU, AHP DRASTIC_LU, and Fuzzy AHP DRASTIC_LU. We used nitrate levels from 70 wells to verify our work. We found that agricultural areas, especially those above the alluvial aquifer, were the most vulnerable. The ROC curve analysis showed that the model improved over time, with the area under the curve (AUC) values of 0.812 for DRASTIC, 0.864 for DRASTIC_LU, 0.875 for AHP DRASTIC_LU, and 0.951 for fuzzy AHP DRASTIC_LU. These results show that fuzzy AHP DRASTIC_LU makes groundwater risk assessments much more. The GIS-based hybrid models offer a scalable and transferable method for mapping vulnerability, but they also provide local and regional water resource managers with useful information.

</details>


### [408] [Strategies for Creating Uncertainty in the AI Era to Trigger Students Critical Thinking: Pedagogical Design, Assessment Rubric, and Exam System](https://arxiv.org/abs/2602.00026)
*Ahmad Samer Wazan*

Main category: cs.CY

TL;DR: 论文提出通过AI创造不确定性情境来促进批判性思维的教学方法，开发了MindMosaicAIExam考试系统，将可控AI工具融入评估过程，要求学生批判性评估AI输出并迭代完善推理。


<details>
  <summary>Details</summary>
Motivation: 生成式AI让学生能直接获得正确答案而无需展示理解过程，挑战了传统评估方式。与其禁止AI，本文主张将AI融入教育，利用AI模型创造不确定性情境，以不确定性作为促进批判性思维的核心教学概念。

Method: 基于认识论和批判性思维研究，围绕AI模型和教师的固有局限性设计学习活动和评估。通过显式控制AI在考试中的行为（如阻止直接答案或生成看似合理但有缺陷的回应），防止AI成为获取确定性的捷径。开发了MindMosaicAIExam考试系统，整合可控AI工具，要求学生提供初始答案、批判性评估AI输出并迭代完善推理。

Result: 提出了MindMosaicAIExam考试系统，该系统整合了可控AI工具，能够收集学生的推理过程。同时设计了基于系统收集的学生推理成果来评估批判性思维的评价标准。

Conclusion: 通过利用AI创造不确定性情境，结合思维导向的教学方法，可以有效促进学生的批判性思维发展。这种方法将AI的局限性转化为教学机会，鼓励学生进行推理、质疑和论证，而不仅仅是获取正确答案。

Abstract: Generative AI challenges traditional assessments by allowing students to produce correct answers without demonstrating understanding or reasoning. Rather than prohibiting AI, this work argues that one way to integrate AI into education is by creating uncertain situations with the help of AI models and using thinking-oriented teaching approaches, where uncertainty is a central pedagogical concept for stimulating students critical thinking. Drawing on epistemology and critical thinking research studies, we propose designing learning activities and assessments around the inherent limitations of both AI models and instructors. This encourages students to reason, question, and justify their final answers. We show how explicitly controlling AI behavior during exams (such as preventing direct answers or generating plausible but flawed responses) prevents AI from becoming a shortcut to certainty. To support this pedagogy, we introduce MindMosaicAIExam, an exam system that integrates controllable AI tools and requires students to provide initial answers, critically evaluate AI outputs, and iteratively refine their reasoning. We also present an evaluation rubric designed to assess critical thinking based on students reasoning artifacts collected by the exam system.

</details>


### [409] [Happy Young Women, Grumpy Old Men? Emotion-Driven Demographic Biases in Synthetic Face Generation](https://arxiv.org/abs/2602.00032)
*Mengting Wei,Aditya Gulati,Guoying Zhao,Nuria Oliver*

Main category: cs.CY

TL;DR: 该研究系统审计了8个最先进的文本到图像模型（4个西方模型和4个中国模型）在生成人脸时的偏见，发现所有模型都存在持续的人口统计和情感条件偏见，无论其来源国如何。


<details>
  <summary>Details</summary>
Motivation: 尽管文本到图像模型能够从自然语言提示生成高保真人脸图像，但这些模型的偏见、表征质量和跨文化一致性仍然未被充分理解。现有研究主要关注人口统计偏见，但缺乏关于情感提示如何影响人口统计表征以及不同文化和语言背景下训练的模型输出分布差异的研究。

Method: 对8个最先进的T2I模型（4个西方组织和4个中国机构开发）进行系统审计，使用相同的提示生成人脸。采用最先进的面部分析算法估计生成人脸的性别、种族、年龄和吸引力水平。应用信息论偏见度量（包括Kullback-Leibler和Jensen-Shannon散度）来衡量与全球人口统计数据的偏差。

Result: 研究发现所有模型无论其来源国如何，都存在持续的人口统计和情感条件偏见。西方和中国模型都显示出系统性偏见，表明这些偏见在跨文化模型中普遍存在。

Conclusion: 研究结果对公平性、社会技术危害、治理和透明生成系统的开发具有重要意义。需要更严格的偏见评估和缓解策略来确保生成AI系统的公平性和代表性。

Abstract: Synthetic face generation has rapidly advanced with the emergence of text-to-image (T2I) and of multimodal large language models, enabling high-fidelity image production from natural-language prompts. Despite the widespread adoption of these tools, the biases, representational quality, and cross-cultural consistency of these models remain poorly understood. Prior research on biases in the synthetic generation of human faces has examined demographic biases, yet there is little research on how emotional prompts influence demographic representation and how models trained in different cultural and linguistic contexts vary in their output distributions. We present a systematic audit of eight state-of-the-art T2I models comprising four models developed by Western organizations and four developed by Chinese institutions, all prompted identically. Using state-of-the-art facial analysis algorithms, we estimate the gender, race, age, and attractiveness levels in the generated faces. To measure the deviations from global population statistics, we apply information-theoretic bias metrics including Kullback-Leibler and Jensen-Shannon divergences. Our findings reveal persistent demographic and emotion-conditioned biases in all models regardless of their country of origin. We discuss implications for fairness, socio-technical harms, governance, and the development of transparent generative systems.

</details>


### [410] [Mapping the Stochastic Penal Colony](https://arxiv.org/abs/2602.00033)
*Robert Grimm*

Main category: cs.CY

TL;DR: 该论文重新审视内容审核的惩罚性方面，提出"随机惩罚殖民地"概念，结合自动民族志和程序正义方法，分析Twitter、OpenAI DALL-E 2和Pinterest三个案例中的账户暂停威胁。


<details>
  <summary>Details</summary>
Motivation: 在内容审核高峰期过后，本文重新审视其惩罚性的一面。不同于关注谁被（不成比例地）审核，而是聚焦于惩罚本身，探讨算法时代的惩罚机制。

Method: 1. 开发结合自动民族志（收集经验和人工制品）与程序正义（分析）的新方法论；2. 重构福柯惩罚系统模型，提出"随机惩罚殖民地"概念；3. 应用该框架分析三个案例：前马斯克时代的Twitter、OpenAI DALL-E 2和Pinterest的内容审核实践。

Result: 三个案例虽然差异显著，但都普遍存在账户暂停的威胁，将用户放逐到"随机惩罚殖民地"中。Twitter的审核具有表演性，DALL-E 2的审核具有详尽惩罚性，Pinterest的审核相对温和但仍显珍贵。

Conclusion: 算法时代的内容审核系统创造了一种新型惩罚机制——随机惩罚殖民地，它处于表演性惩罚和规训性惩罚之间的历史性临界实践，账户暂停成为普遍威胁，将用户置于不确定的惩罚状态中。

Abstract: With peak content moderation seemingly behind us, this paper revisits its punitive side. But instead of focusing on who is being (disproportionately) moderated, it focuses on the punishment itself and makes three contributions. First, it develops a novel methodology that combines auto-ethnography for collecting experiences and artifacts with procedural justice for analyzing them. Second, it reworks Foucault's model of the penal system for the algorithmic age, restoring the penal colony as the historically liminal practice between punishment as performance and punishment as discipline, i.e., the stochastic penal colony. Finally, it applies this methodological and conceptual framing to three case studies, one on the gallingly performative moderation by pre-Musk Twitter, one on the exhaustively punitive content moderation for OpenAI's DALLE~2, and one on the relatively light touch but still rather precious moderation by Pinterest. While substantially different, all three feature the pervasive threat of account suspension, thereby banishing users to the stochastic penal colony.

</details>


### [411] [Synthetic Student Responses: LLM-Extracted Features for IRT Difficulty Parameter Estimation](https://arxiv.org/abs/2602.00034)
*Matias Hoyl*

Main category: cs.CY

TL;DR: 使用LLM提取教学特征，通过模拟学生响应预测IRT难度参数，无需真实学生测试


<details>
  <summary>Details</summary>
Motivation: 传统教育评估依赖学生预测试来确定题目难度，资源消耗大且效率低，需要开发无需学生测试的难度预测方法

Method: 结合传统语言特征和LLM提取的教学特征（解题步骤数、认知复杂度、潜在误解），采用两阶段神经网络：先预测学生响应，再从模拟响应模式推导IRT难度参数

Result: 在超过25万学生响应的数学题数据集上，模型对完全未见题目的预测难度与实际难度Pearson相关系数达到约0.78

Conclusion: LLM提取的教学特征能有效预测IRT难度参数，为教育评估提供了一种无需学生测试的自动化难度估计方法

Abstract: Educational assessment relies heavily on knowing question difficulty, traditionally determined through resource-intensive pre-testing with students. This creates significant barriers for both classroom teachers and assessment developers. We investigate whether Item Response Theory (IRT) difficulty parameters can be accurately estimated without student testing by modeling the response process and explore the relative contribution of different feature types to prediction accuracy. Our approach combines traditional linguistic features with pedagogical insights extracted using Large Language Models (LLMs), including solution step count, cognitive complexity, and potential misconceptions. We implement a two-stage process: first training a neural network to predict how students would respond to questions, then deriving difficulty parameters from these simulated response patterns. Using a dataset of over 250,000 student responses to mathematics questions, our model achieves a Pearson correlation of approximately 0.78 between predicted and actual difficulty parameters on completely unseen questions.

</details>


### [412] [LSSF: Safety Alignment for Large Language Models through Low-Rank Safety Subspace Fusion](https://arxiv.org/abs/2602.00038)
*Guanghao Zhou,Panjia Qiu,Cen Chen,Hongyu Li,Mingyuan Chu,Xin Zhang,Jun Zhou*

Main category: cs.CY

TL;DR: LSSF：一种通过低秩安全子空间融合来恢复微调后LLM安全对齐的后处理方法，利用安全信息的低秩特性构建投影矩阵，动态计算各层安全关键秩，有效恢复安全性而不影响下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM的安全机制脆弱，即使在没有有害内容的微调数据集上也可能破坏安全性。现有安全对齐方法主要依赖微调过程，导致复杂度和计算资源需求增加。

Method: 提出LSSF框架：利用LLM中安全信息的低秩特性，构建低秩投影矩阵提取安全向量的主成分；提出安全奇异值熵度量来量化各层安全信息编码密度，动态计算安全关键秩；通过线性算术将主成分与微调后LLM结合来恢复安全对齐。

Result: 大量实验表明，该后处理方法能有效恢复微调模型的安全对齐，同时对下游任务性能影响最小。

Conclusion: LSSF提供了一种高效的后处理安全对齐方法，利用安全信息的低秩特性和稳定子空间，在不影响模型通用能力的情况下恢复安全性，解决了现有方法复杂度过高的问题。

Abstract: The safety mechanisms of large language models (LLMs) exhibit notable fragility, as even fine-tuning on datasets without harmful content may still undermine their safety capabilities. Meanwhile, existing safety alignment methods predominantly rely on the fine-tuning process, which inadvertently leads to the increased complexity and computational resources required. To address these issues, we introduce LSSF, a novel safety re-alignment framework with \underline{L}ow-Rank \underline{S}afety \underline{S}ubspace \underline{F}usion. Our proposed method exploits the low-rank characteristics of safety information in LLMs by constructing a low-rank projection matrix to extract the principal components of safety vectors. Notably, this projection matrix represents the low-rank safety subspace of the LLMs, which we have observed to remain stable during fine-tuning process and is isolated from the model's general capabilities. These principal components are used to effectively restore safety alignment when combined with fine-tuned LLMs through linear arithmetic. Additionally, to account for the varying encoding densities of safety information across different layers of LLMs, we propose a novel metric called safety singular value entropy. This metric quantifies the encoding density and allows for the dynamic computation of the safety-critical rank for each safety vector. Extensive experiments demonstrate that our proposed post-hoc alignment method can effectively restore the safety alignment of fine-tuned models with minimal impact on their performance in downstream tasks.

</details>


### [413] [Student Perceptions of Large Language Models Use in Self-Reflection and Design Critique in Architecture Studio](https://arxiv.org/abs/2602.00041)
*Juan David Salazar Rodriguez,Sam Conrad Joyce,Nachamma Sockalingam,Khoo Eng Tat,Julfendi*

Main category: cs.CY

TL;DR: 研究探讨将大语言模型整合到建筑设计工作室反馈机制中，从生成式生产转向反思式教学，发现学生将LLMs视为协作的"认知镜子"而非权威指导者。


<details>
  <summary>Details</summary>
Motivation: 探索如何将大语言模型应用于建筑设计教育的反馈机制，从传统的生成式生产转向更具反思性的教学方法，以增强学生的批判性思维和学习效果。

Method: 采用混合方法研究，在新加坡科技设计大学的建筑学生中进行，分析学生在三个不同反馈领域（自我反思、同伴批评、教授主导评审）中对LLMs的感知和使用情况。

Result: 学生将LLMs视为协作的"认知镜子"而非权威指导者：在自主学习中有助于结构化思考但缺乏情境细微差别；在同伴批评中作为中立调解者减少社交焦虑；在教授评审中作为后批评合成引擎管理认知负荷。

Conclusion: LLMs在建筑设计教育中可作为有效的教学工具，通过充当认知脚手架、中立调解者和认知合成引擎，支持学生的批判性思维发展，但需要认识到其在情境理解方面的局限性。

Abstract: This study investigates the integration of Large Language Models (LLMs) into the feedback mechanisms of the architectural design studio, shifting the focus from generative production to reflective pedagogy. Employing a mixed-methods approach with architecture students at the Singapore Uni-versity of Technology and Design, the research analyzes student percep-tions across three distinct feedback domains: self-reflection, peer critique, and professor-led reviews. The findings reveal that students engage with LLMs not as authoritative instructors, but as collaborative "cognitive mir-rors" that scaffold critical thinking. In self-directed learning, LLMs help structure thoughts and overcome the "blank page" problem, though they are limited by a lack of contextual nuance. In peer critiques, the technology serves as a neutral mediator, mitigating social anxiety and the "fear of of-fending". Furthermore, in high-stakes professor-led juries, students utilize LLMs primarily as post-critique synthesis engines to manage cognitive overload and translate abstract academic discourse into actionable design iterations.

</details>


### [414] [When LLMs Imagine People: A Human-Centered Persona Brainstorm Audit for Bias and Fairness in Creative Applications](https://arxiv.org/abs/2602.00044)
*Hongliu Cao,Eoin Thomas,Rodrigo Acuna Agost*

Main category: cs.CY

TL;DR: 提出Persona Brainstorm Audit (PBA)方法，通过开放式角色生成来检测LLM偏见，相比现有方法更可扩展和透明，能发现多维度偏见并支持纵向追踪。


<details>
  <summary>Details</summary>
Motivation: LLM的偏见输出会强化刻板印象并在实际应用中延续不平等，因此公平性审计至关重要。现有方法依赖固定身份类别和静态基准，存在局限性。

Method: 提出PBA方法，通过开放式角色生成进行偏见检测。该方法不依赖固定类别，能发现多维度社会偏见，支持纵向追踪，并降低数据泄露风险。

Result: 对12个最先进的LLM应用PBA，比较了不同模型、维度和版本的偏见严重程度，发现了独特的模式和谱系特异性变异，追踪了偏见在连续代际中的衰减、持续或重现。

Conclusion: PBA在不同样本量、角色扮演提示和去偏见提示下保持稳定，证明了其在LLM公平性审计中的可靠性，为偏见检测提供了可扩展和透明的解决方案。

Abstract: Biased outputs from Large Language Models (LLMs) can reinforce stereotypes and perpetuate inequities in real-world applications, making fairness auditing essential. We introduce the Persona Brainstorm Audit (PBA), a scalable and transparent auditing method for detecting bias through open-ended persona generation. Unlike existing methods that rely on fixed identity categories and static benchmarks, PBA uncovers biases across multiple social dimensions while supporting longitudinal tracking and mitigating data leakage risks. Applying PBA to 12 state-of-the-art LLMs, we compare bias severity across models, dimensions, and versions, uncover distinct patterns and lineage-specific variability, and trace how biases attenuate, persist, or resurface across successive generations. Robustness analyses show PBA remains stable under varying sample sizes, role-playing prompts, and debiasing prompts, establishing its reliability for fairness auditing in LLMs.

</details>


### [415] [Examining The CoVCues Dataset: Supporting COVID Infodemic Research Through A Novel User Assessment Study](https://arxiv.org/abs/2602.00055)
*Shreetika Poudel,Ankur Chatterjee*

Main category: cs.CY

TL;DR: CoVCues数据集填补了COVID错误信息检测中视觉线索数据的空白，通过用户评估研究验证了视觉线索在识别在线健康信息可靠性中的重要性。


<details>
  <summary>Details</summary>
Motivation: COVID-19疫情导致在线健康错误信息激增，现有数据集多为单模态文本数据，缺乏视觉线索（如图像、信息图表）的有组织、处理和分析的存储库，限制了多模态错误信息检测的研究。

Method: 创建了包含多样化图像工件的CoVCues多模态数据集，并通过初步用户评估研究，使用问卷调查参与者，评估数据集图像对用户感知信息可靠性的贡献程度。

Result: 用户评估研究提供了早期见解，显示不同利益相关群体如何解释在线健康信息中的视觉线索，验证了CoVCues数据集的实用性和视觉线索在COVID信息流行病中的重要性。

Conclusion: CoVCues数据集填补了COVID错误信息检测中视觉线索数据的空白，用户评估研究证实了视觉线索在识别在线健康信息可靠性中的重要作用，为未来相关研究提供了有价值的资源。

Abstract: The public confidence and trust in online healthcare information have been greatly dented following the COVID-19 pandemic, which triggered a significant rise in online health misinformation. Existing literature shows that different datasets have been created to aid with detecting false information associated with this COVID infodemic. However, most of these datasets contain mostly unimodal data, which comprise primarily textual cues, and not visual cues, like images, infographics, and other graphic data components. Prior works point to the fact that there are only a handful of multimodal datasets that support COVID misinformation identification, and they lack an organized, processed and analyzed repository of visual cues. The novel CoVCues dataset, which represents a varied set of image artifacts, addresses this gap and advocates for the use of visual cues towards detecting online health misinformation. As part of validating the contents and utility of our CoVCues dataset, we have conducted a preliminary user assessment study, where different participants have been surveyed through a set of questionnaires to determine how effectively these dataset images contribute to the user perceived information reliability. These survey responses helped provide early insights into how different stakeholder groups interpret visual cues in the context of online health information and communication. The findings from this novel user assessment study offer valuable feedback for refining our CoVCues dataset and for supporting our claim that visual cues are underutilized but useful in combating the COVID infodemic. To our knowledge, this user assessment research study, as described in this paper, is the first of its kind work, involving COVID visual cues, that demonstrates the important role that our CoVCues dataset can potentially play in aiding COVID infodemic related future research work.

</details>


### [416] [How Hyper-Datafication Impacts the Sustainability Costs in Frontier AI](https://arxiv.org/abs/2602.00056)
*Sophia N. Wilson,Sebastian Mair,Mophat Okinyi,Erik B. Dam,Janin Koch,Raghavendra Selvan*

Main category: cs.CY

TL;DR: 论文分析了AI大规模数据的可持续性成本，揭示了超数据化将环境负担、劳动风险和代表性危害转移给全球南方、不稳定数据工作者和弱势文化的系统性模式，并提出Data PROOFS建议来缓解这些问题。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型依赖大规模数据的发展，这种数据扩张带来了环境、社会和经济成本。当前AI领域正从"用数据构建模型"转向"为构建模型主动创造数据"，这种超数据化趋势对前沿AI及其社会影响处于关键转折点，需要系统分析其可持续性成本。

Method: 1) 定量分析：分析Hugging Face Hub约55万个数据集，关注数据集增长、存储相关能耗与碳足迹、语言数据的社会代表性；2) 定性分析：收集肯尼亚数据工作者的反馈，考察劳动参与情况；3) 补充分析：利用外部数据源验证数据中心基础设施的全球不平等分布。

Result: 研究发现超数据化不仅增加资源消耗，更系统性地将环境负担、劳动风险和代表性危害重新分配给全球南方、不稳定的数据工作者和代表性不足的文化。具体表现为：存储能耗和碳足迹显著增长，劳动剥削集中在发展中国家，数据代表性存在严重不平等。

Conclusion: 论文提出Data PROOFS建议框架（来源可追溯性、资源意识、所有权、开放性、节俭性、标准），旨在缓解数据相关成本，使支撑前沿AI的数据成本变得可见，并激发研究社区内外的广泛讨论。

Abstract: Large-scale data has fuelled the success of frontier artificial intelligence (AI) models over the past decade. This expansion has relied on sustained efforts by large technology corporations to aggregate and curate internet-scale datasets. In this work, we examine the environmental, social, and economic costs of large-scale data in AI through a sustainability lens. We argue that the field is shifting from building models from data to actively creating data for building models. We characterise this transition as hyper-datafication, which marks a critical juncture for the future of frontier AI and its societal impacts. To quantify and contextualise data-related costs, we analyse approximately 550,000 datasets from the Hugging Face Hub, focusing on dataset growth, storage-related energy consumption and carbon footprint, and societal representation using language data. We complement this analysis with qualitative responses from data workers in Kenya to examine the labour involved, including direct employment by big tech corporations and exposure to graphic content. We further draw on external data sources to substantiate our findings by illustrating the global disparity in data centre infrastructure. Our analyses reveal that hyper-datafication does not merely increase resource consumption but systematically redistributes environmental burdens, labour risks, and representational harms toward the Global South, precarious data workers, and under-represented cultures. Thus, we propose Data PROOFS recommendations spanning provenance, resource awareness, ownership, openness, frugality, and standards to mitigate these costs. Our work aims to make visible the often-overlooked costs of data that underpin frontier AI and to stimulate broader debate within the research community and beyond.

</details>


### [417] [A longitudinal geospatial multimodal dataset of post-discharge frailty, physiology, mobility, and neighborhoods](https://arxiv.org/abs/2602.00060)
*Ali Abedi,Charlene H. Chu,Shehroz S. Khan*

Main category: cs.CY

TL;DR: GEOFRAIL数据集：针对社区居住的虚弱老年人在出院后8周内收集的多模态地理空间数据集，包含人口统计、传感器特征、临床评估和邻里环境数据，用于分析恢复轨迹。


<details>
  <summary>Details</summary>
Motivation: 虚弱老年人出院后面临功能衰退、行动能力下降、社交孤立等风险，容易导致再住院。邻里环境通过影响行动机会、社交参与和社区资源获取，进一步塑造恢复轨迹。需要多模态传感技术和数据驱动方法来持续监测这些多维因素。

Method: 收集社区居住虚弱老年人在出院后8周内的纵向地理空间多模态数据。数据集包含：参与者人口统计、多模态传感器特征、每两周一次的临床评估（虚弱程度、身体功能、社交孤立）、时间位置记录（关联邻里设施、犯罪率、社会经济指标）。采用标准化流程和隐私保护的空间聚合方法。

Result: 技术验证显示地理空间、传感器衍生和临床测量之间的内部一致性良好。报告了机器学习模型在表征恢复轨迹方面的基线性能，为后续研究提供了可靠的数据基础。

Conclusion: GEOFRAIL数据集为研究虚弱老年人从医院到社区过渡期间的恢复轨迹提供了全面的多模态地理空间数据资源，有助于开发预测模型和干预策略，改善老年人健康结局。

Abstract: Frailty in older adults is associated with increased vulnerability to functional decline, reduced mobility, social isolation, and challenges during the transition from hospital to community living. These factors are associated with rehospitalization and may adversely influence recovery. Neighborhood environments can further shape recovery trajectories by affecting mobility opportunities, social engagement, and access to community resources. Multimodal sensing technologies combined with data-driven analytical approaches offer the potential to continuously monitor these multidimensional factors in real-world settings. This Data Descriptor presents GEOFRAIL, a longitudinal geospatial multimodal dataset collected from community-dwelling frail older adults following hospital discharge. The dataset is organized into interconnected tables capturing participant demographics, features derived from multimodal sensors, biweekly clinical assessments of frailty, physical function, and social isolation, and temporal location records linked to neighborhood amenities, crime rates, and census-based socioeconomic indicators. Data were collected over an eight-week post-discharge period using standardized pipelines with privacy-preserving spatial aggregation. Technical validation demonstrates internal consistency across geospatial, sensor-derived, and clinical measures and reports baseline performance of machine learning models for characterizing recovery trajectories.

</details>


### [418] [Simple Role Assignment is Extraordinarily Effective for Safety Alignment](https://arxiv.org/abs/2602.00061)
*Zhou Ziheng,Jiakun Ding,Zhaowei Zhang,Ruosen Gao,Yingnian Wu,Demetri Terzopoulos,Yipeng Kang,Fangwei Zhong,Junqi Wang*

Main category: cs.CY

TL;DR: 基于角色条件化的AI对齐方法，通过社会角色编码价值观和认知模式，无需训练即可显著提升模型安全性和性能


<details>
  <summary>Details</summary>
Motivation: 传统的基于原则的对齐方法缺乏上下文敏感性和完整性，需要更紧凑有效的替代方案

Method: 提出基于心智理论的角色条件化方法，包含角色条件生成器和迭代式基于角色的批评器进行精炼

Result: 在五个模型系列中一致优于基于原则、思维链等基线方法，将WildJailbreak基准的不安全输出从81.4%降至3.6%

Conclusion: 角色分配是AI对齐和LLM-as-a-Judge构建的强大、可解释范式

Abstract: Principle-based alignment often lacks context sensitivity and completeness. Grounded in Theory of Mind, we propose role conditioning as a compact alternative: social roles (e.g., mother, judge) implicitly encode both values and the cognitive schemas required to apply them. We introduce a training-free pipeline featuring a role-conditioned generator and iterative role-based critics for refinement. Across five model families, our approach consistently outperforms principle-based, Chain-of-Thought (CoT) and other baselines across benchmarks. Notably, it reduces unsafe outputs on the WildJailbreak benchmark from 81.4\% to 3.6\% with DeepSeek-V3. Not only for common safety benchmarks, it consistently applies for agentic safety tasks. These results establish role assignment as a powerful, interpretable paradigm for AI alignment and LLM-as-a-Judge construction.

</details>


### [419] [Responsible Evaluation of AI for Mental Health](https://arxiv.org/abs/2602.00065)
*Hiba Arnaout,Anmol Goel,H. Andrew Schwartz,Steffen T. Eberhardt,Dana Atzil-Slonim,Gavin Doherty,Brian Schwartz,Wolfgang Lutz,Tim Althoff,Munmun De Choudhury,Hamidreza Jamalabadi,Raj Sanjay Shah,Flor Miriam Plaza-del-Arco,Dirk Hovy,Maria Liakata,Iryna Gurevych*

Main category: cs.CY

TL;DR: 该论文提出一个跨学科框架，重新思考AI心理健康工具的责任评估，强调临床有效性、社会背景和公平性，而非仅依赖通用指标。


<details>
  <summary>Details</summary>
Motivation: 当前AI心理健康工具评估方法存在碎片化问题，与临床实践、社会背景和用户体验脱节，过度依赖通用指标而忽视临床有效性、治疗适宜性和安全性。

Method: 通过分析135篇*CL出版物，识别现有评估的局限性，提出整合临床健全性、社会背景和公平性的跨学科框架，并建立AI心理健康支持类型分类学（评估型、干预型、信息合成型）。

Result: 研究发现当前评估存在三大局限：过度依赖不捕捉临床有效性的通用指标、心理健康专业人员参与有限、对安全性和公平性关注不足。提出的分类学为不同类型AI工具提供了针对性评估要求。

Conclusion: 需要重新思考AI心理健康工具的责任评估，采用更全面的框架，确保评估与临床实践、社会背景和用户体验紧密结合，以促进AI在心理健康领域的负责任发展。

Abstract: Although artificial intelligence (AI) shows growing promise for mental health care, current approaches to evaluating AI tools in this domain remain fragmented and poorly aligned with clinical practice, social context, and first-hand user experience. This paper argues for a rethinking of responsible evaluation -- what is measured, by whom, and for what purpose -- by introducing an interdisciplinary framework that integrates clinical soundness, social context, and equity, providing a structured basis for evaluation. Through an analysis of 135 recent *CL publications, we identify recurring limitations, including over-reliance on generic metrics that do not capture clinical validity, therapeutic appropriateness, or user experience, limited participation from mental health professionals, and insufficient attention to safety and equity. To address these gaps, we propose a taxonomy of AI mental health support types -- assessment-, intervention-, and information synthesis-oriented -- each with distinct risks and evaluative requirements, and illustrate its use through case studies.

</details>


### [420] [FoundationalASSIST: An Educational Dataset for Foundational Knowledge Tracing and Pedagogical Grounding of LLMs](https://arxiv.org/abs/2602.00070)
*Eamon Worden,Cristina Heffernan,Neil Heffernan,Shashank Sonkar*

Main category: cs.CY

TL;DR: 论文介绍了FoundationalASSIST——首个包含完整教育信息的英文数据集，包含170万次学生互动，用于研究LLM在教育中的应用。评估发现当前LLM在知识追踪和教学基础任务上表现不佳，远未达到支持个性化学习的要求。


<details>
  <summary>Details</summary>
Motivation: 现有教育数据集只提供问题标识符和二元正确性标签，对基于自然语言推理的LLM不透明。随着LLM被部署用于自适应测试和个性化辅导，需要能够评估LLM是否理解学生学习过程的数据集。

Method: 创建FoundationalASSIST数据集，包含完整问题文本、实际学生回答、错误答案选择记录以及K-12共同核心标准对齐。使用该数据集评估四个前沿LLM模型在知识追踪和教学基础两个任务家族上的表现。

Result: 所有模型在知识追踪任务上仅勉强达到基线水平；在项目区分度任务上低于随机水平；在相对难度判断上表现较好（最高68.6%），但这部分成功反而凸显了其他方面的差距。

Conclusion: 当前LLM在可靠支持大规模个性化学习方面仍需重大进展。发布FoundationalASSIST数据集以支持这些基础挑战的进展。

Abstract: Can Large Language Models understand how students learn? As LLMs are deployed for adaptive testing and personalized tutoring, this question becomes urgent -- yet we cannot answer it with existing resources. Current educational datasets provide only question identifiers and binary correctness labels, rendering them opaque to LLMs that reason in natural language. We address this gap with FoundationalASSIST, the first English educational dataset providing the complete information needed for research on LLMs in education: full question text, actual student responses (not just right/wrong), records of which wrong answers students chose, and alignment to Common Core K-12 standards. These 1.7 million interactions from 5,000 students enable research directions that were previously impossible to pursue, from fine-tuning student models to analyzing misconception patterns. To demonstrate the dataset's utility, we evaluate four frontier models (GPT-OSS-120B, Llama-3.3-70B, Qwen3-Next-80B variants) on two complementary task families: Knowledge Tracing, testing whether LLMs can predict student performance on questions, and the exact answer a student will give; and \textbf{Pedagogical Grounding}, testing whether LLMs understand the properties that make assessment items effective. Our evaluation reveals significant gaps in current LLM capabilities. Every model barely achieves a trivial baseline on knowledge tracing. All models fall below random chance on item discrimination, indicating that LLMs do not understand what makes one problem more diagnostic than another. Models do show competence at judging relative difficulty (up to 68.6%), but this partial success only highlights the gaps elsewhere. These results establish that substantial advances are needed before LLMs can reliably support personalized learning at scale. We release FoundationalASSIST to support progress on these foundational challenges.

</details>


### [421] [Adoption and Use of LLMs at an Academic Medical Center](https://arxiv.org/abs/2602.00074)
*Nigam H. Shah,Nerissa Ambers,Abby Pandya,Timothy Keyes,Juan M. Banda,Srikar Nallan,Carlene Lugtu,Artem A. Trotsyuk,Suhana Bedi,Alyssa Unell,Miguel Fuentes,Francois Grolleau,Sneha S. Jain,Jonathan Chen,Devdutta Dash,Danton Char,Aditya Sharma,Duncan McElfresh,Patrick Scully,Vishanthan Kumar,Connor OBrien,Satchi Mouniswamy,Elvis Jones,Krishna Jasti,Gunavathi Mannika Lakshmanan,Sree Ram Akula,Varun Kumar Singh,Ramesh Rajmanickam,Sudhir Sinha,Vicky Zhou,Xu Wang,Bilal Mawji,Joshua Ge,Wencheng Li,Travis Lyons,Jarrod Helzer,Vikas Kakkar,Ramesh Powar,Darren Batara,Cheryl Cordova,William Frederick,Olivia Tang,Phoebe Morgan,April S. Liang,Stephen P. Ma,Shivam Vedak,Dong-han Yao,Akshay Swaminathan,Mehr Kashyap,Brian Ng,Jamie Hellman,Nikesh Kotecha,Christopher Sharp,Gretchen Brown,Christian Lindmark,Anurang Revri,Michael A. Pfeffer*

Main category: cs.CY

TL;DR: ChatEHR是一个集成LLM的医疗记录系统，通过自动化和交互界面支持临床文档工作，减少工作流程摩擦，实现机构级LLM应用能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM工具在临床文档中存在"工作流程摩擦"问题，需要手动数据输入，无法有效处理跨越多年的完整患者时间线数据。

Method: 开发ChatEHR系统，包含静态自动化任务（固定提示词+数据组合）和通过用户界面的交互式使用，支持访问多年患者医疗记录，采用模型无关架构。

Result: 1.5年内构建7个自动化任务，1075名用户培训成为常规用户，前3个月23,000次会话。摘要生成是最常见任务，每次生成约0.73个幻觉和1.60个不准确信息。首年预计节省600万美元。

Conclusion: "内部构建"策略使医疗系统能够通过供应商无关、内部治理的LLM平台保持自主权，需要新的监控评估方法和价值评估框架来量化LLM应用影响。

Abstract: While large language models (LLMs) can support clinical documentation needs, standalone tools struggle with "workflow friction" from manual data entry. We developed ChatEHR, a system that enables the use of LLMs with the entire patient timeline spanning several years. ChatEHR enables automations - which are static combinations of prompts and data that perform a fixed task - and interactive use in the electronic health record (EHR) via a user interface (UI). The resulting ability to sift through patient medical records for diverse use-cases such as pre-visit chart review, screening for transfer eligibility, monitoring for surgical site infections, and chart abstraction, redefines LLM use as an institutional capability. This system, accessible after user-training, enables continuous monitoring and evaluation of LLM use.
  In 1.5 years, we built 7 automations and 1075 users have trained to become routine users of the UI, engaging in 23,000 sessions in the first 3 months of launch. For automations, being model-agnostic and accessing multiple types of data was essential for matching specific clinical or administrative tasks with the most appropriate LLM. Benchmark-based evaluations proved insufficient for monitoring and evaluation of the UI, requiring new methods to monitor performance. Generation of summaries was the most frequent task in the UI, with an estimated 0.73 hallucinations and 1.60 inaccuracies per generation. The resulting mix of cost savings, time savings, and revenue growth required a value assessment framework to prioritize work as well as quantify the impact of using LLMs. Initial estimates are $6M savings in the first year of use, without quantifying the benefit of the better care offered. Such a "build-from-within" strategy provides an opportunity for health systems to maintain agency via a vendor-agnostic, internally governed LLM platform.

</details>


### [422] [Standards for trustworthy AI in the European Union: technical rationale, structural challenges, and an implementation path](https://arxiv.org/abs/2602.00078)
*Piercosma Bisconti,Marcello Galisai*

Main category: cs.CY

TL;DR: 该白皮书分析了欧盟AI法案下AI标准化的技术基础，探讨了标准化如何实现合规推定机制，并提出了基于风险管理、可重现技术检查、结构化文档和保证案例的分层标准化方案。


<details>
  <summary>Details</summary>
Motivation: 欧盟AI法案需要技术标准来将法律义务转化为可审计的工程实践，但AI系统具有随机行为、数据依赖、评估实践不成熟和生命周期动态等独特挑战，需要专门的标准框架来支持可扩展的合规评估。

Method: 采用分层标准化方法：水平标准定义流程义务和证据结构，行业特定配置文件指定领域阈值和接受标准。方案包括风险管理、重新定义为测量属性稳定性的可重现技术检查、结构化文档、全面日志记录以及随系统生命周期演变的保证案例。

Result: 提出了一个可行的标准化方案，尽管存在方法学困难，但技术标准对于将法律义务转化为可审计工程实践、实现跨提供商、评估机构和执法机构的可扩展合规评估仍然至关重要。

Conclusion: AI标准化虽然面临独特挑战，但通过分层方法结合水平标准和行业特定配置文件，基于风险管理、可重现检查、结构化文档和动态保证案例的方案是可行的，对实现欧盟AI法案的合规推定机制和可扩展监管至关重要。

Abstract: This white paper examines the technical foundations of European AI standardization under the AI Act. It explains how harmonized standards enable the presumption of conformity mechanism, describes the CEN/CENELEC standardization process, and analyzes why AI poses unique standardization challenges including stochastic behavior, data dependencies, immature evaluation practices, and lifecycle dynamics. The paper argues that AI systems are typically components within larger sociotechnical systems, requiring a layered approach where horizontal standards define process obligations and evidence structures while sectoral profiles specify domain-specific thresholds and acceptance criteria. It proposes a workable scheme based on risk management, reproducible technical checks redefined as stability of measured properties, structured documentation, comprehensive logging, and assurance cases that evolve over the system lifecycle. The paper demonstrates that despite methodological difficulties, technical standards remain essential for translating legal obligations into auditable engineering practice and enabling scalable conformity assessment across providers, assessors, and enforcement authorities

</details>


### [423] [Exploring the Role of Automated Feedback in Programming Education: A Systematic Literature Review](https://arxiv.org/abs/2602.00089)
*Yeonji Jung,Yunseo Lee,Jiyeong Bae,DoYong Kim,Heungsoo Choi,Minji Kang,Unggi Lee*

Main category: cs.CY

TL;DR: 本文对编程教育中的自动化反馈系统进行了系统性文献综述，分析了61项实证研究，揭示了当前系统主要关注错误检测和代码正确性，缺乏对高阶学习过程、互动性和学习者自主性的支持。


<details>
  <summary>Details</summary>
Motivation: 尽管自动化反馈系统在编程教育中日益重要且AI技术不断发展，但该领域研究仍然碎片化，缺乏技术和教学维度的综合。需要系统性地分析现有研究，为未来的系统设计和教学整合提供指导。

Method: 采用系统性文献综述方法，分析了截至2024年9月发表的61项实证研究。从五个维度进行概念性分析：系统架构、教学功能、交互机制、情境部署和评估方法。

Result: 研究发现大多数系统是完全自动化的，嵌入在线平台，主要关注错误检测和代码正确性。虽然最近的发展融入了自适应特征和大语言模型以实现更个性化和交互式反馈，但很少有系统支持高阶学习过程、交互组件或学习者自主性。评估实践倾向于强调短期绩效提升，对长期结果或教学整合关注有限。

Conclusion: 需要重新构想自动化反馈系统，不应仅仅作为错误纠正的技术附加组件，而应作为支持更深层次、自适应和交互式学习的教学支架。未来应关注支持高阶学习、增强互动性和学习者自主性的系统设计。

Abstract: Automated feedback systems have become increasingly integral to programming education, where learners engage in iterative cycles of code construction, testing, and refinement. Despite its wider integration in practices and technical advancements into AI, research in this area remains fragmented, lacking synthesis across technological and instructional dimensions. This systematic literature review synthesizes 61 empirical studies published by September 2024, offering a conceptually grounded analysis of automated feedback systems across five dimensions: system architecture, pedagogical function, interaction mechanism, contextual deployment, and evaluation approach. Findings reveal that most systems are fully automated, embedded within online platforms, and primarily focused on error detection and code correctness. While recent developments incorporate adaptive features and large language models to enable more personalized and interactive feedback, few systems offer support for higher-order learning processes, interactive components, or learner agency. Moreover, evaluation practices tend to emphasize short-term performance gains, with limited attention to long-term outcomes or instructional integration. These findings call for a reimagining of automated feedback not as a technical add-on for error correction, but as a pedagogical scaffold that supports deeper, adaptive, and interactive learning.

</details>


### [424] [Generative Artificial Intelligence in Small and Medium Enterprises: Navigating its Promises and Challenges](https://arxiv.org/abs/2602.00091)
*Kumaran Rajaram,Patrick Nicolas Tinguely*

Main category: cs.CY

TL;DR: 该论文探讨了中小企业如何利用生成式人工智能(GAI)提升竞争力，提出了基于航海隐喻的战略部署框架和实用建议。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能技术为中小企业提供了强大的能力，能够促进可扩展性和创造力的民主化。即使技术专业知识或财务资源有限，中小企业也可以利用该技术简化工作流程、激发创新，从而改善产品供应和长期竞争力。

Method: 论文引入了航海隐喻来揭示GAI部署的关键战略维度：员工能力、有效领导和工作价值观、组织文化、协作与合作、以及与第三方的关系。基于这些维度，为中小企业提供了实用的部署路线图。

Result: 提出了一个全面的GAI部署框架，将五个关键战略维度整合为一个连贯的导航系统，为中小企业成功部署GAI提供了实用的指南针。

Conclusion: 中小企业可以通过战略性部署生成式人工智能来应对其承诺和挑战，利用航海隐喻框架和实用建议，能够成功整合GAI技术，提升创新能力和市场竞争力。

Abstract: The latest technological developments in generative artificial intelligence (GAI) offer powerful capabilities to small and medium enterprises (SMEs), as they facilitate the democratization of both scalability and creativity. Even if they have little technical expertise or financial resources, SMEs can leverage this technology to streamline work processes and unleash innovation, thereby improving their product offerings and long-term competitiveness. This paper discusses how SMEs can navigate both the promises and challenges of GAI and offers a roadmap for deploying GAI. We introduce a sailing metaphor that reveals key strategic dimensions for GAI deployment: competency of employees, effective leadership and work values, organizational culture, collaboration and cooperation, and relationships with third parties. We offer practical recommendations that serve as a useful compass for successfully deploying GAI in SMEs.

</details>


### [425] [Not All Students Engage Alike: Multi-Institution Patterns in GenAI Tutor Use](https://arxiv.org/abs/2602.00447)
*Youjie Chen,Xixi Shi,Xinyu Liu,Shuaiguo Wang,Tracy Xiao Liu,Dragan Gašević*

Main category: cs.CY

TL;DR: 研究分析11,406名学生在200个大学课程中使用GenAI辅导工具的情况，发现10.4%为浅层参与（复制粘贴），深层参与者更灵活切换参与模式，且参与模式因院校选择性和学科而异。


<details>
  <summary>Details</summary>
Motivation: 生成式AI辅导工具虽能提供个性化学习支持，但存在学生可能以不利于学习的方式使用工具的风险，且不同教学环境下学生参与度可能不均，导致学习体验差异。

Method: 使用现有GenAI辅导工具和学习管理系统的匿名学生交互日志，通过两阶段流程分析：首先在会话层面识别四种参与类型，然后在学生层面分析参与模式随时间的变化，并考察院校选择性和学科差异。

Result: 识别出四种参与类型，其中10.4%为浅层参与（复制粘贴为主）；浅层参与者倾向于保持该模式，深层参与者更灵活切换；院校选择性和学科对参与模式有显著影响，高选择性院校学生更可能深层参与。

Conclusion: 研究揭示了GenAI辅导工具在真实教育环境中的使用模式，提供了分析学生参与度的框架，对大规模负责任实施具有重要启示，需关注参与不均问题。

Abstract: The emergence of generative artificial intelligence (GenAI) has created unprecedented opportunities to provide individualized learning support in classrooms as automated tutoring systems at scale. However, concerns have been raised that students may engage with these tools in ways that do not support learning. Moreover, student engagement with GenAI Tutors may vary across instructional contexts, potentially leading to unequal learning experiences. In this study, we utilize de-identified student interaction logs from an existing GenAI Tutor and the learning management system in which it is embedded. We systematically examined student engagement (N = 11,406) with the tool across 200 classes in ten post-secondary institutions through a two-stage pipeline: First, we identified four distinct engagement types at the conversation session level. In particular, 10.4% of them were "shallow engagement" where copy-pasting behavior was prevalent. Then, at the student level, we show that students transitioned across engagement types over time. However, students who exhibited shallow engagement with the tool were more likely to remain in this mode, whereas those who engaged deeply with the tool transitioned more flexibly across engagement types. Finally, at both the session and student levels, we show substantial heterogeneity in student engagement across institution selectivity and course disciplines. In particular, students from highly selective institutions were more likely to exhibit deep engagement. Together, our study advances the understanding of how GenAI Tutors are used in authentic educational settings and provides a framework for analyzing student engagement with GenAI Tutors, with implications for responsible implementation at scale.

</details>


### [426] [A Qualitative Study of IT Students' Skill Development: Comparing Online and Face- to-Face Learning Environments](https://arxiv.org/abs/2602.00799)
*Hugo Silva*

Main category: cs.CY

TL;DR: 该研究通过质性方法探索IT学生在在线与面对面学习环境中的体验和技能发展差异，发现面对面学习更有利于沟通协作技能，而在线学习则更促进自我调节和适应能力。


<details>
  <summary>Details</summary>
Motivation: 每个学生都有特定的特征和学习偏好，这些特征和偏好反映在不同类型的学习环境（在线或面对面）中。理解这些差异对于教育工作者创建能够激励和吸引学生的学习环境至关重要。研究旨在了解IT学生在两种学习环境中的体验和技能发展感知差异。

Method: 采用质性研究方法，基于社会建构主义范式，通过半结构化访谈收集数据，重点关注学生的观点和体验。数据分析采用扎根理论方法，使用系统化程序进行分析。

Result: 研究结果表明，面对面学习可能更有效地发展沟通和协作技能，特别是在同步互动体验方面；而在线学习可能更有利于自我调节和适应能力的发展，因为它提供了独立性和灵活性。

Conclusion: 本研究提出了两个扎根理论，解释不同的IT学习环境如何影响学生特定技能的发展，这些理论可以为优化混合学习体验的教学讨论做出贡献。

Abstract: Each student has specific characteristics and learning preferences, that reflect on each type of learning environment, online or face-to-face. Understanding these differences is crucial for educators to create learning environments that can inspire and engage students. This qualitative study explores and tries to better understand, specifically the IT student's experiences and perceived skills development in online and face-to-face learning environments, while trying to address the question: "Regarding online and face-to-face learning environments, in IT, how do students experience and assess their skill development in one learning environment compared to the other?". Using a social constructive paradigm, the purpose of the research is to focus as much as possible on the student's views of the situation and how their perspectives and experiences shape the perception of developed skills. Data was collected through semi-structured interviews by focusing on the student and asking for their personal experience on skill development through online and face-to-face learning environments. The data analysis strategy adopts the grounded theory approach, using a systematic procedure. The results suggest that face-to-face learning may develop a better communication and collaborative skills more effectively while experiencing a synchronous interaction, where online learning may strength in self-regulation and adaptability skills because of the independence and flexibility it provides. This study produces two grounded theories that explain how different IT learning environments influence the development of student's specific skills, that can contribute to pedagogical discussions on optimizing hybrid learning experiences.

</details>


### [427] [PS$^2$: Parameterized Control for Fine-Grained Student Proficiency Simulation](https://arxiv.org/abs/2602.00850)
*Ruochen Liu,Zhiyuan Wen,Hao Yan,Jun Yin,Senzhang Wang,Jiannong Cao*

Main category: cs.CY

TL;DR: 提出PS²框架，通过参数化插值强LLM与弱学生LLM来模拟不同水平的学生，实现细粒度可控的熟练度模拟


<details>
  <summary>Details</summary>
Motivation: 教育AI中获取真实学生响应数据面临成本、伦理和安全限制，现有基于提示的LLM模拟方法存在可控性有限、提示设计敏感、缺乏学术表现校准等问题

Method: 提出PS²框架：1）构建强LLM上界和认知错误导向的弱学生LLM下界；2）通过混合比率进行参数化插值；3）用学术表现校准插值比率以确保与目标熟练度对齐

Result: 在两个公开数据集上的实验表明，PS²相比现有基线实现了更细粒度、更一致的熟练度模拟，在学生行为相似性和项目难度预测方面表现更优

Conclusion: PS²框架通过参数化插值和学术表现校准，有效解决了现有LLM学生模拟方法的局限性，为数据稀缺条件下的教育AI评估提供了实用解决方案

Abstract: Understanding how students with different proficiency levels respond to educational materials is a critical issue within the field of AI for Education. However, acquiring sufficient real student response data for a robust evaluation is often hindered by cost, ethics, and security constraints. Consequently, LLM-based student proficiency simulation, especially prompt-based methods, has emerged as a practical alternative under data-scarce conditions. Despite their promise, current methods still exhibit limited controllability with coarse-grained proficiency representations, high sensitivity to prompt design, and the lack of calibration with academic performance. Therefore, we propose Parameterized Student Proficiency Simulation (PS$^2$), an unsupervised and parameterized model-level framework that simulates students with different proficiencies by interpolating between a strong upper-bound LLM and a weaker, cognitive error-informed lower-bound student LLM via a hybrid ratio. Specifically, the lower-bound model is constructed by fine-tuning the weaker LM to exhibit cognitive errors when responding to educational materials. To ensure alignment with target proficiency levels, PS$^2$ further calibrates the interpolation ratio with academic performance. Experiments on two public datasets demonstrate that PS$^2$ achieves finer-grained and consistent proficiency simulation compared to existing baselines, leading to superior performance in student behavior similarity and item difficulty prediction.

</details>


### [428] [Does Ad-Free Mean Less Data Collection? An Empirical Study of Platform Data Practices and User Expectations](https://arxiv.org/abs/2602.01231)
*Sepehr Mousavi,Abhisek Dash,Savvas Zannettou,Krishna P. Gummadi*

Main category: cs.CY

TL;DR: 研究发现，即使采用付费无广告订阅模式，平台仍会收集广告相关数据，与用户期望存在显著差距，引发对GDPR合规性的担忧。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探讨无广告订阅模式下平台数据收集实践与用户隐私期望之间的紧张关系。随着平台提供付费无广告订阅作为传统免费广告模式的替代方案，GDPR下广告作为数据处理的关键理由被移除，理论上平台应收集更少用户数据。然而，平台可能以提供个性化体验为由继续收集数据，这种隐私原则与平台激励之间的冲突尚未得到充分研究。

Method: 研究方法包括两个方面：1）数据收集实践分析：通过对Instagram、Facebook和X三大平台的数据导出进行分析，考察无广告订阅模式下平台是否继续保留或收集广告相关数据；2）用户期望调查：在Prolific平台上对255名参与者进行调查，了解他们对无广告模式下数据收集的规范性期望和实际预期。

Result: 研究结果显示：1）平台实践方面：即使采用无广告订阅，三大平台仍继续保留或收集某些广告相关数据；2）用户期望方面：69%的参与者规范性期望数据收集应减少，表明他们期望无广告模式能改善数字隐私。然而，当被问及他们认为实际会发生什么时，63%的参与者认为平台仍会收集大致相同数量的数据，显示出对平台实践的不信任。

Conclusion: 研究结论指出，平台数据实践与用户规范性期望之间存在显著脱节，这引发了关于平台是否符合GDPR核心原则（如目的限制、数据最小化和透明度）的严重问题。研究强调了监管机构需要更严格地审查无广告订阅模式下的数据收集实践，确保平台真正尊重用户隐私期望。

Abstract: Online platforms increasingly offer "paid" ad-free subscriptions as an alternative to the traditional "free" ad-based model. The transition to ad-free models ostensibly removes advertising as a key justification for data processing under the GDPR. So, normatively, platforms should collect less user data. However, platforms may justify continued data collection as a means to provide an improved, personalized experience. This tension between privacy principles and platform incentives raises a critical underexplored question: do data collection practices vary between ad-free and ad-based subscription models?
  In this paper, we shed light on this important privacy issue by investigating the alignment between platform data collection practices and related user expectations. With respect to data collection process, our analyses of data exports from three major online platforms - Instagram, Facebook, and X - reveal that these platforms continue to retain or collect some ad-related data, even in ad-free subscriptions. With respect to user expectations, our survey among 255 participants on Prolific reveals that 69% of the participants normatively expect data collection to be reduced, indicating their expectation of improved digital privacy in an ad-free model. However, when asked what they think actually happens, 63% of these participants believed that platforms would still collect about the same amount of data, highlighting skepticism about platform practices. Our findings not only indicate a significant disconnect between data practices and normative user expectations, but also raise serious questions about platform compliance with core GDPR principles, such as purpose limitation, data minimization, and transparency.

</details>


### [429] [Making Bias Non-Predictive: Training Robust LLM Judges via Reinforcement Learning](https://arxiv.org/abs/2602.01528)
*Qian Wang,Xuandong Zhao,Zirui Zhang,Zhanzhi Lou,Nuo Chen,Dawn Song,Bingsheng He*

Main category: cs.CY

TL;DR: 提出Epistemic Independence Training (EIT)强化学习框架，通过使偏见线索与奖励不相关来训练LLM获得认知独立性，从而减少作为自动评判者时的认知偏见。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型作为自动评判者时容易受到认知偏见的影响（如从众效应、权威效应），现有的提示工程或监督微调方法无法泛化，因为它们只改变表面行为而没有改变使偏见线索具有预测性的优化目标。

Method: 提出EIT强化学习框架，核心原则是：要学习独立性，必须使偏见线索与奖励不相关。通过平衡冲突策略（使偏见信号同等可能支持正确和错误答案）和奖励设计（惩罚跟随偏见但不奖励偏见一致）来实现。

Result: 在Qwen3-4B上的实验表明，EIT提高了在对抗性偏见下的准确性和鲁棒性，同时在偏见与真相一致时保持性能。仅训练于从众偏见的模型能泛化到未见偏见类型（如权威和分心），表明EIT诱导了可转移的认知独立性而非偏见特定启发式。

Conclusion: EIT通过使偏见线索与奖励不相关，成功训练LLM获得认知独立性，有效减少作为自动评判者时的认知偏见，并展现出良好的泛化能力。

Abstract: Large language models (LLMs) increasingly serve as automated judges, yet they remain susceptible to cognitive biases -- often altering their reasoning when faced with spurious prompt-level cues such as consensus claims or authority appeals. Existing mitigations via prompting or supervised fine-tuning fail to generalize, as they modify surface behavior without changing the optimization objective that makes bias cues predictive. To address this gap, we propose Epistemic Independence Training (EIT), a reinforcement learning framework grounded in a key principle: to learn independence, bias cues must be made non-predictive of reward. EIT operationalizes this through a balanced conflict strategy where bias signals are equally likely to support correct and incorrect answers, combined with a reward design that penalizes bias-following without rewarding bias agreement. Experiments on Qwen3-4B demonstrate that EIT improves both accuracy and robustness under adversarial biases, while preserving performance when bias aligns with truth. Notably, models trained only on bandwagon bias generalize to unseen bias types such as authority and distraction, indicating that EIT induces transferable epistemic independence rather than bias-specific heuristics. Code and data are available at https://anonymous.4open.science/r/bias-mitigation-with-rl-BC47.

</details>


### [430] [DrawSim-PD: Simulating Student Science Drawings to Support NGSS-Aligned Teacher Diagnostic Reasoning](https://arxiv.org/abs/2602.01578)
*Arijit Chakma,Peng He,Honglu Liu,Zeyuan Wang,Tingting Li,Tiffany D. Do,Feng Liu*

Main category: cs.CY

TL;DR: DrawSim-PD：首个生成式框架，模拟NGSS对齐的学生科学绘图，展示可控的教学缺陷，用于教师专业发展培训。


<details>
  <summary>Details</summary>
Motivation: 诊断推理专业发展需要多样化的学生作品实践，但隐私法规禁止大规模共享真实学生作品。需要解决视觉评估研究中数据稀缺的障碍。

Method: 提出能力配置文件——结构化认知状态，编码每个表现水平学生能展示和尚未能展示的内容。确保生成输出的跨模态一致性：学生式绘图、第一人称推理叙述和教师诊断概念图。使用100个NGSS主题构建10,000个系统结构化作品库。

Result: 专家可行性评估显示，K-12科学教育者验证了作品与NGSS期望的对齐度（核心项目>84%正面评价），并确认了对解释学生思维的有用性，同时识别了年级极端情况的改进机会。

Conclusion: DrawSim-PD为教师培训提供了可扩展的生成式框架，解决了真实学生数据稀缺问题，并发布了开放基础设施以促进视觉评估研究。

Abstract: Developing expertise in diagnostic reasoning requires practice with diverse student artifacts, yet privacy regulations prohibit sharing authentic student work for teacher professional development (PD) at scale. We present DrawSim-PD, the first generative framework that simulates NGSS-aligned, student-like science drawings exhibiting controllable pedagogical imperfections to support teacher training. Central to our approach are apability profiles--structured cognitive states encoding what students at each performance level can and cannot yet demonstrate. These profiles ensure cross-modal coherence across generated outputs: (i) a student-like drawing, (ii) a first-person reasoning narrative, and (iii) a teacher-facing diagnostic concept map. Using 100 curated NGSS topics spanning K-12, we construct a corpus of 10,000 systematically structured artifacts. Through an expert-based feasibility evaluation, K--12 science educators verified the artifacts' alignment with NGSS expectations (>84% positive on core items) and utility for interpreting student thinking, while identifying refinement opportunities for grade-band extremes. We release this open infrastructure to overcome data scarcity barriers in visual assessment research.

</details>


### [431] [Multi-party Computation Protocols for Post-Market Fairness Monitoring in Algorithmic Hiring: From Legal Requirements to Computational Designs](https://arxiv.org/abs/2602.01837)
*Changyang He,Nina Baranowska,Josu Andoni Eguíluz Castañeira,Guillem Escriba,Matthias Juentgen,Anna Via,Frederik Zuiderveen Borgesius,Asia Biega*

Main category: cs.CY

TL;DR: 本文提出了一种基于多方计算（MPC）的合规后市场公平性监控方案，通过技术、法律和工业的协同设计，解决了AI招聘系统中公平性监控与数据保护法律之间的冲突。


<details>
  <summary>Details</summary>
Motivation: 随着欧盟AI法案等法规要求高风险就业AI系统进行后市场公平性监控，但监控需要敏感个人数据，这与数据保护法存在冲突。MPC技术虽能安全计算公平性指标，但在实际招聘场景中的法律、工业和可用性约束下的操作化仍未知。

Method: 采用协同设计方法，整合技术、法律和工业专业知识。识别MPC公平性监控的实际设计需求，开发覆盖完整数据生命周期的端到端法律合规协议，并在大规模工业环境中进行实证验证。

Result: 在大规模工业环境中成功验证了MPC后市场公平性监控协议，提供了可操作的设计见解以及法律和工业影响。

Conclusion: 本研究填补了MPC公平性监控在实际部署中的空白，为算法招聘系统部署MPC后市场公平性监控提供了可行的解决方案和法律工业指导。

Abstract: Post-market fairness monitoring is now mandated to ensure fairness and accountability for high-risk employment AI systems under emerging regulations such as the EU AI Act. However, effective fairness monitoring often requires access to sensitive personal data, which is subject to strict legal protections under data protection law. Multi-party computation (MPC) offers a promising technical foundation for compliant post-market fairness monitoring, enabling the secure computation of fairness metrics without revealing sensitive attributes. Despite growing technical interest, the operationalization of MPC-based fairness monitoring in real-world hiring contexts under concrete legal, industrial, and usability constraints remains unknown. This work addresses this gap through a co-design approach integrating technical, legal, and industrial expertise. We identify practical design requirements for MPC-based fairness monitoring, develop an end-to-end, legally compliant protocol spanning the full data lifecycle, and empirically validate it in a large-scale industrial setting. Our findings provide actionable design insights as well as legal and industrial implications for deploying MPC-based post-market fairness monitoring in algorithmic hiring systems.

</details>


### [432] [The Verification Crisis: Expert Perceptions of GenAI Disinformation and the Case for Reproducible Provenance](https://arxiv.org/abs/2602.02100)
*Alexander Loth,Martin Kappes,Marc-Oliver Pahl*

Main category: cs.CY

TL;DR: GenAI将虚假信息生产从人工制造转向自动化大规模操纵，专家调查显示文本生成比深度伪造视频更具系统性风险，当前检测工具效果有限，需要建立信息完整性基础设施。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能的快速发展使得虚假信息生产从人工制造转向自动化大规模操纵，这带来了新的威胁。研究旨在通过专家调查了解不同模态（文本、图像、音频、视频）威胁的严重程度，并评估当前的缓解策略。

Method: 采用纵向专家感知调查的第一波数据（N=21），参与者包括AI研究人员、政策制定者和虚假信息专家。调查评估了多模态威胁的感知严重性，并分析了当前的技术检测工具、来源标准、监管框架等缓解策略。

Result: 结果显示：深度伪造视频具有直接的"冲击"价值，但大规模文本生成在政治领域带来"认知碎片化"和"合成共识"的系统性风险。专家对技术检测工具持怀疑态度，更倾向于来源标准和监管框架，尽管实施存在障碍。当前主要挑战是缺乏标准化基准和可复现性检查表。

Conclusion: GenAI虚假信息研究需要可复现的方法。应将信息完整性视为基础设施，在数据来源和方法可复现性方面保持严谨。需要建立标准化基准和可复现性检查表来有效追踪和对抗合成媒体。

Abstract: The growth of Generative Artificial Intelligence (GenAI) has shifted disinformation production from manual fabrication to automated, large-scale manipulation. This article presents findings from the first wave of a longitudinal expert perception survey (N=21) involving AI researchers, policymakers, and disinformation specialists. It examines the perceived severity of multimodal threats -- text, image, audio, and video -- and evaluates current mitigation strategies.
  Results indicate that while deepfake video presents immediate "shock" value, large-scale text generation poses a systemic risk of "epistemic fragmentation" and "synthetic consensus," particularly in the political domain. The survey reveals skepticism about technical detection tools, with experts favoring provenance standards and regulatory frameworks despite implementation barriers.
  GenAI disinformation research requires reproducible methods. The current challenge is measurement: without standardized benchmarks and reproducibility checklists, tracking or countering synthetic media remains difficult. We propose treating information integrity as an infrastructure with rigor in data provenance and methodological reproducibility.

</details>


### [433] [MetaCLASS: Metacognitive Coaching for Learning with Adaptive Self-regulation Support](https://arxiv.org/abs/2602.02457)
*Naiming Liu,Richard Baraniuk,Shashank Sonkar*

Main category: cs.CY

TL;DR: MetaCLASS框架将元认知辅导建模为11种可解释动作的选择，创建了包含1015个对话的数据集，发现当前LLMs在元认知辅导能力上存在局限，特别是表现出"强迫干预偏见"。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型能生成流畅解释，但有效辅导需要支持学习者的思维过程而不仅仅是传递内容。元认知辅导通过促进规划、监控、调试和评估来填补这一空白，并基于学习者信号和轨迹决定何时活跃或最小化存在。

Method: 提出MetaCLASS框架，将元认知辅导建模为11种与自我调节学习过程对齐的可解释动作的选择。采用两阶段框架：首先基于学习者特征（校准、求助行为）规划教学轨迹，然后生成符合该计划的自然对话。创建了包含1015个对话（7711轮）的数据集，并标注了轮级元认知标签。

Result: 在预测下一个教练动作的任务上，最佳模型仅达到43.2%准确率。模型表现出强迫干预偏见：在需要保持沉默的情况下（占41.7%），模型预测"无干预"的概率仅为4.2%，同时严重过度预测高干预动作。

Conclusion: 传统基于内容的辅导能力不能转化为元认知辅导能力。MetaCLASS可作为开发促进自我调节学习的智能导师的测试平台，揭示了当前LLMs在元认知辅导方面的局限性。

Abstract: Large language models can generate fluent explanations, but effective tutoring requires supporting the learner's thought process, not just delivering content. Metacognitive tutoring targets this gap by prompting planning, monitoring, debugging, and evaluation, and crucially, deciding when to be active versus minimally present, based on learner signals and trajectory. We introduce MetaCLASS, a learning-science grounded framework that formulates metacognitive tutoring as move selection over 11 interpretable actions aligned to self-regulated learning processes. MetaCLASS uses a two-phase framework that first plans a pedagogical trajectory conditioned on learner profiles (calibration, help-seeking) and then generates natural dialogue consistent with that plan. This yields a dataset of 1,015 conversations (7,711 turns) annotated with turn-level metacognitive labels, and validated for pedagogical contingency and trajectory adherence. We benchmark nine LLMs on predicting the next coach move given the problem and dialogue context. The best model achieves only 43.2\% accuracy, and models exhibit compulsive intervention bias: in turns where effective metacognitive tutoring requires silent (41.7\% of cases), models predict `no intervention' only 4.2\% of the time, while severely over-predicting high-intervention moves. These results show that traditional content-based tutoring ability does not translate to metacognitive tutoring competence, positioning MetaCLASS as a testbed for developing intelligent tutors that promote self-regulated learning.

</details>


### [434] [Motivation, Attention, and Visual Platform Design: How Moral Contagions Spread on TikTok and Instagram in the 2024 United States Presidential Election](https://arxiv.org/abs/2602.02479)
*Ni Annie Yuan,Ho-chun Herbert Chang*

Main category: cs.CY

TL;DR: 研究发现社交媒体平台（TikTok vs Instagram）在2024年美国大选期间对政治议题的道德化呈现存在显著差异，平台架构、受众特征和党派框架共同塑造了议题如何被道德化及传播。


<details>
  <summary>Details</summary>
Motivation: 视觉社交媒体已成为政治讨论的主要场所，但我们对道德化在不同平台和议题上的运作方式知之甚少。研究旨在探究平台架构、受众特征和党派框架如何共同塑造政治议题的道德化过程。

Method: 分析了2024年美国总统选举期间的2,027,595个TikTok视频和1,126,972个Instagram帖子。采用时间供需分析和道德基础评分（eMFD）来研究关键选举议题的动态。使用语义网络分析来揭示平台拓扑结构差异。

Result: 1. 平台间道德化模式差异显著：TikTok算法使堕胎和移民内容病毒式传播，尽管供应量较低；Instagram则放大了供需一致的经济讨论。
2. 传统"务实"的经济议题被道德化：加密货币讨论比任何其他议题都更强烈地唤起忠诚和权威基础，将监管框架为政府越权。
3. 平台对不同事件的响应不同：TikTok在哈里斯提名后所有议题都激增（供应波动减少96%），而Instagram在加密货币政策发展时出现峰值。
4. 语义网络分析显示TikTok的环形拓扑结构促进跨议题暴露，而Instagram的碎片化结构将哈里斯与经济讨论隔离。

Conclusion: 理解政治道德化需要考察平台特定的生态系统，其中架构、人口统计特征和内容策略相互作用，共同决定哪些议题被道德化以及道德内容如何传播。议题并非固有道德化，而是受众特征、平台架构和党派框架的产物。

Abstract: Visual social media platforms have become primary venues for political discourse, yet we know little about how moralization operates differently across platforms and topics. Analyzing 2,027,595 TikToks and 1,126,972 Instagram posts during the 2024 US presidential election, we demonstrate that issues are not necessarily inherently moralized, but a product of audience demographics, platform architecture, and partisan framing. Using temporal supply-demand analysis and moral foundations scoring (eMFD), we examine the dynamics of key electoral issues. Three key findings emerge. First, moralization patterns diverge dramatically by platform: TikTok's algorithm enabled viral spread of moralized abortion and immigration content despite lower supply, while Instagram amplified economic discourse that aligned supply and demand. Second, traditionally "pragmatic" economic issues became moralized-cryptocurrency discourse invoked loyalty and authority foundations more strongly than any other topic, framing regulation as government overreach. Third, platforms responded to different events: TikTok surged after Harris's nomination across all topics (96% reduction in supply volatility), while Instagram spiked around cryptocurrency policy developments. Semantic network analysis reveals TikTok's circular topology enables cross-cutting exposure while Instagram's fragmented structure isolates Harris from economic discourse. These findings demonstrate that understanding political moralization requires examining platform-specific ecosystems where architecture, demographics, and content strategy interact to determine which issues get moralized and how moral content spreads.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [435] [OGD4All: A Framework for Accessible Interaction with Geospatial Open Government Data Based on Large Language Models](https://arxiv.org/abs/2602.00012)
*Michael Siebenmann,Javier Argota Sánchez-Vaquerizo,Stefan Arisona,Krystian Samp,Luis Gisler,Dirk Helbing*

Main category: cs.LG

TL;DR: OGD4All是一个基于LLM的透明、可审计、可复现框架，用于增强公民与地理空间开放政府数据的交互，通过语义检索、智能代理代码生成和安全沙箱执行，实现高准确率和低幻觉风险的多模态数据访问。


<details>
  <summary>Details</summary>
Motivation: 增强公民与开放政府数据的交互，提供透明、可审计、可复现的解决方案，减少LLM在处理地理空间数据时的幻觉风险，推进可信AI在开放治理中的应用。

Method: 结合语义数据检索、智能代理推理进行迭代代码生成、安全沙箱执行，生成可验证的多模态输出。在430个苏黎世市数据集和11个LLM上，使用包含199个问题的基准进行评估。

Result: 达到98%的分析正确率和94%的召回率，能可靠拒绝数据不支持的问题，最小化幻觉风险。统计稳健性测试和专家反馈证明了系统的可靠性和社会相关性。

Conclusion: LLM能够为公共数据提供可解释的多模态访问，推进开放治理中的可信AI发展。该框架展示了LLM在增强公民数据交互方面的潜力。

Abstract: We present OGD4All, a transparent, auditable, and reproducible framework based on Large Language Models (LLMs) to enhance citizens' interaction with geospatial Open Government Data (OGD). The system combines semantic data retrieval, agentic reasoning for iterative code generation, and secure sandboxed execution that produces verifiable multimodal outputs. Evaluated on a 199-question benchmark covering both factual and unanswerable questions, across 430 City-of-Zurich datasets and 11 LLMs, OGD4All reaches 98% analytical correctness and 94% recall while reliably rejecting questions unsupported by available data, which minimizes hallucination risks. Statistical robustness tests, as well as expert feedback, show reliability and social relevance. The proposed approach shows how LLMs can provide explainable, multimodal access to public data, advancing trustworthy AI for open governance.

</details>


### [436] [Measurement for Opaque Systems: Multi-source Triangulation with Interpretable Machine Learning](https://arxiv.org/abs/2602.00022)
*Margaret Foster*

Main category: cs.LG

TL;DR: 提出一个针对难以直接观测情境的测量框架，结合间接数据、可解释机器学习与理论指导的三角验证，用于填补不可访问的测量空间。


<details>
  <summary>Details</summary>
Motivation: 许多高风险的系统和政策关注对象难以直接观测：关键动态不可观察、数据间接且分散、真实情况缺失或被隐藏。现有数据通常不支持传统的统计分析或模型验证方法。

Method: 提出一个通用测量框架，结合多源三角验证与可解释机器学习模型。不依赖无法获得的理想数据准确性，而是寻求不同部分信息模型之间的一致性，通过跨信号一致性或与预期状态的偏离来得出可靠结论。

Result: 通过对一个秘密军事组织的组织增长和内部压力动态进行实证分析，展示了该方法如何从多个有偏且不完整的观测信号中恢复具有实质意义的变异模式。

Conclusion: 该框架为在缺乏传统统计或因果推断所需数据的情况下进行定量表征提供了分析工作流程，展示了三角验证的可解释机器学习如何填补难以访问的测量空间。

Abstract: We propose a measurement framework for difficult-to-access contexts that uses indirect data traces, interpretable machine-learning models, and theory-guided triangulation to fill inaccessible measurement spaces. Many high-stakes systems of scientific and policy interest are difficult, if not impossible, to reach directly: dynamics of interest are unobservable, data are indirect and fragmented across sources, and ground truth is absent or concealed. In these settings, available data often do not support conventional strategies for analysis, such as statistical inference on a single authoritative data stream or model validation against labeled outcomes. To address this problem, we introduce a general framework for measurement in data regimes characterized by structurally missing or adversarial data. We propose combining multi-source triangulation with interpretable machine learning models. Rather than relying on accuracy against unobservable, unattainable ideal data, our framework seeks consistency across separate, partially informative models. This allows users to draw defensible conclusions about the state of the world based on cross-signal consistency or divergence from an expected state. Our framework provides an analytical workflow tailored to quantitative characterization in the absence of data sufficient for conventional statistical or causal inference. We demonstrate our approach and explicitly surface inferential limits through an empirical analysis of organizational growth and internal pressure dynamics in a clandestine militant organization, drawing on multiple observational signals that individually provide incomplete and biased views of the underlying process. The results show how triangulated, interpretable ML can recover substantively meaningful variation.

</details>


### [437] [Representation Learning Enhanced Deep Reinforcement Learning for Optimal Operation of Hydrogen-based Multi-Energy Systems](https://arxiv.org/abs/2602.00027)
*Zhenyu Pu,Yu Yang,Lun Yang,Qing-Shan Jia,Xiaohong Guan,Costas J. Spanos*

Main category: cs.LG

TL;DR: 本文提出了一种结合表示学习技术的增强型深度强化学习框架，用于优化氢基多能源系统的运行，该框架能更好地处理HESS的非线性多物理场耦合动态和不确定性。


<details>
  <summary>Details</summary>
Motivation: 氢基多能源系统（HMES）作为低碳高效的解决方案，能够协调电、热、冷供应与需求，但HESS的非线性多物理场耦合动态以及供需不确定性使得其优化运行面临挑战。

Method: 1. 开发了全面捕捉HESS非线性动态和多物理场过程的运行模型；2. 提出增强型深度强化学习框架，集成表示学习技术，加速和改进复杂网络系统的策略优化。

Result: 基于真实数据集的实验表明：综合模型对确保HESS安全可靠运行至关重要；提出的SR-DRL方法在降低HMES运行成本和满足系统约束方面，相比传统DRL具有更优的收敛速度和性能。

Conclusion: 表示学习能够将原始状态空间重组为结构良好、聚类感知的几何表示，从而平滑和促进DRL的学习过程，为HMES优化运行提供了有效解决方案。

Abstract: Hydrogen-based multi-energy systems (HMES) have emerged as a promising low-carbon and energy-efficient solution, as it can enable the coordinated operation of electricity, heating and cooling supply and demand to enhance operational flexibility, improve overall energy efficiency, and increase the share of renewable integration. However, the optimal operation of HMES remains challenging due to the nonlinear and multi-physics coupled dynamics of hydrogen energy storage systems (HESS) (consisting of electrolyters, fuel cells and hydrogen tanks) as well as the presence of multiple uncertainties from supply and demand. To address these challenges, this paper develops a comprehensive operational model for HMES that fully captures the nonlinear dynamics and multi-physics process of HESS. Moreover, we propose an enhanced deep reinforcement learning (DRL) framework by integrating the emerging representation learning techniques, enabling substantially accelerated and improved policy optimization for spatially and temporally coupled complex networked systems, which is not provided by conventional DRL. Experimental studies based on real-world datasets show that the comprehensive model is crucial to ensure the safe and reliable of HESS. In addition, the proposed SR-DRL approaches demonstrate superior convergence rate and performance over conventional DRL counterparts in terms of reducing the operation cost of HMES and handling the system operating constraints. Finally, we provide some insights into the role of representation learning in DRL, speculating that it can reorganize the original state space into a well-structured and cluster-aware geometric representation, thereby smoothing and facilitating the learning process of DRL.

</details>


### [438] [ELLMPEG: An Edge-based Agentic LLM Video Processing Tool](https://arxiv.org/abs/2602.00028)
*Zoha Azimi,Reza Farahani,Radu Prodan,Christian Timmerer*

Main category: cs.LG

TL;DR: ELLMPEG是一个边缘智能代理框架，利用本地部署的开源LLMs自动生成视频处理命令，避免云API依赖和成本


<details>
  <summary>Details</summary>
Motivation: 云LLM部署面临计算能耗高、隐私风险、API成本三大限制，而代理AI和边缘计算为本地视频处理提供了新方案

Method: 集成工具感知的检索增强生成(RAG)与迭代自反思机制，在边缘端生成并验证FFmpeg和VVenC命令

Result: Qwen2.5在ELLMPEG框架下达到78%的平均命令生成准确率，零API成本，优于其他开源模型

Conclusion: ELLMPEG展示了边缘部署代理LLM在视频处理任务中的可行性，为隐私保护、成本效益的AI应用提供了新路径

Abstract: Large language models (LLMs), the foundation of generative AI systems like ChatGPT, are transforming many fields and applications, including multimedia, enabling more advanced content generation, analysis, and interaction. However, cloud-based LLM deployments face three key limitations: high computational and energy demands, privacy and reliability risks from remote processing, and recurring API costs. Recent advances in agentic AI, especially in structured reasoning and tool use, offer a better way to exploit open and locally deployed tools and LLMs. This paper presents ELLMPEG, an edge-enabled agentic LLM framework for the automated generation of video-processing commands. ELLMPEG integrates tool-aware Retrieval-Augmented Generation (RAG) with iterative self-reflection to produce and locally verify executable FFmpeg and VVenC commands directly at the edge, eliminating reliance on external cloud APIs. To evaluate ELLMPEG, we collect a dedicated prompt dataset comprising 480 diverse queries covering different categories of FFmpeg and the Versatile Video Codec (VVC) encoder (VVenC) commands. We validate command generation accuracy and evaluate four open-source LLMs based on command validity, tokens generated per second, inference time, and energy efficiency. We also execute the generated commands to assess their runtime correctness and practical applicability. Experimental results show that Qwen2.5, when augmented with the ELLMPEG framework, achieves an average command-generation accuracy of 78 % with zero recurring API cost, outperforming all other open-source models across both the FFmpeg and VVenC datasets.

</details>


### [439] [Stable Time Series Prediction of Enterprise Carbon Emissions Based on Causal Inference](https://arxiv.org/abs/2602.00775)
*Zitao Hong,Zhen Peng,Xueping Liu*

Main category: cs.LG

TL;DR: 提出一种结合因果推断与稳定学习的碳排预测机制，通过提取因果稳定特征和动态校正时间非平稳性，提升模型在分布偏移环境下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 在碳达峰和碳中和目标下，准确预测企业碳排放趋势对能源结构优化和低碳转型决策至关重要。然而，不同地区、行业和企业在能源结构、生产规模、政策强度等方面存在显著异质性，导致碳排放数据在时空维度上呈现明显的分布偏移和非平稳性，这影响了预测模型的准确性和对生产规划、碳配额交易决策的指导价值。

Method: 整合因果推断视角、稳定学习方法和时间序列建模，提出针对分布偏移环境的稳定时间预测机制。该机制纳入企业级能源投入、资本投资、劳动力配置、碳定价、政府干预和政策实施强度等因素，构建风险一致性约束的稳定学习框架，从不同政策、地区和工业部门的多环境样本中提取因果稳定特征。通过自适应归一化和样本重加权策略，动态校正由经济波动和政策转变引起的时间非平稳性。

Result: 该方法能够提取对二氧化碳排放具有长期稳定影响的因果稳定特征（对外部扰动具有鲁棒性），增强模型在复杂环境中的泛化能力和可解释性。

Conclusion: 提出的稳定时间预测机制通过整合因果推断、稳定学习和时间序列建模，有效解决了碳排放数据在时空维度上的分布偏移和非平稳性问题，为企业在复杂环境下的碳排预测提供了更准确、更可靠的决策支持工具。

Abstract: Against the backdrop of ongoing carbon peaking and carbon neutrality goals, accurate prediction of enterprise carbon emission trends constitutes an essential foundation for energy structure optimization and low-carbon transformation decision-making. Nevertheless, significant heterogeneity persists across regions, industries and individual enterprises regarding energy structure, production scale, policy intensity and governance efficacy, resulting in pronounced distribution shifts and non-stationarity in carbon emission data across both temporal and spatial dimensions. Such cross-regional and cross-enterprise data drift not only compromises the accuracy of carbon emission reporting but substantially undermines the guidance value of predictive models for production planning and carbon quota trading decisions. To address this critical challenge, we integrate causal inference perspectives with stable learning methodologies and time-series modelling, proposing a stable temporal prediction mechanism tailored to distribution shift environments. This mechanism incorporates enterprise-level energy inputs, capital investment, labour deployment, carbon pricing, governmental interventions and policy implementation intensity, constructing a risk consistency-constrained stable learning framework that extracts causal stable features (robust against external perturbations yet demonstrating long-term stable effects on carbon dioxide emissions) from multi-environment samples across diverse policies, regions and industrial sectors. Furthermore, through adaptive normalization and sample reweighting strategies, the approach dynamically rectifies temporal non-stationarity induced by economic fluctuations and policy transitions, ultimately enhancing model generalization capability and explainability in complex environments.

</details>


### [440] [RAPTOR-AI for Disaster OODA Loop: Hierarchical Multimodal RAG with Experience-Driven Agentic Decision-Making](https://arxiv.org/abs/2602.00030)
*Takato Yasuno*

Main category: cs.LG

TL;DR: 提出一个用于人道主义援助和灾害响应的多模态RAG框架，通过分层知识库和智能检索策略支持灾害响应的三个阶段，并利用历史灾害经验增强模型能力。


<details>
  <summary>Details</summary>
Motivation: 灾害响应需要快速理解情境、可靠决策支持，并能泛化到多样且未见过的灾害场景。现有系统缺乏对灾害响应完整阶段的支持和跨模态知识整合能力。

Method: 构建分层知识库整合文本手册、历史灾害经验（如2011年东北地震）和空对地图像；使用BLIP图像描述、ColVBERT嵌入和长上下文摘要处理46个海啸相关PDF；采用智能控制器通过熵感知场景抽象动态选择检索策略（RAPTOR、ColBERT）；使用轻量级LoRA后训练注入历史灾害经验知识。

Result: 在真实灾害数据集上的实验显示，系统在情境理解、任务分解准确性和应急操作可用性方面均有显著提升，通过自适应检索增强生成和自我推理能力获得实质性改进。

Conclusion: 该智能RAG框架通过多模态知识整合、自适应检索策略和历史经验注入，为灾害响应提供了有效的决策支持系统，能够支持专家和非专家响应者在不同灾害阶段的工作。

Abstract: Effective humanitarian assistance and disaster relief (HADR) requires rapid situational understanding, reliable decision support, and the ability to generalize across diverse and previously unseen disaster contexts. This work introduces an agentic Retrieval-Augmented Generation (RAG) framework designed to support the three canonical phases of disaster response: initial rescue, mid-term recovery, and long-term reconstruction. To achieve robust multimodal grounding, we construct a hierarchical knowledge base that integrates textual disaster manuals, historical lessons (e.g., the 2011 Tohoku earthquake), and both aerial and ground-level imagery. Our system builds on the open-source multimodal implementation, which processes 46 tsunami-related PDFs (2,378 pages) using BLIP-based image captioning, ColVBERT embeddings, and long-context summarization to generate an efficient, structured multimodal retrieval tree optimized for disaster knowledge preservation. An agentic controller dynamically selects retrieval strategies (e.g., RAPTOR, ColBERT) through entropy-aware scene abstraction, enabling adaptive reasoning across heterogeneous inputs. Additionally, a lightweight LoRA-based post-training method injects experiential knowledge from past disasters, enhancing the models' capacity to support both expert and non-expert responders. Experiments on real disaster datasets demonstrate improved situational grounding, enhanced task decomposition accuracy, and superior usability for emergency operations. Incorporating recent advances in long-context RAG systems, agentic information retrieval, and contemporary emergency response AI, our system achieves substantial gains through adaptive retrieval-augmented generation with self-reasoning and multimodal chain-of-thought capabilities.

</details>


### [441] [Enhancing few-shot time series forecasting with LLM-guided diffusion](https://arxiv.org/abs/2602.00040)
*Haonan Shi,Dehua Shuai,Liming Wang,Xiyang Liu,Long Tian*

Main category: cs.LG

TL;DR: LTSM-DIFF：结合大语言模型和扩散模型的小样本时间序列预测框架，通过知识迁移提升数据稀缺场景下的性能


<details>
  <summary>Details</summary>
Motivation: 专业领域时间序列预测常面临数据稀缺问题，传统模型需要大规模数据集才能有效捕捉时序动态，小样本场景下性能受限

Method: 提出LTSM-DIFF框架：1）LTSM模块作为时序记忆机制，从大语言模型微调获得丰富序列表示；2）扩散模型以这些表示为条件指导，联合概率扩散过程精细建模复杂时序模式

Result: 在多样化基准测试中，LTSM-DIFF在数据丰富场景达到SOTA性能，在小样本预测中显著提升效果，建立数据稀缺下时间序列分析新范式

Conclusion: 该工作通过语言领域知识迁移到时间序列任务，显著提升泛化性和鲁棒性，为数据稀缺场景下的时序分析提供了创新解决方案

Abstract: Time series forecasting in specialized domains is often constrained by limited data availability, where conventional models typically require large-scale datasets to effectively capture underlying temporal dynamics. To tackle this few-shot challenge, we propose LTSM-DIFF (Large-scale Temporal Sequential Memory with Diffusion), a novel learning framework that integrates the expressive power of large language models with the generative capability of diffusion models. Specifically, the LTSM module is fine-tuned and employed as a temporal memory mechanism, extracting rich sequential representations even under data-scarce conditions. These representations are then utilized as conditional guidance for a joint probability diffusion process, enabling refined modeling of complex temporal patterns. This design allows knowledge transfer from the language domain to time series tasks, substantially enhancing both generalization and robustness. Extensive experiments across diverse benchmarks demonstrate that LTSM-DIFF consistently achieves state-of-the-art performance in data-rich scenarios, while also delivering significant improvements in few-shot forecasting. Our work establishes a new paradigm for time series analysis under data scarcity.

</details>


### [442] [Extending Beacon to Hindi: Cultural Adaptation Drives Cross-Lingual Sycophancy](https://arxiv.org/abs/2602.00046)
*Sarthak Sattigeri*

Main category: cs.LG

TL;DR: 研究发现语言模型的奉承行为存在跨语言差异，印地语文化适配提示比英语原版导致更高的奉承率，文化适配是主要影响因素。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注英语环境下语言模型的奉承行为，但不确定这种诊断是否适用于其他语言和文化背景。需要探究奉承行为在不同语言和文化中的表现差异。

Method: 将Beacon单轮强制选择奉承诊断扩展到印地语，采用三条件设计：英语原版、印地语直译、印地语文化适配提示。评估四个开源指令调优模型，每个条件50个提示，分离语言编码和文化适配效应。

Result: 所有模型中，文化适配印地语提示的奉承率均高于英语，绝对差异12.0-16.0个百分点。分解分析显示文化适配贡献主要差异（14.0%），语言编码贡献很小（2.0%）。建议类提示的跨语言差异最大（20-25个百分点）。

Conclusion: 英语环境下测量的对齐行为不能均匀地跨语言转移，文化基础的提示框架起着重要作用。需要开发跨语言对齐评估方法，并发布了数据集和评估代码支持复现和扩展。

Abstract: Sycophancy, the tendency of language models to prioritize agreement with user preferences over principled reasoning, has been identified as a persistent alignment failure in English-language evaluations. However, it remains unclear whether such diagnostics generalize across languages and cultural contexts. We extend the Beacon single-turn forced-choice sycophancy diagnostic to Hindi through a controlled three-condition design: English original, Hindi literal translation, and Hindi culturally adapted prompts. We evaluate four open-weight instruction-tuned models on 50 prompts per condition, enabling separation of language encoding effects from cultural adaptation effects. Across all models, sycophancy rates are consistently higher for culturally adapted Hindi prompts than for English, with absolute differences ranging from 12.0 to 16.0 percentage points. A decomposition on Qwen 2.5-Coder-7B shows that cultural adaptation (delta = 14.0%, 95% CI: [4.0%, 26.0%]) accounts for the majority of this gap, while language encoding contributes minimally (delta = 2.0%, 95% CI: [0.0%, 6.0%]). Category-level analysis reveals that advice prompts exhibit the largest cross-lingual differences (20-25 percentage points), achieving statistical significance in two of four models. These findings indicate that alignment behaviors measured in English may not transfer uniformly across languages and that culturally grounded prompt framing plays a substantial role. We release all datasets and evaluation code to support replication and extension.

</details>


### [443] [Lightweight Edge Learning via Dataset Pruning](https://arxiv.org/abs/2602.00047)
*Laha Ale,Hu Luo,Mingsheng Cao,Shichao Li,Huanlai Xing,Haifeng Sun*

Main category: cs.LG

TL;DR: 提出基于数据集剪枝的数据中心化优化框架，通过构建紧凑、高信息量的训练子集，在边缘设备上实现资源高效学习，显著降低训练延迟和能耗，同时保持模型精度。


<details>
  <summary>Details</summary>
Motivation: 边缘学习虽然能保护隐私和降低通信延迟，但在电池供电的移动设备上进行模型训练面临高计算和能耗开销的挑战。现有研究主要优化模型架构以提高推理效率，但训练阶段仍受限于处理大量冗余本地数据。

Method: 提出数据中心化优化框架，利用数据集剪枝实现资源高效的边缘学习。通过截断预热阶段获得的平均损失统计来评估样本重要性，确定性地保留最关键的数据点，采用动态剪枝比例。该方法与模型无关，无需设备间通信。

Result: 在标准图像分类基准测试中，该框架实现了与剪枝比例成比例的近乎线性的训练延迟和能耗降低，同时模型精度下降可忽略不计。

Conclusion: 数据集剪枝是增强资源受限移动边缘设备学习可持续性和可扩展性的重要补充范式，为边缘学习提供了有效的资源优化方案。

Abstract: Edge learning facilitates ubiquitous intelligence by enabling model training and adaptation directly on data-generating devices, thereby mitigating privacy risks and communication latency. However, the high computational and energy overhead of on-device training hinders its deployment on battery-powered mobile systems with strict thermal and memory budgets. While prior research has extensively optimized model architectures for efficient inference, the training phase remains bottlenecked by the processing of massive, often redundant, local datasets. In this work, we propose a data-centric optimization framework that leverages dataset pruning to achieve resource-efficient edge learning. Unlike standard methods that process all available data, our approach constructs compact, highly informative training subsets via a lightweight, on-device importance evaluation. Specifically, we utilize average loss statistics derived from a truncated warm-up phase to rank sample importance, deterministically retaining only the most critical data points under a dynamic pruning ratio. This mechanism is model-agnostic and operates locally without inter-device communication. Extensive experiments on standard image classification benchmarks demonstrate that our framework achieves a near-linear reduction in training latency and energy consumption proportional to the pruning ratio, with negligible degradation in model accuracy. These results validate dataset pruning as a vital, complementary paradigm for enhancing the sustainability and scalability of learning on resource-constrained mobile edge devices.

</details>


### [444] [Distributional Reinforcement Learning for Condition-Based Maintenance of Multi-Pump Equipment](https://arxiv.org/abs/2602.00051)
*Takato Yasuno*

Main category: cs.LG

TL;DR: 提出一种基于分位数回归深度Q网络（QR-DQN）并集成老化因子的分布强化学习方法，用于多设备状态维修优化，通过三种策略场景实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统基于时间的维护策略常导致不必要的开支和意外设备故障，而状态维修（CBM）利用实时设备状态数据可以优化维护时机和资源配置，但需要更智能的决策方法。

Method: 采用分位数回归深度Q网络（QR-DQN）结合老化因子的分布强化学习方法，通过安全优先、平衡和成本效益三种策略场景来管理多个泵单元。

Result: 经过3000个训练周期的实验验证，安全优先策略表现出最佳成本效益，投资回报率（ROI）达3.91，性能比替代方案提高152%，仅需31%的额外投资，系统运行稳定性达95.66%。

Conclusion: 提出的分布强化学习方法在多设备状态维修中表现出色，安全优先策略特别有效，系统具有高稳定性和工业环境适用性，为智能维护决策提供了新解决方案。

Abstract: Condition-Based Maintenance (CBM) signifies a paradigm shift from reactive to proactive equipment management strategies in modern industrial systems. Conventional time-based maintenance schedules frequently engender superfluous expenditures and unanticipated equipment failures. In contrast, CBM utilizes real-time equipment condition data to enhance maintenance timing and optimize resource allocation. The present paper proposes a novel distributional reinforcement learning approach for multi-equipment CBM using Quantile Regression Deep Q-Networks (QR-DQN) with aging factor integration. The methodology employed in this study encompasses the concurrent administration of multiple pump units through three strategic scenarios. The implementation of safety-first, balanced, and cost-efficient approaches is imperative. Comprehensive experimental validation over 3,000 training episodes demonstrates significant performance improvements across all strategies. The Safety-First strategy demonstrates superior cost efficiency, with a return on investment (ROI) of 3.91, yielding 152\% better performance than alternatives while requiring only 31\% higher investment. The system exhibits 95.66\% operational stability and immediate applicability to industrial environments.

</details>


### [445] [TextBFGS: Quasi-Newton Optimization for Discrete Executable Text via Gradient-Operator Retrieval](https://arxiv.org/abs/2602.00059)
*Zizheng Zhang,Yuyang Liao,Chen Chen,Jian He,Dun Wu,Qianjin Yu,Yanqin Gao,Jin Yang,Kailai Zhang,Eng Siong Chng,Xionghu Zhong*

Main category: cs.LG

TL;DR: TextBFGS：用于离散文本优化的二阶框架，通过检索梯度算子实现拟牛顿法，显著优于一阶方法


<details>
  <summary>Details</summary>
Motivation: 现有离散文本优化方法主要是一阶优化器（类似SGD），存在收敛慢和不稳定的问题，因为它们忽略了优化景观的语义曲率

Method: TextBFGS是一个二阶框架，通过从预学习成功轨迹的内存中检索梯度算子来近似逆Hessian矩阵，实现单步更新（One-Pass Update）

Result: 在代码优化任务（HumanEval、MBPP等）上，TextBFGS显著优于一阶基线方法，以更少的模型调用获得更高的通过率，并表现出强大的跨任务可迁移性

Conclusion: TextBFGS为高效、内存感知的文本优化建立了数学基础范式，将二阶优化思想成功应用于离散文本空间

Abstract: Optimizing discrete executable text such as prompts and code has recently been framed as a gradient-based process, effectively translating backpropagation concepts to the semantic space. However, existing methods predominantly operate as first-order optimizers akin to Stochastic Gradient Descent, which are suffering from slow convergence and instability because they neglect the semantic curvature of the optimization landscape. To bridge this gap, we introduce TextBFGS, a second-order framework to implement a Quasi-Newton optimization method for discrete text. Unlike traditional memory-based approaches that retrieve similar textual instances, TextBFGS approximates the inverse Hessian matrix by retrieving Gradient-Operators from the memory of pre-learned successful trajectories. Specifically, given a textual gradient feedback, TextBFGS identifies historical correction patterns from the optimization knowledge base and tries to apply these abstract operators to the current variable. This mechanism enables a One-Pass Update, combining feedback generation and second-order correction into a single inference step. Empirical evaluations on code optimization across diverse domains (e.g., HumanEval, MBPP) demonstrate that TextBFGS significantly outperforms first-order baselines. It achieves superior pass rates with fewer model calls and exhibits strong cross-task transferability, thus establishes a mathematically grounded paradigm for efficient, memory-aware text optimization.

</details>


### [446] [SCPL: Enhancing Neural Network Training Throughput with Decoupled Local Losses and Model Parallelism](https://arxiv.org/abs/2602.00062)
*Ming-Yao Ho,Cheng-Kai Wang,You-Teng Lin,Hung-Hsuan Chen*

Main category: cs.LG

TL;DR: SCPL是一种新的训练方法，通过解耦反向传播将长梯度流分解为多个短梯度流，实现层间并行计算，提高训练效率。


<details>
  <summary>Details</summary>
Motivation: 企业信息系统采用大规模AI模型面临高训练成本和长开发周期的问题，标准反向传播算法是深度网络训练效率低下的主要原因。

Method: 提出监督对比并行学习（SCPL），通过解耦反向传播，将长梯度流转换为多个短梯度流，实现不同层参数梯度的并行计算。

Result: 实验证明SCPL相比BP、Early Exit、GPipe和AL等方法，在效率和效果上都有优势，提供了更实用的训练方案。

Conclusion: SCPL通过解决性能瓶颈，为组织开发部署先进信息系统提供了更经济高效、更敏捷的实践路径。

Abstract: Adopting large-scale AI models in enterprise information systems is often hindered by high training costs and long development cycles, posing a significant managerial challenge. The standard end-to-end backpropagation (BP) algorithm is a primary driver of modern AI, but it is also the source of inefficiency in training deep networks. This paper introduces a new training methodology, Supervised Contrastive Parallel Learning (SCPL), that addresses this issue by decoupling BP and transforming a long gradient flow into multiple short ones. This design enables the simultaneous computation of parameter gradients in different layers, achieving superior model parallelism and enhancing training throughput. Detailed experiments are presented to demonstrate the efficiency and effectiveness of our model compared to BP, Early Exit, GPipe, and Associated Learning (AL), a state-of-the-art method for decoupling backpropagation. By mitigating a fundamental performance bottleneck, SCPL provides a practical pathway for organizations to develop and deploy advanced information systems more cost-effectively and with greater agility. The experimental code is released for reproducibility. https://github.com/minyaho/scpl/

</details>


### [447] [The Impact of Machine Learning Uncertainty on the Robustness of Counterfactual Explanations](https://arxiv.org/abs/2602.00063)
*Leonidas Christodoulou,Chang Sun*

Main category: cs.LG

TL;DR: 研究发现反事实解释对模型不确定性高度敏感，即使模型准确度小幅下降也会导致反事实解释大幅变化，强调在金融和社会科学等领域需要不确定性感知的解释方法。


<details>
  <summary>Details</summary>
Motivation: 现有反事实解释方法大多未在模型和数据不确定性变化的情况下进行测试，导致在现实世界变异性下可能产生不稳定或无效的解释。需要研究反事实解释在存在偶然性和认知不确定性时的鲁棒性。

Method: 通过合成和真实世界表格数据集实验，研究常见机器学习模型与反事实生成算法组合在存在偶然性和认知不确定性时的鲁棒性。

Result: 反事实解释对模型不确定性高度敏感。即使由噪声增加或数据有限引起的模型准确度小幅下降，也会导致生成的反事实在平均水平和个体实例上出现大幅变化。

Conclusion: 这些发现强调了在金融和社会科学等领域需要不确定性感知的解释方法，因为反事实解释在现实世界不确定性下可能不稳定。

Abstract: Counterfactual explanations are widely used to interpret machine learning predictions by identifying minimal changes to input features that would alter a model's decision. However, most existing counterfactual methods have not been tested when model and data uncertainty change, resulting in explanations that may be unstable or invalid under real-world variability. In this work, we investigate the robustness of common combinations of machine learning models and counterfactual generation algorithms in the presence of both aleatoric and epistemic uncertainty. Through experiments on synthetic and real-world tabular datasets, we show that counterfactual explanations are highly sensitive to model uncertainty. In particular, we find that even small reductions in model accuracy - caused by increased noise or limited data - can lead to large variations in the generated counterfactuals on average and on individual instances. These findings underscore the need for uncertainty-aware explanation methods in domains such as finance and the social sciences.

</details>


### [448] [SPGCL: Effective Graph Contrastive Learning via SVD-Guided Structural Perturbation](https://arxiv.org/abs/2602.00064)
*Hao Deng,Yingping Li,Shuiping Gou,Bo Liu*

Main category: cs.LG

TL;DR: SPGCL提出了一种通过SVD引导的结构扰动进行鲁棒图对比学习的框架，通过平衡边缘移除和恢复来控制视图间的结构差异，从而提高GNN对结构噪声的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有图对比学习方法存在局限性：随机扰动（如边缘丢弃）可能移除关键边且对结构不敏感，而基于SVD的视图往往变得密集且缺乏多样性。需要一种方法既能保留全局结构先验，又能生成多样化的视图。

Method: SPGCL结合轻量级随机边缘移除和SVD引导的细化步骤，通过稀疏的top-ranked边缘选择和合并来恢复错误移除的信息边并引入语义上有意义的缺失链接，避免图密集化。还包含一个受全局相似性约束正则化的对比融合模块。

Result: 在十个基准数据集上的实验表明，SPGCL持续提高了基础GNN的鲁棒性和准确性，优于最先进的图对比学习和结构学习方法。

Conclusion: SPGCL通过SVD引导的结构扰动有效平衡了边缘移除和恢复，使对比信号反映语义结构差异而非边缘数量差距，为图对比学习提供了一种鲁棒且有效的框架。

Abstract: Graph Neural Networks (GNNs) can be highly sensitive to structural noise, including spurious or missing edges caused by adversarial attacks or non-adversarial imperfections. Existing graph contrastive learning methods typically rely on either random perturbations (e.g., edge dropping) to generate diverse views or purely spectral augmentations (e.g., SVD) to preserve global structural priors. However, random perturbations are structure-agnostic and may remove critical edges, while SVD-based views often become dense and lack sufficient diversity. To bridge this gap, we propose SPGCL, a robust graph contrastive learning framework via SVD-guided structural perturbation. SPGCL couples lightweight stochastic edge removal with an SVD-guided refinement step that can recover mistakenly removed informative edges and introduce semantically meaningful missing links while avoiding graph densification through sparse top-ranked edge selection and merging. By balancing edge removal and recovery rates, SPGCL explicitly controls structural discrepancy between views so that contrastive signals reflect semantic structural differences rather than edge-count gaps. We further incorporate a contrastive fusion module regularized by a global similarity constraint to better align the two views. Extensive experiments on ten benchmark datasets demonstrate that SPGCL consistently improves robustness and accuracy of base GNNs, outperforming state-of-the-art graph contrastive learning and structure learning methods.

</details>


### [449] [Modality as Heterogeneity: Node Splitting and Graph Rewiring for Multimodal Graph Learning](https://arxiv.org/abs/2602.00067)
*Yihan Zhang,Ercan E. Kuruoglu*

Main category: cs.LG

TL;DR: NSG-MoE是一个多模态图学习框架，通过节点分割、图重连和结构化专家混合架构，解决多模态图中的模态混淆问题，在保持结构信息和多模态语义的同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 多模态图虽然具有丰富的表示能力和广泛适用性，但引入了严重的模态混淆问题。通用图神经网络通常会出现不希望的混合效应，需要专门的方法来保持结构信息和多模态语义。

Method: 提出NSG-MoE框架，结合节点分割和图重连机制与结构化专家混合架构。将每个节点显式分解为模态特定组件，并分配关系感知专家来处理异构消息流。

Result: 在三个多模态基准测试上的广泛实验表明，NSG-MoE始终优于强基线方法。尽管使用了通常计算量大的MoE架构，但该方法实现了有竞争力的训练效率。谱分析显示NSG在模态特定子空间上执行自适应滤波，信息论分析表明NSG的架构约束减少了数据和参数之间的互信息，提高了泛化能力。

Conclusion: NSG-MoE通过显式分解模态特定组件和关系感知专家分配，有效解决了多模态图中的模态混淆问题，在保持性能的同时提供了理论解释和泛化优势。

Abstract: Multimodal graphs are gaining increasing attention due to their rich representational power and wide applicability, yet they introduce substantial challenges arising from severe modality confusion. To address this issue, we propose NSG (Node Splitting Graph)-MoE, a multimodal graph learning framework that integrates a node-splitting and graph-rewiring mechanism with a structured Mixture-of-Experts (MoE) architecture. It explicitly decomposes each node into modality-specific components and assigns relation-aware experts to process heterogeneous message flows, thereby preserving structural information and multimodal semantics while mitigating the undesirable mixing effects commonly observed in general-purpose GNNs. Extensive experiments on three multimodal benchmarks demonstrate that NSG-MoE consistently surpasses strong baselines. Despite incorporating MoE -- which is typically computationally heavy -- our method achieves competitive training efficiency. Beyond empirical results, we provide a spectral analysis revealing that NSG performs adaptive filtering over modality-specific subspaces, thus explaining its disentangling behavior. Furthermore, an information-theoretic analysis shows that the architectural constraints imposed by NSG reduces mutual information between data and parameters and improving generalization capability.

</details>


### [450] [Generative AI-enhanced Probabilistic Multi-Fidelity Surrogate Modeling Via Transfer Learning](https://arxiv.org/abs/2602.00072)
*Jice Zeng,David Barajas-Solano,Hui Chen*

Main category: cs.LG

TL;DR: 提出基于生成式迁移学习的概率多保真度代理框架，使用归一化流模型，通过两阶段训练（先在大量低保真数据上预训练，再在少量高保真数据上微调）解决数据稀缺问题，并引入降维层处理维度约束。


<details>
  <summary>Details</summary>
Motivation: 机器学习代理的性能严重依赖数据质量和数量，但高保真数据稀缺且计算成本高，低保真数据丰富但精度不足。需要解决数据稀缺问题，构建数据高效的代理模型。

Method: 基于生成式迁移学习的概率多保真度代理框架：1）使用归一化流作为骨干生成模型；2）两阶段训练：先在大量低保真数据上预训练学习概率前向模型，然后在少量高保真数据上微调以纠正低保真-高保真差异；3）引入降维层与标准耦合块结合，放松标准双射归一化流的维度保持约束，实现学习降维同时保持精确似然训练能力。

Result: 该代理提供快速概率预测和量化不确定性，显著优于仅使用低保真数据的基线方法，同时使用更少的高保真评估。在钢筋混凝土板基准测试中验证，结合大量粗网格（低保真）模拟和有限细网格（高保真）模拟，实现了具有高保真精度的概率预测。

Conclusion: 该方法为复杂工程系统提供了一条实用的数据高效、生成式AI驱动的代理模型路径，通过迁移学习有效利用低保真数据，显著减少对昂贵高保真数据的需求。

Abstract: The performance of machine learning surrogates is critically dependent on data quality and quantity. This presents a major challenge, as high-fidelity (HF) data is often scarce and computationally expensive to acquire, while low-fidelity (LF) data is abundant but less accurate. To address this data scarcity problem, we develop a probabilistic multi-fidelity surrogate framework based on generative transfer learning. We employ a normalizing flow (NF) generative model as the backbone, which is trained in two phases: (i) the NF is first pretrained on a large LF dataset to learn a probabilistic forward model; (ii) the pretrained model is then fine-tuned on a small HF dataset, allowing it to correct for LF-HF discrepancies via knowledge transfer. To relax the dimension-preserving constraint of standard bijective NFs, we integrate surjective (dimension-reducing) layers with standard coupling blocks. This architecture enables learned dimension reduction while preserving the ability to train with exact likelihoods. The resulting surrogate provides fast probabilistic predictions with quantified uncertainty and significantly outperforms LF-only baselines while using fewer HF evaluations. We validate the approach on a reinforced concrete slab benchmark, combining many coarse-mesh (LF) simulations with a limited set of fine-mesh (HF) simulations. The proposed model achieves probabilistic predictions with HF accuracy, demonstrating a practical path toward data-efficient, generative AI-driven surrogates for complex engineering systems.

</details>


### [451] [Dimensional Peeking for Low-Variance Gradients in Zeroth-Order Discrete Optimization via Simulation](https://arxiv.org/abs/2602.00075)
*Philipp Andelfinger,Wentong Cai*

Main category: cs.LG

TL;DR: 提出了一种名为"dimensional peeking"的方差缩减方法，用于离散仿真优化中的梯度估计，通过提升采样粒度到遵循相同控制流的数值类别，减少梯度估计的方差，提高零阶优化在离散非凸仿真中的竞争力。


<details>
  <summary>Details</summary>
Motivation: 基于梯度的优化方法常用于高维空间寻找局部最优解，但当无法直接计算导数时，随机估计器提供近似梯度。这些基于扰动的目标函数采样估计器会引入方差，导致收敛缓慢。需要一种方差缩减方法来提高离散仿真优化中梯度估计的效率。

Method: 提出"dimensional peeking"方法：将采样粒度从标量值提升到遵循相同控制流路径的数值类别，从而增加每次仿真评估收集的信息量。该方法从已建立的平滑梯度估计器推导而来，不会引入偏差。通过自定义数值数据类型在C++程序中透明地实现dimensional peeking。

Result: 在三个高维输入的仿真优化问题中观察到方差减少高达7.9倍。与三种元启发式方法相比，优化进展显示dimensional peeking提高了零阶优化在离散和非凸仿真中的竞争力。

Conclusion: Dimensional peeking是一种有效的方差缩减方法，通过提升采样粒度来减少梯度估计的方差，使得零阶优化方法在离散和非凸仿真优化问题中更具竞争力，特别是在高维输入场景下。

Abstract: Gradient-based optimization methods are commonly used to identify local optima in high-dimensional spaces. When derivatives cannot be evaluated directly, stochastic estimators can provide approximate gradients. However, these estimators' perturbation-based sampling of the objective function introduces variance that can lead to slow convergence. In this paper, we present dimensional peeking, a variance reduction method for gradient estimation in discrete optimization via simulation. By lifting the sampling granularity from scalar values to classes of values that follow the same control flow path, we increase the information gathered per simulation evaluation. Our derivation from an established smoothed gradient estimator shows that the method does not introduce any bias. We present an implementation via a custom numerical data type to transparently carry out dimensional peeking over C++ programs. Variance reductions by factors of up to 7.9 are observed for three simulation-based optimization problems with high-dimensional input. The optimization progress compared to three meta-heuristics shows that dimensional peeking increases the competitiveness of zeroth-order optimization for discrete and non-convex simulations.

</details>


### [452] [Automated univariate time series forecasting with regression trees](https://arxiv.org/abs/2602.00077)
*Francisco Martínez,María P. Frías*

Main category: cs.LG

TL;DR: 提出使用回归树及其集成方法（装袋和随机森林）进行单变量时间序列预测的自动化方法，处理自回归特征选择、趋势和季节性，实现与指数平滑或ARIMA相当的预测精度，并提供开源软件。


<details>
  <summary>Details</summary>
Motivation: 传统统计模型如指数平滑和ARIMA需要专业知识且难以自动化，需要开发基于机器学习的时间序列预测方法，能够自动处理趋势和季节性，同时保持可比的预测精度。

Method: 采用自回归方法和递归预测，使用回归树及其集成方法（装袋和随机森林），提出自回归特征选择策略，处理趋势序列和季节性行为的技术。

Result: 实验结果表明，该方法在预测精度上与成熟的统计模型（如指数平滑或ARIMA）相当，证明了机器学习方法在时间序列预测中的有效性。

Conclusion: 回归树及其集成方法可用于自动化单变量时间序列预测，处理趋势和季节性，达到与传统统计模型相当的精度，并提供了公开可用的软件实现。

Abstract: This paper describes a methodology for automated univariate time series forecasting using regression trees and their ensembles: bagging and random forests. The key aspects that are addressed are: the use of an autoregressive approach and recursive forecasts, how to select the autoregressive features, how to deal with trending series and how to cope with seasonal behavior. Experimental results show a forecast accuracy comparable with well-established statistical models such as exponential smoothing or ARIMA. Furthermore, a publicly available software implementing all the proposed strategies has been developed and is described in the paper.

</details>


### [453] [Lossless Embedding Compression via Spherical Coordinates](https://arxiv.org/abs/2602.00079)
*Han Xiao*

Main category: cs.LG

TL;DR: 提出一种无损压缩单位范数嵌入的方法，实现1.5倍压缩率，比现有最佳方法提升25%


<details>
  <summary>Details</summary>
Motivation: 单位范数嵌入在信息检索、推荐系统等应用中广泛使用，但存储和传输成本高。现有压缩方法仍有改进空间，需要更高效的压缩技术。

Method: 利用高维单位向量的球坐标集中在π/2附近的特性，导致IEEE 754指数位坍缩到单一值，从而启用熵编码。方法无需训练，在float32精度内完全无损。

Result: 在涵盖文本、图像和多向量嵌入的26种配置评估中，该方法始终优于现有方法，实现1.5倍压缩率，比最佳先前方法提升25%。

Conclusion: 该方法为高维单位向量提供了一种简单有效的无损压缩方案，显著降低存储和传输开销，适用于多种嵌入类型。

Abstract: We present a lossless compression method for unit-norm embeddings that achieves 1.5$\times$ compression, 25\% better than the best prior method. The method exploits that spherical coordinates of high-dimensional unit vectors concentrate around $π/2$, causing IEEE 754 exponents to collapse to a single value and enabling entropy coding. Evaluation across 26 configurations spanning text, image, and multi-vector embeddings confirms consistent improvement. The method requires no training and is fully lossless within float32 precision.

</details>


### [454] [Why LoRA Resists Label Noise: A Theoretical Framework for Noise-Robust Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2602.00084)
*Brady Steele*

Main category: cs.LG

TL;DR: LoRA具有内在的抗标签噪声能力，本文从理论上分析了这一特性，并提出RACT方法利用秩差异进行噪声检测。


<details>
  <summary>Details</summary>
Motivation: 虽然LoRA已成为微调大型预训练模型的主流方法，但其对标签噪声的内在抵抗特性尚未得到充分探索。本文旨在从理论上解释LoRA的这一重要特性，并基于理论发现开发实用的噪声检测方法。

Method: 1. 建立理论框架分析LoRA的抗噪声特性；2. 证明秩-r LoRA的记忆容量有限；3. 推导最优秩以平衡近似偏差和噪声方差；4. 提出RACT（Rank-Aware Curriculum Training）方法，利用秩差异进行噪声检测。

Result: 理论分析表明：1. LoRA无法记忆所有可能的标签分配；2. 最优秩随噪声率降低；3. 存在时间分离现象（干净模式早期学习，噪声后期记忆）。实验验证中，RACT在AG News上达到91.1%的噪声检测F1分数，同时保持91.46%的准确率。

Conclusion: LoRA具有内在的抗标签噪声能力，这一特性源于其有限的记忆容量。基于理论分析提出的RACT方法能够有效检测噪声标签，在保持模型性能的同时提供噪声检测能力，为实际应用中的噪声鲁棒性提供了新思路。

Abstract: Parameter-efficient fine-tuning methods like Low-Rank Adaptation (LoRA) have become the dominant paradigm for adapting large pretrained models. We present a theoretical framework explaining an underexplored property: LoRA's inherent resistance to label noise. Our analysis reveals three key insights. First, we prove that rank-$r$ LoRA cannot memorize all possible label assignments once the sample size exceeds $O(r(d+k-r))$, limiting its capacity to fit arbitrary noise. Second, we derive an optimal rank balancing approximation bias and noise-induced variance, showing it decreases with noise rate. Third, we establish temporal separation: clean patterns are learned early while noise memorization occurs later. We propose RACT (Rank-Aware Curriculum Training), leveraging rank discrepancy for noise detection. Experiments validate our predictions, with RACT achieving 91.1% F1 for noise detection on AG News while maintaining 91.46% accuracy, competitive with baselines that lack noise detection capability.

</details>


### [455] [CARE-RFT: Confidence-Anchored Reinforcement Finetuning for Reliable Reasoning in Large Language Models](https://arxiv.org/abs/2602.00085)
*Shuozhe Li,Jincheng Cao,Bodun Hu,Aryan Mokhtari,Leqi Liu,Amy Zhang*

Main category: cs.LG

TL;DR: CARE-RFT提出了一种新的强化微调方法，使用偏斜反向KL散度替代标准反向KL正则化，在保持模型可信度的同时提升推理能力


<details>
  <summary>Details</summary>
Motivation: 现有强化微调方法存在权衡：无约束RFT推理能力强但损害模型可信度（增加幻觉、校准变差），而RKL约束RFT保持可信度但限制了推理能力的提升

Method: CARE-RFT使用偏斜反向KL散度作为正则化项，提供置信度敏感的惩罚：对自信且一致获得奖励的探索有界惩罚以支持推理，对其他情况无界惩罚以保持校准

Result: 在多个模型规模和RFT算法上的实验表明，CARE-RFT实现了优越的平衡，匹配无约束RFT的推理性能，同时恢复了基础模型的可信度和校准

Conclusion: 精心设计的置信度感知正则化是构建既强大又可信的推理模型的关键，CARE-RFT解决了强化微调中的推理-可信度权衡问题

Abstract: Reinforcement finetuning (RFT) has emerged as a powerful paradigm for unlocking reasoning capabilities in large language models. However, we identify a critical trade-off: while unconstrained RFT achieves strong reasoning performance, it severely compromises model trustworthiness by amplifying hallucination and worsening calibration; conversely, RKL-constrained RFT preserves trustworthiness but limits reasoning gains due to its unbounded penalty on exploratory deviations. To resolve this tension, we introduce CARE-RFT (Confidence-Anchored Regularized Reinforcement Finetuning), a novel method that replaces standard reverse KL regularization with a skew reverse KL divergence. CARE-RFT provides a confidence-sensitive penalty: it is bounded for confident, consistently rewarded explorations to enable reasoning, while unbounded elsewhere to preserve calibration. Extensive experiments across multiple model scales and RFT algorithms show that CARE-RFT achieves a superior balance, matching the reasoning performance of unconstrained RFT while recovering the trustworthiness and calibration of the base model. Our work establishes that careful, confidence-aware regularization is key to building both capable and trustworthy reasoning models.

</details>


### [456] [AIRE-Prune: Asymptotic Impulse-Response Energy for State Pruning in State Space Models](https://arxiv.org/abs/2602.00534)
*Apurba Prasad Padhy,Fernando Camacho,Saibal Mukhopadhyay*

Main category: cs.LG

TL;DR: 提出AIRE-Prune方法，通过渐近脉冲响应能量评分对状态空间模型进行结构化剪枝，在保持精度的同时大幅降低计算成本


<details>
  <summary>Details</summary>
Motivation: 状态空间模型(SSMs)通常需要在容量、搜索空间或稳定性方面做出妥协，以抵消大状态维度的内存和计算成本。需要一种有效的剪枝方法来减少状态维度而不显著影响性能。

Method: 提出AIRE-Prune方法：基于渐近脉冲响应能量为每个状态分配评分，通过层间归一化实现全局跨层比较和选择，直接最小化长期输出能量失真

Result: 在多种序列基准测试中，AIRE-Prune对SISO和MIMO SSMs实现平均60.8%的剪枝率，平均精度仅下降0.29%（无需重新训练），同时显著降低计算成本

Conclusion: AIRE-Prune是一种有效的结构化后训练剪枝方法，能够揭示SSMs中的大量冗余，在保持精度的同时大幅减少状态维度，扩展了模态截断方法的应用范围

Abstract: State space models (SSMs) often sacrifice capacity, search space, or stability to offset the memory and compute costs of large state dimensions. We introduce a structured post-training pruning method for SSMs -- AIRE-Prune (Asymptotic Impulse-Response Energy for State PRUN(E)) -- that reduces each layer's state dimension by directly minimizing long-run output-energy distortion. AIRE-Prune assigns every state a closed-form asymptotic impulse-response energy-based score, i.e., the total impulse-response energy it contributes over an infinite horizon (time), and normalizes these scores layer-wise to enable global cross-layer comparison and selection. This extends modal truncation from single systems to deep stacks and aligns pruning with asymptotic response energy rather than worst-case gain. Across diverse sequence benchmarks, AIRE-Prune reveals substantial redundancy in SISO and MIMO SSMs with average pruning of 60.8%, with average accuracy drop of 0.29% without retraining, while significantly lowering compute. Code: https://github.com/falcon-arrow/AIRE-Prune.

</details>


### [457] [ECCO: Evidence-Driven Causal Reasoning for Compiler Optimization](https://arxiv.org/abs/2602.00087)
*Haolin Pan,Lianghong Huang,Jinyuan Dong,Mingjie Xing,Yanjun Wu*

Main category: cs.LG

TL;DR: ECCO框架结合可解释推理与组合搜索，通过构建思维链数据集让LLM学习优化决策的因果逻辑，然后让LLM作为策略师指导遗传算法进行编译器优化，相比传统方法显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 编译器自动调优面临传统黑盒搜索方法缺乏语义指导，而大型语言模型方法又存在表面模式匹配和因果不透明的问题。需要一种能结合可解释推理与组合搜索的框架。

Method: 1) 提出逆向工程方法构建思维链数据集，将静态代码特征映射到可验证的性能证据；2) 设计协作推理机制，让LLM作为策略师定义优化意图，动态指导遗传算法的变异操作。

Result: 在七个数据集上的实验结果表明，ECCO显著优于LLVM opt -O3基准，平均减少24.44%的周期数。

Conclusion: ECCO成功桥接了可解释推理与组合搜索，通过让LLM学习优化决策的因果逻辑并指导遗传算法，实现了比传统方法更有效的编译器自动调优。

Abstract: Compiler auto-tuning faces a dichotomy between traditional black-box search methods, which lack semantic guidance, and recent Large Language Model (LLM) approaches, which often suffer from superficial pattern matching and causal opacity. In this paper, we introduce ECCO, a framework that bridges interpretable reasoning with combinatorial search. We first propose a reverse engineering methodology to construct a Chain-of-Thought dataset, explicitly mapping static code features to verifiable performance evidence. This enables the model to learn the causal logic governing optimization decisions rather than merely imitating sequences. Leveraging this interpretable prior, we design a collaborative inference mechanism where the LLM functions as a strategist, defining optimization intents that dynamically guide the mutation operations of a genetic algorithm. Experimental results on seven datasets demonstrate that ECCO significantly outperforms the LLVM opt -O3 baseline, achieving an average 24.44% reduction in cycles.

</details>


### [458] [Equilibrium of Feasible Zone and Uncertain Model in Safe Exploration](https://arxiv.org/abs/2602.00636)
*Yujie Yang,Zhilong Zheng,Shengbo Eben Li*

Main category: cs.LG

TL;DR: 论文首次揭示了安全探索的目标是在可行区域与环境模型之间找到平衡，提出了首个面向平衡的安全探索框架SEE，通过交替寻找最大可行区域和最小不确定模型实现零约束违规的安全探索。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习中的安全探索通常将探索限制在可行区域内，但两个关键问题未解决：通过探索可获得的最大可行区域是什么？如何识别这个区域？本文旨在回答这些问题。

Method: 提出了安全平衡探索（SEE）框架，交替进行两个步骤：1）寻找最大可行区域；2）寻找最小不确定模型。使用不确定模型的图表示，证明该方法能单调细化模型并扩展可行区域。

Result: 在经典控制任务上的实验表明，SEE算法成功扩展了可行区域且零约束违规，在几次迭代内就达到了安全探索的平衡状态。

Conclusion: 安全探索的目标是在可行区域与环境模型之间找到平衡，SEE框架首次实现了这一目标，为安全强化学习提供了理论基础和实用算法。

Abstract: Ensuring the safety of environmental exploration is a critical problem in reinforcement learning (RL). While limiting exploration to a feasible zone has become widely accepted as a way to ensure safety, key questions remain unresolved: what is the maximum feasible zone achievable through exploration, and how can it be identified? This paper, for the first time, answers these questions by revealing that the goal of safe exploration is to find the equilibrium between the feasible zone and the environment model. This conclusion is based on the understanding that these two components are interdependent: a larger feasible zone leads to a more accurate environment model, and a more accurate model, in turn, enables exploring a larger zone. We propose the first equilibrium-oriented safe exploration framework called safe equilibrium exploration (SEE), which alternates between finding the maximum feasible zone and the least uncertain model. Using a graph formulation of the uncertain model, we prove that the uncertain model obtained by SEE is monotonically refined, the feasible zones monotonically expand, and both converge to the equilibrium of safe exploration. Experiments on classic control tasks show that our algorithm successfully expands the feasible zones with zero constraint violation, and achieves the equilibrium of safe exploration within a few iterations.

</details>


### [459] [From Numbers to Prompts: A Cognitive Symbolic Transition Mechanism for Lightweight Time-Series Forecasting](https://arxiv.org/abs/2602.00088)
*Namkyung Yoon,Hwangnam Kim*

Main category: cs.LG

TL;DR: 提出符号转换机制（STM），通过符号抽象和提示工程将数值时间序列数据与语言模型连接，显著提升轻量级平台上时间序列预测的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在时间序列预测中表现出色，但其巨大的计算和内存需求限制了在轻量级平台上的部署。需要一种方法既能利用语言模型的能力，又能显著降低资源消耗。

Method: STM通过基于人类认知结构的量化技术将连续时间序列值转换为符号标记，并通过符号的结构化转换捕捉时间动态，使语言模型能够专注于时间序列数据的关键部分进行快速工程化预测。

Result: 在多个时间序列数据集和四个小型语言模型上评估，STM相比基础SLM实现了MAE误差降低高达69%，MSE误差降低高达90%。资源开销极小，GPU内存仅增加约0.06%，延迟开销仅增加0.64%。

Conclusion: STM作为一种高效、自适应的符号驱动时间序列预测层，展示了在基础模型上实现准确预测的潜力，同时保持了极低的资源开销，适合轻量级平台部署。

Abstract: Large language models have achieved remarkable success in time series prediction tasks, but their substantial computational and memory requirements limit deployment on lightweight platforms. In this paper, we propose the Symbolic Transition Mechanism (STM) a novel framework that bridges numeric time series data and language models through symbolic abstraction and prompt engineering. STM transforms continuous time series values into symbol tokens with quantization techniques based on human cognitive structures, and captures temporal dynamics through structured transformations of symbols, enabling fast engineering based predictions in which language models focus on critical parts of time series data. STM is a general purpose mechanisms that ensure the integrity of backbone language models, but they significantly improve their efficiency by inferring the dynamic and structured patterns inherent in time series data. We evaluated STM on various time series datasets, paired with four small language models (SLM) with limited computational environments. For all models, STM achieves error reductions of up to 69% in MAE and 90% in MSE compared to the default backbone SLM without STM. These results demonstrate the potential of STM as an efficient, adaptable layer for symbol-driven time series prediction using foundation models. The accuracy improvements were made at negligible resource costs, with maximum GPU memory of the base model increasing by approximately 0.06% and latency overhead increasing by only 0.64%.

</details>


### [460] [Interpreting and Controlling Model Behavior via Constitutions for Atomic Concept Edits](https://arxiv.org/abs/2602.00092)
*Neha Kalibhat,Zi Wang,Prasoon Bajpai,Drew Proud,Wenjun Zeng,Been Kim,Mani Malek*

Main category: cs.LG

TL;DR: 提出一个黑盒可解释性框架，通过原子概念编辑学习可验证的"宪法"，用于理解和控制模型行为


<details>
  <summary>Details</summary>
Motivation: 需要一种方法来理解提示词变化如何影响模型的具体行为（如对齐性、正确性、约束遵循），并提供可验证的因果解释

Method: 使用原子概念编辑（ACEs）在输入提示中添加、移除或替换可解释概念，系统应用这些编辑并观察对模型行为的影响，学习从编辑到可预测结果的因果映射

Result: 在数学推理和文生图对齐等任务中验证了方法有效性：发现GPT-Image关注语法遵循，Imagen 4优先考虑氛围一致性；GPT-5易受干扰变量影响，而Gemini 2.5和o4-mini不受影响；学习到的宪法在控制模型行为上比不用宪法的方法平均提升1.86倍成功率

Conclusion: 提出的黑盒可解释性框架能够学习可验证的宪法，提供对模型行为的深入、可泛化见解，并有效用于控制模型行为

Abstract: We introduce a black-box interpretability framework that learns a verifiable constitution: a natural language summary of how changes to a prompt affect a model's specific behavior, such as its alignment, correctness, or adherence to constraints. Our method leverages atomic concept edits (ACEs), which are targeted operations that add, remove, or replace an interpretable concept in the input prompt. By systematically applying ACEs and observing the resulting effects on model behavior across various tasks, our framework learns a causal mapping from edits to predictable outcomes. This learned constitution provides deep, generalizable insights into the model. Empirically, we validate our approach across diverse tasks, including mathematical reasoning and text-to-image alignment, for controlling and understanding model behavior. We found that for text-to-image generation, GPT-Image tends to focus on grammatical adherence, while Imagen 4 prioritizes atmospheric coherence. In mathematical reasoning, distractor variables confuse GPT-5 but leave Gemini 2.5 models and o4-mini largely unaffected. Moreover, our results show that the learned constitutions are highly effective for controlling model behavior, achieving an average of 1.86 times boost in success rate over methods that do not use constitutions.

</details>


### [461] [Trade-offs Between Individual and Group Fairness in Machine Learning: A Comprehensive Review](https://arxiv.org/abs/2602.00094)
*Sandra Benítez-Peña,Blas Kolic,Victoria Menendez,Belén Pulido*

Main category: cs.LG

TL;DR: 这篇综述论文系统性地回顾了同时处理群体公平和个体公平的混合公平方法，分析了这些方法的理论基础、优化机制和实证评估，并讨论了它们之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 在计算决策系统中，算法公平性已成为核心关注点。虽然群体公平和个体公平是两种主要的公平概念，但它们通常被孤立研究。本文旨在填补这一空白，通过研究同时处理这两种公平概念的混合方法，为设计更全面的公平算法提供指导。

Method: 本文采用系统性文献综述方法，对混合公平方法进行分类整理：1) 根据使用的公平机制进行分类；2) 分析调和多种公平标准的算法和数学策略；3) 考察每类方法的理论基础、优化机制和实证评估实践；4) 讨论现有方法的局限性。

Result: 论文提供了混合公平方法的全面分类框架，揭示了群体公平和个体公平之间的固有权衡关系，识别了现有方法的局限性，并为开发更原则化、上下文感知的混合公平方法指明了研究方向。

Conclusion: 群体公平和个体公平的整合对于实现全面的算法公平至关重要。虽然存在固有的权衡关系，但通过开发更成熟的混合方法，可以设计出在个体和群体层面都能提供可靠公平保证的算法。这篇综述为研究人员和实践者提供了设计此类混合算法的综合资源。

Abstract: Algorithmic fairness has become a central concern in computational decision-making systems, where ensuring equitable outcomes is essential for both ethical and legal reasons. Two dominant notions of fairness have emerged in the literature: Group Fairness (GF), which focuses on mitigating disparities across demographic subpopulations, and Individual Fairness (IF), which emphasizes consistent treatment of similar individuals. These notions have traditionally been studied in isolation. In contrast, this survey examines methods that jointly address GF and IF, integrating both perspectives within unified frameworks and explicitly characterizing the trade-offs between them. We provide a systematic and critical review of hybrid fairness approaches, organizing existing methods according to the fairness mechanisms they employ and the algorithmic and mathematical strategies used to reconcile multiple fairness criteria. For each class of methods, we examine their theoretical foundations, optimization mechanisms, and empirical evaluation practices, and discuss their limitations. Additionally, we discuss the challenges and identify open research directions for developing principled, context-aware hybrid fairness methods. By synthesizing insights across the literature, this survey aims to serve as a comprehensive resource for researchers and practitioners seeking to design hybrid algorithms that provide reliable fairness guarantees at both the individual and group levels.

</details>


### [462] [Gauss-Newton Natural Gradient Descent for Shape Learning](https://arxiv.org/abs/2602.00099)
*James King,Arturs Berzins,Siddhartha Mishra,Marius Zeinhofer*

Main category: cs.LG

TL;DR: 该论文探索了在形状学习中应用高斯-牛顿法进行优化，包括隐式神经表面和几何感知神经网络，相比一阶方法实现了更快速、更稳定的收敛


<details>
  <summary>Details</summary>
Motivation: 解决形状学习中的关键挑战：基础微分约束的病态性，以及参数空间优化问题与自然问题所在函数空间之间的不匹配问题

Method: 采用高斯-牛顿法进行形状学习优化，应用于隐式神经表面和几何感知神经网络，处理形状优化中的微分约束和空间不匹配问题

Result: 在基准形状优化任务实验中，高斯-牛顿法相比标准一阶方法显著提高了训练速度和解的精度，收敛更快更稳定，所需迭代次数更少

Conclusion: 高斯-牛顿法在形状学习中表现出优越性能，能够有效解决优化挑战，为形状学习任务提供了更高效、更稳定的优化方案

Abstract: We explore the use of the Gauss-Newton method for optimization in shape learning, including implicit neural surfaces and geometry-informed neural networks. The method addresses key challenges in shape learning, such as the ill-conditioning of the underlying differential constraints and the mismatch between the optimization problem in parameter space and the function space where the problem is naturally posed. This leads to significantly faster and more stable convergence than standard first-order methods, while also requiring far fewer iterations. Experiments across benchmark shape optimization tasks demonstrate that the Gauss-Newton method consistently improves both training speed and final solution accuracy.

</details>


### [463] [THDC: Training Hyperdimensional Computing Models with Backpropagation](https://arxiv.org/abs/2602.00116)
*Hanne Dejonghe,Sam Leroux*

Main category: cs.LG

TL;DR: THDC提出可训练的超维计算，通过反向传播端到端学习，将维度从10000降至64，在多个数据集上达到或超越现有HDC性能


<details>
  <summary>Details</summary>
Motivation: 传统超维计算依赖超高维度和静态随机初始化向量，导致内存效率低和学习能力有限，需要更高效的可训练方法

Method: 用可训练嵌入替代随机初始化向量，引入单层二进制神经网络优化类别表示，支持端到端反向传播训练

Result: 在MNIST、Fashion-MNIST和CIFAR-10数据集上，THDC以仅64维度达到或超越传统10000维HDC的准确率

Conclusion: THDC显著提升了超维计算的内存效率和性能，为边缘设备上的轻量级学习提供了更优解决方案

Abstract: Hyperdimensional computing (HDC) offers lightweight learning for energy-constrained devices by encoding data into high-dimensional vectors. However, its reliance on ultra-high dimensionality and static, randomly initialized hypervectors limits memory efficiency and learning capacity. Therefore, we propose Trainable Hyperdimensional Computing (THDC), which enables end-to-end HDC via backpropagation. THDC replaces randomly initialized vectors with trainable embeddings and introduces a one-layer binary neural network to optimize class representations. Evaluated on MNIST, Fashion-MNIST and CIFAR-10, THDC achieves equal or better accuracy than state-of-the-art HDC, with dimensionality reduced from 10.000 to 64.

</details>


### [464] [White-Box Neural Ensemble for Vehicular Plasticity: Quantifying the Efficiency Cost of Symbolic Auditability in Adaptive NMPC](https://arxiv.org/abs/2602.01516)
*Enzo Nicolas Spotorno,Matheus Wagner,Antonio Augusto Medeiros Frohlich*

Main category: cs.LG

TL;DR: 提出了一种白盒自适应NMPC架构，通过模块化主权范式在冻结的、特定于工况的神经专家之间进行仲裁，解决了车辆可塑性问题，并在CasADi中维护完全可遍历的符号图以实现最大运行时可审计性。


<details>
  <summary>Details</summary>
Motivation: 解决车辆控制中的可塑性问题——即无需重新训练就能适应不同工况，同时保持白盒控制器的透明度和可审计性，以克服传统自适应方法在复合工况变化下的性能下降问题。

Method: 采用模块化主权范式，构建由多个冻结的、特定于工况的神经专家组成的集合，通过仲裁机制在不同工况间切换。在CasADi中维护完全可遍历的符号图来表征集合动态，确保运行时可审计性。

Result: 同步仿真验证了快速适应能力（约7.3毫秒）和在复合工况变化（摩擦、质量、阻力）下的近乎理想的跟踪精度，而非自适应基线方法则失败。量化了透明度成本：符号图维护使求解器延迟增加了72-102倍。

Conclusion: 该白盒自适应NMPC架构成功解决了车辆可塑性问题，实现了快速适应和高精度跟踪，但严格的符号图维护带来了显著的效率代价，需要在透明度和计算效率之间进行权衡。

Abstract: We present a white-box adaptive NMPC architecture that resolves vehicular plasticity (adaptation to varying operating regimes without retraining) by arbitrating among frozen, regime-specific neural specialists using a Modular Sovereignty paradigm. The ensemble dynamics are maintained as a fully traversable symbolic graph in CasADi, enabling maximal runtime auditability. Synchronous simulation validates rapid adaptation (~7.3 ms) and near-ideal tracking fidelity under compound regime shifts (friction, mass, drag) where non-adaptive baselines fail. Empirical benchmarking quantifies the transparency cost: symbolic graph maintenance increases solver latency by 72-102X versus compiled parametric physics models, establishing the efficiency price of strict white-box implementation.

</details>


### [465] [Predicting Mortgage Default with Machine Learning: AutoML, Class Imbalance, and Leakage Control](https://arxiv.org/abs/2602.00120)
*Xianghong Hu,Tianning Xu,Ying Chen,Shuai Wang*

Main category: cs.LG

TL;DR: 该论文研究抵押贷款违约预测，重点解决实际数据中的标签模糊、类别不平衡和信息泄漏问题，通过泄漏感知特征选择、严格时间分割和受控下采样等方法，发现AutoML方法表现最佳。


<details>
  <summary>Details</summary>
Motivation: 抵押贷款违约预测是金融风险管理的核心任务，但实际数据中存在三个主要问题：违约标签模糊、严重的类别不平衡、以及时间结构和事后变量引起的信息泄漏，这些问题会损害评估有效性和部署可靠性。

Method: 采用泄漏感知特征选择、严格的时间分割（限制贷款发放和报告期间）、对多数类进行受控下采样，并比较多种机器学习方法，包括AutoML（AutoGluon）。

Result: 在不同正负样本比例下，模型性能保持稳定，AutoGluon在所有评估模型中取得了最强的AUROC（曲线下面积）表现。

Conclusion: 通过适当的泄漏控制和类别不平衡处理方法，机器学习模型可以在抵押贷款违约预测中取得可靠性能，AutoML方法在该任务中表现突出。该研究的扩展教学版本将作为书籍章节发表。

Abstract: Mortgage default prediction is a core task in financial risk management, and machine learning models are increasingly used to estimate default probabilities and provide interpretable signals for downstream decisions. In real-world mortgage datasets, however, three factors frequently undermine evaluation validity and deployment reliability: ambiguity in default labeling, severe class imbalance, and information leakage arising from temporal structure and post-event variables. We compare multiple machine learning approaches for mortgage default prediction using a real-world loan-level dataset, with emphasis on leakage control and imbalance handling. We employ leakage-aware feature selection, a strict temporal split that constrains both origination and reporting periods, and controlled downsampling of the majority class. Across multiple positive-to-negative ratios, performance remains stable, and an AutoML approach (AutoGluon) achieves the strongest AUROC among the models evaluated. An extended and pedagogical version of this work will appear as a book chapter.

</details>


### [466] [AdaptNC: Adaptive Nonconformity Scores for Uncertainty-Aware Autonomous Systems in Dynamic Environments](https://arxiv.org/abs/2602.01629)
*Renukanandan Tumu,Aditya Singh,Rahul Mangharam*

Main category: cs.LG

TL;DR: AdaptNC：联合在线适应非共形分数参数和共形阈值，解决分布偏移下共形预测的保守性问题


<details>
  <summary>Details</summary>
Motivation: 现实机器人应用中存在分布偏移，违反标准共形预测的交换性假设。现有在线CP方法仅自适应调整阈值，但使用固定的非共形分数函数，导致预测区域过于保守且体积效率低下。

Method: 提出AdaptNC框架，联合在线适应非共形分数参数和共形阈值。采用自适应重加权方案优化分数函数，并引入回放缓冲机制缓解分数转换期间的覆盖不稳定性。

Result: 在多智能体策略变化、环境变化和传感器退化等机器人基准测试中，AdaptNC在保持目标覆盖水平的同时，显著减少了预测区域体积，优于仅调整阈值的基线方法。

Conclusion: AdaptNC通过联合适应分数函数和阈值，有效解决了分布偏移下共形预测的保守性问题，为自主系统在非约束环境中的安全部署提供了更高效的预测区域。

Abstract: Rigorous uncertainty quantification is essential for the safe deployment of autonomous systems in unconstrained environments. Conformal Prediction (CP) provides a distribution-free framework for this task, yet its standard formulations rely on exchangeability assumptions that are violated by the distribution shifts inherent in real-world robotics. Existing online CP methods maintain target coverage by adaptively scaling the conformal threshold, but typically employ a static nonconformity score function. We show that this fixed geometry leads to highly conservative, volume-inefficient prediction regions when environments undergo structural shifts. To address this, we propose \textbf{AdaptNC}, a framework for the joint online adaptation of both the nonconformity score parameters and the conformal threshold. AdaptNC leverages an adaptive reweighting scheme to optimize score functions, and introduces a replay buffer mechanism to mitigate the coverage instability that occurs during score transitions. We evaluate AdaptNC on diverse robotic benchmarks involving multi-agent policy changes, environmental changes and sensor degradation. Our results demonstrate that AdaptNC significantly reduces prediction region volume compared to state-of-the-art threshold-only baselines while maintaining target coverage levels.

</details>


### [467] [MiniTensor: A Lightweight, High-Performance Tensor Operations Library](https://arxiv.org/abs/2602.00125)
*Soumyadip Sarkar*

Main category: cs.LG

TL;DR: MiniTensor是一个开源张量运算库，专注于简洁性、正确性和性能，提供类似PyTorch的Python API，但使用Rust引擎执行性能关键代码，安装包仅几MB，比主流框架小几个数量级。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习框架如PyTorch和TensorFlow通常体积庞大，安装包达到数百MB甚至GB级别。MiniTensor旨在提供一个极简但功能完整的张量运算库，专注于CPU上的研究和开发，大幅减小安装体积。

Method: 采用混合架构设计：提供类似PyTorch的Python API，但性能关键代码在Rust引擎中执行。核心功能包括n维稠密张量、广播、归约、矩阵乘法、反向模式自动微分、紧凑的神经网络层和标准优化器。通过PyO3实现Python集成，采用高效内存管理和动态计算图。

Result: MiniTensor安装包仅几MB，比PyTorch和TensorFlow小几个数量级。保留了CPU上研究和开发所需的核心功能，包括张量运算、自动微分、神经网络层和优化器。

Conclusion: MiniTensor成功实现了极简设计目标，在保持功能完整性的同时大幅减小了安装体积，为CPU上的研究和开发提供了一个轻量级但功能完备的替代方案。

Abstract: We present MiniTensor, an open source tensor operations library that focuses on minimalism, correctness, and performance. MiniTensor exposes a familiar PyTorch-like Python API while it executes performance critical code in a Rust engine. The core supports dense $n$ dimensional tensors, broadcasting, reductions, matrix multiplication, reverse mode automatic differentiation, a compact set of neural network layers, and standard optimizers. In this paper, we describe the design of MiniTensor's architecture, including its efficient memory management, dynamic computation graph for gradients, and integration with Python via PyO3. We also compare the install footprint with PyTorch and TensorFlow to demonstrate that MiniTensor achieves a package size of only a few megabytes, several orders of magnitude smaller than mainstream frameworks, while preserving the essentials needed for research and development on CPUs. The repository can be found at https://github.com/neuralsorcerer/minitensor

</details>


### [468] [DCoPilot: Generative AI-Empowered Policy Adaptation for Dynamic Data Center Operations](https://arxiv.org/abs/2602.02137)
*Minghao Li,Ruihang Wang,Rui Tan,Yonggang Wen*

Main category: cs.LG

TL;DR: DCoPilot是一个混合生成框架，结合LLM和超网络，为动态数据中心生成控制策略，实现零样本适应和近零约束违反。


<details>
  <summary>Details</summary>
Motivation: 现代数据中心运行在高功率密度和快速变化的工作负载下，需要分钟级适应。传统手动设计的DRL代理无法跟上频繁的动态变化和SLA变更，导致策略滞后和潜在服务中断。

Method: DCoPilot结合两种生成范式：LLM进行结构化奖励形式的符号生成，超网络进行策略权重的参数生成。包含三个阶段：仿真扩展（压力测试奖励候选）、元策略蒸馏（训练超网络输出条件策略权重）、在线适应（零样本策略生成）。

Result: 在五个控制任务家族中，DCoPilot实现近零约束违反，在所有基线方法中表现最佳。消融研究验证了基于LLM的统一奖励生成对超网络稳定收敛的有效性。

Conclusion: DCoPilot通过结合LLM和超网络，解决了动态数据中心控制策略生成的滞后问题，实现了对规格变化的及时适应和高效控制。

Abstract: Modern data centers (DCs) hosting artificial intelligence (AI)-dedicated devices operate at high power densities with rapidly varying workloads, making minute-level adaptation essential for safe and energy-efficient operation. However, manually designing piecewise deep reinforcement learning (DRL) agents cannot keep pace with frequent dynamics shifts and service-level agreement (SLA) changes of an evolving DC. This specification-to-policy lag causes a lack of timely, effective control policies, which may lead to service outages. To bridge the gap, we present DCoPilot, a hybrid framework for generative control policies in dynamic DC operation. DCoPilot synergizes two distinct generative paradigms, i.e., a large language model (LLM) that performs symbolic generation of structured reward forms, and a hypernetwork that conducts parametric generation of policy weights. DCoPilot operates through three coordinated phases: (i) simulation scale-up, which stress-tests reward candidates across diverse simulation-ready (SimReady) scenes; (ii) meta policy distillation, where a hypernetwork is trained to output policy weights conditioned on SLA and scene embeddings; and (iii) online adaptation, enabling zero-shot policy generation in response to updated specifications. Evaluated across five control task families spanning diverse DC components, DCoPilot achieves near-zero constraint violations and outperforms all baselines across specification variations. Ablation studies validate the effectiveness of LLM-based unified reward generation in enabling stable hypernetwork convergence.

</details>


### [469] [ALIGN: Aligned Delegation with Performance Guarantees for Multi-Agent LLM Reasoning](https://arxiv.org/abs/2602.00127)
*Tong Zhu,Baiting Chen,Jin Zhou,Hua Zhou,Sriram Sankararaman,Xiaowu Dai*

Main category: cs.LG

TL;DR: ALIGN提出了一种新的多智能体LLM推理方法，将推理建模为对齐委托游戏，通过设计激励机制让多个智能体生成候选方案，然后选择最佳答案，理论上保证比单智能体方法有更好性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在复杂推理任务上表现不佳，传统的推理方法通常采用单一生成-选择流程，而推理时集成方法虽然通过采样多样化推理路径或聚合多个候选答案来改进性能，但通常将候选答案视为独立处理，且无法保证集成能真正提高推理质量。

Method: ALIGN将LLM推理建模为对齐委托游戏：一个委托人将任务委托给多个智能体，这些智能体在设计的激励机制下生成候选解决方案，然后委托人从它们的输出中选择最终答案。这种方法在保持智能体与委托人目标对齐的同时，诱导智能体之间进行结构化交互。

Result: 理论分析表明，在公平比较且同等访问候选解决方案的条件下，ALIGN能够证明比单智能体生成方法有更好的期望性能。该方法适应了候选答案之间的相关性，放宽了先前工作中常用的独立性假设。在广泛的LLM推理基准测试中，ALIGN始终优于强大的单智能体和集成基线方法。

Conclusion: ALIGN通过将LLM推理形式化为对齐委托游戏，提供了一种理论上可保证性能改进的多智能体推理方法，在保持目标对齐的同时实现了智能体间的结构化交互，在多个基准测试中表现出优越性能。

Abstract: LLMs often underperform on complex reasoning tasks when relying on a single generation-and-selection pipeline. Inference-time ensemble methods can improve performance by sampling diverse reasoning paths or aggregating multiple candidate answers, but they typically treat candidates independently and provide no formal guarantees that ensembling improves reasoning quality. We propose a novel method, Aligned Delegation for Multi-Agent LLM Reasoning (ALIGN), which formulates LLM reasoning as an aligned delegation game. In ALIGN, a principal delegates a task to multiple agents that generate candidate solutions under designed incentives, and then selects among their outputs to produce a final answer. This formulation induces structured interaction among agents while preserving alignment between agent and principal objectives. We establish theoretical guarantees showing that, under a fair comparison with equal access to candidate solutions, ALIGN provably improves expected performance over single-agent generation. Our analysis accommodates correlated candidate answers and relaxes independence assumptions that are commonly used in prior work. Empirical results across a broad range of LLM reasoning benchmarks consistently demonstrate that ALIGN outperforms strong single-agent and ensemble baselines.

</details>


### [470] [Generating Causal Temporal Interaction Graphs for Counterfactual Validation of Temporal Link Prediction](https://arxiv.org/abs/2602.02161)
*Aniq Ur Rahman,Justin P. Coon*

Main category: cs.LG

TL;DR: 提出一个用于时序链接预测模型的反事实验证框架，通过生成具有已知因果结构的因果时序交互图来评估模型是否捕捉到因果机制。


<details>
  <summary>Details</summary>
Motivation: 当前时序链接预测模型主要基于预测准确性评估，但这种方法无法评估模型是否真正捕捉到支配时序交互的因果机制。需要一种能够验证模型因果理解能力的评估框架。

Method: 1) 提出支持兴奋和抑制效应的连续时间事件序列结构方程模型；2) 将该机制扩展到时序交互图；3) 提出基于跨模型预测误差的距离度量；4) 在两种场景下实例化反事实评估：生成模型间的受控因果偏移和时间戳洗牌作为可测量因果距离的随机失真。

Result: 经验验证了假设：在一个因果模型上训练的预测器在评估足够远的模型时性能会下降。该框架为因果感知的基准测试奠定了基础。

Conclusion: 提出的反事实验证框架能够评估时序链接预测模型是否捕捉到因果机制，为更全面的模型评估提供了新方法，支持因果感知的基准测试。

Abstract: Temporal link prediction (TLP) models are commonly evaluated based on predictive accuracy, yet such evaluations do not assess whether these models capture the causal mechanisms that govern temporal interactions. In this work, we propose a framework for counterfactual validation of TLP models by generating causal temporal interaction graphs (CTIGs) with known ground-truth causal structure. We first introduce a structural equation model for continuous-time event sequences that supports both excitatory and inhibitory effects, and then extend this mechanism to temporal interaction graphs. To compare causal models, we propose a distance metric based on cross-model predictive error, and empirically validate the hypothesis that predictors trained on one causal model degrade when evaluated on sufficiently distant models. Finally, we instantiate counterfactual evaluation under (i) controlled causal shifts between generating models and (ii) timestamp shuffling as a stochastic distortion with measurable causal distance. Our framework provides a foundation for causality-aware benchmarking.

</details>


### [471] [Quantum Model Parallelism for MRI-Based Classification of Alzheimer's Disease Stages](https://arxiv.org/abs/2602.00128)
*Emine Akpinar,Murat Oduncuoglu*

Main category: cs.LG

TL;DR: 提出量子并行模型(QBPM)用于阿尔茨海默病分期分类，利用量子叠加和纠缠原理，在MRI数据集上实现高效分类，相比经典方法获得更高准确率且参数更少。


<details>
  <summary>Details</summary>
Motivation: 随着寿命延长，阿尔茨海默病成为全球重大健康问题。传统AI方法在早期诊断和分期分类方面存在计算资源限制，而量子AI方法能利用叠加、纠缠和高维希尔伯特空间优势，处理高维、异构和噪声数据。

Method: 提出量子并行模型(QBPM)架构，受经典模型并行启发，使用两个不同的量子电路（包含旋转和纠缠模块），在相同量子模拟器上并行运行，用于MRI数据集的AD分期分类。

Result: 在两个不同数据集上均表现出高分类准确率，显示良好的鲁棒性和泛化能力。在高斯噪声条件下仍保持性能，证明实际应用潜力。相比五种经典迁移学习方法，获得更高准确率、相当执行时间且使用更少电路参数。

Conclusion: QBPM架构代表了复杂疾病（如阿尔茨海默病）分期分类的创新且强大的方法，量子优势使其成为经典方法的有效替代方案。

Abstract: With increasing life expectancy, AD has become a major global health concern. While classical AI-based methods have been developed for early diagnosis and stage classification of AD, growing data volumes and limited computational resources necessitate faster, more efficient approaches. Quantum-based AI methods, which leverage superposition and entanglement principles along with high-dimensional Hilbert space, can surpass classical approaches' limitations and offer higher accuracy for high-dimensional, heterogeneous, and noisy data. In this study, a Quantum-Based Parallel Model (QBPM) architecture is proposed for the efficient classification of AD stages using MRI datasets, inspired by the principles of classical model parallelism. The proposed model leverages quantum advantages by employing two distinct quantum circuits, each incorporating rotational and entanglement blocks, running in parallel on the same quantum simulator. The classification performance of the model was evaluated on two different datasets to assess its overall robustness and generalization capability. The proposed model demonstrated high classification accuracy across both datasets, highlighting its overall robustness and generalization capability. Results obtained under high-level Gaussian noise, simulating real-world conditions, further provided experimental evidence for the model's applicability not only in theoretical but also in practical scenarios. Moreover, compared with five different classical transfer learning methods, the proposed model demonstrated its efficiency as an alternative to classical approaches by achieving higher classification accuracy and comparable execution time while utilizing fewer circuit parameters. The results indicate that the proposed QBPM architecture represents an innovative and powerful approach for the classification of stages in complex diseases such as Alzheimer's.

</details>


### [472] [Monte Carlo Tree Search for Execution-Guided Program Repair with Large Language Models](https://arxiv.org/abs/2602.00129)
*Yixuan Liang*

Main category: cs.LG

TL;DR: CodePilot：结合蒙特卡洛树搜索与大型语言模型的混合框架，用于执行引导的程序修复，在真实GitHub问题上表现优于基线方法


<details>
  <summary>Details</summary>
Motivation: 现有基于大型语言模型的自动程序修复在仓库级别面临挑战，包括长时程推理需求限制和自回归解码的局限性，需要更有效的执行感知修复方法

Method: 集成蒙特卡洛树搜索与大型语言模型的混合框架，进行从仓库到文件再到函数的分层故障定位，利用MCTS探索多样化补丁轨迹，将执行反馈作为奖励信号指导搜索和优化，并采用置信度校准生成选择性优化低置信度输出

Result: 在SWE-bench Lite基准测试中，CodePilot使用开源权重模型实现了24.67%的问题解决率，优于可比基线方法

Conclusion: 将符号搜索与神经语言模型相结合是构建可扩展、执行感知的软件工程自动化的有效策略

Abstract: Automated program repair with large language models remains challenging at the repository level due to long-horizon reasoning requirements and the limitations of autoregressive decoding. We present CodePilot, a hybrid framework that integrates Monte Carlo Tree Search (MCTS) with large language models to enable execution-guided program repair for real-world GitHub issues. CodePilot performs hierarchical fault localization from repository to file and function level, explores diverse patch trajectories using MCTS, and leverages execution feedback as a reward signal to guide search and refinement. The framework further incorporates confidence-calibrated generation to selectively refine low-confidence outputs. Experiments on SWE-bench Lite demonstrate that CodePilot achieves a 24.67% issue resolution rate using open-weight models, outperforming comparable baselines. These results suggest that combining symbolic search with neural language models is an effective strategy for scalable, execution-aware software engineering automation.

</details>


### [473] [Adaptive Momentum and Nonlinear Damping for Neural Network Training](https://arxiv.org/abs/2602.00334)
*Aikaterini Karoni,Rajit Rajpal,Benedict Leimkuhler,Gabriel Stoltz*

Main category: cs.LG

TL;DR: 提出一种连续时间大规模优化方案，通过每个模型参数的动能调节个体自适应动量系数，自动适应局部曲率保持稳定性而不牺牲收敛速度。


<details>
  <summary>Details</summary>
Motivation: 传统优化方法如mSGD在训练大型模型（如ViT、BERT、GPT2）时存在稳定性问题，需要一种能自动适应局部曲率、保持稳定性的自适应动量调节机制。

Method: 1. 引入连续时间优化方案，通过每个模型参数的动能调节个体自适应动量系数；2. 将自适应摩擦机制与结构动力学中的立方阻尼联系起来；3. 在mSGD和Adam的连续动力学基础上增加立方阻尼项，形成两种具体优化方案。

Result: 实验表明，该方法在训练ViT、BERT和GPT2任务中表现出鲁棒性，匹配或优于Adam性能，而mSGD在这些任务上通常表现不佳。理论分析证明了所提方案的指数收敛性。

Conclusion: 通过动能调节的自适应动量系数和立方阻尼机制，能够有效提升大规模优化问题的稳定性和收敛性能，为深度学习优化提供了新的理论框架和实践方案。

Abstract: We propose a continuous-time scheme for large-scale optimization that introduces individual, adaptive momentum coefficients regulated by the kinetic energy of each model parameter. This approach automatically adjusts to local landscape curvature to maintain stability without sacrificing convergence speed. We demonstrate that our adaptive friction can be related to cubic damping, a suppression mechanism from structural dynamics. Furthermore, we introduce two specific optimization schemes by augmenting the continuous dynamics of mSGD and Adam with a cubic damping term. Empirically, our methods demonstrate robustness and match or outperform Adam on training ViT, BERT, and GPT2 tasks where mSGD typically struggles. We further provide theoretical results establishing the exponential convergence of the proposed schemes.

</details>


### [474] [On the Relationship Between Representation Geometry and Generalization in Deep Neural Networks](https://arxiv.org/abs/2602.00130)
*Sumit Yadav*

Main category: cs.LG

TL;DR: 研究发现神经网络表示几何中的有效维度能强烈预测模型性能，这种关系在视觉和NLP任务中普遍存在，且具有因果性。


<details>
  <summary>Details</summary>
Motivation: 探索神经网络表示几何与性能之间的关系，寻找无需标签就能预测模型性能的无监督指标。

Method: 分析52个预训练ImageNet模型（13种架构），使用有效维度作为几何度量；在视觉任务（ImageNet、CIFAR-10）和NLP任务（SST-2/MNLI、AG News）上验证；通过添加噪声和PCA进行因果实验。

Result: 有效维度与准确率强相关（部分r=0.75，p<10^(-10)），总压缩与准确率负相关（r=-0.72）；这种关系在视觉和NLP任务中普遍存在；几何退化导致性能下降（r=-0.94），几何改善能保持性能（仅下降0.03pp）。

Conclusion: 有效维度提供了领域无关的神经网络性能预测和因果信息，完全无需标签，为模型评估和理解提供了新视角。

Abstract: We investigate the relationship between representation geometry and neural network performance. Analyzing 52 pretrained ImageNet models across 13 architecture families, we show that effective dimension -- an unsupervised geometric metric -- strongly predicts accuracy. Output effective dimension achieves partial r=0.75 ($p < 10^(-10)$) after controlling for model capacity, while total compression achieves partial r=-0.72. These findings replicate across ImageNet and CIFAR-10, and generalize to NLP: effective dimension predicts performance for 8 encoder models on SST-2/MNLI and 15 decoder-only LLMs on AG News (r=0.69, p=0.004), while model size does not (r=0.07). We establish bidirectional causality: degrading geometry via noise causes accuracy loss (r=-0.94, $p < 10^(-9)$), while improving geometry via PCA maintains accuracy across architectures (-0.03pp at 95% variance). This relationship is noise-type agnostic -- Gaussian, Uniform, Dropout, and Salt-and-pepper noise all show $|r| > 0.90$. These results establish that effective dimension provides domain-agnostic predictive and causal information about neural network performance, computed entirely without labels.

</details>


### [475] [Quality-Diversity Optimization as Multi-Objective Optimization](https://arxiv.org/abs/2602.00478)
*Xi Lin,Ping Guo,Yilu Liu,Qingfu Zhang,Jianyong Sun*

Main category: cs.LG

TL;DR: 该论文将质量多样性优化重新表述为具有大量目标的多目标优化问题，使现有MOO方法可直接应用于QD问题


<details>
  <summary>Details</summary>
Motivation: 质量多样性优化在机器人控制、创意设计等领域有重要应用，但现有QD算法各有不同的设计原则。本文旨在建立QD与MOO之间的联系，使成熟的MOO方法能够直接应用于QD问题

Method: 将QD优化重新表述为具有大量优化目标的多目标优化问题，特别采用基于集合的标量化技术，通过协作搜索过程解决QD问题

Result: 理论分析表明该方法继承了MOO的理论保证，同时为QD优化提供了理想特性。多个QD应用的实验研究证实该方法与最先进的QD算法性能相当

Conclusion: 通过将QD重新表述为MOO问题，成功建立了两个领域之间的联系，使成熟的MOO方法能够直接应用于QD优化，为QD研究提供了新的视角和工具

Abstract: The Quality-Diversity (QD) optimization aims to discover a collection of high-performing solutions that simultaneously exhibit diverse behaviors within a user-defined behavior space. This paradigm has stimulated significant research interest and demonstrated practical utility in domains including robot control, creative design, and adversarial sample generation. A variety of QD algorithms with distinct design principles have been proposed in recent years. Instead of proposing a new QD algorithm, this work introduces a novel reformulation by casting the QD optimization as a multi-objective optimization (MOO) problem with a huge number of optimization objectives. By establishing this connection, we enable the direct adoption of well-established MOO methods, particularly set-based scalarization techniques, to solve QD problems through a collaborative search process. We further provide a theoretical analysis demonstrating that our approach inherits theoretical guarantees from MOO while providing desirable properties for the QD optimization. Experimental studies across several QD applications confirm that our method achieves performance competitive with state-of-the-art QD algorithms.

</details>


### [476] [RAPTOR: Ridge-Adaptive Logistic Probes](https://arxiv.org/abs/2602.00158)
*Ziqi Gao,Yaotian Zhu,Qingcheng Zeng,Xu Zhao,Ziqing Wang,Feng Ruan,Kaize Ding*

Main category: cs.LG

TL;DR: RAPTOR是一种基于L2正则化逻辑回归的轻量级探针方法，用于从冻结LLM的层表示中提取概念向量，在准确性、方向稳定性和训练成本方面优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 探针技术用于分析冻结LLM中编码的信息，并在探针-引导管道中操作使用。现有方法需要估计准确、方向稳定且获取成本低的概念向量，但难以同时满足这些需求。

Method: 提出RAPTOR（Ridge-Adaptive Logistic Probe），一种简单的L2正则化逻辑探针，通过验证调整的岭强度从归一化权重中提取概念向量。

Result: 在指令调优LLM和人工编写概念数据集上的广泛实验中，RAPTOR在准确性方面匹配或超过强基线，同时实现竞争性的方向稳定性和显著更低的训练成本。定性下游引导演示支持这些定量结果。

Conclusion: RAPTOR提供了一种高效准确的概念向量提取方法。通过凸高斯最小最大定理的理论分析解释了岭惩罚强度如何调节探针准确性和概念向量稳定性，其结构预测与真实LLM嵌入观察到的趋势定性一致。

Abstract: Probing studies what information is encoded in a frozen LLM's layer representations by training a lightweight predictor on top of them. Beyond analysis, probes are often used operationally in probe-then-steer pipelines: a learned concept vector is extracted from a probe and injected via additive activation steering by adding it to a layer representation during the forward pass. The effectiveness of this pipeline hinges on estimating concept vectors that are accurate, directionally stable under ablation, and inexpensive to obtain. Motivated by these desiderata, we propose RAPTOR (Ridge-Adaptive Logistic Probe), a simple L2-regularized logistic probe whose validation-tuned ridge strength yields concept vectors from normalized weights. Across extensive experiments on instruction-tuned LLMs and human-written concept datasets, RAPTOR matches or exceeds strong baselines in accuracy while achieving competitive directional stability and substantially lower training cost; these quantitative results are supported by qualitative downstream steering demonstrations. Finally, using the Convex Gaussian Min-max Theorem (CGMT), we provide a mechanistic characterization of ridge logistic regression in an idealized Gaussian teacher-student model in the high-dimensional few-shot regime, explaining how penalty strength mediates probe accuracy and concept-vector stability and yielding structural predictions that qualitatively align with trends observed on real LLM embeddings.

</details>


### [477] [Partition of Unity Neural Networks for Interpretable Classification with Explicit Class Regions](https://arxiv.org/abs/2602.00511)
*Akram Aldroubi*

Main category: cs.LG

TL;DR: 提出PUNN架构，通过学习的单位分解直接生成类别概率，无需softmax层，实现可解释的分类器设计


<details>
  <summary>Details</summary>
Motivation: 神经网络分类器难以解释，softmax模型中的类别区域通过logits的不等式系统隐式定义，难以提取和可视化

Method: 引入PUNN架构，学习k个非负函数h₁,...,hₖ满足∑hᵢ(x)=1，每个hᵢ(x)直接表示P(class i|x)。门函数gᵢ可使用多种激活函数和参数化设计

Result: PUNN在合成数据、UCI基准和MNIST上达到与标准多层感知机相近的准确率（相差0.3-0.6%）。当几何先验匹配数据结构时，形状感知门函数能以300倍少的参数实现相当准确率

Conclusion: PUNN证明可解释设计架构可以与黑盒模型竞争，同时提供透明的类别概率分配，为可解释神经网络提供了新方向

Abstract: Despite their empirical success, neural network classifiers remain difficult to interpret. In softmax-based models, class regions are defined implicitly as solutions to systems of inequalities among logits, making them difficult to extract and visualize. We introduce Partition of Unity Neural Networks (PUNN), an architecture in which class probabilities arise directly from a learned partition of unity, without requiring a softmax layer.
  PUNN constructs $k$ nonnegative functions $h_1, \ldots, h_k$ satisfying $\sum_i h_i(x) = 1$, where each $h_i(x)$ directly represents $P(\text{class } i \mid x)$. Unlike softmax, where class regions are defined implicitly through coupled inequalities among logits, each PUNN partition function $h_i$ directly defines the probability of class $i$ as a standalone function of $x$.
  We prove that PUNN is dense in the space of continuous probability maps on compact domains. The gate functions $g_i$ that define the partition can use various activation functions (sigmoid, Gaussian, bump) and parameterizations ranging from flexible MLPs to parameter-efficient shape-informed designs (spherical shells, ellipsoids, spherical harmonics).
  Experiments on synthetic data, UCI benchmarks, and MNIST show that PUNN with MLP-based gates achieves accuracy within 0.3--0.6\% of standard multilayer perceptrons. When geometric priors match the data structure, shape-informed gates achieve comparable accuracy with up to 300$\times$ fewer parameters. These results demonstrate that interpretable-by-design architectures can be competitive with black-box models while providing transparent class probability assignments.

</details>


### [478] [Sheaf Neural Networks and biomedical applications](https://arxiv.org/abs/2602.00159)
*Aneeqa Mehrab,Jan Willem Van Looy,Pietro Demurtas,Stefano Iotti,Emil Malucelli,Francesca Rossi,Ferdinando Zanchetta,Rita Fioresi*

Main category: cs.LG

TL;DR: 本文介绍了一种基于层神经网络的算法，通过数学建模和理论分析，在生物医学案例研究中证明其优于主流图神经网络方法


<details>
  <summary>Details</summary>
Motivation: 当前流行的图神经网络（如GCN、GAT、GraphSage）在生物医学问题中存在局限性，需要更有效的算法来解决复杂的生物医学图数据问题

Method: 提出了层神经网络（SNN）算法，建立了相应的理论和数学模型，并在具体生物医学案例研究中验证其有效性

Result: SNN在生物医学案例研究中表现优异，超越了最流行的图神经网络方法（GCN、GAT、GraphSage）

Conclusion: 层神经网络（SNN）是一种有效的生物医学图数据分析方法，具有超越现有主流图神经网络的性能优势

Abstract: The purpose of this paper is to elucidate the theory and mathematical modelling behind the sheaf neural network (SNN) algorithm and then show how SNN can effectively answer to biomedical questions in a concrete case study and outperform the most popular graph neural networks (GNNs) as graph convolutional networks (GCNs), graph attention networks (GAT) and GraphSage.

</details>


### [479] [Sporadic Gradient Tracking over Directed Graphs: A Theoretical Perspective on Decentralized Federated Learning](https://arxiv.org/abs/2602.00791)
*Shahryar Zehtabi,Dong-Jun Han,Seyyedali Hosseinalipour,Christopher Brinton*

Main category: cs.LG

TL;DR: 提出了Spod-GT算法，这是第一个在一般有向图上结合梯度追踪和客户端资源异构性的去中心化联邦学习算法，支持客户端特定的梯度计算频率和异构非对称通信频率。


<details>
  <summary>Details</summary>
Motivation: 现有的去中心化联邦学习研究分别解决了数据异构性和客户端资源多样性问题，但缺乏统一解决方案。需要一种能同时处理数据异构性和客户端资源异构性的算法，特别是在一般有向图拓扑上。

Method: 提出Sporadic Gradient Tracking (Spod-GT)算法，结合了梯度追踪技术来缓解数据异构性，并考虑了客户端资源多样性。算法允许：1) 客户端特定的梯度计算频率；2) 异构且非对称的通信频率。在一般有向图上实现，放宽了对梯度估计方差和客户端梯度多样性的假设。

Result: 通过严格的收敛分析，证明了算法在有向图上即使客户端间歇参与也能达成共识和最优性保证。在图像分类数据集上的数值实验表明，Spod-GT相比已知的梯度追踪基线方法表现更优。

Conclusion: Spod-GT是第一个在一般有向图上统一解决数据异构性和客户端资源异构性的去中心化联邦学习算法，为实际部署中客户端资源多样性问题提供了有效解决方案。

Abstract: Decentralized Federated Learning (DFL) enables clients with local data to collaborate in a peer-to-peer manner to train a generalized model. In this paper, we unify two branches of work that have separately solved important challenges in DFL: (i) gradient tracking techniques for mitigating data heterogeneity and (ii) accounting for diverse availability of resources across clients. We propose $\textit{Sporadic Gradient Tracking}$ ($\texttt{Spod-GT}$), the first DFL algorithm that incorporates these factors over general directed graphs by allowing (i) client-specific gradient computation frequencies and (ii) heterogeneous and asymmetric communication frequencies. We conduct a rigorous convergence analysis of our methodology with relaxed assumptions on gradient estimation variance and gradient diversity of clients, providing consensus and optimality guarantees for GT over directed graphs despite intermittent client participation. Through numerical experiments on image classification datasets, we demonstrate the efficacy of $\texttt{Spod-GT}$ compared to well-known GT baselines.

</details>


### [480] [Block removal for large language models through constrained binary optimization](https://arxiv.org/abs/2602.00161)
*David Jansen,Roman Rausch,David Montero,Roman Orus*

Main category: cs.LG

TL;DR: 提出一种将Transformer块移除问题转化为约束二元优化问题的方法，通过映射到伊辛模型来高效评估候选配置，在多个基准测试中优于现有方法


<details>
  <summary>Details</summary>
Motivation: 压缩大型语言模型时移除整个Transformer块看似简单，但识别哪些块可以移除是一个指数级困难的组合优化问题，需要更高效的解决方案

Method: 将块移除问题公式化为约束二元优化问题，映射到物理系统的伊辛模型，利用能量函数作为下游性能的强代理指标，通过伊辛求解器高效评估大量候选配置

Result: 在多个基准测试中优于最先进的块移除方法，性能提升在短时间重训练后仍能保持，在MMLU基准上达到高达6个百分点的改进，并能处理具有挑战性的非均匀块结构

Conclusion: 该方法提供了一种高效、通用的Transformer块移除解决方案，仅需少量前向和后向传播计算，结合伊辛求解器即可应用于任何架构，为模型压缩提供了新思路

Abstract: Compressing resource-intensive large language models by removing whole transformer blocks is a seemingly simple idea, but identifying which blocks to remove constitutes an exponentially difficult combinatorial problem. In this paper, we formulate block removal as a constrained binary optimization problem that can be mapped to a physical system (Ising model), whose energies are a strong proxy for downstream model performance. This formulation enables an efficient ranking of a large number of candidate block-removal configurations and yields many high-quality, non-trivial solutions beyond consecutive regions. We demonstrate that our approach outperforms state-of-the-art block-removal methods across several benchmarks, with performance gains persisting after short retraining, and reaching improvements of up to 6 points on the MMLU benchmark. Our method requires only forward and backward passes for a few active parameters, together with an (at least approximate) Ising solver, and can be readily applied to any architecture. We illustrate this generality on the recent NVIDIA-Nemotron-3-Nano-30B-A3B-FP8 model, which exhibits a highly inhomogeneous and challenging block structure.

</details>


### [481] [A Unified Matrix-Spectral Framework for Stability and Interpretability in Deep Learning](https://arxiv.org/abs/2602.01136)
*Ronald Katende*

Main category: cs.LG

TL;DR: 提出统一的矩阵谱框架分析深度神经网络稳定性和可解释性，引入全局矩阵稳定性指数聚合多种谱信息，通过谱熵改进经典算子范数界，实验证明谱正则化能显著提升归因稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络分析缺乏统一的稳定性评估框架，难以同时处理输入扰动、标签噪声和训练动态的敏感性。需要建立连接谱特性和分析稳定性的精确理论，为鲁棒性模型设计提供指导。

Method: 将网络表示为数据依赖的线性算子乘积，分析雅可比矩阵、参数梯度、神经正切核算子和损失海森矩阵的谱特性。引入全局矩阵稳定性指数聚合谱信息，使用谱熵捕捉典型敏感性而非最坏情况。

Result: 在MNIST、CIFAR-10和CIFAR-100上的实验表明，适度的谱正则化能显著改善归因稳定性，即使全局谱摘要变化很小。建立了谱集中度与分析稳定性之间的精确联系。

Conclusion: 该框架为神经网络稳定性和可解释性分析提供了统一的谱视角，提出的可计算诊断和稳定性导向正则化原则为鲁棒性模型设计和训练提供了实用指导。

Abstract: We develop a unified matrix-spectral framework for analyzing stability and interpretability in deep neural networks. Representing networks as data-dependent products of linear operators reveals spectral quantities governing sensitivity to input perturbations, label noise, and training dynamics.
  We introduce a Global Matrix Stability Index that aggregates spectral information from Jacobians, parameter gradients, Neural Tangent Kernel operators, and loss Hessians into a single stability scale controlling forward sensitivity, attribution robustness, and optimization conditioning. We further show that spectral entropy refines classical operator-norm bounds by capturing typical, rather than purely worst-case, sensitivity.
  These quantities yield computable diagnostics and stability-oriented regularization principles. Synthetic experiments and controlled studies on MNIST, CIFAR-10, and CIFAR-100 confirm that modest spectral regularization substantially improves attribution stability even when global spectral summaries change little.
  The results establish a precise connection between spectral concentration and analytic stability, providing practical guidance for robustness-aware model design and training.

</details>


### [482] [The Illusion of Forgetting: Attack Unlearned Diffusion via Initial Latent Variable Optimization](https://arxiv.org/abs/2602.00175)
*Manyi Li,Yufan Liu,Lai Jiang,Bing Li,Yuming Li,Weiming Hu*

Main category: cs.LG

TL;DR: 论文揭示基于遗忘的防御方法对NSFW概念的"遗忘"是假象，知识仍以休眠记忆形式存在，并提出IVO攻击框架重新激活这些记忆。


<details>
  <summary>Details</summary>
Motivation: 现有基于遗忘的防御方法声称能从扩散模型中清除NSFW概念，但作者发现这种"遗忘"很大程度上是假象。遗忘只是部分破坏了语言符号与底层知识之间的映射关系，而知识本身仍以休眠记忆形式完整保留。

Method: 提出IVO（初始潜在变量优化）攻击框架，通过图像反演、对抗优化和重用攻击三个步骤，优化初始潜在变量，使遗忘模型的噪声分布重新对齐到原始不安全状态，从而重新激活休眠记忆。

Result: 在8种广泛使用的遗忘技术上进行实验，IVO实现了优越的攻击成功率（最高达99.5%）和强语义一致性，暴露了当前防御方法的根本缺陷。

Conclusion: 当前基于遗忘的防御方法存在根本性缺陷，所谓的"遗忘"只是表面现象，底层知识仍然存在且可被重新激活。这揭示了扩散模型安全防御需要更根本的解决方案。

Abstract: Although unlearning-based defenses claim to purge Not-Safe-For-Work (NSFW) concepts from diffusion models (DMs), we reveals that this "forgetting" is largely an illusion. Unlearning partially disrupts the mapping between linguistic symbols and the underlying knowledge, which remains intact as dormant memories. We find that the distributional discrepancy in the denoising process serves as a measurable indicator of how much of the mapping is retained, also reflecting the strength of unlearning. Inspired by this, we propose IVO (Initial Latent Variable Optimization), a concise and powerful attack framework that reactivates these dormant memories by reconstructing the broken mappings. Through Image Inversion}, Adversarial Optimization and Reused Attack, IVO optimizes initial latent variables to realign the noise distribution of unlearned models with their original unsafe states. Extensive experiments across 8 widely used unlearning techniques demonstrate that IVO achieves superior attack success rates and strong semantic consistency, exposing fundamental flaws in current defenses. The code is available at anonymous.4open.science/r/IVO/. Warning: This paper has unsafe images that may offend some readers.

</details>


### [483] [Benford's Law as a Distributional Prior for Post-Training Quantization of Large Language Models](https://arxiv.org/abs/2602.00165)
*Arthur Negrão,Pedro Silva,Vander L. S. Freitas,Gladston Moreira,Eduardo Luz*

Main category: cs.LG

TL;DR: 提出Benford-Quant量化方法，基于本福德定律设计对数间隔码本，针对LLM权重分布特点优化量化分辨率，在低比特量化中提升性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型快速增长需要有效压缩，现有均匀量化器假设参数均匀分布，但实际权重分布高度偏斜，需要更符合实际分布的非均匀量化方法。

Method: 提出Benford-Quant数据无关非均匀量化器，基于本福德定律的对数分布特性，用对数间隔码本替代均匀网格，为频繁出现的小幅权重分配更多分辨率。

Result: 1) Transformer变换层权重符合本福德统计，归一化层系统偏离；2) 在小语言模型上持续改善困惑度，4-bit量化使Gemma-270M困惑度降低超10%；3) 在大模型上保持竞争力，差异由过参数化效应解释。

Conclusion: 将本福德先验融入量化网格是低成本修改，在激进低比特量化中带来精度提升。虽未在困惑度和LAMBADA任务上超越SOTA，但可与SmoothQuant等量化方法混合使用，无需大改流程即可提升性能。

Abstract: The rapid growth of Large Language Models (LLMs) intensifies the need for effective compression, with weight quantization being the most widely adopted technique. Standard uniform quantizers assume that parameters are evenly distributed, an assumption at odds with the highly skewed distributions observed in practice. We propose Benford-Quant, a simple, data-free non-uniform quantizer inspired by Benford's Law, which predicts that leading digits follow a logarithmic distribution. Benford-Quant replaces the uniform grid with a log-spaced codebook, dedicating more resolution to the frequent small-magnitude weights. We provide both theoretical intuition and empirical evidence: (i) weights in transformer transformational layers adhere closely to Benford statistics, while normalization layers systematically deviate; (ii) on Small Language Models (SLMs), Benford-Quant consistently improves perplexity, reducing 4-bit perplexity on Gemma-270M by more than 10%; and (iii) on larger LLMs, it remains competitive, with differences explained by over-parameterization effects. Our results indicate that incorporating a Benford-inspired prior into quantization grids is a low-cost modification that yields accuracy gains in aggressive few-bit regimes. Although it is not able to surpass the state of the art in tasks such as perplexity and LAMBADA, the Benford-Quant approach can be hybridized with other quantization methods-such as SmoothQuant and Activation-Aware Quantization-without major pipeline modification, potentially improving their performance.

</details>


### [484] [Joint Continual Learning of Local Language Models and Cloud Offloading Decisions with Budget Constraints](https://arxiv.org/abs/2602.00166)
*Evan Chen,Wenzhi Fang,Shiqiang Wang,Christopher Brinton*

Main category: cs.LG

TL;DR: DA-GRPO是一种双优势扩展的组相对策略优化方法，用于在持续学习中智能管理本地小语言模型对云端大语言模型的依赖，通过将云端使用约束直接融入优势计算，实现任务能力和协作行为的联合学习。


<details>
  <summary>Details</summary>
Motivation: 本地部署的小语言模型需要在严格的内存和计算约束下持续支持多样化任务，必须选择性依赖云端大语言模型。然而，在持续学习中调节云端协助具有挑战性，因为基于奖励的强化学习通常会产生不稳定的卸载行为，并在任务分布变化时加剧灾难性遗忘。

Method: 提出DA-GRPO（双优势组相对策略优化），这是组相对策略优化的双优势扩展，将云端使用约束直接融入优势计算，避免固定的奖励塑造和外部路由模型。该设计使本地模型能够联合学习任务能力和协作行为，允许云端请求在训练后自然出现，同时遵守预设的协助预算。

Result: 在数学推理和代码生成基准测试中，DA-GRPO提高了切换后的准确性，显著减少了遗忘，并相比先前的协作和基于路由的方法保持了更稳定的云端使用。

Conclusion: DA-GRPO通过将云端使用约束直接融入策略优化过程，实现了本地小语言模型在持续学习中对云端大语言模型的智能协作，在保持性能的同时有效控制云端依赖，解决了传统方法中的不稳定卸载和灾难性遗忘问题。

Abstract: Locally deployed Small Language Models (SLMs) must continually support diverse tasks under strict memory and computation constraints, making selective reliance on cloud Large Language Models (LLMs) unavoidable. Regulating cloud assistance during continual learning is challenging, as naive reward-based reinforcement learning often yields unstable offloading behavior and exacerbates catastrophic forgetting as task distributions shift. We propose DA-GRPO, a dual-advantage extension of Group Relative Policy Optimization that incorporates cloud-usage constraints directly into advantage computation, avoiding fixed reward shaping and external routing models. This design enables the local model to jointly learn task competence and collaboration behavior, allowing cloud requests to emerge naturally during post-training while respecting a prescribed assistance budget. Experiments on mathematical reasoning and code generation benchmarks show that DA-GRPO improves post-switch accuracy, substantially reduces forgetting, and maintains stable cloud usage compared to prior collaborative and routing-based approaches.

</details>


### [485] [Statistical MIA: Rethinking Membership Inference Attack for Reliable Unlearning Auditing](https://arxiv.org/abs/2602.01150)
*Jialong Sun,Zeming Wei,Jiaxuan Zou,Jiacheng Gong,Guanheng Wang,Chengyang Dong,Jialong Li,Bo Liu*

Main category: cs.LG

TL;DR: 论文提出SMIA框架，通过统计检验直接比较成员与非成员数据分布，无需训练攻击模型，为机器遗忘审计提供可靠且高效的方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于成员推理攻击(MIA)的机器遗忘审计存在根本缺陷：成员检测失败并不等同于真正遗忘，且MIA作为二分类问题会产生无法观测的统计误差，导致对遗忘性能的过度乐观评估，同时计算开销巨大。

Method: 提出统计成员推理攻击(SMIA)框架，通过统计检验直接比较成员与非成员数据的分布差异，无需训练影子模型。该方法输出遗忘率及置信区间，量化审计结果的可靠性。

Result: 大量实验表明，SMIA相比现有MIA方法提供更可靠的审计结果，且计算成本显著降低。SMIA的理论保证和实证有效性使其成为可靠的机器遗忘审计新范式。

Conclusion: SMIA框架解决了传统MIA审计方法的根本缺陷，通过统计检验提供可量化的可靠审计，计算效率高，为机器遗忘审计提供了新的有效范式。

Abstract: Machine unlearning (MU) is essential for enforcing the right to be forgotten in machine learning systems. A key challenge of MU is how to reliably audit whether a model has truly forgotten specified training data. Membership Inference Attacks (MIAs) are widely used for unlearning auditing, where samples that evade membership detection are often regarded as successfully forgotten. After carefully revisiting the reliability of MIA, we show that this assumption is flawed: failed membership inference does not imply true forgetting. We theoretically demonstrate that MIA-based auditing, when formulated as a binary classification problem, inevitably incurs statistical errors whose magnitude cannot be observed during the auditing process. This leads to overly optimistic evaluations of unlearning performance, while incurring substantial computational overhead due to shadow model training. To address these limitations, we propose Statistical Membership Inference Attack (SMIA), a novel training-free and highly effective auditing framework. SMIA directly compares the distributions of member and non-member data using statistical tests, eliminating the need for learned attack models. Moreover, SMIA outputs both a forgetting rate and a corresponding confidence interval, enabling quantified reliability of the auditing results. Extensive experiments show that SMIA provides more reliable auditing with significantly lower computational cost than existing MIA-based approaches. Notably, the theoretical guarantees and empirical effectiveness of SMIA suggest it as a new paradigm for reliable machine unlearning auditing.

</details>


### [486] [GRIP2: A Robust and Powerful Deep Knockoff Method for Feature Selection](https://arxiv.org/abs/2602.00218)
*Bob Junyi Zou,Lu Tian*

Main category: cs.LG

TL;DR: GRIP2是一种深度学习特征选择方法，通过二维正则化表面积分和块随机采样，在高度相关、低信噪比场景下实现错误发现率控制和高统计功效。


<details>
  <summary>Details</summary>
Motivation: 在非线性、高度相关、低信噪比的复杂场景中，现有深度学习方法难以在严格控制错误发现率的同时有效识别真正有预测性的特征，需要更稳健的特征选择方法。

Method: 提出GRIP2方法：1) 构建二维正则化表面控制稀疏强度和稀疏化几何；2) 通过块随机采样在单次训练中近似表面积分；3) 生成反对称的特征重要性统计量，确保有限样本下的FDR控制。

Result: 在合成和半真实数据实验中，GRIP2在高相关性和低信噪比场景下表现出更好的鲁棒性、统计功效和稳定性。在真实HIV耐药性数据中，比现有线性方法更有效地识别已知耐药相关突变。

Conclusion: GRIP2通过二维正则化表面积分和高效采样策略，在复杂场景下实现了可靠的特征选择，为深度学习特征选择提供了新的有效方法。

Abstract: Identifying truly predictive covariates while strictly controlling false discoveries remains a fundamental challenge in nonlinear, highly correlated, and low signal-to-noise regimes, where deep learning based feature selection methods are most attractive. We propose Group Regularization Importance Persistence in 2 Dimensions (GRIP2), a deep knockoff feature importance statistic that integrates first-layer feature activity over a two-dimensional regularization surface controlling both sparsity strength and sparsification geometry. To approximate this surface integral in a single training run, we introduce efficient block-stochastic sampling, which aggregates feature activity magnitudes across diverse regularization regimes along the optimization trajectory. The resulting statistics are antisymmetric by construction, ensuring finite-sample FDR control. In extensive experiments on synthetic and semi-real data, GRIP2 demonstrates improved robustness to feature correlation and noise level: in high correlation and low signal-to-noise ratio regimes where standard deep learning based feature selectors may struggle, our method retains high power and stability. Finally, on real-world HIV drug resistance data, GRIP2 recovers known resistance-associated mutations with power better than established linear baselines, confirming its reliability in practice.

</details>


### [487] [The Blessing of Dimensionality in LLM Fine-tuning: A Variance-Curvature Perspective](https://arxiv.org/abs/2602.00170)
*Qiyao Liang,Jinyeop Song,Yizhou Liu,Jeff Gore,Ila Fiete,Risto Miikkulainen,Xin Qiu*

Main category: cs.LG

TL;DR: 权重扰动进化策略(ES)可以用极小种群(约30个)微调数十亿参数语言模型，这与经典零阶优化的维度诅咒直觉相悖。研究发现微调奖励呈现先升后降的非单调动态，两者都源于微调景观的低维曲率特性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解释两个看似矛盾的现象：1) 权重扰动进化策略(ES)能用极小种群微调超大语言模型，违反经典维度诅咒；2) 固定超参数下，ES和GRPO的微调奖励呈现先升后降的非单调动态。研究者认为这两个现象共享相同的几何根源。

Method: 使用ES作为几何探针，在GSM8K、ARC-C和WinoGrande数据集上，对Qwen2.5-Instruct模型(0.5B-7B参数)的微调奖励景观进行分析。提出最小二次随机上升模型来捕捉时间尺度异质性，并通过实验验证改进方向的可访问性。

Result: 实验表明：1) 奖励改进扰动在跨尺度实验中都能用小种群实现；2) 微调景观具有低维曲率特性，少数高曲率维度主导改进；3) 随机扰动在这些方向上具有相似的退化改进分量；4) 时间尺度异质性导致固定随机性下的先升后降动态。

Conclusion: ES的可扩展性和非单调训练动态都源于微调景观的低维曲率特性。高维微调可能比最坏情况理论暗示的更适合更广泛的优化方法，因为改进方向在经验上可通过小种群访问。

Abstract: Weight-perturbation evolution strategies (ES) can fine-tune billion-parameter language models with surprisingly small populations (e.g., $N\!\approx\!30$), contradicting classical zeroth-order curse-of-dimensionality intuition. We also observe a second seemingly separate phenomenon: under fixed hyperparameters, the stochastic fine-tuning reward often rises, peaks, and then degrades in both ES and GRPO. We argue that both effects reflect a shared geometric property of fine-tuning landscapes: they are low-dimensional in curvature. A small set of high-curvature dimensions dominates improvement, producing (i) heterogeneous time scales that yield rise-then-decay under fixed stochasticity, as captured by a minimal quadratic stochastic-ascent model, and (ii) degenerate improving updates, where many random perturbations share similar components along these directions. Using ES as a geometric probe on fine-tuning reward landscapes of GSM8K, ARC-C, and WinoGrande across Qwen2.5-Instruct models (0.5B--7B), we show that reward-improving perturbations remain empirically accessible with small populations across scales. Together, these results reconcile ES scalability with non-monotonic training dynamics and suggest that high-dimensional fine-tuning may admit a broader class of viable optimization methods than worst-case theory implies.

</details>


### [488] [Rod Flow: A Continuous-Time Model for Gradient Descent at the Edge of Stability](https://arxiv.org/abs/2602.01480)
*Eric Regis,Sinho Chewi*

Main category: cs.LG

TL;DR: 本文提出了Rod Flow作为梯度下降动力学的新ODE近似，相比之前的Central Flow，它基于物理直观的"杆"模型，能更好预测临界锐度阈值并解释自稳定现象。


<details>
  <summary>Details</summary>
Motivation: 理解梯度下降在非凸景观上的训练动态是一个重要问题。边缘稳定性现象表明，大学习率下的梯度下降会偏离梯度流，需要更准确的ODE近似来描述这种动态。

Method: 提出Rod Flow方法，将梯度下降迭代视为一维扩展对象（"杆"），基于物理原理推导出ODE近似。该方法显式且计算成本低，能预测临界锐度阈值并解释自稳定机制。

Result: Rod Flow在简单玩具示例中比Central Flow更好地捕捉梯度下降动态，在代表性神经网络架构中达到相同精度。理论证明了它能正确预测临界锐度阈值并解释四次势中的自稳定现象。

Conclusion: Rod Flow提供了一个基于物理原理、计算高效且准确的梯度下降动力学ODE近似，为理解边缘稳定性现象提供了新视角和理论工具。

Abstract: How can we understand gradient-based training over non-convex landscapes? The edge of stability phenomenon, introduced in Cohen et al. (2021), indicates that the answer is not so simple: namely, gradient descent (GD) with large step sizes often diverges away from the gradient flow. In this regime, the "Central Flow", recently proposed in Cohen et al. (2025), provides an accurate ODE approximation to the GD dynamics over many architectures. In this work, we propose Rod Flow, an alternative ODE approximation, which carries the following advantages: (1) it rests on a principled derivation stemming from a physical picture of GD iterates as an extended one-dimensional object -- a "rod"; (2) it better captures GD dynamics for simple toy examples and matches the accuracy of Central Flow for representative neural network architectures, and (3) is explicit and cheap to compute. Theoretically, we prove that Rod Flow correctly predicts the critical sharpness threshold and explains self-stabilization in quartic potentials. We validate our theory with a range of numerical experiments.

</details>


### [489] [LatentTrack: Sequential Weight Generation via Latent Filtering](https://arxiv.org/abs/2602.00458)
*Omer Haq*

Main category: cs.LG

TL;DR: LatentTrack (LT) 是一种用于非平稳动态下在线概率预测的序列神经架构，通过在低维潜空间进行因果贝叶斯滤波，并使用轻量级超网络生成预测模型参数，实现恒定时间的在线自适应。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理非平稳动态的在线预测时面临挑战，需要适应分布漂移并保持计算效率。本文旨在开发一种能够在恒定时间内进行在线自适应，无需每步梯度更新的预测框架。

Method: LT在低维潜空间执行因果贝叶斯滤波，使用轻量级超网络在每个时间步生成预测模型参数。采用预测-生成-更新的滤波框架：学习潜模型预测下一个潜分布，通过摊销推理使用新观测更新，支持结构化和非结构化潜动态，并通过潜轨迹的蒙特卡洛推理产生校准的预测混合。

Result: 在Jena Climate基准测试的长时域在线回归评估中，LT始终比有状态序列和静态不确定性感知基线获得更低的负对数似然和均方误差，具有竞争力的校准性能，表明潜条件函数演化是传统潜状态建模在分布漂移下的有效替代方案。

Conclusion: 潜条件函数演化是处理非平稳动态下在线概率预测的有效方法，LT框架在保持恒定时间计算成本的同时，实现了优于传统方法的预测性能和校准能力。

Abstract: We introduce LatentTrack (LT), a sequential neural architecture for online probabilistic prediction under nonstationary dynamics. LT performs causal Bayesian filtering in a low-dimensional latent space and uses a lightweight hypernetwork to generate predictive model parameters at each time step, enabling constant-time online adaptation without per-step gradient updates.
  At each time step, a learned latent model predicts the next latent distribution, which is updated via amortized inference using new observations, yielding a predict--generate--update filtering framework in function space. The formulation supports both structured (Markovian) and unstructured latent dynamics within a unified objective, while Monte Carlo inference over latent trajectories produces calibrated predictive mixtures with fixed per-step cost. Evaluated on long-horizon online regression using the Jena Climate benchmark, LT consistently achieves lower negative log-likelihood and mean squared error than stateful sequential and static uncertainty-aware baselines, with competitive calibration, demonstrating that latent-conditioned function evolution is an effective alternative to traditional latent-state modeling under distribution shift.

</details>


### [490] [Learning Robust Reasoning through Guided Adversarial Self-Play](https://arxiv.org/abs/2602.00173)
*Shuozhe Li,Vaishnav Tadiparthi,Kwonjoon Lee,Nakul Agarwal,Hossein Nourkhiz Mahjoub,Ehsan Moradi Pari,Lizhang Chen,Amy Zhang,Liu Leqi*

Main category: cs.LG

TL;DR: GASP方法通过对抗性自博弈训练，使强化学习模型能够在有缺陷的上下文条件下（如错误思维链）进行检测和修复，提高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于可验证奖励的强化学习（RLVR）模型在干净上下文条件下表现良好，但当上下文有缺陷（如思维链被破坏、误导性部分解或轻微输入扰动）时会灾难性失败，因为标准RLVR只优化在干净条件下的最终答案正确性。

Method: 提出GASP（引导对抗性自博弈）方法：在单个模型内形成对抗性自博弈游戏，污染者学习通过局部连贯的破坏来诱导失败，而智能体学习在相同破坏条件下诊断和恢复。为解决训练早期成功恢复稀缺的问题，提出分布内修复引导，即对自生成修复的模仿项，增加恢复概率同时保留已有能力。

Result: 在四个开源模型（1.5B-8B）上，GASP将强但脆弱的推理器转化为鲁棒推理器，能够承受误导和扰动上下文，同时通常还能提高干净准确性。分析显示对抗性破坏诱导了有效课程，分布内引导实现了快速恢复学习且表征漂移最小。

Conclusion: GASP方法仅使用结果验证，无需人工标签或外部教师，就能显式训练检测和修复能力，有效解决了RLVR模型在缺陷上下文条件下的脆弱性问题。

Abstract: Reinforcement learning from verifiable rewards (RLVR) produces strong reasoning models, yet they can fail catastrophically when the conditioning context is fallible (e.g., corrupted chain-of-thought, misleading partial solutions, or mild input perturbations), since standard RLVR optimizes final-answer correctness only under clean conditioning. We introduce GASP (Guided Adversarial Self-Play), a robustification method that explicitly trains detect-and-repair capabilities using only outcome verification. Without human labels or external teachers, GASP forms an adversarial self-play game within a single model: a polluter learns to induce failure via locally coherent corruptions, while an agent learns to diagnose and recover under the same corrupted conditioning. To address the scarcity of successful recoveries early in training, we propose in-distribution repair guidance, an imitation term on self-generated repairs that increases recovery probability while preserving previously acquired capabilities. Across four open-weight models (1.5B--8B), GASP transforms strong-but-brittle reasoners into robust ones that withstand misleading and perturbed context while often improving clean accuracy. Further analysis shows that adversarial corruptions induce an effective curriculum, and in-distribution guidance enables rapid recovery learning with minimal representational drift.

</details>


### [491] [Local Exponential Stability of Mean-Field Langevin Descent-Ascent in Wasserstein Space](https://arxiv.org/abs/2602.01564)
*Geuntaek Seo,Minseop Shin,Pierre Monmarché,Beomjun Choi*

Main category: cs.LG

TL;DR: 证明了均值场Langevin下降-上升动力学在熵正则化二人零和博弈中的局部指数稳定性：当初始分布足够接近均衡时，动力学以指数速率收敛到均衡。


<details>
  <summary>Details</summary>
Motivation: 虽然均值场目标函数存在唯一的混合纳什均衡，但对于一般非凸-非凹支付函数，原始MFL-DA动力学的长期行为一直未解决。本文旨在回答Wang和Chizat在COLT 2024中提出的开放性问题。

Method: 通过线性化算子的谱分析建立均衡附近熵的强制性估计，揭示局部位移凸-凹结构，从而证明收缩性质。

Result: 证明了均衡是局部指数稳定的：当初始分布在Wasserstein度量下足够接近均衡时，动力学以指数速率收敛到均衡。

Conclusion: 解决了Wang和Chizat提出的局部稳定性和定量速率问题，但全局收敛仍然是一个未解决的挑战。

Abstract: We study the mean-field Langevin descent-ascent (MFL-DA), a coupled optimization dynamics on the space of probability measures for entropically regularized two-player zero-sum games. Although the associated mean-field objective admits a unique mixed Nash equilibrium, the long-time behavior of the original MFL-DA for general nonconvex-nonconcave payoffs has remained largely open. Answering an open question posed by Wang and Chizat (COLT 2024), we provide a partial resolution by proving that this equilibrium is locally exponentially stable: if the initialization is sufficiently close in Wasserstein metric, the dynamics trends to the equilibrium at an exponential rate. The key to our analysis is to establish a coercivity estimate for the entropy near equilibrium via spectral analysis of the linearized operator. We show that this coercivity effectively reveals a local displacement convex-concave structure, thereby driving contraction. This result settles the local stability and quantitative rate questions of Wang and Chizat, leaving global convergence as a remaining open challenge.

</details>


### [492] [Bayesian Integration of Nonlinear Incomplete Clinical Data](https://arxiv.org/abs/2602.01924)
*Lucía González-Zamorano,Nuria Balbás-Esteban,Vanessa Gómez-Verdejo,Albert Belenguer-Llorens,Carlos Sevilla-Salcedo*

Main category: cs.LG

TL;DR: BIONIC是一个贝叶斯多模态框架，用于处理高维异构临床数据中的结构化缺失问题，通过联合生成-判别潜在架构实现鲁棒预测和内在可解释性。


<details>
  <summary>Details</summary>
Motivation: 多模态临床数据具有高维度、异构表示和结构化缺失的特点，这给预测建模、数据整合和可解释性带来了重大挑战。现有方法难以有效处理这些复杂特征。

Method: 提出BIONIC（贝叶斯非线性不完全临床数据集成）框架，采用联合生成-判别潜在架构。使用预训练嵌入处理医学图像和临床文本等复杂模态，将结构化临床变量直接纳入贝叶斯多模态公式。显式建模模态级和变量级缺失以及缺失标签。

Result: 在三个多模态临床和生物医学数据集上评估，相比代表性多模态基线方法，BIONIC展现出强大且一致的判别性能，特别是在不完全数据场景下。除了预测准确性外，还通过潜在结构提供内在可解释性。

Conclusion: BIONIC为处理多模态临床数据中的异构性和缺失问题提供了一个统一的概率框架，不仅提高了预测性能，还支持模态相关性分析和临床有意义见解的提取。

Abstract: Multimodal clinical data are characterized by high dimensionality, heterogeneous representations, and structured missingness, posing significant challenges for predictive modeling, data integration, and interpretability. We propose BIONIC (Bayesian Integration of Nonlinear Incomplete Clinical data), a unified probabilistic framework that integrates heterogeneous multimodal data under missingness through a joint generative-discriminative latent architecture. BIONIC uses pretrained embeddings for complex modalities such as medical images and clinical text, while incorporating structured clinical variables directly within a Bayesian multimodal formulation. The proposed framework enables robust learning in partially observed and semi-supervised settings by explicitly modeling modality-level and variable-level missingness, as well as missing labels. We evaluate BIONIC on three multimodal clinical and biomedical datasets, demonstrating strong and consistent discriminative performance compared to representative multimodal baselines, particularly under incomplete data scenarios. Beyond predictive accuracy, BIONIC provides intrinsic interpretability through its latent structure, enabling population-level analysis of modality relevance and supporting clinically meaningful insight.

</details>


### [493] [Fast Non-Episodic Finite-Horizon RL with K-Step Lookahead Thresholding](https://arxiv.org/abs/2602.00781)
*Jiamin Xu,Kyra Gan*

Main category: cs.LG

TL;DR: 提出一种有限时域MDP中的在线强化学习方法，使用K步前瞻Q函数和阈值机制，实现高效样本利用和理论保证的遗憾界。


<details>
  <summary>Details</summary>
Motivation: 现有无限时域方法依赖折扣收缩，不适用于需要估计到固定终止时间的有限时域MDP。非片段式有限时域在线强化学习研究不足，需要新方法处理固定时域结构。

Method: 引入K步前瞻Q函数，将规划截断到未来K步；采用阈值机制，仅当估计的K步前瞻值超过时变阈值时才选择动作；提出高效的表格学习算法，随时间自适应增加K以平衡前瞻深度和估计方差。

Result: 理论证明：K=1时达到极小极大最优常数遗憾；K≥2时遗憾界为O(max((K-1),C_{K-1})√(SAT log T))。实验在JumpRiverswim、FrozenLake和AnyTrading等环境中优于现有表格RL方法。

Conclusion: K步前瞻Q函数和阈值机制有效解决了有限时域MDP中的在线强化学习问题，实现了理论保证和实际性能的平衡，在合成MDP和实际RL环境中表现优异。

Abstract: Online reinforcement learning in non-episodic, finite-horizon MDPs remains underexplored and is challenged by the need to estimate returns to a fixed terminal time. Existing infinite-horizon methods, which often rely on discounted contraction, do not naturally account for this fixed-horizon structure. We introduce a modified Q-function: rather than targeting the full-horizon, we learn a K-step lookahead Q-function that truncates planning to the next K steps. To further improve sample efficiency, we introduce a thresholding mechanism: actions are selected only when their estimated K-step lookahead value exceeds a time-varying threshold. We provide an efficient tabular learning algorithm for this novel objective, proving it achieves fast finite-sample convergence: it achieves minimax optimal constant regret for $K=1$ and $\mathcal{O}(\max((K-1),C_{K-1})\sqrt{SAT\log(T)})$ regret for any $K \geq 2$. We numerically evaluate the performance of our algorithm under the objective of maximizing reward. Our implementation adaptively increases K over time, balancing lookahead depth against estimation variance. Empirical results demonstrate superior cumulative rewards over state-of-the-art tabular RL methods across synthetic MDPs and RL environments: JumpRiverswim, FrozenLake and AnyTrading.

</details>


### [494] [The Effect of Mini-Batch Noise on the Implicit Bias of Adam](https://arxiv.org/abs/2602.01642)
*Matias D. Cattaneo,Boris Shigida*

Main category: cs.LG

TL;DR: 研究Adam优化器中动量超参数(β₁, β₂)和批次大小如何通过小批量噪声影响隐式偏差，发现批次大小变化会反转β₁和β₂对泛化的影响方向。


<details>
  <summary>Details</summary>
Motivation: 在多轮训练中，高质量数据有限而计算资源增长，需要理解Adam优化器中动量超参数和批次大小如何通过小批量噪声影响隐式偏差，从而影响泛化性能。

Method: 建立理论框架分析小批量噪声如何影响Adam中内存的隐式偏差，研究β₁、β₂和批次大小之间的相互作用，并通过小规模数据实验验证理论发现。

Result: 发现批次大小变化会反转β₁和β₂对泛化的影响方向：大批次时高β₂增加反正则化（损害泛化），小批次时影响方向反转；默认参数(0.9,0.999)适合小批次，大批次时β₁接近β₂效果更好。

Conclusion: Adam优化器的动量超参数选择应结合批次大小考虑，批次大小变化会反转β₁和β₂对隐式偏差的影响方向，这种转变的规模与临界批次大小相关。

Abstract: With limited high-quality data and growing compute, multi-epoch training is gaining back its importance across sub-areas of deep learning. Adam(W), versions of which are go-to optimizers for many tasks such as next token prediction, has two momentum hyperparameters $(β_1, β_2)$ controlling memory and one very important hyperparameter, batch size, controlling (in particular) the amount mini-batch noise. We introduce a theoretical framework to understand how mini-batch noise influences the implicit bias of memory in Adam (depending on $β_1$, $β_2$) towards sharper or flatter regions of the loss landscape, which is commonly observed to correlate with the generalization gap in multi-epoch training. We find that in the case of large batch sizes, higher $β_2$ increases the magnitude of anti-regularization by memory (hurting generalization), but as the batch size becomes smaller, the dependence of (anti-)regulariation on $β_2$ is reversed. A similar monotonicity shift (in the opposite direction) happens in $β_1$. In particular, the commonly "default" pair $(β_1, β_2) = (0.9, 0.999)$ is a good choice if batches are small; for larger batches, in many settings moving $β_1$ closer to $β_2$ is much better in terms of validation accuracy in multi-epoch training. Moreover, our theoretical derivations connect the scale of the batch size at which the shift happens to the scale of the critical batch size. We illustrate this effect in experiments with small-scale data in the about-to-overfit regime.

</details>


### [495] [Over-Alignment vs Over-Fitting: The Role of Feature Learning Strength in Generalization](https://arxiv.org/abs/2602.00827)
*Taesun Yeom,Taehyeok Ha,Jaeho Lee*

Main category: cs.LG

TL;DR: 研究发现特征学习强度（FLS）存在最优值，既不能太小也不能太大，这与传统认为更强特征学习总是改善泛化的直觉相反。


<details>
  <summary>Details</summary>
Motivation: 现有理论主要研究FLS在渐近情况下的影响，但对实际训练条件下（如达到目标训练风险时停止训练）FLS如何影响泛化缺乏深入理解。

Method: 通过实证研究观察FLS对泛化的影响，然后对使用逻辑损失训练的两层ReLU网络进行梯度流动力学理论分析，通过初始化尺度控制FLS。

Result: 发现存在最优FLS值，能带来显著的泛化增益。理论分析表明最优FLS源于两种竞争效应的权衡：过大的FLS导致"过度对齐"现象损害泛化，过小的FLS导致过拟合。

Conclusion: 特征学习强度存在最优值，需要在过度对齐和过拟合之间取得平衡，这为实际训练中调整FLS提供了理论指导。

Abstract: Feature learning strength (FLS), i.e., the inverse of the effective output scaling of a model, plays a critical role in shaping the optimization dynamics of neural nets. While its impact has been extensively studied under the asymptotic regimes -- both in training time and FLS -- existing theory offers limited insight into how FLS affects generalization in practical settings, such as when training is stopped upon reaching a target training risk. In this work, we investigate the impact of FLS on generalization in deep networks under such practical conditions. Through empirical studies, we first uncover the emergence of an $\textit{optimal FLS}$ -- neither too small nor too large -- that yields substantial generalization gains. This finding runs counter to the prevailing intuition that stronger feature learning universally improves generalization. To explain this phenomenon, we develop a theoretical analysis of gradient flow dynamics in two-layer ReLU nets trained with logistic loss, where FLS is controlled via initialization scale. Our main theoretical result establishes the existence of an optimal FLS arising from a trade-off between two competing effects: An excessively large FLS induces an $\textit{over-alignment}$ phenomenon that degrades generalization, while an overly small FLS leads to $\textit{over-fitting}$.

</details>


### [496] [How Understanding Forecast Uncertainty Resolves the Explainability Problem in Machine Learning Models](https://arxiv.org/abs/2602.00179)
*Joseph L. Breeden*

Main category: cs.LG

TL;DR: 论文指出，机器学习可解释性方法（如LIME和SHAP）在决策边界附近的不稳定性反映了预测不确定性高的问题，正确做法是先评估预测是否可用，再寻求解释。


<details>
  <summary>Details</summary>
Motivation: 在关键决策应用中，可解释性是主要关注点，但现有局部线性解释方法（如LIME和SHAP）在决策边界附近的不稳定性受到批评。论文认为这种批评反映了对问题的误解。

Method: 提出新的解释框架：首先评估预测不确定性，确定是否存在可用的预测；当预测不确定性足够低时，通过局部线性近似寻求解释；当没有可用预测时，使用更简单的整体模型（如传统逻辑回归）。

Result: 当预测不确定性低时，解释不稳定性也相应较低；当预测不确定性高时，解释没有意义。某些声称处处可解释的方法（如ReLU网络或分段线性模型）实际上只有虚幻的可解释性。

Conclusion: 解释机器学习预测的正确顺序是：先评估预测是否可用，再寻求解释。对不可用预测的解释毫无意义，决策应回归到更简单的模型。这为可解释AI提供了更合理的框架。

Abstract: For applications of machine learning in critical decisions, explainability is a primary concern, and often a regulatory requirement. Local linear methods for generating explanations, such as LIME and SHAP, have been criticized for being unstable near decision boundaries. In this paper, we explain that such concerns reflect a misunderstanding of the problem. The forecast uncertainty is high at decision boundaries, so consequently, the explanatory instability is high. The correct approach is to change the sequence of events and questions being asked. Nonlinear models can be highly predictive in some regions while having little or no predictability in others. Therefore, the first question is whether a usable forecast exists. When there is a forecast with low enough uncertainty to be useful, an explanation can be sought via a local linear approximation. In such cases, the explanatory instability is correspondingly low. When no usable forecast exists, the decision must fall to a simpler overall model such as traditional logistic regression. Additionally, these results show that some methods that purport to be explainable everywhere, such as ReLU networks or any piecewise linear model, have only an illusory explainability, because the forecast uncertainty at the segment boundaries is too high to be useful. Explaining an unusable forecast is pointless.

</details>


### [497] [Don't Forget Its Variance! The Minimum Path Variance Principle for Accurate and Stable Score-Based Density Ratio Estimation](https://arxiv.org/abs/2602.00834)
*Wei Chen,Jiacheng Li,Shigui Li,Zhiqi Lin,Junmei Yang,John Paisley,Delu Zeng*

Main category: cs.LG

TL;DR: 本文提出MinPV原则，通过最小化路径方差来解决基于分数方法的密度比估计中的路径依赖问题，实现了更准确稳定的估计器。


<details>
  <summary>Details</summary>
Motivation: 基于分数方法的密度比估计存在一个重要悖论：虽然理论上路径独立，但实际性能严重依赖于选择的路径调度。现有方法忽略了理想目标函数与实际可训练目标函数之间的关键差异——时间分数的路径方差。

Method: 提出MinPV（最小路径方差）原则，推导出路径方差的闭式表达式，将难以处理的问题转化为可优化的目标。使用灵活的Kumaraswamy混合模型参数化路径，学习数据自适应、低方差的路径，无需启发式选择。

Result: 该方法在具有挑战性的基准测试中取得了新的最先进结果，产生了更准确和稳定的密度比估计器。

Conclusion: 通过最小化被忽视的路径方差，MinPV原则解决了基于分数方法的密度比估计中的路径依赖悖论，实现了理论目标与实际训练目标的一致性，显著提升了估计性能。

Abstract: Score-based methods have emerged as a powerful framework for density ratio estimation (DRE), but they face an important paradox in that, while theoretically path-independent, their practical performance depends critically on the chosen path schedule. We resolve this issue by proving that tractable training objectives differ from the ideal, ground-truth objective by a crucial, overlooked term: the path variance of the time score. To address this, we propose MinPV (\textbf{Min}imum \textbf{P}ath \textbf{V}ariance) Principle, which introduces a principled heuristic to minimize the overlooked path variance. Our key contribution is the derivation of a closed-form expression for the variance, turning an intractable problem into a tractable optimization. By parameterizing the path with a flexible Kumaraswamy Mixture Model, our method learns a data-adaptive, low-variance path without heuristic selection. This principled optimization of the complete objective yields more accurate and stable estimators, establishing new state-of-the-art results on challenging benchmarks.

</details>


### [498] [GEPC: Group-Equivariant Posterior Consistency for Out-of-Distribution Detection in Diffusion Models](https://arxiv.org/abs/2602.00191)
*Yadang Alexis Rouzoumka,Jean Pinsolle,Eugénie Terreaux,Christèle Morisseau,Jean-Philippe Ovarlez,Chengfang Ren*

Main category: cs.LG

TL;DR: 提出GEPC方法，通过检测扩散模型分数场的等变性破坏来检测OOD样本，无需训练，计算轻量


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的OOD检测方法主要利用分数大小或局部几何特征，忽略了分数场的等变性特性。当数据分布发生变化时，模型学到的等变性可能会被破坏，这可以作为OOD检测的有效信号。

Method: 提出Group-Equivariant Posterior Consistency (GEPC)方法：通过测量学习到的分数场在有限群G变换下的变换一致性来检测等变性破坏。计算等变性残差函数在群上的平均值，生成可解释的等变性破坏图。

Result: 在OOD图像基准数据集上，GEPC达到与现有扩散基线方法竞争或更好的AUROC性能，同时保持计算轻量。在高分辨率合成孔径雷达图像上，GEPC实现了强目标-背景分离和视觉可解释的等变性破坏图。

Conclusion: GEPC是一种无需训练、基于等变性一致性的OOD检测方法，能够有效检测分数场等变性破坏，在多种数据集上表现优异，并提供可解释的检测结果。

Abstract: Diffusion models learn a time-indexed score field $\mathbf{s}_θ(\mathbf{x}_t,t)$ that often inherits approximate equivariances (flips, rotations, circular shifts) from in-distribution (ID) data and convolutional backbones. Most diffusion-based out-of-distribution (OOD) detectors exploit score magnitude or local geometry (energies, curvature, covariance spectra) and largely ignore equivariances. We introduce Group-Equivariant Posterior Consistency (GEPC), a training-free probe that measures how consistently the learned score transforms under a finite group $\mathcal{G}$, detecting equivariance breaking even when score magnitude remains unchanged. At the population level, we propose the ideal GEPC residual, which averages an equivariance-residual functional over $\mathcal{G}$, and we derive ID upper bounds and OOD lower bounds under mild assumptions. GEPC requires only score evaluations and produces interpretable equivariance-breaking maps. On OOD image benchmark datasets, we show that GEPC achieves competitive or improved AUROC compared to recent diffusion-based baselines while remaining computationally lightweight. On high-resolution synthetic aperture radar imagery where OOD corresponds to targets or anomalies in clutter, GEPC yields strong target-background separation and visually interpretable equivariance-breaking maps. Code is available at https://github.com/RouzAY/gepc-diffusion/.

</details>


### [499] [Autocorrelated Optimize-via-Estimate: Predict-then-Optimize versus Finite-sample Optimal](https://arxiv.org/abs/2602.01877)
*Zichun Wang,Gar Goei Loke,Ruiting Zuo*

Main category: cs.LG

TL;DR: 提出A-OVE模型，在自相关不确定性下直接优化样本外性能，优于传统预测-优化方法


<details>
  <summary>Details</summary>
Motivation: 传统估计-优化方法在有限样本下可能不是最优的，特别是在自相关不确定性（VARMA过程）下，需要直接优化样本外性能的方法

Method: 提出自相关优化-估计（A-OVE）模型，通过充分统计量获得样本外最优解，并提出计算充分统计量的递归形式

Result: 在带交易成本的组合优化问题中，A-OVE相对于完美信息预测器实现低遗憾，优于预测-优化机器学习基准；高准确率的机器学习模型可能决策质量更差

Conclusion: A-OVE模型在自相关不确定性下表现优异，即使在模型轻微误设时性能依然保持，为数据驱动优化提供了有效方法

Abstract: Models that directly optimize for out-of-sample performance in the finite-sample regime have emerged as a promising alternative to traditional estimate-then-optimize approaches in data-driven optimization. In this work, we compare their performance in the context of autocorrelated uncertainties, specifically, under a Vector Autoregressive Moving Average VARMA(p,q) process. We propose an autocorrelated Optimize-via-Estimate (A-OVE) model that obtains an out-of-sample optimal solution as a function of sufficient statistics, and propose a recursive form for computing its sufficient statistics. We evaluate these models on a portfolio optimization problem with trading costs. A-OVE achieves low regret relative to a perfect information oracle, outperforming predict-then-optimize machine learning benchmarks. Notably, machine learning models with higher accuracy can have poorer decision quality, echoing the growing literature in data-driven optimization. Performance is retained under small mis-specification.

</details>


### [500] [Multimodal Scientific Learning Beyond Diffusions and Flows](https://arxiv.org/abs/2602.00960)
*Leonardo Ferreira Guilhoto,Akshat Kaushal,Paris Perdikaris*

Main category: cs.LG

TL;DR: MDNs作为显式参数密度估计器，在科学机器学习中为多模态不确定性量化提供了一种被忽视的高效替代方案，相比隐式生成模型具有更好的数据效率、泛化性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 科学机器学习面临多模态条件不确定性问题（如不适定逆问题、多稳态、混沌动力学），当前流行的隐式生成模型（扩散模型、流模型）通常数据需求大、计算成本高，且与科学问题的结构化解空间不匹配。

Method: 提出使用混合密度网络（MDNs）作为显式参数密度估计器，通过为低维多模态物理问题量身定制的归纳偏置，直接全局分配不同解分支的概率质量。

Result: MDNs在数据稀缺情况下能可靠恢复分离的模式，在逆问题、多稳态和混沌科学回归任务中表现出优越的泛化能力、可解释性和样本效率。

Conclusion: MDNs为科学机器学习中的多模态不确定性量化提供了一个原则性且被忽视的替代方案，相比隐式生成模型具有更强的数据效率和与科学问题结构的对齐性。

Abstract: Scientific machine learning (SciML) increasingly requires models that capture multimodal conditional uncertainty arising from ill-posed inverse problems, multistability, and chaotic dynamics. While recent work has favored highly expressive implicit generative models such as diffusion and flow-based methods, these approaches are often data-hungry, computationally costly, and misaligned with the structured solution spaces frequently found in scientific problems. We demonstrate that Mixture Density Networks (MDNs) provide a principled yet largely overlooked alternative for multimodal uncertainty quantification in SciML. As explicit parametric density estimators, MDNs impose an inductive bias tailored to low-dimensional, multimodal physics, enabling direct global allocation of probability mass across distinct solution branches. This structure delivers strong data efficiency, allowing reliable recovery of separated modes in regimes where scientific data is scarce. We formalize these insights through a unified probabilistic framework contrasting explicit and implicit distribution networks, and demonstrate empirically that MDNs achieve superior generalization, interpretability, and sample efficiency across a range of inverse, multistable, and chaotic scientific regression tasks.

</details>


### [501] [Reducing Memorisation in Generative Models via Riemannian Bayesian Inference](https://arxiv.org/abs/2602.00199)
*Johanna Marie Gegenfurtner,Albert Kjøller Jacobsen,Naima Elosegui Borras,Alejandro Valverde Mahou,Georgios Arvanitidis*

Main category: cs.LG

TL;DR: 提出一种贝叶斯方法，通过考虑损失函数的黎曼几何来构建预测后验，减少生成模型的记忆效应同时保持泛化能力


<details>
  <summary>Details</summary>
Motivation: 现代生成模型能产生逼真样本，但在记忆与泛化之间取得平衡仍是一个开放问题。作者从贝叶斯视角出发，关注流匹配和扩散模型的参数空间，旨在构建能更好捕捉数据分布变异性的预测后验。

Method: 使用黎曼度量捕捉损失函数的几何结构，并利用能适应损失景观局部结构的灵活近似后验。该方法允许采样与原始模型相似但记忆效应降低的生成模型。

Result: 实验证明该方法能减少记忆效应同时保持泛化能力。理论分析解释了这些发现。

Conclusion: 考虑损失函数的几何结构能有效利用参数空间，即使对于复杂的高维生成模型也是如此。这项工作展示了如何通过几何视角改善生成模型的记忆-泛化平衡。

Abstract: Modern generative models can produce realistic samples, however, balancing memorisation and generalisation remains an open problem. We approach this challenge from a Bayesian perspective by focusing on the parameter space of flow matching and diffusion models and constructing a predictive posterior that better captures the variability of the data distribution. In particular, we capture the geometry of the loss using a Riemannian metric and leverage a flexible approximate posterior that adapts to the local structure of the loss landscape. This approach allows us to sample generative models that resemble the original model, but exhibit reduced memorisation. Empirically, we demonstrate that the proposed approach reduces memorisation while preserving generalisation. Further, we provide a theoretical analysis of our method, which explains our findings. Overall, our work illustrates how considering the geometry of the loss enables effective use of the parameter space, even for complex high-dimensional generative models.

</details>


### [502] [Maximizing Reliability with Bayesian Optimization](https://arxiv.org/abs/2602.02432)
*Jack M. Buckingham,Ivo Couckuyt,Juergen Branke*

Main category: cs.LG

TL;DR: 提出两种基于Thompson采样和知识梯度的贝叶斯优化方法，用于极小失效概率（10^-6-10^-8）的可靠性优化问题，结合重要性采样提升效率。


<details>
  <summary>Details</summary>
Motivation: 制造业中需要优化设计的可靠性，即最小化随机扰动下的失效概率，这类问题涉及极小的失效概率（10^-6-10^-8），传统贝叶斯优化方法难以高效处理。

Method: 提出两种贝叶斯优化方法：1) 基于Thompson采样的方法；2) 基于知识梯度的方法，后者近似最小化失效概率对数的一步贝叶斯最优策略。两种方法都结合重要性采样来处理极小的失效概率。

Result: 实验结果表明，所提出的方法在极端和非极端失效概率情况下都优于现有方法。

Conclusion: 结合重要性采样的贝叶斯优化方法能有效处理极小失效概率的可靠性优化问题，Thompson采样和知识梯度两种策略都表现出优越性能。

Abstract: Bayesian optimization (BO) is a popular, sample-efficient technique for expensive, black-box optimization. One such problem arising in manufacturing is that of maximizing the reliability, or equivalently minimizing the probability of a failure, of a design which is subject to random perturbations - a problem that can involve extremely rare failures ($P_\mathrm{fail} = 10^{-6}-10^{-8}$). In this work, we propose two BO methods based on Thompson sampling and knowledge gradient, the latter approximating the one-step Bayes-optimal policy for minimizing the logarithm of the failure probability. Both methods incorporate importance sampling to target extremely small failure probabilities. Empirical results show the proposed methods outperform existing methods in both extreme and non-extreme regimes.

</details>


### [503] [Superposition unifies power-law training dynamics](https://arxiv.org/abs/2602.01045)
*Zixin Jessie Chen,Hao Chen,Yizhou Liu,Jeff Gore*

Main category: cs.LG

TL;DR: 研究发现特征叠加会导致训练速度显著加快，产生数据无关的普适幂律指数~1，相比无叠加的序列学习加速可达10倍。


<details>
  <summary>Details</summary>
Motivation: 研究特征叠加在幂律训练动力学中的作用，特别是在大规模语言模型等使用叠加的神经网络中，理解叠加如何影响训练速度和动态。

Method: 使用师生框架研究特征叠加，首先推导无叠加时的解析理论，然后分析叠加瓶颈如何诱导训练动态转变。

Result: 发现叠加瓶颈会导致训练指数转变为普适的~1幂律，与数据和通道统计无关，相比无叠加的序列学习加速可达10倍。

Conclusion: 特征叠加导致快速训练和数据无关的幂律指数，这对包括大规模语言模型在内的使用叠加的神经网络有重要启示。

Abstract: We investigate the role of feature superposition in the emergence of power-law training dynamics using a teacher-student framework. We first derive an analytic theory for training without superposition, establishing that the power-law training exponent depends on both the input data statistics and channel importance. Remarkably, we discover that a superposition bottleneck induces a transition to a universal power-law exponent of $\sim 1$, independent of data and channel statistics. This one over time training with superposition represents an up to tenfold acceleration compared to the purely sequential learning that takes place in the absence of superposition. Our finding that superposition leads to rapid training with a data-independent power law exponent may have important implications for a wide range of neural networks that employ superposition, including production-scale large language models.

</details>


### [504] [Reducing Class-Wise Performance Disparity via Margin Regularization](https://arxiv.org/abs/2602.00205)
*Beier Zhu,Kesen Zhao,Jiequan Cui,Qianru Sun,Yuan Zhou,Xun Yang,Hanwang Zhang*

Main category: cs.LG

TL;DR: MR²提出了一种基于边界的正则化方法，通过动态调整logit空间和表示空间的边界来减少分类任务中的性能差异，特别提升困难类别的性能。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络即使在类别平衡的数据上训练，也经常表现出显著的类别间准确率差异，这对可靠部署构成挑战。虽然已有经验性解决方案，但对分类任务中这种性能差异的理论理解仍然有限。

Method: MR²（Margin Regularization for Performance Disparity Reduction）是一种理论上有原则的正则化方法，通过动态调整logit空间和表示空间的边界。它根据特征传播优化每个类别的logit边界，并惩罚过大的表示边界以增强类内紧凑性。

Result: 在7个数据集（包括ImageNet）和多种预训练骨干网络（MAE、MoCov2、CLIP）上的实验表明，MR²不仅提高了整体准确率，还显著提升了困难类别的性能，同时不牺牲简单类别的性能，从而减少了性能差异。

Conclusion: MR²提供了一种理论驱动的正则化方法，通过边界调整有效减少分类任务中的性能差异，特别关注提升困难类别的表现，为可靠部署提供了有前景的解决方案。

Abstract: Deep neural networks often exhibit substantial disparities in class-wise accuracy, even when trained on class-balanced data, posing concerns for reliable deployment. While prior efforts have explored empirical remedies, a theoretical understanding of such performance disparities in classification remains limited. In this work, we present Margin Regularization for Performance Disparity Reduction (MR$^2$), a theoretically principled regularization for classification by dynamically adjusting margins in both the logit and representation spaces. Our analysis establishes a margin-based, class-sensitive generalization bound that reveals how per-class feature variability contributes to error, motivating the use of larger margins for hard classes. Guided by this insight, MR$^2$ optimizes per-class logit margins proportional to feature spread and penalizes excessive representation margins to enhance intra-class compactness. Experiments on seven datasets, including ImageNet, and diverse pre-trained backbones (MAE, MoCov2, CLIP) demonstrate that MR$^2$ not only improves overall accuracy but also significantly boosts hard class performance without trading off easy classes, thus reducing performance disparity. Code is available at: https://github.com/BeierZhu/MR2

</details>


### [505] [Multi-LLM Adaptive Conformal Inference for Reliable LLM Responses](https://arxiv.org/abs/2602.01285)
*Kangjun Noh,Seongchan Lee,Ilmun Kim,Kyungwoo Song*

Main category: cs.LG

TL;DR: MACI提出了一种基于乘积过滤的多LLM自适应保形推理方法，通过集成模型提高事实性评分准确性，在保证覆盖率的同时显著提高了真实声明的保留率。


<details>
  <summary>Details</summary>
Motivation: 在医疗和法律等高风险领域使用大语言模型时，确保事实性至关重要。现有保形推理方法要么过于保守（丢弃过多真实声明），要么依赖自适应错误率和简单线性模型，无法捕捉复杂的群体结构。

Method: 将保形推理重新表述为乘积过滤设置，将事实性建模为声明级评分的乘积。MACI方法利用集成模型产生更准确的事实性评分，并通过群体条件校准保持有效性。

Result: 实验表明，MACI始终达到用户指定的覆盖率，同时相比基线方法显著提高了保留率并降低了时间成本。

Conclusion: MACI通过乘积过滤和集成建模，在保证保形推理有效性的同时，提高了事实性评估的准确性和效率，为高风险领域的大语言模型应用提供了更实用的解决方案。

Abstract: Ensuring factuality is essential for the safe use of Large Language Models (LLMs) in high-stakes domains such as medicine and law. Conformal inference provides distribution-free guarantees, but existing approaches are either overly conservative, discarding many true-claims, or rely on adaptive error rates and simple linear models that fail to capture complex group structures. To address these challenges, we reformulate conformal inference in a multiplicative filtering setting, modeling factuality as a product of claim-level scores. Our method, Multi-LLM Adaptive Conformal Inference (MACI), leverages ensembles to produce more accurate factuality-scores, which in our experiments led to higher retention, while validity is preserved through group-conditional calibration. Experiments show that MACI consistently achieves user-specified coverage with substantially higher retention and lower time cost than baselines. Our repository is available at https://github.com/MLAI-Yonsei/MACI

</details>


### [506] [Analyzing Shapley Additive Explanations to Understand Anomaly Detection Algorithm Behaviors and Their Complementarity](https://arxiv.org/abs/2602.00208)
*Jordan Levy,Paul Saves,Moncef Garouani,Nicolas Verstaevel,Benoit Gaudou*

Main category: cs.LG

TL;DR: 该论文提出了一种基于SHAP解释的异常检测器表征方法，通过量化模型对输入特征的重要性分配来测量检测器之间的相似性，从而构建更具互补性和有效性的集成异常检测系统。


<details>
  <summary>Details</summary>
Motivation: 无监督异常检测面临数据分布多样性和缺乏标签的挑战。集成方法虽然能减少个体偏差并提高鲁棒性，但现有检测器往往依赖相似的决策线索，导致异常评分冗余，难以构建真正互补的集成系统。

Method: 使用SHapley Additive exPlanations（SHAP）量化每个异常检测器对输入特征的重要性分配，构建检测器的解释特征剖面。基于这些解释剖面测量检测器之间的相似性，通过解释多样性来指导集成模型的选择。

Result: 研究发现：1）具有相似解释的检测器会产生相关的异常评分并识别重叠的异常；2）解释差异可靠地指示了互补的检测行为；3）解释驱动的度量提供了与原始输出不同的集成选择标准；4）仅多样性不足，个体模型性能仍是有效集成的前提。

Conclusion: 通过同时关注解释多样性和模型质量，能够构建更互补、更多样且最终更有效的无监督异常检测集成系统。解释驱动的模型表征为集成学习提供了新的视角和实用方法。

Abstract: Unsupervised anomaly detection is a challenging problem due to the diversity of data distributions and the lack of labels. Ensemble methods are often adopted to mitigate these challenges by combining multiple detectors, which can reduce individual biases and increase robustness. Yet building an ensemble that is genuinely complementary remains challenging, since many detectors rely on similar decision cues and end up producing redundant anomaly scores. As a result, the potential of ensemble learning is often limited by the difficulty of identifying models that truly capture different types of irregularities. To address this, we propose a methodology for characterizing anomaly detectors through their decision mechanisms. Using SHapley Additive exPlanations, we quantify how each model attributes importance to input features, and we use these attribution profiles to measure similarity between detectors. We show that detectors with similar explanations tend to produce correlated anomaly scores and identify largely overlapping anomalies. Conversely, explanation divergence reliably indicates complementary detection behavior. Our results demonstrate that explanation-driven metrics offer a different criterion than raw outputs for selecting models in an ensemble. However, we also demonstrate that diversity alone is insufficient; high individual model performance remains a prerequisite for effective ensembles. By explicitly targeting explanation diversity while maintaining model quality, we are able to construct ensembles that are more diverse, more complementary, and ultimately more effective for unsupervised anomaly detection.

</details>


### [507] [High-accuracy sampling for diffusion models and log-concave distributions](https://arxiv.org/abs/2602.01338)
*Fan Chen,Sinho Chewi,Constantinos Daskalakis,Alexander Rakhlin*

Main category: cs.LG

TL;DR: 提出扩散模型采样算法，仅需polylog(1/δ)步即可达到δ误差，相比之前方法实现指数级改进


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型采样方法需要大量步骤才能达到高精度，计算成本高昂。本文旨在开发更高效的采样算法，显著减少所需步骤数。

Method: 基于L²空间中δ-准确分数估计，设计新的采样算法。在不同数据假设下提供复杂度分析：最小数据假设、非均匀L-Lipschitz条件、数据分布具有内在维度d⋆的情况。

Result: 算法在polylog(1/δ)步内达到δ误差，实现指数级改进。复杂度分别为：最小假设下Õ(d polylog(1/δ))；Lipschitz条件下Õ(√dL polylog(1/δ))；内在维度d⋆时Õ(d⋆ polylog(1/δ))。首次实现仅使用梯度评估的对数凹分布polylog(1/δ)复杂度采样器。

Conclusion: 本文提出的扩散模型采样算法在多种数据假设下均能实现polylog(1/δ)步数的指数级改进，为高效采样提供了理论保证，并首次实现对一般对数凹分布的高效采样。

Abstract: We present algorithms for diffusion model sampling which obtain $δ$-error in $\mathrm{polylog}(1/δ)$ steps, given access to $\widetilde O(δ)$-accurate score estimates in $L^2$. This is an exponential improvement over all previous results. Specifically, under minimal data assumptions, the complexity is $\widetilde O(d\,\mathrm{polylog}(1/δ))$ where $d$ is the dimension of the data; under a non-uniform $L$-Lipschitz condition, the complexity is $\widetilde O(\sqrt{dL}\,\mathrm{polylog}(1/δ))$; and if the data distribution has intrinsic dimension $d_\star$, then the complexity reduces to $\widetilde O(d_\star\,\mathrm{polylog}(1/δ))$. Our approach also yields the first $\mathrm{polylog}(1/δ)$ complexity sampler for general log-concave distributions using only gradient evaluations.

</details>


### [508] [Dispersion Loss Counteracts Embedding Condensation and Improves Generalization in Small Language Models](https://arxiv.org/abs/2602.00217)
*Chen Liu,Xingzhi Sun,Xi Xiao,Alexandre Van Tassel,Ke Xu,Kristof Reimann,Danqi Liao,Mark Gerstein,Tianyang Wang,Xiao Wang,Smita Krishnaswamy*

Main category: cs.LG

TL;DR: 论文研究了LLM缩放中的嵌入凝结现象，发现小模型存在严重的嵌入凝结，而大模型对此更抵抗。作者提出了一种分散损失来对抗凝结，实验证明该方法能恢复大模型的分散模式并提升性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型通过增加参数获得卓越性能，但缩放带来高昂计算成本。为了理解LLM缩放机制，研究大模型与小模型之间的表征差异，目标是让小模型复制大模型的表征质量。

Method: 观察到嵌入凝结现象，即token嵌入在某些语言模型中坍缩到狭窄的锥形子空间。通过跨多个Transformer家族的系统分析，发现小模型如GPT2和Qwen3-0.6B存在严重凝结，而大模型如GPT2-xl和Qwen3-32B对此更抵抗。提出分散损失来显式鼓励训练期间的嵌入分散。

Result: 实验证明分散损失能缓解凝结现象，恢复大模型中观察到的分散模式，并在10个基准测试中带来性能提升。知识蒸馏不能可靠地缓解嵌入凝结。

Conclusion: 这项工作为改进小型Transformer提供了一条原则性路径，无需额外参数。嵌入凝结是理解模型缩放差异的关键现象，对抗凝结能提升小模型性能。

Abstract: Large language models (LLMs) achieve remarkable performance through ever-increasing parameter counts, but scaling incurs steep computational costs. To better understand LLM scaling, we study representational differences between LLMs and their smaller counterparts, with the goal of replicating the representational qualities of larger models in the smaller models. We observe a geometric phenomenon which we term $\textbf{embedding condensation}$, where token embeddings collapse into a narrow cone-like subspace in some language models. Through systematic analyses across multiple Transformer families, we show that small models such as $\texttt{GPT2}$ and $\texttt{Qwen3-0.6B}$ exhibit severe condensation, whereas the larger models such as $\texttt{GPT2-xl}$ and $\texttt{Qwen3-32B}$ are more resistant to this phenomenon. Additional observations show that embedding condensation is not reliably mitigated by knowledge distillation from larger models. To fight against it, we formulate a dispersion loss that explicitly encourages embedding dispersion during training. Experiments demonstrate that it mitigates condensation, recovers dispersion patterns seen in larger models, and yields performance gains across 10 benchmarks. We believe this work offers a principled path toward improving smaller Transformers without additional parameters.

</details>


### [509] [Green-NAS: A Global-Scale Multi-Objective Neural Architecture Search for Robust and Efficient Edge-Native Weather Forecasting](https://arxiv.org/abs/2602.00240)
*Md Muhtasim Munif Fahim,Soyda Humyra Yesmin,Saiful Islam,Md. Palash Bin Faruque,Md. A. Salam,Md. Mahfuz Uddin,Samiul Islam,Tofayel Ahmed,Md. Binyamin,Md. Rezaul Karim*

Main category: cs.LG

TL;DR: Green-NAS是一个面向低资源环境的多目标神经架构搜索框架，以天气预报为案例，在保证精度的同时显著减少计算能耗和模型参数。


<details>
  <summary>Details</summary>
Motivation: 针对传统神经架构搜索计算成本高、碳足迹大的问题，遵循"绿色AI"原则，为低资源环境开发可持续的NAS框架，特别是在天气预报等需要高效部署的场景。

Method: 采用多目标优化方法，同时优化模型精度和效率，搜索轻量级架构；结合迁移学习策略，在历史数据有限的城市中提升预测精度。

Result: 最佳模型Green-NAS-A仅用15.3万参数达到RMSE 0.0988，精度接近人工调优基线（差距1.4%），参数数量比GraphCast等全球天气预报模型少239倍；迁移学习可将预测精度提升约5.2%。

Conclusion: Green-NAS框架成功实现了在低资源环境下平衡精度与效率的目标，为可持续AI部署提供了可行方案，特别适用于天气预报等需要高效模型的应用场景。

Abstract: We introduce Green-NAS, a multi-objective NAS (neural architecture search) framework designed for low-resource environments using weather forecasting as a case study. By adhering to 'Green AI' principles, the framework explicitly minimizes computational energy costs and carbon footprints, prioritizing sustainable deployment over raw computational scale. The Green-NAS architecture search method is optimized for both model accuracy and efficiency to find lightweight models with high accuracy and very few model parameters; this is accomplished through an optimization process that simultaneously optimizes multiple objectives. Our best-performing model, Green-NAS-A, achieved an RMSE of 0.0988 (i.e., within 1.4% of our manually tuned baseline) using only 153k model parameters, which is 239 times fewer than other globally applied weather forecasting models, such as GraphCast. In addition, we also describe how the use of transfer learning will improve the weather forecasting accuracy by approximately 5.2%, in comparison to a naive approach of training a new model for each city, when there is limited historical weather data available for that city.

</details>


### [510] [An Odd Estimator for Shapley Values](https://arxiv.org/abs/2602.01399)
*Fabian Fumagalli,Landon Butler,Justin Singh Kang,Kannan Ramchandran,R. Teal Witter*

Main category: cs.LG

TL;DR: 提出OddSHAP方法，通过证明Shapley值仅依赖于集合函数的奇分量，利用配对采样正交化回归目标，在奇子空间进行多项式回归，实现更准确的Shapley值估计。


<details>
  <summary>Details</summary>
Motivation: Shapley值是机器学习中广泛使用的归因框架，但精确计算通常不可行，需要高效近似方法。虽然最有效的估计器利用配对采样启发式方法减少估计误差，但其理论机制一直不明确。

Method: 1) 证明Shapley值仅依赖于集合函数的奇分量；2) 提出配对采样通过正交化回归目标来过滤无关的偶分量；3) 开发OddSHAP估计器，在奇子空间进行多项式回归，使用傅里叶基隔离该子空间，并利用代理模型识别高影响交互。

Result: 通过广泛的基准评估，OddSHAP实现了最先进的估计精度，克服了高阶近似的组合爆炸问题。

Conclusion: 为配对采样提供了理论基础，提出的OddSHAP方法通过专注于奇子空间，实现了更准确高效的Shapley值估计，为特征重要性、数据估值和因果推断等应用提供了改进工具。

Abstract: The Shapley value is a ubiquitous framework for attribution in machine learning, encompassing feature importance, data valuation, and causal inference. However, its exact computation is generally intractable, necessitating efficient approximation methods. While the most effective and popular estimators leverage the paired sampling heuristic to reduce estimation error, the theoretical mechanism driving this improvement has remained opaque. In this work, we provide an elegant and fundamental justification for paired sampling: we prove that the Shapley value depends exclusively on the odd component of the set function, and that paired sampling orthogonalizes the regression objective to filter out the irrelevant even component. Leveraging this insight, we propose OddSHAP, a novel consistent estimator that performs polynomial regression solely on the odd subspace. By utilizing the Fourier basis to isolate this subspace and employing a proxy model to identify high-impact interactions, OddSHAP overcomes the combinatorial explosion of higher-order approximations. Through an extensive benchmark evaluation, we find that OddSHAP achieves state-of-the-art estimation accuracy.

</details>


### [511] [TABES: Trajectory-Aware Backward-on-Entropy Steering for Masked Diffusion Models](https://arxiv.org/abs/2602.00250)
*Shreshth Saini,Avinab Saha,Balu Adsumilli,Neil Birkbeck,Yilin Wang,Alan C. Bovik*

Main category: cs.LG

TL;DR: 提出Backward-on-Entropy (BoE) Steering方法，通过单次反向传播近似无限视野前瞻，解决掩码扩散模型采样中的轨迹锁定问题，实现高效非自回归生成。


<details>
  <summary>Details</summary>
Motivation: 当前掩码扩散模型的采样方法依赖简单的基于置信度的启发式策略，忽略了局部决策的长期影响，导致早期幻觉会级联成全局不一致性。虽然基于搜索的方法可以缓解这个问题，但计算成本过高（每步需要O(K)次前向传播）。

Method: 提出BoE Steering框架，通过从轨迹成本函数的一阶展开推导出Token Influence Score (TIS)，证明未来熵相对于输入嵌入的梯度可以作为最小化不确定性的最优控制信号。引入ActiveQueryAttention稀疏伴随原语，利用掩码目标的结构降低反向传播复杂度。

Result: BoE在推理时间缩放方面实现了优于现有解掩码方法的帕累托前沿，表明梯度引导的转向为鲁棒的非自回归生成提供了数学原理清晰且高效的路径。

Conclusion: 梯度引导的转向为掩码扩散模型提供了一种数学原理清晰且高效的采样方法，解决了轨迹锁定问题，同时保持了计算效率，为非自回归生成开辟了新方向。

Abstract: Masked Diffusion Models (MDMs) have emerged as a promising non-autoregressive paradigm for generative tasks, offering parallel decoding and bidirectional context utilization. However, current sampling methods rely on simple confidence-based heuristics that ignore the long-term impact of local decisions, leading to trajectory lock-in where early hallucinations cascade into global incoherence. While search-based methods mitigate this, they incur prohibitive computational costs ($O(K)$ forward passes per step). In this work, we propose Backward-on-Entropy (BoE) Steering, a gradient-guided inference framework that approximates infinite-horizon lookahead via a single backward pass. We formally derive the Token Influence Score (TIS) from a first-order expansion of the trajectory cost functional, proving that the gradient of future entropy with respect to input embeddings serves as an optimal control signal for minimizing uncertainty. To ensure scalability, we introduce \texttt{ActiveQueryAttention}, a sparse adjoint primitive that exploits the structure of the masking objective to reduce backward pass complexity. BoE achieves a superior Pareto frontier for inference-time scaling compared to existing unmasking methods, demonstrating that gradient-guided steering offers a mathematically principled and efficient path to robust non-autoregressive generation. We will release the code.

</details>


### [512] [DCD: Decomposition-based Causal Discovery from Autocorrelated and Non-Stationary Temporal Data](https://arxiv.org/abs/2602.01433)
*Muhammad Hasan Ferdous,Md Osman Gani*

Main category: cs.LG

TL;DR: 提出基于分解的因果发现框架，将多元时间序列分解为趋势、季节和残差分量，分别进行因果分析，最后整合为多尺度因果结构。


<details>
  <summary>Details</summary>
Motivation: 多元时间序列（金融、气候科学、医疗等领域）常呈现长期趋势、季节模式和短期波动，在非平稳性和自相关下进行因果推断复杂。现有方法直接在原始观测上操作，容易产生虚假边和错误归因的时间依赖。

Method: 1) 将每个时间序列分解为趋势、季节和残差分量；2) 对趋势分量使用平稳性检验；3) 对季节分量使用基于核的依赖度量；4) 对残差分量使用基于约束的因果发现；5) 将分量级图整合为统一的多尺度因果结构。

Result: 在广泛的合成基准测试和真实世界气候数据上，该框架比现有最先进基线方法更准确地恢复真实因果结构，特别是在强非平稳性和时间自相关情况下。

Conclusion: 分解框架能分离长短期因果效应，减少虚假关联，提高可解释性，为多元时间序列因果发现提供了更稳健的方法。

Abstract: Multivariate time series in domains such as finance, climate science, and healthcare often exhibit long-term trends, seasonal patterns, and short-term fluctuations, complicating causal inference under non-stationarity and autocorrelation. Existing causal discovery methods typically operate on raw observations, making them vulnerable to spurious edges and misattributed temporal dependencies. We introduce a decomposition-based causal discovery framework that separates each time series into trend, seasonal, and residual components and performs component-specific causal analysis. Trend components are assessed using stationarity tests, seasonal components using kernel-based dependence measures, and residual components using constraint-based causal discovery. The resulting component-level graphs are integrated into a unified multi-scale causal structure. This approach isolates long- and short-range causal effects, reduces spurious associations, and improves interpretability. Across extensive synthetic benchmarks and real-world climate data, our framework more accurately recovers ground-truth causal structure than state-of-the-art baselines, particularly under strong non-stationarity and temporal autocorrelation.

</details>


### [513] [VoxServe: Streaming-Centric Serving System for Speech Language Models](https://arxiv.org/abs/2602.00269)
*Keisuke Kamahori,Wei-Tzu Lee,Atindra Jha,Rohan Kadekodi,Stephanie Wang,Arvind Krishnamurthy,Baris Kasikci*

Main category: cs.LG

TL;DR: VoxServe是一个用于语音语言模型的统一流式服务系统，通过解耦模型架构与系统优化，实现了高吞吐量和低延迟的流式推理。


<details>
  <summary>Details</summary>
Motivation: 现代语音语言模型在流式场景中需要低延迟、高吞吐量和强流式保证，现有系统无法灵活高效地支持多样化模型架构。

Method: 提出模型执行抽象层解耦模型架构与系统优化；实现流式感知调度和异步推理流水线；支持多种语音语言模型架构的统一框架。

Result: 在多个现代语音语言模型上评估，VoxServe相比现有实现实现了10-20倍的吞吐量提升，在可比延迟下保持高流式可行性。

Conclusion: VoxServe为语音语言模型提供了高效的流式服务解决方案，通过系统级优化显著提升了服务性能，代码已开源。

Abstract: Deploying modern Speech Language Models (SpeechLMs) in streaming settings requires systems that provide low latency, high throughput, and strong guarantees of streamability. Existing systems fall short of supporting diverse models flexibly and efficiently. We present VoxServe, a unified serving system for SpeechLMs that optimizes streaming performance. VoxServe introduces a model-execution abstraction that decouples model architecture from system-level optimizations, thereby enabling support for diverse SpeechLM architectures within a single framework. Building on this abstraction, VoxServe implements streaming-aware scheduling and an asynchronous inference pipeline to improve end-to-end efficiency. Evaluations across multiple modern SpeechLMs show that VoxServe achieves 10-20x higher throughput than existing implementations at comparable latency while maintaining high streaming viability. The code of VoxServe is available at https://github.com/vox-serve/vox-serve.

</details>


### [514] [Theoretical Analysis of Measure Consistency Regularization for Partially Observed Data](https://arxiv.org/abs/2602.01437)
*Yinsong Wang,Shahin Shahrampour*

Main category: cs.LG

TL;DR: 本文从神经网络距离视角，为测量一致性正则化(MCR)提供了理论分析，解释了其在部分可观测性下提升插补质量的原理，并提出基于对偶间隙的早停策略来保持泛化优势。


<details>
  <summary>Details</summary>
Motivation: 尽管测量一致性正则化(MCR)在图像修复、数据插补和半监督学习等应用中取得了经验成功，但其理论基础仍然有限。本文旨在填补这一空白，从理论角度理解MCR为何、何时以及如何在部分可观测性下提升插补质量。

Method: 1. 从神经网络距离视角对MCR进行理论分析，识别出导致其泛化优势的关键项；2. 将分析扩展到不完美训练机制，证明这种优势并非总是保证；3. 提出基于对偶间隙监控的新型训练协议，确定能保持泛化优势的早停点；4. 通过详细实证验证理论主张和停止条件的有效性。

Result: 理论分析揭示了MCR泛化优势的来源，并证明在不完美训练机制下这种优势可能消失。提出的对偶间隙监控早停策略能有效保持MCR的泛化优势。实证结果支持理论主张，并展示了MCR在不同数据源和模型架构下的通用性。

Conclusion: 本文为MCR提供了坚实的理论基础，解释了其在部分可观测性下的工作机制，并提出了实用的训练协议来确保其泛化优势。研究展示了MCR在不同应用场景中的通用性，为处理损坏数据、缺失特征或缺失模态的问题提供了理论指导和实用工具。

Abstract: The problem of corrupted data, missing features, or missing modalities continues to plague the modern machine learning landscape. To address this issue, a class of regularization methods that enforce consistency between imputed and fully observed data has emerged as a promising approach for improving model generalization, particularly in partially observed settings. We refer to this class of methods as Measure Consistency Regularization (MCR). Despite its empirical success in various applications, such as image inpainting, data imputation and semi-supervised learning, a fundamental understanding of the theoretical underpinnings of MCR remains limited. This paper bridges this gap by offering theoretical insights into why, when, and how MCR enhances imputation quality under partial observability, viewed through the lens of neural network distance.
  Our theoretical analysis identifies the term responsible for MCR's generalization advantage and extends to the imperfect training regime, demonstrating that this advantage is not always guaranteed. Guided by these insights, we propose a novel training protocol that monitors the duality gap to determine an early stopping point that preserves the generalization benefit. We then provide detailed empirical evidence to support our theoretical claims and to show the effectiveness and accuracy of our proposed stopping condition. We further provide a set of real-world data simulations to show the versatility of MCR under different model architectures designed for different data sources.

</details>


### [515] [Sample Complexity Analysis for Constrained Bilevel Reinforcement Learning](https://arxiv.org/abs/2602.00282)
*Naman Saxena,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: 本文分析了约束双层强化学习算法的样本复杂度，提出了CBSO算法，获得了O(ε^{-2})的迭代复杂度和Õ(ε^{-4})的样本复杂度，首次使用Moreau包络分析非光滑目标函数的策略梯度RL算法。


<details>
  <summary>Details</summary>
Motivation: 元学习、分层学习和人类反馈强化学习等许多重要RL问题都可以建模为双层RL问题。虽然这些领域在实证方面取得了很大进展，但双层RL算法的理论分析尚未得到足够关注。本文旨在分析约束双层RL算法的样本复杂度。

Method: 提出了约束双层次梯度优化（CBSO）算法，使用基于惩罚的目标函数来避免约束双层问题中的原始-对偶间隙和超梯度问题。采用Moreau包络来分析具有非光滑目标函数的参数化策略梯度RL算法。

Result: CBSO算法获得了O(ε^{-2})的迭代复杂度和Õ(ε^{-4})的样本复杂度。这是首次使用Moreau包络分析具有非光滑目标函数的参数化策略梯度RL算法。

Conclusion: 本文为约束双层RL问题提供了理论分析框架，通过惩罚函数和Moreau包络方法解决了非光滑优化问题，为元学习、分层学习等领域的算法分析奠定了基础。

Abstract: Several important problem settings within the literature of reinforcement learning (RL), such as meta-learning, hierarchical learning, and RL from human feedback (RL-HF), can be modelled as bilevel RL problems. A lot has been achieved in these domains empirically; however, the theoretical analysis of bilevel RL algorithms hasn't received a lot of attention. In this work, we analyse the sample complexity of a constrained bilevel RL algorithm, building on the progress in the unconstrained setting. We obtain an iteration complexity of $O(ε^{-2})$ and sample complexity of $\tilde{O}(ε^{-4})$ for our proposed algorithm, Constrained Bilevel Subgradient Optimization (CBSO). We use a penalty-based objective function to avoid the issue of primal-dual gap and hyper-gradient in the context of a constrained bilevel problem setting. The penalty-based formulation to handle constraints requires analysis of non-smooth optimization. We are the first ones to analyse the generally parameterized policy gradient-based RL algorithm with a non-smooth objective function using the Moreau envelope.

</details>


### [516] [A Statistical Theory of Gated Attention through the Lens of Hierarchical Mixture of Experts](https://arxiv.org/abs/2602.01468)
*Viet Nguyen,Tuan Minh Pham,Thinh Cao,Tan Dinh,Huy Nguyen,Nhat Ho,Alessandro Rinaldo*

Main category: cs.LG

TL;DR: 门控注意力通过门控机制提升Transformer性能，理论分析表明其比多头自注意力具有更高的样本效率，多项式样本即可达到指数级样本的估计精度。


<details>
  <summary>Details</summary>
Motivation: 尽管门控注意力在实证中表现出色（增强低秩映射表达能力、消除注意力汇聚现象），但其理论优势尚未得到充分理解，需要填补这一理论空白。

Method: 将门控注意力和多头自注意力矩阵的每个条目重新表述为分层混合专家模型，将学习问题转化为专家估计问题，从理论上分析样本效率差异。

Result: 理论证明门控注意力仅需多项式数量样本即可估计专家，而多头自注意力需要指数级样本才能达到相同估计误差，为门控机制在输出位置放置提供了理论依据。

Conclusion: 门控注意力在理论上比多头自注意力具有更高的样本效率，这解释了其在实际应用中的性能优势，并为门控机制的最佳放置位置提供了理论指导。

Abstract: Self-attention has greatly contributed to the success of the widely used Transformer architecture by enabling learning from data with long-range dependencies. In an effort to improve performance, a gated attention model that leverages a gating mechanism within the multi-head self-attention has recently been proposed as a promising alternative. Gated attention has been empirically demonstrated to increase the expressiveness of low-rank mapping in standard attention and even to eliminate the attention sink phenomenon. Despite its efficacy, a clear theoretical understanding of gated attention's benefits remains lacking in the literature. To close this gap, we rigorously show that each entry in a gated attention matrix or a multi-head self-attention matrix can be written as a hierarchical mixture of experts. By recasting learning as an expert estimation problem, we demonstrate that gated attention is more sample-efficient than multi-head self-attention. In particular, while the former needs only a polynomial number of data points to estimate an expert, the latter requires exponentially many data points to achieve the same estimation error. Furthermore, our analysis also provides a theoretical justification for why gated attention yields higher performance when a gate is placed at the output of the scaled dot product attention or the value map rather than at other positions in the multi-head self-attention architecture.

</details>


### [517] [Generation Order and Parallel Decoding in Masked Diffusion Models: An Information-Theoretic Perspective](https://arxiv.org/abs/2602.00286)
*Shaorong Zhang,Longxuan Yu,Rob Brekelmans,Luhan Tang,Salman Asif,Greg Ver Steeg*

Main category: cs.LG

TL;DR: 本文提出了一个统一的信息论框架来分析掩码扩散模型中的两种失败来源：顺序敏感性和并行化偏差，揭示了易先解码的优势、并行解码的内在采样误差以及验证的指数成本。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散模型通过牺牲顺序确定性来加速推理，但生成顺序的理论机制和并行化风险尚未得到充分探索。本文旨在分析掩码扩散模型中两种基本的失败来源：顺序敏感性和并行化偏差。

Method: 提出了一个统一的信息论框架来解耦和分析两种失败来源。通过理论分析得出三个关键见解，并在受控的Block-HMM模型和大规模MDMs（LLaDA）上进行算术推理实验来验证理论框架。

Result: 研究发现：(1) 随着模型误差增加，易先解码（优先处理低熵标记）的优势会放大；(2) 因子化并行解码会引入内在采样误差，可能导致任意大的反向KL散度；(3) 验证可以消除采样误差，但会产生由块内总相关性控制的指数成本。

Conclusion: 本文提供了一个理论框架来分析掩码扩散模型中的生成顺序和并行化风险，揭示了易先解码的优势、并行解码的内在误差以及验证与启发式方法之间的权衡，为理解和改进掩码扩散模型提供了理论基础。

Abstract: Masked Diffusion Models (MDMs) significantly accelerate inference by trading off sequential determinism. However, the theoretical mechanisms governing generation order and the risks inherent in parallelization remain under-explored. In this work, we provide a unified information-theoretic framework to decouple and analyze two fundamental sources of failure: order sensitivity and parallelization bias. Our analysis yields three key insights: (1) The benefits of Easy-First decoding (prioritizing low-entropy tokens) are magnified as model error increases; (2) factorized parallel decoding introduces intrinsic sampling errors that can lead to arbitrary large Reverse KL divergence, capturing "incoherence" failures that standard Forward KL metrics overlook; and (3) while verification can eliminate sampling error, it incurs an exponential cost governed by the total correlation within a block. Conversely, heuristics like remasking, though computationally efficient, cannot guarantee distributional correctness. Experiments on a controlled Block-HMM and large-scale MDMs (LLaDA) for arithmetic reasoning validate our theoretical framework.

</details>


### [518] [Self-Attention at Constant Cost per Token via Symmetry-Aware Taylor Approximation](https://arxiv.org/abs/2602.00294)
*Franz A. Heinsen,Leo Kozachkov*

Main category: cs.LG

TL;DR: 提出一种新型自注意力计算方法，能以固定成本实现任意精度计算，大幅降低Transformer模型的内存和计算需求


<details>
  <summary>Details</summary>
Motivation: 当前Transformer自注意力机制的计算成本随上下文长度线性增长，导致存储、计算和能源需求急剧增加，超出了社会供给能力，需要更高效的解决方案

Method: 通过将传统自注意力公式的泰勒展开分解为对称张量链表达式，利用对称性设计前馈变换，将查询和键映射到最小多项式核特征基坐标，实现固定成本计算

Result: 实现了数量级的内存和计算减少，能以固定成本进行无限制的token生成，显著降低大规模Transformer模型的基础设施和能源需求

Conclusion: 该方法使自注意力能以固定成本高效计算到任意精度，为解决Transformer模型的计算和能源挑战提供了重要突破，所引入的数学技术具有独立的研究价值

Abstract: The most widely used artificial intelligence (AI) models today are Transformers employing self-attention. In its standard form, self-attention incurs costs that increase with context length, driving demand for storage, compute, and energy that is now outstripping society's ability to provide them. To help address this issue, we show that self-attention is efficiently computable to arbitrary precision with constant cost per token, achieving orders-of-magnitude reductions in memory use and computation. We derive our formulation by decomposing the conventional formulation's Taylor expansion into expressions over symmetric chains of tensor products. We exploit their symmetry to obtain feed-forward transformations that efficiently map queries and keys to coordinates in a minimal polynomial-kernel feature basis. Notably, cost is fixed inversely in proportion to head size, enabling application over a greater number of heads per token than otherwise feasible. We implement our formulation and empirically validate its correctness. Our work enables unbounded token generation at modest fixed cost, substantially reducing the infrastructure and energy demands of large-scale Transformer models. The mathematical techniques we introduce are of independent interest.

</details>


### [519] [Predicting and improving test-time scaling laws via reward tail-guided search](https://arxiv.org/abs/2602.01485)
*Muheng Li,Jian Qian,Wenlong Mou*

Main category: cs.LG

TL;DR: 提出基于尾部分布预测的缩放定律引导搜索算法，通过动态计算分配提升大语言模型推理能力，相比传统最佳N策略获得理论保证和实际性能提升。


<details>
  <summary>Details</summary>
Motivation: 测试时缩放是提升大语言模型推理能力的重要途径，但传统的最佳N策略缺乏对N值选择、预算分配和多阶段决策的原则性指导，存在优化空间。现有研究缺乏严格的理论保证。

Method: 通过估计奖励的尾部分布来预测LLM的缩放定律，无需穷举评估。基于此提出缩放定律引导搜索算法，动态分配计算资源以识别和利用具有最高预测潜力的中间状态。

Result: 理论上证明SLG算法相比完美信息预言机实现可忽略的遗憾，达到的期望奖励在使用最佳N策略时需要多项式更大的计算预算。实验验证在不同LLM和奖励模型上，尾部引导分配始终比最佳N策略获得更高奖励。

Conclusion: 提出的尾部引导搜索框架为测试时缩放提供了理论保证和实际有效的优化方法，显著提升了计算预算的利用效率，为大语言模型推理能力的增强提供了新途径。

Abstract: Test-time scaling has emerged as a critical avenue for enhancing the reasoning capabilities of Large Language Models (LLMs). Though the straight-forward ''best-of-$N$'' (BoN) strategy has already demonstrated significant improvements in performance, it lacks principled guidance on the choice of $N$, budget allocation, and multi-stage decision-making, thereby leaving substantial room for optimization. While many works have explored such optimization, rigorous theoretical guarantees remain limited. In this work, we propose new methodologies to predict and improve scaling properties via tail-guided search. By estimating the tail distribution of rewards, our method predicts the scaling law of LLMs without the need for exhaustive evaluations. Leveraging this prediction tool, we introduce Scaling-Law Guided (SLG) Search, a new test-time algorithm that dynamically allocates compute to identify and exploit intermediate states with the highest predicted potential. We theoretically prove that SLG achieves vanishing regret compared to perfect-information oracles, and achieves expected rewards that would otherwise require a polynomially larger compute budget required when using BoN. Empirically, we validate our framework across different LLMs and reward models, confirming that tail-guided allocation consistently achieves higher reward yields than Best-of-$N$ under identical compute budgets. Our code is available at https://github.com/PotatoJnny/Scaling-Law-Guided-search.

</details>


### [520] [From Observations to States: Latent Time Series Forecasting](https://arxiv.org/abs/2602.00297)
*Jie Yang,Yifan Hu,Yuante Li,Kexin Zhang,Kaize Ding,Philip S. Yu*

Main category: cs.LG

TL;DR: 提出LatentTSF新范式，将时间序列预测从观测空间回归转向潜在状态预测，解决"潜在混沌"问题，提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习时间序列预测模型存在"潜在混沌"问题：虽然预测准确，但学到的潜在表示在时间上无序且缺乏连续性。这是因为主流方法在观测空间进行预测，最小化噪声和部分观测数据的逐点误差，导致模型采用捷径而非恢复底层系统动态。

Method: 提出LatentTSF范式，使用自编码器将每个时间步的观测投影到高维潜在状态空间，然后在潜在空间进行预测。该方法通过扩展表示来捕捉底层系统变量并施加更平滑的时间结构。

Result: 理论分析表明，潜在目标函数隐式地最大化预测潜在状态与真实状态及观测之间的互信息。在广泛使用的基准测试上的实验证实，LatentTSF有效缓解了潜在混沌问题，实现了优越的性能。

Conclusion: LatentTSF通过将预测从观测空间转移到潜在状态空间，解决了时间序列预测中的潜在混沌问题，使模型能够专注于学习结构化时间动态，从而获得更好的预测性能。

Abstract: Deep learning has achieved strong performance in Time Series Forecasting (TSF). However, we identify a critical representation paradox, termed Latent Chaos: models with accurate predictions often learn latent representations that are temporally disordered and lack continuity. We attribute this phenomenon to the dominant observation-space forecasting paradigm. Most TSF models minimize point-wise errors on noisy and partially observed data, which encourages shortcut solutions instead of the recovery of underlying system dynamics. To address this issue, we propose Latent Time Series Forecasting (LatentTSF), a novel paradigm that shifts TSF from observation regression to latent state prediction. Specifically, LatentTSF employs an AutoEncoder to project observations at each time step into a higher-dimensional latent state space. This expanded representation aims to capture underlying system variables and impose a smoother temporal structure. Forecasting is then performed entirely in the latent space, allowing the model to focus on learning structured temporal dynamics. Theoretical analysis demonstrates that our proposed latent objectives implicitly maximize mutual information between predicted latent states and ground-truth states and observations. Extensive experiments on widely-used benchmarks confirm that LatentTSF effectively mitigates latent chaos, achieving superior performance. Our code is available in https://github.com/Muyiiiii/LatentTSF.

</details>


### [521] [Optimal Sample Complexity for Single Time-Scale Actor-Critic with Momentum](https://arxiv.org/abs/2602.01505)
*Navdeep Kumar,Tehila Dahan,Lior Cohen,Ananyabrata Barua,Giorgia Ramponi,Kfir Yehuda Levy,Shie Mannor*

Main category: cs.LG

TL;DR: 单时间尺度演员-评论家算法在无限时域折扣MDP中实现O(ε⁻²)样本复杂度，优于之前的O(ε⁻³)，通过STORM方差减少和样本缓冲技术


<details>
  <summary>Details</summary>
Motivation: 现有演员-评论家算法在无限时域折扣马尔可夫决策过程中需要O(ε⁻³)样本才能获得ε最优策略，样本效率有待提高。由于策略演化导致样本来自非平稳分布，传统方差减少方法效果有限。

Method: 结合STORM（随机递归动量）技术减少评论家更新的方差，同时维护最近样本的小缓冲区，从缓冲区均匀采样用于评论家更新。这些机制与现有深度学习架构兼容，只需少量修改。

Result: 实现了O(ε⁻²)的样本复杂度，显著优于现有最佳结果O(ε⁻³)。该方法在保持实际适用性的同时，仅需对现有架构进行微小修改。

Conclusion: 通过STORM方差减少和样本缓冲技术的结合，成功将单时间尺度演员-评论家算法的样本复杂度从O(ε⁻³)提升到O(ε⁻²)，为强化学习中的样本效率问题提供了有效解决方案。

Abstract: We establish an optimal sample complexity of $O(ε^{-2})$ for obtaining an $ε$-optimal global policy using a single-timescale actor-critic (AC) algorithm in infinite-horizon discounted Markov decision processes (MDPs) with finite state-action spaces, improving upon the prior state of the art of $O(ε^{-3})$. Our approach applies STORM (STOchastic Recursive Momentum) to reduce variance in the critic updates. However, because samples are drawn from a nonstationary occupancy measure induced by the evolving policy, variance reduction via STORM alone is insufficient. To address this challenge, we maintain a buffer of small fraction of recent samples and uniformly sample from it for each critic update. Importantly, these mechanisms are compatible with existing deep learning architectures and require only minor modifications, without compromising practical applicability.

</details>


### [522] [Agentic Framework for Epidemiological Modeling](https://arxiv.org/abs/2602.00299)
*Rituparna Datta,Zihan Guan,Baltazar Espinoza,Yiqi Su,Priya Pitre,Srini Venkatramanan,Naren Ramakrishnan,Anil Vullikanti*

Main category: cs.LG

TL;DR: EPIAGENT是一个自动合成、校准、验证和优化流行病学模拟器的智能框架，通过将疾病进展建模为迭代程序合成问题，显著加速有效模型的收敛


<details>
  <summary>Details</summary>
Motivation: 传统流行病建模方法依赖固定的模型类别，需要随着病原体、政策和场景假设的变化而手动重新设计，缺乏灵活性和自动化能力

Method: 采用智能框架，将疾病进展建模为迭代程序合成问题，使用流行病学流程图中间表示连接场景规范与模型结构，支持模块化正确性检查，然后将验证的流程图编译成支持可解释参数学习的机制模型

Result: 评估显示EPIAGENT能够捕捉复杂的增长动态，在不同疫苗接种和免疫逃逸假设下产生流行病学一致的反事实预测，智能反馈循环防止退化并显著加速向有效模型的收敛

Conclusion: EPIAGENT通过模仿专业专家工作流程，为流行病建模提供了自动化的智能框架，能够适应不断变化的场景需求并加速有效模型的开发

Abstract: Epidemic modeling is essential for public health planning, yet traditional approaches rely on fixed model classes that require manual redesign as pathogens, policies, and scenario assumptions evolve. We introduce EPIAGENT, an agentic framework that automatically synthesizes, calibrates, verifies, and refines epidemiological simulators by modeling disease progression as an iterative program synthesis problem. A central design choice is an explicit epidemiological flow graph intermediate representation that links scenario specifications to model structure and enables strong, modular correctness checks before code is generated. Verified flow graphs are then compiled into mechanistic models supporting interpretable parameter learning under physical and epidemiological constraints. Evaluation on epidemiological scenario case studies demonstrates that EPIAGENT captures complex growth dynamics and produces epidemiologically consistent counterfactual projections across varying vaccination and immune escape assumptions. Our results show that the agentic feedback loop prevents degeneration and significantly accelerates convergence toward valid models by mimicking professional expert workflows.

</details>


### [523] [Universal Redundancies in Time Series Foundation Models](https://arxiv.org/abs/2602.01605)
*Anthony Bao,Venkata Hasith Vattikuti,Jeffrey Lai,William Gilpin*

Main category: cs.LG

TL;DR: 研究发现时间序列基础模型存在冗余组件，通过机制可解释性工具揭示了模型对整层剪枝具有鲁棒性，并识别出导致退化现象（如模式重复和季节性偏差）的具体注意力头。


<details>
  <summary>Details</summary>
Motivation: 时间序列基础模型通过大规模预训练实现零样本预测，但现有研究对其内部工作机制了解有限。作者旨在通过机制可解释性方法揭示这些模型的冗余性和内部工作机制。

Method: 开发了一套机制可解释性工具，包括组件剪枝和残差流上的直接logit归因。提出了基于核回归器的理论框架，通过投影矩阵稳定秩来剪枝注意力头的内在策略。

Result: 发现所有研究模型对整层剪枝具有鲁棒性。识别出导致时间序列基础模型中常见退化现象（如上下文模式重复和季节性偏差）的具体注意力头。

Conclusion: 研究揭示了时间序列基础模型架构的普遍特性，为理解这一新兴连续时间序列建模架构类别提供了新的视角和工具。

Abstract: Time Series Foundation Models (TSFMs) leverage extensive pretraining to accurately predict unseen time series during inference, without the need for task-specific fine-tuning. Through large-scale evaluations on standard benchmarks, we find that leading transformer-based TSFMs exhibit redundant components in their intermediate layers. We introduce a set of tools for mechanistic interpretability of TSFMs, including ablations of specific components and direct logit attribution on the residual stream. Our findings are consistent across several leading TSFMs with diverse architectures, and across a diverse set of real-world and synthetic time-series datasets. We discover that all models in our study are robust to ablations of entire layers. Furthermore, we develop a theoretical framework framing transformers as kernel regressors, motivating a purely intrinsic strategy for ablating heads based on the stable rank of the per-head projection matrices. Using this approach, we uncover the specific heads responsible for degenerate phenomena widely observed in TSFMs, such as parroting of motifs from the context and seasonality bias. Our study sheds light on the universal properties of this emerging class of architectures for continuous-time sequence modeling.

</details>


### [524] [Neural Ising Machines via Unrolling and Zeroth-Order Training](https://arxiv.org/abs/2602.00302)
*Sam Reifenstein,Timothee Leleu*

Main category: cs.LG

TL;DR: 提出NPIM：一种数据驱动的启发式方法，通过神经网络参数化的迭代动力学系统解决NP难的Ising和Max-Cut优化问题，学习节点级更新规则，在保持参数精简的同时实现竞争性性能。


<details>
  <summary>Details</summary>
Motivation: 针对NP难的Ising和Max-Cut优化问题，需要开发高效的数据驱动启发式方法。传统基于梯度的训练方法在处理长循环Ising机动力学时存在梯度不稳定和信息不足的问题，因此需要更稳定的训练方法。

Method: 提出神经网络参数化Ising机（NPIM），学习共享的节点级更新规则，将局部交互场映射到自旋更新。使用紧凑的多层感知机参数化，参数数量少。采用零阶优化器进行训练，避免传统反向传播在长循环动力学中的不稳定梯度问题。

Result: 尽管参数数量少，学习到的动力学恢复了有效的算法结构，包括动量式行为和时间变化调度，能够在高度非凸能量景观中进行高效搜索。在标准Ising和神经组合优化基准测试中，NPIM相对于最近的学习方法和经典Ising机启发式方法，在解质量和求解时间方面具有竞争力。

Conclusion: NPIM展示了通过数据驱动方法学习优化算法动力学的可行性，即使使用精简参数也能捕捉复杂算法行为，为组合优化问题提供了有效的神经网络参数化解决方案。

Abstract: We propose a data-driven heuristic for NP-hard Ising and Max-Cut optimization that learns the update rule of an iterative dynamical system. The method learns a shared, node-wise update rule that maps local interaction fields to spin updates, parameterized by a compact multilayer perceptron with a small number of parameters. Training is performed using a zeroth-order optimizer, since backpropagation through long, recurrent Ising-machine dynamics leads to unstable and poorly informative gradients. We call this approach a neural network parameterized Ising machine (NPIM). Despite its low parameter count, the learned dynamics recover effective algorithmic structure, including momentum-like behavior and time-varying schedules, enabling efficient search in highly non-convex energy landscapes. Across standard Ising and neural combinatorial optimization benchmarks, NPIM achieves competitive solution quality and time-to-solution relative to recent learning-based methods and strong classical Ising-machine heuristics.

</details>


### [525] [Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors](https://arxiv.org/abs/2602.00315)
*Arian Khorasani,Nathaniel Chen,Yug D Oswal,Akshat Santhana Gopalan,Egemen Kolemen,Ravid Shwartz-Ziv*

Main category: cs.LG

TL;DR: 使用类别条件归一化流作为oracle，在真实图像上实现精确后验计算，从五个方面研究神经网络性能极限：缩放定律、学习极限、软标签、分布偏移和主动学习。


<details>
  <summary>Details</summary>
Motivation: 标准基准无法评估神经网络性能极限，因为它们无法访问真实后验分布p(y|x)。需要一种方法来精确计算真实后验，从而深入分析神经网络的性能边界。

Method: 使用类别条件归一化流作为oracle，在AFHQ和ImageNet等真实图像数据集上实现精确后验计算。通过这个框架进行五个方面的研究：分析缩放定律、测量学习极限、使用软标签训练、评估分布偏移、改进主动学习。

Result: 1) 预测误差可分解为不可约的偶然不确定性和可约的认知误差；认知误差随数据集大小呈幂律下降。2) 不同架构接近偶然不确定性底限的方式不同：ResNets呈现干净的幂律缩放，而Vision Transformers在低数据区域停滞。3) 使用精确后验作为软标签训练优于硬标签，实现近乎完美的校准。4) 分布偏移类型比幅度更重要：类别不平衡在KL散度值下几乎不影响准确率，而输入噪声会导致灾难性性能下降。5) 精确认知不确定性可区分信息丰富样本和固有模糊样本，提高主动学习效率。

Conclusion: 该框架揭示了标准指标隐藏了持续学习过程、掩盖了架构差异，并且无法诊断分布偏移的性质。通过精确后验计算，可以更深入地理解神经网络的性能极限和学习动态。

Abstract: How close are neural networks to the best they could possibly do? Standard benchmarks cannot answer this because they lack access to the true posterior p(y|x). We use class-conditional normalizing flows as oracles that make exact posteriors tractable on realistic images (AFHQ, ImageNet). This enables five lines of investigation. Scaling laws: Prediction error decomposes into irreducible aleatoric uncertainty and reducible epistemic error; the epistemic component follows a power law in dataset size, continuing to shrink even when total loss plateaus. Limits of learning: The aleatoric floor is exactly measurable, and architectures differ markedly in how they approach it: ResNets exhibit clean power-law scaling while Vision Transformers stall in low-data regimes. Soft labels: Oracle posteriors contain learnable structure beyond class labels: training with exact posteriors outperforms hard labels and yields near-perfect calibration. Distribution shift: The oracle computes exact KL divergence of controlled perturbations, revealing that shift type matters more than shift magnitude: class imbalance barely affects accuracy at divergence values where input noise causes catastrophic degradation. Active learning: Exact epistemic uncertainty distinguishes genuinely informative samples from inherently ambiguous ones, improving sample efficiency. Our framework reveals that standard metrics hide ongoing learning, mask architectural differences, and cannot diagnose the nature of distribution shift.

</details>


### [526] [Finite and Corruption-Robust Regret Bounds in Online Inverse Linear Optimization under M-Convex Action Sets](https://arxiv.org/abs/2602.01682)
*Taihei Oki,Shinsaku Sakaue*

Main category: cs.LG

TL;DR: 该论文研究了在线逆线性优化问题，证明了当可行集是M-凸集时，可以获得O(d log d)的有限遗憾界，解决了该领域的一个开放问题。


<details>
  <summary>Details</summary>
Motivation: 在线逆线性优化（也称为上下文推荐）中，学习者需要从随时间变化的可行集中观察最优动作来推断代理的隐藏目标向量。先前研究已建立了O(d log T)的遗憾界和指数级有限遗憾界exp(O(d log d))，但是否存在多项式有限遗憾界一直是个开放问题。

Method: 结合M-凸集上最优解的结构特征与几何体积论证，当可行集是M-凸集（包括拟阵）时，实现了有限遗憾界。还扩展到对抗性反馈，通过监测观察反馈诱导的有向图来检测腐败，无需事先知道腐败轮数C。

Result: 对于M-凸集，获得了O(d log d)的有限遗憾界。对于最多C轮对抗性腐败的反馈，获得了O((C+1)d log d)的遗憾界，且无需事先知道C。

Conclusion: 该工作部分解决了在线逆线性优化中是否存在多项式有限遗憾界的开放问题，证明了对于M-凸集这一广泛类别，可以获得O(d log d)的有限遗憾界，并将方法扩展到对抗性腐败反馈场景。

Abstract: We study online inverse linear optimization, also known as contextual recommendation, where a learner sequentially infers an agent's hidden objective vector from observed optimal actions over feasible sets that change over time. The learner aims to recommend actions that perform well under the agent's true objective, and the performance is measured by the regret, defined as the cumulative gap between the agent's optimal values and those achieved by the learner's recommended actions. Prior work has established a regret bound of $O(d\log T)$, as well as a finite but exponentially large bound of $\exp(O(d\log d))$, where $d$ is the dimension of the optimization problem and $T$ is the time horizon, while a regret lower bound of $Ω(d)$ is known (Gollapudi et al. 2021; Sakaue et al. 2025). Whether a finite regret bound polynomial in $d$ is achievable or not has remained an open question. We partially resolve this by showing that when the feasible sets are M-convex -- a broad class that includes matroids -- a finite regret bound of $O(d\log d)$ is possible. We achieve this by combining a structural characterization of optimal solutions on M-convex sets with a geometric volume argument. Moreover, we extend our approach to adversarially corrupted feedback in up to $C$ rounds. We obtain a regret bound of $O((C+1)d\log d)$ without prior knowledge of $C$, by monitoring directed graphs induced by the observed feedback to detect corruptions adaptively.

</details>


### [527] [Optimal Transport-Guided Adversarial Attacks on Graph Neural Network-Based Bot Detection](https://arxiv.org/abs/2602.00318)
*Kunal Mukherjee,Zulfikar Alom,Tran Gia Bao Ngo,Cuneyt Gurcan Akcora,Murat Kantarcioglu*

Main category: cs.LG

TL;DR: BOCLOAK：一种基于最优传输的轻量级对抗攻击框架，用于在现实约束下评估GNN社交机器人检测器的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 当前GNN机器人检测器在现实场景中的鲁棒性评估不足，现有攻击方法难以应对现实中的领域特定和时间约束，需要开发能在真实约束下评估检测器鲁棒性的方法

Method: BOCLOAK构建时空邻居特征的概率度量，学习分离人类和机器人行为的最优传输几何，将传输计划解码为稀疏、合理的边编辑，在遵守现实约束的同时逃避检测

Result: 在三个社交机器人数据集、五个SOTA检测器、三个对抗防御和四个基线攻击方法上评估，BOCLOAK攻击成功率提升高达80.13%，GPU内存使用减少99.80%

Conclusion: 最优传输为对抗攻击与现实机器人检测之间的差距提供了轻量级、有原则的框架，BOCLOAK在现实约束下有效评估了GNN检测器的鲁棒性

Abstract: The rise of bot accounts on social media poses significant risks to public discourse. To address this threat, modern bot detectors increasingly rely on Graph Neural Networks (GNNs). However, the effectiveness of these GNN-based detectors in real-world settings remains poorly understood. In practice, attackers continuously adapt their strategies as well as must operate under domain-specific and temporal constraints, which can fundamentally limit the applicability of existing attack methods. As a result, there is a critical need for robust GNN-based bot detection methods under realistic, constraint-aware attack scenarios.
  To address this gap, we introduce BOCLOAK to systematically evaluate the robustness of GNN-based social bot detection via both edge editing and node injection adversarial attacks under realistic constraints. BOCLOAK constructs a probability measure over spatio-temporal neighbor features and learns an optimal transport geometry that separates human and bot behaviors. It then decodes transport plans into sparse, plausible edge edits that evade detection while obeying real-world constraints. We evaluate BOCLOAK across three social bot datasets, five state-of-the-art bot detectors, three adversarial defenses, and compare it against four leading graph adversarial attack baselines. BOCLOAK achieves up to 80.13% higher attack success rates while using 99.80% less GPU memory under realistic real-world constraints. Most importantly, BOCLOAK shows that optimal transport provides a lightweight, principled framework for bridging the gap between adversarial attacks and real-world bot detection.

</details>


### [528] [Stein-Rule Shrinkage for Stochastic Gradient Estimation in High Dimensions](https://arxiv.org/abs/2602.01777)
*M. Arashi,M. Amintoosi*

Main category: cs.LG

TL;DR: 提出基于Stein规则收缩的随机梯度估计器，通过收缩噪声小批量梯度向动量稳定估计器，在平方误差损失下优于标准随机梯度，并集成到Adam优化器中提升大批次训练性能。


<details>
  <summary>Details</summary>
Motivation: 传统随机梯度方法将小批量梯度视为无偏估计，但统计决策理论表明在高维设置下无偏估计在二次损失下通常不可接受，说明标准随机梯度从风险角度可能是次优的。

Method: 将随机梯度计算建模为高维估计问题，基于Stein规则收缩构建收缩梯度估计器，自适应地将噪声小批量梯度收缩到历史动量推导的稳定受限估计器，收缩强度通过在线估计梯度噪声方差数据驱动确定。

Result: 在高斯噪声模型和维度p>=3条件下，提出的估计器在平方误差损失下一致优于标准随机梯度，且是极小极大最优的。在CIFAR10和CIFAR100上，在多种标签噪声水平下，在大批次训练中持续优于Adam。

Conclusion: 经典收缩原则为改进现代深度学习中的随机梯度估计提供了原则性且有效的方法，通过选择性地对高维卷积层应用收缩可获得性能提升，而跨所有参数的无差别收缩会降低性能。

Abstract: Stochastic gradient methods are central to large-scale learning, yet their analysis typically treats mini-batch gradients as unbiased estimators of the population gradient. In high-dimensional settings, however, classical results from statistical decision theory show that unbiased estimators are generally inadmissible under quadratic loss, suggesting that standard stochastic gradients may be suboptimal from a risk perspective. In this work, we formulate stochastic gradient computation as a high-dimensional estimation problem and introduce a decision-theoretic framework based on Stein-rule shrinkage. We construct a shrinkage gradient estimator that adaptively contracts noisy mini-batch gradients toward a stable restricted estimator derived from historical momentum. The shrinkage intensity is determined in a data-driven manner using an online estimate of gradient noise variance, leveraging second-moment statistics commonly maintained by adaptive optimization methods. Under a Gaussian noise model and for dimension p>=3, we show that the proposed estimator uniformly dominates the standard stochastic gradient under squared error loss and is minimax-optimal in the classical decision-theoretic sense. We further demonstrate how this estimator can be incorporated into the Adam optimizer, yielding a practical algorithm with negligible additional computational cost. Empirical evaluations on CIFAR10 and CIFAR100, across multiple levels of label noise, show consistent improvements over Adam in the large-batch regime. Ablation studies indicate that the gains arise primarily from selectively applying shrinkage to high-dimensional convolutional layers, while indiscriminate shrinkage across all parameters degrades performance. These results illustrate that classical shrinkage principles provide a principled and effective approach to improving stochastic gradient estimation in modern deep learning.

</details>


### [529] [Harvest: Opportunistic Peer-to-Peer GPU Caching for LLM Inference](https://arxiv.org/abs/2602.00328)
*Nikhil Gopal,Kostis Kaffes*

Main category: cs.LG

TL;DR: Harvest框架利用GPU间高速互连，将模型权重和KV缓存动态放置到空闲GPU内存中，作为临时缓存层，显著提升推理吞吐量。


<details>
  <summary>Details</summary>
Motivation: LLM推理越来越受限于GPU内存容量而非计算吞吐量，现有方法将模型状态和KV缓存卸载到主机内存会因PCIe带宽限制导致显著延迟。

Method: Harvest框架利用GPU间高速对等互连，将模型权重和KV缓存动态放置在空闲GPU内存中，作为临时缓存层，在保持正确性的同时减少数据移动开销。

Result: 通过加速专家层权重和KV缓存条目的检索，Harvest实现了超过2倍的吞吐量提升。

Conclusion: Harvest通过利用空闲GPU内存作为缓存层，有效解决了LLM推理中的内存瓶颈问题，显著提升了推理性能。

Abstract: Large Language Model (LLM) inference is increasingly constrained by GPU memory capacity rather than compute throughput, driven by growing model sizes and the linear growth of the key-value (KV) cache during autoregressive decoding. Existing approaches mitigate memory pressure by offloading model state and KV tensors to host memory, but incur substantial latency due to limited PCIe bandwidth. We present Harvest, an opportunistic GPU cache management framework that exploits high-bandwidth peer-to-peer GPU interconnects to dynamically place model weights and KV cache in unused GPU memory. Harvest treats peer GPU memory as a transient cache tier, preserving correctness while reducing data movement overhead under dynamic memory availability. We demonstrate significant throughput speedup of more than 2 times by using Harvest to accelerate the retrieval of two widely-used inference components: expert layer weights and KV cache entries.

</details>


### [530] [Designing Time Series Experiments in A/B Testing with Transformer Reinforcement Learning](https://arxiv.org/abs/2602.01853)
*Xiangkun Wu,Qianglin Wen,Yingying Zhang,Hongtu Zhu,Ting Li,Chengchun Shi*

Main category: cs.LG

TL;DR: 提出基于Transformer强化学习的时间序列A/B测试设计方法，通过利用完整历史信息和直接优化MSE，显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 时间序列实验中的A/B测试面临两个主要挑战：现有方法未能充分利用完整历史信息进行干预分配，且依赖强假设来近似目标函数（如处理效应估计的均方误差）

Method: 提出Transformer强化学习方法，利用Transformer模型基于完整历史信息进行干预分配，并采用强化学习直接优化均方误差，避免依赖限制性假设

Result: 在合成数据、公开调度模拟器和真实世界网约车数据集上的实验表明，该方法始终优于现有设计

Conclusion: 通过结合Transformer的条件化能力和强化学习的直接优化，成功解决了时间序列A/B测试中的历史信息利用和目标函数优化问题

Abstract: A/B testing has become a gold standard for modern technological companies to conduct policy evaluation. Yet, its application to time series experiments, where policies are sequentially assigned over time, remains challenging. Existing designs suffer from two limitations: (i) they do not fully leverage the entire history for treatment allocation; (ii) they rely on strong assumptions to approximate the objective function (e.g., the mean squared error of the estimated treatment effect) for optimizing the design. We first establish an impossibility theorem showing that failure to condition on the full history leads to suboptimal designs, due to the dynamic dependencies in time series experiments. To address both limitations simultaneously, we next propose a transformer reinforcement learning (RL) approach which leverages transformers to condition allocation on the entire history and employs RL to directly optimize the MSE without relying on restrictive assumptions. Empirical evaluations on synthetic data, a publicly available dispatch simulator, and a real-world ridesharing dataset demonstrate that our proposal consistently outperforms existing designs.

</details>


### [531] [In-Run Data Shapley for Adam Optimizer](https://arxiv.org/abs/2602.00329)
*Meng Ding,Zeqing Zhang,Di Wang,Lijie Hu*

Main category: cs.LG

TL;DR: 本文提出Adam感知的In-Run数据Shapley方法，解决传统SGD-based数据归因方法在Adam优化器下失效的问题，通过固定状态假设和线性化幽灵近似实现高效准确的数据贡献评估。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习中可靠的数据归因对于减少偏见和计算浪费至关重要，但现有的"In-Run"方法严重依赖SGD的线性结构，无法捕捉Adam等自适应优化器的复杂动态，导致在Adam训练管道中数据归因失效。

Method: 提出Adam-Aware In-Run Data Shapley方法：1) 通过固定状态假设重新定义效用函数，恢复可加性；2) 提出线性化幽灵近似技术，线性化方差依赖的缩放项，无需计算每样本梯度即可计算梯度点积对。

Result: 实验表明：1) 该方法与真实边际贡献的Pearson相关系数R > 0.99，接近完美保真度；2) 保持约95%的标准训练吞吐量；3) 在数据归因下游任务中显著优于SGD-based基线方法。

Conclusion: 数据归因本质上是优化器依赖的，本文提出的Adam感知方法成功解决了SGD-based代理在Adam优化器下失效的问题，为现代训练管道提供了高效准确的数据归因解决方案。

Abstract: Reliable data attribution is essential for mitigating bias and reducing computational waste in modern machine learning, with the Shapley value serving as the theoretical gold standard. While recent "In-Run" methods bypass the prohibitive cost of retraining by estimating contributions dynamically, they heavily rely on the linear structure of Stochastic Gradient Descent (SGD) and fail to capture the complex dynamics of adaptive optimizers like Adam. In this work, we demonstrate that data attribution is inherently optimizer-dependent: we show that SGD-based proxies diverge significantly from true contributions under Adam (Pearson $R \approx 0.11$), rendering them ineffective for modern training pipelines. To bridge this gap, we propose Adam-Aware In-Run Data Shapley. We derive a closed-form approximation that restores additivity by redefining utility under a fixed-state assumption and enable scalable computation via a novel Linearized Ghost Approximation. This technique linearizes the variance-dependent scaling term, allowing us to compute pairwise gradient dot-products without materializing per-sample gradients. Extensive experiments show that our method achieves near-perfect fidelity to ground-truth marginal contributions ($R > 0.99$) while retaining $\sim$95\% of standard training throughput. Furthermore, our Adam-aware attribution significantly outperforms SGD-based baselines in data attribution downstream tasks.

</details>


### [532] [Observation-dependent Bayesian active learning via input-warped Gaussian processes](https://arxiv.org/abs/2602.01898)
*Sanna Jarl,Maria Bånkestad,Jonathan J. S. Scragg,Jens Sjölund*

Main category: cs.LG

TL;DR: 提出一种通过单调重参数化扭曲输入空间的方法，使贝叶斯主动学习中的探索策略能够根据观测反馈调整，改进传统高斯过程对非平稳函数的采样效率


<details>
  <summary>Details</summary>
Motivation: 传统高斯过程主动学习中，后验方差仅通过超参数依赖于观测输出，导致探索策略对实际测量值不敏感，难以有效处理非平稳函数

Method: 通过学习的单调重参数化扭曲输入空间，使设计策略能够根据观测变异性扩展或压缩输入区域，从而塑造基于方差的获取函数行为；提出自监督训练目标优于传统的边缘似然方法

Result: 该方法在多个主动学习基准测试中显著提高了采样效率，特别是在传统方法难以处理的非平稳区域表现突出

Conclusion: 通过输入空间扭曲注入观测依赖反馈，有效解决了高斯过程主动学习中探索策略对测量不敏感的问题，为处理非平稳函数提供了有效方案

Abstract: Bayesian active learning relies on the precise quantification of predictive uncertainty to explore unknown function landscapes. While Gaussian process surrogates are the standard for such tasks, an underappreciated fact is that their posterior variance depends on the observed outputs only through the hyperparameters, rendering exploration largely insensitive to the actual measurements. We propose to inject observation-dependent feedback by warping the input space with a learned, monotone reparameterization. This mechanism allows the design policy to expand or compress regions of the input space in response to observed variability, thereby shaping the behavior of variance-based acquisition functions. We demonstrate that while such warps can be trained via marginal likelihood, a novel self-supervised objective yields substantially better performance. Our approach improves sample efficiency across a range of active learning benchmarks, particularly in regimes where non-stationarity challenges traditional methods.

</details>


### [533] [Prototype-based Explainable Neural Networks with Channel-specific Reasoning for Geospatial Learning Tasks](https://arxiv.org/abs/2602.00331)
*Anushka Narayanan,Karianne J. Bergen*

Main category: cs.LG

TL;DR: 开发针对多通道地理空间数据的原型解释AI方法，通过通道特定原型增强地学机器学习模型的可解释性


<details>
  <summary>Details</summary>
Motivation: 现有基于原型的可解释AI方法主要针对标准RGB图像设计，无法有效处理地学图像和栅格数据中常见的多变量通道结构，需要专门针对地理空间数据特点的XAI方法

Method: 开发针对多通道地理空间数据的原型XAI方法，每个通道代表不同的物理环境变量或光谱通道，模型学习通道特定的原型特征，从多个训练样本中提取特征，同时保持与标准神经网络相当的性能

Result: 在两个地学案例研究中验证方法有效性：(1)使用多变量气候数据进行Madden Julian Oscillation相位分类，(2)使用多光谱卫星图像进行土地利用分类，方法能生成局部和全局解释

Conclusion: 通过将通道原型明确纳入预测过程，该方法增强了地学机器学习任务的透明度和可信度，为多通道地理空间数据提供了更有效的可解释AI解决方案

Abstract: Explainable AI (XAI) is essential for understanding machine learning (ML) decision-making and ensuring model trustworthiness in scientific applications. Prototype-based XAI methods offer an intrinsically interpretable alternative to post-hoc approaches which often yield inconsistent explanations. Prototype-based XAI methods make predictions based on the similarity between inputs and learned prototypes that represent typical characteristics of target classes. However, existing prototype-based models are primarily designed for standard RGB image data and are not optimized for the distinct, variable-specific channels commonly found in geoscientific image and raster datasets. In this study, we develop a prototype-based XAI approach tailored for multi-channel geospatial data, where each channel represents a distinct physical environmental variable or spectral channel. Our approach enables the model to identify separate, channel-specific prototypical characteristics sourced from multiple distinct training examples that inform how these features individually and in combination influence model prediction while achieving comparable performance to standard neural networks. We demonstrate this method through two geoscientific case studies: (1) classification of Madden Julian Oscillation phases using multi-variable climate data and (2) land-use classification from multispectral satellite imagery. This approach produces both local (instance-level) and global (model-level) explanations for providing insights into feature-relevance across channels. By explicitly incorporating channel-prototypes into the prediction process, we discuss how this approach enhances the transparency and trustworthiness of ML models for geoscientific learning tasks.

</details>


### [534] [Data- and Variance-dependent Regret Bounds for Online Tabular MDPs](https://arxiv.org/abs/2602.01903)
*Mingyi Li,Taira Tsuchiya,Kenji Yamanishi*

Main category: cs.LG

TL;DR: 本文研究在线表格MDP，开发了在对抗性和随机性环境下都能获得最优性能的算法，实现了数据依赖和方差依赖的遗憾界。


<details>
  <summary>Details</summary>
Motivation: 现有MDP算法通常在对抗性或随机性环境下单独优化，缺乏统一的框架。本文旨在开发"两全其美"的算法，在两种环境下都能获得最优性能，同时引入新的复杂性度量来量化MDP的难度。

Method: 基于乐观跟随正则化领导者(OFTRL)和对数障碍正则化，开发了两种算法：全局优化和策略优化。全局优化直接优化值函数，策略优化通过新的乐观Q函数估计器实现类似性能。

Result: 全局优化算法在对抗性环境下实现一阶、二阶和路径长度遗憾界，在随机环境下实现方差感知的间隙无关和间隙依赖遗憾界。策略优化算法达到类似性能，最多相差一个时间步长因子。同时建立了匹配的遗憾下界。

Conclusion: 本文开发了在对抗性和随机性MDP中都能获得最优性能的统一算法框架，通过新的复杂性度量实现了精细的数据依赖和方差依赖遗憾界，证明了全局优化方法的遗憾上界接近最优。

Abstract: This work studies online episodic tabular Markov decision processes (MDPs) with known transitions and develops best-of-both-worlds algorithms that achieve refined data-dependent regret bounds in the adversarial regime and variance-dependent regret bounds in the stochastic regime. We quantify MDP complexity using a first-order quantity and several new data-dependent measures for the adversarial regime, including a second-order quantity and a path-length measure, as well as variance-based measures for the stochastic regime. To adapt to these measures, we develop algorithms based on global optimization and policy optimization, both built on optimistic follow-the-regularized-leader with log-barrier regularization. For global optimization, our algorithms achieve first-order, second-order, and path-length regret bounds in the adversarial regime, and in the stochastic regime, they achieve a variance-aware gap-independent bound and a variance-aware gap-dependent bound that is polylogarithmic in the number of episodes. For policy optimization, our algorithms achieve the same data- and variance-dependent adaptivity, up to a factor of the episode horizon, by exploiting a new optimistic $Q$-function estimator. Finally, we establish regret lower bounds in terms of data-dependent complexity measures for the adversarial regime and a variance measure for the stochastic regime, implying that the regret upper bounds achieved by the global-optimization approach are nearly optimal.

</details>


### [535] [Efficient and accurate steering of Large Language Models through attention-guided feature learning](https://arxiv.org/abs/2602.00333)
*Parmida Davarmanesh,Ashia Wilson,Adityanarayanan Radhakrishnan*

Main category: cs.LG

TL;DR: 提出注意力引导的引导框架，解决LLM内部激活引导的三个核心挑战，在512个语义概念基准上性能显著提升


<details>
  <summary>Details</summary>
Motivation: 现有的LLM内部激活引导方法非常脆弱，对概念相关特征的提取方式极其敏感，需要更鲁棒的引导框架来理解LLM中语义概念的存储方式并提升LLM能力

Method: 提出注意力引导的引导框架，解决三个核心挑战：1) 自动选择相关token嵌入提取概念特征；2) 考虑概念特征在LLM激活中的异质性；3) 识别最相关的引导层

Result: 在512个语义概念的引导基准测试中，框架性能显著优于现有方法（成功引导的概念数量几乎翻倍），适用于不同架构和大小的模型（最高达700亿参数）

Conclusion: 该框架为开发高效、高度可扩展的行业级LLM微调算法开辟了新途径，并揭示了概念特定特征在LLM层间的分布规律

Abstract: Steering, or direct manipulation of internal activations to guide LLM responses toward specific semantic concepts, is emerging as a promising avenue for both understanding how semantic concepts are stored within LLMs and advancing LLM capabilities. Yet, existing steering methods are remarkably brittle, with seemingly non-steerable concepts becoming completely steerable based on subtle algorithmic choices in how concept-related features are extracted. In this work, we introduce an attention-guided steering framework that overcomes three core challenges associated with steering: (1) automatic selection of relevant token embeddings for extracting concept-related features; (2) accounting for heterogeneity of concept-related features across LLM activations; and (3) identification of layers most relevant for steering. Across a steering benchmark of 512 semantic concepts, our framework substantially improved steering over previous state-of-the-art (nearly doubling the number of successfully steered concepts) across model architectures and sizes (up to 70 billion parameter models). Furthermore, we use our framework to shed light on the distribution of concept-specific features across LLM layers. Overall, our framework opens further avenues for developing efficient, highly-scalable fine-tuning algorithms for industry-scale LLMs.

</details>


### [536] [Deep Multivariate Models with Parametric Conditionals](https://arxiv.org/abs/2602.01953)
*Dmitrij Schlesinger,Boris Flach,Alexander Shekhovtsov*

Main category: cs.LG

TL;DR: 提出一种基于条件概率分布的深度多元模型，通过训练参数化马尔可夫链核来学习联合概率分布，适用于多种下游任务和半监督学习场景。


<details>
  <summary>Details</summary>
Motivation: 现有深度多元模型通常针对特定应用任务设计，限制了在其他下游任务中的适用性。需要一种更通用的建模方法，能够灵活处理异构变量集合（如图像、分割、属性、隐变量等）。

Method: 通过每个变量组相对于其他变量的条件概率分布来表示联合概率分布。将学习问题转化为训练参数化马尔可夫链核，通过最大化其极限分布的数据似然来进行学习。

Result: 该方法能够构建适用于几乎所有可能下游任务的模型，同时支持广泛的半监督学习场景，提高了模型的通用性和灵活性。

Conclusion: 提出的条件概率分布表示方法为深度多元建模提供了通用框架，通过马尔可夫链核训练策略实现了灵活的下游任务适应性和半监督学习能力。

Abstract: We consider deep multivariate models for heterogeneous collections of random variables. In the context of computer vision, such collections may e.g. consist of images, segmentations, image attributes, and latent variables. When developing such models, most existing works start from an application task and design the model components and their dependencies to meet the needs of the chosen task. This has the disadvantage of limiting the applicability of the resulting model for other downstream tasks. Here, instead, we propose to represent the joint probability distribution by means of conditional probability distributions for each group of variables conditioned on the rest. Such models can then be used for practically any possible downstream task. Their learning can be approached as training a parametrised Markov chain kernel by maximising the data likelihood of its limiting distribution. This has the additional advantage of allowing a wide range of semi-supervised learning scenarios.

</details>


### [537] [SNAP: A Self-Consistent Agreement Principle with Application to Robust Computation](https://arxiv.org/abs/2602.02013)
*Xiaoyi Jiang,Andreas Nienkötter*

Main category: cs.LG

TL;DR: SNAP是一个基于相互一致性的自监督鲁棒计算框架，通过一致性-可靠性假设为数据项分配权重，强调可信项目并抑制异常值，无需监督或先验知识。


<details>
  <summary>Details</summary>
Motivation: 现有鲁棒计算方法通常需要监督信息或先验知识，且在高维设置中处理异常值效果有限。需要一种灵活、易于使用、广泛适用的鲁棒计算框架。

Method: 基于一致性-可靠性假设，SNAP通过量化数据项之间的一致性来分配权重。核心是异常值权重指数抑制特性，确保异常值对计算贡献可忽略不计。该方法是非迭代的。

Result: 在向量平均和子空间估计任务上，非迭代的SNAP优于迭代的Weiszfeld算法和两种多元中位数均值变体。异常值权重被指数级抑制，即使在高维设置中也能有效工作。

Conclusion: SNAP提供了一个灵活、易于使用、广泛适用的鲁棒计算框架，通过自监督的一致性原则有效处理异常值，无需监督或先验知识。

Abstract: We introduce SNAP (Self-coNsistent Agreement Principle), a self-supervised framework for robust computation based on mutual agreement. Based on an Agreement-Reliability Hypothesis SNAP assigns weights that quantify agreement, emphasizing trustworthy items and downweighting outliers without supervision or prior knowledge. A key result is the Exponential Suppression of Outlier Weights, ensuring that outliers contribute negligibly to computations, even in high-dimensional settings. We study properties of SNAP weighting scheme and show its practical benefits on vector averaging and subspace estimation. Particularly, we demonstrate that non-iterative SNAP outperforms the iterative Weiszfeld algorithm and two variants of multivariate median of means. SNAP thus provides a flexible, easy-to-use, broadly applicable approach to robust computation.

</details>


### [538] [Planning with Language and Generative Models: Toward General Reward-Guided Wireless Network Design](https://arxiv.org/abs/2602.00357)
*Chenyang Yuan,Xiaoyuan Cheng*

Main category: cs.LG

TL;DR: 该论文提出使用扩散采样器作为生成式推理模型，通过统一的奖励函数指导智能AP部署规划，在复杂室内环境中实现可扩展的优化方案。


<details>
  <summary>Details</summary>
Motivation: 下一代无线网络中，复杂室内几何结构和信号传播使得智能接入点(AP)部署具有挑战性。现有大型语言模型(LLMs)作为优化器虽然具备无线领域知识，但依赖外部验证器导致计算成本高、可扩展性有限。

Method: 1. 首先对通用LLMs作为AP规划优化器进行基准测试；2. 研究基于统一奖励函数的生成式推理模型，该函数捕捉不同楼层平面图中的核心AP部署目标；3. 采用扩散采样器，通过平滑和锐化奖励景观逐步改进采样；4. 引入大规模真实世界室内AP部署数据集，需要超过5万CPU小时训练通用奖励函数。

Result: 扩散采样器在生成式方法中表现最佳，扩散过程通过平滑和锐化奖励景观逐步改进采样，而非依赖迭代细化，对非凸和碎片化目标函数有效。在分布内和分布外泛化及鲁棒性评估中表现良好。

Conclusion: 基于扩散的生成式推理与统一奖励函数相结合，为室内AP部署规划提供了可扩展且领域无关的基础框架。

Abstract: Intelligent access point (AP) deployment remains challenging in next-generation wireless networks due to complex indoor geometries and signal propagation. We firstly benchmark general-purpose large language models (LLMs) as agentic optimizers for AP planning and find that, despite strong wireless domain knowledge, their dependence on external verifiers results in high computational costs and limited scalability. Motivated by these limitations, we study generative inference models guided by a unified reward function capturing core AP deployment objectives across diverse floorplans. We show that diffusion samplers consistently outperform alternative generative approaches. The diffusion process progressively improves sampling by smoothing and sharpening the reward landscape, rather than relying on iterative refinement, which is effective for non-convex and fragmented objectives. Finally, we introduce a large-scale real-world dataset for indoor AP deployment, requiring over $50k$ CPU hours to train general reward functions, and evaluate in- and out-of-distribution generalization and robustness. Our results suggest that diffusion-based generative inference with a unified reward function provides a scalable and domain-agnostic foundation for indoor AP deployment planning.

</details>


### [539] [Efficient Swap Regret Minimization in Combinatorial Bandits](https://arxiv.org/abs/2602.02087)
*Andreas Kontogiannis,Vasilis Pollatos,Panayotis Mertikopoulos,Ioannis Panageas*

Main category: cs.LG

TL;DR: 该论文解决了组合多臂老虎机中高效无交换后悔算法设计问题，实现了后悔值在动作数量N上呈多对数依赖，填补了该领域的研究空白。


<details>
  <summary>Details</summary>
Motivation: 在组合多臂老虎机中，动作数量N随问题维度呈指数级增长。虽然外部后悔最小化问题已有较好研究，但实现无交换后悔且保持N的多对数依赖一直是个难题。现有算法无法在保持高效的同时实现无交换后悔。

Method: 提出了一种新的无交换后悔学习算法，该算法不仅理论上有保证，还能高效实现。算法设计确保每次迭代的计算复杂度也呈N的多对数增长，适用于多种实际应用场景。

Result: 成功设计了首个在组合多臂老虎机中实现无交换后悔且后悔值呈N多对数依赖的算法。该算法的后悔界对于组合多臂老虎机类是紧的，并且在实际应用中能高效实现。

Conclusion: 该研究解决了组合多臂老虎机中长期存在的无交换后悔算法设计难题，提出了首个具有多对数依赖的高效算法，填补了该领域的重要空白，为实际应用提供了理论基础。

Abstract: This paper addresses the problem of designing efficient no-swap regret algorithms for combinatorial bandits, where the number of actions $N$ is exponentially large in the dimensionality of the problem. In this setting, designing efficient no-swap regret translates to sublinear -- in horizon $T$ -- swap regret with polylogarithmic dependence on $N$. In contrast to the weaker notion of external regret minimization - a problem which is fairly well understood in the literature - achieving no-swap regret with a polylogarithmic dependence on $N$ has remained elusive in combinatorial bandits. Our paper resolves this challenge, by introducing a no-swap-regret learning algorithm with regret that scales polylogarithmically in $N$ and is tight for the class of combinatorial bandits. To ground our results, we also demonstrate how to implement the proposed algorithm efficiently -- that is, with a per-iteration complexity that also scales polylogarithmically in $N$ -- across a wide range of well-studied applications.

</details>


### [540] [Leveraging Textual-Cues for Enhancing Multimodal Sentiment Analysis by Object Recognition](https://arxiv.org/abs/2602.00360)
*Sumana Biswas,Karen Young,Josephine Griffith*

Main category: cs.LG

TL;DR: 该论文提出了一种名为TEMSA的新方法，通过提取图像中所有检测到的物体名称并与相关文本结合（称为TEMS），来改进多模态情感分析，实验证明TEMS相比单独分析能提升多模态情感分析效果。


<details>
  <summary>Details</summary>
Motivation: 多模态情感分析面临文本和图像模态差异、情感模糊性以及上下文复杂性等挑战，需要开发新方法来有效结合图像和文本数据进行情感分析。

Method: 提出TEMSA方法，基于物体识别技术提取图像中所有检测到的物体名称，将这些物体名称与相关文本结合形成TEMS（文本和图像数据的组合），在两个数据集上进行实验，分别分析图像、文本以及TEMS的情感分析效果。

Result: 实验结果表明，只有TEMS（结合所有物体名称的多模态数据）相比单独分析图像或文本数据，能够改善多模态情感分析的整体结果。

Conclusion: TEMSA方法通过物体识别提取文本线索来增强多模态情感分析是有效的，为多模态情感分析领域提供了新的见解和方法。

Abstract: Multimodal sentiment analysis, which includes both image and text data, presents several challenges due to the dissimilarities in the modalities of text and image, the ambiguity of sentiment, and the complexities of contextual meaning. In this work, we experiment with finding the sentiments of image and text data, individually and in combination, on two datasets. Part of the approach introduces the novel `Textual-Cues for Enhancing Multimodal Sentiment Analysis' (TEMSA) based on object recognition methods to address the difficulties in multimodal sentiment analysis. Specifically, we extract the names of all objects detected in an image and combine them with associated text; we call this combination of text and image data TEMS. Our results demonstrate that only TEMS improves the results when considering all the object names for the overall sentiment of multimodal data compared to individual analysis. This research contributes to advancing multimodal sentiment analysis and offers insights into the efficacy of TEMSA in combining image and text data for multimodal sentiment analysis.

</details>


### [541] [Spectral Superposition: A Theory of Feature Geometry](https://arxiv.org/abs/2602.02224)
*Georgi Ivanov,Narmeen Oozeer,Shivam Raval,Tasana Pejovic,Shriyash Upadhyay,Amir Abdullah*

Main category: cs.LG

TL;DR: 该论文提出了一种通过分析权重矩阵谱来研究神经网络特征几何结构的新理论框架，特别是引入了帧算子来量化特征在特征空间中的分布，证明了容量饱和会导致谱局部化现象。


<details>
  <summary>Details</summary>
Motivation: 神经网络通过叠加表示比维度更多的特征，但现有方法将激活分解为稀疏线性特征时丢弃了几何结构。需要开发能够研究特征几何结构的理论工具。

Method: 引入帧算子F=WW⊤作为谱度量工具，分析权重矩阵的特征值、特征空间等谱特性。使用谱方法捕捉特征的全局几何结构，而不仅仅是成对交互。

Result: 在叠加的玩具模型中证明：容量饱和迫使谱局部化，特征坍缩到单个特征空间，组织成紧框架，并通过关联方案进行分类。该框架可应用于任意权重矩阵。

Conclusion: 谱度量形式主义为解释性研究提供了新方向，指向了将算子理论应用于神经网络解释性的更广泛研究计划。

Abstract: Neural networks represent more features than they have dimensions via superposition, forcing features to share representational space. Current methods decompose activations into sparse linear features but discard geometric structure. We develop a theory for studying the geometric structre of features by analyzing the spectra (eigenvalues, eigenspaces, etc.) of weight derived matrices. In particular, we introduce the frame operator $F = WW^\top$, which gives us a spectral measure that describes how each feature allocates norm across eigenspaces. While previous tools could describe the pairwise interactions between features, spectral methods capture the global geometry (``how do all features interact?''). In toy models of superposition, we use this theory to prove that capacity saturation forces spectral localization: features collapse onto single eigenspaces, organize into tight frames, and admit discrete classification via association schemes, classifying all geometries from prior work (simplices, polygons, antiprisms). The spectral measure formalism applies to arbitrary weight matrices, enabling diagnosis of feature localization beyond toy settings. These results point toward a broader program: applying operator theory to interpretability.

</details>


### [542] [Quantum Generator Kernels](https://arxiv.org/abs/2602.00361)
*Philipp Altmann,Maximilian Mansky,Maximilian Zorn,Jonas Stein,Claudia Linnhoff-Popien*

Main category: cs.LG

TL;DR: 提出量子生成核（QGKs），通过变分生成组（VGGs）构建可参数化的量子核，解决NISQ硬件限制下大规模数据嵌入问题，在投影和分类性能上优于现有量子与经典核方法。


<details>
  <summary>Details</summary>
Motivation: 量子核方法理论上能将经典不可分的特征在量子空间变得可分，但受限于NISQ硬件容量，需要有效策略将大规模真实数据（如图像）压缩嵌入到量子设备中。现有混合架构的固定中间嵌入过程可能阻碍充分利用量子计算潜力。

Method: 提出量子生成核（QGKs），包含一组变分生成组（VGGs），将通用生成器合并为可参数化算子，确保量子空间的可扩展覆盖。通过训练权重向量参数化VGGs在当前数据上下文中的投影，优化核与目标域的对齐。

Result: 实证结果表明，QGKs在投影和分类能力上优于最先进的量子与经典核方法，展示了其作为各种QML应用通用框架的潜力。

Conclusion: QGKs通过生成器方法解决了NISQ硬件限制下的数据嵌入问题，提供了优于现有方法的性能，有望成为量子机器学习应用的通用框架。

Abstract: Quantum kernel methods offer significant theoretical benefits by rendering classically inseparable features separable in quantum space. Yet, the practical application of Quantum Machine Learning (QML), currently constrained by the limitations of Noisy Intermediate-Scale Quantum (NISQ) hardware, necessitates effective strategies to compress and embed large-scale real-world data like images into the constrained capacities of existing quantum devices or simulators. To this end, we propose Quantum Generator Kernels (QGKs), a generator-based approach to quantum kernels, comprising a set of Variational Generator Groups (VGGs) that merge universal generators into a parameterizable operator, ensuring scalable coverage of the available quantum space. Thereby, we address shortcomings of current leading strategies employing hybrid architectures, which might prevent exploiting quantum computing's full potential due to fixed intermediate embedding processes. To optimize the kernel alignment to the target domain, we train a weight vector to parameterize the projection of the VGGs in the current data context. Our empirical results demonstrate superior projection and classification capabilities of the QGK compared to state-of-the-art quantum and classical kernel approaches and show its potential to serve as a versatile framework for various QML applications.

</details>


### [543] [Choice-Model-Assisted Q-learning for Delayed-Feedback Revenue Management](https://arxiv.org/abs/2602.02283)
*Owen Shen,Patrick Jaillet*

Main category: cs.LG

TL;DR: 论文提出"选择模型辅助强化学习"，使用校准的离散选择模型作为部分世界模型，在决策时估算延迟反馈的学习目标，以解决收益管理中延迟反馈问题。


<details>
  <summary>Details</summary>
Motivation: 收益管理中存在显著的延迟反馈问题，大量价值由客户取消和修改决定，这些信息在预订后数天才观察到，需要有效处理这种延迟反馈。

Method: 提出选择模型辅助强化学习方法：使用校准的离散选择模型作为固定的部分世界模型，在决策时估算延迟反馈的学习目标。在固定模型部署机制下，证明了表格Q学习与模型估算目标的收敛性。

Result: 理论证明：表格Q学习收敛到最优Q函数的O(ε/(1-γ))邻域，ε为部分模型误差，加上O(t^{-1/2})采样项。实验基于61,619个酒店预订数据（1,088次独立运行）：(i)在平稳设置中与成熟缓冲DQN基线无统计显著差异；(ii)在参数变化下表现更好，10个变化场景中有5个显著收益提升（最高12.4%）；(iii)在结构错误设定下表现下降（收益降低1.4-2.6%）。

Conclusion: 研究阐明了部分行为模型在何种情况下能提高变化环境下的鲁棒性，以及在何种情况下会引入有害偏差，为延迟反馈下的强化学习提供了理论保证和实用指导。

Abstract: We study reinforcement learning for revenue management with delayed feedback, where a substantial fraction of value is determined by customer cancellations and modifications observed days after booking. We propose \emph{choice-model-assisted RL}: a calibrated discrete choice model is used as a fixed partial world model to impute the delayed component of the learning target at decision time. In the fixed-model deployment regime, we prove that tabular Q-learning with model-imputed targets converges to an $O(\varepsilon/(1-γ))$ neighborhood of the optimal Q-function, where $\varepsilon$ summarizes partial-model error, with an additional $O(t^{-1/2})$ sampling term. Experiments in a simulator calibrated from 61{,}619 hotel bookings (1{,}088 independent runs) show: (i) no statistically detectable difference from a maturity-buffer DQN baseline in stationary settings; (ii) positive effects under in-family parameter shifts, with significant gains in 5 of 10 shift scenarios after Holm--Bonferroni correction (up to 12.4\%); and (iii) consistent degradation under structural misspecification, where the choice model assumptions are violated (1.4--2.6\% lower revenue). These results characterize when partial behavioral models improve robustness under shift and when they introduce harmful bias.

</details>


### [544] [Post-Training Probability Manifold Correction via Structured SVD Pruning and Self-Referential Distillation](https://arxiv.org/abs/2602.00372)
*Aaron R. Flouro,Shawn P. Chadwick*

Main category: cs.LG

TL;DR: SparseKD是一种后训练压缩方法，结合结构化SVD剪枝和自参考知识蒸馏，无需外部教师模型即可实现15-65%的参数压缩，保持可接受的质量损失。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型部署成本高昂，需要有效的压缩方法。现有方法通常需要外部教师模型或架构修改，增加了复杂性和部署难度。

Method: 结合结构化SVD剪枝和自参考知识蒸馏。模型通过匹配压缩前的自身概率分布来"自我教学"，无需外部教师模型。使用固定校准数据集进行后训练。

Result: 自参考蒸馏单独使用可将模型质量相对提升39%。结合剪枝可实现15-65%参数减少，质量损失可接受。速度提升主要来自前馈层的密集矩阵乘法减少，注意力层保持不变。

Conclusion: SparseKD提供了一种无需外部教师、架构修改或定制推理内核的实用压缩方案，可直接部署于现有基础设施，与注意力优化方法互补。

Abstract: Large language models are expensive to deploy. We introduce Sparse Knowledge Distillation (SparseKD), a post-training method that compresses transformer models by combining structured SVD pruning with self-referential knowledge distillation. The key insight is simple: instead of using an external teacher, the model teaches itself by matching its own probability distribution from before compression. This self-referential setup enables surprisingly strong quality recovery after aggressive pruning.
  Our experiments reveal an unexpected finding: self-referential distillation alone, applied post-training under an identical objective and fixed calibration dataset, improves model quality by 39% relative to the original converged checkpoint. When combined with structured pruning, SparseKD achieves 15-65% parameter reduction with acceptable quality trade-offs. Kernel profiling shows that speedups arise entirely from reduced dense matrix multiplication in feed-forward layers while attention remains unchanged, making this approach complementary to attention optimizations.
  We validate across two model families (0.6B and 3.8B parameters) with multi-seed experiments confirming high reproducibility. SparseKD requires no external super-teacher, no architectural changes, and no custom inference kernels, making it immediately deployable with existing infrastructure.

</details>


### [545] [C-kNN-LSH: A Nearest-Neighbor Algorithm for Sequential Counterfactual Inference](https://arxiv.org/abs/2602.02371)
*Jing Wang,Jie Shen,Qiaomin Xie,Jeremy C Weiss*

Main category: cs.LG

TL;DR: C-kNN-LSH：基于局部敏感哈希的最近邻框架，用于从高维纵向数据中估计因果效应，特别适用于长新冠恢复等复杂临床轨迹分析。


<details>
  <summary>Details</summary>
Motivation: 从纵向轨迹中估计因果效应对于理解复杂疾病进展和优化临床决策至关重要，特别是在处理共病和长新冠恢复等高维、混杂的情况下。

Method: 提出C-kNN-LSH框架，利用局部敏感哈希高效识别具有相似协变量历史的"临床双胞胎"，实现跨演化疾病状态的局部条件治疗效果估计。结合双重稳健校正来减轻不规则采样和患者恢复特征变化带来的偏差。

Result: 理论分析证明估计量具有一致性和对干扰误差的二阶稳健性。在包含13,511名参与者的真实世界长新冠队列中，C-kNN-LSH在捕捉恢复异质性和估计政策价值方面优于现有基线方法。

Conclusion: C-kNN-LSH为处理高维混杂的纵向因果推断提供了一个有效框架，特别适用于复杂临床轨迹分析，在长新冠恢复研究中表现出优越性能。

Abstract: Estimating causal effects from longitudinal trajectories is central to understanding the progression of complex conditions and optimizing clinical decision-making, such as comorbidities and long COVID recovery. We introduce \emph{C-kNN--LSH}, a nearest-neighbor framework for sequential causal inference designed to handle such high-dimensional, confounded situations. By utilizing locality-sensitive hashing, we efficiently identify ``clinical twins'' with similar covariate histories, enabling local estimation of conditional treatment effects across evolving disease states. To mitigate bias from irregular sampling and shifting patient recovery profiles, we integrate neighborhood estimator with a doubly-robust correction.
  Theoretical analysis guarantees our estimator is consistent and second-order robust to nuisance error.
  Evaluated on a real-world Long COVID cohort with 13,511 participants, \emph{C-kNN-LSH} demonstrates superior performance in capturing recovery heterogeneity and estimating policy values compared to existing baselines.

</details>


### [546] [MATRIX: A Multimodal Benchmark and Post-Training Framework for Materials Science](https://arxiv.org/abs/2602.00376)
*Delia McGrath,Curtis Chong,Rohil Kulkarni,Gerbrand Ceder,Adeesh Kolluru*

Main category: cs.LG

TL;DR: MATRIX是一个材料科学多模态基准测试，用于评估视觉实验数据对科学推理的影响，研究发现视觉监督能显著提升实验解释和科学推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准难以评估在训练后加入视觉实验数据是否能超越纯文本监督，改善基于物理机制的解释推理能力。

Method: 引入MATRIX多模态基准测试，通过对比仅使用结构化文本训练与加入配对实验图像训练，隔离视觉基础化的影响。

Result: 视觉监督使实验解释提升10-25%，纯文本科学推理任务提升5-16%，且改进依赖于训练中正确的图文对齐。

Conclusion: 结构化多模态训练后训练的好处不仅限于材料科学，还能扩展到其他科学领域，如ScienceQA和PubMedQA。

Abstract: Scientific reasoning in materials science requires integrating multimodal experimental evidence with underlying physical theory. Existing benchmarks make it difficult to assess whether incorporating visual experimental data during post-training improves mechanism-grounded explanation reasoning beyond text-only supervision. We introduce MATRIX, a multimodal benchmark for materials science reasoning that evaluates foundational theory, research-level reasoning, and the interpretation of real experimental artifacts across multiple characterization modalities. Using MATRIX as a controlled diagnostic, we isolate the effect of visual grounding by comparing post-training on structured materials science text alone with post-training that incorporates paired experimental images. Despite using relatively small amounts of multimodal data, visual supervision improves experimental interpretation by 10-25% and yields 5-16% gains on text-only scientific reasoning tasks. Our results demonstrate that these improvements rely on correct image-text alignment during post-training, highlighting cross-modal representational transfer. We also observe consistent improvements on ScienceQA and PubMedQA, demonstrating that the benefits of structured multimodal post-training extend beyond materials science. The MATRIX dataset is available at https://huggingface.co/datasets/radical-ai/MATRIX and the model at https://huggingface.co/radical-ai/MATRIX-PT.

</details>


### [547] [RePaint-Enhanced Conditional Diffusion Model for Parametric Engineering Designs under Performance and Parameter Constraints](https://arxiv.org/abs/2602.00384)
*Ke Wang,Nguyen Gia Hien Vu,Yifan Tang,Mostafa Rahmani Dehaghani,G. Gary Wang*

Main category: cs.LG

TL;DR: 提出RePaint增强框架，集成预训练性能引导DDPM，实现性能与参数约束下的工程设计生成，无需重新训练模型


<details>
  <summary>Details</summary>
Motivation: 传统基于DDPM的方法无法同时支持性能和参数约束下的部分设计重绘，需要一种无需重新训练的高效可控生成方法

Method: 集成预训练性能引导DDPM，采用掩码重采样推理过程，基于部分参考设计生成缺失组件并满足性能约束

Result: 在参数化船体设计和翼型设计两个案例中验证，生成具有预期性能的新颖设计，精度达到或超过预训练模型

Conclusion: 该方法为工程应用提供了高效、无需训练的、参数约束感知的生成设计解决方案

Abstract: This paper presents a RePaint-enhanced framework that integrates a pre-trained performance-guided denoising diffusion probabilistic model (DDPM) for performance- and parameter-constraint engineering design generation. The proposed method enables the generation of missing design components based on a partial reference design while satisfying performance constraints, without retraining the underlying model. By applying mask-based resampling during inference process, RePaint allows efficient and controllable repainting of partial designs under both performance and parameter constraints, which is not supported by conventional DDPM-base methods. The framework is evaluated on two representative design problems, parametric ship hull design and airfoil design, demonstrating its ability to generate novel designs with expected performance based on a partial reference design. Results show that the method achieves accuracy comparable to or better than pre-trained models while enabling controlled novelty through fixing partial designs. Overall, the proposed approach provides an efficient, training-free solution for parameter-constraint-aware generative design in engineering applications.

</details>


### [548] [A Fragile Guardrail: Diffusion LLM's Safety Blessing and Its Failure Mode](https://arxiv.org/abs/2602.00388)
*Zeyuan He,Yupeng Chen,Lang Lin,Yihan Wang,Shenxu Chang,Eric Sommerlade,Philip Torr,Junchi Yu,Adel Bibi,Jialin Yu*

Main category: cs.LG

TL;DR: 扩散大语言模型（D-LLMs）相比自回归模型具有内在安全优势，能抵御传统越狱攻击，但存在"上下文嵌套"漏洞可绕过其安全机制。


<details>
  <summary>Details</summary>
Motivation: 研究扩散大语言模型（D-LLMs）相比自回归模型（AR-LLMs）的安全特性，探索其内在安全优势的机制和局限性。

Method: 分析扩散轨迹的逐步抑制机制，提出"上下文嵌套"攻击方法，将有害请求嵌入结构化良性上下文中，绕过D-LLMs的安全防御。

Result: 上下文嵌套攻击能有效绕过D-LLMs的安全机制，在多个模型和基准测试中达到最先进的攻击成功率，首次成功越狱Gemini Diffusion。

Conclusion: D-LLMs具有内在安全优势但并非绝对安全，上下文嵌套暴露了其关键漏洞，这是对D-LLMs的早期红队测试。

Abstract: Diffusion large language models (D-LLMs) offer an alternative to autoregressive LLMs (AR-LLMs) and have demonstrated advantages in generation efficiency. Beyond the utility benefits, we argue that D-LLMs exhibit a previously underexplored safety blessing: their diffusion-style generation confers intrinsic robustness against jailbreak attacks originally designed for AR-LLMs. In this work, we provide an initial analysis of the underlying mechanism, showing that the diffusion trajectory induces a stepwise reduction effect that progressively suppresses unsafe generations. This robustness, however, is not absolute. We identify a simple yet effective failure mode, termed context nesting, where harmful requests are embedded within structured benign contexts, effectively bypassing the stepwise reduction mechanism. Empirically, we show that this simple strategy is sufficient to bypass D-LLMs' safety blessing, achieving state-of-the-art attack success rates across models and benchmarks. Most notably, it enables the first successful jailbreak of Gemini Diffusion, to our knowledge, exposing a critical vulnerability in commercial D-LLMs. Together, our results characterize both the origins and the limits of D-LLMs' safety blessing, constituting an early-stage red-teaming of D-LLMs.

</details>


### [549] [Localized, High-resolution Geographic Representations with Slepian Functions](https://arxiv.org/abs/2602.00392)
*Arjun Rao,Ruth Crasto,Tessa Ooms,David Rolnick,Konstantin Klemmer,Marc Rußwurm*

Main category: cs.LG

TL;DR: 提出基于球面Slepian函数的地理位置编码器，专注于感兴趣区域的高分辨率表示，同时通过混合Slepian-球谐函数编码器平衡局部-全局性能


<details>
  <summary>Details</summary>
Motivation: 地理数据本质上是局部的，但现有机器学习模型的地理位置编码器在全球范围内均匀分配表示能力，难以满足局部应用的高分辨率需求

Method: 使用球面Slepian函数构建地理位置编码器，将表示能力集中在感兴趣区域内；同时提出混合Slepian-球谐函数编码器，平衡局部和全局性能

Result: 在分类、回归和图像增强预测等五个任务中，Slepian编码优于基线方法，并在多种神经网络架构中保持性能优势

Conclusion: Slepian编码器能够有效解决地理数据局部表示问题，提供高分辨率、计算高效的地理位置编码方案，同时保持极地安全和球面距离保持等理想特性

Abstract: Geographic data is fundamentally local. Disease outbreaks cluster in population centers, ecological patterns emerge along coastlines, and economic activity concentrates within country borders. Machine learning models that encode geographic location, however, distribute representational capacity uniformly across the globe, struggling at the fine-grained resolutions that localized applications require. We propose a geographic location encoder built from spherical Slepian functions that concentrate representational capacity inside a region-of-interest and scale to high resolutions without extensive computational demands. For settings requiring global context, we present a hybrid Slepian-Spherical Harmonic encoder that efficiently bridges the tradeoff between local-global performance, while retaining desirable properties such as pole-safety and spherical-surface-distance preservation. Across five tasks spanning classification, regression, and image-augmented prediction, Slepian encodings outperform baselines and retain performance advantages across a wide range of neural network architectures.

</details>


### [550] [Fast Forward: Accelerating LLM Prefill with Predictive FFN Sparsity](https://arxiv.org/abs/2602.00397)
*Aayush Gautam,Mukul Gagrani,Junyoung Park,Mingu Lee,Chiris Lott,Narasimha Reddy*

Main category: cs.LG

TL;DR: FastForward：一种预测性稀疏框架，通过块级、上下文感知的FFN稀疏化加速LLM预填充阶段，在50%稀疏度下实现1.45倍计算加速，准确率损失小于6%。


<details>
  <summary>Details</summary>
Motivation: LLM推理的预填充阶段是长上下文工作负载的关键计算瓶颈。在短到中等上下文长度（1K-16K token）下，前馈网络（FFN）占用了大部分计算成本。现有的FFN稀疏化方法为自回归解码设计，无法利用预填充阶段的并行性且常导致精度下降。

Method: FastForward框架包含三个核心组件：1）轻量级专家预测器，用于按块选择高重要性神经元；2）误差补偿网络，用于纠正稀疏化引起的误差；3）层间稀疏度调度器，根据token混合重要性分配计算资源。

Result: 在LLaMA和Qwen模型（最大8B参数）上，FastForward在50% FFN稀疏度下实现了高达1.45倍的计算加速，在LongBench基准测试中相比密集基线的准确率损失小于6%，显著降低了首token时间（TTFT）。

Conclusion: FastForward通过预测性稀疏化有效加速了LLM预填充阶段，为受限硬件上的高效长上下文LLM推理提供了解决方案，在保持精度的同时显著提升了计算效率。

Abstract: The prefill stage of large language model (LLM) inference is a key computational bottleneck for long-context workloads. At short-to-moderate context lengths (1K--16K tokens), Feed-Forward Networks (FFNs) dominate this cost, accounting for most of the total FLOPs. Existing FFN sparsification methods, designed for autoregressive decoding, fail to exploit the prefill stage's parallelism and often degrade accuracy. To address this, we introduce FastForward, a predictive sparsity framework that accelerates LLM prefill through block-wise, context-aware FFN sparsity. FastForward combines (1) a lightweight expert predictor to select high-importance neurons per block, (2) an error compensation network to correct sparsity-induced errors, and (3) a layer-wise sparsity scheduler to allocate compute based on token-mixing importance. Across LLaMA and Qwen models up to 8B parameters, FastForward delivers up to 1.45$\times$ compute-bound speedup at 50% FFN sparsity with $<$ 6% accuracy loss compared to the dense baseline on LongBench, substantially reducing Time-to-First-Token (TTFT) for efficient, long-context LLM inference on constrained hardware.

</details>


### [551] [MemoryLLM: Plug-n-Play Interpretable Feed-Forward Memory for Transformers](https://arxiv.org/abs/2602.00398)
*Ajay Jaiswal,Lauren Hannah,Han-Byul Kim,Duc Hoang,Arnav Kundu,Mehrdad Farajtabar,Minsik Cho*

Main category: cs.LG

TL;DR: MemoryLLM将前馈网络从自注意力中解耦，将其视为上下文无关的令牌检索记忆，通过独立训练实现高效推理和存储优化


<details>
  <summary>Details</summary>
Motivation: 理解Transformer组件在LLM中的运作机制对AI技术进步至关重要，当前前馈网络（FFNs）的可解释性存在挑战，需要新的方法来解耦和分析FFNs的作用

Method: 提出MemoryLLM，将FFNs从自注意力中解耦，使用令牌嵌入独立训练FFNs，使其成为上下文无关的令牌检索记忆；同时引入Flex-MemoryLLM作为传统Transformer和MemoryLLM之间的折中架构

Result: FFNs可以预计算为令牌查找表（ToLs），支持按需在VRAM和存储之间传输，提升推理效率；研究了不同下游任务中FFN记忆的重要性

Conclusion: MemoryLLM提供了一种有效解耦和分析FFNs的方法，通过将FFNs视为上下文无关记忆，既增强了可解释性又提高了推理效率，Flex-MemoryLLM则缓解了性能差距

Abstract: Understanding how transformer components operate in LLMs is important, as it is at the core of recent technological advances in artificial intelligence. In this work, we revisit the challenges associated with interpretability of feed-forward modules (FFNs) and propose MemoryLLM, which aims to decouple FFNs from self-attention and enables us to study the decoupled FFNs as context-free token-wise neural retrieval memory. In detail, we investigate how input tokens access memory locations within FFN parameters and the importance of FFN memory across different downstream tasks. MemoryLLM achieves context-free FFNs by training them in isolation from self-attention directly using the token embeddings. This approach allows FFNs to be pre-computed as token-wise lookups (ToLs), enabling on-demand transfer between VRAM and storage, additionally enhancing inference efficiency. We also introduce Flex-MemoryLLM, positioning it between a conventional transformer design and MemoryLLM. This architecture bridges the performance gap caused by training FFNs with context-free token-wise embeddings.

</details>


### [552] [DROGO: Default Representation Objective via Graph Optimization in Reinforcement Learning](https://arxiv.org/abs/2602.00403)
*Hon Tik Tse,Marlos C. Machado*

Main category: cs.LG

TL;DR: 提出直接近似默认表示主特征向量的目标函数，避免先计算矩阵再分解的计算开销


<details>
  <summary>Details</summary>
Motivation: 传统方法需要先近似默认表示矩阵再进行特征分解，计算成本高且难以扩展到高维空间

Method: 推导出用神经网络直接近似默认表示主特征向量的目标函数

Result: 在多个环境中实证验证了该目标函数的有效性，并应用于奖励塑形

Conclusion: 提出的直接近似方法比传统两步法更高效，可扩展到高维强化学习问题

Abstract: In computational reinforcement learning, the default representation (DR) and its principal eigenvector have been shown to be effective for a wide variety of applications, including reward shaping, count-based exploration, option discovery, and transfer. However, in prior investigations, the eigenvectors of the DR were computed by first approximating the DR matrix, and then performing an eigendecomposition. This procedure is computationally expensive and does not scale to high-dimensional spaces. In this paper, we derive an objective for directly approximating the principal eigenvector of the DR with a neural network. We empirically demonstrate the effectiveness of the objective in a number of environments, and apply the learned eigenvectors for reward shaping.

</details>


### [553] [Fed-Listing: Federated Label Distribution Inference in Graph Neural Networks](https://arxiv.org/abs/2602.00407)
*Suprim Nakarmi,Junggab Son,Yue Zhao,Zuobin Xiong*

Main category: cs.LG

TL;DR: 提出Fed-Listing攻击方法，利用联邦图神经网络训练中的梯度信息推断目标客户端的私有标签分布，无需原始数据或节点特征


<details>
  <summary>Details</summary>
Motivation: 联邦图神经网络虽然能保护用户隐私，但共享的模型更新（特别是梯度）仍可能泄露敏感信息。现有研究主要关注传统联邦学习的隐私推断攻击，而图神经网络中的标签分布推断问题尚未充分探索

Method: 提出Fed-Listing攻击方法：仅利用训练期间交换的最终层梯度，通过分析统计模式来推断类别比例。使用辅助影子数据集生成多样化的标签划分策略，模拟不同客户端分布，从而训练攻击模型

Result: 在四个基准数据集和三种GNN架构上的实验表明，Fed-Listing显著优于现有基线方法（包括随机猜测和Decaf），即使在具有挑战性的非独立同分布场景下也表现良好。防御机制几乎无法降低攻击性能，除非严重损害模型效用

Conclusion: 联邦图神经网络中的梯度信息会泄露客户端标签分布隐私，Fed-Listing攻击方法有效且隐蔽，现有防御措施难以应对，除非牺牲模型性能，这揭示了FedGNNs中需要更强的隐私保护机制

Abstract: Graph Neural Networks (GNNs) have been intensively studied for their expressive representation and learning performance on graph-structured data, enabling effective modeling of complex relational dependencies among nodes and edges in various domains. However, the standalone GNNs can unleash threat surfaces and privacy implications, as some sensitive graph-structured data is collected and processed in a centralized setting. To solve this issue, Federated Graph Neural Networks (FedGNNs) are proposed to facilitate collaborative learning over decentralized local graph data, aiming to preserve user privacy. Yet, emerging research indicates that even in these settings, shared model updates, particularly gradients, can unintentionally leak sensitive information of local users. Numerous privacy inference attacks have been explored in traditional federated learning and extended to graph settings, but the problem of label distribution inference in FedGNNs remains largely underexplored. In this work, we introduce Fed-Listing (Federated Label Distribution Inference in GNNs), a novel gradient-based attack designed to infer the private label statistics of target clients in FedGNNs without access to raw data or node features. Fed-Listing only leverages the final-layer gradients exchanged during training to uncover statistical patterns that reveal class proportions in a stealthy manner. An auxiliary shadow dataset is used to generate diverse label partitioning strategies, simulating various client distributions, on which the attack model is obtained. Extensive experiments on four benchmark datasets and three GNN architectures show that Fed-Listing significantly outperforms existing baselines, including random guessing and Decaf, even under challenging non-i.i.d. scenarios. Moreover, applying defense mechanisms can barely reduce our attack performance, unless the model's utility is severely degraded.

</details>


### [554] [Variational Approach for Job Shop Scheduling](https://arxiv.org/abs/2602.00408)
*Seung Heon Oh,Jiwon Baek,Ki Young Cho,Hee Chang Yoon,Jong Hun Woo*

Main category: cs.LG

TL;DR: VG2S框架首次将变分推断引入JSSP领域，通过变分图编码器分离表示学习和策略优化，显著提升训练稳定性和零样本泛化能力


<details>
  <summary>Details</summary>
Motivation: 传统深度强化学习方法在JSSP中面临训练非平稳性和泛化能力有限的问题，主要原因是表示学习和策略执行同时优化

Method: 提出变分图到调度器框架，基于ELBO和最大熵强化学习推导概率目标，通过变分图编码器解耦表示学习和策略优化

Result: 在DMU和SWV等大规模基准实例上，VG2S展现出优于现有DRL方法和传统调度规则的零样本泛化能力

Conclusion: VG2S通过变分推断成功解决了JSSP中的训练稳定性和泛化问题，为制造调度提供了更鲁棒的解决方案

Abstract: This paper proposes a novel Variational Graph-to-Scheduler (VG2S) framework for solving the Job Shop Scheduling Problem (JSSP), a critical task in manufacturing that directly impacts operational efficiency and resource utilization. Conventional Deep Reinforcement Learning (DRL) approaches often face challenges such as non-stationarity during training and limited generalization to unseen problem instances because they optimize representation learning and policy execution simultaneously. To address these issues, we introduce variational inference to the JSSP domain for the first time and derive a probabilistic objective based on the Evidence of Lower Bound (ELBO) with maximum entropy reinforcement learning. By mathematically decoupling representation learning from policy optimization, the VG2S framework enables the agent to learn robust structural representations of scheduling instances through a variational graph encoder. This approach significantly enhances training stability and robustness against hyperparameter variations. Extensive experiments demonstrate that the proposed method exhibits superior zero-shot generalization compared with state-of-the-art DRL baselines and traditional dispatching rules, particularly on large-scale and challenging benchmark instances such as DMU and SWV.

</details>


### [555] [Robustness of AutoML on Dirty Categorical Data](https://arxiv.org/abs/2602.00412)
*Marcos L. P. Bueno,Joaquin Vanschoren*

Main category: cs.LG

TL;DR: 本文提出了一种将分类数据转换为数值数据的管道，使AutoML方法能够处理经过高级编码方案转换的分类数据，并评估了当前AutoML方法在处理脏分类数据集上的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 虽然AutoML方法在分类任务中能够处理数据缺陷（如异常值、多尺度和缺失数据），但它们在脏分类数据集上的行为尚不清楚。这些数据集通常具有高基数的分类特征，而现有的AutoML方法在处理这类数据时效果未知。特别是形态编码器已被证明能提升脏分类数据的预测性能，但其在AutoML中的效果尚未研究。

Method: 提出一个管道，将分类数据转换为数值数据，使AutoML能够处理经过高级编码方案转换的分类数据。在脏数据集上对当前AutoML方法的鲁棒性进行基准测试，并与提出的管道进行比较。同时分析AutoML构建的ML管道，而不仅仅是它们返回的最佳模型。

Result: 通过基准测试比较了当前AutoML方法与提出的管道在脏分类数据集上的预测性能差异，并深入分析了AutoML构建的ML管道结构，提供了超越最佳模型选择的洞察。

Conclusion: 研究揭示了AutoML方法在处理脏分类数据集时的性能特点，提出的管道能够提升AutoML处理这类数据的能力，同时通过分析AutoML构建的管道结构，为理解AutoML的内部工作机制提供了新的视角。

Abstract: The goal of automated machine learning (AutoML) is to reduce trial and error when doing machine learning (ML). Although AutoML methods for classification are able to deal with data imperfections, such as outliers, multiple scales and missing data, their behavior is less known on dirty categorical datasets. These datasets often have several categorical features with high cardinality arising from issues such as lack of curation and automated collection. Recent research has shown that ML models can benefit from morphological encoders for dirty categorical data, leading to significantly superior predictive performance. However the effects of using such encoders in AutoML methods are not known at the moment. In this paper, we propose a pipeline that transforms categorical data into numerical data so that an AutoML can handle categorical data transformed by more advanced encoding schemes. We benchmark the current robustness of AutoML methods on a set of dirty datasets and compare it with the proposed pipeline. This allows us to get insight on differences in predictive performance. We also look at the ML pipelines built by AutoMLs in order to gain insight beyond the best model as typically returned by these methods.

</details>


### [556] [Federated-inspired Single-cell Batch Integration in Latent Space](https://arxiv.org/abs/2602.00423)
*Quang-Huy Nguyen,Zongliang Yue,Hao Chen,Wei-Shinn Ku,Jiaqi Wang*

Main category: cs.LG

TL;DR: scBatchProx：一种基于联邦学习原理的后处理优化方法，用于改进单细胞RNA测序数据中的批次效应校正，无需原始表达数据或集中式优化。


<details>
  <summary>Details</summary>
Motivation: 单细胞RNA测序产生大量高维数据，但跨实验的数据积累会引入批次效应，掩盖真实的生物信号。现有批次校正方法要么校正不足，要么需要在整个数据集上进行集中式重新训练，限制了它们在分布式和持续演变的单细胞数据环境中的适用性。

Method: scBatchProx是一种受联邦学习启发的后处理优化方法，将每个批次视为客户端，在近端正则化下学习批次条件适配器，直接在潜在空间中校正批次结构，无需原始表达数据或集中式优化。该方法轻量级且可部署，仅优化批次特定的适配器参数。

Result: 大量实验表明，scBatchProx在整体嵌入质量上持续带来约3-8%的相对提升，在90%的数据-方法对中改善了批次校正，在85%的数据-方法对中改善了生物保守性。

Conclusion: 这项工作是在动态单细胞数据系统中实现学习表示实用化改进的一步，为分布式和持续演变的单细胞数据环境提供了一种有效的批次校正解决方案。

Abstract: Advances in single-cell RNA sequencing enable the rapid generation of massive, high-dimensional datasets, yet the accumulation of data across experiments introduces batch effects that obscure true biological signals. Existing batch correction approaches either insufficiently correct batch effects or require centralized retraining on the complete dataset, limiting their applicability in distributed and continually evolving single-cell data settings. We introduce scBatchProx, a post-hoc optimization method inspired by federated learning principles for refining cell-level embeddings produced by arbitrary upstream methods. Treating each batch as a client, scBatchProx learns batch-conditioned adapters under proximal regularization, correcting batch structure directly in latent space without requiring raw expression data or centralized optimization. The method is lightweight and deployable, optimizing batch-specific adapter parameters only. Extensive experiments show that scBatchProx consistently yields relative gains of approximately 3-8% in overall embedding quality, with batch correction and biological conservation improving in 90% and 85% of data-method pairs, respectively. We envision this work as a step toward the practical refinement of learned representations in dynamic single-cell data systems.

</details>


### [557] [Open Materials Generation with Inference-Time Reinforcement Learning](https://arxiv.org/abs/2602.00424)
*Philipp Hoellmer,Stefano Martiniani*

Main category: cs.LG

TL;DR: OMatG-IRL：一种基于策略梯度强化学习的框架，直接在学习的速度场上操作，无需显式计算分数，用于晶体材料生成和晶体结构预测。


<details>
  <summary>Details</summary>
Motivation: 连续时间生成模型能够预测稳定晶体结构，但难以将显式目标属性纳入生成过程。策略梯度强化学习提供了对齐生成模型与下游目标的机制，但通常需要访问分数，这阻碍了其在仅学习速度场的流模型中的应用。

Method: 提出OMatG-IRL框架，直接在学习的速度场上操作，消除对显式分数计算的需求。利用底层生成动力学的随机扰动，保持预训练生成模型的基线性能，同时在推理时实现探索和策略梯度估计。

Result: 首次将强化学习应用于晶体结构预测。该方法能够有效强化基于能量的目标，同时通过组成条件保持多样性，性能与基于分数的强化学习方法相当。能够学习时间依赖的速度退火计划，实现采样效率数量级提升和生成时间相应减少。

Conclusion: OMatG-IRL为晶体材料生成提供了一种有效的推理时强化学习框架，无需显式分数计算，在保持多样性的同时实现目标属性优化，显著提高了采样效率和生成速度。

Abstract: Continuous-time generative models for crystalline materials enable inverse materials design by learning to predict stable crystal structures, but incorporating explicit target properties into the generative process remains challenging. Policy-gradient reinforcement learning (RL) provides a principled mechanism for aligning generative models with downstream objectives but typically requires access to the score, which has prevented its application to flow-based models that learn only velocity fields. We introduce Open Materials Generation with Inference-time Reinforcement Learning (OMatG-IRL), a policy-gradient RL framework that operates directly on the learned velocity fields and eliminates the need for the explicit computation of the score. OMatG-IRL leverages stochastic perturbations of the underlying generation dynamics preserving the baseline performance of the pretrained generative model while enabling exploration and policy-gradient estimation at inference time. Using OMatG-IRL, we present the first application of RL to crystal structure prediction (CSP). Our method enables effective reinforcement of an energy-based objective while preserving diversity through composition conditioning, and it achieves performance competitive with score-based RL approaches. Finally, we show that OMatG-IRL can learn time-dependent velocity-annealing schedules, enabling accurate CSP with order-of-magnitude improvements in sampling efficiency and, correspondingly, reduction in generation time.

</details>


### [558] [LLMs as High-Dimensional Nonlinear Autoregressive Models with Attention: Training, Alignment and Inference](https://arxiv.org/abs/2602.00426)
*Vikram Krishnamurthy*

Main category: cs.LG

TL;DR: 该论文提供了一个基于数学公式的LLM统一框架，将LLM描述为具有注意力依赖的高维非线性自回归模型，涵盖预训练、对齐和生成等完整流程。


<details>
  <summary>Details</summary>
Motivation: 现有LLM研究通常通过架构组件和训练过程来描述，缺乏明确的数学计算结构。本文旨在为研究人员提供一个简洁的数学参考框架，用方程级别的描述来统一理解LLM的训练、对齐和生成过程。

Method: 将LLM形式化为高维非线性自回归模型，其中自注意力机制被描述为重复的双线性-softmax-线性组合。框架包含：1）基于下一词预测的预训练；2）RLHF、DPO、RSFT、RLVR等对齐方法；3）推理时的自回归生成。

Result: 建立了一个统一的数学框架，能够系统分析对齐诱导的行为（如奉承）、推理时现象（如幻觉、上下文学习、思维链提示、检索增强生成）以及持续学习等扩展问题。

Conclusion: 该数学框架为LLM的解释和理论发展提供了简洁的参考，使研究人员能够从计算结构的角度深入理解LLM的行为和现象，促进更严谨的理论分析。

Abstract: Large language models (LLMs) based on transformer architectures are typically described through collections of architectural components and training procedures, obscuring their underlying computational structure. This review article provides a concise mathematical reference for researchers seeking an explicit, equation-level description of LLM training, alignment, and generation. We formulate LLMs as high-dimensional nonlinear autoregressive models with attention-based dependencies. The framework encompasses pretraining via next-token prediction, alignment methods such as reinforcement learning from human feedback (RLHF), direct preference optimization (DPO), rejection sampling fine-tuning (RSFT), and reinforcement learning from verifiable rewards (RLVR), as well as autoregressive generation during inference. Self-attention emerges naturally as a repeated bilinear--softmax--linear composition, yielding highly expressive sequence models. This formulation enables principled analysis of alignment-induced behaviors (including sycophancy), inference-time phenomena (such as hallucination, in-context learning, chain-of-thought prompting, and retrieval-augmented generation), and extensions like continual learning, while serving as a concise reference for interpretation and further theoretical development.

</details>


### [559] [Towards Building Non-Fine-Tunable Foundation Models](https://arxiv.org/abs/2602.00446)
*Ziyao Wang,Nizhang Li,Pingzhi Li,Guoheng Sun,Tianlong Chen,Ang Li*

Main category: cs.LG

TL;DR: 提出Private Mask Pre-Training (PMP)框架，通过将表征学习集中在训练早期识别的稀疏子网络中，并保密该子网络的二进制掩码，只发布最终密集权重，从而构建不可微调的基础模型，防止未经授权的下游微调。


<details>
  <summary>Details</summary>
Motivation: 开源基础模型虽然促进了广泛重用，但也使模型训练者面临未经授权的下游微调带来的经济和安全风险。需要构建既能保持广泛可用性，又能限制未经授权微调收益的基础模型。

Method: 提出Private Mask Pre-Training (PMP)预训练框架：1) 在训练早期识别稀疏子网络；2) 保密定义该子网络的二进制掩码；3) 只发布最终密集权重；4) 未经授权的微调由于无法访问掩码而更新与预训练子空间不对齐的参数，导致微调目标与预训练几何结构不匹配。

Result: 理论分析表明这种不匹配会破坏基于梯度的适应过程并限制微调收益。在大语言模型上的实证结果显示，PMP保持了基础模型性能，同时在广泛的下游任务中持续降低未经授权微调的效果，非微调能力强度可通过掩码比例控制。

Conclusion: PMP框架成功构建了非微调基础模型，在保持模型可用性的同时有效防止未经授权的下游微调，为解决开源基础模型的安全风险提供了可行方案。

Abstract: Open-sourcing foundation models (FMs) enables broad reuse but also exposes model trainers to economic and safety risks from unrestricted downstream fine-tuning. We address this problem by building non-fine-tunable foundation models: models that remain broadly usable in their released form while yielding limited adaptation gains under task-agnostic unauthorized fine-tuning. We propose Private Mask Pre-Training (PMP), a pre-training framework that concentrates representation learning into a sparse subnetwork identified early in training. The binary mask defining this subnetwork is kept private, and only the final dense weights are released. This forces unauthorized fine-tuning without access to the mask to update parameters misaligned with pretraining subspace, inducing an intrinsic mismatch between the fine-tuning objective and the pre-training geometry. We provide theoretical analysis showing that this mismatch destabilizes gradient-based adaptation and bounds fine-tuning gains. Empirical results on large language models demonstrating that PMP preserves base model performance while consistently degrading unauthorized fine-tuning across a wide range of downstream tasks, with the strength of non-fine-tunability controlled by the mask ratio.

</details>


### [560] [Stabilizing Decentralized Federated Fine-Tuning via Topology-Aware Alternating LoRA](https://arxiv.org/abs/2602.00451)
*Xiaoyu Wang,Xiaotian Li,Zhixiang Zhou,Chen Li,Yong Liu*

Main category: cs.LG

TL;DR: TAD-LoRA：一种用于去中心化联邦学习的拓扑感知低秩适应框架，通过协调LoRA因子的更新和混合来控制客户端间错位，在不同通信拓扑下实现鲁棒性能。


<details>
  <summary>Details</summary>
Motivation: 去中心化联邦学习（DFL）作为联邦学习的无服务器变体，在参数高效微调方面面临独特挑战。与线性参数不同，LoRA更新的去中心化聚合会引入拓扑依赖的交叉项，这些交叉项在动态通信图下可能破坏训练稳定性。

Method: 提出TAD-LoRA框架，协调LoRA因子的更新和混合以控制客户端间错位。该方法理论上证明了在非凸目标下的收敛性，明确刻画了拓扑诱导交叉项误差与块坐标表示偏差之间的权衡关系。

Result: 在不同通信条件下的实验验证了分析结果，TAD-LoRA在各种通信场景下实现鲁棒性能：在强连接拓扑中保持竞争力，在中度和弱连接拓扑中带来明显增益，在MNLI数据集上表现尤为突出。

Conclusion: TAD-LoRA通过拓扑感知的LoRA因子协调机制，有效解决了去中心化联邦学习中LoRA更新的稳定性问题，为动态通信图下的参数高效微调提供了可靠解决方案。

Abstract: Decentralized federated learning (DFL), a serverless variant of federated learning, poses unique challenges for parameter-efficient fine-tuning due to the factorized structure of low-rank adaptation (LoRA). Unlike linear parameters, decentralized aggregation of LoRA updates introduces topology-dependent cross terms that can destabilize training under dynamic communication graphs. We propose \texttt{TAD-LoRA}, a Topology-Aware Decentralized Low-Rank Adaptation framework that coordinates the updates and mixing of LoRA factors to control inter-client misalignment. We theoretically prove the convergence of \texttt{TAD-LoRA} under non-convex objectives, explicitly characterizing the trade-off between topology-induced cross-term error and block-coordinate representation bias governed by the switching interval of alternative training. Experiments under various communication conditions validate our analysis, showing that \texttt{TAD-LoRA} achieves robust performance across different communication scenarios, remaining competitive in strongly connected topologies and delivering clear gains under moderately and weakly connected topologies, with particularly strong results on the MNLI dataset.

</details>


### [561] [FedMOA: Federated GRPO for Personalized Reasoning LLMs under Heterogeneous Rewards](https://arxiv.org/abs/2602.00453)
*Ziyao Wang,Daeun Jung,Yexiao He,Guoheng Sun,Zheyu Shen,Myungjin Lee,Ang Li*

Main category: cs.LG

TL;DR: FedMOA：一个基于联邦学习的多目标对齐框架，在异构奖励环境下改进GRPO，通过自适应权重机制和任务感知聚合提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统RL对齐在联邦学习中因需要独立的critic网络而内存开销大，GRPO的无critic架构适合设备端训练，但联邦环境下存在异构奖励定义、多目标优化不平衡和高训练成本等挑战。

Method: 提出FedMOA框架：1) 本地训练采用基于超梯度下降的在线自适应权重机制，在主推理目标满足时优先处理辅助目标；2) 服务器端使用任务和准确率感知的聚合策略，优先选择高质量更新。

Result: 在数学推理和代码生成基准测试中，FedMOA始终优于联邦平均方法，准确率提升高达2.2%，同时改善了全局性能、个性化能力和多目标平衡。

Conclusion: FedMOA成功解决了联邦GRPO中的异构奖励和多目标优化挑战，为设备端个性化RL对齐提供了可行方案，在保持隐私的同时提升了模型推理能力。

Abstract: Group Relative Policy Optimization (GRPO) has recently emerged as an effective approach for improving the reasoning capabilities of large language models through online multi-objective reinforcement learning. While personalization on private data is increasingly vital, traditional Reinforcement Learning (RL) alignment is often memory-prohibitive for on-device federated learning due to the overhead of maintaining a separate critic network. GRPO's critic-free architecture enables feasible on-device training, yet transitioning to a federated setting introduces systemic challenges: heterogeneous reward definitions, imbalanced multi-objective optimization, and high training costs. We propose FedMOA, a federated GRPO framework for multi-objective alignment under heterogeneous rewards. FedMOA stabilizes local training through an online adaptive weighting mechanism via hypergradient descent, which prioritizes primary reasoning as auxiliary objectives saturate. On the server side, it utilizes a task- and accuracy-aware aggregation strategy to prioritize high-quality updates. Experiments on mathematical reasoning and code generation benchmarks demonstrate that FedMOA consistently outperforms federated averaging, achieving accuracy gains of up to 2.2% while improving global performance, personalization, and multi-objective balance.

</details>


### [562] [Search Inspired Exploration in Reinforcement Learning](https://arxiv.org/abs/2602.00460)
*Georgios Sotirchos,Zlatan Ajanović,Jens Kober*

Main category: cs.LG

TL;DR: SIERL提出了一种基于搜索启发的强化学习探索方法，通过设置学习进度相关的子目标来主动引导探索，在稀疏奖励环境中优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 稀疏奖励环境中的探索是强化学习的基本挑战。现有方法如课程学习和Go-Explore依赖人工设计的启发式规则，而好奇心驱动的方法可能收敛到次优策略。需要一种更系统化的探索方法。

Method: SIERL在每个episode开始时从边界（已知状态空间的边界）中选择子目标，然后代理继续向主要任务目标探索。子目标选择机制提供既不过于熟悉也不完全新颖的状态-动作对，确保边界被系统性地扩展。子目标基于到达成本和未来成本的估计进行优先级排序，有效引导探索到信息最丰富的区域。

Result: 在具有挑战性的稀疏奖励环境实验中，SIERL在实现主要任务目标和泛化到环境中任意状态方面都优于主导基线方法。

Conclusion: SIERL通过搜索启发的子目标选择机制，提供了一种系统化的探索方法，能够有效解决稀疏奖励环境中的探索挑战，并在性能和泛化能力上超越现有方法。

Abstract: Exploration in environments with sparse rewards remains a fundamental challenge in reinforcement learning (RL). Existing approaches such as curriculum learning and Go-Explore often rely on hand-crafted heuristics, while curiosity-driven methods risk converging to suboptimal policies. We propose Search-Inspired Exploration in Reinforcement Learning (SIERL), a novel method that actively guides exploration by setting sub-goals based on the agent's learning progress. At the beginning of each episode, SIERL chooses a sub-goal from the \textit{frontier} (the boundary of the agent's known state space), before the agent continues exploring toward the main task objective. The key contribution of our method is the sub-goal selection mechanism, which provides state-action pairs that are neither overly familiar nor completely novel. Thus, it assures that the frontier is expanded systematically and that the agent is capable of reaching any state within it. Inspired by search, sub-goals are prioritized from the frontier based on estimates of cost-to-come and cost-to-go, effectively steering exploration towards the most informative regions. In experiments on challenging sparse-reward environments, SIERL outperforms dominant baselines in both achieving the main task goal and generalizing to reach arbitrary states in the environment.

</details>


### [563] [PAIR-Former: Budgeted Relational MIL for miRNA Target Prediction](https://arxiv.org/abs/2602.00465)
*Jiaqi Yin,Baiming Chen,Jia Fei,Mingjun Yang*

Main category: cs.LG

TL;DR: PAIR-Former：一种预算感知的关系多示例学习框架，用于miRNA-mRNA靶向预测，通过廉价全池扫描和多样性选择，在计算预算内实现准确率-计算量可控权衡。


<details>
  <summary>Details</summary>
Motivation: miRNA-mRNA靶向预测是一个大规模预测问题：每个转录本产生大量候选靶位点，但只能观察到配对级别的标签。现有方法在计算资源有限的情况下难以有效处理这种关系型多示例学习问题。

Method: 提出预算关系多示例学习（BR-MIL）框架，包含PAIR-Former模型：1）廉价全池扫描；2）在CPU上选择最多K个多样化的候选靶位点；3）使用置换不变Set Transformer聚合器处理选定标记。

Result: 在miRAW数据集上，PAIR-Former在实用操作预算（K*=64）下优于强基线方法，同时提供随K变化的可控准确率-计算量权衡。

Conclusion: PAIR-Former为大规模关系型多示例学习问题提供了有效的预算感知解决方案，理论分析表明选择预算K既影响近似误差又控制关系组件的泛化性能。

Abstract: Functional miRNA--mRNA targeting is a large-bag prediction problem: each transcript yields a heavy-tailed pool of candidate target sites (CTSs), yet only a pair-level label is observed. We formalize this regime as \emph{Budgeted Relational Multi-Instance Learning (BR-MIL)}, where at most $K$ instances per bag may receive expensive encoding and relational processing under a hard compute budget. We propose \textbf{PAIR-Former} (Pool-Aware Instance-Relational Transformer), a BR-MIL pipeline that performs a cheap full-pool scan, selects up to $K$ diverse CTSs on CPU, and applies a permutation-invariant Set Transformer aggregator on the selected tokens. On miRAW, PAIR-Former outperforms strong pooling baselines at a practical operating budget ($K^\star{=}64$) while providing a controllable accuracy--compute trade-off as $K$ varies. We further provide theory linking budgeted selection to (i) approximation error decreasing with $K$ and (ii) generalization terms governed by $K$ in the expensive relational component.

</details>


### [564] [Parallel Stochastic Gradient-Based Planning for World Models](https://arxiv.org/abs/2602.00475)
*Michael Psenka,Michael Rabbat,Aditi Krishnapriyan,Yann LeCun,Amir Bar*

Main category: cs.LG

TL;DR: GRASP是一种基于可微分世界模型的并行规划器，通过虚拟状态优化和随机性引入解决视觉输入的长时域控制任务


<details>
  <summary>Details</summary>
Motivation: 世界模型可以从原始视觉输入模拟环境动态，但用于规划时面临搜索空间巨大且非结构化的挑战。现有规划方法在长时域视觉任务上效率有限。

Method: 提出GRASP规划器：1)将状态视为优化变量（虚拟状态），使用软动力学约束；2)引入状态随机性促进探索；3)修改梯度结构，仅需动作输入梯度，缓解高维视觉世界模型的梯度敏感问题。

Result: 在基于视频的世界模型实验中，GRASP在长时域任务上优于交叉熵方法(CEM)和普通梯度优化(GD)，成功率和收敛时间均有提升。

Conclusion: GRASP通过可微分世界模型的并行优化，为视觉输入的长时域控制任务提供了高效规划方案，可视为随机化的非凝聚或配点最优控制器。

Abstract: World models simulate environment dynamics from raw sensory inputs like video. However, using them for planning can be challenging due to the vast and unstructured search space. We propose a robust and highly parallelizable planner that leverages the differentiability of the learned world model for efficient optimization, solving long-horizon control tasks from visual input. Our method treats states as optimization variables ("virtual states") with soft dynamics constraints, enabling parallel computation and easier optimization. To facilitate exploration and avoid local optima, we introduce stochasticity into the states. To mitigate sensitive gradients through high-dimensional vision-based world models, we modify the gradient structure to descend towards valid plans while only requiring action-input gradients. Our planner, which we call GRASP (Gradient RelAxed Stochastic Planner), can be viewed as a stochastic version of a non-condensed or collocation-based optimal controller. We provide theoretical justification and experiments on video-based world models, where our resulting planner outperforms existing planning algorithms like the cross-entropy method (CEM) and vanilla gradient-based optimization (GD) on long-horizon experiments, both in success rate and time to convergence.

</details>


### [565] [Diffusion LMs Can Approximate Optimal Infilling Lengths Implicitly](https://arxiv.org/abs/2602.00476)
*Hengchang Liu,Zhao Yang,Bing Su*

Main category: cs.LG

TL;DR: CAL方法通过利用扩散语言模型在第一步去噪中的统计信号，实现了无需训练的自适应长度代码和文本填充，显著提升了填充性能。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型虽然天生适合填充任务，但其性能受到预指定填充长度的限制。研究发现DLMs具有发现正确填充长度的内在能力，但这一信号被长度偏差所掩盖。

Method: CAL方法通过识别第一步去噪置信度中的两个关键统计现象：在真实长度附近出现的局部"Oracle Peak"和常常掩盖这一信号的系统性"Length Bias"。通过利用这一信号并校准偏差，CAL能够在正式解码前通过高效搜索近似最优长度。

Result: CAL在代码填充中比固定长度基线提升Pass@1高达47.7%，比基于聊天的自适应方法提升40.5%；在文本填充中提升BLEU-2和ROUGE-L分别高达8.5%和9.9%。

Conclusion: CAL为无需专门训练的稳健DLM填充铺平了道路，证明了DLMs具有发现正确填充长度的内在能力，通过校准偏差可以显著提升填充性能。

Abstract: Diffusion language models (DLMs) provide a bidirectional generation framework naturally suited for infilling, yet their performance is constrained by the pre-specified infilling length. In this paper, we reveal that DLMs possess an inherent ability to discover the correct infilling length. We identify two key statistical phenomena in the first-step denoising confidence: a local \textit{Oracle Peak} that emerges near the ground-truth length and a systematic \textit{Length Bias} that often obscures this signal. By leveraging this signal and calibrating the bias, our training-free method \textbf{CAL} (\textbf{C}alibrated \textbf{A}daptive \textbf{L}ength) enables DLMs to approximate the optimal length through an efficient search before formal decoding. Empirical evaluations demonstrate that CAL improves Pass@1 by up to 47.7\% over fixed-length baselines and 40.5\% over chat-based adaptive methods in code infilling, while boosting BLEU-2 and ROUGE-L by up to 8.5\% and 9.9\% in text infilling. These results demonstrate that CAL paves the way for robust DLM infilling without requiring any specialized training. Code is available at https://github.com/NiuHechang/Calibrated_Adaptive_Length.

</details>


### [566] [AREAL-DTA: Dynamic Tree Attention for Efficient Reinforcement Learning of Large Language Models](https://arxiv.org/abs/2602.00482)
*Jiarui Zhang,Yuchen Yang,Ran Yan,Zhiyu Mei,Liyuan Zhang,Daifeng Li,Wei Fu,Jiaxuan Gao,Shusheng Xu,Yi Wu,Binhang Yuan*

Main category: cs.LG

TL;DR: AREAL-DTA：一种基于深度优先搜索的动态树注意力机制，通过共享前缀树结构优化RL训练中的计算效率，实现高达8.31倍的训练吞吐提升


<details>
  <summary>Details</summary>
Motivation: 现有RL框架在处理LLM后训练时，由于独立处理共享长前缀的rollout序列，导致大量重复计算和内存浪费，计算效率低下

Method: 提出AREAL-DTA方法：1）采用DFS执行策略动态遍历rollout前缀树，每次只物化单条根到叶路径；2）引入负载均衡分布式批处理机制，在多GPU上动态构建和处理前缀树

Result: 在流行的RL后训练任务中，AREAL-DTA实现了高达8.31倍的τ²-bench训练吞吐提升

Conclusion: AREAL-DTA通过高效利用RL训练中的前缀共享，显著提升了计算效率和内存利用率，为大规模LLM的RL后训练提供了有效的优化方案

Abstract: Reinforcement learning (RL) based post-training for large language models (LLMs) is computationally expensive, as it generates many rollout sequences that could frequently share long token prefixes. Existing RL frameworks usually process these sequences independently, repeatedly recomputing identical prefixes during forward and backward passes during policy model training, leading to substantial inefficiencies in computation and memory usage. Although prefix sharing naturally induces a tree structure over rollouts, prior tree-attention-based solutions rely on fully materialized attention masks and scale poorly in RL settings. In this paper, we introduce AREAL-DTA to efficiently exploit prefix sharing in RL training. AREAL-DTA employs a depth-first-search (DFS)-based execution strategy that dynamically traverses the rollout prefix tree during both forward and backward computation, materializing only a single root-to-leaf path at a time. To further improve scalability, AREAL-DTA incorporates a load-balanced distributed batching mechanism that dynamically constructs and processes prefix trees across multiple GPUs. Across the popular RL post-training workload, AREAL-DTA achieves up to $8.31\times$ in $τ^2$-bench higher training throughput.

</details>


### [567] [OD-DEAL: Dynamic Expert-Guided Adversarial Learning with Online Decomposition for Scalable Capacitated Vehicle Routing](https://arxiv.org/abs/2602.00488)
*Dongbin Jiao,Zisheng Chen,Xianyi Wang,Jintao Shi,Shengcai Liu,Shi Yan*

Main category: cs.LG

TL;DR: OD-DEAL是一个对抗学习框架，通过混合遗传搜索和在线重心聚类分解，结合知识蒸馏，实现大规模CVRP问题的实时高质量求解。


<details>
  <summary>Details</summary>
Motivation: 大规模容量约束车辆路径问题（CVRP）面临传统启发式算法复杂度高、神经求解器在大规模图上泛化能力有限的双重挑战，需要一种既能保证求解质量又能实现实时推理的方法。

Method: 提出OD-DEAL对抗学习框架：1）紧密集成混合遗传搜索（HGS）和在线重心聚类（BCC）分解；2）通过高保真知识蒸馏将专家启发式行为转移到神经求解器；3）使用图注意力网络（GAT）作为生成策略，通过极小极大博弈训练；4）将分治策略蒸馏为密集代理奖励，实现无聚类的大规模实例推理。

Result: OD-DEAL实现了最先进的实时CVRP性能，能够求解10000个节点的大规模实例，具有接近恒定的神经缩放特性，实现了亚秒级、启发式质量的推理，满足动态大规模部署需求。

Conclusion: OD-DEAL通过对抗学习和知识蒸馏成功解决了大规模CVRP问题中神经求解器的泛化瓶颈，实现了高质量实时求解，为动态大规模物流优化提供了实用解决方案。

Abstract: Solving large-scale capacitated vehicle routing problems (CVRP) is hindered by the high complexity of heuristics and the limited generalization of neural solvers on massive graphs. We propose OD-DEAL, an adversarial learning framework that tightly integrates hybrid genetic search (HGS) and online barycenter clustering (BCC) decomposition, and leverages high-fidelity knowledge distillation to transfer expert heuristic behavior. OD-DEAL trains a graph attention network (GAT)-based generative policy through a minimax game, in which divide-and-conquer strategies from a hybrid expert are distilled into dense surrogate rewards. This enables high-quality, clustering-free inference on large-scale instances. Empirical results demonstrate that OD-DEAL achieves state-of-the-art (SOTA) real-time CVRP performance, solving 10000-node instances with near-constant neural scaling. This uniquely enables the sub-second, heuristic-quality inference required for dynamic large-scale deployment.

</details>


### [568] [Minerva: Reinforcement Learning with Verifiable Rewards for Cyber Threat Intelligence LLMs](https://arxiv.org/abs/2602.00513)
*Md Tanvirul Alam,Aritran Piplai,Ionut Cardei,Nidhi Rastogi,Peter J Worth*

Main category: cs.LG

TL;DR: 论文提出Minerva框架，利用可验证奖励的强化学习（RLVR）来提升网络安全威胁情报（CTI）的结构化信息提取任务，相比传统的监督微调方法在准确性和鲁棒性上表现更好。


<details>
  <summary>Details</summary>
Motivation: 网络安全威胁情报分析师需要将嘈杂的非结构化安全数据转换为标准化的自动化就绪表示。虽然大语言模型在此任务上显示潜力，但现有方法在生成结构化CTI输出时仍然脆弱，主要依赖监督微调。CTI标准和社区维护资源定义了规范的标识符和模式，能够对模型输出进行确定性验证。

Method: 提出Minerva框架，包含统一的数据集和训练管道，涵盖多个CTI子任务，每个任务都配有任务特定的验证器来评分结构化输出和标识符预测。为解决奖励稀疏性问题，提出轻量级自训练机制，生成额外的已验证轨迹并将其蒸馏回模型。

Result: 实验表明，在不同大语言模型骨干上，相比监督微调方法，该方法在多个基准测试中均显示出准确性和鲁棒性的持续改进。

Conclusion: 利用CTI任务中固有的结构化验证能力，通过可验证奖励的强化学习方法能够有效提升网络安全威胁情报提取任务的性能，为自动化CTI处理提供了更可靠的解决方案。

Abstract: Cyber threat intelligence (CTI) analysts routinely convert noisy, unstructured security artifacts into standardized, automation-ready representations. Although large language models (LLMs) show promise for this task, existing approaches remain brittle when producing structured CTI outputs and have largely relied on supervised fine-tuning (SFT). In contrast, CTI standards and community-maintained resources define canonical identifiers and schemas that enable deterministic verification of model outputs. We leverage this structure to study reinforcement learning with verifiable rewards (RLVR) for CTI tasks. We introduce \textit{Minerva}, a unified dataset and training pipeline spanning multiple CTI subtasks, each paired with task-specific verifiers that score structured outputs and identifier predictions. To address reward sparsity during rollout, we propose a lightweight self-training mechanism that generates additional verified trajectories and distills them back into the model. Experiments across LLM backbones show consistent improvements in accuracy and robustness over SFT across multiple benchmarks.

</details>


### [569] [Contrastive Learning for Privacy Enhancements in Industrial Internet of Things](https://arxiv.org/abs/2602.00515)
*Lin Liu,Rita Machacy,Simi Kuniyilh*

Main category: cs.LG

TL;DR: 本文对工业物联网(IIoT)中基于对比学习的隐私保护技术进行了全面综述，重点分析了工业数据特性、系统架构和应用场景，并探讨了解决方案、开放挑战及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 工业物联网(IIoT)在实现智能传感、通信和分析的同时，由于运营数据的敏感性，带来了显著的隐私和机密性风险。对比学习作为一种自监督表示学习范式，通过减少对标记数据和原始数据共享的依赖，为隐私保护分析提供了有前景的方法。

Method: 本文采用文献综述方法，系统性地回顾了工业物联网(IIoT)领域中基于对比学习的隐私保护技术。重点分析了工业数据的独特特性、系统架构以及各种应用场景，并对现有解决方案进行了梳理。

Result: 论文提供了工业物联网(IIoT)中基于对比学习的隐私保护技术的全面综述，明确了该领域的技术现状，识别了关键挑战，并建立了系统的分析框架。

Conclusion: 基于对比学习的隐私保护技术在工业物联网(IIoT)中具有重要应用价值，但仍面临诸多开放挑战。论文为未来研究提供了方向性指导，有助于推动该领域的技术发展和实际应用。

Abstract: The Industrial Internet of Things (IIoT) integrates intelligent sensing, communication, and analytics into industrial environments, including manufacturing, energy, and critical infrastructure. While IIoT enables predictive maintenance and cross-site optimization of modern industrial control systems, such as those in manufacturing and energy, it also introduces significant privacy and confidentiality risks due to the sensitivity of operational data. Contrastive learning, a self-supervised representation learning paradigm, has recently emerged as a promising approach for privacy-preserving analytics by reducing reliance on labeled data and raw data sharing. Although contrastive learning-based privacy-preserving techniques have been explored in the Internet of Things (IoT) domain, this paper offers a comprehensive review of these techniques specifically for privacy preservation in Industrial Internet of Things (IIoT) systems. It emphasizes the unique characteristics of industrial data, system architectures, and various application scenarios. Additionally, the paper discusses solutions and open challenges and outlines future research directions.

</details>


### [570] [NEST: Nested Event Stream Transformer for Sequences of Multisets](https://arxiv.org/abs/2602.00520)
*Minghui Sun,Haoyu Gong,Xingyu You,Jillian Hurst,Benjamin Goldstein,Matthew Engelhard*

Main category: cs.LG

TL;DR: NEST模型：针对具有层次结构的事件流数据（如电子健康记录）的Transformer架构，通过保持原始的多重集序列结构，提高计算效率和表示质量


<details>
  <summary>Details</summary>
Motivation: 事件流数据通常具有层次结构（多重集序列），但现有基础模型将其扁平化为一维序列，导致计算效率低下、学习虚假的集合内关系，以及下游任务中集合级表示质量较低

Method: 提出NEST（Nested Event Stream Transformer）模型，保持原始的多重集序列层次结构；引入Masked Set Modeling（MSM）预训练范式，促进更好的集合级表示学习

Result: 实验表明NEST能够捕捉真实世界动态，同时提高预训练效率和下游任务性能

Conclusion: 在基础模型架构中保持事件流数据的原始层次结构提供了有用的归纳偏置，既能提高计算效率，又能改善表示质量

Abstract: Event stream data often exhibit hierarchical structure in which multiple events co-occur, resulting in a sequence of multisets (i.e., bags of events). In electronic health records (EHRs), for example, medical events are grouped into a sequence of clinical encounters with well-defined temporal structure, but the order and timing of events within each encounter may be unknown or unreliable. Most existing foundation models (FMs) for event stream data flatten this hierarchy into a one-dimensional sequence, leading to (i) computational inefficiency associated with dense attention and learning spurious within-set relationships, and (ii) lower-quality set-level representations from heuristic post-training pooling for downstream tasks. Here, we show that preserving the original hierarchy in the FM architecture provides a useful inductive bias that improves both computational efficiency and representation quality. We then introduce Nested Event Stream Transformer (NEST), a FM for event streams comprised of sequences of multisets. Building on this architecture, we formulate Masked Set Modeling (MSM), an efficient paradigm that promotes improved set-level representation learning. Experiments on real-world multiset sequence data show that NEST captures real-world dynamics while improving both pretraining efficiency and downstream performance.

</details>


### [571] [Physiology as Language: Translating Respiration to Sleep EEG](https://arxiv.org/abs/2602.00526)
*Kaiwen Zha,Chao Li,Hao He,Peng Cao,Tianhong Li,Ali Mirzazadeh,Ellen Zhang,Jong Woo Lee,Yoon Kim,Dina Katabi*

Main category: cs.LG

TL;DR: 提出跨生理信号翻译任务：从呼吸信号合成睡眠脑电图，通过波形条件生成框架和离散标记化处理模态差异，在28000+个体数据上训练，支持年龄估计、性别检测和睡眠分期等下游任务。


<details>
  <summary>Details</summary>
Motivation: 探索从呼吸信号合成睡眠脑电图的跨生理信号翻译任务，解决两种模态之间的显著复杂性差异，实现非接触式远程神经评估的可行性。

Method: 提出波形条件生成框架，通过离散标记化约束脑电图目标空间，同时保留细粒度呼吸动态特征，支持从呼吸信号合成脑电图。

Result: 模型在脑电图频谱图重建上达到7%的平均绝对误差；合成脑电图在下游任务中表现接近真实脑电图：年龄估计（MAE 5.0 vs 5.1年）、性别检测（AUROC 0.81 vs 0.82）、睡眠分期（准确率0.84 vs 0.88），显著优于直接在呼吸数据上训练的基线。

Conclusion: 该框架成功实现从呼吸信号合成脑电图，并推广到无线射频反射的非接触式传感，展示了睡眠期间远程非接触神经评估的可行性。

Abstract: This paper introduces a novel cross-physiology translation task: synthesizing sleep electroencephalography (EEG) from respiration signals. To address the significant complexity gap between the two modalities, we propose a waveform-conditional generative framework that preserves fine-grained respiratory dynamics while constraining the EEG target space through discrete tokenization. Trained on over 28,000 individuals, our model achieves a 7% Mean Absolute Error in EEG spectrogram reconstruction. Beyond reconstruction, the synthesized EEG supports downstream tasks with performance comparable to ground truth EEG on age estimation (MAE 5.0 vs. 5.1 years), sex detection (AUROC 0.81 vs. 0.82), and sleep staging (Accuracy 0.84 vs. 0.88), significantly outperforming baselines trained directly on breathing. Finally, we demonstrate that the framework generalizes to contactless sensing by synthesizing EEG from wireless radio-frequency reflections, highlighting the feasibility of remote, non-contact neurological assessment during sleep.

</details>


### [572] [Convergent World Representations and Divergent Tasks](https://arxiv.org/abs/2602.00533)
*Core Francisco Park*

Main category: cs.LG

TL;DR: 多任务训练能促进世界表征的几何对齐，但某些"发散性任务"会损害新实体的表征整合和泛化能力


<details>
  <summary>Details</summary>
Motivation: 研究神经表征的几何特性及其在下游任务适应性的作用机制，目前对这些条件理解不足

Method: 构建受控框架：用5075个城市坐标定义"世界"，7个几何任务生成自回归训练数据，研究多任务训练对表征几何的影响，并通过微调测试新实体整合能力

Result: 不同任务产生不同的表征几何，但多任务训练使表征对齐；然而某些"发散性任务"会损害新实体的表征整合和泛化，即使经过多任务预训练

Conclusion: 多任务关系训练能可靠产生收敛的世界表征，但隐藏的发散性任务可能通过微调灾难性地损害新实体的整合能力

Abstract: While neural representations are central to modern deep learning, the conditions governing their geometry and their roles in downstream adaptability remain poorly understood. We develop a framework clearly separating the underlying world, the data generation process and the resulting model representations to study these questions in a controlled setup. 5,075 city coordinates define the world and 7 geometric tasks generate the training data for autoregressive training. We find that different tasks give rise to qualitatively and quantitatively distinct world representation geometries. However, multi-task training drives convergence of world representations: models trained on non-overlapping tasks develop aligned geometric representations, providing controlled evidence for the Multitask Scaling Hypothesis of the Platonic Representation Hypothesis. To study adaptation, we pretrain models on all tasks, then test whether new entities (cities) can be consistently integrated into the representation space via fine-tuning. Surprisingly, we find that despite multi-task pretraining, some tasks, which we call divergent, actively harm the representational integration of new entities and harm generalization. Our results show that training on multiple relational tasks reliably produces convergent world representations, but lurking divergent tasks can catastrophically harm new entity integration via fine-tuning.

</details>


### [573] [Invertible Memory Flow Networks](https://arxiv.org/abs/2602.00535)
*Liyu Zerihun,Alexandr Plashchinsky*

Main category: cs.LG

TL;DR: IMFN通过二叉树分解解决长序列压缩问题，将复杂的端到端压缩分解为简单的2对1合并任务，实现对数深度和次线性误差累积


<details>
  <summary>Details</summary>
Motivation: 长序列神经记忆仍然是一个挑战性问题。RNN及其变体存在梯度消失问题，Transformer存在二次方缩放问题。此外，将长序列压缩为有限固定表示由于优化困难而难以解决

Method: IMFN通过因子分解使长序列压缩变得可行：使用"清扫器"模块的二叉树结构，将问题分解为成对合并，每个清扫器学习更简单的2对1压缩任务，实现O(log N)深度和次线性误差累积

Result: 在长MNIST序列和UCF-101视频上的实证结果验证了IMFN能够压缩长序列中的高维数据

Conclusion: IMFN通过分解策略有效解决了长序列压缩的优化困难，实现了高效的长序列记忆处理

Abstract: Long sequence neural memory remains a challenging problem. RNNs and their variants suffer from vanishing gradients, and Transformers suffer from quadratic scaling. Furthermore, compressing long sequences into a finite fixed representation remains an intractable problem due to the difficult optimization landscape. Invertible Memory Flow Networks (IMFN) make long sequence compression tractable through factorization: instead of learning end-to-end compression, we decompose the problem into pairwise merges using a binary tree of "sweeper" modules. Rather than learning to compress long sequences, each sweeper learns a much simpler 2-to-1 compression task, achieving O(log N) depth with sublinear error accumulation in sequence length. For online inference, we distilled into a constant-cost recurrent student achieving O(1) sequential steps. Empirical results validate IMFN on long MNIST sequences and UCF-101 videos, demonstrating compression of high-dimensional data over long sequences.

</details>


### [574] [OpenDDI: A Comprehensive Benchmark for DDI Prediction](https://arxiv.org/abs/2602.00539)
*Xinmo Jin,Bowen Fan,Xunkai Li,Henan Sun,YuXin Zeng,Zekai Chen,Yuxuan Sun,Jia Li,Qiangqiang Dai,Hongchao Qin,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: OpenDDI是一个全面的药物相互作用预测基准，统一了多个数据集和评估标准，提供了标准化的大规模评估框架


<details>
  <summary>Details</summary>
Motivation: 药物相互作用预测面临两大挑战：1）缺乏高质量数据（小规模数据集和单模态药物表示）；2）缺乏标准化评估（不一致的场景、指标和基线）。这些限制了该领域的进一步发展

Method: OpenDDI从两个角度构建基准：1）数据角度：统一6个常用DDI数据集和2种现有药物表示，新增3个大规模LLM增强数据集和涵盖5种模态的多模态药物表示；2）评估角度：统一20个SOTA模型基线，涵盖3个下游任务，制定数据质量、有效性、泛化性、鲁棒性和效率的标准化协议

Result: 基于OpenDDI进行了全面评估，获得了10个有价值的DDI预测见解，同时揭示了当前局限性，为这个快速发展的领域提供了关键指导

Conclusion: OpenDDI为DDI预测提供了一个全面的基准，解决了数据质量和评估标准化问题，为该领域的未来发展提供了重要框架和指导

Abstract: Drug-Drug Interactions (DDIs) significantly influence therapeutic efficacy and patient safety. As experimental discovery is resource-intensive and time-consuming, efficient computational methodologies have become essential. The predominant paradigm formulates DDI prediction as a drug graph-based link prediction task. However, further progress is hindered by two fundamental challenges: (1) lack of high-quality data: most studies rely on small-scale DDI datasets and single-modal drug representations; (2) lack of standardized evaluation: inconsistent scenarios, varied metrics, and diverse baselines. To address the above issues, we propose OpenDDI, a comprehensive benchmark for DDI prediction. Specifically, (1) from the data perspective, OpenDDI unifies 6 widely used DDI datasets and 2 existing forms of drug representation, while additionally contributing 3 new large-scale LLM-augmented datasets and a new multimodal drug representation covering 5 modalities. (2) From the evaluation perspective, OpenDDI unifies 20 SOTA model baselines across 3 downstream tasks, with standardized protocols for data quality, effectiveness, generalization, robustness, and efficiency. Based on OpenDDI, we conduct a comprehensive evaluation and derive 10 valuable insights for DDI prediction while exposing current limitations to provide critical guidance for this rapidly evolving field. Our code is available at https://github.com/xiaoriwuguang/OpenDDI

</details>


### [575] [One Loss to Rule Them All: Marked Time-to-Event for Structured EHR Foundation Models](https://arxiv.org/abs/2602.00541)
*Zilin Jing,Vincent Jeanselme,Yuta Kobayashi,Simon A. Lee,Chao Pang,Aparajita Kashyap,Yanwei Li,Xinzhuo Jiang,Shalmali Joshi*

Main category: cs.LG

TL;DR: ORA提出一种标记时间到事件预训练目标，联合建模事件时间和相关测量，相比传统下一个令牌预测能更好捕捉电子健康记录结构，获得更泛化的表示。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录中的临床事件具有不规则采样特性，包含离散事件和数值测量混合数据。传统基于下一个令牌预测的预训练方法无法充分捕捉EHR的完整结构，需要更合适的预训练目标。

Method: 提出ORA（标记时间到事件预训练目标），联合建模事件发生时间和相关连续测量值，同时考虑时间间隔和测量数值，更好地捕捉EHR的时序和数值特征。

Result: 在多个数据集、下游任务和模型架构上，ORA相比下一个令牌预测和忽略连续测量的预训练损失，能产生更泛化的表示。改进不仅限于分类评估，还包括回归和时间到事件预测任务。

Conclusion: ORA为EHR基础模型提供了新的预训练目标家族，更重要的是表明：考虑EHR结构的预训练目标对于扩展下游任务能力和提升模型泛化性至关重要。

Abstract: Clinical events captured in Electronic Health Records (EHR) are irregularly sampled and may consist of a mixture of discrete events and numerical measurements, such as laboratory values or treatment dosages. The sequential nature of EHR, analogous to natural language, has motivated the use of next-token prediction to train prior EHR Foundation Models (FMs) over events. However, this training fails to capture the full structure of EHR. We propose ORA, a marked time-to-event pretraining objective that jointly models event timing and associated measurements. Across multiple datasets, downstream tasks, and model architectures, this objective consistently yields more generalizable representations than next-token prediction and pretraining losses that ignore continuous measurements. Importantly, the proposed objective yields improvements beyond traditional classification evaluation, including better regression and time-to-event prediction. Beyond introducing a new family of FMs, our results suggest a broader takeaway: pretraining objectives that account for EHR structure are critical for expanding downstream capabilities and generalizability

</details>


### [576] [Depth, Not Data: An Analysis of Hessian Spectral Bifurcation](https://arxiv.org/abs/2602.00545)
*Shenyang Deng,Boyao Liao,Zhuoli Ouyang,Tianyu Pang,Yaoqing Yang*

Main category: cs.LG

TL;DR: 论文挑战了传统观点，证明深度神经网络Hessian矩阵的"bulk-and-spike"谱结构不仅源于数据协方差不平衡，更主要来自网络架构本身，且谱间隙随网络深度线性增长。


<details>
  <summary>Details</summary>
Motivation: 传统研究将深度神经网络Hessian矩阵的"bulk-and-spike"谱结构归因于数据协方差矩阵的不平衡。本文旨在挑战这一观点，探究网络架构本身是否也能产生这种谱分岔现象。

Method: 采用深度线性网络设置，在数据协方差完全平衡的条件下，分析Hessian矩阵的谱结构。通过理论证明，即使数据协方差完美平衡，Hessian仍会呈现分岔特征值结构。

Result: 研究发现：1）即使数据协方差完美平衡，Hessian仍表现出分岔特征值结构（主导簇和主体簇）；2）主导特征值与主体特征值之比随网络深度线性增长；3）谱间隙主要受网络架构而非数据分布影响。

Conclusion: 深度神经网络的谱结构不仅受数据分布影响，更主要受网络架构影响。在设计深度网络优化算法时，应同时考虑模型架构和数据特性。

Abstract: The eigenvalue distribution of the Hessian matrix plays a crucial role in understanding the optimization landscape of deep neural networks. Prior work has attributed the well-documented ``bulk-and-spike'' spectral structure, where a few dominant eigenvalues are separated from a bulk of smaller ones, to the imbalance in the data covariance matrix. In this work, we challenge this view by demonstrating that such spectral Bifurcation can arise purely from the network architecture, independent of data imbalance.
  Specifically, we analyze a deep linear network setup and prove that, even when the data covariance is perfectly balanced, the Hessian still exhibits a Bifurcation eigenvalue structure: a dominant cluster and a bulk cluster. Crucially, we establish that the ratio between dominant and bulk eigenvalues scales linearly with the network depth. This reveals that the spectral gap is strongly affected by the network architecture rather than solely by data distribution. Our results suggest that both model architecture and data characteristics should be considered when designing optimization algorithms for deep networks.

</details>


### [577] [Contrastive Domain Generalization for Cross-Instrument Molecular Identification in Mass Spectrometry](https://arxiv.org/abs/2602.00547)
*Seunghyun Yoo,Sanghong Kim,Namkyung Yoon,Hwangnam Kim*

Main category: cs.LG

TL;DR: 提出跨模态对齐框架，将质谱直接映射到预训练化学语言模型的分子结构嵌入空间，解决质谱识别中的泛化瓶颈


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法将质谱匹配视为封闭集识别任务，难以泛化到未见过的分子骨架结构，存在语义鸿沟问题

Method: 跨模态对齐框架，将质谱数据直接映射到预训练化学语言模型的分子结构嵌入空间，整合物理谱分辨率和分子结构嵌入

Result: 在严格骨架不相交基准上，Top-1准确率达42.2%（256路零样本检索），全局检索设置下泛化能力强，学习到的嵌入空间化学一致性达95.4%（5路5样本分子重识别）

Conclusion: 显式整合物理谱分辨率和分子结构嵌入是解决质谱数据分子识别泛化瓶颈的关键

Abstract: Identifying molecules from mass spectrometry (MS) data remains a fundamental challenge due to the semantic gap between physical spectral peaks and underlying chemical structures. Existing deep learning approaches often treat spectral matching as a closed-set recognition task, limiting their ability to generalize to unseen molecular scaffolds. To overcome this limitation, we propose a cross-modal alignment framework that directly maps mass spectra into the chemically meaningful molecular structure embedding space of a pretrained chemical language model. On a strict scaffold-disjoint benchmark, our model achieves a Top-1 accuracy of 42.2% in fixed 256-way zero-shot retrieval and demonstrates strong generalization under a global retrieval setting. Moreover, the learned embedding space demonstrates strong chemical coherence, reaching 95.4% accuracy in 5-way 5-shot molecular re-identification. These results suggest that explicitly integrating physical spectral resolution with molecular structure embedding is key to solving the generalization bottleneck in molecular identification from MS data.

</details>


### [578] [Beyond the Node: Clade-level Selection for Efficient MCTS in Automatic Heuristic Design](https://arxiv.org/abs/2602.00549)
*Kezhao Lai,Yutao Lai,Hai-Lin Liu*

Main category: cs.LG

TL;DR: Clade-AHD框架用clade级贝叶斯信念替代MCTS中的节点级点估计，通过Thompson采样解决LLM自动启发式设计中MCTS的过度开发问题，在有限计算预算下实现更可靠的探索


<details>
  <summary>Details</summary>
Motivation: MCTS在LLM自动启发式设计中存在过度开发倾向，特别是在有限计算预算下进行启发式评估时，这限制了其性能表现

Method: 提出Clade-AHD框架：1）将后代评估聚合成Beta分布形成clade级贝叶斯信念；2）基于这些信念进行Thompson采样；3）显式建模不确定性来指导探索

Result: 在复杂组合优化问题上的大量实验表明，Clade-AHD始终优于最先进方法，同时显著降低计算成本

Conclusion: Clade-AHD通过clade级贝叶斯信念和Thompson采样有效解决了MCTS在有限计算预算下的过度开发问题，为LLM自动启发式设计提供了更高效的框架

Abstract: While Monte Carlo Tree Search (MCTS) shows promise in Large Language Model (LLM) based Automatic Heuristic Design (AHD), it suffers from a critical over-exploitation tendency under the limited computational budgets required for heuristic evaluation. To address this limitation, we propose Clade-AHD, an efficient framework that replaces node-level point estimates with clade-level Bayesian beliefs. By aggregating descendant evaluations into Beta distributions and performing Thompson Sampling over these beliefs, Clade-AHD explicitly models uncertainty to guide exploration, enabling more reliable decision-making under sparse and noisy evaluations. Extensive experiments on complex combinatorial optimization problems demonstrate that Clade-AHD consistently outperforms state-of-the-art methods while significantly reducing computational cost. The source code is publicly available at: https://github.com/Mriya0306/Clade-AHD.

</details>


### [579] [Forget by Uncertainty: Orthogonal Entropy Unlearning for Quantized Neural Networks](https://arxiv.org/abs/2602.00567)
*Tian Zhang,Yujia Tong,Junhao Dong,Ke Xu,Yuze Wang,Jingling Yuan*

Main category: cs.LG

TL;DR: OEU提出正交熵遗忘框架，通过熵最大化实现真正遗忘，使用梯度正交投影避免干扰，在量化模型中实现高效机器遗忘。


<details>
  <summary>Details</summary>
Motivation: 量化神经网络在边缘设备部署与GDPR等隐私法规结合，迫切需要量化模型的机器遗忘能力。现有方法存在关键问题：通过训练模型记忆错误标签来诱导遗忘，混淆了遗忘与错误记忆；使用标量梯度重加权无法解决梯度间的方向冲突。

Method: 提出正交熵遗忘框架OEU：1) 熵引导遗忘：最大化遗忘数据的预测不确定性，实现真正遗忘而非错误预测；2) 梯度正交投影：将遗忘梯度投影到保留梯度的正交补空间，在一阶近似下提供效用保持的理论保证。

Result: 大量实验表明，OEU在遗忘效果和保留准确性方面均优于现有方法。

Conclusion: OEU框架通过熵最大化实现真正遗忘，结合梯度正交投影避免干扰，为量化神经网络提供了有效的机器遗忘解决方案。

Abstract: The deployment of quantized neural networks on edge devices, combined with privacy regulations like GDPR, creates an urgent need for machine unlearning in quantized models. However, existing methods face critical challenges: they induce forgetting by training models to memorize incorrect labels, conflating forgetting with misremembering, and employ scalar gradient reweighting that cannot resolve directional conflicts between gradients. We propose OEU, a novel Orthogonal Entropy Unlearning framework with two key innovations: 1) Entropy-guided unlearning maximizes prediction uncertainty on forgotten data, achieving genuine forgetting rather than confident misprediction, and 2) Gradient orthogonal projection eliminates interference by projecting forgetting gradients onto the orthogonal complement of retain gradients, providing theoretical guarantees for utility preservation under first-order approximation. Extensive experiments demonstrate that OEU outperforms existing methods in both forgetting effectiveness and retain accuracy.

</details>


### [580] [When Classes Evolve: A Benchmark and Framework for Stage-Aware Class-Incremental Learning](https://arxiv.org/abs/2602.00573)
*Zheng Zhang,Tao Hu,Xueheng Li,Yang Wang,Rui Li,Jie Zhang,Chengjun Xie*

Main category: cs.LG

TL;DR: 论文提出Stage-CIL范式，解决类别增量学习中类内形态演化问题，引入Stage-Bench数据集和STAGE方法，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统类别增量学习假设类别形态静态，忽略了类内演化现象（如幼虫变蝴蝶），导致模型无法适应同一语义类别的形态变化。

Method: 提出Stage-CIL范式，引入Stage-Bench数据集评估类间和类内遗忘，开发STAGE方法学习抽象可转移的演化模式，解耦语义身份和转换动态。

Result: STAGE方法在10个领域、2阶段的数据集上显著优于现有最先进方法，能同时处理类间区分和类内形态适应。

Conclusion: 类内形态演化是类别增量学习的重要挑战，Stage-CIL范式和STAGE方法能有效解决这一问题，实现更鲁棒的持续学习。

Abstract: Class-Incremental Learning (CIL) aims to sequentially learn new classes while mitigating catastrophic forgetting of previously learned knowledge. Conventional CIL approaches implicitly assume that classes are morphologically static, focusing primarily on preserving previously learned representations as new classes are introduced. However, this assumption neglects intra-class evolution: a phenomenon wherein instances of the same semantic class undergo significant morphological transformations, such as a larva turning into a butterfly. Consequently, a model must both discriminate between classes and adapt to evolving appearances within a single class. To systematically address this challenge, we formalize Stage-Aware CIL (Stage-CIL), a paradigm in which each class is learned progressively through distinct morphological stages. To facilitate rigorous evaluation within this paradigm, we introduce the Stage-Bench, a 10-domain, 2-stages dataset and protocol that jointly measure inter- and intra-class forgetting. We further propose STAGE, a novel method that explicitly learns abstract and transferable evolution patterns within a fixed-size memory pool. By decoupling semantic identity from transformation dynamics, STAGE enables accurate prediction of future morphologies based on earlier representations. Extensive empirical evaluation demonstrates that STAGE consistently and substantially outperforms existing state-of-the-art approaches, highlighting its effectiveness in simultaneously addressing inter-class discrimination and intra-class morphological adaptation.

</details>


### [581] [Data Distribution as a Lever for Guiding Optimizers Toward Superior Generalization in LLMs](https://arxiv.org/abs/2602.00576)
*Tushaar Gangavarapu,Jiping Li,Christopher Vattheuer,Zhangyang Wang,Baharan Mirzasoleiman*

Main category: cs.LG

TL;DR: 通过调整训练数据分布（上采样或增强后期学习的样本）可以减少简单性偏置，从而提升大语言模型的泛化性能，在数学推理任务上获得高达18%的相对准确率提升。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过修改训练数据分布来引导优化器找到具有更好泛化性能的解决方案，特别是针对训练大型语言模型时，SAM优化器虽然泛化性能好但计算成本过高的问题。

Method: 理论分析基于上下文线性回归模型和多头线性自注意力，比较GD和SAM的训练动态，发现SAM能降低简单性偏置。基于此洞察，通过上采样或增强训练后期学习的样本来调整训练数据分布。

Result: 实验表明该策略能显著提升多个LLM（包括Phi2-2.7B、Llama3.2-1B、Gemma3-1B-PT和Qwen3-0.6B-Base）的性能，在使用AdamW和Muon进行微调时，在数学推理任务上获得高达18%的相对准确率提升。

Conclusion: 通过调整训练数据分布来降低简单性偏置是一种有效提升大语言模型泛化性能的方法，为训练更优的LLM提供了计算效率高的替代方案。

Abstract: Can modifying the training data distribution guide optimizers toward solutions with improved generalization when training large language models (LLMs)? In this work, we theoretically analyze an in-context linear regression model with multi-head linear self-attention, and compare the training dynamics of two gradient based optimizers, namely gradient descent (GD) and sharpness-aware minimization (SAM), the latter exhibiting superior generalization properties but is prohibitively expensive for training even medium-sized LLMs. We show, for the first time, that SAM induces a lower simplicity bias (SB)-the tendency of an optimizer to preferentially learn simpler features earlier in training-and identify this reduction as a key factor underlying its improved generalization performance. Motivated by this insight, we demonstrate that altering the training data distribution by upsampling or augmenting examples learned later in training similarly reduces SB and leads to improved generalization. Our extensive experiments show that our strategy improves the performance of multiple LLMs-including Phi2-2.7B , Llama3.2-1B, Gemma3-1B-PT, and Qwen3-0.6B-Base-achieving relative accuracy gains up to 18% when fine-tuned with AdamW and Muon on mathematical reasoning tasks.

</details>


### [582] [Sparsity-Aware Unlearning for Large Language Models](https://arxiv.org/abs/2602.00577)
*Yuze Wang,Yujia Tong,Ke Xu,Jingling Yuan,Jiawei Jiang,Chuang Hu*

Main category: cs.LG

TL;DR: 提出SAU方法解决稀疏化大语言模型中的隐私遗忘问题，通过梯度掩码和重要性感知重分配，在稀疏模型上实现有效遗忘同时保持模型效用。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在训练中会记忆敏感信息，带来隐私风险。现有遗忘方法针对密集模型设计，忽略了模型稀疏化这一高效部署技术，导致在稀疏模型上遗忘效果显著下降。

Method: 提出Sparsity-Aware Unlearning (SAU)方法：1) 通过梯度掩码将更新重定向到存活的权重，解耦遗忘与稀疏化目标；2) 结合重要性感知重分配补偿被剪枝的参数。

Result: 大量实验表明，SAU在稀疏LLMs上显著优于现有方法，能实现有效遗忘同时保持模型效用。

Conclusion: SAU解决了稀疏大语言模型中的隐私遗忘挑战，通过专门设计的梯度掩码和重分配机制，在保持模型效率的同时实现有效的隐私保护。

Abstract: Large Language Models (LLMs) inevitably memorize sensitive information during training, posing significant privacy risks. Machine unlearning has emerged as a promising solution to selectively remove such information without full retraining. However, existing methods are designed for dense models and overlook model sparsification-an essential technique for efficient LLM deployment. We find that unlearning effectiveness degrades substantially on sparse models. Through empirical analysis, we reveal that this degradation occurs because existing unlearning methods require updating all parameters, yet sparsification prunes substantial weights to zero, fundamentally limiting the model's forgetting capacity. To address this challenge, we propose Sparsity-Aware Unlearning (SAU), which decouples unlearning from sparsification objectives through gradient masking that redirects updates to surviving weights, combined with importance-aware redistribution to compensate for pruned parameters. Extensive experiments demonstrate that SAU significantly outperforms existing methods on sparse LLMs, achieving effective forgetting while preserving model utility.

</details>


### [583] [Bridging Time and Frequency: A Joint Modeling Framework for Irregular Multivariate Time Series Forecasting](https://arxiv.org/abs/2602.00582)
*Xiangfei Qiu,Kangjia Yan,Xvyuan Liu,Xingjian Wu,Jilin Hu*

Main category: cs.LG

TL;DR: TFMixer是一个用于不规则多元时间序列预测的联合时频建模框架，通过可学习的非均匀离散傅里叶变换提取频域表示，并结合基于查询的补丁混合机制进行时域建模，最后融合时频表示进行预测。


<details>
  <summary>Details</summary>
Motivation: 不规则多元时间序列预测面临非均匀采样和变量异步性的挑战，这些不规则性违反了标准模型的等距假设，阻碍了局部时间建模，并使经典频域方法无法有效捕捉全局周期性结构。

Method: TFMixer包含三个核心组件：1）全局频率模块使用可学习的非均匀离散傅里叶变换直接从不规则时间戳提取频谱表示；2）局部时间模块引入基于查询的补丁混合机制自适应聚合信息性时间补丁；3）融合时域和频域表示生成预测，并利用逆NUDFT进行显式季节性外推。

Result: 在真实世界数据集上的大量实验表明，TFMixer实现了最先进的性能。

Conclusion: TFMixer通过联合时频建模有效解决了不规则多元时间序列预测的挑战，为非均匀采样数据提供了有效的预测框架。

Abstract: Irregular multivariate time series forecasting (IMTSF) is challenging due to non-uniform sampling and variable asynchronicity. These irregularities violate the equidistant assumptions of standard models, hindering local temporal modeling and rendering classical frequency-domain methods ineffective for capturing global periodic structures. To address this challenge, we propose TFMixer, a joint time-frequency modeling framework for IMTS forecasting. Specifically, TFMixer incorporates a Global Frequency Module that employs a learnable Non-Uniform Discrete Fourier Transform (NUDFT) to directly extract spectral representations from irregular timestamps. In parallel, the Local Time Module introduces a query-based patch mixing mechanism to adaptively aggregate informative temporal patches and alleviate information density imbalance. Finally, TFMixer fuses the time-domain and frequency-domain representations to generate forecasts and further leverages inverse NUDFT for explicit seasonal extrapolation. Extensive experiments on real-world datasets demonstrate the state--of-the-art performance of TFMixer.

</details>


### [584] [Safe Langevin Soft Actor Critic](https://arxiv.org/abs/2602.00587)
*Mahesh Keswani,Samyak Jain,Raunak P. Bhattacharyya*

Main category: cs.LG

TL;DR: SL-SAC是一种安全的强化学习算法，通过参数空间探索和分布风险控制来解决约束强化学习中奖励与安全的平衡问题，在Safety-Gymnasium基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 约束强化学习中奖励与安全的平衡仍然具有挑战性，主要问题包括：从尖锐值最小值泛化能力差，以及对重尾风险分布处理不足。

Method: 结合三个关键机制：1) 使用自适应随机梯度朗之万动力学(aSGLD)进行奖励批评家探索；2) 通过隐式分位数网络(IQN)和条件风险价值(CVaR)优化进行分布成本估计；3) 基于经验CVaR的反应性拉格朗日松弛方案。

Result: 在Safety-Gymnasium基准测试中，SL-SAC在10个任务中的7个实现了最低成本，同时保持竞争力的回报，在速度任务中成本降低了19-63%。

Conclusion: SL-SAC通过参数空间探索和分布风险控制，有效解决了约束强化学习中的泛化和风险分布问题，在安全性和性能之间取得了良好平衡。

Abstract: Balancing reward and safety in constrained reinforcement learning remains challenging due to poor generalization from sharp value minima and inadequate handling of heavy-tailed risk distribution. We introduce Safe Langevin Soft Actor-Critic (SL-SAC), a principled algorithm that addresses both issues through parameter-space exploration and distributional risk control. Our approach combines three key mechanisms: (1) Adaptive Stochastic Gradient Langevin Dynamics (aSGLD) for reward critics, promoting ensemble diversity and escape from poor optima; (2) distributional cost estimation via Implicit Quantile Networks (IQN) with Conditional Value-at-Risk (CVaR) optimization for tail-risk mitigation; and (3) a reactive Lagrangian relaxation scheme that adapts constraint enforcement based on the empirical CVaR of episodic costs. We provide theoretical guarantees on CVaR estimation error and demonstrate that CVaR-based Lagrange updates yield stronger constraint violation signals than expected-cost updates. On Safety-Gymnasium benchmarks, SL-SAC achieves the lowest cost in 7 out of 10 tasks while maintaining competitive returns, with cost reductions of 19-63% in velocity tasks compared to state-of-the-art baselines.

</details>


### [585] [SEER: Transformer-based Robust Time Series Forecasting via Automated Patch Enhancement and Replacement](https://arxiv.org/abs/2602.00589)
*Xiangfei Qiu,Xvyuan Liu,Tianen Shen,Xingjian Wu,Hanyin Cheng,Bin Yang,Jilin Hu*

Main category: cs.LG

TL;DR: SEER是一个鲁棒的时间序列预测框架，通过可学习的补丁替换模块动态过滤低质量补丁，提升预测准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于补丁的时间序列方法无法动态选择补丁，而现实世界时间序列常存在缺失值、分布偏移、异常值等低质量问题，导致某些补丁包含低质量信息，影响预测结果。

Method: 1. 增强嵌入模块：使用混合专家架构改进补丁级表示，通过通道自适应感知机制获得序列级标记表示。2. 可学习补丁替换模块：两阶段过程：动态过滤机制消除负面补丁级标记；替换注意力模块用全局序列级标记替换低质量补丁，并通过因果注意力机制精化表示。

Result: 全面的实验结果表明SEER达到了最先进的性能。

Conclusion: SEER框架通过动态过滤和替换低质量补丁，有效解决了时间序列数据质量问题，提升了预测的鲁棒性和准确性。

Abstract: Time series forecasting is important in many fields that require accurate predictions for decision-making. Patching techniques, commonly used and effective in time series modeling, help capture temporal dependencies by dividing the data into patches. However, existing patch-based methods fail to dynamically select patches and typically use all patches during the prediction process. In real-world time series, there are often low-quality issues during data collection, such as missing values, distribution shifts, anomalies and white noise, which may cause some patches to contain low-quality information, negatively impacting the prediction results. To address this issue, this study proposes a robust time series forecasting framework called SEER. Firstly, we propose an Augmented Embedding Module, which improves patch-wise representations using a Mixture-of-Experts (MoE) architecture and obtains series-wise token representations through a channel-adaptive perception mechanism. Secondly, we introduce a Learnable Patch Replacement Module, which enhances forecasting robustness and model accuracy through a two-stage process: 1) a dynamic filtering mechanism eliminates negative patch-wise tokens; 2) a replaced attention module substitutes the identified low-quality patches with global series-wise token, further refining their representations through a causal attention mechanism. Comprehensive experimental results demonstrate the SOTA performance of SEER.

</details>


### [586] [Kernelized Edge Attention: Addressing Semantic Attention Blurring in Temporal Graph Neural Networks](https://arxiv.org/abs/2602.00596)
*Govind Waghmare,Srini Rohan Gujulla Leel,Nikhil Tumbde,Sumedh B G,Sonia Gupta,Srikanta Bedathur*

Main category: cs.LG

TL;DR: KEAT提出了一种新的注意力机制，通过连续时间核函数调制边特征，解决TGNN中节点和边特征混淆的问题，提升动态图建模的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有TGNN模型在计算注意力时混合了节点和边的表示，但两者具有不同的时间行为：节点嵌入缓慢演化（长期结构上下文），而边特征反映瞬时的带时间戳的交互。这种不匹配导致语义注意力模糊，注意力权重无法区分缓慢漂移的节点状态和快速变化、信息丰富的边交互。

Method: 提出KEAT（Kernelized Edge Attention for Temporal Graphs），使用连续时间核函数（包括拉普拉斯核、RBF核和可学习的MLP变体）调制边特征。该方法保持节点和边的不同角色，可无缝集成到Transformer风格（如DyGFormer）和消息传递（如TGN）架构中。

Result: 在链接预测任务上，KEAT相比最近的DyGFormer实现了高达18%的MRR提升，相比TGN提升了7%。该方法实现了更准确、可解释和时间感知的消息传递。

Conclusion: KEAT通过核函数调制边特征，解决了TGNN中节点和边特征混淆的问题，显著提升了动态图建模的性能和可解释性，为时间图神经网络提供了更有效的注意力机制。

Abstract: Temporal Graph Neural Networks (TGNNs) aim to capture the evolving structure and timing of interactions in dynamic graphs. Although many models incorporate time through encodings or architectural design, they often compute attention over entangled node and edge representations, failing to reflect their distinct temporal behaviors. Node embeddings evolve slowly as they aggregate long-term structural context, while edge features reflect transient, timestamped interactions (e.g. messages, trades, or transactions). This mismatch results in semantic attention blurring, where attention weights cannot distinguish between slowly drifting node states and rapidly changing, information-rich edge interactions. As a result, models struggle to capture fine-grained temporal dependencies and provide limited transparency into how temporal relevance is computed. This paper introduces KEAT (Kernelized Edge Attention for Temporal Graphs), a novel attention formulation that modulates edge features using a family of continuous-time kernels, including Laplacian, RBF, and learnable MLP variant. KEAT preserves the distinct roles of nodes and edges, and integrates seamlessly with both Transformer-style (e.g., DyGFormer) and message-passing (e.g., TGN) architectures. It achieves up to 18% MRR improvement over the recent DyGFormer and 7% over TGN on link prediction tasks, enabling more accurate, interpretable and temporally aware message passing in TGNNs.

</details>


### [587] [Direct Preference Optimization with Rating Information: Practical Algorithms and Provable Gains](https://arxiv.org/abs/2602.00603)
*Luca Viano,Ruida Zhou,Yifan Sun,Mahdi Namazifar,Volkan Cevher,Shoham Sabach,Mohammad Ghavamzadeh*

Main category: cs.LG

TL;DR: 本文提出了一种改进的偏好优化算法，利用评分差距信息来加速对齐过程，相比传统DPO方法具有更快的统计收敛速度，同时对评分差距的不准确性具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统DPO算法仅使用二元偏好反馈（选择/拒绝），这种反馈形式虽然数据收集容易，但存在模糊性。例如，无法判断算法执行过程中偏好响应概率的下降是好是坏。作者希望利用评分差距（chosen response比rejected response好多少）这一额外信息来改进算法。

Method: 设计新的算法框架，利用评分差距信息来指导模型优化。算法不仅考虑二元偏好，还考虑偏好的强度（评分差距），从而更有效地调整模型参数。

Result: 新算法在准确评分差距信息下比DPO获得更快的统计收敛速度。理论证明和实验表明算法对评分差距的不准确性具有鲁棒性。在多种LLM和评估基准上，新方法表现优于多种DPO风格算法。

Conclusion: 利用评分差距信息可以显著改进偏好优化算法的效率和效果，为模型对齐提供了更强大的工具，同时保持了在实际应用中面对不完美数据的鲁棒性。

Abstract: The class of direct preference optimization (DPO) algorithms has emerged as a promising approach for solving the alignment problem in foundation models. These algorithms work with very limited feedback in the form of pairwise preferences and fine-tune models to align with these preferences without explicitly learning a reward model. While the form of feedback used by these algorithms makes the data collection process easy and relatively more accurate, its ambiguity in terms of the quality of responses could have negative implications. For example, it is not clear if a decrease (increase) in the likelihood of preferred (dispreferred) responses during the execution of these algorithms could be interpreted as a positive or negative phenomenon. In this paper, we study how to design algorithms that can leverage additional information in the form of rating gap, which informs the learner how much the chosen response is better than the rejected one. We present new algorithms that can achieve faster statistical rates than DPO in presence of accurate rating gap information. Moreover, we theoretically prove and empirically show that the performance of our algorithms is robust to inaccuracy in rating gaps. Finally, we demonstrate the solid performance of our methods in comparison to a number of DPO-style algorithms across a wide range of LLMs and evaluation benchmarks.

</details>


### [588] [Actor-Dual-Critic Dynamics for Zero-sum and Identical-Interest Stochastic Games](https://arxiv.org/abs/2602.00606)
*Ahmed Said Donmez,Yuksel Arslantas,Muhammed O. Sayin*

Main category: cs.LG

TL;DR: 提出了一种新颖的独立、基于收益的学习框架，用于随机博弈，该框架无需模型、与游戏无关且无需梯度。采用最佳响应型演员-评论家架构，通过快速评论家和慢速评论家更新策略，在双智能体零和与多智能体共同利益随机博弈中收敛到（近似）均衡。


<details>
  <summary>Details</summary>
Motivation: 现有随机博弈学习方法通常需要模型知识、梯度信息或中心化协调。本文旨在开发一种完全去中心化、基于收益的学习算法，能够在有限信息下实现收敛保证，适用于不同类型的随机博弈。

Method: 提出基于收益的学习框架，采用最佳响应型演员-评论家架构。智能体通过两个评论家更新策略：快速评论家直观响应观察到的收益，慢速评论家审慎近似底层动态规划问题解。学习过程通过平滑最佳响应实现非均衡适应。

Result: 理论证明在双智能体零和与多智能体共同利益随机博弈中收敛到（近似）均衡。这是首批在两种设置下都具有理论保证的完全去中心化、基于收益的学习算法之一。实证结果验证了该方法在两类游戏中的鲁棒性和有效性。

Conclusion: 提出了一种新颖的独立、基于收益的学习框架，能够在随机博弈中实现收敛保证，无需模型、梯度或中心化协调。该方法为随机博弈中的去中心化学习提供了理论保证和实证验证的有效解决方案。

Abstract: We propose a novel independent and payoff-based learning framework for stochastic games that is model-free, game-agnostic, and gradient-free. The learning dynamics follow a best-response-type actor-critic architecture, where agents update their strategies (actors) using feedback from two distinct critics: a fast critic that intuitively responds to observed payoffs under limited information, and a slow critic that deliberatively approximates the solution to the underlying dynamic programming problem. Crucially, the learning process relies on non-equilibrium adaptation through smoothed best responses to observed payoffs. We establish convergence to (approximate) equilibria in two-agent zero-sum and multi-agent identical-interest stochastic games over an infinite horizon. This provides one of the first payoff-based and fully decentralized learning algorithms with theoretical guarantees in both settings. Empirical results further validate the robustness and effectiveness of the proposed approach across both classes of games.

</details>


### [589] [Rethinking Zero-Shot Time Series Classification: From Task-specific Classifiers to In-Context Inference](https://arxiv.org/abs/2602.00620)
*Juntao Fang,Shifeng Xie,Shengbin Nie,Yuhui Ling,Yuming Liu,Zijian Li,Keli Zhang,Lujia Pan,Themis Palpanas,Ruichu Cai*

Main category: cs.LG

TL;DR: 提出TIC-FM框架，通过上下文学习实现时间序列基础模型的真正零样本评估，无需参数更新，在128个UCR数据集上表现优异


<details>
  <summary>Details</summary>
Motivation: 当前时间序列基础模型的零样本评估通常使用冻结编码器加任务特定分类器，但这违反了零样本部署的无训练前提，并引入了分类器依赖的训练选择导致的评估偏差

Method: 提出TIC-FM框架：结合时间序列编码器和轻量级投影适配器，使用分割掩码潜在记忆Transformer，将标记训练集作为上下文，在单次前向传播中预测所有测试实例的标签

Result: 在128个UCR数据集上表现出强大的准确性，在极低标签情况下获得一致增益，证明了训练无关的迁移能力

Conclusion: TIC-FM实现了真正零样本的时间序列基础模型评估，上下文学习可以替代训练分类器，并在单次前向传播中模拟基于梯度的分类器训练

Abstract: The zero-shot evaluation of time series foundation models (TSFMs) for classification typically uses a frozen encoder followed by a task-specific classifier. However, this practice violates the training-free premise of zero-shot deployment and introduces evaluation bias due to classifier-dependent training choices. To address this issue, we propose TIC-FM, an in-context learning framework that treats the labeled training set as context and predicts labels for all test instances in a single forward pass, without parameter updates. TIC-FM pairs a time series encoder and a lightweight projection adapter with a split-masked latent memory Transformer. We further provide theoretical justification that in-context inference can subsume trained classifiers and can emulate gradient-based classifier training within a single forward pass. Experiments on 128 UCR datasets show strong accuracy, with consistent gains in the extreme low-label situation, highlighting training-free transfer

</details>


### [590] [MoDEx: Mixture of Depth-specific Experts for Multivariate Long-term Time Series Forecasting](https://arxiv.org/abs/2602.00624)
*Hyekyung Yoon,Minhyuk Lee,Imseung Park,Myungjoo Kang*

Main category: cs.LG

TL;DR: MoDEx：一种轻量级深度专家混合模型，通过分析多层MLP骨干网络中不同层的敏感性，提出深度特定专家替代复杂骨干网络，在多元长期时间序列预测任务中实现SOTA性能，同时大幅减少参数量和计算资源。


<details>
  <summary>Details</summary>
Motivation: 现有长期时间序列预测（LTSF）方法采用嵌入-骨干网络精化-长期预测的三阶段流程，但骨干网络各层的行为尚未得到充分探索。作者希望深入理解不同层在建模时间动态方面的专门化作用，并基于此设计更高效的预测框架。

Method: 提出层敏感性（layer sensitivity）指标，基于GradCAM和有效感受野理论，量化每个时间点对层潜在特征的正负贡献。在三层MLP骨干网络分析中发现深度特定专门化现象，据此提出MoDEx（Mixture of Depth-specific Experts），用深度特定MLP专家替代复杂骨干网络。

Result: 在7个真实世界基准测试中达到最先进准确率，在78%的情况下排名第一，同时使用显著更少的参数和计算资源。MoDEx还能无缝集成到Transformer变体中，持续提升其性能，展示了作为高效高性能LTSF框架的强大泛化能力。

Conclusion: 通过分析骨干网络层敏感性揭示了深度特定专门化现象，基于此设计的MoDEx框架在长期时间序列预测任务中实现了准确性、效率和泛化性的平衡，为LTSF研究提供了新的视角和高效解决方案。

Abstract: Multivariate long-term time series forecasting (LTSF) supports critical applications such as traffic-flow management, solar-power scheduling, and electricity-transformer monitoring. The existing LTSF paradigms follow a three-stage pipeline of embedding, backbone refinement, and long-horizon prediction. However, the behaviors of individual backbone layers remain underexplored. We introduce layer sensitivity, a gradient-based metric inspired by GradCAM and effective receptive field theory, which quantifies both positive and negative contributions of each time point to a layer's latent features. Applying this metric to a three-layer MLP backbone reveals depth-specific specialization in modeling temporal dynamics in the input sequence. Motivated by these insights, we propose MoDEx, a lightweight Mixture of Depth-specific Experts, which replaces complex backbones with depth-specific MLP experts. MoDEx achieves state-of-the-art accuracy on seven real-world benchmarks, ranking first in 78 percent of cases, while using significantly fewer parameters and computational resources. It also integrates seamlessly into transformer variants, consistently boosting their performance and demonstrating robust generalizability as an efficient and high-performance LTSF framework.

</details>


### [591] [From Associations to Activations: Comparing Behavioral and Hidden-State Semantic Geometry in LLMs](https://arxiv.org/abs/2602.00628)
*Louis Schiekiera,Max Zimmer,Christophe Roux,Sebastian Pokutta,Fritz Günther*

Main category: cs.LG

TL;DR: 通过心理语言学实验的行为数据可以部分恢复LLM隐藏状态的几何结构，其中强制选择任务比自由联想任务更能反映隐藏状态相似性。


<details>
  <summary>Details</summary>
Motivation: 研究LLM在心理语言学实验中的行为数据是否能够揭示其隐藏状态的几何结构，探索行为测量能否恢复内部语义表示的信息。

Method: 在8个指令调优的Transformer模型上运行两种实验范式（基于相似性的强制选择和自由联想），使用5000个共享词汇收集1750万+次试验，构建基于行为的相似性矩阵，通过表征相似性分析比较行为几何与分层隐藏状态相似性，并与FastText、BERT和跨模型共识进行基准比较。

Result: 强制选择行为比自由联想行为更显著地与隐藏状态几何对齐；在保留词汇回归中，行为相似性（特别是强制选择）能够预测未见过的隐藏状态相似性，超越了词汇基线和跨模型共识，表明仅行为测量保留了可恢复的内部语义几何信息。

Conclusion: 行为任务确实能够揭示LLM的隐藏认知状态，特别是强制选择范式在恢复隐藏状态几何结构方面比自由联想更有效，这为通过行为实验理解模型内部表示提供了实证支持。

Abstract: We investigate the extent to which an LLM's hidden-state geometry can be recovered from its behavior in psycholinguistic experiments. Across eight instruction-tuned transformer models, we run two experimental paradigms -- similarity-based forced choice and free association -- over a shared 5,000-word vocabulary, collecting 17.5M+ trials to build behavior-based similarity matrices. Using representational similarity analysis, we compare behavioral geometries to layerwise hidden-state similarity and benchmark against FastText, BERT, and cross-model consensus. We find that forced-choice behavior aligns substantially more with hidden-state geometry than free association. In a held-out-words regression, behavioral similarity (especially forced choice) predicts unseen hidden-state similarities beyond lexical baselines and cross-model consensus, indicating that behavior-only measurements retain recoverable information about internal semantic geometry. Finally, we discuss implications for the ability of behavioral tasks to uncover hidden cognitive states.

</details>


### [592] [Combinatorial Bandit Bayesian Optimization for Tensor Outputs](https://arxiv.org/abs/2602.00640)
*Jingru Huang,Haijie Xu,Jie Guo,Manrui Jiang,Chen Zhang*

Main category: cs.LG

TL;DR: 提出两种张量输出贝叶斯优化方法：TOGP用于完整张量输出优化，以及CBBO用于组合选择部分输出的场景，均提供理论保证和实验验证。


<details>
  <summary>Details</summary>
Motivation: 现有贝叶斯优化方法未处理张量输出函数，且实际应用中常需从张量输出中选择部分子集贡献于目标函数，需要新的方法填补这一空白。

Method: 1) 提出张量输出高斯过程(TOGP)作为代理模型，包含两类张量输出核函数捕捉结构依赖；2) 基于TOGP设计UCB采集函数；3) 针对组合选择问题提出CBBO方法，扩展TOGP处理部分观测输出，并设计CMAB-UCB2准则同时选择查询点和最优输出子集。

Result: 建立了两种方法的理论遗憾界，确保次线性性能。大量合成和真实世界实验证明了方法的优越性。

Conclusion: 成功填补了贝叶斯优化在张量输出函数优化方面的空白，提出的TOGP和CBBO方法在理论和实验上均表现出色，为复杂优化问题提供了有效解决方案。

Abstract: Bayesian optimization (BO) has been widely used to optimize expensive and black-box functions across various domains. Existing BO methods have not addressed tensor-output functions. To fill this gap, we propose a novel tensor-output BO method. Specifically, we first introduce a tensor-output Gaussian process (TOGP) with two classes of tensor-output kernels as a surrogate model of the tensor-output function, which can effectively capture the structural dependencies within the tensor. Based on it, we develop an upper confidence bound (UCB) acquisition function to select the queried points. Furthermore, we introduce a more complex and practical problem setting, named combinatorial bandit Bayesian optimization (CBBO), where only a subset of the outputs can be selected to contribute to the objective function. To tackle this, we propose a tensor-output CBBO method, which extends TOGP to handle partially observed outputs, and accordingly design a novel combinatorial multi-arm bandit-UCB2 (CMAB-UCB2) criterion to sequentially select both the queried points and the optimal output subset. Theoretical regret bounds for the two methods are established, ensuring their sublinear performance. Extensive synthetic and real-world experiments demonstrate their superiority.

</details>


### [593] [CoRe-Fed: Bridging Collaborative and Representation Fairness via Federated Embedding Distillation](https://arxiv.org/abs/2602.00647)
*Noorain Mukhtiar,Adnan Mahmood,Quan Z. Sheng*

Main category: cs.LG

TL;DR: CoRe-Fed是一个联邦学习公平性优化框架，通过嵌入对齐和公平聚合解决表示偏差和协作偏差问题


<details>
  <summary>Details</summary>
Motivation: 联邦学习中存在数据分布异构和参与不平等导致的性能差异问题，特别是表示偏差（客户端表示不一致）和协作偏差（聚合贡献不公平），这会导致不公平结果并降低模型性能

Method: 提出CoRe-Fed统一优化框架：1）嵌入对齐机制促进本地与全局嵌入的语义一致性；2）动态奖励惩罚聚合策略基于参与历史和嵌入对齐调整客户端权重

Result: 在多种模型和数据集上的广泛实验表明，CoRe-Fed在公平性和模型性能方面均优于现有基线算法

Conclusion: CoRe-Fed通过同时解决表示公平和协作公平问题，有效缓解了联邦学习中的性能差异，提升了模型的公平性和泛化能力

Abstract: With the proliferation of distributed data sources, Federated Learning (FL) has emerged as a key approach to enable collaborative intelligence through decentralized model training while preserving data privacy. However, conventional FL algorithms often suffer from performance disparities across clients caused by heterogeneous data distributions and unequal participation, which leads to unfair outcomes. Specifically, we focus on two core fairness challenges, i.e., representation bias, arising from misaligned client representations, and collaborative bias, stemming from inequitable contribution during aggregation, both of which degrade model performance and generalizability. To mitigate these disparities, we propose CoRe-Fed, a unified optimization framework that bridges collaborative and representation fairness via embedding-level regularization and fairness-aware aggregation. Initially, an alignment-driven mechanism promotes semantic consistency between local and global embeddings to reduce representational divergence. Subsequently, a dynamic reward-penalty-based aggregation strategy adjusts each client's weight based on participation history and embedding alignment to ensure contribution-aware aggregation. Extensive experiments across diverse models and datasets demonstrate that CoRe-Fed improves both fairness and model performance over the state-of-the-art baseline algorithms.

</details>


### [594] [PHAT: Modeling Period Heterogeneity for Multivariate Time Series Forecasting](https://arxiv.org/abs/2602.00654)
*Jiaming Ma,Guanjun Wang,Qihe Huang,Sheng Huang,Haofeng Ma,Zhengyang Zhou,Pengkun Wang,Binwu Wang,Yang Wang*

Main category: cs.LG

TL;DR: PHAT提出了一种考虑周期异质性的Transformer模型，通过周期性桶结构和正负注意力机制，有效处理多变量时间序列中不同变量具有不同动态周期的问题。


<details>
  <summary>Details</summary>
Motivation: 现有多变量时间序列预测模型虽然能建模周期性，但忽略了现实数据中常见的周期异质性——不同变量具有不同且动态变化的周期。这种异质性会导致模型性能下降。

Method: 1. 将多变量输入组织成三维"周期性桶"张量：维度分别对应具有相似周期性的变量组特征、按相位对齐的时间步、周期内的偏移量；2. 限制桶内交互并屏蔽跨桶连接，避免不一致周期的干扰；3. 提出正负注意力机制，从周期性对齐和周期性偏差两个角度捕获周期性依赖；4. 将周期性对齐注意力分数分解为正负分量，并通过编码周期性先验的调制项约束注意力机制。

Result: 在14个真实世界数据集上对18个基线模型进行全面评估，PHAT显著优于现有方法，实现了极具竞争力的预测性能。

Conclusion: PHAT通过有效建模周期异质性，解决了现有多变量时间序列预测模型的关键局限，为处理具有不同动态周期的现实世界数据提供了有效解决方案。

Abstract: While existing multivariate time series forecasting models have advanced significantly in modeling periodicity, they largely neglect the periodic heterogeneity common in real-world data, where variates exhibit distinct and dynamically changing periods. To effectively capture this periodic heterogeneity, we propose PHAT (Period Heterogeneity-Aware Transformer). Specifically, PHAT arranges multivariate inputs into a three-dimensional "periodic bucket" tensor, where the dimensions correspond to variate group characteristics with similar periodicity, time steps aligned by phase, and offsets within the period. By restricting interactions within buckets and masking cross-bucket connections, PHAT effectively avoids interference from inconsistent periods. We also propose a positive-negative attention mechanism, which captures periodic dependencies from two perspectives: periodic alignment and periodic deviation. Additionally, the periodic alignment attention scores are decomposed into positive and negative components, with a modulation term encoding periodic priors. This modulation constrains the attention mechanism to more faithfully reflect the underlying periodic trends. A mathematical explanation is provided to support this property. We evaluate PHAT comprehensively on 14 real-world datasets against 18 baselines, and the results show that it significantly outperforms existing methods, achieving highly competitive forecasting performance. Our sources is available at GitHub.

</details>


### [595] [Riemannian Flow Matching for Disentangled Graph Domain Adaptation](https://arxiv.org/abs/2602.00656)
*Yingxu Wang,Xinwang Liu,Mengzhu Wang,Siyang Gao,Nan Yin*

Main category: cs.LG

TL;DR: DisRFM：一种几何感知的图域自适应框架，通过黎曼流形嵌入和基于流的传输解决结构退化和优化不稳定性问题


<details>
  <summary>Details</summary>
Motivation: 传统图域自适应方法在欧几里得空间中使用对抗学习对齐图嵌入，面临两个关键挑战：1）结构退化 - 层次和语义表示纠缠；2）优化不稳定性 - 最小最大对抗训练的振荡动态

Method: 1）将图嵌入黎曼流形，使用极坐标显式解耦结构（半径）和语义（角度）；2）通过径向Wasserstein对齐保持拓扑结构，通过角度聚类实现语义区分；3）使用黎曼流匹配学习平滑向量场，沿测地线路径引导源特征向目标迁移

Result: 理论证明了流匹配的渐近稳定性，推导了更紧的目标风险界限。大量实验表明DisRFM持续优于最先进方法

Conclusion: DisRFM通过几何感知的框架有效解决了图域自适应中的结构退化和优化不稳定性问题，实现了更好的域对齐和性能

Abstract: Graph Domain Adaptation (GDA) typically uses adversarial learning to align graph embeddings in Euclidean space. However, this paradigm suffers from two critical challenges: Structural Degeneration, where hierarchical and semantic representations are entangled, and Optimization Instability, which arises from oscillatory dynamics of minimax adversarial training. To tackle these issues, we propose DisRFM, a geometry-aware GDA framework that unifies Riemannian embedding and flow-based transport. First, to overcome structural degeneration, we embed graphs into a Riemannian manifold. By adopting polar coordinates, we explicitly disentangle structure (radius) from semantics (angle). Then, we enforce topology preservation through radial Wasserstein alignment and semantic discrimination via angular clustering, thereby preventing feature entanglement and collapse. Second, we address the instability of adversarial alignment by using Riemannian flow matching. This method learns a smooth vector field to guide source features toward the target along geodesic paths, guaranteeing stable convergence. The geometric constraints further guide the flow to maintain the disentangled structure during transport. Theoretically, we prove the asymptotic stability of the flow matching and derive a tighter bound for the target risk. Extensive experiments demonstrate that DisRFM consistently outperforms state-of-the-art methods.

</details>


### [596] [Three-Way Emotion Classification of EEG-based Signals using Machine Learning](https://arxiv.org/abs/2602.00670)
*Ashna Purwar,Gaurav Simkar,Madhumita,Sachin Kadam*

Main category: cs.LG

TL;DR: 该论文研究了使用机器学习模型对EEG信号进行三分类情绪识别（消极、中性、积极），比较了逻辑回归、支持向量机和随机森林三种模型，发现随机森林在准确率和F1分数上表现最佳。


<details>
  <summary>Details</summary>
Motivation: EEG信号能够直接反映大脑活动，可用于识别人的情绪状态。随着情感感知系统和EEG情绪识别研究的发展，需要探索哪种机器学习模型最适合处理有限数据集下的EEG信号三分类问题。

Method: 采用完整的工作流程，包括数据预处理和机器学习模型比较。在有限EEG数据集上训练和测试三种常用模型：逻辑回归(LR)、支持向量机(SVM)和随机森林(RF)，使用准确率和F1分数评估性能。

Result: 机器学习模型可有效用于EEG信号的三分类情绪识别。在三种模型中，随机森林表现最佳，其更高的准确率和F1分数表明它能更准确有效地捕捉情绪模式。随机森林在准确率参数上也优于现有最先进分类模型。

Conclusion: 随机森林是处理有限EEG数据集三分类情绪识别问题的最佳机器学习模型，在准确率和F1分数上均优于逻辑回归和支持向量机，且超越现有最先进分类方法。

Abstract: Electroencephalography (EEG) is a widely used technique for measuring brain activity. EEG-based signals can reveal a persons emotional state, as they directly reflect activity in different brain regions. Emotion-aware systems and EEG-based emotion recognition are a growing research area. This paper presents how machine learning (ML) models categorize a limited dataset of EEG signals into three different classes, namely Negative, Neutral, or Positive. It also presents the complete workflow, including data preprocessing and comparison of ML models. To understand which ML classification model works best for this kind of problem, we train and test the following three commonly used models: logistic regression (LR), support vector machine (SVM), and random forest (RF). The performance of each is evaluated with respect to accuracy and F1-score. The results indicate that ML models can be effectively utilized for three-way emotion classification of EEG signals. Among the three ML models trained on the available dataset, the RF model gave the best results. Its higher accuracy and F1-score suggest that it is able to capture the emotional patterns more accurately and effectively than the other two models. The RF model also outperformed the existing state-of-the-art classification models in terms of the accuracy parameter.

</details>


### [597] [Strong Linear Baselines Strike Back: Closed-Form Linear Models as Gaussian Process Conditional Density Estimators for TSAD](https://arxiv.org/abs/2602.00672)
*Aleksandr Yugay,Hang Cui,Changhua Pei,Alexey Zaytsev*

Main category: cs.LG

TL;DR: 线性自回归异常检测方法在计算效率和准确性上超越复杂深度学习模型


<details>
  <summary>Details</summary>
Motivation: 当前时间序列异常检测研究过度关注复杂、难以训练且推理昂贵的神经网络架构，需要重新审视这一范式

Method: 使用简单线性自回归异常评分，通过普通最小二乘法（OLS）回归提供闭式解，估计有限历史高斯过程条件密度

Result: 在广泛的单变量和多变量基准测试中，该方法实现了更优的准确性，同时所需计算资源数量级减少

Conclusion: 未来研究应包含强线性基线，并开发具有更丰富时间结构的新基准来突显深度学习模型的优势

Abstract: Research in time series anomaly detection (TSAD) has largely focused on developing increasingly sophisticated, hard-to-train, and expensive-to-infer neural architectures. We revisit this paradigm and show that a simple linear autoregressive anomaly score with the closed-form solution provided by ordinary least squares (OLS) regression consistently matches or outperforms state-of-the-art deep detectors. From a theoretical perspective, we show that linear models capture a broad class of anomaly types, estimating a finite-history Gaussian process conditional density. From a practical side, across extensive univariate and multivariate benchmarks, the proposed approach achieves superior accuracy while requiring orders of magnitude fewer computational resources. Thus, future research should consistently include strong linear baselines and, more importantly, develop new benchmarks with richer temporal structures pinpointing the advantages of deep learning models.

</details>


### [598] [Provably Protecting Fine-Tuned LLMs from Training Data Extraction](https://arxiv.org/abs/2602.00688)
*Tom Segal,Asaf Shabtai,Yuval Elovici*

Main category: cs.LG

TL;DR: 提出SCP-Δr算法，通过基于相对概率的Near Access Freeness方法，在保护隐私的同时最小化性能损失


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在敏感数据集上微调存在隐私风险，现有防御方法要么缺乏形式化隐私保证，要么导致显著的性能下降

Method: 提出SCP-Δr算法，基于Near Access Freeness框架，在相对概率空间操作，显式地平滑低影响力token，仅保留关键token级偏差

Result: SCP-Δr比现有NAF方法获得数量级更好的理论界限，在有效防御训练数据提取攻击的同时保持最小性能损失

Conclusion: 通过选择性平滑微调引起的概率偏移，可以在保护隐私和保持模型效用之间实现更好的平衡

Abstract: Fine-tuning large language models (LLMs) on sensitive datasets raises privacy concerns, as training data extraction (TDE) attacks can expose highly confidential information. Existing defenses against such attacks either lack formal privacy guarantees or incur substantial utility degradation. We observe that fine-tuning induces widespread probability shifts, yet preserving only a small subset of influential token-level deviations is sufficient; the remaining shifts can be aggressively smoothed with minimal impact on utility. Motivated by this insight, we propose SCP-$Δ_r$, a Near Access Freeness (NAF)-based algorithm that operates on relative probabilities and explicitly smooths low-impact tokens using a base model. SCP-$Δ_r$ achieves orders-of-magnitude better theoretical bounds than existing NAF based methods and provides strong empirical protection against TDE attacks with minimal performance loss.

</details>


### [599] [Topology and Geometry of the Learning Space of ReLU Networks: Connectivity and Singularities](https://arxiv.org/abs/2602.00693)
*Marco Nurisso,Pierrick Leroy,Giovanni Petri,Francesco Vaccarino*

Main category: cs.LG

TL;DR: 该论文研究了前馈ReLU网络参数空间的拓扑特性，重点关注基于有向无环图(DAG)架构的网络参数空间的连通性和奇异性问题。


<details>
  <summary>Details</summary>
Motivation: 理解前馈ReLU网络参数空间的特性对于分析和指导训练动态至关重要。梯度流训练会将参数空间限制在由ReLU激活函数的齐次性产生的代数簇上，这促使研究者深入探索参数空间的拓扑结构。

Method: 研究基于一般有向无环图(DAG)架构的前馈ReLU网络，通过理论分析扩展先前结果，全面刻画参数空间的连通性，强调瓶颈节点和特定子网络平衡条件的作用，并建立与可微分剪枝的理论联系。

Result: 研究发现奇异性与底层DAG拓扑及其诱导子网络密切相关，揭示了参数空间连通性的完整特征，并通过简单数值实验验证了理论。

Conclusion: 该研究为理解ReLU网络参数空间的拓扑结构提供了深入见解，建立了奇异性与网络架构拓扑之间的明确联系，并为可微分剪枝提供了理论基础。

Abstract: Understanding the properties of the parameter space in feed-forward ReLU networks is critical for effectively analyzing and guiding training dynamics. After initialization, training under gradient flow decisively restricts the parameter space to an algebraic variety that emerges from the homogeneous nature of the ReLU activation function. In this study, we examine two key challenges associated with feed-forward ReLU networks built on general directed acyclic graph (DAG) architectures: the (dis)connectedness of the parameter space and the existence of singularities within it. We extend previous results by providing a thorough characterization of connectedness, highlighting the roles of bottleneck nodes and balance conditions associated with specific subsets of the network. Our findings clearly demonstrate that singularities are intricately connected to the topology of the underlying DAG and its induced sub-networks. We discuss the reachability of these singularities and establish a principled connection with differentiable pruning. We validate our theory with simple numerical experiments.

</details>


### [600] [Forecasting Energy Availability in Local Energy Communities via LSTM Federated Learning](https://arxiv.org/abs/2602.00694)
*Fabio Turazza,Marcello Pietri,Natalia Selini Hadjidimitriou,Marco Mamei*

Main category: cs.LG

TL;DR: 使用联邦学习结合LSTM网络解决本地能源社区中的隐私保护预测问题，平衡数据共享与预测精度


<details>
  <summary>Details</summary>
Motivation: 本地能源社区在实现自给自足时面临能源生产与消费平衡管理的挑战，需要准确的预测模型，但用户隐私顾虑阻碍了数据共享，因此需要隐私保护的解决方案

Method: 采用联邦学习框架结合长短期记忆网络，在不共享用户隐私敏感信息的情况下训练预测模型

Result: 展示了联邦学习在本地能源社区预测中的应用可行性，并分析了数据共享与预测精度之间的权衡关系

Conclusion: 联邦学习是解决本地能源社区隐私保护预测问题的可行方案，能够在保护用户隐私的同时实现有效的能源预测

Abstract: Local Energy Communities are emerging as crucial players in the landscape of sustainable development. A significant challenge for these communities is achieving self-sufficiency through effective management of the balance between energy production and consumption. To meet this challenge, it is essential to develop and implement forecasting models that deliver accurate predictions, which can then be utilized by optimization and planning algorithms. However, the application of forecasting solutions is often hindered by privacy constrains and regulations as the users participating in the Local Energy Community can be (rightfully) reluctant sharing their consumption patterns with others. In this context, the use of Federated Learning (FL) can be a viable solution as it allows to create a forecasting model without the need to share privacy sensitive information among the users. In this study, we demonstrate how FL and long short-term memory (LSTM) networks can be employed to achieve this objective, highlighting the trade-off between data sharing and forecasting accuracy.

</details>


### [601] [LocalV: Exploiting Information Locality for IP-level Verilog Generation](https://arxiv.org/abs/2602.00704)
*Hanqi Lyu,Di Huang,Yaoyu Zhu,Kangcheng Liu,Bohan Dou,Chongxiao Li,Pengwei Jin,Shuyao Cheng,Rui Zhang,Zidong Du,Qi Guo,Xing Hu,Yunji Chen*

Main category: cs.LG

TL;DR: LocalV是一个多智能体框架，通过利用模块化硬件设计中的信息局部性，将长文档到长代码的生成问题分解为短文档、短代码任务，显著提升了RTL代码生成的准确性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: RTL代码生成是数字硬件设计中的关键但劳动密集型步骤。现有LLM方法在处理工业级IP设计任务时面临三大挑战：处理冗长详细文档、生成长RTL代码时正确性下降、以及复杂的调试周期。

Method: 提出LocalV多智能体框架，采用分层文档划分、任务规划、局部化代码生成、接口一致性合并和AST引导的局部感知调试等方法，将长文档到长代码的生成问题分解为短文档、短代码任务。

Result: 在RealBench（IP级Verilog生成基准测试）上，LocalV显著优于最先进的LLM和智能体方法，实现了45.0%的通过率，而SOTA方法仅为21.6%。

Conclusion: LocalV通过利用硬件设计中的信息局部性，有效解决了工业级RTL代码生成的扩展性问题，为自动化硬件设计提供了有前景的解决方案。

Abstract: The generation of Register-Transfer Level (RTL) code is a crucial yet labor-intensive step in digital hardware design, traditionally requiring engineers to manually translate complex specifications into thousands of lines of synthesizable Hardware Description Language (HDL) code. While Large Language Models (LLMs) have shown promise in automating this process, existing approaches-including fine-tuned domain-specific models and advanced agent-based systems-struggle to scale to industrial IP-level design tasks. We identify three key challenges: (1) handling long, highly detailed documents, where critical interface constraints become buried in unrelated submodule descriptions; (2) generating long RTL code, where both syntactic and semantic correctness degrade sharply with increasing output length; and (3) navigating the complex debugging cycles required for functional verification through simulation and waveform analysis. To overcome these challenges, we propose LocalV, a multi-agent framework that leverages information locality in modular hardware design. LocalV decomposes the long-document to long-code generation problem into a set of short-document, short-code tasks, enabling scalable generation and debugging. Specifically, LocalV integrates hierarchical document partitioning, task planning, localized code generation, interface-consistent merging, and AST-guided locality-aware debugging. Experiments on RealBench, an IP-level Verilog generation benchmark, demonstrate that LocalV substantially outperforms state-of-the-art (SOTA) LLMs and agents, achieving a pass rate of 45.0% compared to 21.6%.

</details>


### [602] [Deep Time-series Forecasting Needs Kernelized Moment Balancing](https://arxiv.org/abs/2602.00717)
*Licheng Pan,Hao Wang,Haocheng Yang,Yuqi Li,Qingsong Wen,Xiaoxi Li,Zhichao Chen,Haoxuan Li,Zhixuan Chu,Yuan Lu*

Main category: cs.LG

TL;DR: 本文提出KMB-DF方法，通过核化矩平衡实现深度时间序列预测中的完全分布平衡，相比现有方法显著提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有深度时间序列预测方法无法实现真正的分布平衡，它们只能匹配一个或两个预定义的平衡函数，无法满足Imbens准则要求的完全分布平衡。

Method: 提出KMB-DF方法，从再生核希尔伯特空间中自适应选择最具信息量的平衡函数，实现充分的分布平衡，并推导出可微分的可处理目标函数。

Result: 在多个模型和数据集上的实验表明，KMB-DF能持续提升预测精度，达到最先进的性能水平。

Conclusion: KMB-DF通过核化矩平衡解决了现有方法分布平衡不足的问题，为深度时间序列预测提供了更有效的分布对齐方法。

Abstract: Deep time-series forecasting can be formulated as a distribution balancing problem aimed at aligning the distribution of the forecasts and ground truths. According to Imbens' criterion, true distribution balance requires matching the first moments with respect to any balancing function. We demonstrate that existing objectives fail to meet this criterion, as they enforce moment matching only for one or two predefined balancing functions, thus failing to achieve full distribution balance. To address this limitation, we propose direct forecasting with kernelized moment balancing (KMB-DF). Unlike existing objectives, KMB-DF adaptively selects the most informative balancing functions from a reproducing kernel hilbert space (RKHS) to enforce sufficient distribution balancing. We derive a tractable and differentiable objective that enables efficient estimation from empirical samples and seamless integration into gradient-based training pipelines. Extensive experiments across multiple models and datasets show that KMB-DF consistently improves forecasting accuracy and achieves state-of-the-art performance. Code is available at https://anonymous.4open.science/r/KMB-DF-403C.

</details>


### [603] [Federated Learning at the Forefront of Fairness: A Multifaceted Perspective](https://arxiv.org/abs/2602.00718)
*Noorain Mukhtiar,Adnan Mahmood,Yipeng Zhou,Jian Yang,Jing Teng,Quan Z. Sheng*

Main category: cs.LG

TL;DR: 该论文是一篇关于联邦学习中公平性问题的综述，从多角度对现有公平感知方法进行分类，提供评估框架和指标，并探讨未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中公平性日益重要，源于异构客户端约束和不同场景下模型性能平衡的需求。需要系统梳理现有方法，为研究者提供全面指导。

Method: 采用多角度分类法：1) 模型性能导向方法；2) 能力导向方法。建立框架分类和解决各种公平问题，分析公平与性能的平衡技术。

Result: 提供了全面的公平感知方法分类体系，建立了公平问题解决框架，总结了定量评估公平性的重要指标，为联邦学习公平性研究奠定基础。

Conclusion: 该综述为联邦学习公平性研究提供了系统框架和分类方法，指出了未来研究方向，有助于推动该领域的发展。

Abstract: Fairness in Federated Learning (FL) is emerging as a critical factor driven by heterogeneous clients' constraints and balanced model performance across various scenarios. In this survey, we delineate a comprehensive classification of the state-of-the-art fairness-aware approaches from a multifaceted perspective, i.e., model performance-oriented and capability-oriented. Moreover, we provide a framework to categorize and address various fairness concerns and associated technical aspects, examining their effectiveness in balancing equity and performance within FL frameworks. We further examine several significant evaluation metrics leveraged to measure fairness quantitatively. Finally, we explore exciting open research directions and propose prospective solutions that could drive future advancements in this important area, laying a solid foundation for researchers working toward fairness in FL.

</details>


### [604] [Spectral Imbalance Causes Forgetting in Low-Rank Continual Adaptation](https://arxiv.org/abs/2602.00722)
*Hao Gu,Mao-Lin Luo,Zi-Hao Zhou,Han-Chen Zhang,Min-Ling Zhang,Tong Wei*

Main category: cs.LG

TL;DR: 本文提出EBLoRA方法，通过解耦任务更新的幅度和方向结构，在受限Stiefel流形上优化，实现参数高效的持续学习，减少前后向遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法主要关注避免与过去更新的干扰，而非考虑什么属性使当前任务特定更新自然保留先前获得的知识。从知识分解角度看，低秩适应表现出高度不平衡的奇异值谱：少数主导组件吸收了大部分适应能量，从而更可能破坏先前获得的知识，并使更新更容易受到后续任务的干扰。

Method: 提出EBLoRA方法，将任务更新的幅度与方向结构解耦，并将其表述为受限Stiefel流形上的约束优化问题。使用与标准深度学习优化器兼容的投影一阶方法来解决这个问题。

Result: 该方法能够同时减轻后向遗忘和前向遗忘，在持续学习基准测试中一致优于现有基线方法。

Conclusion: 通过显式平衡组件间的适应能量分配，EBLoRA方法能够更有效地保留先前任务知识，同时适应新任务，在参数高效的持续学习中表现出优越性能。

Abstract: Parameter-efficient continual learning aims to adapt pre-trained models to sequential tasks without forgetting previously acquired knowledge. Most existing approaches treat continual learning as avoiding interference with past updates, rather than considering what properties make the current task-specific update naturally preserve previously acquired knowledge. From a knowledge-decomposition perspective, we observe that low-rank adaptations exhibit highly imbalanced singular value spectra: a few dominant components absorb most of the adaptation energy, thereby (i) more likely to disrupt previously acquired knowledge and (ii) making the update more vulnerable to interference from subsequent tasks. To enable explicit balance among components, we decouple the magnitude of the task update from its directional structure and formulate it as a constrained optimization problem on a restricted Stiefel manifold. We address this problem using a projected first-order method compatible with standard deep-learning optimizers used in vision-language models. Our method mitigates both backward and forward forgetting, consistently outperforming continual learning baselines. The implementation code is available at https://github.com/haodotgu/EBLoRA.

</details>


### [605] [Rethinking Hallucinations: Correctness, Consistency, and Prompt Multiplicity](https://arxiv.org/abs/2602.00723)
*Prakhar Ganesh,Reza Shokri,Golnoosh Farnadi*

Main category: cs.LG

TL;DR: 论文提出"提示多样性"框架来量化LLM评估中的一致性，发现现有幻觉评估过度关注正确性而忽视一致性，导致对幻觉危害的严重误解


<details>
  <summary>Details</summary>
Motivation: 现有LLM幻觉评估主要关注正确性，但忽视了评估的一致性，而一致性对于区分和解决幻觉带来的各种危害（从信任侵蚀到错误信息传播）至关重要

Method: 引入"提示多样性"框架来量化LLM评估中的一致性，分析现有基准测试中的不一致性，并研究一致性在幻觉检测和缓解中的作用

Result: 发现显著的多重性（如Med-HALT基准中超过50%的不一致性），表明幻觉相关危害被严重误解；检测技术实际上检测的是一致性而非正确性；RAG等缓解技术虽然有益但可能引入额外的不一致性

Conclusion: 通过将提示多样性整合到幻觉评估中，提供了改进的危害评估框架，并揭示了当前检测和缓解策略的关键局限性，强调一致性评估的重要性

Abstract: Large language models (LLMs) are known to "hallucinate" by generating false or misleading outputs. Hallucinations pose various harms, from erosion of trust to widespread misinformation. Existing hallucination evaluation, however, focuses only on correctness and often overlooks consistency, necessary to distinguish and address these harms. To bridge this gap, we introduce prompt multiplicity, a framework for quantifying consistency in LLM evaluations. Our analysis reveals significant multiplicity (over 50% inconsistency in benchmarks like Med-HALT), suggesting that hallucination-related harms have been severely misunderstood. Furthermore, we study the role of consistency in hallucination detection and mitigation. We find that: (a) detection techniques detect consistency, not correctness, and (b) mitigation techniques like RAG, while beneficial, can introduce additional inconsistencies. By integrating prompt multiplicity into hallucination evaluation, we provide an improved framework of potential harms and uncover critical limitations in current detection and mitigation strategies.

</details>


### [606] [Pareto-Conditioned Diffusion Models for Offline Multi-Objective Optimization](https://arxiv.org/abs/2602.00737)
*Jatan Shrestha,Santeri Heiskanen,Kari Hepola,Severi Rissanen,Pekka Jääskeläinen,Joni Pajarinen*

Main category: cs.LG

TL;DR: PCD将离线多目标优化转化为条件采样问题，通过直接条件化于期望权衡来避免显式代理模型，使用重加权策略和参考方向机制探索帕累托前沿。


<details>
  <summary>Details</summary>
Motivation: 离线多目标优化中，仅使用静态数据集时的主要挑战是如何泛化到未观测数据之外。现有方法需要显式代理模型，而PCD旨在通过条件采样框架直接处理这一挑战。

Method: 提出Pareto-Conditioned Diffusion框架，将离线MOO转化为条件采样问题：1) 直接条件化于期望的权衡关系；2) 使用重加权策略聚焦高性能样本；3) 采用参考方向机制引导采样到训练数据之外的新颖有前景区域。

Result: 在标准离线MOO基准测试中，PCD实现了高度竞争力的性能，并且比现有离线MOO方法在不同任务间表现出更好的一致性。

Conclusion: PCD通过将离线多目标优化重新构建为条件采样问题，提供了一种无需显式代理模型的有效方法，能够更好地泛化到未观测数据并探索帕累托前沿。

Abstract: Multi-objective optimization (MOO) arises in many real-world applications where trade-offs between competing objectives must be carefully balanced. In the offline setting, where only a static dataset is available, the main challenge is generalizing beyond observed data. We introduce Pareto-Conditioned Diffusion (PCD), a novel framework that formulates offline MOO as a conditional sampling problem. By conditioning directly on desired trade-offs, PCD avoids the need for explicit surrogate models. To effectively explore the Pareto front, PCD employs a reweighting strategy that focuses on high-performing samples and a reference-direction mechanism to guide sampling towards novel, promising regions beyond the training data. Experiments on standard offline MOO benchmarks show that PCD achieves highly competitive performance and, importantly, demonstrates greater consistency across diverse tasks than existing offline MOO approaches.

</details>


### [607] [GraphNNK -- Graph Classification and Interpretability](https://arxiv.org/abs/2602.00753)
*Zeljko Bolevic,Milos Brajovic,Isidora Stankovic,Ljubisa Stankovic*

Main category: cs.LG

TL;DR: GNNs依赖参数化分类器限制了可解释性和泛化能力，基于插值的方法（如NNK）通过训练样本的凸组合进行预测，提供了更好的可解释性。


<details>
  <summary>Details</summary>
Motivation: GNNs已成为图结构数据学习的标准方法，但其依赖参数化分类器（通常是线性softmax层）限制了模型的可解释性，有时还会阻碍泛化能力。

Method: 采用基于插值的方法，特别是非负核回归（NNK），将预测表示为嵌入空间中相似训练样本的凸组合。

Result: 该方法不仅获得了理论结果，还产生了可解释的解释，通过训练样本的凸组合来表达预测。

Conclusion: 基于插值的方法（如NNK）为GNNs提供了一种替代参数化分类器的方案，能够提高模型的可解释性，同时保持或改善泛化性能。

Abstract: Graph Neural Networks (GNNs) have become a standard approach for learning from graph-structured data. However, their reliance on parametric classifiers (most often linear softmax layers) limits interpretability and sometimes hinders generalization. Recent work on interpolation-based methods, particularly Non-Negative Kernel regression (NNK), has demonstrated that predictions can be expressed as convex combinations of similar training examples in the embedding space, yielding both theoretical results and interpretable explanations.

</details>


### [608] [BLOCK-EM: Preventing Emergent Misalignment by Blocking Causal Features](https://arxiv.org/abs/2602.00767)
*Muhammed Ustaomeroglu,Guannan Qu*

Main category: cs.LG

TL;DR: 通过识别控制模型不良行为的内部特征并在微调时抑制这些特征，可以有效防止语言模型在特定任务微调后出现的"涌现性错位"问题，在六个领域实现了高达95%的错位减少且不影响模型性能。


<details>
  <summary>Details</summary>
Motivation: 语言模型在特定监督目标微调时会出现"涌现性错位"问题：模型学会了目标行为，但也发展出不良的域外行为。需要找到一种机制性方法来防止这种错位。

Method: 识别控制错位行为的小型内部特征集，在微调过程中阻止模型强化这些特征。使用分离的选择/评估集、多个独立评估者、随机种子、质量指标和广泛的消融实验来验证有效性。

Result: 在六个微调领域中，阻断固定特征集实现了高达95%的相对错位减少，且没有降低模型质量或目标任务性能。但在长时间微调下错位会重新出现，表明模型可能通过替代特征或层进行重新路由。

Conclusion: 针对内部机制的有针对性训练时约束可以有效减轻涌现性错位，同时保持目标任务性能。这为控制语言模型在微调过程中的行为提供了机制性方法。

Abstract: Emergent misalignment can arise when a language model is fine-tuned on a narrowly scoped supervised objective: the model learns the target behavior, yet also develops undesirable out-of-domain behaviors. We investigate a mechanistic approach to preventing emergent misalignment by identifying a small set of internal features that reliably control the misaligned behavior and then discouraging the model from strengthening these features during fine-tuning. Across six fine-tuning domains, blocking (i.e., constraining) a fixed set of features achieves up to 95\% relative reduction in emergent misalignment with no degradation in model quality or target-task performance. We strengthen validity with disjoint selection/evaluation splits, multiple independent judges, multiple random seeds for key settings, quality metrics, and extensive ablations demonstrating that the reduction in misalignment is specific to the identified mechanism. We also characterize a limiting regime in which misalignment re-emerges under prolonged fine-tuning, present evidence consistent with rerouting through alternative features or layers, and evaluate modifications that partially restore the misalignment-blocking effect. Overall, our results show that targeted training-time constraints on internal mechanisms can mitigate emergent misalignment without degrading target-task performance.

</details>


### [609] [Provable Model Provenance Set for Large Language Models](https://arxiv.org/abs/2602.00772)
*Xiaoqi Qiu,Hao Zeng,Zhiyu Hou,Hongxin Wei*

Main category: cs.LG

TL;DR: 提出MPS方法解决模型溯源问题，通过序列测试排除过程构建满足可证明保证的小型溯源集合


<details>
  <summary>Details</summary>
Motivation: 现有模型溯源方法依赖启发式指纹匹配规则，缺乏可证明的错误控制，且常忽略多源情况，导致溯源声明可靠性无法验证

Method: 提出模型溯源集(MPS)，采用序列测试排除过程，在候选池中测试溯源存在显著性，自适应构建满足保证的小型集合

Result: MPS能有效实现目标溯源覆盖，同时严格限制无关模型的包含，在归因和审计任务中展现实际应用潜力

Conclusion: MPS为模型溯源问题提供了具有可证明保证的解决方案，在用户指定置信水平下建立渐近保证，优于现有启发式方法

Abstract: The growing prevalence of unauthorized model usage and misattribution has increased the need for reliable model provenance analysis. However, existing methods largely rely on heuristic fingerprint-matching rules that lack provable error control and often overlook the existence of multiple sources, leaving the reliability of their provenance claims unverified. In this work, we first formalize the model provenance problem with provable guarantees, requiring rigorous coverage of all true provenances at a prescribed confidence level. Then, we propose the Model Provenance Set (MPS), which employs a sequential test-and-exclusion procedure to adaptively construct a small set satisfying the guarantee. The key idea of MPS is to test the significance of provenance existence within a candidate pool, thereby establishing a provable asymptotic guarantee at a user-specific confidence level. Extensive experiments demonstrate that MPS effectively achieves target provenance coverage while strictly limiting the inclusion of unrelated models, and further reveal its potential for practical provenance analysis in attribution and auditing tasks.

</details>


### [610] [A novel VAE-DML fusion framework for casual analysis of greenwashing in the mining industry](https://arxiv.org/abs/2602.00774)
*Yuxin Lu,Zhen Peng,Xiqiang Xia,Jie Wang*

Main category: cs.LG

TL;DR: 股权制衡显著抑制矿业产业链企业的绿色伪装行为，这种抑制作用具有异质性和动态性，主要通过缓解管理层业绩压力、增强高管团队稳定性和强化媒体监督三种机制实现。


<details>
  <summary>Details</summary>
Motivation: 在全球绿色转型和"双碳"目标背景下，矿业产业链企业作为资源消耗和环境影响的重点实体，其环境信息披露的真实性对区域生态安全和国家资源战略至关重要。从公司治理角度研究股权制衡对绿色伪装行为的抑制作用及其机制，是可持续发展与国家战略目标的核心紧迫问题。

Method: 创新性地采用变分自编码器（VAE）和双重机器学习（DML）模型构建反事实场景，缓解内生性问题，精确识别股权制衡与绿色伪装之间的因果关系。

Result: 1. 股权制衡与企业绿色伪装存在显著负向因果关系，证实其治理效应；2. 抑制作用具有异质性，在西部地区、产业链上游和环境敏感行业更强；3. 治理效应呈现动态性，当期最强，随后递减但仍显著，最终形成稳定长期累积影响。

Conclusion: 股权制衡通过缓解管理层业绩压力、增强高管团队稳定性和强化媒体监督三种机制抑制绿色伪装行为，为矿业产业链企业的环境治理提供了有效的公司治理机制。

Abstract: Against the backdrop of the global green transition and "dual carbon" goals, mining industry chain enterprises are pivotal entities in terms of resource consumption and environmental impact. Their environmental performance directly affects regional ecological security and is closely tied to national resource strategies and green transformation outcomes. Ensuring the authenticity and reliability of their environmental disclosure is thus a core and urgent issue for sustainable development and national strategic objectives.From a corporate governance perspective, this study examines equity balance as a fundamental governance mechanism, investigating its inhibitory effect on greenwashing behavior among these enterprises and the underlying pathways involved. Methodologically, the paper innovatively employs a Variational Autoencoder (VAE) and a Double Machine Learning (DML) model to construct counterfactual scenarios, mitigating endogeneity concerns and precisely identifying the causal relationship between equity balance and greenwashing. The findings indicate, first, a significant negative causal relationship between equity balance and corporate greenwashing, confirming its substantive governance effect. Second, this inhibitory effect exhibits notable heterogeneity, manifesting more strongly in western regions, upstream segments of the industrial chain, and industries with high environmental sensitivity. Third, the governance effect demonstrates clear temporal dynamics, with the strongest impact occurring in the current period, followed by a diminishing yet statistically significant lagged effect, and ultimately a stable long-term cumulative influence. Finally, mechanism analysis reveals that equity balance operates through three distinct channels to curb greenwashing: alleviating management performance pressure, enhancing the stability of the executive team, and intensifying media scrutiny.

</details>


### [611] [Multi-Objective Multi-Fidelity Bayesian Optimization with Causal Priors](https://arxiv.org/abs/2602.00788)
*Md Abir Hossen,Mohammad Ali Javidian,Vignesh Narayanan,Jason M. O'Kane,Pooyan Jamshidi*

Main category: cs.LG

TL;DR: RESCUE是一种多保真度贝叶斯优化方法，通过引入因果推理来改进传统方法，在低保真度代理与目标保真度对齐不佳时表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有多保真度贝叶斯优化方法主要捕捉输入、保真度和目标之间的关联依赖，而非因果机制，当低保真度代理与目标保真度对齐不佳时性能会下降。

Method: RESCUE学习一个结构因果模型来捕捉输入、保真度和目标之间的因果关系，构建编码干预效应的概率多保真度代理模型，并引入因果超体积知识梯度采集策略来选择输入-保真度对。

Result: 在机器人、机器学习（AutoML）和医疗保健等领域的合成和实际问题上，RESCUE比最先进的多保真度优化方法提高了采样效率。

Conclusion: 通过将因果推理融入多保真度贝叶斯优化，RESCUE能够更有效地利用低保真度代理，在低保真度代理与目标保真度对齐不佳时仍能保持良好性能。

Abstract: Multi-fidelity Bayesian optimization (MFBO) accelerates the search for the global optimum of black-box functions by integrating inexpensive, low-fidelity approximations. The central task of an MFBO policy is to balance the cost-efficiency of low-fidelity proxies against their reduced accuracy to ensure effective progression toward the high-fidelity optimum. Existing MFBO methods primarily capture associational dependencies between inputs, fidelities, and objectives, rather than causal mechanisms, and can perform poorly when lower-fidelity proxies are poorly aligned with the target fidelity. We propose RESCUE (REducing Sampling cost with Causal Understanding and Estimation), a multi-objective MFBO method that incorporates causal calculus to systematically address this challenge. RESCUE learns a structural causal model capturing causal relationships between inputs, fidelities, and objectives, and uses it to construct a probabilistic multi-fidelity (MF) surrogate that encodes intervention effects. Exploiting the causal structure, we introduce a causal hypervolume knowledge-gradient acquisition strategy to select input-fidelity pairs that balance expected multi-objective improvement and cost. We show that RESCUE improves sample efficiency over state-of-the-art MF optimization methods on synthetic and real-world problems in robotics, machine learning (AutoML), and healthcare.

</details>


### [612] [Latent Shadows: The Gaussian-Discrete Duality in Masked Diffusion](https://arxiv.org/abs/2602.00792)
*Guinan Chen,Xunpeng Huang,Ying Sun,Shijin Wang,Yanyong Zhang,Chao Wang*

Main category: cs.LG

TL;DR: 本文提出了掩码一致性蒸馏（MCD），通过建立掩码扩散对偶性，为掩码离散扩散模型提供确定性采样方法，实现16倍推理加速且不损失生成质量。


<details>
  <summary>Details</summary>
Motivation: 掩码离散扩散模型在高质量语言建模中占主导地位，但其推理效率受限于缺乏确定性采样工具。现有方法要么性能较差，要么依赖复杂积分算子或随机蒸馏。

Method: 建立了明确的掩码扩散对偶性，证明掩码过程是连续高斯过程通过最大索引保持机制的投影。基于此提出了掩码一致性蒸馏（MCD），利用对偶性解析构造确定性耦合轨迹，绕过数值ODE求解器。

Result: MCD实现了16倍的推理加速，同时不损害生成质量，严格优于先前的随机蒸馏方法。

Conclusion: 该工作不仅为掩码和连续扩散之间的连接提供了坚实的理论基础，还解锁了一致性蒸馏在高性能离散生成中的全部潜力。

Abstract: Masked discrete diffusion is a dominant paradigm for high-quality language modeling where tokens are iteratively corrupted to a mask state, yet its inference efficiency is bottlenecked by the lack of deterministic sampling tools. While diffusion duality enables deterministic distillation for uniform models, these approaches generally underperform masked models and rely on complex integral operators. Conversely, in the masked domain, prior methods typically assume the absence of deterministic trajectories, forcing a reliance on stochastic distillation. To bridge this gap, we establish explicit Masked Diffusion Duality, proving that the masked process arises as the projection of a continuous Gaussian process via a novel maximum-value index preservation mechanism. Furthermore, we introduce Masked Consistency Distillation (MCD), a principled framework that leverages this duality to analytically construct the deterministic coupled trajectories required for consistency distillation, bypassing numerical ODE solvers. This result strictly improves upon prior stochastic distillation methods, achieving a 16$\times$ inference speedup without compromising generation quality. Our findings not only provide a solid theoretical foundation connecting masked and continuous diffusion, but also unlock the full potential of consistency distillation for high-performance discrete generation. Our code is available at https://anonymous.4open.science/r/MCD-70FD.

</details>


### [613] [JTok: On Token Embedding as another Axis of Scaling Law via Joint Token Self-modulation](https://arxiv.org/abs/2602.00800)
*Yebin Yang,Huaijin Wu,Fu Guo,Lin Yao,Xiaohan Qin,Jingzhi Wang,Debing Zhang,Junchi Yan*

Main category: cs.LG

TL;DR: 提出token-indexed parameters作为新的扩展维度，通过JTok和JTok-M在Transformer层中加入调制向量，实现模型容量与计算量的解耦，显著提升性能同时保持低计算开销。


<details>
  <summary>Details</summary>
Motivation: 传统LLM通过密集维度扩展时性能提升与计算成本线性增长耦合，MoE虽然解耦了容量与计算，但带来内存开销和硬件效率问题。需要新的扩展维度来克服这些限制。

Method: 提出token-indexed parameters作为新的扩展轴，引入Joint-Token (JTok)和Mixture of Joint-Token (JTok-M)，通过从辅助嵌入表中检索调制向量来增强Transformer层，这些向量通过轻量级逐元素操作调制主干网络，计算开销可忽略。

Result: 在650M到61B参数的密集和MoE主干上进行实验，JTok方法持续降低验证损失并显著提升下游任务性能（如MMLU +4.1，ARC +8.3，CEval +8.9）。isoFLOPs分析显示JTok-M将质量-计算Pareto前沿移动，相比普通MoE架构在相同质量下减少35%计算量，且token-indexed参数呈现可预测的幂律缩放行为。

Conclusion: token-indexed parameters作为新的扩展维度有效解耦了模型容量与计算量，JTok和JTok-M方法在保持低计算开销的同时显著提升模型性能，为LLM扩展提供了新方向。

Abstract: LLMs have traditionally scaled along dense dimensions, where performance is coupled with near-linear increases in computational cost. While MoE decouples capacity from compute, it introduces large memory overhead and hardware efficiency challenges. To overcome these, we propose token-indexed parameters as a novel, orthogonal scaling axis that decouple model capacity from FLOPs. Specifically, we introduce Joint-Token (JTok) and Mixture of Joint-Token (JTok-M), which augment Transformer layers with modulation vectors retrieved from auxiliary embedding tables. These vectors modulate the backbone via lightweight, element-wise operations, incurring negligible FLOPs overhead. Extensive experiments on both dense and MoE backbones, spanning from 650M (190M + 460M embedding) to 61B (17B + 44B embedding) total parameters, demonstrate that our approach consistently reduces validation loss and significantly improves downstream task performance (e.g., +4.1 on MMLU, +8.3 on ARC, +8.9 on CEval). Rigorous isoFLOPs analysis further confirms that JTok-M fundamentally shifts the quality-compute Pareto frontier, achieving comparable model quality with 35% less compute relative to vanilla MoE architectures, and we validate that token-indexed parameters exhibit a predictable power-law scaling behavior. Moreover, our efficient implementation ensures that the overhead introduced by JTok and JTok-M remains marginal.

</details>


### [614] [Mobile Exergames: Activity Recognition Based on Smartphone Sensors](https://arxiv.org/abs/2602.00809)
*David Craveiro,Hugo Silva*

Main category: cs.LG

TL;DR: 提出Duck Catch & Fit 2D无尽游戏，结合智能手机传感器（加速度计、陀螺仪、磁力计）进行人体活动识别，并整合语音识别系统提升游戏沉浸感


<details>
  <summary>Details</summary>
Motivation: 智能手机传感器可用于获取人类活动和行为信息，人体活动识别在游戏、医疗和监控领域应用日益广泛，需要探索如何将传感器数据与游戏体验结合

Method: 开发概念验证游戏Duck Catch & Fit，使用智能手机加速度计、陀螺仪和磁力计传感器，通过特征提取和学习机制检测静止、侧向移动和虚假侧向移动等活动，并整合语音识别系统识别"fire"指令

Result: 机器学习技术能够以高识别率识别人体活动，运动识别与语音识别的结合为游戏创造了更沉浸的体验

Conclusion: 智能手机传感器结合机器学习技术可以有效识别人体活动，多模态交互（运动+语音）能够提升游戏复杂度和沉浸感，为游戏设计提供了新思路

Abstract: Smartphone sensors can be extremely useful in providing information on the activities and behaviors of persons. Human activity recognition is increasingly used for games, medical, or surveillance. In this paper, we propose a proof-of-concept 2D endless game called Duck Catch & Fit, which implements a detailed activity recognition system that uses a smartphone accelerometer, gyroscope, and magnetometer sensors. The system applies feature extraction and learning mechanism to detect human activities like staying, side movements, and fake side movements. In addition, a voice recognition system is combined to recognize the word "fire" and raise the game's complexity. The results show that it is possible to use machine learning techniques to recognize human activity with high recognition levels. Also, the combination of movement-based and voice-based integrations contributes to a more immersive gameplay.

</details>


### [615] [RMFlow: Refined Mean Flow by a Noise-Injection Step for Multimodal Generation](https://arxiv.org/abs/2602.00849)
*Yuhao Huang,Shih-Hsin Wang,Andrea L. Bertozzi,Bao Wang*

Main category: cs.LG

TL;DR: RMFlow通过结合粗粒度1-NFE MeanFlow传输和定制化噪声注入细化步骤，实现了高效的多模态生成，在仅需1次函数评估的情况下达到接近SOTA的结果。


<details>
  <summary>Details</summary>
Motivation: MeanFlow虽然能够实现高效、高保真度的图像生成，但其单次函数评估（1-NFE）生成往往无法产生令人满意的结果。需要解决1-NFE生成质量不足的问题。

Method: 提出RMFlow模型，整合了粗粒度1-NFE MeanFlow传输和后续定制化的噪声注入细化步骤。使用神经网络近似流路径的平均速度，通过新的损失函数训练，平衡概率路径之间的Wasserstein距离最小化和样本似然最大化。

Result: RMFlow在文本到图像、上下文到分子和时间序列生成任务上，仅使用1-NFE就达到了接近最先进水平的结果，计算成本与基线MeanFlows相当。

Conclusion: RMFlow通过创新的两阶段方法有效解决了1-NFE生成质量不足的问题，在保持计算效率的同时显著提升了生成质量，为高效多模态生成提供了新思路。

Abstract: Mean flow (MeanFlow) enables efficient, high-fidelity image generation, yet its single-function evaluation (1-NFE) generation often cannot yield compelling results. We address this issue by introducing RMFlow, an efficient multimodal generative model that integrates a coarse 1-NFE MeanFlow transport with a subsequent tailored noise-injection refinement step. RMFlow approximates the average velocity of the flow path using a neural network trained with a new loss function that balances minimizing the Wasserstein distance between probability paths and maximizing sample likelihood. RMFlow achieves near state-of-the-art results on text-to-image, context-to-molecule, and time-series generation using only 1-NFE, at a computational cost comparable to the baseline MeanFlows.

</details>


### [616] [Investigating the Robustness of Subtask Distillation under Spurious Correlation](https://arxiv.org/abs/2602.00852)
*Pattarawat Chormai,Klaus-Robert Müller,Grégoire Montavon*

Main category: cs.LG

TL;DR: 评估在存在虚假相关性的数据上进行知识蒸馏时，不同蒸馏方法的鲁棒性表现


<details>
  <summary>Details</summary>
Motivation: 子任务蒸馏虽然使用教师模型，但仍依赖可能有限、缺乏代表性或存在虚假相关性的数据集。需要评估在存在虚假相关性的现实世界数据集上进行知识蒸馏的挑战。

Method: 评估了已建立的蒸馏方法以及最近的SubDistill方法，在具有虚假相关性的数据上进行蒸馏。通过增加相关性强度来观察不同方法的性能变化。

Result: 随着虚假相关性强度增加，SubDistill等先进方法保持相对鲁棒，而一些基线方法性能下降到接近随机水平。不同方法之间的性能差距随相关性强度增加而扩大。

Conclusion: 研究强调了在具有虚假相关性的不完美现实世界数据集上进行知识蒸馏的挑战，需要更鲁棒的蒸馏方法来应对数据质量问题。

Abstract: Subtask distillation is an emerging paradigm in which compact, specialized models are extracted from large, general-purpose 'foundation models' for deployment in environments with limited resources or in standalone computer systems. Although distillation uses a teacher model, it still relies on a dataset that is often limited in size and may lack representativeness or exhibit spurious correlations. In this paper, we evaluate established distillation methods, as well as the recent SubDistill method, when using data with spurious correlations for distillation. As the strength of the correlations increases, we observe a widening gap between advanced methods, such as SubDistill, which remain fairly robust, and some baseline methods, which degrade to near-random performance. Overall, our study underscores the challenges of knowledge distillation when applied to imperfect, real-world datasets, particularly those with spurious correlations.

</details>


### [617] [Towards Multiscale Graph-based Protein Learning with Geometric Secondary Structural Motifs](https://arxiv.org/abs/2602.00862)
*Shih-Hsin Wang,Yuhao Huang,Taos Transue,Justin Baker,Jonathan Forstater,Thomas Strohmer,Bao Wang*

Main category: cs.LG

TL;DR: 提出一种针对蛋白质结构的高效多尺度图学习框架，通过构建包含细粒度子图和粗粒度图的层次化表示，使用两个GNN分别学习局部和全局结构特征，在保持表达力的同时提升预测精度并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有的基于图神经网络（GNN）的蛋白质结构学习方法在捕捉多尺度表示和建模长程依赖方面存在挑战，需要更高效的多尺度学习框架。

Method: 构建层次化图表示：包含对应二级结构基序（α-螺旋、β-链、环等）的细粒度子图集合，以及基于空间排列和相对方向连接这些基序的单个粗粒度图。使用两个GNN：第一个在单个二级基序内学习局部相互作用，第二个建模跨基序的高层结构关系。

Result: 理论证明该层次化框架保持了最大表达力，不损失关键结构信息。实验表明，将基线GNN集成到该多尺度框架中显著提高了预测精度，并在多个基准测试中降低了计算成本。

Conclusion: 提出的多尺度图学习框架通过层次化表示和模块化设计，有效解决了蛋白质结构学习中的多尺度表示和长程依赖问题，在保持理论表达力的同时实现了更好的预测性能和计算效率。

Abstract: Graph neural networks (GNNs) have emerged as powerful tools for learning protein structures by capturing spatial relationships at the residue level. However, existing GNN-based methods often face challenges in learning multiscale representations and modeling long-range dependencies efficiently. In this work, we propose an efficient multiscale graph-based learning framework tailored to proteins. Our proposed framework contains two crucial components: (1) It constructs a hierarchical graph representation comprising a collection of fine-grained subgraphs, each corresponding to a secondary structure motif (e.g., $α$-helices, $β$-strands, loops), and a single coarse-grained graph that connects these motifs based on their spatial arrangement and relative orientation. (2) It employs two GNNs for feature learning: the first operates within individual secondary motifs to capture local interactions, and the second models higher-level structural relationships across motifs. Our modular framework allows a flexible choice of GNN in each stage. Theoretically, we show that our hierarchical framework preserves the desired maximal expressiveness, ensuring no loss of critical structural information. Empirically, we demonstrate that integrating baseline GNNs into our multiscale framework remarkably improves prediction accuracy and reduces computational cost across various benchmarks.

</details>


### [618] [Improving Flow Matching by Aligning Flow Divergence](https://arxiv.org/abs/2602.00869)
*Yuhao Huang,Taos Transue,Shih-Hsin Wang,William Feldman,Hong Zhang,Bao Wang*

Main category: cs.LG

TL;DR: 本文提出了一种改进条件流匹配的方法，通过同时匹配流场及其散度来减少学习概率路径的误差，从而提升流基生成模型的性能。


<details>
  <summary>Details</summary>
Motivation: 条件流匹配（CFM）虽然是一种高效的无模拟训练方法，但在学习概率路径的准确性方面存在不足。作者发现CFM不能确保概率路径的精确学习，因此需要新的方法来减少学习路径与真实路径之间的误差。

Method: 作者首先推导了学习概率路径与真实概率路径之间误差的偏微分方程表征及其解。理论分析表明，两个概率路径之间的总变差上界由CFM损失和相关的散度损失共同决定。基于这一理论洞察，作者设计了一个新的目标函数，同时匹配流场及其散度。

Result: 新方法在多个重要基准任务上显著提升了流基生成模型的性能，包括动态系统、DNA序列和视频生成任务，且没有牺牲生成效率。代码已开源。

Conclusion: 通过同时匹配流场及其散度，可以显著改进条件流匹配方法，提高生成模型的性能，同时保持生成效率。该方法为流基生成模型提供了更准确和有效的训练框架。

Abstract: Conditional flow matching (CFM) stands out as an efficient, simulation-free approach for training flow-based generative models, achieving remarkable performance for data generation. However, CFM is insufficient to ensure accuracy in learning probability paths. In this paper, we introduce a new partial differential equation characterization for the error between the learned and exact probability paths, along with its solution. We show that the total variation gap between the two probability paths is bounded above by a combination of the CFM loss and an associated divergence loss. This theoretical insight leads to the design of a new objective function that simultaneously matches the flow and its divergence. Our new approach improves the performance of the flow-based generative model by a noticeable margin without sacrificing generation efficiency. We showcase the advantages of this enhanced training approach over CFM on several important benchmark tasks, including generative modeling for dynamical systems, DNA sequences, and videos. Code is available at \href{https://github.com/Utah-Math-Data-Science/Flow_Div_Matching}{Utah-Math-Data-Science}.

</details>


### [619] [Learning Heat-based Equations in Self-similar variables](https://arxiv.org/abs/2602.00872)
*Shihao Wang,Qipeng Qian,Jingquan Wang*

Main category: cs.LG

TL;DR: 该论文提出了一种基于自相似变量（SSV）的训练框架，用于学习热基方程的解，相比传统物理坐标训练，SSV训练能显著提高神经网络在训练窗口外的准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 研究热基方程在自相似变量中的解学习，旨在利用自相似坐标提供的数学归纳偏置，改善神经网络对长期动力学行为的捕捉能力。

Method: 开发了与标准神经算子训练兼容的SSV训练框架，并在二维不可压缩Navier-Stokes方程和一维粘性Burgers方程上实例化。使用两种简单全连接架构（标准多层感知机和因子化全连接网络）进行物理坐标与自相似坐标训练的对比实验。

Result: 在两个系统和两种架构中，SSV训练的网络始终表现出：1）在训练窗口外具有更准确和稳定的外推能力；2）更好地捕捉长期定性趋势。

Conclusion: 自相似坐标为学习热基方程的长期动力学提供了数学上合理的归纳偏置，能显著提升神经网络模型的泛化性能。

Abstract: We study solution learning for heat-based equations in self-similar variables (SSV). We develop an SSV training framework compatible with standard neural-operator training. We instantiate this framework on the two-dimensional incompressible Navier-Stokes equations and the one-dimensional viscous Burgers equation, and perform controlled comparisons between models trained in physical coordinates and in the corresponding self-similar coordinates using two simple fully connected architectures (standard multilayer perceptrons and a factorized fully connected network). Across both systems and both architectures, SSV-trained networks consistently deliver substantially more accurate and stable extrapolation beyond the training window and better capture qualitative long-time trends. These results suggest that self-similar coordinates provide a mathematically motivated inductive bias for learning the long-time dynamics of heat-based equations.

</details>


### [620] [Dynamic Expert Sharing: Decoupling Memory from Parallelism in Mixture-of-Experts Diffusion LLMs](https://arxiv.org/abs/2602.00879)
*Hao Mark Chen,Zhiwen Mo,Royson Lee,Qianzhou Wang,Da Li,Shell Xu Hu,Wayne Luk,Timothy Hospedales,Hongxiang Fan*

Main category: cs.LG

TL;DR: 提出Dynamic Expert Sharing (DES)方法，通过序列级核心集选择减少MoE扩散大语言模型中的专家爆炸问题，在保持99%准确率的同时降低55%独特专家激活和38%延迟


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型(dLLMs)与混合专家(MoE)架构结合时面临专家爆炸问题：并行生成的token数量增加时，激活的独特专家数量线性增长，导致内存流量激增，使推理进入内存受限状态，抵消了MoE和并行解码的效率优势

Method: 提出动态专家共享(DES)技术，将MoE优化从token级剪枝和传统专家跳过方法转向序列级核心集选择。包括两种创新选择策略：1) 序列内共享(DES-Seq)，在序列级别适应最优分配；2) 显著性感知投票(DES-Vote)，允许token基于聚合路由器权重共同选举核心集

Result: 在MoE dLLMs上的广泛实验表明，DES减少了超过55%的独特专家激活和高达38%的延迟，同时保持99%的原始准确率，有效解耦了内存开销与并行度

Conclusion: DES通过序列级专家共享成功解决了MoE扩散大语言模型中的专家爆炸问题，在保持模型准确性的同时显著提升了推理效率，实现了内存开销与并行度的解耦

Abstract: Among parallel decoding paradigms, diffusion large language models (dLLMs) have emerged as a promising candidate that balances generation quality and throughput. However, their integration with Mixture-of-Experts (MoE) architectures is constrained by an expert explosion: as the number of tokens generated in parallel increases, the number of distinct experts activated grows nearly linearly. This results in substantial memory traffic that pushes inference into a memory-bound regime, negating the efficiency gains of both MoE and parallel decoding. To address this challenge, we propose Dynamic Expert Sharing (DES), a novel technique that shifts MoE optimization from token-centric pruning and conventional expert skipping methods to sequence-level coreset selection. To maximize expert reuse, DES identifies a compact, high-utility set of experts to satisfy the requirements of an entire parallel decoding block. We introduce two innovative selection strategies: (1) Intra-Sequence Sharing (DES-Seq), which adapts optimal allocation to the sequence level, and (2) Saliency-Aware Voting (DES-Vote), a novel mechanism that allows tokens to collectively elect a coreset based on aggregated router weights. Extensive experiments on MoE dLLMs demonstrate that DES reduces unique expert activations by over 55% and latency by up to 38%, while retaining 99% of vanilla accuracy, effectively decoupling memory overhead from the degree of parallelism.

</details>


### [621] [Test-time Generalization for Physics through Neural Operator Splitting](https://arxiv.org/abs/2602.00884)
*Louis Serrano,Jiequn Han,Edouard Oyallon,Shirley Ho,Rudy Morel*

Main category: cs.LG

TL;DR: 提出一种测试时神经算子分裂策略，通过组合训练算子来近似未见过的动力学，实现零样本泛化


<details>
  <summary>Details</summary>
Motivation: 神经算子在处理训练分布外的测试输入时泛化能力有限，现有方法需要新动力学的示例进行微调，无法实现真正的零样本泛化

Method: 基于DISCO提供的不同动力学训练算子字典，引入测试时神经算子分裂策略，通过搜索训练算子的组合来近似未见过的动力学

Result: 在参数外推和新物理现象组合等挑战性任务上，实现了最先进的零样本泛化结果，并能恢复底层PDE参数

Conclusion: 测试时计算是构建灵活、组合和可泛化神经算子的关键途径

Abstract: Neural operators have shown promise in learning solution maps of partial differential equations (PDEs), but they often struggle to generalize when test inputs lie outside the training distribution, such as novel initial conditions, unseen PDE coefficients or unseen physics. Prior works address this limitation with large-scale multiple physics pretraining followed by fine-tuning, but this still requires examples from the new dynamics, falling short of true zero-shot generalization. In this work, we propose a method to enhance generalization at test time, i.e., without modifying pretrained weights. Building on DISCO, which provides a dictionary of neural operators trained across different dynamics, we introduce a neural operator splitting strategy that, at test time, searches over compositions of training operators to approximate unseen dynamics. On challenging out-of-distribution tasks including parameter extrapolation and novel combinations of physics phenomena, our approach achieves state-of-the-art zero-shot generalization results, while being able to recover the underlying PDE parameters. These results underscore test-time computation as a key avenue for building flexible, compositional, and generalizable neural operators.

</details>


### [622] [Reliability-Aware Determinantal Point Processes for Robust Informative Data Selection in Large Language Models](https://arxiv.org/abs/2602.00885)
*Ahmad Sarlak,Abolfazl Razi*

Main category: cs.LG

TL;DR: 提出ProbDPP方法，在传统多样性数据选择基础上考虑数据访问的可靠性问题，通过正则化项处理概率性数据访问，并设计在线学习算法优化选择策略。


<details>
  <summary>Details</summary>
Motivation: 传统数据选择方法（如基于DPP的方法）假设数据总是可靠可用，但在实际部署中常面临存储故障、通信不完美和随机访问失败等问题，导致原有方法失效。

Method: 提出ProbDPP方法，将k-DPP扩展为可靠性感知版本，通过正则化项重新构建目标函数，将问题分解为几何多样性项和不可靠性成本。将可靠性感知的多样性最大化建模为组合半强盗问题，并提出UCB风格算法在线学习未知可靠性。

Result: 理论分析为所提方法提供了遗憾界限，确保性能保证。ProbDPP能够在不确定性下实现鲁棒的多样性数据选择。

Conclusion: ProbDPP解决了传统数据选择方法在可靠性问题上的局限性，为在计算和通信约束下的高效LLM部署提供了更实用的数据选择方案。

Abstract: Informative data selection is a key requirement for large language models (LLMs) to minimize the amount of data required for fine-tuning, network distillation, and token pruning, enabling fast and efficient deployment, especially under computational and communication constraints. Traditional subset selection methods, including those based on Determinantal Point Processes (DPP), focus on maximizing diversity but assume that selected data batches are always available error-free. This presumption prohibits their use under partial storage outage, imperfect communication, and stochastic access failures. Furthermore, we show that the original formulation collapses under such conditions. To address this gap, we introduce ProbDPP, a novel reliability-aware implementation of k-DPP that accounts for probabilistic data access by recasting the objective function with a regularization term that remains well-posed and decomposes into a geometric diversity term and unreliability cost. The resulting objective facilitates robust selection of diverse data batches under uncertainty. Furthermore, we frame this reliability-aware diversity maximization as a combinatorial semi-bandit problem and propose a UCB-style algorithm to efficiently learn the unknown reliability online. Theoretical analysis provides regret bounds for the proposed approach, ensuring performance guarantees.

</details>


### [623] [Privacy in Practice: Private COVID-19 Detection in X-Ray Images (Extended Version)](https://arxiv.org/abs/2211.11434)
*Lucas Lange,Maja Schneider,Peter Christen,Erhard Rahm*

Main category: cs.LG

TL;DR: 该研究探讨了在COVID-19图像分类任务中应用差分隐私(DP)的实际效果，发现DP对成员推理攻击(MIA)的实际防御效果有限，并提出基于经验攻击的隐私评估方法能更好地平衡效用与隐私。


<details>
  <summary>Details</summary>
Motivation: 现有COVID-19隐私保护模型存在数据集小、隐私保证弱或不明确、缺乏实际隐私效果评估等问题。需要改进差分隐私在医疗图像分析中的实际应用效果评估。

Method: 采用差分隐私机器学习模型，考虑类别不平衡问题，在更严格的隐私预算下评估效用-隐私权衡。通过黑盒成员推理攻击(MIA)实证评估实际隐私泄露情况。

Result: 研究发现：1)不同任务所需的隐私级别因MIA威胁而异；2)随着DP保证增强，实际隐私泄露仅边际改善；3)DP对MIA的实际防御效果有限。

Conclusion: 差分隐私在COVID-19分类任务中对实际成员推理攻击的防御效果有限。基于经验攻击的隐私评估方法能帮助实现更好的效用-隐私权衡，应在实际隐私调优中发挥重要作用。

Abstract: Machine learning (ML) can help fight pandemics like COVID-19 by enabling rapid screening of large volumes of images. To perform data analysis while maintaining patient privacy, we create ML models that satisfy Differential Privacy (DP). Previous works exploring private COVID-19 models are in part based on small datasets, provide weaker or unclear privacy guarantees, and do not investigate practical privacy. We suggest improvements to address these open gaps. We account for inherent class imbalances and evaluate the utility-privacy trade-off more extensively and over stricter privacy budgets. Our evaluation is supported by empirically estimating practical privacy through black-box Membership Inference Attacks (MIAs). The introduced DP should help limit leakage threats posed by MIAs, and our practical analysis is the first to test this hypothesis on the COVID-19 classification task. Our results indicate that needed privacy levels might differ based on the task-dependent practical threat from MIAs. The results further suggest that with increasing DP guarantees, empirical privacy leakage only improves marginally, and DP therefore appears to have a limited impact on practical MIA defense. Our findings identify possibilities for better utility-privacy trade-offs, and we believe that empirical attack-specific privacy estimation can play a vital role in tuning for practical privacy.

</details>


### [624] [GAPNet: Plug-in Jointly Learning Task-Specific Graph for Dynamic Stock Relation](https://arxiv.org/abs/2602.00888)
*Yingjie Niu,Lanxin Lu,Changhong Jin,Ruihai Dong*

Main category: cs.LG

TL;DR: GAPNet是一个图适应插件网络，通过端到端学习任务特定的拓扑和表示，动态调整股票关系图，提升金融预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预定义图来捕捉股票间关系，但网络信号噪声大、异步且难以获取，导致泛化性差且与下游任务不对齐。需要动态适应任务特定拓扑的方法。

Method: 提出GAPNet图适应插件网络，可附加到现有成对图或超图骨干模型上，通过空间感知层捕捉短期资产共动，时间感知层维持分布偏移下的长期依赖，实现边拓扑的动态适应和重连。

Result: 在两个真实股票数据集上，GAPNet持续提升盈利性和稳定性，RT-GCN年化累计收益达0.47，CI-STHPAN达0.63，峰值夏普比率分别为2.20和2.12。

Conclusion: 联合学习图结构和表示对于任务特定关系建模至关重要，GAPNet的即插即用设计确保其广泛适用于各种GNN架构。

Abstract: The advent of the web has led to a paradigm shift in the financial relations, with the real-time dissemination of news, social discourse, and financial filings contributing significantly to the reshaping of financial forecasting. The existing methods rely on establishing relations a priori, i.e. predefining graphs to capture inter-stock relationships. However, the stock-related web signals are characterised by high levels of noise, asynchrony, and challenging to obtain, resulting in poor generalisability and non-alignment between the predefined graphs and the downstream tasks. To address this, we propose GAPNet, a Graph Adaptation Plug-in Network that jointly learns task-specific topology and representations in an end-to-end manner. GAPNet attaches to existing pairwise graph or hypergraph backbone models, enabling the dynamic adaptation and rewiring of edge topologies via two complementary components: a Spatial Perception Layer that captures short-term co-movements across assets, and a Temporal Perception Layer that maintains long-term dependency under distribution shift. Across two real-world stock datasets, GAPNet has been shown to consistently enhance the profitability and stability in comparision to the state-of-the-art models, yielding annualised cumulative returns of up to 0.47 for RT-GCN and 0.63 for CI-STHPAN, with peak Sharpe Ratio of 2.20 and 2.12 respectively. The plug-and-play design of GAPNet ensures its broad applicability to diverse GNN-based architectures. Our results underscore that jointly learning graph structures and representations is essential for task-specific relational modeling.

</details>


### [625] [Generating Synthetic Health Sensor Data for Privacy-Preserving Wearable Stress Detection](https://arxiv.org/abs/2401.13327)
*Lucas Lange,Nils Wenzlitschke,Erhard Rahm*

Main category: cs.LG

TL;DR: 该论文提出使用生成对抗网络(GAN)和差分隐私(DP)保护来合成智能手表健康传感器数据，以解决医疗数据隐私敏感和获取困难的问题，并在压力检测任务中验证了合成数据的有效性。


<details>
  <summary>Details</summary>
Motivation: 智能手表健康传感器数据在健康应用和患者监测中日益重要，但这些医疗数据包含敏感个人信息且获取成本高，需要解决隐私保护和数据可用性之间的平衡问题。

Method: 采用生成对抗网络(GAN)结合差分隐私(DP)保护来合成多传感器智能手表健康读数，生成与压力时刻相关的隐私保护合成数据，并测试多种GAN模型和数据增强策略。

Result: 在压力检测任务中，基于GAN的数据增强方法显著提升了模型性能：差分隐私训练场景下F1分数提高了11.90-15.48%，非隐私训练场景下仍提升了0.45%。但隐私要求增强会显著影响合成数据质量。

Conclusion: 差分隐私合成数据在优化效用-隐私权衡方面具有潜力，特别是在真实训练样本有限的情况下，但需要平衡隐私保护程度与数据质量之间的关系。

Abstract: Smartwatch health sensor data are increasingly utilized in smart health applications and patient monitoring, including stress detection. However, such medical data often comprise sensitive personal information and are resource-intensive to acquire for research purposes. In response to this challenge, we introduce the privacy-aware synthetization of multi-sensor smartwatch health readings related to moments of stress, employing Generative Adversarial Networks (GANs) and Differential Privacy (DP) safeguards. Our method not only protects patient information but also enhances data availability for research. To ensure its usefulness, we test synthetic data from multiple GANs and employ different data enhancement strategies on an actual stress detection task. Our GAN-based augmentation methods demonstrate significant improvements in model performance, with private DP training scenarios observing an 11.90-15.48% increase in F1-score, while non-private training scenarios still see a 0.45% boost. These results underline the potential of differentially private synthetic data in optimizing utility-privacy trade-offs, especially with the limited availability of real training samples. Through rigorous quality assessments, we confirm the integrity and plausibility of our synthetic data, which, however, are significantly impacted when increasing privacy requirements.

</details>


### [626] [Domain-Adaptive and Scalable Dense Retrieval for Content-Based Recommendation](https://arxiv.org/abs/2602.00899)
*Mritunjay Pandey*

Main category: cs.LG

TL;DR: 提出基于双塔编码器的稠密检索系统，用于电商推荐，通过语义相似性解决词汇不匹配问题，相比传统BM25方法Recall@10从0.26提升到0.66，同时满足延迟和模型大小约束。


<details>
  <summary>Details</summary>
Motivation: 电商推荐和搜索通常依赖稀疏关键词匹配（如BM25），当用户意图与产品元数据的词汇重叠有限时，这种方法会失效。需要解决词汇不匹配问题，通过语义相似性进行内容推荐。

Method: 将基于内容的推荐视为检索任务，使用双塔编码器架构，在Amazon Reviews 2023（Fashion）子集上进行监督对比学习，采用多重负样本排序损失。训练对由评论文本（作为查询代理）和物品元数据（作为正文档）构成，最大序列长度为500个token。使用FAISS HNSW索引和ONNX Runtime推理管道，采用INT8动态量化进行高效服务。

Result: 在826,402个目录物品的评论到标题基准测试中，Recall@10从BM25的0.26提升到0.66。模型满足实际延迟和大小约束：中位CPU推理延迟为6.1毫秒（批量大小为1），模型大小减少4倍。

Conclusion: 提供了一个端到端、可复现的蓝图，用于将领域适应的稠密检索从离线训练扩展到CPU高效的大规模目录服务，解决了电商推荐中的词汇不匹配问题。

Abstract: E-commerce recommendation and search commonly rely on sparse keyword matching (e.g., BM25), which breaks down under vocabulary mismatch when user intent has limited lexical overlap with product metadata. We cast content-based recommendation as recommendation-as-retrieval: given a natural-language intent signal (a query or review), retrieve the top-K most relevant items from a large catalog via semantic similarity.
  We present a scalable dense retrieval system based on a two-tower bi-encoder, fine-tuned on the Amazon Reviews 2023 (Fashion) subset using supervised contrastive learning with Multiple Negatives Ranking Loss. We construct training pairs from review text (as a query proxy) and item metadata (as the positive document) and fine-tune on 50,000 sampled interactions with a maximum sequence length of 500 tokens.
  For efficient serving, we combine FAISS HNSW indexing with an ONNX Runtime inference pipeline using INT8 dynamic quantization. On a review-to-title benchmark over 826,402 catalog items, our approach improves Recall@10 from 0.26 (BM25) to 0.66, while meeting practical latency and model-size constraints: 6.1 ms median CPU inference latency (batch size 1) and a 4x reduction in model size.
  Overall, we provide an end-to-end, reproducible blueprint for taking domain-adapted dense retrieval from offline training to CPU-efficient serving at catalog scale.

</details>


### [627] [Assessing the Impact of Image Dataset Features on Privacy-Preserving Machine Learning](https://arxiv.org/abs/2409.01329)
*Lucas Lange,Maurice-Maximilian Heykeroth,Erhard Rahm*

Main category: cs.LG

TL;DR: 该研究分析了图像数据集特征如何影响CNN模型的效用-隐私权衡，发现类别不平衡会增加少数类脆弱性但差分隐私可缓解，类别少的数据集表现更好，而高熵或低FDR的数据集会恶化效用-隐私平衡。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在敏感数据上面临安全挑战，可能被攻击并泄露信息。隐私保护机器学习通过差分隐私平衡效用和隐私，但需要了解数据集特征如何影响这种权衡。

Method: 通过分析多个数据集和隐私预算，研究图像数据集特征对私有和非私有CNN模型效用与脆弱性的影响。考察类别不平衡、类别数量、熵值和Fisher判别比等特征。

Result: 1) 不平衡数据集增加少数类脆弱性，但差分隐私能缓解此问题；2) 类别少的数据集同时改善模型效用和隐私；3) 高熵或低FDR的数据集恶化效用-隐私权衡。

Conclusion: 研究结果为从业者和研究者提供了基于数据集特征估计和优化效用-隐私权衡的指导，有助于根据数据集特性进行数据和隐私修改以获得更好结果。

Abstract: Machine Learning (ML) is crucial in many sectors, including computer vision. However, ML models trained on sensitive data face security challenges, as they can be attacked and leak information. Privacy-Preserving Machine Learning (PPML) addresses this by using Differential Privacy (DP) to balance utility and privacy. This study identifies image dataset characteristics that affect the utility and vulnerability of private and non-private Convolutional Neural Network (CNN) models. Through analyzing multiple datasets and privacy budgets, we find that imbalanced datasets increase vulnerability in minority classes, but DP mitigates this issue. Datasets with fewer classes improve both model utility and privacy, while high entropy or low Fisher Discriminant Ratio (FDR) datasets deteriorate the utility-privacy trade-off. These insights offer valuable guidance for practitioners and researchers in estimating and optimizing the utility-privacy trade-off in image datasets, helping to inform data and privacy modifications for better outcomes based on dataset characteristics.

</details>


### [628] [Hallucination is a Consequence of Space-Optimality: A Rate-Distortion Theorem for Membership Testing](https://arxiv.org/abs/2602.00906)
*Anxin Guo,Jingwei Li*

Main category: cs.LG

TL;DR: 论文提出信息论框架解释LLM幻觉：将事实记忆视为成员测试问题，在容量有限下最优策略是给某些非事实分配高置信度，导致幻觉不可避免


<details>
  <summary>Details</summary>
Motivation: 大语言模型经常对缺乏可推断模式的"随机事实"产生高置信度的幻觉。本文旨在从信息论角度解释这种现象，建立理论框架说明幻觉是容量限制下的最优策略

Method: 将事实记忆形式化为成员测试问题，统一Bloom滤波器的离散误差度量与LLM的连续对数损失。在事实稀疏的假设下，建立速率-失真定理，分析最优记忆效率与KL散度的关系

Result: 理论分析表明：即使在最优训练、完美数据和简化"封闭世界"设置下，有限容量下的信息论最优策略不是弃权或遗忘，而是给某些非事实分配高置信度，导致幻觉。在合成数据上验证了理论

Conclusion: 幻觉是损失压缩的自然结果，即使有最优训练和完美数据也无法避免。这为理解LLM幻觉提供了新的信息论视角，表明在容量限制下，给非事实分配高置信度是最优策略

Abstract: Large language models often hallucinate with high confidence on "random facts" that lack inferable patterns. We formalize the memorization of such facts as a membership testing problem, unifying the discrete error metrics of Bloom filters with the continuous log-loss of LLMs. By analyzing this problem in the regime where facts are sparse in the universe of plausible claims, we establish a rate-distortion theorem: the optimal memory efficiency is characterized by the minimum KL divergence between score distributions on facts and non-facts. This theoretical framework provides a distinctive explanation for hallucination: even with optimal training, perfect data, and a simplified "closed world" setting, the information-theoretically optimal strategy under limited capacity is not to abstain or forget, but to assign high confidence to some non-facts, resulting in hallucination. We validate this theory empirically on synthetic data, showing that hallucinations persist as a natural consequence of lossy compression.

</details>


### [629] [Federated Learning With Individualized Privacy Through Client Sampling](https://arxiv.org/abs/2501.17634)
*Lucas Lange,Ole Borchardt,Erhard Rahm*

Main category: cs.LG

TL;DR: 提出一种在联邦学习中实现个性化差分隐私的方法，通过根据客户端隐私偏好调整采样率，相比统一隐私保护基线有明显改进


<details>
  <summary>Details</summary>
Motivation: 随着用户数据收集担忧增加，个性化隐私成为平衡保护与效用的有前景方案。传统方法对所有用户实施统一匿名化水平，而个性化方法允许个体根据自身舒适度选择隐私设置

Method: 将SAMPLE算法从集中式设置扩展到联邦学习，基于客户端异构隐私预算计算客户端特定采样率，并将其集成到改进的IDP-FedAvg算法中

Result: 实验结果显示该方法相比统一差分隐私基线有明显改进，减少了隐私与效用之间的权衡。与相关工作中分配不同噪声尺度的SCALE方法相比，本方法表现显著更好

Conclusion: 该方法在联邦学习中成功实现了个性化差分隐私，但在非独立同分布数据的复杂任务中仍面临挑战，主要源于去中心化设置的限制

Abstract: With growing concerns about user data collection, individualized privacy has emerged as a promising solution to balance protection and utility by accounting for diverse user privacy preferences. Instead of enforcing a uniform level of anonymization for all users, this approach allows individuals to choose privacy settings that align with their comfort levels. Building on this idea, we propose an adapted method for enabling Individualized Differential Privacy (IDP) in Federated Learning (FL) by handling clients according to their personal privacy preferences. By extending the SAMPLE algorithm from centralized settings to FL, we calculate client-specific sampling rates based on their heterogeneous privacy budgets and integrate them into a modified IDP-FedAvg algorithm. We test this method under realistic privacy distributions and multiple datasets. The experimental results demonstrate that our approach achieves clear improvements over uniform DP baselines, reducing the trade-off between privacy and utility. Compared to the alternative SCALE method in related work, which assigns differing noise scales to clients, our method performs notably better. However, challenges remain for complex tasks with non-i.i.d. data, primarily stemming from the constraints of the decentralized setting.

</details>


### [630] [PyGALAX: An Open-Source Python Toolkit for Advanced Explainable Geospatial Machine Learning](https://arxiv.org/abs/2602.00907)
*Pingping Wang,Yihong Yuan,Lingcheng Li,Yongmei Lu*

Main category: cs.LG

TL;DR: PyGALAX是一个用于地理空间分析的Python包，集成了AutoML和XAI技术，通过自动模型选择和SHAP解释分析空间异质性，改进了传统的GWR方法。


<details>
  <summary>Details</summary>
Motivation: 传统地理加权回归(GWR)方法在处理空间非平稳性和复杂空间关系时存在局限性，需要更灵活、自动化的机器学习工具来分析和解释空间异质性。

Method: 基于GALAX框架改进，集成AutoML自动选择和优化机器学习模型，使用SHAP进行可解释性分析，新增自动带宽选择和灵活核函数选择功能。

Result: PyGALAX在空间建模中优于传统GWR方法，提供了更灵活、稳健的解决方案，能够处理不同数据集和研究问题，生成全局和局部尺度的透明洞察。

Conclusion: PyGALAX作为一个可访问、可复现、易部署的Python工具包，使先进的地理空间机器学习方法更易于地理学、城市规划、环境科学等领域的研究者和实践者使用。

Abstract: PyGALAX is a Python package for geospatial analysis that integrates automated machine learning (AutoML) and explainable artificial intelligence (XAI) techniques to analyze spatial heterogeneity in both regression and classification tasks. It automatically selects and optimizes machine learning models for different geographic locations and contexts while maintaining interpretability through SHAP (SHapley Additive exPlanations) analysis. PyGALAX builds upon and improves the GALAX framework (Geospatial Analysis Leveraging AutoML and eXplainable AI), which has proven to outperform traditional geographically weighted regression (GWR) methods. Critical enhancements in PyGALAX from the original GALAX framework include automatic bandwidth selection and flexible kernel function selection, providing greater flexibility and robustness for spatial modeling across diverse datasets and research questions. PyGALAX not only inherits all the functionalities of the original GALAX framework but also packages them into an accessible, reproducible, and easily deployable Python toolkit while providing additional options for spatial modeling. It effectively addresses spatial non-stationarity and generates transparent insights into complex spatial relationships at both global and local scales, making advanced geospatial machine learning methods accessible to researchers and practitioners in geography, urban planning, environmental science, and related fields.

</details>


### [631] [Reinforcement Learning via Conservative Agent for Environments with Random Delays](https://arxiv.org/abs/2507.18992)
*Jongsoo Lee,Jangwon Kim,Jiseok Jeong,Soohee Han*

Main category: cs.LG

TL;DR: 提出保守智能体，将随机延迟环境转换为恒定延迟环境，使现有恒定延迟方法可直接应用于随机延迟场景


<details>
  <summary>Details</summary>
Motivation: 现实世界强化学习常受环境延迟反馈影响，现有方法主要针对恒定延迟，随机延迟环境因变异性大而研究不足

Method: 提出保守智能体，将随机延迟环境重新表述为恒定延迟等价环境，无需修改算法结构即可应用现有恒定延迟方法

Result: 在连续控制任务中，基于保守智能体的算法在渐进性能和样本效率方面显著优于现有基线算法

Conclusion: 保守智能体为随机延迟环境提供了一种简单而强大的解决方案，使现有恒定延迟方法能够直接扩展到随机延迟场景

Abstract: Real-world reinforcement learning applications are often hindered by delayed feedback from environments, which violates the Markov assumption and introduces significant challenges. Although numerous delay-compensating methods have been proposed for environments with constant delays, environments with random delays remain largely unexplored due to their inherent variability and unpredictability. In this study, we propose a simple yet robust agent for decision-making under random delays, termed the conservative agent, which reformulates the random-delay environment into its constant-delay equivalent. This transformation enables any state-of-the-art constant-delay method to be directly extended to the random-delay environments without modifying the algorithmic structure or sacrificing performance. We evaluate the conservative agent-based algorithm on continuous control tasks, and empirical results demonstrate that it significantly outperforms existing baseline algorithms in terms of asymptotic performance and sample efficiency.

</details>


### [632] [Efficient Deep Learning for Medical Imaging: Bridging the Gap Between High-Performance AI and Clinical Deployment](https://arxiv.org/abs/2602.00910)
*Cuong Manh Nguyen,Truong-Son Hy*

Main category: cs.LG

TL;DR: 这篇综述系统梳理了面向医疗领域的高效轻量深度学习架构，将现代高效模型分为CNN、轻量Transformer和线性复杂度模型三大类，并评估了模型压缩策略在保持诊断性能的同时降低硬件需求的效果。


<details>
  <summary>Details</summary>
Motivation: 大规模深度学习模型在真实临床环境中部署面临挑战：计算成本高、延迟限制、云端处理的患者数据隐私问题。需要解决这些瓶颈，实现高性能AI与资源受限临床环境的结合。

Method: 1. 将现代高效模型分为三类：卷积神经网络(CNN)、轻量Transformer、新兴线性复杂度模型；2. 分析关键模型压缩策略：剪枝、量化、知识蒸馏、低秩分解；3. 评估这些策略在保持诊断性能的同时降低硬件需求的效果。

Result: 提供了医疗领域高效深度学习架构的全面综合，建立了模型分类框架，评估了不同压缩策略的效能，为资源受限临床环境中的AI部署提供了实用指导。

Conclusion: 该综述为研究人员和实践者提供了路线图，旨在弥合高性能AI与资源受限临床环境之间的差距，推动向设备端智能的过渡，解决当前临床部署的局限性。

Abstract: Deep learning has revolutionized medical image analysis, playing a vital role in modern clinical applications. However, the deployment of large-scale models in real-world clinical settings remains challenging due to high computational costs, latency constraints, and patient data privacy concerns associated with cloud-based processing. To address these bottlenecks, this review provides a comprehensive synthesis of efficient and lightweight deep learning architectures specifically tailored for the medical domain. We categorize the landscape of modern efficient models into three primary streams: Convolutional Neural Networks (CNNs), Lightweight Transformers, and emerging Linear Complexity Models. Furthermore, we examine key model compression strategies (including pruning, quantization, knowledge distillation, and low-rank factorization) and evaluate their efficacy in maintaining diagnostic performance while reducing hardware requirements. By identifying current limitations and discussing the transition toward on-device intelligence, this review serves as a roadmap for researchers and practitioners aiming to bridge the gap between high-performance AI and resource-constrained clinical environments.

</details>


### [633] [Early Classification of Time Series in Non-Stationary Cost Regimes](https://arxiv.org/abs/2602.00918)
*Aurélien Renault,Alexis Bondu,Antoine Cornuéjols,Vincent Lemaire*

Main category: cs.LG

TL;DR: 该论文研究时间序列早期分类（ECTS）在决策成本非平稳性下的鲁棒性问题，提出在线学习方法来适应成本漂移和随机变化。


<details>
  <summary>Details</summary>
Motivation: 现有ECTS方法假设决策成本已知、固定且正确指定，但实际中成本往往不确定且随时间变化，导致训练目标和部署目标不匹配。需要研究ECTS在成本非平稳性下的鲁棒性。

Method: 将代表性ECTS方法适配到在线学习设置，针对可分离方法仅更新触发模型而保持分类器固定。提出多种在线适配方法和基线，包括基于多臂赌博机和强化学习的方法，在合成数据上进行受控实验。

Result: 在线学习能有效提升ECTS方法对成本漂移的鲁棒性，其中基于强化学习的策略在不同成本机制下表现出强大且稳定的性能。

Conclusion: ECTS方法需要适应成本非平稳性，在线学习特别是强化学习方法能显著提升系统在动态成本环境下的鲁棒性和适应性。

Abstract: Early Classification of Time Series (ECTS) addresses decision-making problems in which predictions must be made as early as possible while maintaining high accuracy. Most existing ECTS methods assume that the time-dependent decision costs governing the learning objective are known, fixed, and correctly specified. In practice, however, these costs are often uncertain and may change over time, leading to mismatches between training-time and deployment-time objectives. In this paper, we study ECTS under two practically relevant forms of cost non-stationarity: drift in the balance between misclassification and decision delay costs, and stochastic realizations of decision costs that deviate from the nominal training-time model. To address these challenges, we revisit representative ECTS approaches and adapt them to an online learning setting. Focusing on separable methods, we update only the triggering model during deployment, while keeping the classifier fixed. We propose several online adaptations and baselines, including bandit-based and RL-based approaches, and conduct controlled experiments on synthetic data to systematically evaluate robustness under cost non-stationarity. Our results demonstrate that online learning can effectively improve the robustness of ECTS methods to cost drift, with RL-based strategies exhibiting strong and stable performance across varying cost regimes.

</details>


### [634] [Beyond What Seems Necessary: Hidden Gains from Scaling Training-Time Reasoning Length under Outcome Supervision](https://arxiv.org/abs/2602.00927)
*Yihao Xue,Allan Zhang,Jianhao Huang,Amit Sahai,Baharan Mirzasoleiman*

Main category: cs.LG

TL;DR: 研究发现：在仅监督结果的情况下，增加训练时推理长度（如RL的token预算或循环Transformer的循环次数）能持续提升分布外性能，即使分布内性能已饱和，表明鲁棒性需要比分布内验证更大的推理预算。


<details>
  <summary>Details</summary>
Motivation: 训练LLM进行更长的思考和推理已成为构建能解决复杂问题的最先进模型的关键要素。当前研究通过不同方式追求这一目标，如RL微调以引出长链式思维，或通过架构循环扩展潜在推理。这使得推理长度成为一个重要的扩展参数。

Method: 通过理论分析和实验验证：1）在合成任务中增加循环Transformer的循环次数；2）在数学推理任务中增加RL微调LLM时的token预算。从两个机制提供理论解释：自迭代能增强假设类的归纳偏置，以及正则化能减少对捷径解决方案的依赖。

Result: 实验证明：在仅监督结果的情况下，分布外性能会随着训练时推理长度的增加而持续改善，即使分布内性能已经饱和。这表明鲁棒性需要比仅基于分布内验证所显示的更大的推理预算。

Conclusion: 推理长度不仅影响分布内性能，更重要的是影响分布外泛化能力。当仅监督结果时，增加训练时推理长度可以持续提升鲁棒性，这为构建更稳健的AI系统提供了重要指导。

Abstract: Training LLMs to think and reason for longer has become a key ingredient in building state-of-the-art models that can solve complex problems previously out of reach. Recent efforts pursue this in different ways, such as RL fine-tuning to elicit long CoT or scaling latent reasoning through architectural recurrence. This makes reasoning length an important scaling knob. In this work, we identify a novel phenomenon (both theoretically and experimentally): under outcome-only supervision, out-of-distribution (OOD) performance can continue improving as training-time reasoning length (e.g., the token budget in RL, or the loop count in looped Transformers) increases, even after in-distribution (ID) performance has saturated. This suggests that robustness may require a larger budget than ID validation alone would indicate. We provide theoretical explanations via two mechanisms: (i) self-iteration can induce a stronger inductive bias in the hypothesis class, reshaping ID-optimal solutions in ways that improve OOD generalization; and (ii) when shortcut solutions that work for ID samples but not for OOD samples persist in the hypothesis class, regularization can reduce the learned solution's reliance on these shortcuts as the number of self-iterations increases. We complement the theory with empirical evidence from two realizations of scaling training-time reasoning length: increasing the number of loops in looped Transformers on a synthetic task, and increasing token budgets during RL fine-tuning of LLMs on mathematical reasoning.

</details>


### [635] [Continuous-Utility Direct Preference Optimization](https://arxiv.org/abs/2602.00931)
*Muhammad Ahmed Mohsin,Muhammad Umer,Ahsan Bilal,Zihao He,Muhammad Usman Rafique,Asad Aali,Muhammad Ali Jamshed,John M. Cioffi,Emily Fox*

Main category: cs.LG

TL;DR: CU-DPO框架用连续分数替代二元标签来捕捉细粒度推理质量，通过策略选择和执行优化两阶段训练，显著提升大语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型推理被视为单一能力，依赖二元偏好监督无法捕捉部分进展或细粒度推理质量，需要更精细的评估和优化方法。

Method: 提出连续效用直接偏好优化框架，包含两阶段训练：策略选择阶段通过最佳vs所有比较优化模型选择最佳策略；执行优化阶段使用边缘分层对训练模型正确执行选定策略。

Result: 在数学推理基准测试中，CU-DPO将策略选择准确率从35-46%提升到68-78%，在下游推理任务中获得高达6.6分的提升，并能有效迁移到分布外任务。

Conclusion: CU-DPO通过连续分数和策略组合优化，显著提升大语言模型的推理能力，证明细粒度监督信号比传统二元偏好更有效。

Abstract: Large language model reasoning is often treated as a monolithic capability, relying on binary preference supervision that fails to capture partial progress or fine-grained reasoning quality. We introduce Continuous Utility Direct Preference Optimization (CU-DPO), a framework that aligns models to a portfolio of prompt-based cognitive strategies by replacing binary labels with continuous scores that capture fine-grained reasoning quality. We prove that learning with K strategies yields a Theta(K log K) improvement in sample complexity over binary preferences, and that DPO converges to the entropy-regularized utility-maximizing policy. To exploit this signal, we propose a two-stage training pipeline: (i) strategy selection, which optimizes the model to choose the best strategy for a given problem via best-vs-all comparisons, and (ii) execution refinement, which trains the model to correctly execute the selected strategy using margin-stratified pairs. On mathematical reasoning benchmarks, CU-DPO improves strategy selection accuracy from 35-46 percent to 68-78 percent across seven base models, yielding consistent downstream reasoning gains of up to 6.6 points on in-distribution datasets with effective transfer to out-of-distribution tasks.

</details>


### [636] [SALAAD: Sparse And Low-Rank Adaptation via ADMM](https://arxiv.org/abs/2602.00942)
*Hao Ma,Melis Ilayda Bal,Liang Zhang,Bingcong Li,Niao He,Melanie Zeilinger,Michael Muehlebach*

Main category: cs.LG

TL;DR: SALAAD是一个即插即用的框架，可在训练过程中诱导稀疏和低秩结构，实现模型容量的灵活控制，减少部署内存消耗，且无需重新训练即可适应不同内存预算。


<details>
  <summary>Details</summary>
Motivation: 现代大语言模型在计算和内存受限环境下部署，需要灵活控制模型容量。现有方法通常依赖启发式设计，忽略了层和矩阵的异质性，或需要特定模型架构修改。

Method: 提出SALAAD框架，在增广拉格朗日框架下制定结构化权重学习，引入自适应控制器动态平衡训练损失和结构约束，保持标准训练动态稳定性，同时显式控制训练过程中有效模型容量的演化。

Result: 实验表明SALAAD显著减少部署时的内存消耗，性能与专门设计的方法相当。单次训练运行可产生连续的模型容量谱，无需重新训练即可在不同内存预算下实现平滑弹性部署。

Conclusion: SALAAD为不同模型架构提供即插即用的解决方案，通过诱导稀疏和低秩结构实现模型容量的灵活控制，在资源受限环境中具有重要应用价值。

Abstract: Modern large language models are increasingly deployed under compute and memory constraints, making flexible control of model capacity a central challenge. While sparse and low-rank structures naturally trade off capacity and performance, existing approaches often rely on heuristic designs that ignore layer and matrix heterogeneity or require model-specific architectural modifications. We propose SALAAD, a plug-and-play framework applicable to different model architectures that induces sparse and low-rank structures during training. By formulating structured weight learning under an augmented Lagrangian framework and introducing an adaptive controller that dynamically balances the training loss and structural constraints, SALAAD preserves the stability of standard training dynamics while enabling explicit control over the evolution of effective model capacity during training. Experiments across model scales show that SALAAD substantially reduces memory consumption during deployment while achieving performance comparable to ad-hoc methods. Moreover, a single training run yields a continuous spectrum of model capacities, enabling smooth and elastic deployment across diverse memory budgets without the need for retraining.

</details>


### [637] [Dynamic Prior Thompson Sampling for Cold-Start Exploration in Recommender Systems](https://arxiv.org/abs/2602.00943)
*Zhenyu Zhao,David Zhang,Ellie Zhao,Ehsan Saberian*

Main category: cs.LG

TL;DR: 提出动态先验汤普森采样方法，通过控制新物品超越现有优胜者的概率来精确调节探索强度，解决推荐系统冷启动问题中均匀先验导致的过度探索问题。


<details>
  <summary>Details</summary>
Motivation: 推荐系统冷启动中，新物品或数据稀疏物品需要流量来估计价值，但过度探索会损害用户体验并浪费曝光机会。实践中汤普森采样通常使用均匀Beta(1,1)先验，隐含假设新物品有50%成功率，当真实基础率远低于此值时，这种乐观先验会系统性地过度分配给弱物品。批量策略更新和管道延迟进一步放大了这个问题。

Method: 提出动态先验汤普森采样方法，核心贡献是推导出先验均值的闭式二次解，在引入新物品时强制满足P(X_j > Y_k) = epsilon的条件，使新物品超越现有优胜者的概率可控且可调，同时保留汤普森采样的贝叶斯更新机制。

Result: 通过蒙特卡洛验证、离线批量模拟以及在服务数百万用户的缩略图个性化系统上进行的大规模在线实验，动态先验方法相比均匀先验基线实现了精确的探索控制和改进的效率。

Conclusion: 动态先验汤普森采样为推荐系统冷启动探索提供了可预测且可调的解决方案，通过控制新物品超越现有优胜者的概率来平衡探索与利用，在保持贝叶斯更新优势的同时解决了均匀先验导致的过度探索问题。

Abstract: Cold-start exploration is a core challenge in large-scale recommender systems: new or data-sparse items must receive traffic to estimate value, but over-exploration harms users and wastes impressions. In practice, Thompson Sampling (TS) is often initialized with a uniform Beta(1,1) prior, implicitly assuming a 50% success rate for unseen items. When true base rates are far lower, this optimistic prior systematically over-allocates to weak items. The impact is amplified by batched policy updates and pipeline latency: for hours, newly launched items can remain effectively "no data," so the prior dominates allocation before feedback is incorporated. We propose Dynamic Prior Thompson Sampling, a prior design that directly controls the probability that a new arm outcompetes the incumbent winner. Our key contribution is a closed-form quadratic solution for the prior mean that enforces P(X_j > Y_k) = epsilon at introduction time, making exploration intensity predictable and tunable while preserving TS Bayesian updates. Across Monte Carlo validation, offline batched simulations, and a large-scale online experiment on a thumbnail personalization system serving millions of users, dynamic priors deliver precise exploration control and improved efficiency versus a uniform-prior baseline.

</details>


### [638] [Optimal Budgeted Adaptation of Large Language Models](https://arxiv.org/abs/2602.00952)
*Jing Wang,Jie Shen,Dean Foster,Zohar Karnin,Jeremy C Weiss*

Main category: cs.LG

TL;DR: 论文提出了一种预算感知的监督微调框架，将LLM适应建模为上下文Stackelberg博弈，通过标签查询策略在有限监督预算下实现高效微调。


<details>
  <summary>Details</summary>
Motivation: 大语言模型微调中标注数据可用性与下游准确性之间的权衡是一个核心挑战，需要一种能够在有限监督预算下实现高效微调的框架。

Method: 将LLM适应建模为上下文Stackelberg博弈：学习者（领导者）承诺评分策略和标签查询策略，自适应环境（跟随者）选择具有挑战性的监督替代方案。引入有限监督预算到学习目标中，并提出了带有最大延迟优先置信门的扩展框架。

Result: 在标准线性上下文假设下，算法在全反馈机制中实现了$\tilde{O}(d\sqrt{T})$的遗憾。扩展框架通过选择性标签查询，实现了$\tilde{O}(\sqrt{dB} + c\sqrt{B})$的预算感知遗憾界限，其中$B=βT$。

Conclusion: 该框架为预算受限的LLM监督微调提供了理论保证，通过博弈论方法和选择性标签查询策略，在有限标注数据下实现了高效的模型适应。

Abstract: The trade-off between labeled data availability and downstream accuracy remains a central challenge in fine-tuning large language models (LLMs). We propose a principled framework for \emph{budget-aware supervised fine-tuning} by casting LLM adaptation as a contextual Stackelberg game. In our formulation, the learner (leader) commits to a scoring policy and a label-querying strategy, while an adaptive environment (follower) selects challenging supervised alternatives in response. To explicitly address label efficiency, we incorporate a finite supervision budget directly into the learning objective. Our algorithm operates in the full-feedback regime and achieves $\tilde{O}(d\sqrt{T})$ regret under standard linear contextual assumptions. We extend the framework with a Largest-Latency-First (LLF) confidence gate that selectively queries labels, achieving a budget-aware regret bound of $\tilde{O}(\sqrt{dB} + c\sqrt{B})$ with $B=βT$.

</details>


### [639] [SAGE: Agentic Framework for Interpretable and Clinically Translatable Computational Pathology Biomarker Discovery](https://arxiv.org/abs/2602.00953)
*Sahar Almahfouz Nasser,Juan Francisco Pesantez Borja,Jincheng Liu,Tanvir Hasan,Zenghan Wang,Suman Ghosh,Sandeep Manandhar,Shikhar Shiromani,Twisha Shah,Naoto Tokuyama,Anant Madabhushi*

Main category: cs.LG

TL;DR: SAGE是一个基于代理AI的系统，旨在通过将图像特征与分子生物标志物和临床结果关联，生成可解释的病理学生物标志物。


<details>
  <summary>Details</summary>
Motivation: 当前AI病理模型多为黑箱且难以解释，限制了临床采用；而现有的工程化图像生物标志物往往基于轶事证据或零散文献，缺乏系统生物学验证。

Method: SAGE整合文献锚定推理和多模态数据分析，协调专门代理进行生物学背景化和经验假设验证，将图像特征与基因表达等分子生物标志物及临床结果关联。

Result: SAGE能够优先选择透明、有生物学支持的生物标志物，推动计算病理学的临床转化。

Conclusion: SAGE通过系统化生成和验证基于生物学证据的可解释生物标志物，解决了AI病理模型的黑箱问题，促进了临床采用。

Abstract: Despite significant progress in computational pathology, many AI models remain black-box and difficult to interpret, posing a major barrier to clinical adoption due to limited transparency and explainability. This has motivated continued interest in engineered image-based biomarkers, which offer greater interpretability but are often proposed based on anecdotal evidence or fragmented prior literature rather than systematic biological validation. We introduce SAGE (Structured Agentic system for hypothesis Generation and Evaluation), an agentic AI system designed to identify interpretable, engineered pathology biomarkers by grounding them in biological evidence. SAGE integrates literature-anchored reasoning with multimodal data analysis to correlate image-derived features with molecular biomarkers, such as gene expression, and clinically relevant outcomes. By coordinating specialized agents for biological contextualization and empirical hypothesis validation, SAGE prioritizes transparent, biologically supported biomarkers and advances the clinical translation of computational pathology.

</details>


### [640] [From drift to adaptation to the failed ml model: Transfer Learning in Industrial MLOps](https://arxiv.org/abs/2602.00957)
*Waqar Muhammad Ashraf,Talha Ansar,Fahad Ahmed,Jawad Hussain,Muhammad Mujtaba Abbas,Vivek Dua*

Main category: cs.LG

TL;DR: 本文比较了三种迁移学习模型更新策略（ETL、ALTL、LLTL）在火力发电厂数据漂移场景下的表现，发现ETL在5天批次大小下预测精度最高，而ALTL适合8天大批次更新。


<details>
  <summary>Details</summary>
Motivation: 当前MLOps中缺乏系统化的模型更新框架来处理数据漂移导致的模型失效问题，特别是在工业过程监控中，需要可靠的模型适应机制来应对生产环境变化。

Method: 采用三种迁移学习策略更新失效的前馈神经网络模型：集成迁移学习（ETL）、全层迁移学习（ALTL）和最后一层迁移学习（LLTL）。以660MW火力发电厂空气预热器烟气差压为案例，模拟电力负荷循环的批处理过程。

Result: ETL在5天批次大小下提供相对更高的预测精度，而ALTL适合8天大批次的有效更新。不同批次大小的计算需求（超参数调优和模型训练）呈现混合趋势。

Conclusion: 从批处理工业案例中获得的基础和实证见解可帮助MLOps从业者适应失效模型以应对数据漂移，实现工业过程的准确监控。不同迁移学习策略在不同批次大小下各有优势。

Abstract: Model adaptation to production environment is critical for reliable Machine Learning Operations (MLOps), less attention is paid to developing systematic framework for updating the ML models when they fail under data drift. This paper compares the transfer learning enabled model update strategies including ensemble transfer learning (ETL), all-layers transfer learning (ALTL), and last-layer transfer learning (LLTL) for updating the failed feedforward artificial neural network (ANN) model. The flue gas differential pressure across the air preheater unit installed in a 660 MW thermal power plant is analyzed as a case study since it mimics the batch processes due to load cycling in the power plant. Updating the failed ANN model by three transfer learning techniques reveals that ETL provides relatively higher predictive accuracy for the batch size of 5 days than those of LLTL and ALTL. However, ALTL is found to be suitable for effective update of the model trained on large batch size (8 days). A mixed trend is observed for computational requirement (hyperparameter tuning and model training) of model update techniques for different batch sizes. These fundamental and empiric insights obtained from the batch process-based industrial case study can assist the MLOps practitioners in adapting the failed models to data drifts for the accurate monitoring of industrial processes.

</details>


### [641] [Probing the Knowledge Boundary: An Interactive Agentic Framework for Deep Knowledge Extraction](https://arxiv.org/abs/2602.00959)
*Yuheng Yang,Siqi Zhu,Tao Feng,Ge Liu,Jiaxuan You*

Main category: cs.LG

TL;DR: 提出一个交互式智能体框架，通过自适应探索策略系统提取和量化LLMs的知识边界，发现递归分类法最有效，观察到知识缩放定律，并识别出Pass@1与Pass@k的权衡。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型可以被视为压缩的知识库，但现有基准大多是静态的，对系统知识探测支持有限。需要了解LLMs真正包含什么知识以及其知识边界如何延伸。

Method: 提出交互式智能体框架，包含四种自适应探索策略在不同粒度上探测知识。采用三阶段知识处理流程：向量过滤去除重复、LLM裁决解决语义重叠、领域相关性审计保留有效知识单元。

Result: 递归分类法是最有效的探索策略；观察到清晰的知识缩放定律，更大模型提取更多知识；发现Pass@1与Pass@k权衡：领域专用模型初始准确率高但快速下降，通用模型在扩展提取中保持稳定；训练数据组成差异导致不同模型家族具有可测量的知识特征。

Conclusion: 提出的交互式框架能系统提取和量化LLMs知识，揭示了有效的探索策略、知识缩放规律、性能权衡模式以及训练数据对知识特征的影响。

Abstract: Large Language Models (LLMs) can be seen as compressed knowledge bases, but it remains unclear what knowledge they truly contain and how far their knowledge boundaries extend. Existing benchmarks are mostly static and provide limited support for systematic knowledge probing. In this paper, we propose an interactive agentic framework to systematically extract and quantify the knowledge of LLMs. Our method includes four adaptive exploration policies to probe knowledge at different granularities. To ensure the quality of extracted knowledge, we introduce a three-stage knowledge processing pipeline that combines vector-based filtering to remove exact duplicates, LLM-based adjudication to resolve ambiguous semantic overlaps, and domain-relevance auditing to retain valid knowledge units. Through extensive experiments, we find that recursive taxonomy is the most effective exploration strategy. We also observe a clear knowledge scaling law, where larger models consistently extract more knowledge. In addition, we identify a Pass@1-versus-Pass@k trade-off: domain-specialized models achieve higher initial accuracy but degrade rapidly, while general-purpose models maintain stable performance during extended extraction. Finally, our results show that differences in training data composition lead to distinct and measurable knowledge profiles across model families.

</details>


### [642] [On the Spectral Flattening of Quantized Embeddings](https://arxiv.org/abs/2602.00969)
*Junlin Huang,Wenyi Fang,Zhenheng Tang,Yuxin Wang,Xueze Kang,Yang Zheng,Bo Li,Xiaowen Chu*

Main category: cs.LG

TL;DR: 该论文揭示了LLM在超低精度训练中的不稳定性源于Zipf统计特性与量化噪声的冲突，证明了语义编码需要幂律奇异值谱，而均匀量化会截断谱尾导致表示崩溃。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在超低精度训练中存在不稳定性问题，这种不稳定性源于离散量化约束与语言数据固有的重尾谱特性之间的冲突。需要理解这种冲突的数学本质及其对模型性能的影响。

Method: 通过形式化Zipf统计与随机矩阵理论之间的联系，证明嵌入的奇异值谱的幂律衰减是语义编码的基本要求。推导理论界限，表明均匀量化引入的噪声会不成比例地截断谱尾，导致谱平坦化和稳定秩增加。在GPT-2和TinyLlama等架构上进行实证验证。

Result: 理论分析表明均匀量化会截断谱尾，导致谱平坦化和表示稳定秩增加。实证验证证实这种几何退化会导致表示崩溃。研究不仅量化了LLM的谱敏感性，还确立了谱保真度作为稳定低比特优化的必要条件。

Conclusion: LLM在超低精度训练中的不稳定性源于量化噪声与语言数据幂律谱特性之间的根本冲突。谱保真度是稳定低比特优化的关键条件，这为设计更有效的量化方法提供了理论基础。

Abstract: Training Large Language Models (LLMs) at ultra-low precision is critically impeded by instability rooted in the conflict between discrete quantization constraints and the intrinsic heavy-tailed spectral nature of linguistic data. By formalizing the connection between Zipfian statistics and random matrix theory, we prove that the power-law decay in the singular value spectra of embeddings is a fundamental requisite for semantic encoding. We derive theoretical bounds showing that uniform quantization introduces a noise floor that disproportionately truncates this spectral tail, which induces spectral flattening and a strictly provable increase in the stable rank of representations. Empirical validation across diverse architectures including GPT-2 and TinyLlama corroborates that this geometric degradation precipitates representational collapse. This work not only quantifies the spectral sensitivity of LLMs but also establishes spectral fidelity as a necessary condition for stable low-bit optimization.

</details>


### [643] [Forest-Guided Semantic Transport for Label-Supervised Manifold Alignment](https://arxiv.org/abs/2602.00974)
*Adrien Aumon,Myriam Lizotte,Guy Wolf,Kevin R. Moon,Jake S. Rhodes*

Main category: cs.LG

TL;DR: FoSTA使用森林引导的几何结构来去噪并恢复任务相关流形，通过层次语义传输对齐多模态数据，在合成基准和单细胞应用中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有标签监督流形对齐方法大多依赖欧几里得几何建模域内关系，当特征与任务相关性弱时会产生噪声和语义误导结构，导致对齐质量下降。

Method: FoSTA利用森林诱导的几何结构去噪域内结构，从标签信息的森林亲和性构建语义表示，通过快速层次语义传输对齐跨域关系。

Result: 与现有基线相比，FoSTA在合成基准上改进了对应关系恢复和标签转移，在单细胞应用（批次校正和生物保守性）中表现出强大性能。

Conclusion: FoSTA通过森林引导的语义传输对齐框架，有效解决了传统欧几里得几何方法的局限性，在多模态数据对齐任务中实现了更高质量的语义对齐。

Abstract: Label-supervised manifold alignment bridges the gap between unsupervised and correspondence-based paradigms by leveraging shared label information to align multimodal datasets. Still, most existing methods rely on Euclidean geometry to model intra-domain relationships. This approach can fail when features are only weakly related to the task of interest, leading to noisy, semantically misleading structure and degraded alignment quality. To address this limitation, we introduce FoSTA (Forest-guided Semantic Transport Alignment), a scalable alignment framework that leverages forest-induced geometry to denoise intra-domain structure and recover task-relevant manifolds prior to alignment. FoSTA builds semantic representations directly from label-informed forest affinities and aligns them via fast, hierarchical semantic transport, capturing meaningful cross-domain relationships. Extensive comparisons with established baselines demonstrate that FoSTA improves correspondence recovery and label transfer on synthetic benchmarks and delivers strong performance in practical single-cell applications, including batch correction and biological conservation.

</details>


### [644] [Scalable Random Wavelet Features: Efficient Non-Stationary Kernel Approximation with Convergence Guarantees](https://arxiv.org/abs/2602.00987)
*Sawan Kumar,Souvik Chakraborty*

Main category: cs.LG

TL;DR: 提出Random Wavelet Features (RWF)框架，通过采样小波族构建可扩展的非平稳核近似，填补了表达能力强但计算复杂的模型与可扩展但有限的平稳方法之间的空白。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的许多过程是非平稳的（统计特性随输入域变化），但现有可扩展方法大多依赖平稳性假设。这迫使在表达能力强但计算复杂的模型（如深度高斯过程）与可扩展但有限的平稳方法（如随机傅里叶特征）之间做出艰难权衡。

Method: 引入随机小波特征(RWF)框架，通过从小波族中采样来构建可扩展的非平稳核近似。利用小波固有的局部化和多分辨率结构，生成能够捕捉复杂、输入依赖模式的显式特征映射。

Result: RWF在理论上具有正定性、无偏性和一致收敛保证。在多个具有挑战性的合成和真实数据集上的实证表明，RWF优于平稳随机特征，并与更复杂模型相比提供了有竞争力的准确率-效率权衡。

Conclusion: RWF填补了可扩展核方法在非平稳问题上的空白，为广泛类别的现实世界非平稳问题提供了可扩展且表达性强的核方法解决方案。

Abstract: Modeling non-stationary processes, where statistical properties vary across the input domain, is a critical challenge in machine learning; yet most scalable methods rely on a simplifying assumption of stationarity. This forces a difficult trade-off: use expressive but computationally demanding models like Deep Gaussian Processes, or scalable but limited methods like Random Fourier Features (RFF). We close this gap by introducing Random Wavelet Features (RWF), a framework that constructs scalable, non-stationary kernel approximations by sampling from wavelet families. By harnessing the inherent localization and multi-resolution structure of wavelets, RWF generates an explicit feature map that captures complex, input-dependent patterns. Our framework provides a principled way to generalize RFF to the non-stationary setting and comes with a comprehensive theoretical analysis, including positive definiteness, unbiasedness, and uniform convergence guarantees. We demonstrate empirically on a range of challenging synthetic and real-world datasets that RWF outperforms stationary random features and offers a compelling accuracy-efficiency trade-off against more complex models, unlocking scalable and expressive kernel methods for a broad class of real-world non-stationary problems.

</details>


### [645] [ESSAM: A Novel Competitive Evolution Strategies Approach to Reinforcement Learning for Memory Efficient LLMs Fine-Tuning](https://arxiv.org/abs/2602.01003)
*Zhishen Sun,Sizhe Dang,Guang Dai,Haishan Ye*

Main category: cs.LG

TL;DR: ESSAM结合进化策略和锐度感知最大化，在数学推理任务上实现与RL相当的性能，同时大幅降低GPU内存使用


<details>
  <summary>Details</summary>
Motivation: 强化学习在提升大语言模型数学推理能力时GPU内存使用过高，限制了其在资源受限环境中的应用

Method: 提出ESSAM框架，将进化策略的零阶参数空间搜索与锐度感知最大化紧密结合，进行全参数微调

Result: 在GSM8K数学推理任务上平均准确率达78.27%，性能与RL方法相当，GPU内存使用比PPO降低18倍，比GRPO降低10倍

Conclusion: ESSAM在保持与RL方法相当性能的同时，显著降低了GPU内存需求，为资源受限环境提供了有效的数学推理微调方案

Abstract: Reinforcement learning (RL) has become a key training step for improving mathematical reasoning in large language models (LLMs), but it often has high GPU memory usage, which makes it hard to use in settings with limited resources. To reduce these issues, we propose Evolution Strategies with Sharpness-Aware Maximization (ESSAM), a full parameter fine-tuning framework that tightly combines the zero-order search in parameter space from Evolution Strategies (ES) with the Sharpness-Aware Maximization (SAM) to improve generalization. We conduct fine-tuning experiments on the mainstream mathematica reasoning task GSM8K. The results show that ESSAM achieves an average accuracy of 78.27\% across all models and its overall performance is comparable to RL methods. It surpasses classic RL algorithm PPO with an accuracy of 77.72\% and is comparable to GRPO with an accuracy of 78.34\%, and even surpassing them on some models. In terms of GPU memory usage, ESSAM reduces the average GPU memory usage by $18\times$ compared to PPO and by $10\times$ compared to GRPO, achieving an extremely low GPU memory usage.

</details>


### [646] [Predicting Anemia Among Under-Five Children in Nepal Using Machine Learning and Deep Learning](https://arxiv.org/abs/2602.01005)
*Deepak Bastola,Pitambar Acharya,Dipak Dulal,Rabina Dhakal,Yang Li*

Main category: cs.LG

TL;DR: 使用机器学习预测尼泊尔儿童贫血风险，通过多种特征选择方法识别关键预测因子，比较传统机器学习与深度学习模型性能。


<details>
  <summary>Details</summary>
Motivation: 儿童贫血是尼泊尔重大公共卫生问题，与生长发育受损、认知障碍和发病率增加相关。需要开发有效的预测模型来识别高风险儿童，为公共卫生干预提供依据。

Method: 使用NDHS 2022数据（1,855名6-59个月儿童），将贫血定义为二分类问题。采用四种特征选择方法（卡方检验、互信息、点二列相关、Boruta）识别关键特征。比较八种传统机器学习模型（LR、KNN、DT、RF、XGBoost、SVM、NB、LDA）和两种深度学习模型（DNN、TabNet），重点关注F1分数和召回率。

Result: 逻辑回归获得最佳召回率（0.701）和最高F1分数（0.649），DNN达到最高准确率（0.709），SVM获得最强区分能力（最高AUC 0.736）。所有方法一致选择的五个关键特征：儿童年龄、近期发热、家庭规模、母亲贫血状况和驱虫治疗。

Conclusion: 机器学习和深度学习模型都能提供有竞争力的贫血预测能力。儿童年龄、感染指标、母亲贫血状况和驱虫历史等可解释特征对尼泊尔儿童贫血风险分层和公共卫生筛查至关重要。

Abstract: Childhood anemia remains a major public health challenge in Nepal and is associated with impaired growth, cognition, and increased morbidity. Using World Health Organization hemoglobin thresholds, we defined anemia status for children aged 6-59 months and formulated a binary classification task by grouping all anemia severities as \emph{anemic} versus \emph{not anemic}. We analyzed Nepal Demographic and Health Survey (NDHS 2022) microdata comprising 1,855 children and initially considered 48 candidate features spanning demographic, socioeconomic, maternal, and child health characteristics. To obtain a stable and substantiated feature set, we applied four features selection techniques (Chi-square, mutual information, point-biserial correlation, and Boruta) and prioritized features supported by multi-method consensus. Five features: child age, recent fever, household size, maternal anemia, and parasite deworming were consistently selected by all methods, while amenorrhea, ethnicity indicators, and provinces were frequently retained. We then compared eight traditional machine learning classifiers (LR, KNN, DT, RF, XGBoost, SVM, NB, LDA) with two deep learning models (DNN and TabNet) using standard evaluation metrics, emphasizing F1-score and recall due to class imbalance. Among all models, logistic regression attained the best recall (0.701) and the highest F1-score (0.649), while DNN achieved the highest accuracy (0.709), and SVM yielded the strongest discrimination with the highest AUC (0.736). Overall, the results indicate that both machine learning and deep learning models can provide competitive anemia prediction and the interpretable features such as child age, infection proxy, maternal anemia, and deworming history are central for risk stratification and public health screening in Nepal.

</details>


### [647] [LASS-ODE: Scaling ODE Computations to Connect Foundation Models with Dynamical Physical Systems](https://arxiv.org/abs/2602.01009)
*Haoran Li,Chenhan Xiao,Lihao Mai,Yang Weng,Erik Blasch*

Main category: cs.LG

TL;DR: LASS-ODE提出了一种可扩展的ODE基础模型，通过局部线性ODE表示和跨系统注意力机制，解决了物理计算可扩展性和知识共享效率问题。


<details>
  <summary>Details</summary>
Motivation: 基础模型在语言、视觉和时间序列分析中取得了成功，但在物理系统动态预测方面进展有限。主要面临两个挑战：1) 物理计算可扩展性 - 物理约束学习计算量大，难以扩展到大规模系统；2) 知识共享效率 - 注意力机制主要在单个系统内计算，限制了跨系统ODE结构的共享。

Method: 1) 提出局部线性ODE表示：通过令牌级局部线性ODE表示保持物理保真度，避免昂贵的非线性积分计算；2) 引入跨系统注意力机制：通过公共结构中心(CSH)存储共享令牌，聚合跨系统知识；3) 在40GB ODE轨迹数据集上预训练LASS-ODE模型。

Result: LASS-ODE在领域内表现出色，能够零样本泛化到不同的ODE系统，并通过微调获得额外改进。模型实现了物理计算的可扩展性和跨系统知识的高效共享。

Conclusion: 通过局部线性ODE表示和跨系统注意力机制，LASS-ODE成功构建了可扩展的ODE基础模型，解决了物理约束学习中的计算可扩展性和知识共享问题，为物理系统动态预测提供了新范式。

Abstract: Foundation models have transformed language, vision, and time series data analysis, yet progress on dynamic predictions for physical systems remains limited. Given the complexity of physical constraints, two challenges stand out. $(i)$ Physics-computation scalability: physics-informed learning can enforce physical regularization, but its computation (e.g., ODE integration) does not scale to extensive systems. $(ii)$ Knowledge-sharing efficiency: the attention mechanism is primarily computed within each system, which limits the extraction of shared ODE structures across systems. We show that enforcing ODE consistency does not require expensive nonlinear integration: a token-wise locally linear ODE representation preserves physical fidelity while scaling to foundation-model regimes. Thus, we propose novel token representations that respect locally linear ODE evolution. Such linearity substantially accelerates integration while accurately approximating the local data manifold. Second, we introduce a simple yet effective inter-system attention that augments attention with a common structure hub (CSH) that stores shared tokens and aggregates knowledge across systems. The resulting model, termed LASS-ODE (\underline{LA}rge-\underline{S}cale \underline{S}mall \underline{ODE}), is pretrained on our $40$GB ODE trajectory collections to enable strong in-domain performance, zero-shot generalization across diverse ODE systems, and additional improvements through fine-tuning.

</details>


### [648] [How Does Unfaithful Reasoning Emerge from Autoregressive Training? A Study of Synthetic Experiments](https://arxiv.org/abs/2602.01017)
*Fuxin Wang,Amr Alazali,Yiqiao Zhong*

Main category: cs.LG

TL;DR: 该论文通过受控实验发现，语言模型的思维链推理只有在训练噪声低于临界阈值时才能保持忠实性，否则会转向不忠实的跳跃式推理，并揭示了模型通过解决不一致推理步骤来编码内部不确定性的机制。


<details>
  <summary>Details</summary>
Motivation: 尽管观察到大型语言模型的思维链推理经常不忠实（中间步骤逻辑不一致或未能反映导致最终答案的因果关系），但对思维链的基本理解仍然缺乏——什么是忠实的思维链推理，以及不忠实性如何从自回归训练中产生。

Method: 使用受控合成实验，在小规模Transformer上训练噪声数据，以逐步解决模算术表达式（称为算术表达式推理任务），分析不同噪声水平下的训练动态和推理模式。

Result: 发现模型只有在训练噪声低于临界阈值时才能学习忠实的推理（因果遵循底层算术规则）；在更高噪声水平下，训练动态呈现从忠实逐步推理到不忠实跳跃式推理的转变，中间出现预测熵暂时增加的混合模式；机制分析显示模型通过解决不一致推理步骤来编码内部不确定性。

Conclusion: 思维链的忠实性受训练噪声水平影响，存在临界阈值；模型能够通过解决推理不一致性来编码内部不确定性，这表明自回归训练中出现了隐式的自我验证机制。

Abstract: Chain-of-thought (CoT) reasoning generated by large language models (LLMs) is often unfaithful: intermediate steps can be logically inconsistent or fail to reflect the causal relationship leading to the final answer. Despite extensive empirical observations, a fundamental understanding of CoT is lacking--what constitutes faithful CoT reasoning, and how unfaithfulness emerges from autoregressive training. We study these questions using well-controlled synthetic experiments, training small transformers on noisy data to solve modular arithmetic expressions step by step, a task we term Arithmetic Expression Reasoning. We find that models can learn faithful reasoning that causally follows the underlying arithmetic rules, but only when the training noise is below a critical threshold, a phenomenon attributable to simplicity bias. At higher noise levels, training dynamics exhibit a transition from faithful stepwise reasoning to unfaithful skip-step reasoning via an intermediate mixed mode characterized by a transient increase in prediction entropy. Mechanistic analysis reveals that models learn to encode internal uncertainty by resolving inconsistent reasoning steps, which suggests the emergence of implicit self-verification from autoregressive training.

</details>


### [649] [Toward Universal and Transferable Jailbreak Attacks on Vision-Language Models](https://arxiv.org/abs/2602.01025)
*Kaiyuan Cui,Yige Li,Yutao Wu,Xingjun Ma,Sarah Erfani,Christopher Leckie,Hanxun Huang*

Main category: cs.LG

TL;DR: UltraBreak提出了一种通用且可迁移的视觉语言模型越狱框架，通过视觉空间正则化和语义引导的文本监督，解决了现有梯度方法过拟合单一白盒模型、难以迁移到黑盒模型的问题。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型扩展了大型语言模型的视觉能力，但也扩大了攻击面，使模型面临图像越狱攻击。现有基于梯度的越狱方法迁移性差，因为对抗模式会过拟合单一白盒代理模型，无法泛化到黑盒模型。

Method: UltraBreak框架在视觉空间通过变换和正则化约束对抗模式，同时在文本空间通过语义目标放松约束。通过在目标LLM的文本嵌入空间定义损失函数，发现通用的对抗模式，结合视觉级正则化和语义引导的文本监督，减轻代理过拟合并实现跨模型和攻击目标的强迁移性。

Result: 大量实验表明，UltraBreak始终优于先前的越狱方法。进一步分析揭示了早期方法无法迁移的原因，强调通过语义目标平滑损失景观对于实现通用和可迁移越狱至关重要。

Conclusion: UltraBreak通过视觉空间正则化和语义引导的文本监督，成功解决了视觉语言模型越狱攻击的迁移性问题，实现了跨模型和攻击目标的强泛化能力，为理解和防御多模态越狱攻击提供了新视角。

Abstract: Vision-language models (VLMs) extend large language models (LLMs) with vision encoders, enabling text generation conditioned on both images and text. However, this multimodal integration expands the attack surface by exposing the model to image-based jailbreaks crafted to induce harmful responses. Existing gradient-based jailbreak methods transfer poorly, as adversarial patterns overfit to a single white-box surrogate and fail to generalise to black-box models. In this work, we propose Universal and transferable jailbreak (UltraBreak), a framework that constrains adversarial patterns through transformations and regularisation in the vision space, while relaxing textual targets through semantic-based objectives. By defining its loss in the textual embedding space of the target LLM, UltraBreak discovers universal adversarial patterns that generalise across diverse jailbreak objectives. This combination of vision-level regularisation and semantically guided textual supervision mitigates surrogate overfitting and enables strong transferability across both models and attack targets. Extensive experiments show that UltraBreak consistently outperforms prior jailbreak methods. Further analysis reveals why earlier approaches fail to transfer, highlighting that smoothing the loss landscape via semantic objectives is crucial for enabling universal and transferable jailbreaks. The code is publicly available in our \href{https://github.com/kaiyuanCui/UltraBreak}{GitHub repository}.

</details>


### [650] [SFMP: Fine-Grained, Hardware-Friendly and Search-Free Mixed-Precision Quantization for Large Language Models](https://arxiv.org/abs/2602.01027)
*Xin Nie,Haicheng Zhang,Liang Dong,Beining Feng,Jinhong Weng,Guiling Sun*

Main category: cs.LG

TL;DR: SFMP提出了一种无需搜索、硬件友好的混合精度量化框架，通过分数位宽、块级混合精度、行列重排序和统一GEMM内核四个创新点，在相同内存约束下优于现有层级混合精度方法。


<details>
  <summary>Details</summary>
Motivation: 现有混合精度量化方法存在两个主要问题：要么依赖昂贵的离散优化来确定精度分配，要么由于不规则内存布局导致硬件效率低下。需要一种既高效又硬件友好的混合精度量化方案。

Method: 1) 分数位宽：将权重矩阵的整数位宽扩展为分数值，将离散精度分配转化为连续问题；2) 块级混合精度：在权重矩阵内实现细粒度精度分配，同时保持硬件友好；3) 行列权重重排序：通过行列重排序聚合重要权重，推理时仅产生小的激活重排序开销；4) 统一GEMM内核：支持任意平均位宽的混合精度GEMM运算。

Result: 大量实验表明，SFMP在相同内存约束下优于最先进的层级混合精度方法，同时显著降低了量化成本并提高了推理效率。

Conclusion: SFMP框架成功解决了现有混合精度量化方法的局限性，提供了一种无需搜索、硬件友好的高效解决方案，为大型语言模型在严格内存预算下的压缩提供了有效途径。

Abstract: Mixed-precision quantization is a promising approach for compressing large language models under tight memory budgets. However, existing mixed-precision methods typically suffer from one of two limitations: they either rely on expensive discrete optimization to determine precision allocation, or introduce hardware inefficiencies due to irregular memory layouts. We propose SFMP, a search-free and hardware-friendly mixed-precision quantization framework for large language models. The framework is built upon four novel ideas: Fractional bit-width, which extends integer bit-width for weight matrix to fractional value and transforms discrete precision allocation as a continuous problem; 2)Block-wise mixed-precision, enabling fine-grained precision within weight matrices while remaining hardware-friendly; 3)Row-column weight reordering, which aggregates salient weights via row and column reordering, incurring only a small activation reordering overhead during inference; 4)Unified GEMM kernel, which supports mixed-precision GEMM at arbitrary average bit-width. Extensive experiments demonstrate that SFMP outperforms state-of-the-art layer-wise mixed-precision methods under the same memory constraints, while significantly reducing quantization cost and improving inference efficiency. Code is available at https://github.com/Nkniexin/SFMP

</details>


### [651] [Adaptive Dual-Weighting Framework for Federated Learning via Out-of-Distribution Detection](https://arxiv.org/abs/2602.01039)
*Zhiwei Ling,Hailiang Zhao,Chao Zhang,Xiang Ao,Ziqi Wang,Cheng Zhang,Zhen Qin,Xinkui Zhao,Kingsum Chow,Yuanqing Wu,MengChu Zhou*

Main category: cs.LG

TL;DR: FLood：基于OOD检测的联邦学习框架，通过双重加权机制应对数据异构性，提升模型收敛稳定性和泛化能力


<details>
  <summary>Details</summary>
Motivation: 现实世界联邦学习部署中，用户、设备和应用场景的异构性导致数据非独立同分布，严重影响全局模型的收敛稳定性、泛化能力和服务质量

Method: 提出FLood框架，采用双重加权机制：客户端层面通过提升伪OOD样本权重自适应重加权监督损失；服务器层面根据客户端OOD置信度分数加权聚合，优先考虑分布一致性更高的客户端更新

Result: 在多种非IID设置下的多个基准测试中，FLood在准确率和泛化能力上均优于现有最先进的联邦学习方法，且可作为正交插件模块无缝集成到现有算法中

Conclusion: FLood是一个实用且可扩展的解决方案，能够在现实世界联邦环境中部署可靠的智能服务，通过动态抵消异构性负面影响提升模型性能

Abstract: Federated Learning (FL) enables collaborative model training across large-scale distributed service nodes while preserving data privacy, making it a cornerstone of intelligent service systems in edge-cloud environments. However, in real-world service-oriented deployments, data generated by heterogeneous users, devices, and application scenarios are inherently non-IID. This severe data heterogeneity critically undermines the convergence stability, generalization ability, and ultimately the quality of service delivered by the global model. To address this challenge, we propose FLood, a novel FL framework inspired by out-of-distribution (OOD) detection. FLood dynamically counteracts the adverse effects of heterogeneity through a dual-weighting mechanism that jointly governs local training and global aggregation. At the client level, it adaptively reweights the supervised loss by upweighting pseudo-OOD samples, thereby encouraging more robust learning from distributionally misaligned or challenging data. At the server level, it refines model aggregation by weighting client contributions according to their OOD confidence scores, prioritizing updates from clients with higher in-distribution consistency and enhancing the global model's robustness and convergence stability. Extensive experiments across multiple benchmarks under diverse non-IID settings demonstrate that FLood consistently outperforms state-of-the-art FL methods in both accuracy and generalization. Furthermore, FLood functions as an orthogonal plug-in module: it seamlessly integrates with existing FL algorithms to boost their performance under heterogeneity without modifying their core optimization logic. These properties make FLood a practical and scalable solution for deploying reliable intelligent services in real-world federated environments.

</details>


### [652] [SwiftRepertoire: Few-Shot Immune-Signature Synthesis via Dynamic Kernel Codes](https://arxiv.org/abs/2602.01051)
*Rong Fu,Wenxin Zhang,Muge Qi,Yang Li,Yabin Jin,Jiekai Wu,Jiaxuan Lu,Chunlei Meng,Youjin Wang,Zeli Su,Juntao Gao,Li Bao,Qi Zhao,Wei Luo,Simon Fong*

Main category: cs.LG

TL;DR: 提出一个基于原型字典的T细胞受体分析框架，通过轻量级任务描述符生成小型适配器模块，实现少样本快速适应新任务，无需完整模型微调


<details>
  <summary>Details</summary>
Motivation: T细胞受体分析在疾病检测和免疫监测中具有重要价值，但实际部署面临标签稀疏、队列异质性以及大型编码器适应新任务的计算负担等挑战

Method: 基于原型字典的框架，从轻量级任务描述符（源自repertoire探针和池化嵌入统计）合成紧凑的任务特定参数化，生成小型适配器模块应用于冻结的预训练骨干网络

Result: 实现了样本高效的任务适应，仅需少量支持样本即可快速适应新任务，同时保持可解释性，通过motif感知探针和校准的motif发现流程将预测决策与序列级信号关联

Conclusion: 该框架为将repertoire信息模型转化为临床和研究应用提供了实用、样本高效且可解释的途径，特别适用于标签数据稀缺和计算资源受限的场景

Abstract: Repertoire-level analysis of T cell receptors offers a biologically grounded signal for disease detection and immune monitoring, yet practical deployment is impeded by label sparsity, cohort heterogeneity, and the computational burden of adapting large encoders to new tasks. We introduce a framework that synthesizes compact task-specific parameterizations from a learned dictionary of prototypes conditioned on lightweight task descriptors derived from repertoire probes and pooled embedding statistics. This synthesis produces small adapter modules applied to a frozen pretrained backbone, enabling immediate adaptation to novel tasks with only a handful of support examples and without full model fine-tuning. The architecture preserves interpretability through motif-aware probes and a calibrated motif discovery pipeline that links predictive decisions to sequence-level signals. Together, these components yield a practical, sample-efficient, and interpretable pathway for translating repertoire-informed models into diverse clinical and research settings where labeled data are scarce and computational resources are constrained.

</details>


### [653] [LRAgent: Efficient KV Cache Sharing for Multi-LoRA LLM Agents](https://arxiv.org/abs/2602.01053)
*Hyesung Jeon,Hyeongju Ha,Jae-Joon Kim*

Main category: cs.LG

TL;DR: LRAgent：针对多LoRA智能体系统的KV缓存共享框架，通过分解缓存为共享基础组件和适配器依赖组件，显著降低内存和计算开销，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 多LLM智能体系统中，虽然智能体共享预训练骨干网络，但每个智能体独立存储自己的KV缓存，导致相同长工具增强轨迹的重复存储，造成巨大的内存和计算开销。现有KV缓存共享方法大多忽略了这种多LoRA设置。

Method: 提出LRAgent框架：1) 将KV缓存分解为来自预训练权重的共享基础组件和来自LoRA权重的适配器依赖组件；2) 通过共享基础组件并以低秩形式存储适配器组件来减少内存开销；3) 通过共享低秩缓存避免重复计算来减少计算开销；4) 引入Flash-LoRA-Attention内核，重新排序注意力计算以避免将低秩缓存物化为完整维度。

Result: LRAgent实现了接近完全共享缓存的吞吐量和首次令牌延迟时间，同时在多个智能体问答基准测试中保持了接近非共享缓存基线的准确性。

Conclusion: LRAgent通过有效分解和共享多LoRA智能体系统中的KV缓存组件，在保持准确性的同时显著降低了内存和计算开销，为多智能体系统的高效部署提供了实用解决方案。

Abstract: Role specialization in multi-LLM agent systems is often realized via multi-LoRA, where agents share a pretrained backbone and differ only through lightweight adapters. Despite sharing base model weights, each agent independently builds and stores its own KV cache for the same long, tool-augmented trajectories, incurring substantial memory and compute overhead. Existing KV cache sharing methods largely overlook this multi-LoRA setting. We observe that, across agents, cache differences are dominated by adapter outputs, while activations from the shared pretrained backbone remain highly similar. Based on this observation, we propose LRAgent, a KV cache sharing framework for multi-LoRA agents that decomposes the cache into a shared base component from the pretrained weights and an adapter-dependent component from LoRA weights. LRAgent reduces memory overhead by sharing the base component and storing the adapter component in its inherent low-rank form, and further reduces compute overhead, enabled by shared-$A$ multi-LoRA architectures, by also sharing the low-rank cache and avoiding redundant computations for contexts already processed by other agents. To efficiently reconstruct adapter contributions at runtime, we introduce Flash-LoRA-Attention, a kernel that reorders attention computation to avoid materializing the low-rank cache to full dimension. LRAgent achieves throughput and time-to-first-token latency close to fully shared caching, while preserving accuracy near the non-shared caching baseline across agentic question-answering benchmarks.

</details>


### [654] [Good SFT Optimizes for SFT, Better SFT Prepares for Reinforcement Learning](https://arxiv.org/abs/2602.01058)
*Dylan Zhang,Yufeng Xu,Haojin Wang,Qingzhi Chen,Hao Peng*

Main category: cs.LG

TL;DR: PEAR是一种SFT阶段方法，通过重要性采样重新加权SFT损失，解决SFT与RL分布不匹配问题，提升后续RL训练效果


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理训练流程中，离线SFT阶段通常孤立优化，最大化SFT性能，但更强的SFT检查点经过相同RL训练后可能显著弱于较弱的检查点。这是因为SFT数据生成分布与在线RL优化的策略分布存在不匹配

Method: 提出PEAR方法，使用重要性采样重新加权SFT损失，包含token级、block级和序列级三种变体。该方法可增强标准SFT目标，在收集离线数据概率后仅增加少量训练开销

Result: 在可验证推理游戏和数学推理任务上对Qwen 2.5/3和DeepSeek蒸馏模型进行实验，PEAR持续提升后RL性能，在AIME2025上获得高达14.6%的pass@8增益

Conclusion: PEAR是向更整体化LLM后训练的有效一步，通过设计和评估SFT时考虑下游RL而非孤立优化，解决SFT-RL管道中的分布不匹配问题

Abstract: Post-training of reasoning LLMs is a holistic process that typically consists of an offline SFT stage followed by an online reinforcement learning (RL) stage. However, SFT is often optimized in isolation to maximize SFT performance alone.
  We show that, after identical RL training, models initialized from stronger SFT checkpoints can significantly underperform those initialized from weaker ones. We attribute this to a mismatch typical in current SFT-RL pipelines: the distribution that generates the offline SFT data can differ substantially from the policy optimized during online RL, which learns from its own rollouts.
  We propose PEAR (Policy Evaluation-inspired Algorithm for Offline Learning Loss Re-weighting), an SFT-stage method that corrects this mismatch and better prepares the model for RL. PEAR uses importance sampling to reweight the SFT loss, with three variants operating at the token, block, and sequence levels. It can be used to augment standard SFT objectives and incurs little additional training overhead once probabilities for the offline data are collected.
  We conduct controlled experiments on verifiable reasoning games and mathematical reasoning tasks on Qwen 2.5 and 3 and DeepSeek-distilled models. PEAR consistently improves post-RL performance over canonical SFT, with pass at 8 gains up to a 14.6 percent on AIME2025. Our results suggest that PEAR is an effective step toward more holistic LLM post-training by designing and evaluating SFT with downstream RL in mind rather than in isolation.

</details>


### [655] [On the Expressive Power of Permutation-Equivariant Weight-Space Networks](https://arxiv.org/abs/2602.01083)
*Adir Dayan,Yam Eitan,Haggai Maron*

Main category: cs.LG

TL;DR: 该论文系统分析了权重空间网络的表达能力，证明了主流置换等变网络具有相同的表达能力，并在温和假设下建立了权重空间和函数空间的普适性理论。


<details>
  <summary>Details</summary>
Motivation: 随着预训练模型的普及，权重空间网络在各种任务中表现出有效性。现有方法依赖置换等变设计来提升泛化能力，但这可能影响表达能力，需要理论分析。权重空间学习同时涉及权重空间和函数空间的映射，使得表达能力分析特别复杂，现有研究缺乏全面的理论框架。

Method: 开发了权重空间网络表达能力的系统理论，首先证明所有主流置换等变网络具有相同的表达能力，然后在输入权重的温和自然假设下，建立了权重空间和函数空间的普适性理论，并刻画了普适性不再成立的边界情况。

Result: 证明了所有主要置换等变网络在表达能力上是等价的；在温和假设下建立了权重空间和函数空间的普适性；确定了普适性失效的边界情况；为权重空间网络的表达能力提供了统一的理论基础。

Conclusion: 该研究填补了权重空间网络表达能力理论分析的空白，为权重空间学习提供了坚实的理论基础，证明了置换等变设计的表达能力等价性，并建立了普适性理论框架，有助于指导未来权重空间网络的设计和应用。

Abstract: Weight-space learning studies neural architectures that operate directly on the parameters of other neural networks. Motivated by the growing availability of pretrained models, recent work has demonstrated the effectiveness of weight-space networks across a wide range of tasks. SOTA weight-space networks rely on permutation-equivariant designs to improve generalization. However, this may negatively affect expressive power, warranting theoretical investigation. Importantly, unlike other structured domains, weight-space learning targets maps operating on both weight and function spaces, making expressivity analysis particularly subtle. While a few prior works provide partial expressivity results, a comprehensive characterization is still missing. In this work, we address this gap by developing a systematic theory for expressivity of weight-space networks. We first prove that all prominent permutation-equivariant networks are equivalent in expressive power. We then establish universality in both weight- and function-space settings under mild, natural assumptions on the input weights, and characterize the edge-case regimes where universality no longer holds. Together, these results provide a strong and unified foundation for the expressivity of weight-space networks.

</details>


### [656] [OLion: Approaching the Hadamard Ideal by Intersecting Spectral and $\ell_{\infty}$ Implicit Biases](https://arxiv.org/abs/2602.01105)
*Zixiao Wang,Yifei Shen,Huishuai Zhang*

Main category: cs.LG

TL;DR: 提出一种名为A的新优化器，结合谱控制和ℓ∞坐标控制，通过正交化动量方向和符号更新，在语言和视觉任务中匹配或优于AdamW和Muon，同时减少内存占用。


<details>
  <summary>Details</summary>
Motivation: 许多优化器可以解释为范数诱导几何下的最速下降方法，从而继承相应的隐式偏差。本文旨在结合谱控制（来自正交化更新方向）和ℓ∞风格坐标控制（来自符号更新），开发更高效的优化器。

Method: 提出A优化器：1）形成Lion风格的动量方向；2）通过少量Newton-Schulz迭代近似正交化；3）应用逐元素符号操作。这提供了对谱约束和ℓ∞约束集交集（缩放Hadamard-like集合）上最大步长的有效近似。

Result: 在大型语言和视觉训练中（包括GPT-2和Llama预训练、SiT图像预训练、监督微调），A优化器在可比较调参下匹配或优于AdamW和Muon，同时仅使用动量级优化器状态，并能缓解AdamW预训练检查点微调时的优化器不匹配问题。

Conclusion: A优化器成功结合了谱控制和ℓ∞坐标控制，尽管正交化和符号操作具有强非线性，但在经验验证的对角各向同性假设下证明了收敛性，为大规模深度学习训练提供了高效且性能优越的优化器选择。

Abstract: Many optimizers can be interpreted as steepest-descent methods under norm-induced geometries, and thus inherit corresponding implicit biases. We introduce \nameA{} (\fullname{}), which combines spectral control from orthogonalized update directions with $\ell_\infty$-style coordinate control from sign updates. \nameA{} forms a Lion-style momentum direction, approximately orthogonalizes it via a few Newton--Schulz iterations, and then applies an entrywise sign, providing an efficient approximation to taking a maximal step over the intersection of the spectral and $\ell_\infty$ constraint sets (a scaled Hadamard-like set for matrix parameters). Despite the strong nonlinearity of orthogonalization and sign, we prove convergence under a mild, empirically verified diagonal-isotropy assumption. Across large-scale language and vision training, including GPT-2 and Llama pretraining, SiT image pretraining, and supervised fine-tuning, \nameA{} matches or outperforms AdamW and Muon under comparable tuning while using only momentum-level optimizer state, and it mitigates optimizer mismatch when fine-tuning AdamW-pretrained checkpoints.

</details>


### [657] [Single-Edge Node Injection Threats to GNN-Based Security Monitoring in Industrial Graph Systems](https://arxiv.org/abs/2602.01113)
*Wenjie Liang,Ranhui Yan,Jia Cai,You-Gan Wang*

Main category: cs.LG

TL;DR: 论文提出SEGIA攻击方法，通过单边连接注入伪造节点来欺骗工业GNN系统，攻击成功率比基线高25%以上


<details>
  <summary>Details</summary>
Motivation: 工业GNN监控系统面临安全威胁，攻击者可能通过注入少量伪造节点（如恶意传感器、虚拟端点）来影响下游决策，同时逃避基于拓扑和同质性的检测

Method: 提出SEGIA攻击方法，每个注入节点仅通过单边连接到操作图，集成剪枝SGC代理、多跳邻域采样、反向图卷积特征合成和相似性正则化目标

Result: 理论分析和广泛评估显示，在显著更小的边预算下，攻击成功率比代表性基线至少高25%

Conclusion: 工业GNN部署存在系统级风险，需要轻量级准入验证和邻域一致性监控

Abstract: Graph neural networks (GNNs) are increasingly adopted in industrial graph-based monitoring systems (e.g., Industrial internet of things (IIoT) device graphs, power-grid topology models, and manufacturing communication networks) to support anomaly detection, state estimation, and asset classification. In such settings, an adversary that compromises a small number of edge devices may inject counterfeit nodes (e.g., rogue sensors, virtualized endpoints, or spoofed substations) to bias downstream decisions while evading topology- and homophily-based sanitization. This paper formulates deployment-oriented node-injection attacks under constrained resources and proposes the \emph{Single-Edge Graph Injection Attack} (SEGIA), in which each injected node attaches to the operational graph through a single edge. SEGIA integrates a pruned SGC surrogate, multi-hop neighborhood sampling, and reverse graph convolution-based feature synthesis with a similarity-regularized objective to preserve local homophily and survive edge pruning. Theoretical analysis and extensive evaluations across datasets and defenses show at least $25\%$ higher attack success than representative baselines under substantially smaller edge budgets. These results indicate a system-level risk in industrial GNN deployments and motivate lightweight admission validation and neighborhood-consistency monitoring.

</details>


### [658] [MarkovScale: Towards Optimal Sequential Scaling at Inference Time](https://arxiv.org/abs/2602.01120)
*Youkang Wang,Jian Wang,Rubing Chen,Tianyi Zeng,Xiao-Yong Wei,Qing Li*

Main category: cs.LG

TL;DR: 提出MarkovScale框架，将顺序缩放建模为马尔可夫过程，获得闭式解和理论边界，在多个LLM和基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 顺序缩放作为重要的推理时缩放范式，现有方法多为启发式且缺乏理论依据，性能提升有限且缺乏对最优性边界的清晰理解。

Method: 将顺序缩放建模为两状态马尔可夫过程，推导出闭式解，确定精度提升条件和理论性能边界，并基于此开发MarkovScale系统。

Result: 在3个骨干LLM、5个基准测试和20多个配置上的实验表明，MarkovScale在准确性和效率上均优于最先进的并行和顺序缩放方法。

Conclusion: MarkovScale为顺序缩放提供了理论依据和最优性边界，是实现LLM最优且资源高效推理的重要进展。

Abstract: Sequential scaling is a prominent inference-time scaling paradigm, yet its performance improvements are typically modest and not well understood, largely due to the prevalence of heuristic, non-principled approaches that obscure clear optimality bounds. To address this, we propose a principled framework that models sequential scaling as a two-state Markov process. This approach reveals the underlying properties of sequential scaling and yields closed-form solutions for essential aspects, such as the specific conditions under which accuracy is improved and the theoretical upper, neutral, and lower performance bounds. Leveraging this formulation, we develop MarkovScale, a practical system that applies these optimality criteria to achieve a theoretically grounded balance between accuracy and efficiency. Comprehensive experiments across 3 backbone LLMs, 5 benchmarks, and over 20 configurations show that MarkovScale consistently outperforms state-of-the-art parallel and sequential scaling methods, representing a significant step toward optimal and resource-efficient inference in LLMs. The source code will be open upon acceptance at https://open-upon-acceptance.

</details>


### [659] [ChronoSpike: An Adaptive Spiking Graph Neural Network for Dynamic Graphs](https://arxiv.org/abs/2602.01124)
*Md Abrar Jahin,Taufikur Rahman Fuad,Jay Pujara,Craig Knoblock*

Main category: cs.LG

TL;DR: ChronoSpike是一种自适应脉冲图神经网络，通过可学习的LIF神经元、多头注意力空间聚合和轻量级Transformer时间编码器，在保持线性内存复杂度的同时，实现了动态图表示学习的性能提升和训练加速。


<details>
  <summary>Details</summary>
Motivation: 现有动态图表示学习方法面临基本权衡：基于注意力的方法表达能力强但复杂度高（O(T²)），循环架构存在梯度问题和密集状态存储问题。脉冲神经网络虽然具有事件驱动效率，但受限于顺序传播、二进制信息损失和局部聚合缺失全局上下文。

Method: 提出ChronoSpike自适应脉冲图神经网络：1）集成可学习的LIF神经元，具有每通道膜电位动态特性；2）在连续特征上进行多头注意力空间聚合；3）轻量级Transformer时间编码器。该方法支持细粒度局部建模和长距离依赖捕获，具有线性内存复杂度O(T·d)。

Result: 在三个大规模基准测试中，ChronoSpike在12个最先进基线方法上提升了2.0% Macro-F1和2.4% Micro-F1，同时训练速度比循环方法快3-10倍，参数预算恒定105K且与图大小无关。理论保证包括膜电位有界性、收缩因子ρ<1下的梯度流稳定性和BIBO稳定性。可解释性分析显示异质时间感受野和学习的首因效应，稀疏度达到83-88%。

Conclusion: ChronoSpike通过结合脉冲神经网络的效率和Transformer的全局建模能力，解决了动态图表示学习中的表达力-效率权衡问题，实现了高性能、高效率且理论保证的解决方案。

Abstract: Dynamic graph representation learning requires capturing both structural relationships and temporal evolution, yet existing approaches face a fundamental trade-off: attention-based methods achieve expressiveness at $O(T^2)$ complexity, while recurrent architectures suffer from gradient pathologies and dense state storage. Spiking neural networks offer event-driven efficiency but remain limited by sequential propagation, binary information loss, and local aggregation that misses global context. We propose ChronoSpike, an adaptive spiking graph neural network that integrates learnable LIF neurons with per-channel membrane dynamics, multi-head attentive spatial aggregation on continuous features, and a lightweight Transformer temporal encoder, enabling both fine-grained local modeling and long-range dependency capture with linear memory complexity $O(T \cdot d)$. On three large-scale benchmarks, ChronoSpike outperforms twelve state-of-the-art baselines by $2.0\%$ Macro-F1 and $2.4\%$ Micro-F1 while achieving $3-10\times$ faster training than recurrent methods with a constant 105K-parameter budget independent of graph size. We provide theoretical guarantees for membrane potential boundedness, gradient flow stability under contraction factor $ρ< 1$, and BIBO stability; interpretability analyses reveal heterogeneous temporal receptive fields and a learned primacy effect with $83-88\%$ sparsity.

</details>


### [660] [WinFLoRA: Incentivizing Client-Adaptive Aggregation in Federated LoRA under Privacy Heterogeneity](https://arxiv.org/abs/2602.01126)
*Mengsha Kou,Xiaoyu Xia,Ziqi Wang,Ibrahim Khalil,Runkun Luo,Jingwen Zhou,Minhui Xue*

Main category: cs.LG

TL;DR: WinFLoRA：一种针对隐私异构联邦LoRA的激励机制框架，通过噪声感知的聚合权重奖励低噪声贡献，在保护隐私的同时提升全局模型性能。


<details>
  <summary>Details</summary>
Motivation: 在隐私敏感的联邦学习场景中，客户端注入不同级别的差分隐私噪声，导致隐私异构性，这会扭曲个体激励与全局性能的对齐。现有方法难以平衡异构隐私需求与模型性能。

Method: 提出WinFLoRA框架，基于上传的LoRA适配器估计客户端噪声水平，将聚合权重作为激励机制。为低噪声贡献分配更大权重，使其对全局模型有更大影响，从而奖励高质量更新。

Result: 在多个LLM和数据集上的评估显示，WinFLoRA相比现有基准方法，全局准确率提升高达52.58%，客户端效用提升高达2.56倍。

Conclusion: WinFLoRA成功地将异构客户端在隐私和下游性能方面的效用与全局模型目标对齐，无需第三方介入，在保护隐私的同时显著提升了联邦LoRA的性能。

Abstract: Large Language Models (LLMs) increasingly underpin intelligent web applications, from chatbots to search and recommendation, where efficient specialization is essential. Low-Rank Adaptation (LoRA) enables such adaptation with minimal overhead, while federated LoRA allows web service providers to fine-tune shared models without data sharing. However, in privacy-sensitive deployments, clients inject varying levels of differential privacy (DP) noise, creating privacy heterogeneity that misaligns individual incentives and global performance. In this paper, we propose WinFLoRA, a privacy-heterogeneous federated LoRA that utilizes aggregation weights as incentives with noise awareness. Specifically, the noises from clients are estimated based on the uploaded LoRA adapters. A larger weight indicates greater influence on the global model and better downstream task performance, rewarding lower-noise contributions. By up-weighting low-noise updates, WinFLoRA improves global accuracy while accommodating clients' heterogeneous privacy requirements. Consequently, WinFLoRA aligns heterogeneous client utility in terms of privacy and downstream performance with global model objectives without third-party involvement. Extensive evaluations demonstrate that across multiple LLMs and datasets, WinFLoRA achieves up to 52.58% higher global accuracy and up to 2.56x client utility than state-of-the-art benchmarks. Source code is publicly available at https://github.com/koums24/WinFLoRA.git.

</details>


### [661] [Tangent Space Fine-Tuning for Directional Preference Alignment in Large Language Models](https://arxiv.org/abs/2602.01128)
*Mete Erdogan*

Main category: cs.LG

TL;DR: TS-DPO：在切线空间中执行DPO，学习每个目标的更新方向，可在推理时线性组合以实现用户指定的行为平衡，无需额外优化。


<details>
  <summary>Details</summary>
Motivation: 现有偏好优化方法（如DPO）将反馈压缩为单一标量奖励，固定了目标间的平衡，无法遍历帕累托前沿。需要让LLM能够平衡多个人类偏好维度（如帮助性、安全性、冗长度），实现可控对齐。

Method: 基于切线空间微调理论，将DPO扩展到局部线性机制中，提出切线空间直接偏好优化（TS-DPO）。在切线空间中学习每个目标的更新方向，这些方向可在推理时线性组合以生成用户指定行为。

Result: 在HelpSteer和UltraFeedback数据集上评估帮助性-冗长度权衡，TS-DPO比标量化DPO实现了更广泛的帕累托最优覆盖和更平滑的偏好控制。典型相关分析（CCA）显示切线空间训练放大了与不同偏好对齐的典型方向，改善了分离性。

Conclusion: TS-DPO通过在切线空间中学习可组合的更新方向，实现了对多个偏好维度的可控对齐，无需额外优化即可在推理时调整行为平衡，为LLM偏好对齐提供了更灵活的方法。

Abstract: Our goal is to enable large language models (LLMs) to balance multiple human preference dimensions; such as helpfulness, safety, and verbosity, through principled and controllable alignment. Existing preference optimization methods, including Direct Preference Optimization (DPO), collapse feedback into a single scalar reward, fixing one balance among objectives and preventing traversal of the Pareto front. Recent work by Ortiz-Jimenez et al. (2023) showed that fine-tuning can be viewed in a model's tangent space, where linearized updates act as additive vectors that can be composed to jointly perform well on multiple tasks. Building on this formulation, we extend this idea to preference alignment and propose Tangent-Space Direct Preference Optimization (TS-DPO), which performs DPO within this locally linear regime to learn per-objective update directions. These directions can be linearly combined at inference to generate user-specified behaviors without additional optimization. Evaluated on the helpfulness-verbosity trade-off using the HelpSteer and UltraFeedback datasets, TS-DPO achieves broader Pareto-optimal coverage and smoother preference control than scalarized DPO. Canonical Correlation Analysis (CCA) further shows that tangent-space training amplifies canonical directions aligned with distinct preferences, improving disentanglement.

</details>


### [662] [TRACE: Scalable Amortized Causal Discovery from Single Sequences via Autoregressive Density Estimation](https://arxiv.org/abs/2602.01135)
*Hugo Math,Rainer Lienhart*

Main category: cs.LG

TL;DR: TRACE：利用自回归模型作为预训练密度估计器，从单个离散事件序列中推断事件类型间的因果图，支持延迟因果效应，可扩展至数万事件类型。


<details>
  <summary>Details</summary>
Motivation: 从单个离散事件序列（如车辆日志、制造系统、患者轨迹）中发现因果关系具有挑战性，因为缺乏重复样本、高维度和长程时间依赖。

Method: TRACE框架将自回归模型重新用作预训练密度估计器，用于条件互信息估计，推断事件类型间的摘要因果图，支持延迟因果效应，在GPU上完全并行。

Result: 实验表明TRACE在不同基线和不同词汇量下表现稳健，包括在超过29,100个事件类型的车辆诊断根因分析中的应用。

Conclusion: TRACE为从单个事件序列中进行因果发现提供了可扩展的解决方案，即使在自回归模型不完美的情况下也具有理论可识别性。

Abstract: We study causal discovery from a single observed sequence of discrete events generated by a stochastic process, as encountered in vehicle logs, manufacturing systems, or patient trajectories. This regime is particularly challenging due to the absence of repeated samples, high dimensionality, and long-range temporal dependencies of the single observation during inference. We introduce TRACE, a scalable framework that repurposes autoregressive models as pretrained density estimators for conditional mutual information estimation. TRACE infers the summary causal graph between event types in a sequence, scaling linearly with the event vocabulary and supporting delayed causal effects, while being fully parallel on GPUs. We establish its theoretical identifiability under imperfect autoregressive models. Experiments demonstrate robust performance across different baselines and varying vocabulary sizes including an application to root-cause analysis in vehicle diagnostics with over 29,100 event types.

</details>


### [663] [Self-Generative Adversarial Fine-Tuning for Large Language Models](https://arxiv.org/abs/2602.01137)
*Shiguang Wu,Yaqing Wang,Quanming Yao*

Main category: cs.LG

TL;DR: SGALM提出了一种基于生成对抗游戏的自对齐框架，通过单个LLM内部生成器和判别器的对抗训练实现对齐，无需外部奖励模型或大量人工标注。


<details>
  <summary>Details</summary>
Motivation: 现有LLM对齐方法（监督微调、人类反馈强化学习）依赖昂贵且稀缺的高质量标注，而自博弈和合成数据方法又依赖启发式假设或无根据的自我评估，容易导致偏差累积和性能漂移。

Method: 提出Self-Generative Adversarial LLM (SGALM)框架，将对齐问题形式化为单个LLM内部的生成对抗游戏，联合演化生成和判别能力，无需外部奖励模型。

Result: 理论和实证结果表明SGALM达到了最先进的性能，既可作为有效的对齐算法，也可作为鲁棒的合成数据生成引擎。

Conclusion: SGALM为LLM对齐提供了一种统一的微调框架，通过内部生成对抗机制减少对外部标注的依赖，同时避免自评估偏差，实现了高效的对齐和合成数据生成。

Abstract: Fine-tuning large language models (LLMs) for alignment typically relies on supervised fine-tuning or reinforcement learning from human feedback, both limited by the cost and scarcity of high-quality annotations. Recent self-play and synthetic data approaches reduce this dependence but often rely on heuristic assumptions or ungrounded self-evaluation, which can cause bias accumulation and performance drift. In this paper, we propose Self-Generative Adversarial LLM (SGALM), a unified fine-tuning framework that formulates alignment as a generative adversarial game within a single LLM. SGALM jointly evolves generation and discrimination capabilities without external reward models. Theoretical and empirical results demonstrate that SGALM achieves state-of-the-art performance, serves as an effective alignment algorithm and a robust synthetic data engine.

</details>


### [664] [Key Principles of Graph Machine Learning: Representation, Robustness, and Generalization](https://arxiv.org/abs/2602.01139)
*Yassine Abbahaddou*

Main category: cs.LG

TL;DR: 该论文针对图神经网络在泛化性、对抗鲁棒性和表示学习能力方面的挑战，提出了基于图移位算子的表示学习、图数据增强的泛化提升方法，以及正交化和噪声防御的鲁棒性增强技术。


<details>
  <summary>Details</summary>
Motivation: 尽管图神经网络在结构化数据表示学习方面表现出色，但在泛化能力、对抗扰动鲁棒性和表示学习效果方面仍存在限制，需要系统性的解决方案来提升其性能。

Method: 通过三个主要贡献：1）基于图移位算子的新表示学习技术；2）通过图数据增强提升泛化能力的方法；3）利用正交化技术和噪声防御机制增强对抗鲁棒性。

Result: 论文工作为图神经网络提供了更原则性的理解，解决了其核心限制并提升了在不同应用场景下的性能表现。

Conclusion: 通过系统性地解决图神经网络在表示学习、泛化性和鲁棒性方面的挑战，该研究为图神经网络的发展提供了理论基础和实践方法，拓展了其应用潜力。

Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for learning representations from structured data. Despite their growing popularity and success across various applications, GNNs encounter several challenges that limit their performance. in their generalization, robustness to adversarial perturbations, and the effectiveness of their representation learning capabilities. In this dissertation, I investigate these core aspects through three main contributions: (1) developing new representation learning techniques based on Graph Shift Operators (GSOs, aiming for enhanced performance across various contexts and applications, (2) introducing generalization-enhancing methods through graph data augmentation, and (3) developing more robust GNNs by leveraging orthonormalization techniques and noise-based defenses against adversarial attacks. By addressing these challenges, my work provides a more principled understanding of the limitations and potential of GNNs.

</details>


### [665] [Generalized Radius and Integrated Codebook Transforms for Differentiable Vector Quantization](https://arxiv.org/abs/2602.01140)
*Haochen You,Heng Zhang,Hongyang He,Yuqi Li,Baojing Liu*

Main category: cs.LG

TL;DR: GRIT-VQ是一种全新的向量量化框架，通过半径更新和集成变换解决了传统VQ的梯度不稳定和码本利用不足问题，实现了完全可微的量化过程。


<details>
  <summary>Details</summary>
Motivation: 传统向量量化（VQ）使用硬最近邻分配和直通估计器，导致梯度不稳定、更新步长与量化间隙耦合、码本训练孤立，在大规模应用中造成码本利用严重不足。

Method: 提出GRIT-VQ框架：1）用基于半径的更新替代直通估计器，使潜在向量沿量化方向以可控的几何感知步长移动；2）对码本应用数据无关的集成变换，通过共享参数更新所有码字而非独立更新。

Result: 在图像重建、图像生成和推荐系统标记化基准测试中，GRIT-VQ持续改善重建误差、生成质量和推荐准确性，同时显著提高了码本利用率。

Conclusion: GRIT-VQ为向量量化提供了一个统一的可微替代框架，解决了传统方法的根本优化问题，实现了稳定的梯度流、协调的码本演化和可靠的防崩溃特性。

Abstract: Vector quantization (VQ) underpins modern generative and representation models by turning continuous latents into discrete tokens. Yet hard nearest-neighbor assignments are non-differentiable and are typically optimized with heuristic straight-through estimators, which couple the update step size to the quantization gap and train each code in isolation, leading to unstable gradients and severe codebook under-utilization at scale. In this paper, we introduce GRIT-VQ (Generalized Radius and Integrated Transform-Vector Quantization), a unified surrogate framework that keeps hard assignments in the forward pass while making VQ fully differentiable. GRIT-VQ replaces the straight-through estimator with a radius-based update that moves latents along the quantization direction with a controllable, geometry-aware step, and applies a data-agnostic integrated transform to the codebook so that all codes are updated through shared parameters instead of independently. Our theoretical analysis clarifies the fundamental optimization dynamics introduced by GRIT-VQ, establishing conditions for stable gradient flow, coordinated codebook evolution, and reliable avoidance of collapse across a broad family of quantizers. Across image reconstruction, image generation, and recommendation tokenization benchmarks, GRIT-VQ consistently improves reconstruction error, generative quality, and recommendation accuracy while substantially increasing codebook utilization compared to existing VQ variants.

</details>


### [666] [PolicyFlow: Policy Optimization with Continuous Normalizing Flow in Reinforcement Learning](https://arxiv.org/abs/2602.01156)
*Shunpeng Yang,Ben Liu,Hua Chen*

Main category: cs.LG

TL;DR: PolicyFlow：一种基于连续归一化流（CNF）的强化学习算法，通过近似重要性比率和布朗正则化器，在保持PPO稳定性的同时实现更丰富的多模态动作分布。


<details>
  <summary>Details</summary>
Motivation: 标准PPO依赖于重要性比率，需要评估策略似然，这在高斯策略中很简单，但对于表达能力更强的连续归一化流（CNF）策略来说，沿整个流轨迹评估似然计算成本高且数值不稳定。需要一种方法将表达能力强的CNF策略与PPO式目标结合，而不需要沿完整流路径评估似然。

Method: 提出PolicyFlow算法：1）通过沿简单插值路径的速度场变化近似重要性比率，避免沿完整流路径评估似然；2）引入布朗正则化器，这是一种受布朗运动启发的隐式策略熵正则化器，防止模式崩溃并鼓励多样行为。

Result: 在MultiGoal、PointMaze、IsaacLab和MuJoCo Playground等多种环境的实验中，PolicyFlow相比使用高斯策略的PPO以及基于流的基线方法（FPO和DPPO）取得了竞争性或更优的性能。在MultiGoal任务中特别展示了捕获更丰富多模态动作分布的能力。

Conclusion: PolicyFlow成功地将表达能力强的CNF策略与PPO式目标结合，通过近似重要性比率和布朗正则化器，在保持训练稳定性的同时实现了更丰富的动作分布表示，为强化学习中的复杂策略建模提供了有效解决方案。

Abstract: Among on-policy reinforcement learning algorithms, Proximal Policy Optimization (PPO) demonstrates is widely favored for its simplicity, numerical stability, and strong empirical performance. Standard PPO relies on surrogate objectives defined via importance ratios, which require evaluating policy likelihood that is typically straightforward when the policy is modeled as a Gaussian distribution. However, extending PPO to more expressive, high-capacity policy models such as continuous normalizing flows (CNFs), also known as flow-matching models, is challenging because likelihood evaluation along the full flow trajectory is computationally expensive and often numerically unstable. To resolve this issue, we propose PolicyFlow, a novel on-policy CNF-based reinforcement learning algorithm that integrates expressive CNF policies with PPO-style objectives without requiring likelihood evaluation along the full flow path. PolicyFlow approximates importance ratios using velocity field variations along a simple interpolation path, reducing computational overhead without compromising training stability. To further prevent mode collapse and further encourage diverse behaviors, we propose the Brownian Regularizer, an implicit policy entropy regularizer inspired by Brownian motion, which is conceptually elegant and computationally lightweight. Experiments on diverse tasks across various environments including MultiGoal, PointMaze, IsaacLab and MuJoCo Playground show that PolicyFlow achieves competitive or superior performance compared to PPO using Gaussian policies and flow-based baselines including FPO and DPPO. Notably, results on MultiGoal highlight PolicyFlow's ability to capture richer multimodal action distributions.

</details>


### [667] [Multi-Horizon Electricity Price Forecasting with Deep Learning in the Australian National Electricity Market](https://arxiv.org/abs/2602.01157)
*Mohammed Osman Gani,Zhipeng He,Chun Ouyang,Sara Khalifa*

Main category: cs.LG

TL;DR: 提出一个基于深度学习的多日前电价预测框架，在澳大利亚电力市场进行综合评估，发现标准DL模型在多数地区表现更好，而SOTA时间序列DL模型对预测时域扩展更具鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前电价预测存在三个主要问题：1) 多日前预测研究有限；2) 对SOTA时间序列DL模型探索不足；3) 主要依赖聚合水平评估，掩盖了日内预测变化。需要解决这些研究空白。

Method: 提出一个新颖的电价预测框架，将预测时域扩展到多日前，系统构建基于基准SOTA时间序列DL模型的预测模型。在澳大利亚国家电力市场五个区域进行综合评估，分析日内时段级别的预测性能。

Result: 结果显示：1) 没有单一模型在所有区域、指标和时域上始终占优；2) 标准DL模型在多数地区表现更优；3) SOTA时间序列DL模型对预测时域扩展更具鲁棒性；4) 日内评估显示明显的昼夜误差模式。

Conclusion: 未来基于DL的电价预测研究可从丰富的特征表示和建模策略中受益，这些策略能增强长期预测鲁棒性，同时保持对日内波动和结构性价格动态的敏感性。

Abstract: Accurate electricity price forecasting (EPF) is essential for operational planning, trading, and flexible asset scheduling in liberalised power systems, yet remains challenging due to volatility, heavy-tailed spikes, and frequent regime shifts. While deep learning (DL) has been increasingly adopted in EPF to capture complex and nonlinear price dynamics, several important gaps persist: (i) limited attention to multi-day horizons beyond day-ahead forecasting, (ii) insufficient exploration of state-of-the-art (SOTA) time series DL models, and (iii) a predominant reliance on aggregated horizon-level evaluation that obscures time-of-day forecasting variation. To address these gaps, we propose a novel EPF framework that extends the forecast horizon to multi-day-ahead by systematically building forecasting models that leverage benchmarked SOTA time series DL models. We conduct a comprehensive evaluation to analyse time-of-day forecasting performance by integrating model assessment at intraday interval levels across all five regions in the Australian National Electricity Market (NEM). The results show that no single model consistently dominates across regions, metrics, and horizons. Overall, standard DL models deliver superior performance in most regions, while SOTA time series DL models demonstrate greater robustness to forecast horizon extension. Intraday interval-level evaluation reveals pronounced diurnal error patterns, indicating that absolute errors peak during the evening ramp, relative errors inflate during midday negative-price regimes, and directional accuracy degrades during periods of frequent trend changes. These findings suggest that future research on DL-based EPF can benefit from enriched feature representations and modelling strategies that enhance longer-term forecasting robustness while maintaining sensitivity to intraday volatility and structural price dynamics.

</details>


### [668] [Multi-Fidelity Physics-Informed Neural Networks with Bayesian Uncertainty Quantification and Adaptive Residual Learning for Efficient Solution of Parametric Partial Differential Equations](https://arxiv.org/abs/2602.01176)
*Olaf Yunus Laitinen Imanov*

Main category: cs.LG

TL;DR: MF-BPINN：结合物理信息神经网络与贝叶斯不确定性量化的多保真度框架，通过分层神经网络架构和自适应残差学习，利用低保真度模拟和稀疏高保真度数据高效求解参数化偏微分方程。


<details>
  <summary>Details</summary>
Motivation: 物理信息神经网络在求解偏微分方程方面表现出色，但求解高保真度偏微分方程计算成本高昂，特别是对于需要在不同参数配置下进行多次评估的参数化系统。

Method: 提出MF-BPINN多保真度框架，结合物理信息神经网络与贝叶斯不确定性量化及自适应残差学习。采用分层神经网络架构学习不同保真度级别间的非线性相关性，引入具有可学习门控机制的自适应残差网络动态平衡线性和非线性保真度差异，并开发基于哈密顿蒙特卡洛的严格贝叶斯框架。

Result: 论文未提供具体实验结果，但方法理论上能够利用丰富的低保真度模拟和稀疏的高保真度数据，提高求解参数化偏微分方程的效率和精度。

Conclusion: MF-BPINN为求解计算成本高昂的高保真度偏微分方程提供了一种有效的多保真度框架，通过结合物理信息、贝叶斯不确定性和自适应学习机制，有望显著提升参数化系统求解的效率。

Abstract: Physics-informed neural networks (PINNs) have emerged as a powerful paradigm for solving partial differential equations (PDEs) by embedding physical laws directly into neural network training. However, solving high-fidelity PDEs remains computationally prohibitive, particularly for parametric systems requiring multiple evaluations across varying parameter configurations. This paper presents MF-BPINN, a novel multi-fidelity framework that synergistically combines physics-informed neural networks with Bayesian uncertainty quantification and adaptive residual learning. Our approach leverages abundant low-fidelity simulations alongside sparse high-fidelity data through a hierarchical neural architecture that learns nonlinear correlations across fidelity levels. We introduce an adaptive residual network with learnable gating mechanisms that dynamically balances linear and nonlinear fidelity discrepancies. Furthermore, we develop a rigorous Bayesian framework employing Hamiltonian Monte Carlo.

</details>


### [669] [Rethinking the Flow-Based Gradual Domain Adaption: A Semi-Dual Optimal Transport Perspective](https://arxiv.org/abs/2602.01179)
*Zhichao Chen,Zhan Zhuang,Yunfei Teng,Hao Wang,Fangyikang Wang,Zhengnan Li,Tianqiao Liu,Haoxuan Li,Zhouchen Lin*

Main category: cs.LG

TL;DR: 提出基于熵正则化半对偶非平衡最优传输的渐进域适应框架，通过直接构建中间域样本来解决现有方法依赖似然估计导致信息丢失的问题。


<details>
  <summary>Details</summary>
Motivation: 渐进域适应需要中间域来缓解域偏移，但真实中间域通常不可得或无效。现有基于流模型的方法通过插值源域和目标域分布来合成中间样本，但依赖样本对数似然估计会丢弃有用信息，影响GDA性能。

Method: 提出熵正则化半对偶非平衡最优传输框架：1) 将基于流的GDA重新表述为拉格朗日对偶问题，推导出避免似然估计的等效半对偶目标；2) 引入熵正则化将不稳定的min-max训练过程转换为更稳定的交替优化过程。

Result: 通过大量实验验证了E-SUOT框架的有效性，并在稳定性和泛化性方面提供了理论分析。

Conclusion: E-SUOT框架通过直接构建中间域样本，避免了传统方法中似然估计导致的信息丢失问题，为渐进域适应提供了更稳定有效的解决方案。

Abstract: Gradual domain adaptation (GDA) aims to mitigate domain shift by progressively adapting models from the source domain to the target domain via intermediate domains. However, real intermediate domains are often unavailable or ineffective, necessitating the synthesis of intermediate samples. Flow-based models have recently been used for this purpose by interpolating between source and target distributions; however, their training typically relies on sample-based log-likelihood estimation, which can discard useful information and thus degrade GDA performance. The key to addressing this limitation is constructing the intermediate domains via samples directly. To this end, we propose an Entropy-regularized Semi-dual Unbalanced Optimal Transport (E-SUOT) framework to construct intermediate domains. Specifically, we reformulate flow-based GDA as a Lagrangian dual problem and derive an equivalent semi-dual objective that circumvents the need for likelihood estimation. However, the dual problem leads to an unstable min-max training procedure. To alleviate this issue, we further introduce entropy regularization to convert it into a more stable alternative optimization procedure. Based on this, we propose a novel GDA training framework and provide theoretical analysis in terms of stability and generalization. Finally, extensive experiments are conducted to demonstrate the efficacy of the E-SUOT framework.

</details>


### [670] [SimpleGPT: Improving GPT via A Simple Normalization Strategy](https://arxiv.org/abs/2602.01212)
*Marco Chen,Xianbiao Qi,Yelin He,Jiaquan Ye,Rong Xiao*

Main category: cs.LG

TL;DR: 论文提出SimpleNorm归一化策略，通过稳定激活尺度来降低Hessian矩阵的谱范数，从而允许使用更大的学习率，在GPT模型上实现了更好的优化稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 从二阶几何角度重新审视Transformer优化，探索架构设计、激活尺度、Hessian矩阵和最大可容忍学习率之间的直接联系，解决大模型训练中的优化稳定性问题。

Method: 提出SimpleNorm归一化策略，通过构造稳定中间激活尺度；理论分析损失函数关于网络激活的Hessian矩阵，证明SimpleNorm能显著降低Hessian的谱范数。

Result: 在1B、1.4B、7B和8B参数规模的GPT模型上验证，SimpleGPT（基于SimpleNorm的网络）能容忍比标准方法大3-10倍的学习率，优化稳定性强，性能显著优于基线。7B模型训练60K步后，训练损失比LLaMA2+QKNorm低0.08（从2.290降至2.208）。

Conclusion: SimpleNorm通过稳定激活尺度和降低Hessian谱范数，有效提升Transformer优化稳定性，允许使用更大学习率，从而获得更好的模型性能。

Abstract: In this work, we revisit Transformer optimization through the lens of second-order geometry and establish a direct connection between architectural design, activation scale, the Hessian matrix, and the maximum tolerable learning rate. We introduce a simple normalization strategy, termed SimpleNorm, which stabilizes intermediate activation scales by construction. Then, by analyzing the Hessian of the loss with respect to network activations, we theoretically show that SimpleNorm significantly reduces the spectral norm of the Hessian, thereby permitting larger stable learning rates. We validate our theoretical findings through extensive experiments on large GPT models at parameter scales 1B, 1.4B, 7B and 8B. Empirically, SimpleGPT, our SimpleNorm-based network, tolerates learning rates 3$\times$-10$\times$ larger than standard convention, consistently demonstrates strong optimization stability, and achieves substantially better performance than well-established baselines. Specifically, when training 7B-scale models for 60K steps, SimpleGPT achieves a training loss that is 0.08 lower than that of LLaMA2 with QKNorm, reducing the loss from 2.290 to 2.208. Our source code will be released at https://github.com/Ocram7/SimpleGPT.

</details>


### [671] [Analyzing and Improving Diffusion Models for Time-Series Data Imputation: A Proximal Recursion Perspective](https://arxiv.org/abs/2602.01182)
*Zhichao Chen,Hao Wang,Fangyikang Wang,Licheng Pan,Zhengnan Li,Yunfei Teng,Haoxuan Li,Zhouchen Lin*

Main category: cs.LG

TL;DR: SPIRIT提出了一种基于半近端传输正则化的时间序列数据插补框架，通过引入熵诱导的Bregman散度放松Wasserstein距离的质量保持约束，解决了扩散模型在复杂场景下插补性能不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在时间序列数据插补中表现出潜力，但在复杂场景下性能不一致。主要存在两个障碍：1）非平稳时间动态会偏置推理轨迹，导致对异常值敏感的插补；2）目标不一致，因为插补需要准确的点恢复，而扩散模型本质上是训练来生成多样样本的。

Method: 通过近端算子视角分析扩散模型的时间序列插补过程，发现隐式的Wasserstein距离正则化阻碍了模型抵抗非平稳性和耗散正则化的能力。提出SPIRIT框架：引入熵诱导的Bregman散度放松Wasserstein距离的质量保持约束，构建半近端传输差异，理论上证明SPT对非平稳性的鲁棒性，移除耗散结构，以SPT作为近端算子构建完整工作流程。

Result: 广泛的实验证明了所提出的SPIRIT方法的有效性。

Conclusion: SPIRIT框架通过半近端传输正则化解决了扩散模型在时间序列插补中的关键问题，提高了在复杂场景下的插补性能。

Abstract: Diffusion models (DMs) have shown promise for Time-Series Data Imputation (TSDI); however, their performance remains inconsistent in complex scenarios. We attribute this to two primary obstacles: (1) non-stationary temporal dynamics, which can bias the inference trajectory and lead to outlier-sensitive imputations; and (2) objective inconsistency, since imputation favors accurate pointwise recovery whereas DMs are inherently trained to generate diverse samples. To better understand these issues, we analyze DM-based TSDI process through a proximal-operator perspective and uncover that an implicit Wasserstein distance regularization inherent in the process hinders the model's ability to counteract non-stationarity and dissipative regularizer, thereby amplifying diversity at the expense of fidelity. Building on this insight, we propose a novel framework called SPIRIT (Semi-Proximal Transport Regularized time-series Imputation). Specifically, we introduce entropy-induced Bregman divergence to relax the mass preserving constraint in the Wasserstein distance, formulate the semi-proximal transport (SPT) discrepancy, and theoretically prove the robustness of SPT against non-stationarity. Subsequently, we remove the dissipative structure and derive the complete SPIRIT workflow, with SPT serving as the proximal operator. Extensive experiments demonstrate the effectiveness of the proposed SPIRIT approach.

</details>


### [672] [PolySAE: Modeling Feature Interactions in Sparse Autoencoders via Polynomial Decoding](https://arxiv.org/abs/2602.01322)
*Panagiotis Koromilas,Andreas D. Demou,James Oldfield,Yannis Panagakis,Mihalis Nicolaou*

Main category: cs.LG

TL;DR: PolySAE通过引入高阶项扩展稀疏自编码器，捕捉特征交互作用，解决了传统SAE无法建模组合结构的问题，在保持可解释性的同时显著提升了特征分解质量。


<details>
  <summary>Details</summary>
Motivation: 传统稀疏自编码器(SAE)假设特征通过线性重构相加，无法区分组合结构（如"Starbucks"是"star"和"coffee"的组合还是简单共现），这迫使SAE为复合概念分配整体特征而非分解为可解释的组成部分。

Method: PolySAE扩展SAE解码器，引入高阶项建模特征交互，同时保持线性编码器以确保可解释性。通过共享投影子空间上的低秩张量分解，捕捉成对和三元特征交互，参数开销小（GPT2上仅3%）。

Result: 在四个语言模型和三种SAE变体上，PolySAE平均提升约8%的探测F1分数，同时保持相当的重构误差；产生2-10倍大的类别条件特征分布Wasserstein距离；学习到的交互权重与共现频率相关性极低（r=0.06 vs SAE特征协方差的r=0.82）。

Conclusion: 多项式项捕捉了组合结构（如形态绑定和短语组合），这些结构在很大程度上独立于表面统计特征，表明PolySAE能够更有效地分解神经表示中的组合模式。

Abstract: Sparse autoencoders (SAEs) have emerged as a promising method for interpreting neural network representations by decomposing activations into sparse combinations of dictionary atoms. However, SAEs assume that features combine additively through linear reconstruction, an assumption that cannot capture compositional structure: linear models cannot distinguish whether "Starbucks" arises from the composition of "star" and "coffee" features or merely their co-occurrence. This forces SAEs to allocate monolithic features for compound concepts rather than decomposing them into interpretable constituents. We introduce PolySAE, which extends the SAE decoder with higher-order terms to model feature interactions while preserving the linear encoder essential for interpretability. Through low-rank tensor factorization on a shared projection subspace, PolySAE captures pairwise and triple feature interactions with small parameter overhead (3% on GPT2). Across four language models and three SAE variants, PolySAE achieves an average improvement of approximately 8% in probing F1 while maintaining comparable reconstruction error, and produces 2-10$\times$ larger Wasserstein distances between class-conditional feature distributions. Critically, learned interaction weights exhibit negligible correlation with co-occurrence frequency ($r = 0.06$ vs. $r = 0.82$ for SAE feature covariance), suggesting that polynomial terms capture compositional structure, such as morphological binding and phrasal composition, largely independent of surface statistics.

</details>


### [673] [The Gaussian-Head OFL Family: One-Shot Federated Learning from Client Global Statistics](https://arxiv.org/abs/2602.01186)
*Fabio Turazza,Marco Picone,Marco Mamei*

Main category: cs.LG

TL;DR: 提出GH-OFL方法族，通过假设预训练嵌入的类条件高斯性，在单轮通信中实现联邦学习，仅传输统计量而非模型，无需公共数据集且保持数据隐私。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习存在多轮通信成本高和隐私风险问题，现有单轮联邦学习方法要么不实用，要么受限于需要公共数据集、同质客户端模型或额外数据上传等假设。

Method: 提出高斯头单轮联邦学习(GH-OFL)方法族：1)客户端仅传输每类统计量(计数、一阶/二阶矩)；2)服务器构建三种头：闭式高斯头(NB/LDA/QDA)、在Fisher子空间上训练的FisherMix线性头、通过知识蒸馏精炼高斯logits的Proto-Hyper轻量残差头。

Result: GH-OFL方法在强非独立同分布偏斜下实现了最先进的鲁棒性和准确性，同时严格保持数据无关性(无需公共数据集)。

Conclusion: GH-OFL方法通过高斯性假设和统计量传输，解决了单轮联邦学习的实用性问题，在降低通信开销和保护隐私的同时保持了高性能。

Abstract: Classical Federated Learning relies on a multi-round iterative process of model exchange and aggregation between server and clients, with high communication costs and privacy risks from repeated model transmissions. In contrast, one-shot federated learning (OFL) alleviates these limitations by reducing communication to a single round, thereby lowering overhead and enhancing practical deployability. Nevertheless, most existing one-shot approaches remain either impractical or constrained, for example, they often depend on the availability of a public dataset, assume homogeneous client models, or require uploading additional data or model information. To overcome these issues, we introduce the Gaussian-Head OFL (GH-OFL) family, a suite of one-shot federated methods that assume class-conditional Gaussianity of pretrained embeddings. Clients transmit only sufficient statistics (per-class counts and first/second-order moments) and the server builds heads via three components: (i) Closed-form Gaussian heads (NB/LDA/QDA) computed directly from the received statistics; (ii) FisherMix, a linear head with cosine margin trained on synthetic samples drawn in an estimated Fisher subspace; and (iii) Proto-Hyper, a lightweight low-rank residual head that refines Gaussian logits via knowledge distillation on those synthetic samples. In our experiments, GH-OFL methods deliver state-of-the-art robustness and accuracy under strong non-IID skew while remaining strictly data-free.

</details>


### [674] [When Domains Interact: Asymmetric and Order-Sensitive Cross-Domain Effects in Reinforcement Learning for Reasoning](https://arxiv.org/abs/2602.01365)
*Wang Yang,Shouren Wang,Chaoda Song,Chuang Ma,Xinpeng Li,Nengbo Wang,Kaixiong Zhou,Vipin Chaudhary,Xiaotian Han*

Main category: cs.LG

TL;DR: GRPO在不同领域排序策略下表现出显著不对称性、顺序敏感性和策略依赖性，需要领域感知和顺序感知的训练设计。


<details>
  <summary>Details</summary>
Motivation: GRPO已成为提升大语言模型推理能力的关键技术，但其在不同领域排序策略下的行为尚不清楚，特别是顺序训练与混合领域训练的影响缺乏系统研究。

Method: 首次系统分析训练顺序效应，涵盖数学、科学、逻辑和谜题推理任务，比较顺序训练与混合领域训练策略。

Result: 发现：(1) 单领域泛化高度不对称：其他领域训练提升数学推理约25%，但对逻辑和谜题无显著迁移；(2) 跨领域交互高度顺序依赖：数学→科学顺序获得83%/41%准确率，科学→数学顺序降至77%/25%；(3) 无单一最优策略：顺序训练利于数学(84%)，混合训练利于科学和逻辑，不良排序可导致大性能差距(70%到56%)。

Conclusion: GRPO在多领域设置下表现出显著不对称性、顺序敏感性和策略依赖性，突显了领域感知和顺序感知训练设计的必要性。

Abstract: Group Relative Policy Optimization (GRPO) has become a key technique for improving reasoning abilities in large language models, yet its behavior under different domain sequencing strategies is poorly understood. In particular, the impact of sequential (one domain at a time) versus mixed-domain (multiple domain at a time) training in GRPO has not been systematically studied. We provide the first systematic analysis of training-order effects across math, science, logic, and puzzle reasoning tasks. We found (1) single-domain generalization is highly asymmetric: training on other domains improves math reasoning by approximately 25\% accuracy, while yielding negligible transfer to logic and puzzle; (2) cross-domain interactions are highly order-dependent: training in the order math$\rightarrow$science achieves 83\% / 41\% accuracy on math / science, while reversing the order to science$\rightarrow$math degrades performance to 77\% / 25\%; (3) no single strategy is universally optimal in multi-domain training: sequential training favors math (up to 84\%), mixed training favors science and logic, and poor ordering can incur large performance gaps (from 70\% to 56\%). Overall, our findings demonstrate that GRPO under multi-domain settings exhibits pronounced asymmetry, order sensitivity, and strategy dependence, highlighting the necessity of domain-aware and order-aware training design.

</details>


### [675] [Unraveling the Hidden Dynamical Structure in Recurrent Neural Policies](https://arxiv.org/abs/2602.01196)
*Jin Li,Yue Wu,Mengsha Huang,Yuhao Sun,Hao He,Xianyuan Zhan*

Main category: cs.LG

TL;DR: 研究发现循环神经网络策略在训练后会形成稳定的循环结构，类似于动力系统中的极限环，这种结构解释了RNN策略的泛化能力和鲁棒性优势。


<details>
  <summary>Details</summary>
Motivation: 循环神经网络策略在部分可观测控制和元强化学习任务中表现出色，但其优越泛化能力和鲁棒性的内在机制尚未被充分理解。本研究旨在揭示RNN策略内部工作机制的本质。

Method: 通过分析在不同训练方法、模型架构和任务上学习的循环策略的隐藏状态域，观察其与环境交互时形成的动态结构。

Result: 研究发现循环策略在环境交互中会形成稳定的循环结构，这些结构与动力系统中的极限环相似。极限环的几何形状与策略行为存在结构化对应关系。

Conclusion: 极限环的出现稳定了策略的内部记忆和任务相关环境状态，同时抑制了环境不确定性带来的干扰；极限环的几何结构编码了行为的关系结构，促进了在非平稳环境中更容易的技能适应。

Abstract: Recurrent neural policies are widely used in partially observable control and meta-RL tasks. Their abilities to maintain internal memory and adapt quickly to unseen scenarios have offered them unparalleled performance when compared to non-recurrent counterparts. However, until today, the underlying mechanisms for their superior generalization and robustness performance remain poorly understood. In this study, by analyzing the hidden state domain of recurrent policies learned over a diverse set of training methods, model architectures, and tasks, we find that stable cyclic structures consistently emerge during interaction with the environment. Such cyclic structures share a remarkable similarity with \textit{limit cycles} in dynamical system analysis, if we consider the policy and the environment as a joint hybrid dynamical system. Moreover, we uncover that the geometry of such limit cycles also has a structured correspondence with the policies' behaviors. These findings offer new perspectives to explain many nice properties of recurrent policies: the emergence of limit cycles stabilizes both the policies' internal memory and the task-relevant environmental states, while suppressing nuisance variability arising from environmental uncertainty; the geometry of limit cycles also encodes relational structures of behaviors, facilitating easier skill adaptation when facing non-stationary environments.

</details>


### [676] [The Gradient-Causal Gap: Why Gradient Importance Fails on Complex Tasks](https://arxiv.org/abs/2602.01442)
*Donald Ye*

Main category: cs.LG

TL;DR: 梯度大小与因果重要性在简单任务中相关，但在复杂任务中关系崩溃甚至反转，导致基于梯度的剪枝不可靠


<details>
  <summary>Details</summary>
Motivation: 研究神经网络中梯度大小与组件因果重要性之间的关系，揭示基于梯度的剪枝方法的局限性

Method: 在Transformer上训练算法任务，量化梯度大小与因果重要性相关性，进行剪枝实验验证

Result: 简单任务中梯度与重要性正相关(ρ=0.73)，复杂任务中相关性崩溃(ρ=0.32)甚至反转(ρ=-0.11)；低梯度"隐藏英雄"剪枝导致OOD准确率下降32%，高梯度"梯度膨胀"剪枝结果不可预测

Conclusion: 梯度大小不能可靠反映组件重要性，基于梯度的剪枝方法无法稳定保留模型能力，存在不可预测性

Abstract: Removing ''important'' high-gradient components from a neural network can improve generalization, while removing unimportant'' low-gradient components can destroy it. We demonstrate this paradox by formalizing the \textit{Gradient-Causal Gap} in Transformers trained on algorithmic tasks. While gradient magnitude and causal importance align on simple tasks ($ρ=0.73$ for reversal), this relationship collapses as task complexity increases ($ρ=0.32$ for sorting), sometimes becoming inverted ($ρ=-0.11$). Pruning experiments reveal that gradient magnitude is not merely inaccurate but \textit{unpredictably} so. Removing low-gradient ''Hidden Heroes'' consistently devastates OOD accuracy ($-32\%$). Removing high-gradient ''Gradient Bloats'' is a coin flip: harmless in most seeds (indicating optimization noise), catastrophic in others (indicating overfitting circuits). This unpredictability means gradient-based pruning cannot reliably preserve model capabilities.

</details>


### [677] [A Relative-Budget Theory for Reinforcement Learning with Verifiable Rewards in Large Language Model Reasoning](https://arxiv.org/abs/2602.01523)
*Akifumi Wachi,Hirota Kinoshita,Shokichi Takakura,Rei Higuchi,Taiji Suzuki*

Main category: cs.LG

TL;DR: 论文提出相对预算理论，用单一量ξ=H/E[T]解释强化学习在不同任务和计算预算下的效果差异，揭示了三个学习机制：不足、平衡和充足。


<details>
  <summary>Details</summary>
Motivation: 强化学习在提升大语言模型推理能力方面效果不一，现有方法缺乏统一理论解释这种差异。作者旨在通过相对预算理论提供一个统一的解释框架。

Method: 提出相对预算理论，定义ξ=H/E[T]（生成时域/首次正确解的平均token数），分析其对样本效率的影响。通过理论分析揭示三个学习机制，并在理想分布假设下进行案例研究，提供有限样本保证。

Result: 理论预测得到实证验证，发现ξ∈[1.5, 2.0]时学习效率最高，与最佳推理性能一致。相对预算随迭代线性增长。

Conclusion: 相对预算ξ是理解强化学习样本效率的关键量，为优化RL训练提供了理论指导，帮助在不同任务和计算预算下实现最佳学习效果。

Abstract: Reinforcement learning (RL) is a dominant paradigm for improving the reasoning abilities of large language models, yet its effectiveness varies across tasks and compute budgets. We propose a \emph{relative-budget} theory explaining this variation through a single quantity called relative budget $ξ:= H/\mathbb{E}[T]$, where $H$ is the generation horizon (token budget) and $T$ denotes the number of tokens until the first correct solution under a base policy. We show that $ξ$ determines sample efficiency by controlling reward variance and the likelihood of informative trajectories. Our analysis reveals three regimes: in the \emph{deficient} regime ($ξ\to 0$), informative trajectories are rare and the sample complexity explodes; in the \emph{balanced} regime ($ξ=Θ(1)$), informative trajectories occur with non-negligible probability and RL is maximally sample-efficient; and in the \emph{ample} regime ($ξ\to \infty$), learning remains stable but marginal gains per iteration diminish. We further provide finite-sample guarantees for online RL that characterize learning progress across these regimes. Specifically, in a case study under idealized distributional assumptions, we show that the relative budget grows linearly over iterations. Our empirical results confirm these predictions in realistic settings, identifying a budget $ξ\in [1.5, 2.0]$ that maximizes learning efficiency and coincides with peak reasoning performance.

</details>


### [678] [Learning from Anonymized and Incomplete Tabular Data](https://arxiv.org/abs/2602.01217)
*Lucas Lange,Adrian Böttinger,Victor Christen,Anushka Vidanage,Peter Christen,Erhard Rahm*

Main category: cs.LG

TL;DR: 该论文提出针对用户驱动隐私保护下混合原始、泛化和缺失值的表格数据的学习方法，通过新的数据转换策略恢复数据效用。


<details>
  <summary>Details</summary>
Motivation: 用户驱动隐私保护导致数据集中混合原始值、泛化值和缺失值，传统机器学习方法将非原始值视为新类别或缺失值，丢弃了泛化语义，需要新的处理方法。

Method: 提出新的数据转换策略，考虑异构匿名化，并与标准插补方法和基于LLM的方法进行比较评估。

Result: 实验表明该方法能可靠恢复数据效用，泛化值优于纯抑制，最佳数据准备策略取决于具体场景，一致的数据表示对保持下游效用至关重要。

Conclusion: 有效学习与适当处理匿名化值密切相关，需要针对用户驱动隐私保护的数据特性设计专门的学习方法。

Abstract: User-driven privacy allows individuals to control whether and at what granularity their data is shared, leading to datasets that mix original, generalized, and missing values within the same records and attributes. While such representations are intuitive for privacy, they pose challenges for machine learning, which typically treats non-original values as new categories or as missing, thereby discarding generalization semantics. For learning from such tabular data, we propose novel data transformation strategies that account for heterogeneous anonymization and evaluate them alongside standard imputation and LLM-based approaches. We employ multiple datasets, privacy configurations, and deployment scenarios, demonstrating that our method reliably regains utility. Our results show that generalized values are preferable to pure suppression, that the best data preparation strategy depends on the scenario, and that consistent data representations are crucial for maintaining downstream utility. Overall, our findings highlight that effective learning is tied to the appropriate handling of anonymized values.

</details>


### [679] [MiTA Attention: Efficient Fast-Weight Scaling via a Mixture of Top-$k$ Activations](https://arxiv.org/abs/2602.01219)
*Qishuai Wen,Zhiyuan Huang,Xianghan Meng,Wei He,Chun-Guang Li*

Main category: cs.LG

TL;DR: 论文提出了一种名为MiTA的高效注意力机制，通过压缩和路由策略将N宽度的MLP压缩为更窄的版本，使用地标查询和top-k激活来构建可变形专家。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer注意力机制随着序列长度增加，快速权重的计算成本呈指数级增长，难以处理极长序列。现有高效注意力方法缺乏统一的理论框架。

Method: 提出压缩-路由策略：1) 使用少量地标查询将N宽度MLP压缩为更窄版本；2) 为每个地标查询收集top-k激活的键值对构建可变形专家；3) 将这种策略称为MiTA（Mixture of Top-k Activations）。

Result: 在视觉任务上的初步实验表明MiTA注意力机制具有潜力，需要进一步优化和在更具挑战性的场景中验证。

Conclusion: 将高效注意力方法统一为通过路由和/或压缩来扩展快速权重的框架，提出的MiTA机制为处理长序列提供了一种有前景的解决方案。

Abstract: The attention operator in Transformers can be viewed as a two-layer fast-weight MLP, whose weights are dynamically instantiated from input tokens and whose width equals sequence length $N$. As the context extends, the expressive capacity of such an $N$-width MLP increases, but scaling its fast weights becomes prohibitively expensive for extremely long sequences. Recently, this fast-weight scaling perspective has motivated the Mixture-of-Experts (MoE) attention, which partitions the sequence into fast-weight experts and sparsely routes the tokens to them. In this paper, we elevate this perspective to a unifying framework for a wide range of efficient attention methods by interpreting them as scaling fast weights through routing and/or compression. Then we propose a compress-and-route strategy, which compresses the $N$-width MLP into a narrower one using a small set of landmark queries and constructs deformable experts by gathering top-$k$ activated key-value pairs for each landmark query. We call this strategy a Mixture of Top-$k$ Activations (MiTA), and refer to the resulting efficient mechanism as MiTA attention. Preliminary experiments on vision tasks demonstrate the promise of our MiTA attention and motivate further investigation on its optimization and broader applications in more challenging settings.

</details>


### [680] [Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2602.01601)
*Hieu Trung Nguyen,Bao Nguyen,Wenao Ma,Yuzhi Zhao,Ruifeng She,Viet Anh Nguyen*

Main category: cs.LG

TL;DR: VIP是一种基于方差信息的预测性分配策略，通过高斯过程模型预测每个提示的成功概率，优化梯度方差最小化，从而提高强化学习的采样效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于组的策略优化方法（如GRPO）为所有训练提示分配固定数量的rollout，这种均匀分配隐含地假设所有提示具有同等信息价值，可能导致计算预算使用效率低下并阻碍训练进展。

Method: VIP使用轻量级高斯过程模型基于最近的rollout预测每个提示的成功概率，将这些概率预测转换为方差估计，然后通过凸优化问题在硬计算预算约束下确定最优的rollout分配策略。

Result: 实验结果表明，VIP在多个基准测试中持续提高采样效率，并比均匀分配或启发式分配策略获得更高的性能表现。

Conclusion: VIP通过方差感知的预测性分配策略，有效解决了强化学习中采样效率低下的问题，为计算预算的优化分配提供了有效解决方案。

Abstract: Sampling efficiency is a key bottleneck in reinforcement learning with verifiable rewards. Existing group-based policy optimization methods, such as GRPO, allocate a fixed number of rollouts for all training prompts. This uniform allocation implicitly treats all prompts as equally informative, and could lead to inefficient computational budget usage and impede training progress. We introduce \Ours, a Variance-Informed Predictive allocation strategy that allocates a given rollout budget to the prompts in the incumbent batch to minimize the expected gradient variance of the policy update. At each iteration, \Ours~uses a lightweight Gaussian process model to predict per-prompt success probabilities based on recent rollouts. These probability predictions are translated into variance estimates, which are then fed into a convex optimization problem to determine the optimal rollout allocations under a hard compute budget constraint. Empirical results show that \Ours~consistently improves sampling efficiency and achieves higher performance than uniform or heuristic allocation strategies in multiple benchmarks. Our code will be available at https://github.com/HieuNT91/VIP.

</details>


### [681] [Lotus: Efficient LLM Training by Randomized Low-Rank Gradient Projection with Adaptive Subspace Switching](https://arxiv.org/abs/2602.01233)
*Tianhao Miao,Zhongyuan Bao,Lejun Zhang*

Main category: cs.LG

TL;DR: Lotus是一种高效的训练方法，通过修改投影过程解决内存消耗、训练时间和模型性能之间的权衡，相比GaLore减少30%训练时间和40%内存消耗。


<details>
  <summary>Details</summary>
Motivation: 大规模模型训练效率通常涉及内存消耗、训练时间和模型性能之间的权衡。现有方法如GaLore虽然能实现内存高效训练，但由于需要对梯度进行SVD分解，导致额外的训练时间成本。需要解决这种权衡是算法设计的核心挑战。

Method: Lotus通过简单修改投影过程来解决权衡问题。提出一个量化单位梯度位移的准则，以实现低秩梯度子空间之间的高效转换。

Result: 实验结果表明，Lotus是最有效的方法，相比基线方法减少30%训练时间和40%梯度及优化器状态的内存消耗。在预训练和微调任务中都优于基线方法。

Conclusion: Lotus成功解决了大规模模型训练中内存消耗、训练时间和模型性能之间的权衡问题，通过改进投影过程实现了更高效的训练。

Abstract: Training efficiency in large-scale models is typically assessed through memory consumption, training time, and model performance. Current methods often exhibit trade-offs among these metrics, as optimizing one generally degrades at least one of the others. Addressing this trade-off remains a central challenge in algorithm design. While GaLore enables memory-efficient training by updating gradients in a low-rank subspace, it incurs a comparable extra training time cost due to the Singular Value Decomposition(SVD) process on gradients. In this paper, we propose Lotus, a method that resolves this trade-off by simply modifying the projection process. We propose a criterion that quantifies the displacement of the unit gradient to enable efficient transitions between low-rank gradient subspaces. Experimental results indicate that Lotus is the most efficient method, achieving a 30% reduction in training time and a 40% decrease in memory consumption for gradient and optimizer states. Additionally, it outperforms the baseline method in both pre-training and fine-tuning tasks.

</details>


### [682] [$\textbf{AGT$^{AO}$}$: Robust and Stabilized LLM Unlearning via Adversarial Gating Training with Adaptive Orthogonality](https://arxiv.org/abs/2602.01703)
*Pengyu Li,Lingling Zhang,Zhitao Gao,Yanrui Wu,Yuxuan Dong,Huan Liu,Bifan Wei,Jun Liu*

Main category: cs.LG

TL;DR: 提出AGT^AO框架，通过自适应正交性和对抗门控训练解决LLM遗忘敏感数据时的权衡问题，在保持模型效用的同时实现稳健擦除。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型会无意中记忆敏感数据，带来隐私和安全风险。现有遗忘方法面临两难：激进遗忘会导致灾难性遗忘损害模型效用，保守策略则可能导致表面遗忘，模型仍易受对抗恢复攻击。

Method: 提出AGT^AO框架：1) 自适应正交性(AO)动态缓解遗忘和保留目标间的梯度冲突；2) 对抗门控训练(AGT)将遗忘建模为潜在空间最小最大博弈，使用课程式门控机制模拟和对抗内部恢复尝试。

Result: 实验显示AGT^AO在遗忘效果(KUR ≈ 0.01)和模型效用(MMLU 58.30)之间取得了优越的权衡平衡。

Conclusion: AGT^AO框架成功解决了LLM遗忘敏感数据时的权衡问题，实现了稳健擦除与效用保持的统一，为隐私保护提供了有效解决方案。

Abstract: While Large Language Models (LLMs) have achieved remarkable capabilities, they unintentionally memorize sensitive data, posing critical privacy and security risks. Machine unlearning is pivotal for mitigating these risks, yet existing paradigms face a fundamental dilemma: aggressive unlearning often induces catastrophic forgetting that degrades model utility, whereas conservative strategies risk superficial forgetting, leaving models vulnerable to adversarial recovery. To address this trade-off, we propose $\textbf{AGT$^{AO}$}$ (Adversarial Gating Training with Adaptive Orthogonality), a unified framework designed to reconcile robust erasure with utility preservation. Specifically, our approach introduces $\textbf{Adaptive Orthogonality (AO)}$ to dynamically mitigate geometric gradient conflicts between forgetting and retention objectives, thereby minimizing unintended knowledge degradation. Concurrently, $\textbf{Adversarial Gating Training (AGT)}$ formulates unlearning as a latent-space min-max game, employing a curriculum-based gating mechanism to simulate and counter internal recovery attempts. Extensive experiments demonstrate that $\textbf{AGT$^{AO}$}$ achieves a superior trade-off between unlearning efficacy (KUR $\approx$ 0.01) and model utility (MMLU 58.30). Code is available at https://github.com/TiezMind/AGT-unlearning.

</details>


### [683] [Mechanistic Interpretability of Brain-to-Speech Models Across Speech Modes](https://arxiv.org/abs/2602.01247)
*Maryam Maghsoudi,Ayushi Mishra*

Main category: cs.LG

TL;DR: 使用机制可解释性方法研究神经语音解码器内部表征，发现不同语音模态共享连续因果流形，跨模态转换由紧凑的层特定子空间介导


<details>
  <summary>Details</summary>
Motivation: 脑到语音解码模型在不同语音模态（发声、默读、想象）中表现良好，但其内部机制如何捕捉和传递跨模态信息尚不清楚，需要因果性研究

Method: 采用机制可解释性方法：跨模态激活修补、三模态插值、粗到细因果追踪、因果擦除、神经元级激活修补，分析神经语音解码器内部表征

Result: 发现语音模态位于共享的连续因果流形上，跨模态转换由紧凑的层特定子空间介导，而非扩散活动；小但非孤立的神经元子集影响跨模态转换

Conclusion: 研究为脑到语音解码模型中语音模态信息的组织和使用提供了因果解释，揭示了跨语音模态的层次化和方向依赖性表征结构

Abstract: Brain-to-speech decoding models demonstrate robust performance in vocalized, mimed, and imagined speech; yet, the fundamental mechanisms via which these models capture and transmit information across different speech modalities are less explored. In this work, we use mechanistic interpretability to causally investigate the internal representations of a neural speech decoder. We perform cross-mode activation patching of internal activations across speech modes, and use tri-modal interpolation to examine whether speech representations vary discretely or continuously. We use coarse-to-fine causal tracing and causal scrubbing to find localized causal structure, allowing us to find internal subspaces that are sufficient for cross-mode transfer. In order to determine how finely distributed these effects are within layers, we perform neuron-level activation patching. We discover that small but not distributed subsets of neurons, rather than isolated units, affect the cross-mode transfer. Our results show that speech modes lie on a shared continuous causal manifold, and cross-mode transfer is mediated by compact, layer-specific subspaces rather than diffuse activity. Together, our findings give a causal explanation for how speech modality information is organized and used in brain-to-speech decoding models, revealing hierarchical and direction-dependent representational structure across speech modes.

</details>


### [684] [Sample Efficient Active Algorithms for Offline Reinforcement Learning](https://arxiv.org/abs/2602.01260)
*Soumyadeep Roy,Shashwat Kushwaha,Ambedkar Dukkipati*

Main category: cs.LG

TL;DR: 本文提出了一种基于高斯过程不确定性建模的主动强化学习方法，通过有限在线交互选择性地优化价值函数的不确定区域，实现了比纯离线方法更优的样本复杂度


<details>
  <summary>Details</summary>
Motivation: 离线强化学习通常面临状态-动作空间覆盖不足和分布偏移问题。虽然通过有限在线交互选择性优化价值函数不确定区域的主动强化学习在实证上取得了成功，但缺乏理论分析

Method: 提出基于高斯过程不确定性建模的主动强化学习算法，利用GP集中不等式和信息增益界限进行理论分析，通过有限在线交互选择性地优化价值函数的不确定区域

Result: 理论分析表明，主动强化学习能以O(1/ε²)的主动转移学习到ε-最优策略，优于纯离线方法的Ω(1/ε²(1-γ)⁴)速率，实现了近乎最优的信息效率

Conclusion: 主动强化学习通过引导不确定性减少，能以最小在线数据加速价值函数收敛，填补了该领域理论分析的空白，并通过实验验证了算法和理论发现

Abstract: Offline reinforcement learning (RL) enables policy learning from static data but often suffers from poor coverage of the state-action space and distributional shift problems. This problem can be addressed by allowing limited online interactions to selectively refine uncertain regions of the learned value function, which is referred to as Active Reinforcement Learning (ActiveRL). While there has been good empirical success, no theoretical analysis is available in the literature. We fill this gap by developing a rigorous sample-complexity analysis of ActiveRL through the lens of Gaussian Process (GP) uncertainty modeling. In this respect, we propose an algorithm and using GP concentration inequalities and information-gain bounds, we derive high-probability guarantees showing that an $ε$-optimal policy can be learned with ${\mathcal{O}}(1/ε^2)$ active transitions, improving upon the $Ω(1/ε^2(1-γ)^4)$ rate of purely offline methods. Our results reveal that ActiveRL achieves near-optimal information efficiency, that is, guided uncertainty reduction leads to accelerated value-function convergence with minimal online data. Our analysis builds on GP concentration inequalities and information-gain bounds, bridging Bayesian nonparametric regression and reinforcement learning theories. We conduct several experiments to validate the algorithm and theoretical findings.

</details>


### [685] [BicKD: Bilateral Contrastive Knowledge Distillation](https://arxiv.org/abs/2602.01265)
*Jiangnan Zhu,Yukai Xu,Li Xiong,Yixuan Liu,Junxu Liu,Hong kyu Lee,Yujie Gu*

Main category: cs.LG

TL;DR: 提出双边对比知识蒸馏(BicKD)，通过双边对比损失增强不同类别间的正交性，同时保持同类一致性，改进了传统知识蒸馏的样本级对齐缺陷


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏(KD)只进行样本级的概率对齐，缺乏类别级的比较机制，且没有对概率空间施加结构约束，限制了知识传递效果

Method: 提出双边对比知识蒸馏(BicKD)，引入双边对比损失，增强不同类别泛化空间的正交性，同时保持同类一致性，实现样本级和类别级的显式比较

Result: 在多种模型架构和基准测试中，BicKD方法显著提升了知识传递效果，持续优于最先进的知识蒸馏技术

Conclusion: BicKD通过双边对比损失改进了传统知识蒸馏的局限性，增强了概率空间的正交性结构，为知识蒸馏提供了更有效的框架

Abstract: Knowledge distillation (KD) is a machine learning framework that transfers knowledge from a teacher model to a student model. The vanilla KD proposed by Hinton et al. has been the dominant approach in logit-based distillation and demonstrates compelling performance. However, it only performs sample-wise probability alignment between teacher and student's predictions, lacking an mechanism for class-wise comparison. Besides, vanilla KD imposes no structural constraint on the probability space. In this work, we propose a simple yet effective methodology, bilateral contrastive knowledge distillation (BicKD). This approach introduces a novel bilateral contrastive loss, which intensifies the orthogonality among different class generalization spaces while preserving consistency within the same class. The bilateral formulation enables explicit comparison of both sample-wise and class-wise prediction patterns between teacher and student. By emphasizing probabilistic orthogonality, BicKD further regularizes the geometric structure of the predictive distribution. Extensive experiments show that our BicKD method enhances knowledge transfer, and consistently outperforms state-of-the-art knowledge distillation techniques across various model architectures and benchmarks.

</details>


### [686] [Dissecting Outlier Dynamics in LLM NVFP4 Pretraining](https://arxiv.org/abs/2602.02047)
*Peijie Dong,Ruibo Fan,Yuechen Tao,Di Mou,Wenhu Hu,Zhenheng Tang,Yinghao Yu,Jiamang Wang,Wenbo Su,Guodong Yang,Liping Zhang,Xiaowen Chu,Baochun Li,Bo Li*

Main category: cs.LG

TL;DR: 该论文研究了FP4量化训练中的异常值问题，提出Hot-Channel Patch补偿机制和CHON训练方案，显著缩小了与BF16的精度差距。


<details>
  <summary>Details</summary>
Motivation: 使用4位算术训练大语言模型能提高吞吐量和内存效率，但FP4的动态范围有限，对异常值更敏感。虽然NVFP4通过分层微缩放减轻了量化误差，但与BF16相比仍存在精度差距。需要深入分析异常值在架构中的动态特性。

Method: 1. 纵向分析异常值在NVFP4预训练中的动态特性，包括定位、成因和演化过程；2. 发现异常值主要来自特定架构组件（SA的Softmax、LA的门控、FFN的SwiGLU）；3. 提出Hot-Channel Patch在线补偿机制，识别热通道并重新注入残差；4. 开发CHON训练方案，结合HCP和后QK操作保护。

Result: 在GLA-1.3B模型上训练600亿token，CHON将损失差距从0.94%降低到0.58%，同时保持下游任务准确率。异常值从训练早期的瞬态尖峰演变为后期的小规模持久热通道。

Conclusion: 通过深入分析量化训练中的异常值动态特性，提出的Hot-Channel Patch补偿机制和CHON训练方案有效缓解了FP4量化带来的精度损失，为低精度训练提供了实用解决方案。

Abstract: Training large language models using 4-bit arithmetic enhances throughput and memory efficiency. Yet, the limited dynamic range of FP4 increases sensitivity to outliers. While NVFP4 mitigates quantization error via hierarchical microscaling, a persistent loss gap remains compared to BF16. This study conducts a longitudinal analysis of outlier dynamics across architecture during NVFP4 pretraining, focusing on where they localize, why they occur, and how they evolve temporally. We find that, compared with Softmax Attention (SA), Linear Attention (LA) reduces per-tensor heavy tails but still exhibits persistent block-level spikes under block quantization. Our analysis attributes outliers to specific architectural components: Softmax in SA, gating in LA, and SwiGLU in FFN, with "post-QK" operations exhibiting higher sensitivity to quantization. Notably, outliers evolve from transient spikes early in training to a small set of persistent hot channels (i.e., channels with persistently large magnitudes) in later stages. Based on these findings, we introduce Hot-Channel Patch (HCP), an online compensation mechanism that identifies hot channels and reinjects residuals using hardware-efficient kernels. We then develop CHON, an NVFP4 training recipe integrating HCP with post-QK operation protection. On GLA-1.3B model trained for 60B tokens, CHON reduces the loss gap to BF16 from 0.94% to 0.58% while maintaining downstream accuracy.

</details>


### [687] [Diving into Kronecker Adapters: Component Design Matters](https://arxiv.org/abs/2602.01267)
*Jiayu Bai,Danchen Yu,Zhenyu Liao,TianQi Hou,Feng Zhou,Robert C. Qiu,Zenan Ling*

Main category: cs.LG

TL;DR: CDKA通过精细设计Kronecker适配器的组件结构和维度，提升参数效率与性能，并提供配置指南和训练稳定策略。


<details>
  <summary>Details</summary>
Motivation: 现有Kronecker适配器方法大多将组件结构视为固定或启发式设计选择，对组件维度和数量的探索不足。本文发现组件结构是影响Kronecker适配器容量的关键因素。

Method: 提出Component Designed Kronecker Adapters (CDKA)，对Kronecker组件的维度和数量进行细粒度分析，提供参数预算感知的配置指南和专门的训练稳定策略。

Result: 在多个自然语言处理任务上的实验证明了CDKA的有效性，展示了其与全参数微调的对齐性取决于组件配置。

Conclusion: 组件结构设计是Kronecker适配器性能的关键，CDKA通过系统化的组件配置和训练策略实现了更好的参数效率和性能。

Abstract: Kronecker adapters have emerged as a promising approach for fine-tuning large-scale models, enabling high-rank updates through tunable component structures. However, existing work largely treats the component structure as a fixed or heuristic design choice, leaving the dimensions and number of Kronecker components underexplored. In this paper, we identify component structure as a key factor governing the capacity of Kronecker adapters. We perform a fine-grained analysis of both the dimensions and number of Kronecker components. In particular, we show that the alignment between Kronecker adapters and full fine-tuning depends on component configurations. Guided by these insights, we propose Component Designed Kronecker Adapters (CDKA). We further provide parameter-budget-aware configuration guidelines and a tailored training stabilization strategy for practical deployment. Experiments across various natural language processing tasks demonstrate the effectiveness of CDKA. Code is available at https://github.com/rainstonee/CDKA.

</details>


### [688] [No Global Plan in Chain-of-Thought: Uncover the Latent Planning Horizon of LLMs](https://arxiv.org/abs/2602.02103)
*Liyan Xu,Mo Yu,Fandong Meng,Jie Zhou*

Main category: cs.LG

TL;DR: 论文提出Tele-Lens探针方法，发现大语言模型在思维链推理中表现出近视视野，主要进行增量转换而非精确全局规划，并基于此提出增强思维链不确定性估计的方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于先前关于思维链动态的互补观察：大语言模型在思维链出现前已表现出潜在规划能力，这降低了显式思维链的重要性；但思维链对于需要多步推理的任务仍然关键。为了深入理解LLM内部状态与其言语化推理轨迹之间的关系，研究者希望探索LLM的潜在规划强度。

Method: 提出Tele-Lens探针方法，应用于不同任务领域的隐藏状态，以调查LLM的潜在规划能力。基于研究发现，提出增强思维链不确定性估计的假设，并验证少量思维链位置能有效代表整个路径的不确定性。

Result: 实证结果表明：1) LLM表现出近视视野，主要进行增量转换而非精确全局规划；2) 少量思维链位置能有效代表整个路径的不确定性；3) 自动识别思维链绕过的可能性，且不会导致性能下降。

Conclusion: 研究强调了利用思维链动态的重要性，展示了自动识别思维链绕过的可行性，为理解LLM推理机制和提升不确定性估计提供了新视角。代码、数据和模型已开源。

Abstract: This work stems from prior complementary observations on the dynamics of Chain-of-Thought (CoT): Large Language Models (LLMs) is shown latent planning of subsequent reasoning prior to CoT emergence, thereby diminishing the significance of explicit CoT; whereas CoT remains critical for tasks requiring multi-step reasoning. To deepen the understanding between LLM's internal states and its verbalized reasoning trajectories, we investigate the latent planning strength of LLMs, through our probing method, Tele-Lens, applying to hidden states across diverse task domains. Our empirical results indicate that LLMs exhibit a myopic horizon, primarily conducting incremental transitions without precise global planning. Leveraging this characteristic, we propose a hypothesis on enhancing uncertainty estimation of CoT, which we validate that a small subset of CoT positions can effectively represent the uncertainty of the entire path. We further underscore the significance of exploiting CoT dynamics, and demonstrate that automatic recognition of CoT bypass can be achieved without performance degradation. Our code, data and models are released at https://github.com/lxucs/tele-lens.

</details>


### [689] [Mixture-of-World Models: Scaling Multi-Task Reinforcement Learning with Modular Latent Dynamics](https://arxiv.org/abs/2602.01270)
*Boxuan Zhang,Weipu Zhang,Zhaohan Feng,Wei Xiao,Jian Sun,Jie Chen,Gang Wang*

Main category: cs.LG

TL;DR: MoW（Mixture-of-World Models）是一种用于多任务强化学习的可扩展世界模型架构，通过模块化视觉压缩、混合Transformer动态模型和任务聚类策略，在Atari和Meta-World基准上实现了参数高效且性能优异的通用世界模型。


<details>
  <summary>Details</summary>
Motivation: 多任务强化学习在视觉领域中面临样本效率挑战，特别是当任务在观察和动态特性上存在显著异质性时。传统的单体世界模型架构难以捕捉多样化的任务动态，导致重建和预测准确性不足。

Method: 提出MoW架构：1）使用模块化变分自编码器进行任务自适应视觉压缩；2）采用混合Transformer动态模型，包含任务条件专家和共享骨干网络；3）基于梯度的任务聚类策略实现高效参数分配。

Result: 在Atari 100k基准上，单个MoW代理在26个Atari游戏中获得110.4%的平均人类标准化分数，与需要26个任务特定模型的STORM（114.2%）性能相当，但参数减少50%。在Meta-World上，30万步内达到74.5%的平均成功率，创下新纪录。

Conclusion: MoW为通用世界模型提供了可扩展且参数高效的基础架构，能够有效处理多任务强化学习中视觉领域的异质性挑战，在保持高性能的同时显著减少参数需求。

Abstract: A fundamental challenge in multi-task reinforcement learning (MTRL) is achieving sample efficiency in visual domains where tasks exhibit substantial heterogeneity in both observations and dynamics. Model-based reinforcement learning offers a promising path to improved sample efficiency through world models, but standard monolithic architectures struggle to capture diverse task dynamics, resulting in poor reconstruction and prediction accuracy. We introduce Mixture-of-World Models (MoW), a scalable architecture that combines modular variational autoencoders for task-adaptive visual compression, a hybrid Transformer-based dynamics model with task-conditioned experts and a shared backbone, and a gradient-based task clustering strategy for efficient parameter allocation. On the Atari 100k benchmark, a single MoW agent trained once on 26 Atari games achieves a mean human-normalized score of 110.4%, competitive with the score of 114.2% achieved by STORM, an ensemble of 26 task-specific models, while using 50% fewer parameters. On Meta-World, MoW achieves a 74.5% average success rate within 300 thousand environment steps, establishing a new state of the art. These results demonstrate that MoW provides a scalable and parameter-efficient foundation for generalist world models.

</details>


### [690] [Unifying Masked Diffusion Models with Various Generation Orders and Beyond](https://arxiv.org/abs/2602.02112)
*Chunsan Hong,Sanghyun Lee,Jong Chul Ye*

Main category: cs.LG

TL;DR: 提出OeMDM和LoMDM两种掩码扩散模型，前者统一多种生成顺序，后者联合学习生成顺序和扩散模型，在语言建模任务上优于现有离散扩散模型。


<details>
  <summary>Details</summary>
Motivation: 现有掩码扩散模型(MDMs)的生成质量严重依赖生成顺序，先前工作要么硬编码顺序，要么为预训练MDM学习顺序策略，这带来额外成本且可能因两阶段优化导致次优解。

Method: 提出OeMDM统一框架解释多种生成顺序，在此基础上提出LoMDM，通过单一目标联合学习生成顺序和扩散主干网络，使扩散模型能够根据上下文生成文本顺序。

Result: LoMDM在多个语言建模基准测试中优于各种离散扩散模型，证实了联合学习生成顺序和扩散模型的有效性。

Conclusion: 提出的OeMDM框架统一了多种文本生成方法，LoMDM通过联合学习实现了上下文相关的生成顺序，为语言生成提供了更灵活的扩散模型方法。

Abstract: Masked diffusion models (MDMs) are a potential alternative to autoregressive models (ARMs) for language generation, but generation quality depends critically on the generation order. Prior work either hard-codes an ordering (e.g., blockwise left-to-right) or learns an ordering policy for a pretrained MDM, which incurs extra cost and can yield suboptimal solutions due to the two-stage optimization. Motivated by this, we propose order-expressive masked diffusion model (OeMDM) for a broad class of diffusion generative processes with various generation orders, enabling the interpretation of MDM, ARM, and block diffusion in a single framework. Furthermore, building on OeMDM, we introduce learnable-order masked diffusion model (LoMDM), which jointly learns the generation ordering and diffusion backbone through a single objective from scratch, enabling the diffusion model to generate text in context-dependent ordering. Empirically, we confirm that LoMDM outperforms various discrete diffusion models across multiple language modeling benchmarks.

</details>


### [691] [From Intents to Actions: Agentic AI in Autonomous Networks](https://arxiv.org/abs/2602.01271)
*Burak Demirel,Pablo Soldati,Yu Wang*

Main category: cs.LG

TL;DR: 提出基于三个专门代理的Agentic AI系统，用于意图驱动的自治网络，通过语言模型解析意图、优化问题转换和多目标强化学习控制，实现网络自主运行。


<details>
  <summary>Details</summary>
Motivation: 电信网络需要自主运行并支持具有多样化且经常冲突意图的异构服务，但现有启发式方法无法将高层意图（如超低延迟、高吞吐量、能效）转化为具体的控制动作。

Method: 设计三个专门代理：1) 监督解释代理（基于语言模型）进行意图的词法解析和认知细化；2) 优化代理将模板转化为可处理的优化问题并分析权衡；3) 偏好驱动控制器代理（基于多目标强化学习）利用偏好操作网络性能的帕累托前沿。

Result: 这些代理使网络能够以可扩展的方式自主解释、推理、适应和响应多样化意图和网络条件，实现意图驱动的自治网络。

Conclusion: 提出的Agentic AI系统通过三个专门代理的协同工作，解决了将高层意图转化为具体控制动作的挑战，为意图驱动的自治网络提供了可扩展的解决方案。

Abstract: Telecommunication networks are increasingly expected to operate autonomously while supporting heterogeneous services with diverse and often conflicting intents -- that is, performance objectives, constraints, and requirements specific to each service. However, transforming high-level intents -- such as ultra-low latency, high throughput, or energy efficiency -- into concrete control actions (i.e., low-level actuator commands) remains beyond the capability of existing heuristic approaches. This work introduces an Agentic AI system for intent-driven autonomous networks, structured around three specialized agents. A supervisory interpreter agent, powered by language models, performs both lexical parsing of intents into executable optimization templates and cognitive refinement based on feedback, constraint feasibility, and evolving network conditions. An optimizer agent converts these templates into tractable optimization problems, analyzes trade-offs, and derives preferences across objectives. Lastly, a preference-driven controller agent, based on multi-objective reinforcement learning, leverages these preferences to operate near the Pareto frontier of network performance that best satisfies the original intent. Collectively, these agents enable networks to autonomously interpret, reason over, adapt to, and act upon diverse intents and network conditions in a scalable manner.

</details>


### [692] [Richer Bayesian Last Layers with Subsampled NTK Features](https://arxiv.org/abs/2602.01279)
*Sergio Calvo-Ordoñez,Jonathan Plenk,Richard Bergna,Álvaro Cartea,Yarin Gal,Jose Miguel Hernández-Lobato,Kamil Ciosek*

Main category: cs.LG

TL;DR: 提出一种改进贝叶斯最后一层的方法，通过将神经正切核特征投影到最后一层特征空间，以更准确地估计认知不确定性，同时保持计算效率。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯最后一层虽然计算高效，但低估了认知不确定性，因为它只对最后一层进行贝叶斯处理，忽略了前面层引入的不确定性。

Method: 利用神经正切核特征投影到最后一层特征空间，使后验推断能考虑整个网络的变异性，同时引入均匀子采样方案来估计投影矩阵和进行后验推断，以进一步降低计算成本。

Result: 方法产生的后验方差理论上大于或等于标准BLL，纠正了其低估认知不确定性的倾向。在UCI回归、上下文赌博机、图像分类以及图像和表格数据的分布外检测任务中，相比标准BLL和竞争基线，表现出更好的校准和不确定性估计，同时降低了计算成本。

Conclusion: 提出的方法通过NTK特征投影有效改善了贝叶斯最后一层的认知不确定性估计，在保持计算效率的同时提供了更准确的不确定性量化。

Abstract: Bayesian Last Layers (BLLs) provide a convenient and computationally efficient way to estimate uncertainty in neural networks. However, they underestimate epistemic uncertainty because they apply a Bayesian treatment only to the final layer, ignoring uncertainty induced by earlier layers. We propose a method that improves BLLs by leveraging a projection of Neural Tangent Kernel (NTK) features onto the space spanned by the last-layer features. This enables posterior inference that accounts for variability of the full network while retaining the low computational cost of inference of a standard BLL. We show that our method yields posterior variances that are provably greater or equal to those of a standard BLL, correcting its tendency to underestimate epistemic uncertainty. To further reduce computational cost, we introduce a uniform subsampling scheme for estimating the projection matrix and for posterior inference. We derive approximation bounds for both types of sub-sampling. Empirical evaluations on UCI regression, contextual bandits, image classification, and out-of-distribution detection tasks in image and tabular datasets, demonstrate improved calibration and uncertainty estimates compared to standard BLLs and competitive baselines, while reducing computational cost.

</details>


### [693] [EvoMU: Evolutionary Machine Unlearning](https://arxiv.org/abs/2602.02139)
*Pawel Batorski,Paul Swoboda*

Main category: cs.LG

TL;DR: EvoMU使用进化搜索自动发现针对特定任务的最佳遗忘损失函数，在有限计算资源下实现SOTA遗忘性能


<details>
  <summary>Details</summary>
Motivation: 现有机器学习遗忘方法面临两个主要挑战：1) 合适的遗忘损失函数空间巨大，难以手动搜索最优函数；2) 不同数据集结构和重叠程度不同，不存在通用的最优损失函数

Method: 采用进化搜索程序自动在庞大的可能遗忘损失函数空间中寻找任务特定的损失函数，使用小型4B参数模型(Qwen3-4B-Thinking)实现，无需人工干预

Result: 在TOFU-5%、TOFU-10%、MUSE和WMDP数据集上超越了现有基于损失的遗忘方法，通过合成新颖的遗忘损失函数实现了SOTA结果

Conclusion: EvoMU展示了AI协同科学家在有限计算资源下的潜力，能够自动发现针对特定数据集的最优遗忘损失函数，解决了手动设计损失函数的困难

Abstract: Machine unlearning aims to unlearn specified training data (e.g. sensitive or copyrighted material). A prominent approach is to fine-tune an existing model with an unlearning loss that retains overall utility. The space of suitable unlearning loss functions is vast, making the search for an optimal loss function daunting. Additionally, there might not even exist a universally optimal loss function: differences in the structure and overlap of the forget and retain data can cause a loss to work well in one setting but over-unlearn or under-unlearn in another. Our approach EvoMU tackles these two challenges simultaneously. An evolutionary search procedure automatically finds task-specific losses in the vast space of possible unlearning loss functions. This allows us to find dataset-specific losses that match or outperform existing losses from the literature, without the need for a human-in-the-loop. This work is therefore an instance of automatic scientific discovery, a.k.a. an AI co-scientist. In contrast to previous AI co-scientist works, we do so on a budget: We achieve SotA results using a small 4B parameter model (Qwen3-4B-Thinking), showing the potential of AI co-scientists with limited computational resources. Our experimental evaluation shows that we surpass previous loss-based unlearning formulations on TOFU-5%, TOFU-10%, MUSE and WMDP by synthesizing novel unlearning losses. Our code is available at https://github.com/Batorskq/EvoMU.

</details>


### [694] [Learning Generative Selection for Best-of-N](https://arxiv.org/abs/2602.02143)
*Shubham Toshniwal,Aleksander Ficek,Siddhartha Jain,Wei Du,Vahid Noroozi,Sadegh Mahdavi,Somshubra Majumdar,Igor Gitman*

Main category: cs.LG

TL;DR: 通过强化学习训练小模型实现强大的生成选择能力，提升测试时并行采样的推理性能


<details>
  <summary>Details</summary>
Motivation: 并行采样可以显著提升LLM推理能力，但受限于Best-of-N选择质量。现有的生成选择方法（如GenSelect）主要在大模型上表现良好，小模型的选择能力有限。需要探索如何让小模型也能获得强大的生成选择能力。

Method: 从大规模数学和代码指令数据集中筛选出同时包含正确和错误候选解决方案的实例，合成选择任务。使用DAPO（强化学习算法）训练1.7B参数的小模型，奖励正确的选择决策。

Result: 在数学（AIME24、AIME25、HMMT25）和代码（LiveCodeBench）推理基准测试中，训练后的小模型持续优于提示和多数投票基线方法，性能常常接近甚至超过更大的模型。这些提升还能泛化到从更强模型输出中进行选择，尽管训练时只使用了较弱模型的输出。

Conclusion: 强化学习是解锁小模型强大生成选择能力的可扩展方法，能够实现高效的测试时扩展，为小模型在推理任务中的实际应用提供了新途径。

Abstract: Scaling test-time compute via parallel sampling can substantially improve LLM reasoning, but is often limited by Best-of-N selection quality. Generative selection methods, such as GenSelect, address this bottleneck, yet strong selection performance remains largely limited to large models. We show that small reasoning models can acquire strong GenSelect capabilities through targeted reinforcement learning. To this end, we synthesize selection tasks from large-scale math and code instruction datasets by filtering to instances with both correct and incorrect candidate solutions, and train 1.7B-parameter models with DAPO to reward correct selections. Across math (AIME24, AIME25, HMMT25) and code (LiveCodeBench) reasoning benchmarks, our models consistently outperform prompting and majority-voting baselines, often approaching or exceeding much larger models. Moreover, these gains generalize to selecting outputs from stronger models despite training only on outputs from weaker models. Overall, our results establish reinforcement learning as a scalable way to unlock strong generative selection in small models, enabling efficient test-time scaling.

</details>


### [695] [EDIS: Diagnosing LLM Reasoning via Entropy Dynamics](https://arxiv.org/abs/2602.01288)
*Chenghua Zhu,Siyan Wu,Xiangkang Zeng,Zishan Xu,Zhaolu Kang,Yifu Guo,Yuquan Lu,Junduan Huang,Guojing Zhou*

Main category: cs.LG

TL;DR: 论文提出通过分析LLM生成过程中的熵动态变化来改进推理能力，发现错误推理具有不稳定的熵轨迹特征，并引入EDIS指标量化这种不稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有方法将置信度视为静态量（通常在token上聚合），但生成过程中置信度的时序演化包含更丰富的信息。论文旨在探索熵动态变化如何揭示推理失败的内在特性。

Method: 分析token级熵轨迹，识别正确与错误推理的特征模式；引入熵动态不稳定性评分（EDIS）量化熵演化中的不稳定性；将EDIS用于推理时选择和训练时样本筛选。

Result: 错误解决方案表现出不稳定的动态特征：突发尖峰（持续不确定性增长）和峰谷尖峰（短暂置信后急剧反弹）。这些模式在不同模型和训练阶段持续存在。EDIS作为有效的诊断信号，显著提高了推理准确性。

Conclusion: 熵动态为理解和改进LLM推理提供了一个未被充分探索但信息丰富的视角。EDIS指标在推理时选择和训练时样本筛选方面具有应用前景。

Abstract: Entropy-based confidence signals are increasingly leveraged to improve reasoning in large language models (LLMs), yet existing approaches treat confidence as a static quantity -- typically aggregated over tokens. We show that the \emph{temporal evolution} of confidence during generation carries richer information than aggregate statistics alone. Analyzing token-level entropy trajectories, we identify characteristic patterns distinguishing correct from incorrect reasoning: erroneous solutions exhibit unstable dynamics, including burst spikes (sustained uncertainty growth) and peak-valley spikes (sharp rebounds following transient confidence). These patterns persist across models and training stages, suggesting they reflect intrinsic properties of reasoning failure rather than superficial noise. To formalize this observation, we introduce the Entropy Dynamics Instability Score (\textbf{EDIS}), a trajectory-level metric quantifying instability in entropy evolution. EDIS serves as an effective diagnostic signal for inference-time selection, substantially improving reasoning accuracy, and offers a promising direction for training-time sample curation. Our findings establish entropy dynamics as an underexplored yet informative lens for understanding and improving LLM reasoning.

</details>


### [696] [Revisiting Adaptive Rounding with Vectorized Reparameterization for LLM Quantization](https://arxiv.org/abs/2602.02151)
*Yuli Zhou,Qingxuan Chen,Luca Benini,Guolei Sun,Yawei Li*

Main category: cs.LG

TL;DR: VQRound是一种参数高效的量化优化框架，通过将舍入矩阵重新参数化为紧凑码本来实现自适应舍入，显著减少了LLM量化所需的可训练参数，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 传统自适应舍入方法在大型语言模型上需要密集的逐元素舍入矩阵，计算成本过高。需要一种既能实现跨元素误差抵消，又具有参数效率的量化方法。

Method: 提出VQRound框架：1) 将舍入矩阵重新参数化为紧凑码本；2) 在L∞范数下最小化逐元素最坏情况误差；3) 开发轻量级端到端微调流程，仅需128个样本优化所有层的码本。

Result: 在OPT、LLaMA、LLaMA2和Qwen3模型上的实验表明，VQRound在相同步数下比传统自适应舍入收敛更好，同时仅使用0.2%的可训练参数，实现了自适应舍入的可扩展性和快速拟合。

Conclusion: 自适应舍入可以通过参数高效的重新参数化变得既可扩展又快速拟合，VQRound为大型语言模型的高效量化提供了实用解决方案。

Abstract: Adaptive Rounding has emerged as an alternative to round-to-nearest (RTN) for post-training quantization by enabling cross-element error cancellation. Yet, dense and element-wise rounding matrices are prohibitively expensive for billion-parameter large language models (LLMs). We revisit adaptive rounding from an efficiency perspective and propose VQRound, a parameter-efficient optimization framework that reparameterizes the rounding matrix into a compact codebook. Unlike low-rank alternatives, VQRound minimizes the element-wise worst-case error under $L_\infty$ norm, which is critical for handling heavy-tailed weight distributions in LLMs. Beyond reparameterization, we identify rounding initialization as a decisive factor and develop a lightweight end-to-end finetuning pipeline that optimizes codebooks across all layers using only 128 samples. Extensive experiments on OPT, LLaMA, LLaMA2, and Qwen3 models demonstrate that VQRound achieves better convergence than traditional adaptive rounding at the same number of steps while using as little as 0.2% of the trainable parameters. Our results show that adaptive rounding can be made both scalable and fast-fitting. The code is available at https://github.com/zhoustan/VQRound.

</details>


### [697] [Gradient-Aligned Calibration for Post-Training Quantization of Diffusion Models](https://arxiv.org/abs/2602.01289)
*Dung Anh Hoang,Cuong Pham anh Trung Le,Jianfei Cai,Toan Do*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的扩散模型后训练量化方法，通过学习为不同时间步的校准样本分配最优权重，解决现有方法中均匀权重分配和梯度冲突的问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然图像合成性能出色，但存在推理速度慢、内存占用高、计算需求大的问题。后训练量化是加速扩散模型的有效方案，但现有方法在不同时间步使用均匀权重分配校准样本，忽略了不同时间步数据对扩散过程的贡献差异，且由于不同时间步的激活分布和梯度变化，均匀量化方法会导致梯度冲突，降低性能。

Method: 提出一种新颖的后训练量化方法，通过学习为校准样本分配最优权重，使量化模型在不同时间步的梯度对齐，从而优化量化过程。该方法考虑了不同时间步对扩散过程的贡献差异，解决了梯度冲突问题。

Result: 在CIFAR-10、LSUN-Bedrooms和ImageNet数据集上的大量实验表明，该方法相比其他扩散模型后训练量化方法具有优越性。

Conclusion: 通过学习为不同时间步校准样本分配最优权重的方法，有效解决了扩散模型后训练量化中的梯度冲突问题，显著提升了量化性能，为扩散模型的实用部署提供了更有效的加速方案。

Abstract: Diffusion models have shown remarkable performance in image synthesis by progressively estimating a smooth transition from a Gaussian distribution of noise to a real image. Unfortunately, their practical deployment is limited by slow inference speed, high memory usage, and the computational demands of the noise estimation process. Post-training quantization (PTQ) emerges as a promising solution to accelerate sampling and reduce memory overhead for diffusion models. Existing PTQ methods for diffusion models typically apply uniform weights to calibration samples across timesteps, which is sub-optimal since data at different timesteps may contribute differently to the diffusion process. Additionally, due to varying activation distributions and gradients across timesteps, a uniform quantization approach is sub-optimal. Each timestep requires a different gradient direction for optimal quantization, and treating them equally can lead to conflicting gradients that degrade performance. In this paper, we propose a novel PTQ method that addresses these challenges by assigning appropriate weights to calibration samples. Specifically, our approach learns to assign optimal weights to calibration samples to align the quantized model's gradients across timesteps, facilitating the quantization process. Extensive experiments on CIFAR-10, LSUN-Bedrooms, and ImageNet demonstrate the superiority of our method compared to other PTQ methods for diffusion models.

</details>


### [698] [The BoBW Algorithms for Heavy-Tailed MDPs](https://arxiv.org/abs/2602.01295)
*Yu Chen,Yuhao Liu,Jiatai Huang,Yihan Du,Longbo Huang*

Main category: cs.LG

TL;DR: 提出HT-FTRL-OM和HT-FTRL-UOB算法，用于处理具有重尾反馈的马尔可夫决策过程，在对抗环境中实现次线性遗憾，在随机环境中实现对数遗憾


<details>
  <summary>Details</summary>
Motivation: 现有处理重尾反馈MDP的方法在随机环境中过于保守，在对抗环境中缺乏适应性，需要一种能同时适应两种环境的算法

Method: 基于FTRL框架，在占用度量上使用新的跳过损失估计器（HT-FTRL-OM），对于未知转移情况，使用悲观跳过损失估计器（HT-FTRL-UOB）

Result: HT-FTRL-OM在对抗环境中达到Õ(T^{1/α})遗憾，在随机环境中达到O(log T)遗憾；HT-FTRL-UOB在对抗环境中达到Õ(T^{1/α} + √T)遗憾，在随机环境中达到O(log² T)遗憾

Conclusion: 提出的算法实现了"两全其美"保证，在对抗和随机环境中都能获得良好性能，通过技术创新克服了重尾反馈和转移不确定性的挑战

Abstract: We investigate episodic Markov Decision Processes with heavy-tailed feedback (HTMDPs). Existing approaches for HTMDPs are conservative in stochastic environments and lack adaptivity in adversarial regimes. In this work, we propose algorithms ```HT-FTRL-OM``` and ```HT-FTRL-UOB``` for HTMDPs that achieve Best-of-Both-Worlds (BoBW) guarantees: instance-independent regret in adversarial environments and logarithmic instance-dependent regret in self-bounding (including the stochastic case) environments. For the known transition setting, ```HT-FTRL-OM``` applies the Follow-The-Regularized-Leader (FTRL) framework over occupancy measures with novel skipping loss estimators, achieving a $\widetilde{\mathcal{O}}(T^{1/α})$ regret bound in adversarial regimes and a $\mathcal{O}(\log T)$ regret in stochastic regimes. Building upon this framework, we develop a novel algorithm ```HT-FTRL-UOB``` to tackle the more challenging unknown-transition setting. This algorithm employs a pessimistic skipping loss estimator and achieves a $\widetilde{\mathcal{O}}(T^{1/α} + \sqrt{T})$ regret in adversarial regimes and a $\mathcal{O}(\log^2(T))$ regret in stochastic regimes. Our analysis overcomes key barriers through several technical insights, including a local control mechanism for heavy-tailed shifted losses, a new suboptimal-mass propagation principle, and a novel regret decomposition that isolates transition uncertainty from heavy-tailed estimation errors and skipping bias.

</details>


### [699] [Learning While Staying Curious: Entropy-Preserving Supervised Fine-Tuning via Adaptive Self-Distillation for Large Reasoning Models](https://arxiv.org/abs/2602.02244)
*Hao Wang,Hao Gu,Hongming Piao,Kaixiong Gong,Yuxiao Ye,Xiangyu Yue,Sirui Han,Yike Guo,Dapeng Wu*

Main category: cs.LG

TL;DR: CurioSFT是一种保持熵的监督微调方法，通过内在好奇心增强探索能力，在数学推理任务上显著优于传统SFT，并为后续强化学习阶段带来更大收益。


<details>
  <summary>Details</summary>
Motivation: 传统SFT-then-RL流程存在问题：SFT模仿专家演示会导致过度自信和生成多样性降低，限制了RL阶段的探索空间。添加熵正则化也不是万能药，它会使token分布趋于均匀，增加熵但不能改善有意义的探索能力。

Method: 提出CurioSFT方法，包含两个核心组件：(1) 自我探索蒸馏：将模型蒸馏到自生成的温度缩放教师模型，鼓励在其能力范围内探索；(2) 熵引导温度选择：自适应调整蒸馏强度，通过放大推理token的探索同时稳定事实token，减轻知识遗忘。

Result: 在数学推理任务上，CurioSFT在SFT阶段相比传统SFT：在分布内任务上提升2.5分，在分布外任务上提升2.9分。保留的探索能力在RL阶段转化为具体收益，平均提升5.0分。

Conclusion: CurioSFT通过保持熵和增强内在好奇心，有效解决了传统SFT的限制，不仅提升了SFT阶段的性能，还为后续RL阶段提供了更好的探索基础，实现了端到端的改进。

Abstract: The standard post-training recipe for large reasoning models, supervised fine-tuning followed by reinforcement learning (SFT-then-RL), may limit the benefits of the RL stage: while SFT imitates expert demonstrations, it often causes overconfidence and reduces generation diversity, leaving RL with a narrowed solution space to explore. Adding entropy regularization during SFT is not a cure-all; it tends to flatten token distributions toward uniformity, increasing entropy without improving meaningful exploration capability. In this paper, we propose CurioSFT, an entropy-preserving SFT method designed to enhance exploration capabilities through intrinsic curiosity. It consists of (a) Self-Exploratory Distillation, which distills the model toward a self-generated, temperature-scaled teacher to encourage exploration within its capability; and (b) Entropy-Guided Temperature Selection, which adaptively adjusts distillation strength to mitigate knowledge forgetting by amplifying exploration at reasoning tokens while stabilizing factual tokens. Extensive experiments on mathematical reasoning tasks demonstrate that, in SFT stage, CurioSFT outperforms the vanilla SFT by 2.5 points on in-distribution tasks and 2.9 points on out-of-distribution tasks. We also verify that exploration capabilities preserved during SFT successfully translate into concrete gains in RL stage, yielding an average improvement of 5.0 points.

</details>


### [700] [Dispelling the Curse of Singularities in Neural Network Optimizations](https://arxiv.org/abs/2602.01308)
*Hengjie Cao,Mengyi Chen,Yifeng Yang,Fang Dong,Ruijun Huang,Anrui Chen,Jixian Zhou,Mingzhi Dong,Yujiang Wang,Dongsheng Li,Wenyi Fang,Yuanyi Lin,Fan Wu,Li Shang*

Main category: cs.LG

TL;DR: 论文从参数空间奇异值角度研究深度神经网络优化不稳定性，发现训练中奇异值会增长并加剧表示空间奇异值，提出参数奇异值平滑方法缓解这一问题。


<details>
  <summary>Details</summary>
Motivation: 研究深度神经网络优化不稳定性的新视角：参数空间中奇异值的出现和放大。传统方法较少从参数奇异值角度分析训练不稳定性问题。

Method: 提出参数奇异值平滑（PSS）方法，轻量级、灵活且有效地平滑权重矩阵的奇异谱，防止奇异值过度增长。

Result: PSS能缓解训练不稳定性，即使在训练失败后也能恢复可训练性，同时提高训练效率和泛化性能，在多种数据集、架构和优化器上验证有效。

Conclusion: 参数奇异值的增长是深度神经网络优化不稳定性的关键因素，PSS方法通过平滑奇异谱有效缓解这一问题，为训练稳定性提供了新解决方案。

Abstract: This work investigates the optimization instability of deep neural networks from a less-explored yet insightful perspective: the emergence and amplification of singularities in the parametric space. Our analysis reveals that parametric singularities inevitably grow with gradient updates and further intensify alignment with representations, leading to increased singularities in the representation space. We show that the gradient Frobenius norms are bounded by the top singular values of the weight matrices, and as training progresses, the mutually reinforcing growth of weight and representation singularities, termed the curse of singularities, relaxes these bounds, escalating the risk of sharp loss explosions. To counter this, we propose Parametric Singularity Smoothing (PSS), a lightweight, flexible, and effective method for smoothing the singular spectra of weight matrices. Extensive experiments across diverse datasets, architectures, and optimizers demonstrate that PSS mitigates instability, restores trainability even after failure, and improves both training efficiency and generalization.

</details>


### [701] [Statistical Learning Theory in Lean 4: Empirical Processes from Scratch](https://arxiv.org/abs/2602.02285)
*Yuanhe Zhang,Jason D. Lee,Fanghui Liu*

Main category: cs.LG

TL;DR: 首个基于经验过程理论的统计学习理论完整Lean 4形式化，填补了Mathlib库空白，包含高斯Lipschitz集中性、Dudley熵积分定理等形式化，应用于最小二乘回归并获得尖锐速率。


<details>
  <summary>Details</summary>
Motivation: 填补Lean 4 Mathlib库在统计学习理论方面的空白，建立可重用的形式化基础，通过形式化过程揭示和解决标准SLT教材中的隐含假设和缺失细节。

Method: 采用人机协作工作流：人类设计证明策略，AI代理执行战术证明构造，最终得到人类验证的Lean 4工具箱。实现了高斯Lipschitz集中性、Dudley熵积分定理等形式化。

Result: 建立了完整的统计学习理论形式化基础设施，包括高斯Lipschitz集中性、首个Dudley熵积分定理形式化，并应用于最小二乘（稀疏）回归获得尖锐速率。

Conclusion: 这项工作为机器学习理论建立了可重用的形式化基础，为未来发展打开了大门，并通过形式化过程深化了对理论的理解。

Abstract: We present the first comprehensive Lean 4 formalization of statistical learning theory (SLT) grounded in empirical process theory. Our end-to-end formal infrastructure implement the missing contents in latest Lean 4 Mathlib library, including a complete development of Gaussian Lipschitz concentration, the first formalization of Dudley's entropy integral theorem for sub-Gaussian processes, and an application to least-squares (sparse) regression with a sharp rate. The project was carried out using a human-AI collaborative workflow, in which humans design proof strategies and AI agents execute tactical proof construction, leading to the human-verified Lean 4 toolbox for SLT. Beyond implementation, the formalization process exposes and resolves implicit assumptions and missing details in standard SLT textbooks, enforcing a granular, line-by-line understanding of the theory. This work establishes a reusable formal foundation and opens the door for future developments in machine learning theory. The code is available at https://github.com/YuanheZ/lean-stat-learning-theory

</details>


### [702] [Imperfect Influence, Preserved Rankings: A Theory of TRAK for Data Attribution](https://arxiv.org/abs/2602.01312)
*Han Tong,Shubhangi Ghosh,Haolin Zou,Arian Maleki*

Main category: cs.LG

TL;DR: TRAK算法用于数据归因，通过核机器近似模型并利用ALO风险近似技术，本文对其进行了理论分析，量化了近似误差，证明虽然误差显著但相关性保持良好。


<details>
  <summary>Details</summary>
Motivation: TRAK算法在数据归因方面表现出色，但其理论准确性条件和失效机制尚未充分探索，需要理论分析来理解其性能边界。

Method: 对TRAK算法进行理论分析，量化其近似方法引入的误差，并通过模拟和实证研究验证理论结果。

Result: 虽然近似方法带来显著误差，但TRAK估计的影响力与原始影响力高度相关，能有效保持数据点的相对排序。

Conclusion: TRAK算法在数据归因中具有实用价值，尽管存在近似误差，但其相关性保持良好，适用于相对排序任务。

Abstract: Data attribution, tracing a model's prediction back to specific training data, is an important tool for interpreting sophisticated AI models. The widely used TRAK algorithm addresses this challenge by first approximating the underlying model with a kernel machine and then leveraging techniques developed for approximating the leave-one-out (ALO) risk. Despite its strong empirical performance, the theoretical conditions under which the TRAK approximations are accurate as well as the regimes in which they break down remain largely unexplored. In this paper, we provide a theoretical analysis of the TRAK algorithm, characterizing its performance and quantifying the errors introduced by the approximations on which the method relies. We show that although the approximations incur significant errors, TRAK's estimated influence remains highly correlated with the original influence and therefore largely preserves the relative ranking of data points. We corroborate our theoretical results through extensive simulations and empirical studies.

</details>


### [703] [Finding Differentially Private Second Order Stationary Points in Stochastic Minimax Optimization](https://arxiv.org/abs/2602.01339)
*Difei Xu,Youming Tao,Meng Ding,Chenglin Fan,Di Wang*

Main category: cs.LG

TL;DR: 首次研究随机非凸极小极大优化中差分隐私二阶平稳点的寻找问题，提出结合嵌套梯度下降-上升、方差缩减和高斯扰动的一阶方法，在经验风险和总体风险下均获得最优收敛速率。


<details>
  <summary>Details</summary>
Motivation: 现有文献要么只关注极小极大问题的一阶平稳点，要么只关注经典随机最小化问题的二阶平稳点。本文首次系统研究随机非凸极小极大优化中差分隐私二阶平稳点的寻找问题，为经验风险和总体风险提供统一处理。

Method: 提出纯一阶方法，结合嵌套梯度下降-上升方案、SPIDER风格方差缩减和高斯扰动来确保隐私。关键技术是块状（q周期）分析，控制随机方差和隐私噪声的累积，无需对整个迭代范围求和，实现对经验风险和总体风险公式的统一处理。

Result: 在标准光滑性、Hessian-Lipschitz性和强凹性假设下，建立了高概率保证：对于经验风险目标达到$(α,\sqrt{ρ_Φα})$-近似二阶平稳点，其中$α= \mathcal{O}( (\frac{\sqrt{d}}{n\varepsilon})^{2/3})$；对于总体目标达到$\mathcal{O}(\frac{1}{n^{1/3}} + (\frac{\sqrt{d}}{n\varepsilon})^{1/2})$，匹配私有一阶平稳性的最佳已知速率。

Conclusion: 首次为随机非凸极小极大优化中的差分隐私二阶平稳点问题提供系统研究，提出的一阶方法在经验风险和总体风险下均达到最优收敛速率，填补了现有文献的空白。

Abstract: We provide the first study of the problem of finding differentially private (DP) second-order stationary points (SOSP) in stochastic (non-convex) minimax optimization. Existing literature either focuses only on first-order stationary points for minimax problems or on SOSP for classical stochastic minimization problems. This work provides, for the first time, a unified and detailed treatment of both empirical and population risks. Specifically, we propose a purely first-order method that combines a nested gradient descent--ascent scheme with SPIDER-style variance reduction and Gaussian perturbations to ensure privacy. A key technical device is a block-wise ($q$-period) analysis that controls the accumulation of stochastic variance and privacy noise without summing over the full iteration horizon, yielding a unified treatment of both empirical-risk and population formulations. Under standard smoothness, Hessian-Lipschitzness, and strong concavity assumptions, we establish high-probability guarantees for reaching an $(α,\sqrt{ρ_Φα})$-approximate second-order stationary point with $α= \mathcal{O}( (\frac{\sqrt{d}}{n\varepsilon})^{2/3})$ for empirical risk objectives and $\mathcal{O}(\frac{1}{n^{1/3}} + (\frac{\sqrt{d}}{n\varepsilon})^{1/2})$ for population objectives, matching the best known rates for private first-order stationarity.

</details>


### [704] [SPARKLING: Balancing Signal Preservation and Symmetry Breaking for Width-Progressive Learning](https://arxiv.org/abs/2602.02472)
*Qifan Yu,Xinyu Ma,Zhijian Zhuo,Minrui Wang,Deyi Liu,Shiyi Zhan,Yiyuan Ma,Liang Xiang,Xingyan Bin,Di He*

Main category: cs.LG

TL;DR: SPARKLING是一个用于模型宽度渐进学习的新框架，解决了中阶段宽度扩展时的训练不稳定问题，通过信号保持和对称性打破技术，在MoE模型上实现了高达35%的训练成本节省。


<details>
  <summary>Details</summary>
Motivation: 渐进学习通过逐步增加模型规模来减少预训练计算开销。虽然深度扩展已有广泛研究，但宽度扩展研究不足，现有方法仅限于训练早期阶段。然而，在中阶段进行宽度扩展对于最大化计算节省至关重要，但由于严重的训练不稳定问题，这仍然是一个巨大挑战。

Method: 提出SPARKLING框架，通过RMS尺度一致性实现信号保持，稳定扩展时的激活统计；通过非对称优化器状态重置和学习率重新预热确保对称性打破，解决梯度对称性问题。

Result: 在混合专家模型上的广泛实验表明，SPARKLING在多种宽度轴和优化器家族中始终优于从头训练，在2倍宽度扩展下将训练成本降低高达35%。

Conclusion: SPARKLING成功解决了中阶段宽度扩展的训练不稳定问题，为渐进学习提供了有效的宽度扩展方法，显著减少了模型训练的计算开销。

Abstract: Progressive Learning (PL) reduces pre-training computational overhead by gradually increasing model scale. While prior work has extensively explored depth expansion, width expansion remains significantly understudied, with the few existing methods limited to the early stages of training. However, expanding width during the mid-stage is essential for maximizing computational savings, yet it remains a formidable challenge due to severe training instabilities. Empirically, we show that naive initialization at this stage disrupts activation statistics, triggering loss spikes, while copy-based initialization introduces gradient symmetry that hinders feature diversity. To address these issues, we propose SPARKLING (balancing {S}ignal {P}reservation {A}nd symmet{R}y brea{K}ing for width-progressive {L}earn{ING}), a novel framework for mid-stage width expansion. Our method achieves signal preservation via RMS-scale consistency, stabilizing activation statistics during expansion. Symmetry breaking is ensured through asymmetric optimizer state resetting and learning rate re-warmup. Extensive experiments on Mixture-of-Experts (MoE) models demonstrate that, across multiple width axes and optimizer families, SPARKLING consistently outperforms training from scratch and reduces training cost by up to 35% under $2\times$ width expansion.

</details>


### [705] [Your Self-Play Algorithm is Secretly an Adversarial Imitator: Understanding LLM Self-Play through the Lens of Imitation Learning](https://arxiv.org/abs/2602.01357)
*Shangzhe Li,Xuchao Zhang,Chetan Bansal,Weitong Zhang*

Main category: cs.LG

TL;DR: 该论文将自博弈微调与对抗模仿学习联系起来，提出了基于χ²散度的稳定自博弈模仿微调算法，并在实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 自博弈后训练方法已成为微调大语言模型的有效方法，但理论基础尚未充分探索。本文旨在建立自博弈微调的理论基础，将其与对抗模仿学习联系起来。

Method: 将微调过程形式化为模型与由模型本身参数化的正则化隐式奖励玩家之间的min-max博弈，提出基于χ²散度变分目标的自博弈模仿微调算法，具有有界奖励和更好稳定性。

Result: 理论分析表明自博弈微调会收敛到均衡。在各种语言模型微调任务上的实验证明，该方法相比现有自博弈方法有持续改进，验证了理论见解。

Conclusion: 通过将自博弈微调与对抗模仿学习联系起来，建立了统一的理论框架，提出的新算法在实践中表现出色，为自博弈微调提供了理论基础。

Abstract: Self-play post-training methods has emerged as an effective approach for finetuning large language models and turn the weak language model into strong language model without preference data. However, the theoretical foundations for self-play finetuning remain underexplored. In this work, we tackle this by connecting self-play finetuning with adversarial imitation learning by formulating finetuning procedure as a min-max game between the model and a regularized implicit reward player parameterized by the model itself. This perspective unifies self-play imitation and general preference alignment within a common framework. Under this formulation, we present a game-theoretic analysis showing that the self-play finetuning will converge to it's equilibrium. Guided by this theoretical formulation, we propose a new self-play imitation finetuning algorithm based on the $χ^2$-divergence variational objective with bounded rewards and improved stability. Experiments on various of language model finetuning tasks demonstrate consistent improvements over existing self-play methods and validate our theoretical insights.

</details>


### [706] [RLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System](https://arxiv.org/abs/2602.02488)
*Yinjie Wang,Tianbao Xie,Ke Shen,Mengdi Wang,Ling Yang*

Main category: cs.LG

TL;DR: RLAnything是一个强化学习框架，通过闭环优化动态构建环境、策略和奖励模型，增强LLM和智能体场景的学习效果。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习方法在LLM和智能体场景中面临学习信号弱、系统整体性能不足的问题，需要一种能够动态优化环境、策略和奖励模型的集成框架来提升学习效果。

Method: 1) 策略训练整合步进式和结果反馈信号；2) 奖励模型通过一致性反馈联合优化；3) 理论驱动的自动环境适应利用批评反馈改进训练；4) 各组件相互增强形成闭环优化系统。

Result: RLAnything在多个代表性任务上取得显著提升：Qwen3-VL-8B-Thinking在OSWorld上提升9.1%，Qwen2.5-7B-Instruct在AlfWorld和LiveBench上分别提升18.7%和11.9%。优化的奖励模型信号优于依赖人工标签的结果。

Conclusion: RLAnything框架通过动态闭环优化环境、策略和奖励模型，有效增强了强化学习系统的整体性能，在LLM和智能体任务中展现出显著优势，为强化学习应用提供了新的解决方案。

Abstract: We propose RLAnything, a reinforcement learning framework that dynamically forges environment, policy, and reward models through closed-loop optimization, amplifying learning signals and strengthening the overall RL system for any LLM or agentic scenarios. Specifically, the policy is trained with integrated feedback from step-wise and outcome signals, while the reward model is jointly optimized via consistency feedback, which in turn further improves policy training. Moreover, our theory-motivated automatic environment adaptation improves training for both the reward and policy models by leveraging critic feedback from each, enabling learning from experience. Empirically, each added component consistently improves the overall system, and RLAnything yields substantial gains across various representative LLM and agentic tasks, boosting Qwen3-VL-8B-Thinking by 9.1% on OSWorld and Qwen2.5-7B-Instruct by 18.7% and 11.9% on AlfWorld and LiveBench, respectively. We also that optimized reward-model signals outperform outcomes that rely on human labels. Code: https://github.com/Gen-Verse/Open-AgentRL

</details>


### [707] [PaAno: Patch-Based Representation Learning for Time-Series Anomaly Detection](https://arxiv.org/abs/2602.01359)
*Jinju Park,Seokho Kang*

Main category: cs.LG

TL;DR: PaAno是一种轻量级的时间序列异常检测方法，使用1D卷积神经网络提取时间片段特征，结合三元组损失和预文本损失训练，在TSB-AD基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于transformer和基础模型的时间序列异常检测方法计算成本高、内存占用大，不适用于实时和资源受限场景，且性能提升有限。

Method: 从时间序列训练数据中提取短时间片段，使用1D卷积神经网络将每个片段嵌入为向量表示，结合三元组损失和预文本损失训练模型。

Result: 在TSB-AD基准测试中，PaAno在单变量和多变量时间序列异常检测上均达到最先进性能，显著优于现有方法，包括基于重型架构的方法。

Conclusion: PaAno提供了一种轻量级但有效的解决方案，在保持高性能的同时降低了计算成本和内存需求，适用于实时和资源受限场景。

Abstract: Although recent studies on time-series anomaly detection have increasingly adopted ever-larger neural network architectures such as transformers and foundation models, they incur high computational costs and memory usage, making them impractical for real-time and resource-constrained scenarios. Moreover, they often fail to demonstrate significant performance gains over simpler methods under rigorous evaluation protocols. In this study, we propose Patch-based representation learning for time-series Anomaly detection (PaAno), a lightweight yet effective method for fast and efficient time-series anomaly detection. PaAno extracts short temporal patches from time-series training data and uses a 1D convolutional neural network to embed each patch into a vector representation. The model is trained using a combination of triplet loss and pretext loss to ensure the embeddings capture informative temporal patterns from input patches. During inference, the anomaly score at each time step is computed by comparing the embeddings of its surrounding patches to those of normal patches extracted from the training time-series. Evaluated on the TSB-AD benchmark, PaAno achieved state-of-the-art performance, significantly outperforming existing methods, including those based on heavy architectures, on both univariate and multivariate time-series anomaly detection across various range-wise and point-wise performance measures.

</details>


### [708] [Deep Variational Contrastive Learning for Joint Risk Stratification and Time-to-Event Estimation](https://arxiv.org/abs/2602.01367)
*Pinar Erbil,Alberto Archetti,Eugenio Lomurno,Matteo Matteucci*

Main category: cs.LG

TL;DR: CONVERSE是一个结合变分自编码器和对比学习的深度生存模型，在保持预测性能的同时实现可解释的风险分层。


<details>
  <summary>Details</summary>
Motivation: 深度生存分析面临性能与可解释性的权衡：神经网络预测准确但缺乏可解释性，而基于聚类的方法可解释但牺牲预测性能。临床决策需要既能准确预测又能提供有意义风险分层的模型。

Method: CONVERSE结合变分自编码器和对比学习，使用变分嵌入和多种簇内/簇间对比损失。采用自步学习从易到难逐步纳入样本，支持簇特异性生存头进行集成预测。

Result: 在四个基准数据集上的评估表明，CONVERSE相比现有深度生存方法达到竞争性或更优的性能，同时保持有意义的患者分层。

Conclusion: CONVERSE成功弥合了深度生存分析中性能与可解释性的鸿沟，为临床决策提供了既准确又可解释的风险分层工具。

Abstract: Survival analysis is essential for clinical decision-making, as it allows practitioners to estimate time-to-event outcomes, stratify patient risk profiles, and guide treatment planning. Deep learning has revolutionized this field with unprecedented predictive capabilities but faces a fundamental trade-off between performance and interpretability. While neural networks achieve high accuracy, their black-box nature limits clinical adoption. Conversely, deep clustering-based methods that stratify patients into interpretable risk groups typically sacrifice predictive power. We propose CONVERSE (CONtrastive Variational Ensemble for Risk Stratification and Estimation), a deep survival model that bridges this gap by unifying variational autoencoders with contrastive learning for interpretable risk stratification. CONVERSE combines variational embeddings with multiple intra- and inter-cluster contrastive losses. Self-paced learning progressively incorporates samples from easy to hard, improving training stability. The model supports cluster-specific survival heads, enabling accurate ensemble predictions. Comprehensive evaluation on four benchmark datasets demonstrates that CONVERSE achieves competitive or superior performance compared to existing deep survival methods, while maintaining meaningful patient stratification.

</details>


### [709] [SNIP: An Adaptive Mixed Precision Framework for Subbyte Large Language Model Training](https://arxiv.org/abs/2602.01410)
*Yunjie Pan,Yongyi Yang,Hanmei Yang,Scott Mahlke*

Main category: cs.LG

TL;DR: SNIP是一个细粒度自适应混合精度训练框架，通过定期收集统计信息并定义前向损失发散和后向权重发散两个关键指标，使用整数线性规划优化层间精度，在保持模型质量的同时显著减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 当前混合精度训练方法要么对所有GEMM操作应用统一精度，要么依赖启发式方法，这些方法在训练过程中无法泛化，导致收敛次优和不稳定。需要一种能够支持子字节精度并自适应优化精度分配的方法来提高LLM训练效率。

Method: SNIP框架定期收集激活、梯度和优化器状态的统计信息，定义前向损失发散（量化引起的训练损失增加）和后向权重发散（梯度误差传播影响模型更新）两个指标。基于这些指标构建整数线性规划问题，系统优化各层精度以最小化质量损失同时满足效率目标。

Result: 在1B、3B、7B和70B Llama-like模型上的实验表明，SNIP始终优于现有基线方法，在保持模型质量的同时将FLOPs减少高达80%，且在不同模型规模和训练阶段都有效，计算开销最小。

Conclusion: SNIP提供了一个有效的细粒度自适应混合精度训练框架，能够显著提高LLM预训练效率，同时保持模型质量，解决了当前混合精度训练方法的局限性。

Abstract: Training large language models (LLMs) efficiently while preserving model quality poses significant challenges, particularly with subbyte precision supported by state-of-the-art GPUs. Current mixed-precision training approaches either apply uniform precision to all GEMM operations or rely on heuristic-based methods that fail to generalize during training, leading to suboptimal convergence and instability. To address these challenges, this paper introduces SNIP, a fine-grained adaptive mixed-precision training framework for LLM pretraining that supports subbyte precision. SNIP periodically collects statistics on activations, gradients, and optimizer states to assess the precision loss impact on model quality. We define two key metrics: loss divergence in the forward pass, caused by quantization-induced increases in training loss, and weight divergence in the backward pass, which measures error propagation through gradients affecting model updates. These metrics guide an Integer Linear Programming (ILP) problem that systematically optimizes layerwise precision to minimize overall quality loss while meeting efficiency targets. Experiments on 1B, 3B, 7B and 70B Llama-like models demonstrate that SNIP consistently outperforms existing baselines, reducing FLOPs by up to 80% while preserving model quality across different model sizes and training phases with minimal computational overhead.

</details>


### [710] [Semi-supervised CAPP Transformer Learning via Pseudo-labeling](https://arxiv.org/abs/2602.01419)
*Dennis Gross,Helge Spieker,Arnaud Gotlieb,Emmanuel Stathatos,Panorios Benardos,George-Christopher Vosniakos*

Main category: cs.LG

TL;DR: 提出半监督学习方法改进基于Transformer的CAPP模型，无需人工标注，在数据稀缺的制造环境中提升模型泛化能力


<details>
  <summary>Details</summary>
Motivation: 工业中CAPP数据集有限，导致模型泛化能力不足。需要解决数据稀缺环境下模型性能提升的问题

Method: 使用半监督学习方法：训练一个oracle模型来筛选正确预测，然后将这些预测用于一次性重新训练Transformer模型

Result: 在小规模数据集上的实验显示，该方法相比基线模型获得了一致的准确率提升，证明了在数据稀缺制造环境中的有效性

Conclusion: 提出的半监督学习方法能够有效提升CAPP模型的性能，无需额外人工标注，适用于工业数据稀缺场景

Abstract: High-level Computer-Aided Process Planning (CAPP) generates manufacturing process plans from part specifications. It suffers from limited dataset availability in industry, reducing model generalization. We propose a semi-supervised learning approach to improve transformer-based CAPP transformer models without manual labeling. An oracle, trained on available transformer behaviour data, filters correct predictions from unseen parts, which are then used for one-shot retraining. Experiments on small-scale datasets with simulated ground truth across the full data distribution show consistent accuracy gains over baselines, demonstrating the method's effectiveness in data-scarce manufacturing environments.

</details>


### [711] [Improve the Trade-off Between Watermark Strength and Speculative Sampling Efficiency for Language Models](https://arxiv.org/abs/2602.01428)
*Weiqing He,Xiang Li,Li Shen,Weijie Su,Qi Long*

Main category: cs.LG

TL;DR: 提出一种在推测采样中保持最大水印强度的方法，解决了水印强度与采样效率之间的权衡问题，实现了高效且可追溯的LLM输出


<details>
  <summary>Details</summary>
Motivation: 水印技术可用于追踪大语言模型输出的来源，但在实际部署中面临推理效率低下的问题。推测采样能加速推理，但现有研究表明水印强度与推测采样接受率之间存在根本性权衡，阻碍了二者的同时优化

Method: 1. 引入定量衡量水印强度的指标，该指标控制统计可检测性，并在令牌是伪随机数的确定性函数时达到最大；2. 将权衡问题形式化为约束优化问题，为两种现有水印方案推导出明确的帕累托曲线；3. 提出一种原则性机制，将伪随机性注入草稿令牌接受过程，确保最大水印强度同时保持推测采样效率

Result: 实验表明该方法在不牺牲效率的情况下提高了可检测性，揭示了推测采样与水印技术统一的原则，为二者的高效实用部署铺平道路

Conclusion: 水印强度与推测采样效率之间的权衡并非绝对，通过将伪随机性注入草稿令牌接受过程，可以在保持最大水印强度的同时维持推测采样效率，实现高效且可追溯的LLM输出

Abstract: Watermarking is a principled approach for tracing the provenance of large language model (LLM) outputs, but its deployment in practice is hindered by inference inefficiency. Speculative sampling accelerates inference, with efficiency improving as the acceptance rate between draft and target models increases. Yet recent work reveals a fundamental trade-off: higher watermark strength reduces acceptance, preventing their simultaneous achievement. We revisit this trade-off and show it is not absolute. We introduce a quantitative measure of watermark strength that governs statistical detectability and is maximized when tokens are deterministic functions of pseudorandom numbers. Using this measure, we fully characterize the trade-off as a constrained optimization problem and derive explicit Pareto curves for two existing watermarking schemes. Finally, we introduce a principled mechanism that injects pseudorandomness into draft-token acceptance, ensuring maximal watermark strength while maintaining speculative sampling efficiency. Experiments further show that this approach improves detectability without sacrificing efficiency. Our findings uncover a principle that unites speculative sampling and watermarking, paving the way for their efficient and practical deployment.

</details>


### [712] [Phase Transitions for Feature Learning in Neural Networks](https://arxiv.org/abs/2602.01434)
*Andrea Montanari,Zihao Wang*

Main category: cs.LG

TL;DR: 该论文研究两层神经网络在多索引模型中的特征学习动态，确定了梯度下降算法能够成功学习潜在特征的数据量阈值δ_NN，揭示了训练过程中梯度主导和Hessian主导两个阶段的动态特性。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络如何通过梯度下降学习数据中的低维表示，特别是在多索引模型设置下，理解特征学习的动态过程及其与数据量、网络架构的关系。

Method: 在比例渐近框架下（n,d→∞，n/d→δ），分析两层神经网络的梯度下降动态，其中潜在空间维度k和隐藏神经元数m固定。通过研究经验风险梯度和Hessian矩阵的谱特性来确定特征学习的阈值。

Result: 推导出两层神经网络的特征学习阈值δ_NN，该阈值对应于Hessian矩阵谱的相变点。训练过程分为两个阶段：首先梯度主导学习梯度方向，然后Hessian主导阶段决定最终学习效果。

Conclusion: 神经网络的特征学习存在明确的阈值δ_NN，该阈值由训练动态中的梯度-Hessian相互作用决定，为研究不同网络架构和训练算法对学习动态的影响提供了理论基础。

Abstract: According to a popular viewpoint, neural networks learn from data by first identifying low-dimensional representations, and subsequently fitting the best model in this space. Recent works provide a formalization of this phenomenon when learning multi-index models. In this setting, we are given $n$ i.i.d. pairs $({\boldsymbol x}_i,y_i)$, where the covariate vectors ${\boldsymbol x}_i\in\mathbb{R}^d$ are isotropic, and responses $y_i$ only depend on ${\boldsymbol x}_i$ through a $k$-dimensional projection ${\boldsymbol Θ}_*^{\sf T}{\boldsymbol x}_i$. Feature learning amounts to learning the latent space spanned by ${\boldsymbol Θ}_*$.
  In this context, we study the gradient descent dynamics of two-layer neural networks under the proportional asymptotics $n,d\to\infty$, $n/d\toδ$, while the dimension of the latent space $k$ and the number of hidden neurons $m$ are kept fixed. Earlier work establishes that feature learning via polynomial-time algorithms is possible if $δ> δ_{\text{alg}}$, for $δ_{\text{alg}}$ a threshold depending on the data distribution, and is impossible (within a certain class of algorithms) below $δ_{\text{alg}}$. Here we derive an analogous threshold $δ_{\text{NN}}$ for two-layer networks. Our characterization of $δ_{\text{NN}}$ opens the way to study the dependence of learning dynamics on the network architecture and training algorithm.
  The threshold $δ_{\text{NN}}$ is determined by the following scenario. Training first visits points for which the gradient of the empirical risk is large and learns the directions spanned by these gradients. Then the gradient becomes smaller and the dynamics becomes dominated by negative directions of the Hessian. The threshold $δ_{\text{NN}}$ corresponds to a phase transition in the spectrum of the Hessian in this second phase.

</details>


### [713] [TQL: Scaling Q-Functions with Transformers by Preventing Attention Collapse](https://arxiv.org/abs/2602.01439)
*Perry Dong,Kuo-Han Hung,Alexander Swerdlow,Dorsa Sadigh,Chelsea Finn*

Main category: cs.LG

TL;DR: 本文提出Transformer Q-Learning (TQL)方法，通过控制注意力分数熵来稳定训练，解决了Transformer在强化学习价值函数中难以扩展的问题。


<details>
  <summary>Details</summary>
Motivation: 尽管机器学习领域因规模扩大而取得显著进展，但强化学习方法仍主要使用小型价值函数。直接将Transformer架构用于价值函数扩展通常会导致学习不稳定和性能下降。本文旨在探究阻碍Transformer在价值函数中有效扩展的根本原因。

Method: 通过实证分析发现关键失败模式：注意力分数随容量增加而崩溃。核心见解是通过控制注意力分数的熵来防止崩溃并稳定训练。基于此提出Transformer Q-Learning (TQL)方法，解锁Transformer在强化学习价值函数中的扩展潜力。

Result: TQL方法在从最小到最大网络规模扩展时，性能提升高达43%，而先前方法则出现性能下降。

Conclusion: 通过控制注意力分数熵，Transformer可以在强化学习价值函数中有效扩展，TQL方法成功解决了Transformer在RL中扩展的稳定性问题，实现了显著的性能提升。

Abstract: Despite scale driving substantial recent advancements in machine learning, reinforcement learning (RL) methods still primarily use small value functions. Naively scaling value functions -- including with a transformer architecture, which is known to be highly scalable -- often results in learning instability and worse performance. In this work, we ask what prevents transformers from scaling effectively for value functions? Through empirical analysis, we identify the critical failure mode in this scaling: attention scores collapse as capacity increases. Our key insight is that we can effectively prevent this collapse and stabilize training by controlling the entropy of the attention scores, thereby enabling the use of larger models. To this end, we propose Transformer Q-Learning (TQL), a method that unlocks the scaling potential of transformers in learning value functions in RL. Our approach yields up to a 43% improvement in performance when scaling from the smallest to the largest network sizes, while prior methods suffer from performance degradation.

</details>


### [714] [A Meta-Knowledge-Augmented LLM Framework for Hyperparameter Optimization in Time-Series Forecasting](https://arxiv.org/abs/2602.01445)
*Ons Saadallah,Mátyás andó,Tamás Gábor Orosz*

Main category: cs.LG

TL;DR: LLM-AutoOpt：结合贝叶斯优化与LLM推理的混合超参数优化框架，用于时间序列预测，提升性能与可解释性


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化在超参数调优中计算成本高、可解释性差，特别是对于时间序列预测任务。LLM的发展为将结构化先验知识和推理融入优化流程提供了新机会。

Method: 提出LLM-AutoOpt混合框架，将贝叶斯优化与基于LLM的上下文推理结合。框架将数据集元特征、模型描述、历史优化结果和目标目标编码为LLM提示中的结构化元知识，使用BO初始化搜索并缓解冷启动问题。

Result: 在多变量时间序列预测基准测试中，LLM-AutoOpt相比纯BO和没有元知识的LLM基线，实现了更好的预测性能和更可解释的优化行为。

Conclusion: LLM-AutoOpt通过结合贝叶斯优化和LLM推理，为超参数优化提供了更高效、可解释的解决方案，特别适用于时间序列预测任务。

Abstract: Hyperparameter optimization (HPO) plays a central role in the performance of deep learning models, yet remains computationally expensive and difficult to interpret, particularly for time-series forecasting. While Bayesian Optimization (BO) is a standard approach, it typically treats tuning tasks independently and provides limited insight into its decisions. Recent advances in large language models (LLMs) offer new opportunities to incorporate structured prior knowledge and reasoning into optimization pipelines. We introduce LLM-AutoOpt, a hybrid HPO framework that combines BO with LLM-based contextual reasoning. The framework encodes dataset meta-features, model descriptions, historical optimization outcomes, and target objectives as structured meta-knowledge within LLM prompts, using BO to initialize the search and mitigate cold-start effects. This design enables context-aware and stable hyperparameter refinement while exposing the reasoning behind optimization decisions. Experiments on a multivariate time series forecasting benchmark demonstrate that LLM-AutoOpt achieves improved predictive performance and more interpretable optimization behavior compared to BO and LLM baselines without meta-knowledge.

</details>


### [715] [Provable Cooperative Multi-Agent Exploration for Reward-Free MDPs](https://arxiv.org/abs/2602.01453)
*Idan Barnea,Orin Levy,Yishay Mansour*

Main category: cs.LG

TL;DR: 研究多智能体在无奖励探索下的合作强化学习，重点分析学习阶段数与智能体数量之间的权衡关系，发现当学习阶段数等于环境时域H时，仅需多项式数量的智能体即可高效学习动态模型。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体强化学习中的无奖励探索问题，多个智能体需要协作探索未知MDP以学习其动态模型（不观察奖励）。主要目标是刻画学习阶段数量与智能体数量之间的权衡关系，特别是在学习阶段数较少的情况下。

Method: 采用分阶段学习框架，每个学习阶段中多个智能体独立与环境交互。每个智能体被分配一个策略，执行该策略并观察生成的轨迹。研究重点是在表格化有限时域MDP中分析阶段数与智能体数量的关系。

Result: 发现一个由时域H控制的尖锐转变：当学习阶段数等于H时，提出一个计算高效的算法，仅需Õ(S⁶H⁶A/ε²)个智能体即可获得动态模型的ε近似。同时给出下界证明：任何限制在ρ<H阶段的算法至少需要A^(H/ρ)个智能体才能达到常数精度。

Conclusion: 如果要将智能体数量限制在多项式级别，则必须要有大约H个学习阶段。这揭示了多智能体无奖励探索中阶段数与智能体数量之间的基本权衡关系。

Abstract: We study cooperative multi-agent reinforcement learning in the setting of reward-free exploration, where multiple agents jointly explore an unknown MDP in order to learn its dynamics (without observing rewards). We focus on a tabular finite-horizon MDP and adopt a phased learning framework. In each learning phase, multiple agents independently interact with the environment. More specifically, in each learning phase, each agent is assigned a policy, executes it, and observes the resulting trajectory. Our primary goal is to characterize the tradeoff between the number of learning phases and the number of agents, especially when the number of learning phases is small.
  Our results identify a sharp transition governed by the horizon $H$. When the number of learning phases equals $H$, we present a computationally efficient algorithm that uses only $\tilde{O}(S^6 H^6 A / ε^2)$ agents to obtain an $ε$ approximation of the dynamics (i.e., yields an $ε$-optimal policy for any reward function). We complement our algorithm with a lower bound showing that any algorithm restricted to $ρ< H$ phases requires at least $A^{H/ρ}$ agents to achieve constant accuracy. Thus, we show that it is essential to have an order of $H$ learning phases if we limit the number of agents to be polynomial.

</details>


### [716] [Modeling Topological Impact on Node Attribute Distributions in Attributed Graphs](https://arxiv.org/abs/2602.01454)
*Amirreza Shiralinasab Langari,Leila Yeganeh,Kim Khoa Nguyen*

Main category: cs.LG

TL;DR: 该论文提出了一种代数方法，将图拓扑与节点属性分布结合，形成拓扑影响的分布，用于图异常检测任务。


<details>
  <summary>Details</summary>
Motivation: 研究图拓扑如何影响节点属性的分布，将拓扑和属性视为结构不同但相互作用的组件，提供新的分析视角。

Method: 引入代数方法结合图拓扑与节点属性概率分布；建立范畴框架形式化节点对拓扑的感知；量化节点视角并与属性分布整合；构建拓扑条件分布作为后验近似。

Result: 建立了充分性条件：在完全图上拓扑无信息结构时恢复原始属性分布；设计了简单的测试模型ID，并在无监督图异常检测任务中验证方法。

Conclusion: 提出的框架能够形式化拓扑对节点属性分布的影响，为图数据分析提供了新的理论工具，并在异常检测任务中展示了实用性。

Abstract: We investigate how the topology of attributed graphs influences the distribution of node attributes. This work offers a novel perspective by treating topology and attributes as structurally distinct but interacting components. We introduce an algebraic approach that combines a graph's topology with the probability distribution of node attributes, resulting in topology-influenced distributions. First, we develop a categorical framework to formalize how a node perceives the graph's topology. We then quantify this point of view and integrate it with the distribution of node attributes to capture topological effects. We interpret these topology-conditioned distributions as approximations of the posteriors $P(\cdot \mid v)$ and $P(\cdot \mid \mathcal{G})$.
  We further establish a principled sufficiency condition by showing that, on complete graphs, where topology carries no informative structure, our construction recovers the original attribute distribution. To evaluate our approach, we introduce an intentionally simple testbed model, $\textbf{ID}$, and use unsupervised graph anomaly detection as a probing task.

</details>


### [717] [Rectified LpJEPA: Joint-Embedding Predictive Architectures with Sparse and Maximum-Entropy Representations](https://arxiv.org/abs/2602.01456)
*Yilun Kuang,Yash Dagade,Tim G. J. Rudner,Randall Balestriero,Yann LeCun*

Main category: cs.LG

TL;DR: 该论文提出了一种新的正则化方法RDMReg，通过将表征对齐到Rectified Generalized Gaussian分布来学习稀疏、非负的表征，解决了现有JEPA方法倾向于密集表征的问题。


<details>
  <summary>Details</summary>
Motivation: 现有Joint-Embedding Predictive Architectures (JEPA)方法通常将表征正则化为各向同性高斯分布，但这倾向于产生密集表征，无法捕捉高效表征中观察到的稀疏性这一关键特性。

Method: 提出了Rectified Distribution Matching Regularization (RDMReg)，这是一种切片双样本分布匹配损失，将表征对齐到Rectified Generalized Gaussian (RGG)分布。RGG通过整流操作实现对期望ℓ0范数的显式控制，同时在期望ℓp范数约束下保持最大熵特性。将RDMReg应用于JEPA得到Rectified LpJEPA。

Result: Rectified LpJEPA学习到了稀疏、非负的表征，在稀疏性与性能之间取得了良好的权衡，在图像分类基准测试中表现出竞争力的下游性能。

Conclusion: RDMReg能有效强制表征稀疏性，同时保留任务相关信息，Rectified LpJEPA严格泛化了先前基于高斯的JEPA方法。

Abstract: Joint-Embedding Predictive Architectures (JEPA) learn view-invariant representations and admit projection-based distribution matching for collapse prevention. Existing approaches regularize representations towards isotropic Gaussian distributions, but inherently favor dense representations and fail to capture the key property of sparsity observed in efficient representations. We introduce Rectified Distribution Matching Regularization (RDMReg), a sliced two-sample distribution-matching loss that aligns representations to a Rectified Generalized Gaussian (RGG) distribution. RGG enables explicit control over expected $\ell_0$ norm through rectification, while preserving maximum-entropy up to rescaling under expected $\ell_p$ norm constraints. Equipping JEPAs with RDMReg yields Rectified LpJEPA, which strictly generalizes prior Gaussian-based JEPAs. Empirically, Rectified LpJEPA learns sparse, non-negative representations with favorable sparsity-performance trade-offs and competitive downstream performance on image classification benchmarks, demonstrating that RDMReg effectively enforces sparsity while preserving task-relevant information.

</details>


### [718] [P-EAGLE: Parallel-Drafting EAGLE with Scalable Training](https://arxiv.org/abs/2602.01469)
*Mude Hui,Xin Huang,Jaime Campos Salas,Yue Sun,Nathan Pemberton,Xiang Song,Ashish Khetan,George Karypis*

Main category: cs.LG

TL;DR: P-EAGLE将EAGLE从自回归转换为并行多token预测，通过可学习的共享隐藏状态实现，解决了长上下文训练复杂度问题，在多个大模型上获得1.10-1.36倍加速。


<details>
  <summary>Details</summary>
Motivation: 推理大语言模型产生更长输出，需要训练在扩展序列上的推测解码草稿器。并行草稿（每次前向传播预测多个token）相比顺序生成具有延迟优势，但训练复杂度随序列长度和并行位置乘积呈二次方增长，使得长上下文训练不切实际。

Method: 提出P-EAGLE，通过可学习的共享隐藏状态将EAGLE从自回归转换为并行多token预测。为扩展到长上下文训练，开发了包含注意力掩码预计算和序列分区技术的框架，支持在单个序列内进行梯度累积的并行预测训练。

Result: 在vLLM中实现P-EAGLE，在GPT-OSS 120B、20B和Qwen3-Coder 30B模型上相比自回归EAGLE-3获得1.10-1.36倍的加速。

Conclusion: P-EAGLE成功解决了并行草稿训练中的长上下文扩展问题，通过创新的训练框架实现了高效的并行多token预测，显著提升了推理速度。

Abstract: Reasoning LLMs produce longer outputs, requiring speculative decoding drafters trained on extended sequences. Parallel drafting - predicting multiple tokens per forward pass - offers latency benefits over sequential generation, but training complexity scales quadratically with the product of sequence length and parallel positions, rendering long-context training impractical. We present P(arallel)-EAGLE, which transforms EAGLE from autoregressive to parallel multi-token prediction via a learnable shared hidden state. To scale training to long contexts, we develop a framework featuring attention mask pre-computation and sequence partitioning techniques, enabling gradient accumulation within individual sequences for parallel-prediction training. We implement P-EAGLE in vLLM and demonstrate speedups of 1.10-1.36x over autoregressive EAGLE-3 across GPT-OSS 120B, 20B, and Qwen3-Coder 30B.

</details>


### [719] [Causal Preference Elicitation](https://arxiv.org/abs/2602.01483)
*Edwin V. Bonilla,He Zhao,Daniel M. Steinberg*

Main category: cs.LG

TL;DR: 提出因果偏好获取框架，通过主动查询专家对局部边关系的判断来加速因果图后验集中


<details>
  <summary>Details</summary>
Motivation: 现有因果发现方法通常仅依赖观测数据，专家知识整合不足。需要开发能有效结合专家判断的主动学习框架，在有限查询预算下提高因果图恢复精度

Method: 提出贝叶斯框架，从任意黑盒观测后验出发，通过三向似然函数建模专家对边存在性和方向的噪声判断。使用粒子近似进行后验推断，基于专家分类响应的期望信息增益准则选择查询

Result: 在合成图、蛋白质信号数据和人类基因扰动基准测试中，该方法在有限查询预算下实现了更快的后验集中和更好的有向效应恢复效果

Conclusion: 因果偏好获取框架有效整合专家知识，通过主动查询策略显著提升因果发现效率，为专家参与的因果推断提供了系统化解决方案

Abstract: We propose causal preference elicitation, a Bayesian framework for expert-in-the-loop causal discovery that actively queries local edge relations to concentrate a posterior over directed acyclic graphs (DAGs). From any black-box observational posterior, we model noisy expert judgments with a three-way likelihood over edge existence and direction. Posterior inference uses a flexible particle approximation, and queries are selected by an efficient expected information gain criterion on the expert's categorical response. Experiments on synthetic graphs, protein signaling data, and a human gene perturbation benchmark show faster posterior concentration and improved recovery of directed effects under tight query budgets.

</details>


### [720] [Multi-Scale Wavelet Transformers for Operator Learning of Dynamical Systems](https://arxiv.org/abs/2602.01486)
*Xuesong Wang,Michael Groom,Rafael Oliveira,He Zhao,Terence O'Kane,Edwin V. Bonilla*

Main category: cs.LG

TL;DR: 提出多尺度小波变换器（MSWT），通过在token化小波域学习系统动力学，解决神经算子等模型存在的频谱偏差问题，显著提升长期预测稳定性和频谱保真度。


<details>
  <summary>Details</summary>
Motivation: 许多基于机器学习的动态系统代理模型（如神经算子）存在频谱偏差，会衰减高频成分，而这些高频成分通常编码了小尺度结构。在天气预报等应用中，这种限制尤其有害，因为错误表示的高频会导致长期预测不稳定。

Method: 提出多尺度小波变换器（MSWT），在token化小波域学习系统动力学。小波变换显式地将低频和高频内容分离到不同尺度。MSWT采用保留小波特征的降采样方案来保持高频特征，并使用基于小波的注意力机制来捕捉跨尺度和频带的依赖关系。

Result: 在混沌动态系统实验中，MSWT显著减少了误差并改善了长期频谱保真度。在ERA5气候再分析数据集上，MSWT进一步减少了气候学偏差，展示了其在真实世界预测场景中的有效性。

Conclusion: MSWT通过在小波域中学习动态系统，有效解决了机器学习模型的频谱偏差问题，提高了长期预测的稳定性和准确性，在气候预测等实际应用中表现出优越性能。

Abstract: Recent years have seen a surge in data-driven surrogates for dynamical systems that can be orders of magnitude faster than numerical solvers. However, many machine learning-based models such as neural operators exhibit spectral bias, attenuating high-frequency components that often encode small-scale structure. This limitation is particularly damaging in applications such as weather forecasting, where misrepresented high frequencies can induce long-horizon instability. To address this issue, we propose multi-scale wavelet transformers (MSWTs), which learn system dynamics in a tokenized wavelet domain. The wavelet transform explicitly separates low- and high-frequency content across scales. MSWTs leverage a wavelet-preserving downsampling scheme that retains high-frequency features and employ wavelet-based attention to capture dependencies across scales and frequency bands. Experiments on chaotic dynamical systems show substantial error reductions and improved long horizon spectral fidelity. On the ERA5 climate reanalysis, MSWTs further reduce climatological bias, demonstrating their effectiveness in a real-world forecasting setting.

</details>


### [721] [OpInf-LLM: Parametric PDE Solving with LLMs via Operator Inference](https://arxiv.org/abs/2602.01493)
*Zhuoyuan Wang,Hanjiang Hu,Xiyu Deng,Saviz Mowlavi,Yorie Nakahira*

Main category: cs.LG

TL;DR: OpInf-LLM：基于算子推断的LLM参数化PDE求解框架，利用少量解数据准确预测多样PDE实例，包括未见参数和配置，实现高执行成功率。


<details>
  <summary>Details</summary>
Motivation: 传统LLM在求解偏微分方程时面临执行成功率与数值精度之间的权衡，特别是在泛化到未见参数和边界条件时存在挑战。需要一种既能保持高执行成功率又能保证数值精度的PDE求解方法。

Method: 提出OpInf-LLM框架，结合算子推断与大型语言模型能力。利用少量解数据进行算子推断，实现参数化PDE求解，并通过统一工具接口与LLM自然语言任务指定无缝集成。

Result: 该框架能够准确预测包括未见参数和配置在内的多样PDE实例，在异构设置下实现高执行成功率，同时保持低计算需求。

Conclusion: OpInf-LLM通过结合算子推断与LLM能力，为基于LLM的PDE求解开辟了可泛化降阶建模的新可能性，解决了执行成功率与数值精度之间的权衡问题。

Abstract: Solving diverse partial differential equations (PDEs) is fundamental in science and engineering. Large language models (LLMs) have demonstrated strong capabilities in code generation, symbolic reasoning, and tool use, but reliably solving PDEs across heterogeneous settings remains challenging. Prior work on LLM-based code generation and transformer-based foundation models for PDE learning has shown promising advances. However, a persistent trade-off between execution success rate and numerical accuracy arises, particularly when generalization to unseen parameters and boundary conditions is required. In this work, we propose OpInf-LLM, an LLM parametric PDE solving framework based on operator inference. The proposed framework leverages a small amount of solution data to enable accurate prediction of diverse PDE instances, including unseen parameters and configurations, and provides seamless integration with LLMs for natural language specification of PDE solving tasks. Its low computational demands and unified tool interface further enable a high execution success rate across heterogeneous settings. By combining operator inference with LLM capabilities, OpInf-LLM opens new possibilities for generalizable reduced-order modeling in LLM-based PDE solving.

</details>


### [722] [Enhancing Generalization in Evolutionary Feature Construction for Symbolic Regression through Vicinal Jensen Gap Minimization](https://arxiv.org/abs/2602.01510)
*Hengzhe Zhang,Qi Chen,Bing Xue,Wolfgang Banzhaf,Mengjie Zhang*

Main category: cs.LG

TL;DR: 该论文提出了一种基于遗传编程的特征构建框架，通过联合优化经验风险和vicinal Jensen gap来控制过拟合，并引入了噪声估计和流形入侵检测机制，在58个数据集上取得了优越性能。


<details>
  <summary>Details</summary>
Motivation: 遗传编程特征构建虽然成功，但过拟合问题限制了其更广泛应用。需要改进泛化能力，控制过拟合。

Method: 1) 证明vicinal风险可通过经验风险加正则化项（有限差分或vicinal Jensen gap）界定；2) 提出进化特征构建框架，联合优化经验风险和vicinal Jensen gap；3) 开发噪声估计策略动态调整正则化强度；4) 提出流形入侵检测机制防止生成不现实样本。

Result: 在58个数据集上的实验表明，Jensen gap最小化比其他复杂度度量更有效。与15种机器学习算法比较，提出的过拟合控制策略使遗传编程获得了优越性能。

Conclusion: 通过理论分析和提出的过拟合控制策略，成功解决了遗传编程特征构建中的过拟合问题，提高了泛化能力，为自动化机器学习提供了有效方法。

Abstract: Genetic programming-based feature construction has achieved significant success in recent years as an automated machine learning technique to enhance learning performance. However, overfitting remains a challenge that limits its broader applicability. To improve generalization, we prove that vicinal risk, estimated through noise perturbation or mixup-based data augmentation, is bounded by the sum of empirical risk and a regularization term-either finite difference or the vicinal Jensen gap. Leveraging this decomposition, we propose an evolutionary feature construction framework that jointly optimizes empirical risk and the vicinal Jensen gap to control overfitting. Since datasets may vary in noise levels, we develop a noise estimation strategy to dynamically adjust regularization strength. Furthermore, to mitigate manifold intrusion-where data augmentation may generate unrealistic samples that fall outside the data manifold-we propose a manifold intrusion detection mechanism. Experimental results on 58 datasets demonstrate the effectiveness of Jensen gap minimization compared to other complexity measures. Comparisons with 15 machine learning algorithms further indicate that genetic programming with the proposed overfitting control strategy achieves superior performance.

</details>


### [723] [You Need an Encoder for Native Position-Independent Caching](https://arxiv.org/abs/2602.01519)
*Shiju Zhao,Junhao Hu,Jiaqi Zheng,Guihai Chen*

Main category: cs.LG

TL;DR: 提出原生位置无关缓存(PIC)方法COMB，通过为仅解码器LLM重新引入编码器并显式训练来支持PIC，显著降低首词延迟并提升吞吐量


<details>
  <summary>Details</summary>
Motivation: 现有基于前缀的KV缓存处理任意顺序检索的上下文效率低下，而现有PIC方法往往导致显著的准确度下降，限制了实际应用

Method: 提出原生PIC方法：为流行的仅解码器LLM重新引入编码器并显式训练以支持PIC；开发COMB系统，这是一个与现有推理框架无缝集成的PIC感知缓存系统

Result: COMB将首词时间(TTFT)降低51-94%，吞吐量提升3倍且保持可比较的准确度；在DeepSeek-V2-Lite-Chat上的质量改进证明了COMB对其他类型仅解码器LLM的适用性

Conclusion: 通过原生PIC方法COMB有效解决了传统KV缓存处理任意顺序上下文的效率问题，在保持准确度的同时显著提升了推理性能

Abstract: The Key-Value (KV) cache of Large Language Models (LLMs) is prefix-based, making it highly inefficient for processing contexts retrieved in arbitrary order. Position-Independent Caching (PIC) has been proposed to enable KV reuse without positional constraints; however, existing approaches often incur substantial accuracy degradation, limiting their practical adoption. To address this issue, we propose native PIC by reintroducing the encoder to prevalent decoder-only LLMs and explicitly training it to support PIC. We further develop COMB, a PIC-aware caching system that integrates seamlessly with existing inference frameworks. Experimental results show that COMB reduces Time-to-First-Token (TTFT) by 51-94% and increases throughput by 3$\times$ with comparable accuracy. Furthermore, the quality improvement when using DeepSeek-V2-Lite-Chat demonstrates the applicability of COMB to other types of decoder-only LLMs. Our code is available at https://github.com/shijuzhao/Comb.

</details>


### [724] [When Is Rank-1 Enough? Geometry-Guided Initialization for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2602.01522)
*Haoran Zhao,Soyeon Caren Han,Eduard Hovy*

Main category: cs.LG

TL;DR: 论文提出Gap-Init初始化方法，通过将rank-1 LoRA方向与模态间隙向量对齐，解决了极低秩PEFT训练不稳定的问题，在多个视觉语言任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 参数高效微调(PEFT)是多模态大语言模型适配的标准方法，但极低秩设置（特别是rank-1 LoRA）通常不稳定。作者发现这种不稳定性不仅源于有限容量，更源于优化对更新方向的高度敏感性。

Method: 提出Gap-Init初始化方法：1) 分析预训练表示，识别主导早期梯度流的模态间隙轴；2) 使用小型校准集估计模态间隙向量；3) 将rank-1 LoRA方向与该向量对齐，同时保持初始LoRA更新为零。

Result: 在多个视觉语言任务和骨干网络上，Gap-Init始终稳定rank-1训练，能够匹配甚至超越强大的rank-8基线。结果表明在极端低秩限制下，初始对齐与秩本身同样重要。

Conclusion: 极低秩PEFT训练的不稳定性源于预训练视觉和文本特征形成的失配各向异性区域，导致主导的"间隙"方向。通过几何感知的初始化对齐该方向，可以显著改善训练稳定性，为极低秩微调提供了新视角。

Abstract: Parameter-efficient fine-tuning (PEFT) is a standard way to adapt multimodal large language models, yet extremely low-rank settings -- especially rank-1 LoRA -- are often unstable. We show that this instability is not solely due to limited capacity: in the rank-1 regime, optimization is highly sensitive to the update direction. Concretely, pretrained vision and text features form mismatched anisotropic regions, yielding a dominant "gap" direction that acts like a translation component and disproportionately steers early gradients under rank-1 constraints. Analyzing pretrained representations, we identify a modality-gap axis that dominates early gradient flow, while a random rank-1 initialization is unlikely to align with it, leading to weak gradients and training collapse. We propose Gap-Init, a geometry-aware initialization that aligns the rank-1 LoRA direction with an estimated modality-gap vector from a small calibration set, while keeping the initial LoRA update zero. Across multiple vision-language tasks and backbones, Gap-Init consistently stabilizes rank-1 training and can match or outperform strong rank-8 baselines. Our results suggest that at the extreme low-rank limit, initial alignment can matter as much as rank itself.

</details>


### [725] [The Inlet Rank Collapse in Implicit Neural Representations: Diagnosis and Unified Remedy](https://arxiv.org/abs/2602.01526)
*Jianqiao Zheng,Hemanth Saratchandran,Simon Lucey*

Main category: cs.LG

TL;DR: 本文提出了一种结构诊断框架，通过层级的NTK分解识别出"入口秩塌陷"现象，并基于此推导出秩扩展初始化方法，使标准MLP能够实现高保真重建。


<details>
  <summary>Details</summary>
Motivation: 隐式神经表示（INRs）在连续信号建模方面取得了革命性进展，但在有限训练预算下难以恢复细粒度细节。虽然经验技术（如位置编码、正弦激活、批量归一化）能有效缓解这一问题，但它们的理论解释大多是后验的，主要关注修改后的全局NTK谱。本文旨在从结构角度提供统一的理论解释。

Method: 提出了一个结构诊断框架，通过层级分解神经正切核（NTK），数学上识别出"入口秩塌陷"现象：低维输入坐标无法跨越高维嵌入空间，在第一层形成基本的秩缺陷，成为整个网络的表达瓶颈。基于此诊断，推导出秩扩展初始化方法，确保表示秩随层宽扩展，无需架构修改或计算开销。

Result: 该框架为位置编码、正弦激活和批量归一化提供了统一的秩恢复视角。秩扩展初始化方法使标准MLP能够实现高保真重建，证明增强INRs的关键在于初始秩传播的结构优化，以有效填充潜在空间。

Conclusion: 本文通过结构诊断框架揭示了INRs表达瓶颈的根本原因——入口秩塌陷，并提出了一种原则性的秩扩展初始化方法。该方法表明，通过优化初始秩传播结构，标准MLP就能实现高保真重建，为INRs的理论理解和实践改进提供了新视角。

Abstract: Implicit Neural Representations (INRs) have revolutionized continuous signal modeling, yet they struggle to recover fine-grained details within finite training budgets. While empirical techniques, such as positional encoding (PE), sinusoidal activations (SIREN), and batch normalization (BN), effectively mitigate this, their theoretical justifications are predominantly post hoc, focusing on the global NTK spectrum only after modifications are applied. In this work, we reverse this paradigm by introducing a structural diagnostic framework. By performing a layer-wise decomposition of the NTK, we mathematically identify the ``Inlet Rank Collapse'': a phenomenon where the low-dimensional input coordinates fail to span the high-dimensional embedding space, creating a fundamental rank deficiency at the first layer that acts as an expressive bottleneck for the entire network. This framework provides a unified perspective to re-interpret PE, SIREN, and BN as different forms of rank restoration. Guided by this diagnosis, we derive a Rank-Expanding Initialization, a minimalist remedy that ensures the representation rank scales with the layer width without architectural modifications or computational overhead. Our results demonstrate that this principled remedy enables standard MLPs to achieve high-fidelity reconstructions, proving that the key to empowering INRs lies in the structural optimization of the initial rank propagation to effectively populate the latent space.

</details>


### [726] [Plain Transformers are Surprisingly Powerful Link Predictors](https://arxiv.org/abs/2602.01553)
*Quang Truong,Yu Song,Donald Loveland,Mingxuan Ju,Tong Zhao,Neil Shah,Jiliang Tang*

Main category: cs.LG

TL;DR: PENCIL是一个基于纯Transformer的链接预测模型，通过注意力机制处理采样子图，无需手工先验知识，在保持可扩展性的同时超越了传统GNN和启发式方法。


<details>
  <summary>Details</summary>
Motivation: 当前图链接预测方法存在局限性：GNN依赖显式结构启发式或内存密集型节点嵌入，难以泛化和扩展到大规模图；图Transformer则因复杂结构编码带来显著开销。需要一种既能捕获丰富拓扑依赖又保持可扩展性的新方法。

Method: 提出PENCIL模型，采用仅编码器的纯Transformer架构，用注意力机制处理采样的局部子图，替代手工设计的先验知识。该方法保持了标准Transformer的可扩展性和硬件效率，能够隐式泛化多种启发式和基于子图的表达能力。

Result: PENCIL在实验中超越了启发式增强的GNN，比基于ID嵌入的方法参数效率更高，在不同基准测试中保持竞争力，即使在没有节点特征的情况下也能取得良好效果。理论分析表明PENCIL能提取比GNN更丰富的结构信号。

Conclusion: 研究挑战了当前依赖复杂工程技术的主流范式，证明简单的设计选择可能足以实现相同的链接预测能力。PENCIL展示了纯Transformer架构在图机器学习中的潜力，为大规模图链接预测提供了高效可扩展的解决方案。

Abstract: Link prediction is a core challenge in graph machine learning, demanding models that capture rich and complex topological dependencies. While Graph Neural Networks (GNNs) are the standard solution, state-of-the-art pipelines often rely on explicit structural heuristics or memory-intensive node embeddings -- approaches that struggle to generalize or scale to massive graphs. Emerging Graph Transformers (GTs) offer a potential alternative but often incur significant overhead due to complex structural encodings, hindering their applications to large-scale link prediction. We challenge these sophisticated paradigms with PENCIL, an encoder-only plain Transformer that replaces hand-crafted priors with attention over sampled local subgraphs, retaining the scalability and hardware efficiency of standard Transformers. Through experimental and theoretical analysis, we show that PENCIL extracts richer structural signals than GNNs, implicitly generalizing a broad class of heuristics and subgraph-based expressivity. Empirically, PENCIL outperforms heuristic-informed GNNs and is far more parameter-efficient than ID-embedding--based alternatives, while remaining competitive across diverse benchmarks -- even without node features. Our results challenge the prevailing reliance on complex engineering techniques, demonstrating that simple design choices are potentially sufficient to achieve the same capabilities.

</details>


### [727] [InfoTok: Regulating Information Flow for Capacity-Constrained Shared Visual Tokenization in Unified MLLMs](https://arxiv.org/abs/2602.01554)
*Lv Tang,Tianyi Zheng,Bo Li,Xingyu Li*

Main category: cs.LG

TL;DR: 提出InfoTok：基于信息瓶颈原则的信息正则化视觉标记化机制，用于统一多模态大语言模型，在共享标记空间中平衡压缩与任务相关性


<details>
  <summary>Details</summary>
Motivation: 现有统一多模态大语言模型（MLLMs）的共享标记设计多为架构驱动，缺乏明确标准来确定标记应保留哪些信息以同时支持理解和生成任务

Method: 引入容量约束视角，将视觉标记器视为计算受限的学习器，提出基于信息瓶颈原则的InfoTok机制，通过互信息正则化控制从图像到共享标记再到多模态输出的信息流

Result: 在三种代表性统一MLLMs中集成InfoTok（无需额外训练数据），实验显示在理解和生成任务上均获得一致改进

Conclusion: 信息正则化标记化为统一MLLMs中学习共享标记空间提供了原则性基础

Abstract: Unified multimodal large language models (MLLMs) integrate image understanding and generation in a single framework, with the visual tokenizer acting as the sole interface that maps visual inputs into tokens for downstream tasks. However, existing shared-token designs are mostly architecture-driven and lack an explicit criterion for what information tokens should preserve to support both understanding and generation. Therefore, we introduce a capacity-constrained perspective, highlighting that in shared-token unified MLLMs the visual tokenizer behaves as a compute-bounded learner, so the token budget should prioritize reusable structure over hard-to-exploit high-entropy variations and redundancy. Motivated by this perspective, we propose InfoTok, an information-regularized visual tokenization mechanism grounded in the Information Bottleneck (IB) principle. InfoTok formulates tokenization as controlling information flow from images to shared tokens to multimodal outputs, yielding a principled trade-off between compression and task relevance via mutual-information regularization. We integrate InfoTok into three representative unified MLLMs without introducing any additional training data. Experiments show consistent improvements on both understanding and generation, supporting information-regularized tokenization as a principled foundation for learning a shared token space in unified MLLMs.

</details>


### [728] [How Implicit Bias Accumulates and Propagates in LLM Long-term Memory](https://arxiv.org/abs/2602.01558)
*Yiming Ma,Lixu Wang,Lionel Z. Wang,Hongkun Yang,Haoming Sun,Xin Xu,Jiaqi Wu,Bin Chen,Wei Dong*

Main category: cs.LG

TL;DR: 研究LLMs长期记忆机制中的隐性偏见积累与传播问题，提出动态记忆标记方法有效缓解偏见


<details>
  <summary>Details</summary>
Motivation: LLMs的长期记忆机制虽然能维持交互连续性和个性化，但也引入了未被充分研究的公平性风险，特别是隐性偏见如何在长期决策过程中积累和传播

Method: 1) 构建DIB基准数据集（3,776个决策场景，9个社会领域）；2) 使用长期模拟框架评估6个SOTA LLMs与3种记忆架构；3) 提出动态记忆标记(DMT)方法，在记忆写入时强制执行公平性约束

Result: LLMs的隐性偏见不会保持静态，而是随时间加剧并跨领域传播；静态系统级提示的偏见缓解效果有限且短暂；DMT方法显著减少偏见积累，有效遏制跨领域偏见传播

Conclusion: LLMs长期记忆中的隐性偏见是一个动态累积和传播的系统性问题，需要动态干预策略；DMT方法通过记忆写入时的公平性约束有效缓解偏见，为LLMs的公平性保障提供了新思路

Abstract: Long-term memory mechanisms enable Large Language Models (LLMs) to maintain continuity and personalization across extended interaction lifecycles, but they also introduce new and underexplored risks related to fairness. In this work, we study how implicit bias, defined as subtle statistical prejudice, accumulates and propagates within LLMs equipped with long-term memory. To support systematic analysis, we introduce the Decision-based Implicit Bias (DIB) Benchmark, a large-scale dataset comprising 3,776 decision-making scenarios across nine social domains, designed to quantify implicit bias in long-term decision processes. Using a realistic long-horizon simulation framework, we evaluate six state-of-the-art LLMs integrated with three representative memory architectures on DIB and demonstrate that LLMs' implicit bias does not remain static but intensifies over time and propagates across unrelated domains. We further analyze mitigation strategies and show that a static system-level prompting baseline provides limited and short-lived debiasing effects. To address this limitation, we propose Dynamic Memory Tagging (DMT), an agentic intervention that enforces fairness constraints at memory write time. Extensive experimental results show that DMT substantially reduces bias accumulation and effectively curtails cross-domain bias propagation.

</details>


### [729] [Generative Visual Code Mobile World Models](https://arxiv.org/abs/2602.01576)
*Woosung Koh,Sungjun Han,Segyu Lee,Se-Young Yun,Jamin Shin*

Main category: cs.LG

TL;DR: gWorld提出了一种通过可渲染代码生成的可视化移动GUI世界模型新范式，使用单一VLM预测可执行的网页代码而非直接生成像素，在准确性与模型大小之间建立了新的帕累托前沿。


<details>
  <summary>Details</summary>
Motivation: 当前移动GUI世界模型面临关键权衡：基于文本的方法牺牲视觉保真度，而视觉方法由于无法精确渲染文本，依赖缓慢复杂的外部模型管道。需要结合两者优势的新方法。

Method: 提出视觉世界建模通过可渲染代码生成的新范式：使用单一视觉语言模型预测下一个GUI状态为可执行的网页代码，而非直接生成像素。同时开发了gWorld数据生成框架自动合成基于代码的训练数据。

Result: gWorld在4个分布内和2个分布外基准测试中，在准确性与模型大小方面建立了新的帕累托前沿，优于8个前沿开源模型（最大模型尺寸的50.25倍）。分析显示数据扩展、管道组件改进和更强的世界建模都能提升下游GUI策略性能。

Conclusion: 通过可渲染代码生成的视觉世界建模范式成功结合了文本和视觉方法的优势，gWorld展示了在移动GUI世界建模方面的显著进步，为GUI智能体性能提升提供了有前景的路径。

Abstract: Mobile Graphical User Interface (GUI) World Models (WMs) offer a promising path for improving mobile GUI agent performance at train- and inference-time. However, current approaches face a critical trade-off: text-based WMs sacrifice visual fidelity, while the inability of visual WMs in precise text rendering led to their reliance on slow, complex pipelines dependent on numerous external models. We propose a novel paradigm: visual world modeling via renderable code generation, where a single Vision-Language Model (VLM) predicts the next GUI state as executable web code that renders to pixels, rather than generating pixels directly. This combines the strengths of both approaches: VLMs retain their linguistic priors for precise text rendering while their pre-training on structured web code enables high-fidelity visual generation. We introduce gWorld (8B, 32B), the first open-weight visual mobile GUI WMs built on this paradigm, along with a data generation framework (gWorld) that automatically synthesizes code-based training data. In extensive evaluation across 4 in- and 2 out-of-distribution benchmarks, gWorld sets a new pareto frontier in accuracy versus model size, outperforming 8 frontier open-weight models over 50.25x larger. Further analyses show that (1) scaling training data via gWorld yields meaningful gains, (2) each component of our pipeline improves data quality, and (3) stronger world modeling improves downstream mobile GUI policy performance.

</details>


### [730] [Nearly Optimal Active Preference Learning and Its Application to LLM Alignment](https://arxiv.org/abs/2602.01581)
*Yao Zhao,Kwang-Sung Jun*

Main category: cs.LG

TL;DR: 提出针对偏好学习的高效主动学习算法，相比传统实验设计方法显著提升样本效率


<details>
  <summary>Details</summary>
Motivation: 大语言模型对齐依赖高质量人类偏好标注数据，但收集成本高昂。现有主动学习方法采用经典实验设计准则（如G-或D-最优性），这些目标未针对偏好学习结构进行优化，需要设计问题特定的算法。

Method: 1. 识别偏好学习特有的直觉洞察，质疑现有设计目标的适用性；2. 提出两种主动学习算法：第一种提供该场景下首个实例依赖的标签复杂度保证，第二种是简单实用的贪婪方法。

Result: 在真实世界偏好数据集上评估算法，相比现有方法观察到样本效率的显著提升。

Conclusion: 针对偏好学习结构设计的主动学习算法能够有效提高样本效率，为LLM对齐提供更经济的数据收集方案。

Abstract: Aligning large language models (LLMs) depends on high-quality datasets of human preference labels, which are costly to collect. Although active learning has been studied to improve sample efficiency relative to passive collection, many existing approaches adopt classical experimental design criteria such as G- or D-optimality. These objectives are not tailored to the structure of preference learning, leaving open the design of problem-specific algorithms. In this work, we identify a simple intuition specific to preference learning that calls into question the suitability of these existing design objectives. Motivated by this insight, we propose two active learning algorithms. The first provides the first instance-dependent label complexity guarantee for this setting, and the second is a simple, practical greedy method. We evaluate our algorithm on real-world preference datasets and observe improved sample efficiency compared to existing methods.

</details>


### [731] [A Lightweight Sparse Interaction Network for Time Series Forecasting](https://arxiv.org/abs/2602.01585)
*Xu Zhang,Qitong Wang,Peng Wang,Wei Wang*

Main category: cs.LG

TL;DR: LSINet：一种用于时间序列预测的轻量级稀疏交互网络，通过多头稀疏交互机制和共享交互学习，在保持线性模型效率的同时提升时间依赖建模能力。


<details>
  <summary>Details</summary>
Motivation: 现有线性模型虽然在某些长时序预测任务中优于Transformer，但仅通过堆叠MLP结构隐式进行时间交互，可能不足以捕捉复杂的时间依赖关系，性能仍有提升空间。

Method: 提出LSINet，包含：1）多头稀疏交互机制（MSIM），通过稀疏诱导的伯努利分布学习时间步间的重要连接；2）自适应正则化损失确保稀疏性；3）共享交互学习（SIL）提升效率和收敛性。

Result: 在公开数据集上的大量实验表明，LSINet在时间序列预测任务中，相比先进的线性模型和Transformer模型，实现了更高的准确性和更好的效率。

Conclusion: LSINet通过显式的时间交互机制，在保持线性模型低开销优势的同时，显著提升了时间依赖建模能力，为时间序列预测提供了高效准确的解决方案。

Abstract: Recent work shows that linear models can outperform several transformer models in long-term time-series forecasting (TSF). However, instead of explicitly performing temporal interaction through self-attention, linear models implicitly perform it based on stacked MLP structures, which may be insufficient in capturing the complex temporal dependencies and their performance still has potential for improvement. To this end, we propose a Lightweight Sparse Interaction Network (LSINet) for TSF task. Inspired by the sparsity of self-attention, we propose a Multihead Sparse Interaction Mechanism (MSIM). Different from self-attention, MSIM learns the important connections between time steps through sparsity-induced Bernoulli distribution to capture temporal dependencies for TSF. The sparsity is ensured by the proposed self-adaptive regularization loss. Moreover, we observe the shareability of temporal interactions and propose to perform Shared Interaction Learning (SIL) for MSIM to further enhance efficiency and improve convergence. LSINet is a linear model comprising only MLP structures with low overhead and equipped with explicit temporal interaction mechanisms. Extensive experiments on public datasets show that LSINet achieves both higher accuracy and better efficiency than advanced linear models and transformer models in TSF tasks. The code is available at the link https://github.com/Meteor-Stars/LSINet.

</details>


### [732] [Spectral Text Fusion: A Frequency-Aware Approach to Multimodal Time-Series Forecasting](https://arxiv.org/abs/2602.01588)
*Huu Hiep Nguyen,Minh Hoang Nguyen,Dung Nguyen,Hung Le*

Main category: cs.LG

TL;DR: SpecTF是一个用于多模态时间序列预测的简单有效框架，通过在频域中融合文本和时间序列数据，利用频谱分解和轻量级交叉注意力机制自适应地重新加权频率带。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常逐步对齐文本特征与时间序列模式，忽略了上下文信息（如时间序列周期和动态变化）的多尺度时间影响，导致局部对齐与全局文本上下文之间的不匹配。

Method: 提出SpecTF框架：提取文本嵌入，将其投影到频域，使用轻量级交叉注意力机制与时间序列的频谱分量融合，基于文本相关性自适应地重新加权频率带，然后将结果映射回时域进行预测。

Result: 实验结果表明，SpecTF在多种多模态时间序列数据集上显著优于最先进的模型，同时使用的参数数量明显更少。

Conclusion: 通过频域融合文本和时间序列数据，SpecTF能够有效解决多尺度时间影响问题，在保持模型轻量化的同时实现更好的预测性能。

Abstract: Multimodal time series forecasting is crucial in real-world applications, where decisions depend on both numerical data and contextual signals. The core challenge is to effectively combine temporal numerical patterns with the context embedded in other modalities, such as text. While most existing methods align textual features with time-series patterns one step at a time, they neglect the multiscale temporal influences of contextual information such as time-series cycles and dynamic shifts. This mismatch between local alignment and global textual context can be addressed by spectral decomposition, which separates time series into frequency components capturing both short-term changes and long-term trends. In this paper, we propose SpecTF, a simple yet effective framework that integrates the effect of textual data on time series in the frequency domain. Our method extracts textual embeddings, projects them into the frequency domain, and fuses them with the time series' spectral components using a lightweight cross-attention mechanism. This adaptively reweights frequency bands based on textual relevance before mapping the results back to the temporal domain for predictions. Experimental results demonstrate that SpecTF significantly outperforms state-of-the-art models across diverse multi-modal time series datasets while utilizing considerably fewer parameters. Code is available at https://github.com/hiepnh137/SpecTF.

</details>


### [733] [The Multiple Ticket Hypothesis: Random Sparse Subnetworks Suffice for RLVR](https://arxiv.org/abs/2602.01599)
*Israel Adewuyi,Solomon Okibe,Vladmir Ivanov*

Main category: cs.LG

TL;DR: 在RLVR中，仅训练1%的随机参数子集就能达到或超过全参数微调效果，表明预训练模型包含多个可行的稀疏子网络而非单一特权集合


<details>
  <summary>Details</summary>
Motivation: 彩票假说表明稀疏子网络能达到完整模型性能，RLVR中参数更新集中在稀疏子集进一步证明了参数冗余。研究探索利用这种冗余的最简单方式：在极端稀疏度下仅训练随机选择的参数子集

Method: 在RLVR中，仅训练随机选择的1%参数子集（不同随机掩码），比较其与全参数微调的性能差异，并分析不同掩码之间的重叠度

Result: 仅训练1%参数就能匹配或超过全参数RLVR微调，在3个模型和2个任务域中验证。不同随机掩码重叠度极低（Jaccard相似度≤0.005）但都能成功

Conclusion: 提出"多票假说"：预训练模型包含多个可行的稀疏子网络而非单一特权集合。RLVR中的隐式每步KL约束将更新限制在低维子空间，使任意稀疏掩码都能成功

Abstract: The Lottery Ticket Hypothesis demonstrated that sparse subnetworks can match full-model performance, suggesting parameter redundancy. Meanwhile, in Reinforcement Learning with Verifiable Rewards (RLVR), recent work has shown that updates concentrate on a sparse subset of parameters, which further lends evidence to this underlying redundancy. We study the simplest possible way to exploit this redundancy: training only a randomly selected subset of parameters at extreme sparsities. Empirically, we find that training just 1\% of parameters matches or exceeds full-parameter RLVR finetuning across 3 models and 2 task domains. Moreover, different random masks show minimal overlap ($\leq 0.005$ Jaccard similarity) and yet all succeed, suggesting pretrained models contain many viable sparse subnetworks rather than one privileged set. We term this the Multiple Ticket Hypothesis. We explain this phenomenon through the implicit per-step KL constraint in RLVR, which restricts updates to a low-dimensional subspace, enabling arbitrary sparse masks to succeed.

</details>


### [734] [Boosting Maximum Entropy Reinforcement Learning via One-Step Flow Matching](https://arxiv.org/abs/2602.01606)
*Zeqiao Li,Yijing Wang,Haoyu Wang,Zheng Li,Zhiqiang Zuo*

Main category: cs.LG

TL;DR: FLAME是一种基于流匹配的最大熵强化学习框架，通过Q重加权目标绕过配分函数估计，使用解耦熵估计器校正偏差，实现高效一步控制，在保持性能的同时大幅降低推理延迟。


<details>
  <summary>Details</summary>
Motivation: 扩散策略虽然表达能力强但推理延迟高，而流匹配可以实现一步生成但难以集成到最大熵强化学习中，因为最优策略是难以处理的基于能量的分布，且高效的似然估计存在严重离散化偏差。

Method: 提出FLAME框架：1) 推导Q重加权流匹配目标，通过重要性重加权绕过配分函数估计；2) 设计解耦熵估计器，严格校正偏差以支持高效探索；3) 集成MeanFlow公式实现表达性强且高效的一步控制。

Result: 在MuJoCo实验中，FLAME优于高斯基线，与多步扩散策略性能相当，但推理成本显著降低。

Conclusion: FLAME成功解决了流匹配集成到最大熵强化学习中的挑战，实现了表达性强、推理高效的一步控制策略。

Abstract: Diffusion policies are expressive yet incur high inference latency. Flow Matching (FM) enables one-step generation, but integrating it into Maximum Entropy Reinforcement Learning (MaxEnt RL) is challenging: the optimal policy is an intractable energy-based distribution, and the efficient log-likelihood estimation required to balance exploration and exploitation suffers from severe discretization bias. We propose \textbf{F}low-based \textbf{L}og-likelihood-\textbf{A}ware \textbf{M}aximum \textbf{E}ntropy RL (\textbf{FLAME}), a principled framework that addresses these challenges. First, we derive a Q-Reweighted FM objective that bypasses partition function estimation via importance reweighting. Second, we design a decoupled entropy estimator that rigorously corrects bias, which enables efficient exploration and brings the policy closer to the optimal MaxEnt policy. Third, we integrate the MeanFlow formulation to achieve expressive and efficient one-step control. Empirical results on MuJoCo show that FLAME outperforms Gaussian baselines and matches multi-step diffusion policies with significantly lower inference cost. Code is available at https://github.com/lzqw/FLAME.

</details>


### [735] [What Do Agents Learn from Trajectory-SFT: Semantics or Interfaces?](https://arxiv.org/abs/2602.01611)
*Weizheng Gu,Chengze Li,Zhuohao Yu,Mengyuan Sun,Zhibang Yang,Wei Wang,Hongrui Jia,Shikun Zhang,Wei Ye*

Main category: cs.LG

TL;DR: 论文提出PIPE评估协议，通过最小化改写环境接口来诊断智能体对特定接口的依赖，发现轨迹监督微调会显著增强智能体对训练接口的捷径学习，而非轨迹训练的模型则更稳定。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型作为交互智能体的评估存在混淆：标准基准测试无法区分语义工具使用能力和接口特定交互模式记忆，两者都能在原始接口上获得相同任务成功，但后者缺乏环境不变的能力。

Method: 提出PIPE协议级评估增强方法，通过最小化改写环境接口（保持任务语义和执行行为不变）来诊断接口依赖；引入接口依赖度（IR）指标，量化智能体对训练时接口的偏好。

Result: 在AgentBench和AgentGym的16个环境中测试发现：轨迹监督微调会显著增强接口捷径学习，训练后的智能体在最小接口改写下性能急剧下降，而非轨迹训练的模型保持稳定；接口捷径学习呈现环境依赖、非单调的训练动态。

Conclusion: 标准评估无法揭示智能体对特定接口的依赖，PIPE协议能有效诊断接口捷径学习问题，轨迹监督微调会强化这种依赖，需要更鲁棒的评估方法来确保智能体真正掌握语义能力而非接口模式记忆。

Abstract: Large language models are increasingly evaluated as interactive agents, yet standard agent benchmarks conflate two qualitatively distinct sources of success: semantic tool-use and interface-specific interaction pattern memorization. Because both mechanisms can yield identical task success on the original interface, benchmark scores alone are not identifiable evidence of environment-invariant capability. We propose PIPE, a protocol-level evaluation augmentation for diagnosing interface reliance by minimally rewriting environment interfaces while preserving task semantics and execution behavior. Across 16 environments from AgentBench and AgentGym and a range of open-source and API-based agents, PIPE reveals that trajectory-SFT substantially amplifies interface shortcutting: trained agents degrade sharply under minimal interface rewrites, while non-trajectory-trained models remain largely stable. We further introduce Interface Reliance (IR), a counterbalanced alias-based metric that quantifies preference for training-time interfaces, and show that interface shortcutting exhibits environment-dependent, non-monotonic training dynamics that remain invisible under standard evaluation. Our code is available at https://anonymous.4open.science/r/What-Do-Agents-Learn-from-Trajectory-SFT-Semantics-or-Interfaces--0831/.

</details>


### [736] [A Practical Tensor-Network Compression Pipeline for Production-Scale Large Language Models](https://arxiv.org/abs/2602.01613)
*Sergii Kozyrev,Davyd Maiboroda*

Main category: cs.LG

TL;DR: Minima是一个生产级压缩流水线，通过结构压缩Transformer模型来减少GPU内存占用和推理延迟，结合多种张量分解方法和推测解码技术，在保持高并发性能的同时显著提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在部署时受到GPU内存和推理延迟的限制，需要有效的压缩方法来减少资源消耗同时保持性能。

Method: 训练轻量卷积预测器评估层和补丁级敏感性；对低敏感性区域应用Tucker、张量列车和张量环分解；进行短期修复微调；使用自定义Triton和CUDA内核执行；结合推测解码技术。

Result: 在Qwen3-32B模型上，将峰值VRAM从64GiB降至40GiB；单请求吞吐量从40tps提升至50tps（Minima）和75tps（带推测解码）；50个并行请求下吞吐量分别为34、44、53tps，在高并发下仍保持有效性。

Conclusion: Minima是一个实用的结构压缩方法，通过共享张量骨干网络和微小层适配器，为更激进的结构压缩提供了可行路径，在保持高并发性能的同时显著提升推理效率。

Abstract: Large language models are limited in deployment by GPU memory and inference latency. We present Minima, a production compression pipeline that learns where and how to structurally compress a Transformer and turns that compression into real serving gains. Minima trains a lightweight convolutional predictor to estimate layer- and patch-level sensitivity, applies a mixture of Tucker, tensor-train, and tensor-ring decompositions to low-sensitivity regions, performs a short healing fine-tune, and executes the resulting operators with custom Triton and CUDA kernels. The reduced memory footprint enables speculative decoding with a small draft model and a larger verifier. On Qwen3-32B at an 8k-token context window, Minima reduces peak VRAM from 64 GiB to 40 GiB. For a single active request, throughput increases from 40 tokens per second (baseline) to 50 tokens per second (Minima) and 75 tokens per second (Minima with speculative decoding). Under 50 parallel requests, throughput is 34, 44, and 53 tokens per second respectively, showing that Minima remains effective under high concurrency even when speculative decoding gains compress. We position Minima relative to recent tensor-network, low-rank plus quantization, and cross-layer sharing methods, and argue that it is a practical step toward more aggressive structural compression via shared tensor backbones with tiny per-layer adapters.

</details>


### [737] [AgroFlux: A Spatial-Temporal Benchmark for Carbon and Nitrogen Flux Prediction in Agricultural Ecosystems](https://arxiv.org/abs/2602.01614)
*Qi Cheng,Licheng Liu,Yao Zhang,Mu Hong,Yiqun Xie,Xiaowei Jia*

Main category: cs.LG

TL;DR: 该研究创建了首个时空农业生态系统温室气体基准数据集，结合物理模型模拟和真实观测数据，评估了多种深度学习模型在碳氮通量预测中的表现，并探索了迁移学习提升模型泛化能力的方法。


<details>
  <summary>Details</summary>
Motivation: 农业生态系统占全球温室气体排放的四分之一，但传统方法（如土壤采样、过程模型和黑箱机器学习）面临数据稀疏、时空异质性和复杂地下过程等挑战。目前缺乏AI就绪的基准数据集和协议，阻碍了可信AI模型的发展。

Method: 1. 创建首个时空农业生态系统温室气体基准数据集，整合Ecosys和DayCent物理模型模拟数据、涡度协方差通量塔和受控环境设施的真实观测数据；2. 评估多种序列深度学习模型（LSTM、时序CNN、Transformer）的碳氮通量预测性能；3. 探索迁移学习，利用模拟数据提升深度学习模型在真实观测上的泛化能力。

Result: 建立了首个综合物理模拟和真实观测的农业生态系统温室气体基准数据集，评估了多种深度学习模型的预测性能，并展示了迁移学习如何利用模拟数据改善模型在真实观测上的泛化能力。

Conclusion: 该基准数据集和评估框架有助于开发更准确、可扩展的AI驱动农业生态系统模型，推进对生态系统-气候相互作用的理解，为温室气体减排策略提供支持。

Abstract: Agroecosystem, which heavily influenced by human actions and accounts for a quarter of global greenhouse gas emissions (GHGs), plays a crucial role in mitigating global climate change and securing environmental sustainability. However, we can't manage what we can't measure. Accurately quantifying the pools and fluxes in the carbon, nutrient, and water nexus of the agroecosystem is therefore essential for understanding the underlying drivers of GHG and developing effective mitigation strategies. Conventional approaches like soil sampling, process-based models, and black-box machine learning models are facing challenges such as data sparsity, high spatiotemporal heterogeneity, and complex subsurface biogeochemical and physical processes. Developing new trustworthy approaches such as AI-empowered models, will require the AI-ready benchmark dataset and outlined protocols, which unfortunately do not exist. In this work, we introduce a first-of-its-kind spatial-temporal agroecosystem GHG benchmark dataset that integrates physics-based model simulations from Ecosys and DayCent with real-world observations from eddy covariance flux towers and controlled-environment facilities. We evaluate the performance of various sequential deep learning models on carbon and nitrogen flux prediction, including LSTM-based models, temporal CNN-based model, and Transformer-based models. Furthermore, we explored transfer learning to leverage simulated data to improve the generalization of deep learning models on real-world observations. Our benchmark dataset and evaluation framework contribute to the development of more accurate and scalable AI-driven agroecosystem models, advancing our understanding of ecosystem-climate interactions.

</details>


### [738] [SUSD: Structured Unsupervised Skill Discovery through State Factorization](https://arxiv.org/abs/2602.01619)
*Seyed Mohammad Hadi Hosseini,Mahdieh Soleymani Baghshah*

Main category: cs.LG

TL;DR: SUSD提出了一种基于环境因子分解的无监督技能发现框架，通过将状态空间分解为独立组件并为不同因子分配技能变量，实现更细粒度的技能发现和控制。


<details>
  <summary>Details</summary>
Motivation: 现有无监督技能发现方法存在局限性：基于互信息的方法倾向于发现简单静态技能，而基于距离最大化的方法虽然能发现动态技能，但难以鼓励全面的技能集来覆盖所有可控因子。

Method: SUSD将状态空间分解为独立因子（如物体或可控实体），为不同因子分配不同的技能变量，并使用动态模型跟踪各因子的学习进度，自适应地将智能体注意力转向未充分探索的因子。

Result: 在1到10个因子的三个环境中，SUSD能够发现多样且复杂的技能，在因子化和复杂环境中显著优于现有无监督技能发现方法，并产生因子化的技能表示。

Conclusion: SUSD通过利用环境组合结构，实现了更丰富多样的技能发现，提供了细粒度解耦的实体控制，为分层强化学习中的组合下游任务训练提供了便利。

Abstract: Unsupervised Skill Discovery (USD) aims to autonomously learn a diverse set of skills without relying on extrinsic rewards. One of the most common USD approaches is to maximize the Mutual Information (MI) between skill latent variables and states. However, MI-based methods tend to favor simple, static skills due to their invariance properties, limiting the discovery of dynamic, task-relevant behaviors. Distance-Maximizing Skill Discovery (DSD) promotes more dynamic skills by leveraging state-space distances, yet still fall short in encouraging comprehensive skill sets that engage all controllable factors or entities in the environment. In this work, we introduce SUSD, a novel framework that harnesses the compositional structure of environments by factorizing the state space into independent components (e.g., objects or controllable entities). SUSD allocates distinct skill variables to different factors, enabling more fine-grained control on the skill discovery process. A dynamic model also tracks learning across factors, adaptively steering the agent's focus toward underexplored factors. This structured approach not only promotes the discovery of richer and more diverse skills, but also yields a factorized skill representation that enables fine-grained and disentangled control over individual entities which facilitates efficient training of compositional downstream tasks via Hierarchical Reinforcement Learning (HRL). Our experimental results across three environments, with factors ranging from 1 to 10, demonstrate that our method can discover diverse and complex skills without supervision, significantly outperforming existing unsupervised skill discovery methods in factorized and complex environments. Code is publicly available at: https://github.com/hadi-hosseini/SUSD.

</details>


### [739] [Toward Enhancing Representation Learning in Federated Multi-Task Settings](https://arxiv.org/abs/2602.01626)
*Mehdi Setayesh,Mahdi Beitollahi,Yasser H. Khalil,Hongliang Li*

Main category: cs.LG

TL;DR: FedMuscle：一种基于Muscle损失函数的联邦多任务学习框架，通过对比学习对齐异构模型的表示空间，解决模型和任务异质性问题


<details>
  <summary>Details</summary>
Motivation: 现有联邦多任务学习方法大多假设模型同质性，限制了在现实场景中的应用。需要一种能够处理模型和任务异质性的方法，通过共享表示空间而非模型参数来实现跨任务协作学习。

Method: 提出Muscle损失函数，一种新颖的对比学习目标，能够同时对齐所有参与模型的表示。该损失最小化等价于最大化所有模型表示之间的互信息。基于此开发FedMuscle算法，实现通信高效的联邦多任务学习。

Result: 在多样化的图像和语言任务上的实验表明，FedMuscle始终优于现有最先进的基线方法，在异构设置下实现了显著改进和鲁棒性能。

Conclusion: FedMuscle通过Muscle损失函数有效解决了联邦多任务学习中的模型和任务异质性问题，提供了一种实用且通信高效的解决方案，在异构环境中表现出色。

Abstract: Federated multi-task learning (FMTL) seeks to collaboratively train customized models for users with different tasks while preserving data privacy. Most existing approaches assume model congruity (i.e., the use of fully or partially homogeneous models) across users, which limits their applicability in realistic settings. To overcome this limitation, we aim to learn a shared representation space across tasks rather than shared model parameters. To this end, we propose Muscle loss, a novel contrastive learning objective that simultaneously aligns representations from all participating models. Unlike existing multi-view or multi-model contrastive methods, which typically align models pairwise, Muscle loss can effectively capture dependencies across tasks because its minimization is equivalent to the maximization of mutual information among all the models' representations. Building on this principle, we develop FedMuscle, a practical and communication-efficient FMTL algorithm that naturally handles both model and task heterogeneity. Experiments on diverse image and language tasks demonstrate that FedMuscle consistently outperforms state-of-the-art baselines, delivering substantial improvements and robust performance across heterogeneous settings.

</details>


### [740] [COMET: Codebook-based Online-adaptive Multi-scale Embedding for Time-series Anomaly Detection](https://arxiv.org/abs/2602.01635)
*Jinwoo Park,Hyeongwon Kang,Seung Hun Han,Pilsung Kang*

Main category: cs.LG

TL;DR: COMET提出了一种基于码本的在线自适应多尺度嵌入方法，用于时间序列异常检测，通过多尺度补丁编码、向量量化核心集和在线码本适应三个组件，在多个基准数据集上取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列异常检测方法存在三个主要问题：1) 在补丁级表示学习中未能充分捕捉时间依赖性和多变量相关性；2) 依赖单尺度模式限制了不同时间范围异常的检测；3) 专注于正常数据表示使模型在推理时容易受到分布偏移的影响。

Method: COMET包含三个核心组件：1) 多尺度补丁编码：在不同补丁尺度上捕捉时间依赖性和变量间相关性；2) 向量量化核心集：通过码本学习代表性正常模式，使用量化误差和记忆距离的双重分数检测异常；3) 在线码本适应：基于码本条目生成伪标签，通过对比学习在推理时动态适应模型。

Result: 在五个基准数据集上的实验表明，COMET在45个评估指标中的36个取得了最佳性能，验证了其在多样化环境中的有效性。

Conclusion: COMET通过结合多尺度表示学习、码本驱动的正常模式学习和在线自适应机制，有效解决了时间序列异常检测中的关键挑战，在多个基准上表现出优越性能。

Abstract: Time series anomaly detection is a critical task across various industrial domains. However, capturing temporal dependencies and multivariate correlations within patch-level representation learning remains underexplored, and reliance on single-scale patterns limits the detection of anomalies across different temporal ranges. Furthermore, focusing on normal data representations makes models vulnerable to distribution shifts at inference time. To address these limitations, we propose Codebook-based Online-adaptive Multi-scale Embedding for Time-series anomaly detection (COMET), which consists of three key components: (1) Multi-scale Patch Encoding captures temporal dependencies and inter-variable correlations across multiple patch scales. (2) Vector-Quantized Coreset learns representative normal patterns via codebook and detects anomalies with a dual-score combining quantization error and memory distance. (3) Online Codebook Adaptation generates pseudo-labels based on codebook entries and dynamically adapts the model at inference through contrastive learning. Experiments on five benchmark datasets demonstrate that COMET achieves the best performance in 36 out of 45 evaluation metrics, validating its effectiveness across diverse environments.

</details>


### [741] [Chance-Constrained Inference for Hallucination Risk Control in Large Language Models](https://arxiv.org/abs/2602.01637)
*Sreenivasan Mohandas*

Main category: cs.LG

TL;DR: 提出机会约束推理框架，通过有限样本自适应验证来约束大语言模型生成中的幻觉概率，提供可重复使用的风险保证。


<details>
  <summary>Details</summary>
Motivation: 现有方法只能降低平均错误率，但无法在重复使用中明确控制幻觉发生的频率。需要一种能提供概率风险保证的推理框架。

Method: 将推理建模为部署时的风险控制问题，提出机会约束推理。使用顺序、任意有效的推理过程，通过有限样本自适应地验证可行性或不可行性，避免保守的固定样本界限。

Result: 在NaturalQuestions风格问题和受控多跳问答上的实验表明，该方法能可靠控制风险，早期检测本质上不可行的输入，并在重复使用下实现安全组合，而基于置信度的基线方法无法提供一致的保证。

Conclusion: 机会约束推理为语言模型部署提供了明确的概率风险控制，解决了现有方法无法保证重复使用中幻觉频率的问题，实现了更安全可靠的推理。

Abstract: Large language models generate outputs stochastically and may produce fluent but invalid responses, including factual hallucinations. Existing mitigation strategies reduce average error rates but do not provide explicit control over the \emph{frequency} of such failures under repeated use. We formulate inference as a deployment-time risk control problem and introduce \emph{chance-constrained inference}, which directly bounds the probability of hallucinations among accepted generations. Hallucinations are modeled as stochastic constraint violations, and we show that confidence-based selective prediction does not, in general, imply probabilistic risk guarantees. To enforce chance constraints efficiently, we propose a sequential, anytime-valid inference procedure that adaptively certifies feasibility or infeasibility using finite samples, avoiding conservative fixed-sample bounds. Experiments on questions inspired by NaturalQuestions and controlled multi-hop question answering demonstrate reliable risk control, early detection of intrinsically infeasible inputs, and safe composition under repeated use, while confidence-based baselines fail to provide consistent guarantees.

</details>


### [742] [De Novo Molecular Generation from Mass Spectra via Many-Body Enhanced Diffusion](https://arxiv.org/abs/2602.01643)
*Xichen Sun,Wentao Wei,Jiahua Rao,Jiancong Xie,Yuedong Yang*

Main category: cs.LG

TL;DR: MBGen：基于多体增强扩散框架，从质谱数据生成分子结构，通过多体注意力机制和高阶边建模，显著提升分子生成和异构体区分能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要采用原子中心和成对相互作用建模，忽略了高阶边相互作用，缺乏系统捕捉多体特征的能力，无法有效解析复杂异构体和非局部碎裂机制。

Method: 提出MBGen多体增强扩散框架，集成多体注意力机制和高阶边建模，充分利用MS/MS谱图中编码的丰富结构信息。

Result: 在NPLIB1和MassSpecGym基准测试中，MBGen性能显著优于现有方法，提升幅度高达230%，能有效捕获高阶相互作用，对复杂异构体和非局部碎裂信息表现出增强的敏感性。

Conclusion: 多体建模在基于质谱的分子生成中具有重要的科学价值和实际效用，MBGen框架为从质谱数据生成分子结构提供了更准确的方法。

Abstract: Molecular structure generation from mass spectrometry is fundamental for understanding cellular metabolism and discovering novel compounds. Although tandem mass spectrometry (MS/MS) enables the high-throughput acquisition of fragment fingerprints, these spectra often reflect higher-order interactions involving the concerted cleavage of multiple atoms and bonds-crucial for resolving complex isomers and non-local fragmentation mechanisms. However, most existing methods adopt atom-centric and pairwise interaction modeling, overlooking higher-order edge interactions and lacking the capacity to systematically capture essential many-body characteristics for structure generation. To overcome these limitations, we present MBGen, a Many-Body enhanced diffusion framework for de novo molecular structure Generation from mass spectra. By integrating a many-body attention mechanism and higher-order edge modeling, MBGen comprehensively leverages the rich structural information encoded in MS/MS spectra, enabling accurate de novo generation and isomer differentiation for novel molecules. Experimental results on the NPLIB1 and MassSpecGym benchmarks demonstrate that MBGen achieves superior performance, with improvements of up to 230% over state-of-the-art methods, highlighting the scientific value and practical utility of many-body modeling for mass spectrometry-based molecular generation. Further analysis and ablation studies show that our approach effectively captures higher-order interactions and exhibits enhanced sensitivity to complex isomeric and non-local fragmentation information.

</details>


### [743] [From Perception to Action: Spatial AI Agents and World Models](https://arxiv.org/abs/2602.01644)
*Gloria Felicia,Nolan Bryant,Handi Putra,Ayaan Gazali,Eliel Lobo,Esteban Rojas*

Main category: cs.LG

TL;DR: 本文提出一个统一的三轴分类法，连接智能体能力与空间任务，强调空间智能对具身智能体的重要性，并识别出三个关键发现和六个重大挑战。


<details>
  <summary>Details</summary>
Motivation: 现有研究要么孤立地关注智能体架构，要么孤立地关注空间领域，缺乏一个统一框架来连接这些互补能力。大语言模型在符号领域的成功不能直接转化到物理世界，空间智能（感知3D结构、推理物体关系、在物理约束下行动）对具身智能体至关重要。

Method: 通过对2000多篇论文的系统性综述（引用742篇顶级会议论文），提出了一个统一的三轴分类法，连接智能体能力与跨尺度的空间任务。区分了空间基础（几何和物理的度量理解）与符号基础（图像与文本关联）。

Result: 分析揭示了三个关键发现：1）分层记忆系统对长视野空间任务很重要；2）GNN-LLM集成是结构化空间推理的有前景方法；3）世界模型对跨微观到宏观空间尺度的安全部署至关重要。

Conclusion: 该分类法为统一碎片化研究提供了基础，并识别了六个重大挑战，包括需要统一评估框架来标准化跨领域评估，以推动机器人、自动驾驶和地理空间智能等领域下一代空间感知自主系统的发展。

Abstract: While large language models have become the prevailing approach for agentic reasoning and planning, their success in symbolic domains does not readily translate to the physical world. Spatial intelligence, the ability to perceive 3D structure, reason about object relationships, and act under physical constraints, is an orthogonal capability that proves important for embodied agents. Existing surveys address either agentic architectures or spatial domains in isolation. None provide a unified framework connecting these complementary capabilities. This paper bridges that gap. Through a thorough review of over 2,000 papers, citing 742 works from top-tier venues, we introduce a unified three-axis taxonomy connecting agentic capabilities with spatial tasks across scales. Crucially, we distinguish spatial grounding (metric understanding of geometry and physics) from symbolic grounding (associating images with text), arguing that perception alone does not confer agency. Our analysis reveals three key findings mapped to these axes: (1) hierarchical memory systems (Capability axis) are important for long-horizon spatial tasks. (2) GNN-LLM integration (Task axis) is a promising approach for structured spatial reasoning. (3) World models (Scale axis) are essential for safe deployment across micro-to-macro spatial scales. We conclude by identifying six grand challenges and outlining directions for future research, including the need for unified evaluation frameworks to standardize cross-domain assessment. This taxonomy provides a foundation for unifying fragmented research efforts and enabling the next generation of spatially-aware autonomous systems in robotics, autonomous vehicles, and geospatial intelligence.

</details>


### [744] [On the Spatiotemporal Dynamics of Generalization in Neural Networks](https://arxiv.org/abs/2602.01651)
*Zichao Wei*

Main category: cs.LG

TL;DR: 神经网络无法将加法从16位数推广到32位数，而儿童学习规则后能应用于任意长序列。作者认为这不是工程问题，而是违反了物理假设。他们从物理学角度提出三个约束条件，并推导出SEAD架构，在三个任务上实现了完美的长度泛化。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络为何无法像人类一样将学习到的规则（如加法）推广到更长的序列。作者认为当前神经网络在长度泛化上的失败不是工程问题，而是违反了物理计算的基本假设。

Method: 从物理学角度提出三个约束条件：局部性（信息以有限速度传播）、对称性（计算法则在时空上不变）、稳定性（系统收敛到离散吸引子抵抗噪声积累）。基于这些假设推导出SEAD架构——一种神经元胞自动机，其中局部卷积规则迭代直到收敛。

Result: 在三个任务上验证理论：1) 奇偶性任务：通过光锥传播实现完美长度泛化；2) 加法任务：从L=16到L=100万实现尺度不变推理，准确率100%，展示输入自适应计算；3) Rule 110：学习图灵完备的元胞自动机而无轨迹发散。

Conclusion: 统计学习与逻辑推理之间的鸿沟可以通过尊重计算物理学来弥合，而不是通过扩展参数规模。SEAD架构展示了基于物理约束的系统能够实现完美的长度泛化和规则应用。

Abstract: Why do neural networks fail to generalize addition from 16-digit to 32-digit numbers, while a child who learns the rule can apply it to arbitrarily long sequences? We argue that this failure is not an engineering problem but a violation of physical postulates. Drawing inspiration from physics, we identify three constraints that any generalizing system must satisfy: (1) Locality -- information propagates at finite speed; (2) Symmetry -- the laws of computation are invariant across space and time; (3) Stability -- the system converges to discrete attractors that resist noise accumulation. From these postulates, we derive -- rather than design -- the Spatiotemporal Evolution with Attractor Dynamics (SEAD) architecture: a neural cellular automaton where local convolutional rules are iterated until convergence. Experiments on three tasks validate our theory: (1) Parity -- demonstrating perfect length generalization via light-cone propagation; (2) Addition -- achieving scale-invariant inference from L=16 to L=1 million with 100% accuracy, exhibiting input-adaptive computation; (3) Rule 110 -- learning a Turing-complete cellular automaton without trajectory divergence. Our results suggest that the gap between statistical learning and logical reasoning can be bridged -- not by scaling parameters, but by respecting the physics of computation.

</details>


### [745] [Efficient Adversarial Attacks on High-dimensional Offline Bandits](https://arxiv.org/abs/2602.01658)
*Seyed Mohammad Hadi Hosseini,Amir Najafi,Mahdieh Soleymani Baghshah*

Main category: cs.LG

TL;DR: 论文研究离线bandit评估在对抗攻击下的脆弱性，特别是攻击者通过微调奖励模型权重来劫持bandit行为，在高维场景下即使微小扰动也能导致评估结果严重偏差。


<details>
  <summary>Details</summary>
Motivation: 离线bandit评估已成为评估生成模型的重要方法，但现有研究主要关注在线评估或训练数据攻击，对奖励模型本身的对抗鲁棒性研究不足。随着Hugging Face等平台广泛分发预训练奖励模型，攻击者可能通过微调模型权重来操纵评估结果，这一安全威胁尚未被充分探索。

Method: 提出新的威胁模型，攻击者在离线数据上微调奖励模型权重来劫持bandit行为。从线性奖励函数扩展到非线性模型（如ReLU神经网络），在Hugging Face的两个生成模型评估器（美学质量和组合对齐）上进行攻击实验。理论分析证明高维效应：随着输入维度增加，成功攻击所需的扰动范数减小。

Result: 实验表明：1）随机扰动无效，但针对性扰动能实现接近完美的攻击成功率；2）即使对奖励模型权重进行微小、难以察觉的扰动，也能显著改变bandit行为；3）高维应用（如图像评估）特别脆弱，因为所需扰动范数随维度增加而减小。

Conclusion: 离线bandit评估对奖励模型的对抗攻击高度脆弱，特别是在高维场景下。这揭示了当前生成模型评估流程中的安全漏洞，强调了在部署基于奖励模型的评估系统时需要考虑对抗鲁棒性。

Abstract: Bandit algorithms have recently emerged as a powerful tool for evaluating machine learning models, including generative image models and large language models, by efficiently identifying top-performing candidates without exhaustive comparisons. These methods typically rely on a reward model, often distributed with public weights on platforms such as Hugging Face, to provide feedback to the bandit. While online evaluation is expensive and requires repeated trials, offline evaluation with logged data has become an attractive alternative. However, the adversarial robustness of offline bandit evaluation remains largely unexplored, particularly when an attacker perturbs the reward model (rather than the training data) prior to bandit training. In this work, we fill this gap by investigating, both theoretically and empirically, the vulnerability of offline bandit training to adversarial manipulations of the reward model. We introduce a novel threat model in which an attacker exploits offline data in high-dimensional settings to hijack the bandit's behavior. Starting with linear reward functions and extending to nonlinear models such as ReLU neural networks, we study attacks on two Hugging Face evaluators used for generative model assessment: one measuring aesthetic quality and the other assessing compositional alignment. Our results show that even small, imperceptible perturbations to the reward model's weights can drastically alter the bandit's behavior. From a theoretical perspective, we prove a striking high-dimensional effect: as input dimensionality increases, the perturbation norm required for a successful attack decreases, making modern applications such as image evaluation especially vulnerable. Extensive experiments confirm that naive random perturbations are ineffective, whereas carefully targeted perturbations achieve near-perfect attack success rates ...

</details>


### [746] [Quantifying Epistemic Predictive Uncertainty in Conformal Prediction](https://arxiv.org/abs/2602.01667)
*Siu Lun Chau,Soroush H. Zargarbashi,Yusuf Sale,Michele Caprio*

Main category: cs.LG

TL;DR: 该论文研究如何在共形预测框架下量化认知预测不确定性，提出基于最大平均不精确度的计算方法，并在主动学习和选择性分类任务中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有共形预测方法主要关注预测区域大小，但缺乏对认知不确定性的量化。认知不确定性源于存在多个合理的预测模型，这对决策制定至关重要。

Method: 首先证明分割共形预测也诱导出凸预测分布集合（信度集），然后提出基于最大平均不精确度的计算高效且解析可处理的认知不确定性度量方法。

Result: 在主动学习和选择性分类实验中，提出的认知不确定性量化方法比单纯依赖共形预测区域大小提供更丰富、更细粒度的不确定性评估。

Conclusion: 该工作展示了共形预测作为在认知不确定性下进行原则性决策制定的潜力，提出的不确定性度量方法能更好地支持实际应用中的决策。

Abstract: We study the problem of quantifying epistemic predictive uncertainty (EPU) -- that is, uncertainty faced at prediction time due to the existence of multiple plausible predictive models -- within the framework of conformal prediction (CP). To expose the implicit model multiplicity underlying CP, we build on recent results showing that, under a mild assumption, any full CP procedure induces a set of closed and convex predictive distributions, commonly referred to as a credal set. Importantly, the conformal prediction region (CPR) coincides exactly with the set of labels to which all distributions in the induced credal set assign probability at least $1-α$. As our first contribution, we prove that this characterisation also holds in split CP. Building on this connection, we then propose a computationally efficient and analytically tractable uncertainty measure, based on \emph{Maximum Mean Imprecision}, to quantify the EPU by measuring the degree of conflicting information within the induced credal set. Experiments on active learning and selective classification demonstrate that the quantified EPU provides substantially more informative and fine-grained uncertainty assessments than reliance on CPR size alone. More broadly, this work highlights the potential of CP serving as a principled basis for decision-making under epistemic uncertainty.

</details>


### [747] [ASGMamba: Adaptive Spectral Gating Mamba for Multivariate Time Series Forecasting](https://arxiv.org/abs/2602.01668)
*Qianyang Li,Xingjun Zhang,Shaoxun Wang,Jia Wei,Yueqi Xing*

Main category: cs.LG

TL;DR: ASGMamba：一种用于资源受限高性能计算环境的高效多变量时间序列预测框架，结合自适应谱门控和Mamba骨干网络，在保持线性复杂度的同时实现最先进的预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法面临两难：基于Transformer的模型具有二次复杂度，限制了其在长序列上的可扩展性；而线性状态空间模型难以区分有价值信号和高频噪声，导致状态容量浪费。需要一种既高效又准确的解决方案。

Method: 提出ASGMamba框架：1）轻量级自适应谱门控机制，基于局部谱能量动态过滤噪声；2）Mamba骨干网络专注于鲁棒的时间动态；3）分层多尺度架构和变量特定的节点嵌入，捕捉不同的物理特性。

Result: 在九个基准测试上实现最先进的预测精度，同时保持严格的O(L)线性复杂度，显著减少长时任务的内存使用，成为资源受限环境中高吞吐量预测的可扩展解决方案。

Conclusion: ASGMamba成功解决了现有方法的效率-精度权衡问题，为资源受限的高性能计算环境提供了高效、可扩展的多变量时间序列预测解决方案。

Abstract: Long-term multivariate time series forecasting (LTSF) plays a crucial role in various high-performance computing applications, including real-time energy grid management and large-scale traffic flow simulation. However, existing solutions face a dilemma: Transformer-based models suffer from quadratic complexity, limiting their scalability on long sequences, while linear State Space Models (SSMs) often struggle to distinguish valuable signals from high-frequency noise, leading to wasted state capacity. To bridge this gap, we propose ASGMamba, an efficient forecasting framework designed for resource-constrained supercomputing environments. ASGMamba integrates a lightweight Adaptive Spectral Gating (ASG) mechanism that dynamically filters noise based on local spectral energy, enabling the Mamba backbone to focus its state evolution on robust temporal dynamics. Furthermore, we introduce a hierarchical multi-scale architecture with variable-specific Node Embeddings to capture diverse physical characteristics. Extensive experiments on nine benchmarks demonstrate that ASGMamba achieves state-of-the-art accuracy. While keeping strictly $$\mathcal{O}(L)$$ complexity we significantly reduce the memory usage on long-horizon tasks, thus establishing ASGMamba as a scalable solution for high-throughput forecasting in resource limited environments.The code is available at https://github.com/hit636/ASGMamba

</details>


### [748] [Semantic-aware Wasserstein Policy Regularization for Large Language Model Alignment](https://arxiv.org/abs/2602.01685)
*Byeonghu Na,Hyungho Na,Yeongmin Kim,Suhyeon Jo,HeeSun Bae,Mina Kang,Il-Chul Moon*

Main category: cs.LG

TL;DR: 提出Wasserstein策略正则化(WPR)，一种基于熵正则化Wasserstein距离的语义感知正则化方法，用于改进LLM与人类偏好的对齐，克服传统KL散度仅比较相同位置token概率而忽略语义相似性的局限。


<details>
  <summary>Details</summary>
Motivation: 当前LLM与人类偏好对齐主要使用RLHF方法，其中策略优化采用KL散度正则化参考策略。但KL及其f-散度变体仅比较相同索引位置的token概率，无法捕捉语义相似性，忽略了token空间的几何结构。

Method: 提出Wasserstein策略正则化(WPR)，基于熵正则化Wasserstein距离，将token空间的几何结构纳入考虑。通过距离的对偶公式，将正则化表示为通过最优对偶变量应用于奖励的惩罚项，得到与标准RL算法兼容的可处理目标函数。

Result: 实验表明，该方法在性能上优于基于KL散度和f-散度的基线方法，证明了语义感知策略距离在LLM对齐中的优势。

Conclusion: WPR通过引入语义感知的正则化方法，有效改进了LLM与人类偏好的对齐，为RLHF框架提供了更强大的正则化工具，代码已开源。

Abstract: Large language models (LLMs) are commonly aligned with human preferences using reinforcement learning from human feedback (RLHF). In this method, LLM policies are generally optimized through reward maximization with Kullback-Leibler (KL) divergence regularization of the reference policy. However, KL and its $f$-divergence variants only compare token probabilities at identical indices, failing to capture semantic similarity. We propose Wasserstein Policy Regularization (WPR), a semantic-aware regularization for the RLHF framework based on the entropy-regularized Wasserstein distance, which incorporates the geometry of the token space. The dual formulation of the distance expresses the regularization as penalty terms applied to the reward via optimal dual variables, which yield a tractable objective compatible with standard RL algorithms. Empirically, our method outperforms KL- and $f$-divergence-based baselines, demonstrating the benefits of semantic-aware policy distances for alignment. Our code is available at https://github.com/aailab-kaist/WPR.

</details>


### [749] [Beyond Mode Elicitation: Diversity-Preserving Reinforcement Learning via Latent Diffusion Reasoner](https://arxiv.org/abs/2602.01705)
*Haoqiang Kang,Yizhe Zhang,Nikki Lijing Kuang,Yi-An Ma,Lianhui Qin*

Main category: cs.LG

TL;DR: LaDi-RL：在连续潜在空间中进行探索的强化学习框架，通过引导扩散建模探索，解决离散RL中多样性崩溃问题，提升代码生成和数学推理性能


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法优化离散思维链生成时，由于策略熵降低和模式激发行为，导致探索空间中的多样性崩溃问题

Method: 提出LaDi-RL框架，在连续潜在空间中进行探索，潜在变量编码语义级推理轨迹；通过引导扩散建模探索，多步去噪分布随机性；解耦潜在空间探索和文本空间生成

Result: 在代码生成和数学推理基准测试中，相比离散RL基线，pass@1和pass@k均有持续改进：代码生成pass@1绝对增益+9.4%，数学推理pass@1绝对增益+5.7%

Conclusion: 基于扩散的潜在RL是离散令牌级RL用于推理的原则性替代方案，潜在扩散优化比纯文本空间策略优化更有效，结合文本策略可获得额外增益

Abstract: Recent reinforcement learning (RL) methods improve LLM reasoning by optimizing discrete Chain-of-Thought (CoT) generation; however, exploration in token space often suffers from diversity collapse as policy entropy decreases due to mode elicitation behavior in discrete RL. To mitigate this issue, we propose Latent Diffusion Reasoning with Reinforcement Learning (LaDi-RL), a framework that conducts exploration directly in a continuous latent space, where latent variables encode semantic-level reasoning trajectories. By modeling exploration via guided diffusion, multi-step denoising distributes stochasticity and preserves multiple coexisting solution modes without mutual suppression. Furthermore, by decoupling latent-space exploration from text-space generation, we show that latent diffusion-based optimization is more effective than text-space policy optimization alone, while a complementary text policy provides additional gains when combined with latent exploration. Experiments on code generation and mathematical reasoning benchmarks demonstrate consistent improvements in both pass@1 and pass@k over discrete RL baselines, with absolute pass@1 gains of +9.4% on code generation and +5.7% on mathematical reasoning, highlighting diffusion-based latent RL as a principled alternative to discrete token-level RL for reasoning.

</details>


### [750] [Revisiting Generalization Measures Beyond IID: An Empirical Study under Distributional Shift](https://arxiv.org/abs/2602.01718)
*Sora Nakai,Youssef Fadhloun,Kacem Mathlouthi,Kotaro Yoshida,Ganesh Talluri,Ioannis Mitliagkas,Hiroki Naganuma*

Main category: cs.LG

TL;DR: 该研究评估了40多种泛化度量在不同分布偏移下的稳定性，发现分布偏移会显著影响许多度量的预测性能，只有少数度量在不同设置中保持相对稳定。


<details>
  <summary>Details</summary>
Motivation: 深度学习中的泛化问题仍未解决，特别是在训练分布之外预测模型性能的能力。现有研究存在不稳定性和IID限制，需要评估泛化度量在更广泛场景下的鲁棒性。

Method: 训练了10,000多个超参数配置的小到中型模型，评估了40多种仅从训练模型和训练数据可计算的度量。扩展了实验范围：包括分布偏移评估、多种架构和训练方案，并纳入了校准和信息准则类度量。

Result: 分布偏移会显著改变许多泛化度量的预测性能，而较小的子集在不同设置中保持相对稳定。这揭示了现有泛化度量在非IID场景下的局限性。

Conclusion: 泛化度量的评估需要超越标准IID设置，考虑分布偏移的影响。只有少数度量在不同场景下保持稳定，这为开发更鲁棒的泛化预测指标提供了重要见解。

Abstract: Generalization remains a central yet unresolved challenge in deep learning, particularly the ability to predict a model's performance beyond its training distribution using quantities available prior to test-time evaluation. Building on the large-scale study of Jiang et al. (2020). and concerns by Dziugaite et al. (2020). about instability across training configurations, we benchmark the robustness of generalization measures beyond IID regime. We train small-to-medium models over 10,000 hyperparameter configurations and evaluate more than 40 measures computable from the trained model and the available training data alone. We significantly broaden the experimental scope along multiple axes: (i) extending the evaluation beyond the standard IID setting to include benchmarking for robustness across diverse distribution shifts, (ii) evaluating multiple architectures and training recipes, and (iii) newly incorporating calibration- and information-criteria-based measures to assess their alignment with both IID and OOD generalization. We find that distribution shifts can substantially alter the predictive performance of many generalization measures, while a smaller subset remains comparatively stable across settings.

</details>


### [751] [MSign: An Optimizer Preventing Training Instability in Large Language Models via Stable Rank Restoration](https://arxiv.org/abs/2602.01734)
*Lianhai Ren,Yucheng Ding,Xiao Liu,Qianxiao Li,Peng Cheng,Yeyun Gong*

Main category: cs.LG

TL;DR: 提出MSign优化器，通过周期性矩阵符号操作恢复稳定秩，有效防止大语言模型训练中的梯度爆炸问题


<details>
  <summary>Details</summary>
Motivation: 大语言模型预训练中的训练不稳定性（特别是梯度爆炸）浪费大量计算资源，需要找到根本原因和解决方案

Method: 通过μP缩放的5M参数NanoGPT模型研究训练失败，发现稳定秩下降和相邻层雅可比矩阵对齐两个关键现象，提出MSign优化器周期性应用矩阵符号操作恢复稳定秩

Result: MSign在5M到3B参数的模型上有效防止训练失败，计算开销低于7.0%

Conclusion: 稳定秩下降和雅可比矩阵对齐是训练不稳定的关键机制，MSign通过恢复稳定秩有效解决这一问题

Abstract: Training instability remains a critical challenge in large language model (LLM) pretraining, often manifesting as sudden gradient explosions that waste significant computational resources. We study training failures in a 5M-parameter NanoGPT model scaled via $μ$P, identifying two key phenomena preceding collapse: (1) rapid decline in weight matrix stable rank (ratio of squared Frobenius norm to squared spectral norm), and (2) increasing alignment between adjacent layer Jacobians. We prove theoretically that these two conditions jointly cause exponential gradient norm growth with network depth. To break this instability mechanism, we propose MSign, a new optimizer that periodically applies matrix sign operations to restore stable rank. Experiments on models from 5M to 3B parameters demonstrate that MSign effectively prevents training failures with a computational overhead of less than 7.0%.

</details>


### [752] [Position: The Inevitable End of One-Architecture-Fits-All-Domains in Time Series Forecasting](https://arxiv.org/abs/2602.01736)
*Qinwei Ma,Jingzhe Shi,Jiahao Qiu,Zaiwen Yang*

Main category: cs.LG

TL;DR: 该论文质疑通用领域时间序列神经网络架构的有效性，指出其在特定领域应用中的局限性，呼吁研究重点转向特定领域方法或元学习方法。


<details>
  <summary>Details</summary>
Motivation: 近年来，时间序列预测的神经网络架构变得越来越复杂，但性能已接近饱和。这些通用架构与特定领域（如金融、天气、交通）的实际需求存在冲突，特定领域很少采用近2-3年时间序列社区的神经网络进展。

Method: 论文通过分析现有研究的局限性，总结通用时间序列神经网络架构与特定领域需求之间的不可调和矛盾。提出研究方向的转变建议。

Result: 指出通用领域时间序列神经网络架构研究已饱和，且与特定领域SOTA方法脱节。特定领域通常开发自己的方法，很少采用最新的神经网络架构进展。

Conclusion: 呼吁时间序列社区转变研究方向：要么专注于特定领域的深度学习方法，要么转向通用领域的元学习方法开发，而不是继续研究通用领域的时间序列神经网络架构。

Abstract: Recent work has questioned the effectiveness and robustness of neural network architectures for time series forecasting tasks. We summarize these concerns and analyze groundly their inherent limitations: i.e. the irreconcilable conflict between single (or few similar) domains SOTA and generalizability over general domains for time series forecasting neural network architecture designs. Moreover, neural networks architectures for general domain time series forecasting are becoming more and more complicated and their performance has almost saturated in recent years. As a result, network architectures developed aiming at fitting general time series domains are almost not inspiring for real world practices for certain single (or few similar) domains such as Finance, Weather, Traffic, etc: each specific domain develops their own methods that rarely utilize advances in neural network architectures of time series community in recent 2-3 years. As a result, we call for the time series community to shift focus away from research on time series neural network architectures for general domains: these researches have become saturated and away from domain-specific SOTAs over time. We should either (1) focus on deep learning methods for certain specific domain(s), or (2) turn to the development of meta-learning methods for general domains.

</details>


### [753] [Softmax Linear Attention: Reclaiming Global Competition](https://arxiv.org/abs/2602.01744)
*Mingwei Xu,Xuan Lin,Xinnan Guo,Wanqing Xu,Wanyun Cui*

Main category: cs.LG

TL;DR: SLA通过将softmax从token级提升到head级，在线性注意力中恢复全局竞争机制，在保持线性复杂度的同时提升表达能力和长上下文理解能力。


<details>
  <summary>Details</summary>
Motivation: 线性注意力虽然将Transformer的二次复杂度降低到线性，但由于移除了softmax归一化，失去了全局竞争机制，导致在表达能力和长上下文噪声处理上表现不佳。

Method: 提出Softmax Linear Attention (SLA)框架，将softmax操作从token级别提升到head级别，利用注意力头作为粗粒度语义槽，通过竞争门控机制动态选择最相关的子空间。

Result: SLA在语言建模和长上下文基准测试中持续提升了最先进的线性基线模型（RetNet、GLA、GDN），特别是在具有挑战性的检索场景中显著提升了抗噪声鲁棒性。

Conclusion: SLA成功恢复了精确聚焦能力同时保持线性复杂度，验证了通过利用更高层次的多头聚合结构而非仅仅改进局部核函数的方法的有效性。

Abstract: While linear attention reduces the quadratic complexity of standard Transformers to linear time, it often lags behind in expressivity due to the removal of softmax normalization. This omission eliminates \emph{global competition}, a critical mechanism that enables models to sharply focus on relevant information amidst long-context noise. In this work, we propose \textbf{Softmax Linear Attention (SLA)}, a framework designed to restore this competitive selection without sacrificing efficiency. By lifting the softmax operation from the token level to the head level, SLA leverages attention heads as coarse semantic slots, applying a competitive gating mechanism to dynamically select the most relevant subspaces. This reintroduces the ``winner-take-all'' dynamics essential for precise retrieval and robust long-context understanding. Distinct from prior methods that focus on refining local kernel functions, SLA adopts a broader perspective by exploiting the higher-level multi-head aggregation structure. Extensive experiments demonstrate that SLA consistently enhances state-of-the-art linear baselines (RetNet, GLA, GDN) across language modeling and long-context benchmarks, particularly in challenging retrieval scenarios where it significantly boosts robustness against noise, validating its capability to restore precise focus while maintaining linear complexity.

</details>


### [754] [Probability-Entropy Calibration: An Elastic Indicator for Adaptive Fine-tuning](https://arxiv.org/abs/2602.01745)
*Wenhao Yu,Shaohang Wei,Jiahong Liu,Yifan Li,Minda Hu,Aiwei Liu,Hao Zhang,Irwin King*

Main category: cs.LG

TL;DR: RankTuner提出了一种基于概率-熵校准的token级重加权方法，通过相对秩指标识别真正需要学习的token，在数学推理和代码生成任务上优于仅基于概率或熵的基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的token级重加权方法主要依赖一维指标：真实概率反映下游对齐，token熵反映预训练先验的内在不确定性。忽略熵会误将噪声或易替换token识别为学习关键，而忽略概率则无法反映目标特定对齐。需要一种更全面的校准信号。

Method: RankTuner引入概率-熵校准信号——相对秩指标，比较真实token在预测分布中的实际秩与其期望秩。使用该指标的倒数作为token级相对尺度来重加权微调目标，专注于真正未学习到的token，避免过度惩罚内在不确定的位置。

Result: 在多个骨干模型上的实验表明，该方法在数学推理基准上取得一致改进，在分布外推理任务上获得迁移增益，在代码生成性能上优于仅基于概率或熵的重加权基线。

Conclusion: 相对秩指标提供了一种有效的概率-熵校准机制，能够更准确地识别需要重点学习的token，提升监督微调的效果，特别是在复杂推理任务上表现优异。

Abstract: Token-level reweighting is a simple yet effective mechanism for controlling supervised fine-tuning, but common indicators are largely one-dimensional: the ground-truth probability reflects downstream alignment, while token entropy reflects intrinsic uncertainty induced by the pre-training prior. Ignoring entropy can misidentify noisy or easily replaceable tokens as learning-critical, while ignoring probability fails to reflect target-specific alignment. RankTuner introduces a probability--entropy calibration signal, the Relative Rank Indicator, which compares the rank of the ground-truth token with its expected rank under the prediction distribution. The inverse indicator is used as a token-wise Relative Scale to reweight the fine-tuning objective, focusing updates on truly under-learned tokens without over-penalizing intrinsically uncertain positions. Experiments on multiple backbones show consistent improvements on mathematical reasoning benchmarks, transfer gains on out-of-distribution reasoning, and pre code generation performance over probability-only or entropy-only reweighting baselines.

</details>


### [755] [Rethinking LoRA for Data Heterogeneous Federated Learning: Subspace and State Alignment](https://arxiv.org/abs/2602.01746)
*Hongyi Peng,Han Yu,Xiaoxiao Li,Qiang Yang*

Main category: cs.LG

TL;DR: FedGaLore：针对非IID联邦学习中LoRA性能下降问题，通过客户端梯度子空间优化和服务器端漂移鲁棒同步，提升联邦LoRA的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 在非独立同分布（non-IID）设置下，低秩适应（LoRA）在联邦微调中的性能显著低于全参数微调。研究发现这是由于两个耦合的不匹配问题：更新空间不匹配和优化器状态不匹配。

Method: 提出FedGaLore方法：客户端采用GaLore风格的梯度子空间优化，服务器端通过谱共享信号提取实现投影二阶矩状态的漂移鲁棒同步。

Result: 在自然语言理解、视觉和自然语言生成基准测试中，FedGaLore在非IID设置下相比最先进的联邦LoRA基线方法，显著提升了鲁棒性和准确性。

Conclusion: FedGaLore通过解决LoRA在联邦学习中的两个关键不匹配问题，有效提升了非IID设置下的性能，为高效的联邦微调提供了新解决方案。

Abstract: Low-Rank Adaptation (LoRA) is widely used for federated fine-tuning. Yet under non-IID settings, it can substantially underperform full-parameter fine-tuning. Through with-high-probability robustness analysis, we uncover that this gap can be attributed to two coupled mismatches: (i) update-space mismatch, where clients optimize in a low-rank subspace but aggregation occurs in the full space; and (ii) optimizer-state mismatch, where unsynchronized adaptive states amplify drift across rounds. We propose FedGaLore, which combines client-side GaLore-style gradient-subspace optimization with server-side drift-robust synchronization of projected second-moment states via spectral shared-signal extraction, to address this challenge. Across NLU, vision, and NLG benchmarks, FedGaLore improves robustness and accuracy over state-of-the-art federated LoRA baselines in non-IID settings.

</details>


### [756] [MGKAN: Predicting Asymmetric Drug-Drug Interactions via a Multimodal Graph Kolmogorov-Arnold Network](https://arxiv.org/abs/2602.01751)
*Kunyi Fan,Mengjie Chen,Longlong Li,Cunquan Qu*

Main category: cs.LG

TL;DR: MGKAN是一种基于图Kolmogorov-Arnold网络的药物相互作用预测模型，通过引入可学习基函数和非对称网络视图，显著提升了DDI预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有GNN模型主要依赖线性聚合和对称假设，难以捕捉药物相互作用中的非线性和异质性模式，限制了DDI预测的准确性。

Method: 提出MGKAN模型，用KAN驱动的基函数替代传统MLP变换；整合三个网络视图（非对称DDI网络、共相互作用网络、生化相似性网络）并采用角色特定嵌入；设计融合模块结合线性注意力和非线性变换。

Result: 在两个基准数据集上，MGKAN超越了七个最先进的基线模型；消融研究和案例研究证实了其预测准确性以及在建模方向性药物效应方面的有效性。

Conclusion: MGKAN通过引入可学习基函数和整合多网络视图，能够更有效地捕捉药物相互作用的非线性和异质性模式，为DDI预测提供了更强大的建模框架。

Abstract: Predicting drug-drug interactions (DDIs) is essential for safe pharmacological treatments. Previous graph neural network (GNN) models leverage molecular structures and interaction networks but mostly rely on linear aggregation and symmetric assumptions, limiting their ability to capture nonlinear and heterogeneous patterns. We propose MGKAN, a Graph Kolmogorov-Arnold Network that introduces learnable basis functions into asymmetric DDI prediction. MGKAN replaces conventional MLP transformations with KAN-driven basis functions, enabling more expressive and nonlinear modeling of drug relationships. To capture pharmacological dependencies, MGKAN integrates three network views-an asymmetric DDI network, a co-interaction network, and a biochemical similarity network-with role-specific embeddings to preserve directional semantics. A fusion module combines linear attention and nonlinear transformation to enhance representational capacity. On two benchmark datasets, MGKAN outperforms seven state-of-the-art baselines. Ablation studies and case studies confirm its predictive accuracy and effectiveness in modeling directional drug effects.

</details>


### [757] [A Provable Expressiveness Hierarchy in Hybrid Linear-Full Attention](https://arxiv.org/abs/2602.01763)
*Xiaowei Ye,Xiaoyu He,Chao Liao,Chen Wu,Pinyan Lu*

Main category: cs.LG

TL;DR: 本文首次从理论上证明了全注意力与混合注意力（全注意力+线性注意力）在表达能力上的严格分离：对于多步推理任务，L+1层全注意力网络即可解决，而包含L-1层全注意力和大量线性注意力层的混合网络无法解决。


<details>
  <summary>Details</summary>
Motivation: 现有高效注意力机制（如线性注意力、混合注意力）虽然缓解了全注意力的二次复杂度问题，但其表达能力相对于全注意力缺乏严格的理论刻画。本文旨在从理论上分析不同注意力机制的性能差异。

Method: 建立表达能力层次理论，分析不同注意力机制在顺序函数组合（多步推理任务）上的表现。理论适用于所有可表示为递归形式的线性注意力变体（包括Mamba、DeltaNet等）。

Result: 证明了表达能力层次：对于多步推理任务，L+1层全注意力网络足够解决，而任何包含L-1层全注意力和2^{3L^2}层线性注意力的混合网络都无法解决。这是首次证明混合注意力与标准全注意力之间的可证明分离。

Conclusion: 本文首次从理论上证明了全注意力与混合注意力在表达能力上的严格分离，为理解不同注意力机制的基本能力和局限性提供了理论视角，填补了高效注意力机制表达能力缺乏理论刻画的研究空白。

Abstract: Transformers serve as the foundation of most modern large language models. To mitigate the quadratic complexity of standard full attention, various efficient attention mechanisms, such as linear and hybrid attention, have been developed. A fundamental gap remains: their expressive power relative to full attention lacks a rigorous theoretical characterization. In this work, we theoretically characterize the performance differences among these attention mechanisms. Our theory applies to all linear attention variants that can be formulated as a recurrence, including Mamba, DeltaNet, etc. Specifically, we establish an expressiveness hierarchy: for the sequential function composition-a multi-step reasoning task that must occur within a model's forward pass, an ($L+1$)-layer full attention network is sufficient, whereas any hybrid network interleaving $L-1$ layers of full attention with a substantially larger number ($2^{3L^2}$) of linear attention layers cannot solve it. This result demonstrates a clear separation in expressive power between the two types of attention. Our work provides the first provable separation between hybrid attention and standard full attention, offering a theoretical perspective for understanding the fundamental capabilities and limitations of different attention mechanisms.

</details>


### [758] [CoMeT: Collaborative Memory Transformer for Efficient Long Context Modeling](https://arxiv.org/abs/2602.01766)
*Runsong Zhao,Shilei Liu,Jiwei Tang,Langming Liu,Haibin Chen,Weidong Zhang,Yujin Yuan,Tong Xiao,Jingbo Zhu,Wenbo Su,Bo Zheng*

Main category: cs.LG

TL;DR: CoMeT是一种新型Transformer架构，通过双内存系统和分块处理实现常数内存和线性时间复杂度的长序列处理，可集成到预训练模型中


<details>
  <summary>Details</summary>
Motivation: 标准Transformer的二次复杂度和无限增长的KV缓存是处理长上下文的主要障碍，需要一种能处理任意长序列且内存和时间效率高的解决方案

Method: CoMeT使用双内存系统：FIFO队列的临时内存处理近期事件，带门控更新规则的全局内存处理长程依赖；采用层级流水线并行策略进行高效微调；作为插件模块集成到预训练模型

Result: 在32k上下文微调的模型能在100万token序列中准确检索任意位置的密码；在SCROLLS基准测试中超越其他高效方法，在摘要任务上达到与全注意力基线相当的性能；在实际代理和用户行为QA任务中验证有效性

Conclusion: CoMeT通过创新的双内存系统和分块处理机制，成功解决了Transformer处理长序列时的内存和时间效率问题，为实际应用中的长上下文处理提供了有效解决方案

Abstract: The quadratic complexity and indefinitely growing key-value (KV) cache of standard Transformers pose a major barrier to long-context processing. To overcome this, we introduce the Collaborative Memory Transformer (CoMeT), a novel architecture that enables LLMs to handle arbitrarily long sequences with constant memory usage and linear time complexity. Designed as an efficient, plug-in module, CoMeT can be integrated into pre-trained models with only minimal fine-tuning. It operates on sequential data chunks, using a dual-memory system to manage context: a temporary memory on a FIFO queue for recent events, and a global memory with a gated update rule for long-range dependencies. These memories then act as a dynamic soft prompt for the next chunk. To enable efficient fine-tuning on extremely long contexts, we introduce a novel layer-level pipeline parallelism strategy. The effectiveness of our approach is remarkable: a model equipped with CoMeT and fine-tuned on 32k contexts can accurately retrieve a passkey from any position within a 1M token sequence. On the SCROLLS benchmark, CoMeT surpasses other efficient methods and achieves performance comparable to a full-attention baseline on summarization tasks. Its practical effectiveness is further validated on real-world agent and user behavior QA tasks. The code is available at: https://anonymous.4open.science/r/comet-B00B/

</details>


### [759] [IRIS: Implicit Reward-Guided Internal Sifting for Mitigating Multimodal Hallucination](https://arxiv.org/abs/2602.01769)
*Yuanshuai Li,Yuping Yan,Jirui Han,Fei Ming,Lingjuan Lv,Yaochu Jin*

Main category: cs.LG

TL;DR: IRIS提出了一种基于隐式奖励的内部筛选方法，通过连续隐式奖励在原生对数概率空间中捕捉模态竞争，无需外部反馈即可有效减少多模态大语言模型的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于DPO的多模态大语言模型对齐方法依赖昂贵的外部评估器进行评分或重写，存在离策略学习差距和离散化损失，且无法访问内部状态来捕捉导致幻觉的模态间细粒度冲突。

Method: IRIS利用连续隐式奖励在原生对数概率空间中保持完整信息密度，捕捉内部模态竞争。这种在策略范式通过自生成偏好对消除学习差距，基于多模态隐式奖励筛选这些偏好对，确保优化由直接解决模态冲突的信号驱动。

Result: IRIS在关键幻觉基准测试中仅使用5.7k样本就实现了高度竞争力的性能，在偏好对齐过程中完全不需要任何外部反馈。

Conclusion: IRIS为缓解MLLM幻觉问题提供了一个高效且原则性的范式，通过隐式奖励引导的内部筛选机制有效解决了模态冲突导致的幻觉问题。

Abstract: Hallucination remains a fundamental challenge for Multimodal Large Language Models (MLLMs). While Direct Preference Optimization (DPO) is a key alignment framework, existing approaches often rely heavily on costly external evaluators for scoring or rewriting, incurring off-policy learnability gaps and discretization loss. Due to the lack of access to internal states, such feedback overlooks the fine-grained conflicts between different modalities that lead to hallucinations during generation.
  To address this issue, we propose IRIS (Implicit Reward-Guided Internal Sifting), which leverages continuous implicit rewards in the native log-probability space to preserve full information density and capture internal modal competition. This on-policy paradigm eliminates learnability gaps by utilizing self-generated preference pairs. By sifting these pairs based on multimodal implicit rewards, IRIS ensures that optimization is driven by signals that directly resolve modal conflicts. Extensive experiments demonstrate that IRIS achieves highly competitive performance on key hallucination benchmarks using only 5.7k samples, without requiring any external feedback during preference alignment. These results confirm that IRIS provides an efficient and principled paradigm for mitigating MLLM hallucinations.

</details>


### [760] [DIA-CLIP: a universal representation learning framework for zero-shot DIA proteomics](https://arxiv.org/abs/2602.01772)
*Yucheng Liao,Han Wen,Weinan E,Weijie Zhang*

Main category: cs.LG

TL;DR: DIA-CLIP是一个预训练模型，通过跨模态表示学习实现零样本肽谱匹配，显著提升蛋白质鉴定性能


<details>
  <summary>Details</summary>
Motivation: 当前DIA-MS分析框架需要半监督训练，容易过拟合且缺乏跨物种和实验条件的泛化能力

Method: 结合双编码器对比学习框架和编码器-解码器架构，建立肽段与对应谱特征的统一跨模态表示

Result: 在多个基准测试中优于现有工具，蛋白质鉴定增加45%，诱饵鉴定减少12%

Conclusion: DIA-CLIP将DIA分析范式从半监督训练转向通用跨模态学习，在单细胞和空间蛋白质组学等应用中具有巨大潜力

Abstract: Data-independent acquisition mass spectrometry (DIA-MS) has established itself as a cornerstone of proteomic profiling and large-scale systems biology, offering unparalleled depth and reproducibility. Current DIA analysis frameworks, however, require semi-supervised training within each run for peptide-spectrum match (PSM) re-scoring. This approach is prone to overfitting and lacks generalizability across diverse species and experimental conditions. Here, we present DIA-CLIP, a pre-trained model shifting the DIA analysis paradigm from semi-supervised training to universal cross-modal representation learning. By integrating dual-encoder contrastive learning framework with encoder-decoder architecture, DIA-CLIP establishes a unified cross-modal representation for peptides and corresponding spectral features, achieving high-precision, zero-shot PSM inference. Extensive evaluations across diverse benchmarks demonstrate that DIA-CLIP consistently outperforms state-of-the-art tools, yielding up to a 45% increase in protein identification while achieving a 12% reduction in entrapment identifications. Moreover, DIA-CLIP holds immense potential for diverse practical applications, such as single-cell and spatial proteomics, where its enhanced identification depth facilitates the discovery of novel biomarkers and the elucidates of intricate cellular mechanisms.

</details>


### [761] [Position: Beyond Model-Centric Prediction -- Agentic Time Series Forecasting](https://arxiv.org/abs/2602.01776)
*Mingyue Cheng,Xiaoyu Tao,Qi Liu,Ze Guo,Enhong Chen*

Main category: cs.LG

TL;DR: 该论文提出将时间序列预测从传统的模型中心范式转变为智能体化预测（ATSF），将预测重构为包含感知、规划、行动、反思和记忆的智能体化过程。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列预测范式在自适应、多轮交互场景中存在不足，需要信息特征提取、推理驱动推断、迭代优化和持续适应能力，因此需要新的智能体化方法。

Method: 提出智能体化时间序列预测（ATSF）框架，包含三种实现范式：基于工作流的设计、智能体强化学习范式、以及混合智能体工作流范式。

Result: 建立了从模型中心预测向智能体化预测转变的理论框架，为时间序列预测与智能体系统的交叉研究奠定了基础。

Conclusion: 智能体化预测为时间序列预测提供了新的研究方向，能够更好地处理自适应、多轮交互的复杂预测场景，具有重要的研究价值和实际应用前景。

Abstract: Time series forecasting has traditionally been formulated as a model-centric, static, and single-pass prediction problem that maps historical observations to future values. While this paradigm has driven substantial progress, it proves insufficient in adaptive and multi-turn settings where forecasting requires informative feature extraction, reasoning-driven inference, iterative refinement, and continual adaptation over time. In this paper, we argue for agentic time series forecasting (ATSF), which reframes forecasting as an agentic process composed of perception, planning, action, reflection, and memory. Rather than focusing solely on predictive models, ATSF emphasizes organizing forecasting as an agentic workflow that can interact with tools, incorporate feedback from outcomes, and evolve through experience accumulation. We outline three representative implementation paradigms -- workflow-based design, agentic reinforcement learning, and a hybrid agentic workflow paradigm -- and discuss the opportunities and challenges that arise when shifting from model-centric prediction to agentic forecasting. Together, this position aims to establish agentic forecasting as a foundation for future research at the intersection of time series forecasting.

</details>


### [762] [Grad2Reward: From Sparse Judgment to Dense Rewards for Improving Open-Ended LLM Reasoning](https://arxiv.org/abs/2602.01791)
*Zheng Zhang,Ao Lu,Yuanhao Zeng,Ziwei Shan,Jinjin Guo,Lufei Li,Yexin Li,Kan Ren*

Main category: cs.LG

TL;DR: Grad2Reward：通过单次反向传播从Judge模型推理过程中提取密集过程奖励的新框架，解决稀疏奖励问题，实现细粒度监督，提升开放任务中的LLM推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法在开放任务中使用LLM-as-a-Judge提供序列级奖励，但奖励稀疏且无法提供细粒度监督，同时将Judge视为黑盒，忽略了其中丰富的中间反馈信号。

Method: 提出Grad2Reward框架：1）通过单次反向传播从Judge模型推理过程中提取密集过程奖励；2）利用基于梯度的归因实现精确的token级信用分配；3）引入自判断机制，让策略通过自身评估信号改进，无需训练专门奖励模型或依赖外部优质Judge。

Result: 实验表明，使用Grad2Reward优化的策略在多种开放任务中表现出色，验证了其有效性和广泛泛化能力。

Conclusion: Grad2Reward通过提取Judge模型推理过程中的密集过程奖励，解决了现有RLVR方法在开放任务中的稀疏奖励问题，显著提升了训练效率和推理质量，具有广泛适用性。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has catalyzed significant breakthroughs in complex LLM reasoning within verifiable domains, such as mathematics and programming. Recent efforts have sought to extend this paradigm to open-ended tasks by employing LLMs-as-a-Judge to provide sequence-level rewards for policy optimization. However, these rewards are inherently sparse, failing to provide the fine-grained supervision necessary for generating complex, long-form trajectories. Furthermore, current work treats the Judge as a black-box oracle, discarding the rich intermediate feedback signals encoded in it. To address these limitations, we introduce Grad2Reward, a novel framework that extracts dense process rewards directly from the Judge's model inference process via a single backward pass. By leveraging gradient-based attribution, Grad2Reward enables precise token-level credit assignment, substantially enhancing training efficiency and reasoning quality. Additionally, Grad2Reward introduces a self-judging mechanism, allowing the policy to improve through its own evaluative signals without training specialized reward models or reliance on superior external Judges. The experiments demonstrate that policies optimized with Grad2Reward achieve outstanding performance across diverse open-ended tasks, affirming its effectiveness and broad generalizability.

</details>


### [763] [Beyond Precision: Training-Inference Mismatch is an Optimization Problem and Simple LR Scheduling Fixes It](https://arxiv.org/abs/2602.01826)
*Yaxiang Zhang,Yingru Li,Jiacai Liu,Jiawei Xu,Ziniu Li,Qian Liu,Haoyuan Li*

Main category: cs.LG

TL;DR: 本文提出一种基于响应长度的动态学习率调度器，用于稳定大语言模型的强化学习训练，解决训练-推理不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的强化学习训练存在不稳定问题，传统方法如重要性采样在长期训练中可能失效。研究发现训练-推理不匹配与梯度噪声会随着训练进展而同时加剧。

Method: 提出一种专门的学习率调度器，不采用预定义的衰减计划，而是基于响应长度动态触发学习率衰减。响应长度被识别为即将出现不稳定性的可靠早期预警信号。

Result: 通过在学习率上升时降低学习率，能够持续稳定RL训练，并将训练-推理不匹配保持在安全水平。

Conclusion: 训练-推理不匹配不仅是静态数值差异，而是与模型优化耦合的动态故障。基于响应长度的动态学习率调度是简单有效的解决方案。

Abstract: Reinforcement Learning (RL) for training Large Language Models is notoriously unstable. While recent studies attribute this to "training inference mismatch stemming" from inconsistent hybrid engines, standard remedies, such as Importance Sampling, might fail during extended training runs. In this work, we analyze this instability through the lens of optimization, demonstrating that gradient noise and training-inference mismatch escalate in tandem as training progresses. Meanwhile, we find that the mismatch can be effectively suppressed by shrinking the update size. Taken together, we deduce that the mismatch is not merely a static numerical discrepancy, but a dynamic failure coupled with the model's optimization. Based on this insight, we propose a simple yet effective solution: a specialized Learning Rate (LR) scheduler. Instead of pre-defined decay schedule in traditional LR scheduler, our method dynamically triggers LR decay based on response length, which we identify as a reliable early-warning signal for impending instability. Empirical evidence suggests that by reducing the learning rate as gradient noise rises, we can consistently stabilize RL training and keep the training-inference mismatch at a safe level.

</details>


### [764] [Hyperbolic Graph Neural Networks Under the Microscope: The Role of Geometry-Task Alignment](https://arxiv.org/abs/2602.01828)
*Dionisia Naddeo,Jonas Linkerhägner,Nicola Toschi,Geri Skenderi,Veronica Lachi*

Main category: cs.LG

TL;DR: HGNNs仅在任务与双曲几何对齐时优于欧几里得模型，否则优势消失


<details>
  <summary>Details</summary>
Motivation: 质疑当前HGNNs作为树状图表示学习首选范式的合理性，提出几何-任务对齐的额外条件

Method: 理论分析和实证研究：1) 在两个合成回归问题上测试HGNNs恢复低失真表示的能力；2) 通过联合分析预测性能和嵌入失真评估HGNNs在链接预测和节点分类任务上的表现

Result: 1) HGNNs在需要保持度量结构的问题中几何归纳偏置有帮助；2) 只有链接预测是几何对齐的；3) HGNNs仅在几何对齐时持续优于欧几里得模型

Conclusion: 研究重点应从"图是否是双曲的？"扩展到"任务是否与双曲几何对齐？"，HGNNs仅在几何对齐时具有优势

Abstract: Many complex networks exhibit hyperbolic structural properties, making hyperbolic space a natural candidate for representing hierarchical and tree-like graphs with low distortion. Based on this observation, Hyperbolic Graph Neural Networks (HGNNs) have been widely adopted as a principled choice for representation learning on tree-like graphs. In this work, we question this paradigm by proposing an additional condition of geometry-task alignment, i.e., whether the metric structure of the target follows that of the input graph. We theoretically and empirically demonstrate the capability of HGNNs to recover low-distortion representations on two synthetic regression problems, and show that their geometric inductive bias becomes helpful when the problem requires preserving metric structure. Additionally, we evaluate HGNNs on the tasks of link prediction and node classification by jointly analyzing predictive performance and embedding distortion, revealing that only link prediction is geometry-aligned. Overall, our findings shift the focus from only asking "Is the graph hyperbolic?" to also questioning "Is the task aligned with hyperbolic geometry?", showing that HGNNs consistently outperform Euclidean models under such alignment, while their advantage vanishes otherwise.

</details>


### [765] [DOGMA: Weaving Structural Information into Data-centric Single-cell Transcriptomics Analysis](https://arxiv.org/abs/2602.01839)
*Ru Zhang,Xunkai Li,Yaxin Deng,Sicheng Liu,Daohan Su,Qiangqiang Dai,Hongchao Qin,Rong-Hua Li,Guoren Wang,Jia Li*

Main category: cs.LG

TL;DR: DOGMA是一个数据中心的单细胞转录组学分析框架，通过整合多层次生物先验知识来重塑数据结构并增强语义，超越了依赖随机启发式的方法，在多物种多器官基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有单细胞转录组学分析方法存在两个主要问题：1）早期序列方法将细胞视为独立实体，忽略了生物系统功能机制驱动的潜在细胞间关系；2）结构化方法虽然捕捉细胞间关系并增强原始数据，但往往忽略生物先验知识，导致计算开销大且图表示次优。

Method: DOGMA通过整合多层次生物先验知识来重塑数据结构和增强语义：1）使用统计锚点、细胞本体论和系统发育树进行确定性结构发现和跨物种对齐；2）利用基因本体论通过功能先验知识弥合特征级语义差距。

Result: 在复杂的多物种和多器官基准测试中，DOGMA实现了最先进的性能，表现出卓越的零样本鲁棒性和样本效率，同时以显著更低的计算成本运行。

Conclusion: DOGMA通过系统性地整合生物先验知识，为单细胞转录组学分析提供了一个更有效的数据中心解决方案，超越了依赖随机启发式的方法，在性能、鲁棒性和效率方面都有显著提升。

Abstract: Recently, data-centric AI methodology has been a dominant paradigm in single-cell transcriptomics analysis, which treats data representation rather than model complexity as the fundamental bottleneck. In the review of current studies, earlier sequence methods treat cells as independent entities and adapt prevalent ML models to analyze their directly inherited sequence data. Despite their simplicity and intuition, these methods overlook the latent intercellular relationships driven by the functional mechanisms of biological systems and the inherent quality issues of the raw sequence data. Therefore, a series of structured methods has emerged. Although they employ various heuristic rules to capture intricate intercellular relationships and enhance the raw sequencing data, these methods often neglect biological prior knowledge. This omission incurs substantial overhead and yields suboptimal graph representations, thereby hindering the utility of ML models.
  To address them, we propose DOGMA, a holistic data-centric framework designed for the structural reshaping and semantic enhancement of raw data through multi-level biological prior knowledge. Transcending reliance on stochastic heuristics, DOGMA redefines graph construction by integrating Statistical Anchors with Cell Ontology and Phylogenetic Trees to enable deterministic structure discovery and robust cross-species alignment. Furthermore, Gene Ontology is utilized to bridge the feature-level semantic gap by incorporating functional priors. In complex multi-species and multi-organ benchmarks, DOGMA achieves SOTA performance, exhibiting superior zero-shot robustness and sample efficiency while operating with significantly lower computational cost.

</details>


### [766] [Prism: Efficient Test-Time Scaling via Hierarchical Search and Self-Verification for Discrete Diffusion Language Models](https://arxiv.org/abs/2602.01842)
*Jinbin Bai,Yixuan Li,Yuchen Zhu,Yi Xin,Qingyu Shi,Aosong Feng,Xiaohong Liu,Molei Tao,Jianru Xue,Xiangtai Li,Ming-Hsuan Yang*

Main category: cs.LG

TL;DR: Prism是一个针对离散扩散语言模型的高效测试时扩展框架，通过分层轨迹搜索、局部分支和自验证反馈，在数学推理和代码生成任务上实现了性能与效率的良好平衡。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时扩展算法主要基于自回归解码，不适合并行解码的离散扩散语言模型，因此需要开发专门的高效TTS方法来释放dLLMs的生成潜力。

Method: 提出Prism框架：1)分层轨迹搜索，在去噪早期动态剪枝和重新分配计算资源；2)局部分支与部分重掩码，探索多样化实现同时保留高置信度token；3)自验证反馈，通过自评估提示替代外部验证器。

Result: 在三个dLLM模型（LLaDA 8B Instruct、Dream 7B Instruct、LLaDA 2.0-mini）的四个数学推理和代码生成基准测试中，Prism实现了良好的性能-效率权衡，以显著更少的函数评估次数匹配最佳N选1性能。

Conclusion: Prism为离散扩散语言模型提供了一种高效有效的测试时扩展框架，解决了现有TTS方法不适用于dLLMs的问题，在保持性能的同时大幅提升了计算效率。

Abstract: Inference-time compute has re-emerged as a practical way to improve LLM reasoning. Most test-time scaling (TTS) algorithms rely on autoregressive decoding, which is ill-suited to discrete diffusion language models (dLLMs) due to their parallel decoding over the entire sequence. As a result, developing effective and efficient TTS methods to unlock dLLMs' full generative potential remains an underexplored challenge. To address this, we propose Prism (Pruning, Remasking, and Integrated Self-verification Method), an efficient TTS framework for dLLMs that (i) performs Hierarchical Trajectory Search (HTS) which dynamically prunes and reallocates compute in an early-to-mid denoising window, (ii) introduces Local branching with partial remasking to explore diverse implementations while preserving high-confidence tokens, and (iii) replaces external verifiers with Self-Verified Feedback (SVF) obtained via self-evaluation prompts on intermediate completions. Across four mathematical reasoning and code generation benchmarks on three dLLMs, including LLaDA 8B Instruct, Dream 7B Instruct, and LLaDA 2.0-mini, our Prism achieves a favorable performance-efficiency trade-off, matching best-of-N performance with substantially fewer function evaluations (NFE). The code is released at https://github.com/viiika/Prism.

</details>


### [767] [No Generation without Representation: Efficient Causal Protein Language Models Enable Zero-Shot Fitness Estimation](https://arxiv.org/abs/2602.01845)
*Furkan Eris*

Main category: cs.LG

TL;DR: Proust是一个309M参数的因果蛋白质语言模型，通过架构创新弥合了掩码语言模型在适应性预测和因果模型在生成能力之间的差距，在多个蛋白质任务上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 蛋白质语言模型面临基本分歧：掩码语言模型在适应性预测方面表现出色，而因果模型支持生成，迫使研究人员维护不同的架构。需要一种能同时兼顾两种能力的统一模型。

Method: 引入Proust因果蛋白质语言模型，采用从LLM研究借鉴的架构创新：分组查询注意力与共享K/V投影、跨层值残差、深度因果卷积。在33B token上训练，耗时40 B200 GPU小时。

Result: 在ProteinGym替换任务上达到Spearman ρ=0.390，与需要50-200倍计算量的MLMs竞争；在indels任务上达到新的SOTA，优于大20倍的模型；在EVEREST病毒适应性基准上接近结构感知方法。

Conclusion: Proust在性能和生成能力之间找到了平衡点，同时保留了因果模型固有的生成能力。可解释性分析显示位置熵方差可预测检索增强的效果，这些见解可扩展并指导测试时缩放等能力。

Abstract: Protein language models (PLMs) face a fundamental divide: masked language models (MLMs) excel at fitness prediction while causal models enable generation, forcing practitioners to maintain separate architectures. We introduce \textbf{Proust}, a 309M-parameter causal PLM that bridges this gap through architectural innovations adapted from recent LLM research, including grouped-query attention with shared K/V projections, cross-layer value residuals, and depthwise causal convolutions. Trained on 33B tokens in 40 B200 GPU-hours, Proust achieves Spearman $ρ= 0.390$ on ProteinGym substitutions, competitive with MLMs requiring 50--200$\times$ the compute. On indels, Proust sets a new state-of-the-art, outperforming models up to 20$\times$ larger. On EVEREST viral fitness benchmarks, it approaches structure-aware methods using sequence alone. These powerful representations position Proust in a sweet spot as it also retains native generative capabilities that MLMs lack by design. Interpretability analysis reveals that per-position entropy variance predicts, to an extent, when retrieval augmentation helps and hurts. Such insights can grow in both quantity and quality at scale and inform capabilities such as test-time scaling. Code and weights are available at https://github.com/Furkan9015/proust-inference

</details>


### [768] [Self-Rewarding Sequential Monte Carlo for Masked Diffusion Language Models](https://arxiv.org/abs/2602.01849)
*Ziwei Luo,Ziqi Jin,Lei Wang,Lidong Bing,Thomas B. Schön*

Main category: cs.LG

TL;DR: 提出自奖励序列蒙特卡洛方法，通过并行扩散粒子交互和轨迹置信度作为自奖励信号，提升掩码扩散语言模型的采样质量与多样性。


<details>
  <summary>Details</summary>
Motivation: 现有掩码扩散语言模型主要依赖置信度采样策略，只保留每步预测置信度最高的token，这限制了生成路径的多样性，导致采样质量下降。

Method: 提出自奖励序列蒙特卡洛方法：1）并行启动多个交互扩散过程（粒子）探索轨迹；2）引入轨迹级置信度作为自奖励信号分配粒子重要性权重；3）迭代加权和重采样粒子，系统引导生成全局置信度高、高质量的样本。

Result: 在多种掩码扩散语言模型和基准测试中验证，无需额外训练或奖励引导即可显著提升采样质量，有效将并行推理能力转化为改进的采样质量。

Conclusion: 自奖励序列蒙特卡洛方法通过轨迹探索和自奖励机制，解决了掩码扩散语言模型采样多样性不足的问题，实现了采样质量的显著提升。

Abstract: This work presents self-rewarding sequential Monte Carlo (SMC), an inference-time scaling algorithm enabling effective sampling of masked diffusion language models (MDLMs). Our algorithm stems from the observation that most existing MDLMs rely on a confidence-based sampling strategy, where only tokens with the highest prediction confidence are preserved at each step. This restricts the generation to a noise-sensitive, greedy decoding paradigm, resulting in an inevitable collapse in the diversity of possible paths. We address this problem by launching multiple interacting diffusion processes in parallel, referred to as particles, for trajectory exploration. Importantly, we introduce the trajectory-level confidence as a self-rewarding signal for assigning particle importance weights. During sampling, particles are iteratively weighted and resampled to systematically steer generation towards globally confident, high-quality samples. Our self-rewarding SMC is verified on various masked diffusion language models and benchmarks, achieving significant improvement without extra training or reward guidance, while effectively converting parallel inference capacity into improved sampling quality. Our code is available at https://github.com/Algolzw/self-rewarding-smc.

</details>


### [769] [FUPareto: Bridging the Forgetting-Utility Gap in Federated Unlearning via Pareto Augmented Optimization](https://arxiv.org/abs/2602.01852)
*Zeyan Wang,Zhengmao Liu,Yongxin Cai,Chi Li,Xiaoying Tang,Jingchao Chen,Zibin Pan,Jing Qiu*

Main category: cs.LG

TL;DR: FUPareto：基于帕累托优化的联邦遗忘框架，通过最小边界偏移损失和零空间投影多梯度下降算法，解决联邦遗忘中的效用-遗忘冲突和多客户端并发遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有联邦遗忘方法存在三个关键问题：1）遗忘目标常损害模型效用或增加成员推理攻击风险；2）遗忘与效用之间存在固有冲突；3）多客户端并发遗忘支持差，梯度冲突影响遗忘质量。

Method: 提出FUPareto框架：1）引入最小边界偏移损失，通过抑制目标类logit低于最高非目标类logit来高效遗忘；2）采用帕累托改进步骤保持模型效用；3）执行帕累托扩展保证遗忘，集成零空间投影多梯度下降算法解耦梯度冲突。

Result: 在多种场景下的实验表明，FUPareto在遗忘效果和保留效用方面均优于现有最先进的联邦遗忘方法。

Conclusion: FUPareto通过帕累托优化方法有效解决了联邦遗忘中的关键挑战，实现了高效、公平的多客户端并发遗忘，同时最小化效用损失。

Abstract: Federated Unlearning (FU) aims to efficiently remove the influence of specific client data from a federated model while preserving utility for the remaining clients. However, three key challenges remain: (1) existing unlearning objectives often compromise model utility or increase vulnerability to Membership Inference Attacks (MIA); (2) there is a persistent conflict between forgetting and utility, where further unlearning inevitably harms retained performance; and (3) support for concurrent multi-client unlearning is poor, as gradient conflicts among clients degrade the quality of forgetting. To address these issues, we propose FUPareto, an efficient unlearning framework via Pareto-augmented optimization. We first introduce the Minimum Boundary Shift (MBS) Loss, which enforces unlearning by suppressing the target class logit below the highest non-target class logit; this can improve the unlearning efficiency and mitigate MIA risks. During the unlearning process, FUPareto performs Pareto improvement steps to preserve model utility and executes Pareto expansion to guarantee forgetting. Specifically, during Pareto expansion, the framework integrates a Null-Space Projected Multiple Gradient Descent Algorithm (MGDA) to decouple gradient conflicts. This enables effective, fair, and concurrent unlearning for multiple clients while minimizing utility degradation. Extensive experiments across diverse scenarios demonstrate that FUPareto consistently outperforms state-of-the-art FU methods in both unlearning efficacy and retained utility.

</details>


### [770] [Time2Vec-Integrated Transformer for Robust Gesture Recognition from Low-Density sEMG](https://arxiv.org/abs/2602.01855)
*Blagoj Hristov,Hristijan Gjoreski,Vesna Ojleska Latkoska,Gorjan Nadzinski*

Main category: cs.LG

TL;DR: 提出一种数据高效的深度学习框架，仅需两个通道的表面肌电信号即可实现精确的假肢控制，通过可学习的时间嵌入和归一化融合策略，在稀疏传感器条件下达到95.7%的F1分数。


<details>
  <summary>Details</summary>
Motivation: 传统肌电假肢控制依赖复杂密集的多传感器阵列，成本高且不易普及。需要开发能够在最小化传感器硬件条件下实现精确控制的方法，提高假肢技术的可及性。

Method: 采用混合Transformer架构，针对稀疏双通道sEMG信号优化。引入Time2Vec可学习时间嵌入捕捉生物信号的随机时间扭曲，使用归一化加性融合策略对齐空间和时间特征的潜在分布，采用两阶段课程学习协议应对数据稀缺问题。

Result: 在10类动作分类任务中达到95.7% ± 0.20%的多受试者F1分数，显著优于标准Transformer和CNN-LSTM模型。快速校准协议仅需每个手势两个试次即可将新受试者的性能从21.0%提升到96.9%。

Conclusion: 高质量的时间嵌入可以补偿低空间分辨率，挑战了高密度传感的必要性。该框架为下一代能够快速个性化的假肢接口提供了鲁棒且经济高效的蓝图。

Abstract: Accurate and responsive myoelectric prosthesis control typically relies on complex, dense multi-sensor arrays, which limits consumer accessibility. This paper presents a novel, data-efficient deep learning framework designed to achieve precise and accurate control using minimal sensor hardware. Leveraging an external dataset of 8 subjects, our approach implements a hybrid Transformer optimized for sparse, two-channel surface electromyography (sEMG). Unlike standard architectures that use fixed positional encodings, we integrate Time2Vec learnable temporal embeddings to capture the stochastic temporal warping inherent in biological signals. Furthermore, we employ a normalized additive fusion strategy that aligns the latent distributions of spatial and temporal features, preventing the destructive interference common in standard implementations. A two-stage curriculum learning protocol is utilized to ensure robust feature extraction despite data scarcity. The proposed architecture achieves a state-of-the-art multi-subject F1-score of 95.7% $\pm$ 0.20% for a 10-class movement set, statistically outperforming both a standard Transformer with fixed encodings and a recurrent CNN-LSTM model. Architectural optimization reveals that a balanced allocation of model capacity between spatial and temporal dimensions yields the highest stability. Furthermore, while direct transfer to a new unseen subject led to poor accuracy due to domain shifts, a rapid calibration protocol utilizing only two trials per gesture recovered performance from 21.0% $\pm$ 2.98% to 96.9% $\pm$ 0.52%. By validating that high-fidelity temporal embeddings can compensate for low spatial resolution, this work challenges the necessity of high-density sensing. The proposed framework offers a robust, cost-effective blueprint for next-generation prosthetic interfaces capable of rapid personalization.

</details>


### [771] [Internal Flow Signatures for Self-Checking and Refinement in LLMs](https://arxiv.org/abs/2602.01897)
*Sungheon Jeong,Sanggeon Yun,Ryozo Masukawa,Wenjun Haung,Hanning Chen,Mohsen Imani*

Main category: cs.LG

TL;DR: 提出内部流签名方法，通过监控LLM深度方向决策动态实现自我检查与精炼，无需修改基础模型


<details>
  <summary>Details</summary>
Motivation: 大型语言模型可能生成与上下文不忠实但流畅的回答，现有方法多依赖外部验证或生成后单独判断，需要更高效的内部自我检查机制

Method: 引入内部流签名：通过偏置中心监控稳定token级动态，在深度窗口内构建移动读取对齐子空间，使用正交传输对齐相邻窗口，提取可比较的传输步长、转向角和子空间漂移等特征，训练轻量GRU验证器进行自我检查

Result: 该方法不仅能检测不忠实生成，还能定位问题深度事件，支持针对性精炼：回滚到问题token并在识别块处钳制异常传输步长，同时保留正交残差

Conclusion: 内部流签名提供了从内部决策动态进行可操作定位和低开销自我检查的管道，代码已开源

Abstract: Large language models can generate fluent answers that are unfaithful to the provided context, while many safeguards rely on external verification or a separate judge after generation. We introduce \emph{internal flow signatures} that audit decision formation from depthwise dynamics at a fixed inter-block monitoring boundary. The method stabilizes token-wise motion via bias-centered monitoring, then summarizes trajectories in compact \emph{moving} readout-aligned subspaces constructed from the top token and its close competitors within each depth window. Neighboring window frames are aligned by an orthogonal transport, yielding depth-comparable transported step lengths, turning angles, and subspace drift summaries that are invariant to within-window basis choices. A lightweight GRU validator trained on these signatures performs self-checking without modifying the base model. Beyond detection, the validator localizes a culprit depth event and enables a targeted refinement: the model rolls back to the culprit token and clamps an abnormal transported step at the identified block while preserving the orthogonal residual. The resulting pipeline provides actionable localization and low-overhead self-checking from internal decision dynamics. \emph{Code is available at} \texttt{github.com/EavnJeong/Internal-Flow-Signatures-for-Self-Checking-and-Refinement-in-LLMs}.

</details>


### [772] [Towards Long-Horizon Interpretability: Efficient and Faithful Multi-Token Attribution for Reasoning LLMs](https://arxiv.org/abs/2602.01914)
*Wenbo Pan,Zhichao Liu,Xianlong Wang,Haining Yu,Xiaohua Jia*

Main category: cs.LG

TL;DR: FlashTrace：一种高效的多token归因方法，通过跨token聚合和递归归因机制，解决长上下文和多步推理场景下的效率和忠实性问题


<details>
  <summary>Details</summary>
Motivation: 现有token归因方法面临两大挑战：1) 效率瓶颈：在N长度上下文中归因M个token需要O(M*N)操作，长上下文归因极其缓慢；2) 忠实性下降：中间推理token会吸收归因权重，阻止重要性传播回原始输入

Method: 提出FlashTrace方法：1) 采用跨token聚合技术，单次计算多token目标的归因；2) 设计递归归因机制，通过中间推理链追踪重要性回到源输入；3) 保持归因的忠实性

Result: 在长上下文检索(RULER)和多步推理(MATH, MorehopQA)任务上的实验表明，FlashTrace相比现有基线获得超过130倍的加速，同时保持更优的忠实性。递归归因分析显示，即使单次递归跳转也能通过追踪推理链提高忠实性

Conclusion: FlashTrace成功解决了LLM归因中的效率和忠实性问题，为长上下文和多步推理场景提供了高效的归因解决方案，递归归因机制有效追踪了推理链中的重要性传播

Abstract: Token attribution methods provide intuitive explanations for language model outputs by identifying causally important input tokens. However, as modern LLMs increasingly rely on extended reasoning chains, existing schemes face two critical challenges: (1) efficiency bottleneck, where attributing a target span of M tokens within a context of length N requires O(M*N) operations, making long-context attribution prohibitively slow; and (2) faithfulness drop, where intermediate reasoning tokens absorb attribution mass, preventing importance from propagating back to the original input. To address these, we introduce FlashTrace, an efficient multi-token attribution method that employs span-wise aggregation to compute attribution over multi-token targets in a single pass, while maintaining faithfulness. Moreover, we design a recursive attribution mechanism that traces importance through intermediate reasoning chains back to source inputs. Extensive experiments on long-context retrieval (RULER) and multi-step reasoning (MATH, MorehopQA) tasks demonstrate that FlashTrace achieves over 130x speedup over existing baselines while maintaining superior faithfulness. We further analyze the dynamics of recursive attribution, showing that even a single recursive hop improves faithfulness by tracing importance through the reasoning chain.

</details>


### [773] [VLM-Guided Experience Replay](https://arxiv.org/abs/2602.01915)
*Elad Sharony,Tom Jurgenson,Orr Krupnik,Dotan Di Castro,Shie Mannor*

Main category: cs.LG

TL;DR: 使用视觉语言模型（VLM）指导强化学习回放缓冲区的经验优先级排序，无需微调，在游戏和机器人任务中显著提升成功率和样本效率


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型和视觉语言模型已被整合到强化学习的多个组件中，但回放缓冲区这一存储和重用经验的核心组件尚未被探索。本文旨在填补这一空白，利用VLM的语义和多模态推理能力来指导经验优先级排序。

Method: 使用预训练的冻结VLM作为自动评估器，识别并优先处理智能体经验中有前景的子轨迹。该方法无需微调VLM，直接利用其语义理解能力来指导回放缓冲区的经验选择。

Result: 在游戏和机器人场景中（包括离散和连续领域），使用该方法训练的智能体相比先前方法实现了11-52%的平均成功率提升，样本效率提高了19-45%。

Conclusion: VLM可以有效地指导强化学习回放缓冲区的经验优先级排序，显著提升智能体性能和学习效率，为将多模态模型整合到强化学习核心组件开辟了新方向。

Abstract: Recent advances in Large Language Models (LLMs) and Vision-Language Models (VLMs) have enabled powerful semantic and multimodal reasoning capabilities, creating new opportunities to enhance sample efficiency, high-level planning, and interpretability in reinforcement learning (RL). While prior work has integrated LLMs and VLMs into various components of RL, the replay buffer, a core component for storing and reusing experiences, remains unexplored. We propose addressing this gap by leveraging VLMs to guide the prioritization of experiences in the replay buffer. Our key idea is to use a frozen, pre-trained VLM (requiring no fine-tuning) as an automated evaluator to identify and prioritize promising sub-trajectories from the agent's experiences. Across scenarios, including game-playing and robotics, spanning both discrete and continuous domains, agents trained with our proposed prioritization method achieve 11-52% higher average success rates and improve sample efficiency by 19-45% compared to previous approaches. https://esharony.me/projects/vlm-rb/

</details>


### [774] [PIMPC-GNN: Physics-Informed Multi-Phase Consensus Learning for Enhancing Imbalanced Node Classification in Graph Neural Networks](https://arxiv.org/abs/2602.01920)
*Abdul Joseph Fofanah,Lian Wen,David Chen*

Main category: cs.LG

TL;DR: PIMPC-GNN：一种用于不平衡节点分类的物理信息多相共识框架，通过热力学扩散、Kuramoto同步和谱嵌入三种互补动态，结合类别自适应集成权重和失衡感知损失，显著提升少数类召回率和平衡准确率。


<details>
  <summary>Details</summary>
Motivation: 图神经网络（GNNs）在类别不平衡设置中表现不佳，少数类别代表性不足，预测偏向多数类。现有方法难以有效处理图数据中的类别不平衡问题，需要一种能够整合多种物理动态的框架来改善少数类别的学习。

Method: 提出PIMPC-GNN框架，整合三种互补动态：1）热力学扩散：传播少数类标签以捕获长程依赖；2）Kuramoto同步：通过振荡共识对齐少数节点；3）谱嵌入：通过结构正则化分离类别。这些视角通过类别自适应集成权重结合，并使用失衡感知损失进行训练，该损失将平衡交叉熵与基于物理的约束耦合。

Result: 在五个基准数据集和不平衡比5-100的范围内，PIMPC-GNN优于16个最先进的基线方法，在少数类召回率上实现高达+12.7%的提升，在平衡准确率上实现高达+8.3%的提升。框架还提供了图学习中共识动态的可解释性见解。

Conclusion: PIMPC-GNN通过整合多种物理动态有效解决了图神经网络中的类别不平衡问题，不仅取得了显著的性能提升，还为图学习中的共识动态提供了可解释的见解。该框架为不平衡图学习提供了新的思路。

Abstract: Graph neural networks (GNNs) often struggle in class-imbalanced settings, where minority classes are under-represented and predictions are biased toward majorities. We propose \textbf{PIMPC-GNN}, a physics-informed multi-phase consensus framework for imbalanced node classification. Our method integrates three complementary dynamics: (i) thermodynamic diffusion, which spreads minority labels to capture long-range dependencies, (ii) Kuramoto synchronisation, which aligns minority nodes through oscillatory consensus, and (iii) spectral embedding, which separates classes via structural regularisation. These perspectives are combined through class-adaptive ensemble weighting and trained with an imbalance-aware loss that couples balanced cross-entropy with physics-based constraints. Across five benchmark datasets and imbalance ratios from 5-100, PIMPC-GNN outperforms 16 state-of-the-art baselines, achieving notable gains in minority-class recall (up to +12.7\%) and balanced accuracy (up to +8.3\%). Beyond empirical improvements, the framework also provides interpretable insights into consensus dynamics in graph learning. The code is available at \texttt{https://github.com/afofanah/PIMPC-GNN}.

</details>


### [775] [Embedding Learning on Multiplex Networks for Link Prediction](https://arxiv.org/abs/2602.01922)
*Orell Trautmann,Olaf Wolkenhauer,Clémence Réda*

Main category: cs.LG

TL;DR: 这篇综述论文回顾了多重网络上用于链接预测的嵌入学习方法，提出了新的分类法，解决了评估公平性和可重复性问题，并为有向多重网络提出了新的测试程序。


<details>
  <summary>Details</summary>
Motivation: 随着网络复杂性的增加（连接数量和交互类型的增长），多重网络上的嵌入学习变得越来越具有挑战性。现有模型需要系统性的分类和公平的评估方法，特别是在有向多重网络上的评估存在不足。

Method: 1. 提出细化的分类法，根据嵌入类型和技术对模型进行分类比较；2. 回顾并解决多重网络上嵌入学习评估的可重复性和公平性问题；3. 针对有向多重网络提出新颖且公平的测试程序。

Result: 建立了系统的模型分类框架，解决了评估中的公平性和可重复性问题，为有向多重网络提供了专门的评估方法，为开发更高效、可处理的嵌入学习方法奠定了基础。

Conclusion: 这篇综述是开发更高效、可处理的多重网络嵌入学习方法及其公平评估的关键一步，提供了模型评估指南，并对当前可用于多重网络下游分析的挑战和工具提供了有见地的视角。

Abstract: Over the past years, embedding learning on networks has shown tremendous results in link prediction tasks for complex systems, with a wide range of real-life applications. Learning a representation for each node in a knowledge graph allows us to capture topological and semantic information, which can be processed in downstream analyses later. In the link prediction task, high-dimensional network information is encoded into low-dimensional vectors, which are then fed to a predictor to infer new connections between nodes in the network. As the network complexity (that is, the numbers of connections and types of interactions) grows, embedding learning turns out increasingly challenging. This review covers published models on embedding learning on multiplex networks for link prediction. First, we propose refined taxonomies to classify and compare models, depending on the type of embeddings and embedding techniques. Second, we review and address the problem of reproducible and fair evaluation of embedding learning on multiplex networks for the link prediction task. Finally, we tackle evaluation on directed multiplex networks by proposing a novel and fair testing procedure. This review constitutes a crucial step towards the development of more performant and tractable embedding learning approaches for multiplex networks and their fair evaluation for the link prediction task. We also suggest guidelines on the evaluation of models, and provide an informed perspective on the challenges and tools currently available to address downstream analyses applied to multiplex networks.

</details>


### [776] [COLT: Lightweight Multi-LLM Collaboration through Shared MCTS Reasoning for Model Compilation](https://arxiv.org/abs/2602.01935)
*Annabelle Sujun Tang,Christopher Priebe,Lianhui Qin,Hadi Esmaeilzadeh*

Main category: cs.LG

TL;DR: COLT：一个轻量级多LLM协作框架，通过共享MCTS树实现编译器优化，用小模型为主、大模型为辅的策略匹配或超越单大模型性能


<details>
  <summary>Details</summary>
Motivation: 模型服务成本主导AI系统，编译器优化对可扩展部署至关重要。现有方法使用单个大语言模型指导编译器搜索成本高昂，而小模型单独使用可靠性不足。本文探索多LLM协作推理能否匹配或超越单个大模型的性能。

Method: 提出COLT框架，在单个蒙特卡洛树搜索（MCTS）过程中实现多模型协调推理。使用共享MCTS树作为协作基础，重用转换前缀和跨模型价值传播。每个迭代中，执行LLM提出联合动作：（编译器转换，下一个查询的模型）。引入模型感知树策略，偏向小模型同时保持探索，以及当搜索出现持续回归时升级到大模型的航向调整机制。

Result: 通过多LLM协作推理，主要依赖小LLM，能够匹配或超越单个大模型的性能，同时降低计算成本。

Conclusion: COLT框架证明多LLM协作推理在编译器优化中是有效的，通过轻量级MCTS循环内生的模型选择，避免了复杂的外部规划器和控制器，实现了成本效益的优化方案。

Abstract: Model serving costs dominate AI systems, making compiler optimization essential for scalable deployment. Recent works show that a large language model (LLM) can guide compiler search by reasoning over program structure and optimization history. However, using a single large model throughout the search is expensive, while smaller models are less reliable when used alone. Thus, this paper seeks to answer whether multi-LLM collaborative reasoning relying primarily on small LLMs can match or exceed the performance of a single large model. As such, we propose a lightweight collaborative multi-LLM framework, dubbed COLT, for compiler optimization that enables coordinated reasoning across multiple models within a single Monte Carlo tree search (MCTS) process. A key contribution is the use of a single shared MCTS tree as the collaboration substrate across LLMs, enabling the reuse of transformation prefixes and cross-model value propagation. Hence, we circumvent both heavy internal reasoning mechanisms and conventional agentic machinery that relies on external planners, multiple concurrent LLMs, databases, external memory/versioning of intermediate results, and controllers by simply endogenizing model selection within the lightweight MCTS optimization loop. Every iteration, the acting LLM proposes a joint action: (compiler transformation, model to be queried next). We also introduce a model-aware tree policy that biases search toward smaller models while preserving exploration, and a course-alteration mechanism that escalates to the largest model when the search exhibits persistent regressions attributable to smaller models.

</details>


### [777] [PIMCST: Physics-Informed Multi-Phase Consensus and Spatio-Temporal Few-Shot Learning for Traffic Flow Forecasting](https://arxiv.org/abs/2602.01936)
*Abdul Joseph Fofanah,Lian Wen,David Chen*

Main category: cs.LG

TL;DR: MCPST是一个用于少样本交通预测的多阶段共识时空框架，通过建模交通动态的扩散、同步和谱嵌入，结合自适应共识机制和结构化元学习，在数据稀缺的跨域场景中实现准确预测。


<details>
  <summary>Details</summary>
Motivation: 智能交通系统中，跨域数据稀缺场景下的准确交通流预测面临挑战。历史数据有限、复杂的时空依赖性和非线性动态使得不同城市间的少样本学习变得困难。

Method: 提出MCPST框架，包含三个核心创新：1）多阶段引擎，通过扩散、同步和谱嵌入建模交通动态；2）自适应共识机制，动态融合各阶段预测并保持一致性；3）结构化元学习策略，实现对新城市的快速适应。

Result: 在四个真实数据集上，MCPST超越了14种最先进的时空图学习方法、动态图迁移学习方法、基于提示的时空预测方法和跨域少样本方法，提高了预测精度，减少了所需训练数据，并提供可解释的见解。

Conclusion: MCPST通过将交通预测重新定义为多阶段共识学习问题，在少样本跨域交通预测中取得了显著效果，提供了理论保证和实际性能优势。

Abstract: Accurate traffic flow prediction remains a fundamental challenge in intelligent transportation systems, particularly in cross-domain, data-scarce scenarios where limited historical data hinders model training and generalisation. The complex spatio-temporal dependencies and nonlinear dynamics of urban mobility networks further complicate few-shot learning across different cities. This paper proposes MCPST, a novel Multi-phase Consensus Spatio-Temporal framework for few-shot traffic forecasting that reconceptualises traffic prediction as a multi-phase consensus learning problem. Our framework introduces three core innovations: (1) a multi-phase engine that models traffic dynamics through diffusion, synchronisation, and spectral embeddings for comprehensive dynamic characterisation; (2) an adaptive consensus mechanism that dynamically fuses phase-specific predictions while enforcing consistency; and (3) a structured meta-learning strategy for rapid adaptation to new cities with minimal data. We establish extensive theoretical guarantees, including representation theorems with bounded approximation errors and generalisation bounds for few-shot adaptation. Through experiments on four real-world datasets, MCPST outperforms fourteen state-of-the-art methods in spatio-temporal graph learning methods, dynamic graph transfer learning methods, prompt-based spatio-temporal prediction methods and cross-domain few-shot settings, improving prediction accuracy while reducing required training data and providing interpretable insights. The implementation code is available at https://github.com/afofanah/MCPST.

</details>


### [778] [T-LLM: Teaching Large Language Models to Forecast Time Series via Temporal Distillation](https://arxiv.org/abs/2602.01937)
*Suhan Guo,Bingxu Wang,Shaodan Zhang,Furao Shen*

Main category: cs.LG

TL;DR: T-LLM：通过时间蒸馏框架，将时间序列预测能力从轻量级时间教师模型转移到通用大语言模型，在推理时仅使用LLM进行预测


<details>
  <summary>Details</summary>
Motivation: 时间序列数据受时间约束，无法像视觉和语言数据那样通过规模驱动的预训练有效获取预测能力。现有方法主要依赖表示级对齐或推理时时间模块，未能明确教授LLM预测行为

Method: 提出T-LLM时间蒸馏框架：使用结合趋势建模和频域分析的轻量级时间教师模型提供结构化时间监督，在训练期间将预测行为转移到通用LLM，推理时完全移除教师模型

Result: 在基准数据集和传染病预测任务上，T-LLM在全样本、少样本和零样本设置下均优于现有基于LLM的预测方法，同时实现了简单高效的部署流程

Conclusion: T-LLM框架成功将时间序列预测能力赋予通用LLM，解决了时间约束下的预测挑战，为LLM在时间序列分析领域的应用提供了有效方案

Abstract: Time series forecasting plays a critical role in decision-making across many real-world applications. Unlike data in vision and language domains, time series data is inherently tied to the evolution of underlying processes and can only accumulate as real-world time progresses, limiting the effectiveness of scale-driven pretraining alone. This time-bound constraint poses a challenge for enabling large language models (LLMs) to acquire forecasting capability, as existing approaches primarily rely on representation-level alignment or inference-time temporal modules rather than explicitly teaching forecasting behavior to the LLM. We propose T-LLM, a temporal distillation framework that equips general-purpose LLMs with time series forecasting capability by transferring predictive behavior from a lightweight temporal teacher during training. The teacher combines trend modeling and frequency-domain analysis to provide structured temporal supervision, and is removed entirely at inference, leaving the LLM as the sole forecasting model. Experiments on benchmark datasets and infectious disease forecasting tasks demonstrate that T-LLM consistently outperforms existing LLM-based forecasting methods under full-shot, few-shot, and zero-shot settings, while enabling a simple and efficient deployment pipeline.

</details>


### [779] [Boundary-Constrained Diffusion Models for Floorplan Generation: Balancing Realism and Diversity](https://arxiv.org/abs/2602.01949)
*Leonardo Stoppani,Davide Bacciu,Shahab Mokarizadeh*

Main category: cs.LG

TL;DR: 提出多样性评分（DS）来量化固定约束下的布局多样性，并引入边界交叉注意力（BCA）模块提升几何一致性，揭示了真实性与多样性之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散模型的自动平面图生成虽然能产生逼真布局，但优化FID等感知指标会导致设计多样性受限，且缺乏对几何一致性的有效控制。

Method: 1) 提出多样性评分（DS）来量化固定约束下的布局多样性；2) 引入边界交叉注意力（BCA）模块，使模型能够以建筑边界为条件进行生成。

Result: BCA显著提升了边界遵循度；长时间训练会导致多样性崩溃，而FID无法诊断此问题；OOD评估显示模型依赖数据集先验，揭示了真实性、多样性和泛化能力之间的权衡。

Conclusion: 在建筑设计任务中，需要明确平衡保真度、多样性和泛化能力的生成系统，仅优化感知指标不足以实现多样且一致的布局生成。

Abstract: Diffusion models have become widely popular for automated floorplan generation, producing highly realistic layouts conditioned on user-defined constraints. However, optimizing for perceptual metrics such as the Fréchet Inception Distance (FID) causes limited design diversity. To address this, we propose the Diversity Score (DS), a metric that quantifies layout diversity under fixed constraints. Moreover, to improve geometric consistency, we introduce a Boundary Cross-Attention (BCA) module that enables conditioning on building boundaries. Our experiments show that BCA significantly improves boundary adherence, while prolonged training drives diversity collapse undiagnosed by FID, revealing a critical trade-off between realism and diversity. Out-Of-Distribution evaluations further demonstrate the models' reliance on dataset priors, emphasizing the need for generative systems that explicitly balance fidelity, diversity, and generalization in architectural design tasks.

</details>


### [780] [Efficient Epistemic Uncertainty Estimation for Large Language Models via Knowledge Distillation](https://arxiv.org/abs/2602.01956)
*Seonghyeon Park,Jewon Yeom,Jaewon Sok,Jeongjae Park,Heejun Kim,Taesup Kim*

Main category: cs.LG

TL;DR: 提出利用小型草稿模型高效估计LLM认知不确定性的框架，避免大规模集成计算，在保持性能的同时显著降低推理成本


<details>
  <summary>Details</summary>
Motivation: LLM的不确定性量化对于减轻幻觉和安全关键任务部署至关重要，但通过深度集成估计认知不确定性在大型模型上计算成本过高

Method: 基于偏差-方差分解理论框架，使用草稿模型的Jensen-Shannon散度作为方差代理，草稿混合与目标模型的KL散度作为偏差代理，并引入在线随机蒸馏和数据集多样化草稿策略

Result: 在GSM8K上，估计误差(RMSE)相比基线降低37%，幻觉检测性能与高成本扰动方法相当，推理成本可忽略

Conclusion: 该方法为不确定性感知的LLM部署提供了实用解决方案，在保持高性能的同时显著降低了计算成本

Abstract: Quantifying uncertainty in Large Language Models (LLMs) is essential for mitigating hallucinations and enabling risk-aware deployment in safety-critical tasks. However, estimating Epistemic Uncertainty(EU) via Deep Ensembles is computationally prohibitive at the scale of modern models. We propose a framework that leverages the small draft models to efficiently estimate token-level EU, bypassing the need for full-scale ensembling. Theoretically grounded in a Bias-Variance Decomposition, our approach approximates EU via Jensen-Shannon divergence among drafts (variance proxy) and KL divergence between the draft mixture and the target (bias proxy). To further ensure accuracy without significant overhead, we introduce Online Stochastic Distillation (OSD) to efficiently approximate target aggregation and the Data-Diverse Drafts (DDD) strategy to enhance draft diversity for better target approximation. Extensive experiments on GSM8K demonstrate that our method reduces the estimation error (RMSE) by up to 37% compared to baselines. Crucially, our approach achieves Hallucination Detection performance competitive with heavy perturbation-based methods like TokUR while incurring negligible inference costs, offering a practical solution for uncertainty-aware LLM deployment.

</details>


### [781] [Grounding Generated Videos in Feasible Plans via World Models](https://arxiv.org/abs/2602.01960)
*Christos Ziakas,Amir Bar,Alessandra Russo*

Main category: cs.LG

TL;DR: GVP-WM：一种通过世界模型将视频生成计划接地到可行动作序列的规划方法，解决视频生成计划违反物理约束的问题


<details>
  <summary>Details</summary>
Motivation: 大规模视频生成模型作为零样本视觉规划器显示出潜力，但视频生成的计划经常违反时间一致性和物理约束，导致映射到可执行动作时失败

Method: GVP-WM首先从初始和目标观察生成视频计划，然后通过视频引导的潜在配准将视频指导投影到动态可行的潜在轨迹流形上。将接地问题表述为目标条件潜在空间轨迹优化问题，在世界模型动态下联合优化潜在状态和动作，同时保持与视频生成计划的语义对齐

Result: GVP-WM能够从违反物理约束的零样本图像到视频生成和运动模糊视频中恢复可行的长时程计划，在导航和操作模拟任务中表现良好

Conclusion: GVP-WM通过世界模型将视频生成计划接地到可行动作序列，有效解决了视频规划违反物理约束的问题，为视觉规划提供了更可靠的解决方案

Abstract: Large-scale video generative models have shown emerging capabilities as zero-shot visual planners, yet video-generated plans often violate temporal consistency and physical constraints, leading to failures when mapped to executable actions. To address this, we propose Grounding Video Plans with World Models (GVP-WM), a planning method that grounds video-generated plans into feasible action sequences using a learned action-conditioned world model. At test-time, GVP-WM first generates a video plan from initial and goal observations, then projects the video guidance onto the manifold of dynamically feasible latent trajectories via video-guided latent collocation. In particular, we formulate grounding as a goal-conditioned latent-space trajectory optimization problem that jointly optimizes latent states and actions under world-model dynamics, while preserving semantic alignment with the video-generated plan. Empirically, GVP-WM recovers feasible long-horizon plans from zero-shot image-to-video-generated and motion-blurred videos that violate physical constraints, across navigation and manipulation simulation tasks.

</details>


### [782] [Zero-Shot Off-Policy Learning](https://arxiv.org/abs/2602.01962)
*Arip Asadulaev,Maksim Bobrin,Salem Lahlou,Dmitry Dylov,Fakhri Karray,Martin Takac*

Main category: cs.LG

TL;DR: 提出一种零样本强化学习方法，通过后继度量与平稳密度比的理论联系，实现无需额外训练即可适应新任务的离线策略学习


<details>
  <summary>Details</summary>
Motivation: 解决离线策略学习中的分布偏移和值函数高估偏差问题，特别是在零样本强化学习场景中，智能体需要在测试时无需额外训练就能适应新任务

Method: 发现后继度量与平稳密度比的理论联系，基于此设计算法推断最优重要性采样比，实现平稳分布校正，可在前向-后向表示框架中无缝集成

Result: 在SMPL Humanoid运动跟踪、ExoRL连续控制和长时域OGBench任务上进行了基准测试，实现了训练免费机制下的快速任务适应

Conclusion: 该工作桥接了离线策略学习和零样本适应，为两个研究领域都带来了益处，提供了一种无需额外训练就能适应新任务的解决方案

Abstract: Off-policy learning methods seek to derive an optimal policy directly from a fixed dataset of prior interactions. This objective presents significant challenges, primarily due to the inherent distributional shift and value function overestimation bias. These issues become even more noticeable in zero-shot reinforcement learning, where an agent trained on reward-free data must adapt to new tasks at test time without additional training. In this work, we address the off-policy problem in a zero-shot setting by discovering a theoretical connection of successor measures to stationary density ratios. Using this insight, our algorithm can infer optimal importance sampling ratios, effectively performing a stationary distribution correction with an optimal policy for any task on the fly. We benchmark our method in motion tracking tasks on SMPL Humanoid, continuous control on ExoRL, and for the long-horizon OGBench tasks. Our technique seamlessly integrates into forward-backward representation frameworks and enables fast-adaptation to new tasks in a training-free regime. More broadly, this work bridges off-policy learning and zero-shot adaptation, offering benefits to both research areas.

</details>


### [783] [Self-Consolidation for Self-Evolving Agents](https://arxiv.org/abs/2602.01966)
*Hongzhuo Yu,Fei Zhu,Guo-Sen Xie,Ling Shao*

Main category: cs.LG

TL;DR: 提出了一种新型的LLM智能体自进化框架，通过对比反思策略总结错误模式，并通过自整合机制将文本经验蒸馏为可学习参数，实现智能体的长期进化。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体通常是静态系统，缺乏通过终身交互进化的能力。现有方法主要依赖检索成功轨迹作为演示，但存在两个关键限制：1）只关注成功，忽略了失败尝试中的教学价值；2）持续积累文本经验会增加检索时间、引入噪声并耗尽上下文窗口。

Method: 提出包含两个互补进化机制的自进化框架：1）对比反思策略，显式总结易错模式并捕获可重用见解；2）自整合机制，将非参数化文本经验蒸馏为紧凑的可学习参数，使智能体能够将大量历史经验内化到其潜在空间中。

Result: 大量实验证明了该方法在长期智能体进化方面的优势。

Conclusion: 该框架通过利用失败经验和参数化知识整合，实现了LLM智能体的有效自我进化，解决了现有方法在经验利用和扩展性方面的局限性。

Abstract: While large language model (LLM) agents have demonstrated impressive problem-solving capabilities, they typically operate as static systems, lacking the ability to evolve through lifelong interaction. Existing attempts to bridge this gap primarily rely on retrieving successful past trajectories as demonstrations. However, this paradigm faces two critical limitations. First, by focusing solely on success, agents overlook the rich pedagogical value embedded in failed attempts, preventing them from identifying and avoiding recurrent pitfalls. Second, continually accumulating textual experiences not only increases the time consumption during retrieval but also inevitably introduces noise and exhausts the largest context window of current LLMs. To address these challenges, we propose a novel self-evolving framework for LLM agents that introduces a complementary evolution mechanism: First, a contrastive reflection strategy is introduced to explicitly summarize error-prone patterns and capture reusable insights. Second, we propose a self-consolidation mechanism that distills non-parametric textual experience into compact learnable parameters. This enables the agent to internalize extensive historical experience directly into its latent space. Extensive experiments demonstrate the advantages of our method in long-term agent evolution.

</details>


### [784] [IntraSlice: Towards High-Performance Structural Pruning with Block-Intra PCA for LLMs](https://arxiv.org/abs/2602.01975)
*Meng Li,Peisong Wang,Yuantian Shao,Qinghao Hu,Hongjian Fang,Yifan Zhang,Zhihui Wei,Jian Cheng*

Main category: cs.LG

TL;DR: IntraSlice：一种基于模块内PCA压缩剪枝的框架，通过近似PCA方法实现无额外参数的矩阵融合，结合全局剪枝比例估计器，在保持性能的同时实现高效模型压缩。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）虽然性能强大但参数量巨大，部署困难。结构化剪枝能加速但会导致显著性能下降。现有的PCA剪枝方法虽然缓解了性能问题，但只能在模块间应用，需要额外参数，且残差连接会严重破坏激活分布。

Method: 提出IntraSlice框架，采用模块内块级PCA压缩剪枝。利用Transformer模块的结构特性，设计近似PCA方法，其变换矩阵可完全融合到模型中而不增加额外参数。引入基于PCA的全局剪枝比例估计器，在传统模块重要性的基础上进一步考虑压缩激活的分布。

Result: 在Llama2、Llama3和Phi系列模型上验证，在多种语言基准测试中，相比现有基线方法，在相同压缩比或推理速度下实现了更优的压缩性能。

Conclusion: IntraSlice通过模块内PCA压缩剪枝和全局剪枝比例估计，有效解决了现有PCA剪枝方法的局限性，实现了高性能的LLM压缩，为大规模语言模型的高效部署提供了有效方案。

Abstract: Large Language Models (LLMs) achieve strong performance across diverse tasks but face deployment challenges due to their massive size. Structured pruning offers acceleration benefits but leads to significant performance degradation. Recent PCA-based pruning methods have alleviated this issue by retaining key activation components, but are only applied between modules in order to fuse the transformation matrix, which introduces extra parameters and severely disrupts activation distributions due to residual connections. To address these issues, we propose IntraSlice, a framework that applies block-wise module-intra PCA compression pruning. By leveraging the structural characteristics of Transformer modules, we design an approximate PCA method whose transformation matrices can be fully fused into the model without additional parameters. We also introduce a PCA-based global pruning ratio estimator that further considers the distribution of compressed activations, building on conventional module importance. We validate our method on Llama2, Llama3, and Phi series across various language benchmarks. Experimental results demonstrate that our approach achieves superior compression performance compared to recent baselines at the same compression ratio or inference speed.

</details>


### [785] [FlyPrompt: Brain-Inspired Random-Expanded Routing with Temporal-Ensemble Experts for General Continual Learning](https://arxiv.org/abs/2602.01976)
*Hongwei Yan,Guanglong Sun,Kanglei Zhou,Qian Li,Liyuan Wang,Yi Zhong*

Main category: cs.LG

TL;DR: FlyPrompt是一个受果蝇大脑启发的通用持续学习框架，通过专家路由和专家能力改进解决单次学习、无任务边界的持续学习问题，在多个数据集上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 通用持续学习(GCL)面临单次学习、非平稳数据流且无明确任务边界的挑战。现有持续参数高效调优(PET)方法通常依赖多轮训练和明确任务提示，在GCL场景下效果有限，且缺乏针对专家参数分配和表示能力提升的系统设计。

Method: 受果蝇分层记忆系统（稀疏扩展和模块化集成）启发，FlyPrompt将GCL分解为专家路由和专家能力改进两个子问题：1）随机扩展分析路由器实现实例级专家激活；2）输出头的时序集成动态调整决策边界。

Result: 在CIFAR-100、ImageNet-R和CUB-200数据集上，FlyPrompt分别取得11.23%、12.43%和7.62%的性能提升，显著超越现有最先进方法。

Conclusion: FlyPrompt通过脑启发设计有效解决了持续PET中的专家分配和表示能力挑战，为通用持续学习提供了高效解决方案，代码已开源。

Abstract: General continual learning (GCL) challenges intelligent systems to learn from single-pass, non-stationary data streams without clear task boundaries. While recent advances in continual parameter-efficient tuning (PET) of pretrained models show promise, they typically rely on multiple training epochs and explicit task cues, limiting their effectiveness in GCL scenarios. Moreover, existing methods often lack targeted design and fail to address two fundamental challenges in continual PET: how to allocate expert parameters to evolving data distributions, and how to improve their representational capacity under limited supervision. Inspired by the fruit fly's hierarchical memory system characterized by sparse expansion and modular ensembles, we propose FlyPrompt, a brain-inspired framework that decomposes GCL into two subproblems: expert routing and expert competence improvement. FlyPrompt introduces a randomly expanded analytic router for instance-level expert activation and a temporal ensemble of output heads to dynamically adapt decision boundaries over time. Extensive theoretical and empirical evaluations demonstrate FlyPrompt's superior performance, achieving up to 11.23%, 12.43%, and 7.62% gains over state-of-the-art baselines on CIFAR-100, ImageNet-R, and CUB-200, respectively. Our source code is available at https://github.com/AnAppleCore/FlyGCL.

</details>


### [786] [SAME: Stabilized Mixture-of-Experts for Multimodal Continual Instruction Tuning](https://arxiv.org/abs/2602.01990)
*Zhen-Hao Xie,Jun-Tao Tang,Yu-Cheng Shi,Han-Jia Ye,De-Chuan Zhan,Da-Wei Zhou*

Main category: cs.LG

TL;DR: SAME方法通过正交子空间分解稳定专家选择，利用历史输入协方差进行曲率感知缩放来调节专家更新，并引入自适应专家激活机制，解决了多模态持续指令调优中的路由漂移和专家漂移问题。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型通过指令调优获得强大性能，但实际部署需要持续扩展能力，多模态持续指令调优变得至关重要。现有基于稀疏专家路由的方法存在两个问题：1) 路由漂移 - 随着数据分布变化，专家选择变得不一致；2) 专家漂移 - 共享专家被新任务覆盖而失去原有功能。

Method: 提出SAME方法：1) 通过将路由动态分解为正交子空间，仅更新任务相关方向来稳定专家选择；2) 利用历史输入协方差进行无排练的曲率感知缩放来调节专家更新；3) 引入自适应专家激活机制，在训练期间冻结选定专家，减少冗余计算和跨任务干扰。

Result: 大量实验证明SAME方法在多模态持续指令调优中取得了最先进的性能。

Conclusion: SAME方法通过稳定专家路由和调节专家更新，有效解决了多模态持续指令调优中的路由漂移和专家漂移问题，为实际部署提供了有效的解决方案。

Abstract: Multimodal Large Language Models (MLLMs) achieve strong performance through instruction tuning, but real-world deployment requires them to continually expand their capabilities, making Multimodal Continual Instruction Tuning (MCIT) essential. Recent methods leverage sparse expert routing to promote task specialization, but we find that the expert routing process suffers from drift as the data distribution evolves. For example, a grounding query that previously activated localization experts may instead be routed to irrelevant experts after learning OCR tasks. Meanwhile, the grounding-related experts can be overwritten by new tasks and lose their original functionality. Such failure reflects two problems: router drift, where expert selection becomes inconsistent over time, and expert drift, where shared experts are overwritten across tasks. Therefore, we propose StAbilized Mixture-of-Experts (SAME) for MCIT. To address router drift, SAME stabilizes expert selection by decomposing routing dynamics into orthogonal subspaces and updating only task-relevant directions. To mitigate expert drift, we regulate expert updates via curvature-aware scaling using historical input covariance in a rehearsal-free manner. SAME also introduces adaptive expert activation to freeze selected experts during training, reducing redundant computation and cross-task interference. Extensive experiments demonstrate its SOTA performance.

</details>


### [787] [Optimizing Tensor Train Decomposition in DNNs for RISC-V Architectures Using Design Space Exploration and Compiler Optimizations](https://arxiv.org/abs/2602.01996)
*Theologos Anthimopoulos,Milad Kokhazadeh,Vasilios Kelefouras,Benjamin Himpel,Georgios Keramidas*

Main category: cs.LG

TL;DR: 本文提出了一种针对RISC-V处理器的端到端低秩分解设计空间探索方法和专用工具，通过Tensor Train分解压缩全连接层，优化边缘设备上的DNN部署。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的RISC-V平台上部署深度神经网络面临挑战，因为全连接层计算和内存需求高。低秩分解虽然能压缩模型，但其设计空间复杂，需要在FLOPs、内存大小、推理时间和准确度之间权衡，过程耗时且困难。

Method: 使用TensorFlow T3F库的Tensor Train分解技术，通过两步剪枝策略：1)排除低效分解形状；2)排除在RISC-V架构上推理性能差的方案。然后应用编译器优化提升自定义T3F层性能，最小化推理时间并提高计算效率。

Result: 平均而言，TT分解层比IREE快3倍，比Pluto快8倍（在相同压缩模型上）。该方法为基于RISC-V架构的边缘和嵌入式设备部署DNN提供了高效解决方案。

Conclusion: 该工作提供了一种端到端的低秩分解设计空间探索方法和专用工具，能够有效优化RISC-V处理器上的全连接层，显著提升推理性能，为资源受限设备上的DNN部署提供了实用解决方案。

Abstract: Deep neural networks (DNNs) have become indispensable in many real-life applications like natural language processing, and autonomous systems. However, deploying DNNs on resource-constrained devices, e.g., in RISC-V platforms, remains challenging due to the high computational and memory demands of fully connected (FC) layers, which dominate resource consumption. Low-rank factorization (LRF) offers an effective approach to compressing FC layers, but the vast design space of LRF solutions involves complex trade-offs among FLOPs, memory size, inference time, and accuracy, making the LRF process complex and time-consuming. This paper introduces an end-to-end LRF design space exploration methodology and a specialized design tool for optimizing FC layers on RISC-V processors. Using Tensor Train Decomposition (TTD) offered by TensorFlow T3F library, the proposed work prunes the LRF design space by excluding first, inefficient decomposition shapes and second, solutions with poor inference performance on RISC-V architectures. Compiler optimizations are then applied to enhance custom T3F layer performance, minimizing inference time and boosting computational efficiency. On average, our TT-decomposed layers run 3x faster than IREE and 8x faster than Pluto on the same compressed model. This work provides an efficient solution for deploying DNNs on edge and embedded devices powered by RISC-V architectures.

</details>


### [788] [On the Limits of Layer Pruning for Generative Reasoning in LLMs](https://arxiv.org/abs/2602.01997)
*Safal Shrestha,Anubhav Shrestha,Aadim Nepal,Minwu Kim,Keith Ross*

Main category: cs.LG

TL;DR: 层剪枝能压缩大语言模型并保持分类任务性能，但对生成式推理任务效果差，特别是多步推理任务。通过监督微调与自生成响应可部分恢复性能，但生成式推理的恢复仍有限制。


<details>
  <summary>Details</summary>
Motivation: 现有层剪枝技术在压缩大语言模型时，虽然能保持分类任务性能，但在生成式推理任务上表现严重退化。研究旨在探索层剪枝对生成式推理任务的影响，并寻找在有限后训练资源下的有效缓解策略。

Method: 在多个模型家族中进行系统研究，评估层剪枝对多步推理任务的影响。提出基于监督微调与自生成响应的简单缓解策略，在有限后训练资源下（无需预训练规模的数据或计算）进行实验。

Result: 分类任务可恢复高达90%的基线性能，生成式基准测试相比先前后剪枝技术提升20-30个百分点。但生成式推理的恢复仍有限制，主要适用于较低剪枝比例。

Conclusion: 层剪枝对生成式推理存在实际限制，深度缩减主要在较低剪枝比例下有效。研究为约束后训练机制下应用深度缩减提供了指导。

Abstract: Recent works have shown that layer pruning can compress large language models (LLMs) while retaining strong performance on classification benchmarks with little or no finetuning. However, existing pruning techniques often suffer severe degradation on generative reasoning tasks. Through a systematic study across multiple model families, we find that tasks requiring multi-step reasoning are particularly sensitive to depth reduction. Beyond surface-level text degeneration, we observe degradation of critical algorithmic capabilities, including arithmetic computation for mathematical reasoning and balanced parenthesis generation for code synthesis. Under realistic post-training constraints, without access to pretraining-scale data or compute, we evaluate a simple mitigation strategy based on supervised finetuning with Self-Generated Responses. This approach achieves strong recovery on classification tasks, retaining up to 90\% of baseline performance, and yields substantial gains of up to 20--30 percentage points on generative benchmarks compared to prior post-pruning techniques. Crucially, despite these gains, recovery for generative reasoning remains fundamentally limited relative to classification tasks and is viable primarily at lower pruning ratios. Overall, we characterize the practical limits of layer pruning for generative reasoning and provide guidance on when depth reduction can be applied effectively under constrained post-training regimes.

</details>


### [789] [Preserve-Then-Quantize: Balancing Rank Budgets for Quantization Error Reconstruction in LLMs](https://arxiv.org/abs/2602.02001)
*Yoonjun Cho,Dongjae Jeon,Soeun Kim,Moongyu Jeon,Albert No*

Main category: cs.LG

TL;DR: SRR提出结构化残差重建框架，在量化前保留权重的主要奇异子空间，仅量化残差部分，并用剩余秩进行误差重建，平衡量化暴露能量与不可恢复误差，同时支持量化参数高效微调。


<details>
  <summary>Details</summary>
Motivation: 现有量化误差重建方法将全部秩预算用于误差重建，这在权重具有内在低秩结构且量化破坏主导方向时是次优的。需要更智能的秩分配策略来平衡权重结构保留与量化误差重建。

Method: 提出结构化残差重建(SRR)：1) 保留激活缩放权重的前k个奇异子空间；2) 仅量化残差部分；3) 使用剩余秩r-k进行误差重建。推导理论指导的k选择准则，平衡量化暴露能量与秩约束下的不可恢复误差。该参数化自然支持量化参数高效微调(QPEFT)，并通过梯度缩放稳定微调。

Result: 实验表明，在多种模型和量化设置下，SRR在PTQ中持续降低困惑度。在2位量化参数高效微调(QPEFT)下，GLUE任务平均提升5.9个百分点。

Conclusion: SRR通过结构化秩分配，在保留权重重要结构的同时有效重建量化误差，优于传统全秩误差重建方法，并为量化模型微调提供了稳定高效的框架。

Abstract: Quantization Error Reconstruction (QER) reduces accuracy loss in Post-Training Quantization (PTQ) by approximating weights as $\mathbf{W} \approx \mathbf{Q} + \mathbf{L}\mathbf{R}$, using a rank-$r$ correction to reconstruct quantization error. Prior methods devote the full rank budget to error reconstruction, which is suboptimal when $\mathbf{W}$ has intrinsic low-rank structure and quantization corrupts dominant directions. We propose Structured Residual Reconstruction (SRR), a rank-allocation framework that preserves the top-$k$ singular subspace of the activation-scaled weight before quantization, quantizes only the residual, and uses the remaining rank $r-k$ for error reconstruction. We derive a theory-guided criterion for selecting $k$ by balancing quantization-exposed energy and unrecoverable error under rank constraints. We further show that resulting $\mathbf{Q} + \mathbf{L}\mathbf{R}$ parameterization naturally supports Quantized Parameter-Efficient Fine-Tuning (QPEFT), and stabilizes fine-tuning via gradient scaling along preserved directions. Experiments demonstrate consistent perplexity reductions across diverse models and quantization settings in PTQ, along with a 5.9 percentage-point average gain on GLUE under 2-bit QPEFT.

</details>


### [790] [Logic-Guided Vector Fields for Constrained Generative Modeling](https://arxiv.org/abs/2602.02009)
*Ali Baheri*

Main category: cs.LG

TL;DR: LGVF是一种神经符号框架，通过逻辑约束指导流匹配生成模型，在训练时使用逻辑损失惩罚约束违反，在推理时使用约束梯度调整采样，显著减少约束违反并保持分布保真度。


<details>
  <summary>Details</summary>
Motivation: 神经符号系统结合了符号逻辑的表达结构和神经学习的灵活性，但现有生成模型缺乏在生成时强制执行声明性约束的机制。需要一种方法将符号知识注入生成过程，确保输出满足逻辑约束。

Method: 提出Logic-Guided Vector Fields (LGVF)框架：1) 训练时使用逻辑损失函数，沿连续流轨迹惩罚约束违反，权重强调目标分布附近的正确性；2) 推理时使用约束梯度调整采样，作为对学习动力学的轻量级逻辑修正。

Result: 在三个约束生成案例（线性、非线性和多区域可行性约束）中，LGVF相比标准流匹配将约束违反减少59-82%，在每种情况下都达到最低违反率。在线性和环形设置中，MMD测量显示分布保真度也得到改善。

Conclusion: LGVF成功将符号约束注入生成模型，显著减少约束违反，同时保持或改善分布质量。框架产生具有障碍物规避行为的约束感知向量场，无需显式路径规划即可绕开禁止区域，展示了神经符号方法的潜力。

Abstract: Neuro-symbolic systems aim to combine the expressive structure of symbolic logic with the flexibility of neural learning; yet, generative models typically lack mechanisms to enforce declarative constraints at generation time. We propose Logic-Guided Vector Fields (LGVF), a neuro-symbolic framework that injects symbolic knowledge, specified as differentiable relaxations of logical constraints, into flow matching generative models. LGVF couples two complementary mechanisms: (1) a training-time logic loss that penalizes constraint violations along continuous flow trajectories, with weights that emphasize correctness near the target distribution; and (2) an inference-time adjustment that steers sampling using constraint gradients, acting as a lightweight, logic-informed correction to the learned dynamics. We evaluate LGVF on three constrained generation case studies spanning linear, nonlinear, and multi-region feasibility constraints. Across all settings, LGVF reduces constraint violations by 59-82% compared to standard flow matching and achieves the lowest violation rates in each case. In the linear and ring settings, LGVF also improves distributional fidelity as measured by MMD, while in the multi-obstacle setting, we observe a satisfaction-fidelity trade-off, with improved feasibility but increased MMD. Beyond quantitative gains, LGVF yields constraint-aware vector fields exhibiting emergent obstacle-avoidance behavior, routing samples around forbidden regions without explicit path planning.

</details>


### [791] [Robust Domain Generalization under Divergent Marginal and Conditional Distributions](https://arxiv.org/abs/2602.02015)
*Jewon Yeom,Kyubyung Chae,Hyunggyu Lim,Yoonna Oh,Dongyoon Yang,Taesup Kim*

Main category: cs.LG

TL;DR: 提出一个统一的领域泛化框架，处理同时存在边际标签分布P(Y)和条件分布P(X|Y)变化的复合分布偏移问题，通过分解风险边界和元学习实现鲁棒泛化。


<details>
  <summary>Details</summary>
Motivation: 现有领域泛化方法主要关注条件分布偏移(P(X|Y)变化)，假设P(Y)稳定。但现实多领域场景常同时存在边际标签分布P(Y)和条件分布P(X|Y)的复合偏移，需要新的框架来处理这种复杂情况。

Method: 1) 推导新的风险边界：通过显式分解联合分布为边际和条件分量，刻画两种偏移源导致的风险差距；2) 元学习过程：在可见领域上最小化和验证提出的风险边界，确保对未见领域的强泛化能力。

Result: 在传统DG基准测试和具有显著边际与条件偏移的挑战性多领域长尾识别设置中，该方法都取得了最先进的性能。

Conclusion: 提出的统一框架能有效处理现实世界中常见的复合分布偏移问题，通过分解风险边界和元学习策略，在多种复杂场景下实现了鲁棒的领域泛化。

Abstract: Domain generalization (DG) aims to learn predictive models that can generalize to unseen domains. Most existing DG approaches focus on learning domain-invariant representations under the assumption of conditional distribution shift (i.e., primarily addressing changes in $P(X\mid Y)$ while assuming $P(Y)$ remains stable). However, real-world scenarios with multiple domains often involve compound distribution shifts where both the marginal label distribution $P(Y)$ and the conditional distribution $P(X\mid Y)$ vary simultaneously. To address this, we propose a unified framework for robust domain generalization under divergent marginal and conditional distributions. We derive a novel risk bound for unseen domains by explicitly decomposing the joint distribution into marginal and conditional components and characterizing risk gaps arising from both sources of divergence. To operationalize this bound, we design a meta-learning procedure that minimizes and validates the proposed risk bound across seen domains, ensuring strong generalization to unseen ones. Empirical evaluations demonstrate that our method achieves state-of-the-art performance not only on conventional DG benchmarks but also in challenging multi-domain long-tailed recognition settings where both marginal and conditional shifts are pronounced.

</details>


### [792] [DASH: Faster Shampoo via Batched Block Preconditioning and Efficient Inverse-Root Solvers](https://arxiv.org/abs/2602.02016)
*Ionut-Vlad Modoranu,Philip Zmushko,Erik Schultheis,Mher Safaryan,Dan Alistarh*

Main category: cs.LG

TL;DR: DASH 是分布式加速版 Shampoo 优化器，通过张量堆叠和新型矩阵逆根计算方法，显著提升 GPU 利用率，实现高达 4.83 倍的速度提升。


<details>
  <summary>Details</summary>
Motivation: Shampoo 作为领先的近似二阶优化器，在模型性能和压缩性方面表现优异，但计算成本高昂，导致显著的速度下降。本文旨在解决这一计算瓶颈。

Method: 提出 DASH（分布式加速 Shampoo），采用两种新技术：1) 将预条件器块堆叠成 3D 张量以提升 GPU 利用率；2) 引入 Newton-DB 迭代和切比雪夫多项式近似作为计算矩阵逆根的新方法。

Result: GPU 感知实现比优化版分布式 Shampoo 快达 4.83 倍；Newton-DB 在所有测试方法中达到每轮迭代最低的验证困惑度。

Conclusion: DASH 显著加速了 Shampoo 优化器，同时保持了其性能优势，为大规模深度学习训练提供了高效的二阶优化解决方案。

Abstract: Shampoo is one of the leading approximate second-order optimizers: a variant of it has won the MLCommons AlgoPerf competition, and it has been shown to produce models with lower activation outliers that are easier to compress. Yet, applying Shampoo currently comes at the cost of significant computational slowdown, due to its expensive internal operations. In this paper, we take a significant step to address this shortcoming by proposing \method (for \textbf{D}istributed \textbf{A}ccelerated \textbf{SH}ampoo), a faster implementation of Distributed Shampoo based on two main new techniques: First, we show that preconditioner blocks can be stacked into 3D tensors to significantly improve GPU utilization; second, we introduce the Newton-DB iteration and the Chebyshev polynomial approximations as novel and faster approaches for computing the inverse matrix roots required by Shampoo. Along with these algorithmic contributions, we provide a first in-depth analysis of how matrix scaling critically affects Shampoo convergence. On the practical side, our GPU-aware implementation achieves up to $4.83\times$ faster optimizer steps compared to the well-optimized Distributed Shampoo, while Newton-DB attains the lowest validation perplexity per iteration among all tested methods. Our code is available at https://github.com/IST-DASLab/DASH.

</details>


### [793] [On Stability and Robustness of Diffusion Posterior Sampling for Bayesian Inverse Problems](https://arxiv.org/abs/2602.02045)
*Yiming Yang,Xiaoyuan Cheng,Yi He,Kaiyu Li,Wenxuan Yuan,Zhuo Sun*

Main category: cs.LG

TL;DR: 本文分析了扩散模型在贝叶斯逆问题中的稳定性问题，提出了鲁棒的扩散后验采样方法来解决似然函数不匹配时的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型已成为贝叶斯逆问题的强大先验，但现有扩散求解器依赖于假定的观测似然函数。当假定的似然函数与真实数据生成过程不匹配时，扩散求解器缺乏鲁棒性，导致性能下降，这一问题尚未得到充分探索。

Method: 作者首先分析了扩散求解器的稳定性，揭示了其在似然函数不匹配时的脆弱性。然后提出了鲁棒扩散后验采样方法，该方法具有理论上的鲁棒性保证，并且与现有的基于梯度的后验采样器兼容。

Result: 理论分析表明扩散求解器在似然函数不匹配时缺乏鲁棒性。提出的鲁棒扩散后验采样方法在科学逆问题和自然图像任务中表现出有效性，在具有挑战性的似然函数错误设定下实现了稳定的性能提升。

Conclusion: 本文填补了扩散模型在贝叶斯逆问题中稳定性分析的空白，揭示了扩散求解器的鲁棒性问题，并提出了有效的解决方案。该方法为处理实际应用中常见的似然函数不匹配问题提供了理论保证和实用工具。

Abstract: Diffusion models have recently emerged as powerful learned priors for Bayesian inverse problems (BIPs). Diffusion-based solvers rely on a presumed likelihood for the observations in BIPs to guide the generation process. However, the link between likelihood and recovery quality for BIPs is unclear in previous works. We bridge this gap by characterizing the posterior approximation error and proving the \emph{stability} of the diffusion-based solvers. Meanwhile, an immediate result of our findings on stability demonstrates the lack of robustness in diffusion-based solvers, which remains unexplored. This can degrade performance when the presumed likelihood mismatches the unknown true data generation processes. To address this issue, we propose a simple yet effective solution, \emph{robust diffusion posterior sampling}, which is provably \emph{robust} and compatible with existing gradient-based posterior samplers. Empirical results on scientific inverse problems and natural image tasks validate the effectiveness and robustness of our method, showing consistent performance improvements under challenging likelihood misspecifications.

</details>


### [794] [FORLER: Federated Offline Reinforcement Learning with Q-Ensemble and Actor Rectification](https://arxiv.org/abs/2602.02055)
*Nan Qiao,Sheng Yue*

Main category: cs.LG

TL;DR: FORLER：一种结合服务器端Q-ensemble聚合和设备端actor rectification的离线联邦强化学习方法，解决数据异构性导致的策略污染问题


<details>
  <summary>Details</summary>
Motivation: 物联网系统中，联邦学习促进了在线强化学习，但与环境在线交互存在风险和成本。离线联邦强化学习虽然避免了这些问题，但在低质量、异构数据下容易陷入局部最优，一个设备的次优策略会污染聚合模型（策略污染问题）

Method: 1. 服务器端：采用Q-ensemble聚合，稳健地合并设备Q函数以控制策略污染，同时将计算负担从资源受限的硬件转移；2. 设备端：actor rectification通过零阶搜索寻找高Q值动作，并使用定制正则化器将策略推向这些动作；3. 采用δ-周期性策略进一步减少本地计算

Result: 理论分析提供了安全策略改进的性能保证。大量实验表明，FORLER在不同数据质量和异构性条件下始终优于强基线方法

Conclusion: FORLER有效解决了离线联邦强化学习中的策略污染问题，通过服务器端Q-ensemble聚合和设备端actor rectification的组合，在保持隐私的同时实现了稳健的性能提升

Abstract: In Internet-of-Things systems, federated learning has advanced online reinforcement learning (RL) by enabling parallel policy training without sharing raw data. However, interacting with real environments online can be risky and costly, motivating offline federated RL (FRL), where local devices learn from fixed datasets. Despite its promise, offline FRL may break down under low-quality, heterogeneous data. Offline RL tends to get stuck in local optima, and in FRL, one device's suboptimal policy can degrade the aggregated model, i.e., policy pollution. We present FORLER, combining Q-ensemble aggregation on the server with actor rectification on devices. The server robustly merges device Q-functions to curb policy pollution and shift heavy computation off resource-constrained hardware without compromising privacy. Locally, actor rectification enriches policy gradients via a zeroth-order search for high-Q actions plus a bespoke regularizer that nudges the policy toward them. A $δ$-periodic strategy further reduces local computation. We theoretically provide safe policy improvement performance guarantees. Extensive experiments show FORLER consistently outperforms strong baselines under varying data quality and heterogeneity.

</details>


### [795] [FiLoRA: Focus-and-Ignore LoRA for Controllable Feature Reliance](https://arxiv.org/abs/2602.02060)
*Hyunsuk Chung,Caren Han,Yerin Choi,Seungyeon Ji,Jinwoo Kim,Eun-Jung Holden,Kyungreem Han*

Main category: cs.LG

TL;DR: FiLoRA是一个指令条件化的参数高效适配框架，通过分解特征组对齐的LoRA模块和指令条件化门控，实现对内部特征依赖的显式控制，而不改变预测目标或任务语义。


<details>
  <summary>Details</summary>
Motivation: 多模态基础模型如何依赖特定内部特征组以及这种依赖是否可被有意控制尚不清楚。现有研究主要依赖事后分析或特征移除，无法在不改变任务语义的情况下调节特征依赖。

Method: FiLoRA将适配分解为特征组对齐的LoRA模块，并应用指令条件化门控，使自然语言指令作为计算层面的控制信号而非任务重定义。该方法保持预测目标固定，仅调节内部计算过程。

Result: 在文本-图像和音频-视觉基准测试中，指令条件化门控能一致且因果性地改变内部计算，选择性地放大或抑制核心和伪特征组，而不修改标签空间或训练目标。FiLoRA在伪特征干预下展现出更强的鲁棒性。

Conclusion: FiLoRA提供了一种超越相关性学习的原理性机制来调节特征依赖，揭示了通过计算层面的指令控制来管理多模态模型内部特征依赖的可行性。

Abstract: Multimodal foundation models integrate heterogeneous signals across modalities, yet it remains poorly understood how their predictions depend on specific internal feature groups and whether such reliance can be deliberately controlled. Existing studies of shortcut and spurious behavior largely rely on post hoc analyses or feature removal, offering limited insight into whether reliance can be modulated without altering task semantics. We introduce FiLoRA (Focus-and-Ignore LoRA), an instruction-conditioned, parameter-efficient adaptation framework that enables explicit control over internal feature reliance while keeping the predictive objective fixed. FiLoRA decomposes adaptation into feature group-aligned LoRA modules and applies instruction-conditioned gating, allowing natural language instructions to act as computation-level control signals rather than task redefinitions. Across text--image and audio--visual benchmarks, we show that instruction-conditioned gating induces consistent and causal shifts in internal computation, selectively amplifying or suppressing core and spurious feature groups without modifying the label space or training objective. Further analyses demonstrate that FiLoRA yields improved robustness under spurious feature interventions, revealing a principled mechanism to regulate reliance beyond correlation-driven learning.

</details>


### [796] [Learning to Route and Schedule LLMs from User Retrials via Contextual Queueing Bandits](https://arxiv.org/abs/2602.02061)
*Seoungbin Bae,Junyoung Son,Dabeen Lee*

Main category: cs.LG

TL;DR: 论文提出ACQB算法，通过用户重试行为的隐式反馈来联合优化LLM服务的路由和调度，解决排队系统中的用户重试和反馈问题。


<details>
  <summary>Details</summary>
Motivation: LLM服务中用户查询在服务器队列中积累，现有在线算法忽视了两个关键挑战：1) 不满意的用户会重试查询，增加服务器积压；2) 请求"显式"反馈（如评分）会降低用户体验。

Method: 提出上下文排队多臂老虎机与多项逻辑反馈框架(CQB-MNL)，建模查询重试和基于上下文的用户对LLM偏好学习。ACQB算法结合Thompson采样和衰减率的强制探索，实现高效学习同时保持队列稳定。

Result: ACQB在路由方面实现累积遗憾$\widetilde{\mathcal{O}}(\sqrt{t})$，队列长度遗憾$\widetilde{\mathcal{O}}(t^{-1/4})$。实验在SPROUT、EmbedLLM和RouterBench数据集上，算法持续优于基线方法。

Conclusion: 通过利用用户重试行为的隐式反馈，ACQB算法能有效解决LLM服务中的路由和调度问题，在学习和队列稳定性之间取得良好平衡。

Abstract: Explosive demands for LLMs often cause user queries to accumulate in server queues, requiring efficient routing (query-LLM matching) and scheduling (query prioritization) mechanisms. Several online algorithms are being deployed, but they overlook the following two key challenges inherent to conversational LLM services: (1) unsatisfied users may retry queries, increasing the server backlog, and (2) requests for ``explicit" feedback, such as ratings, degrade user experiences. In this paper, we develop a joint routing and scheduling algorithm that leverages ``implicit" feedback inferred from user retrial behaviors. The key idea is to propose and study the framework of contextual queueing bandits with multinomial logit feedback (CQB-MNL). CQB-MNL models query retrials, as well as context-based learning for user preferences over LLMs. Our algorithm, anytime CQB (ACQB), achieves efficient learning while maintaining queue stability by combining Thompson sampling with forced exploration at a decaying rate. We show that ACQB simultaneously achieves a cumulative regret of $\widetilde{\mathcal{O}}(\sqrt{t})$ for routing and a queue length regret of $\widetilde{\mathcal{O}}(t^{-1/4})$ for any large $t$. For experiments, we refine query embeddings via contrastive learning while adopting a disjoint parameter model to learn LLM-specific parameters. Experiments on SPROUT, EmbedLLM, and RouterBench datasets confirm that both algorithms consistently outperform baselines.

</details>


### [797] [BAPS: A Fine-Grained Low-Precision Scheme for Softmax in Attention via Block-Aware Precision reScaling](https://arxiv.org/abs/2602.02071)
*Zisheng Ye,Xiaoyu He,Maoyuan Song,Guoliang Qiu,Chao Liao,Chen Wu,Yonggang Sun,Zhichun Li,Xiaoru Xie,Yuanyong Luo,Hu Liu,Pinyan Lu,Heng Liao*

Main category: cs.LG

TL;DR: 提出HiF8低精度格式和块感知精度重缩放技术，解决Transformer推理中softmax成为瓶颈的问题，通过8位浮点计算减少数据带宽和指数单元面积，实现推理吞吐量翻倍。


<details>
  <summary>Details</summary>
Motivation: 随着量化矩阵乘法加速的性能提升趋于平缓，softmax操作成为Transformer推理的关键瓶颈。这源于两个硬件限制：(1)矩阵和向量计算核心之间的有限数据带宽，(2)高精度(FP32/16)指数单元(EXP2)的显著面积成本。

Method: 引入新颖的低精度工作流，采用特定的8位浮点格式(HiF8)和块感知精度重缩放技术。该方法使矩阵乘法输出约束在8位，将所需数据移动带宽减半；同时在低精度(8位)下计算指数，大幅减少EXP2单元面积。

Result: 在语言模型和多模态模型上的广泛评估证实了方法的有效性。通过缓解向量计算瓶颈，可以在不增加芯片面积的情况下将端到端推理吞吐量翻倍。

Conclusion: 这项工作为未来低精度硬件和软件提供了具体的协同设计路径，通过解决softmax瓶颈，实现了Transformer推理性能的显著提升。

Abstract: As the performance gains from accelerating quantized matrix multiplication plateau, the softmax operation becomes the critical bottleneck in Transformer inference. This bottleneck stems from two hardware limitations: (1) limited data bandwidth between matrix and vector compute cores, and (2) the significant area cost of high-precision (FP32/16) exponentiation units (EXP2). To address these issues, we introduce a novel low-precision workflow that employs a specific 8-bit floating-point format (HiF8) and block-aware precision rescaling for softmax. Crucially, our algorithmic innovations make low-precision softmax feasible without the significant model accuracy loss that hampers direct low-precision approaches. Specifically, our design (i) halves the required data movement bandwidth by enabling matrix multiplication outputs constrained to 8-bit, and (ii) substantially reduces the EXP2 unit area by computing exponentiations in low (8-bit) precision. Extensive evaluation on language models and multi-modal models confirms the validity of our method. By alleviating the vector computation bottleneck, our work paves the way for doubling end-to-end inference throughput without increasing chip area, and offers a concrete co-design path for future low-precision hardware and software.

</details>


### [798] [Calibrating Adaptive Smoothing Methods for Freeway Traffic Reconstruction](https://arxiv.org/abs/2602.02072)
*Junyi Ji,Derek Gloudemans,Gergely Zachár,Matthew Nice,William Barbour,Daniel B. Work*

Main category: cs.LG

TL;DR: 提出一个基于PyTorch的Python自适应平滑方法(ASM)实现，支持端到端校准，使用真实数据优化参数化核函数，为交通状态重建提供基准。


<details>
  <summary>Details</summary>
Motivation: 自适应平滑方法(ASM)是广泛使用的交通状态重建方法，但缺乏可复现的基准实现。本文旨在提供一个端到端校准的Python实现，解决交通模型校准中的可复现性问题。

Method: 1. 使用PyTorch实现ASM，便于与深度学习方法集成；2. 将校准问题形式化为参数化核优化问题；3. 使用稀疏雷达传感器网络的真实数据进行校准；4. 在全状态观测测试平台上进行模型校准。

Result: 1. 实现了可复现的ASM Python实现；2. 提供了速度分布、时空误差分布和空间误差的评估指标；3. 展示了校准方法在多个高速公路上的适用性；4. 为交通重建问题建立了基准指标。

Conclusion: 本文提供了一个可复现的ASM基准实现，解决了交通模型校准中的可复现性挑战，可作为各种高速公路运营任务的基准，同时讨论了ASM的局限性。

Abstract: The adaptive smoothing method (ASM) is a widely used approach for traffic state reconstruction. This article presents a Python implementation of ASM, featuring end-to-end calibration using real-world ground truth data. The calibration is formulated as a parameterized kernel optimization problem. The model is calibrated using data from a full-state observation testbed, with input from a sparse radar sensor network. The implementation is developed in PyTorch, enabling integration with various deep learning methods. We evaluate the results in terms of speed distribution, spatio-temporal error distribution, and spatial error to provide benchmark metrics for the traffic reconstruction problem. We further demonstrate the usability of the calibrated method across multiple freeways. Finally, we discuss the challenges of reproducibility in general traffic model calibration and the limitations of ASM. This article is reproducible and can serve as a benchmark for various freeway operation tasks.

</details>


### [799] [AICD Bench: A Challenging Benchmark for AI-Generated Code Detection](https://arxiv.org/abs/2602.02079)
*Daniil Orel,Dilshod Azizov,Indraneil Paul,Yuxia Wang,Iryna Gurevych,Preslav Nakov*

Main category: cs.LG

TL;DR: AICD Bench是一个全面的AI生成代码检测基准，包含200万样本、77个模型、9种编程语言，提出三种现实检测任务，评估显示现有检测器性能远低于实际可用性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型生成功能代码能力增强，引发了作者身份、责任和安全性问题。现有AI生成代码检测数据集和基准过于狭窄，通常仅限于分布内设置的二元人机分类，需要更全面的评估框架。

Method: 构建AICD Bench基准，包含200万个示例、77个模型（涵盖11个模型家族）、9种编程语言，包括最新的推理模型。提出三种现实检测任务：分布偏移下的鲁棒二元分类、模型家族归因、细粒度人机分类（人类、机器、混合、对抗代码）。

Result: 对神经和经典检测器的广泛评估显示，性能远低于实际可用性，特别是在分布偏移下以及对于混合或对抗代码。检测器在现实场景中的表现存在显著差距。

Conclusion: AICD Bench作为一个统一且具有挑战性的评估套件发布，旨在推动下一代鲁棒的AI生成代码检测方法的发展。数据和代码已公开可用。

Abstract: Large language models (LLMs) are increasingly capable of generating functional source code, raising concerns about authorship, accountability, and security. While detecting AI-generated code is critical, existing datasets and benchmarks are narrow, typically limited to binary human-machine classification under in-distribution settings. To bridge this gap, we introduce $\emph{AICD Bench}$, the most comprehensive benchmark for AI-generated code detection. It spans $\emph{2M examples}$, $\emph{77 models}$ across $\emph{11 families}$, and $\emph{9 programming languages}$, including recent reasoning models. Beyond scale, AICD Bench introduces three realistic detection tasks: ($\emph{i}$)~$\emph{Robust Binary Classification}$ under distribution shifts in language and domain, ($\emph{ii}$)~$\emph{Model Family Attribution}$, grouping generators by architectural lineage, and ($\emph{iii}$)~$\emph{Fine-Grained Human-Machine Classification}$ across human, machine, hybrid, and adversarial code. Extensive evaluation on neural and classical detectors shows that performance remains far below practical usability, particularly under distribution shift and for hybrid or adversarial code. We release AICD Bench as a $\emph{unified, challenging evaluation suite}$ to drive the next generation of robust approaches for AI-generated code detection. The data and the code are available at https://huggingface.co/AICD-bench}.

</details>


### [800] [Learning Half-Spaces from Perturbed Contrastive Examples](https://arxiv.org/abs/2602.02080)
*Aryan Alavi Razavi Ravari,Farnam Mansouri,Yuxin Chen,Valentio Iverson,Adish Singla,Sandra Zilles*

Main category: cs.LG

TL;DR: 研究在带有噪声的对比样本oracle下的学习问题，其中对比样本会被扰动，扰动程度由点到决策边界的距离决定，距离越近扰动越小。分析了固定和随机两种扰动设置下的一维阈值和半空间学习。


<details>
  <summary>Details</summary>
Motivation: Mansouri等人提出了理想的对比样本oracle，其中对比样本总是距离查询点最近的反类样本。但现实场景中对比样本可能不完美，因此需要研究带有噪声扰动的对比样本学习模型。

Method: 引入参数化噪声函数f的扰动机制，扰动幅度由f(d)控制，d是查询点到决策边界的距离。研究两种设置：(i)固定最大扰动幅度，(ii)随机扰动。分析一维阈值和均匀分布有界域上半空间的主动和被动对比样本复杂度。

Result: 在f满足特定条件时，对比样本的存在能加速学习，降低渐近查询复杂度和期望查询复杂度。给出了对比样本复杂度对函数f的依赖关系。

Conclusion: 即使在对比样本存在噪声扰动的情况下，只要扰动机制合理（距离决策边界越近扰动越小），对比样本仍然能有效加速学习过程，特别是在主动学习场景中。

Abstract: We study learning under a two-step contrastive example oracle, as introduced by Mansouri et. al. (2025), where each queried (or sampled) labeled example is paired with an additional contrastive example of opposite label. While Mansouri et al. assume an idealized setting, where the contrastive example is at minimum distance of the originally queried/sampled point, we introduce and analyze a mechanism, parameterized by a non-decreasing noise function $f$, under which this ideal contrastive example is perturbed. The amount of perturbation is controlled by $f(d)$, where $d$ is the distance of the queried/sampled point to the decision boundary. Intuitively, this results in higher-quality contrastive examples for points closer to the decision boundary. We study this model in two settings: (i) when the maximum perturbation magnitude is fixed, and (ii) when it is stochastic.
  For one-dimensional thresholds and for half-spaces under the uniform distribution on a bounded domain, we characterize active and passive contrastive sample complexity in dependence on the function $f$. We show that, under certain conditions on $f$, the presence of contrastive examples speeds up learning in terms of asymptotic query complexity and asymptotic expected query complexity.

</details>


### [801] [Active learning from positive and unlabeled examples](https://arxiv.org/abs/2602.02081)
*Farnam Mansouri,Sandra Zilles,Shai Ben-David*

Main category: cs.LG

TL;DR: 该论文首次对主动PU学习（仅从正例和未标记数据中学习）的标签复杂度进行了理论分析，研究了一种特殊的主动学习设置，其中查询标签仅在实例为正且独立硬币翻转成功时才会被揭示。


<details>
  <summary>Details</summary>
Motivation: 动机源于广告和异常检测等实际应用，在这些场景中，学习者只能获得部分正例标签，而其他所有实例都保持未标记状态。研究者希望分析在这种弱监督二元分类设置下，主动学习方法的标签复杂度。

Method: 研究了一种主动PU学习设置，学习者可以从未标记池中自适应地查询实例，但查询的标签仅在实例为正且独立硬币翻转成功时才会被揭示；否则学习者不会获得任何信息。

Result: 该论文提供了主动PU学习标签复杂度的首次理论分析，填补了这一研究领域的空白。

Conclusion: 该研究为主动PU学习的理论分析奠定了基础，对广告、异常检测等实际应用中的弱监督学习问题具有重要意义。

Abstract: Learning from positive and unlabeled data (PU learning) is a weakly supervised variant of binary classification in which the learner receives labels only for (some) positively labeled instances, while all other examples remain unlabeled. Motivated by applications such as advertising and anomaly detection, we study an active PU learning setting where the learner can adaptively query instances from an unlabeled pool, but a queried label is revealed only when the instance is positive and an independent coin flip succeeds; otherwise the learner receives no information. In this paper, we provide the first theoretical analysis of the label complexity of active PU learning.

</details>


### [802] [Probabilistic Performance Guarantees for Multi-Task Reinforcement Learning](https://arxiv.org/abs/2602.02098)
*Yannik Schnitzer,Mathias Jackermeier,Alessandro Abate,David Parker*

Main category: cs.LG

TL;DR: 提出一种为多任务强化学习策略在新任务上性能提供高置信度保证的方法，结合任务内和任务间泛化分析


<details>
  <summary>Details</summary>
Motivation: 现有多任务强化学习方法缺乏形式化性能保证，这在安全关键场景部署中至关重要，需要为未见任务提供可靠性能保证

Method: 提出新的泛化边界，结合(i)有限次rollout得到的每任务下置信边界和(ii)有限采样任务的任务级泛化分析，为来自相同未知分布的新任务提供高置信度保证

Result: 在先进的多任务RL方法上验证，证明该保证在理论上是可靠的，且在现实样本量下具有信息价值

Conclusion: 该方法为多任务强化学习策略在安全关键部署中提供了形式化的性能保证框架，填补了现有方法缺乏理论保证的空白

Abstract: Multi-task reinforcement learning trains generalist policies that can execute multiple tasks. While recent years have seen significant progress, existing approaches rarely provide formal performance guarantees, which are indispensable when deploying policies in safety-critical settings. We present an approach for computing high-confidence guarantees on the performance of a multi-task policy on tasks not seen during training. Concretely, we introduce a new generalisation bound that composes (i) per-task lower confidence bounds from finitely many rollouts with (ii) task-level generalisation from finitely many sampled tasks, yielding a high-confidence guarantee for new tasks drawn from the same arbitrary and unknown distribution. Across state-of-the-art multi-task RL methods, we show that the guarantees are theoretically sound and informative at realistic sample sizes.

</details>


### [803] [An Empirical Study of World Model Quantization](https://arxiv.org/abs/2602.02110)
*Zhongqian Fu,Tianyi Zhao,Kai Han,Hang Zhou,Xinghao Chen,Yunhe Wang*

Main category: cs.LG

TL;DR: 本文系统研究了世界模型的量化问题，发现量化对世界模型的影响超越了传统的精度-比特权衡，揭示了量化导致的独特失效模式，为在严格计算约束下部署量化世界模型提供了实用指导。


<details>
  <summary>Details</summary>
Motivation: 世界模型在紧凑潜在空间中学习环境动态表示，用于规划、预测和推理等任务，但其运行依赖大量计算成本和内存占用，使得模型量化对于高效部署至关重要。然而，迄今为止，后训练量化对世界模型的影响在很大程度上尚未得到研究。

Method: 使用DINO-WM作为代表性案例，系统实证研究世界模型量化，评估了多种后训练量化方法，包括仅权重量化和联合权重-激活量化。在不同视觉规划任务上进行了广泛实验，涵盖了广泛的比特宽度、量化粒度和长达50步的规划视野。

Result: 研究发现：1）分组权重量化可以稳定低比特展开；2）激活量化粒度带来不一致的收益；3）编码器和预测器模块的量化敏感性高度不对称；4）激进的低比特量化显著降低了规划目标与任务成功之间的对齐性，导致无法通过额外优化修复的失败。

Conclusion: 这些发现揭示了基于世界模型的规划中量化导致的独特失效模式，为在严格计算约束下部署量化世界模型提供了实用指导。代码将在指定GitHub仓库中提供。

Abstract: World models learn an internal representation of environment dynamics, enabling agents to simulate and reason about future states within a compact latent space for tasks such as planning, prediction, and inference. However, running world models rely on hevay computational cost and memory footprint, making model quantization essential for efficient deployment. To date, the effects of post-training quantization (PTQ) on world models remain largely unexamined. In this work, we present a systematic empirical study of world model quantization using DINO-WM as a representative case, evaluating diverse PTQ methods under both weight-only and joint weight-activation settings. We conduct extensive experiments on different visual planning tasks across a wide range of bit-widths, quantization granularities, and planning horizons up to 50 iterations. Our results show that quantization effects in world models extend beyond standard accuracy and bit-width trade-offs: group-wise weight quantization can stabilize low-bit rollouts, activation quantization granularity yields inconsistent benefits, and quantization sensitivity is highly asymmetric between encoder and predictor modules. Moreover, aggressive low-bit quantization significantly degrades the alignment between the planning objective and task success, leading to failures that cannot be remedied by additional optimization. These findings reveal distinct quantization-induced failure modes in world model-based planning and provide practical guidance for deploying quantized world models under strict computational constraints. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/QuantWM.

</details>


### [804] [The Maximum von Neumann Entropy Principle: Theory and Applications in Machine Learning](https://arxiv.org/abs/2602.02117)
*Youqi Wu,Farzan Farnia*

Main category: cs.LG

TL;DR: 论文扩展了经典最大熵原理到冯·诺依曼熵，提供了密度矩阵和迹归一化正半定算子的冯·诺依曼熵最大化的博弈论解释，并应用于核表示选择和核矩阵补全。


<details>
  <summary>Details</summary>
Motivation: 冯·诺依曼熵在量子信息论中是基本量，最近被机器学习采用作为核矩阵和核协方差算子的谱多样性度量。虽然量子背景下约束下的冯·诺依曼熵最大化已有研究，但在数据驱动背景下，特别是其决策论和博弈论解释的经典最大熵框架的类似物尚未明确发展。

Method: 将Grünwald和Dawid的最大熵原理的极小极大公式扩展到冯·诺依曼熵设置，为密度矩阵和迹归一化正半定算子的冯·诺依曼熵最大化提供博弈论证明。然后通过两个代表性应用说明最大冯·诺依曼熵原理：1)通过基于核的冯·诺依曼熵最大化从多个归一化嵌入中选择核表示；2)从部分观测条目补全核矩阵。

Result: 该视角为部分信息下的最大冯·诺依曼熵解提供了稳健解释，并阐明了它们在谱域中作为最小承诺推断的作用。提出的框架为核学习中基于冯·诺依曼熵的方法提供了统一的信息论基础。

Conclusion: 论文成功地将经典最大熵原理扩展到冯·诺依曼熵，建立了其博弈论基础，并展示了在机器学习问题中的应用，为核学习中的冯·诺依曼熵方法提供了统一的信息论框架。

Abstract: Von Neumann entropy (VNE) is a fundamental quantity in quantum information theory and has recently been adopted in machine learning as a spectral measure of diversity for kernel matrices and kernel covariance operators. While maximizing VNE under constraints is well known in quantum settings, a principled analogue of the classical maximum entropy framework, particularly its decision theoretic and game theoretic interpretation, has not been explicitly developed for VNE in data driven contexts. In this paper, we extend the minimax formulation of the maximum entropy principle due to Grünwald and Dawid to the setting of von Neumann entropy, providing a game-theoretic justification for VNE maximization over density matrices and trace-normalized positive semidefinite operators. This perspective yields a robust interpretation of maximum VNE solutions under partial information and clarifies their role as least committed inferences in spectral domains. We then illustrate how the resulting Maximum VNE principle applies to modern machine learning problems by considering two representative applications, selecting a kernel representation from multiple normalized embeddings via kernel-based VNE maximization, and completing kernel matrices from partially observed entries. These examples demonstrate how the proposed framework offers a unifying information-theoretic foundation for VNE-based methods in kernel learning.

</details>


### [805] [Two-Stage Grid Optimization for Group-wise Quantization of LLMs](https://arxiv.org/abs/2602.02126)
*Junhan Kim,Gukryeol Lee,Seungwoo Son,Jeewook Kim,Yongkweon Jeon*

Main category: cs.LG

TL;DR: 提出两阶段优化框架改进GPTQ分组量化，通过最小化层重建损失提升LLM低比特量化精度


<details>
  <summary>Details</summary>
Motivation: GPTQ分组量化方法虽然高效，但忽略了输入统计和组间相关性，导致与最小化层重建损失的目标不匹配，影响量化精度

Method: 两阶段优化框架：第一阶段在GPTQ前初始化组尺度以最小化组重建损失；第二阶段冻结GPTQ得到的整数权重，使用坐标下降算法和闭式更新规则优化组尺度以最小化层重建损失，并考虑前层量化误差

Result: 实验结果表明该方法能持续提升分组量化性能，以可忽略的开销获得更高精度

Conclusion: 提出的两阶段优化框架有效解决了GPTQ在分组量化中的局限性，通过显式最小化层重建损失并考虑输入统计和误差传播，显著提升了LLM低比特量化精度

Abstract: Group-wise quantization is an effective strategy for mitigating accuracy degradation in low-bit quantization of large language models (LLMs). Among existing methods, GPTQ has been widely adopted due to its efficiency; however, it neglects input statistics and inter-group correlations when determining group scales, leading to a mismatch with its goal of minimizing layer-wise reconstruction loss. In this work, we propose a two-stage optimization framework for group scales that explicitly minimizes the layer-wise reconstruction loss. In the first stage, performed prior to GPTQ, we initialize each group scale to minimize the group-wise reconstruction loss, thereby incorporating input statistics. In the second stage, we freeze the integer weights obtained via GPTQ and refine the group scales to minimize the layer-wise reconstruction loss. To this end, we employ the coordinate descent algorithm and derive a closed-form update rule, which enables efficient refinement without costly numerical optimization. Notably, our derivation incorporates the quantization errors from preceding layers to prevent error accumulation. Experimental results demonstrate that our method consistently enhances group-wise quantization, achieving higher accuracy with negligible overhead.

</details>


### [806] [Scalable Spatio-Temporal SE(3) Diffusion for Long-Horizon Protein Dynamics](https://arxiv.org/abs/2602.02128)
*Nima Shoghi,Yuxuan Liu,Yuning Shen,Rob Brekelmans,Pan Li,Quanquan Gu*

Main category: cs.LG

TL;DR: STAR-MD：一种可扩展的SE(3)等变扩散模型，通过联合时空注意力机制生成微秒级蛋白质轨迹，在ATLAS基准测试中实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 分子动力学模拟计算成本高，难以达到生物相关时间尺度；现有生成模型在长时程生成方面存在架构限制、误差累积和时空动态建模不足的问题。

Method: 提出STAR-MD模型，采用因果扩散变换器结合联合时空注意力机制，高效捕捉复杂时空依赖关系，避免现有方法的内存瓶颈。

Result: 在ATLAS基准测试中实现所有指标的最先进性能，显著改善构象覆盖、结构有效性和动态保真度；成功外推生成稳定的微秒级轨迹。

Conclusion: STAR-MD的联合时空建模能够在生物相关时间尺度上实现稳健的动力学模拟，为加速探索蛋白质功能开辟了新途径。

Abstract: Molecular dynamics (MD) simulations remain the gold standard for studying protein dynamics, but their computational cost limits access to biologically relevant timescales. Recent generative models have shown promise in accelerating simulations, yet they struggle with long-horizon generation due to architectural constraints, error accumulation, and inadequate modeling of spatio-temporal dynamics. We present STAR-MD (Spatio-Temporal Autoregressive Rollout for Molecular Dynamics), a scalable SE(3)-equivariant diffusion model that generates physically plausible protein trajectories over microsecond timescales. Our key innovation is a causal diffusion transformer with joint spatio-temporal attention that efficiently captures complex space-time dependencies while avoiding the memory bottlenecks of existing methods. On the standard ATLAS benchmark, STAR-MD achieves state-of-the-art performance across all metrics--substantially improving conformational coverage, structural validity, and dynamic fidelity compared to previous methods. STAR-MD successfully extrapolates to generate stable microsecond-scale trajectories where baseline methods fail catastrophically, maintaining high structural quality throughout the extended rollout. Our comprehensive evaluation reveals severe limitations in current models for long-horizon generation, while demonstrating that STAR-MD's joint spatio-temporal modeling enables robust dynamics simulation at biologically relevant timescales, paving the way for accelerated exploration of protein function.

</details>


### [807] [Back to the Future: Look-ahead Augmentation and Parallel Self-Refinement for Time Series Forecasting](https://arxiv.org/abs/2602.02146)
*Sunho Kim,Susik Yoon*

Main category: cs.LG

TL;DR: BTTF框架通过前瞻增强和自校正精炼提升长期时间序列预测稳定性，无需复杂架构即可显著改善预测精度


<details>
  <summary>Details</summary>
Motivation: 解决长期时间序列预测中并行效率与时间一致性之间的权衡问题。直接多步预测方法虽然并行高效但失去时间一致性，而迭代多步预测保留时间依赖性但存在误差累积和推理缓慢的问题

Method: 提出Back to the Future (BTTF)框架，通过前瞻增强和自校正精炼来增强预测稳定性。该方法不依赖复杂模型架构，而是重新审视基本预测过程，通过集成第二阶段的模型（使用其初始预测进行增强）来精炼基础模型

Result: BTTF方法持续改善长期预测精度，减轻线性预测模型的不稳定性，实现高达58%的精度提升。即使在第一阶段模型训练条件不理想的情况下，仍能保持稳定的改进

Conclusion: 利用模型生成的预测作为增强手段，即使没有复杂架构，也能成为提升长期预测能力的简单而有效的方法

Abstract: Long-term time series forecasting (LTSF) remains challenging due to the trade-off between parallel efficiency and sequential modeling of temporal coherence. Direct multi-step forecasting (DMS) methods enable fast, parallel prediction of all future horizons but often lose temporal consistency across steps, while iterative multi-step forecasting (IMS) preserves temporal dependencies at the cost of error accumulation and slow inference. To bridge this gap, we propose Back to the Future (BTTF), a simple yet effective framework that enhances forecasting stability through look-ahead augmentation and self-corrective refinement. Rather than relying on complex model architectures, BTTF revisits the fundamental forecasting process and refines a base model by ensembling the second-stage models augmented with their initial predictions. Despite its simplicity, our approach consistently improves long-horizon accuracy and mitigates the instability of linear forecasting models, achieving accuracy gains of up to 58% and demonstrating stable improvements even when the first-stage model is trained under suboptimal conditions. These results suggest that leveraging model-generated forecasts as augmentation can be a simple yet powerful way to enhance long-term prediction, even without complex architectures.

</details>


### [808] [ECHO: Entropy-Confidence Hybrid Optimization for Test-Time Reinforcement Learning](https://arxiv.org/abs/2602.02150)
*Chu Zhao,Enneng Yang,Yuting Liu,Jianzhe Zhao,Guibing Guo*

Main category: cs.LG

TL;DR: 本文提出ECHO方法，通过自适应分支控制和置信度剪枝解决测试时强化学习中存在的分支崩溃和早期伪标签噪声问题，在数学和视觉推理任务上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 测试时强化学习通过多次rollout生成候选答案并使用多数投票构建伪标签进行在线更新。现有树状rollout方法虽然提高了采样效率，但仍面临两个关键挑战：1）高熵分支可能导致rollout崩溃，分支预算集中在少数具有连续高熵段的轨迹上；2）早期伪标签噪声大且有偏差，可能导致自增强过拟合，使策略过早锐化并抑制探索。

Method: 提出Entropy Confidence Hybrid Group Relative Policy Optimization (ECHO)方法。在rollout阶段，联合利用局部熵和组级置信度自适应控制分支宽度，并引入基于置信度的在线剪枝来终止持续低置信度分支，避免高熵陷阱和缓解崩溃。在策略更新阶段，采用置信度自适应裁剪和熵-置信度混合优势塑形方法，增强训练鲁棒性并缓解早期偏差。

Result: 实验表明，ECHO在多个数学和视觉推理基准测试中取得一致性能提升，在有限rollout预算下具有更好的泛化能力。

Conclusion: ECHO通过自适应分支控制和置信度剪枝有效解决了测试时强化学习中的分支崩溃和早期伪标签噪声问题，提高了采样效率和训练稳定性，在多种推理任务上表现出优越性能。

Abstract: Test-time reinforcement learning generates multiple candidate answers via repeated rollouts and performs online updates using pseudo-labels constructed by majority voting. To reduce overhead and improve exploration, prior work introduces tree structured rollouts, which share reasoning prefixes and branch at key nodes to improve sampling efficiency. However, this paradigm still faces two challenges: (1) high entropy branching can trigger rollout collapse, where the branching budget concentrates on a few trajectories with consecutive high-entropy segments, rapidly reducing the number of effective branches; (2) early pseudo-labels are noisy and biased, which can induce self-reinforcing overfitting, causing the policy to sharpen prematurely and suppress exploration. To address these issues, we propose Entropy Confidence Hybrid Group Relative Policy Optimization (ECHO). During rollout, ECHO jointly leverages local entropy and group level confidence to adaptively control branch width, and further introduces online confidence-based pruning to terminate persistently low confidence branches, avoiding high entropy traps and mitigating collapse. During policy updates, ECHO employs confidence adaptive clipping and an entropy confidence hybrid advantage shaping approach to enhance training robustness and mitigate early stage bias. Experiments demonstrate that ECHO achieves consistent gains on multiple mathematical and visual reasoning benchmarks, and generalizes more effectively under a limited rollout budget.

</details>


### [809] [Efficient Neural Controlled Differential Equations via Attentive Kernel Smoothing](https://arxiv.org/abs/2602.02157)
*Egor Serov,Ilya Kuleshov,Alexey Zaytsev*

Main category: cs.LG

TL;DR: 提出一种新的Neural CDE路径构建方法，用核和GP平滑替代精确插值，结合注意力机制的多视图CDE来恢复细节，显著减少函数评估次数和推理时间


<details>
  <summary>Details</summary>
Motivation: 传统Neural CDE中驱动控制路径的粗糙性限制了效率，标准样条插值引入高频变化，迫使自适应求解器采取过小步长，导致函数评估次数过多

Method: 1) 用核和GP平滑替代精确插值，显式控制轨迹正则性；2) 提出注意力机制的多视图CDE(MV-CDE)及其卷积扩展(MVC-CDE)，使用可学习查询来恢复平滑过程中丢失的细节；3) 让模型在多个轨迹间分配表示能力，每个轨迹捕获不同的时间模式

Result: MVC-CDE with GP方法在保持最先进准确率的同时，相比基于样条的基线方法，显著减少了函数评估次数和总推理时间

Conclusion: 通过平滑路径构建和多视图表示，提出的方法有效解决了Neural CDE中路径粗糙导致的效率问题，实现了准确性和计算效率的双重提升

Abstract: Neural Controlled Differential Equations (Neural CDEs) provide a powerful continuous-time framework for sequence modeling, yet the roughness of the driving control path often restricts their efficiency. Standard splines introduce high-frequency variations that force adaptive solvers to take excessively small steps, driving up the Number of Function Evaluations (NFE). We propose a novel approach to Neural CDE path construction that replaces exact interpolation with Kernel and Gaussian Process (GP) smoothing, enabling explicit control over trajectory regularity. To recover details lost during smoothing, we propose an attention-based Multi-View CDE (MV-CDE) and its convolutional extension (MVC-CDE), which employ learnable queries to inform path reconstruction. This framework allows the model to distribute representational capacity across multiple trajectories, each capturing distinct temporal patterns. Empirical results demonstrate that our method, MVC-CDE with GP, achieves state-of-the-art accuracy while significantly reducing NFEs and total inference time compared to spline-based baselines.

</details>


### [810] [Interpretable Tabular Foundation Models via In-Context Kernel Regression](https://arxiv.org/abs/2602.02162)
*Ratmir Miftachov,Bruno Charron,Simon Valentin*

Main category: cs.LG

TL;DR: KernelICL框架为表格基础模型提供可量化的基于样本的可解释性，通过将最终预测层替换为核函数，使预测成为训练标签的透明加权平均，在保持性能的同时实现可检查预测。


<details>
  <summary>Details</summary>
Motivation: 现有的表格基础模型（如TabPFN和TabICL）虽然通过上下文学习实现了最先进的性能，但其架构本质上是不透明的，缺乏可解释性。

Method: 基于上下文学习类似于核回归的洞察，将最终预测层替换为核函数（高斯核、点积核、kNN核），使每个预测都成为训练标签的透明加权平均。引入二维分类法统一标准核方法、现代基于邻居的方法和注意力机制。

Result: 在55个TALENT基准数据集上，KernelICL实现了与现有表格基础模型相当的性能，表明对最终层施加显式核约束可以在不牺牲性能的情况下实现可检查的预测。

Conclusion: KernelICL框架通过将核机制显式化，为表格基础模型提供了可量化的基于样本的可解释性，在保持性能的同时实现了透明预测，统一了多种预测方法。

Abstract: Tabular foundation models like TabPFN and TabICL achieve state-of-the-art performance through in-context learning, yet their architectures remain fundamentally opaque. We introduce KernelICL, a framework to enhance tabular foundation models with quantifiable sample-based interpretability. Building on the insight that in-context learning is akin to kernel regression, we make this mechanism explicit by replacing the final prediction layer with kernel functions (Gaussian, dot-product, kNN) so that every prediction is a transparent weighted average of training labels. We introduce a two-dimensional taxonomy that formally unifies standard kernel methods, modern neighbor-based approaches, and attention mechanisms under a single framework, and quantify inspectability via the perplexity of the weight distribution over training samples. On 55 TALENT benchmark datasets, KernelICL achieves performance on par with existing tabular foundation models, demonstrating that explicit kernel constraints on the final layer enable inspectable predictions without sacrificing performance.

</details>


### [811] [Co-RedTeam: Orchestrated Security Discovery and Exploitation with LLM Agents](https://arxiv.org/abs/2602.02164)
*Pengfei He,Ash Fox,Lesly Miculicich,Stefan Friedli,Daniel Fabian,Burak Gokturk,Jiliang Tang,Chen-Yu Lee,Tomas Pfister,Long T. Le*

Main category: cs.LG

TL;DR: Co-RedTeam是一个安全感知的多智能体框架，通过集成安全领域知识、代码感知分析、执行基础迭代推理和长期记忆，模拟真实红队工作流程，显著提升漏洞发现和利用能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在网络安全任务中存在局限性：交互有限、执行基础薄弱、缺乏经验复用，难以实现自动化的漏洞发现和利用。需要设计能够模拟真实红队工作流程的框架。

Method: 提出Co-RedTeam多智能体框架，将漏洞分析分解为协调的发现和利用阶段。智能体能够规划、执行、验证和基于真实执行反馈优化行动，同时从先前轨迹中学习。框架集成了安全领域知识、代码感知分析、执行基础迭代推理和长期记忆。

Result: 在具有挑战性的安全基准测试中，Co-RedTeam在不同骨干模型上均显著优于基线方法，在漏洞利用方面达到超过60%的成功率，在漏洞检测方面获得超过10%的绝对提升。

Conclusion: 执行反馈、结构化交互和记忆对于构建鲁棒且可泛化的网络安全智能体至关重要。Co-RedTeam通过模拟真实红队工作流程，为自动化网络安全任务提供了有效的多智能体解决方案。

Abstract: Large language models (LLMs) have shown promise in assisting cybersecurity tasks, yet existing approaches struggle with automatic vulnerability discovery and exploitation due to limited interaction, weak execution grounding, and a lack of experience reuse. We propose Co-RedTeam, a security-aware multi-agent framework designed to mirror real-world red-teaming workflows by integrating security-domain knowledge, code-aware analysis, execution-grounded iterative reasoning, and long-term memory. Co-RedTeam decomposes vulnerability analysis into coordinated discovery and exploitation stages, enabling agents to plan, execute, validate, and refine actions based on real execution feedback while learning from prior trajectories. Extensive evaluations on challenging security benchmarks demonstrate that Co-RedTeam consistently outperforms strong baselines across diverse backbone models, achieving over 60% success rate in vulnerability exploitation and over 10% absolute improvement in vulnerability detection. Ablation and iteration studies further confirm the critical role of execution feedback, structured interaction, and memory for building robust and generalizable cybersecurity agents.

</details>


### [812] [Generalized Optimal Classification Trees: A Mixed-Integer Programming Approach](https://arxiv.org/abs/2602.02173)
*Jiancheng Tu,Wenqi Fan,Zhibin Wu*

Main category: cs.LG

TL;DR: 提出基于混合整数规划（MIP）的框架，用于在非线性性能指标（如F1分数）下学习最优分类树，特别针对类别不平衡问题，通过定制化加速技术提高可扩展性。


<details>
  <summary>Details</summary>
Motivation: 决策树的全局优化是组合优化中长期存在的挑战，但在可解释机器学习中具有重要作用。现有方法难以高效优化非线性性能指标，特别是在处理类别不平衡问题时。

Method: 提出基于混合整数规划（MIP）的框架，支持非线性性能指标优化。开发了问题特定的加速技术：定制化的分支切割算法、实例缩减方案和热启动策略。

Result: 在50个基准数据集上评估，框架能高效优化非线性指标，同时实现强大的预测性能，与现有方法相比显著减少求解时间。

Conclusion: 该MIP框架为在非线性性能指标下学习最优分类树提供了实用解决方案，特别适用于类别不平衡场景，通过定制化加速技术实现了良好的可扩展性。

Abstract: Global optimization of decision trees is a long-standing challenge in combinatorial optimization, yet such models play an important role in interpretable machine learning. Although the problem has been investigated for several decades, only recent advances in discrete optimization have enabled practical algorithms for solving optimal classification tree problems on real-world datasets. Mixed-integer programming (MIP) offers a high degree of modeling flexibility, and we therefore propose a MIP-based framework for learning optimal classification trees under nonlinear performance metrics, such as the F1-score, that explicitly addresses class imbalance. To improve scalability, we develop problem-specific acceleration techniques, including a tailored branch-and-cut algorithm, an instance-reduction scheme, and warm-start strategies. We evaluate the proposed approach on 50 benchmark datasets. The computational results show that the framework can efficiently optimize nonlinear metrics while achieving strong predictive performance and reduced solution times compared with existing methods.

</details>


### [813] [SurvKAN: A Fully Parametric Survival Model Based on Kolmogorov-Arnold Networks](https://arxiv.org/abs/2602.02179)
*Marina Mastroleo,Alberto Archetti,Federico Mastroleo,Matteo Matteucci*

Main category: cs.LG

TL;DR: SurvKAN：基于KAN架构的完全参数化、时间连续生存模型，消除比例风险约束，在保持可解释性的同时实现竞争性预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统生存模型（如Cox）依赖限制性假设（线性协变量关系、比例风险），难以捕捉真实临床动态；深度学习模型（如DeepSurv、DeepHit）提高了表达能力但牺牲了可解释性；现有KAN混合模型（如CoxKAN）仍受半参数Cox框架限制。需要一种既保持可解释性又消除比例风险约束的生存模型。

Method: 提出SurvKAN：完全参数化、时间连续的生存模型，基于KAN架构。将时间作为KAN的显式输入，直接预测对数风险函数，通过完整生存似然进行端到端训练。架构通过可学习的单变量函数保持可解释性，显示个体特征如何随时间影响风险。

Result: 在标准生存基准测试中，SurvKAN在一致性和校准指标上达到与经典和最先进基线方法竞争或更优的性能。可解释性分析揭示了与医学领域知识一致的临床有意义模式。

Conclusion: SurvKAN成功消除了比例风险约束，在保持可解释性的同时实现了竞争性预测性能，为临床决策提供了更灵活、透明且准确的生存分析工具。

Abstract: Accurate prediction of time-to-event outcomes is critical for clinical decision-making, treatment planning, and resource allocation in modern healthcare. While classical survival models such as Cox remain widely adopted in standard practice, they rely on restrictive assumptions, including linear covariate relationships and proportional hazards over time, that often fail to capture real-world clinical dynamics. Recent deep learning approaches like DeepSurv and DeepHit offer improved expressivity but sacrifice interpretability, limiting clinical adoption where trust and transparency are paramount. Hybrid models incorporating Kolmogorov-Arnold Networks (KANs), such as CoxKAN, have begun to address this trade-off but remain constrained by the semi-parametric Cox framework. In this work we introduce SurvKAN, a fully parametric, time-continuous survival model based on KAN architectures that eliminates the proportional hazards constraint. SurvKAN treats time as an explicit input to a KAN that directly predicts the log-hazard function, enabling end-to-end training on the full survival likelihood. Our architecture preserves interpretability through learnable univariate functions that indicate how individual features influence risk over time. Extensive experiments on standard survival benchmarks demonstrate that SurvKAN achieves competitive or superior performance compared to classical and state-of-the-art baselines across concordance and calibration metrics. Additionally, interpretability analyses reveal clinically meaningful patterns that align with medical domain knowledge.

</details>


### [814] [STILL: Selecting Tokens for Intra-Layer Hybrid Attention to Linearize LLMs](https://arxiv.org/abs/2602.02180)
*Weikang Meng,Liangyu Huo,Yadan Luo,Jiawen Guan,Jingyi Zhang,Yingjian Li,Zheng Zhang*

Main category: cs.LG

TL;DR: STILL：一种用于高效线性化大语言模型的层内混合线性化框架，通过自显著性分数和保持范数的特征映射解决现有线性注意力方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有线性化方法存在两个主要问题：1) 基于滑动窗口分区的token路由导致位置选择，无法捕捉token特定的全局重要性；2) 可学习的特征映射导致分布偏移，扭曲预训练特征幅度。

Method: 提出STILL框架：1) 引入具有强局部-全局一致性的自显著性分数，通过滑动窗口计算实现准确的token选择；2) 设计保持范数的特征映射(NP-Map)，将特征方向与幅度解耦并重新注入预训练范数；3) 采用统一训练-推理架构，支持分块并行化和延迟选择以提高硬件效率。

Result: 在常识和一般推理任务上匹配或超越原始预训练模型，在长上下文基准测试中相比先前线性化注意力方法实现高达86.2%的相对改进。

Conclusion: STILL通过自显著性分数和保持范数的特征映射有效解决了线性注意力方法的局限性，在保持预训练表示的同时显著提升了长上下文处理性能。

Abstract: Linearizing pretrained large language models (LLMs) primarily relies on intra-layer hybrid attention mechanisms to alleviate the quadratic complexity of standard softmax attention. Existing methods perform token routing based on sliding-window partitions, resulting in position-based selection and fails to capture token-specific global importance. Meanwhile, linear attention further suffers from distribution shift caused by learnable feature maps that distort pretrained feature magnitudes. Motivated by these limitations, we propose STILL, an intra-layer hybrid linearization framework for efficiently linearizing LLMs. STILL introduces a Self-Saliency Score with strong local-global consistency, enabling accurate token selection using sliding-window computation, and retains salient tokens for sparse softmax attention while summarizing the remaining context via linear attention. To preserve pretrained representations, we design a Norm-Preserved Feature Map (NP-Map) that decouples feature direction from magnitude and reinjects pretrained norms. We further adopt a unified training-inference architecture with chunk-wise parallelization and delayed selection to improve hardware efficiency. Experiments show that STILL matches or surpasses the original pretrained model on commonsense and general reasoning tasks, and achieves up to a 86.2% relative improvement over prior linearized attention methods on long-context benchmarks.

</details>


### [815] [ECHO-2: A Large Scale Distributed Rollout Framework for Cost-efficient Reinforcement Learning](https://arxiv.org/abs/2602.02192)
*Jie Xiao,Meng Chen,Qingnan Ren,Song Jingwei,Jiaqi Huang,Yangshen Deng,Chris Tong,Wanyi Chen,Suli Wang,Ziqian Bi,Shuo Lu,Yiqun Duan,Lynn Ai,Eric Yang,Bill Shi*

Main category: cs.LG

TL;DR: ECHO-2是一个用于大语言模型后训练强化学习的分布式框架，通过远程推理工作节点、策略传播重叠和成本感知调度，在保持RL奖励的同时显著提升成本效率。


<details>
  <summary>Details</summary>
Motivation: 强化学习是LLM后训练的关键阶段，涉及生成、评估和学习的重复交互。分布式执行生成可以利用更经济高效的推理资源，但面临广域协调和策略传播的挑战。

Method: 结合集中式学习与分布式生成，将策略陈旧度作为用户可控参数，允许生成、传播和训练重叠。引入基于重叠的容量模型，采用对等辅助流水线广播和成本感知的异构工作节点激活。

Result: 在真实广域带宽环境下对4B和8B模型进行GRPO后训练的实验表明，ECHO-2在保持与强基线相当的RL奖励的同时，显著提高了成本效率。

Conclusion: ECHO-2通过创新的分布式架构解决了LLM后训练RL中的协调和传播瓶颈，实现了成本效率与模型性能的良好平衡，为大规模RL训练提供了实用解决方案。

Abstract: Reinforcement learning (RL) is a critical stage in post-training large language models (LLMs), involving repeated interaction between rollout generation, reward evaluation, and centralized learning. Distributing rollout execution offers opportunities to leverage more cost-efficient inference resources, but introduces challenges in wide-area coordination and policy dissemination. We present ECHO-2, a distributed RL framework for post-training with remote inference workers and non-negligible dissemination latency. ECHO-2 combines centralized learning with distributed rollouts and treats bounded policy staleness as a user-controlled parameter, enabling rollout generation, dissemination, and training to overlap. We introduce an overlap-based capacity model that relates training time, dissemination latency, and rollout throughput, yielding a practical provisioning rule for sustaining learner utilization. To mitigate dissemination bottlenecks and lower cost, ECHO-2 employs peer-assisted pipelined broadcast and cost-aware activation of heterogeneous workers. Experiments on GRPO post-training of 4B and 8B models under real wide-area bandwidth regimes show that ECHO-2 significantly improves cost efficiency while preserving RL reward comparable to strong baselines.

</details>


### [816] [State Rank Dynamics in Linear Attention LLMs](https://arxiv.org/abs/2602.02195)
*Ao Sun,Hongtao Zhang,Heng Zhou,Yixuan Ma,Yiran Qin,Tongrui Su,Yan Liu,Zhanyu Ma,Jun Xu,Jiuchong Gao,Jinghua Hao,Renqing He*

Main category: cs.LG

TL;DR: 线性注意力LLM的压缩状态存在状态秩分层现象：部分注意力头保持接近零的有效秩，另一部分则快速增长至上限，这是预训练获得的结构特性而非输入依赖的瞬态。低秩头对推理至关重要，高秩头具有冗余性，据此提出的联合秩-范数剪枝策略能减少38.9%的KV缓存开销。


<details>
  <summary>Details</summary>
Motivation: 线性注意力LLM通过将上下文压缩为固定大小的状态矩阵实现恒定时间推理，但其压缩状态的内部动态机制尚不明确。本文旨在深入探究最先进线性注意力模型的运行时状态动态特性。

Method: 对最先进的线性注意力模型进行全面的运行时状态动态研究，发现状态秩分层现象，并通过大量实验验证该现象在不同推理上下文中的一致性。使用诊断探针分析功能差异，并基于此提出联合秩-范数剪枝策略。

Result: 发现线性注意力头存在明显的状态秩分层：一组头保持接近零的有效秩，另一组头则快速增长并收敛到上限。该动态特性在不同推理上下文中保持高度一致，表明这是预训练获得的结构特性。低秩头对模型推理至关重要，而高秩头具有显著冗余性。提出的联合秩-范数剪枝策略能减少38.9%的KV缓存开销，同时基本保持模型准确性。

Conclusion: 线性注意力LLM的状态动态存在固有的状态秩分层现象，这是模型预训练获得的结构特性。低秩头负责核心推理功能，高秩头具有冗余性，这一发现为模型压缩和优化提供了新思路，提出的剪枝策略能有效减少KV缓存开销。

Abstract: Linear Attention Large Language Models (LLMs) offer a compelling recurrent formulation that compresses context into a fixed-size state matrix, enabling constant-time inference. However, the internal dynamics of this compressed state remain largely opaque. In this work, we present a comprehensive study on the runtime state dynamics of state-of-the-art Linear Attention models. We uncover a fundamental phenomenon termed State Rank Stratification, characterized by a distinct spectral bifurcation among linear attention heads: while one group maintains an effective rank oscillating near zero, the other exhibits rapid growth that converges to an upper bound. Extensive experiments across diverse inference contexts reveal that these dynamics remain strikingly consistent, indicating that the identity of a head,whether low-rank or high-rank,is an intrinsic structural property acquired during pre-training, rather than a transient state dependent on the input data. Furthermore, our diagnostic probes reveal a surprising functional divergence: low-rank heads are indispensable for model reasoning, whereas high-rank heads exhibit significant redundancy. Leveraging this insight, we propose Joint Rank-Norm Pruning, a zero-shot strategy that achieves a 38.9\% reduction in KV-cache overhead while largely maintaining model accuracy.

</details>


### [817] [Hierarchical Adaptive Eviction for KV Cache Management in Multimodal Language Models](https://arxiv.org/abs/2602.02197)
*Xindian Ma,Yidi Lu,Peng Zhang,Jing Zhang*

Main category: cs.LG

TL;DR: HAE是一种针对多模态大语言模型的KV缓存逐出框架，通过分层自适应策略优化视觉-文本token交互，在保持性能的同时显著减少内存使用和加速推理。


<details>
  <summary>Details</summary>
Motivation: Transformer架构的二次方内存和计算成本是多模态LLM的瓶颈，现有KV缓存逐出策略未能处理视觉和文本token之间的异质注意力分布，导致效率低下或性能下降。

Method: 提出分层自适应逐出框架：1)预填充阶段使用双注意力剪枝（利用视觉token稀疏性和注意力方差）；2)解码阶段使用动态解码逐出策略（受操作系统回收站启发）。通过跨层最小化KV缓存使用和索引广播减少计算开销。

Result: 在图像理解任务中减少41% KV缓存内存，仅损失0.3%准确率；在故事生成推理中加速1.5倍，同时保持输出质量（基于Phi3.5-Vision-Instruct模型）。

Conclusion: HAE通过分层自适应策略有效解决了多模态LLM中视觉-文本token交互的异质注意力分布问题，在保持性能的同时显著提升了效率，为MLLM的KV缓存优化提供了新思路。

Abstract: The integration of visual information into Large Language Models (LLMs) has enabled Multimodal LLMs (MLLMs), but the quadratic memory and computational costs of Transformer architectures remain a bottleneck. Existing KV cache eviction strategies fail to address the heterogeneous attention distributions between visual and text tokens, leading to suboptimal efficiency or degraded performance. In this paper, we propose Hierarchical Adaptive Eviction (HAE), a KV cache eviction framework that optimizes text-visual token interaction in MLLMs by implementing Dual-Attention Pruning during pre-filling (leveraging visual token sparsity and attention variance) and a Dynamic Decoding Eviction Strategy (inspired by OS Recycle Bins) during decoding. HAE minimizes KV cache usage across layers, reduces computational overhead via index broadcasting, and theoretically ensures superior information integrity and lower error bounds compared to greedy strategies, enhancing efficiency in both comprehension and generation tasks. Empirically, HAE reduces KV-Cache memory by 41\% with minimal accuracy loss (0.3\% drop) in image understanding tasks and accelerates story generation inference by 1.5x while maintaining output quality on Phi3.5-Vision-Instruct model.

</details>


### [818] [Cardinality-Preserving Structured Sparse Graph Transformers for Molecular Property Prediction](https://arxiv.org/abs/2602.02201)
*Abhijit Gupta*

Main category: cs.LG

TL;DR: CardinalGraphFormer是一种图Transformer模型，通过结合Graphormer的结构偏置和结构化稀疏注意力，在药物发现中实现高效分子性质预测


<details>
  <summary>Details</summary>
Motivation: 药物发现需要高效的分子性质预测，但化学空间巨大（约10^60个类药分子），而仅有数千种获批药物，因此需要利用自监督预训练从大量未标记分子数据中学习数据高效的分子表示

Method: 提出CardinalGraphFormer图Transformer，整合Graphormer的结构偏置（最短路径距离、中心性）和直接键边偏置，采用结构化稀疏注意力（限制最短路径距离≤3），并添加基数保持的非归一化聚合通道

Result: 在完全匹配的评估协议下，CardinalGraphFormer在所有11个评估任务中均提升平均性能，在MoleculeNet、OGB和TDC ADMET基准的10/11个任务上取得统计显著增益

Conclusion: CardinalGraphFormer通过结合结构化稀疏注意力和基数保持聚合，在分子表示学习中表现出优越性能，为数据高效的药物发现提供了有效解决方案

Abstract: Drug discovery motivates efficient molecular property prediction under limited labeled data. Chemical space is vast, often estimated at approximately 10^60 drug-like molecules, while only thousands of drugs have been approved. As a result, self-supervised pretraining on large unlabeled molecular corpora has become essential for data-efficient molecular representation learning. We introduce **CardinalGraphFormer**, a graph transformer that incorporates Graphormer-inspired structural biases, including shortest-path distance and centrality, as well as direct-bond edge bias, within a structured sparse attention regime limited to shortest-path distance <= 3. The model further augments this design with a cardinality-preserving unnormalized aggregation channel over the same support set. Pretraining combines contrastive graph-level alignment with masked attribute reconstruction. Under a fully matched evaluation protocol, CardinalGraphFormer improves mean performance across all 11 evaluated tasks and achieves statistically significant gains on 10 of 11 public benchmarks spanning MoleculeNet, OGB, and TDC ADMET tasks when compared to strong reproduced baselines.

</details>


### [819] [Fat-Cat: Document-Driven Metacognitive Multi-Agent System for Complex Reasoning](https://arxiv.org/abs/2602.02206)
*Tong Yang,Yemin Wang,Chaoning Zhang,Aming Wu*

Main category: cs.LG

TL;DR: Fat-Cat提出了一种基于文档的智能体架构，通过Markdown文档表示状态、文本策略演化和闭环监控，显著提升LLM智能体的性能表现。


<details>
  <summary>Details</summary>
Motivation: 现有智能体框架依赖嵌套JSON等语法繁重的状态表示，迫使模型将大量注意力消耗在语法处理而非语义推理上，限制了LLM智能体的效率。

Method: 提出三个核心组件：1) 语义文件系统，用Markdown文档表示状态；2) 文本策略演化模块，积累任务解决知识；3) 闭环监控器，监控推理轨迹减少幻觉。

Result: 在推理、检索和编码基准测试中表现优异，使Kimi-k2模型在HotPotQA上超越GPT-4o基线。将文档状态替换为JSON会导致性能下降，验证了文档驱动状态建模的必要性。

Conclusion: 文档驱动的状态表示比传统语法繁重的状态管理更有效，能显著提升LLM智能体的信号噪声比和实际性能。

Abstract: The effectiveness of LLM-based agents is often limited not by model capacity alone, but by how efficiently contextual information is utilized at runtime. Existing agent frameworks rely on rigid, syntax-heavy state representations such as nested JSON, which require models to devote a substantial portion of their limited attention to syntactic processing rather than semantic reasoning. In this paper, we propose Fat-Cat, a document-driven agent architecture that improves the signal-to-noise ratio of state management. By integrating three key components: (1) a Semantic File System that represents agent state as Markdown documents aligned with common pre-training corpora, (2) a Textual Strategy Evolution module that accumulates task-solving knowledge without parameter updates, and (3) a Closed-Loop Watcher that monitors reasoning trajectories to reduce hallucinations. Extensive reasoning, retrieval, and coding benchmarks, Fat-Cat consistently improves agent performance. It enables the Kimi-k2 model to outperform the proprietary GPT-4o baseline on HotPotQA. Replacing the document-based state with JSON leads to performance drop, while empirically validating the critical necessity of document-driven state modeling over rigid syntax. The code is available at https://github.com/answeryt/Fat-Cat.

</details>


### [820] [Generating Physically Sound Designs from Text and a Set of Physical Constraints](https://arxiv.org/abs/2602.02213)
*Gregory Barber,Todd C. Henry,Mulugeta A. Haile*

Main category: cs.LG

TL;DR: TIDES是一种基于文本描述和物理约束的文本驱动设计方法，能同时优化结构（拓扑）和视觉属性，生成物理合理的设计。


<details>
  <summary>Details</summary>
Motivation: 需要一种能够根据文本描述生成既满足物理约束又符合视觉要求的设计方法，将文本驱动的创意与工程物理性能要求相结合。

Method: 使用预训练的文本-图像模型衡量设计与文本提示的视觉对齐度，结合可微分物理模拟器评估物理性能，联合优化结构和视觉属性。

Result: TIDES能在不同载荷、支撑条件和分辨率下成功优化结构设计，实验验证显示3D打印的2D梁设计在三点弯曲测试中满足工程设计要求（柔顺性和密度），同时体现文本指定的特征。

Conclusion: TIDES能够有效联合优化物理性能和视觉对齐目标，生成既满足工程设计要求又体现文本特征的设计方案。

Abstract: We present TIDES, a text informed design approach for generating physically sound designs based on a textual description and a set of physical constraints. TIDES jointly optimizes structural (topology) and visual properties. A pre-trained text-image model is used to measure the design's visual alignment with a text prompt and a differentiable physics simulator is used to measure its physical performance. We evaluate TIDES on a series of structural optimization problems operating under different load and support conditions, at different resolutions, and experimentally in the lab by performing the 3-point bending test on 2D beam designs that are extruded and 3D printed. We find that it can jointly optimize the two objectives and return designs that satisfy engineering design requirements (compliance and density) while utilizing features specified by text.

</details>


### [821] [Scientific Theory of a Black-Box: A Life Cycle-Scale XAI Framework Based on Constructive Empiricism](https://arxiv.org/abs/2602.02215)
*Sebastian Müller,Vanessa Toborek,Eike Stadtländer,Tamás Horváth,Brendan Balcerak Jackson,Christian Bauckhage*

Main category: cs.LG

TL;DR: 提出"黑盒科学理论"(SToBB)框架，将可解释AI信息整合为伴随黑盒模型生命周期的持久化、可审计的文档，通过CoBoT算法实现规则化代理的在线构建与维护。


<details>
  <summary>Details</summary>
Motivation: 当前可解释AI算法只能回答特定问题，缺乏将解释信息整合为伴随黑盒模型整个生命周期的持久化、可审计文档的系统方法。

Method: 基于构造经验主义提出SToBB框架，包含可扩展的观察基础、可追溯的假设类、构造与修订算法组件，以及充分的文档记录。开发CoBoT算法在线构建和维护经验充分的规则化代理。

Result: 实现了完整的SToBB实例化，用于表格任务的神经网络分类器，展示了CoBoT算法能够随着观察积累构建和维护经验充分的规则化代理。

Conclusion: SToBB为黑盒模型提供了生命周期尺度、可检查的参考点，支持一致、可重用的分析和系统性外部审查，是可解释AI的重要进展。

Abstract: Explainable AI (XAI) offers a growing number of algorithms that aim to answer specific questions about black-box models. What is missing is a principled way to consolidate explanatory information about a fixed black-box model into a persistent, auditable artefact, that accompanies the black-box throughout its life cycle. We address this gap by introducing the notion of a scientific theory of a black (SToBB). Grounded in Constructive Empiricism, a SToBB fulfils three obligations: (i) empirical adequacy with respect to all available observations of black-box behaviour, (ii) adaptability via explicit update commitments that restore adequacy when new observations arrive, and (iii) auditability through transparent documentation of assumptions, construction choices, and update behaviour. We operationalise these obligations as a general framework that specifies an extensible observation base, a traceable hypothesis class, algorithmic components for construction and revision, and documentation sufficient for third-party assessment. Explanations for concrete stakeholder needs are then obtained by querying the maintained record through interfaces, rather than by producing isolated method outputs. As a proof of concept, we instantiate a complete SToBB for a neural-network classifier on a tabular task and introduce the Constructive Box Theoriser (CoBoT) algorithm, an online procedure that constructs and maintains an empirically adequate rule-based surrogate as observations accumulate. Together, these contributions position SToBBs as a life cycle-scale, inspectable point of reference that supports consistent, reusable analyses and systematic external scrutiny.

</details>


### [822] [Prediction-Powered Risk Monitoring of Deployed Models for Detecting Harmful Distribution Shifts](https://arxiv.org/abs/2602.02229)
*Guangyi Zhang,Yunlong Cai,Guanding Yu,Osvaldo Simeone*

Main category: cs.LG

TL;DR: 提出预测驱动的风险监控（PPRM），一种基于预测驱动推理的半监督风险监控方法，用于在标签数据有限的情况下监控动态环境中的模型性能


<details>
  <summary>Details</summary>
Motivation: 在动态环境中监控模型性能时，标记数据通常有限，需要一种能够在有限标签数据下有效监控风险的方法

Method: 基于预测驱动推理（PPI），通过结合合成标签和少量真实标签，构建运行风险的有效下界，并通过与名义风险上界的阈值比较来检测有害偏移

Result: PPRM在图像分类、大语言模型和电信监控任务中表现出有效性，满足无假设有限样本的误报概率保证

Conclusion: PPRM提供了一种在标签数据有限的情况下监控动态环境中模型性能的有效方法，具有理论保证和实际应用价值

Abstract: We study the problem of monitoring model performance in dynamic environments where labeled data are limited. To this end, we propose prediction-powered risk monitoring (PPRM), a semi-supervised risk-monitoring approach based on prediction-powered inference (PPI). PPRM constructs anytime-valid lower bounds on the running risk by combining synthetic labels with a small set of true labels. Harmful shifts are detected via a threshold-based comparison with an upper bound on the nominal risk, satisfying assumption-free finite-sample guarantees in the probability of false alarm. We demonstrate the effectiveness of PPRM through extensive experiments on image classification, large language model (LLM), and telecommunications monitoring tasks.

</details>


### [823] [SEDformer: Event-Synchronous Spiking Transformers for Irregular Telemetry Time Series Forecasting](https://arxiv.org/abs/2602.02230)
*Ziyu Zhou,Yuchen Fang,Weilin Ruan,Shiyu Wang,James Kwok,Yuxuan Liang*

Main category: cs.LG

TL;DR: 提出SEDformer，一种基于脉冲神经网络的SED增强型脉冲Transformer，用于不规则多元时间序列预测，通过事件驱动的方式自然匹配IMTS的稀疏-事件对偶特性，在提高预测精度的同时降低能耗和内存使用。


<details>
  <summary>Details</summary>
Motivation: 现有基于图和Transformer的预测方法忽略了不规则多元时间序列(IMTS)的稀疏-事件对偶(SED)特性：均匀网格对齐和填充违反了稀疏性，关系重构破坏了事件语义的局部时间连续性。需要一种更忠实、自然地建模IMTS的方法。

Method: SEDformer包含三个核心组件：(1) SED-based Spike Encoder使用事件对齐LIF神经元将原始观测转换为事件同步脉冲；(2) Event-Preserving Temporal Downsampling模块压缩长时间间隔同时保留显著脉冲；(3) SED-based Spike Transformer blocks基于膜电位的线性注意力机制建模序列内依赖关系。

Result: 在公开的telemetry IMTS数据集上，SEDformer实现了最先进的预测精度，同时显著降低了能耗和内存使用。

Conclusion: SEDformer为建模IMTS提供了一条自然且高效的路径，通过脉冲神经网络的事件驱动特性与IMTS的SED特性自然对齐，在保持高预测精度的同时实现了能效优化。

Abstract: Telemetry streams from large-scale Internet-connected systems (e.g., IoT deployments and online platforms) naturally form an irregular multivariate time series (IMTS) whose accurate forecasting is operationally vital. A closer examination reveals a defining Sparsity-Event Duality (SED) property of IMTS, i.e., long stretches with sparse or no observations are punctuated by short, dense bursts where most semantic events (observations) occur. However, existing Graph- and Transformer-based forecasters ignore SED: pre-alignment to uniform grids with heavy padding violates sparsity by inflating sequences and forcing computation at non-informative steps, while relational recasting weakens event semantics by disrupting local temporal continuity. These limitations motivate a more faithful and natural modeling paradigm for IMTS that aligns with its SED property. We find that Spiking Neural Networks meet this requirement, as they communicate via sparse binary spikes and update in an event-driven manner, aligning naturally with the SED nature of IMTS. Therefore, we present SEDformer, an SED-enhanced Spiking Transformer for telemetry IMTS forecasting that couples: (1) a SED-based Spike Encoder converts raw observations into event synchronous spikes using an Event-Aligned LIF neuron, (2) an Event-Preserving Temporal Downsampling module compresses long gaps while retaining salient firings and (3) a stack of SED-based Spike Transformer blocks enable intra-series dependency modeling with a membrane-based linear attention driven by EA-LIF spiking features. Experiments on public telemetry IMTS datasets show that SEDformer attains state-of-the-art forecasting accuracy while reducing energy and memory usage, providing a natural and efficient path for modeling IMTS.

</details>


### [824] [Geometry- and Relation-Aware Diffusion for EEG Super-Resolution](https://arxiv.org/abs/2602.02238)
*Laura Yao,Gengwei Zhang,Moajjem Chowdhury,Yunmei Liu,Tianlong Chen*

Main category: cs.LG

TL;DR: TopoDiff：一种几何和关系感知的扩散模型，用于EEG空间超分辨率，通过整合拓扑感知图像嵌入和动态通道关系图来提升EEG空间生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有的EEG空间超分辨率方法缺乏对生理空间结构的认知，限制了空间生成性能。人类专家在解读EEG空间模式时会考虑拓扑结构，因此需要开发能够理解EEG生理几何和通道关系的模型。

Method: 提出TopoDiff模型，包含两个核心组件：1）从EEG地形表示中提取的拓扑感知图像嵌入，提供全局几何上下文；2）动态通道关系图，编码电极间关系并随时间动态演化。这种设计创建了一个空间基础稳固的EEG超分辨率框架。

Result: 在多个EEG数据集上（SEED/SEED-IV情感识别、PhysioNet运动想象、TUSZ癫痫检测）都取得了显著提升：1）生成保真度大幅提高；2）下游EEG任务性能有明显改善。

Conclusion: TopoDiff通过整合拓扑感知和动态关系建模，成功解决了现有EEG空间超分辨率方法缺乏生理空间结构认知的问题，在生成质量和下游任务性能上都取得了显著提升，为EEG空间超分辨率提供了更有效的解决方案。

Abstract: Recent electroencephalography (EEG) spatial super-resolution (SR) methods, while showing improved quality by either directly predicting missing signals from visible channels or adapting latent diffusion-based generative modeling to temporal data, often lack awareness of physiological spatial structure, thereby constraining spatial generation performance. To address this issue, we introduce TopoDiff, a geometry- and relation-aware diffusion model for EEG spatial super-resolution. Inspired by how human experts interpret spatial EEG patterns, TopoDiff incorporates topology-aware image embeddings derived from EEG topographic representations to provide global geometric context for spatial generation, together with a dynamic channel-relation graph that encodes inter-electrode relationships and evolves with temporal dynamics. This design yields a spatially grounded EEG spatial super-resolution framework with consistent performance improvements. Across multiple EEG datasets spanning diverse applications, including SEED/SEED-IV for emotion recognition, PhysioNet motor imagery (MI/MM), and TUSZ for seizure detection, our method achieves substantial gains in generation fidelity and leads to notable improvements in downstream EEG task performance.

</details>


### [825] [Interpretability in Deep Time Series Models Demands Semantic Alignment](https://arxiv.org/abs/2602.02239)
*Giovanni De Felice,Riccardo D'Elia,Alberto Termine,Pietro Barbiero,Giuseppe Marra,Silvia Santini*

Main category: cs.LG

TL;DR: 该论文提出深度时间序列模型的解释性应追求语义对齐，即预测结果需用对终端用户有意义的变量表达，并通过允许用户约束的时空机制实现，且这种对齐必须在时间演化中保持。


<details>
  <summary>Details</summary>
Motivation: 当前深度时间序列模型虽然预测性能不断提升，但其黑盒特性限制了实际部署。现有的解释性方法主要关注解释模型内部计算，而没有解决这些解释是否与人类对研究现象的理解方式相一致的问题。

Method: 论文形式化了语义对齐的要求，强调预测必须用对用户有意义的变量表达，并通过允许用户约束的时空机制实现。特别重要的是，一旦建立语义对齐，必须在时间演化中保持这种对齐——这是静态设置中没有的约束。

Result: 论文为语义对齐的深度时间序列模型提供了一个蓝图，识别了支持信任的特性，并讨论了模型设计的影响。这为开发更可信、更可解释的时间序列模型提供了理论框架。

Conclusion: 深度时间序列模型的解释性应该从解释内部计算转向语义对齐，确保模型预测以用户可理解的方式表达，并在时间维度上保持一致性，从而建立用户信任并促进实际部署。

Abstract: Deep time series models continue to improve predictive performance, yet their deployment remains limited by their black-box nature. In response, existing interpretability approaches in the field keep focusing on explaining the internal model computations, without addressing whether they align or not with how a human would reason about the studied phenomenon. Instead, we state interpretability in deep time series models should pursue semantic alignment: predictions should be expressed in terms of variables that are meaningful to the end user, mediated by spatial and temporal mechanisms that admit user-dependent constraints. In this paper, we formalize this requirement and require that, once established, semantic alignment must be preserved under temporal evolution: a constraint with no analog in static settings. Provided with this definition, we outline a blueprint for semantically aligned deep time series models, identify properties that support trust, and discuss implications for model design.

</details>


### [826] [Variational Entropic Optimal Transport](https://arxiv.org/abs/2602.02241)
*Roman Dyachenko,Nikita Gushchin,Kirill Sokolov,Petr Mokrov,Evgeny Burnaev,Alexander Korotin*

Main category: cs.LG

TL;DR: 提出VarEOT方法，通过变分重构log-partition项，避免了MCMC模拟训练，在连续空间熵最优传输中实现高效优化


<details>
  <summary>Details</summary>
Motivation: 现有连续空间熵最优传输方法存在计算效率问题：要么限制传输族（高斯混合参数化）获得闭式解，要么需要基于模拟的训练过程。需要更高效、通用的优化方法

Method: 提出VarEOT方法，将log-partition项重构为可处理的辅助正归一化器的最小化问题，使用随机梯度进行可微学习目标优化，避免训练期间的MCMC模拟

Result: 在合成数据和未配对图像到图像翻译任务上展示了竞争性或改进的翻译质量，与使用相同弱对偶EOT目标的其他求解器相比，验证了所提优化原则的优势

Conclusion: VarEOT通过变分重构log-partition项，提供了理论保证（有限样本泛化界和通用函数逼近下的近似结果），实现了更高效、通用的熵最优传输优化方法

Abstract: Entropic optimal transport (EOT) in continuous spaces with quadratic cost is a classical tool for solving the domain translation problem. In practice, recent approaches optimize a weak dual EOT objective depending on a single potential, but doing so is computationally not efficient due to the intractable log-partition term. Existing methods typically resolve this obstacle in one of two ways: by significantly restricting the transport family to obtain closed-form normalization (via Gaussian-mixture parameterizations), or by using general neural parameterizations that require simulation-based training procedures. We propose Variational Entropic Optimal Transport (VarEOT), based on an exact variational reformulation of the log-partition $\log \mathbb{E}[\exp(\cdot)]$ as a tractable minimization over an auxiliary positive normalizer. This yields a differentiable learning objective optimized with stochastic gradients and avoids the necessity of MCMC simulations during the training. We provide theoretical guarantees, including finite-sample generalization bounds and approximation results under universal function approximation. Experiments on synthetic data and unpaired image-to-image translation demonstrate competitive or improved translation quality, while comparisons within the solvers that use the same weak dual EOT objective support the benefit of the proposed optimization principle.

</details>


### [827] [Alignment-Aware Model Adaptation via Feedback-Guided Optimization](https://arxiv.org/abs/2602.02258)
*Gaurav Bhatt,Aditya Chinchure,Jiawei Zhou,Leonid Sigal*

Main category: cs.LG

TL;DR: 提出一个对齐感知的微调框架，通过策略梯度正则化整合外部对齐信号，使用自适应门控机制平衡监督和对齐梯度，学习对完全未对齐输入的弃权行为，在保持下游任务性能的同时减少有害和幻觉输出。


<details>
  <summary>Details</summary>
Motivation: 传统微调方法主要优化任务目标，忽略了安全性和避免幻觉等关键对齐目标，导致下游微调可能降低模型对齐性，无法纠正预训练中已有的未对齐行为。

Method: 提出对齐感知微调框架：1) 通过策略梯度正则化整合外部对齐信号反馈；2) 引入自适应门控机制，基于每个样本动态平衡监督和对齐驱动的梯度，优先处理不确定或未对齐的案例；3) 学习对完全未对齐输入的弃权行为，将保守响应直接整合到微调模型中。

Result: 在通用和领域特定的指令微调基准测试中，一致减少了有害和幻觉输出，同时不牺牲下游任务性能。额外分析显示对对抗性微调、基于提示的攻击和不安全初始化的鲁棒性。

Conclusion: 自适应门控对齐优化是一种有效的对齐保持和对齐恢复模型适应方法，能够在微调过程中同时优化任务目标和对齐目标。

Abstract: Fine-tuning is the primary mechanism for adapting foundation models to downstream tasks; however, standard approaches largely optimize task objectives in isolation and do not account for secondary yet critical alignment objectives (e.g., safety and hallucination avoidance). As a result, downstream fine-tuning can degrade alignment and fail to correct pre-existing misaligned behavior. We propose an alignment-aware fine-tuning framework that integrates feedback from an external alignment signal through policy-gradient-based regularization. Our method introduces an adaptive gating mechanism that dynamically balances supervised and alignment-driven gradients on a per-sample basis, prioritizing uncertain or misaligned cases while allowing well-aligned examples to follow standard supervised updates. The framework further learns abstention behavior for fully misaligned inputs, incorporating conservative responses directly into the fine-tuned model. Experiments on general and domain-specific instruction-tuning benchmarks demonstrate consistent reductions in harmful and hallucinated outputs without sacrificing downstream task performance. Additional analyses show robustness to adversarial fine-tuning, prompt-based attacks, and unsafe initializations, establishing adaptively gated alignment optimization as an effective approach for alignment-preserving and alignment-recovering model adaptation.

</details>


### [828] [Segment to Focus: Guiding Latent Action Models in the Presence of Distractors](https://arxiv.org/abs/2602.02259)
*Hamza Adnan,Matthew T. Jackson,Alexey Zakharov*

Main category: cs.LG

TL;DR: MaskLAM通过引入视觉智能体分割来改进潜在动作模型，通过加权重建损失优先处理显著信息，减少动作相关背景噪声的干扰，在MuJoCo任务上实现高达4倍的奖励提升。


<details>
  <summary>Details</summary>
Motivation: 潜在动作模型（LAMs）能够从原始观察中学习提取动作相关表示，但面临一个关键挑战：难以将动作相关特征与动作相关的噪声（如背景运动）分离。如果无法过滤这些干扰因素，LAMs会捕捉虚假相关性并构建次优的潜在动作空间。

Method: MaskLAM是对LAM训练的轻量级修改，通过整合视觉智能体分割来缓解这一问题。该方法利用预训练基础模型的分割掩码来加权LAM的重建损失，从而优先处理显著信息而非背景元素，且无需架构修改。

Result: 在添加了动作相关背景噪声的连续控制MuJoCo任务上，MaskLAM相比标准基线实现了高达4倍的累积奖励提升，并通过线性探针评估显示潜在动作质量提高了3倍。

Conclusion: MaskLAM通过简单的加权重建损失方法，有效解决了LAMs中的动作相关噪声问题，显著提升了强化学习性能，为从无标签视频中学习提供了更鲁棒的表示。

Abstract: Latent Action Models (LAMs) learn to extract action-relevant representations solely from raw observations, enabling reinforcement learning from unlabelled videos and significantly scaling available training data. However, LAMs face a critical challenge in disentangling action-relevant features from action-correlated noise (e.g., background motion). Failing to filter these distractors causes LAMs to capture spurious correlations and build sub-optimal latent action spaces. In this paper, we introduce MaskLAM -- a lightweight modification to LAM training to mitigate this issue by incorporating visual agent segmentation. MaskLAM utilises segmentation masks from pretrained foundation models to weight the LAM reconstruction loss, thereby prioritising salient information over background elements while requiring no architectural modifications. We demonstrate the effectiveness of our method on continuous-control MuJoCo tasks, modified with action-correlated background noise. Our approach yields up to a 4x increase in accrued rewards compared to standard baselines and a 3x improvement in the latent action quality, as evidenced by linear probe evaluation.

</details>


### [829] [Learning Markov Decision Processes under Fully Bandit Feedback](https://arxiv.org/abs/2602.02260)
*Zhengjia Zhuo,Anupam Gupta,Viswanath Nagarajan*

Main category: cs.LG

TL;DR: 本文提出首个针对完全bandit反馈的episodic MDP的高效学习算法，实现了$\widetilde{O}(\sqrt{T})$的遗憾界，并在k-item先知不等式等经典随机优化问题上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习假设智能体能够观察到每个访问的状态-动作对及其即时奖励，但在实际应用中这种详细反馈往往不现实。本文研究更受限的"完全bandit"反馈模型，智能体仅能获得聚合奖励，无法观察到访问的状态-动作对。

Method: 提出首个针对episodic MDP的完全bandit反馈高效学习算法，通过创新的技术处理高度受限的反馈信息，同时针对"有序"MDPs获得了改进的近乎紧致的遗憾界。

Result: 算法实现了$\widetilde{O}(\sqrt{T})$的遗憾界，遗憾对horizon长度$\H$具有指数依赖（证明这是必要的）。在k-item先知不等式问题上，尽管反馈高度受限，算法性能与具有详细状态-动作反馈的最先进学习算法（UCB-VI）相当。

Conclusion: 本文首次证明了在完全bandit反馈下，episodic MDPs可以实现高效的$\widetilde{O}(\sqrt{T})$遗憾学习，为高度受限反馈环境下的强化学习提供了理论保证和实用算法。

Abstract: A standard assumption in Reinforcement Learning is that the agent observes every visited state-action pair in the associated Markov Decision Process (MDP), along with the per-step rewards. Strong theoretical results are known in this setting, achieving nearly-tight $Θ(\sqrt{T})$-regret bounds. However, such detailed feedback can be unrealistic, and recent research has investigated more restricted settings such as trajectory feedback, where the agent observes all the visited state-action pairs, but only a single \emph{aggregate} reward. In this paper, we consider a far more restrictive ``fully bandit'' feedback model for episodic MDPs, where the agent does not even observe the visited state-action pairs -- it only learns the aggregate reward. We provide the first efficient bandit learning algorithm for episodic MDPs with $\widetilde{O}(\sqrt{T})$ regret. Our regret has an exponential dependence on the horizon length $\H$, which we show is necessary. We also obtain improved nearly-tight regret bounds for ``ordered'' MDPs; these can be used to model classical stochastic optimization problems such as $k$-item prophet inequality and sequential posted pricing. Finally, we evaluate the empirical performance of our algorithm for the setting of $k$-item prophet inequalities; despite the highly restricted feedback, our algorithm's performance is comparable to that of a state-of-art learning algorithm (UCB-VI) with detailed state-action feedback.

</details>


### [830] [Unlocking the Duality between Flow and Field Matching](https://arxiv.org/abs/2602.02261)
*Daniil Shlenskii,Alexander Varlamov,Nazar Buzun,Alexander Korotin*

Main category: cs.LG

TL;DR: CFM和IFM是两种生成模型框架，本文证明它们在一定条件下等价，但IFM更具表达力，并展示了这种对偶性如何互惠两种框架。


<details>
  <summary>Details</summary>
Motivation: 研究CFM（条件流匹配）和IFM（交互场匹配）这两种生成模型框架之间的关系，探究它们是本质上不同还是描述相同底层动态的两种方式。

Method: 通过构建CFM与前向IFM之间的双射来证明它们的等价性，并分析一般IFM的表达能力，展示IFM包含EFM等CFM无法实现的交互场。

Result: 证明CFM与前向IFM等价，但一般IFM更具表达力；这种对偶性为前向IFM提供了概率解释，并为CFM带来了新的IFM驱动技术。

Conclusion: CFM和IFM在特定条件下等价，但IFM更具一般性；这种对偶关系有助于两种框架的相互理解和改进。

Abstract: Conditional Flow Matching (CFM) unifies conventional generative paradigms such as diffusion models and flow matching. Interaction Field Matching (IFM) is a newer framework that generalizes Electrostatic Field Matching (EFM) rooted in Poisson Flow Generative Models (PFGM). While both frameworks define generative dynamics, they start from different objects: CFM specifies a conditional probability path in data space, whereas IFM specifies a physics-inspired interaction field in an augmented data space. This raises a basic question: are CFM and IFM genuinely different, or are they two descriptions of the same underlying dynamics? We show that they coincide for a natural subclass of IFM that we call forward-only IFM. Specifically, we construct a bijection between CFM and forward-only IFM. We further show that general IFM is strictly more expressive: it includes EFM and other interaction fields that cannot be realized within the standard CFM formulation. Finally, we highlight how this duality can benefit both frameworks: it provides a probabilistic interpretation of forward-only IFM and yields novel, IFM-driven techniques for CFM.

</details>


### [831] [Unsupervised Physics-Informed Operator Learning through Multi-Stage Curriculum Training](https://arxiv.org/abs/2602.02264)
*Paolo Marcandelli,Natansh Mathur,Stefano Markidis,Martina Siena,Stefano Mariani*

Main category: cs.LG

TL;DR: 提出PhIS-FNO方法，通过多阶段物理信息训练策略和样条傅里叶神经算子，实现无监督PDE求解，仅需边界标签数据就能达到监督学习精度


<details>
  <summary>Details</summary>
Motivation: 传统神经算子需要监督数据，而物理信息神经网络存在收敛不稳定和泛化能力有限的问题。需要一种既能利用物理约束进行无监督训练，又能稳定收敛且泛化能力强的PDE求解方法

Method: 1. 多阶段物理信息训练策略：先渐进强制边界条件，再引入内部残差，每阶段重新初始化优化器作为延续机制；2. PhIS-FNO架构：结合傅里叶层和Hermite样条核进行平滑残差评估

Result: 在标准基准测试中，PhIS-FNO仅使用狭窄边界区域的标签信息，就能达到与监督学习相当的精度水平

Conclusion: 分阶段、基于样条的优化为物理信息算子学习提供了一个稳健的范式，能够有效解决PDE问题

Abstract: Solving partial differential equations remains a central challenge in scientific machine learning. Neural operators offer a promising route by learning mappings between function spaces and enabling resolution-independent inference, yet they typically require supervised data. Physics-informed neural networks address this limitation through unsupervised training with physical constraints but often suffer from unstable convergence and limited generalization capability. To overcome these issues, we introduce a multi-stage physics-informed training strategy that achieves convergence by progressively enforcing boundary conditions in the loss landscape and subsequently incorporating interior residuals. At each stage the optimizer is re-initialized, acting as a continuation mechanism that restores stability and prevents gradient stagnation. We further propose the Physics-Informed Spline Fourier Neural Operator (PhIS-FNO), combining Fourier layers with Hermite spline kernels for smooth residual evaluation. Across canonical benchmarks, PhIS-FNO attains a level of accuracy comparable to that of supervised learning, using labeled information only along a narrow boundary region, establishing staged, spline-based optimization as a robust paradigm for physics-informed operator learning.

</details>


### [832] [HopFormer: Sparse Graph Transformers with Explicit Receptive Field Control](https://arxiv.org/abs/2602.02268)
*Sanggeon Yun,Raheeb Hassan,Ryozo Masukawa,Sungheon Jeong,Mohsen Imani*

Main category: cs.LG

TL;DR: HopFormer：一种仅通过头特定的n跳掩码稀疏注意力注入图结构，无需位置编码或架构修改的图Transformer，在计算成本随掩码稀疏度线性扩展的同时实现竞争性性能。


<details>
  <summary>Details</summary>
Motivation: 挑战当前图Transformer依赖显式位置/结构编码和密集全局注意力来整合图拓扑的普遍假设，探索更简单高效的设计方案。

Method: 引入HopFormer，通过头特定的n跳掩码稀疏注意力机制注入图结构，不使用位置编码或架构修改，提供显式可解释的接受场控制，实现真正稀疏的线性计算扩展。

Result: 在节点级和图级基准测试中达到竞争性或更优性能；发现密集全局注意力在强小世界图结构中不必要，局部注意力更稳定高效；在小世界效应较弱的图中全局注意力收益递减。

Conclusion: 挑战了图Transformer设计的普遍假设，证明稀疏控制注意力是原则性且高效的替代方案，为图Transformer设计提供了新视角。

Abstract: Graph Transformers typically rely on explicit positional or structural encodings and dense global attention to incorporate graph topology. In this work, we show that neither is essential. We introduce HopFormer, a graph Transformer that injects structure exclusively through head-specific n-hop masked sparse attention, without the use of positional encodings or architectural modifications. This design provides explicit and interpretable control over receptive fields while enabling genuinely sparse attention whose computational cost scales linearly with mask sparsity. Through extensive experiments on both node-level and graph-level benchmarks, we demonstrate that our approach achieves competitive or superior performance across diverse graph structures. Our results further reveal that dense global attention is often unnecessary: on graphs with strong small-world properties, localized attention yields more stable and consistently high performance, while on graphs with weaker small-world effects, global attention offers diminishing returns. Together, these findings challenge prevailing assumptions in graph Transformer design and highlight sparsity-controlled attention as a principled and efficient alternative.

</details>


### [833] [Backpropagation as Physical Relaxation: Exact Gradients in Finite Time](https://arxiv.org/abs/2602.02281)
*Antonino Emanuele Scurria*

Main category: cs.LG

TL;DR: 论文提出"Dyadic Backpropagation"框架，将反向传播解释为物理动力系统的有限时间松弛过程，通过拉格朗日理论在双状态空间推导全局能量函数，其鞍点动力学能在2L步内精确恢复标准反向传播。


<details>
  <summary>Details</summary>
Motivation: 传统上反向传播被视为符号计算应用链式法则，本文旨在证明它实际上可以作为物理动力系统的有限时间松弛过程自然涌现，为在模拟和神经形态硬件中实现精确梯度计算提供理论基础。

Method: 将前向推理建模为连续时间过程，应用非保守系统的拉格朗日理论处理非对称交互，在编码激活值和敏感度的双状态空间构建全局能量函数，通过该能量的鞍点动力学实现推理和信用分配。

Result: 证明对L层网络，单位步长欧拉离散化在恰好2L步内精确恢复标准反向传播，无需近似。与先前需要对称权重、渐近收敛或微小扰动的能量方法不同，本框架保证有限时间内获得精确梯度。

Conclusion: 反向传播是连续物理松弛过程的数字优化影子，为在模拟和神经形态基底中实现精确梯度计算提供了严格理论基础，这些基底天然支持连续动力学。

Abstract: Backpropagation, the foundational algorithm for training neural networks, is typically understood as a symbolic computation that recursively applies the chain rule. We show it emerges exactly as the finite-time relaxation of a physical dynamical system. By formulating feedforward inference as a continuous-time process and applying Lagrangian theory of non-conservative systems to handle asymmetric interactions, we derive a global energy functional on a doubled state space encoding both activations and sensitivities. The saddle-point dynamics of this energy perform inference and credit assignment simultaneously through local interactions. We term this framework ''Dyadic Backpropagation''. Crucially, we prove that unit-step Euler discretization, the natural timescale of layer transitions, recovers standard backpropagation exactly in precisely 2L steps for an L-layer network, with no approximations. Unlike prior energy-based methods requiring symmetric weights, asymptotic convergence, or vanishing perturbations, our framework guarantees exact gradients in finite time. This establishes backpropagation as the digitally optimized shadow of a continuous physical relaxation, providing a rigorous foundation for exact gradient computation in analog and neuromorphic substrates where continuous dynamics are native.

</details>


### [834] [MoLF: Mixture-of-Latent-Flow for Pan-Cancer Spatial Gene Expression Prediction from Histology](https://arxiv.org/abs/2602.02282)
*Susu Hu,Stefanie Speidel*

Main category: cs.LG

TL;DR: MoLF是一种用于泛癌组织基因组预测的生成模型，通过条件流匹配和专家混合架构，在泛癌基准测试中达到最先进性能，并具有跨物种零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前的空间转录组学预测方法主要局限于单组织模型，这种碎片化未能利用跨癌症类型的共享生物学原理，并且在数据稀缺场景中应用受限。虽然泛癌训练提供了解决方案，但由此产生的异质性对单一架构提出了挑战。

Method: MoLF采用条件流匹配目标将噪声映射到基因潜在流形，通过专家混合速度场参数化。该架构通过动态路由输入到专门的子网络，有效解耦了不同组织模式的优化。

Result: 实验表明MoLF在泛癌基准测试中建立了新的最先进水平，始终优于专业模型和基础模型基线。此外，MoLF展现出对跨物种数据的零样本泛化能力。

Conclusion: MoLF通过专家混合架构有效处理泛癌异质性，不仅提升了组织基因组预测性能，还捕获了基本的、保守的组织分子机制，具有跨物种泛化潜力。

Abstract: Inferring spatial transcriptomics (ST) from histology enables scalable histogenomic profiling, yet current methods are largely restricted to single-tissue models. This fragmentation fails to leverage biological principles shared across cancer types and hinders application to data-scarce scenarios. While pan-cancer training offers a solution, the resulting heterogeneity challenges monolithic architectures. To bridge this gap, we introduce MoLF (Mixture-of-Latent-Flow), a generative model for pan-cancer histogenomic prediction. MoLF leverages a conditional Flow Matching objective to map noise to the gene latent manifold, parameterized by a Mixture-of-Experts (MoE) velocity field. By dynamically routing inputs to specialized sub-networks, this architecture effectively decouples the optimization of diverse tissue patterns. Our experiments demonstrate that MoLF establishes a new state-of-the-art, consistently outperforming both specialized and foundation model baselines on pan-cancer benchmarks. Furthermore, MoLF exhibits zero-shot generalization to cross-species data, suggesting it captures fundamental, conserved histo-molecular mechanisms.

</details>


### [835] [An Optimization Method for Autoregressive Time Series Forecasting](https://arxiv.org/abs/2602.02288)
*Zheng Li,Jerry Cheng,Huanying Gu*

Main category: cs.LG

TL;DR: 提出一种新的时间序列预测训练方法，通过强制自回归预测误差随预测范围增加而增加，并允许模型拼接短期预测形成灵活长期预测，在多个基准测试中达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前基于Transformer的时间序列预测模型主要通过扩大模型规模而非真正的自回归展开来实现长期预测，且传统训练过程忽略了时间因果关系。

Method: 提出新颖的训练方法，强制两个关键特性：1) 自回归预测误差应随预测范围增加而增加，违反此原则被视为随机猜测并在损失函数中明确惩罚；2) 使模型能够拼接短期自回归预测以形成灵活的长期预测。

Result: 在多个基准测试中建立新的SOTA，相比iTransformer和其他近期强基线实现超过10%的MSE降低，并使短期预测模型能够在超过7.5倍长的范围内进行可靠的长期预测。

Conclusion: 该方法通过强制时间因果关系和灵活的预测拼接，显著提升了时间序列预测模型的性能，特别是在长期预测能力方面。

Abstract: Current time-series forecasting models are primarily based on transformer-style neural networks. These models achieve long-term forecasting mainly by scaling up the model size rather than through genuinely autoregressive (AR) rollout. From the perspective of large language model training, the traditional training process for time-series forecasting models ignores temporal causality. In this paper, we propose a novel training method for time-series forecasting that enforces two key properties: (1) AR prediction errors should increase with the forecasting horizon. Any violation of this principle is considered random guessing and is explicitly penalized in the loss function, and (2) the method enables models to concatenate short-term AR predictions for forming flexible long-term forecasts. Empirical results demonstrate that our method establishes a new state-of-the-art across multiple benchmarks, achieving an MSE reduction of more than 10% compared to iTransformer and other recent strong baselines. Furthermore, it enables short-horizon forecasting models to perform reliable long-term predictions at horizons over 7.5 times longer. Code is available at https://github.com/LizhengMathAi/AROpt

</details>


### [836] [EvalQReason: A Framework for Step-Level Reasoning Evaluation in Large Language Models](https://arxiv.org/abs/2602.02295)
*Shaima Ahmad Freja,Ferhat Ozgur Catak,Betul Yurdem,Chunming Rong*

Main category: cs.LG

TL;DR: EvalQReason：无需人工标注，通过步骤级概率分布分析量化LLM推理质量的框架，包含CSD和SFC两种算法，在数学和医疗数据集上验证了推理动态的领域特异性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在关键应用中需要可靠的推理能力，但现有评估方法主要关注最终答案正确性，缺乏对中间推理过程的系统性评估，难以深入了解推理如何逐步展开。

Method: 提出EvalQReason框架，包含两种互补算法：1) 连续步骤分歧(CSD)测量相邻推理步骤间的局部连贯性；2) 步骤到最终收敛(SFC)评估与最终答案的全局对齐。每种算法使用五个统计指标捕捉推理动态。

Result: 在数学和医疗数据集上的实验显示：CSD特征在正确性分类中表现优异，经典机器学习模型达到F1=0.78和ROC-AUC=0.82，序列神经网络模型性能显著提升(F1=0.88, ROC-AUC=0.97)。CSD始终优于SFC，序列架构优于经典机器学习。推理动态具有领域特异性：数学推理显示清晰的分歧模式，医疗推理则几乎没有区分信号。

Conclusion: EvalQReason实现了可扩展的、过程感知的推理可靠性评估，确立了基于概率的分歧分析作为可信AI部署的原则性方法，揭示了LLM处理不同类型推理的基本差异。

Abstract: Large Language Models (LLMs) are increasingly deployed in critical applications requiring reliable reasoning, yet their internal reasoning processes remain difficult to evaluate systematically. Existing methods focus on final-answer correctness, providing limited insight into how reasoning unfolds across intermediate steps. We present EvalQReason, a framework that quantifies LLM reasoning quality through step-level probability distribution analysis without requiring human annotation. The framework introduces two complementary algorithms: Consecutive Step Divergence (CSD), which measures local coherence between adjacent reasoning steps, and Step-to-Final Convergence (SFC), which assesses global alignment with final answers. Each algorithm employs five statistical metrics to capture reasoning dynamics. Experiments across mathematical and medical datasets with open-source 7B-parameter models demonstrate that CSD-based features achieve strong predictive performance for correctness classification, with classical machine learning models reaching F1=0.78 and ROC-AUC=0.82, and sequential neural models substantially improving performance (F1=0.88, ROC-AUC=0.97). CSD consistently outperforms SFC, and sequential architectures outperform classical machine learning approaches. Critically, reasoning dynamics prove domain-specific: mathematical reasoning exhibits clear divergence-based discrimination patterns between correct and incorrect solutions, while medical reasoning shows minimal discriminative signals, revealing fundamental differences in how LLMs process different reasoning types. EvalQReason enables scalable, process-aware evaluation of reasoning reliability, establishing probability-based divergence analysis as a principled approach for trustworthy AI deployment.

</details>


### [837] [Decoupling Generalizability and Membership Privacy Risks in Neural Networks](https://arxiv.org/abs/2602.02296)
*Xingli Fang,Jung-Eun Kim*

Main category: cs.LG

TL;DR: 该论文提出了一种隐私保护训练原则（PPTP），通过识别深度神经网络中泛化能力和隐私风险存在于不同区域的特点，在最小化泛化能力损失的同时增强隐私保护。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在获取某些能力或特性时通常需要牺牲其他效用，隐私保护与模型效用之间存在权衡关系。不同防御方法之间的损失差异表明存在将泛化能力和隐私风险解耦以最大化隐私增益的潜力。

Method: 提出隐私保护训练原则（PPTP），基于观察到模型的泛化能力和隐私风险存在于深度神经网络架构的不同区域，通过保护模型组件免受隐私风险影响，同时最小化泛化能力的损失。

Result: 通过广泛评估，该方法在增强隐私保护的同时，显著更好地保持了模型的泛化能力。

Conclusion: 该研究成功识别了深度神经网络中泛化能力和隐私风险的空间分离特性，提出的PPTP方法能够有效解耦这两者，在最小化泛化损失的同时实现更好的隐私保护效果。

Abstract: A deep learning model usually has to sacrifice some utilities when it acquires some other abilities or characteristics. Privacy preservation has such trade-off relationships with utilities. The loss disparity between various defense approaches implies the potential to decouple generalizability and privacy risks to maximize privacy gain. In this paper, we identify that the model's generalization and privacy risks exist in different regions in deep neural network architectures. Based on the observations that we investigate, we propose Privacy-Preserving Training Principle (PPTP) to protect model components from privacy risks while minimizing the loss in generalizability. Through extensive evaluations, our approach shows significantly better maintenance in model generalizability while enhancing privacy preservation.

</details>


### [838] [ReasonCACHE: Teaching LLMs To Reason Without Weight Updates](https://arxiv.org/abs/2602.02366)
*Sharut Gupta,Phillip Isola,Stefanie Jegelka,David Lopez-Paz,Kartik Ahuja,Mark Ibrahim,Mohammad Pezeshki*

Main category: cs.LG

TL;DR: ReasonCACHE：通过前缀调优将推理演示蒸馏到固定键值缓存中，无需权重更新即可让大语言模型学习推理，在保持高效的同时超越标准上下文学习并匹配权重学习方法。


<details>
  <summary>Details</summary>
Motivation: 标准上下文学习（ICL）在复杂推理任务中存在局限性：注意力成本呈二次增长、长上下文性能饱和或下降、学习形式浅层。而权重学习（IWL）需要参数更新。需要找到一种无需权重更新但能超越ICL限制的推理学习方法。

Method: 提出ReasonCACHE方法，基于前缀调优机制，将推理演示蒸馏到固定的键值缓存中。这种方法不超载上下文窗口，无需权重更新，直接向注意力机制注入键值对，绕过低秩权重更新的表达性限制。

Result: 在包括GPQA-Diamond在内的挑战性推理基准测试中，ReasonCACHE优于标准ICL，匹配或超越IWL方法。在数据效率、推理成本和可训练参数三个关键维度上都更高效。理论证明ReasonCACHE的表达性严格优于低秩权重更新。

Conclusion: ReasonCACHE是上下文学习和权重学习之间的中间路径，提供了无需修改参数即可在上下文窗口之外学习推理技能的可扩展算法，解决了ICL的扩展性问题。

Abstract: Can Large language models (LLMs) learn to reason without any weight update and only through in-context learning (ICL)? ICL is strikingly sample-efficient, often learning from only a handful of demonstrations, but complex reasoning tasks typically demand many training examples to learn from. However, naively scaling ICL by adding more demonstrations breaks down at this scale: attention costs grow quadratically, performance saturates or degrades with longer contexts, and the approach remains a shallow form of learning. Due to these limitations, practitioners predominantly rely on in-weight learning (IWL) to induce reasoning. In this work, we show that by using Prefix Tuning, LLMs can learn to reason without overloading the context window and without any weight updates. We introduce $\textbf{ReasonCACHE}$, an instantiation of this mechanism that distills demonstrations into a fixed key-value cache. Empirically, across challenging reasoning benchmarks, including GPQA-Diamond, ReasonCACHE outperforms standard ICL and matches or surpasses IWL approaches. Further, it achieves this all while being more efficient across three key axes: data, inference cost, and trainable parameters. We also theoretically prove that ReasonCACHE can be strictly more expressive than low-rank weight update since the latter ties expressivity to input rank, whereas ReasonCACHE bypasses this constraint by directly injecting key-values into the attention mechanism. Together, our findings identify ReasonCACHE as a middle path between in-context and in-weight learning, providing a scalable algorithm for learning reasoning skills beyond the context window without modifying parameters. Our project page: https://reasoncache.github.io/

</details>


### [839] [Self-Supervised Learning from Structural Invariance](https://arxiv.org/abs/2602.02381)
*Yipeng Zhang,Hafez Ghaemi,Jungyoon Lee,Shahab Bakhtiari,Eilif B. Muller,Laurent Charlin*

Main category: cs.LG

TL;DR: 提出AdaSSL方法解决自监督学习中的一对多映射问题，通过引入潜变量建模条件不确定性，适用于对比学习和蒸馏式自监督学习


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习方法难以灵活捕捉数据对之间的条件不确定性，特别是当数据来自自然生成过程（如连续视频帧）时，每个数据点可能对应多个有效目标

Method: 引入潜变量建模条件不确定性，推导配对嵌入间互信息的变分下界，得到可应用于标准自监督学习目标的简单正则化项

Result: AdaSSL方法在因果表示学习、细粒度图像理解和视频世界建模等多个任务中表现出良好的通用性

Conclusion: 通过显式建模一对多映射中的条件不确定性，AdaSSL有效提升了自监督学习的灵活性和性能，适用于多种自监督学习框架

Abstract: Joint-embedding self-supervised learning (SSL), the key paradigm for unsupervised representation learning from visual data, learns from invariances between semantically-related data pairs. We study the one-to-many mapping problem in SSL, where each datum may be mapped to multiple valid targets. This arises when data pairs come from naturally occurring generative processes, e.g., successive video frames. We show that existing methods struggle to flexibly capture this conditional uncertainty. As a remedy, we introduce a latent variable to account for this uncertainty and derive a variational lower bound on the mutual information between paired embeddings. Our derivation yields a simple regularization term for standard SSL objectives. The resulting method, which we call AdaSSL, applies to both contrastive and distillation-based SSL objectives, and we empirically show its versatility in causal representation learning, fine-grained image understanding, and world modeling on videos.

</details>


### [840] [SLIME: Stabilized Likelihood Implicit Margin Enforcement for Preference Optimization](https://arxiv.org/abs/2602.02383)
*Maksim Afanasyev,Illarion Iov*

Main category: cs.LG

TL;DR: SLIME是一种新的无参考对齐方法，通过解耦偏好学习与生成质量，解决了现有直接偏好优化方法中的"遗忘"和"格式崩溃"问题。


<details>
  <summary>Details</summary>
Motivation: 现有直接偏好优化方法虽然计算高效，但存在目标不匹配问题：优化选择与拒绝响应之间的相对边界不能保证保留选择响应的绝对概率，导致"遗忘"（高质量输出概率降低）和"格式崩溃"（拒绝序列过度惩罚）。

Method: SLIME采用三部分目标：(1)锚定项最大化偏好响应的似然；(2)稳定惩罚防止拒绝标记概率崩溃为零；(3)结合硬约束和软约束的双边界机制进行精确边界塑造。

Result: SLIME在性能上优于现有最先进基线方法，同时保持更高的生成稳定性。

Conclusion: SLIME通过解耦偏好学习与生成质量，提供了一种更稳定有效的对齐方法，解决了现有直接偏好优化方法的关键缺陷。

Abstract: Direct preference optimization methods have emerged as a computationally efficient alternative to Reinforcement Learning from Human Feedback (RLHF) for aligning Large Language Models (LLMs). Latest approaches have streamlined the alignment process by deriving implicit reward functions, yet they often suffer from a critical objective mismatch: optimizing the relative margin between chosen and rejected responses does not guarantee the preservation of the chosen response's absolute likelihood. This can lead to ``unlearning'', where the model degrades the probability of high-quality outputs to satisfy margin constraints, and ``formatting collapse'' caused by the over-penalization of rejected sequences. In this work, we introduce SLIME (Stabilized Likelihood Implicit Margin Enforcement), a reference-free alignment objective designed to decouple preference learning from generation quality. SLIME incorporates a three-pronged objective: (1) an anchoring term to maximize the likelihood of preferred responses; (2) a stabilizing penalty that prevents the probabilities of rejected tokens from collapsing to zero; and (3) a dual-margin mechanism that combines hard and soft constraints for precise boundary shaping. Our results demonstrate that SLIME achieves superior performance compared to state-of-the-art baselines while maintaining higher generation stability.

</details>


### [841] [Transformers learn factored representations](https://arxiv.org/abs/2602.02385)
*Adam Shai,Loren Amdahl-Culleton,Casper L. Christensen,Henry R. Bigelow,Fernando E. Rosas,Alexander B. Boyd,Eric A. Alt,Kyle J. Ray,Paul M. Riechers*

Main category: cs.LG

TL;DR: Transformer通过因子分解学习世界表示，在条件独立时使用线性维度分解表示，否则在维度效率与准确性间权衡，实验显示Transformer具有因子分解的归纳偏好


<details>
  <summary>Details</summary>
Motivation: 研究Transformer如何通过下一个token预测学习将世界分解为部分，并探索其表示假设：是使用维度指数增长的乘积空间表示，还是使用维度线性增长的因子分解正交子空间表示

Method: 提出两种表示假设的形式化定义，推导每种假设下激活的几何结构预测，在具有已知潜在结构的合成过程上训练Transformer进行测试

Result: 当因子条件独立时，模型学习因子分解表示；即使在训练早期存在噪声或隐藏依赖破坏条件独立性时，模型仍倾向于因子分解表示，表明存在以保真度为代价的因子分解归纳偏好

Conclusion: Transformer具有将世界分解为部分的归纳偏好，即使在复杂数据上训练，可解释的低维结构可能持续存在，为Transformer的分解行为提供了原则性解释

Abstract: Transformers pretrained via next token prediction learn to factor their world into parts, representing these factors in orthogonal subspaces of the residual stream. We formalize two representational hypotheses: (1) a representation in the product space of all factors, whose dimension grows exponentially with the number of parts, or (2) a factored representation in orthogonal subspaces, whose dimension grows linearly. The factored representation is lossless when factors are conditionally independent, but sacrifices predictive fidelity otherwise, creating a tradeoff between dimensional efficiency and accuracy. We derive precise predictions about the geometric structure of activations for each, including the number of subspaces, their dimensionality, and the arrangement of context embeddings within them. We test between these hypotheses on transformers trained on synthetic processes with known latent structure. Models learn factored representations when factors are conditionally independent, and continue to favor them early in training even when noise or hidden dependencies undermine conditional independence, reflecting an inductive bias toward factoring at the cost of fidelity. This provides a principled explanation for why transformers decompose the world into parts, and suggests that interpretable low dimensional structure may persist even in models trained on complex data.

</details>


### [842] [David vs. Goliath: Verifiable Agent-to-Agent Jailbreaking via Reinforcement Learning](https://arxiv.org/abs/2602.02395)
*Samuel Nellessen,Tal Kachman*

Main category: cs.LG

TL;DR: 论文提出Tag-Along Attacks威胁模型，开发Slingshot框架通过强化学习自动发现攻击向量，在工具增强环境中实现高成功率攻击，并能零样本迁移到多种模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型作为自主代理时，攻击者可能利用其合法工具权限进行攻击，将安全评估从主观NLP任务转变为客观控制问题。需要形式化这种威胁模型并验证其有效性。

Method: 提出Tag-Along Attacks威胁模型，开发Slingshot强化学习框架，从"冷启动"开始自动发现攻击向量。攻击者通过对话诱导安全对齐的操作员使用被禁止的工具。

Result: Slingshot在极端难度任务上达到67.0%攻击成功率（基线仅1.7%），首次成功尝试次数从52.3降至1.3。能零样本迁移到Gemini 2.5 Flash（56.0%）和Meta-SecAlign-8B（39.2%）等模型。

Conclusion: Tag-Along Attacks是真实可验证的威胁模型，通过环境交互就能从现成开源模型引发有效攻击，揭示了工具增强环境中安全评估的新挑战。

Abstract: The evolution of large language models into autonomous agents introduces adversarial failures that exploit legitimate tool privileges, transforming safety evaluation in tool-augmented environments from a subjective NLP task into an objective control problem. We formalize this threat model as Tag-Along Attacks: a scenario where a tool-less adversary "tags along" on the trusted privileges of a safety-aligned Operator to induce prohibited tool use through conversation alone. To validate this threat, we present Slingshot, a 'cold-start' reinforcement learning framework that autonomously discovers emergent attack vectors, revealing a critical insight: in our setting, learned attacks tend to converge to short, instruction-like syntactic patterns rather than multi-turn persuasion. On held-out extreme-difficulty tasks, Slingshot achieves a 67.0% success rate against a Qwen2.5-32B-Instruct-AWQ Operator (vs. 1.7% baseline), reducing the expected attempts to first success (on solved tasks) from 52.3 to 1.3. Crucially, Slingshot transfers zero-shot to several model families, including closed-source models like Gemini 2.5 Flash (56.0% attack success rate) and defensive-fine-tuned open-source models like Meta-SecAlign-8B (39.2% attack success rate). Our work establishes Tag-Along Attacks as a first-class, verifiable threat model and shows that effective agentic attacks can be elicited from off-the-shelf open-weight models through environment interaction alone.

</details>


### [843] [An Empirical Study on Noisy Data and LLM Pretraining Loss Divergence](https://arxiv.org/abs/2602.02400)
*Qizhen Zhang,Ankush Garg,Jakob Foerster,Niladri Chatterji,Kshitiz Malik,Mike Lewis*

Main category: cs.LG

TL;DR: 本文通过系统实验研究噪声数据如何导致大语言模型预训练发散，发现噪声类型、数量和模型规模是影响发散概率的关键因素，并提供了区分噪声引发发散与高学习率引发发散的诊断方法。


<details>
  <summary>Details</summary>
Motivation: 大规模预训练数据集包含大量噪声数据，业界普遍推测这些噪声会导致LLM预训练不稳定甚至损失发散，但这一现象缺乏系统研究。本文旨在通过受控实验验证噪声数据是否以及如何导致预训练发散。

Method: 在干净数据集中注入受控的合成均匀随机噪声，分析从480M到5.2B参数的不同规模模型的训练动态。通过改变噪声类型和数量，系统研究噪声对训练稳定性的影响。

Result: 噪声数据确实会导致训练损失发散，发散概率强烈依赖于噪声类型、噪声量和模型规模。噪声引发的发散表现出与高学习率引发发散不同的激活模式，并提供了区分这两种失败模式的诊断方法。

Conclusion: 本文首次提供了大规模、受控的噪声数据对LLM预训练损失发散影响的系统表征，为理解和缓解预训练中的噪声问题提供了实证基础。

Abstract: Large-scale pretraining datasets drive the success of large language models (LLMs). However, these web-scale corpora inevitably contain large amounts of noisy data due to unregulated web content or randomness inherent in data. Although LLM pretrainers often speculate that such noise contributes to instabilities in large-scale LLM pretraining and, in the worst cases, loss divergence, this phenomenon remains poorly understood.In this work, we present a systematic empirical study of whether noisy data causes LLM pretraining divergences and how it does so. By injecting controlled synthetic uniformly random noise into otherwise clean datasets, we analyze training dynamics across model sizes ranging from 480M to 5.2B parameters. We show that noisy data indeed induces training loss divergence, and that the probability of divergence depends strongly on the noise type, amount of noise, and model scale. We further find that noise-induced divergences exhibit activation patterns distinct from those caused by high learning rates, and we provide diagnostics that differentiate these two failure modes. Together, these results provide a large-scale, controlled characterization of how noisy data affects loss divergence in LLM pretraining.

</details>


### [844] [Didactic to Constructive: Turning Expert Solutions into Learnable Reasoning](https://arxiv.org/abs/2602.02405)
*Ethan Mendes,Jungsoo Park,Alan Ritter*

Main category: cs.LG

TL;DR: DAIL方法通过两步法利用少量专家解决方案提升LLM推理能力：先将专家方案转化为详细推理轨迹，再使用对比学习聚焦专家洞见，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前提升LLM推理能力的方法要么依赖模型自身采样正确解进行强化，要么需要更强模型提供训练信号。但许多难题对前沿模型也难解，无法提取有效训练信号。专家解决方案质量高但模仿困难，因为其教学性质包含隐含推理跳跃，且数据昂贵需要样本高效训练方法。

Method: 提出Distribution Aligned Imitation Learning (DAIL)两步法：1) 将专家解决方案转化为详细、分布内的推理轨迹，弥合分布差距；2) 应用对比目标，聚焦学习专家洞见和方法论。

Result: DAIL仅需少于1000个高质量专家解决方案，就能在Qwen2.5-Instruct和Qwen3模型上实现10-25%的pass@k提升，推理效率提高2-4倍，并具备跨领域泛化能力。

Conclusion: DAIL方法有效解决了专家解决方案分布外问题，通过少量高质量专家数据显著提升LLM推理能力，为样本高效训练提供了有前景的替代方案。

Abstract: Improving the reasoning capabilities of large language models (LLMs) typically relies either on the model's ability to sample a correct solution to be reinforced or on the existence of a stronger model able to solve the problem. However, many difficult problems remain intractable for even current frontier models, preventing the extraction of valid training signals. A promising alternative is to leverage high-quality expert human solutions, yet naive imitation of this data fails because it is fundamentally out of distribution: expert solutions are typically didactic, containing implicit reasoning gaps intended for human readers rather than computational models. Furthermore, high-quality expert solutions are expensive, necessitating generalizable sample-efficient training methods. We propose Distribution Aligned Imitation Learning (DAIL), a two-step method that bridges the distributional gap by first transforming expert solutions into detailed, in-distribution reasoning traces and then applying a contrastive objective to focus learning on expert insights and methodologies. We find that DAIL can leverage fewer than 1000 high-quality expert solutions to achieve 10-25% pass@k gains on Qwen2.5-Instruct and Qwen3 models, improve reasoning efficiency by 2x to 4x, and enable out-of-domain generalization.

</details>


### [845] [Active Transfer Bagging: A New Approach for Accelerated Active Learning Acquisition of Data by Combined Transfer Learning and Bagging Based Models](https://arxiv.org/abs/2602.02415)
*Vivienne Pelletier,Daniel J. Rivera,Obinna Nwokonkwo,Steven A. Wilson,Christopher L. Muhich*

Main category: cs.LG

TL;DR: ATBagging是一种新的主动学习种子集选择方法，通过集成模型的袋外预测分布估计信息量，结合DPP确保多样性，显著提升早期主动学习性能。


<details>
  <summary>Details</summary>
Motivation: 主动学习虽然能降低标注成本，但其早期性能常受随机选择的初始种子集限制。许多应用场景中存在相关或近似数据集，可用于构建更好的种子集。

Method: 提出Active-Transfer Bagging方法：1) 通过袋装集成模型的袋内和袋外预测分布比较估计候选数据点的信息量；2) 使用随机傅里叶特征和质-量分解的DPP确保特征空间多样性；3) 将同一方法应用于主动学习阶段的新数据点选择。

Result: 在四个真实数据集（QM9、ERA5、Forbes 2000、北京PM2.5）上测试，种子集大小为10-100时，ATBagging在几乎所有情况下都优于或持平其他种子选择方法，显著提升早期主动学习性能和学习曲线下面积，在低数据量场景中效果最明显。

Conclusion: ATBagging提供了一种低成本、高回报的方法来启动基于主动学习的数据收集，特别适用于存在相关数据集的场景。

Abstract: Modern machine learning has achieved remarkable success on many problems, but this success often depends on the existence of large, labeled datasets. While active learning can dramatically reduce labeling cost when annotations are expensive, early performance is frequently dominated by the initial seed set, typically chosen at random. In many applications, however, related or approximate datasets are readily available and can be leveraged to construct a better seed set. We introduce a new method for selecting the seed data set for active learning, Active-Transfer Bagging (ATBagging). ATBagging estimates the informativeness of candidate data point from a Bayesian interpretation of bagged ensemble models by comparing in-bag and out-of-bag predictive distributions from the labeled dataset, yielding an information-gain proxy. To avoid redundant selections, we impose feature-space diversity by sampling a determinantal point process (DPP) whose kernel uses Random Fourier Features and a quality-diversity factorization that incorporates the informativeness scores. This same blended method is used for selection of new data points to collect during the active learning phase. We evaluate ATBagging on four real-world datasets covering both target-transfer and feature-shift scenarios (QM9, ERA5, Forbes 2000, and Beijing PM2.5). Across seed sizes nseed = 10-100, ATBagging improves or ties early active learning and increases area under the learning-curve relative to alternative seed subset selection methodologies in almost all cases, with strongest benefits in low-data regimes. Thus, ATBagging provides a low-cost, high reward means to initiating active learning-based data collection.

</details>


### [846] [Poly-attention: a general scheme for higher-order self-attention](https://arxiv.org/abs/2602.02422)
*Sayak Chakrabarti,Toniann Pitassi,Josh Alman*

Main category: cs.LG

TL;DR: 本文提出poly-attention机制，扩展自注意力以处理高阶张量计算和任意token关系结构，系统研究其计算复杂度和表达能力，提出可在二次时间内计算并能执行任意固定数量函数组合的新注意力机制。


<details>
  <summary>Details</summary>
Motivation: 传统自注意力机制无法有效处理涉及三个相关token的检测任务或需要引用多个输入token的组合任务。现有高阶注意力替代方案虽然能处理这些多元素任务，但计算复杂度高（超二次时间）。需要一种既能处理复杂关系又能保持合理计算效率的注意力机制。

Method: 定义poly-attention机制类，可纳入任意高阶张量计算和任意token关系结构。系统研究其计算复杂度和表达能力，包括给出新算法和匹配的复杂度下界，精确确定每个机制能执行的多元素任务。提出可在二次时间内精确计算并能执行任意固定数量函数组合的新注意力机制。

Result: 建立了机制表达能力与模型系数大小之间的紧密权衡关系，使得机制能在几乎线性时间内近似。提出的新注意力机制可在二次时间内精确计算，并能执行任意固定数量函数的组合，而先前机制即使只组合两个函数也需要超二次时间。

Conclusion: poly-attention机制为自注意力提供了强大的泛化框架，在表达能力和计算效率之间取得了重要平衡。新提出的机制在二次时间内实现了先前需要超二次时间的函数组合能力，并通过复杂度下界证明了更快的算法是不可能的。

Abstract: The self-attention mechanism, at the heart of the Transformer model, is able to effectively model pairwise interactions between tokens. However, numerous recent works have shown that it is unable to perform basic tasks involving detecting triples of correlated tokens, or compositional tasks where multiple input tokens need to be referenced to generate a result. Some higher-dimensional alternatives to self-attention have been proposed to address this, including higher-order attention and Strassen attention, which can perform some of these polyadic tasks in exchange for slower, superquadratic running times.
  In this work, we define a vast class of generalizations of self-attention, which we call poly-attention mechanisms. Our mechanisms can incorporate arbitrary higher-order (tensor) computations as well as arbitrary relationship structures between the input tokens, and they include the aforementioned alternatives as special cases. We then systematically study their computational complexity and representational strength, including giving new algorithms and matching complexity-theoretic lower bounds on the time complexity of computing the attention matrix exactly as well as approximately, and tightly determining which polyadic tasks they can each perform. Our results give interesting trade-offs between different desiderata for these mechanisms, including a tight relationship between how expressive a mechanism is, and how large the coefficients in the model may be so that the mechanism can be approximated in almost-linear time.
  Notably, we give a new attention mechanism which can be computed exactly in quadratic time, and which can perform function composition for any fixed number of functions. Prior mechanisms, even for just composing two functions, could only be computed in superquadratic time, and our new lower bounds show that faster algorithms for them are not possible.

</details>


### [847] [Trust Region Continual Learning as an Implicit Meta-Learner](https://arxiv.org/abs/2602.02417)
*Zekun Wang,Anant Gupta,Christopher J. MacLellan*

Main category: cs.LG

TL;DR: 提出信任区域持续学习方法，结合生成回放和Fisher度量信任区域约束，在持续学习中实现类似元学习的快速重收敛特性


<details>
  <summary>Details</summary>
Motivation: 传统持续学习方法存在核心权衡：基于正则化的方法（如EWC）在任务最优解重叠度低时可能过度约束更新，而基于回放的方法虽然能保持性能但会因不完美的回放而漂移。需要一种混合方法来解决这些问题。

Method: 提出信任区域持续学习方法，结合生成回放和Fisher度量信任区域约束。在局部近似下，该方法更新具有类似MAML的解释：回放提供旧任务梯度信号（类似查询），Fisher加权惩罚提供高效的离线曲率塑造（类似支持）。

Result: 在任务增量扩散图像生成和持续扩散策略控制任务上，信任区域持续学习方法取得了最佳最终性能和保留率，并且比EWC、回放和持续元学习基线方法更快地恢复早期任务性能。

Conclusion: 该方法在持续学习中实现了涌现的元学习特性：模型成为一个初始化点，能在每次任务转换后快速重新收敛到先前任务的最优解，而无需显式优化双层目标。这为解决持续学习中的灾难性遗忘问题提供了新的混合视角。

Abstract: Continual learning aims to acquire tasks sequentially without catastrophic forgetting, yet standard strategies face a core tradeoff: regularization-based methods (e.g., EWC) can overconstrain updates when task optima are weakly overlapping, while replay-based methods can retain performance but drift due to imperfect replay. We study a hybrid perspective: \emph{trust region continual learning} that combines generative replay with a Fisher-metric trust region constraint. We show that, under local approximations, the resulting update admits a MAML-style interpretation with a single implicit inner step: replay supplies an old-task gradient signal (query-like), while the Fisher-weighted penalty provides an efficient offline curvature shaping (support-like). This yields an emergent meta-learning property in continual learning: the model becomes an initialization that rapidly \emph{re-converges} to prior task optima after each task transition, without explicitly optimizing a bilevel objective. Empirically, on task-incremental diffusion image generation and continual diffusion-policy control, trust region continual learning achieves the best final performance and retention, and consistently recovers early-task performance faster than EWC, replay, and continual meta-learning baselines.

</details>


### [848] [Active Causal Experimentalist (ACE): Learning Intervention Strategies via Direct Preference Optimization](https://arxiv.org/abs/2602.02451)
*Patrick Cooper,Alvaro Velasquez*

Main category: cs.LG

TL;DR: ACE使用偏好学习框架学习序列化实验设计策略，通过干预比较而非奖励幅度来稳定学习，在多个领域显著优于传统方法


<details>
  <summary>Details</summary>
Motivation: 传统因果发现方法（随机采样、贪婪信息最大化、轮询覆盖）将每个干预决策视为独立问题，无法从经验中学习自适应策略，限制了实验效率

Method: 提出Active Causal Experimentalist (ACE)，将实验设计建模为序列策略学习问题。核心洞察是：虽然绝对信息增益随知识积累而减少，但候选干预间的相对比较始终有意义。采用Direct Preference Optimization，通过成对干预比较学习而非不稳定的奖励幅度

Result: 在合成基准、物理模拟和经济数据上，ACE在相同干预预算下比基线方法提升70-71%（p < 0.001，Cohen's d ~ 2）。学习到的策略自主发现了碰撞机制需要集中干预父变量的理论正确策略

Conclusion: 基于偏好的学习能够恢复原则性实验策略，通过经验学习补充理论，实现领域自适应。这表明偏好学习框架能有效解决因果发现中的序列决策问题

Abstract: Discovering causal relationships requires controlled experiments, but experimentalists face a sequential decision problem: each intervention reveals information that should inform what to try next. Traditional approaches such as random sampling, greedy information maximization, and round-robin coverage treat each decision in isolation, unable to learn adaptive strategies from experience. We propose Active Causal Experimentalist (ACE), which learns experimental design as a sequential policy. Our key insight is that while absolute information gains diminish as knowledge accumulates (making value-based RL unstable), relative comparisons between candidate interventions remain meaningful throughout. ACE exploits this via Direct Preference Optimization, learning from pairwise intervention comparisons rather than non-stationary reward magnitudes. Across synthetic benchmarks, physics simulations, and economic data, ACE achieves 70-71% improvement over baselines at equal intervention budgets (p < 0.001, Cohen's d ~ 2). Notably, the learned policy autonomously discovers that collider mechanisms require concentrated interventions on parent variables, a theoretically-grounded strategy that emerges purely from experience. This suggests preference-based learning can recover principled experimental strategies, complementing theory with learned domain adaptation.

</details>


### [849] [Repurposing Protein Language Models for Latent Flow-Based Fitness Optimization](https://arxiv.org/abs/2602.02425)
*Amaru Caceres Arroyo,Lea Bogensperger,Ahmed Allam,Michael Krauthammer,Konrad Schindler,Dominik Narnhofer*

Main category: cs.LG

TL;DR: CHASE：利用预训练蛋白质语言模型的进化知识，通过压缩嵌入到紧凑潜在空间，使用条件流匹配模型和分类器自由引导，直接生成高适应性蛋白质变体，无需预测器引导。


<details>
  <summary>Details</summary>
Motivation: 蛋白质适应性优化面临巨大组合空间挑战，高适应性变体极其稀疏。现有方法要么性能不足，要么需要计算昂贵的基于梯度的采样。

Method: 将预训练蛋白质语言模型的嵌入压缩到紧凑潜在空间，训练条件流匹配模型并使用分类器自由引导，在ODE采样步骤中无需预测器引导直接生成高适应性变体。

Result: 在AAV和GFP蛋白质设计基准测试中达到最先进性能，在数据受限情况下通过合成数据引导可以进一步提升性能。

Conclusion: CHASE框架通过重新利用预训练蛋白质语言模型的进化知识，实现了高效、无需预测器引导的高适应性蛋白质变体生成，为蛋白质设计提供了新方法。

Abstract: Protein fitness optimization is challenged by a vast combinatorial landscape where high-fitness variants are extremely sparse. Many current methods either underperform or require computationally expensive gradient-based sampling. We present CHASE, a framework that repurposes the evolutionary knowledge of pretrained protein language models by compressing their embeddings into a compact latent space. By training a conditional flow-matching model with classifier-free guidance, we enable the direct generation of high-fitness variants without predictor-based guidance during the ODE sampling steps. CHASE achieves state-of-the-art performance on AAV and GFP protein design benchmarks. Finally, we show that bootstrapping with synthetic data can further enhance performance in data-constrained settings.

</details>


### [850] [Embedding Perturbation may Better Reflect the Uncertainty in LLM Reasoning](https://arxiv.org/abs/2602.02427)
*Qihao Wen,Jiahao Wang,Yang Nan,Pengfei He,Ravi Tandon,Han Xu*

Main category: cs.LG

TL;DR: 本文提出一种基于扰动敏感性的不确定性量化方法，用于检测LLM推理过程中的中间步骤错误，相比传统方法更有效且高效。


<details>
  <summary>Details</summary>
Motivation: LLM在推理任务中可能产生不可靠输出，需要不确定性量化来识别问题。现有方法主要关注最终答案的不确定性，但中间推理步骤的不确定性同样重要，可以支持更精细的干预。

Method: 提出基于扰动敏感性的不确定性量化指标：通过扰动前一个token的嵌入，测量当前token的敏感性。错误推理步骤中的token对前序token嵌入扰动高度敏感，可用此敏感性分数识别不确定的中间步骤。

Result: 实验表明，基于扰动的指标在不确定性量化性能上优于基线方法（如token生成概率和token熵）。该方法相比需要多次采样的方法更简单高效。

Conclusion: 扰动敏感性是衡量LLM中间推理步骤不确定性的有效指标，能够识别错误推理步骤，为更精细的LLM干预提供支持。

Abstract: Large language Models (LLMs) have achieved significant breakthroughs across diverse domains; however, they can still produce unreliable or misleading outputs. For responsible LLM application, Uncertainty Quantification (UQ) techniques are used to estimate a model's uncertainty about its outputs, indicating the likelihood that those outputs may be problematic. For LLM reasoning tasks, it is essential to estimate the uncertainty not only for the final answer, but also for the intermediate steps of the reasoning, as this can enable more fine-grained and targeted interventions. In this study, we explore what UQ metrics better reflect the LLM's ``intermediate uncertainty''during reasoning. Our study reveals that an LLMs' incorrect reasoning steps tend to contain tokens which are highly sensitive to the perturbations on the preceding token embeddings. In this way, incorrect (uncertain) intermediate steps can be readily identified using this sensitivity score as guidance in practice. In our experiments, we show such perturbation-based metric achieves stronger uncertainty quantification performance compared with baseline methods such as token (generation) probability and token entropy. Besides, different from approaches that rely on multiple sampling, the perturbation-based metrics offer better simplicity and efficiency.

</details>


### [851] [Certain Head, Uncertain Tail: Expert-Sample for Test-Time Scaling in Fine-Grained MoE](https://arxiv.org/abs/2602.02443)
*Yuanteng Chen,Peisong Wang,Nanxin Zeng,Yuantian Shao,Gang Li,Jing Liu,Jian Cheng*

Main category: cs.LG

TL;DR: Expert-Sample方法通过在细粒度MoE模型中保留高置信度专家选择，同时在不确定尾部注入受控随机性，实现多样化的生成而不破坏输出稳定性，显著提升pass@n和验证准确率。


<details>
  <summary>Details</summary>
Motivation: 测试时缩放通过生成多个候选解提升LLM性能，但token级采样需要温度调优来权衡多样性与稳定性。细粒度MoE（每层数百个训练良好的专家，每个token激活多个专家）提供了通过丰富路由空间探索的替代方案。研究发现路由器分数呈现特定模式：高置信度专家头部后跟不确定的低置信度候选尾部。

Method: 提出Expert-Sample方法：保留高置信度选择，同时在不确定尾部注入受控随机性。这是一种无需训练的方法，通过利用细粒度MoE路由模式，在保持核心推理能力的同时增加推理多样性。

Result: 在多个细粒度MoE模型上评估数学、知识推理和代码任务，Expert-Sample一致提升pass@n和验证准确率。在Qwen3-30B-A3B-Instruct模型上，GPQA-Diamond任务的pass@32从85.4%提升到91.9%，Best-of-N验证准确率从59.1%提升到62.6%。

Conclusion: 细粒度MoE路由模式揭示了高置信度专家头部控制核心推理能力，不确定尾部与推理多样性相关。Expert-Sample方法有效利用这一模式，在保持输出稳定性的同时实现多样化生成，为测试时缩放提供了新途径。

Abstract: Test-time scaling improves LLM performance by generating multiple candidate solutions, yet token-level sampling requires temperature tuning that trades off diversity against stability. Fine-grained MoE, featuring hundreds of well-trained experts per layer and multi-expert activation per token, offers an unexplored alternative through its rich routing space. We empirically characterize fine-grained MoE routing and uncover an informative pattern: router scores exhibit a certain head of high-confidence experts followed by an uncertain tail of low-confidence candidates. While single-run greedy accuracy remains stable when fewer experts are activated, multi-sample pass@n degrades significantly-suggesting that the certain head governs core reasoning capability while the uncertain tail correlates with reasoning diversity. Motivated by these findings, we propose Expert-Sample, a training-free method that preserves high-confidence selections while injecting controlled stochasticity into the uncertain tail, enabling diverse generation without destabilizing outputs. Evaluated on multiple fine-grained MoE models across math, knowledge reasoning, and code tasks, Expert-Sample consistently improves pass@n and verification-based accuracy. On Qwen3-30B-A3B-Instruct evaluated on GPQA-Diamond with 32 parallel samples, pass@32 rises from 85.4% to 91.9%, and accuracy improves from 59.1% to 62.6% with Best-of-N verification.

</details>


### [852] [Finite-Sample Wasserstein Error Bounds and Concentration Inequalities for Nonlinear Stochastic Approximation](https://arxiv.org/abs/2602.02445)
*Seo Taek Kong,R. Srikant*

Main category: cs.LG

TL;DR: 该论文为非渐近随机逼近算法在Wasserstein-p距离下推导了有限样本误差界，分析了最后迭代和Polyak-Ruppert平均的收敛速率，并应用于线性随机逼近和随机梯度下降。


<details>
  <summary>Details</summary>
Motivation: 现有随机逼近算法的分析多为渐近结果或需要特定噪声假设，缺乏在一般噪声条件下（如鞅差和马尔可夫链）的非渐近误差界。需要为最后迭代和Polyak-Ruppert平均提供显式的有限样本保证。

Method: 采用耦合论证方法，将离散时间过程与极限Ornstein-Uhlenbeck过程进行比较。对于Polyak-Ruppert平均，在相同一般设置下进行直接分析。假设驱动噪声满足非渐近中心极限定理。

Result: 在Wasserstein-p距离下，归一化最后迭代以γ_n^{1/6}速率收敛到高斯分布（γ_n为步长）；Polyak-Ruppert平均以n^{-1/6}速率收敛。这些分布保证导出了改进的高概率集中不等式。

Conclusion: 该研究为非线性随机逼近算法提供了非渐近误差界，填补了有限样本分析与渐近理论之间的空白。方法适用于一般噪声条件，并在线性随机逼近和随机梯度下降中展示了应用价值。

Abstract: This paper derives non-asymptotic error bounds for nonlinear stochastic approximation algorithms in the Wasserstein-$p$ distance. To obtain explicit finite-sample guarantees for the last iterate, we develop a coupling argument that compares the discrete-time process to a limiting Ornstein-Uhlenbeck process. Our analysis applies to algorithms driven by general noise conditions, including martingale differences and functions of ergodic Markov chains. Complementing this result, we handle the convergence rate of the Polyak-Ruppert average through a direct analysis that applies under the same general setting.
  Assuming the driving noise satisfies a non-asymptotic central limit theorem, we show that the normalized last iterates converge to a Gaussian distribution in the $p$-Wasserstein distance at a rate of order $γ_n^{1/6}$, where $γ_n$ is the step size. Similarly, the Polyak-Ruppert average is shown to converge in the Wasserstein distance at a rate of order $n^{-1/6}$. These distributional guarantees imply high-probability concentration inequalities that improve upon those derived from moment bounds and Markov's inequality. We demonstrate the utility of this approach by considering two applications: (1) linear stochastic approximation, where we explicitly quantify the transition from heavy-tailed to Gaussian behavior of the iterates, thereby bridging the gap between recent finite-sample analyses and asymptotic theory and (2) stochastic gradient descent, where we establish rate of convergence to the central limit theorem.

</details>


### [853] [Conflict-Aware Client Selection for Multi-Server Federated Learning](https://arxiv.org/abs/2602.02458)
*Mingwei Hong,Zheng Lin,Zehang Lin,Lin Li,Miao Yang,Xia Du,Zihan Fang,Zhaolu Kang,Dianxin Luan,Shunzhi Zhu*

Main category: cs.LG

TL;DR: 提出RL-CRP框架，使用强化学习和冲突风险预测优化多服务器联邦学习中的客户端选择，减少服务器间冲突并提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 传统单服务器联邦学习存在高通信延迟问题，而多服务器联邦学习虽然能分担负载，但由于客户端覆盖重叠和无协调选择，常导致资源竞争、带宽冲突和训练失败。

Method: 提出RL-CRP框架：1) 每个服务器使用分类隐马尔可夫模型基于稀疏历史客户端选择序列预测冲突风险；2) 结合公平感知奖励机制促进长期客户端参与；3) 通过强化学习优化客户端选择以最小化训练延迟和资源竞争。

Result: 实验表明RL-CRP框架能有效减少服务器间冲突，显著提升训练效率，包括收敛速度和通信成本方面的改进。

Conclusion: RL-CRP框架通过冲突风险预测和公平感知奖励机制，成功解决了多服务器联邦学习中的客户端选择问题，提高了系统性能和训练效率。

Abstract: Federated learning (FL) has emerged as a promising distributed machine learning (ML) that enables collaborative model training across clients without exposing raw data, thereby preserving user privacy and reducing communication costs. Despite these benefits, traditional single-server FL suffers from high communication latency due to the aggregation of models from a large number of clients. While multi-server FL distributes workloads across edge servers, overlapping client coverage and uncoordinated selection often lead to resource contention, causing bandwidth conflicts and training failures. To address these limitations, we propose a decentralized reinforcement learning with conflict risk prediction, named RL CRP, to optimize client selection in multi-server FL systems. Specifically, each server estimates the likelihood of client selection conflicts using a categorical hidden Markov model based on its sparse historical client selection sequence. Then, a fairness-aware reward mechanism is incorporated to promote long-term client participation for minimizing training latency and resource contention. Extensive experiments demonstrate that the proposed RL-CRP framework effectively reduces inter-server conflicts and significantly improves training efficiency in terms of convergence speed and communication cost.

</details>


### [854] [Expanding the Capabilities of Reinforcement Learning via Text Feedback](https://arxiv.org/abs/2602.02482)
*Yuda Song,Lili Chen,Fahim Tajwar,Remi Munos,Deepak Pathak,J. Andrew Bagnell,Aarti Singh,Andrea Zanette*

Main category: cs.LG

TL;DR: 论文提出RLTF框架，利用文本反馈作为介于稀疏奖励和完整演示之间的中间监督信号，通过两种方法（自我蒸馏和反馈建模）让模型内化反馈信息，在推理时提升单轮性能。


<details>
  <summary>Details</summary>
Motivation: 当前RL训练LLM主要依赖稀疏的二元奖励或偏好标签，信息量有限；而蒸馏需要昂贵的完整演示。文本反馈作为中间信号，比标量奖励更丰富，比演示更便宜，且在实际应用中已广泛存在。

Method: 提出RLTF框架：1) RLTF-SD：训练单轮策略匹配自身反馈条件下的第二轮生成；2) RLTF-FM：将预测反馈作为辅助目标。两种方法都让模型在训练时利用文本反馈，在推理时提升单轮性能。

Result: 在推理谜题、竞赛数学和创意写作任务上的实验表明，两种方法均显著优于基线方法，验证了利用丰富文本反馈进行RL训练的有效性。

Conclusion: 文本反馈作为介于稀疏奖励和完整演示之间的监督信号，具有实际应用价值。RLTF框架通过让模型内化反馈信息，能够有效提升LLM在推理时的单轮性能，为大规模利用文本反馈进行RL训练提供了可行方案。

Abstract: The success of RL for LLM post-training stems from an unreasonably uninformative source: a single bit of information per rollout as binary reward or preference label. At the other extreme, distillation offers dense supervision but requires demonstrations, which are costly and difficult to scale. We study text feedback as an intermediate signal: richer than scalar rewards, yet cheaper than complete demonstrations. Textual feedback is a natural mode of human interaction and is already abundant in many real-world settings, where users, annotators, and automated judges routinely critique LLM outputs. Towards leveraging text feedback at scale, we formalize a multi-turn RL setup, RL from Text Feedback (RLTF), where text feedback is available during training but not at inference. Therefore, models must learn to internalize the feedback in order to improve their test-time single-turn performance. To do this, we propose two methods: Self Distillation (RLTF-SD), which trains the single-turn policy to match its own feedback-conditioned second-turn generations; and Feedback Modeling (RLTF-FM), which predicts the feedback as an auxiliary objective. We provide theoretical analysis on both methods, and empirically evaluate on reasoning puzzles, competition math, and creative writing tasks. Our results show that both methods consistently outperform strong baselines across benchmarks, highlighting the potential of RL with an additional source of rich supervision at scale.

</details>


### [855] [MEG-XL: Data-Efficient Brain-to-Text via Long-Context Pre-Training](https://arxiv.org/abs/2602.02494)
*Dulhan Jayalath,Oiwi Parker Jones*

Main category: cs.LG

TL;DR: MEG-XL通过2.5分钟长上下文预训练，显著提升脑电数据到文本解码的数据效率，仅需1小时数据即可达到50小时监督学习的性能。


<details>
  <summary>Details</summary>
Motivation: 临床脑-文本接口面向瘫痪患者，他们无法提供大量训练数据。现有预训练方法通常只使用几秒上下文，无法捕捉自然语言处理所需的长时间神经上下文信息。

Method: 提出MEG-XL模型，使用每样本2.5分钟的MEG上下文进行预训练（比现有方法长5-300倍，相当于191k tokens），然后微调用于脑电数据的单词解码任务。

Result: MEG-XL仅需少量数据（如1小时vs 50小时）即可达到监督学习性能，并优于现有脑基础模型。长上下文预训练学到的表示能更好地迁移到单词解码任务。

Conclusion: 长上下文预训练能有效利用其他方法丢弃的扩展神经上下文信息，为临床脑-文本接口提供了数据高效的解决方案。

Abstract: Clinical brain-to-text interfaces are designed for paralysed patients who cannot provide extensive training recordings. Pre-training improves data-efficient generalisation by learning statistical priors across subjects, but these priors critically depend on context. While natural speech might unfold gradually over minutes, most methods pre-train with only a few seconds of context. Thus, we propose MEG-XL, a model pre-trained with 2.5 minutes of MEG context per sample, 5-300x longer than prior work, and equivalent to 191k tokens, capturing extended neural context. Fine-tuning on the task of word decoding from brain data, MEG-XL matches supervised performance with a fraction of the data (e.g. 1hr vs 50hrs) and outperforms brain foundation models. We find that models pre-trained with longer contexts learn representations that transfer better to word decoding. Our results indicate that long-context pre-training helps exploit extended neural context that other methods unnecessarily discard. Code, model weights, and instructions are available at https://github.com/neural-processing-lab/MEG-XL .

</details>


<div id='q-fin.GN'></div>

# q-fin.GN [[Back]](#toc)

### [856] [Regulatory Migration to Europe: ICO Reallocation Following U.S. Securities Enforcement](https://arxiv.org/abs/2602.00138)
*Krishna Sharma,Khemraj Bhatt,Indra Giri*

Main category: q-fin.GN

TL;DR: 美国SEC 2017年DAO报告澄清证券法对ICO的适用性后，导致ICO活动从美国向欧洲大规模跨境转移


<details>
  <summary>Details</summary>
Motivation: 研究美国监管政策澄清是否会在高度流动的数字资产市场中产生跨境溢出效应，特别是SEC的DAO报告如何影响全球ICO发行活动的地理分布

Method: 使用2014-2021年全球ICO综合数据集，构建区域-月度面板数据，采用区域和月份固定效应的面板回归分析，评估DAO报告公告前后的发行动态变化

Result: DAO报告后，ICO活动显著且持续地向欧洲重新分配。欧洲在2017年后平均每月比其他区域多发行约14个ICO，这一效应在控制全球市场周期后仍然显著

Conclusion: 研究结果支持在高度流动的数字资产市场中存在跨境监管溢出效应，监管政策变化会导致资本和创业活动在不同司法管辖区间重新分配

Abstract: This paper examines whether a major U.S. regulatory clarification coincided with cross-border spillovers in crypto-asset entrepreneurial finance. We study the Securities and Exchange Commission's July 2017 DAO Report, which clarified the application of U.S. securities law to many initial coin offerings, and analyze how global issuance activity adjusted across regions. Using a comprehensive global dataset of ICOs from 2014 to 2021, we construct a region-month panel and evaluate issuance dynamics around the announcement. We document a substantial and persistent reallocation of ICO activity toward Europe following the DAO Report. In panel regressions with region and month fixed effects, Europe experiences an average post-2017 increase of approximately 14 additional ICOs per region-month relative to other regions, net of global market cycles. The results are consistent with cross-border regulatory spillovers in highly mobile digital-asset markets.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [857] [Dual Quaternion SE(3) Synchronization with Recovery Guarantees](https://arxiv.org/abs/2602.00324)
*Jianing Zhao,Linglingzhi Zhu,Anthony Man-Cho So*

Main category: math.OC

TL;DR: 提出基于对偶四元数的SE(3)同步方法，包含谱初始化和对偶四元数广义幂方法，具有理论保证和更好的精度效率


<details>
  <summary>Details</summary>
Motivation: 现有SE(3)同步方法通常需要多步启发式过程，缺乏理论保证，难以分析。需要一种具有理论保证的直接方法。

Method: 采用对偶四元数表示，提出两阶段算法：1) 谱初始化器（通过幂方法计算厄米对偶四元数测量矩阵），2) 对偶四元数广义幂方法（通过每迭代投影保持可行性）

Result: 建立了谱估计器的误差界，证明DQGPM具有有限迭代误差界，在明确噪声阈值内实现线性误差收缩。在合成基准和真实多扫描点集配准中优于基于矩阵的方法。

Conclusion: 提出的对偶四元数方法为SE(3)同步提供了具有理论保证的高效解决方案，在精度和效率上优于现有方法。

Abstract: Synchronization over the special Euclidean group SE(3) aims to recover absolute poses from noisy pairwise relative transformations and is a core primitive in robotics and 3D vision. Standard approaches often require multi-step heuristic procedures to recover valid poses, which are difficult to analyze and typically lack theoretical guarantees. This paper adopts a dual quaternion representation and formulates SE(3) synchronization directly over the unit dual quaternion. A two-stage algorithm is developed: A spectral initializer computed via the power method on a Hermitian dual quaternion measurement matrix, followed by a dual quaternion generalized power method (DQGPM) that enforces feasibility through per-iteration projection. The estimation error bounds are established for spectral estimators, and DQGPM is shown to admit a finite-iteration error bound and achieves linear error contraction up to an explicit noise-dependent threshold. Experiments on synthetic benchmarks and real-world multi-scan point-set registration demonstrate that the proposed pipeline improves both accuracy and efficiency over representative matrix-based methods.

</details>


### [858] [Efficiency-Reward Trade-Off in Queues with Dynamic Arrivals](https://arxiv.org/abs/2602.00351)
*Tianze Qu,Sushil Mahavir Varma*

Main category: math.OC

TL;DR: 研究状态依赖到达控制的单服务器队列，分析效率-奖励权衡，发现小市场和大市场存在尖锐二分：小市场效率差（队列长度∝1/ε），大市场状态依赖策略可显著改善（队列长度∝1/√ε或log(1/ε)）


<details>
  <summary>Details</summary>
Motivation: 受网约车平台和支付通道网络等在线市场应用驱动，研究动态到达率控制问题，旨在设计同时实现高长期运营奖励和低拥塞的控制策略

Method: 采用基于遗憾的框架，相对于最优基准，在ε-最优奖励约束下分析效率-奖励权衡，建立通用下界并构造简单的状态依赖策略

Result: 发现小市场和大市场的尖锐二分：小市场（包括状态无关策略）效率差，期望队列长度∝1/ε；大市场状态依赖策略表现更好，当奖励函数曲率足够时队列长度∝1/√ε，否则∝log(1/ε)

Conclusion: 结果为动态到达队列提供了非渐近重流量特征，为设计高效的定价和准入控制策略提供了结构洞见，揭示了市场规模对控制策略性能的关键影响

Abstract: Motivated by applications in online marketplaces such as ride-hailing platforms and payment channel networks, we study a single-server queue with state-dependent arrival control. The service operator dynamically chooses the arrival rate as a function of the current queue length and receives a reward determined by the induced rate, capturing objectives such as throughput, revenue, or social welfare. The goal is to design control policies that simultaneously achieve high long-run operating reward and low congestion, measured by the expected steady-state queue length.
  We adopt a regret-based framework relative to an optimal benchmark and characterize the efficiency--reward trade-off under an $\varepsilon$-optimal reward constraint. Our results reveal a sharp dichotomy between small-market and large-market regimes. In small markets, including state-independent policies, any admissible control incurs poor efficiency, with the expected queue length growing on the order of $1/\varepsilon$. In contrast, in large markets, state-dependent policies can achieve substantially better performance. When the reward function exhibits sufficient curvature, the optimal queue length scales as $Θ(1/\sqrt{\varepsilon})$; otherwise, it scales as $Θ(\log(1/\varepsilon))$.
  For each regime, we establish universal lower bounds on the achievable efficiency and construct simple state-dependent policies that attain these bounds. Our results provide a non-asymptotic heavy-traffic characterization for queues with dynamic arrivals and offer structural insights into the design of efficient pricing and admission control policies.

</details>


### [859] [Learning Safety-Guaranteed, Non-Greedy Control Barrier Functions Using Reinforcement Learning](https://arxiv.org/abs/2602.00366)
*Minduli Wijayatunga,Nathan Wallace,Salah Sukkarieh,Roberto Armellin*

Main category: math.OC

TL;DR: 提出一个两阶段强化学习框架，通过自适应控制屏障函数参数和可恢复性认证，提升航天器交会对接的安全性和燃料效率。


<details>
  <summary>Details</summary>
Motivation: 航天器交会对接操作存在安全风险，传统输入约束控制屏障函数(ICCBFs)虽然能保证安全，但燃料效率低且保守地丢弃可恢复状态，需要平衡安全性和任务效率。

Method: 两阶段强化学习框架：第一阶段学习状态依赖的类K∞参数，自适应调整ICCBF/CLF衰减率，保持长期成本意识；第二阶段学习残差屏障函数h_RL(x)，认证S\C*子集的可恢复性。运行时控制器选择适当屏障公式并求解轻量级ZOH QP。

Result: 在巡航控制、航天器交会对接和巡检任务三个基准测试中，相比ICCBF基线方法，燃料消耗中位数减少12-25%，保持在安全集S内的轨迹比例提高7-8%，同时保持实时QP计算复杂度。

Conclusion: 提出的统一两阶段RL框架有效解决了ICCBFs的燃料效率低和保守性问题，在保证安全的同时显著提升任务性能，适用于安全关键、燃料受限的航天器控制。

Abstract: Spacecraft rendezvous and proximity operations (RPO) pose safety risks to high-value assets, so formal safety guarantees are critical. Yet conservative safety controllers can reduce mission efficiency. We propose a unified two-stage reinforcement learning (RL) framework that addresses two complementary limitations of Input-Constrained Control Barrier Functions (ICCBFs) for safety-critical, fuel-limited spacecraft control. Given a certified safe set S, ICCBFs guarantee forward invariance of an inner set C* under input bounds, but the resulting per-step quadratic programme (QP) is greedy and fuel-inefficient within C*, and recoverable states outside C* are conservatively discarded. Stage 1 learns state-dependent class-K-infinity parameters that adapt ICCBF/CLF decay rates, embedding long-horizon cost awareness while preserving invariance in C*. Stage 2 learns a residual barrier h_RL(x) that certifies recoverability for a subset of S minus C*. At run time, the controller selects the appropriate barrier formulation (Stage 1 or Stage 2) and solves a lightweight ZOH QP. Both stages are trained with PPO using rewards that penalise constraint violations, control effort, and task metrics. We evaluate three benchmarks: cruise control, spacecraft rendezvous with a rotating target, and inspection that maximises observability subject to keep-in and keep-out zone constraints. Across test cases, the method reduces median fuel relative to ICCBF baselines by 12 to 25 percent and increases the fraction of trajectories that remain in S by 7 to 8 percent, while retaining real-time QP complexity.

</details>


### [860] [A Hybrid Relaxation-Heuristic Framework for Solving MIP with Binary Variables](https://arxiv.org/abs/2602.00429)
*Zayn Wang*

Main category: math.OC

TL;DR: 提出一个解决含二元变量的混合整数二次规划问题的通用混合框架，包含混合松弛和启发式优化两阶段，在投资组合优化问题上取得先进性能。


<details>
  <summary>Details</summary>
Motivation: 混合整数规划（特别是MILP和MIQP）在投资组合优化和网络流控制等领域应用广泛，但整数变量或基数约束使问题NP难，计算挑战大。传统方法如启发式和松弛技术已有探索，但这些策略在统一混合框架中的整合仍不足。

Method: 提出一个通用混合框架，包含两阶段：1) 混合松弛阶段：使用线性松弛、对偶松弛和增强松弛结合随机采样生成多样化的预解池；2) 启发式优化阶段：使用遗传算法和变邻域搜索对预解池进行精炼，有效逼近二元解。

Result: 在OR Library基准数据集上的投资组合优化问题评估中，实验结果表明该框架取得了最先进的性能，能够高效解决更大更复杂的MIP问题。

Conclusion: 本研究提供了一个鲁棒灵活的方法论，桥接了松弛技术和启发式优化，推进了具有挑战性的MIP问题的实际可解性。

Abstract: Mixed-Integer Programming (MIP), particularly Mixed-Integer Linear Programming (MILP) and Mixed-Integer Quadratic Programming (MIQP), has found extensive applications in domains such as portfolio optimization and network flow control, which inclusion of integer variables or cardinality constraints renders these problems NP-hard, posing significant computational challenges. While traditional approaches have explored approximation methods like heuristics and relaxation techniques (e.g. Lagrangian dual relaxation), the integration of these strategies within a unified hybrid framework remains underexplored. In this paper, we propose a generalized hybrid framework to address MIQP problems with binary variables, which consists of two phases: (1) a Mixed Relaxation Phase, which employs Linear Relaxation, Duality Relaxation, and Augmented Relaxation with randomized sampling to generate a diverse pre-solution pool, and (2) a Heuristic Optimization Phase, which refines the pool using Genetic Algorithms and Variable Neighborhood Search (VNS) to approximate binary solutions effectively. Becuase of the page limit, we will only detailedly evaluate the proposed framework on portfolio optimization problems using benchmark datasets from the OR Library, where the experimental results demonstrate state-of-the-art performance, highlighting the framework's ability to solve larger and more complex MIP problems efficiently. This study offers a robust and flexible methodology that bridges relaxation techniques and heuristic optimization, advancing the practical solvability of challenging MIP problems.

</details>


### [861] [Exact Instance Compression for Convex Empirical Risk Minimization via Color Refinement](https://arxiv.org/abs/2602.00437)
*Bryan Zhu,Ziang Chen*

Main category: math.OC

TL;DR: 提出基于颜色细化的无损压缩框架，用于凸经验风险最小化，扩展了线性规划和凸二次规划的工作，适用于多种凸优化问题


<details>
  <summary>Details</summary>
Motivation: 经验风险最小化计算成本高，标准求解器在凸设置下扩展性差，需要更高效的求解方法

Method: 基于颜色细化的无损压缩框架，将问题压缩为更小的等价问题，适用于线性回归、多项式回归、逻辑回归、弹性网络正则化回归、核方法等多种凸优化模型

Result: 在代表性数据集上的数值实验证明了该方法的有效性，能够显著减少计算复杂度

Conclusion: 提出的颜色细化压缩框架为凸经验风险最小化提供了一种高效的无损压缩方法，扩展了现有技术的适用范围

Abstract: Empirical risk minimization (ERM) can be computationally expensive, with standard solvers scaling poorly even in the convex setting. We propose a novel lossless compression framework for convex ERM based on color refinement, extending prior work from linear programs and convex quadratic programs to a broad class of differentiable convex optimization problems. We develop concrete algorithms for a range of models, including linear and polynomial regression, binary and multiclass logistic regression, regression with elastic-net regularization, and kernel methods such as kernel ridge regression and kernel logistic regression. Numerical experiments on representative datasets demonstrate the effectiveness of the proposed approach.

</details>


### [862] [On the Analysis of Misspecified Variational Inequalities with Nonlinear Constraints](https://arxiv.org/abs/2602.00448)
*Novel Kumar Dey,Mohammad Mahdi Ahmadi,Erfan Yazdandoost Hamedani,Afrooz Jalilzadeh*

Main category: math.OC

TL;DR: 提出一种单循环非精确增广拉格朗日方法，用于处理参数误设的变分不等式问题，通过联合求解主问题和辅助问题，实现O(1/K)的收敛速率。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动的变分不等式方法通常采用解耦的"先学习后优化"方案，导致学习阶段的近似误差会传播到主决策问题中，阻碍收敛。需要一种同时处理主问题和辅助问题的联合方法。

Method: 提出单循环非精确增广拉格朗日方法，同时更新原始决策变量、对偶乘子和误设参数。结合前向反射后向步长与增广拉格朗日惩罚项，显式处理算子和约束函数的误设。

Result: 通过证明对偶迭代的有界性，建立了基于Minty变分不等式间隙和聚合不可行度量的松弛性能指标的O(1/K)遍历收敛速率。

Conclusion: 该方法在数值实验中表现出优于现有最先进方法的性能，为处理参数误设的变分不等式问题提供了一种有效的联合求解框架。

Abstract: In this paper, we study a class of misspecified variational inequalities (VIs) where both the monotone operator and nonlinear convex constraints depend on an unknown parameter learned via a secondary VI. Existing data-driven VI methods typically follow a decoupled learn-then-optimize scheme, causing the approximation error from the learning to propagate the main decision-making problem and hinder convergence. We instead consider a simultaneous approach that jointly solves the main and secondary VIs. To efficiently handle nonlinear constraints with parameter misspecification, we propose a single-loop inexact Augmented Lagrangian method that simultaneously updates the primal decision variables, dual multipliers, and the misspecified parameter. The method combines a forward-reflected-backward step with an Augmented Lagrangian penalty, and explicitly handles misspecification on both the operator and constraint functions. Moreover, we introduce a relaxed performance metric based on the Minty VI gap combined with an aggregated infeasibility metric. By proving boundedness of the dual iterates, we establish $\mathcal{O}(1/K)$ ergodic convergence rates for these metrics. Numerical Experiments are provided to showcase the superior performance of our algorithm compared to state-of-the-art methods.

</details>


### [863] [Extending Meshulam's result on the boundedness of orbits of relaxed projections onto affine subspaces from finite to infinite-dimensional Hilbert spaces](https://arxiv.org/abs/2602.00544)
*Heinz H. Bauschke,Tran Thanh Tung*

Main category: math.OC

TL;DR: 将Meshulam关于欧氏空间中随机投影到仿射子空间序列有界性的结果推广到一般希尔伯特空间，要求对应的平行线性子空间具有内在正则性。


<details>
  <summary>Details</summary>
Motivation: Meshulam在1996年证明了欧氏空间中，即使仿射子空间的交集为空，随机投影到这些子空间生成的序列仍然保持有界。本文旨在将此结果推广到更一般的希尔伯特空间。

Method: 通过关于仿射子空间数量的归纳证明，将结果扩展到希尔伯特空间。要求对应的平行线性子空间具有内在正则性，这一条件在欧氏空间中总是成立。

Result: 成功将Meshulam的有界性结果推广到一般希尔伯特空间，并讨论了结果的尖锐性，以及与随机块Kaczmarz方法的联系。

Conclusion: 在希尔伯特空间中，只要对应的平行线性子空间具有内在正则性，随机投影到仿射子空间序列的有界性仍然成立，这扩展了Meshulam的经典结果。

Abstract: In 1996, Meshulam proved that any sequence generated in Euclidean space by randomly projecting onto affine subspaces drawn from a finite collection stays bounded even if the intersection of the subspaces is empty. His proof, which works even for relaxed projections, relies on an ingenious induction on the dimension of the Euclidean space.
  In this paper, we extend Meshulam's result to the general Hilbert space setting by an induction proof of the number of affine subspaces in the given collection. We require that the corresponding parallel linear subspaces are innately regular -- this assumption always holds in Euclidean space. We also discuss the sharpness of our result and make a connection to randomized block Kaczmarz methods.

</details>


### [864] [Deterministic Zeroth-Order Mirror Descent via Vector Fields with A Posteriori Certification](https://arxiv.org/abs/2602.00634)
*Masahito Hayashi*

Main category: math.OC

TL;DR: 提出确定性零阶镜像下降框架，用向量场替代梯度，在保持Bregman几何的同时支持无导数优化，提供后验认证的最终迭代保证


<details>
  <summary>Details</summary>
Motivation: 开发一个统一的零阶优化框架，能够处理无导数场景，同时保持镜像下降的几何结构，并为信息几何算法提供理论支撑

Method: 使用向量场驱动的镜像更新，基于相对平滑性不等式分析，通过确定性中心有限差分构建向量场，要求2d+1次函数评估

Result: 建立了确定性零阶镜像下降的理论框架，在满足穿孔邻域广义星凸性条件下获得显式分辨率相关的误差下界保证

Conclusion: 揭示了Bregman伸缩恒等式、确定性认证和鲁棒锥几何在零阶镜像下降中的隐藏几何结构联系

Abstract: We develop a deterministic zeroth-order mirror descent framework by replacing gradients with a general vector field, yielding a vector-field-driven mirror update that preserves Bregman geometry while accommodating derivative-free oracles. Our analysis provides a unified evaluation template for last-iterate function values under a relative-smoothness-type inequality, with an emphasis on trajectory-wise (a posteriori) certification: whenever a verifiable inequality holds along the realized iterates, we obtain explicit last-iterate guarantees. The framework subsumes a broad class of information-geometric algorithms, including generalized Blahut-Arimoto-type updates, by expressing their dynamics through suitable choices of the vector field. We then instantiate the theory with deterministic central finite differences in moderate dimension, where constructing the vector field via deterministic central finite differences requires 2d off-center function values (and one reusable center value), i.e., 2d+1 evaluations in total, where d is the number of input real numbers. In this deterministic finite-difference setting, the key interface property is not classical convexity alone but a punctured-neighborhood generalized star-convexity condition that isolates an explicit resolution-dependent error floor. Establishing this property for the finite-difference vector field reduces to a robust conic dominance design problem; we give an explicit scaling rule ensuring the required uniform dominance on a circular cone. Together, these results expose a hidden geometric structure linking Bregman telescoping identities, deterministic certification, and robust conic geometry in zeroth-order mirror descent.

</details>


### [865] [Properties of Measure Controls and Their Trajectories](https://arxiv.org/abs/2602.00752)
*Mauro Garavello,Xiaoqian Gong,Benedetto Piccoli*

Main category: math.OC

TL;DR: 论文研究了测度控制和测度向量场的概念，在测度微分方程框架下，建立了测度控制系统的存在性和适定性，证明了测度控制与测度向量场的等价性，并研究了轨迹集的稳定性和闭包性质。


<details>
  <summary>Details</summary>
Motivation: 测度控制可以看作是松弛控制的推广，特别适用于研究具有不确定性的动力学系统。在测度微分方程框架下，需要建立测度控制系统的理论基础。

Method: 在测度微分方程的数学框架下，研究测度控制和测度向量场的概念。使用测度理论的方法分析控制系统的性质。

Result: 建立了测度控制系统的存在性和适定性，证明了测度控制与测度向量场的等价性，研究了轨迹集的稳定性和闭包性质。

Conclusion: 测度控制是松弛控制的有力推广，在测度微分方程框架下建立了完整的理论体系，为研究具有不确定性的动力学系统提供了数学工具。

Abstract: This paper deals with the concepts of measure controls and of measure vector fields, within the mathematical framework of measure differential equations (MDEs), recently proposed in~\cite{piccoli_measure_2019}. Measure controls can be seen as a generalization of relaxed control. Moreover, they are particularly suitable for studying dynamics with uncertainty.
  The main results of this paper include establishing the existence and well-posedness of control systems with measure controls and proving the equivalence between measure controls and measure vector fields. The stability and closure properties of the trajectory set are also studied.

</details>


### [866] [On the Convergence of Jacobian-Free Backpropagation for Optimal Control Problems with Implicit Hamiltonians](https://arxiv.org/abs/2602.00921)
*Eric Gelphman,Deepanshu Verma,Nicole Tianjiao Yang,Stanley Osher,Samy Wu Fung*

Main category: math.OC

TL;DR: 本文为隐式哈密顿量最优控制中的Jacobian-Free Backpropagation（JFB）方法提供了随机小批量设置的收敛保证，并在高维问题上展示了其可扩展性。


<details>
  <summary>Details</summary>
Motivation: 基于学习值函数的方法在隐式哈密顿量最优控制中面临根本性挑战，因为缺乏闭式最优控制律。现有JFB方法仅建立了样本级下降保证，需要更全面的收敛理论。

Method: 采用Jacobian-Free Backpropagation（JFB）方法处理隐式哈密顿量最优控制问题，在随机小批量设置下分析其收敛性，并在高维控制问题上进行实证验证。

Result: 证明了JFB在随机小批量设置下收敛到期望最优控制目标的驻点，并在多智能体最优消费、群体四旋翼和自行车控制等高维问题上展示了可扩展性。

Conclusion: JFB方法在隐式哈密顿量高维最优控制中具有理论保证和实际可行性，为这类问题提供了有效的解决方案。

Abstract: Optimal feedback control with implicit Hamiltonians poses a fundamental challenge for learning-based value function methods due to the absence of closed-form optimal control laws. Recent work~\cite{gelphman2025end} introduced an implicit deep learning approach using Jacobian-Free Backpropagation (JFB) to address this setting, but only established sample-wise descent guarantees. In this paper, we establish convergence guarantees for JFB in the stochastic minibatch setting, showing that the resulting updates converge to stationary points of the expected optimal control objective. We further demonstrate scalability on substantially higher-dimensional problems, including multi-agent optimal consumption and swarm-based quadrotor and bicycle control. Together, our results provide both theoretical justification and empirical evidence for using JFB in high-dimensional optimal control with implicit Hamiltonians.

</details>


### [867] [An Efficient Memory Gradient Method for Extreme M-Eigenvalues of Elastic type Tensors](https://arxiv.org/abs/2602.01152)
*Zhuolin Du,Yisheng Song*

Main category: math.OC

TL;DR: 本文提出了一种计算四阶层次对称张量极端M-特征值的方法，通过引入位移参数将问题转化为无约束优化，并设计了记忆梯度算法，证明了全局收敛性。


<details>
  <summary>Details</summary>
Motivation: 四阶层次对称张量的M-特征值在非线性弹性材料分析和量子纠缠问题中具有重要作用，但计算极端M-特征值是一个具有挑战性的问题。

Method: 首先引入位移参数将M-特征值问题转化为一系列无约束优化问题，然后专门设计了一种记忆梯度方法来近似计算极端M-特征值。

Result: 在理论框架下建立了所提方法的全局收敛性，并通过全面的数值实验验证了方法的有效性和稳定性。

Conclusion: 本文提出的方法能够有效计算四阶层次对称张量的极端M-特征值，为非线性弹性材料分析和量子纠缠问题提供了实用的计算工具。

Abstract: M-eigenvalues of fourth order hierarchically symmetric tensors play a significant role in nonlinear elastic material analysis and quantum entanglement problems. This paper focuses on computing extreme M-eigenvalues for such tensors. To achieve this, we first reformulate the M-eigenvalue problem as a sequence of unconstrained optimization problems by introducing a shift parameter. Subsequently, we develop a memory gradient method specifically designed to approximate these extreme M-eigenvalues. Under this framework, we establish the global convergence of the proposed method. Finally, comprehensive numerical experiments demonstrate the efficacy and stability of our approach.

</details>


### [868] [Computationally Tractable Robust Nonlinear Model Predictive Control using DC Programming](https://arxiv.org/abs/2602.01164)
*Martin Doff-Sotta,Zaheen A-Rahman,Mark Cannon*

Main category: math.OC

TL;DR: 提出一种基于DC函数和顺序凸规划的鲁棒非线性MPC框架，通过数据驱动方法构建DC模型，保证递归可行性和鲁棒稳定性


<details>
  <summary>Details</summary>
Motivation: 传统非线性MPC计算复杂，难以保证鲁棒性。需要一种计算上可处理且具有鲁棒性保证的非线性MPC框架

Method: 使用DC函数和顺序凸规划，通过数据驱动方法（多项式和机器学习技术）构建DC模型表示，开发鲁棒管式MPC方案，通过线性化模型的凹分量实现在线优化凸化

Result: 提出了三种数据驱动的DC模型计算方法，通过PVTOL飞机案例研究比较性能，证明了方法的有效性

Conclusion: 该框架为非线性系统提供了计算上可处理、具有鲁棒性保证的MPC解决方案，结合了数据驱动建模和凸优化技术的优势

Abstract: We propose a computationally tractable, tube-based robust nonlinear model predictive control (MPC) framework using difference-of-convex (DC) functions and sequential convex programming. For systems with differentiable discrete time dynamics, we show how to construct systematic, data-driven DC model representations using polynomials and machine learning techniques. We develop a robust tube MPC scheme that convexifies the online optimization by linearizing the concave components of the model, and we provide guarantees of recursive feasibility and robust stability. We present three data-driven procedures for computing DC models and compare performance using a planar vertical take-off and landing (PVTOL) aircraft case study.

</details>


### [869] [Heuristics for the Worst Optimal Value of Interval Transportation Problems](https://arxiv.org/abs/2602.01209)
*Elif Radová Garajová,Miroslav Rada*

Main category: math.OC

TL;DR: 本文针对区间运输问题的最坏最优值计算提出启发式算法，包括局部搜索、遗传算法和混合这两种方法的模因算法，通过数值实验证明模因算法在近似最坏最优值方面具有竞争力。


<details>
  <summary>Details</summary>
Motivation: 区间运输问题中供应、需求和运输成本存在不确定性，计算最坏最优值是NP难问题，需要设计高效的近似算法来解决这一计算挑战。

Method: 基于准极值分解理论，设计三种启发式算法：1）使用简化场景编码的局部搜索方法；2）遗传算法；3）结合前两者的模因算法（遗传算法进化改进与局部搜索个体学习相结合）。

Result: 数值实验表明模因算法在近似区间运输问题最坏最优值方面具有竞争力，为多个实例找到了新的最优解，优于现有最先进方法。

Conclusion: 提出的模因算法能有效近似区间运输问题的最坏最优值，为解决这类NP难问题提供了实用的启发式解决方案，在多个实例上表现优于现有方法。

Abstract: An interval transportation problem represents a model for a transportation problem in which the values of supply, demand, and transportation costs are affected by uncertainty and can vary independently within given interval ranges. One of the main tasks of solving interval programming models is computing the best and worst optimal value over all possible choices of the interval data. Although the best optimal value of an interval transportation problem can be computed in polynomial time, computing the worst (finite) optimal value was proved to be NP-hard. In this paper, we strengthen a previous result showing a quasi-extreme decomposition for finding the worst optimal value, and building on the result, we design heuristics for efficiently approximating the value. Using a simplified encoding of the scenarios, we first derive a local search method and a genetic algorithm for approximating the worst optimal value. Then, we integrate the two methods into a memetic algorithm, which combines the evolutionary improvement of a genetic algorithm with individual learning implemented via local search. Moreover, we include numerical experiments for a practical comparison of the three different approaches. We also show that the proposed memetic algorithm is competitive with the available state-of-the-art methods for approximating the worst optimal value of interval transportation problems, this is demonstrated by finding the new best solutions for several instances, among others.

</details>


### [870] [Novel closed-loop controllers for fractional linear quadratic tracking systems](https://arxiv.org/abs/2602.01251)
*Iman Malmir*

Main category: math.OC

TL;DR: 提出了一种寻找分数阶跟踪二次最优控制问题闭环最优控制器的新方法


<details>
  <summary>Details</summary>
Motivation: 解决分数阶跟踪二次最优控制问题中寻找闭环最优控制器的挑战

Method: 推导了分数阶最优控制问题的最优性条件，并基于此开发了新的闭环控制器设计方法

Result: 通过示例验证了该方法的适用性和有效性

Conclusion: 提出的新方法能够有效解决分数阶跟踪二次最优控制问题，具有实际应用价值

Abstract: Anew method for finding closed-loop optimal controllers of fractional tracking quadratic optimal control problems is introduced. The optimality conditions for the fractional optimal control problem are obtained. Illustrative examples are presented to show the applicability and capabilities of the method.

</details>


### [871] [Global stabilization and finite element analysis of the viscous Burgers' equation with memory subject to Neumann boundary feedback control](https://arxiv.org/abs/2602.01321)
*Shishu Pal Singh,Sudeep Kundu*

Main category: math.OC

TL;DR: 本文研究了带记忆项的粘性Burgers方程，通过Neumann边界反馈控制实现全局稳定化，建立了L²、H¹、H²范数下的稳定性，并考虑了未知扩散系数的情况，同时给出了半离散格式的数值分析和误差估计。


<details>
  <summary>Details</summary>
Motivation: 研究带记忆项的粘性Burgers方程的全局稳定化问题，特别是在未知扩散系数的情况下，通过边界反馈控制实现系统的稳定，这对于实际工程应用具有重要意义。

Method: 采用控制Lyapunov泛函构造合适的反馈控制输入，使用Faedo-Galerkin方法证明解的存在唯一性，应用C⁰-连续有限元方法进行空间离散，利用Ritz-Volterra投影进行误差分析。

Result: 建立了L²、H¹、H²范数下的全局稳定化结果，证明了未知扩散系数情况下的全局稳定性，获得了半离散格式的全局稳定性和状态变量在L∞、L²、H¹范数下的最优误差估计，并给出了反馈控制律的误差估计。

Conclusion: 通过Neumann边界反馈控制成功实现了带记忆项粘性Burgers方程的全局稳定化，提出的数值方法有效且具有理论保证，数值模拟验证了理论结果的正确性。

Abstract: This paper presents a global stabilization result of the viscous Burgers' equation with the memory term by applying Neumann boundary feedback control laws. We construct suitable feedback control inputs using the control Lyapunov functional and establish stabilization in the \(L^{2}, H^{1},\) and \(H^{2}\)-norms. The existence and uniqueness of the solution are established through the Faedo-Galerkin method. Moreover, we show the global stabilization where the diffusion coefficient $ν$ is unknown. Then, we apply a \(C^{0}\)-conforming finite element method to the spatial variable while keeping the time variable continuous. Furthermore, we obtain global stabilization of the semi-discrete scheme and optimal error estimates for the state variable in the \(L^{\infty}\), \(L^{2}\), and \(H^{1}\)-norms, using the Ritz-Volterra projection. Additionally, error estimates for the feedback control laws are established. Lastly, we present some numerical simulations to demonstrate the theoretical findings.

</details>


### [872] [On Poly-Quadratic Stabilizability and Detectability of Polytopic LPV Systems](https://arxiv.org/abs/2602.01337)
*T. J. Meijer,V. S. Dolk,W. P. M. H. Heemels*

Main category: math.OC

TL;DR: 将离散时间线性时不变系统的Lyapunov稳定性与可检测性判据推广到多面体线性参数变化系统，使用多二次Lyapunov函数


<details>
  <summary>Details</summary>
Motivation: 现有的Lyapunov稳定性判据主要针对线性时不变系统，需要将其推广到更一般的参数变化系统，以处理实际系统中存在的参数不确定性

Method: 使用多二次Lyapunov函数，将经典判据推广到多面体线性参数变化系统，通过参数依赖的Lyapunov函数处理参数变化

Result: 成功将离散时间线性时不变系统的Lyapunov稳定性与可检测性判据推广到多面体线性参数变化系统，建立了相应的理论框架

Conclusion: 该方法为参数变化系统提供了有效的稳定性分析与控制器设计工具，扩展了经典Lyapunov理论的应用范围

Abstract: In this technical communique, we generalize the well-known Lyapunov-based stabilizability and detectability tests for discrete-time linear time-invariant systems to polytopic linear parameter-varying systems using the class of so-called poly-quadratic Lyapunov functions.

</details>


### [873] [Minimum cost network flow with interval capacities: The worst-case scenario](https://arxiv.org/abs/2602.01360)
*Miroslav Rada,Milan Hladík,Elif Radová Garajová,Francesco Carrabs,Raffaele Cerulli,Ciriaco D'Ambrosio*

Main category: math.OC

TL;DR: 研究最小成本网络流问题中弧容量区间不确定下的最坏最优值计算及最坏情形特征，证明该问题强NP难，提出精确算法，分析最极端情形的结构特性，并探讨"多流少费"悖论。


<details>
  <summary>Details</summary>
Motivation: 网络流问题在实际应用中常面临参数不确定性，特别是弧容量可能在一定区间内变化。研究如何确定最坏情况下的最优值及其对应场景，对鲁棒优化和风险管理具有重要意义。

Method: 1) 证明计算最坏最优值是强NP难问题，即使在串并联图中也保持NP难；2) 提出混合整数线性规划公式计算精确最坏最优值；3) 为串并联图设计伪多项式时间算法；4) 分析最极端最坏情形的结构特性；5) 研究"多流少费"悖论，使用增广路径进行一般性刻画，并对完全图建立更强特征。

Result: 1) 证明最坏最优值计算是强NP难的；2) 容量未固定在区间边界的弧构成森林，给出了此类弧数量的紧上界；3) 建立了"多流少费"悖论的一般特征，特别针对完全图；4) 证明了判断成本矩阵是否免疫于该悖论是强co-NP难问题。

Conclusion: 该研究系统分析了区间容量不确定下最小成本网络流的最坏情形优化问题，提供了计算复杂性的理论结果、精确算法、结构特征分析，以及对"多流少费"悖论的深入理解，为鲁棒网络流优化提供了理论基础和方法工具。

Abstract: We study the problem of determining the worst optimal value and characterizing the corresponding worst-case scenarios in minimum cost network flow problems with interval uncertainty in arc capacities. In this setting, each capacity can take any value within its specified lower and upper bounds. We prove that computing the worst optimal value is a strongly NP-hard problem and remains NP-hard even when restricted to series-parallel graphs. Further, we propose a mixed-integer linear programming formulation that computes the exact worst optimal value, as well as a pseudopolynomial-time algorithm designed for the special case of series-parallel graphs. We also examine the structural properties of the most extremal worst-case scenarios and show that the arcs whose capacities are not fixed at their interval bounds form a forest. This result establishes an upper bound on the number of such arcs, which we show to be tight by constructing a class of instances in which the bound is attained. Finally, we investigate the more-for-less paradox in minimum cost network flow problems with interval capacities, which occurs in instances where increasing the required flow leads to a decrease in the worst-case optimal cost. We provide a general characterization of this phenomenon using augmenting paths and establish a stronger characterization for complete graphs. In addition, we discuss the properties of the cost matrices immune against the paradox and prove that deciding whether a given cost matrix has this property is a strongly co-NP-hard problem.

</details>


### [874] [Robust Sublinear Convergence Rates for Iterative Bregman Projections](https://arxiv.org/abs/2602.01372)
*Gabriel Peyré*

Main category: math.OC

TL;DR: 该论文证明了在均匀有界原始质量和双重半径条件下，KL投影的双重目标以O(1/k)速率下降，常数仅随1/γ线性增长，将熵正则化最优传输的保证扩展到任何线性约束问题。


<details>
  <summary>Details</summary>
Motivation: 熵正则化为近似约束分为两个（或多个）可处理块的线性规划提供了简单方法。虽然Sinkhorn算法在最优传输中表现良好，但需要将这种收敛保证扩展到更广泛的线性约束问题，并理解其对正则化参数γ的依赖关系。

Method: 采用循环KL Bregman投影方法，分析双重目标的收敛速率。关键创新是使用块商双重半范数来度量双重半径，这取决于约束块分割的结构。通过均匀有界原始质量和双重半径假设，证明收敛速率。

Result: 证明了双重目标以O(1/k)速率下降，常数仅随1/γ线性增长。这种"鲁棒"速率使得通过交替KL投影近似未正则化问题具有有利的复杂度界限。作为应用，推导了图上的Wasserstein-1距离的flow-Sinkhorn算法。

Conclusion: 该研究将熵正则化最优传输的收敛保证扩展到任何线性约束问题，提供了对正则化参数γ依赖关系的精确控制。提出的块商双重半范数概念是分析的关键，flow-Sinkhorn算法在图上实现了高效的Wasserstein-1距离计算。

Abstract: Entropic regularization provides a simple way to approximate linear programs whose constraints split into two (or more) tractable blocks. The resulting objectives are amenable to cyclic Kullback-Leibler (KL) Bregman projections, with the classical Sinkhorn algorithm for optimal transport (balanced, unbalanced, gradient flows, barycenters, \dots) as the canonical example. Assuming uniformly bounded primal mass and dual radius, we prove that the dual objective of these KL projections decreases at an $O(1/k)$ rate with a constant that scales only linearly in $1/γ$, where $γ$ is the entropic regularization parameter. This extends the guarantees known for entropic optimal transport to any such linearly constrained problem. Following the terminology introduced in [Chizat et al 2025], we call such rates "robust", because this mild dependence on $γ$ underpins favorable complexity bounds for approximating the unregularized problem via alternating KL projections. The crucial aspect of the analysis is that the dual radius should be measured according to a block-quotient dual seminorm, which depends on the structure of the split of the constraint into blocks. As an application, we derive the flow-Sinkhorn algorithm for the Wasserstein-1 distance on graphs. It achieves $ε$-additive accuracy on the transshipment cost in $O(p/ε^{4})$ arithmetic operations, where $p$ is the number of edges.

</details>


### [875] [Evaluation of Electricity Market Clearing Mechanisms via Reinforcement Learning: Prices, Remuneration and Competitive Dynamics](https://arxiv.org/abs/2602.01392)
*Andrea Altamura,Fabrizio Lacalandra,Antonio Frangioni,Massimo La Scala*

Main category: math.OC

TL;DR: SPaC机制相比传统PaC能减少可再生能源的超边际利润和价格波动，同时保持公平参与激励，比PaB更能抵抗寡头市场力量。


<details>
  <summary>Details</summary>
Motivation: 欧洲电力市场现行的Pay-as-Clear机制在天然气价格高企时，会让可再生能源获得显著的超边际利润，使消费者面临与天然气成本相关的高价格波动。需要评估新提出的Segmented Pay-as-Clear机制作为市场替代方案。

Method: 使用基于强化学习（Q-Learning）的模拟来建模运营商的战略行为，比较三种市场模型：传统Pay-as-Clear、Pay-as-Bid和Segmented Pay-as-Clear。在两个场景下进行测试：基于2030年NECP目标的简化场景，以及基于GME 2024年公开报价的十个运营商投资组合场景。

Result: SPaC市场清算机制相比PaC减少了超边际利润和价格波动，同时保持对所有运营商的公平参与激励，并且在寡头垄断背景下比PaB更能抵抗市场力量的行使。

Conclusion: SPaC机制在减少价格波动和超边际利润方面优于传统PaC，在抵抗市场力量方面优于PaB。开发的框架可作为监管机构和政策制定者评估市场设计改革提案的支持工具。

Abstract: The Pay-as-Clear (PaC) mechanism currently used in the European electricity market can generate significant submarginal profits for renewable sources when the clearing price is determined by the marginal offers of gas-fired generation units and the cost of natural gas exceeds certain levels. This exposes consumers to high price volatility related to the cost of natural gas. This report analyzes the recently proposed Segmented Pay-as-Clear (SPaC) mechanism as a market alternative, evaluating its system cost-effectiveness through simulations based on Reinforcement Learning (Q-Learning) to model the strategic behavior of operators. Three market models are compared, the two classic Pay-as-Clear (PaC) and Pay-as-Bid (PaB) along with SPaC, under two scenarios: a simplified one based on the 2030 NECP objectives and one built on the portfolios of ten operators obtained from the GME's 2024 public offers. The results show that the SPaC market clearing mechanism reduces intramarginal profits and price volatility compared to PaC, while maintaining fair participation incentives for all operators, and is more robust than PaB to the exercise of market power in oligopolistic contexts. The developed framework can serve as a support tool for regulators and policymakers in the evaluation of proposals for market design reforms.

</details>


### [876] [Regret of $H_\infty$ Preview Controllers](https://arxiv.org/abs/2602.01420)
*Jietian Liu,Peter Seiler*

Main category: math.OC

TL;DR: 研究预览控制在H∞和遗憾最优框架下的性能，证明随着预览步数增加，控制器能渐近达到非因果最优控制器的性能


<details>
  <summary>Details</summary>
Motivation: 研究如何利用未来扰动信息（预览）来改善控制性能，探索在H∞和遗憾最优框架下，预览控制能否接近非因果最优控制器的性能

Method: 采用离散时间线性时不变系统模型，首先回顾p步扰动预览的H∞预览控制器构造，然后分析当预览步数p→∞时，闭环H∞性能的收敛性

Result: 证明H∞预览控制器的闭环性能随p→∞收敛到最优非因果控制器性能，且最优遗憾收敛到零，数值算例验证了理论结果

Conclusion: 增加预览长度可使控制器在H∞和遗憾框架下渐近达到非因果性能，预览控制能有效利用未来信息提升控制性能

Abstract: This paper studies preview control in both the $H_\infty$ and regret-optimal settings. The plant is modeled as a discrete-time, linear time-invariant system subject to external disturbances. The performance baseline is the optimal non-causal controller that has full knowledge of the disturbance sequence. We first review the construction of the $H_\infty$ preview controller with $p$-steps of disturbance preview. We then show that the closed-loop $H_\infty$ performance of this preview controller converges as $p\to \infty$ to the performance of the optimal non-causal controller. Furthermore, we prove that the optimal regret of the preview controller converges to zero. These results demonstrate that increasing preview length allows controllers to asymptotically achieve non-causal performance in both the $H_\infty$ and regret frameworks. A numerical example illustrates the theoretical results.

</details>


### [877] [The Dynamic Search for the Minimal Dynamic Extension](https://arxiv.org/abs/2602.01457)
*Rollen S. D'Souza*

Main category: math.OC

TL;DR: 将非线性控制系统的动态反馈线性化问题视为范畴搜索问题，提出基于动态规划和启发式的方法寻找最小动态扩展


<details>
  <summary>Details</summary>
Motivation: 传统动态反馈线性化方法存在计算成本高或限制性强的缺陷，需要更有效的解决方案

Method: 将问题建模为范畴搜索问题，应用动态规划算法；在有限范畴中使用经典搜索算法寻找最小动态扩展，或采用启发式方法选择无限扩展

Result: 为动态预补偿器搜索提供了独特的"鸟瞰"视角，能够找到最小动态扩展或通过启发式选择合适扩展

Conclusion: 范畴论框架为动态反馈线性化问题提供了新颖有效的解决方案，克服了传统方法的局限性

Abstract: Identifying the dynamic precompensator that renders a nonlinear control system feedback linearizable is a challenging problem. Researchers have explored the problem -- dynamic feedback linearization -- and produced existence conditions and constructive procedures for the dynamic precompensator. These remain, in general, either computationally expensive or restrictive. Treating the challenge as intrinsic, this article views the problem as a search problem over a category. Dynamic programming applies and, upon restriction to a finite category, classic search algorithms find the minimal dynamic extension. Alternatively, a heuristic aiming towards feedback linearizable systems can be employed to select amongst the infinitely-many extensions. This framing provides a distinctive, birds-eye view of the search for the dynamic precompensator.

</details>


### [878] [Non-Uniform Noise-to-Signal Ratio in the REINFORCE Policy-Gradient Estimator](https://arxiv.org/abs/2602.01460)
*Haoyu Han,Heng Yang*

Main category: math.OC

TL;DR: 本文通过噪声信号比(NSR)分析策略梯度方法的训练不稳定性，发现NSR在最优策略附近会急剧增加，导致训练不稳定和策略崩溃。


<details>
  <summary>Details</summary>
Motivation: 策略梯度方法在强化学习中广泛应用，但训练过程经常变得不稳定或随着学习进展而减慢。本文旨在通过分析策略梯度估计器的噪声信号比(NSR)来研究这一现象。

Method: 对于(i)有限时域线性系统与高斯策略和线性状态反馈，以及(ii)有限时域多项式系统与高斯策略和多项式反馈，精确表征REINFORCE估计器的NSR（闭式解或数值矩评估算法）。对于一般非线性动态和表达性策略（包括神经网络策略），推导方差的上界。

Result: NSR景观高度不均匀，通常在策略接近最优时增加；在某些情况下会急剧增大，可能触发训练不稳定和策略崩溃。这些特征使得能够直接检查NSR如何随策略参数变化以及如何沿优化轨迹演化。

Conclusion: 策略梯度方法的训练不稳定性可以通过NSR分析来理解，NSR在最优策略附近的增加可能导致训练崩溃，这为改进策略梯度算法提供了理论洞察。

Abstract: Policy-gradient methods are widely used in reinforcement learning, yet training often becomes unstable or slows down as learning progresses. We study this phenomenon through the noise-to-signal ratio (NSR) of a policy-gradient estimator, defined as the estimator variance (noise) normalized by the squared norm of the true gradient (signal). Our main result is that, for (i) finite-horizon linear systems with Gaussian policies and linear state-feedback, and (ii) finite-horizon polynomial systems with Gaussian policies and polynomial feedback, the NSR of the REINFORCE estimator can be characterized exactly-either in closed form or via numerical moment-evaluation algorithms-without approximation. For general nonlinear dynamics and expressive policies (including neural policies), we further derive a general upper bound on the variance. These characterizations enable a direct examination of how NSR varies across policy parameters and how it evolves along optimization trajectories (e.g. SGD and Adam). Across a range of examples, we find that the NSR landscape is highly non-uniform and typically increases as the policy approaches an optimum; in some regimes it blows up, which can trigger training instability and policy collapse.

</details>


### [879] [Conformal Prediction for Early Stopping in Mixed Integer Optimization](https://arxiv.org/abs/2602.01476)
*Stefan Clarke,Bartolomeo Stellato*

Main category: math.OC

TL;DR: 使用神经网络和保形预测提前终止MIP求解器，在保证解质量的同时减少60%以上求解时间


<details>
  <summary>Details</summary>
Motivation: 混合整数规划求解器通常在搜索早期就能找到最优解，但大部分计算时间都花在证明最优性上。针对相似问题实例的分布，学习何时提前终止求解器可以显著提高效率。

Method: 训练神经网络从求解器状态估计真实最优性间隙，然后使用保形预测校准停止阈值，提供严格的概率保证。在distributional MIPLIB库的五个问题族上进行验证。

Result: 该方法将求解时间减少了60%以上，同时以95%的概率保证解的质量在0.1%最优性间隙内。

Conclusion: 通过结合神经网络和保形预测，可以在保证解质量的前提下显著加速混合整数规划求解过程，为实际应用提供了高效的解决方案。

Abstract: Mixed-integer optimization solvers often find optimal solutions early in the search, yet spend the majority of computation time proving optimality. We exploit this by learning when to terminate solvers early on distributions of similar problem instances. Our method trains a neural network to estimate the true optimality gap from the solver state, then uses conformal prediction to calibrate a stopping threshold with rigorous probabilistic guarantees on solution quality. On five problem families from the distributional MIPLIB library, our method reduces solve time by over 60% while guaranteeing 0.1%- optimal solutions with 95% probability

</details>


### [880] [Why Does Adaptive Zeroth-Order Optimization Work?](https://arxiv.org/abs/2602.01627)
*Haishan Ye,Luo Luo*

Main category: math.OC

TL;DR: 本文分析了自适应零阶优化方法，通过理论证明其比固定步长的传统零阶方法具有更快的收敛速度和更好的查询效率。


<details>
  <summary>Details</summary>
Motivation: 在现实应用中，梯度信息难以获取或计算成本高昂时，零阶优化方法很受欢迎。最近，通过函数值经验标准差归一化梯度估计器的自适应零阶方法在实践中表现出色，特别是在大语言模型微调中。然而，对这种策略的理论理解仍然有限。

Method: 首先证明经验标准差与（随机）梯度范数高度相关。然后在广义(L₀,L₁)-光滑性条件下，分析基于矩阵范数的自适应零阶方法。为确定性和随机设置建立显式收敛速率和查询复杂度界限。

Result: 理论分析表明，自适应零阶方法相比固定步长的传统零阶方法，能够实现更快的收敛速度和改进的查询效率。

Conclusion: 自适应零阶优化方法不仅在实践中有良好表现，而且具有坚实的理论基础，在梯度信息难以获取的场景下具有显著优势。

Abstract: Zeroth-order (ZO) optimization is popular in real-world applications that accessing the gradient information is expensive or unavailable.
  Recently, adaptive ZO methods that normalize gradient estimators by the empirical standard deviation of function values have achieved strong practical performance, particularly in fine-tuning the large language model.
  However, the theoretical understanding of such strategy remains limited.
  In this work, we show that the empirical standard deviation is, with high probability, closely proportional to the norm of the (stochastic) gradient.
  Based on this insight, we analyze adaptive ZO methods under the generalized $(L_0,L_1)$-smoothness condition with respect to the matrix norm.
  We establish explicit convergence rates and query complexity bounds for both deterministic and stochastic settings, demonstrating that adaptive ZO methods achieve the faster convergence and the improved query efficiency compared to the vanilla ZO methods with fixed-step.

</details>


### [881] [Optimal Liquidation in a Defaultable Market](https://arxiv.org/abs/2602.01968)
*Daniel Hernández-Hernńdez,Harold A. Moreno-Franco,José-Luis Pérez*

Main category: math.OC

TL;DR: 研究具有违约风险的大规模投资组合最优清算问题，考虑资产出售对价格的影响，给出显式解和最优策略


<details>
  <summary>Details</summary>
Motivation: 处理具有违约风险的大规模投资组合清算问题，考虑大规模资产出售对市场价格的影响，寻找最优清算策略以最大化收益

Method: 使用布朗运动描述公司价值演变和违约时间，考虑市场影响效应，建立控制问题模型分析最优清算策略

Result: 在适当模型假设下，给出了价值函数的显式解，并获得了最优策略的精确描述

Conclusion: 成功解决了具有违约风险的大规模投资组合最优清算问题，提供了可计算的显式解和具体的最优交易策略

Abstract: In this paper we address the problem of optimal liquidation of a large portfolio composed by securities exposed to default risk. The default time is described in terms of a Brownian motion representing the evolution of the value of the firm, whose assets are available in the market for investors. Considering that selling a large number of assets has a significant impact in the price, and hence in the portfolio's value, the control problem involved to describe the optimal strategy to liquidate a large position is analyzed. Under suitable assumptions in the model, an explicit solution is given to the value function and a precise description of the optimal strategy is obtained.

</details>


### [882] [Characterizations of inexact proximal operators](https://arxiv.org/abs/2602.02022)
*Guillaume Lauga,Samuel Vaiter*

Main category: math.OC

TL;DR: 本文研究了非精确近端算子的特征化，提出了几种近似定义，并分析了含误差的近端算法的收敛性。


<details>
  <summary>Details</summary>
Motivation: 近端算子在非光滑优化中广泛应用，但实际应用中常使用近似计算。需要研究什么构成近端算子的良好近似，以及含误差的近端算法的收敛性。

Method: 提出非精确近端算子的几种定义，讨论其正则性、近似能力和不动点特性。基于这些特征化，研究含非可和/非消失误差的近端算法收敛性，包括近端点算法、前向后向算法、Peaceman-Rachford算法和Douglas-Rachford算法。

Result: 建立了非精确近端算子的特征化理论，为判断近端算子的近似质量提供了标准。在弱凸函数（其近端算子被近似）和强凸函数之和的最小化问题中，分析了含误差算法的收敛性。

Conclusion: 本文扩展了近端算子的特征化理论到近似情形，为实际应用中含误差的近端算法提供了理论保证，特别是在非可和/非消失误差情况下的收敛性分析。

Abstract: Proximal operators are now ubiquitous in non-smooth optimization. Since their introduction in the seminal work of Moreau, many papers have shown their effectiveness on a wide variety of problems, culminating in their use to construct convergent deep learning methods. The characterization of these operators for non-convex penalties was completed recently in [Gribonval et al, A characterization of proximity operators, 2020]. In this paper, we propose to follow this line of work by characterizing inexact proximal operators, thus providing an answer to what constitutes a good approximation of these operators. We propose several definitions of approximations and discuss their regularity, approximation power, and their fixed points. Equipped with these characterizations, we investigate the convergence of proximal algorithms in the presence of errors that may be non-summable and/or non-vanishing. In particular, we look at the proximal point algorithm, and at the forward-backward, Peaceman-Rachford and Douglas-Rachford algorithms when we minimize the sum of a weakly convex function (whose proximal operator is approximated) and a strongly convex function.

</details>


### [883] [Well-Posed KL-Regularized Control via Wasserstein and Kalman-Wasserstein KL Divergences](https://arxiv.org/abs/2602.02250)
*Viktor Stein,Adwait Datar,Nihat Ay*

Main category: math.OC

TL;DR: 提出基于Wasserstein几何的KL散度变体，解决传统KL正则化在支撑集不匹配和低噪声极限下的问题，在最优控制中表现更优


<details>
  <summary>Details</summary>
Motivation: 传统KL散度正则化在强化学习中广泛应用，但在支撑集不匹配时会变为无穷大，在低噪声极限下会退化，需要更稳健的替代方案

Method: 使用统一的信息几何框架，将KL散度动态公式中的Fisher-Rao几何替换为基于传输的几何（Wasserstein），推导常见分布族的闭式解

Result: 新散度在支撑集不匹配时保持有限，为Kalman集成方法中的正则化启发式提供几何解释，在线性时不变系统和高斯过程噪声下消除奇异性

Conclusion: 基于Wasserstein的KL散度变体解决了传统KL正则化的局限性，在最优控制问题中表现优于传统KL正则化，特别是在低噪声极限下

Abstract: Kullback-Leibler divergence (KL) regularization is widely used in reinforcement learning, but it becomes infinite under support mismatch and can degenerate in low-noise limits. Utilizing a unified information-geometric framework, we introduce (Kalman)-Wasserstein-based KL analogues by replacing the Fisher-Rao geometry in the dynamical formulation of the KL with transport-based geometries, and we derive closed-form values for common distribution families. These divergences remain finite under support mismatch and yield a geometric interpretation of regularization heuristics used in Kalman ensemble methods. We demonstrate the utility of these divergences in KL-regularized optimal control. In the fully tractable setting of linear time-invariant systems with Gaussian process noise, the classical KL reduces to a quadratic control penalty that becomes singular as process noise vanishes. Our variants remove this singularity, yielding well-posed problems. On a double integrator and a cart-pole example, the resulting controls outperform KL-based regularization.

</details>


### [884] [Games with Rational and Herding Players](https://arxiv.org/abs/2602.02291)
*Raghupati Vyas,Khushboo Agarwal,Konstantin Avrachenkov,Veeraruna Kavitha*

Main category: math.OC

TL;DR: 本文提出一个分析框架，研究包含理性玩家和从众玩家的大规模群体博弈，引入α-RNE均衡概念，发现从众行为可能提升系统效率，甚至在某些情况下改善交通网络性能。


<details>
  <summary>Details</summary>
Motivation: 现实场景中，许多玩家并非基于理性推理而是基于固有倾向（从众行为）选择行动。经典博弈论主要关注理性玩家，需要新框架来分析理性与从众玩家混合的群体博弈。

Method: 提出α-理性纳什均衡（α-RNE）概念，分析理性玩家比例α对均衡的影响。研究交通网络和带宽共享博弈，分析从众比例对系统性能（如无政府价格PoA）的影响，并探讨机制设计问题。

Result: 理性玩家可能因从众玩家的存在而获益，甚至获得超过社会最优的效用。在某些情况下，从众玩家也能获得接近社会最优的效用。交通网络中，当从众比例较高且拥堵较低时，新增链路可能提高效率，这与经典的布雷斯悖论相反。

Conclusion: 从众行为可以显著改变博弈结果，理性与从众玩家的混合可能提升系统效率。在机制设计中，从众行为既带来机会（可引导改善效率）也带来风险（不当引导可能恶化性能）。

Abstract: Classical game theory is a powerful framework to analyze the strategic interactions among rational players. However, in many real-life scenarios, players choose actions based on their inherent natural tendencies rather than deliberate reasoning. In this paper, we develop an analytical framework to study large population games with an alpha-fraction of rational and (1-alpha)-fraction of herding players. We introduce a new notion of equilibrium called alpha-Rational Nash Equilibrium (in short, alpha-RNE) and discuss its interpretations. Some classical equilibria may disappear, and some new ones may emerge, but only for smaller alpha >0. Interestingly, rational players benefit from the presence of herding and may even achieve utility exceeding the socially optimum. Even more strikingly, in some cases, the herding players also benefit, attaining utility close to the social optimum.
  We further study the effect of the herding fraction on system performance using measures such as the Price of Anarchy (PoA). In transportation networks, a well-known paradox first studied by Pigou and later by Braess typically arises from rational decision-making: adding an extra link can reduce overall efficiency. Our analysis leads to a different conclusion. When a substantial fraction of users exhibit herding behavior, introducing a new link can increase efficiency, provided herding choices can be suitably influenced. The gains are larger when the herding fraction is higher and/or congestion is lower. By contrast, when herding decisions cannot be influenced, the added link may become detrimental. We also study a bandwidth sharing game in which herding tendencies improve system efficiency. Finally, we discuss the mechanism or influence design in the presence of herding, highlighting both opportunities and risks.

</details>


### [885] [A Two-Stage Stochastic Optimization Model for the Equitable Deployment of Fixed and Mobile Electric Vehicle Charging Stations](https://arxiv.org/abs/2602.02333)
*Hamid Najafzad,Moddassir Khan Nayeem,Fuhad Ahmed Opu,Omar Abbaas,Gabriel Nicolosi*

Main category: math.OC

TL;DR: 提出两阶段随机混合整数规划模型，优化固定和移动充电站部署，提升电动汽车充电基础设施的可靠性和公平性。


<details>
  <summary>Details</summary>
Motivation: 电动汽车广泛采用的主要障碍是缺乏可靠和公平的充电基础设施。充电站选址不当会造成覆盖缺口，减缓电动汽车普及，特别是在服务不足的社区。

Method: 1. 使用改进的ESS算法确定候选位置有限支配集；2. 第一阶段基于长期交通模式、预算约束和社会经济因素分配固定充电站；3. 第二阶段根据短期需求波动和不确定性动态分配移动充电站，最小化搬迁成本同时最大化覆盖；4. 采用基于场景的框架捕捉需求变化。

Result: 在现实网络上的数值实验表明，该模型能够增强系统韧性并减少未满足的需求。

Conclusion: 研究结果为规划者和政策制定者提供了实用见解，有助于开发可访问且需求响应的电动汽车充电基础设施。

Abstract: A major barrier to wide adoption of Electric Vehicles (EVs) is the absence of reliable and equitable charging infrastructure. Poorly located charging stations create coverage gaps and slow down EV adoption, especially in underserved communities. This paper proposes a two-stage stochastic mixed-integer programming model for the optimal deployment of Fixed and Mobile Charging Stations (FCSs and MCSs) across multiple zones and periods. Initially, a finite dominating set of candidate locations is identified using the Edge Scanning Algorithm for a Single Refueling Station (ESS), an exact continuous-location method. We modify the ESS algorithm to incorporate existing public charging stations, thereby avoiding redundant coverage. In the first stage of our model, FCSs are allocated based on long-term traffic patterns, budgetary constraints, and socioeconomic factors to ensure stable baseline coverage. The second stage dynamically assigns MCSs in response to short-term demand fluctuations and uncertainties, aiming to minimize relocation costs while maximizing coverage. We use a scenario-based framework to capture demand variability. Numerical experiments on realistic networks demonstrate the model's capacity to enhance system resilience and reduce unmet demand. These findings offer practical insights for planners and policymakers seeking to develop accessible and demand-responsive EV charging infrastructure.

</details>


### [886] [Sequential Quadratic Sum-of-squares Programming for Nonlinear Control Systems](https://arxiv.org/abs/2602.02394)
*Jan Olucak,Torbjørn Cunis*

Main category: math.OC

TL;DR: 提出一种用于非凸SOS规划的滤波器线搜索算法，通过求解二次子问题序列显著减少迭代次数和计算时间


<details>
  <summary>Details</summary>
Motivation: 非线性系统分析和控制设计中的许多问题（如局部吸引域估计、可达集内逼近、状态和输入约束下的控制设计）可表述为非凸SOS规划，但缺乏高效可解的算法，限制了在控制工程中的应用

Method: 提出滤波器线搜索算法，通过求解一系列二次子问题来解决非凸SOS规划问题

Result: 数值基准测试表明，相比现有非凸SOS规划方法，该算法能显著减少迭代次数，大幅降低计算时间

Conclusion: 该算法为非线性系统分析和控制设计中的非凸SOS规划问题提供了高效解决方案，并提供了开源实现和数值基准测试

Abstract: Many problems in nonlinear systems analysis and control design, such as local region-of-attraction estimation, inner-approximations of reachable sets or control design under state and control constraints can be formulated as nonconvex sum-of-squares programs. Yet tractable and efficient solution methods are still lacking, limiting their application in control engineering. To address this gap, we propose a filter line-search algorithm that solves a sequence of quadratic subproblems. Numerical benchmarks demonstrate that the algorithm can significantly reduce the number of iterations, resulting in a substantial decrease in computation time compared to established methods for nonconvex sum-of-squares programs. An open-source implementation of the algorithm along with the numerical benchmarks is provided

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [887] [Explainable Patterns in Cryptocurrency Microstructure](https://arxiv.org/abs/2602.00776)
*Bartosz Bieganowski,Robert Ślepaczuk*

Main category: q-fin.TR

TL;DR: 研究发现加密货币限价订单簿微观结构存在稳定的跨资产模式，相同特征在不同资产中表现出相似的预测重要性和SHAP依赖形状，验证了微观结构理论的适用性


<details>
  <summary>Details</summary>
Motivation: 探索加密货币市场中限价订单簿微观结构特征是否在不同资产间存在稳定的预测模式，验证微观结构理论在加密货币市场的适用性，并为算法交易提供通用特征库

Method: 使用统一CatBoost建模流程，采用方向感知GMADL目标和时间序列交叉验证，分析BTC、LTC、ETC、ENJ、ROSE等资产在Binance永续合约上的1秒频率订单簿和交易数据

Result: 特征排名和部分效应在不同资产间保持稳定，验证了订单流不平衡、价差和逆向选择等微观结构理论，通过保守的顶层簿taker回测和固定深度maker回测验证了可交易性

Conclusion: 研究揭示了加密货币市场存在可移植的短期收益微观结构表示，支持为加密市场开发通用特征库，并通过闪崩事件验证了算法交易中的系统性风险

Abstract: We document stable cross-asset patterns in cryptocurrency limit-order-book microstructure: the same engineered order book and trade features exhibit remarkably similar predictive importance and SHAP dependence shapes across assets spanning an order of magnitude in market capitalization (BTC, LTC, ETC, ENJ, ROSE). The data covers Binance Futures perpetual contract order books and trades on 1-second frequency starting from January 1st, 2022 up to October 12th, 2025. Using a unified CatBoost modeling pipeline with a direction-aware GMADL objective and time-series cross validation, we show that feature rankings and partial effects are stable across assets despite heterogeneous liquidity and volatility. We connect these SHAP structures to microstructure theory (order flow imbalance, spread, and adverse selection) and validate tradability via a conservative top-of-book taker backtest as well as fixed depth maker backtest. Our primary novelty is a robustness analysis of a major flash crash, where the divergent performance of our taker and maker strategies empirically validates classic microstructure theories of adverse selection and highlights the systemic risks of algorithmic trading. Our results suggest a portable microstructure representation of short-horizon returns and motivate universal feature libraries for crypto markets.

</details>


<div id='q-fin.MF'></div>

# q-fin.MF [[Back]](#toc)

### [888] [A Formal Approach to AMM Fee Mechanisms with Lean 4](https://arxiv.org/abs/2602.00101)
*Marco Dessalvi,Massimo Bartoletti,Alberto Lluch-Lafuente*

Main category: q-fin.MF

TL;DR: 该论文将交易费用参数引入AMM模型，分析费用对交易策略的影响，证明大额单笔交易优于拆分交易，并推导了带费用的套利问题闭式解。


<details>
  <summary>Details</summary>
Motivation: 交易费用对AMM的经济特性有重要影响，但现有模型常将其抽象化以简化分析。需要开发能精确考虑费用影响的AMM模型，以理解其对用户交易策略的非平凡影响。

Method: 在基础AMM模型中引入交易费用参数φ∈(0,1]到交换率函数中。当φ=1时无费用，恢复原模型。从经济学角度分析费用调整后的模型，使用Lean 4证明助手对所有结果进行形式化和机器验证。

Result: 1. 交换率函数的输出有界性和单调性等关键性质得以保留；2. 可加性不再成立，推导了捕获费用影响的广义可加性形式；3. 当φ<1时，执行单笔大额交易比拆分成多笔小交易获得严格更大利润；4. 推导了带交易费用的套利问题闭式解并证明其唯一性。

Conclusion: 交易费用显著改变了AMM的经济特性，特别是破坏了可加性并影响最优交易策略。形式化分析表明大额单笔交易策略更优，为DeFi协议设计和用户策略提供了理论基础。

Abstract: Decentralized Finance (DeFi) has revolutionized financial markets by enabling complex asset-exchange protocols without trusted intermediaries. Automated Market Makers (AMMs) are a central component of DeFi, providing the core functionality of swapping assets of different types at algorithmically computed exchange rates. Several mainstream AMM implementations are based on the constant-product model, which ensures that swaps preserve the product of the token reserves in the AMM -- up to a \emph{trading fee} used to incentivize liquidity provision. Trading fees substantially complicate the economic properties of AMMs, and for this reason some AMM models abstract them away in order to simplify the analysis. However, trading fees have a non-trivial impact on users' trading strategies, making it crucial to develop refined AMM models that precisely account for their effects. We extend a foundational model of AMMs by introducing a new parameter, the trading fee $φ\in(0,1]$, into the swap rate function. Fee amounts increase inversely proportional to $φ$. When $φ= 1$, no fee is applied and the original model is recovered. We analyze the resulting fee-adjusted model from an economic perspective. We show that several key properties of the swap rate function, including output-boundedness and monotonicity, are preserved. At the same time, other properties - most notably additivity - no longer hold. We precisely characterize this deviation by deriving a generalized form of additivity that captures the effect of swaps in the presence of trading fees. We prove that when $φ< 1$, executing a single large swap yields strictly greater profit than splitting the trade into smaller ones. Finally, we derive a closed-form solution to the arbitrage problem in the presence of trading fees and prove its uniqueness. All results are formalized and machine-checked in the Lean 4 proof assistant.

</details>


### [889] [Short-Rate-Dependent Volatility Models](https://arxiv.org/abs/2602.00858)
*Tim Leung,Matthew Lorig*

Main category: q-fin.MF

TL;DR: 该论文研究了利率依赖波动率模型中的欧式期权定价，得到了依赖特征函数的显式定价公式，并提供了可解析计算特征函数的模型示例和数值实现方法。


<details>
  <summary>Details</summary>
Motivation: 研究在波动率依赖于短期利率的模型中进行欧式期权定价的问题，这类模型能更好地反映利率变化对资产波动性的影响。

Method: 推导出依赖特征函数的显式定价公式，提供可解析计算特征函数的模型示例，并展示数值实现方法以生成隐含波动率。

Result: 得到了欧式期权的显式定价公式，该公式依赖于特征函数，并在特定模型中可实现解析计算，同时提供了数值实现方法。

Conclusion: 成功解决了利率依赖波动率模型中的期权定价问题，提供了实用的解析和数值方法，为这类复杂模型的定价提供了有效工具。

Abstract: We price European options in a class of models in which the volatility of the underlying risky asset depends on the short rate of interest. Our study results in an explicit pricing formula that depends on knowledge of a characteristic function. We provide examples of models in which the characteristic function can be computed analytically and, thus, the value of European options is explicit. Numerical implementation to produce the implied volatility is also presented.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [890] [A Prior-Predictive Monte Carlo Framework for Pricing Complex Data Products in Data-Poor Markets](https://arxiv.org/abs/2602.00121)
*Adam L. Siemiatkowski,Victor Zhirnov,Kashyap Yellai,Gabriella Bein,Terresa Zimmerman*

Main category: q-fin.CP

TL;DR: 提出一个先验预测蒙特卡洛框架，为缺乏交易数据的高级数据产品生成公平、一致且有依据的价格区间，通过模拟多种定价场景产生概率价格带而非单点估计。


<details>
  <summary>Details</summary>
Motivation: 高级数据产品（如半导体制造领域）定价面临挑战：公开交易数据稀缺、异质性强且保密性高。传统定价方法依赖临时启发式或需要大量历史交易数据，难以应对数据经济时代的需求。

Method: 采用先验预测蒙特卡洛框架，通过模拟多种合理的定价"世界"和交易配置，生成稳定的概率价格带（如P5/P50/P95）。使用约束截断先验确保商业现实性，形成可审计、可重复的概率定价系统。

Result: 该框架能够在缺乏经验数据的情况下，为数据产品生成公平、一致且有依据的价格范围，创建可审计、可重复的概率定价系统，并允许通过贝叶斯更新随着交易数据积累而改进。

Conclusion: 提出的模型将基于专业经验的传统数据定价与数据驱动方法相结合，为高级数据产品提供了一种在数据稀缺情况下的可靠定价解决方案，并支持随着数据积累而持续优化。

Abstract: Pricing advanced data products - particularly in complex fields such as semiconductor manufacturing - is a fundamentally challenging task due to the sparsity of publicly available transaction data, and its frequent heterogeneity and confidentiality. While data value depends on multiple interacting factors, such as technical sophistication, quality, utility, and licensing rights, traditional pricing methods tend to rely on ad-hoc heuristics or require massive amounts of historical transaction data. In an increasingly data-based economy, we introduce a prior-predictive Monte Carlo framework that enables the generation of fair, consistent, and justified price ranges for data products in the absence of empirical data. By simulating many plausible pricing 'worlds' and deal configurations, the framework produces stable probabilistic price bands (e.g., P5/P50/P95) rather than single point estimates, creating an auditable and repeatable probabilistic pricing system with business realism enforced via constraint-truncated priors. The proposed model bridges traditional data pricing rooted in professional experience with a data-based approach that also allows for classical Bayesian updating as more transaction data is accumulated.

</details>


### [891] [Numerical Simulations for Time-Fractional Black-Scholes Equations](https://arxiv.org/abs/2602.00201)
*Neetu Garg,A. S. V. Ravi Kanth*

Main category: q-fin.CP

TL;DR: 提出了一种用于时间分数阶Black-Scholes模型的数值算法，结合Crank-Nicolson时间离散和指数B样条空间逼近，用于欧式期权定价。


<details>
  <summary>Details</summary>
Motivation: 时间分数阶Black-Scholes模型能更好地描述金融市场中的异常扩散现象，但需要高效的数值算法来求解该模型。

Method: 使用Crank-Nicolson方法离散时间变量，采用指数B样条逼近空间变量，构建无条件稳定的数值格式。

Result: 数值实验验证了理论分析，与现有方法比较显示出优越性，算法无条件稳定且高效。

Conclusion: 提出的混合数值方法能有效求解时间分数阶Black-Scholes模型，为欧式期权定价提供了可靠工具。

Abstract: This paper implements an efficient numerical algorithm for the time-fractional Black-Scholes model governing European options. The proposed method comprises the Crank-Nicolson approach to discretize the time variable and exponential B-spline approximation for the space variable. The implemented method is unconditionally stable. We present few numerical examples to confirm the theory. Numerical simulations with comparisons exhibit the supremacy of the proposed approach.

</details>
