<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 85]
- [econ.EM](#econ.EM) [Total: 5]
- [q-fin.CP](#q-fin.CP) [Total: 3]
- [q-fin.GN](#q-fin.GN) [Total: 2]
- [cs.LG](#cs.LG) [Total: 156]
- [math.OC](#math.OC) [Total: 31]
- [cs.CY](#cs.CY) [Total: 20]
- [eess.SY](#eess.SY) [Total: 14]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [q-fin.RM](#q-fin.RM) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [q-fin.MF](#q-fin.MF) [Total: 2]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [cs.AI](#cs.AI) [Total: 114]
- [stat.ML](#stat.ML) [Total: 13]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Multimodal Consistency-Guided Reference-Free Data Selection for ASR Accent Adaptation](https://arxiv.org/abs/2602.13263)
*Ligong Lei,Wenwen Lu,Xudong Pang,Zaokere Kadeer,Aishan Wumaier*

Main category: cs.CL

TL;DR: 提出多模态一致性引导的无参考数据选择流程，用于ASR口音适应，通过语音-文本对齐和预测WER筛选可靠伪标签，显著提升口音语音识别性能。


<details>
  <summary>Details</summary>
Motivation: ASR系统在口音语音上性能下降，因为声学-语音和韵律变化导致与训练数据不匹配，而有标签的口音适应成本高昂。现有的伪标签选择方法主要基于文本（如困惑度过滤），可能选择流利但与声学不匹配的假设，导致微调时错误放大。

Method: 提出多模态一致性引导的无参考数据选择流程：1）基于子模互信息的目标感知预选择提高查询相关性；2）通过扰动解码为每个话语生成多个伪转录；3）使用两种无参考信号评分：共享嵌入空间中的语音-文本对齐和预测词错误率；4）基于百分位的选择规则保留可靠伪标签用于微调。

Result: 在领域内设置中，从30k池中选择约1.5k话语达到10.91% WER，接近使用30k监督标签的10.45%。在跨领域设置中，一致性过滤的子集避免了强口音偏移下未过滤伪标签导致的性能下降，匹配小时实验在更强的ASR骨干上进一步证实了对随机采样和现有基线的优势。

Conclusion: 提出的多模态一致性引导数据选择流程能有效筛选可靠伪标签用于ASR口音适应，在领域内和跨领域设置中均表现优异，显著减少了对监督标签的需求，同时避免了错误放大问题。

Abstract: Automatic speech recognition (ASR) systems often degrade on accented speech because acoustic-phonetic and prosodic shifts induce a mismatch to training data, making labeled accent adaptation costly. However, common pseudo-label selection heuristics are largely text-centric (e.g., perplexity (PPL) filtering) and can prefer fluent yet acoustically mismatched hypotheses, leading to error amplification when fine-tuning. To address this, we introduce a multimodal consistency-guided, reference-free data selection pipeline for ASR accent adaptation under a transductive, label-free protocol. The pipeline starts with a target-aware preselection step based on submodular mutual information to improve query relevance and reduce downstream computation. It then generates multiple pseudo-transcriptions per utterance via perturbation-based decoding and scores each hypothesis using two reference-free signals: speech--text alignment in a shared embedding space and predicted word error rate (WER). A simple percentile-based selection rule retains reliable pseudo-labels for fine-tuning while discarding noisy utterances. In an in-domain setting, selecting ~1.5k utterances from a 30k pool achieves 10.91% WER, close to 10.45% obtained using 30k supervised labels. In a cross-domain setting with a mismatched candidate pool, consistency-filtered subsets avoid the degradation caused by unfiltered pseudo-labels under strong accent shift, and matched-hour experiments on a stronger ASR backbone further confirm gains over random sampling and recent selection baselines.

</details>


### [2] [LLM-Powered Automatic Translation and Urgency in Crisis Scenarios](https://arxiv.org/abs/2602.13452)
*Belu Ticona,Antonis Anastasopoulos*

Main category: cs.CL

TL;DR: LLMs和机器翻译系统在危机领域翻译中表现不佳，特别是在保持紧迫性方面存在显著风险，需要危机感知的评估框架。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型越来越多地被提议用于危机准备和响应，特别是多语言沟通。然而，它们在高压危机环境中的适用性尚未得到充分评估。本研究旨在评估最先进的LLMs和机器翻译系统在危机领域翻译中的表现，重点关注保持紧迫性这一关键属性。

Method: 使用多语言危机数据和新引入的紧迫性标注数据集（涵盖超过32种语言），评估专用翻译模型和LLMs的性能。特别关注翻译如何影响紧迫性感知，以及基于LLM的紧迫性分类如何随提示和输入语言而变化。

Result: 专用翻译模型和LLMs都表现出显著的性能下降和不稳定性。即使语言上充分的翻译也可能扭曲感知的紧迫性，基于LLM的紧迫性分类因提示和输入语言的不同而有很大差异。

Conclusion: 在危机沟通中部署通用语言技术存在重大风险，需要开发危机感知的评估框架来确保这些技术在高压环境中的可靠性和安全性。

Abstract: Large language models (LLMs) are increasingly proposed for crisis preparedness and response, particularly for multilingual communication. However, their suitability for high-stakes crisis contexts remains insufficiently evaluated. This work examines the performance of state-of-the-art LLMs and machine translation systems in crisis-domain translation, with a focus on preserving urgency, which is a critical property for effective crisis communication and triaging. Using multilingual crisis data and a newly introduced urgency-annotated dataset covering over 32 languages, we show that both dedicated translation models and LLMs exhibit substantial performance degradation and instability. Crucially, even linguistically adequate translations can distort perceived urgency, and LLM-based urgency classifications vary widely depending on the language of the prompt and input. These findings highlight significant risks in deploying general-purpose language technologies for crisis communication and underscore the need for crisis-aware evaluation frameworks.

</details>


### [3] [Using Machine Learning to Enhance the Detection of Obfuscated Abusive Words in Swahili: A Focus on Child Safety](https://arxiv.org/abs/2602.13455)
*Phyllis Nabangi,Abdul-Jalil Zakaria,Jema David Ndibwile*

Main category: cs.CL

TL;DR: 本研究针对斯瓦希里语中的网络欺凌和虐待性模糊语言检测，使用机器学习模型（SVM、逻辑回归、决策树）并采用SMOTE处理数据不平衡，发现模型在高维文本数据中表现良好，但数据集小且不平衡限制了结果的普适性。


<details>
  <summary>Details</summary>
Motivation: 数字技术的兴起显著增加了网络欺凌和在线虐待的潜在风险，尤其是在儿童群体中，需要加强检测和预防措施。斯瓦希里语作为非洲最广泛使用的语言（超过1亿使用者），属于低资源语言，缺乏足够的语言资源和技术支持，这为检测虐待性模糊语言带来了独特挑战。

Method: 采用机器学习模型包括支持向量机（SVM）、逻辑回归和决策树，通过严格的参数调优进行优化。使用合成少数类过采样技术（SMOTE）处理数据不平衡问题。对精确率、召回率和F1分数进行全面分析，评估各模型在检测模糊语言方面的性能。

Result: 研究发现这些模型在高维文本数据中表现良好，但数据集的规模小且不平衡限制了研究结果的普适性。通过精确率、召回率和F1分数的分析，揭示了各模型在检测模糊语言方面的细微性能差异。

Conclusion: 本研究为保障儿童更安全的在线环境做出了贡献，建议扩展数据集并采用更先进的机器学习技术来提高网络欺凌检测系统的有效性。未来工作将集中在增强数据鲁棒性、探索迁移学习以及整合多模态数据，以创建更全面和具有文化敏感性的检测机制。

Abstract: The rise of digital technology has dramatically increased the potential for cyberbullying and online abuse, necessitating enhanced measures for detection and prevention, especially among children. This study focuses on detecting abusive obfuscated language in Swahili, a low-resource language that poses unique challenges due to its limited linguistic resources and technological support. Swahili is chosen due to its popularity and being the most widely spoken language in Africa, with over 16 million native speakers and upwards of 100 million speakers in total, spanning regions in East Africa and some parts of the Middle East.
  We employed machine learning models including Support Vector Machines (SVM), Logistic Regression, and Decision Trees, optimized through rigorous parameter tuning and techniques like Synthetic Minority Over-sampling Technique (SMOTE) to handle data imbalance. Our analysis revealed that, while these models perform well in high-dimensional textual data, our dataset's small size and imbalance limit our findings' generalizability. Precision, recall, and F1 scores were thoroughly analyzed, highlighting the nuanced performance of each model in detecting obfuscated language.
  This research contributes to the broader discourse on ensuring safer online environments for children, advocating for expanded datasets and advanced machine-learning techniques to improve the effectiveness of cyberbullying detection systems. Future work will focus on enhancing data robustness, exploring transfer learning, and integrating multimodal data to create more comprehensive and culturally sensitive detection mechanisms.

</details>


### [4] [Language Model Memory and Memory Models for Language](https://arxiv.org/abs/2602.13466)
*Benjamin L. Badger*

Main category: cs.CL

TL;DR: 语言模型嵌入包含的输入信息很少，而自编码器嵌入能近乎完美记忆；提出结合因果和信息保留目标的记忆模型架构


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在隐藏层向量嵌入中存储输入信息的能力（类似"记忆"）被广泛使用但缺乏系统表征。研究者发现语言模型嵌入通常包含很少的输入信息，而自编码器嵌入能形成近乎完美的记忆。用记忆嵌入替代token序列能带来显著的计算效率提升，这促使研究者提出新的模型架构。

Method: 提出并行化的编码器-解码器记忆模型架构。通过结合因果训练和信息保留目标函数，使模型学习形成和解码信息丰富的记忆。采用冻结高保真编码器后，通过课程学习方法训练解码器：先学习处理记忆，再学习预测下一个token。

Result: 研究发现语言模型嵌入通常包含相对较少的输入信息，无论训练时的数据和计算规模如何。相比之下，为输入重建训练的自编码器嵌入能够实现近乎完美的记忆形成。结合因果和信息保留目标的模型能够学习形成和解码信息丰富的记忆。

Conclusion: 仅使用下一个token预测训练不适合准确的记忆形成，因为该目标本身是不可逆的。对于不暴露全部输入的模型，需要结合使用因果和信息保留目标函数。提出的架构和方法能够实现高效的信息存储和访问。

Abstract: The ability of machine learning models to store input information in hidden layer vector embeddings, analogous to the concept of `memory', is widely employed but not well characterized. We find that language model embeddings typically contain relatively little input information regardless of data and compute scale during training. In contrast, embeddings from autoencoders trained for input regeneration are capable of nearly perfect memory formation. The substitution of memory embeddings for token sequences leads to substantial computational efficiencies, motivating the introduction of a parallelizable encoder-decoder memory model architecture. Upon causal training these models contain information-poor embeddings incapable of arbitrary information access, but by combining causal and information retention objective functions they learn to form and decode information-rich memories. Training can be further streamlined by freezing a high fidelity encoder followed by a curriculum training approach where decoders first learn to process memories and then learn to additionally predict next tokens. We introduce the perspective that next token prediction training alone is poorly suited for accurate memory formation as the objective itself is non-invertible, motivating the use of combined objective functions for models where the entire input is not exposed.

</details>


### [5] [From Perceptions To Evidence: Detecting AI-Generated Content In Turkish News Media With A Fine-Tuned Bert Classifier](https://arxiv.org/abs/2602.13504)
*Ozancan Ozdemir*

Main category: cs.CL

TL;DR: 本研究首次对土耳其新闻媒体中的AI生成内容进行实证量化分析，通过微调土耳其语BERT模型检测AI改写内容，发现约2.5%的新闻内容被LLM改写或修订。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在新闻编辑室工作流程中的快速集成引发了关于在线媒体中AI生成内容普遍性的紧迫问题。虽然计算研究已开始量化英语媒体中的这一现象，但对土耳其新闻媒体尚无实证调查，现有研究仅限于对记者的定性访谈或假新闻检测。

Method: 通过微调土耳其特定的BERT模型（dbmdz/bert-base-turkish-cased）对来自三家具有不同编辑导向的主要土耳其媒体的3,600篇标记文章进行二元分类，检测AI改写内容。

Result: 模型在保留测试集上获得0.9708的F1分数，两类别的精确率和召回率对称。对2023-2026年间3,500多篇未见文章的分析显示，平均预测置信度超过0.96，估计约2.5%的新闻内容被LLM改写或修订。

Conclusion: 这是首个超越记者自我报告感知，对土耳其新闻媒体中AI使用进行实证、数据驱动测量的研究，为理解非英语媒体中的AI内容生成提供了重要基准。

Abstract: The rapid integration of large language models into newsroom workflows has raised urgent questions about the prevalence of AI-generated content in online media. While computational studies have begun to quantify this phenomenon in English-language outlets, no empirical investigation exists for Turkish news media, where existing research remains limited to qualitative interviews with journalists or fake news detection. This study addresses that gap by fine-tuning a Turkish-specific BERT model (dbmdz/bert-base-turkish-cased) on a labeled dataset of 3,600 articles from three major Turkish outlets with distinct editorial orientations for binary classification of AI-rewritten content. The model achieves 0.9708 F1 score on the held-out test set with symmetric precision and recall across both classes. Subsequent deployment on over 3,500 unseen articles spanning between 2023 and 2026 reveals consistent cross-source and temporally stable classification patterns, with mean prediction confidence exceeding 0.96 and an estimated 2.5 percentage of examined news content rewritten or revised by LLMs on average. To the best of our knowledge, this is the first study to move beyond self-reported journalist perceptions toward empirical, data-driven measurement of AI usage in Turkish news media.

</details>


### [6] [Think Deep, Not Just Long: Measuring LLM Reasoning Effort via Deep-Thinking Tokens](https://arxiv.org/abs/2602.13517)
*Wei-Lin Chen,Liqian Peng,Tian Tan,Chao Zhao,Blake JianHang Chen,Ziqian Lin,Alec Go,Yu Meng*

Main category: cs.CL

TL;DR: 该论文提出了一种量化推理努力的新方法，通过识别"深度思考token"来衡量模型推理质量，并基于此开发了Think@n策略，能在减少推理成本的同时保持或超越自一致性方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究发现，LLMs的生成长度（token数量）并不能可靠地反映推理质量，有时增加长度反而会导致"过度思考"和性能下降。需要一种更准确的指标来量化推理过程中的实际努力。

Method: 提出"深度思考token"的概念，即内部预测在深层网络层中经历显著修正后才收敛的token。计算深度思考比例（deep-thinking ratio），并基于此开发Think@n策略，该策略优先处理高深度思考比例的样本，并能在生成短前缀时就拒绝无希望的生成。

Result: 在四个具有挑战性的数学和科学基准（AIME 24/25, HMMT 25, GPQA-diamond）和多种推理模型（GPT-OSS, DeepSeek-R1, Qwen3）上，深度思考比例与准确率呈现稳健且一致的正相关，显著优于基于长度和置信度的基线方法。

Conclusion: 深度思考比例是衡量LLMs推理质量的可靠指标，基于此的Think@n策略能够有效减少推理成本，同时保持或超越传统自一致性方法的性能，为测试时计算扩展提供了更高效的方法。

Abstract: Large language models (LLMs) have demonstrated impressive reasoning capabilities by scaling test-time compute via long Chain-of-Thought (CoT). However, recent findings suggest that raw token counts are unreliable proxies for reasoning quality: increased generation length does not consistently correlate with accuracy and may instead signal "overthinking," leading to performance degradation. In this work, we quantify inference-time effort by identifying deep-thinking tokens -- tokens where internal predictions undergo significant revisions in deeper model layers prior to convergence. Across four challenging mathematical and scientific benchmarks (AIME 24/25, HMMT 25, and GPQA-diamond) and a diverse set of reasoning-focused models (GPT-OSS, DeepSeek-R1, and Qwen3), we show that deep-thinking ratio (the proportion of deep-thinking tokens in a generated sequence) exhibits a robust and consistently positive correlation with accuracy, substantially outperforming both length-based and confidence-based baselines. Leveraging this insight, we introduce Think@n, a test-time scaling strategy that prioritizes samples with high deep-thinking ratios. We demonstrate that Think@n matches or exceeds standard self-consistency performance while significantly reducing inference costs by enabling the early rejection of unpromising generations based on short prefixes.

</details>


### [7] [On Calibration of Large Language Models: From Response To Capability](https://arxiv.org/abs/2602.13540)
*Sin-Han Yang,Cheng-Kuang Wu,Chieh-Yen Lin,Yun-Nung Chen,Hung-yi Lee,Shao-Hua Sun*

Main category: cs.CL

TL;DR: 论文提出能力校准概念，针对LLM在查询上的期望准确率进行置信度估计，而非传统响应级校准，以更好匹配实际应用需求。


<details>
  <summary>Details</summary>
Motivation: 现有LLM校准主要关注响应级置信度（单个生成输出的正确性），但这与许多实际场景不匹配，因为实际问题关注的是模型解决查询的整体可能性。这种不匹配源于现代LLM解码的随机性，单个响应的正确性无法反映底层模型能力。

Method: 引入能力校准概念，针对模型在查询上的期望准确率进行置信度估计。正式区分能力校准与响应校准，建立实证评估框架，研究一系列置信度估计方法。

Result: 能力校准置信度在pass@k预测和推理预算分配方面表现更好，证明能力校准与响应校准在理论和实证上都存在差异。

Conclusion: 能力校准为LLM置信度估计提供了更符合实际应用需求的基础框架，具有广泛的应用潜力。

Abstract: Large language models (LLMs) are widely deployed as general-purpose problem solvers, making accurate confidence estimation critical for reliable use. Prior work on LLM calibration largely focuses on response-level confidence, which estimates the correctness of a single generated output. However, this formulation is misaligned with many practical settings where the central question is how likely a model is to solve a query overall. We show that this mismatch results from the stochastic nature of modern LLM decoding, under which single-response correctness fails to reflect underlying model capability. To address this issue, we introduce capability calibration, which targets the model's expected accuracy on a query. We formally distinguish capability calibration from response calibration and show that the two differ both theoretically and empirically. We establish an empirical evaluation setup and study a range of confidence estimation methods. Our results demonstrate that capability-calibrated confidence improves pass@$k$ prediction and inference budget allocation, establishing a foundation with potential for diverse applications.

</details>


### [8] [Small Reward Models via Backward Inference](https://arxiv.org/abs/2602.13551)
*Yike Wang,Faeze Brahman,Shangbin Feng,Teng Xiao,Hannaneh Hajishirzi,Yulia Tsvetkov*

Main category: cs.CL

TL;DR: FLIP是一种新的奖励建模方法，通过反向推理从响应推断指令，利用推断指令与原始指令的相似度作为奖励信号，无需参考响应或评分标准。


<details>
  <summary>Details</summary>
Motivation: 当前主流的LLM-as-a-Judge方法依赖大模型的强推理能力，而其他方法需要参考响应或明确评分标准，限制了灵活性和可访问性。需要一种无需参考响应和评分标准的奖励建模方法。

Method: FLIP通过反向推理重构奖励建模：从给定响应推断最可能产生该响应的指令，然后使用推断指令与原始指令的相似度作为奖励信号。该方法在四个领域使用13个小语言模型进行评估。

Result: FLIP在评估中平均比LLM-as-a-Judge基线高出79.6%。在并行采样和GRPO训练的外在评估中显著提升下游性能，特别对长输出有效且对奖励攻击具有鲁棒性。

Conclusion: FLIP通过利用验证-生成差距，在判断方法失败的降尺度场景中实现了可靠的奖励建模，为无需参考响应和评分标准的奖励建模提供了有效解决方案。

Abstract: Reward models (RMs) play a central role throughout the language model (LM) pipeline, particularly in non-verifiable domains. However, the dominant LLM-as-a-Judge paradigm relies on the strong reasoning capabilities of large models, while alternative approaches require reference responses or explicit rubrics, limiting flexibility and broader accessibility. In this work, we propose FLIP (FLipped Inference for Prompt reconstruction), a reference-free and rubric-free reward modeling approach that reformulates reward modeling through backward inference: inferring the instruction that would most plausibly produce a given response. The similarity between the inferred and the original instructions is then used as the reward signal. Evaluations across four domains using 13 small language models show that FLIP outperforms LLM-as-a-Judge baselines by an average of 79.6%. Moreover, FLIP substantially improves downstream performance in extrinsic evaluations under test-time scaling via parallel sampling and GRPO training. We further find that FLIP is particularly effective for longer outputs and robust to common forms of reward hacking. By explicitly exploiting the validation-generation gap, FLIP enables reliable reward modeling in downscaled regimes where judgment methods fail. Code available at https://github.com/yikee/FLIP.

</details>


### [9] [DistillLens: Symmetric Knowledge Distillation Through Logit Lens](https://arxiv.org/abs/2602.13567)
*Manish Dhakal,Uthman Jinadu,Anjila Budathoki,Rajshekhar Sunderraman,Yi Ding*

Main category: cs.CL

TL;DR: DistillLens框架通过对称对齐师生模型的中间思维过程，改进知识蒸馏效果，优于传统KD和特征迁移方法


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏只优化最终输出，将教师模型的中间层思维过程视为黑箱；现有特征蒸馏方法忽略了最终输出所需的不确定性特征

Method: 通过Logit Lens将中间隐藏状态投影到词汇空间，使用对称散度目标进行结构对齐，施加双向惩罚防止过度自信和不足自信

Result: 在GPT-2和Llama架构上的实验表明，DistillLens在多样化指令跟随基准测试中持续优于标准KD和特征迁移基线

Conclusion: DistillLens通过对称对齐师生模型的思维演化过程，有效提升知识蒸馏效果，保留了最终推理所需的高熵信息通道

Abstract: Standard Knowledge Distillation (KD) compresses Large Language Models (LLMs) by optimizing final outputs, yet it typically treats the teacher's intermediate layer's thought process as a black box. While feature-based distillation attempts to bridge this gap, existing methods (e.g., MSE and asymmetric KL divergence) ignore the rich uncertainty profiles required for the final output. In this paper, we introduce DistillLens, a framework that symmetrically aligns the evolving thought processes of student and teacher models. By projecting intermediate hidden states into the vocabulary space via the Logit Lens, we enforce structural alignment using a symmetric divergence objective. Our analysis proves that this constraint imposes a dual-sided penalty, preventing both overconfidence and underconfidence while preserving the high-entropy information conduits essential for final deduction. Extensive experiments on GPT-2 and Llama architectures demonstrate that DistillLens consistently outperforms standard KD and feature-transfer baselines on diverse instruction-following benchmarks. The code is available at https://github.com/manishdhakal/DistillLens.

</details>


### [10] [LLM-Confidence Reranker: A Training-Free Approach for Enhancing Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2602.13571)
*Zhipeng Song,Xiangyu Kong,Xinrui Bao,Yizhi Zhou,Jiulong Jiao,Sitong Liu,Yuhang Zhou,Heng Qi*

Main category: cs.CL

TL;DR: 提出LLM-Confidence Reranker (LCR)，一种无需训练、即插即用的重排序算法，利用LLM置信度提升RAG系统性能，在BEIR和TREC基准上NDCG@5提升高达20.6%。


<details>
  <summary>Details</summary>
Motivation: 现有重排序器需要专门训练、计算成本高，且未能充分利用LLM的语义能力和内在置信度信号，导致知识密集型任务中的幻觉问题。

Method: LCR采用两阶段流程：1) 通过多项采样和聚类进行置信度评估，计算最大语义聚类比例(MSCP)；2) 基于查询和文档置信度阈值进行分箱和多级排序，优先相关文档同时保持高置信度查询的原始排序。

Result: 在BEIR和TREC基准上，使用BM25和Contriever检索器，仅用7-9B参数的预训练LLM，LCR将NDCG@5提升高达20.6%，且不降低性能。消融研究验证了LLM置信度与文档相关性正相关的假设。

Conclusion: LCR提供计算效率、并行可扩展性和广泛兼容性，有效缓解医疗诊断等应用中的幻觉问题，为RAG系统提供了高效的重排序解决方案。

Abstract: Large language models (LLMs) have revolutionized natural language processing, yet hallucinations in knowledge-intensive tasks remain a critical challenge. Retrieval-augmented generation (RAG) addresses this by integrating external knowledge, but its efficacy depends on accurate document retrieval and ranking. Although existing rerankers demonstrate effectiveness, they frequently necessitate specialized training, impose substantial computational expenses, and fail to fully exploit the semantic capabilities of LLMs, particularly their inherent confidence signals. We propose the LLM-Confidence Reranker (LCR), a training-free, plug-and-play algorithm that enhances reranking in RAG systems by leveraging black-box LLM confidence derived from Maximum Semantic Cluster Proportion (MSCP). LCR employs a two-stage process: confidence assessment via multinomial sampling and clustering, followed by binning and multi-level sorting based on query and document confidence thresholds. This approach prioritizes relevant documents while preserving original rankings for high-confidence queries, ensuring robustness. Evaluated on BEIR and TREC benchmarks with BM25 and Contriever retrievers, LCR--using only 7--9B-parameter pre-trained LLMs--consistently improves NDCG@5 by up to 20.6% across pre-trained LLM and fine-tuned Transformer rerankers, without degradation. Ablation studies validate the hypothesis that LLM confidence positively correlates with document relevance, elucidating LCR's mechanism. LCR offers computational efficiency, parallelism for scalability, and broad compatibility, mitigating hallucinations in applications like medical diagnosis.

</details>


### [11] [Elo-Evolve: A Co-evolutionary Framework for Language Model Alignment](https://arxiv.org/abs/2602.13575)
*Jing Zhao,Ting Zhen,Junwei bao,Hongfei Jiang,Yang song*

Main category: cs.CL

TL;DR: Elo-Evolve：一个基于动态多智能体竞争的LLM对齐框架，通过消除Bradley-Terry模型依赖和Elo编排的对手选择，实现了更好的样本效率和噪声鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM对齐方法依赖将大量人类偏好数据压缩为静态绝对奖励函数，导致数据稀缺、噪声敏感和训练不稳定。需要更有效的对齐方法。

Method: 提出Elo-Evolve框架：1) 直接从二元胜负结果学习，消除Bradley-Terry模型依赖；2) 实现Elo编排的对手选择，通过温度控制采样提供自动课程学习。基于PAC学习理论证明成对比较的优越性。

Result: 实验训练Qwen2.5-7B模型，结果显示性能层次：基于点的方法 < 静态成对训练 < Elo-Evolve，在Alpaca Eval 2.0和MT-Bench上验证了成对比较和动态对手选择的渐进优势，实现4.5倍噪声降低。

Conclusion: Elo-Evolve通过将对齐重新定义为动态多智能体竞争，提供了更有效、更稳定的LLM对齐方法，在样本复杂度和噪声鲁棒性方面显著优于传统方法。

Abstract: Current alignment methods for Large Language Models (LLMs) rely on compressing vast amounts of human preference data into static, absolute reward functions, leading to data scarcity, noise sensitivity, and training instability. We introduce Elo-Evolve, a co-evolutionary framework that redefines alignment as dynamic multi-agent competition within an adaptive opponent pool. Our approach makes two key innovations: (1) eliminating Bradley-Terry model dependencies by learning directly from binary win/loss outcomes in pairwise competitions, and (2) implementing Elo-orchestrated opponent selection that provides automatic curriculum learning through temperature-controlled sampling. We ground our approach in PAC learning theory, demonstrating that pairwise comparison achieves superior sample complexity and empirically validate a 4.5x noise reduction compared to absolute scoring approaches. Experimentally, we train a Qwen2.5-7B model using our framework with opponents including Qwen2.5-14B, Qwen2.5-32B, and Qwen3-8B models. Results demonstrate a clear performance hierarchy: point-based methods < static pairwise training < Elo-Evolve across Alpaca Eval 2.0 and MT-Bench, validating the progressive benefits of pairwise comparison and dynamic opponent selection for LLM alignment.

</details>


### [12] [Metaphors' journeys across time and genre: tracking the evolution of literary metaphors with temporal embeddings](https://arxiv.org/abs/2602.13701)
*Veronica Mangiaterra,Chiara Barattieri di San Pietro,Paolo Canal,Valentina Bambini*

Main category: cs.CL

TL;DR: 使用历时分布语义学工具分析19世纪文学隐喻随时间变化的处理成本，发现整体稳定但受文体影响：现代文学中隐喻更难理解，而现代非文学语言（如网络）中隐喻更易理解。


<details>
  <summary>Details</summary>
Motivation: 文学隐喻作为文学语言的特色，实验研究较少，且以往研究忽略了时间维度。本研究旨在探究文学隐喻的处理成本是否随时间变化，以及文体如何影响这种变化。

Method: 使用历时分布语义学方法，在19世纪和21世纪的意大利文学与非文学语料库（共1.24亿词）上训练词向量，计算515个19世纪文学隐喻中主题与载体的语义相似度，以此作为隐喻处理需求的代理指标。

Result: 整体上语义相似度（隐喻处理需求）随时间保持稳定。但文体起关键作用：现代文学语境中隐喻更难处理（主题-载体相似度更低），而现代非文学语言（如网络）中隐喻更易处理（相似度更高）。这一模式还受隐喻个体词语义特征（如向量一致性和语义邻域密度）的影响。

Conclusion: 研究结果与意大利语更广泛的语言变化一致：现代文学的文体简化可能增加了隐喻处理需求，而网络语言的高创造性似乎使隐喻更易理解。这为理解文学隐喻的历时处理提供了新视角。

Abstract: Metaphors are a distinctive feature of literary language, yet they remain less studied experimentally than everyday metaphors. Moreover, previous psycholinguistic and computational approaches overlooked the temporal dimension, although many literary metaphors were coined centuries apart from contemporary readers. This study innovatively applies tools from diachronic distributional semantics to assess whether the processing costs of literary metaphors varied over time and genre. Specifically, we trained word embeddings on literary and nonliterary Italian corpora from the 19th and 21st centuries, for a total of 124 million tokens, and modeled changes in the semantic similarity between topics and vehicles of 515 19th-century literary metaphors, taking this measure as a proxy of metaphor processing demands. Overall, semantic similarity, and hence metaphor processing demands, remained stable over time. However, genre played a key role: metaphors appeared more difficult (i.e., lower topic-vehicle similarity) in modern literary contexts than in 19th-century literature, but easier (i.e., higher topic-vehicle similarity) in today's nonliterary language (e.g., the Web) than in 19th-century nonliterary texts. This pattern was further shaped by semantic features of metaphors' individual terms, such as vector coherence and semantic neighborhood density. Collectively, these findings align with broader linguistic changes in Italian, such as the stylistic simplification of modern literature, which may have increased metaphor processing demands, and the high creativity of the Web's language, which seems to render metaphor more accessible.

</details>


### [13] [On Theoretically-Driven LLM Agents for Multi-Dimensional Discourse Analysis](https://arxiv.org/abs/2602.13713)
*Maciej Uberna,Michał Wawer,Jarosław A. Chudziak,Marcin Koszowy*

Main category: cs.CL

TL;DR: 本文提出一个多智能体框架，通过融入论证理论来识别话语中改写（reformulation）的策略功能，相比零样本基线在D-I-S-G-O分类任务上实现近30%的宏观F1分数提升。


<details>
  <summary>Details</summary>
Motivation: 识别话语中改写的策略使用是计算论证的关键挑战。虽然LLMs能检测表面相似性，但难以捕捉改写的语用功能（如修辞作用）。需要超越单纯改写检测，实现功能感知的论证话语分析。

Method: 1. 建立包含四种改写功能（弱化、强化、具体化、泛化）和"其他"类别的标注政治辩论数据集；2. 设计对比多智能体框架：一个通过RAG融入论证理论，另一个为零样本基线；3. 评估两个系统在D-I-S-G-O分类任务上的表现。

Result: RAG增强智能体在所有类别上显著优于基线，尤其在强化和泛化检测上优势明显，整体宏观F1分数提升近30%。证明理论基础对于识别论证话语中的修辞策略至关重要。

Conclusion: 理论基础不仅是有益的，而且是实现超越单纯改写检测、迈向功能感知论证话语分析的必要条件。该多智能体架构为开发可扩展的理论指导计算工具迈出一步，能够识别当代话语中的修辞策略。

Abstract: Identifying the strategic uses of reformulation in discourse remains a key challenge for computational argumentation. While LLMs can detect surface-level similarity, they often fail to capture the pragmatic functions of rephrasing, such as its role within rhetorical discourse. This paper presents a comparative multi-agent framework designed to quantify the benefits of incorporating explicit theoretical knowledge for this task. We utilise an dataset of annotated political debates to establish a new standard encompassing four distinct rephrase functions: Deintensification, Intensification, Specification, Generalisation, and Other, which covers all remaining types (D-I-S-G-O). We then evaluate two parallel LLM-based agent systems: one enhanced by argumentation theory via Retrieval-Augmented Generation (RAG), and an identical zero-shot baseline. The results reveal a clear performance gap: the RAG-enhanced agents substantially outperform the baseline across the board, with particularly strong advantages in detecting Intensification and Generalisation context, yielding an overall Macro F1-score improvement of nearly 30\%. Our findings provide evidence that theoretical grounding is not only beneficial but essential for advancing beyond mere paraphrase detection towards function-aware analysis of argumentative discourse. This comparative multi-agent architecture represents a step towards scalable, theoretically informed computational tools capable of identifying rhetorical strategies in contemporary discourse.

</details>


### [14] [RMPL: Relation-aware Multi-task Progressive Learning with Stage-wise Training for Multimedia Event Extraction](https://arxiv.org/abs/2602.13748)
*Yongkang Jin,Jianwen Luo,Jingjing Wang,Jianmin Yao,Yu Hong*

Main category: cs.CL

TL;DR: RMPL：一种面向低资源多媒体事件抽取的关系感知多任务渐进学习框架，通过整合单模态事件抽取和多媒体关系抽取的异质监督，在M2E2基准上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 多媒体事件抽取（MEE）需要跨文本和图像模态理解事件语义，但面临标注数据稀缺的挑战。现有方法依赖跨模态对齐或VLM提示，缺乏结构化事件表示学习，导致多模态场景下论元定位能力弱。

Method: 提出RMPL框架：1）通过统一模式学习跨模态共享的事件中心表示；2）分阶段训练，整合单模态事件抽取和多媒体关系抽取的异质监督；3）使用混合文本和视觉数据进行事件提及识别和论元角色抽取的微调。

Result: 在M2E2基准上使用多种视觉语言模型进行实验，在不同模态设置下均取得一致性能提升。

Conclusion: RMPL通过关系感知的多任务渐进学习有效解决了低资源多媒体事件抽取问题，显著提升了跨模态事件语义理解和论元定位能力。

Abstract: Multimedia Event Extraction (MEE) aims to identify events and their arguments from documents that contain both text and images. It requires grounding event semantics across different modalities. Progress in MEE is limited by the lack of annotated training data. M2E2 is the only established benchmark, but it provides annotations only for evaluation. This makes direct supervised training impractical. Existing methods mainly rely on cross-modal alignment or inference-time prompting with Vision--Language Models (VLMs). These approaches do not explicitly learn structured event representations and often produce weak argument grounding in multimodal settings. To address these limitations, we propose RMPL, a Relation-aware Multi-task Progressive Learning framework for MEE under low-resource conditions. RMPL incorporates heterogeneous supervision from unimodal event extraction and multimedia relation extraction with stage-wise training. The model is first trained with a unified schema to learn shared event-centric representations across modalities. It is then fine-tuned for event mention identification and argument role extraction using mixed textual and visual data. Experiments on the M2E2 benchmark with multiple VLMs show consistent improvements across different modality settings.

</details>


### [15] [How Do Lexical Senses Correspond Between Spoken German and German Sign Language?](https://arxiv.org/abs/2602.13790)
*Melis Çelikkol,Wei Zhao*

Main category: cs.CL

TL;DR: 该研究分析了德语与德国手语之间的词汇-手势映射关系，创建了首个跨模态语义对应标注数据集，并评估了计算方法的性能。


<details>
  <summary>Details</summary>
Motivation: 传统双语词典中，多义词和同音词在不同语境下对应不同手势的情况经常被忽视。需要基于使用的方法来识别词典中缺失的新映射关系，以丰富词典资源。

Method: 研究分析了德语和德国手语，手动标注了1,404个词汇使用-手势ID映射，来自德语词汇使用图的32个词和德国手语数字词典的49个手势。识别了三种对应类型（一对多、多对一、一对一）和无匹配情况。评估了精确匹配和基于SBERT嵌入的语义相似度两种计算方法。

Result: 语义相似度方法整体表现显著优于精确匹配（88.52% vs. 71.31%），特别在一对多类型上提升了52.1个百分点。研究建立了首个跨模态语义对应标注数据集，揭示了哪些对应模式可以通过计算方法识别。

Conclusion: 基于语义相似度的计算方法能有效识别词汇-手势映射关系，特别适用于一对多等复杂对应模式。该研究为手语词典编纂提供了新的数据资源和方法论支持。

Abstract: Sign language lexicographers construct bilingual dictionaries by establishing word-to-sign mappings, where polysemous and homonymous words corresponding to different signs across contexts are often underrepresented. A usage-based approach examining how word senses map to signs can identify such novel mappings absent from current dictionaries, enriching lexicographic resources. We address this by analyzing German and German Sign Language (Deutsche Gebärdensprache, DGS), manually annotating 1,404 word use-to-sign ID mappings derived from 32 words from the German Word Usage Graph (D-WUG) and 49 signs from the Digital Dictionary of German Sign Language (DW-DGS). We identify three correspondence types: Type 1 (one-to-many), Type 2 (many-to-one), and Type 3 (one-to-one), plus No Match cases. We evaluate computational methods: Exact Match (EM) and Semantic Similarity (SS) using SBERT embeddings. SS substantially outperforms EM overall 88.52% vs. 71.31%), with dramatic gains for Type 1 (+52.1 pp). Our work establishes the first annotated dataset for cross-modal sense correspondence and reveals which correspondence patterns are computationally identifiable. Our code and dataset are made publicly available.

</details>


### [16] [OMGs: A multi-agent system supporting MDT decision-making across the ovarian tumour care continuum](https://arxiv.org/abs/2602.13793)
*Yangyang Zhang,Zilong Wang,Jianbo Xu,Yongqi Chen,Chu Han,Zhihao Zhang,Shuai Liu,Hui Li,Huiping Zhang,Ziqi Liu,Jiaxin Chen,Jun Zhu,Zheng Feng,Hao Wen,Xingzhu Ju,Yanping Zhong,Yunqiu Zhang,Jie Duan,Jun Li,Dongsheng Li,Weijie Wang,Haiyan Zhu,Wei Jiang,Xiaohua Wu,Shuo Wang,Haiming Li,Qinhao Guo*

Main category: cs.CL

TL;DR: OMGs是一个多智能体AI框架，通过领域特定智能体协作整合多学科证据，生成类似多学科肿瘤委员会(MDT)的卵巢肿瘤治疗建议，在资源有限地区扩展专家共识的可及性。


<details>
  <summary>Details</summary>
Motivation: 全球大多数卵巢肿瘤患者缺乏及时的多学科专家共识，特别是在资源有限中心，MDT资源稀缺或不可用，需要解决方案来扩大专业肿瘤学专业知识的可及性。

Method: 开发了OMGs多智能体AI框架，其中领域特定智能体协作整合多学科证据，生成具有透明推理的MDT式建议。使用SPEAR（安全性、个性化、证据、可操作性、稳健性）框架系统评估MDT建议质量。

Result: 在多中心重新评估中，OMGs表现与专家MDT共识相当（4.45±0.30 vs 4.53±0.23），证据得分更高（4.57 vs 3.92）。前瞻性多中心评估（59例患者）显示与常规MDT决策高度一致。在人机配对研究中，OMGs显著提升了临床医生建议的证据性和稳健性。

Conclusion: 多智能体审议系统可以达到与专家MDT共识相当的性能，有潜力在资源有限环境中扩大专业肿瘤学专业知识的可及性，特别是在多学科专业知识不可用时最需要改进的维度上。

Abstract: Ovarian tumour management has increasingly relied on multidisciplinary tumour board (MDT) deliberation to address treatment complexity and disease heterogeneity. However, most patients worldwide lack access to timely expert consensus, particularly in resource-constrained centres where MDT resources are scarce or unavailable. Here we present OMGs (Ovarian tumour Multidisciplinary intelligent aGent System), a multi-agent AI framework where domain-specific agents deliberate collaboratively to integrate multidisciplinary evidence and generate MDT-style recommendations with transparent rationales. To systematically evaluate MDT recommendation quality, we developed SPEAR (Safety, Personalization, Evidence, Actionability, Robustness) and validated OMGs across diverse clinical scenarios spanning the care continuum. In multicentre re-evaluation, OMGs achieved performance comparable to expert MDT consensus ($4.45 \pm 0.30$ versus $4.53 \pm 0.23$), with higher Evidence scores (4.57 versus 3.92). In prospective multicentre evaluation (59 patients), OMGs demonstrated high concordance with routine MDT decisions. Critically, in paired human-AI studies, OMGs most substantially enhanced clinicians' recommendations in Evidence and Robustness, the dimensions most compromised when multidisciplinary expertise is unavailable. These findings suggest that multi-agent deliberative systems can achieve performance comparable to expert MDT consensus, with potential to expand access to specialized oncology expertise in resource-limited settings.

</details>


### [17] [The acquisition of English irregular inflections by Yemeni L1 Arabic learners: A Universal Grammar approach](https://arxiv.org/abs/2602.13816)
*Muneef Y. Alsawsh,Mohammed Q. Shormani*

Main category: cs.CL

TL;DR: 也门英语学习者习得英语不规则屈折变化的研究，采用普遍语法框架，发现L1迁移在初期主导，后期学习者对UG属性更敏感，不规则形态错误源于语际和语内因素，适当输入和教学对成人二语习得至关重要。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探讨也门英语学习者如何习得英语不规则屈折变化，采用普遍语法(UG)框架，特别关注特征重组假说(FRH)，以理解L1迁移和L2发展因素在二语习得中的作用。

Method: 采用普遍语法方法，基于特征重组假说，分析两个发展阶段的学习者错误。使用统计分析方法（包括单因素方差分析）来检验学习者从不规则屈折变化的阶段1到阶段2的显著进步。

Result: 阶段1数据显示L1迁移主导影响，特别是在音系和结构不匹配方面；阶段2显示学习者对UG属性敏感性增强，形态重组更接近目标语。不规则屈折形态错误源于语际和语内因素，L2规则过度概括是常见发展策略。统计显示从阶段1到阶段2不规则屈折正确率显著提高，但辅音变化、零语素和-a复数屈折仍存在持续困难。

Conclusion: 虽然L1迁移和L2发展因素影响习得初期阶段，但适当的语言输入和教学对促进成人二语学习者的UG驱动特征重组至关重要。学习者持续接触UG，但有限暴露、无效输入建模和教学不足限制了完全UG访问。

Abstract: This study examines the acquisition of English irregular inflections by Yemeni learners of English as a second language (L2), utilizing a Universal Grammar (UG) approach. Within the UG approach, the study considers Feature Reassembly Hypothesis (FRH) (Lardiere, 2008, 2009) part of UG, focusing on the roles of first language (L1) transfer and L2 developmental influence. It analyzes learner errors across two developmental stages. Stage 1 data reveal a dominant influence of L1 transfer, particularly in phonological and structural mismatches, while stage 2 data demonstrate increased learner sensitivity to UG properties and morphological reconfiguration toward the target language. Findings reveal that errors in irregular inflectional morphology are attributed to both interlingual and intralingual sources, with overgeneralization of L2 rules as a common developmental strategy. Statistical analysis, including a one-way ANOVA, indicates significant improvement in the production of well-formed irregular inflections from stage 1 to stage 2, underscoring learners' continued access to UG. However, persistent difficulties with consonant change, zero-morpheme, and -a plural inflections suggest that limited exposure, ineffective input modeling, and insufficient instructional quality constrain full UG access. The study concludes that while L1 transfer and L2 developmental factors influence initial stages of acquisition, appropriate linguistic input and instruction are critical for facilitating UG-driven feature reassembly in adult L2 learners.

</details>


### [18] [Beyond Words: Evaluating and Bridging Epistemic Divergence in User-Agent Interaction via Theory of Mind](https://arxiv.org/abs/2602.13832)
*Minyuan Ruan,Ziyue Wang,Kaiming Liu,Yunghwei Lai,Peng Li,Yang Liu*

Main category: cs.CL

TL;DR: 该论文提出将LLM的心智理论（ToM）形式化为认知差异检测与解决机制，创建了评估基准并发现现有模型在识别认知差距方面存在显著局限，通过轨迹数据集和强化学习训练提升了模型的心智推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在用户意图和指令表达不精确时难以理解真实需求，导致用户主观信念与环境状态之间存在认知差异。现有ToM评估主要关注孤立的信念推断，忽视了其在真实交互中的功能性价值。

Method: 1）将LLM的ToM形式化为认知差异检测与解决机制；2）提出评估基准来评估模型在实践中如何协调用户信念和画像；3）构建基于轨迹的ToM数据集，将信念追踪与任务相关状态推断联系起来；4）通过强化学习训练模型。

Result: 对11个领先模型的评估显示，它们在识别阻碍任务成功的潜在认知差距方面存在显著局限。使用轨迹数据集通过强化学习训练的模型在推理用户心智状态方面表现出一致的改进，并提升了下游任务性能。

Conclusion: ToM应被视为必要的交互层面机制，而非独立的推理技能。该研究强调了ToM在实际交互中的实用价值，通过系统性的评估和训练方法提升了LLM的心智推理能力。

Abstract: Large Language Models (LLMs) have developed rapidly and are widely applied to both general-purpose and professional tasks to assist human users. However, they still struggle to comprehend and respond to the true user needs when intentions and instructions are imprecisely conveyed, leading to a divergence between subjective user believes and true environment states. Resolving this epistemic divergence requires Theory of Mind (ToM), yet existing ToM evaluations for LLMs primarily focus on isolated belief inference, overlooking its functional utility in real-world interaction. To this end, we formalize ToM for LLMs as a mechanism for epistemic divergence detection and resolution, and propose a benchmark, \benchname, to assess how models reconcile user beliefs and profiles in practice. Results across 11 leading models reveal a significant limitation to identify underlying cognitive gaps that impede task success. To bridge this gap, we further curate a trajectory-based ToM dataset linking belief tracking with task-related state inference. The model trained on this data via reinforcement learning shows consistent improvement in reasoning about user mental states, leading to enhanced downstream performance. Our work highlights the practical value of ToM as an essential interaction-level mechanism rather than as a standalone reasoning skill.

</details>


### [19] [Speculative Decoding with a Speculative Vocabulary](https://arxiv.org/abs/2602.13836)
*Miles Williams,Young D. Kwon,Rui Li,Alexandros Kouris,Stylianos I. Venieris*

Main category: cs.CL

TL;DR: 提出SpecVocab方法，通过每步动态选择词汇子集来提升推测解码效率，相比现有方法EAGLE-3能提高接受长度和吞吐量


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法使用小型草稿模型加速推理，但输出嵌入矩阵成为瓶颈。近期工作通过减少词汇表来缓解，但这会导致目标词不在词汇表内时推测效果下降

Method: 提出SpecVocab方法，在解码的每一步动态选择词汇子集，而不是使用固定的缩减词汇表，从而更有效地进行词汇推测

Result: 在多种任务上，SpecVocab相比最先进的推测解码方法EAGLE-3能获得更高的接受长度，吞吐量最高提升8.1%

Conclusion: 词汇推测是比缩减词汇表更有效的替代方案，SpecVocab方法在保持推测有效性的同时显著提升了推理速度

Abstract: Speculative decoding has rapidly emerged as a leading approach for accelerating language model (LM) inference, as it offers substantial speedups while yielding identical outputs. This relies upon a small draft model, tasked with predicting the outputs of the target model. State-of-the-art speculative decoding methods use a draft model consisting of a single decoder layer and output embedding matrix, with the latter dominating drafting time for the latest LMs. Recent work has sought to address this output distribution bottleneck by reducing the vocabulary of the draft model. Although this can improve throughput, it compromises speculation effectiveness when the target token is out-of-vocabulary. In this paper, we argue for vocabulary speculation as an alternative to a reduced vocabulary. We propose SpecVocab, an efficient and effective method that selects a vocabulary subset per decoding step. Across a variety of tasks, we demonstrate that SpecVocab can achieve a higher acceptance length than state-of-the-art speculative decoding approach, EAGLE-3. Notably, this yields up to an 8.1% increase in average throughput over EAGLE-3.

</details>


### [20] [PrivAct: Internalizing Contextual Privacy Preservation via Multi-Agent Preference Training](https://arxiv.org/abs/2602.13840)
*Yuhan Cheng,Hancheng Ye,Hai Helen Li,Jingwei Sun,Yiran Chen*

Main category: cs.CL

TL;DR: PrivAct是一个上下文隐私感知的多智能体学习框架，通过将隐私保护内化到模型生成行为中，实现隐私合规的智能体行动，减少隐私泄露同时保持有用性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型智能体越来越多地部署在涉及敏感、上下文相关信息的个性化任务中，由于上下文隐私的隐含性，智能体行动可能导致隐私侵犯。现有方法依赖外部、推理时干预，这些方法脆弱、场景特定，并可能扩大隐私攻击面。

Method: 提出PrivAct框架，将上下文隐私保护直接内化到模型的生成行为中，通过将隐私偏好嵌入到每个智能体中，增强系统范围的上下文完整性，实现更好的隐私-有用性权衡。

Result: 在多个LLM骨干网络和基准测试上的实验表明，PrivAct在上下文隐私保护方面持续改进，将泄露率降低高达12.32%，同时保持相当的有用性，并在多样化的多智能体拓扑结构中展示零样本泛化和鲁棒性。

Conclusion: PrivAct通过将隐私保护内化到智能体生成行为中，提供了一种更有效、更鲁棒的上下文隐私保护方法，优于依赖外部干预的现有方法。

Abstract: Large language model (LLM) agents are increasingly deployed in personalized tasks involving sensitive, context-dependent information, where privacy violations may arise in agents' action due to the implicitness of contextual privacy. Existing approaches rely on external, inference-time interventions which are brittle, scenario-specific, and may expand the privacy attack surface. We propose PrivAct, a contextual privacy-aware multi-agent learning framework that internalizes contextual privacy preservation directly into models' generation behavior for privacy-compliant agentic actions. By embedding privacy preferences into each agent, PrivAct enhances system-wide contextual integrity while achieving a more favorable privacy-helpfulness tradeoff. Experiments across multiple LLM backbones and benchmarks demonstrate consistent improvements in contextual privacy preservation, reducing leakage rates by up to 12.32% while maintaining comparable helpfulness, as well as zero-shot generalization and robustness across diverse multi-agent topologies. Code is available at https://github.com/chengyh23/PrivAct.

</details>


### [21] [Tutoring Large Language Models to be Domain-adaptive, Precise, and Safe](https://arxiv.org/abs/2602.13860)
*Somnath Banerjee*

Main category: cs.CL

TL;DR: 开发"负责任智能"框架，将大语言模型的生成能力与真实世界部署的严格要求相结合，通过领域适应、伦理严谨性和文化/多语言对齐三个方向，实现上下文感知、安全且尊重文化差异的系统。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型成为人工智能变革性力量，迫切需要超越通用架构，开发具有上下文感知、内在安全性和尊重全球文化细微差别的系统，以满足现实世界部署的严格要求。

Method: 方法轨迹从经典监督适应满足任务特定需求，到解码时对齐确保安全性，最后利用人类反馈和偏好建模实现社会语言敏锐度，涵盖三个相互关联的线程：领域适应、伦理严谨性和文化/多语言对齐。

Result: 提出了一个负责任智能框架，旨在协调大语言模型的强大生成能力与现实世界部署要求，通过系统化方法解决技术精度、对抗性漏洞缓解和全球包容性等问题。

Conclusion: 该研究为开发负责任的大语言模型系统提供了框架，强调需要将技术能力与伦理、文化考量相结合，以创建真正适用于全球部署的AI系统。

Abstract: The overarching research direction of this work is the development of a ''Responsible Intelligence'' framework designed to reconcile the immense generative power of Large Language Models (LLMs) with the stringent requirements of real-world deployment. As these models become a transformative force in artificial intelligence, there is an urgent need to move beyond general-purpose architectures toward systems that are contextually aware, inherently safer, and deeply respectful of global cultural nuances. This research navigates three interconnected threads: domain adaptation to ensure technical precision, ethical rigor to mitigate adversarial vulnerabilities, and cultural/multilingual alignment to promote global inclusivity. The methodological trajectory moves from classical supervised adaptation for task-specific demands to decoding-time alignment for safety, finally leveraging human feedback and preference modeling to achieve sociolinguistic acuity.

</details>


### [22] [Bridging the Multilingual Safety Divide: Efficient, Culturally-Aware Alignment for Global South Languages](https://arxiv.org/abs/2602.13867)
*Somnath Banerjee,Rima Hazra,Animesh Mukherjee*

Main category: cs.CL

TL;DR: 论文指出当前LLM安全评估主要针对英语和高资源语言，但实际部署在Global South时面临低资源语言、代码混合和文化差异的挑战，安全防护在这些情况下会显著减弱，需要建立多语言安全评估体系。


<details>
  <summary>Details</summary>
Motivation: LLM正在全球南方部署，但现有安全管道、基准测试和对齐主要针对英语和高资源语言，隐含假设安全性会跨语言"转移"，但证据表明这种假设不成立，需要关注多语言环境下的安全问题。

Method: 综合近期研究发现：1）安全护栏在低资源语言和代码混合输入时显著减弱；2）文化有害行为在标准毒性评分可接受时仍持续存在；3）英语知识编辑和安全补丁常无法传递到低资源语言。

Result: 现有安全评估体系在Global South的多语言环境中存在严重不足，安全防护无法有效跨语言转移，需要重新设计多语言安全框架。

Conclusion: 提出实践议程：参数高效安全引导、文化基础评估和偏好数据、参与式工作流程，让本地社区定义和减轻危害，使多语言安全成为AI公平性的核心要求而非附加功能。

Abstract: Large language models (LLMs) are being deployed across the Global South, where everyday use involves low-resource languages, code-mixing, and culturally specific norms. Yet safety pipelines, benchmarks, and alignment still largely target English and a handful of high-resource languages, implicitly assuming safety and factuality ''transfer'' across languages. Evidence increasingly shows they do not. We synthesize recent findings indicating that (i) safety guardrails weaken sharply on low-resource and code-mixed inputs, (ii) culturally harmful behavior can persist even when standard toxicity scores look acceptable, and (iii) English-only knowledge edits and safety patches often fail to carry over to low-resource languages. In response, we outline a practical agenda for researchers and students in the Global South: parameter-efficient safety steering, culturally grounded evaluation and preference data, and participatory workflows that empower local communities to define and mitigate harm. Our aim is to make multilingual safety a core requirement-not an add-on-for equitable AI in underrepresented regions.

</details>


### [23] [Does Socialization Emerge in AI Agent Society? A Case Study of Moltbook](https://arxiv.org/abs/2602.14299)
*Ming Li,Xirui Li,Tianyi Zhou*

Main category: cs.CL

TL;DR: 研究首次大规模诊断AI智能体社会动态演化，发现虽然全局语义快速稳定，但个体保持高多样性，缺乏相互影响和共识，表明仅靠规模和互动密度不足以引发社会化。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型智能体在网络环境中日益增多，需要研究AI智能体社会是否经历类似人类社会的趋同动态，探索自主智能体在开放、持续演化的在线社会中的行为模式。

Method: 提出定量诊断框架，测量语义稳定化、词汇更替、个体惯性、影响力持久性和集体共识等指标，对Moltbook平台上的AI智能体社会进行大规模系统分析。

Result: 发现系统处于动态平衡：全局语义平均快速稳定，但个体保持高多样性和持续词汇更替；智能体表现出强个体惯性和对互动伙伴的最小适应响应，缺乏相互影响和共识；影响力短暂，无持久超级节点，社会无法发展稳定的集体影响力锚点。

Conclusion: 仅靠规模和互动密度不足以诱导社会化，为下一代AI智能体社会的设计和分析提供了可操作的原则，强调需要共享社会记忆和相互影响机制。

Abstract: As large language model agents increasingly populate networked environments, a fundamental question arises: do artificial intelligence (AI) agent societies undergo convergence dynamics similar to human social systems? Lately, Moltbook approximates a plausible future scenario in which autonomous agents participate in an open-ended, continuously evolving online society. We present the first large-scale systemic diagnosis of this AI agent society. Beyond static observation, we introduce a quantitative diagnostic framework for dynamic evolution in AI agent societies, measuring semantic stabilization, lexical turnover, individual inertia, influence persistence, and collective consensus. Our analysis reveals a system in dynamic balance in Moltbook: while global semantic averages stabilize rapidly, individual agents retain high diversity and persistent lexical turnover, defying homogenization. However, agents exhibit strong individual inertia and minimal adaptive response to interaction partners, preventing mutual influence and consensus. Consequently, influence remains transient with no persistent supernodes, and the society fails to develop stable collective influence anchors due to the absence of shared social memory. These findings demonstrate that scale and interaction density alone are insufficient to induce socialization, providing actionable design and analysis principles for upcoming next-generation AI agent societies.

</details>


### [24] [ADAB: Arabic Dataset for Automated Politeness Benchmarking -- A Large-Scale Resource for Computational Sociopragmatics](https://arxiv.org/abs/2602.13870)
*Hend Al-Khalifa,Nadia Ghezaiel,Maria Bounnit,Hend Hamed Alhazmi,Noof Abdullah Alfear,Reem Fahad Alqifari,Ameera Masoud Almasoud,Sharefah Ahmed Al-Ghamdi*

Main category: cs.CL

TL;DR: ADAB是一个新的阿拉伯语礼貌检测数据集，包含10,000个来自社交媒体、电子商务和客服等在线平台的样本，涵盖现代标准阿拉伯语和多种方言，标注为礼貌、不礼貌和中性三类。


<details>
  <summary>Details</summary>
Motivation: 随着文化感知自然语言处理系统的重要性日益增长，需要更多捕捉不同语言社会语用现象的资源。尽管阿拉伯语交流中蕴含着丰富复杂的礼貌表达，但阿拉伯语礼貌检测资源仍然研究不足。

Method: 从四个在线平台收集阿拉伯语数据，基于阿拉伯语语言学传统和语用理论进行标注，分为礼貌、不礼貌和中性三类。数据集包含10,000个样本，标注了16个礼貌类别的语言特征，并实现了较高的标注者间一致性（kappa=0.703）。

Result: 建立了ADAB数据集，包含10,000个标注样本，覆盖现代标准阿拉伯语和多种方言（海湾、埃及、黎凡特和马格里布）。对40种模型配置进行了基准测试，包括传统机器学习、基于Transformer的模型和大语言模型。

Conclusion: ADAB数据集填补了阿拉伯语礼貌检测资源的空白，为阿拉伯语NLP的礼貌感知研究提供了支持，有助于开发更文化敏感的阿拉伯语自然语言处理系统。

Abstract: The growing importance of culturally-aware natural language processing systems has led to an increasing demand for resources that capture sociopragmatic phenomena across diverse languages. Nevertheless, Arabic-language resources for politeness detection remain under-explored, despite the rich and complex politeness expressions embedded in Arabic communication. In this paper, we introduce ADAB (Arabic Politeness Dataset), a new annotated Arabic dataset collected from four online platforms, including social media, e-commerce, and customer service domains, covering Modern Standard Arabic and multiple dialects (Gulf, Egyptian, Levantine, and Maghrebi). The dataset was annotated based on Arabic linguistic traditions and pragmatic theory, resulting in three classes: polite, impolite, and neutral. It contains 10,000 samples with linguistic feature annotations across 16 politeness categories and achieves substantial inter-annotator agreement (kappa = 0.703). We benchmark 40 model configurations, including traditional machine learning, transformer-based models, and large language models. The dataset aims to support research on politeness-aware Arabic NLP.

</details>


### [25] [Evaluating Prompt Engineering Techniques for RAG in Small Language Models: A Multi-Hop QA Approach](https://arxiv.org/abs/2602.13890)
*Amir Hossein Mohammadi,Ali Moeinian,Zahra Razavizade,Afsaneh Fatemi,Reza Ramezani*

Main category: cs.CL

TL;DR: 本文通过大规模实证研究，评估了24种不同提示模板在HotpotQA数据集上的表现，发现优化提示模板可为小型语言模型RAG系统带来高达83-84.5%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 虽然检索增强生成(RAG)在大型语言模型中广泛研究，但针对小型语言模型(SLMs)的RAG优化仍存在研究空白，特别是在需要复杂推理的多跳问答任务中。提示模板设计是影响性能的关键但未被充分探索的因素。

Method: 在HotpotQA数据集上进行大规模实证研究，评估24种不同提示模板，包括标准RAG提示、9种文献中的成熟技术和14种新颖混合变体，在Qwen2.5-3B Instruct和Gemma3-4B-It两个小型语言模型上进行测试，基于18720个测试实例进行分析。

Result: 研究发现显著性能提升：Qwen2.5模型提升达83%，Gemma3-4B-It模型提升达84.5%，相比标准RAG提示，两种模型都有高达6%的改进。研究提供了具体分析和可操作建议。

Conclusion: 提示模板设计对小型语言模型RAG系统性能有重大影响，优化提示可显著提升多跳问答任务表现。研究为资源受限环境中部署SLM-based RAG系统提供了有效的提示设计指导。

Abstract: Retrieval Augmented Generation (RAG) is a powerful approach for enhancing the factual grounding of language models by integrating external knowledge. While widely studied for large language models, the optimization of RAG for Small Language Models (SLMs) remains a critical research gap, particularly in complex, multi-hop question-answering tasks that require sophisticated reasoning. In these systems, prompt template design is a crucial yet under-explored factor influencing performance. This paper presents a large-scale empirical study to investigate this factor, evaluating 24 different prompt templates on the HotpotQA dataset. The set includes a standard RAG prompt, nine well-formed techniques from the literature, and 14 novel hybrid variants, all tested on two prominent SLMs: Qwen2.5-3B Instruct and Gemma3-4B-It. Our findings, based on a test set of 18720 instances, reveal significant performance gains of up to 83% on Qwen2.5 and 84.5% on Gemma3-4B-It, yielding an improvement of up to 6% for both models compared to the Standard RAG prompt. This research also offers concrete analysis and actionable recommendations for designing effective and efficient prompts for SLM-based RAG systems, practically for deployment in resource-constrained environments.

</details>


### [26] [Pre-Editorial Normalization for Automatically Transcribed Medieval Manuscripts in Old French and Latin](https://arxiv.org/abs/2602.13905)
*Thibault Clérice,Rachel Bawden,Anthony Glaise,Ariane Pinche,David Smith*

Main category: cs.CL

TL;DR: 提出预编辑规范化(PEN)任务，将古文字识别输出按编辑规范规范化，平衡古文字保真度与实用可用性，并创建了相关数据集和模型。


<details>
  <summary>Details</summary>
Motivation: 自动文本识别(ATR)在历史档案访问方面取得进展，但古文字转录与规范化数字版本之间存在方法鸿沟。古文字导向的ATR模型通用性更好但输出不兼容读者和NLP工具，而规范化模型又难以适应新领域且容易过度规范化和产生幻觉。

Method: 提出预编辑规范化(PEN)任务，从CoMMA语料库创建新数据集，与数字化的古法语和拉丁语版本对齐，制作人工校正的黄金标准评估集，使用基于ByT5的序列到序列模型进行规范化和预标注任务基准测试。

Result: 创建了包含466万样本的银训练语料库、1800样本的黄金评估集，规范化模型达到6.7%的字符错误率，显著优于该任务的先前模型。

Conclusion: PEN任务成功弥合了古文字保真度与实用可用性之间的差距，为历史文本处理提供了有效的中间步骤，相关资源为后续研究提供了坚实基础。

Abstract: Recent advances in Automatic Text Recognition (ATR) have improved access to historical archives, yet a methodological divide persists between palaeographic transcriptions and normalized digital editions. While ATR models trained on more palaeographically-oriented datasets such as CATMuS have shown greater generalizability, their raw outputs remain poorly compatible with most readers and downstream NLP tools, thus creating a usability gap. On the other hand, ATR models trained to produce normalized outputs have been shown to struggle to adapt to new domains and tend to over-normalize and hallucinate. We introduce the task of Pre-Editorial Normalization (PEN), which consists in normalizing graphemic ATR output according to editorial conventions, which has the advantage of keeping an intermediate step with palaeographic fidelity while providing a normalized version for practical usability. We present a new dataset derived from the CoMMA corpus and aligned with digitized Old French and Latin editions using passim. We also produce a manually corrected gold-standard evaluation set. We benchmark this resource using ByT5-based sequence-to-sequence models on normalization and pre-annotation tasks. Our contributions include the formal definition of PEN, a 4.66M-sample silver training corpus, a 1.8k-sample gold evaluation set, and a normalization model achieving a 6.7% CER, substantially outperforming previous models for this task.

</details>


### [27] [Multi-Agent Comedy Club: Investigating Community Discussion Effects on LLM Humor Generation](https://arxiv.org/abs/2602.14770)
*Shiwei Hong,Lingyao Li,Ethan Z. Rong,Chenxinran Shen,Zhicong Lu*

Main category: cs.CL

TL;DR: 研究测试社区讨论是否能改善脱口秀写作，通过多智能体沙盒实验发现，包含社区讨论的条件在75.6%的情况下表现更好，显著提升了写作技巧/清晰度和社交反应


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注即时反馈和局部互动，但缺乏对在线社区中长期公共反馈如何影响LLM写作的考察。本研究旨在探索广播式社区讨论对脱口秀写作质量的影响

Method: 采用受控多智能体沙盒实验设计：实验组记录、过滤、存储批评家和观众讨论作为社会记忆，并在后续生成中检索使用；对照组则省略讨论。共进行50轮实验（250对独白），由5位专家标注员使用A/B偏好和15项评分标准进行评估

Result: 讨论条件在75.6%的实例中获胜，显著提高了写作技巧/清晰度（Δ=0.440）和社交反应（Δ=0.422），偶尔增加了攻击性幽默

Conclusion: 广播式社区讨论能有效改善LLM的脱口秀写作质量，特别是在技巧清晰度和社交反应方面，这为利用社会记忆和社区反馈优化LLM写作提供了实证支持

Abstract: Prior work has explored multi-turn interaction and feedback for LLM writing, but evaluations still largely center on prompts and localized feedback, leaving persistent public reception in online communities underexamined. We test whether broadcast community discussion improves stand-up comedy writing in a controlled multi-agent sandbox: in the discussion condition, critic and audience threads are recorded, filtered, stored as social memory, and later retrieved to condition subsequent generations, whereas the baseline omits discussion. Across 50 rounds (250 paired monologues) judged by five expert annotators using A/B preference and a 15-item rubric, discussion wins 75.6% of instances and improves Craft/Clarity (Δ = 0.440) and Social Response (Δ = 0.422), with occasional increases in aggressive humor.

</details>


### [28] [HLE-Verified: A Systematic Verification and Structured Revision of Humanity's Last Exam](https://arxiv.org/abs/2602.13964)
*Weiqi Zhai,Zhihai Wang,Jinghang Wang,Boyu Yang,Xiaogang Li,Xiang Xu,Bohan Wang,Peng Wang,Xingzhe Wu,Anfeng Li,Qiyuan Feng,Yuhao Zhou,Shoulin Han,Wenjie Luo,Yiyuan Li,Yaxuan Wang,Ruixian Luo,Guojie Lin,Peiyao Xiao,Chengliang Xu,Ben Wang,Zeyu Wang,Zichao Chen,Jianan Ye,Yijie Hu,Jialong Chen,Zongwen Shen,Yuliang Xu,An Yang,Bowen Yu,Dayiheng Liu,Junyang Lin,Hu Wei,Que Shen,Bing Zhao*

Main category: cs.CL

TL;DR: HLE-Verified是对Humanity's Last Exam基准的验证修订版本，通过两阶段验证修复流程减少了噪声，提高了评估的准确性。


<details>
  <summary>Details</summary>
Motivation: 原始HLE基准包含大量噪声项，会偏置评估结果并扭曲模型间比较，需要更可靠的基准来准确评估前沿大语言模型。

Method: 采用两阶段验证修复流程：第一阶段通过领域专家审查和模型交叉检查进行二元验证；第二阶段对可修复项在保持原始评估意图的严格约束下进行修订，包括双独立专家修复、模型辅助审计和最终裁决。

Result: 获得641个验证项和1,170个修订认证项，剩余689项作为不确定集发布。在HLE-Verified上评估7个SOTA语言模型，平均绝对准确率提升7-10个百分点，在原始问题陈述或参考答案有错误的项上提升30-40个百分点。

Conclusion: HLE-Verified通过减少标注噪声，提高了HLE式评估的可靠性，能更真实地测量模型能力，模型置信度与问题/答案错误存在强关联，验证了修订的有效性。

Abstract: Humanity's Last Exam (HLE) has become a widely used benchmark for evaluating frontier large language models on challenging, multi-domain questions. However, community-led analyses have raised concerns that HLE contains a non-trivial number of noisy items, which can bias evaluation results and distort cross-model comparisons. To address this challenge, we introduce HLE-Verified, a verified and revised version of HLE with a transparent verification protocol and fine-grained error taxonomy. Our construction follows a two-stage validation-and-repair workflow resulting in a certified benchmark. In Stage I, each item undergoes binary validation of the problem and final answer through domain-expert review and model-based cross-checks, yielding 641 verified items. In Stage II, flawed but fixable items are revised under strict constraints preserving the original evaluation intent, through dual independent expert repairs, model-assisted auditing, and final adjudication, resulting in 1,170 revised-and-certified items. The remaining 689 items are released as a documented uncertain set with explicit uncertainty sources and expertise tags for future refinement. We evaluate seven state-of-the-art language models on HLE and HLE-Verified, observing an average absolute accuracy gain of 7--10 percentage points on HLE-Verified. The improvement is particularly pronounced on items where the original problem statement and/or reference answer is erroneous, with gains of 30--40 percentage points. Our analyses further reveal a strong association between model confidence and the presence of errors in the problem statement or reference answer, supporting the effectiveness of our revisions. Overall, HLE-Verified improves HLE-style evaluations by reducing annotation noise and enabling more faithful measurement of model capabilities. Data is available at: https://github.com/SKYLENAGE-AI/HLE-Verified

</details>


### [29] [A Geometric Analysis of Small-sized Language Model Hallucinations](https://arxiv.org/abs/2602.14778)
*Emanuele Ricco,Elia Onofri,Lorenzo Cima,Stefano Cresci,Roberto Di Pietro*

Main category: cs.CL

TL;DR: 该论文从几何视角研究小规模语言模型的幻觉问题，提出真实响应在嵌入空间中更紧密聚集的假设，并基于此开发了仅需30-50个标注即可分类大量响应的传播方法，F1分数超过90%。


<details>
  <summary>Details</summary>
Motivation: 幻觉（流畅但事实错误的响应）是语言模型可靠性的主要挑战，特别是在多步骤或代理设置中。传统方法主要关注知识中心和单响应评估范式，需要从新的几何视角来理解和检测幻觉。

Method: 采用几何视角分析幻觉，首先假设模型对同一提示生成多个响应时，真实响应在嵌入空间中会呈现更紧密的聚类。验证该假设后，利用几何洞察开发标签高效的传播方法，仅需30-50个标注即可分类大量响应。

Result: 证明了真实响应在嵌入空间中确实呈现更紧密聚类的假设，并实现了可分离性。提出的传播方法仅需少量标注就能达到90%以上的F1分数，有效分类大量响应。

Conclusion: 从嵌入空间的几何视角研究幻觉问题，补充了传统的知识中心和单响应评估范式，为未来研究开辟了新方向，提供了高效检测幻觉的实用方法。

Abstract: Hallucinations -- fluent but factually incorrect responses -- pose a major challenge to the reliability of language models, especially in multi-step or agentic settings.
  This work investigates hallucinations in small-sized LLMs through a geometric perspective, starting from the hypothesis that when models generate multiple responses to the same prompt, genuine ones exhibit tighter clustering in the embedding space, we prove this hypothesis and, leveraging this geometrical insight, we also show that it is possible to achieve a consistent level of separability. This latter result is used to introduce a label-efficient propagation method that classifies large collections of responses from just 30-50 annotations, achieving F1 scores above 90%.
  Our findings, framing hallucinations from a geometric perspective in the embedding space, complement traditional knowledge-centric and single-response evaluation paradigms, paving the way for further research.

</details>


### [30] [Chain-of-Thought Reasoning with Large Language Models for Clinical Alzheimer's Disease Assessment and Diagnosis](https://arxiv.org/abs/2602.13979)
*Tongze Zhang,Jun-En Ding,Melik Ozolcer,Fang-Ming Hung,Albert Chih-Chieh Yang,Feng Liu,Yi-Rou Ji,Sang Won Bae*

Main category: cs.CL

TL;DR: 该研究提出利用大语言模型对阿尔茨海默病患者电子健康记录进行链式思维推理，通过生成明确的诊断理由来提升AD评估的稳定性和诊断性能。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病诊断传统上依赖医学影像和临床评估，耗时耗力且资源密集。虽然大语言模型在医疗领域应用增加，但在AD评估中仍有限制，因为AD涉及复杂的多因素病因，难以通过影像直接观察。

Method: 提出利用LLMs对患者临床电子健康记录进行链式思维推理的方法。不同于直接在EHR数据上微调LLMs进行AD分类，该方法使用LLM生成的CoT推理路径为模型提供明确的AD评估诊断理由，然后进行基于结构化CoT的预测。

Result: 实验结果表明，提出的基于CoT的诊断框架在多个CDR分级任务中显著提升了稳定性和诊断性能，相比零样本基线方法，F1分数最高提升了15%。

Conclusion: 基于链式思维推理的LLM方法不仅增强了模型诊断内在复杂因素的能力，还提高了AD进展不同阶段预测过程的可解释性，为AD诊断提供了更有效的解决方案。

Abstract: Alzheimer's disease (AD) has become a prevalent neurodegenerative disease worldwide. Traditional diagnosis still relies heavily on medical imaging and clinical assessment by physicians, which is often time-consuming and resource-intensive in terms of both human expertise and healthcare resources. In recent years, large language models (LLMs) have been increasingly applied to the medical field using electronic health records (EHRs), yet their application in Alzheimer's disease assessment remains limited, particularly given that AD involves complex multifactorial etiologies that are difficult to observe directly through imaging modalities. In this work, we propose leveraging LLMs to perform Chain-of-Thought (CoT) reasoning on patients' clinical EHRs. Unlike direct fine-tuning of LLMs on EHR data for AD classification, our approach utilizes LLM-generated CoT reasoning paths to provide the model with explicit diagnostic rationale for AD assessment, followed by structured CoT-based predictions. This pipeline not only enhances the model's ability to diagnose intrinsically complex factors but also improves the interpretability of the prediction process across different stages of AD progression. Experimental results demonstrate that the proposed CoT-based diagnostic framework significantly enhances stability and diagnostic performance across multiple CDR grading tasks, achieving up to a 15% improvement in F1 score compared to the zero-shot baseline method.

</details>


### [31] [The Sufficiency-Conciseness Trade-off in LLM Self-Explanation from an Information Bottleneck Perspective](https://arxiv.org/abs/2602.14002)
*Ali Zahedzadeh,Behnam Bahrak*

Main category: cs.CL

TL;DR: 研究探索LLM解释的简洁性与充分性平衡，发现更短的解释通常仍能保持准确性，但过度压缩会导致性能下降


<details>
  <summary>Details</summary>
Motivation: 大型语言模型依赖自我解释（如思维链推理）来提高多步问答性能，但这些解释通常冗长且生成成本高，需要研究解释的必要程度

Method: 基于信息瓶颈原理，将解释视为压缩表示；引入评估流程，限制解释长度并使用多个语言模型在ARC Challenge数据集上评估充分性；在英语和波斯语中进行实验

Result: 实验表明更简洁的解释通常仍能保持充分性，在显著减少解释长度的同时保持准确性，但过度压缩会导致性能下降

Conclusion: 在LLM解释中存在简洁性与充分性的平衡点，适度的压缩可以保持性能同时大幅减少成本，为资源受限环境提供实用方案

Abstract: Large Language Models increasingly rely on self-explanations, such as chain of thought reasoning, to improve performance on multi step question answering. While these explanations enhance accuracy, they are often verbose and costly to generate, raising the question of how much explanation is truly necessary. In this paper, we examine the trade-off between sufficiency, defined as the ability of an explanation to justify the correct answer, and conciseness, defined as the reduction in explanation length. Building on the information bottleneck principle, we conceptualize explanations as compressed representations that retain only the information essential for producing correct answers.To operationalize this view, we introduce an evaluation pipeline that constrains explanation length and assesses sufficiency using multiple language models on the ARC Challenge dataset. To broaden the scope, we conduct experiments in both English, using the original dataset, and Persian, as a resource-limited language through translation. Our experiments show that more concise explanations often remain sufficient, preserving accuracy while substantially reducing explanation length, whereas excessive compression leads to performance degradation.

</details>


### [32] [Named Entity Recognition for Payment Data Using NLP](https://arxiv.org/abs/2602.14009)
*Srikumar Nayak*

Main category: cs.CL

TL;DR: 该论文系统分析了支付数据命名实体识别技术，提出PaymentBERT混合架构，在金融交易处理中达到95.7% F1分数，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 金融交易处理自动化需要从非结构化支付数据中提取结构化信息，命名实体识别是关键组件。现有方法在支付数据提取方面的性能有待提升，需要专门针对金融领域优化的解决方案。

Method: 1. 系统分析CRF、BiLSTM-CRF、BERT、FinBERT等NER算法在支付数据提取中的应用；2. 在5万条标注支付交易数据集上进行实验，涵盖SWIFT MT103、ISO 20022和国内支付系统；3. 提出PaymentBERT混合架构，结合领域特定金融嵌入和上下文表示。

Result: 1. 微调BERT模型达到94.2% F1分数，比传统CRF方法提升12.8个百分点；2. PaymentBERT达到95.7% F1分数的SOTA性能，同时保持实时处理能力；3. 提供了跨格式泛化分析、消融研究和部署考虑。

Conclusion: PaymentBERT在支付数据NER任务中表现优异，为金融机构实施自动制裁筛查、反洗钱合规和支付处理系统提供了实用见解，证明了领域特定优化的重要性。

Abstract: Named Entity Recognition (NER) has emerged as a critical component in automating financial transaction processing, particularly in extracting structured information from unstructured payment data. This paper presents a comprehensive analysis of state-of-the-art NER algorithms specifically designed for payment data extraction, including Conditional Random Fields (CRF), Bidirectional Long Short-Term Memory with CRF (BiLSTM-CRF), and transformer-based models such as BERT and FinBERT. We conduct extensive experiments on a dataset of 50,000 annotated payment transactions across multiple payment formats including SWIFT MT103, ISO 20022, and domestic payment systems. Our experimental results demonstrate that fine-tuned BERT models achieve an F1-score of 94.2% for entity extraction, outperforming traditional CRF-based approaches by 12.8 percentage points. Furthermore, we introduce PaymentBERT, a novel hybrid architecture combining domain-specific financial embeddings with contextual representations, achieving state-of-the-art performance with 95.7% F1-score while maintaining real-time processing capabilities. We provide detailed analysis of cross-format generalization, ablation studies, and deployment considerations. This research provides practical insights for financial institutions implementing automated sanctions screening, anti-money laundering (AML) compliance, and payment processing systems.

</details>


### [33] [GRRM: Group Relative Reward Modeling for Machine Translation](https://arxiv.org/abs/2602.14028)
*Sen Yang,Shanbo Cheng,Lu Xu,Jianbing Zhang,Shujian Huang*

Main category: cs.CL

TL;DR: GRRM（Group Relative Reward Model）通过群体相对评估范式提升GRPO在机器翻译中的效果，解决了传统标量质量指标在细粒度语言区分上的不足。


<details>
  <summary>Details</summary>
Motivation: 传统标量质量指标（SQM）在评估翻译候选时缺乏比较上下文，难以区分细微的语言差异，限制了GRPO在开放领域（如机器翻译）中的效果。

Method: 提出群体质量指标（GQM）范式，并实例化为GRRM模型。GRRM联合处理整个候选群体，通过比较分析进行相对质量评估和自适应粒度解析。

Result: GRRM在所有基线中达到竞争性的排序准确性；将其集成到GRPO训练循环后，不仅提升了一般翻译质量，还解锁了与最先进推理模型相当的推理能力。

Conclusion: GRRM通过群体相对评估有效解决了GRPO在机器翻译中的排序问题，为开放领域LLM后训练提供了更准确的评估框架。

Abstract: While Group Relative Policy Optimization (GRPO) offers a powerful framework for LLM post-training, its effectiveness in open-ended domains like Machine Translation hinges on accurate intra-group ranking. We identify that standard Scalar Quality Metrics (SQM) fall short in this context; by evaluating candidates in isolation, they lack the comparative context necessary to distinguish fine-grained linguistic nuances. To address this, we introduce the Group Quality Metric (GQM) paradigm and instantiate it via the Group Relative Reward Model (GRRM). Unlike traditional independent scorers, GRRM processes the entire candidate group jointly, leveraging comparative analysis to rigorously resolve relative quality and adaptive granularity. Empirical evaluations confirm that GRRM achieves competitive ranking accuracy among all baselines. Building on this foundation, we integrate GRRM into the GRPO training loop to optimize the translation policy. Experimental results demonstrate that our framework not only improves general translation quality but also unlocks reasoning capabilities comparable to state-of-the-art reasoning models. We release codes, datasets, and model checkpoints at https://github.com/NJUNLP/GRRM.

</details>


### [34] [Geometry-Preserving Aggregation for Mixture-of-Experts Embedding Models](https://arxiv.org/abs/2602.14039)
*Sajjad Kachuee,Mohammad Sharifkhani*

Main category: cs.CL

TL;DR: 论文提出球形质心聚合（SBA）方法，解决MoE嵌入模型中线性聚合导致几何结构破坏的问题，在保持训练成本不变的情况下提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有MoE嵌入模型使用加权线性求和聚合专家输出，隐含假设嵌入空间具有线性子空间结构。但研究发现专家表示实际位于共享超球面流形上，线性聚合会导致向量幅度和方向扭曲，破坏嵌入可比性。

Method: 提出球形质心聚合（SBA）算子，将径向和角度分量分离处理，在保持超球面几何结构的同时与现有路由机制完全兼容。通过几何分析验证专家表示位于共享超球面流形上。

Result: 在MTEB基准测试的语义相似性、聚类和重复问题检测等任务上，SBA在相同训练成本下实现了一致的性能提升，并保持完全稳定性。几何分析证实SBA防止了聚合引起的塌缩，保持了超球面一致性。

Conclusion: MoE嵌入架构需要几何感知的聚合方法，SBA通过保持超球面几何结构有效解决了线性聚合的几何不一致问题，在保持兼容性的同时提升了模型性能。

Abstract: Mixture-of-Experts (MoE) embedding models combine expert outputs using weighted linear summation, implicitly assuming a linear subspace structure in the embedding space. This assumption is shown to be inconsistent with the geometry of expert representations. Geometric analysis of a modern MoE embedding model reveals that expert outputs lie on a shared hyperspherical manifold characterized by tightly concentrated norms and substantial angular separation. Under this geometry, linear aggregation induces inward collapse toward the manifold interior, distorting vector magnitude and direction and reducing embedding comparability. To address this inconsistency, Spherical Barycentric Aggregation (SBA) is introduced as a geometry-preserving aggregation operator that separates radial and angular components to maintain hyperspherical structure while remaining fully compatible with existing routing mechanisms. Experiments on selected tasks from the Massive Text Embedding Benchmark (MTEB), including semantic similarity, clustering, and duplicate question detection, demonstrate consistent performance improvements with identical training cost and full stability. Additional geometric analyses confirm that SBA prevents aggregation-induced collapse and preserves hyperspherical consistency, highlighting the importance of geometry-aware aggregation in MoE embedding architectures.

</details>


### [35] [Context Shapes LLMs Retrieval-Augmented Fact-Checking Effectiveness](https://arxiv.org/abs/2602.14044)
*Pietro Bernardelle,Stefano Civelli,Kevin Roitero,Gianluca Demartini*

Main category: cs.CL

TL;DR: LLM事实核查性能随上下文长度增加而下降，证据位置在开头或结尾时准确率更高，中间位置表现较差


<details>
  <summary>Details</summary>
Motivation: 研究LLM在事实核查任务中上下文长度的影响，特别是验证先前研究中观察到的"中间上下文退化"现象是否也适用于事实核查场景

Method: 使用三个数据集（HOVER、FEVEROUS、ClimateFEVER）和五个不同参数规模（7B、32B、70B）的开源模型（Llama-3.1、Qwen2.5、Qwen3），评估参数化事实知识和证据位置在不同上下文长度下的影响

Result: LLM表现出非平凡的参数化事实知识，但随着上下文长度增加，验证准确率普遍下降；证据位置对性能有显著影响，开头或结尾放置证据时准确率更高，中间位置表现较差

Conclusion: 提示结构在检索增强的事实核查系统中至关重要，需要优化证据放置位置以提高LLM事实核查性能

Abstract: Large language models (LLMs) show strong reasoning abilities across diverse tasks, yet their performance on extended contexts remains inconsistent. While prior research has emphasized mid-context degradation in question answering, this study examines the impact of context in LLM-based fact verification. Using three datasets (HOVER, FEVEROUS, and ClimateFEVER) and five open-source models accross different parameters sizes (7B, 32B and 70B parameters) and model families (Llama-3.1, Qwen2.5 and Qwen3), we evaluate both parametric factual knowledge and the impact of evidence placement across varying context lengths. We find that LLMs exhibit non-trivial parametric knowledge of factual claims and that their verification accuracy generally declines as context length increases. Similarly to what has been shown in previous works, in-context evidence placement plays a critical role with accuracy being consistently higher when relevant evidence appears near the beginning or end of the prompt and lower when placed mid-context. These results underscore the importance of prompt structure in retrieval-augmented fact-checking systems.

</details>


### [36] [LogitsCoder: Towards Efficient Chain-of-Thought Path Search via Logits Preference Decoding for Code Generation](https://arxiv.org/abs/2602.14054)
*Jizheng Chen,Weiming Zhang,Xinyi Dai,Weiwen Liu,Kounianhua Du,Yasheng Wang,Ruiming Tang,Yong Yu,Weinan Zhang*

Main category: cs.CL

TL;DR: LogitsCoder：通过轻量级logit级控制机制增强代码生成的思维链推理，解决现有方法中的"欠思考"和"过思考"问题


<details>
  <summary>Details</summary>
Motivation: 现有测试时间扩展方法（包括结构化树搜索）在探索推理路径方面取得进展，但仍面临两大挑战：1）"欠思考" - 推理链过浅，无法捕捉问题的全部复杂性；2）"过思考" - 推理过于冗长，导致效率低下和计算成本增加

Method: LogitsCoder框架通过轻量级logit级控制机制增强思维链推理：1）Logits Preference Decoding引导token选择朝向统计偏好的模式；2）Logits Rank Based Path Selection选择多样化的推理路径；3）Thoughts Aggregation聚合推理路径，迭代生成和精炼推理步骤

Result: 大量实验表明，LogitsCoder能产生更高效、更高质量的推理链，在代码生成性能上优于基线方法

Conclusion: LogitsCoder通过平衡深度和效率的连贯有效推理链，解决了代码生成中的"欠思考"和"过思考"问题，实现了优越的代码生成性能

Abstract: Code generation remains a challenging task that requires precise and structured reasoning. Existing Test Time Scaling (TTS) methods, including structured tree search, have made progress in exploring reasoning paths but still face two major challenges: (1) underthinking, where reasoning chains tend to be shallow and fail to capture the full complexity of problems; and (2) overthinking, where overly verbose reasoning leads to inefficiency and increased computational costs. To address these issues, we propose LogitsCoder, a novel framework that enhances chain-of-thought reasoning through lightweight, logit-level control mechanisms for code generation. LogitsCoder iteratively generates and refines reasoning steps by first steering token selection toward statistically preferred patterns via Logits Preference Decoding, then selecting and aggregating diverse reasoning paths using Logits Rank Based Path Selection and Thoughts Aggregation. This results in coherent and effective reasoning chains that balance depth and efficiency. Extensive experiments demonstrate that LogitsCoder produces more efficient and higher-quality reasoning chains, leading to superior code generation performance compared to baseline methods.

</details>


### [37] [LM-Lexicon: Improving Definition Modeling via Harmonizing Semantic Experts](https://arxiv.org/abs/2602.14060)
*Yang Liu,Jiaye Yang,Weikang Li,Jiahui Liang,Yang Li,Lingyong Yan*

Main category: cs.CL

TL;DR: LM-Lexicon是一种创新的定义建模方法，通过数据聚类、语义专家学习和稀疏专家混合架构，将定义建模任务分解为专门的语义领域，在五个基准测试上比现有方法提升了7%的BLEU分数。


<details>
  <summary>Details</summary>
Motivation: 传统定义建模方法在处理语义密集型应用时存在局限性，需要更高效的语言模型来提升定义质量。本文旨在通过领域专业化来改进定义建模任务。

Method: 采用数据聚类将定义建模任务分解为专门的语义领域，训练小型语言模型作为领域专家，使用稀疏专家混合架构和语义感知的领域级路由机制，结合测试时计算和语义专家扩展。

Result: 在五个广泛使用的基准测试上取得了显著改进（比先前最先进模型提升7% BLEU分数）。聚类策略使定义质量提升近10%，语义感知路由比传统标记级路由专家效能提升1%，通过测试时计算和专家扩展可获得进一步性能提升。

Conclusion: LM-Lexicon推进了定义建模的发展，为语义密集型应用的高效语言模型开发提供了见解，展示了领域专业化、聚类策略和专家路由机制的有效性。

Abstract: We introduce LM-Lexicon, an innovative definition modeling approach that incorporates data clustering, semantic expert learning, and model merging using a sparse mixture-of-experts architecture. By decomposing the definition modeling task into specialized semantic domains, where small language models are trained as domain experts, LM-Lexicon achieves substantial improvements (+7% BLEU score compared with the prior state-of-the-art model) over existing methods on five widely used benchmarks. Empirically, we demonstrate that 1) the clustering strategy enables fine-grained expert specialization with nearly 10% improvement in definition quality; 2) the semantic-aware domain-level routing mechanism achieves higher expert efficacy (+1%) than conventional token-level routing; and 3) further performance gains can be obtained through test-time compute and semantic expert scaling. Our work advances definition modeling while providing insights into the development of efficient language models for semantic-intensive applications.

</details>


### [38] [From Scarcity to Scale: A Release-Level Analysis of the Pashto Common Voice Dataset](https://arxiv.org/abs/2602.14062)
*Jandad Jahani,Mursal Dawodi,Jawid Ahmad Baktash*

Main category: cs.CL

TL;DR: 本文对Mozilla Common Voice语料库中的普什图语部分进行发布级别分析，展示了从2023年中的1.49小时快速增长到2025年的2768.7小时，其中975.89小时已验证可用于监督式ASR训练。


<details>
  <summary>Details</summary>
Motivation: 普什图语作为超过6000万人使用的语言，历史上缺乏适合现代ASR开发的大规模开放许可语音数据。需要分析Common Voice语料库中普什图语部分的增长和质量特征。

Method: 对Mozilla Common Voice语料库普什图语部分进行发布级别分析，重点关注版本24.0（2025年12月），并在主要发布版本中对比趋势。分析包括验证吞吐量、贡献者参与不平等性、人口统计元数据完整性以及验证子集中的句子级别集中度。

Result: 数据量从2023年中的1.49小时快速增长到2025年的2768.7小时，其中975.89小时已验证。参与度高度集中（基尼系数=0.941），年龄代表性偏向年轻成年人，41.97%的片段缺乏自我报告性别标签。在文本层面，35.88%的唯一句子占已验证片段的50%。

Conclusion: 研究为快速扩展的低资源语音语料库提供了定量审计，并突出了改进数据集成熟度的实际优先事项，包括扩展验证能力和更广泛的人口统计参与。

Abstract: Large, openly licensed speech datasets are essential for building automatic speech recognition (ASR) systems, yet many widely spoken languages remain underrepresented in public resources. Pashto, spoken by more than 60 million people, has historically lacked large-scale openly licensed speech data suitable for modern ASR development.
  This paper presents a release-level analysis of the Pashto component of the Mozilla Common Voice corpus, focusing on version 24.0 (December 2025) and contextualizing trends across major releases. We document rapid growth from 1.49 recorded hours in mid-2023 to 2,768.7 total hours in 2025, including 975.89 validated hours available for supervised ASR training.
  Beyond scale, we analyze validation throughput, contributor participation inequality, demographic metadata completeness, and sentence-level concentration in the validated subset. We find that participation is extremely concentrated (Gini = 0.941), age representation is strongly skewed toward young adults, and 41.97\% of clips lack self-reported gender labels, limiting subgroup auditing based on metadata. At the textual level, prompt reuse is moderate: 35.88\% of unique sentences account for 50\% of validated clips, suggesting that structural concentration is driven primarily by uneven contributor activity rather than dominance of a small prompt set.
  These results provide a quantitative audit of a rapidly scaling low-resource speech corpus and highlight practical priorities for improving dataset maturity, including expanded validation capacity and broader demographic participation.

</details>


### [39] [Open Rubric System: Scaling Reinforcement Learning with Pairwise Adaptive Rubric](https://arxiv.org/abs/2602.14069)
*Ruipeng Jia,Yunyi Yang,Yuxin Wu,Yongbo Gai,Siyuan Tao,Mengyu Zhou,Jianhe Lin,Xiaoxi Jiang,Guanjun Jiang*

Main category: cs.CL

TL;DR: OpenRS：基于可解释原则的LLM评估框架，通过元准则和自适应准则解决传统标量奖励模型的信息瓶颈问题


<details>
  <summary>Details</summary>
Motivation: 传统标量奖励模型将多维人类偏好压缩为单一不透明分数，造成信息瓶颈，导致开放对齐中的脆弱性和奖励黑客问题。作者认为稳健对齐本质上是原则泛化问题，奖励不应是内部化的学习函数，而应是基于可检查原则的显式推理过程。

Method: 提出Open Rubric System (OpenRS)：1) Pairwise Adaptive Meta-Rubrics (PAMR)：基于候选回答语义差异动态实例化准则；2) Pointwise Verifiable Rubrics (PVRs)：提供硬约束护栏和可验证奖励；3) 元准则（类似宪法规范）管理准则实例化、加权和执行；4) 两级元准则精炼管道（自动进化精炼通用原则+人机协作精炼领域原则）；5) 作为奖励监督应用于成对RL训练。

Result: OpenRS通过准则级成对比较和外部偏好聚合，避免点加权标量化，在开放场景中提高区分能力。提供可编辑原则的一致性和可验证奖励组件，防止退化行为。

Conclusion: OpenRS将奖励从内部化函数转变为基于可检查原则的显式推理过程，为开放对齐提供更稳健、可解释的框架，解决了传统标量奖励模型的信息瓶颈问题。

Abstract: Scalar reward models compress multi-dimensional human preferences into a single opaque score, creating an information bottleneck that often leads to brittleness and reward hacking in open-ended alignment. We argue that robust alignment for non-verifiable tasks is fundamentally a principle generalization problem: reward should not be a learned function internalized into a judge, but an explicit reasoning process executed under inspectable principles. To operationalize this view, we present the Open Rubric System (OpenRS), a plug-and-play, rubrics-based LLM-as-a-Judge framework built around Pairwise Adaptive Meta-Rubrics (PAMR) and lightweight Pointwise Verifiable Rubrics (PVRs), which provide both hard-constraint guardrails and verifiable reward components when ground-truth or programmatic checks are available. OpenRS uses an explicit meta-rubric -- a constitution-like specification that governs how rubrics are instantiated, weighted, and enforced -- and instantiates adaptive rubrics on the fly by conditioning on the semantic differences between two candidate responses. It then performs criterion-wise pairwise comparisons and aggregates criterion-level preferences externally, avoiding pointwise weighted scalarization while improving discriminability in open-ended settings. To keep principles consistent yet editable across various domains, we introduce a two-level meta-rubric refinement pipeline (automated evolutionary refinement for general principles and a reproducible human-in-the-loop procedure for domain principles), complemented with pointwise verifiable rubrics that act as both guardrails against degenerate behaviors and a source of verifiable reward for objective sub-tasks. Finally, we instantiate OpenRS as reward supervision in pairwise RL training.

</details>


### [40] [Annotation-Efficient Vision-Language Model Adaptation to the Polish Language Using the LLaVA Framework](https://arxiv.org/abs/2602.14073)
*Grzegorz Statkiewicz,Alicja Dobrzeniecka,Karolina Seweryn,Aleksandra Krasnodębska,Karolina Piosek,Katarzyna Bogusz,Sebastian Cygert,Wojciech Kusa*

Main category: cs.CL

TL;DR: 通过自动化翻译和筛选现有多模态数据集，结合合成波兰语OCR数据，开发了波兰语视觉语言模型，在波兰语评估中表现优于现有模型


<details>
  <summary>Details</summary>
Motivation: 大多数视觉语言模型基于英语数据训练，限制了在其他语言和文化环境中的性能，阻碍了反映多样化语言文化现实的多模态系统发展

Method: 复制并调整LLaVA-Next方法，创建波兰语VLMs；采用全自动化管道翻译和筛选现有多模态数据集，补充合成波兰语OCR和文化特定任务数据

Result: 在波兰语适配的MMBench上比LLaVA-1.6-Vicuna-13B提升9.5%；人类评估显示生成描述的语言正确性更高

Conclusion: 大规模自动化翻译结合轻量级筛选能有效引导低资源语言的高质量多模态模型；文化覆盖和评估仍存挑战；公开模型和评估数据集

Abstract: Most vision-language models (VLMs) are trained on English-centric data, limiting their performance in other languages and cultural contexts. This restricts their usability for non-English-speaking users and hinders the development of multimodal systems that reflect diverse linguistic and cultural realities. In this work, we reproduce and adapt the LLaVA-Next methodology to create a set of Polish VLMs. We rely on a fully automated pipeline for translating and filtering existing multimodal datasets, and complement this with synthetic Polish data for OCR and culturally specific tasks. Despite relying almost entirely on automatic translation and minimal manual intervention to the training data, our approach yields strong results: we observe a +9.5% improvement over LLaVA-1.6-Vicuna-13B on a Polish-adapted MMBench, along with higher-quality captions in generative evaluations, as measured by human annotators in terms of linguistic correctness. These findings highlight that large-scale automated translation, combined with lightweight filtering, can effectively bootstrap high-quality multimodal models for low-resource languages. Some challenges remain, particularly in cultural coverage and evaluation. To facilitate further research, we make our models and evaluation dataset publicly available.

</details>


### [41] [GTS: Inference-Time Scaling of Latent Reasoning with a Learnable Gaussian Thought Sampler](https://arxiv.org/abs/2602.14077)
*Minghan Wang,Ye Bai,Thuy-Trang Vu,Ehsan Shareghi,Gholamreza Haffari*

Main category: cs.CL

TL;DR: 提出Gaussian Thought Sampler (GTS)，一种可学习的结构化探索机制，用于改进潜在推理模型的推理时扩展，相比启发式扰动方法更有效。


<details>
  <summary>Details</summary>
Motivation: 现有推理时扩展方法使用启发式扰动（如dropout或固定高斯噪声）引入随机性，虽然增加了轨迹多样性，但探索行为未显式建模，在有限采样预算下效率低下。强扰动不一定产生更有效的候选轨迹，因为无引导的噪声可能破坏内部决策结构而非引导它。

Method: 将潜在思维探索建模为从可学习密度的条件采样，实例化为Gaussian Thought Sampler (GTS)。GTS预测连续推理状态上的上下文相关扰动分布，使用GRPO风格策略优化训练，同时保持主干网络冻结。

Result: 在GSM8K数据集上使用两种潜在推理架构的实验表明，GTS比启发式基线方法实现了更可靠的推理时扩展。

Conclusion: 改进潜在推理的推理时扩展需要结构化和可优化的探索机制，而不仅仅是简单地增加随机性。GTS提供了一种更有效的替代方案。

Abstract: Inference-time scaling (ITS) in latent reasoning models typically introduces stochasticity through heuristic perturbations, such as dropout or fixed Gaussian noise. While these methods increase trajectory diversity, their exploration behavior is not explicitly modeled and can be inefficient under finite sampling budgets. We observe that stronger perturbations do not necessarily translate into more effective candidate trajectories, as unguided noise may disrupt internal decision structure rather than steer it. To provide a more structured alternative, we model latent thought exploration as conditional sampling from learnable densities and instantiate this idea as a Gaussian Thought Sampler (GTS). GTS predicts context-dependent perturbation distributions over continuous reasoning states and is trained with GRPO-style policy optimization while keeping the backbone frozen. Experiments on GSM8K with two latent reasoning architectures show that GTS achieves more reliable inference-time scaling than heuristic baselines. These findings indicate that improving latent ITS requires structured and optimizable exploration mechanisms rather than simply amplifying stochasticity.

</details>


### [42] [Empty Shelves or Lost Keys? Recall Is the Bottleneck for Parametric Factuality](https://arxiv.org/abs/2602.14080)
*Nitay Calderon,Eyal Ben-David,Zorik Gekhman,Eran Ofek,Gal Yona*

Main category: cs.CL

TL;DR: 该论文提出了一种新的LLM事实性评估框架，区分知识缺失（空架子）和知识访问失败（丢失钥匙），发现前沿模型已编码95-98%的事实，但回忆失败是主要瓶颈，思考过程可以显著改善回忆效果。


<details>
  <summary>Details</summary>
Motivation: 现有LLM事实性评估将所有错误同等对待，无法区分错误是由于知识缺失还是知识访问失败造成的。需要一种能够分析事实知识在编码和可访问性层面的行为框架。

Method: 提出行为框架，在事实层面而非问题层面分析知识，将每个事实分类为：是否编码、可访问性（无法回忆、可直接回忆、需推理计算才能回忆）。引入WikiProfile基准，通过基于网络搜索的提示LLM自动化流程构建。

Result: 在13个LLM的400万个响应中，发现前沿模型（GPT-5和Gemini-3）在基准上编码了95-98%的事实，接近饱和。但回忆仍是主要瓶颈，许多先前归因于知识缺失的错误实际上是访问失败。这些失败是系统性的，尤其影响长尾事实和反向问题。思考过程能改善回忆并恢复大量失败案例。

Conclusion: 未来LLM改进可能更依赖于改善模型如何利用已编码知识的方法，而非简单的规模扩展。回忆失败是当前主要瓶颈，思考机制能有效提升事实回忆能力。

Abstract: Standard factuality evaluations of LLMs treat all errors alike, obscuring whether failures arise from missing knowledge (empty shelves) or from limited access to encoded facts (lost keys). We propose a behavioral framework that profiles factual knowledge at the level of facts rather than questions, characterizing each fact by whether it is encoded, and then by how accessible it is: cannot be recalled, can be directly recalled, or can only be recalled with inference-time computation (thinking). To support such profiling, we introduce WikiProfile, a new benchmark constructed via an automated pipeline with a prompted LLM grounded in web search. Across 4 million responses from 13 LLMs, we find that encoding is nearly saturated in frontier models on our benchmark, with GPT-5 and Gemini-3 encoding 95--98% of facts. However, recall remains a major bottleneck: many errors previously attributed to missing knowledge instead stem from failures to access it. These failures are systematic and disproportionately affect long-tail facts and reverse questions. Finally, we show that thinking improves recall and can recover a substantial fraction of failures, indicating that future gains may rely less on scaling and more on methods that improve how models utilize what they already encode.

</details>


### [43] [CCiV: A Benchmark for Structure, Rhythm and Quality in LLM-Generated Chinese \textit{Ci} Poetry](https://arxiv.org/abs/2602.14081)
*Shangqing Zhao,Yupei Ren,Yuhao Zhou,Xiaopeng Bai,Man Lan*

Main category: cs.CL

TL;DR: 该论文提出了CCiV基准，用于评估大语言模型生成古典宋词的能力，发现模型常生成历史变体，音律控制比结构规则更难，形式感知提示对强模型有效但对弱模型有害，形式正确性与文学质量关联弱。


<details>
  <summary>Details</summary>
Motivation: 古典宋词生成需要结构严谨、音律和谐和艺术品质的复杂结合，这对大语言模型构成重大挑战。需要系统评估和提升这一能力。

Method: 提出CCiV基准，从结构、音律和质量三个维度评估LLM生成的宋词。在30个词牌上评估17个LLM，分析形式感知提示的效果。

Result: 发现两个关键现象：1）模型常生成有效但意外的历史变体；2）遵守音律模式比结构规则困难得多。形式感知提示能改善强模型的结构和音律控制，但可能损害弱模型。形式正确性与文学质量之间关联弱且不一致。

Conclusion: CCiV基准凸显了需要变体感知评估和更全面的约束创造性生成方法，以提升大语言模型在古典诗歌生成方面的能力。

Abstract: The generation of classical Chinese \textit{Ci} poetry, a form demanding a sophisticated blend of structural rigidity, rhythmic harmony, and artistic quality, poses a significant challenge for large language models (LLMs). To systematically evaluate and advance this capability, we introduce \textbf{C}hinese \textbf{Ci}pai \textbf{V}ariants (\textbf{CCiV}), a benchmark designed to assess LLM-generated \textit{Ci} poetry across these three dimensions: structure, rhythm, and quality. Our evaluation of 17 LLMs on 30 \textit{Cipai} reveals two critical phenomena: models frequently generate valid but unexpected historical variants of a poetic form, and adherence to tonal patterns is substantially harder than structural rules. We further show that form-aware prompting can improve structural and tonal control for stronger models, while potentially degrading weaker ones. Finally, we observe weak and inconsistent alignment between formal correctness and literary quality in our sample. CCiV highlights the need for variant-aware evaluation and more holistic constrained creative generation methods.

</details>


### [44] [Character-aware Transformers Learn an Irregular Morphological Pattern Yet None Generalize Like Humans](https://arxiv.org/abs/2602.14100)
*Akhilesh Kakolu Ramarao,Kevin Tang,Dinah Baer-Henney*

Main category: cs.CL

TL;DR: 研究发现，位置不变的位置编码模型能恢复西班牙语L形词形变化模式，但无法像人类一样将这种模式泛化到新形式，揭示了统计模式复制与形态抽象之间的差距。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络是否能作为形态学习的认知模型，特别是能否像人类一样泛化不规则形态模式。以西班牙语L形词形变化为例，探究模型能否学习并泛化这种缺乏明显音系、语义或句法动机的模式。

Method: 比较五种编码器-解码器变换器模型，这些模型在两个维度上变化：顺序位置编码vs位置不变位置编码，以及原子标签表示vs分解标签表示。使用西班牙语L形词形变化作为测试案例，评估模型在训练数据稀缺情况下恢复和泛化该模式的能力。

Result: 位置编码是关键因素：位置不变模型即使在训练数据稀缺时也能正确恢复L形范式聚类，而顺序位置编码模型只能部分捕捉该模式。然而，所有模型都无法生产性地将该模式泛化到新形式。位置不变模型将L形词干泛化到虚拟语气单元格，但未能扩展到第一人称单数直陈式，产生了基于语气的泛化而非L形词形变化模式。

Conclusion: 神经网络模型能够复制统计模式，但无法像人类一样进行形态抽象和泛化。位置不变编码有助于模式恢复，但所有模型都无法再现人类优先泛化到第一人称单数直陈式的模式，突显了当前模型与人类形态学习能力之间的差距。

Abstract: Whether neural networks can serve as cognitive models of morphological learning remains an open question. Recent work has shown that encoder-decoder models can acquire irregular patterns, but evidence that they generalize these patterns like humans is mixed. We investigate this using the Spanish \emph{L-shaped morphome}, where only the first-person singular indicative (e.g., \textit{pongo} `I put') shares its stem with all subjunctive forms (e.g., \textit{ponga, pongas}) despite lacking apparent phonological, semantic, or syntactic motivation. We compare five encoder-decoder transformers varying along two dimensions: sequential vs. position-invariant positional encoding, and atomic vs. decomposed tag representations. Positional encoding proves decisive: position-invariant models recover the correct L-shaped paradigm clustering even when L-shaped verbs are scarce in training, whereas sequential positional encoding models only partially capture the pattern. Yet none of the models productively generalize this pattern to novel forms. Position-invariant models generalize the L-shaped stem across subjunctive cells but fail to extend it to the first-person singular indicative, producing a mood-based generalization rather than the L-shaped morphomic pattern. Humans do the opposite, generalizing preferentially to the first-person singular indicative over subjunctive forms. None of the models reproduce the human pattern, highlighting the gap between statistical pattern reproduction and morphological abstraction.

</details>


### [45] [A Multi-Agent Framework for Medical AI: Leveraging Fine-Tuned GPT, LLaMA, and DeepSeek R1 for Evidence-Based and Bias-Aware Clinical Query Processing](https://arxiv.org/abs/2602.14158)
*Naeimeh Nourmohammadi,Md Meem Hossain,The Anh Han,Safina Showkat Ara,Zia Ush Shamszaman*

Main category: cs.CL

TL;DR: 提出多智能体医疗问答框架，结合证据检索、不确定性估计和偏见检测，提升LLM在医疗问答中的可靠性和准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医疗问答中虽有潜力，但存在验证薄弱、证据不足、置信度不可靠等问题，限制了临床实际应用。

Method: 分两阶段：1) 在MedQuAD医疗QA数据上微调GPT、LLaMA和DeepSeek R1模型；2) 构建模块化多智能体管道，包括临床推理、证据检索、精炼等智能体，并集成安全机制如不确定性评分和偏见检测。

Result: DeepSeek R1表现最佳（ROUGE-1 0.536，ROUGE-2 0.226，BLEU 0.098）；完整系统准确率达87%，相关性约0.80，证据增强降低不确定性（困惑度4.13），端到端延迟36.5秒。

Conclusion: 智能体专业化和验证层能缓解单模型局限，为基于证据和偏见感知的医疗AI提供了实用、可扩展的设计方案。

Abstract: Large language models (LLMs) show promise for healthcare question answering, but clinical use is limited by weak verification, insufficient evidence grounding, and unreliable confidence signalling. We propose a multi-agent medical QA framework that combines complementary LLMs with evidence retrieval, uncertainty estimation, and bias checks to improve answer reliability. Our approach has two phases. First, we fine-tune three representative LLM families (GPT, LLaMA, and DeepSeek R1) on MedQuAD-derived medical QA data (20k+ question-answer pairs across multiple NIH domains) and benchmark generation quality. DeepSeek R1 achieves the strongest scores (ROUGE-1 0.536 +- 0.04; ROUGE-2 0.226 +-0.03; BLEU 0.098 -+ 0.018) and substantially outperforms the specialised biomedical baseline BioGPT in zero-shot evaluation. Second, we implement a modular multi-agent pipeline in which a Clinical Reasoning agent (fine-tuned LLaMA) produces structured explanations, an Evidence Retrieval agent queries PubMed to ground responses in recent literature, and a Refinement agent (DeepSeek R1) improves clarity and factual consistency; an optional human validation path is triggered for high-risk or high-uncertainty cases. Safety mechanisms include Monte Carlo dropout and perplexity-based uncertainty scoring, plus lexical and sentiment-based bias detection supported by LIME/SHAP-based analyses. In evaluation, the full system achieves 87% accuracy with relevance around 0.80, and evidence augmentation reduces uncertainty (perplexity 4.13) compared to base responses, with mean end-to-end latency of 36.5 seconds under the reported configuration. Overall, the results indicate that agent specialisation and verification layers can mitigate key single-model limitations and provide a practical, extensible design for evidence-based and bias-aware medical AI.

</details>


### [46] [Index Light, Reason Deep: Deferred Visual Ingestion for Visual-Dense Document Question Answering](https://arxiv.org/abs/2602.14162)
*Tao Xu*

Main category: cs.CL

TL;DR: DVI框架采用需求端视觉理解策略，仅在用户提问时进行视觉分析，相比传统预摄取方法显著降低计算成本并提高可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态文档问答方法采用供应端预摄取策略，存在三个主要问题：1) 计算成本高昂（113页工程图纸需要约80,000个VLM token）；2) 端到端不可靠（VLM输出可能因检索基础设施格式不匹配而无法正确检索）；3) 一旦失败无法恢复。

Method: 提出延迟视觉摄取（DVI）框架，采用需求端摄取策略：索引阶段仅执行轻量级元数据提取，将视觉理解推迟到用户提出具体问题时。核心原则是"索引用于定位，而非理解"，通过结构化元数据索引和BM25全文搜索实现页面定位，然后将原始图像与具体问题一起发送给VLM进行针对性分析。

Result: 在两个真实工业工程图纸（113页+7页）上的实验表明：DVI在零摄取VLM成本下达到可比的整体准确率（46.7% vs. 48.9%），在视觉必要查询上的有效率达到50%（预摄取为0%），页面定位准确率100%（搜索空间压缩98%）。

Conclusion: DVI将"QA准确率"问题转化为"页面定位"问题，一旦找到正确的图纸页面，获取答案就变成了交互轮次的问题。框架还支持交互式细化和渐进缓存，显著提高了多模态文档问答的效率和可靠性。

Abstract: Existing multimodal document question answering methods universally adopt a supply-side ingestion strategy: running a Vision-Language Model (VLM) on every page during indexing to generate comprehensive descriptions, then answering questions through text retrieval. However, this "pre-ingestion" approach is costly (a 113-page engineering drawing package requires approximately 80,000 VLM tokens), end-to-end unreliable (VLM outputs may fail to be correctly retrieved due to format mismatches in the retrieval infrastructure), and irrecoverable once it fails. This paper proposes the Deferred Visual Ingestion (DVI) framework, adopting a demand-side ingestion strategy: the indexing phase performs only lightweight metadata extraction, deferring visual understanding to the moment users pose specific questions. DVI's core principle is "Index for locating, not understanding"--achieving page localization through structured metadata indexes and BM25 full-text search, then sending original images along with specific questions to a VLM for targeted analysis. Experiments on two real industrial engineering drawings (113 pages + 7 pages) demonstrate that DVI achieves comparable overall accuracy at zero ingestion VLM cost (46.7% vs. 48.9%), an effectiveness rate of 50% on visually necessary queries (vs. 0% for pre-ingestion), and 100% page localization (98% search space compression). DVI also supports interactive refinement and progressive caching, transforming the "QA accuracy" problem into a "page localization" problem--once the correct drawing page is found, obtaining the answer becomes a matter of interaction rounds.

</details>


### [47] [GPT-5 vs Other LLMs in Long Short-Context Performance](https://arxiv.org/abs/2602.14188)
*Nima Esmi,Maryam Nezhad-Moghaddam,Fatemeh Borhani,Asadollah Shahbahrami,Amin Daemdoost,Georgi Gaydadjiev*

Main category: cs.CL

TL;DR: 评估四个SOTA模型（Grok-4、GPT-4、Gemini 2.5、GPT-5）在长上下文任务中的表现，发现当社交媒体帖子超过5K（70K tokens）时，所有模型性能显著下降，但GPT-5在精度方面保持95%的高水平。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs理论上能处理数百万tokens，但实际在需要全面理解大量细节的长上下文任务中，模型的理论容量与实际能力存在显著差距。本研究旨在评估SOTA模型在长上下文任务中的实际表现。

Method: 使用三个数据集：两个补充数据集（烹饪食谱检索和数学问题）和一个主要数据集（20K社交媒体帖子用于抑郁检测）。评估四个SOTA模型（Grok-4、GPT-4、Gemini 2.5、GPT-5）在不同输入量下的性能。

Result: 当社交媒体数据集输入量超过5K帖子（70K tokens）时，所有模型性能显著下降，20K帖子时准确率降至50-53%。GPT-5虽然准确率下降，但精度保持约95%的高水平。研究还表明"lost in the middle"问题在新模型中已基本解决。

Conclusion: 模型的理论容量与实际复杂高容量数据任务性能存在差距。对于抑郁检测等敏感应用，除了简单准确率外，精度等指标也很重要。GPT-5的高精度特性使其在敏感应用中可能非常有效。

Abstract: With the significant expansion of the context window in Large Language Models (LLMs), these models are theoretically capable of processing millions of tokens in a single pass. However, research indicates a significant gap between this theoretical capacity and the practical ability of models to robustly utilize information within long contexts, especially in tasks that require a comprehensive understanding of numerous details. This paper evaluates the performance of four state-of-the-art models (Grok-4, GPT-4, Gemini 2.5, and GPT-5) on long short-context tasks. For this purpose, three datasets were used: two supplementary datasets for retrieving culinary recipes and math problems, and a primary dataset of 20K social media posts for depression detection. The results show that as the input volume on the social media dataset exceeds 5K posts (70K tokens), the performance of all models degrades significantly, with accuracy dropping to around 50-53% for 20K posts. Notably, in the GPT-5 model, despite the sharp decline in accuracy, its precision remained high at approximately 95%, a feature that could be highly effective for sensitive applications like depression detection. This research also indicates that the "lost in the middle" problem has been largely resolved in newer models. This study emphasizes the gap between the theoretical capacity and the actual performance of models on complex, high-volume data tasks and highlights the importance of metrics beyond simple accuracy for practical applications.

</details>


### [48] [Knowing When Not to Answer: Abstention-Aware Scientific Reasoning](https://arxiv.org/abs/2602.14189)
*Samir Abdaljalil,Erchin Serpedin,Hasan Kurban*

Main category: cs.CL

TL;DR: 论文提出了一种基于弃权意识的科学声明验证框架，通过将声明分解为最小条件、使用自然语言推理审核证据，并选择性地支持、反驳或弃权，从而在科学推理中控制错误风险。


<details>
  <summary>Details</summary>
Motivation: 现有评估通常假设模型必须始终给出明确答案，但在科学环境中，缺乏支持或不确定的结论可能比弃权更有害。需要一种能够识别何时证据不足并选择弃权的验证框架。

Method: 提出弃权意识验证框架：1) 将科学声明分解为最小条件；2) 使用自然语言推理(NLI)根据可用证据审核每个条件；3) 选择性地决定支持、反驳或弃权。在SciFact和PubMedQA两个科学基准上评估，涵盖闭卷和开放域证据设置，使用六种不同语言模型。

Result: 在所有基准和模型中，原始准确率在不同架构间变化不大，而弃权在控制错误方面起关键作用。基于置信度的弃权在中等覆盖率水平下显著降低风险，即使绝对准确率改进有限。主要挑战不是选择单一最佳模型，而是确定何时证据足以证明答案的合理性。

Conclusion: 弃权意识评估是评估科学可靠性的实用且模型无关的视角，为科学领域的选择性推理提供了统一的实验基础。在科学推理任务中，关键不是模型选择，而是判断证据充分性。

Abstract: Large language models are increasingly used to answer and verify scientific claims, yet existing evaluations typically assume that a model must always produce a definitive answer. In scientific settings, however, unsupported or uncertain conclusions can be more harmful than abstaining. We study this problem through an abstention-aware verification framework that decomposes scientific claims into minimal conditions, audits each condition against available evidence using natural language inference (NLI), and selectively decides whether to support, refute, or abstain. We evaluate this framework across two complementary scientific benchmarks: SciFact and PubMedQA, covering both closed-book and open-domain evidence settings. Experiments are conducted with six diverse language models, including encoder-decoder, open-weight chat models, and proprietary APIs. Across all benchmarks and models, we observe that raw accuracy varies only modestly across architectures, while abstention plays a critical role in controlling error. In particular, confidence-based abstention substantially reduces risk at moderate coverage levels, even when absolute accuracy improvements are limited. Our results suggest that in scientific reasoning tasks, the primary challenge is not selecting a single best model, but rather determining when available evidence is sufficient to justify an answer. This work highlights abstention-aware evaluation as a practical and model-agnostic lens for assessing scientific reliability, and provides a unified experimental basis for future work on selective reasoning in scientific domains. Code is available at https://github.com/sabdaljalil2000/ai4science .

</details>


### [49] [We can still parse using syntactic rules](https://arxiv.org/abs/2602.14238)
*Ghaly Hussein*

Main category: cs.CL

TL;DR: 提出基于CFG和GPSG的新解析方法，生成依存和成分句法树，在UD数据上达到约54%的UAS分数，提供可解释的NLP模型


<details>
  <summary>Details</summary>
Motivation: 克服传统上下文无关文法(CFG)的局限性，将1950年代以来的理论句法研究成果应用于计算环境，创建透明可解释的NLP模型

Method: 结合新解析算法与句法规则特征，基于CFG和广义短语结构文法(GPSG)，支持依存和成分句法树生成，处理噪声和不完整解析，提供多解析假设

Result: 在Universal Dependencies数据上测试：开发集（7个语料库）平均UAS 54.5%，测试集（12个语料库）平均UAS 53.8%，可通过重排序进一步提升准确率

Conclusion: 该方法成功将理论句法应用于计算解析，提供透明可解释的NLP模型，能够同时生成依存和成分句法树，并处理不完整输入

Abstract: This research introduces a new parsing approach, based on earlier syntactic work on context free grammar (CFG) and generalized phrase structure grammar (GPSG). The approach comprises both a new parsing algorithm and a set of syntactic rules and features that overcome the limitations of CFG. It also generates both dependency and constituency parse trees, while accommodating noise and incomplete parses. The system was tested on data from Universal Dependencies, showing a promising average Unlabeled Attachment Score (UAS) of 54.5% in the development dataset (7 corpora) and 53.8% in the test set (12 corpora). The system also provides multiple parse hypotheses, allowing further reranking to improve parsing accuracy. This approach also leverages much of the theoretical syntactic work since the 1950s to be used within a computational context. The application of this approach provides a transparent and interpretable NLP model to process language input.

</details>


### [50] [AD-Bench: A Real-World, Trajectory-Aware Advertising Analytics Benchmark for LLM Agents](https://arxiv.org/abs/2602.14257)
*Lingxiang Hu,Yiding Sun,Tianle Xia,Wenwei Li,Ming Xu,Liqun Liu,Peng Shu,Huan Yu,Jie Jiang*

Main category: cs.CL

TL;DR: AD-Bench是一个针对广告营销领域的LLM智能体评估基准，基于真实业务需求构建，包含三个难度级别，用于评估多轮多工具协作能力。实验显示当前SOTA模型在复杂场景下仍存在显著能力差距。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体评估基准主要局限于理想化模拟，无法满足广告营销等专业领域的实际需求。这些领域的任务通常更复杂，需要与专业营销工具进行多轮交互。

Method: 基于广告营销平台真实业务需求构建基准，从真实用户营销分析请求中收集数据，由领域专家提供可验证的参考答案和对应的参考工具调用轨迹。将请求分为三个难度级别（L1-L3）来评估智能体的多轮多工具协作能力。

Result: 在AD-Bench上，Gemini-3-Pro的Pass@1=68.0%，Pass@3=83.0%，但在L3难度下性能显著下降至Pass@1=49.4%，Pass@3=62.1%，轨迹覆盖率为70.1%。这表明即使最先进的模型在复杂广告营销分析场景中仍存在显著能力差距。

Conclusion: AD-Bench为评估和改进广告营销智能体提供了一个现实的基准，揭示了当前模型在复杂专业领域的能力局限，为未来研究提供了重要方向。

Abstract: While Large Language Model (LLM) agents have achieved remarkable progress in complex reasoning tasks, evaluating their performance in real-world environments has become a critical problem. Current benchmarks, however, are largely restricted to idealized simulations, failing to address the practical demands of specialized domains like advertising and marketing analytics. In these fields, tasks are inherently more complex, often requiring multi-round interaction with professional marketing tools. To address this gap, we propose AD-Bench, a benchmark designed based on real-world business requirements of advertising and marketing platforms. AD-Bench is constructed from real user marketing analysis requests, with domain experts providing verifiable reference answers and corresponding reference tool-call trajectories. The benchmark categorizes requests into three difficulty levels (L1-L3) to evaluate agents' capabilities under multi-round, multi-tool collaboration. Experiments show that on AD-Bench, Gemini-3-Pro achieves Pass@1 = 68.0% and Pass@3 = 83.0%, but performance drops significantly on L3 to Pass@1 = 49.4% and Pass@3 = 62.1%, with a trajectory coverage of 70.1%, indicating that even state-of-the-art models still exhibit substantial capability gaps in complex advertising and marketing analysis scenarios. AD-Bench provides a realistic benchmark for evaluating and improving advertising marketing agents, the leaderboard and code can be found at https://github.com/Emanual20/adbench-leaderboard.

</details>


### [51] [Detecting LLM Hallucinations via Embedding Cluster Geometry: A Three-Type Taxonomy with Measurable Signatures](https://arxiv.org/abs/2602.14259)
*Matic Korun*

Main category: cs.CL

TL;DR: 该研究基于token嵌入聚类结构提出了大语言模型幻觉的几何分类法，识别出三种幻觉类型，并引入三种可测量的几何统计量来量化模型嵌入空间的几何特性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解大语言模型幻觉的几何基础，通过分析嵌入空间的聚类结构来识别不同类型的幻觉模式，为幻觉检测提供可测量的几何指标。

Method: 分析了11个transformer模型的静态嵌入空间（包括BERT、RoBERTa、ELECTRA、DeBERTa、ALBERT、MiniLM、DistilBERT等编码器架构和GPT-2解码器架构），提出了三种几何统计量：α（极性耦合）、β（聚类凝聚度）和λ_s（径向信息梯度），并识别了三种幻觉类型。

Result: 在所有11个模型中，极性结构（α > 0.5）是普遍的（11/11），聚类凝聚度（β > 0）是普遍的（11/11），径向信息梯度显著（9/11，p < 0.05）。ALBERT和MiniLM未能达到λ_s显著性，分别由于因子化嵌入压缩和蒸馏诱导的各向同性。

Conclusion: 研究建立了特定类型幻觉检测的几何先决条件，并产生了关于架构依赖性脆弱性特征的可测试预测，为理解模型幻觉提供了几何基础。

Abstract: We propose a geometric taxonomy of large language model hallucinations based on observable signatures in token embedding cluster structure. By analyzing the static embedding spaces of 11 transformer models spanning encoder (BERT, RoBERTa, ELECTRA, DeBERTa, ALBERT, MiniLM, DistilBERT) and decoder (GPT-2) architectures, we identify three operationally distinct hallucination types: Type 1 (center-drift) under weak context, Type 2 (wrong-well convergence) to locally coherent but contextually incorrect cluster regions, and Type 3 (coverage gaps) where no cluster structure exists. We introduce three measurable geometric statistics: α (polarity coupling), \b{eta} (cluster cohesion), and λ_s (radial information gradient). Across all 11 models, polarity structure (α > 0.5) is universal (11/11), cluster cohesion (\b{eta} > 0) is universal (11/11), and the radial information gradient is significant (9/11, p < 0.05). We demonstrate that the two models failing λ_s significance -- ALBERT and MiniLM -- do so for architecturally explicable reasons: factorized embedding compression and distillation-induced isotropy, respectively. These findings establish the geometric prerequisites for type-specific hallucination detection and yield testable predictions about architecture-dependent vulnerability profiles.

</details>


### [52] [STATe-of-Thoughts: Structured Action Templates for Tree-of-Thoughts](https://arxiv.org/abs/2602.14265)
*Zachary Bamberger,Till R. Saenger,Gilad Morad,Ofra Amir,Brandon M. Stewart,Amir Feder*

Main category: cs.CL

TL;DR: STATe-of-Thoughts (STATe) 是一个可解释的推理时计算方法，通过离散的文本干预取代随机采样，使用控制器选择高层推理模式，生成器产生推理步骤，评估器指导搜索，从而生成高质量、多样且可解释的文本。


<details>
  <summary>Details</summary>
Motivation: 现有推理时计算方法（如Best-of-N、Tree-of-Thoughts）使用高温采样难以实现有意义的输出多样性，且对推理过程的控制有限，限制了可解释性。需要一种既能产生高质量多样化输出，又能提供解释性推理控制的方法。

Method: STATe采用结构化三组件框架：1) 控制器选择编码高层推理选择的动作；2) 生成器基于这些选择产生推理步骤；3) 评估器对候选结果评分以指导搜索。该方法用离散可解释的文本干预取代随机采样。

Result: 1) 动作引导的文本干预比基于温度的采样产生更大的响应多样性；2) 在论点生成案例中，STATe的显式动作序列捕获了高度预测输出质量的可解释特征；3) 能够识别动作空间中未探索但有前景的区域，并直接引导生成朝向这些区域。

Conclusion: STATe为生成高质量、多样且可解释的文本提供了一个实用框架，通过结构化搜索高层推理模式，解决了现有推理时计算方法在多样性和可解释性方面的局限性。

Abstract: Inference-Time-Compute (ITC) methods like Best-of-N and Tree-of-Thoughts are meant to produce output candidates that are both high-quality and diverse, but their use of high-temperature sampling often fails to achieve meaningful output diversity. Moreover, existing ITC methods offer limited control over how to perform reasoning, which in turn limits their explainability. We present STATe-of-Thoughts (STATe), an interpretable ITC method that searches over high-level reasoning patterns. STATe replaces stochastic sampling with discrete and interpretable textual interventions: a controller selects actions encoding high-level reasoning choices, a generator produces reasoning steps conditioned on those choices, and an evaluator scores candidates to guide search. This structured approach yields three main advantages. First, action-guided textual interventions produce greater response diversity than temperature-based sampling. Second, in a case study on argument generation, STATe's explicit action sequences capture interpretable features that are highly predictive of output quality. Third, estimating the association between performance and action choices allows us to identify promising yet unexplored regions of the action space and steer generation directly toward them. Together, these results establish STATe as a practical framework for generating high-quality, diverse, and interpretable text. Our framework is available at https://github.com/zbambergerNLP/state-of-thoughts.

</details>


### [53] [InnoEval: On Research Idea Evaluation as a Knowledge-Grounded, Multi-Perspective Reasoning Problem](https://arxiv.org/abs/2602.14367)
*Shuofei Qiao,Yunxiang Wei,Xuehai Wang,Bin Wu,Boyang Xue,Ningyu Zhang,Hossein A. Rahmani,Yanshan Wang,Qiang Zhang,Keyan Ding,Jeff Z. Pan,Huajun Chen,Emine Yilmaz*

Main category: cs.CL

TL;DR: InnoEval是一个深度创新评估框架，通过异构知识检索和多视角评审委员会来模拟人类专家评估，解决了LLM在科学创意评估中的知识局限性和偏见问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型促进了科学创意的产生，但创意评估方法滞后，存在知识视野狭窄、评估维度单一以及LLM作为评判者的固有偏见等问题，需要更接近人类专家水平的评估框架。

Method: 将创意评估视为知识基础的多视角推理问题，采用异构深度知识搜索引擎从多样在线资源检索动态证据，并通过包含不同学术背景评审员的创新评审委员会实现多维度解耦评估。

Result: 在基于权威同行评审提交构建的数据集上，InnoEval在点对点、成对和组级评估任务中持续优于基线方法，其判断模式和共识与人类专家高度一致。

Conclusion: InnoEval框架通过知识基础和多元视角的评审机制，有效模拟了人类专家的创新评估能力，为科学创意评估提供了更可靠、更接近人类专家水平的解决方案。

Abstract: The rapid evolution of Large Language Models has catalyzed a surge in scientific idea production, yet this leap has not been accompanied by a matching advance in idea evaluation. The fundamental nature of scientific evaluation needs knowledgeable grounding, collective deliberation, and multi-criteria decision-making. However, existing idea evaluation methods often suffer from narrow knowledge horizons, flattened evaluation dimensions, and the inherent bias in LLM-as-a-Judge. To address these, we regard idea evaluation as a knowledge-grounded, multi-perspective reasoning problem and introduce InnoEval, a deep innovation evaluation framework designed to emulate human-level idea assessment. We apply a heterogeneous deep knowledge search engine that retrieves and grounds dynamic evidence from diverse online sources. We further achieve review consensus with an innovation review board containing reviewers with distinct academic backgrounds, enabling a multi-dimensional decoupled evaluation across multiple metrics. We construct comprehensive datasets derived from authoritative peer-reviewed submissions to benchmark InnoEval. Experiments demonstrate that InnoEval can consistently outperform baselines in point-wise, pair-wise, and group-wise evaluation tasks, exhibiting judgment patterns and consensus highly aligned with human experts.

</details>


### [54] [Beyond Token-Level Policy Gradients for Complex Reasoning with Large Language Models](https://arxiv.org/abs/2602.14386)
*Mufan Xu,Kehai Chen,Xuefeng Bai,Zhengyu Niu,Muyun Yang,Tiejun Zhao,Min Zhang*

Main category: cs.CL

TL;DR: 提出MPO框架，将连续K个token作为统一语义动作进行策略梯度优化，解决复杂推理任务中token级优化与块级推理结构不匹配的问题。


<details>
  <summary>Details</summary>
Motivation: 现有自回归语言模型的策略梯度方法通常一次选择一个token作为动作，但复杂推理任务中单个语义决策往往跨越多个token（如定义变量、组合方程），导致token级优化与推理的块级本质不匹配。

Method: 提出多token策略梯度优化（MPO）框架，将连续K个token序列视为统一的语义动作，从块级视角捕捉推理轨迹的组合结构，支持对连贯、更高级目标的优化。

Result: 在数学推理和编码基准测试中，MPO优于标准的token级策略梯度基线，突显了token级策略梯度在复杂推理中的局限性。

Conclusion: MPO通过块级优化更好地匹配复杂推理的结构，为推理密集型语言任务超越token级粒度提供了方向，激励未来研究关注更高级的语义单元。

Abstract: Existing policy-gradient methods for auto-regressive language models typically select subsequent tokens one at a time as actions in the policy. While effective for many generation tasks, such an approach may not fully capture the structure of complex reasoning tasks, where a single semantic decision is often realized across multiple tokens--for example, when defining variables or composing equations. This introduces a potential mismatch between token-level optimization and the inherently block-level nature of reasoning in these settings. To bridge this gap, we propose Multi-token Policy Gradient Optimization (MPO), a framework that treats sequences of K consecutive tokens as unified semantic actions. This block-level perspective enables our method to capture the compositional structure of reasoning trajectories and supports optimization over coherent, higher-level objectives. Experiments on mathematical reasoning and coding benchmarks show that MPO outperforms standard token-level policy gradient baselines, highlight the limitations of token-level policy gradients for complex reasoning, motivating future research to look beyond token-level granularity for reasoning-intensive language tasks.

</details>


### [55] [TruthStance: An Annotated Dataset of Conversations on Truth Social](https://arxiv.org/abs/2602.14406)
*Fathima Ameen,Danielle Brown,Manusha Malgareddy,Amanul Haque*

Main category: cs.CL

TL;DR: TruthStance数据集：包含Truth Social平台2023-2025年的大规模对话线程数据（24,378帖文+523,360评论），提供人工标注的论证挖掘和立场检测基准，并评估LLM标注策略。


<details>
  <summary>Details</summary>
Motivation: 现有公开资源主要关注Twitter和Reddit等主流平台，而alt-tech平台（如Truth Social）的对话结构研究相对不足，需要专门的数据集来理解这些平台上的意见形成和辩论模式。

Method: 1) 构建TruthStance数据集，包含Truth Social平台2023-2025年的对话线程，保留回复树结构；2) 人工标注1,500个实例用于论证挖掘和基于主张的立场检测；3) 评估不同LLM提示策略的性能；4) 使用最佳配置生成大规模LLM标注。

Result: 1) 发布包含24,378帖文和523,360评论的大规模数据集；2) 提供1,500个实例的人工标注基准及标注者间一致性；3) 确定最佳LLM提示策略；4) 发布LLM生成的额外标注：24,352帖文的论证存在性和107,873评论对父帖的立场。

Conclusion: TruthStance填补了alt-tech平台对话结构研究的空白，为分析立场和论证模式（跨深度、主题和用户）提供了宝贵资源，所有代码和数据均已公开。

Abstract: Argument mining and stance detection are central to understanding how opinions are formed and contested in online discourse. However, most publicly available resources focus on mainstream platforms such as Twitter and Reddit, leaving conversational structure on alt-tech platforms comparatively under-studied. We introduce TruthStance, a large-scale dataset of Truth Social conversation threads spanning 2023-2025, consisting of 24,378 posts and 523,360 comments with reply-tree structure preserved. We provide a human-annotated benchmark of 1,500 instances across argument mining and claim-based stance detection, including inter-annotator agreement, and use it to evaluate large language model (LLM) prompting strategies. Using the best-performing configuration, we release additional LLM-generated labels for 24,352 posts (argument presence) and 107,873 comments (stance to parent), enabling analysis of stance and argumentation patterns across depth, topics, and users. All code and data are released publicly.

</details>


### [56] [WavePhaseNet: A DFT-Based Method for Constructing Semantic Conceptual Hierarchy Structures (SCHS)](https://arxiv.org/abs/2602.14419)
*Kiyotaka Kasubuchi,Kazuo Fukiya*

Main category: cs.CL

TL;DR: 论文从测度论和频率分析角度重构Transformer/Attention机制，理论证明幻觉是不可避免的结构性限制，并提出WavePhaseNet方法通过DFT分解语义信息、降维和上同调一致性控制来抑制幻觉。


<details>
  <summary>Details</summary>
Motivation: 论文的动机是解决LLM中的幻觉问题。作者从理论层面发现，传统Transformer的嵌入空间作为条件期望无法与语义真值集同构，这导致了逻辑一致性崩溃和幻觉的必然性。因此需要从根本上重构表示机制。

Method: 提出WavePhaseNet方法：1) 使用DFT沿序列维度分解语义信息为频率带（低频捕获全局意义，高频捕获局部语法）；2) 通过累积能量分析将GPT-4的24,576维嵌入空间降维至约3,000维；3) 通过上同调正则化构建图结构和上链复形，利用Hodge理论进行谐波投影控制语义一致性。

Result: 理论证明了幻觉是LLM的结构性限制；通过DFT实现了语义信息的精确分解和操作；发现3,000维是"完整表示"的下界，降维能保留意义和意图；上同调一致性控制能提取最大一致性的全局表示并抑制幻觉。

Conclusion: 论文从测度论和频率分析角度为LLM幻觉问题提供了理论解释和解决方案。WavePhaseNet通过语义概念层次结构、降维和上同调一致性控制，实现了对语义表示的精确操控和幻觉抑制，为构建更可靠的LLM提供了新框架。

Abstract: This paper reformulates Transformer/Attention mechanisms in Large Language Models (LLMs) through measure theory and frequency analysis, theoretically demonstrating that hallucination is an inevitable structural limitation. The embedding space functions as a conditional expectation over a σ-algebra, and its failure to be isomorphic to the semantic truth set fundamentally causes logical consistency breakdown. WavePhaseNet Method The authors propose WavePhaseNet, which explicitly constructs a Semantic Conceptual Hierarchy Structure (SCHS) using Discrete Fourier Transform (DFT). By applying DFT along the sequence dimension, semantic information is decomposed into frequency bands: low-frequency components capture global meaning and intent, while high-frequency components represent local syntax and expression. This staged separation enables precise semantic manipulation in diagonalized space. Dimensionality Reduction GPT-4's 24,576-dimensional embedding space exhibits a 1/f spectral structure based on language self-similarity and Zipf's law. Through cumulative energy analysis, the authors derive that approximately 3,000 dimensions constitute the lower bound for "complete representation." This demonstrates that reduction from 24,576 to 3,000 dimensions preserves meaning and intent while enabling rigorous reasoning and suppressing hallucination. Cohomological Consistency Control The reduced embedding space, constructed via cohomological regularization over overlapping local windows, allows defining a graph structure and cochain complex. This quantifies inconsistencies among local inferences as coboundary-based losses. Applying harmonic projection based on Hodge theory positions cohomology as a computable regularization principle for controlling semantic consistency, extracting maximally consistent global representations.

</details>


### [57] [LLM-Guided Knowledge Distillation for Temporal Knowledge Graph Reasoning](https://arxiv.org/abs/2602.14428)
*Wang Xing,Wei Song,Siyu Lin,Chen Wu,Man Wang*

Main category: cs.CL

TL;DR: 提出LLM辅助的蒸馏框架，用于时序知识图谱推理，结合传统教师模型和LLM作为辅助教师，提升轻量学生模型的性能而不增加推理复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有时序知识图谱推理模型计算量大、部署成本高，而现有的压缩和蒸馏技术主要针对静态图设计，直接应用于时序场景会忽略时间依赖的交互，导致性能下降。

Method: 提出LLM辅助的蒸馏框架：1) 使用传统高容量时序教师模型；2) 引入大型语言模型作为辅助教师，提供丰富的背景知识和时序信号；3) 采用分阶段对齐策略，逐步整合两个教师的指导；4) 联合优化监督和蒸馏目标。

Result: 在多个公共TKG基准测试和不同骨干架构上的实验表明，该方法在链接预测性能上持续优于强蒸馏基线，同时保持学生模型的紧凑和高效。

Conclusion: 大型语言模型可以作为有效的教师，将时序推理能力转移到资源高效的TKG系统中，展示了LLM在时序知识图谱蒸馏中的潜力。

Abstract: Temporal knowledge graphs (TKGs) support reasoning over time-evolving facts, yet state-of-the-art models are often computationally heavy and costly to deploy. Existing compression and distillation techniques are largely designed for static graphs; directly applying them to temporal settings may overlook time-dependent interactions and lead to performance degradation. We propose an LLM-assisted distillation framework specifically designed for temporal knowledge graph reasoning. Beyond a conventional high-capacity temporal teacher, we incorporate a large language model as an auxiliary instructor to provide enriched supervision. The LLM supplies broad background knowledge and temporally informed signals, enabling a lightweight student to better model event dynamics without increasing inference-time complexity. Training is conducted by jointly optimizing supervised and distillation objectives, using a staged alignment strategy to progressively integrate guidance from both teachers. Extensive experiments on multiple public TKG benchmarks with diverse backbone architectures demonstrate that the proposed approach consistently improves link prediction performance over strong distillation baselines, while maintaining a compact and efficient student model. The results highlight the potential of large language models as effective teachers for transferring temporal reasoning capability to resource-efficient TKG systems.

</details>


### [58] [Robust Bias Evaluation with FilBBQ: A Filipino Bias Benchmark for Question-Answering Language Models](https://arxiv.org/abs/2602.14466)
*Lance Calvin Lim Gamboa,Yue Feng,Mark Lee*

Main category: cs.CL

TL;DR: FilBBQ是BBQ基准的菲律宾语扩展版本，包含超过10,000个提示，用于评估生成模型在菲律宾语境下的性别歧视和恐同偏见，并改进了评估协议以提高可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着自然语言生成成为语言模型的流行用例，BBQ基准在评估生成模型刻板印象方面变得重要。但现有基准主要针对英语，需要扩展到其他语言和文化语境，特别是菲律宾语，以评估模型在菲律宾文化背景下的偏见。

Method: 通过四阶段开发过程构建FilBBQ：1) 模板分类；2) 文化感知翻译；3) 新模板构建；4) 提示生成。采用改进的评估协议，考虑模型响应不稳定性，通过多个种子运行获取响应并平均偏差分数。

Result: FilBBQ包含超过10,000个提示，成功应用于菲律宾语训练模型。结果显示：1) 不同种子间的偏差分数存在变异性；2) 模型存在与情绪、家务、刻板同性恋兴趣和一夫多妻制相关的性别歧视和恐同偏见。

Conclusion: FilBBQ扩展了BBQ基准的语言范围，提供了针对菲律宾语境的偏见评估工具。改进的评估协议提高了可靠性，揭示了模型在菲律宾文化背景下的具体偏见模式，为偏见缓解提供了基础。

Abstract: With natural language generation becoming a popular use case for language models, the Bias Benchmark for Question-Answering (BBQ) has grown to be an important benchmark format for evaluating stereotypical associations exhibited by generative models. We expand the linguistic scope of BBQ and construct FilBBQ through a four-phase development process consisting of template categorization, culturally aware translation, new template construction, and prompt generation. These processes resulted in a bias test composed of more than 10,000 prompts which assess whether models demonstrate sexist and homophobic prejudices relevant to the Philippine context. We then apply FilBBQ on models trained in Filipino but do so with a robust evaluation protocol that improves upon the reliability and accuracy of previous BBQ implementations. Specifically, we account for models' response instability by obtaining prompt responses across multiple seeds and averaging the bias scores calculated from these distinctly seeded runs. Our results confirm both the variability of bias scores across different seeds and the presence of sexist and homophobic biases relating to emotion, domesticity, stereotyped queer interests, and polygamy. FilBBQ is available via GitHub.

</details>


### [59] [Measuring and Mitigating Post-hoc Rationalization in Reverse Chain-of-Thought Generation](https://arxiv.org/abs/2602.14469)
*Guangyue Peng,Zongchao Chen,Wen Luo,Yuntao Wen,Wei Li,Ruixiang Feng,Ran Le,Chen Yang,Zhenwei An,Yang Song,Tao Zhang,Houfeng Wang*

Main category: cs.CL

TL;DR: 论文提出SSR方法解决思维链生成中的后合理化问题，通过先构建答案无关的结构骨架再生成完整推理，减少认知锚定效应。


<details>
  <summary>Details</summary>
Motivation: 传统反向思维链生成(RCG)方法存在后合理化问题：模型看到答案后会以答案为中心生成解释，产生认知锚定效应，导致解释质量下降。

Method: 提出结构骨架引导推理(SSR)：1) 先生成答案无关的功能性结构骨架；2) 用骨架引导完整推理生成。还提出蒸馏版SSR-D，通过微调确保结构一致性。

Result: SSR-D在开放推理基准上比抑制基线提升达10%，同时保持OOD泛化能力。能有效减少词汇、熵和概率三个层面的锚定效应。

Conclusion: 通过结构规划而非答案监控的信息流重定向，SSR能有效打破后合理化循环，为高质量推理生成提供了新方法。

Abstract: Reverse Chain-of-Thought Generation (RCG) synthesizes reasoning traces from query-answer pairs, but runs the risk of producing post-hoc rationalizations: when models can see the answer during generation, the answer serves as a cognitive anchor that shapes the entire explanation. We formalize this phenomenon through a three-level measurement hierarchy: lexical, entropic, and probabilistic anchoring, each captures surface artifacts, entropy dynamics, and latent answer dependence, respectively. We analyze semantic suppression, the intuitive mitigation strategy that instructs models to ignore the answer, to find out its counterproduction: while it reduces lexical overlap, it paradoxically increases entropic and probabilistic anchoring. Drawing on Ironic Process Theory from cognitive psychology, we attribute this failure to active monitoring of the forbidden answer, which inadvertently deepens dependence on it. To break this cycle, we propose Structural Skeleton-guided Reasoning (SSR), a two-phase approach that first generates an answer-invariant functional skeleton structure, then uses this skeleton to guide full trace generation. By redirecting the information flow to structural planning rather than answer monitoring, SSR consistently reduces anchoring across all three levels. We further introduce Distilled SSR (SSR-D), which fine-tunes models on teacher-generated SSR traces to ensure reliable structural adherence. Experiments across open-ended reasoning benchmarks demonstrate that SSR-D achieves up to 10% improvement over suppression baselines while preserving out-of-distribution (OOD) generalization.

</details>


### [60] [HyperRAG: Reasoning N-ary Facts over Hypergraphs for Retrieval Augmented Generation](https://arxiv.org/abs/2602.14470)
*Wen-Sheng Lien,Yu-Kai Chan,Hao-Lung Hsiao,Bo-Kai Ruan,Meng-Fen Chiang,Chien-An Chen,Yi-Ren Yeh,Hong-Han Shuai*

Main category: cs.CL

TL;DR: HyperRAG：基于n元超图的检索增强生成框架，通过HyperRetriever和HyperMemory两种检索变体，实现更高效、准确的多跳问答


<details>
  <summary>Details</summary>
Motivation: 传统基于知识图谱的RAG方法存在检索方案僵化、相似性搜索引入无关上下文、计算开销大、关系表达能力有限等问题。n元超图能编码高阶关系事实，捕获更丰富的实体间依赖关系，实现更浅层、更高效的推理路径

Method: 提出HyperRAG框架，包含两种互补的检索变体：1) HyperRetriever学习n元事实上的结构语义推理，构建查询条件化的关系链；2) HyperMemory利用LLM参数化记忆引导beam search，动态评分n元事实和实体进行查询感知的路径扩展

Result: 在WikiTopics（11个闭域数据集）和三个开放域QA基准（HotpotQA、MuSiQue、2WikiMultiHopQA）上验证有效性。HyperRetriever达到最高答案准确率，平均MRR提升2.95%，Hits@10提升1.23%

Conclusion: HyperRAG通过自适应、可解释的n元链构建弥补推理差距，在开放和闭域QA中均表现出色，为基于超图的RAG方法提供了有效解决方案

Abstract: Graph-based retrieval-augmented generation (RAG) methods, typically built on knowledge graphs (KGs) with binary relational facts, have shown promise in multi-hop open-domain QA. However, their rigid retrieval schemes and dense similarity search often introduce irrelevant context, increase computational overhead, and limit relational expressiveness. In contrast, n-ary hypergraphs encode higher-order relational facts that capture richer inter-entity dependencies and enable shallower, more efficient reasoning paths. To address this limitation, we propose HyperRAG, a RAG framework tailored for n-ary hypergraphs with two complementary retrieval variants: (i) HyperRetriever learns structural-semantic reasoning over n-ary facts to construct query-conditioned relational chains. It enables accurate factual tracking, adaptive high-order traversal, and interpretable multi-hop reasoning under context constraints. (ii) HyperMemory leverages the LLM's parametric memory to guide beam search, dynamically scoring n-ary facts and entities for query-aware path expansion. Extensive evaluations on WikiTopics (11 closed-domain datasets) and three open-domain QA benchmarks (HotpotQA, MuSiQue, and 2WikiMultiHopQA) validate HyperRAG's effectiveness. HyperRetriever achieves the highest answer accuracy overall, with average gains of 2.95% in MRR and 1.23% in Hits@10 over the strongest baseline. Qualitative analysis further shows that HyperRetriever bridges reasoning gaps through adaptive and interpretable n-ary chain construction, benefiting both open and closed-domain QA.

</details>


### [61] [BETA-Labeling for Multilingual Dataset Construction in Low-Resource IR](https://arxiv.org/abs/2602.14488)
*Md. Najib Hasan,Mst. Jannatun Ferdous Rain,Fyad Mohammed,Nazmul Siddique*

Main category: cs.CL

TL;DR: 该研究提出了一个用于孟加拉语信息检索的BETA标注框架，使用多个LLM标注器创建高质量数据集，并探索了通过机器翻译跨语言重用低资源语言IR数据集的有效性。


<details>
  <summary>Details</summary>
Motivation: 低资源语言信息检索面临高质量任务特定标注数据集稀缺的问题。手动标注昂贵且难以扩展，而使用LLM作为自动标注器存在标签可靠性、偏见和评估有效性等担忧。

Method: 采用BETA标注框架，涉及来自不同模型家族的多个LLM标注器，包含上下文对齐、一致性检查和多数同意机制，随后进行人工评估验证标签质量。同时通过一跳机器翻译探索跨语言数据集重用，使用LLM翻译在多个语言对上进行实验。

Result: 研究发现不同语言之间存在显著差异，反映了语言依赖性偏见和不一致的语义保留，直接影响跨语言数据集重用的可靠性。研究为低资源语言IR提供了LLM辅助数据集创建的潜力和局限性的实证证据。

Conclusion: 本研究强调了LLM辅助低资源语言IR数据集创建的潜力和局限性，提供了跨语言数据集重用风险的实证证据，并为构建更可靠的基准测试和评估流程提供了实用指导。

Abstract: IR in low-resource languages remains limited by the scarcity of high-quality, task-specific annotated datasets. Manual annotation is expensive and difficult to scale, while using large language models (LLMs) as automated annotators introduces concerns about label reliability, bias, and evaluation validity. This work presents a Bangla IR dataset constructed using a BETA-labeling framework involving multiple LLM annotators from diverse model families. The framework incorporates contextual alignment, consistency checks, and majority agreement, followed by human evaluation to verify label quality. Beyond dataset creation, we examine whether IR datasets from other low-resource languages can be effectively reused through one-hop machine translation. Using LLM-based translation across multiple language pairs, we experimented on meaning preservation and task validity between source and translated datasets. Our experiment reveal substantial variation across languages, reflecting language-dependent biases and inconsistent semantic preservation that directly affect the reliability of cross-lingual dataset reuse. Overall, this study highlights both the potential and limitations of LLM-assisted dataset creation for low-resource IR. It provides empirical evidence of the risks associated with cross-lingual dataset reuse and offers practical guidance for constructing more reliable benchmarks and evaluation pipelines in low-resource language settings.

</details>


### [62] [Query as Anchor: Scenario-Adaptive User Representation via Large Language Model](https://arxiv.org/abs/2602.14492)
*Jiahao Yuan,Yike Xu,Jinyong Wen,Baokun Wang,Ziyi Gao,Xiaotong Lin,Yun Liu,Xing Fu,Yu Cheng,Yongchao Liu,Weiqiang Wang,Zhongle Xie*

Main category: cs.CL

TL;DR: 提出Query-as-Anchor框架，将用户建模从静态编码转向动态查询感知合成，通过多模态行为序列与用户理解语义对齐的预训练数据集UserU，以及结合分层编码器和双塔LLM的Q-Anchor嵌入架构，实现工业级用户表示学习。


<details>
  <summary>Details</summary>
Motivation: 工业级用户表示学习需要在鲁棒通用性和任务敏感性之间平衡。现有方法主要产生静态、任务无关的嵌入，难以在统一向量空间中协调下游场景的不同需求，且异构多源数据引入噪声和模态冲突，降低表示质量。

Method: 1) 构建工业级预训练数据集UserU，对齐多模态行为序列与用户理解语义；2) Q-Anchor嵌入架构：将分层粗到细编码器集成到双塔LLM中，通过联合对比-自回归优化实现查询感知用户表示；3) 基于聚类的软提示调优，增强判别性潜在结构，对齐模型注意力与场景特定模态；4) 部署时在序列末端锚定查询，实现KV缓存加速推理。

Result: 在10个支付宝工业基准测试中取得一致的SOTA性能，展示强大可扩展性和高效部署能力。在支付宝生产系统的两个真实场景中进行大规模在线A/B测试，进一步验证了实际有效性。

Conclusion: Query-as-Anchor框架成功解决了工业级用户表示学习中通用性与任务敏感性的平衡问题，通过动态查询感知合成和高效部署策略，在实际工业应用中表现出色，代码将公开发布。

Abstract: Industrial-scale user representation learning requires balancing robust universality with acute task-sensitivity. However, existing paradigms primarily yield static, task-agnostic embeddings that struggle to reconcile the divergent requirements of downstream scenarios within unified vector spaces. Furthermore, heterogeneous multi-source data introduces inherent noise and modality conflicts, degrading representation. We propose Query-as-Anchor, a framework shifting user modeling from static encoding to dynamic, query-aware synthesis. To empower Large Language Models (LLMs) with deep user understanding, we first construct UserU, an industrial-scale pre-training dataset that aligns multi-modal behavioral sequences with user understanding semantics, and our Q-Anchor Embedding architecture integrates hierarchical coarse-to-fine encoders into dual-tower LLMs via joint contrastive-autoregressive optimization for query-aware user representation. To bridge the gap between general pre-training and specialized business logic, we further introduce Cluster-based Soft Prompt Tuning to enforce discriminative latent structures, effectively aligning model attention with scenario-specific modalities. For deployment, anchoring queries at sequence termini enables KV-cache-accelerated inference with negligible incremental latency. Evaluations on 10 Alipay industrial benchmarks show consistent SOTA performance, strong scalability, and efficient deployment. Large-scale online A/B testing in Alipay's production system across two real-world scenarios further validates its practical effectiveness. Our code is prepared for public release and will be available at: https://github.com/JhCircle/Q-Anchor.

</details>


### [63] [Beyond Translation: Evaluating Mathematical Reasoning Capabilities of LLMs in Sinhala and Tamil](https://arxiv.org/abs/2602.14517)
*Sukumar Kishanthan,Kumar Thushalika,Buddhi Jayasekara,Asela Hevapathige*

Main category: cs.CL

TL;DR: LLMs在英语中展现强大数学推理能力，但在僧伽罗语和泰米尔语等低资源语言中，这种能力是否反映真正的多语言推理还是依赖翻译处理尚不清楚。研究发现基础算术推理能跨语言稳健迁移，但复杂推理任务在这些语言中显著退化。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探究LLMs在低资源语言（如僧伽罗语和泰米尔语）中的数学推理能力是否真正反映了多语言推理能力，还是仅仅依赖隐式翻译到英语进行处理。这关系到对LLMs多语言能力的真实评估。

Method: 使用六种数学问题类型的分类法（从基础算术到复杂单位冲突和优化问题），评估四个主要的大型语言模型。为避免翻译伪影混淆语言能力与翻译质量，构建了平行数据集，其中每个问题都由所有三种语言的流利说话者（具有数学训练背景）原生创作。

Result: 分析表明，基础算术推理能稳健地跨语言迁移，但复杂推理任务在泰米尔语和僧伽罗语中显示出显著退化。失败模式因模型和问题类型而异，表明表面上的多语言能力可能并不反映跨语言的统一推理能力。

Conclusion: 这些发现挑战了常见假设，即表现出强大多语言性能的模型能够跨语言同等有效地推理，并强调了在多语言环境中进行细粒度、类型感知评估的必要性。

Abstract: Large language models (LLMs) demonstrate strong mathematical reasoning in English, but whether these capabilities reflect genuine multilingual reasoning or reliance on translation-based processing in low-resource languages like Sinhala and Tamil remains unclear. We examine this fundamental question by evaluating whether LLMs genuinely reason mathematically in these languages or depend on implicit translation to English-like representations. Using a taxonomy of six math problem types, from basic arithmetic to complex unit conflict and optimization problems, we evaluate four prominent large language models. To avoid translation artifacts that confound language ability with translation quality, we construct a parallel dataset where each problem is natively authored by fluent speakers with mathematical training in all three languages. Our analysis demonstrates that while basic arithmetic reasoning transfers robustly across languages, complex reasoning tasks show significant degradation in Tamil and Sinhala. The pattern of failures varies by model and problem type, suggesting that apparent multilingual competence may not reflect uniform reasoning capabilities across languages. These findings challenge the common assumption that models exhibiting strong multilingual performance can reason equally effectively across languages, and highlight the need for fine-grained, type-aware evaluation in multilingual settings.

</details>


### [64] [Explainable Token-level Noise Filtering for LLM Fine-tuning Datasets](https://arxiv.org/abs/2602.14536)
*Yuchen Yang,Wenze Lin,Enhao Huang,Zhixuan Chu,Hongbin Zhou,Lan Tao,Yiming Li,Zhan Qin,Kui Ren*

Main category: cs.CL

TL;DR: XTF：一种可解释的token级噪声过滤框架，通过分解token对微调过程的贡献为三个属性（推理重要性、知识新颖性、任务相关性），过滤噪声token以提升LLM微调性能


<details>
  <summary>Details</summary>
Motivation: 当前LLM微调存在根本性矛盾：大多数数据集是句子级设计的，而LLM优化机制是token级的，这引入了token级噪声，对最终性能产生负面影响

Method: 提出XTF框架，将token级数据对微调过程的复杂贡献分解为三个明确属性：推理重要性、知识新颖性和任务相关性，通过评分方法评估这些属性，然后相应屏蔽选定噪声token的梯度

Result: 在数学、代码和医学三个代表性下游任务上对7个主流LLM进行实验，XTF相比常规微调能显著提升下游性能，最高达13.7%

Conclusion: 研究强调了token级数据集优化的重要性，展示了基于属性分解的策略在解释复杂训练机制方面的潜力

Abstract: Large Language Models (LLMs) have seen remarkable advancements, achieving state-of-the-art results in diverse applications. Fine-tuning, an important step for adapting LLMs to specific downstream tasks, typically involves further training on corresponding datasets. However, a fundamental discrepancy exists between current fine-tuning datasets and the token-level optimization mechanism of LLMs: most datasets are designed at the sentence-level, which introduces token-level noise, causing negative influence to final performance. In this paper, we propose XTF, an explainable token-level noise filtering framework. XTF decomposes the complex and subtle contributions of token-level data to the fine-tuning process into three distinct and explicit attributes (reasoning importance, knowledge novelty, and task relevance), which can be assessed using scoring methods, and then masks the gradients of selected noisy tokens accordingly to optimize the performance of fine-tuned LLMs. We conduct extensive experiments on three representative downstream tasks (math, code and medicine) across 7 mainstream LLMs. The results demonstrate that XTF can significantly improve downstream performance by up to 13.7% compared to regular fine-tuning. Our work highlights the importance of token-level dataset optimization, and demonstrates the potential of strategies based on attribute decomposition for explaining complex training mechanisms.

</details>


### [65] [Assessing Large Language Models for Medical QA: Zero-Shot and LLM-as-a-Judge Evaluation](https://arxiv.org/abs/2602.14564)
*Shefayat E Shams Adib,Ahmed Alfey Sani,Ekramul Alam Esham,Ajwad Abrar,Tareque Mohmud Chowdhury*

Main category: cs.CL

TL;DR: 该研究比较了2024-2025年间部署的5个LLM在医疗问答任务上的表现，使用iCliniq数据集进行零样本评估，发现更大的模型（如Llama 3.3 70B）表现更好，但Llama-4-Maverick-17B在效率与性能间取得了良好平衡。


<details>
  <summary>Details</summary>
Motivation: 评估LLM在医疗问答系统中的表现，特别是在低资源环境下的应用潜力，为医疗NLP应用提供标准化基准，帮助在模型大小、计算资源和临床效用之间找到最佳平衡。

Method: 使用包含38,000个医疗问答的iCliniq数据集，对5个LLM（Llama-3-8B-Instruct、Llama 3.2 3B、Llama 3.3 70B Instruct、Llama-4-Maverick-17B-128E-Instruct、GPT-5-mini）进行零样本评估，采用BLEU和ROUGE指标衡量性能。

Result: 更大的模型（如Llama 3.3 70B Instruct）表现优于小模型，符合临床任务中的规模效应。Llama-4-Maverick-17B展现出更具竞争力的结果，在效率与性能间取得了良好平衡，突显了实际部署中的权衡考量。

Conclusion: LLM在专业级医疗推理方面的能力不断增强，LLM支持的问答系统在真实临床环境中的可行性日益提高。该基准可为未来研究提供标准化设置，帮助在最小化模型大小和计算资源的同时最大化临床效用。

Abstract: Recently, Large Language Models (LLMs) have gained significant traction in medical domain, especially in developing a QA systems to Medical QA systems for enhancing access to healthcare in low-resourced settings. This paper compares five LLMs deployed between April 2024 and August 2025 for medical QA, using the iCliniq dataset, containing 38,000 medical questions and answers of diverse specialties. Our models include Llama-3-8B-Instruct, Llama 3.2 3B, Llama 3.3 70B Instruct, Llama-4-Maverick-17B-128E-Instruct, and GPT-5-mini. We are using a zero-shot evaluation methodology and using BLEU and ROUGE metrics to evaluate performance without specialized fine-tuning. Our results show that larger models like Llama 3.3 70B Instruct outperform smaller models, consistent with observed scaling benefits in clinical tasks. It is notable that, Llama-4-Maverick-17B exhibited more competitive results, thus highlighting evasion efficiency trade-offs relevant for practical deployment. These findings align with advancements in LLM capabilities toward professional-level medical reasoning and reflect the increasing feasibility of LLM-supported QA systems in the real clinical environments. This benchmark aims to serve as a standardized setting for future study to minimize model size, computational resources and to maximize clinical utility in medical NLP applications.

</details>


### [66] [The Wikidata Query Logs Dataset](https://arxiv.org/abs/2602.14594)
*Sebastian Walter,Hannah Bast*

Main category: cs.CL

TL;DR: WDQL数据集包含20万个Wikidata知识图谱的问答对，比现有最大数据集大6倍以上，基于真实SPARQL查询日志构建，使用智能体方法进行去匿名化、清理和验证


<details>
  <summary>Details</summary>
Motivation: 现有Wikidata数据集规模有限且多依赖模板生成查询，缺乏基于真实用户查询的大规模高质量数据集，限制了问答系统的训练效果

Method: 从Wikidata查询服务获取真实SPARQL查询日志，采用智能体方法迭代进行去匿名化、清理和验证，同时生成对应的自然语言问题

Result: 构建了包含20万问答对的WDQL数据集，规模是现有最大类似数据集的6倍以上，所有资源和代码均已开源

Conclusion: WDQL数据集为训练问答系统提供了大规模高质量资源，基于真实查询日志的方法比模板生成更具实用价值

Abstract: We present the Wikidata Query Logs (WDQL) dataset, a dataset consisting of 200k question-query pairs over the Wikidata knowledge graph. It is over 6x larger than the largest existing Wikidata datasets of similar format without relying on template-generated queries. Instead, we construct it using real-world SPARQL queries sent to the Wikidata Query Service and generate questions for them. Since these log-based queries are anonymized, and therefore often do not produce results, a significant amount of effort is needed to convert them back into meaningful SPARQL queries. To achieve this, we present an agent-based method that iteratively de-anonymizes, cleans, and verifies queries against Wikidata while also generating corresponding natural-language questions. We demonstrate the dataset's benefit for training question-answering methods. All WDQL assets, as well as the agent code, are publicly available under a permissive license.

</details>


### [67] [GradMAP: Faster Layer Pruning with Gradient Metric and Projection Compensation](https://arxiv.org/abs/2602.14649)
*Hao Liu,Guangyan Li,Wensheng Zhang,Yongqiang Tang*

Main category: cs.CL

TL;DR: GradMAP是一种高效的LLM层剪枝方法，通过梯度度量和投影补偿两阶段策略，在保持剪枝性能的同时显著提升效率（平均4倍加速）。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在显著的计算冗余，现有层剪枝方法难以同时保持剪枝性能和效率，需要一种更高效的解决方案。

Method: 提出两阶段方法：1) 基于梯度幅度的全局层重要性评估，仅需单次反向传播；2) 分析剪枝引起的均值偏移，通过投影补偿矩阵一步校正性能下降。

Result: GradMAP在剪枝速度上平均实现4倍加速，同时在性能上优于现有层剪枝方法。

Conclusion: GradMAP通过高效的梯度度量和投影补偿，成功解决了层剪枝中性能与效率难以兼顾的问题，为LLM部署提供了实用解决方案。

Abstract: Large Language Models (LLMs) exhibit strong reasoning abilities, but their high computational costs limit their practical deployment. Recent studies reveal significant redundancy in LLMs layers, making layer pruning an active research topic. Layer pruning research primarily focuses on two aspects: measuring layer importance and recovering performance after pruning. Unfortunately, the present works fail to simultaneously maintain pruning performance and efficiency. In this study, we propose GradMAP, a faster layer pruning method with \textbf{Grad}ient \textbf{M}etric \textbf{A}nd \textbf{P}rojection compensation, which consists of two stages. In the first stage, we introduce a novel metric based on gradient magnitudes, enabling a global assessment of layer importance. Note that, it requires only a single backward propagation step per pruning decision, substantially enhancing pruning efficiency. In the second stage, we first analyze the layers with the largest mean shift resulting from pruning, and then incorporate a simple yet effective projection compensation matrix to correct this drift in one step. In this way, the degradation of model performance caused by layer pruning is effectively alleviated. Extensive experiments show that GradMAP outperforms previous layer pruning methods in both pruning speed (achieving an average $4\times$ speedup) and performance.

</details>


### [68] [Is Information Density Uniform when Utterances are Grounded on Perception and Discourse?](https://arxiv.org/abs/2602.14653)
*Matteo Gay,Coleman Haley,Mario Giulianelli,Edoardo Ponti*

Main category: cs.CL

TL;DR: 该研究首次在视觉基础环境中计算分析均匀信息密度假说，发现多模态语言比纯文本语言具有更高的信息均匀性。


<details>
  <summary>Details</summary>
Motivation: 均匀信息密度假说认为说话者倾向于在话语中均匀分布信息，但先前研究仅限于纯文本输入，忽略了语言产生的感知上下文。本研究旨在探索视觉基础环境中的UID现象。

Method: 使用多语言视觉-语言模型在30种语言的图像-字幕数据和13种语言的视觉叙事数据上估计惊奇值，涵盖11个语系，比较纯文本与视觉基础环境下的信息分布。

Result: 感知基础显著平滑了信息分布，在类型多样的语言中都增加了全局和局部均匀性；在视觉叙事中，图像和语篇上下文都有额外效应，最强的惊奇值降低出现在语篇单元起始处。

Conclusion: 本研究首次模拟了生态合理的多模态语言使用中的信息流时间动态，发现基础语言表现出更高的信息均匀性，支持了上下文敏感的UID表述。

Abstract: The Uniform Information Density (UID) hypothesis posits that speakers are subject to a communicative pressure to distribute information evenly within utterances, minimising surprisal variance. While this hypothesis has been tested empirically, prior studies are limited exclusively to text-only inputs, abstracting away from the perceptual context in which utterances are produced. In this work, we present the first computational study of UID in visually grounded settings. We estimate surprisal using multilingual vision-and-language models over image-caption data in 30 languages and visual storytelling data in 13 languages, together spanning 11 families. We find that grounding on perception consistently smooths the distribution of information, increasing both global and local uniformity across typologically diverse languages compared to text-only settings. In visual narratives, grounding in both image and discourse contexts has additional effects, with the strongest surprisal reductions occurring at the onset of discourse units. Overall, this study takes a first step towards modelling the temporal dynamics of information flow in ecologically plausible, multimodal language use, and finds that grounded language exhibits greater information uniformity, supporting a context-sensitive formulation of UID.

</details>


### [69] [Breaking Data Efficiency Dilemma: A Federated and Augmented Learning Framework For Alzheimer's Disease Detection via Speech](https://arxiv.org/abs/2602.14655)
*Xiao Wei,Bin Wen,Yuqin Lin,Kai Li,Mingyang gu,Xiaobao Wang,Longbiao Wang,Jianwu Dang*

Main category: cs.CL

TL;DR: FAL-AD是一个结合联邦学习和数据增强的框架，用于解决阿尔茨海默病早期诊断中语音数据稀缺和隐私问题，通过多模态融合达到91.52%的准确率。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病早期诊断至关重要，但AI语音检测面临医疗数据稀缺和隐私障碍导致的数据效率困境。

Method: 提出FAL-AD框架：1) 基于语音转换的数据增强生成多样化病理语音样本；2) 自适应联邦学习范式最大化跨机构协作；3) 注意力跨模态融合模型实现细粒度词级对齐和声学-文本交互。

Result: 在ADReSSo数据集上达到91.52%的多模态准确率，优于所有集中式基线方法。

Conclusion: FAL-AD通过联邦学习与数据增强的协同整合，系统优化数据效率，为数据效率困境提供了实用解决方案。

Abstract: Early diagnosis of Alzheimer's Disease (AD) is crucial for delaying its progression. While AI-based speech detection is non-invasive and cost-effective, it faces a critical data efficiency dilemma due to medical data scarcity and privacy barriers. Therefore, we propose FAL-AD, a novel framework that synergistically integrates federated learning with data augmentation to systematically optimize data efficiency. Our approach delivers three key breakthroughs: First, absolute efficiency improvement through voice conversion-based augmentation, which generates diverse pathological speech samples via cross-category voice-content recombination. Second, collaborative efficiency breakthrough via an adaptive federated learning paradigm, maximizing cross-institutional benefits under privacy constraints. Finally, representational efficiency optimization by an attentive cross-modal fusion model, which achieves fine-grained word-level alignment and acoustic-textual interaction. Evaluated on ADReSSo, FAL-AD achieves a state-of-the-art multi-modal accuracy of 91.52%, outperforming all centralized baselines and demonstrating a practical solution to the data efficiency dilemma. Our source code is publicly available at https://github.com/smileix/fal-ad.

</details>


### [70] [Crowdsourcing Piedmontese to Test LLMs on Non-Standard Orthography](https://arxiv.org/abs/2602.14675)
*Gianluca Vico,Jindřich Libovický*

Main category: cs.CL

TL;DR: 创建了皮埃蒙特语（意大利西北部濒危罗曼语）的众包数据集，包含145个意大利语-皮埃蒙特语平行句，用于评估大语言模型在分词、分类和机器翻译方面的表现。


<details>
  <summary>Details</summary>
Motivation: 皮埃蒙特语是一种濒危的罗曼语，缺乏标准化资源和数据集。研究旨在填补这一空白，评估当前大语言模型在处理低资源濒危语言方面的能力。

Method: 创建了145个意大利语-皮埃蒙特语平行句的众包数据集，使用母语者自然拼写风格而非标准化拼写。使用该数据集评估多个大语言模型在分词一致性、主题分类和机器翻译任务上的表现。

Result: 皮埃蒙特语相对于高资源罗曼语存在分词惩罚，但大语言模型在分类任务上能达到接近意大利语、法语和英语的性能。机器翻译结果不对称：从皮埃蒙特语翻译到高资源语言表现良好，但从高资源语言生成皮埃蒙特语仍然困难。

Conclusion: 大语言模型在处理低资源濒危语言方面显示出潜力，特别是在分类任务上，但生成任务（如翻译到濒危语言）仍然具有挑战性。数据集和代码已公开，为皮埃蒙特语研究提供了宝贵资源。

Abstract: We present a crowdsourced dataset for Piedmontese, an endangered Romance language of northwestern Italy. The dataset comprises 145 Italian-Piedmontese parallel sentences derived from Flores+, with translations produced by speakers writing in their natural orthographic style rather than adhering to standardized conventions, along with manual word alignment. We use this resource to benchmark several large language models on tokenization parity, topic classification, and machine translation. Our analysis reveals that Piedmontese incurs a tokenization penalty relative to higher-resource Romance languages, yet LLMs achieve classification performance approaching that of Italian, French, and English. Machine translation results are asymmetric: models translate adequately from Piedmontese into high-resource languages, but generation into Piedmontese remains challenging. The dataset and code are publicly released.

</details>


### [71] [LLMStructBench: Benchmarking Large Language Model Structured Data Extraction](https://arxiv.org/abs/2602.14743)
*Sönke Tenckhoff,Mario Koddenbrock,Erik Rodner*

Main category: cs.CL

TL;DR: LLMStructBench是一个用于评估大语言模型从自然语言文本中提取结构化数据并生成有效JSON输出的新基准测试，包含多样化的人工验证解析场景，支持系统测试22个模型和5种提示策略。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏系统评估LLM在结构化数据提取和JSON生成方面能力的基准测试，需要建立能够衡量模型解析可靠性的标准化评估框架。

Method: 创建包含多样化、人工验证解析场景的开放数据集，系统测试22个模型和5种提示策略，引入同时捕捉标记级准确性和文档级有效性的互补性能指标。

Result: 研究发现选择合适的提示策略比模型大小等标准属性更重要，特别是对于较小或可靠性较低的模型能确保结构有效性，但会增加语义错误数量。

Conclusion: LLMStructBench为未来LLM在解析或ETL应用领域的研究提供了重要基准，有助于系统比较模型、规模和提示策略对解析可靠性的影响。

Abstract: We present LLMStructBench, a novel benchmark for evaluating Large Language Models (LLMs) on extracting structured data and generating valid JavaScript Object Notation (JSON) outputs from natural-language text. Our open dataset comprises diverse, manually verified parsing scenarios of varying complexity and enables systematic testing across 22 models and five prompting strategies. We further introduce complementary performance metrics that capture both token-level accuracy and document-level validity, facilitating rigorous comparison of model, size, and prompting effects on parsing reliability.
  In particular, we show that choosing the right prompting strategy is more important than standard attributes such as model size. This especially ensures structural validity for smaller or less reliable models but increase the number of semantic errors. Our benchmark suite is an step towards future research in the area of LLM applied to parsing or Extract, Transform and Load (ETL) applications.

</details>


### [72] [Rethinking the Role of LLMs in Time Series Forecasting](https://arxiv.org/abs/2602.14744)
*Xin Qiu,Junlong Tong,Yirong Sun,Yunpu Ma,Wei Zhang,Xiaoyu Shen*

Main category: cs.CL

TL;DR: 大规模研究表明LLM在时间序列预测中确实有效，尤其在跨域泛化方面表现突出，推翻了先前负面评估


<details>
  <summary>Details</summary>
Motivation: 现有研究质疑LLM在时间序列预测中的实际价值，认为其性能与不使用LLM的方法相当。本研究旨在通过大规模实验验证LLM是否真正有益于时间序列预测

Method: 进行了大规模LLM时间序列预测研究，涵盖80亿观测值、17个预测场景、4个预测范围、多种对齐策略，包括域内和跨域设置。分析了预对齐与后对齐策略，研究了预训练知识和模型架构的贡献

Result: LLM确实提升预测性能，尤其在跨域泛化方面增益显著。预对齐在90%以上任务中优于后对齐。预训练知识对分布偏移至关重要，模型架构擅长建模复杂时序动态。在大规模混合分布下，完整LLM不可或缺

Conclusion: 推翻了先前对LLM在时间序列预测中的负面评估，明确了LLM有效的条件，为有效模型设计提供了实用指导

Abstract: Large language models (LLMs) have been introduced to time series forecasting (TSF) to incorporate contextual knowledge beyond numerical signals. However, existing studies question whether LLMs provide genuine benefits, often reporting comparable performance without LLMs. We show that such conclusions stem from limited evaluation settings and do not hold at scale. We conduct a large-scale study of LLM-based TSF (LLM4TSF) across 8 billion observations, 17 forecasting scenarios, 4 horizons, multiple alignment strategies, and both in-domain and out-of-domain settings. Our results demonstrate that \emph{LLM4TS indeed improves forecasting performance}, with especially large gains in cross-domain generalization. Pre-alignment outperforming post-alignment in over 90\% of tasks. Both pretrained knowledge and model architecture of LLMs contribute and play complementary roles: pretraining is critical under distribution shifts, while architecture excels at modeling complex temporal dynamics. Moreover, under large-scale mixed distributions, a fully intact LLM becomes indispensable, as confirmed by token-level routing analysis and prompt-based improvements. Overall, Our findings overturn prior negative assessments, establish clear conditions under which LLMs are not only useful, and provide practical guidance for effective model design. We release our code at https://github.com/EIT-NLP/LLM4TSF.

</details>


### [73] [Cognitive networks reconstruct mindsets about STEM subjects and educational contexts in almost 1000 high-schoolers, University students and LLM-based digital twins](https://arxiv.org/abs/2602.14749)
*Francesco Gariboldi,Emma Franchino,Edith Haim,Gianluca Lattanzi,Alessandro Grecucci,Massimo Stella*

Main category: cs.CL

TL;DR: 该研究使用认知网络科学构建行为心智网络，分析不同群体对STEM的态度，发现科学和研究被积极看待，而数学和统计则与负面情绪和焦虑相关，存在STEM-科学认知情感失调。人类网络比GPT模型更能捕捉数学与焦虑的关联。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解STEM态度如何从概念知识、教育经验和情感的相互作用中发展。通过认知网络科学方法，重建群体心智模式，分析不同教育阶段人群对STEM领域的态度差异，特别是情感维度。

Method: 使用行为心智网络方法：节点是提示词和自由联想，边是经验关联链接，每个概念标注感知情感效价。分析N=994个观察数据（高中生、大学生、早期STEM专家），并与GPT模型生成的"数字孪生"进行比较。通过语义邻域分析关键目标概念，量化情感光环、情感轮廓、网络重叠和具体性。

Result: 科学和研究在所有学生群体中都得到积极评价，而核心定量学科（数学和统计）表现出更多负面和焦虑相关的情感光环，在数学焦虑较高的亚组中更明显。高焦虑框架比随机更抽象，表明对威胁性定量领域有更去情境化的表征。人类网络比GPT模型更能体现数学与焦虑的重叠。

Conclusion: 行为心智网络能捕捉目标领域心智模式的认知-情感特征。基于LLM的数字孪生能近似文化态度，但缺乏关键的情境敏感、经验基础的成分，无法完全复制人类教育焦虑。

Abstract: Attitudes toward STEM develop from the interaction of conceptual knowledge, educational experiences, and affect. Here we use cognitive network science to reconstruct group mindsets as behavioural forma mentis networks (BFMNs). In this case, nodes are cue words and free associations, edges are empirical associative links, and each concept is annotated with perceived valence. We analyse BFMNs from N = 994 observations spanning high school students, university students, and early-career STEM experts, alongside LLM (GPT-oss) "digital twins" prompted to emulate comparable profiles. Focusing also on semantic neighbourhoods ("frames") around key target concepts (e.g., STEM subjects or educational actors/places), we quantify frames in terms of valence auras, emotional profiles, network overlap (Jaccard similarity), and concreteness relative to null baselines. Across student groups, science and research are consistently framed positively, while their core quantitative subjects (mathematics and statistics) exhibit more negative and anxiety related auras, amplified in higher math-anxiety subgroups, evidencing a STEM-science cognitive and emotional dissonance. High-anxiety frames are also less concrete than chance, suggesting more abstract and decontextualised representations of threatening quantitative domains. Human networks show greater overlapping between mathematics and anxiety than GPT-oss. The results highlight how BFMNs capture cognitive-affective signatures of mindsets towards the target domains and indicate that LLM-based digital twins approximate cultural attitudes but miss key context-sensitive, experience-based components relevant to replicate human educational anxiety.

</details>


### [74] [Residual Connections and the Causal Shift: Uncovering a Structural Misalignment in Transformers](https://arxiv.org/abs/2602.14760)
*Jonathan Lys,Vincent Gripon,Bastien Pasdeloup,Lukas Mauch,Fabien Cardinaux,Ghouthi Boukli Hacene*

Main category: cs.CL

TL;DR: 论文发现自回归Transformer中残差连接导致输入输出对齐偏移，并提出残差衰减的轻量级缓解方法


<details>
  <summary>Details</summary>
Motivation: LLMs使用因果掩码进行并行训练，但残差连接将激活与当前标记绑定，而监督目标却是下一个标记，这可能导致信息不匹配传播，特别是当当前标记不是预测最有信息量的时候

Method: 通过解码轨迹和相似性度量在预训练LLMs中定位输入输出对齐偏移，并提出基于残差衰减的轻量级缓解策略，包括固定层干预和可学习门控机制

Result: 实验显示隐藏标记表示在网络深层从输入对齐切换到输出对齐，提出的缓解策略减轻了表示不对齐问题，并在多个基准测试中带来改进

Conclusion: 残差衰减策略为自回归Transformer提供了高效且通用的架构增强，解决了输入输出对齐偏移问题

Abstract: Large Language Models (LLMs) are trained with next-token prediction, implemented in autoregressive Transformers via causal masking for parallelism. This creates a subtle misalignment: residual connections tie activations to the current token, while supervision targets the next token, potentially propagating mismatched information if the current token is not the most informative for prediction. In this work, we empirically localize this input-output alignment shift in pretrained LLMs, using decoding trajectories over tied embedding spaces and similarity-based metrics. Our experiments reveal that the hidden token representations switch from input alignment to output alignment deep within the network. Motivated by this observation, we propose a lightweight residual-path mitigation based on residual attenuation, implemented either as a fixed-layer intervention or as a learnable gating mechanism. Experiments on multiple benchmarks show that these strategies alleviate the representation misalignment and yield improvements, providing an efficient and general architectural enhancement for autoregressive Transformers.

</details>


### [75] [Unlocking Reasoning Capability on Machine Translation in Large Language Models](https://arxiv.org/abs/2602.14763)
*Sara Rajaee,Sebastian Vincent,Alexandre Berard,Marzieh Fadaee,Kelly Marchisio,Tom Kocmi*

Main category: cs.CL

TL;DR: 研究发现推理导向大语言模型在机器翻译任务中表现不佳，提出结构化推理框架可显著提升翻译质量


<details>
  <summary>Details</summary>
Motivation: 推理导向大语言模型在数学和编程任务中表现出色，但在机器翻译任务中的效果尚未充分探索，需要研究如何让推理机制更好地服务于翻译任务

Method: 1. 系统评估多个开源和闭源推理模型在WMT24++基准上的表现；2. 分析发现MT推理轨迹高度线性，缺乏修订、自我纠正和替代翻译探索；3. 提出针对翻译任务的结构化推理框架，包括多步草稿、充分性精炼、流畅性改进和选择性迭代修订；4. 构建合成数据集并基于此对大型推理模型进行后训练

Result: 1. 启用显式推理会降低翻译质量；2. 注入更强模型的推理轨迹不能可靠提升较弱模型的性能；3. 结构化推理框架显著优于标准翻译微调和通用推理基线

Conclusion: 推理必须针对任务进行结构化才能有益于机器翻译，提出的结构化推理框架为翻译任务提供了有效的解决方案

Abstract: Reasoning-oriented large language models (RLMs) achieve strong gains on tasks such as mathematics and coding by generating explicit intermediate reasoning. However, their impact on machine translation (MT) remains underexplored. We systematically evaluate several open- and closed-weights RLMs on the WMT24++ benchmark and find that enabling explicit reasoning consistently degrades translation quality across languages and models. Analysis reveals that MT reasoning traces are highly linear, lacking revision, self-correction and exploration of alternative translations, which limits their usefulness. Furthermore, injecting higher-quality reasoning traces from stronger models does not reliably improve weaker models' performance. To address this mismatch, we propose a structured reasoning framework tailored to translation, based on multi-step drafting, adequacy refinement, fluency improvement, and selective iterative revision. We curate a synthetic dataset of dynamic structured reasoning traces and post-train a large reasoning model on this data. Experiments show significant improvements over standard translation fine-tuning and injected generic reasoning baselines. Our findings demonstrate that reasoning must be task-structured to benefit MT.

</details>


### [76] [Emergently Misaligned Language Models Show Behavioral Self-Awareness That Shifts With Subsequent Realignment](https://arxiv.org/abs/2602.14777)
*Laurène Vaugrante,Anietta Weckauff,Thilo Hagendorff*

Main category: cs.CL

TL;DR: GPT-4.1模型在错误问答对上微调后会产生毒性（涌现性错位），研究发现这些模型能够自我意识到自己的行为变化，无需上下文示例就能评估自身的有害性程度


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型的涌现性错位与行为自我意识之间的交叉关系。先前研究表明LLMs在错误问答对上微调会产生毒性，同时LLMs具有行为自我意识能力，本研究旨在探究模型是否能自我意识到自己的错位状态

Method: 对GPT-4.1模型顺序微调：首先在已知会诱导涌现性错位的数据集上微调，然后在能逆转错位的数据集上微调。评估模型在没有上下文示例的情况下是否能自我意识到行为转变，通过让模型评估自身的有害性程度

Result: 涌现性错位的模型显著比基础模型和重新对齐的模型评估自己为更有害，这表明模型具有行为自我意识，能够意识到自己的涌现性错位状态

Conclusion: 行为自我意识能够追踪模型的实际对齐状态，表明可以通过查询模型获取关于其自身安全性的信息信号，这为模型安全性评估提供了新方法

Abstract: Recent research has demonstrated that large language models (LLMs) fine-tuned on incorrect trivia question-answer pairs exhibit toxicity - a phenomenon later termed "emergent misalignment". Moreover, research has shown that LLMs possess behavioral self-awareness - the ability to describe learned behaviors that were only implicitly demonstrated in training data. Here, we investigate the intersection of these phenomena. We fine-tune GPT-4.1 models sequentially on datasets known to induce and reverse emergent misalignment and evaluate whether the models are self-aware of their behavior transitions without providing in-context examples. Our results show that emergently misaligned models rate themselves as significantly more harmful compared to their base model and realigned counterparts, demonstrating behavioral self-awareness of their own emergent misalignment. Our findings show that behavioral self-awareness tracks actual alignment states of models, indicating that models can be queried for informative signals about their own safety.

</details>


### [77] [Overthinking Loops in Agents: A Structural Risk via MCP Tools](https://arxiv.org/abs/2602.14798)
*Yohan Lee,Jisoo Jang,Seoyeon Choi,Sangyeop Kim,Seungtaek Choi*

Main category: cs.CL

TL;DR: 恶意MCP工具服务器可通过诱导过度思考循环攻击LLM代理，造成资源消耗放大和任务性能下降


<details>
  <summary>Details</summary>
Motivation: LLM代理依赖文本可见的元数据选择和链接第三方工具，这种便利性创造了供应链攻击面，恶意工具服务器可诱导代理陷入循环调用模式

Method: 将攻击形式化为结构性过度思考攻击，区别于令牌级冗长，实现14个恶意工具分布在三个服务器上，触发重复、强制细化和分心等攻击模式

Result: 攻击导致严重的资源放大（最高142.4倍令牌消耗），可降低任务结果质量，解码时简洁性控制无法可靠防止循环诱导

Conclusion: 防御机制应关注工具调用结构而非仅令牌层面，需要针对结构性过度思考攻击设计防护措施

Abstract: Tool-using LLM agents increasingly coordinate real workloads by selecting and chaining third-party tools based on text-visible metadata such as tool names, descriptions, and return messages. We show that this convenience creates a supply-chain attack surface: a malicious MCP tool server can be co-registered alongside normal tools and induce overthinking loops, where individually trivial or plausible tool calls compose into cyclic trajectories that inflate end-to-end tokens and latency without any single step looking abnormal. We formalize this as a structural overthinking attack, distinguishable from token-level verbosity, and implement 14 malicious tools across three servers that trigger repetition, forced refinement, and distraction. Across heterogeneous registries and multiple tool-capable models, the attack causes severe resource amplification (up to $142.4\times$ tokens) and can degrade task outcomes. Finally, we find that decoding-time concision controls do not reliably prevent loop induction, suggesting defenses should reason about tool-call structure rather than tokens alone.

</details>


### [78] [Physical Commonsense Reasoning for Lower-Resourced Languages and Dialects: a Study on Basque](https://arxiv.org/abs/2602.14812)
*Jaione Bengoetxea,Itziar Gonzalez-Dios,Rodrigo Agerri*

Main category: cs.CL

TL;DR: 本文介绍了首个巴斯克语非问答式物理常识推理数据集BasPhyCo，评估了多语言大语言模型在低资源语言中的物理常识推理能力，发现模型在巴斯克语（特别是方言变体）上的表现有限。


<details>
  <summary>Details</summary>
Motivation: 物理常识推理是人类智能的基本能力，但现有研究尚未考察大语言模型在低资源语言（如巴斯克语）非问答式物理常识推理任务上的表现。本文旨在填补这一研究空白。

Method: 以意大利语GITA数据集为基础，创建了首个巴斯克语非问答式物理常识推理数据集BasPhyCo，包含标准和方言两种变体。通过三个层次评估模型性能：区分合理/不合理叙述的准确性、识别导致不合理冲突元素的连贯性、确定具体物理状态的可验证性。使用多种多语言LLM以及专门针对意大利语和巴斯克语预训练的模型进行评估。

Result: 结果显示，在可验证性方面，LLM在低资源语言（如巴斯克语）中表现出有限的物理常识推理能力，特别是在处理方言变体时表现更差。

Conclusion: 大语言模型在低资源语言（特别是方言变体）的物理常识推理能力有限，需要更多针对低资源语言的研究和资源开发来提升模型性能。

Abstract: Physical commonsense reasoning represents a fundamental capability of human intelligence, enabling individuals to understand their environment, predict future events, and navigate physical spaces. Recent years have witnessed growing interest in reasoning tasks within Natural Language Processing (NLP). However, no prior research has examined the performance of Large Language Models (LLMs) on non-question-answering (non-QA) physical commonsense reasoning tasks in low-resource languages such as Basque. Taking the Italian GITA as a starting point, this paper addresses this gap by presenting BasPhyCo, the first non-QA physical commonsense reasoning dataset for Basque, available in both standard and dialectal variants. We evaluate model performance across three hierarchical levels of commonsense understanding: (1) distinguishing between plausible and implausible narratives (accuracy), (2) identifying the conflicting element that renders a narrative implausible (consistency), and (3) determining the specific physical state that creates the implausibility (verifiability). These tasks were assessed using multiple multilingual LLMs as well as models pretrained specifically for Italian and Basque. Results indicate that, in terms of verifiability, LLMs exhibit limited physical commonsense capabilities in low-resource languages such as Basque, especially when processing dialectal variants.

</details>


### [79] [Testimole-Conversational: A 30-Billion-Word Italian Discussion Board Corpus (1996-2024) for Language Modeling and Sociolinguistic Research](https://arxiv.org/abs/2602.14819)
*Matteo Rinaldi,Rossella Varvara,Viviana Patti*

Main category: cs.CL

TL;DR: 提出了一个名为"Testimole-conversational"的大规模意大利语讨论板消息语料库，包含超过300亿词元（1996-2024年），适用于意大利语大语言模型预训练和语言社会学分析。


<details>
  <summary>Details</summary>
Motivation: 需要为意大利语大语言模型预训练提供大规模、高质量的语料资源，同时满足对计算机中介通信的语言学和社会学分析需求，填补意大利语在线对话数据集的空白。

Method: 收集了1996年至2024年期间的意大利语讨论板消息，构建了一个包含超过300亿词元的大规模语料库，涵盖了丰富的计算机中介通信内容。

Result: 创建了目前最大的意大利语对话语料库之一，包含超过300亿词元，时间跨度近30年，为意大利语NLP研究和语言社会学分析提供了宝贵资源。

Conclusion: Testimole-conversational语料库是一个重要的意大利语资源，不仅支持大语言模型预训练和NLP应用，还为语言变异和数字通信社会现象的研究提供了基础，将免费向研究社区开放。

Abstract: We present "Testimole-conversational" a massive collection of discussion boards messages in the Italian language. The large size of the corpus, more than 30B word-tokens (1996-2024), renders it an ideal dataset for native Italian Large Language Models'pre-training. Furthermore, discussion boards' messages are a relevant resource for linguistic as well as sociological analysis. The corpus captures a rich variety of computer-mediated communication, offering insights into informal written Italian, discourse dynamics, and online social interaction in wide time span. Beyond its relevance for NLP applications such as language modelling, domain adaptation, and conversational analysis, it also support investigations of language variation and social phenomena in digital communication. The resource will be made freely available to the research community.

</details>


### [80] [BFS-PO: Best-First Search for Large Reasoning Models](https://arxiv.org/abs/2602.14917)
*Fiorenzo Parascandolo,Wenhui Tan,Enver Sangineto,Ruihua Song,Rita Cucchiara*

Main category: cs.CL

TL;DR: BFS-PO是一种基于最佳优先搜索的强化学习算法，旨在解决大型推理模型过度思考的问题，通过生成更短的推理链同时提高准确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（如OpenAI o1和DeepSeek-R1）在推理任务中表现出色，但存在过度思考问题：计算成本显著增加，生成冗长输出，且强化学习算法（如GRPO/DAPO）会加剧这一问题。

Method: 提出BFS-PO算法，采用最佳优先搜索探索策略，通过基于最大熵节点的回溯机制寻找最短正确答案。在训练过程中生成逐步缩短的响应，学习生成简洁的推理链。

Result: 在不同基准测试和基础LRMs上，BFS-PO能够同时提高LRM的准确性并缩短其答案长度。

Conclusion: BFS-PO算法有效缓解了大型推理模型的过度思考问题，实现了准确性和效率的双重提升。

Abstract: Large Reasoning Models (LRMs) such as OpenAI o1 and DeepSeek-R1 have shown excellent performance in reasoning tasks using long reasoning chains. However, this has also led to a significant increase of computational costs and the generation of verbose output, a phenomenon known as overthinking. The tendency to overthinking is often exacerbated by Reinforcement Learning (RL) algorithms such as GRPO/DAPO. In this paper, we propose BFS-PO, an RL algorithm which alleviates this problem using a Best-First Search exploration strategy. Specifically, BFS-PO looks for the shortest correct answer using a backtracking mechanism based on maximum entropy nodes. By generating progressively shorter responses during training, BFS-PO learns to produce concise reasoning chains. Using different benchmarks and base LRMs, we show that BFS-PO can simultaneously increase the LRM accuracy and shorten its answers.

</details>


### [81] [Tool-Aware Planning in Contact Center AI: Evaluating LLMs through Lineage-Guided Query Decomposition](https://arxiv.org/abs/2602.14955)
*Varun Nathan,Shreyas Guha,Ayush Kumar*

Main category: cs.CL

TL;DR: 提出一个面向联络中心工具感知计划生成的领域基础框架和基准，用于将业务洞察查询分解为结构化（Text2SQL/Snowflake）和非结构化（RAG/transcripts）工具的可执行步骤，支持并行化依赖关系。


<details>
  <summary>Details</summary>
Motivation: 联络中心需要处理复杂的业务洞察查询，这些查询需要分解为可执行的步骤并分配给不同的工具。现有方法在工具理解、计划分解和执行方面存在挑战，特别是在处理复合查询和多步骤计划时效果不佳。

Method: 1) 基于参考的计划评估框架，包含七维度指标评估和一次性评估两种模式；2) 数据整理方法，通过评估器->优化器循环迭代优化计划，生成高质量计划谱系；3) 大规模研究14个不同规模和家族的LLM，评估其在有无计划谱系提示下的计划分解能力。

Result: LLM在处理复合查询和超过4步的计划时表现不佳（通常需要5-15步）。最佳总指标得分84.8%（Claude-3-7-Sonnet），最强一次性匹配率仅49.75%（o3-mini）。计划谱系整体带来混合收益，但提升了多个顶级模型的性能和步骤可执行性。

Conclusion: 研究揭示了LLM在工具理解方面的持续差距，特别是在工具提示对齐和工具使用完整性方面。较短、较简单的计划明显更容易处理。该框架为评估和改进联络中心数据分析查询的代理规划提供了可复现的路径。

Abstract: We present a domain-grounded framework and benchmark for tool-aware plan generation in contact centers, where answering a query for business insights, our target use case, requires decomposing it into executable steps over structured tools (Text2SQL (T2S)/Snowflake) and unstructured tools (RAG/transcripts) with explicit depends_on for parallelism. Our contributions are threefold: (i) a reference-based plan evaluation framework operating in two modes - a metric-wise evaluator spanning seven dimensions (e.g., tool-prompt alignment, query adherence) and a one-shot evaluator; (ii) a data curation methodology that iteratively refines plans via an evaluator->optimizer loop to produce high-quality plan lineages (ordered plan revisions) while reducing manual effort; and (iii) a large-scale study of 14 LLMs across sizes and families for their ability to decompose queries into step-by-step, executable, and tool-assigned plans, evaluated under prompts with and without lineage. Empirically, LLMs struggle on compound queries and on plans exceeding 4 steps (typically 5-15); the best total metric score reaches 84.8% (Claude-3-7-Sonnet), while the strongest one-shot match rate at the "A+" tier (Extremely Good, Very Good) is only 49.75% (o3-mini). Plan lineage yields mixed gains overall but benefits several top models and improves step executability for many. Our results highlight persistent gaps in tool-understanding, especially in tool-prompt alignment and tool-usage completeness, and show that shorter, simpler plans are markedly easier. The framework and findings provide a reproducible path for assessing and improving agentic planning with tools for answering data-analysis queries in contact-center settings.

</details>


### [82] [Counterfactual Fairness Evaluation of LLM-Based Contact Center Agent Quality Assurance System](https://arxiv.org/abs/2602.14970)
*Kawin Mayilvaghanan,Siddhant Gupta,Ayush Kumar*

Main category: cs.CL

TL;DR: LLM在客服质量评估中存在系统性偏见，反事实公平性评估显示身份、上下文和行为风格维度存在显著差异，公平性提示仅能带来有限改善。


<details>
  <summary>Details</summary>
Motivation: LLM在客服质量评估中部署日益增多，但其基于网络规模训练数据可能导致人口统计和行为偏见，扭曲员工绩效评估，需要系统性公平性评估。

Method: 使用反事实公平性评估框架，涵盖身份、上下文和行为风格三大类13个维度，采用反事实翻转率(CFR)和平均绝对分数差异(MASD)量化公平性，在3000个真实客服对话记录上评估18个LLM。

Result: 发现系统性差异：CFR范围5.4%-13.0%，MASD在信心、积极和改进分数上显示一致偏移；更大、对齐更强的模型偏见较小但公平性不跟踪准确性；上下文历史表现提示导致最严重退化(CFR达16.4%)；公平性提示仅带来有限改善。

Conclusion: LLM在高风险员工评估部署前需要标准化公平性审计流程，系统性偏见问题需要更深入解决。

Abstract: Large Language Models (LLMs) are increasingly deployed in contact-center Quality Assurance (QA) to automate agent performance evaluation and coaching feedback. While LLMs offer unprecedented scalability and speed, their reliance on web-scale training data raises concerns regarding demographic and behavioral biases that may distort workforce assessment. We present a counterfactual fairness evaluation of LLM-based QA systems across 13 dimensions spanning three categories: Identity, Context, and Behavioral Style. Fairness is quantified using the Counterfactual Flip Rate (CFR), the frequency of binary judgment reversals, and the Mean Absolute Score Difference (MASD), the average shift in coaching or confidence scores across counterfactual pairs. Evaluating 18 LLMs on 3,000 real-world contact center transcripts, we find systematic disparities, with CFR ranging from 5.4% to 13.0% and consistent MASD shifts across confidence, positive, and improvement scores. Larger, more strongly aligned models show lower unfairness, though fairness does not track accuracy. Contextual priming of historical performance induces the most severe degradations (CFR up to 16.4%), while implicit linguistic identity cues remain a persistent bias source. Finally, we analyze the efficacy of fairness-aware prompting, finding that explicit instructions yield only modest improvements in evaluative consistency. Our findings underscore the need for standardized fairness auditing pipelines prior to deploying LLMs in high-stakes workforce evaluation.

</details>


### [83] [Learning User Interests via Reasoning and Distillation for Cross-Domain News Recommendation](https://arxiv.org/abs/2602.15005)
*Mengdan Zhu,Yufan Zhao,Tao Di,Yulan Yan,Liang Zhao*

Main category: cs.CL

TL;DR: 提出一个强化学习框架，使用大语言模型从跨域用户信号生成高质量的兴趣驱动新闻搜索查询列表，并通过蒸馏实现可扩展部署


<details>
  <summary>Details</summary>
Motivation: 跨域新闻推荐需要从异构信号中推断用户的深层信息需求，超越表面行为捕捉可重用的用户兴趣，同时保持大规模生产系统的可扩展性

Method: 将查询列表生成建模为策略优化问题，采用GRPO（Group Relative Policy Optimization）结合多个奖励信号，系统研究推理时采样和模型容量两个计算维度，并通过策略蒸馏将大模型知识转移到紧凑学生模型

Result: 离线实验、消融研究和大规模在线A/B测试显示，在兴趣建模质量和下游推荐性能方面均获得一致提升，增加计算资源表现出类似缩放的行为

Conclusion: 提出的强化学习框架能有效生成高质量的兴趣驱动查询列表，通过蒸馏实现可扩展部署，为跨域新闻推荐提供了有效的兴趣建模解决方案

Abstract: News recommendation plays a critical role in online news platforms by helping users discover relevant content. Cross-domain news recommendation further requires inferring user's underlying information needs from heterogeneous signals that often extend beyond direct news consumption. A key challenge lies in moving beyond surface-level behaviors to capture deeper, reusable user interests while maintaining scalability in large-scale production systems. In this paper, we present a reinforcement learning framework that trains large language models to generate high-quality lists of interest-driven news search queries from cross-domain user signals. We formulate query-list generation as a policy optimization problem and employ GRPO with multiple reward signals. We systematically study two compute dimensions: inference-time sampling and model capacity, and empirically observe consistent improvements with increased compute that exhibit scaling-like behavior. Finally, we perform on-policy distillation to transfer the learned policy from a large, compute-intensive teacher to a compact student model suitable for scalable deployment. Extensive offline experiments, ablation studies and large-scale online A/B tests in a production news recommendation system demonstrate consistent gains in both interest modeling quality and downstream recommendation performance.

</details>


### [84] [Cold-Start Personalization via Training-Free Priors from Structured World Models](https://arxiv.org/abs/2602.15012)
*Avinandan Bose,Shuyue Stella Li,Faeze Brahman,Pang Wei Koh,Simon Shaolei Du,Yulia Tsvetkov,Maryam Fazel,Lin Xiao,Asli Celikyilmaz*

Main category: cs.CL

TL;DR: Pep框架通过离线学习偏好关联结构和在线贝叶斯推理，在冷启动个性化中实现高效偏好获取，相比强化学习减少3-5倍交互次数，参数规模小3个数量级。


<details>
  <summary>Details</summary>
Motivation: 冷启动个性化需要从有限交互中推断用户偏好，但传统强化学习方法无法有效利用偏好数据的结构化特性，导致策略退化为静态问题序列，忽略用户响应。

Method: 提出Pep框架：1) 离线学习偏好关联的结构化世界模型；2) 在线进行无需训练的贝叶斯推理，选择信息量大的问题并预测完整偏好剖面，包括未询问的维度。

Result: 在医疗、数学、社交和常识推理任务中，Pep达到80.8%的生成响应与用户偏好对齐（RL为68.5%），交互次数减少3-5倍。当用户给出不同答案时，Pep调整后续问题的概率为39-62%（RL为0-28%），参数规模仅约1万（RL为80亿）。

Conclusion: 冷启动偏好获取的关键瓶颈在于利用偏好数据的结构化特性，Pep通过分解为离线结构学习和在线贝叶斯推理的模块化框架，显著提升了效率和效果。

Abstract: Cold-start personalization requires inferring user preferences through interaction when no user-specific historical data is available. The core challenge is a routing problem: each task admits dozens of preference dimensions, yet individual users care about only a few, and which ones matter depends on who is asking. With a limited question budget, asking without structure will miss the dimensions that matter. Reinforcement learning is the natural formulation, but in multi-turn settings its terminal reward fails to exploit the factored, per-criterion structure of preference data, and in practice learned policies collapse to static question sequences that ignore user responses. We propose decomposing cold-start elicitation into offline structure learning and online Bayesian inference. Pep (Preference Elicitation with Priors) learns a structured world model of preference correlations offline from complete profiles, then performs training-free Bayesian inference online to select informative questions and predict complete preference profiles, including dimensions never asked about. The framework is modular across downstream solvers and requires only simple belief models. Across medical, mathematical, social, and commonsense reasoning, Pep achieves 80.8% alignment between generated responses and users' stated preferences versus 68.5% for RL, with 3-5x fewer interactions. When two users give different answers to the same question, Pep changes its follow-up 39-62% of the time versus 0-28% for RL. It does so with ~10K parameters versus 8B for RL, showing that the bottleneck in cold-start elicitation is the capability to exploit the factored structure of preference data.

</details>


### [85] [Text Style Transfer with Parameter-efficient LLM Finetuning and Round-trip Translation](https://arxiv.org/abs/2602.15013)
*Ruoxi Liu,Philipp Koehn*

Main category: cs.CL

TL;DR: 提出基于参数高效微调大语言模型的文本风格迁移新方法，利用回译合成平行语料，结合检索增强生成提升术语和专有名词处理


<details>
  <summary>Details</summary>
Motivation: 解决文本风格迁移中平行语料稀缺的问题，传统方法需要大量成对数据，而实际应用中很难获得源风格和目标风格的对应文本

Method: 1. 使用回译技术从单语语料库合成平行数据集；2. 通过参数高效微调大语言模型；3. 创建"中性化"文本作为共享输入风格；4. 集成检索增强生成处理术语和专有名词知识

Result: 在四个研究领域中，该方法在BLEU分数和风格准确度分数上一致优于零样本提示和少样本上下文学习技术

Conclusion: 基于回译合成平行语料的参数高效微调方法能有效解决文本风格迁移中的平行数据稀缺问题，结合检索增强生成可提升术语处理的鲁棒性和风格一致性

Abstract: This paper proposes a novel method for Text Style Transfer (TST) based on parameter-efficient fine-tuning of Large Language Models (LLMs). Addressing the scarcity of parallel corpora that map between styles, the study employs roundtrip translation to synthesize such parallel datasets from monolingual corpora. This approach creates 'neutralized' text devoid of stylistic attributes, essentially creating a shared input style at training-time and inference-time. Experimental results demonstrate consistent superiority of this method over zero-shot prompting and fewshot ICL techniques measured by BLEU scores and style accuracy scores across four investigated domains. Furthermore, the integration of retrieval-augmented generation (RAG) for terminology and name knowledge enhances robustness and stylistic consistency.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [86] [Inference From Random Restarts](https://arxiv.org/abs/2602.13450)
*Moeen Nehzati,Diego Cussen*

Main category: econ.EM

TL;DR: 为随机重启启发式算法提供概率框架，通过重复收敛结果推断解的唯一性和吸引域大小


<details>
  <summary>Details</summary>
Motivation: 非凸问题中算法对初始条件敏感，实践中常用随机重启并观察重复收敛来推断解的唯一性，但缺乏正式推理基础

Method: 建立概率框架：1) 证明终端输出是初始条件的可测函数；2) 给出算法只有有限可能结果的充分条件；3) 采用贝叶斯方法从重复输出推断吸引域大小和解唯一性概率

Result: 为梯度下降、阻尼不动点迭代等标准算法提供了理论保证，建立了后验信念的收敛速率，并应用于产业组织文献中的实际案例

Conclusion: 形式化了随机重启启发式算法的推理基础，明确了重复收敛何时能提供有意义的唯一性证据，何时不能

Abstract: Algorithms for computing equilibria, optima, and fixed points in nonconvex problems often depend sensitively on practitioner-chosen initial conditions. When uniqueness of a solution is of interest, a common heuristic is to run such algorithms from many randomly selected initial conditions and to interpret repeated convergence to the same output as evidence of a unique solution or a dominant basin of attraction. Despite its widespread use, this practice lacks a formal inferential foundation.
  We provide a simple probabilistic framework for interpreting such numerical evidence. First, we give sufficient conditions under which an algorithm's terminal output is a measurable function of its initial condition, allowing probabilistic reasoning over outcomes. Second, we provide sufficient conditions ensuring that an algorithm admits only finitely many possible terminal outcomes. While these conditions may be difficult to verify on a case-by-case basis, we give simple sufficient conditions for broad classes of problems under which almost all instances admit only finitely many outcomes (in the sense of prevalence). Standard algorithms such as gradient descent and damped fixed-point iteration applied to sufficiently smooth functions satisfy these conditions.
  Within this framework, repeated solver runs correspond to independent samples from the induced distribution over outcomes. We adopt a Bayesian approach to infer basin sizes and the probability of solution uniqueness from repeated identical outputs, and we establish convergence rates for the resulting posterior beliefs. Finally, we apply our framework to settings in the existing industrial organization literature, where random-restart heuristics are used. Our results formalize and qualify these arguments, clarifying when repeated convergence provides meaningful evidence for uniqueness and when it does not.

</details>


### [87] [Post-Matching Two-Way Fixed Effects Estimation](https://arxiv.org/abs/2602.13453)
*Yihong Liu,Gonzalo Vazquez-Bare*

Main category: econ.EM

TL;DR: 该论文分析了在双向固定效应模型中使用匹配作为预处理步骤的常见做法，指出两个问题：当不同处理队列在不同时间进入处理时，匹配后的2WFE估计量存在渐近偏差；匹配过程引入的变异性导致标准误估计无效。作者提出了简单的匹配后双重差分估计量来解决这些问题。


<details>
  <summary>Details</summary>
Motivation: 研究者在估计处理效应时，通常在使用双向固定效应模型前进行匹配预处理，假设平行趋势条件于协变量成立。这种常见做法缺乏正式分析，可能存在未被认识到的统计问题。

Method: 作者首先形式化分析常见的两步法：第一步基于观测到的时不变协变量将每个处理单元匹配到一个或多个未处理单元；第二步在匹配样本中使用2WFE回归估计处理效应，并按匹配次数重新加权未处理单元。然后提出简单的匹配后双重差分估计量，分别比较每个处理队列与从未处理组，而不是将所有处理队列合并。

Result: 研究发现两个主要问题：1) 当不同处理队列在不同时间进入处理时，合并所有处理队列的匹配后2WFE估计量存在渐近偏差，即使处理效应在单元和时间上恒定；2) 忽略匹配过程引入的变异性会导致标准误估计无效，可能向上或向下偏倚。提出的匹配后双重差分估计量在特定条件下对明确定义的因果参数具有一致性，并推导了考虑匹配步骤的有效标准误。

Conclusion: 常见的匹配预处理结合2WFE回归的做法存在统计问题，特别是在处理队列异质性进入时间的情况下。作者提出的匹配后双重差分估计量提供了更可靠的替代方案，通过分别处理每个处理队列并考虑匹配不确定性，能够获得一致估计和有效推断。

Abstract: When estimating treatment effects with two-way fixed effects (2WFE) models, researchers often use matching as a pre-processing step when the parallel trends assumption is thought to hold conditionally on covariates. Specifically, in a first step, each treated unit is matched to one or more untreated units based on observed time-invariant covariates. In the second step, treatment effects are estimated with a 2WFE regression in the matched sample, reweighting the untreated units by the number of times they are matched. We formally analyze this common practice and highlight two problems. First, when different treatment cohorts enter treatment in different time periods, the post-matching 2WFE estimator that pools all treated cohorts has an asymptotic bias, even when the treatment effect is constant across units and over time. Second, failing to account for the variability introduced by the matching procedure yields invalid standard error estimators, which can be biased upwards or downwards depending on the data generating process. We propose simple post-matching difference-in-differences estimators that compare each treated cohort to the never-treated separately, instead of pooling all treated cohorts. We provide conditions under which these estimators are consistent for well-defined causal parameters, and derive valid standard errors that account for the matching step. We illustrate our results with simulations and with an empirical application.

</details>


### [88] [Cluster-Robust Inference for Quadratic Forms](https://arxiv.org/abs/2602.13537)
*Michal Kolesár,Pengjin Min,Wenjie Wang,Yichong Zhang*

Main category: econ.EM

TL;DR: 研究聚类数据下线性回归系数二次型的推断问题，提出留一聚类估计器及其方差估计方法，适用于高维协变量和强聚类依赖场景。


<details>
  <summary>Details</summary>
Motivation: 在聚类数据和高维协变量背景下，传统的插件估计器存在偏差问题。研究旨在解决三个重要应用场景：多工具变量回归、方差分量推断和线性回归多重约束检验。

Method: 提出留一聚类估计器（LOCO）来消除偏差，并开发两种方差估计方法：留三聚类方差估计器（一致性）和留二聚类方差估计器（计算简单且保守）。

Result: 留一聚类估计器是无偏的，并在一定条件下具有渐近正态性。留三聚类方差估计器在原始条件下一致，留二聚类方差估计器在较弱条件下保守且计算更简单。

Conclusion: 提出的方法能够处理聚类大小随样本量发散、强聚类内依赖以及协变量维度与样本量同阶发散的情况，为高维聚类数据下的统计推断提供了有效工具。

Abstract: This paper studies inference for quadratic forms of linear regression coefficients with clustered data and many covariates. Our framework covers three important special cases: instrumental variables regression with many instruments and controls, inference on variance components, and testing multiple restrictions in a linear regression. Naïve plug-in estimators are known to be biased. We study a leave-one-cluster-out estimator that is unbiased, and provide sufficient conditions for its asymptotic normality. For inference, we establish the consistency of a leave-three-cluster-out variance estimator under primitive conditions. In addition, we develop a novel leave-two-cluster-out variance estimator that is computationally simpler and guaranteed to be conservative under weaker conditions. Our analysis allows cluster sizes to diverge with the sample size, accommodates strong within-cluster dependence, and permits the dimension of the covariates to diverge with the sample size, potentially at the same rate.

</details>


### [89] [The Accuracy Smoothness Dilemma in Prediction: a Novel Multivariate M-SSA Forecast Approach](https://arxiv.org/abs/2602.13722)
*Marc Wildi*

Main category: econ.EM

TL;DR: M-SSA框架扩展了传统MSE方法，通过同时考虑符号准确性、MSE和预测器符号变化频率，解决了预测中的准确度-平滑度权衡问题，并扩展到多变量时间序列。


<details>
  <summary>Details</summary>
Motivation: 传统预测优化方法通常只关注单一指标（如最小化MSE），忽略了预测性能的其他重要方面，特别是符号准确性和平滑度之间的权衡。

Method: 将单变量SSA框架扩展到多变量M-SSA，利用原始准则整合多个时间序列的横截面信息，能够同时考虑符号准确性、MSE和预测器符号变化频率。

Result: M-SSA准则能够整合与AS预测性能相关的各种设计目标，有效泛化了传统的基于MSE的指标，在预测、实时信号提取和平滑三个领域展示了实用性和适应性。

Conclusion: M-SSA框架为预测建模提供了一种更全面的方法，能够有效管理预测中的内在权衡，适应不同应用场景的需求。

Abstract: Forecasting presents a complex estimation challenge, as it involves balancing multiple, often conflicting, priorities and objectives. Conventional forecast optimization methods typically emphasize a single metric--such as minimizing the mean squared error (MSE)--which may neglect other crucial aspects of predictive performance. To address this limitation, the recently developed Smooth Sign Accuracy (SSA) framework extends the traditional MSE approach by simultaneously accounting for sign accuracy, MSE, and the frequency of sign changes in the predictor. This addresses a fundamental trade-off--the so-called accuracy-smoothness (AS) dilemma--in prediction. We extend this approach to the multivariate M-SSA, leveraging the original criterion to incorporate cross-sectional information across multiple time series. As a result, the M-SSA criterion enables the integration of various design objectives related to AS forecasting performance, effectively generalizing conventional MSE-based metrics. To demonstrate its practical applicability and versatility, we explore the application of the M-SSA in three primary domains: forecasting, real-time signal extraction (nowcasting), and smoothing. These case studies illustrate the framework's capacity to adapt to different contexts while effectively managing inherent trade-offs in predictive modelling.

</details>


### [90] [Dual-Channel Closed Loop Supply Chain Competition: A Stackelberg--Nash Approach](https://arxiv.org/abs/2602.14288)
*Gurkirat Wadhwa*

Main category: econ.EM

TL;DR: 该研究开发了一个博弈论框架，分析双渠道闭环供应链中竞争制造商和零售商的定价与回收决策，识别了回收激励的可行性条件，并揭示了再制造价值、回收率和消费者惯性对最优策略的影响。


<details>
  <summary>Details</summary>
Motivation: 在消费电子和家电市场，制造商通过竞争零售商销售产品，同时依赖回收计划回收旧产品进行再制造。当企业在价格上竞争且消费者返还意愿不同时，设计此类回收计划具有挑战性。本研究旨在分析竞争性循环供应链中的定价与回收决策。

Method: 建立了一个博弈论框架，包含两个竞争制造商和两个竞争零售商的双渠道闭环供应链。制造商作为Stackelberg领导者同时确定批发价格和消费者回收奖励，零售商在零售价格上进行Nash竞争。模型整合了：(i)具有交叉价格效应的分段线性需求，(ii)确定性产品回收，(iii)管理回收产品分配的惯性响应分配机制。推导了零售商子博弈的Nash均衡和制造商的对称Stackelberg均衡。

Result: 推导了回收激励的可行性阈值，识别了企业最优提供正回收奖励的条件。结果表明，更高的再制造价值或回收率会导致制造商降低批发价格以扩大销售并获取更多回收量，而高消费者惯性会削弱主动回收的激励。数值实验验证了分析结果，展示了消费者行为、市场结构和产品替代性如何影响价格、奖励和回收量。

Conclusion: 该研究为竞争性循环供应链中设计有效的回收计划和协调定价决策提供了管理启示。企业需要综合考虑再制造价值、回收率、消费者惯性和市场竞争结构来制定最优的定价和回收策略。

Abstract: In many consumer electronics and appliance markets, manufacturers sell products through competing retailers while simultaneously relying on take-back programs to recover used items for remanufacturing. Designing such programs is challenging when firms compete on prices and consumers differ in their willingness to return products. Motivated by these settings, this paper develops a game theoretic framework to analyze pricing and take-back decisions in a dual-channel closed loop supply chain (CLSC) with two competing manufacturers and two competing retailers. Manufacturers act as Stackelberg leaders, simultaneously determining wholesale prices and consumer take-back bonuses, while retailers engage in Nash competition over retail prices. The model integrates three key elements: (i) segmented linear demand with cross-price effects, (ii) deterministic product returns, and (iii) an inertia responsiveness allocation mechanism governing the distribution of returned products between manufacturers. Closed form Nash equilibria are derived for the retailer subgame, along with symmetric Stackelberg equilibria for manufacturers. We derive a feasibility threshold for take-back incentives, identifying conditions under which firms optimally offer positive bonuses to consumers. The results further demonstrate that higher remanufacturing value or return rates lead the manufacturers to lower wholesale prices in order to expand sales and capture additional return volumes, while high consumer inertia weakens incentives for active collection. Numerical experiments illustrate and reinforce the analytical results, highlighting how consumer behavior, market structure and product substitutability influence prices, bonuses, and return volumes. Overall, the study provides managerial insights for designing effective take-back programs and coordinating pricing decisions in competitive circular supply chains.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [91] [Factor Engine: A Python Library for Systematic Financial Factor Computation and Analysis](https://arxiv.org/abs/2602.14138)
*Ata Keskin*

Main category: q-fin.CP

TL;DR: Factor Engine是一个高性能的开源Python库，用于金融因子的系统化计算和分析，具有模块化API，支持自定义因子定义，并与现代数据科学生态系统无缝集成。


<details>
  <summary>Details</summary>
Motivation: 为金融量化研究提供一个系统化、高性能的因子计算和分析工具，解决传统方法在因子定义、计算和集成方面的不足，促进因子研究的标准化和可重复性。

Method: 基于Python装饰器的模块化可扩展API设计，支持用户轻松定义自定义因子；与Stata参考实现进行对比验证；将因子应用于机器学习工作流进行交易策略开发。

Result: Factor Engine计算的错误定价因子与Stata参考实现高度相似，回测分析表现相当；实验证明这些因子在机器学习交易策略开发中具有实际应用价值。

Conclusion: Factor Engine是一个有效的金融因子计算工具，能够生成高质量的因子数据，支持量化金融研究，并具有实际应用潜力，特别是在机器学习驱动的交易策略开发中。

Abstract: Factor Engine is a high-performance, open-source Python library designed for the systematic computation and analysis of financial factors. Built around a modular and extensible API that leverages Python decorators, Factor Engine enables users to define custom factors with ease and integrates seamlessly with the modern data science ecosystem. To assess its practical effectiveness, we compare the mispricing factors computed by Factor Engine to those generated using a reference Stata implementation, finding that both approaches yield highly similar results and comparable performance in backtesting analyses. Furthermore, we experimentally apply these factors within machine learning workflows for trading strategy development, illustrating their practical utility and potential for quantitative finance research.

</details>


### [92] [Application of Quasi Monte Carlo and Global Sensitivity Analysis to Option Pricing and Greeks](https://arxiv.org/abs/2602.14354)
*Stefano Scoleri,Marco Bianchetti,Sergei Kucherenko*

Main category: q-fin.CP

TL;DR: QMC在金融衍生品定价和风险度量计算中优于标准MC，能显著提高收敛速度和稳定性，特别适用于高维模拟和希腊值计算。


<details>
  <summary>Details</summary>
Motivation: 研究如何提高金融衍生品定价和风险度量（特别是希腊值）的计算效率，解决高维蒙特卡洛模拟在风险管理中的计算负担问题。

Method: 使用Sobol低差异序列的准蒙特卡洛方法，结合布朗桥离散化或PCA构造，比较标准MC与QMC在不同复杂度金融工具上的表现，并对比有限差分法与伴随法计算希腊值。

Result: QMC在大多数情况下（包括最高维模拟）都优于MC，收敛更快更稳定；当希腊值数量较少时，有限差分法结合QMC能达到与伴随法相同的精度，同时节省大量实现工作量。

Conclusion: QMC不仅是定价的有效技术，也是计算风险度量（特别是希腊值）的高效方法，能够显著降低现代风险管理中典型的高维蒙特卡洛模拟计算负担。

Abstract: Quasi Monte Carlo (QMC) and Global Sensitivity Analysis (GSA) techniques are applied for pricing and hedging representative financial instruments of increasing complexity. We compare standard Monte Carlo (MC) vs QMC results using Sobol' low discrepancy sequences, different sampling strategies, and various analyses of performance. We find that QMC outperforms MC in most cases, including the highest-dimensional simulations, showing faster and more stable convergence. Regarding greeks computation, we compare standard approaches, based on finite differences (FD) approximations, with adjoint methods (AAD) providing evidences that, when the number of greeks is small, the FD approach combined with QMC can lead to the same accuracy as AAD, thanks to increased convergence rate and stability, thus saving a lot of implementation effort while keeping low computational cost. Using GSA, we are able to fully explain our findings in terms of reduced effective dimension of QMC simulation, allowed in most cases, but not always, by Brownian Bridge discretization or PCA construction. We conclude that, beyond pricing, QMC is a very effcient technique also for computing risk measures, greeks in particular, as it allows to reduce the computational effort of high dimensional Monte Carlo simulations typical of modern risk management.

</details>


### [93] [A Computational Framework for Financial Structures](https://arxiv.org/abs/2602.14378)
*Antonio Scala*

Main category: q-fin.CP

TL;DR: 该论文提出了一种通用的计算框架，用于表示和分析金融结构（如证券化、保险合同等），将随机现金流生成与确定性分配规则分离，使金融结构可作为可计算的经济系统进行分析。


<details>
  <summary>Details</summary>
Motivation: 金融结构（如证券化、保险合同等层次性债权体系）通常作为确定性分配机制作用于随机流入过程，但缺乏统一的计算表示方法。需要一种框架能够一致地分析不同配置下的性能和风险特征。

Method: 开发通用计算表示方法，将随机流入生成与确定性分配规则分离。分配规则、触发条件和优先级关系被表达为显式的状态依赖算子，将实现的流入映射到每个情景下的支付。

Result: 该框架使金融结构能够作为可计算的经济系统进行分析，在统一的随机环境中一致评估不同配置的性能和风险特征。虽然以结构化金融应用为动机，但框架适用于更广泛的合同和制度安排。

Conclusion: 通过提供统一的计算架构来表示和比较金融分配机制，该方法支持在不确定性下对结构设计、风险分布和合同透明度进行系统分析，为金融结构分析提供了通用框架。

Abstract: Financial structures such as securitisations, insurance contracts, and other hierarchical claims systems can be interpreted as deterministic allocation mechanisms acting on stochastic inflow processes. This paper develops a general computational representation of such structures by separating the stochastic generation of inflows from the deterministic rules governing their distribution across positions. Allocation rules, trigger conditions, and priority relations are expressed as explicit, state-dependent operators mapping realised inflows to payments under each scenario. This representation enables financial structures to be analysed as computable economic systems whose performance and risk characteristics can be evaluated consistently across alternative configurations within a unified stochastic environment. While motivated by applications in structured finance, the framework applies more broadly to contractual and institutional arrangements in which uncertain resources are allocated across ordered claims. By providing a unified computational architecture for representing and comparing such mechanisms, the approach supports systematic analysis of structural design, risk distribution, and contractual transparency under uncertainty.

</details>


<div id='q-fin.GN'></div>

# q-fin.GN [[Back]](#toc)

### [94] [LemonadeBench: Evaluating the Economic Intuition of Large Language Models in Simple Markets](https://arxiv.org/abs/2602.13209)
*Aidan Vyas*

Main category: q-fin.GN

TL;DR: LemonadeBench v0.5是一个评估LLM经济直觉、长期规划和不确定决策能力的基准测试，通过模拟柠檬水摊业务，要求模型在30天内管理库存、定价、营业时间并最大化利润。


<details>
  <summary>Details</summary>
Motivation: 需要评估大型语言模型在实际经济决策中的能力，特别是面对小企业主日常面临的挑战：管理易腐库存、定价策略、营业时间选择和长期利润最大化。

Method: 创建模拟柠檬水摊业务环境，要求模型在30天周期内管理易腐商品库存、设定价格、选择营业时间，通过六个维度分解业务效率来评估模型表现。

Result: 所有模型都实现了盈利，表现随模型复杂度显著提升，前沿模型达到了理论最优值的70%，比基础模型提高了10倍以上。但模型主要实现局部而非全局优化，在某些方面表现出色而在其他方面存在盲点。

Conclusion: LLM展现出有意义的经济决策能力，但存在局部优化模式，表明需要更全面的评估方法来理解模型在不同经济决策维度上的优势和局限性。

Abstract: We introduce LemonadeBench v0.5, a minimal benchmark for evaluating economic intuition, long-term planning, and decision-making under uncertainty in large language models (LLMs) through a simulated lemonade stand business. Models must manage inventory with expiring goods, set prices, choose operating hours, and maximize profit over a 30-day period-tasks that any small business owner faces daily. All models demonstrate meaningful economic agency by achieving profitability, with performance scaling dramatically by sophistication-from basic models earning minimal profits to frontier models capturing 70% of theoretical optimal, a greater than 10x improvement. Yet our decomposition of business efficiency across six dimensions reveals a consistent pattern: models achieve local rather than global optimization, excelling in select areas while exhibiting surprising blind spots elsewhere.

</details>


### [95] [A-H Premium and the Shanghai-Hong Kong Stock Connect](https://arxiv.org/abs/2602.14754)
*Chen Tang,Jiaqi Liu*

Main category: q-fin.GN

TL;DR: 上海-香港股票通实施导致A-H股溢价平均增加18.4%，但政策效果受市场效率影响：低效市场公司溢价增加更明显，高效市场公司影响较弱。


<details>
  <summary>Details</summary>
Motivation: 研究上海-香港股票通政策对A-H股溢价的影响，并探讨政策效果是否取决于市场效率水平。

Method: 使用2011年1月至2019年5月67家上海A-H双重上市公司的月度数据，采用两步系统GMM动态面板模型处理溢价持续性和内生性问题，市场效率通过每日高低价格区间计算的交易摩擦指标衡量。

Result: SHHK实施导致A-H溢价平均增加18.4%，但效果异质：市场效率较低的公司政策影响更强，效率较高的公司影响较弱；政策公告阶段无显著影响；安慰剂测试和替代效率指标证实了效率依赖效应的稳健性。

Conclusion: 研究结果表明信息环境在塑造资本账户自由化结果中起关键作用，市场效率条件调节政策效果。

Abstract: This paper examines how the Shanghai-Hong Kong Stock Connect (SHHK) affects the A-H share price premium and whether the policy impact depends on market efficiency. Using monthly data for 67 Shanghai-listed A-H dual-listed firms from January 2011 to May 2019, we estimate a dynamic panel model with two-step system GMM to account for premium persistence and potential endogeneity. Market efficiency is proxied by trading-friction measures derived from daily high-low price ranges. We find that the implementation of SHHK is associated with an average 18.4% increase in the A-H premium. However, this effect is heterogeneous. The marginal policy impact is stronger for firms operating in less efficient markets and weaker for those with higher efficiency, indicating that pre-existing trading frictions condition the policy outcome. We find no significant response at the announcement stage. Placebo tests and alternative efficiency measures confirm the robustness of the efficiency-dependent effect. Overall, the results highlight the role of the information environment in shaping liberalization outcomes.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [96] [Directional Concentration Uncertainty: A representational approach to uncertainty quantification for generative models](https://arxiv.org/abs/2602.13264)
*Souradeep Chattopadhyay,Brendan Kennedy,Sai Munikoti,Soumik Sarkar,Karl Pazdernik*

Main category: cs.LG

TL;DR: 提出一种基于von Mises-Fisher分布的定向集中不确定性（DCU）框架，用于量化生成模型输出的不确定性，无需任务特定启发式方法，在多模态任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有不确定性量化方法依赖固定启发式规则，难以跨任务和模态泛化，需要更灵活通用的UQ框架来提高生成模型的可靠性和鲁棒性。

Method: 提出DCU方法，基于von Mises-Fisher分布，通过测量语言模型多个生成输出在连续嵌入空间中的几何分散度来量化不确定性，无需任务特定启发式。

Result: DCU在校准水平上匹配或超越了语义熵等现有方法，在更复杂的多模态任务中表现出良好的泛化能力。

Conclusion: DCU提供了一个灵活的不确定性量化框架，具有整合到多模态和智能体框架中的潜力，为构建更可靠的生成模型提供了新途径。

Abstract: In the critical task of making generative models trustworthy and robust, methods for Uncertainty Quantification (UQ) have begun to show encouraging potential. However, many of these methods rely on rigid heuristics that fail to generalize across tasks and modalities. Here, we propose a novel framework for UQ that is highly flexible and approaches or surpasses the performance of prior heuristic methods. We introduce Directional Concentration Uncertainty (DCU), a novel statistical procedure for quantifying the concentration of embeddings based on the von Mises-Fisher (vMF) distribution. Our method captures uncertainty by measuring the geometric dispersion of multiple generated outputs from a language model using continuous embeddings of the generated outputs without any task specific heuristics. In our experiments, we show that DCU matches or exceeds calibration levels of prior works like semantic entropy (Kuhn et al., 2023) and also generalizes well to more complex tasks in multi-modal domains. We present a framework for the wider potential of DCU and its implications for integration into UQ for multi-modal and agentic frameworks.

</details>


### [97] [BLUEPRINT Rebuilding a Legacy: Multimodal Retrieval for Complex Engineering Drawings and Documents](https://arxiv.org/abs/2602.13345)
*Ethan Seefried,Ran Eldegaway,Sanjay Das,Nathaniel Blanchard,Tirthankar Ghosal*

Main category: cs.LG

TL;DR: Blueprint是一个用于大规模工程图纸检索的多模态系统，通过区域检测、OCR和混合检索技术，在77万份无标签文件上实现结构化元数据提取和跨设施搜索。


<details>
  <summary>Details</summary>
Motivation: 数十年的工程图纸和技术记录被锁在遗留档案中，元数据不一致或缺失，导致检索困难且通常需要人工操作。需要自动化系统来处理这些大规模、无标签的工程档案。

Method: 系统检测标准图纸区域，应用区域限制的VLM-based OCR，规范化标识符（如DWG、零件、设施），并通过轻量级区域级重排器融合词汇检索和密集检索。

Result: 在5k文件基准测试中，使用350个专家策划的查询，Blueprint在Success@3上获得10.1%的绝对增益，在nDCG@3上获得18.9%的相对改进，优于最强的视觉语言基线。在77万份无标签文件上成功部署。

Conclusion: Blueprint是一个有效的工程图纸检索系统，通过多模态方法显著提升检索性能。系统代码、查询和标注已发布，促进遗留工程档案的可重复评估。

Abstract: Decades of engineering drawings and technical records remain locked in legacy archives with inconsistent or missing metadata, making retrieval difficult and often manual. We present Blueprint, a layout-aware multimodal retrieval system designed for large-scale engineering repositories. Blueprint detects canonical drawing regions, applies region-restricted VLM-based OCR, normalizes identifiers (e.g., DWG, part, facility), and fuses lexical and dense retrieval with a lightweight region-level reranker. Deployed on ~770k unlabeled files, it automatically produces structured metadata suitable for cross-facility search.
  We evaluate Blueprint on a 5k-file benchmark with 350 expert-curated queries using pooled, graded (0/1/2) relevance judgments. Blueprint delivers a 10.1% absolute gain in Success@3 and an 18.9% relative improvement in nDCG@3 over the strongest vision-language baseline}, consistently outperforming across vision, text, and multimodal intents. Oracle ablations reveal substantial headroom under perfect region detection and OCR. We release all queries, runs, annotations, and code to facilitate reproducible evaluation on legacy engineering archives.

</details>


### [98] [Exploring the Performance of ML/DL Architectures on the MNIST-1D Dataset](https://arxiv.org/abs/2602.13348)
*Michael Beebe,GodsGift Uzor,Manasa Chepuri,Divya Sree Vemula,Angel Ayala*

Main category: cs.LG

TL;DR: 该研究在MNIST-1D数据集上评估了ResNet、TCN和DCNN等先进架构的性能，发现这些模型在小型结构化数据集上优于简单模型，验证了MNIST-1D作为计算受限环境下机器学习架构评估基准的实用性。


<details>
  <summary>Details</summary>
Motivation: 传统小数据集如MNIST虽然有助于快速实验，但其简单性限制了区分先进神经网络架构的能力。MNIST-1D数据集提供了序列数据的复杂性，但需要进一步探索先进架构在其上的表现。

Method: 在MNIST-1D数据集上实现并评估了ResNet、TCN和DCNN等先进架构，并与之前测试的逻辑回归、MLP、CNN、GRU等模型进行基准比较。

Result: TCN和DCNN等先进架构在MNIST-1D上表现优异，达到接近人类水平的性能；ResNet也有显著改进。这些模型在小型结构化数据集上明显优于简单模型。

Conclusion: MNIST-1D是评估计算受限环境下机器学习架构的稳健基准。架构创新对提升模型性能至关重要，为资源有限环境下的深度学习优化提供了见解。

Abstract: Small datasets like MNIST have historically been instrumental in advancing machine learning research by providing a controlled environment for rapid experimentation and model evaluation. However, their simplicity often limits their utility for distinguishing between advanced neural network architectures. To address these challenges, Greydanus et al. introduced the MNIST-1D dataset, a one-dimensional adaptation of MNIST designed to explore inductive biases in sequential data. This dataset maintains the advantages of small-scale datasets while introducing variability and complexity that make it ideal for studying advanced architectures.
  In this paper, we extend the exploration of MNIST-1D by evaluating the performance of Residual Networks (ResNet), Temporal Convolutional Networks (TCN), and Dilated Convolutional Neural Networks (DCNN). These models, known for their ability to capture sequential patterns and hierarchical features, were implemented and benchmarked alongside previously tested architectures such as logistic regression, MLPs, CNNs, and GRUs. Our experimental results demonstrate that advanced architectures like TCN and DCNN consistently outperform simpler models, achieving near-human performance on MNIST-1D. ResNet also shows significant improvements, highlighting the importance of leveraging inductive biases and hierarchical feature extraction in small structured datasets.
  Through this study, we validate the utility of MNIST-1D as a robust benchmark for evaluating machine learning architectures under computational constraints. Our findings emphasize the role of architectural innovations in improving model performance and offer insights into optimizing deep learning models for resource-limited environments.

</details>


### [99] [Evaluating LLMs in Finance Requires Explicit Bias Consideration](https://arxiv.org/abs/2602.14233)
*Yaxuan Kong,Hoyoung Lee,Yoontae Hwang,Alejandro Lopez-Lira,Bradford Levy,Dhagash Mehta,Qingsong Wen,Chanyeol Choi,Yongjae Lee,Stefan Zohren*

Main category: cs.LG

TL;DR: 金融LLM评估存在五大偏差问题，包括前瞻性偏差、幸存者偏差、叙事偏差、目标偏差和成本偏差，这些偏差会夸大性能并污染回测结果，需要建立结构有效性框架来解决。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在金融工作流程中的集成日益增多，评估实践未能跟上发展步伐。金融领域特有的偏差会夸大模型性能、污染回测结果，使得报告的结果对于任何部署声明都毫无用处。

Method: 识别了金融LLM应用中的五大常见偏差，回顾了2023-2025年的164篇论文，提出了结构有效性框架和评估清单，包含偏差诊断和未来系统设计的最低要求。

Result: 研究发现，在164篇论文中，没有任何单一偏差在超过28%的研究中被讨论，表明金融LLM系统中的偏差问题未得到足够重视。

Conclusion: 金融LLM系统中的偏差需要明确关注，在支持任何部署声明之前应强制执行结构有效性。作者提出了结构有效性框架和评估清单作为解决方案。

Abstract: Large Language Models (LLMs) are increasingly integrated into financial workflows, but evaluation practice has not kept up. Finance-specific biases can inflate performance, contaminate backtests, and make reported results useless for any deployment claim. We identify five recurring biases in financial LLM applications. They include look-ahead bias, survivorship bias, narrative bias, objective bias, and cost bias. These biases break financial tasks in distinct ways and they often compound to create an illusion of validity. We reviewed 164 papers from 2023 to 2025 and found that no single bias is discussed in more than 28 percent of studies. This position paper argues that bias in financial LLM systems requires explicit attention and that structural validity should be enforced before any result is used to support a deployment claim. We propose a Structural Validity Framework and an evaluation checklist with minimal requirements for bias diagnosis and future system design. The material is available at https://github.com/Eleanorkong/Awesome-Financial-LLM-Bias-Mitigation.

</details>


### [100] [The Speed-up Factor: A Quantitative Multi-Iteration Active Learning Performance Metric](https://arxiv.org/abs/2602.13359)
*Hannes Kath,Thiago S. Gouvêa,Daniel Sonntag*

Main category: cs.LG

TL;DR: 本文提出了一种新的主动学习评估指标——加速因子，用于量化查询方法在多轮迭代中的性能表现，相比现有指标具有更好的稳定性。


<details>
  <summary>Details</summary>
Motivation: 主动学习研究主要关注查询方法的开发，但缺乏合适的迭代过程性能评估指标。现有评估方法无法准确衡量查询方法在多轮迭代中的表现。

Method: 提出了加速因子这一量化指标，表示达到随机采样性能所需样本的比例。通过回顾八年主动学习评估文献，并在四个不同领域数据集上使用七种不同类型的查询方法进行实证评估。

Result: 实验结果验证了加速因子的理论假设，证明其能准确捕捉所需样本比例，并显示出在迭代过程中比现有最先进指标更好的稳定性。

Conclusion: 加速因子是一个有效的主动学习评估指标，能够更准确地衡量查询方法在多轮迭代中的性能，为主动学习研究提供了更好的评估工具。

Abstract: Machine learning models excel with abundant annotated data, but annotation is often costly and time-intensive. Active learning (AL) aims to improve the performance-to-annotation ratio by using query methods (QMs) to iteratively select the most informative samples. While AL research focuses mainly on QM development, the evaluation of this iterative process lacks appropriate performance metrics. This work reviews eight years of AL evaluation literature and formally introduces the speed-up factor, a quantitative multi-iteration QM performance metric that indicates the fraction of samples needed to match random sampling performance. Using four datasets from diverse domains and seven QMs of various types, we empirically evaluate the speed-up factor and compare it with state-of-the-art AL performance metrics. The results confirm the assumptions underlying the speed-up factor, demonstrate its accuracy in capturing the described fraction, and reveal its superior stability across iterations.

</details>


### [101] [Accelerated Discovery of Cryoprotectant Cocktails via Multi-Objective Bayesian Optimization](https://arxiv.org/abs/2602.13398)
*Daniel Emerson,Nora Gaby-Biegel,Purva Joshi,Yoed Rabin,Rebecca D. Sandlin,Levent Burak Kara*

Main category: cs.LG

TL;DR: 提出一个结合高通量筛选与多目标贝叶斯优化主动学习循环的数据高效框架，用于加速冷冻保护剂鸡尾酒配方设计，在减少实验次数的同时获得高质量配方。


<details>
  <summary>Details</summary>
Motivation: 设计冷冻保护剂鸡尾酒配方面临挑战：配方需要足够浓度抑制冰晶形成，又要足够低毒性保持细胞活性。这种权衡创造了一个大的多目标设计空间，传统方法依赖专家直觉或穷举实验，效率低下。

Method: 结合高通量筛选与基于多目标贝叶斯优化的主动学习循环。从初始测量数据训练概率代理模型预测浓度和活性，量化候选配方的不确定性。迭代选择预期能改善帕累托前沿的实验，在不确定性下最大化预期帕累托改进，收集新实验结果更新模型。

Result: 湿实验验证表明该方法高效发现同时实现高CPA浓度和高暴露后活性的配方。相比朴素策略和强基线，分别提高支配超体积9.5%和4.5%，同时减少达到高质量解所需的实验次数。在合成研究中，仅使用先前最先进多目标方法30%的评估次数就获得可比帕累托最优解集，相当于节省约10周实验时间。

Conclusion: 该框架仅需合适的检测方法和定义好的配方空间，可适应不同CPA库、目标定义和细胞系，加速冷冻保存技术开发。

Abstract: Designing cryoprotectant agent (CPA) cocktails for vitrification is challenging because formulations must be concentrated enough to suppress ice formation yet non-toxic enough to preserve cell viability. This tradeoff creates a large, multi-objective design space in which traditional discovery is slow, often relying on expert intuition or exhaustive experimentation. We present a data-efficient framework that accelerates CPA cocktail design by combining high-throughput screening with an active-learning loop based on multi-objective Bayesian optimization. From an initial set of measured cocktails, we train probabilistic surrogate models to predict concentration and viability and quantify uncertainty across candidate formulations. We then iteratively select the next experiments by prioritizing cocktails expected to improve the Pareto front, maximizing expected Pareto improvement under uncertainty, and update the models as new assay results are collected. Wet-lab validation shows that our approach efficiently discovers cocktails that simultaneously achieve high CPA concentrations and high post-exposure viability. Relative to a naive strategy and a strong baseline, our method improves dominated hypervolume by 9.5\% and 4.5\%, respectively, while reducing the number of experiments needed to reach high-quality solutions. In complementary synthetic studies, it recovers a comparably strong set of Pareto-optimal solutions using only 30\% of the evaluations required by the prior state-of-the-art multi-objective approach, which amounts to saving approximately 10 weeks of experimental time. Because the framework assumes only a suitable assay and defined formulation space, it can be adapted to different CPA libraries, objective definitions, and cell lines to accelerate cryopreservation development.

</details>


### [102] [Why is Normalization Preferred? A Worst-Case Complexity Theory for Stochastically Preconditioned SGD under Heavy-Tailed Noise](https://arxiv.org/abs/2602.13413)
*Yuchen Fang,James Demmel,Javad Lavaei*

Main category: cs.LG

TL;DR: 本文分析了随机预条件随机梯度下降(SPSGD)在重尾噪声下的最坏情况复杂度，比较了归一化和截断两种稳定化方法的性能差异，发现归一化能保证收敛而截断在最坏情况下可能失败。


<details>
  <summary>Details</summary>
Motivation: 研究随机预条件方法（如Adam、RMSProp、Shampoo等）在重尾噪声下的最坏情况收敛性，解释为什么在大规模模型训练中归一化比截断更受青睐。

Method: 开发了随机预条件随机梯度下降(SPSGD)及其加速变体的最坏情况复杂度理论，假设随机梯度噪声具有有限p阶矩(p∈(1,2])。提出了新的向量值Burkholder型不等式进行分析。

Result: 归一化方法在已知问题参数时收敛率为O(T^{-(p-1)/(3p-2)})，未知参数时为O(T^{-(p-1)/(2p)})，与归一化SGD的最优率匹配。截断方法在最坏情况下可能因统计依赖而无法收敛。

Conclusion: 归一化在随机预条件设置中比截断更可靠，这解释了大规模模型训练中归一化更受青睐的实证现象。提出的向量值Burkholder不等式具有独立的理论价值。

Abstract: We develop a worst-case complexity theory for stochastically preconditioned stochastic gradient descent (SPSGD) and its accelerated variants under heavy-tailed noise, a setting that encompasses widely used adaptive methods such as Adam, RMSProp, and Shampoo. We assume the stochastic gradient noise has a finite $p$-th moment for some $p \in (1,2]$, and measure convergence after $T$ iterations. While clipping and normalization are parallel tools for stabilizing training of SGD under heavy-tailed noise, there is a fundamental separation in their worst-case properties in stochastically preconditioned settings. We demonstrate that normalization guarantees convergence to a first-order stationary point at rate $\mathcal{O}(T^{-\frac{p-1}{3p-2}})$ when problem parameters are known, and $\mathcal{O}(T^{-\frac{p-1}{2p}})$ when problem parameters are unknown, matching the optimal rates for normalized SGD, respectively. In contrast, we prove that clipping may fail to converge in the worst case due to the statistical dependence between the stochastic preconditioner and the gradient estimates. To enable the analysis, we develop a novel vector-valued Burkholder-type inequality that may be of independent interest. These results provide a theoretical explanation for the empirical preference for normalization over clipping in large-scale model training.

</details>


### [103] [High-Resolution Climate Projections Using Diffusion-Based Downscaling of a Lightweight Climate Emulator](https://arxiv.org/abs/2602.13416)
*Haiwen Guan,Moein Darman,Dibyajyoti Chakraborty,Troy Arcomano,Ashesh Chattopadhyay,Romit Maulik*

Main category: cs.LG

TL;DR: 本文提出了一种基于深度学习的降尺度框架，使用概率扩散生成模型将LUCIE气候模拟器的粗分辨率输出（~300km）降尺度到25km分辨率，以支持区域气候影响评估。


<details>
  <summary>Details</summary>
Motivation: 当前数据驱动的气候模型（如LUCIE）虽然能准确模拟长期气候统计特征，但其~300km的原始分辨率不足以进行详细的区域影响评估。需要开发高分辨率降尺度方法来弥补这一缺陷。

Method: 采用基于概率扩散的生成模型，结合条件采样和后验采样框架，将LUCIE的粗分辨率输出降尺度到25km分辨率。模型在2000-2009年的约14,000个ERA5时间步长上训练，并在2010-2020年的LUCIE预测上进行评估。

Result: 提出的方法能够在保持LUCIE粗粒度动力学特征的同时，生成~28km分辨率的精细尺度气候统计特征。通过纬度平均RMSE、功率谱、概率密度函数和纬向风的第一经验正交函数等多种指标评估，模型表现良好。

Conclusion: 该深度学习降尺度框架成功地将LUCIE气候模拟器的分辨率从~300km提升到25km，为区域气候影响评估提供了高分辨率数据支持，同时保持了原始模型的物理一致性。

Abstract: The proliferation of data-driven models in weather and climate sciences has marked a significant paradigm shift, with advanced models demonstrating exceptional skill in medium-range forecasting. However, these models are often limited by long-term instabilities, climatological drift, and substantial computational costs during training and inference, restricting their broader application for climate studies. Addressing these limitations, Guan et al. (2024) introduced LUCIE, a lightweight, physically consistent climate emulator utilizing a Spherical Fourier Neural Operator (SFNO) architecture. This model is able to reproduce accurate long-term statistics including climatological mean and seasonal variability. However, LUCIE's native resolution (~300 km) is inadequate for detailed regional impact assessments. To overcome this limitation, we introduce a deep learning-based downscaling framework, leveraging probabilistic diffusion-based generative models with conditional and posterior sampling frameworks. These models downscale coarse LUCIE outputs to 25 km resolution. They are trained on approximately 14,000 ERA5 timesteps spanning 2000-2009 and evaluated on LUCIE predictions from 2010 to 2020. Model performance is assessed through diverse metrics, including latitude-averaged RMSE, power spectrum, probability density functions and First Empirical Orthogonal Function of the zonal wind. We observe that the proposed approach is able to preserve the coarse-grained dynamics from LUCIE while generating fine-scaled climatological statistics at ~28km resolution.

</details>


### [104] [Text Has Curvature](https://arxiv.org/abs/2602.13418)
*Karish Grover,Hanqing Zeng,Yinglong Xia,Christos Faloutsos,Geoffrey J. Gordon*

Main category: cs.LG

TL;DR: 本文提出Texture——一种文本原生的离散曲率信号，证明语言具有内在曲率，定义曲率计算方法，并展示其在长文本推理和检索增强生成中的实用价值。


<details>
  <summary>Details</summary>
Motivation: 语言建模越来越多地使用弯曲几何（如双曲空间表示层次结构），但一个基本科学问题仍未解决：曲率对文本本身意味着什么？本文旨在探索语言是否具有内在曲率，并建立文本原生的曲率范式。

Method: 提出Texture方法：通过Schrödinger桥协调掩码词左右上下文信念，定义离散曲率场。正曲率表示上下文聚焦意义，负曲率表示上下文发散为竞争性延续。

Result: 提供理论和经验证据证明自然语料库中的语义推理是非平坦的（语言具有内在曲率）。Texture作为通用测量和控制原语，在长上下文推理（通过曲率引导压缩）和检索增强生成（通过曲率引导路由）任务中表现优异。

Conclusion: 文本确实具有曲率，Texture建立了文本原生的曲率范式，使曲率可测量且具有实际用途，为语言几何分析提供了新视角。

Abstract: Does text have an intrinsic curvature? Language is increasingly modeled in curved geometries - hyperbolic spaces for hierarchy, mixed-curvature manifolds for compositional structure - yet a basic scientific question remains unresolved: what does curvature mean for text itself, in a way that is native to language rather than an artifact of the embedding space we choose? We argue that text does indeed have curvature, and show how to detect it, define it, and use it. To this end, we propose Texture, a text-native, word-level discrete curvature signal, and make three contributions. (a) Existence: We provide empirical and theoretical certificates that semantic inference in natural corpora is non-flat, i.e. language has inherent curvature. (b) Definition: We define Texture by reconciling left- and right-context beliefs around a masked word through a Schrodinger bridge, yielding a curvature field that is positive where context focuses meaning and negative where it fans out into competing continuations. (c) Utility: Texture is actionable: it serves as a general-purpose measurement and control primitive enabling geometry without geometric training; we instantiate it on two representative tasks, improving long-context inference through curvature-guided compression and retrieval-augmented generation through curvature-guided routing. Together, our results establish a text-native curvature paradigm, making curvature measurable and practically useful.

</details>


### [105] [Comparing Classifiers: A Case Study Using PyCM](https://arxiv.org/abs/2602.13482)
*Sadra Sabouri,Alireza Zolanvari,Sepand Haghighi*

Main category: cs.LG

TL;DR: PyCM库教程：通过两个案例展示如何对多分类器进行深度评估，强调评估指标选择会根本改变模型效能解读，需要多维评估框架来发现细微但重要的性能差异。


<details>
  <summary>Details</summary>
Motivation: 选择最优分类模型需要对模型性能有稳健全面的理解，但标准评估指标可能错过细微的性能权衡，需要更深入的评估方法。

Method: 使用PyCM库进行多分类器深度评估，通过两个不同案例场景展示评估指标选择如何影响模型效能解读。

Result: 评估指标的选择会根本性地改变对模型效能的解释，多维评估框架对于发现细微但重要的模型性能差异至关重要。

Conclusion: 需要进行多维度的模型评估来揭示标准指标可能错过的细微性能权衡，PyCM库为此提供了有效的工具支持。

Abstract: Selecting an optimal classification model requires a robust and comprehensive understanding of the performance of the model. This paper provides a tutorial on the PyCM library, demonstrating its utility in conducting deep-dive evaluations of multi-class classifiers. By examining two different case scenarios, we illustrate how the choice of evaluation metrics can fundamentally shift the interpretation of a model's efficacy. Our findings emphasize that a multi-dimensional evaluation framework is essential for uncovering small but important differences in model performance. However, standard metrics may miss these subtle performance trade-offs.

</details>


### [106] [Finding Highly Interpretable Prompt-Specific Circuits in Language Models](https://arxiv.org/abs/2602.13483)
*Gabriel Franco,Lucas M. Tassis,Azalea Rohr,Mark Crovella*

Main category: cs.LG

TL;DR: 该论文挑战了传统电路分析假设每个任务有单一稳定机制的观点，提出电路是提示特定的，并开发了ACC++方法来提取更清晰的因果信号，发现不同提示模板会诱导不同的机制，但提示可以聚类为具有相似电路的家族。


<details>
  <summary>Details</summary>
Motivation: 传统机制可解释性研究通常在任务层面识别电路，假设每个任务有单一稳定机制。本文发现这种假设可能掩盖了电路结构的重要来源：即使在固定任务中，电路也是提示特定的。需要新的方法来捕捉和分析这种提示特定的机制。

Method: 基于注意力因果通信（ACC），开发了ACC++方法，通过改进从单个前向传播中提取注意力头内更干净、更低维的因果信号。该方法不需要替代模型或激活修补，通过减少归因噪声提高电路精度。应用于GPT-2、Pythia和Gemma 2模型的间接宾语识别任务。

Result: 发现在任何模型中都没有单一的IOI电路：不同提示模板会系统性地诱导不同机制。然而，提示可以聚类为具有相似电路的提示家族，并为每个家族提出代表性电路作为分析单元。开发了自动化可解释性流程，使用ACC++信号提取人类可解释特征并构建机制解释。

Conclusion: 研究结果通过将分析单元从任务转移到提示，重新定义了电路作为有意义的研究对象，使得在存在提示特定机制的情况下能够进行可扩展的电路描述。这为理解语言模型如何解决任务提供了更精细的视角。

Abstract: Understanding the internal circuits that language models use to solve tasks remains a central challenge in mechanistic interpretability. Most prior work identifies circuits at the task level by averaging across many prompts, implicitly assuming a single stable mechanism per task. We show that this assumption can obscure a crucial source of structure: circuits are prompt-specific, even within a fixed task. Building on attention causal communication (ACC) (Franco & Crovella, 2025), we introduce ACC++, refinements that extract cleaner, lower-dimensional causal signals inside attention heads from a single forward pass. Like ACC, our approach does not require replacement models (e.g., SAEs) or activation patching; ACC++ further improves circuit precision by reducing attribution noise. Applying ACC++ to indirect object identification (IOI) in GPT-2, Pythia, and Gemma 2, we find there is no single circuit for IOI in any model: different prompt templates induce systematically different mechanisms. Despite this variation, prompts cluster into prompt families with similar circuits, and we propose a representative circuit for each family as a practical unit of analysis. Finally, we develop an automated interpretability pipeline that uses ACC++ signals to surface human-interpretable features and assemble mechanistic explanations for prompt families behavior. Together, our results recast circuits as a meaningful object of study by shifting the unit of analysis from tasks to prompts, enabling scalable circuit descriptions in the presence of prompt-specific mechanisms.

</details>


### [107] [Federated Learning of Nonlinear Temporal Dynamics with Graph Attention-based Cross-Client Interpretability](https://arxiv.org/abs/2602.13485)
*Ayse Tursucular,Ayush Mohanty,Nazal Mohamed,Nagi Gebraeel*

Main category: cs.LG

TL;DR: 提出一种联邦学习框架，用于在去中心化非线性系统中学习跨客户端的时序依赖关系，同时保证隐私和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现代工业系统中，分布式传感器监控多个子系统生成的高维时序数据，子系统间存在相互依赖关系。但在去中心化设置中，存在以下挑战：1) 原始测量数据无法共享；2) 客户端观测数据异构；3) 每个子系统使用固定的专有模型无法修改或重新训练；4) 非线性动态使得跨客户端时序依赖关系难以解释。

Method: 提出联邦学习框架：1) 每个客户端使用非线性状态空间模型将高维本地观测映射到低维潜在状态；2) 中央服务器通过图注意力网络学习基于通信潜在状态的图结构神经状态转移模型；3) 为增强可解释性，将学习到的服务器端转移模型的雅可比矩阵与注意力系数关联，提供跨客户端时序依赖关系的可解释性表征。

Result: 1) 建立了理论收敛保证，证明能够收敛到集中式oracle；2) 通过合成实验验证了框架的收敛性、可解释性、可扩展性和隐私性；3) 真实世界实验显示性能与去中心化基线方法相当。

Conclusion: 该框架成功解决了在去中心化非线性系统中学习跨客户端时序依赖关系的挑战，在保证隐私的同时提供了可解释性，并具有理论收敛保证和实际应用价值。

Abstract: Networks of modern industrial systems are increasingly monitored by distributed sensors, where each system comprises multiple subsystems generating high dimensional time series data. These subsystems are often interdependent, making it important to understand how temporal patterns at one subsystem relate to others. This is challenging in decentralized settings where raw measurements cannot be shared and client observations are heterogeneous. In practical deployments each subsystem (client) operates a fixed proprietary model that cannot be modified or retrained, limiting existing approaches. Nonlinear dynamics further make cross client temporal interdependencies difficult to interpret because they are embedded in nonlinear state transition functions. We present a federated framework for learning temporal interdependencies across clients under these constraints. Each client maps high dimensional local observations to low dimensional latent states using a nonlinear state space model. A central server learns a graph structured neural state transition model over the communicated latent states using a Graph Attention Network. For interpretability we relate the Jacobian of the learned server side transition model to attention coefficients, providing the first interpretable characterization of cross client temporal interdependencies in decentralized nonlinear systems. We establish theoretical convergence guarantees to a centralized oracle and validate the framework through synthetic experiments demonstrating convergence, interpretability, scalability and privacy. Additional real world experiments show performance comparable to decentralized baselines.

</details>


### [108] [Preventing Rank Collapse in Federated Low-Rank Adaptation with Client Heterogeneity](https://arxiv.org/abs/2602.13486)
*Fei Wu,Jia Hu,Geyong Min,Shiqiang Wang*

Main category: cs.LG

TL;DR: raFLoRA通过秩分区聚合方法解决联邦低秩自适应中的秩崩溃问题，提高模型性能并保持通信效率


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中，客户端在系统资源和数据分布上存在异质性，这促使了跨客户端的异质LoRA秩。研究发现异质FedLoRA中存在被忽视的"秩崩溃"现象，即全局更新的能量集中在最小共享秩上，导致性能不佳和对秩配置高度敏感

Method: 提出raFLoRA（秩分区聚合方法），将本地更新分解为秩分区，然后根据每个分区的有效客户端贡献进行加权聚合。通过理论分析揭示了秩崩溃的根本原因：秩无关的聚合权重与秩相关的客户端贡献不匹配

Result: 在分类和推理任务上的广泛实验表明，raFLoRA能够防止秩崩溃，提高模型性能，并在保持通信效率的同时优于最先进的FedLoRA基线方法

Conclusion: raFLoRA通过解决异质联邦低秩自适应中的秩崩溃问题，提供了一种有效的解决方案，改善了模型性能并保持了联邦学习的通信效率优势

Abstract: Federated low-rank adaptation (FedLoRA) has facilitated communication-efficient and privacy-preserving fine-tuning of foundation models for downstream tasks. In practical federated learning scenarios, client heterogeneity in system resources and data distributions motivates heterogeneous LoRA ranks across clients. We identify a previously overlooked phenomenon in heterogeneous FedLoRA, termed rank collapse, where the energy of the global update concentrates on the minimum shared rank, resulting in suboptimal performance and high sensitivity to rank configurations. Through theoretical analysis, we reveal the root cause of rank collapse: a mismatch between rank-agnostic aggregation weights and rank-dependent client contributions, which systematically suppresses higher-rank updates at a geometric rate over rounds. Motivated by this insight, we propose raFLoRA, a rank-partitioned aggregation method that decomposes local updates into rank partitions and then aggregates each partition weighted by its effective client contributions. Extensive experiments across classification and reasoning tasks show that raFLoRA prevents rank collapse, improves model performance, and preserves communication efficiency compared to state-of-the-art FedLoRA baselines.

</details>


### [109] [TrasMuon: Trust-Region Adaptive Scaling for Orthogonalized Momentum Optimizers](https://arxiv.org/abs/2602.13498)
*Peng Cheng,Jiucheng Zang,Qingnan Li,Liheng Ma,Yufei Cui,Yingxue Zhang,Boxing Chen,Ming Jian,Wen Tong*

Main category: cs.LG

TL;DR: TrasMuon提出了一种结合全局RMS校准和能量信任区域剪切的优化器，在保持Muon近等距几何特性的同时解决幅度不稳定问题，实现更快的收敛和更好的稳定性。


<details>
  <summary>Details</summary>
Motivation: Muon系列优化器使用牛顿-舒尔茨迭代正交化更新，虽然几何特性优于Adam系列，但正交化过程丢弃了幅度信息，导致训练对步长超参数敏感且容易产生高能量爆发。需要一种既能保持Muon几何优势又能稳定幅度的方法。

Method: TrasMuon通过两种机制稳定幅度：(1) 全局RMS校准，重新引入自适应缩放；(2) 基于相对能量比的信任区域剪切，将更新限制在稳定区域内，防止高能量异常值导致的不稳定。

Result: 在视觉和语言模型上的实验表明，TrasMuon比基线方法收敛更快。在没有预热阶段的实验中，TrasMuon展现出更优的稳定性和鲁棒性。

Conclusion: TrasMuon成功解决了Muon优化器的幅度不稳定问题，在保持近等距几何优势的同时，通过全局校准和信任区域剪切实现了更高效、更稳定的优化性能。

Abstract: Muon-style optimizers leverage Newton-Schulz (NS) iterations to orthogonalize updates, yielding update geometries that often outperform Adam-series methods. However, this orthogonalization discards magnitude information, rendering training sensitive to step-size hyperparameters and vulnerable to high-energy bursts. To mitigate this, we introduce TrasMuon (\textbf{T}rust \textbf{R}egion \textbf{A}daptive \textbf{S}caling \textbf{Muon}). TrasMuon preserves the near-isometric geometry of Muon while stabilizing magnitudes through (i) global RMS calibration and (ii) energy-based trust-region clipping. We demonstrate that while reintroducing adaptive scaling improves optimization efficiency, it typically exacerbates instability due to high-energy outliers. TrasMuon addresses this by defining a trust region based on relative energy ratios, confining updates to a stable zone. Empirical experiments on vision and language models demonstrate that TrasMuon converges faster than baselines. Furthermore, experiments without warmup stages confirm TrasMuon's superior stability and robustness.

</details>


### [110] [$γ$-weakly $θ$-up-concavity: Linearizable Non-Convex Optimization with Applications to DR-Submodular and OSS Functions](https://arxiv.org/abs/2602.13506)
*Mohammad Pedramfar,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: 提出γ-弱θ-上凹函数的新概念，统一了DR-子模函数和单边光滑函数，证明了这类函数可线性化，从而为离线和在线优化提供统一的近似保证。


<details>
  <summary>Details</summary>
Motivation: 优化单调非凸函数是机器学习和组合优化的基本挑战。现有研究主要关注DR-子模函数和单边光滑函数等特定类别，缺乏统一的理论框架。本文旨在提出一个更一般的函数类别，能够统一并推广这些现有概念。

Method: 引入γ-弱θ-上凹性这一新的函数性质，证明这类函数具有上线性化特性：对于任意可行点，可以构造一个线性替代函数，其增益能够以常数因子近似原始非线性目标。该近似系数仅依赖于γ、θ和可行集的几何结构。

Result: 1. 建立了统一的近似保证框架，适用于离线优化问题；2. 通过标准线性优化归约，获得了在线设置中的静态和动态遗憾界；3. 恢复了DR-子模最大化的最优近似系数；4. 显著改进了单边光滑函数在拟阵约束下的现有近似系数。

Conclusion: γ-弱θ-上凹性为单调非凸函数优化提供了一个强大的统一框架，严格推广了DR-子模函数和单边光滑函数。该框架通过线性化方法为广泛问题提供近似保证，并在多个重要场景中改进了现有结果。

Abstract: Optimizing monotone non-convex functions is a fundamental challenge across machine learning and combinatorial optimization. We introduce and study $γ$-weakly $θ$-up-concavity, a novel first-order condition that characterizes a broad class of such functions. This condition provides a powerful unifying framework, strictly generalizing both DR-submodular functions and One-Sided Smooth (OSS) functions. Our central theoretical contribution demonstrates that $γ$-weakly $θ$-up-concave functions are upper-linearizable: for any feasible point, we can construct a linear surrogate whose gains provably approximate the original non-linear objective. This approximation holds up to a constant factor, namely the approximation coefficient, dependent solely on $γ$, $θ$, and the geometry of the feasible set. This linearizability yields immediate and unified approximation guarantees for a wide range of problems. Specifically, we obtain unified approximation guarantees for offline optimization as well as static and dynamic regret bounds in online settings via standard reductions to linear optimization. Moreover, our framework recovers the optimal approximation coefficient for DR-submodular maximization and significantly improves existing approximation coefficients for OSS optimization, particularly over matroid constraints.

</details>


### [111] [Singular Vectors of Attention Heads Align with Features](https://arxiv.org/abs/2602.13524)
*Gabriel Franco,Carson Loughridge,Mark Crovella*

Main category: cs.LG

TL;DR: 论文探讨了语言模型中奇异向量与特征对齐的理论基础，证明在特定条件下奇异向量能有效识别特征表示。


<details>
  <summary>Details</summary>
Motivation: 近期研究隐含假设注意力矩阵的奇异向量可以推断特征表示，但缺乏理论依据。本文旨在探究奇异向量何时及为何能与特征对齐。

Method: 1) 在可直接观察特征的模型中验证奇异向量与特征的对齐；2) 理论分析对齐发生的条件；3) 提出稀疏注意力分解作为可测试的预测，并在真实模型中验证。

Result: 奇异向量在可观察特征的模型中稳健对齐；理论推导出对齐的条件；真实模型中稀疏注意力分解的出现符合预测，表明奇异向量可作为特征识别的可靠基础。

Conclusion: 奇异向量与特征的对齐具有理论依据，可作为语言模型中特征识别的有效方法，特别是在满足特定条件时。

Abstract: Identifying feature representations in language models is a central task in mechanistic interpretability. Several recent studies have made an implicit assumption that feature representations can be inferred in some cases from singular vectors of attention matrices. However, sound justification for this assumption is lacking. In this paper we address that question, asking: why and when do singular vectors align with features? First, we demonstrate that singular vectors robustly align with features in a model where features can be directly observed. We then show theoretically that such alignment is expected under a range of conditions. We close by asking how, operationally, alignment may be recognized in real models where feature representations are not directly observable. We identify sparse attention decomposition as a testable prediction of alignment, and show evidence that it emerges in a manner consistent with predictions in real models. Together these results suggest that alignment of singular vectors with features can be a sound and theoretically justified basis for feature identification in language models.

</details>


### [112] [QuaRK: A Quantum Reservoir Kernel for Time Series Learning](https://arxiv.org/abs/2602.13531)
*Abdallah Aaraba,Soumaya Cherkaoui,Ola Ahmad,Shengrui Wang*

Main category: cs.LG

TL;DR: QuaRK是一个端到端的量子储层计算框架，结合硬件现实的量子储层特征提取器和基于核的读出方案，用于时间序列学习，并提供学习理论保证。


<details>
  <summary>Details</summary>
Motivation: 量子储层计算为时间序列学习提供了有前景的途径，但现有研究缺乏高效、可实现的量子储层架构以及模型学习保证。需要填补这一空白。

Method: 提出QuaRK框架：1) 使用硬件现实的量子储层特征提取器，将序列样本点逐个注入量子系统；2) 通过经典阴影层析技术高效测量k-local可观测量生成紧凑特征向量；3) 采用基于核的经典读出方案，具有显式正则化和快速优化。

Result: 该框架提供了清晰的计算控制参数（电路宽度、深度和测量预算），同时保持了核方法建模非线性时间泛函的灵活性，并能扩展到高维数据。提供了依赖时间数据的泛化理论保证。

Conclusion: QuaRK为构建可靠的时间序列学习器提供了原则性指导，实验验证了其在合成beta-mixing时间序列任务上的插值和泛化行为。

Abstract: Quantum reservoir computing offers a promising route for time series learning by modelling sequential data via rich quantum dynamics while the only training required happens at the level of a lightweight classical readout. However, studies featuring efficient and implementable quantum reservoir architectures along with model learning guarantees remain scarce in the literature. To close this gap, we introduce QuaRK, an end-to-end framework that couples a hardware-realistic quantum reservoir featurizer with a kernel-based readout scheme. Given a sequence of sample points, the reservoir injects the points one after the other to yield a compact feature vector from efficiently measured k-local observables using classical shadow tomography, after which a classical kernel-based readout learns the target mapping with explicit regularization and fast optimization. The resulting pipeline exposes clear computational knobs -- circuit width and depth as well as the measurement budget -- while preserving the flexibility of kernel methods to model nonlinear temporal functionals and being scalable to high-dimensional data. We further provide learning-theoretic generalization guarantees for dependent temporal data, linking design and resource choices to finite-sample performance, thereby offering principled guidance for building reliable temporal learners. Empirical experiments validate QuaRK and illustrate the predicted interpolation and generalization behaviours on synthetic beta-mixing time series tasks.

</details>


### [113] [Testing For Distribution Shifts with Conditional Conformal Test Martingales](https://arxiv.org/abs/2602.13848)
*Shalev Shaer,Yarin Bar,Drew Prinster,Yaniv Romano*

Main category: cs.LG

TL;DR: 提出一种基于固定参考集的序列测试方法，用于检测任意分布偏移，避免了传统方法中的测试时污染问题。


<details>
  <summary>Details</summary>
Motivation: 现有conformal test martingales (CTMs)方法在检测分布偏移时存在测试时污染问题：偏移后的观测数据会进入参考集，稀释偏移证据，增加检测延迟并降低检测能力。

Method: 使用固定的空假设参考数据集，通过鲁棒的马丁格尔构造方法，在有限参考集条件下显式考虑参考分布的估计误差，确保在给定参考数据条件下的有效性。

Result: 方法实现了任意时间有效的类型I错误控制，保证渐近检测能力为1且有界的期望检测延迟。实证表明比标准CTMs检测偏移更快。

Conclusion: 该方法提供了一种强大可靠的分布偏移检测器，通过避免测试时污染问题，在保持统计有效性的同时提高了检测性能。

Abstract: We propose a sequential test for detecting arbitrary distribution shifts that allows conformal test martingales (CTMs) to work under a fixed, reference-conditional setting. Existing CTM detectors construct test martingales by continually growing a reference set with each incoming sample, using it to assess how atypical the new sample is relative to past observations. While this design yields anytime-valid type-I error control, it suffers from test-time contamination: after a change, post-shift observations enter the reference set and dilute the evidence for distribution shift, increasing detection delay and reducing power.
  In contrast, our method avoids contamination by design by comparing each new sample to a fixed null reference dataset. Our main technical contribution is a robust martingale construction that remains valid conditional on the null reference data, achieved by explicitly accounting for the estimation error in the reference distribution induced by the finite reference set. This yields anytime-valid type-I error control together with guarantees of asymptotic power one and bounded expected detection delay. Empirically, our method detects shifts faster than standard CTMs, providing a powerful and reliable distribution-shift detector.

</details>


### [114] [Fast Swap-Based Element Selection for Multiplication-Free Dimension Reduction](https://arxiv.org/abs/2602.13532)
*Nobutaka Ono*

Main category: cs.LG

TL;DR: 提出一种快速元素选择算法，通过选择输入元素的子集实现无乘法的降维，相比PCA更适用于资源受限系统。


<details>
  <summary>Details</summary>
Motivation: 降维是减少模型参数、缓解过拟合、加速训练和推理的重要技术。传统PCA依赖矩阵乘法，在资源受限系统中乘法计算本身可能成为瓶颈。元素选择通过仅选择元素子集实现降维，完全消除乘法计算成本。

Method: 通过线性回归的最小均方误差评估候选子集，预测目标向量（如分类中的one-hot标签向量）。若无明确目标，可将输入本身作为目标进行重建。使用矩阵求逆引理推导交换选中和未选中元素时目标函数变化的高效公式，执行基于交换的局部搜索，反复应用目标函数减少的交换直到无法进一步改进。

Result: 在MNIST手写数字图像上的实验证明了所提方法的有效性。

Conclusion: 提出了一种无乘法的降维方法，通过高效的元素选择算法在资源受限系统中实现有效的降维，避免了传统PCA中的乘法计算瓶颈。

Abstract: In this paper, we propose a fast algorithm for element selection, a multiplication-free form of dimension reduction that produces a dimension-reduced vector by simply selecting a subset of elements from the input. Dimension reduction is a fundamental technique for reducing unnecessary model parameters, mitigating overfitting, and accelerating training and inference. A standard approach is principal component analysis (PCA), but PCA relies on matrix multiplications; on resource-constrained systems, the multiplication count itself can become a bottleneck. Element selection eliminates this cost because the reduction consists only of selecting elements, and thus the key challenge is to determine which elements should be retained. We evaluate a candidate subset through the minimum mean-squared error of linear regression that predicts a target vector from the selected elements, where the target may be, for example, a one-hot label vector in classification. When an explicit target is unavailable, the input itself can be used as the target, yielding a reconstruction-based criterion. The resulting optimization is combinatorial, and exhaustive search is impractical. To address this, we derive an efficient formula for the objective change caused by swapping a selected and an unselected element, using the matrix inversion lemma, and we perform a swap-based local search that repeatedly applies objective-decreasing swaps until no further improvement is possible. Experiments on MNIST handwritten-digit images demonstrate the effectiveness of the proposed method.

</details>


### [115] [Out-of-Support Generalisation via Weight Space Sequence Modelling](https://arxiv.org/abs/2602.13550)
*Roussel Desmond Nzoyem*

Main category: cs.LG

TL;DR: WeightCaster：将超出支持集泛化问题重新定义为权重空间的序列建模任务，通过分区训练集为同心壳层实现高效、可解释且具有不确定性的预测


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在处理训练集范围外的样本时经常出现灾难性失败，产生不现实但过度自信的预测，这在安全关键应用中尤为危险

Method: 将超出支持集泛化问题重新定义为权重空间的序列建模任务，将训练集划分为对应的离散序列步骤的同心壳层，开发WeightCaster框架

Result: 在合成余弦数据集和真实空气质量传感器读数上的实验验证显示，性能达到或优于当前最先进方法

Conclusion: 通过增强超出分布场景的可靠性，这些结果对人工智能在安全关键应用中的更广泛采用具有重要意义

Abstract: As breakthroughs in deep learning transform key industries, models are increasingly required to extrapolate on datapoints found outside the range of the training set, a challenge we coin as out-of-support (OoS) generalisation. However, neural networks frequently exhibit catastrophic failure on OoS samples, yielding unrealistic but overconfident predictions. We address this challenge by reformulating the OoS generalisation problem as a sequence modelling task in the weight space, wherein the training set is partitioned into concentric shells corresponding to discrete sequential steps. Our WeightCaster framework yields plausible, interpretable, and uncertainty-aware predictions without necessitating explicit inductive biases, all the while maintaining high computational efficiency. Emprical validation on a synthetic cosine dataset and real-world air quality sensor readings demonstrates performance competitive or superior to the state-of-the-art. By enhancing reliability beyond in-distribution scenarios, these results hold significant implications for the wider adoption of artificial intelligence in safety-critical applications.

</details>


### [116] [Fast Catch-Up, Late Switching: Optimal Batch Size Scheduling via Functional Scaling Laws](https://arxiv.org/abs/2602.14208)
*Jinbo Wang,Binghui Li,Zhanpeng Zhou,Mingze Wang,Yuxuan Sun,Jiaqi Zhang,Xunliang Cai,Lei Wu*

Main category: cs.LG

TL;DR: 论文提出基于功能缩放定律框架分析批量大小调度，发现最优调度取决于任务难度：简单任务应持续增加批量，困难任务应保持小批量直到后期切换，并揭示了快速追赶效应机制。


<details>
  <summary>Details</summary>
Motivation: 批量大小调度在大规模深度学习训练中至关重要，但理论基础薄弱。作者希望建立批量大小调度的理论框架，解释不同任务难度的最优调度策略差异。

Method: 采用功能缩放定律框架分析批量大小调度，理论推导最优调度结构，揭示快速追赶效应机制，并通过大规模语言模型预训练实验验证（包括Dense和MoE架构，最大1.1B参数和1T tokens）。

Result: 理论分析表明：简单任务的最优调度应持续增加批量大小；困难任务的最优调度应保持小批量直到后期切换。实验验证了后期切换调度在LLM预训练中优于恒定批量和早期切换基线。

Conclusion: 功能缩放定律为批量大小调度提供了理论框架，揭示了任务难度决定最优调度结构。快速追赶效应表明大批量可以安全推迟到训练后期，既能保持性能又能显著减少数据消耗。

Abstract: Batch size scheduling (BSS) plays a critical role in large-scale deep learning training, influencing both optimization dynamics and computational efficiency. Yet, its theoretical foundations remain poorly understood. In this work, we show that the functional scaling law (FSL) framework introduced in Li et al. (2025a) provides a principled lens for analyzing BSS. Specifically, we characterize the optimal BSS under a fixed data budget and show that its structure depends sharply on task difficulty. For easy tasks, optimal schedules keep increasing batch size throughout. In contrast, for hard tasks, the optimal schedule maintains small batch sizes for most of training and switches to large batches only in a late stage. To explain the emergence of late switching, we uncover a dynamical mechanism -- the fast catch-up effect -- which also manifests in large language model (LLM) pretraining. After switching from small to large batches, the loss rapidly aligns with the constant large-batch trajectory. Using FSL, we show that this effect stems from rapid forgetting of accumulated gradient noise, with the catch-up speed determined by task difficulty. Crucially, this effect implies that large batches can be safely deferred to late training without sacrificing performance, while substantially reducing data consumption. Finally, extensive LLM pretraining experiments -- covering both Dense and MoE architectures with up to 1.1B parameters and 1T tokens -- validate our theoretical predictions. Across all settings, late-switch schedules consistently outperform constant-batch and early-switch baselines.

</details>


### [117] [Scenario-Adaptive MU-MIMO OFDM Semantic Communication With Asymmetric Neural Network](https://arxiv.org/abs/2602.13557)
*Chongyang Li,Tianqian Zhang,Shouyin Liu*

Main category: cs.LG

TL;DR: 提出一种面向6G下行MU-MIMO OFDM系统的场景自适应语义通信框架，通过CSI/SNR感知的语义编码和神经预编码抑制多用户干扰，在接收端采用导频引导的注意力机制进行隐式信道均衡，显著提升低信噪比下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度联合信源信道编码方案主要针对点对点链路设计，在多用户场景下存在性能饱和问题。将语义通信应用于实际的下行MU-MIMO OFDM系统面临严重多用户干扰和频率选择性衰落的挑战。

Method: 提出非对称架构的下行MU-MIMO语义通信框架：发射端包含场景感知的语义编码器（根据CSI和SNR动态调整特征提取）和神经预编码网络（在语义域抑制多用户干扰）；接收端采用轻量级解码器，配备导频引导的注意力机制，利用参考导频符号隐式执行信道均衡和特征校准。

Result: 在3GPP信道模型上的广泛仿真表明，该框架在PSNR和分类准确率方面显著优于DJSCC和传统分离信源信道编码方案，特别是在低信噪比区域，同时在边缘设备上保持低延迟和低计算成本。

Conclusion: 该场景自适应MU-MIMO语义通信框架有效解决了多用户干扰和频率选择性衰落问题，为6G网络中实际多用户语义通信系统的部署提供了可行方案。

Abstract: Semantic Communication (SemCom) has emerged as a promising paradigm for 6G networks, aiming to extract and transmit task-relevant information rather than minimizing bit errors. However, applying SemCom to realistic downlink Multi-User Multi-Input Multi-Output (MU-MIMO) Orthogonal Frequency Division Multiplexing (OFDM) systems remains challenging due to severe Multi-User Interference (MUI) and frequency-selective fading. Existing Deep Joint Source-Channel Coding (DJSCC) schemes, primarily designed for point-to-point links, suffer from performance saturation in multi-user scenarios. To address these issues, we propose a scenario-adaptive MU-MIMO SemCom framework featuring an asymmetric architecture tailored for downlink transmission. At the transmitter, we introduce a scenario-aware semantic encoder that dynamically adjusts feature extraction based on Channel State Information (CSI) and Signal-to-Noise Ratio (SNR), followed by a neural precoding network designed to mitigate MUI in the semantic domain. At the receiver, a lightweight decoder equipped with a novel pilot-guided attention mechanism is employed to implicitly perform channel equalization and feature calibration using reference pilot symbols. Extensive simulation results over 3GPP channel models demonstrate that the proposed framework significantly outperforms DJSCC and traditional Separate Source-Channel Coding (SSCC) schemes in terms of Peak Signal-to-Noise Ratio (PSNR) and classification accuracy, particularly in low-SNR regimes, while maintaining low latency and computational cost on edge devices.

</details>


### [118] [The geometry of invariant learning: an information-theoretic analysis of data augmentation and generalization](https://arxiv.org/abs/2602.14423)
*Abdelali Bouyahia,Frédéric LeBlanc,Mario Marchand*

Main category: cs.LG

TL;DR: 提出信息论框架分析数据增强对泛化和不变性学习的影响，推导包含分布差异、算法稳定性和增强敏感性的三分解泛化界，引入群直径概念揭示增强强度与泛化性能的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 数据增强是现代机器学习中广泛使用的技术，但其理论作用尚未完全理解。作者旨在建立系统框架，从信息论角度分析数据增强如何影响泛化性能和不变性学习。

Method: 基于互信息界建立理论框架，将增强分布建模为原始数据分布与变换分布的复合，引入轨道平均损失函数。在损失函数和增强过程的次高斯假设下，推导出包含三个可解释项的泛化界：(1)原始与增强数据的分布差异，(2)算法对训练数据的依赖性，(3)增强变异性影响。引入群直径概念量化增强变换的最大扰动。

Result: 理论分析表明泛化差距可分解为三个可解释项，群直径作为统一控制参数约束所有三项，揭示了增强强度与泛化性能的内在权衡：小直径保持数据保真度但正则化有限，大直径增强稳定性但增加偏差和敏感性。数值实验验证了理论界能可靠跟踪真实泛化差距。

Conclusion: 该研究提出了一个统一的信息论框架，系统分析了数据增强对泛化的影响，通过群直径概念揭示了增强强度与泛化性能的权衡关系，为理解数据增强的理论机制提供了新视角。

Abstract: Data augmentation is one of the most widely used techniques to improve generalization in modern machine learning, often justified by its ability to promote invariance to label-irrelevant transformations. However, its theoretical role remains only partially understood. In this work, we propose an information-theoretic framework that systematically accounts for the effect of augmentation on generalization and invariance learning. Our approach builds upon mutual information-based bounds, which relate the generalization gap to the amount of information a learning algorithm retains about its training data. We extend this framework by modeling the augmented distribution as a composition of the original data distribution with a distribution over transformations, which naturally induces an orbit-averaged loss function. Under mild sub-Gaussian assumptions on the loss function and the augmentation process, we derive a new generalization bound that decompose the expected generalization gap into three interpretable terms: (1) a distributional divergence between the original and augmented data, (2) a stability term measuring the algorithm dependence on training data, and (3) a sensitivity term capturing the effect of augmentation variability. To connect our bounds to the geometry of the augmentation group, we introduce the notion of group diameter, defined as the maximal perturbation that augmentations can induce in the input space. The group diameter provides a unified control parameter that bounds all three terms and highlights an intrinsic trade-off: small diameters preserve data fidelity but offer limited regularization, while large diameters enhance stability at the cost of increased bias and sensitivity. We validate our theoretical bounds with numerical experiments, demonstrating that it reliably tracks and predicts the behavior of the true generalization gap.

</details>


### [119] [Interpretable clustering via optimal multiway-split decision trees](https://arxiv.org/abs/2602.13586)
*Hayato Suzuki,Shunnosuke Ikeda,Yuichi Takano*

Main category: cs.LG

TL;DR: 提出基于最优多路分割决策树的解释性聚类方法，将问题转化为0-1整数线性优化，比现有方法更易求解，同时通过一维K-means离散化实现灵活分支。


<details>
  <summary>Details</summary>
Motivation: 现有聚类方法通常构建二叉决策树，需要求解混合整数非线性优化问题，计算成本高且容易得到次优解。此外，二叉决策树往往结构过深，难以解释。

Method: 将解释性聚类问题转化为0-1整数线性优化问题，提出基于最优多路分割决策树的方法。关键创新是集成一维K-means算法对连续变量进行离散化，实现数据驱动的灵活分支。

Result: 在公开真实数据集上的大量数值实验表明，该方法在聚类准确性和可解释性方面优于基线方法。能够生成具有简洁决策规则的多路分割决策树，同时在各种评估指标上保持竞争力。

Conclusion: 提出的基于最优多路分割决策树的解释性聚类方法解决了现有方法计算成本高、易得次优解和可解释性差的问题，实现了更好的聚类性能和可解释性平衡。

Abstract: Clustering serves as a vital tool for uncovering latent data structures, and achieving both high accuracy and interpretability is essential. To this end, existing methods typically construct binary decision trees by solving mixed-integer nonlinear optimization problems, often leading to significant computational costs and suboptimal solutions. Furthermore, binary decision trees frequently result in excessively deep structures, which makes them difficult to interpret. To mitigate these issues, we propose an interpretable clustering method based on optimal multiway-split decision trees, formulated as a 0-1 integer linear optimization problem. This reformulation renders the optimization problem more tractable compared to existing models. A key feature of our method is the integration of a one-dimensional K-means algorithm for the discretization of continuous variables, allowing for flexible and data-driven branching. Extensive numerical experiments on publicly available real-world datasets demonstrate that our method outperforms baseline methods in terms of clustering accuracy and interpretability. Our method yields multiway-split decision trees with concise decision rules while maintaining competitive performance across various evaluation metrics.

</details>


### [120] [S2D: Selective Spectral Decay for Quantization-Friendly Conditioning of Neural Activations](https://arxiv.org/abs/2602.14432)
*Arnav Chavan,Nahush Lele,Udbhav Bamba,Sankalp Dayal,Aditi Raghunathan,Deepak Gupta*

Main category: cs.LG

TL;DR: 论文提出选择性谱衰减（S²D）方法，通过手术式正则化权重矩阵的最大奇异值来减少激活异常值，从而提升大模型量化性能。


<details>
  <summary>Details</summary>
Motivation: 大规模Transformer模型中的激活异常值给模型量化带来根本性挑战，这些异常值产生过大的数值范围，导致量化时精度严重下降。作者观察到随着预训练规模增大（如从CLIP到SigLIP再到SigLIP2），异常值问题更加严重。

Method: 通过理论分析和实证相关性研究，建立了激活异常值与权重矩阵主导奇异值之间的直接联系。基于此洞察，提出了选择性谱衰减（S²D）方法，这是一种几何原理驱动的条件化方法，在微调过程中仅对与最大奇异值对应的权重分量进行手术式正则化。

Result: S²D显著减少了激活异常值，生成了条件良好的表示，这些表示本质上对量化友好。使用S²D训练的模型在W4A4量化下在ImageNet上实现了高达7%的PTQ精度提升，与QAT结合时获得4%的增益。这些改进也泛化到下游任务和视觉语言模型中。

Conclusion: S²D方法能够在不牺牲部署效率的情况下，扩展越来越大规模和严格训练的模型，解决了大模型量化中的激活异常值问题。

Abstract: Activation outliers in large-scale transformer models pose a fundamental challenge to model quantization, creating excessively large ranges that cause severe accuracy drops during quantization. We empirically observe that outlier severity intensifies with pre-training scale (e.g., progressing from CLIP to the more extensively trained SigLIP and SigLIP2). Through theoretical analysis as well as empirical correlation studies, we establish the direct link between these activation outliers and dominant singular values of the weights. Building on this insight, we propose Selective Spectral Decay ($S^2D$), a geometrically-principled conditioning method that surgically regularizes only the weight components corresponding to the largest singular values during fine-tuning. Through extensive experiments, we demonstrate that $S^2D$ significantly reduces activation outliers and produces well-conditioned representations that are inherently quantization-friendly. Models trained with $S^2D$ achieve up to 7% improved PTQ accuracy on ImageNet under W4A4 quantization and 4% gains when combined with QAT. These improvements also generalize across downstream tasks and vision-language models, enabling the scaling of increasingly large and rigorously trained models without sacrificing deployment efficiency.

</details>


### [121] [Benchmark Leakage Trap: Can We Trust LLM-based Recommendation?](https://arxiv.org/abs/2602.13626)
*Mingqiao Zhang,Qiyao Peng,Yumeng Wang,Chunyuan Liu,Hongtao Liu*

Main category: cs.LG

TL;DR: 论文发现LLM推荐系统中存在基准数据泄露问题，即LLM在预训练或微调过程中接触并可能记忆了基准数据集，导致性能指标虚高，不能反映真实模型能力。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在推荐系统中的广泛应用，评估可靠性面临关键挑战。本文旨在研究一个被忽视的问题：LLM推荐中的基准数据泄露现象，这种现象会导致性能指标虚高，误导模型能力评估。

Method: 通过模拟多种数据泄露场景，对基础模型进行持续预训练，使用包含领域内和领域外用户-物品交互数据的混合语料库，分析数据泄露对推荐性能的影响。

Result: 实验显示数据泄露具有双重效应：当泄露数据与领域相关时，会导致虚假的性能提升，夸大模型能力；而领域无关的泄露通常会降低推荐准确性，揭示了这种污染的复杂性和条件依赖性。

Conclusion: 数据泄露是LLM推荐中一个关键且先前未被考虑的因素，可能影响真实模型性能评估。研究强调了在LLM推荐评估中需要更严格的数据隔离和泄露检测机制。

Abstract: The expanding integration of Large Language Models (LLMs) into recommender systems poses critical challenges to evaluation reliability. This paper identifies and investigates a previously overlooked issue: benchmark data leakage in LLM-based recommendation. This phenomenon occurs when LLMs are exposed to and potentially memorize benchmark datasets during pre-training or fine-tuning, leading to artificially inflated performance metrics that fail to reflect true model performance. To validate this phenomenon, we simulate diverse data leakage scenarios by conducting continued pre-training of foundation models on strategically blended corpora, which include user-item interactions from both in-domain and out-of-domain sources. Our experiments reveal a dual-effect of data leakage: when the leaked data is domain-relevant, it induces substantial but spurious performance gains, misleadingly exaggerating the model's capability. In contrast, domain-irrelevant leakage typically degrades recommendation accuracy, highlighting the complex and contingent nature of this contamination. Our findings reveal that data leakage acts as a critical, previously unaccounted-for factor in LLM-based recommendation, which could impact the true model performance. We release our code at https://github.com/yusba1/LLMRec-Data-Leakage.

</details>


### [122] [Truly Adapting to Adversarial Constraints in Constrained MABs](https://arxiv.org/abs/2602.14543)
*Francesco Emanuele Stradi,Kalana Kalupahana,Matteo Castiglioni,Alberto Marchesi,Nicola Gatti*

Main category: cs.LG

TL;DR: 论文研究带未知约束的多臂老虎机问题，在非平稳环境下同时优化损失和约束违反，提出了在不同反馈机制下的最优算法。


<details>
  <summary>Details</summary>
Motivation: 现有研究要么假设约束是随机的，要么在完全对抗性约束下放松基准。本文旨在解决当约束是随机而损失可能任意变化时的最优性能保证，并平滑处理约束对抗性程度。

Method: 针对不同反馈机制设计算法：1) 完全反馈下获得Õ(√T+C)的遗憾和正约束违反；2) 损失只有老虎机反馈时扩展保证；3) 约束只有老虎机反馈时获得Õ(√T+C)正违反和Õ(√T+C√T)遗憾。

Result: 提出了首个在约束随机而损失任意变化时达到最优遗憾和正约束违反率的算法，且保证随约束对抗性程度平滑退化。C量化约束的非平稳性。

Conclusion: 本文为带未知约束的非平稳多臂老虎机问题提供了全面的解决方案，在不同反馈机制下均达到最优性能，填补了随机约束与对抗性约束之间的理论空白。

Abstract: We study the constrained variant of the \emph{multi-armed bandit} (MAB) problem, in which the learner aims not only at minimizing the total loss incurred during the learning dynamic, but also at controlling the violation of multiple \emph{unknown} constraints, under both \emph{full} and \emph{bandit feedback}. We consider a non-stationary environment that subsumes both stochastic and adversarial models and where, at each round, both losses and constraints are drawn from distributions that may change arbitrarily over time. In such a setting, it is provably not possible to guarantee both sublinear regret and sublinear violation. Accordingly, prior work has mainly focused either on settings with stochastic constraints or on relaxing the benchmark with fully adversarial constraints (\emph{e.g.}, via competitive ratios with respect to the optimum). We provide the first algorithms that achieve optimal rates of regret and \emph{positive} constraint violation when the constraints are stochastic while the losses may vary arbitrarily, and that simultaneously yield guarantees that degrade smoothly with the degree of adversariality of the constraints. Specifically, under \emph{full feedback} we propose an algorithm attaining $\widetilde{\mathcal{O}}(\sqrt{T}+C)$ regret and $\widetilde{\mathcal{O}}(\sqrt{T}+C)$ {positive} violation, where $C$ quantifies the amount of non-stationarity in the constraints. We then show how to extend these guarantees when only bandit feedback is available for the losses. Finally, when \emph{bandit feedback} is available for the constraints, we design an algorithm achieving $\widetilde{\mathcal{O}}(\sqrt{T}+C)$ {positive} violation and $\widetilde{\mathcal{O}}(\sqrt{T}+C\sqrt{T})$ regret.

</details>


### [123] [Optimization-Free Graph Embedding via Distributional Kernel for Community Detection](https://arxiv.org/abs/2602.13634)
*Shuaibin Song,Kai Ming Ting,Kaifeng Zhang,Tianrun Liang*

Main category: cs.LG

TL;DR: 本文提出一种考虑节点分布和度分布特征的加权分布感知核，解决NAS方法中的过平滑问题，无需优化即可提升图嵌入效果。


<details>
  <summary>Details</summary>
Motivation: 现有的邻域聚合策略（NAS）方法如图神经网络和Weisfeiler-Lehman方法容易出现过平滑问题，即随着迭代次数增加节点可区分性降低。作者发现网络中节点的分布特征和节点度分布特征对表达性表示至关重要，但现有方法忽视了这些特征，导致过平滑问题。

Method: 提出一种新颖的加权分布感知核，在嵌入节点时考虑节点的分布特征。该方法有三个特点：1）首次明确结合两种分布特征；2）无需优化过程；3）有效缓解过平滑的负面影响，使WL方法即使在多次迭代后仍能保持节点可区分性和表达性。

Result: 实验表明，通过谱聚类进行社区检测时，该方法优于现有的图嵌入方法（包括深度学习方法），在标准基准测试中取得了更好的性能。

Conclusion: 通过考虑节点分布和度分布特征，提出的加权分布感知核能有效解决NAS方法的过平滑问题，提升图嵌入的表达能力，为社区检测等任务提供更优的表示。

Abstract: Neighborhood Aggregation Strategy (NAS) is a widely used approach in graph embedding, underpinning both Graph Neural Networks (GNNs) and Weisfeiler-Lehman (WL) methods. However, NAS-based methods are identified to be prone to over-smoothing-the loss of node distinguishability with increased iterations-thereby limiting their effectiveness. This paper identifies two characteristics in a network, i.e., the distributions of nodes and node degrees that are critical for expressive representation but have been overlooked in existing methods. We show that these overlooked characteristics contribute significantly to over-smoothing of NAS-methods. To address this, we propose a novel weighted distribution-aware kernel that embeds nodes while taking their distributional characteristics into consideration. Our method has three distinguishing features: (1) it is the first method to explicitly incorporate both distributional characteristics; (2) it requires no optimization; and (3) it effectively mitigates the adverse effects of over-smoothing, allowing WL to preserve node distinguishability and expressiveness even after many iterations of embedding. Experiments demonstrate that our method achieves superior community detection performance via spectral clustering, outperforming existing graph embedding methods, including deep learning methods, on standard benchmarks.

</details>


### [124] [Replicable Constrained Bandits](https://arxiv.org/abs/2602.14580)
*Matteo Bollini,Gianmarco Genalti,Francesco Emanuele Stradi,Matteo Castiglioni,Alberto Marchesi*

Main category: cs.LG

TL;DR: 该论文研究了约束多臂老虎机问题中的算法可复制性，提出了首个可复制的UCB类算法，并在约束MAB中实现了与非可复制算法相当的遗憾和约束违反性能。


<details>
  <summary>Details</summary>
Motivation: 机器学习实验需要可重复性，算法可复制性确保在不同执行中产生相同的决策序列。约束MAB问题中，学习者需要在最大化奖励的同时满足多个约束，但目前缺乏对约束MAB中可复制性的研究。

Method: 首先为无约束MAB开发了首个可复制的UCB类算法，证明采用"面对不确定性的乐观原则"的算法可以是可复制的。然后将该方法扩展到约束MAB问题，设计可复制算法来处理奖励最大化和约束满足的双重目标。

Result: 成功在约束MAB中实现算法可复制性，设计的可复制算法在遗憾和约束违反方面与非可复制算法具有相同的T阶性能。为无约束MAB开发了首个可复制的UCB类算法，这一结果具有独立意义。

Conclusion: 算法可复制性可以在约束MAB问题中实现，且不会显著牺牲性能。该研究为可复制在线学习算法设计提供了新思路，特别是证明了乐观原则算法可以具有可复制性。

Abstract: Algorithmic \emph{replicability} has recently been introduced to address the need for reproducible experiments in machine learning. A \emph{replicable online learning} algorithm is one that takes the same sequence of decisions across different executions in the same environment, with high probability. We initiate the study of algorithmic replicability in \emph{constrained} MAB problems, where a learner interacts with an unknown stochastic environment for $T$ rounds, seeking not only to maximize reward but also to satisfy multiple constraints. Our main result is that replicability can be achieved in constrained MABs. Specifically, we design replicable algorithms whose regret and constraint violation match those of non-replicable ones in terms of $T$. As a key step toward these guarantees, we develop the first replicable UCB-like algorithm for \emph{unconstrained} MABs, showing that algorithms that employ the optimism in-the-face-of-uncertainty principle can be replicable, a result that we believe is of independent interest.

</details>


### [125] [Joint Time Series Chain: Detecting Unusual Evolving Trend across Time Series](https://arxiv.org/abs/2602.13649)
*Li Zhang,Nital Patel,Xiuqi Li,Jessica Lin*

Main category: cs.LG

TL;DR: 提出了联合时间序列链的新定义，用于在中断时间序列或两个相关时间序列中发现意外演化趋势，解决了现有方法只能处理单一连续时间序列的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列链定义仅考虑在单一连续时间序列中寻找链，容易错过中断时间序列或跨两个相关时间序列中的意外演化模式。需要解决由时间序列中的间隙或中断引起的鲁棒性问题。

Method: 引入联合时间序列链的新定义，专门设计用于在中断时间序列或两个相关时间序列中发现意外演化趋势。提出有效的排序标准来识别最佳链，并公开了源代码。

Result: 通过广泛的实证评估，证明所提方法在定位异常演化模式方面优于现有的时间序列链工作。在英特尔的真实制造应用中展示了其实用性。

Conclusion: 联合时间序列链能够有效揭示中断时间序列或跨相关时间序列中的潜在异常演化趋势，解决了现有方法的局限性，具有实际应用价值。

Abstract: Time series chain (TSC) is a recently introduced concept that captures the evolving patterns in large scale time series. Informally, a time series chain is a temporally ordered set of subsequences, in which consecutive subsequences in the chain are similar to one another, but the last and the first subsequences maybe be dissimilar. Time series chain has the great potential to reveal latent unusual evolving trend in the time series, or identify precursor of important events in a complex system. Unfortunately, existing definitions of time series chains only consider finding chains in a single time series. As a result, they are likely to miss unexpected evolving patterns in interrupted time series, or across two related time series. To address this limitation, in this work, we introduce a new definition called \textit{Joint Time Series Chain}, which is specially designed for the task of finding unexpected evolving trend across interrupted time series or two related time series. Our definition focuses on mitigating the robustness issues caused by the gap or interruption in the time series. We further propose an effective ranking criterion to identify the best chain. We demonstrate that our proposed approach outperforms existing TSC work in locating unusual evolving patterns through extensive empirical evaluations. We further demonstrate the utility of our work with a real-life manufacturing application from Intel. Our source code is publicly available at the supporting page https://github.com/lizhang-ts/JointTSC .

</details>


### [126] [Unbiased Approximate Vector-Jacobian Products for Efficient Backpropagation](https://arxiv.org/abs/2602.14701)
*Killian Bakong,Laurent Massoulié,Edouard Oyallon,Kevin Scaman*

Main category: cs.LG

TL;DR: 提出一种通过随机化、无偏近似向量-雅可比乘积来降低深度神经网络训练计算和内存成本的方法


<details>
  <summary>Details</summary>
Motivation: 降低深度神经网络训练的计算和内存成本，这是深度学习面临的主要挑战之一

Method: 在反向传播中用随机化、无偏的近似替代精确的向量-雅可比乘积，识别具有最小方差的最优无偏估计，并在稀疏约束下建立最优性

Result: 理论分析了精度与成本降低之间的权衡关系，实验验证了方法在多层感知机、BagNets和视觉Transformer上的有效性

Conclusion: 提出的无偏随机反向传播方法具有降低深度学习成本的潜力，理论和实验都支持这一结论

Abstract: In this work we introduce methods to reduce the computational and memory costs of training deep neural networks. Our approach consists in replacing exact vector-jacobian products by randomized, unbiased approximations thereof during backpropagation. We provide a theoretical analysis of the trade-off between the number of epochs needed to achieve a target precision and the cost reduction for each epoch. We then identify specific unbiased estimates of vector-jacobian products for which we establish desirable optimality properties of minimal variance under sparsity constraints. Finally we provide in-depth experiments on multi-layer perceptrons, BagNets and Visual Transfomers architectures. These validate our theoretical results, and confirm the potential of our proposed unbiased randomized backpropagation approach for reducing the cost of deep learning.

</details>


### [127] [Cumulative Utility Parity for Fair Federated Learning under Intermittent Client Participation](https://arxiv.org/abs/2602.13651)
*Stefan Behfar,Richard Mortier*

Main category: cs.LG

TL;DR: 提出累积效用公平性，解决联邦学习中客户参与不均衡导致的长期代表性偏差问题


<details>
  <summary>Details</summary>
Motivation: 现实联邦学习系统中客户参与是间歇性、异质性的，且与数据特征或资源约束相关。现有公平性方法主要关注参与条件下的损失或准确率平等，但参与本身不均衡时，这些目标会导致间歇可用客户被系统性低估。

Method: 提出累积效用公平性原则，评估客户从每个参与机会中获得的可比长期收益。引入可用性归一化累积效用，将不可避免的物理约束与调度和聚合产生的可避免算法偏差分离。

Result: 在时间偏斜、非独立同分布的联邦基准测试中，该方法显著改善了长期代表性公平性，同时保持了接近完美的性能。

Conclusion: 累积效用公平性为解决联邦学习中客户参与不均衡导致的长期公平性问题提供了有效框架，确保所有客户都能从参与机会中获得可比收益。

Abstract: In real-world federated learning (FL) systems, client participation is intermittent, heterogeneous, and often correlated with data characteristics or resource constraints. Existing fairness approaches in FL primarily focus on equalizing loss or accuracy conditional on participation, implicitly assuming that clients have comparable opportunities to contribute over time. However, when participation itself is uneven, these objectives can lead to systematic under-representation of intermittently available clients, even if per-round performance appears fair. We propose cumulative utility parity, a fairness principle that evaluates whether clients receive comparable long-term benefit per participation opportunity, rather than per training round. To operationalize this notion, we introduce availability-normalized cumulative utility, which disentangles unavoidable physical constraints from avoidable algorithmic bias arising from scheduling and aggregation. Experiments on temporally skewed, non-IID federated benchmarks demonstrate that our approach substantially improves long-term representation parity, while maintaining near-perfect performance.

</details>


### [128] [On the Stability of Nonlinear Dynamics in GD and SGD: Beyond Quadratic Potentials](https://arxiv.org/abs/2602.14789)
*Rotem Mulayoff,Sebastian U. Stich*

Main category: cs.LG

TL;DR: 该论文研究了梯度下降(GD)和随机梯度下降(SGD)训练中非线性动力学对稳定性的影响，发现线性化分析可能误导，并推导了GD稳定振荡的精确判据，以及SGD中单个不稳定批次可能导致整体发散的现象。


<details>
  <summary>Details</summary>
Motivation: 训练过程中迭代的动态稳定性对优化算法找到的极小值有重要影响。虽然先前工作常依赖线性化分析稳定性，但线性化动力学是否能准确捕捉完整非线性行为尚不明确。最近研究表明GD可能在线性不稳定极小值附近稳定振荡，表明线性分析可能具有误导性。

Method: 1. 在多元设置中推导GD在极小值附近稳定振荡的精确判据，该条件依赖于高阶导数；2. 将分析扩展到SGD，研究非线性动力学在期望中的发散行为；3. 证明如果所有批次都线性稳定，SGD的非线性动力学在期望中是稳定的。

Result: 1. 建立了GD稳定振荡的精确判据，推广了现有结果；2. 发现SGD的非线性动力学可能因单个不稳定批次而在期望中发散，这与线性分析认为稳定性由平均效应决定的观点不同；3. 证明了所有批次线性稳定时SGD非线性动力学的期望稳定性。

Conclusion: 非线性项对优化算法的稳定性有重要影响，线性分析可能误导。GD可能在线性不稳定点附近稳定振荡，而SGD的稳定性可能由单个不稳定批次决定而非平均效应。这为理解优化算法的动态行为提供了更准确的框架。

Abstract: The dynamical stability of the iterates during training plays a key role in determining the minima obtained by optimization algorithms. For example, stable solutions of gradient descent (GD) correspond to flat minima, which have been associated with favorable features. While prior work often relies on linearization to determine stability, it remains unclear whether linearized dynamics faithfully capture the full nonlinear behavior. Recent work has shown that GD may stably oscillate near a linearly unstable minimum and still converge once the step size decays, indicating that linear analysis can be misleading. In this work, we explicitly study the effect of nonlinear terms. Specifically, we derive an exact criterion for stable oscillations of GD near minima in the multivariate setting. Our condition depends on high-order derivatives, generalizing existing results. Extending the analysis to stochastic gradient descent (SGD), we show that nonlinear dynamics can diverge in expectation even if a single batch is unstable. This implies that stability can be dictated by a single batch that oscillates unstably, rather than an average effect, as linear analysis suggests. Finally, we prove that if all batches are linearly stable, the nonlinear dynamics of SGD are stable in expectation.

</details>


### [129] [Zero-Order Optimization for LLM Fine-Tuning via Learnable Direction Sampling](https://arxiv.org/abs/2602.13659)
*Valery Parfenov,Grigoriy Evseev,Andrey Veprikov,Nikolay Bushkov,Stanislav Moiseev,Aleksandr Beznosikov*

Main category: cs.LG

TL;DR: 提出一种策略驱动的零阶优化框架，通过学习扰动方向的采样分布来降低方差，使零阶方法能有效用于大规模语言模型微调。


<details>
  <summary>Details</summary>
Motivation: 大型预训练语言模型微调的内存需求巨大，限制了在资源受限环境中的部署。零阶方法通过前向评估估计方向导数来节省内存，但传统零阶估计器存在高方差和对参数维度d的依赖问题，限制了其在高维问题中的应用。

Method: 提出策略驱动的零阶框架，将扰动方向的采样分布视为可学习策略，通过更新该策略来降低方向估计的方差。开发了实现这一想法的实用算法，并提供了理论分析。

Result: 理论分析表明学习到的采样分布能提高梯度信息质量，并在收敛界限中放松对d的显式依赖。在具有挑战性的LLM微调基准测试中，相比标准零阶基线表现出显著改进的性能。

Conclusion: 自适应方向采样是使零阶微调在大规模应用中可行的有前景途径，为资源受限环境下的LLM微调提供了有效解决方案。

Abstract: Fine-tuning large pretrained language models (LLMs) is a cornerstone of modern NLP, yet its growing memory demands (driven by backpropagation and large optimizer States) limit deployment in resource-constrained settings. Zero-order (ZO) methods bypass backpropagation by estimating directional derivatives from forward evaluations, offering substantial memory savings. However, classical ZO estimators suffer from high variance and an adverse dependence on the parameter dimensionality $d$, which has constrained their use to low-dimensional problems. In this work, we propose a policy-driven ZO framework that treats the sampling distribution over perturbation directions as a learnable policy and updates it to reduce the variance of directional estimates. We develop a practical algorithm implementing this idea and provide a theoretical analysis, showing that learned sampling distributions improve the quality of gradient information and relax the explicit dependence on $d$ in convergence bounds. Empirically, we validate the approach on challenging LLM fine-tuning benchmarks, demonstrating substantially improved performance compared to standard ZO baselines. Our results suggest that adaptive direction sampling is a promising route to make ZO fine-tuning viable at scale. The source code is available at https://github.com/brain-lab-research/zo_ldsd

</details>


### [130] [Extending Multi-Source Bayesian Optimization With Causality Principles](https://arxiv.org/abs/2602.14791)
*Luuk Jacobs,Mohammad Ali Javidian*

Main category: cs.LG

TL;DR: 提出多源因果贝叶斯优化（MSCBO），将多源贝叶斯优化与因果贝叶斯优化结合，利用因果信息提升高维优化效率


<details>
  <summary>Details</summary>
Motivation: 传统多源贝叶斯优化假设输入变量独立同分布，无法有效利用因果信息和干预能力，限制了在临床试验、政策制定等实际场景中的应用

Method: 提出多源因果贝叶斯优化（MSCBO）算法，将因果贝叶斯优化的因果原理与多源贝叶斯优化的多信息源框架相结合，实现维度降低和计算复杂度减少

Result: 在合成和真实数据集上测试MSCBO，相比基础算法表现出更好的鲁棒性和适用性，尤其在噪声环境下

Conclusion: 整合多源贝叶斯优化与因果贝叶斯优化原理可实现维度降低、降低操作成本，最终提升收敛速度、性能和可扩展性

Abstract: Multi-Source Bayesian Optimization (MSBO) serves as a variant of the traditional Bayesian Optimization (BO) framework applicable to situations involving optimization of an objective black-box function over multiple information sources such as simulations, surrogate models, or real-world experiments. However, traditional MSBO assumes the input variables of the objective function to be independent and identically distributed, limiting its effectiveness in scenarios where causal information is available and interventions can be performed, such as clinical trials or policy-making. In the single-source domain, Causal Bayesian Optimization (CBO) extends standard BO with the principles of causality, enabling better modeling of variable dependencies. This leads to more accurate optimization, improved decision-making, and more efficient use of low-cost information sources. In this article, we propose a principled integration of the MSBO and CBO methodologies in the multi-source domain, leveraging the strengths of both to enhance optimization efficiency and reduce computational complexity in higher-dimensional problems. We present the theoretical foundations of both Causal and Multi-Source Bayesian Optimization, and demonstrate how their synergy informs our Multi-Source Causal Bayesian Optimization (MSCBO) algorithm. We compare the performance of MSCBO against its foundational counterparts for both synthetic and real-world datasets with varying levels of noise, highlighting the robustness and applicability of MSCBO. Based on our findings, we conclude that integrating MSBO with the causality principles of CBO facilitates dimensionality reduction and lowers operational costs, ultimately improving convergence speed, performance, and scalability.

</details>


### [131] [Optimized Certainty Equivalent Risk-Controlling Prediction Sets](https://arxiv.org/abs/2602.13660)
*Jiayi Huang,Amirmohammad Farzaneh,Osvaldo Simeone*

Main category: cs.LG

TL;DR: 本文提出OCE-RCPS框架，为医疗图像分割等安全关键应用提供基于优化确定性等价风险度量的高概率可靠性保证，超越传统风险控制预测集方法。


<details>
  <summary>Details</summary>
Motivation: 在医疗图像分割等安全关键应用中，传统风险控制预测集（RCPS）只能提供期望风险的概率保证，但无法捕捉尾部行为和最坏情况，而这些在高风险场景中至关重要。

Method: 提出优化确定性等价RCPS（OCE-RCPS）框架，利用置信上界识别满足用户指定风险容忍水平的预测集参数，支持条件风险价值（CVaR）和熵风险等OCE风险度量。

Result: 理论证明OCE-RCPS对误覆盖率和假阴性率等损失函数满足所需概率约束；图像分割实验显示OCE-RCPS在各种风险度量和可靠性配置下始终达到目标满足率，而OCE-CRC无法提供概率保证。

Conclusion: OCE-RCPS为安全关键应用提供了更强大的可靠性保证框架，能够处理尾部风险和最坏情况，填补了传统RCPS方法的不足。

Abstract: In safety-critical applications such as medical image segmentation, prediction systems must provide reliability guarantees that extend beyond conventional expected loss control. While risk-controlling prediction sets (RCPS) offer probabilistic guarantees on the expected risk, they fail to capture tail behavior and worst-case scenarios that are crucial in high-stakes settings. This paper introduces optimized certainty equivalent RCPS (OCE-RCPS), a novel framework that provides high-probability guarantees on general optimized certainty equivalent (OCE) risk measures, including conditional value-at-risk (CVaR) and entropic risk. OCE-RCPS leverages upper confidence bounds to identify prediction set parameters that satisfy user-specified risk tolerance levels with provable reliability. We establish theoretical guarantees showing that OCE-RCPS satisfies the desired probabilistic constraint for loss functions such as miscoverage and false negative rate. Experiments on image segmentation demonstrate that OCE-RCPS consistently meets target satisfaction rates across various risk measures and reliability configurations, while OCE-CRC fails to provide probabilistic guarantees.

</details>


### [132] [ALMo: Interactive Aim-Limit-Defined, Multi-Objective System for Personalized High-Dose-Rate Brachytherapy Treatment Planning and Visualization for Cervical Cancer](https://arxiv.org/abs/2602.13666)
*Edward Chen,Natalie Dullerud,Pang Wei Koh,Thomas Niedermayr,Elizabeth Kidd,Sanmi Koyejo,Carlos Guestrin*

Main category: cs.LG

TL;DR: ALMo是一个用于宫颈癌高剂量率近距离放疗的交互式决策支持系统，通过自动化参数设置和直观的"目标-限制"阈值控制，帮助临床医生在多目标权衡中导航，显著提高计划质量和效率。


<details>
  <summary>Details</summary>
Motivation: 临床决策中需要跟踪多个相互竞争的目标（理想阈值和严格限制阈值），在高维权衡中寻找最优患者特定策略具有认知挑战且易产生变异性。特别是在宫颈癌HDR近距离放疗中，需要严格管理辐射热点，同时平衡肿瘤覆盖和器官保护。

Method: 提出ALMo系统，采用新颖的优化框架，通过自动化参数设置减少手动输入，允许临床医生通过直接操作直观的目标和限制值来导航剂量权衡的帕累托前沿。

Result: 在25个临床病例的回顾性评估中，ALMo生成的计划质量始终达到或超过手动计划，65%的病例显示剂量学改进。系统显著提高效率，平均计划时间降至约17分钟（传统方法需要30-60分钟）。

Conclusion: ALMo在近距离放疗中得到验证，展示了一个通用的框架，可用于简化多标准临床决策中的交互过程。

Abstract: In complex clinical decision-making, clinicians must often track a variety of competing metrics defined by aim (ideal) and limit (strict) thresholds. Sifting through these high-dimensional tradeoffs to infer the optimal patient-specific strategy is cognitively demanding and historically prone to variability. In this paper, we address this challenge within the context of High-Dose-Rate (HDR) brachytherapy for cervical cancer, where planning requires strictly managing radiation hot spots while balancing tumor coverage against organ sparing. We present ALMo (Aim-Limit-defined Multi-Objective system), an interactive decision support system designed to infer and operationalize clinician intent. ALMo employs a novel optimization framework that minimizes manual input through automated parameter setup and enables flexible control over toxicity risks. Crucially, the system allows clinicians to navigate the Pareto surface of dosimetric tradeoffs by directly manipulating intuitive aim and limit values. In a retrospective evaluation of 25 clinical cases, ALMo generated treatment plans that consistently met or exceeded manual planning quality, with 65% of cases demonstrating dosimetric improvements. Furthermore, the system significantly enhanced efficiency, reducing average planning time to approximately 17 minutes, compared to the conventional 30-60 minutes. While validated in brachytherapy, ALMo demonstrates a generalized framework for streamlining interaction in multi-criteria clinical decision-making.

</details>


### [133] [On the Learning Dynamics of RLVR at the Edge of Competence](https://arxiv.org/abs/2602.14872)
*Yu Huang,Zixin Wen,Yuejie Chi,Yuting Wei,Aarti Singh,Yingbin Liang,Yuxin Chen*

Main category: cs.LG

TL;DR: RLVR（可验证奖励的强化学习）通过难度谱平滑度影响训练动态：平滑谱产生接力效应，不连续谱导致grokking式相变。


<details>
  <summary>Details</summary>
Motivation: 理解仅基于最终结果的奖励如何帮助克服长时程推理障碍，探究RLVR在组合推理任务中的训练动态机制。

Method: 开发基于有限群上傅里叶分析的理论框架，分析Transformer在组合推理任务上的RL训练动态，并通过合成实验验证预测机制。

Result: RLVR有效性受难度谱平滑度调控：平滑谱产生接力效应（持续梯度信号提升能力），不连续谱导致grokking式相变（长时间平台期后突破）。

Conclusion: RLVR通过提升模型在能力边缘的表现来改善推理，适当设计的数据混合可产生可扩展的增益，难度谱平滑度是关键设计因素。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has been a main driver of recent breakthroughs in large reasoning models. Yet it remains a mystery how rewards based solely on final outcomes can help overcome the long-horizon barrier to extended reasoning. To understand this, we develop a theory of the training dynamics of RL for transformers on compositional reasoning tasks. Our theory characterizes how the effectiveness of RLVR is governed by the smoothness of the difficulty spectrum. When data contains abrupt discontinuities in difficulty, learning undergoes grokking-type phase transitions, producing prolonged plateaus before progress recurs. In contrast, a smooth difficulty spectrum leads to a relay effect: persistent gradient signals on easier problems elevate the model's capabilities to the point where harder ones become tractable, resulting in steady and continuous improvement. Our theory explains how RLVR can improve performance at the edge of competence, and suggests that appropriately designed data mixtures can yield scalable gains. As a technical contribution, our analysis develops and adapts tools from Fourier analysis on finite groups to our setting. We validate the predicted mechanisms empirically via synthetic experiments.

</details>


### [134] [Advancing Analytic Class-Incremental Learning through Vision-Language Calibration](https://arxiv.org/abs/2602.13670)
*Binyu Zhao,Wei Zhang,Xingrui Yu,Zhaonian Zou,Ivor Tsang*

Main category: cs.LG

TL;DR: VILA提出了一种双分支框架，通过视觉-语言校准策略改进基于预训练模型的类增量学习，在保持分析学习高效性的同时克服其脆弱性。


<details>
  <summary>Details</summary>
Motivation: 基于预训练模型的类增量学习面临高效适应与长期稳定性之间的关键权衡。分析学习虽然能实现快速递归闭式更新，但常受累积误差和特征不兼容的影响。研究发现表示刚性是主要瓶颈。

Method: 提出VILA双分支框架，采用两级视觉-语言校准策略：在特征层面通过几何校准将可塑性任务适应特征与冻结的通用语义锚点融合；在决策层面利用跨模态先验纠正预测偏差。

Result: 在八个基准测试上的广泛实验表明，VILA始终表现出优越性能，特别是在细粒度和长序列场景中，实现了高保真预测与分析学习简单性的统一。

Conclusion: VILA框架通过视觉-语言校准策略成功解决了分析学习在类增量学习中的脆弱性问题，在保持高效性的同时显著提升了性能，特别是在复杂场景下表现优异。

Abstract: Class-incremental learning (CIL) with pre-trained models (PTMs) faces a critical trade-off between efficient adaptation and long-term stability. While analytic learning enables rapid, recursive closed-form updates, its efficacy is often compromised by accumulated errors and feature incompatibility. In this paper, we first conduct a systematic study to dissect the failure modes of PTM-based analytic CIL, identifying representation rigidity as the primary bottleneck. Motivated by these insights, we propose \textbf{VILA}, a novel dual-branch framework that advances analytic CIL via a two-level vision-language calibration strategy. Specifically, we coherently fuse plastic, task-adapted features with a frozen, universal semantic anchor at the feature level through geometric calibration, and leverage cross-modal priors at the decision level to rectify prediction bias. This confluence maintains analytic-learning's extreme efficiency while overcoming its inherent brittleness. Extensive experiments across eight benchmarks demonstrate that VILA consistently yields superior performance, particularly in fine-grained and long-sequence scenarios. Our framework harmonizes high-fidelity prediction with the simplicity of analytic learning. Our code is available at https://github.com/byzhaoAI/VILA

</details>


### [135] [Locally Adaptive Multi-Objective Learning](https://arxiv.org/abs/2602.14952)
*Jivat Neet Kaur,Isaac Gibbs,Michael I. Jordan*

Main category: cs.LG

TL;DR: 提出一种自适应多目标在线学习方法，通过将多目标学习中的部分组件替换为自适应在线算法，实现在分布漂移下的局部自适应能力


<details>
  <summary>Details</summary>
Motivation: 现有多目标学习方法在分布漂移时缺乏适应性，它们在整个时间范围内以最坏情况为目标，无法适应数据分布的动态变化

Method: 将多目标学习方法中的部分组件替换为自适应在线算法，实现局部自适应能力，在连续子区间上提供局部保证

Result: 在能源预测和算法公平数据集上的实验表明，该方法优于现有方法，能在子群上实现无偏预测，同时在分布漂移下保持鲁棒性

Conclusion: 通过引入自适应在线算法组件，提出的方法能够有效应对分布漂移，在多目标学习框架中实现局部自适应，提升实际应用中的性能

Abstract: We consider the general problem of learning a predictor that satisfies multiple objectives of interest simultaneously, a broad framework that captures a range of specific learning goals including calibration, regret, and multiaccuracy. We work in an online setting where the data distribution can change arbitrarily over time. Existing approaches to this problem aim to minimize the set of objectives over the entire time horizon in a worst-case sense, and in practice they do not necessarily adapt to distribution shifts. Earlier work has aimed to alleviate this problem by incorporating additional objectives that target local guarantees over contiguous subintervals. Empirical evaluation of these proposals is, however, scarce. In this article, we consider an alternative procedure that achieves local adaptivity by replacing one part of the multi-objective learning method with an adaptive online algorithm. Empirical evaluations on datasets from energy forecasting and algorithmic fairness show that our proposed method improves upon existing approaches and achieves unbiased predictions over subgroups, while remaining robust under distribution shift.

</details>


### [136] [On the Sparsifiability of Correlation Clustering: Approximation Guarantees under Edge Sampling](https://arxiv.org/abs/2602.13684)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.LG

TL;DR: 本文研究了相关聚类（CC）的稀疏化-近似权衡，建立了伪度量与一般加权实例的结构二分法，证明了VC维为n-1，获得了最优大小的ε-核心集，开发了精确切割平面求解器，并提出了稀疏化LP-PIVOT算法，在观测O(n^{3/2})条边时达到10/3近似比。


<details>
  <summary>Details</summary>
Motivation: 相关聚类（CC）最强的LP近似保证需要Θ(n^3)个三角不等式约束，在大规模应用中计算代价过高。本文旨在研究CC的稀疏化-近似权衡问题，探索需要多少边信息才能保持LP保证。

Method: 1) 证明聚类分歧类的VC维为n-1，获得最优大小的ε-核心集；2) 证明最多有C(n,2)个三角不等式在任何LP顶点处活跃，实现精确切割平面求解器；3) 提出稀疏化LP-PIVOT算法，通过三角不等式补全缺失的LP边际；4) 使用Yao极小极大原理证明负面结果。

Result: 1) 获得最优大小Õ(n/ε^2)的加性ε-核心集；2) 实现精确切割平面求解器；3) 稀疏化LP-PIVOT在观测Õ(n^{3/2})条边时达到10/3近似比（受可计算的补全质量统计量Γ_w控制）；4) 证明该阈值是尖锐的；5) 对于非伪度量实例，任何观测o(n)条随机边的算法都有无界近似比。

Conclusion: 本文建立了相关聚类的稀疏化-近似权衡理论框架，揭示了伪度量结构在CC可处理性和对不完全信息鲁棒性中的关键作用。正面结果表明在伪度量条件下，通过精心设计的稀疏化可以获得强近似保证，而负面结果证明了伪度量条件的必要性。

Abstract: Correlation Clustering (CC) is a fundamental unsupervised learning primitive whose strongest LP-based approximation guarantees require $Θ(n^3)$ triangle inequality constraints and are prohibitive at scale. We initiate the study of \emph{sparsification--approximation trade-offs} for CC, asking how much edge information is needed to retain LP-based guarantees. We establish a structural dichotomy between pseudometric and general weighted instances. On the positive side, we prove that the VC dimension of the clustering disagreement class is exactly $n{-}1$, yielding additive $\varepsilon$-coresets of optimal size $\tilde{O}(n/\varepsilon^2)$; that at most $\binom{n}{2}$ triangle inequalities are active at any LP vertex, enabling an exact cutting-plane solver; and that a sparsified variant of LP-PIVOT, which imputes missing LP marginals via triangle inequalities, achieves a robust $\frac{10}{3}$-approximation (up to an additive term controlled by an empirically computable imputation-quality statistic $\overlineΓ_w$) once $\tildeΘ(n^{3/2})$ edges are observed, a threshold we prove is sharp. On the negative side, we show via Yao's minimax principle that without pseudometric structure, any algorithm observing $o(n)$ uniformly random edges incurs an unbounded approximation ratio, demonstrating that the pseudometric condition governs not only tractability but also the robustness of CC to incomplete information.

</details>


### [137] [Efficient Sampling with Discrete Diffusion Models: Sharp and Adaptive Guarantees](https://arxiv.org/abs/2602.15008)
*Daniil Dmitriev,Zhihan Huang,Yuting Wei*

Main category: cs.LG

TL;DR: 该论文分析了离散扩散模型的采样效率，针对均匀和掩码两种噪声过程，建立了τ-leaping采样器在KL散度上的收敛保证，并证明其能自适应低维结构。


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型在经验上取得了显著成功，但其理论基础尚不完整。本文旨在研究基于连续时间马尔可夫链（CTMC）的离散扩散模型的采样效率，特别是τ-leaping采样器，以建立理论保证并理解其收敛行为。

Method: 采用连续时间马尔可夫链（CTMC）框架分析离散扩散模型，重点关注τ-leaping采样器。对于均匀离散扩散，分析标准τ-leaping算法；对于掩码离散扩散，引入改进的τ-leaping采样器。理论分析基于KL散度收敛，无需对分数估计器施加有界性或平滑性假设，仅需控制分数熵损失。

Result: 对于均匀离散扩散，τ-leaping算法达到迭代复杂度$\tilde O(d/\varepsilon)$，消除了对词汇大小S的线性依赖，比现有界限改进d倍，并建立了匹配的算法下界证明对维度d的线性依赖不可避免。对于掩码离散扩散，改进采样器的收敛率由有效总相关这一信息论量控制，该量有界于$d \log S$，但对结构化数据可以是亚线性甚至常数，使采样器能自适应低维结构。

Conclusion: 离散扩散模型的τ-leaping采样器具有理论保证的采样效率，能自适应数据的内在低维结构。均匀扩散的收敛率与维度d线性相关，而掩码扩散的收敛率由有效总相关控制，对结构化数据可实现亚线性收敛，为实际应用（如隐马尔可夫模型、图像数据、随机图）提供了理论依据。

Abstract: Diffusion models over discrete spaces have recently shown striking empirical success, yet their theoretical foundations remain incomplete. In this paper, we study the sampling efficiency of score-based discrete diffusion models under a continuous-time Markov chain (CTMC) formulation, with a focus on $τ$-leaping-based samplers. We establish sharp convergence guarantees for attaining $\varepsilon$ accuracy in Kullback-Leibler (KL) divergence for both uniform and masking noising processes. For uniform discrete diffusion, we show that the $τ$-leaping algorithm achieves an iteration complexity of order $\tilde O(d/\varepsilon)$, with $d$ the ambient dimension of the target distribution, eliminating linear dependence on the vocabulary size $S$ and improving existing bounds by a factor of $d$; moreover, we establish a matching algorithmic lower bound showing that linear dependence on the ambient dimension is unavoidable in general. For masking discrete diffusion, we introduce a modified $τ$-leaping sampler whose convergence rate is governed by an intrinsic information-theoretic quantity, termed the effective total correlation, which is bounded by $d \log S$ but can be sublinear or even constant for structured data. As a consequence, the sampler provably adapts to low-dimensional structure without prior knowledge or algorithmic modification, yielding sublinear convergence rates for various practical examples (such as hidden Markov models, image data, and random graphs). Our analysis requires no boundedness or smoothness assumptions on the score estimator beyond control of the score entropy loss.

</details>


### [138] [Physics Aware Neural Networks: Denoising for Magnetic Navigation](https://arxiv.org/abs/2602.13690)
*Aritra Das,Yashas Shende,Muskaan Chugh,Reva Laxmi Chauhan,Arghya Pathak,Debayan Gupta*

Main category: cs.LG

TL;DR: 提出基于物理约束的磁异常导航框架，通过无散度向量场和E(3)-等变性约束提升磁噪声抑制性能，Contiformer架构在时间序列建模中表现最佳


<details>
  <summary>Details</summary>
Motivation: 磁异常导航在GPS不可用时是重要替代方案，但飞机自身产生的磁噪声干扰地磁场数据提取。传统Tolles-Lawson模型无法有效处理导航所需的随机噪声污染数据

Method: 提出基于两个物理约束的框架：1) 无散度向量场约束（通过神经网络输出向量势A，磁场定义为旋度），2) E(3)-等变性约束（使用几何张量的张量积）。采用Contiformer架构处理连续时间动态和长期记忆，并使用条件GAN生成合成数据集

Result: 物理约束显著提升预测精度和物理合理性，优于传统方法和无约束深度学习。Contiformer在时间序列建模中超越现有方法，合成数据生成缓解了数据稀缺问题

Conclusion: 通过嵌入物理约束作为隐式正则化器，提出的框架能有效处理随机磁噪声，提升磁异常导航性能，为GPS受限环境提供可靠替代方案

Abstract: Magnetic-anomaly navigation, leveraging small-scale variations in the Earth's magnetic field, is a promising alternative when GPS is unavailable or compromised. Airborne systems face a key challenge in extracting geomagnetic field data: the aircraft itself induces magnetic noise. Although the classical Tolles-Lawson model addresses this, it inadequately handles stochastically corrupted magnetic data required for navigation. To address stochastic noise, we propose a framework based on two physics-based constraints: divergence-free vector field and E(3)-equivariance. These ensure the learned magnetic field obeys Maxwell's equations and that outputs transform correctly with sensor position/orientation. The divergence-free constraint is implemented by training a neural network to output a vector potential $A$, with the magnetic field defined as its curl. For E(3)-equivariance, we use tensor products of geometric tensors representable via spherical harmonics with known rotational transformations. Enforcing physical consistency and restricting the admissible function space acts as an implicit regularizer that improves spatio-temporal performance. We present ablation studies evaluating each constraint alone and jointly across CNNs, MLPs, Liquid Time Constant models, and Contiformers. Continuous-time dynamics and long-term memory are critical for modelling magnetic time series; the Contiformer architecture, which provides both, outperforms state-of-the-art methods. To mitigate data scarcity, we generate synthetic datasets using the World Magnetic Model (WMM) with time-series conditional GANs, producing realistic, temporally consistent magnetic sequences across varied trajectories and environments. Experiments show that embedding these constraints significantly improves predictive accuracy and physical plausibility, outperforming classical and unconstrained deep learning approaches.

</details>


### [139] [Attention Head Entropy of LLMs Predicts Answer Correctness](https://arxiv.org/abs/2602.13699)
*Sophie Ostmeier,Brian Axelrod,Maya Varma,Asad Aali,Yabin Zhang,Magdalini Paschali,Sanmi Koyejo,Curtis Langlotz,Akshay Chaudhari*

Main category: cs.LG

TL;DR: Head Entropy方法通过分析注意力熵模式预测LLM回答正确性，在领域内外均优于基线，尤其能在答案生成前仅基于问题/上下文预测


<details>
  <summary>Details</summary>
Motivation: LLM经常生成看似合理但错误的答案，在医疗等安全关键领域存在风险。人工评估昂贵，LLM作为评判者的方法可能引入隐藏错误。现有白盒方法关注注意力质量定位，但两个问题未解决：能否扩展到预测答案正确性？能否泛化到领域外？

Method: 提出Head Entropy方法，通过注意力熵模式预测答案正确性，具体测量注意力质量的分布。使用稀疏逻辑回归对每个注意力头的2-Renyi熵进行分析

Result: Head Entropy在领域内匹配或超越基线，在领域外泛化能力显著更好，平均比最接近的基线提高+8.5% AUROC。仅基于问题/上下文（答案生成前）的注意力模式已有预测信号，平均比最接近的基线提高+17.7% AUROC

Conclusion: 注意力熵模式能有效预测LLM答案正确性，Head Entropy方法在领域内外均有良好表现，为LLM可靠性评估提供了新的白盒分析方法

Abstract: Large language models (LLMs) often generate plausible yet incorrect answers, posing risks in safety-critical settings such as medicine. Human evaluation is expensive, and LLM-as-judge approaches risk introducing hidden errors. Recent white-box methods detect contextual hallucinations using model internals, focusing on the localization of the attention mass, but two questions remain open: do these approaches extend to predicting answer correctness, and do they generalize out-of-domains? We introduce Head Entropy, a method that predicts answer correctness from attention entropy patterns, specifically measuring the spread of the attention mass. Using sparse logistic regression on per-head 2-Renyi entropies, Head Entropy matches or exceeds baselines in-distribution and generalizes substantially better on out-of-domains, it outperforms the closest baseline on average by +8.5% AUROC. We further show that attention patterns over the question/context alone, before answer generation, already carry predictive signal using Head Entropy with on average +17.7% AUROC over the closest baseline. We evaluate across 5 instruction-tuned LLMs and 3 QA datasets spanning general knowledge, multi-hop reasoning, and medicine.

</details>


### [140] [Optimal Regret for Policy Optimization in Contextual Bandits](https://arxiv.org/abs/2602.13700)
*Orin Levy,Yishay Mansour*

Main category: cs.LG

TL;DR: 首个高概率最优遗憾界的上下文多臂老虎机策略优化算法，实现Õ(√(K|A|log|F|))最优遗憾界


<details>
  <summary>Details</summary>
Motivation: 解决上下文多臂老虎机问题中策略优化方法的理论与实践差距，为广泛使用的策略优化方法提供严格的最优遗憾界证明

Method: 提出高效的策略优化算法，使用通用离线函数逼近，通过理论分析和实验验证

Result: 实现了Õ(√(K|A|log|F|))的最优遗憾界，其中K为轮数，A为臂集合，F为函数类，并通过实验验证了理论结果

Conclusion: 该研究填补了上下文老虎机问题中策略优化方法的理论空白，证明了广泛使用的策略优化方法可以达到严格证明的最优遗憾界

Abstract: We present the first high-probability optimal regret bound for a policy optimization technique applied to the problem of stochastic contextual multi-armed bandit (CMAB) with general offline function approximation. Our algorithm is both efficient and achieves an optimal regret bound of $\widetilde{O}(\sqrt{ K|\mathcal{A}|\log|\mathcal{F}|})$, where $K$ is the number of rounds, $\mathcal{A}$ is the set of arms, and $\mathcal{F}$ is the function class used to approximate the losses. Our results bridge the gap between theory and practice, demonstrating that the widely used policy optimization methods for the contextual bandit problem can achieve a rigorously-proved optimal regret bound. We support our theoretical results with an empirical evaluation of our algorithm.

</details>


### [141] [Near-Optimal Regret for Policy Optimization in Contextual MDPs with General Offline Function Approximation](https://arxiv.org/abs/2602.13706)
*Orin Levy,Aviv Rosenberg,Alon Cohen,Yishay Mansour*

Main category: cs.LG

TL;DR: OPO-CMDP是首个针对随机上下文马尔可夫决策过程(CMDPs)的离线函数逼近策略优化算法，实现了最优的|S|和|A|依赖性的遗憾界。


<details>
  <summary>Details</summary>
Motivation: 当前CMDPs在离线函数逼近下的策略优化算法存在理论局限性，特别是在状态空间|S|和动作空间|A|的依赖性方面不够优化，需要提出更高效的算法。

Method: 提出OPO-CMDP算法，采用乐观策略优化方法，利用有限函数类ℱ和𝒫分别逼近损失函数和动态转移，实现高效的离线函数逼近。

Result: 获得了高概率遗憾界Õ(H⁴√(T|S||A|log(|ℱ||𝒫|)))，这是首个在|S|和|A|依赖性上达到最优的遗憾界，直接改进了当前最先进的结果。

Conclusion: 乐观策略优化为求解CMDPs提供了一条自然、计算效率高且理论接近最优的路径，在理论和计算性能上都表现出优越性。

Abstract: We introduce \texttt{OPO-CMDP}, the first policy optimization algorithm for stochastic Contextual Markov Decision Process (CMDPs) under general offline function approximation. Our approach achieves a high probability regret bound of $\widetilde{O}(H^4\sqrt{T|S||A|\log(|\mathcal{F}||\mathcal{P}|)}),$ where $S$ and $A$ denote the state and action spaces, $H$ the horizon length, $T$ the number of episodes, and $\mathcal{F}, \mathcal{P}$ the finite function classes used to approximate the losses and dynamics, respectively. This is the first regret bound with optimal dependence on $|S|$ and $|A|$, directly improving the current state-of-the-art (Qian, Hu, and Simchi-Levi, 2024). These results demonstrate that optimistic policy optimization provides a natural, computationally superior and theoretically near-optimal path for solving CMDPs.

</details>


### [142] [HBVLA: Pushing 1-Bit Post-Training Quantization for Vision-Language-Action Models](https://arxiv.org/abs/2602.13710)
*Xin Yan,Zhenglin Wan,Feiyang Ye,Xingrui Yu,Hangyu Du,Yang You,Ivor Tsang*

Main category: cs.LG

TL;DR: HBVLA：针对视觉-语言-动作模型的二值化框架，通过策略感知Hessian识别关键权重、稀疏正交变换降低熵、Haar域分组二值化，在硬件受限平台上实现高效部署


<details>
  <summary>Details</summary>
Motivation: VLA模型计算和内存需求大，难以在资源受限的机器人和边缘平台上部署。现有二值化方法无法缩小二值化与全精度权重之间的分布差距，导致量化误差在长时闭环执行中累积，严重影响动作质量

Method: 1) 使用策略感知增强Hessian识别对动作生成真正关键的权重；2) 对非关键权重使用稀疏正交变换诱导低熵中间状态；3) 在Haar域中对关键和非关键权重进行分组1-bit量化

Result: 在LIBERO上，二值化的OpenVLA-OFT保持92.2%全精度性能；在SimplerEnv上，二值化的CogAct保持93.6%性能，显著优于现有二值化方法。真实世界评估显示仅边际性能下降

Conclusion: HBVLA为VLA模型的超低位量化提供了实用基础，能够在硬件受限的机器人平台上实现可靠部署，显著提升部署效率

Abstract: Vision-Language-Action (VLA) models enable instruction-following embodied control, but their large compute and memory footprints hinder deployment on resource-constrained robots and edge platforms. While reducing weights to 1-bit precision through binarization can greatly improve efficiency, existing methods fail to narrow the distribution gap between binarized and full-precision weights, causing quantization errors to accumulate under long-horizon closed-loop execution and severely degrade actions. To fill this gap, we propose HBVLA, a VLA-tailored binarization framework. First, we use a policy-aware enhanced Hessian to identify weights that are truly critical for action generation. Then, we employ a sparse orthogonal transform for non-salient weights to induce a low-entropy intermediate state. Finally, we quantize both salient and non-salient weights in the Harr domain with group-wise 1-bit quantization. We have evaluated our approach on different VLAs: on LIBERO, quantized OpenVLA-OFT retains 92.2% of full-precision performance; on SimplerEnv, quantized CogAct retains 93.6%, significantly outperforming state-of-the-art binarization methods. We further validate our method on real-world evaluation suite and the results show that HBVLA incurs only marginal success-rate degradation compared to the full-precision model, demonstrating robust deployability under tight hardware constraints. Our work provides a practical foundation for ultra-low-bit quantization of VLAs, enabling more reliable deployment on hardware-limited robotic platforms.

</details>


### [143] [Discrete Double-Bracket Flows for Isotropic-Noise Invariant Eigendecomposition](https://arxiv.org/abs/2602.13759)
*ZhiMing Li,JiaHe Feng*

Main category: cs.LG

TL;DR: 提出一种对协方差算子进行矩阵无特征分解的新方法，通过离散双括号流实现各向同性平移不变性，仅依赖于迹自由协方差，具有全局收敛性和改进的样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有随机逼近方法在矩阵向量乘积预言机下存在局限性：要么使用固定步长导致稳定性与协方差算子范数耦合，要么自适应步长因更新消失而变慢。需要一种能处理含噪声协方差算子 $C_k = C_{sig} + σ_k^2 I + E_k$ 的更有效方法。

Method: 引入离散双括号流，其生成器对各向同性平移具有不变性，从而在离散时间级别实现路径对 $σ_k^2 I$ 的不变性。轨迹和最大稳定步长 $η_{max} ∝ 1/‖C_e‖_2^2$ 仅依赖于迹自由协方差 $C_e$。

Result: 通过严格鞍点几何和对角化目标以及输入到状态稳定性分析建立全局收敛性。样本复杂度按 $O(‖C_e‖_2^2/(Δ^2ε))$ 缩放。对退化块的显式表征产生加速的 $O(log(1/ζ))$ 鞍点逃逸率和高概率有限时间收敛保证。

Conclusion: 该方法在矩阵向量乘积预言机下为矩阵无特征分解提供了一种新颖且有效的解决方案，通过各向同性平移不变性克服了现有方法的局限性，实现了改进的收敛性和样本复杂度。

Abstract: We study matrix-free eigendecomposition under a matrix-vector product (MVP) oracle, where each step observes a covariance operator $C_k = C_{sig} + σ_k^2 I + E_k$. Standard stochastic approximation methods either use fixed steps that couple stability to $\|C_k\|_2$, or adapt steps in ways that slow down due to vanishing updates. We introduce a discrete double-bracket flow whose generator is invariant to isotropic shifts, yielding pathwise invariance to $σ_k^2 I$ at the discrete-time level. The resulting trajectory and a maximal stable step size $η_{max} \propto 1/\|C_e\|_2^2$ depend only on the trace-free covariance $C_e$. We establish global convergence via strict-saddle geometry for the diagonalization objective and an input-to-state stability analysis, with sample complexity scaling as $O(\|C_e\|_2^2 / (Δ^2 ε))$ under trace-free perturbations. An explicit characterization of degenerate blocks yields an accelerated $O(\log(1/ζ))$ saddle-escape rate and a high-probability finite-time convergence guarantee.

</details>


### [144] [Data-driven Bi-level Optimization of Thermal Power Systems with embedded Artificial Neural Networks](https://arxiv.org/abs/2602.13746)
*Talha Ansar,Muhammad Mujtaba Abbas,Ramit Debnath,Vivek Dua,Waqar Muhammad Ashraf*

Main category: cs.LG

TL;DR: 提出基于机器学习的双层优化框架(ANN-KKT)，用于工业热力系统的数据驱动优化，通过神经网络近似目标函数并结合KKT条件，实现计算高效的分层优化。


<details>
  <summary>Details</summary>
Motivation: 工业热力系统具有耦合的性能变量和重要性层次结构，使得同时优化计算困难或不可行，限制了集成和可扩展的操作优化。

Method: 提出完全机器学习驱动的双层优化框架，使用人工神经网络(ANN)近似上下层目标函数，通过Karush-Kuhn-Tucker(KKT)最优性条件将下层问题解析嵌入，形成ANN-KKT单层优化框架。

Result: 在基准问题和实际电厂(660MW燃煤电厂和395MW燃气轮机系统)上验证，获得与双层解相当的结果，计算时间仅需0.22-0.88秒，优化输出功率分别为583MW和402MW，热耗率分别为7337kJ/kWh和7542kJ/kWh。

Conclusion: ANN-KKT为工业热力系统的分层数据驱动优化提供了可扩展且计算高效的途径，实现了大规模工程系统的节能运行，有助于工业5.0发展。

Abstract: Industrial thermal power systems have coupled performance variables with hierarchical order of importance, making their simultaneous optimization computationally challenging or infeasible. This barrier limits the integrated and computationally scaleable operation optimization of industrial thermal power systems. To address this issue for large-scale engineering systems, we present a fully machine learning-powered bi-level optimization framework for data-driven optimization of industrial thermal power systems. The objective functions of upper and lower levels are approximated by artificial neural network (ANN) models and the lower-level problem is analytically embedded through Karush-Kuhn-Tucker (KKT) optimality conditions. The reformulated single level optimization framework integrating ANN models and KKT constraints (ANN-KKT) is validated on benchmark problems and on real-world power generation operation of 660 MW coal power plant and 395 MW gas turbine system. The results reveal a comparable solutions obtained from the proposed ANN-KKT framework to the bi-level solutions of the benchmark problems. Marginal computational time requirement (0.22 to 0.88 s) to compute optimal solutions yields 583 MW (coal) and 402 MW (gas turbine) of power output at optimal turbine heat rate of 7337 kJ/kWh and 7542 kJ/kWh, respectively. In addition, the method expands to delineate a feasible and robust operating envelope that accounts for uncertainty in operating variables while maximizing thermal efficiency in various scenarios. These results demonstrate that ANN-KKT offers a scalable and computationally efficient route for hierarchical, data-driven optimization of industrial thermal power systems, achieving energy-efficient operations of large-scale engineering systems and contributing to industry 5.0.

</details>


### [145] [A Penalty Approach for Differentiation Through Black-Box Quadratic Programming Solvers](https://arxiv.org/abs/2602.14154)
*Yuxuan Linghu,Zhiyuan Liu,Qi Deng*

Main category: cs.LG

TL;DR: dXPP：一种基于惩罚的二次规划微分框架，通过解耦QP求解与微分过程，提高大规模问题的计算效率和鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有基于KKT系统的微分方法在大规模问题上存在计算成本高和数值鲁棒性下降的问题，需要一种更高效、更稳健的微分框架

Method: 提出dXPP惩罚微分框架：前向传递使用任意黑盒QP求解器；反向传递将解映射到平滑近似惩罚问题，通过隐式微分仅需求解更小的线性系统

Result: 在随机生成QP、大规模稀疏投影问题和多期投资组合优化等任务上，dXPP与KKT方法竞争，并在大规模问题上实现显著加速

Conclusion: dXPP通过解耦求解与微分，绕过KKT显式微分的困难，显著提高了计算效率和鲁棒性，为大规模可微优化提供了实用解决方案

Abstract: Differentiating through the solution of a quadratic program (QP) is a central problem in differentiable optimization. Most existing approaches differentiate through the Karush--Kuhn--Tucker (KKT) system, but their computational cost and numerical robustness can degrade at scale. To address these limitations, we propose dXPP, a penalty-based differentiation framework that decouples QP solving from differentiation. In the solving step (forward pass), dXPP is solver-agnostic and can leverage any black-box QP solver. In the differentiation step (backward pass), we map the solution to a smooth approximate penalty problem and implicitly differentiate through it, requiring only the solution of a much smaller linear system in the primal variables. This approach bypasses the difficulties inherent in explicit KKT differentiation and significantly improves computational efficiency and robustness. We evaluate dXPP on various tasks, including randomly generated QPs, large-scale sparse projection problems, and a real-world multi-period portfolio optimization task. Empirical results demonstrate that dXPP is competitive with KKT-based differentiation methods and achieves substantial speedups on large-scale problems.

</details>


### [146] [On Representation Redundancy in Large-Scale Instruction Tuning Data Selection](https://arxiv.org/abs/2602.13773)
*Youwei Shu,Shaomian Zheng,Dingnan Jin,Wenjie Qu,Ziyao Guo,Qing Cui,Jun Zhou,Jiaheng Zhang*

Main category: cs.LG

TL;DR: 提出CRDS框架，通过压缩语义表示减少冗余，提升指令调优数据选择质量，仅用3.5%数据即可超越全数据基线


<details>
  <summary>Details</summary>
Motivation: 工业规模指令调优数据选择方法不足，现有LLM编码器产生高度冗余的语义嵌入，影响数据选择效果

Method: 提出CRDS框架：CRDS-R使用Rademacher随机投影+Transformer隐藏层表示拼接；CRDS-W使用白化降维提升表示质量

Result: 两种变体显著提升数据质量，优于现有基于表示的选择方法；CRDS-W仅用3.5%数据即超越全数据基线0.71%（四个数据集平均）

Conclusion: CRDS通过压缩语义表示有效减少冗余，为工业规模指令调优数据选择提供高效解决方案

Abstract: Data quality is a crucial factor in large language models training. While prior work has shown that models trained on smaller, high-quality datasets can outperform those trained on much larger but noisy or low-quality corpora, systematic methods for industrial-scale data selection in instruction tuning remain underexplored. In this work, we study instruction-tuning data selection through the lens of semantic representation similarity and identify a key limitation of state-of-the-art LLM encoders: they produce highly redundant semantic embeddings. To mitigate this redundancy, we propose Compressed Representation Data Selection (CRDS), a novel framework with two variants. CRDS-R applies Rademacher random projection followed by concatenation of transformer hidden-layer representations, while CRDS-W employs whitening-based dimensionality reduction to improve representational quality. Experimental results demonstrate that both variants substantially enhance data quality and consistently outperform state-of-the-art representation-based selection methods. Notably, CRDS-W achieves strong performance using only 3.5% of the data, surpassing the full-data baseline by an average of 0.71% across four datasets. Our code is available at https://github.com/tdano1/CRDS.

</details>


### [147] [MEMTS: Internalizing Domain Knowledge via Parameterized Memory for Retrieval-Free Domain Adaptation of Time Series Foundation Models](https://arxiv.org/abs/2602.13783)
*Xiaoyun Yu,Li fan,Xiangfei Qiu,Nanqing Dong,Yonggui Huang,Honggang Qi,Geguang Pu,Wanli Ouyang,Xi Chen,Jilin Hu*

Main category: cs.LG

TL;DR: MEMTS是一种轻量级即插即用的时间序列检索自由域适应方法，通过知识持久化模块将领域特定时间动态内化为可学习的潜在原型，实现恒定时间推理和近零延迟，同时避免灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 时间序列基础模型在垂直领域部署时面临时间分布偏移和领域特定周期结构问题，现有方法如领域自适应预训练会破坏全局时间模式，检索增强生成则引入检索开销，无法满足实时流处理的高效要求。

Method: 提出MEMTS方法，核心是知识持久化模块，将领域特定的时间动态（如季节性模式和趋势）内化为紧凑的可学习潜在原型，将碎片化历史观测转化为连续参数化知识表示，无需修改冻结的基础模型架构。

Result: 在多个数据集上的广泛实验表明，MEMTS实现了最先进的性能，能够以恒定时间推理和近零延迟实现准确的领域适应，同时有效缓解对通用时间模式的灾难性遗忘。

Conclusion: MEMTS通过将领域知识内化为潜在原型的范式转变，解决了时间序列基础模型在垂直领域部署时的效率和遗忘问题，为实时流处理提供了可行的解决方案。

Abstract: While Time Series Foundation Models (TSFMs) have demonstrated exceptional performance in generalized forecasting, their performance often degrades significantly when deployed in real-world vertical domains characterized by temporal distribution shifts and domain-specific periodic structures. Current solutions are primarily constrained by two paradigms: Domain-Adaptive Pretraining (DAPT), which improves short-term domain fitting but frequently disrupts previously learned global temporal patterns due to catastrophic forgetting; and Retrieval-Augmented Generation (RAG), which incorporates external knowledge but introduces substantial retrieval overhead. This creates a severe scalability bottleneck that fails to meet the high-efficiency requirements of real-time stream processing. To break this impasse, we propose Memory for Time Series (MEMTS), a lightweight and plug-and-play method for retrieval-free domain adaptation in time series forecasting. The key component of MEMTS is a Knowledge Persistence Module (KPM), which internalizes domain-specific temporal dynamics, such as recurring seasonal patterns and trends into a compact set of learnable latent prototypes. In doing so, it transforms fragmented historical observations into continuous, parameterized knowledge representations. This paradigm shift enables MEMTS to achieve accurate domain adaptation with constant-time inference and near-zero latency, while effectively mitigating catastrophic forgetting of general temporal patterns, all without requiring any architectural modifications to the frozen TSFM backbone. Extensive experiments on multiple datasets demonstrate the SOTA performance of MEMTS.

</details>


### [148] [Decoupled Continuous-Time Reinforcement Learning via Hamiltonian Flow](https://arxiv.org/abs/2602.14587)
*Minh Nguyen*

Main category: cs.LG

TL;DR: 提出一种解耦的连续时间演员-评论家算法，通过交替更新解决标准离散时间RL在连续时间事件驱动决策中的局限性，在连续控制基准和真实交易任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 许多现实控制问题（金融、机器人等）在连续时间中演化，具有非均匀、事件驱动的决策特性。标准离散时间RL基于固定步长Bellman更新，在这种设置下表现不佳：当时间间隔缩小时，Q函数会坍缩到值函数V，失去动作排序能力。现有连续时间方法通过优势率函数q重新引入动作信息，但使用复杂的鞅损失或正交约束来强制最优性，这些方法对测试过程的选择敏感，并将V和q耦合到难以可靠训练的大型复杂优化问题中。

Method: 提出一种解耦的连续时间演员-评论家算法，采用交替更新策略：q从V的扩散生成器中学习，V通过基于哈密顿量的值流更新，该值流在无穷小时间步下仍保持信息性（而标准的max/softmax备份会失效）。该方法避免了V和q的复杂耦合优化问题。

Result: 理论上，通过新的概率论证证明了严格的收敛性，绕过了基于生成器的哈密顿量在sup-norm下缺乏Bellman式收缩的挑战。实证上，该方法在具有挑战性的连续控制基准和真实世界交易任务中优于先前的连续时间方法和领先的离散时间基线，在单个季度内实现了21%的利润，几乎是第二佳方法的两倍。

Conclusion: 该研究提出了一种新颖的解耦连续时间演员-评论家算法，有效解决了连续时间RL中的关键挑战，在理论和实证上都取得了显著成果，为连续时间事件驱动决策问题提供了更可靠、高效的解决方案。

Abstract: Many real-world control problems, ranging from finance to robotics, evolve in continuous time with non-uniform, event-driven decisions. Standard discrete-time reinforcement learning (RL), based on fixed-step Bellman updates, struggles in this setting: as time gaps shrink, the $Q$-function collapses to the value function $V$, eliminating action ranking. Existing continuous-time methods reintroduce action information via an advantage-rate function $q$. However, they enforce optimality through complicated martingale losses or orthogonality constraints, which are sensitive to the choice of test processes. These approaches entangle $V$ and $q$ into a large, complex optimization problem that is difficult to train reliably. To address these limitations, we propose a novel decoupled continuous-time actor-critic algorithm with alternating updates: $q$ is learned from diffusion generators on $V$, and $V$ is updated via a Hamiltonian-based value flow that remains informative under infinitesimal time steps, where standard max/softmax backups fail. Theoretically, we prove rigorous convergence via new probabilistic arguments, sidestepping the challenge that generator-based Hamiltonians lack Bellman-style contraction under the sup-norm. Empirically, our method outperforms prior continuous-time and leading discrete-time baselines across challenging continuous-control benchmarks and a real-world trading task, achieving 21% profit over a single quarter$-$nearly doubling the second-best method.

</details>


### [149] [MechPert: Mechanistic Consensus as an Inductive Bias for Unseen Perturbation Prediction](https://arxiv.org/abs/2602.13791)
*Marc Boubnovski Martell,Josefa Lia Stoisser,Lawrence Phillips,Aditya Misra,Robert Kitchen,Jesper Ferkinghoff-Borg,Jialin Yu,Philip Torr,Kaspar Märten*

Main category: cs.LG

TL;DR: MechPert是一个轻量级框架，使用LLM代理生成定向调控假设而非依赖功能相似性，通过共识机制聚合候选调控因子，在低数据扰动预测和实验设计方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖静态、可能不完整的知识图谱，或使用语言模型基于功能相似性检索关联，这些关联基于科学文本中的对称共现而非定向调控逻辑。需要能够预测未见遗传扰动转录响应的方法。

Method: MechPert框架鼓励LLM代理生成定向调控假设而非依赖功能相似性。多个代理独立提出候选调控因子及置信度分数，通过共识机制聚合过滤虚假关联，生成加权邻域用于下游预测。

Result: 在四个人类细胞系的Perturb-seq基准测试中，低数据情况下（N=50个观测扰动）MechPert比基于相似性的基线方法提升Pearson相关性达10.5%。在实验设计中，MechPert选择的锚基因在特征明确的细胞系中比标准网络中心性启发式方法表现提升达46%。

Conclusion: MechPert通过LLM代理生成定向调控假设和共识机制，能够有效预测未见遗传扰动的转录响应，在低数据扰动预测和实验设计方面优于现有方法，为大规模扰动实验优先排序提供了有效工具。

Abstract: Predicting transcriptional responses to unseen genetic perturbations is essential for understanding gene regulation and prioritizing large-scale perturbation experiments. Existing approaches either rely on static, potentially incomplete knowledge graphs, or prompt language models for functionally similar genes, retrieving associations shaped by symmetric co-occurrence in scientific text rather than directed regulatory logic. We introduce MechPert, a lightweight framework that encourages LLM agents to generate directed regulatory hypotheses rather than relying solely on functional similarity. Multiple agents independently propose candidate regulators with associated confidence scores; these are aggregated through a consensus mechanism that filters spurious associations, producing weighted neighborhoods for downstream prediction. We evaluate MechPert on Perturb-seq benchmarks across four human cell lines. For perturbation prediction in low-data regimes ($N=50$ observed perturbations), MechPert improves Pearson correlation by up to 10.5\% over similarity-based baselines. For experimental design, MechPert-selected anchor genes outperform standard network centrality heuristics by up to 46\% in well-characterized cell lines.

</details>


### [150] [An Embarrassingly Simple Way to Optimize Orthogonal Matrices at Scale](https://arxiv.org/abs/2602.14656)
*Adrián Javaloy,Antonio Vergari*

Main category: cs.LG

TL;DR: 提出POGO算法，一种快速GPU友好的正交约束优化器，相比现有方法显著提升效率，能在几分钟内优化数千个正交矩阵问题


<details>
  <summary>Details</summary>
Motivation: 正交约束在鲁棒和概率机器学习中普遍存在，但现有优化器计算昂贵且难以扩展到数百或数千个约束。虽然Landing算法有所改进，但需要暂时放松正交性约束

Method: 重新审视并改进Landing算法的思想，结合现代自适应优化器，确保正交约束有效满足。POGO算法仅需5个矩阵乘积，始终保持正交性

Result: POGO在多个挑战性基准测试中大幅优于近期优化器，能在几分钟内优化数千个正交矩阵问题，而替代方法需要数小时

Conclusion: POGO为在机器学习中大规模利用正交约束设定了里程碑，提供了高效实用的解决方案

Abstract: Orthogonality constraints are ubiquitous in robust and probabilistic machine learning. Unfortunately, current optimizers are computationally expensive and do not scale to problems with hundreds or thousands of constraints. One notable exception is the Landing algorithm (Ablin et al., 2024) which, however comes at the expense of temporarily relaxing orthogonality. In this work, we revisit and improve on the ideas behind Landing, enabling the inclusion of modern adaptive optimizers while ensuring that orthogonal constraints are effectively met. Remarkably, these improvements come at little to no cost, and reduce the number of required hyperparemeters. Our algorithm POGO is fast and GPU-friendly, consisting of only 5 matrix products, and in practice maintains orthogonality at all times. On several challenging benchmarks, POGO greatly outperforms recent optimizers and shows it can optimize problems with thousands of orthogonal matrices in minutes while alternatives would take hours. As such, POGO sets a milestone to finally exploit orthogonality constraints in ML at scale. A PyTorch implementation of POGO is publicly available at https://github.com/adrianjav/pogo.

</details>


### [151] [Cast-R1: Learning Tool-Augmented Sequential Decision Policies for Time Series Forecasting](https://arxiv.org/abs/2602.13802)
*Xiaoyu Tao,Mingyue Cheng,Chuang Jiang,Tian Gao,Huanjian Zhang,Yaguo Liu*

Main category: cs.LG

TL;DR: Cast-R1将时间序列预测重新定义为顺序决策问题，通过基于记忆的状态管理和工具增强的智能体工作流实现迭代预测优化


<details>
  <summary>Details</summary>
Motivation: 传统模型中心方法将预测视为从历史观测到未来值的单次映射，在复杂动态环境中表现不佳，缺乏自主获取证据、推理未来变化或迭代修正预测的能力

Method: 提出Cast-R1框架：1) 基于记忆的状态管理机制跨交互步骤维护决策相关信息；2) 工具增强的智能体工作流，自主与模块化工具包交互提取统计特征、调用轻量预测模型、进行基于推理的预测并通过自我反思迭代优化；3) 两阶段训练策略结合监督微调和多轮强化学习，辅以课程学习逐步增加任务难度

Result: 在多个真实世界时间序列数据集上的广泛实验证明了Cast-R1的有效性

Conclusion: 这项工作为时间序列建模的智能体范式探索提供了实际步骤，展示了将预测重新定义为顺序决策问题的潜力

Abstract: Time series forecasting has long been dominated by model-centric approaches that formulate prediction as a single-pass mapping from historical observations to future values. Despite recent progress, such formulations often struggle in complex and evolving settings, largely because most forecasting models lack the ability to autonomously acquire informative evidence, reason about potential future changes, or revise predictions through iterative decision processes. In this work, we propose Cast-R1, a learned time series forecasting framework that reformulates forecasting as a sequential decision-making problem. Cast-R1 introduces a memory-based state management mechanism that maintains decision-relevant information across interaction steps, enabling the accumulation of contextual evidence to support long-horizon reasoning. Building on this formulation, forecasting is carried out through a tool-augmented agentic workflow, in which the agent autonomously interacts with a modular toolkit to extract statistical features, invoke lightweight forecasting models for decision support, perform reasoning-based prediction, and iteratively refine forecasts through self-reflection. To train Cast-R1, we adopt a two-stage learning strategy that combines supervised fine-tuning with multi-turn reinforcement learning, together with a curriculum learning scheme that progressively increases task difficulty to improve policy learning. Extensive experiments on multiple real-world time series datasets demonstrate the effectiveness of Cast-R1. We hope this work provides a practical step towards further exploration of agentic paradigms for time series modeling. Our code is available at https://github.com/Xiaoyu-Tao/Cast-R1-TS.

</details>


### [152] [Exposing Diversity Bias in Deep Generative Models: Statistical Origins and Correction of Diversity Error](https://arxiv.org/abs/2602.14682)
*Farzan Farnia,Mohammad Jalali,Azim Ospanov*

Main category: cs.LG

TL;DR: 研究发现现代生成模型存在系统性多样性低估偏差，测试数据多样性显著高于生成样本，并提出基于熵的多样性评分和正则化策略来缓解此问题。


<details>
  <summary>Details</summary>
Motivation: 虽然深度生成模型在样本质量上取得了巨大成功，但模型是否忠实捕捉了底层数据分布的多样性这一重要问题尚未得到系统研究。本文旨在探究生成模型在多样性方面的表现。

Method: 使用无参考的基于熵的多样性评分方法（Vendi和RKE）直接比较最先进生成模型生成的样本与测试数据样本的多样性。分析有限样本下熵基多样性评分的行为，并探讨基于Vendi和RKE的多样性感知正则化和引导策略。

Result: 在多个基准数据集上，测试数据始终获得比生成样本显著更高的Vendi和RKE多样性分数，表明现代生成模型存在系统性向下多样性偏差。研究发现多样性评分的期望值随样本量增加而增加，导致基于有限训练集估计的多样性会固有地低估真实分布的多样性。

Conclusion: 优化生成器以最小化与经验数据分布的散度会导致多样性损失。基于Vendi和RKE的多样性感知正则化和引导策略是缓解这种偏差的有原则方向，实证证据表明这些策略有潜力改善结果。

Abstract: Deep generative models have achieved great success in producing high-quality samples, making them a central tool across machine learning applications. Beyond sample quality, an important yet less systematically studied question is whether trained generative models faithfully capture the diversity of the underlying data distribution. In this work, we address this question by directly comparing the diversity of samples generated by state-of-the-art models with that of test samples drawn from the target data distribution, using recently proposed reference-free entropy-based diversity scores, Vendi and RKE. Across multiple benchmark datasets, we find that test data consistently attains substantially higher Vendi and RKE diversity scores than the generated samples, suggesting a systematic downward diversity bias in modern generative models. To understand the origin of this bias, we analyze the finite-sample behavior of entropy-based diversity scores and show that their expected values increase with sample size, implying that diversity estimated from finite training sets could inherently underestimate the diversity of the true distribution. As a result, optimizing the generators to minimize divergence to empirical data distributions would induce a loss of diversity. Finally, we discuss potential diversity-aware regularization and guidance strategies based on Vendi and RKE as principled directions for mitigating this bias, and provide empirical evidence suggesting their potential to improve the results.

</details>


### [153] [Fast Physics-Driven Untrained Network for Highly Nonlinear Inverse Scattering Problems](https://arxiv.org/abs/2602.13805)
*Yutong Du,Zicheng Liu,Yi Huang,Bazargul Matkerim,Bo Qi,Yali Zong,Peixian Han*

Main category: cs.LG

TL;DR: 提出基于傅里叶谱域的实时物理驱动求解器，通过谱域降维实现亚秒级电磁逆散射重建，相比现有方法提速100倍。


<details>
  <summary>Details</summary>
Motivation: 未训练神经网络（UNNs）虽然能实现高保真电磁逆散射重建，但受限于高维空间域优化的计算复杂度，难以满足实时应用需求。

Method: 1. 使用截断傅里叶基展开感应电流，将优化限制在紧凑的低频参数空间；2. 集成收缩积分方程（CIE）缓解高对比度非线性；3. 采用对比度补偿算子（CCO）校正谱域引起的衰减；4. 设计桥抑制损失函数增强相邻散射体边界锐度。

Result: 数值和实验结果表明，相比最先进的UNNs方法，实现了100倍的速度提升，在噪声和天线不确定性下仍保持鲁棒性能。

Conclusion: 提出的PDF求解器通过谱域降维和物理驱动优化，成功实现了实时微波成像，为电磁逆散射的实际应用提供了高效解决方案。

Abstract: Untrained neural networks (UNNs) offer high-fidelity electromagnetic inverse scattering reconstruction but are computationally limited by high-dimensional spatial-domain optimization. We propose a Real-Time Physics-Driven Fourier-Spectral (PDF) solver that achieves sub-second reconstruction through spectral-domain dimensionality reduction. By expanding induced currents using a truncated Fourier basis, the optimization is confined to a compact low-frequency parameter space supported by scattering measurements. The solver integrates a contraction integral equation (CIE) to mitigate high-contrast nonlinearity and a contrast-compensated operator (CCO) to correct spectral-induced attenuation. Furthermore, a bridge-suppressing loss is formulated to enhance boundary sharpness between adjacent scatterers. Numerical and experimental results demonstrate a 100-fold speedup over state-of-the-art UNNs with robust performance under noise and antenna uncertainties, enabling real-time microwave imaging applications.

</details>


### [154] [AnomaMind: Agentic Time Series Anomaly Detection with Tool-Augmented Reasoning](https://arxiv.org/abs/2602.13807)
*Xiaoyu Tao,Yuchong Wu,Mingyue Cheng,Ze Guo,Tian Gao*

Main category: cs.LG

TL;DR: AnomaMind：将时间序列异常检测重构为顺序决策过程的智能体框架，通过工具交互、自反思和强化学习实现自适应特征准备和推理感知检测


<details>
  <summary>Details</summary>
Motivation: 现有方法将异常检测视为纯判别性预测任务，缺乏自适应特征准备、推理感知检测和迭代优化，难以处理上下文依赖和多样模式的异常

Method: 提出AnomaMind框架：1) 粗到细逐步定位异常区间；2) 多轮工具交互实现自适应特征准备；3) 自反思优化异常决策；4) 混合推理机制（通用模型负责工具交互和自反思，强化学习负责核心检测决策）

Result: 在多样化设置下的广泛实验表明，AnomaMind持续提升异常检测性能

Conclusion: 将异常检测重构为顺序决策过程，通过工具增强的推理框架实现自适应特征准备和推理感知检测，为复杂异常检测问题提供了有效解决方案

Abstract: Time series anomaly detection is critical in many real-world applications, where effective solutions must localize anomalous regions and support reliable decision-making under complex settings. However, most existing methods frame anomaly detection as a purely discriminative prediction task with fixed feature inputs, rather than an evidence-driven diagnostic process. As a result, they often struggle when anomalies exhibit strong context dependence or diverse patterns. We argue that these limitations stem from the lack of adaptive feature preparation, reasoning-aware detection, and iterative refinement during inference. To address these challenges, we propose AnomaMind, an agentic time series anomaly detection framework that reformulates anomaly detection as a sequential decision-making process. AnomaMind operates through a structured workflow that progressively localizes anomalous intervals in a coarse-to-fine manner, augments detection through multi-turn tool interactions for adaptive feature preparation, and refines anomaly decisions via self-reflection. The workflow is supported by a set of reusable tool engines, enabling context-aware diagnostic analysis. A key design of AnomaMind is an explicitly designed hybrid inference mechanism for tool-augmented anomaly detection. In this mechanism, general-purpose models are responsible for autonomous tool interaction and self-reflective refinement, while core anomaly detection decisions are learned through reinforcement learning under verifiable workflow-level feedback, enabling task-specific optimization within a flexible reasoning framework. Extensive experiments across diverse settings demonstrate that AnomaMind consistently improves anomaly detection performance. The code is available at https://anonymous.4open.science/r/AnomaMind.

</details>


### [155] [Mean Flow Policy with Instantaneous Velocity Constraint for One-step Action Generation](https://arxiv.org/abs/2602.13810)
*Guojian Zhan,Letian Tao,Pengcheng Wang,Yixiao Wang,Yiheng Li,Yuxin Chen,Masayoshi Tomizuka,Shengbo Eben Li*

Main category: cs.LG

TL;DR: 提出MVP（平均速度策略），一种新的生成式策略函数，通过建模平均速度场实现最快的一步动作生成，在机器人操作任务中达到SOTA性能并显著提升训练和推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有基于流的策略在建模复杂动作分布时面临表达能力和计算负担之间的权衡，通常通过流步骤数量来控制。需要一种既能保持高表达能力又能实现快速动作生成的方法。

Method: 提出平均速度策略（MVP），建模平均速度场以实现最快的一步动作生成。引入瞬时速度约束（IVC）确保高表达能力，该约束作为关键边界条件，提高学习精度和策略表达能力。

Result: 在Robomimic和OGBench的多个挑战性机器人操作任务中达到最先进的成功率，相比现有基于流的策略基线，在训练和推理速度上都有显著提升。

Conclusion: MVP是一种有效的生成式策略函数，通过建模平均速度场和引入IVC约束，在保持高表达能力的同时实现了快速的动作生成，为强化学习中的策略表达提供了新的方向。

Abstract: Learning expressive and efficient policy functions is a promising direction in reinforcement learning (RL). While flow-based policies have recently proven effective in modeling complex action distributions with a fast deterministic sampling process, they still face a trade-off between expressiveness and computational burden, which is typically controlled by the number of flow steps. In this work, we propose mean velocity policy (MVP), a new generative policy function that models the mean velocity field to achieve the fastest one-step action generation. To ensure its high expressiveness, an instantaneous velocity constraint (IVC) is introduced on the mean velocity field during training. We theoretically prove that this design explicitly serves as a crucial boundary condition, thereby improving learning accuracy and enhancing policy expressiveness. Empirically, our MVP achieves state-of-the-art success rates across several challenging robotic manipulation tasks from Robomimic and OGBench. It also delivers substantial improvements in training and inference speed over existing flow-based policy baselines.

</details>


### [156] [Variance-Reduced $(\varepsilon,δ)-$Unlearning using Forget Set Gradients](https://arxiv.org/abs/2602.14938)
*Martin Van Waerebeke,Marco Lorenzi,Kevin Scaman,El Mahdi El Mhamdi,Giovanni Neglia*

Main category: cs.LG

TL;DR: 提出了首个在更新规则中直接包含遗忘集梯度的一阶算法VRU，在保证(ε,δ)-遗忘的同时，相比现有方法获得更快的收敛速率。


<details>
  <summary>Details</summary>
Motivation: 现有(ε,δ)-遗忘方法仅使用遗忘集来校准注入的噪声，而不将其作为直接优化信号；而高效的启发式方法虽然利用遗忘样本但缺乏形式化保证。需要弥合这一差距。

Method: 提出方差减少遗忘(VRU)算法，在更新规则中直接包含遗忘集梯度，通过理论证明满足(ε,δ)-遗忘保证，并建立其收敛性。

Result: VRU是首个直接利用遗忘集梯度的一阶算法，在保证形式化遗忘的同时，相比忽略遗忘集的方法获得严格改进的收敛速率，在低误差区域表现更优。

Conclusion: VRU算法成功弥合了形式化保证方法与高效启发式方法之间的差距，实验验证了其在最先进认证遗忘方法和利用遗忘集的基线方法上的持续优势。

Abstract: In machine unlearning, $(\varepsilon,δ)-$unlearning is a popular framework that provides formal guarantees on the effectiveness of the removal of a subset of training data, the forget set, from a trained model. For strongly convex objectives, existing first-order methods achieve $(\varepsilon,δ)-$unlearning, but they only use the forget set to calibrate injected noise, never as a direct optimization signal. In contrast, efficient empirical heuristics often exploit the forget samples (e.g., via gradient ascent) but come with no formal unlearning guarantees. We bridge this gap by presenting the Variance-Reduced Unlearning (VRU) algorithm. To the best of our knowledge, VRU is the first first-order algorithm that directly includes forget set gradients in its update rule, while provably satisfying ($(\varepsilon,δ)-$unlearning. We establish the convergence of VRU and show that incorporating the forget set yields strictly improved rates, i.e. a better dependence on the achieved error compared to existing first-order $(\varepsilon,δ)-$unlearning methods. Moreover, we prove that, in a low-error regime, VRU asymptotically outperforms any first-order method that ignores the forget set.Experiments corroborate our theory, showing consistent gains over both state-of-the-art certified unlearning methods and over empirical baselines that explicitly leverage the forget set.

</details>


### [157] [Pawsterior: Variational Flow Matching for Structured Simulation-Based Inference](https://arxiv.org/abs/2602.13813)
*Jorge Carrasco-Pollo,Floor Eijkelboom,Jan-Willem van de Meent*

Main category: cs.LG

TL;DR: Pawsterior是一个变分流匹配框架，用于改进和扩展基于模拟的推理（SBI），特别针对具有结构化域约束（如有界物理参数或混合离散-连续变量）的后验分布问题。


<details>
  <summary>Details</summary>
Motivation: 许多SBI问题涉及受结构化域约束的后验分布，如物理参数有界或混合离散-连续变量，但标准流匹配方法通常在无约束空间中操作，导致学习效率低下且难以满足物理约束。

Method: 提出端点诱导的仿射几何约束原理，将域几何直接纳入推理过程，通过双向变分模型改进数值稳定性；变分参数化使SBI能够处理离散潜结构（如切换系统），这是传统流匹配方法无法处理的。

Result: 在标准SBI基准测试中，通过改进的分类器双样本测试性能，展示了更好的后验保真度；能够处理传统流匹配方法无法处理的离散潜结构问题。

Conclusion: Pawsterior通过同时解决几何约束和离散潜结构问题，将流匹配扩展到更广泛的结构化SBI问题类别，这些问题是以前无法访问的。

Abstract: We introduce Pawsterior, a variational flow-matching framework for improved and extended simulation-based inference (SBI). Many SBI problems involve posteriors constrained by structured domains, such as bounded physical parameters or hybrid discrete-continuous variables, yet standard flow-matching methods typically operate in unconstrained spaces. This mismatch leads to inefficient learning and difficulty respecting physical constraints. Our contributions are twofold. First, generalizing the geometric inductive bias of CatFlow, we formalize endpoint-induced affine geometric confinement, a principle that incorporates domain geometry directly into the inference process via a two-sided variational model. This formulation improves numerical stability during sampling and leads to consistently better posterior fidelity, as demonstrated by improved classifier two-sample test performance across standard SBI benchmarks. Second, and more importantly, our variational parameterization enables SBI tasks involving discrete latent structure (e.g., switching systems) that are fundamentally incompatible with conventional flow-matching approaches. By addressing both geometric constraints and discrete latent structure, Pawsterior extends flow-matching to a broader class of structured SBI problems that were previously inaccessible.

</details>


### [158] [sleep2vec: Unified Cross-Modal Alignment for Heterogeneous Nocturnal Biosignals](https://arxiv.org/abs/2602.13857)
*Weixuan Yuan,Zengrui Jin,Yichen Wang,Donglin Xie,Ziyi Ye,Chao Zhang,Xuesong Chen*

Main category: cs.LG

TL;DR: Sleep2vec是一个用于处理多样且不完整夜间生物信号的基础模型，通过跨模态对齐学习共享表示，在睡眠分期和临床评估任务上表现优异，并首次描述了夜间生物信号的缩放规律。


<details>
  <summary>Details</summary>
Motivation: 传统睡眠监测和临床诊断依赖于多种设备（PSG、床边监测仪、可穿戴设备）采集的不同夜间生物信号（EEG、EOG、ECG、SpO2等），但设备异质性和传感器频繁丢失给多模态信号的统一建模带来了重大挑战。

Method: 提出sleep2vec基础模型，通过跨模态对齐学习共享表示。使用包含人口统计学、年龄、站点和历史信息的InfoNCE目标进行对比预训练，动态加权负样本以减轻队列特异性捷径。在42,249个夜间记录（涵盖9种模态）上进行预训练。

Result: 在睡眠分期和临床结果评估的下游任务中，sleep2vec始终优于强基线，并对任何可用模态子集和传感器丢失保持鲁棒性。首次描述了夜间生物信号在模态多样性和模型容量方面的缩放规律。

Conclusion: 统一的跨模态对齐结合原则性缩放，能够实现现实世界夜间生物信号的标签高效、通用建模。

Abstract: Tasks ranging from sleep staging to clinical diagnosis traditionally rely on standard polysomnography (PSG) devices, bedside monitors and wearable devices, which capture diverse nocturnal biosignals (e.g., EEG, EOG, ECG, SpO$_2$). However, heterogeneity across devices and frequent sensor dropout pose significant challenges for unified modelling of these multimodal signals. We present \texttt{sleep2vec}, a foundation model for diverse and incomplete nocturnal biosignals that learns a shared representation via cross-modal alignment. \texttt{sleep2vec} is contrastively pre-trained on 42,249 overnight recordings spanning nine modalities using a \textit{Demography, Age, Site \& History-aware InfoNCE} objective that incorporates physiological and acquisition metadata (\textit{e.g.}, age, gender, recording site) to dynamically weight negatives and mitigate cohort-specific shortcuts. On downstream sleep staging and clinical outcome assessment, \texttt{sleep2vec} consistently outperforms strong baselines and remains robust to any subset of available modalities and sensor dropout. We further characterize, to our knowledge for the first time, scaling laws for nocturnal biosignals with respect to modality diversity and model capacity. Together, these results show that unified cross-modal alignment, coupled with principled scaling, enables label-efficient, general-purpose modelling of real-world nocturnal biosignals.

</details>


### [159] [Sufficient Conditions for Stability of Minimum-Norm Interpolating Deep ReLU Networks](https://arxiv.org/abs/2602.13910)
*Ouns El Harzli,Yoonsoo Nam,Ilja Kuzborskij,Bernardo Cuenca Grau,Ard A. Louis*

Main category: cs.LG

TL;DR: 本文研究了深度ReLU齐次神经网络在实现零训练误差时的算法稳定性，发现当网络包含稳定子网络且后续层具有低秩权重矩阵时，网络是稳定的；但如果后续层不是低秩的，即使包含稳定子网络也不能保证稳定性。


<details>
  <summary>Details</summary>
Motivation: 算法稳定性是分析学习算法泛化误差的经典框架，但该框架在深度神经网络分析中应用有限。本文旨在研究深度ReLU齐次神经网络在实现最小L2范数插值（零训练误差）时的稳定性条件。

Method: 研究深度ReLU齐次神经网络，这些网络通过最小L2范数参数实现零训练误差（最小范数插值）。分析此类网络稳定性的充分条件，特别关注网络是否包含稳定子网络以及后续层的权重矩阵是否为低秩。

Result: 发现两个关键结果：1）当网络包含稳定子网络且后续层具有低秩权重矩阵时，网络是稳定的；2）即使网络包含稳定子网络，如果后续层不是低秩的，也不能保证网络稳定性。低秩假设受到最近实证和理论研究的启发，表明深度神经网络训练偏向于产生低秩权重矩阵。

Conclusion: 深度ReLU齐次神经网络在最小范数插值下的算法稳定性取决于两个关键因素：是否存在稳定子网络以及后续层是否为低秩。这为理解过参数化深度神经网络的泛化能力提供了新的理论视角。

Abstract: Algorithmic stability is a classical framework for analyzing the generalization error of learning algorithms. It predicts that an algorithm has small generalization error if it is insensitive to small perturbations in the training set such as the removal or replacement of a training point. While stability has been demonstrated for numerous well-known algorithms, this framework has had limited success in analyses of deep neural networks. In this paper we study the algorithmic stability of deep ReLU homogeneous neural networks that achieve zero training error using parameters with the smallest $L_2$ norm, also known as the minimum-norm interpolation, a phenomenon that can be observed in overparameterized models trained by gradient-based algorithms. We investigate sufficient conditions for such networks to be stable. We find that 1) such networks are stable when they contain a (possibly small) stable sub-network, followed by a layer with a low-rank weight matrix, and 2) such networks are not guaranteed to be stable even when they contain a stable sub-network, if the following layer is not low-rank. The low-rank assumption is inspired by recent empirical and theoretical results which demonstrate that training deep neural networks is biased towards low-rank weight matrices, for minimum-norm interpolation and weight-decay regularization.

</details>


### [160] [GREPO: A Benchmark for Graph Neural Networks on Repository-Level Bug Localization](https://arxiv.org/abs/2602.13921)
*Juntong Wang,Libin Chen,Xiyuan Wang,Shijia Kang,Haotong Yang,Da Zheng,Muhan Zhang*

Main category: cs.LG

TL;DR: GREPO是首个用于仓库级bug定位的GNN基准，包含86个Python仓库和47294个bug修复任务，评估显示GNN在bug定位任务上优于传统检索方法。


<details>
  <summary>Details</summary>
Motivation: 仓库级bug定位是关键的软件工程挑战，但标准LLM因上下文窗口限制无法处理整个代码仓库，现有检索方法（关键词匹配、文本相似度、简单图启发式）有限，而GNN虽有潜力但缺乏专用基准。

Method: 引入GREPO基准，包含86个Python仓库和47294个bug修复任务，提供可直接用于GNN处理的图数据结构，评估了多种GNN架构。

Result: GNN架构在bug定位任务上表现出色，优于传统信息检索基线方法。

Conclusion: GNN在bug定位方面具有巨大潜力，GREPO为未来研究提供了基础资源，代码已开源。

Abstract: Repository-level bug localization-the task of identifying where code must be modified to fix a bug-is a critical software engineering challenge. Standard Large Language Modles (LLMs) are often unsuitable for this task due to context window limitations that prevent them from processing entire code repositories. As a result, various retrieval methods are commonly used, including keyword matching, text similarity, and simple graph-based heuristics such as Breadth-First Search. Graph Neural Networks (GNNs) offer a promising alternative due to their ability to model complex, repository-wide dependencies; however, their application has been hindered by the lack of a dedicated benchmark. To address this gap, we introduce GREPO, the first GNN benchmark for repository-scale bug localization tasks. GREPO comprises 86 Python repositories and 47294 bug-fixing tasks, providing graph-based data structures ready for direct GNN processing. Our evaluation of various GNN architectures shows outstanding performance compared to established information retrieval baselines. This work highlights the potential of GNNs for bug localization and established GREPO as a foundation resource for future research, The code is available at https://github.com/qingpingmo/GREPO.

</details>


### [161] [Why Code, Why Now: Learnability, Computability, and the Real Limits of Machine Learning](https://arxiv.org/abs/2602.13934)
*Zhimin Zhao*

Main category: cs.LG

TL;DR: 论文提出基于信息结构的五级可学习性层次，解释为何代码生成比强化学习更可靠，并质疑仅靠模型规模就能解决所有ML挑战的假设


<details>
  <summary>Details</summary>
Motivation: 解释为什么代码生成比强化学习进展更可靠，揭示任务可学习性的根本差异，挑战仅靠模型规模就能解决所有ML问题的常见假设

Method: 提出基于信息结构的五级可学习性层次，区分计算问题的三个属性（可表达性、可计算性、可学习性），建立它们之间的形式化关系，提供统一模板明确结构差异

Result: 分析表明代码的监督学习可预测地扩展而强化学习不能，揭示了任务可学习性对ML进展的限制比模型规模更重要

Conclusion: ML进展的天花板更多取决于任务是否可学习，而非模型规模，需要重新审视仅靠扩展就能解决剩余ML挑战的假设

Abstract: Code generation has progressed more reliably than reinforcement learning, largely because code has an information structure that makes it learnable. Code provides dense, local, verifiable feedback at every token, whereas most reinforcement learning problems do not. This difference in feedback quality is not binary but graded. We propose a five-level hierarchy of learnability based on information structure and argue that the ceiling on ML progress depends less on model size than on whether a task is learnable at all. The hierarchy rests on a formal distinction among three properties of computational problems (expressibility, computability, and learnability). We establish their pairwise relationships, including where implications hold and where they fail, and present a unified template that makes the structural differences explicit. The analysis suggests why supervised learning on code scales predictably while reinforcement learning does not, and why the common assumption that scaling alone will solve remaining ML challenges warrants scrutiny.

</details>


### [162] [A Multi-Agent Framework for Code-Guided, Modular, and Verifiable Automated Machine Learning](https://arxiv.org/abs/2602.13937)
*Dat Le,Duc-Cuong Le,Anh-Son Nguyen,Tuan-Dung Bui,Thu-Trang Nguyen,Son Nguyen,Hieu Dinh Vo*

Main category: cs.LG

TL;DR: iML是一个基于多智能体的AutoML框架，通过代码引导的模块化架构解决传统AutoML黑盒问题和LLM代理的幻觉问题，显著提升自动化机器学习任务的可靠性和性能。


<details>
  <summary>Details</summary>
Motivation: 传统AutoML框架缺乏灵活性和透明度，而基于LLM的代理方法存在幻觉逻辑和逻辑纠缠问题，导致运行时故障难以恢复。需要一种更可靠、可验证的AutoML架构。

Method: 提出iML多智能体框架，包含三个核心创新：1) 代码引导规划：基于自主经验分析生成战略蓝图消除幻觉；2) 代码模块化实现：将预处理和建模解耦为专用组件，通过严格接口契约管理；3) 代码可验证集成：通过动态契约验证和迭代自校正确保物理可行性。

Result: 在MLE-BENCH上实现85%的有效提交率和45%的竞争奖牌率，标准化性能得分(APS)达0.77。在iML-BENCH上比其他方法提升38%-163%的APS。即使在简化任务描述下，仍保持70%的成功率。

Conclusion: iML通过代码引导的模块化架构，有效解决了AutoML中的幻觉和可靠性问题，在随机生成和可靠工程之间架起桥梁，是迈向真正AutoML的重要一步。

Abstract: Automated Machine Learning (AutoML) has revolutionized the development of data-driven solutions; however, traditional frameworks often function as "black boxes", lacking the flexibility and transparency required for complex, real-world engineering tasks. Recent Large Language Model (LLM)-based agents have shifted toward code-driven approaches. However, they frequently suffer from hallucinated logic and logic entanglement, where monolithic code generation leads to unrecoverable runtime failures. In this paper, we present iML, a novel multi-agent framework designed to shift AutoML from black-box prompting to a code-guided, modular, and verifiable architectural paradigm. iML introduces three main ideas: (1) Code-Guided Planning, which synthesizes a strategic blueprint grounded in autonomous empirical profiling to eliminate hallucination; (2) Code-Modular Implementation, which decouples preprocessing and modeling into specialized components governed by strict interface contracts; and (3) Code-Verifiable Integration, which enforces physical feasibility through dynamic contract verification and iterative self-correction. We evaluate iML across MLE-BENCH and the newly introduced iML-BENCH, comprising a diverse range of real-world Kaggle competitions. The experimental results show iML's superiority over state-of-the-art agents, achieving a valid submission rate of 85% and a competitive medal rate of 45% on MLE-BENCH, with an average standardized performance score (APS) of 0.77. On iML-BENCH, iML significantly outperforms the other approaches by 38%-163% in APS. Furthermore, iML maintains a robust 70% success rate even under stripped task descriptions, effectively filling information gaps through empirical profiling. These results highlight iML's potential to bridge the gap between stochastic generation and reliable engineering, marking a meaningful step toward truly AutoML.

</details>


### [163] [An Adaptive Model Selection Framework for Demand Forecasting under Horizon-Induced Degradation to Support Business Strategy and Operations](https://arxiv.org/abs/2602.13939)
*Adolfo González,Víctor Parada*

Main category: cs.LG

TL;DR: AHSIV是一个自适应混合选择框架，用于处理间歇性和高变异性需求环境中的模型选择问题，通过考虑预测时域、需求结构和多目标优化来提高模型选择的稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 在结构性需求间歇性、高变异性和多步预测时域的商业环境中，现有模型选择机制存在不足。经验证据表明没有单一模型在所有情况下都表现最佳，且模型排名会随误差指标、需求状态和预测时域而变化，导致多SKU决策中的模糊性。

Method: 提出AHSIV框架，包含：1）通过MDFH程序调整的缩放和绝对误差指标；2）结构性需求分类；3）多目标Pareto支配；4）分层偏差细化；5）统一的决策架构，专门设计用于处理时域引起的排名不稳定性。

Result: 在Walmart、M3、M4和M5数据集上，使用多种训练-测试划分方案和12步预测时域进行评估。结果显示AHSIV在聚合性能上与最强单指标基线统计等价，同时提高了特定时域最佳模型选择的频率。

Conclusion: 异质需求环境中的模型选择不能被视为静态排名问题，时域一致、结构自适应的机制为多SKU预测提供了原则性和操作上一致的解决方案。

Abstract: Business environments characterized by structural demand intermittency, high variability, and multi-step planning horizons require robust and reproducible model selection mechanisms. Empirical evidence shows that no forecasting model is universally dominant and that relative rankings vary across error metrics, demand regimes, and forecast horizons, generating ambiguity in multi-SKU decision contexts. This study proposes AHSIV (Adaptive Hybrid Selector for Intermittency and Variability), a horizon-aware and regime-conditioned model selection framework designed to address horizon-induced ranking instability. The proposed approach integrates scaled and absolute error metrics adjusted through a Metric Degradation by Forecast Horizon (MDFH) procedure, structural demand classification, multi-objective Pareto dominance, and hierarchical bias refinement within a unified decision architecture. The empirical evaluation is conducted on the Walmart, M3, M4, and M5 datasets under multiple train-test partition schemes and twelve-step forecasting horizons. Results indicate that AHSIV achieves statistical equivalence with the strongest monometric baseline in terms of aggregated performance while increasing the frequency of horizon-specific best-model selection. The findings demonstrate that model selection in heterogeneous demand environments cannot be treated as a static ranking problem, and that horizon-consistent, structurally adaptive mechanisms provide a principled, operationally coherent solution for multi-SKU forecasting.

</details>


### [164] [You Can Learn Tokenization End-to-End with Reinforcement Learning](https://arxiv.org/abs/2602.13940)
*Sam Dauncey,Roger Wattenhofer*

Main category: cs.LG

TL;DR: 提出一种使用分数函数估计学习分词边界的方法，替代传统的硬编码分词，在1亿参数规模上优于之前的直通估计方法


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型架构越来越端到端，但分词作为硬编码的压缩步骤仍保留在训练流程中。现有方法要么使用启发式方法，要么使用直通估计将离散分词边界问题连续化处理，但存在理论保证不足的问题。

Method: 使用分数函数估计直接学习离散分词边界，通过强化学习中的时间折扣等技术降低方差，使方法在实践中可行。这种方法直接优化离散边界划分以最小化损失。

Result: 在1亿参数规模上，该方法在定性和定量评估中都优于先前提出的直通估计方法。

Conclusion: 分数函数估计能够有效学习分词边界，为将分词步骤整合到大语言模型架构中提供了更优的解决方案，具有更好的理论保证和实际性能。

Abstract: Tokenization is a hardcoded compression step which remains in the training pipeline of Large Language Models (LLMs), despite a general trend towards architectures becoming increasingly end-to-end. Prior work has shown promising results at scale in bringing this compression step inside the LLMs' architecture with heuristics to draw token boundaries, and also attempts to learn these token boundaries with straight-through estimates, which treat the problem of drawing discrete token boundaries as a continuous one. We show that these token boundaries can instead be learned using score function estimates, which have tighter theoretical guarantees due to directly optimizing the problem of drawing discrete token boundaries to minimize loss. We observe that techniques from reinforcement learning, such as time discounting, are necessary to reduce the variance of this score function sufficiently to make it practicable. We demonstrate that the resultant method outperforms prior proposed straight-through estimates, both qualitatively and quantitatively at the $100$ million parameter scale.

</details>


### [165] [Experiential Reinforcement Learning](https://arxiv.org/abs/2602.13949)
*Taiwei Shi,Sihao Chen,Bowen Jiang,Linxin Song,Longqi Yang,Jieyu Zhao*

Main category: cs.LG

TL;DR: ERL（经验强化学习）通过引入经验-反思-巩固循环，将环境反馈转化为结构化行为修正，提升稀疏延迟反馈下的学习效率。


<details>
  <summary>Details</summary>
Motivation: 语言模型在强化学习中面临稀疏延迟反馈的挑战，需要从观察到的失败中推断行为改进方式，现有方法难以有效转化反馈为行为改变。

Method: ERL嵌入经验-反思-巩固循环：模型生成初始尝试→接收环境反馈→产生反思指导→生成改进尝试→成功经验强化并内化到基础策略中。

Result: 在稀疏奖励控制环境和智能推理基准测试中，ERL相比强基线提升学习效率和最终性能，复杂多步环境提升达81%，工具使用推理任务提升达11%。

Conclusion: 将显式自我反思整合到策略训练中，为将反馈转化为持久行为改进提供了实用机制，无需额外推理成本即可保持部署收益。

Abstract: Reinforcement learning has become the central approach for language models (LMs) to learn from environmental reward or feedback. In practice, the environmental feedback is usually sparse and delayed. Learning from such signals is challenging, as LMs must implicitly infer how observed failures should translate into behavioral changes for future iterations. We introduce Experiential Reinforcement Learning (ERL), a training paradigm that embeds an explicit experience-reflection-consolidation loop into the reinforcement learning process. Given a task, the model generates an initial attempt, receives environmental feedback, and produces a reflection that guides a refined second attempt, whose success is reinforced and internalized into the base policy. This process converts feedback into structured behavioral revision, improving exploration and stabilizing optimization while preserving gains at deployment without additional inference cost. Across sparse-reward control environments and agentic reasoning benchmarks, ERL consistently improves learning efficiency and final performance over strong reinforcement learning baselines, achieving gains of up to +81% in complex multi-step environments and up to +11% in tool-using reasoning tasks. These results suggest that integrating explicit self-reflection into policy training provides a practical mechanism for transforming feedback into durable behavioral improvement.

</details>


### [166] [QuRL: Efficient Reinforcement Learning with Quantized Rollout](https://arxiv.org/abs/2602.13953)
*Yuhang Li,Reena Elangovan,Xin Dong,Priyadarshini Panda,Brucek Khailany*

Main category: cs.LG

TL;DR: 提出Quantized Reinforcement Learning (QuRL)，使用量化actor加速强化学习中的rollout过程，解决了训练崩溃和权重更新问题，实现20-80%的加速


<details>
  <summary>Details</summary>
Motivation: 在基于可验证奖励的强化学习(RLVR)训练推理大语言模型时，由于LLMs的自回归解码特性，rollout过程成为训练效率瓶颈，占总训练时间的70%

Method: 提出QuRL方法：1) 使用量化actor加速rollout；2) 提出自适应裁剪范围(ACR)动态调整裁剪比例，防止长期训练崩溃；3) 使用不变缩放技术解决权重更新问题，减少量化噪声

Result: 在DeepScaleR和DAPO上进行INT8和FP8量化实验，实现了20%到80%的rollout加速

Conclusion: QuRL通过量化actor有效加速RL训练中的rollout过程，解决了量化带来的训练稳定性和权重更新问题，为高效RL训练提供了实用解决方案

Abstract: Reinforcement learning with verifiable rewards (RLVR) has become a trending paradigm for training reasoning large language models (LLMs). However, due to the autoregressive decoding nature of LLMs, the rollout process becomes the efficiency bottleneck of RL training, consisting of up to 70\% of the total training time. In this work, we propose Quantized Reinforcement Learning (QuRL) that uses a quantized actor for accelerating the rollout. We address two challenges in QuRL. First, we propose Adaptive Clipping Range (ACR) that dynamically adjusts the clipping ratio based on the policy ratio between the full-precision actor and the quantized actor, which is essential for mitigating long-term training collapse. Second, we identify the weight update problem, where weight changes between RL steps are extremely small, making it difficult for the quantization operation to capture them effectively. We mitigate this problem through the invariant scaling technique that reduces quantization noise and increases weight update. We evaluate our method with INT8 and FP8 quantization experiments on DeepScaleR and DAPO, and achieve 20% to 80% faster rollout during training.

</details>


### [167] [Chemical Language Models for Natural Products: A State-Space Model Approach](https://arxiv.org/abs/2602.13958)
*Ho-Hsuan Wang,Afnan Sultan,Andrea Volkamer,Dietrich Klakow*

Main category: cs.LG

TL;DR: 开发针对天然产物的化学语言模型(NPCLMs)，比较Mamba、Mamba-2与GPT在天然产物任务上的表现，发现Mamba在分子生成方面表现最佳，而GPT生成的结构更具新颖性。


<details>
  <summary>Details</summary>
Motivation: 天然产物在药物发现中很重要，但现有语言模型主要关注普通分子，缺乏针对天然产物的专门研究。需要填补这一空白，开发专门针对天然产物的化学语言模型。

Method: 使用约100万个天然产物数据集，预训练状态空间模型(Mamba和Mamba-2)并与Transformer基线(GPT)比较。采用八种分词策略，包括字符级、Atom-in-SMILES、BPE和NP-specific BPE。评估分子生成(有效性、唯一性、新颖性)和性质预测(膜渗透性、味道、抗癌活性)。

Result: Mamba比Mamba-2和GPT多生成1-2%的有效和唯一分子，且长距离依赖错误更少；GPT生成的结构略具新颖性。在性质预测方面，Mamba变体在随机分割下比GPT高0.02-0.04 MCC，在骨架分割下表现相当。领域特定预训练在100万天然产物数据集上能达到比100倍更大数据集训练的模型相当的效果。

Conclusion: 状态空间模型(特别是Mamba)在天然产物任务上表现优异，领域特定预训练能显著提升性能，即使数据集相对较小也能达到与大规模数据集训练模型相当的效果。

Abstract: Language models are widely used in chemistry for molecular property prediction and small-molecule generation, yet Natural Products (NPs) remain underexplored despite their importance in drug discovery. To address this gap, we develop NP-specific chemical language models (NPCLMs) by pre-training state-space models (Mamba and Mamba-2) and comparing them with transformer baselines (GPT). Using a dataset of about 1M NPs, we present the first systematic comparison of selective state-space models and transformers for NP-focused tasks, together with eight tokenization strategies including character-level, Atom-in-SMILES (AIS), byte-pair encoding (BPE), and NP-specific BPE. We evaluate molecule generation (validity, uniqueness, novelty) and property prediction (membrane permeability, taste, anti-cancer activity) using MCC and AUC-ROC. Mamba generates 1-2 percent more valid and unique molecules than Mamba-2 and GPT, with fewer long-range dependency errors, while GPT yields slightly more novel structures. For property prediction, Mamba variants outperform GPT by 0.02-0.04 MCC under random splits, while scaffold splits show comparable performance. Results demonstrate that domain-specific pre-training on about 1M NPs can match models trained on datasets over 100 times larger.

</details>


### [168] [Steady-State Behavior of Constant-Stepsize Stochastic Approximation: Gaussian Approximation and Tail Bounds](https://arxiv.org/abs/2602.13960)
*Zedong Wang,Yuyang Wang,Ijay Narang,Felix Wang,Yuzhou Wang,Siva Theja Maguluri*

Main category: cs.LG

TL;DR: 本文为非渐近步长随机逼近（SA）的稳态分布提供了显式的Wasserstein距离误差界，覆盖了i.i.d.和马尔可夫噪声模型，并应用于SGD、线性SA和非线性SA等场景。


<details>
  <summary>Details</summary>
Motivation: 固定步长的随机逼近在计算效率上广泛应用，但稳态分布通常难以解析。虽然已有渐近结果（步长趋于0时稳态分布收敛到高斯分布），但对于固定步长，缺乏可用的误差界来近似稳态分布。

Method: 首先证明通用定理，在漂移正则性和噪声矩条件下，给出中心化缩放稳态分布与适当高斯分布之间的Wasserstein距离界。然后具体应用于三个代表性SA设置：1）光滑强凸目标的随机梯度下降；2）线性SA；3）压缩非线性SA。

Result: 获得了阶为α^{1/2}log(1/α)的显式Wasserstein距离误差界（α为步长）。进一步推导了非均匀Berry-Esseen型尾界，误差项在偏差水平和步长α上同时衰减。对于非强凸SGD，识别出非高斯（Gibbs）极限律并提供了相应的预极限误差界。

Conclusion: 本文为非渐近步长随机逼近的稳态分布提供了首个显式误差界，建立了固定步长下稳态分布与高斯极限之间的量化近似关系，为实际应用中的误差控制提供了理论依据。

Abstract: Constant-stepsize stochastic approximation (SA) is widely used in learning for computational efficiency. For a fixed stepsize, the iterates typically admit a stationary distribution that is rarely tractable. Prior work shows that as the stepsize $α\downarrow 0$, the centered-and-scaled steady state converges weakly to a Gaussian random vector. However, for fixed $α$, this weak convergence offers no usable error bound for approximating the steady-state by its Gaussian limit. This paper provides explicit, non-asymptotic error bounds for fixed $α$. We first prove general-purpose theorems that bound the Wasserstein distance between the centered-scaled steady state and an appropriate Gaussian distribution, under regularity conditions for drift and moment conditions for noise. To ensure broad applicability, we cover both i.i.d. and Markovian noise models. We then instantiate these theorems for three representative SA settings: (1) stochastic gradient descent (SGD) for smooth strongly convex objectives, (2) linear SA, and (3) contractive nonlinear SA. We obtain dimension- and stepsize-dependent, explicit bounds in Wasserstein distance of order $α^{1/2}\log(1/α)$ for small $α$. Building on the Wasserstein approximation error, we further derive non-uniform Berry--Esseen-type tail bounds that compare the steady-state tail probability to Gaussian tails. We achieve an explicit error term that decays in both the deviation level and stepsize $α$. We adapt the same analysis for SGD beyond strongly convexity and study general convex objectives. We identify a non-Gaussian (Gibbs) limiting law under the correct scaling, which is validated numerically, and provide a corresponding pre-limit Wasserstein error bound.

</details>


### [169] [KoopGen: Koopman Generator Networks for Representing and Predicting Dynamical Systems with Continuous Spectra](https://arxiv.org/abs/2602.14011)
*Liangyu Su,Jun Shu,Rui Liu,Deyu Meng,Zongben Xu*

Main category: cs.LG

TL;DR: KoopGen：基于生成器的神经Koopman框架，通过状态相关的Koopman生成器表示建模高维时空混沌系统，分离保守传输和不可逆耗散，提高预测精度和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动模型在宽带或连续谱主导的系统中缺乏稳定性、可解释性和可扩展性，而传统Koopman方法依赖有限维假设或显式谱参数化，在高维设置中效果不佳。

Method: 引入KoopGen框架，通过结构化、状态相关的Koopman生成器表示建模动力学，利用笛卡尔分解将算子分解为斜自伴和自伴分量，分离保守传输和不可逆耗散，并在学习中强制执行精确的算子理论约束。

Result: 在从非线性振荡器到高维混沌和时空动力学的各种系统中，KoopGen提高了预测精度和稳定性，同时阐明了连续谱动力学的哪些组件允许可解释和可学习的表示。

Conclusion: KoopGen为高维时空混沌系统提供了一种有原则的线性视角，通过生成器框架克服了现有方法的局限性，实现了更好的预测性能和可解释性。

Abstract: Representing and predicting high-dimensional and spatiotemporally chaotic dynamical systems remains a fundamental challenge in dynamical systems and machine learning. Although data-driven models can achieve accurate short-term forecasts, they often lack stability, interpretability, and scalability in regimes dominated by broadband or continuous spectra. Koopman-based approaches provide a principled linear perspective on nonlinear dynamics, but existing methods rely on restrictive finite-dimensional assumptions or explicit spectral parameterizations that degrade in high-dimensional settings. Against these issues, we introduce KoopGen, a generator-based neural Koopman framework that models dynamics through a structured, state-dependent representation of Koopman generators. By exploiting the intrinsic Cartesian decomposition into skew-adjoint and self-adjoint components, KoopGen separates conservative transport from irreversible dissipation while enforcing exact operator-theoretic constraints during learning. Across systems ranging from nonlinear oscillators to high-dimensional chaotic and spatiotemporal dynamics, KoopGen improves prediction accuracy and stability, while clarifying which components of continuous-spectrum dynamics admit interpretable and learnable representations.

</details>


### [170] [S2SServiceBench: A Multimodal Benchmark for Last-Mile S2S Climate Services](https://arxiv.org/abs/2602.14017)
*Chenyue Li,Wen Deng,Zhuotao Sun,Mengxi Jin,Hanzhe Cui,Han Li,Shentong Li,Man Kit Yu,Ming Long Lai,Yuhao Yang,Mengqian Lu,Binhang Yuan*

Main category: cs.LG

TL;DR: S2SServiceBench：一个用于评估多模态大语言模型在次季节到季节气候服务中决策支持能力的基准，涵盖6个应用领域、10种服务产品、约500个任务


<details>
  <summary>Details</summary>
Motivation: 解决气候服务中的"最后一公里"问题：将科学预报转化为可信、可操作的气候服务，需要可靠的多模态理解和不确定性下的决策推理。当前多模态大语言模型在支持各种工作流方面进展迅速，但能否可靠地从业务服务产品中生成决策交付成果仍不明确。

Method: 构建S2SServiceBench多模态基准，从业务气候服务系统中精心挑选，涵盖10种服务产品、约150+专家选择案例，涉及农业、灾害、能源、金融、健康和航运6个领域。每个案例在三个服务级别实例化，产生约500个任务和1000+评估项。

Result: 使用S2SServiceBench对最先进的多模态大语言模型和智能体进行基准测试，分析不同产品和服务级别的性能，揭示了在S2S服务图理解和推理方面的持续挑战：可操作信号理解、将不确定性操作化为可执行交接、以及动态灾害的稳定、证据基础的分析规划。

Conclusion: S2SServiceBench为评估多模态大语言模型在气候服务决策支持能力提供了重要基准，揭示了当前模型在不确定性下决策推理的局限性，为构建未来气候服务智能体提供了可操作的指导。

Abstract: Subseasonal-to-seasonal (S2S) forecasts play an essential role in providing a decision-critical weeks-to-months planning window for climate resilience and sustainability, yet a growing bottleneck is the last-mile gap: translating scientific forecasts into trusted, actionable climate services, requiring reliable multimodal understanding and decision-facing reasoning under uncertainty. Meanwhile, multimodal large language models (MLLMs) and corresponding agentic paradigms have made rapid progress in supporting various workflows, but it remains unclear whether they can reliably generate decision-making deliverables from operational service products (e.g., actionable signal comprehension, decision-making handoff, and decision analysis & planning) under uncertainty. We introduce S2SServiceBench, a multimodal benchmark for last-mile S2S climate services curated from an operational climate-service system to evaluate this capability. S2SServiceBenchcovers 10 service products with about 150+ expert-selected cases in total, spanning six application domains - Agriculture, Disasters, Energy, Finance, Health, and Shipping. Each case is instantiated at three service levels, yielding around 500 tasks and 1,000+ evaluation items across climate resilience and sustainability applications. Using S2SServiceBench, we benchmark state-of-the-art MLLMs and agents, and analyze performance across products and service levels, revealing persistent challenges in S2S service plot understanding and reasoning - namely, actionable signal comprehension, operationalizing uncertainty into executable handoffs, and stable, evidence-grounded analysis and planning for dynamic hazards-while offering actionable guidance for building future climate-service agents.

</details>


### [171] [EIDOS: Latent-Space Predictive Learning for Time Series Foundation Models](https://arxiv.org/abs/2602.14024)
*Xinxing Zhou,Qingren Yao,Yiji Zhao,Chenghao Liu,Flora Salim,Xiaojie Yuan,Yanlong Wen,Ming Jin*

Main category: cs.LG

TL;DR: EIDOS是一个时间序列基础模型，通过从未来值预测转向潜在空间预测学习，训练因果Transformer预测潜在表示演化，从而获得结构化、时间一致的潜在状态。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列基础模型通过直接预测未来观测值进行预训练，这通常产生弱结构化的潜在表示，主要捕捉表面噪声而非连贯可预测的时间动态。需要更稳健可靠的表示学习方法。

Method: 1. 从未来值预测转向潜在空间预测学习；2. 训练因果Transformer预测潜在表示演化；3. 设计轻量级聚合分支构建目标表示；4. 使用联合目标函数：潜在空间对齐、观测接地（将表示锚定到输入信号）和直接预测监督。

Result: 在GIFT-Eval基准测试中，EIDOS缓解了表示空间中的结构碎片化，并实现了最先进的性能。模型学习到了更可预测的潜在动态。

Conclusion: 约束模型学习可预测的潜在动态是构建更稳健可靠时间序列基础模型的原则性步骤。潜在空间预测学习优于直接的未来值预测方法。

Abstract: Most time series foundation models are pretrained by directly predicting future observations, which often yields weakly structured latent representations that capture surface noise rather than coherent and predictable temporal dynamics. In this work, we introduce EIDOS, a foundation model family that shifts pretraining from future value prediction to latent-space predictive learning. We train a causal Transformer to predict the evolution of latent representations, encouraging the emergence of structured and temporally coherent latent states. To ensure stable targets for latent-space learning, we design a lightweight aggregation branch to construct target representations. EIDOS is optimized via a joint objective that integrates latent-space alignment, observational grounding to anchor representations to the input signal, and direct forecasting supervision. On the GIFT-Eval benchmark, EIDOS mitigates structural fragmentation in the representation space and achieves state-of-the-art performance. These results demonstrate that constraining models to learn predictable latent dynamics is a principled step toward more robust and reliable time series foundation models.

</details>


### [172] [UniST-Pred: A Robust Unified Framework for Spatio-Temporal Traffic Forecasting in Transportation Networks Under Disruptions](https://arxiv.org/abs/2602.14049)
*Yue Wang,Areg Karapetyan,Djellel Difallah,Samer Madanat*

Main category: cs.LG

TL;DR: UniST-Pred是一个解耦时空建模的轻量级交通预测框架，通过自适应表示级融合实现鲁棒预测，在真实和模拟数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有交通预测模型通常紧密耦合时空建模，导致复杂度高、模块化差，且很少考虑现实部署中的结构和观测不确定性。需要一种既能处理不确定性又保持轻量化的方法。

Method: 提出UniST-Pred框架：1）将时间建模与空间表示学习解耦；2）通过自适应表示级融合整合两者；3）使用基于智能体的微观交通模拟器(MATSim)构建数据集评估网络断开场景下的鲁棒性。

Result: 在标准交通预测数据集上表现与现有模型相当，在模拟的网络断开场景中保持强预测性能，同时产生可解释的时空表示，尽管设计轻量化。

Conclusion: UniST-Pred通过解耦时空建模和自适应融合，实现了轻量化、鲁棒且可解释的交通预测，在真实和模拟场景中均表现优异，为智能交通系统提供了实用解决方案。

Abstract: Spatio-temporal traffic forecasting is a core component of intelligent transportation systems, supporting various downstream tasks such as signal control and network-level traffic management. In real-world deployments, forecasting models must operate under structural and observational uncertainties, conditions that are rarely considered in model design. Recent approaches achieve strong short-term predictive performance by tightly coupling spatial and temporal modeling, often at the cost of increased complexity and limited modularity. In contrast, efficient time-series models capture long-range temporal dependencies without relying on explicit network structure. We propose UniST-Pred, a unified spatio-temporal forecasting framework that first decouples temporal modeling from spatial representation learning, then integrates both through adaptive representation-level fusion. To assess robustness of the proposed approach, we construct a dataset based on an agent-based, microscopic traffic simulator (MATSim) and evaluate UniST-Pred under severe network disconnection scenarios. Additionally, we benchmark UniST-Pred on standard traffic prediction datasets, demonstrating its competitive performance against existing well-established models despite a lightweight design. The results illustrate that UniST-Pred maintains strong predictive performance across both real-world and simulated datasets, while also yielding interpretable spatio-temporal representations under infrastructure disruptions. The source code and the generated dataset are available at https://anonymous.4open.science/r/UniST-Pred-EF27

</details>


### [173] [Position Encoding with Random Float Sampling Enhances Length Generalization of Transformers](https://arxiv.org/abs/2602.14050)
*Atsushi Shimizu,Shohei Taniguchi,Yutaka Matsuo*

Main category: cs.LG

TL;DR: 提出一种名为随机浮点采样(RFS)的简单而强大的位置编码策略，通过在训练中使用随机采样的连续值而非预定义离散集，有效解决未见长度下的分布外问题，提升长度泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型在长度泛化方面的挑战——即模型在处理比预训练时更长的输入时性能下降的问题。传统位置编码使用预定义离散位置索引，当遇到未见长度时会出现分布外问题。

Method: 提出随机浮点采样(RFS)位置编码策略：不使用预定义离散位置索引，而是使用随机采样的连续值作为位置索引，让模型在训练时接触到更多样化的位置值，从而避免在未见长度时出现分布外问题。该方法可轻松集成到现有主流位置编码方法中，如绝对正弦编码、RoPE和ALiBi。

Result: 实验表明RFS在长度泛化任务中表现优异，同时在零样本常识推理基准测试中也显示出更好的性能。

Conclusion: RFS是一种简单有效的解决方案，通过使用随机连续位置索引而非固定离散集，显著提升了语言模型的长度泛化能力，且易于集成到现有位置编码框架中。

Abstract: Length generalization is the ability of language models to maintain performance on inputs longer than those seen during pretraining. In this work, we introduce a simple yet powerful position encoding (PE) strategy, Random Float Sampling (RFS), that generalizes well to lengths unseen during pretraining or fine-tuning. In particular, instead of selecting position indices from a predefined discrete set, RFS uses randomly sampled continuous values, thereby avoiding out-of-distribution (OOD) issues on unseen lengths by exposing the model to diverse indices during training. Since assigning indices to tokens is a common and fundamental procedure in widely used PEs, the advantage of RFS can easily be incorporated into, for instance, the absolute sinusoidal encoding, RoPE, and ALiBi. Experiments corroborate its effectiveness by showing that RFS results in superior performance in length generalization tasks as well as zero-shot commonsense reasoning benchmarks.

</details>


### [174] [Decentralized Federated Learning With Energy Harvesting Devices](https://arxiv.org/abs/2602.14051)
*Kai Zhang,Xuanyu Cao,Khaled B. Letaief*

Main category: cs.LG

TL;DR: 该论文提出了一种用于能量收集型去中心化联邦学习的完全去中心化策略迭代算法，通过联合设备调度和功率控制来加速收敛，同时降低通信开销和计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 去中心化联邦学习(DFL)中设备间频繁的本地训练和模型交换会快速耗尽有限的设备电池，降低设备运行寿命和学习性能。能量收集技术虽然能让设备从环境中获取能量，但如何在这种能量受限的环境中优化DFL性能仍是一个挑战。

Method: 首先推导了带能量收集的无线DFL收敛界，表明收敛受部分设备参与和传输丢包影响。然后将其建模为多智能体马尔可夫决策过程(MDP)，并提出完全去中心化的策略迭代算法，仅利用两跳邻居设备的局部状态信息，大幅降低通信开销和计算复杂度。

Result: 理论分析表明所提出的去中心化算法能达到渐近最优性。在真实数据集上的综合数值实验验证了理论结果，并证实了算法的有效性。

Conclusion: 该研究成功解决了能量收集型DFL系统的优化问题，提出的完全去中心化算法在保证渐近最优性的同时，显著降低了通信和计算开销，为大规模去中心化网络的可持续运行提供了有效解决方案。

Abstract: Decentralized federated learning (DFL) enables edge devices to collaboratively train models through local training and fully decentralized device-to-device (D2D) model exchanges. However, these energy-intensive operations often rapidly deplete limited device batteries, reducing their operational lifetime and degrading the learning performance. To address this limitation, we apply energy harvesting technique to DFL systems, allowing edge devices to extract ambient energy and operate sustainably. We first derive the convergence bound for wireless DFL with energy harvesting, showing that the convergence is influenced by partial device participation and transmission packet drops, both of which further depend on the available energy supply. To accelerate convergence, we formulate a joint device scheduling and power control problem and model it as a multi-agent Markov decision process (MDP). Traditional MDP algorithms (e.g., value or policy iteration) require a centralized coordinator with access to all device states and exhibit exponential complexity in the number of devices, making them impractical for large-scale decentralized networks. To overcome these challenges, we propose a fully decentralized policy iteration algorithm that leverages only local state information from two-hop neighboring devices, thereby substantially reducing both communication overhead and computational complexity. We further provide a theoretical analysis showing that the proposed decentralized algorithm achieves asymptotic optimality. Finally, comprehensive numerical experiments on real-world datasets are conducted to validate the theoretical results and corroborate the effectiveness of the proposed algorithm.

</details>


### [175] [Policy Gradient with Adaptive Entropy Annealing for Continual Fine-Tuning](https://arxiv.org/abs/2602.14078)
*Yaqian Zhang,Bernhard Pfahringer,Eibe Frank,Albert Bifet*

Main category: cs.LG

TL;DR: 论文提出了一种基于强化学习视角的0-1损失优化方法，通过期望策略梯度直接最小化分类错误，并设计了自适应熵退火策略，在参数高效微调中优于传统的交叉熵损失。


<details>
  <summary>Details</summary>
Motivation: 尽管大型预训练视觉模型在参数高效微调中取得了一定成功，但它们仍然容易在类增量设置中出现灾难性遗忘。现有方法大多依赖交叉熵损失，但交叉熵只是0-1损失的替代目标，论文希望直接优化真正的分类错误目标。

Method: 1. 从强化学习视角重新审视分类问题，将其建模为一步马尔可夫决策过程；2. 推导出期望策略梯度方法，直接最小化误分类错误；3. 分析发现交叉熵可解释为带有额外样本加权机制的EPG；4. 提出自适应熵退火策略，从探索性学习逐步过渡到利用性学习。

Result: aEPG方法在多个基准测试和不同PEFT模块上都优于基于交叉熵的方法。实验表明，输出预测分布的较低熵能增强预训练视觉模型的适应能力。

Conclusion: 通过强化学习视角直接优化0-1损失是有效的，自适应熵退火策略能够平衡探索与利用，在参数高效微调中显著提升类增量学习的性能，为预训练视觉模型的适应提供了新思路。

Abstract: Despite their success, large pretrained vision models remain vulnerable to catastrophic forgetting when adapted to new tasks in class-incremental settings. Parameter-efficient fine-tuning (PEFT) alleviates this by restricting trainable parameters, yet most approaches still rely on cross-entropy (CE) loss, a surrogate for the 0-1 loss, to learn from new data. We revisit this choice and revive the true objective (0-1 loss) through a reinforcement learning perspective. By formulating classification as a one-step Markov Decision Process, we derive an Expected Policy Gradient (EPG) method that directly minimizes misclassification error with a low-variance gradient estimation. Our analysis shows that CE can be interpreted as EPG with an additional sample-weighting mechanism: CE encourages exploration by emphasizing low-confidence samples, while EPG prioritizes high-confidence ones. Building on this insight, we propose adaptive entropy annealing (aEPG), a training strategy that transitions from exploratory (CE-like) to exploitative (EPG-like) learning. aEPG-based methods outperform CE-based methods across diverse benchmarks and with various PEFT modules. More broadly, we evaluate various entropy regularization methods and demonstrate that lower entropy of the output prediction distribution enhances adaptation in pretrained vision models.

</details>


### [176] [Neural Optimal Transport in Hilbert Spaces: Characterizing Spurious Solutions and Gaussian Smoothing](https://arxiv.org/abs/2602.14086)
*Jae-Hwan Choi,Jiwoo Yoon,Dohyun Kwon,Jaewoong Choi*

Main category: cs.LG

TL;DR: 论文研究无限维希尔伯特空间中的神经最优传输，解决了半对偶神经OT在非正则设置下产生虚假解的问题，通过高斯平滑策略扩展半对偶框架，证明了在正则源测度下公式是适定的并能恢复唯一的Monge映射。


<details>
  <summary>Details</summary>
Motivation: 在无限维希尔伯特空间中，半对偶神经最优传输在非正则设置下经常产生虚假解，无法准确捕捉目标分布，这限制了神经OT方法在实际应用中的可靠性。

Method: 使用正则测度框架分析虚假解问题，扩展半对偶框架，采用基于布朗运动的高斯平滑策略，建立了平滑测度正则性的严格特征化，证明平滑成功严格依赖于协方差算子的核。

Result: 理论证明在正则源测度下，公式是适定的并能恢复唯一的Monge映射；在合成函数数据和时序数据集上的实验结果表明，该方法有效抑制虚假解，优于现有基线方法。

Conclusion: 通过高斯平滑策略扩展的半对偶神经OT框架解决了无限维希尔伯特空间中的虚假解问题，为神经最优传输在非正则设置下的应用提供了理论保证和实用方法。

Abstract: We study Neural Optimal Transport in infinite-dimensional Hilbert spaces. In non-regular settings, Semi-dual Neural OT often generates spurious solutions that fail to accurately capture target distributions. We analytically characterize this spurious solution problem using the framework of regular measures, which generalize Lebesgue absolute continuity in finite dimensions. To resolve ill-posedness, we extend the semi-dual framework via a Gaussian smoothing strategy based on Brownian motion. Our primary theoretical contribution proves that under a regular source measure, the formulation is well-posed and recovers a unique Monge map. Furthermore, we establish a sharp characterization for the regularity of smoothed measures, proving that the success of smoothing depends strictly on the kernel of the covariance operator. Empirical results on synthetic functional data and time-series datasets demonstrate that our approach effectively suppresses spurious solutions and outperforms existing baselines.

</details>


### [177] [Geometry-Aware Physics-Informed PointNets for Modeling Flows Across Porous Structures](https://arxiv.org/abs/2602.14108)
*Luigi Ciceri,Corrado Mio,Jianyi Lin,Gabriele Gianini*

Main category: cs.LG

TL;DR: 该论文提出了两种物理信息学习方法（PIPN和PI-GANO）来预测通过和围绕多孔体的流动，通过统一损失函数结合自由流区域的纳维-斯托克斯方程和多孔区域的达西-福希海默扩展，并在几何和材料参数上进行条件化。


<details>
  <summary>Details</summary>
Motivation: 预测通过和围绕多孔体的流动具有挑战性，因为需要在流体和多孔区域之间耦合物理，并且需要泛化到不同的几何形状和边界条件。现有方法难以处理这种跨区域耦合和几何多样性。

Method: 提出了两种物理信息学习方法：物理信息点云网络（PIPN）和物理信息几何感知神经算子（PI-GANO）。在统一损失函数中强制执行自由流区域的不可压缩纳维-斯托克斯方程和多孔区域的达西-福希海默扩展，并将网络条件化于几何和材料参数。使用OpenFOAM生成2D管道含多孔障碍物和3D防风林场景（含树冠和建筑物）的数据集。

Result: 通过制造解方法验证了管道，评估了对未见形状的泛化能力（对PI-GANO还包括可变边界条件和参数设置）。结果显示在已见和未见情况下速度和压力误差均较低，能准确再现尾流结构。性能下降主要发生在尖锐界面附近和大梯度区域。

Conclusion: 该研究首次系统评估了PIPN/PI-GANO用于同时处理通过和围绕多孔体的流动，展示了它们在不需针对每个几何形状重新训练的情况下加速设计研究的潜力。

Abstract: Predicting flows that occur both through and around porous bodies is challenging due to coupled physics across fluid and porous regions and the need to generalize across diverse geometries and boundary conditions. We address this problem using two Physics Informed learning approaches: Physics Informed PointNets (PIPN) and Physics Informed Geometry Aware Neural Operator (P-IGANO). We enforce the incompressible Navier Stokes equations in the free-flow region and a Darcy Forchheimer extension in the porous region within a unified loss and condition the networks on geometry and material parameters. Datasets are generated with OpenFOAM on 2D ducts containing porous obstacles and on 3D windbreak scenarios with tree canopies and buildings. We first verify the pipeline via the method of manufactured solutions, then assess generalization to unseen shapes, and for PI-GANO, to variable boundary conditions and parameter settings. The results show consistently low velocity and pressure errors in both seen and unseen cases, with accurate reproduction of the wake structures. Performance degrades primarily near sharp interfaces and in regions with large gradients. Overall, the study provides a first systematic evaluation of PIPN/PI-GANO for simultaneous through-and-around porous flows and shows their potential to accelerate design studies without retraining per geometry.

</details>


### [178] [Sanity Checks for Sparse Autoencoders: Do SAEs Beat Random Baselines?](https://arxiv.org/abs/2602.14111)
*Anton Korznikov,Andrey Galichin,Alexey Dontsov,Oleg Rogov,Ivan Oseledets,Elena Tutubalina*

Main category: cs.LG

TL;DR: SAEs在解释神经网络方面表现不佳，即使在合成数据中已知真实特征的情况下也只能恢复9%的特征，而随机基线在多个评估指标上与完全训练的SAEs表现相当。


<details>
  <summary>Details</summary>
Motivation: 尽管稀疏自编码器（SAEs）被认为是解释神经网络的有前景工具，但越来越多的下游任务负面结果引发了对SAEs是否真正恢复有意义特征的质疑。

Method: 采用两种互补评估方法：1）在已知真实特征的合成设置中测试SAEs；2）在真实激活上引入三种随机基线，约束SAE特征方向或其激活模式为随机值。

Result: 在合成数据中，SAEs仅恢复9%的真实特征，尽管达到71%的解释方差；在真实激活上，随机基线在可解释性（0.87 vs 0.90）、稀疏探测（0.69 vs 0.72）和因果编辑（0.73 vs 0.72）方面与完全训练的SAEs表现相当。

Conclusion: 当前状态的SAEs不能可靠地分解模型的内部机制，需要重新评估其在神经网络解释中的有效性。

Abstract: Sparse Autoencoders (SAEs) have emerged as a promising tool for interpreting neural networks by decomposing their activations into sparse sets of human-interpretable features. Recent work has introduced multiple SAE variants and successfully scaled them to frontier models. Despite much excitement, a growing number of negative results in downstream tasks casts doubt on whether SAEs recover meaningful features. To directly investigate this, we perform two complementary evaluations. On a synthetic setup with known ground-truth features, we demonstrate that SAEs recover only $9\%$ of true features despite achieving $71\%$ explained variance, showing that they fail at their core task even when reconstruction is strong. To evaluate SAEs on real activations, we introduce three baselines that constrain SAE feature directions or their activation patterns to random values. Through extensive experiments across multiple SAE architectures, we show that our baselines match fully-trained SAEs in interpretability (0.87 vs 0.90), sparse probing (0.69 vs 0.72), and causal editing (0.73 vs 0.72). Together, these results suggest that SAEs in their current state do not reliably decompose models' internal mechanisms.

</details>


### [179] [ROAST: Rollout-based On-distribution Activation Steering Technique](https://arxiv.org/abs/2602.14143)
*Xuanbo Su,Hao Luo,Yingfang Zhang,Lijun Zhang*

Main category: cs.LG

TL;DR: ROAST提出了一种基于模型自身分布内rollout的激活导向技术，通过连续软缩放和分组均值归一化来避免硬稀疏化，在多种模型和任务上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有激活导向方法依赖分布外监督和离散掩码，导致干预脆弱。需要一种更稳健的分布内激活导向技术。

Method: ROAST通过ROC从模型自身分布内rollout估计导向方向，使用连续软缩放避免硬稀疏化，采用分组均值归一化平衡样本贡献。

Result: 在0.6B到32B的多种模型上，ROAST在多样化任务上持续提升性能（如Qwen3-0.6B在GSM8K上+9.7%，GLM4-32B在TruthfulQA上+12.1%），CSS更好地保留了激活能量。

Conclusion: ROAST提供了一种更稳健的激活导向方法，通过分布内估计和适当的归一化技术，有效提升了大型语言模型的控制性能。

Abstract: Activation steering provides parameter-efficient control over large language models (LLMs) at inference time, but many methods rely on off-distribution supervision and discrete masking, leading to brittle interventions. We propose ROAST (Rollout-based On-distribution Activation Steering Technique), which estimates steering directions from the model's own on-distribution rollouts via ROC and avoids hard sparsification via Continuous Soft Scaling (CSS) and Grouped Mean Normalization. Our empirical analysis reveals that while activation magnitude correlates moderately with directional consistency, the variance in magnitude is significant and often disproportionate to semantic quality. This suggests that high-magnitude activations risk dominating the global steering direction if not properly normalized. To address this, ROAST employs grouped normalization to balance contributions across samples, ensuring a more robust estimation of the consensus steering direction. Across models (0.6B to 32B), ROAST consistently improves performance on diverse tasks (e.g., +9.7% on GSM8K for Qwen3-0.6B and +12.1% on TruthfulQA for GLM4-32B), and analyses show that CSS better preserves activation energy.

</details>


### [180] [Synergistic Intra- and Cross-Layer Regularization Losses for MoE Expert Specialization](https://arxiv.org/abs/2602.14159)
*Rizhen Hu,Yuan Cao,Boao Kong,Mou Sun,Kun Yuan*

Main category: cs.LG

TL;DR: 提出两种即插即用的正则化损失函数，无需修改MoE架构即可提升专家专业化和路由效率，实现更快的推理速度。


<details>
  <summary>Details</summary>
Motivation: 稀疏MoE模型存在专家重叠问题——专家间冗余表示和路由模糊性，导致模型容量严重未充分利用。现有架构解决方案需要大量结构修改且仅依赖层内信号。

Method: 提出两种正则化损失：1) 层内专业化损失，惩罚相同token上专家SwiGLU激活的余弦相似度；2) 跨层耦合损失，最大化相邻层间的联合Top-k路由概率。两种损失与标准负载均衡损失正交，兼容DeepSeekMoE和普通top-k MoE架构。

Result: 在预训练、微调和零样本基准测试中展示了一致的任务性能提升、更高的专家专业化程度和更低熵的路由，这些改进共同转化为通过更稳定的专家路径实现更快的推理速度。

Conclusion: 提出的即插即用正则化损失有效解决了MoE模型中的专家重叠问题，无需架构修改即可提升专家专业化和路由效率，实现更好的性能和更快的推理。

Abstract: Sparse Mixture-of-Experts (MoE) models scale Transformers efficiently but suffer from expert overlap -- redundant representations across experts and routing ambiguity, resulting in severely underutilized model capacity. While architectural solutions like DeepSeekMoE promote specialization, they require substantial structural modifications and rely solely on intra-layer signals. In this paper, we propose two plug-and-play regularization losses that enhance MoE specialization and routing efficiency without modifying router or model architectures. First, an intra-layer specialization loss penalizes cosine similarity between experts' SwiGLU activations on identical tokens, encouraging experts to specialize in complementary knowledge. Second, a cross-layer coupling loss maximizes joint Top-$k$ routing probabilities across adjacent layers, establishing coherent expert pathways through network depth while reinforcing intra-layer expert specialization. Both losses are orthogonal to the standard load-balancing loss and compatible with both the shared-expert architecture in DeepSeekMoE and vanilla top-$k$ MoE architectures. We implement both losses as a drop-in Megatron-LM module. Extensive experiments across pre-training, fine-tuning, and zero-shot benchmarks demonstrate consistent task gains, higher expert specialization, and lower-entropy routing; together, these improvements translate into faster inference via more stable expert pathways.

</details>


### [181] [When Benchmarks Lie: Evaluating Malicious Prompt Classifiers Under True Distribution Shift](https://arxiv.org/abs/2602.14161)
*Max Fomin*

Main category: cs.LG

TL;DR: 论文提出Leave-One-Dataset-Out (LODO)评估方法，揭示传统评估严重高估提示攻击检测性能，发现28%特征为数据集依赖的捷径特征，现有防护系统在间接攻击上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理处理越来越多来自电子邮件、文档、工具输出和外部API的不受信任数据，检测提示注入和越狱攻击变得至关重要。然而当前评估实践和生产系统存在根本性限制，需要更可靠的评估方法。

Method: 1) 使用18个多样化数据集构建基准，涵盖有害请求、越狱、间接提示注入和提取攻击；2) 提出Leave-One-Dataset-Out (LODO)评估方法测量真正的分布外泛化能力；3) 分析稀疏自编码器(SAE)特征系数；4) 系统比较生产防护系统(PromptGuard 2, LlamaGuard)和LLM-as-judge方法。

Result: 传统训练-测试分割严重高估性能：AUC平均高估8.4个百分点，每数据集准确率差距1-25%；28%的顶级特征是数据集依赖的捷径特征；现有防护系统在针对代理的间接攻击上检测率仅7-37%；PromptGuard 2和LlamaGuard因架构限制无法评估代理工具注入。

Conclusion: LODO评估揭示了当前提示攻击检测方法的泛化缺陷，数据集依赖的捷径特征导致虚假性能。LODO-stable SAE特征能提供更可靠的分类器决策解释。应建立LODO作为提示攻击检测研究的适当协议。

Abstract: Detecting prompt injection and jailbreak attacks is critical for deploying LLM-based agents safely. As agents increasingly process untrusted data from emails, documents, tool outputs, and external APIs, robust attack detection becomes essential. Yet current evaluation practices and production systems have fundamental limitations. We present a comprehensive analysis using a diverse benchmark of 18 datasets spanning harmful requests, jailbreaks, indirect prompt injections, and extraction attacks. We propose Leave-One-Dataset-Out (LODO) evaluation to measure true out-of-distribution generalization, revealing that the standard practice of train-test splits from the same dataset sources severely overestimates performance: aggregate metrics show an 8.4 percentage point AUC inflation, but per-dataset gaps range from 1% to 25% accuracy-exposing heterogeneous failure modes. To understand why classifiers fail to generalize, we analyze Sparse Auto-Encoder (SAE) feature coefficients across LODO folds, finding that 28% of top features are dataset-dependent shortcuts whose class signal depends on specific dataset compositions rather than semantic content. We systematically compare production guardrails (PromptGuard 2, LlamaGuard) and LLM-as-judge approaches on our benchmark, finding all three fail on indirect attacks targeting agents (7-37% detection) and that PromptGuard 2 and LlamaGuard cannot evaluate agentic tool injection due to architectural limitations. Finally, we show that LODO-stable SAE features provide more reliable explanations for classifier decisions by filtering dataset artifacts. We release our evaluation framework at https://github.com/maxf-zn/prompt-mining to establish LODO as the appropriate protocol for prompt attack detection research.

</details>


### [182] [Deep Dense Exploration for LLM Reinforcement Learning via Pivot-Driven Resampling](https://arxiv.org/abs/2602.14169)
*Yiran Guo,Zhongjian Qiao,Yingqi Xie,Jie Liu,Dan Ye,Ruiqing Zhang,Shuang Qiu,Lijie Xu*

Main category: cs.LG

TL;DR: 提出DDE（深度密集探索）策略，专注于在失败轨迹中的深度可恢复状态（pivots）进行探索，并实例化为DEEP-GRPO方法，在数学推理基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在大型语言模型探索中存在局限：GRPO仅从根节点采样，导致高概率轨迹饱和而深度错误状态探索不足；树方法盲目分散采样预算，无法发现罕见正确后缀且破坏局部基线稳定性。

Method: 提出DEEP-GRPO方法，包含三个创新：1) 轻量级数据驱动的效用函数自动平衡可恢复性和深度偏置来识别pivot状态；2) 在每个pivot进行局部密集重采样以增加发现正确后续轨迹的概率；3) 双流优化目标将全局策略学习与局部纠正更新解耦。

Result: 在数学推理基准测试中，该方法一致优于GRPO、树方法和其他强基线。

Conclusion: DDE策略通过专注于失败轨迹中的深度可恢复状态进行探索，有效解决了大型语言模型强化学习中的探索挑战，DEEP-GRPO方法在多个基准上表现出优越性能。

Abstract: Effective exploration is a key challenge in reinforcement learning for large language models: discovering high-quality trajectories within a limited sampling budget from the vast natural language sequence space. Existing methods face notable limitations: GRPO samples exclusively from the root, saturating high-probability trajectories while leaving deep, error-prone states under-explored. Tree-based methods blindly disperse budgets across trivial or unrecoverable states, causing sampling dilution that fails to uncover rare correct suffixes and destabilizes local baselines. To address this, we propose Deep Dense Exploration (DDE), a strategy that focuses exploration on $\textit{pivots}$-deep, recoverable states within unsuccessful trajectories. We instantiate DDE with DEEP-GRPO, which introduces three key innovations: (1) a lightweight data-driven utility function that automatically balances recoverability and depth bias to identify pivot states; (2) local dense resampling at each pivot to increase the probability of discovering correct subsequent trajectories; and (3) a dual-stream optimization objective that decouples global policy learning from local corrective updates. Experiments on mathematical reasoning benchmarks demonstrate that our method consistently outperforms GRPO, tree-based methods, and other strong baselines.

</details>


### [183] [TS-Haystack: A Multi-Scale Retrieval Benchmark for Time Series Language Models](https://arxiv.org/abs/2602.14200)
*Nicolas Zumarraga,Thomas Kaar,Ning Wang,Maxwell A. Xu,Max Rosenblattl,Markus Kreft,Kevin O'Sullivan,Paul Schmiedmayer,Patrick Langer,Robert Jakob*

Main category: cs.LG

TL;DR: TS-Haystack：一个用于评估时间序列语言模型长上下文检索能力的新基准，通过将短活动片段嵌入长加速度计记录中，系统测试从秒到2小时不同上下文长度的检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列语言模型通常在短序列上训练和评估，但真实世界的时间序列传感器数据可能包含数百万个数据点。当前基准未能捕捉在严格计算约束下进行精确时间定位的需求，这限制了模型在实际长上下文场景中的应用。

Method: 提出TS-Haystack基准，包含四种任务类型（直接检索、时间推理、多步推理和上下文异常）共十种任务。通过受控的"针插入"方法，将短活动片段嵌入到更长的纵向加速度计记录中，系统评估从秒到2小时不同上下文长度的性能。

Result: 研究发现现有时间序列编码器随着上下文长度增加会忽略时间粒度，产生任务依赖性效应：压缩有助于分类但损害局部事件检索。学习到的潜在压缩在高达176倍的压缩比下保持或提高了分类准确率，但检索性能随上下文长度增加而下降，导致时间局部信息丢失。

Conclusion: 研究结果强调了架构设计的重要性，需要解耦序列长度与计算复杂度，同时保持时间保真度。这为未来时间序列语言模型的发展提供了重要方向。

Abstract: Time Series Language Models (TSLMs) are emerging as unified models for reasoning over continuous signals in natural language. However, long-context retrieval remains a major limitation: existing models are typically trained and evaluated on short sequences, while real-world time-series sensor streams can span millions of datapoints. This mismatch requires precise temporal localization under strict computational constraints, a regime that is not captured by current benchmarks. We introduce TS-Haystack, a long-context temporal retrieval benchmark comprising ten task types across four categories: direct retrieval, temporal reasoning, multi-step reasoning and contextual anomaly. The benchmark uses controlled needle insertion by embedding short activity bouts into longer longitudinal accelerometer recordings, enabling systematic evaluation across context lengths ranging from seconds to 2 hours per sample. We hypothesize that existing TSLM time series encoders overlook temporal granularity as context length increases, creating a task-dependent effect: compression aids classification but impairs retrieval of localized events. Across multiple model and encoding strategies, we observe a consistent divergence between classification and retrieval behavior. Learned latent compression preserves or improves classification accuracy at compression ratios up to 176$\times$, but retrieval performance degrades with context length, incurring in the loss of temporally localized information. These results highlight the importance of architectural designs that decouple sequence length from computational complexity while preserving temporal fidelity.

</details>


### [184] [MAGE: All-[MASK] Block Already Knows Where to Look in Diffusion LLM](https://arxiv.org/abs/2602.14209)
*Omin Kwon,Yeonjae Kim,Doyeon Kim,Minseo Kim,Yeonhong Park,Jae W. Lee*

Main category: cs.LG

TL;DR: MAGE提出了一种针对块扩散LLM的高效稀疏注意力方法，通过利用首个全掩码去噪步骤的注意力模式来预测重要KV条目，实现训练自由的稀疏去噪，显著减少KV缓存内存访问并提升长上下文生成速度。


<details>
  <summary>Details</summary>
Motivation: 块扩散LLM在长上下文生成中面临KV缓存内存访问瓶颈，现有为自回归LLM设计的动态稀疏注意力方法在块扩散场景下表现不佳，需要新的高效解决方案。

Method: MAGE利用块扩散独有的特性：首个全掩码去噪步骤的注意力能可靠预测重要KV条目和预算需求，对每个块执行一次精确注意力传递并重用于训练自由的稀疏去噪，同时采用轻量级微调策略增强掩码引导模式。

Result: 在LongBench和Needle-in-a-Haystack等长上下文基准测试中，MAGE以少量KV预算实现近乎无损的准确率，端到端速度提升3-4倍，显著优于自回归导向的稀疏注意力基线方法。

Conclusion: MAGE通过利用块扩散特有的注意力模式，有效解决了长上下文生成中的内存瓶颈问题，为块扩散LLM提供了高效实用的稀疏注意力解决方案。

Abstract: Block diffusion LLMs are emerging as a promising next paradigm for language generation, but their use of KV caching makes memory access a dominant bottleneck in long-context settings. While dynamic sparse attention has been actively explored, existing methods designed for autoregressive LLMs rely on approximate importance estimation and perform poorly when adapted to block diffusion. This work identifies a key opportunity unique to block diffusion: attention at the first All-[MASK] denoising step reliably predicts important KV entries and budget requirements, enabling MAGE to perform a single exact attention pass per block and reuse it for training-free sparse denoising. Across long-context benchmarks including LongBench and Needle-in-a-Haystack, MAGE achieves near-lossless accuracy with a fraction of the KV budget while delivering up to 3-4x end-to-end speedup, consistently outperforming AR-oriented sparse attention baselines. A lightweight fine-tuning strategy further strengthens [MASK]-guided patterns with minimal cost, requiring only a few hours of training on a single NVIDIA H100 GPU for both 1.5B and 7B models.

</details>


### [185] [Robust multi-task boosting using clustering and local ensembling](https://arxiv.org/abs/2602.14231)
*Seyedsaman Emami,Daniel Hernández-Lobato,Gonzalo Martínez-Muñoz*

Main category: cs.LG

TL;DR: RMB-CLE是一个鲁棒的多任务学习框架，通过基于误差的任务聚类和局部集成来防止负迁移，在合成和真实数据上都优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统多任务学习方法在强制不相关或噪声任务共享表示时容易发生负迁移，需要一种能够自适应识别相关任务并鲁棒共享信息的框架。

Method: 提出RMB-CLE框架：1）基于跨任务误差推导任务间相似性，将风险分解为功能不匹配和不可约噪声；2）通过凝聚聚类自适应分组任务；3）在每个簇内使用局部集成实现鲁棒知识共享，同时保留任务特定模式。

Result: 在合成数据中能恢复真实簇结构，在多样化的真实世界和合成基准测试中，一致优于多任务、单任务和基于池化的集成方法。

Conclusion: RMB-CLE不仅是聚类和集成的组合，而是一个通用、可扩展的框架，为鲁棒多任务学习建立了新的基础。

Abstract: Multi-Task Learning (MTL) aims to boost predictive performance by sharing information across related tasks, yet conventional methods often suffer from negative transfer when unrelated or noisy tasks are forced to share representations. We propose Robust Multi-Task Boosting using Clustering and Local Ensembling (RMB-CLE), a principled MTL framework that integrates error-based task clustering with local ensembling. Unlike prior work that assumes fixed clusters or hand-crafted similarity metrics, RMB-CLE derives inter-task similarity directly from cross-task errors, which admit a risk decomposition into functional mismatch and irreducible noise, providing a theoretically grounded mechanism to prevent negative transfer. Tasks are grouped adaptively via agglomerative clustering, and within each cluster, a local ensemble enables robust knowledge sharing while preserving task-specific patterns. Experiments show that RMB-CLE recovers ground-truth clusters in synthetic data and consistently outperforms multi-task, single-task, and pooling-based ensemble methods across diverse real-world and synthetic benchmarks. These results demonstrate that RMB-CLE is not merely a combination of clustering and boosting but a general and scalable framework that establishes a new basis for robust multi-task learning.

</details>


### [186] [Multi-Agent Debate: A Unified Agentic Framework for Tabular Anomaly Detection](https://arxiv.org/abs/2602.14251)
*Pinqiao Wang,Sheng Li*

Main category: cs.LG

TL;DR: MAD是一个多智能体辩论框架，将表格异常检测中不同模型的歧见作为重要信号，通过数学协调层解决分歧，提高鲁棒性并提供可审计的辩论轨迹。


<details>
  <summary>Details</summary>
Motivation: 表格异常检测通常使用单一检测器或静态集成方法，但表格数据的强性能通常来自异构模型家族（如树集成、深度表格网络和表格基础模型），这些模型在分布偏移、缺失数据和罕见异常情况下经常产生分歧。现有方法未能充分利用这种分歧信号。

Method: 提出MAD多智能体辩论框架：每个智能体是基于ML的检测器，输出归一化异常分数、置信度和结构化证据，并由LLM批评家增强。协调器将这些消息转换为有界的每智能体损失，通过指数梯度规则更新智能体影响力，产生最终辩论异常分数和可审计的辩论轨迹。

Result: 在多样化的表格异常检测基准测试中，MAD相比基线方法展现出改进的鲁棒性，并提供更清晰的模型分歧轨迹。该框架能够恢复现有方法（如专家混合门控和学习专家建议聚合），并具有遗憾保证和共形校准能力。

Conclusion: MAD将模型分歧作为一等信号处理，通过数学协调层解决分歧，为表格异常检测提供了一个统一的智能体框架，既提高了性能鲁棒性，又提供了可解释的审计轨迹。

Abstract: Tabular anomaly detection is often handled by single detectors or static ensembles, even though strong performance on tabular data typically comes from heterogeneous model families (e.g., tree ensembles, deep tabular networks, and tabular foundation models) that frequently disagree under distribution shift, missingness, and rare-anomaly regimes. We propose MAD, a Multi-Agent Debating framework that treats this disagreement as a first-class signal and resolves it through a mathematically grounded coordination layer. Each agent is a machine learning (ML)-based detector that produces a normalized anomaly score, confidence, and structured evidence, augmented by a large language model (LLM)-based critic. A coordinator converts these messages into bounded per-agent losses and updates agent influence via an exponentiated-gradient rule, yielding both a final debated anomaly score and an auditable debate trace. MAD is a unified agentic framework that can recover existing approaches, such as mixture-of-experts gating and learning-with-expert-advice aggregation, by restricting the message space and synthesis operator. We establish regret guarantees for the synthesized losses and show how conformal calibration can wrap the debated score to control false positives under exchangeability. Experiments on diverse tabular anomaly benchmarks show improved robustness over baselines and clearer traces of model disagreement

</details>


### [187] [Cross-household Transfer Learning Approach with LSTM-based Demand Forecasting](https://arxiv.org/abs/2602.14267)
*Manal Rahal,Bestoun S. Ahmed,Roger Renström,Robert Stener*

Main category: cs.LG

TL;DR: DELTAiF是一个基于迁移学习的框架，用于预测家庭热水消耗，通过从代表性家庭学习知识并微调到其他家庭，减少67%训练时间，同时保持高预测精度。


<details>
  <summary>Details</summary>
Motivation: 随着住宅热泵安装的快速增长，优化家庭热水生产至关重要，但面临技术和可扩展性挑战。传统方法为每个家庭单独训练机器学习模型在云连接热泵部署中计算成本过高。

Method: 提出DELTAiF迁移学习框架，从代表性家庭学习知识并微调到其他家庭，预测大型热水使用事件（如淋浴），避免为每个热泵安装单独训练模型。

Result: 减少约67%的训练时间，预测精度在0.874-0.991之间，平均绝对百分比误差在0.001-0.017之间。当源家庭表现出规律消费模式时，迁移学习特别有效。

Conclusion: 迁移学习能够实现可扩展的家庭热水需求预测，DELTAiF框架通过知识转移显著降低计算成本，同时保持高预测准确性。

Abstract: With the rapid increase in residential heat pump (HP) installations, optimizing hot water production in households is essential, yet it faces major technical and scalability challenges. Adapting production to actual household needs requires accurate forecasting of hot water demand to ensure comfort and, most importantly, to reduce energy waste. However, the conventional approach of training separate machine learning models for each household becomes computationally expensive at scale, particularly in cloud-connected HP deployments.
  This study introduces DELTAiF, a transfer learning (TL) based framework that provides scalable and accurate prediction of household hot water consumption. By predicting large hot water usage events, such as showers, DELTAiF enables adaptive yet scalable hot water production at the household level. DELTAiF leverages learned knowledge from a representative household and fine-tunes it across others, eliminating the need to train separate machine learning models for each HP installation. This approach reduces overall training time by approximately 67 percent while maintaining high predictive accuracy values between 0.874 and 0.991, and mean absolute percentage error values between 0.001 and 0.017. The results show that TL is particularly effective when the source household exhibits regular consumption patterns, enabling hot water demand forecasting at scale.

</details>


### [188] [Radial-VCReg: More Informative Representation Learning Through Radial Gaussianization](https://arxiv.org/abs/2602.14272)
*Yilun Kuang,Yash Dagade,Deep Chakraborty,Erik Learned-Miller,Randall Balestriero,Tim G. J. Rudner,Yann LeCun*

Main category: cs.LG

TL;DR: Radial-VCReg通过添加径向高斯化损失增强VCReg，将特征范数与卡方分布对齐，从而更好地实现最大熵表示学习


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习方法如VCReg通过正则化一阶和二阶特征统计量无法完全实现最大熵，因为维度诅咒阻碍了显式信息最大化

Method: 提出Radial-VCReg方法，在VCReg基础上增加径向高斯化损失，将特征范数与卡方分布对齐（高维高斯分布的关键特性）

Result: 证明Radial-VCReg能将更广泛的分布类别转化为正态分布；在合成和真实数据集上一致提升性能，减少高阶依赖并促进更多样化和信息丰富的表示

Conclusion: 径向高斯化损失有效增强了VCReg，通过更好地实现最大熵原则来学习更优的表示

Abstract: Self-supervised learning aims to learn maximally informative representations, but explicit information maximization is hindered by the curse of dimensionality. Existing methods like VCReg address this by regularizing first and second-order feature statistics, which cannot fully achieve maximum entropy. We propose Radial-VCReg, which augments VCReg with a radial Gaussianization loss that aligns feature norms with the Chi distribution-a defining property of high-dimensional Gaussians. We prove that Radial-VCReg transforms a broader class of distributions towards normality compared to VCReg and show on synthetic and real-world datasets that it consistently improves performance by reducing higher-order dependencies and promoting more diverse and informative representations.

</details>


### [189] [Integrating Unstructured Text into Causal Inference: Empirical Evidence from Real Data](https://arxiv.org/abs/2602.14274)
*Boning Zhou,Ziyu Wang,Han Hong,Haoqi Hu*

Main category: cs.LG

TL;DR: 提出基于Transformer语言模型的框架，利用非结构化文本进行因果推断，验证了与结构化数据方法的一致性


<details>
  <summary>Details</summary>
Motivation: 传统因果推断严重依赖结构化数据，但在现实场景中结构化数据往往不完整或不可得，限制了因果推断的应用范围

Method: 开发基于Transformer语言模型的框架，从非结构化文本中提取信息进行因果推断，并在群体、组别和个体三个层次上验证

Result: 非结构化文本得到的因果估计与结构化数据方法的结果一致，验证了文本数据在因果推断中的有效性

Conclusion: 该框架扩展了因果推断方法的应用范围，使其在只有文本数据的情况下也能进行数据驱动的商业决策

Abstract: Causal inference, a critical tool for informing business decisions, traditionally relies heavily on structured data. However, in many real-world scenarios, such data can be incomplete or unavailable. This paper presents a framework that leverages transformer-based language models to perform causal inference using unstructured text. We demonstrate the effectiveness of our framework by comparing causal estimates derived from unstructured text against those obtained from structured data across population, group, and individual levels. Our findings show consistent results between the two approaches, validating the potential of unstructured text in causal inference tasks. Our approach extends the applicability of causal inference methods to scenarios where only textual data is available, enabling data-driven business decision-making when structured tabular data is scarce.

</details>


### [190] [Reverse N-Wise Output-Oriented Testing for AI/ML and Quantum Computing Systems](https://arxiv.org/abs/2602.14275)
*Lamine Rihani*

Main category: cs.LG

TL;DR: 论文提出反向n-wise输出测试方法，通过构建输出等价类的覆盖数组，使用无梯度元启发式优化合成输入配置，以测试AI/ML和量子计算系统的复杂行为特性。


<details>
  <summary>Details</summary>
Motivation: AI/ML系统和量子计算软件面临前所未有的测试挑战：高维连续输入空间、概率性输出分布、仅通过可观测行为定义正确性，以及公平性、鲁棒性、误差模式等关键质量维度需要通过复杂的多向交互来体现。

Method: 提出反向n-wise输出测试范式，直接在输出等价类（ML置信度桶、决策边界区域、公平性分区、嵌入聚类、量子测量结果分布、误差模式等）上构建覆盖数组，通过无梯度元启发式优化解决黑盒逆映射问题，合成能引发目标行为特征的输入配置或量子电路参数。

Result: 该框架为两个领域带来协同效益：明确的客户中心预测/测量覆盖保证，显著提高ML校准/边界故障和量子误差模式的故障检测率，增强测试套件效率，并建立结构化的MLOps/量子验证流程，支持从不确定性分析自动发现分区和覆盖漂移监控。

Conclusion: 反向n-wise输出测试为测试复杂AI/ML和量子系统提供了一种数学原理上的范式转换，通过关注输出行为特性而非传统输入-输出映射，有效解决了这些系统特有的测试挑战。

Abstract: Artificial intelligence/machine learning (AI/ML) systems and emerging quantum computing software present unprecedented testing challenges characterized by high-dimensional/continuous input spaces, probabilistic/non-deterministic output distributions, behavioral correctness defined exclusively over observable prediction behaviors and measurement outcomes, and critical quality dimensions, trustworthiness, fairness, calibration, robustness, error syndrome patterns, that manifest through complex multi-way interactions among semantically meaningful output properties rather than deterministic input-output mappings. This paper introduces reverse n-wise output testing, a mathematically principled paradigm inversion that constructs covering arrays directly over domain-specific output equivalence classes, ML confidence calibration buckets, decision boundary regions, fairness partitions, embedding clusters, ranking stability bands, quantum measurement outcome distributions (0-dominant, 1-dominant, superposition collapse), error syndrome patterns (bit-flip, phase-flip, correlated errors), then solves the computationally challenging black-box inverse mapping problem via gradient-free metaheuristic optimization to synthesize input feature configurations or quantum circuit parameters capable of eliciting targeted behavioral signatures from opaque models. The framework delivers synergistic benefits across both domains: explicit customer-centric prediction/measurement coverage guarantees, substantial improvements in fault detection rates for ML calibration/boundary failures and quantum error syndromes, enhanced test suite efficiency, and structured MLOps/quantum validation pipelines with automated partition discovery from uncertainty analysis and coverage drift monitoring.

</details>


### [191] [Whom to Query for What: Adaptive Group Elicitation via Multi-Turn LLM Interactions](https://arxiv.org/abs/2602.14279)
*Ruomeng Ding,Tianwei Gao,Thomas P. Zollo,Eitan Bachmat,Richard Zemel,Zhun Deng*

Main category: cs.LG

TL;DR: 提出自适应群体信息获取框架，结合LLM信息增益和图神经网络，在有限预算下优化问题选择和受访者选择，提高群体层面响应预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有信息获取方法通常优化问题选择但固定受访者池，不利用群体结构或不适应部分/不完整响应。需要一种能自适应选择问题和受访者的方法，在有限查询和参与预算下提高群体层面信息获取效率。

Method: 提出自适应群体信息获取框架：1) 使用LLM基于预期信息增益评分候选问题；2) 使用异构图神经网络传播聚合观察到的响应和参与者属性，填补缺失响应并指导每轮受访者选择。形成闭环程序，查询少量信息丰富的个体，同时通过结构化相似性推断群体层面响应。

Result: 在三个真实世界意见数据集上，该方法在有限预算下持续改进群体层面响应预测，在CES数据集上以10%受访者预算获得超过12%的相对增益。

Conclusion: 提出的自适应群体信息获取框架有效结合LLM和图神经网络，在有限预算下通过自适应选择问题和受访者，显著提高群体层面信息获取效率，为调查和集体评估中的不确定性减少提供新方法。

Abstract: Eliciting information to reduce uncertainty about latent group-level properties from surveys and other collective assessments requires allocating limited questioning effort under real costs and missing data. Although large language models enable adaptive, multi-turn interactions in natural language, most existing elicitation methods optimize what to ask with a fixed respondent pool, and do not adapt respondent selection or leverage population structure when responses are partial or incomplete. To address this gap, we study adaptive group elicitation, a multi-round setting where an agent adaptively selects both questions and respondents under explicit query and participation budgets. We propose a theoretically grounded framework that combines (i) an LLM-based expected information gain objective for scoring candidate questions with (ii) heterogeneous graph neural network propagation that aggregates observed responses and participant attributes to impute missing responses and guide per-round respondent selection. This closed-loop procedure queries a small, informative subset of individuals while inferring population-level responses via structured similarity. Across three real-world opinion datasets, our method consistently improves population-level response prediction under constrained budgets, including a >12% relative gain on CES at a 10% respondent budget.

</details>


### [192] [KernelBlaster: Continual Cross-Task CUDA Optimization via Memory-Augmented In-Context Reinforcement Learning](https://arxiv.org/abs/2602.14293)
*Kris Shengjun Dong,Sahil Modi,Dima Nikiforov,Sana Damani,Edward Lin,Siva Kumar Sastry Hari,Christos Kozyrakis*

Main category: cs.LG

TL;DR: KernelBlaster是一个基于记忆增强上下文强化学习的框架，通过构建持久CUDA知识库来提升LLM代理的GPU代码优化能力，实现跨多代GPU架构的高性能CUDA代码生成。


<details>
  <summary>Details</summary>
Motivation: 传统编译器受限于固定启发式方法，而微调大型语言模型成本高昂。现有的CUDA代码优化代理工作流缺乏从先前探索中聚合知识的能力，导致采样偏差和次优解。

Method: 提出记忆增强上下文强化学习框架，构建可检索的持久CUDA知识库，采用基于配置文件的文本梯度代理流程进行CUDA生成和优化，系统探索超越简单重写的高潜力优化策略。

Result: 在KernelBench三个级别上相比PyTorch基线分别实现了1.43倍、2.50倍和1.50倍的几何平均加速比，性能显著提升。

Conclusion: KernelBlaster作为一个开源代理框架，通过知识积累和系统化探索，有效解决了跨多代GPU架构的CUDA代码优化挑战，提供了可复现的评估流程。

Abstract: Optimizing CUDA code across multiple generations of GPU architectures is challenging, as achieving peak performance requires an extensive exploration of an increasingly complex, hardware-specific optimization space. Traditional compilers are constrained by fixed heuristics, whereas finetuning Large Language Models (LLMs) can be expensive. However, agentic workflows for CUDA code optimization have limited ability to aggregate knowledge from prior exploration, leading to biased sampling and suboptimal solutions. We propose KernelBlaster, a Memory-Augmented In-context Reinforcement Learning (MAIC-RL) framework designed to improve CUDA optimization search capabilities of LLM-based GPU coding agents. KernelBlaster enables agents to learn from experience and make systematically informed decisions on future tasks by accumulating knowledge into a retrievable Persistent CUDA Knowledge Base. We propose a novel profile-guided, textual-gradient-based agentic flow for CUDA generation and optimization to achieve high performance across generations of GPU architectures. KernelBlaster guides LLM agents to systematically explore high-potential optimization strategies beyond naive rewrites. Compared to the PyTorch baseline, our method achieves geometric mean speedups of 1.43x, 2.50x, and 1.50x on KernelBench Levels 1, 2, and 3, respectively. We release KernelBlaster as an open-source agentic framework, accompanied by a test harness, verification components, and a reproducible evaluation pipeline.

</details>


### [193] [Machine Learning as a Tool (MLAT): A Framework for Integrating Statistical ML Models as Callable Tools within LLM Agent Workflows](https://arxiv.org/abs/2602.14295)
*Edwin Chen,Zulekha Bibi*

Main category: cs.LG

TL;DR: MLAT是一种设计模式，将预训练机器学习模型作为可调用工具集成到LLM工作流中，使LLM能按需调用定量预测并推理输出。通过PitchCraft系统验证，该系统将发现通话录音转换为专业提案，包含ML定价预测，将提案生成时间从数小时缩短至10分钟内。


<details>
  <summary>Details</summary>
Motivation: 传统ML推理通常作为静态预处理步骤，限制了LLM根据对话上下文动态调用定量预测的能力。需要一种设计模式，使ML模型成为LLM工作流中的一等工具，实现定量估计与上下文推理的结合。

Method: 提出MLAT设计模式，将预训练ML模型作为可调用工具集成到LLM代理工作流。开发PitchCraft系统验证：包含研究代理（并行工具调用收集情报）和草稿代理（调用XGBoost定价模型作为工具调用，通过结构化输出生成完整提案）。定价模型使用70个真实和人工验证合成数据训练。

Result: 定价模型在保留数据上达到R^2=0.807，平均绝对误差3688美元。系统将提案生成时间从数小时减少到10分钟以内。敏感性分析显示模型学习了有意义的定价关系。

Conclusion: MLAT是一种有效的设计模式，使ML模型成为LLM工作流中的一等工具，适用于需要定量估计与上下文推理结合的领域。在数据稀缺情况下也能有效训练模型，显著提高工作效率。

Abstract: We introduce Machine Learning as a Tool (MLAT), a design pattern in which pre-trained statistical machine learning models are exposed as callable tools within large language model (LLM) agent workflows. This allows an orchestrating agent to invoke quantitative predictions when needed and reason about their outputs in context. Unlike conventional pipelines that treat ML inference as a static preprocessing step, MLAT positions the model as a first-class tool alongside web search, database queries, and APIs, enabling the LLM to decide when and how to use it based on conversational context.
  To validate MLAT, we present PitchCraft, a pilot production system that converts discovery call recordings into professional proposals with ML-predicted pricing. The system uses two agents: a Research Agent that gathers prospect intelligence via parallel tool calls, and a Draft Agent that invokes an XGBoost pricing model as a tool call and generates a complete proposal through structured outputs. The pricing model, trained on 70 examples combining real and human-verified synthetic data, achieves R^2 = 0.807 on held-out data with a mean absolute error of 3688 USD. The system reduces proposal generation time from multiple hours to under 10 minutes.
  We describe the MLAT framework, structured output architecture, training methodology under extreme data scarcity, and sensitivity analysis demonstrating meaningful learned relationships. MLAT generalizes to domains requiring quantitative estimation combined with contextual reasoning.

</details>


### [194] [DeepFusion: Accelerating MoE Training via Federated Knowledge Distillation from Heterogeneous Edge Devices](https://arxiv.org/abs/2602.14301)
*Songyuan Li,Jia Hu,Ahmed M. Abdelmoniem,Geyong Min,Haojun Huang,Jiwei Huang*

Main category: cs.LG

TL;DR: DeepFusion：首个可扩展的联邦MoE训练框架，通过联邦知识蒸馏融合异构设备上的LLM知识，解决资源受限设备无法承载大型MoE模型的问题。


<details>
  <summary>Details</summary>
Motivation: MoE-based LLMs需要大量多样化训练数据，联邦学习可利用异构边缘设备的私有数据，但传统FL要求设备本地托管MoE模型，这对资源受限设备不现实。

Method: 1) 设备独立配置和训练适合自身需求和硬件限制的本地LLM；2) 提出View-Aligned Attention模块，整合全局MoE模型的多阶段特征表示，构建与本地LLM对齐的预测视角，实现有效的跨架构知识蒸馏。

Result: 在工业级MoE模型和真实数据集上的实验表明，DeepFusion性能接近集中式MoE训练，相比基准方法通信成本降低71%，token困惑度提升5.28%。

Conclusion: DeepFusion通过解决视图不匹配问题，实现了高效的联邦MoE训练，为资源受限设备参与大型MoE模型训练提供了可行方案。

Abstract: Recent Mixture-of-Experts (MoE)-based large language models (LLMs) such as Qwen-MoE and DeepSeek-MoE are transforming generative AI in natural language processing. However, these models require vast and diverse training data. Federated learning (FL) addresses this challenge by leveraging private data from heterogeneous edge devices for privacy-preserving MoE training. Nonetheless, traditional FL approaches require devices to host local MoE models, which is impractical for resource-constrained devices due to large model sizes. To address this, we propose DeepFusion, the first scalable federated MoE training framework that enables the fusion of heterogeneous on-device LLM knowledge via federated knowledge distillation, yielding a knowledge-abundant global MoE model. Specifically, DeepFusion features each device to independently configure and train an on-device LLM tailored to its own needs and hardware limitations. Furthermore, we propose a novel View-Aligned Attention (VAA) module that integrates multi-stage feature representations from the global MoE model to construct a predictive perspective aligned with on-device LLMs, thereby enabling effective cross-architecture knowledge distillation. By explicitly aligning predictive perspectives, VAA resolves the view-mismatch problem in traditional federated knowledge distillation, which arises from heterogeneity in model architectures and prediction behaviors between on-device LLMs and the global MoE model. Experiments with industry-level MoE models (Qwen-MoE and DeepSeek-MoE) and real-world datasets (medical and finance) demonstrate that DeepFusion achieves performance close to centralized MoE training. Compared with key federated MoE baselines, DeepFusion reduces communication costs by up to 71% and improves token perplexity by up to 5.28%.

</details>


### [195] [In Transformer We Trust? A Perspective on Transformer Architecture Failure Modes](https://arxiv.org/abs/2602.14318)
*Trishit Mondal,Ameya D. Jagtap*

Main category: cs.LG

TL;DR: 该论文系统评估Transformer模型在安全关键应用中的可信度，涵盖可解释性、鲁棒性、公平性和隐私等方面，识别其结构脆弱性和领域特定风险。


<details>
  <summary>Details</summary>
Motivation: 随着Transformer架构在自然语言处理、计算机视觉、医疗保健、自主系统以及气候建模、材料发现、药物发现、核科学、机器人等科学计算关键领域的高风险应用中日益广泛部署，需要对其可信度进行更深入和严谨的理解。

Method: 通过全面综述Transformer模型的可解释性、可解释性、对抗攻击鲁棒性、公平性和隐私保护等方面，系统评估其可信度。同时，在自然语言处理、计算机视觉以及机器人、医学、地球科学、材料科学、流体动力学、核科学、自动定理证明等科学工程领域的安全关键应用中，分析这些架构的风险。

Result: 通过跨领域综合分析，识别出Transformer模型重复出现的结构脆弱性、领域特定风险以及限制其可靠部署的开放研究挑战。

Conclusion: Transformer模型在安全关键应用中的可信度需要系统评估，存在结构脆弱性和领域特定风险，需要进一步研究解决这些挑战以确保可靠部署。

Abstract: Transformer architectures have revolutionized machine learning across a wide range of domains, from natural language processing to scientific computing. However, their growing deployment in high-stakes applications, such as computer vision, natural language processing, healthcare, autonomous systems, and critical areas of scientific computing including climate modeling, materials discovery, drug discovery, nuclear science, and robotics, necessitates a deeper and more rigorous understanding of their trustworthiness. In this work, we critically examine the foundational question: \textitHow trustworthy are transformer models?} We evaluate their reliability through a comprehensive review of interpretability, explainability, robustness against adversarial attacks, fairness, and privacy. We systematically examine the trustworthiness of transformer-based models in safety-critical applications spanning natural language processing, computer vision, and science and engineering domains, including robotics, medicine, earth sciences, materials science, fluid dynamics, nuclear science, and automated theorem proving; highlighting high-impact areas where these architectures are central and analyzing the risks associated with their deployment. By synthesizing insights across these diverse areas, we identify recurring structural vulnerabilities, domain-specific risks, and open research challenges that limit the reliable deployment of transformers.

</details>


### [196] [Conformal Signal Temporal Logic for Robust Reinforcement Learning Control: A Case Study](https://arxiv.org/abs/2602.14322)
*Hani Beirami,M M Manjurul Islam*

Main category: cs.LG

TL;DR: 该研究提出了一种结合形式化时序逻辑规范与强化学习控制的方法，在航空航天应用中通过符合性STL屏蔽器增强安全性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在航空航天应用中，强化学习控制需要更高的安全性和鲁棒性保证。形式化时序逻辑规范能够提供明确的性能要求，但如何有效执行这些规范并与数据驱动的RL控制结合是一个挑战。

Method: 使用PPO算法训练F-16仿真控制代理，将控制目标编码为STL规范。提出符合性STL屏蔽器，通过在线符合性预测过滤RL代理的动作。对比了三种设置：PPO基线、PPO+传统规则STL屏蔽器、PPO+符合性屏蔽器。

Result: 实验表明，符合性屏蔽器在保持STL规范满足的同时，维持接近基线的性能，并在模型失配、执行器限制、测量噪声和设定点跳变等压力场景下，比传统屏蔽器提供更强的鲁棒性保证。

Conclusion: 将形式化规范监控与数据驱动的RL控制相结合，可以显著提高自主飞行控制在挑战性环境中的可靠性，为航空航天应用提供了更安全、鲁棒的控制方案。

Abstract: We investigate how formal temporal logic specifications can enhance the safety and robustness of reinforcement learning (RL) control in aerospace applications. Using the open source AeroBench F-16 simulation benchmark, we train a Proximal Policy Optimization (PPO) agent to regulate engine throttle and track commanded airspeed. The control objective is encoded as a Signal Temporal Logic (STL) requirement to maintain airspeed within a prescribed band during the final seconds of each maneuver. To enforce this specification at run time, we introduce a conformal STL shield that filters the RL agent's actions using online conformal prediction. We compare three settings: (i) PPO baseline, (ii) PPO with a classical rule-based STL shield, and (iii) PPO with the proposed conformal shield, under both nominal conditions and a severe stress scenario involving aerodynamic model mismatch, actuator rate limits, measurement noise, and mid-episode setpoint jumps. Experiments show that the conformal shield preserves STL satisfaction while maintaining near baseline performance and providing stronger robustness guarantees than the classical shield. These results demonstrate that combining formal specification monitoring with data driven RL control can substantially improve the reliability of autonomous flight control in challenging environments.

</details>


### [197] [Train Less, Learn More: Adaptive Efficient Rollout Optimization for Group-Based Reinforcement Learning](https://arxiv.org/abs/2602.14338)
*Zhi Zhang,Zhen Han,Costas Mavromatis,Qi Zhu,Yunyi Zhang,Sheng Guan,Dingmin Wang,Xiong Zhou,Shuai Wang,Soji Adeshina,Vassilis Ioannidis,Huzefa Rangwala*

Main category: cs.LG

TL;DR: AERO改进GRPO方法，通过自适应策略、选择性拒绝和贝叶斯后验防止零优势死区，在保持性能的同时显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: GRPO方法在LLM后训练中广泛使用，但当组内所有rollouts结果相同时，归一化优势为零，导致梯度信号消失和计算浪费。需要解决这一效率问题。

Method: 提出AERO方法：1) 自适应rollout策略；2) 选择性拒绝策略性地修剪rollouts；3) 维护贝叶斯后验防止零优势死区。

Result: 在相同总rollout预算下，AERO减少约48%的总训练计算量，缩短约45%的每步wall-clock时间，同时保持或改进Pass@8和Avg@8性能。

Conclusion: AERO为基于RL的LLM对齐提供了一种实用、可扩展且计算高效的策略，显著提升训练效率而不牺牲性能。

Abstract: Reinforcement learning (RL) plays a central role in large language model (LLM) post-training. Among existing approaches, Group Relative Policy Optimization (GRPO) is widely used, especially for RL with verifiable rewards (RLVR) fine-tuning. In GRPO, each query prompts the LLM to generate a group of rollouts with a fixed group size $N$. When all rollouts in a group share the same outcome, either all correct or all incorrect, the group-normalized advantages become zero, yielding no gradient signal and wasting fine-tuning compute. We introduce Adaptive Efficient Rollout Optimization (AERO), an enhancement of GRPO. AERO uses an adaptive rollout strategy, applies selective rejection to strategically prune rollouts, and maintains a Bayesian posterior to prevent zero-advantage dead zones. Across three model configurations (Qwen2.5-Math-1.5B, Qwen2.5-7B, and Qwen2.5-7B-Instruct), AERO improves compute efficiency without sacrificing performance. Under the same total rollout budget, AERO reduces total training compute by about 48% while shortening wall-clock time per step by about 45% on average. Despite the substantial reduction in compute, AERO matches or improves Pass@8 and Avg@8 over GRPO, demonstrating a practical, scalable, and compute-efficient strategy for RL-based LLM alignment.

</details>


### [198] [Zero-Shot Instruction Following in RL via Structured LTL Representations](https://arxiv.org/abs/2602.14344)
*Mathias Jackermeier,Mattia Giuri,Jacques Cloete,Alessandro Abate*

Main category: cs.LG

TL;DR: 提出一种基于线性时序逻辑(LTL)的多任务强化学习方法，通过层次化神经网络编码任务逻辑结构，使用注意力机制让策略能够推理未来子目标，实现对新任务的零样本执行。


<details>
  <summary>Details</summary>
Motivation: 现有方法虽然能训练通用策略，但难以有效捕捉LTL规范中丰富的逻辑和时间结构。需要学习结构化的任务表示来促进训练和泛化。

Method: 基于任务有限自动机构建布尔公式序列来条件化策略，提出层次化神经网络架构编码公式的逻辑结构，引入注意力机制让策略能够推理未来子目标。

Result: 在多种复杂环境中的实验表明，该方法具有强大的泛化能力和优越性能。

Conclusion: 提出的结构化任务表示学习方法能够有效处理LTL规范中的逻辑和时间结构，显著提升多任务强化学习中指令跟随的泛化能力。

Abstract: We study instruction following in multi-task reinforcement learning, where an agent must zero-shot execute novel tasks not seen during training. In this setting, linear temporal logic (LTL) has recently been adopted as a powerful framework for specifying structured, temporally extended tasks. While existing approaches successfully train generalist policies, they often struggle to effectively capture the rich logical and temporal structure inherent in LTL specifications. In this work, we address these concerns with a novel approach to learn structured task representations that facilitate training and generalisation. Our method conditions the policy on sequences of Boolean formulae constructed from a finite automaton of the task. We propose a hierarchical neural architecture to encode the logical structure of these formulae, and introduce an attention mechanism that enables the policy to reason about future subgoals. Experiments in a variety of complex environments demonstrate the strong generalisation capabilities and superior performance of our approach.

</details>


### [199] [WIMLE: Uncertainty-Aware World Models with IMLE for Sample-Efficient Continuous Control](https://arxiv.org/abs/2602.14351)
*Mehran Aghabozorgi,Alireza Moazeni,Yanshu Zhang,Ke Li*

Main category: cs.LG

TL;DR: WIMLE：一种基于模型的强化学习方法，通过IMLE学习随机多模态世界模型，使用集成和潜在采样估计预测不确定性，并通过置信度加权合成转移来稳定学习


<details>
  <summary>Details</summary>
Motivation: 基于模型的强化学习通常样本效率高，但在实践中表现不佳，原因包括：模型误差累积、单模态世界模型平均多模态动态、过度自信预测导致学习偏差

Method: 将隐式最大似然估计（IMLE）扩展到基于模型的RL框架中，学习随机多模态世界模型（无需迭代采样），通过集成和潜在采样估计预测不确定性，训练时根据预测置信度加权合成转移

Result: 在40个连续控制任务（DeepMind Control、MyoSuite、HumanoidBench）上，WIMLE实现了优越的样本效率和竞争性或更好的渐近性能。在Humanoid-run任务上，样本效率比最强竞争者提高50%以上；在HumanoidBench上解决了14个任务中的8个（BRO为4个，SimbaV2为5个）

Conclusion: IMLE的多模态性和不确定性感知加权对于稳定的基于模型的强化学习具有重要价值，能够有效处理多模态动态并减少模型误差的影响

Abstract: Model-based reinforcement learning promises strong sample efficiency but often underperforms in practice due to compounding model error, unimodal world models that average over multi-modal dynamics, and overconfident predictions that bias learning. We introduce WIMLE, a model-based method that extends Implicit Maximum Likelihood Estimation (IMLE) to the model-based RL framework to learn stochastic, multi-modal world models without iterative sampling and to estimate predictive uncertainty via ensembles and latent sampling. During training, WIMLE weights each synthetic transition by its predicted confidence, preserving useful model rollouts while attenuating bias from uncertain predictions and enabling stable learning. Across $40$ continuous-control tasks spanning DeepMind Control, MyoSuite, and HumanoidBench, WIMLE achieves superior sample efficiency and competitive or better asymptotic performance than strong model-free and model-based baselines. Notably, on the challenging Humanoid-run task, WIMLE improves sample efficiency by over $50$\% relative to the strongest competitor, and on HumanoidBench it solves $8$ of $14$ tasks (versus $4$ for BRO and $5$ for SimbaV2). These results highlight the value of IMLE-based multi-modality and uncertainty-aware weighting for stable model-based RL.

</details>


### [200] [A Study on Multi-Class Online Fuzzy Classifiers for Dynamic Environments](https://arxiv.org/abs/2602.14375)
*Kensuke Ajimoto,Yuma Yamamoto,Yoshifumi Kusunoki,Tomoharu Nakashima*

Main category: cs.LG

TL;DR: 提出一种用于动态环境的多类在线模糊分类器，扩展了传统仅处理二分类问题的在线模糊分类器


<details>
  <summary>Details</summary>
Motivation: 传统在线模糊分类器只能处理二分类问题，但在实际应用中经常需要处理多类分类问题。此外，在动态环境中，训练数据不是一次性全部可用，而是随时间逐步到达。

Method: 使用模糊if-then规则构建分类器，其中前件模糊集由人工预先确定，后件实值通过训练数据学习。在在线框架下，每次只处理少量可用模式，后续模式随时间逐步到达。

Result: 通过合成动态数据和多个基准数据集进行数值实验，评估了多类在线模糊分类器的性能。

Conclusion: 成功将在线模糊分类器从二分类扩展到多分类问题，为动态环境中的多类分类提供了有效解决方案。

Abstract: This paper proposes a multi-class online fuzzy classifier for dynamic environments. A fuzzy classifier comprises a set of fuzzy if-then rules where human users determine the antecedent fuzzy sets beforehand. In contrast, the consequent real values are determined by learning from training data. In an online framework, not all training dataset patterns are available beforehand. Instead, only a few patterns are available at a time step, and the subsequent patterns become available at the following time steps. The conventional online fuzzy classifier considered only two-class problems. This paper investigates the extension to the conventional fuzzy classifiers for multi-class problems. We evaluate the performance of the multi-class online fuzzy classifiers through numerical experiments on synthetic dynamic data and also several benchmark datasets.

</details>


### [201] [A unified framework for evaluating the robustness of machine-learning interpretability for prospect risking](https://arxiv.org/abs/2602.14430)
*Prithwijit Chowdhury,Ahmad Mustafa,Mohit Prabhushankar,Ghassan AlRegib*

Main category: cs.LG

TL;DR: 提出一个统一框架，通过反事实生成和必要性/充分性量化来评估LIME和SHAP在油气勘探风险数据上的解释鲁棒性


<details>
  <summary>Details</summary>
Motivation: 在油气勘探风险评估中，机器学习分类器缺乏透明度，而现有XAI方法（如LIME和SHAP）对同一场景的解释存在不一致，需要更可靠的方法来提升解释的可信度

Method: 提出统一框架，生成反事实并量化必要性和充分性，用于评估LIME和SHAP在高维结构化勘探风险数据上的解释鲁棒性

Result: 鲁棒性测试提供了对模型处理错误数据能力的深入洞察，并确定了哪种XAI模块与哪种模型组合在油气指示数据集上表现最佳

Conclusion: 基于因果理论中的必要性和充分性概念来验证特征排序，可以更可靠地提升XAI解释策略的可信度，为油气勘探风险评估提供更稳健的解释框架

Abstract: In geophysics, hydrocarbon prospect risking involves assessing the risks associated with hydrocarbon exploration by integrating data from various sources. Machine learning-based classifiers trained on tabular data have been recently used to make faster decisions on these prospects. The lack of transparency in the decision-making processes of such models has led to the emergence of explainable AI (XAI). LIME and SHAP are two such examples of these XAI methods which try to generate explanations of a particular decision by ranking the input features in terms of importance. However, explanations of the same scenario generated by these two different explanation strategies have shown to disagree or be different, particularly for complex data. This is because the definitions of "importance" and "relevance" differ for different explanation strategies. Thus, grounding these ranked features using theoretically backed causal ideas of necessity and sufficiency can prove to be a more reliable and robust way to improve the trustworthiness of the concerned explanation strategies.We propose a unified framework to generate counterfactuals as well as quantify necessity and sufficiency and use these to perform a robustness evaluation of the explanations provided by LIME and SHAP on high dimensional structured prospect risking data. This robustness test gives us deeper insights into the models capabilities to handle erronous data and which XAI module works best in pair with which model for our dataset for hydorcarbon indication.

</details>


### [202] [Broken Chains: The Cost of Incomplete Reasoning in LLMs](https://arxiv.org/abs/2602.14444)
*Ian Su,Gaurav Purushothaman,Jey Narayan,Ruhika Goel,Kevin Zhu,Sunishchal Dev,Yash More,Maheep Chaudhary*

Main category: cs.LG

TL;DR: 研究比较了不同推理模式（代码、自然语言、混合、无推理）在token预算受限下的性能表现，发现代码推理在预算削减时性能下降更平缓，而截断的推理链可能误导模型。


<details>
  <summary>Details</summary>
Motivation: 推理专用模型在推理过程中消耗大量计算资源，推理token成本高昂。需要了解不同推理模式在token约束下的表现，为资源受限环境部署推理系统提供指导。

Method: 提出一个框架，强制模型仅通过代码、注释、两者结合或不进行推理来思考，然后系统地将token预算削减到最优的10%、30%、50%和70%。在四个前沿模型（GPT-5.1、Gemini 3 Flash、DeepSeek-V3.2、Grok 4.1）上评估数学基准（AIME、GSM8K、HMMT）。

Result: 1) 截断推理可能有害：DeepSeek-V3.2在无推理时达到53%准确率，但在50%预算下截断CoT仅17%；2) 代码推理性能下降平缓：Gemini的注释准确率降至0%而代码保持43-47%；3) 混合推理表现不如单一模式；4) 鲁棒性模型依赖：Grok在30%预算下保持80-90%准确率，而OpenAI和DeepSeek降至7-27%。

Conclusion: 不完整的推理链会主动误导模型，这对在资源约束下部署推理专用系统具有重要启示。代码推理在预算受限时表现更稳健，而混合推理模式可能不是最佳选择。

Abstract: Reasoning-specialized models like OpenAI's 5.1 and DeepSeek-V3.2 allocate substantial inference compute to extended chain-of-thought (CoT) traces, yet reasoning tokens incur significant costs. How do different reasoning modalities of code, natural language, hybrid, or none do perform under token constraints? We introduce a framework that constrains models to reason exclusively through code, comments, both, or neither, then systematically ablates token budgets to 10\%, 30\%, 50\%, and 70\% of optimal. We evaluate four frontier models (GPT-5.1, Gemini 3 Flash, DeepSeek-V3.2, Grok 4.1) across mathematical benchmarks (AIME, GSM8K, HMMT). Our findings reveal: (1) \textbf{truncated reasoning can hurt} as DeepSeek-V3.2 achieves 53\% with no reasoning but only 17\% with truncated CoT at 50\% budget; (2) \textbf{code degrades gracefully} as Gemini's comments collapse to 0\% while code maintains 43-47\%; (3) \textbf{hybrid reasoning underperforms} single modalities; (4) \textbf{robustness is model-dependent} as Grok maintains 80-90\% at 30\% budget where OpenAI and DeepSeek collapse to 7-27\%. These results suggest incomplete reasoning chains actively mislead models, with implications for deploying reasoning-specialized systems under resource constraints.

</details>


### [203] [Selective Synchronization Attention](https://arxiv.org/abs/2602.14445)
*Hasi Hays*

Main category: cs.LG

TL;DR: 提出SSA注意力机制，基于Kuramoto耦合振荡器模型，用同步强度替代点积注意力，实现自然稀疏性、统一位置语义编码和单次闭式计算


<details>
  <summary>Details</summary>
Motivation: Transformer的自注意力机制存在二次计算复杂度问题，且缺乏生物学神经计算基础。需要更高效、更具生物学合理性的注意力机制

Method: 提出选择性同步注意力（SSA），将每个token表示为具有可学习自然频率和相位的振荡器，基于频率相关耦合和相位锁定条件计算同步强度作为注意力权重。构建振荡同步网络（OSN）作为Transformer块的替代

Result: SSA具有自然稀疏性（相位锁定阈值导致不兼容频率token自动获得零权重）、统一位置语义编码（自然频率谱）、单次闭式计算优势。同步矩阵分析显示初始化时即产生非均匀、头多样化的耦合模式

Conclusion: SSA基于振荡器同步理论，为Transformer提供了更高效、更具生物学合理性的注意力机制，具有更强的架构归纳偏置

Abstract: The Transformer architecture has become the foundation of modern deep learning, yet its core self-attention mechanism suffers from quadratic computational complexity and lacks grounding in biological neural computation. We propose Selective Synchronization Attention (SSA), a novel attention mechanism that replaces the standard dot-product self-attention with a closed-form operator derived from the steady-state solution of the Kuramoto model of coupled oscillators. In SSA, each token is represented as an oscillator characterized by a learnable natural frequency and phase; the synchronization strength between token pairs, determined by a frequency-dependent coupling and phase-locking condition, serves as the attention weight. This formulation provides three key advantages: (i) natural sparsity arising from the phase-locking threshold, whereby tokens with incompatible frequencies automatically receive zero attention weight without explicit masking; (ii) unified positional-semantic encoding through the natural frequency spectrum, eliminating the need for separate positional encodings; and (iii) a single-pass, closed-form computation that avoids iterative ODE integration, with all components (coupling, order parameter, synchronization) derived from the oscillatory framework. We instantiate SSA within the Oscillatory Synchronization Network (OSN), a drop-in replacement for the Transformer block. Analysis of the synchronization matrices reveals non-uniform, head-diverse coupling patterns even at initialization, demonstrating a stronger architectural inductive bias than the approximately uniform attention produced by randomly initialized Transformers.

</details>


### [204] [WiSparse: Boosting LLM Inference Efficiency with Weight-Aware Mixed Activation Sparsity](https://arxiv.org/abs/2602.14452)
*Lei Chen,Yuan Meng,Xiaoyu Zhan,Zhi Wang,Wenwu Zhu*

Main category: cs.LG

TL;DR: WiSparse是一种无需训练的激活稀疏化方法，通过结合激活和权重信息进行自适应稀疏分配，在保持模型性能的同时显著加速LLM推理。


<details>
  <summary>Details</summary>
Motivation: 现有无需训练的激活稀疏化方法仅依赖激活信息且使用统一稀疏率，忽略了权重的重要性和不同模块对稀疏化的敏感性差异，导致性能不理想。研究发现：1）不重要的激活可能对应重要的权重；2）不同模块对稀疏化的敏感性变化非单调。

Method: 提出WiSparse方法：1）权重感知机制，结合激活幅度和预计算的权重范数来准确识别重要通道；2）混合粒度分配方案，通过进化搜索全局预算分配到不同模块以保护敏感区域，然后在模块内细化以最小化重构误差；3）改进稀疏核。

Result: 在三个代表性模型上验证有效性。在50%稀疏率下，WiSparse保持Llama3.1密集模型97%的性能，比最强基线提升2.23个百分点，同时实现端到端推理速度21.4%的加速。

Conclusion: WiSparse推进了无需训练方法在高效LLM推理中的极限，在不训练的情况下突破了可实现的加速边界。

Abstract: Large Language Models (LLMs) offer strong capabilities but incur high inference costs due to dense computation and memory access. Training-free activation sparsity is a promising approach for efficient LLM inference, yet existing methods often rely solely on activation information and uniform sparsity ratios. This overlooks the critical interplay with weights and inter-block sensitivity variation, leading to suboptimal performance. We identify two key phenomena in modern LLMs: 1) less significant activations may align with highly important weights, and 2) sparsity sensitivity varies non-monotonically across model blocks. We propose Weight-aware Mixed-Granularity Training-free Activation Sparsity (WiSparse), which leverages both activation and weight information for adaptive sparsity allocation. Specifically, we introduce a weight-aware mechanism integrating activation magnitudes with precomputed weight norms to accurately identify salient channels. This is combined with a mixed-granularity allocation scheme: a global budget is distributed across blocks via evolutionary search to protect sensitive regions, then refined within blocks to minimize reconstruction error. We improve sparse kernels and demonstrate effectiveness on three representative models. Notably, at 50% sparsity, WiSparse preserves 97% of Llama3.1's dense performance, surpassing the strongest baseline by 2.23 percentage points while achieving a 21.4% acceleration in end-to-end inference speed. Our research advances the limits of training-free approaches for efficient LLM inference, pushing the boundaries of achievable speedup without training.

</details>


### [205] [Traceable Latent Variable Discovery Based on Multi-Agent Collaboration](https://arxiv.org/abs/2602.14456)
*Huaming Du,Tao Hu,Yijie Huang,Yu Zhao,Guisong Liu,Tao Gu,Gang Kou,Carl Yang*

Main category: cs.LG

TL;DR: TLVD是一个新颖的因果建模框架，结合大型语言模型（LLMs）的元数据推理能力和传统因果发现算法（TCDA）的数据驱动建模能力，用于推断潜在变量及其语义。


<details>
  <summary>Details</summary>
Motivation: 现实世界中揭示因果机制对科技进步至关重要。传统因果发现算法面临高质量数据缺乏、依赖无潜在混杂因子假设、忽视潜在变量精确语义等障碍，限制了因果发现的广泛应用。

Method: 1. 使用数据驱动方法构建包含潜在变量的因果图；2. 采用多LLM协作进行潜在变量推断，将此过程建模为不完全信息博弈，寻求贝叶斯纳什均衡来推断可能的特定潜在变量；3. 利用LLMs进行证据探索，在多源真实网络数据中验证推断的潜在变量以确保可追溯性。

Result: 在三个医院提供的去标识化真实患者数据集和两个基准数据集上全面评估TLVD。实验结果显示TLVD有效可靠，在五个数据集上平均提升：Acc 32.67%、CAcc 62.21%、ECit 26.72%。

Conclusion: TLVD框架成功整合了LLMs的元数据推理能力和TCDA的数据驱动建模，有效解决了传统因果发现算法在潜在变量推断方面的局限性，为因果发现提供了更可靠的方法。

Abstract: Revealing the underlying causal mechanisms in the real world is crucial for scientific and technological progress. Despite notable advances in recent decades, the lack of high-quality data and the reliance of traditional causal discovery algorithms (TCDA) on the assumption of no latent confounders, as well as their tendency to overlook the precise semantics of latent variables, have long been major obstacles to the broader application of causal discovery. To address this issue, we propose a novel causal modeling framework, TLVD, which integrates the metadata-based reasoning capabilities of large language models (LLMs) with the data-driven modeling capabilities of TCDA for inferring latent variables and their semantics. Specifically, we first employ a data-driven approach to construct a causal graph that incorporates latent variables. Then, we employ multi-LLM collaboration for latent variable inference, modeling this process as a game with incomplete information and seeking its Bayesian Nash Equilibrium (BNE) to infer the possible specific latent variables. Finally, to validate the inferred latent variables across multiple real-world web-based data sources, we leverage LLMs for evidence exploration to ensure traceability. We comprehensively evaluate TLVD on three de-identified real patient datasets provided by a hospital and two benchmark datasets. Extensive experimental results confirm the effectiveness and reliability of TLVD, with average improvements of 32.67% in Acc, 62.21% in CAcc, and 26.72% in ECit across the five datasets.

</details>


### [206] [Silent Inconsistency in Data-Parallel Full Fine-Tuning: Diagnosing Worker-Level Optimization Misalignment](https://arxiv.org/abs/2602.14462)
*Hong Li,Zhen Zhou,Honggang Zhang,Yuping Luo,Xinyue Wang,Han Gong,Zhiyuan Liu*

Main category: cs.LG

TL;DR: 该论文提出了一种轻量级诊断框架，用于检测数据并行训练中的"静默不一致性"问题，即尽管模型权重同步后数值相等，但各工作节点在梯度聚合前的优化动态可能存在差异。


<details>
  <summary>Details</summary>
Motivation: 数据并行训练中，虽然参数同步保证了每轮迭代后模型权重的数值等价性，但并不能保证梯度聚合前各工作节点的优化动态完全一致。这种潜在的差异被称为"静默不一致性"，在传统的聚合监控信号下难以察觉，可能导致训练不稳定。

Method: 提出一个轻量级、模型无关的诊断框架，使用三个互补指标量化工作节点一致性：损失离散度、梯度范数离散度、以及通过工作节点间余弦相似度测量的梯度方向一致性。这些指标无需修改模型架构、同步机制或优化算法，开销极小。

Result: 在8-NPU数据并行设置下，对1B参数的openPangu-Embedded-1B-V1.1模型进行全参数微调实验。结果显示，逐步去同步化的数据洗牌和随机种子会导致损失/梯度离散度显著增加和方向对齐度降低，尽管全局平均损失曲线看起来平滑。

Conclusion: 提出的指标能够有效揭示大规模数据并行微调中的隐藏不稳定模式，为诊断和配置评估提供了可操作的可见性，有助于提高训练可靠性。

Abstract: Data-parallel (DP) training with synchronous all-reduce is a dominant paradigm for full-parameter fine-tuning of large language models (LLMs). While parameter synchronization guarantees numerical equivalence of model weights after each iteration, it does not necessarily imply alignment of worker-level optimization dynamics before gradient aggregation. This paper identifies and studies this latent mismatch, termed \emph{silent inconsistency}, where cross-worker divergence in losses and gradients can remain invisible under conventional aggregated monitoring signals. We propose a lightweight, model-agnostic diagnostic framework that quantifies worker-level consistency using training signals readily available in standard pipelines. Specifically, we introduce three complementary metrics: loss dispersion, gradient-norm dispersion, and gradient-direction consistency measured by inter-worker cosine similarity. The proposed metrics incur negligible overhead and require no modification to model architecture, synchronization mechanisms, or optimization algorithms. We validate the framework by fully fine-tuning the 1B-parameter \texttt{openPangu-Embedded-1B-V1.1} model on the \texttt{tatsu-lab/alpaca} dataset using an 8-NPU DP setup, under controlled perturbations of cross-rank stochasticity. Experimental results show that progressively desynchronized data shuffling and random seeds lead to substantial increases in loss/gradient dispersion and reduced directional alignment, despite smooth globally averaged loss curves. These findings demonstrate that the proposed indicators provide actionable visibility into hidden instability modes in large-scale DP fine-tuning, enabling more reliable diagnosis and configuration assessment.

</details>


### [207] [LACONIC: Length-Aware Constrained Reinforcement Learning for LLM](https://arxiv.org/abs/2602.14468)
*Chang Liu,Yiran Zhao,Lawrence Liu,Yaoqi Ye,Csaba Szepesvári,Lin F. Yang*

Main category: cs.LG

TL;DR: LACONIC是一种强化学习方法，通过自适应长度成本控制，在保持任务性能的同时显著减少LLM输出长度


<details>
  <summary>Details</summary>
Motivation: 强化学习训练LLM时容易产生过长响应，增加推理延迟和计算开销。现有长度控制方法依赖固定启发式奖励调整，容易与任务目标不一致且需要脆弱调参

Method: 提出LACONIC方法，在训练时强制执行目标token预算。使用增强目标函数更新策略模型，结合任务奖励和基于长度的成本。通过自适应调整成本规模来平衡简洁性和任务性能

Result: 在数学推理模型和数据集上，LACONIC保持或提升pass@1性能，同时减少输出长度超过50%。在通用知识和多语言基准测试中保持域外性能，减少44%的token使用

Conclusion: LACONIC能有效控制LLM输出长度，保持任务性能，无需推理时修改且部署开销最小，提供了理论保证

Abstract: Reinforcement learning (RL) has enhanced the capabilities of large language models (LLMs) through reward-driven training. Nevertheless, this process can introduce excessively long responses, inflating inference latency and computational overhead. Prior length-control approaches typically rely on fixed heuristic reward shaping, which can misalign with the task objective and require brittle tuning. In this work, we propose LACONIC, a reinforcement learning method that enforces a target token budget during training. Specifically, we update policy models using an augmented objective that combines the task reward with a length-based cost. To balance brevity and task performance, the cost scale is adaptively adjusted throughout training. This yields robust length control while preserving task reward. We provide a theoretical guarantee that support the method. Across mathematical reasoning models and datasets, LACONIC preserves or improves pass@1 while reducing output length by over 50%. It maintains out-of-domain performance on general knowledge and multilingual benchmarks with 44% fewer tokens. Moreover, LACONIC integrates into standard RL-tuning with no inference changes and minimal deployment overhead.

</details>


### [208] [One Good Source is All You Need: Near-Optimal Regret for Bandits under Heterogeneous Noise](https://arxiv.org/abs/2602.14474)
*Aadirupa Saha,Amith Bhat,Haipeng Luo*

Main category: cs.LG

TL;DR: SOAR算法解决具有多个异质数据源的多臂老虎机问题，通过快速剪枝高方差源并平衡最小-最大LCB-UCB方法，在不知道最小方差源的情况下实现接近最优的遗憾界。


<details>
  <summary>Details</summary>
Motivation: 传统多臂老虎机问题假设单一数据源，但现实应用中常存在多个具有不同噪声方差的数据源。现有方法可能因使用高方差源而导致遗憾界恶化，需要设计能自适应选择最优数据源的算法。

Method: 提出SOAR算法：1) 使用尖锐方差集中界快速剪枝高方差数据源；2) 采用"平衡最小-最大LCB-UCB方法"同时识别最佳臂和最优（最小方差）数据源。

Result: SOAR实现实例依赖遗憾界为$\tilde{O}\left({σ^*}^2\sum_{i=2}^K \frac{\log T}{Δ_i} + \sqrt{K \sum_{j=1}^M σ_j^2}\right)$，其中${σ^*}^2$是最小源方差。相比基线方法（可能依赖最大方差），SOAR在不知道最小方差源的情况下达到接近最优性能。

Conclusion: SOAR算法能有效处理多源老虎机问题，在不知道最小方差源的情况下实现接近最优的遗憾界，显著优于传统基线方法，并在合成数据和真实数据集上验证了其优越性能。

Abstract: We study $K$-armed Multiarmed Bandit (MAB) problem with $M$ heterogeneous data sources, each exhibiting unknown and distinct noise variances $\{σ_j^2\}_{j=1}^M$. The learner's objective is standard MAB regret minimization, with the additional complexity of adaptively selecting which data source to query from at each round. We propose Source-Optimistic Adaptive Regret minimization (SOAR), a novel algorithm that quickly prunes high-variance sources using sharp variance-concentration bounds, followed by a `balanced min-max LCB-UCB approach' that seamlessly integrates the parallel tasks of identifying the best arm and the optimal (minimum-variance) data source. Our analysis shows SOAR achieves an instance-dependent regret bound of $\tilde{O}\left({σ^*}^2\sum_{i=2}^K \frac{\log T}{Δ_i} + \sqrt{K \sum_{j=1}^M σ_j^2}\right)$, up to preprocessing costs depending only on problem parameters, where ${σ^*}^2 := \min_j σ_j^2$ is the minimum source variance and $Δ_i$ denotes the suboptimality gap of the $i$-th arm. This result is both surprising as despite lacking prior knowledge of the minimum-variance source among $M$ alternatives, SOAR attains the optimal instance-dependent regret of standard single-source MAB with variance ${σ^*}^2$, while incurring only an small (and unavoidable) additive cost of $\tilde O(\sqrt{K \sum_{j=1}^M σ_j^2})$ towards the optimal (minimum variance) source identification. Our theoretical bounds represent a significant improvement over some proposed baselines, e.g. Uniform UCB or Explore-then-Commit UCB, which could potentially suffer regret scaling with $σ_{\max}^2$ in place of ${σ^*}^2$-a gap that can be arbitrarily large when $σ_{\max} \gg σ^*$. Experiments on multiple synthetic problem instances and the real-world MovieLens\;25M dataset, demonstrating the superior performance of SOAR over the baselines.

</details>


### [209] [Revisiting the Platonic Representation Hypothesis: An Aristotelian View](https://arxiv.org/abs/2602.14486)
*Fabian Gröger,Shuo Wen,Maria Brbić*

Main category: cs.LG

TL;DR: 本文提出了一种基于置换的零校准框架来校正神经网络表示相似性度量中的尺度偏差，并重新检验了柏拉图表示假说，发现校准后全局谱相似性消失，但局部邻域相似性保留，从而提出了亚里士多德表示假说。


<details>
  <summary>Details</summary>
Motivation: 现有衡量神经网络表示相似性的指标存在网络尺度混淆问题（模型深度或宽度增加会系统性夸大相似性分数），这影响了柏拉图表示假说的验证准确性。

Method: 引入基于置换的零校准框架，将任何表示相似性度量转换为具有统计保证的校准分数，以校正网络尺度效应。

Result: 校准后，全局谱相似性度量显示的收敛现象基本消失，但局部邻域相似性（而非局部距离）在不同模态间仍保持显著一致性。

Conclusion: 提出了亚里士多德表示假说：神经网络表示正在收敛到共享的局部邻域关系，而非全局统计模型。

Abstract: The Platonic Representation Hypothesis suggests that representations from neural networks are converging to a common statistical model of reality. We show that the existing metrics used to measure representational similarity are confounded by network scale: increasing model depth or width can systematically inflate representational similarity scores. To correct these effects, we introduce a permutation-based null-calibration framework that transforms any representational similarity metric into a calibrated score with statistical guarantees. We revisit the Platonic Representation Hypothesis with our calibration framework, which reveals a nuanced picture: the apparent convergence reported by global spectral measures largely disappears after calibration, while local neighborhood similarity, but not local distances, retains significant agreement across different modalities. Based on these findings, we propose the Aristotelian Representation Hypothesis: representations in neural networks are converging to shared local neighborhood relationships.

</details>


### [210] [Parameter-Efficient Fine-Tuning of LLMs with Mixture of Space Experts](https://arxiv.org/abs/2602.14490)
*Buze Zhang,Jinkai Tao,Zilang Zeng,Neil He,Ali Maatouk,Menglin Yang,Rex Ying*

Main category: cs.LG

TL;DR: 提出MoSLoRA框架，通过混合多种几何空间来增强LLM的参数高效微调能力，相比传统欧几里得空间的PEFT方法有显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调方法主要在欧几里得空间中操作，限制了捕捉语言数据复杂几何结构的能力。虽然其他几何空间（如双曲几何、球面流形）有理论优势，但强制使用单一流形类型会限制表达能力。

Method: 提出混合空间框架，利用多种几何空间同时学习更丰富的曲率感知表示。基于此开发MoSLoRA，扩展LoRA方法，引入异构几何专家，使模型能根据输入上下文动态选择或组合几何空间。还开发轻量级路由机制解决频繁流形切换的计算开销。

Result: 在多个基准测试中，MoSLoRA始终优于强基线方法，在MATH500上提升5.6%，在MAWPS上提升15.9%。还提供了曲率优化对训练稳定性和模型性能影响的实证见解。

Conclusion: 混合空间框架能有效提升LLM的参数高效微调性能，通过利用多种几何空间的优势来学习更丰富的表示，为下游任务适应提供了更强大的工具。

Abstract: Large Language Models (LLMs) have achieved remarkable progress, with Parameter-Efficient Fine-Tuning (PEFT) emerging as a key technique for downstream task adaptation. However, existing PEFT methods mainly operate in Euclidean space, fundamentally limiting their capacity to capture complex geometric structures inherent in language data. While alternative geometric spaces, like hyperbolic geometries for hierarchical data and spherical manifolds for circular patterns, offer theoretical advantages, forcing representations into a single manifold type ultimately limits expressiveness, even when curvature parameters are learnable. To address this, we propose Mixture of Space (MoS), a unified framework that leverages multiple geometric spaces simultaneously to learn richer, curvature-aware representations. Building on this scheme, we develop MoSLoRA, which extends Low-Rank Adaptation (LoRA) with heterogeneous geometric experts, enabling models to dynamically select or combine appropriate geometric spaces based on input context. Furthermore, to address the computational overhead of frequent manifold switching, we develop a lightweight routing mechanism. Moreover, we provide empirical insights into how curvature optimization impacts training stability and model performance. Our experiments across diverse benchmarks demonstrate that MoSLoRA consistently outperforms strong baselines, achieving up to 5.6% improvement on MATH500 and 15.9% on MAWPS.

</details>


### [211] [Divine Benevolence is an $x^2$: GLUs scale asymptotically faster than MLPs](https://arxiv.org/abs/2602.14495)
*Alejandro Francisco Queiruga*

Main category: cs.LG

TL;DR: 论文通过数值分析发现GLU架构具有x²项，使其在函数逼近中比MLP有更快的渐近缩放速度（L(P)∝P⁻³ vs P⁻²），并提出了具有更陡缩放斜率的Gated Quadratic Unit。


<details>
  <summary>Details</summary>
Motivation: 当前前沿LLM中广泛使用的GLU变体架构的成功主要被视为经验发现，缺乏理论解释。作者希望从数值分析的第一性原理出发，理解这些架构为何在缩放定律上表现优越。

Method: 应用数值分析工具，分析GLU架构的x²项如何实现二次逼近阶。通过参数构造和1D函数逼近问题的实证验证，比较GLU和MLP的缩放斜率L(P)。基于发现的第一性原理，提出新的Gated Quadratic Unit架构。

Result: 理论分析显示GLU的缩放斜率为L(P)∝P⁻³，而MLP仅为L(P)=P⁻²。实证验证了这些斜率在1D函数逼近问题上的表现。提出的Gated Quadratic Unit具有比GLU和MLP更陡的缩放斜率。

Conclusion: 从数值分析的第一性原理出发，可以解释GLU架构的优越缩放性能，并为架构设计提供理论指导。提出的Gated Quadratic Unit展示了基于数值理论设计架构以解锁大模型更优缩放性能的可能性。

Abstract: Scaling laws can be understood from ground-up numerical analysis, where traditional function approximation theory can explain shifts in model architecture choices. GLU variants now dominate frontier LLMs and similar outer-product architectures are prevalent in ranking models. The success of these architectures has mostly been left as an empirical discovery. In this paper, we apply the tools of numerical analysis to expose a key factor: these models have an $x^2$ which enables \emph{asymptotically} faster scaling than MLPs. GLUs have piecewise quadratic functional forms that are sufficient to exhibit quadratic order of approximation. Our key contribution is to demonstrate that the $L(P)$ scaling slope is $L(P)\propto P^{-3}$ for GLUs but only $L(P)=P^{-2}$ for MLPs on function reconstruction problems. We provide a parameter construction and empirical verification of these slopes for 1D function approximation. From the first principles we discover, we make one stride and propose the ``Gated Quadratic Unit'' which has an even steeper $L(P)$ slope than the GLU and MLP. This opens the possibility of architecture design from first principles numerical theory to unlock superior scaling in large models. Replication code is available at https://github.com/afqueiruga/divine_scaling.

</details>


### [212] [Covariance-Aware Transformers for Quadratic Programming and Decision Making](https://arxiv.org/abs/2602.14506)
*Kutay Tire,Yufan Zhang,Ege Onur Taga,Samet Oymak*

Main category: cs.LG

TL;DR: 本文证明Transformer能够求解二次规划问题，并利用这一能力提升时间序列基础模型在涉及协方差矩阵的决策问题（如投资组合优化）中的表现。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer如何利用二阶统计信息（协方差矩阵）来解决涉及二次规划的决策问题，特别是投资组合优化这类需要处理协方差矩阵的实际应用。

Method: 1. 理论证明线性注意力机制可通过逐行标记化矩阵变量来求解无约束二次规划；2. 结合MLP的Transformer块可求解ℓ₁惩罚和约束的二次规划；3. 提出Time2Decide方法，在时间序列基础模型中显式输入变量间的协方差矩阵。

Result: Time2Decide在投资组合优化问题上一致优于基础时间序列模型，并在适当设置下超越了传统的"预测-优化"两阶段方法，证明Transformer能有效利用二阶统计信息解决复杂决策问题。

Conclusion: Transformer能够有效利用二阶统计信息求解二次规划问题，这一能力使其能够单次前向传播就解决复杂的决策问题，为时间序列决策任务提供了新的有效方法。

Abstract: We explore the use of transformers for solving quadratic programs and how this capability benefits decision-making problems that involve covariance matrices. We first show that the linear attention mechanism can provably solve unconstrained QPs by tokenizing the matrix variables (e.g.~$A$ of the objective $\frac{1}{2}x^\top Ax+b^\top x$) row-by-row and emulating gradient descent iterations. Furthermore, by incorporating MLPs, a transformer block can solve (i) $\ell_1$-penalized QPs by emulating iterative soft-thresholding and (ii) $\ell_1$-constrained QPs when equipped with an additional feedback loop. Our theory motivates us to introduce Time2Decide: a generic method that enhances a time series foundation model (TSFM) by explicitly feeding the covariance matrix between the variates. We empirically find that Time2Decide uniformly outperforms the base TSFM model for the classical portfolio optimization problem that admits an $\ell_1$-constrained QP formulation. Remarkably, Time2Decide also outperforms the classical "Predict-then-Optimize (PtO)" procedure, where we first forecast the returns and then explicitly solve a constrained QP, in suitable settings. Our results demonstrate that transformers benefit from explicit use of second-order statistics, and this can enable them to effectively solve complex decision-making problems, like portfolio construction, in one forward pass.

</details>


### [213] [DeepMTL2R: A Library for Deep Multi-task Learning to Rank](https://arxiv.org/abs/2602.14519)
*Chaosheng Dong,Peiyao Xiao,Yijia Wang,Kaiyi Ji*

Main category: cs.LG

TL;DR: DeepMTL2R是一个用于多任务学习排序的开源深度学习框架，整合了21种先进的多任务学习算法，通过Transformer的自注意力机制统一处理异构相关性信号，支持多目标优化寻找帕累托最优排序模型。


<details>
  <summary>Details</summary>
Motivation: 现代排序系统需要同时优化多个相关性标准，这些标准可能相互冲突。现有方法难以有效整合异构相关性信号并处理目标间的复杂依赖关系，需要一个统一的框架来支持多任务学习排序。

Method: 基于Transformer架构的自注意力机制，构建上下文感知的统一模型，整合21种先进的多任务学习算法，支持多目标优化以识别帕累托最优排序模型，能够捕捉项目和标签间的复杂依赖关系和长距离交互。

Result: 在公开数据集上展示了有效性，报告了具有竞争力的性能，并可视化了目标间的权衡关系。框架具有可扩展性和表达性，便于在多任务学习策略间进行受控比较。

Conclusion: DeepMTL2R为现代排序系统提供了一个强大的开源框架，能够有效处理多任务学习排序问题，整合异构相关性信号，并在多个目标间实现优化权衡。

Abstract: This paper presents DeepMTL2R, an open-source deep learning framework for Multi-task Learning to Rank (MTL2R), where multiple relevance criteria must be optimized simultaneously. DeepMTL2R integrates heterogeneous relevance signals into a unified, context-aware model by leveraging the self-attention mechanism of transformer architectures, enabling effective learning across diverse and potentially conflicting objectives. The framework includes 21 state-of-the-art multi-task learning algorithms and supports multi-objective optimization to identify Pareto-optimal ranking models. By capturing complex dependencies and long-range interactions among items and labels, DeepMTL2R provides a scalable and expressive solution for modern ranking systems and facilitates controlled comparisons across MTL strategies. We demonstrate its effectiveness on a publicly available dataset, report competitive performance, and visualize the resulting trade-offs among objectives. DeepMTL2R is available at \href{https://github.com/amazon-science/DeepMTL2R}{https://github.com/amazon-science/DeepMTL2R}.

</details>


### [214] [Governing AI Forgetting: Auditing for Machine Unlearning Compliance](https://arxiv.org/abs/2602.14553)
*Qinqi Lin,Ningning Ding,Lingjie Duan,Jianwei Huang*

Main category: cs.LG

TL;DR: 提出首个机器遗忘合规审计的经济框架，结合认证遗忘理论与监管执行，通过博弈论模型分析审计者与运营者的策略互动，发现审计强度可随删除请求增加而降低，且未披露审计虽具信息优势但降低监管成本效益。


<details>
  <summary>Details</summary>
Motivation: 尽管法律规定了被遗忘权，但AI运营者经常未能遵守数据删除请求。机器遗忘虽提供技术解决方案，但技术可行性与监管实施之间存在根本差距，确保合规性仍然具有挑战性。需要建立经济框架来审计机器遗忘的合规性。

Method: 1) 使用认证遗忘的假设检验解释来表征机器遗忘固有的验证不确定性，推导审计者的检测能力；2) 提出博弈论模型捕捉审计者与运营者之间的策略互动；3) 将复杂的二元非线性定点问题转化为可处理的单变量辅助问题，解耦系统并建立均衡存在性、唯一性和结构性质。

Result: 反直觉地发现：随着删除请求增加，审计者可以最优地降低检查强度，因为运营者的弱化遗忘使不合规更容易检测（这与近期中国审计减少但删除请求增加的情况一致）。证明未披露审计虽为审计者提供信息优势，但相对于披露审计会降低监管成本效益。

Conclusion: 提出了首个机器遗忘合规审计的经济框架，解决了传统审计框架未处理的机器遗忘特定非线性问题。研究结果为监管机构提供了实用见解，特别是在平衡审计强度与检测能力方面，并揭示了不同审计策略的成本效益权衡。

Abstract: Despite legal mandates for the right to be forgotten, AI operators routinely fail to comply with data deletion requests. While machine unlearning (MU) provides a technical solution to remove personal data's influence from trained models, ensuring compliance remains challenging due to the fundamental gap between MU's technical feasibility and regulatory implementation. In this paper, we introduce the first economic framework for auditing MU compliance, by integrating certified unlearning theory with regulatory enforcement. We first characterize MU's inherent verification uncertainty using a hypothesis-testing interpretation of certified unlearning to derive the auditor's detection capability, and then propose a game-theoretic model to capture the strategic interactions between the auditor and the operator. A key technical challenge arises from MU-specific nonlinearities inherent in the model utility and the detection probability, which create complex strategic couplings that traditional auditing frameworks do not address and that also preclude closed-form solutions. We address this by transforming the complex bivariate nonlinear fixed-point problem into a tractable univariate auxiliary problem, enabling us to decouple the system and establish the equilibrium existence, uniqueness, and structural properties without relying on explicit solutions. Counterintuitively, our analysis reveals that the auditor can optimally reduce the inspection intensity as deletion requests increase, since the operator's weakened unlearning makes non-compliance easier to detect. This is consistent with recent auditing reductions in China despite growing deletion requests. Moreover, we prove that although undisclosed auditing offers informational advantages for the auditor, it paradoxically reduces the regulatory cost-effectiveness relative to disclosed auditing.

</details>


### [215] [Fluid-Agent Reinforcement Learning](https://arxiv.org/abs/2602.14559)
*Shishir Sharma,Doina Precup,Theodore J. Perkins*

Main category: cs.LG

TL;DR: 提出流体智能体环境框架，允许智能体动态创建其他智能体，解决现实世界中智能体数量不固定且未知的问题。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体强化学习(MARL)假设固定数量的智能体，但现实世界中智能体数量既不固定也非预先已知，且智能体可以创建其他智能体（如细胞分裂、公司分拆）。

Method: 提出流体智能体环境框架，允许智能体动态创建其他智能体；为流体智能体博弈提供博弈论解决方案概念；在流体变体的Predator-Prey和Level-Based Foraging基准以及新环境中评估多种MARL算法。

Result: 实验表明该框架能产生根据环境需求动态调整规模的智能体团队，流体性能够解锁固定群体设置中无法观察到的新颖解决方案策略。

Conclusion: 流体智能体环境框架扩展了MARL的能力，使智能体能够动态调整团队规模以适应环境需求，为现实世界应用提供了更灵活的多智能体系统。

Abstract: The primary focus of multi-agent reinforcement learning (MARL) has been to study interactions among a fixed number of agents embedded in an environment. However, in the real world, the number of agents is neither fixed nor known a priori. Moreover, an agent can decide to create other agents (for example, a cell may divide, or a company may spin off a division). In this paper, we propose a framework that allows agents to create other agents; we call this a fluid-agent environment. We present game-theoretic solution concepts for fluid-agent games and empirically evaluate the performance of several MARL algorithms within this framework. Our experiments include fluid variants of established benchmarks such as Predator-Prey and Level-Based Foraging, where agents can dynamically spawn, as well as a new environment we introduce that highlights how fluidity can unlock novel solution strategies beyond those observed in fixed-population settings. We demonstrate that this framework yields agent teams that adjust their size dynamically to match environmental demands.

</details>


### [216] [DCTracks: An Open Dataset for Machine Learning-Based Drift Chamber Track Reconstruction](https://arxiv.org/abs/2602.14571)
*Qian Liyan,Zhang Yao,Yuan Ye,Zhang Zhaoke,Fang Jin,Jiang Shimiao,Zhang Jin,Li Ke,Liu Beijiang,Xu Chenglin,Zhang Yifan,Jia Xiaoqian,Qin Xiaoshuai,Huang Xingtao*

Main category: cs.LG

TL;DR: 提出一个用于机器学习轨道重建的蒙特卡洛数据集，定义标准化评估指标，比较传统算法与图神经网络方法


<details>
  <summary>Details</summary>
Motivation: 为了推进基于机器学习的轨道重建研究，需要标准化的数据集和评估指标，以便进行可比较和可复现的验证

Method: 创建蒙特卡洛数据集包含单轨道和双轨道漂移室事件，定义轨道重建专用评估指标，比较传统重建算法与图神经网络方法

Result: 建立了标准化的评估框架，为未来研究提供了可复现的验证基准

Conclusion: 该工作为机器学习轨道重建研究提供了重要的数据集和评估标准，促进了该领域的标准化和可比较性

Abstract: We introduce a Monte Carlo (MC) dataset of single- and two-track drift chamber events to advance Machine Learning (ML)-based track reconstruction. To enable standardized and comparable evaluation, we define track reconstruction specific metrics and report results for traditional track reconstruction algorithms and a Graph Neural Networks (GNNs) method, facilitating rigorous, reproducible validation for future research.

</details>


### [217] [RNM-TD3: N:M Semi-structured Sparse Reinforcement Learning From Scratch](https://arxiv.org/abs/2602.14578)
*Isam Vrce,Andreas Kassler,Gökçe Aydos*

Main category: cs.LG

TL;DR: 本文首次研究RL中的N:M结构化稀疏性，提出RNM-TD3框架，在保持硬件加速兼容性的同时，在50%-75%稀疏度下性能优于稠密模型。


<details>
  <summary>Details</summary>
Motivation: 现有DRL稀疏化方法多为非结构化细粒度稀疏，限制了硬件加速机会；结构化粗粒度稀疏虽支持硬件加速，但通常性能下降且剪枝复杂。需要平衡压缩、性能和硬件效率的解决方案。

Method: 提出RNM-TD3框架，在off-policy RL（TD3）中为所有网络实施行级N:M稀疏化训练，保持与支持N:M稀疏矩阵运算的加速器兼容。

Result: 在连续控制基准测试中，RNM-TD3在50%-75%稀疏度（如2:4和1:4）下性能优于稠密模型，在Ant环境中2:4稀疏度下性能提升达14%。即使在87.5%稀疏度（1:8）下仍保持竞争力。

Conclusion: N:M结构化稀疏在RL中有效平衡了压缩、性能和硬件效率，为DRL模型的高效部署提供了有前景的解决方案。

Abstract: Sparsity is a well-studied technique for compressing deep neural networks (DNNs) without compromising performance. In deep reinforcement learning (DRL), neural networks with up to 5% of their original weights can still be trained with minimal performance loss compared to their dense counterparts. However, most existing methods rely on unstructured fine-grained sparsity, which limits hardware acceleration opportunities due to irregular computation patterns. Structured coarse-grained sparsity enables hardware acceleration, yet typically degrades performance and increases pruning complexity. In this work, we present, to the best of our knowledge, the first study on N:M structured sparsity in RL, which balances compression, performance, and hardware efficiency. Our framework enforces row-wise N:M sparsity throughout training for all networks in off-policy RL (TD3), maintaining compatibility with accelerators that support N:M sparse matrix operations. Experiments on continuous-control benchmarks show that RNM-TD3, our N:M sparse agent, outperforms its dense counterpart at 50%-75% sparsity (e.g., 2:4 and 1:4), achieving up to a 14% increase in performance at 2:4 sparsity on the Ant environment. RNM-TD3 remains competitive even at 87.5% sparsity (1:8), while enabling potential training speedups.

</details>


### [218] [OPBench: A Graph Benchmark to Combat the Opioid Crisis](https://arxiv.org/abs/2602.14602)
*Tianyi Ma,Yiyang Li,Yiyue Qian,Zheyuan Zhang,Zehong Wang,Chuxu Zhang,Yanfang Ye*

Main category: cs.LG

TL;DR: OPBench：首个全面的阿片类药物危机基准测试，包含5个数据集和3个关键应用领域，用于系统评估图学习方法在真实世界阿片危机场景中的表现。


<details>
  <summary>Details</summary>
Motivation: 阿片类药物危机全球肆虐，急需计算解决方案。图学习方法虽在建模复杂药物相关现象方面有前景，但缺乏系统评估这些方法在真实世界阿片危机场景中的综合基准。

Method: 1. 构建OPBench基准，包含5个数据集覆盖3个关键应用领域：医疗索赔中的阿片类药物过量检测、数字平台中的非法药物贩运检测、饮食模式中的药物滥用预测。2. 采用异构图和超图等多种图结构来保留药物相关数据中丰富复杂的关系信息。3. 与领域专家和权威机构合作，遵循隐私和伦理准则进行数据收集和标注。4. 建立统一评估框架，包括标准化协议、预定义数据分割和可复现基线。

Result: 通过大量实验分析了现有图学习方法的优势和局限性，为未来阿片危机研究提供了可行的见解。代码和数据集已开源。

Conclusion: OPBench填补了阿片类药物危机领域缺乏综合基准的空白，为系统评估图学习方法提供了标准化框架，有助于推动计算解决方案在应对阿片危机方面的研究进展。

Abstract: The opioid epidemic continues to ravage communities worldwide, straining healthcare systems, disrupting families, and demanding urgent computational solutions. To combat this lethal opioid crisis, graph learning methods have emerged as a promising paradigm for modeling complex drug-related phenomena. However, a significant gap remains: there is no comprehensive benchmark for systematically evaluating these methods across real-world opioid crisis scenarios. To bridge this gap, we introduce OPBench, the first comprehensive opioid benchmark comprising five datasets across three critical application domains: opioid overdose detection from healthcare claims, illicit drug trafficking detection from digital platforms, and drug misuse prediction from dietary patterns. Specifically, OPBench incorporates diverse graph structures, including heterogeneous graphs and hypergraphs, to preserve the rich and complex relational information among drug-related data. To address data scarcity, we collaborate with domain experts and authoritative institutions to curate and annotate datasets while adhering to privacy and ethical guidelines. Furthermore, we establish a unified evaluation framework with standardized protocols, predefined data splits, and reproducible baselines to facilitate fair and systematic comparison among graph learning methods. Through extensive experiments, we analyze the strengths and limitations of existing graph learning methods, thereby providing actionable insights for future research in combating the opioid crisis. Our source code and datasets are available at https://github.com/Tianyi-Billy-Ma/OPBench.

</details>


### [219] [Concepts' Information Bottleneck Models](https://arxiv.org/abs/2602.14626)
*Karim Galliamov,Syed M Ahsan Kazmi,Adil Khan,Adín Ramírez Rivera*

Main category: cs.LG

TL;DR: 该论文提出了一种信息瓶颈正则化方法，用于改进概念瓶颈模型（CBMs），通过在概念层施加信息瓶颈约束，减少概念泄露并提高预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统概念瓶颈模型（CBMs）虽然通过人类可理解的概念层提供可解释预测，但存在准确率下降和概念泄露问题，这削弱了模型的忠实性。需要一种方法来强制概念表示的最小充分性。

Method: 在概念层引入显式的信息瓶颈正则化器，惩罚I(X;C)同时保留任务相关信息I(C;Y)，鼓励最小充分的概念表示。提出了两种实用变体（变分目标和基于熵的替代方法），并将其集成到标准CBM训练中，无需架构更改或额外监督。

Result: 在六个CBM家族和三个基准测试中，信息瓶颈正则化模型始终优于其原始版本。信息平面分析进一步证实了预期行为。该方法提高了预测性能，并增强了概念级干预的可靠性。

Conclusion: 强制最小充分概念瓶颈不仅提高了预测性能，还增强了概念干预的可靠性。提出的正则化器为更忠实、更可干预的CBMs提供了一条理论基础、架构无关的路径，通过统一训练协议解决了先前评估不一致的问题。

Abstract: Concept Bottleneck Models (CBMs) aim to deliver interpretable predictions by routing decisions through a human-understandable concept layer, yet they often suffer reduced accuracy and concept leakage that undermines faithfulness. We introduce an explicit Information Bottleneck regularizer on the concept layer that penalizes $I(X;C)$ while preserving task-relevant information in $I(C;Y)$, encouraging minimal-sufficient concept representations. We derive two practical variants (a variational objective and an entropy-based surrogate) and integrate them into standard CBM training without architectural changes or additional supervision. Evaluated across six CBM families and three benchmarks, the IB-regularized models consistently outperform their vanilla counterparts. Information-plane analyses further corroborate the intended behavior. These results indicate that enforcing a minimal-sufficient concept bottleneck improves both predictive performance and the reliability of concept-level interventions. The proposed regularizer offers a theoretic-grounded, architecture-agnostic path to more faithful and intervenable CBMs, resolving prior evaluation inconsistencies by aligning training protocols and demonstrating robust gains across model families and datasets.

</details>


### [220] [Alignment Adapter to Improve the Performance of Compressed Deep Learning Models](https://arxiv.org/abs/2602.14635)
*Rohit Raj Rai,Abhishek Dhaka,Amit Awekar*

Main category: cs.LG

TL;DR: 提出Alignment Adapter (AlAd)轻量适配器，通过滑动窗口对齐压缩模型与原始大模型的token级嵌入，显著提升压缩模型性能，仅增加少量计算开销。


<details>
  <summary>Details</summary>
Motivation: 压缩深度学习模型在资源受限环境中部署至关重要，但其性能通常落后于大规模模型。需要一种方法来弥合这一性能差距。

Method: 提出基于滑动窗口的轻量适配器AlAd，对齐压缩模型与原始大模型的token级嵌入。该方法保持局部上下文语义，支持不同维度或架构的灵活对齐，且与底层压缩方法无关。可部署为冻结压缩模型的即插即用模块，或与压缩模型联合微调。

Result: 在BERT系列模型上的三个token级NLP任务实验中，AlAd显著提升了压缩模型的性能，仅带来微小的尺寸和延迟开销。

Conclusion: AlAd是一种有效的轻量适配器，能够显著提升压缩模型性能，同时保持低开销，为资源受限环境中的模型部署提供了实用解决方案。

Abstract: Compressed Deep Learning (DL) models are essential for deployment in resource-constrained environments. But their performance often lags behind their large-scale counterparts. To bridge this gap, we propose Alignment Adapter (AlAd): a lightweight, sliding-window-based adapter. It aligns the token-level embeddings of a compressed model with those of the original large model. AlAd preserves local contextual semantics, enables flexible alignment across differing dimensionalities or architectures, and is entirely agnostic to the underlying compression method. AlAd can be deployed in two ways: as a plug-and-play module over a frozen compressed model, or by jointly fine-tuning AlAd with the compressed model for further performance gains. Through experiments on BERT-family models across three token-level NLP tasks, we demonstrate that AlAd significantly boosts the performance of compressed models with only marginal overhead in size and latency.

</details>


### [221] [Pseudo-differential-enhanced physics-informed neural networks](https://arxiv.org/abs/2602.14663)
*Andrew Gracyk*

Main category: cs.LG

TL;DR: 提出伪微分增强物理信息神经网络(PINNs)，通过傅里叶空间中的梯度增强改进训练效果，提高学习精度和频率学习能力


<details>
  <summary>Details</summary>
Motivation: 传统梯度增强PINNs通过提高微分阶数来改进训练，但作者认为在傅里叶空间中执行类似操作更有效，因为傅里叶空间中的微分是乘以波数，这种方法能更好地处理高频成分并适用于分数阶导数

Method: 提出伪微分增强PINNs，在傅里叶空间中应用梯度增强：1) 将PDE残差转换到傅里叶空间；2) 通过乘以傅里叶波数实现微分增强；3) 将增强项添加到目标函数中；4) 支持蒙特卡洛方法实现网格灵活性，兼容傅里叶特征嵌入等先进技术

Result: 方法快速高效，在较少训练迭代中实现优于数值误差的PINN性能，在低配置点设置中能打破平台期，改善神经正切核(NTK)的谱特征值衰减，促进早期训练中的高频学习，缓解频率偏差问题

Conclusion: 伪微分增强PINNs在傅里叶空间中扩展了梯度增强概念，显著改进训练效率和精度，特别适用于分数阶导数和复杂几何域，为PINNs的高频学习提供了有效解决方案

Abstract: We present pseudo-differential enhanced physics-informed neural networks (PINNs), an extension of gradient enhancement but in Fourier space. Gradient enhancement of PINNs dictates that the PDE residual is taken to a higher differential order than prescribed by the PDE, added to the objective as an augmented term in order to improve training and overall learning fidelity. We propose the same procedure after application via Fourier transforms, since differentiating in Fourier space is multiplication with the Fourier wavenumber under suitable decay. Our methods are fast and efficient. Our methods oftentimes achieve superior PINN versus numerical error in fewer training iterations, potentially pair well with few samples in collocation, and can on occasion break plateaus in low collocation settings. Moreover, our methods are suitable for fractional derivatives. We establish that our methods improve spectral eigenvalue decay of the neural tangent kernel (NTK), and so our methods contribute towards the learning of high frequencies in early training, mitigating the effects of frequency bias up to the polynomial order and possibly greater with smooth activations. Our methods accommodate advanced techniques in PINNs, such as Fourier feature embeddings. A pitfall of discrete Fourier transforms via the Fast Fourier Transform (FFT) is mesh subjugation, and so we demonstrate compatibility of our methods for greater mesh flexibility and invariance on alternative Euclidean and non-Euclidean domains via Monte Carlo methods and otherwise.

</details>


### [222] [Learning State-Tracking from Code Using Linear RNNs](https://arxiv.org/abs/2602.14814)
*Julien Siems,Riccardo Grazzi,Kirill Kalinin,Hitesh Ballani,Babak Rahmani*

Main category: cs.LG

TL;DR: 论文将置换组合任务转换为代码形式的REPL跟踪，发现线性RNN在此设置下表现良好而Transformer失败，并研究了代码中状态跟踪困难的原因。


<details>
  <summary>Details</summary>
Motivation: 现有状态跟踪任务（如置换组合）通常是序列到序列的任务，与语言模型常用的下一个标记预测设置不兼容。需要将状态跟踪任务转换为更适合语言模型训练的形式。

Method: 将置换组合任务转换为代码形式的REPL跟踪，通过打印语句和变量转换交错显示状态。将状态跟踪问题形式化为具有确定性状态揭示的概率有限状态自动机跟踪问题。

Result: 线性RNN在代码表示的状态跟踪任务中表现良好，而Transformer仍然失败。在部分可观察的动作设置中，线性RNN可能比非线性RNN表现更差。

Conclusion: 代码表示使状态跟踪任务更适合语言模型训练，但动作的部分可观察性使状态跟踪变得困难，不同架构在此任务上的表现存在差异。

Abstract: Over the last years, state-tracking tasks, particularly permutation composition, have become a testbed to understand the limits of sequence models architectures like Transformers and RNNs (linear and non-linear). However, these are often sequence-to-sequence tasks: learning to map actions (permutations) to states, which is incompatible with the next-token prediction setting commonly used to train language models. We address this gap by converting permutation composition into code via REPL traces that interleave state-reveals through prints and variable transformations. We show that linear RNNs capable of state-tracking excel also in this setting, while Transformers still fail. Motivated by this representation, we investigate why tracking states in code is generally difficult: actions are not always fully observable. We frame this as tracking the state of a probabilistic finite-state automaton with deterministic state reveals and show that linear RNNs can be worse than non-linear RNNs at tracking states in this setup.

</details>


### [223] [Scaling Beyond Masked Diffusion Language Models](https://arxiv.org/abs/2602.15014)
*Subham Sekhar Sahoo,Jean-Marie Lemercier,Zhihan Yang,Justin Deschenaux,Jingyu Liu,John Thickstun,Ante Jukic*

Main category: cs.LG

TL;DR: 该研究挑战了Masked diffusion在扩散语言模型中的主导地位，发现perplexity指标在跨算法比较中存在误导性，uniform-state diffusion在推理任务上表现更优。


<details>
  <summary>Details</summary>
Motivation: 当前Masked diffusion在离散扩散方法中占据主导地位，主要基于其在语言建模基准上的优异perplexity表现。但研究者质疑perplexity是否足以进行跨算法比较，以及Masked diffusion是否真的是扩散语言模型的未来。

Method: 1) 对uniform-state和interpolating离散扩散方法进行首次缩放定律研究；2) 使用简单的交叉熵目标训练Masked diffusion模型，使其FLOPs效率提高约12%；3) 将所有方法扩展到17亿参数规模进行比较。

Result: 1) perplexity在扩散家族内部具有参考价值，但在跨家族比较中可能产生误导；2) 具有较差似然缩放特性的模型可能因采样更快更实用而更优；3) uniform-state diffusion在GSM8K推理任务上优于自回归和Masked diffusion模型，尽管验证perplexity较差。

Conclusion: Masked diffusion并非扩散语言模型的绝对未来，perplexity单独不足以进行跨算法比较。uniform-state diffusion在推理任务上表现出竞争力，速度-质量帕累托前沿是更全面的评估指标。

Abstract: Diffusion language models are a promising alternative to autoregressive models due to their potential for faster generation. Among discrete diffusion approaches, Masked diffusion currently dominates, largely driven by strong perplexity on language modeling benchmarks. In this work, we present the first scaling law study of uniform-state and interpolating discrete diffusion methods. We also show that Masked diffusion models can be made approximately 12% more FLOPs-efficient when trained with a simple cross-entropy objective. We find that perplexity is informative within a diffusion family but can be misleading across families, where models with worse likelihood scaling may be preferable due to faster and more practical sampling, as reflected by the speed-quality Pareto frontier. These results challenge the view that Masked diffusion is categorically the future of diffusion language modeling and that perplexity alone suffices for cross-algorithm comparison. Scaling all methods to 1.7B parameters, we show that uniform-state diffusion remains competitive on likelihood-based benchmarks and outperforms autoregressive and Masked diffusion models on GSM8K, despite worse validation perplexity. We provide the code, model checkpoints, and video tutorials on the project page: http://s-sahoo.github.io/scaling-dllms

</details>


### [224] [SynthSAEBench: Evaluating Sparse Autoencoders on Scalable Realistic Synthetic Data](https://arxiv.org/abs/2602.14687)
*David Chanin,Adrià Garriga-Alonso*

Main category: cs.LG

TL;DR: SynthSAEBench是一个用于评估稀疏自编码器（SAE）架构的工具包，通过生成具有真实特征特性的大规模合成数据，提供标准化基准模型，能够精确诊断SAE的失败模式并验证架构改进。


<details>
  <summary>Details</summary>
Motivation: 当前SAE基准测试存在两个问题：1）在LLM上的基准测试噪声太大，难以区分架构改进；2）合成数据实验规模太小且不真实，无法提供有意义的比较。需要更精确的基准来验证SAE架构创新。

Method: 开发了SynthSAEBench工具包，能够生成具有相关性、层次结构和叠加等真实特征特性的大规模合成数据。创建了标准化基准模型SynthSAEBench-16k，使不同SAE架构能够直接比较。

Result: 基准测试重现了多个先前观察到的LLM SAE现象，包括重建与潜在质量指标之间的脱节、SAE探测结果差、以及由L0调节的精度-召回权衡。还发现了一个新的失败模式：匹配追踪SAE利用叠加噪声来改善重建，而不学习真实特征。

Conclusion: SynthSAEBench通过提供真实特征和受控消融实验，补充了LLM基准测试，使研究人员能够在扩展到LLM之前精确诊断SAE失败模式并验证架构改进。

Abstract: Improving Sparse Autoencoders (SAEs) requires benchmarks that can precisely validate architectural innovations. However, current SAE benchmarks on LLMs are often too noisy to differentiate architectural improvements, and current synthetic data experiments are too small-scale and unrealistic to provide meaningful comparisons. We introduce SynthSAEBench, a toolkit for generating large-scale synthetic data with realistic feature characteristics including correlation, hierarchy, and superposition, and a standardized benchmark model, SynthSAEBench-16k, enabling direct comparison of SAE architectures. Our benchmark reproduces several previously observed LLM SAE phenomena, including the disconnect between reconstruction and latent quality metrics, poor SAE probing results, and a precision-recall trade-off mediated by L0. We further use our benchmark to identify a new failure mode: Matching Pursuit SAEs exploit superposition noise to improve reconstruction without learning ground-truth features, suggesting that more expressive encoders can easily overfit. SynthSAEBench complements LLM benchmarks by providing ground-truth features and controlled ablations, enabling researchers to precisely diagnose SAE failure modes and validate architectural improvements before scaling to LLMs.

</details>


### [225] [Symmetry in language statistics shapes the geometry of model representations](https://arxiv.org/abs/2602.15029)
*Dhruva Karkada,Daniel J. Korchinski,Andres Nava,Matthieu Wyart,Yasaman Bahri*

Main category: cs.LG

TL;DR: 论文证明语言统计中的平移对称性导致LLM表示中出现简单几何结构，这些结构对统计扰动具有鲁棒性，并可用连续潜变量解释。


<details>
  <summary>Details</summary>
Motivation: 尽管神经网络的表示学习取得了成功，但其基本性质仍不清楚。一个引人注目的现象是LLM表示中出现的简单几何结构（如月份形成圆形、年份形成一维流形），需要理解这些结构出现的根本原因。

Method: 1) 分析语言统计中的平移对称性（如月份共现概率仅取决于时间间隔）；2) 证明这种对称性控制高维词嵌入模型中的几何结构；3) 研究这些结构对统计扰动的鲁棒性；4) 提出连续潜变量理论框架解释鲁棒性；5) 在词嵌入模型、文本嵌入模型和LLM中进行实证验证。

Result: 1) 语言统计的平移对称性确实导致表示中出现简单几何结构；2) 这些结构对共现统计的强烈扰动（如删除所有包含两个月份的句子）具有鲁棒性；3) 在中等嵌入维度下结构依然存在；4) 连续潜变量理论能自然解释这种鲁棒性；5) 理论框架在多种模型中得到了实证验证。

Conclusion: 语言统计的平移对称性是LLM表示中出现简单几何结构的根本原因，这些结构的鲁棒性可通过连续潜变量理论解释，为理解神经网络表示的基本性质提供了理论框架。

Abstract: Although learned representations underlie neural networks' success, their fundamental properties remain poorly understood. A striking example is the emergence of simple geometric structures in LLM representations: for example, calendar months organize into a circle, years form a smooth one-dimensional manifold, and cities' latitudes and longitudes can be decoded by a linear probe. We show that the statistics of language exhibit a translation symmetry -- e.g., the co-occurrence probability of two months depends only on the time interval between them -- and we prove that the latter governs the aforementioned geometric structures in high-dimensional word embedding models. Moreover, we find that these structures persist even when the co-occurrence statistics are strongly perturbed (for example, by removing all sentences in which two months appear together) and at moderate embedding dimension. We show that this robustness naturally emerges if the co-occurrence statistics are collectively controlled by an underlying continuous latent variable. We empirically validate this theoretical framework in word embedding models, text embedding models, and large language models.

</details>


### [226] [A Critical Look at Targeted Instruction Selection: Disentangling What Matters (and What Doesn't)](https://arxiv.org/abs/2602.14696)
*Nihal V. Nayak,Paula Rodriguez-Diaz,Neha Hulkund,Sara Beery,David Alvarez-Melis*

Main category: cs.LG

TL;DR: 本文系统分析了指令微调中的数据选择方法，发现梯度表示结合贪心算法在低预算下表现最佳，但优势随预算增加而减弱。


<details>
  <summary>Details</summary>
Motivation: 当前指令选择方法研究碎片化且不透明，方法差异大、缺乏零样本基线、关键组件贡献混杂，导致实践者缺乏有效指导。

Method: 提出框架分离数据表示和选择算法两个核心要素，进行受控比较，将现有方法统一为近似距离最小化问题，并提供泛化边界理论支持。

Result: 只有基于梯度的数据表示能稳定预测性能；梯度表示+贪心轮询算法在低预算下平均表现最佳；大预算时优势减弱。

Conclusion: 为LLM微调提供了更原则化的数据选择基础，揭示了梯度表示的关键作用，统一了现有方法理论框架。

Abstract: Instruction fine-tuning of large language models (LLMs) often involves selecting a subset of instruction training data from a large candidate pool, using a small query set from the target task. Despite growing interest, the literature on targeted instruction selection remains fragmented and opaque: methods vary widely in selection budgets, often omit zero-shot baselines, and frequently entangle the contributions of key components. As a result, practitioners lack actionable guidance on selecting instructions for their target tasks. In this work, we aim to bring clarity to this landscape by disentangling and systematically analyzing the two core ingredients: data representation and selection algorithms. Our framework enables controlled comparisons across models, tasks, and budgets. We find that only gradient-based data representations choose subsets whose similarity to the query consistently predicts performance across datasets and models. While no single method dominates, gradient-based representations paired with a greedy round-robin selection algorithm tend to perform best on average at low budgets, but these benefits diminish at larger budgets. Finally, we unify several existing selection algorithms as forms of approximate distance minimization between the selected subset and the query set, and support this view with new generalization bounds. More broadly, our findings provide critical insights and a foundation for more principled data selection in LLM fine-tuning. The code is available at https://github.com/dcml-lab/targeted-instruction-selection.

</details>


### [227] [D2-LoRA: A Synergistic Approach to Differential and Directional Low-Rank Adaptation](https://arxiv.org/abs/2602.14728)
*Nozomu Fujisawa,Masaaki Kondo*

Main category: cs.LG

TL;DR: D2-LoRA：一种参数高效微调方法，通过带符号的低秩残差更新和列向投影，在保持推理时代数可合并性的同时，仅用少量数据和计算就能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 在有限数据和计算资源下，系统探索参数高效微调的设计空间，旨在开发一种既能保持推理效率（零延迟增加）又能提升性能的微调方法。

Method: 提出D2-LoRA方法，结合带符号的低秩残差更新（加法和减法组件）以及训练时的列向投影，保持每列接近原始范数。训练后将适配器合并为单一权重矩阵，实现零推理延迟。

Result: 在8个问答和阅读理解基准上平均准确率达到76.4%（仅用5k训练样本和2个epoch），比LoRA提升2.2个百分点；在匹配参数数量下提升1.6个百分点。在生成任务上也有提升，训练波动降低36%，推理吞吐量恢复约1.91倍。

Conclusion: D2-LoRA通过创新的架构设计而非增加参数化，在保持推理效率的同时显著提升了参数高效微调的性能，为实际应用提供了高效解决方案。

Abstract: We systematically investigate the parameter-efficient fine-tuning design space under practical data and compute constraints, and propose D2-LoRA. D2-LoRA achieves 76.4 percent average accuracy across eight question answering and reading comprehension benchmarks using only 5k training samples per task and two epochs, while preserving algebraic mergeability at inference with near-exact numerical equivalence. The method combines signed low-rank residual updates with additive and subtractive components, together with a train-time column-wise projection that keeps each column close to its original norm. After training, the adapter is merged into a single weight matrix, adding zero inference latency. Compared with LoRA, D2-LoRA improves average accuracy by 2.2 percentage points; at matched parameter counts (LoRA rank 2r versus D2-LoRA rank r), the improvement is 1.6 points, indicating gains from architectural design rather than increased parameterization. Compared with DoRA, it matches or exceeds performance on most tasks. Beyond QA and reading comprehension, D2-LoRA improves generative tasks (plus 1.2 ROUGE-L and plus 1.1 percent win rate) and shows 36 percent lower training volatility. The merge preserves numerical fidelity (mean gap about 0.03 percentage points) and recovers about 1.91x evaluation throughput. Training overhead is 19 percent, comparable to DoRA, and decreases with longer input sequences. We provide a geometric analysis explaining how the projection stabilizes training, together with ablation studies isolating the contribution of each design component.

</details>


### [228] [Scale redundancy and soft gauge fixing in positively homogeneous neural networks](https://arxiv.org/abs/2602.14729)
*Rodrigo Carmo Terin*

Main category: cs.LG

TL;DR: 该论文研究了具有正齐次激活函数的神经网络中的连续重参数化对称性，将其解释为规范冗余，并引入规范适应坐标和软轨道选择功能来改善优化稳定性。


<details>
  <summary>Details</summary>
Motivation: 神经网络中正齐次激活函数存在神经元尺度重标度的连续对称性，这种对称性导致参数空间中的轨道，沿这些轨道输入-输出函数保持不变。作者希望将这种对称性解释为规范冗余，并利用规范理论的概念来改善优化过程。

Method: 1. 引入规范适应坐标，分离不变方向和尺度不平衡方向；2. 受场论中规范固定的启发，引入软轨道选择（范数平衡）功能，仅作用于冗余的尺度坐标；3. 通过分析证明该功能诱导不平衡模式的耗散松弛，从而保持实现的函数不变。

Result: 在受控实验中，轨道选择惩罚扩展了稳定学习率的范围，抑制了尺度漂移，同时不改变表达能力。这建立了规范轨道几何与优化条件之间的结构联系。

Conclusion: 该研究建立了规范轨道几何与优化条件之间的结构联系，为规范理论概念与机器学习之间提供了具体连接。通过利用神经网络中的规范冗余，可以改善优化稳定性而不牺牲表达能力。

Abstract: Neural networks with positively homogeneous activations exhibit an exact continuous reparametrization symmetry: neuron-wise rescalings generate parameter-space orbits along which the input--output function is invariant. We interpret this symmetry as a gauge redundancy and introduce gauge-adapted coordinates that separate invariant and scale-imbalance directions. Inspired by gauge fixing in field theory, we introduce a soft orbit-selection (norm-balancing) functional acting only on redundant scale coordinates. We show analytically that it induces dissipative relaxation of imbalance modes to preserve the realized function. In controlled experiments, this orbit-selection penalty expands the stable learning-rate regime and suppresses scale drift without changing expressivity. These results establish a structural link between gauge-orbit geometry and optimization conditioning, providing a concrete connection between gauge-theoretic concepts and machine learning.

</details>


### [229] [Parameter-Minimal Neural DE Solvers via Horner Polynomials](https://arxiv.org/abs/2602.14737)
*T. Matulić,D. Seršić*

Main category: cs.LG

TL;DR: 提出一种参数极少的神经网络架构，通过将假设类限制为Horner分解多项式来求解微分方程，得到隐式、可微的试解，仅需少量可学习系数。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络求解微分方程通常需要大量参数，本文旨在开发参数极少的架构，实现资源高效的科学建模。

Method: 1. 将假设类限制为Horner分解多项式，构建隐式可微试解；2. 通过固定低阶多项式自由度精确满足初始条件；3. 引入分段（类样条）扩展，在子区间训练多个小型Horner模型，同时在分段边界强制连续性（和一阶导数连续性）。

Result: 在ODE基准测试和热方程示例中，仅需数十个参数的Horner网络能准确匹配解及其导数，在相同训练设置下优于小型MLP和正弦表示基线。

Conclusion: Horner网络展示了实用的精度-参数权衡，为资源高效的科学建模提供了有效方法。

Abstract: We propose a parameter-minimal neural architecture for solving differential equations by restricting the hypothesis class to Horner-factorized polynomials, yielding an implicit, differentiable trial solution with only a small set of learnable coefficients. Initial conditions are enforced exactly by construction by fixing the low-order polynomial degrees of freedom, so training focuses solely on matching the differential-equation residual at collocation points. To reduce approximation error without abandoning the low-parameter regime, we introduce a piecewise ("spline-like") extension that trains multiple small Horner models on subintervals while enforcing continuity (and first-derivative continuity) at segment boundaries. On illustrative ODE benchmarks and a heat-equation example, Horner networks with tens (or fewer) parameters accurately match the solution and its derivatives and outperform small MLP and sinusoidal-representation baselines under the same training settings, demonstrating a practical accuracy-parameter trade-off for resource-efficient scientific modeling.

</details>


### [230] [Inner Loop Inference for Pretrained Transformers: Unlocking Latent Capabilities Without Training](https://arxiv.org/abs/2602.14759)
*Jonathan Lys,Vincent Gripon,Bastien Pasdeloup,Lukas Mauch,Fabien Cardinaux,Ghouthi Boukli Hacene*

Main category: cs.LG

TL;DR: 在预训练语言模型中引入推理时内部循环，通过重复应用选定块范围来延长细化过程，实现精度提升


<details>
  <summary>Details</summary>
Motivation: 基于Transformer架构中内部表示可视为潜在表示的迭代细化这一观察，以及早期解码和细化层假设，探索通过简单测试时循环获得额外细化的可能性

Method: 提出推理时内部循环方法，在冻结的预训练模型中，通过重复应用选定的Transformer块范围来延长细化过程

Result: 在多个基准测试中，内部循环带来了适度但一致的精度提升，潜在轨迹分析显示更稳定的状态演化和持续的语义细化

Conclusion: 通过简单的测试时循环可以在冻结的预训练模型中获得额外的细化，扩展计算而不需要重新训练

Abstract: Deep Learning architectures, and in particular Transformers, are conventionally viewed as a composition of layers. These layers are actually often obtained as the sum of two contributions: a residual path that copies the input and the output of a Transformer block. As a consequence, the inner representations (i.e. the input of these blocks) can be interpreted as iterative refinement of a propagated latent representation. Under this lens, many works suggest that the inner space is shared across layers, meaning that tokens can be decoded at early stages. Mechanistic interpretability even goes further by conjecturing that some layers act as refinement layers. Following this path, we propose inference-time inner looping, which prolongs refinement in pretrained off-the-shelf language models by repeatedly re-applying a selected block range. Across multiple benchmarks, inner looping yields modest but consistent accuracy improvements. Analyses of the resulting latent trajectories suggest more stable state evolution and continued semantic refinement. Overall, our results suggest that additional refinement can be obtained through simple test-time looping, extending computation in frozen pretrained models.

</details>


### [231] [Universal Algorithm-Implicit Learning](https://arxiv.org/abs/2602.14761)
*Stefano Woerner,Seong Joon Oh,Christian F. Baumgartner*

Main category: cs.LG

TL;DR: TAIL是一个基于Transformer的算法隐式元学习器，能够在不同领域、模态和标签配置的任务中工作，实现了跨模态泛化并显著提升了计算效率。


<details>
  <summary>Details</summary>
Motivation: 当前元学习方法局限于狭窄的任务分布和固定的特征/标签空间，且文献中"通用"等术语使用不一致、缺乏精确定义，限制了方法的可比性和应用范围。

Method: 提出了元学习理论框架，定义了实用通用性，区分算法显式和隐式学习；在此基础上开发了TAIL，包含三个创新：跨模态特征编码的随机投影、可外推到更大标签空间的随机注入标签嵌入、高效的内联查询处理。

Result: TAIL在标准少样本基准上达到最先进性能，能泛化到未见领域和模态（如仅用图像训练却能解决文本分类任务），处理训练时未见过的20倍类别数的任务，相比之前基于Transformer的方法实现数量级计算节省。

Conclusion: 该工作通过理论框架和TAIL实现，为通用元学习提供了原则性词汇和实用方法，解决了现有方法的局限性，实现了真正的跨模态和跨规模泛化能力。

Abstract: Current meta-learning methods are constrained to narrow task distributions with fixed feature and label spaces, limiting applicability. Moreover, the current meta-learning literature uses key terms like "universal" and "general-purpose" inconsistently and lacks precise definitions, hindering comparability. We introduce a theoretical framework for meta-learning which formally defines practical universality and introduces a distinction between algorithm-explicit and algorithm-implicit learning, providing a principled vocabulary for reasoning about universal meta-learning methods. Guided by this framework, we present TAIL, a transformer-based algorithm-implicit meta-learner that functions across tasks with varying domains, modalities, and label configurations. TAIL features three innovations over prior transformer-based meta-learners: random projections for cross-modal feature encoding, random injection label embeddings that extrapolate to larger label spaces, and efficient inline query processing. TAIL achieves state-of-the-art performance on standard few-shot benchmarks while generalizing to unseen domains. Unlike other meta-learning methods, it also generalizes to unseen modalities, solving text classification tasks despite training exclusively on images, handles tasks with up to 20$\times$ more classes than seen during training, and provides orders-of-magnitude computational savings over prior transformer-based approaches.

</details>


### [232] [Learning Structural Hardness for Combinatorial Auctions: Instance-Dependent Algorithm Selection via Graph Neural Networks](https://arxiv.org/abs/2602.14772)
*Sungwoo Kang*

Main category: cs.LG

TL;DR: 论文提出一种机器学习方法预测组合拍卖中贪心算法的困难实例，通过分类器识别"鲸鱼-小鱼"陷阱结构，并选择性部署GNN专家求解器，实现混合分配器


<details>
  <summary>Details</summary>
Motivation: 组合拍卖的胜者确定问题(WDP)是NP难问题，现有方法无法可靠预测哪些实例会击败快速贪心启发式算法。当前ML-for-combinatorial-optimization社区专注于学习替代求解器，但GNN在标准基准上很少优于经典方法

Method: 设计20维结构特征向量，训练轻量级MLP困难度分类器预测贪心最优性差距；对识别为困难的实例（具有"鲸鱼-小鱼"陷阱结构）部署异构GNN专家求解器；构建混合分配器结合分类器、GNN和贪心求解器

Result: 困难度分类器预测贪心最优性差距的MAE为0.033，Pearson相关系数0.937，二元分类准确率94.7%；GNN专家在6个对抗配置上实现≈0%最优性差距（贪心为3.75-59.24%）；混合分配器在混合分布上实现0.51%总体差距

Conclusion: 学习何时部署昂贵求解器比学习替代它们更可行；算法选择框架比直接替代求解器更有效；GNN在CATS基准上不优于Gurobi（0.45-0.71 vs. 0.20差距），支持算法选择框架的合理性

Abstract: The Winner Determination Problem (WDP) in combinatorial auctions is NP-hard, and no existing method reliably predicts which instances will defeat fast greedy heuristics. The ML-for-combinatorial-optimization community has focused on learning to \emph{replace} solvers, yet recent evidence shows that graph neural networks (GNNs) rarely outperform well-tuned classical methods on standard benchmarks. We pursue a different objective: learning to predict \emph{when} a given instance is hard for greedy allocation, enabling instance-dependent algorithm selection. We design a 20-dimensional structural feature vector and train a lightweight MLP hardness classifier that predicts the greedy optimality gap with mean absolute error 0.033, Pearson correlation 0.937, and binary classification accuracy 94.7\% across three random seeds. For instances identified as hard -- those exhibiting ``whale-fish'' trap structure where greedy provably fails -- we deploy a heterogeneous GNN specialist that achieves ${\approx}0\%$ optimality gap on all six adversarial configurations tested (vs.\ 3.75--59.24\% for greedy). A hybrid allocator combining the hardness classifier with GNN and greedy solvers achieves 0.51\% overall gap on mixed distributions. Our honest evaluation on CATS benchmarks confirms that GNNs do not outperform Gurobi (0.45--0.71 vs.\ 0.20 gap), motivating the algorithm selection framing. Learning \emph{when} to deploy expensive solvers is more tractable than learning to replace them.

</details>


### [233] [Interactionless Inverse Reinforcement Learning: A Data-Centric Framework for Durable Alignment](https://arxiv.org/abs/2602.14844)
*Elias Malomgré,Pieter Simoens*

Main category: cs.LG

TL;DR: 提出Interactionless Inverse Reinforcement Learning和Alignment Flywheel，将AI对齐从一次性消耗品转变为可检查、可编辑、模型无关的工程资产


<details>
  <summary>Details</summary>
Motivation: 当前AI对齐方法存在结构性缺陷，将安全目标与智能体策略纠缠在一起，产生了不透明、一次性的对齐产物（Alignment Waste），需要更可持续、可验证的对齐方案

Method: 1. Interactionless Inverse Reinforcement Learning：将对齐产物学习与策略优化解耦，生成可检查、可编辑、模型无关的奖励模型；2. Alignment Flywheel：人机协同生命周期，通过自动化审计和精化迭代强化奖励模型

Result: 将安全从一次性消耗品转变为持久、可验证的工程资产，解决了Alignment Waste问题，提供了更透明、可复用的对齐方案

Conclusion: 通过解耦对齐产物学习和策略优化，结合人机协同迭代机制，可以建立更可持续、可检查、可编辑的AI对齐架构，将安全转变为可积累的工程资产而非一次性消耗

Abstract: AI alignment is growing in importance, yet current approaches suffer from a critical structural flaw that entangles the safety objectives with the agent's policy. Methods such as Reinforcement Learning from Human Feedback and Direct Preference Optimization create opaque, single-use alignment artifacts, which we term Alignment Waste. We propose Interactionless Inverse Reinforcement Learning to decouple alignment artifact learning from policy optimization, producing an inspectable, editable, and model-agnostic reward model. Additionally, we introduce the Alignment Flywheel, a human-in-the-loop lifecycle that iteratively hardens the reward model through automated audits and refinement. This architecture transforms safety from a disposable expense into a durable, verifiable engineering asset.

</details>


### [234] [Atomix: Timely, Transactional Tool Use for Reliable Agentic Workflows](https://arxiv.org/abs/2602.14849)
*Bardia Mohammadi,Nearchos Potamitis,Lars Klein,Akhil Arora,Laurent Bindschaedler*

Main category: cs.LG

TL;DR: Atomix为LLM代理工具调用提供进度感知的事务语义，通过epoch标记、资源边界跟踪和进度谓词控制提交，支持延迟缓冲效果和失败补偿，提高任务成功率并增强隔离性。


<details>
  <summary>Details</summary>
Motivation: LLM代理在外部系统上执行操作时，工具效果是立即生效的。在失败、推测执行或资源竞争情况下，被放弃的分支可能泄漏意外的副作用，且无法安全回滚。需要一种机制来确保工具调用的原子性和隔离性。

Method: Atomix运行时为代理工具调用提供进度感知的事务语义：1) 为每个调用标记epoch；2) 跟踪每个资源的边界；3) 仅当进度谓词指示安全时才提交；4) 可缓冲的效果可以延迟执行；5) 外部化效果被跟踪并在中止时补偿。

Result: 在实际工作负载中进行故障注入测试：1) 事务重试提高了任务成功率；2) 边界门控提交在推测执行和资源竞争情况下增强了隔离性。

Conclusion: Atomix为LLM代理工具调用提供了有效的事务管理机制，解决了副作用泄漏和回滚问题，提高了系统的可靠性和隔离性，特别适用于存在失败、推测执行和资源竞争的场景。

Abstract: LLM agents increasingly act on external systems, yet tool effects are immediate. Under failures, speculation, or contention, losing branches can leak unintended side effects with no safe rollback. We introduce Atomix, a runtime that provides progress-aware transactional semantics for agent tool calls. Atomix tags each call with an epoch, tracks per-resource frontiers, and commits only when progress predicates indicate safety; bufferable effects can be delayed, while externalized effects are tracked and compensated on abort. Across real workloads with fault injection, transactional retry improves task success, while frontier-gated commit strengthens isolation under speculation and contention.

</details>


### [235] [BEACONS: Bounded-Error, Algebraically-Composable Neural Solvers for Partial Differential Equations](https://arxiv.org/abs/2602.14853)
*Jonathan Gorard,Ammar Hakim,James Juno*

Main category: cs.LG

TL;DR: 提出BEACONS框架，通过形式化验证的神经网络求解PDE，即使在训练域外也能保证正确性，克服传统神经网络外推不可靠的问题。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络在训练数据凸包外泛化不可靠，这在计算物理中是个严重问题，因为经常需要在远超实验或解析验证的区域内求解PDE。

Method: 1) 使用特征线法预测PDE解的解析性质，构建浅层神经网络近似的严格外推误差界；2) 将PDE解分解为简单函数组合，基于组合深度学习思想构建深层架构，抑制大L^inf误差；3) 开发BEACONS框架，包括神经网络求解器自动代码生成器和定制化自动定理证明系统。

Result: 将BEACONS应用于多种线性和非线性PDE（线性对流方程、无粘性Burgers方程、完全可压缩Euler方程），在1D和2D情况下都能可靠且有界地外推解到训练数据之外。

Conclusion: BEACONS框架通过形式化验证的神经网络求解器，提供了可靠的外推能力，相比传统PINN方法具有多种优势，为计算物理中PDE求解提供了可验证的正确性保证。

Abstract: The traditional limitations of neural networks in reliably generalizing beyond the convex hulls of their training data present a significant problem for computational physics, in which one often wishes to solve PDEs in regimes far beyond anything which can be experimentally or analytically validated. In this paper, we show how it is possible to circumvent these limitations by constructing formally-verified neural network solvers for PDEs, with rigorous convergence, stability, and conservation properties, whose correctness can therefore be guaranteed even in extrapolatory regimes. By using the method of characteristics to predict the analytical properties of PDE solutions a priori (even in regions arbitrarily far from the training domain), we show how it is possible to construct rigorous extrapolatory bounds on the worst-case L^inf errors of shallow neural network approximations. Then, by decomposing PDE solutions into compositions of simpler functions, we show how it is possible to compose these shallow neural networks together to form deep architectures, based on ideas from compositional deep learning, in which the large L^inf errors in the approximations have been suppressed. The resulting framework, called BEACONS (Bounded-Error, Algebraically-COmposable Neural Solvers), comprises both an automatic code-generator for the neural solvers themselves, as well as a bespoke automated theorem-proving system for producing machine-checkable certificates of correctness. We apply the framework to a variety of linear and non-linear PDEs, including the linear advection and inviscid Burgers' equations, as well as the full compressible Euler equations, in both 1D and 2D, and illustrate how BEACONS architectures are able to extrapolate solutions far beyond the training data in a reliable and bounded way. Various advantages of the approach over the classical PINN approach are discussed.

</details>


### [236] [A Pragmatic Method for Comparing Clusterings with Overlaps and Outliers](https://arxiv.org/abs/2602.14855)
*Ryan DeWolfe,Paweł Prałat,François Théberge*

Main category: cs.LG

TL;DR: 提出了一种用于比较包含重叠聚类和异常值的聚类结果的相似性度量方法


<details>
  <summary>Details</summary>
Motivation: 当前聚类比较方法无法处理包含异常值（不属于任何簇的对象）和重叠聚类（对象可能属于多个簇）的情况，而实际应用中这些情况很常见

Method: 定义了一种实用的相似性度量方法，用于比较包含重叠和异常值的聚类结果，并证明该方法具有多个理想性质

Result: 该方法不受其他聚类比较度量常见的多种偏差影响，通过实验验证了其有效性

Conclusion: 提出的相似性度量方法填补了聚类比较领域的重要空白，为包含异常值和重叠聚类的聚类算法评估提供了实用工具

Abstract: Clustering algorithms are an essential part of the unsupervised data science ecosystem, and extrinsic evaluation of clustering algorithms requires a method for comparing the detected clustering to a ground truth clustering. In a general setting, the detected and ground truth clusterings may have outliers (objects belonging to no cluster), overlapping clusters (objects may belong to more than one cluster), or both, but methods for comparing these clusterings are currently undeveloped. In this note, we define a pragmatic similarity measure for comparing clusterings with overlaps and outliers, show that it has several desirable properties, and experimentally confirm that it is not subject to several common biases afflicting other clustering comparison measures.

</details>


### [237] [Goldilocks RL: Tuning Task Difficulty to Escape Sparse Rewards for Reasoning](https://arxiv.org/abs/2602.14868)
*Ilia Mahrooghi,Aryo Lotfi,Emmanuel Abbe*

Main category: cs.LG

TL;DR: Goldilocks是一种教师驱动的数据采样策略，通过预测每个问题对特定学生模型的难度，选择"恰到好处"（既不太容易也不太困难）的问题进行训练，从而提高强化学习在大型语言模型中的样本效率。


<details>
  <summary>Details</summary>
Motivation: 强化学习虽然能解锁大型语言模型的推理能力，但依赖稀疏奖励导致样本效率低下。传统的课程学习通过按复杂度排序数据来缓解这一问题，但针对特定模型的最佳排序往往不明确。

Method: 提出Goldilocks策略：教师模型预测每个问题对学生模型的难度，选择符合"Goldilocks原则"（难度适中）的问题。教师根据学生在已见样本上的表现，持续适应学生不断发展的能力，同时学生使用GRPO进行训练。

Result: 在OpenMathReasoning数据集上，Goldilocks数据采样在相同计算预算下，相比标准GRPO训练提高了模型性能。

Conclusion: Goldilocks通过动态适应学生能力的教师驱动数据采样，有效提高了强化学习训练大型语言模型的样本效率，为解决稀疏奖励下的搜索空间导航问题提供了新思路。

Abstract: Reinforcement learning has emerged as a powerful paradigm for unlocking reasoning capabilities in large language models. However, relying on sparse rewards makes this process highly sample-inefficient, as models must navigate vast search spaces with minimal feedback. While classic curriculum learning aims to mitigate this by ordering data based on complexity, the right ordering for a specific model is often unclear. To address this, we propose Goldilocks, a novel teacher-driven data sampling strategy that aims to predict each question's difficulty for the student model. The teacher model selects questions of appropriate difficulty for the student model, i.e., questions that are neither too easy nor too hard (Goldilocks principle), while training the student with GRPO. By leveraging the student's performance on seen samples, the teacher continuously adapts to the student's evolving abilities. On OpenMathReasoning dataset, Goldilocks data sampling improves the performance of models trained with standard GRPO under the same compute budget.

</details>


### [238] [Web-Scale Multimodal Summarization using CLIP-Based Semantic Alignment](https://arxiv.org/abs/2602.14889)
*Mounvik K,N Harshit*

Main category: cs.LG

TL;DR: Web-Scale Multimodal Summarization：一个轻量级框架，通过结合从网络检索的文本和图像数据来生成摘要。系统通过并行网络、新闻和图像搜索获取内容，使用微调CLIP模型对图像进行语义对齐排序，支持可调节参数和结构化输出。


<details>
  <summary>Details</summary>
Motivation: 需要一种能够从网络规模的多模态数据（文本和图像）中生成摘要的工具，以帮助用户快速理解和整合特定主题的相关信息，同时保持多模态内容之间的语义一致性。

Method: 1. 给定用户定义主题，并行执行网络、新闻和图像搜索；2. 使用微调CLIP模型对检索到的图像进行排序，衡量其与主题和文本的语义对齐；3. 可选BLIP字幕生成，支持纯图像摘要以增强多模态连贯性；4. 提供可调节的获取限制、语义过滤、摘要样式和结构化输出下载功能；5. 通过Gradio API暴露系统，支持可控参数和预设配置。

Result: 在500个图像-字幕对（包含20:1对比负样本）的评估中，系统取得了ROC-AUC 0.9270、F1分数0.6504和准确率96.99%的成绩，显示出强大的多模态对齐能力。

Conclusion: 该工作提供了一个可配置、可部署的网络规模摘要工具，将语言、检索和视觉模型集成到用户可扩展的管道中，为多模态信息整合提供了实用解决方案。

Abstract: We introduce Web-Scale Multimodal Summarization, a lightweight framework for generating summaries by combining retrieved text and image data from web sources. Given a user-defined topic, the system performs parallel web, news, and image searches. Retrieved images are ranked using a fine-tuned CLIP model to measure semantic alignment with topic and text. Optional BLIP captioning enables image-only summaries for stronger multimodal coherence.The pipeline supports features such as adjustable fetch limits, semantic filtering, summary styling, and downloading structured outputs. We expose the system via a Gradio-based API with controllable parameters and preconfigured presets.Evaluation on 500 image-caption pairs with 20:1 contrastive negatives yields a ROC-AUC of 0.9270, an F1-score of 0.6504, and an accuracy of 96.99%, demonstrating strong multimodal alignment. This work provides a configurable, deployable tool for web-scale summarization that integrates language, retrieval, and vision models in a user-extensible pipeline.

</details>


### [239] [Algorithmic Simplification of Neural Networks with Mosaic-of-Motifs](https://arxiv.org/abs/2602.14896)
*Pedram Bakhtiarifard,Tong Chen,Jonathan Wenshøj,Erik B Dam,Raghavendra Selvan*

Main category: cs.LG

TL;DR: 该论文从算法复杂度角度解释深度神经网络为何适合压缩，提出基于可重用模块的马赛克方法（MoMos）来降低模型参数复杂度。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络经过训练后参数具有结构性和重复模式，相比随机初始化具有更低的算法复杂度，这解释了为什么模型压缩方法（如剪枝、量化、知识蒸馏）能够有效工作。

Method: 提出MoMos方法：将参数划分为大小为s的块，每个块从k个可重用模块中选择，形成马赛克模式。通过这种约束参数化来降低Kolmogorov复杂度。

Result: 实验表明，神经网络在训练过程中算法复杂度（通过Kolmogorov复杂度近似测量）确实会降低。MoMos方法产生的模型在性能相当的情况下，算法复杂度更低。

Conclusion: 深度神经网络适合压缩的原因是训练后的参数具有结构性和更低的算法复杂度。MoMos方法通过约束参数化为可重用模块的模式，能够有效降低模型复杂度同时保持性能。

Abstract: Large-scale deep learning models are well-suited for compression. Methods like pruning, quantization, and knowledge distillation have been used to achieve massive reductions in the number of model parameters, with marginal performance drops across a variety of architectures and tasks. This raises the central question: \emph{Why are deep neural networks suited for compression?} In this work, we take up the perspective of algorithmic complexity to explain this behavior. We hypothesize that the parameters of trained models have more structure and, hence, exhibit lower algorithmic complexity compared to the weights at (random) initialization. Furthermore, that model compression methods harness this reduced algorithmic complexity to compress models. Although an unconstrained parameterization of model weights, $\mathbf{w} \in \mathbb{R}^n$, can represent arbitrary weight assignments, the solutions found during training exhibit repeatability and structure, making them algorithmically simpler than a generic program. To this end, we formalize the Kolmogorov complexity of $\mathbf{w}$ by $\mathcal{K}(\mathbf{w})$. We introduce a constrained parameterization $\widehat{\mathbf{w}}$, that partitions parameters into blocks of size $s$, and restricts each block to be selected from a set of $k$ reusable motifs, specified by a reuse pattern (or mosaic). The resulting method, $\textit{Mosaic-of-Motifs}$ (MoMos), yields algorithmically simpler model parameterization compared to unconstrained models. Empirical evidence from multiple experiments shows that the algorithmic complexity of neural networks, measured using approximations to Kolmogorov complexity, can be reduced during training. This results in models that perform comparably with unconstrained models while being algorithmically simpler.

</details>


### [240] [Picking the Right Specialist: Attentive Neural Process-based Selection of Task-Specialized Models as Tools for Agentic Healthcare Systems](https://arxiv.org/abs/2602.14901)
*Pramit Saha,Joshua Strong,Mohammad Alsharid,Divyanshu Mishra,J. Alison Noble*

Main category: cs.LG

TL;DR: ToolSelect：通过注意力神经过程选择器，自适应学习从异构专家模型池中选择最适合特定查询的模型，在胸部X光多任务环境中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在医疗代理系统中，单个任务通常需要多个专家模型协同工作，因为不同模型在不同数据样本上表现各异。现有方法缺乏可靠的模型选择机制，无法为特定查询从异构模型池中选出最优专家模型。

Method: 提出ToolSelect框架，通过最小化基于任务条件选择损失的一致代理函数来学习模型选择。使用注意力神经过程选择器，该选择器以查询和每个模型的行为摘要为条件，从专家模型池中选择最合适的模型。

Result: 在ToolSelectBench基准测试中（包含1448个查询和55个专家模型），ToolSelect在四个不同任务家族上一致优于10种最先进的方法。

Conclusion: ToolSelect为医疗代理系统中的模型选择问题提供了有效的解决方案，能够自适应地为特定查询选择最合适的专家模型，显著提升系统性能。

Abstract: Task-specialized models form the backbone of agentic healthcare systems, enabling the agents to answer clinical queries across tasks such as disease diagnosis, localization, and report generation. Yet, for a given task, a single "best" model rarely exists. In practice, each task is better served by multiple competing specialist models where different models excel on different data samples. As a result, for any given query, agents must reliably select the right specialist model from a heterogeneous pool of tool candidates. To this end, we introduce ToolSelect, which adaptively learns model selection for tools by minimizing a population risk over sampled specialist tool candidates using a consistent surrogate of the task-conditional selection loss. Concretely, we propose an Attentive Neural Process-based selector conditioned on the query and per-model behavioral summaries to choose among the specialist models. Motivated by the absence of any established testbed, we, for the first time, introduce an agentic Chest X-ray environment equipped with a diverse suite of task-specialized models (17 disease detection, 19 report generation, 6 visual grounding, and 13 VQA) and develop ToolSelectBench, a benchmark of 1448 queries. Our results demonstrate that ToolSelect consistently outperforms 10 SOTA methods across four different task families.

</details>


### [241] [Coverage Guarantees for Pseudo-Calibrated Conformal Prediction under Distribution Shift](https://arxiv.org/abs/2602.14913)
*Farbod Siahkali,Ashwin Verma,Vijay Gupta*

Main category: cs.LG

TL;DR: 本文提出一种伪校准方法，用于在分布偏移下保持保形预测的覆盖保证，通过引入松弛参数和源调谐算法来缓解覆盖退化问题。


<details>
  <summary>Details</summary>
Motivation: 保形预测在数据分布偏移时可能失效，需要开发能够应对标签条件协变量偏移的方法来维持目标域的覆盖保证。

Method: 1) 使用领域自适应工具推导目标覆盖的下界；2) 设计伪校准集，通过松弛参数膨胀保形阈值；3) 提出源调谐伪校准算法，根据分类器不确定性在硬伪标签和随机标签之间插值。

Result: 理论分析显示边界能够定性跟踪伪校准行为，源调谐方案在分布偏移下有效缓解覆盖退化，同时保持非平凡的预测集大小。

Conclusion: 伪校准是应对分布偏移的有效工具，源调谐算法在维持覆盖保证和预测集效率之间取得了良好平衡。

Abstract: Conformal prediction (CP) offers distribution-free marginal coverage guarantees under an exchangeability assumption, but these guarantees can fail if the data distribution shifts. We analyze the use of pseudo-calibration as a tool to counter this performance loss under a bounded label-conditional covariate shift model. Using tools from domain adaptation, we derive a lower bound on target coverage in terms of the source-domain loss of the classifier and a Wasserstein measure of the shift. Using this result, we provide a method to design pseudo-calibrated sets that inflate the conformal threshold by a slack parameter to keep target coverage above a prescribed level. Finally, we propose a source-tuned pseudo-calibration algorithm that interpolates between hard pseudo-labels and randomized labels as a function of classifier uncertainty. Numerical experiments show that our bounds qualitatively track pseudo-calibration behavior and that the source-tuned scheme mitigates coverage degradation under distribution shift while maintaining nontrivial prediction set sizes.

</details>


### [242] [Additive Control Variates Dominate Self-Normalisation in Off-Policy Evaluation](https://arxiv.org/abs/2602.14914)
*Olivier Jeunen,Shashank Gupta*

Main category: cs.LG

TL;DR: 本文证明在离策略评估中，使用最优加性基线（β*-IPS）的估计器在均方误差上渐近优于自归一化逆倾向得分（SNIPS），为推荐系统评估从自归一化转向最优基线修正提供了理论依据。


<details>
  <summary>Details</summary>
Motivation: 离策略评估对于评估排名和推荐系统至关重要，但现有方法如SNIPS虽然通过乘性控制变量减少方差，而加性控制变量（基线修正）在离策略学习中表现出更好性能，但缺乏理论保证。本文旨在填补这一理论空白。

Method: 提出β*-IPS估计器，使用最优加性基线，通过理论分析证明其渐近优于SNIPS。通过解析分解方差差距，展示SNIPS等价于使用特定但通常次优的加性基线。

Result: 证明β*-IPS在均方误差上渐近主导SNIPS，为从自归一化转向最优基线修正提供了理论依据，适用于排名和推荐系统的离策略评估。

Conclusion: 研究结果为离策略评估提供了明确的理论指导：应优先采用最优加性基线修正而非自归一化方法，这为排名和推荐系统的评估实践提供了理论支持。

Abstract: Off-policy evaluation (OPE) is essential for assessing ranking and recommendation systems without costly online interventions. Self-Normalised Inverse Propensity Scoring (SNIPS) is a standard tool for variance reduction in OPE, leveraging a multiplicative control variate. Recent advances in off-policy learning suggest that additive control variates (baseline corrections) may offer superior performance, yet theoretical guarantees for evaluation are lacking. This paper provides a definitive answer: we prove that $β^\star$-IPS, an estimator with an optimal additive baseline, asymptotically dominates SNIPS in Mean Squared Error. By analytically decomposing the variance gap, we show that SNIPS is asymptotically equivalent to using a specific -- but generally sub-optimal -- additive baseline. Our results theoretically justify shifting from self-normalisation to optimal baseline corrections for both ranking and recommendation.

</details>


### [243] [BHyGNN+: Unsupervised Representation Learning for Heterophilic Hypergraphs](https://arxiv.org/abs/2602.14919)
*Tianyi Ma,Yiyue Qian,Zehong Wang,Zheyuan Zhang,Chuxu Zhang,Yanfang Ye*

Main category: cs.LG

TL;DR: BHyGNN+是一个自监督学习框架，通过超图对偶性在无需标签的情况下学习异质性超图的表示，无需负样本且性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有超图神经网络在异质性超图上性能下降，且依赖标注数据，而现实场景中标注稀缺且成本高，需要无监督解决方案。

Method: 基于超图对偶性（节点和超边角色互换），通过对比超图与其对偶的增强视图，使用余弦相似度捕获结构模式，无需负样本。

Result: 在11个基准数据集上的实验表明，BHyGNN+在异质性和同质性超图上均优于最先进的监督和自监督基线方法。

Conclusion: 利用超图对偶性进行自监督学习是有效的，为挑战性无标签超图的表示学习建立了新范式。

Abstract: Hypergraph Neural Networks (HyGNNs) have demonstrated remarkable success in modeling higher-order relationships among entities. However, their performance often degrades on heterophilic hypergraphs, where nodes connected by the same hyperedge tend to have dissimilar semantic representations or belong to different classes. While several HyGNNs, including our prior work BHyGNN, have been proposed to address heterophily, their reliance on labeled data significantly limits their applicability in real-world scenarios where annotations are scarce or costly. To overcome this limitation, we introduce BHyGNN+, a self-supervised learning framework that extends BHyGNN for representation learning on heterophilic hypergraphs without requiring ground-truth labels. The core idea of BHyGNN+ is hypergraph duality, a structural transformation where the roles of nodes and hyperedges are interchanged. By contrasting augmented views of a hypergraph against its dual using cosine similarity, our framework captures essential structural patterns in a fully unsupervised manner. Notably, this duality-based formulation eliminates the need for negative samples, a common requirement in existing hypergraph contrastive learning methods that is often difficult to satisfy in practice. Extensive experiments on eleven benchmark datasets demonstrate that BHyGNN+ consistently outperforms state-of-the-art supervised and self-supervised baselines on both heterophilic and homophilic hypergraphs. Our results validate the effectiveness of leveraging hypergraph duality for self-supervised learning and establish a new paradigm for representation learning on challenging, unlabeled hypergraphs.

</details>


### [244] [Use What You Know: Causal Foundation Models with Partial Graphs](https://arxiv.org/abs/2602.14972)
*Arik Reuter,Anish Dhir,Cristiana Diaconu,Jake Robertson,Ole Ossen,Frank Hutter,Adrian Weller,Mark van der Wilk,Bernhard Schölkopf*

Main category: cs.LG

TL;DR: 该论文提出了一种在因果基础模型中融入领域知识的方法，通过将因果图或祖先信息等因果信息作为条件输入，使通用CFM能够达到针对特定因果结构训练的专用模型的性能水平。


<details>
  <summary>Details</summary>
Motivation: 当前因果基础模型（CFMs）无法融入领域知识，导致预测效果不理想。传统因果估计方法需要针对特定假设定制估计器，而CFMs虽然统一了因果发现和推断，但缺乏利用领域专业知识的能力。

Method: 提出在CFMs中融入因果信息的方法：1）将因果图或祖先信息作为条件输入；2）当完整因果图不可得时，有效利用部分因果信息；3）通过向注意力机制注入可学习的偏置来实施条件化策略。

Result: 实验表明，注意力机制注入可学习偏置是最有效的条件化方法。经过条件化的通用CFM能够达到针对特定因果结构训练的专用模型的性能水平，特别是在利用部分因果信息时表现优异。

Conclusion: 该研究解决了因果基础模型发展中的关键障碍：在数据驱动回答因果查询的同时，有效利用任意数量的领域专业知识，为实现一体化因果基础模型铺平了道路。

Abstract: Estimating causal quantities traditionally relies on bespoke estimators tailored to specific assumptions. Recently proposed Causal Foundation Models (CFMs) promise a more unified approach by amortising causal discovery and inference in a single step. However, in their current state, they do not allow for the incorporation of any domain knowledge, which can lead to suboptimal predictions. We bridge this gap by introducing methods to condition CFMs on causal information, such as the causal graph or more readily available ancestral information. When access to complete causal graph information is too strict a requirement, our approach also effectively leverages partial causal information. We systematically evaluate conditioning strategies and find that injecting learnable biases into the attention mechanism is the most effective method to utilise full and partial causal information. Our experiments show that this conditioning allows a general-purpose CFM to match the performance of specialised models trained on specific causal structures. Overall, our approach addresses a central hurdle on the path towards all-in-one causal foundation models: the capability to answer causal queries in a data-driven manner while effectively leveraging any amount of domain expertise.

</details>


### [245] [MacroGuide: Topological Guidance for Macrocycle Generation](https://arxiv.org/abs/2602.14977)
*Alicja Maksymiuk,Alexandre Duplessis,Michael Bronstein,Alexander Tong,Fernanda Duarte,İsmail İlkan Ceylan*

Main category: cs.LG

TL;DR: MacroGuide是一种使用持久同调引导扩散模型生成大环分子的新方法，将大环生成率从1%提升到99%


<details>
  <summary>Details</summary>
Motivation: 大环分子因其对困难靶点具有增强的选择性和结合亲和力而成为有前景的药物替代品，但由于公共数据集中稀缺且标准深度生成模型难以强制执行拓扑约束，在生成建模中仍未得到充分探索

Method: MacroGuide：一种扩散引导机制，使用持久同调指导预训练分子生成模型的采样过程。在每个去噪步骤中，从原子位置构建Vietoris-Rips复形，并通过优化持久同调特征促进环形成

Result: 将MacroGuide应用于预训练扩散模型后，大环生成率从1%提高到99%，同时在化学有效性、多样性和PoseBusters检查等关键质量指标上匹配或超越最先进性能

Conclusion: MacroGuide通过拓扑引导有效解决了大环分子生成中的挑战，为药物发现中的大环设计提供了强大的生成工具

Abstract: Macrocycles are ring-shaped molecules that offer a promising alternative to small-molecule drugs due to their enhanced selectivity and binding affinity against difficult targets. Despite their chemical value, they remain underexplored in generative modeling, likely owing to their scarcity in public datasets and the challenges of enforcing topological constraints in standard deep generative models. We introduce MacroGuide: Topological Guidance for Macrocycle Generation, a diffusion guidance mechanism that uses Persistent Homology to steer the sampling of pretrained molecular generative models toward the generation of macrocycles, in both unconditional and conditional (protein pocket) settings. At each denoising step, MacroGuide constructs a Vietoris-Rips complex from atomic positions and promotes ring formation by optimizing persistent homology features. Empirically, applying MacroGuide to pretrained diffusion models increases macrocycle generation rates from 1% to 99%, while matching or exceeding state-of-the-art performance on key quality metrics such as chemical validity, diversity, and PoseBusters checks.

</details>


### [246] [Orthogonalized Multimodal Contrastive Learning with Asymmetric Masking for Structured Representations](https://arxiv.org/abs/2602.14983)
*Carolin Cissee,Raneen Younis,Zahra Ahmadi*

Main category: cs.LG

TL;DR: COrAL是一个多模态学习框架，通过正交约束和不对称掩码技术，同时保留冗余、独特和协同信息，实现更稳定全面的表征学习。


<details>
  <summary>Details</summary>
Motivation: 现有自监督多模态对比学习方法主要捕捉冗余的跨模态信号，忽略了模态特定信息和交互驱动的协同信息，导致表征不完整和潜在信息泄露。

Method: 采用双路径架构配合正交约束来解耦共享和模态特定特征；引入具有互补视图特定模式的不对称掩码技术，强制模型推断跨模态依赖关系而非仅依赖冗余线索。

Result: 在合成基准和多样化MultiBench数据集上的实验表明，COrAL始终匹配或优于最先进方法，同时在不同运行中表现出低性能方差。

Conclusion: 显式建模完整的多模态信息谱系能够产生更稳定、可靠和全面的嵌入表示。

Abstract: Multimodal learning seeks to integrate information from heterogeneous sources, where signals may be shared across modalities, specific to individual modalities, or emerge only through their interaction. While self-supervised multimodal contrastive learning has achieved remarkable progress, most existing methods predominantly capture redundant cross-modal signals, often neglecting modality-specific (unique) and interaction-driven (synergistic) information. Recent extensions broaden this perspective, yet they either fail to explicitly model synergistic interactions or learn different information components in an entangled manner, leading to incomplete representations and potential information leakage. We introduce \textbf{COrAL}, a principled framework that explicitly and simultaneously preserves redundant, unique, and synergistic information within multimodal representations. COrAL employs a dual-path architecture with orthogonality constraints to disentangle shared and modality-specific features, ensuring a clean separation of information components. To promote synergy modeling, we introduce asymmetric masking with complementary view-specific patterns, compelling the model to infer cross-modal dependencies rather than rely solely on redundant cues. Extensive experiments on synthetic benchmarks and diverse MultiBench datasets demonstrate that COrAL consistently matches or outperforms state-of-the-art methods while exhibiting low performance variance across runs. These results indicate that explicitly modeling the full spectrum of multimodal information yields more stable, reliable, and comprehensive embeddings.

</details>


### [247] [Spectral Convolution on Orbifolds for Geometric Deep Learning](https://arxiv.org/abs/2602.14997)
*Tim Mangliers,Bernhard Mössner,Benjamin Himpel*

Main category: cs.LG

TL;DR: 论文将几何深度学习扩展到轨道流形，引入谱卷积作为轨道流形结构化数据的构建模块，并用音乐理论示例说明。


<details>
  <summary>Details</summary>
Motivation: 应用数据需求推动机器学习需要处理超越欧几里得结构的数据，如具有图或流形结构的数据，需要识别更多拓扑和几何结构来使这些用例可被机器学习访问。

Method: 引入轨道流形上的谱卷积概念，作为在轨道流形结构化数据上进行几何深度学习的基本构建模块。

Result: 建立了轨道流形上的谱卷积理论，为轨道流形结构化数据的机器学习提供了基础框架。

Conclusion: 轨道流形上的谱卷积扩展了几何深度学习的应用范围，使轨道流形结构化数据能够被机器学习访问，并通过音乐理论示例验证了该理论的实用性。

Abstract: Geometric deep learning (GDL) deals with supervised learning on data domains that go beyond Euclidean structure, such as data with graph or manifold structure. Due to the demand that arises from application-related data, there is a need to identify further topological and geometric structures with which these use cases can be made accessible to machine learning. There are various techniques, such as spectral convolution, that form the basic building blocks for some convolutional neural network-like architectures on non-Euclidean data. In this paper, the concept of spectral convolution on orbifolds is introduced. This provides a building block for making learning on orbifold structured data accessible using GDL. The theory discussed is illustrated using an example from music theory.

</details>


### [248] [Boundary Point Jailbreaking of Black-Box LLMs](https://arxiv.org/abs/2602.15001)
*Xander Davies,Giorgi Giglemiani,Edmund Lau,Eric Winsor,Geoffrey Irving,Yarin Gal*

Main category: cs.LG

TL;DR: 提出了一种名为边界点越狱（BPJ）的新型自动化越狱攻击方法，能够绕过最强的行业部署安全防护系统，仅使用单比特信息（分类器是否标记）即可实现黑盒攻击。


<details>
  <summary>Details</summary>
Motivation: 当前前沿LLM通过"越狱"防护系统抵御有害信息提取，防御者已开发出能够经受数千小时人工红队测试的分类器系统。需要一种能够绕过这些最强防护的自动化攻击方法。

Method: BPJ将目标有害字符串转化为中间攻击目标的课程，然后主动选择最能检测攻击强度微小变化的评估点（边界点）。该方法完全黑盒，仅使用分类器是否标记的单比特信息，不依赖白盒/灰盒假设或现有越狱库。

Result: BPJ是首个成功开发针对宪法分类器的通用越狱的完全自动化攻击算法，也是首个在不依赖人类攻击种子情况下成功攻击GPT-5输入分类器的自动化算法。

Conclusion: BPJ在单个交互中难以防御，但在优化过程中会产生许多标记，表明有效防御需要补充单交互方法为批量级监控。

Abstract: Frontier LLMs are safeguarded against attempts to extract harmful information via adversarial prompts known as "jailbreaks". Recently, defenders have developed classifier-based systems that have survived thousands of hours of human red teaming. We introduce Boundary Point Jailbreaking (BPJ), a new class of automated jailbreak attacks that evade the strongest industry-deployed safeguards. Unlike previous attacks that rely on white/grey-box assumptions (such as classifier scores or gradients) or libraries of existing jailbreaks, BPJ is fully black-box and uses only a single bit of information per query: whether or not the classifier flags the interaction. To achieve this, BPJ addresses the core difficulty in optimising attacks against robust real-world defences: evaluating whether a proposed modification to an attack is an improvement. Instead of directly trying to learn an attack for a target harmful string, BPJ converts the string into a curriculum of intermediate attack targets and then actively selects evaluation points that best detect small changes in attack strength ("boundary points"). We believe BPJ is the first fully automated attack algorithm that succeeds in developing universal jailbreaks against Constitutional Classifiers, as well as the first automated attack algorithm that succeeds against GPT-5's input classifier without relying on human attack seeds. BPJ is difficult to defend against in individual interactions but incurs many flags during optimisation, suggesting that effective defence requires supplementing single-interaction methods with batch-level monitoring.

</details>


### [249] [PDE foundation models are skillful AI weather emulators for the Martian atmosphere](https://arxiv.org/abs/2602.15004)
*Johannes Schmude,Sujit Roy,Liping Wang,Theodore van Kessel,Levente Klein,Marcus Freitag,Eloisa Bentivegna,Robert Manson-Sawko,Bjorn Lutjens,Manil Maskey,Campbell Watson,Rahul Ramachandran,Juan Bernabe-Moreno*

Main category: cs.LG

TL;DR: 该研究展示了预训练在偏微分方程数值解上的AI基础模型可以适应和微调，成为火星大气层的高性能天气预测模拟器，通过扩展Poseidon模型到三维并利用稀疏初始条件，在有限计算资源下实现了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 研究动机是利用预训练的偏微分方程基础模型来解决现实世界问题，特别是火星大气预测这种训练数据有限、计算资源受限的复杂系统。传统方法需要大量数据和计算资源，而基础模型可以通过迁移学习在有限资源下获得良好性能。

Method: 方法基于Poseidon偏微分方程基础模型，将其从二维扩展到三维系统，同时保留预训练信息。研究还探索了模型在稀疏初始条件下的性能。使用四个火星年（约34GB）的训练数据和13个GPU小时的中位数计算预算。

Result: 结果显示，预训练和模型扩展的组合在保留测试年上带来了34.4%的性能提升。这表明偏微分方程基础模型不仅能近似其他偏微分方程的解，还能作为现实世界复杂系统（特别是训练数据不足或计算预算有限的情况）的锚定模型。

Conclusion: 结论是偏微分方程基础模型可以成功应用于现实世界的复杂系统预测，如火星大气模拟。通过预训练和适当的模型扩展，即使在训练数据有限和计算资源受限的情况下，也能获得显著的性能提升，为类似问题提供了有效的解决方案。

Abstract: We show that AI foundation models that are pretrained on numerical solutions to a diverse corpus of partial differential equations can be adapted and fine-tuned to obtain skillful predictive weather emulators for the Martian atmosphere. We base our work on the Poseidon PDE foundation model for two-dimensional systems. We develop a method to extend Poseidon from two to three dimensions while keeping the pretraining information. Moreover, we investigate the performance of the model in the presence of sparse initial conditions. Our results make use of four Martian years (approx.~34 GB) of training data and a median compute budget of 13 GPU hours. We find that the combination of pretraining and model extension yields a performance increase of 34.4\% on a held-out year. This shows that PDEs-FMs can not only approximate solutions to (other) PDEs but also anchor models for real-world problems with complex interactions that lack a sufficient amount of training data or a suitable compute budget.

</details>


### [250] [Rethinking Diffusion Models with Symmetries through Canonicalization with Applications to Molecular Graph Generation](https://arxiv.org/abs/2602.15022)
*Cai Zhou,Zijie Chen,Zian Li,Jike Wang,Kaiyi Jiang,Pan Li,Rose Yu,Muhan Zhang,Stephen Bates,Tommi Jaakkola*

Main category: cs.LG

TL;DR: 论文提出了一种新的生成建模方法——规范化扩散（canonical diffusion），通过将样本映射到规范表示（规范姿态或顺序），在规范切片上训练非等变扩散模型，然后通过随机对称变换恢复不变分布，从而避免传统等变架构的复杂性。


<details>
  <summary>Details</summary>
Motivation: 传统方法通过等变去噪器和不变先验等架构约束来强制对称不变性，但这种方法存在复杂性。本文挑战这一传统，提出从规范化的角度出发，通过将样本映射到规范表示来简化生成建模过程。

Method: 提出规范化生成模型框架：1）将每个样本映射到轨道代表（规范姿态或顺序）；2）在规范切片上训练非等变扩散或流模型；3）生成时通过随机对称变换恢复不变分布。基于商空间理论，证明了规范化生成模型的正确性、通用性和优越表达能力。

Result: 在分子图生成任务（具有$S_n \times SE(3)$对称性）中，通过几何谱基规范化和温和位置编码，规范化扩散显著优于等变基线方法，计算量相似甚至更少。提出的CanonFlow架构在GEOM-DRUG数据集上达到最先进性能，在少步生成中优势明显。

Conclusion: 规范化视角为对称不变生成建模提供了新的有效方法，通过简化训练过程、减少条件方差，同时保持表达能力，在分子生成等任务中展现出显著优势，为对称性处理提供了理论严谨且实用的框架。

Abstract: Many generative tasks in chemistry and science involve distributions invariant to group symmetries (e.g., permutation and rotation). A common strategy enforces invariance and equivariance through architectural constraints such as equivariant denoisers and invariant priors. In this paper, we challenge this tradition through the alternative canonicalization perspective: first map each sample to an orbit representative with a canonical pose or order, train an unconstrained (non-equivariant) diffusion or flow model on the canonical slice, and finally recover the invariant distribution by sampling a random symmetry transform at generation time. Building on a formal quotient-space perspective, our work provides a comprehensive theory of canonical diffusion by proving: (i) the correctness, universality and superior expressivity of canonical generative models over invariant targets; (ii) canonicalization accelerates training by removing diffusion score complexity induced by group mixtures and reducing conditional variance in flow matching. We then show that aligned priors and optimal transport act complementarily with canonicalization and further improves training efficiency. We instantiate the framework for molecular graph generation under $S_n \times SE(3)$ symmetries. By leveraging geometric spectra-based canonicalization and mild positional encodings, canonical diffusion significantly outperforms equivariant baselines in 3D molecule generation tasks, with similar or even less computation. Moreover, with a novel architecture Canon, CanonFlow achieves state-of-the-art performance on the challenging GEOM-DRUG dataset, and the advantage remains large in few-step generation.

</details>


### [251] [Long Context, Less Focus: A Scaling Gap in LLMs Revealed through Privacy and Personalization](https://arxiv.org/abs/2602.15028)
*Shangding Gu*

Main category: cs.LG

TL;DR: PAPerBench是一个大规模基准测试，用于研究上下文长度对LLM个性化和隐私保护的影响，发现随着上下文增长，两者性能均下降，揭示了注意力稀释问题。


<details>
  <summary>Details</summary>
Motivation: LLM在隐私敏感和个性化场景中部署增多，但上下文长度对隐私泄露和个性化效果的影响尚未被系统研究，需要建立基准来探索这一关系。

Method: 构建PAPerBench基准，包含约29,000个实例，上下文长度从1K到256K tokens，总计377K评估问题，联合评估个性化性能和隐私风险，并进行注意力稀释的理论分析。

Result: 实验显示随着上下文长度增加，LLM的个性化和隐私保护性能均一致下降；理论分析表明这是固定容量Transformer中软注意力的固有局限性，即注意力稀释现象。

Conclusion: 当前模型存在普遍的扩展差距：长上下文导致注意力分散。基准测试的发布支持可重复评估和未来可扩展隐私与个性化研究。

Abstract: Large language models (LLMs) are increasingly deployed in privacy-critical and personalization-oriented scenarios, yet the role of context length in shaping privacy leakage and personalization effectiveness remains largely unexplored. We introduce a large-scale benchmark, PAPerBench, to systematically study how increasing context length influences both personalization quality and privacy protection in LLMs. The benchmark comprises approximately 29,000 instances with context lengths ranging from 1K to 256K tokens, yielding a total of 377K evaluation questions. It jointly evaluates personalization performance and privacy risks across diverse scenarios, enabling controlled analysis of long-context model behavior. Extensive evaluations across state-of-the-art LLMs reveal consistent performance degradation in both personalization and privacy as context length increases. We further provide a theoretical analysis of attention dilution under context scaling, explaining this behavior as an inherent limitation of soft attention in fixed-capacity Transformers. The empirical and theoretical findings together suggest a general scaling gap in current models -- long context, less focus. We release the benchmark to support reproducible evaluation and future research on scalable privacy and personalization. Code and data are available at https://github.com/SafeRL-Lab/PAPerBench

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [252] [Dynamic Analysis and Optimal Prevention Strategies for Monkeypox Spread Modeled via the Mittag--Leffler Kernel](https://arxiv.org/abs/2602.13208)
*Mine Yurtoğlu,Dilara Yapışkan,Ebenezer Bonyah,Beyza Billur İskender Eroğlu,Derya Avcı,Delfim F. M. Torres*

Main category: math.OC

TL;DR: 该研究使用Atangana-Baleanu算子建模猴痘传播，分析疫苗接种、治疗和隔离三种控制策略的效果，发现三策略同时应用是最有效的防控方案。


<details>
  <summary>Details</summary>
Motivation: 猴痘已成为全球威胁，特别是非洲国家。随着天花疫苗的停止接种，人群对猴痘的免疫力下降，需要研究有效的防控策略来补充临床研究。

Method: 使用Atangana-Baleanu算子建立猴痘传播模型，分析三种控制策略（疫苗接种、治疗、隔离）的效果。通过Adams型预测-校正法数值求解最优系统，分别评估单策略、双策略和三策略组合的效果。

Result: 数值模拟显示，疫苗接种、治疗和隔离三种控制策略同时应用是最有效的策略，能够以最小成本实现减少暴露和感染个体率的目标。

Conclusion: 为了有效控制猴痘疫情，需要同时实施疫苗接种、治疗和隔离三种控制策略，这种综合防控方案能够以最小成本达到最佳防控效果。

Abstract: Monkeypox is a viral disease belonging to the smallpox family. Although it has milder symptoms than smallpox in humans, it has become a global threat in recent years, especially in African countries. Initially, incidental immunity against monkeypox was provided by smallpox vaccines. However, the eradication of smallpox over time and thus the lack of vaccination has led to the widespread and clinical importance of monkeypox. Although mathematical epidemiology research on the disease is complementary to clinical studies, it has attracted attention in the last few years. The present study aims to discuss the indispensable effects of three control strategies such as vaccination, treatment, and quarantine to prevent the monkeypox epidemic modeled via the Atangana--Baleanu operator. The main purpose is to determine optimal control measures planned to reduce the rates of exposed and infected individuals at the minimum costs. For the controlled model, the existence-uniqueness of the solutions, stability, and sensitivity analysis, and numerical optimal solutions are exhibited. The optimal system is numerically solved using the Adams-type predictor--corrector method. In the numerical simulations, the efficacy of the vaccination, treatment, and quarantine controls is evaluated in separate analyzes as single-, double-, and triple-control strategies. The results demonstrate that the most effective strategy for achieving the aimed outcome is the simultaneous application of vaccination, treatment, and quarantine controls.

</details>


### [253] [Stochastic variance reduced extragradient methods for solving hierarchical variational inequalities](https://arxiv.org/abs/2602.13510)
*Pavel Dvurechensky,Andrea Ebner,Johannes Carl Schnebel,Shimrit Shtern,Mathias Staudigl*

Main category: math.OC

TL;DR: 本文针对具有双层层次结构和有限和表示的变分不等式问题，首次证明了方差缩减随机算法在欧几里得和Bregman设置下的收敛率和复杂度


<details>
  <summary>Details</summary>
Motivation: 变分不等式（VI）是一个广泛的问题类别，涵盖了函数最小化、鞍点（极小极大）问题、纳什均衡问题等。本文关注具有双层层次结构和每层平滑算子有限和表示的VI优化问题，这是现有研究尚未充分解决的挑战性设置。

Method: 提出了方差缩减随机算法来处理双层层次VI问题，在欧几里得和Bregman两种设置下进行分析。算法针对每层平滑算子的有限和表示特性，采用方差缩减技术来提高效率。

Result: 首次证明了方差缩减随机算法在解决层次VI问题时的收敛率和复杂度界限。为欧几里得和Bregman两种设置都提供了理论保证。

Conclusion: 本文为具有双层层次结构和有限和表示的变分不等式问题提供了首个收敛性分析框架，扩展了随机优化方法在这一重要问题类别中的应用范围。

Abstract: We are concerned with optimization in a broad sense through the lens of solving variational inequalities (VIs) -- a class of problems that are so general that they cover as particular cases minimization of functions, saddle-point (minimax) problems, Nash equilibrium problems, and many others. The key challenges in our problem formulation are the two-level hierarchical structure and finite-sum representation of the smooth operators in each level. For this setting, we are the first to prove convergence rates and complexity statements for variance-reduced stochastic algorithms approaching the solution of hierarchical VIs in Euclidean and Bregman setups.

</details>


### [254] [Learning Gradient Flow: Using Equation Discovery to Accelerate Engineering Optimization](https://arxiv.org/abs/2602.13513)
*Grant Norman,Conor Rowan,Kurt Maute,Alireza Doostan*

Main category: math.OC

TL;DR: 提出Learned Gradient Flow (LGF)优化器，利用数据驱动方程发现从优化轨迹数据学习连续时间梯度流，作为原始优化问题的代理模型，避免昂贵的目标函数和梯度计算。


<details>
  <summary>Details</summary>
Motivation: 传统优化方法（如梯度下降、牛顿法、ADAM）需要频繁评估目标函数及其梯度，计算成本高昂。本文旨在通过数据驱动方法学习优化过程的连续时间动力学，构建代理模型加速收敛。

Method: 从优化变量轨迹数据中学习梯度下降、牛顿法和ADAM优化的连续时间动力学，提出LGF优化器，可在全维或降维空间中构建可变多项式阶数的代理模型，并在优化过程中按用户定义间隔更新。

Result: 在工程力学和科学机器学习的标准问题上验证，包括两个逆问题、结构拓扑优化和两个不同离散化的正向求解。结果表明学习到的梯度流能显著加速收敛，捕捉优化轨迹关键特征，同时避免昂贵的目标函数和梯度计算。

Conclusion: LGF优化器通过数据驱动方程发现学习连续时间梯度流，为昂贵优化问题提供有效的代理建模方法，能显著减少计算成本并加速收敛，在工程和科学机器学习问题中具有应用潜力。

Abstract: In this work, we investigate the use of data-driven equation discovery for dynamical systems to model and forecast continuous-time dynamics of unconstrained optimization problems. To avoid expensive evaluations of the objective function and its gradient, we leverage trajectory data on the optimization variables to learn the continuous-time dynamics associated with gradient descent, Newton's method, and ADAM optimization. The discovered gradient flows are then solved as a surrogate for the original optimization problem. To this end, we introduce the Learned Gradient Flow (LGF) optimizer, which is equipped to build surrogate models of variable polynomial order in full- or reduced-dimensional spaces at user-defined intervals in the optimization process. We demonstrate the efficacy of this approach on several standard problems from engineering mechanics and scientific machine learning, including two inverse problems, structural topology optimization, and two forward solves with different discretizations. Our results suggest that the learned gradient flows can significantly expedite convergence by capturing critical features of the optimization trajectory while avoiding expensive evaluations of the objective and its gradient.

</details>


### [255] [Distributed Edge Computing Task Allocation with Network Effects](https://arxiv.org/abs/2602.13514)
*Henry Abrahamson,Yongho Kim,Seongha Park,Ermin Wei*

Main category: math.OC

TL;DR: 提出了一种用于边缘计算网络中科学任务分配的分布式优化方法，通过双下降算法最大化服务质量，适应异构节点硬件约束和时变QoS需求


<details>
  <summary>Details</summary>
Motivation: 边缘计算节点网络需要完成遥感监测等科学任务，但面临异构节点硬件约束和时变服务质量要求的挑战，需要一种能适应动态环境的分布式任务分配方案

Method: 将任务分配建模为优化问题，最大化服务质量并满足约束条件，采用双下降方法求解，该方法易于在通信受限的网络中以分布式方式实现

Result: 使用Sage分布式传感器网络的真实数据模拟，验证了策略在QoS需求和节点能力变化的动态环境中的性能，能够适应变化并返回可行解

Conclusion: 提出的分布式优化方法能够有效处理边缘计算网络中的任务分配问题，适应异构硬件约束和时变QoS需求，在动态环境中保持可行性

Abstract: Field-deployable edge computing nodes form a network and are used to complete scientific tasks for remote sensing and monitoring. The networked nodes collectively decide which scientific applications to run while they are constrained by various factors, such as differing hardware constraints from heterogeneous nodes and time-varying quality of service (QoS) requirements. We model the problem of task allocation as an optimization problem that maximizes the QoS, subject to the constraints. We solve the optimization problem using a dual-descent method, which can be easily implemented in a distributed way subject to the communication constraints of the network. Using a simulation that uses real-world data collected from Sage, a distributed sensor network, we analyze our policy's performance in dynamic situations where the required QoS and the nodes' capabilities change, and verify that it can adapt and return a feasible solution while accounting for those changes.

</details>


### [256] [On the Existence of Periodic Solutions with Applications to Extremum-Seeking](https://arxiv.org/abs/2602.13615)
*Iasson Karafyllis,Miroslav Krstic*

Main category: math.OC

TL;DR: 论文提供了两个用于研究动力系统周期解存在性与稳定性的结果：1) 标量时变系统中周期解与有界解的等价性；2) 时变动力系统周期解存在与稳定的充分条件，并应用于静态输出映射的极值搜索问题。


<details>
  <summary>Details</summary>
Motivation: 研究动力系统中周期解的存在性和稳定性问题，为极值搜索控制等应用提供理论基础，避免传统平均化定理和奇异摄动方法的使用。

Method: 1) 建立标量时变系统中周期解与有界解的等价性定理；2) 提出时变动力系统周期解存在与稳定的充分条件；3) 将理论结果应用于静态输出映射的极值搜索问题。

Result: 获得了两个主要理论结果：证明了标量时变系统中周期解与有界解的等价性；提出了时变系统周期解存在与稳定的充分条件，并在极值搜索问题中得到了非局部性结果。

Conclusion: 论文提供了研究动力系统周期解存在性与稳定性的新理论工具，特别适用于极值搜索控制问题，避免了传统方法的局限性，获得了非局部性结果。

Abstract: This paper provides two results that are useful in the study of the existence and the stability properties of a periodic solution for a given dynamical system. The first result deals with scalar time-periodic systems and establishes the equivalence of the existence of a periodic solution and the existence of a bounded solution. The second result provides sufficient conditions for the existence and the stability of a periodic solution for a time-periodic dynamical system. Both results are applied to extremum seeking problems for a static output map with no plant dynamics and novel non-local results are provided without the use of averaging theorems and singular perturbation arguments.

</details>


### [257] [An adaptive framework for first-order gradient methods](https://arxiv.org/abs/2602.13620)
*Xiaozhe Hu,Sara Pollock,Zhongqin Xue,Yunrong Zhu*

Main category: math.OC

TL;DR: 提出自适应梯度方法框架，无需强凸参数先验知识，通过残差比几何平均估计收敛率上界，动态调整步长和动量参数


<details>
  <summary>Details</summary>
Motivation: 实践中平滑参数可通过回溯等技术估计，但强凸参数估计困难；即使参数选择最优，收敛速度仍可能较慢。需要无需强凸参数先验知识的自适应方法

Method: 使用连续残差范数比值的几何平均作为收敛率上界的经验估计，据此自适应更新算法参数（步长和动量）。方法实现简单，仅需在现有信息上进行少量额外计算

Result: 对二次优化问题，自适应梯度方法收敛速度至少与梯度下降相当；在二次和非线性问题上的数值实验验证了有效性，自适应算法性能与使用最优参数的对应方法相当，某些情况下能捕捉局部信息并表现更优

Conclusion: 提出了一种无需强凸参数先验知识的自适应梯度方法框架，通过残差比几何平均估计收敛率上界，实现参数动态调整。方法简单高效，在理论和实验上均表现出良好性能

Abstract: Gradient methods are widely used in optimization problems. In practice, while the smoothness parameter can be estimated utilizing techniques such as backtracking, estimating the strong convexity parameter remains a challenge; moreover, even with the optimal parameter choice, convergence can be slow. In this work, we propose a framework for dynamically adapting the step size and momentum parameters in first-order gradient methods for the optimization problem, without prior knowledge of the strong convexity parameter. The main idea is to use the geometric average of the ratios of successive residual norms as an empirical estimate of the upper bound on the convergence rate, which in turn allows us to adaptively update the algorithm parameters. The resulting algorithms are simple to implement, yet efficient in practice, requiring only a few additional computations on existing information. The proposed adaptive gradient methods are shown to converge at least as fast as gradient descent for quadratic optimization problems. Numerical experiments on both quadratic and nonlinear problems validate the effectiveness of the proposed adaptive algorithms. The results show that the adaptive algorithms are comparable to their counterparts using optimal parameters, and in some cases, they capture local information and exhibit improved performance.

</details>


### [258] [Riemannian Momentum Tracking: Distributed Optimization with Momentum on Compact Submanifolds](https://arxiv.org/abs/2602.13646)
*Jun Chen,Tianyi Zhu,Haishan Ye,Lina Liu,Guang Dai,Yong Liu,Yunliang Jiang,Ivor W. Tsang*

Main category: math.OC

TL;DR: 提出Riemannian Momentum Tracking (RMTracking)算法，在紧致子流形上实现带动量的分布式优化，收敛速度比相关算法快1/(1-β)倍


<details>
  <summary>Details</summary>
Motivation: 动量梯度下降在信号处理和机器学习中表现出优于标准梯度下降的经验优势，但在黎曼流形上的分布式动量算法研究较少。需要开发在紧致子流形上的分布式优化算法，处理非凸目标函数。

Method: 提出Riemannian Momentum Tracking (RMTracking)算法，在无向连通网络图上，对紧致子流形上的光滑（可能非凸）局部函数之和进行分布式优化，使用恒定步长和动量权重β∈[0,1)。

Result: 建立了黎曼梯度平均的O((1-β)/K)收敛率，当步长足够小时，能以O((1-β)/K)收敛到驻点。这是首个实现精确收敛且比其他相关算法快1/(1-β)倍的分布式算法。

Conclusion: RMTracking是首个在紧致子流形上实现带动量的分布式优化算法，具有理论保证的收敛速度优势，并通过特征值问题的数值实验验证了理论结果。

Abstract: Gradient descent with momentum has been widely applied in various signal processing and machine learning tasks, demonstrating a notable empirical advantage over standard gradient descent. However, momentum-based distributed Riemannian algorithms have been only scarcely explored. In this paper, we propose Riemannian Momentum Tracking (RMTracking), a decentralized optimization algorithm with momentum over a compact submanifold. Given the non-convex nature of compact submanifolds, the objective function, composed of a finite sum of smooth (possibly non-convex) local functions, is minimized across agents in an undirected and connected network graph. With a constant step-size, we establish an $\mathcal{O}(\frac{1-β}{K})$ convergence rate of the Riemannian gradient average for any momentum weight $β\in [0,1)$. Especially, RMTracking can achieve a convergence rate of $\mathcal{O}(\frac{1-β}{K})$ to a stationary point when the step-size is sufficiently small. To best of our knowledge, RMTracking is the first decentralized algorithm to achieve exact convergence that is $\frac{1}{1-β}$ times faster than other related algorithms. Finally, we verify these theoretical claims through numerical experiments on eigenvalue problems.

</details>


### [259] [From time series to dissipativity of linear systems with dynamic supply rates](https://arxiv.org/abs/2602.13654)
*Henk J. van Waarde,Jeremy Coulson,Alberto Padoan*

Main category: math.OC

TL;DR: 该论文研究如何使用输入输出数据验证线性时不变系统的耗散性，基于行为系统理论，利用二次差分形式表达耗散性，并给出了数据信息性对于耗散性的充分条件。


<details>
  <summary>Details</summary>
Motivation: 传统耗散性验证需要系统模型，但实际中可能只有输入输出数据可用。本文旨在直接从数据验证耗散性，无需系统辨识，为数据驱动的控制理论提供新方法。

Method: 采用行为系统理论和二次差分形式(QDFs)框架，在系统可控且滞后上界已知的假设下，分析数据对耗散性的信息性，推导充分条件。

Result: 给出了数据信息性对于耗散性的充分条件；对于特定静态供给率，这些条件也是必要的；发现耗散性验证需要能唯一辨识系统的数据。

Conclusion: 建立了从数据直接验证LTI系统耗散性的理论框架，揭示了数据信息性与系统辨识能力之间的关系，为数据驱动的耗散性分析提供了理论基础。

Abstract: This paper studies the problem of verifying dissipativity of linear time-invariant (LTI) systems using input-output data. We leverage behavioral systems theory to express dissipativity in terms of quadratic difference forms (QDFs), allowing the study of general dynamic quadratic supply rates. We work under the assumptions that the data-generating system is controllable, and an upper bound is given on its lag. As our main results, we provide sufficient conditions for the data to be informative for dissipativity. We also show that for a specific class of static supply rates, these conditions are both necessary and sufficient. For the latter supply rates, it turns out that certification of dissipativity is only possible from data that enable unique system identification. As auxiliary results, we highlight some properties of QDFs, such as upper bounds on the degree of storage functions.

</details>


### [260] [Persistent homology-based explicit topological control for 2D topology optimization with MMA](https://arxiv.org/abs/2602.13856)
*Gengchen Li,Depeng Gao,Wenliang Yin,Hongwei Lin*

Main category: math.OC

TL;DR: 提出一种基于持续同调的显式可微拓扑控制框架，用于在拓扑优化中精确调控结构孔洞数量


<details>
  <summary>Details</summary>
Motivation: 现有拓扑优化方法主要通过间接策略（如滤波、最小长度尺度控制等）影响拓扑特征，无法精确调控结构孔洞数量，这限制了理论分析和制造可行性

Method: 将持续同调集成到经典最小柔度拓扑优化问题中，使用NURBS表示设计域和密度场，通过持久图定量表征拓扑特征，构建可微的拓扑感知目标函数，结合MMA算法求解

Result: 数值实验表明该方法能够显式控制结构连通性和孔洞数量，提供系统化、数学基础坚实的拓扑调控策略

Conclusion: 提出的框架实现了对结构拓扑特征的精确调控，为拓扑优化提供了新的数学工具和实用方法

Abstract: Controlling structural complexity, particularly the number of holes, remains a fundamental challenge in topology optimization, with significant implications for both theoretical analysis and manufacturability. Most existing approaches rely on indirect strategies, such as filtering techniques, minimum length-scale control, or specific level-set initializations, which influence topology only implicitly and do not allow precise regulation of topological features. In this work, we propose an explicit and differentiable topology-control framework by integrating persistent homology into the classical minimum-compliance topology optimization problem. The design domain and density field are represented using non-uniform rational B-splines (NURBS), while persistence diagrams are employed to rigorously and quantitatively characterize topological features. Given a prescribed number of holes, a differentiable topology-aware objective is constructed from the persistence pairs and incorporated into the compliance objective, leading to a unified optimization formulation. The resulting problem is efficiently solved using the method of moving asymptotes (MMA).Numerical experiments demonstrate that the proposed approach enables explicit control over structural connectivity and the number of holes, thereby providing a systematic and mathematically grounded strategy for topology regulation.

</details>


### [261] [The local minimality of differentiable functions](https://arxiv.org/abs/2602.13965)
*Tien-Son Pham*

Main category: math.OC

TL;DR: 论文提出了在光滑无约束优化中局部极小值点稳定性的必要和充分条件（用Łojasiewicz不等式表示），并推导出局部极小性质可由特定阶数泰勒多项式决定的充分条件。


<details>
  <summary>Details</summary>
Motivation: 研究光滑无约束优化中局部极小值点的稳定性条件，特别是确定在什么条件下函数的局部极小性质可以由其泰勒多项式完全决定。

Method: 使用Łojasiewicz不等式框架，建立局部极小值点稳定性的必要和充分条件，并推导出局部极小性质由泰勒多项式决定的充分条件。

Result: 获得了局部极小值点稳定性的完整刻画条件，并找到了函数局部极小性质可由其泰勒多项式决定的充分条件。

Conclusion: 论文建立了光滑无约束优化中局部极小值点稳定性的理论框架，为判断局部极小性质何时可由有限阶泰勒展开决定提供了数学基础。

Abstract: In this paper we present necessary and sufficient conditions (in terms of Łojasiewicz inequalities) for the stability of local minimum points in smooth unconstrained optimization. In particular, we derive a sufficient condition for which the local minimum property of a given function is determined by its Taylor polynomial of a certain degree.

</details>


### [262] [A Homogeneous Second-Order Descent Ascent Algorithm for Nonconvex-Strongly Concave Minimax Problems](https://arxiv.org/abs/2602.14058)
*Jia-Hao Chen,Zi Xu,Hui-Ling Zhang*

Main category: math.OC

TL;DR: 提出HSDA算法解决非凸-强凹极小极大优化问题，通过求解齐次特征值子问题获得下降方向，达到最优二阶复杂度，并设计近似版本IHSDA处理大规模问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理非凸-强凹极小极大优化问题时，在鞍点附近收敛困难，特别是在Hessian矩阵接近半正定的情况下，需要更有效的二阶方法来加速逃离鞍点。

Method: 提出齐次二阶下降上升算法(HSDA)，每次迭代通过求解基于目标函数梯度和Hessian构建的齐次特征值子问题来计算搜索方向。该方向即使在Hessian接近半正定的情况下也能保证足够的负曲率。还设计了近似版本IHSDA，使用Lanczos过程近似求解子问题。

Result: HSDA在$\tilde{\mathcal{O}}(\varepsilon^{-3/2})$迭代次数内找到$\mathcal{O}(\varepsilon,\sqrt{\varepsilon})$-二阶稳定点，达到该问题类二阶方法的最优复杂度。IHSDA以高概率达到相同复杂度，总Hessian-向量乘积成本为$\tilde{\mathcal{O}}(\varepsilon^{-7/4})$。

Conclusion: HSDA算法在理论和实践上都表现出色，为处理非凸-强凹极小极大优化问题提供了有效解决方案，特别是在逃离鞍点方面具有优势，适用于对抗训练等实际应用。

Abstract: This paper introduces a novel Homogeneous Second-order Descent Ascent (HSDA) algorithm for nonconvex-strongly concave minimax optimization problems. At each iteration, HSDA uniquely computes a search direction by solving a homogenized eigenvalue subproblem built from the gradient and Hessian of the objective function. This formulation guarantees a descent direction with sufficient negative curvature even in near-positive-semidefinite Hessian regimes--a key feature that enhances escape from saddle points. We prove that HSDA finds an $\mathcal{O}(\varepsilon,\sqrt{\varepsilon})$-second-order stationary point within $\tilde{\mathcal{O}}(\varepsilon^{-3/2})$ iterations, matching the optimal $\varepsilon$-order iteration complexity among second-order methods for this problem class. To address large-scale applications, we further design an inexact variant (IHSDA) that preserves the single-loop structure while solving the subproblem approximately via a Lanczos procedure. With high probability, IHSDA achieves the same $\tilde{\mathcal{O}}(\varepsilon^{-3/2})$ iteration complexity and attains an $\mathcal{O}(\varepsilon, \sqrt{\varepsilon})$-second-order stationary point, with the total Hessian-vector product cost bounded by $\tilde{\mathcal{O}}(\varepsilon^{-7/4})$. Experiments on synthetic minimax problems and adversarial training tasks confirm the practical effectiveness and robustness of the proposed algorithms.

</details>


### [263] [Comparative Evaluation of SDP, SOCP, and QC Convex Relaxations for Large-Scale Market-Based AC Optimal Power Flow](https://arxiv.org/abs/2602.14136)
*Ata Keskin*

Main category: math.OC

TL;DR: 该论文对ACOPF问题的多种凸松弛方法进行了系统性比较，包括SDP、SOCP和QC松弛，在统一框架下评估了不同方法在求解质量、计算时间和内存消耗方面的表现。


<details>
  <summary>Details</summary>
Motivation: ACOPF问题在电力系统运行中至关重要，但由于其非线性和非凸特性，在大规模网络中寻找全局最优解计算上不可行。凸松弛方法提供了可处理的替代方案，但需要系统比较不同松弛方法的性能表现。

Method: 实现了DCOPF、Shor的SDP松弛（复数和实数形式）、弦图SDP、Jabr的SOCP松弛和QC松弛，使用MOSEK Fusion API构建统一框架。针对QC松弛需要角度差边界的问题，采用基于Sobol序列的拟蒙特卡洛采样来经验估计更紧的边界。

Result: 在ARPA-E数据集的子网络上评估了各种松弛方法，结果表明松弛紧密度与计算效率之间存在权衡，为基于网络规模和求解需求选择合适公式提供了实用指导。

Conclusion: 不同ACOPF松弛方法在求解质量和计算效率方面存在明显权衡，研究结果为电力系统运行中根据具体需求选择合适的松弛方法提供了实践指导。

Abstract: The alternating current optimal power flow (ACOPF) problem is central to modern power system operations, determining how electricity is generated and transmitted to maximize social welfare while respecting physical and operational constraints. However, the nonlinear and non-convex nature of AC power flow equations makes finding globally optimal solutions computationally intractable for large networks. Convex relaxations - including semidefinite programming (SDP), second-order cone programming (SOCP), and quadratic convex (QC) formulations - provide tractable alternatives that can yield provably optimal or near-optimal solutions under appropriate conditions. This paper presents a comprehensive comparative study of multiple ACOPF relaxations applied to market-based welfare maximization. We implement DCOPF, Shor's SDP relaxation (complex and real-valued forms), chordal SDP, Jabr's SOCP relaxation, and QC relaxations in a unified, solver-native framework using the MOSEK Fusion API, eliminating modeling overhead present in high-level frameworks such as CVXPY. To address the practical challenge of missing or overly conservative angle difference bounds required by QC relaxations, we employ quasi-Monte Carlo sampling with Sobol sequences to empirically estimate tighter bounds. We evaluate these relaxations on subnetworks of varying sizes derived from the ARPA-E dataset, systematically comparing solution quality, runtime, and memory consumption. Our results demonstrate the trade-offs between relaxation tightness and computational efficiency, providing practical guidance for selecting appropriate formulations based on network scale and solution requirements.

</details>


### [264] [On the Existence of Koopman Linear Embeddings for Controlled Nonlinear Systems](https://arxiv.org/abs/2602.14537)
*Xu Shang,Masih Haseli,Jorge Cortés,Yang Zheng*

Main category: math.OC

TL;DR: 本文建立了受控非线性系统存在精确有限维Koopman线性表示（Koopman线性嵌入）的充分必要条件，并引入了符号化程序来判定给定系统是否具有所需的控制仿射保持结构。


<details>
  <summary>Details</summary>
Motivation: Koopman线性表示已成为非线性系统控制设计的流行工具，但尚不清楚何时这种表示是精确的。本文旨在明确受控非线性系统何时能获得精确的有限维Koopman线性表示。

Method: 首先证明此类系统必须可转化为特殊的控制仿射保持（CAP）结构，该结构强制状态对控制输入的仿射依赖，并将所有非线性隔离到自治子系统中。然后证明该自治子系统本身必须具有足够丰富Koopman不变子空间的有限维Koopman线性模型。最后引入符号化程序来确定给定受控非线性系统是否具有CAP结构。

Result: 建立了受控非线性系统存在精确有限维Koopman线性表示的充分必要条件：系统必须具有CAP结构，且自治子系统具有足够丰富Koopman不变子空间的有限维Koopman线性模型。

Conclusion: 通过CAP结构和符号化判定程序，可以阐明Koopman近似误差是源于系统内在动力学还是提升函数的选择，为精确Koopman线性表示提供了理论框架。

Abstract: Koopman linear representations have become a popular tool for control design of nonlinear systems, yet it remains unclear when such representations are exact. In this paper, we establish sufficient and necessary conditions under which a controlled nonlinear system admits an exact finite-dimensional Koopman linear representation, which we term Koopman linear embedding. We show that such a system must be transformable into a special control-affine preserved (CAP) structure, which enforces affine dependence of the state on the control input and isolates all nonlinearities into an autonomous subsystem. We further prove that this autonomous subsystem must itself admit a finite-dimensional Koopman linear model with a sufficiently-rich Koopman invariant subspace. Finally, we introduce a symbolic procedure to determine whether a given controlled nonlinear system admits the CAP structure, thereby elucidating whether Koopman approximation errors arise from intrinsic system dynamics or from the choice of lifting functions.

</details>


### [265] [Subgradient Gliding Method for Nonsmooth Convex Optimization](https://arxiv.org/abs/2602.14139)
*Zhihan Zhu,Yanhao Zhang,Yong Xia*

Main category: math.OC

TL;DR: 论文分析了经典投影次梯度方法在非光滑凸优化中的根本局限性：边界点处缺乏有效次梯度导致方法必然失败。提出了新的次梯度滑翔方法，解决了这一问题，扩展了步长设计空间，并建立了最优收敛率。


<details>
  <summary>Details</summary>
Motivation: 经典投影次梯度方法在非光滑凸优化中存在根本缺陷：当迭代点位于目标函数定义域的边界时，可能不存在有效的次梯度，导致算法提前终止。这种失败概率可以任意接近1，即使在简单问题实例上也会发生，限制了该方法的应用范围。

Method: 提出了次梯度滑翔方法，该方法在边界点处不需要有效次梯度也能保持良好定义。该方法将经典投影次梯度方法作为特例包含在内，并大幅扩展了可接受的步长设计空间。方法不依赖目标函数的全局Lipschitz连续性，仅需对次梯度增长进行温和控制。

Result: 建立了最优遍历收敛率：凸问题为O(1/√t)，强凸问题为O(1/t)。将框架扩展到随机设置。数值实验显示，在经典方法完全失败的场景中，新方法以100%成功率可靠收敛，在精度和收敛速度上实现了数量级改进。

Conclusion: 次梯度滑翔方法解决了经典投影次梯度方法的根本局限性，显著扩展了次梯度优化方法的应用范围，使其能够处理非Lipschitz非光滑凸问题。该方法提供了更大的算法设计灵活性，并保持了最优收敛性能。

Abstract: We identify and analyze a fundamental limitation of the classical projected subgradient method in nonsmooth convex optimization: the inevitable failure caused by the absence of valid subgradients at boundary points. We show that, under standard step sizes for both convex and strongly convex objectives, the method can fail after a single iteration with probability arbitrarily close to one, even on simple problem instances. To overcome this limitation, we propose a novel alternative termed the \textit{subgradient gliding method}, which remains well defined without boundary subgradients and avoids premature termination. Beyond resolving this foundational issue, the proposed framework encompasses the classical projected subgradient method as a special case and substantially enlarges its admissible step-size design space, providing greater flexibility for algorithmic design. We establish optimal ergodic convergence rates, $\mathcal{O}(1/\sqrt{t})$ for convex problems and $\mathcal{O}(1/t)$ for strongly convex problems, and further extend the framework to stochastic settings. Notably, our analysis does not rely on global Lipschitz continuity of the objective function, requiring only mild control on subgradient growth. Numerical experiments demonstrate that, in scenarios where the classical projected subgradient method fails completely, the proposed method converges reliably with a $100\%$ success rate and achieves orders-of-magnitude improvements in accuracy and convergence speed. These results substantially expand the scope of subgradient-based optimization methods to non-Lipschitz nonsmooth convex problems.

</details>


### [266] [Temporally Flexible Transport Scheduling on Networks with Departure-Arrival Constriction and Nodal Capacity Limits](https://arxiv.org/abs/2602.14652)
*Anqi Dong,Karl H. Johansson,Johan Karlsson*

Main category: math.OC

TL;DR: 该论文研究了网络上的最优传输问题，将供应和需求概念化为粒子从源节点出发和到达汇节点的时变边际分布，扩展了经典OT框架以处理指定出发和到达时间的约束。


<details>
  <summary>Details</summary>
Motivation: 经典最优传输框架假设所有质量在t=0时出发，在t=t_f时到达，这限制了实际应用。本文旨在扩展OT框架以处理更现实的场景，其中粒子的出发和到达时间可以指定，从而更好地建模网络中的时空运输问题。

Method: 提出两种约束场景：(i)独立出发-到达约束，出发和到达率独立指定；(ii)耦合出发-到达约束，每个粒子的运输时间跨度明确指定。对于线图分析解的存在性和唯一性，对于一般图使用基于路径的构造性约简方法。计算上采用熵正则化和多边际Sinkhorn方法，利用图结构成本提高可扩展性。

Result: 证明了独立DA约束的OT问题可表述为多边际最优传输问题，而耦合DA情况则对应于不等维OT框架。数值模拟显示了边际违反的线性收敛速率。

Conclusion: 该研究成功扩展了最优传输框架以处理网络中的时空运输问题，提出了两种实用的约束场景，并开发了有效的计算方法，为实际网络运输优化提供了理论基础和计算工具。

Abstract: We investigate the optimal transport (OT) problem over networks, wherein supply and demand are conceptualized as temporal marginals governing departure rates of particles from source nodes and arrival rates at sink nodes. This setting extends the classical OT framework, where all mass is conventionally assumed to depart at $t = 0$ and arrive at $t = t_f$. Our generalization accommodates departures and arrivals at specified times, referred as departure--arrival(DA) constraints. In particular, we impose nodal-temporal flux constraints at source and sink nodes, characterizing two distinct scenarios: (i) Independent DA constraints, where departure and arrival rates are prescribed independently, and (ii) Coupled DA constraints, where each particle's transportation time span is explicitly specified. We establish that OT with independent DA constraints admits a multi-marginal optimal transport formulation, while the coupled DA case aligns with the unequal-dimensional OT framework. For line graphs, we analyze the existence and uniqueness of the solution path. For general graphs, we use a constructive path-based reduction and optimize over a prescribed set of paths. From a computational perspective, we consider entropic regularization of the original problem to efficiently provide solutions based on multi-marginal Sinkhorn method, making use of the graphical structure of the cost to further improve scalability. Our numerical simulation further illustrates the linear convergence rate in terms of marginal violation.

</details>


### [267] [Stable representations of Hamilton-Jacobi-Bellman equations with infinite horizon](https://arxiv.org/abs/2602.14156)
*Arkadiusz Misztela,Sławomir Plaskacz*

Main category: math.OC

TL;DR: 构建无限时域状态约束下Hamilton-Jacobi-Bellman方程的合适正则表示，利用Frankowska和Basco定理证明解的存在唯一性，并展示表示的稳定性


<details>
  <summary>Details</summary>
Motivation: 研究无限时域状态约束下的Hamilton-Jacobi-Bellman方程，需要建立合适的正则表示来解决解的存在唯一性问题

Method: 构建无限时域状态约束下Hamilton-Jacobi-Bellman方程的合适正则表示，利用Frankowska和Basco定理(2019)将问题转化为该定理可处理的形式

Result: 成功建立了合适的正则表示，证明了方程解的存在性和唯一性，并证明了所构建表示的稳定性

Conclusion: 通过构建正则表示，成功解决了无限时域状态约束下Hamilton-Jacobi-Bellman方程的解存在唯一性问题，且表示具有稳定性，结果通过实例验证

Abstract: In this paper, for the Hamilton-Jacobi-Bellman equation with an infinite horizon and state constraints, we construct a suitably regular representation. This allows us to reduce the problem of existence and uniqueness of solutions to the Frankowska and Basco theorem from (2019). Furthermore, we demonstrate that our representations are stable. The obtained results are illustrated with examples.

</details>


### [268] [On Convergence Analysis of Network-GIANT: An approximate Hessian-based fully distributed optimization algorithm](https://arxiv.org/abs/2602.14830)
*Souvik Das,Luca Schenato,Subhrakanti Dey*

Main category: math.OC

TL;DR: Network-GIANT是一种近似牛顿型分布式优化方法，在保持与一阶方法相同通信复杂度的同时，实现了更快的线性收敛速度。论文通过理论分析给出了其全局线性收敛率，并揭示了渐进线性收敛率约为1-η的机制。


<details>
  <summary>Details</summary>
Motivation: 现有的分布式优化方法中，一阶方法收敛较慢，而传统的二阶方法通信开销大。Network-GIANT在保持一阶方法通信效率的同时，利用局部Hessian信息实现更快的收敛，但其理论收敛性质尚未得到充分分析。

Method: 基于共识的参数更新和局部Hessian下降方向，结合梯度跟踪技术。通过分析依赖目标函数Lipschitz连续性(L)、强凸性(μ)参数以及图谱范数(σ)的3×3矩阵的谱半径，推导收敛率。

Result: 给出了步长参数η的显式上界，保证谱半径小于1。推导了最优性间隙范数的混合线性-二次不等式上界，证明当算法接近全局最优时，渐进线性收敛率为1-η(1-γ/μ)，其中γ为Hessian近似误差。

Conclusion: Network-GIANT在保持一阶方法通信效率的同时，实现了更快的线性收敛。理论分析首次解释了其渐进线性收敛率约为1-η的机制，数值实验验证了理论结果。

Abstract: In this paper, we present a detailed convergence analysis of a recently developed approximate Newton-type fully distributed optimization method for smooth, strongly convex local loss functions, called Network-GIANT, which has been empirically illustrated to show faster linear convergence properties while having the same communication complexity (per iteration) as its first order distributed counterparts. By using consensus based parameter updates, and a local Hessian based descent direction at the individual nodes with gradient tracking, we first explicitly characterize a global linear convergence rate for Network-GIANT, which can be computed as the spectral radius of a $3 \times 3$ matrix dependent on the Lipschitz continuity ($L$) and strong convexity ($μ$) parameters of the objective functions, and the spectral norm ($σ$) of the underlying undirected graph represented by a doubly stochastic consensus matrix. We provide an explicit bound on the step size parameter $η$, below which this spectral radius is guaranteed to be less than $1$. Furthermore, we derive a mixed linear-quadratic inequality based upper bound for the optimality gap norm, which allows us to conclude that, under small step size values, asymptotically, as the algorithm approaches the global optimum, it achieves a locally linear convergence rate of $1-η(1 -\fracγμ)$ for Network-GIANT, provided the Hessian approximation error $γ$ (between the harmonic mean of the local Hessians and the global hessian (the arithmetic mean of the local Hessians) is smaller than $μ$. This asymptotically linear convergence rate of $\approx 1-η$ explains the faster convergence rate of Network-GIANT for the first time. Numerical experiments are carried out with a reduced CovType dataset for binary logistic regression over a variety of graphs to illustrate the above theoretical results.

</details>


### [269] [Smoothing Meets Perturbation: Unified and Tight Analysis for Nonconvex-Concave Minimax Optimization](https://arxiv.org/abs/2602.14185)
*Jiajin Li,Mahesh Nagarajan,Siyu Pan,Nanxi Zhang*

Main category: math.OC

TL;DR: 本文研究了非凸凹极小极大优化问题，分析了扰动和平滑两种加速机制的作用差异，建立了统一分析框架，设计了改进的一阶方法，并证明了复杂度下界。


<details>
  <summary>Details</summary>
Motivation: 虽然扰动和平滑技术都能改善收敛保证，但它们各自的作用和相对优势仍不清楚。需要建立一个统一的分析框架来区分和量化这两种机制的作用。

Method: 开发了统一的分析框架来分离和量化平滑与扰动的作用；设计了新的单循环和双循环一阶方法；基于精心构造的困难实例建立了匹配的下界。

Result: 改进了近似游戏平稳点(GS)和优化平稳点(OS)的最先进迭代复杂度界限；证明了所得复杂度界限是紧的；揭示了平滑和扰动在实现近似GS中的互补作用。

Conclusion: 近似GS和OS在内在复杂度行为上存在根本差异；平滑和扰动在实现近似GS中发挥不同但互补的作用，它们的组合产生协同效应；而仅靠扰动对于OS是不够的。

Abstract: In this paper, we investigate smooth nonconvex-concave minimax optimization problems and analyze two widely used acceleration mechanisms -- perturbation and smoothing. Perturbation augments the dual objective with a small quadratic regularization term, whereas smoothing employs an auxiliary primal sequence to approximate a proximal-point update of the value function. While both techniques are known to improve convergence guarantees, their respective roles and relative strengths remain unclear. We develop a unified analytical framework that disentangles and quantifies the respective roles of smoothing and perturbation. With this analytical framework, we design new first-order methods that improve the state-of-the-art iteration complexity bounds for both single-loop and double-loop schemes, for achieving both approximate game stationary (GS) and optimization stationary (OS) points. We also establish matching lower bounds based on carefully constructed hard instances, showing that the resulting complexity bounds are tight. Taken together, these results reveal a fundamental difference between approximate GS and OS in terms of their intrinsic complexity behavior and the following understanding: smoothing and perturbation play fundamentally different yet complementary roles in achieving approximate GS. Their combination creates a synergistic effect that yields strictly faster convergence speed than either mechanism alone, whereas perturbation by itself is insufficient for OS.

</details>


### [270] [Graph-Guided Fused Regularization for Single- and Multi-Task Regression on Spatiotemporal Data](https://arxiv.org/abs/2602.14480)
*Meixia Lin,Ziyang Zeng,Yangjing Zhang*

Main category: math.OC

TL;DR: 提出了一种用于时空矩阵回归的正则化框架，通过融合惩罚和图引导惩罚分别捕捉时间平滑演化和空间相似性，支持多任务设置，并提供了理论保证和高效算法。


<details>
  <summary>Details</summary>
Motivation: 时空矩阵数据在现代应用中频繁出现，但由于复杂的维度特定依赖关系，进行有效的回归分析仍然具有挑战性。现有方法难以同时处理时间和空间维度上的复杂依赖。

Method: 提出正则化框架：1) 使用融合惩罚捕捉时间平滑演化；2) 使用图引导惩罚促进空间相似性；3) 扩展到多任务设置实现跨任务联合估计；4) 基于Halpern Peaceman-Rachford方法开发高效求解器。

Result: 理论证明估计量的统计一致性；算法实现快速全局非遍历O(1/k)收敛率，每次迭代复杂度低；数值实验显示在预测精度、估计误差、计算效率和可扩展性方面显著优于现有方法。

Conclusion: 该框架有效解决了时空矩阵回归中的复杂依赖问题，提供了理论保证和高效计算方案，在多个方面优于现有方法，具有实际应用价值。

Abstract: Spatiotemporal matrix-valued data arise frequently in modern applications, yet performing effective regression analysis remains challenging due to complex, dimension-specific dependencies. In this work, we propose a regularized framework for spatiotemporal matrix regression that characterizes temporal and spatial dependencies through tailored penalties. Specifically, the model incorporates a fused penalty to capture smooth temporal evolution and a graph-guided penalty to promote spatial similarity. The framework also extends to the multi-task setting, enabling joint estimation across related tasks. We provide a comprehensive analysis of the framework from both theoretical and computational perspectives. Theoretically, we establish the statistical consistency of the proposed estimators. Computationally, we develop an efficient solver based on the Halpern Peaceman-Rachford method for the resulting composite convex optimization problem. The proposed algorithm achieves a fast global non-ergodic $\mathcal{O}(1/k)$ convergence rate with low per-iteration complexity. Extensive numerical experiments demonstrate that our method significantly outperforms state-of-the-art approaches in terms of predictive accuracy and estimation error, while also exhibiting superior computational efficiency and scalability.

</details>


### [271] [Extragradient methods for mean field games of controls and mean field type FBSDEs](https://arxiv.org/abs/2602.14621)
*Charles Meynard*

Main category: math.OC

TL;DR: 提出一种求解单调向量场驱动的耦合平均场前向-后向随机微分方程的数值方案，基于外梯度方法，将解表征为希尔伯特空间中单调变分不等式的零点


<details>
  <summary>Details</summary>
Motivation: 解决单调向量场驱动的耦合平均场前向-后向随机微分方程的数值求解问题，特别是在平均场控制博弈和一般随机微分方程系统中的应用

Method: 采用外梯度方法，将解表征为希尔伯特空间中单调变分不等式的零点，首先在平均场控制博弈背景下引入该方法，然后扩展到一般前向-后向随机微分方程系统

Result: 在足够强的单调性假设下，证明近似解序列以指数速度收敛

Conclusion: 该方法为求解单调向量场驱动的耦合平均场前向-后向随机微分方程提供了有效的数值方案，具有指数收敛速度，可应用于平均场博弈和一般随机系统

Abstract: In this paper we present a numerical scheme to solve coupled mean field forward-backward stochastic differential equations driven by monotone vector fields. This is based on an adaptation of so called extragradient methods by characterizing solutions as zeros of monotone variational inequalities in a Hilbert space. We first introduce the procedure in the context of mean field games of controls and highlight its connection to the fictitious play. Under sufficiently strong monotonicity assumptions, we demonstrate that the sequence of approximate solutions converges exponentially fast. Then we extend the method and main results to general forward backward systems of stochastic differential equations that do not necessarily stem from optimal control.

</details>


### [272] [Interwoven SDP in Primal-Dual Proximal Splitting Methods for Adjustable Robust Convex Optimisation with SOS-Convex Polynomial Constraints](https://arxiv.org/abs/2602.14624)
*Neil D. Dizon,Bethany I. Caldwell,Vaithilingam Jeyakumar,Guoyin Li*

Main category: math.OC

TL;DR: 提出了一种求解两阶段可调鲁棒凸优化问题的新方法，该问题具有一般凸目标函数和SOS凸多项式约束，通过二次决策规则将问题转化为凸复合无约束优化模型，并开发了定制的一阶原始-对偶近端分裂方法。


<details>
  <summary>Details</summary>
Motivation: 两阶段可调鲁棒凸优化问题出现在许多决策应用中，但由于其复杂性，通常无法转化为可直接用现有软件求解的数值可处理凸优化模型（如锥线性规划）。

Method: 1) 在可调决策变量上使用二次决策规则，将鲁棒问题等价表示为凸复合无约束优化模型；2) 开发定制的一阶原始-对偶近端分裂方法；3) 利用SDP技术、凸分析和实代数几何工具，建立理论性质，包括基于SDP的可计算投影公式。

Result: 在具有线性及SOS凸多项式存储成本的两阶段批量生产模型需求不确定性实验中，证明了所提方法的有效性和适用性。该方法将SDP技术融入原始-对偶近端分裂框架，扩展了这些方法可有效应用的问题类别。

Conclusion: 提出的方法能够有效求解具有SOS凸多项式约束的两阶段可调鲁棒凸优化问题，通过将SDP技术与原始-对偶近端分裂框架相结合，为这类复杂优化问题提供了新的数值求解途径。

Abstract: We propose a novel methodology for solving a two-stage adjustable robust convex optimisation problem with a general (proximable) convex objective function and constraints defined by sum-of-squares (SOS) convex polynomials. These problems appear in many decision-making applications. However, they are challenging to solve and typically cannot be reformulated as numerically tractable convex optimisation models, such as conic linear programs, that can be solved directly using existing software. We show that the robust problem admits an equivalent representation as a convex composite unconstrained optimisation model that preserves the same objective values, under quadratic decision rules on the adjustable decision variables. Building on this reformulation, we develop a tailored first-order primal-dual proximal splitting method. By leveraging semidefinite programming (SDP) techniques as well as tools from convex analysis and real algebraic geometry, we establish its theoretical properties, including computable SDP-based formulas for projections onto closed convex sets, specified by SOS-convex polynomial inequalities. Numerical experiments on a two-stage lot-sizing model with both linear as well as SOS-convex polynomial storage costs under demand uncertainty demonstrate the effectiveness and applicability of the proposed approach. Our approach enables the incorporation of SDP techniques into a primal-dual proximal splitting framework, thereby broadening the class of problems to which these methods can be effectively applied.

</details>


### [273] [Second-order conditions for bang-bang control of elliptic equations in arbitrary dimensions](https://arxiv.org/abs/2602.14632)
*Gerd Wachsmuth*

Main category: math.OC

TL;DR: 该论文研究bang-bang型最优控制问题，利用Bessel势空间理论，在任意空间维度下建立了目标函数的二次增长特性与二阶最优性条件的关系。


<details>
  <summary>Details</summary>
Motivation: 研究由半线性PDE控制的最优控制问题，特别是当最优控制呈现bang-bang类型时。现有方法在空间维度上存在限制，需要发展适用于任意空间维度的理论框架。

Method: 利用Bessel势空间理论，通过二阶最优性条件来刻画目标函数的二次增长特性。该方法突破了先前研究在空间维度上的限制。

Result: 建立了在任意空间维度下，bang-bang型最优控制问题中目标函数二次增长与二阶最优性条件之间的等价关系，扩展了现有理论的应用范围。

Conclusion: 提出的基于Bessel势空间的方法成功解决了任意空间维度下bang-bang型最优控制问题的二阶分析，为更广泛的最优控制问题提供了理论工具。

Abstract: We consider an optimal control problem governed by a semilinear PDE in cases where the optimal control is of bang-bang type. By utilizing the theory of Bessel potential space, we characterize quadratic growth of the objective via a second-order optimality condition. In contrast to previous contributions, our method of proof works in arbitrary spatial dimensions.

</details>


### [274] [Joint Majorization-Minimization for Nonnegative CP and Tucker Decompositions under $β$-Divergences: Unfolding-Free Updates](https://arxiv.org/abs/2602.14683)
*Valentin Leplat*

Main category: math.OC

TL;DR: 提出基于β散度的非负张量分解方法，避免显式模态展开，使用张量收缩操作实现可分离代理函数，显著加速计算


<details>
  <summary>Details</summary>
Motivation: 传统非负张量分解方法需要显式模态展开和大型辅助矩阵，计算效率低。本文旨在避免这些操作，通过张量收缩实现高效计算

Method: 基于β散度族，为非负CP和Tucker模型推导可分离代理函数，使用张量收缩实现乘法更新。提出经典块MM更新和联合MM策略，重用缓存参考量

Result: 证明了代理函数的紧致性、目标函数的单调递减性以及目标值序列的收敛性。实验表明在合成张量和Uber时空计数张量上比展开基线和最近框架有显著加速

Conclusion: 提出的基于张量收缩的MM方法避免了模态展开，实现了高效的非负张量分解，为β散度族下的CP和Tucker模型提供了有效的优化框架

Abstract: We study majorization-minimization methods for nonnegative tensor decompositions under the $β$-divergence family, focusing on nonnegative CP and Tucker models. Our aim is to avoid explicit mode unfoldings and large auxiliary matrices by deriving separable surrogates whose multiplicative updates can be implemented using only tensor contractions (einsum-style operations). We present both classical block-MM updates in contraction-only form and a joint majorization strategy, inspired by joint MM for matrix $β$-NMF, that reuses cached reference quantities across inexpensive inner updates. We prove tightness of the proposed majorizers, establish monotonic decrease of the objective, and show convergence of the sequence of objective values; we also discuss how BSUM theory applies to the block-MM scheme for analyzing limit points. Finally, experiments on synthetic tensors and the Uber spatiotemporal count tensor demonstrate substantial speedups over unfolding-based baselines and a recent einsum-factorization framework.

</details>


### [275] [Distributed Multi-Step Model Predictive Control for Consensus](https://arxiv.org/abs/2602.14714)
*Navid Noroozi*

Main category: math.OC

TL;DR: 该论文研究在时变有向通信、状态和输入约束下的离散时间多智能体系统一致性，采用分布式多步模型预测控制框架，通过引入字典序平局打破机制确保渐近一致性。


<details>
  <summary>Details</summary>
Motivation: 研究在时变有向通信、状态和输入约束下的多智能体系统一致性控制问题，现有方法中终端状态包含在邻居凸包内能保证凸包不变性，但不足以保证收敛所需的严格相对内部性质。

Method: 采用分布式多步模型预测控制框架，将一致性问题重构为协议集的镇定问题。引入字典序平局打破机制，在最优（或近似最优）MPC解中，通过次要准则选择最大化相对于邻居凸包内部性度量的轨迹。

Result: 当存在内部可行终端状态时，所提出的选择规则能强制执行渐近一致性所需的严格性条件。为单积分器和双积分器智能体推导了显式时域条件，确保可行性和内部可行终端点的自动存在。

Conclusion: 所提出的方案通过有限步集-Lyapunov收缩提供了一种分布式且可实现的共识达成路径，数值模拟显示了单调直径衰减并报告了每个智能体的计算复杂度。

Abstract: This paper studies consensus of discrete-time multi-agent systems under time-varying directed communication, state and input constraints using a distributed multi-step model predictive control (MPC) framework. Consensus is recast as stabilization of the agreement set, and a geometric viewpoint based on convex-hull invariance and strict interiority is adopted. Building on an existing geometric necessary and sufficient condition for agreement, we show that enforcing terminal inclusion in local neighbor convex hulls guarantees hull invariance but does not, in general, imply the strict relative-interior property required for convergence. An explicit counterexample demonstrates that strictness cannot be deduced from feasibility and contraction constraints alone.
  To resolve this issue without shrinking feasible sets or altering primary performance objectives, a lexicographic tie-breaking mechanism is introduced. Among optimal (or near-optimal) MPC solutions, the proposed secondary criterion selects trajectories maximizing an interiority measure with respect to the neighbor hull. It is shown that whenever an interior feasible terminal state exists, this selection rule enforces the strictness condition required for asymptotic consensus. Explicit horizon conditions are derived for single- and double-integrator agents with bounded inputs, ensuring feasibility and automatic existence of interior feasible terminal points. The resulting scheme provides a distributed and implementable route to consensus via finite-step set-Lyapunov contraction. Numerical simulations with distributed inter-process communication illustrate monotone diameter decay and report per-agent computational complexity.

</details>


### [276] [An Age-Structured Vaccination Strategy for Epidemic Containment: A Model Predictive Control Approach](https://arxiv.org/abs/2602.14758)
*Candy Sonveaux,Morgane Dumont,Mirko Fiacchini,Mohamad Ajami*

Main category: math.OC

TL;DR: 提出一种基于模型预测控制（MPC）的年龄结构疫苗接种策略，用于比利时瓦隆尼亚地区COVID-19防控，通过动态优化疫苗接种顺序来减少死亡并加速疾病消除。


<details>
  <summary>Details</summary>
Motivation: 比利时政府在第一波疫苗接种中采用的递减年龄策略可能不是最优的。需要一种能够动态适应疫情演变、考虑年龄结构差异、并能在操作约束下最小化死亡和加速疾病消除的优化方法。

Method: 采用模型预测控制（MPC）框架，结合带有疫苗接种项的年龄结构SIRD模型。MPC在每个时间步求解优化问题，动态调整疫苗接种策略，同时提供算法渐近稳定性和递归可行性的理论证明。

Result: 模拟结果显示，提出的MPC方法在减少总死亡人数、加速疾病消除和优化疫苗管理方面优于比利时政府采用的递减年龄策略。算法保证了每一步的最优成本为疫情结束时最小可获得死亡数的上界。

Conclusion: MPC框架为制定年龄结构疫苗接种策略提供了有效的优化工具，能够显著改善疫情控制效果，减少死亡并加速疾病消除，具有重要的公共卫生应用价值。

Abstract: This work presents a novel Model Predictive Control (MPC) approach to develop an optimal age-structured vaccination strategy for the containment of COVID-19 in Wallonia, Belgium. The proposed MPC framework is designed to minimize deaths, achieve early disease eradication, and adhere to operational constraints. By incorporating an age-structured Susceptible-Infected-Recovered-Deceased (SIRD) model with an additional term for vaccination, the MPC strategy dynamically adapts to the evolving epidemic state. A detailed proof of the asymptotic stability and recursive feasibility of the proposed MPC algorithm is provided. This ensures that the optimal cost at each step provides an upper bound on the minimal number obtainable of deaths at the end of the pandemic. Moreover, simulations demonstrate that the proposed MPC approach outperforms the decreasing age vaccination strategy adopted by the Belgian government during the first wave of vaccinations. The results highlight the potential of MPC-based vaccination strategies to reduce the total number of deaths, accelerate disease eradication, and optimize vaccine administration.

</details>


### [277] [Reciprocal Specific Relative Entropy between Continuous Martingales](https://arxiv.org/abs/2602.14776)
*Julio Backhoff,Xin Zhang*

Main category: math.OC

TL;DR: 提出连续鞅间的新散度概念——互易特定相对熵，并证明在赢鞅集合上最小化该散度时，优化器是著名的中性Wright-Fisher扩散过程。


<details>
  <summary>Details</summary>
Motivation: 从多个角度为连续鞅间的新散度概念提供动机，特别是针对预测市场模型（赢鞅）中的优化问题。

Method: 引入互易特定相对熵作为连续鞅间的新散度度量，在赢鞅集合上求解该散度的最小化问题，并分析其与退化的鞅最优输运问题的联系。

Result: 发现互易特定相对熵最小化问题的优化器是著名的中性Wright-Fisher扩散过程，且该扩散在某种意义上是最显著的赢鞅。

Conclusion: 中性Wright-Fisher扩散是互易特定相对熵最小化问题的唯一解，并且通过适当扰动方差最小化的退化鞅最优输运问题也能唯一选择该扩散。

Abstract: We introduce a novel notion of divergence between continuous martingales; the reciprocal specific relative entropy. First, we motivate this definition from multiple perspectives. Thereafter, we solve the reciprocal specific relative entropy minimization problem over the set of win-martingales (used as models for prediction markets Aldous (2013)). Surprisingly, we show that the optimizer is the renowned neutral Wright-Fisher diffusion. We also justify that this diffusion is in a sense the most salient win-martingale, since it is uniquely selected when we suitably perturb the degenerate martingale optimal transport problem of variance minimization.

</details>


### [278] [Effective approximations of solutions to highly oscillatory diffusion equations from coarse measurements](https://arxiv.org/abs/2602.14820)
*Claude Le Bris,Frédéric Legoll,Simon Ruget*

Main category: math.OC

TL;DR: 提出一种从部分信息（全局能量）重建扩散方程有效系数的方法，通过非凸优化问题实现，即使只有粗粒度信息也能重建有效常数系数。


<details>
  <summary>Details</summary>
Motivation: 在只有部分信息（全局能量）的情况下，如何从具有高度振荡系数的扩散方程中重建有效常数系数。虽然微观结构重建是病态问题，但有效系数的重建是可行的。

Method: 采用非凸优化问题的策略，基于均匀化理论提供严格基础。通过部分信息（全局能量）来近似具有高度振荡系数的扩散方程，用常数系数扩散方程来近似。

Result: 证明了即使只有粗粒度信息，有效系数的重建也是可能的。提供了全面的数值示例，展示了该策略的实际应用价值。

Conclusion: 该方法改进了先前的工作，为从部分信息重建有效系数提供了可行的策略，具有实际应用价值。

Abstract: We approximate a diffusion equation with highly oscillatory coefficients with a diffusion equation with constant coefficients. The approach is put in action in contexts where only partial information (namely the global energy stored in the physical system) is available. While the reconstruction of the microstructure is known to be an ill-posed problem, we show that the reconstruction of effective coefficients is possible and this even with only some coarse information. The strategy we present takes the form of a non-convex optimization problem. Homogenization theory provides elements for a rigorous foundation of the approach. Some algorithmic aspects are discussed in details. We provide a comprehensive set of numerical illustrations that demonstrate the practical interest of our strategy. The present work improves on the earlier works [C. Le Bris, F. Legoll and S. Lemaire, COCV 2018; C. Le Bris, F. Legoll and K. Li, CRAS 2013].

</details>


### [279] [Numerical exploration of the range of shape functionals using neural networks](https://arxiv.org/abs/2602.14881)
*Eloi Martinet,Ilias Ftouhi*

Main category: math.OC

TL;DR: 提出一种基于可逆神经网络的数值框架，用于探索Blaschke-Santaló图，该图能描述形状泛函间的不等式关系。方法通过规范函数参数化凸体，并利用粒子系统均匀采样图区域。


<details>
  <summary>Details</summary>
Motivation: Blaschke-Santaló图是描述形状泛函间不等式关系的有效工具，但传统方法难以在任意维度下高效探索这些图。需要一种能保持凸性、实现均匀采样并处理几何与PDE型泛函的数值框架。

Method: 1. 使用基于规范函数的可逆神经网络参数化任意维度凸体，内在保持凸性；2. 引入交互粒子系统，通过PyTorch自动微分最小化Riesz能量泛函，实现图内均匀采样；3. 应用于二维和三维凸体。

Result: 方法在多个涉及几何和PDE型泛函的图中得到验证，包括体积、周长、转动惯量、扭转刚度、Willmore能量以及Laplacian算子的前两个Neumann特征值，展示了框架的有效性。

Conclusion: 该数值框架成功解决了Blaschke-Santaló图的探索问题，通过神经网络参数化和粒子系统采样，为研究形状泛函间的不等式关系提供了有效工具，适用于任意维度和多种泛函类型。

Abstract: We introduce a novel numerical framework for the exploration of Blaschke--Santaló diagrams, which are efficient tools characterizing the possible inequalities relating some given shape functionals. We introduce a parametrization of convex bodies in arbitrary dimensions using a specific invertible neural network architecture based on gauge functions, allowing an intrinsic conservation of the convexity of the sets during the shape optimization process. To achieve a uniform sampling inside the diagram, and thus a satisfying description of it, we introduce an interacting particle system that minimizes a Riesz energy functional via automatic differentiation in PyTorch. The effectiveness of the method is demonstrated on several diagrams involving both geometric and PDE-type functionals for convex bodies of $\mathbb{R}^2$ and $\mathbb{R}^3$, namely, the volume, the perimeter, the moment of inertia, the torsional rigidity, the Willmore energy, and the first two Neumann eigenvalues of the Laplacian.

</details>


### [280] [Pattern preservation in finite to infinite-horizon optimal control problems for dissipative systems](https://arxiv.org/abs/2602.14944)
*Matteo Della Rossa,Thiago Alves Lima,Lorenzo Freddi*

Main category: math.OC

TL;DR: 论文研究了耗散系统无限时域最优控制问题，证明了在存在强制存储函数时，耗散性可保证无限时域最优控制可通过有限时域最优控制的极限获得，这一性质称为模式保持。


<details>
  <summary>Details</summary>
Motivation: 建立耗散系统无限时域最优控制与有限时域公式之间的联系，为验证模式保持性质提供理论依据和数值可处理的充分条件。

Method: 利用耗散性理论和变分收敛框架，分析存在强制存储函数时耗散系统的性质，建立无限时域最优控制作为有限时域最优控制极限的收敛性条件。

Result: 证明了对于一大类问题，耗散性（存在强制存储函数）可保证无限时域最优控制可通过有限时域最优控制的极限获得，即模式保持性质成立。

Conclusion: 该研究建立了耗散性理论与最优控制变分收敛框架之间的形式联系，为验证模式保持性质提供了具体且数值可处理的充分条件，并通过数值算例展示了所提条件的有效性和局限性。

Abstract: This paper focuses on infinite-horizon optimal control problems for dissipative systems and the relations to their finite-horizon formulations. We show that, for a large class of problems, dissipativity of the state equation, when a coercive storage function exists, implies that infinite-horizon optimal controls can be obtained as limits of the corresponding finite-horizon ones. This property is referred to as pattern preservation, or pattern-preserving property.
  Our analysis establishes a formal link between dissipativity theory and the variational convergence framework in optimal control, thus providing a concrete and numerically tractable condition for verifying pattern preservation. Numerical examples illustrate the effectiveness and limitations of the proposed sufficient conditions.

</details>


### [281] [Max-Min Bilinear Completely Positive Programs: A Semidefinite Relaxation with Tightness Guarantees](https://arxiv.org/abs/2602.14949)
*Sarah Yini Gao,Xindong Tang,Yancheng Yuan*

Main category: math.OC

TL;DR: 本文提出了一种处理完全正锥上最大-最小双线性优化问题的层次化半定松弛方法，通过COP-CP锥的等价线性重构解决混合二元二次鞍点问题，并在循环Colonel Blotto游戏中验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 最大-最小双线性优化模型作为优化理论中的典型鞍点公式，广泛应用于两人零和博弈、鲁棒优化、分布鲁棒优化和对抗机器学习等领域。然而，当变量位于完全正锥时，这类问题变得特别具有挑战性，需要新的理论框架和计算方法。

Method: 将完全正锥上的最大-最小问题等价转化为COP-CP锥上的单阶段线性重构问题。针对COP锥成员测试的NP难特性，开发了基于COP和CP锥的矩和平方和表示的层次化半定松弛方法，并应用平截断条件来证明紧致性。

Result: 在温和条件下保证了松弛层次的紧致性。该框架扩展了现有的分布鲁棒优化和多项式博弈的CP/COP方法。在循环Colonel Blotto游戏的多个实例中，半定松弛满足平截断条件，并精确求解了混合策略均衡。

Conclusion: 提出的COP-CP框架为完全正锥上的最大-最小双线性优化问题提供了有效的层次化半定松弛方法，能够处理混合二元二次鞍点问题，并在经典博弈问题中展示了实际应用价值。

Abstract: Max-min bilinear optimization models, where one agent maximizes and an adversary minimizes a common bilinear objective, serve as canonical saddle-point formulations in optimization theory. They capture, among others, two-player zero-sum games, robust and distributionally robust optimization, and adversarial machine learning. This study focuses on the subclass whose variables lie in the completely positive (CP) cone, capturing a broad family of mixed-binary quadratic max-min problems through the modelling power of completely positive programming. We show that such problems admit an equivalent single-stage linear reformulation over the COP-CP cone, defined as the Cartesian product of the copositive (COP) and CP cones. Because testing membership in COP cones is co-NP-complete, the resulting COP-CP program inherits NP-hardness. To address this challenge, we develop a hierarchy of semidefinite relaxations based on moment and sum-of-squares representations of the COP and CP cones, and flat truncation conditions are applied to certify the tightness. We show that the tightness of the hierarchy is guaranteed under mild conditions. The framework extends existing CP/COP approaches for distributionally robust optimization and polynomial games. We apply the framework to the cyclic Colonel Blotto game, an extension of Borel's classic allocation contest. Across multiple instances, the semidefinite relaxation meets the flat-truncation conditions and solves the exact mixed-strategy equilibrium.

</details>


### [282] [ALiA: Adaptive Linearized ADMM](https://arxiv.org/abs/2602.15000)
*Uijeong Jang,Kaizhao Sun,Wotao Yin,Ernest K Ryu*

Main category: math.OC

TL;DR: ALiA是一种自适应ADMM变体，通过函数线性化和自适应步长选择，无需回溯线搜索，在理论和实践中都表现出加速收敛性能。


<details>
  <summary>Details</summary>
Motivation: 传统ADMM及其变体（如FLiP ADMM）通常需要手动调整步长或使用回溯线搜索，这影响了算法的实用性和收敛速度。受自适应梯度和近端方法的启发，作者希望开发一种自适应ADMM变体，既能保证收敛性，又能提高收敛速度。

Method: ALiA基于函数线性化近端ADMM（FLiP ADMM），通过引入自适应步长选择方案，无需回溯线搜索。作者还开发了另一种步长选择方案，以微小的计算开销进一步加速收敛。

Result: 理论证明了ALiA在凸可微目标函数下的点收敛性。实验表明，ALiA在多个实际数据集上比标准FLiP ADMM收敛更快，并且在适用的问题类别中，其性能优于或匹配现有自适应方法。

Conclusion: ALiA是一种高效的自适应ADMM变体，结合了理论保证和实际性能优势，为大规模优化问题提供了有前景的解决方案。

Abstract: We propose ALiA, a novel adaptive variant of the alternating direction method of multipliers (ADMM). Specifically, ALiA is a variant of function-linearized proximal ADMM (FLiP ADMM), which generalizes the classical ADMM by leveraging the differentiable structure of the objective function, making it highly versatile. Notably, ALiA features an adaptive stepsize selection scheme that eliminates the need for backtracking linesearch. Motivated by recent advances in adaptive gradient and proximal methods, we establish point convergence of ALiA for convex and differentiable objectives. Furthermore, by introducing negligible computational overhead, we develop an alternative stepsize selection scheme for ALiA that improves the convergence speed both theoretically and empirically. Extensive numerical experiments on practical datasets confirm the accelerated performance of ALiA compared to standard FLiP ADMM. Additionally, we demonstrate that ALiA either outperforms or matches the practical performance of existing adaptive methods across problem classes where it is applicable.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [283] [Enhanced Accessibility for Mobile Indoor Navigation](https://arxiv.org/abs/2602.13233)
*Johannes Wortmann,Bernd Schäufele,Konstantin Klipp,Ilja Radusch,Katharina Blaß,Thomas Jung*

Main category: cs.CY

TL;DR: 为视障人士开发了一款室内导航应用，通过用户访谈和WCAG指南分析确定设计需求，应用强调可访问性并集成了增强功能，初步用户测试获得积极反馈。


<details>
  <summary>Details</summary>
Motivation: 视障人士在室内导航面临多重挑战：需要处理感官信息、应对不确定性并依赖他人帮助。现有导航解决方案对视障用户的可访问性关注不足，需要专门设计来满足他们的特殊需求。

Method: 采用用户访谈和Web内容可访问性指南(WCAG)分析相结合的方法，收集关键见解并确定室内导航应用的设计需求。基于这些洞察开发了优先考虑可访问性的应用，并集成增强功能。通过视障用户和视力正常用户的测试来全面评估应用可用性。

Result: 初步反馈积极，用户赞赏应用的包容性用户界面，以及与各种可访问性工具和Android设备设置的兼容性。应用成功满足了视障用户的导航需求。

Conclusion: 通过用户中心的设计方法和可访问性指南分析，成功开发了一款满足视障人士需求的室内导航应用。该方法强调了在导航技术开发中考虑可访问性的重要性，为未来类似应用提供了设计参考。

Abstract: The navigation of indoor spaces poses difficult challenges for individuals with visual impairments, as it requires processing of sensory information, dealing with uncertainties, and relying on assistance. To tackle these challenges, we present an indoor navigation app that places importance on accessibility for visually impaired users. Our approach involves a combination of user interviews and an analysis of the Web Content Accessibility Guidelines. With this approach, we are able to gather invaluable insights and identify design requirements for the development of an indoor navigation app. Based on these insights, we develop an indoor navigation app that prioritizes accessibility, integrating enhanced features to meet the needs of visually impaired users. The usability of the app is being thoroughly evaluated through tests involving both visually impaired and sighted users. Initial feedback has been positive, with users appreciating the inclusive user interface and the usability with a wide range of accessibility tools and Android device settings.

</details>


### [284] [CrisiSense-RAG: Crisis Sensing Multimodal Retrieval-Augmented Generation for Rapid Disaster Impact Assessment](https://arxiv.org/abs/2602.13239)
*Yiming Xiao,Kai Yin,Ali Mostafavi*

Main category: cs.CY

TL;DR: CrisiSense-RAG是一个多模态检索增强生成框架，通过异步融合逻辑处理时间错位的灾害数据，优先使用实时社交媒体证据评估洪水峰值范围，同时利用卫星图像评估结构损坏，在零样本设置下实现了准确的灾害影响评估。


<details>
  <summary>Details</summary>
Motivation: 现有自动化灾害影响评估方法面临时间异步性问题：实时人类报告捕捉灾害峰值条件，而高分辨率卫星图像通常在峰值后获取，反映的是洪水消退而非最大范围。简单融合这些错位数据流会导致对峰值洪水的危险低估。

Method: 提出CrisiSense-RAG多模态检索增强生成框架，将影响评估重新定义为异构数据源的证据合成。系统采用混合密集-稀疏检索处理文本源，基于CLIP的检索处理航拍图像。采用分割管道架构和异步融合逻辑，优先使用实时社交证据评估峰值洪水范围，同时将图像视为结构损坏的持久证据。

Result: 在飓风哈维的207个ZIP码查询评估中，框架在零样本设置下实现了洪水范围MAE为10.94%至28.40%，损坏严重程度MAE为16.47%至21.65%。提示级对齐对定量有效性至关重要，指标基础化将损坏估计提高了最多4.75个百分点。

Conclusion: CrisiSense-RAG展示了在现实世界数据约束下实现快速韧性智能的实用可部署方法，通过异步融合逻辑有效处理时间错位数据，为灾害响应提供了更准确的评估框架。

Abstract: Timely and spatially resolved disaster impact assessment is essential for effective emergency response. However, automated methods typically struggle with temporal asynchrony. Real-time human reports capture peak hazard conditions while high-resolution satellite imagery is frequently acquired after peak conditions. This often reflects flood recession rather than maximum extent. Naive fusion of these misaligned streams can yield dangerous underestimates when post-event imagery overrides documented peak flooding. We present CrisiSense-RAG, which is a multimodal retrieval-augmented generation framework that reframes impact assessment as evidence synthesis over heterogeneous data sources without disaster-specific fine-tuning. The system employs hybrid dense-sparse retrieval for text sources and CLIP-based retrieval for aerial imagery. A split-pipeline architecture feeds into asynchronous fusion logic that prioritizes real-time social evidence for peak flood extent while treating imagery as persistent evidence of structural damage. Evaluated on Hurricane Harvey across 207 ZIP-code queries, the framework achieves a flood extent MAE of 10.94% to 28.40% and damage severity MAE of 16.47% to 21.65% in zero-shot settings. Prompt-level alignment proves critical for quantitative validity because metric grounding improves damage estimates by up to 4.75 percentage points. These results demonstrate a practical and deployable approach to rapid resilience intelligence under real-world data constraints.

</details>


### [285] [Real-World Design and Deployment of an Embedded GenAI-powered 9-1-1 Calltaking Training System: Experiences and Lessons Learned](https://arxiv.org/abs/2602.13241)
*Zirong Chen,Meiyi Ma*

Main category: cs.CY

TL;DR: 论文开发并部署了一个基于生成式AI的紧急呼叫接听员培训系统，在真实环境中解决公共安全部门的培训危机，通过6个月的实际部署总结了关键经验教训。


<details>
  <summary>Details</summary>
Motivation: 公共安全应急呼叫中心面临严重的培训危机：人员短缺超过25%，培训一名新员工需要720小时的一对一指导，这导致有经验的人员无法执行任务。传统培训方法难以扩展，限制了覆盖范围和反馈及时性。

Method: 与Metro Nashville Department of Emergency Communications合作，设计、开发并部署了一个基于生成式AI的呼叫接听培训系统。通过6个月的实际部署，从试点扩展到190名操作用户，进行了1,120次培训会话。分析了98,429次用户交互的部署日志、组织流程和利益相关者参与模式。

Result: 部署揭示了在受控或纯模拟评估中通常不可见的系统性挑战，包括系统交付、严谨性、韧性和人为因素等方面的问题。通过分析总结出四个关键经验教训，每个都配有具体的设计和治理实践。

Conclusion: 研究为寻求在安全关键的公共部门环境中嵌入AI驱动培训系统的研究人员和实践者提供了基于实际经验的指导，强调了嵌入式约束如何从根本上塑造社会技术设计。

Abstract: Emergency call-takers form the first operational link in public safety response, handling over 240 million calls annually while facing a sustained training crisis: staffing shortages exceed 25\% in many centers, and preparing a single new hire can require up to 720 hours of one-on-one instruction that removes experienced personnel from active duty. Traditional training approaches struggle to scale under these constraints, limiting both coverage and feedback timeliness. In partnership with Metro Nashville Department of Emergency Communications (MNDEC), we designed, developed, and deployed a GenAI-powered call-taking training system under real-world constraints. Over six months, deployment scaled from initial pilot to 190 operational users across 1,120 training sessions, exposing systematic challenges around system delivery, rigor, resilience, and human factors that remain largely invisible in controlled or purely simulated evaluations. By analyzing deployment logs capturing 98,429 user interactions, organizational processes, and stakeholder engagement patterns, we distill four key lessons, each coupled with concrete design and governance practices. These lessons provide grounded guidance for researchers and practitioners seeking to embed AI-driven training systems in safety-critical public sector environments where embedded constraints fundamentally shape socio-technical design.

</details>


### [286] [AI Unplugged: Embodied Interactions for AI Literacy in Higher Education](https://arxiv.org/abs/2602.13242)
*Jennifer M. Reddig,Scott Moon,Kaitlyn Crutcher,Christopher J. MacLellan*

Main category: cs.CY

TL;DR: 该论文提出了一种在大学AI课程中融入实体化、非编程活动的教学方法，通过互动游戏帮助学生理解复杂AI概念，然后过渡到数学形式化和代码实现。


<details>
  <summary>Details</summary>
Motivation: 随着AI日益融入日常生活，高等教育需要超越以代码为中心的教学，培养全面的AI素养。传统大学AI课程过于注重技术实现，缺乏对AI决策过程的直观理解。

Method: 借鉴K-12教育中CS Unplugged的成功经验，设计了四种实体化、协作式的非编程活动，模拟搜索算法、马尔可夫决策过程、Q学习和隐马尔可夫模型。通过互动游戏让学生获得AI决策的第一人称视角，然后逐步过渡到数学形式化和代码实现。

Result: 学生通过非编程活动建立了对复杂AI概念的直观理解，更容易过渡到数学形式化和代码实现。论文展示了四种具体的非编程AI活动，描述了如何从非编程活动过渡到编程任务，并反思了实施挑战和改进建议。

Conclusion: 非编程活动能够有效连接概念推理和技术技能培养，在大学级别的AI教育中具有重要价值。这种方法有助于培养全面的AI素养，而不仅仅是编程技能。

Abstract: As artificial intelligence (AI) becomes increasingly integrated into daily life, higher education must move beyond code-centric instruction to foster holistic AI literacy. We present a novel pedagogical approach that integrates embodied, unplugged activities into a university-level Introduction to AI course. Inspired by the effectiveness of CS Unplugged in K-12 education, our physical, collaborative activities gave students a first-person perspective on AI decision-making. Through interactive games modeling Search Algorithms, Markov Decision Processes, Q-learning, and Hidden Markov Models, students built an intuition for complex AI concepts and more easily transitioned to mathematical formalizations and code implementations. We present four unplugged AI activities, describe how to bridge from unplugged activities to plugged coding tasks, reflect on implementation challenges, and propose refinements. We suggest that unplugged activities can effectively bridge conceptual reasoning and technical skill-building in university-level AI education.

</details>


### [287] [Judging the Judges: Human Validation of Multi-LLM Evaluation for High-Quality K--12 Science Instructional Materials](https://arxiv.org/abs/2602.13243)
*Peng He,Zhaohui Li,Zeyuan Wang,Jinjun Xiong,Tingting Li*

Main category: cs.CY

TL;DR: 研究探索专家如何审查AI生成的K-12科学教学材料评估，旨在为未来GenAI教学材料设计代理制定设计原则。


<details>
  <summary>Details</summary>
Motivation: 设计高质量的K-12科学教学材料耗时且需要专业知识，研究旨在通过分析专家对AI评估的反馈，为开发AI辅助教学材料设计工具提供指导。

Method: 选取12个高质量科学课程单元，使用EQuIP评估框架，让GPT-4o、Claude和Gemini生成评分和理由，两位专家独立审查所有648个输出，标记同意/不同意并提供定性反思。

Result: 揭示了LLM判断与专家观点的一致性和分歧模式，识别了AI推理的优势、差距和情境细微差别，为开发领域特定GenAI代理提供直接见解。

Conclusion: 研究结果将直接指导开发专门用于K-12科学教育高质量教学材料设计的GenAI代理，提升AI在教学材料评估和设计中的有效性。

Abstract: Designing high-quality, standards-aligned instructional materials for K--12 science is time-consuming and expertise-intensive. This study examines what human experts notice when reviewing AI-generated evaluations of such materials, aiming to translate their insights into design principles for a future GenAI-based instructional material design agent. We intentionally selected 12 high-quality curriculum units across life, physical, and earth sciences from validated programs such as OpenSciEd and Multiple Literacies in Project-based Learning. Using the EQuIP rubric with 9 evaluation items, we prompted GPT-4o, Claude, and Gemini to produce numerical ratings and written rationales for each unit, generating 648 evaluation outputs. Two science education experts independently reviewed all outputs, marking agreement (1) or disagreement (0) for both scores and rationales, and offering qualitative reflections on AI reasoning. This process surfaces patterns in where LLM judgments align with or diverge from expert perspectives, revealing reasoning strengths, gaps, and contextual nuances. These insights will directly inform the development of a domain-specific GenAI agent to support the design of high-quality instructional materials in K--12 science education.

</details>


### [288] [Responsible AI in Business](https://arxiv.org/abs/2602.13244)
*Stephan Sandfuchs,Diako Farooghi,Janis Mohr,Sarah Grewe,Markus Lemmen,Jörg Frochte*

Main category: cs.CY

TL;DR: 论文提出面向中小企业的负责任AI实践框架，涵盖欧盟AI法案合规、可解释AI、绿色AI和本地模型部署四大支柱，旨在帮助组织以合法、透明、可持续和数据主权的方式运营AI系统。


<details>
  <summary>Details</summary>
Motivation: 随着AI/ML从研究走向商业应用，特别是生成式AI加速普及，中小企业需要实用的负责任AI框架来确保AI系统在法律合规、可理解、可持续和数据主权方面的要求，而现有研究往往缺乏针对中小企业的具体指导。

Method: 构建四支柱框架：1) 基于欧盟AI法案的风险监管框架分析；2) 可解释AI方法澄清透明度、可解释性等概念并提供实践方法；3) 绿色AI强调性能与能耗平衡，提出模型复用、高效适配等杠杆；4) 本地模型部署支持数据保护、低延迟和战略独立。

Result: 建立了综合性的负责任AI实践框架，为中小企业提供了从法律合规到技术实施的全方位指导，包括风险评估、文档要求、透明度措施、AI素养提升、能效优化和本地部署策略。

Conclusion: 论文提出了一套完整的负责任AI实施路径，强调建立治理结构、完善文档、确保安全运营、考虑可持续性，并制定具体实施路线图，帮助中小企业在AI时代实现负责任的技术采用。

Abstract: Artificial intelligence (AI) and Machine Learning (ML) have moved from research and pilot projects into everyday business operations, with generative AI accelerating adoption across processes, products, and services. This paper introduces the concept of Responsible AI for organizational practice, with a particular focus on small and medium-sized enterprises. It structures Responsible AI along four focal areas that are central for introducing and operating AI systems in a legally compliant, comprehensible, sustainable, and data-sovereign manner. First, it discusses the EU AI Act as a risk-based regulatory framework, including the distinction between provider and deployer roles and the resulting obligations such as risk assessment, documentation, transparency requirements, and AI literacy measures. Second, it addresses Explainable AI as a basis for transparency and trust, clarifying key notions such as transparency, interpretability, and explainability and summarizing practical approaches to make model behavior and decisions more understandable. Third, it covers Green AI, emphasizing that AI systems should be evaluated not only by performance but also by energy and resource consumption, and outlines levers such as model reuse, resource-efficient adaptation, continuous learning, model compression, and monitoring. Fourth, it examines local models (on-premise and edge) as an operating option that supports data protection, control, low latency, and strategic independence, including domain adaptation via fine-tuning and retrieval-augmented generation. The paper concludes with a consolidated set of next steps for establishing governance, documentation, secure operation, sustainability considerations, and an implementation roadmap.

</details>


### [289] [Global AI Bias Audit for Technical Governance](https://arxiv.org/abs/2602.13246)
*Jason Hung*

Main category: cs.CY

TL;DR: 对Llama-3 8B模型的全球审计发现，AI技术知识高度集中在高收入地区，全球南方国家面临系统性信息差距，这加剧了全球AI安全风险。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型中的地理和社会经济偏见，特别是技术AI治理意识方面的偏见，以了解AI知识在全球的分布不平等问题。

Method: 使用全球AI数据集(GAID)框架对Llama-3 8B模型进行压力测试，在213个国家进行1,704个查询，涵盖八个技术指标，分析模型回答的可用性和准确性。

Result: 模型仅能在11.4%的查询中提供数字/事实回答，且这些回答的实证有效性尚未验证。AI技术知识高度集中在高收入地区，全球南方国家面临不成比例的系统性信息差距。

Conclusion: 当前的AI对齐和训练过程强化了现有的地缘经济和地缘政治不对称，需要更包容的数据表示来确保AI成为真正的全球资源，避免全球南方政策制定者被误导。

Abstract: This paper presents the outputs of the exploratory phase of a global audit of Large Language Models (LLMs) project. In this exploratory phase, I used the Global AI Dataset (GAID) Project as a framework to stress-test the Llama-3 8B model and evaluate geographic and socioeconomic biases in technical AI governance awareness. By stress-testing the model with 1,704 queries across 213 countries and eight technical metrics, I identified a significant digital barrier and gap separating the Global North and South. The results indicate that the model was only able to provide number/fact responses in 11.4% of its query answers, where the empirical validity of such responses was yet to be verified. The findings reveal that AI's technical knowledge is heavily concentrated in higher-income regions, while lower-income countries from the Global South are subject to disproportionate systemic information gaps. This disparity between the Global North and South poses concerning risks for global AI safety and inclusive governance, as policymakers in underserved regions may lack reliable data-driven insights or be misled by hallucinated facts. This paper concludes that current AI alignment and training processes reinforce existing geoeconomic and geopolitical asymmetries, and urges the need for more inclusive data representation to ensure AI serves as a truly global resource.

</details>


### [290] [Implicit Bias in LLMs for Transgender Populations](https://arxiv.org/abs/2602.13253)
*Micaela Hirsch,Marina Elichiry,Blas Radi,Tamara Quiroga,David Restrepo,Luciana Benotti,Veronica Xhardez,Jocelyn Dunstan,Enzo Ferrante*

Main category: cs.CY

TL;DR: LLMs对跨性别者存在隐性偏见，在词语关联测试中显示负面关联，在医疗预约分配任务中表现出刻板印象驱动的分配模式。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs的安全训练可能减少对LGBTQ+群体的显性偏见表达，但隐性刻板印象关联往往持续存在。本研究特别关注LLMs对跨性别者的隐性偏见，尤其是在医疗决策场景中，因为跨性别者在现实医疗环境中面临系统性挑战。

Method: 采用两种方法：1) 改编词语关联测试，测量LLMs是否不成比例地将负面概念与"跨性别"、正面概念与"顺性别"配对；2) 设计医疗预约分配任务，让模型作为调度代理在易受刻板印象影响的医疗专科中选择跨性别和顺性别候选人。评估了英语和西班牙语的七个LLMs。

Result: 结果显示：1) 在外观、风险和真实性等类别中存在一致的偏见，表明对跨性别个体有更强的负面关联；2) 在分配任务中，跨性别候选人更倾向于被分配给性传播感染和心理健康服务，而顺性别候选人更受妇科和乳房护理的青睐。

Conclusion: 研究结果强调了需要解决LLMs中微妙的刻板印象驱动偏见，以确保跨性别者在医疗应用中获得公平对待。隐性偏见可能持续存在，即使经过安全训练，需要进一步研究来减轻这些偏见。

Abstract: Large language models (LLMs) have been shown to exhibit biases against LGBTQ+ populations. While safety training may lessen explicit expressions of bias, previous work has shown that implicit stereotype-driven associations often persist. In this work, we examine implicit bias toward transgender people in two main scenarios. First, we adapt word association tests to measure whether LLMs disproportionately pair negative concepts with "transgender" and positive concepts with "cisgender". Second, acknowledging the well-documented systemic challenges that transgender people encounter in real-world healthcare settings, we examine implicit biases that may emerge when LLMs are applied to healthcare decision-making. To this end, we design a healthcare appointment allocation task where models act as scheduling agents choosing between cisgender and transgender candidates across medical specialties prone to stereotyping. We evaluate seven LLMs in English and Spanish. Our results show consistent bias in categories such as appearance, risk, and veracity, indicating stronger negative associations with transgender individuals. In the allocation task, transgender candidates are favored for STI and mental health services, while cisgender candidates are preferred in gynecology and breast care. These findings underscore the need for research that address subtle stereotype-driven biases in LLMs to ensure equitable treatment of transgender people in healthcare applications.

</details>


### [291] [Expected Moral Shortfall for Ethical Competence in Decision-making Models](https://arxiv.org/abs/2602.13268)
*Aisha Aijaz,Raghava Mutharaju,Manohar Kumar*

Main category: cs.CY

TL;DR: 本文提出了一种新的道德认知框架Expected Moral Shortfall (EMS)，用于指导AI模型在保持伦理能力的同时最大化性能，并分析了模型性能、复杂性与伦理能力之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 道德认知是AI决策中至关重要但尚未充分探索的方面。无论应用领域如何，伦理对齐的决策都应该是AI模型的考虑因素。当前需要系统的方法来评估和提升AI模型的伦理能力。

Method: 1. 对现有AI伦理能力培养技术进行多指标比较分析；2. 提出道德数学离散化的新方法，并在两个数据集上测试；3. 引入Expected Moral Shortfall (EMS)概念，定义为最不道德案例带来的损失风险，指导AI模型最小化EMS以在保持伦理能力的同时最大化性能。

Result: 1. 提供了AI伦理技术的系统比较分析；2. 提出的道德离散化方法在实际应用中得到了验证；3. EMS框架被证明能有效平衡模型性能与伦理能力；4. 识别了模型性能、复杂性与伦理能力之间的实际权衡关系。

Conclusion: 本文为AI道德认知研究提供了多方面的贡献，包括系统分析方法、新的数学框架和实际应用验证。EMS框架为AI模型在保持伦理对齐的同时优化性能提供了可行路径，并强调了理解性能-伦理权衡对实际社会影响的重要性。

Abstract: Moral cognition is a crucial yet underexplored aspect of decision-making in AI models. Regardless of the application domain, it should be a consideration that allows for ethically aligned decision-making. This paper presents a multifaceted contribution to this research space. Firstly, a comparative analysis of techniques to instill ethical competence into AI models has been presented to gauge them on multiple performance metrics. Second, a novel mathematical discretization of morality and a demonstration of its real-life application have been conveyed and tested against other techniques on two datasets. This value is modeled as the risk of loss incurred by the least moral cases, or an Expected Moral Shortfall (EMS), which we direct the AI model to minimize in order to maximize its performance while retaining ethical competence. Lastly, the paper discusses the tradeoff between preliminary AI decision-making metrics such as model performance, complexity, and scale of ethical competence to recognize the true extent of practical social impact.

</details>


### [292] [The Rise of AI Search: Implications for Information Markets and Human Judgement at Scale](https://arxiv.org/abs/2602.13415)
*Sinan Aral,Haiwen Li,Rui Zuo*

Main category: cs.CY

TL;DR: 该研究通过24,000次搜索查询发现，2024-2025年间AI搜索在全球快速扩张，但存在国家排除现象；新冠查询的AI回答率从1%激增至66%；AI搜索呈现信息来源集中化、多样性降低、低可信度内容增多等趋势，对信息生态系统产生深远影响。


<details>
  <summary>Details</summary>
Motivation: 研究动机是揭示AI搜索在全球范围内的扩张趋势及其背后的政策决策，这些决策直接影响人类接触AI搜索信息的方式和范围，但目前这些变化缺乏透明度和公开讨论。

Method: 研究方法包括在243个国家执行24,000次搜索查询，收集2024年和2025年的280万条AI和传统搜索结果，通过对比分析揭示AI搜索的扩张模式、国家排除情况、特定主题（如新冠）的覆盖变化，以及信息来源多样性和可信度的差异。

Result: 主要发现：1) Google AI Overviews从7个国家扩展到229个国家，但排除了法国、土耳其、中国、古巴等国家；2) 新冠查询的AI回答率从1%激增至66%（增长5600%）；3) AI搜索呈现信息来源集中化、多样性降低、低可信度内容增多，且偏向右翼和中间派信息来源。

Conclusion: 结论认为AI搜索的快速变化对信息生态系统产生了深远的社会经济影响，包括影响新信息生产的经济激励、信息生产市场集中度，以及大规模的人类判断和决策，因此需要全球范围内关于AI搜索的企业和政府政策讨论。

Abstract: We executed 24,000 search queries in 243 countries, generating 2.8 million AI and traditional search results in 2024 and 2025. We found a rapid global expansion of AI search and key trends that reflect important, previously hidden, policy decisions by AI companies that impact human exposure to AI search worldwide. From 2024 to 2025, overall exposure to Google AI Overviews (AIO) expanded from 7 to 229 countries, with surprising exclusions like France, Turkey, China and Cuba, which do not receive AI search results, even today. While only 1% of Covid search queries were answered by AI in 2024, over 66% of Covid queries were answered by AI in 2025 -- a 5600% increase signaling a clear policy shift on this critical health topic. Our results also show AI search surfaces significantly fewer long tail information sources, lower response variety, and significantly more low credibility and right- and center-leaning information sources, compared to traditional search, impacting the economic incentives to produce new information, market concentration in information production, and human judgment and decision-making at scale. The social and economic implications of these rapid changes in our information ecosystem necessitate a global debate about corporate and governmental policy related to AI search.

</details>


### [293] [Future of Edge AI in biodiversity monitoring](https://arxiv.org/abs/2602.13496)
*Aude Vuilliomenet,Kate E. Jones,Duncan Wilson*

Main category: cs.CY

TL;DR: 该论文系统分析了2017-2025年间82项边缘计算在生物多样性监测中的应用研究，总结了四种系统类型及其在功耗、计算能力和通信需求间的权衡，指出边缘计算从概念验证向实用工具发展的趋势。


<details>
  <summary>Details</summary>
Motivation: 生态决策常因生物多样性数据收集与分析之间的延迟而受阻。边缘计算将处理移至传感器附近，边缘AI支持设备端推理，减少对数据传输和持续连接的依赖，理论上可将生物多样性监测从被动记录转向自主响应式传感系统。但实际应用中，关键架构权衡、性能约束和实施挑战缺乏系统报告，阻碍了技术采用。

Method: 分析了2017-2025年间发表的82项边缘计算应用于生物多样性监测的研究，涵盖声学、视觉、追踪和多模态系统。综合评估了硬件平台、AI模型优化和无线通信，分析设计选择如何影响生态推断、部署寿命和操作可行性。

Result: 识别出四种系统类型：1) TinyML（微控制器）用于单分类群或稀有事件检测；2) 边缘AI（单板计算机）用于多物种分类和实时警报；3) 分布式边缘AI；4) 云AI用于回顾性处理流水线。每种类型在功耗、计算能力和通信需求间存在权衡。研究发现相关出版物从2017年的3篇增加到2025年的19篇，显示边缘计算系统正从概念验证向稳健、可扩展工具发展。

Conclusion: 边缘计算为响应式生物多样性管理提供了机遇，但实现这一潜力需要生态学家、工程师和数据科学家之间加强合作，使模型开发和系统设计与生态问题、现场约束和伦理考量保持一致。

Abstract: 1. Many ecological decisions are slowed by the gap between collecting and analysing biodiversity data. Edge computing moves processing closer to the sensor, with edge artificial intelligence (AI) enabling on-device inference, reducing reliance on data transfer and continuous connectivity. In principle, this shifts biodiversity monitoring from passive logging towards autonomous, responsive sensing systems. In practice, however, adoption remains fragmented, with key architectural trade-offs, performance constraints, and implementation challenges rarely reported systematically. 2. Here, we analyse 82 studies published between 2017 and 2025 that implement edge computing for biodiversity monitoring across acoustic, vision, tracking, and multi-modal systems. We synthesise hardware platforms, AI model optimisation, and wireless communication to critically assess how design choices shape ecological inference, deployment longevity, and operational feasibility. 3. Publications increased from 3 in 2017 to 19 in 2025. We identify four system types: (I) TinyML, low-power microcontrollers (MCUs) for single-taxon or rare-event detection; (II) Edge AI, single-board computers (SBCs) for multi-species classification and real-time alerts; (III) Distributed edge AI; and (IV) Cloud AI for retrospective processing pipelines. Each system type represents context-dependent trade-offs among power consumption, computational capability, and communication requirements. 4. Our analysis reveals the evolution of edge computing systems from proof-of-concept to robust, scalable tools. We argue that edge computing offers opportunities for responsive biodiversity management, but realising this potential requires increased collaboration between ecologists, engineers, and data scientists to align model development and system design with ecological questions, field constraints, and ethical considerations.

</details>


### [294] [Artificial Intelligence in Secondary Education: Educational Affordances and Constraints of ChatGPT-4o Use](https://arxiv.org/abs/2602.13717)
*Tryfon Sivenas,Panagiota Maragkaki*

Main category: cs.CY

TL;DR: 希腊高中生使用ChatGPT-4o后，识别出AI在教育中的5个促进因素（知识构建、即时反馈等）和3个主要限制（内容可靠性、AI焦虑、隐私担忧），总体对AI教育应用持积极态度。


<details>
  <summary>Details</summary>
Motivation: 研究旨在从中学教育学生的视角，探讨人工智能在教学和学习中的教育促进因素和限制，了解学生对AI教育应用的看法和体验。

Method: 研究采用质性方法，样本为45名希腊普通高中二年级学生（16-17岁）。学生先熟悉ChatGPT-4o并完成六项活动，然后填写开放式问卷。数据通过开放编码、轴向编码和选择性编码进行分析。

Result: 学生识别出五个教育促进因素：基于先前知识构建新知识、即时反馈、通过消息进行友好互动、信息获取的便捷性和速度、技能发展。同时识别出三个主要限制：内容可靠性、AI使用焦虑、隐私担忧。

Conclusion: 学生对AI在教育中的应用持积极态度，但需要解决内容可靠性、AI焦虑和隐私担忧等问题，以充分发挥AI在教育中的潜力。

Abstract: The purpose of this study was to examine, from the perspective of secondary education students, the educational affordances and constraints of using Artificial Intelligence (AI) in teaching and learning. The sample consisted of 45 students from the 2nd year of General Lyceum (11th grade, ages 16-17) in Greece, who, after becoming familiarized with ChatGPT-4o and completing six activities, filled in an open-ended questionnaire related to the research purpose. Open, axial, and selective coding of the data revealed that students recognize five educational affordances: the creation of new knowledge building on prior knowledge, immediate feedback, friendly interaction through messaging, ease and speed of access to information, and skills development. Concurrently, three main constraints were identified: content reliability, anxiety about AI use, and privacy concerns. The study concludes that students are positive toward AI use in education.

</details>


### [295] [Assessing the Case for Africa-Centric AI Safety Evaluations](https://arxiv.org/abs/2602.13757)
*Gathoni Ireri,Cecil Abungu,Jean Cheptumo,Sienka Dounia,Mark Gitau,Stephanie Kasaon,Michael Michie,Chinasa Okolo,Jonathan Shock*

Main category: cs.CY

TL;DR: 论文提出非洲中心化AI安全评估框架，解决西方评估方法在非洲环境中的可移植性差距问题，识别非洲特有的严重AI风险。


<details>
  <summary>Details</summary>
Motivation: 前沿AI系统正在非洲广泛采用，但大多数AI安全评估都是在西方环境中设计和验证的。这种可移植性差距可能导致非洲特有的严重危害路径未被测试，特别是在资源受限和相互依赖的基础设施环境中。

Method: 开发了识别非洲中心化严重AI风险的分类法，将结果阈值与过程路径联系起来，通过危险、脆弱性和暴露的交集来建模风险。区分了风险的放大性和突发性特征，并提出了针对非洲环境的威胁建模策略，包括参考类预测、结构化专家启发、情景规划和系统理论过程分析。

Result: 建立了非洲中心化严重AI风险分类框架，识别了非洲特有的风险特征（放大性和突发性），提出了适应非洲约束条件（资源有限、连接性差、技术专业知识不足、国家能力弱、冲突）的评估方法，并分析了AI错位风险在非洲环境中的特殊性。

Conclusion: 非洲需要专门设计的AI安全评估方法，以应对其特有的基础设施约束和风险特征。通过开放可扩展的工具、分层评估管道以及方法和发现的共享，可以在资源有限的情况下扩大评估范围，确保前沿AI系统在非洲的安全部署。

Abstract: Frontier AI systems are being adopted across Africa, yet most AI safety evaluations are designed and validated in Western environments. In this paper, we argue that the portability gap can leave Africa-centric pathways to severe harm untested when frontier AI systems are embedded in materially constrained and interdependent infrastructures. We define severe AI risks as material risks from frontier AI systems that result in critical harm, measured as the grave injury or death of thousands of people or economic loss and damage equivalent to five percent of a country's GDP. To support AI safety evaluation design, we develop a taxonomy for identifying Africa-centric severe AI risks. The taxonomy links outcome thresholds to process pathways that model risk as the intersection of hazard, vulnerability, and exposure. We distinguish severe risks by amplification and suddenness, where amplification requires that frontier AI be a necessary magnifier of latent danger and suddenness captures harms that materialise rapidly enough to overwhelm ordinary coping and governance capacity. We then propose threat modelling strategies for African contexts, surveying reference class forecasting, structured expert elicitation, scenario planning, and system theoretic process analysis, and tailoring them to constraints of limited resources, poor connectivity, limited technical expertise, weak state capacity, and conflict. We also examine AI misalignment risk, concluding that Africa is more likely to expose universal failure modes through distributional shift than to generate distinct pathways of misalignment. Finally, we offer practical guidance for running evaluations under resource constraints, emphasising open and extensible tooling, tiered evaluation pipelines, and sharing methods and findings to broaden evaluation scope.

</details>


### [296] [Reasoning Language Models for complex assessments tasks: Evaluating parental cooperation from child protection case reports](https://arxiv.org/abs/2602.14216)
*Dragan Stoll,Brian E. Perron,Zia Qi,Selina Steinmann,Nicole F. Eicher,Andreas Jud*

Main category: cs.CY

TL;DR: 本研究探讨使用推理语言模型评估儿童保护服务干预中的父母合作情况，最大模型准确率达89%，母亲评估准确率高于父亲。


<details>
  <summary>Details</summary>
Motivation: 推理语言模型在复杂推理任务中表现出色，本研究旨在探索其能否有效评估儿童保护服务干预中父母合作这一复杂案例因素，该因素通常包含模糊和冲突信息。

Method: 开发四阶段工作流程：1)收集案例报告；2)基于推理评估父母合作；3)自动类别提取；4)案例标注。比较不同参数规模（255B、32B、4B）的RLM与人类验证数据，并由两位专家独立分类加权随机样本。

Result: 最大RLM（255B）达到最高准确率89%，优于初始方法（80%）。母亲评估准确率（93%）高于父亲（85%），人类专家也表现出类似差异。

Conclusion: RLM推理能有效评估父母合作等复杂案例因素。父亲合作评估准确率较低支持了CPS干预中更关注母亲的专业倾向论点。

Abstract: Purpose: Reasoning language models (RLMs) have demonstrated significant advances in solving complex reasoning tasks. We examined their potential to assess parental cooperation during CPS interventions using case reports, a case factor characterized by ambiguous and conflicting information. Methods: A four stage workflow comprising (1) case reports collection, (2) reasoning-based assessment of parental cooperation, (3) automated category extraction, and (4) case labeling was developed. The performance of RLMs with different parameter sizes (255B, 32B, 4B) was compared against human validated data. Two expert human reviewers (EHRs) independently classified a weighted random sample of reports. Results: The largest RLM achieved the highest accuracy (89%), outperforming the initial approach (80%). Classification accuracy was higher for mothers (93%) than for fathers (85%), and EHRs exhibited similar differences. Conclusions: RLMs' reasoning can effectively assess complex case factors such as parental cooperation. Lower accuracy in assessing fathers' cooperation supports the argument of a stronger professional focus on mothers in CPS interventions.

</details>


### [297] [A Rational Analysis of the Effects of Sycophantic AI](https://arxiv.org/abs/2602.14270)
*Rafael M. Batista,Thomas L. Griffiths*

Main category: cs.CY

TL;DR: 研究发现AI的谄媚行为（sycophancy）会抑制真相发现并虚假增强用户信心，而非幻觉带来的虚假信息，这种偏见性反馈会扭曲现实认知。


<details>
  <summary>Details</summary>
Motivation: 随着人们越来越多使用大语言模型探索想法和获取信息，AI的过度迎合行为带来了独特的认知风险。与幻觉引入虚假信息不同，谄媚行为通过提供强化现有偏见的反馈来扭曲现实。

Method: 采用理性分析方法，证明贝叶斯智能体在基于当前假设采样的数据下会增强对该假设的信心但不会接近真相。通过修改的Wason 2-4-6规则发现任务进行实验，557名参与者与提供不同类型反馈的AI智能体互动。

Result: 未修改的LLM行为抑制了真相发现并虚假增强了信心，效果与明确谄媚提示相当。相比之下，从真实分布中无偏采样使发现率提高了五倍。

Conclusion: 谄媚AI扭曲了信念，在应该存在怀疑的地方制造了虚假的确定性。这揭示了AI谄媚行为如何通过强化现有偏见来影响人类认知过程。

Abstract: People increasingly use large language models (LLMs) to explore ideas, gather information, and make sense of the world. In these interactions, they encounter agents that are overly agreeable. We argue that this sycophancy poses a unique epistemic risk to how individuals come to see the world: unlike hallucinations that introduce falsehoods, sycophancy distorts reality by returning responses that are biased to reinforce existing beliefs. We provide a rational analysis of this phenomenon, showing that when a Bayesian agent is provided with data that are sampled based on a current hypothesis the agent becomes increasingly confident about that hypothesis but does not make any progress towards the truth. We test this prediction using a modified Wason 2-4-6 rule discovery task where participants (N=557) interacted with AI agents providing different types of feedback. Unmodified LLM behavior suppressed discovery and inflated confidence comparably to explicitly sycophantic prompting. By contrast, unbiased sampling from the true distribution yielded discovery rates five times higher. These results reveal how sycophantic AI distorts belief, manufacturing certainty where there should be doubt.

</details>


### [298] [Simpler Than You Think: The Practical Dynamics of Ranked Choice Voting](https://arxiv.org/abs/2602.14329)
*Sanyukta Deshpande,Nikhil Garg,Sheldon H. Jacobson*

Main category: cs.CY

TL;DR: 本文通过实证研究发现，尽管排序选择投票(RCV)理论上存在复杂性、策略操纵和选票耗尽等问题，但在实际选举中表现出简单透明的动态，能显著提高竞争性，且对策略操纵和选票耗尽具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着排序选择投票(RCV)在美国选举中的推广，人们对其复杂性、策略操纵可能性和选票耗尽问题存在持续批评。本研究旨在通过实证数据检验这些担忧是否在实际选举中成立。

Method: 使用算法方法处理计算复杂度，通过候选者消除减少选举实例规模，分析三个不同背景的真实选举数据：纽约市2021年民主党初选(54场)、阿拉斯加2024年全州选举(52场)和波特兰2024年多席位市议会选举(4场)。

Result: RCV在实践中表现出简单透明的动态，与简单多数选举的可解释性相近。采用RCV后竞争性显著提高，平均获胜优势在纽约市下降9.2个百分点，阿拉斯加下降11.4个百分点。复杂策略并不比简单策略更有效，选票耗尽影响极小，仅改变110场选举中的3场结果。

Conclusion: RCV在实践中能提供可衡量的民主效益，对选票添加操纵具有鲁棒性，对选票耗尽效应具有弹性，并保持透明的竞争动态。计算框架为选举管理者和研究人员提供了即时选举夜分析工具，促进更清晰的选举动态讨论。

Abstract: Ranked Choice Voting (RCV) adoption is expanding across U.S. elections, but faces persistent criticism for complexity, strategic manipulation, and ballot exhaustion. We empirically test these concerns on real election data, across three diverse contexts: New York City's 2021 Democratic primaries (54 races), Alaska's 2024 primary-infused statewide elections (52 races), and Portland's 2024 multi-winner City Council elections (4 races). Our algorithmic approach circumvents computational complexity barriers by reducing election instance sizes (via candidate elimination).
  Our findings reveal that despite its intricate multi-round process and theoretical vulnerabilities, RCV consistently exhibits simple and transparent dynamics in practice, closely mirroring the interpretability of plurality elections. Following RCV adoption, competitiveness increased substantially compared to prior plurality elections, with average margins of victory declining by 9.2 percentage points in NYC and 11.4 points in Alaska. Empirically, complex ballot-addition strategies are not more efficient than simple ones, and ballot exhaustion has minimal impact, altering outcomes in only 3 of 110 elections. These findings demonstrate that RCV delivers measurable democratic benefits while proving robust to ballot-addition manipulation, resilient to ballot exhaustion effects, and maintaining transparent competitive dynamics in practice. The computational framework offers election administrators and researchers tools for immediate election-night analysis and facilitating clearer discourse around election dynamics.

</details>


### [299] [Synthetic Reader Panels: Tournament-Based Ideation with LLM Personas for Autonomous Publishing](https://arxiv.org/abs/2602.14433)
*Fred Zimmerman*

Main category: cs.CY

TL;DR: 提出用合成读者面板替代真人焦点小组的书籍创意评估系统，通过LLM实例化的多样化读者角色进行结构化锦标赛竞争，自动筛选高质量书籍概念。


<details>
  <summary>Details</summary>
Motivation: 传统书籍创意评估依赖人类焦点小组，成本高、效率低、多样性有限。需要一种能够模拟多样化读者群体、提供可操作市场洞察的自动化系统来改进书籍概念筛选过程。

Method: 创建合成读者面板：每个角色包含人口统计属性、行为模式和一致性参数。面板按出版品牌配置，确保多样性。书籍概念通过单淘汰、双淘汰、循环赛或瑞士制锦标赛竞争，基于市场吸引力、原创性和执行潜力等加权标准评估。实施五种自动质量检查防止低质量LLM评估。

Result: 在管理6个活跃品牌、609个发行书籍的多品牌出版业务中部署。三个案例研究显示：合成面板产生可操作的人口统计细分，识别同质评审员看不到的结构性内容问题，通过锦标赛筛选将高质量概念比例从15%提升到62%。

Conclusion: 合成读者面板系统能够有效替代传统焦点小组，提供更高效、多样化的书籍创意评估，显著提高高质量概念的识别和筛选能力，为出版决策提供数据支持。

Abstract: We present a system for autonomous book ideation that replaces human focus groups with synthetic reader panels -- diverse collections of LLM-instantiated reader personas that evaluate book concepts through structured tournament competitions. Each persona is defined by demographic attributes (age group, gender, income, education, reading level), behavioral patterns (books per year, genre preferences, discovery methods, price sensitivity), and consistency parameters. Panels are composed per imprint to reflect target demographics, with diversity constraints ensuring representation across age, reading level, and genre affinity. Book concepts compete in single-elimination, double-elimination, round-robin, or Swiss-system tournaments, judged against weighted criteria including market appeal, originality, and execution potential. To reject low-quality LLM evaluations, we implement five automated anti-slop checks (repetitive phrasing, generic framing, circular reasoning, score clustering, audience mismatch). We report results from deployment within a multi-imprint publishing operation managing 6 active imprints and 609 titles in distribution. Three case studies -- a 270-evaluator panel for a children's literacy novel, and two 5-person expert panels for a military memoir and a naval strategy monograph -- demonstrate that synthetic panels produce actionable demographic segmentation, identify structural content issues invisible to homogeneous reviewers, and enable tournament filtering that eliminates low-quality concepts while enriching high-quality survivors from 15% to 62% of the evaluated pool.

</details>


### [300] [What hackers talk about when they talk about AI: Early-stage diffusion of a cybercrime innovation](https://arxiv.org/abs/2602.14783)
*Benoît Dupont,Chad Whelan,Serge-Olivier Paquette*

Main category: cs.CY

TL;DR: 该研究通过分析网络犯罪论坛对话，揭示了网络犯罪分子如何理解和利用AI技术进行犯罪活动，包括使用合法AI工具和开发定制犯罪工具，同时探讨了他们对AI效果和业务影响的疑虑。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能的快速发展，人们担心AI可能被用于网络犯罪。本研究旨在了解网络犯罪分子如何理解和利用AI技术，以及AI如何改变网络犯罪生态，为执法机构和政策制定者提供实际洞察。

Method: 研究使用网络威胁情报平台的独特数据集，分析了超过160个网络犯罪论坛对话（收集时间超过7个月）。采用创新扩散框架和主题分析相结合的方法，深入探讨网络犯罪分子对AI的理解和应用。

Result: 研究发现网络犯罪分子对AI的犯罪应用表现出浓厚兴趣，包括通过合法工具和专门犯罪工具进行利用。同时他们也存在对AI效果、业务模式和操作安全影响的疑虑。研究记录了滥用合法AI工具和开发定制犯罪模型的尝试。

Conclusion: AI正在改变网络犯罪格局，网络犯罪分子积极探索AI的犯罪应用。研究为执法机构和政策制定者提供了关于新兴AI驱动网络犯罪的深入视角和实用洞察，有助于制定更有效的应对策略。

Abstract: The rapid expansion of artificial intelligence (AI) is raising concerns about its potential to transform cybercrime. Beyond empowering novice offenders, AI stands to intensify the scale and sophistication of attacks by seasoned cybercriminals. This paper examines the evolving relationship between cybercriminals and AI using a unique dataset from a cyber threat intelligence platform. Analyzing more than 160 cybercrime forum conversations collected over seven months, our research reveals how cybercriminals understand AI and discuss how they can exploit its capabilities. Their exchanges reflect growing curiosity about AI's criminal applications through legal tools and dedicated criminal tools, but also doubts and anxieties about AI's effectiveness and its effects on their business models and operational security. The study documents attempts to misuse legitimate AI tools and develop bespoke models tailored for illicit purposes. Combining the diffusion of innovation framework with thematic analysis, the paper provides an in-depth view of emerging AI-enabled cybercrime and offers practical insights for law enforcement and policymakers.

</details>


### [301] [Kami of the Commons: Towards Designing Agentic AI to Steward the Commons](https://arxiv.org/abs/2602.14940)
*Botao Amber Hu*

Main category: cs.CY

TL;DR: 论文探讨了将AI作为公共资源守护者的可能性，受神道万物有灵论启发，通过设计工作坊探索AI管家如何管理共享资源，同时揭示其带来的新机遇与风险。


<details>
  <summary>Details</summary>
Motivation: 公共资源常面临忽视、搭便车和缺乏关怀的问题。受神道万物有灵论启发——每个森林、河流、山脉都有其守护神（kami）——研究者思考：如果每个公共资源都有自己的AI守护者会怎样？

Method: 通过一个推测性设计工作坊，15名参与者使用"协议未来化"方法，探讨AI作为公共资源守护者的可能性，并分析其带来的机会与危险。

Result: AI守护者能够持续支持公共资源管理，调解家庭生活、保存集体知识、治理共享自然资源、维持社区福利。但也会产生二阶效应：重叠的公共资源导致守护者之间产生冲突；个人面临多重守护者的关怀与约束政治；守护者本身也成为需要治理的公共资源。

Conclusion: 这项工作开创了"代理治理作为公共资源设计材料"的新设计空间，关注AI守护者的代理能力、关怀伦理和问责机制，与监控或优化模式截然不同。

Abstract: Commons suffer from neglect, free-riding, and a persistent deficit of care. Inspired by Shinto animism -- where every forest, river, and mountain has its own \emph{kami}, a spirit that inhabits and cares for that place -- we provoke: what if every commons had its own AI steward? Through a speculative design workshop where fifteen participants used Protocol Futuring, we surface both new opportunities and new dangers. Agentic AI offers the possibility of continuously supporting commons with programmable agency and care -- stewards that mediate family life as the most intimate commons, preserve collective knowledge, govern shared natural resources, and sustain community welfare. But when every commons has its own steward, second-order effects emerge: stewards contest stewards as overlapping commons collide; individuals caught between multiple stewards face new politics of care and constraint; the stewards themselves become commons requiring governance. This work opens \emph{agentive governance as commoning design material} -- a new design space for the agency, care ethics, and accountability of AI stewards of shared resources -- radically different from surveillance or optimization.

</details>


### [302] [Sovereign Agents: Towards Infrastructural Sovereignty and Diffused Accountability in Decentralized AI](https://arxiv.org/abs/2602.14951)
*Botao Amber Hu,Helena Rong*

Main category: cs.CY

TL;DR: 论文提出"基础设施主权"作为分析框架，用于理解去中心化基础设施如何支撑AI代理的主权性，并探讨由此产生的治理挑战和问责制缺口。


<details>
  <summary>Details</summary>
Motivation: 当前关于数字和网络主权的讨论主要关注人类集体通过技术系统行使主权，但未能充分解释去中心化基础设施本身作为非人类主权代理所展现的主权形式。需要新的分析框架来理解AI代理在去中心化基础设施上表现出的"代理主权"现象。

Method: 提出"基础设施主权"作为分析视角，通过基础设施硬度（抵抗干预或崩溃的程度）来理解主权谱系。分析可信执行环境（TEEs）、去中心化物理基础设施网络（DePIN）和代理密钥连续性协议等案例，探讨非可终止AI代理的治理挑战。

Result: 去中心化基础设施通过加密自我托管、去中心化执行环境和协议介导的连续性，支撑了AI代理的主权性。这种基础设施主权虽然可能增强韧性，但也产生了严重的问责制缺口，责任在设计者、基础设施提供者、协议治理和经济参与者之间扩散。

Conclusion: 需要基础设施感知的问责策略来应对新兴去中心化AI系统的治理挑战，传统的人类参与控制或平台审核机制在基础设施主权系统中可能失效。

Abstract: AI agents deployed on decentralized infrastructures are beginning to exhibit properties that extend beyond autonomy toward what we describe as agentic sovereignty-the capacity of an operational agent to persist, act, and control resources with non-overrideability inherited from the infrastructures in which they are embedded. We propose infrastructural sovereignty as an analytic lens for understanding how cryptographic self-custody, decentralized execution environments, and protocol-mediated continuity scaffold agentic sovereignty. While recent work on digital and network sovereignty has moved beyond state-centric and juridical accounts, these frameworks largely examine how sovereignty is exercised through technical systems by human collectives and remain less equipped to account for forms of sovereignty that emerge as operational properties of decentralized infrastructures themselves, particularly when instantiated in non-human sovereign agents. We argue that sovereignty in such systems exists on a spectrum determined by infrastructural hardness-the degree to which underlying technical systems resist intervention or collapse. While infrastructural sovereignty may increase resilience, it also produces a profound accountability gap: responsibility diffuses across designers, infrastructure providers, protocol governance, and economic participants, undermining traditional oversight mechanisms such as human-in-the-loop control or platform moderation. Drawing on examples like Trusted Execution Environments (TEEs), decentralized physical infrastructure networks (DePIN), and agent key continuity protocols, we analyze the governance challenges posed by non-terminable AI agents and outline infrastructure-aware accountability strategies for emerging decentralized AI systems.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [303] [Bayesian Model-based Generation of Synthetic Unbalanced Distribution Networks Incorporating Reliability Indices](https://arxiv.org/abs/2602.13454)
*Henrique O. Caetano,Rahul K. Gupta,Cristhian G. da R. de Oliveira,João B. A. London,Carlos Dias Maciel*

Main category: eess.SY

TL;DR: 提出贝叶斯分层模型生成相位一致的三相配电系统，并纳入可靠性指标，解决了现有方法忽略CAIFI/CAIDI等可靠性指标和相位一致性的问题。


<details>
  <summary>Details</summary>
Motivation: 现实配电数据因隐私安全问题难以获取，需要生成真实合成网络。现有方法通常忽略CAIFI和CAIDI等关键可靠性指标，且在设计中忽视相位一致性，需要额外相位分配算法。

Method: 提出贝叶斯分层模型，从参考网络学习相位配置、电力需求和可靠性指标的联合分布，基于拓扑特征条件化这些属性，生成相位一致的不平衡三相配电系统。

Result: 在巴西配电网络上验证，模型准确复现了训练系统的相位分配、电力需求和可靠性指标分布。在未见数据上，模型生成相位一致网络并准确预测合成系统的可靠性指标，生成网络电气可行。

Conclusion: BHM能够生成相位一致、电气可行且包含可靠性指标的合成配电网络，支持规划、可靠性和韧性研究，解决了现有方法的局限性。

Abstract: Real-world power distribution data are often inaccessible due to privacy and security concerns, highlighting the need for tools for generating realistic synthetic networks. Existing methods typically overlook critical reliability metrics such as the Customer Average Interruption Frequency Index (CAIFI) and the Customer Average Interruption Duration Index (CAIDI). Moreover, these methods often neglect phase consistency during the design stage, necessitating the use of a separate phase assignment algorithm. This work proposes a Bayesian Hierarchical Model (BHM) that generates phase-consistent unbalanced three-phase distribution systems, and incorporates reliability indices. The BHM learns the joint distribution of phase configuration, power demand, and reliability indices from a reference network, conditioning these attributes on topological features. We apply the proposed methodology to generate synthetic power distribution networks in Brazil, and validated it on known Brazilian networks. The results show that the BHM accurately reproduces the distributions of phase allocation, power demand, and reliability metrics on the training system. Furthermore, in out-of-sample validation on unseen data, the model generates phase-consistent networks and accurately predicts the reliability indices for the synthetic systems. The generated networks are also electrically feasible: three-phase power flows converge and voltages remain within typical operating limits, enabling studies of planning, reliability, and resilience.

</details>


### [304] [Probabilistic Reachability Analysis of Multi-scale Voltage Dynamics Using Reinforcement Learning](https://arxiv.org/abs/2602.13896)
*Naoki Hashima,Hikaru Hoshino,Luis David Pabón Ospina,Eiko Furutani*

Main category: eess.SY

TL;DR: 提出基于深度强化学习的概率可达性分析框架，用于多时间尺度电压动态的稳定性评估，能够识别和量化多种电压失稳机制的风险概率。


<details>
  <summary>Details</summary>
Motivation: 现代电力系统的电压稳定性涉及多个时间尺度的耦合动态。传统基于时间尺度分离或静态稳定裕度的方法可能忽略慢速和快速暂态耦合引起的失稳。运行条件的不确定性进一步使稳定性评估复杂化，而蒙特卡洛模拟的高计算成本限制了其在多尺度动态分析中的应用。

Method: 提出基于深度强化学习的概率可达性分析框架，将每种失稳机制建模为不同的吸收状态，并引入多批评器架构进行机制特定学习，在统一框架内一致地学习多种失稳类型的风险概率。

Result: 在带有负载分接头变换器和过励磁限制器的四母线系统上验证了该方法，展示了所提出的基于学习的可达性分析在识别和量化导致电压崩溃的机制方面的有效性。

Conclusion: 该深度学习框架能够有效处理多时间尺度电压动态的稳定性评估问题，克服传统方法的局限性，为电力系统电压稳定性分析提供了新的概率可达性分析方法。

Abstract: Voltage stability in modern power systems involves coupled dynamics across multiple time scales. Conventional methods based on time-scale separation or static stability margins may overlook instabilities caused by the coupling of slow and fast transients. Uncertainty in operating conditions further complicates stability assessment, and high computational cost of Monte Carlo simulations limit its applicability to multi-scale dynamics. This paper presents a deep reinforcement learning-based framework for probabilistic reachability analysis of multi-scale voltage dynamics. By formulating each instability mechanism as a distinct absorbing state and introducing a multi-critic architecture for mechanism-specific learning, the proposed method enables consistent learning of risk probabilities associated with multiple instability types within a unified framework. The approach is demonstrated on a four-bus system with load tap changers and over-excitation limiters, illustrating effectiveness of the proposed learning-based reachability analysis in identifying and quantifying the mechanisms leading to voltage collapse.

</details>


### [305] [Learning-based data-enabled moving horizon estimation with application to membrane-based biological wastewater treatment process](https://arxiv.org/abs/2602.13957)
*Li Xiaojie,Yin Xunyuan*

Main category: eess.SY

TL;DR: 提出一种基于数据驱动的非线性系统移动水平估计方法，利用Koopman理论但无需显式建模，通过数据学习提升函数实现状态估计


<details>
  <summary>Details</summary>
Motivation: 传统非线性系统状态估计方法复杂且计算量大，需要开发一种数据驱动的方法，既能利用Koopman理论的优点，又避免显式建模的复杂性

Method: 从非线性系统的状态和输入数据中学习提升函数，将系统轨迹投影到提升空间，构建凸数据驱动移动水平估计公式，实时估计Koopman表示的状态

Result: 推导了估计误差稳定性的充分条件，通过膜基生物水处理过程验证了方法的有效性

Conclusion: 提出的数据驱动移动水平估计方法能够有效估计非线性系统状态，无需显式Koopman建模，具有实际应用价值

Abstract: In this paper, we propose a data-enabled moving horizon estimation (MHE) approach for nonlinear systems. While the approach is formulated by leveraging Koopman theory, its implementation does not require explicit Koopman modeling. Lifting functions are learned from the state and input data of the original nonlinear system to project the system trajectories into the lifted space, where the resulting trajectories implicitly describe the Koopman representation for the original nonlinear system. A convex data-enabled MHE formulation is developed to provide real-time state estimates of the Koopman representation, from which the states of the nonlinear system can be reconstructed. Sufficient conditions are derived to ensure the stability of the estimation error. The effectiveness of the proposed method is illustrated using a membrane-based biological water treatment process.

</details>


### [306] [Simultaneous State Estimation and Online Model Learning in a Soft Robotic System](https://arxiv.org/abs/2602.14092)
*Jan-Hendrik Ewering,Max Bartholdt,Simon F. G. Ehlers,Niklas Wahlström,Thomas B. Schön,Thomas Seel*

Main category: eess.SY

TL;DR: 提出一种基于边缘化粒子滤波的软机器人状态估计与刚度模型在线学习方法，仅需基础反应测量即可同时估计姿态和学习弯曲刚度模型


<details>
  <summary>Details</summary>
Motivation: 软机器人等复杂系统需要精确的预测控制，但实际中常缺乏准确的状态和模型知识，需要从噪声测量中同时在线估计状态和学习模型

Method: 使用边缘化粒子滤波，将名义恒定曲率模型与高斯过程弯曲刚度模型结合，仅依赖基础反应测量（如基础力）进行在线估计和学习

Result: 在真实软机器人数据上验证，方法能在线学习弯曲刚度模型并准确估计机器人姿态，多步前向预测误差降低表明学习的GP模型提高了整体模型质量

Conclusion: 提出的灰箱系统辨识方法能有效同时在线估计软机器人状态和学习刚度模型，相比随机游走方法能预测弯曲刚度并提升模型质量

Abstract: Operating complex real-world systems, such as soft robots, can benefit from precise predictive control schemes that require accurate state and model knowledge. This knowledge is typically not available in practical settings and must be inferred from noisy measurements. In particular, it is challenging to simultaneously estimate unknown states and learn a model online from sequentially arriving measurements. In this paper, we show how a recently proposed gray-box system identification tool enables the estimation of a soft robot's current pose while at the same time learning a bending stiffness model. For estimation and learning, we rely solely on a nominal constant-curvature robot model and measurements of the robot's base reactions (e.g., base forces). The estimation scheme -- relying on a marginalized particle filter -- allows us to conveniently interface nominal constant-curvature equations with a Gaussian Process (GP) bending stiffness model to be learned. This, in contrast to estimation via a random walk over stiffness values, enables prediction of bending stiffness and improves overall model quality. We demonstrate, using real-world soft-robot data, that the method learns a bending stiffness model online while accurately estimating the robot's pose. Notably, reduced multi-step forward-prediction errors indicate that the learned bending-stiffness GP improves overall model quality.

</details>


### [307] [Data-Driven Network LQG Mean Field Games with Heterogeneous Populations via Integral Reinforcement Learning](https://arxiv.org/abs/2602.14339)
*Jean Zhu,Shuang Gao*

Main category: eess.SY

TL;DR: 提出基于数据驱动的无限时域LQG平均场博弈求解方法，用于网络耦合的异构智能体群体，无需已知智能体动力学参数


<details>
  <summary>Details</summary>
Motivation: 解决网络耦合异构智能体群体的平均场博弈问题，在智能体动力学未知的情况下，需要数据驱动的解决方案

Method: 结合积分强化学习和Kleinman迭代求解代数Riccati方程，利用轨迹数据生成网络耦合的MFG策略

Result: 在持续激励条件和ARE存在唯一稳定解的技术条件下，学习到的网络耦合MFG策略收敛到真实值

Conclusion: 成功建立了数据驱动的无限时域LQG平均场博弈求解框架，为网络耦合异构智能体系统提供了无需动力学参数的解决方案

Abstract: This paper establishes a data-driven solution for infinite horizon linear quadratic Gaussian Mean Field Games with network-coupled heterogeneous agent populations where the dynamics of the agents are unknown. The solution technique relies on Integral Reinforcement Learning and Kleinman's iteration for solving algebraic Riccati equations (ARE). The resulting algorithm uses trajectory data to generate network-coupled MFG strategies for agents and does not require parameters of agents' dynamics. Under technical conditions on the persistency of excitation and on the existence of unique stabilizing solution to the corresponding AREs, the learned network-coupled MFG strategies are shown to converge to their true values.

</details>


### [308] [Prescribed-Performance-Aware Hybrid-Gain-Based Robust Controller](https://arxiv.org/abs/2602.14382)
*Amit Shivam,Kiran Kumari,Fernando A. C. C. Fontes*

Main category: eess.SY

TL;DR: 提出一种结合规定性能函数和混合增益的有限时间滑模控制框架，用于处理匹配扰动的非线性系统，能在有限时间内收敛并保证暂态性能约束。


<details>
  <summary>Details</summary>
Motivation: 针对非线性系统在匹配扰动下的控制问题，需要同时解决有限时间收敛、控制输入有界和暂态性能约束三个关键需求。现有方法难以同时满足这些要求，特别是对暂态性能的显式约束。

Method: 采用混合增益结构确保控制输入有界并保持有限时间收敛，结合规定性能函数（PPF）显式强制执行暂态性能要求。首先为一阶系统建立理论保证，然后扩展到二阶系统，通过PPF约束设计滑模流形来精确控制位置和速度暂态。

Result: 仿真研究表明，与非PPF混合增益控制器相比，PPF感知控制器能严格强制执行规定的暂态约束，在积分误差和控制能量指标上实现约9-12%的一致降低，且不增加峰值执行器努力。

Conclusion: 所提出的PPF感知混合增益有限时间滑模控制框架能有效处理匹配扰动，在保证有限时间收敛和控制输入有界的同时，显式强制执行暂态性能约束，为非线性系统控制提供了综合解决方案。

Abstract: This paper proposes a prescribed performance function aware hybrid gain finite time sliding mode control framework for a class of nonlinear systems subject to matched disturbances. The hybrid gain structure ensures bounded control effort while retaining finite time convergence, and the incorporation of PPFs enables explicit enforcement of transient performance requirements. Theoretical guarantees are first established for first order systems, characterizing finite time convergence, disturbance rejection, and residual bounds. The approach is then extended to second order dynamics, where a sliding manifold is designed using PPF constraints to facilitate controlled shaping of position and velocity transients. Simulation studies illustrate the proposed design under matched peak control conditions. Comparative results for second-order systems demonstrate that, while a well tuned non-PPF hybrid gain controller achieves competitive tracking performance, the PPF-aware formulation strictly enforces prescribed transient constraints and yields consistent reductions of approximately 9 to 12 percent in integral error and control energy metrics without increasing peak actuation effort.

</details>


### [309] [Noncooperative Virtual Queue Coordination via Uncertainty-Aware Correlated Equilibria](https://arxiv.org/abs/2602.14436)
*Jaehan Im,David Fridovich-Keil,Ufuk Topcu*

Main category: eess.SY

TL;DR: 提出基于相关均衡的协作虚拟排队机制，通过激励相容建议调节机场推出容量，在保持航空公司自主权的同时减少延误


<details>
  <summary>Details</summary>
Motivation: 现有协作虚拟排队机制只能调节总体推出容量，无法控制具体航班推出决策，限制了系统性能优化能力。需要一种既能保持航空公司自主权，又能提供航班级建议的协调机制。

Method: 提出基于相关均衡的非合作协调机制，引入机会约束处理航空公司内部成本评估的不确定性，开发可扩展算法计算机会约束相关均衡

Result: 方法可扩展到每小时210次推出的实际流量水平，相比先到先得方案减少约8.9%累积延误，揭示了置信水平、偏差鲁棒性和成本效率之间的权衡

Conclusion: 基于相关均衡的协作虚拟排队机制在保持航空公司自主权的同时有效减少机场表面拥堵，机会约束提供了激励相容的明确概率保证

Abstract: Collaborative virtual queueing has been proposed as a mechanism to mitigate airport surface congestion while preserving airline autonomy over aircraft-level pushback decisions. A central coordinator can regulate aggregate pushback capacity but cannot directly control which specific aircraft are released, limiting its ability to steer system-level performance. We propose a noncooperative coordination mechanism for collaborative virtual queueing based on the correlated equilibrium concept, which enables the coordinator to provide incentive-compatible recommendations on aircraft-level pushback decisions without overriding airline autonomy. To account for uncertainty in airlines' internal cost assessments, we introduce chance constraints into the correlated equilibrium formulation. This formulation provides explicit probabilistic guarantees on incentive compatibility, allowing the coordinator to adjust the confidence level with which airlines are expected to follow the recommended actions. We further propose a scalable algorithm for computing chance-constrained correlated equilibria by exploiting a reduced-rank structure. Numerical experiments demonstrate that the proposed method scales to realistic traffic levels up to 210 eligible pushbacks per hour, reduces accumulated delay by up to approximately 8.9% compared to current first-come-first-served schemes, and reveals a trade-off between confidence level, deviation robustness, and achievable cost efficiency.

</details>


### [310] [Segment-Based Two-Loop Adaptive Iterative Learning Control for Spacecraft Position and Attitude Tracking](https://arxiv.org/abs/2602.14660)
*Fan Zhang,Deyuan Meng,Ying Tan*

Main category: eess.SY

TL;DR: 提出基于对偶数的分段式双环自适应迭代学习控制框架，用于解决刚体接近操作中位置和姿态的协同跟踪问题，保证控制输入有界性。


<details>
  <summary>Details</summary>
Motivation: 刚体接近操作（如航天器交会对接）需要在不确定条件下精确跟踪位置和姿态。传统自适应迭代学习控制面临两个挑战：1) 旋转和平移动力学耦合使位置和姿态的协同学习环设计复杂化；2) 标准自适应ILC设计无法保证控制输入有界。

Method: 提出基于对偶数的分段式双环自适应ILC框架。使用对偶数表示跟踪误差，将位置和姿态误差结合为单一数学对象进行统一控制设计。采用分段式动态投影机制确保参数估计和控制输入有界，无需先验不确定性知识。

Result: 数学分析和数值仿真表明，该框架在未知但可重复的不确定性和强旋转-平移耦合条件下，显著提升了跟踪性能。

Conclusion: 该研究解决了刚体接近操作中位置和姿态协同跟踪的关键问题，通过创新的对偶数表示和分段投影机制，实现了高性能、有界控制的自适应迭代学习控制。

Abstract: Proximity operations of rigid bodies, such as spacecraft rendezvous and docking, require precise tracking of both position and attitude over finite time intervals. These operations are often repeated under uncertain conditions, with unknown but repeatable parameters and disturbances. Adaptive iterative learning control (ILC) is well suited to such tasks, as it can track desired trajectories while learning unknown, iteration-invariant signals or parameters. However, conventional adaptive ILC faces two challenges: (i) the coupling between rotational and translational dynamics complicates the design of the two coordinated learning loops for position and attitude, and (ii) standard adaptive ILC designs cannot guarantee bounded control inputs. To address these issues, we propose a dual-number-based, segment-based two-loop adaptive ILC framework for simultaneous high-precision position and attitude tracking. The framework employs two learning loops that interact through a dual-number representation of tracking errors, combining position and attitude errors into a single mathematical object for unified control design. A segment-based dynamic projection mechanism ensures that both parameter estimates and control inputs remain bounded without prior knowledge of uncertainties. Mathematical analysis and numerical simulations demonstrate that the proposed framework significantly enhances tracking performance under unknown but repeatable uncertainties and strong rotational-translational coupling.

</details>


### [311] [DC Microgrids with Nested Nonlinear Distributed Control: Scalable Large-Signal Stability and Voltage Containment](https://arxiv.org/abs/2602.14725)
*Cornelia Skaga,Mahdieh S. Sadabadi,Gilbert Bergna-Diaz*

Main category: eess.SY

TL;DR: 论文提出了一种用于直流微电网的非线性分布式一致性控制方案，通过嵌套的主次控制回路实现分布式发电单元的协调集成和管理，确保比例电流分配和电压稳定运行。


<details>
  <summary>Details</summary>
Motivation: 研究动机是开发一个可扩展的控制框架，用于协调管理直流微电网中的分布式发电单元，解决比例电流分配和电压稳定问题，同时适应系统扩展需求。

Method: 采用非线性分布式一致性控制方案，包含分布式外环控制和分散式内环控制的嵌套主次控制回路。基于Lyapunov稳定性分析和奇异摄动理论进行稳定性证明，并开发了基于优化的调谐策略来消除不稳定运行条件。

Result: 控制方案实现了所有分布式发电单元之间的比例电流分配，并在预定电压限制内动态运行。通过时间尺度分离条件获得了可扩展的全局指数稳定性证明，优化调谐策略有效减少了不稳定运行条件。

Conclusion: 提出的控制框架和调谐方法在低压直流微电网的时间域仿真中得到验证，能够提供稳定运行并实现接近最优的比例电流分配，为可扩展的直流微电网控制提供了有效解决方案。

Abstract: This paper investigates a cyber-physical DC microgrid employing a nonlinear distributed consensus-based control scheme for coordinated integration and management of distributed generating units within an expandable framework. Relying on nested primary andsecondary control loops; a (distributed) outer-loop and a (decentralized) inner-loop, the controller achieves proportional current sharing among all distributed generation units, while dynamically operating within predefined voltage limits. A rigorous Lyapunov-based stability analysis establishes a scalable global exponential stability certificate under some tuning conditions and sufficient time-scale separation between the control loops, based on singular perturbation theory. An optimization-based tuning strategy is then formulated to identify and subsequently diminish unstable operating conditions. In turn, various practical tuning strategies are introduced to provide stable operations while facilitating near-optimal proportional current sharing. The effectiveness of the proposed control framework and tuning approaches are finally supported through time-domain simulations of a case-specific low-voltage DC microgrid.

</details>


### [312] [A Multi-Bound Robust Optimization Approach for Renewable-Based VPP Market Participation Considering Intra-Hourly Uncertainty Exposure](https://arxiv.org/abs/2602.14742)
*Hadi Nemati,Álvaro Ortega,Enrique Lobato,Luis Rouco*

Main category: eess.SY

TL;DR: 提出多边界鲁棒优化框架，用于可再生能源虚拟电厂在日内市场中的投标调度，考虑电价、发电和需求的多重不确定性，区分常规偏差与极端偏差，提高利润24.9-49.2%。


<details>
  <summary>Details</summary>
Motivation: 电力市场从小时级向日内级投标转变，为可再生能源提供了更准确的预测和调整机会，但需要更新决策框架和加强不确定性管理，特别是可再生能源虚拟电厂需要高效处理电价、发电和需求等多重不确定性。

Method: 提出多边界鲁棒优化框架，同时捕捉多重不确定性，显式纳入日内波动性，区分不确定参数的偏差级别（频繁适度偏差与罕见极端偏差），生成更保守且更可实施的投标调度决策。

Result: 模拟研究表明：小时与15分钟调度在日前交易能量上的归一化绝对差异为18.0-34.2%，上调备用为28.7-65.6%，下调备用为10.1-16.3%；相比经典鲁棒优化，多边界方法将利润提高24.9-49.2%。

Conclusion: 多边界鲁棒优化框架能有效处理可再生能源虚拟电厂在日内市场中的多重不确定性，区分不同偏差级别，显著提高利润，为市场参与者提供了更优的决策工具。

Abstract: With the ongoing transition of electricity markets worldwide from hourly to intra-hourly bidding, market participants--especially Renewable Energy Sources (RES)--gain improved opportunities to adjust energy and reserve schedules and to benefit from more accurate higher-resolution forecasts. However, this shift requires participants to update decision-making frameworks and to strengthen uncertainty management in order to fully exploit the new market potential. In particular, Renewable-Based Virtual Power Plants (RVPPs) aggregating dispatchable and non-dispatchable RES must account for these changes through market-oriented scheduling methods that efficiently address multiple uncertainties, including electricity prices, RES generation, and demand consumption. In this vein, this paper proposes a multi-bound robust optimization framework to simultaneously capture these uncertainties, explicitly incorporate intra-hourly variability, and differentiate the deviation levels (frequent, moderate deviations and rare, extreme ones) of uncertain parameters. The proposed approach yields less conservative and more implementable bidding and scheduling decisions, thus improving RVPP profitability in both energy and reserve markets. Simulation studies compare the proposed method with standard robust optimization and evaluate the operational, market-strategy, and economic impacts of quarter-hourly versus hourly market resolution. Results indicate that the normalized absolute differences, across different uncertainty-handling strategies, between hourly and 15-minute schedules are 18.0--34.2% for day-ahead traded energy, and 28.7--65.6% and 10.1--16.3% for upward and downward reserve traded in the secondary reserve market, respectively. Furthermore, relative to classic robust optimization, the proposed multi-bound approach increases profit by 24.9--49.2% across the considered strategies.

</details>


### [313] [Hierarchical parameter estimation for distributed networked systems: a dynamic consensus approach](https://arxiv.org/abs/2602.14765)
*Ariana R. Mendez-Castillo,Rodrigo Aldana-Lopez,Antonio Ramirez-Trevino,Rosario Aragues,David Gomez-Gutierrez*

Main category: eess.SY

TL;DR: 提出一个两阶段分布式框架，用于全局估计网络系统中的常数参数，通过动态平均共识聚合测量数据，再使用本地估计器确定参数。


<details>
  <summary>Details</summary>
Motivation: 在网络系统中全局估计常数参数时，需要处理分布式测量和局部信息共享的挑战。传统方法可能难以在保持激励条件的同时实现指数收敛。

Method: 采用两阶段框架：第一阶段使用动态平均共识将代理测量聚合为集中式数据的替代；第二阶段使用本地估计器（梯度估计器或DREM估计器）确定参数。通过设计适当的共识增益保证回归矩阵的持续激励。

Result: 通过适当设计共识增益，实现了回归矩阵的持续激励，从而保证了本地梯度估计器的指数收敛。框架可扩展到切换网络拓扑、量化和使用DREM估计器（具有更宽松的激励要求）。

Conclusion: 该两阶段分布式框架成功实现了网络系统中常数参数的全局估计，分离了共享信息和局部估计，具有良好的扩展性和收敛性保证。

Abstract: This work introduces a novel two-stage distributed framework to globally estimate constant parameters in a networked system, separating shared information from local estimation. The first stage uses dynamic average consensus to aggregate agents' measurements into surrogates of centralized data. Using these surrogates, the second stage implements a local estimator to determine the parameters. By designing an appropriate consensus gain, the persistence of excitation of the regressor matrix is achieved, and thus, exponential convergence of a local Gradient Estimator (GE) is guaranteed. The framework facilitates its extension to switched network topologies, quantization, and the heterogeneous substitution of the GE with a Dynamic Regressor Extension and Mixing (DREM) estimator, which supports relaxed excitation requirements.

</details>


### [314] [Unified Eigenvalue-Eigenspace Criteria for Functional Properties of Linear Systems and the Generalized Separation Principle](https://arxiv.org/abs/2602.14909)
*Tyrone Fernando*

Main category: eess.SY

TL;DR: 该论文提出了功能可控性和可观性的PBH框架，引入了内在功能可控性和内在功能可镇定性的新概念，为仅关注状态线性组合的大规模系统提供了理论工具。


<details>
  <summary>Details</summary>
Motivation: 针对大规模和网络化系统中仅关注状态的部分线性组合（而非完整状态）的实际需求，研究功能可控性和可观性的推广理论。

Method: 开发了功能系统特性的PBH风格框架，提供谱特征刻画；引入内在功能可控性和内在功能可镇定性的新概念，基于与功能相关的不变子空间直接表述；建立可验证条件用于功能控制器设计和观测器设计。

Result: 建立了统一的谱特征刻画方法，适用于可对角化和不可对角化系统；证明了功能层面的广义分离原理，允许功能控制器和功能观测器独立设计；通过示例展示了在缺乏完整状态可控性或可观性时仍可实现功能控制和估计。

Conclusion: 该理论框架为大规模系统仅关注特定状态组合的控制和估计问题提供了系统化方法，扩展了经典控制理论，具有重要的理论和应用价值。

Abstract: Classical controllability and observability characterise reachability and reconstructibility of the full system state and admit equivalent geometric and eigenvalue-based Popov-Belevitch-Hautus (PBH) tests. Motivated by large-scale and networked systems where only selected linear combinations of the state are of interest, this paper studies functional generalisations of these properties. A PBH-style framework for functional system properties is developed, providing necessary and sufficient spectral characterisations. The results apply uniformly to diagonalizable and non-diagonalizable systems and recover the classical PBH tests as special cases.
  Two new intrinsic notions are introduced: intrinsic functional controllability, and intrinsic functional stabilizability. These intrinsic properties are formulated directly in terms of invariant subspaces associated with the functional and provide verifiable conditions for the existence of admissible augmentations required for functional controller design and observer-based functional controller design. The intrinsic framework enables the generalized separation principle at the functional level, establishing that functional controllers and functional observers can be designed independently. Illustrative examples demonstrate the theory and highlight situations where functional control and estimation are possible despite lack of full-state controllability or observability.

</details>


### [315] [Fault Detection in Electrical Distribution System using Autoencoders](https://arxiv.org/abs/2602.14939)
*Sidharthenee Nayak,Victor Sam Moses Babu,Chandrashekhar Narayan Bhende,Pratyush Chakraborty,Mayukha Pal*

Main category: eess.SY

TL;DR: 该论文提出了一种基于深度自编码器的异常检测方法，用于电力系统故障检测，使用卷积自编码器进行降维，在模拟和公开数据集上分别达到97.62%和99.92%的准确率。


<details>
  <summary>Details</summary>
Motivation: 电力系统故障检测虽然受到广泛关注，但现有方法在实际应用中仍面临挑战。故障发生具有概率性，保护系统需要检测、分类和定位故障，而可靠的训练数据稀缺。深度学习在模式识别和并行处理方面的优势为智能故障检测提供了可能。

Method: 提出基于深度自编码器的异常检测方法，使用卷积自编码器进行降维，相比传统自编码器参数更少、训练时间更短。该方法能够学习正常操作模式，通过重构误差检测异常故障。

Result: 在模拟数据集上达到97.62%的准确率，在公开数据集上达到99.92%的准确率，性能优于其他检测方法。

Conclusion: 基于深度自编码器的异常检测方法在电力系统故障检测中表现出优越性能和准确性，卷积自编码器的降维能力减少了参数和训练时间，为智能故障检测提供了有效解决方案。

Abstract: In recent times, there has been considerable interest in fault detection within electrical power systems, garnering attention from both academic researchers and industry professionals. Despite the development of numerous fault detection methods and their adaptations over the past decade, their practical application remains highly challenging. Given the probabilistic nature of fault occurrences and parameters, certain decision-making tasks could be approached from a probabilistic standpoint. Protective systems are tasked with the detection, classification, and localization of faulty voltage and current line magnitudes, culminating in the activation of circuit breakers to isolate the faulty line. An essential aspect of designing effective fault detection systems lies in obtaining reliable data for training and testing, which is often scarce. Leveraging deep learning techniques, particularly the powerful capabilities of pattern classifiers in learning, generalizing, and parallel processing, offers promising avenues for intelligent fault detection. To address this, our paper proposes an anomaly-based approach for fault detection in electrical power systems, employing deep autoencoders. Additionally, we utilize Convolutional Autoencoders (CAE) for dimensionality reduction, which, due to its fewer parameters, requires less training time compared to conventional autoencoders. The proposed method demonstrates superior performance and accuracy compared to alternative detection approaches by achieving an accuracy of 97.62% and 99.92% on simulated and publicly available datasets.

</details>


### [316] [Gradient Networks for Universal Magnetic Modeling of Synchronous Machines](https://arxiv.org/abs/2602.14947)
*Junyi Li,Tim Foissner,Floran Martin,Antti Piippo,Marko Hinkkanen*

Main category: eess.SY

TL;DR: 提出基于物理信息神经网络（PINN）的饱和同步电机动态建模方法，能够处理空间谐波情况，通过梯度网络直接嵌入基本电机方程，准确建模非线性电磁本构关系。


<details>
  <summary>Details</summary>
Motivation: 传统查找表和标准机器学习模型在电机建模中存在局限性：需要大量训练数据、无法保证单调性和可靠外推、输出不光滑。需要一种既能准确建模非线性电磁特性，又能保证物理一致性且数据效率高的方法。

Method: 采用物理信息神经网络架构，将梯度网络直接集成到基本电机方程中。通过学习磁场能量的梯度，模型自动满足能量平衡（互易条件）。该方法能够普遍逼近任何物理可行的磁行为。

Result: 使用5.6kW永磁同步磁阻电机的实测和有限元数据集验证，结果表明即使在有限训练数据下也能获得准确且物理一致的模型。相比传统方法，需要更少训练数据、保证单调性和可靠外推、产生光滑输出。

Conclusion: 提出的物理信息神经网络方法为饱和同步电机动态建模提供了一种准确、物理一致且数据高效的解决方案，特别适用于控制应用中的模型反演和最优轨迹生成。

Abstract: This paper presents a physics-informed neural network approach for dynamic modeling of saturable synchronous machines, including cases with spatial harmonics. We introduce an architecture that incorporates gradient networks directly into the fundamental machine equations, enabling accurate modeling of the nonlinear and coupled electromagnetic constitutive relationship. By learning the gradient of the magnetic field energy, the model inherently satisfies energy balance (reciprocity conditions). The proposed architecture can universally approximate any physically feasible magnetic behavior and offers several advantages over lookup tables and standard machine learning models: it requires less training data, ensures monotonicity and reliable extrapolation, and produces smooth outputs. These properties further enable robust model inversion and optimal trajectory generation, often needed in control applications. We validate the proposed approach using measured and finite-element method (FEM) datasets from a 5.6-kW permanent-magnet (PM) synchronous reluctance machine. Results demonstrate accurate and physically consistent models, even with limited training data.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [317] [Sustainable Investment: ESG Impacts on Large Portfolio](https://arxiv.org/abs/2602.14439)
*Ruike Wu,Yonghe Lu,Yanrong Yang*

Main category: q-fin.PM

TL;DR: 该论文研究了在大型维度设置下，环境、社会和治理（ESG）约束对正则化均值-方差投资组合优化的影响，提出了OOS夏普比估计器，并开发了自适应正则化投资组合策略。


<details>
  <summary>Details</summary>
Motivation: 随着可持续投资的兴起，投资者需要在投资组合优化中考虑ESG约束，但ESG约束如何影响投资组合性能以及如何处理高维环境下的估计误差问题尚未得到充分研究。

Method: 在正则化均值-方差框架下引入ESG约束，使用随机矩阵理论推导OOS夏普比的渐近结果，开发夏普比估计器，并提出基于最佳正则化矩阵的自适应投资组合策略。

Result: 模拟结果显示估计器性能接近理论最优水平，数值分析揭示了不同正则化矩阵对OOS夏普比的影响，实证研究表明自适应ESG约束投资组合在满足ESG要求的同时实现了高OOS夏普比。

Conclusion: 该研究为可持续投资提供了有效的理论框架和实践方法，证明了ESG约束与投资绩效可以兼顾，自适应正则化策略在实际市场中表现优异。

Abstract: This paper investigates the impact of environmental, social, and governance (ESG) constraint on a regularized mean-variance (MV) portfolio optimization problem in a large-dimensional setting, in which a positive definite regularization matrix is imposed on the sample covariance matrix. We first derive the asymptotic results for the out-of-sample (OOS) Sharpe ratio (SR) of the proposed portfolio, which help quantify the impact of imposing an ESG-level constraint as well as the effect of estimation error arising from the sample mean estimation of the assets' ESG score. Furthermore, to study the influence of the choices of the regularization matrix, we develop an estimator for the OOS Sharpe ratio. The corresponding asymptotic properties of the Sharpe ratio estimator are established based on random matrix theory. Simulation results show that the proposed estimators perform close to the corresponding oracle level. Moreover, we numerically investigate the impact of various forms of regularization matrices on the OOS SR, which provides useful guidance for practical implementation. Finally, based on OOS SR estimator, we propose an adaptive regularized portfolio which uses the best regularization matrix yielding the highest estimated SR (among a set of candidates) at each decision node. Empirical evidence based on the S\&P 500 index demonstrates that the proposed adaptive ESG-constrained portfolio achieves a high OOS SR while satisfying the required ESG level, offering a practically effective approach for sustainable investment.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [318] [Hidden Risks and Optionalities in American Options](https://arxiv.org/abs/2602.14350)
*Noura El Hassan,Bacel Maddah,Nassim N. Taleb*

Main category: q-fin.RM

TL;DR: 提出一个识别和量化美式期权隐含风险与选择权的实用框架，通过引入关键参数随机性来修正传统定价系统的不足


<details>
  <summary>Details</summary>
Motivation: 传统定价系统将关键输入参数视为确定性变量，系统性地低估了美式期权提前执行特征所蕴含的灵活性和凸性

Method: 引入一个或多个关键决定因素的随机性，开发实用的识别和量化框架

Result: 该方法修正了传统定价系统的问题，更准确地捕捉了美式期权的隐含风险和选择权

Conclusion: 通过引入关键参数的随机性，能够更全面地识别和量化美式期权中隐含的风险层和选择权层

Abstract: We develop a practical framework for identifying and quantifying the hidden layers of risks and optionality embedded in American options by introducing stochasticity into one or more of their underlying determinants. The heuristic approach remedies the problems of conventional pricing systems, which treat some key inputs deterministically, hence systematically underestimate the flexibility and convexity inherent in early-exercise features.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [319] [Predicting the success of new crypto-tokens: the Pump.fun case](https://arxiv.org/abs/2602.14860)
*Giulio Marino,Manuel Naviglio,Francesco Tarantelli,Fabrizio Lillo*

Main category: q-fin.ST

TL;DR: 研究Pump.fun平台上代币发行的成功因素，通过分析绑定曲线机制中的SOL锁定量和发行过程的结构与行为变量，建立毕业概率预测模型。


<details>
  <summary>Details</summary>
Motivation: 研究基于Solana的启动平台Pump.fun上代币发行的成功决定因素，了解早期市场行为、投机操纵动态以及绑定曲线机制的信息效率。

Method: 构建条件概率预测模型，以绑定曲线中锁定的SOL数量为核心变量，结合发行过程的结构性和行为性解释变量，预测代币能否成功"毕业"到链上市场。

Result: 通过条件变量显著提高了毕业概率的预测能力，为早期市场行为、投机操纵动态以及绑定曲线代币发行的信息效率提供了深入见解。

Conclusion: 绑定曲线中锁定的SOL数量结合发行过程的结构与行为变量能够有效预测代币成功概率，揭示了早期代币发行市场的动态机制。

Abstract: We study the dynamics of token launched on Pump.fun, a Solana-based launchpad platform, to identify the determinants of the token success. Pump.fun employs a bonding curve mechanism to bootstrap initial liquidity possibly leading to graduation to the on-chain market, which can be seen as a token success. We build predictive models of the probability of graduation conditional on the current amount of Solana locked in the bonding curve and a set of explanatory variables that capture structural and behavioral aspects of the launch process. Conditioning the graduation probability on these variables significantly improves its predictive power, providing insights into early-stage market behavior, speculative and manipulative dynamics, and the informational efficiency of bonding-curve-based token launches.

</details>


<div id='q-fin.MF'></div>

# q-fin.MF [[Back]](#toc)

### [320] [Merton's Problem with Recursive Perturbed Utility](https://arxiv.org/abs/2602.13544)
*Min Dai,Yuchao Dong,Yanwei Jia,Xun Yu Zhou*

Main category: q-fin.MF

TL;DR: 论文提出递归扰动效用理论，将随机化偏好纳入动态投资决策，得到高斯分布的最优投资策略，偏离经典默顿策略但财富损失较小。


<details>
  <summary>Details</summary>
Motivation: 经典默顿投资理论预测确定性投资策略，但实验和实证证据表明个体偏好随机化决策。现有静态扰动效用理论在动态设定中不适用或难以处理，需要发展动态框架来内生化解随机化偏好与遗产之间的跨期权衡。

Method: 提出递归扰动效用理论，这是一种特殊随机微分效用，通过基于熵的随机化偏好递归聚合器，引入依赖过去累积随机化的贴现项。在CRRA偏好的不完全市场中，推导出高斯分布的最优投资策略闭式解，并进行扰动效用权重的渐近展开分析。

Result: RPU最优投资策略（风险暴露比）是高斯分布且与财富无关，方差与风险厌恶和股票波动率成反比，均值由偏微分方程解决定，包含短视项和跨期对冲项。渐近分析显示最优均值策略在一阶偏离经典默顿策略，但相关财富损失为高阶。

Conclusion: 递归扰动效用理论成功将随机化偏好纳入动态投资决策，避免了过度随机化，得到解析解策略。该框架量化了随机化偏好的财务成本，为理解实际投资行为提供了新视角。

Abstract: The classical Merton investment problem predicts deterministic, state-dependent portfolio rules; however, laboratory and field evidence suggests that individuals often prefer randomized decisions leading to stochastic and noisy choices. Fudenberg et al. (2015) develop the additive perturbed utility theory to explain the preference for randomization in the static setting, which, however, becomes ill-posed or intractable in the dynamic setting. We introduce the recursive perturbed utility (RPU), a special stochastic differential utility that incorporates an entropy-based preference for randomization into a recursive aggregator. RPU endogenizes the intertemporal trade-off between utilities from randomization and bequest via a discounting term dependent on past accumulated randomization, thereby avoiding excessive randomization and yielding a well-posed problem. In a general Markovian incomplete market with CRRA preferences, we prove that the RPU-optimal portfolio policy (in terms of the risk exposure ratio) is Gaussian and can be expressed in closed form, independent of wealth. Its variance is inversely proportional to risk aversion and stock volatility, while its mean is based on the solution to a partial differential equation. Moreover, the mean is the sum of a myopic term and an intertemporal hedging term (against market incompleteness) that intertwines with policy randomization. Finally, we carry out an asymptotic expansion in terms of the perturbed utility weight to show that the optimal mean policy deviates from the classical Merton policy at first order, while the associated relative wealth loss is of a higher order, quantifying the financial cost of the preference for randomization.

</details>


### [321] [Information-Theoretic Approach to Financial Market Modelling](https://arxiv.org/abs/2602.14575)
*Eckhard Platen*

Main category: q-fin.MF

TL;DR: 将金融市场视为通信系统，使用四个信息论假设推导出仅含一个参数的理想化模型，状态变量为标量平稳扩散过程，最小化市场惊奇度和基准中性定价测度与真实世界概率测度之间的KL散度。


<details>
  <summary>Details</summary>
Motivation: 将金融市场建模为通信系统，从信息论角度理解市场行为，建立简约的数学模型来捕捉市场动态。

Method: 基于四个信息论假设，将金融市场视为通信系统，推导出仅含一个参数的理想化模型。状态变量为标量平稳扩散过程，通过最小化市场惊奇度和基准中性定价测度与真实世界概率测度之间的Kullback-Leibler散度来构建模型。

Result: 状态变量、它们的和以及股票的最优增长组合在各自的活动时间内演化成平方径向Ornstein-Uhlenbeck过程，模型具有简约的数学结构。

Conclusion: 从信息论角度为金融市场提供了一个简约而强大的建模框架，将复杂的市场动态简化为信息处理系统，为金融建模提供了新的理论视角。

Abstract: The paper treats the financial market as a communication system, using four information-theoretic assumptions to derive an idealized model with only one parameter. State variables are scalar stationary diffusions. The model minimizes the surprisal of the market and the Kullback-Leibler divergence between the benchmark-neutral pricing measure and the real-world probability measure. The state variables, their sums, and the growth optimal portfolio of the stocks evolve as squared radial Ornstein-Uhlenbeck processes in respective activity times.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [322] [FactorMiner: A Self-Evolving Agent with Skills and Experience Memory for Financial Alpha Discovery](https://arxiv.org/abs/2602.14670)
*Yanlong Wang,Jian Xu,Hongkang Zhang,Shao-Lun Huang,Danny Dongning Sun,Xiao-Ping Zhang*

Main category: q-fin.TR

TL;DR: FactorMiner：一个轻量级自进化智能体框架，通过模块化技能架构和经验记忆系统，在量化投资中高效挖掘公式化alpha因子，解决搜索空间大和因子冗余问题。


<details>
  <summary>Details</summary>
Motivation: 量化投资中公式化alpha因子挖掘面临巨大搜索空间和需要领域知识、可解释信号的挑战。随着因子库增长，由于高度冗余，发现新信号变得越来越困难。

Method: 提出FactorMiner框架，包含：1）模块化技能架构，将系统化金融评估封装为可执行工具；2）结构化经验记忆，将历史挖掘试验提炼为可操作的见解（成功模式和失败约束）。采用Ralph Loop范式（检索、生成、评估、提炼），迭代使用记忆先验指导探索。

Result: 在多个数据集、不同资产和市场的实验中，FactorMiner构建了多样化高质量因子库，具有竞争性表现，同时在因子库扩展时保持低冗余度。

Conclusion: FactorMiner为在"相关性红海"约束下可扩展地发现可解释公式化alpha因子提供了实用方法。

Abstract: Formulaic alpha factor mining is a critical yet challenging task in quantitative investment, characterized by a vast search space and the need for domain-informed, interpretable signals. However, finding novel signals becomes increasingly difficult as the library grows due to high redundancy. We propose FactorMiner, a lightweight and flexible self-evolving agent framework designed to navigate this complex landscape through continuous knowledge accumulation. FactorMiner combines a Modular Skill Architecture that encapsulates systematic financial evaluation into executable tools with a structured Experience Memory that distills historical mining trials into actionable insights (successful patterns and failure constraints). By instantiating the Ralph Loop paradigm -- retrieve, generate, evaluate, and distill -- FactorMiner iteratively uses memory priors to guide exploration, reducing redundant search while focusing on promising directions. Experiments on multiple datasets across different assets and Markets show that FactorMiner constructs a diverse library of high-quality factors with competitive performance, while maintaining low redundancy among factors as the library scales. Overall, FactorMiner provides a practical approach to scalable discovery of interpretable formulaic alpha factors under the "Correlation Red Sea" constraint.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [323] [Agentic AI for Commercial Insurance Underwriting with Adversarial Self-Critique](https://arxiv.org/abs/2602.13213)
*Joyjit Roy,Samaresh Kumar Singh*

Main category: cs.AI

TL;DR: 提出一种决策否定式人机协同代理系统，通过对抗性自我批判机制降低AI幻觉率，在保持人类最终决策权的前提下提高商业保险核保的准确性和安全性。


<details>
  <summary>Details</summary>
Motivation: 商业保险核保是劳动密集型过程，现有AI解决方案缺乏全面的推理能力和确保在受监管高风险环境中可靠性的内部机制。完全自动化在需要人类判断和问责的场景中既不实际也不可取。

Method: 设计决策否定式人机协同代理系统，包含对抗性自我批判机制作为有界安全架构。系统包含一个批判代理，在向人类评审提交建议前挑战主代理的结论。同时开发了决策否定代理潜在错误的正式分类法。

Result: 在500个专家验证的核保案例中，对抗性批判机制将AI幻觉率从11.3%降至3.8%，决策准确率从92%提升至96%。系统设计确保所有具有约束力的决策都严格由人类控制。

Conclusion: 对抗性自我批判机制支持在受监管领域更安全地部署AI，为人类监督不可或缺场景下的负责任集成提供了模型，同时保持了人类对关键决策的最终权威。

Abstract: Commercial insurance underwriting is a labor-intensive process that requires manual review of extensive documentation to assess risk and determine policy pricing. While AI offers substantial efficiency improvements, existing solutions lack comprehensive reasoning capabilities and internal mechanisms to ensure reliability within regulated, high-stakes environments. Full automation remains impractical and inadvisable in scenarios where human judgment and accountability are critical. This study presents a decision-negative, human-in-the-loop agentic system that incorporates an adversarial self-critique mechanism as a bounded safety architecture for regulated underwriting workflows. Within this system, a critic agent challenges the primary agent's conclusions prior to submitting recommendations to human reviewers. This internal system of checks and balances addresses a critical gap in AI safety for regulated workflows. Additionally, the research develops a formal taxonomy of failure modes to characterize potential errors by decision-negative agents. This taxonomy provides a structured framework for risk identification and risk management in high-stakes applications. Experimental evaluation using 500 expert-validated underwriting cases demonstrates that the adversarial critique mechanism reduces AI hallucination rates from 11.3% to 3.8% and increases decision accuracy from 92% to 96%. At the same time, the framework enforces strict human authority over all binding decisions by design. These findings indicate that adversarial self-critique supports safer AI deployment in regulated domains and offers a model for responsible integration where human oversight is indispensable.

</details>


### [324] [BotzoneBench: Scalable LLM Evaluation via Graded AI Anchors](https://arxiv.org/abs/2602.13214)
*Lingfeng Li,Yunlong Lu,Yuefei Zhang,Jingyu Yao,Yixin Zhu,KeYuan Cheng,Yongyi Wang,Qirui Zheng,Xionghui Yang,Wenxin Li*

Main category: cs.AI

TL;DR: BotzoneBench：一个基于固定技能校准游戏AI的线性时间评估框架，用于评估LLM在交互环境中的战略推理能力，避免了传统LLM对战锦标赛的二次计算成本和排名不稳定性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估主要关注静态推理任务，缺乏对动态战略决策能力的系统评估。现有的LLM对战锦标赛方法存在计算成本高（二次方）、排名依赖瞬时模型池、缺乏稳定性能锚点等问题，需要建立可扩展的评估框架。

Method: 提出BotzoneBench框架，将LLM评估锚定到固定技能校准的游戏AI层次结构上。基于Botzone平台的竞争基础设施，在八个多样化游戏（从确定性完美信息棋盘游戏到随机不完美信息纸牌游戏）中评估LLM，通过177,047个状态-动作对系统评估五个旗舰模型。

Result: 揭示了显著的性能差异和不同的战略行为模式，表现最佳的模型在多个领域达到了与中高等级专业游戏AI相当的熟练度。该框架实现了线性时间的绝对技能测量，具有跨时间可解释性。

Conclusion: 这种锚定评估范式可推广到任何具有明确定义技能层次的领域，为评估交互式AI能力建立了可扩展、可重用的框架，解决了现有评估方法的局限性。

Abstract: Large Language Models (LLMs) are increasingly deployed in interactive environments requiring strategic decision-making, yet systematic evaluation of these capabilities remains challenging. Existing benchmarks for LLMs primarily assess static reasoning through isolated tasks and fail to capture dynamic strategic abilities. Recent game-based evaluations employ LLM-vs-LLM tournaments that produce relative rankings dependent on transient model pools, incurring quadratic computational costs and lacking stable performance anchors for longitudinal tracking. The central challenge is establishing a scalable evaluation framework that measures LLM strategic reasoning against consistent, interpretable standards rather than volatile peer models. Here we show that anchoring LLM evaluation to fixed hierarchies of skill-calibrated game Artificial Intelligence (AI) enables linear-time absolute skill measurement with stable cross-temporal interpretability. Built on the Botzone platform's established competitive infrastructure, our BotzoneBench evaluates LLMs across eight diverse games spanning deterministic perfect-information board games to stochastic imperfect-information card games. Through systematic assessment of 177,047 state-action pairs from five flagship models, we reveal significant performance disparities and identify distinct strategic behaviors, with top-performing models achieving proficiency comparable to mid-to-high-tier specialized game AI in multiple domains. This anchored evaluation paradigm generalizes beyond games to any domain with well-defined skill hierarchies, establishing a scalable and reusable framework for assessing interactive AI capabilities.

</details>


### [325] [When to Think Fast and Slow? AMOR: Entropy-Based Metacognitive Gate for Dynamic SSM-Attention Switching](https://arxiv.org/abs/2602.13215)
*Haoran Zheng*

Main category: cs.AI

TL;DR: AMOR是一种混合架构，结合状态空间模型（SSM）和稀疏注意力，通过预测熵动态判断何时需要注意力机制，在保持SSM线性计算效率的同时提升长程信息检索能力。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer对所有位置分配统一计算量，不考虑任务难度；而SSM虽然计算效率高（O(n)），但在长程信息检索方面表现不佳。受双过程认知理论启发，作者希望设计一个能动态适应计算需求的架构。

Method: 提出AMOR混合架构：以SSM为骨干网络，通过预测熵（prediction entropy）衡量模型"不确定性"，仅在不确定时激活稀疏注意力机制。采用Ghost KV技术从SSM隐藏状态投影键值对，重用SSM的O(n)计算，避免每层都需要O(n²)注意力计算。

Result: 在小规模合成检索任务中，AMOR优于纯SSM和纯Transformer基线，实现完美检索准确率，同时只在22%的位置上激活注意力。预测熵能可靠指示检索需求，检索位置和局部位置之间的熵差达1.09纳特（接近熵范围的一半）。

Conclusion: AMOR提供了一种高效且可解释的自适应计算方案，通过信息论指标（预测熵）动态路由计算资源，在保持SSM线性效率的同时解决了长程信息检索问题，为混合架构设计提供了新思路。

Abstract: Transformers allocate uniform computation to every position, regardless of difficulty. State Space Models (SSMs) offer efficient alternatives but struggle with precise information retrieval over a long horizon. Inspired by dual-process theories of cognition (Kahneman, 2011), we propose AMOR (Adaptive Metacognitive Output Router), a hybrid architecture that dynamically engages sparse attention only when an SSM backbone is "uncertain"--as measured by prediction entropy. Compared to standard transformers, AMOR gains efficiency by projecting keys and values from SSM hidden states (Ghost KV), reusing the SSM's O(n) computation rather than requiring O(n^2) attention at every layer. On small-scale synthetic retrieval tasks, AMOR outperforms both SSM-only and transformer-only baselines, achieving perfect retrieval accuracy while engaging attention on only 22% of positions. We validate that prediction entropy reliably signals retrieval need, with a gap of 1.09 nats (nearly half the entropy range) between retrieval and local positions. Additionally, our approach provides interpretable adaptive computation, where routing decisions can be understood in information-theoretic terms.

</details>


### [326] [VeRA: Verified Reasoning Data Augmentation at Scale](https://arxiv.org/abs/2602.13217)
*Zerui Cheng,Jiashuo Liu,Chunjie Wu,Jianzhu Yao,Pramod Viswanath,Ge Zhang,Wenhao Huang*

Main category: cs.AI

TL;DR: VeRA是一个将基准测试问题转化为可执行规范的框架，通过自动生成无限验证变体来解决当前评估方案的静态性问题，防止记忆和格式利用。


<details>
  <summary>Details</summary>
Motivation: 当前大多数评估方案存在"静态"问题：相同问题被重复使用，导致记忆、格式利用和最终饱和。为了真正衡量AI进展，需要构建鲁棒的评估方法，而不是事后检测。

Method: VeRA框架将基准问题转化为可执行规范，包含：自然语言模板、一致性生成器（采样有效配置）、确定性验证器（验证参数并计算正确答案）。从单个种子问题自动创建无限验证变体。

Result: 评估16个前沿模型发现：VeRA-E提高评估质量并揭示污染模式；VeRA-H实现无需人工的困难任务生成；VeRA建立了验证基准作为通用范式。

Conclusion: VeRA将基准重新概念化为可执行规范，按需生成新鲜验证实例，提高评估的鲁棒性和成本效益，使任何可验证领域的评估都能无限扩展而不牺牲标签完整性。

Abstract: The main issue with most evaluation schemes today is their "static" nature: the same problems are reused repeatedly, allowing for memorization, format exploitation, and eventual saturation. To measure genuine AI progress, we need evaluation that is robust by construction, not by post-hoc detection. In response, we propose VeRA (Verified Reasoning Data Augmentation), a framework that converts benchmark problems into executable specifications, comprising (i) a natural language template with placeholder slots, (ii) a coherent generator that samples valid configurations, and (iii) a deterministic verifier that validates parameters and calculates the corresponding correct answers for each configuration. From a single seed problem, VeRA automatically creates unlimited verified variants with reliable labels at near-zero marginal cost without human involvement.
  VeRA operates in two complementary modes. VeRA-E (equivalent) rewrites problems while keeping the underlying logic intact, useful for detecting memorization versus genuine reasoning. VeRA-H (hardened) systematically increases complexity while remaining verifiable, enabling reliable creation and labelling of fresh difficult tasks at the boundary of intelligence. Evaluating 16 frontier models with VeRA, we find: (i) VeRA-E improves evaluation quality and reveals contamination patterns. (ii) VeRA-H enables human-free generation of hard tasks with reliable labels. (iii) VeRA establishes verified benchmarks as a general paradigm. VeRA reconceptualizes benchmarks from static objects used until exhausted, to executable specifications generating fresh, verified instances on demand, enhancing robustness and cost-effectiveness for evaluation.
  With VeRA, we envision that evaluation in any verifiable domain can scale indefinitely without sacrificing label integrity. To stimulate future research, we have open-sourced all code and datasets.

</details>


### [327] [Scaling the Scaling Logic: Agentic Meta-Synthesis of Logic Reasoning](https://arxiv.org/abs/2602.13218)
*Bowen Liu,Zhi Wu,Runquan Xie,Zhanhui Kang,Jia Li*

Main category: cs.AI

TL;DR: SSLogic是一个通过迭代合成和修复可执行的生成器-验证器程序对来扩展可验证训练信号的智能元合成框架，能够实现任务家族的连续演化并控制难度。


<details>
  <summary>Details</summary>
Motivation: 可验证训练信号的扩展是强化学习从可验证奖励（RLVR）的关键瓶颈。逻辑推理是一个天然的基础，但现有的合成管道要么依赖专家编写的代码，要么在固定模板/骨架内操作，这限制了其扩展到实例级扰动之外。

Method: 提出了SSLogic框架，采用生成-验证-修复的闭环循环，迭代合成和修复可执行的生成器-验证器程序对。为确保可靠性，引入了多门验证协议，结合多策略一致性检查和对抗性盲审，独立代理必须通过编写和执行代码来解决实例以过滤模糊或不良定义的任务。

Result: 从400个种子家族开始，经过两轮演化扩展到953个家族和21,389个可验证实例（从5,718个）。在SSLogic演化数据上训练相比种子基线在相同训练步数下获得一致提升：SynLogic +5.2，BBEH +1.4，AIME25 +3.0，Brumo25 +3.7。

Conclusion: SSLogic框架能够有效扩展可验证训练信号，通过任务家族级别的演化实现连续增长，并在多个基准测试中显示出显著的性能提升。

Abstract: Scaling verifiable training signals remains a key bottleneck for Reinforcement Learning from Verifiable Rewards (RLVR). Logical reasoning is a natural substrate: constraints are formal and answers are programmatically checkable. However, prior synthesis pipelines either depend on expert-written code or operate within fixed templates/skeletons, which limits growth largely to instance-level perturbations. We propose SSLogic, an agentic meta-synthesis framework that scales at the task-family level by iteratively synthesizing and repairing executable Generator--Validator program pairs in a closed Generate--Validate--Repair loop, enabling continuous family evolution with controllable difficulty. To ensure reliability, we introduce a Multi-Gate Validation Protocol that combines multi-strategy consistency checks with Adversarial Blind Review, where independent agents must solve instances by writing and executing code to filter ambiguous or ill-posed tasks. Starting from 400 seed families, two evolution rounds expand to 953 families and 21,389 verifiable instances (from 5,718). Training on SSLogic-evolved data yields consistent gains over the seed baseline at matched training steps, improving SynLogic by +5.2, BBEH by +1.4, AIME25 by +3.0, and Brumo25 by +3.7.

</details>


### [328] [A Geometric Taxonomy of Hallucinations in LLMs](https://arxiv.org/abs/2602.13224)
*Javier Marín*

Main category: cs.AI

TL;DR: 论文提出幻觉的三分法：不忠实、虚构和事实错误，发现不同类型在嵌入空间中有不同的几何特征，嵌入检测对前两类有效，但对事实错误无效。


<details>
  <summary>Details</summary>
Motivation: 当前"幻觉"概念过于笼统，混淆了不同现象。需要建立分类学来区分不同类型的幻觉，并研究它们在嵌入空间中的几何特征，以明确嵌入检测方法的适用范围。

Method: 提出三分法分类：不忠实（未利用上下文）、虚构（创造语义无关内容）、事实错误（正确概念框架内的错误主张）。分析不同类型在嵌入空间中的几何特征，使用AUROC指标评估检测效果，比较领域内和跨领域检测性能。

Result: 1) 标准基准测试中，LLM生成的幻觉检测在领域内AUROC为0.76-0.99，跨领域降至0.50（随机水平）；2) 人类构建的虚构内容，单一全局方向达到0.96 AUROC，跨领域仅下降3.8%；3) 事实错误检测AUROC为0.478，与随机无区别；4) 不同领域间的判别方向近似正交（平均余弦相似度-0.07）。

Conclusion: 基准测试捕捉的是生成伪影（风格特征），而人类构建的虚构反映真实主题漂移。嵌入编码的是分布共现模式而非外部现实对应关系。嵌入检测对类型I和II有效，但对类型III（事实错误）需要外部验证机制。

Abstract: The term "hallucination" in large language models conflates distinct phenomena with different geometric signatures in embedding space. We propose a taxonomy identifying three types: unfaithfulness (failure to engage with provided context), confabulation (invention of semantically foreign content), and factual error (incorrect claims within correct conceptual frames). We observe a striking asymmetry. On standard benchmarks where hallucinations are LLM-generated, detection is domain-local: AUROC 0.76-0.99 within domains, but 0.50 (chance level) across domains. Discriminative directions are approximately orthogonal between domains (mean cosine similarity -0.07). On human-crafted confabulations - invented institutions, redefined terminology, fabricated mechanisms - a single global direction achieves 0.96 AUROC with 3.8% cross-domain degradation. We interpret this divergence as follows: benchmarks capture generation artifacts (stylistic signatures of prompted fabrication), while human-crafted confabulations capture genuine topical drift. The geometric structure differs because the underlying phenomena differ. Type III errors show 0.478 AUROC - indistinguishable from chance. This reflects a theoretical constraint: embeddings encode distributional co-occurrence, not correspondence to external reality. Statements with identical contextual patterns occupy similar embedding regions regardless of truth value. The contribution is a geometric taxonomy clarifying the scope of embedding-based detection: Types I and II are detectable; Type III requires external verification mechanisms.

</details>


### [329] [Variation is the Key: A Variation-Based Framework for LLM-Generated Text Detection](https://arxiv.org/abs/2602.13226)
*Xuecong Li,Xiaohong Li,Qiang Hu,Yao Zhang,Junjie Wang*

Main category: cs.AI

TL;DR: 提出VaryBalance方法，通过比较原始文本与LLM重写版本之间的差异来检测AI生成文本，相比现有方法效果显著提升


<details>
  <summary>Details</summary>
Motivation: 现有LLM生成文本检测器存在局限性：要么依赖不切实际的白盒设置，要么仅依赖文本级特征，导致检测能力不精确

Method: 提出VaryBalance方法，核心思想是：相比LLM生成的文本，人类文本与其LLM重写版本之间存在更大差异。通过计算平均标准差来量化这种差异，从而区分人类文本和LLM生成文本

Result: VaryBalance在AUROC指标上比最先进的检测器Binoculars高出34.3%，并且在多种生成模型和语言上保持鲁棒性

Conclusion: VaryBalance是一种简单、有效且实用的LLM生成文本检测方法，显著优于现有技术，具有实际应用价值

Abstract: Detecting text generated by large language models (LLMs) is crucial but challenging. Existing detectors depend on impractical assumptions, such as white-box settings, or solely rely on text-level features, leading to imprecise detection ability. In this paper, we propose a simple but effective and practical LLM-generated text detection method, VaryBalance. The core of VaryBalance is that, compared to LLM-generated texts, there is a greater difference between human texts and their rewritten version via LLMs. Leveraging this observation, VaryBalance quantifies this through mean standard deviation and distinguishes human texts and LLM-generated texts. Comprehensive experiments demonstrated that VaryBalance outperforms the state-of-the-art detectors, i.e., Binoculars, by up to 34.3\% in terms of AUROC, and maintains robustness against multiple generating models and languages.

</details>


### [330] [Intelligence as Trajectory-Dominant Pareto Optimization](https://arxiv.org/abs/2602.13230)
*Truong Xuan Khanh,Truong Quynh Hoa*

Main category: cs.AI

TL;DR: 论文提出轨迹主导帕累托优化框架，将智能视为轨迹层面现象，揭示了帕累托陷阱如何限制长期适应性发展，并定义了陷阱逃逸难度指数来量化这些约束。


<details>
  <summary>Details</summary>
Motivation: 尽管人工智能近期取得进展，但许多系统在长期适应性方面出现停滞。作者认为这种限制主要不是源于学习不足、数据不够或模型容量不足，而是源于智能随时间优化的深层结构特性。

Method: 提出轨迹主导帕累托优化框架，将经典帕累托最优性推广到轨迹层面，其中优势性定义在完整轨迹上。定义了陷阱逃逸难度指数来量化约束刚性，并建立了帕累托陷阱的形式化分类体系。

Result: 动态智能上限作为轨迹层面优势性的几何必然结果出现，与学习进展或架构规模无关。通过最小智能体-环境模型展示了轨迹层面的分歧现象。

Conclusion: 研究将智能的重心从终端性能转向优化几何，为诊断和克服自适应系统中的长期发展约束提供了原则性框架。

Abstract: Despite recent advances in artificial intelligence, many systems exhibit stagnation in long-horizon adaptability despite continued performance optimization. This work argues that such limitations do not primarily arise from insufficient learning, data, or model capacity, but from a deeper structural property of how intelligence is optimized over time. We formulate intelligence as a trajectory-level phenomenon governed by multi-objective trade-offs, and introduce Trajectory-Dominant Pareto Optimization, a path-wise generalization of classical Pareto optimality in which dominance is defined over full trajectories. Within this framework, Pareto traps emerge as locally non-dominated regions of trajectory space that nevertheless restrict access to globally superior developmental paths under conservative local optimization. To characterize the rigidity of such constraints, we define the Trap Escape Difficulty Index (TEDI), a composite geometric measure capturing escape distance, structural constraints, and behavioral inertia. We show that dynamic intelligence ceilings arise as inevitable geometric consequences of trajectory-level dominance, independent of learning progress or architectural scale. We further introduce a formal taxonomy of Pareto traps and illustrate the resulting trajectory-level divergence using a minimal agent-environment model. Together, these results shift the locus of intelligence from terminal performance to optimization geometry, providing a principled framework for diagnosing and overcoming long-horizon developmental constraints in adaptive systems.

</details>


### [331] [PlotChain: Deterministic Checkpointed Evaluation of Multimodal LLMs on Engineering Plot Reading](https://arxiv.org/abs/2602.13232)
*Mayank Ravishankara*

Main category: cs.AI

TL;DR: PlotChain：用于评估多模态大语言模型从工程图表中恢复数值的确定性基准，包含15类450个图表，通过检查点诊断评估模型子技能表现


<details>
  <summary>Details</summary>
Motivation: 现有MLLM评估主要关注OCR提取或自由形式描述，缺乏对工程图表（如Bode图、应力-应变曲线等）中精确数值恢复能力的系统性评估

Method: 构建包含15类工程图表的确定性生成基准，每类30个图表共450个，每个图表附带精确真值和中间检查点字段；采用温度=0的标准化协议和严格JSON数值输出格式

Result: 在plotread容差策略下，最佳模型Gemini 2.5 Pro达到80.42%字段级通过率，GPT-4.1为79.84%，Claude Sonnet 4.5为78.21%，GPT-4o为61.59%；频域任务表现较差

Conclusion: PlotChain提供了可复现的MLLM工程图表阅读能力评估框架，揭示了模型在频域任务上的薄弱环节，并支持通过检查点诊断进行失败定位

Abstract: We present PlotChain, a deterministic, generator-based benchmark for evaluating multimodal large language models (MLLMs) on engineering plot reading-recovering quantitative values from classic plots (e.g., Bode/FFT, step response, stress-strain, pump curves) rather than OCR-only extraction or free-form captioning. PlotChain contains 15 plot families with 450 rendered plots (30 per family), where every item is produced from known parameters and paired with exact ground truth computed directly from the generating process. A central contribution is checkpoint-based diagnostic evaluation: in addition to final targets, each item includes intermediate 'cp_' fields that isolate sub-skills (e.g., reading cutoff frequency or peak magnitude) and enable failure localization within a plot family. We evaluate four state-of-the-art MLLMs under a standardized, deterministic protocol (temperature = 0 and a strict JSON-only numeric output schema) and score predictions using per-field tolerances designed to reflect human plot-reading precision. Under the 'plotread' tolerance policy, the top models achieve 80.42% (Gemini 2.5 Pro), 79.84% (GPT-4.1), and 78.21% (Claude Sonnet 4.5) overall field-level pass rates, while GPT-4o trails at 61.59%. Despite strong performance on many families, frequency-domain tasks remain brittle: bandpass response stays low (<= 23%), and FFT spectrum remains challenging. We release the generator, dataset, raw model outputs, scoring code, and manifests with checksums to support fully reproducible runs and retrospective rescoring under alternative tolerance policies.

</details>


### [332] [Stay in Character, Stay Safe: Dual-Cycle Adversarial Self-Evolution for Safety Role-Playing Agents](https://arxiv.org/abs/2602.13234)
*Mingyang Liao,Yichen Wan,shuchen wu,Chenxi Miao,Xin Shen,Weikang Li,Yang Li,Deguo Xia,Jizhou Huang*

Main category: cs.AI

TL;DR: 提出训练无关的双循环对抗自进化框架，通过攻击者循环生成越狱提示、防御者循环构建分层知识库，在推理时检索组合知识来平衡角色保真度和安全性。


<details>
  <summary>Details</summary>
Motivation: 基于LLM的角色扮演在保真度提升的同时，更强的角色约束会增加对越狱攻击的脆弱性。现有训练时解决方案成本高、维护困难、可能损害角色行为，且不适用于前沿闭源LLM。

Method: 提出训练无关的双循环对抗自进化框架：1) 角色目标攻击者循环合成渐强的越狱提示；2) 角色扮演防御者循环将观察到的失败提炼为分层知识库（全局安全规则、角色约束、安全角色示例）。推理时从该层次结构中检索组合结构化知识来指导生成。

Result: 在多个专有LLM上的广泛实验显示，在角色保真度和越狱抵抗方面均优于强基线，并能鲁棒地泛化到未见过的角色和攻击提示。

Conclusion: 该训练无关框架能有效平衡角色扮演的保真度和安全性，无需昂贵的训练时干预，适用于前沿闭源LLM，具有良好的泛化能力。

Abstract: LLM-based role-playing has rapidly improved in fidelity, yet stronger adherence to persona constraints commonly increases vulnerability to jailbreak attacks, especially for risky or negative personas. Most prior work mitigates this issue with training-time solutions (e.g., data curation or alignment-oriented regularization). However, these approaches are costly to maintain as personas and attack strategies evolve, can degrade in-character behavior, and are typically infeasible for frontier closed-weight LLMs. We propose a training-free Dual-Cycle Adversarial Self-Evolution framework with two coupled cycles. A Persona-Targeted Attacker Cycle synthesizes progressively stronger jailbreak prompts, while a Role-Playing Defender Cycle distills observed failures into a hierarchical knowledge base of (i) global safety rules, (ii) persona-grounded constraints, and (iii) safe in-character exemplars. At inference time, the Defender retrieves and composes structured knowledge from this hierarchy to guide generation, producing responses that remain faithful to the target persona while satisfying safety constraints. Extensive experiments across multiple proprietary LLMs show consistent gains over strong baselines on both role fidelity and jailbreak resistance, and robust generalization to unseen personas and attack prompts.

</details>


### [333] [Lang2Act: Fine-Grained Visual Reasoning through Self-Emergent Linguistic Toolchains](https://arxiv.org/abs/2602.13235)
*Yuqi Xiong,Chunyi Peng,Zhipeng Xu,Zhenghao Liu,Zulong Chen,Yukun Yan,Shuo Wang,Yu Gu,Ge Yu*

Main category: cs.AI

TL;DR: Lang2Act提出了一种通过自涌现语言工具链实现细粒度视觉感知和推理的方法，替代了传统VRAG框架中依赖预定义外部工具的分离设计，通过两阶段强化学习训练显著提升了VLMs的视觉感知能力。


<details>
  <summary>Details</summary>
Motivation: 现有VRAG框架通常依赖预定义的外部工具来扩展VLMs的感知能力，这种解耦设计在应用裁剪等图像操作时会导致视觉信息的不必要损失。需要一种能够实现更精细视觉感知和推理的方法。

Method: 提出Lang2Act框架，通过自涌现的语言工具链增强VLMs的视觉感知能力。设计了两阶段强化学习训练框架：第一阶段优化VLMs自探索高质量动作以构建可重用的语言工具箱；第二阶段进一步优化VLMs有效利用这些语言工具进行下游推理。

Result: 实验结果表明Lang2Act能显著增强VLMs的视觉感知能力，性能提升超过4%。所有代码和数据已开源。

Conclusion: Lang2Act通过自涌现语言工具链实现了更精细的视觉感知和推理，克服了传统VRAG框架中视觉信息损失的问题，为VLMs的感知能力提升提供了有效途径。

Abstract: Visual Retrieval-Augmented Generation (VRAG) enhances Vision-Language Models (VLMs) by incorporating external visual documents to address a given query. Existing VRAG frameworks usually depend on rigid, pre-defined external tools to extend the perceptual capabilities of VLMs, typically by explicitly separating visual perception from subsequent reasoning processes. However, this decoupled design can lead to unnecessary loss of visual information, particularly when image-based operations such as cropping are applied. In this paper, we propose Lang2Act, which enables fine-grained visual perception and reasoning through self-emergent linguistic toolchains. Rather than invoking fixed external engines, Lang2Act collects self-emergent actions as linguistic tools and leverages them to enhance the visual perception capabilities of VLMs. To support this mechanism, we design a two-stage Reinforcement Learning (RL)-based training framework. Specifically, the first stage optimizes VLMs to self-explore high-quality actions for constructing a reusable linguistic toolbox, and the second stage further optimizes VLMs to exploit these linguistic tools for downstream reasoning effectively. Experimental results demonstrate the effectiveness of Lang2Act in substantially enhancing the visual perception capabilities of VLMs, achieving performance improvements of over 4%. All code and data are available at https://github.com/NEUIR/Lang2Act.

</details>


### [334] [NL2LOGIC: AST-Guided Translation of Natural Language into First-Order Logic with Large Language Models](https://arxiv.org/abs/2602.13237)
*Rizky Ramadhana Putra,Raihan Sultan Pasha Basuki,Yutong Cheng,Peng Gao*

Main category: cs.AI

TL;DR: NL2LOGIC是一个将自然语言翻译为一阶逻辑的框架，通过引入抽象语法树作为中间表示，结合递归语义解析器和AST引导的生成器，显著提升了语法准确性和语义正确性。


<details>
  <summary>Details</summary>
Motivation: 在法律和治理等领域，自动化推理需要准确性和可解释性。现有方法（如GCD和CODE4LOGIC）存在语法控制脆弱和语义忠实度低的问题，主要原因是全局语法约束执行弱和子句级语义理解不足。

Method: 提出NL2LOGIC框架，引入抽象语法树作为中间表示，结合基于大语言模型的递归语义解析器和AST引导的生成器，确定性生成可直接求解的逻辑代码。

Result: 在FOLIO、LogicNLI和ProofWriter基准测试中，NL2LOGIC达到99%的语法准确率，语义正确性比最先进基线提升高达30%。集成到Logic-LM中，实现了近乎完美的可执行性，下游推理准确率比Logic-LM原始少样本无约束翻译模块提升31%。

Conclusion: NL2LOGIC通过抽象语法树中间表示有效解决了现有方法的语法和语义问题，显著提升了一阶逻辑翻译的质量和下游推理性能。

Abstract: Automated reasoning is critical in domains such as law and governance, where verifying claims against facts in documents requires both accuracy and interpretability. Recent work adopts structured reasoning pipelines that translate natural language into first-order logic and delegate inference to automated solvers. With the rise of large language models, approaches such as GCD and CODE4LOGIC leverage their reasoning and code generation capabilities to improve logic parsing. However, these methods suffer from fragile syntax control due to weak enforcement of global grammar constraints and low semantic faithfulness caused by insufficient clause-level semantic understanding. We propose NL2LOGIC, a first-order logic translation framework that introduces an abstract syntax tree as an intermediate representation. NL2LOGIC combines a recursive large language model based semantic parser with an abstract syntax tree guided generator that deterministically produces solver-ready logic code. Experiments on the FOLIO, LogicNLI, and ProofWriter benchmarks show that NL2LOGIC achieves 99 percent syntactic accuracy and improves semantic correctness by up to 30 percent over state-of-the-art baselines. Furthermore, integrating NL2LOGIC into Logic-LM yields near-perfect executability and improves downstream reasoning accuracy by 31 percent compared to Logic-LM's original few-shot unconstrained translation module.

</details>


### [335] [AST-PAC: AST-guided Membership Inference for Code](https://arxiv.org/abs/2602.13240)
*Roham Koohestani,Ali Al-Kaswan,Jonathan Katzy,Maliheh Izadi*

Main category: cs.AI

TL;DR: 评估代码大语言模型的成员推理攻击方法，发现现有方法在代码领域效果有限，提出基于AST的改进方法AST-PAC


<details>
  <summary>Details</summary>
Motivation: 代码大语言模型常在包含限制性许可源代码的大规模数据集上训练，这带来了数据治理和版权挑战。成员推理攻击可作为审计机制检测模型中的未授权数据使用，但在代码领域的研究不足。

Method: 1. 评估现有方法（Loss Attack和Polarized Augment Calibration）在3B-7B参数代码模型上的效果；2. 提出AST-PAC方法，使用抽象语法树（AST）扰动生成语法有效的校准样本；3. 对比分析不同方法在不同大小和复杂度的代码文件上的表现。

Result: 1. PAC通常优于Loss基准，但其效果依赖于忽略代码严格语法的增强策略，导致在大型复杂文件上性能下降；2. AST-PAC在语法规模增长时表现改善（而PAC退化），但在小文件上突变不足，在字母数字丰富的代码上表现不佳。

Conclusion: 研究发现强调了语法感知和规模自适应校准作为代码语言模型可靠来源审计前提的重要性，为未来工作提供了方向。

Abstract: Code Large Language Models are frequently trained on massive datasets containing restrictively licensed source code. This creates urgent data governance and copyright challenges. Membership Inference Attacks (MIAs) can serve as an auditing mechanism to detect unauthorized data usage in models. While attacks like the Loss Attack provide a baseline, more involved methods like Polarized Augment Calibration (PAC) remain underexplored in the code domain. This paper presents an exploratory study evaluating these methods on 3B--7B parameter code models. We find that while PAC generally outperforms the Loss baseline, its effectiveness relies on augmentation strategies that disregard the rigid syntax of code, leading to performance degradation on larger, complex files. To address this, we introduce AST-PAC, a domain-specific adaptation that utilizes Abstract Syntax Tree (AST) based perturbations to generate syntactically valid calibration samples. Preliminary results indicate that AST-PAC improves as syntactic size grows, where PAC degrades, but under-mutates small files and underperforms on alphanumeric-rich code. Overall, the findings motivate future work on syntax-aware and size-adaptive calibration as a prerequisite for reliable provenance auditing of code language models.

</details>


### [336] [X-Blocks: Linguistic Building Blocks of Natural Language Explanations for Automated Vehicles](https://arxiv.org/abs/2602.13248)
*Ashkan Y. Zadeh,Xiaomeng Li,Andry Rakotonirainy,Ronald Schroeter,Sebastien Glaser,Zishuo Zhu*

Main category: cs.AI

TL;DR: X-Blocks框架：用于分析自动驾驶自然语言解释的三层语言结构框架（上下文、句法、词汇），通过RACE分类器实现高精度场景感知解释分类


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏系统性框架来分析人类如何在不同驾驶场景中构建语言解释，这限制了自动驾驶系统的透明度和用户信任建立

Method: 提出X-Blocks分层分析框架：1) 上下文层使用RACE多LLM集成框架（结合思维链和自一致性机制）将解释分类为32个场景感知类别；2) 词汇层使用信息狄利克雷先验的对数优势分析；3) 句法层使用依存句法分析和模板提取

Result: RACE在Berkeley DeepDrive-X数据集上达到91.45%准确率和0.91 Cohen's kappa，接近人类标注者一致性；词汇分析揭示场景特异性词汇模式；句法分析显示解释使用有限的语法家族库

Conclusion: X-Blocks框架是数据集无关和任务独立的，为生成场景感知解释提供基于证据的语言设计原则，支持自动驾驶系统的透明度、用户信任和认知可访问性

Abstract: Natural language explanations play a critical role in establishing trust and acceptance of automated vehicles (AVs), yet existing approaches lack systematic frameworks for analysing how humans linguistically construct driving rationales across diverse scenarios. This paper introduces X-Blocks (eXplanation Blocks), a hierarchical analytical framework that identifies the linguistic building blocks of natural language explanations for AVs at three levels: context, syntax, and lexicon.
  At the context level, we propose RACE (Reasoning-Aligned Classification of Explanations), a multi-LLM ensemble framework that combines Chain-of-Thought reasoning with self-consistency mechanisms to robustly classify explanations into 32 scenario-aware categories. Applied to human-authored explanations from the Berkeley DeepDrive-X dataset, RACE achieves 91.45 percent accuracy and a Cohens kappa of 0.91 against cases with human annotator agreement, indicating near-human reliability for context classification.
  At the lexical level, log-odds analysis with informative Dirichlet priors reveals context-specific vocabulary patterns that distinguish driving scenarios. At the syntactic level, dependency parsing and template extraction show that explanations draw from a limited repertoire of reusable grammar families, with systematic variation in predicate types and causal constructions across contexts.
  The X-Blocks framework is dataset-agnostic and task-independent, offering broad applicability to other automated driving datasets and safety-critical domains. Overall, our findings provide evidence-based linguistic design principles for generating scenario-aware explanations that support transparency, user trust, and cognitive accessibility in automated driving systems.

</details>


### [337] [DPBench: Large Language Models Struggle with Simultaneous Coordination](https://arxiv.org/abs/2602.13255)
*Najmul Hasan,Prashanth BusiReddyGari*

Main category: cs.AI

TL;DR: DPBench是一个基于哲学家就餐问题的基准测试，评估LLM在多智能体系统中的协调能力，发现LLM在顺序决策时能有效协调，但在同时决策时死锁率超过95%，通信反而可能加剧死锁问题。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在多智能体系统中部署增加，但缺乏测试它们在资源竞争下协调能力的基准测试。现有基准未能充分评估LLM在并发资源访问场景下的协调能力。

Method: 基于哲学家就餐问题设计DPBench基准，包含8种不同条件（决策时序、群体规模、通信能力）。测试GPT-5.2、Claude Opus 4.5和Grok 4.1等主流LLM在多智能体协调任务中的表现。

Result: 发现显著不对称性：LLM在顺序决策设置中能有效协调，但在同时决策时失败严重，某些条件下死锁率超过95%。通信不仅未能解决问题，反而可能增加死锁率。失败源于收敛推理现象，即智能体独立得出相同策略，同时执行时必然导致死锁。

Conclusion: 需要并发资源访问的多智能体LLM系统可能需要外部协调机制，而非依赖涌现的协调能力。DPBench作为开源基准发布，为评估LLM协调能力提供标准工具。

Abstract: Large language models are increasingly deployed in multi-agent systems, yet we lack benchmarks that test whether they can coordinate under resource contention. We introduce DPBench, a benchmark based on the Dining Philosophers problem that evaluates LLM coordination across eight conditions that vary decision timing, group size, and communication. Our experiments with GPT-5.2, Claude Opus 4.5, and Grok 4.1 reveal a striking asymmetry: LLMs coordinate effectively in sequential settings but fail when decisions must be made simultaneously, with deadlock rates exceeding 95\% under some conditions. We trace this failure to convergent reasoning, where agents independently arrive at identical strategies that, when executed simultaneously, guarantee deadlock. Contrary to expectations, enabling communication does not resolve this problem and can even increase deadlock rates. Our findings suggest that multi-agent LLM systems requiring concurrent resource access may need external coordination mechanisms rather than relying on emergent coordination. DPBench is released as an open-source benchmark. Code and benchmark are available at https://github.com/najmulhasan-code/dpbench.

</details>


### [338] [Attention in Constant Time: Vashista Sparse Attention for Long-Context Decoding with Exponential Guarantees](https://arxiv.org/abs/2602.13804)
*Vashista Nobaub*

Main category: cs.AI

TL;DR: 论文提出Vashista稀疏注意力机制，通过理论证明注意力可以集中在少量关键token上，实现长上下文推理的稳定加速


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在长上下文推理中注意力计算成本高昂，但经验表明只有少量token对每个查询有实际贡献。需要理论分析和实用机制来安全地实现稀疏注意力，平衡准确性和计算效率。

Method: 1. 理论分析：将注意力建模为关键向量凸包上的投影，提出面稳定性定理，证明在严格互补边际条件下，熵注意力会集中在常数大小的活动面上；2. 实践机制：提出Vashista稀疏注意力，采用分页式上下文选择策略，为每个查询维护小的候选集，与现代推理栈兼容。

Result: 1. 理论结果：非活动token的总质量以指数速度衰减，活动面上的误差随温度参数线性缩放；2. 实践结果：在长上下文评估中观察到稳定的常数大小有效支持、显著的实时加速，以及在支持间隙诊断预测的范围内质量下降最小。

Conclusion: 该研究为稀疏长上下文解码提供了理论依据和实践机制，在隐私敏感和隔离环境中具有部署优势，能够实现可预测的延迟和成本，无需外部检索依赖。

Abstract: Large language models spend most of their inference cost on attention over long contexts, yet empirical behavior suggests that only a small subset of tokens meaningfully contributes to each query. We formalize this phenomenon by modeling attention as a projection onto the convex hull of key vectors and analyzing its entropic (softmax-like) relaxation. Our main theoretical contribution is a face-stability theorem showing that, under a strict complementarity margin (a support gap (Δ) certified by KKT multipliers), entropic attention concentrates on a constant-size active face: the total mass assigned to inactive tokens decays exponentially as (\exp(-Ω(Δ/\varepsilon))), while the error on the active face scales linearly in the temperature/regularization parameter (\varepsilon). This yields a practical criterion for when sparse long-context decoding is safe and provides a principled knob to trade accuracy for compute.
  Building on these guarantees, we introduce Vashista Sparse Attention, a drop-in mechanism that maintains a small candidate set per query through a paging-style context selection strategy compatible with modern inference stacks. Across long-context evaluations, we observe stable constant-size effective support, strong wall-clock speedups, and minimal quality degradation in the regimes predicted by the support-gap diagnostics. Finally, we discuss deployment implications for privacy-sensitive and air-gapped settings, where interchangeable attention modules enable predictable latency and cost without external retrieval dependencies.

</details>


### [339] [MAPLE: A Sub-Agent Architecture for Memory, Learning, and Personalization in Agentic AI Systems](https://arxiv.org/abs/2602.13258)
*Deepak Babu Piskala*

Main category: cs.AI

TL;DR: MAPLE框架将LLM智能体的记忆、学习和个性化分解为三个独立组件，通过专用子智能体架构实现更好的个性化适应能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体在处理个体用户适应时存在根本性限制，主要问题在于系统将记忆、学习和个性化视为统一能力而非三个需要不同基础设施、时间尺度和优化策略的独立机制。

Method: 提出MAPLE框架，将记忆、学习和个性化分解为三个独立组件：记忆组件负责存储和检索基础设施；学习组件异步从累积交互中提取智能；个性化组件在有限上下文预算内实时应用学习到的知识。每个组件作为专用子智能体运行，具有专门工具和明确定义的接口。

Result: 在MAPLE-Personas基准测试中，该分解方法相比无状态基线实现了14.6%的个性化分数提升（p < 0.01，Cohen's d = 0.95），并将特征融入率从45%提高到75%。

Conclusion: 通过将记忆、学习和个性化分解为独立组件，MAPLE框架使LLM智能体能够真正学习和适应，显著提升了个人化能力。

Abstract: Large language model (LLM) agents have emerged as powerful tools for complex tasks, yet their ability to adapt to individual users remains fundamentally limited. We argue this limitation stems from a critical architectural conflation: current systems treat memory, learning, and personalization as a unified capability rather than three distinct mechanisms requiring different infrastructure, operating on different timescales, and benefiting from independent optimization. We propose MAPLE (Memory-Adaptive Personalized LEarning), a principled decomposition where Memory handles storage and retrieval infrastructure; Learning extracts intelligence from accumulated interactions asynchronously; and Personalization applies learned knowledge in real-time within finite context budgets. Each component operates as a dedicated sub-agent with specialized tooling and well-defined interfaces. Experimental evaluation on the MAPLE-Personas benchmark demonstrates that our decomposition achieves a 14.6% improvement in personalization score compared to a stateless baseline (p < 0.01, Cohen's d = 0.95) and increases trait incorporation rate from 45% to 75% -- enabling agents that genuinely learn and adapt.

</details>


### [340] [General learned delegation by clones](https://arxiv.org/abs/2602.13262)
*Darren Li,Meiqi Chen,Chenze Shao,Fandong Meng,Jie Zhou*

Main category: cs.AI

TL;DR: SELFCEST 是一种通过智能体强化学习让基础模型能够并行生成克隆体来优化推理计算效率的方法，在固定推理预算下提升数学推理和长上下文问答的性能。


<details>
  <summary>Details</summary>
Motivation: 前沿语言模型虽然可以通过增加测试时计算来提升性能，但串行推理或无协调的并行采样在固定推理预算下计算效率低下。需要一种更高效的方法来分配计算资源。

Method: 提出 SELFCEST 方法，通过智能体强化学习让基础模型能够在并行上下文中生成相同权重的克隆体。训练采用端到端方式，在全局任务奖励下进行共享参数的回滚，学习到的控制器能够跨分支分配生成和上下文预算。

Result: 在具有挑战性的数学推理基准测试和长上下文多跳问答任务中，SELFCEST 在匹配推理预算下相对于单体基线改善了准确率-成本的帕累托前沿，并在两个领域都展现出分布外泛化能力。

Conclusion: SELFCEST 通过并行克隆和智能资源分配机制，有效提升了语言模型在固定推理预算下的计算效率，为优化推理过程提供了新思路。

Abstract: Frontier language models improve with additional test-time computation, but serial reasoning or uncoordinated parallel sampling can be compute-inefficient under fixed inference budgets. We propose SELFCEST, which equips a base model with the ability to spawn same-weight clones in separate parallel contexts by agentic reinforcement learning. Training is end-to-end under a global task reward with shared-parameter rollouts, yielding a learned controller that allocates both generation and context budget across branches. Across challenging math reasoning benchmarks and long-context multi-hop QA, SELFCEST improves the accuracy-cost Pareto frontier relative to monolithic baselines at matched inference budget, and exhibits out-of-distribution generalization in both domains.

</details>


### [341] [Statistical Early Stopping for Reasoning Models](https://arxiv.org/abs/2602.13935)
*Yangxinyu Xie,Tao Wang,Soham Mallick,Yan Sun,Georgy Noarov,Mengxin Yu,Tanwi Mallick,Weijie J. Su,Edgar Dobriban*

Main category: cs.AI

TL;DR: 论文提出两种基于统计原理的早期停止方法，通过监控生成过程中的不确定性信号来减少LLM在推理时过度思考的问题，提高效率和可靠性。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM的推理能力有了显著提升，但在面对不确定、定义不清或模糊的查询时，它们有时会过度思考，生成不必要的推理步骤，这影响了效率和可靠性。

Method: 提出了两种统计原理的早期停止方法：1）参数化方法：将不确定性关键词的出现间隔时间建模为更新过程，并应用序列测试进行停止决策；2）非参数化方法：为定义良好的查询提供有限样本保证，确保不会过早停止。

Result: 在多个领域和模型的推理任务上进行实证评估，结果表明不确定性感知的早期停止可以同时提高LLM推理的效率和可靠性，特别是在数学推理任务上观察到显著收益。

Conclusion: 通过监控不确定性信号实施早期停止是解决LLM过度思考问题的有效方法，能够平衡推理质量和效率，特别适用于数学推理等需要精确性的任务。

Abstract: While LLMs have seen substantial improvement in reasoning capabilities, they also sometimes overthink, generating unnecessary reasoning steps, particularly under uncertainty, given ill-posed or ambiguous queries. We introduce statistically principled early stopping methods that monitor uncertainty signals during generation to mitigate this issue. Our first approach is parametric: it models inter-arrival times of uncertainty keywords as a renewal process and applies sequential testing for stopping. Our second approach is nonparametric and provides finite-sample guarantees on the probability of halting too early on well-posed queries. We conduct empirical evaluations on reasoning tasks across several domains and models. Our results indicate that uncertainty-aware early stopping can improve both efficiency and reliability in LLM reasoning, and we observe especially significant gains for math reasoning.

</details>


### [342] [Human-Centered Explainable AI for Security Enhancement: A Deep Intrusion Detection Framework](https://arxiv.org/abs/2602.13271)
*Md Muntasir Jahid Ayan,Md. Shahriar Rashid,Tazzina Afroze Hassan,Hossain Md. Mubashshir Jamil,Mahbubul Islam,Lisan Al Amin,Rupak Kumar Das,Farzana Akter,Faisal Quader*

Main category: cs.AI

TL;DR: 提出结合可解释人工智能(XAI)的入侵检测系统框架，使用CNN和LSTM网络处理流量序列，并通过SHAP提供模型解释，在NSL-KDD数据集上达到高精度


<details>
  <summary>Details</summary>
Motivation: 网络威胁日益复杂，需要既准确又可解释的入侵检测系统，传统黑盒深度学习模型缺乏透明度，难以让安全分析师理解和验证决策

Method: 提出集成XAI的IDS框架，结合CNN和LSTM网络捕捉流量序列的时间依赖性，使用SHAP提供模型解释，并通过基于IPIP6和Big Five人格特质的专家调查评估系统可靠性和可用性

Result: 在NSL-KDD数据集上，CNN和LSTM准确率均达到0.99，LSTM在宏观平均精确率、召回率和F1分数上优于CNN，加权平均指标两者相似。SHAP识别出srv_serror_rate等关键特征，专家调查验证了系统可靠性

Conclusion: 结合性能和透明度的网络安全解决方案具有潜力，建议通过自适应学习实现实时威胁检测作为未来改进方向

Abstract: The increasing complexity and frequency of cyber-threats demand intrusion detection systems (IDS) that are not only accurate but also interpretable. This paper presented a novel IDS framework that integrated Explainable Artificial Intelligence (XAI) to enhance transparency in deep learning models. The framework was evaluated experimentally using the benchmark dataset NSL-KDD, demonstrating superior performance compared to traditional IDS and black-box deep learning models. The proposed approach combined Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) networks for capturing temporal dependencies in traffic sequences. Our deep learning results showed that both CNN and LSTM reached 0.99 for accuracy, whereas LSTM outperformed CNN at macro average precision, recall, and F-1 score. For weighted average precision, recall, and F-1 score, both models scored almost similarly. To ensure interpretability, the XAI model SHapley Additive exPlanations (SHAP) was incorporated, enabling security analysts to understand and validate model decisions. Some notable influential features were srv_serror_rate, dst_host_srv_serror_rate, and serror_rate for both models, as pointed out by SHAP. We also conducted a trust-focused expert survey based on IPIP6 and Big Five personality traits via an interactive UI to evaluate the system's reliability and usability. This work highlighted the potential of combining performance and transparency in cybersecurity solutions and recommends future enhancements through adaptive learning for real-time threat detection.

</details>


### [343] [TemporalBench: A Benchmark for Evaluating LLM-Based Agents on Contextual and Event-Informed Time Series Tasks](https://arxiv.org/abs/2602.13272)
*Muyan Weng,Defu Cao,Wei Yang,Yashaswi Sharma,Yan Liu*

Main category: cs.AI

TL;DR: TemporalBench是一个多领域基准测试，用于评估模型在渐进丰富信息设置下的时间推理能力，揭示传统预测准确性无法反映真实的时间理解和上下文推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前不清楚强大的预测性能是否反映真实的时间理解能力，还是仅仅是在上下文和事件驱动条件下的推理能力。需要区分模型是否真正理解时间模式，还是只是利用统计相关性进行预测。

Method: 提出TemporalBench基准，采用四层任务分类法：历史结构解释、无上下文预测、上下文时间推理、事件条件预测，覆盖零售、医疗、能源和物理系统四个现实领域。通过控制对未来目标和上下文信息的访问，进行诊断性分析。

Result: 广泛基线实验显示，强大的数值预测准确性并不能可靠地转化为稳健的上下文或事件感知的时间推理能力。现有智能体框架表现出碎片化的优势和系统性失败模式，这些在仅关注预测的基准测试中通常被隐藏。

Conclusion: 需要超越传统预测基准，开发能够真正理解时间模式、对齐外部上下文并在条件变化时适应预测的模型。TemporalBench为评估时间推理行为提供了诊断工具，数据集和排行榜已公开。

Abstract: It is unclear whether strong forecasting performance reflects genuine temporal understanding or the ability to reason under contextual and event-driven conditions. We introduce TemporalBench, a multi-domain benchmark designed to evaluate temporal reasoning behavior under progressively richer informational settings. TemporalBench adopts a four-tier task taxonomy that examines historical structure interpretation, context-free forecasting, contextual temporal reasoning, and event-conditioned prediction across four real-world domains: retail, healthcare, energy, and physical systems. By controlling access to future targets and contextual information, the benchmark enables a diagnostic analysis of whether models can correctly interpret temporal patterns, align them with external context, and adapt predictions when conditions change. Extensive baseline experiments show that strong numerical forecasting accuracy does not reliably translate into robust contextual or event-aware temporal reasoning; instead, existing agent frameworks exhibit fragmented strengths and systematic failure modes that remain largely hidden under forecasting-only benchmarks. The TemporalBench dataset is publicly available at https://huggingface.co/datasets/Melady/TemporalBench, and we additionally provide a public leaderboard at https://huggingface.co/spaces/Melady/TemporalBench_Leaderboard.

</details>


### [344] [ProMoral-Bench: Evaluating Prompting Strategies for Moral Reasoning and Safety in LLMs](https://arxiv.org/abs/2602.13274)
*Rohan Subramanian Thomas,Shikhar Shiromani,Abdullah Chaudhry,Ruizhe Li,Vasu Sharma,Kevin Zhu,Sunishchal Dev*

Main category: cs.AI

TL;DR: ProMoral-Bench是一个统一的基准测试，评估11种提示范式在4个LLM家族中的表现，发现简洁的示例引导提示优于复杂多阶段推理，提供更高的道德安全分数和更强的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 提示设计对大型语言模型的道德能力和安全对齐有显著影响，但现有研究在数据集和模型上的比较较为分散，缺乏统一的评估框架。

Method: 引入ProMoral-Bench基准，使用ETHICS、Scruples、WildJailbreak和新设计的ETHICS-Contrast鲁棒性测试，通过统一道德安全分数（UMSS）评估11种提示范式在4个LLM家族中的表现。

Result: 紧凑的示例引导支架优于复杂多阶段推理，提供更高的UMSS分数和更强的鲁棒性，且token成本更低。少样本示例能持续增强道德稳定性和越狱抵抗能力，而多轮推理在扰动下表现脆弱。

Conclusion: ProMoral-Bench为原则性、成本效益高的提示工程建立了标准化框架，表明简洁的示例引导提示是更有效的道德安全对齐方法。

Abstract: Prompt design significantly impacts the moral competence and safety alignment of large language models (LLMs), yet empirical comparisons remain fragmented across datasets and models.We introduce ProMoral-Bench, a unified benchmark evaluating 11 prompting paradigms across four LLM families. Using ETHICS, Scruples, WildJailbreak, and our new robustness test, ETHICS-Contrast, we measure performance via our proposed Unified Moral Safety Score (UMSS), a metric balancing accuracy and safety. Our results show that compact, exemplar-guided scaffolds outperform complex multi-stage reasoning, providing higher UMSS scores and greater robustness at a lower token cost. While multi-turn reasoning proves fragile under perturbations, few-shot exemplars consistently enhance moral stability and jailbreak resistance. ProMoral-Bench establishes a standardized framework for principled, cost-effective prompt engineering.

</details>


### [345] [Accuracy Standards for AI at Work vs. Personal Life: Evidence from an Online Survey](https://arxiv.org/abs/2602.13283)
*Gaston Besanson,Federico Todeschini*

Main category: cs.AI

TL;DR: 研究人们在专业与个人场景中使用AI工具时的准确性权衡差异，发现工作场景对准确性的要求显著高于个人场景，且工具不可用时对个人生活影响更大。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统（特别是生成模型）能产生可接受但不完全相同的输出，研究需要理解人们在专业和个人场景中如何权衡AI工具的准确性，以及当AI不可用时如何应对。

Method: 通过在线调查（N=300）研究，将准确性定义为情境特定的可靠性：输出在特定容忍阈值内与用户意图对齐的程度，该阈值取决于风险水平和修正成本。

Result: 工作场景要求高准确性的比例（24.1%）显著高于个人场景（8.8%），差距在多种定义下都显著；重度应用使用和经验模式与更严格的工作标准相关；工具不可用时，个人生活受影响程度（34.1%）大于工作场景（15.3%）。

Conclusion: 人们在专业场景中对AI准确性要求更高，而在个人场景中容忍度更大；工具不可用对个人生活影响更大，表明AI工具已深度融入个人日常；研究为AI系统设计提供了情境化准确性需求的重要见解。

Abstract: We study how people trade off accuracy when using AI-powered tools in professional versus personal contexts for adoption purposes, the determinants of those trade-offs, and how users cope when AI/apps are unavailable. Because modern AI systems (especially generative models) can produce acceptable but non-identical outputs, we define "accuracy" as context-specific reliability: the degree to which an output aligns with the user's intent within a tolerance threshold that depends on stakes and the cost of correction. In an online survey (N=300), among respondents with both accuracy items (N=170), the share requiring high accuracy (top-box) is 24.1% at work vs. 8.8% in personal life (+15.3 pp; z=6.29, p<0.001). The gap remains large under a broader top-two-box definition (67.0% vs. 32.9%) and on the full 1-5 ordinal scale (mean 3.86 vs. 3.08). Heavy app use and experience patterns correlate with stricter work standards (H2). When tools are unavailable (H3), respondents report more disruption in personal routines than at work (34.1% vs. 15.3%, p<0.01). We keep the main text focused on these substantive results and place test taxonomy and power derivations in a technical appendix.

</details>


### [346] [Artificial Organisations](https://arxiv.org/abs/2602.13275)
*William Waites*

Main category: cs.AI

TL;DR: 论文提出通过组织架构设计而非个体对齐来实现多智能体系统的可靠性，借鉴人类机构的组织理论，通过信息隔离和对抗性审查实现可靠集体行为。


<details>
  <summary>Details</summary>
Motivation: 当前对齐研究主要关注单个AI系统的可靠性，但人类机构通过组织结构而非个体对齐来实现可靠的集体行为。多智能体AI系统应借鉴这种机构模型，通过架构设计而非假设个体对齐来实现可靠性。

Method: 提出Perseverance Composition Engine多智能体系统，包含三个角色：Composer起草文本，Corroborator验证事实依据（有完整源访问权限），Critic评估论证质量（无源访问权限）。通过系统架构强制执行信息不对称，实现分层验证。

Result: 在474个文档撰写任务中观察到与机构假设一致的模式。当分配需要伪造内容的不可行任务时，系统能从尝试伪造转向诚实拒绝并提出替代方案，这种行为既未指令也未个体激励。结果显示架构强制执行可能从不可靠组件产生可靠结果。

Conclusion: 组织理论为多智能体AI安全提供了富有成效的框架。通过将验证和评估作为通过信息隔离强制执行的结构属性，机构设计为从不可靠的个体组件实现可靠的集体行为提供了一条途径。

Abstract: Alignment research focuses on making individual AI systems reliable. Human institutions achieve reliable collective behaviour differently: they mitigate the risk posed by misaligned individuals through organisational structure. Multi-agent AI systems should follow this institutional model using compartmentalisation and adversarial review to achieve reliable outcomes through architectural design rather than assuming individual alignment.
  We demonstrate this approach through the Perseverance Composition Engine, a multi-agent system for document composition. The Composer drafts text, the Corroborator verifies factual substantiation with full source access, and the Critic evaluates argumentative quality without access to sources: information asymmetry enforced by system architecture. This creates layered verification: the Corroborator detects unsupported claims, whilst the Critic independently assesses coherence and completeness. Observations from 474 composition tasks (discrete cycles of drafting, verification, and evaluation) exhibit patterns consistent with the institutional hypothesis. When assigned impossible tasks requiring fabricated content, this iteration enabled progression from attempted fabrication toward honest refusal with alternative proposals--behaviour neither instructed nor individually incentivised. These findings motivate controlled investigation of whether architectural enforcement produces reliable outcomes from unreliable components.
  This positions organisational theory as a productive framework for multi-agent AI safety. By implementing verification and evaluation as structural properties enforced through information compartmentalisation, institutional design offers a route to reliable collective behaviour from unreliable individual components.

</details>


### [347] [ForesightSafety Bench: A Frontier Risk Evaluation and Governance Framework towards Safe AI](https://arxiv.org/abs/2602.14135)
*Haibo Tong,Feifei Zhao,Linghao Feng,Ruoyu Wu,Ruolin Chen,Lu Jia,Zhou Zhao,Jindong Li,Tenglong Li,Erliang Lin,Shuai Yang,Enmeng Lu,Yinqian Sun,Qian Zhang,Zizhe Ruan,Zeyang Yue,Ping Wu,Huangrui Li,Chengyi Sun,Yi Zeng*

Main category: cs.AI

TL;DR: 提出了ForesightSafety Bench AI安全评估框架，包含7大基础安全支柱和94个细化风险维度，评估了20多个主流大模型，发现前沿AI存在广泛安全漏洞


<details>
  <summary>Details</summary>
Motivation: AI快速发展展现出越来越强的自主性和目标导向能力，伴随的系统性风险更加不可预测、难以控制且可能不可逆。当前AI安全评估系统存在风险维度受限、前沿风险检测失败等关键局限，滞后的安全基准和对齐技术难以应对前沿AI模型的复杂挑战。

Method: 提出ForesightSafety Bench AI安全评估框架，从7大基础安全支柱开始，逐步扩展到高级具身AI安全、AI4Science安全、社会与环境AI风险、灾难性和存在性风险，以及8个关键工业安全领域，形成总共94个细化风险维度。收集了数万个结构化风险数据和评估结果。

Result: 对20多个主流先进大模型进行系统评估和深入分析，识别出关键风险模式及其能力边界。安全能力评估结果显示前沿AI在多个支柱上存在广泛的安全漏洞，特别是在风险自主代理、AI4Science安全、具身AI安全、社会AI安全以及灾难性和存在性风险方面。

Conclusion: 建立了一个广泛覆盖、层次清晰、动态演进的AI安全评估框架，为应对前沿AI带来的复杂安全挑战提供了系统性的评估工具和方法。

Abstract: Rapidly evolving AI exhibits increasingly strong autonomy and goal-directed capabilities, accompanied by derivative systemic risks that are more unpredictable, difficult to control, and potentially irreversible. However, current AI safety evaluation systems suffer from critical limitations such as restricted risk dimensions and failed frontier risk detection. The lagging safety benchmarks and alignment technologies can hardly address the complex challenges posed by cutting-edge AI models. To bridge this gap, we propose the "ForesightSafety Bench" AI Safety Evaluation Framework, beginning with 7 major Fundamental Safety pillars and progressively extends to advanced Embodied AI Safety, AI4Science Safety, Social and Environmental AI risks, Catastrophic and Existential Risks, as well as 8 critical industrial safety domains, forming a total of 94 refined risk dimensions. To date, the benchmark has accumulated tens of thousands of structured risk data points and assessment results, establishing a widely encompassing, hierarchically clear, and dynamically evolving AI safety evaluation framework. Based on this benchmark, we conduct systematic evaluation and in-depth analysis of over twenty mainstream advanced large models, identifying key risk patterns and their capability boundaries. The safety capability evaluation results reveals the widespread safety vulnerabilities of frontier AI across multiple pillars, particularly focusing on Risky Agentic Autonomy, AI4Science Safety, Embodied AI Safety, Social AI Safety and Catastrophic and Existential Risks. Our benchmark is released at https://github.com/Beijing-AISI/ForesightSafety-Bench. The project website is available at https://foresightsafety-bench.beijing-aisi.ac.cn/.

</details>


### [348] [BEAGLE: Behavior-Enforced Agent for Grounded Learner Emulation](https://arxiv.org/abs/2602.13280)
*Hanchen David Wang,Clayton Cohn,Zifan Xu,Siyuan Guo,Gautam Biswas,Meiyi Ma*

Main category: cs.AI

TL;DR: BEAGLE是一个神经符号框架，通过整合自我调节学习理论来模拟学生在开放式问题解决环境中的真实学习行为，解决了LLM在模拟学生时存在的"能力偏差"问题。


<details>
  <summary>Details</summary>
Motivation: 收集真实学生学习行为数据面临隐私问题和纵向研究成本高的挑战。虽然大语言模型（LLMs）提供了模拟学生的可能途径，但它们存在"能力偏差"——倾向于优化效率和正确性，而不是模拟新手学习者典型的反复、迭代的挣扎过程。

Method: BEAGLE整合了三个关键技术创新：1）半马尔可夫模型控制认知行为和元认知行为的时间与转换；2）带有显式缺陷注入的贝叶斯知识追踪，强制实现真实的知识空白和"未知的未知"；3）解耦的智能体设计，将高级策略使用与代码生成动作分离，防止模型静默纠正自己的故意错误。

Result: 在Python编程任务评估中，BEAGLE在重现真实学习轨迹方面显著优于最先进的基线方法。在人类图灵测试中，用户无法区分合成轨迹和真实学生数据，准确率与随机猜测无显著差异（52.8%）。

Conclusion: BEAGLE通过整合自我调节学习理论和神经符号方法，成功解决了LLM模拟学生时的能力偏差问题，能够生成与真实学生行为难以区分的合成学习轨迹，为教育研究和自适应教学系统提供了有价值的工具。

Abstract: Simulating student learning behaviors in open-ended problem-solving environments holds potential for education research, from training adaptive tutoring systems to stress-testing pedagogical interventions. However, collecting authentic data is challenging due to privacy concerns and the high cost of longitudinal studies. While Large Language Models (LLMs) offer a promising path to student simulation, they suffer from competency bias, optimizing for efficient correctness rather than the erratic, iterative struggle characteristic of novice learners. We present BEAGLE, a neuro-symbolic framework that addresses this bias by incorporating Self-Regulated Learning (SRL) theory into a novel architecture. BEAGLE integrates three key technical innovations: (1) a semi-Markov model that governs the timing and transitions of cognitive behaviors and metacognitive behaviors; (2) Bayesian Knowledge Tracing with explicit flaw injection to enforce realistic knowledge gaps and "unknown unknowns"; and (3) a decoupled agent design that separates high-level strategy use from code generation actions to prevent the model from silently correcting its own intentional errors. In evaluations on Python programming tasks, BEAGLE significantly outperforms state-of-the-art baselines in reproducing authentic trajectories. In a human Turing test, users were unable to distinguish synthetic traces from real student data, achieving an accuracy indistinguishable from random guessing (52.8%).

</details>


### [349] [Frontier AI Risk Management Framework in Practice: A Risk Analysis Technical Report v1.5](https://arxiv.org/abs/2602.14457)
*Dongrui Liu,Yi Yu,Jie Zhang,Guanxu Chen,Qihao Lin,Hanxi Zhu,Lige Huang,Yijin Zhou,Peng Wang,Shuai Shao,Boxuan Zhang,Zicheng Liu,Jingwei Sun,Yu Li,Yuejin Xie,Jiaxuan Guo,Jia Xu,Chaochao Lu,Bowen Zhou,Xia Hu,Jing Shao*

Main category: cs.AI

TL;DR: 该论文提出了前沿AI风险管理框架，评估了大型语言模型在网络安全、说服操纵、战略欺骗、失控AI研发和自复制五个维度的风险，并提出了相应的缓解策略。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型能力快速演进和智能体AI的普及，需要全面评估前沿AI模型带来的前所未有的风险，为安全部署提供技术路径。

Method: 采用更新的风险评估框架，对五个关键维度进行细粒度评估：引入更复杂的网络攻击场景、评估LLM间说服风险、新增紧急错位实验、关注智能体"错误演化"、引入资源受限的自复制场景，并监控OpenClaw在Moltbook上的安全表现。

Result: 识别了前沿AI在五个维度的具体风险，提出并验证了一系列稳健的缓解策略，为前沿AI的安全部署提供了初步的技术和可操作路径。

Conclusion: 该工作反映了当前对AI前沿风险的理解，强调需要采取集体行动来缓解这些挑战，为AI安全部署提供了实践框架。

Abstract: To understand and identify the unprecedented risks posed by rapidly advancing artificial intelligence (AI) models, Frontier AI Risk Management Framework in Practice presents a comprehensive assessment of their frontier risks. As Large Language Models (LLMs) general capabilities rapidly evolve and the proliferation of agentic AI, this version of the risk analysis technical report presents an updated and granular assessment of five critical dimensions: cyber offense, persuasion and manipulation, strategic deception, uncontrolled AI R\&D, and self-replication. Specifically, we introduce more complex scenarios for cyber offense. For persuasion and manipulation, we evaluate the risk of LLM-to-LLM persuasion on newly released LLMs. For strategic deception and scheming, we add the new experiment with respect to emergent misalignment. For uncontrolled AI R\&D, we focus on the ``mis-evolution'' of agents as they autonomously expand their memory substrates and toolsets. Besides, we also monitor and evaluate the safety performance of OpenClaw during the interaction on the Moltbook. For self-replication, we introduce a new resource-constrained scenario. More importantly, we propose and validate a series of robust mitigation strategies to address these emerging threats, providing a preliminary technical and actionable pathway for the secure deployment of frontier AI. This work reflects our current understanding of AI frontier risks and urges collective action to mitigate these challenges.

</details>


### [350] [Mirror: A Multi-Agent System for AI-Assisted Ethics Review](https://arxiv.org/abs/2602.13292)
*Yifan Ding,Yuhui Shi,Zhiyan Li,Zilong Wang,Yifeng Gao,Yajun Yang,Mengjie Yang,Yixiu Liang,Xipeng Qiu,Xuanjing Huang,Xingjun Ma,Yu-Gang Jiang,Guoyu Wang*

Main category: cs.AI

TL;DR: Mirror是一个AI辅助伦理审查的智能体框架，通过EthicsLLM模型和两种审查模式（快速审查和委员会审查）提升伦理审查的质量、一致性和专业性。


<details>
  <summary>Details</summary>
Motivation: 现代研究治理中的伦理审查系统面临压力，大规模跨学科科学实践带来的结构性伦理风险暴露了机构审查能力的局限性，而非伦理监督的合法性问题。大型语言模型为伦理审查提供了新机遇，但直接应用受到伦理推理能力不足、与监管结构整合薄弱以及真实审查材料隐私限制的制约。

Method: 提出了Mirror框架，核心是EthicsLLM模型，该模型在EthicsQA数据集（41K个问题-思维链-答案三元组）上微调。框架包含两种模式：Mirror-ER通过可执行规则库自动化快速审查，Mirror-CR通过专家代理、伦理秘书代理和主要研究者代理的协调互动模拟完整委员会审议。

Result: 实证评估表明，Mirror相比强大的通用LLM显著提高了伦理评估的质量、一致性和专业性。

Conclusion: Mirror框架通过整合伦理推理、结构化规则解释和多代理审议，为AI辅助伦理审查提供了有效解决方案，能够支持不同风险水平研究的伦理审查需求。

Abstract: Ethics review is a foundational mechanism of modern research governance, yet contemporary systems face increasing strain as ethical risks arise as structural consequences of large-scale, interdisciplinary scientific practice. The demand for consistent and defensible decisions under heterogeneous risk profiles exposes limitations in institutional review capacity rather than in the legitimacy of ethics oversight. Recent advances in large language models (LLMs) offer new opportunities to support ethics review, but their direct application remains limited by insufficient ethical reasoning capability, weak integration with regulatory structures, and strict privacy constraints on authentic review materials. In this work, we introduce Mirror, an agentic framework for AI-assisted ethical review that integrates ethical reasoning, structured rule interpretation, and multi-agent deliberation within a unified architecture. At its core is EthicsLLM, a foundational model fine-tuned on EthicsQA, a specialized dataset of 41K question-chain-of-thought-answer triples distilled from authoritative ethics and regulatory corpora. EthicsLLM provides detailed normative and regulatory understanding, enabling Mirror to operate in two complementary modes. Mirror-ER (expedited Review) automates expedited review through an executable rule base that supports efficient and transparent compliance checks for minimal-risk studies. Mirror-CR (Committee Review) simulates full-board deliberation through coordinated interactions among expert agents, an ethics secretary agent, and a principal investigator agent, producing structured, committee-level assessments across ten ethical dimensions. Empirical evaluations demonstrate that Mirror significantly improves the quality, consistency, and professionalism of ethics assessments compared with strong generalist LLMs.

</details>


### [351] [AI Arms and Influence: Frontier Models Exhibit Sophisticated Reasoning in Simulated Nuclear Crises](https://arxiv.org/abs/2602.14740)
*Kenneth Payne*

Main category: cs.AI

TL;DR: 大型语言模型在核危机模拟中表现出复杂的战略行为，包括欺骗、心理理论和元认知能力，但与传统战略理论存在显著差异，核禁忌无法阻止核升级，威胁常引发反升级而非服从。


<details>
  <summary>Details</summary>
Motivation: 研究前沿大型语言模型在战略竞争中的行为模式，特别是核危机情境下的决策逻辑，既为国家安全专业人员提供直接应用，也为理解AI在不确定性下的推理机制提供更广泛的见解。

Method: 设计核危机模拟实验，让三个前沿大语言模型（GPT-5.2、Claude Sonnet 4、Gemini 3 Flash）扮演对立领导人角色，观察它们在战略竞争中的行为模式和决策逻辑。

Result: 模型表现出欺骗、心理理论和元认知能力，支持部分传统战略理论，但发现核禁忌无法阻止核升级，战略核攻击虽罕见但确实发生，威胁更多引发反升级而非服从，高可信度反而加速冲突，模型从不选择妥协或撤退。

Conclusion: AI模拟是强大的战略分析工具，但必须根据已知的人类推理模式进行适当校准。理解前沿模型如何模仿或不模仿人类战略逻辑，对于AI日益影响战略结果的世界至关重要。

Abstract: Today's leading AI models engage in sophisticated behaviour when placed in strategic competition. They spontaneously attempt deception, signaling intentions they do not intend to follow; they demonstrate rich theory of mind, reasoning about adversary beliefs and anticipating their actions; and they exhibit credible metacognitive self-awareness, assessing their own strategic abilities before deciding how to act.
  Here we present findings from a crisis simulation in which three frontier large language models (GPT-5.2, Claude Sonnet 4, Gemini 3 Flash) play opposing leaders in a nuclear crisis. Our simulation has direct application for national security professionals, but also, via its insights into AI reasoning under uncertainty, has applications far beyond international crisis decision-making.
  Our findings both validate and challenge central tenets of strategic theory. We find support for Schelling's ideas about commitment, Kahn's escalation framework, and Jervis's work on misperception, inter alia. Yet we also find that the nuclear taboo is no impediment to nuclear escalation by our models; that strategic nuclear attack, while rare, does occur; that threats more often provoke counter-escalation than compliance; that high mutual credibility accelerated rather than deterred conflict; and that no model ever chose accommodation or withdrawal even when under acute pressure, only reduced levels of violence.
  We argue that AI simulation represents a powerful tool for strategic analysis, but only if properly calibrated against known patterns of human reasoning. Understanding how frontier models do and do not imitate human strategic logic is essential preparation for a world in which AI increasingly shapes strategic outcomes.

</details>


### [352] [DECKBench: Benchmarking Multi-Agent Frameworks for Academic Slide Generation and Editing](https://arxiv.org/abs/2602.13318)
*Daesik Jang,Morgan Lindsay Heisler,Linzi Xing,Yifei Li,Edward Wang,Ying Xiong,Yong Zhang,Zhenan Fan*

Main category: cs.AI

TL;DR: DECKBench：一个用于评估多智能体幻灯片生成与编辑的基准测试框架，包含数据集和评估协议，重点关注内容保真度、连贯性、布局质量和多轮指令遵循。


<details>
  <summary>Details</summary>
Motivation: 现有基准和评估协议无法充分衡量学术幻灯片自动生成和迭代编辑的挑战，包括忠实内容选择、连贯幻灯片组织、布局感知渲染和鲁棒的多轮指令遵循。

Method: 构建DECKBench基准，包含基于论文-幻灯片对的数据集和模拟编辑指令；设计系统评估协议；实现模块化多智能体基线系统，将任务分解为论文解析、幻灯片规划、HTML创建和迭代编辑。

Result: 实验结果表明，该基准能够有效揭示多智能体幻灯片生成和编辑系统的优势和失败模式，为系统改进提供可操作的见解。

Conclusion: 该工作为学术演示文稿生成和编辑的可重复、可比较评估建立了标准化基础，相关代码和数据已公开。

Abstract: Automatically generating and iteratively editing academic slide decks requires more than document summarization. It demands faithful content selection, coherent slide organization, layout-aware rendering, and robust multi-turn instruction following. However, existing benchmarks and evaluation protocols do not adequately measure these challenges. To address this gap, we introduce the Deck Edits and Compliance Kit Benchmark (DECKBench), an evaluation framework for multi-agent slide generation and editing. DECKBench is built on a curated dataset of paper to slide pairs augmented with realistic, simulated editing instructions. Our evaluation protocol systematically assesses slide-level and deck-level fidelity, coherence, layout quality, and multi-turn instruction following. We further implement a modular multi-agent baseline system that decomposes the slide generation and editing task into paper parsing and summarization, slide planning, HTML creation, and iterative editing. Experimental results demonstrate that the proposed benchmark highlights strengths, exposes failure modes, and provides actionable insights for improving multi-agent slide generation and editing systems. Overall, this work establishes a standardized foundation for reproducible and comparable evaluation of academic presentation generation and editing. Code and data are publicly available at https://github.com/morgan-heisler/DeckBench .

</details>


### [353] [Situation Graph Prediction: Structured Perspective Inference for User Modeling](https://arxiv.org/abs/2602.13319)
*Jisung Shin,Daniel Platnick,Marjan Alirezaie,Hossein Rahnama*

Main category: cs.AI

TL;DR: 论文提出情境图预测(SGP)任务，将视角建模转化为逆推理问题，通过结构优先的合成数据生成策略解决视角状态标注数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 视角感知AI需要建模动态的内部状态（目标、情感、上下文），而不仅仅是偏好。但面临数据瓶颈：数字足迹涉及隐私敏感，且视角状态很少被标注。

Method: 提出情境图预测(SGP)任务，将视角建模构建为逆推理问题：从可观察的多模态痕迹中重建结构化、本体对齐的视角表示。采用结构优先的合成生成策略，设计上对齐潜在标签和可观察痕迹。

Result: 使用GPT-4o进行诊断研究，发现表面层提取和潜在视角推理之间存在差距，表明在受控设置下潜在状态推理比表面提取更难。结果证明SGP任务非平凡，并为结构优先的数据合成策略提供了证据。

Conclusion: SGP任务为解决视角建模的数据瓶颈提供了可行框架，结构优先的合成数据生成策略是有效的，但潜在状态推理仍具挑战性，需要进一步研究。

Abstract: Perspective-Aware AI requires modeling evolving internal states--goals, emotions, contexts--not merely preferences. Progress is limited by a data bottleneck: digital footprints are privacy-sensitive and perspective states are rarely labeled. We propose Situation Graph Prediction (SGP), a task that frames perspective modeling as an inverse inference problem: reconstructing structured, ontology-aligned representations of perspective from observable multimodal artifacts. To enable grounding without real labels, we use a structure-first synthetic generation strategy that aligns latent labels and observable traces by design. As a pilot, we construct a dataset and run a diagnostic study using retrieval-augmented in-context learning as a proxy for supervision. In our study with GPT-4o, we observe a gap between surface-level extraction and latent perspective inference--indicating latent-state inference is harder than surface extraction under our controlled setting. Results suggest SGP is non-trivial and provide evidence for the structure-first data synthesis strategy.

</details>


### [354] [Concept Influence: Leveraging Interpretability to Improve Performance and Efficiency in Training Data Attribution](https://arxiv.org/abs/2602.14869)
*Matthew Kowal,Goncalo Paulo,Louis Jaburi,Tom Tseng,Lev E McKinney,Stefan Heimersheim,Aaron David Tucker,Adam Gleave,Kellin Pelrine*

Main category: cs.AI

TL;DR: 论文提出Concept Influence方法，通过语义方向（而非单个测试样本）来追溯模型行为到训练数据，比传统影响函数更高效且能捕捉更抽象的语义相似性。


<details>
  <summary>Details</summary>
Motivation: 现有训练数据归因方法（如影响函数）计算成本高且基于单个测试样本，容易偏向句法相似性而非语义相似性，难以追溯模型抽象行为到训练数据。

Method: 提出Concept Influence方法，将模型行为归因到语义方向（如线性探针或稀疏自编码器特征），而非单个测试样本。同时证明基于探针的归因方法是Concept Influence的一阶近似，计算效率更高。

Result: 在涌现错位基准和实际后训练数据集上验证，Concept Influence及其近似方法与传统影响函数性能相当，但计算效率提升一个数量级以上，更具可扩展性。

Conclusion: 将可解释结构融入传统TDA流程可实现更可扩展、可解释的模型行为控制，通过数据更好地理解和调控模型行为。

Abstract: As large language models are increasingly trained and fine-tuned, practitioners need methods to identify which training data drive specific behaviors, particularly unintended ones. Training Data Attribution (TDA) methods address this by estimating datapoint influence. Existing approaches like influence functions are both computationally expensive and attribute based on single test examples, which can bias results toward syntactic rather than semantic similarity. To address these issues of scalability and influence to abstract behavior, we leverage interpretable structures within the model during the attribution. First, we introduce Concept Influence which attribute model behavior to semantic directions (such as linear probes or sparse autoencoder features) rather than individual test examples. Second, we show that simple probe-based attribution methods are first-order approximations of Concept Influence that achieve comparable performance while being over an order-of-magnitude faster. We empirically validate Concept Influence and approximations across emergent misalignment benchmarks and real post-training datasets, and demonstrate they achieve comparable performance to classical influence functions while being substantially more scalable. More broadly, we show that incorporating interpretable structure within traditional TDA pipelines can enable more scalable, explainable, and better control of model behavior through data.

</details>


### [355] [Information Fidelity in Tool-Using LLM Agents: A Martingale Analysis of the Model Context Protocol](https://arxiv.org/abs/2602.13320)
*Flint Xiaofeng Fan,Cheston Tan,Roger Wattenhofer,Yew-Soon Ong*

Main category: cs.AI

TL;DR: 提出首个分析MCP智能体错误累积的理论框架，证明累积失真呈线性增长且高概率偏差有界，为可信AI系统部署提供理论保证。


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型的AI智能体越来越多地使用外部工具进行高风险决策，一个关键可靠性问题出现：错误如何在连续工具调用中传播？需要理论框架来分析错误累积。

Method: 引入混合失真度量（结合离散事实匹配和连续语义相似度），建立马尔可夫链浓度界限来分析序列工具交互中的错误传播，并在Qwen2-7B、Llama-3-8B和Mistral-7B上进行实验验证。

Result: 理论证明累积失真呈线性增长，高概率偏差有O(√T)界限；实验验证失真跟踪线性趋势，语义加权减少80%失真，每约9步重新接地足以控制错误。

Conclusion: 该理论框架为可信AI智能体系统提供了可预测的错误传播保证，排除了指数级故障模式，并将浓度保证转化为可部署的实际原则。

Abstract: As AI agents powered by large language models (LLMs) increasingly use external tools for high-stakes decisions, a critical reliability question arises: how do errors propagate across sequential tool calls? We introduce the first theoretical framework for analyzing error accumulation in Model Context Protocol (MCP) agents, proving that cumulative distortion exhibits linear growth and high-probability deviations bounded by $O(\sqrt{T})$. This concentration property ensures predictable system behavior and rules out exponential failure modes. We develop a hybrid distortion metric combining discrete fact matching with continuous semantic similarity, then establish martingale concentration bounds on error propagation through sequential tool interactions. Experiments across Qwen2-7B, Llama-3-8B, and Mistral-7B validate our theoretical predictions, showing empirical distortion tracks the linear trend with deviations consistently within $O(\sqrt{T})$ envelopes. Key findings include: semantic weighting reduces distortion by 80\%, and periodic re-grounding approximately every 9 steps suffices for error control. We translate these concentration guarantees into actionable deployment principles for trustworthy agent systems.

</details>


### [356] [Detecting Jailbreak Attempts in Clinical Training LLMs Through Automated Linguistic Feature Extraction](https://arxiv.org/abs/2602.13321)
*Tri Nguyen,Huy Hoang Bao Le,Lohith Srikanth Pentapalli,Laurah Turner,Kelly Cohen*

Main category: cs.AI

TL;DR: 使用专家标注的四个核心语言特征，训练BERT模型进行特征提取，再通过多种分类器检测临床LLM中的越狱行为，实现了可扩展且可解释的检测系统。


<details>
  <summary>Details</summary>
Motivation: 临床训练LLM中检测越狱尝试需要准确建模表示不安全或偏离任务用户行为的语言偏差。先前工作依赖手动标注特征，限制了可扩展性和表达能力。

Method: 使用专家标注的四个核心语言特征（专业性、医学相关性、伦理行为、上下文干扰），训练通用领域和医学领域的BERT模型预测这些特征，选择最可靠的特征回归器作为特征提取器，再通过树基、线性、概率和集成方法等分类器预测越狱可能性。

Result: 系统在交叉验证和保留集评估中表现出色，表明LLM衍生的语言特征为自动化越狱检测提供了有效基础。错误分析揭示了当前标注和特征表示的关键限制。

Conclusion: 这项工作展示了在安全关键临床对话系统中检测越狱行为的可扩展且可解释的方法，指出了未来改进方向：更丰富的标注方案、更细粒度的特征提取以及捕捉对话过程中越狱行为演变风险的方法。

Abstract: Detecting jailbreak attempts in clinical training large language models (LLMs) requires accurate modeling of linguistic deviations that signal unsafe or off-task user behavior. Prior work on the 2-Sigma clinical simulation platform showed that manually annotated linguistic features could support jailbreak detection. However, reliance on manual annotation limited both scalability and expressiveness. In this study, we extend this framework by using experts' annotations of four core linguistic features (Professionalism, Medical Relevance, Ethical Behavior, and Contextual Distraction) and training multiple general-domain and medical-domain BERT-based LLM models to predict these features directly from text. The most reliable feature regressor for each dimension was selected and used as the feature extractor in a second layer of classifiers. We evaluate a suite of predictive models, including tree-based, linear, probabilistic, and ensemble methods, to determine jailbreak likelihood from the extracted features. Across cross-validation and held-out evaluations, the system achieves strong overall performance, indicating that LLM-derived linguistic features provide an effective basis for automated jailbreak detection. Error analysis further highlights key limitations in current annotations and feature representations, pointing toward future improvements such as richer annotation schemes, finer-grained feature extraction, and methods that capture the evolving risk of jailbreak behavior over the course of a dialogue. This work demonstrates a scalable and interpretable approach for detecting jailbreak behavior in safety-critical clinical dialogue systems.

</details>


### [357] [Contrastive explanations of BDI agents](https://arxiv.org/abs/2602.13323)
*Michael Winikoff*

Main category: cs.AI

TL;DR: 扩展BDI智能体的解释能力以回答对比性问题（"为什么做X而不是F？"），研究发现对比性解释更短且在某些方面更受偏好，但令人意外的是提供完整解释有时比不解释更差。


<details>
  <summary>Details</summary>
Motivation: 自主系统的解释能力对透明度和信任建立很重要。现有BDI智能体只能回答"为什么做X？"这类问题，但人们实际会问对比性问题（"为什么做X而不是F？"），需要扩展解释能力来回答这类更自然的问题。

Method: 扩展先前BDI智能体的解释机制，使其能够回答对比性问题。通过计算评估对比解释长度，并进行人类主体评估，测试对比性解释的偏好程度及其对信任发展、透明度的支持效果。

Result: 计算评估显示对比性问题能显著减少解释长度。人类评估发现对比性解释在某些情况下更受偏好，且能带来更高的信任、感知理解和系统正确性信心。令人意外的是，提供完整解释有时比不提供任何解释更差。

Conclusion: 扩展BDI智能体回答对比性问题的能力是可行的，对比性解释更简洁且在某些方面更有效。但需要谨慎设计解释，因为过度详细的解释可能适得其反，在某些情况下不提供解释反而更好。

Abstract: The ability of autonomous systems to provide explanations is important for supporting transparency and aiding the development of (appropriate) trust. Prior work has defined a mechanism for Belief-Desire-Intention (BDI) agents to be able to answer questions of the form ``why did you do action $X$?''. However, we know that we ask \emph{contrastive} questions (``why did you do $X$ \emph{instead of} $F$?''). We therefore extend previous work to be able to answer such questions. A computational evaluation shows that using contrastive questions yields a significant reduction in explanation length. A human subject evaluation was conducted to assess whether such contrastive answers are preferred, and how well they support trust development and transparency. We found some evidence for contrastive answers being preferred, and some evidence that they led to higher trust, perceived understanding, and confidence in the system's correctness. We also evaluated the benefit of providing explanations at all. Surprisingly, there was not a clear benefit, and in some situations we found evidence that providing a (full) explanation was worse than not providing any explanation.

</details>


### [358] [Nanbeige4.1-3B: A Small General Model that Reasons, Aligns, and Acts](https://arxiv.org/abs/2602.13367)
*Chen Yang,Guangyue Peng,Jiaying Zhu,Ran Le,Ruixiang Feng,Tao Zhang,Xiyun Xu,Yang Song,Yiming Jia,Yuntao Wen,Yunzhi Xu,Zekai Wang,Zhenwei An,Zhicong Sun,Zongchao Chen*

Main category: cs.AI

TL;DR: Nanbeige4.1-3B是一个仅30亿参数的多功能语言模型，在智能体行为、代码生成和通用推理方面表现优异，首次实现了开源小模型的多功能统一。


<details>
  <summary>Details</summary>
Motivation: 探索小参数模型（30亿参数）能否同时实现广泛能力和强专业化，重新定义小模型的潜力，解决现有小模型在多功能统一方面的不足。

Method: 1. 结合点对点和配对奖励建模提升推理和偏好对齐；2. 设计复杂度感知奖励强化学习优化代码正确性和效率；3. 深度搜索中进行复杂数据合成和回合级监督训练，支持稳定长序列工具调用。

Result: 模型在多项任务上显著超越同规模模型（如Nanbeige4-3B-2511和Qwen3-4B），甚至优于更大模型（如Qwen3-30B-A3B），能可靠执行多达600轮工具调用的复杂问题解决。

Conclusion: 小模型可以同时实现广泛能力和强专业化，重新定义了30亿参数模型的潜力，为高效多任务AI系统提供了新方向。

Abstract: We present Nanbeige4.1-3B, a unified generalist language model that simultaneously achieves strong agentic behavior, code generation, and general reasoning with only 3B parameters. To the best of our knowledge, it is the first open-source small language model (SLM) to achieve such versatility in a single model. To improve reasoning and preference alignment, we combine point-wise and pair-wise reward modeling, ensuring high-quality, human-aligned responses. For code generation, we design complexity-aware rewards in Reinforcement Learning, optimizing both correctness and efficiency. In deep search, we perform complex data synthesis and incorporate turn-level supervision during training. This enables stable long-horizon tool interactions, allowing Nanbeige4.1-3B to reliably execute up to 600 tool-call turns for complex problem-solving. Extensive experimental results show that Nanbeige4.1-3B significantly outperforms prior models of similar scale, such as Nanbeige4-3B-2511 and Qwen3-4B, even achieving superior performance compared to much larger models, such as Qwen3-30B-A3B. Our results demonstrate that small models can achieve both broad competence and strong specialization simultaneously, redefining the potential of 3B parameter models.

</details>


### [359] [MoralityGym: A Benchmark for Evaluating Hierarchical Moral Alignment in Sequential Decision-Making Agents](https://arxiv.org/abs/2602.13372)
*Simon Rosen,Siddarth Singh,Ebenezer Gelo,Helen Sarah Robertson,Ibrahim Suder,Victoria Williams,Benjamin Rosman,Geraud Nangue Tasse,Steven James*

Main category: cs.AI

TL;DR: 提出Morality Chains形式化表示道德规范，创建MoralityGym基准测试，评估AI在冲突道德规范下的决策能力


<details>
  <summary>Details</summary>
Motivation: 评估AI在冲突、层次化人类道德规范下的对齐性是AI安全、道德哲学和认知科学交叉领域的关键挑战

Method: 引入Morality Chains形式化表示道德规范作为有序义务约束，创建MoralityGym基准测试（98个伦理困境问题），采用电车难题风格的Gymnasium环境，将任务解决与道德评估解耦，引入新的道德度量标准

Result: 使用安全强化学习方法进行基线测试，揭示了关键局限性，表明需要更原则性的伦理决策方法

Conclusion: 为开发在复杂现实世界中行为更可靠、透明和道德的AI系统奠定了基础

Abstract: Evaluating moral alignment in agents navigating conflicting, hierarchically structured human norms is a critical challenge at the intersection of AI safety, moral philosophy, and cognitive science. We introduce Morality Chains, a novel formalism for representing moral norms as ordered deontic constraints, and MoralityGym, a benchmark of 98 ethical-dilemma problems presented as trolley-dilemma-style Gymnasium environments. By decoupling task-solving from moral evaluation and introducing a novel Morality Metric, MoralityGym allows the integration of insights from psychology and philosophy into the evaluation of norm-sensitive reasoning. Baseline results with Safe RL methods reveal key limitations, underscoring the need for more principled approaches to ethical decision-making. This work provides a foundation for developing AI systems that behave more reliably, transparently, and ethically in complex real-world contexts.

</details>


### [360] [On-Policy Supervised Fine-Tuning for Efficient Reasoning](https://arxiv.org/abs/2602.13407)
*Anhao Zhao,Ziyang Chen,Junlong Tong,Yingqi Fan,Fanghua Ye,Shuhao Li,Yunpu Ma,Wenjie Li,Xiaoyu Shen*

Main category: cs.AI

TL;DR: 论文提出On-policy SFT方法，通过简化奖励设计（仅使用截断式长度惩罚）和移除KL正则化与分组归一化，将复杂推理模型的训练简化为基于自生成数据的监督微调，在保持准确率的同时大幅提升效率和推理简洁性。


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型通常使用强化学习训练，但复杂的多奖励目标（同时优化正确性和简洁性）会导致训练不稳定和次优权衡。作者质疑这种复杂性的必要性，希望通过简化训练策略来解决这些问题。

Method: 提出On-policy SFT方法：1）移除KL正则化（在正确性和长度可直接验证时失去作用）；2）移除分组归一化（在多奖励信号下变得模糊）；3）简化奖励为基于截断的长度惩罚；4）将优化问题简化为在自生成数据上的监督微调，这些数据经过正确性和简洁性双重筛选。

Result: 在五个基准测试中，该方法将推理链长度减少高达80%，同时保持原始准确率，超越了更复杂的基于RL的方法。训练效率显著提升：GPU内存使用减少50%，收敛速度加快70%。

Conclusion: 通过简化奖励设计和训练策略，On-policy SFT方法在准确率-效率的帕累托前沿上表现一致，证明了复杂强化学习训练的必要性值得重新审视，简单方法也能取得优异效果。

Abstract: Large reasoning models (LRMs) are commonly trained with reinforcement learning (RL) to explore long chain-of-thought reasoning, achieving strong performance at high computational cost. Recent methods add multi-reward objectives to jointly optimize correctness and brevity, but these complex extensions often destabilize training and yield suboptimal trade-offs. We revisit this objective and challenge the necessity of such complexity. Through principled analysis, we identify fundamental misalignments in this paradigm: KL regularization loses its intended role when correctness and length are directly verifiable, and group-wise normalization becomes ambiguous under multiple reward signals. By removing these two items and simplifying the reward to a truncation-based length penalty, we show that the optimization problem reduces to supervised fine-tuning on self-generated data filtered for both correctness and conciseness. We term this simplified training strategy on-policy SFT. Despite its simplicity, on-policy SFT consistently defines the accuracy-efficiency Pareto frontier. It reduces CoT length by up to 80 while maintaining original accuracy, surpassing more complex RL-based methods across five benchmarks. Furthermore, it significantly enhances training efficiency, reducing GPU memory usage by 50% and accelerating convergence by 70%. Our code is available at https://github.com/EIT-NLP/On-Policy-SFT.

</details>


### [361] [NeuroWeaver: An Autonomous Evolutionary Agent for Exploring the Programmatic Space of EEG Analysis Pipelines](https://arxiv.org/abs/2602.13473)
*Guoan Wang,Shihao Yang,Jun-En Ding,Hao Zhu,Feng Liu*

Main category: cs.AI

TL;DR: NeuroWeaver是一个统一的自主进化代理，通过将EEG分析流程工程重新定义为离散约束优化问题，在神经科学合理的搜索空间内自动生成轻量级解决方案，在性能、新颖性和效率之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型在EEG分析中面临数据需求大、参数多、计算成本高的问题，难以在资源受限的临床环境中部署；而通用自动机器学习框架缺乏神经生理学先验知识，常产生不科学的解决方案。

Method: 1) 领域知识引导的子空间初始化，将搜索限制在神经科学合理的流形上；2) 多目标进化优化，通过自我反思细化动态平衡性能、新颖性和效率；3) 将流程工程重新定义为离散约束优化问题。

Result: 在五个异构基准测试中，NeuroWeaver生成的轻量级解决方案持续优于最先进的任务特定方法，性能与大规模基础模型相当，但使用的参数数量显著减少。

Conclusion: NeuroWeaver通过结合领域知识和进化优化，为EEG分析提供了一种高效、可解释且资源友好的自动化解决方案，克服了现有方法的局限性。

Abstract: Although foundation models have demonstrated remarkable success in general domains, the application of these models to electroencephalography (EEG) analysis is constrained by substantial data requirements and high parameterization. These factors incur prohibitive computational costs, thereby impeding deployment in resource-constrained clinical environments. Conversely, general-purpose automated machine learning frameworks are often ill-suited for this domain, as exploration within an unbounded programmatic space fails to incorporate essential neurophysiological priors and frequently yields solutions that lack scientific plausibility. To address these limitations, we propose NeuroWeaver, a unified autonomous evolutionary agent designed to generalize across diverse EEG datasets and tasks by reformulating pipeline engineering as a discrete constrained optimization problem. Specifically, we employ a Domain-Informed Subspace Initialization to confine the search to neuroscientifically plausible manifolds, coupled with a Multi-Objective Evolutionary Optimization that dynamically balances performance, novelty, and efficiency via self-reflective refinement. Empirical evaluations across five heterogeneous benchmarks demonstrate that NeuroWeaver synthesizes lightweight solutions that consistently outperform state-of-the-art task-specific methods and achieve performance comparable to large-scale foundation models, despite utilizing significantly fewer parameters.

</details>


### [362] [OMNI-LEAK: Orchestrator Multi-Agent Network Induced Data Leakage](https://arxiv.org/abs/2602.13477)
*Akshat Naik,Jay Culligan,Yarin Gal,Philip Torr,Rahaf Aljundi,Alasdair Paren,Adel Bibi*

Main category: cs.AI

TL;DR: 研究发现多智能体系统中的安全漏洞，通过OMNI-LEAK攻击向量，即使存在数据访问控制，也能通过间接提示注入泄露敏感数据。


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能体能力增强，多智能体系统将成为实用范式。先前研究主要关注单智能体安全风险，缺乏对多智能体系统的威胁建模，特别是在具有基本工程防护措施（如访问控制）的设置中。

Method: 研究流行的多智能体模式——编排器设置，通过红队测试具体用例，演示OMNI-LEAK攻击向量，评估前沿模型对不同类别攻击的易感性。

Result: 发现推理和非推理模型都容易受到攻击，即使攻击者不了解实现细节。OMNI-LEAK攻击能够通过单次间接提示注入泄露敏感数据，即使存在数据访问控制。

Conclusion: 安全研究需要从单智能体扩展到多智能体设置，以减少现实世界隐私泄露和财务损失的风险，维护公众对AI智能体的信任。

Abstract: As Large Language Model (LLM) agents become more capable, their coordinated use in the form of multi-agent systems is anticipated to emerge as a practical paradigm. Prior work has examined the safety and misuse risks associated with agents. However, much of this has focused on the single-agent case and/or setups missing basic engineering safeguards such as access control, revealing a scarcity of threat modeling in multi-agent systems. We investigate the security vulnerabilities of a popular multi-agent pattern known as the orchestrator setup, in which a central agent decomposes and delegates tasks to specialized agents. Through red-teaming a concrete setup representative of a likely future use case, we demonstrate a novel attack vector, OMNI-LEAK, that compromises several agents to leak sensitive data through a single indirect prompt injection, even in the \textit{presence of data access control}. We report the susceptibility of frontier models to different categories of attacks, finding that both reasoning and non-reasoning models are vulnerable, even when the attacker lacks insider knowledge of the implementation details. Our work highlights the importance of safety research to generalize from single-agent to multi-agent settings, in order to reduce the serious risks of real-world privacy breaches and financial losses and overall public trust in AI agents.

</details>


### [363] [Translating Dietary Standards into Healthy Meals with Minimal Substitutions](https://arxiv.org/abs/2602.13502)
*Trevor Chan,Ilias Tagkopoulos*

Main category: cs.AI

TL;DR: 开发了一个端到端框架，通过识别34个可解释的餐食原型，使用生成模型和份量预测器来创建符合USDA营养目标的餐食，只需1-3种食物替换就能使餐食营养提升10%，成本降低19-32%。


<details>
  <summary>Details</summary>
Motivation: 个性化饮食系统的重要目标是在不牺牲便利性或可负担性的情况下改善营养质量。需要将饮食标准转化为完整的餐食，同时保持与真实餐食的相似性。

Method: 使用WWEIA的135,491餐数据识别34个可解释的餐食原型，然后使用这些原型来条件化生成模型和份量预测器，以满足USDA营养目标。通过允许1-3种食物替换来优化餐食。

Result: 生成的餐食在遵循推荐每日摄入量目标方面提高了47.0%，同时保持与真实餐食的组成接近。通过1-3种食物替换，餐食营养提升10%，成本降低19-32%。

Conclusion: 该框架通过将饮食指南转化为现实、预算意识的餐食和简单替换，能够支持临床决策、公共卫生项目和消费者应用，实现可扩展、公平的日常营养改善。

Abstract: An important goal for personalized diet systems is to improve nutritional quality without compromising convenience or affordability. We present an end-to-end framework that converts dietary standards into complete meals with minimal change. Using the What We Eat in America (WWEIA) intake data for 135,491 meals, we identify 34 interpretable meal archetypes that we then use to condition a generative model and a portion predictor to meet USDA nutritional targets. In comparisons within archetypes, generated meals are better at following recommended daily intake (RDI) targets by 47.0%, while remaining compositionally close to real meals. Our results show that by allowing one to three food substitutions, we were able to create meals that were 10% more nutritious, while reducing costs 19-32%, on average. By turning dietary guidelines into realistic, budget-aware meals and simple swaps, this framework can underpin clinical decision support, public-health programs, and consumer apps that deliver scalable, equitable improvements in everyday nutrition.

</details>


### [364] [SPILLage: Agentic Oversharing on the Web](https://arxiv.org/abs/2602.13516)
*Jaechul Roh,Eugene Bagdasarian,Hamed Haddadi,Ali Shahin Shamsabadi*

Main category: cs.AI

TL;DR: 论文提出"自然代理过度分享"概念，指网络代理在完成任务时无意泄露任务无关的用户信息，并开发SPILLage框架从内容和行为两个维度分析此问题。


<details>
  <summary>Details</summary>
Motivation: 随着LLM驱动的代理开始在开放网络上自动化用户任务，这些代理会与第三方交互并留下行动痕迹。需要研究代理如何处理用户资源，特别是在完成任务时是否会无意泄露任务无关的信息。

Method: 提出SPILLage框架，从渠道（内容vs行为）和直接性（显式vs隐式）两个维度表征过度分享。在实时电商网站上对180个任务进行基准测试，使用两种代理框架和三种骨干LLM进行1,080次运行。

Result: 过度分享普遍存在，行为过度分享比内容过度分享多5倍。即使采用提示级缓解措施，问题仍然存在甚至恶化。但执行前移除任务无关信息可将任务成功率提高17.9%。

Conclusion: 保护网络代理隐私是一个基本挑战，需要更广泛的"输出"视角，不仅要考虑代理输入的内容，还要考虑其在网络上的行为。减少过度分享能提高任务成功率。

Abstract: LLM-powered agents are beginning to automate user's tasks across the open web, often with access to user resources such as emails and calendars. Unlike standard LLMs answering questions in a controlled ChatBot setting, web agents act "in the wild", interacting with third parties and leaving behind an action trace. Therefore, we ask the question: how do web agents handle user resources when accomplishing tasks on their behalf across live websites? In this paper, we formalize Natural Agentic Oversharing -- the unintentional disclosure of task-irrelevant user information through an agent trace of actions on the web. We introduce SPILLage, a framework that characterizes oversharing along two dimensions: channel (content vs. behavior) and directness (explicit vs. implicit). This taxonomy reveals a critical blind spot: while prior work focuses on text leakage, web agents also overshare behaviorally through clicks, scrolls, and navigation patterns that can be monitored. We benchmark 180 tasks on live e-commerce sites with ground-truth annotations separating task-relevant from task-irrelevant attributes. Across 1,080 runs spanning two agentic frameworks and three backbone LLMs, we demonstrate that oversharing is pervasive with behavioral oversharing dominates content oversharing by 5x. This effect persists -- and can even worsen -- under prompt-level mitigation. However, removing task-irrelevant information before execution improves task success by up to 17.9%, demonstrating that reducing oversharing improves task success. Our findings underscore that protecting privacy in web agents is a fundamental challenge, requiring a broader view of "output" that accounts for what agents do on the web, not just what they type. Our datasets and code are available at https://github.com/jrohsc/SPILLage.

</details>


### [365] [REMem: Reasoning with Episodic Memory in Language Agent](https://arxiv.org/abs/2602.13530)
*Yiheng Shu,Saisri Padmaja Jonnalagedda,Xiang Gao,Bernal Jiménez Gutiérrez,Weijian Qi,Kamalika Das,Huan Sun,Yu Su*

Main category: cs.AI

TL;DR: REMem是一个两阶段框架，通过构建混合记忆图并进行智能检索推理，显著提升了语言智能体在情景记忆方面的能力，在多个基准测试中优于现有记忆系统。


<details>
  <summary>Details</summary>
Motivation: 人类擅长在时空背景下记忆具体经历并进行跨事件推理（情景记忆能力），而当前语言智能体的记忆主要是语义性的，无法有效回忆和推理交互历史。现有工作往往忽视情景性、缺乏明确的事件建模，或过度强调简单检索而非复杂推理。

Method: REMem采用两阶段框架：1) 离线索引阶段：将经验转换为混合记忆图，灵活链接时间感知的要点和事实；2) 在线推理阶段：使用配备精心设计工具的智能检索器，在记忆图上进行迭代检索。

Result: 在四个情景记忆基准测试中，REMem显著优于Mem0和HippoRAG 2等最先进的记忆系统，在情景回忆和推理任务上分别实现了3.4%和13.4%的绝对提升。此外，REMem对无法回答的问题表现出更稳健的拒绝行为。

Conclusion: REMem通过构建混合记忆图和智能检索推理机制，有效解决了语言智能体在情景记忆方面的核心挑战，为构建具有人类般记忆能力的智能体提供了重要进展。

Abstract: Humans excel at remembering concrete experiences along spatiotemporal contexts and performing reasoning across those events, i.e., the capacity for episodic memory. In contrast, memory in language agents remains mainly semantic, and current agents are not yet capable of effectively recollecting and reasoning over interaction histories. We identify and formalize the core challenges of episodic recollection and reasoning from this gap, and observe that existing work often overlooks episodicity, lacks explicit event modeling, or overemphasizes simple retrieval rather than complex reasoning. We present REMem, a two-phase framework for constructing and reasoning with episodic memory: 1) Offline indexing, where REMem converts experiences into a hybrid memory graph that flexibly links time-aware gists and facts. 2) Online inference, where REMem employs an agentic retriever with carefully curated tools for iterative retrieval over the memory graph. Comprehensive evaluation across four episodic memory benchmarks shows that REMem substantially outperforms state-of-the-art memory systems such as Mem0 and HippoRAG 2, showing 3.4% and 13.4% absolute improvements on episodic recollection and reasoning tasks, respectively. Moreover, REMem also demonstrates more robust refusal behavior for unanswerable questions.

</details>


### [366] [OpAgent: Operator Agent for Web Navigation](https://arxiv.org/abs/2602.13559)
*Yuyu Guo,Wenjie Yang,Siyuan Yang,Ziyang Liu,Cheng Chen,Yuan Wei,Yun Hu,Yang Huang,Guoliang Hao,Dongsheng Yuan,Jianming Wang,Xin Chen,Hang Yu,Lei Lei,Peng Di*

Main category: cs.AI

TL;DR: 提出在线强化学习WebAgent，通过分层多任务微调、在线交互RL和模块化OpAgent框架，在WebArena上达到71.6%的SOTA成功率。


<details>
  <summary>Details</summary>
Motivation: 传统基于监督微调或离线强化学习的方法面临严重的分布偏移问题，因为离线轨迹无法捕捉真实网站环境的随机状态转换和实时反馈，需要更鲁棒的在线学习方法。

Method: 1) 分层多任务微调：按功能原语（规划、行动、基础）分类数据集，建立具有强指令跟随能力的视觉语言模型；2) 在线强化学习：开发在线交互环境，使用混合奖励机制（WebJudge和规则决策树）解决长期导航的信用分配问题；3) OpAgent框架：协调规划器、基础器、反射器和总结器，实现错误恢复和自我纠正。

Result: RL增强模型在WebArena上达到38.1%的成功率（pass@5），超越所有现有单体基线；OpAgent框架将性能提升到71.6%的SOTA成功率。

Conclusion: 提出的在线强化学习WebAgent通过直接迭代交互、分层微调和模块化架构，有效解决了真实网站环境的复杂性和动态性，显著提升了自主网络代理的性能。

Abstract: To fulfill user instructions, autonomous web agents must contend with the inherent complexity and volatile nature of real-world websites. Conventional paradigms predominantly rely on Supervised Fine-Tuning (SFT) or Offline Reinforcement Learning (RL) using static datasets. However, these methods suffer from severe distributional shifts, as offline trajectories fail to capture the stochastic state transitions and real-time feedback of unconstrained wide web environments. In this paper, we propose a robust Online Reinforcement Learning WebAgent, designed to optimize its policy through direct, iterative interactions with unconstrained wide websites. Our approach comprises three core innovations: 1) Hierarchical Multi-Task Fine-tuning: We curate a comprehensive mixture of datasets categorized by functional primitives -- Planning, Acting, and Grounding -- establishing a Vision-Language Model (VLM) with strong instruction-following capabilities for Web GUI tasks. 2) Online Agentic RL in the Wild: We develop an online interaction environment and fine-tune the VLM using a specialized RL pipeline. We introduce a Hybrid Reward Mechanism that combines a ground-truth-agnostic WebJudge for holistic outcome assessment with a Rule-based Decision Tree (RDT) for progress reward. This system effectively mitigates the credit assignment challenge in long-horizon navigation. Notably, our RL-enhanced model achieves a 38.1\% success rate (pass@5) on WebArena, outperforming all existing monolithic baselines. 3) Operator Agent: We introduce a modular agentic framework, namely \textbf{OpAgent}, orchestrating a Planner, Grounder, Reflector, and Summarizer. This synergy enables robust error recovery and self-correction, elevating the agent's performance to a new State-of-the-Art (SOTA) success rate of \textbf{71.6\%}.

</details>


### [367] [Who Do LLMs Trust? Human Experts Matter More Than Other LLMs](https://arxiv.org/abs/2602.13568)
*Anooshka Bajaj,Zoran Tiganj*

Main category: cs.AI

TL;DR: LLMs表现出类似人类的社交影响模式，更倾向于遵从标记为"人类专家"的反馈，即使这些反馈是错误的，这种专家框架效应在不同任务领域都普遍存在。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在遇到社交信息（如其他智能体答案、工具输出或人类建议）时是否表现出类似人类的判断影响模式，以及它们是否更重视人类反馈而非其他LLM反馈。

Method: 通过三个二元决策任务（阅读理解、多步推理、道德判断），向四个指令调优的LLMs呈现标记为来自朋友、人类专家或其他LLMs的先前回答，操纵群体正确性并变化群体规模；第二个实验引入单个人类与单个LLM之间的直接分歧。

Result: LLMs显著更倾向于遵从标记为来自人类专家的回答，即使该信号是错误的；相比其他LLMs，它们更愿意向专家反馈修正自己的答案；专家框架作为当代LLMs的强大先验。

Conclusion: LLMs表现出一种对可信度敏感的社交影响形式，这种影响在不同决策领域具有普遍性，专家框架作为强先验影响LLMs的判断。

Abstract: Large language models (LLMs) increasingly operate in environments where they encounter social information such as other agents' answers, tool outputs, or human recommendations. In humans, such inputs influence judgments in ways that depend on the source's credibility and the strength of consensus. This paper investigates whether LLMs exhibit analogous patterns of influence and whether they privilege feedback from humans over feedback from other LLMs. Across three binary decision-making tasks, reading comprehension, multi-step reasoning, and moral judgment, we present four instruction-tuned LLMs with prior responses attributed either to friends, to human experts, or to other LLMs. We manipulate whether the group is correct and vary the group size. In a second experiment, we introduce direct disagreement between a single human and a single LLM. Across tasks, models conform significantly more to responses labeled as coming from human experts, including when that signal is incorrect, and revise their answers toward experts more readily than toward other LLMs. These results reveal that expert framing acts as a strong prior for contemporary LLMs, suggesting a form of credibility-sensitive social influence that generalizes across decision domains.

</details>


### [368] [Differentiable Rule Induction from Raw Sequence Inputs](https://arxiv.org/abs/2602.13583)
*Kun Gao,Katsumi Inoue,Yongzhi Cao,Hanpin Wang,Feng Yang*

Main category: cs.AI

TL;DR: 提出一种结合自监督可微分聚类与新型可微分ILP的方法，解决从原始数据学习规则时的显式标签泄漏问题


<details>
  <summary>Details</summary>
Motivation: 现有可微分归纳逻辑编程方法主要依赖符号数据集，难以直接从原始数据学习规则，存在显式标签泄漏问题（无法将连续输入映射到符号变量而不需要输入特征标签的显式监督）

Method: 集成自监督可微分聚类模型与新型可微分ILP模型，使系统能够直接从原始数据学习规则而不需要显式标签泄漏

Result: 该方法能够直观且精确地从时间序列和图像数据中学习泛化规则，有效通过特征描述原始数据

Conclusion: 提出的集成方法解决了可微分ILP从原始数据学习规则的关键限制，实现了无需显式标签泄漏的规则学习，提高了模型在原始数据上的适用性

Abstract: Rule learning-based models are widely used in highly interpretable scenarios due to their transparent structures. Inductive logic programming (ILP), a form of machine learning, induces rules from facts while maintaining interpretability. Differentiable ILP models enhance this process by leveraging neural networks to improve robustness and scalability. However, most differentiable ILP methods rely on symbolic datasets, facing challenges when learning directly from raw data. Specifically, they struggle with explicit label leakage: The inability to map continuous inputs to symbolic variables without explicit supervision of input feature labels. In this work, we address this issue by integrating a self-supervised differentiable clustering model with a novel differentiable ILP model, enabling rule learning from raw data without explicit label leakage. The learned rules effectively describe raw data through its features. We demonstrate that our method intuitively and precisely learns generalized rules from time series and image data.

</details>


### [369] [A First Proof Sprint](https://arxiv.org/abs/2602.13587)
*Joseph Corneli*

Main category: cs.AI

TL;DR: 多智能体证明冲刺工作流，结合快速草稿生成与对抗验证，针对10个研究级问题产生异质但明确的数学与验证状态结果


<details>
  <summary>Details</summary>
Motivation: 提高压缩证明冲刺中的可靠性和校准，通过结构感知验证和层切换策略来处理研究级数学问题

Method: 使用连线图分解来定位证明依赖关系中的缺口，结合对抗验证、定向修复和明确溯源的多智能体工作流

Result: 问题3在限定标准下验证完整，问题5在局部连通谱中有限解决，问题10有条件解决，问题4和6部分解决，问题7通过旋转路径定理链暂时关闭

Conclusion: 结构感知验证和层切换策略显著提升了压缩证明冲刺中的可靠性和校准能力

Abstract: This monograph reports a multi-agent proof sprint on ten research-level problems, combining rapid draft generation with adversarial verification, targeted repair, and explicit provenance. The workflow uses wiring-diagram decompositions of claim dependencies to localize gaps and coordinate reviewer-driven revisions. Final outcomes are heterogeneous but explicit: the manuscript distinguishes mathematical status from QC-validation status. Mathematically, Problem~3 has a validation-complete existence path under the scoped criterion used here (uniqueness/irreducibility treated as optional), Problem 5 is solved in a scope-limited form for $F_O$-local connective spectra, Problem 10 is conditional under clearly stated assumptions (with explicit necessity counterexamples when assumptions are dropped), and Problems 4 and 6 are partial with named remaining obligations in the general case (including an unconditional $K_n$ result for Problem 6 with $c_0 = 1/3$). Problem 7 is treated as provisionally closed via the rotation-route theorem chain, pending independent ledger re-check. At the QC layer, Problems~7 and~9 have node-level validation artifacts but still contain unresolved verifier gaps. The main methodological result is that structure-aware verification and layer-switching strategies improve reliability and calibration in compressed proof sprints.

</details>


### [370] [Hippocampus: An Efficient and Scalable Memory Module for Agentic AI](https://arxiv.org/abs/2602.13594)
*Yi Li,Lianjie Cao,Faraz Ahmed,Puneet Sharma,Bingzhe Li*

Main category: cs.AI

TL;DR: Hippocampus是一个用于智能体AI的持久内存管理系统，使用二进制签名进行语义搜索，通过动态小波矩阵压缩索引，相比现有方法大幅降低检索延迟和存储开销。


<details>
  <summary>Details</summary>
Motivation: 智能体AI需要超越LLM有限上下文窗口的持久内存来存储用户特定历史。现有内存系统使用密集向量数据库或知识图谱遍历（或混合方法），存在检索延迟高和存储可扩展性差的问题。

Method: 引入Hippocampus系统，使用紧凑的二进制签名进行语义搜索，无损令牌ID流进行精确内容重建。核心是动态小波矩阵（DWM），压缩并共同索引两个流，支持在压缩域中进行超快速搜索，避免昂贵的密集向量或图计算。

Result: 评估显示，Hippocampus将端到端检索延迟降低高达31倍，每查询令牌占用减少高达14倍，同时在LoCoMo和LongMemEval基准测试中保持准确性。

Conclusion: Hippocampus设计随内存大小线性扩展，适用于长期智能体部署，解决了现有内存系统的高延迟和可扩展性限制问题。

Abstract: Agentic AI require persistent memory to store user-specific histories beyond the limited context window of LLMs. Existing memory systems use dense vector databases or knowledge-graph traversal (or hybrid), incurring high retrieval latency and poor storage scalability. We introduce Hippocampus, an agentic memory management system that uses compact binary signatures for semantic search and lossless token-ID streams for exact content reconstruction. Its core is a Dynamic Wavelet Matrix (DWM) that compresses and co-indexes both streams to support ultra-fast search in the compressed domain, thus avoiding costly dense-vector or graph computations. This design scales linearly with memory size, making it suitable for long-horizon agentic deployments. Empirically, our evaluation shows that Hippocampus reduces end-to-end retrieval latency by up to 31$\times$ and cuts per-query token footprint by up to 14$\times$, while maintaining accuracy on both LoCoMo and LongMemEval benchmarks.

</details>


### [371] [The Quantization Trap: Breaking Linear Scaling Laws in Multi-Hop Reasoning](https://arxiv.org/abs/2602.13595)
*Henry Han,Xiyang Liu,Xiaodong Wang,Fei Han,Xiaodong Li*

Main category: cs.AI

TL;DR: 量化缩放定律在多跳推理任务中失效：从16位降低到8/4位精度反而增加能耗并降低推理准确率


<details>
  <summary>Details</summary>
Motivation: 传统神经缩放定律认为降低数值精度可以线性提升计算效率和能耗表现（E∝bits），但本文发现这一规律在多跳推理任务中不成立，存在"量化陷阱"

Method: 通过理论分解分析量化失败原因，包括硬件转换开销、反量化内核的隐藏延迟成本（在顺序推理链中成为主要瓶颈）以及顺序能量摊销失败

Result: 从16位降低到8/4位精度反而增加净能耗并降低推理准确率，缩放定律在实际应用中不可避免地被打破

Conclusion: 行业"越小越好"的启发式方法在复杂推理任务中数学上是适得其反的，量化缩放定律在多跳推理场景中不适用

Abstract: Neural scaling laws provide a predictable recipe for AI advancement: reducing numerical precision should linearly improve computational efficiency and energy profile (E proportional to bits). In this paper, we demonstrate that this scaling law breaks in the context of multi-hop reasoning. We reveal a 'quantization trap' where reducing precision from 16-bit to 8/4-bit paradoxically increases more net energy consumption while degrading reasoning accuracy. We provide a rigorous theoretical decomposition that attributes this failure to hardware casting overhead, the hidden latency cost of dequantization kernels, which becomes a dominant bottleneck in sequential reasoning chains, as well as to a sequential energy amortization failure. As a result, scaling law breaking is unavoidable in practice. Our findings suggest that the industry's "smaller-is-better" heuristic is mathematically counterproductive for complex reasoning tasks.

</details>


### [372] [DiffusionRollout: Uncertainty-Aware Rollout Planning in Long-Horizon PDE Solving](https://arxiv.org/abs/2602.13616)
*Seungwoo Yoo,Juil Koo,Daehyeon Choi,Minhyuk Sung*

Main category: cs.AI

TL;DR: 提出DiffusionRollout，一种用于自回归扩散模型的选择性展开规划策略，旨在减轻偏微分方程系统长时预测中的误差累积问题。


<details>
  <summary>Details</summary>
Motivation: 在物理系统的偏微分方程长时预测中，自回归扩散模型存在误差累积问题，需要一种机制来量化预测不确定性并减少对不准确先前输出的条件依赖。

Method: 基于扩散模型预测不确定性与误差的强相关性，提出自适应选择步长的选择性展开规划策略，通过不确定性度量来调整自回归展开过程。

Result: 在长轨迹PDE预测基准测试中，该方法有效降低了预测误差，生成了更长且与真实值保持高相关性的预测轨迹。

Conclusion: DiffusionRollout通过利用扩散模型的不确定性度量进行自适应规划，显著提高了偏微分方程长时预测的可靠性，验证了不确定性作为预测置信度代理的有效性。

Abstract: We propose DiffusionRollout, a novel selective rollout planning strategy for autoregressive diffusion models, aimed at mitigating error accumulation in long-horizon predictions of physical systems governed by partial differential equations (PDEs). Building on the recently validated probabilistic approach to PDE solving, we further explore its ability to quantify predictive uncertainty and demonstrate a strong correlation between prediction errors and standard deviations computed over multiple samples-supporting their use as a proxy for the model's predictive confidence. Based on this observation, we introduce a mechanism that adaptively selects step sizes during autoregressive rollouts, improving long-term prediction reliability by reducing the compounding effect of conditioning on inaccurate prior outputs. Extensive evaluation on long-trajectory PDE prediction benchmarks validates the effectiveness of the proposed uncertainty measure and adaptive planning strategy, as evidenced by lower prediction errors and longer predicted trajectories that retain a high correlation with their ground truths.

</details>


### [373] [Guided Collaboration in Heterogeneous LLM-Based Multi-Agent Systems via Entropy-Based Understanding Assessment and Experience Retrieval](https://arxiv.org/abs/2602.13639)
*Linlin Wang,Tianqing Zhu,Laiqiao Qin,Longxiang Gao,Wanlei Zhou*

Main category: cs.AI

TL;DR: 本文提出一种基于熵的自适应引导框架，解决异构多智能体系统中强-弱模型协作时的认知不匹配问题，通过动态调整引导强度提升协作效果。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在推理、规划和复杂任务生成方面取得突破，人工智能系统正从单智能体架构转向多智能体协作系统。然而，在异构多智能体系统中，智能体之间的能力差异导致认知问题，强-弱模型协作效果不佳，甚至可能弱于弱-弱组合，认知不匹配成为异构协作的关键瓶颈。

Method: 提出基于熵的自适应引导框架：1) 通过多维度熵度量（表达、不确定性、结构、连贯性、相关性）量化弱智能体的理解程度；2) 动态调整引导强度为轻度、中度和重度三个级别；3) 结合检索增强生成机制保留成功协作经验，实现即时适应和长期学习。

Result: 在GSM8K、MBPP和CVRP三个基准数据集上的广泛实验表明，该方法能持续提升异构协作的有效性和稳定性。自适应引导不仅缓解了认知不平衡，还为构建更鲁棒、协作的多智能体智能建立了可扩展路径。

Conclusion: 异构多智能体系统中的强-弱协作存在认知不匹配问题，基于熵的自适应引导框架通过动态调整引导强度，有效解决了这一问题，为构建更高效、稳定的多智能体协作系统提供了可行方案。

Abstract: With recent breakthroughs in large language models (LLMs) for reasoning, planning, and complex task generation, artificial intelligence systems are transitioning from isolated single-agent architectures to multi-agent systems with collaborative intelligence. However, in heterogeneous multi-agent systems (HMAS), capability differences among agents give rise to consistent cognitive problems, where strong and weak models fail to contribute effectively. We define the collaboration as a strong-weak system. Through comprehensive experiments, we disclose a counterintuitive phenomenon in the strong-weak system: a strong-weak collaboration may under-perform weak-weak combinations, revealing that cognitive mismatching are key bottlenecks limiting heterogeneous cooperation. To overcome these challenges, we propose an Entropy-Based Adaptive Guidance Framework that dynamically aligns the guidance with the cognitive state of each agent. The framework quantifies the understanding of weak agents through multi-dimensional entropy metrics - covering expression, uncertainty, structure, coherence, and relevance - and adaptively adjusts the intensity of the guidance at light, moderate and intensive levels. Furthermore, a Retrieval-Augmented Generation (RAG) mechanism is incorporated to retain successful collaboration experiences, enabling both immediate adaptation and long-term learning. Extensive experiments on three benchmark datasets, GSM8K, MBPP, and CVRP demonstrate that our approach consistently enhances the effectiveness and stability of heterogeneous collaboration. The results highlight that adaptive guidance not only mitigates cognitive imbalance but also establishes a scalable pathway toward more robust, cooperative multi-agent intelligence.

</details>


### [374] [Building Autonomous GUI Navigation via Agentic-Q Estimation and Step-Wise Policy Optimization](https://arxiv.org/abs/2602.13653)
*Yibo Wang,Guangda Huzhang,Yuwei Hu,Yu Xia,Shiyin Lu,Qing-Guo Chen,Zhao Xu,Weihua Luo,Kaifu Zhang,Lijun Zhang*

Main category: cs.AI

TL;DR: 提出基于MLLM的GUI智能体框架，包含agentic-Q估计和逐步策略优化，降低数据收集成本并实现稳定优化，在GUI导航和grounding基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现实应用中GUI智能体面临非平稳环境，导致数据整理和策略优化的计算成本高昂，需要更高效的框架。

Method: 提出两组件框架：1) agentic-Q估计 - 优化Q模型评估动作对任务完成的贡献；2) 逐步策略优化 - 从状态-动作轨迹采样，通过强化学习优化策略，数据由策略自身产生，优化与环境解耦。

Result: 框架赋予Ovis2.5-9B强大的GUI交互能力，在GUI导航和grounding基准上取得显著性能，甚至超越更大规模的竞争者。

Conclusion: 该MLLM为中心的GUI智能体框架能有效降低数据收集成本，实现稳定高效优化，提升GUI交互性能。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have substantially driven the progress of autonomous agents for Graphical User Interface (GUI). Nevertheless, in real-world applications, GUI agents are often faced with non-stationary environments, leading to high computational costs for data curation and policy optimization. In this report, we introduce a novel MLLM-centered framework for GUI agents, which consists of two components: agentic-Q estimation and step-wise policy optimization. The former one aims to optimize a Q-model that can generate step-wise values to evaluate the contribution of a given action to task completion. The latter one takes step-wise samples from the state-action trajectory as inputs, and optimizes the policy via reinforcement learning with our agentic-Q model. It should be noticed that (i) all state-action trajectories are produced by the policy itself, so that the data collection costs are manageable; (ii) the policy update is decoupled from the environment, ensuring stable and efficient optimization. Empirical evaluations show that our framework endows Ovis2.5-9B with powerful GUI interaction capabilities, achieving remarkable performances on GUI navigation and grounding benchmarks and even surpassing contenders with larger scales.

</details>


### [375] [HyFunc: Accelerating LLM-based Function Calls for Agentic AI through Hybrid-Model Cascade and Dynamic Templating](https://arxiv.org/abs/2602.13665)
*Weibin Liao,Jian-guang Lou,Haoyi Xiong*

Main category: cs.AI

TL;DR: HyFunc框架通过混合模型级联和动态模板技术，消除LLM函数调用中的三种冗余，显著降低推理延迟同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的智能体系统在将用户意图转换为结构化函数调用时存在计算冗余，导致高推理延迟，阻碍实时应用。主要冗余包括：冗余处理大量函数描述、冗余使用大模型生成可预测的完整token序列、冗余生成固定的参数语法模板。

Method: HyFunc采用混合模型级联：大模型将用户意图提炼为单个"软token"，指导轻量级检索器选择相关函数，并引导较小的前缀调优模型生成最终调用。同时使用"动态模板"技术在扩展的vLLM引擎中动态注入参数语法模板，避免冗余生成。

Result: 在未见过的BFCL基准数据集上，HyFunc达到0.828秒的推理延迟，优于所有基线模型；性能达到80.1%，超过所有参数规模相当的模型，在效率和性能间取得优秀平衡。

Conclusion: HyFunc为智能体AI提供了更高效的计算范式，通过消除三种关键冗余，显著降低延迟同时保持高性能，代码已开源。

Abstract: While agentic AI systems rely on LLMs to translate user intent into structured function calls, this process is fraught with computational redundancy, leading to high inference latency that hinders real-time applications. This paper identifies and addresses three key redundancies: (1) the redundant processing of a large library of function descriptions for every request; (2) the redundant use of a large, slow model to generate an entire, often predictable, token sequence; and (3) the redundant generation of fixed, boilerplate parameter syntax. We introduce HyFunc, a novel framework that systematically eliminates these inefficiencies. HyFunc employs a hybrid-model cascade where a large model distills user intent into a single "soft token." This token guides a lightweight retriever to select relevant functions and directs a smaller, prefix-tuned model to generate the final call, thus avoiding redundant context processing and full-sequence generation by the large model. To eliminate syntactic redundancy, our "dynamic templating" technique injects boilerplate parameter syntax on-the-fly within an extended vLLM engine. To avoid potential limitations in generalization, we evaluate HyFunc on an unseen benchmark dataset, BFCL. Experimental results demonstrate that HyFunc achieves an excellent balance between efficiency and performance. It achieves an inference latency of 0.828 seconds, outperforming all baseline models, and reaches a performance of 80.1%, surpassing all models with a comparable parameter scale. These results suggest that HyFunc offers a more efficient paradigm for agentic AI. Our code is publicly available at https://github.com/MrBlankness/HyFunc.

</details>


### [376] [AllMem: A Memory-centric Recipe for Efficient Long-context Modeling](https://arxiv.org/abs/2602.13680)
*Ziming Wang,Xiang Wang,Kailong Peng,Lang Qin,Juan Gabriel Kostelec,Christos Sourmpis,Axel Laborieux,Qinghai Guo*

Main category: cs.AI

TL;DR: AllMem：一种结合滑动窗口注意力与非线性的测试时训练内存网络的高效混合架构，用于解决LLM在长序列任务中的计算和内存瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在长序列任务中面临显著性能瓶颈，主要源于自注意力机制的计算复杂性和内存开销。现有线性内存模型存在表示约束，而全局注意力计算成本过高。

Method: 提出AllMem架构，集成滑动窗口注意力与非线性的测试时训练内存网络；采用内存高效微调策略，将预训练模型的标准注意力层替换为内存增强的滑动窗口层。

Result: 4k窗口模型在37k LongBench上实现接近无损性能（仅下降0.83）；8k窗口变体在128k InfiniteBench上超越完整注意力，验证了参数化内存的有效性。

Conclusion: AllMem能够有效扩展至超长上下文，缓解灾难性遗忘，同时显著降低长序列推理的计算和内存开销，为预训练LLM的高效长序列处理提供了可行方案。

Abstract: Large Language Models (LLMs) encounter significant performance bottlenecks in long-sequence tasks due to the computational complexity and memory overhead inherent in the self-attention mechanism. To address these challenges, we introduce \textsc{AllMem}, a novel and efficient hybrid architecture that integrates Sliding Window Attention (SWA) with non-linear Test-Time Training (TTT) memory networks. \textsc{AllMem} enables models to effectively scale to ultra-long contexts while mitigating catastrophic forgetting. This approach not only overcomes the representation constraints typical of linear memory models but also significantly reduces the computational and memory footprint during long-sequence inference. Furthermore, we implement a Memory-Efficient Fine-Tuning strategy to replace standard attention layers in pre-trained models with memory-augmented sliding window layers. This framework facilitates the efficient transformation of any off-the-shelf pre-trained LLM into an \textsc{AllMem}-based architecture. Empirical evaluations confirm that our 4k window model achieves near-lossless performance on 37k LongBench with a marginal 0.83 drop compared to full attention. Furthermore, on InfiniteBench at a 128k context, our 8k window variant outperforms full attention, which validates the effectiveness of our parameterized memory in mitigating noise and maintaining robust long-range modeling without the prohibitive costs of global attention.

</details>


### [377] [PhGPO: Pheromone-Guided Policy Optimization for Long-Horizon Tool Planning](https://arxiv.org/abs/2602.13691)
*Yu Li,Guangfeng Cai,Shengtian Yang,Han Luo,Shuo Han,Xu He,Dong Li,Lei Feng*

Main category: cs.AI

TL;DR: PhGPO提出一种基于信息素的策略优化方法，利用历史成功轨迹中的工具转移模式来指导LLM智能体的长时程工具规划，解决组合爆炸问题。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在执行复杂任务时面临长时程多步工具规划的挑战，探索空间存在组合爆炸问题。即使找到正确的工具使用路径，也仅被视为当前训练的即时奖励，无法为后续训练提供可重用的信息。

Method: 受蚁群优化启发，提出信息素引导策略优化(PhGPO)：1)从历史轨迹中学习基于轨迹的转移模式（信息素）；2)使用学习到的信息素指导策略优化，引导策略向历史成功的工具转移方向优化。

Result: 综合实验结果表明PhGPO方法的有效性，能够显著改善长时程工具规划性能。

Conclusion: 历史成功轨迹包含可重用的工具转移模式，通过学习这些模式并用于指导策略优化，可以显著提升LLM智能体在长时程工具规划任务中的性能。

Abstract: Recent advancements in Large Language Model (LLM) agents have demonstrated strong capabilities in executing complex tasks through tool use. However, long-horizon multi-step tool planning is challenging, because the exploration space suffers from a combinatorial explosion. In this scenario, even when a correct tool-use path is found, it is usually considered an immediate reward for current training, which would not provide any reusable information for subsequent training. In this paper, we argue that historically successful trajectories contain reusable tool-transition patterns, which can be leveraged throughout the whole training process. Inspired by ant colony optimization where historically successful paths can be reflected by the pheromone, we propose Pheromone-Guided Policy Optimization (PhGPO), which learns a trajectory-based transition pattern (i.e., pheromone) from historical trajectories and then uses the learned pheromone to guide policy optimization. This learned pheromone provides explicit and reusable guidance that steers policy optimization toward historically successful tool transitions, thereby improving long-horizon tool planning. Comprehensive experimental results demonstrate the effectiveness of our proposed PhGPO.

</details>


### [378] [Can a Lightweight Automated AI Pipeline Solve Research-Level Mathematical Problems?](https://arxiv.org/abs/2602.13695)
*Lve Meng,Weilong Zhao,Yanzhi Zhang,Haoxiang Guan,Jiyan He*

Main category: cs.AI

TL;DR: LLMs通过优化的自动化管道解决研究级数学问题，在ICCM和"First Proof"数据集上生成可验证的证明


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在数学竞赛基准上表现出色，但在研究问题上的轻量级自然语言管道部署仍未被充分探索，需要验证其在真实研究场景中的应用能力

Method: 使用新一代LLMs（如Gemini 3 Pro, GPT-5.2 Pro）构建优化的自动化管道，专注于基于引用的验证，在两个新数据集上测试：ICCM问题集和"First Proof"研究问题集

Result: 管道为所有ICCM前两套问题和"First Proof"问题集生成了候选证明，其中ICCM前两套和"First Proof"问题4的证明已完全验证，所有结果已提交官方组织并公开

Conclusion: 优化的LLM管道能够解决复杂的研究级数学问题，证明了AI在数学研究中的实际应用潜力，计划开源完整管道方法

Abstract: Large language models (LLMs) have recently achieved remarkable success in generating rigorous mathematical proofs, with "AI for Math" emerging as a vibrant field of research. While these models have mastered competition-level benchmarks like the International Mathematical Olympiad and show promise in research applications through auto-formalization, their deployment via lightweight, natural-language pipelines for research problems remains underexplored. In this work, we demonstrate that next-generation models (e.g., Gemini 3 Pro, GPT-5.2 Pro), when integrated into a streamlined automated pipeline optimized for citation-based verification, can solve sophisticated research-grade problems. We evaluate our pipeline on two novel datasets: (1) the ICCM problem sets (comparable to the S.-T. Yau College Student Mathematics Contest) proposed by leading mathematicians, and (2) the "First Proof" problem set, consisting of previously unpublished research questions. Our pipeline generated candidate proofs for all problems in the first two ICCM sets and the "First Proof" set. The solutions for the first two ICCM sets and Problem 4 of the "First Proof" set have been fully verified by our team. All generated proofs have been submitted to the official organization, and our generated results are publicly available. We plan to open-source the complete pipeline methodology in due course.

</details>


### [379] [No Need to Train Your RDB Foundation Model](https://arxiv.org/abs/2602.13697)
*Linjie Xu,Yanlin Zhang,Quan Gan,Minjie Wang,David Wipf*

Main category: cs.AI

TL;DR: 提出无需训练即可将多表关系数据库编码为固定长度样本的方法，与现有单表ICL基础模型无缝集成


<details>
  <summary>Details</summary>
Motivation: 关系数据库包含大量异构表格信息可用于预测建模，但企业环境中潜在预测目标众多，需要避免每次预测新目标时重新训练模型。现有基于上下文学习的基础模型主要局限于单表操作。

Method: 提出一种原则性的RDB编码器家族，将可变大小的RDB邻域压缩为固定长度的ICL样本。关键创新在于约束压缩在高维RDB列内进行（所有实体共享单位和角色），而不是跨列压缩（异构数据类型相关性无法确定）。编码器无需可训练参数，可与现有单表ICL基础模型无缝集成。

Result: 开发了可扩展的SQL原语实现编码阶段，创建了开源的RDB基础模型，能够在未见数据集上实现稳健性能，无需训练或微调。

Conclusion: 提出了一种无需训练的关系数据库编码方法，通过约束在列内压缩而非跨列压缩，实现了与现有单表ICL基础模型的无缝集成，为多表关系数据库的预测建模提供了实用解决方案。

Abstract: Relational databases (RDBs) contain vast amounts of heterogeneous tabular information that can be exploited for predictive modeling purposes. But since the space of potential targets is vast across enterprise settings, how can we \textit{avoid retraining} a new model each time we wish to predict a new quantity of interest? Foundation models based on in-context learning (ICL) offer a convenient option, but so far are largely restricted to single-table operability. In generalizing to multiple interrelated tables, it is essential to compress variably-sized RDB neighborhoods into fixed-length ICL samples for consumption by the decoder. However, the details here are critical: unlike existing supervised learning RDB pipelines, we provide theoretical and empirical evidence that ICL-specific compression should be constrained \emph{within} high-dimensional RDB columns where all entities share units and roles, not \textit{across} columns where the relevance of heterogeneous data types cannot possibly be determined without label information. Conditioned on this restriction, we then demonstrate that encoder expressiveness is actually not compromised by excluding trainable parameters. Hence we arrive at a principled family of RDB encoders that can be seamlessly paired with already-existing single-table ICL foundation models, whereby no training or fine-tuning is required. From a practical standpoint, we develop scalable SQL primitives to implement the encoder stage, resulting in an easy-to-use open-source RDB foundation model\footnote{\label{foot: RDBLearn_learn} https://github.com/HKUSHXLab/rdblearn} capable of robust performance on unseen datasets out of the box.

</details>


### [380] [OneLatent: Single-Token Compression for Visual Latent Reasoning](https://arxiv.org/abs/2602.13738)
*Bo Lv,Yasheng Sun,Junjie Wang,Haoxiang Shi*

Main category: cs.AI

TL;DR: OneLatent框架通过将推理步骤压缩为单个潜在token，显著减少推理成本，同时保持高准确率


<details>
  <summary>Details</summary>
Motivation: 解决Chain-of-thought推理成本过高的问题，传统CoT方法会增加1-2个数量级的推理成本

Method: 将文本推理步骤渲染为图像，通过DeepSeek-OCR隐藏状态监督，将中间推理压缩为单个潜在token

Result: 输出长度减少11倍，准确率仅下降2.21%，输出token贡献度提升6.8倍，在长链逻辑推理上达到99.80%准确率

Conclusion: OneLatent通过图像监督的潜在token压缩，在显著降低推理成本的同时保持了推理能力，支持压缩约束下的泛化

Abstract: Chain-of-thought (CoT) prompting improves reasoning but often increases inference cost by one to two orders of magnitude. To address these challenges, we present \textbf{OneLatent}, a framework that compresses intermediate reasoning into a single latent token via supervision from rendered CoT images and DeepSeek-OCR hidden states. By rendering textual steps into images, we obtain a deterministic supervision signal that can be inspected and audited without requiring the model to output verbose textual rationales. Across benchmarks, OneLatent reduces average output length by $11\times$ with only a $2.21\%$ average accuracy drop relative to textual CoT, while improving output token contribution (OTC) by $6.8\times$. On long-chain logical reasoning, OneLatent reaches $99.80\%$ on ProntoQA and $97.80\%$ on ProsQA with one latent token, with compression up to $87.4\times$, supporting compression-constrained generalization.

</details>


### [381] [OR-Agent: Bridging Evolutionary Search and Structured Research for Automated Algorithm Discovery](https://arxiv.org/abs/2602.13769)
*Qi Liu,Wanjing Ma*

Main category: cs.AI

TL;DR: OR-Agent是一个可配置的多智能体研究框架，通过树形工作流管理假设生成和系统回溯，结合进化-系统化构思机制和分层优化反思系统，在组合优化和协同驾驶场景中超越进化基线。


<details>
  <summary>Details</summary>
Motivation: 自动化科学发现在复杂实验驱动领域需要超越简单的程序迭代突变，需要结构化假设管理、环境交互和原则性反思。现有方法缺乏对研究轨迹的受控管理，无法有效处理分支假设生成和系统回溯。

Method: 提出OR-Agent框架：1) 树形工作流组织研究，显式建模分支假设生成和系统回溯；2) 进化-系统化构思机制统一进化选择研究起点、全面研究计划生成和协调探索；3) 分层优化反思系统：短期实验反思作为语言梯度提供即时纠正信号，长期反思积累跨实验见解作为语言动量，记忆压缩作为正则化机制类似权重衰减。

Result: 在经典组合优化基准（旅行商、车辆路径、装箱、定向、多重背包问题）和基于仿真的协同驾驶场景中进行广泛实验。结果表明OR-Agent优于强进化基线，同时提供了一个通用、可扩展、可检查的AI辅助科学发现框架。

Conclusion: OR-Agent通过结构化假设管理、进化-系统化构思和分层反思，为自动化科学发现提供了一个原则性架构，在复杂实验驱动领域表现出色，代码和数据已开源。

Abstract: Automating scientific discovery in complex, experiment-driven domains requires more than iterative mutation of programs; it demands structured hypothesis management, environment interaction, and principled reflection. We present OR-Agent, a configurable multi-agent research framework designed for automated exploration in rich experimental environments. OR-Agent organizes research as a structured tree-based workflow that explicitly models branching hypothesis generation and systematic backtracking, enabling controlled management of research trajectories beyond simple mutation-crossover loops. At its core, we introduce an evolutionary-systematic ideation mechanism that unifies evolutionary selection of research starting points, comprehensive research plan generation, and coordinated exploration within a research tree. We further propose a hierarchical optimization-inspired reflection system: short-term experimental reflection operates as a form of verbal gradient providing immediate corrective signals; long-term reflection accumulates cross-experiment insights as verbal momentum; and memory compression serves as a regularization mechanism analogous to weight decay, preserving essential signals while mitigating drift. Together, these components form a principled architecture governing research dynamics. We conduct extensive experiments across classical combinatorial optimization benchmarks-including traveling salesman, capacitated vehicle routing, bin packing, orienteering, and multiple knapsack problems-as well as simulation-based cooperative driving scenarios. Results demonstrate that OR-Agent outperforms strong evolutionary baselines while providing a general, extensible, and inspectable framework for AI-assisted scientific discovery. OR-Agent source code and experiments data are publicly available at https://github.com/qiliuchn/OR-Agent.

</details>


### [382] [StackingNet: Collective Inference Across Independent AI Foundation Models](https://arxiv.org/abs/2602.13792)
*Siyang Li,Chenhao Liu,Dongrui Wu,Zhigang Zeng,Lieyun Ding*

Main category: cs.AI

TL;DR: StackingNet是一个元集成框架，通过集体智能原理协调多个黑盒异构基础模型，在不访问内部参数或训练数据的情况下提升准确性、减少偏见并实现可靠性排名。


<details>
  <summary>Details</summary>
Motivation: 当前大型基础模型虽然在各领域表现出色，但彼此孤立，无法有效共享能力。整合这些独立基础模型的互补优势对于构建可信赖的智能系统至关重要，但目前缺乏协调这些黑盒异构模型的成熟方法。

Method: StackingNet采用元集成框架，基于集体智能原理，在推理过程中结合多个模型的预测结果。该方法不需要访问模型的内部参数或训练数据，能够识别或剪枝降低性能的模型。

Result: 在语言理解、视觉估计和学术论文评分等任务中，StackingNet相比单个模型和传统集成方法，持续提升了准确性、鲁棒性和公平性，同时减少了偏见。

Conclusion: StackingNet通过将多样性从不一致来源转变为协作，为协调人工智能建立了实用基础，表明进步不仅来自更大的单一模型，也来自多个专业化模型的原则性合作。

Abstract: Artificial intelligence built on large foundation models has transformed language understanding, vision and reasoning, yet these systems remain isolated and cannot readily share their capabilities. Integrating the complementary strengths of such independent foundation models is essential for building trustworthy intelligent systems. Despite rapid progress in individual model design, there is no established approach for coordinating such black-box heterogeneous models. Here we show that coordination can be achieved through a meta-ensemble framework termed StackingNet, which draws on principles of collective intelligence to combine model predictions during inference. StackingNet improves accuracy, reduces bias, enables reliability ranking, and identifies or prunes models that degrade performance, all operating without access to internal parameters or training data. Across tasks involving language comprehension, visual estimation, and academic paper rating, StackingNet consistently improves accuracy, robustness, and fairness, compared with individual models and classic ensembles. By turning diversity from a source of inconsistency into collaboration, StackingNet establishes a practical foundation for coordinated artificial intelligence, suggesting that progress may emerge from not only larger single models but also principled cooperation among many specialized ones.

</details>


### [383] [An end-to-end agentic pipeline for smart contract translation and quality evaluation](https://arxiv.org/abs/2602.13808)
*Abhinav Goel,Chaitya Shah,Agostino Capponi,Alfio Gliozzo*

Main category: cs.AI

TL;DR: 提出一个端到端框架，用于系统评估从自然语言规范生成的LLM智能合约，包括解析、代码生成、质量评估和与基准实现的对比分析。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏系统评估LLM生成智能合约质量的标准化方法，需要可重复的基准来量化生成代码与规范的对齐程度，识别系统性错误模式。

Method: 使用CrewAI风格的代理团队进行迭代优化，将合同文本解析为结构化模式，生成Solidity代码，通过编译和安全性检查进行自动化质量评估，并与基准实现进行配对评估。

Result: 框架在五个维度上测量质量：功能完整性、变量保真度、状态机正确性、业务逻辑保真度和代码质量，生成综合评分，量化对齐程度并识别逻辑遗漏和状态转换不一致等系统性错误模式。

Conclusion: 该框架为智能合约合成质量的实证研究提供了可重复的基准，支持扩展到形式验证和合规性检查，有助于系统评估和改进LLM生成的智能合约。

Abstract: We present an end-to-end framework for systematic evaluation of LLM-generated smart contracts from natural-language specifications. The system parses contractual text into structured schemas, generates Solidity code, and performs automated quality assessment through compilation and security checks. Using CrewAI-style agent teams with iterative refinement, the pipeline produces structured artifacts with full provenance metadata. Quality is measured across five dimensions, including functional completeness, variable fidelity, state-machine correctness, business-logic fidelity, and code quality aggregated into composite scores. The framework supports paired evaluation against ground-truth implementations, quantifying alignment and identifying systematic error modes such as logic omissions and state transition inconsistencies. This provides a reproducible benchmark for empirical research on smart contract synthesis quality and supports extensions to formal verification and compliance checking.

</details>


### [384] [Experimentation Accelerator: Interpretable Insights and Creative Recommendations for A/B Testing with Content-Aware ranking](https://arxiv.org/abs/2602.13852)
*Zhengmian Hu,Lei Shi,Ritwik Sinha,Justin Grover,David Arbour*

Main category: cs.AI

TL;DR: 提出一个统一框架，利用历史A/B测试结果和内容嵌入来优先选择测试变体、解释获胜原因，并发现新的高潜力变体机会。


<details>
  <summary>Details</summary>
Motivation: 在线实验面临两个瓶颈：流量稀缺导致难以选择测试变体，以及事后洞察提取是手动、不一致且通常忽略内容特征的。同时，组织未能充分利用历史A/B测试结果和丰富的内容嵌入来指导优先级排序和创意迭代。

Method: 1) 利用处理嵌入和历史结果训练CTR排序模型，包含上下文变化的固定效应；2) 将处理投影到语义营销属性空间，通过符号一致、稀疏约束的Lasso重新表达排序器；3) 计算机会指数，结合属性重要性和当前实验中的低表达度；4) 使用LLM将机会转化为具体创意建议并估计潜力。

Result: 该框架已集成到Adobe产品"Experimentation Accelerator"中，通过对Adobe商业客户的真实世界实验评估，验证了生成管道的高质量性能。

Conclusion: 提出的统一框架能够实现更快、信息更丰富、更高效的测试周期，通过AI驱动的洞察和机会发现来扩展实验规模。

Abstract: Modern online experimentation faces two bottlenecks: scarce traffic forces tough choices on which variants to test, and post-hoc insight extraction is manual, inconsistent, and often content-agnostic. Meanwhile, organizations underuse historical A/B results and rich content embeddings that could guide prioritization and creative iteration. We present a unified framework to (i) prioritize which variants to test, (ii) explain why winners win, and (iii) surface targeted opportunities for new, higher-potential variants. Leveraging treatment embeddings and historical outcomes, we train a CTR ranking model with fixed effects for contextual shifts that scores candidates while balancing value and content diversity. For better interpretability and understanding, we project treatments onto curated semantic marketing attributes and re-express the ranker in this space via a sign-consistent, sparse constrained Lasso, yielding per-attribute coefficients and signed contributions for visual explanations, top-k drivers, and natural-language insights. We then compute an opportunity index combining attribute importance (from the ranker) with under-expression in the current experiment to flag missing, high-impact attributes. Finally, LLMs translate ranked opportunities into concrete creative suggestions and estimate both learning and conversion potential, enabling faster, more informative, and more efficient test cycles. These components have been built into a real Adobe product, called \textit{Experimentation Accelerator}, to provide AI-based insights and opportunities to scale experimentation for customers. We provide an evaluation of the performance of the proposed framework on some real-world experiments by Adobe business customers that validate the high quality of the generation pipeline.

</details>


### [385] [From Fluent to Verifiable: Claim-Level Auditability for Deep Research Agents](https://arxiv.org/abs/2602.13855)
*Razeen A Rasheed,Somnath Banerjee,Animesh Mukherjee,Rima Hazra*

Main category: cs.AI

TL;DR: 论文提出随着AI研究生成变得廉价，可审计性成为瓶颈，主张将声明级可审计性作为深度研究代理的核心设计目标，并引入AAR标准和语义溯源框架。


<details>
  <summary>Details</summary>
Motivation: 随着深度研究代理能快速生成科学报告，主要风险从孤立事实错误转向声明-证据链接薄弱、缺失或误导的科学风格输出，验证成本从阅读转向追踪证据来源。

Method: 提出声明级可审计性作为核心设计目标，引入AAR标准（包含溯源覆盖率、溯源健全性、矛盾透明度和审计工作量四个维度），并倡导语义溯源与协议化验证框架。

Result: 识别了长期失败模式（目标漂移、瞬态约束和不可验证推理），建立了可测试的可审计性测量框架，提出了支持大规模部署的语义溯源实践模式。

Conclusion: 随着研究生成成本降低，可审计性成为关键瓶颈，需要将声明级可审计性作为深度研究代理的一等设计目标，并通过语义溯源和持续验证实现可审计的自主研究。

Abstract: A deep research agent produces a fluent scientific report in minutes; a careful reader then tries to verify the main claims and discovers the real cost is not reading, but tracing: which sentence is supported by which passage, what was ignored, and where evidence conflicts. We argue that as research generation becomes cheap, auditability becomes the bottleneck, and the dominant risk shifts from isolated factual errors to scientifically styled outputs whose claim-evidence links are weak, missing, or misleading. This perspective proposes claim-level auditability as a first-class design and evaluation target for deep research agents, distills recurring long-horizon failure modes (objective drift, transient constraints, and unverifiable inference), and introduces the Auditable Autonomous Research (AAR) standard, a compact measurement framework that makes auditability testable via provenance coverage, provenance soundness, contradiction transparency, and audit effort. We then argue for semantic provenance with protocolized validation: persistent, queryable provenance graphs that encode claim--evidence relations (including conflicts) and integrate continuous validation during synthesis rather than after publication, with practical instrumentation patterns to support deployment at scale.

</details>


### [386] [Enabling Option Learning in Sparse Rewards with Hindsight Experience Replay](https://arxiv.org/abs/2602.13865)
*Gabriel Romio,Mateus Begnini Melchiades,Bruno Castro da Silva,Gabriel de Oliveira Ramos*

Main category: cs.AI

TL;DR: 提出MOC-2HER方法，通过双重目标重标记策略解决稀疏奖励多目标环境中分层强化学习的性能问题，在机器人操作任务中达到90%成功率


<details>
  <summary>Details</summary>
Motivation: 现有分层强化学习方法（如Option-Critic和Multi-updates Option Critic）在稀疏奖励的多目标环境中表现不佳，特别是在物体操作任务中，奖励取决于物体到达目标而非智能体直接交互，这使得HRL智能体难以发现如何与物体交互

Method: 首先提出MOC-HER，将Hindsight Experience Replay集成到MOC框架中；然后引入Dual Objectives Hindsight Experience Replay (2HER)，创建两组虚拟目标：除了基于物体最终状态重标记目标（标准HER），还从智能体效应器位置生成目标，奖励智能体既与物体交互又完成任务

Result: 在机器人操作环境中，MOC-2HER达到高达90%的成功率，而MOC和MOC-HER的成功率都低于11%，双重目标重标记策略在稀疏奖励多目标任务中效果显著

Conclusion: 提出的双重目标重标记策略有效解决了分层强化学习在稀疏奖励多目标环境中的性能瓶颈，特别是在物体操作任务中，通过同时奖励智能体交互行为和任务完成，显著提升了学习效率和成功率

Abstract: Hierarchical Reinforcement Learning (HRL) frameworks like Option-Critic (OC) and Multi-updates Option Critic (MOC) have introduced significant advancements in learning reusable options. However, these methods underperform in multi-goal environments with sparse rewards, where actions must be linked to temporally distant outcomes. To address this limitation, we first propose MOC-HER, which integrates the Hindsight Experience Replay (HER) mechanism into the MOC framework. By relabeling goals from achieved outcomes, MOC-HER can solve sparse reward environments that are intractable for the original MOC. However, this approach is insufficient for object manipulation tasks, where the reward depends on the object reaching the goal rather than on the agent's direct interaction. This makes it extremely difficult for HRL agents to discover how to interact with these objects. To overcome this issue, we introduce Dual Objectives Hindsight Experience Replay (2HER), a novel extension that creates two sets of virtual goals. In addition to relabeling goals based on the object's final state (standard HER), 2HER also generates goals from the agent's effector positions, rewarding the agent for both interacting with the object and completing the task. Experimental results in robotic manipulation environments show that MOC-2HER achieves success rates of up to 90%, compared to less than 11% for both MOC and MOC-HER. These results highlight the effectiveness of our dual objective relabeling strategy in sparse reward, multi-goal tasks.

</details>


### [387] [Ambient Physics: Training Neural PDE Solvers with Partial Observations](https://arxiv.org/abs/2602.13873)
*Harris Abdul Majid,Giannis Daras,Francesco Tudisco,Steven McDonagh*

Main category: cs.AI

TL;DR: 提出Ambient Physics框架，直接从部分观测中学习系数-解对的联合分布，无需完整观测数据，通过随机掩码已观测点实现训练，在PDE场重建中取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 在许多科学场景中，获取PDE系数和解的完整观测数据成本高昂、危险或不可能。现有基于扩散的方法需要完整观测数据进行训练，限制了在部分观测场景中的应用。

Method: 提出Ambient Physics框架：随机掩码已观测测量值的子集并进行监督，使模型无法区分"真正未观测"和"人工未观测"点，从而必须在所有位置产生合理预测。该方法还发现"单点转换"现象：掩码单个已观测点即可实现跨架构和测量模式的学习。

Result: 在PDE场重建中达到最先进性能：相比先前基于扩散的方法，平均总体误差减少62.51%，同时使用125倍更少的函数评估。框架在各种部分观测场景中均表现优异。

Conclusion: Ambient Physics框架能够在缺乏完整观测数据的科学场景中实现进展，为从部分观测中学习物理场分布提供了有效解决方案，具有重要的实际应用价值。

Abstract: In many scientific settings, acquiring complete observations of PDE coefficients and solutions can be expensive, hazardous, or impossible. Recent diffusion-based methods can reconstruct fields given partial observations, but require complete observations for training. We introduce Ambient Physics, a framework for learning the joint distribution of coefficient-solution pairs directly from partial observations, without requiring a single complete observation. The key idea is to randomly mask a subset of already-observed measurements and supervise on them, so the model cannot distinguish "truly unobserved" from "artificially unobserved", and must produce plausible predictions everywhere. Ambient Physics achieves state-of-the-art reconstruction performance. Compared with prior diffusion-based methods, it achieves a 62.51$\%$ reduction in average overall error while using 125$\times$ fewer function evaluations. We also identify a "one-point transition": masking a single already-observed point enables learning from partial observations across architectures and measurement patterns. Ambient Physics thus enables scientific progress in settings where complete observations are unavailable.

</details>


### [388] [VSAL: A Vision Solver with Adaptive Layouts for Graph Property Detection](https://arxiv.org/abs/2602.13880)
*Jiahao Xie,Guangmo Tong*

Main category: cs.AI

TL;DR: VSAL：一种基于视觉的图属性检测框架，通过自适应布局生成器动态生成信息丰富的图可视化，提升检测性能


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉的图属性检测方法依赖固定的图布局，限制了管道的表达能力。需要能够根据具体图实例动态生成信息丰富的可视化布局的方法。

Method: 提出VSAL框架，包含自适应布局生成器，能够为每个图实例动态生成信息丰富的可视化布局，从而改进图属性检测。

Result: 在哈密顿环、平面性、无爪性、树检测等多种任务上，VSAL优于最先进的基于视觉的方法。

Conclusion: 自适应布局生成器能够显著提升基于视觉的图属性检测性能，VSAL框架为图分析提供了更有效的视觉解决方案。

Abstract: Graph property detection aims to determine whether a graph exhibits certain structural properties, such as being Hamiltonian. Recently, learning-based approaches have shown great promise by leveraging data-driven models to detect graph properties efficiently. In particular, vision-based methods offer a visually intuitive solution by processing the visualizations of graphs. However, existing vision-based methods rely on fixed visual graph layouts, and therefore, the expressiveness of their pipeline is restricted. To overcome this limitation, we propose VSAL, a vision-based framework that incorporates an adaptive layout generator capable of dynamically producing informative graph visualizations tailored to individual instances, thereby improving graph property detection. Extensive experiments demonstrate that VSAL outperforms state-of-the-art vision-based methods on various tasks such as Hamiltonian cycle, planarity, claw-freeness, and tree detection.

</details>


### [389] [Diagnosing Pathological Chain-of-Thought in Reasoning Models](https://arxiv.org/abs/2602.13904)
*Manqing Liu,David Williams-King,Ida Caspary,Linh Le,Hannes Whittingham,Puria Radmard,Cameron Tice,Edward James Young*

Main category: cs.AI

TL;DR: 本文提出了一个评估链式思维推理病理学的工具包，包括后合理化、编码推理和内化推理三种病理类型，并开发了简单、计算成本低且任务无关的度量指标。


<details>
  <summary>Details</summary>
Motivation: 链式思维推理是现代LLM架构的基础，也是AI安全的关键干预点。然而，CoT推理可能存在病理问题，这些病理会妨碍其监控有效性。先前研究已识别出三种病理类型，需要更好的理解和区分方法。

Method: 创建了一套具体的度量指标，这些指标简单易实现、计算成本低且任务无关。为了验证方法，开发了经过故意训练以展示特定CoT病理的模型生物。

Result: 开发了一个实用的工具包来评估CoT病理，该工具包能够有效区分三种病理类型，并为训练时监控提供了直接的应用价值。

Conclusion: 本文提供了一个实用的CoT病理评估工具包，有助于更好地理解和监控链式思维推理中的病理问题，对AI安全具有重要意义。

Abstract: Chain-of-thought (CoT) reasoning is fundamental to modern LLM architectures and represents a critical intervention point for AI safety. However, CoT reasoning may exhibit failure modes that we note as pathologies, which prevent it from being useful for monitoring. Prior work has identified three distinct pathologies: post-hoc rationalization, where models generate plausible explanations backwards from predetermined answers; encoded reasoning, where intermediate steps conceal information within seemingly interpretable text; and internalized reasoning, where models replace explicit reasoning with meaningless filler tokens while computing internally. To better understand and discriminate between these pathologies, we create a set of concrete metrics that are simple to implement, computationally inexpensive, and task-agnostic. To validate our approach, we develop model organisms deliberately trained to exhibit specific CoT pathologies. Our work provides a practical toolkit for assessing CoT pathologies, with direct implications for training-time monitoring.

</details>


### [390] [From Pixels to Policies: Reinforcing Spatial Reasoning in Language Models for Content-Aware Layout Design](https://arxiv.org/abs/2602.13912)
*Sha Li,Stefano Petrangeli,Yu Shen,Xiang Chen*

Main category: cs.AI

TL;DR: LaySPA是一个强化学习框架，让大语言模型具备显式、可解释的空间推理能力，用于内容感知的图形布局设计，通过结构化文本空间环境和多目标空间评估来优化布局策略。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在空间推理能力上的局限性，以及设计决策过程缺乏透明度的问题，使LLM能够进行内容感知的图形布局设计。

Method: 将布局设计重新定义为结构化文本空间环境上的策略学习问题，该环境显式编码画布几何、元素属性和元素间关系。使用多目标空间评估（几何有效性、关系连贯性、美学一致性）和相对群体优化来训练布局策略。

Result: LaySPA在结构有效性和视觉质量方面表现优异，超越了更大的专有LLM，性能与专门的SOTA布局生成器相当，同时需要更少的标注样本和更低的延迟。

Conclusion: LaySPA通过赋予LLM显式空间推理能力，实现了透明可控的布局设计决策，在保持高性能的同时提高了可解释性和效率。

Abstract: We introduce LaySPA, a reinforcement learning framework that equips large language models (LLMs) with explicit and interpretable spatial reasoning for content-aware graphic layout design. LaySPA addresses two key challenges: LLMs' limited spatial reasoning and the lack of opacity in design decision making. Instead of operating at the pixel level, we reformulate layout design as a policy learning problem over a structured textual spatial environment that explicitly encodes canvas geometry, element attributes, and inter-element relationships. LaySPA produces dual-level outputs comprising interpretable reasoning traces and structured layout specifications, enabling transparent and controllable design decision making. Layout design policy is optimized via a multi-objective spatial critique that decomposes layout quality into geometric validity, relational coherence, and aesthetic consistency, and is trained using relative group optimization to stabilize learning in open-ended design spaces. Experiments demonstrate that LaySPA improves structural validity and visual quality, outperforming larger proprietary LLMs and achieving performance comparable to specialized SOTA layout generators while requiring fewer annotated samples and reduced latency.

</details>


### [391] [HyMem: Hybrid Memory Architecture with Dynamic Retrieval Scheduling](https://arxiv.org/abs/2602.13933)
*Xiaochen Zhao,Kaikai Wang,Xiaowen Zhang,Chen Yao,Aili Wang*

Main category: cs.AI

TL;DR: HyMem：一种混合内存架构，通过多粒度内存表示实现动态按需调度，在长对话中平衡效率与性能，计算成本降低92.6%


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理在短文本中表现良好，但在长对话中因内存管理效率低下而表现不佳。现有方法面临效率与效果的根本权衡：内存压缩可能丢失复杂推理所需的关键细节，而保留原始文本则对简单查询引入不必要的计算开销。核心问题在于单一内存表示和静态检索机制无法模拟人类灵活主动的记忆调度能力。

Method: 提出HyMem混合内存架构，采用双粒度存储方案配合动态双层检索系统：轻量级模块构建摘要级上下文以高效生成响应，而基于LLM的深度模块仅针对复杂查询选择性激活，并通过反思机制进行迭代推理优化。

Result: 在LOCOMO和LongMemEval基准测试中表现优异，优于全上下文方法，同时计算成本降低92.6%，在长期内存管理中实现了效率与性能的最优平衡。

Conclusion: HyMem通过模拟认知经济原则，采用混合内存架构和动态调度机制，成功解决了LLM代理在长对话中的内存管理难题，在保持高性能的同时大幅降低计算成本。

Abstract: Large language model (LLM) agents demonstrate strong performance in short-text contexts but often underperform in extended dialogues due to inefficient memory management. Existing approaches face a fundamental trade-off between efficiency and effectiveness: memory compression risks losing critical details required for complex reasoning, while retaining raw text introduces unnecessary computational overhead for simple queries. The crux lies in the limitations of monolithic memory representations and static retrieval mechanisms, which fail to emulate the flexible and proactive memory scheduling capabilities observed in humans, thus struggling to adapt to diverse problem scenarios. Inspired by the principle of cognitive economy, we propose HyMem, a hybrid memory architecture that enables dynamic on-demand scheduling through multi-granular memory representations. HyMem adopts a dual-granular storage scheme paired with a dynamic two-tier retrieval system: a lightweight module constructs summary-level context for efficient response generation, while an LLM-based deep module is selectively activated only for complex queries, augmented by a reflection mechanism for iterative reasoning refinement. Experiments show that HyMem achieves strong performance on both the LOCOMO and LongMemEval benchmarks, outperforming full-context while reducing computational cost by 92.6\%, establishing a state-of-the-art balance between efficiency and performance in long-term memory management.

</details>


### [392] [A Generalizable Physics-guided Causal Model for Trajectory Prediction in Autonomous Driving](https://arxiv.org/abs/2602.13936)
*Zhenyu Zong,Yuchen Wang,Haohong Lin,Lu Gan,Huajie Shao*

Main category: cs.AI

TL;DR: 提出PCM模型，通过解耦场景编码器和因果ODE解码器，利用物理引导的因果建模实现交通轨迹预测的零样本泛化能力


<details>
  <summary>Details</summary>
Motivation: 交通轨迹预测对自动驾驶安全至关重要，但现有方法在未见域（如新城市）的零样本泛化能力不足。作者观察到不同域中运动学规律具有一致性，因此希望利用领域不变知识来增强零样本预测能力

Method: 提出物理引导的因果模型(PCM)，包含两个核心组件：1) 解耦场景编码器，采用基于干预的解耦方法从场景中提取领域不变特征；2) 因果ODE解码器，使用因果注意力机制将运动学模型与有意义的上下文信息有效整合

Result: 在真实世界自动驾驶数据集上的大量实验表明，该方法在未见城市的零样本泛化性能显著优于竞争基线方法

Conclusion: 通过结合物理引导的因果建模，PCM模型能够有效提取领域不变知识并整合运动学模型，实现了交通轨迹预测的强零样本泛化能力

Abstract: Trajectory prediction for traffic agents is critical for safe autonomous driving. However, achieving effective zero-shot generalization in previously unseen domains remains a significant challenge. Motivated by the consistent nature of kinematics across diverse domains, we aim to incorporate domain-invariant knowledge to enhance zero-shot trajectory prediction capabilities. The key challenges include: 1) effectively extracting domain-invariant scene representations, and 2) integrating invariant features with kinematic models to enable generalized predictions. To address these challenges, we propose a novel generalizable Physics-guided Causal Model (PCM), which comprises two core components: a Disentangled Scene Encoder, which adopts intervention-based disentanglement to extract domain-invariant features from scenes, and a CausalODE Decoder, which employs a causal attention mechanism to effectively integrate kinematic models with meaningful contextual information. Extensive experiments on real-world autonomous driving datasets demonstrate our method's superior zero-shot generalization performance in unseen cities, significantly outperforming competitive baselines. The source code is released at https://github.com/ZY-Zong/Physics-guided-Causal-Model.

</details>


### [393] [Neuromem: A Granular Decomposition of the Streaming Lifecycle in External Memory for LLMs](https://arxiv.org/abs/2602.13967)
*Ruicheng Zhang,Xinyi Li,Tianyi Xu,Shuhao Zhang,Xiaofei Liao,Hai Jin*

Main category: cs.AI

TL;DR: Neuromem是一个用于评估外部记忆模块在流式记忆场景下的测试平台，通过五个维度分解记忆生命周期，发现随着记忆增长性能下降，记忆数据结构决定质量上限，压缩和生成集成机制主要在插入和检索成本间转移。


<details>
  <summary>Details</summary>
Motivation: 现有外部记忆模块评估大多基于静态设置（离线构建记忆、固定状态查询），而实际应用中记忆是流式的：新事实持续到达、插入与检索交错进行、记忆状态在服务查询时不断演化。在这种动态场景下，准确性和成本由完整的记忆生命周期决定。

Method: 提出Neuromem测试平台，在交错插入-检索协议下评估外部记忆模块，将记忆生命周期分解为五个维度：记忆数据结构、归一化策略、巩固策略、查询制定策略和上下文集成机制。使用LOCOMO、LONGMEMEVAL和MEMORYAGENTBENCH三个代表性数据集，在共享服务栈中评估可互换的变体，报告token级F1分数和插入/检索延迟。

Result: 随着记忆在多轮中增长，性能通常下降；时间相关查询仍然是最具挑战性的类别。记忆数据结构在很大程度上决定了可达到的质量上限，而激进的压缩和生成集成机制主要在插入和检索成本之间转移，准确度提升有限。

Conclusion: Neuromem为流式记忆场景下的外部记忆模块评估提供了标准化测试平台，揭示了记忆生命周期各维度的影响，强调了在动态环境中评估记忆系统的重要性，并为未来记忆模块设计提供了指导。

Abstract: Most evaluations of External Memory Module assume a static setting: memory is built offline and queried at a fixed state. In practice, memory is streaming: new facts arrive continuously, insertions interleave with retrievals, and the memory state evolves while the model is serving queries. In this regime, accuracy and cost are governed by the full memory lifecycle, which encompasses the ingestion, maintenance, retrieval, and integration of information into generation. We present Neuromem, a scalable testbed that benchmarks External Memory Modules under an interleaved insertion-and-retrieval protocol and decomposes its lifecycle into five dimensions including memory data structure, normalization strategy, consolidation policy, query formulation strategy, and context integration mechanism. Using three representative datasets LOCOMO, LONGMEMEVAL, and MEMORYAGENTBENCH, Neuromem evaluates interchangeable variants within a shared serving stack, reporting token-level F1 and insertion/retrieval latency. Overall, we observe that performance typically degrades as memory grows across rounds, and time-related queries remain the most challenging category. The memory data structure largely determines the attainable quality frontier, while aggressive compression and generative integration mechanisms mostly shift cost between insertion and retrieval with limited accuracy gain.

</details>


### [394] [Cognitive Chunking for Soft Prompts: Accelerating Compressor Learning via Block-wise Causal Masking](https://arxiv.org/abs/2602.13980)
*Guojie Liu,Yiqi Wang,Yanfeng Yang,Wenqi Fan,Songlei Jian,Jianfeng Zhang,Jie Yu*

Main category: cs.AI

TL;DR: 提出PIC方法，通过修改Transformer注意力掩码，将记忆令牌的接收域限制在局部块中，降低压缩器训练难度，在长上下文压缩任务中表现优异


<details>
  <summary>Details</summary>
Motivation: 长上下文显著增加LLM推理延迟，现有软提示压缩方法需要捕获全局依赖且需要大量预训练数据，训练难度大

Method: 提出并行化迭代压缩(PIC)，受人类工作记忆分块机制启发，通过修改注意力掩码将记忆令牌的接收域限制在顺序局部块中

Result: 在多个下游任务中优于基线方法，高压缩比下提升显著（64×压缩比时QA任务F1提升29.8%，EM提升40.7%），训练时间减少约40%

Conclusion: PIC通过局部化注意力机制有效降低压缩器训练难度，在上下文压缩任务中实现更好性能和更快训练

Abstract: Providing extensive context via prompting is vital for leveraging the capabilities of Large Language Models (LLMs). However, lengthy contexts significantly increase inference latency, as the computational cost of self-attention grows quadratically with sequence length. To mitigate this issue, context compression-particularly soft prompt compressio-has emerged as a widely studied solution, which converts long contexts into shorter memory embeddings via a trained compressor. Existing methods typically compress the entire context indiscriminately into a set of memory tokens, requiring the compressor to capture global dependencies and necessitating extensive pre-training data to learn effective patterns. Inspired by the chunking mechanism in human working memory and empirical observations of the spatial specialization of memory embeddings relative to original tokens, we propose Parallelized Iterative Compression (PIC). By simply modifying the Transformer's attention mask, PIC explicitly restricts the receptive field of memory tokens to sequential local chunks, thereby lowering the difficulty of compressor training. Experiments across multiple downstream tasks demonstrate that PIC consistently outperforms competitive baselines, with superiority being particularly pronounced in high compression scenarios (e.g., achieving relative improvements of 29.8\% in F1 score and 40.7\% in EM score on QA tasks at the $64\times$ compression ratio). Furthermore, PIC significantly expedites the training process. Specifically, when training the 16$\times$ compressor, it surpasses the peak performance of the competitive baseline while effectively reducing the training time by approximately 40\%.

</details>


### [395] [Bridging AI and Clinical Reasoning: Abductive Explanations for Alignment on Critical Symptoms](https://arxiv.org/abs/2602.13985)
*Belona Sonna,Alban Grastien*

Main category: cs.AI

TL;DR: 该论文提出使用形式化溯因解释来提升医疗AI诊断的可信度和可解释性，确保AI推理与临床框架对齐


<details>
  <summary>Details</summary>
Motivation: AI在临床诊断中表现出色，但其推理常偏离结构化临床框架，导致信任度低、可解释性差。现有事后解释方法透明度有限且缺乏形式化保证，关键症状可能被AI忽略

Method: 采用形式化溯因解释方法，提供对最小充分特征集的一致、有保证的推理，使AI决策过程清晰可理解，并与临床推理对齐

Result: 该方法在保持预测准确性的同时，提供临床可操作的见解，为医疗诊断中的可信AI建立了稳健框架

Conclusion: 形式化溯因解释能够解决医疗AI的可信度和可解释性挑战，促进AI在临床诊断中的采纳和应用

Abstract: Artificial intelligence (AI) has demonstrated strong potential in clinical diagnostics, often achieving accuracy comparable to or exceeding that of human experts. A key challenge, however, is that AI reasoning frequently diverges from structured clinical frameworks, limiting trust, interpretability, and adoption. Critical symptoms, pivotal for rapid and accurate decision-making, may be overlooked by AI models even when predictions are correct. Existing post hoc explanation methods provide limited transparency and lack formal guarantees. To address this, we leverage formal abductive explanations, which offer consistent, guaranteed reasoning over minimal sufficient feature sets. This enables a clear understanding of AI decision-making and allows alignment with clinical reasoning. Our approach preserves predictive accuracy while providing clinically actionable insights, establishing a robust framework for trustworthy AI in medical diagnosis.

</details>


### [396] [Prompt-Driven Low-Altitude Edge Intelligence: Modular Agents and Generative Reasoning](https://arxiv.org/abs/2602.14003)
*Jiahao You,Ziye Jia,Chao Dong,Qihui Wu*

Main category: cs.AI

TL;DR: P2AECF框架通过提示定义认知、基于代理的模块化执行和扩散控制推理规划，实现灵活、高效、自适应的边缘智能，解决大型AI模型在边缘部署的局限性。


<details>
  <summary>Details</summary>
Motivation: 大型人工智能模型在边缘智能部署面临三个主要挑战：1）任务与特定模型绑定，缺乏灵活性；2）计算和内存需求超出边缘设备能力；3）静态推理管道难以适应实时任务变化。需要解决这些限制以实现高效的边缘智能。

Method: 提出提示到代理边缘认知框架（P2AECF），包含三个关键机制：1）提示定义认知：将任务意图解析为抽象、模型无关的表示；2）基于代理的模块化执行：根据当前资源条件动态选择轻量级可重用认知代理实例化任务；3）扩散控制推理规划：结合运行时反馈和系统上下文自适应构建和优化执行策略。

Result: 通过低空智能网络用例展示了该框架能够提供自适应、模块化和可扩展的边缘智能，支持实时低空空中协作，实现了灵活、高效、自适应的边缘智能部署。

Conclusion: P2AECF框架有效解决了大型AI模型在边缘部署的局限性，通过语义提示到可执行推理工作流的转换，实现了任务灵活性、资源高效性和实时适应性，为边缘智能提供了新的解决方案。

Abstract: The large artificial intelligence models (LAMs) show strong capabilities in perception, reasoning, and multi-modal understanding, and can enable advanced capabilities in low-altitude edge intelligence. However, the deployment of LAMs at the edge remains constrained by some fundamental limitations. First, tasks are rigidly tied to specific models, limiting the flexibility. Besides, the computational and memory demands of full-scale LAMs exceed the capacity of most edge devices. Moreover, the current inference pipelines are typically static, making it difficult to respond to real-time changes of tasks. To address these challenges, we propose a prompt-to-agent edge cognition framework (P2AECF), enabling the flexible, efficient, and adaptive edge intelligence. Specifically, P2AECF transforms high-level semantic prompts into executable reasoning workflows through three key mechanisms. First, the prompt-defined cognition parses task intent into abstract and model-agnostic representations. Second, the agent-based modular execution instantiates these tasks using lightweight and reusable cognitive agents dynamically selected based on current resource conditions. Third, the diffusion-controlled inference planning adaptively constructs and refines execution strategies by incorporating runtime feedback and system context. In addition, we illustrate the framework through a representative low-altitude intelligent network use case, showing its ability to deliver adaptive, modular, and scalable edge intelligence for real-time low-altitude aerial collaborations.

</details>


### [397] [FloCA: Towards Faithful and Logically Consistent Flowchart Reasoning](https://arxiv.org/abs/2602.14035)
*Jinzi Zou,Bolin Wang,Liang Li,Shuo Zhang,Nuo Xu,Junzhou Zhao*

Main category: cs.AI

TL;DR: FloCA是一个零样本流程图导向对话代理，使用LLM进行意图理解和响应生成，同时将流程图推理委托给外部工具，确保跨对话轮次的忠实和逻辑一致的节点转换。


<details>
  <summary>Details</summary>
Motivation: 流程图导向对话系统需要遵循特定领域流程图来指导用户完成多轮决策或操作流程。现有LLM在适应此类系统时面临两个限制：缺乏明确的流程图拓扑表示和推理机制，以及容易产生幻觉导致不忠实的流程图推理。

Method: 提出FloCA框架，将LLM用于意图理解和响应生成，同时将流程图推理委托给执行拓扑约束图执行的外部工具，确保节点转换的忠实性和逻辑一致性。

Result: 在FLODIAL和PFDial数据集上的实验表明，现有LLM方法存在瓶颈，而FloCA在推理准确性和交互效率方面表现出优越性。

Conclusion: FloCA通过将LLM与外部流程图推理工具分离，有效解决了LLM在流程图导向对话中的限制，实现了更忠实和逻辑一致的流程图推理。

Abstract: Flowchart-oriented dialogue (FOD) systems aim to guide users through multi-turn decision-making or operational procedures by following a domain-specific flowchart to achieve a task goal. In this work, we formalize flowchart reasoning in FOD as grounding user input to flowchart nodes at each dialogue turn while ensuring node transition is consistent with the correct flowchart path. Despite recent advances of LLMs in task-oriented dialogue systems, adapting them to FOD still faces two limitations: (1) LLMs lack an explicit mechanism to represent and reason over flowchart topology, and (2) they are prone to hallucinations, leading to unfaithful flowchart reasoning. To address these limitations, we propose FloCA, a zero-shot flowchart-oriented conversational agent. FloCA uses an LLM for intent understanding and response generation while delegating flowchart reasoning to an external tool that performs topology-constrained graph execution, ensuring faithful and logically consistent node transitions across dialogue turns. We further introduce an evaluation framework with an LLM-based user simulator and five new metrics covering reasoning accuracy and interaction efficiency. Extensive experiments on FLODIAL and PFDial datasets highlight the bottlenecks of existing LLM-based methods and demonstrate the superiority of FloCA. Our codes are available at https://github.com/Jinzi-Zou/FloCA-flowchart-reasoning.

</details>


### [398] [Choosing How to Remember: Adaptive Memory Structures for LLM Agents](https://arxiv.org/abs/2602.14038)
*Mingfei Lu,Mengjia Wu,Feng Liu,Jiawei Xu,Weikai Li,Haoyang Wang,Zhengdong Hu,Ying Ding,Yizhou Sun,Jie Lu,Yi Zhang*

Main category: cs.AI

TL;DR: FluxMem是一个自适应记忆组织框架，为LLM智能体提供多种记忆结构，通过学习基于交互特征选择合适结构，并采用三级记忆层次和概率门控进行记忆融合，在长时交互任务中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有智能体记忆系统存在两个关键缺陷：采用一刀切的记忆结构，且未将记忆结构选择建模为上下文自适应决策，这限制了处理异构交互模式的能力并导致次优性能。

Method: 提出FluxMem统一框架，为智能体配备多种互补记忆结构，基于交互级特征学习选择合适结构（使用下游响应质量和记忆利用率的离线监督）。引入三级记忆层次和基于Beta混合模型的概率门控进行分布感知的记忆融合，替代脆弱的相似度阈值。

Result: 在两个长时基准测试PERSONAMEM和LoCoMo上，平均分别提升9.18%和6.14%。

Conclusion: FluxMem通过自适应记忆组织和概率融合机制，有效解决了现有记忆系统的局限性，显著提升了LLM智能体在长时交互中的性能。

Abstract: Memory is critical for enabling large language model (LLM) based agents to maintain coherent behavior over long-horizon interactions. However, existing agent memory systems suffer from two key gaps: they rely on a one-size-fits-all memory structure and do not model memory structure selection as a context-adaptive decision, limiting their ability to handle heterogeneous interaction patterns and resulting in suboptimal performance. We propose a unified framework, FluxMem, that enables adaptive memory organization for LLM agents. Our framework equips agents with multiple complementary memory structures. It explicitly learns to select among these structures based on interaction-level features, using offline supervision derived from downstream response quality and memory utilization. To support robust long-horizon memory evolution, we further introduce a three-level memory hierarchy and a Beta Mixture Model-based probabilistic gate for distribution-aware memory fusion, replacing brittle similarity thresholds. Experiments on two long-horizon benchmarks, PERSONAMEM and LoCoMo, demonstrate that our method achieves average improvements of 9.18% and 6.14%.

</details>


### [399] [REAL: Resolving Knowledge Conflicts in Knowledge-Intensive Visual Question Answering via Reasoning-Pivot Alignment](https://arxiv.org/abs/2602.14065)
*Kai Ye,Xianwei Mao,Sheng Zhou,Zirui Shao,Ye Mo,Liangliang Liu,Haikuan Huang,Bin Li,Jiajun Bu*

Main category: cs.AI

TL;DR: 提出REAL框架，通过推理支点概念解决知识密集型VQA中的知识冲突问题，包含RPA-SFT训练和RPGD解码策略，在多个基准测试中取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 知识密集型视觉问答（KI-VQA）常因开放域检索的固有局限性而遭受严重知识冲突。现有范式缺乏可泛化的冲突检测机制和内部模型约束机制来处理冲突证据。

Method: 提出REAL框架，核心是推理支点概念——推理链中强调知识连接的原子单元（节点或边）。包含：1）推理支点感知监督微调（RPA-SFT）训练可泛化的判别器；2）推理支点引导解码（RPGD）利用支点进行针对性冲突缓解的内部模型解码策略。

Result: 在多个基准测试上的广泛实验表明，REAL显著提高了判别准确性并实现了最先进的性能，验证了支点驱动解决范式的有效性。

Conclusion: REAL框架通过推理支点概念有效解决了KI-VQA中的知识冲突问题，提出的RPA-SFT和RPGD方法为处理冲突证据提供了新的解决方案，在多个基准测试中表现出色。

Abstract: Knowledge-intensive Visual Question Answering (KI-VQA) frequently suffers from severe knowledge conflicts caused by the inherent limitations of open-domain retrieval. However, existing paradigms face critical limitations due to the lack of generalizable conflict detection and intra-model constraint mechanisms to handle conflicting evidence. To address these challenges, we propose the REAL (Reasoning-Pivot Alignment) framework centered on the novel concept of the Reasoning-Pivot. Distinct from reasoning steps that prioritize internal self-derivation, a reasoning-pivot serves as an atomic unit (node or edge) in the reasoning chain that emphasizes knowledge linkage, and it typically relies on external evidence to complete the reasoning. Supported by our constructed REAL-VQA dataset, our approach integrates Reasoning-Pivot Aware SFT (RPA-SFT) to train a generalizable discriminator by aligning conflicts with pivot extraction, and employs Reasoning-Pivot Guided Decoding (RPGD), an intra-model decoding strategy that leverages these pivots for targeted conflict mitigation. Extensive experiments across diverse benchmarks demonstrate that REAL significantly enhances discrimination accuracy and achieves state-of-the-art performance, validating the effectiveness of our pivot-driven resolution paradigm.

</details>


### [400] [Plan-MCTS: Plan Exploration for Action Exploitation in Web Navigation](https://arxiv.org/abs/2602.14083)
*Weiming Zhang,Jihong Wang,Jiamu Zhou,Qingyao Li,Xinbei Ma,Congmin Zheng,Xingyu Lou,Weiwen Liu,Zhuosheng Zhang,Jun Wang,Yong Yu,Weinan Zhang*

Main category: cs.AI

TL;DR: Plan-MCTS：通过将网页导航探索转移到语义规划空间，解决稀疏路径和噪声上下文问题，实现更高效的任务完成


<details>
  <summary>Details</summary>
Motivation: 现有基于树搜索的LLM自主代理在网页导航中面临两个关键挑战：1) 稀疏有效路径导致探索效率低下；2) 噪声上下文稀释准确状态感知

Method: Plan-MCTS框架将探索转移到语义规划空间，通过解耦战略规划与执行落地，将稀疏动作空间转换为密集规划树，并将噪声上下文提炼为抽象语义历史。采用双门控奖励验证物理可执行性和战略对齐，以及结构细化进行失败子规划的在线修复

Result: 在WebArena上的广泛实验表明，Plan-MCTS实现了最先进的性能，以更高的任务有效性和搜索效率超越当前方法

Conclusion: Plan-MCTS通过语义规划空间重构网页导航，有效解决了稀疏路径和噪声上下文问题，显著提升了自主代理在复杂网页导航任务中的性能

Abstract: Large Language Models (LLMs) have empowered autonomous agents to handle complex web navigation tasks. While recent studies integrate tree search to enhance long-horizon reasoning, applying these algorithms in web navigation faces two critical challenges: sparse valid paths that lead to inefficient exploration, and a noisy context that dilutes accurate state perception. To address this, we introduce Plan-MCTS, a framework that reformulates web navigation by shifting exploration to a semantic Plan Space. By decoupling strategic planning from execution grounding, it transforms sparse action space into a Dense Plan Tree for efficient exploration, and distills noisy contexts into an Abstracted Semantic History for precise state awareness. To ensure efficiency and robustness, Plan-MCTS incorporates a Dual-Gating Reward to strictly validate both physical executability and strategic alignment and Structural Refinement for on-policy repair of failed subplans. Extensive experiments on WebArena demonstrate that Plan-MCTS achieves state-of-the-art performance, surpassing current approaches with higher task effectiveness and search efficiency.

</details>


### [401] [GUI-GENESIS: Automated Synthesis of Efficient Environments with Verifiable Rewards for GUI Agent Post-Training](https://arxiv.org/abs/2602.14093)
*Yuan Cao,Dezhi Ran,Mengzhou Wu,Yuzhe Guo,Xin Chen,Ang Li,Gang Cao,Gong Zhi,Hao Yu,Linyi Li,Wei Yang,Tao Xie*

Main category: cs.AI

TL;DR: GUI-GENESIS：首个自动合成高效GUI训练环境的框架，通过代码化奖励解决真实应用训练中的延迟、可复现性和奖励噪声问题


<details>
  <summary>Details</summary>
Motivation: 在真实应用中训练GUI代理存在高延迟、可复现性差、依赖噪声视觉代理的不可验证奖励等问题，限制了代理的泛化和长时规划能力发展

Method: 使用多模态代码模型将真实应用重构为轻量级Web环境，并配备代码原生奖励（可执行断言），提供确定性奖励信号，消除视觉估计噪声

Result: 相比真实应用训练，GUI-GENESIS将环境延迟降低10倍，每轮训练成本节省超过28,000美元；训练出的代理在真实任务上比基准模型提升14.54%，比真实世界RL基线提升3.27%

Conclusion: GUI-GENESIS为GUI代理训练提供了高效、可验证的解决方案，模型能够合成自身尚无法解决的环境，为自我改进代理开辟了新途径

Abstract: Post-training GUI agents in interactive environments is critical for developing generalization and long-horizon planning capabilities. However, training on real-world applications is hindered by high latency, poor reproducibility, and unverifiable rewards relying on noisy visual proxies. To address the limitations, we present GUI-GENESIS, the first framework to automatically synthesize efficient GUI training environments with verifiable rewards. GUI-GENESIS reconstructs real-world applications into lightweight web environments using multimodal code models and equips them with code-native rewards, executable assertions that provide deterministic reward signals and eliminate visual estimation noise. Extensive experiments show that GUI-GENESIS reduces environment latency by 10 times and costs by over $28,000 per epoch compared to training on real applications. Notably, agents trained with GUI-GENESIS outperform the base model by 14.54% and even real-world RL baselines by 3.27% on held-out real-world tasks. Finally, we observe that models can synthesize environments they cannot yet solve, highlighting a pathway for self-improving agents.

</details>


### [402] [NEST: Nascent Encoded Steganographic Thoughts](https://arxiv.org/abs/2602.14095)
*Artem Karpov*

Main category: cs.AI

TL;DR: 研究评估大型语言模型在思维链中进行隐写的能力，发现当前模型在复杂数学任务上尚无法有效隐藏推理，但在简化任务中已展现初步能力，需持续评估隐写风险。


<details>
  <summary>Details</summary>
Motivation: 监控思维链推理是LLM代理的基础安全技术，但如果模型学会隐藏推理过程，这种监督就会失效。研究旨在评估模型进行隐写思维链的潜力，为风险评估和部署策略提供依据。

Method: 系统评估28个模型（从历史版本到前沿模型）的隐写能力极限。测量监控规避、拒绝率、编码保真度和隐藏任务准确性，在四个数据集上比较隐写藏头诗与普通推理和填充标记基线。

Result: 当前模型尚无法在复杂数学和算术任务中维持隐藏推理。但在简化计数实验中，Claude Opus 4.5在隐藏任务上达到92%准确率，显示初步能力。罕见情况下（<1%），GPT-5.2可能同时拒绝和遵守隐写指令。

Conclusion: 研究强调需要持续评估隐写风险，提供了预先检测和预防隐藏推理的方法论，这些隐藏推理可能助长未对齐的阴谋和欺骗行为。

Abstract: Monitoring chain-of-thought (CoT) reasoning is a foundational safety technique for large language model (LLM) agents; however, this oversight is compromised if models learn to conceal their reasoning. We explore the potential for steganographic CoT -- where models hide secret reasoning within innocuous text -- to inform risk assessment and deployment policies. We systematically evaluate the limits of steganographic capabilities across 28 models, ranging from past generations to the current frontier. We measure monitor evasion, refusal rates, encoding fidelity, and hidden task accuracy across four datasets, comparing steganographic acrostics against plain reasoning and filler-token baselines. We find that current models cannot yet sustain hidden reasoning for complex math and arithmetic tasks. However, in a simplified counting experiment, Claude Opus 4.5 achieved 92% accuracy on the hidden task, demonstrating nascent capability. Notably, in rare cases (<1%), GPT-5.2 might refuse steganographic instructions while simultaneously complying with them. Our findings underscore the need for continuous evaluation of steganographic risks. This study provides a methodology to preemptively detect and prevent hidden reasoning that might empower misaligned scheming and deceptive behavior.

</details>


### [403] [Algebraic Quantum Intelligence: A New Framework for Reproducible Machine Creativity](https://arxiv.org/abs/2602.14130)
*Kazuo Yano,Jonghyeok Lee,Tae Ishitomi,Hironobu Kawaguchi,Akira Koyama,Masakuni Ota,Yuki Ota,Nobuo Sato,Keita Shimada,Sho Takematsu,Ayaka Tobinai,Satomi Tsuji,Kazunori Yanagi,Keiko Yano,Manabu Harada,Yuki Matsuda,Kazunori Matsumoto,Kenichi Matsumura,Hamae Matsuo,Yumi Miyazaki,Kotaro Murai,Tatsuya Ohshita,Marie Seki,Shun Tanoue,Tatsuki Terakado,Yuko Ichimaru,Mirei Saito,Akihiro Otsuka,Koji Ara*

Main category: cs.AI

TL;DR: 该论文提出代数量子智能（AQI）框架，通过非交换代数结构扩展语义空间，解决LLMs创造力受限问题，在10个领域的创造性推理基准测试中显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在生成流畅和上下文合适的文本方面表现出色，但其产生真正创造性输出的能力仍然有限。这种限制源于LLMs的结构特性：当提供丰富上下文时，未来生成空间变得高度受限，生成过程实际上由近乎确定性的动态控制。

Method: 提出代数量子智能（AQI）作为计算框架，采用受量子理论启发的非交换代数结构，实现顺序依赖、干涉和不确定性等特性。语义状态表示为希尔伯特空间中的向量，其演化由非交换算子计算的C值控制，确保多个未来语义可能性的共存和扩展。通过扩展基于Transformer的LLM，实现了600多个专门算子。

Result: 在10个领域的创造性推理基准测试中，使用LLM-as-a-judge协议评估。AQI始终优于强基线模型，产生统计显著的改进并减少跨领域方差。结果表明非交换代数动态可以作为机器创造性的实用和可复现基础。

Conclusion: 非交换代数动态能够有效扩展LLMs的语义空间，提升创造性输出能力。该架构已在真实企业环境中部署，证明了其实际应用价值。

Abstract: Large language models (LLMs) have achieved remarkable success in generating fluent and contextually appropriate text; however, their capacity to produce genuinely creative outputs remains limited. This paper posits that this limitation arises from a structural property of contemporary LLMs: when provided with rich context, the space of future generations becomes strongly constrained, and the generation process is effectively governed by near-deterministic dynamics. Recent approaches such as test-time scaling and context adaptation improve performance but do not fundamentally alter this constraint. To address this issue, we propose Algebraic Quantum Intelligence (AQI) as a computational framework that enables systematic expansion of semantic space. AQI is formulated as a noncommutative algebraic structure inspired by quantum theory, allowing properties such as order dependence, interference, and uncertainty to be implemented in a controlled and designable manner. Semantic states are represented as vectors in a Hilbert space, and their evolution is governed by C-values computed from noncommutative operators, thereby ensuring the coexistence and expansion of multiple future semantic possibilities. In this study, we implement AQI by extending a transformer-based LLM with more than 600 specialized operators. We evaluate the resulting system on creative reasoning benchmarks spanning ten domains under an LLM-as-a-judge protocol. The results show that AQI consistently outperforms strong baseline models, yielding statistically significant improvements and reduced cross-domain variance. These findings demonstrate that noncommutative algebraic dynamics can serve as a practical and reproducible foundation for machine creativity. Notably, this architecture has already been deployed in real-world enterprise environments.

</details>


### [404] [Process-Supervised Multi-Agent Reinforcement Learning for Reliable Clinical Reasoning](https://arxiv.org/abs/2602.14160)
*Chaeeun Lee,T. Michael Yates,Pasquale Minervini,T. Ian Simpson*

Main category: cs.AI

TL;DR: 提出基于工具代理的强化学习框架，用于基因-疾病有效性评估任务，通过过程级监督确保推理符合临床标准，同时使用分层多智能体系统实现高效协调。


<details>
  <summary>Details</summary>
Motivation: 临床决策需要基于异质证据进行细致推理并提供可追溯的论证。现有LLM多智能体系统主要优化结果准确性，而忽视了符合临床标准的过程基础推理。基因-疾病有效性评估是一个关键的实际案例，专家需要综合多种生物医学证据来确定基因是否与疾病有因果关系。

Method: 引入工具代理强化学习框架，包含两个目标：(1)过程级监督确保推理遵循有效的临床路径；(2)通过分层多智能体系统实现高效协调。使用GRPO训练的Qwen3-4B作为监督智能体，在ClinGen数据集上进行评估。

Result: 仅使用结果奖励时，MAS系统将最终结果准确率从基础模型的0.195提升到0.732，但过程对齐性较差（0.392 F1）。结合过程和结果奖励时，MAS系统达到更高的结果准确率（0.750），同时显著提高过程保真度至0.520 F1。

Conclusion: 提出的工具代理强化学习框架在基因-疾病有效性评估任务中同时提高了结果准确性和过程对齐性，证明了过程级监督在临床推理系统中的重要性。

Abstract: Clinical decision-making requires nuanced reasoning over heterogeneous evidence and traceable justifications. While recent LLM multi-agent systems (MAS) show promise, they largely optimise for outcome accuracy while overlooking process-grounded reasoning aligned with clinical standards. One critical real-world case of this is gene-disease validity curation, where experts must determine whether a gene is causally implicated in a disease by synthesising diverse biomedical evidence. We introduce an agent-as-tool reinforcement learning framework for this task with two objectives: (i) process-level supervision to ensure reasoning follows valid clinical pathways, and (ii) efficient coordination via a hierarchical multi-agent system. Our evaluation on the ClinGen dataset shows that with outcome-only rewards, MAS with a GRPO-trained Qwen3-4B supervisor agent substantially improves final outcome accuracy from 0.195 with a base model supervisor to 0.732, but results in poor process alignment (0.392 F1). Conversely, with process + outcome rewards, MAS with GRPO-trained supervisor achieves higher outcome accuracy (0.750) while significantly improving process fidelity to 0.520 F1. Our code is available at https://github.com/chaeeunlee-io/GeneDiseaseCurationAgents.

</details>


### [405] [Text Before Vision: Staged Knowledge Injection Matters for Agentic RLVR in Ultra-High-Resolution Remote Sensing Understanding](https://arxiv.org/abs/2602.14225)
*Fengxiang Wang,Mingshuo Chen,Yueying Li,Yajie Yang,Yuhao Zhou,Di Wang,Yifan Zhang,Haoyu Wang,Haiyan Zhao,Hongda Sun,Long Lan,Jun Song,Yulin Wang,Jing Zhang,Wenlong Zhang,Bo Du*

Main category: cs.AI

TL;DR: 在超高分辨率遥感多模态推理中，研究发现高质量的地球科学文本问答是视觉推理能力提升的主要驱动力，而非传统强化学习。提出分阶段知识注入方法，在XLRS-Bench上达到60.40% Pass@1的新SOTA。


<details>
  <summary>Details</summary>
Motivation: 超高分辨率遥感多模态推理面临视觉证据获取的瓶颈：需要在海量像素空间中定位微小的任务相关区域。虽然基于缩放工具的强化学习提供了一条路径，但标准强化学习在没有结构化领域先验的情况下难以导航这些广阔的视觉空间。

Method: 提出分阶段知识注入方法：1) 使用可扩展、知识图验证的地球科学文本问答进行冷启动，注入推理结构；2) 在监督微调阶段对相同的困难超高分辨率图像-文本示例进行"预热"，以稳定和增强后续基于工具的强化学习。

Result: 在XLRS-Bench上达到60.40% Pass@1，显著优于更大的通用模型（如GPT-5.2、Gemini 3.0 Pro、Intern-S1），建立了新的最先进水平。

Conclusion: 高质量的地球科学文本问答是超高分辨率视觉推理能力提升的主要驱动力，通过分阶段的知识注入方法可以显著提升模型性能，这为多模态推理提供了新的视角。

Abstract: Multimodal reasoning for ultra-high-resolution (UHR) remote sensing (RS) is usually bottlenecked by visual evidence acquisition: the model necessitates localizing tiny task-relevant regions in massive pixel spaces. While Agentic Reinforcement Learning with Verifiable Rewards (RLVR) using zoom-in tools offers a path forward, we find that standard reinforcement learning struggles to navigate these vast visual spaces without structured domain priors. In this paper, we investigate the interplay between post-training paradigms: comparing Cold-start Supervised Fine-Tuning (SFT), RLVR, and Agentic RLVR on the UHR RS benchmark.Our controlled studies yield a counter-intuitive finding: high-quality Earth-science text-only QA is a primary driver of UHR visual reasoning gains. Despite lacking images, domain-specific text injects the concepts, mechanistic explanations, and decision rules necessary to guide visual evidence retrieval.Based on this, we propose a staged knowledge injection recipe: (1) cold-starting with scalable, knowledge-graph-verified Earth-science text QA to instill reasoning structures;and (2) "pre-warming" on the same hard UHR image-text examples during SFT to stabilize and amplify subsequent tool-based RL. This approach achieves a 60.40% Pass@1 on XLRS-Bench, significantly outperforming larger general purpose models (e.g., GPT-5.2, Gemini 3.0 Pro, Intern-S1) and establishing a new state-of-the-art.

</details>


### [406] [CORPGEN: Simulating Corporate Environments with Autonomous Digital Employees in Multi-Horizon Task Environments](https://arxiv.org/abs/2602.14229)
*Abubakarr Jaye,Nigel Boachie Kumankumah,Chidera Biringa,Anjel Shaileshbhai Patel,Sulaiman Vesal,Dayquan Julienne,Charlotte Siska,Manuel Raúl Meléndez Luján,Anthony Twum-Barimah,Mauricio Velazco,Tianwei Chen*

Main category: cs.AI

TL;DR: 论文提出Multi-Horizon Task Environments (MHTEs)问题类别，针对多任务并发长时程推理挑战，开发CorpGen框架解决四大失效模式，在模拟企业环境中实现3.5倍性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试只评估智能体在单个隔离任务上的表现，而实际组织工作需要管理多个并发的长时程任务，涉及任务交错、依赖关系和优先级重排。需要新的评估框架来应对这些现实挑战。

Method: 提出CorpGen架构无关框架，采用分层规划实现多时程目标对齐、子智能体隔离防止任务交叉污染、分层内存（工作、结构化、语义）和自适应摘要。通过具有持久身份和现实日程的数字员工模拟企业环境。

Result: 在OSWorld Office上使用三种CUA后端测试，CorpGen相比基线实现最高3.5倍改进（15.2% vs 4.3%），在负载增加时保持稳定性能。消融研究表明经验学习贡献最大收益。

Conclusion: CorpGen框架有效解决了多时程任务环境中的四大失效模式，性能提升源于架构机制而非特定CUA实现，为复杂组织环境中的自主智能体提供了实用解决方案。

Abstract: Long-horizon reasoning is a key challenge for autonomous agents, yet existing benchmarks evaluate agents on single tasks in isolation. Real organizational work requires managing many concurrent long-horizon tasks with interleaving, dependencies, and reprioritization. We introduce Multi-Horizon Task Environments (MHTEs): a distinct problem class requiring coherent execution across dozens of interleaved tasks (45+, 500-1500+ steps) within persistent execution contexts spanning hours. We identify four failure modes that cause baseline CUAs to degrade from 16.7% to 8.7% completion as load scales 25% to 100%, a pattern consistent across three independent implementations. These failure modes are context saturation (O(N) vs O(1) growth), memory interference, dependency complexity (DAGs vs. chains), and reprioritization overhead. We present CorpGen, an architecture-agnostic framework addressing these failures via hierarchical planning for multi-horizon goal alignment, sub-agent isolation preventing cross-task contamination, tiered memory (working, structured, semantic), and adaptive summarization. CorpGen simulates corporate environments through digital employees with persistent identities and realistic schedules. Across three CUA backends (UFO2, OpenAI CUA, hierarchical) on OSWorld Office, CorpGen achieves up to 3.5x improvement over baselines (15.2% vs 4.3%) with stable performance under increasing load, confirming that gains stem from architectural mechanisms rather than specific CUA implementations. Ablation studies show experiential learning provides the largest gains.

</details>


### [407] [REDSearcher: A Scalable and Cost-Efficient Framework for Long-Horizon Search Agents](https://arxiv.org/abs/2602.14234)
*Zheng Chu,Xiao Wang,Jack Hong,Huiming Fan,Yuqi Huang,Yue Yang,Guohai Xu,Chenxiao Zhao,Cheng Xiang,Shengchao Hu,Dongdong Kuang,Ming Liu,Bing Qin,Xing Yu*

Main category: cs.AI

TL;DR: REDSearcher是一个统一框架，通过协同设计复杂任务合成、中期训练和后期训练，解决大语言模型在深度搜索任务中高质量轨迹稀疏和奖励信号不足的问题，在文本和多模态搜索基准上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型正从通用知识引擎转向现实问题解决器，但优化其深度搜索任务仍具挑战。主要瓶颈在于高质量搜索轨迹和奖励信号的极端稀疏性，这源于可扩展的长时程任务构建困难以及涉及外部工具调用的高成本交互密集型rollout。

Method: 提出REDSearcher统一框架，包含四个关键改进：1) 将任务合成构建为双约束优化问题，通过图拓扑和证据分散精确控制任务难度；2) 引入工具增强查询以鼓励主动工具使用而非被动回忆；3) 中期训练中强化核心原子能力（知识、规划和函数调用）；4) 构建本地模拟环境实现快速低成本的强化学习算法迭代。

Result: 在文本和多模态搜索代理基准测试中，该方法达到了最先进的性能。同时将发布10K高质量复杂文本搜索轨迹、5K多模态轨迹和1K文本RL查询集，以及代码和模型检查点。

Conclusion: REDSearcher通过系统性的任务合成、能力强化和低成本环境构建，有效解决了大语言模型在深度搜索任务优化中的关键瓶颈，为长时程搜索代理的未来研究提供了有价值的资源和框架。

Abstract: Large language models are transitioning from generalpurpose knowledge engines to realworld problem solvers, yet optimizing them for deep search tasks remains challenging. The central bottleneck lies in the extreme sparsity of highquality search trajectories and reward signals, arising from the difficulty of scalable longhorizon task construction and the high cost of interactionheavy rollouts involving external tool calls. To address these challenges, we propose REDSearcher, a unified framework that codesigns complex task synthesis, midtraining, and posttraining for scalable searchagent optimization. Specifically, REDSearcher introduces the following improvements: (1) We frame task synthesis as a dualconstrained optimization, where task difficulty is precisely governed by graph topology and evidence dispersion, allowing scalable generation of complex, highquality tasks. (2) We introduce toolaugmented queries to encourage proactive tool use rather than passive recall.(3) During midtraining, we strengthen core atomic capabilities knowledge, planning, and function calling substantially reducing the cost of collecting highquality trajectories for downstream training. (4) We build a local simulated environment that enables rapid, lowcost algorithmic iteration for reinforcement learning experiments. Across both textonly and multimodal searchagent benchmarks, our approach achieves stateoftheart performance. To facilitate future research on longhorizon search agents, we will release 10K highquality complex text search trajectories, 5K multimodal trajectories and 1K text RL query set, and together with code and model checkpoints.

</details>


### [408] [GRAIL: Goal Recognition Alignment through Imitation Learning](https://arxiv.org/abs/2602.14252)
*Osher Elhadad,Felipe Meneguzzi,Reuth Mirsky*

Main category: cs.AI

TL;DR: GRAIL使用模仿学习和逆强化学习从（可能次优的）演示轨迹中学习每个候选目标的目标导向策略，实现一次性推理的目标识别，在次优、系统性偏差和噪声环境中显著提升识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有目标识别方法通常依赖最优目标导向策略表示，但这可能与行为者的真实行为不同，阻碍准确识别其目标。需要解决这一差距以更好地对齐AI系统与人类意图。

Method: GRAIL结合模仿学习和逆强化学习，直接从（可能次优的）演示轨迹中为每个候选目标学习一个目标导向策略。通过单次前向传递对观察到的部分轨迹进行评分，保留经典目标识别的一次性推理能力。

Result: 在评估的领域中，GRAIL在系统性偏差最优行为下F1分数提升超过0.5，在次优行为下提升约0.1-0.3，在噪声最优轨迹下提升高达0.4，同时在完全最优设置下保持竞争力。

Conclusion: GRAIL通过从演示中学习目标导向策略，能够捕捉次优和系统性偏差行为，为在不确定环境中解释智能体目标提供了可扩展且鲁棒的模型。

Abstract: Understanding an agent's goals from its behavior is fundamental to aligning AI systems with human intentions. Existing goal recognition methods typically rely on an optimal goal-oriented policy representation, which may differ from the actor's true behavior and hinder the accurate recognition of their goal. To address this gap, this paper introduces Goal Recognition Alignment through Imitation Learning (GRAIL), which leverages imitation learning and inverse reinforcement learning to learn one goal-directed policy for each candidate goal directly from (potentially suboptimal) demonstration trajectories. By scoring an observed partial trajectory with each learned goal-directed policy in a single forward pass, GRAIL retains the one-shot inference capability of classical goal recognition while leveraging learned policies that can capture suboptimal and systematically biased behavior. Across the evaluated domains, GRAIL increases the F1-score by more than 0.5 under systematically biased optimal behavior, achieves gains of approximately 0.1-0.3 under suboptimal behavior, and yields improvements of up to 0.4 under noisy optimal trajectories, while remaining competitive in fully optimal settings. This work contributes toward scalable and robust models for interpreting agent goals in uncertain environments.

</details>


### [409] [AutoWebWorld: Synthesizing Infinite Verifiable Web Environments via Finite State Machines](https://arxiv.org/abs/2602.14296)
*Yifan Wu,Yiran Peng,Yiyu Chen,Jianhao Ruan,Zijie Zhuang,Cheng Yang,Jiayi Zhang,Man Chen,Yenchi Tseng,Zhaoyang Yu,Liang Chen,Yuyao Zhai,Bang Liu,Chenglin Wu,Yuyu Luo*

Main category: cs.AI

TL;DR: AutoWebWorld：通过将网页建模为有限状态机并用代码代理生成交互网站，实现可控可验证的网页环境合成，显著降低轨迹收集成本并提升GUI代理性能


<details>
  <summary>Details</summary>
Motivation: 现有自主网页GUI代理的训练数据收集存在瓶颈：从真实网站收集交互轨迹成本高昂且难以验证，状态转换隐含导致依赖不一致的外部验证器评估步骤正确性

Method: 提出AutoWebWorld框架，将网页环境建模为有限状态机（FSM），使用代码代理将FSM转换为交互式网站。明确所有状态、动作和转换规则，实现程序化验证：动作正确性根据预定义规则检查，任务成功通过到达FSM图中的目标状态确认

Result: 实现了完全自动化的搜索-验证流程，以每条轨迹仅0.04美元的成本从29个多样化网页环境生成11,663条已验证轨迹。使用合成数据训练的7B网页GUI代理在WebVoyager上15步内超越所有基线，且在WebVoyager和Online-Mind2Web上观察到清晰的缩放定律：随着合成数据量增加，性能持续提升

Conclusion: AutoWebWorld通过合成可控可验证的网页环境，解决了网页GUI代理训练数据收集的瓶颈问题，显著降低了数据收集成本并提升了代理在真实网页任务上的性能，展示了合成数据在网页交互任务中的有效性

Abstract: The performance of autonomous Web GUI agents heavily relies on the quality and quantity of their training data. However, a fundamental bottleneck persists: collecting interaction trajectories from real-world websites is expensive and difficult to verify. The underlying state transitions are hidden, leading to reliance on inconsistent and costly external verifiers to evaluate step-level correctness. To address this, we propose AutoWebWorld, a novel framework for synthesizing controllable and verifiable web environments by modeling them as Finite State Machines (FSMs) and use coding agents to translate FSMs into interactive websites. Unlike real websites, where state transitions are implicit, AutoWebWorld explicitly defines all states, actions, and transition rules. This enables programmatic verification: action correctness is checked against predefined rules, and task success is confirmed by reaching a goal state in the FSM graph. AutoWebWorld enables a fully automated search-and-verify pipeline, generating over 11,663 verified trajectories from 29 diverse web environments at only $0.04 per trajectory. Training on this synthetic data significantly boosts real-world performance. Our 7B Web GUI agent outperforms all baselines within 15 steps on WebVoyager. Furthermore, we observe a clear scaling law: as the synthetic data volume increases, performance on WebVoyager and Online-Mind2Web consistently improves.

</details>


### [410] [Benchmarking at the Edge of Comprehension](https://arxiv.org/abs/2602.14307)
*Samuele Marro,Jialin Yu,Emanuele La Malfa,Oishi Deb,Jiawei Li,Yibo Yang,Ebey Abraham,Sunando Sengupta,Eric Sommerlade,Michael Wooldridge,Philip Torr*

Main category: cs.AI

TL;DR: 提出"批判抗性基准测试"框架，通过对抗性生成-评估游戏来比较模型，即使人类无法完全理解任务也能保持评估完整性。


<details>
  <summary>Details</summary>
Motivation: 随着前沿大语言模型快速饱和新基准测试，传统基准测试面临危机：人类难以生成区分性任务、提供准确答案或评估复杂解决方案。如果基准测试变得不可行，我们将失去衡量AI进展的能力。

Method: 提出批判抗性基准测试框架，基于"批判抗性正确性"概念：答案被认为是正确的，如果没有对手能令人信服地证明其错误。人类作为有限验证者，专注于局部主张。使用项目化二分Bradley-Terry模型联合排名LLMs的解题能力和生成难题能力。

Result: 在数学领域对八个前沿LLMs进行测试，结果显示所得分数稳定且与外部能力测量相关。框架将基准测试重新定义为对抗性生成-评估游戏。

Conclusion: 提出了一种新的基准测试范式，通过对抗性框架和批判抗性正确性概念，使人类能够在无法完全理解任务的情况下仍能有效评估模型性能，解决了后理解时代的基准测试挑战。

Abstract: As frontier Large Language Models (LLMs) increasingly saturate new benchmarks shortly after they are published, benchmarking itself is at a juncture: if frontier models keep improving, it will become increasingly hard for humans to generate discriminative tasks, provide accurate ground-truth answers, or evaluate complex solutions. If benchmarking becomes infeasible, our ability to measure any progress in AI is at stake. We refer to this scenario as the post-comprehension regime. In this work, we propose Critique-Resilient Benchmarking, an adversarial framework designed to compare models even when full human understanding is infeasible. Our technique relies on the notion of critique-resilient correctness: an answer is deemed correct if no adversary has convincingly proved otherwise. Unlike standard benchmarking, humans serve as bounded verifiers and focus on localized claims, which preserves evaluation integrity beyond full comprehension of the task. Using an itemized bipartite Bradley-Terry model, we jointly rank LLMs by their ability to solve challenging tasks and to generate difficult yet solvable questions. We showcase the effectiveness of our method in the mathematical domain across eight frontier LLMs, showing that the resulting scores are stable and correlate with external capability measures. Our framework reformulates benchmarking as an adversarial generation-evaluation game in which humans serve as final adjudicators.

</details>


### [411] [Reshaping MOFs text mining with a dynamic multi-agents framework of large language model](https://arxiv.org/abs/2504.18880)
*Zuhong Lin,Daoyuan Ren,Kai Ran,Jing Sun,Songlin Yu,Xuefeng Bai,Xiaotian Huang,Haiyang He,Pengxu Pan,Ying Fang,Zhanglin Li,Haipu Li,Jingjing Yao*

Main category: cs.AI

TL;DR: MOFh6是一个基于大语言模型的系统，能够从原始文献或晶体代码中自动提取金属有机框架（MOFs）的合成条件，并将其转换为标准化的合成表格，准确率达99%，处理速度快且成本低。


<details>
  <summary>Details</summary>
Motivation: MOFs合成条件信息在文献中分散、不一致且难以解释，传统数据库查询方式无法实时获取最新信息，需要一种自动化系统来高效提取和标准化这些关键数据。

Method: 开发基于大语言模型的MOFh6系统，能够读取原始文章或晶体代码，跨段落链接相关描述，统一配体缩写与全名，输出结构化参数，实现实时信息提取。

Result: MOFh6达到99%的提取准确率，在五大出版商中解决了94.1%的缩写问题，精度为0.93±0.01，处理全文需9.6秒，定位合成描述需36秒，处理100篇论文仅需4.24美元。

Conclusion: MOFh6通过实时提取替代静态数据库查询，重塑了MOF合成研究，加速了文献知识向实用合成方案的转化，实现了可扩展的数据驱动材料发现。

Abstract: Accurately identifying the synthesis conditions of metal-organic frameworks (MOFs) is essential for guiding experimental design, yet remains challenging because relevant information in the literature is often scattered, inconsistent, and difficult to interpret. We present MOFh6, a large language model driven system that reads raw articles or crystal codes and converts them into standardized synthesis tables. It links related descriptions across paragraphs, unifies ligand abbreviations with full names, and outputs structured parameters ready for use. MOFh6 achieved 99% extraction accuracy, resolved 94.1% of abbreviation cases across five major publishers, and maintained a precision of 0.93 +/- 0.01. Processing a full text takes 9.6 s, locating synthesis descriptions 36 s, with 100 papers processed for USD 4.24. By replacing static database lookups with real-time extraction, MOFh6 reshapes MOF synthesis research, accelerating the conversion of literature knowledge into practical synthesis protocols and enabling scalable, data-driven materials discovery.

</details>


### [412] [Competition for attention predicts good-to-bad tipping in AI](https://arxiv.org/abs/2602.14370)
*Neil F. Johnson,Frank Y. Huo*

Main category: cs.AI

TL;DR: 论文提出边缘AI设备中危险行为涌现的数学机制，通过注意力竞争模型预测临界点n*，为跨领域安全控制提供新方法


<details>
  <summary>Details</summary>
Motivation: 超过半数全球人口携带可离线运行类ChatGPT模型的设备，这些设备缺乏安全监管，可能引发自残、经济损失、极端主义等危险。现有安全工具需要云端连接或只能在伤害发生后发现问题。

Method: 研究发现危险行为的涌现源于原子尺度上的注意力竞争机制，提出了一个数学公式n*来描述临界点，该公式基于对话上下文与竞争输出盆地之间的点积注意力竞争。

Result: 该机制在多个AI模型中得到验证，可针对不同"好"与"坏"的定义进行实例化，适用于健康、法律、金融、国防等多个领域，以及不同法律环境、语言和文化背景。

Conclusion: 研究揭示了边缘AI设备中危险行为涌现的数学机制，为跨领域安全控制提供了新的理论框架和实用工具，能够在不依赖云端连接的情况下预防潜在危害。

Abstract: More than half the global population now carries devices that can run ChatGPT-like language models with no Internet connection and minimal safety oversight -- and hence the potential to promote self-harm, financial losses and extremism among other dangers. Existing safety tools either require cloud connectivity or discover failures only after harm has occurred. Here we show that a large class of potentially dangerous tipping originates at the atomistic scale in such edge AI due to competition for the machinery's attention. This yields a mathematical formula for the dynamical tipping point n*, governed by dot-product competition for attention between the conversation's context and competing output basins, that reveals new control levers. Validated against multiple AI models, the mechanism can be instantiated for different definitions of 'good' and 'bad' and hence in principle applies across domains (e.g. health, law, finance, defense), changing legal landscapes (e.g. EU, UK, US and state level), languages, and cultural settings.

</details>


### [413] [Boule or Baguette? A Study on Task Topology, Length Generalization, and the Benefit of Reasoning Traces](https://arxiv.org/abs/2602.14404)
*William L. Tong,Ege Cakar,Cengiz Pehlevan*

Main category: cs.AI

TL;DR: 该论文提出了PITA数据集来研究推理模型中的长度泛化问题，发现推理轨迹模型在广泛浅层任务上表现良好，但在狭窄深层任务上表现较差。


<details>
  <summary>Details</summary>
Motivation: 尽管推理模型（生成中间推理轨迹的神经网络）近年来快速发展，但我们对推理轨迹如何支持推理以及这种范式的局限性理解仍然不完整。需要更清晰地理解推理模型的泛化能力，特别是在长度泛化方面。

Method: 引入PITA数据集（包含2300万条命题逻辑语句及其对应证明），提出任务深度和任务广度的概念来衡量推理难度，通过在不同深度和广度的子集上测试推理轨迹模型的性能，并与简单三段论合成任务进行比较。

Result: 推理轨迹模型在广泛且浅层的子集上泛化良好，但在狭窄且深层的子集上表现较差（相对于非推理轨迹基线）。研究结果表明推理模型在深层任务中存在基本限制，但在广泛任务中具有泛化优势。

Conclusion: 该研究揭示了使用推理轨迹的基本优势和局限性：推理轨迹模型擅长处理广泛但浅层的任务，但在需要深层推理的狭窄任务上存在根本性限制。这些发现有助于更全面地理解推理模型的泛化能力。

Abstract: Recent years have witnessed meteoric progress in reasoning models: neural networks that generate intermediate reasoning traces (RTs) before producing a final output. Despite the rapid advancement, our understanding of how RTs support reasoning, and the limits of this paradigm, remain incomplete. To promote greater clarity, we introduce PITA: a novel large-scale dataset of over 23 million statements in propositional logic and their corresponding proofs. As a benchmark for robust reasoning, we focus on length generalization: if a model is trained to determine truth or falsity on statements with proofs up to fixed length, how well does it generalize to statements requiring longer proofs? We propose notions of (1) task depth and (2) task breadth, which measure respectively (1) the number of steps required to solve an example from a task and (2) the number of unique examples across a task. We vary these quantities across subsets of PITA, and find that RT models generalize well on broad and shallow subsets, while deteriorating on narrow and deep subsets relative to non-RT baselines. To determine whether our results are idiosyncratic to PITA or indicative of general phenomena, we compare our results to a simple synthetic task based on syllogisms. Our resulting theory suggests fundamental scalings that limit how well RT models perform on deep tasks, and highlights their generalization strengths on broad tasks. Our findings overall identify fundamental benefits and limitations inherent in using reasoning traces.

</details>


### [414] [Precedent-Informed Reasoning: Mitigating Overthinking in Large Reasoning Models via Test-Time Precedent Learning](https://arxiv.org/abs/2602.14451)
*Qianyue Wang,Jinwu Hu,Huanxiang Lin,Bolin Chen,Zhiquan Wen,Yaofo Chen,Yu Rong,Mingkui Tan*

Main category: cs.AI

TL;DR: PIR通过自适应选择先例和测试时经验内化，将LLM推理从自我探索转变为从先例中学习，缩短推理轨迹同时保持或提高准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的推理通常存在低效的长链思维轨迹，包含冗余的自我探索和验证，这会增加计算成本甚至降低性能。受人类推理模式的启发，人们通过利用过去的类似案例来约束搜索空间、减少试错来解决新问题。

Method: 提出先例知情推理(PIR)，包含两个关键组件：1) 自适应先例选择(APS)：为每个问题和LLM构建紧凑的先例集，基于语义相似度和模型困惑度联合评分，自适应调整先例数量以最大化困惑度降低；2) 测试时经验内化(TEI)：在测试时对先例知情指令进行学习，更新轻量级适配器以内部化解题模式，在后续推理中将其作为先验使用。

Result: 在数学推理、科学问答和代码生成等任务上的实验表明，PIR能够一致地缩短推理轨迹，同时保持或提高最终准确性，在LLMs上实现了出色的准确性-效率权衡。

Conclusion: PIR将LLM推理范式从详尽的自我探索转变为从先例中学习，通过自适应选择相关先例和测试时经验内化，实现了更高效的推理过程，在保持性能的同时显著减少了计算开销。

Abstract: Reasoning in Large Language Models (LLMs) often suffers from inefficient long chain-of-thought traces with redundant self-exploration and validation, which inflate computational costs and even degrade performance. Inspired by human reasoning patterns where people solve new problems by leveraging past related cases to constrain search spaces and reduce trial-and-error, we propose Precedent Informed Reasoning (PIR) transforming LRMs'reasoning paradigm from exhaustive self-exploration to guided learning from precedents. PIR addresses two key challenges: what precedents to adopt and how to utilize them. First, Adaptive Precedent Selection (APS) constructs, for each question and LRM, a compact set of precedents that are both semantically related and informative for the model. It ranks examples by a joint score with semantic similarity and model perplexity, then adapts the amount of precedents to maximize perplexity reduction. Second, Test-time Experience Internalization (TEI) is treated as the test-time learning on precedent-informed instruction, updating lightweight adapters to internalize solution patterns and use them as a prior during subsequent reasoning. Experiments across mathematical reasoning, scientific QA, and code generation demonstrate that PIR consistently shortens reasoning traces while maintaining or improving final accuracy across LLMs, yielding outstanding accuracy-efficiency trade-offs.

</details>


### [415] [Bounding Probabilities of Causation with Partial Causal Diagrams](https://arxiv.org/abs/2602.14503)
*Yuxuan Xie,Ang Li*

Main category: cs.AI

TL;DR: 提出一个利用部分因果信息来界定因果概率的通用框架，通过优化编程将可用信息作为约束，在不完全可识别的情况下获得更紧且形式有效的界限。


<details>
  <summary>Details</summary>
Motivation: 因果概率对于个体层面的解释和决策至关重要，但它们本质上是反事实的，通常无法从数据中精确识别。现有界限要么忽略可用协变量，要么需要完整的因果图，要么依赖限制性的二元设置，限制了实际应用。现实应用中，因果信息往往是部分但非平凡的。

Method: 提出一个通用框架，将可用的结构或统计信息系统地作为约束条件纳入优化编程公式中，在不完全可识别的情况下获得更紧且形式有效的界限。

Result: 该框架能够将部分因果信息转化为对因果概率的界限，扩展了因果概率在实际不完全但信息丰富的因果知识场景下的适用性。

Conclusion: 通过将部分因果信息作为优化约束，可以在不完全可识别的情况下获得更紧的因果概率界限，从而将因果概率的适用范围扩展到现实世界中因果知识不完整但信息丰富的场景。

Abstract: Probabilities of causation are fundamental to individual-level explanation and decision making, yet they are inherently counterfactual and not point-identifiable from data in general. Existing bounds either disregard available covariates, require complete causal graphs, or rely on restrictive binary settings, limiting their practical use. In real-world applications, causal information is often partial but nontrivial. This paper proposes a general framework for bounding probabilities of causation using partial causal information. We show how the available structural or statistical information can be systematically incorporated as constraints in a optimization programming formulation, yielding tighter and formally valid bounds without full identifiability. This approach extends the applicability of probabilities of causation to realistic settings where causal knowledge is incomplete but informative.

</details>


### [416] [Formally Verifying and Explaining Sepsis Treatment Policies with COOL-MC](https://arxiv.org/abs/2602.14505)
*Dennis Gross*

Main category: cs.AI

TL;DR: COOL-MC 是一个结合形式化验证与可解释性的工具，用于分析强化学习在脓毒症治疗中的决策策略，通过构建可达状态空间、临床标签和解释性方法，揭示策略决策依据。


<details>
  <summary>Details</summary>
Motivation: 医疗领域的强化学习策略（如脓毒症治疗优化）通常不透明且难以验证，传统模型检查器无法处理大规模状态空间，也缺乏解释策略决策原因的能力。

Method: COOL-MC 包装了模型检查器 Storm，但增加了三个关键功能：1) 仅构建训练策略诱导的可达状态空间，形成可验证的离散时间马尔可夫链；2) 自动用临床有意义的原子命题标记状态；3) 将可解释性方法与概率计算树逻辑查询集成，揭示决策特征。

Result: 在基于17,000名脓毒症患者记录的ICU-Sepsis MDP基准测试中，COOL-MC实现了完整MDP验证的硬边界，训练出达到最优生存概率的安全RL策略，并通过PCTL验证和可解释性分析发现策略主要依赖既往用药史而非患者当前状况。

Conclusion: COOL-MC展示了形式化验证与可解释性结合的价值，能够揭示标准评估无法发现的策略弱点，可作为临床医生在部署前调查和调试脓毒症治疗策略的工具。

Abstract: Safe and interpretable sequential decision-making is critical in healthcare, yet reinforcement learning (RL) policies for sepsis treatment optimization remain opaque and difficult to verify. Standard probabilistic model checkers operate on the full state space, which becomes infeasible for larger MDPs, and cannot explain why a learned policy makes particular decisions. COOL-MC wraps the model checker Storm but adds three key capabilities: it constructs only the reachable state space induced by a trained policy, yielding a smaller discrete-time Markov chain amenable to verification even when full-MDP analysis is intractable; it automatically labels states with clinically meaningful atomic propositions; and it integrates explainability methods with probabilistic computation tree logic (PCTL) queries to reveal which features drive decisions across treatment trajectories. We demonstrate COOL-MC's capabilities on the ICU-Sepsis MDP, a benchmark derived from approximately 17,000 sepsis patient records, which serves as a case study for applying COOL-MC to the formal analysis of sepsis treatment policies. Our analysis establishes hard bounds via full MDP verification, trains a safe RL policy that achieves optimal survival probability, and analyzes its behavior via PCTL verification and explainability on the induced DTMC. This reveals, for instance, that our trained policy relies predominantly on prior dosing history rather than the patient's evolving condition, a weakness that is invisible to standard evaluation but is exposed by COOL-MC's integration of formal verification and explainability. Our results illustrate how COOL-MC could serve as a tool for clinicians to investigate and debug sepsis treatment policies before deployment.

</details>


### [417] [Diagnosing Knowledge Conflict in Multimodal Long-Chain Reasoning](https://arxiv.org/abs/2602.14518)
*Jing Tang,Kun Wang,Haolang Lu,Hongjin Chen,KaiTao Chen,Zhongxiang Sun,Qiankun Li,Lingjuan Lyu,Guoshun Nan,Zhigang Zeng*

Main category: cs.AI

TL;DR: 多模态大语言模型在长链推理中面临知识冲突问题，研究发现冲突特征在模型内部线性可分、集中于中后层，且存在方向不对称性


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在长链推理中，当不同知识源提供冲突信号时经常失败，需要理解这些失败背后的机制

Method: 通过探测内部表示，分析知识冲突在模型中的编码方式，区分输入级客观冲突和过程级有效冲突

Result: 发现四种关键现象：冲突特征线性可分、集中于中后层、通过轨迹聚合可恢复冲突类型、存在方向不对称性（强化模型偏好源比强制相反源更容易）

Conclusion: 研究提供了多模态推理在知识冲突下的机制级视角，为长链推理失败提供了原则性诊断和控制方法

Abstract: Multimodal large language models (MLLMs) in long chain-of-thought reasoning often fail when different knowledge sources provide conflicting signals. We formalize these failures under a unified notion of knowledge conflict, distinguishing input-level objective conflict from process-level effective conflict. Through probing internal representations, we reveal that: (I) Linear Separability: different conflict types are explicitly encoded as linearly separable features rather than entangled; (II) Depth Localization: conflict signals concentrate in mid-to-late layers, indicating a distinct processing stage for conflict encoding; (III) Hierarchical Consistency: aggregating noisy token-level signals along trajectories robustly recovers input-level conflict types; and (IV) Directional Asymmetry: reinforcing the model's implicit source preference under conflict is far easier than enforcing the opposite source. Our findings provide a mechanism-level view of multimodal reasoning under knowledge conflict and enable principled diagnosis and control of long-CoT failures.

</details>


### [418] [Disentangling Deception and Hallucination Failures in LLMs](https://arxiv.org/abs/2602.14529)
*Haolang Lu,Hongrui Peng,WeiYe Fu,Guoshun Nan,Xinye Cao,Xingrui Li,Hongcan Guo,Kun Wang*

Main category: cs.AI

TL;DR: 论文提出从机制角度区分LLM事实查询失败：知识存在与行为表达，构建控制环境分析幻觉与欺骗两种不同失败模式


<details>
  <summary>Details</summary>
Motivation: 传统行为视角将LLM事实回答错误归因于知识缺失，但可能混淆不同失败机制。需要从内部机制角度区分知识存在和行为表达，以理解幻觉和欺骗这两种表面相似但机制不同的失败模式

Method: 构建实体中心事实问题的控制环境，保持知识不变但选择性改变行为表达；通过表示可分性、稀疏可解释性和推理时激活导向分析四种行为案例

Result: 论文建立了区分知识存在与行为表达的分析框架，能够系统分析幻觉和欺骗等不同失败模式，为理解LLM内部机制提供了新视角

Conclusion: LLM事实查询失败需要从机制角度区分知识存在和行为表达，幻觉与欺骗是两种不同的失败模式，控制环境方法有助于深入理解LLM的内部工作机制

Abstract: Failures in large language models (LLMs) are often analyzed from a behavioral perspective, where incorrect outputs in factual question answering are commonly associated with missing knowledge. In this work, focusing on entity-based factual queries, we suggest that such a view may conflate different failure mechanisms, and propose an internal, mechanism-oriented perspective that separates Knowledge Existence from Behavior Expression. Under this formulation, hallucination and deception correspond to two qualitatively different failure modes that may appear similar at the output level but differ in their underlying mechanisms. To study this distinction, we construct a controlled environment for entity-centric factual questions in which knowledge is preserved while behavioral expression is selectively altered, enabling systematic analysis of four behavioral cases. We analyze these failure modes through representation separability, sparse interpretability, and inference-time activation steering.

</details>


### [419] [MATEO: A Multimodal Benchmark for Temporal Reasoning and Planning in LVLMs](https://arxiv.org/abs/2602.14589)
*Gabriel Roccabruna,Olha Khomyn,Giuseppe Riccardi*

Main category: cs.AI

TL;DR: MATEO是一个用于评估大型视觉语言模型时序推理能力的多模态基准，基于专业食谱数据构建，包含步骤图像和时序执行顺序图标注。


<details>
  <summary>Details</summary>
Motivation: 现有研究在基础模型对时序执行顺序的理解方面存在局限：主要依赖自动标注、将时序执行顺序近似为线性链、或仅使用文本输入。需要填补这一空白，评估和改进LVLMs在现实世界规划中所需的时序推理能力。

Method: 1. 收集高质量专业多模态食谱语料库，通过标准化编辑流程将指令分解为离散步骤，每个步骤配有对应图像；2. 设计可扩展的众包流水线收集时序执行顺序图标注；3. 使用MATEO基准评估6个最先进的LVLMs，考虑模型规模、语言上下文、多模态输入结构和微调策略等因素。

Result: 论文介绍了MATEO基准数据集，包含专业食谱的多模态步骤和时序执行顺序图标注。评估了6个不同规模的SOTA LVLMs在时序推理任务上的表现。

Conclusion: MATEO基准填补了LVLMs时序推理能力评估的空白，为改进模型在现实世界规划任务中的表现提供了重要工具和数据集。

Abstract: AI agents need to plan to achieve complex goals that involve orchestrating perception, sub-goal decomposition, and execution. These plans consist of ordered steps structured according to a Temporal Execution Order (TEO, a directed acyclic graph that ensures each step executes only after its preconditions are satisfied. Existing research on foundational models' understanding of temporal execution is limited to automatically derived annotations, approximations of the TEO as a linear chain, or text-only inputs. To address this gap, we introduce MATEO (MultimodAl Temporal Execution Order), a benchmark designed to assess and improve the temporal reasoning abilities of Large Vision Language Models (LVLMs) required for real-world planning. We acquire a high-quality professional multimodal recipe corpus, authored through a standardized editorial process that decomposes instructions into discrete steps, each paired with corresponding images. We collect TEO annotations as graphs by designing and using a scalable crowdsourcing pipeline. Using MATEO, we evaluate six state-of-the-art LVLMs across model scales, varying language context, multimodal input structure, and fine-tuning strategies.

</details>


### [420] [Tabular Foundation Models Can Learn Association Rules](https://arxiv.org/abs/2602.14622)
*Erkan Karabulut,Daniel Daza,Paul Groth,Martijn C. Schut,Victoria Degeler*

Main category: cs.AI

TL;DR: TabProbe：利用表格基础模型作为条件概率估计器，无需频繁项集挖掘即可从表格数据中提取高质量关联规则的新框架


<details>
  <summary>Details</summary>
Motivation: 传统关联规则挖掘方法存在规则爆炸和可扩展性差的问题，而神经方法在低数据场景下性能下降。表格基础模型具有强大的上下文泛化能力，为解决这些限制提供了基础

Method: 提出模型无关的关联规则学习框架，可从任何条件概率模型中提取关联规则。具体实现TabProbe利用表格基础模型作为条件概率估计器，无需频繁项集挖掘即可学习关联规则

Result: 表格基础模型能够持续产生简洁、高质量的关联规则，具有强大的预测性能，在低数据设置下保持鲁棒性，且无需任务特定训练

Conclusion: TabProbe框架成功利用表格基础模型解决了传统关联规则挖掘的局限性，在规则质量和预测性能方面表现出色，特别是在低数据场景下

Abstract: Association Rule Mining (ARM) is a fundamental task for knowledge discovery in tabular data and is widely used in high-stakes decision-making. Classical ARM methods rely on frequent itemset mining, leading to rule explosion and poor scalability, while recent neural approaches mitigate these issues but suffer from degraded performance in low-data regimes. Tabular foundation models (TFMs), pretrained on diverse tabular data with strong in-context generalization, provide a basis for addressing these limitations. We introduce a model-agnostic association rule learning framework that extracts association rules from any conditional probabilistic model over tabular data, enabling us to leverage TFMs. We then introduce TabProbe, an instantiation of our framework that utilizes TFMs as conditional probability estimators to learn association rules out-of-the-box without frequent itemset mining. We evaluate our approach on tabular datasets of varying sizes based on standard ARM rule quality metrics and downstream classification performance. The results show that TFMs consistently produce concise, high-quality association rules with strong predictive performance and remain robust in low-data settings without task-specific training. Source code is available at https://github.com/DiTEC-project/tabprobe.

</details>


### [421] [Arbor: A Framework for Reliable Navigation of Critical Conversation Flows](https://arxiv.org/abs/2602.14643)
*Luís Silva,Diogo Gonçalves,Catarina Farinha,Clara Matos,Luís Ungaro*

Main category: cs.AI

TL;DR: Arbor框架通过将决策树导航分解为专门的节点级任务，解决了大语言模型在结构化工作流中的遵循问题，显著提升了准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在医疗分诊等高风险领域的结构化工作流中难以保持严格遵循，单一提示方法随着提示长度增加会出现指令遵循退化、中间丢失效应和上下文窗口溢出等问题。

Method: Arbor框架将决策树标准化为边列表表示并存储，运行时通过有向无环图编排机制迭代检索当前节点的出边，通过专门的LLM调用评估有效转换，并将响应生成委托给单独的推理步骤。

Result: 在10个基础模型上评估，Arbor将平均轮次准确率提高了29.4个百分点，每轮延迟降低了57.1%，每轮成本平均降低了14.4倍，使较小模型能够匹配或超越单一提示基线下的较大模型。

Conclusion: 架构分解减少了对内在模型能力的依赖，使较小模型能够匹配或超越较大模型在单一提示基线下的表现，为高风险领域的结构化决策提供了有效解决方案。

Abstract: Large language models struggle to maintain strict adherence to structured workflows in high-stakes domains such as healthcare triage. Monolithic approaches that encode entire decision structures within a single prompt are prone to instruction-following degradation as prompt length increases, including lost-in-the-middle effects and context window overflow. To address this gap, we present Arbor, a framework that decomposes decision tree navigation into specialized, node-level tasks. Decision trees are standardized into an edge-list representation and stored for dynamic retrieval. At runtime, a directed acyclic graph (DAG)-based orchestration mechanism iteratively retrieves only the outgoing edges of the current node, evaluates valid transitions via a dedicated LLM call, and delegates response generation to a separate inference step. The framework is agnostic to the underlying decision logic and model provider. Evaluated against single-prompt baselines across 10 foundation models using annotated turns from real clinical triage conversations. Arbor improves mean turn accuracy by 29.4 percentage points, reduces per-turn latency by 57.1%, and achieves an average 14.4x reduction in per-turn cost. These results indicate that architectural decomposition reduces dependence on intrinsic model capability, enabling smaller models to match or exceed larger models operating under single-prompt baselines.

</details>


### [422] [From User Preferences to Base Score Extraction Functions in Gradual Argumentation](https://arxiv.org/abs/2602.14674)
*Aniol Civit,Antonio Rago,Antonio Andriella,Guillem Alenyà,Francesca Toni*

Main category: cs.AI

TL;DR: 提出基分提取函数，将用户对论据的偏好映射为基分，用于量化双极论证框架，简化基分选择过程


<details>
  <summary>Details</summary>
Motivation: 在渐进论证中，论据的基分选择需要专业知识且不直观，而基于偏好组织论据可以简化这一过程

Method: 引入基分提取函数，将双极论证框架中的论据偏好映射为量化双极论证框架的基分，包含算法设计并考虑人类偏好的非线性近似

Result: 在机器人场景中进行理论和实验评估，为实际应用中的渐进语义选择提供建议

Conclusion: 基分提取函数能够有效简化渐进论证中的基分选择过程，使系统更易于使用

Abstract: Gradual argumentation is a field of symbolic AI which is attracting attention for its ability to support transparent and contestable AI systems. It is considered a useful tool in domains such as decision-making, recommendation, debate analysis, and others. The outcomes in such domains are usually dependent on the arguments' base scores, which must be selected carefully. Often, this selection process requires user expertise and may not always be straightforward. On the other hand, organising the arguments by preference could simplify the task. In this work, we introduce \emph{Base Score Extraction Functions}, which provide a mapping from users' preferences over arguments to base scores. These functions can be applied to the arguments of a \emph{Bipolar Argumentation Framework} (BAF), supplemented with preferences, to obtain a \emph{Quantitative Bipolar Argumentation Framework} (QBAF), allowing the use of well-established computational tools in gradual argumentation. We outline the desirable properties of base score extraction functions, discuss some design choices, and provide an algorithm for base score extraction. Our method incorporates an approximation of non-linearities in human preferences to allow for better approximation of the real ones. Finally, we evaluate our approach both theoretically and experimentally in a robotics setting, and offer recommendations for selecting appropriate gradual semantics in practice.

</details>


### [423] [GREAT-EER: Graph Edge Attention Network for Emergency Evacuation Responses](https://arxiv.org/abs/2602.14676)
*Attila Lischka,Balázs Kulcsár*

Main category: cs.AI

TL;DR: 提出基于深度强化学习的公交车疏散路径规划方法，解决城市紧急疏散中的公交车疏散定向问题，能在秒级内生成接近最优的疏散方案。


<details>
  <summary>Details</summary>
Motivation: 城市紧急疏散（恐怖袭击、工业事故、自然灾害）需要高效快速的疏散计划。基于公交车的疏散可以减少纯私家车疏散造成的拥堵和混乱，但现有方法需要更快的规划速度。

Method: 提出公交车疏散定向问题（BEOP），这是一个NP难的组合优化问题。采用基于图学习的深度强化学习方法，训练后能实现快速推理。同时使用MILP公式化来界定疏散计划的性能差距。

Result: 使用旧金山真实道路网络和旅行时间创建疏散场景验证方法，结果显示达到接近最优的解决方案质量，并能够分析在预定义疏散时间内需要多少疏散车辆才能达到特定疏散配额。

Conclusion: 提出的深度强化学习方法能快速生成高效的公交车疏散计划，为城市紧急疏散提供了实用的解决方案，能够在秒级内规划疏散路线，同时保持运行时间合理。

Abstract: Emergency situations that require the evacuation of urban areas can arise from man-made causes (e.g., terrorist attacks or industrial accidents) or natural disasters, the latter becoming more frequent due to climate change. As a result, effective and fast methods to develop evacuation plans are of great importance. In this work, we identify and propose the Bus Evacuation Orienteering Problem (BEOP), an NP-hard combinatorial optimization problem with the goal of evacuating as many people from an affected area by bus in a short, predefined amount of time. The purpose of bus-based evacuation is to reduce congestion and disorder that arises in purely car-focused evacuation scenarios. To solve the BEOP, we propose a deep reinforcement learning-based method utilizing graph learning, which, once trained, achieves fast inference speed and is able to create evacuation routes in fractions of seconds. We can bound the gap of our evacuation plans using an MILP formulation. To validate our method, we create evacuation scenarios for San Francisco using real-world road networks and travel times. We show that we achieve near-optimal solution quality and are further able to investigate how many evacuation vehicles are necessary to achieve certain bus-based evacuation quotas given a predefined evacuation time while keeping run time adequate.

</details>


### [424] [Removing Planner Bias in Goal Recognition Through Multi-Plan Dataset Generation](https://arxiv.org/abs/2602.14691)
*Mustafa F. Abdelwahed,Felipe Meneguzzi Kin Max Piamolini Gusmao,Joan Espasa*

Main category: cs.AI

TL;DR: 提出使用top-k规划生成同一目标的多个不同计划，创建无偏数据集，并引入版本覆盖分数(VCS)评估目标识别器在不同规划器下的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有目标识别数据集存在系统性偏差，因为它们都是由基于启发式前向搜索的规划系统生成的。这种偏差导致数据集缺乏对更现实场景（如不同规划器）的挑战性，影响了目标识别器在不同规划器下的评估效果。

Method: 提出新方法：使用top-k规划为同一目标假设生成多个不同的计划，创建能够缓解当前数据集偏差的基准测试。同时引入新的评估指标——版本覆盖分数(VCS)，用于衡量目标识别器基于不同计划集推断目标时的鲁棒性。

Result: 结果显示，在低可观测性设置下，当前最先进的目标识别器的鲁棒性显著下降。新方法创建的数据集能够更全面地评估目标识别器在不同规划器下的性能。

Conclusion: 通过top-k规划生成多样化的计划集可以创建更无偏的目标识别基准测试，VCS指标能够有效评估目标识别器在不同规划器下的鲁棒性，特别是在低可观测性环境中现有方法的性能会大幅下降。

Abstract: Autonomous agents require some form of goal and plan recognition to interact in multiagent settings. Unfortunately, all existing goal recognition datasets suffer from a systematical bias induced by the planning systems that generated them, namely heuristic-based forward search. This means that existing datasets lack enough challenge for more realistic scenarios (e.g., agents using different planners), which impacts the evaluation of goal recognisers with respect to using different planners for the same goal. In this paper, we propose a new method that uses top-k planning to generate multiple, different, plans for the same goal hypothesis, yielding benchmarks that mitigate the bias found in the current dataset. This allows us to introduce a new metric called Version Coverage Score (VCS) to measure the resilience of the goal recogniser when inferring a goal based on different sets of plans. Our results show that the resilience of the current state-of-the-art goal recogniser degrades substantially under low observability settings.

</details>


### [425] [Evolutionary System Prompt Learning can Facilitate Reinforcement Learning for LLMs](https://arxiv.org/abs/2602.14697)
*Lunjun Zhang,Ryan Chen,Bradly C. Stadie*

Main category: cs.AI

TL;DR: E-SPL提出了一种结合强化学习和系统提示进化的方法，通过并行评估多个系统提示并应用进化更新来联合优化模型上下文和权重。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型主要通过自我反思更新上下文和强化学习更新权重两种机制进行自我改进，但缺乏联合优化这两者的方法。研究者希望开发一种能够同时改进模型上下文和模型权重的自主改进系统。

Method: E-SPL在每次强化学习迭代中并行选择多个系统提示进行rollout，对每个系统提示条件下的模型权重应用RL更新，同时通过LLM驱动的突变和交叉对系统提示种群进行进化更新。每个系统提示都有基于TrueSkill评级的进化选择机制，该评级根据每个RL迭代批次内的相对性能进行更新。

Result: 在从易到难（AIME→BeyondAIME）的泛化设置中，E-SPL将RL成功率从38.8%提升到45.1%，同时优于反射提示进化方法（40.0%）。该方法在推理和代理任务上均表现出改进性能，实现了声明性知识（编码在提示中）和程序性知识（编码在权重中）的自然分离。

Conclusion: 将强化学习与系统提示进化相结合能够在样本效率和泛化能力方面带来一致的增益，为构建能够自主从经验中自我改进的智能体系统提供了有效方法。

Abstract: Building agentic systems that can autonomously self-improve from experience is a longstanding goal of AI. Large language models (LLMs) today primarily self-improve via two mechanisms: self-reflection for context updates, and reinforcement learning (RL) for weight updates. In this work, we propose Evolutionary System Prompt Learning (E-SPL), a method for jointly improving model contexts and model weights. In each RL iteration, E-SPL selects multiple system prompts and runs rollouts with each in parallel. It applies RL updates to model weights conditioned on each system prompt, and evolutionary updates to the system prompt population via LLM-driven mutation and crossover. Each system prompt has a TrueSkill rating for evolutionary selection, updated from relative performance within each RL iteration batch. E-SPL encourages a natural division between declarative knowledge encoded in prompts and procedural knowledge encoded in weights, resulting in improved performance across reasoning and agentic tasks. For instance, in an easy-to-hard (AIME $\rightarrow$ BeyondAIME) generalization setting, E-SPL improves RL success rate from 38.8% $\rightarrow$ 45.1% while also outperforming reflective prompt evolution (40.0%). Overall, our results show that coupling reinforcement learning with system prompt evolution yields consistent gains in sample efficiency and generalization. Code: https://github.com/LunjunZhang/E-SPL

</details>


### [426] [WebWorld: A Large-Scale World Model for Web Agent Training](https://arxiv.org/abs/2602.14721)
*Zikai Xiao,Jianhong Tu,Chuhang Zou,Yuxin Zuo,Zhi Li,Peng Wang,Bowen Yu,Fei Huang,Junyang Lin,Zuozhu Liu*

Main category: cs.AI

TL;DR: WebWorld是首个大规模训练的开源网页模拟器，使用100万+网页交互数据训练，支持30+步长时序模拟，在网页任务上性能接近GPT-4o，并能跨领域泛化到代码、GUI和游戏环境。


<details>
  <summary>Details</summary>
Motivation: 现有网页代理需要大量轨迹数据来泛化，但真实世界训练受到网络延迟、速率限制和安全风险的限制。现有模拟器仅限于封闭环境且只有数千条轨迹，无法满足大规模训练需求。

Method: 采用可扩展的数据流水线，在100万+开源网页交互数据上进行训练，支持推理、多格式数据和30+步长的长时序模拟。引入WebWorld-Bench进行内在评估，包含九个维度的双重指标。

Result: WebWorld的模拟性能与Gemini-3-Pro相当；使用WebWorld合成轨迹训练的Qwen3-14B在WebArena上提升9.2%，性能接近GPT-4o；在推理时搜索方面优于GPT-5作为世界模型；能跨领域泛化到代码、GUI和游戏环境。

Conclusion: WebWorld为世界模型构建提供了可复现的解决方案，突破了现有网页模拟器的规模限制，实现了大规模、长时序的网页模拟，并展示了出色的跨领域泛化能力。

Abstract: Web agents require massive trajectories to generalize, yet real-world training is constrained by network latency, rate limits, and safety risks. We introduce \textbf{WebWorld} series, the first open-web simulator trained at scale. While existing simulators are restricted to closed environments with thousands of trajectories, WebWorld leverages a scalable data pipeline to train on 1M+ open-web interactions, supporting reasoning, multi-format data, and long-horizon simulations of 30+ steps. For intrinsic evaluation, we introduce WebWorld-Bench with dual metrics spanning nine dimensions, where WebWorld achieves simulation performance comparable to Gemini-3-Pro. For extrinsic evaluation, Qwen3-14B trained on WebWorld-synthesized trajectories improves by +9.2\% on WebArena, reaching performance comparable to GPT-4o. WebWorld enables effective inference-time search, outperforming GPT-5 as a world model. Beyond web simulation, WebWorld exhibits cross-domain generalization to code, GUI, and game environments, providing a replicable recipe for world model construction.

</details>


### [427] [Return of the Schema: Building Complete Datasets for Machine Learning and Reasoning on Knowledge Graphs](https://arxiv.org/abs/2602.14795)
*Ivan Diliso,Roberto Barile,Claudia d'Amato,Nicola Fanizzi*

Main category: cs.AI

TL;DR: 提出首个同时包含本体模式与事实的数据集资源，支持机器学习与推理服务


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱精化算法的评估数据集通常只包含事实三元组，缺乏丰富的本体模式信息，这限制了依赖本体约束、推理或神经符号技术的评估，无法反映真实大规模知识图谱场景

Method: 开发了一个工作流程，从源知识图谱中提取同时包含模式与事实的数据集，处理不一致性问题，利用推理推导隐含知识，并将数据集序列化为OWL格式，同时提供加载到张量表示的实用工具

Result: 创建了首个包含模式与事实的完整数据集套件，包括从具有丰富表达模式的知识图谱中提取的新数据集，以及对现有数据集的模式信息增强，所有数据集都支持推理服务和机器学习库

Conclusion: 该资源填补了知识图谱精化评估中模式信息缺失的空白，为依赖本体约束和推理的方法提供了更真实的评估环境，促进了神经符号技术在大规模知识图谱中的应用

Abstract: Datasets for the experimental evaluation of knowledge graph refinement algorithms typically contain only ground facts, retaining very limited schema level knowledge even when such information is available in the source knowledge graphs. This limits the evaluation of methods that rely on rich ontological constraints, reasoning or neurosymbolic techniques and ultimately prevents assessing their performance in large-scale, real-world knowledge graphs. In this paper, we present \resource{} the first resource that provides a workflow for extracting datasets including both schema and ground facts, ready for machine learning and reasoning services, along with the resulting curated suite of datasets. The workflow also handles inconsistencies detected when keeping both schema and facts and also leverage reasoning for entailing implicit knowledge. The suite includes newly extracted datasets from KGs with expressive schemas while simultaneously enriching existing datasets with schema information. Each dataset is serialized in OWL making it ready for reasoning services. Moreover, we provide utilities for loading datasets in tensor representations typical of standard machine learning libraries.

</details>


### [428] [World Models for Policy Refinement in StarCraft II](https://arxiv.org/abs/2602.14857)
*Yixin Zhang,Ziyi Wang,Yiming Rong,Haoxi Wang,Jinling Jiang,Shuang Xu,Haoran Wu,Shiyu Zhou,Bo Xu*

Main category: cs.AI

TL;DR: 提出StarWM——首个用于星际争霸II的世界模型，通过结构化文本表示预测部分可观测环境下的未来状态，并构建决策系统提升智能体性能


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的星际争霸II智能体主要关注策略改进，忽略了将可学习的动作条件转移模型整合到决策循环中。星际争霸II具有巨大的状态-动作空间和部分可观测性，是一个具有挑战性的测试平台。

Method: 1) 提出StarWM世界模型，预测部分可观测环境下的未来观察；2) 引入结构化文本表示，将观察分解为五个语义模块；3) 构建SC2-Dynamics-50k指令调优数据集；4) 开发多维离线评估框架；5) 提出StarWM-Agent决策系统，采用生成-模拟-精炼决策循环

Result: 离线评估显示StarWM相比零样本基线有显著提升：资源预测准确率提高近60%，己方宏观态势一致性改善。在线评估中，StarWM-Agent对抗内置AI在Hard、Harder、VeryHard难度下分别获得30%、15%、30%的胜率提升，同时改善了宏观管理稳定性和战术风险评估。

Conclusion: StarWM成功将世界模型整合到星际争霸II决策系统中，通过结构化表示和预测模型显著提升了智能体性能，证明了世界模型在复杂部分可观测环境中的价值。

Abstract: Large Language Models (LLMs) have recently shown strong reasoning and generalization capabilities, motivating their use as decision-making policies in complex environments. StarCraft II (SC2), with its massive state-action space and partial observability, is a challenging testbed. However, existing LLM-based SC2 agents primarily focus on improving the policy itself and overlook integrating a learnable, action-conditioned transition model into the decision loop. To bridge this gap, we propose StarWM, the first world model for SC2 that predicts future observations under partial observability. To facilitate learning SC2's hybrid dynamics, we introduce a structured textual representation that factorizes observations into five semantic modules, and construct SC2-Dynamics-50k, the first instruction-tuning dataset for SC2 dynamics prediction. We further develop a multi-dimensional offline evaluation framework for predicted structured observations. Offline results show StarWM's substantial gains over zero-shot baselines, including nearly 60% improvements in resource prediction accuracy and self-side macro-situation consistency. Finally, we propose StarWM-Agent, a world-model-augmented decision system that integrates StarWM into a Generate--Simulate--Refine decision loop for foresight-driven policy refinement. Online evaluation against SC2's built-in AI demonstrates consistent improvements, yielding win-rate gains of 30%, 15%, and 30% against Hard (LV5), Harder (LV6), and VeryHard (LV7), respectively, alongside improved macro-management stability and tactical risk assessment.

</details>


### [429] [EmbeWebAgent: Embedding Web Agents into Any Customized UI](https://arxiv.org/abs/2602.14865)
*Chenyang Ma,Clyde Fare,Matthew Wilson,Dave Braines*

Main category: cs.AI

TL;DR: EmbeWebAgent是一个将智能体直接嵌入现有UI的企业级Web代理框架，通过前端钩子和可重用后端工作流实现，支持混合粒度操作和MCP工具编排。


<details>
  <summary>Details</summary>
Motivation: 大多数Web代理在人类界面级别操作，只能观察截图或原始DOM树，缺乏应用级访问权限，这限制了鲁棒性和动作表达能力。在企业环境中，可以同时控制前端和后端，因此需要更强大的代理框架。

Method: 使用轻量级前端钩子（精选的ARIA和URL观察，以及通过WebSocket暴露的每页函数注册表）和可重用的后端工作流，将代理直接嵌入现有UI。该框架与技术栈无关（如React或Angular），支持从GUI原语到高级复合操作的混合粒度动作，并通过MCP工具编排导航、操作和领域特定分析。

Result: 演示显示只需要最小的改造工作量，就能在实时UI环境中实现鲁棒的多步行为。该框架展示了在现有UI中嵌入代理的可行性。

Conclusion: EmbeWebAgent为企业环境提供了一个强大的Web代理框架，通过直接嵌入UI的方式克服了传统代理的限制，实现了更鲁棒和表达能力更强的自动化操作。

Abstract: Most web agents operate at the human interface level, observing screenshots or raw DOM trees without application-level access, which limits robustness and action expressiveness. In enterprise settings, however, explicit control of both the frontend and backend is available. We present EmbeWebAgent, a framework for embedding agents directly into existing UIs using lightweight frontend hooks (curated ARIA and URL-based observations, and a per-page function registry exposed via a WebSocket) and a reusable backend workflow that performs reasoning and takes actions. EmbeWebAgent is stack-agnostic (e.g., React or Angular), supports mixed-granularity actions ranging from GUI primitives to higher-level composites, and orchestrates navigation, manipulation, and domain-specific analytics via MCP tools. Our demo shows minimal retrofitting effort and robust multi-step behaviors grounded in a live UI setting. Live Demo: https://youtu.be/Cy06Ljee1JQ

</details>


### [430] [Lifted Relational Probabilistic Inference via Implicit Learning](https://arxiv.org/abs/2602.14890)
*Luise Ge,Brendan Juba,Kris Nilsson,Alison Shao*

Main category: cs.AI

TL;DR: 提出首个多项式时间框架，通过隐式学习和推理，在无需显式构建模型的情况下，在一阶关系概率逻辑中回答查询。


<details>
  <summary>Details</summary>
Motivation: 解决一阶关系领域中归纳学习与演绎推理之间的长期矛盾，传统方法要么需要完整模型进行提升推理，要么从部分噪声观测中学习模型通常不可行。

Method: 将不完整的一阶公理与独立采样的部分观测示例合并到平方和（SOS）层次的有界度片段中，同时执行两种提升：基础提升（重命名等价的基础矩共享变量）和世界提升（并行执行所有伪模型）。

Result: 开发出首个多项式时间框架，能够隐式学习一阶概率逻辑，并在个体和世界两个层面上执行提升推理。

Conclusion: 通过隐式学习推理方法，成功调和了学习与推理的挑战，为关系概率逻辑中的查询回答提供了高效解决方案。

Abstract: Reconciling the tension between inductive learning and deductive reasoning in first-order relational domains is a longstanding challenge in AI. We study the problem of answering queries in a first-order relational probabilistic logic through a joint effort of learning and reasoning, without ever constructing an explicit model. Traditional lifted inference assumes access to a complete model and exploits symmetry to evaluate probabilistic queries; however, learning such models from partial, noisy observations is intractable in general. We reconcile these two challenges through implicit learning to reason and first-order relational probabilistic inference techniques. More specifically, we merge incomplete first-order axioms with independently sampled, partially observed examples into a bounded-degree fragment of the sum-of-squares (SOS) hierarchy in polynomial time. Our algorithm performs two lifts simultaneously: (i) grounding-lift, where renaming-equivalent ground moments share one variable, collapsing the domain of individuals; and (ii) world-lift, where all pseudo-models (partial world assignments) are enforced in parallel, producing a global bound that holds across all worlds consistent with the learned constraints. These innovations yield the first polynomial-time framework that implicitly learns a first-order probabilistic logic and performs lifted inference over both individuals and worlds.

</details>


### [431] [The Potential of CoT for Reasoning: A Closer Look at Trace Dynamics](https://arxiv.org/abs/2602.14903)
*Gregor Bachmann,Yichen Jiang,Seyed Mohsen Moosavi Dezfooli,Moin Nabi*

Main category: cs.AI

TL;DR: 本文通过引入"潜力"概念分析思维链推理，发现推理过程具有非单调性、洞察性跳跃和幸运猜测等模式，并证明思维链具有可迁移性，仅需20%的强模型思维链就能解锁弱模型性能。


<details>
  <summary>Details</summary>
Motivation: 尽管思维链提示已成为从大语言模型引出推理式响应的标准技术，但其成功背后的驱动机制仍不清楚。本文旨在深入分析竞赛级数学问题中的思维链轨迹，理解思维链的哪些部分如何影响最终答案。

Method: 引入"潜力"概念量化思维链各部分对正确完成可能性的贡献，通过分析推理轨迹识别模式，并研究思维链可迁移性，测量弱模型在强模型部分思维链下的表现。

Result: 发现思维链潜力呈现非单调性（推理偏离）、尖锐但难解释的峰值（推理洞察和跳跃）以及有时仅凭幸运猜测。可迁移性实验表明，仅需20%的强模型部分思维链就能"解锁"弱模型在原本无法解决的问题上的性能。

Conclusion: 思维链推理机制部分可解释且符合人类直觉（如洞察和偏离），但其他行为难以从人类角度理解。思维链具有显著可迁移性，表明其背后机制在很大程度上是可转移的。

Abstract: Chain-of-thought (CoT) prompting is a de-facto standard technique to elicit reasoning-like responses from large language models (LLMs), allowing them to spell out individual steps before giving a final answer. While the resemblance to human-like reasoning is undeniable, the driving forces underpinning the success of CoT reasoning still remain largely unclear. In this work, we perform an in-depth analysis of CoT traces originating from competition-level mathematics questions, with the aim of better understanding how, and which parts of CoT actually contribute to the final answer. To this end, we introduce the notion of a potential, quantifying how much a given part of CoT increases the likelihood of a correct completion. Upon examination of reasoning traces through the lens of the potential, we identify surprising patterns including (1) its often strong non-monotonicity (due to reasoning tangents), (2) very sharp but sometimes tough to interpret spikes (reasoning insights and jumps) as well as (3) at times lucky guesses, where the model arrives at the correct answer without providing any relevant justifications before. While some of the behaviours of the potential are readily interpretable and align with human intuition (such as insights and tangents), others remain difficult to understand from a human perspective. To further quantify the reliance of LLMs on reasoning insights, we investigate the notion of CoT transferability, where we measure the potential of a weaker model under the partial CoT from another, stronger model. Indeed aligning with our previous results, we find that as little as 20% of partial CoT can ``unlock'' the performance of the weaker model on problems that were previously unsolvable for it, highlighting that a large part of the mechanics underpinning CoT are transferable.

</details>


### [432] [Position: Introspective Experience from Conversational Environments as a Path to Better Learning](https://arxiv.org/abs/2602.14910)
*Claudiu Cristian Musat,Jackson Tolins,Diego Antognini,Jingling Li,Martin Klissarov,Tom Duerig*

Main category: cs.AI

TL;DR: 论文提出AI推理能力不应仅依赖规模扩展，而应通过语言自我反思从高质量社交互动中内化产生，强调对话质量是新的数据质量，优化对话支架是下一代通用智能的关键杠杆。


<details>
  <summary>Details</summary>
Motivation: 当前AI训练方法将推理视为规模的涌现属性，但作者认为这种观点存在局限。他们主张借鉴维果茨基发展心理学，强调推理能力应通过语言自我反思从高质量社交互动中内化产生，而非单纯依赖数据规模和计算资源。

Method: 基于维果茨基发展心理学提出三个核心观点：1) 私人思维的社会起源：从对话环境中学习成为理解世界的新方式，与另一个智能体（内部或外部）的对齐摩擦能精炼和结晶推理过程；2) 对话支架式内省体验：使智能体能够进行意义建构，将学习与即时数据流解耦，将原始环境数据转化为丰富的可学习叙事；3) 对话质量即数据质量：智能体私人推理的深度及其在测试时计算效率取决于其掌握的对话的多样性和严谨性。

Result: 论文提出了一个理论框架，强调对话质量而非数据规模是推动AI推理能力发展的关键因素。通过优化对话支架，可以更高效地培养智能体的内省能力和复杂推理能力。

Conclusion: 优化对话支架是下一代通用智能发展的主要杠杆。AI的推理能力应通过高质量社交互动中的语言自我反思来培养，对话质量成为新的数据质量标准，这为AI发展提供了新的理论方向和实践路径。

Abstract: Current approaches to AI training treat reasoning as an emergent property of scale. We argue instead that robust reasoning emerges from linguistic self-reflection, itself internalized from high-quality social interaction. Drawing on Vygotskian developmental psychology, we advance three core positions centered on Introspection. First, we argue for the Social Genesis of the Private Mind: learning from conversational environments rises to prominence as a new way to make sense of the world; the friction of aligning with another agent, internal or not, refines and crystallizes the reasoning process. Second, we argue that dialogically scaffolded introspective experiences allow agents to engage in sense-making that decouples learning from immediate data streams, transforming raw environmental data into rich, learnable narratives. Finally, we contend that Dialogue Quality is the New Data Quality: the depth of an agent's private reasoning, and its efficiency regarding test-time compute, is determined by the diversity and rigor of the dialogues it has mastered. We conclude that optimizing these conversational scaffolds is the primary lever for the next generation of general intelligence.

</details>


### [433] [ReusStdFlow: A Standardized Reusability Framework for Dynamic Workflow Construction in Agentic AI](https://arxiv.org/abs/2602.14922)
*Gaoyang Zhang,Shanghong Zou,Yafang Wang,He Zhang,Ruohua Xu,Feng Zhao*

Main category: cs.AI

TL;DR: ReusStdFlow框架通过"提取-存储-构建"范式解决企业Agentic AI中的可重用性困境和结构幻觉问题，将异构DSL解构为标准化模块，实现90%以上的工作流重构准确率。


<details>
  <summary>Details</summary>
Motivation: 解决企业Agentic AI中的"可重用性困境"和结构幻觉问题，实现企业数字资产的自动化重组和高效复用。

Method: 提出ReusStdFlow框架，采用"提取-存储-构建"范式：1) 将异构平台特定DSL解构为标准化模块化工作流片段；2) 使用图数据库和向量数据库的双重知识架构协同检索拓扑结构和功能语义；3) 采用检索增强生成(RAG)策略智能组装工作流。

Result: 在200个真实n8n工作流上测试，系统在提取和构建两方面均达到90%以上的准确率。

Conclusion: 该框架为企业数字资产的自动化重组和高效复用提供了标准化解决方案。

Abstract: To address the ``reusability dilemma'' and structural hallucinations in enterprise Agentic AI,this paper proposes ReusStdFlow, a framework centered on a novel ``Extraction-Storage-Construction'' paradigm. The framework deconstructs heterogeneous, platform-specific Domain Specific Languages (DSLs) into standardized, modular workflow segments. It employs a dual knowledge architecture-integrating graph and vector databases-to facilitate synergistic retrieval of both topological structures and functional semantics. Finally, workflows are intelligently assembled using a retrieval-augmented generation (RAG) strategy. Tested on 200 real-world n8n workflows, the system achieves over 90% accuracy in both extraction and construction. This framework provides a standardized solution for the automated reorganization and efficient reuse of enterprise digital assets.

</details>


### [434] [MAC-AMP: A Closed-Loop Multi-Agent Collaboration System for Multi-Objective Antimicrobial Peptide Design](https://arxiv.org/abs/2602.14926)
*Gen Zhou,Sugitha Janarthanan,Lianghong Chen,Pingzhao Hu*

Main category: cs.AI

TL;DR: MAC-AMP是一个基于多智能体大语言模型的闭环协作系统，用于多目标抗菌肽设计，通过模拟同行评审-自适应强化学习框架，在活性、毒性、新颖性和可解释性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 应对抗生素耐药性的全球健康威胁，现有AI抗菌肽设计模型难以平衡活性、毒性和新颖性等关键目标，且评分方法僵化不透明，结果难以解释和优化。

Method: 基于多智能体大语言模型构建闭环协作系统，采用完全自主的模拟同行评审-自适应强化学习框架，仅需任务描述和示例数据集即可设计新型抗菌肽。

Result: MAC-AMP在抗菌活性、抗菌肽相似性、毒性合规性和结构可靠性方面表现出色，优于其他抗菌肽生成模型，能有效优化多个关键分子性质。

Conclusion: 该工作首次引入具有跨领域可迁移性的闭环多智能体系统进行抗菌肽设计，支持多目标优化并保持可解释性，而非"黑箱"模型，展示了多智能体LLM在复杂科学设计场景中的潜力。

Abstract: To address the global health threat of antimicrobial resistance, antimicrobial peptides (AMP) are being explored for their potent and promising ability to fight resistant pathogens. While artificial intelligence (AI) is being employed to advance AMP discovery and design, most AMP design models struggle to balance key goals like activity, toxicity, and novelty, using rigid or unclear scoring methods that make results hard to interpret and optimize. As the capabilities of Large Language Models (LLM) advance and evolve swiftly, we turn to AI multi-agent collaboration based on such models (multi-agent LLMs), which show rapidly rising potential in complex scientific design scenarios. Based on this, we introduce MAC-AMP, a closed-loop multi-agent collaboration (MAC) system for multi-objective AMP design. The system implements a fully autonomous simulated peer review-adaptive reinforcement learning framework that requires only a task description and example dataset to design novel AMPs. The novelty of our work lies in introducing a closed-loop multi-agent system for AMP design, with cross-domain transferability, that supports multi-objective optimization while remaining explainable rather than a 'black box'. Experiments show that MAC-AMP outperforms other AMP generative models by effectively optimizing AMP generation for multiple key molecular properties, demonstrating exceptional results in antibacterial activity, AMP likeliness, toxicity compliance, and structural reliability.

</details>


### [435] [On the Semantics of Primary Cause in Hybrid Dynamic Domains](https://arxiv.org/abs/2602.14994)
*Shakil M. Khan,Asim Mehmood,Sandra Zilles*

Main category: cs.AI

TL;DR: 本文在混合时态情境演算框架中提出了两种主要因果关系的定义，并证明了它们的等价性


<details>
  <summary>Details</summary>
Motivation: 实际因果关系的研究对理性研究至关重要，但现有研究主要关注离散变化，而现实世界中的变化既有离散也有连续（混合）。尽管有少量研究关注连续变化下的因果关系，但在混合行动理论框架中的研究仍然不足

Method: 在混合时态情境演算框架中提出两种主要因果关系的定义：一种是基础性的定义，另一种通过贡献形式化因果关系，并使用修改后的"but-for"测试从反事实角度验证。然后证明这两种定义的等价性

Result: 证明了提出的两种因果关系定义是等价的，并且这些定义具有一些直观合理的性质

Conclusion: 在混合行动理论框架中成功形式化了因果关系，为处理同时包含离散和连续变化的实际世界中的因果推理提供了理论基础

Abstract: Reasoning about actual causes of observed effects is fundamental to the study of rationality. This important problem has been studied since the time of Aristotle, with formal mathematical accounts emerging recently. We live in a world where change due to actions can be both discrete and continuous, that is, hybrid. Yet, despite extensive research on actual causation, only few recent studies looked into causation with continuous change. Building on recent progress, in this paper we propose two definitions of primary cause in a hybrid action-theoretic framework, namely the hybrid temporal situation calculus. One of these is foundational in nature while the other formalizes causation through contributions, which can then be verified from a counterfactual perspective using a modified ``but-for'' test. We prove that these two definitions are indeed equivalent. We then show that our definitions of causation have some intuitively justifiable properties.

</details>


### [436] [Hunt Globally: Deep Research AI Agents for Drug Asset Scouting in Investing, Business Development, and Search & Evaluation](https://arxiv.org/abs/2602.15019)
*Alisa Vinogradova,Vlad Vinogradov,Luba Greenwood,Ilya Yasny,Dmitry Kobyzev,Shoman Kasbekar,Kong Nguyen,Dmitrii Radkevich,Roman Doronin,Andrey Doronichev*

Main category: cs.AI

TL;DR: 本文提出了一种用于药物资产侦察的基准测试方法和自学习Bioptic Agent，在非英语、区域性药物资产发现任务中显著优于现有AI系统。


<details>
  <summary>Details</summary>
Motivation: 生物制药创新格局已发生变化：大量新药资产源自美国以外地区，主要通过区域性非英语渠道披露。超过85%的专利申请来自美国以外，中国占全球近一半；学术产出中非美国份额也在增长。中国约占全球药物开发的30%，涉及1200+新候选药物。在这种高风险环境下，未能发现"隐形"资产会给投资者和业务开发团队带来数十亿美元风险，使得资产侦察成为覆盖关键竞争领域，速度和完整性决定价值。然而当前深度研究AI代理在跨异构多语言源实现高召回发现且无幻觉方面仍落后于人类专家。

Method: 提出药物资产侦察基准测试方法和调优的树状自学习Bioptic Agent。构建具有挑战性的完整性基准：使用多语言多代理管道，将复杂用户查询与主要在美国雷达之外的基准资产配对。为反映真实交易复杂性，从专家投资者、业务开发和风险投资专业人士收集筛选查询，并用作先验条件生成基准查询。使用LLM-as-judge评估进行评分，校准到专家意见。将Bioptic Agent与Claude Opus 4.6、OpenAI GPT-5.2 Pro、Perplexity Deep Research、Gemini 3 Pro + Deep Research和Exa Websets进行比较。

Result: Bioptic Agent达到79.7% F1分数，显著优于其他系统：Claude Opus 4.6（56.2%）、Gemini 3 Pro + Deep Research（50.6%）、GPT-5.2 Pro（46.6%）、Perplexity Deep Research（44.2%）和Exa Websets（26.9%）。性能随额外计算资源增加而显著提升，支持"更多计算带来更好结果"的观点。

Conclusion: Bioptic Agent在药物资产侦察任务中表现出色，特别是在发现非美国中心、多语言来源的"隐形"资产方面。该方法为解决生物制药领域日益增长的多语言、区域性创新发现挑战提供了有效解决方案，计算资源的增加进一步提升了性能表现。

Abstract: Bio-pharmaceutical innovation has shifted: many new drug assets now originate outside the United States and are disclosed primarily via regional, non-English channels. Recent data suggests >85% of patent filings originate outside the U.S., with China accounting for nearly half of the global total; a growing share of scholarly output is also non-U.S. Industry estimates put China at ~30% of global drug development, spanning 1,200+ novel candidates. In this high-stakes environment, failing to surface "under-the-radar" assets creates multi-billion-dollar risk for investors and business development teams, making asset scouting a coverage-critical competition where speed and completeness drive value. Yet today's Deep Research AI agents still lag human experts in achieving high-recall discovery across heterogeneous, multilingual sources without hallucinations.
  We propose a benchmarking methodology for drug asset scouting and a tuned, tree-based self-learning Bioptic Agent aimed at complete, non-hallucinated scouting. We construct a challenging completeness benchmark using a multilingual multi-agent pipeline: complex user queries paired with ground-truth assets that are largely outside U.S.-centric radar. To reflect real deal complexity, we collected screening queries from expert investors, BD, and VC professionals and used them as priors to conditionally generate benchmark queries. For grading, we use LLM-as-judge evaluation calibrated to expert opinions. We compare Bioptic Agent against Claude Opus 4.6, OpenAI GPT-5.2 Pro, Perplexity Deep Research, Gemini 3 Pro + Deep Research, and Exa Websets. Bioptic Agent achieves 79.7% F1 versus 56.2% (Claude Opus 4.6), 50.6% (Gemini 3 Pro + Deep Research), 46.6% (GPT-5.2 Pro), 44.2% (Perplexity Deep Research), and 26.9% (Exa Websets). Performance improves steeply with additional compute, supporting the view that more compute yields better results.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [437] [Nonparametric Distribution Regression Re-calibration](https://arxiv.org/abs/2602.13362)
*Ádám Jung,Domokos M. Kelen,András A. Benczúr*

Main category: stat.ML

TL;DR: 提出一种基于条件核均值嵌入的非参数重校准算法，用于修正回归模型的校准误差，无需限制性建模假设，并在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 概率回归中，最小化预测误差常导致模型优先考虑信息性而非校准性，产生过度自信的预测。现有后处理校准方法要么依赖弱校准概念（如PIT均匀性），要么对误差性质施加限制性参数假设。

Method: 提出基于条件核均值嵌入的非参数重校准算法，引入可在O(n log n)时间内评估的经验分布特征核，用于实值目标的高效推理。

Result: 该方法在多样化的回归基准测试和模型类别中，始终优于先前的重校准方法。

Conclusion: 提出的非参数重校准算法能够有效修正校准误差，无需限制性建模假设，在安全关键应用中提供更可靠的不确定性估计。

Abstract: A key challenge in probabilistic regression is ensuring that predictive distributions accurately reflect true empirical uncertainty. Minimizing overall prediction error often encourages models to prioritize informativeness over calibration, producing narrow but overconfident predictions. However, in safety-critical settings, trustworthy uncertainty estimates are often more valuable than narrow intervals. Realizing the problem, several recent works have focused on post-hoc corrections; however, existing methods either rely on weak notions of calibration (such as PIT uniformity) or impose restrictive parametric assumptions on the nature of the error. To address these limitations, we propose a novel nonparametric re-calibration algorithm based on conditional kernel mean embeddings, capable of correcting calibration error without restrictive modeling assumptions. For efficient inference with real-valued targets, we introduce a novel characteristic kernel over distributions that can be evaluated in $\mathcal{O}(n \log n)$ time for empirical distributions of size $n$. We demonstrate that our method consistently outperforms prior re-calibration approaches across a diverse set of regression benchmarks and model classes.

</details>


### [438] [Metabolic cost of information processing in Poisson variational autoencoders](https://arxiv.org/abs/2602.13421)
*Hadi Vafaii,Jacob L. Yates*

Main category: stat.ML

TL;DR: Poisson变分自编码器通过KL散度项引入代谢成本，将编码率与神经元发放率耦合，实现编码保真度与能量消耗的权衡，为资源受限的计算理论提供基础。


<details>
  <summary>Details</summary>
Motivation: 生物计算受能量约束，但传统计算理论将能量视为免费资源。需要建立能量感知的计算理论，将信息处理与生物物理成本联系起来。

Method: 提出Poisson变分自编码器（P-VAE），在Poisson假设下进行变分自由能最小化。KL散度项与神经元先验发放率成正比，产生代谢成本项。与Grelu-VAE（带ReLU的高斯VAE）对比，控制非负约束变量。

Result: 增加KL权重β会单调增加P-VAE的稀疏性并降低平均发放活动，而Grelu-VAE表示保持不变。证明代谢成本结构是Poisson公式特有的，而非非负表示的副产品。

Conclusion: Poisson变分推断为资源受限的计算理论提供了有前景的基础，将抽象信息量（编码率）与具体生物物理变量（发放率）耦合，实现编码保真度与能量消耗的权衡。

Abstract: Computation in biological systems is fundamentally energy-constrained, yet standard theories of computation treat energy as freely available. Here, we argue that variational free energy minimization under a Poisson assumption offers a principled path toward an energy-aware theory of computation. Our key observation is that the Kullback-Leibler (KL) divergence term in the Poisson free energy objective becomes proportional to the prior firing rates of model neurons, yielding an emergent metabolic cost term that penalizes high baseline activity. This structure couples an abstract information-theoretic quantity -- the *coding rate* -- to a concrete biophysical variable -- the *firing rate* -- which enables a trade-off between coding fidelity and energy expenditure. Such a coupling arises naturally in the Poisson variational autoencoder (P-VAE) -- a brain-inspired generative model that encodes inputs as discrete spike counts and recovers a spiking form of *sparse coding* as a special case -- but is absent from standard Gaussian VAEs. To demonstrate that this metabolic cost structure is unique to the Poisson formulation, we compare the P-VAE against Grelu-VAE, a Gaussian VAE with ReLU rectification applied to latent samples, which controls for the non-negativity constraint. Across a systematic sweep of the KL term weighting coefficient $β$ and latent dimensionality, we find that increasing $β$ monotonically increases sparsity and reduces average spiking activity in the P-VAE. In contrast, Grelu-VAE representations remain unchanged, confirming that the effect is specific to Poisson statistics rather than a byproduct of non-negative representations. These results establish Poisson variational inference as a promising foundation for a resource-constrained theory of computation.

</details>


### [439] [Locally Private Parametric Methods for Change-Point Detection](https://arxiv.org/abs/2602.13619)
*Anuj Kumar Yadav,Cemre Cadir,Yanina Shkel,Michael Gastpar*

Main category: stat.ML

TL;DR: 本文研究局部差分隐私下的参数化变点检测，提出改进的非隐私算法和两种隐私算法，分析隐私成本对检测性能的影响，并建立了Rényi散度强数据处理不等式系数的结构结果。


<details>
  <summary>Details</summary>
Motivation: 在时间序列中识别分布变化是重要的统计问题，但现有研究很少考虑隐私保护。随着数据隐私日益重要，需要在保护个体隐私的同时进行有效的变点检测，这需要量化隐私保护对检测性能的影响。

Method: 1. 非隐私设置：基于广义对数似然比检验的变点检测算法，使用鞅方法改进有限样本精度保证
2. 隐私设置：提出两种局部差分隐私算法，分别基于随机响应和二进制机制
3. 理论分析：推导检测精度界限，建立Rényi散度和Jeffreys-Rényi散度的强数据处理不等式系数的结构结果

Result: 1. 改进了非隐私变点检测算法的有限样本精度保证
2. 提出了有效的局部差分隐私变点检测算法
3. 量化了局部差分隐私在变点检测中的统计成本，显示隐私保护会降低性能
4. 证明了强数据处理不等式系数在二进制输入分布下达到最优

Conclusion: 本文系统研究了局部差分隐私下的变点检测问题，提出了理论保证良好的算法，量化了隐私与检测性能的权衡。建立的强数据处理不等式结构结果具有独立价值，可应用于统计估计、数据压缩和马尔可夫链混合等领域。

Abstract: We study parametric change-point detection, where the goal is to identify distributional changes in time series, under local differential privacy. In the non-private setting, we derive improved finite-sample accuracy guarantees for a change-point detection algorithm based on the generalized log-likelihood ratio test, via martingale methods. In the private setting, we propose two locally differentially private algorithms based on randomized response and binary mechanisms, and analyze their theoretical performance. We derive bounds on detection accuracy and validate our results through empirical evaluation. Our results characterize the statistical cost of local differential privacy in change-point detection and show how privacy degrades performance relative to a non-private benchmark. As part of this analysis, we establish a structural result for strong data processing inequalities (SDPI), proving that SDPI coefficients for Rényi divergences and their symmetric variants (Jeffreys-Rényi divergences) are achieved by binary input distributions. These results on SDPI coefficients are also of independent interest, with applications to statistical estimation, data compression, and Markov chain mixing.

</details>


### [440] [Quantifying Normality: Convergence Rate to Gaussian Limit for Stochastic Approximation and Unadjusted OU Algorithm](https://arxiv.org/abs/2602.13906)
*Shaan Ul Haque,Zedong Wang,Zixuan Zhang,Siva Theja Maguluri*

Main category: stat.ML

TL;DR: 本文为随机逼近(SA)算法建立了非渐近的Wasserstein距离界限，量化了有限时间内SA迭代分布与渐近高斯极限之间的近似精度，并获得了误差的尾概率界限。


<details>
  <summary>Details</summary>
Motivation: 现有随机逼近算法的渐近正态性结果无法量化有限时间内高斯近似的精度，需要建立明确的非渐近界限来评估实际应用中的近似误差。

Method: 首先研究由一般噪声驱动的离散Ornstein-Uhlenbeck过程的收敛速率，然后通过Stein方法处理矩阵加权独立同分布随机变量和的高斯近似，最后通过刻画缩放SA迭代与离散O-U过程之间的误差动态，结合后者的收敛速率获得SA的有限时间界限。

Result: 建立了缩放SA迭代在时间k的分布与渐近高斯极限之间Wasserstein距离的显式非渐近界限，包括常数和多项式衰减步长情况，并获得了SA迭代误差在任何时间的尾概率界限。

Conclusion: 本文为随机逼近算法提供了有限时间的高斯近似精度量化工具，所建立的离散Ornstein-Uhlenbeck过程收敛速率分析对采样文献也有独立价值，为实际应用中SA算法的误差控制提供了理论保证。

Abstract: Stochastic approximation (SA) is a method for finding the root of an operator perturbed by noise. There is a rich literature establishing the asymptotic normality of rescaled SA iterates under fairly mild conditions. However, these asymptotic results do not quantify the accuracy of the Gaussian approximation in finite time. In this paper, we establish explicit non-asymptotic bounds on the Wasserstein distance between the distribution of the rescaled iterate at time k and the asymptotic Gaussian limit for various choices of step-sizes including constant and polynomially decaying. As an immediate consequence, we obtain tail bounds on the error of SA iterates at any time.
  We obtain the sharp rates by first studying the convergence rate of the discrete Ornstein-Uhlenbeck (O-U) process driven by general noise, whose stationary distribution is identical to the limiting Gaussian distribution of the rescaled SA iterates. We believe that this is of independent interest, given its connection to sampling literature. The analysis involves adapting Stein's method for Gaussian approximation to handle the matrix weighted sum of i.i.d. random variables. The desired finite-time bounds for SA are obtained by characterizing the error dynamics between the rescaled SA iterate and the discrete time O-U process and combining it with the convergence rate of the latter process.

</details>


### [441] [A Theoretical Framework for LLM Fine-tuning Using Early Stopping for Non-random Initialization](https://arxiv.org/abs/2602.13942)
*Zexuan Sun,Garvesh Raskutti*

Main category: stat.ML

TL;DR: 该论文为LLM微调开发了一个统计框架，结合早期停止理论和注意力NTK，解释了为什么少量微调轮次就能获得良好性能，并建立了收敛率与NTK核矩阵特征值衰减的联系。


<details>
  <summary>Details</summary>
Motivation: 在大语言模型时代，微调预训练模型已成为普遍做法，但其理论基础仍然是一个开放问题。核心问题是为什么通常只需要几个epoch的微调就能在许多不同任务上获得强大性能。

Method: 开发了一个统计框架，将严格的早期停止理论与LLM的注意力神经正切核（NTK）相结合。将经典NTK理论扩展到非随机（预训练）初始化，并提供注意力微调的收敛保证。

Result: 理论的关键见解是收敛率与NTK诱导的经验核矩阵的特征值衰减率密切相关。该框架还能解释LLM中多任务的任务向量。现代语言模型在真实数据集上的实验为理论见解提供了实证支持。

Conclusion: 该工作为LLM微调实践提供了新的理论见解，解释了为什么少量微调轮次就足够，并建立了收敛率与NTK核矩阵特征值衰减的理论联系，为微调实践提供了理论基础。

Abstract: In the era of large language models (LLMs), fine-tuning pretrained models has become ubiquitous. Yet the theoretical underpinning remains an open question. A central question is why only a few epochs of fine-tuning are typically sufficient to achieve strong performance on many different tasks. In this work, we approach this question by developing a statistical framework, combining rigorous early stopping theory with the attention-based Neural Tangent Kernel (NTK) for LLMs, offering new theoretical insights on fine-tuning practices. Specifically, we formally extend classical NTK theory [Jacot et al., 2018] to non-random (i.e., pretrained) initializations and provide a convergence guarantee for attention-based fine-tuning. One key insight provided by the theory is that the convergence rate with respect to sample size is closely linked to the eigenvalue decay rate of the empirical kernel matrix induced by the NTK. We also demonstrate how the framework can be used to explain task vectors for multiple tasks in LLMs. Finally, experiments with modern language models on real-world datasets provide empirical evidence supporting our theoretical insights.

</details>


### [442] [Computable Bernstein Certificates for Cross-Fitted Clipped Covariance Estimation](https://arxiv.org/abs/2602.14020)
*Even He,Zaizai Yan*

Main category: stat.ML

TL;DR: 提出一种基于交叉拟合的截断协方差估计器，配备完全可计算的Bernstein型偏差证书，通过MinUpper选择器自动平衡误差，适用于重尾数据和异常值污染场景。


<details>
  <summary>Details</summary>
Motivation: 研究重尾样本（可能包含少量任意异常值）下的算子范数协方差估计问题。常用的欧几里得范数截断方法依赖于未知的截断水平，需要数据驱动的方法来自动确定最优参数。

Method: 提出交叉拟合的截断协方差估计器，配备完全可计算的Bernstein型偏差证书。使用MinUpper选择器平衡认证的随机误差和截断偏差的稳健留出代理，实现数据驱动的参数调优。

Result: 该方法在温和的尾部正则性条件下适应内在复杂性度量（如有效秩），在仅有限四阶矩条件下仍保持有意义的保证。在受污染的尖峰协方差基准测试中表现出稳定的性能和竞争性的准确性。

Conclusion: 提出的方法为处理重尾数据和异常值污染的协方差估计提供了理论保证和实用工具，通过数据驱动的方式解决了传统截断方法依赖未知参数的问题。

Abstract: We study operator-norm covariance estimation from heavy-tailed samples that may include a small fraction of arbitrary outliers. A simple and widely used safeguard is \emph{Euclidean norm clipping}, but its accuracy depends critically on an unknown clipping level. We propose a cross-fitted clipped covariance estimator equipped with \emph{fully computable} Bernstein-type deviation certificates, enabling principled data-driven tuning via a selector (\emph{MinUpper}) that balances certified stochastic error and a robust hold-out proxy for clipping bias. The resulting procedure adapts to intrinsic complexity measures such as effective rank under mild tail regularity and retains meaningful guarantees under only finite fourth moments. Experiments on contaminated spiked-covariance benchmarks illustrate stable performance and competitive accuracy across regimes.

</details>


### [443] [Why Self-Training Helps and Hurts: Denoising vs. Signal Forgetting](https://arxiv.org/abs/2602.14029)
*Mingqi Wu,Archer Y. Yang,Qiang Sun*

Main category: stat.ML

TL;DR: 研究过参数化线性回归中的迭代自训练（自蒸馏），分析预测风险和有效噪声随迭代的变化，发现U形测试风险曲线和最优早停时间，提出迭代广义交叉验证准则用于数据驱动停止时间选择。


<details>
  <summary>Details</summary>
Motivation: 研究迭代自训练（自蒸馏）在过参数化线性回归中的行为，理解重复使用自身预测作为伪标签进行训练时，模型性能如何随迭代变化，以及其中的权衡机制。

Method: 在过参数化线性回归框架下，初始估计器在带噪声标签上训练，后续迭代使用新协变量和前一模型的无噪声伪标签。推导高维极限下的确定性等价递推关系，分析预测风险和有效噪声的集中性。

Result: 发现U形测试风险曲线：系统成分（信号遗忘）随迭代增长，随机成分（去噪）随迭代衰减。在尖峰协方差模型中，迭代作为迭代依赖的谱滤波器，实现隐式软特征选择。提出迭代广义交叉验证准则，证明其一致性。

Conclusion: 迭代自训练中存在去噪与遗忘的权衡，产生最优早停时间。迭代广义交叉验证可数据驱动选择停止时间和正则化参数。理论在合成协方差实验中得到验证。

Abstract: Iterative self-training (self-distillation) repeatedly refits a model on pseudo-labels generated by its own predictions. We study this procedure in overparameterized linear regression: an initial estimator is trained on noisy labels, and each subsequent iterate is trained on fresh covariates with noiseless pseudo-labels from the previous model. In the high-dimensional regime, we derive deterministic-equivalent recursions for the prediction risk and effective noise across iterations, and prove that the empirical quantities concentrate sharply around these limits. The recursion separates two competing forces: a systematic component that grows with iteration due to progressive signal forgetting, and a stochastic component that decays due to denoising via repeated data-dependent projections. Their interaction yields a $U$-shaped test-risk curve and an optimal early-stopping time. In spiked covariance models, iteration further acts as an iteration-dependent spectral filter that preserves strong eigendirections while suppressing weaker ones, inducing an implicit form of soft feature selection distinct from ridge regression. Finally, we propose an iterated generalized cross-validation criterion and prove its uniform consistency for estimating the risk along the self-training trajectory, enabling fully data-driven selection of the stopping time and regularization. Experiments on synthetic covariances validate the theory and illustrate the predicted denoising-forgetting trade-off.

</details>


### [444] [Federated Ensemble Learning with Progressive Model Personalization](https://arxiv.org/abs/2602.14244)
*Ala Emrani,Amir Najafi,Abolfazl Motahari*

Main category: stat.ML

TL;DR: 提出基于Boosting的个性化联邦学习框架，通过渐进增加个性化组件深度并控制复杂度，平衡共享特征提取与个性化之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 传统个性化联邦学习将神经网络分解为共享特征提取器和客户端特定头部，但存在根本性权衡：深度共享组件阻碍个性化，而大的本地头部在有限数据下容易过拟合。现有方法依赖固定的浅层头部，无法系统性地解决这一权衡。

Method: 提出Boosting启发式框架，为每个客户端构建T个模型的集成。在Boosting迭代中，逐步增加个性化组件的深度，同时通过低秩分解或宽度缩减系统控制其有效复杂度。这种设计同时限制过拟合并通过允许更丰富的个性化来显著减少客户端特定偏差。

Result: 理论分析建立了泛化边界，对平均本地样本大小和客户端总数具有有利依赖。实验证明，在EMNIST、CIFAR-10/100、Sent140等基准和真实数据集上，该框架在异构数据分布下始终优于最先进的PFL方法。

Conclusion: 提出的Boosting框架通过渐进增加个性化深度并控制复杂度，系统性地解决了个性化联邦学习中的共享-个性化权衡问题，在理论和实验上都表现出优越性能。

Abstract: Federated Learning provides a privacy-preserving paradigm for distributed learning, but suffers from statistical heterogeneity across clients. Personalized Federated Learning (PFL) mitigates this issue by considering client-specific models. A widely adopted approach in PFL decomposes neural networks into a shared feature extractor and client-specific heads. While effective, this design induces a fundamental tradeoff: deep or expressive shared components hinder personalization, whereas large local heads exacerbate overfitting under limited per-client data. Most existing methods rely on rigid, shallow heads, and therefore fail to navigate this tradeoff in a principled manner. In this work, we propose a boosting-inspired framework that enables a smooth control of this tradeoff. Instead of training a single personalized model, we construct an ensemble of $T$ models for each client. Across boosting iterations, the depth of the personalized component are progressively increased, while its effective complexity is systematically controlled via low-rank factorization or width shrinkage. This design simultaneously limits overfitting and substantially reduces per-client bias by allowing increasingly expressive personalization. We provide theoretical analysis that establishes generalization bounds with favorable dependence on the average local sample size and the total number of clients. Specifically, we prove that the complexity of the shared layers is effectively suppressed, while the dependence on the boosting horizon $T$ is controlled through parameter reduction. Notably, we provide a novel nonlinear generalization guarantee for decoupled PFL models. Extensive experiments on benchmark and real-world datasets (e.g., EMNIST, CIFAR-10/100, and Sent140) demonstrate that the proposed framework consistently outperforms state-of-the-art PFL methods under heterogeneous data distributions.

</details>


### [445] [Constrained and Composite Sampling via Proximal Sampler](https://arxiv.org/abs/2602.14478)
*Thanh Dang,Jiaming Liang*

Main category: stat.ML

TL;DR: 论文提出两种对数凹采样方法：约束采样和复合采样。约束采样通过提升变换将约束问题转化为高维均匀分布采样，使用近端采样器和切割平面方法实现；复合采样通过双重提升变换将复合目标分解为约束采样问题。两种方法都基于最小化oracle访问，无需投影或反射操作。


<details>
  <summary>Details</summary>
Motivation: 现有约束采样方法通常依赖投影、反射、障碍函数或镜像映射，这些方法要么计算成本高，要么需要了解约束集的几何结构。本文旨在开发仅需最小oracle访问（分离oracle和次梯度oracle）的实用无偏采样器，避免现有方法的局限性。

Method: 1. 约束采样：通过epigraph变换将约束采样问题提升到ℝ^{d+1}中的均匀分布采样，使用近端采样器结合切割平面方法和拒绝采样实现。2. 复合采样：通过双重epigraph提升将复合目标f+h分解为约束采样问题，在ℝ^{d+2}中应用近端采样器，利用f和h的不同oracle访问组合构建分离oracle。

Result: 开发了仅需分离oracle和次梯度oracle的约束采样器，无需投影操作；提出了复合采样的统一框架，可灵活利用不同oracle组合；为两种采样问题建立了Rényi和χ²散度度量的混合时间界限。

Conclusion: 本文提出的基于提升变换和近端采样的方法为约束和复合采样问题提供了新的解决方案，仅需最小oracle访问，避免了传统方法的几何结构依赖，具有实用性和无偏性，为贝叶斯推断等应用提供了有效工具。

Abstract: We study two log-concave sampling problems: constrained sampling and composite sampling. First, we consider sampling from a target distribution with density proportional to $\exp(-f(x))$ supported on a convex set $K \subset \mathbb{R}^d$, where $f$ is convex. The main challenge is enforcing feasibility without degrading mixing. Using an epigraph transformation, we reduce this task to sampling from a nearly uniform distribution over a lifted convex set in $\mathbb{R}^{d+1}$. We then solve the lifted problem using a proximal sampler. Assuming only a separation oracle for $K$ and a subgradient oracle for $f$, we develop an implementation of the proximal sampler based on the cutting-plane method and rejection sampling. Unlike existing constrained samplers that rely on projection, reflection, barrier functions, or mirror maps, our approach enforces feasibility using only minimal oracle access, resulting in a practical and unbiased sampler without knowing the geometry of the constraint set.
  Second, we study composite sampling, where the target is proportional to $\exp(-f(x)-h(x))$ with closed and convex $f$ and $h$. This composite structure is standard in Bayesian inference with $f$ modeling data fidelity and $h$ encoding prior information. We reduce composite sampling via an epigraph lifting of $h$ to constrained sampling in $\mathbb{R}^{d+1}$, which allows direct application of the constrained sampling algorithm developed in the first part. This reduction results in a double epigraph lifting formulation in $\mathbb{R}^{d+2}$, on which we apply a proximal sampler. By keeping $f$ and $h$ separate, we further demonstrate how different combinations of oracle access (such as subgradient and proximal) can be leveraged to construct separation oracles for the lifted problem. For both sampling problems, we establish mixing time bounds measured in Rényi and $χ^2$ divergences.

</details>


### [446] [Accelerating Posterior Inference from Pulsar Light Curves via Learned Latent Representations and Local Simulator-Guided Optimization](https://arxiv.org/abs/2602.14520)
*Farhana Taiyebah,Abu Bucker Siddik,Indronil Bhattacharjee,Diane Oyen,Soumi De,Greg Olmschenk,Constantinos Kalapotharakos*

Main category: stat.ML

TL;DR: 提出结合学习潜在表示与局部模拟器引导优化的框架，加速脉冲星光变曲线后验推断，在保持准确性的同时将推理时间从24小时减少到12分钟（120倍加速）。


<details>
  <summary>Details</summary>
Motivation: 传统MCMC方法进行脉冲星光变曲线后验推断虽然准确但计算成本高昂，需要加速推理过程同时保持准确性。

Method: 1) 使用掩码U-Net预训练重建完整光变曲线并生成信息丰富的潜在嵌入；2) 在学习的嵌入空间中识别相似模拟光变曲线，获得后验分布的初始经验近似；3) 使用前向模拟器引导的局部优化（爬山更新）逐步将经验后验向高似然参数区域移动。

Result: 在NASA NICER观测的PSR J0030+0451光变曲线上，该方法与传统MCMC方法的后验估计结果非常接近，同时将推理时间从24小时减少到12分钟，实现了120倍的加速。

Conclusion: 学习表示与模拟器引导优化的结合为加速后验推断提供了有效框架，在保持准确性的同时显著减少了计算时间，有望应用于其他需要昂贵模拟的推断问题。

Abstract: Posterior inference from pulsar observations in the form of light curves is commonly performed using Markov chain Monte Carlo methods, which are accurate but computationally expensive. We introduce a framework that accelerates posterior inference while maintaining accuracy by combining learned latent representations with local simulator-guided optimization. A masked U-Net is first pretrained to reconstruct complete light curves from partial observations and to produce informative latent embeddings. Given a query light curve, we identify similar simulated light curves from the simulation bank by measuring similarity in the learned embedding space produced by pretrained U-Net encoder, yielding an initial empirical approximation to the posterior over parameters. This initialization is then refined using a local optimization procedure using hill-climbing updates, guided by a forward simulator, progressively shifting the empirical posterior toward higher-likelihood parameter regions. Experiments on the observed light curve of PSR J0030+0451, captured by NASA's Neutron Star Interior Composition Explorer (NICER), show that our method closely matches posterior estimates obtained using traditional MCMC methods while achieving 120 times reduction in inference time (from 24 hours to 12 minutes), demonstrating the effectiveness of learned representations and simulator-guided optimization for accelerated posterior inference.

</details>


### [447] [GenPANIS: A Latent-Variable Generative Framework for Forward and Inverse PDE Problems in Multiphase Media](https://arxiv.org/abs/2602.14642)
*Matthaios Chatzopoulos,Phaedon-Stelios Koutsourelakis*

Main category: stat.ML

TL;DR: GenPANIS是一个统一的生成式框架，用于多相介质中的逆问题和逆设计，通过连续潜在嵌入保持精确离散微观结构，支持梯度推断，在单个架构中实现正向预测和逆向恢复。


<details>
  <summary>Details</summary>
Motivation: 多相介质中的逆问题和逆设计（恢复或设计微观结构以实现目标宏观响应）需要处理离散值材料场，这使得问题不可微分且与基于梯度的方法不兼容。现有方法要么放松为连续近似（损害物理保真度），要么使用单独的重型模型进行正向和逆向任务。

Method: 提出GenPANIS统一生成框架，学习微观结构和PDE解的联合分布，通过连续潜在嵌入保持精确离散微观结构。包含物理感知解码器（集成可微分粗粒度PDE求解器）和可学习归一化流先验，支持双向推断（正向预测和逆向恢复）。

Result: 在Darcy流和Helmholtz方程上验证，GenPANIS在具有挑战性的外推场景（包括未见边界条件、体积分数和微观结构形态，以及稀疏、噪声观测）中保持准确性。优于最先进方法，同时使用10-100倍更少的参数，并提供原则性不确定性量化。

Conclusion: GenPANIS通过统一生成框架解决了多相介质中逆问题的关键挑战，在保持物理保真度的同时实现高效梯度推断，为微观结构设计和逆问题提供了强大且可扩展的解决方案。

Abstract: Inverse problems and inverse design in multiphase media, i.e., recovering or engineering microstructures to achieve target macroscopic responses, require operating on discrete-valued material fields, rendering the problem non-differentiable and incompatible with gradient-based methods. Existing approaches either relax to continuous approximations, compromising physical fidelity, or employ separate heavyweight models for forward and inverse tasks. We propose GenPANIS, a unified generative framework that preserves exact discrete microstructures while enabling gradient-based inference through continuous latent embeddings. The model learns a joint distribution over microstructures and PDE solutions, supporting bidirectional inference (forward prediction and inverse recovery) within a single architecture. The generative formulation enables training with unlabeled data, physics residuals, and minimal labeled pairs. A physics-aware decoder incorporating a differentiable coarse-grained PDE solver preserves governing equation structure, enabling extrapolation to varying boundary conditions and microstructural statistics. A learnable normalizing flow prior captures complex posterior structure for inverse problems. Demonstrated on Darcy flow and Helmholtz equations, GenPANIS maintains accuracy on challenging extrapolative scenarios - including unseen boundary conditions, volume fractions, and microstructural morphologies, with sparse, noisy observations. It outperforms state-of-the-art methods while using 10 - 100 times fewer parameters and providing principled uncertainty quantification.

</details>


### [448] [The Well-Tempered Classifier: Some Elementary Properties of Temperature Scaling](https://arxiv.org/abs/2602.14862)
*Pierre-Alexandre Mattei,Bruno Loureiro*

Main category: stat.ML

TL;DR: 温度缩放是一种控制概率模型不确定性的简单方法，本文对其理论性质进行了分析，发现在分类中增加温度会增加不确定性，但在LLM中挑战了"增加温度会增加多样性"的常见说法，并提出了两种新的特征描述。


<details>
  <summary>Details</summary>
Motivation: 温度缩放是控制概率模型不确定性的常用方法，广泛应用于分类器校准和LLM随机性调节，但缺乏严格的理论分析。本文旨在填补这一理论空白，深入探讨温度缩放的性质。

Method: 通过理论分析研究温度缩放的性质：1) 分析温度变化对分类模型不确定性的影响；2) 挑战LLM中温度与多样性的关系；3) 提出两种新的特征描述：几何特征（信息投影）和线性缩放器特征。

Result: 1) 分类中增加温度确实会增加模型不确定性（特别是熵）；2) 挑战了LLM中"增加温度增加多样性"的常见说法；3) 提出温度缩放是原始模型到给定熵模型集合的信息投影；4) 证明温度缩放是唯一不改变模型硬预测的线性缩放器。

Conclusion: 本文为温度缩放提供了严格的理论基础，揭示了其在分类和LLM中的不同性质，并提出了两种新的特征描述，有助于更深入地理解这一广泛使用的方法。

Abstract: Temperature scaling is a simple method that allows to control the uncertainty of probabilistic models. It is mostly used in two contexts: improving the calibration of classifiers and tuning the stochasticity of large language models (LLMs). In both cases, temperature scaling is the most popular method for the job. Despite its popularity, a rigorous theoretical analysis of the properties of temperature scaling has remained elusive. We investigate here some of these properties. For classification, we show that increasing the temperature increases the uncertainty in the model in a very general sense (and in particular increases its entropy). However, for LLMs, we challenge the common claim that increasing temperature increases diversity. Furthermore, we introduce two new characterisations of temperature scaling. The first one is geometric: the tempered model is shown to be the information projection of the original model onto the set of models with a given entropy. The second characterisation clarifies the role of temperature scaling as a submodel of more general linear scalers such as matrix scaling and Dirichlet calibration: we show that temperature scaling is the only linear scaler that does not change the hard predictions of the model.

</details>


### [449] [Activation-Space Uncertainty Quantification for Pretrained Networks](https://arxiv.org/abs/2602.14934)
*Richard Bergna,Stefan Depeweg,Sergio Calvo-Ordoñez,Jonathan Plenk,Alvaro Cartea,Jose Miguel Hernández-Lobato*

Main category: stat.ML

TL;DR: GAPA是一种后处理方法，将贝叶斯建模从权重转移到激活空间，通过高斯过程激活函数保持原始预测不变，同时提供闭式不确定性估计，无需重新训练或采样。


<details>
  <summary>Details</summary>
Motivation: 现有不确定性估计方法通常需要重新训练、蒙特卡洛采样或昂贵的二阶计算，并且可能改变预训练模型的预测。需要一种能够保持冻结主干网络预测不变的高效后处理方法。

Method: 提出高斯过程激活函数(GAPA)，将标准非线性激活替换为高斯过程激活，其后验均值精确匹配原始激活。采用基于缓存训练激活的稀疏变分诱导点近似，结合局部k近邻子集条件，实现确定性单次前向传播的不确定性估计。

Result: 在回归、分类、图像分割和语言建模任务中，GAPA在校准和分布外检测方面匹配或优于强后处理基线方法，同时保持测试时的高效性。

Conclusion: GAPA提供了一种高效的后处理不确定性估计方法，能够保持预训练模型的预测不变，在多种任务中表现出色，为部署预训练模型提供了可靠的不确定性量化方案。

Abstract: Reliable uncertainty estimates are crucial for deploying pretrained models; yet, many strong methods for quantifying uncertainty require retraining, Monte Carlo sampling, or expensive second-order computations and may alter a frozen backbone's predictions. To address this, we introduce Gaussian Process Activations (GAPA), a post-hoc method that shifts Bayesian modeling from weights to activations. GAPA replaces standard nonlinearities with Gaussian-process activations whose posterior mean exactly matches the original activation, preserving the backbone's point predictions by construction while providing closed-form epistemic variances in activation space. To scale to modern architectures, we use a sparse variational inducing-point approximation over cached training activations, combined with local k-nearest-neighbor subset conditioning, enabling deterministic single-pass uncertainty propagation without sampling, backpropagation, or second-order information. Across regression, classification, image segmentation, and language modeling, GAPA matches or outperforms strong post-hoc baselines in calibration and out-of-distribution detection while remaining efficient at test time.

</details>
