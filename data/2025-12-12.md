<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 31]
- [cs.AI](#cs.AI) [Total: 54]
- [q-fin.PR](#q-fin.PR) [Total: 1]
- [eess.SY](#eess.SY) [Total: 14]
- [econ.EM](#econ.EM) [Total: 4]
- [stat.ML](#stat.ML) [Total: 8]
- [cs.CY](#cs.CY) [Total: 4]
- [math.OC](#math.OC) [Total: 12]
- [cs.LG](#cs.LG) [Total: 74]
- [q-fin.PM](#q-fin.PM) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [What Kind of Reasoning (if any) is an LLM actually doing? On the Stochastic Nature and Abductive Appearance of Large Language Models](https://arxiv.org/abs/2512.10080)
*Luciano Floridi,Jessica Morley,Claudio Novelli,David Watson*

Main category: cs.CL

TL;DR: 本文分析当前基于token补全的大型语言模型的推理机制，认为它们本质上是随机的模式匹配而非真正的溯因推理，其看似推理的能力源于对人类生成文本中推理结构的学习。


<details>
  <summary>Details</summary>
Motivation: 研究动机是深入理解当前大型语言模型的推理本质，澄清它们是否真正具备溯因推理能力，以及这种能力与人类推理的根本区别。

Method: 通过分析LLMs的随机性本质和与人类溯因推理的相似性，使用具体示例展示LLMs如何生成看似合理的想法、模仿常识推理和提供解释性答案，同时论证这些输出缺乏真实性、语义基础、验证或理解。

Result: 研究发现LLMs本质上是基于学习模式的随机文本生成器，而非真正的溯因推理系统。它们能够产生看似推理的输出，但这主要源于对人类文本中推理结构的学习，而非真正的理解或验证能力。

Conclusion: LLMs具有随机基础但看似溯因的双重特性，这对评估和应用具有重要意义。它们可以辅助创意生成和人类思考，但其输出必须批判性评估，因为它们无法识别真相或验证解释。文章最后回应了五个反对意见并进行了整体评估。

Abstract: This article looks at how reasoning works in current Large Language Models (LLMs) that function using the token-completion method. It examines their stochastic nature and their similarity to human abductive reasoning. The argument is that these LLMs create text based on learned patterns rather than performing actual abductive reasoning. When their output seems abductive, this is largely because they are trained on human-generated texts that include reasoning structures. Examples are used to show how LLMs can produce plausible ideas, mimic commonsense reasoning, and give explanatory answers without being grounded in truth, semantics, verification, or understanding, and without performing any real abductive reasoning. This dual nature, where the models have a stochastic base but appear abductive in use, has important consequences for how LLMs are evaluated and applied. They can assist with generating ideas and supporting human thinking, but their outputs must be critically assessed because they cannot identify truth or verify their explanations. The article concludes by addressing five objections to these points, noting some limitations in the analysis, and offering an overall evaluation.

</details>


### [2] [Generate-Then-Validate: A Novel Question Generation Approach Using Small Language Models](https://arxiv.org/abs/2512.10110)
*Yumou Wei,John Stamper,Paulo F. Carvalho*

Main category: cs.CL

TL;DR: 提出一个利用小型语言模型（SLM）进行自动问题生成的新管道，通过"生成-验证"策略结合文本生成和概率推理能力来产生高质量问题。


<details>
  <summary>Details</summary>
Motivation: 当前学习分析研究中主要使用大型语言模型进行问题生成，但小型语言模型具有潜力，需要探索如何有效利用其能力来生成高质量问题。

Method: 采用"生成-验证"策略的管道：首先进行扩展性生成产生大量候选问题，然后通过基于新颖概率推理的选择性验证来精炼问题。

Result: 通过人类专家和大型语言模型的双重评估，大多数评估者（人类或LLM）认为生成的问题具有明确答案，并且与预期学习目标基本一致。

Conclusion: 当通过精心设计的管道引导并充分利用其优势时，小型语言模型能够有效生成高质量问题，可作为大型语言模型的补充。

Abstract: We explore the use of small language models (SLMs) for automatic question generation as a complement to the prevalent use of their large counterparts in learning analytics research. We present a novel question generation pipeline that leverages both the text generation and the probabilistic reasoning abilities of SLMs to generate high-quality questions. Adopting a "generate-then-validate" strategy, our pipeline first performs expansive generation to create an abundance of candidate questions and refine them through selective validation based on novel probabilistic reasoning. We conducted two evaluation studies, one with seven human experts and the other with a large language model (LLM), to assess the quality of the generated questions. Most judges (humans or LLMs) agreed that the generated questions had clear answers and generally aligned well with the intended learning objectives. Our findings suggest that an SLM can effectively generate high-quality questions when guided by a well-designed pipeline that leverages its strengths.

</details>


### [3] [Workflow is All You Need: Escaping the "Statistical Smoothing Trap" via High-Entropy Information Foraging and Adversarial Pacing](https://arxiv.org/abs/2512.10121)
*Zhongjie Jiang*

Main category: cs.CL

TL;DR: 论文提出DeepNews框架解决长文本生成的"不可能三角"问题，通过模拟金融记者的认知过程，结合双粒度检索、模式引导规划和对抗约束提示，显著降低幻觉并提高逻辑连贯性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在垂直领域长文本生成中存在"不可能三角"瓶颈：难以同时实现低幻觉、深度逻辑连贯性和个性化表达。研究发现这是由于现有生成范式陷入"统计平滑陷阱"，忽略了专家写作所需的高熵信息获取和结构化认知过程。

Method: 提出DeepNews框架，模拟资深金融记者的隐式认知过程，包含三个核心模块：1)基于信息觅食理论的双粒度检索机制，强制10:1饱和信息输入比；2)模式引导的战略规划，利用领域专家知识库和原子块构建逻辑骨架；3)对抗约束提示技术，使用节奏打破和逻辑迷雾等策略破坏模型生成文本的概率平滑性。

Result: 实验发现深度财经报道存在"知识悬崖"现象：检索上下文低于15,000字符时内容真实性崩溃，而高冗余输入超过30,000字符可将无幻觉率稳定在85%以上。在顶级中文科技媒体的生态效度盲测中，基于前代模型的DeepNews系统获得25%的投稿接受率，显著优于SOTA模型的零样本生成（0%接受率）。

Conclusion: DeepNews框架通过显式建模专家认知过程，成功突破了长文本生成的"不可能三角"瓶颈，为垂直领域高质量内容生成提供了有效解决方案，证明了认知过程建模在提升大语言模型生成质量中的重要性。

Abstract: Central to long-form text generation in vertical domains is the "impossible trinity" confronting current large language models (LLMs): the simultaneous achievement of low hallucination, deep logical coherence, and personalized expression. This study establishes that this bottleneck arises from existing generative paradigms succumbing to the Statistical Smoothing Trap, a phenomenon that overlooks the high-entropy information acquisition and structured cognitive processes integral to expert-level writing. To address this limitation, we propose the DeepNews Framework, an agentic workflow that explicitly models the implicit cognitive processes of seasoned financial journalists. The framework integrates three core modules: first, a dual-granularity retrieval mechanism grounded in information foraging theory, which enforces a 10:1 saturated information input ratio to mitigate hallucinatory outputs; second, schema-guided strategic planning, a process leveraging domain expert knowledge bases (narrative schemas) and Atomic Blocks to forge a robust logical skeleton; third, adversarial constraint prompting, a technique deploying tactics including Rhythm Break and Logic Fog to disrupt the probabilistic smoothness inherent in model-generated text. Experiments delineate a salient Knowledge Cliff in deep financial reporting: content truthfulness collapses when retrieved context falls below 15,000 characters, while a high-redundancy input exceeding 30,000 characters stabilizes the Hallucination-Free Rate (HFR) above 85%. In an ecological validity blind test conducted with a top-tier Chinese technology media outlet, the DeepNews system--built on a previous-generation model (DeepSeek-V3-0324)-achieved a 25% submission acceptance rate, significantly outperforming the 0% acceptance rate of zero-shot generation by a state-of-the-art (SOTA) model (GPT-5).

</details>


### [4] [PARAN: Persona-Augmented Review ANswering system on Food Delivery Review Dataset](https://arxiv.org/abs/2512.10148)
*Moonsoo Park,Jeongseok Yun,Bohyung Kim*

Main category: cs.CL

TL;DR: 提出兩階段提示框架，從短評文本推斷顯性與隱性用戶特徵，並將其融入回應生成提示，以提升個性化回覆品質


<details>
  <summary>Details</summary>
Motivation: 在用戶信息有限的領域（如外送平台），LLM常因缺乏上下文而產生通用回應，降低互動效果與參與度

Method: 兩階段提示框架：第一階段從短評推斷顯性（偏好）與隱性（人口統計、風格）特徵；第二階段將特徵融入生成提示，並調整解碼溫度以平衡多樣性與忠實度

Result: 在韓國外送平台真實數據集上評估，顯示方法能有效提升精確度、多樣性與語義一致性，增強回應的相關性與個性化

Conclusion: 人物特徵增強提示能有效提升自動回應的相關性與個性化，無需模型微調，在用戶信息有限的場景中具有實用價值

Abstract: Personalized review response generation presents a significant challenge in domains where user information is limited, such as food delivery platforms. While large language models (LLMs) offer powerful text generation capabilities, they often produce generic responses when lacking contextual user data, reducing engagement and effectiveness. In this work, we propose a two-stage prompting framework that infers both explicit (e.g., user-stated preferences) and implicit (e.g., demographic or stylistic cues) personas directly from short review texts. These inferred persona attributes are then incorporated into the response generation prompt to produce user-tailored replies. To encourage diverse yet faithful generations, we adjust decoding temperature during inference. We evaluate our method using a real-world dataset collected from a Korean food delivery app, and assess its impact on precision, diversity, and semantic consistency. Our findings highlight the effectiveness of persona-augmented prompting in enhancing the relevance and personalization of automated responses without requiring model fine-tuning.

</details>


### [5] [Unforgotten Safety: Preserving Safety Alignment of Large Language Models with Continual Learning](https://arxiv.org/abs/2512.10150)
*Lama Alssum,Hani Itani,Hasan Abed Al Kader Hammoud,Philip Torr,Adel Bibi,Bernard Ghanem*

Main category: cs.CL

TL;DR: 研究大语言模型在适应新任务时的安全性退化问题，将安全保护视为持续学习问题，通过多种CL方法有效降低攻击成功率


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的普及，安全性对齐变得越来越重要。研究发现模型在适应新任务时会出现安全性退化，这主要归因于灾难性遗忘。在用户上传数据给服务提供商进行模型定制的场景下，需要解决如何保持模型安全性的问题。

Method: 将微调过程中的安全保护问题框架化为持续学习问题，采用三种CL方法：基于正则化的方法、基于记忆的方法和模型融合方法。在两种场景下评估：良性用户数据和中毒用户数据。在三个下游任务（GSM8K、SST2、Code）和三个模型家族（LLaMA2-7B、Mistral-7B、Gemma-2B）上进行系统评估。

Result: CL方法相比标准微调始终获得更低的攻击成功率。其中DER方法在保持任务效用的同时，优于其他CL方法和现有的安全保护基线方法。这些发现在不同任务和模型家族中具有普遍性。

Conclusion: 持续学习是保护大语言模型在微调过程中安全性的实用解决方案，能够有效缓解安全性退化问题，特别是在微调即服务的场景下。

Abstract: The safety alignment of large language models (LLMs) is becoming increasingly important with their democratization. In this paper, we study the safety degradation that comes with adapting LLMs to new tasks. We attribute this safety compromise to catastrophic forgetting and frame the problem of preserving safety when fine-tuning as a continual learning (CL) problem. We consider the fine-tuning-as-a-service setup where the user uploads their data to a service provider to get a customized model that excels on the user's selected task. We adapt several CL approaches from the literature and systematically evaluate their ability to mitigate safety degradation. These include regularization-based, memory-based, and model merging approaches. We consider two scenarios, (1) benign user data and (2) poisoned user data. Our results demonstrate that CL approaches consistently achieve lower attack success rates than standard fine-tuning. Among these, DER outperforms both other CL methods and existing safety-preserving baselines while maintaining task utility. These findings generalize across three downstream tasks (GSM8K, SST2, Code) and three model families (LLaMA2-7B, Mistral-7B, Gemma-2B), establishing CL as a practical solution to preserve safety.

</details>


### [6] [AutoMedic: An Automated Evaluation Framework for Clinical Conversational Agents with Medical Dataset Grounding](https://arxiv.org/abs/2512.10195)
*Gyutaek Oh,Sangjoon Park,Byung-Hoon Kim*

Main category: cs.CL

TL;DR: AutoMedic是一个多智能体仿真框架，可将静态医疗QA数据集转换为虚拟患者档案，用于自动评估LLM作为临床对话代理的性能，使用CARE多维度指标。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在医疗领域的评估主要依赖静态QA基准，缺乏对动态交互式临床对话场景的有效评估方法，且现有评估策略单一，难以标准化衡量复杂的临床交互情境。

Method: 提出AutoMedic多智能体仿真框架，将现有静态医疗QA数据集转换为虚拟患者档案，创建真实的临床多轮对话环境，使用CARE指标（临床对话准确性、效率/策略、同理心、鲁棒性）进行多维度评估。

Result: AutoMedic被证明是有效的临床对话代理自动评估框架，经人类专家验证，为LLM在对话式医疗应用中的有效开发提供了实用指南。

Conclusion: AutoMedic解决了临床对话评估的标准化难题，提供了自动化、多维度、可扩展的评估方案，有助于推动LLM在医疗对话应用中的安全可信发展。

Abstract: Evaluating large language models (LLMs) has recently emerged as a critical issue for safe and trustworthy application of LLMs in the medical domain. Although a variety of static medical question-answering (QA) benchmarks have been proposed, many aspects remain underexplored, such as the effectiveness of LLMs in generating responses in dynamic, interactive clinical multi-turn conversation situations and the identification of multi-faceted evaluation strategies beyond simple accuracy. However, formally evaluating a dynamic, interactive clinical situation is hindered by its vast combinatorial space of possible patient states and interaction trajectories, making it difficult to standardize and quantitatively measure such scenarios. Here, we introduce AutoMedic, a multi-agent simulation framework that enables automated evaluation of LLMs as clinical conversational agents. AutoMedic transforms off-the-shelf static QA datasets into virtual patient profiles, enabling realistic and clinically grounded multi-turn clinical dialogues between LLM agents. The performance of various clinical conversational agents is then assessed based on our CARE metric, which provides a multi-faceted evaluation standard of clinical conversational accuracy, efficiency/strategy, empathy, and robustness. Our findings, validated by human experts, demonstrate the validity of AutoMedic as an automated evaluation framework for clinical conversational agents, offering practical guidelines for the effective development of LLMs in conversational medical applications.

</details>


### [7] [Multilingual VLM Training: Adapting an English-Trained VLM to French](https://arxiv.org/abs/2512.10336)
*Jules Lahmi,Alexis Roger*

Main category: cs.CL

TL;DR: 本文探讨了将英语训练的视觉语言模型扩展到其他语言的方法，发现数据集翻译是主要瓶颈，未来应关注原生语言数据收集和改进翻译策略。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型主要针对英语开发，限制了非英语用户的可访问性，需要将这些能力扩展到更广泛的语言。

Method: 比较了三种方法：基于翻译的流程、LoRA微调、以及将视觉适应与语言适应分离的两阶段微调策略，使用翻译后的多模态基准和专家人工评估进行评估。

Result: 数据集翻译是多语言VLM性能的主要瓶颈，数据质量限制了训练和评估的有效性。

Conclusion: 未来工作应聚焦于原生语言数据收集和改进翻译策略，以提升多语言视觉语言模型的性能。

Abstract: Artificial intelligence has made great progress in recent years, particularly in the development of Vision--Language Models (VLMs) that understand both visual and textual data. However, these advancements remain largely limited to English, reducing their accessibility for non--English speakers. It is essential to extend these capabilities to a broader range of languages. This paper explores the challenges of adapting an English-trained VLM to different languages. To this end, we will explore and compare different methods for their performance and computational cost. We consider a translation-based pipeline, LoRA finetuning, and a two-stage finetuning strategy that separates vision adaptation from language adaptation. To evaluate these methods, we use a combination of standard multimodal benchmarks translated into the target language and manual assessments by native experts. The results reveal that dataset translation remains a major bottleneck in multilingual VLM performance, with data quality limiting the effectiveness of training and evaluation. These findings suggest that future efforts should focus on native-language dataset collection and improved translation strategies.

</details>


### [8] [Confucius Code Agent: An Open-sourced AI Software Engineer at Industrial Scale](https://arxiv.org/abs/2512.10398)
*Zhaodong Wang,Zhenting Qi,Sherman Wong,Nathan Hu,Samuel Lin,Jun Ge,Erwin Gao,Yining Yang,Ben Maurer,Wenlin Chen,David Recordon,Yilun Du,Minlan Yu,Ying Zhang*

Main category: cs.CL

TL;DR: Confucius Code Agent (CCA) 是一个开源的工业级AI软件工程师，基于Confucius SDK构建，在SWE-Bench-Pro上达到54.3%的Resolve@1性能，显著优于现有编码代理。


<details>
  <summary>Details</summary>
Motivation: 现实世界的AI软件工程需要编码代理能够处理大规模代码库、维持跨会话的持久记忆，并协调复杂的工具链。现有开源代理在工业级工作负载上表现不足，而专有代理虽然性能强但缺乏可扩展性、可解释性和可控性。

Method: 基于Confucius SDK构建，该平台采用三个互补视角：Agent Experience (AX)、User Experience (UX)和Developer Experience (DX)。包含统一编排器（分层工作记忆）、持久笔记系统（跨会话持续学习）、模块化扩展模块（鲁棒工具使用），以及通过构建-测试-改进循环自动化配置合成的元代理。

Result: 在SWE-Bench-Pro基准测试中，CCA实现了54.3%的Resolve@1性能，达到了最先进水平，显著优于先前的编码代理。

Conclusion: Confucius SDK和CCA提供了一个透明、可扩展、可复现的AI代理基础架构，弥合了研究原型与生产级系统之间的差距，支持工业规模的代理开发和部署。

Abstract: Real-world AI software engineering demands coding agents that can reason over massive repositories, maintain durable memory across and within long sessions, and robustly coordinate complex toolchains at test time. Existing open-source coding agents provide transparency but frequently fall short when pushed to these industrial-scale workloads, while proprietary coding agents offer strong practical performance but limited extensibility, interpretability, and controllability. We present the Confucius Code Agent (CCA), an open-sourced AI software engineer that can operate at an industrial scale. CCA is built atop the Confucius SDK, an open-sourced agent development platform designed around three complementary perspectives: Agent Experience (AX), User Experience (UX), and Developer Experience (DX). The SDK introduces a unified orchestrator with hierarchical working memory for long-context reasoning, a persistent note-taking system for cross-session continual learning, and a modular extension module for robust tool use. Moreover, a meta-agent automates the synthesis, evaluation, and refinement of agent configurations through a build-test-improve loop, enabling rapid agent development on new tasks, environments, and tool stacks. Instantiated on Confucius SDK with these mechanisms, CCA delivers strong performance on real-world software engineering tasks. On SWE-Bench-Pro, CCA achieves a state-of-the-art Resolve@1 performance of 54.3%, substantially improving over prior coding agents. Together, the Confucius SDK and CCA provide a transparent, extensible, and reproducible foundation for AI agents, bridge gaps between research prototypes and production-grade systems, and support agent development and deployment at industrial scale.

</details>


### [9] [Sliding Window Attention Adaptation](https://arxiv.org/abs/2512.10411)
*Yijiong Yu,Jiale Liu,Qingyun Wu,Huazheng Wang,Ji Pei*

Main category: cs.CL

TL;DR: FA预训练的LLMs可以通过SWAA方法有效适应滑动窗口注意力，无需重新预训练，在保持长上下文性能的同时降低推理成本


<details>
  <summary>Details</summary>
Motivation: Transformer自注意力机制在长上下文推理中具有二次复杂度，成本高昂。滑动窗口注意力(SWA)可将复杂度降至线性，但直接将FA预训练的模型切换到SWA会导致严重的性能下降。研究目标是探索FA预训练的LLMs能否在不重新预训练的情况下有效适应SWA。

Method: 提出了滑动窗口注意力适应(SWAA)方法，包含五种实用技术：(1)仅在预填充阶段使用SWA；(2)保留"sink"令牌；(3)交错FA/SWA层；(4)思维链(CoT)；(5)微调。这些方法可以组合使用以实现更好的适应效果。

Result: 实验表明SWA适应是可行但非平凡的：单一方法不足够，但特定的协同组合能有效恢复原始长上下文性能。研究分析了不同SWAA配置的性能-效率权衡，并为不同场景提供了推荐方案。

Conclusion: FA预训练的LLMs可以通过精心设计的SWAA方法有效适应SWA，在保持性能的同时显著降低长上下文推理成本。该方法为实际部署提供了实用的解决方案。

Abstract: The self-attention mechanism in Transformer-based Large Language Models (LLMs) scales quadratically with input length, making long-context inference expensive. Sliding window attention (SWA) reduces this cost to linear complexity, but naively enabling complete SWA at inference-time for models pretrained with full attention (FA) causes severe long-context performance degradation due to training-inference mismatch. This makes us wonder: Can FA-pretrained LLMs be well adapted to SWA without pretraining? We investigate this by proposing Sliding Window Attention Adaptation (SWAA), a set of practical recipes that combine five methods for better adaptation: (1) applying SWA only during prefilling; (2) preserving "sink" tokens; (3) interleaving FA/SWA layers; (4) chain-of-thought (CoT); and (5) fine-tuning. Our experiments show that SWA adaptation is feasible while non-trivial: no single method suffices, yet specific synergistic combinations effectively recover the original long-context performance. We further analyze the performance-efficiency trade-offs of different SWAA configurations and provide recommended recipes for diverse scenarios. Our code is available at https://github.com/yuyijiong/sliding-window-attention-adaptation

</details>


### [10] [From Data Scarcity to Data Care: Reimagining Language Technologies for Serbian and other Low-Resource Languages](https://arxiv.org/abs/2512.10630)
*Smiljana Antonijevic Ubois*

Main category: cs.CL

TL;DR: 该研究以塞尔维亚语为例，探讨低资源语言在AI时代面临的技术发展挑战，提出基于CARE原则的Data Care框架，将偏见缓解从技术修复转变为语料库设计、注释和治理的组成部分。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型主要基于英语等主导语言训练，对低资源语言（如塞尔维亚语）的表征往往反映源语言材料的文化和语言偏见。研究旨在探索影响低资源语言技术发展的结构、历史和社会技术因素。

Method: 采用半结构化访谈法，采访了十位学者和从业者（包括语言学家、数字人文研究者和AI开发者），分析塞尔维亚语面临的挑战，并提出Data Care框架。

Result: 研究发现挑战包括：历史文本遗产破坏、浅层音译、依赖英语训练模型、数据偏见、缺乏文化特异性的数据集策展。这些导致工程优先方法，牺牲语言细微差别。

Conclusion: 提出基于CARE原则（集体利益、控制权、责任、伦理）的Data Care框架，将偏见缓解重新定位为语料库设计、注释和治理的核心部分，为在传统LLM开发复制权力不平衡和文化盲点的背景下构建包容、可持续和基于文化的语言技术提供可复制模型。

Abstract: Large language models are commonly trained on dominant languages like English, and their representation of low resource languages typically reflects cultural and linguistic biases present in the source language materials. Using the Serbian language as a case, this study examines the structural, historical, and sociotechnical factors shaping language technology development for low resource languages in the AI age. Drawing on semi structured interviews with ten scholars and practitioners, including linguists, digital humanists, and AI developers, it traces challenges rooted in historical destruction of Serbian textual heritage, intensified by contemporary issues that drive reductive, engineering first approaches prioritizing functionality over linguistic nuance. These include superficial transliteration, reliance on English-trained models, data bias, and dataset curation lacking cultural specificity. To address these challenges, the study proposes Data Care, a framework grounded in CARE principles (Collective Benefit, Authority to Control, Responsibility, and Ethics), that reframes bias mitigation from a post hoc technical fix to an integral component of corpus design, annotation, and governance, and positions Data Care as a replicable model for building inclusive, sustainable, and culturally grounded language technologies in contexts where traditional LLM development reproduces existing power imbalances and cultural blind spots.

</details>


### [11] [Cooperative Retrieval-Augmented Generation for Question Answering: Mutual Information Exchange and Ranking by Contrasting Layers](https://arxiv.org/abs/2512.10422)
*Youmin Ko,Sungjong Seo,Hyunjoon Kim*

Main category: cs.CL

TL;DR: CoopRAG是一个新颖的检索增强生成框架，通过检索器和LLM的协同工作以及检索器内部层的对比，显著提升问答任务的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型容易产生事实不准确的输出，现有的检索增强生成方法在简单和多跳问答中仍然存在检索错误和幻觉问题，需要更有效的解决方案。

Method: 提出CoopRAG框架：1)将问题分解为子问题和推理链并掩码不确定位置；2)基于增强信息检索相关文档；3)通过检索器层对比重排文档；4)LLM填充掩码位置重建推理链。

Result: 在三个多跳问答数据集和一个简单问答数据集上，CoopRAG在检索和问答性能方面均优于现有最先进方法。

Conclusion: CoopRAG通过检索器和LLM的协同工作以及检索器内部层的对比机制，有效解决了现有RAG方法的局限性，显著提升了问答任务的准确性和可靠性。

Abstract: Since large language models (LLMs) have a tendency to generate factually inaccurate output, retrieval-augmented generation (RAG) has gained significant attention as a key means to mitigate this downside of harnessing only LLMs. However, existing RAG methods for simple and multi-hop question answering (QA) are still prone to incorrect retrievals and hallucinations. To address these limitations, we propose CoopRAG, a novel RAG framework for the question answering task in which a retriever and an LLM work cooperatively with each other by exchanging informative knowledge, and the earlier and later layers of the retriever model work cooperatively with each other to accurately rank the retrieved documents relevant to a given query. In this framework, we (i) unroll a question into sub-questions and a reasoning chain in which uncertain positions are masked, (ii) retrieve the documents relevant to the question augmented with the sub-questions and the reasoning chain, (iii) rerank the documents by contrasting layers of the retriever, and (iv) reconstruct the reasoning chain by filling the masked positions via the LLM. Our experiments demonstrate that CoopRAG consistently outperforms state-of-the-art QA methods on three multi-hop QA datasets as well as a simple QA dataset in terms of both the retrieval and QA performances. Our code is available.\footnote{https://github.com/meaningful96/CoopRAG}

</details>


### [12] [T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground](https://arxiv.org/abs/2512.10430)
*Dmitrii Stoianov,Danil Taranets,Olga Tsymboi,Ramil Latypov,Almaz Dautov,Vladislav Kruglikov,Nikita Surkov,German Abramov,Pavel Gein,Dmitry Abulkhanov,Mikhail Gashkov,Viktor Zelenkovskiy,Artem Batalov,Aleksandr Medvedev,Anatolii Potapov*

Main category: cs.CL

TL;DR: T-pro 2.0是一个开源俄语大语言模型，支持混合推理和高效推理，包含模型权重、指令数据集、推理基准和推理优化工具


<details>
  <summary>Details</summary>
Motivation: 为俄语社区提供可复现、可扩展的开源LLM系统，支持俄语推理研究和高效推理应用开发

Method: 使用西里尔字母密集的分词器，采用适配的EAGLE推测解码管道降低延迟，支持直接回答和推理轨迹生成

Result: 发布了完整的开源资源：模型权重、T-Wix 50万指令语料库、T-Math推理基准、EAGLE权重，并提供公开网页演示展示推理加速效果

Conclusion: T-pro 2.0为构建和评估高效实用的俄语LLM应用提供了一个可访问的开源系统

Abstract: We introduce T-pro 2.0, an open-weight Russian LLM for hybrid reasoning and efficient inference. The model supports direct answering and reasoning-trace generation, using a Cyrillic-dense tokenizer and an adapted EAGLE speculative-decoding pipeline to reduce latency. To enable reproducible and extensible research, we release the model weights, the T-Wix 500k instruction corpus, the T-Math reasoning benchmark, and the EAGLE weights on Hugging Face. These resources allow users to study Russian-language reasoning and to extend or adapt both the model and the inference pipeline. A public web demo exposes reasoning and non-reasoning modes and illustrates the speedups achieved by our inference stack across domains. T-pro 2.0 thus serves as an accessible open system for building and evaluating efficient, practical Russian LLM applications.

</details>


### [13] [Semantic Reconstruction of Adversarial Plagiarism: A Context-Aware Framework for Detecting and Restoring "Tortured Phrases" in Scientific Literature](https://arxiv.org/abs/2512.10435)
*Agniva Maiti,Prajwal Panth,Suresh Chandra Satapathy*

Main category: cs.CL

TL;DR: SRAP框架通过两阶段方法检测并恢复对抗性抄袭中的"扭曲短语"，使用统计异常检测和语义重建技术，在科学文本中实现23.67%的恢复准确率。


<details>
  <summary>Details</summary>
Motivation: 科学文献的完整性正受到对抗性文本生成技术的威胁，特别是使用自动改写工具掩盖抄袭行为。现有检测方法依赖静态黑名单或通用语言模型，对新型混淆检测效果差，且无法确定抄袭来源。

Method: 提出SRAP框架：第一阶段使用领域特定的掩码语言模型（SciBERT）进行统计异常检测；第二阶段使用密集向量检索（FAISS）和句子级对齐（SBERT）进行基于源的语义重建。

Result: 在对抗性科学文本平行语料上的实验显示，零基线方法完全失败（0.00%恢复准确率），而检索增强方法达到23.67%恢复准确率，显著优于基线方法。静态决策边界在术语密集的科学文本中表现更稳健。

Conclusion: SRAP框架不仅能检测对抗性抄袭，还能数学上恢复原始术语，通过将混淆表达链接回最可能的源文档，实现法证分析能力。

Abstract: The integrity and reliability of scientific literature is facing a serious threat by adversarial text generation techniques, specifically from the use of automated paraphrasing tools to mask plagiarism. These tools generate "tortured phrases", statistically improbable synonyms (e.g. "counterfeit consciousness" for "artificial intelligence"), that preserve the local grammar while obscuring the original source. Most existing detection methods depend heavily on static blocklists or general-domain language models, which suffer from high false-negative rates for novel obfuscations and cannot determine the source of the plagiarized content. In this paper, we propose Semantic Reconstruction of Adversarial Plagiarism (SRAP), a framework designed not only to detect these anomalies but to mathematically recover the original terminology. We use a two-stage architecture: (1) statistical anomaly detection with a domain-specific masked language model (SciBERT) using token-level pseudo-perplexity, and (2) source-based semantic reconstruction using dense vector retrieval (FAISS) and sentence-level alignment (SBERT). Experiments on a parallel corpus of adversarial scientific text show that while zero-shot baselines fail completely (0.00 percent restoration accuracy), our retrieval-augmented approach achieves 23.67 percent restoration accuracy, significantly outperforming baseline methods. We also show that static decision boundaries are necessary for robust detection in jargon-heavy scientific text, since dynamic thresholding fails under high variance. SRAP enables forensic analysis by linking obfuscated expressions back to their most probable source documents.

</details>


### [14] [Enhancing Next-Generation Language Models with Knowledge Graphs: Extending Claude, Mistral IA, and GPT-4 via KG-BERT](https://arxiv.org/abs/2512.10440)
*Nour El Houda Ben Chaabene,Hamza Hammami*

Main category: cs.CL

TL;DR: 通过知识图谱增强大语言模型，解决事实不一致问题，提升问答和实体链接等知识密集型任务性能


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型虽然擅长自然语言处理，但缺乏结构化知识，导致事实不一致问题。需要增强模型的事实可靠性和推理能力。

Method: 通过KG-BERT将知识图谱集成到大语言模型中，实现知识图谱的融合和增强，提升模型的grounding和推理能力。

Result: 实验显示在问答、实体链接等知识密集型任务上取得显著性能提升，提高了事实可靠性。

Conclusion: 知识图谱集成能有效增强大语言模型的事实可靠性和推理能力，为下一代上下文感知的LLMs提供支持。

Abstract: Large language models (LLMs) like Claude, Mistral IA, and GPT-4 excel in NLP but lack structured knowledge, leading to factual inconsistencies. We address this by integrating Knowledge Graphs (KGs) via KG-BERT to enhance grounding and reasoning. Experiments show significant gains in knowledge-intensive tasks such as question answering and entity linking. This approach improves factual reliability and enables more context-aware next-generation LLMs.

</details>


### [15] [Decoding Student Minds: Leveraging Conversational Agents for Psychological and Learning Analysis](https://arxiv.org/abs/2512.10441)
*Nour El Houda Ben Chaabene,Hamza Hammami,Laid Kahloul*

Main category: cs.CL

TL;DR: 开发了一个心理感知对话代理，结合LLM、知识图谱增强的BERT和双向LSTM注意力机制，通过多模态数据实时分类学生的认知和情感状态，在教育环境中提升学习表现和情感健康。


<details>
  <summary>Details</summary>
Motivation: 现有教育聊天机器人通常只专注于辅导或情感支持中的单一功能，缺乏能够同时处理认知和情感状态、利用多模态数据进行实时分析的综合性系统。需要开发一个能够理解学生认知和情感状态、提供个性化支持的智能教育代理。

Method: 结合大型语言模型(LLMs)、知识图谱增强的BERT(KG-BERT)和带注意力的双向长短期记忆网络(LSTM)，通过多模态数据（文本语义、语音韵律特征、时间行为趋势）实时分类学生的认知和情感状态，推断参与度、压力和概念理解。

Result: 对大学生进行的试点研究表明，与基线方法相比，该系统提高了学习动机、减轻了压力，并带来了适度的学业提升。结果验证了整合语义推理、多模态融合和时间建模的有效性。

Conclusion: 该研究展示了整合语义推理、多模态融合和时间建模在支持适应性、以学生为中心的教育干预方面的潜力。心理感知对话代理能够同时提升学习表现和情感健康，为智能教育系统提供了有前景的方向。

Abstract: This paper presents a psychologically-aware conversational agent designed to enhance both learning performance and emotional well-being in educational settings. The system combines Large Language Models (LLMs), a knowledge graph-enhanced BERT (KG-BERT), and a bidirectional Long Short-Term Memory (LSTM) with attention to classify students' cognitive and affective states in real time. Unlike prior chatbots limited to either tutoring or affective support, our approach leverages multimodal data-including textual semantics, prosodic speech features, and temporal behavioral trends-to infer engagement, stress, and conceptual understanding. A pilot study with university students demonstrated improved motivation, reduced stress, and moderate academic gains compared to baseline methods. These results underline the promise of integrating semantic reasoning, multimodal fusion, and temporal modeling to support adaptive, student-centered educational interventions.

</details>


### [16] [Grammaticality Judgments in Humans and Language Models: Revisiting Generative Grammar with LLMs](https://arxiv.org/abs/2512.10453)
*Lars G. B. Johnsen*

Main category: cs.CL

TL;DR: LLMs通过表面形式训练能够区分语法结构差异，表明它们对句法结构敏感，而不仅仅是线性顺序。


<details>
  <summary>Details</summary>
Motivation: 传统生成语法认为语法性系统对比（如主语-助动词倒装和寄生空位许可）是内部层次语法的证据。本文测试仅通过表面形式训练的大语言模型是否能在暗示底层结构表征的方式下重现这些对比。

Method: 聚焦两个经典结构：主语-助动词倒装（测试主语边界识别）和寄生空位许可（测试抽象依赖结构）。评估GPT-4和LLaMA-3等模型，使用提示引出可接受性评分。

Result: LLMs可靠地区分两种结构中的语法和不合语法变体，表明它们对结构敏感而不仅仅是线性顺序。结构概括（区别于认知知识）从表面形式的预测训练中涌现。

Conclusion: 大语言模型通过表面形式训练发展出对句法结构的功能敏感性，无需显式编码语法知识。这表明预测性训练能够产生结构敏感性，支持LLMs具有底层句法表征能力。

Abstract: What counts as evidence for syntactic structure? In traditional generative grammar, systematic contrasts in grammaticality such as subject-auxiliary inversion and the licensing of parasitic gaps are taken as evidence for an internal, hierarchical grammar. In this paper, we test whether large language models (LLMs), trained only on surface forms, reproduce these contrasts in ways that imply an underlying structural representation.
  We focus on two classic constructions: subject-auxiliary inversion (testing recognition of the subject boundary) and parasitic gap licensing (testing abstract dependency structure). We evaluate models including GPT-4 and LLaMA-3 using prompts eliciting acceptability ratings. Results show that LLMs reliably distinguish between grammatical and ungrammatical variants in both constructions, and as such support that they are sensitive to structure and not just linear order. Structural generalizations, distinct from cognitive knowledge, emerge from predictive training on surface forms, suggesting functional sensitivity to syntax without explicit encoding.

</details>


### [17] [XDoGE: Multilingual Data Reweighting to Enhance Language Inclusivity in LLMs](https://arxiv.org/abs/2512.10545)
*Iñaki Lacunza,José Javier Saiz,Alexander Shvets,Aitor Gonzalez-Agirre,Marta Villegas*

Main category: cs.CL

TL;DR: 提出XDoGE方法优化多语言LLM训练中的语言分布，通过代理模型确定最佳语言权重，并训练了专注于伊比利亚语言的7B模型


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型过度依赖英语等高资源语言，导致在中低资源语言上表现不佳，需要优化训练数据的语言分布

Method: 扩展DoGE算法为XDoGE用于多语言设置，通过小代理模型优化语言权重，然后使用这些权重从头训练或持续预训练完整模型

Result: 实验了六种语言（英语、西班牙语、葡萄牙语、加泰罗尼亚语、加利西亚语、巴斯克语），研究了数据重复和欠采样的影响，并发布了IberianLLM-7B-Instruct模型

Conclusion: 提出的XDoGE方法能有效优化多语言LLM的语言分布，改善了中低资源语言的性能，并成功训练了专注于伊比利亚语言的模型

Abstract: Current large language models (LLMs) are trained on massive amounts of text data, primarily from a few dominant languages. Studies suggest that this over-reliance on high-resource languages, such as English, hampers LLM performance in mid- and low-resource languages. To mitigate this problem, we propose to (i) optimize the language distribution by training a small proxy model within a domain-reweighing DoGE algorithm that we extend to XDoGE for a multilingual setup, and (ii) rescale the data and train a full-size model with the established language weights either from scratch or within a continual pre-training phase (CPT). We target six languages possessing a variety of geographic and intra- and inter-language-family relations, namely, English and Spanish (high-resource), Portuguese and Catalan (mid-resource), Galician and Basque (low-resource). We experiment with Salamandra-2b, which is a promising model for these languages. We investigate the effects of substantial data repetition on minor languages and under-sampling on dominant languages using the IberoBench framework for quantitative evaluation. Finally, we release a new promising IberianLLM-7B-Instruct model centering on Iberian languages and English that we pretrained from scratch and further improved using CPT with the XDoGE weights.

</details>


### [18] [Causal Reasoning Favors Encoders: On The Limits of Decoder-Only Models](https://arxiv.org/abs/2512.10561)
*Amartya Roy,Elamparithy M,Kripabandhu Ghosh,Ponnurangam Kumaraguru,Adrian de Wynter*

Main category: cs.CL

TL;DR: 本文研究了不同架构大语言模型在因果推理任务中的表现，发现仅靠上下文学习（ICL）不足以进行可靠的因果推理，而经过微调的编码器和编码器-解码器架构在成本效益和鲁棒性方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 尽管上下文学习推动了大型语言模型的发展，但其在因果推理中的作用和性能尚不明确。因果推理需要多跳组合和严格的合取控制，而依赖输入的虚假词汇关系可能导致误导性结果。

Method: 比较了编码器、编码器-解码器和仅解码器架构的微调版本，在零样本和少样本上下文学习设置下，分别在自然语言和非自然语言场景中进行测试。

Result: 上下文学习本身不足以进行可靠的因果推理，经常过度关注不相关的输入特征。仅解码器模型对分布变化特别脆弱，而微调的编码器和编码器-解码器模型在各种测试中（包括非自然语言场景）表现出更强的泛化能力。仅解码器架构只有在规模非常大时才能匹配或超越前两种架构。

Conclusion: 对于成本效益高、短期鲁棒的因果推理任务，经过针对性微调的编码器或编码器-解码器架构是更优选择。

Abstract: In context learning (ICL) underpins recent advances in large language models (LLMs), although its role and performance in causal reasoning remains unclear. Causal reasoning demands multihop composition and strict conjunctive control, and reliance on spurious lexical relations of the input could provide misleading results. We hypothesize that, due to their ability to project the input into a latent space, encoder and encoder decoder architectures are better suited for said multihop conjunctive reasoning versus decoder only models. To do this, we compare fine-tuned versions of all the aforementioned architectures with zero and few shot ICL in both natural language and non natural language scenarios. We find that ICL alone is insufficient for reliable causal reasoning, often overfocusing on irrelevant input features. In particular, decoder only models are noticeably brittle to distributional shifts, while finetuned encoder and encoder decoder models can generalize more robustly across our tests, including the non natural language split. Both architectures are only matched or surpassed by decoder only architectures at large scales. We conclude by noting that for cost effective, short horizon robust causal reasoning, encoder or encoder decoder architectures with targeted finetuning are preferable.

</details>


### [19] [RoleRMBench & RoleRM: Towards Reward Modeling for Profile-Based Role Play in Dialogue Systems](https://arxiv.org/abs/2512.10575)
*Hang Ding,Qiming Feng,Dongqi Liu,Qi Zhao,Tao Yao,Shuo Wang,Dongsheng Chen,Jian Li,Zhenye Gan,Jiangning Zhang,Chengjie Wang,Yabiao Wang*

Main category: cs.CL

TL;DR: RoleRMBench：首个角色扮演对话奖励建模基准，揭示现有奖励模型在主观开放领域表现严重退化，提出RoleRM模型通过连续隐式偏好提升叙事一致性和风格保真度


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型在主观开放领域（如角色扮演）表现严重退化，难以捕捉基于角色的细微人类判断，需要专门针对角色扮演对话的奖励建模基准和方法

Method: 1) 提出RoleRMBench基准，覆盖叙事管理、角色一致性、参与度等7个细粒度能力；2) 提出RoleRM模型，采用连续隐式偏好（CIP）方法，将主观评估重构为多结构化策略下的连续一致成对监督

Result: 评估显示通用奖励模型与人类判断存在显著差距，尤其在叙事和风格维度；RoleRM超越强开源和闭源奖励模型平均超过24%，在叙事连贯性和风格保真度方面取得显著提升

Conclusion: 连续偏好表示和标注一致性对于主观对齐至关重要，为以人为本的对话系统的主观对齐奠定基础，RoleRMBench和RoleRM为角色扮演领域的奖励建模提供了系统评估框架

Abstract: Reward modeling has become a cornerstone of aligning large language models (LLMs) with human preferences. Yet, when extended to subjective and open-ended domains such as role play, existing reward models exhibit severe degradation, struggling to capture nuanced and persona-grounded human judgments. To address this gap, we introduce RoleRMBench, the first systematic benchmark for reward modeling in role-playing dialogue, covering seven fine-grained capabilities from narrative management to role consistency and engagement. Evaluation on RoleRMBench reveals large and consistent gaps between general-purpose reward models and human judgment, particularly in narrative and stylistic dimensions. We further propose RoleRM, a reward model trained with Continuous Implicit Preferences (CIP), which reformulates subjective evaluation as continuous consistent pairwise supervision under multiple structuring strategies. Comprehensive experiments show that RoleRM surpasses strong open- and closed-source reward models by over 24% on average, demonstrating substantial gains in narrative coherence and stylistic fidelity. Our findings highlight the importance of continuous preference representation and annotation consistency, establishing a foundation for subjective alignment in human-centered dialogue systems.

</details>


### [20] [AgriGPT-Omni: A Unified Speech-Vision-Text Framework for Multilingual Agricultural Intelligence](https://arxiv.org/abs/2512.10624)
*Bo Yang,Lanfei Feng,Yunkui Chen,Yu Zhang,Jianyu Zhang,Xiao Xu,Nueraili Aierken,Shijian Li*

Main category: cs.CL

TL;DR: AgriGPT-Omni是一个农业全模态框架，整合语音、视觉和文本，通过三阶段训练范式实现跨语言和模态的统一推理，显著优于通用基线模型。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在农业应用中面临三大限制：缺乏多语言语音数据、统一的多模态架构以及全面的评估基准，这阻碍了农业智能化的发展。

Method: 1) 构建可扩展的数据合成与收集管道，创建了最大的农业语音数据集；2) 采用三阶段训练范式：文本知识注入、渐进式多模态对齐和GRPO强化学习；3) 提出首个农业三模态基准AgriBench-Omni-2K。

Result: AgriGPT-Omni在多语言多模态推理和真实世界语音理解方面显著优于通用基线模型，包含492K合成和1.4K真实语音样本的六语言数据集，以及覆盖多样化任务的评估基准。

Conclusion: 该工作为农业AI提供了首个全模态框架、最大语音数据集和综合评估基准，将促进可重复研究、包容性农业智能和低资源地区的可持续AI发展。

Abstract: Despite rapid advances in multimodal large language models, agricultural applications remain constrained by the lack of multilingual speech data, unified multimodal architectures, and comprehensive evaluation benchmarks. To address these challenges, we present AgriGPT-Omni, an agricultural omni-framework that integrates speech, vision, and text in a unified framework. First, we construct a scalable data synthesis and collection pipeline that converts agricultural texts and images into training data, resulting in the largest agricultural speech dataset to date, including 492K synthetic and 1.4K real speech samples across six languages. Second, based on this, we train the first agricultural omni-model via a three-stage paradigm: textual knowledge injection, progressive multimodal alignment, and GRPO-based reinforcement learning, enabling unified reasoning across languages and modalities. Third, we propose AgriBench-Omni-2K, the first tri-modal benchmark for agriculture, covering diverse speech-vision-text tasks and multilingual slices, with standardized protocols and reproducible tools. Experiments show that AgriGPT-Omni significantly outperforms general-purpose baselines on multilingual and multimodal reasoning as well as real-world speech understanding. All models, data, benchmarks, and code will be released to promote reproducible research, inclusive agricultural intelligence, and sustainable AI development for low-resource regions.

</details>


### [21] [Textual Data Bias Detection and Mitigation - An Extensible Pipeline with Experimental Evaluation](https://arxiv.org/abs/2512.10734)
*Rebekka Görge,Sujan Sai Gannamaneni,Tabea Naeven,Hammam Abdelwahab,Héctor Allende-Cid,Armin B. Cremers,Lennard Helmer,Michael Mock,Anna Schmitz,Songkai Xue,Elif Yildirir,Maximilian Poretschkin,Stefan Wrobel*

Main category: cs.CL

TL;DR: 提出一个包含四个组件的全面数据偏见检测与缓解管道，用于处理表示偏见和显式刻板印象，并通过实验验证数据去偏见效果，但发现模型偏见评估存在差距。


<details>
  <summary>Details</summary>
Motivation: 训练大语言模型的文本数据存在多方面偏见，包括有害语言和倾斜的人口分布。虽然欧盟AI法案等法规要求识别和缓解针对受保护群体的数据偏见，但缺乏实践指导和操作化方法。

Method: 提出四组件管道：1) 基于质量标准的LLM生成相关群体标签词表；2) 使用人口统计表示分数量化表示偏见；3) 基于社会语言学过滤检测和缓解刻板印象；4) 通过语法和上下文感知的反事实数据增强补偿表示偏见。

Result: 在性别、宗教和年龄等敏感属性上评估：1) 数据去偏见方面，成功减少了文本数据集中的表示偏见和显式刻板印象；2) 模型偏见方面，在去偏见数据上微调的LLMs（0.6B-8B参数）在偏见基准测试中并未一致表现出改进，暴露了当前评估方法的差距。

Conclusion: 虽然提出的管道能有效减少数据偏见，但数据去偏见不一定能转化为模型偏见的减少，这揭示了当前评估方法的局限性，并强调需要针对性的数据操作来解决显现的模型偏见。

Abstract: Textual data used to train large language models (LLMs) exhibits multifaceted bias manifestations encompassing harmful language and skewed demographic distributions. Regulations such as the European AI Act require identifying and mitigating biases against protected groups in data, with the ultimate goal of preventing unfair model outputs. However, practical guidance and operationalization are lacking. We propose a comprehensive data bias detection and mitigation pipeline comprising four components that address two data bias types, namely representation bias and (explicit) stereotypes for a configurable sensitive attribute. First, we leverage LLM-generated word lists created based on quality criteria to detect relevant group labels. Second, representation bias is quantified using the Demographic Representation Score. Third, we detect and mitigate stereotypes using sociolinguistically informed filtering. Finally, we compensate representation bias through Grammar- and Context-Aware Counterfactual Data Augmentation. We conduct a two-fold evaluation using the examples of gender, religion and age. First, the effectiveness of each individual component on data debiasing is evaluated through human validation and baseline comparison. The findings demonstrate that we successfully reduce representation bias and (explicit) stereotypes in a text dataset. Second, the effect of data debiasing on model bias reduction is evaluated by bias benchmarking of several models (0.6B-8B parameters), fine-tuned on the debiased text dataset. This evaluation reveals that LLMs fine-tuned on debiased data do not consistently show improved performance on bias benchmarks, exposing critical gaps in current evaluation methodologies and highlighting the need for targeted data manipulation to address manifested model bias.

</details>


### [22] [Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving](https://arxiv.org/abs/2512.10739)
*Songyang Gao,Yuzhe Gu,Zijian Wu,Lingkai Kong,Wenwei Zhang,Zhongrui Cai,Fan Zheng,Tianyou Ma,Junhao Shen,Haiteng Zhao,Duanyang Zhang,Huilun Zhang,Kuikun Liu,Chengqi Lyu,Yanhui Duan,Chiyu Chen,Ningsheng Ma,Jianfei Gao,Han Lyu,Dahua Lin,Kai Chen*

Main category: cs.CL

TL;DR: OPV是一种结合结果验证和过程验证的新方法，通过总结长推理链的结果来验证推理过程，实现准确高效的大规模标注，在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前的结果验证器无法检查长推理链中的不可靠中间步骤，而过程验证器由于人工标注成本过高导致高质量标注稀缺，难以可靠检测复杂长推理链中的错误。

Method: 提出基于结果的流程验证器OPV，通过总结长推理链的结果来验证推理过程。采用迭代主动学习框架，通过专家标注逐步提升验证能力，使用拒绝微调和RLVR进行训练。

Result: 在held-out基准测试中达到83.1的F1分数，优于Qwen3-Max-Preview的76.3。能有效检测合成数据集中的假阳性，与专家评估高度一致。与策略模型协作时显著提升性能，如将DeepSeek-R1-Distill-Qwen-32B在AIME2025上的准确率从55.2%提升到73.3%。

Conclusion: OPV通过结合结果验证和过程验证的优势，实现了准确高效的推理验证，解决了长推理链验证的挑战，具有广泛的适用性和显著的性能提升。

Abstract: Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the \textbf{O}utcome-based \textbf{P}rocess \textbf{V}erifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out \textsc{\thisbench}, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2\% to 73.3\% on AIME2025 as the compute budget scales.

</details>


### [23] [TRIDENT: A Redundant Architecture for Caribbean-Accented Emergency Speech Triage](https://arxiv.org/abs/2512.10741)
*Elroy Galbraith,Chadwick Sutherland,Donahue Morgan*

Main category: cs.CL

TL;DR: TRIDENT是一个三层调度员支持系统，旨在解决加勒比口音英语在紧急语音识别中的性能下降问题，通过结合口音调优的ASR、本地实体提取和生物声学压力检测，确保加勒比人群能够公平获得国家分流协议服务。


<details>
  <summary>Details</summary>
Motivation: 紧急语音识别系统在非标准英语变体（特别是加勒比口音）上存在系统性性能下降，导致加勒比人群在紧急服务中面临关键的服务缺口。需要开发能够处理口音变化的系统，确保所有人都能平等获得既定的分流协议（如ESI常规操作和START大规模伤亡事件协议）。

Method: TRIDENT采用三层架构：1) 加勒比口音调优的自动语音识别(ASR)；2) 基于大语言模型的本地实体提取；3) 生物声学压力检测。系统为调度员提供三个互补信号：转录置信度、结构化临床实体和声音压力指标。关键洞察是低ASR置信度可作为队列优先级信号，特别是与升高的声音压力指标结合时。

Result: 论文提出了TRIDENT的架构设计、基于心理语言学研究的理论基础（压力诱导的语码转换），以及灾难场景下离线操作的部署考虑。这建立了一个口音弹性的紧急AI框架，确保加勒比声音能够公平获得国家分流协议。对加勒比紧急呼叫的实证验证是未来的工作。

Conclusion: TRIDENT为口音弹性的紧急AI建立了一个框架，通过将低ASR置信度重新定义为有价值的优先级信号，并结合实体提取和压力检测，确保加勒比人群在紧急情况下能够获得公平的服务。该系统在灾难期间支持离线操作，填补了当前紧急语音识别系统的关键服务缺口。

Abstract: Emergency speech recognition systems exhibit systematic performance degradation on non-standard English varieties, creating a critical gap in services for Caribbean populations. We present TRIDENT (Transcription and Routing Intelligence for Dispatcher-Empowered National Triage), a three-layer dispatcher-support architecture designed to structure emergency call inputs for human application of established triage protocols (the ESI for routine operations and START for mass casualty events), even when automatic speech recognition fails.
  The system combines Caribbean-accent-tuned ASR, local entity extraction via large language models, and bio-acoustic distress detection to provide dispatchers with three complementary signals: transcription confidence, structured clinical entities, and vocal stress indicators. Our key insight is that low ASR confidence, rather than representing system failure, serves as a valuable queue prioritization signal -- particularly when combined with elevated vocal distress markers indicating a caller in crisis whose speech may have shifted toward basilectal registers. A complementary insight drives the entity extraction layer: trained responders and composed bystanders may report life-threatening emergencies without elevated vocal stress, requiring semantic analysis to capture clinical indicators that paralinguistic features miss.
  We describe the architectural design, theoretical grounding in psycholinguistic research on stress-induced code-switching, and deployment considerations for offline operation during disaster scenarios. This work establishes a framework for accent-resilient emergency AI that ensures Caribbean voices receive equitable access to established national triage protocols. Empirical validation on Caribbean emergency calls remains future work.

</details>


### [24] [OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification](https://arxiv.org/abs/2512.10756)
*Zijian Wu,Lingkai Kong,Wenwei Zhang,Songyang Gao,Yuzhe Gu,Zhongrui Cai,Tianyou Ma,Yuhong Liu,Zhi Wang,Runyuan Ma,Guangyu Wang,Wei Li,Conghui He,Dahua Lin,Kai Chen*

Main category: cs.CL

TL;DR: 提出OPV验证器，通过总结长思维链的结果来验证推理过程，结合主动学习和专家标注实现高效准确验证，在多个基准上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于结果的验证器无法检查长思维链中的中间步骤，而基于过程的验证器由于高质量标注成本高昂，难以可靠检测复杂长思维链中的错误。

Method: 提出基于结果的流程验证器OPV，通过总结长思维链的结果来验证推理过程。采用迭代主动学习框架，通过专家标注不确定案例，使用拒绝微调和RLVR训练新OPV。

Result: 在OPV-Bench上取得83.1的F1分数，优于Qwen3-Max-Preview的76.3。能有效检测合成数据集中的假阳性，与专家评估一致。与策略模型协作时显著提升性能，如将DeepSeek-R1-Distill-Qwen-32B在AIME2025上的准确率从55.2%提升到73.3%。

Conclusion: OPV验证器通过结合结果总结和过程验证，实现了对长思维链推理的高效准确验证，并通过主动学习框架降低了标注成本，在多个任务上展现出优越性能和广泛适用性。

Abstract: Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out OPV-Bench, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2% to 73.3% on AIME2025 as the compute budget scales.

</details>


### [25] [Grow Up and Merge: Scaling Strategies for Efficient Language Adaptation](https://arxiv.org/abs/2512.10772)
*Kevin Glocker,Kätriin Kukk,Romina Oji,Marcel Bollmann,Marco Kuhlmann,Jenny Kunz*

Main category: cs.CL

TL;DR: 研究发现通过扩大模型规模可以更有效地适应新语言，相比传统持续预训练，扩大规模的模型能用更少的目标语言数据达到更好效果，同时减少灾难性遗忘，但合并语言特定模型仍不如联合多语言训练。


<details>
  <summary>Details</summary>
Motivation: 当前大规模多语言模型在中等和低资源语言上表现仍不如语言特定模型，尤其是在较小模型规模下。需要探索更有效的语言适应策略。

Method: 通过FLOP匹配的模型进行全面的规模扩展消融实验，比较扩大英语基础模型与标准持续预训练的效果，并探索合并语言特定模型构建模块化多语言系统。

Result: 一旦接触足够的目标语言数据，扩大规模的模型能够匹配甚至超越使用更多数据进行持续预训练的小模型，显示规模扩展对数据效率的益处。扩大规模还有助于保持基础模型的英语能力，减少灾难性遗忘。合并语言特定模型的效果仍不如联合多语言训练，但扩大规模的合并表现优于小模型合并。

Conclusion: 规模扩展是适应新语言的有效策略，能提高数据效率和减少灾难性遗忘。虽然合并语言特定模型构建多语言系统仍有改进空间，但规模扩展能提升合并效果，未来可通过专门的语言级集成合并方法进一步改进。

Abstract: Achieving high-performing language models which include medium- and lower-resource languages remains a challenge. Massively multilingual models still underperform compared to language-specific adaptations, especially at smaller model scales. In this work, we investigate scaling as an efficient strategy for adapting pretrained models to new target languages. Through comprehensive scaling ablations with approximately FLOP-matched models, we test whether upscaling an English base model enables more effective and resource-efficient adaptation than standard continued pretraining. We find that, once exposed to sufficient target-language data, larger upscaled models can match or surpass the performance of smaller models continually pretrained on much more data, demonstrating the benefits of scaling for data efficiency. Scaling also helps preserve the base model's capabilities in English, thus reducing catastrophic forgetting. Finally, we explore whether such scaled, language-specific models can be merged to construct modular and flexible multilingual systems. We find that while merging remains less effective than joint multilingual training, upscaled merges perform better than smaller ones. We observe large performance differences across merging methods, suggesting potential for improvement through merging approaches specialized for language-level integration.

</details>


### [26] [Script Gap: Evaluating LLM Triage on Indian Languages in Native vs Roman Scripts in a Real World Setting](https://arxiv.org/abs/2512.10780)
*Manurag Khullar,Utkarsh Desai,Poorva Malviya,Aman Dalmia,Zheyuan Ryan Shi*

Main category: cs.CL

TL;DR: LLMs在印度临床应用中表现不佳，特别是处理罗马化文本时性能下降5-12个F1点，可能导致近200万次分诊错误


<details>
  <summary>Details</summary>
Motivation: 印度语言使用者经常使用罗马化文本而非原生文字进行交流，但现有研究很少使用真实数据评估这种拼写变化对LLMs在临床应用中可靠性的影响

Method: 在母体和新生儿医疗分诊领域，使用包含五种印度语言和尼泊尔语的真实用户查询数据集，对领先的LLMs进行基准测试，比较原生文字和罗马化文本的性能差异

Result: 罗马化消息导致性能持续下降，F1分数比原生文字低5-12点；在印度合作医疗机构中，这种差距可能导致近200万次额外分诊错误；LLMs通常能正确推断罗马化查询的语义意图，但最终分类输出在罗马化输入存在拼写噪声时仍然脆弱

Conclusion: 研究揭示了基于LLMs的健康系统中一个关键的安全盲点：看似理解罗马化输入的模型可能仍然无法可靠地对其采取行动，这对高风险的临床应用构成安全隐患

Abstract: Large Language Models (LLMs) are increasingly deployed in high-stakes clinical applications in India. In many such settings, speakers of Indian languages frequently communicate using romanized text rather than native scripts, yet existing research rarely evaluates this orthographic variation using real-world data. We investigate how romanization impacts the reliability of LLMs in a critical domain: maternal and newborn healthcare triage. We benchmark leading LLMs on a real-world dataset of user-generated queries spanning five Indian languages and Nepali. Our results reveal consistent degradation in performance for romanized messages, with F1 scores trailing those of native scripts by 5-12 points. At our partner maternal health organization in India, this gap could cause nearly 2 million excess errors in triage. Crucially, this performance gap by scripts is not due to a failure in clinical reasoning. We demonstrate that LLMs often correctly infer the semantic intent of romanized queries. Nevertheless, their final classification outputs remain brittle in the presence of orthographic noise in romanized inputs. Our findings highlight a critical safety blind spot in LLM-based health systems: models that appear to understand romanized input may still fail to act on it reliably.

</details>


### [27] [The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality](https://arxiv.org/abs/2512.10791)
*Aileen Cheng,Alon Jacovi,Amir Globerson,Ben Golan,Charles Kwong,Chris Alberti,Connie Tao,Eyal Ben-David,Gaurav Singh Tomar,Lukas Haas,Yonatan Bitton,Adam Bloniarz,Aijun Bai,Andrew Wang,Anfal Siddiqui,Arturo Bajuelos Castillo,Aviel Atias,Chang Liu,Corey Fry,Daniel Balle,Deepanway Ghosal,Doron Kukliansky,Dror Marcus,Elena Gribovskaya,Eran Ofek,Honglei Zhuang,Itay Laish,Jan Ackermann,Lily Wang,Meg Risdal,Megan Barnes,Michael Fink,Mohamed Amin,Moran Ambar,Natan Potikha,Nikita Gupta,Nitzan Katz,Noam Velan,Ofir Roval,Ori Ram,Polina Zablotskaia,Prathamesh Bang,Priyanka Agrawal,Rakesh Ghiya,Sanjay Ganapathy,Simon Baumgartner,Sofia Erell,Sushant Prakash,Thibault Sellam,Vikram Rao,Xuanhui Wang,Yaroslav Akulov,Yulong Yang,Zhen Yang,Zhixin Lai,Zhongru Wu,Anca Dragan,Avinatan Hassidim,Fernando Pereira,Slav Petrov,Srinivasan Venkatachary,Tulsee Doshi,Yossi Matias,Sasha Goldshtein,Dipanjan Das*

Main category: cs.CL

TL;DR: FACTS Leaderboard是一个综合评估语言模型事实准确性的在线基准套件，包含四个子基准：多模态、参数化、搜索和基于文档的生成，通过自动化评分模型全面衡量模型的事实性表现。


<details>
  <summary>Details</summary>
Motivation: 现有的事实性评估方法往往只关注特定场景，缺乏对语言模型在不同情境下生成事实准确文本能力的全面评估。需要建立一个综合性的基准来全面衡量模型的事实性能力。

Method: 开发了包含四个子基准的评估套件：1) FACTS Multimodal评估图像问题回答的事实性；2) FACTS Parametric评估模型内部参数的世界知识；3) FACTS Search评估使用搜索API的信息寻求场景；4) FACTS Grounding (v2)评估长文本生成是否基于给定文档。使用自动化评分模型进行评分，最终得分为四个子基准的平均值。

Result: 创建了一个全面的事实性评估框架，包含公开和私有数据集以保护完整性，提供了在线访问平台（Kaggle），并将持续维护更新。

Conclusion: FACTS Leaderboard提供了一个全面、平衡且稳健的语言模型事实性评估框架，能够从多个维度综合评估模型的事实准确性，为模型开发和评估提供了重要工具。

Abstract: We introduce The FACTS Leaderboard, an online leaderboard suite and associated set of benchmarks that comprehensively evaluates the ability of language models to generate factually accurate text across diverse scenarios. The suite provides a holistic measure of factuality by aggregating the performance of models on four distinct sub-leaderboards: (1) FACTS Multimodal, which measures the factuality of responses to image-based questions; (2) FACTS Parametric, which assesses models' world knowledge by answering closed-book factoid questions from internal parameters; (3) FACTS Search, which evaluates factuality in information-seeking scenarios, where the model must use a search API; and (4) FACTS Grounding (v2), which evaluates whether long-form responses are grounded in provided documents, featuring significantly improved judge models. Each sub-leaderboard employs automated judge models to score model responses, and the final suite score is an average of the four components, designed to provide a robust and balanced assessment of a model's overall factuality. The FACTS Leaderboard Suite will be actively maintained, containing both public and private splits to allow for external participation while guarding its integrity. It can be found at https://www.kaggle.com/benchmarks/google/facts .

</details>


### [28] [LabelFusion: Learning to Fuse LLMs and Transformer Classifiers for Robust Text Classification](https://arxiv.org/abs/2512.10793)
*Michael Schlee,Christoph Weisser,Timo Kivimäki,Melchizedek Mashiku,Benjamin Saefken*

Main category: cs.CL

TL;DR: LabelFusion是一个融合集成框架，通过结合传统Transformer分类器和大型语言模型，实现准确且成本感知的文本分类，支持多类别和多标签任务。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer分类器（如RoBERTa）和大型语言模型（如GPT、Gemini）各有优势，前者计算高效但可能缺乏深度推理能力，后者推理能力强但成本高。需要一种方法能结合两者优势，在保证准确性的同时控制成本和延迟。

Method: 1. 将传统Transformer分类器的嵌入向量与LLM生成的每类得分（通过结构化提示工程获得）拼接；2. 将拼接后的联合表示输入到紧凑的多层感知机（FusionMLP）中生成最终预测；3. 提供简单高层接口（AutoFusionClassifier）和灵活API，支持端到端训练。

Result: 在多个数据集上取得优异表现：AG News达到92.4%准确率，Reuters 21578（10类主题分类）达到92.3%准确率。框架能够在准确性、延迟和成本之间实现实用权衡。

Conclusion: LabelFusion通过融合传统Transformer分类器和LLM的优势，提供了一种鲁棒、高效的文本分类解决方案，在保持高性能的同时允许用户根据实际需求在准确性、延迟和成本之间进行权衡。

Abstract: LabelFusion is a fusion ensemble for text classification that learns to combine a traditional transformer-based classifier (e.g., RoBERTa) with one or more Large Language Models (LLMs such as OpenAI GPT, Google Gemini, or DeepSeek) to deliver accurate and cost-aware predictions across multi-class and multi-label tasks. The package provides a simple high-level interface (AutoFusionClassifier) that trains the full pipeline end-to-end with minimal configuration, and a flexible API for advanced users. Under the hood, LabelFusion integrates vector signals from both sources by concatenating the ML backbone's embeddings with the LLM-derived per-class scores -- obtained through structured prompt-engineering strategies -- and feeds this joint representation into a compact multi-layer perceptron (FusionMLP) that produces the final prediction. This learned fusion approach captures complementary strengths of LLM reasoning and traditional transformer-based classifiers, yielding robust performance across domains -- achieving 92.4% accuracy on AG News and 92.3% on 10-class Reuters 21578 topic classification -- while enabling practical trade-offs between accuracy, latency, and cost.

</details>


### [29] [Quantifying Emotional Tone in Tolkien's The Hobbit: Dialogue Sentiment Analysis with RegEx, NRC-VAD, and Python](https://arxiv.org/abs/2512.10865)
*Lilin Qiu*

Main category: cs.CL

TL;DR: 使用计算文本分析研究《霍比特人》对话的情感基调，发现整体积极平静，随着故事发展角色能动性增强，情感在紧张与舒适间循环


<details>
  <summary>Details</summary>
Motivation: 探索如何通过计算文本分析方法揭示文学作品中的微妙情感结构，特别是分析托尔金《霍比特人》对话的情感节奏和模式

Method: 使用正则表达式提取对话文本，进行预处理后，采用NRC-VAD词典对情感维度进行量化评分，并通过情感轨迹图和词云进行可视化分析

Result: 对话整体保持积极（高效价）和冷静（低唤醒）的基调，随着故事进展角色能动性（支配性）逐渐增强，情感在危险紧张与幽默安慰之间循环平衡

Conclusion: 计算工具与文学解读相结合能够揭示文学作品中的微妙情感结构，展示了托尔金如何通过情感节奏和调节来塑造《霍比特人》的叙事

Abstract: This study analyzes the emotional tone of dialogue in J. R. R. Tolkien's The Hobbit (1937) using computational text analysis. Dialogue was extracted with regular expressions, then preprocessed, and scored using the NRC-VAD lexicon to quantify emotional dimensions. The results show that the dialogue maintains a generally positive (high valence) and calm (low arousal) tone, with a gradually increasing sense of agency (dominance) as the story progresses. These patterns reflect the novel's emotional rhythm: moments of danger and excitement are regularly balanced by humor, camaraderie, and relief. Visualizations -- including emotional trajectory graphs and word clouds -- highlight how Tolkien's language cycles between tension and comfort. By combining computational tools with literary interpretation, this study demonstrates how digital methods can uncover subtle emotional structures in literature, revealing the steady rhythm and emotional modulation that shape the storytelling in The Hobbit.

</details>


### [30] [Computational emotion analysis with multimodal LLMs: Current evidence on an emerging methodological opportunity](https://arxiv.org/abs/2512.10882)
*Hauke Licht*

Main category: cs.CL

TL;DR: 评估多模态大语言模型在政治视频情绪唤醒分析中的表现，发现在理想条件下表现良好，但在真实议会辩论中效果不佳


<details>
  <summary>Details</summary>
Motivation: 情绪在政治沟通中至关重要，多模态生成AI为情绪分析带来新机遇，但缺乏对其有效性的实证证据

Method: 使用两个互补的人类标注视频数据集，评估当前多模态大语言模型在视频情绪唤醒分析中的表现

Result: 理想条件下mLLMs情绪唤醒评分高度可靠且无人口统计偏差，但在真实议会辩论录音中表现不佳，可能影响下游统计推断

Conclusion: 需要持续深入评估新兴生成AI方法在政治分析中的应用，并提供了可复制的评估框架

Abstract: Emotions are central to politics and analyzing their role in political communication has a long tradition. As research increasingly leverages audio-visual materials to analyze the display of emotions, the emergence of multimodal generative AI promises great advances. However, we lack evidence about the effectiveness of multimodal AI in emotion analysis. This paper addresses this gap by evaluating current multimodal large language models (mLLMs) in video-based analysis of emotional arousal in two complementary data sets of human-labeled video recordings. I find that under ideal circumstances, mLLMs' emotional arousal ratings are highly reliable and show little to know indication of demographic bias. However, in recordings of speakers in real-world parliamentary debates, mLLMs' arousal ratings fail to deliver on this promise with potential negative consequences for downstream statistical inferences. This study therefore underscores the need for continued, thorough evaluation of emerging generative AI methods in political analysis and contributes a suitable replicable framework.

</details>


### [31] [Unsupervised Acquisition of Discrete Grammatical Categories](https://arxiv.org/abs/2503.18702)
*David Ph. Shakouri,Crit Cremers,Niels O. Schiller*

Main category: cs.CL

TL;DR: 该研究构建了一个计算实验室环境，通过多智能体系统（母语模型和子语模型）模拟语言习得过程，子智能体仅通过母语模型生成的语言示例学习抽象语法知识，使用层次聚类分析从输入数据中提取离散语法规则。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索如何通过计算模型模拟人类语言习得过程，特别是子代如何仅通过观察母代生成的语言示例（而非直接获取内部知识）来获得抽象语法知识，验证统计分析方法能否从语言数据中提取类似自然语言的语法范畴。

Method: 方法包括：1）构建多智能体系统（母语模型和子语模型）；2）子智能体仅访问母智能体生成的语言示例；3）应用层次凝聚聚类分析对母语模型连续生成的语句进行模式分析；4）从统计模式中提取离散语法规则并加入子语模型的语法知识库；5）使用训练数据确定计算实验室环境的参数配置，并在测试集上进行验证。

Result: 实验结果表明：1）系统能够从输入数据的统计模式中获得抽象语法知识；2）层次聚类分析能够产生类似语言学家为自然语言提出的语法范畴结构；3）成功获取了非平凡的语法知识；4）在训练数据上确定的参数配置在测试集实验中同样有效，能够获得非平凡的语法范畴。

Conclusion: 该研究建立了一个有效的计算实验室环境，证明通过多智能体系统和统计分析方法，子代智能体仅通过观察母代生成的语言示例就能获得抽象语法知识，这为理解人类语言习得机制提供了计算模型支持，并验证了从语言数据中提取语法范畴的可行性。

Abstract: This article presents experiments performed using a computational laboratory environment for language acquisition experiments. It implements a multi-agent system consisting of two agents: an adult language model and a daughter language model that aims to learn the mother language. Crucially, the daughter agent does not have access to the internal knowledge of the mother language model but only to the language exemplars the mother agent generates. These experiments illustrate how this system can be used to acquire abstract grammatical knowledge. We demonstrate how statistical analyses of patterns in the input data corresponding to grammatical categories yield discrete grammatical rules. These rules are subsequently added to the grammatical knowledge of the daughter language model. To this end, hierarchical agglomerative cluster analysis was applied to the utterances consecutively generated by the mother language model. It is argued that this procedure can be used to acquire structures resembling grammatical categories proposed by linguists for natural languages. Thus, it is established that non-trivial grammatical knowledge has been acquired. Moreover, the parameter configuration of this computational laboratory environment determined using training data generated by the mother language model is validated in a second experiment with a test set similarly resulting in the acquisition of non-trivial categories.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [32] [ExaCraft: Dynamic Learning Context Adaptation for Personalized Educational Examples](https://arxiv.org/abs/2512.09931)
*Akaash Chatterjee,Suman Kundu*

Main category: cs.AI

TL;DR: ExaCraft是一个AI驱动的个性化示例生成系统，通过分析学习者的动态上下文（包括位置、教育背景、专业、复杂偏好等）来创建文化相关且符合个人学习需求的示例，能够适应学习者的困难指标、掌握模式、主题进展历史等五个关键方面。


<details>
  <summary>Details</summary>
Motivation: 现有教育AI工具不关注生成示例或适应学习者不断变化的理解、困难或技能增长。学习最有效时是与相关、可关联的示例相联系，这些示例能在个人层面上引起学习者共鸣。

Method: 通过Google Gemini AI和Python Flask API构建的Chrome扩展系统，结合用户定义的个人资料（位置、教育、职业、复杂度偏好）和实时学习者行为分析，生成个性化示例。系统核心创新是能够适应学习上下文的五个关键方面：困难指标、掌握模式、主题进展历史、会话边界和学习进展信号。

Result: 系统能够生成从基础概念到高级技术实现的演化示例，响应主题重复、重新生成请求和不同用例中的主题进展模式。示例既具有文化相关性，又针对个人学习需求进行定制。

Conclusion: ExaCraft通过动态适应学习者的上下文，提供个性化、文化相关的示例，解决了现有教育AI工具在示例生成和适应性方面的不足，从而提高了学习效果。

Abstract: Learning is most effective when it's connected to relevant, relatable examples that resonate with learners on a personal level. However, existing educational AI tools don't focus on generating examples or adapting to learners' changing understanding, struggles, or growing skills. We've developed ExaCraft, an AI system that generates personalized examples by adapting to the learner's dynamic context. Through the Google Gemini AI and Python Flask API, accessible via a Chrome extension, ExaCraft combines user-defined profiles (including location, education, profession, and complexity preferences) with real-time analysis of learner behavior. This ensures examples are both culturally relevant and tailored to individual learning needs. The system's core innovation is its ability to adapt to five key aspects of the learning context: indicators of struggle, mastery patterns, topic progression history, session boundaries, and learning progression signals. Our demonstration will show how ExaCraft's examples evolve from basic concepts to advanced technical implementations, responding to topic repetition, regeneration requests, and topic progression patterns in different use cases.

</details>


### [33] [Suzume-chan: Your Personal Navigator as an Embodied Information Hub](https://arxiv.org/abs/2512.09932)
*Maya Grace Torii,Takahito Murakami,Shuka Koseki,Yoichi Ochiai*

Main category: cs.AI

TL;DR: 提出"具身信息枢纽"概念，通过物理对话交互减少心理距离，使知识分享更温暖、更人性化


<details>
  <summary>Details</summary>
Motivation: 数字工具虽然改善了信息获取，但缺乏深度理解所需的情感连接。专家知识获取通常需要实时人际沟通，而现有工具难以创造"在一起"的感觉

Method: 基于社会临场感理论，开发名为Suzume-chan的具身信息枢纽原型。这是一个小型、柔软的AI代理，本地运行语言模型和检索增强生成(RAG)技术，通过语音解释学习和对话响应

Result: 原型系统能够通过物理和对话交互学习知识并响应，减少了心理距离，使知识分享更加温暖和以人为中心

Conclusion: 具身信息枢纽通过结合物理存在和对话交互，有效提升了知识分享的情感连接和深度理解，为人机交互提供了新方向

Abstract: Access to expert knowledge often requires real-time human communication. Digital tools improve access to information but rarely create the sense of connection needed for deep understanding. This study addresses this issue using Social Presence Theory, which explains how a feeling of "being together" enhances communication. An "Embodied Information Hub" is proposed as a new way to share knowledge through physical and conversational interaction. The prototype, Suzume-chan, is a small, soft AI agent running locally with a language model and retrieval-augmented generation (RAG). It learns from spoken explanations and responds through dialogue, reducing psychological distance and making knowledge sharing warmer and more human-centered.

</details>


### [34] [Exploring Health Misinformation Detection with Multi-Agent Debate](https://arxiv.org/abs/2512.09935)
*Chih-Han Chen,Chen-Han Tsai,Yu-Shao Peng*

Main category: cs.AI

TL;DR: 提出一个两阶段框架用于健康信息检测：先通过LLM评估证据计算一致性分数，若分数不足则启动多智能体辩论来生成有理由的结论。


<details>
  <summary>Details</summary>
Motivation: 随着健康相关错误信息在网上激增，有效的验证需要高质量证据检索和严谨推理过程。现有方法在处理复杂验证任务时存在局限性。

Method: 两阶段框架：1) 一致性分数预测阶段：使用LLM独立评估检索到的文章，计算反映证据立场的聚合一致性分数；2) 多智能体辩论阶段：当分数低于阈值时，多个智能体进行结构化辩论，综合冲突证据并生成有明确理由的结论。

Result: 实验结果表明，这种两阶段方法相比基线方法取得了优越性能，突显了将自动评分与协作推理结合在复杂验证任务中的价值。

Conclusion: 结合自动化评分和协作推理的两阶段框架能有效检测健康错误信息，为复杂验证任务提供了有价值的解决方案。

Abstract: Fact-checking health-related claims has become increasingly critical as misinformation proliferates online. Effective verification requires both the retrieval of high-quality evidence and rigorous reasoning processes. In this paper, we propose a two-stage framework for health misinformation detection: Agreement Score Prediction followed by Multi-Agent Debate. In the first stage, we employ large language models (LLMs) to independently evaluate retrieved articles and compute an aggregated agreement score that reflects the overall evidence stance. When this score indicates insufficient consensus-falling below a predefined threshold-the system proceeds to a second stage. Multiple agents engage in structured debate to synthesize conflicting evidence and generate well-reasoned verdicts with explicit justifications. Experimental results demonstrate that our two-stage approach achieves superior performance compared to baseline methods, highlighting the value of combining automated scoring with collaborative reasoning for complex verification tasks.

</details>


### [35] [Echo-CoPilot: A Multi-View, Multi-Task Agent for Echocardiography Interpretation and Reporting](https://arxiv.org/abs/2512.09944)
*Moein Heidari,Mohammad Amin Roohi,Armin Khosravi,Ilker Hacihaliloglu*

Main category: cs.AI

TL;DR: Echo-CoPilot是一个基于大语言模型的多视图、多任务超声心动图智能代理，通过协调专用工具实现临床连贯的评估，在MIMIC-EchoQA基准上达到50.8%的准确率。


<details>
  <summary>Details</summary>
Motivation: 超声心动图是心血管诊疗的核心技术，但全研究解读仍是需要人工完成的认知密集型多视图任务。现有基础模型虽然在单个感知子任务（如视图分类、分割、疾病预测）上表现良好，但各自孤立运行，无法提供统一、临床连贯的评估。

Method: 提出Echo-CoPilot，这是一个多视图、多任务代理，使用大语言模型协调一套专门的超声心动图工具。在ReAct风格循环中，代理分解临床医生查询，调用视图识别、心脏结构分割、测量和疾病预测、报告生成等工具，并将输出整合为符合指南的答案和叙述性总结。

Result: 在公开的MIMIC-EchoQA基准测试中，Echo-CoPilot达到50.8%的准确率，优于通用和生物医学视频视觉语言模型。定性分析显示，代理能够利用定量测量和生理学背景解决临床决策阈值附近的挑战性病例，如临界左心室肥厚或心包积液严重程度。

Conclusion: Echo-CoPilot通过大语言模型协调专门工具，实现了超声心动图的多视图、多任务统一评估，在基准测试中表现优异，并能处理临床决策阈值附近的复杂病例，为超声心动图自动化解读提供了有前景的解决方案。

Abstract: Echocardiography is central to contemporary cardiovascular care, but full-study interpretation remains a cognitively demanding, multi-view task that is still performed manually. While recent foundation models for echocardiography can achieve strong performance on individual perceptual subtasks such as view classification, segmentation, or disease prediction, they typically operate in isolation and do not provide a unified, clinically coherent assessment. In this work, we introduce Echo-CoPilot, a multi-view, multi-task agent that uses a large language model to orchestrate a suite of specialized echocardiography tools. Within a ReAct-style loop, the agent decomposes clinician queries, invokes tools for view recognition, cardiac structure segmentation, measurement and disease prediction, and report synthesis, and integrates their outputs into guideline-aware answers and narrative summaries. We evaluate Echo-CoPilot on the public MIMIC-EchoQA benchmark, where it achieves an accuracy of 50.8\%, outperforming both general-purpose and biomedical video vision-language models. Qualitative analyses further show that the agent leverages quantitative measurements and physiologic context to resolve challenging cases near clinical decision thresholds, such as borderline left ventricular hypertrophy or pericardial effusion severity. The code will be released upon acceptance of the paper.

</details>


### [36] [Fuzzy Hierarchical Multiplex](https://arxiv.org/abs/2512.09976)
*Alexis Kafantaris*

Main category: cs.AI

TL;DR: 提出新的模糊优化框架，扩展FCM因果关系，利用动力学将数据映射到度量空间，通过多重网络分析概念间的逻辑蕴含和层次关系，用于服务流程设计中的信息传输优化。


<details>
  <summary>Details</summary>
Motivation: 现有FCM（模糊认知图）在因果关系建模方面存在局限性，需要扩展以更好地处理服务流程设计中的信息传输优化问题，特别是概念间的逻辑蕴含和层次关系分析。

Method: 提出新的模糊优化框架，扩展FCM因果关系，利用动力学将数据映射到度量空间，构建多重网络框架来分析逻辑蕴含和概念层次关系，采用白盒理论方法分析框架的逻辑和数学基础。

Result: 建立了完整的模糊优化框架，能够有效分析概念间的逻辑蕴含和层次关系，为服务流程设计中的信息传输优化提供了理论工具，并对FHM进行了深入分析。

Conclusion: 提出的新框架成功扩展了FCM因果关系，为服务优化中的信息传输问题提供了有效的分析工具，框架具有理论严谨性和实际应用价值。

Abstract: A new fuzzy optimization framework that extends FCM causality is proposed. This model utilizes the dynamics to map data into metrics and create a framework that examines logical implication and hierarchy of concepts using a multiplex. Moreover, this is a white-theoretical paper introducing the framework and analyzing the logic and math behind it. Upon this extension the main objectives and the orientation of this framework is expounded and exemplified; this framework is meant for service optimization of information transmission in service process design. Lastly, a thorough analysis of the FHM is included which is done following the logical steps in a simple and elegant manner.

</details>


### [37] [Mind the Gap! Pathways Towards Unifying AI Safety and Ethics Research](https://arxiv.org/abs/2512.10058)
*Dani Roytburg,Beck Miller*

Main category: cs.AI

TL;DR: 通过大规模文献计量和合作网络分析发现，AI安全与AI伦理研究存在显著的结构性分裂，80%以上合作发生在各自社区内部，跨领域交流高度集中于少数"桥梁"研究者。


<details>
  <summary>Details</summary>
Motivation: 当前AI对齐研究分裂为两个平行轨道：安全研究关注规模化智能、欺骗行为和存在风险；伦理研究关注当前危害、社会偏见和生产流程缺陷。两个社区对"对齐"的定义存在分歧，导致研究相对孤立，需要实证分析这种分裂的结构性特征。

Method: 采用文献计量和合作网络分析方法，对2020-2025年间12个主要ML和NLP会议的6,442篇论文进行研究，分析安全与伦理社区的合作模式和网络结构。

Result: 研究发现：1) 超过80%的合作发生在安全或伦理社区内部；2) 跨领域连通性高度集中，约5%的论文贡献了超过85%的桥梁链接；3) 移除少数关键"桥梁"研究者会显著增加社区隔离，表明跨学科交流依赖于少数关键人物而非广泛合作。

Conclusion: 安全与伦理的分裂不仅是概念性的，更是制度性的。需要通过共享基准、跨机构平台和混合方法，将技术安全工作与规范伦理相结合，以构建既稳健又公正的AI系统。

Abstract: While much research in artificial intelligence (AI) has focused on scaling capabilities, the accelerating pace of development makes countervailing work on producing harmless, "aligned" systems increasingly urgent. Yet research on alignment has diverged along two largely parallel tracks: safety--centered on scaled intelligence, deceptive or scheming behaviors, and existential risk--and ethics--focused on present harms, the reproduction of social bias, and flaws in production pipelines. Although both communities warn of insufficient investment in alignment, they disagree on what alignment means or ought to mean. As a result, their efforts have evolved in relative isolation, shaped by distinct methodologies, institutional homes, and disciplinary genealogies.
  We present a large-scale, quantitative study showing the structural split between AI safety and AI ethics. Using a bibliometric and co-authorship network analysis of 6,442 papers from twelve major ML and NLP conferences (2020-2025), we find that over 80% of collaborations occur within either the safety or ethics communities, and cross-field connectivity is highly concentrated: roughly 5% of papers account for more than 85% of bridging links. Removing a small number of these brokers sharply increases segregation, indicating that cross-disciplinary exchange depends on a handful of actors rather than broad, distributed collaboration. These results show that the safety-ethics divide is not only conceptual but institutional, with implications for research agendas, policy, and venues. We argue that integrating technical safety work with normative ethics--via shared benchmarks, cross-institutional venues, and mixed-method methodologies--is essential for building AI systems that are both robust and just.

</details>


### [38] [Exploring LLMs for Scientific Information Extraction Using The SciEx Framework](https://arxiv.org/abs/2512.10004)
*Sha Li,Ayush Sadekar,Nathan Self,Yiqi Su,Lars Andersland,Mira Chaplin,Annabel Zhang,Hyoju Yang,James B Henderson,Krista Wigginton,Linsey Marr,T. M. Murali,Naren Ramakrishnan*

Main category: cs.AI

TL;DR: SciEx是一个模块化框架，用于从科学文献中提取细粒度信息，解决了长文档、多模态内容和快速变化的提取模式等挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型工具在处理科学文献时面临挑战：长上下文文档、多模态内容、跨多篇文献的不一致细粒度信息整合，以及数据模式快速变化时难以重新架构系统。

Method: 提出SciEx模块化框架，将PDF解析、多模态检索、信息提取和聚合等关键组件解耦，支持灵活集成新模型、提示策略和推理机制。

Result: 在三个科学主题的数据集上评估了SciEx提取细粒度信息的准确性和一致性，提供了对当前基于LLM的提取流程优势和局限性的实用见解。

Conclusion: SciEx框架通过模块化设计简化了按需数据提取，同时支持可扩展性和灵活性，为科学信息提取提供了更有效的解决方案。

Abstract: Large language models (LLMs) are increasingly touted as powerful tools for automating scientific information extraction. However, existing methods and tools often struggle with the realities of scientific literature: long-context documents, multi-modal content, and reconciling varied and inconsistent fine-grained information across multiple publications into standardized formats. These challenges are further compounded when the desired data schema or extraction ontology changes rapidly, making it difficult to re-architect or fine-tune existing systems. We present SciEx, a modular and composable framework that decouples key components including PDF parsing, multi-modal retrieval, extraction, and aggregation. This design streamlines on-demand data extraction while enabling extensibility and flexible integration of new models, prompting strategies, and reasoning mechanisms. We evaluate SciEx on datasets spanning three scientific topics for its ability to extract fine-grained information accurately and consistently. Our findings provide practical insights into both the strengths and limitations of current LLM-based pipelines.

</details>


### [39] [DynaMate: An Autonomous Agent for Protein-Ligand Molecular Dynamics Simulations](https://arxiv.org/abs/2512.10034)
*Salomé Guilbert,Cassandra Masschelein,Jeremy Goumaz,Bohdan Naida,Philippe Schwaller*

Main category: cs.AI

TL;DR: DynaMate是一个基于多智能体LLM的框架，能够自主设计和执行蛋白质-配体系统的分子动力学模拟完整工作流程，包括自由能结合亲和力计算。


<details>
  <summary>Details</summary>
Motivation: 尽管分子动力学模拟在药物发现和蛋白质工程中应用广泛，但其技术复杂性（参数化、输入准备、软件配置）阻碍了广泛高效使用。目前还没有成功使用智能体LLM自动化蛋白质-配体MD工作流程的解决方案。

Method: DynaMate是一个模块化多智能体框架，包含三个专门模块：实验规划、模拟执行和结果分析。框架集成了动态工具使用、网络搜索、PaperQA和自我纠正行为，能够自主设计完整MD工作流程并进行MM/PB(GB)SA自由能计算。

Result: 在12个不同复杂度的基准系统上评估显示，DynaMate能够可靠执行完整MD模拟，通过迭代推理纠正运行时错误，并生成有意义的蛋白质-配体相互作用分析。

Conclusion: 该自动化框架为未来生物分子和药物设计应用中的标准化、可扩展和高效分子建模流程铺平了道路。

Abstract: Force field-based molecular dynamics (MD) simulations are indispensable for probing the structure, dynamics, and functions of biomolecular systems, including proteins and protein-ligand complexes. Despite their broad utility in drug discovery and protein engineering, the technical complexity of MD setup, encompassing parameterization, input preparation, and software configuration, remains a major barrier for widespread and efficient usage. Agentic LLMs have demonstrated their capacity to autonomously execute multi-step scientific processes, and to date, they have not successfully been used to automate protein-ligand MD workflows. Here, we present DynaMate, a modular multi-agent framework that autonomously designs and executes complete MD workflows for both protein and protein-ligand systems, and offers free energy binding affinity calculations with the MM/PB(GB)SA method. The framework integrates dynamic tool use, web search, PaperQA, and a self-correcting behavior. DynaMate comprises three specialized modules, interacting to plan the experiment, perform the simulation, and analyze the results. We evaluated its performance across twelve benchmark systems of varying complexity, assessing success rate, efficiency, and adaptability. DynaMate reliably performed full MD simulations, corrected runtime errors through iterative reasoning, and produced meaningful analyses of protein-ligand interactions. This automated framework paves the way toward standardized, scalable, and time-efficient molecular modeling pipelines for future biomolecular and drug design applications.

</details>


### [40] [The 2025 Foundation Model Transparency Index](https://arxiv.org/abs/2512.10169)
*Alexander Wan,Kevin Klyman,Sayash Kapoor,Nestor Maslej,Shayne Longpre,Betty Xiong,Percy Liang,Rishi Bommasani*

Main category: cs.AI

TL;DR: 2025年基础模型透明度指数显示，主要AI公司的透明度从2024年的58分下降到2025年的40分，透明度恶化，特别是在训练数据和部署后影响方面最不透明。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型开发商成为全球最重要的公司，需要评估其透明度实践如何演变，并为政策制定者提供透明度现状的量化分析，以指导监管干预。

Method: 采用年度基础模型透明度指数方法，引入新的评估指标（数据获取、使用数据、监控），首次评估阿里巴巴、DeepSeek、xAI等公司，对主要基础模型开发商进行量化评分。

Result: 透明度整体恶化：平均分从2024年58分降至2025年40分；IBM表现最佳（95分），xAI和Midjourney最差（14分）；Frontier Model Forum成员处于中间位置；公司在训练数据、训练计算和部署后影响方面最不透明。

Conclusion: 基础模型开发商的透明度在恶化，需要更积极的政策干预来解决关键信息缺失问题，特别是随着全球政策制定者开始强制要求透明度，需要更有力的监管措施。

Abstract: Foundation model developers are among the world's most important companies. As these companies become increasingly consequential, how do their transparency practices evolve? The 2025 Foundation Model Transparency Index is the third edition of an annual effort to characterize and quantify the transparency of foundation model developers. The 2025 FMTI introduces new indicators related to data acquisition, usage data, and monitoring and evaluates companies like Alibaba, DeepSeek, and xAI for the first time. The 2024 FMTI reported that transparency was improving, but the 2025 FMTI finds this progress has deteriorated: the average score out of 100 fell from 58 in 2024 to 40 in 2025. Companies are most opaque about their training data and training compute as well as the post-deployment usage and impact of their flagship models. In spite of this general trend, IBM stands out as a positive outlier, scoring 95, in contrast to the lowest scorers, xAI and Midjourney, at just 14. The five members of the Frontier Model Forum we score end up in the middle of the Index: we posit that these companies avoid reputational harms from low scores but lack incentives to be transparency leaders. As policymakers around the world increasingly mandate certain types of transparency, this work reveals the current state of transparency for foundation model developers, how it may change given newly enacted policy, and where more aggressive policy interventions are necessary to address critical information deficits.

</details>


### [41] [SimWorld-Robotics: Synthesizing Photorealistic and Dynamic Urban Environments for Multimodal Robot Navigation and Collaboration](https://arxiv.org/abs/2512.10046)
*Yan Zhuang,Jiawei Ren,Xiaokang Ye,Jianzhi Shen,Ruixuan Zhang,Tianai Yue,Muhammad Faayez,Xuhong He,Ziqiao Ma,Lianhui Qin,Zhiting Hu,Tianmin Shu*

Main category: cs.AI

TL;DR: SimWorld-Robotics (SWR) 是一个基于 Unreal Engine 5 构建的大规模、逼真城市环境仿真平台，用于具身 AI 研究，包含两个具有挑战性的机器人基准测试。


<details>
  <summary>Details</summary>
Motivation: 当前基础模型在通用机器人领域的研究主要集中在室内家庭场景，缺乏针对大规模、复杂城市环境的仿真平台和评估基准。

Method: 基于 Unreal Engine 5 构建 SWR 平台，程序化生成无限逼真的城市场景，包含动态元素（行人、交通系统），支持多机器人控制和通信。在此基础上创建了两个基准测试：多模态指令跟随任务和多智能体搜索任务。

Result: 实验结果表明，当前最先进的模型（包括视觉语言模型）在 SWR 的任务中表现不佳，缺乏城市环境所需的稳健感知、推理和规划能力。

Conclusion: SWR 平台填补了城市环境具身 AI 研究的空白，其基准测试能够全面评估机器人在真实场景中的关键能力，为未来研究提供了重要工具和挑战。

Abstract: Recent advances in foundation models have shown promising results in developing generalist robotics that can perform diverse tasks in open-ended scenarios given multimodal inputs. However, current work has been mainly focused on indoor, household scenarios. In this work, we present SimWorld-Robotics~(SWR), a simulation platform for embodied AI in large-scale, photorealistic urban environments. Built on Unreal Engine 5, SWR procedurally generates unlimited photorealistic urban scenes populated with dynamic elements such as pedestrians and traffic systems, surpassing prior urban simulations in realism, complexity, and scalability. It also supports multi-robot control and communication. With these key features, we build two challenging robot benchmarks: (1) a multimodal instruction-following task, where a robot must follow vision-language navigation instructions to reach a destination in the presence of pedestrians and traffic; and (2) a multi-agent search task, where two robots must communicate to cooperatively locate and meet each other. Unlike existing benchmarks, these two new benchmarks comprehensively evaluate a wide range of critical robot capacities in realistic scenarios, including (1) multimodal instructions grounding, (2) 3D spatial reasoning in large environments, (3) safe, long-range navigation with people and traffic, (4) multi-robot collaboration, and (5) grounded communication. Our experimental results demonstrate that state-of-the-art models, including vision-language models (VLMs), struggle with our tasks, lacking robust perception, reasoning, and planning abilities necessary for urban environments.

</details>


### [42] [EpiPlanAgent: Agentic Automated Epidemic Response Planning](https://arxiv.org/abs/2512.10313)
*Kangkun Mao,Fang Xu,Jinru Ding,Yidong Jiang,Yujun Yao,Yirong Chen,Junming Liu,Xiaoqin Wu,Qian Wu,Xiaoyan Huang,Jie Xu*

Main category: cs.AI

TL;DR: EpiPlanAgent是一个基于大语言模型的多智能体系统，能够自动化生成和验证数字应急响应计划，显著提高计划完整性和指南一致性，同时大幅减少开发时间。


<details>
  <summary>Details</summary>
Motivation: 传统的流行病应对计划制定依赖劳动密集型的手工方法，效率低下且难以规模化。需要开发自动化解决方案来提升公共卫生应急准备能力。

Method: 设计并评估EpiPlanAgent系统，这是一个基于大语言模型的多智能体框架，集成了任务分解、知识基础和模拟模块。公共卫生专业人员使用真实世界疫情场景在受控环境中测试系统。

Result: EpiPlanAgent显著提高了计划的完整性和指南一致性，同时大幅减少了开发时间。专家评估确认AI生成内容与人工编写内容高度一致，用户反馈显示系统具有强感知效用。

Conclusion: EpiPlanAgent为智能流行病应对计划提供了有效、可扩展的解决方案，展示了智能体AI在转变公共卫生准备方面的潜力。

Abstract: Epidemic response planning is essential yet traditionally reliant on labor-intensive manual methods. This study aimed to design and evaluate EpiPlanAgent, an agent-based system using large language models (LLMs) to automate the generation and validation of digital emergency response plans. The multi-agent framework integrated task decomposition, knowledge grounding, and simulation modules. Public health professionals tested the system using real-world outbreak scenarios in a controlled evaluation. Results demonstrated that EpiPlanAgent significantly improved the completeness and guideline alignment of plans while drastically reducing development time compared to manual workflows. Expert evaluation confirmed high consistency between AI-generated and human-authored content. User feedback indicated strong perceived utility. In conclusion, EpiPlanAgent provides an effective, scalable solution for intelligent epidemic response planning, demonstrating the potential of agentic AI to transform public health preparedness.

</details>


### [43] [Parallel Decoder Transformer: Model-Internal Parallel Decoding with Speculative Invariance via Note Conditioning](https://arxiv.org/abs/2512.10054)
*Logan Robbins*

Main category: cs.AI

TL;DR: PDT是一种参数高效的并行解码架构，通过轻量级适配器让冻结的预训练模型实现并行解码流之间的协调，解决传统方法中的连贯性漂移问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的自回归解码本质上是顺序的，导致延迟随输出长度线性增长。现有的"分解-填充"方法虽然尝试并行化生成，但缺乏跨流通信会导致连贯性漂移问题。

Method: 提出并行解码器变换器(PDT)，在冻结的预训练模型中注入轻量级的"推测性笔记条件化"适配器，通过共享的动态潜在空间实现并行解码流的同步。将协调问题形式化为"推测共识"问题，让兄弟流通过全局总线广播语义"笔记"，并由学习的验证头进行门控。

Result: 在冻结的200亿参数骨干模型上使用5万步课程进行验证，PDT实现了有效的自校正，覆盖预测精度达到77.8%，能够恢复近似的串行语义而不修改主干权重。

Conclusion: PDT为结构化并行生成提供了一个可扩展、高效的替代方案，无需进行完整的模型微调，通过参数高效的方式解决了并行解码中的协调问题。

Abstract: Autoregressive decoding in Large Language Models (LLMs) is inherently sequential, creating a latency bottleneck that scales linearly with output length. While ``Decomposition-and-Fill'' methods like Skeleton-of-Thought attempt to parallelize generation via external orchestration, they suffer from \textit{coherence drift} due to the lack of cross-stream communication. In this work, we introduce the \textbf{Parallel Decoder Transformer (PDT)}, a parameter-efficient architecture that embeds coordination primitives directly into the inference process of a frozen pre-trained model.
  Instead of retraining the base model, PDT injects lightweight \textit{Speculative Note Conditioning (SNC)} adapters that allow parallel decoding streams to synchronize via a shared, dynamic latent space. We formulate coordination as a \textit{speculative consensus} problem, where sibling streams broadcast semantic ``notes'' to a global bus, gated by a learned verification head. We validate our approach on a 50,000-step curriculum using a frozen 20B-parameter backbone. Our results demonstrate that PDT achieves effective self-correction, reaching \textbf{77.8\% precision} in coverage prediction and recovering approximate serial semantics without modifying the trunk weights. This establishes PDT as a scalable, efficient alternative to full model fine-tuning for structured parallel generation.

</details>


### [44] [Challenges of Evaluating LLM Safety for User Welfare](https://arxiv.org/abs/2512.10687)
*Manon Kempermann,Sai Suresh Macharla Vasu,Mahalakshmi Raveenthiran,Theo Farrell,Ingmar Weber*

Main category: cs.AI

TL;DR: 论文提出LLM安全评估需关注用户个体风险而非通用风险，通过实验证明评估者需了解用户背景信息，且仅靠用户主动提供背景信息不足以改善评估质量，特别是对弱势群体。


<details>
  <summary>Details</summary>
Motivation: 当前LLM安全评估主要关注通用风险（如危险能力），但数百万用户将LLM用于金融、健康等高风险个人咨询，这些危害是情境依赖的。现有评估框架缺乏针对用户个体福利的安全评估方法。

Method: 对GPT-5、Claude Sonnet 4和Gemini 2.5 Pro在金融和健康建议方面进行评估：1）比较有/无用户背景信息的评估者评分差异；2）使用用户报告会披露的背景信息重新评估，检验改进效果。

Result: 1）了解用户背景的评估者给出的安全评分显著低于不了解背景的评估者（高风险用户评分从5/7降至3/7）；2）仅使用用户会主动披露的背景信息并不能显著改善评估质量，特别是对弱势群体。

Conclusion: 有效的用户福利安全评估需要评估者基于多样化的用户背景进行评估，仅靠用户主动提供背景信息不足够。需要开发与现有通用风险评估框架不同的方法，论文提供了情境感知评估的方法论基础。

Abstract: Safety evaluations of large language models (LLMs) typically focus on universal risks like dangerous capabilities or undesirable propensities. However, millions use LLMs for personal advice on high-stakes topics like finance and health, where harms are context-dependent rather than universal. While frameworks like the OECD's AI classification recognize the need to assess individual risks, user-welfare safety evaluations remain underdeveloped. We argue that developing such evaluations is non-trivial due to fundamental questions about accounting for user context in evaluation design. In this exploratory study, we evaluated advice on finance and health from GPT-5, Claude Sonnet 4, and Gemini 2.5 Pro across user profiles of varying vulnerability. First, we demonstrate that evaluators must have access to rich user context: identical LLM responses were rated significantly safer by context-blind evaluators than by those aware of user circumstances, with safety scores for high-vulnerability users dropping from safe (5/7) to somewhat unsafe (3/7). One might assume this gap could be addressed by creating realistic user prompts containing key contextual information. However, our second study challenges this: we rerun the evaluation on prompts containing context users report they would disclose, finding no significant improvement. Our work establishes that effective user-welfare safety evaluation requires evaluators to assess responses against diverse user profiles, as realistic user context disclosure alone proves insufficient, particularly for vulnerable populations. By demonstrating a methodology for context-aware evaluation, this study provides both a starting point for such assessments and foundational evidence that evaluating individual welfare demands approaches distinct from existing universal-risk frameworks. We publish our code and dataset to aid future developments.

</details>


### [45] [Linear socio-demographic representations emerge in Large Language Models from indirect cues](https://arxiv.org/abs/2512.10065)
*Paul Bouchaud,Pedro Ramaciotti*

Main category: cs.AI

TL;DR: LLMs通过名字和职业等间接线索编码用户的社会人口属性，形成线性表示，这些表示会影响下游行为如职业推荐，即使通过偏见测试的模型仍可能隐含偏见。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs如何从名字、职业等间接线索推断人类对话伙伴的社会人口属性，揭示模型是否以及如何编码这些属性，即使模型通过了偏见测试。

Method: 在四个开源Transformer模型上，通过显式人口属性披露提示，探测各层残差流；使用相同探针从名字预测性别和种族，从职业预测与真实劳动力统计数据相关的表示。

Result: LLMs在激活空间中形成用户人口属性的线性表示，刻板印象相关属性沿可解释的几何方向编码；名字激活与人口普查一致的性别和种族表示，职业触发与真实劳动力统计相关的表示；这些隐含的人口表示主动影响下游行为如职业推荐。

Conclusion: LLMs从间接线索编码社会人口属性，形成线性表示并影响行为；即使通过偏见基准测试的模型仍可能隐含和利用偏见，在大规模应用时对公平性有重要影响。

Abstract: We investigate how LLMs encode sociodemographic attributes of human conversational partners inferred from indirect cues such as names and occupations. We show that LLMs develop linear representations of user demographics within activation space, wherein stereotypically associated attributes are encoded along interpretable geometric directions. We first probe residual streams across layers of four open transformer-based LLMs (Magistral 24B, Qwen3 14B, GPT-OSS 20B, OLMo2-1B) prompted with explicit demographic disclosure. We show that the same probes predict demographics from implicit cues: names activate census-aligned gender and race representations, while occupations trigger representations correlated with real-world workforce statistics. These linear representations allow us to explain demographic inferences implicitly formed by LLMs during conversation. We demonstrate that these implicit demographic representations actively shape downstream behavior, such as career recommendations. Our study further highlights that models that pass bias benchmark tests may still harbor and leverage implicit biases, with implications for fairness when applied at scale.

</details>


### [46] [Interpretable Embeddings with Sparse Autoencoders: A Data Analysis Toolkit](https://arxiv.org/abs/2512.10092)
*Nick Jiang,Xiaoqing Sun,Lisa Dunlap,Lewis Smith,Neel Nanda*

Main category: cs.AI

TL;DR: 使用稀疏自编码器（SAE）创建可解释的概念嵌入，相比LLM更经济可靠，相比稠密嵌入更可控，适用于大规模文本分析任务


<details>
  <summary>Details</summary>
Motivation: 当前大规模文本分析主要依赖昂贵的LLM技术或缺乏可控性的稠密嵌入模型，需要一种更经济、可靠且可控的分析方法

Method: 提出使用稀疏自编码器（SAEs）创建SAE嵌入表示，其维度映射到可解释的概念，通过四个数据分析任务验证其有效性

Result: SAE嵌入比LLM成本低2-8倍且更可靠，比稠密嵌入更可控；成功应用于数据集语义差异分析、概念相关性发现、模型行为研究等任务

Conclusion: SAE是一种多功能的无结构数据分析工具，强调通过数据解释模型的重要性，为大规模文本分析提供了经济、可靠且可控的解决方案

Abstract: Analyzing large-scale text corpora is a core challenge in machine learning, crucial for tasks like identifying undesirable model behaviors or biases in training data. Current methods often rely on costly LLM-based techniques (e.g. annotating dataset differences) or dense embedding models (e.g. for clustering), which lack control over the properties of interest. We propose using sparse autoencoders (SAEs) to create SAE embeddings: representations whose dimensions map to interpretable concepts. Through four data analysis tasks, we show that SAE embeddings are more cost-effective and reliable than LLMs and more controllable than dense embeddings. Using the large hypothesis space of SAEs, we can uncover insights such as (1) semantic differences between datasets and (2) unexpected concept correlations in documents. For instance, by comparing model responses, we find that Grok-4 clarifies ambiguities more often than nine other frontier models. Relative to LLMs, SAE embeddings uncover bigger differences at 2-8x lower cost and identify biases more reliably. Additionally, SAE embeddings are controllable: by filtering concepts, we can (3) cluster documents along axes of interest and (4) outperform dense embeddings on property-based retrieval. Using SAE embeddings, we study model behavior with two case studies: investigating how OpenAI model behavior has changed over time and finding "trigger" phrases learned by Tulu-3 (Lambert et al., 2024) from its training data. These results position SAEs as a versatile tool for unstructured data analysis and highlight the neglected importance of interpreting models through their data.

</details>


### [47] [Robust AI Security and Alignment: A Sisyphean Endeavor?](https://arxiv.org/abs/2512.10100)
*Apostol Vassilev*

Main category: cs.AI

TL;DR: 该论文将哥德尔不完备性定理扩展到AI领域，建立了AI安全和对齐性的信息论限制，证明了AI系统认知推理的局限性，并提供了应对这些挑战的实用方法。


<details>
  <summary>Details</summary>
Motivation: 认识到AI安全和对齐性存在根本性的信息论限制对于负责任地采用AI技术至关重要。了解这些限制并为其带来的挑战做好准备是推动AI负责任发展的关键。

Method: 通过将哥德尔不完备性定理扩展到AI领域，建立信息论框架来分析AI安全和对齐性的根本限制。该方法从数学和逻辑层面证明AI系统存在的固有局限性。

Result: 证明了AI安全和对齐性存在不可逾越的信息论限制，这些限制类似于哥德尔不完备性定理在数学系统中的表现。同时证明了AI系统在认知推理方面存在固有局限性。

Conclusion: AI安全和对齐性存在根本性的信息论限制，这些限制无法完全克服。论文提供了应对这些挑战的实用方法，并强调了在AI发展中认识和准备这些限制的重要性，这对AI技术的负责任采用具有深远意义。

Abstract: This manuscript establishes information-theoretic limitations for robustness of AI security and alignment by extending Gödel's incompleteness theorem to AI. Knowing these limitations and preparing for the challenges they bring is critically important for the responsible adoption of the AI technology. Practical approaches to dealing with these challenges are provided as well. Broader implications for cognitive reasoning limitations of AI systems are also proven.

</details>


### [48] [Modeling Narrative Archetypes in Conspiratorial Narratives: Insights from Singapore-Based Telegram Groups](https://arxiv.org/abs/2512.10105)
*Soorya Ram Shimgekar,Abhay Goyal,Lam Yin Cheung,Roy Ka-Wei Lee,Koustuv Saha,Pi Zonooz,Navin Kumar*

Main category: cs.AI

TL;DR: 论文提出一个两阶段计算框架分析新加坡Telegram群组中的阴谋论话语，通过微调RoBERTa-large分类消息，构建带符号信念图，开发SiBeGNN模型分离意识形态与风格特征，识别出7种叙事原型，发现阴谋论话语嵌入日常讨论而非孤立回音室。


<details>
  <summary>Details</summary>
Motivation: 阴谋论话语在数字通信生态中日益普遍，但其结构和传播难以研究。现有研究常假设阴谋论存在于孤立回音室，需要更精细的方法分析其如何嵌入日常社会互动。

Method: 1. 微调RoBERTa-large模型分类553,648条消息中的阴谋论内容（F1=0.866）；2. 构建带符号信念图，节点为消息，边符号反映信念标签一致性，权重为文本相似度；3. 提出SiBeGNN模型，使用符号解缠损失学习分离意识形态对齐与风格特征的嵌入；4. 对嵌入进行层次聚类识别叙事原型。

Result: 识别出7种叙事原型：法律话题、医疗关切、媒体讨论、金融、权威矛盾、群组管理、一般聊天。SiBeGNN聚类质量（cDBI=8.38）优于基线方法（13.60-67.27），专家评估一致性达88%。发现阴谋论消息不仅出现在怀疑/不信任集群，也嵌入金融、法律等日常讨论中。

Conclusion: 阴谋论话语在普通社会互动中运作，挑战了在线激进化常见假设。该框架推进了信念驱动话语分析的计算方法，可用于立场检测、政治传播研究和内容审核政策。

Abstract: Conspiratorial discourse is increasingly embedded within digital communication ecosystems, yet its structure and spread remain difficult to study. This work analyzes conspiratorial narratives in Singapore-based Telegram groups, showing that such content is woven into everyday discussions rather than confined to isolated echo chambers. We propose a two-stage computational framework. First, we fine-tune RoBERTa-large to classify messages as conspiratorial or not, achieving an F1-score of 0.866 on 2,000 expert-labeled messages. Second, we build a signed belief graph in which nodes represent messages and edge signs reflect alignment in belief labels, weighted by textual similarity. We introduce a Signed Belief Graph Neural Network (SiBeGNN) that uses a Sign Disentanglement Loss to learn embeddings that separate ideological alignment from stylistic features.
  Using hierarchical clustering on these embeddings, we identify seven narrative archetypes across 553,648 messages: legal topics, medical concerns, media discussions, finance, contradictions in authority, group moderation, and general chat. SiBeGNN yields stronger clustering quality (cDBI = 8.38) than baseline methods (13.60 to 67.27), supported by 88 percent inter-rater agreement in expert evaluations. Our analysis shows that conspiratorial messages appear not only in clusters focused on skepticism or distrust, but also within routine discussions of finance, law, and everyday matters. These findings challenge common assumptions about online radicalization by demonstrating that conspiratorial discourse operates within ordinary social interaction. The proposed framework advances computational methods for belief-driven discourse analysis and offers applications for stance detection, political communication studies, and content moderation policy.

</details>


### [49] [AgriRegion: Region-Aware Retrieval for High-Fidelity Agricultural Advice](https://arxiv.org/abs/2512.10114)
*Mesafint Fanuel,Mahmoud Nabil Mahmoud,Crystal Cook Marshal,Vishal Lakhotia,Biswanath Dari,Kaushik Roy,Shaohu Zhang*

Main category: cs.AI

TL;DR: AgriRegion是一个专门为农业咨询设计的检索增强生成框架，通过地理空间元数据注入和区域优先重排序机制，减少LLM在农业领域的上下文幻觉，提供区域准确的农业建议。


<details>
  <summary>Details</summary>
Motivation: 通用大语言模型在农业领域经常出现上下文幻觉问题，提供非事实性建议或在不同地区可能产生灾难性后果的建议，因为农业建议需要考虑土壤、气候和地方法规的差异。

Method: AgriRegion采用检索增强生成框架，包含地理空间元数据注入层和区域优先重排序机制。通过将知识库限制在已验证的当地农业推广服务，并在检索时强制执行地理空间约束，确保建议的本地准确性。

Result: 实验表明，AgriRegion相比最先进的LLM系统减少了10-20%的幻觉，并在综合评估中显著提高了信任分数。创建了包含160个领域特定问题的AgriRegion-Eval基准数据集。

Conclusion: AgriRegion通过区域感知的RAG框架有效解决了农业领域LLM的上下文幻觉问题，为农民提供了更可靠、本地化的农业建议，在农业信息民主化方面具有重要意义。

Abstract: Large Language Models (LLMs) have demonstrated significant potential in democratizing access to information. However, in the domain of agriculture, general-purpose models frequently suffer from contextual hallucination, which provides non-factual advice or answers are scientifically sound in one region but disastrous in another due to variations in soil, climate, and local regulations. We introduce AgriRegion, a Retrieval-Augmented Generation (RAG) framework designed specifically for high-fidelity, region-aware agricultural advisory. Unlike standard RAG approaches that rely solely on semantic similarity, AgriRegion incorporates a geospatial metadata injection layer and a region-prioritized re-ranking mechanism. By restricting the knowledge base to verified local agricultural extension services and enforcing geo-spatial constraints during retrieval, AgriRegion ensures that the advice regarding planting schedules, pest control, and fertilization is locally accurate. We create a novel benchmark dataset, AgriRegion-Eval, which comprises 160 domain-specific questions across 12 agricultural subfields. Experiments demonstrate that AgriRegion reduces hallucinations by 10-20% compared to state-of-the-art LLMs systems and significantly improves trust scores according to a comprehensive evaluation.

</details>


### [50] [CP-Env: Evaluating Large Language Models on Clinical Pathways in a Controllable Hospital Environment](https://arxiv.org/abs/2512.10206)
*Yakun Zhu,Zhongzhen Huang,Qianhan Feng,Linjie Mu,Yannian Gu,Shaoting Zhang,Qi Dou,Xiaofan Zhang*

Main category: cs.AI

TL;DR: CP-Env是一个可控的医院环境模拟器，用于评估大语言模型在端到端临床路径中的表现，包含患者和医生代理，支持从分诊到多学科会诊的复杂场景，采用三层评估框架。


<details>
  <summary>Details</summary>
Motivation: 当前基准测试主要关注静态考试或孤立对话，无法充分评估LLM在动态临床场景中的表现。医疗护理遵循复杂的临床路径，涉及多阶段决策和过渡，需要更全面的评估框架。

Method: 开发CP-Env可控医院环境，模拟包含患者和医生代理的医院生态系统，构建从分诊、专科会诊到诊断测试和多学科会诊的临床场景。提出包含临床疗效、流程能力和职业道德的三层评估框架。

Result: 大多数模型在临床路径复杂性方面表现不佳，出现幻觉和关键诊断细节丢失问题。有趣的是，过多的推理步骤有时会产生反效果，而顶级模型倾向于通过内化知识减少工具依赖。

Conclusion: CP-Env通过全面的端到端临床评估推进医疗AI代理的发展，提供了基准测试和评估工具供进一步研究使用。

Abstract: Medical care follows complex clinical pathways that extend beyond isolated physician-patient encounters, emphasizing decision-making and transitions between different stages. Current benchmarks focusing on static exams or isolated dialogues inadequately evaluate large language models (LLMs) in dynamic clinical scenarios. We introduce CP-Env, a controllable agentic hospital environment designed to evaluate LLMs across end-to-end clinical pathways. CP-Env simulates a hospital ecosystem with patient and physician agents, constructing scenarios ranging from triage and specialist consultation to diagnostic testing and multidisciplinary team meetings for agent interaction. Following real hospital adaptive flow of healthcare, it enables branching, long-horizon task execution. We propose a three-tiered evaluation framework encompassing Clinical Efficacy, Process Competency, and Professional Ethics. Results reveal that most models struggle with pathway complexity, exhibiting hallucinations and losing critical diagnostic details. Interestingly, excessive reasoning steps can sometimes prove counterproductive, while top models tend to exhibit reduced tool dependency through internalized knowledge. CP-Env advances medical AI agents development through comprehensive end-to-end clinical evaluation. We provide the benchmark and evaluation tools for further research and development at https://github.com/SPIRAL-MED/CP-Env.

</details>


### [51] [An exploration for higher efficiency in multi objective optimisation with reinforcement learning](https://arxiv.org/abs/2512.10208)
*Mehmet Emin Aydin*

Main category: cs.AI

TL;DR: 本文提出基于多目标强化学习的广义方法，用于优化多目标优化问题中的算子序列选择，旨在提高优化效率。


<details>
  <summary>Details</summary>
Motivation: 优化和搜索过程中的效率问题持续影响算法性能。使用算子池而非单一算子处理邻域移动操作具有潜力，但最优或近优算子序列仍需研究。虽然单目标优化已有相关工作，但多目标优化领域在此方面研究不足。

Method: 采用基于多目标强化学习的广义方法，通过多阶段设计来学习最优算子序列。该方法包含已完成阶段和待完成阶段，旨在系统性地解决多目标优化中的算子选择问题。

Result: 论文概述了提出的广义方法框架，展示了已完成的工作阶段，并指出了待完成的阶段。该方法为使用多目标强化学习提高优化效率提供了理论基础和实现路径。

Conclusion: 基于多目标强化学习的广义方法为解决多目标优化中的算子序列选择问题提供了有前景的解决方案，有望显著提高优化算法的效率和性能。

Abstract: Efficiency in optimisation and search processes persists to be one of the challenges, which affects the performance and use of optimisation algorithms. Utilising a pool of operators instead of a single operator to handle move operations within a neighbourhood remains promising, but an optimum or near optimum sequence of operators necessitates further investigation. One of the promising ideas is to generalise experiences and seek how to utilise it. Although numerous works are done around this issue for single objective optimisation, multi-objective cases have not much been touched in this regard. A generalised approach based on multi-objective reinforcement learning approach seems to create remedy for this issue and offer good solutions. This paper overviews a generalisation approach proposed with certain stages completed and phases outstanding that is aimed to help demonstrate the efficiency of using multi-objective reinforcement learning.

</details>


### [52] [ID-PaS : Identity-Aware Predict-and-Search for General Mixed-Integer Linear Programs](https://arxiv.org/abs/2512.10211)
*Junyang Cai,El Mehdi Er Raqabi,Pascal Van Hentenryck,Bistra Dilkina*

Main category: cs.AI

TL;DR: ID-PaS扩展了Predict-and-Search框架，通过身份感知学习处理参数化混合整数规划中的异构变量，在多个现实大规模问题上优于Gurobi和传统PaS方法。


<details>
  <summary>Details</summary>
Motivation: 现有的Predict-and-Search方法虽然结合机器学习提升了性能，但仅限于二元问题，且忽略了实际应用中常见的固定变量问题，需要扩展到更通用的参数化混合整数规划问题。

Method: 提出了ID-PaS（身份感知学习框架），扩展Predict-and-Search框架到参数化MIPs，使机器学习模型能够更有效地处理异构变量，包括固定变量和不同类型变量。

Result: 在多个现实世界大规模问题上进行实验，ID-PaS始终表现出优于最先进求解器Gurobi和传统Predict-and-Search方法的性能。

Conclusion: ID-PaS成功扩展了Predict-and-Search框架，通过身份感知学习有效处理参数化混合整数规划中的异构变量，为实际应用中的组合优化问题提供了更强大的解决方案。

Abstract: Mixed-Integer Linear Programs (MIPs) are powerful and flexible tools for modeling a wide range of real-world combinatorial optimization problems. Predict-and-Search methods operate by using a predictive model to estimate promising variable assignments and then guiding a search procedure toward high-quality solutions. Recent research has demonstrated that incorporating machine learning (ML) into the Predict-and-Search framework significantly enhances its performance. Still, it is restricted to binary problems and overlooks the presence of fixed variables that commonly arise in practical settings. This work extends the Predict-and-Search (PaS) framework to parametric MIPs and introduces ID-PaS, an identity-aware learning framework that enables the ML model to handle heterogeneous variables more effectively. Experiments on several real-world large-scale problems demonstrate that ID-PaS consistently achieves superior performance compared to the state-of-the-art solver Gurobi and PaS.

</details>


### [53] [Reverse Thinking Enhances Missing Information Detection in Large Language Models](https://arxiv.org/abs/2512.10273)
*Yuxin Liu,Chaojie Gu,Yihang Zhang,Bin Qian,Shibo He*

Main category: cs.AI

TL;DR: 论文提出了一种基于逆向思维的新框架，用于提升大语言模型在缺失信息检测任务上的性能，相比传统前向推理方法有显著提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理缺失信息问题时表现不佳，常出现回答不完整、事实错误和幻觉等问题。现有的前向推理方法（如CoT和ToT）无法系统性地识别和恢复被省略的信息。

Method: 提出一种逆向思维框架，引导大语言模型通过逆向推理来识别必要条件和定位缺失元素，将缺失信息识别任务转化为更易处理的逆向推理问题。

Result: 实验结果表明，逆向思维方法相比传统前向推理方法取得了显著的性能提升，为增强大语言模型的逻辑完整性和推理鲁棒性提供了有前景的方向。

Conclusion: 逆向思维方法能有效提升大语言模型在缺失信息检测任务上的表现，为解决模型在处理不完整信息时的局限性提供了新思路。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in various reasoning tasks, yet they often struggle with problems involving missing information, exhibiting issues such as incomplete responses, factual errors, and hallucinations. While forward reasoning approaches like Chain-of-Thought (CoT) and Tree-of-Thought (ToT) have shown success in structured problem-solving, they frequently fail to systematically identify and recover omitted information. In this paper, we explore the potential of reverse thinking methodologies to enhance LLMs' performance on missing information detection tasks. Drawing inspiration from recent work on backward reasoning, we propose a novel framework that guides LLMs through reverse thinking to identify necessary conditions and pinpoint missing elements. Our approach transforms the challenging task of missing information identification into a more manageable backward reasoning problem, significantly improving model accuracy. Experimental results demonstrate that our reverse thinking approach achieves substantial performance gains compared to traditional forward reasoning methods, providing a promising direction for enhancing LLMs' logical completeness and reasoning robustness.

</details>


### [54] [Neuronal Attention Circuit (NAC) for Representation Learning](https://arxiv.org/abs/2512.10282)
*Waleed Razzaq,Izis Kankaraway,Yun-Bo Zhao*

Main category: cs.AI

TL;DR: 提出神经注意力电路(NAC)，一种生物启发的连续时间注意力机制，通过线性一阶ODE和稀疏门控结构实现高效自适应动态，在多个领域表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统注意力机制虽然改进了RNN的表征学习，但其离散特性限制了连续时间建模能力。需要一种既能保持注意力优势又能进行连续时间建模的生物合理机制。

Method: 将注意力对数计算重新表述为线性一阶ODE的解，利用改造的秀丽隐杆线虫神经电路策略(NCPs)的布线机制创建非线性互连门。用稀疏感官门代替密集投影进行键-查询投影，使用具有两个头的稀疏骨干网络计算内容-目标和可学习时间常数门。提供三种注意力对数计算模式：显式欧拉积分、精确闭式解和稳态近似。采用稀疏Top-K成对连接方案提高内存效率。

Result: NAC在多个领域（不规则时间序列分类、自动驾驶车道保持、工业预测）中匹配或优于竞争基线方法的准确性，在运行时间和内存效率方面处于多个连续时间基线的中间位置。提供了状态稳定性、有界近似误差和通用逼近能力的理论保证。

Conclusion: NAC是一种有效的生物合理连续时间注意力机制，能够平衡准确性、计算效率和内存使用，为连续时间建模提供了新的注意力范式。

Abstract: Attention improves representation learning over RNNs, but its discrete nature limits continuous-time (CT) modeling. We introduce Neuronal Attention Circuit (NAC), a novel, biologically plausible CT-Attention mechanism that reformulates attention logits computation as the solution to a linear first-order ODE with nonlinear interlinked gates derived from repurposing \textit{C. elegans} Neuronal Circuit Policies (NCPs) wiring mechanism. NAC replaces dense projections with sparse sensory gates for key-query projections and a sparse backbone network with two heads for computing \textit{content-target} and \textit{learnable time-constant} gates, enabling efficient adaptive dynamics. NAC supports three attention logit computation modes: (i) explicit Euler integration, (ii) exact closed-form solution, and (iii) steady-state approximation. To improve memory intensity, we implemented a sparse Top-\emph{K} pairwise concatenation scheme that selectively curates key-query interactions. We provide rigorous theoretical guarantees, including state stability, bounded approximation errors, and universal approximation. Empirically, we implemented NAC in diverse domains, including irregular time-series classification, lane-keeping for autonomous vehicles, and industrial prognostics. We observed that NAC matches or outperforms competing baselines in accuracy and occupies an intermediate position in runtime and memory efficiency compared with several CT baselines.

</details>


### [55] [Investigating The Functional Roles of Attention Heads in Vision Language Models: Evidence for Reasoning Modules](https://arxiv.org/abs/2512.10300)
*Yanbei Jiang,Xueqi Ma,Shu Liu,Sarah Monazam Erfani,Tongliang Liu,James Bailey,Jey Han Lau,Krista A. Ehinger*

Main category: cs.AI

TL;DR: 本文提出了一种新的可解释性框架，通过CogVision数据集和探测方法分析视觉语言模型中注意力头的功能角色，发现功能头稀疏、分布不均且对多模态推理至关重要。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型在多模态基准测试中表现出色，但其内部机制仍然是一个黑箱。需要系统性地分析VLMs的内部工作机制，特别是注意力头在多模态推理中的功能角色。

Method: 1. 引入CogVision数据集，将复杂多模态问题分解为逐步子问题，模拟人类链式推理过程；2. 采用基于探测的方法，识别专门处理特定接收或认知功能（如高级视觉接收和推理）的注意力头；3. 在不同VLM家族中进行分析，并进行干预实验（移除或强调功能头）。

Result: 1. 功能头普遍稀疏，不同功能间的数量和分布存在差异；2. 这些头介导交互和层次组织；3. 干预实验表明功能头对多模态推理至关重要：移除会导致性能下降，强调则能提高准确性。

Conclusion: 该研究为VLMs的认知组织提供了新见解，并为设计具有更符合人类感知和推理能力的模型指明了有前景的方向。

Abstract: Despite excelling on multimodal benchmarks, vision-language models (VLMs) largely remain a black box. In this paper, we propose a novel interpretability framework to systematically analyze the internal mechanisms of VLMs, focusing on the functional roles of attention heads in multimodal reasoning. To this end, we introduce CogVision, a dataset that decomposes complex multimodal questions into step-by-step subquestions designed to simulate human reasoning through a chain-of-thought paradigm, with each subquestion associated with specific receptive or cognitive functions such as high-level visual reception and inference. Using a probing-based methodology, we identify attention heads that specialize in these functions and characterize them as functional heads. Our analysis across diverse VLM families reveals that these functional heads are universally sparse, vary in number and distribution across functions, and mediate interactions and hierarchical organization. Furthermore, intervention experiments demonstrate their critical role in multimodal reasoning: removing functional heads leads to performance degradation, while emphasizing them enhances accuracy. These findings provide new insights into the cognitive organization of VLMs and suggest promising directions for designing models with more human-aligned perceptual and reasoning abilities.

</details>


### [56] [Trustworthy Orchestration Artificial Intelligence by the Ten Criteria with Control-Plane Governance](https://arxiv.org/abs/2512.10304)
*Byeong Ho Kang,Wenli Yang,Muhammad Bilal Amin*

Main category: cs.AI

TL;DR: 提出可信编排AI的十大标准框架，将治理嵌入AI生态系统执行架构，确保可验证、透明、可重现和有意义的人类控制


<details>
  <summary>Details</summary>
Motivation: AI系统在关键决策中作用日益重要，但技术能力与制度问责之间存在差距，仅靠伦理指导不足，需要将治理架构嵌入执行层面

Method: 提出可信编排AI的十大标准框架，整合人类输入、语义一致性、审计和溯源完整性，构建统一的控制面板架构，借鉴国际标准和澳大利亚国家AI保障框架

Result: 开发了一个全面的保障框架，为整个AI组件、消费者和人类参与者提供治理覆盖，证明可信性可以通过工程方法系统性地融入AI系统

Conclusion: 可信编排AI框架能够将治理嵌入AI生态系统，确保执行层面保持可验证、透明、可重现和有意义的人类控制，填补技术能力与制度问责之间的差距

Abstract: As Artificial Intelligence (AI) systems increasingly assume consequential decision-making roles, a widening gap has emerged between technical capabilities and institutional accountability. Ethical guidance alone is insufficient to counter this challenge; it demands architectures that embed governance into the execution fabric of the ecosystem. This paper presents the Ten Criteria for Trustworthy Orchestration AI, a comprehensive assurance framework that integrates human input, semantic coherence, audit and provenance integrity into a unified Control-Panel architecture. Unlike conventional agentic AI initiatives that primarily focus on AI-to-AI coordination, the proposed framework provides an umbrella of governance to the entire AI components, their consumers and human participants. By taking aspiration from international standards and Australia's National Framework for AI Assurance initiative, this work demonstrates that trustworthiness can be systematically incorporated (by engineering) into AI systems, ensuring the execution fabric remains verifiable, transparent, reproducible and under meaningful human control.

</details>


### [57] [InfoCom: Kilobyte-Scale Communication-Efficient Collaborative Perception with Information Bottleneck](https://arxiv.org/abs/2512.10305)
*Quanmin Wei,Penglin Dai,Wei Li,Bingyi Liu,Xiao Wu*

Main category: cs.AI

TL;DR: InfoCom是一个基于信息瓶颈理论的信息感知协作感知框架，通过信息净化范式将通信开销从MB级降至KB级，实现近无损感知性能。


<details>
  <summary>Details</summary>
Motivation: 协作感知虽然能通过信息共享缓解单智能体感知的局限性，但面临通信与性能的权衡问题。现有通信高效方法通常假设MB级数据传输，在实际网络约束下可能失效。

Method: 提出信息感知框架InfoCom，基于扩展的信息瓶颈原理：1) 信息感知编码将特征压缩为最小消息同时保留感知相关信息；2) 稀疏掩码生成以可忽略的通信成本识别空间线索；3) 多尺度解码通过掩码引导机制逐步恢复感知信息而非简单特征重建。

Result: 在多个数据集上的实验表明，InfoCom实现近无损感知性能，同时将通信开销从MB级降至KB级，相比Where2comm和ERMVP分别减少440倍和90倍。

Conclusion: InfoCom为通信高效的协作感知建立了开创性理论基础，通过信息净化范式在信息瓶颈约束下理论优化提取最小充分的任务关键信息，显著降低通信开销同时保持感知性能。

Abstract: Precise environmental perception is critical for the reliability of autonomous driving systems. While collaborative perception mitigates the limitations of single-agent perception through information sharing, it encounters a fundamental communication-performance trade-off. Existing communication-efficient approaches typically assume MB-level data transmission per collaboration, which may fail due to practical network constraints. To address these issues, we propose InfoCom, an information-aware framework establishing the pioneering theoretical foundation for communication-efficient collaborative perception via extended Information Bottleneck principles. Departing from mainstream feature manipulation, InfoCom introduces a novel information purification paradigm that theoretically optimizes the extraction of minimal sufficient task-critical information under Information Bottleneck constraints. Its core innovations include: i) An Information-Aware Encoding condensing features into minimal messages while preserving perception-relevant information; ii) A Sparse Mask Generation identifying spatial cues with negligible communication cost; and iii) A Multi-Scale Decoding that progressively recovers perceptual information through mask-guided mechanisms rather than simple feature reconstruction. Comprehensive experiments across multiple datasets demonstrate that InfoCom achieves near-lossless perception while reducing communication overhead from megabyte to kilobyte-scale, representing 440-fold and 90-fold reductions per agent compared to Where2comm and ERMVP, respectively.

</details>


### [58] [User-Feedback-Driven Continual Adaptation for Vision-and-Language Navigation](https://arxiv.org/abs/2512.10322)
*Yongqiang Yu,Xuhui Li,Hazza Mahmood,Jinxing Zhou,Haodong Hong,Longtao Jiang,Zhiqiang Xu,Qi Wu,Xiaojun Chang*

Main category: cs.AI

TL;DR: 提出用户反馈驱动的视觉语言导航适应框架，通过整合人类交互和纠正信号来提升环境特定适应质量，在GSA-R2R基准上超越现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前GSA-VLN框架缺乏用户反馈机制，仅依赖无监督的环境暴露适应，而实际部署中用户反馈能提供有价值的监督信号，显著提升适应质量。

Method: 1. 将用户反馈（导航指令和纠正信号）转化为高质量环境对齐训练数据；2. 引入记忆库热启动机制重用已获取的环境知识；3. 在GSA-R2R基准上进行持续和混合适应设置实验。

Result: 在GSA-R2R基准上超越GR-DUET等强基线，提升导航成功率和路径效率；记忆库热启动稳定早期导航并减少更新后的性能下降；在持续和混合适应设置下均表现稳健。

Conclusion: 用户反馈驱动的适应框架能有效整合人类监督，提升视觉语言导航的环境特定适应能力，为实际部署提供更现实和高效的解决方案。

Abstract: Vision-and-Language Navigation (VLN) requires agents to navigate complex environments by following natural-language instructions. General Scene Adaptation for VLN (GSA-VLN) shifts the focus from zero-shot generalization to continual, environment-specific adaptation, narrowing the gap between static benchmarks and real-world deployment. However, current GSA-VLN frameworks exclude user feedback, relying solely on unsupervised adaptation from repeated environmental exposure. In practice, user feedback offers natural and valuable supervision that can significantly enhance adaptation quality. We introduce a user-feedback-driven adaptation framework that extends GSA-VLN by systematically integrating human interactions into continual learning. Our approach converts user feedback-navigation instructions and corrective signals-into high-quality, environment-aligned training data, enabling efficient and realistic adaptation. A memory-bank warm-start mechanism further reuses previously acquired environmental knowledge, mitigating cold-start degradation and ensuring stable redeployment. Experiments on the GSA-R2R benchmark show that our method consistently surpasses strong baselines such as GR-DUET, improving navigation success and path efficiency. The memory-bank warm start stabilizes early navigation and reduces performance drops after updates. Results under both continual and hybrid adaptation settings confirm the robustness and generality of our framework, demonstrating sustained improvement across diverse deployment conditions.

</details>


### [59] [On the Collapse of Generative Paths: A Criterion and Correction for Diffusion Steering](https://arxiv.org/abs/2512.10339)
*Ziseok Lee,Minyeong Hwang,Sanghyun Jo,Wooyeol Lee,Jihyung Ko,Young Bin Park,Jae-Mun Choi,Eunho Yang,Kyungsu Kim*

Main category: cs.AI

TL;DR: 论文提出ACE方法解决概率密度比引导中的边际路径崩溃问题，使异构模型组合生成更加稳定可靠


<details>
  <summary>Details</summary>
Motivation: 现有的概率密度比方法在组合异构模型时存在边际路径崩溃问题，导致中间密度不可归一化，限制了可控生成的应用

Method: 1) 推导路径存在性判据，仅从噪声调度和指数预测崩溃；2) 提出自适应路径校正指数(ACE)，扩展Feynman-Kac引导到时变指数，保证有效概率路径

Result: 在合成2D基准和柔性姿态支架装饰任务中，ACE消除了崩溃，实现了高引导组合生成，在分布和对接指标上优于恒定指数基线和专用模型

Conclusion: ACE将异构专家密度比引导从不稳定的启发式方法转变为可控生成的可靠工具

Abstract: Inference-time steering enables pretrained diffusion/flow models to be adapted to new tasks without retraining. A widely used approach is the ratio-of-densities method, which defines a time-indexed target path by reweighting probability-density trajectories from multiple models with positive, or in some cases, negative exponents. This construction, however, harbors a critical and previously unformalized failure mode: Marginal Path Collapse, where intermediate densities become non-normalizable even though endpoints remain valid. Collapse arises systematically when composing heterogeneous models trained on different noise schedules or datasets, including a common setting in molecular design where de-novo, conformer, and pocket-conditioned models must be combined for tasks such as flexible-pose scaffold decoration. We provide a novel and complete solution for the problem. First, we derive a simple path existence criterion that predicts exactly when collapse occurs from noise schedules and exponents alone. Second, we introduce Adaptive path Correction with Exponents (ACE), which extends Feynman-Kac steering to time-varying exponents and guarantees a valid probability path. On a synthetic 2D benchmark and on flexible-pose scaffold decoration, ACE eliminates collapse and enables high-guidance compositional generation, improving distributional and docking metrics over constant-exponent baselines and even specialized task-specific scaffold decoration models. Our work turns ratio-of-densities steering with heterogeneous experts from an unstable heuristic into a reliable tool for controllable generation.

</details>


### [60] [REMISVFU: Vertical Federated Unlearning via Representation Misdirection for Intermediate Output Feature](https://arxiv.org/abs/2512.10348)
*Wenhan Wu,Zhili He,Huanghuang Liang,Yili Gong,Jiawei Jiang,Chuang Hu,Dazhao Cheng*

Main category: cs.AI

TL;DR: REMISVFU：一种用于垂直联邦学习的表示误导框架，通过将遗忘方的编码器输出坍缩到单位球上的随机锚点来实现快速客户端级遗忘，同时通过正交投影对齐梯度来保持剩余方的模型效用。


<details>
  <summary>Details</summary>
Motivation: 现有联邦遗忘技术主要针对水平联邦学习（HFL），而垂直联邦学习（VFL）的特征分区架构使得HFL的遗忘方法无效。GDPR等数据保护法规要求联邦系统支持参与方的"被遗忘权"，因此需要专门针对VFL的遗忘解决方案。

Method: 提出REMISVFU表示误导框架：1）当收到删除请求时，遗忘方将其编码器输出坍缩到单位球上随机采样的锚点，切断其特征与全局模型的统计联系；2）服务器联合优化保留损失和遗忘损失，通过正交投影对齐梯度以消除破坏性干扰。

Result: 在公开基准测试中，REMISVFU将后门攻击成功率抑制到自然类先验水平，仅牺牲约2.5%的干净准确率，优于现有最先进的基线方法。

Conclusion: REMISVFU是首个针对splitVFL系统的即插即用表示误导框架，实现了快速、有效的客户端级遗忘，在保护隐私的同时保持了模型效用，为VFL环境下的数据保护法规合规提供了可行方案。

Abstract: Data-protection regulations such as the GDPR grant every participant in a federated system a right to be forgotten. Federated unlearning has therefore emerged as a research frontier, aiming to remove a specific party's contribution from the learned model while preserving the utility of the remaining parties. However, most unlearning techniques focus on Horizontal Federated Learning (HFL), where data are partitioned by samples. In contrast, Vertical Federated Learning (VFL) allows organizations that possess complementary feature spaces to train a joint model without sharing raw data. The resulting feature-partitioned architecture renders HFL-oriented unlearning methods ineffective. In this paper, we propose REMISVFU, a plug-and-play representation misdirection framework that enables fast, client-level unlearning in splitVFL systems. When a deletion request arrives, the forgetting party collapses its encoder output to a randomly sampled anchor on the unit sphere, severing the statistical link between its features and the global model. To maintain utility for the remaining parties, the server jointly optimizes a retention loss and a forgetting loss, aligning their gradients via orthogonal projection to eliminate destructive interference. Evaluations on public benchmarks show that REMISVFU suppresses back-door attack success to the natural class-prior level and sacrifices only about 2.5% points of clean accuracy, outperforming state-of-the-art baselines.

</details>


### [61] [LLM-Empowered Representation Learning for Emerging Item Recommendation](https://arxiv.org/abs/2512.10370)
*Ziying Zhang,Quanming Yao,Yaqing Wang*

Main category: cs.AI

TL;DR: 提出EmerFlow框架，利用LLM增强表示学习，为新兴物品生成独特嵌入，解决推荐系统中新兴物品因交互数据有限而难以推荐的问题。


<details>
  <summary>Details</summary>
Motivation: 现有推荐方法通常假设新兴物品历史交互很少甚至没有，这种假设过于简化问题。好的模型需要同时保留新兴物品的独特性，并利用其与成熟物品的共享模式。

Method: EmerFlow框架：1) 通过LLM推理丰富新兴物品的原始特征；2) 将这些表示与现有推荐模型的嵌入空间对齐；3) 通过元学习整合新交互来优化嵌入。

Result: 在电影和医药等多个领域的广泛实验中，EmerFlow始终优于现有方法，能够从有限的交互中学习到新兴物品的表达性嵌入。

Conclusion: EmerFlow通过LLM增强的表示学习框架，有效解决了新兴物品推荐问题，在保留物品独特性的同时利用共享模式，仅需有限交互即可学习高质量嵌入。

Abstract: In this work, we tackle the challenge of recommending emerging items, whose interactions gradually accumulate over time. Existing methods often overlook this dynamic process, typically assuming that emerging items have few or even no historical interactions. Such an assumption oversimplifies the problem, as a good model must preserve the uniqueness of emerging items while leveraging their shared patterns with established ones. To address this challenge, we propose EmerFlow, a novel LLM-empowered representation learning framework that generates distinctive embeddings for emerging items. It first enriches the raw features of emerging items through LLM reasoning, then aligns these representations with the embedding space of the existing recommendation model. Finally, new interactions are incorporated through meta-learning to refine the embeddings. This enables EmerFlow to learn expressive embeddings for emerging items from only limited interactions. Extensive experiments across diverse domains, including movies and pharmaceuticals, show that EmerFlow consistently outperforms existing methods.

</details>


### [62] [AgentProg: Empowering Long-Horizon GUI Agents with Program-Guided Context Management](https://arxiv.org/abs/2512.10371)
*Shizuo Tian,Hao Wen,Yuxuan Chen,Jiacheng Liu,Shanhui Zhao,Guohong Liu,Ju Ren,Yunxin Liu,Yuanchun Li*

Main category: cs.AI

TL;DR: AgentProg：一种程序引导的移动GUI智能体上下文管理方法，通过将交互历史重构为程序结构来减少上下文开销，同时保持任务性能


<details>
  <summary>Details</summary>
Motivation: 移动GUI智能体的长时任务自动化面临关键瓶颈：依赖不断扩展的交互历史会导致大量上下文开销。现有的上下文管理和压缩技术往往无法保留关键语义信息，导致任务性能下降。

Method: 提出AgentProg，一种程序引导的智能体上下文管理方法：1) 将交互历史重构为包含变量和控制流的程序结构；2) 基于程序结构提供原则性机制来决定保留或丢弃哪些信息；3) 集成受信念MDP框架启发的全局信念状态机制，处理部分可观测性和适应环境变化。

Result: 在AndroidWorld和扩展的长时任务套件上的实验表明，AgentProg在这些基准测试中达到了最先进的成功率。更重要的是，在长时任务中保持稳健性能，而基线方法则出现灾难性性能下降。

Conclusion: AgentProg通过程序化重构交互历史，有效解决了移动GUI智能体长时任务中的上下文管理问题，在减少开销的同时保持了任务性能，为智能体上下文管理提供了新思路。

Abstract: The rapid development of mobile GUI agents has stimulated growing research interest in long-horizon task automation. However, building agents for these tasks faces a critical bottleneck: the reliance on ever-expanding interaction history incurs substantial context overhead. Existing context management and compression techniques often fail to preserve vital semantic information, leading to degraded task performance. We propose AgentProg, a program-guided approach for agent context management that reframes the interaction history as a program with variables and control flow. By organizing information according to the structure of program, this structure provides a principled mechanism to determine which information should be retained and which can be discarded. We further integrate a global belief state mechanism inspired by Belief MDP framework to handle partial observability and adapt to unexpected environmental changes. Experiments on AndroidWorld and our extended long-horizon task suite demonstrate that AgentProg has achieved the state-of-the-art success rates on these benchmarks. More importantly, it maintains robust performance on long-horizon tasks while baseline methods experience catastrophic degradation. Our system is open-sourced at https://github.com/MobileLLM/AgentProg.

</details>


### [63] [Boosting RL-Based Visual Reasoning with Selective Adversarial Entropy Intervention](https://arxiv.org/abs/2512.10414)
*Yang Yu,Zhuangzhuang Chen,Siqi Wang,Lanqing Li,Xiaomeng Li*

Main category: cs.AI

TL;DR: 提出SaEI方法，通过选择性对抗熵干预增强视觉语言模型的推理能力，在RL采样阶段引入熵干预来提升响应多样性


<details>
  <summary>Details</summary>
Motivation: 现有基于RL的微调方法主要在策略优化阶段干预特定token的熵，忽略了在RL采样阶段进行熵干预可以提升响应多样性，从而改善GRPO性能

Method: 提出选择性对抗熵干预(SaEI)：1) 熵引导对抗采样(EgAS)，将采样响应的熵作为对抗目标，用对抗梯度攻击视觉输入生成对抗样本；2) token选择性熵计算(TsEC)，在不扭曲VLM事实知识的前提下最大化对抗攻击效果

Result: 在领域内和领域外数据集上的实验表明，该方法能显著提升策略探索能力，增强推理能力

Conclusion: SaEI通过在RL采样阶段引入选择性对抗熵干预，有效提升了视觉语言模型的推理能力，解决了现有方法忽略采样阶段熵干预的问题

Abstract: Recently, reinforcement learning (RL) has become a common choice in enhancing the reasoning capabilities of vision-language models (VLMs). Considering existing RL- based finetuning methods, entropy intervention turns out to be an effective way to benefit exploratory ability, thereby improving policy performance. Notably, most existing stud- ies intervene in entropy by simply controlling the update of specific tokens during policy optimization of RL. They ig- nore the entropy intervention during the RL sampling that can boost the performance of GRPO by improving the di- versity of responses. In this paper, we propose Selective- adversarial Entropy Intervention, namely SaEI, which en- hances policy entropy by distorting the visual input with the token-selective adversarial objective coming from the en- tropy of sampled responses. Specifically, we first propose entropy-guided adversarial sampling (EgAS) that formu- lates the entropy of sampled responses as an adversarial ob- jective. Then, the corresponding adversarial gradient can be used to attack the visual input for producing adversarial samples, allowing the policy model to explore a larger an- swer space during RL sampling. Then, we propose token- selective entropy computation (TsEC) to maximize the ef- fectiveness of adversarial attack in EgAS without distorting factual knowledge within VLMs. Extensive experiments on both in-domain and out-of-domain datasets show that our proposed method can greatly improve policy exploration via entropy intervention, to boost reasoning capabilities. Code will be released once the paper is accepted.

</details>


### [64] [Planning, Living and Judging: A Multi-agent LLM-based Framework for Cyclical Urban Planning](https://arxiv.org/abs/2412.20505)
*Hang Ni,Yuzhi Wang,Hao Liu*

Main category: cs.AI

TL;DR: 提出CUP（循环城市规划）新范式，利用多智能体LLM框架实现城市计划的持续生成、评估和优化


<details>
  <summary>Details</summary>
Motivation: 城市化背景下城市更新面临重大挑战，需要适应性的方法来应对不断变化的需求。利用大语言模型的进展，提出一种能够持续优化城市计划的新方法。

Method: 提出循环城市规划（CUP）范式，采用多智能体LLM框架，包含三个核心组件：规划（生成和优化城市计划）、生活（模拟居民行为互动）、评判（评估计划效果并提供迭代反馈），形成闭环循环过程。

Result: 在真实世界数据集上的实验证明了该框架作为连续自适应规划过程的有效性。

Conclusion: CUP框架能够实现动态响应式的城市规划方法，为城市更新提供了一种创新的技术解决方案。

Abstract: Urban regeneration presents significant challenges within the context of urbanization, requiring adaptive approaches to tackle evolving needs. Leveraging advancements in large language models (LLMs), we propose Cyclical Urban Planning (CUP), a new paradigm that continuously generates, evaluates, and refines urban plans in a closed-loop. Specifically, our multi-agent LLM-based framework consists of three key components: (1) Planning, where LLM agents generate and refine urban plans based on contextual data; (2) Living, where agents simulate the behaviors and interactions of residents, modeling life in the urban environment; and (3) Judging, which involves evaluating plan effectiveness and providing iterative feedback for improvement. The cyclical process enables a dynamic and responsive planning approach. Experiments on the real-world dataset demonstrate the effectiveness of our framework as a continuous and adaptive planning process.

</details>


### [65] [Representation of the structure of graphs by sequences of instructions](https://arxiv.org/abs/2512.10429)
*Ezequiel Lopez-Rubio*

Main category: cs.AI

TL;DR: 提出一种将图邻接矩阵表示为字符串指令序列的新方法，使图结构能适配深度学习语言模型处理


<details>
  <summary>Details</summary>
Motivation: 当前图表示方法（基于邻接矩阵）不适合深度学习语言模型处理，而深度学习语言模型在文本处理方面表现出强大能力，需要一种能让图结构适配这些模型的新表示方法

Method: 将图的邻接矩阵转化为一系列简单指令组成的字符串，这些指令逐步构建邻接矩阵，该转换是可逆的（图↔字符串）

Result: 提出的表示方法紧凑且保持图的局部结构模式，初步计算实验显示有良好效果

Conclusion: 这种新的图表示方法有望提升深度学习模型对图的处理能力

Abstract: The representation of graphs is commonly based on the adjacency matrix concept. This formulation is the foundation of most algebraic and computational approaches to graph processing. The advent of deep learning language models offers a wide range of powerful computational models that are specialized in the processing of text. However, current procedures to represent graphs are not amenable to processing by these models. In this work, a new method to represent graphs is proposed. It represents the adjacency matrix of a graph by a string of simple instructions. The instructions build the adjacency matrix step by step. The transformation is reversible, i.e. given a graph the string can be produced and vice versa. The proposed representation is compact and it maintains the local structural patterns of the graph. Therefore, it is envisaged that it could be useful to boost the processing of graphs by deep learning models. A tentative computational experiment is reported, with favorable results.

</details>


### [66] [Targeted Data Protection for Diffusion Model by Matching Training Trajectory](https://arxiv.org/abs/2512.10433)
*Hojun Lee,Mijin Koo,Yeji Song,Nojun Kwak*

Main category: cs.AI

TL;DR: TAFAP是一种通过轨迹对齐实现目标数据保护的新方法，能够有效控制扩散模型微调过程，将输出重定向到用户指定的目标概念，同时保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型个性化微调技术虽然易于使用，但存在未经授权数据使用和隐私侵犯的风险。现有保护方法仅能被动降低图像质量，缺乏稳定控制能力。目标数据保护（TDP）虽能主动重定向输出，但现有方法基于快照匹配，无法考虑完整学习动态，导致可控性差。

Method: TAFAP（基于对抗扰动的轨迹对齐微调）通过控制整个训练轨迹来实现有效的TDP。该方法受数据集蒸馏启发，采用轨迹匹配而非快照匹配，在整个微调过程中强制执行持久、可验证的转换。通过对抗扰动确保模型输出持续重定向到目标概念。

Result: TAFAP在扩散模型中首次成功实现了目标转换，同时控制身份和视觉模式。该方法显著优于现有TDP尝试，能够实现向目标概念的稳健重定向，同时保持高图像质量。实验验证了其有效性。

Conclusion: TAFAP为扩散模型提供了可验证的安全保障，建立了一个控制和追踪模型输出变化的新框架，解决了现有保护方法的局限性，实现了对训练过程的全面控制。

Abstract: Recent advancements in diffusion models have made fine-tuning text-to-image models for personalization increasingly accessible, but have also raised significant concerns regarding unauthorized data usage and privacy infringement. Current protection methods are limited to passively degrading image quality, failing to achieve stable control. While Targeted Data Protection (TDP) offers a promising paradigm for active redirection toward user-specified target concepts, existing TDP attempts suffer from poor controllability due to snapshot-matching approaches that fail to account for complete learning dynamics. We introduce TAFAP (Trajectory Alignment via Fine-tuning with Adversarial Perturbations), the first method to successfully achieve effective TDP by controlling the entire training trajectory. Unlike snapshot-based methods whose protective influence is easily diluted as training progresses, TAFAP employs trajectory-matching inspired by dataset distillation to enforce persistent, verifiable transformations throughout fine-tuning. We validate our method through extensive experiments, demonstrating the first successful targeted transformation in diffusion models with simultaneous control over both identity and visual patterns. TAFAP significantly outperforms existing TDP attempts, achieving robust redirection toward target concepts while maintaining high image quality. This work enables verifiable safeguards and provides a new framework for controlling and tracing alterations in diffusion model outputs.

</details>


### [67] [When Reject Turns into Accept: Quantifying the Vulnerability of LLM-Based Scientific Reviewers to Indirect Prompt Injection](https://arxiv.org/abs/2512.10449)
*Devanshu Sahoo,Manish Prasad,Vasudev Majhi,Jahnvi Singh,Vinay Chamola,Yash Sinha,Murari Mandal,Dhruv Kumar*

Main category: cs.AI

TL;DR: 论文研究了科学同行评审中LLM评估系统的对抗性PDF操纵脆弱性，开发了WAVS评估指标，展示了攻击策略如何成功翻转评审决定


<details>
  <summary>Details</summary>
Motivation: 随着LLM在科学同行评审中的广泛应用（包括个人使用和机构部署），需要评估这些"LLM-as-a-Judge"系统对对抗性PDF操纵的鲁棒性，特别是针对"拒绝"翻转为"接受"这一特定动机的攻击

Method: 构建了200篇科学论文的数据集，针对评审任务调整了15种领域特定的攻击策略，在包括GPT-5、Claude Haiku和DeepSeek在内的13个语言模型上进行评估，开发了WAVS（加权对抗脆弱性分数）作为评估指标

Result: 研究结果显示，如"Maximum Mark Magyk"等混淆策略能够成功操纵评分，即使在大型模型中也能实现令人担忧的决定翻转率，证明了LLM评审系统对对抗性攻击的脆弱性

Conclusion: LLM评审系统容易受到对抗性PDF操纵攻击，需要加强安全措施，作者将发布完整数据集和注入框架以促进该领域更多研究

Abstract: The landscape of scientific peer review is rapidly evolving with the integration of Large Language Models (LLMs). This shift is driven by two parallel trends: the widespread individual adoption of LLMs by reviewers to manage workload (the "Lazy Reviewer" hypothesis) and the formal institutional deployment of AI-powered assessment systems by conferences like AAAI and Stanford's Agents4Science. This study investigates the robustness of these "LLM-as-a-Judge" systems (both illicit and sanctioned) to adversarial PDF manipulation. Unlike general jailbreaks, we focus on a distinct incentive: flipping "Reject" decisions to "Accept," for which we develop a novel evaluation metric which we term as WAVS (Weighted Adversarial Vulnerability Score). We curated a dataset of 200 scientific papers and adapted 15 domain-specific attack strategies to this task, evaluating them across 13 Language Models, including GPT-5, Claude Haiku, and DeepSeek. Our results demonstrate that obfuscation strategies like "Maximum Mark Magyk" successfully manipulate scores, achieving alarming decision flip rates even in large-scale models. We will release our complete dataset and injection framework to facilitate more research on this topic.

</details>


### [68] [Zero-shot 3D Map Generation with LLM Agents: A Dual-Agent Architecture for Procedural Content Generation](https://arxiv.org/abs/2512.10501)
*Lim Chien Her,Ming Yan,Yunshu Bai,Ruihao Li,Hao Zhang*

Main category: cs.AI

TL;DR: 提出一种无需训练的LLM智能体架构，通过Actor-Critic双智能体迭代工作流，实现零样本PCG参数配置，将自然语言指令转化为精确的技术参数。


<details>
  <summary>Details</summary>
Motivation: PCG需要精确配置不透明的技术参数，而现成的LLM难以弥合抽象用户指令与严格参数规范之间的语义鸿沟，需要更有效的控制方法。

Method: 采用Actor-Critic双智能体架构：Actor负责生成参数配置，Critic负责评估和反馈，通过迭代工作流自主推理工具参数并逐步优化配置以符合人类设计偏好。

Result: 在3D地图生成任务上验证，建立了PCG指令跟随的新基准，性能优于单智能体基线，能从自然语言描述生成多样且结构有效的环境。

Conclusion: 现成LLM可有效重用作通用智能体处理任意PCG工具，通过将负担从模型训练转向架构推理，为无需任务特定微调而掌握复杂软件提供了可扩展框架。

Abstract: Procedural Content Generation (PCG) offers scalable methods for algorithmically creating complex, customizable worlds. However, controlling these pipelines requires the precise configuration of opaque technical parameters. We propose a training-free architecture that utilizes LLM agents for zero-shot PCG parameter configuration. While Large Language Models (LLMs) promise a natural language interface for PCG tools, off-the-shelf models often fail to bridge the semantic gap between abstract user instructions and strict parameter specifications. Our system pairs an Actor agent with a Critic agent, enabling an iterative workflow where the system autonomously reasons over tool parameters and refines configurations to progressively align with human design preferences. We validate this approach on the generation of various 3D maps, establishing a new benchmark for instruction-following in PCG. Experiments demonstrate that our approach outperforms single-agent baselines, producing diverse and structurally valid environments from natural language descriptions. These results demonstrate that off-the-shelf LLMs can be effectively repurposed as generalized agents for arbitrary PCG tools. By shifting the burden from model training to architectural reasoning, our method offers a scalable framework for mastering complex software without task-specific fine-tuning.

</details>


### [69] [Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning](https://arxiv.org/abs/2512.10534)
*Haiteng Zhao,Junhao Shen,Yiming Zhang,Songyang Gao,Kuikun Liu,Tianyou Ma,Fan Zheng,Dahua Lin,Wenwei Zhang,Kai Chen*

Main category: cs.AI

TL;DR: InternGeometry是首个达到IMO金牌水平的LLM几何智能体，仅用13K训练样本（AlphaGeometry 2的0.004%）解决了44/50个IMO几何问题，超越了人类金牌选手平均分。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在几何问题解决中由于辅助构造的启发式能力较弱，仍被AlphaGeometry 2等专家模型主导，这些模型依赖大规模数据合成和搜索。本研究旨在构建首个达到奖牌水平的LLM几何智能体。

Method: 1. 迭代提出命题和辅助构造，用符号引擎验证，并根据反馈指导后续提议；2. 动态记忆机制支持每个问题与符号引擎进行200多次交互；3. 引入复杂度提升强化学习（CBRL），在训练阶段逐步增加合成问题的复杂度。

Result: 在InternThinker-32B基础上构建的InternGeometry解决了2000-2024年50个IMO几何问题中的44个（88%），超过了人类金牌选手平均分（40.9），仅使用13K训练样本（AlphaGeometry 2的0.004%）。还能为IMO问题提出人类解法中未出现的新颖辅助构造。

Conclusion: InternGeometry展示了LLM智能体在专家级几何任务上的潜力，通过创新的交互式验证和强化学习方法，以极小的数据需求达到了金牌水平，为几何AI研究提供了新方向。

Abstract: Large language model (LLM) agents exhibit strong mathematical problem-solving abilities and can even solve International Mathematical Olympiad (IMO) level problems with the assistance of formal proof systems. However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation. In this work, we make the first attempt to build a medalist-level LLM agent for geometry and present InternGeometry. InternGeometry overcomes the heuristic limitations in geometry by iteratively proposing propositions and auxiliary constructions, verifying them with a symbolic engine, and reflecting on the engine's feedback to guide subsequent proposals. A dynamic memory mechanism enables InternGeometry to conduct more than two hundred interactions with the symbolic engine per problem. To further accelerate learning, we introduce Complexity-Boosting Reinforcement Learning (CBRL), which gradually increases the complexity of synthesized problems across training stages. Built on InternThinker-32B, InternGeometry solves 44 of 50 IMO geometry problems (2000-2024), exceeding the average gold medalist score (40.9), using only 13K training examples, just 0.004% of the data used by AlphaGeometry 2, demonstrating the potential of LLM agents on expert-level geometry tasks. InternGeometry can also propose novel auxiliary constructions for IMO problems that do not appear in human solutions. We will release the model, data, and symbolic engine to support future research.

</details>


### [70] [NormCode: A Semi-Formal Language for Context-Isolated AI Planning](https://arxiv.org/abs/2512.10563)
*Xin Guan*

Main category: cs.AI

TL;DR: NormCode是一种半正式语言，用于构建推理计划，通过数据隔离消除多步LLM工作流中的上下文污染问题。


<details>
  <summary>Details</summary>
Motivation: 多步LLM工作流存在上下文污染问题：随着信息在步骤间累积，模型会产生幻觉、混淆中间输出并失去任务约束跟踪。需要一种方法来消除跨步骤污染。

Method: NormCode是一种半正式语言，创建推理计划的结构化分解，每个步骤在数据隔离中运行，仅接收显式传递的输入。它严格分离语义操作（LLM驱动的推理，非确定性）和语法操作（确定性数据重组），支持三种同构格式：.ncds（人工编写）、.ncd（机器执行）和.ncn（人工验证）。

Result: 通过两个演示验证：1）基础X加法算法在任意长度输入上实现100%准确率；2）自托管执行NormCode自身的五阶段编译器流水线。工作编排器提供依赖驱动调度、SQLite支持的检查点和循环管理。

Conclusion: NormCode使AI工作流程可审计设计，解决了法律推理、医疗决策和金融分析等高风险领域对透明度的关键需求。

Abstract: Multistep workflows that chain large language model (LLM) calls suffer from context pollution: as information accumulates across steps, models hallucinate, confuse intermediate outputs, and lose track of task constraints. We present NormCode, a semiformal language for constructing plans of inferences, structured decompositions where each step operates in data isolation and receives only explicitly passed inputs, which eliminates crossstep contamination by design. NormCode enforces a strict separation between semantic operations (LLMdriven reasoning, nondeterministic) and syntactic operations (deterministic data restructuring), enabling precise cost and reliability tracing. The language exists in three isomorphic formats: .ncds for human authoring, .ncd for machine execution, and .ncn for human verification, supporting progressive formalization from sketch to production. We validate NormCode through two demonstrations: (1) a base X addition algorithm achieving 100 percent accuracy on arbitrary length inputs, and (2) self hosted execution of NormCode's own five phase compiler pipeline. The working orchestrator provides dependency driven scheduling, SQLite backed checkpointing, and loop management, making AI workflows auditable by design and addressing a critical need for transparency in high stakes domains such as legal reasoning, medical decision making, and financial analysis.

</details>


### [71] [Phythesis: Physics-Guided Evolutionary Scene Synthesis for Energy-Efficient Data Center Design via LLMs](https://arxiv.org/abs/2512.10611)
*Minghao LI,Ruihang Wang,Rui Tan,Yonggang Wen*

Main category: cs.AI

TL;DR: Phythesis是一个结合大语言模型和物理引导进化优化的框架，用于自动化生成仿真就绪的数据中心场景，实现能源高效设计。


<details>
  <summary>Details</summary>
Motivation: 传统数据中心设计方法依赖人工经验和专用仿真工具，难以应对日益增长的系统复杂性。现有生成式AI方法不考虑底层物理原理，无法满足数据中心对量化运营目标和严格物理约束的要求。

Method: 采用双层迭代优化架构：1) LLM驱动优化层生成物理合理的三维布局并进行自我批评以优化场景拓扑；2) 物理信息优化层识别最优资产参数并选择最佳资产组合。

Result: 在三个生成规模上的实验表明，与纯LLM解决方案相比，Phythesis实现了57.3%的生成成功率提升和11.5%的电源使用效率(PUE)改进。

Conclusion: Phythesis成功将LLM与物理引导优化相结合，为能源高效的数据中心设计提供了自动化仿真就绪场景合成的有效框架。

Abstract: Data center (DC) infrastructure serves as the backbone to support the escalating demand for computing capacity. Traditional design methodologies that blend human expertise with specialized simulation tools scale poorly with the increasing system complexity. Recent studies adopt generative artificial intelligence to design plausible human-centric indoor layouts. However, they do not consider the underlying physics, making them unsuitable for the DC design that sets quantifiable operational objectives and strict physical constraints. To bridge the gap, we propose Phythesis, a novel framework that synergizes large language models (LLMs) and physics-guided evolutionary optimization to automate simulation-ready (SimReady) scene synthesis for energy-efficient DC design. Phythesis employs an iterative bi-level optimization architecture, where (i) the LLM-driven optimization level generates physically plausible three-dimensional layouts and self-criticizes them to refine the scene topology, and (ii) the physics-informed optimization level identifies the optimal asset parameters and selects the best asset combination. Experiments on three generation scales show that Phythesis achieves 57.3% generation success rate increase and 11.5% power usage effectiveness (PUE) improvement, compared with the vanilla LLM-based solution.

</details>


### [72] [Remember Me, Refine Me: A Dynamic Procedural Memory Framework for Experience-Driven Agent Evolution](https://arxiv.org/abs/2512.10696)
*Zouying Cao,Jiaji Deng,Li Yu,Weikang Zhou,Zhaoyang Liu,Bolin Ding,Hai Zhao*

Main category: cs.AI

TL;DR: ReMe是一个记忆驱动的智能体进化框架，通过多维度经验提炼、上下文自适应重用和效用驱动的记忆精炼，实现智能体的持续学习和进化。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体记忆框架主要采用"被动积累"范式，将记忆视为静态的只追加档案，无法实现动态推理和持续进化。需要弥合静态存储与动态推理之间的差距。

Method: 提出ReMe框架，包含三个核心机制：1) 多维度经验提炼：识别成功模式、分析失败触发因素、生成比较性见解；2) 上下文自适应重用：通过场景感知索引将历史见解适配到新情境；3) 效用驱动精炼：自主添加有效记忆并修剪过时记忆，维护紧凑高质量的经验池。

Result: 在BFCL-V3和AppWorld基准测试中达到最先进水平。观察到显著的内存缩放效应：配备ReMe的Qwen3-8B性能优于更大的无记忆Qwen3-14B，表明自进化记忆为终身学习提供了计算高效的途径。

Conclusion: ReMe框架通过主动记忆管理实现了智能体的持续进化，为LLM智能体的终身学习提供了有效的解决方案，并发布了代码和reme.library数据集以促进进一步研究。

Abstract: Procedural memory enables large language model (LLM) agents to internalize "how-to" knowledge, theoretically reducing redundant trial-and-error. However, existing frameworks predominantly suffer from a "passive accumulation" paradigm, treating memory as a static append-only archive. To bridge the gap between static storage and dynamic reasoning, we propose $\textbf{ReMe}$ ($\textit{Remember Me, Refine Me}$), a comprehensive framework for experience-driven agent evolution. ReMe innovates across the memory lifecycle via three mechanisms: 1) $\textit{multi-faceted distillation}$, which extracts fine-grained experiences by recognizing success patterns, analyzing failure triggers and generating comparative insights; 2) $\textit{context-adaptive reuse}$, which tailors historical insights to new contexts via scenario-aware indexing; and 3) $\textit{utility-based refinement}$, which autonomously adds valid memories and prunes outdated ones to maintain a compact, high-quality experience pool. Extensive experiments on BFCL-V3 and AppWorld demonstrate that ReMe establishes a new state-of-the-art in agent memory system. Crucially, we observe a significant memory-scaling effect: Qwen3-8B equipped with ReMe outperforms larger, memoryless Qwen3-14B, suggesting that self-evolving memory provides a computation-efficient pathway for lifelong learning. We release our code and the $\texttt{reme.library}$ dataset to facilitate further research.

</details>


### [73] [Refinement Contrastive Learning of Cell-Gene Associations for Unsupervised Cell Type Identification](https://arxiv.org/abs/2512.10640)
*Liang Peng,Haopeng Liu,Yixuan Ye,Cheng Liu,Wenjun Shen,Si Wu,Hau-San Wong*

Main category: cs.AI

TL;DR: scRCL是一个无监督细胞类型识别框架，通过结合细胞-基因相互作用和对比学习，提升单细胞组学数据中细胞类型识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的无监督细胞类型识别方法大多只关注细胞内在结构，忽略了细胞-基因关联的关键作用，导致难以区分密切相关的细胞类型。

Method: 提出scRCL框架，包含两个对比分布对齐组件来揭示可靠的细胞内在结构，以及一个整合基因相关结构学习的精炼模块来增强细胞嵌入表示。

Result: 在多个单细胞RNA-seq和空间转录组基准数据集上的实验表明，该方法在细胞类型识别准确率上持续优于现有最先进方法。

Conclusion: scRCL通过有效利用细胞-基因相互作用，能够获得更具生物学意义的细胞表示，提高细胞类型识别的准确性，并得到下游生物学分析的验证。

Abstract: Unsupervised cell type identification is crucial for uncovering and characterizing heterogeneous populations in single cell omics studies. Although a range of clustering methods have been developed, most focus exclusively on intrinsic cellular structure and ignore the pivotal role of cell-gene associations, which limits their ability to distinguish closely related cell types. To this end, we propose a Refinement Contrastive Learning framework (scRCL) that explicitly incorporates cell-gene interactions to derive more informative representations. Specifically, we introduce two contrastive distribution alignment components that reveal reliable intrinsic cellular structures by effectively exploiting cell-cell structural relationships. Additionally, we develop a refinement module that integrates gene-correlation structure learning to enhance cell embeddings by capturing underlying cell-gene associations. This module strengthens connections between cells and their associated genes, refining the representation learning to exploiting biologically meaningful relationships. Extensive experiments on several single-cell RNA-seq and spatial transcriptomics benchmark datasets demonstrate that our method consistently outperforms state-of-the-art baselines in cell-type identification accuracy. Moreover, downstream biological analyses confirm that the recovered cell populations exhibit coherent gene-expression signatures, further validating the biological relevance of our approach. The code is available at https://github.com/THPengL/scRCL.

</details>


### [74] [Replace, Don't Expand: Mitigating Context Dilution in Multi-Hop RAG via Fixed-Budget Evidence Assembly](https://arxiv.org/abs/2512.10787)
*Moshe Lahmy,Roi Yozevitch*

Main category: cs.AI

TL;DR: SEAL-RAG提出"替换而非扩展"策略，通过搜索-提取-评估-循环机制主动替换干扰信息，解决多跳查询中的上下文稀释问题，在固定检索深度下显著提升答案正确性和证据精度。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统在处理多跳查询时，当初始检索遗漏桥接事实时会失败。传统纠正方法（如Self-RAG、CRAG、Adaptive-k）通常通过添加更多上下文或修剪现有列表来解决，但这会导致上下文稀释问题，即干扰信息挤占相关信息。

Method: SEAL-RAG采用训练免费的控制器，执行搜索→提取→评估→循环（SEAL）周期：进行实时实体锚定提取构建缺失实体/关系的间隙规范，触发定向微查询，并使用实体优先排名主动将干扰信息替换为填补间隙的证据。

Result: 在HotpotQA（k=3）上，SEAL比Self-RAG提高答案正确性3-13个百分点，证据精度提高12-18个百分点。在2WikiMultiHopQA（k=5）上，比Adaptive-k准确率提高8.0个百分点，证据精度保持96%（CRAG仅为22%）。所有改进均具有统计显著性（p<0.001）。

Conclusion: SEAL-RAG通过固定k替换策略，在保持可预测成本的同时，确保top-k槽位针对精度而非广度进行优化，有效解决了多跳查询中的上下文稀释问题。

Abstract: Retrieval-Augmented Generation (RAG) systems often fail on multi-hop queries when the initial retrieval misses a bridge fact. Prior corrective approaches, such as Self-RAG, CRAG, and Adaptive-$k$, typically address this by \textit{adding} more context or pruning existing lists. However, simply expanding the context window often leads to \textbf{context dilution}, where distractors crowd out relevant information. We propose \textbf{SEAL-RAG}, a training-free controller that adopts a \textbf{``replace, don't expand''} strategy to fight context dilution under a fixed retrieval depth $k$. SEAL executes a (\textbf{S}earch $\rightarrow$ \textbf{E}xtract $\rightarrow$ \textbf{A}ssess $\rightarrow$ \textbf{L}oop) cycle: it performs on-the-fly, entity-anchored extraction to build a live \textit{gap specification} (missing entities/relations), triggers targeted micro-queries, and uses \textit{entity-first ranking} to actively swap out distractors for gap-closing evidence. We evaluate SEAL-RAG against faithful re-implementations of Basic RAG, CRAG, Self-RAG, and Adaptive-$k$ in a shared environment on \textbf{HotpotQA} and \textbf{2WikiMultiHopQA}. On HotpotQA ($k=3$), SEAL improves answer correctness by \textbf{+3--13 pp} and evidence precision by \textbf{+12--18 pp} over Self-RAG. On 2WikiMultiHopQA ($k=5$), it outperforms Adaptive-$k$ by \textbf{+8.0 pp} in accuracy and maintains \textbf{96\%} evidence precision compared to 22\% for CRAG. These gains are statistically significant ($p<0.001$). By enforcing fixed-$k$ replacement, SEAL yields a predictable cost profile while ensuring the top-$k$ slots are optimized for precision rather than mere breadth. We release our code and data at https://github.com/mosherino/SEAL-RAG.

</details>


### [75] [CAPTAIN: Semantic Feature Injection for Memorization Mitigation in Text-to-Image Diffusion Models](https://arxiv.org/abs/2512.10655)
*Tong Zhang,Carlos Hinojosa,Bernard Ghanem*

Main category: cs.AI

TL;DR: CAPTAIN：一种无需训练的框架，通过修改去噪过程中的潜在特征来减少扩散模型的记忆化问题，在保持提示对齐的同时显著降低训练数据复制


<details>
  <summary>Details</summary>
Motivation: 扩散模型可能会无意中复制训练示例，引发隐私和版权问题。现有的推理时缓解方法（如操纵无分类器引导或扰动提示嵌入）往往难以在不损害提示对齐的情况下减少记忆化

Method: CAPTAIN采用三阶段方法：1）基于频率的噪声初始化，减少早期去噪过程中复制记忆模式的倾向；2）识别特征注入的最佳去噪时间步并定位记忆区域；3）将非记忆参考图像的语义对齐特征注入到定位的潜在区域，抑制记忆化同时保持提示保真度和视觉质量

Result: 实验表明，CAPTAIN相比基于CFG的基线方法显著减少了记忆化，同时保持了与预期提示的强对齐

Conclusion: CAPTAIN提供了一种有效的训练免费框架，能够在扩散模型中缓解记忆化问题，平衡隐私保护与生成质量的需求

Abstract: Diffusion models can unintentionally reproduce training examples, raising privacy and copyright concerns as these systems are increasingly deployed at scale. Existing inference-time mitigation methods typically manipulate classifier-free guidance (CFG) or perturb prompt embeddings; however, they often struggle to reduce memorization without compromising alignment with the conditioning prompt. We introduce CAPTAIN, a training-free framework that mitigates memorization by directly modifying latent features during denoising. CAPTAIN first applies frequency-based noise initialization to reduce the tendency to replicate memorized patterns early in the denoising process. It then identifies the optimal denoising timesteps for feature injection and localizes memorized regions. Finally, CAPTAIN injects semantically aligned features from non-memorized reference images into localized latent regions, suppressing memorization while preserving prompt fidelity and visual quality. Our experiments show that CAPTAIN achieves substantial reductions in memorization compared to CFG-based baselines while maintaining strong alignment with the intended prompt.

</details>


### [76] [On the Dynamics of Multi-Agent LLM Communities Driven by Value Diversity](https://arxiv.org/abs/2512.10665)
*Muhua Huang,Qinlin Zhao,Xiaoyuan Yi,Xing Xie*

Main category: cs.AI

TL;DR: 研究探讨了大型语言模型多智能体系统中价值多样性如何影响集体行为，发现价值多样性增强价值稳定性、促进涌现行为、带来更多创造性原则，但存在边际递减效应


<details>
  <summary>Details</summary>
Motivation: 随着基于LLM的多智能体系统日益普及，这些人工社区的集体行为（如集体智能）受到越来越多的关注。本研究旨在回答一个基本问题：价值多样性如何塑造AI社区的集体行为？

Method: 使用基于施瓦茨基本人类价值理论的自然主义价值启发方法，构建了多智能体模拟，让不同规模的社区参与开放式互动和宪法制定

Result: 结果显示：价值多样性增强价值稳定性、促进涌现行为、带来更多由智能体自身开发而无外部指导的创造性原则。但这些效应也显示边际递减：极端异质性会导致不稳定性

Conclusion: 本研究将价值多样性定位为未来AI能力的新维度，连接了AI能力与社会学中制度涌现的研究

Abstract: As Large Language Models (LLM) based multi-agent systems become increasingly prevalent, the collective behaviors, e.g., collective intelligence, of such artificial communities have drawn growing attention. This work aims to answer a fundamental question: How does diversity of values shape the collective behavior of AI communities? Using naturalistic value elicitation grounded in the prevalent Schwartz's Theory of Basic Human Values, we constructed multi-agent simulations where communities with varying numbers of agents engaged in open-ended interactions and constitution formation. The results show that value diversity enhances value stability, fosters emergent behaviors, and brings more creative principles developed by the agents themselves without external guidance. However, these effects also show diminishing returns: extreme heterogeneity induces instability. This work positions value diversity as a new axis of future AI capability, bridging AI ability and sociological studies of institutional emergence.

</details>


### [77] [AEBNAS: Strengthening Exit Branches in Early-Exit Networks through Hardware-Aware Neural Architecture Search](https://arxiv.org/abs/2512.10671)
*Oscar Robben,Saeed Khalilian,Nirvana Meratnia*

Main category: cs.AI

TL;DR: 本文提出一个硬件感知的神经架构搜索框架，用于设计更高效的早退网络，通过优化退出分支的深度和层类型，结合自适应阈值调整，在保持或降低计算量的同时提高准确率。


<details>
  <summary>Details</summary>
Motivation: 早退网络能根据输入数据复杂度调整计算量，对资源受限设备很有价值。但设计早退网络很困难，需要平衡效率和性能。现有NAS方法主要关注退出位置和数量，而退出分支的深度和层类型对效率和准确率也有重要影响。

Method: 采用硬件感知的神经架构搜索（NAS）来强化退出分支，在优化过程中同时考虑准确率和效率。框架考虑了退出分支的不同深度和层类型，并结合自适应阈值调整。

Result: 在CIFAR-10、CIFAR-100和SVHN数据集上的评估表明，提出的框架设计的早退网络在相同或更低的平均MACs（乘加运算）下，相比最先进方法获得了更高的准确率。

Conclusion: 通过硬件感知NAS优化退出分支的深度和层类型，结合自适应阈值调整，可以设计出更高效的早退网络，在保持计算效率的同时提高模型准确率。

Abstract: Early-exit networks are effective solutions for reducing the overall energy consumption and latency of deep learning models by adjusting computation based on the complexity of input data. By incorporating intermediate exit branches into the architecture, they provide less computation for simpler samples, which is particularly beneficial for resource-constrained devices where energy consumption is crucial. However, designing early-exit networks is a challenging and time-consuming process due to the need to balance efficiency and performance. Recent works have utilized Neural Architecture Search (NAS) to design more efficient early-exit networks, aiming to reduce average latency while improving model accuracy by determining the best positions and number of exit branches in the architecture. Another important factor affecting the efficiency and accuracy of early-exit networks is the depth and types of layers in the exit branches. In this paper, we use hardware-aware NAS to strengthen exit branches, considering both accuracy and efficiency during optimization. Our performance evaluation on the CIFAR-10, CIFAR-100, and SVHN datasets demonstrates that our proposed framework, which considers varying depths and layers for exit branches along with adaptive threshold tuning, designs early-exit networks that achieve higher accuracy with the same or lower average number of MACs compared to the state-of-the-art approaches.

</details>


### [78] [Enhancing Radiology Report Generation and Visual Grounding using Reinforcement Learning](https://arxiv.org/abs/2512.10691)
*Benjamin Gundersen,Nicolas Deperrois,Samuel Ruiperez-Campillo,Thomas M. Sutter,Julia E. Vogt,Michael Moor,Farhad Nooralahzadeh,Michael Krauthammer*

Main category: cs.AI

TL;DR: 研究探索强化学习(RL)和显式推理(thinking)在胸部X光视觉语言模型中的应用，发现RL能提升报告生成和视觉定位性能，但显式推理无明显增益。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉语言模型主要依赖监督微调(SFT)，但缺乏对答案质量的评估。强化学习能融入任务特定反馈，结合显式推理在数学和编程任务中已显示显著优势，研究旨在探索这些方法在胸部X光解释中的效果。

Method: 1. 基于Qwen3-VL进行大规模CXR数据的SFT构建RadVLM；2. 冷启动SFT阶段赋予模型基本推理能力；3. 应用GRPO强化学习，结合临床任务特定奖励进行报告生成和视觉定位；4. 在领域特定和通用领域变体上进行匹配RL实验，对比有无推理能力的效果。

Result: 1. 强SFT对基础性能至关重要；2. RL在两个任务上均提供额外增益；3. 显式推理未进一步改善结果；4. RL优化的RadVLM模型在报告生成和视觉定位上超越基线并达到SOTA性能。

Conclusion: 临床对齐的强化学习是医学视觉语言模型监督微调的有力补充，能显著提升胸部X光解释性能，但显式推理在当前设置下未显示额外优势。

Abstract: Recent advances in vision-language models (VLMs) have improved Chest X-ray (CXR) interpretation in multiple aspects. However, many medical VLMs rely solely on supervised fine-tuning (SFT), which optimizes next-token prediction without evaluating answer quality. In contrast, reinforcement learning (RL) can incorporate task-specific feedback, and its combination with explicit intermediate reasoning ("thinking") has demonstrated substantial gains on verifiable math and coding tasks. To investigate the effects of RL and thinking in a CXR VLM, we perform large-scale SFT on CXR data to build an updated RadVLM based on Qwen3-VL, followed by a cold-start SFT stage that equips the model with basic thinking ability. We then apply Group Relative Policy Optimization (GRPO) with clinically grounded, task-specific rewards for report generation and visual grounding, and run matched RL experiments on both domain-specific and general-domain Qwen3-VL variants, with and without thinking. Across these settings, we find that while strong SFT remains crucial for high base performance, RL provides additional gains on both tasks, whereas explicit thinking does not appear to further improve results. Under a unified evaluation pipeline, the RL-optimized RadVLM models outperform their baseline counterparts and reach state-of-the-art performance on both report generation and grounding, highlighting clinically aligned RL as a powerful complement to SFT for medical VLMs.

</details>


### [79] [COMPARE: Clinical Optimization with Modular Planning and Assessment via RAG-Enhanced AI-OCT: Superior Decision Support for Percutaneous Coronary Intervention Compared to ChatGPT-5 and Junior Operators](https://arxiv.org/abs/2512.10702)
*Wei Fang,Chiyao Wang,Wenshuai Ma,Hui Liu,Jianqiang Hu,Xiaona Niu,Yi Chu,Mingming Zhang,Jingxiao Yang,Dongwei Zhang,Zelin Li,Pengyun Liu,Jiawei Zheng,Pengke Zhang,Chaoshi Qin,Wangang Guo,Bin Wang,Yugang Xue,Wei Zhang,Zikuan Wang,Rui Zhu,Yihui Cao,Quanmao Lu,Rui Meng,Yan Li*

Main category: cs.AI

TL;DR: CA-GPT AI系统在OCT引导的PCI规划和评估中，显著优于通用ChatGPT-5和初级医生，提供更标准可靠的血管内影像解读。


<details>
  <summary>Details</summary>
Motivation: 虽然血管内成像（特别是OCT）能改善PCI结果，但其解读依赖操作者经验。通用AI有潜力但缺乏领域特异性可靠性，需要开发专门针对OCT引导PCI的AI系统。

Method: 单中心分析96例接受OCT引导PCI的患者，比较CA-GPT、ChatGPT-5和初级医生的手术决策与专家记录的一致性。使用10个预设指标评估PCI前规划和PCI后评估阶段。

Result: PCI前规划：CA-GPT中位一致性评分(5[3.75-5])显著高于ChatGPT-5(3[2-4])和初级医生(4[3-4])。在支架直径(90.3% vs 72.2%)和长度选择(80.6% vs 52.8%)上优于初级医生。PCI后评估：CA-GPT(5[4.75-5])仍显著优于ChatGPT-5(4[4-5])和初级医生(5[4-5])。亚组分析显示在复杂场景中优势明显。

Conclusion: 基于CA-GPT的AI-OCT系统在PCI规划和评估阶段均优于通用大语言模型和初级医生，为血管内影像解读提供了标准化可靠方法，有潜力增强操作者专业能力并优化OCT引导的PCI。

Abstract: Background: While intravascular imaging, particularly optical coherence tomography (OCT), improves percutaneous coronary intervention (PCI) outcomes, its interpretation is operator-dependent. General-purpose artificial intelligence (AI) shows promise but lacks domain-specific reliability. We evaluated the performance of CA-GPT, a novel large model deployed on an AI-OCT system, against that of the general-purpose ChatGPT-5 and junior physicians for OCT-guided PCI planning and assessment.
  Methods: In this single-center analysis of 96 patients who underwent OCT-guided PCI, the procedural decisions generated by the CA-GPT, ChatGPT-5, and junior physicians were compared with an expert-derived procedural record. Agreement was assessed using ten pre-specified metrics across pre-PCI and post-PCI phases.
  Results: For pre-PCI planning, CA-GPT demonstrated significantly higher median agreement scores (5[IQR 3.75-5]) compared to both ChatGPT-5 (3[2-4], P<0.001) and junior physicians (4[3-4], P<0.001). CA-GPT significantly outperformed ChatGPT-5 across all individual pre-PCI metrics and showed superior performance to junior physicians in stent diameter (90.3% vs. 72.2%, P<0.05) and length selection (80.6% vs. 52.8%, P<0.01). In post-PCI assessment, CA-GPT maintained excellent overall agreement (5[4.75-5]), significantly higher than both ChatGPT-5 (4[4-5], P<0.001) and junior physicians (5[4-5], P<0.05). Subgroup analysis confirmed CA-GPT's robust performance advantage in complex scenarios.
  Conclusion: The CA-GPT-based AI-OCT system achieved superior decision-making agreement versus a general-purpose large language model and junior physicians across both PCI planning and assessment phases. This approach provides a standardized and reliable method for intravascular imaging interpretation, demonstrating significant potential to augment operator expertise and optimize OCT-guided PCI.

</details>


### [80] [HAROOD: A Benchmark for Out-of-distribution Generalization in Sensor-based Human Activity Recognition](https://arxiv.org/abs/2512.10807)
*Wang Lu,Yao Zhu,Jindong Wang*

Main category: cs.AI

TL;DR: HAROOD：一个用于人类活动识别（HAR）在分布外（OOD）设置下的综合基准测试，涵盖4种OOD场景、6个数据集、16种方法，旨在评估现有OOD算法在HAR中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现实场景中，个体、设备、环境和时间的差异导致相同活动存在显著分布偏移。现有研究仅在特定分布偏移场景（如跨设备或跨位置）应用或调整OOD算法，缺乏对这些算法有效性的全面洞察。需要回答：OOD对HAR是否必要？哪种OOD算法表现最佳？

Method: 提出HAROOD基准测试，定义4种OOD场景：跨人、跨位置、跨数据集、跨时间。构建测试平台覆盖6个数据集、16种比较方法（基于CNN和Transformer架构），采用两种模型选择协议。进行大量实验，提供模块化代码库便于扩展。

Result: 实验发现：没有单一方法在所有场景中持续优于其他方法，表明该领域仍有巨大改进空间。代码库已开源，便于新数据集、算法和比较研究的扩展。

Conclusion: HAROOD填补了HAR在OOD设置下的研究空白，为未来研究提供了全面基准。发现当前OOD算法在HAR中尚未达到一致性优越表现，凸显了该领域的进步机会。开源实现旨在促进OOD-based HAR研究。

Abstract: Sensor-based human activity recognition (HAR) mines activity patterns from the time-series sensory data. In realistic scenarios, variations across individuals, devices, environments, and time introduce significant distributional shifts for the same activities. Recent efforts attempt to solve this challenge by applying or adapting existing out-of-distribution (OOD) algorithms, but only in certain distribution shift scenarios (e.g., cross-device or cross-position), lacking comprehensive insights on the effectiveness of these algorithms. For instance, is OOD necessary to HAR? Which OOD algorithm performs the best? In this paper, we fill this gap by proposing HAROOD, a comprehensive benchmark for HAR in OOD settings. We define 4 OOD scenarios: cross-person, cross-position, cross-dataset, and cross-time, and build a testbed covering 6 datasets, 16 comparative methods (implemented with CNN-based and Transformer-based architectures), and two model selection protocols. Then, we conduct extensive experiments and present several findings for future research, e.g., no single method consistently outperforms others, highlighting substantial opportunity for advancement. Our codebase is highly modular and easy to extend for new datasets, algorithms, comparisons, and analysis, with the hope to facilitate the research in OOD-based HAR. Our implementation is released and can be found at https://github.com/AIFrontierLab/HAROOD.

</details>


### [81] [Agile Deliberation: Concept Deliberation for Subjective Visual Classification](https://arxiv.org/abs/2512.10821)
*Leijie Wang,Otilia Stretcu,Wei Qiao,Thomas Denby,Krishnamurthy Viswanathan,Enming Luo,Chun-Ta Lu,Tushar Dogra,Ranjay Krishna,Ariel Fuxman*

Main category: cs.AI

TL;DR: 提出Agile Deliberation框架，支持用户通过边界案例迭代定义模糊概念，提升视觉分类器与用户意图的对齐效果


<details>
  <summary>Details</summary>
Motivation: 现有的人机交互方法假设用户有清晰稳定的概念理解，但现实中用户常从模糊概念开始，需要通过"概念审议"迭代细化。从内容审核专家的访谈中发现这一实践需求

Method: 将真实内容审核员的审议策略操作化为Agile Deliberation框架，包含两个阶段：1) 概念范围界定-将初始概念分解为结构化子概念层次；2) 概念迭代-呈现语义边界案例供用户反思反馈，迭代对齐图像分类器与用户意图

Result: 通过18个用户会话(每个1.5小时)评估，Agile Deliberation比自动分解基线F1分数高7.5%，比手动审议高3%以上，参与者报告概念理解更清晰、认知努力更低

Conclusion: Agile Deliberation有效支持演化中的主观概念定义，通过暴露边界案例帮助用户明确概念理解，在概念审议任务中优于传统方法

Abstract: From content moderation to content curation, applications requiring vision classifiers for visual concepts are rapidly expanding. Existing human-in-the-loop approaches typically assume users begin with a clear, stable concept understanding to be able to provide high-quality supervision. In reality, users often start with a vague idea and must iteratively refine it through "concept deliberation", a practice we uncovered through structured interviews with content moderation experts. We operationalize the common strategies in deliberation used by real content moderators into a human-in-the-loop framework called "Agile Deliberation" that explicitly supports evolving and subjective concepts. The system supports users in defining the concept for themselves by exposing them to borderline cases. The system does this with two deliberation stages: (1) concept scoping, which decomposes the initial concept into a structured hierarchy of sub-concepts, and (2) concept iteration, which surfaces semantically borderline examples for user reflection and feedback to iteratively align an image classifier with the user's evolving intent. Since concept deliberation is inherently subjective and interactive, we painstakingly evaluate the framework through 18 user sessions, each 1.5h long, rather than standard benchmarking datasets. We find that Agile Deliberation achieves 7.5% higher F1 scores than automated decomposition baselines and more than 3% higher than manual deliberation, while participants reported clearer conceptual understanding and lower cognitive effort.

</details>


### [82] [V-OCBF: Learning Safety Filters from Offline Data via Value-Guided Offline Control Barrier Functions](https://arxiv.org/abs/2512.10822)
*Mumuksh Tayal,Manan Tayal,Aditya Singh,Shishir Kolathaya,Ravi Prakash*

Main category: cs.AI

TL;DR: V-OCBF：从离线演示中学习神经控制屏障函数，实现无模型的安全控制器合成，无需在线交互或人工设计屏障函数。


<details>
  <summary>Details</summary>
Motivation: 现有安全离线强化学习方法通常只强制执行软期望成本约束，无法保证前向不变性；而控制屏障函数（CBFs）虽然提供严格安全保证，但依赖专家设计的屏障函数或完整的系统动力学知识。需要一种能从离线数据学习安全屏障、无需模型知识的方法。

Method: 提出值引导离线控制屏障函数（V-OCBF）：1）从离线演示中学习神经CBF，无需动力学模型；2）推导递归有限差分屏障更新，实现无模型学习；3）采用期望分位数目标，避免在分布外动作上查询屏障；4）将学习到的屏障与二次规划（QP）结合，合成实时安全控制。

Result: 在多个案例研究中，V-OCBF相比基线方法显著减少了安全违规次数，同时保持了强大的任务性能，展示了其在无需在线交互或人工设计屏障的情况下合成安全关键控制器的可扩展性。

Conclusion: V-OCBF为离线合成安全关键控制器提供了一个有效框架，能够从离线数据中学习安全屏障，无需动力学模型或专家设计的屏障函数，在保证安全的同时维持任务性能。

Abstract: Ensuring safety in autonomous systems requires controllers that satisfy hard, state-wise constraints without relying on online interaction. While existing Safe Offline RL methods typically enforce soft expected-cost constraints, they do not guarantee forward invariance. Conversely, Control Barrier Functions (CBFs) provide rigorous safety guarantees but usually depend on expert-designed barrier functions or full knowledge of the system dynamics. We introduce Value-Guided Offline Control Barrier Functions (V-OCBF), a framework that learns a neural CBF entirely from offline demonstrations. Unlike prior approaches, V-OCBF does not assume access to the dynamics model; instead, it derives a recursive finite-difference barrier update, enabling model-free learning of a barrier that propagates safety information over time. Moreover, V-OCBF incorporates an expectile-based objective that avoids querying the barrier on out-of-distribution actions and restricts updates to the dataset-supported action set. The learned barrier is then used with a Quadratic Program (QP) formulation to synthesize real-time safe control. Across multiple case studies, V-OCBF yields substantially fewer safety violations than baseline methods while maintaining strong task performance, highlighting its scalability for offline synthesis of safety-critical controllers without online interaction or hand-engineered barriers.

</details>


### [83] [LLMs Can Assist with Proposal Selection at Large User Facilities](https://arxiv.org/abs/2512.10895)
*Lijie Ding,Janell Thomson,Jon Taylor,Changwoo Do*

Main category: cs.AI

TL;DR: LLMs可用于大型用户设施的提案评审，提供比传统人工评审更可扩展、一致且经济高效的替代方案，在排名相关性、识别高潜力提案方面表现不逊于人类，且成本低两个数量级。


<details>
  <summary>Details</summary>
Motivation: 传统人工提案评审存在提案间相关性弱、评审者偏见和不一致的问题。基于成对偏好的方法理论上更优，但工作量呈二次方增长，对人类评审者不切实际。LLMs可以解决这一限制。

Method: 利用LLMs进行提案排名，采用成对偏好方法。基于橡树岭国家实验室散裂中子源三个束线的精心策划的提案和发表记录，评估LLM排名与人工排名的相关性，并比较识别高发表潜力提案的能力。

Result: LLM排名与人工排名有强相关性（Spearman ρ≈0.2-0.8，去除10%异常值后≥0.5）。在识别高发表潜力提案方面，LLM表现不逊于人类评审者，同时成本低两个数量级。LLMs还能进行提案相似性量化分析等高级分析。

Conclusion: LLMs为大型用户设施的提案选择提供了可行且优越的替代方案，不仅解决了传统人工评审的局限性，还能进行人类难以完成的高级分析，具有显著的成本效益优势。

Abstract: We explore how large language models (LLMs) can enhance the proposal selection process at large user facilities, offering a scalable, consistent, and cost-effective alternative to traditional human review. Proposal selection depends on assessing the relative strength among submitted proposals; however, traditional human scoring often suffers from weak inter-proposal correlations and is subject to reviewer bias and inconsistency. A pairwise preference-based approach is logically superior, providing a more rigorous and internally consistent basis for ranking, but its quadratic workload makes it impractical for human reviewers. We address this limitation using LLMs. Leveraging the uniquely well-curated proposals and publication records from three beamlines at the Spallation Neutron Source (SNS), Oak Ridge National Laboratory (ORNL), we show that the LLM rankings correlate strongly with the human rankings (Spearman $ρ\simeq 0.2-0.8$, improving to $\geq 0.5$ after 10\% outlier removal). Moreover, LLM performance is no worse than that of human reviewers in identifying proposals with high publication potential, while costing over two orders of magnitude less. Beyond ranking, LLMs enable advanced analyses that are challenging for humans, such as quantitative assessment of proposal similarity via embedding models, which provides information crucial for review committees.

</details>


### [84] [Multi-Granular Node Pruning for Circuit Discovery](https://arxiv.org/abs/2512.10903)
*Muhammad Umair Haider,Hammad Rizwan,Hassan Sajjad,A. B. Siddique*

Main category: cs.AI

TL;DR: 提出一种节点级剪枝框架用于电路发现，解决现有方法计算成本高、粒度粗的问题，通过多粒度可学习掩码和稀疏惩罚实现单次微调中的全面压缩。


<details>
  <summary>Details</summary>
Motivation: 现有电路发现方法主要依赖迭代边剪枝，计算成本高且仅限于粗粒度单元（如注意力头或MLP块），忽略了神经元等更细粒度的结构。

Method: 提出节点级剪枝框架，引入跨多个粒度（从整个块到单个神经元）的可学习掩码，在统一优化目标中使用粒度特定的稀疏惩罚指导剪枝过程。

Result: 该方法发现的电路节点数少于先前方法，证明许多被粗粒度方法认为重要的神经元实际上无关紧要，同时保持任务性能，内存占用降低5-10倍。

Conclusion: 提出的节点级剪枝框架在电路发现中实现了更好的可扩展性和粒度控制，显著降低了计算和内存需求，同时揭示了粗粒度方法可能高估了某些组件的重要性。

Abstract: Circuit discovery aims to identify minimal subnetworks that are responsible for specific behaviors in large language models (LLMs). Existing approaches primarily rely on iterative edge pruning, which is computationally expensive and limited to coarse-grained units such as attention heads or MLP blocks, overlooking finer structures like individual neurons. We propose a node-level pruning framework for circuit discovery that addresses both scalability and granularity limitations. Our method introduces learnable masks across multiple levels of granularity, from entire blocks to individual neurons, within a unified optimization objective. Granularity-specific sparsity penalties guide the pruning process, allowing a comprehensive compression in a single fine-tuning run. Empirically, our approach identifies circuits that are smaller in nodes than those discovered by prior methods; moreover, we demonstrate that many neurons deemed important by coarse methods are actually irrelevant, while still maintaining task performance. Furthermore, our method has a significantly lower memory footprint, 5-10x, as it does not require keeping intermediate activations in the memory to work.

</details>


### [85] [On Decision-Making Agents and Higher-Order Causal Processes](https://arxiv.org/abs/2512.10937)
*Matt Wilson*

Main category: cs.AI

TL;DR: 该论文建立了POMDP决策代理与量子高阶操作经典极限（单输入过程函数）之间的精确对应关系，揭示了AI与物理视角的双重解释，并将此框架扩展到多智能体系统。


<details>
  <summary>Details</summary>
Motivation: 探索人工智能中的决策理论（特别是部分可观察马尔可夫决策过程POMDP）与量子信息理论中过程函数概念之间的深刻联系，为这两个领域提供统一的数学框架。

Method: 通过将代理的策略和记忆更新组合成过程函数w，使用链接积与POMDP环境交互，建立精确的数学对应关系。然后将此框架扩展到多智能体系统，将观察独立的分散式POMDP识别为多输入过程函数的自然领域。

Result: 成功建立了POMDP决策代理与单输入过程函数之间的精确对应关系，揭示了物理视角（过程函数作为环境）和AI视角（过程函数编码代理）的双重解释，并将框架扩展到多智能体系统。

Conclusion: 该研究为人工智能决策理论与量子信息理论之间建立了重要的桥梁，展示了POMDP与过程函数之间的深刻对应关系，为跨学科研究提供了新的视角和数学工具。

Abstract: We establish a precise correspondence between decision-making agents in partially observable Markov decision processes (POMDPs) and one-input process functions, the classical limit of higher-order quantum operations. In this identification an agent's policy and memory update combine into a process function w that interacts with a POMDP environment via the link product. This suggests a dual interpretation: in the physics view, the process function acts as the environment into which local operations (agent interventions) are inserted, whereas in the AI view it encodes the agent and the inserted functions represent environments. We extend this perspective to multi-agent systems by identifying observation-independent decentralized POMDPs as natural domains for multi-input process functions.

</details>


<div id='q-fin.PR'></div>

# q-fin.PR [[Back]](#toc)

### [86] [Option-Implied Zero-Coupon Yields: Unifying Bond and Equity Markets](https://arxiv.org/abs/2512.10823)
*Ting-Jung Lee,W. Brent Lindquist,Svetlozar T. Rachev,Abootaleb Shirvani*

Main category: q-fin.PR

TL;DR: 该论文提出统一利率期限结构框架，将零息债券视为欧式期权，确保其与股票衍生品在同一风险中性测度下定价，并从标普500指数期权中提取隐含收益率曲线。


<details>
  <summary>Details</summary>
Motivation: 解决利率期限结构模型中存在的不一致问题：零息债券与股票市场使用不同的风险中性测度进行定价。需要建立统一框架，确保债券和股票衍生品在同一风险中性测度下定价。

Method: 将零息债券视为具有确定性支付的欧式期权，利用看跌-看涨平价关系，从标普500指数期权中提取隐含收益率曲线，并与美国国债收益率曲线进行比较，研究隐含收益率曲线的执行价格依赖性。

Result: 研究发现，平价期权的隐含收益率曲线与国债收益率曲线最为接近，表明股票期权市场包含对利率期限结构高度相关的信息。

Conclusion: 通过要求债券估值与股票衍生品使用相同的风险中性测度，为未来利率期限结构研究提供了新的组织原则，强调了市场间风险中性测度一致性的重要性。

Abstract: This paper addresses a critical inconsistency in models of the term structure of interest rates (TSIR), where zero-coupon bonds are priced under risk-neutral measures distinct from those used in equity markets. We propose a unified TSIR framework that treats zero-coupon bonds as European options with deterministic payoffs ensuring that they are priced under the same risk-neutral measure that governs equity derivatives. Using put-call parity, we extract zero-coupon bond implied yield curves from S&P 500 index options and compare them with the US daily treasury par yield curves. As the implied yield curves contain maturity time T and strike price K as independent variables, we investigate the K-dependence of the implied yield curve. Our findings, that at-the-money, option-implied yield curves provide the closest match to treasury par yield curves, support the view that the equity options market contains information that is highly relevant for the TSIR. By insisting that the risk-neutral measure used for bond valuation is the same as that revealed by equity derivatives, we offer a new organizing principle for future TSIR research.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [87] [QSTAformer: A Quantum-Enhanced Transformer for Robust Short-Term Voltage Stability Assessment against Adversarial Attacks](https://arxiv.org/abs/2512.09936)
*Yang Li,Chong Ma,Yuanzheng Li,Sen Li,Yanbo Chen,Zhaoyang Dong*

Main category: eess.SY

TL;DR: 提出QSTAformer量子增强Transformer架构，将参数化量子电路嵌入注意力机制，用于鲁棒的短期电压稳定评估，并开发对抗训练策略防御白盒和灰盒攻击。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法在对抗条件下鲁棒性不足，需要开发更鲁棒和高效的短期电压稳定评估方法，量子机器学习在此领域的对抗脆弱性尚未被系统研究。

Method: 提出QSTAformer架构，将参数化量子电路嵌入Transformer的注意力机制；开发专门的对抗训练策略防御白盒和灰盒攻击；对多种PQC架构进行基准测试，平衡表达能力、收敛性和效率。

Result: 在IEEE 39总线系统上的案例研究表明，QSTAformer实现了有竞争力的准确率、降低的复杂度和更强的鲁棒性，在对抗条件下表现出优越性能。

Conclusion: 这是首个系统研究量子机器学习在短期电压稳定评估中对抗脆弱性的工作，QSTAformer展示了在对抗条件下实现安全、可扩展短期电压稳定评估的潜力。

Abstract: Short-term voltage stability assessment (STVSA) is critical for secure power system operation. While classical machine learning-based methods have demonstrated strong performance, they still face challenges in robustness under adversarial conditions. This paper proposes QSTAformer-a tailored quantum-enhanced Transformer architecture that embeds parameterized quantum circuits (PQCs) into attention mechanisms-for robust and efficient STVSA. A dedicated adversarial training strategy is developed to defend against both white-box and gray-box attacks. Furthermore, diverse PQC architectures are benchmarked to explore trade-offs between expressiveness, convergence, and efficiency. To the best of our knowledge, this is the first work to systematically investigate the adversarial vulnerability of quantum machine learning-based STVSA. Case studies on the IEEE 39-bus system demonstrate that QSTAformer achieves competitive accuracy, reduced complexity, and stronger robustness, underscoring its potential for secure and scalable STVSA under adversarial conditions.

</details>


### [88] [Explicit Control Barrier Function-based Safety Filters and their Resource-Aware Computation](https://arxiv.org/abs/2512.10118)
*Pol Mestres,Shima Sadat Mousavi,Pio Ong,Lizhi Yang,Ersin Das,Joel W. Burdick,Aaron D. Ames*

Main category: eess.SY

TL;DR: 提出了一种基于控制屏障函数(CBF)的安全滤波器的闭式表达式实现方法，通过状态空间分区实现高效计算，避免实时求解二次规划问题。


<details>
  <summary>Details</summary>
Motivation: CBF安全滤波器通常需要实时求解二次规划(QP)，但在需要高频控制的应用中，使用现成求解器计算效率不足。需要更高效的实现方法。

Method: 将状态空间划分为不同区域，在每个区域推导出闭式表达式。提出资源感知的实现方法，检测分区变化并在变化间使用闭式解。

Result: 实现了基于CBF的安全滤波器的闭式表达式，能够高效计算控制动作，适用于航空航天控制和安全强化学习等多种应用场景。

Conclusion: 通过状态空间分区和闭式表达式，显著提高了CBF安全滤波器的计算效率，使其适用于需要高频控制的应用场景。

Abstract: This paper studies the efficient implementation of safety filters that are designed using control barrier functions (CBFs), which minimally modify a nominal controller to render it safe with respect to a prescribed set of states. Although CBF-based safety filters are often implemented by solving a quadratic program (QP) in real time, the use of off-the-shelf solvers for such optimization problems poses a challenge in applications where control actions need to be computed efficiently at very high frequencies. In this paper, we introduce a closed-form expression for controllers obtained through CBF-based safety filters. This expression is obtained by partitioning the state-space into different regions, with a different closed-form solution in each region. We leverage this formula to introduce a resource-aware implementation of CBF-based safety filters that detects changes in the partition region and uses the closed-form expression between changes. We showcase the applicability of our approach in examples ranging from aerospace control to safe reinforcement learning.

</details>


### [89] [Traffic Equilibrium in Mixed-Autonomy Network with Capped Customer Waiting](https://arxiv.org/abs/2512.10194)
*Jiaxin Hou,Kexin Wang,Ruolin Li,Jong-shi Pang*

Main category: eess.SY

TL;DR: 本文开发了一个统一建模框架，用于捕捉网约车公司、出行者和混合自动驾驶交通网络之间的均衡状态交互。该框架整合了四个相互关联的子模块，并建立了非线性互补问题(NCP)和变分不等式(VI)公式，研究了自动驾驶渗透率和路径偏离对系统性能的影响。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶技术的发展和信息可及性的提高，现代交通系统正在快速变化。需要建立一个统一的建模框架来理解混合自动驾驶交通网络中网约车公司、出行者和交通拥堵之间的复杂相互作用，为政策制定者提供关于自动驾驶采用和车辆路径偏离行为的监管见解。

Method: 开发了一个包含四个子模块的统一建模框架：(1) 管理自动驾驶车辆(AV)和人工驾驶车辆(HV)车队的混合车队网约车公司运营行为；(2) 考虑旅行成本和等待时间的出行者模式选择决策；(3) 反映出行者等待耐心的上限等待时间模型；(4) 流量依赖的交通拥堵模型。该框架被表述为非线性互补问题(NCP)，等价于变分不等式(VI)公式，并建立了变分均衡解的存在性。

Result: 数值实验研究了自动驾驶渗透率和Wardrop松弛因子（限制路径偏离程度）如何不同程度地影响公司、出行者和系统性能。结果表明这些因素对系统性能有显著影响，为政策制定者提供了可操作的见解。

Conclusion: 该框架成功捕捉了混合自动驾驶交通网络中多方参与者的均衡交互，为监管自动驾驶采用和公司车辆路径偏离行为提供了理论依据和实用工具，有助于在技术快速发展的背景下优化现代交通系统。

Abstract: This paper develops a unified modeling framework to capture the equilibrium-state interactions among ride-hailing companies, travelers, and traffic of mixed-autonomy transportation networks. Our framework integrates four interrelated sub-modules: (i) the operational behavior of representative ride-hailing Mixed-Fleet Traffic Network Companies (MiFleet TNCs) managing autonomous vehicle (AV) and human-driven vehicle (HV) fleets, (ii) traveler mode-choice decisions taking into account travel costs and waiting time, (iii) capped customer waiting times to reflect the option available to travelers not to wait for TNCs' service beyond his/her patience and to resort to existing travel modes, and (iv) a flow-dependent traffic congestion model for travel times. A key modeling feature distinguishes AVs and HVs across the pickup and service (customer-on-board) stages: AVs follow Wardrop pickup routes but may deviate during service under company coordination, whereas HVs operate in the reverse manner. The overall framework is formulated as a Nonlinear Complementarity Problem (NCP), which is equivalent to a Variational Inequality(VI) formulation based on which the existence of a variational equilibrium solution to the traffic model is established. Numerical experiments examine how AV penetration and Wardrop relaxation factors, which bound route deviation, affect company, traveler, and system performance to various degrees. The results provide actionable insights for policymakers on regulating AV adoption and company vehicle deviation behavior in modern-day traffic systems that are fast changing due to the advances in technology and information accessibility.

</details>


### [90] [Permutation-Equivariant Learning for Dynamic Security Assessment of Power System Frequency Response](https://arxiv.org/abs/2512.10232)
*Francisco Zelaya-Arrazabal,Sebastian Martinez-Lizana,Hector Pulgar-Painemal,Jin Zhao*

Main category: eess.SY

TL;DR: 提出基于模态分解和深度学习的混合AI框架，用于电力系统频率稳定性的实时动态安全评估，能够快速预测频率最低点及其发生时间。


<details>
  <summary>Details</summary>
Motivation: 电力系统频率稳定性评估需要实时、准确且可扩展的方法。传统方法要么计算量大，要么需要大量PMU部署。需要一种既能快速评估，又能在不同运行条件和扰动下保持准确性的解决方案。

Method: 结合模态分解与深度学习：1）基于系统频率响应的模态分解公式，利用系统特征结构预测频率稳定性指标；2）采用Deep Sets启发的神经网络，以置换等变学习问题形式估计复杂模态系数；3）通过重用预计算的模态结构并仅更新扰动特定系数实现可扩展性。

Result: 在IEEE 39总线和118总线系统上验证，相比纯数据驱动方法，在准确性、鲁棒性和计算效率方面表现优越。无需大量训练场景或广泛部署PMU，展示了强大的泛化能力。

Conclusion: 提出的混合AI框架为电力系统频率稳定性实时评估提供了高效、准确且可扩展的解决方案，结合了物理模型与数据驱动方法的优势，适用于动态运行环境。

Abstract: This paper presents a hybrid model-AI framework for real-time dynamic security assessment of frequency stability in power systems. The proposed method rapidly estimates key frequency parameters under a dynamic set of disturbances, which are continuously updated based on operating conditions and unit commitment. To achieve this, the framework builds on a modal-based formulation of the system frequency response (SFR), which leverages the system's eigenstructure to predict key frequency stability metrics. A Deep Sets-inspired neural network is employed to estimate the complex modal coefficients required by the modal-based SFR approach, formulated as a permutation-equivariant learning problem. This enables fast and accurate prediction of the frequency nadir and its timing across different operating conditions and disturbances. The framework achieves scalability by reusing precomputed modal structures and updating only the disturbance-specific coefficients. It demonstrates strong generalization capabilities without requiring an extensive set of operating scenarios during training or the widespread deployment of phasor measurement units (PMUs). The method is validated on the IEEE 39-bus and 118-bus systems, showing superior accuracy, robustness, and computational efficiency compared to purely data-driven approaches.

</details>


### [91] [A Novel Phase-Noise Module for the QUCS Circuit Simulator. Part I : the Periodic Steady-State](https://arxiv.org/abs/2512.10373)
*Torsten Djurhuus,Viktor Krozer*

Main category: eess.SY

TL;DR: 在QUCS开源电路仿真器中实现了首个时域稳态分析模块，支持自主电路仿真，填补了原有谐波平衡法仅适用于非自主电路的空白。


<details>
  <summary>Details</summary>
Motivation: QUCS作为开源电路仿真器，此前仅支持非自主（驱动）电路的谐波平衡分析，缺乏对自主电路（如振荡器）的稳态分析能力。研究旨在扩展QUCS功能，使其能够分析自主电路的大信号稳态行为，包括自由运行和耦合振荡器网络。

Method: 在QUCS中实现了一个计算高效的时域稳态分析模块，该模块支持自主电路仿真，并能模拟大信号条件下的电路噪声性能。这是两篇系列论文的第一部分，重点介绍新型（耦合）振荡器相位噪声仿真引擎的实现。

Result: 成功在QUCS环境中实现了首个时域稳态分析模块，填补了开源电路仿真器在自主电路分析方面的空白。该工具能够分析、合成和优化各种重要工业电路和系统的振荡行为，并支持大信号条件下的噪声性能仿真。

Conclusion: 该研究显著扩展了QUCS的功能，使其能够处理自主电路的稳态分析问题，为开源电路仿真器成为商业仿真器的可行竞争者奠定了基础。这是两篇系列论文的第一部分，后续将进一步完善（耦合）振荡器相位噪声仿真引擎。

Abstract: The paper discusses work done to expand and extend the capabilities of the open-source QUCS circuit simulator through the implementation of a computationally efficient time-domain steady-state analysis module, supporting simulation of autonomous circuits. To our knowledge, this represents the first time such an analysis module has been implemented in the QUCS environment. Hitherto, the only available option was a harmonic-balance module which was strictly limited to non-autonomous (driven) circuits. The research has several important scientific and industrial applications in the area of large-signal steady-state analysis of autonomous circuits e.g. free-running and coupled oscillator circuit networks. The reported results will have great impact w.r.t. analyzing, synthesizing and optimizing oscillatory behavior of various important industrial circuits and systems. The developed tool, furthermore, introduces support for simulating noise performance of circuits operating under large-signal conditions. This paper is the first part of a two-part series documenting the implementation of a novel (coupled)-oscillator phase-noise simulator engine in the QUCS environment. The goal of this undertaking is the advancement of the open-source QUCS project towards becoming a viable competitor to the commercial simulators currently on the market.

</details>


### [92] [Structural Sign Herdability in Temporally Switching Networks with Fixed Topology](https://arxiv.org/abs/2512.10374)
*Pradeep M,Twinkle Tripathy*

Main category: eess.SY

TL;DR: 研究固定拓扑时变切换网络的结构可牧性，发现当所有快照共享相同有向图时，网络能在两个快照内实现完全SS可牧性，克服了静态网络的限制。


<details>
  <summary>Details</summary>
Motivation: 探索时变切换网络相比静态网络在结构可牧性方面的优势，揭示切换动态如何克服经典可牧性障碍。

Method: 分析固定拓扑的时变切换网络，使用更宽松的符号匹配条件，研究在存在符号或层扩张情况下网络的可牧性。

Result: 当所有快照共享相同拓扑时，时变切换网络能在两个快照内实现完全SS可牧性，这比结构可控性所需快照数更少，且能克服静态网络的限制。

Conclusion: 时变动态具有结构优势，切换机制能克服经典可牧性障碍，固定拓扑的时变网络在可牧性方面优于静态网络。

Abstract: This paper investigates structural herdability in a special class of temporally switching networks with fixed topology. We show that when the underlying digraph remains unchanged across all snapshots, the network attains complete SS herdability even in the presence of signed or layer dilations, a condition not applicable to static networks. This reveals a fundamental structural advantage of temporal dynamics and highlights a novel mechanism through which switching can overcome classical obstructions to herdability. To validate these conclusions, we utilize a more relaxed form of sign matching within each snapshot of the temporal network. Furthermore, we show that when all snapshots share the same underlying topology, the temporally switching network achieves $\mathcal{SS}$ herdability within just two snapshots, which is fewer than the number required for structural controllability. Several examples are included to demonstrate these results.

</details>


### [93] [Collision-Aware Density-Driven Control of Multi-Agent Systems via Control Barrier Functions](https://arxiv.org/abs/2512.10392)
*Sungjun Seo,Kooktae Lee*

Main category: eess.SY

TL;DR: 提出了一种结合密度驱动控制和障碍物感知控制屏障函数的多智能体区域覆盖方法，在保证安全的同时提高覆盖效率。


<details>
  <summary>Details</summary>
Motivation: 环境监测和搜救等应用需要机器人群体在资源受限条件下安全高效地覆盖大面积区域，现有方法在障碍物环境中的安全性和效率有待改进。

Method: 采用密度驱动控制框架指导智能体按参考分布运动，结合控制屏障函数确保安全，特别针对圆形和矩形障碍物推导了障碍物特定的CBF公式，并引入速度相关项增强避碰。

Result: 仿真结果表明，与现有方法相比，所提方法在障碍物附近导航更平滑，区域覆盖效率更高，同时确保无碰撞操作。

Conclusion: 通过将密度驱动控制与障碍物感知的控制屏障函数相结合，实现了多智能体系统在复杂障碍物环境中安全高效的区域覆盖。

Abstract: This paper tackles the problem of safe and efficient area coverage using a multi-agent system operating in environments with obstacles. Applications such as environmental monitoring and search and rescue require robot swarms to cover large domains under resource constraints, making both coverage efficiency and safety essential. To address the efficiency aspect, we adopt the Density-Driven Control (D$^2$C) framework, which uses optimal transport theory to steer agents according to a reference distribution that encodes spatial coverage priorities. To ensure safety, we incorporate Control Barrier Functions (CBFs) into the framework. While CBFs are commonly used for collision avoidance, we extend their applicability by introducing obstacle-specific formulations for both circular and rectangular shapes. In particular, we analytically derive a unit normal vector based on the agent's position relative to the nearest face of a rectangular obstacle, improving safety enforcement in environments with non-smooth boundaries. Additionally, a velocity-dependent term is incorporated into the CBF to enhance collision avoidance. Simulation results validate the proposed method by demonstrating smoother navigation near obstacles and more efficient area coverage than the existing method, while still ensuring collision-free operation.

</details>


### [94] [Structural Methods for handling mode changes in multimode DAE systems](https://arxiv.org/abs/2512.10580)
*Albert Benveniste,Benoit Caillaud,Yahao Chen,Khalil Ghorbal,Mathias Malandain*

Main category: eess.SY

TL;DR: 提出了一种用于混合系统模式切换的热重启新方法，为物理无关建模范式（如Modelica）中的模式切换提供数学定义和算法支持。


<details>
  <summary>Details</summary>
Motivation: 在物理无关的建模范式（如Modelica）中，模式切换的数学含义不明确，导致编译困难，用户需要手动平滑模式变化。现有方法缺乏对模式切换中热重启的数学定义和处理。

Method: 提出模式切换中热重启的数学定义，结合结构分析和脉冲分析来处理模式变化，即使在存在脉冲的情况下也能生成热重启。算法在编译时检测模式切换是否规范不足，并提供诊断信息。

Result: 开发了一种新的热重启方法，能够为物理无关建模范式中的模式切换提供数学基础，自动处理模式变化中的脉冲行为，并能在编译时检测规范不足的情况。

Conclusion: 该方法为物理无关建模范式中的模式切换提供了数学基础和实用算法，解决了现有方法中热重启定义不明确的问题，提高了混合系统建模的可靠性和易用性。

Abstract: Hybrid systems are an important concept in Cyber-Physical Systems modeling, for which multiphysics modeling from first principles and the reuse of models from libraries are key. To achieve this, DAEs must be used to specify the dynamics in each discrete state (or mode in our context). This led to the development of DAE-based equational languages supporting multiple modes, of which Modelica is a popular standard. Mode switching can be time- or state-based. Impulsive behaviors can occur at mode changes. While mode changes are well understood in particular physics (e.g., contact mechanics), this is not the case in physics-agnostic paradigms such as Modelica. This situation causes difficulties for the compilation of programs, often requiring users to manually smooth out mode changes. In this paper, we propose a novel approach for the hot restart at mode changes in such paradigms. We propose a mathematical meaning for hot restarts (such a mathematical meaning does not exist in general), as well as a combined structural and impulse analysis for mode changes, generating the hot restart even in the presence of impulses. Our algorithm detects at compile time if the mode change is insufficiently specified, in which case it returns diagnostics information to the user.

</details>


### [95] [NWP-based Atmospheric Refractivity Modeling and Fast & Stable Non-uniform Plane Wave Ray-Tracing Simulations for LEO Link Analysis](https://arxiv.org/abs/2512.10598)
*Bowoo Jang,Jun Heo,Yong Bae Park,Dong-Yeop Na*

Main category: eess.SY

TL;DR: 本文解决了LEO通信链路分析中的两个挑战：大气折射率重建精度不足和非均匀平面波射线追踪数值不稳定问题，通过高分辨率3D复折射率模型和稳定算法，证明均匀平面波近似在实际应用中仍然适用。


<details>
  <summary>Details</summary>
Motivation: 现有LEO通信链路分析面临两个主要问题：1) 稀疏探空数据重建的3D大气折射率精度有限；2) 非均匀平面波射线追踪算法在双精度下数值不稳定（由极小大气损耗项引起）。这些问题影响了LEO链路分析的准确性和计算效率。

Method: 1) 使用数值天气预报数据重建高分辨率3D复折射率模型；2) 开发快速且数值稳定的非均匀平面波射线追踪器，在双精度下保持稳定，比高精度基准快24倍；3) 与均匀平面波近似方法进行比较分析。

Result: 1) 新方法在双精度下保持数值稳定，计算速度提升24倍；2) 即使在强降水条件下，严格方法与均匀平面波近似的波束指向误差偏差和路径损耗差异仍可忽略；3) 损耗大气中的射线虽然形成非均匀平面波，但沿路径的有效衰减与均匀平面波模型预测几乎相同。

Conclusion: 研究结果表明，非均匀平面波效应在实际LEO链路分析中影响很小，这为继续使用均匀平面波射线追踪方法提供了理论依据，简化了实际工程应用中的计算复杂度。

Abstract: Existing low-Earth-orbit (LEO) communication link analyses face two main challenges: (1) limited accuracy of 3D atmospheric refractivity reconstructed from sparsely sampled radiosonde data, and (2) numerical instability in previous non-uniform plane-wave ray-tracing algorithms (i.e., underflow under standard double precision), where non-uniform plane waves inevitably arise at complex-valued dielectric interfaces, is caused by extremely small atmospheric loss terms. To address these issues, we reconstruct a high-resolution 3D complex-valued refractivity model using numerical weather prediction data, and develop a fast and numerically stable non-uniform plane-wave ray tracer. The method remains stable in double precision and delivers a 24-fold speedup over high-precision benchmarks. Comparisons show that boresight-error deviations and path-loss differences between the rigorous method and the uniform-plane-wave approximation remain negligible, even under heavy precipitation. Although rays in a lossy atmosphere experience different phase- and attenuation-direction vectors-forming non-uniform plane waves-the resulting effective attenuation along the path is nearly identical to that predicted by the uniform-plane-wave model. These findings justify the continued use of uniform-plane-wave ray tracing in practical LEO link analyses.

</details>


### [96] [Codeshare agreements between airlines: literature review with the aid of artificial intelligence](https://arxiv.org/abs/2512.10639)
*Lucas T. B. Mendes,Alessandro V. M. Oliveira*

Main category: eess.SY

TL;DR: 本文回顾了代码共享协议文献，重点关注巴西市场，使用Litmaps工具分析文献引用关系，评估这些协议在巴西的影响。


<details>
  <summary>Details</summary>
Motivation: 代码共享协议在商业航空中广泛存在，但文献显示其影响存在矛盾：既有增加供应和降低价格的互补效应，也有反竞争的竞争效应。特别是在巴西这样具有独特特征和监管历史的市场中，需要系统回顾文献来理解这些协议的演变和影响。

Method: 使用基于人工智能技术的Litmaps计算工具，通过分析文献的引用关系来支持对代码共享协议相关出版物的背景分析，重点关注巴西市场。

Result: 通过联合分析文献贡献，能够勾勒出当前知识状态，描述巴西市场观察到的特殊性，并识别可能指导未来研究的空白领域。

Conclusion: 本文通过文献回顾和计算工具分析，识别和评估了数十年来关于代码共享协议在巴西影响的累积证据，为理解这一复杂现象提供了系统框架。

Abstract: Codeshare agreements are contracts that allow two or more airlines to share seats on the same flight. These agreements, which are widespread in commercial aviation as a response to highly competitive environments, have enabled the expansion of airline networks without additional costs or risks for the companies involved. The literature presents ambiguous effects associated with the practice, with evidence of increased supply and reduced prices in situations of route complementarity, while also pointing to anti-competitive impacts in markets where companies act as competitors. A review of scientific production over time, including theoretical contributions and case studies, is essential to understand the evolution of these agreements and their implications, especially in the Brazilian context, marked by its own characteristics and particular regulatory history. Thus, this article reviews the literature on codesharing, with an emphasis on the Brazilian market, and uses the Litmaps computational tool, based on artificial intelligence techniques, to support the contextual analysis of publications through their citation relationships. The ultimate goal is to identify and evaluate the main evidence accumulated over decades on the effects of these agreements in Brazil. The joint analysis of the contributions allows us to outline the current state of knowledge, characterize specificities observed in the Brazilian market, and identify gaps that may guide future studies.

</details>


### [97] [Estimating Hormone Concentrations in the Pituitary-Thyroid Feedback Loop from Irregularly Sampled Measurements](https://arxiv.org/abs/2512.10657)
*Seth Siriya,Tobias M. Wolff,Victor G. Lopez,Matthias A. Müller*

Main category: eess.SY

TL;DR: 论文验证了基于采样的可检测性概念，并在垂体-甲状腺回路模型上实现了基于采样的移动水平估计，展示了估计器在不同采样方案下的鲁棒稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有基于模型的甲状腺疾病药物剂量推荐技术依赖于无法从血液样本测量的内部激素浓度，且可测量浓度通常只能在非规则采样时间获得，这给准确估计带来了挑战。

Method: 在代表甲状腺功能减退和亢进患者的垂体-甲状腺回路模型上，经验验证了考虑可测量浓度非规则采样的基于采样的可检测性概念，并实现了基于采样的移动水平估计方法。

Result: 研究表明，估计器在所有场景下都表现出鲁棒稳定性，并且在存在模型不确定性和剂量误报的情况下，更频繁的采样会导致更小的估计误差。

Conclusion: 基于采样的移动水平估计方法能够有效处理甲状腺疾病药物剂量推荐中的非规则采样问题，提高内部激素浓度的估计精度，为临床决策提供更可靠的支持。

Abstract: Model-based control techniques have recently been investigated for the recommendation of medication dosages to address thyroid diseases. These techniques often rely on knowledge of internal hormone concentrations that cannot be measured from blood samples. Moreover, the measurable concentrations are typically only obtainable at irregular sampling times. In this work, we empirically verify a notion of sample-based detectability that accounts for irregular sampling of the measurable concentrations on two pituitary-thyroid loop models representing patients with hypo- and hyperthyroidism, respectively, and include the internal concentrations as states. We then implement sample-based moving horizon estimation for the models, and test its performance on virtual patients across a range of sampling schemes. Our study shows robust stability of the estimator across all scenarios, and that more frequent sampling leads to less estimation error in the presence of model uncertainty and misreported dosages.

</details>


### [98] [Active prognosis and diagnosis of modular discrete-event systems](https://arxiv.org/abs/2512.10684)
*Shaopeng Hu,Shaowen Miao,Jan Komenda,Zhiwu Li*

Main category: eess.SY

TL;DR: 该论文研究了离散事件系统中可预测性和可诊断性的验证与强制实现，建立了这些性质与预正规性的等价关系，并设计了计算最优可控、正规、可预测/可诊断子语言的算法，同时提出了模块化监督器设计方法。


<details>
  <summary>Details</summary>
Motivation: 离散事件系统通常由多个并行组件组成，纯局部监督器通常不足以保证系统的可预测性和可诊断性，因为这些是系统的全局属性。现有研究在模块化DES中强制实现这些性质的工作有限，需要开发有效的方法来设计模块化监督器。

Method: 1. 建立可预测性/可诊断性与非故障语言子集（或故障语言后缀）上预正规性的等价关系；2. 证明存在最优可预测/可诊断且正规的子语言；3. 设计算法计算最优可控、正规、可预测/可诊断子语言；4. 利用改进的预正规性概念为局部子系统计算模块化监督器。

Result: 证明了可预测性/可诊断性与预正规性的等价性，存在最优子语言，设计了有效算法，并展示了所得闭环系统具有全局可控性、正规性和可预测性/可诊断性。通过示例说明了所提方法的有效性。

Conclusion: 该论文为离散事件系统的可预测性和可诊断性提供了系统的验证和强制实现框架，通过预正规性概念建立了理论等价关系，并开发了实用的算法和模块化监督器设计方法，能够有效处理多组件并行系统的全局性质要求。

Abstract: This paper addresses the verification and enforcement of prognosability and diagnosability for discreteevent systems (DESs) modeled by deterministic finite automata. We establish the equivalence between prognosability (respectively, diagnosability) and pre-normality over a subset of the non-faulty language (respectively, a suffix of the faulty language). We then demonstrate the existence of supremal prognosable (respectively, diagnosable) and normal sublanguages. Furthermore, an algorithm is then designed to compute the supremal controllable, normal, and prognosable (respectively, diagnosable) sublanguages. Since DESs are typically composed of multiple components operating in parallel, pure local supervisors are generally insufficient, as prognosability and diagnosability are global properties of a system. Given the limited work on enforcing prognosability or diagnosability in modular DESs, where these properties are enforced through local supervisors, this paper leverages a refined version of pre-normality to compute modular supervisors for local subsystems. The resulting closed-loop system is shown to be globally controllable, normal, and prognosable/ diagnosable. Examples are provided to illustrate the proposed method.

</details>


### [99] [Distribution-Free Stochastic MPC for Joint-in-Time Chance-Constrained Linear Systems](https://arxiv.org/abs/2512.10738)
*Lukas Vogel,Andrea Carron,Eleftherios E. Vlahakis,Dimos V. Dimarogonas*

Main category: eess.SY

TL;DR: 提出一种基于共形预测的随机模型预测控制框架，用于处理未知扰动分布下的联合机会约束线性系统，无需参数假设或昂贵离线计算。


<details>
  <summary>Details</summary>
Motivation: 现有随机MPC方法通常依赖参数或高斯假设，或需要昂贵的离线计算，限制了在未知扰动分布下的实际应用。需要一种更灵活、计算高效的方法来处理联合时间机会约束。

Method: 利用共形预测作为简化工具，构建系统随机误差轨迹的有限样本置信区域，通过间接反馈机制和概率集合公式，将概率约束松弛为可优化问题。

Result: 证明了松弛优化问题的递归可行性，建立了闭环机会约束满足性，并将方法扩展到具有未知测量噪声分布的输出反馈设置，仅通过输出测量即可保证约束满足。

Conclusion: 该方法为处理未知扰动分布的随机MPC提供了一种计算高效、理论保证的框架，相比现有方法具有显著优势，数值实验验证了其有效性和优越性。

Abstract: This work presents a stochastic model predictive control (MPC) framework for linear systems subject to joint-in-time chance constraints under unknown disturbance distributions. Unlike existing stochastic MPC formulations that rely on parametric or Gaussian assumptions or require expensive offline computations, the proposed method leverages conformal prediction (CP) as a streamlined tool to construct finite-sample confidence regions for the system's stochastic error trajectories with minimal computational effort. These regions enable the relaxation of probabilistic constraints while providing formal guarantees. By employing an indirect feedback mechanism and a probabilistic set-based formulation, we prove recursive feasibility of the relaxed optimization problem and establish chance constraint satisfaction in closed-loop. Furthermore, we extend the approach to the more general output feedback setting with unknown measurement noise distributions. Given available noise samples, we establish satisfaction of the joint chance constraints and recursive feasibility via output measurements alone. Numerical examples demonstrate the effectiveness and advantages of the proposed method compared to existing approaches.

</details>


### [100] [Low-Order $\mathcal{H}_2 / \mathcal{H}_\infty$ Controller Design for Aeroelastic Vibration Suppression](https://arxiv.org/abs/2512.10841)
*Mohammad Mirtaba,Juan Augusto Paredes Salazar,Daning Huang,Ankit Goel*

Main category: eess.SY

TL;DR: 本文提出了一种基于H₂/H∞最小化的输出反馈控制器，用于悬臂梁的主动气动弹性振动抑制。


<details>
  <summary>Details</summary>
Motivation: 需要开发有效的主动控制方法来抑制悬臂梁结构在气动载荷下的振动，提高结构稳定性和性能。

Method: 1. 建立包含中等挠度和气动载荷的非线性结构模型，并用有限元法离散化；2. 从FEM模型的随机高斯输入响应数据中识别低阶线性模型；3. 使用H₂/H∞框架合成输出反馈控制器；4. 引入频率加权动态滤波器以关注感兴趣的扰动频率。

Result: 仿真结果表明所提技术在振动抑制方面有效，并研究了其对系统参数变化（包括作动器位置）的鲁棒性。

Conclusion: 提出的H₂/H∞输出反馈控制器能有效抑制悬臂梁的气动弹性振动，且对系统参数变化具有鲁棒性。

Abstract: This paper presents an $\mathcal{H}_2 / \mathcal{H}_\infty$ minimization-based output-feedback controller for active aeroelastic vibration suppression in a cantilevered beam. First, a nonlinear structural model incorporating moderate deflection and aerodynamic loading is derived and discretized using the finite element method (FEM). Then, a low-order linear model is identified from random gaussian input response data from the FEM model to synthesize an output-feedback controller using the $\mathcal{H}_2 / \mathcal{H}_\infty$ framework. A frequency-weighted dynamic filter is introduced to emphasize disturbance frequencies of interest, enabling the controller to target dominant vibration modes. Simulation results demonstrate the effectiveness of the proposed technique for vibration suppression and study its robustness to system parameter variations, including actuator placement.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [101] [Microfoundations and the Causal Interpretation of Price-Exposure Designs](https://arxiv.org/abs/2512.10076)
*Luca Moreno-Louzada,Guilherme Figueira,Pedro Picchetti*

Main category: econ.EM

TL;DR: 该论文研究利用商品价格作为工具变量来研究总体冲击对地区影响的区域暴露设计，解决了识别、估计和推断中的挑战，并提出了新的标准误估计方法。


<details>
  <summary>Details</summary>
Motivation: 传统shift-share设计利用多个冲击的差异暴露，而价格暴露设计依赖单一冲击的外生变化，这带来了识别和推断的挑战。需要为这类设计提供理论框架和可靠的推断方法。

Method: 在多部门劳动力模型和潜在结果框架下，分析2SLS和TWFE估计量的性质，推导具有清晰因果解释的条件，提出敏感性分析程序，并开发新的标准误估计方法。

Result: 标准推断方法在价格暴露设计中存在过度拒绝问题，新提出的标准误估计器具有更好的有限样本性质。在亚马逊金矿开采与凶杀案的应用中，新标准误大约是传统聚类标准误的两倍，使主要效应变得统计不显著。

Conclusion: 价格暴露设计需要专门的识别和推断方法，论文提供了理论框架、敏感性分析工具和新的标准误估计器，有助于更可靠地评估总体冲击的地区效应。

Abstract: This paper studies regional exposure designs that use commodity prices as instruments to study local effects of aggregate shocks. Unlike standard shift share designs that leverage differential exposure to many shocks, the price exposure relies on exogenous variation from a single shock, leading to challenges for both identification and inference. We motivate the design using a multi sector labor model. Under the model and a potential outcomes framework, we characterize the 2SLS and TWFE estimands as weighted averages of region and sector specific effects plus contamination terms driven by the covariance structure of prices and by general-equilibrium output responses. We derive conditions under which these estimands have a clear causal interpretation and provide simple sensitivity analysis procedures for violations. Finally, we show that standard inference procedures suffer from an overrejection problem in price-exposure designs. We derive a new standard error estimator and show its desirable finite-sample properties through Monte Carlo simulations. In an application to gold mining and homicides in the Amazon, the price exposure standard errors are roughly twice as large as conventional clustered standard errors, making the main effect statistically insignificant.

</details>


### [102] [Inference for Batched Adaptive Experiments](https://arxiv.org/abs/2512.10156)
*Jan Kemper,Davud Rostam-Afschar*

Main category: econ.EM

TL;DR: 提出BOLS统计量用于自适应实验中的因果推断，通过精度均衡化聚合处理期差异，构建渐近有效的置信区间


<details>
  <summary>Details</summary>
Motivation: 自适应实验在经济学等领域的广泛应用带来了因果推断的挑战，需要开发适用于自适应设计的统计推断方法

Method: 提出BOLS（分批普通最小二乘法）检验统计量，通过精度均衡化聚合处理期差异，结合异方差性下的每期z统计量构建渐近有效的置信区间

Result: 通过模拟实验比较了在少量处理期和不同批次观测数情况下的拒绝率，验证了方法的有效性

Conclusion: BOLS统计量为自适应实验中的因果推断提供了一种有效的解决方案，能够处理异方差性并构建渐近有效的置信区间

Abstract: The advantages of adaptive experiments have led to their rapid adoption in economics, other fields, as well as among practitioners. However, adaptive experiments pose challenges for causal inference. This note suggests a BOLS (batched ordinary least squares) test statistic for inference of treatment effects in adaptive experiments. The statistic provides a precision-equalizing aggregation of per-period treatment-control differences under heteroskedasticity. The combined test statistic is a normalized average of heteroskedastic per-period z-statistics and can be used to construct asymptotically valid confidence intervals. We provide simulation results comparing rejection rates in the typical case with few treatment periods and few (or many) observations per batch.

</details>


### [103] [AI-Enhanced TOE Framework for Sustainable Industrial Performance in Fragile and Transforming Economies: Evidence from Yemen and Saudi Arabia](https://arxiv.org/abs/2512.10333)
*Shaima Farhana,Dong Yua,Amirhossein Karamoozianc,Ali Al-shawafid,Amar N. Alsheavif*

Main category: econ.EM

TL;DR: 本研究基于AI增强的TOE框架，探讨在也门和沙特阿拉伯等脆弱转型环境中，AI如何影响工业绩效和环境可持续性。通过对600家中小企业的调查数据分析，发现AI-TOE对环境和制造绩效有显著正向影响，且工业绩效在其中起重要中介作用。


<details>
  <summary>Details</summary>
Motivation: 研究旨在填补数字转型文献中关于资源受限环境下AI采纳的空白，特别是在也门和沙特阿拉伯等脆弱和快速转型的背景下，探索如何通过AI提升工业绩效和环境可持续性。

Method: 采用基于TOE模型并增强AI的整合框架，收集了600家中小企业（两国各300家）的实地数据，通过294份管理者问卷，使用偏最小二乘结构方程模型（PLS-SEM）进行分析。

Result: PLS-SEM分析显示：AI-TOE对环境绩效有显著正向影响（β=0.487），对制造绩效影响更强（β=0.759）。沙特中小企业受益于制度支持和先进技术，也门企业则依赖低成本AI采纳和组织灵活性。工业绩效在关系中起重要中介作用。

Conclusion: AI作为变革力量，其影响取决于基础设施成熟度和组织准备度。研究为资源受限环境中的AI采纳提供了可操作框架，为政策制定者和组织在数字转型与可持续性权衡中提供了指导，特别有助于工程管理者在脆弱环境中制定更具韧性和可持续性的运营策略。

Abstract: Using an integrated framework rooted in the TOE model enhanced with AI, this study looks at ways to improve industrial performance and environmental sustainability in fragile and rapidly transforming contexts such as those found in Yemen and Saudi Arabia. Data for the research are field-based and were obtained from a total of 600 SMEs operating in both countries. Based on the questionnaires' responses by 294 managers, results from the partial least squares structural equation modeling (PLS-SEM) have indicated significant positive effects of AI-TOE on environmental performance (beta = 0.487) and manufacturing performance (beta = 0.759). Results indicate that AI acts as a transformative force, though its impact differs based on the maturity of infrastructure and organizational readiness. The Saudi SMEs gain from their institutional support and advanced technologies, while those in Yemen are dependent on the low-cost adoption of AI and organizational flexibility to accept structural challenges. PLS-SEM analysis of the study showed that integrating AI into the TOE dimensions accelerates operational efficiency in order to support environmental performance. Industrial performance was found to be a very important mediator in this relationship. This study responds to the call for digital transformation literature by providing an actionable framework of AI adoption in resource-constrained environments. These findings offer insights that might guide policymakers and organizations toward more resilient and sustainable operational strategies. These findings provide valuable guidance for engineering managers within the context of negotiating digital transformation and sustainability trade-offs in fragile and resource-constrained contexts.

</details>


### [104] [Optimal Embeddedness and Governance in Biotech Venture Capital Syndicates](https://arxiv.org/abs/2512.10568)
*Yuxin Hu,Nektarios Oraiopoulos*

Main category: econ.EM

TL;DR: 生物科技初创企业的退出成功与VC网络嵌入度呈倒U型关系：适度的共同投资联系和领域专业同质性最有利，过度嵌入反而降低成功率。治理机制（如药企VC或独立董事会）能缓解过度嵌入的负面影响。


<details>
  <summary>Details</summary>
Motivation: 现有VC网络研究主要基于软件和消费科技领域，缺乏对生物科技这种资本密集、监管严格、技术不确定性高的特殊行业的研究。需要探究在生物科技领域，重复共同投资联系和领域专业同质性如何影响初创企业的退出结果。

Method: 使用2010-2024年美国、加拿大和欧洲11,680家生物科技初创企业的面板数据，采用混合logit模型、Cox比例风险模型、多项logit模型和Fine-Gray竞争风险模型进行分析。

Result: 1. 平均先前共同投资和投资者同质性都与退出结果呈现稳健的倒U型关系
2. 适度的熟悉度和科学重叠最大化退出概率，稀疏或过度嵌入都会降低成功率
3. 治理机制起关键作用：药企VC参与或高度独立董事会能缓解过度嵌入的负面影响
4. 最优嵌入度因退出路径而异：IPO需要比并购更深的协调，而收购对同质性更不敏感

Conclusion: 研究在生命科学背景下完善了网络嵌入理论，识别了治理机制的调节作用，为从业者提供了平衡信任、专业知识和监督的量化指标，对生物科技融资实践具有指导意义。

Abstract: The biotech venture market faces intense capital demands and regulatory scrutiny, yet academic research on VC networks remains rooted in software and consumer-tech contexts. This dissertation investigates how repeated co-investment ties and domain-expertise homophily influence a venture's exit likelihood, timing, and route amid the sector's pronounced technological and market uncertainty. Using a novel panel of 11,680 biotechnology start-ups from the United States, Canada, and Europe (2010-2024), we apply pooled logit, Cox proportional-hazards, multinomial logit, and Fine-Gray competing-risk models. Our findings show that both average prior co-investment and investor homophily exhibit robust inverted-U relationships with exit outcomes. Moderate familiarity and scientific overlap maximize exit probability, while either sparse or excessive embedding reduces success. Governance mechanisms also play a crucial role: participation of a pharmaceutical corporate VC or a highly independent board flattens the negative effects of over-embedding, enabling syndicates to sustain exit momentum at higher levels of familiarity or homogeneity. Furthermore, the optimal degree of embeddedness is route-specific: IPOs require deeper coordination than trade sales, while acquisitions peak earlier and are less sensitive to homophily. These findings refine network-embeddedness theory in the life-science context, identify governance contingencies, and offer practitioners quantitative metrics to balance trust, expertise, and oversight in biotech financing.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [105] [LxCIM: a new rank-based binary classifier performance metric invariant to local exchange of classes](https://arxiv.org/abs/2512.10053)
*Tiago Brogueira,Mário A. T. Figueiredo*

Main category: stat.ML

TL;DR: 提出LxCIM新指标，解决AUROC在局部类别交换不变性问题中的局限性，应用于因果发现领域


<details>
  <summary>Details</summary>
Motivation: 虽然AUROC是二分类问题的标准评估指标，但它不适用于具有局部类别交换不变性(LxC)的问题。现有指标在因果发现等领域存在局限性，需要新的评估指标。

Method: 提出LxCIM指标，该指标基于排序、具有局部类别交换不变性、直观、逻辑一致且始终可计算。通过累积准确率-决策率曲线支持更详细分析，并与AUROC、准确率和AUDRC建立理论联系。

Result: LxCIM具有多重互补解释：作为AUROC的对称形式、准确率的排序类比、或AUDRC更具代表性和可解释性的变体。在双变量因果发现问题中直接应用，解决了现有指标的局限性。

Conclusion: LxCIM是一个有理论基础的评估指标，适用于具有局部类别交换不变性的问题，特别在因果发现领域具有实用价值，代码已公开。

Abstract: Binary classification is one of the oldest, most prevalent, and studied problems in machine learning. However, the metrics used to evaluate model performance have received comparatively little attention. The area under the receiver operating characteristic curve (AUROC) has long been a standard choice for model comparison. Despite its advantages, AUROC is not always ideal, particularly for problems that are invariant to local exchange of classes (LxC), a new form of metric invariance introduced in this work. To address this limitation, we propose LxCIM (LxC-invariant metric), which is not only rank-based and invariant under local exchange of classes, but also intuitive, logically consistent, and always computable, while enabling more detailed analysis through the cumulative accuracy-decision rate curve. Moreover, LxCIM exhibits clear theoretical connections to AUROC, accuracy, and the area under the accuracy-decision rate curve (AUDRC). These relationships allow for multiple complementary interpretations: as a symmetric form of AUROC, a rank-based analogue of accuracy, or a more representative and more interpretable variant of AUDRC. Finally, we demonstrate the direct applicability of LxCIM to the bivariate causal discovery problem (which exhibits invariance to local exchange of classes) and show how it addresses the acknowledged limitations of existing metrics used in this field. All code and implementation details are publicly available at github.com/tiagobrogueira/Causal-Discovery-In-Exchangeable-Data.

</details>


### [106] [The Interplay of Statistics and Noisy Optimization: Learning Linear Predictors with Random Data Weights](https://arxiv.org/abs/2512.10188)
*Gabriel Clara,Yazan Mash'al*

Main category: stat.ML

TL;DR: 分析随机加权梯度下降在线性回归中的隐式正则化效应，建立收敛性理论框架，揭示优化速度与统计性能的权衡


<details>
  <summary>Details</summary>
Motivation: 为随机加权梯度下降（包括SGD、重要性采样等）提供统一分析框架，研究不同权重分布对训练轨迹的影响，理解噪声如何影响优化过程和统计性能

Method: 在线性回归模型中分析随机加权梯度下降，使用几何矩收缩技术，推导一阶和二阶矩的非渐近收敛界，研究平稳分布特性

Result: 建立了随机加权的隐式正则化与加权线性回归的联系，给出了收敛性理论保证，发现某些权重分布虽然收敛快但统计性能差

Conclusion: 随机权重分布的选择同时影响优化效率和统计性能，存在收敛速度与估计质量的权衡，为设计更好的随机优化算法提供理论指导

Abstract: We analyze gradient descent with randomly weighted data points in a linear regression model, under a generic weighting distribution. This includes various forms of stochastic gradient descent, importance sampling, but also extends to weighting distributions with arbitrary continuous values, thereby providing a unified framework to analyze the impact of various kinds of noise on the training trajectory. We characterize the implicit regularization induced through the random weighting, connect it with weighted linear regression, and derive non-asymptotic bounds for convergence in first and second moments. Leveraging geometric moment contraction, we also investigate the stationary distribution induced by the added noise. Based on these results, we discuss how specific choices of weighting distribution influence both the underlying optimization problem and statistical properties of the resulting estimator, as well as some examples for which weightings that lead to fast convergence cause bad statistical performance.

</details>


### [107] [Error Analysis of Generalized Langevin Equations with Approximated Memory Kernels](https://arxiv.org/abs/2512.10256)
*Quanjun Lang,Jianfeng Lu*

Main category: stat.ML

TL;DR: 该论文分析了具有记忆的随机动力系统中的预测误差，重点关注广义朗之万方程（GLEs）。研究建立了在强凸势下轨迹差异的衰减速率，该速率由记忆核的衰减决定，并通过加权范数中的核估计误差进行定量界定。


<details>
  <summary>Details</summary>
Motivation: 研究随机动力系统中预测误差的动机在于理解具有记忆的系统（如广义朗之万方程）中轨迹预测的准确性。这类系统在物理、化学和生物学中广泛存在，但记忆效应对预测误差的影响尚未得到充分量化。

Method: 方法包括：1）使用同步噪声耦合和Volterra比较定理分析轨迹差异衰减；2）对一阶模型使用加权空间中的预解估计推导矩和扰动边界；3）对二阶模型使用低耗散Lyapunov型距离证明收缩性和稳定性；4）处理非平移不变核和白噪声强迫。

Result: 主要结果：1）在强凸势下，轨迹差异以记忆核衰减速率衰减；2）预测误差由核估计误差在加权范数中界定；3）框架适用于指数和次指数衰减核；4）数值验证了理论发现。

Conclusion: 结论：该研究建立了一个理论框架，将改进的核估计与增强的轨迹预测明确联系起来。分析表明，在具有记忆的随机动力系统中，预测精度直接取决于记忆核的估计质量，为这类系统的预测误差分析提供了量化工具。

Abstract: We analyze prediction error in stochastic dynamical systems with memory, focusing on generalized Langevin equations (GLEs) formulated as stochastic Volterra equations. We establish that, under a strongly convex potential, trajectory discrepancies decay at a rate determined by the decay of the memory kernel and are quantitatively bounded by the estimation error of the kernel in a weighted norm. Our analysis integrates synchronized noise coupling with a Volterra comparison theorem, encompassing both subexponential and exponential kernel classes. For first-order models, we derive moment and perturbation bounds using resolvent estimates in weighted spaces. For second-order models with confining potentials, we prove contraction and stability under kernel perturbations using a hypocoercive Lyapunov-type distance. This framework accommodates non-translation-invariant kernels and white-noise forcing, explicitly linking improved kernel estimation to enhanced trajectory prediction. Numerical examples validate these theoretical findings.

</details>


### [108] [Diffusion differentiable resampling](https://arxiv.org/abs/2512.10401)
*Jennifer Rosina Andersson,Zheng Zhao*

Main category: stat.ML

TL;DR: 提出基于集成分数扩散模型的可微分重采样方法，用于序列蒙特卡洛，在随机滤波和参数估计中优于现有方法


<details>
  <summary>Details</summary>
Motivation: 在序列蒙特卡洛（如粒子滤波）中，需要可微分的重采样方法，以便进行端到端的梯度优化和参数估计

Method: 基于集成分数扩散模型，提出了一种新的信息性重采样方法，该方法具有即时路径可微性

Result: 证明了扩散重采样方法对重采样分布提供了一致性估计，实验表明在随机滤波和参数估计任务中优于现有最先进的可微分重采样方法

Conclusion: 提出的扩散重采样方法为序列蒙特卡洛提供了有效的可微分重采样方案，在滤波和参数估计应用中表现出优越性能

Abstract: This paper is concerned with differentiable resampling in the context of sequential Monte Carlo (e.g., particle filtering). We propose a new informative resampling method that is instantly pathwise differentiable, based on an ensemble score diffusion model. We prove that our diffusion resampling method provides a consistent estimate to the resampling distribution, and we show by experiments that it outperforms the state-of-the-art differentiable resampling methods when used for stochastic filtering and parameter estimation.

</details>


### [109] [Supervised Learning of Random Neural Architectures Structured by Latent Random Fields on Compact Boundaryless Multiply-Connected Manifolds](https://arxiv.org/abs/2512.10407)
*Christian Soize*

Main category: stat.ML

TL;DR: 提出一个基于几何感知随机场的概率框架，用于神经网络监督学习，将架构拓扑和权重联合建模为流形上随机场的实现。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络缺乏对复杂不确定系统的概率建模能力，特别是当输出具有强非高斯特性时。需要建立几何感知的随机生成框架，将神经网络架构本身视为随机对象。

Method: 在紧致无边多连通流形上定义潜在各向异性高斯随机场，通过非齐次泊松过程采样神经元位置，基于测地线邻近性和局部场亲和性建立连接，权重从场实现中条件采样，使用百分位数扩散掩码实现稀疏化。

Result: 建立了模型的基本数学性质，包括适定性、可测性，以及诱导随机映射的表达变异性分析，为几何驱动随机学习的更广泛理论奠定了基础。

Conclusion: 提出了一个新颖的几何感知概率框架，将神经网络架构视为随机场的实现，为建模复杂不确定系统提供了数学基础，并展示了其内部一致性。

Abstract: This paper introduces a new probabilistic framework for supervised learning in neural systems. It is designed to model complex, uncertain systems whose random outputs are strongly non-Gaussian given deterministic inputs. The architecture itself is a random object stochastically generated by a latent anisotropic Gaussian random field defined on a compact, boundaryless, multiply-connected manifold. The goal is to establish a novel conceptual and mathematical framework in which neural architectures are realizations of a geometry-aware, field-driven generative process. Both the neural topology and synaptic weights emerge jointly from a latent random field. A reduced-order parameterization governs the spatial intensity of an inhomogeneous Poisson process on the manifold, from which neuron locations are sampled. Input and output neurons are identified via extremal evaluations of the latent field, while connectivity is established through geodesic proximity and local field affinity. Synaptic weights are conditionally sampled from the field realization, inducing stochastic output responses even for deterministic inputs. To ensure scalability, the architecture is sparsified via percentile-based diffusion masking, yielding geometry-aware sparse connectivity without ad hoc structural assumptions. Supervised learning is formulated as inference on the generative hyperparameters of the latent field, using a negative log-likelihood loss estimated through Monte Carlo sampling from single-observation-per-input datasets. The paper initiates a mathematical analysis of the model, establishing foundational properties such as well-posedness, measurability, and a preliminary analysis of the expressive variability of the induced stochastic mappings, which support its internal coherence and lay the groundwork for a broader theory of geometry-driven stochastic learning.

</details>


### [110] [Maximum Risk Minimization with Random Forests](https://arxiv.org/abs/2512.10445)
*Francesco Freni,Anya Fries,Linus Kühne,Markus Reichstein,Jonas Peters*

Main category: stat.ML

TL;DR: 提出基于最大风险最小化(MaxRM)原则的随机森林变体，用于处理分布外泛化问题，在模拟和真实数据上评估性能。


<details>
  <summary>Details</summary>
Motivation: 在分布外泛化场景中，训练数据和测试数据来自不同分布环境。现有方法如最大风险最小化(MaxRM)原则旨在设计能更好泛化到未见测试分布的方法，但需要适用于随机森林等实际算法。

Method: 提出基于MaxRM原则的随机森林变体，提供计算高效算法。支持三种风险度量：均方误差、负奖励（与解释方差相关）和遗憾（相对于最佳预测器的超额风险）。对于基于遗憾的MaxRM，证明了未见测试分布上的样本外保证。

Result: 证明了主要方法的统计一致性，为基于遗憾的MaxRM提供了新颖的样本外泛化保证。在模拟和真实世界数据上评估了所提方法。

Conclusion: 成功将MaxRM原则应用于随机森林框架，提供了理论保证和实际算法，为分布外泛化问题提供了有效的解决方案。

Abstract: We consider a regression setting where observations are collected in different environments modeled by different data distributions. The field of out-of-distribution (OOD) generalization aims to design methods that generalize better to test environments whose distributions differ from those observed during training. One line of such works has proposed to minimize the maximum risk across environments, a principle that we refer to as MaxRM (Maximum Risk Minimization). In this work, we introduce variants of random forests based on the principle of MaxRM. We provide computationally efficient algorithms and prove statistical consistency for our primary method. Our proposed method can be used with each of the following three risks: the mean squared error, the negative reward (which relates to the explained variance), and the regret (which quantifies the excess risk relative to the best predictor). For MaxRM with regret as the risk, we prove a novel out-of-sample guarantee over unseen test distributions. Finally, we evaluate the proposed methods on both simulated and real-world data.

</details>


### [111] [Flexible Deep Neural Networks for Partially Linear Survival Data](https://arxiv.org/abs/2512.10570)
*Asaf Ben Arie,Malka Gorfine*

Main category: stat.ML

TL;DR: 提出FLEXI-Haz框架，结合参数线性部分和非参数神经网络部分，在保持可解释性的同时灵活建模生存数据，不依赖比例风险假设。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度神经网络的部分线性Cox模型依赖比例风险假设，限制了应用范围。需要一种既保持可解释性（通过线性部分）又能灵活建模复杂时间-协变量交互（通过神经网络）的方法，且不依赖比例风险假设。

Method: 提出FLEXI-Haz框架，采用部分线性回归结构：参数线性部分处理主要关注协变量（保持可解释性），非参数DNN部分捕捉混杂变量的复杂时间-协变量交互。不依赖比例风险假设。

Result: 理论保证：神经网络部分在复合Holder类上达到极小极大最优收敛率；线性估计量具有根号n一致性、渐近正态性和半参数有效性。模拟和真实数据分析显示FLEXI-Haz能准确估计线性效应。

Conclusion: FLEXI-Haz为基于比例风险的现代方法提供了原则性和可解释的替代方案，在保持可解释性的同时灵活建模生存数据，不依赖比例风险假设，具有理论保证和良好实证性能。

Abstract: We propose a flexible deep neural network (DNN) framework for modeling survival data within a partially linear regression structure. The approach preserves interpretability through a parametric linear component for covariates of primary interest, while a nonparametric DNN component captures complex time-covariate interactions among nuisance variables. We refer to the method as FLEXI-Haz, a flexible hazard model with a partially linear structure. In contrast to existing DNN approaches for partially linear Cox models, FLEXI-Haz does not rely on the proportional hazards assumption. We establish theoretical guarantees: the neural network component attains minimax-optimal convergence rates based on composite Holder classes, and the linear estimator is root-n consistent, asymptotically normal, and semiparametrically efficient. Extensive simulations and real-data analyses demonstrate that FLEXI-Haz provides accurate estimation of the linear effect, offering a principled and interpretable alternative to modern methods based on proportional hazards. Code for implementing FLEXI-Haz, as well as scripts for reproducing data analyses and simulations, is available at: https://github.com/AsafBanana/FLEXI-Haz

</details>


### [112] [Physics-informed Polynomial Chaos Expansion with Enhanced Constrained Optimization Solver and D-optimal Sampling](https://arxiv.org/abs/2512.10873)
*Qitian Lu,Himanshu Sharma,Michael D. Shields,Lukáš Novák*

Main category: stat.ML

TL;DR: 该研究提出两种增强物理信息多项式混沌展开(PC²)的方法：采用SULM优化求解器降低高维问题计算成本，以及使用D-最优采样策略选择信息丰富的虚拟点，提升模型稳定性和效率平衡。


<details>
  <summary>Details</summary>
Motivation: 标准PC²方法在处理高维参数空间、数据有限或训练数据不具代表性时，性能和效率会下降。需要改进物理约束代理模型框架，以应对高维不确定性量化任务。

Method: 1. 采用SULM（直接更新拉格朗日乘子）作为KKT条件的替代求解器，降低高维问题和导数边界条件的计算成本；2. 使用D-最优采样策略选择信息丰富的虚拟点，平衡精度与效率。

Result: 增强后的PC²框架在常微分方程和偏微分方程控制的代表性物理系统数值实验中，展现出比标准PC²更好的综合能力，特别适合高维不确定性量化任务。

Conclusion: 通过SULM求解器和D-最优采样策略的集成，显著提升了PC²框架在高维参数空间下的计算效率和稳定性，为复杂物理系统的约束代理建模提供了更有效的解决方案。

Abstract: Physics-informed polynomial chaos expansions (PC$^2$) provide an efficient physically constrained surrogate modeling framework by embedding governing equations and other physical constraints into the standard data-driven polynomial chaos expansions (PCE) and solving via the Karush-Kuhn-Tucker (KKT) conditions. This approach improves the physical interpretability of surrogate models while achieving high computational efficiency and accuracy. However, the performance and efficiency of PC$^2$ can still be degraded with high-dimensional parameter spaces, limited data availability, or unrepresentative training data. To address this problem, this study explores two complementary enhancements to the PC$^2$ framework. First, a numerically efficient constrained optimization solver, straightforward updating of Lagrange multipliers (SULM), is adopted as an alternative to the conventional KKT solver. The SULM method significantly reduces computational cost when solving physically constrained problems with high-dimensionality and derivative boundary conditions that require a large number of virtual points. Second, a D-optimal sampling strategy is utilized to select informative virtual points to improve the stability and achieve the balance of accuracy and efficiency of the PC$^2$. The proposed methods are integrated into the PC$^2$ framework and evaluated through numerical examples of representative physical systems governed by ordinary or partial differential equations. The results demonstrate that the enhanced PC$^2$ has better comprehensive capability than standard PC$^2$, and is well-suited for high-dimensional uncertainty quantification tasks.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [113] [Defining the Scope of Learning Analytics: An Axiomatic Approach for Analytic Practice and Measurable Learning Phenomena](https://arxiv.org/abs/2512.10081)
*Kensuke Takii,Changhao Liang,Hiroaki Ogata*

Main category: cs.CY

TL;DR: 本文提出了学习分析（LA）的第一个公理化理论，通过五个公理正式定义了LA的基本结构、范围和限制，为LA作为一门严谨的科学学科奠定了理论基础。


<details>
  <summary>Details</summary>
Motivation: 学习分析领域在实践和技术创新方面迅速发展，但其基础身份在理论上仍然不够明确。本文旨在填补这一空白，为LA提供一个正式的理论基础。

Method: 从心理学的学习定义和LA的方法论要求出发，提出了包含五个公理的框架，这些公理规定了离散观察、经验构建、状态转换和推断。从这些公理中推导出一系列定理和命题，并定义了LA结构和LA实践作为形式对象。

Result: 该理论阐明了LA的认识论立场，包括学习者状态的固有不可观察性、时间顺序的不可约性、可达状态的约束以及确定性预测未来学习的不可能性。证明了该框架能够统一解释各种LA方法（如贝叶斯知识追踪和仪表板）。

Conclusion: 该工作将LA定位为基于可观察性的状态转换系统的严谨科学，为LA作为一个学术学科的成熟提供了必要的理论基础，并为设计分析方法和解释学习数据提供了指导原则。

Abstract: Learning Analytics (LA) has rapidly expanded through practical and technological innovation, yet its foundational identity has remained theoretically under-specified. This paper addresses this gap by proposing the first axiomatic theory that formally defines the essential structure, scope, and limitations of LA. Derived from the psychological definition of learning and the methodological requirements of LA, the framework consists of five axioms specifying discrete observation, experience construction, state transition, and inference. From these axioms, we derive a set of theorems and propositions that clarify the epistemological stance of LA, including the inherent unobservability of learner states, the irreducibility of temporal order, constraints on reachable states, and the impossibility of deterministically predicting future learning. We further define LA structure and LA practice as formal objects, demonstrating the sufficiency and necessity of the axioms and showing that diverse LA approaches -- such as Bayesian Knowledge Tracing and dashboards -- can be uniformly explained within this framework. The theory provides guiding principles for designing analytic methods and interpreting learning data while avoiding naive behaviorism and category errors by establishing an explicit theoretical inference layer between observations and states. This work positions LA as a rigorous science of state transition systems based on observability, establishing the theoretical foundation necessary for the field's maturation as a scholarly discipline.

</details>


### [114] [Dark Personality Traits and Online Toxicity: Linking Self-Reports to Reddit Activity](https://arxiv.org/abs/2512.10113)
*Aldo Cerulli,Benedetta Tessa,Giuseppe La Selva,Oronzo Mazzeo,Lorenzo Cima,Lucia Monacis,Stefano Cresci*

Main category: cs.CY

TL;DR: 研究黑暗人格特质与在线毒性行为的关系，发现黑暗特质主要影响毒性内容的生产而非感知，施虐狂和精神病态倾向与明显毒性语言最相关，现有文本代理指标与验证测量对应有限。


<details>
  <summary>Details</summary>
Motivation: 黑暗人格特质与在线不当行为（如网络骚扰、不文明言论、毒性言论）有关，但这些特质与实际在线行为的关系研究不足，需要深入探究黑暗特质、在线毒性和用户活动的社会语言特征之间的关联。

Method: 开发了一个Web应用程序，整合Amazon Mechanical Turk用户的验证心理问卷数据与他们的Reddit活动数据，收集了近57K条Reddit评论（包含220万词元和15.27万句子），通过224个语言和行为特征系统表征，使用多重相关分析检验这些特征与问卷测量特质的关系。

Result: 黑暗特质主要影响在线不文明行为的生产而非感知；施虐狂和精神病态倾向与明显毒性语言最相关；其他黑暗倾向表现更微妙，常逃过简单文本代理；自我报告的攻击行为与实际在线活动相符；现有手工制作的黑暗三联征文本代理指标与验证测量对应有限；光明与黑暗特质以微妙方式相互作用，外向性减少网络骚扰倾向，尽责性与权利感和冷漠有适度关联。

Conclusion: 研究深化了对人格如何塑造在线毒性行为的理解，强调了开发可靠计算工具和针对性有效审核策略的机遇与挑战，表明黑暗特质主要通过内容生产而非感知影响在线毒性，需要更精细的方法来检测微妙表现。

Abstract: Dark personality traits have been linked to online misbehavior such as trolling, incivility, and toxic speech. Yet the relationship between these traits and actual online conduct remains understudied. Here we investigate the associations between dark traits, online toxicity, and the socio-linguistic characteristics of online user activity. To explore this relationship, we developed a Web application that integrates validated psychological questionnaires from Amazon Mechanical Turk users to their Reddit activity data. This allowed collecting nearly 57K Reddit comments, including 2.2M tokens and 152.7K sentences from 114 users, that we systematically represent through 224 linguistic and behavioral features. We then examined their relationship to questionnaire-based trait measures via multiple correlation analyses. Among our findings is that dark traits primarily influence the production rather than the perception of online incivility. Sadistic and psychopathic tendencies are most strongly associated with overtly toxic language, whereas other dark dispositions manifest more subtly, often eluding simple textual proxies. Self-reported engagement in hostile behavior mirrors actual online activity, while existing hand-crafted textual proxies for dark triad traits show limited correspondence with our validated measures. Finally, bright and dark traits interact in nuanced ways, with extraversion reducing trolling tendencies and conscientiousness showing modest associations with entitlement and callousness. These findings deepen understanding of how personality shapes toxic online behavior and highlight both opportunities and challenges for developing reliable computational tools and targeted, effective moderation strategies.

</details>


### [115] [Enhancing Large Language Models for End-to-End Circuit Analysis Problem Solving](https://arxiv.org/abs/2512.10159)
*Liangliang Chen,Weiyu Sun,Ying Zhang*

Main category: cs.CY

TL;DR: 基于Gemini 2.5 Pro构建的增强型端到端电路问题求解器，通过集成YOLO检测器和ngspice验证循环，将电路问题求解准确率从79.52%提升至97.59%


<details>
  <summary>Details</summary>
Motivation: 大语言模型在工程任务中的可靠性有限，特别是在需要多模态理解和精确数学推理的电路分析领域。工程教育需要能够生成准确解决方案的可扩展AI工具，用于自动化作业反馈和问答系统。

Method: 1) 对Gemini 2.5 Pro在本科电路问题上进行基准测试，识别出电路识别幻觉和推理过程幻觉两大失败模式；2) 集成微调YOLO检测器和OpenCV处理来隔离电压和电流源，重新识别源极性；3) 引入基于ngspice的验证循环，通过仿真验证电路解决方案，并在出现差异时触发迭代重新生成。

Result: 在83个问题中，提出的管道实现了97.59%的成功率（81个正确解决方案），显著优于Gemini 2.5 Pro原始的79.52%准确率。系统能够近乎完美地重新识别源极性。

Conclusion: 该系统扩展了LLM在多模态工程问题求解中的能力，支持创建高质量教育数据集和AI驱动的教学工具，为工程教育提供了可扩展的准确解决方案生成方法。

Abstract: Large language models (LLMs) have shown strong performance in data-rich domains such as programming, but their reliability in engineering tasks remains limited. Circuit analysis -- requiring multimodal understanding and precise mathematical reasoning -- highlights these challenges. Although Gemini 2.5 Pro improves diagram interpretation and analog-circuit reasoning, it still struggles to consistently produce correct solutions when given both text and circuit diagrams. At the same time, engineering education needs scalable AI tools capable of generating accurate solutions for tasks such as automated homework feedback and question-answering. This paper presents an enhanced, end-to-end circuit problem solver built on Gemini 2.5 Pro. We first benchmark Gemini on a representative set of undergraduate circuit problems and identify two major failure modes: 1) circuit-recognition hallucinations, particularly incorrect source polarity detection, and 2) reasoning-process hallucinations, such as incorrect current directions. To address recognition errors, we integrate a fine-tuned YOLO detector and OpenCV processing to isolate voltage and current sources, enabling Gemini to re-identify source polarities from cropped images with near-perfect accuracy. To reduce reasoning errors, we introduce an ngspice-based verification loop in which Gemini generates a .cir file, ngspice simulates the circuit, and discrepancies trigger iterative regeneration with optional human-in-the-loop review. Across 83 problems, the proposed pipeline achieves a 97.59% success rate (81 correct solutions), substantially outperforming Gemini 2.5 Pro's original 79.52% accuracy. This system extends LLM capabilities for multimodal engineering problem-solving and supports the creation of high-quality educational datasets and AI-powered instructional tools.

</details>


### [116] [Designing AI-Resilient Assessments Using Interconnected Problems: A Theoretically Grounded and Empirically Validated Framework](https://arxiv.org/abs/2512.10758)
*Kaihua Ding*

Main category: cs.CY

TL;DR: 提出AI弹性评估框架，通过互联问题设计抵抗生成式AI滥用，恢复学术诚信


<details>
  <summary>Details</summary>
Motivation: 生成式AI的快速普及破坏了传统模块化评估，导致学术评估与行业实践脱节，需要设计能抵抗AI滥用的评估方法

Method: 建立理论框架：1) 互联问题比模块化评估更AI弹性；2) 半结构化问题比完全开放式项目更可靠。通过4门大学数据科学课程(N=138)进行多年实证验证，并转化为实用评估设计框架

Result: AI辅助下模块化作业接近满分，监考考试下降约30个百分点；互联项目与模块化评估强相关，能测量相同技能同时抵抗AI滥用；监考考试相关性较弱，可能测量应试能力而非学习成果

Conclusion: 提出实用的AI弹性评估设计框架，促进整合性思维，反映真实世界AI增强工作流程，自然抵抗生成式AI的简单委托，帮助恢复学术诚信

Abstract: The rapid adoption of generative AI has undermined traditional modular assessments in computing education, creating a disconnect between academic evaluation and industry practice. This paper presents a theoretically grounded framework for designing AI-resilient assessments, supported by formal analysis and multi-year empirical validation.
  We make three contributions. First, we establish two theoretical results: (1) assessments composed of interconnected problems, where outputs feed into subsequent stages, are more AI-resilient than modular assessments because current language models struggle with sustained multi-step reasoning and context; and (2) semi-structured problems with deterministic success criteria provide more reliable measures of student competency than fully open-ended projects, which allow AI systems to default to familiar solution patterns. These results challenge common policy and institutional guidance that promotes open-ended assessments as the primary safeguard for academic integrity.
  Second, we validate these results using data from four university data science courses (N = 138). While students achieve near-perfect scores on AI-assisted modular homework, performance drops by roughly 30 percentage points on proctored exams, indicating substantial AI score inflation. Interconnected projects remain strongly correlated with modular assessments, suggesting they measure the same underlying skills while resisting AI misuse. Proctored exams show weaker alignment, implying they may assess test-taking ability rather than intended learning outcomes.
  Third, we translate these findings into a practical assessment design framework. The proposed approach enables educators to create assessments that promote integrative thinking, reflect real-world AI-augmented workflows, and naturally resist trivial delegation to generative AI, thereby helping restore academic integrity.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [117] [Quantitative results on a generalized viscosity approximation method](https://arxiv.org/abs/2512.09968)
*Paulo Firmino,Laurentiu Leustean*

Main category: math.OC

TL;DR: 在非线性设置下研究广义粘性逼近方法的渐近行为，应用于满足类预解条件的非扩张映射族，使用证明挖掘方法获得W-双曲空间中的渐近正则性定量结果和CAT(0)空间中的亚稳定性速率。


<details>
  <summary>Details</summary>
Motivation: 研究非线性空间中广义粘性逼近方法的渐近行为，特别是针对满足类预解条件的非扩张映射族，旨在获得定量结果而非仅定性分析。

Method: 采用证明挖掘方法，在W-双曲空间中分析渐近正则性，在CAT(0)空间中分析亚稳定性速率，研究广义粘性逼近方法在非线性设置下的行为。

Result: 获得了W-双曲空间中渐近正则性的定量结果和CAT(0)空间中亚稳定性的具体速率，为非线性空间中粘性逼近方法提供了有效的定量分析工具。

Conclusion: 证明挖掘方法能有效分析非线性空间中广义粘性逼近方法的渐近行为，为满足类预解条件的非扩张映射族提供了定量结果，扩展了相关理论的应用范围。

Abstract: In this paper, we study, in a nonlinear setting, the asymptotic behaviour of a generalized viscosity approximation method associated with a countable family of nonexpansive mappings satisfying resolvent-like conditions. We apply proof mining methods to obtain quantitative results on asymptotic regularity in W-hyperbolic spaces and rates of metastability in CAT(0) spaces.

</details>


### [118] [Fast projection onto the top-k-sum constraint](https://arxiv.org/abs/2512.10255)
*Jianting Pan,Ming Yan*

Main category: math.OC

TL;DR: 提出一种高效算法用于计算到top-k-sum约束的欧几里得投影，避免了传统方法中的排序操作，将复杂度从O(nlogn)降低到O(n)，在金融风险管理和矩阵优化中具有重要应用。


<details>
  <summary>Details</summary>
Motivation: 现有投影方法依赖于排序操作，具有O(nlogn)的初始复杂度，这限制了在高维设置中的可扩展性。需要开发更高效的算法来处理金融风险管理和矩阵优化中的大规模问题。

Method: 重新审视投影问题的KKT条件，引入松弛条件来刻画解的特征。基于几何解释（寻找两个单调分段线性函数的交点），提出一种迭代算法直接搜索交点，完全避免所有排序过程。

Result: 算法全局收敛并在有限迭代次数内达到精确解。大量数值实验表明，该算法显著优于现有算法，在广泛问题实例中表现出经验O(n)复杂度。

Conclusion: 提出的算法通过避免排序操作，为top-k-sum约束的欧几里得投影提供了高效解决方案，在计算复杂度和实际性能方面都有显著提升，适用于高维金融风险管理和矩阵优化问题。

Abstract: This paper develops an efficient algorithm for computing the Euclidean projection onto the top-k-sum constraint, a key operation in financial risk management and matrix optimization problems. Existing projection methods rely on sorting and therefore incur an initial O(nlogn) complexity, which limits their scalability in high-dimensional settings. To address this difficulty, we revisit the Karush-Kuhn-Tucker (KKT) conditions of the projection problem and introduce relaxed conditions that remain sufficient for characterizing the solution. These conditions lead to a simple geometric interpretation: finding the solutions is equivalent to locating the intersection of two monotone piecewise linear functions. Building on this insight, we propose an iterative and highly efficient algorithm that searches directly for the intersection point and completely avoids all sorting procedures. We prove that the algorithm converges globally and reaches the exact solution in a finite number of iterations. Extensive numerical experiments further demonstrate that the proposed algorithm substantially outperforms existing algorithms and exhibits empirical O(n) complexity across a broad range of problem instances.

</details>


### [119] [Optimality Deviation using the Koopman Operator](https://arxiv.org/abs/2512.10270)
*Yicheng Lin,Bingxian Wu,Nan Bai,Yunxiao Ren,Zhisheng Duan*

Main category: math.OC

TL;DR: 该论文研究了在非线性系统数据驱动最优控制中使用Koopman算子时近似误差的影响，推导了最优控制器和价值函数偏差的显式上界，并通过数值示例验证了理论发现。


<details>
  <summary>Details</summary>
Motivation: Koopman算子虽然能通过提升状态空间简化非线性动力学表示，但近似误差的存在必然导致计算的最优控制器和价值函数出现偏差，需要量化分析这种影响。

Method: 推导了近似误差导致的最优控制器和价值函数偏差的显式上界，这些上界表征了近似误差的最坏情况影响，并通过数值示例进行支持验证。

Result: 获得了最优性偏差的显式上界，为改进数据驱动最优控制器设计的鲁棒性提供了量化基础，数值示例支持了理论发现。

Conclusion: 该研究为Koopman算子框架下数据驱动最优控制提供了理论保证，量化了近似误差的影响，有助于设计更鲁棒的控制系统。

Abstract: This paper investigates the impact of approximation error in data-driven optimal control problem of nonlinear systems while using the Koopman operator. While the Koopman operator enables a simplified representation of nonlinear dynamics through a lifted state space, the presence of approximation error inevitably leads to deviations in the computed optimal controller and the resulting value function. We derive explicit upper bounds for these optimality deviations, which characterize the worst-case effect of approximation error. Supported by numerical examples, these theoretical findings provide a quantitative foundation for improving the robustness of data-driven optimal controller design.

</details>


### [120] [Residual subspace evolution strategies for nonlinear inverse problems](https://arxiv.org/abs/2512.10325)
*Francesco Alemanno*

Main category: math.OC

TL;DR: RSES是一种无导数优化器，通过构建残差子空间代理模型，在低维子空间中进行最优更新，避免了雅可比矩阵和协方差矩阵的计算，适用于非线性逆问题。


<details>
  <summary>Details</summary>
Motivation: 非线性逆问题通常具有噪声、不可微分或计算昂贵的残差评估，使得基于雅可比矩阵的求解器不可靠。现有的无导数优化器如NES或NEWUOA假设平滑性或需要大量评估来保持稳定性，而集成卡尔曼反演（EKI）需要预条件处理且随残差维度扩展性差。

Method: RSES在当前位置周围采样高斯探针，从它们的差异构建仅基于残差的代理模型，通过最小二乘求解重新组合探针，产生最优更新而不形成雅可比矩阵或协方差矩阵。每次迭代成本为k+1次残差评估，其中k远小于问题维度n。

Result: 在标定、回归和反卷积问题的基准测试中，RSES在确定性和随机设置下都表现出一致的失配减少。RSES匹配或超越了xNES和NEWUOA，在与EKI匹配评估预算时保持竞争力，特别是在平滑性或协方差假设失败的情况下。

Conclusion: RSES提供了一种高效的无导数优化方法，通过残差子空间代理模型在低维子空间中进行优化，避免了传统方法对平滑性或协方差结构的假设，在多种非线性逆问题中表现出色。

Abstract: Nonlinear inverse problems often feature noisy, non-differentiable, or expensive residual evaluations that make Jacobian-based solvers unreliable. Popular derivative-free optimizers such as natural evolution strategies (NES) or Powell's NEWUOA still assume smoothness or expend many evaluations to maintain stability. Ensemble Kalman inversion (EKI) relies on empirical covariances that require preconditioning and scale poorly with residual dimension.
  We introduce residual subspace evolution strategies (RSES), a derivative-free solver that samples Gaussian probes around the current iterate, builds a residual-only surrogate from their differences, and recombines the probes through a least-squares solve yielding an optimal update without forming Jacobians or covariances. Each iteration costs $k+1$ residual evaluations, where $k \ll n$ for $n$-dimensional problems, with $O(k^3)$ linear algebra overhead.
  Benchmarks on calibration, regression, and deconvolution problems demonstrate consistent misfit reduction in both deterministic and stochastic settings. RSES matches or surpasses xNES and NEWUOA while staying competitive with EKI under matched evaluation budgets, particularly when smoothness or covariance assumptions fail.

</details>


### [121] [Primal-dual splitting for structured composite monotone inclusions with or without cocoercivity](https://arxiv.org/abs/2512.10366)
*Minh N. Dao,Hung M. Phan,Matthew K. Tam,Thang D. Truong*

Main category: math.OC

TL;DR: 提出一种原始-对偶分裂算法，用于处理包含多个集值算子、有界线性算子复合以及非共轭单值算子的结构化复合单调包含问题，相比标准乘积空间方法降低了维度，具有更大的步长范围。


<details>
  <summary>Details</summary>
Motivation: 现有算法在处理结构化复合单调包含问题时，通常使用乘积空间技术将原问题转化为两个极大单调算子的和，但这会增加维度。需要一种能直接处理多算子结构、降低维度、具有更大步长范围的统一算法框架。

Method: 提出一种原始-对偶分裂算法，能够处理包含多个集值算子、集值算子与有界线性算子的复合、以及可能非共轭的单值算子的结构化问题。该算法基于图结构设计，统一了多种当代算法，并为生成新算法提供了蓝图。

Result: 算法相比标准乘积空间方法降低了维度，能适应不同的共轭或Lipschitz常数以及不同的预解参数，提供了更大的允许步长范围。通过去中心化融合lasso问题的数值实验验证了实用性。

Conclusion: 该算法不仅统一了多种现有算法，还为生成具有图结构的新算法提供了蓝图，在处理结构化复合单调包含问题时具有维度降低和步长范围更大的优势，具有实际应用价值。

Abstract: In this paper, we propose a primal-dual splitting algorithm for a broad class of structured composite monotone inclusions that involve finitely many set-valued operators, compositions of set-valued operators with bounded linear operators, and single-valued operators possibly without cocoercivity. The proposed algorithm is not only a unification for several contemporary algorithms but also a blueprint to generate new algorithms with graph-based structures. Our approach reduces dimensionality compared with the standard product space technique, which typically reformulates the original problem as the sum of two maximally monotone operators in order to apply splitting methods. It accommodates different cocoercive or Lipschitz constants as well as different resolvent parameters, and yields a larger allowable step-size range than in product space reformulations. We demonstrate the practicality of the approach by a numerical experiment on the decentralized fused lasso problem.

</details>


### [122] [Objective Coefficient Rounding and Almost Symmetries in Binary Programs](https://arxiv.org/abs/2512.10507)
*Dominik Kuzinowicz,Paweł Lichocki,Gioni Mexi,Marc E. Pfetsch,Sebastian Pokutta,Max Zimmer*

Main category: math.OC

TL;DR: 研究二进制规划中目标系数舍入与近似对称性的关系，通过减少有效位数使实例更易求解，同时保证解的质量


<details>
  <summary>Details</summary>
Motivation: 经验上，通过舍入减少目标系数的有效位数通常会使实例更容易求解，这可能是因为对称性增加，或者原始实例存在"近似对称性"。同时，求解舍入后的问题可以为原始目标值提供近似解。

Method: 在容量设施选址问题、背包问题和其他多样化实例上，使用SCIP和CP-SAT求解器进行实证研究，分析舍入对求解时间和解质量的影响

Result: 对于所有研究的问题类别，该方法都产生了更快的算法并保证了解的质量。对称性的影响取决于实例类型和求解器

Conclusion: 目标系数舍入通过增加对称性使二进制规划实例更容易求解，同时提供有质量保证的近似解，这种方法在不同问题类型和求解器上都有效

Abstract: This article investigates the interplay of rounding objective coefficients in binary programs and almost symmetries. Empirically, reducing the number of significant bits through rounding often leads to instances that are easier to solve. One reason can be that the amount of symmetries increases, which enables solvers to be more effective when they are exploited. This can signify that the original instance contains 'almost symmetries'. Furthermore, solving the rounded problems provides approximations to the original objective values. We empirically investigate these relations on instances of the capacitated facility location problem, the knapsack problem and a diverse collection of additional instances, using the solvers SCIP and CP-SAT. For all investigated problem classes, we show empirically that this yields faster algorithms with guaranteed solution quality. The influence of symmetry depends on the instance type and solver.

</details>


### [123] [Linear Quadratic Regulators: A New Look](https://arxiv.org/abs/2512.10641)
*CéEdric Join,Emmanuel Delaveau,Michel Fliess*

Main category: math.OC

TL;DR: 论文提出了一种基于代数系统理论的线性时不变控制系统分析方法，将Kalman可控性转化为系统模的自由性，结合线性二次调节器和无模型控制实现闭环控制。


<details>
  <summary>Details</summary>
Motivation: 传统线性时不变控制系统分析中，Kalman可控性条件与二次调节器的结合需要更系统化的数学框架。论文旨在通过代数方法（将系统视为微分算子环上的模）统一处理可控性、最优控制和闭环控制问题。

Method: 1. 将线性时不变系统表示为实数域上微分算子环R[d/dt]上的有限生成模；2. 将Kalman可控性条件转化为系统模的自由性；3. 利用平坦输出（自由模的基）通过Euler-Lagrange方程推导开环控制策略；4. 处理两点边值问题以获得最优时间范围、参数设计和轨迹；5. 结合无模型控制实现闭环控制。

Result: 建立了线性时不变控制系统在代数框架下的统一分析方法，将可控性、最优控制和闭环控制有机结合，提供了处理两点边值问题、最优时间范围和轨迹设计的新途径。

Conclusion: 代数方法为线性时不变控制系统提供了强有力的分析框架，将Kalman可控性、线性二次调节器和无模型控制统一起来，为解决最优控制问题提供了系统化的数学工具，同时保持了良好的鲁棒性。

Abstract: Linear time-invariant control systems can be considered as finitely generated modules over the commutative principal ideal ring $\mathbb{R}[\frac{d}{dt}]$ of linear differential operators with respect to the time derivative. The Kalman controllability in this algebraic language is translated as the freeness of the system module. Linear quadratic regulators rely on quadratic Lagrangians, or cost functions. Any flat output, i.e., any basis of the corresponding free module leads to an open-loop control strategy via an Euler-Lagrange equation, which becomes here a linear ordinary differential equation with constant coefficients. In this approach, the two-point boundary value problem, including the control variables, becomes tractable. It yields notions of optimal time horizon, optimal parameter design and optimal rest-to-rest trajectories. The loop is closed via an intelligent controller derived from model-free control, which is known to exhibit excellent performance concerning model mismatches and disturbances.

</details>


### [124] [Strong Global Convergence of the Consensus-Based Optimization Algorithm](https://arxiv.org/abs/2512.10654)
*Sabrina Bonandin,Konstantin Riedl,Sara Veneruso*

Main category: math.OC

TL;DR: 本文证明了实践数值时间离散共识优化算法在丰富目标函数类上强均方收敛到全局最小点，给出了超参数选择条件和收敛速率


<details>
  <summary>Details</summary>
Motivation: 共识优化是一种多智能体元启发式无导数优化算法，已在多种应用中证明能全局最小化非凸非光滑函数，但需要理论分析其数值离散算法的收敛性

Method: 将时间离散算法解释为连续时间水平的随机微分方程系统，结合传统有限时间收敛理论和处理非全局Lipschitz条件的Sznitman论证推广，通过精细矩估计控制小概率事件

Result: 证明了各向同性和各向异性扩散的CBO算法强均方收敛到全局最小点，给出了时间离散步长Δt和粒子数N的显式收敛速率，以及超参数选择条件

Conclusion: 为实践数值CBO算法提供了严格收敛理论保证，扩展了算法适用范围，为参数选择提供理论指导

Abstract: Consensus-based optimization (CBO) is a multi-agent metaheuristic derivative-free optimization algorithm that has proven to be capable of globally minimizing nonconvex nonsmooth functions across a diverse range of applications while being amenable to theoretical analysis. The method leverages an interplay between exploration of the energy landscape of the objective function through a system of interacting particles subject to stochasticity and exploitation of the particles' positions through the computation of a global consensus about the location of the minimizer based on the Laplace principle. In this paper, we prove strong mean square convergence of the practical numerical time-discrete CBO algorithm to the global minimizer for a rich class of objective functions. For CBO with both isotropic and anisotropic diffusion, our convergence result features conditions on the choice of the hyperparameters as well as explicit rates of convergence in the time discretization step size $Δt$ and the number of particles $N$. By interpreting the time-discrete algorithm at the continuous-time level through a system of stochastic differential equations (SDEs), our proof strategy combines traditional finite-time convergence theory for numerical methods applied to SDEs with careful considerations due to the fact that the CBO coefficients do not satisfy a global Lipschitz condition. To accomodate the latter, we adopt a recently proposed generalization of Sznitman's classical argument, which allows to discard an event of small probability, controllable through fine moment estimates for the particle systems.

</details>


### [125] [On the Convergence Analysis of an Inexact Preconditioned Stochastic Model-Based Algorithm](https://arxiv.org/abs/2512.10826)
*Chenglong Bao,Yancheng Yuan,Shulan Zhu*

Main category: math.OC

TL;DR: 提出了一种结合预条件技术的随机模型优化算法，用于求解复合优化问题，建立了收敛性保证和收敛速率分析。


<details>
  <summary>Details</summary>
Motivation: 针对随机复合优化问题，现有方法通常需要目标函数光滑或全局Lipschitz连续等较强假设。本文旨在开发更通用的算法框架，放宽这些限制条件。

Method: 提出了一种预条件的不精确随机模型优化算法，统一并扩展了固定度量的随机模型算法。算法结合预条件技术处理复合优化问题，允许不精确计算。

Result: 在弱凸和凸设置下建立了收敛性保证，无需目标函数光滑或全局Lipschitz连续。在局部Lipschitz条件下，推导了Moreau包络梯度的非渐近和渐近收敛速率。在二次增长条件下，获得了到最优解集距离的收敛速率。

Conclusion: 该算法框架具有较好的理论保证和实际性能，数值实验验证了理论结果，为随机复合优化问题提供了有效的解决方案。

Abstract: This paper focuses on investigating an inexact stochastic model-based optimization algorithm that integrates preconditioning techniques for solving stochastic composite optimization problems. The proposed framework unifies and extends the fixed-metric stochastic model-based algorithm to its preconditioned and inexact variants. Convergence guarantees are established under mild assumptions for both weakly convex and convex settings, without requiring smoothness or global Lipschitz continuity of the objective function. By assuming a local Lipschitz condition, we derive nonasymptotic and asymptotic convergence rates measured by the gradient of the Moreau envelope. Furthermore, convergence rates in terms of the distance to the optimal solution set are obtained under an additional quadratic growth condition on the objective function. Numerical experiment results demonstrate the theoretical findings for the proposed algorithm.

</details>


### [126] [Indirect methods in optimal control on Banach spaces](https://arxiv.org/abs/2512.10831)
*Roman Chertovskih,Nikolay Pogodaev,Maxim Staritsyn,A. Pedro Aguiar*

Main category: math.OC

TL;DR: 本文提出了一种基于精确成本增量公式和终端成本有限差分探针的间接下降方法，用于解决Banach空间中非线性常微分方程控制的最优控制问题，相比经典Pontryagin极大值原理方法具有更好的单调收敛性。


<details>
  <summary>Details</summary>
Motivation: 经典基于Pontryagin极大值原理的间接下降方法对局部凸性敏感且缺乏单调收敛性，需要开发更稳定、收敛性更好的替代方法。

Method: 提出基于精确成本增量公式和终端成本有限差分探针的间接下降方法，用于Banach空间中非线性常微分方程控制的最优控制问题。

Result: 新方法在Amari型神经场控制问题的数值分析中表现出稳定的单调收敛性。

Conclusion: 相比经典Pontryagin极大值原理方法，提出的基于精确成本增量公式的方法具有更好的稳定性和单调收敛性，适用于分布式动力学的最优控制问题。

Abstract: This work focuses on indirect descent methods for optimal control problems governed by nonlinear ordinary differential equations in Banach spaces, viewed as abstract models of distributed dynamics. As a reference line, we revisit the classical schemes, rooted in Pontryagin's maximum principle, and highlight their sensitivity to local convexity and lack of monotone convergence. We then develop an alternative method based on exact cost-increment formulas and finite-difference probes of the terminal cost. We show that our method exhibits stable monotone convergence in numerical analysis of an Amari-type neural field control problem.

</details>


### [127] [Spectral Decompositions of Controllability Gramian and Its Inverse based on System Eigenvalues in Companion Form](https://arxiv.org/abs/2512.10851)
*Alexey Iskakov,Igor Yadykin*

Main category: math.OC

TL;DR: 该论文提出了基于系统特征值的可控性格拉姆矩阵及其逆矩阵的谱分解方法，用于连续LTI系统在可控规范（伴随）形式下的分析。


<details>
  <summary>Details</summary>
Motivation: 可控性和可观测性格拉姆矩阵及其逆矩阵在控制理论中广泛应用，但缺乏基于系统特征值的谱分解方法，无法定量分析各特征模态对系统性质的影响。

Method: 针对连续LTI系统在可控规范形式，提出可控性格拉姆矩阵及其逆矩阵的谱分解，表示为对应单个特征值或特征值对组合的埃尔米特矩阵之和。该方法适用于代数/微分Lyapunov和Riccati方程的解，可处理任意初始条件和多重特征值情况。

Result: 获得了可控性格拉姆矩阵及其逆矩阵的谱分解表达式，能够定量表征各特征模态对可控性、可观测性和扰动能量渐近动力学的影响，为最小能量控制问题提供了可测量的物理量解释。

Conclusion: 提出的谱分解方法提供了系统谱特性的定量分析工具，能够提高稳定性分析、最小能量控制、结构设计、调节器调谐、执行器/传感器优化布置、网络分析和模型降阶等实际问题的算法精度。

Abstract: Controllability and observability Gramians, along with their inverses, are widely used to solve various problems in control theory. This paper proposes spectral decompositions of the controllability Gramian and its inverse based on system eigenvalues for a continuous LTI dynamical system in the controllability canonical (companion) form. The Gramian and its inverse are represented as sums of Hermitian matrices, each corresponding to individual system eigenvalues or their pairwise combinations. These decompositions are obtained for the solutions of both algebraic and differential Lyapunov and Riccati equations with arbitrary initial conditions, allowing for the estimation of system spectral properties over an arbitrary time interval and their prediction at future moments. The derived decompositions are also generalized to the case of multiple eigenvalues in the dynamics matrix spectrum, enabling a closed-form estimation of the effects of resonant interactions with the system's eigenmodes. The spectral components are interpreted as measurable quantities in the minimum energy control problem. Therefore, they are unambiguously defined and can quantitatively characterize the influence of individual eigenmodes and associated system devices on controllability, observability, and the asymptotic dynamics of perturbation energy. The additional information obtained from these decompositions can improve the accuracy of algorithms in solving various practical problems, such as stability analysis, minimum energy control, structural design, tuning regulators, optimal placement of actuators and sensors, network analysis, and model order reduction.

</details>


### [128] [Distributionally Robust Regret Optimal Control Under Moment-Based Ambiguity Sets](https://arxiv.org/abs/2512.10906)
*Feras Al Taha,Eilyan Bitar*

Main category: math.OC

TL;DR: 该论文研究有限时域线性二次随机控制问题，在噪声分布未知但属于均值协方差模糊集的情况下，设计因果仿射控制策略以最小化最坏情况期望遗憾，提出可扩展的对偶投影次梯度方法求解。


<details>
  <summary>Details</summary>
Motivation: 现实控制问题中噪声分布往往未知，传统方法假设精确分布不切实际。需要设计对分布模糊具有鲁棒性的控制策略，同时避免过度保守，在计算效率与鲁棒性之间取得平衡。

Method: 将最坏情况期望遗憾最小化问题转化为可处理的凸规划问题，对应名义线性二次随机控制问题的正则化版本。提出对偶投影次梯度方法替代传统半定规划求解，提高计算可扩展性。

Result: 理论证明最坏情况控制问题可等价转化为凸规划问题。数值实验表明所提方法在计算效率上优于现有数据驱动和分布鲁棒控制方法，能处理更大规模问题。

Conclusion: 该研究为分布模糊下的线性二次控制问题提供了理论框架和高效算法，平衡了鲁棒性与计算复杂度，对实际工程应用具有重要价值。

Abstract: In this paper, we consider a class of finite-horizon, linear-quadratic stochastic control problems, where the probability distribution governing the noise process is unknown but assumed to belong to an ambiguity set consisting of all distributions whose mean and covariance lie within norm balls centered at given nominal values. To address the distributional ambiguity, we explore the design of causal affine control policies to minimize the worst-case expected regret over all distributions in the given ambiguity set. The resulting minimax optimal control problem is shown to admit an equivalent reformulation as a tractable convex program that corresponds to a regularized version of the nominal linear-quadratic stochastic control problem. While this convex program can be recast as a semidefinite program, semidefinite programs are typically solved using primal-dual interior point methods that scale poorly with the problem size in practice. To address this limitation, we propose a scalable dual projected subgradient method to compute optimal controllers to an arbitrary accuracy. Numerical experiments are presented to benchmark the proposed method against state-of-the-art data-driven and distributionally robust control design approaches.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [129] [HGC-Herd: Efficient Heterogeneous Graph Condensation via Representative Node Herding](https://arxiv.org/abs/2512.09947)
*Fuyan Ou,Siqi Ai,Yulin Hu*

Main category: cs.LG

TL;DR: HGC-Herd：一种免训练的异构图压缩框架，通过轻量级特征传播和类内聚类机制生成紧凑且信息丰富的子图，在保持语义和结构保真度的同时显著降低计算和内存开销。


<details>
  <summary>Details</summary>
Motivation: 异构图神经网络（HGNNs）在处理大规模图时面临结构冗余和高维节点特征带来的可扩展性挑战。现有的图压缩方法（如GCond）主要针对同构图设计，依赖梯度匹配，导致计算、内存和优化开销较大。

Method: 提出HGC-Herd框架：1）集成轻量级特征传播编码多跳关系上下文；2）采用类内聚类机制识别每类代表性节点；3）生成平衡且具有判别性的子集用于下游学习任务。

Result: 在ACM、DBLP和Freebase数据集上的实验表明，HGC-Herd在保持与全图训练相当或更优准确率的同时，显著降低了运行时间和内存消耗。

Conclusion: HGC-Herd为高效、可扩展的异构图表示学习提供了实用价值，通过免训练压缩框架解决了HGNNs在大规模图上的可扩展性问题。

Abstract: Heterogeneous graph neural networks (HGNNs) have demonstrated strong capability in modeling complex semantics across multi-type nodes and relations. However, their scalability to large-scale graphs remains challenging due to structural redundancy and high-dimensional node features. Existing graph condensation approaches, such as GCond, are primarily developed for homogeneous graphs and rely on gradient matching, resulting in considerable computational, memory, and optimization overhead. We propose HGC-Herd, a training-free condensation framework that generates compact yet informative heterogeneous graphs while maintaining both semantic and structural fidelity. HGC-Herd integrates lightweight feature propagation to encode multi-hop relational context and employs a class-wise herding mechanism to identify representative nodes per class, producing balanced and discriminative subsets for downstream learning tasks. Extensive experiments on ACM, DBLP, and Freebase validate that HGC-Herd attains comparable or superior accuracy to full-graph training while markedly reducing both runtime and memory consumption. These results underscore its practical value for efficient and scalable heterogeneous graph representation learning.

</details>


### [130] [BAMBO: Construct Ability and Efficiency LLM Pareto Set via Bayesian Adaptive Multi-objective Block-wise Optimization](https://arxiv.org/abs/2512.09972)
*Kesheng Chen,Wenjian Luo,Zhenqian Zhu,Yamin Hu,Yiya Xi*

Main category: cs.LG

TL;DR: BAMBO是一个贝叶斯自适应多目标块级优化框架，用于自动构建LLM的帕累托前沿，通过混合最优块划分策略解决维度灾难问题。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型能力-效率权衡方法存在不足：粗粒度的模型级方法只能产生稀疏的次优解集，而细粒度的层级方法面临"维度灾难"，搜索空间计算不可行。

Method: 提出BAMBO框架，采用混合最优块划分策略，将问题建模为一维聚类问题，使用动态规划平衡块内同质性和块间信息分布，在进化循环中使用qEHVI采集函数自动搜索。

Result: 实验表明BAMBO能够发现比基线方法更优、更全面的帕累托前沿，能够根据不同操作约束进行敏捷的模型选择。

Conclusion: BAMBO通过创新的块划分策略解决了现有方法在构建LLM帕累托集时的局限性，实现了高效的多目标优化，代码已开源。

Abstract: Constructing a Pareto set is pivotal for navigating the capability-efficiency trade-offs in Large Language Models (LLMs); however, existing merging techniques remain inadequate for this task. Coarse-grained, model-level methods yield only a sparse set of suboptimal solutions, while fine-grained, layer-wise approaches suffer from the "curse of dimensionality," rendering the search space computationally intractable. To resolve this dichotomy, we propose BAMBO (Bayesian Adaptive Multi-objective Block-wise Optimization), a novel framework that automatically constructs the LLM Pareto set. BAMBO renders the search tractable by introducing a Hybrid Optimal Block Partitioning strategy. Formulated as a 1D clustering problem, this strategy leverages a dynamic programming approach to optimally balance intra-block homogeneity and inter-block information distribution, thereby dramatically reducing dimensionality without sacrificing critical granularity. The entire process is automated within an evolutionary loop driven by the q-Expected Hypervolume Improvement (qEHVI) acquisition function. Experiments demonstrate that BAMBO discovers a superior and more comprehensive Pareto frontier than baselines, enabling agile model selection tailored to diverse operational constraints. Code is available at: https://github.com/xin8coder/BAMBO.

</details>


### [131] [Latent Action World Models for Control with Unlabeled Trajectories](https://arxiv.org/abs/2512.10016)
*Marvin Alles,Xingyuan Zhang,Patrick van der Smagt,Philip Becker-Ehmck*

Main category: cs.LG

TL;DR: 提出潜在动作世界模型，通过联合使用动作条件数据和无动作数据（如视频），在少量动作标签样本下实现高效的世界模型学习


<details>
  <summary>Details</summary>
Motivation: 传统世界模型依赖动作条件轨迹，当动作标签稀缺时效果受限。人类能够结合直接交互和无动作经验（如观看视频），因此需要能够从异构数据中学习的世界模型

Method: 引入潜在动作世界模型家族，学习共享的潜在动作表示，将观察到的控制信号与从被动观察中推断的动作对齐。使用单个动态模型在大规模无标签轨迹上训练，仅需少量动作标签样本。通过离线强化学习学习潜在动作策略

Result: 在DeepMind Control Suite上，该方法在使用比纯动作条件基线少约一个数量级的动作标签样本情况下，实现了强大的性能表现

Conclusion: 潜在动作使世界模型能够在被动和交互数据上训练，提高了学习效率，弥合了离线强化学习（依赖动作条件数据）和无动作训练（很少用于后续强化学习）两个传统分离领域

Abstract: Inspired by how humans combine direct interaction with action-free experience (e.g., videos), we study world models that learn from heterogeneous data. Standard world models typically rely on action-conditioned trajectories, which limits effectiveness when action labels are scarce. We introduce a family of latent-action world models that jointly use action-conditioned and action-free data by learning a shared latent action representation. This latent space aligns observed control signals with actions inferred from passive observations, enabling a single dynamics model to train on large-scale unlabeled trajectories while requiring only a small set of action-labeled ones. We use the latent-action world model to learn a latent-action policy through offline reinforcement learning (RL), thereby bridging two traditionally separate domains: offline RL, which typically relies on action-conditioned data, and action-free training, which is rarely used with subsequent RL. On the DeepMind Control Suite, our approach achieves strong performance while using about an order of magnitude fewer action-labeled samples than purely action-conditioned baselines. These results show that latent actions enable training on both passive and interactive data, which makes world models learn more efficiently.

</details>


### [132] [Cluster-Dags as Powerful Background Knowledge For Causal Discovery](https://arxiv.org/abs/2512.10032)
*Jan Marco Ruiz de Vargas,Kirtan Padh,Niki Kilbertus*

Main category: cs.LG

TL;DR: 提出基于Cluster-DAG先验知识框架的因果发现方法，通过Cluster-PC和Cluster-FCI算法在完全和部分观测设置中提升性能


<details>
  <summary>Details</summary>
Motivation: 当前因果发现方法在处理高维数据和复杂依赖关系时面临挑战，而融入系统先验知识可以辅助因果发现。现有基于分层背景知识的方法灵活性不足，需要更灵活的先验知识框架

Method: 提出Cluster-DAG作为先验知识框架，并基于此开发两种改进的基于约束的算法：Cluster-PC用于完全观测设置，Cluster-FCI用于部分观测设置

Result: 在模拟数据上的实证评估表明，Cluster-PC和Cluster-FCI算法在各自设置下均优于没有先验知识的基线方法

Conclusion: Cluster-DAG框架提供了比现有分层背景知识方法更灵活的先验知识表示，能够有效提升因果发现算法的性能，特别是在高维和复杂依赖场景下

Abstract: Finding cause-effect relationships is of key importance in science. Causal discovery aims to recover a graph from data that succinctly describes these cause-effect relationships. However, current methods face several challenges, especially when dealing with high-dimensional data and complex dependencies. Incorporating prior knowledge about the system can aid causal discovery. In this work, we leverage Cluster-DAGs as a prior knowledge framework to warm-start causal discovery. We show that Cluster-DAGs offer greater flexibility than existing approaches based on tiered background knowledge and introduce two modified constraint-based algorithms, Cluster-PC and Cluster-FCI, for causal discovery in the fully and partially observed setting, respectively. Empirical evaluation on simulated data demonstrates that Cluster-PC and Cluster-FCI outperform their respective baselines without prior knowledge.

</details>


### [133] [Robust Gradient Descent via Heavy-Ball Momentum with Predictive Extrapolation](https://arxiv.org/abs/2512.10033)
*Sarwan Ali*

Main category: cs.LG

TL;DR: HB-SGE是一种结合重球动量和梯度外推的鲁棒一阶优化方法，在病态和非凸问题上比NAG和标准动量方法更稳定。


<details>
  <summary>Details</summary>
Motivation: 传统加速梯度方法（如NAG）在病态或非凸问题上容易发散，因为动量会累积历史梯度导致不稳定。需要一种既能加速又保持稳定的优化方法。

Method: HB-SGE结合重球动量和预测梯度外推，使用局部泰勒近似估计未来梯度方向，而不是累积历史梯度。这种方法提供自适应加速同时保持稳定性。

Result: 在病态二次问题（条件数κ=50）上，HB-SGE在119次迭代收敛，而SGD和NAG都发散。在非凸Rosenbrock函数上，HB-SGE在2718次迭代收敛，而经典动量方法在10步内发散。

Conclusion: HB-SGE提供了一种鲁棒的优化替代方案，在病态和非凸问题上比NAG和标准动量更稳定，虽然NAG在良态问题上更快，但HB-SGE在各种地形上都比SGD快，且内存开销仅为O(d)。

Abstract: Accelerated gradient methods like Nesterov's Accelerated Gradient (NAG) achieve faster convergence on well-conditioned problems but often diverge on ill-conditioned or non-convex landscapes due to aggressive momentum accumulation. We propose Heavy-Ball Synthetic Gradient Extrapolation (HB-SGE), a robust first-order method that combines heavy-ball momentum with predictive gradient extrapolation. Unlike classical momentum methods that accumulate historical gradients, HB-SGE estimates future gradient directions using local Taylor approximations, providing adaptive acceleration while maintaining stability. We prove convergence guarantees for strongly convex functions and demonstrate empirically that HB-SGE prevents divergence on problems where NAG and standard momentum fail. On ill-conditioned quadratics (condition number $κ=50$), HB-SGE converges in 119 iterations while both SGD and NAG diverge. On the non-convex Rosenbrock function, HB-SGE achieves convergence in 2,718 iterations where classical momentum methods diverge within 10 steps. While NAG remains faster on well-conditioned problems, HB-SGE provides a robust alternative with speedup over SGD across diverse landscapes, requiring only $O(d)$ memory overhead and the same hyperparameters as standard momentum.

</details>


### [134] [Intelligently Weighting Multiple Reference Models for Direct Preference Optimization of LLMs](https://arxiv.org/abs/2512.10040)
*Skyler Wu,Aymen Echarghaoui*

Main category: cs.LG

TL;DR: 本文提出四种新的参考权重设置策略来改进多参考偏好优化(MRPO)，但实验发现单参考DPO通常优于多参考方法。


<details>
  <summary>Details</summary>
Motivation: 当前MRPO方法中设置参考权重的策略是临时的且统计上不可靠，导致性能不稳定，需要更系统的方法来利用多个参考模型的集体优势。

Method: 提出了四种新的权重设置策略：两种离线方法利用验证集信号；一种在线方法使用滑动窗口估计器减少过拟合；另一种在线方法将参考权重设置视为K臂老虎机问题，采用Thompson Sampling。

Result: 所有四种新策略在UltraFeedback和SafeRLHF数据集上的偏好准确率都优于现有MRPO权重方法。但更令人深思的是，使用7个参考模型中任意6个的单参考DPO方法，都持续优于所有测试的多参考方法。

Conclusion: 虽然提出的新权重策略改进了MRPO，但单参考DPO的优越表现对多参考方法的实际吸引力提出了质疑，需要重新评估多参考方法的实用价值。

Abstract: Fine-tuning is integral for aligning large language models (LLMs) with human preferences. Multiple-Reference Preference Optimization (MRPO) builds on Direct Preference Optimization (DPO) by fine-tuning LLMs on preference datasets while regularizing the policy towards a mixture of reference models to leverage their collective desirable properties. However, current methods for setting the reference weights are ad-hoc and statistically unsound, leading to unreliable performance. To address this, we introduce four new weighting strategies: two offline methods that leverage held-out validation signal; one online method that uses a sliding-window estimator to reduce overfitting; and an online method that treats reference weighting as a $K$-armed bandit via Thompson Sampling. Experiments using Qwen2.5-0.5B as the policy model and seven reference models from the Llama, Mistral, Qwen, Yi, and Phi families (0.5B-14B each) show that all 4 of our strategies outperform the current MRPO weighting methods on UltraFeedback and SafeRLHF in preference accuracy. More thought-provokingly, however, we find that single-reference DPO, using any of 6 out of 7 references, consistently outperforms all tested multiple-reference approaches -- calling into question the practical appeal of multiple-reference approaches.

</details>


### [135] [SEMDICE: Off-policy State Entropy Maximization via Stationary Distribution Correction Estimation](https://arxiv.org/abs/2512.10042)
*Jongmin Lee,Meiqi Sun,Pieter Abbeel*

Main category: cs.LG

TL;DR: SEMDICE：一种从任意离策略数据集中计算状态熵最大化策略的无监督强化学习预训练方法


<details>
  <summary>Details</summary>
Motivation: 在无监督强化学习预训练中，智能体需要在没有任务特定奖励函数的情况下学习下游任务的先验策略。状态熵最大化（SEM）旨在学习最大化状态平稳分布熵的策略，但现有方法在从离策略数据集中有效计算SEM策略方面存在局限。

Method: 提出SEMDICE算法，一种原则性的离策略方法，直接从任意离策略数据集中计算状态熵最大化策略。该方法在平稳分布空间中直接优化策略，计算单一、平稳的马尔可夫状态熵最大化策略。

Result: 实验结果表明，SEMDICE在最大化状态熵方面优于基线算法，同时在基于SEM的无监督RL预训练方法中实现了最佳的下游任务适应效率。

Conclusion: SEMDICE提供了一种有效的无监督预训练方法，能够从任意离策略数据集中学习状态熵最大化策略，为下游任务提供良好的先验策略，具有优越的适应效率。

Abstract: In the unsupervised pre-training for reinforcement learning, the agent aims to learn a prior policy for downstream tasks without relying on task-specific reward functions. We focus on state entropy maximization (SEM), where the goal is to learn a policy that maximizes the entropy of the state stationary distribution. In this paper, we introduce SEMDICE, a principled off-policy algorithm that computes an SEM policy from an arbitrary off-policy dataset, which optimizes the policy directly within the space of stationary distributions. SEMDICE computes a single, stationary Markov state-entropy-maximizing policy from an arbitrary off-policy dataset. Experimental results demonstrate that SEMDICE outperforms baseline algorithms in maximizing state entropy while achieving the best adaptation efficiency for downstream tasks among SEM-based unsupervised RL pre-training methods.

</details>


### [136] [Local LLM Ensembles for Zero-shot Portuguese Named Entity Recognition](https://arxiv.org/abs/2512.10043)
*João Lucas Luz Lima Sarcinelli,Diego Furtado Silva*

Main category: cs.LG

TL;DR: 提出一种新颖的三步集成方法，用于零样本命名实体识别，通过组合多个本地运行的小型LLM，在葡萄牙语NER任务上超越单个模型性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自然语言处理任务中表现出色，但在命名实体识别（尤其是葡萄牙语等低资源语言）上表现不佳。虽然开源模型支持本地部署，但没有单一模型在所有任务上占优，因此需要集成方法。现有LLM集成主要关注文本生成或分类，NER领域研究不足。

Method: 提出三步集成流程：1）使用启发式方法选择最优模型组合；2）利用最少标注数据；3）在零样本设置下组合多个本地运行的小型LLM。该方法不需要微调，通过集成多个相似能力的小型LLM来提升NER性能。

Result: 在五个葡萄牙语NER数据集中，该方法在四个数据集上超越了单个LLM的性能。此外，在不同源数据集上获得的集成模型在跨数据集配置中通常优于单个LLM，可能消除对当前任务标注数据的需求。

Conclusion: 该工作通过有效组合多个小型LLM而不需要微调，推进了可扩展、低资源和零样本的命名实体识别。提出的集成方法为低资源语言NER提供了实用解决方案，代码已开源。

Abstract: Large Language Models (LLMs) excel in many Natural Language Processing (NLP) tasks through in-context learning but often under-perform in Named Entity Recognition (NER), especially for lower-resource languages like Portuguese. While open-weight LLMs enable local deployment, no single model dominates all tasks, motivating ensemble approaches. However, existing LLM ensembles focus on text generation or classification, leaving NER under-explored. In this context, this work proposes a novel three-step ensemble pipeline for zero-shot NER using similarly capable, locally run LLMs. Our method outperforms individual LLMs in four out of five Portuguese NER datasets by leveraging a heuristic to select optimal model combinations with minimal annotated data. Moreover, we show that ensembles obtained on different source datasets generally outperform individual LLMs in cross-dataset configurations, potentially eliminating the need for annotated data for the current task. Our work advances scalable, low-resource, and zero-shot NER by effectively combining multiple small LLMs without fine-tuning. Code is available at https://github.com/Joao-Luz/local-llm-ner-ensemble.

</details>


### [137] [Detailed balance in large language model-driven agents](https://arxiv.org/abs/2512.10047)
*Zhuo-Yang Song,Qing-Hong Cao,Ming-xing Luo,Hua Xing Zhu*

Main category: cs.LG

TL;DR: 该论文发现大语言模型驱动的智能体在状态转换中存在详细平衡，表明LLM生成可能不是通过学习规则集和策略，而是通过隐式学习一类潜在函数实现的，这可能是首个不依赖具体模型细节的LLM生成动力学宏观物理定律。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM驱动的智能体在解决复杂问题方面取得了经验性成功，但缺乏理解其宏观动力学的理论框架。需要建立统一的理论基础，将AI智能体研究从工程实践提升到可预测、可量化的科学层面。

Method: 基于最小作用原理，通过实验测量LLM生成状态之间的转移概率，统计分析LLM生成转换中的详细平衡现象。

Result: 统计发现了LLM生成转换中的详细平衡，表明LLM生成可能不是通过学习规则集和策略，而是通过隐式学习一类潜在函数实现的，这类函数可能超越不同的LLM架构和提示模板。

Conclusion: 这是首个不依赖具体模型细节的LLM生成动力学宏观物理定律的发现，为建立复杂AI系统的宏观动力学理论迈出了重要一步，旨在将AI智能体研究从工程实践提升到基于有效测量的科学层面。

Abstract: Large language model (LLM)-driven agents are emerging as a powerful new paradigm for solving complex problems. Despite the empirical success of these practices, a theoretical framework to understand and unify their macroscopic dynamics remains lacking. This Letter proposes a method based on the least action principle to estimate the underlying generative directionality of LLMs embedded within agents. By experimentally measuring the transition probabilities between LLM-generated states, we statistically discover a detailed balance in LLM-generated transitions, indicating that LLM generation may not be achieved by generally learning rule sets and strategies, but rather by implicitly learning a class of underlying potential functions that may transcend different LLM architectures and prompt templates. To our knowledge, this is the first discovery of a macroscopic physical law in LLM generative dynamics that does not depend on specific model details. This work is an attempt to establish a macroscopic dynamics theory of complex AI systems, aiming to elevate the study of AI agents from a collection of engineering practices to a science built on effective measurements that are predictable and quantifiable.

</details>


### [138] [DB2-TransF: All You Need Is Learnable Daubechies Wavelets for Time Series Forecasting](https://arxiv.org/abs/2512.10051)
*Moulik Gupta,Achyut Mani Tripathi*

Main category: cs.LG

TL;DR: DB2-TransF是一种基于Transformer的改进架构，用可学习的Daubechies小波系数层替代自注意力机制，在时间序列预测任务中实现了与Transformer相当或更好的精度，同时大幅降低内存使用。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在处理时间序列预测时虽然能有效捕捉长期依赖关系，但其二次计算复杂度限制了可扩展性和适应性，特别是在大规模高维场景下。

Method: 提出DB2-TransF架构，用可学习的Daubechies小波系数层替代传统的自注意力机制，该小波模块能有效捕捉多尺度局部和全局模式，并增强多个时间序列间的相关性建模。

Result: 在13个标准预测基准测试中，DB2-TransF达到了与常规Transformer相当或更优的预测精度，同时显著减少了内存使用。

Conclusion: DB2-TransF为高级时间序列预测提供了一个可扩展且资源高效的框架，在保持预测性能的同时解决了Transformer的计算复杂度问题。

Abstract: Time series forecasting requires models that can efficiently capture complex temporal dependencies, especially in large-scale and high-dimensional settings. While Transformer-based architectures excel at modeling long-range dependencies, their quadratic computational complexity poses limitations on scalability and adaptability. To overcome these challenges, we introduce DB2-TransF, a novel Transformer-inspired architecture that replaces the self-attention mechanism with a learnable Daubechies wavelet coefficient layer. This wavelet-based module efficiently captures multi-scale local and global patterns and enhances the modeling of correlations across multiple time series for the time series forecasting task. Extensive experiments on 13 standard forecasting benchmarks demonstrate that DB2-TransF achieves comparable or superior predictive accuracy to conventional Transformers, while substantially reducing memory usage for the time series forecasting task. The obtained experimental results position DB2-TransF as a scalable and resource-efficient framework for advanced time series forecasting. Our code is available at https://github.com/SteadySurfdom/DB2-TransF

</details>


### [139] [Mitigating Exposure Bias in Risk-Aware Time Series Forecasting with Soft Tokens](https://arxiv.org/abs/2512.10056)
*Alireza Namazi,Amirreza Dolatpour Fathkouhi,Heman Shakeri*

Main category: cs.LG

TL;DR: SoTra方法通过软令牌传播连续概率分布来减少曝光偏差，实现校准的、不确定性感知的轨迹预测，结合风险感知解码最小化临床风险，在血糖和血压预测中分别降低18%和15%的风险。


<details>
  <summary>Details</summary>
Motivation: 自回归预测在糖尿病和血流动力学管理中至关重要，不同操作区域具有不同的临床风险。标准模型使用教师强制训练时存在曝光偏差，导致闭环使用中的多步预测不稳定。

Method: 提出软令牌轨迹预测(SoTra)方法，传播连续概率分布（软令牌）来减轻曝光偏差，学习校准的、不确定性感知的轨迹。结合风险感知解码模块最小化预期临床危害。

Result: 在血糖预测中，SoTra将平均区域风险降低18%；在血压预测中，将有效临床风险降低约15%。

Conclusion: 这些改进支持SoTra在安全关键的预测控制中的应用，为临床风险管理提供了有效的解决方案。

Abstract: Autoregressive forecasting is central to predictive control in diabetes and hemodynamic management, where different operating zones carry different clinical risks. Standard models trained with teacher forcing suffer from exposure bias, yielding unstable multi-step forecasts for closed-loop use. We introduce Soft-Token Trajectory Forecasting (SoTra), which propagates continuous probability distributions (``soft tokens'') to mitigate exposure bias and learn calibrated, uncertainty-aware trajectories. A risk-aware decoding module then minimizes expected clinical harm. In glucose forecasting, SoTra reduces average zone-based risk by 18\%; in blood-pressure forecasting, it lowers effective clinical risk by approximately 15\%. These improvements support its use in safety-critical predictive control.

</details>


### [140] [\textsc{Text2Graph}: Combining Lightweight LLMs and GNNs for Efficient Text Classification in Label-Scarce Scenarios](https://arxiv.org/abs/2512.10061)
*João Lucas Luz Lima Sarcinelli,Ricardo Marcondes Marcacini*

Main category: cs.LG

TL;DR: Text2Graph：开源Python包，结合LLM部分标注与GNN标签传播，实现高效零样本文本分类，显著降低能耗和碳排放


<details>
  <summary>Details</summary>
Motivation: LLMs作为零样本分类器虽然有效，但计算需求高、环境成本大，限制了在高性能计算环境中大规模标注的实用性。需要更可持续的工作流程。

Method: 开发Text2Graph开源框架，模块化实现文本到图分类方法，结合LLM部分标注与GNN标签传播，可灵活替换特征提取器、边构建方法和采样策略等组件。

Result: 在5个数据集（主题分类和情感分析）的零样本设置下进行基准测试，图传播方法在保持竞争力的同时，能耗和碳排放大幅降低。

Conclusion: Text2Graph框架通过结合LLM与GNN，实现了高效可持续的文本分类，为大规模标注提供了环保解决方案。

Abstract: Large Language Models (LLMs) have become effective zero-shot classifiers, but their high computational requirements and environmental costs limit their practicality for large-scale annotation in high-performance computing (HPC) environments. To support more sustainable workflows, we present \textsc{Text2Graph}, an open-source Python package that provides a modular implementation of existing text-to-graph classification approaches. The framework enables users to combine LLM-based partial annotation with Graph Neural Network (GNN) label propagation in a flexible manner, making it straightforward to swap components such as feature extractors, edge construction methods, and sampling strategies. We benchmark \textsc{Text2Graph} on a zero-shot setting using five datasets spanning topic classification and sentiment analysis tasks, comparing multiple variants against other zero-shot approaches for text classification. In addition to reporting performance, we provide detailed estimates of energy consumption and carbon emissions, showing that graph-based propagation achieves competitive results at a fraction of the energy and environmental cost.

</details>


### [141] [Hybrid Physics-ML Model for Forward Osmosis Flux with Complete Uncertainty Quantification](https://arxiv.org/abs/2512.10457)
*Shiv Ratn,Shivang Rampriyan,Bahni Ray*

Main category: cs.LG

TL;DR: 本研究提出了一种结合物理模型与机器学习的高斯过程回归混合框架，用于精确预测正向渗透过程中的水通量，并实现全面的不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 正向渗透是一种有前景的低能耗膜分离技术，但传统机理模型受经验参数变化影响，纯数据驱动模型缺乏物理一致性和不确定性量化能力，需要开发更可靠的预测方法。

Method: 采用鲁棒混合物理-机器学习框架，使用高斯过程回归训练物理模型预测值与实验水通量之间的残差误差，并通过Delta方法分解总预测方差为模型不确定性和输入不确定性。

Result: 仅用120个数据点训练，模型在独立测试数据上达到了0.26%的平均绝对百分比误差和0.999的R²，实现了最先进的预测性能。

Conclusion: 该框架为正向渗透过程优化和数字孪生开发提供了真正鲁棒可靠的代理模型，展示了物理与机器学习结合在低数据条件下的强大预测能力。

Abstract: Forward Osmosis (FO) is a promising low-energy membrane separation technology, but challenges in accurately modelling its water flux (Jw) persist due to complex internal mass transfer phenomena. Traditional mechanistic models struggle with empirical parameter variability, while purely data-driven models lack physical consistency and rigorous uncertainty quantification (UQ). This study introduces a novel Robust Hybrid Physics-ML framework employing Gaussian Process Regression (GPR) for highly accurate, uncertainty-aware Jw prediction. The core innovation lies in training the GPR on the residual error between the detailed, non-linear FO physical model prediction (Jw_physical) and the experimental water flux (Jw_actual). Crucially, we implement a full UQ methodology by decomposing the total predictive variance (sigma2_total) into model uncertainty (epistemic, from GPR's posterior variance) and input uncertainty (aleatoric, analytically propagated via the Delta method for multi-variate correlated inputs). Leveraging the inherent strength of GPR in low-data regimes, the model, trained on a meagre 120 data points, achieved a state-of-the-art Mean Absolute Percentage Error (MAPE) of 0.26% and an R2 of 0.999 on the independent test data, validating a truly robust and reliable surrogate model for advanced FO process optimization and digital twin development.

</details>


### [142] [MedXAI: A Retrieval-Augmented and Self-Verifying Framework for Knowledge-Guided Medical Image Analysis](https://arxiv.org/abs/2512.10098)
*Midhat Urooj,Ayan Banerjee,Farhat Shaikh,Kuntal Thakur,Sandeep Gupta*

Main category: cs.LG

TL;DR: MedXAI是一个结合深度学习与临床专家知识的可解释医学影像诊断框架，通过整合专家知识提升泛化能力、减少罕见类别偏见，并提供临床可理解的解释，在癫痫灶定位和糖尿病视网膜病变分级任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 医学AI面临领域偏移、罕见类别偏见和缺乏可解释性三大挑战。深度学习模型在真实世界分布变化下表现不佳，对罕见病理存在偏见，且缺乏临床部署所需的透明度。

Method: MedXAI框架整合深度学习视觉模型与临床专家知识，通过符号化组件作为临床先验和正则化器，定位相关诊断特征而非依赖技术性后处理方法（如显著性图、LIME）。

Result: 在10个多中心数据集上评估显示，跨领域泛化能力提升3%，罕见类别F1分数提升10%，显著优于强深度学习基线。消融实验证实符号组件作为有效临床先验和正则化器，提升分布偏移下的鲁棒性。

Conclusion: MedXAI在提供临床对齐解释的同时，实现了优异的域内和跨域性能，特别适用于多模态医学AI中的罕见疾病诊断，解决了医学AI部署的关键挑战。

Abstract: Accurate and interpretable image-based diagnosis remains a fundamental challenge in medical AI, particularly un- der domain shifts and rare-class conditions. Deep learning mod- els often struggle with real-world distribution changes, exhibit bias against infrequent pathologies, and lack the transparency required for deployment in safety-critical clinical environments. We introduce MedXAI (An Explainable Framework for Med- ical Imaging Classification), a unified expert knowledge based framework that integrates deep vision models with clinician- derived expert knowledge to improve generalization, reduce rare- class bias, and provide human-understandable explanations by localizing the relevant diagnostic features rather than relying on technical post-hoc methods (e.g., Saliency Maps, LIME). We evaluate MedXAI across heterogeneous modalities on two challenging tasks: (i) Seizure Onset Zone localization from resting-state fMRI, and (ii) Diabetic Retinopathy grading. Ex periments on ten multicenter datasets show consistent gains, including a 3% improvement in cross-domain generalization and a 10% improvmnet in F1 score of rare class, substantially outperforming strong deep learning baselines. Ablations confirm that the symbolic components act as effective clinical priors and regularizers, improving robustness under distribution shift. MedXAI delivers clinically aligned explanations while achieving superior in-domain and cross-domain performance, particularly for rare diseases in multimodal medical AI.

</details>


### [143] [Extrapolation of Periodic Functions Using Binary Encoding of Continuous Numerical Values](https://arxiv.org/abs/2512.10817)
*Brian P. Powell,Jordan A. Caraballo-Vega,Mark L. Carroll,Thomas Maxwell,Andrew Ptak,Greg Olmschenk,Jorge Martinez-Palomera*

Main category: cs.LG

TL;DR: 论文发现二进制编码使神经网络能够外推周期函数，提出NB2E编码方法，使普通MLP无需先验知识即可成功外推各种周期信号。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络在处理周期函数外推时存在困难，需要先验知识或特殊架构。本文旨在探索是否可以通过简单的输入编码使标准神经网络具备周期函数外推能力。

Method: 提出归一化二进制编码(NB2E)方法，将连续数值编码为二进制表示。使用普通多层感知机(MLP)作为基础架构，无需特殊设计或先验知识。

Result: 实验表明，使用NB2E编码的MLP能够成功外推多种周期信号。内部激活分析显示NB2E诱导了比特相位表示，使MLP能够独立于位置学习和外推信号结构。

Conclusion: 二进制编码为神经网络提供了有效的周期函数外推能力，NB2E方法使标准MLP无需先验知识即可学习和外推周期信号，揭示了输入编码对神经网络表达能力的重要影响。

Abstract: We report the discovery that binary encoding allows neural networks to extrapolate periodic functions beyond their training bounds. We introduce Normalized Base-2 Encoding (NB2E) as a method for encoding continuous numerical values and demonstrate that, using this input encoding, vanilla multi-layer perceptrons (MLP) successfully extrapolate diverse periodic signals without prior knowledge of their functional form. Internal activation analysis reveals that NB2E induces bit-phase representations, enabling MLPs to learn and extrapolate signal structure independently of position.

</details>


### [144] [CHyLL: Learning Continuous Neural Representations of Hybrid Systems](https://arxiv.org/abs/2512.10117)
*Sangli Teng,Hang Liu,Jingyu Song,Koushil Sreenath*

Main category: cs.LG

TL;DR: CHyLL提出了一种在潜在空间中学习混合系统连续神经表示的方法，无需轨迹分割、事件函数或模式切换，通过将状态空间重构为分段光滑商流形使流变得空间连续。


<details>
  <summary>Details</summary>
Motivation: 现有方法分别学习每个离散模式的动力学，但面临模式切换和流不连续性的组合挑战。混合系统同时具有连续和离散时间动力学，学习其流具有挑战性。

Method: CHyLL的核心洞察是重置映射在守卫表面粘合状态空间，将状态空间重构为分段光滑商流形，使流变得空间连续。基于微分拓扑的嵌入定理，CHyLL同时学习高维空间中的无奇点神经嵌入和其中的连续流。

Result: CHyLL能够准确预测混合系统的流，具有优越的准确性，并能识别混合系统的拓扑不变量。最后成功应用于随机最优控制问题。

Conclusion: CHyLL提供了一种无需轨迹分割或模式切换的连续混合系统学习方法，通过将状态空间重构为商流形，在潜在空间中学习连续表示，为混合系统学习提供了新框架。

Abstract: Learning the flows of hybrid systems that have both continuous and discrete time dynamics is challenging. The existing method learns the dynamics in each discrete mode, which suffers from the combination of mode switching and discontinuities in the flows. In this work, we propose CHyLL (Continuous Hybrid System Learning in Latent Space), which learns a continuous neural representation of a hybrid system without trajectory segmentation, event functions, or mode switching. The key insight of CHyLL is that the reset map glues the state space at the guard surface, reformulating the state space as a piecewise smooth quotient manifold where the flow becomes spatially continuous. Building upon these insights and the embedding theorems grounded in differential topology, CHyLL concurrently learns a singularity-free neural embedding in a higher-dimensional space and the continuous flow in it. We showcase that CHyLL can accurately predict the flow of hybrid systems with superior accuracy and identify the topological invariants of the hybrid systems. Finally, we apply CHyLL to the stochastic optimal control problem.

</details>


### [145] [Generative Modeling from Black-box Corruptions via Self-Consistent Stochastic Interpolants](https://arxiv.org/abs/2512.10857)
*Chirag Modi,Jiequn Han,Eric Vanden-Eijnden,Joan Bruna*

Main category: cs.LG

TL;DR: 提出自洽随机插值法(SCSI)，通过迭代更新噪声数据与干净数据之间的传输映射，仅使用噪声数据集和黑盒访问噪声通道，构建干净数据的生成模型。


<details>
  <summary>Details</summary>
Motivation: 在科学和工程领域，通常只能获得经过噪声、病态通道污染的测量数据，而非干净数据。需要解决分布层面的逆问题来构建原始数据的生成模型。

Method: 基于随机插值法，迭代更新噪声数据与干净数据样本之间的传输映射，仅使用噪声数据集和黑盒访问噪声通道。该方法收敛于自洽的传输映射，有效反转噪声通道。

Result: SCSI方法在自然图像处理和科学重建的逆问题上表现出优越性能，计算效率高，能处理任意非线性前向模型，且具有理论保证。

Conclusion: 自洽随机插值法为从噪声数据构建干净数据生成模型提供了高效、灵活且理论可靠的新方法，特别适用于科学和工程领域的逆问题。

Abstract: Transport-based methods have emerged as a leading paradigm for building generative models from large, clean datasets. However, in many scientific and engineering domains, clean data are often unavailable: instead, we only observe measurements corrupted through a noisy, ill-conditioned channel. A generative model for the original data thus requires solving an inverse problem at the level of distributions. In this work, we introduce a novel approach to this task based on Stochastic Interpolants: we iteratively update a transport map between corrupted and clean data samples using only access to the corrupted dataset as well as black box access to the corruption channel. Under appropriate conditions, this iterative procedure converges towards a self-consistent transport map that effectively inverts the corruption channel, thus enabling a generative model for the clean data. We refer to the resulting method as the self-consistent stochastic interpolant (SCSI). It (i) is computationally efficient compared to variational alternatives, (ii) highly flexible, handling arbitrary nonlinear forward models with only black-box access, and (iii) enjoys theoretical guarantees. We demonstrate superior performance on inverse problems in natural image processing and scientific reconstruction, and establish convergence guarantees of the scheme under appropriate assumptions.

</details>


### [146] [T-SKM-Net: Trainable Neural Network Framework for Linear Constraint Satisfaction via Sampling Kaczmarz-Motzkin Method](https://arxiv.org/abs/2512.10461)
*Haoyu Zhu,Yao Zhang,Jiashen Ren,Qingchun Hou*

Main category: cs.LG

TL;DR: 提出T-SKM-Net框架，首次系统地将SKM方法集成到神经网络约束满足中，通过空空间变换处理混合约束，提供理论保证，在电力系统优化中实现25倍加速且零约束违反。


<details>
  <summary>Details</summary>
Motivation: 神经网络约束满足在安全关键应用中至关重要，但现有方法面临效率与适用性的权衡。SKM方法具有良好的收敛性，但其argmax操作引入不可微性，难以在神经网络中应用。

Method: 提出T-SKM-Net框架：1) 通过空空间变换将混合约束问题转化为纯不等式问题；2) 使用SKM进行迭代求解；3) 将解映射回原始约束空间。提供后处理有效性和端到端可训练性的理论保证。

Result: 在DCOPF case118基准测试中：后处理模式4.27ms/项，最大最优性差距0.0025%；联合训练模式5.25ms/项，最大最优性差距0.0008%。相比pandapower求解器实现25倍加速，在给定容差下保持零约束违反。

Conclusion: T-SKM-Net框架成功解决了SKM方法在神经网络应用中的不可微性问题，为神经网络约束满足提供了高效且理论保证的解决方案，在安全关键应用中具有重要价值。

Abstract: Neural network constraint satisfaction is crucial for safety-critical applications such as power system optimization, robotic path planning, and autonomous driving. However, existing constraint satisfaction methods face efficiency-applicability trade-offs, with hard constraint methods suffering from either high computational complexity or restrictive assumptions on constraint structures. The Sampling Kaczmarz-Motzkin (SKM) method is a randomized iterative algorithm for solving large-scale linear inequality systems with favorable convergence properties, but its argmax operations introduce non-differentiability, posing challenges for neural network applications. This work proposes the Trainable Sampling Kaczmarz-Motzkin Network (T-SKM-Net) framework and, for the first time, systematically integrates SKM-type methods into neural network constraint satisfaction. The framework transforms mixed constraint problems into pure inequality problems through null space transformation, employs SKM for iterative solving, and maps solutions back to the original constraint space, efficiently handling both equality and inequality constraints. We provide theoretical proof of post-processing effectiveness in expectation and end-to-end trainability guarantees based on unbiased gradient estimators, demonstrating that despite non-differentiable operations, the framework supports standard backpropagation. On the DCOPF case118 benchmark, our method achieves 4.27ms/item GPU serial forward inference with 0.0025% max optimality gap with post-processing mode and 5.25ms/item with 0.0008% max optimality gap with joint training mode, delivering over 25$\times$ speedup compared to the pandapower solver while maintaining zero constraint violations under given tolerance.

</details>


### [147] [Partitioning the Sample Space for a More Precise Shannon Entropy Estimation](https://arxiv.org/abs/2512.10133)
*Gabriel F. A. Bastos,Jugurta Montalvão*

Main category: cs.LG

TL;DR: 提出一种基于分解性、缺失质量估计和未见结果数量估计的离散熵估计器，用于小数据集场景


<details>
  <summary>Details</summary>
Motivation: 从小数据集（样本数可能小于可能结果数）可靠估计香农熵在多个应用中至关重要，传统方法在欠采样情况下存在负偏差问题

Method: 利用分解性特性，结合缺失质量估计和未见结果数量估计来补偿负偏差

Result: 在欠采样情况下优于一些经典估计器，与一些成熟的最先进估计器性能相当

Conclusion: 提出的离散熵估计器在小数据集场景下能够有效补偿负偏差，提供更可靠的熵估计

Abstract: Reliable data-driven estimation of Shannon entropy from small data sets, where the number of examples is potentially smaller than the number of possible outcomes, is a critical matter in several applications. In this paper, we introduce a discrete entropy estimator, where we use the decomposability property in combination with estimations of the missing mass and the number of unseen outcomes to compensate for the negative bias induced by them. Experimental results show that the proposed method outperforms some classical estimators in undersampled regimes, and performs comparably with some well-established state-of-the-art estimators.

</details>


### [148] [Decoupled Q-Chunking](https://arxiv.org/abs/2512.10926)
*Qiyang Li,Seohong Park,Sergey Levine*

Main category: cs.LG

TL;DR: 提出一种新算法，通过解耦评论家与策略的块长度，让策略在较短动作块上操作，同时保留多步价值传播优势，解决块评论家方法中策略提取的挑战。


<details>
  <summary>Details</summary>
Motivation: 时间差分方法存在自引导偏差问题，块评论家通过估计动作序列价值加速价值备份，但从中提取策略面临挑战：策略必须开环输出整个动作块，这在需要策略反应性的环境中可能次优，且长动作块建模困难。

Method: 提出解耦评论家与策略块长度的算法，通过从原始块评论家乐观回溯构建部分动作块的蒸馏评论家，近似部分动作块扩展为完整块时可实现的最大价值，让策略在较短动作块上优化。

Result: 在具有挑战性的长时域离线目标条件任务上评估，该方法可靠地优于先前方法。

Conclusion: 该方法既保留了多步价值传播的优势，又避免了开环次优性和学习长动作块策略的困难，为块评论家方法提供了有效的策略提取解决方案。

Abstract: Temporal-difference (TD) methods learn state and action values efficiently by bootstrapping from their own future value predictions, but such a self-bootstrapping mechanism is prone to bootstrapping bias, where the errors in the value targets accumulate across steps and result in biased value estimates. Recent work has proposed to use chunked critics, which estimate the value of short action sequences ("chunks") rather than individual actions, speeding up value backup. However, extracting policies from chunked critics is challenging: policies must output the entire action chunk open-loop, which can be sub-optimal for environments that require policy reactivity and also challenging to model especially when the chunk length grows. Our key insight is to decouple the chunk length of the critic from that of the policy, allowing the policy to operate over shorter action chunks. We propose a novel algorithm that achieves this by optimizing the policy against a distilled critic for partial action chunks, constructed by optimistically backing up from the original chunked critic to approximate the maximum value achievable when a partial action chunk is extended to a complete one. This design retains the benefits of multi-step value propagation while sidestepping both the open-loop sub-optimality and the difficulty of learning action chunking policies for long action chunks. We evaluate our method on challenging, long-horizon offline goal-conditioned tasks and show that it reliably outperforms prior methods. Code: github.com/ColinQiyangLi/dqc.

</details>


### [149] [Sequence-to-Image Transformation for Sequence Classification Using Rips Complex Construction and Chaos Game Representation](https://arxiv.org/abs/2512.10141)
*Sarwan Ali,Taslim Murad,Imdadullah Khan*

Main category: cs.LG

TL;DR: 提出一种将分子序列转化为图像的拓扑方法，结合混沌游戏表示和Rips复形构建，用于抗癌肽分类，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统分子序列分类方法存在稀疏性和计算复杂度问题，而深度学习模型在表格化生物数据上表现不佳，需要一种能有效捕捉序列结构特征的新方法。

Method: 结合混沌游戏表示将序列元素映射到2D坐标，计算成对距离，构建Rips复形来捕捉局部结构和全局拓扑特征，提供表示唯一性、拓扑稳定性和信息保留的形式保证。

Result: 在抗癌肽数据集上取得优异性能，在乳腺癌和肺癌数据集上分别达到86.8%和94.5%的准确率，优于基于向量、序列语言模型和现有图像方法。

Conclusion: 拓扑表示能保留关键序列信息，同时使基于视觉的深度学习架构能有效用于分子序列分析，为生物序列分类提供了新思路。

Abstract: Traditional feature engineering approaches for molecular sequence classification suffer from sparsity issues and computational complexity, while deep learning models often underperform on tabular biological data. This paper introduces a novel topological approach that transforms molecular sequences into images by combining Chaos Game Representation (CGR) with Rips complex construction from algebraic topology. Our method maps sequence elements to 2D coordinates via CGR, computes pairwise distances, and constructs Rips complexes to capture both local structural and global topological features. We provide formal guarantees on representation uniqueness, topological stability, and information preservation. Extensive experiments on anticancer peptide datasets demonstrate superior performance over vector-based, sequence language models, and existing image-based methods, achieving 86.8\% and 94.5\% accuracy on breast and lung cancer datasets, respectively. The topological representation preserves critical sequence information while enabling effective utilization of vision-based deep learning architectures for molecular sequence analysis.

</details>


### [150] [Murmur2Vec: A Hashing Based Solution For Embedding Generation Of COVID-19 Spike Sequences](https://arxiv.org/abs/2512.10147)
*Sarwan Ali,Taslim Murad*

Main category: cs.LG

TL;DR: 提出一种基于哈希的SARS-CoV-2刺突蛋白序列嵌入方法，用于高效的大规模病毒谱系分类，相比现有方法显著提升效率。


<details>
  <summary>Details</summary>
Motivation: COVID-19早期检测和特征分析对临床响应和公共卫生规划至关重要。现有方法存在局限性：系统发育树方法计算密集，无法扩展到数百万序列数据集；现有嵌入方法要么依赖序列比对，要么预测性能不佳且运行成本高，阻碍大规模分析。

Method: 针对SARS-CoV-2刺突蛋白区域的最常见谱系，提出一种可扩展的嵌入方法，利用哈希技术生成紧凑的低维序列表示。使用这些嵌入训练多种机器学习模型进行监督谱系分类。

Result: 与多种基线方法和最先进的生物序列嵌入方法相比，所提嵌入方法在效率上有显著改进：分类准确率达到86.4%，同时将嵌入生成时间减少高达99.81%。

Conclusion: 该方法为大规模病毒序列分析提供了一个快速、有效且可扩展的解决方案，具有在大规模基因组监测中应用的潜力。

Abstract: Early detection and characterization of coronavirus disease (COVID-19), caused by SARS-CoV-2, remain critical for effective clinical response and public-health planning. The global availability of large-scale viral sequence data presents significant opportunities for computational analysis; however, existing approaches face notable limitations. Phylogenetic tree-based methods are computationally intensive and do not scale efficiently to today's multi-million-sequence datasets. Similarly, current embedding-based techniques often rely on aligned sequences or exhibit suboptimal predictive performance and high runtime costs, creating barriers to practical large-scale analysis. In this study, we focus on the most prevalent SARS-CoV-2 lineages associated with the spike protein region and introduce a scalable embedding method that leverages hashing to generate compact, low-dimensional representations of spike sequences. These embeddings are subsequently used to train a variety of machine learning models for supervised lineage classification. We conduct an extensive evaluation comparing our approach with multiple baseline and state-of-the-art biological sequence embedding methods across diverse metrics. Our results demonstrate that the proposed embeddings offer substantial improvements in efficiency, achieving up to 86.4\% classification accuracy while reducing embedding generation time by as much as 99.81\%. This highlights the method's potential as a fast, effective, and scalable solution for large-scale viral sequence analysis.

</details>


### [151] [Rethinking Causal Discovery Through the Lens of Exchangeability](https://arxiv.org/abs/2512.10152)
*Tiago Brogueira,Mário Figueiredo*

Main category: cs.LG

TL;DR: 该论文提出将传统i.i.d.因果发现框架重构为更一般的可交换性框架，并创建了首个仅基于可交换性假设的合成数据集，其神经网络算法在真实基准测试中达到SOTA水平。


<details>
  <summary>Details</summary>
Motivation: 传统因果发现方法在i.i.d.和时间序列数据下采用不同建模假设，作者认为i.i.d.设置应重构为更一般的可交换性框架，因为许多现有i.i.d.方法实际依赖可交换性假设，且广泛使用的Tübingen基准数据集主要由可交换而非i.i.d.样本组成。

Method: 1) 概念论证：将实验因果推断对可交换性的依赖扩展到因果发现；2) 实证论证：分析现有i.i.d.方法实际依赖可交换性假设，并展示Tübingen数据集主要由可交换样本组成；3) 创建仅基于可交换性假设的合成数据集；4) 开发在该数据集上训练的神经网络因果发现算法。

Result: 1) 新的可交换性合成数据集比所有i.i.d.合成数据集更接近真实世界"i.i.d."数据集的统计结构；2) 仅在该数据集上训练的神经网络因果发现算法在真实世界基准测试中与其他SOTA i.i.d.方法表现相当。

Conclusion: 因果发现应从i.i.d.框架转向更一般的可交换性框架，这能更好地反映现实世界数据特性，且基于可交换性假设的方法在实际应用中具有竞争力。

Abstract: Causal discovery methods have traditionally been developed under two distinct regimes: independent and identically distributed (i.i.d.) and timeseries data, each governed by separate modelling assumptions. In this paper, we argue that the i.i.d. setting can and should be reframed in terms of exchangeability, a strictly more general symmetry principle. We present the implications of this reframing, alongside two core arguments: (1) a conceptual argument, based on extending the dependency of experimental causal inference on exchangeability to causal discovery; and (2) an empirical argument, showing that many existing i.i.d. causal-discovery methods are predicated on exchangeability assumptions, and that the sole extensive widely-used real-world "i.i.d." benchmark (the Tübingen dataset) consists mainly of exchangeable (and not i.i.d.) examples. Building on this insight, we introduce a novel synthetic dataset that enforces only the exchangeability assumption, without imposing the stronger i.i.d. assumption. We show that our exchangeable synthetic dataset mirrors the statistical structure of the real-world "i.i.d." dataset more closely than all other i.i.d. synthetic datasets. Furthermore, we demonstrate the predictive capability of this dataset by proposing a neural-network-based causal-discovery algorithm trained exclusively on our synthetic dataset, and which performs similarly to other state-of-the-art i.i.d. methods on the real-world benchmark.

</details>


### [152] [CIEGAD: Cluster-Conditioned Interpolative and Extrapolative Framework for Geometry-Aware and Domain-Aligned Data Augmentation](https://arxiv.org/abs/2512.10178)
*Keito Inoshita,Xiaokang Zhou,Akira Kawai,Katsutoshi Yada*

Main category: cs.LG

TL;DR: CIEGAD是一个几何感知和领域对齐的数据增强框架，通过聚类条件、插值/外推合成和几何约束过滤，系统性地补充分布内和分布外的语义未覆盖区域，提升长尾和多分类任务的性能。


<details>
  <summary>Details</summary>
Motivation: 实际深度学习部署中，数据稀缺和标签分布不平衡导致真实数据分布中存在语义未覆盖区域，这阻碍模型训练，导致类别边界附近误分类和边缘区域不稳定行为。现有LLM数据增强方法缺乏方向控制、领域对齐和质量控制的集成框架。

Method: 提出CIEGAD框架：1) 通过聚类条件构建领域轮廓；2) 使用集成类别频率和几何指标的分层频率-几何分配进行生成分配；3) 通过插值和外推合成的共存精细控制生成方向；4) 通过几何约束过滤结合LLM-as-a-Judge机制进行质量控制。

Result: 在多个分类任务上的实验表明，CIEGAD能有效扩展真实数据分布的边缘，同时保持生成数据与真实数据的高对齐度和语义多样性。特别是在长尾和多分类任务中，CIEGAD持续提升F1和召回率，验证了分布一致性、多样性和质量的三重和谐。

Conclusion: CIEGAD作为一个实用导向的数据增强框架，能够补充代表性不足的区域，同时保持与真实数据的对齐，为解决数据稀缺和分布不平衡问题提供了有效方案。

Abstract: In practical deep learning deployment, the scarcity of data and the imbalance of label distributions often lead to semantically uncovered regions within the real-world data distribution, hindering model training and causing misclassification near class boundaries as well as unstable behaviors in peripheral areas. Although recent large language models (LLMs) show promise for data augmentation, an integrated framework that simultaneously achieves directional control of generation, domain alignment, and quality control has not yet been fully established. To address these challenges, we propose a Cluster-conditioned Interpolative and Extrapolative framework for Geometry-Aware and Domain-aligned data augmentation (CIEGAD), which systematically complements both in-distribution and out-of-distribution semantically uncovered regions. CIEGAD constructs domain profiles through cluster conditioning, allocates generation with a hierarchical frequency-geometric allocation integrating class frequency and geometric indicators, and finely controls generation directions via the coexistence of interpolative and extrapolative synthesis. It further performs quality control through geometry-constrained filtering combined with an LLM-as-a-Judge mechanism. Experiments on multiple classification tasks demonstrate that CIEGAD effectively extends the periphery of real-world data distributions while maintaining high alignment between generated and real-world data as well as semantic diversity. In particular, for long-tailed and multi-class classification tasks, CIEGAD consistently improves F1 and recall, validating the triple harmony of distributional consistency, diversity, and quality. These results indicate that CIEGAD serves as a practically oriented data augmentation framework that complements underrepresented regions while preserving alignment with real-world data.

</details>


### [153] [Assessing Neuromorphic Computing for Fingertip Force Decoding from Electromyography](https://arxiv.org/abs/2512.10179)
*Abolfazl Shahrooei,Luke Arthur,Om Patel,Derek Kamper*

Main category: cs.LG

TL;DR: 比较SNN和TCN在从HD-sEMG解码指尖力方面的性能，TCN表现更优但SNN作为神经形态基线有改进潜力


<details>
  <summary>Details</summary>
Motivation: HD-sEMG为非侵入式神经接口提供了可能，但将神经活动映射到运动意图仍具挑战性，需要探索更有效的解码方法

Method: 使用SNN和TCN两种架构解码从HD-sEMG通过FastICA分解得到的运动单元放电信号，在重叠窗口上训练端到端因果卷积模型

Result: TCN表现更好：4.44% MVC RMSE（r=0.974），SNN为8.25% MVC（r=0.922），但SNN作为神经形态基线有改进空间

Conclusion: 虽然TCN更准确，但SNN作为现实的神经形态架构基线，通过适度的架构和超参数优化可以缩小与TCN的性能差距

Abstract: High-density surface electromyography (HD-sEMG) provides a noninvasive neural interface for assistive and rehabilitation control, but mapping neural activity to user motor intent remains challenging. We assess a spiking neural network (SNN) as a neuromorphic architecture against a temporal convolutional network (TCN) for decoding fingertip force from motor-unit (MU) firing derived from HD-sEMG. Data were collected from a single participant (10 trials) with two forearm electrode arrays; MU activity was obtained via FastICA-based decomposition, and models were trained on overlapping windows with end-to-end causal convolutions. On held-out trials, the TCN achieved 4.44% MVC RMSE (Pearson r = 0.974) while the SNN achieved 8.25% MVC (r = 0.922). While the TCN was more accurate, we view the SNN as a realistic neuromorphic baseline that could close much of this gap with modest architectural and hyperparameter refinements.

</details>


### [154] [MiniF2F-Dafny: LLM-Guided Mathematical Theorem Proving via Auto-Active Verification](https://arxiv.org/abs/2512.10187)
*Mantas Baksys,Stefan Zetzsche,Olivier Bouissou*

Main category: cs.LG

TL;DR: 将数学推理基准miniF2F首次翻译到自动定理证明器Dafny，测试自动验证效果并评估LLMs提供证明提示的能力


<details>
  <summary>Details</summary>
Motivation: 将数学推理基准从交互式定理证明器扩展到自动定理证明器，探索自动验证与LLMs结合的新方法

Method: 将miniF2F基准翻译到Dafny，测试自动验证成功率，并评估12个现成LLMs在提供证明提示方面的表现

Result: Dafny自动验证了测试集40.6%和验证集44.7%的问题（空证明），最佳LLM通过迭代纠错达到55.7%的pass@4成功率

Conclusion: 展示了LLMs提供高层次指导与自动化处理低层次细节的有效分工，为数学推理自动化提供了新思路

Abstract: We present miniF2F-Dafny, the first translation of the mathematical reasoning benchmark miniF2F to an automated theorem prover: Dafny. Previously, the benchmark existed only in interactive theorem provers (Lean, Isabelle, HOL Light, Metamath). We find that Dafny's automation verifies 99/244 (40.6%) of the test set and 109/244 (44.7%) of the validation set with empty proofs--requiring no manual proof steps. For problems where empty proofs fail, we evaluate 12 off-the-shelf LLMs on providing proof hints. The best model we test achieves 55.7% pass@4 success rate employing iterative error correction. These preliminary results highlight an effective division of labor: LLMs provide high-level guidance while automation handles low-level details. Our benchmark can be found on GitHub at http://github.com/dafny-lang/miniF2F .

</details>


### [155] [Exact Recovery of Non-Random Missing Multidimensional Time Series via Temporal Isometric Delay-Embedding Transform](https://arxiv.org/abs/2512.10191)
*Hao Shu,Jicheng Li,Yu Jin,Ling Zhou*

Main category: cs.LG

TL;DR: 提出LRTC-TIDT模型，通过时间等距延迟嵌入变换构建Hankel张量，利用时间序列的平滑性和周期性自然诱导低秩性，解决多维时间序列中非随机缺失数据的精确恢复问题。


<details>
  <summary>Details</summary>
Motivation: 非随机缺失数据是多维时间序列中普遍存在但处理不足的缺陷，严重威胁数据驱动分析和决策的可靠性。传统的纯低秩张量补全方法在处理非随机缺失时存在方法论和理论上的不足，而现有的Hankel结构张量补全方法缺乏对Hankel张量低秩性来源的清晰解释，且缺少针对非随机缺失数据的精确恢复理论。

Method: 提出时间等距延迟嵌入变换，构建Hankel张量，其低秩性由底层时间序列的平滑性和周期性自然诱导。在此基础上开发LRTC-TIDT模型，在张量奇异值分解框架下刻画低秩结构。模型在满足规定的非随机采样条件和温和的不相干假设下，能够实现精确恢复。

Result: 在各种非随机缺失模式下的仿真实验中验证了LRTC-TIDT模型的精确恢复能力。在多个真实世界任务中，包括网络流量重建、城市交通估计和温度场预测，LRTC-TIDT始终优于现有的基于张量的方法。

Conclusion: LRTC-TIDT模型通过时间等距延迟嵌入变换为Hankel张量提供了明确的低秩性来源，并建立了针对非随机缺失数据的精确恢复理论，为多维时间序列的非随机缺失数据恢复提供了有效解决方案。

Abstract: Non-random missing data is a ubiquitous yet undertreated flaw in multidimensional time series, fundamentally threatening the reliability of data-driven analysis and decision-making. Pure low-rank tensor completion, as a classical data recovery method, falls short in handling non-random missingness, both methodologically and theoretically. Hankel-structured tensor completion models provide a feasible approach for recovering multidimensional time series with non-random missing patterns. However, most Hankel-based multidimensional data recovery methods both suffer from unclear sources of Hankel tensor low-rankness and lack an exact recovery theory for non-random missing data. To address these issues, we propose the temporal isometric delay-embedding transform, which constructs a Hankel tensor whose low-rankness is naturally induced by the smoothness and periodicity of the underlying time series. Leveraging this property, we develop the \textit{Low-Rank Tensor Completion with Temporal Isometric Delay-embedding Transform} (LRTC-TIDT) model, which characterizes the low-rank structure under the \textit{Tensor Singular Value Decomposition} (t-SVD) framework. Once the prescribed non-random sampling conditions and mild incoherence assumptions are satisfied, the proposed LRTC-TIDT model achieves exact recovery, as confirmed by simulation experiments under various non-random missing patterns. Furthermore, LRTC-TIDT consistently outperforms existing tensor-based methods across multiple real-world tasks, including network flow reconstruction, urban traffic estimation, and temperature field prediction. Our implementation is publicly available at https://github.com/HaoShu2000/LRTC-TIDT.

</details>


### [156] [Federated Domain Generalization with Latent Space Inversion](https://arxiv.org/abs/2512.10224)
*Ragja Palakkadavath,Hung Le,Thanh Nguyen-Tang,Svetha Venkatesh,Sunil Gupta*

Main category: cs.LG

TL;DR: 提出一种新的联邦域泛化方法，通过潜在空间反转和重要权重聚合策略，在保护隐私的同时提升全局模型对未见客户端的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有联邦域泛化方法在提升全局模型泛化能力时，通过共享客户端数据统计信息会损害隐私保护。需要一种既能保护隐私又能处理非独立同分布客户端的方法。

Method: 1) 潜在空间反转：通过强制局部模型间的域不变性来保护客户端隐私；2) 重要权重聚合策略：在模型聚合时优先考虑对局部模型预测有显著影响的参数，保留局部适应能力。

Result: 实验表明，该方法在减少通信开销的同时，取得了优于现有最先进方法的结果。

Conclusion: 提出的联邦域泛化方法通过隐私保护的局部训练和智能模型聚合，有效解决了现有方法在隐私保护和处理非独立同分布客户端方面的局限性。

Abstract: Federated domain generalization (FedDG) addresses distribution shifts among clients in a federated learning framework. FedDG methods aggregate the parameters of locally trained client models to form a global model that generalizes to unseen clients while preserving data privacy. While improving the generalization capability of the global model, many existing approaches in FedDG jeopardize privacy by sharing statistics of client data between themselves. Our solution addresses this problem by contributing new ways to perform local client training and model aggregation. To improve local client training, we enforce (domain) invariance across local models with the help of a novel technique, \textbf{latent space inversion}, which enables better client privacy. When clients are not \emph{i.i.d}, aggregating their local models may discard certain local adaptations. To overcome this, we propose an \textbf{important weight} aggregation strategy to prioritize parameters that significantly influence predictions of local models during aggregation. Our extensive experiments show that our approach achieves superior results over state-of-the-art methods with less communication overhead.

</details>


### [157] [Adaptive Information Routing for Multimodal Time Series Forecasting](https://arxiv.org/abs/2512.10229)
*Jun Seo,Hyeokjun Choe,Seohui Bae,Soyeon Park,Wonbin Ahn,Taeyoon Lim,Junhyuk Kang,Sangjun Han,Jaehoon Lee,Dongwan Kang,Minjae Kim,Sungdong Yoo,Soonyoung Lee*

Main category: cs.LG

TL;DR: 提出AIR框架，通过文本信息动态指导时间序列模型，提升多模态时间序列预测准确性


<details>
  <summary>Details</summary>
Motivation: 传统时间序列预测仅依赖历史数据，信息有限导致预测不准确。多模态方法加入文本数据作为补充，但现有方法将文本数据与时间序列数据同等对待，未能充分利用文本信息指导时间序列模型

Method: 提出自适应信息路由(AIR)框架，利用文本信息动态控制多元时间序列信息的组合方式和程度。同时开发文本精炼流程，使用大语言模型将原始文本转换为适合多模态预测的形式，并建立相应的基准测试

Result: 在原油价格和汇率等真实市场数据上的实验表明，AIR能有效利用文本输入调节时间序列模型行为，在各种时间序列预测任务中显著提高预测准确性

Conclusion: AIR框架通过文本信息动态指导时间序列模型，相比现有方法能更有效地利用多模态信息，显著提升预测性能

Abstract: Time series forecasting is a critical task for artificial intelligence with numerous real-world applications. Traditional approaches primarily rely on historical time series data to predict the future values. However, in practical scenarios, this is often insufficient for accurate predictions due to the limited information available. To address this challenge, multimodal time series forecasting methods which incorporate additional data modalities, mainly text data, alongside time series data have been explored. In this work, we introduce the Adaptive Information Routing (AIR) framework, a novel approach for multimodal time series forecasting. Unlike existing methods that treat text data on par with time series data as interchangeable auxiliary features for forecasting, AIR leverages text information to dynamically guide the time series model by controlling how and to what extent multivariate time series information should be combined. We also present a text-refinement pipeline that employs a large language model to convert raw text data into a form suitable for multimodal forecasting, and we introduce a benchmark that facilitates multimodal forecasting experiments based on this pipeline. Experiment results with the real world market data such as crude oil price and exchange rates demonstrate that AIR effectively modulates the behavior of the time series model using textual inputs, significantly enhancing forecasting accuracy in various time series forecasting tasks.

</details>


### [158] [R^2-HGP: A Double-Regularized Gaussian Process for Heterogeneous Transfer Learning](https://arxiv.org/abs/2512.10258)
*Duo Wang,Xinming Wang,Chao Wang,Xiaowei Yue,Jianguo Wu*

Main category: cs.LG

TL;DR: 提出R²-HGP框架，通过可训练先验概率映射对齐异构输入域，结合物理知识正则化和稀疏惩罚，解决多输出高斯过程在异构迁移学习中的挑战。


<details>
  <summary>Details</summary>
Motivation: 多输出高斯过程在迁移学习中面临三个主要挑战：1) 源域和目标域输入空间异构导致直接知识转移困难；2) 忽略先验知识和物理信息，导致映射不稳定；3) 不当的信息共享容易导致负迁移。传统模型无法统一解决这些问题。

Method: 提出双正则化异构高斯过程框架(R²-HGP)：1) 可训练先验概率映射模型对齐异构输入域；2) 将对齐结果作为隐变量构建多源迁移GP模型；3) 集成到条件变分自编码器框架中；4) 加入物理知识作为正则项；5) 对迁移系数施加稀疏惩罚，自适应选择信息源。

Result: 通过大量仿真和真实工程案例验证，R²-HGP在多种评估指标上均优于现有最先进基准方法，表现出一致优越性。

Conclusion: R²-HGP框架有效解决了异构迁移学习中的关键挑战，通过输入对齐、物理知识整合和自适应源选择，实现了稳定高效的知识转移，避免了负迁移问题。

Abstract: Multi-output Gaussian process (MGP) models have attracted significant attention for their flexibility and uncertainty-quantification capabilities, and have been widely adopted in multi-source transfer learning scenarios due to their ability to capture inter-task correlations. However, they still face several challenges in transfer learning. First, the input spaces of the source and target domains are often heterogeneous, which makes direct knowledge transfer difficult. Second, potential prior knowledge and physical information are typically ignored during heterogeneous transfer, hampering the utilization of domain-specific insights and leading to unstable mappings. Third, inappropriate information sharing among target and sources can easily lead to negative transfer. Traditional models fail to address these issues in a unified way. To overcome these limitations, this paper proposes a Double-Regularized Heterogeneous Gaussian Process framework (R^2-HGP). Specifically, a trainable prior probability mapping model is first proposed to align the heterogeneous input domains. The resulting aligned inputs are treated as latent variables, upon which a multi-source transfer GP model is constructed and the entire structure is integrated into a novel conditional variational autoencoder (CVAE) based framework. Physical insights is further incorporated as a regularization term to ensure that the alignment results adhere to known physical knowledge. Next, within the multi-source transfer GP model, a sparsity penalty is imposed on the transfer coefficients, enabling the model to adaptively select the most informative source outputs and suppress negative transfer. Extensive simulations and real-world engineering case studies validate the effectiveness of our R^2-HGP, demonstrating consistent superiority over state-of-the-art benchmarks across diverse evaluation metrics.

</details>


### [159] [A Kernel-based Resource-efficient Neural Surrogate for Multi-fidelity Prediction of Aerodynamic Field](https://arxiv.org/abs/2512.10287)
*Apurba Sarker,Reza T. Batley,Darshan Sarojini,Sourav Saha*

Main category: cs.LG

TL;DR: KHRONOS是一种基于核的神经代理模型，通过变分原理、插值理论和张量分解构建，在计算资源受限条件下，相比传统密集神经网络，能以更少的参数和更快的速度实现多保真度空气动力学场预测。


<details>
  <summary>Details</summary>
Motivation: 传统空气动力学模拟计算成本高，代理模型能提供快速替代方案。研究旨在开发一种在计算资源受限条件下仍能高效工作的多保真度代理模型，平衡预测精度与计算效率。

Method: 提出KHRONOS模型，基于变分原理、插值理论和张量分解构建。使用AirfRANS数据集作为高保真基准，NeuralFoil生成低保真数据。比较KHRONOS与MLP、GNN、PINN三种架构在不同高保真数据可用性（0%、10%、30%）和复杂几何参数化条件下的性能，预测翼型表面压力系数分布。

Result: 虽然所有模型最终都能达到相当的预测精度，但KHRONOS在资源受限条件下表现优异。在可比精度下，KHRONOS需要的可训练参数少几个数量级，训练和推理速度远快于传统密集神经网络。

Conclusion: KHRONOS及其类似架构在多保真度空气动力学场预测中能有效平衡精度与效率，特别适用于计算资源受限的应用场景。

Abstract: Surrogate models provide fast alternatives to costly aerodynamic simulations and are extremely useful in design and optimization applications. This study proposes the use of a recent kernel-based neural surrogate, KHRONOS. In this work, we blend sparse high-fidelity (HF) data with low-fidelity (LF) information to predict aerodynamic fields under varying constraints in computational resources. Unlike traditional approaches, KHRONOS is built upon variational principles, interpolation theory, and tensor decomposition. These elements provide a mathematical basis for heavy pruning compared to dense neural networks. Using the AirfRANS dataset as a high-fidelity benchmark and NeuralFoil to generate low-fidelity counterparts, this work compares the performance of KHRONOS with three contemporary model architectures: a multilayer perceptron (MLP), a graph neural network (GNN), and a physics-informed neural network (PINN). We consider varying levels of high-fidelity data availability (0%, 10%, and 30%) and increasingly complex geometry parameterizations. These are used to predict the surface pressure coefficient distribution over the airfoil. Results indicate that, whilst all models eventually achieve comparable predictive accuracy, KHRONOS excels in resource-constrained conditions. In this domain, KHRONOS consistently requires orders of magnitude fewer trainable parameters and delivers much faster training and inference than contemporary dense neural networks at comparable accuracy. These findings highlight the potential of KHRONOS and similar architectures to balance accuracy and efficiency in multi-fidelity aerodynamic field prediction.

</details>


### [160] [An Interpretable AI Tool for SAVR vs TAVR in Low to Intermediate Risk Patients with Severe Aortic Stenosis](https://arxiv.org/abs/2512.10308)
*Vasiliki Stoumpou,Maciej Tysarowski,Talhat Azemi,Jawad Haider,Howard L. Haronian,Robert C. Hagberg,Dimitris Bertsimas*

Main category: cs.LG

TL;DR: 开发可解释的处方框架，通过最优策略树为主动脉瓣狭窄患者提供个体化的TAVR或SAVR治疗建议，以优化5年死亡率。


<details>
  <summary>Details</summary>
Motivation: 目前临床实践中对于低至中危严重主动脉瓣狭窄患者的治疗选择（外科手术SAVR vs 经导管TAVR）存在差异，缺乏能够直接优化长期结果的可解释个体化治疗推荐模型。

Method: 提出可解释的处方框架，整合预后匹配、反事实结果建模和最优策略树（OPT）。通过预后匹配和样本加权模拟随机化，估计SAVR和TAVR下的反事实死亡率，训练策略模型将患者划分为临床连贯亚组并推荐风险较低的治疗。

Result: 应用OPT建议后，反事实评估显示5年死亡率在Hartford医院降低20.3%，在St. Vincent's医院降低13.8%（相对于实际处方），显示出良好的跨机构泛化能力。学习到的决策边界与实际结果和临床观察一致。

Conclusion: 该可解释处方框架首次为TAVR vs SAVR提供了透明、数据驱动的治疗建议，在内部和外部队列中均改善了估计的长期结果，同时保持临床合理性，有助于结构性心脏病精准医学的系统化和循证化。

Abstract: Background. Treatment selection for low to intermediate risk patients with severe aortic stenosis between surgical (SAVR) and transcatheter (TAVR) aortic valve replacement remains variable in clinical practice, driven by patient heterogeneity and institutional preferences. While existing models predict postprocedural risk, there is a lack of interpretable, individualized treatment recommendations that directly optimize long-term outcomes.
  Methods. We introduce an interpretable prescriptive framework that integrates prognostic matching, counterfactual outcome modeling, and an Optimal Policy Tree (OPT) to recommend the treatment minimizing expected 5-year mortality. Using data from Hartford Hospital and St. Vincent's Hospital, we emulate randomization via prognostic matching and sample weighting and estimate counterfactual mortality under both SAVR and TAVR. The policy model, trained on these counterfactual predictions, partitions patients into clinically coherent subgroups and prescribes the treatment associated with lower estimated risk.
  Findings. If the OPT prescriptions are applied, counterfactual evaluation showed an estimated reduction in 5-year mortality of 20.3\% in Hartford and 13.8\% in St. Vincent's relative to real-life prescriptions, showing promising generalizability to unseen data from a different institution. The learned decision boundaries aligned with real-world outcomes and clinical observations.
  Interpretation. Our interpretable prescriptive framework is, to the best of our knowledge, the first to provide transparent, data-driven recommendations for TAVR versus SAVR that improve estimated long-term outcomes both in an internal and external cohort, while remaining clinically grounded and contributing toward a more systematic and evidence-based approach to precision medicine in structural heart disease.

</details>


### [161] [A Privacy-Preserving Cloud Architecture for Distributed Machine Learning at Scale](https://arxiv.org/abs/2512.10341)
*Vinoth Punniyamoorthy,Ashok Gadi Parthi,Mayilsamy Palanigounder,Ravi Kiran Kodali,Bikesh Kumar,Kabilan Kannan*

Main category: cs.LG

TL;DR: 提出一个云原生隐私保护架构，整合联邦学习、差分隐私、零知识合规证明和强化学习驱动的自适应治理，用于分布式机器学习系统


<details>
  <summary>Details</summary>
Motivation: 分布式机器学习系统需要强大的隐私保证、可验证的合规性以及在异构多云环境中的可扩展部署

Method: 集成联邦学习、差分隐私、零知识合规证明和强化学习驱动的自适应治理的云原生架构，支持安全模型训练和推理而不集中敏感数据

Result: 原型部署显示减少了成员推理风险，一致执行正式隐私预算，差分隐私下保持稳定模型性能，在多机构工作负载中保持效用且开销最小

Conclusion: 该框架为大规模部署可信赖且合规的分布式机器学习系统建立了实用基础

Abstract: Distributed machine learning systems require strong privacy guarantees, verifiable compliance, and scalable deploy- ment across heterogeneous and multi-cloud environments. This work introduces a cloud-native privacy-preserving architecture that integrates federated learning, differential privacy, zero- knowledge compliance proofs, and adaptive governance powered by reinforcement learning. The framework supports secure model training and inference without centralizing sensitive data, while enabling cryptographically verifiable policy enforcement across institutions and cloud platforms. A full prototype deployed across hybrid Kubernetes clusters demonstrates reduced membership- inference risk, consistent enforcement of formal privacy budgets, and stable model performance under differential privacy. Ex- perimental evaluation across multi-institution workloads shows that the architecture maintains utility with minimal overhead while providing continuous, risk-aware governance. The pro- posed framework establishes a practical foundation for deploying trustworthy and compliant distributed machine learning systems at scale.

</details>


### [162] [Dynamics of Agentic Loops in Large Language Models: A Geometric Theory of Trajectories](https://arxiv.org/abs/2512.10350)
*Nicolas Tacheny*

Main category: cs.LG

TL;DR: 论文提出几何框架分析智能体循环在语义嵌入空间中的动态行为，区分语言变换的工件空间和几何测量的嵌入空间，通过等渗校准消除余弦相似度偏差，识别收缩和发散两种基本机制。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的智能体系统通过递归反馈循环运行，但智能体循环的几何行为（收敛、发散或更复杂动态）仍缺乏理解，需要系统分析框架。

Method: 引入几何框架分析智能体轨迹在语义嵌入空间中的行为，区分工件空间和嵌入空间，提出等渗校准消除余弦相似度偏差，将迭代变换视为离散动力系统进行研究。

Result: 通过受控实验识别两种基本机制：收缩性重写循环收敛于稳定吸引子且分散度减小，探索性总结和否定循环产生无界发散且无聚类形成，显示收缩和扩张的定性不同几何特征。

Conclusion: 提示设计直接控制智能体循环的动力机制，能够系统控制迭代LLM变换中的收敛、发散和轨迹结构，为智能体循环行为提供可预测框架。

Abstract: Agentic systems built on large language models operate through recursive feedback loops, where each output becomes the next input. Yet the geometric behavior of these agentic loops (whether they converge, diverge, or exhibit more complex dynamics) remains poorly understood. This paper introduces a geometric framework for analyzing agentic trajectories in semantic embedding space, treating iterative transformations as discrete dynamical systems. We distinguish the artifact space, where linguistic transformations occur, from the embedding space, where geometric measurements are performed. Because cosine similarity is biased by embedding anisotropy, we introduce an isotonic calibration that eliminates systematic bias and aligns similarities with human semantic judgments while preserving high local stability. This enables rigorous measurement of trajectories, clusters and attractors. Through controlled experiments on singular agentic loops, we identify two fundamental regimes. A contractive rewriting loop converges toward a stable attractor with decreasing dispersion, while an exploratory summarize and negate loop produces unbounded divergence with no cluster formation. These regimes display qualitatively distinct geometric signatures of contraction and expansion. Our results show that prompt design directly governs the dynamical regime of an agentic loop, enabling systematic control of convergence, divergence and trajectory structure in iterative LLM transformations.

</details>


### [163] [Better Prevent than Tackle: Valuing Defense in Soccer Based on Graph Neural Networks](https://arxiv.org/abs/2512.10355)
*Hyunsung Kim,Sangwoo Seo,Hoyoung Choi,Tom Boomstra,Jinsung Yoon,Chanyoung Park*

Main category: cs.LG

TL;DR: DEFCON是一个评估足球防守贡献的框架，使用图注意力网络量化球员在每次进攻情况中的防守价值，通过对比进攻方预期控球价值的变化来评估防守表现。


<details>
  <summary>Details</summary>
Motivation: 现有足球防守评估方法主要关注可见的抢断、拦截等有球动作，但有效防守往往体现在预防危险机会发生之前，导致防守球员的真实影响未被充分衡量。

Method: 使用图注意力网络估计每个进攻选项的成功概率和预期价值，以及每个防守球员的责任分配。通过比较每次动作前后进攻方的预期控球价值变化，给防守球员分配正负积分。

Result: 在2023-24赛季训练、2024-25赛季荷甲数据评估中，DEFCON的球员积分与市场估值呈现强正相关性。框架还展示了多种实际应用，包括比赛中防守贡献时间线、场地区域空间分析和攻防球员互动总结。

Conclusion: DEFCON提供了一个全面量化足球防守贡献的框架，能够更准确地评估防守球员的真实影响，为教练、分析师和球探提供了有价值的工具。

Abstract: Evaluating defensive performance in soccer remains challenging, as effective defending is often expressed not through visible on-ball actions such as interceptions and tackles, but through preventing dangerous opportunities before they arise. Existing approaches have largely focused on valuing on-ball actions, leaving much of defenders' true impact unmeasured. To address this gap, we propose DEFCON (DEFensive CONtribution evaluator), a comprehensive framework that quantifies player-level defensive contributions for every attacking situation in soccer. Leveraging Graph Attention Networks, DEFCON estimates the success probability and expected value of each attacking option, along with each defender's responsibility for stopping it. These components yield an Expected Possession Value (EPV) for the attacking team before and after each action, and DEFCON assigns positive or negative credits to defenders according to whether they reduced or increased the opponent's EPV. Trained on 2023-24 and evaluated on 2024-25 Eredivisie event and tracking data, DEFCON's aggregated player credits exhibit strong positive correlations with market valuations. Finally, we showcase several practical applications, including in-game timelines of defensive contributions, spatial analyses across pitch zones, and pairwise summaries of attacker-defender interactions.

</details>


### [164] [GPG: Generalized Policy Gradient Theorem for Transformer-based Policies](https://arxiv.org/abs/2512.10365)
*Hangyu Mao,Guangting Dong,Zhicheng Dou*

Main category: cs.LG

TL;DR: 提出了专门针对Transformer策略的广义策略梯度(GPG)定理，将标准策略梯度定理和GRPO作为其特例，并探索了在LLM训练中的实际应用


<details>
  <summary>Details</summary>
Motivation: 现有的策略梯度方法在Transformer-based策略上可能不是最优的，需要专门针对Transformer架构设计的策略梯度理论框架

Method: 提出了广义策略梯度(GPG)定理，专门为Transformer-based策略设计，将标准策略梯度定理和GRPO统一到该框架下

Result: 证明了标准策略梯度定理和GRPO都是GPG框架的特殊情况，为Transformer策略优化提供了统一的理论基础

Conclusion: GPG定理为Transformer-based策略提供了更通用的梯度计算框架，在LLM训练中具有实际应用价值，为高效策略优化提供了新见解

Abstract: We present the Generalized Policy Gradient (GPG) Theorem, specifically designed for Transformer-based policies. Notably, we demonstrate that both standard Policy Gradient Theorem and GRPO emerge as special cases within our GPG framework. Furthermore, we explore its practical applications in training Large Language Models (LLMs), offering new insights into efficient policy optimization.

</details>


### [165] [Fitting magnetization data using continued fraction of straight lines](https://arxiv.org/abs/2512.10390)
*Vijay Prakash S*

Main category: cs.LG

TL;DR: 该论文提出使用直线连分数来近似铁磁材料的非线性磁化曲线，用于解释磁畴生长和收缩的非线性行为。


<details>
  <summary>Details</summary>
Motivation: 铁磁材料的磁化强度随外加磁场增强而增加，这是由于微观磁畴逐渐与外场对齐导致的非线性现象。需要一种数学方法来描述这种非线性行为，特别是磁畴生长和收缩过程中的变化。

Method: 采用直线连分数来近似非线性磁化函数，这种代数表达式可以通过非线性回归来估计参数，从而拟合磁化曲线。

Result: 直线连分数方法成功拟合了铁磁材料的非线性磁化行为，能够解释磁畴在外加磁场下的对齐过程，包括磁畴生长和收缩两种情况。

Conclusion: 直线连分数是一种有效的数学工具，可用于近似铁磁材料的非线性磁化曲线，为理解磁畴对齐的微观机制提供了量化分析方法。

Abstract: Magnetization of a ferromagnetic substance in response to an externally applied magnetic field increases with the strength of the field. This is because at the microscopic level, magnetic moments in certain regions or domains of the substance increasingly align with the applied field, while the amount of misaligned domains decreases. The alignment of such magnetic domains with an applied magnetic field forms the physical basis for the nonlinearity of magnetization. In this paper, the nonlinear function is approximated as a combination of continued fraction of straight lines. The resulting fit is used to interpret the nonlinear behavior in both growing and shrinking magnetic domains. The continued fraction of straight lines used here is an algebraic expression which can be used to estimate parameters using nonlinear regression.

</details>


### [166] [The Eminence in Shadow: Exploiting Feature Boundary Ambiguity for Robust Backdoor Attacks](https://arxiv.org/abs/2512.10402)
*Zhou Feng,Jiahao Chen,Chunyi Zhou,Yuwen Pu,Tianyu Du,Jinbao Li,Jianhai Chen,Shouling Ji*

Main category: cs.LG

TL;DR: 本文提出Eminence框架，通过理论分析揭示稀疏决策边界如何使极小量投毒样本（<0.1%）即可实现高效后门攻击，相比现有方法（>1%）显著降低投毒率。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络虽广泛应用于关键应用，但易受后门攻击。现有方法多依赖启发式暴力搜索，缺乏理论分析，限制了攻击的可预测性和适应性。因此需要从理论上分析后门攻击机制，特别是稀疏决策边界如何实现不成比例的网络操纵。

Method: 首先理论分析稀疏决策边界如何实现不成比例的网络操纵，推导出闭式模糊边界区域，其中少量重标记样本即可引发显著误分类。通过影响函数分析量化这些边界样本引起的显著参数偏移。基于这些洞见，提出Eminence框架：优化视觉隐蔽的通用触发器，战略性地利用脆弱决策边界，以极低投毒率实现稳健误分类。

Result: Eminence在极低投毒率（<0.1%）下实现>90%的攻击成功率，对干净准确率影响可忽略，在不同模型、数据集和场景中表现出高可迁移性。实验验证了理论分析，确认了边界投毒与对抗边界操纵之间的指数关系。

Conclusion: 本文通过理论分析揭示了后门攻击的根本机制，提出具有理论保证的Eminence框架，以极低投毒率实现高效攻击，为理解后门攻击提供了理论基础，并为开发更稳健的防御方法提供了洞见。

Abstract: Deep neural networks (DNNs) underpin critical applications yet remain vulnerable to backdoor attacks, typically reliant on heuristic brute-force methods. Despite significant empirical advancements in backdoor research, the lack of rigorous theoretical analysis limits understanding of underlying mechanisms, constraining attack predictability and adaptability. Therefore, we provide a theoretical analysis targeting backdoor attacks, focusing on how sparse decision boundaries enable disproportionate model manipulation. Based on this finding, we derive a closed-form, ambiguous boundary region, wherein negligible relabeled samples induce substantial misclassification. Influence function analysis further quantifies significant parameter shifts caused by these margin samples, with minimal impact on clean accuracy, formally grounding why such low poison rates suffice for efficacious attacks. Leveraging these insights, we propose Eminence, an explainable and robust black-box backdoor framework with provable theoretical guarantees and inherent stealth properties. Eminence optimizes a universal, visually subtle trigger that strategically exploits vulnerable decision boundaries and effectively achieves robust misclassification with exceptionally low poison rates (< 0.1%, compared to SOTA methods typically requiring > 1%). Comprehensive experiments validate our theoretical discussions and demonstrate the effectiveness of Eminence, confirming an exponential relationship between margin poisoning and adversarial boundary manipulation. Eminence maintains > 90% attack success rate, exhibits negligible clean-accuracy loss, and demonstrates high transferability across diverse models, datasets and scenarios.

</details>


### [167] [The Operator Origins of Neural Scaling Laws: A Generalized Spectral Transport Dynamics of Deep Learning](https://arxiv.org/abs/2512.10427)
*Yizhou Zhang*

Main category: cs.LG

TL;DR: 该论文提出了一个统一的算子理论框架来描述神经网络的训练动态，将梯度下降转化为谱传输-耗散偏微分方程，解释了双下降现象和缩放定律。


<details>
  <summary>Details</summary>
Motivation: 现代深度网络在粗糙、有限正则性区域运行，其中雅可比诱导的算子表现出重尾谱和强基漂移。需要从梯度下降直接推导出统一的算子理论描述来解释训练动态。

Method: 从函数空间中的精确演化方程出发，应用加藤扰动理论得到耦合模态常微分方程系统，通过粗粒化收敛到谱传输-耗散偏微分方程。分析该PDE的自相似解，推导缩放定律指数。

Result: 证明了神经网络训练保持函数正则性，迫使漂移取渐近幂律形式。在弱耦合区域，PDE允许具有分辨率前沿、多项式振幅增长和幂律耗散的自相似解。该框架统一解释了NTK训练和特征学习作为同一PDE的两个极限。

Conclusion: 该研究提供了一个统一的谱框架，连接了算子几何、优化动态和现代深度网络的通用缩放行为，解释了双下降几何并推导了有效训练时间的缩放定律。

Abstract: Modern deep networks operate in a rough, finite-regularity regime where Jacobian-induced operators exhibit heavy-tailed spectra and strong basis drift. In this work, we derive a unified operator-theoretoretic description of neural training dynamics directly from gradient descent. Starting from the exact evolution $\dot e_t = -M(t)e_t$ in function space, we apply Kato perturbation theory to obtain a rigorous system of coupled mode ODEs and show that, after coarse-graining, these dynamics converge to a spectral transport--dissipation PDE \[ \partial_t g + \partial_λ(v g) = -λg + S, \] where $v$ captures eigenbasis drift and $S$ encodes nonlocal spectral coupling.
  We prove that neural training preserves functional regularity, forcing the drift to take an asymptotic power-law form $v(λ,t)\sim -c(t)λ^b$. In the weak-coupling regime -- naturally induced by spectral locality and SGD noise -- the PDE admits self-similar solutions with a resolution frontier, polynomial amplitude growth, and power-law dissipation. This structure yields explicit scaling-law exponents, explains the geometry of double descent, and shows that the effective training time satisfies $τ(t)=t^αL(t)$ for slowly varying $L$.
  Finally, we show that NTK training and feature learning arise as two limits of the same PDE: $v\equiv 0$ recovers lazy dynamics, while $v\neq 0$ produces representation drift. Our results provide a unified spectral framework connecting operator geometry, optimization dynamics, and the universal scaling behavior of modern deep networks.

</details>


### [168] [Metacognitive Sensitivity for Test-Time Dynamic Model Selection](https://arxiv.org/abs/2512.10451)
*Le Tuan Minh Trinh,Le Minh Vu Pham,Thi Minh Anh Pham,An Duc Nguyen*

Main category: cs.LG

TL;DR: 提出基于人类元认知科学的新框架，使用meta-d'衡量AI模型的元认知敏感性，并利用该动态分数进行测试时模型选择，提高联合推理准确性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型虽然能表达预测置信度，但往往存在校准不良的问题（置信度不能反映真实能力）。受人类认知科学启发，需要评估和利用AI的元认知能力，回答"模型是否真正知道它们知道什么"的问题。

Method: 1. 引入心理学基础的meta-d'指标来衡量模型元认知敏感性（置信度预测自身准确性的可靠性）。2. 使用该动态敏感性分数作为上下文，构建基于多臂赌博机的仲裁器，在测试时学习信任哪个专家模型。3. 在多个数据集和深度学习模型组合（包括CNN和VLM）上进行实验验证。

Result: 实验表明，这种元认知方法在多个数据集和模型组合上，相比组成模型提高了联合推理准确性。为AI模型提供了新颖的行为描述框架。

Conclusion: 将集成选择问题重新定义为同时评估短期信号（置信度预测分数）和中期特征（元认知敏感性）的问题，为AI模型评估提供了基于认知科学的新视角。

Abstract: A key aspect of human cognition is metacognition - the ability to assess one's own knowledge and judgment reliability. While deep learning models can express confidence in their predictions, they often suffer from poor calibration, a cognitive bias where expressed confidence does not reflect true competence. Do models truly know what they know? Drawing from human cognitive science, we propose a new framework for evaluating and leveraging AI metacognition. We introduce meta-d', a psychologically-grounded measure of metacognitive sensitivity, to characterise how reliably a model's confidence predicts its own accuracy. We then use this dynamic sensitivity score as context for a bandit-based arbiter that performs test-time model selection, learning which of several expert models to trust for a given task. Our experiments across multiple datasets and deep learning model combinations (including CNNs and VLMs) demonstrate that this metacognitive approach improves joint-inference accuracy over constituent models. This work provides a novel behavioural account of AI models, recasting ensemble selection as a problem of evaluating both short-term signals (confidence prediction scores) and medium-term traits (metacognitive sensitivity).

</details>


### [169] [UACER: An Uncertainty-Aware Critic Ensemble Framework for Robust Adversarial Reinforcement Learning](https://arxiv.org/abs/2512.10492)
*Jiaxi Wu,Tiantian Zhang,Yuxing Wang,Yongzhe Chang,Xueqian Wang*

Main category: cs.LG

TL;DR: UACER提出了一种用于鲁棒对抗强化学习的新方法，通过多样化评论家集成和时间衰减不确定性机制来解决对抗训练中的非平稳性和不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 鲁棒对抗强化学习在自动驾驶和机器人控制等顺序决策领域有重要应用，但可训练的对抗者会导致学习动态的非平稳性，加剧训练不稳定性和收敛困难，特别是在高维复杂环境中。

Method: 提出UACER方法，包含两个策略：1) 多样化评论家集成：使用K个并行评论家网络稳定Q值估计，减少方差并增强鲁棒性；2) 时间衰减不确定性机制：基于方差的Q值聚合策略，显式结合认知不确定性来动态调节探索-利用权衡，同时稳定训练过程。

Result: 在多个MuJoCo控制问题上进行的综合实验验证了UACER的优越有效性，在整体性能、稳定性和效率方面优于最先进的方法。

Conclusion: UACER通过集成多样化的评论家网络和创新的不确定性机制，有效解决了对抗强化学习中的训练不稳定问题，为鲁棒策略学习提供了有效的解决方案。

Abstract: Robust adversarial reinforcement learning has emerged as an effective paradigm for training agents to handle uncertain disturbance in real environments, with critical applications in sequential decision-making domains such as autonomous driving and robotic control. Within this paradigm, agent training is typically formulated as a zero-sum Markov game between a protagonist and an adversary to enhance policy robustness. However, the trainable nature of the adversary inevitably induces non-stationarity in the learning dynamics, leading to exacerbated training instability and convergence difficulties, particularly in high-dimensional complex environments. In this paper, we propose a novel approach, Uncertainty-Aware Critic Ensemble for robust adversarial Reinforcement learning (UACER), which consists of two strategies: 1) Diversified critic ensemble: a diverse set of K critic networks is exploited in parallel to stabilize Q-value estimation rather than conventional single-critic architectures for both variance reduction and robustness enhancement. 2) Time-varying Decay Uncertainty (TDU) mechanism: advancing beyond simple linear combinations, we develop a variance-derived Q-value aggregation strategy that explicitly incorporates epistemic uncertainty to dynamically regulate the exploration-exploitation trade-off while simultaneously stabilizing the training process. Comprehensive experiments across several MuJoCo control problems validate the superior effectiveness of UACER, outperforming state-of-the-art methods in terms of overall performance, stability, and efficiency.

</details>


### [170] [Adaptive Replay Buffer for Offline-to-Online Reinforcement Learning](https://arxiv.org/abs/2512.10510)
*Chihyeon Song,Jaewoo Lee,Jinkyoo Park*

Main category: cs.LG

TL;DR: 本文提出自适应回放缓冲区（ARB），通过动态调整离线与在线数据采样权重来解决离线到在线强化学习中的平衡问题，无需复杂学习过程即可提升算法性能。


<details>
  <summary>Details</summary>
Motivation: 离线到在线强化学习面临关键困境：如何在固定离线数据集和新收集的在线经验之间取得平衡。标准方法通常依赖固定的数据混合比例，难以在早期学习稳定性和渐进性能之间进行权衡。

Method: 提出自适应回放缓冲区（ARB），这是一种基于轻量级指标"on-policyness"的动态数据采样优先级方法。ARB无需复杂学习过程，易于实现，可无缝集成到现有O2O RL算法中。它评估收集的轨迹与当前策略行为的接近程度，并为该轨迹中的每个转换分配相应的采样权重。

Result: 在D4RL基准测试上的广泛实验表明，ARB能持续缓解早期性能下降问题，并显著提高各种O2O RL算法的最终性能。

Conclusion: 自适应、行为感知的回放缓冲区设计对于离线到在线强化学习至关重要，ARB通过简单有效的方式解决了数据采样平衡问题，为O2O RL算法提供了性能提升。

Abstract: Offline-to-Online Reinforcement Learning (O2O RL) faces a critical dilemma in balancing the use of a fixed offline dataset with newly collected online experiences. Standard methods, often relying on a fixed data-mixing ratio, struggle to manage the trade-off between early learning stability and asymptotic performance. To overcome this, we introduce the Adaptive Replay Buffer (ARB), a novel approach that dynamically prioritizes data sampling based on a lightweight metric we call 'on-policyness'. Unlike prior methods that rely on complex learning procedures or fixed ratios, ARB is designed to be learning-free and simple to implement, seamlessly integrating into existing O2O RL algorithms. It assesses how closely collected trajectories align with the current policy's behavior and assigns a proportional sampling weight to each transition within that trajectory. This strategy effectively leverages offline data for initial stability while progressively focusing learning on the most relevant, high-rewarding online experiences. Our extensive experiments on D4RL benchmarks demonstrate that ARB consistently mitigates early performance degradation and significantly improves the final performance of various O2O RL algorithms, highlighting the importance of an adaptive, behavior-aware replay buffer design.

</details>


### [171] [Asynchronous Reasoning: Training-Free Interactive Thinking LLMs](https://arxiv.org/abs/2512.10931)
*George Yakushev,Nataliia Babina,Masoud Vahid Dastgerdi,Vyacheslav Zhdanovskiy,Alina Shutova,Denis Kuznedelev*

Main category: cs.LG

TL;DR: 提出一种无需额外训练的方法，让具备推理能力的LLM能够异步地思考、倾听和生成输出，显著减少实时交互延迟


<details>
  <summary>Details</summary>
Motivation: 现有LLM需要先完成思考才能给出答案，这种顺序交互方式不适合需要实时响应和适应额外信息的语音助手等应用场景。人类可以异步地倾听、思考和行动，论文希望让LLM也能实现类似的异步能力

Method: 利用旋转嵌入（rotary embeddings）的特性，使原本设计用于顺序交互的LLM能够同时进行思考、倾听和生成输出，无需额外训练

Result: 在数学、常识和安全推理任务上评估，方法能够实时生成准确的思考增强答案，将第一个非思考token的生成时间从几分钟减少到≤5秒，整体实时延迟降低6-11倍

Conclusion: 通过利用旋转嵌入的特性，成功让LLM实现了类似人类的异步思考能力，显著提升了实时交互性能，为语音助手等需要实时响应的应用场景提供了可行的解决方案

Abstract: Many state-of-the-art LLMs are trained to think before giving their answer. Reasoning can greatly improve language model capabilities and safety, but it also makes them less interactive: given a new input, a model must stop thinking before it can respond. Real-world use cases such as voice-based or embedded assistants require an LLM agent to respond and adapt to additional information in real time, which is incompatible with sequential interactions. In contrast, humans can listen, think, and act asynchronously: we begin thinking about the problem while reading it and continue thinking while formulating the answer. In this work, we augment LLMs capable of reasoning to operate in a similar way without additional training. Our method uses the properties of rotary embeddings to enable LLMs built for sequential interactions to simultaneously think, listen, and generate outputs. We evaluate our approach on math, commonsense, and safety reasoning and find that it can generate accurate thinking-augmented answers in real time, reducing time to first non-thinking token from minutes to <= 5s. and the overall real-time delays by 6-11x.

</details>


### [172] [Disentangled and Distilled Encoder for Out-of-Distribution Reasoning with Rademacher Guarantees](https://arxiv.org/abs/2512.10522)
*Zahra Rahiminasab,Michael Yuhas,Arvind Easwaran*

Main category: cs.LG

TL;DR: 提出DDE框架，通过师生蒸馏压缩模型，在保持解耦性的同时减小OOD推理器尺寸，适用于资源受限设备部署


<details>
  <summary>Details</summary>
Motivation: 现有基于VAE解耦潜空间的OOD推理器模型较大，难以部署在资源受限设备上，需要压缩模型同时保持解耦性

Method: 提出解耦蒸馏编码器(DDE)框架，将师生蒸馏形式化为带约束的优化问题，通过解耦约束保持解耦性，并基于Rademacher复杂度建立理论保证

Result: 在NVIDIA设备上实证评估，成功压缩模型尺寸同时保持解耦性，适用于资源受限部署

Conclusion: DDE框架能有效压缩OOD推理器模型，保持解耦性，为资源受限设备部署提供可行方案

Abstract: Recently, the disentangled latent space of a variational autoencoder (VAE) has been used to reason about multi-label out-of-distribution (OOD) test samples that are derived from different distributions than training samples. Disentangled latent space means having one-to-many maps between latent dimensions and generative factors or important characteristics of an image. This paper proposes a disentangled distilled encoder (DDE) framework to decrease the OOD reasoner size for deployment on resource-constrained devices while preserving disentanglement. DDE formalizes student-teacher distillation for model compression as a constrained optimization problem while preserving disentanglement with disentanglement constraints. Theoretical guarantees for disentanglement during distillation based on Rademacher complexity are established. The approach is evaluated empirically by deploying the compressed model on an NVIDIA

</details>


### [173] [Stronger Normalization-Free Transformers](https://arxiv.org/abs/2512.10938)
*Mingzhi Chen,Taiming Lu,Jiachen Zhu,Mingjie Sun,Zhuang Liu*

Main category: cs.LG

TL;DR: 本文提出了一种新的点状函数Derf（erf函数的缩放版本），用于替代传统的归一化层，在多种任务中超越了LayerNorm、RMSNorm和DyT等现有方法。


<details>
  <summary>Details</summary>
Motivation: 虽然归一化层长期以来被视为深度学习架构中不可或缺的组件，但最近提出的Dynamic Tanh（DyT）表明存在替代方案。DyT通过约束极端值实现稳定收敛并达到归一化级别的性能，本研究旨在寻找能够超越DyT的函数设计。

Method: 首先研究点状函数的内在特性如何影响训练和性能，基于这些发现进行大规模搜索以寻找更有效的函数设计。最终提出Derf函数：Derf(x) = erf(αx + s)，其中erf(x)是重新缩放的高斯累积分布函数。

Result: Derf在多个领域超越了LayerNorm、RMSNorm和DyT，包括视觉（图像识别和生成）、语音表示和DNA序列建模。性能提升主要源于改进的泛化能力而非更强的拟合能力。

Conclusion: Derf的简单性和更强的性能使其成为无归一化Transformer架构的实用选择，为深度学习架构设计提供了新的方向。

Abstract: Although normalization layers have long been viewed as indispensable components of deep learning architectures, the recent introduction of Dynamic Tanh (DyT) has demonstrated that alternatives are possible. The point-wise function DyT constrains extreme values for stable convergence and reaches normalization-level performance; this work seeks further for function designs that can surpass it. We first study how the intrinsic properties of point-wise functions influence training and performance. Building on these findings, we conduct a large-scale search for a more effective function design. Through this exploration, we introduce $\mathrm{Derf}(x) = \mathrm{erf}(αx + s)$, where $\mathrm{erf}(x)$ is the rescaled Gaussian cumulative distribution function, and identify it as the most performant design. Derf outperforms LayerNorm, RMSNorm, and DyT across a wide range of domains, including vision (image recognition and generation), speech representation, and DNA sequence modeling. Our findings suggest that the performance gains of Derf largely stem from its improved generalization rather than stronger fitting capacity. Its simplicity and stronger performance make Derf a practical choice for normalization-free Transformer architectures.

</details>


### [174] [Mode-Seeking for Inverse Problems with Diffusion Models](https://arxiv.org/abs/2512.10524)
*Sai Bharath Chandra Gutha,Ricardo Vinuesa,Hossein Azizpour*

Main category: cs.LG

TL;DR: 提出VML损失函数，通过最小化扩散后验与测量后验的KL散度，引导生成样本趋向MAP估计，无需任务特定训练，在计算效率和性能上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于预训练扩散模型的后验采样和MAP估计方法存在建模近似和计算复杂度高的问题，需要更高效准确的逆问题求解方法。

Method: 提出变分模式寻求损失(VML)，通过最小化扩散后验p(x₀|xₜ)与测量后验p(x₀|y)的KL散度，在每个反向扩散步骤中引导样本趋向MAP估计。对于线性逆问题，VML可解析推导无需近似。

Result: 提出的VML-MAP算法在多个数据集的不同图像恢复任务中，在性能和计算时间上均优于现有方法。

Conclusion: VML提供了一种无需任务特定训练的高效逆问题求解框架，通过理论推导和实验验证了其在计算效率和恢复质量上的优势。

Abstract: A pre-trained unconditional diffusion model, combined with posterior sampling or maximum a posteriori (MAP) estimation techniques, can solve arbitrary inverse problems without task-specific training or fine-tuning. However, existing posterior sampling and MAP estimation methods often rely on modeling approximations and can be computationally demanding. In this work, we propose the variational mode-seeking loss (VML), which, when minimized during each reverse diffusion step, guides the generated sample towards the MAP estimate. VML arises from a novel perspective of minimizing the Kullback-Leibler (KL) divergence between the diffusion posterior $p(\mathbf{x}_0|\mathbf{x}_t)$ and the measurement posterior $p(\mathbf{x}_0|\mathbf{y})$, where $\mathbf{y}$ denotes the measurement. Importantly, for linear inverse problems, VML can be analytically derived and need not be approximated. Based on further theoretical insights, we propose VML-MAP, an empirically effective algorithm for solving inverse problems, and validate its efficacy over existing methods in both performance and computational time, through extensive experiments on diverse image-restoration tasks across multiple datasets.

</details>


### [175] [Unlocking the Address Book: Dissecting the Sparse Semantic Structure of LLM Key-Value Caches via Sparse Autoencoders](https://arxiv.org/abs/2512.10547)
*Qingsen Ma,Dianyun Wang,Jiaming Lyu,Yaoye Wang,Lechen Ning,Sujie Zhu,Zhenbo Xu,Liuyu Xiang,Huining Li,Huijia Wu,Zhaofeng He*

Main category: cs.LG

TL;DR: STA-Attention框架使用Top-K稀疏自编码器将KV缓存分解为可解释的语义原子，通过双预算策略选择性保留关键语义成分，在保持模型性能的同时提升KV缓存的可解释性。


<details>
  <summary>Details</summary>
Motivation: KV缓存是长上下文大语言模型的主要内存瓶颈，但通常被视为不透明的数值张量。需要一种方法来分解KV缓存以获得可解释性，同时保持注意力机制所需的精确点积几何结构。

Method: 提出STA-Attention框架，使用Top-K稀疏自编码器分解KV缓存为语义原子。与传统L1正则化SAE不同，Top-K方法消除收缩偏差。基于发现的Key-Value不对称性（Key作为稀疏路由器，Value携带密集内容），引入双预算策略选择性保留最有信息的语义成分。

Result: 在Yi-6B、Mistral-7B、Qwen2.5-32B等模型上的实验表明，语义重建保持了与原始模型相当的困惑度和零样本性能，有效弥合了机制可解释性与忠实注意力建模之间的差距。

Conclusion: STA-Attention成功将KV缓存分解为可解释的语义原子，揭示了Key-Value不对称性，并通过双预算策略实现了性能保持的语义压缩，为长上下文大语言模型的KV缓存优化提供了新思路。

Abstract: The Key-Value (KV) cache is the primary memory bottleneck in long-context Large Language Models, yet it is typically treated as an opaque numerical tensor. In this work, we propose \textbf{STA-Attention}, a framework that utilizes Top-K Sparse Autoencoders (SAEs) to decompose the KV cache into interpretable ``semantic atoms.'' Unlike standard $L_1$-regularized SAEs, our Top-K approach eliminates shrinkage bias, preserving the precise dot-product geometry required for attention. Our analysis uncovers a fundamental \textbf{Key-Value Asymmetry}: while Key vectors serve as highly sparse routers dominated by a ``Semantic Elbow,'' deep Value vectors carry dense content payloads requiring a larger budget. Based on this structure, we introduce a Dual-Budget Strategy that selectively preserves the most informative semantic components while filtering representational noise. Experiments on Yi-6B, Mistral-7B, Qwen2.5-32B, and others show that our semantic reconstructions maintain perplexity and zero-shot performance comparable to the original models, effectively bridging the gap between mechanistic interpretability and faithful attention modeling.

</details>


### [176] [Is the Information Bottleneck Robust Enough? Towards Label-Noise Resistant Information Bottleneck Learning](https://arxiv.org/abs/2512.10573)
*Yi Huang,Qingyun Sun,Yisen Gao,Haonan Yuan,Xingcheng Fu,Jianxin Li*

Main category: cs.LG

TL;DR: LaT-IB是一种抗标签噪声的信息瓶颈方法，通过"最小充分干净"准则和噪声感知潜在解缠，在保留任务相关信息的同时丢弃噪声，解决了标准IB对标签噪声的脆弱性问题。


<details>
  <summary>Details</summary>
Motivation: 信息瓶颈(IB)原理在表示学习中很有效，但它严重依赖准确的标签，因此在现实世界中普遍存在的标签噪声场景下容易导致性能下降和过拟合。需要一种能够抵抗标签噪声的IB方法。

Method: 提出LaT-IB方法，引入"最小充分干净"准则作为互信息正则化器。采用噪声感知潜在解缠，将潜在表示分解为与干净标签空间和噪声空间对齐的组件。设计三阶段训练框架：预热、知识注入和鲁棒训练。

Result: LaT-IB在标签噪声下实现了优越的鲁棒性和效率，显著增强了在现实标签噪声场景下的鲁棒性和适用性。

Conclusion: LaT-IB通过创新的噪声感知解缠和渐进式训练框架，有效解决了信息瓶颈对标签噪声的脆弱性问题，为现实世界中的噪声标签场景提供了实用的解决方案。

Abstract: The Information Bottleneck (IB) principle facilitates effective representation learning by preserving label-relevant information while compressing irrelevant information. However, its strong reliance on accurate labels makes it inherently vulnerable to label noise, prevalent in real-world scenarios, resulting in significant performance degradation and overfitting. To address this issue, we propose LaT-IB, a novel Label-Noise ResistanT Information Bottleneck method which introduces a "Minimal-Sufficient-Clean" (MSC) criterion. Instantiated as a mutual information regularizer to retain task-relevant information while discarding noise, MSC addresses standard IB's vulnerability to noisy label supervision. To achieve this, LaT-IB employs a noise-aware latent disentanglement that decomposes the latent representation into components aligned with to the clean label space and the noise space. Theoretically, we first derive mutual information bounds for each component of our objective including prediction, compression, and disentanglement, and moreover prove that optimizing it encourages representations invariant to input noise and separates clean and noisy label information. Furthermore, we design a three-phase training framework: Warmup, Knowledge Injection and Robust Training, to progressively guide the model toward noise-resistant representations. Extensive experiments demonstrate that LaT-IB achieves superior robustness and efficiency under label noise, significantly enhancing robustness and applicability in real-world scenarios with label noise.

</details>


### [177] [THeGAU: Type-Aware Heterogeneous Graph Autoencoder and Augmentation](https://arxiv.org/abs/2512.10589)
*Ming-Yi Hong,Miao-Chen Chiang,Youchen Teng,Yu-Hsiang Wang,Chih-Yu Wang,Che Lin*

Main category: cs.LG

TL;DR: THeGAU是一个模型无关的异构图神经网络框架，通过类型感知图自编码器和引导图增强来提升节点分类性能，解决类型信息丢失和结构噪声问题。


<details>
  <summary>Details</summary>
Motivation: 现有的异构图神经网络（HGNNs）在处理异构信息网络（HINs）时存在类型信息丢失和结构噪声问题，这限制了模型的表示保真度和泛化能力。

Method: 提出THeGAU框架，包含两个核心组件：1）类型感知图自编码器，通过重构符合模式的有效边来保持节点类型语义；2）解码器驱动的增强机制，选择性地精炼噪声结构。

Result: 在三个基准HIN数据集（IMDB、ACM、DBLP）上的大量实验表明，THeGAU在多个骨干网络上持续优于现有HGNN方法，达到了最先进的性能。

Conclusion: THeGAU通过联合设计增强了鲁棒性、准确性和效率，同时显著降低了计算开销，为异构图神经网络提供了一种有效的模型无关改进框架。

Abstract: Heterogeneous Graph Neural Networks (HGNNs) are effective for modeling Heterogeneous Information Networks (HINs), which encode complex multi-typed entities and relations. However, HGNNs often suffer from type information loss and structural noise, limiting their representational fidelity and generalization. We propose THeGAU, a model-agnostic framework that combines a type-aware graph autoencoder with guided graph augmentation to improve node classification. THeGAU reconstructs schema-valid edges as an auxiliary task to preserve node-type semantics and introduces a decoder-driven augmentation mechanism to selectively refine noisy structures. This joint design enhances robustness, accuracy, and efficiency while significantly reducing computational overhead. Extensive experiments on three benchmark HIN datasets (IMDB, ACM, and DBLP) demonstrate that THeGAU consistently outperforms existing HGNN methods, achieving state-of-the-art performance across multiple backbones.

</details>


### [178] [Multi-Objective Reward and Preference Optimization: Theory and Algorithms](https://arxiv.org/abs/2512.10601)
*Akhil Agnihotri*

Main category: cs.LG

TL;DR: 该论文在约束强化学习领域提出了一系列理论框架和算法，涵盖平均成本CMDPs、有限时域CMDPs、人类偏好学习以及大语言模型对齐，提供了具有理论保证的实用工具。


<details>
  <summary>Details</summary>
Motivation: 当前约束强化学习在控制、偏好学习和模型对齐等应用中面临挑战，需要统一的框架来处理不同范式下的约束问题，同时保证理论严谨性和实际可扩展性。

Method: 1) ACPO算法：结合敏感度分析和信任域更新处理平均成本CMDPs；2) e-COP算法：基于时域策略差异引理处理有限时域CMDPs；3) warmPref-PS：在线性赌博机中集成异构评分者的离线偏好数据；4) PSPL：从成对轨迹比较中联合采样奖励模型和转移动态；5) MOPO：多目标约束优化方法用于大语言模型对齐。

Result: ACPO在平均成本CMDPs中达到最先进的实证性能；e-COP在安全关键环境中提供可证明的性能和可扩展性；warmPref-PS显著减少遗憾并提高数据收集效率；PSPL提供贝叶斯简单遗憾保证；MOPO可扩展到数十亿参数的语言模型并在不同对齐设置中保持鲁棒性。

Conclusion: 该论文统一了平均成本、时域和偏好驱动的约束强化学习范式，为安全和对齐的决策制定提供了理论进展和实用工具，推动了约束RL在控制、偏好学习和模型对齐等领域的应用。

Abstract: This thesis develops theoretical frameworks and algorithms that advance constrained reinforcement learning (RL) across control, preference learning, and alignment of large language models. The first contribution addresses constrained Markov Decision Processes (CMDPs) under the average-cost criterion through the Average-Constrained Policy Optimization (ACPO) algorithm. ACPO integrates sensitivity analysis with trust-region updates to ensure stable constraint handling, achieving state-of-the-art empirical performance with theoretical guarantees. Constrained RL is then extended to finite-horizon settings via e-COP, the first policy optimization method for episodic CMDPs. Built on an episodic policy difference lemma, e-COP offers provable performance, simplicity, and scalability in safety-critical environments. The thesis then investigates reinforcement learning from human preferences. warmPref-PS introduces a posterior sampling strategy for linear bandits that integrates offline preference data from heterogeneous raters into online learning. Explicit modeling of rater competence yields substantial regret reduction and more efficient data collection for RLHF. The PSPL algorithm further advances preference-based RL by jointly sampling reward models and transition dynamics from pairwise trajectory comparisons, providing Bayesian simple-regret guarantees and robust empirical identification of optimal policies. The final contribution applies these methods to large-scale model alignment. A multi-objective constrained optimization view yields MOPO, an iterative algorithm with closed-form updates that scales to multi-billion-parameter language models and remains robust across alignment settings. Collectively, the thesis unifies constrained RL across average-cost, episodic, and preference-driven paradigms, delivering theoretical advances and practical tools for safe and aligned decision-making.

</details>


### [179] [Uncertainty-Preserving QBNNs: Multi-Level Quantization of SVI-Based Bayesian Neural Networks for Image Classification](https://arxiv.org/abs/2512.10602)
*Hendrik Borras,Yong Wu,Bernhard Klein,Holger Fröning*

Main category: cs.LG

TL;DR: 本文提出了一种贝叶斯神经网络的多级量化框架，能够在4位精度下保持分类准确性和不确定性估计能力，实现8倍内存减少。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯神经网络（BNNs）能够提供原则性的不确定性量化，但相比确定性网络存在显著的计算和内存开销。虽然量化技术已成功减少标准深度学习模型的资源需求，但在概率模型中的应用仍未被充分探索。

Method: 提出了基于随机变分推断BNNs的系统性多级量化框架，包含三种量化策略：变分参数量化（VPQ）、采样参数量化（SPQ）和联合量化（JQ）。采用方差参数的对数量化和专门的激活函数来保持分布结构。

Result: 在Dirty-MNIST上的实验表明，BNNs可以量化到4位精度，同时保持分类准确性和不确定性解耦能力。在4位精度下，联合量化相比浮点实现实现了高达8倍的内存减少，且认知和偶然不确定性估计仅有最小退化。

Conclusion: 这些结果使得BNNs能够部署在资源受限的边缘设备上，并为未来在固有低精度下运行的模拟"贝叶斯机器"提供了设计指南。

Abstract: Bayesian Neural Networks (BNNs) provide principled uncertainty quantification but suffer from substantial computational and memory overhead compared to deterministic networks. While quantization techniques have successfully reduced resource requirements in standard deep learning models, their application to probabilistic models remains largely unexplored. We introduce a systematic multi-level quantization framework for Stochastic Variational Inference based BNNs that distinguishes between three quantization strategies: Variational Parameter Quantization (VPQ), Sampled Parameter Quantization (SPQ), and Joint Quantization (JQ). Our logarithmic quantization for variance parameters, and specialized activation functions to preserve the distributional structure are essential for calibrated uncertainty estimation. Through comprehensive experiments on Dirty-MNIST, we demonstrate that BNNs can be quantized down to 4-bit precision while maintaining both classification accuracy and uncertainty disentanglement. At 4 bits, Joint Quantization achieves up to 8x memory reduction compared to floating-point implementations with minimal degradation in epistemic and aleatoric uncertainty estimation. These results enable deployment of BNNs on resource-constrained edge devices and provide design guidelines for future analog "Bayesian Machines" operating at inherently low precision.

</details>


### [180] [Supporting Migration Policies with Forecasts: Illegal Border Crossings in Europe through a Mixed Approach](https://arxiv.org/abs/2512.10633)
*C. Bosco,U. Minora,D. de Rigo,J. Pingsdorf,R. Cortinovis*

Main category: cs.LG

TL;DR: 提出一种混合方法预测欧洲非法越境，结合机器学习与专家定性分析，为欧盟移民政策提供一年期预测支持


<details>
  <summary>Details</summary>
Motivation: 应对移民模式突然变化和传统数据局限性的挑战，满足欧盟《移民与庇护公约》的预测需求，支持《庇护与移民管理法规》实施

Method: 混合方法整合机器学习技术与移民专家定性洞察，创新性地引入人工评估协变量，提升数据驱动模型的预测能力

Result: 方法经过已知数据测试验证，证明其在移民政策背景下的适用性和可靠性，能够提供政策相关的预测

Conclusion: 该研究结合数据驱动建模与专家判断，符合学术建议，为欧盟移民治理提供了新颖的操作工具，支持战略决策、预警系统和成员国团结机制

Abstract: This paper presents a mixed-methodology to forecast illegal border crossings in Europe across five key migratory routes, with a one-year time horizon. The methodology integrates machine learning techniques with qualitative insights from migration experts. This approach aims at improving the predictive capacity of data-driven models through the inclusion of a human-assessed covariate, an innovation that addresses challenges posed by sudden shifts in migration patterns and limitations in traditional datasets. The proposed methodology responds directly to the forecasting needs outlined in the EU Pact on Migration and Asylum, supporting the Asylum and Migration Management Regulation (AMMR). It is designed to provide policy-relevant forecasts that inform strategic decisions, early warning systems, and solidarity mechanisms among EU Member States. By joining data-driven modeling with expert judgment, this work aligns with existing academic recommendations and introduces a novel operational tool tailored for EU migration governance. The methodology is tested and validated with known data to demonstrate its applicability and reliability in migration-related policy context.

</details>


### [181] [Token Sample Complexity of Attention](https://arxiv.org/abs/2512.10656)
*Léa Bohbot,Cyril Letrouit,Gabriel Peyré,François-Xavier Vialard*

Main category: cs.LG

TL;DR: 论文研究了注意力机制在极长序列下的收敛行为，提出了token-sample complexity概念，分析了注意力映射在不同条件下的收敛速率。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型上下文窗口不断扩大，需要研究注意力在极长序列长度下的行为特征，特别是注意力计算如何随着token数量增加而收敛到其无限token极限。

Method: 引入token-sample complexity概念，从两个层面分析收敛性：1)注意力映射的点对点一致收敛；2)变换后token分布矩的收敛。针对紧支撑分布和亚高斯分布，推导有限n下的收敛界，并分析注意力参数趋于无穷时softmax接近hardmax的情况。

Result: 对于紧支撑分布，注意力映射在半径为R的球上以C(R)/√n速率一致收敛，其中C(R)随R指数增长。对于变换分布的矩收敛，速率为C'(R)/n^β，β<1/2，C'(R)与分布支撑大小呈多项式关系。在softmax接近hardmax时，收敛速率为对数级。实验在合成高斯数据和真实BERT模型上验证了理论预测。

Conclusion: 论文建立了注意力机制在长序列下的收敛理论框架，揭示了注意力映射和变换分布的收敛特性，为理解大语言模型在极长上下文中的行为提供了理论基础。

Abstract: As context windows in large language models continue to expand, it is essential to characterize how attention behaves at extreme sequence lengths. We introduce token-sample complexity: the rate at which attention computed on $n$ tokens converges to its infinite-token limit. We estimate finite-$n$ convergence bounds at two levels: pointwise uniform convergence of the attention map, and convergence of moments for the transformed token distribution. For compactly supported (and more generally sub-Gaussian) distributions, our first result shows that the attention map converges uniformly on a ball of radius $R$ at rate $C(R)/\sqrt{n}$, where $C(R)$ grows exponentially with $R$. For large $R$, this estimate loses practical value, and our second result addresses this issue by establishing convergence rates for the moments of the transformed distribution (the token output of the attention layer). In this case, the rate is $C'(R)/n^β$ with $β<\tfrac{1}{2}$, and $C'(R)$ depends polynomially on the size of the support of the distribution. The exponent $β$ depends on the attention geometry and the spectral properties of the tokens distribution. We also examine the regime in which the attention parameter tends to infinity and the softmax approaches a hardmax, and in this setting, we establish a logarithmic rate of convergence. Experiments on synthetic Gaussian data and real BERT models on Wikipedia text confirm our predictions.

</details>


### [182] [DCFO Additional Material](https://arxiv.org/abs/2512.10659)
*Tommaso Amico,Pernille Matthews,Lena Krieger,Arthur Zimek,Ira Assent*

Main category: cs.LG

TL;DR: DCFO是一种专门为LOF异常检测算法设计的反事实解释方法，通过数据空间分区实现梯度优化，在50个数据集上验证优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 异常检测中的解释至关重要，但现有反事实解释方法大多未针对经典异常检测算法（如LOF）设计，而LOF作为最流行的无监督异常检测方法之一缺乏可解释性。

Method: 提出DCFO方法，专门为LOF生成反事实解释。通过将数据空间划分为LOF行为平滑的区域，实现高效的基于梯度的优化。

Result: 在50个OpenML数据集上的广泛实验验证表明，DCFO在生成反事实的邻近性和有效性方面持续优于基准竞争对手。

Conclusion: DCFO成功解决了LOF异常检测算法的可解释性问题，为这一广泛使用的经典算法提供了有效的反事实解释框架。

Abstract: Outlier detection identifies data points that significantly deviate from the majority of the data distribution. Explaining outliers is crucial for understanding the underlying factors that contribute to their detection, validating their significance, and identifying potential biases or errors. Effective explanations provide actionable insights, facilitating preventive measures to avoid similar outliers in the future. Counterfactual explanations clarify why specific data points are classified as outliers by identifying minimal changes required to alter their prediction. Although valuable, most existing counterfactual explanation methods overlook the unique challenges posed by outlier detection, and fail to target classical, widely adopted outlier detection algorithms. Local Outlier Factor (LOF) is one the most popular unsupervised outlier detection methods, quantifying outlierness through relative local density. Despite LOF's widespread use across diverse applications, it lacks interpretability. To address this limitation, we introduce Density-based Counterfactuals for Outliers (DCFO), a novel method specifically designed to generate counterfactual explanations for LOF. DCFO partitions the data space into regions where LOF behaves smoothly, enabling efficient gradient-based optimisation. Extensive experimental validation on 50 OpenML datasets demonstrates that DCFO consistently outperforms benchmarked competitors, offering superior proximity and validity of generated counterfactuals.

</details>


### [183] [Learning by Analogy: A Causal Framework for Composition Generalization](https://arxiv.org/abs/2512.10669)
*Lingjing Kong,Shaoan Xie,Yang Jiao,Yetian Chen,Yanhui Guo,Simone Shao,Yan Gao,Guangyi Chen,Kun Zhang*

Main category: cs.LG

TL;DR: 论文提出基于因果模块性和最小变化原则的形式化框架，通过层次化数据生成过程实现组合泛化，证明潜在层次结构可识别，并在基准数据集上取得显著改进。


<details>
  <summary>Details</summary>
Motivation: 组合泛化能力使模型能够超越有限经验理解和生成新概念组合，但目前缺乏对其数据结构和原理的深入理解。论文旨在形式化人类类比推理过程，建立支持复杂概念关系的组合泛化理论框架。

Method: 引入基于因果模块性和最小变化原则的层次化数据生成过程，将高层概念分解为可跨相似上下文重组的低层基本概念。理论证明潜在层次结构可从文本-图像对等观测数据中可识别恢复。

Result: 理论证明该方法支持复杂概念关系的组合泛化，超越先前假设简单交互（如加性效应）的工作。在基准数据集上应用理论框架见解取得显著性能改进。

Conclusion: 通过形式化人类类比推理过程，论文建立了支持复杂概念交互的组合泛化理论框架，证明潜在层次结构可识别性，为理解和实现组合泛化提供了理论基础和实用方法。

Abstract: Compositional generalization -- the ability to understand and generate novel combinations of learned concepts -- enables models to extend their capabilities beyond limited experiences. While effective, the data structures and principles that enable this crucial capability remain poorly understood. We propose that compositional generalization fundamentally requires decomposing high-level concepts into basic, low-level concepts that can be recombined across similar contexts, similar to how humans draw analogies between concepts. For example, someone who has never seen a peacock eating rice can envision this scene by relating it to their previous observations of a chicken eating rice.
  In this work, we formalize these intuitive processes using principles of causal modularity and minimal changes. We introduce a hierarchical data-generating process that naturally encodes different levels of concepts and their interaction mechanisms. Theoretically, we demonstrate that this approach enables compositional generalization supporting complex relations between composed concepts, advancing beyond prior work that assumes simpler interactions like additive effects. Critically, we also prove that this latent hierarchical structure is provably recoverable (identifiable) from observable data like text-image pairs, a necessary step for learning such a generative process. To validate our theory, we apply insights from our theoretical framework and achieve significant improvements on benchmark datasets.

</details>


### [184] [HybridVFL: Disentangled Feature Learning for Edge-Enabled Vertical Federated Multimodal Classification](https://arxiv.org/abs/2512.10701)
*Mostafa Anoosha,Zeinab Dehghani,Kuniko Paxton,Koorosh Aslansefat,Dhavalkumar Thakker*

Main category: cs.LG

TL;DR: HybridVFL框架通过客户端特征解耦和服务器端跨模态Transformer融合，在垂直联邦学习中显著提升多模态数据融合性能


<details>
  <summary>Details</summary>
Motivation: 垂直联邦学习在移动健康诊断等边缘AI场景中具有隐私保护优势，但现有系统因简单的特征融合方法而性能受限，需要更先进的融合机制

Method: 提出HybridVFL框架：客户端进行特征解耦，服务器端使用跨模态Transformer进行上下文感知的特征融合

Result: 在多模态HAM10000皮肤病变数据集上的系统评估显示，HybridVFL显著优于标准联邦学习基线方法

Conclusion: 先进的融合机制对于构建鲁棒、隐私保护的垂直联邦学习系统至关重要，HybridVFL为此提供了有效解决方案

Abstract: Vertical Federated Learning (VFL) offers a privacy-preserving paradigm for Edge AI scenarios like mobile health diagnostics, where sensitive multimodal data reside on distributed, resource-constrained devices. Yet, standard VFL systems often suffer performance limitations due to simplistic feature fusion. This paper introduces HybridVFL, a novel framework designed to overcome this bottleneck by employing client-side feature disentanglement paired with a server-side cross-modal transformer for context-aware fusion. Through systematic evaluation on the multimodal HAM10000 skin lesion dataset, we demonstrate that HybridVFL significantly outperforms standard federated baselines, validating the criticality of advanced fusion mechanisms in robust, privacy-preserving systems.

</details>


### [185] [Beyond the Black Box: Identifiable Interpretation and Control in Generative Models via Causal Minimality](https://arxiv.org/abs/2512.10720)
*Lingjing Kong,Shaoan Xie,Guangyi Chen,Yuewen Sun,Xiangchen Song,Eric P. Xing,Kun Zhang*

Main category: cs.LG

TL;DR: 论文提出基于因果最小化原则的理论框架，为深度生成模型提供可解释的因果表示，实现组件级可识别控制。


<details>
  <summary>Details</summary>
Motivation: 当前深度生成模型（如扩散模型和自回归语言模型）作为黑箱运行，缺乏人类理解、控制和对齐的理论基础。现有方法如稀疏自编码器虽有经验成功，但缺乏理论保证，可能导致主观见解。

Method: 引入层次选择模型的理论框架，其中高层概念由低层变量约束组合而成。在理论推导的最小化条件（表现为稀疏性或压缩约束）下，学习到的表示可等价于数据生成过程的真实潜在变量。

Result: 将因果最小化约束应用于主流生成模型，能够提取其内在的层次概念图，揭示模型内部知识组织。这些因果基础概念可作为细粒度模型控制的杠杆。

Conclusion: 因果最小化原则为可解释生成模型提供了理论基础，使潜在表示具有清晰的因果解释和组件级可识别控制，为实现透明可靠系统铺平道路。

Abstract: Deep generative models, while revolutionizing fields like image and text generation, largely operate as opaque black boxes, hindering human understanding, control, and alignment. While methods like sparse autoencoders (SAEs) show remarkable empirical success, they often lack theoretical guarantees, risking subjective insights. Our primary objective is to establish a principled foundation for interpretable generative models. We demonstrate that the principle of causal minimality -- favoring the simplest causal explanation -- can endow the latent representations of diffusion vision and autoregressive language models with clear causal interpretation and robust, component-wise identifiable control. We introduce a novel theoretical framework for hierarchical selection models, where higher-level concepts emerge from the constrained composition of lower-level variables, better capturing the complex dependencies in data generation. Under theoretically derived minimality conditions (manifesting as sparsity or compression constraints), we show that learned representations can be equivalent to the true latent variables of the data-generating process. Empirically, applying these constraints to leading generative models allows us to extract their innate hierarchical concept graphs, offering fresh insights into their internal knowledge organization. Furthermore, these causally grounded concepts serve as levers for fine-grained model steering, paving the way for transparent, reliable systems.

</details>


### [186] [Generalized Spherical Neural Operators: Green's Function Formulation](https://arxiv.org/abs/2512.10723)
*Hao Tang,Hao Chen,Chao Li*

Main category: cs.LG

TL;DR: 提出基于可设计球面格林函数及其谐波展开的通用算子设计框架GSNO，用于解决球面域上的参数偏微分方程，平衡等变性和不变性，在多个实际应用中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有神经算子在球面域上存在挑战，需要保持内在几何结构同时避免扭曲破坏旋转一致性。现有球面算子依赖旋转等变性但缺乏处理现实世界复杂性的灵活性。

Method: 提出基于可设计球面格林函数及其谐波展开的算子设计框架，引入绝对和相对位置依赖的格林函数来平衡等变性和不变性。开发GSNO算子和GSHNet层次架构，结合多尺度谱建模与球面上下采样。

Result: 在扩散MRI、浅水动力学和全球天气预报等任务中，GSNO和GSHNet持续优于最先进方法，展示了在球面算子学习中的优越性能。

Conclusion: GSNO为球面算子学习提供了一个原则性和通用的框架，将严谨理论与现实世界复杂性相结合，为球面域上的参数偏微分方程求解提供了有效解决方案。

Abstract: Neural operators offer powerful approaches for solving parametric partial differential equations, but extending them to spherical domains remains challenging due to the need to preserve intrinsic geometry while avoiding distortions that break rotational consistency. Existing spherical operators rely on rotational equivariance but often lack the flexibility for real-world complexity. We propose a general operator-design framework based on the designable spherical Green's function and its harmonic expansion, establishing a solid operator-theoretic foundation for spherical learning. Based on this, we propose an absolute and relative position-dependent Green's function that enables flexible balance of equivariance and invariance for real-world modeling. The resulting operator, Green's-function Spherical Neural Operator (GSNO) with a novel spectral learning method, can adapt to anisotropic, constraint-rich systems while retaining spectral efficiency. To exploit GSNO, we develop GSHNet, a hierarchical architecture that combines multi-scale spectral modeling with spherical up-down sampling, enhancing global feature representation. Evaluations on diffusion MRI, shallow water dynamics, and global weather forecasting, GSNO and GSHNet consistently outperform state-of-the-art methods. Our results position GSNO as a principled and general framework for spherical operator learning, bridging rigorous theory with real-world complexity.

</details>


### [187] [LGAN: An Efficient High-Order Graph Neural Network via the Line Graph Aggregation](https://arxiv.org/abs/2512.10735)
*Lin Du,Lu Bai,Jincheng Li,Lixin Cui,Hangyuan Du,Lichi Zhang,Yuting Chen,Zhao Li*

Main category: cs.LG

TL;DR: LGAN通过构建以每个节点为中心的子图的线图进行高阶聚合，在表达能力上超越2-WL且计算复杂度更低，同时保持更好的可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有GNN主要依赖邻居节点间的消息传递，表达能力受限于1-WL测试。虽然k-WL-based GNN能克服此限制，但计算成本随k快速增加，且由于主要操作节点元组，无法保留细粒度节点/边级语义，导致可解释性较差。

Method: 提出线图聚合网络(LGAN)，从以每个节点为中心的诱导子图构建线图，执行高阶聚合。理论证明LGAN在单射聚合假设下比2-WL具有更强表达能力，且时间复杂度更低。

Result: 在基准测试上的实证评估表明，LGAN优于最先进的k-WL-based GNN，同时提供更好的可解释性。

Conclusion: LGAN成功解决了现有k-WL-based GNN的计算成本高和可解释性差的问题，在保持高效计算的同时提供了更强的表达能力和更好的可解释性。

Abstract: Graph Neural Networks (GNNs) have emerged as a dominant paradigm for graph classification. Specifically, most existing GNNs mainly rely on the message passing strategy between neighbor nodes, where the expressivity is limited by the 1-dimensional Weisfeiler-Lehman (1-WL) test. Although a number of k-WL-based GNNs have been proposed to overcome this limitation, their computational cost increases rapidly with k, significantly restricting the practical applicability. Moreover, since the k-WL models mainly operate on node tuples, these k-WL-based GNNs cannot retain fine-grained node- or edge-level semantics required by attribution methods (e.g., Integrated Gradients), leading to the less interpretable problem. To overcome the above shortcomings, in this paper, we propose a novel Line Graph Aggregation Network (LGAN), that constructs a line graph from the induced subgraph centered at each node to perform the higher-order aggregation. We theoretically prove that the LGAN not only possesses the greater expressive power than the 2-WL under injective aggregation assumptions, but also has lower time complexity. Empirical evaluations on benchmarks demonstrate that the LGAN outperforms state-of-the-art k-WL-based GNNs, while offering better interpretability.

</details>


### [188] [UniExtreme: A Universal Foundation Model for Extreme Weather Forecasting](https://arxiv.org/abs/2508.01426)
*Hang Ni,Weijia Zhang,Hao Liu*

Main category: cs.LG

TL;DR: UniExtreme是一个通用的极端天气预测基础模型，通过自适应频率调制和事件先验增强模块，解决了现有模型在预测多样化极端天气事件方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有天气预测基础模型主要关注一般天气条件或特定类型的极端事件，忽略了现实世界中多样化极端事件的大气模式，无法有效预测复合型和层次化的极端天气。

Method: 提出UniExtreme模型，包含两个核心模块：1) 自适应频率调制模块，通过可学习的Beta分布滤波器和多粒度频谱聚合捕捉正常与极端天气的区域频谱差异；2) 事件先验增强模块，通过双级记忆融合网络整合区域特定的极端事件先验，解决极端事件的层次多样性和复合模式。

Result: 大量实验表明，UniExtreme在极端天气和一般天气预测任务上都优于现有最先进基线模型，展现出在不同极端场景下的卓越适应能力。

Conclusion: UniExtreme通过整合频谱差异建模和事件先验增强，成功构建了一个通用的极端天气预测基础模型，能够有效处理多样化、复合型的极端天气事件，为极端天气预测提供了新的解决方案。

Abstract: Recent advancements in deep learning have led to the development of Foundation Models (FMs) for weather forecasting, yet their ability to predict extreme weather events remains limited. Existing approaches either focus on general weather conditions or specialize in specific-type extremes, neglecting the real-world atmospheric patterns of diversified extreme events. In this work, we identify two key characteristics of extreme events: (1) the spectral disparity against normal weather regimes, and (2) the hierarchical drivers and geographic blending of diverse extremes. Along this line, we propose UniExtreme, a universal extreme weather forecasting foundation model that integrates (1) an Adaptive Frequency Modulation (AFM) module that captures region-wise spectral differences between normal and extreme weather, through learnable Beta-distribution filters and multi-granularity spectral aggregation, and (2) an Event Prior Augmentation (EPA) module which incorporates region-specific extreme event priors to resolve hierarchical extreme diversity and composite extreme schema, via a dual-level memory fusion network. Extensive experiments demonstrate that UniExtreme outperforms state-of-the-art baselines in both extreme and general weather forecasting, showcasing superior adaptability across diverse extreme scenarios.

</details>


### [189] [Template-Free Retrosynthesis with Graph-Prior Augmented Transformers](https://arxiv.org/abs/2512.10770)
*Youjun Zhao*

Main category: cs.LG

TL;DR: 提出一种无模板的Transformer框架，通过将分子图信息注入注意力机制，结合SMILES序列和结构线索，并采用配对数据增强策略，在USPTO-50K基准上达到无模板方法的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 逆合成反应预测是有机合成计算机辅助的核心问题，现有模型在准确性和鲁棒性方面仍不足以满足实际部署需求。需要开发不依赖手工反应模板或额外化学规则引擎的模板自由方法。

Method: 采用无模板的Transformer框架，将分子图信息注入注意力机制，联合利用SMILES序列和结构线索，并应用配对数据增强策略以提高训练多样性和规模。

Result: 在USPTO-50K基准测试中，该方法在无模板方法中达到最先进性能，并显著优于基础的Transformer基线模型。

Conclusion: 提出的模板自由Transformer框架通过结合分子图信息和数据增强策略，有效提升了逆合成反应预测的准确性和鲁棒性，为实际应用提供了有前景的解决方案。

Abstract: Retrosynthesis reaction prediction seeks to infer plausible reactant molecules for a given product and is a central problem in computer-aided organic synthesis. Despite recent progress, many existing models still fall short of the accuracy and robustness required for practical deployment. This work studies a template-free, Transformer-based framework that eliminates reliance on handcrafted reaction templates or additional chemical rule engines. The model injects molecular graph information into the attention mechanism to jointly exploit \SMILES\ sequences and structural cues, and further applies a paired data augmentation strategy to enhance training diversity and scale. On the USPTO-50K benchmark, our proposed approach achieves state-of-the-art performance among template-free methods and substantially outperforming a vanilla Transformer baseline.

</details>


### [190] [Interpretable and Steerable Concept Bottleneck Sparse Autoencoders](https://arxiv.org/abs/2512.10805)
*Akshay Kulkarni,Tsui-Wei Weng,Vivek Narayanaswamy,Shusen Liu,Wesam A. Sakla,Kowshik Thopalli*

Main category: cs.LG

TL;DR: 论文提出了CB-SAE框架，通过概念瓶颈层增强稀疏自编码器，提升大型视觉语言模型的特征可解释性和可操控性。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏自编码器(SAE)在大型视觉语言模型(LVLMs)中存在两个主要问题：1）大部分神经元要么可解释性低，要么可操控性低，无法有效用于下游任务；2）由于无监督学习特性，用户所需的概念往往不在学习到的字典中，限制了实际应用价值。

Method: 提出概念瓶颈稀疏自编码器(CB-SAE)框架：1）剪枝低效用神经元；2）在潜在空间中添加轻量级概念瓶颈层，与用户定义的概念集对齐；3）引入两个计算成本低的可解释性和可操控性评估指标。

Result: CB-SAE在LVLMs和图像生成任务中，将可解释性提升32.1%，可操控性提升14.5%。

Conclusion: CB-SAE通过结合概念瓶颈和稀疏自编码器，有效解决了现有SAE在可解释性和可操控性方面的局限性，为模型解释、概念发现和模型操控提供了更实用的工具。

Abstract: Sparse autoencoders (SAEs) promise a unified approach for mechanistic interpretability, concept discovery, and model steering in LLMs and LVLMs. However, realizing this potential requires that the learned features be both interpretable and steerable. To that end, we introduce two new computationally inexpensive interpretability and steerability metrics and conduct a systematic analysis on LVLMs. Our analysis uncovers two observations; (i) a majority of SAE neurons exhibit either low interpretability or low steerability or both, rendering them ineffective for downstream use; and (ii) due to the unsupervised nature of SAEs, user-desired concepts are often absent in the learned dictionary, thus limiting their practical utility. To address these limitations, we propose Concept Bottleneck Sparse Autoencoders (CB-SAE) - a novel post-hoc framework that prunes low-utility neurons and augments the latent space with a lightweight concept bottleneck aligned to a user-defined concept set. The resulting CB-SAE improves interpretability by +32.1% and steerability by +14.5% across LVLMs and image generation tasks. We will make our code and model weights available.

</details>


### [191] [Learning Controllable and Diverse Player Behaviors in Multi-Agent Environments](https://arxiv.org/abs/2512.10835)
*Atahan Cilan,Atay Özgövde*

Main category: cs.LG

TL;DR: 提出一个强化学习框架，无需人类游戏数据即可生成可控且多样的玩家行为，通过行为向量空间实现平滑控制


<details>
  <summary>Details</summary>
Motivation: 现有方法需要大量玩家轨迹数据、需为不同玩家类型训练单独模型，或缺乏可解释行为参数与学习策略的直接映射，限制了可扩展性和可控性

Method: 在N维连续空间中定义玩家行为，从包含真实人类风格的区域均匀采样目标行为向量；训练时每个智能体接收当前和目标行为向量作为输入，奖励基于两者距离的归一化减少；使用PPO-based多智能体策略

Result: 在自定义多玩家Unity游戏中，该方法比仅追求胜利的基线产生显著更大的行为多样性，并能可靠匹配指定的行为向量；单个策略可重现新或未见过的游戏风格而无需重新训练

Conclusion: 该方法为自动化游戏测试、游戏平衡、人类行为模拟和在线游戏中替换断开连接的玩家提供了可扩展的解决方案

Abstract: This paper introduces a reinforcement learning framework that enables controllable and diverse player behaviors without relying on human gameplay data. Existing approaches often require large-scale player trajectories, train separate models for different player types, or provide no direct mapping between interpretable behavioral parameters and the learned policy, limiting their scalability and controllability. We define player behavior in an N-dimensional continuous space and uniformly sample target behavior vectors from a region that encompasses the subset representing real human styles. During training, each agent receives both its current and target behavior vectors as input, and the reward is based on the normalized reduction in distance between them. This allows the policy to learn how actions influence behavioral statistics, enabling smooth control over attributes such as aggressiveness, mobility, and cooperativeness. A single PPO-based multi-agent policy can reproduce new or unseen play styles without retraining. Experiments conducted in a custom multi-player Unity game show that the proposed framework produces significantly greater behavioral diversity than a win-only baseline and reliably matches specified behavior vectors across diverse targets. The method offers a scalable solution for automated playtesting, game balancing, human-like behavior simulation, and replacing disconnected players in online games.

</details>


### [192] [Bayesian Symbolic Regression via Posterior Sampling](https://arxiv.org/abs/2512.10849)
*Geoffrey F. Bomarito,Patrick E. Leser*

Main category: cs.LG

TL;DR: 提出基于序贯蒙特卡洛的贝叶斯符号回归框架，通过近似符号表达式的后验分布来增强噪声环境下的鲁棒性和不确定性量化能力。


<details>
  <summary>Details</summary>
Motivation: 符号回归是直接从数据中发现控制方程的有力工具，但对噪声的敏感性限制了其更广泛应用。需要开发更鲁棒的方法来处理噪声数据。

Method: 采用序贯蒙特卡洛框架进行贝叶斯符号回归，结合概率选择、自适应退火和使用归一化边际似然，高效探索符号表达式搜索空间。

Result: 相比传统遗传编程方法，该方法在处理噪声基准数据集方面表现更好，减少了过拟合倾向，能发现更准确、可解释的方程。

Conclusion: 该方法为科学发现和工程设计中更鲁棒的符号回归铺平了道路，通过后验分布近似实现了不确定性量化和改进的泛化能力。

Abstract: Symbolic regression is a powerful tool for discovering governing equations directly from data, but its sensitivity to noise hinders its broader application. This paper introduces a Sequential Monte Carlo (SMC) framework for Bayesian symbolic regression that approximates the posterior distribution over symbolic expressions, enhancing robustness and enabling uncertainty quantification for symbolic regression in the presence of noise. Differing from traditional genetic programming approaches, the SMC-based algorithm combines probabilistic selection, adaptive tempering, and the use of normalized marginal likelihood to efficiently explore the search space of symbolic expressions, yielding parsimonious expressions with improved generalization. When compared to standard genetic programming baselines, the proposed method better deals with challenging, noisy benchmark datasets. The reduced tendency to overfit and enhanced ability to discover accurate and interpretable equations paves the way for more robust symbolic regression in scientific discovery and engineering design applications.

</details>


### [193] [Scaling Behavior of Discrete Diffusion Language Models](https://arxiv.org/abs/2512.10858)
*Dimitri von Rütte,Janis Fluri,Omead Pooladzandi,Bernhard Schölkopf,Thomas Hofmann,Antonio Orvieto*

Main category: cs.LG

TL;DR: 本文研究了离散扩散语言模型（DLMs）在不同噪声类型下的缩放规律，发现均匀扩散在计算效率训练中比掩码扩散需要更多参数但更少数据，适合数据受限场景，并将均匀扩散模型扩展到100亿参数。


<details>
  <summary>Details</summary>
Motivation: 现代LLM预训练消耗大量计算资源和训练数据，不同模型的缩放规律成为关键区分因素。离散扩散语言模型作为自回归语言模型的替代方案，其缩放行为尚未被充分探索，先前研究表明DLMs需要更多数据和计算才能达到ALMs的性能。

Method: 通过平滑插值掩码扩散和均匀扩散来研究不同噪声类型下DLMs的缩放行为，重点关注批量大小和学习率等关键超参数。实验比较了不同噪声类型的计算效率和参数需求。

Result: DLMs的缩放行为强烈依赖于噪声类型，与ALMs有显著差异。在计算受限的缩放中，所有噪声类型都收敛到相似的损失值，但均匀扩散在计算效率训练中需要更多参数和更少数据，而掩码扩散则相反。研究将均匀扩散模型扩展到100亿参数，训练计算量达10^22 FLOPs。

Conclusion: 均匀扩散在数据受限场景中具有优势，是离散扩散语言模型的有前景候选方案。研究证实了预测的缩放规律，并创建了目前已知最大的公开均匀扩散模型。

Abstract: Modern LLM pre-training consumes vast amounts of compute and training data, making the scaling behavior, or scaling laws, of different models a key distinguishing factor. Discrete diffusion language models (DLMs) have been proposed as an alternative to autoregressive language models (ALMs). However, their scaling behavior has not yet been fully explored, with prior work suggesting that they require more data and compute to match the performance of ALMs.
  We study the scaling behavior of DLMs on different noise types by smoothly interpolating between masked and uniform diffusion while paying close attention to crucial hyperparameters such as batch size and learning rate. Our experiments reveal that the scaling behavior of DLMs strongly depends on the noise type and is considerably different from ALMs. While all noise types converge to similar loss values in compute-bound scaling, we find that uniform diffusion requires more parameters and less data for compute-efficient training compared to masked diffusion, making them a promising candidate in data-bound settings. We scale our uniform diffusion model up to 10B parameters trained for $10^{22}$ FLOPs, confirming the predicted scaling behavior and making it the largest publicly known uniform diffusion model to date.

</details>


### [194] [UrbanAI 2025 Challenge: Linear vs Transformer Models for Long-Horizon Exogenous Temperature Forecasting](https://arxiv.org/abs/2512.10866)
*Ruslan Gokhman*

Main category: cs.LG

TL;DR: 线性模型（特别是DLinear）在外生温度预测任务中优于Transformer架构，表明在仅使用历史值的挑战性场景下，精心设计的线性模型仍是强基线。


<details>
  <summary>Details</summary>
Motivation: 研究长期外生温度预测的挑战性场景，即仅使用室内温度历史值进行预测，评估不同模型在这种单变量设置下的表现。

Method: 使用线性模型（Linear、NLinear、DLinear）和Transformer家族模型（Transformer、Informer、Autoformer）在标准化的训练、验证和测试分割下进行对比评估。

Result: 线性基线模型（Linear、NLinear、DLinear）一致优于更复杂的Transformer架构，其中DLinear在所有分割上实现了最佳整体准确率。

Conclusion: 在仅使用外生变量的挑战性时间序列预测场景中，精心设计的线性模型仍然是强大的基线方法，不应被忽视。

Abstract: We study long-horizon exogenous-only temperature forecasting - a challenging univariate setting where only the past values of the indoor temperature are used for prediction - using linear and Transformer-family models. We evaluate Linear, NLinear, DLinear, Transformer, Informer, and Autoformer under standardized train, validation, and test splits. Results show that linear baselines (Linear, NLinear, DLinear) consistently outperform more complex Transformer-family architectures, with DLinear achieving the best overall accuracy across all splits. These findings highlight that carefully designed linear models remain strong baselines for time series forecasting in challenging exogenous-only settings.

</details>


### [195] [Guided Transfer Learning for Discrete Diffusion Models](https://arxiv.org/abs/2512.10877)
*Julian Kleutgens,Claudio Battiloro,Lingkai Kong,Benjamin Grewe,Francesca Dominici,Mauricio Tec*

Main category: cs.LG

TL;DR: 提出GTL方法，无需微调预训练离散扩散模型即可适应新领域，通过引导采样实现迁移学习，并开发高效采样器降低计算成本


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型在语言等离散领域表现优异，但需要大量训练数据，在新领域适应成本高。现有迁移学习方法需要微调整个模型，计算代价大且不实用

Method: 基于连续扩散的比率迁移学习，提出GTL方法，通过引导采样从目标分布生成样本而不修改预训练去噪器。方法适用于离散时间和连续时间离散扩散。进一步开发高效采样器，通过规划器选择位置和候选词来减少计算

Result: GTL在序列数据（包括合成马尔可夫链和语言建模）上评估有效，高效采样器使大规模语言建模在实际应用中可行

Conclusion: GTL为离散扩散模型提供了一种实用的迁移学习方法，无需微调预训练模型，通过引导采样和高效采样器解决了大规模词汇和长序列的计算挑战

Abstract: Discrete diffusion models achieve strong performance across language and other discrete domains, providing a powerful alternative to autoregressive models. However, their strong performance relies on large training datasets, which are costly or risky to obtain, especially when adapting to new domains. Transfer learning is the natural way to adapt pretrained discrete diffusion models, but current methods require fine-tuning large diffusion models, which is computationally expensive and often impractical. Building on ratio-based transfer learning for continuous diffusion, we provide Guided Transfer Learning for discrete diffusion models (GTL). This enables sampling from a target distribution without modifying the pretrained denoiser. The same guidance formulation applies to both discrete-time diffusion and continuous-time score-based discrete diffusion, yielding a unified treatment. Guided discrete diffusion often requires many forward passes of the guidance network, which becomes impractical for large vocabularies and long sequences. To address this, we further present an efficient guided sampler that concentrates evaluations on planner-selected positions and top candidate tokens, thus lowering sampling time and computation. This makes guided language modeling practical at scale for large vocabularies and long sequences. We evaluate GTL on sequential data, including synthetic Markov chains and language modeling, and provide empirical analyses of its behavior.

</details>


### [196] [Classifier Reconstruction Through Counterfactual-Aware Wasserstein Prototypes](https://arxiv.org/abs/2512.10878)
*Xuan Zhao,Zhuo Cao,Arya Bangun,Hanno Scharr,Ira Assent*

Main category: cs.LG

TL;DR: 论文提出一种利用反事实解释改进模型重构的方法，通过将原始数据与反事实样本结合，使用Wasserstein重心近似类别原型，从而提升代理模型的质量并缓解决策边界偏移问题。


<details>
  <summary>Details</summary>
Motivation: 反事实解释不仅能提供可解释性，还可用于模型重构。然而，反事实样本通常靠近决策边界，作为训练样本时代表性不足，可能导致决策边界偏移。特别是在标记数据有限的情况下，如何有效利用反事实样本来改进模型重构是一个重要问题。

Method: 提出一种将原始数据样本与反事实样本相结合的方法，使用Wasserstein重心来近似每个类别的原型，从而保留类别的基础分布结构。这种方法避免了将反事实样本简单视为普通训练实例的问题，提升了代理模型的质量。

Result: 在多个数据集上的实验结果表明，该方法显著提高了代理模型与目标模型之间的保真度，验证了其有效性。

Conclusion: 通过合理利用反事实样本作为信息性但代表性较低的样本，结合原始数据并使用Wasserstein重心近似类别原型，可以有效改进模型重构，缓解决策边界偏移问题，在标记数据有限的情况下尤其有益。

Abstract: Counterfactual explanations provide actionable insights by identifying minimal input changes required to achieve a desired model prediction. Beyond their interpretability benefits, counterfactuals can also be leveraged for model reconstruction, where a surrogate model is trained to replicate the behavior of a target model. In this work, we demonstrate that model reconstruction can be significantly improved by recognizing that counterfactuals, which typically lie close to the decision boundary, can serve as informative though less representative samples for both classes. This is particularly beneficial in settings with limited access to labeled data. We propose a method that integrates original data samples with counterfactuals to approximate class prototypes using the Wasserstein barycenter, thereby preserving the underlying distributional structure of each class. This approach enhances the quality of the surrogate model and mitigates the issue of decision boundary shift, which commonly arises when counterfactuals are naively treated as ordinary training instances. Empirical results across multiple datasets show that our method improves fidelity between the surrogate and target models, validating its effectiveness.

</details>


### [197] [Physics-Informed Learning of Flow Distribution and Receiver Heat Losses in Parabolic Trough Solar Fields](https://arxiv.org/abs/2512.10886)
*Stefan Matthes,Markus Schramm*

Main category: cs.LG

TL;DR: 提出基于物理信息学习的框架，从常规运行数据中推断槽式光热电站的回路质量流量比和接收器传热系数，利用夜间均质化过程分离水力与热损失效应。


<details>
  <summary>Details</summary>
Motivation: 槽式光热电站的液压网络需要均匀出口温度，但存在光学性能、热损失和压降的空间异质性。回路温度可测量，但回路级质量流量和接收器热损失参数无法直接观测，导致无法诊断水力不平衡或接收器退化。

Method: 提出物理信息学习框架，利用夜间均质化过程（热油在无辐照场中循环）分离水力与热损失效应。将可微分共轭传热模型离散化并嵌入端到端学习管道，使用Andasol 3电站50 MW太阳能场的历史数据进行优化。

Result: 模型准确重建回路温度（RMSE <2°C），产生物理上有意义的回路不平衡和接收器热损失估计。与基于无人机的红外热成像（QScan）对比显示强相关性，正确识别所有高损失接收器区域。

Conclusion: 嘈杂的真实世界光热电站运行数据包含足够信息来恢复潜在物理参数，当与适当的建模和可微分优化结合时。该方法为电站诊断和性能优化提供了新工具。

Abstract: Parabolic trough Concentrating Solar Power (CSP) plants operate large hydraulic networks of collector loops that must deliver a uniform outlet temperature despite spatially heterogeneous optical performance, heat losses, and pressure drops. While loop temperatures are measured, loop-level mass flows and receiver heat-loss parameters are unobserved, making it impossible to diagnose hydraulic imbalances or receiver degradation using standard monitoring tools.
  We present a physics-informed learning framework that infers (i) loop-level mass-flow ratios and (ii) time-varying receiver heat-transfer coefficients directly from routine operational data. The method exploits nocturnal homogenization periods -- when hot oil is circulated through a non-irradiated field -- to isolate hydraulic and thermal-loss effects. A differentiable conjugate heat-transfer model is discretized and embedded into an end-to-end learning pipeline optimized using historical plant data from the 50 MW Andasol 3 solar field.
  The model accurately reconstructs loop temperatures (RMSE $<2^\circ$C) and produces physically meaningful estimates of loop imbalances and receiver heat losses. Comparison against drone-based infrared thermography (QScan) shows strong correspondence, correctly identifying all areas with high-loss receivers. This demonstrates that noisy real-world CSP operational data contain enough information to recover latent physical parameters when combined with appropriate modeling and differentiable optimization.

</details>


### [198] [SparseSwaps: Tractable LLM Pruning Mask Refinement at Scale](https://arxiv.org/abs/2512.10922)
*Max Zimmer,Christophe Roux,Moritz Wagner,Deborah Hendrych,Sebastian Pokutta*

Main category: cs.LG

TL;DR: 提出一种高效的1-swap剪枝算法，通过行级等稀疏度约束和Gram矩阵计算，显著降低LLM剪枝误差，无需超参数调优


<details>
  <summary>Details</summary>
Motivation: 传统剪枝方法在大型语言模型上效果不佳，完整重训练成本过高，现有方法依赖近似或启发式方法，需要更高效的精确剪枝方案

Method: 通过行级等稀疏度约束解耦问题，利用校准数据的Gram矩阵高效计算最优1-swap（交换保留和剪枝权重），提出简单高效的1-swap算法

Result: 相比Wanda方法减少单层剪枝误差达60%，在多种GPT架构上持续改善困惑度和零样本准确率

Conclusion: 提出的1-swap算法在LLM规模下显著提升剪枝问题可解性，无需超参数调优，在GPU上高效运行，优于现有最先进方法

Abstract: The resource requirements of Neural Networks can be significantly reduced through pruning -- the removal of seemingly less important parameters. However, with the rise of Large Language Models (LLMs), full retraining to recover pruning-induced performance degradation is often prohibitive and classical approaches such as global magnitude pruning are suboptimal on Transformer architectures. State-of-the-art methods hence solve a layer-wise mask selection problem, the problem of finding a pruning mask which minimizes the per-layer pruning error on a small set of calibration data. Exactly solving this problem to optimality using Integer Programming (IP) solvers is computationally infeasible due to its combinatorial nature and the size of the search space, and existing approaches therefore rely on approximations or heuristics. In this work, we demonstrate that the mask selection problem can be made drastically more tractable at LLM scale. To that end, we decouple the rows by enforcing equal sparsity levels per row. This allows us to derive optimal 1-swaps (exchanging one kept and one pruned weight) that can be computed efficiently using the Gram matrix of the calibration data. Using these observations, we propose a tractable and simple 1-swap algorithm that warm starts from any pruning mask, runs efficiently on GPUs at LLM scale, and is essentially hyperparameter-free. We demonstrate that our approach reduces per-layer pruning error by up to 60% over Wanda (Sun et al., 2023) and consistently improves perplexity and zero-shot accuracy across state-of-the-art GPT architectures.

</details>


### [199] [Digital Twin Supervised Reinforcement Learning Framework for Autonomous Underwater Navigation](https://arxiv.org/abs/2512.10925)
*Zamirddine Mari,Mohamad Motasem Nawaf,Pierre Drap*

Main category: cs.LG

TL;DR: 使用PPO深度强化学习算法训练水下机器人BlueROV2的自主导航策略，在复杂障碍环境中优于传统DWA方法，并成功实现从仿真到实物的迁移。


<details>
  <summary>Details</summary>
Motivation: 水下环境自主导航面临GPS缺失、能见度差、障碍物多等挑战，需要开发鲁棒的导航算法来应对这些困难。

Method: 采用基于PPO算法的深度强化学习方法，结合目标导航信息、虚拟占用网格和边界射线投射的观测空间，在仿真环境中训练策略，并通过3D数字孪生技术进行实物验证。

Result: PPO策略在高度杂乱环境中持续优于DWA方法，具有更好的局部适应性和更少的碰撞，成功实现了从仿真到真实世界的迁移。

Conclusion: 深度强化学习在水下机器人自主导航中具有实际应用价值，能够有效处理复杂障碍环境，并可通过仿真训练实现向真实环境的迁移。

Abstract: Autonomous navigation in underwater environments remains a major challenge due to the absence of GPS, degraded visibility, and the presence of submerged obstacles. This article investigates these issues through the case of the BlueROV2, an open platform widely used for scientific experimentation. We propose a deep reinforcement learning approach based on the Proximal Policy Optimization (PPO) algorithm, using an observation space that combines target-oriented navigation information, a virtual occupancy grid, and ray-casting along the boundaries of the operational area. The learned policy is compared against a reference deterministic kinematic planner, the Dynamic Window Approach (DWA), commonly employed as a robust baseline for obstacle avoidance. The evaluation is conducted in a realistic simulation environment and complemented by validation on a physical BlueROV2 supervised by a 3D digital twin of the test site, helping to reduce risks associated with real-world experimentation. The results show that the PPO policy consistently outperforms DWA in highly cluttered environments, notably thanks to better local adaptation and reduced collisions. Finally, the experiments demonstrate the transferability of the learned behavior from simulation to the real world, confirming the relevance of deep RL for autonomous navigation in underwater robotics.

</details>


### [200] [Empirical evaluation of the Frank-Wolfe methods for constructing white-box adversarial attacks](https://arxiv.org/abs/2512.10936)
*Kristina Korotkova,Aleksandr Katrutsa*

Main category: cs.LG

TL;DR: 提出使用改进的Frank-Wolfe方法构建白盒对抗攻击，相比基于投影或几何直觉的标准方法更高效


<details>
  <summary>Details</summary>
Motivation: 神经网络对抗攻击构建对其部署至关重要，需要快速有效的方法评估对抗鲁棒性。由于对抗攻击构建本质上是优化问题，需要从数值优化角度寻找高效解决方案

Method: 采用改进的Frank-Wolfe方法（无投影方法）构建白盒对抗攻击，与基于投影操作或几何直觉的标准方法进行对比

Result: 在MNIST和CIFAR-10数据集上进行了数值实验，使用了多类逻辑回归模型、卷积神经网络和Vision Transformer，对方法进行了理论和数值评估

Conclusion: 改进的Frank-Wolfe方法为构建高效有效的对抗攻击提供了有前景的数值优化方案

Abstract: The construction of adversarial attacks for neural networks appears to be a crucial challenge for their deployment in various services. To estimate the adversarial robustness of a neural network, a fast and efficient approach is needed to construct adversarial attacks. Since the formalization of adversarial attack construction involves solving a specific optimization problem, we consider the problem of constructing an efficient and effective adversarial attack from a numerical optimization perspective. Specifically, we suggest utilizing advanced projection-free methods, known as modified Frank-Wolfe methods, to construct white-box adversarial attacks on the given input data. We perform a theoretical and numerical evaluation of these methods and compare them with standard approaches based on projection operations or geometrical intuition. Numerical experiments are performed on the MNIST and CIFAR-10 datasets, utilizing a multiclass logistic regression model, the convolutional neural networks (CNNs), and the Vision Transformer (ViT).

</details>


### [201] [Hierarchical Dataset Selection for High-Quality Data Sharing](https://arxiv.org/abs/2512.10952)
*Xiaona Zhou,Yingyan Zeng,Ran Jin,Ismini Lourentzou*

Main category: cs.LG

TL;DR: DaSH方法通过层次化建模数据集和组别效用，在资源约束下从异构数据池中选择整个数据集以提升下游性能，相比现有方法准确率提升最高达26.2%


<details>
  <summary>Details</summary>
Motivation: 现实场景中数据通常以离散数据集形式组织，不同数据集在相关性、质量和效用上存在差异。现有方法主要选择单个样本且将所有数据视为同等相关，忽略了数据集及其来源之间的差异，因此需要开发能够选择整个数据集的方法

Method: 提出DaSH（Dataset Selection via Hierarchies）方法，在数据集和组别（如集合、机构）两个层次上建模效用，通过层次化建模实现从有限观察中的高效泛化

Result: 在两个公共基准测试（Digit-Five和DomainNet）上，DaSH比最先进的数据选择基线方法准确率提升最高达26.2%，同时需要显著更少的探索步骤。消融实验显示DaSH在低资源设置和相关数据集缺乏的情况下具有鲁棒性

Conclusion: DaSH方法适用于实际多源学习工作流中的可扩展和自适应数据集选择，能够有效处理异构数据池中的数据集选择问题

Abstract: The success of modern machine learning hinges on access to high-quality training data. In many real-world scenarios, such as acquiring data from public repositories or sharing across institutions, data is naturally organized into discrete datasets that vary in relevance, quality, and utility. Selecting which repositories or institutions to search for useful datasets, and which datasets to incorporate into model training are therefore critical decisions, yet most existing methods select individual samples and treat all data as equally relevant, ignoring differences between datasets and their sources. In this work, we formalize the task of dataset selection: selecting entire datasets from a large, heterogeneous pool to improve downstream performance under resource constraints. We propose Dataset Selection via Hierarchies (DaSH), a dataset selection method that models utility at both dataset and group (e.g., collections, institutions) levels, enabling efficient generalization from limited observations. Across two public benchmarks (Digit-Five and DomainNet), DaSH outperforms state-of-the-art data selection baselines by up to 26.2% in accuracy, while requiring significantly fewer exploration steps. Ablations show DaSH is robust to low-resource settings and lack of relevant datasets, making it suitable for scalable and adaptive dataset selection in practical multi-source learning workflows.

</details>


### [202] [Bidirectional Normalizing Flow: From Data to Noise and Back](https://arxiv.org/abs/2512.10953)
*Yiyang Lu,Qiao Sun,Xianbang Wang,Zhicheng Jiang,Hanhong Zhao,Kaiming He*

Main category: cs.LG

TL;DR: BiFlow是一种双向归一化流框架，通过近似逆映射而非精确解析逆，解决了因果解码瓶颈，在ImageNet上实现了质量提升和百倍加速采样。


<details>
  <summary>Details</summary>
Motivation: 传统归一化流需要精确解析逆变换，这限制了架构灵活性。最近TARFlow等结合Transformer和自回归流的方法虽然复兴了NF，但暴露了因果解码作为主要瓶颈的问题。

Method: 提出BiFlow框架，放弃精确解析逆的要求，学习一个近似噪声到数据的逆映射模型，从而支持更灵活的损失函数和架构设计。

Result: 在ImageNet上，相比因果解码方法，BiFlow提高了生成质量，同时加速采样达两个数量级（百倍）。在基于NF的方法中达到SOTA，在单次评估方法中具有竞争力。

Conclusion: BiFlow通过近似逆映射解决了NF的因果解码瓶颈，为NF范式提供了更灵活高效的框架，有望进一步推动这一经典范式的发展。

Abstract: Normalizing Flows (NFs) have been established as a principled framework for generative modeling. Standard NFs consist of a forward process and a reverse process: the forward process maps data to noise, while the reverse process generates samples by inverting it. Typical NF forward transformations are constrained by explicit invertibility, ensuring that the reverse process can serve as their exact analytic inverse. Recent developments in TARFlow and its variants have revitalized NF methods by combining Transformers and autoregressive flows, but have also exposed causal decoding as a major bottleneck. In this work, we introduce Bidirectional Normalizing Flow ($\textbf{BiFlow}$), a framework that removes the need for an exact analytic inverse. BiFlow learns a reverse model that approximates the underlying noise-to-data inverse mapping, enabling more flexible loss functions and architectures. Experiments on ImageNet demonstrate that BiFlow, compared to its causal decoding counterpart, improves generation quality while accelerating sampling by up to two orders of magnitude. BiFlow yields state-of-the-art results among NF-based methods and competitive performance among single-evaluation ("1-NFE") methods. Following recent encouraging progress on NFs, we hope our work will draw further attention to this classical paradigm.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [203] [Local and Global Balance in Financial Correlation Networks: an Application to Investment Decisions](https://arxiv.org/abs/2512.10606)
*Paolo Bartesaghi,Rosanna Grassi,Pierpaolo Uberti*

Main category: q-fin.PM

TL;DR: 该研究提出使用局部平衡与全局平衡的偏差作为选股标准，认为在市场危机时，局部平衡显著偏离全局平衡的资产表现会优于市场


<details>
  <summary>Details</summary>
Motivation: 在金融危机期间，大多数资产表现相似，传统的分散化投资无法有效降低风险。需要寻找那些与市场整体行为不同的资产，这些资产可能在危机中表现更好

Method: 基于符号网络理论，将资产相关性网络转化为符号网络，计算全局平衡和局部平衡，使用局部平衡与全局平衡的偏差作为选股标准

Result: 实证分析显示，局部平衡显著偏离全局平衡的资产在描述性和预测性分析中都表现出优于市场的表现，验证了研究假设

Conclusion: 局部平衡与全局平衡的偏差可以作为有效的资产选择标准，特别是在市场危机时期，能够识别出表现优于市场的资产

Abstract: The global balance is a well-known indicator of the behavior of a signed network. Recent literature has introduced the concept of local balance as a measure of the contribution of a single node to the overall balance of the network. In the present research, we investigate the potential of using deviations of local balance from global balance as a criterion for selecting outperforming assets. The underlying idea is that, during financial crises, most assets in the investment universe behave similarly: losses are severe and widespread, and the global balance of the correlation-based signed network reaches its maximum value. Under such circumstances, standard diversification (mainly related to portfolio size) is unable to reduce risk or limit losses. Therefore, it may be useful to concentrate portfolio exposures on the few assets - if such assets exist-that behave differently from the rest of the market. We argue that these assets are those for which the local balance strongly departs from the global balance of the underlying signed network. The paper supports this hypothesis through an application using real financial data. The results, in both descriptive and predictive contexts, confirm the proposed intuition.

</details>
