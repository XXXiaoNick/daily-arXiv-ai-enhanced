{"id": "2510.16224", "categories": ["econ.EM"], "pdf": "https://arxiv.org/pdf/2510.16224", "abs": "https://arxiv.org/abs/2510.16224", "authors": ["Zhongjun Qu", "Wendun Wang", "Xiaomeng Zhang"], "title": "Prediction Intervals for Model Averaging", "comment": null, "summary": "A rich set of frequentist model averaging methods has been developed, but\ntheir applications have largely been limited to point prediction, as measuring\nprediction uncertainty in general settings remains an open problem. In this\npaper we propose prediction intervals for model averaging based on conformal\ninference. These intervals cover out-of-sample realizations of the outcome\nvariable with a pre-specified probability, providing a way to assess predictive\nuncertainty beyond point prediction. The framework allows general model\nmisspecification and applies to averaging across multiple models that can be\nnested, disjoint, overlapping, or any combination thereof, with weights that\nmay depend on the estimation sample. We establish coverage guarantees under two\nsets of assumptions: exact finite-sample validity under exchangeability,\nrelevant for cross-sectional data, and asymptotic validity under stationarity,\nrelevant for time-series data. We first present a benchmark algorithm and then\nintroduce a locally adaptive refinement and split-sample procedures that\nbroaden applicability. The methods are illustrated with a cross-sectional\napplication to real estate appraisal and a time-series application to equity\npremium forecasting.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5171\u5f62\u63a8\u65ad\u7684\u6a21\u578b\u5e73\u5747\u9884\u6d4b\u533a\u95f4\u65b9\u6cd5\uff0c\u80fd\u591f\u4e3a\u6a21\u578b\u5e73\u5747\u9884\u6d4b\u63d0\u4f9b\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cf\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u6a21\u578b\u7c7b\u578b\u548c\u6570\u636e\u573a\u666f\u3002", "motivation": "\u73b0\u6709\u7684\u9891\u7387\u6a21\u578b\u5e73\u5747\u65b9\u6cd5\u4e3b\u8981\u5c40\u9650\u4e8e\u70b9\u9884\u6d4b\uff0c\u5728\u4e00\u822c\u8bbe\u7f6e\u4e0b\u8861\u91cf\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u4ecd\u7136\u662f\u4e00\u4e2a\u672a\u89e3\u51b3\u7684\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u5171\u5f62\u63a8\u65ad\u6784\u5efa\u9884\u6d4b\u533a\u95f4\uff0c\u5141\u8bb8\u4e00\u822c\u6a21\u578b\u8bef\u8bbe\uff0c\u9002\u7528\u4e8e\u5d4c\u5957\u3001\u4e0d\u76f8\u4ea4\u3001\u91cd\u53e0\u6216\u4efb\u610f\u7ec4\u5408\u7684\u591a\u4e2a\u6a21\u578b\u5e73\u5747\uff0c\u6743\u91cd\u53ef\u4ee5\u4f9d\u8d56\u4e8e\u4f30\u8ba1\u6837\u672c\u3002", "result": "\u5efa\u7acb\u4e86\u4e24\u79cd\u5047\u8bbe\u4e0b\u7684\u8986\u76d6\u4fdd\u8bc1\uff1a\u5728\u53ef\u4ea4\u6362\u6027\u4e0b\u7684\u7cbe\u786e\u6709\u9650\u6837\u672c\u6709\u6548\u6027\uff08\u9002\u7528\u4e8e\u6a2a\u622a\u9762\u6570\u636e\uff09\u548c\u5728\u5e73\u7a33\u6027\u4e0b\u7684\u6e10\u8fd1\u6709\u6548\u6027\uff08\u9002\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff09\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u6a21\u578b\u5e73\u5747\u9884\u6d4b\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cf\u6846\u67b6\uff0c\u5728\u623f\u5730\u4ea7\u8bc4\u4f30\u548c\u80a1\u7968\u6ea2\u4ef7\u9884\u6d4b\u7b49\u5e94\u7528\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002"}}
{"id": "2510.16661", "categories": ["econ.EM"], "pdf": "https://arxiv.org/pdf/2510.16661", "abs": "https://arxiv.org/abs/2510.16661", "authors": ["Jing Kong"], "title": "On the Asymptotics of the Minimax Linear Estimator", "comment": null, "summary": "Many causal estimands, such as average treatment effects under\nunconfoundedness, can be written as continuous linear functionals of an unknown\nregression function. We study a weighting estimator that sets weights by a\nminimax procedure: solving a convex optimization problem that trades off\nworst-case conditional bias against variance. Despite its growing use, general\nroot-$n$ theory for this method has been limited. This paper fills that gap.\nUnder regularity conditions, we show that the minimax linear estimator is\nroot-$n$ consistent and asymptotically normal, and we derive its asymptotic\nvariance. These results justify ignoring worst-case bias when forming\nlarge-sample confidence intervals and make inference less sensitive to the\nscaling of the function class. With a mild variance condition, the estimator\nattains the semiparametric efficiency bound, so an augmentation step commonly\nused in the literature is not needed to achieve first-order optimality.\nEvidence from simulations and three empirical applications, including\njob-training and minimum-wage policies, points to a simple rule: in designs\nsatisfying our regularity conditions, standard-error confidence intervals\nsuffice; otherwise, bias-aware intervals remain important.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u901a\u8fc7\u6781\u5c0f\u6781\u5927\u51f8\u4f18\u5316\u65b9\u6cd5\u8bbe\u7f6e\u6743\u91cd\u7684\u52a0\u6743\u4f30\u8ba1\u5668\uff0c\u8bc1\u660e\u4e86\u8be5\u4f30\u8ba1\u5668\u5728\u6b63\u5219\u6761\u4ef6\u4e0b\u5177\u6709\u6839\u53f7n\u4e00\u81f4\u6027\u548c\u6e10\u8fd1\u6b63\u6001\u6027\uff0c\u5e76\u63a8\u5bfc\u4e86\u5176\u6e10\u8fd1\u65b9\u5dee\u3002", "motivation": "\u8bb8\u591a\u56e0\u679c\u4f30\u8ba1\u91cf\u53ef\u4ee5\u8868\u793a\u4e3a\u672a\u77e5\u56de\u5f52\u51fd\u6570\u7684\u8fde\u7eed\u7ebf\u6027\u6cdb\u51fd\uff0c\u4f46\u73b0\u6709\u7684\u6781\u5c0f\u6781\u5927\u7ebf\u6027\u4f30\u8ba1\u5668\u7684\u6839\u53f7n\u7406\u8bba\u4ecd\u7136\u6709\u9650\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u6781\u5c0f\u6781\u5927\u7a0b\u5e8f\u8bbe\u7f6e\u6743\u91cd\u7684\u52a0\u6743\u4f30\u8ba1\u5668\uff0c\u901a\u8fc7\u51f8\u4f18\u5316\u95ee\u9898\u5728\u6761\u4ef6\u504f\u5dee\u548c\u65b9\u5dee\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\u3002", "result": "\u5728\u6b63\u5219\u6761\u4ef6\u4e0b\uff0c\u6781\u5c0f\u6781\u5927\u7ebf\u6027\u4f30\u8ba1\u5668\u5177\u6709\u6839\u53f7n\u4e00\u81f4\u6027\u548c\u6e10\u8fd1\u6b63\u6001\u6027\uff1b\u5728\u6e29\u548c\u65b9\u5dee\u6761\u4ef6\u4e0b\uff0c\u8be5\u4f30\u8ba1\u5668\u8fbe\u5230\u534a\u53c2\u6570\u6548\u7387\u754c\uff0c\u65e0\u9700\u6587\u732e\u4e2d\u5e38\u7528\u7684\u589e\u5f3a\u6b65\u9aa4\u5373\u53ef\u5b9e\u73b0\u4e00\u9636\u6700\u4f18\u6027\u3002", "conclusion": "\u6a21\u62df\u548c\u5b9e\u8bc1\u5e94\u7528\u8868\u660e\uff0c\u5728\u6ee1\u8db3\u6b63\u5219\u6761\u4ef6\u7684\u8bbe\u8ba1\u4e2d\uff0c\u6807\u51c6\u8bef\u5dee\u7f6e\u4fe1\u533a\u95f4\u8db3\u591f\uff1b\u5426\u5219\uff0c\u504f\u5dee\u611f\u77e5\u533a\u95f4\u4ecd\u7136\u91cd\u8981\u3002"}}
{"id": "2510.16669", "categories": ["econ.EM"], "pdf": "https://arxiv.org/pdf/2510.16669", "abs": "https://arxiv.org/abs/2510.16669", "authors": ["Jing Kong"], "title": "Causal Inference in High-Dimensional Generalized Linear Models with Binary Outcomes", "comment": null, "summary": "This paper proposes a debiased estimator for causal effects in\nhigh-dimensional generalized linear models with binary outcomes and general\nlink functions. The estimator augments a regularized regression plug-in with\nweights computed from a convex optimization problem that approximately balances\nlink-derivative-weighted covariates and controls variance; it does not rely on\nestimated propensity scores. Under standard conditions, the estimator is\n$\\sqrt{n}$-consistent and asymptotically normal for dense linear contrasts and\ncausal parameters. Simulation results show the superior performance of our\napproach in comparison to alternatives such as inverse propensity score\nestimators and double machine learning estimators in finite samples. In an\napplication to the National Supported Work training data, our estimates and\nconfidence intervals are close to the experimental benchmark.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u9ad8\u7ef4\u5e7f\u4e49\u7ebf\u6027\u6a21\u578b\u4e2d\u56e0\u679c\u6548\u5e94\u7684\u53bb\u504f\u4f30\u8ba1\u5668\uff0c\u9002\u7528\u4e8e\u4e8c\u5143\u7ed3\u679c\u548c\u4e00\u822c\u94fe\u63a5\u51fd\u6570\uff0c\u4e0d\u4f9d\u8d56\u503e\u5411\u5f97\u5206\u4f30\u8ba1\u3002", "motivation": "\u5728\u9ad8\u7ef4\u5e7f\u4e49\u7ebf\u6027\u6a21\u578b\u4e2d\uff0c\u73b0\u6709\u56e0\u679c\u6548\u5e94\u4f30\u8ba1\u65b9\u6cd5\uff08\u5982\u9006\u503e\u5411\u5f97\u5206\u4f30\u8ba1\u5668\u548c\u53cc\u673a\u5668\u5b66\u4e60\u4f30\u8ba1\u5668\uff09\u5728\u6709\u9650\u6837\u672c\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u7a33\u5065\u7684\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u6b63\u5219\u5316\u56de\u5f52\u63d2\u4ef6\u589e\u5f3a\u6743\u91cd\u8ba1\u7b97\uff0c\u6743\u91cd\u6765\u81ea\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u8fd1\u4f3c\u5e73\u8861\u94fe\u63a5\u5bfc\u6570\u52a0\u6743\u534f\u53d8\u91cf\u5e76\u63a7\u5236\u65b9\u5dee\uff0c\u4e0d\u4f9d\u8d56\u503e\u5411\u5f97\u5206\u4f30\u8ba1\u3002", "result": "\u5728\u6807\u51c6\u6761\u4ef6\u4e0b\uff0c\u4f30\u8ba1\u5668\u5bf9\u5bc6\u96c6\u7ebf\u6027\u5bf9\u6bd4\u548c\u56e0\u679c\u53c2\u6570\u5177\u6709\u221an\u4e00\u81f4\u6027\u548c\u6e10\u8fd1\u6b63\u6001\u6027\u3002\u6a21\u62df\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u6709\u9650\u6837\u672c\u4e2d\u4f18\u4e8e\u9006\u503e\u5411\u5f97\u5206\u4f30\u8ba1\u5668\u548c\u53cc\u673a\u5668\u5b66\u4e60\u4f30\u8ba1\u5668\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u53bb\u504f\u4f30\u8ba1\u5668\u5728\u9ad8\u7ef4\u5e7f\u4e49\u7ebf\u6027\u6a21\u578b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u56fd\u5bb6\u652f\u6301\u5de5\u4f5c\u57f9\u8bad\u6570\u636e\u5e94\u7528\u4e2d\uff0c\u4f30\u8ba1\u7ed3\u679c\u548c\u7f6e\u4fe1\u533a\u95f4\u63a5\u8fd1\u5b9e\u9a8c\u57fa\u51c6\u3002"}}
{"id": "2510.15879", "categories": ["q-fin.GN", "econ.GN", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2510.15879", "abs": "https://arxiv.org/abs/2510.15879", "authors": ["Jiaquan Nicholas Chen", "Marcel Ausloos"], "title": "A study about who is interested in stock splitting and why: considering companies, shareholders or managers", "comment": "40 pages, 16 figures, 3 tables, 31 references; prepared for Journal\n  of Risk and Financial Management", "summary": "There are many misconceptions around stock prices, stock splits,\nshareholders, investors, and managers behaviour about such informations due to\na number of confounding factors. This paper tests hypotheses with a selected\ndatabase, about the question ''is stock split attractive for companies?'' in\nanother words, ''why companies split their stock?'', ''why managers split their\nstock?'', sometimes for no benefit, and ''why shareholders agree with such\ndecisions?''. We contribute to the existing knowledge through a discussion of\nnine events in recent (selectively chosen) years, observing the role of\ninformation asymmetries, the returns and traded volumes before and after the\nevent. Therefore, calculating the beta for each sample, it is found that stock\nsplits (i) affect the market and slightly enhance the trading volume in a\nshort-term, (ii) increase the shareholder base for its firm, (iii) have a\npositive effect on the liquidity of the market. We concur that stock split\nannouncements can reduce the level of information asymmetric. Investors\nreadjust their beliefs in the firm, although most of the firms are mispriced in\nthe stock split year.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u5206\u67909\u4e2a\u80a1\u7968\u62c6\u5206\u4e8b\u4ef6\uff0c\u53d1\u73b0\u80a1\u7968\u62c6\u5206\u80fd\u77ed\u671f\u63d0\u5347\u4ea4\u6613\u91cf\u3001\u6269\u5927\u80a1\u4e1c\u57fa\u7840\u3001\u63d0\u9ad8\u5e02\u573a\u6d41\u52a8\u6027\uff0c\u5e76\u51cf\u5c11\u4fe1\u606f\u4e0d\u5bf9\u79f0\u3002", "motivation": "\u7814\u7a76\u80a1\u7968\u62c6\u5206\u5bf9\u516c\u53f8\u7684\u5438\u5f15\u529b\uff0c\u63a2\u8ba8\u7ba1\u7406\u5c42\u548c\u80a1\u4e1c\u4e3a\u4f55\u540c\u610f\u80a1\u7968\u62c6\u5206\u51b3\u7b56\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u51b3\u7b56\u80cc\u540e\u7684\u539f\u56e0\u3002", "method": "\u9009\u62e9\u7279\u5b9a\u5e74\u4efd\u76849\u4e2a\u80a1\u7968\u62c6\u5206\u4e8b\u4ef6\u8fdb\u884c\u5206\u6790\uff0c\u8ba1\u7b97\u6bcf\u4e2a\u6837\u672c\u7684beta\u503c\uff0c\u89c2\u5bdf\u4fe1\u606f\u4e0d\u5bf9\u79f0\u3001\u4e8b\u4ef6\u524d\u540e\u7684\u56de\u62a5\u7387\u548c\u4ea4\u6613\u91cf\u53d8\u5316\u3002", "result": "\u80a1\u7968\u62c6\u5206\u5728\u77ed\u671f\u5185\u5f71\u54cd\u5e02\u573a\u5e76\u8f7b\u5fae\u63d0\u5347\u4ea4\u6613\u91cf\uff0c\u589e\u52a0\u516c\u53f8\u80a1\u4e1c\u57fa\u7840\uff0c\u5bf9\u5e02\u573a\u6d41\u52a8\u6027\u6709\u79ef\u6781\u5f71\u54cd\u3002", "conclusion": "\u80a1\u7968\u62c6\u5206\u516c\u544a\u53ef\u4ee5\u51cf\u5c11\u4fe1\u606f\u4e0d\u5bf9\u79f0\u6c34\u5e73\uff0c\u6295\u8d44\u8005\u4f1a\u91cd\u65b0\u8c03\u6574\u5bf9\u516c\u53f8\u4fe1\u5ff5\uff0c\u5c3d\u7ba1\u5927\u591a\u6570\u516c\u53f8\u5728\u80a1\u7968\u62c6\u5206\u5e74\u4efd\u88ab\u9519\u8bef\u5b9a\u4ef7\u3002"}}
{"id": "2510.16681", "categories": ["econ.EM", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.16681", "abs": "https://arxiv.org/abs/2510.16681", "authors": ["Sukjin Han", "Haiqing Xu"], "title": "On Quantile Treatment Effects, Rank Similarity,and Variation of Instrumental Variables", "comment": "49 pages, 11 figures", "summary": "This paper develops a nonparametric framework to identify and estimate\ndistributional treatment effects under nonseparable endogeneity. We begin by\nrevisiting the widely adopted \\emph{rank similarity} (RS) assumption and\ncharacterizing it by the relationship it imposes between observed and\ncounterfactual potential outcome distributions. The characterization highlights\nthe restrictiveness of RS, motivating a weaker identifying condition. Under\nthis alternative, we construct identifying bounds on the distributional\ntreatment effects of interest through a linear semi-infinite programming (SILP)\nformulation. Our identification strategy also clarifies how richer exogenous\ninstrument variation, such as multi-valued or multiple instruments, can further\ntighten these bounds. Finally, exploiting the SILP's saddle-point structure and\nKarush-Kuhn-Tucker (KKT) conditions, we establish large-sample properties for\nthe empirical SILP: consistency and asymptotic distribution results for the\nestimated bounds and associated solutions.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u4e2a\u975e\u53c2\u6570\u6846\u67b6\uff0c\u7528\u4e8e\u8bc6\u522b\u548c\u4f30\u8ba1\u4e0d\u53ef\u5206\u79bb\u5185\u751f\u6027\u4e0b\u7684\u5206\u5e03\u5904\u7406\u6548\u5e94\u3002\u901a\u8fc7\u653e\u5bbd\u79e9\u76f8\u4f3c\u6027\u5047\u8bbe\uff0c\u6784\u5efa\u4e86\u57fa\u4e8e\u7ebf\u6027\u534a\u65e0\u9650\u89c4\u5212\u7684\u8bc6\u522b\u8fb9\u754c\uff0c\u5e76\u5efa\u7acb\u4e86\u5927\u6837\u672c\u6027\u8d28\u3002", "motivation": "\u91cd\u65b0\u5ba1\u89c6\u5e7f\u6cdb\u91c7\u7528\u7684\u79e9\u76f8\u4f3c\u6027\u5047\u8bbe\uff0c\u53d1\u73b0\u5176\u9650\u5236\u6027\u8f83\u5f3a\uff0c\u9700\u8981\u66f4\u5f31\u7684\u8bc6\u522b\u6761\u4ef6\u6765\u5904\u7406\u4e0d\u53ef\u5206\u79bb\u5185\u751f\u6027\u95ee\u9898\u3002", "method": "\u91c7\u7528\u7ebf\u6027\u534a\u65e0\u9650\u89c4\u5212\u65b9\u6cd5\u6784\u5efa\u8bc6\u522b\u8fb9\u754c\uff0c\u5229\u7528\u978d\u70b9\u7ed3\u6784\u548cKKT\u6761\u4ef6\u5206\u6790\u5927\u6837\u672c\u6027\u8d28\u3002", "result": "\u5efa\u7acb\u4e86\u5206\u5e03\u5904\u7406\u6548\u5e94\u7684\u8bc6\u522b\u8fb9\u754c\uff0c\u5e76\u8bc1\u660e\u4e86\u4f30\u8ba1\u8fb9\u754c\u7684\u4e00\u81f4\u6027\u53ca\u6e10\u8fd1\u5206\u5e03\u7ed3\u679c\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u5728\u8f83\u5f31\u5047\u8bbe\u4e0b\u6709\u6548\u8bc6\u522b\u5206\u5e03\u5904\u7406\u6548\u5e94\uff0c\u5916\u751f\u5de5\u5177\u53d8\u91cf\u53d8\u5316\u53ef\u8fdb\u4e00\u6b65\u6536\u7d27\u8fb9\u754c\uff0c\u5177\u6709\u826f\u597d\u7edf\u8ba1\u6027\u8d28\u3002"}}
{"id": "2510.16021", "categories": ["cs.LG", "econ.GN", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2510.16021", "abs": "https://arxiv.org/abs/2510.16021", "authors": ["Arega Getaneh Abate", "Xiufeng Liu", "Ruyu Liu", "Xiaobing Zhang"], "title": "Feature-driven reinforcement learning for photovoltaic in continuous intraday trading", "comment": null, "summary": "Photovoltaic (PV) operators face substantial uncertainty in generation and\nshort-term electricity prices. Continuous intraday markets enable producers to\nadjust their positions in real time, potentially improving revenues and\nreducing imbalance costs. We propose a feature-driven reinforcement learning\n(RL) approach for PV intraday trading that integrates data-driven features into\nthe state and learns bidding policies in a sequential decision framework. The\nproblem is cast as a Markov Decision Process with a reward that balances\ntrading profit and imbalance penalties and is solved with Proximal Policy\nOptimization (PPO) using a predominantly linear, interpretable policy. Trained\non historical market data and evaluated out-of-sample, the strategy\nconsistently outperforms benchmark baselines across diverse scenarios.\nExtensive validation shows rapid convergence, real-time inference, and\ntransparent decision rules. Learned weights highlight the central role of\nmarket microstructure and historical features. Taken together, these results\nindicate that feature-driven RL offers a practical, data-efficient, and\noperationally deployable pathway for active intraday participation by PV\nproducers.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7279\u5f81\u9a71\u52a8\u5f3a\u5316\u5b66\u4e60\u7684\u5149\u4f0f\u65e5\u5185\u4ea4\u6613\u7b56\u7565\uff0c\u4f7f\u7528PPO\u7b97\u6cd5\u5b66\u4e60\u6295\u6807\u7b56\u7565\uff0c\u5728\u5386\u53f2\u5e02\u573a\u6570\u636e\u4e0a\u8bad\u7ec3\u5e76\u9a8c\u8bc1\uff0c\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\u3002", "motivation": "\u5149\u4f0f\u8fd0\u8425\u5546\u9762\u4e34\u53d1\u7535\u91cf\u548c\u77ed\u671f\u7535\u4ef7\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u65e5\u5185\u5e02\u573a\u5141\u8bb8\u5b9e\u65f6\u8c03\u6574\u5934\u5bf8\u4ee5\u6539\u5584\u6536\u76ca\u548c\u51cf\u5c11\u4e0d\u5e73\u8861\u6210\u672c\u3002", "method": "\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u4f7f\u7528PPO\u7b97\u6cd5\u8bad\u7ec3\u53ef\u89e3\u91ca\u7684\u7ebf\u6027\u7b56\u7565\uff0c\u6574\u5408\u6570\u636e\u9a71\u52a8\u7279\u5f81\u5230\u72b6\u6001\u7a7a\u95f4\u4e2d\u3002", "result": "\u7b56\u7565\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u6301\u7eed\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u663e\u793a\u5feb\u901f\u6536\u655b\u3001\u5b9e\u65f6\u63a8\u7406\u548c\u900f\u660e\u51b3\u7b56\u89c4\u5219\uff0c\u5b66\u4e60\u6743\u91cd\u7a81\u663e\u5e02\u573a\u5fae\u89c2\u7ed3\u6784\u548c\u5386\u53f2\u7279\u5f81\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u7279\u5f81\u9a71\u52a8\u5f3a\u5316\u5b66\u4e60\u4e3a\u5149\u4f0f\u751f\u4ea7\u5546\u63d0\u4f9b\u4e86\u5b9e\u7528\u3001\u6570\u636e\u9ad8\u6548\u4e14\u53ef\u64cd\u4f5c\u90e8\u7f72\u7684\u65e5\u5185\u53c2\u4e0e\u9014\u5f84\u3002"}}
{"id": "2510.15883", "categories": ["q-fin.CP", "cs.AI", "cs.LG", "q-fin.TR"], "pdf": "https://arxiv.org/pdf/2510.15883", "abs": "https://arxiv.org/abs/2510.15883", "authors": ["Yang Li", "Zhi Chen"], "title": "FinFlowRL: An Imitation-Reinforcement Learning Framework for Adaptive Stochastic Control in Finance", "comment": "21 pages, 5 algorithms, 4 tables, 5 figures", "summary": "Traditional stochastic control methods in finance struggle in real world\nmarkets due to their reliance on simplifying assumptions and stylized\nframeworks. Such methods typically perform well in specific, well defined\nenvironments but yield suboptimal results in changed, non stationary ones. We\nintroduce FinFlowRL, a novel framework for financial optimal stochastic\ncontrol. The framework pretrains an adaptive meta policy learning from multiple\nexpert strategies, then finetunes through reinforcement learning in the noise\nspace to optimize the generative process. By employing action chunking\ngenerating action sequences rather than single decisions, it addresses the non\nMarkovian nature of markets. FinFlowRL consistently outperforms individually\noptimized experts across diverse market conditions.", "AI": {"tldr": "FinFlowRL\u662f\u4e00\u4e2a\u7528\u4e8e\u91d1\u878d\u6700\u4f18\u968f\u673a\u63a7\u5236\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5143\u7b56\u7565\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u5728\u566a\u58f0\u7a7a\u95f4\u5fae\u8c03\uff0c\u4f7f\u7528\u52a8\u4f5c\u5206\u5757\u5904\u7406\u5e02\u573a\u7684\u975e\u9a6c\u5c14\u53ef\u592b\u7279\u6027\uff0c\u5728\u5404\u79cd\u5e02\u573a\u6761\u4ef6\u4e0b\u6301\u7eed\u4f18\u4e8e\u5355\u72ec\u4f18\u5316\u7684\u4e13\u5bb6\u7b56\u7565\u3002", "motivation": "\u4f20\u7edf\u91d1\u878d\u968f\u673a\u63a7\u5236\u65b9\u6cd5\u5728\u73b0\u5b9e\u5e02\u573a\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u5b83\u4eec\u4f9d\u8d56\u7b80\u5316\u5047\u8bbe\u548c\u7a0b\u5f0f\u5316\u6846\u67b6\uff0c\u5728\u53d8\u5316\u3001\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u4ea7\u751f\u6b21\u4f18\u7ed3\u679c\u3002", "method": "\u6846\u67b6\u9884\u8bad\u7ec3\u4e00\u4e2a\u4ece\u591a\u4e2a\u4e13\u5bb6\u7b56\u7565\u5b66\u4e60\u7684\u81ea\u9002\u5e94\u5143\u7b56\u7565\uff0c\u7136\u540e\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5728\u566a\u58f0\u7a7a\u95f4\u8fdb\u884c\u5fae\u8c03\u4ee5\u4f18\u5316\u751f\u6210\u8fc7\u7a0b\uff0c\u91c7\u7528\u52a8\u4f5c\u5206\u5757\u751f\u6210\u52a8\u4f5c\u5e8f\u5217\u800c\u975e\u5355\u4e2a\u51b3\u7b56\u3002", "result": "FinFlowRL\u5728\u5404\u79cd\u5e02\u573a\u6761\u4ef6\u4e0b\u6301\u7eed\u4f18\u4e8e\u5355\u72ec\u4f18\u5316\u7684\u4e13\u5bb6\u7b56\u7565\u3002", "conclusion": "FinFlowRL\u6846\u67b6\u901a\u8fc7\u5143\u7b56\u7565\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u91d1\u878d\u968f\u673a\u63a7\u5236\u65b9\u6cd5\u5728\u975e\u5e73\u7a33\u5e02\u573a\u73af\u5883\u4e2d\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.16127", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16127", "abs": "https://arxiv.org/abs/2510.16127", "authors": ["Oliver J. Hines", "Caleb H. Miles"], "title": "Learning density ratios in causal inference using Bregman-Riesz regression", "comment": "Replication code is available from\n  https://github.com/CI-NYC/densityratios", "summary": "The ratio of two probability density functions is a fundamental quantity that\nappears in many areas of statistics and machine learning, including causal\ninference, reinforcement learning, covariate shift, outlier detection,\nindependence testing, importance sampling, and diffusion modeling. Naively\nestimating the numerator and denominator densities separately using, e.g.,\nkernel density estimators, can lead to unstable performance and suffers from\nthe curse of dimensionality as the number of covariates increases. For this\nreason, several methods have been developed for estimating the density ratio\ndirectly based on (a) Bregman divergences or (b) recasting the density ratio as\nthe odds in a probabilistic classification model that predicts whether an\nobservation is sampled from the numerator or denominator distribution.\nAdditionally, the density ratio can be viewed as the Riesz representer of a\ncontinuous linear map, making it amenable to estimation via (c) minimization of\nthe so-called Riesz loss, which was developed to learn the Riesz representer in\nthe Riesz regression procedure in causal inference. In this paper we show that\nall three of these methods can be unified in a common framework, which we call\nBregman-Riesz regression. We further show how data augmentation techniques can\nbe used to apply density ratio learning methods to causal problems, where the\nnumerator distribution typically represents an unobserved intervention. We show\nthrough simulations how the choice of Bregman divergence and data augmentation\nstrategy can affect the performance of the resulting density ratio learner. A\nPython package is provided for researchers to apply Bregman-Riesz regression in\npractice using gradient boosting, neural networks, and kernel methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Bregman-Riesz\u56de\u5f52\u6846\u67b6\uff0c\u7edf\u4e00\u4e86\u4e09\u79cd\u5bc6\u5ea6\u6bd4\u4f30\u8ba1\u65b9\u6cd5\uff1aBregman\u6563\u5ea6\u3001\u6982\u7387\u5206\u7c7b\u548cRiesz\u635f\u5931\u6700\u5c0f\u5316\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u6280\u672f\u5c06\u5bc6\u5ea6\u6bd4\u5b66\u4e60\u65b9\u6cd5\u5e94\u7528\u4e8e\u56e0\u679c\u95ee\u9898\u3002", "motivation": "\u5bc6\u5ea6\u6bd4\u662f\u7edf\u8ba1\u5b66\u548c\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u57fa\u672c\u91cf\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u5982\u5206\u522b\u4f30\u8ba1\u5206\u5b50\u548c\u5206\u6bcd\u5bc6\u5ea6\u4f1a\u5bfc\u81f4\u4e0d\u7a33\u5b9a\u6027\u548c\u7ef4\u5ea6\u707e\u96be\u95ee\u9898\u3002\u73b0\u6709\u4e09\u79cd\u65b9\u6cd5\u5404\u81ea\u72ec\u7acb\u53d1\u5c55\uff0c\u9700\u8981\u7edf\u4e00\u6846\u67b6\u6765\u6574\u5408\u8fd9\u4e9b\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86Bregman-Riesz\u56de\u5f52\u6846\u67b6\uff0c\u5c06Bregman\u6563\u5ea6\u65b9\u6cd5\u3001\u6982\u7387\u5206\u7c7b\u65b9\u6cd5\u548cRiesz\u635f\u5931\u6700\u5c0f\u5316\u65b9\u6cd5\u7edf\u4e00\u8d77\u6765\u3002\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u6280\u672f\u5904\u7406\u56e0\u679c\u95ee\u9898\u4e2d\u7684\u672a\u89c2\u6d4b\u5e72\u9884\u5206\u5e03\uff0c\u5e76\u4f7f\u7528\u68af\u5ea6\u63d0\u5347\u3001\u795e\u7ecf\u7f51\u7edc\u548c\u6838\u65b9\u6cd5\u8fdb\u884c\u5b9e\u73b0\u3002", "result": "\u901a\u8fc7\u6a21\u62df\u5b9e\u9a8c\u5c55\u793a\u4e86\u4e0d\u540cBregman\u6563\u5ea6\u548c\u6570\u636e\u589e\u5f3a\u7b56\u7565\u5bf9\u5bc6\u5ea6\u6bd4\u5b66\u4e60\u5668\u6027\u80fd\u7684\u5f71\u54cd\u3002\u63d0\u4f9b\u4e86Python\u5305\u652f\u6301\u5b9e\u9645\u5e94\u7528\u3002", "conclusion": "Bregman-Riesz\u56de\u5f52\u4e3a\u5bc6\u5ea6\u6bd4\u4f30\u8ba1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u80fd\u591f\u6574\u5408\u73b0\u6709\u65b9\u6cd5\u5e76\u6709\u6548\u5e94\u7528\u4e8e\u56e0\u679c\u63a8\u65ad\u7b49\u5b9e\u9645\u95ee\u9898\uff0c\u5177\u6709\u91cd\u8981\u7684\u7406\u8bba\u548c\u5b9e\u8df5\u4ef7\u503c\u3002"}}
{"id": "2510.15921", "categories": ["q-fin.PM", "cs.NE", "q-fin.CP"], "pdf": "https://arxiv.org/pdf/2510.15921", "abs": "https://arxiv.org/abs/2510.15921", "authors": ["Amarendra Mohan", "Ameer Tamoor Khan", "Shuai Li", "Xinwei Cao", "Zhibin Li"], "title": "Spiking Neural Network for Cross-Market Portfolio Optimization in Financial Markets: A Neuromorphic Computing Approach", "comment": null, "summary": "Cross-market portfolio optimization has become increasingly complex with the\nglobalization of financial markets and the growth of high-frequency,\nmulti-dimensional datasets. Traditional artificial neural networks, while\neffective in certain portfolio management tasks, often incur substantial\ncomputational overhead and lack the temporal processing capabilities required\nfor large-scale, multi-market data. This study investigates the application of\nSpiking Neural Networks (SNNs) for cross-market portfolio optimization,\nleveraging neuromorphic computing principles to process equity data from both\nthe Indian (Nifty 500) and US (S&P 500) markets. A five-year dataset comprising\napproximately 1,250 trading days of daily stock prices was systematically\ncollected via the Yahoo Finance API. The proposed framework integrates Leaky\nIntegrate-andFire neuron dynamics with adaptive thresholding,\nspike-timingdependent plasticity, and lateral inhibition to enable event-driven\nprocessing of financial time series. Dimensionality reduction is achieved\nthrough hierarchical clustering, while populationbased spike encoding and\nmultiple decoding strategies support robust portfolio construction under\nrealistic trading constraints, including cardinality limits, transaction costs,\nand adaptive risk aversion. Experimental evaluation demonstrates that the\nSNN-based framework delivers superior risk-adjusted returns and reduced\nvolatility compared to ANN benchmarks, while substantially improving\ncomputational efficiency. These findings highlight the promise of neuromorphic\ncomputation for scalable, efficient, and robust portfolio optimization across\nglobal financial markets.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8109\u51b2\u795e\u7ecf\u7f51\u7edc(SNN)\u7684\u8de8\u5e02\u573a\u6295\u8d44\u7ec4\u5408\u4f18\u5316\u6846\u67b6\uff0c\u5229\u7528\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u539f\u7406\u5904\u7406\u5370\u5ea6\u548c\u7f8e\u56fd\u80a1\u5e02\u6570\u636e\uff0c\u76f8\u6bd4\u4f20\u7edf\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u5177\u6709\u66f4\u597d\u7684\u98ce\u9669\u8c03\u6574\u6536\u76ca\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u968f\u7740\u91d1\u878d\u5e02\u573a\u5168\u7403\u5316\u548c\u9ad8\u9891\u591a\u7ef4\u6570\u636e\u96c6\u7684\u589e\u957f\uff0c\u8de8\u5e02\u573a\u6295\u8d44\u7ec4\u5408\u4f18\u5316\u53d8\u5f97\u65e5\u76ca\u590d\u6742\u3002\u4f20\u7edf\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u8ba1\u7b97\u5f00\u9500\u5927\u4e14\u7f3a\u4e4f\u5904\u7406\u5927\u89c4\u6a21\u591a\u5e02\u573a\u6570\u636e\u6240\u9700\u7684\u65f6\u95f4\u5904\u7406\u80fd\u529b\u3002", "method": "\u6574\u5408\u4e86\u6cc4\u6f0f\u79ef\u5206\u53d1\u653e\u795e\u7ecf\u5143\u52a8\u529b\u5b66\u3001\u81ea\u9002\u5e94\u9608\u503c\u3001\u8109\u51b2\u65f6\u5e8f\u4f9d\u8d56\u53ef\u5851\u6027\u548c\u4fa7\u5411\u6291\u5236\uff0c\u91c7\u7528\u5206\u5c42\u805a\u7c7b\u8fdb\u884c\u964d\u7ef4\uff0c\u57fa\u4e8e\u7fa4\u4f53\u7684\u8109\u51b2\u7f16\u7801\u548c\u591a\u79cd\u89e3\u7801\u7b56\u7565\uff0c\u5728\u4ea4\u6613\u7ea6\u675f\u4e0b\u6784\u5efa\u6295\u8d44\u7ec4\u5408\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793aSNN\u6846\u67b6\u76f8\u6bd4ANN\u57fa\u51c6\u5177\u6709\u66f4\u4f18\u7684\u98ce\u9669\u8c03\u6574\u6536\u76ca\u548c\u66f4\u4f4e\u7684\u6ce2\u52a8\u6027\uff0c\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u4e3a\u5168\u7403\u91d1\u878d\u5e02\u573a\u4e2d\u53ef\u6269\u5c55\u3001\u9ad8\u6548\u548c\u7a33\u5065\u7684\u6295\u8d44\u7ec4\u5408\u4f18\u5316\u5c55\u73b0\u4e86\u826f\u597d\u524d\u666f\u3002"}}
{"id": "2510.15936", "categories": ["cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.15936", "abs": "https://arxiv.org/abs/2510.15936", "authors": ["Juan David Salazar Rodriguez", "Sam Conrad Joyce", "Nachamma Sockalingam", "Julfendi"], "title": "Large Language Models in Architecture Studio: A Framework for Learning Outcomes", "comment": null, "summary": "The study explores the role of large language models (LLMs) in the context of\nthe architectural design studio, understood as the pedagogical core of\narchitectural education. Traditionally, the studio has functioned as an\nexperiential learning space where students tackle design problems through\nreflective practice, peer critique, and faculty guidance. However, the\nintegration of artificial intelligence (AI) in this environment has been\nlargely focused on form generation, automation, and representation-al\nefficiency, neglecting its potential as a pedagogical tool to strengthen\nstudent autonomy, collaboration, and self-reflection. The objectives of this\nresearch were: (1) to identify pedagogical challenges in self-directed,\npeer-to-peer, and teacher-guided learning processes in architecture studies;\n(2) to propose AI interventions, particularly through LLM, that contribute to\novercoming these challenges; and (3) to align these interventions with\nmeasurable learning outcomes using Bloom's taxonomy. The findings show that the\nmain challenges include managing student autonomy, tensions in peer feedback,\nand the difficulty of balancing the transmission of technical knowledge with\nthe stimulation of creativity in teaching. In response to this, LLMs are\nemerging as complementary agents capable of generating personalized feedback,\norganizing collaborative interactions, and offering adaptive cognitive\nscaffolding. Furthermore, their implementation can be linked to the cognitive\nlevels of Bloom's taxonomy: facilitating the recall and understanding of\narchitectural concepts, supporting application and analysis through interactive\ncase studies, and encouraging synthesis and evaluation through hypothetical\ndesign scenarios.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5efa\u7b51\u6559\u80b2\u6838\u5fc3\u2014\u2014\u8bbe\u8ba1\u5de5\u4f5c\u5ba4\u4e2d\u7684\u6559\u5b66\u5e94\u7528\u6f5c\u529b\uff0c\u63d0\u51faLLMs\u53ef\u4f5c\u4e3a\u8865\u5145\u5de5\u5177\u589e\u5f3a\u5b66\u751f\u81ea\u4e3b\u6027\u3001\u534f\u4f5c\u548c\u53cd\u601d\u80fd\u529b\uff0c\u5e76\u4e0e\u5e03\u9c81\u59c6\u5206\u7c7b\u6cd5\u7684\u8ba4\u77e5\u5c42\u6b21\u76f8\u5339\u914d\u3002", "motivation": "\u4f20\u7edf\u5efa\u7b51\u8bbe\u8ba1\u5de5\u4f5c\u5ba4\u4e3b\u8981\u4f9d\u8d56\u7ecf\u9a8c\u5b66\u4e60\u3001\u540c\u4f34\u8bc4\u4ef7\u548c\u6559\u5e08\u6307\u5bfc\uff0c\u800cAI\u5728\u8be5\u9886\u57df\u7684\u5e94\u7528\u591a\u96c6\u4e2d\u4e8e\u5f62\u5f0f\u751f\u6210\u548c\u6548\u7387\u63d0\u5347\uff0c\u5ffd\u89c6\u4e86\u5176\u4f5c\u4e3a\u6559\u5b66\u5de5\u5177\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u589e\u5f3a\u5b66\u751f\u81ea\u4e3b\u6027\u3001\u534f\u4f5c\u548c\u81ea\u6211\u53cd\u601d\u65b9\u9762\u7684\u4ef7\u503c\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u8bc6\u522b\u5efa\u7b51\u5b66\u4e60\u4e2d\u81ea\u4e3b\u3001\u540c\u4f34\u548c\u6559\u5e08\u6307\u5bfc\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u7684\u6559\u5b66\u6311\u6218\uff0c\u63d0\u51fa\u57fa\u4e8eLLM\u7684AI\u5e72\u9884\u65b9\u6848\uff0c\u5e76\u5c06\u8fd9\u4e9b\u5e72\u9884\u4e0e\u5e03\u9c81\u59c6\u5206\u7c7b\u6cd5\u7684\u53ef\u8861\u91cf\u5b66\u4e60\u6210\u679c\u5bf9\u9f50\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e3b\u8981\u6311\u6218\u5305\u62ec\u5b66\u751f\u81ea\u4e3b\u6027\u7ba1\u7406\u3001\u540c\u4f34\u53cd\u9988\u4e2d\u7684\u7d27\u5f20\u5173\u7cfb\u4ee5\u53ca\u6280\u672f\u77e5\u8bc6\u4f20\u6388\u4e0e\u521b\u9020\u529b\u6fc0\u53d1\u4e4b\u95f4\u7684\u5e73\u8861\u56f0\u96be\u3002LLMs\u80fd\u591f\u751f\u6210\u4e2a\u6027\u5316\u53cd\u9988\u3001\u7ec4\u7ec7\u534f\u4f5c\u4e92\u52a8\u5e76\u63d0\u4f9b\u9002\u5e94\u6027\u8ba4\u77e5\u652f\u67b6\u3002", "conclusion": "LLMs\u53ef\u4f5c\u4e3a\u5efa\u7b51\u6559\u80b2\u4e2d\u7684\u8865\u5145\u4ee3\u7406\uff0c\u901a\u8fc7\u4e2a\u6027\u5316\u53cd\u9988\u3001\u534f\u4f5c\u7ec4\u7ec7\u548c\u8ba4\u77e5\u652f\u67b6\u652f\u6301\uff0c\u6709\u6548\u5e94\u5bf9\u4f20\u7edf\u6559\u5b66\u6311\u6218\uff0c\u5e76\u80fd\u4e0e\u5e03\u9c81\u59c6\u5206\u7c7b\u6cd5\u7684\u5404\u8ba4\u77e5\u5c42\u6b21\u76f8\u5339\u914d\uff0c\u4fc3\u8fdb\u4ece\u8bb0\u5fc6\u7406\u89e3\u5230\u7efc\u5408\u8bc4\u4f30\u7684\u5168\u65b9\u4f4d\u5b66\u4e60\u3002"}}
{"id": "2510.15948", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.15948", "abs": "https://arxiv.org/abs/2510.15948", "authors": ["MingSheng Li", "Guangze Zhao", "Sichen Liu"], "title": "VisuoAlign: Safety Alignment of LVLMs with Multimodal Tree Search", "comment": null, "summary": "Large Vision-Language Models (LVLMs) have achieved remarkable progress in\nmultimodal perception and generation, yet their safety alignment remains a\ncritical challenge.Existing defenses and vulnerable to multimodal jailbreaks,\nas visual inputs introduce new attack surfaces, reasoning chains lack safety\nsupervision, and alignment often degrades under modality fusion.To overcome\nthese limitation, we propose VisuoAlign, a framework for multi-modal safety\nalignment via prompt-guided tree search.VisuoAlign embeds safety constrains\ninto the reasoning process through visual-textual interactive prompts, employs\nMonte Carlo Tree Search(MCTS) to systematically construct diverse\nsafety-critical prompt trajectories, and introduces prompt-based scaling to\nensure real-time risk detection and compliant responses.Extensive experiments\ndemonstrate that VisuoAlign proactively exposes risks, enables comprehensive\ndataset generation, and significantly improves the robustness of LVLMs against\ncomplex cross-modal threats.", "AI": {"tldr": "VisuoAlign\u662f\u4e00\u4e2a\u901a\u8fc7\u63d0\u793a\u5f15\u5bfc\u6811\u641c\u7d22\u8fdb\u884c\u591a\u6a21\u6001\u5b89\u5168\u5bf9\u9f50\u7684\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u5bf9\u9f50\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u5bf9\u591a\u6a21\u6001\u8d8a\u72f1\u653b\u51fb\u5b58\u5728\u8106\u5f31\u6027\uff0c\u89c6\u89c9\u8f93\u5165\u5f15\u5165\u65b0\u7684\u653b\u51fb\u9762\uff0c\u63a8\u7406\u94fe\u7f3a\u4e4f\u5b89\u5168\u76d1\u7763\uff0c\u6a21\u6001\u878d\u5408\u4f1a\u964d\u4f4e\u5bf9\u9f50\u6548\u679c\u3002", "method": "\u901a\u8fc7\u89c6\u89c9-\u6587\u672c\u4ea4\u4e92\u63d0\u793a\u5c06\u5b89\u5168\u7ea6\u675f\u5d4c\u5165\u63a8\u7406\u8fc7\u7a0b\uff0c\u4f7f\u7528\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u6784\u5efa\u591a\u6837\u5316\u7684\u5b89\u5168\u5173\u952e\u63d0\u793a\u8f68\u8ff9\uff0c\u5f15\u5165\u57fa\u4e8e\u63d0\u793a\u7684\u7f29\u653e\u786e\u4fdd\u5b9e\u65f6\u98ce\u9669\u68c0\u6d4b\u548c\u5408\u89c4\u54cd\u5e94\u3002", "result": "\u5b9e\u9a8c\u8868\u660eVisuoAlign\u80fd\u4e3b\u52a8\u66b4\u9732\u98ce\u9669\uff0c\u5b9e\u73b0\u5168\u9762\u7684\u6570\u636e\u96c6\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347LVLMs\u5bf9\u590d\u6742\u8de8\u6a21\u6001\u5a01\u80c1\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "VisuoAlign\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5b89\u5168\u5bf9\u9f50\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2510.16148", "categories": ["math.OC", "math.FA", "41A99, 41-04, 49K10, 65K10"], "pdf": "https://arxiv.org/pdf/2510.16148", "abs": "https://arxiv.org/abs/2510.16148", "authors": ["Sebastien Bossu", "Andrew Papanicolaou", "Nour El Hatto"], "title": "Fitting an Escalier to a Curve", "comment": "27 pages, 7 figures, 5 tables", "summary": "We analyze the problem of fitting a fonction en escalier or multi-step\nfunction to a curve in L^2 Hilbert space. We propose a two-stage optimization\napproach whereby the step positions are initially fixed, corresponding to a\nclassic linear least-squares problem with closed-form solution, and then are\nallowed to vary, leading to first-order conditions that can be solved\nrecursively. We find that, subject to regularity conditions, the speed of\nconvergence is linear as the number of steps $n$ goes to infinity, and we\ndevelop a simple algorithm to recover the global optimum fit. Our numerical\nresults based on a sweep search implementation show promising performance in\nterms of speed and accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728L\u00b2\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u62df\u5408\u591a\u6b65\u51fd\u6570\u7684\u4e24\u9636\u6bb5\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u56fa\u5b9a\u6b65\u957f\u4f4d\u7f6e\u548c\u53ef\u53d8\u6b65\u957f\u4f4d\u7f6e\u7684\u9012\u5f52\u6c42\u89e3\uff0c\u5b9e\u73b0\u4e86\u7ebf\u6027\u6536\u655b\u901f\u5ea6\u3002", "motivation": "\u89e3\u51b3\u5728L\u00b2\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u62df\u5408\u591a\u6b65\u51fd\u6570\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u4fdd\u8bc1\u8ba1\u7b97\u6548\u7387\u548c\u5168\u5c40\u6700\u4f18\u89e3\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u4f18\u5316\u65b9\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u56fa\u5b9a\u6b65\u957f\u4f4d\u7f6e\uff0c\u4f7f\u7528\u7ebf\u6027\u6700\u5c0f\u4e8c\u4e58\u6cd5\u83b7\u5f97\u95ed\u5f0f\u89e3\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5141\u8bb8\u6b65\u957f\u4f4d\u7f6e\u53d8\u5316\uff0c\u901a\u8fc7\u9012\u5f52\u6c42\u89e3\u4e00\u9636\u6761\u4ef6\u3002", "result": "\u5728\u6ee1\u8db3\u6b63\u5219\u6027\u6761\u4ef6\u4e0b\uff0c\u968f\u7740\u6b65\u6570n\u8d8b\u4e8e\u65e0\u7a77\u5927\uff0c\u6536\u655b\u901f\u5ea6\u4e3a\u7ebf\u6027\uff1b\u6570\u503c\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u901f\u5ea6\u548c\u7cbe\u5ea6\u65b9\u9762\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u6062\u590d\u5168\u5c40\u6700\u4f18\u62df\u5408\uff0c\u901a\u8fc7\u626b\u63cf\u641c\u7d22\u5b9e\u73b0\u7684\u8ba1\u7b97\u7b97\u6cd5\u5728\u901f\u5ea6\u548c\u51c6\u786e\u6027\u65b9\u9762\u5177\u6709\u4f18\u52bf\u3002"}}
{"id": "2510.15940", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15940", "abs": "https://arxiv.org/abs/2510.15940", "authors": ["Jialin Lu", "Kye Emond", "Kaiyu Yang", "Swarat Chaudhuri", "Weiran Sun", "Wuyang Chen"], "title": "Lean Finder: Semantic Search for Mathlib That Understands User Intents", "comment": null, "summary": "We present Lean Finder, a semantic search engine for Lean and mathlib that\nunderstands and aligns with the intents of mathematicians. Progress in formal\ntheorem proving is often hindered by the difficulty of locating relevant\ntheorems and the steep learning curve of the Lean 4 language, making\nadvancement slow and labor-intensive. Existing Lean search engines, though\nhelpful, rely primarily on informalizations (natural language translation of\nthe formal statements), while largely overlooking the mismatch with real-world\nuser queries. In contrast, we propose a user-centered semantic search tailored\nto the needs of mathematicians. Our approach begins by analyzing and clustering\nthe semantics of public Lean discussions, then fine-tuning text embeddings on\nsynthesized queries that emulate user intents. We further align Lean Finder\nwith mathematicians' preferences using diverse feedback signals, encoding it\nwith a rich awareness of their goals from multiple perspectives. Evaluations on\nreal-world queries, informalized statements, and proof states demonstrate that\nour Lean Finder achieves over $30\\%$ relative improvement compared to previous\nsearch engines and GPT-4o. In addition, Lean Finder is compatible with\nLLM-based theorem provers, bridging retrieval with formal reasoning. Lean\nFinder is available at: https://leanfinder.github.io", "AI": {"tldr": "Lean Finder\u662f\u4e00\u4e2a\u4e13\u4e3a\u6570\u5b66\u5bb6\u548cLean\u7528\u6237\u8bbe\u8ba1\u7684\u8bed\u4e49\u641c\u7d22\u5f15\u64ce\uff0c\u901a\u8fc7\u7406\u89e3\u7528\u6237\u610f\u56fe\u548c\u6570\u5b66\u8bed\u4e49\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b9a\u7406\u641c\u7d22\u6548\u679c\uff0c\u6bd4\u73b0\u6709\u641c\u7d22\u5f15\u64ce\u548cGPT-4o\u670930%\u4ee5\u4e0a\u7684\u76f8\u5bf9\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709Lean\u641c\u7d22\u5f15\u64ce\u4e3b\u8981\u4f9d\u8d56\u5f62\u5f0f\u5316\u8bed\u53e5\u7684\u975e\u6b63\u5f0f\u7ffb\u8bd1\uff0c\u4f46\u5ffd\u89c6\u4e86\u4e0e\u73b0\u5b9e\u7528\u6237\u67e5\u8be2\u7684\u5339\u914d\u95ee\u9898\uff0c\u5bfc\u81f4\u6570\u5b66\u5bb6\u5728\u5b9a\u7406\u8bc1\u660e\u8fc7\u7a0b\u4e2d\u96be\u4ee5\u627e\u5230\u76f8\u5173\u5b9a\u7406\uff0c\u5b66\u4e60\u66f2\u7ebf\u9661\u5ced\uff0c\u8fdb\u5c55\u7f13\u6162\u3002", "method": "\u5206\u6790\u5e76\u805a\u7c7b\u516c\u5f00Lean\u8ba8\u8bba\u7684\u8bed\u4e49\uff0c\u5728\u6a21\u62df\u7528\u6237\u610f\u56fe\u7684\u5408\u6210\u67e5\u8be2\u4e0a\u5fae\u8c03\u6587\u672c\u5d4c\u5165\uff0c\u901a\u8fc7\u591a\u6837\u5316\u53cd\u9988\u4fe1\u53f7\u4e0e\u6570\u5b66\u5bb6\u504f\u597d\u5bf9\u9f50\uff0c\u4ece\u591a\u89d2\u5ea6\u7f16\u7801\u7528\u6237\u76ee\u6807\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u67e5\u8be2\u3001\u975e\u6b63\u5f0f\u5316\u8bed\u53e5\u548c\u8bc1\u660e\u72b6\u6001\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cLean Finder\u76f8\u6bd4\u4e4b\u524d\u7684\u641c\u7d22\u5f15\u64ce\u548cGPT-4o\u5b9e\u73b0\u4e86\u8d85\u8fc730%\u7684\u76f8\u5bf9\u6539\u8fdb\u3002", "conclusion": "Lean Finder\u662f\u4e00\u4e2a\u7528\u6237\u4e2d\u5fc3\u7684\u8bed\u4e49\u641c\u7d22\u7cfb\u7edf\uff0c\u80fd\u591f\u6709\u6548\u7406\u89e3\u6570\u5b66\u5bb6\u610f\u56fe\uff0c\u4e0e\u57fa\u4e8eLLM\u7684\u5b9a\u7406\u8bc1\u660e\u5668\u517c\u5bb9\uff0c\u8fde\u63a5\u4e86\u68c0\u7d22\u4e0e\u5f62\u5f0f\u63a8\u7406\u3002"}}
{"id": "2510.15972", "categories": ["cs.CL", "cs.AI", "81P68 (Primary), 68T50, 68T07 (Secondary)", "I.2.7; F.1.2"], "pdf": "https://arxiv.org/pdf/2510.15972", "abs": "https://arxiv.org/abs/2510.15972", "authors": ["Ling Sun", "Peter Sullivan", "Michael Martin", "Yun Zhou"], "title": "Quantum NLP models on Natural Language Inference", "comment": "Accepted, presented, and to appear in the Proceedings of the Quantum\n  AI and NLP 2025 Conference", "summary": "Quantum natural language processing (QNLP) offers a novel approach to\nsemantic modeling by embedding compositional structure directly into quantum\ncircuits. This paper investigates the application of QNLP models to the task of\nNatural Language Inference (NLI), comparing quantum, hybrid, and classical\ntransformer-based models under a constrained few-shot setting. Using the lambeq\nlibrary and the DisCoCat framework, we construct parameterized quantum circuits\nfor sentence pairs and train them for both semantic relatedness and inference\nclassification. To assess efficiency, we introduce a novel\ninformation-theoretic metric, Information Gain per Parameter (IGPP), which\nquantifies learning dynamics independent of model size. Our results demonstrate\nthat quantum models achieve performance comparable to classical baselines while\noperating with dramatically fewer parameters. The Quantum-based models\noutperform randomly initialized transformers in inference and achieve lower\ntest error on relatedness tasks. Moreover, quantum models exhibit significantly\nhigher per-parameter learning efficiency (up to five orders of magnitude more\nthan classical counterparts), highlighting the promise of QNLP in low-resource,\nstructure-sensitive settings. To address circuit-level isolation and promote\nparameter sharing, we also propose a novel cluster-based architecture that\nimproves generalization by tying gate parameters to learned word clusters\nrather than individual tokens.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u91cf\u5b50\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5728\u5c11\u6837\u672c\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u6bd4\u8f83\u4e86\u91cf\u5b50\u3001\u6df7\u5408\u548c\u7ecf\u5178\u6a21\u578b\uff0c\u53d1\u73b0\u91cf\u5b50\u6a21\u578b\u5728\u53c2\u6570\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u7ecf\u5178\u6a21\u578b\u3002", "motivation": "\u63a2\u7d22\u91cf\u5b50\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5728\u8bed\u4e49\u5efa\u6a21\u4e2d\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u4f4e\u8d44\u6e90\u3001\u7ed3\u6784\u654f\u611f\u7684\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u4efb\u52a1\u4e2d\uff0c\u901a\u8fc7\u91cf\u5b50\u7535\u8def\u76f4\u63a5\u5d4c\u5165\u7ec4\u5408\u7ed3\u6784\u3002", "method": "\u4f7f\u7528lambeq\u5e93\u548cDisCoCat\u6846\u67b6\u6784\u5efa\u53c2\u6570\u5316\u91cf\u5b50\u7535\u8def\uff0c\u8bad\u7ec3\u53e5\u5b50\u5bf9\u8fdb\u884c\u8bed\u4e49\u76f8\u5173\u6027\u548c\u63a8\u7406\u5206\u7c7b\uff0c\u5e76\u5f15\u5165\u4fe1\u606f\u589e\u76ca\u6bcf\u53c2\u6570\u6307\u6807\u8bc4\u4f30\u5b66\u4e60\u6548\u7387\u3002", "result": "\u91cf\u5b50\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u4e0e\u7ecf\u5178\u57fa\u7ebf\u76f8\u5f53\uff0c\u4f46\u53c2\u6570\u6570\u91cf\u5927\u5e45\u51cf\u5c11\uff1b\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u4f18\u4e8e\u968f\u673a\u521d\u59cb\u5316transformer\uff0c\u5728\u76f8\u5173\u6027\u4efb\u52a1\u4e2d\u6d4b\u8bd5\u8bef\u5dee\u66f4\u4f4e\uff1b\u91cf\u5b50\u6a21\u578b\u7684\u5b66\u4e60\u6548\u7387\u6bd4\u7ecf\u5178\u6a21\u578b\u9ad8\u51fa\u6700\u591a\u4e94\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u91cf\u5b50\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5728\u4f4e\u8d44\u6e90\u3001\u7ed3\u6784\u654f\u611f\u573a\u666f\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u63d0\u51fa\u7684\u57fa\u4e8e\u805a\u7c7b\u7684\u67b6\u6784\u901a\u8fc7\u53c2\u6570\u5171\u4eab\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.16247", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.16247", "abs": "https://arxiv.org/abs/2510.16247", "authors": ["H. Mozaffari", "A. Nahvi"], "title": "A Motivational Driver Steering Model: Task Difficulty Homeostasis From Control Theory Perspective", "comment": "Cognitive systems Research", "summary": "A general and psychologically plausible collision avoidance driver model can\nimprove transportation safety significantly. Most computational driver models\nfound in the literature have used control theory methods only, and they are not\nestablished based on psychological theories. In this paper, a unified approach\nis presented based on concepts taken from psychology and control theory. The\n\"task difficulty homeostasis theory\", a prominent motivational theory, is\ncombined with the \"Lyapunov stability method\" in control theory to present a\ngeneral and psychologically plausible model. This approach is used to model\ndriver steering behavior for collision avoidance. The performance of this model\nis measured by simulation of two collision avoidance scenarios at a wide range\nof speeds from 20 km/h to 170 km/h. The model is validated by experiments on a\ndriving simulator. The results demonstrate that the model follows human\nbehavior accurately with a mean error of 7 percent.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5fc3\u7406\u5b66\u548c\u63a7\u5236\u7684\u7edf\u4e00\u65b9\u6cd5\uff0c\u7528\u4e8e\u5efa\u6a21\u9a7e\u9a76\u5458\u907f\u78b0\u8f6c\u5411\u884c\u4e3a\uff0c\u572820-170km/h\u901f\u5ea6\u8303\u56f4\u5185\u9a8c\u8bc1\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u5e73\u5747\u8bef\u5dee\u4e3a7%\u3002", "motivation": "\u73b0\u6709\u9a7e\u9a76\u5458\u6a21\u578b\u5927\u591a\u4ec5\u57fa\u4e8e\u63a7\u5236\u7406\u8bba\uff0c\u7f3a\u4e4f\u5fc3\u7406\u5b66\u7406\u8bba\u57fa\u7840\u3002\u9700\u8981\u5efa\u7acb\u65e2\u901a\u7528\u53c8\u7b26\u5408\u5fc3\u7406\u5b66\u7684\u78b0\u649e\u907f\u514d\u9a7e\u9a76\u5458\u6a21\u578b\u6765\u63d0\u9ad8\u4ea4\u901a\u5b89\u5168\u3002", "method": "\u5c06\u5fc3\u7406\u5b66\u4e2d\u7684\"\u4efb\u52a1\u96be\u5ea6\u7a33\u6001\u7406\u8bba\"\u4e0e\u63a7\u5236\u7406\u8bba\u4e2d\u7684\"\u674e\u96c5\u666e\u8bfa\u592b\u7a33\u5b9a\u6027\u65b9\u6cd5\"\u76f8\u7ed3\u5408\uff0c\u63d0\u51fa\u7edf\u4e00\u65b9\u6cd5\u5efa\u6a21\u9a7e\u9a76\u5458\u907f\u78b0\u8f6c\u5411\u884c\u4e3a\u3002", "result": "\u901a\u8fc7\u9a7e\u9a76\u6a21\u62df\u5668\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u6a21\u578b\u572820-170km/h\u901f\u5ea6\u8303\u56f4\u5185\u7684\u4e24\u79cd\u907f\u78b0\u573a\u666f\u4e2d\uff0c\u5e73\u5747\u8bef\u5dee\u4e3a7%\uff0c\u80fd\u51c6\u786e\u8ddf\u968f\u4eba\u7c7b\u884c\u4e3a\u3002", "conclusion": "\u7ed3\u5408\u5fc3\u7406\u5b66\u548c\u63a7\u5236\u7406\u8bba\u7684\u7edf\u4e00\u65b9\u6cd5\u80fd\u591f\u5efa\u7acb\u901a\u7528\u4e14\u7b26\u5408\u5fc3\u7406\u5b66\u7684\u9a7e\u9a76\u5458\u6a21\u578b\uff0c\u5728\u907f\u78b0\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u6027\u80fd\u3002"}}
{"id": "2510.16938", "categories": ["q-fin.MF", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16938", "abs": "https://arxiv.org/abs/2510.16938", "authors": ["Alok Das", "Kiseop Lee"], "title": "A Topological Approach to Parameterizing Deep Hedging Networks", "comment": null, "summary": "Deep hedging uses recurrent neural networks to hedge financial products that\ncannot be fully hedged in incomplete markets. Previous work in this area\nfocuses on minimizing some measure of quadratic hedging error by calculating\npathwise gradients, but doing so requires large batch sizes and can make\ntraining effective models in a reasonable amount of time challenging. We show\nthat by adding certain topological features, we can reduce batch sizes\nsubstantially and make training these models more practically feasible without\ngreatly compromising hedging performance.", "AI": {"tldr": "\u901a\u8fc7\u6dfb\u52a0\u62d3\u6251\u7279\u5f81\uff0c\u663e\u8457\u51cf\u5c0f\u6df1\u5ea6\u5bf9\u51b2\u8bad\u7ec3\u6240\u9700\u7684\u6279\u91cf\u5927\u5c0f\uff0c\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u800c\u4e0d\u663e\u8457\u5f71\u54cd\u5bf9\u51b2\u6027\u80fd", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5bf9\u51b2\u65b9\u6cd5\u9700\u8981\u5927\u6279\u91cf\u5927\u5c0f\u6765\u8ba1\u7b97\u8def\u5f84\u68af\u5ea6\uff0c\u8bad\u7ec3\u65f6\u95f4\u957f\u4e14\u4e0d\u5b9e\u7528", "method": "\u5728\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u4e2d\u6dfb\u52a0\u7279\u5b9a\u7684\u62d3\u6251\u7279\u5f81", "result": "\u5927\u5e45\u51cf\u5c0f\u6279\u91cf\u5927\u5c0f\uff0c\u4f7f\u8bad\u7ec3\u66f4\u5b9e\u7528\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u51b2\u6027\u80fd", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u4f7f\u6df1\u5ea6\u5bf9\u51b2\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u66f4\u52a0\u53ef\u884c"}}
{"id": "2510.15900", "categories": ["q-fin.ST", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15900", "abs": "https://arxiv.org/abs/2510.15900", "authors": ["Emmanuel Boadi"], "title": "Bitcoin Price Forecasting Based on Hybrid Variational Mode Decomposition and Long Short Term Memory Network", "comment": null, "summary": "This study proposes a hybrid deep learning model for forecasting the price of\nBitcoin, as the digital currency is known to exhibit frequent fluctuations. The\nmodels used are the Variational Mode Decomposition (VMD) and the Long\nShort-Term Memory (LSTM) network. First, VMD is used to decompose the original\nBitcoin price series into Intrinsic Mode Functions (IMFs). Each IMF is then\nmodeled using an LSTM network to capture temporal patterns more effectively.\nThe individual forecasts from the IMFs are aggregated to produce the final\nprediction of the original Bitcoin Price Series. To determine the prediction\npower of the proposed hybrid model, a comparative analysis was conducted\nagainst the standard LSTM. The results confirmed that the hybrid VMD+LSTM model\noutperforms the standard LSTM across all the evaluation metrics, including\nRMSE, MAE and R2 and also provides a reliable 30-day forecast.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u53d8\u5206\u6a21\u6001\u5206\u89e3(VMD)\u548c\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc(LSTM)\u7684\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u6ce2\u52a8\u9891\u7e41\u7684\u6bd4\u7279\u5e01\u4ef7\u683c\uff0c\u8be5\u6a21\u578b\u5728\u5404\u9879\u8bc4\u4f30\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u6807\u51c6LSTM\u6a21\u578b\u3002", "motivation": "\u7531\u4e8e\u6bd4\u7279\u5e01\u7b49\u6570\u5b57\u8d27\u5e01\u4ef7\u683c\u6ce2\u52a8\u9891\u7e41\u4e14\u96be\u4ee5\u9884\u6d4b\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u6709\u6548\u7684\u9884\u6d4b\u6a21\u578b\u6765\u6355\u6349\u5176\u590d\u6742\u7684\u65f6\u95f4\u6a21\u5f0f\u3002", "method": "\u9996\u5148\u4f7f\u7528VMD\u5c06\u539f\u59cb\u6bd4\u7279\u5e01\u4ef7\u683c\u5e8f\u5217\u5206\u89e3\u4e3a\u591a\u4e2a\u672c\u5f81\u6a21\u6001\u51fd\u6570(IMF)\uff0c\u7136\u540e\u5bf9\u6bcf\u4e2aIMF\u4f7f\u7528LSTM\u7f51\u7edc\u5efa\u6a21\u4ee5\u66f4\u6709\u6548\u5730\u6355\u6349\u65f6\u95f4\u6a21\u5f0f\uff0c\u6700\u540e\u5c06\u5404IMF\u7684\u9884\u6d4b\u7ed3\u679c\u805a\u5408\u5f97\u5230\u6700\u7ec8\u4ef7\u683c\u9884\u6d4b\u3002", "result": "\u6df7\u5408VMD+LSTM\u6a21\u578b\u5728\u6240\u6709\u8bc4\u4f30\u6307\u6807(RMSE\u3001MAE\u3001R2)\u4e0a\u5747\u4f18\u4e8e\u6807\u51c6LSTM\u6a21\u578b\uff0c\u5e76\u80fd\u63d0\u4f9b\u53ef\u9760\u768430\u5929\u9884\u6d4b\u3002", "conclusion": "VMD+LSTM\u6df7\u5408\u6a21\u578b\u80fd\u591f\u6709\u6548\u63d0\u5347\u6bd4\u7279\u5e01\u4ef7\u683c\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u6570\u5b57\u8d27\u5e01\u5e02\u573a\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u9884\u6d4b\u5de5\u5177\u3002"}}
{"id": "2510.15934", "categories": ["q-fin.RM", "math.PR", "91G70 (Primary), 62H05 (Secondary), 62P05(Secondary), 60E15\n  (Secondary)"], "pdf": "https://arxiv.org/pdf/2510.15934", "abs": "https://arxiv.org/abs/2510.15934", "authors": ["Daniela I. Flores-Silva", "Miguel A. Sordo", "Alfonso Su\u00e1rez-Llorens"], "title": "Probability equivalent level for CoVaR and VaR in bivariate Student-\\textit{t} copulas with application to foreign exchange risk monitoring", "comment": "25 pages,5 figures", "summary": "We extend the \"probability-equivalent level of VaR and CoVaR\" (PELCoV)\nmethodology to accommodate bivariate risks modeled by a Student-t copula,\nrelaxing the strong dependence assumptions of earlier approaches and enhancing\nthe framework's ability to capture tail dependence and asymmetric co-movements.\nWhile the theoretical results are developed in a static setting, we implement\nthem dynamically to track evolving risk spillovers over time. We illustrate the\npractical relevance of our approach through an application to the foreign\nexchange market, monitoring the USD/GBP exchange rate with the USD/EUR series\nas an auxiliary early warning indicator over the period 1999-2024. Our results\nhighlight the potential of the extended PELCoV framework to detect early signs\nof risk underestimation during periods of financial stress.", "AI": {"tldr": "\u5c06PELCoV\u65b9\u6cd5\u6269\u5c55\u5230\u5b66\u751ft copula\u5efa\u6a21\u7684\u53cc\u53d8\u91cf\u98ce\u9669\uff0c\u653e\u677e\u4e86\u65e9\u671f\u65b9\u6cd5\u7684\u5f3a\u4f9d\u8d56\u6027\u5047\u8bbe\uff0c\u589e\u5f3a\u4e86\u6355\u6349\u5c3e\u90e8\u4f9d\u8d56\u548c\u4e0d\u5bf9\u79f0\u5171\u53d8\u7684\u80fd\u529b\uff0c\u5e76\u5728\u5916\u6c47\u5e02\u573a\u5e94\u7528\u4e2d\u5c55\u793a\u4e86\u5176\u5728\u91d1\u878d\u538b\u529b\u671f\u95f4\u68c0\u6d4b\u98ce\u9669\u4f4e\u4f30\u65e9\u671f\u8ff9\u8c61\u7684\u6f5c\u529b\u3002", "motivation": "\u65e9\u671fPELCoV\u65b9\u6cd5\u5b58\u5728\u5f3a\u4f9d\u8d56\u6027\u5047\u8bbe\u7684\u9650\u5236\uff0c\u65e0\u6cd5\u5145\u5206\u6355\u6349\u5c3e\u90e8\u4f9d\u8d56\u548c\u4e0d\u5bf9\u79f0\u5171\u53d8\uff0c\u9700\u8981\u6269\u5c55\u6846\u67b6\u4ee5\u66f4\u51c6\u786e\u5730\u5efa\u6a21\u53cc\u53d8\u91cf\u98ce\u9669\u3002", "method": "\u5c06PELCoV\u65b9\u6cd5\u6269\u5c55\u5230\u5b66\u751ft copula\u5efa\u6a21\u7684\u53cc\u53d8\u91cf\u98ce\u9669\uff0c\u5728\u9759\u6001\u8bbe\u5b9a\u4e0b\u53d1\u5c55\u7406\u8bba\u7ed3\u679c\uff0c\u5e76\u52a8\u6001\u5b9e\u65bd\u4ee5\u8ddf\u8e2a\u968f\u65f6\u95f4\u6f14\u5316\u7684\u98ce\u9669\u6ea2\u51fa\u3002", "result": "\u5728\u5916\u6c47\u5e02\u573a\u5e94\u7528\u4e2d\uff0c\u4f7f\u7528USD/EUR\u7cfb\u5217\u4f5c\u4e3a\u8f85\u52a9\u9884\u8b66\u6307\u6807\u76d1\u6d4bUSD/GBP\u6c47\u7387\uff081999-2024\u5e74\uff09\uff0c\u6269\u5c55\u7684PELCoV\u6846\u67b6\u80fd\u591f\u68c0\u6d4b\u91d1\u878d\u538b\u529b\u671f\u95f4\u98ce\u9669\u4f4e\u4f30\u7684\u65e9\u671f\u8ff9\u8c61\u3002", "conclusion": "\u6269\u5c55\u7684PELCoV\u6846\u67b6\u901a\u8fc7\u653e\u677e\u4f9d\u8d56\u6027\u5047\u8bbe\u548c\u589e\u5f3a\u5c3e\u90e8\u4f9d\u8d56\u6355\u6349\u80fd\u529b\uff0c\u5728\u98ce\u9669\u76d1\u6d4b\u4e2d\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u7279\u522b\u662f\u5728\u68c0\u6d4b\u91d1\u878d\u538b\u529b\u671f\u95f4\u7684\u98ce\u9669\u4f4e\u4f30\u65b9\u9762\u3002"}}
{"id": "2510.15949", "categories": ["q-fin.TR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15949", "abs": "https://arxiv.org/abs/2510.15949", "authors": ["Charidimos Papadakis", "Angeliki Dimitriou", "Giorgos Filandrianos", "Maria Lymperaiou", "Konstantinos Thomas", "Giorgos Stamou"], "title": "ATLAS: Adaptive Trading with LLM AgentS Through Dynamic Prompt Optimization and Multi-Agent Coordination", "comment": null, "summary": "Large language models show promise for financial decision-making, yet\ndeploying them as autonomous trading agents raises fundamental challenges: how\nto adapt instructions when rewards arrive late and obscured by market noise,\nhow to synthesize heterogeneous information streams into coherent decisions,\nand how to bridge the gap between model outputs and executable market actions.\nWe present ATLAS (Adaptive Trading with LLM AgentS), a unified multi-agent\nframework that integrates structured information from markets, news, and\ncorporate fundamentals to support robust trading decisions. Within ATLAS, the\ncentral trading agent operates in an order-aware action space, ensuring that\noutputs correspond to executable market orders rather than abstract signals.\nThe agent can incorporate feedback while trading using Adaptive-OPRO, a novel\nprompt-optimization technique that dynamically adapts the prompt by\nincorporating real-time, stochastic feedback, leading to increasing performance\nover time. Across regime-specific equity studies and multiple LLM families,\nAdaptive-OPRO consistently outperforms fixed prompts, while reflection-based\nfeedback fails to provide systematic gains.", "AI": {"tldr": "ATLAS\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u4ea4\u6613\u6846\u67b6\uff0c\u901a\u8fc7Adaptive-OPRO\u6280\u672f\u52a8\u6001\u4f18\u5316\u63d0\u793a\uff0c\u5728\u5ef6\u8fdf\u4e14\u566a\u58f0\u7684\u5e02\u573a\u5956\u52b1\u4e0b\u5b9e\u73b0\u81ea\u4e3b\u4ea4\u6613\u51b3\u7b56\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u91d1\u878d\u4ea4\u6613\u4e2d\u9762\u4e34\u7684\u6311\u6218\uff1a\u5ef6\u8fdf\u5956\u52b1\u3001\u5e02\u573a\u566a\u58f0\u3001\u5f02\u6784\u4fe1\u606f\u6574\u5408\u4ee5\u53ca\u6a21\u578b\u8f93\u51fa\u4e0e\u53ef\u6267\u884c\u5e02\u573a\u884c\u4e3a\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u63d0\u51faATLAS\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u6574\u5408\u5e02\u573a\u3001\u65b0\u95fb\u548c\u57fa\u672c\u9762\u4fe1\u606f\uff0c\u91c7\u7528Adaptive-OPRO\u63d0\u793a\u4f18\u5316\u6280\u672f\u52a8\u6001\u8c03\u6574\u63d0\u793a\uff0c\u5728\u8ba2\u5355\u611f\u77e5\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u751f\u6210\u53ef\u6267\u884c\u4ea4\u6613\u6307\u4ee4\u3002", "result": "\u5728\u4e0d\u540c\u5e02\u573a\u673a\u5236\u548c\u591a\u79cdLLM\u5bb6\u65cf\u4e2d\uff0cAdaptive-OPRO\u59cb\u7ec8\u4f18\u4e8e\u56fa\u5b9a\u63d0\u793a\u65b9\u6cd5\uff0c\u800c\u57fa\u4e8e\u53cd\u601d\u7684\u53cd\u9988\u672a\u80fd\u63d0\u4f9b\u7cfb\u7edf\u6027\u6539\u8fdb\u3002", "conclusion": "ATLAS\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u63d0\u793a\u4f18\u5316\u6210\u529f\u89e3\u51b3\u4e86LLM\u5728\u91d1\u878d\u4ea4\u6613\u4e2d\u7684\u6838\u5fc3\u6311\u6218\uff0c\u4e3a\u81ea\u4e3b\u4ea4\u6613\u4ee3\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.16683", "categories": ["econ.EM"], "pdf": "https://arxiv.org/pdf/2510.16683", "abs": "https://arxiv.org/abs/2510.16683", "authors": ["Xiaohong Chen", "Haitian Xie"], "title": "Local Overidentification and Efficiency Gains in Modern Causal Inference and Data Combination", "comment": null, "summary": "This paper studies nonparametric local (over-)identification, in the sense of\nChen and Santos (2018), and the associated semiparametric efficiency in modern\ncausal frameworks. We develop a unified approach that begins by translating\nstructural models with latent variables into their induced statistical models\nof observables and then analyzes local overidentification through conditional\nmoment restrictions. We apply this approach to three leading models: (i) the\ngeneral treatment model under unconfoundedness, (ii) the negative control\nmodel, and (iii) the long-term causal inference model under unobserved\nconfounding. The first design yields a locally just-identified statistical\nmodel, implying that all regular asymptotically linear estimators of the\ntreatment effect share the same asymptotic variance, equal to the (trivial)\nsemiparametric efficiency bound. In contrast, the latter two models involve\nnonparametric endogeneity and are naturally locally overidentified;\nconsequently, some doubly robust orthogonal moment estimators of the average\ntreatment effect are inefficient. Whereas existing work typically imposes\nstrong conditions to restore just-identification before deriving the efficiency\nbound, we relax such assumptions and characterize the general efficiency bound,\nalong with efficient estimators, in the overidentified models (ii) and (iii).", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u975e\u53c2\u6570\u5c40\u90e8\uff08\u8fc7\u5ea6\uff09\u8bc6\u522b\u548c\u534a\u53c2\u6570\u6548\u7387\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u7edf\u4e00\u65b9\u6cd5\u5c06\u7ed3\u6784\u6a21\u578b\u8f6c\u5316\u4e3a\u53ef\u89c2\u6d4b\u7684\u7edf\u8ba1\u6a21\u578b\uff0c\u5e76\u5e94\u7528\u4e8e\u4e09\u79cd\u56e0\u679c\u6a21\u578b\uff1a\u65e0\u6df7\u6742\u7684\u4e00\u822c\u5904\u7406\u6a21\u578b\u3001\u8d1f\u63a7\u5236\u6a21\u578b\u548c\u672a\u89c2\u6d4b\u6df7\u6742\u7684\u957f\u671f\u56e0\u679c\u63a8\u65ad\u6a21\u578b\u3002", "motivation": "\u7814\u7a76\u73b0\u4ee3\u56e0\u679c\u6846\u67b6\u4e2d\u7684\u975e\u53c2\u6570\u5c40\u90e8\u8bc6\u522b\u548c\u534a\u53c2\u6570\u6548\u7387\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5b58\u5728\u8fc7\u5ea6\u8bc6\u522b\u7684\u60c5\u51b5\u4e0b\u5982\u4f55\u63a8\u5bfc\u6548\u7387\u754c\u9650\u548c\u6709\u6548\u4f30\u8ba1\u91cf\u3002", "method": "\u5f00\u53d1\u7edf\u4e00\u65b9\u6cd5\uff1a\u5c06\u5e26\u6709\u6f5c\u53d8\u91cf\u7684\u7ed3\u6784\u6a21\u578b\u8f6c\u5316\u4e3a\u53ef\u89c2\u6d4b\u7684\u7edf\u8ba1\u6a21\u578b\uff0c\u901a\u8fc7\u6761\u4ef6\u77e9\u9650\u5236\u5206\u6790\u5c40\u90e8\u8fc7\u5ea6\u8bc6\u522b\uff0c\u5e94\u7528\u4e8e\u4e09\u79cd\u56e0\u679c\u6a21\u578b\u8bbe\u8ba1\u3002", "result": "\u7b2c\u4e00\u79cd\u8bbe\u8ba1\u4ea7\u751f\u5c40\u90e8\u6070\u597d\u8bc6\u522b\u7684\u7edf\u8ba1\u6a21\u578b\uff0c\u6240\u6709\u6b63\u5219\u6e10\u8fd1\u7ebf\u6027\u4f30\u8ba1\u91cf\u5177\u6709\u76f8\u540c\u6e10\u8fd1\u65b9\u5dee\uff1b\u540e\u4e24\u79cd\u6a21\u578b\u6d89\u53ca\u975e\u53c2\u6570\u5185\u751f\u6027\uff0c\u81ea\u7136\u4ea7\u751f\u5c40\u90e8\u8fc7\u5ea6\u8bc6\u522b\uff0c\u5bfc\u81f4\u67d0\u4e9b\u53cc\u91cd\u7a33\u5065\u6b63\u4ea4\u77e9\u4f30\u8ba1\u91cf\u6548\u7387\u4f4e\u4e0b\u3002", "conclusion": "\u5728\u8fc7\u5ea6\u8bc6\u522b\u6a21\u578b\uff08ii\uff09\u548c\uff08iii\uff09\u4e2d\uff0c\u653e\u677e\u4e86\u73b0\u6709\u6587\u732e\u4e2d\u6062\u590d\u6070\u597d\u8bc6\u522b\u7684\u5f3a\u5047\u8bbe\u6761\u4ef6\uff0c\u63a8\u5bfc\u4e86\u5e7f\u4e49\u6548\u7387\u754c\u9650\u5e76\u6784\u5efa\u4e86\u6709\u6548\u4f30\u8ba1\u91cf\u3002"}}
{"id": "2510.16232", "categories": ["stat.ML", "cs.LG", "cs.MA", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.16232", "abs": "https://arxiv.org/abs/2510.16232", "authors": ["Chenyu Zhang", "Navid Azizan"], "title": "Personalized Collaborative Learning with Affinity-Based Variance Reduction", "comment": null, "summary": "Multi-agent learning faces a fundamental tension: leveraging distributed\ncollaboration without sacrificing the personalization needed for diverse\nagents. This tension intensifies when aiming for full personalization while\nadapting to unknown heterogeneity levels -- gaining collaborative speedup when\nagents are similar, without performance degradation when they are different.\nEmbracing the challenge, we propose personalized collaborative learning (PCL),\na novel framework for heterogeneous agents to collaboratively learn\npersonalized solutions with seamless adaptivity. Through carefully designed\nbias correction and importance correction mechanisms, our method AffPCL\nrobustly handles both environment and objective heterogeneity. We prove that\nAffPCL reduces sample complexity over independent learning by a factor of\n$\\max\\{n^{-1}, \\delta\\}$, where $n$ is the number of agents and\n$\\delta\\in[0,1]$ measures their heterogeneity. This affinity-based acceleration\nautomatically interpolates between the linear speedup of federated learning in\nhomogeneous settings and the baseline of independent learning, without\nrequiring prior knowledge of the system. Our analysis further reveals that an\nagent may obtain linear speedup even by collaborating with arbitrarily\ndissimilar agents, unveiling new insights into personalization and\ncollaboration in the high heterogeneity regime.", "AI": {"tldr": "\u63d0\u51fa\u4e2a\u6027\u5316\u534f\u4f5c\u5b66\u4e60\u6846\u67b6PCL\uff0c\u901a\u8fc7\u504f\u5dee\u6821\u6b63\u548c\u91cd\u8981\u6027\u6821\u6b63\u673a\u5236\u5904\u7406\u5f02\u6784\u6027\uff0c\u5728\u672a\u77e5\u5f02\u6784\u7a0b\u5ea6\u4e0b\u81ea\u52a8\u9002\u5e94\uff0c\u5b9e\u73b0\u4ece\u8054\u90a6\u5b66\u4e60\u7684\u7ebf\u6027\u52a0\u901f\u5230\u72ec\u7acb\u5b66\u4e60\u7684\u65e0\u7f1d\u8fc7\u6e21\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u5b66\u4e60\u9762\u4e34\u5206\u5e03\u5f0f\u534f\u4f5c\u4e0e\u4e2a\u6027\u5316\u9700\u6c42\u4e4b\u95f4\u7684\u57fa\u672c\u77db\u76fe\uff0c\u7279\u522b\u662f\u5728\u5b8c\u5168\u4e2a\u6027\u5316\u4e14\u9002\u5e94\u672a\u77e5\u5f02\u6784\u7a0b\u5ea6\u7684\u60c5\u51b5\u4e0b\uff0c\u9700\u8981\u5728\u667a\u80fd\u4f53\u76f8\u4f3c\u65f6\u83b7\u5f97\u534f\u4f5c\u52a0\u901f\uff0c\u800c\u5728\u4e0d\u540c\u65f6\u4e0d\u964d\u4f4e\u6027\u80fd\u3002", "method": "\u63d0\u51faAffPCL\u65b9\u6cd5\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u504f\u5dee\u6821\u6b63\u548c\u91cd\u8981\u6027\u6821\u6b63\u673a\u5236\uff0c\u7a33\u5065\u5904\u7406\u73af\u5883\u548c\u76ee\u6807\u5f02\u6784\u6027\uff0c\u57fa\u4e8e\u4eb2\u548c\u529b\u7684\u52a0\u901f\u81ea\u52a8\u5728\u8054\u90a6\u5b66\u4e60\u7684\u7ebf\u6027\u52a0\u901f\u548c\u72ec\u7acb\u5b66\u4e60\u4e4b\u95f4\u63d2\u503c\u3002", "result": "\u8bc1\u660eAffPCL\u76f8\u6bd4\u72ec\u7acb\u5b66\u4e60\u5c06\u6837\u672c\u590d\u6742\u5ea6\u964d\u4f4emax{n^{-1}, \u03b4}\u500d\uff0c\u5176\u4e2dn\u662f\u667a\u80fd\u4f53\u6570\u91cf\uff0c\u03b4\u2208[0,1]\u8861\u91cf\u5f02\u6784\u6027\u3002\u5373\u4f7f\u4e0e\u4efb\u610f\u4e0d\u540c\u7684\u667a\u80fd\u4f53\u534f\u4f5c\uff0c\u5355\u4e2a\u667a\u80fd\u4f53\u4ecd\u53ef\u83b7\u5f97\u7ebf\u6027\u52a0\u901f\u3002", "conclusion": "\u63ed\u793a\u4e86\u5728\u9ad8\u5f02\u6784\u6027\u60c5\u51b5\u4e0b\u4e2a\u6027\u5316\u548c\u534f\u4f5c\u7684\u65b0\u89c1\u89e3\uff0cAffPCL\u6846\u67b6\u5728\u672a\u77e5\u7cfb\u7edf\u5148\u9a8c\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u65e0\u7f1d\u81ea\u9002\u5e94\u534f\u4f5c\u5b66\u4e60\u3002"}}
{"id": "2510.15993", "categories": ["q-fin.PM", "cs.LG", "q-fin.ST"], "pdf": "https://arxiv.org/pdf/2510.15993", "abs": "https://arxiv.org/abs/2510.15993", "authors": ["Fernando Spadea", "Oshani Seneviratne"], "title": "Aligning Language Models with Investor and Market Behavior for Financial Recommendations", "comment": null, "summary": "Most financial recommendation systems often fail to account for key\nbehavioral and regulatory factors, leading to advice that is misaligned with\nuser preferences, difficult to interpret, or unlikely to be followed. We\npresent FLARKO (Financial Language-model for Asset Recommendation with\nKnowledge-graph Optimization), a novel framework that integrates Large Language\nModels (LLMs), Knowledge Graphs (KGs), and Kahneman-Tversky Optimization (KTO)\nto generate asset recommendations that are both profitable and behaviorally\naligned. FLARKO encodes users' transaction histories and asset trends as\nstructured KGs, providing interpretable and controllable context for the LLM.\nTo demonstrate the adaptability of our approach, we develop and evaluate both a\ncentralized architecture (CenFLARKO) and a federated variant (FedFLARKO). To\nour knowledge, this is the first demonstration of combining KTO for fine-tuning\nof LLMs for financial asset recommendation. We also present the first use of\nstructured KGs to ground LLM reasoning over behavioral financial data in a\nfederated learning (FL) setting. Evaluated on the FAR-Trans dataset, FLARKO\nconsistently outperforms state-of-the-art recommendation baselines on\nbehavioral alignment and joint profitability, while remaining interpretable and\nresource-efficient.", "AI": {"tldr": "FLARKO\u662f\u4e00\u4e2a\u7ed3\u5408LLM\u3001\u77e5\u8bc6\u56fe\u8c31\u548cKahneman-Tversky\u4f18\u5316\u7684\u91d1\u878d\u63a8\u8350\u6846\u67b6\uff0c\u751f\u6210\u65e2\u76c8\u5229\u53c8\u7b26\u5408\u884c\u4e3a\u504f\u597d\u7684\u8d44\u4ea7\u63a8\u8350\uff0c\u652f\u6301\u96c6\u4e2d\u5f0f\u548c\u8054\u90a6\u5f0f\u67b6\u6784\u3002", "motivation": "\u73b0\u6709\u91d1\u878d\u63a8\u8350\u7cfb\u7edf\u5f80\u5f80\u5ffd\u7565\u884c\u4e3a\u56e0\u7d20\u548c\u76d1\u7ba1\u8981\u6c42\uff0c\u5bfc\u81f4\u5efa\u8bae\u4e0e\u7528\u6237\u504f\u597d\u4e0d\u7b26\u3001\u96be\u4ee5\u89e3\u91ca\u6216\u96be\u4ee5\u9075\u5faa\u3002", "method": "\u5c06\u7528\u6237\u4ea4\u6613\u5386\u53f2\u548c\u8d44\u4ea7\u8d8b\u52bf\u7f16\u7801\u4e3a\u7ed3\u6784\u5316\u77e5\u8bc6\u56fe\u8c31\uff0c\u4e3aLLM\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u4e0a\u4e0b\u6587\uff0c\u7ed3\u5408KTO\u8fdb\u884c\u5fae\u8c03\uff0c\u652f\u6301\u96c6\u4e2d\u5f0f(CenFLARKO)\u548c\u8054\u90a6\u5f0f(FedFLARKO)\u67b6\u6784\u3002", "result": "\u5728FAR-Trans\u6570\u636e\u96c6\u4e0a\uff0cFLARKO\u5728\u884c\u4e3a\u5bf9\u9f50\u548c\u8054\u5408\u76c8\u5229\u80fd\u529b\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u63a8\u8350\u57fa\u7ebf\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u548c\u8d44\u6e90\u6548\u7387\u3002", "conclusion": "FLARKO\u9996\u6b21\u5c06KTO\u7528\u4e8e\u91d1\u878d\u8d44\u4ea7\u63a8\u8350\u7684LLM\u5fae\u8c03\uff0c\u9996\u6b21\u5728\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e2d\u4f7f\u7528\u7ed3\u6784\u5316\u77e5\u8bc6\u56fe\u8c31\u6765\u652f\u6491LLM\u5bf9\u884c\u4e3a\u91d1\u878d\u6570\u636e\u7684\u63a8\u7406\u3002"}}
{"id": "2510.15943", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2510.15943", "abs": "https://arxiv.org/abs/2510.15943", "authors": ["Jimmy Joseph"], "title": "Enabling Responsible, Secure and Sustainable Healthcare AI - A Strategic Framework for Clinical and Operational Impact", "comment": null, "summary": "We offer a pragmatic model to operationalize responsible, secure, and\nsustainable healthcare AI, aligning world-class technical excellence with\norganizational readiness. The framework includes five key pillars - Leadership\n& Strategy, MLOps & Technical Infrastructure, Governance & Ethics, Education &\nWorkforce Development, and Change Management & Adoption - and is intended to\noperationalize 'compliance-by-design' while delivering measurable impact. We\ndemonstrate its utility through two deployments. (A) An inpatient length of\nstay (LOS) prediction service had R^2=0.41-0.58 with validation cohorts in an\nobservational pilot (n = 3,184 encounters, 4 units, June-August 2025). Adoption\nwas 78 percent by week 6, and target units saw 5-10 percent relative declines\nin mean LOS for complex cases vs. pre-pilot baselines. (B) An AI-augmented\nradiology second-reader for lung nodules (PACS-integrated with thresholding and\nexplanation overlays) achieved high sensitivity (95 percent) and provided a\n+8.0 percentage-point lift in detection of sub-centimeter actionable findings,\nwithout slowing workflow (median report TAT 23 min, p = 0.64). Both services\nexecuted in monitored, auditable pipelines with well-defined rollback, bias\nchecks, and no evidence of security incidents. These findings indicate that by\ncombining strong MLOps and AI security with governance, education, and\nhuman-centric change, we can accelerate adoption of AI while improving security\nand outcomes. We end with limitations, generalization considerations, and a\nroadmap for scaling across varied clinical and operational use cases.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8d1f\u8d23\u4efb\u3001\u5b89\u5168\u3001\u53ef\u6301\u7eed\u7684\u533b\u7597AI\u64cd\u4f5c\u5316\u6846\u67b6\uff0c\u5305\u542b\u4e94\u4e2a\u5173\u952e\u652f\u67f1\uff0c\u5e76\u901a\u8fc7\u4e24\u4e2a\u5b9e\u9645\u90e8\u7f72\u6848\u4f8b\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u65e8\u5728\u5c06\u4e16\u754c\u7ea7\u6280\u672f\u5353\u8d8a\u4e0e\u7ec4\u7ec7\u51c6\u5907\u5ea6\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0'\u5408\u89c4\u8bbe\u8ba1'\u7684\u53ef\u64cd\u4f5c\u5316\uff0c\u540c\u65f6\u63d0\u4f9b\u53ef\u8861\u91cf\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u5305\u542b\u9886\u5bfc\u529b\u4e0e\u6218\u7565\u3001MLOps\u4e0e\u6280\u672f\u57fa\u7840\u8bbe\u65bd\u3001\u6cbb\u7406\u4e0e\u4f26\u7406\u3001\u6559\u80b2\u4e0e\u52b3\u52a8\u529b\u53d1\u5c55\u3001\u53d8\u9769\u7ba1\u7406\u4e0e\u91c7\u7528\u4e94\u4e2a\u652f\u67f1\u7684\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u4e24\u4e2a\u5b9e\u9645\u90e8\u7f72\u6848\u4f8b\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u4f4f\u9662\u65f6\u957f\u9884\u6d4b\u670d\u52a1R\u00b2=0.41-0.58\uff0c\u91c7\u7528\u738778%\uff0c\u590d\u6742\u75c5\u4f8b\u5e73\u5747\u4f4f\u9662\u65f6\u957f\u76f8\u5bf9\u4e0b\u964d5-10%\uff1bAI\u589e\u5f3a\u653e\u5c04\u5b66\u4e8c\u6b21\u9605\u7247\u7075\u654f\u5ea695%\uff0c\u4e9a\u5398\u7c73\u53ef\u64cd\u4f5c\u53d1\u73b0\u68c0\u6d4b\u63d0\u53478.0\u4e2a\u767e\u5206\u70b9\uff0c\u5de5\u4f5c\u6d41\u7a0b\u672a\u53d7\u5f71\u54cd\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u5f3a\u5927\u7684MLOps\u548cAI\u5b89\u5168\u4e0e\u6cbb\u7406\u3001\u6559\u80b2\u4ee5\u53ca\u4ee5\u4eba\u4e3a\u672c\u7684\u53d8\u9769\uff0c\u53ef\u4ee5\u5728\u63d0\u9ad8\u5b89\u5168\u6027\u548c\u7ed3\u679c\u7684\u540c\u65f6\u52a0\u901fAI\u7684\u91c7\u7528\u3002"}}
{"id": "2510.15952", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15952", "abs": "https://arxiv.org/abs/2510.15952", "authors": ["Myung Ho Kim"], "title": "Executable Epistemology: The Structured Cognitive Loop as an Architecture of Intentional Understanding", "comment": "27 pages", "summary": "Large language models exhibit intelligence without genuine epistemic\nunderstanding, exposing a key gap: the absence of epistemic architecture. This\npaper introduces the Structured Cognitive Loop (SCL) as an executable\nepistemological framework for emergent intelligence. Unlike traditional AI\nresearch asking \"what is intelligence?\" (ontological), SCL asks \"under what\nconditions does cognition emerge?\" (epistemological). Grounded in philosophy of\nmind and cognitive phenomenology, SCL bridges conceptual philosophy and\nimplementable cognition. Drawing on process philosophy, enactive cognition, and\nextended mind theory, we define intelligence not as a property but as a\nperformed process -- a continuous loop of judgment, memory, control, action,\nand regulation. SCL makes three contributions. First, it operationalizes\nphilosophical insights into computationally interpretable structures, enabling\n\"executable epistemology\" -- philosophy as structural experiment. Second, it\nshows that functional separation within cognitive architecture yields more\ncoherent and interpretable behavior than monolithic prompt based systems,\nsupported by agent evaluations. Third, it redefines intelligence: not\nrepresentational accuracy but the capacity to reconstruct its own epistemic\nstate through intentional understanding. This framework impacts philosophy of\nmind, epistemology, and AI. For philosophy, it allows theories of cognition to\nbe enacted and tested. For AI, it grounds behavior in epistemic structure\nrather than statistical regularity. For epistemology, it frames knowledge not\nas truth possession but as continuous reconstruction within a\nphenomenologically coherent loop. We situate SCL within debates on cognitive\nphenomenology, emergence, normativity, and intentionality, arguing that real\nprogress requires not larger models but architectures that realize cognitive\nprinciples structurally.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u7ed3\u6784\u5316\u8ba4\u77e5\u5faa\u73af\uff08SCL\uff09\u4f5c\u4e3a\u53ef\u6267\u884c\u7684\u8ba4\u8bc6\u8bba\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u73b0\u6d8c\u73b0\u667a\u80fd\uff0c\u5c06\u54f2\u5b66\u6d1e\u89c1\u8f6c\u5316\u4e3a\u53ef\u8ba1\u7b97\u7ed3\u6784\uff0c\u91cd\u65b0\u5b9a\u4e49\u667a\u80fd\u4e3a\u901a\u8fc7\u610f\u5411\u6027\u7406\u89e3\u91cd\u5efa\u81ea\u8eab\u8ba4\u77e5\u72b6\u6001\u7684\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u771f\u6b63\u7684\u8ba4\u77e5\u7406\u89e3\uff0c\u66b4\u9732\u4e86\u8ba4\u77e5\u67b6\u6784\u7684\u7f3a\u5931\u3002\u4f20\u7edfAI\u7814\u7a76\u5173\u6ce8\"\u4ec0\u4e48\u662f\u667a\u80fd\"\u7684\u672c\u4f53\u8bba\u95ee\u9898\uff0c\u800cSCL\u5173\u6ce8\"\u5728\u4ec0\u4e48\u6761\u4ef6\u4e0b\u8ba4\u77e5\u6d8c\u73b0\"\u7684\u8ba4\u8bc6\u8bba\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u5fc3\u667a\u54f2\u5b66\u548c\u8ba4\u77e5\u73b0\u8c61\u5b66\uff0c\u7ed3\u5408\u8fc7\u7a0b\u54f2\u5b66\u3001\u5177\u8eab\u8ba4\u77e5\u548c\u6269\u5c55\u5fc3\u667a\u7406\u8bba\uff0c\u5c06\u667a\u80fd\u5b9a\u4e49\u4e3a\u6267\u884c\u8fc7\u7a0b\u2014\u2014\u5224\u65ad\u3001\u8bb0\u5fc6\u3001\u63a7\u5236\u3001\u884c\u52a8\u548c\u8c03\u8282\u7684\u8fde\u7eed\u5faa\u73af\u3002", "result": "SCL\u5c06\u54f2\u5b66\u6d1e\u89c1\u64cd\u4f5c\u5316\u4e3a\u53ef\u8ba1\u7b97\u7ed3\u6784\uff0c\u5b9e\u73b0\"\u53ef\u6267\u884c\u8ba4\u8bc6\u8bba\"\uff1b\u529f\u80fd\u5206\u79bb\u7684\u8ba4\u77e5\u67b6\u6784\u6bd4\u5355\u4e00\u63d0\u793a\u7cfb\u7edf\u4ea7\u751f\u66f4\u4e00\u81f4\u548c\u53ef\u89e3\u91ca\u7684\u884c\u4e3a\uff1b\u91cd\u65b0\u5b9a\u4e49\u667a\u80fd\u4e3a\u91cd\u5efa\u81ea\u8eab\u8ba4\u77e5\u72b6\u6001\u7684\u80fd\u529b\u3002", "conclusion": "SCL\u6846\u67b6\u5bf9\u5fc3\u667a\u54f2\u5b66\u3001\u8ba4\u8bc6\u8bba\u548cAI\u4ea7\u751f\u91cd\u8981\u5f71\u54cd\uff1a\u5141\u8bb8\u8ba4\u77e5\u7406\u8bba\u88ab\u5b9e\u65bd\u548c\u6d4b\u8bd5\uff1b\u5c06\u884c\u4e3a\u5efa\u7acb\u5728\u8ba4\u77e5\u7ed3\u6784\u800c\u975e\u7edf\u8ba1\u89c4\u5f8b\u4e0a\uff1b\u5c06\u77e5\u8bc6\u89c6\u4e3a\u5728\u73b0\u8c61\u5b66\u4e00\u81f4\u5faa\u73af\u4e2d\u7684\u6301\u7eed\u91cd\u5efa\u3002"}}
{"id": "2510.16154", "categories": ["math.OC", "cs.NA", "math.NA", "68U10, 82C22, 49M25, 49N90"], "pdf": "https://arxiv.org/pdf/2510.16154", "abs": "https://arxiv.org/abs/2510.16154", "authors": ["Alessio Oliviero", "Simone Cacace", "Giuseppe Visconti"], "title": "Agent-Based Optimal Control for Image Processing", "comment": "19 pages, 7 figures", "summary": "We investigate the use of multi-agent systems to solve classical image\nprocessing tasks, such as colour quantization and segmentation. We frame the\ntask as an optimal control problem, where the objective is to steer the\nmulti-agent dynamics to obtain colour clusters that segment the image. To do\nso, we balance the total variation of the colour field and fidelity to the\noriginal image. The solution is obtained resorting to primal-dual splitting and\nthe method of multipliers. Numerical experiments, implemented in parallel with\nCUDA, demonstrate the efficacy of the approach and its potential for\nhigh-dimensional data.", "AI": {"tldr": "\u4f7f\u7528\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u89e3\u51b3\u56fe\u50cf\u5904\u7406\u4efb\u52a1\uff0c\u5c06\u4efb\u52a1\u5efa\u6a21\u4e3a\u6700\u4f18\u63a7\u5236\u95ee\u9898\uff0c\u901a\u8fc7\u5e73\u8861\u989c\u8272\u573a\u7684\u603b\u53d8\u5dee\u548c\u5bf9\u539f\u59cb\u56fe\u50cf\u7684\u4fdd\u771f\u5ea6\u6765\u83b7\u5f97\u989c\u8272\u805a\u7c7b\u548c\u5206\u5272\u3002", "motivation": "\u63a2\u7d22\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u7ecf\u5178\u56fe\u50cf\u5904\u7406\u4efb\u52a1\uff08\u5982\u989c\u8272\u91cf\u5316\u548c\u5206\u5272\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u5bfb\u6c42\u9ad8\u6548\u5904\u7406\u9ad8\u7ef4\u6570\u636e\u7684\u65b9\u6cd5\u3002", "method": "\u5c06\u4efb\u52a1\u6784\u5efa\u4e3a\u6700\u4f18\u63a7\u5236\u95ee\u9898\uff0c\u4f7f\u7528\u539f\u59cb-\u5bf9\u5076\u5206\u88c2\u548c\u4e58\u5b50\u6cd5\u6c42\u89e3\uff0c\u901a\u8fc7CUDA\u5e76\u884c\u5b9e\u73b0\u6570\u503c\u5b9e\u9a8c\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u5904\u7406\u9ad8\u7ef4\u6570\u636e\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6846\u67b6\u4e3a\u56fe\u50cf\u5904\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u5408\u5904\u7406\u9ad8\u7ef4\u6570\u636e\u3002"}}
{"id": "2510.15944", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15944", "abs": "https://arxiv.org/abs/2510.15944", "authors": ["Tianyu Bell Pan", "Mengdi Zhu", "Alexa Jordyn Cole", "Ronald Wilson", "Damon L. Woodard"], "title": "Lyapunov-Stable Adaptive Control for Multimodal Concept Drift", "comment": null, "summary": "Multimodal learning systems often struggle in non-stationary environments due\nto concept drift, where changing data distributions can degrade performance.\nModality-specific drifts and the lack of mechanisms for continuous, stable\nadaptation compound this challenge. This paper introduces LS-OGD, a novel\nadaptive control framework for robust multimodal learning in the presence of\nconcept drift. LS-OGD uses an online controller that dynamically adjusts the\nmodel's learning rate and the fusion weights between different data modalities\nin response to detected drift and evolving prediction errors. We prove that\nunder bounded drift conditions, the LS-OGD system's prediction error is\nuniformly ultimately bounded and converges to zero if the drift ceases.\nAdditionally, we demonstrate that the adaptive fusion strategy effectively\nisolates and mitigates the impact of severe modality-specific drift, thereby\nensuring system resilience and fault tolerance. These theoretical guarantees\nestablish a principled foundation for developing reliable and continuously\nadapting multimodal learning systems.", "AI": {"tldr": "LS-OGD\u662f\u4e00\u4e2a\u9488\u5bf9\u6982\u5ff5\u6f02\u79fb\u7684\u9c81\u68d2\u591a\u6a21\u6001\u5b66\u4e60\u81ea\u9002\u5e94\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5b66\u4e60\u7387\u548c\u6a21\u6001\u878d\u5408\u6743\u91cd\u6765\u5e94\u5bf9\u6570\u636e\u5206\u5e03\u53d8\u5316\u3002", "motivation": "\u591a\u6a21\u6001\u5b66\u4e60\u7cfb\u7edf\u5728\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u5bb9\u6613\u56e0\u6982\u5ff5\u6f02\u79fb\u800c\u6027\u80fd\u4e0b\u964d\uff0c\u7279\u522b\u662f\u6a21\u6001\u7279\u5b9a\u7684\u6f02\u79fb\u548c\u7f3a\u4e4f\u6301\u7eed\u7a33\u5b9a\u9002\u5e94\u673a\u5236\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u5728\u7ebf\u63a7\u5236\u5668\u52a8\u6001\u8c03\u6574\u6a21\u578b\u5b66\u4e60\u7387\u548c\u4e0d\u540c\u6570\u636e\u6a21\u6001\u7684\u878d\u5408\u6743\u91cd\uff0c\u54cd\u5e94\u68c0\u6d4b\u5230\u7684\u6f02\u79fb\u548c\u9884\u6d4b\u8bef\u5dee\u53d8\u5316\u3002", "result": "\u5728\u6709\u9650\u6f02\u79fb\u6761\u4ef6\u4e0b\uff0cLS-OGD\u7cfb\u7edf\u7684\u9884\u6d4b\u8bef\u5dee\u4e00\u81f4\u6700\u7ec8\u6709\u754c\uff0c\u5982\u679c\u6f02\u79fb\u505c\u6b62\u5219\u6536\u655b\u5230\u96f6\uff1b\u81ea\u9002\u5e94\u878d\u5408\u7b56\u7565\u80fd\u6709\u6548\u9694\u79bb\u548c\u51cf\u8f7b\u4e25\u91cd\u6a21\u6001\u7279\u5b9a\u6f02\u79fb\u7684\u5f71\u54cd\u3002", "conclusion": "\u8be5\u7406\u8bba\u4fdd\u8bc1\u4e3a\u5f00\u53d1\u53ef\u9760\u4e14\u6301\u7eed\u81ea\u9002\u5e94\u7684\u591a\u6a21\u6001\u5b66\u4e60\u7cfb\u7edf\u5960\u5b9a\u4e86\u539f\u5219\u6027\u57fa\u7840\u3002"}}
{"id": "2510.16057", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16057", "abs": "https://arxiv.org/abs/2510.16057", "authors": ["Md Kamrul Siam", "Md Jobair Hossain Faruk", "Jerry Q. Cheng", "Huanying Gu"], "title": "Fusion-Augmented Large Language Models: Boosting Diagnostic Trustworthiness via Model Consensus", "comment": "7 pages (Accepted to IEEE BHI 2025)", "summary": "This study presents a novel multi-model fusion framework leveraging two\nstate-of-the-art large language models (LLMs), ChatGPT and Claude, to enhance\nthe reliability of chest X-ray interpretation on the CheXpert dataset. From the\nfull CheXpert corpus of 224,316 chest radiographs, we randomly selected 234\nradiologist-annotated studies to evaluate unimodal performance using image-only\nprompts. In this setting, ChatGPT and Claude achieved diagnostic accuracies of\n62.8% and 76.9%, respectively. A similarity-based consensus approach, using a\n95% output similarity threshold, improved accuracy to 77.6%. To assess the\nimpact of multimodal inputs, we then generated synthetic clinical notes\nfollowing the MIMIC-CXR template and evaluated a separate subset of 50 randomly\nselected cases paired with both images and synthetic text. On this multimodal\ncohort, performance improved to 84% for ChatGPT and 76% for Claude, while\nconsensus accuracy reached 91.3%. Across both experimental conditions,\nagreement-based fusion consistently outperformed individual models. These\nfindings highlight the utility of integrating complementary modalities and\nusing output-level consensus to improve the trustworthiness and clinical\nutility of AI-assisted radiological diagnosis, offering a practical path to\nreduce diagnostic errors with minimal computational overhead.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eChatGPT\u548cClaude\u7684\u591a\u6a21\u578b\u878d\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u76f8\u4f3c\u6027\u5171\u8bc6\u65b9\u6cd5\u63d0\u5347\u80f8\u90e8X\u5149\u7247\u8bca\u65ad\u7684\u53ef\u9760\u6027\uff0c\u5728CheXpert\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u591a\u6a21\u6001\u8f93\u5165\u548c\u5171\u8bc6\u878d\u5408\u7684\u6709\u6548\u6027\u3002", "motivation": "\u63d0\u9ad8AI\u8f85\u52a9\u653e\u5c04\u5b66\u8bca\u65ad\u7684\u53ef\u9760\u6027\u548c\u4e34\u5e8a\u5b9e\u7528\u6027\uff0c\u901a\u8fc7\u591a\u6a21\u578b\u878d\u5408\u51cf\u5c11\u8bca\u65ad\u9519\u8bef\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u4f4e\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "method": "\u4f7f\u7528ChatGPT\u548cClaude\u4e24\u79cd\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u91c7\u7528\u76f8\u4f3c\u6027\u5171\u8bc6\u65b9\u6cd5\uff0895%\u8f93\u51fa\u76f8\u4f3c\u5ea6\u9608\u503c\uff09\uff0c\u5728CheXpert\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u5355\u6a21\u6001\uff08\u4ec5\u56fe\u50cf\uff09\u548c\u591a\u6a21\u6001\uff08\u56fe\u50cf+\u5408\u6210\u4e34\u5e8a\u7b14\u8bb0\uff09\u8f93\u5165\u7684\u6027\u80fd\u3002", "result": "\u5355\u6a21\u6001\u4e0bChatGPT\u51c6\u786e\u738762.8%\uff0cClaude\u51c6\u786e\u738776.9%\uff1b\u5171\u8bc6\u878d\u5408\u540e\u63d0\u5347\u81f377.6%\u3002\u591a\u6a21\u6001\u4e0bChatGPT\u63d0\u5347\u81f384%\uff0cClaude\u4e3a76%\uff0c\u5171\u8bc6\u51c6\u786e\u7387\u8fbe\u523091.3%\u3002\u5171\u8bc6\u878d\u5408\u5728\u6240\u6709\u5b9e\u9a8c\u6761\u4ef6\u4e0b\u5747\u4f18\u4e8e\u5355\u4e2a\u6a21\u578b\u3002", "conclusion": "\u591a\u6a21\u6001\u8f93\u5165\u548c\u8f93\u51fa\u7ea7\u5171\u8bc6\u878d\u5408\u80fd\u663e\u8457\u63d0\u9ad8AI\u8f85\u52a9\u653e\u5c04\u5b66\u8bca\u65ad\u7684\u53ef\u9760\u6027\u548c\u4e34\u5e8a\u5b9e\u7528\u6027\uff0c\u4e3a\u51cf\u5c11\u8bca\u65ad\u9519\u8bef\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2510.16262", "categories": ["eess.SY", "cs.IT", "cs.SY", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.16262", "abs": "https://arxiv.org/abs/2510.16262", "authors": ["Jose Guajardo", "Ali Niknejad"], "title": "Spatial-to-Spectral Harmonic-Modulated Arrays for 6G Multi-Beam MIMO", "comment": null, "summary": "This article presents an overview and analysis of spatial-to-spectral\nharmonic-modulated arrays (SHAs). Compared to traditional analog or digital\nbeamforming arrays, SHAs enable concurrent multi-beamforming without requiring\nsubstantial hardware replication. SHAs replace the need for hardware\nreplication with frequency-domain multiplexing. Furthermore, SHAs have the\npotential to become key contributors to future 6G networks by enabling scalable\nmulti-user communications, joint communication and sensing, and spatial\ninterference mitigation. In addition, an analysis of the SHA's\nharmonic-modulation waveform and its effects on gain, noise and bandwidth is\npresented. A comb-like modulation waveform for SHAs that minimizes spectral\ninefficiency is proposed. Further, an analysis of the SHA's capability to\nindependently steer multiple beams is presented. This capability is quantified\nin terms of the SHA's spatial-to-spectral degrees of freedom. Lastly, this work\nintroduces a novel SHA architecture that provides three spatial-to-spectral\ndegrees of freedom with minimal hardware replication.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u7a7a\u95f4-\u9891\u8c31\u8c10\u6ce2\u8c03\u5236\u9635\u5217(SHAs)\uff0c\u76f8\u6bd4\u4f20\u7edf\u6ce2\u675f\u6210\u5f62\u9635\u5217\uff0cSHAs\u65e0\u9700\u5927\u91cf\u786c\u4ef6\u590d\u5236\u5373\u53ef\u5b9e\u73b0\u5e76\u53d1\u591a\u6ce2\u675f\u6210\u5f62\uff0c\u901a\u8fc7\u9891\u57df\u590d\u7528\u66ff\u4ee3\u786c\u4ef6\u590d\u5236\uff0c\u6709\u671b\u6210\u4e3a6G\u7f51\u7edc\u7684\u5173\u952e\u6280\u672f\u3002", "motivation": "\u4f20\u7edf\u6a21\u62df\u6216\u6570\u5b57\u6ce2\u675f\u6210\u5f62\u9635\u5217\u9700\u8981\u5927\u91cf\u786c\u4ef6\u590d\u5236\u6765\u5b9e\u73b0\u591a\u6ce2\u675f\u6210\u5f62\uff0cSHAs\u65e8\u5728\u901a\u8fc7\u9891\u57df\u590d\u7528\u6280\u672f\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4e3a\u672a\u67656G\u7f51\u7edc\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u591a\u7528\u6237\u901a\u4fe1\u3001\u8054\u5408\u901a\u4fe1\u611f\u77e5\u548c\u7a7a\u95f4\u5e72\u6270\u6291\u5236\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u7a7a\u95f4-\u9891\u8c31\u8c10\u6ce2\u8c03\u5236\u9635\u5217\u67b6\u6784\uff0c\u5206\u6790\u8c10\u6ce2\u8c03\u5236\u6ce2\u5f62\u5bf9\u589e\u76ca\u3001\u566a\u58f0\u548c\u5e26\u5bbd\u7684\u5f71\u54cd\uff0c\u8bbe\u8ba1\u68b3\u72b6\u8c03\u5236\u6ce2\u5f62\u4ee5\u6700\u5c0f\u5316\u9891\u8c31\u6548\u7387\u635f\u5931\uff0c\u5e76\u91cf\u5316SHAs\u72ec\u7acb\u63a7\u5236\u591a\u6ce2\u675f\u7684\u80fd\u529b\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u9896\u7684SHA\u67b6\u6784\uff0c\u4ec5\u9700\u6700\u5c0f\u786c\u4ef6\u590d\u5236\u5373\u53ef\u63d0\u4f9b\u4e09\u4e2a\u7a7a\u95f4-\u9891\u8c31\u81ea\u7531\u5ea6\uff0c\u5b9e\u73b0\u4e86\u72ec\u7acb\u7684\u591a\u6ce2\u675f\u63a7\u5236\u80fd\u529b\u3002", "conclusion": "SHAs\u901a\u8fc7\u9891\u57df\u590d\u7528\u6280\u672f\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u6ce2\u675f\u6210\u5f62\u9635\u5217\u7684\u786c\u4ef6\u590d\u5236\u95ee\u9898\uff0c\u4e3a6G\u7f51\u7edc\u7684\u591a\u7528\u6237\u901a\u4fe1\u3001\u8054\u5408\u901a\u4fe1\u611f\u77e5\u548c\u5e72\u6270\u6291\u5236\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2510.15984", "categories": ["q-fin.PR", "math.PR", "q-fin.MF"], "pdf": "https://arxiv.org/pdf/2510.15984", "abs": "https://arxiv.org/abs/2510.15984", "authors": ["K. E. Feldman"], "title": "Berms without Calibration", "comment": null, "summary": "We derive a new semi-analytical pricing model for Bermudan swaptions based on\nswap rates distributions and correlations between them. The model does not\nrequire product specific calibration.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4e92\u6362\u7387\u5206\u5e03\u548c\u76f8\u5173\u6027\u7684\u767e\u6155\u5927\u4e92\u6362\u671f\u6743\u534a\u89e3\u6790\u5b9a\u4ef7\u6a21\u578b\uff0c\u65e0\u9700\u4ea7\u54c1\u7279\u5b9a\u6821\u51c6", "motivation": "\u5f00\u53d1\u65e0\u9700\u4ea7\u54c1\u7279\u5b9a\u6821\u51c6\u7684\u767e\u6155\u5927\u4e92\u6362\u671f\u6743\u5b9a\u4ef7\u65b9\u6cd5", "method": "\u57fa\u4e8e\u4e92\u6362\u7387\u5206\u5e03\u548c\u4e92\u6362\u7387\u95f4\u76f8\u5173\u6027\u7684\u534a\u89e3\u6790\u5b9a\u4ef7\u6a21\u578b", "result": "\u63a8\u5bfc\u51fa\u65b0\u7684\u767e\u6155\u5927\u4e92\u6362\u671f\u6743\u5b9a\u4ef7\u6a21\u578b", "conclusion": "\u8be5\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u4ea7\u54c1\u7279\u5b9a\u6821\u51c6\u7684\u767e\u6155\u5927\u4e92\u6362\u671f\u6743\u5b9a\u4ef7\u89e3\u51b3\u65b9\u6848"}}
{"id": "2510.15903", "categories": ["q-fin.ST", "cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2510.15903", "abs": "https://arxiv.org/abs/2510.15903", "authors": ["Chi-Sheng Chen", "Aidan Hung-Wen Tsai"], "title": "Quantum and Classical Machine Learning in Decentralized Finance: Comparative Evidence from Multi-Asset Backtesting of Automated Market Makers", "comment": null, "summary": "This study presents a comprehensive empirical comparison between quantum\nmachine learning (QML) and classical machine learning (CML) approaches in\nAutomated Market Makers (AMM) and Decentralized Finance (DeFi) trading\nstrategies through extensive backtesting on 10 models across multiple\ncryptocurrency assets. Our analysis encompasses classical ML models (Random\nForest, Gradient Boosting, Logistic Regression), pure quantum models (VQE\nClassifier, QNN, QSVM), hybrid quantum-classical models (QASA Hybrid, QASA\nSequence, QuantumRWKV), and transformer models. The results demonstrate that\nhybrid quantum models achieve superior overall performance with 11.2\\% average\nreturn and 1.42 average Sharpe ratio, while classical ML models show 9.8\\%\naverage return and 1.47 average Sharpe ratio. The QASA Sequence hybrid model\nachieves the highest individual return of 13.99\\% with the best Sharpe ratio of\n1.76, demonstrating the potential of quantum-classical hybrid approaches in AMM\nand DeFi trading strategies.", "AI": {"tldr": "\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u4e0e\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u5728AMM\u548cDeFi\u4ea4\u6613\u7b56\u7565\u4e2d\u7684\u5b9e\u8bc1\u6bd4\u8f83\uff0c\u7ed3\u679c\u663e\u793a\u6df7\u5408\u91cf\u5b50\u6a21\u578b\u8868\u73b0\u6700\u4f73", "motivation": "\u6bd4\u8f83\u91cf\u5b50\u673a\u5668\u5b66\u4e60(QML)\u548c\u7ecf\u5178\u673a\u5668\u5b66\u4e60(CML)\u5728\u81ea\u52a8\u505a\u5e02\u5546(AMM)\u548c\u53bb\u4e2d\u5fc3\u5316\u91d1\u878d(DeFi)\u4ea4\u6613\u7b56\u7565\u4e2d\u7684\u6027\u80fd\u5dee\u5f02", "method": "\u5bf910\u79cd\u6a21\u578b\u8fdb\u884c\u5e7f\u6cdb\u56de\u6d4b\uff0c\u5305\u62ec\u7ecf\u5178ML\u6a21\u578b\u3001\u7eaf\u91cf\u5b50\u6a21\u578b\u3001\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u6a21\u578b\u548ctransformer\u6a21\u578b\uff0c\u6db5\u76d6\u591a\u79cd\u52a0\u5bc6\u8d27\u5e01\u8d44\u4ea7", "result": "\u6df7\u5408\u91cf\u5b50\u6a21\u578b\u83b7\u5f9711.2%\u5e73\u5747\u56de\u62a5\u7387\u548c1.42\u5e73\u5747\u590f\u666e\u6bd4\u7387\uff0c\u8868\u73b0\u6700\u4f18\uff1bQASA Sequence\u6df7\u5408\u6a21\u578b\u8fbe\u523013.99%\u6700\u9ad8\u56de\u62a5\u7387\u548c1.76\u6700\u4f73\u590f\u666e\u6bd4\u7387", "conclusion": "\u91cf\u5b50-\u7ecf\u5178\u6df7\u5408\u65b9\u6cd5\u5728AMM\u548cDeFi\u4ea4\u6613\u7b56\u7565\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b"}}
{"id": "2510.15937", "categories": ["q-fin.RM", "q-fin.TR", "91G80, 93E20, 90C20, 60H10", "G.1.6; I.2.8"], "pdf": "https://arxiv.org/pdf/2510.15937", "abs": "https://arxiv.org/abs/2510.15937", "authors": ["Jian'an Zhang"], "title": "Tail-Safe Stochastic-Control SPX-VIX Hedging: A White-Box Bridge Between AI Sensitivities and Arbitrage-Free Market Dynamics", "comment": "52 pages; 3 figures; PRIMEarxiv template; fully reproducible artifact\n  (code, configs, plots)", "summary": "We present a white-box, risk-sensitive framework for jointly hedging SPX and\nVIX exposures under transaction costs and regime shifts. The approach couples\nan arbitrage-free market teacher with a control layer that enforces safety as\nconstraints. On the market side, we integrate an SSVI-based implied-volatility\nsurface and a Cboe-compliant VIX computation (including wing pruning and 30-day\ninterpolation), and connect prices to dynamics via a clipped,\nconvexity-preserving Dupire local-volatility extractor. On the control side, we\npose hedging as a small quadratic program with control-barrier-function (CBF)\nboxes for inventory, rate, and tail risk; a sufficient-descent execution gate\nthat trades only when risk drop justifies cost; and three targeted tail-safety\nupgrades: a correlation/expiry-aware VIX weight, guarded no-trade bands, and\nexpiry-aware micro-trade thresholds with cooldown. We prove\nexistence/uniqueness and KKT regularity of the per-step QP, forward invariance\nof safety sets, one-step risk descent when the gate opens, and no chattering\nwith bounded trade rates. For the dynamics layer, we establish positivity and\nsecond-order consistency of the discrete Dupire estimator and give an\nindex-coherence bound linking the teacher VIX to a CIR-style proxy with\nexplicit quadrature and projection errors. In a reproducible synthetic\nenvironment mirroring exchange rules and execution frictions, the controller\nreduces expected shortfall while suppressing nuisance turnover, and the\nteacher-surface construction keeps index-level residuals small and stable.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u767d\u76d2\u98ce\u9669\u654f\u611f\u6846\u67b6\uff0c\u5728\u4ea4\u6613\u6210\u672c\u548c\u5236\u5ea6\u8f6c\u6362\u4e0b\u8054\u5408\u5bf9\u51b2SPX\u548cVIX\u98ce\u9669\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u65e0\u5957\u5229\u5e02\u573a\u6a21\u578b\u548c\u5b89\u5168\u7ea6\u675f\u63a7\u5236\u5c42\uff0c\u901a\u8fc7\u5c0f\u578b\u4e8c\u6b21\u89c4\u5212\u548c\u5c4f\u969c\u51fd\u6570\u5b9e\u73b0\u98ce\u9669\u964d\u4f4e\u548c\u4ea4\u6613\u6548\u7387\u3002", "motivation": "\u9700\u8981\u89e3\u51b3\u5728\u4ea4\u6613\u6210\u672c\u548c\u5e02\u573a\u5236\u5ea6\u53d8\u5316\u4e0b\uff0c\u540c\u65f6\u7ba1\u7406SPX\u548cVIX\u98ce\u9669\u655e\u53e3\u7684\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u98ce\u9669\u63a7\u5236\u548c\u4ea4\u6613\u6548\u7387\u3002", "method": "\u96c6\u6210SSVI\u9690\u542b\u6ce2\u52a8\u7387\u66f2\u9762\u548cCboe\u5408\u89c4VIX\u8ba1\u7b97\uff0c\u4f7f\u7528Dupire\u5c40\u90e8\u6ce2\u52a8\u7387\u63d0\u53d6\u5668\u8fde\u63a5\u4ef7\u683c\u4e0e\u52a8\u6001\u3002\u63a7\u5236\u5c42\u91c7\u7528\u5e26\u5c4f\u969c\u51fd\u6570\u7684\u5c0f\u578b\u4e8c\u6b21\u89c4\u5212\uff0c\u5305\u542b\u98ce\u9669\u4e0b\u964d\u6267\u884c\u95e8\u63a7\u548c\u4e09\u79cd\u5c3e\u90e8\u5b89\u5168\u5347\u7ea7\u673a\u5236\u3002", "result": "\u5728\u6a21\u62df\u4ea4\u6613\u73af\u5883\u4e2d\uff0c\u63a7\u5236\u5668\u964d\u4f4e\u4e86\u9884\u671f\u635f\u5931\u5e76\u6291\u5236\u4e86\u4e0d\u5fc5\u8981\u7684\u4ea4\u6613\u5468\u8f6c\uff0c\u5e02\u573a\u6a21\u578b\u6784\u5efa\u4fdd\u6301\u4e86\u6307\u6570\u6c34\u5e73\u6b8b\u5dee\u7684\u5c0f\u800c\u7a33\u5b9a\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86\u5b89\u5168\u96c6\u7684\u6b63\u5411\u4e0d\u53d8\u6027\u548c\u98ce\u9669\u4e0b\u964d\u7279\u6027\uff0c\u5728\u5b9e\u8df5\u4e2d\u6709\u6548\u5e73\u8861\u4e86\u98ce\u9669\u63a7\u5236\u548c\u4ea4\u6613\u6210\u672c\uff0c\u4e3a\u8054\u5408\u5bf9\u51b2\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u767d\u76d2\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.15988", "categories": ["q-fin.TR", "math.PR", "q-fin.MF", "60H10, 93E20, 93E15"], "pdf": "https://arxiv.org/pdf/2510.15988", "abs": "https://arxiv.org/abs/2510.15988", "authors": ["M. I. Balakaeva", "A. Yu. Veretennikov"], "title": "On Bellman equation in the limit order optimization problem for high-frequency trading", "comment": "19 pages, 7 references", "summary": "An approximation method for construction of optimal strategies in the bid \\&\nask limit order book in the high-frequency trading (HFT) is studied. The basis\nis the article by M. Avellaneda \\& S. Stoikov 2008, in which certain seemingly\nserious gaps have been found; in the present paper they are carefully\ncorrected. However, a bit surprisingly, our corrections do not change the main\nanswer in the cited paper, so that, in fact, the gaps turn out to be\nunimportant. An explanation of this effect is offered.", "AI": {"tldr": "\u672c\u6587\u4fee\u6b63\u4e86Avellaneda & Stoikov (2008)\u8bba\u6587\u4e2d\u7684\u4e00\u4e9b\u91cd\u8981\u7f3a\u9677\uff0c\u4f46\u610f\u5916\u53d1\u73b0\u8fd9\u4e9b\u4fee\u6b63\u5e76\u672a\u6539\u53d8\u539f\u8bba\u6587\u7684\u4e3b\u8981\u7ed3\u8bba\u3002", "motivation": "\u53d1\u73b0Avellaneda & Stoikov (2008)\u5728\u9ad8\u9891\u4ea4\u6613\u9650\u4ef7\u8ba2\u5355\u7c3f\u6700\u4f18\u7b56\u7565\u6784\u5efa\u65b9\u6cd5\u4e2d\u5b58\u5728\u770b\u4f3c\u4e25\u91cd\u7684\u7f3a\u9677\uff0c\u9700\u8981\u4ed4\u7ec6\u4fee\u6b63\u3002", "method": "\u5bf9\u539f\u8bba\u6587\u4e2d\u7684\u8fd1\u4f3c\u65b9\u6cd5\u8fdb\u884c\u4fee\u6b63\uff0c\u586b\u8865\u53d1\u73b0\u7684\u7f3a\u9677\uff0c\u5e76\u5206\u6790\u4fee\u6b63\u5bf9\u6700\u7ec8\u7ed3\u679c\u7684\u5f71\u54cd\u3002", "result": "\u4fee\u6b63\u540e\u7684\u65b9\u6cd5\u867d\u7136\u586b\u8865\u4e86\u539f\u8bba\u6587\u7684\u7f3a\u9677\uff0c\u4f46\u51fa\u4eba\u610f\u6599\u5730\u6ca1\u6709\u6539\u53d8\u4e3b\u8981\u7b54\u6848\u548c\u7ed3\u8bba\u3002", "conclusion": "\u539f\u8bba\u6587\u4e2d\u7684\u7f3a\u9677\u5b9e\u9645\u4e0a\u5e76\u4e0d\u91cd\u8981\uff0c\u4fee\u6b63\u540e\u4ecd\u5f97\u5230\u76f8\u540c\u7684\u4e3b\u8981\u7ed3\u679c\uff0c\u8fd9\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u8da3\u7684\u7814\u7a76\u73b0\u8c61\u89e3\u91ca\u3002"}}
{"id": "2510.16886", "categories": ["econ.EM", "stat.CO"], "pdf": "https://arxiv.org/pdf/2510.16886", "abs": "https://arxiv.org/abs/2510.16886", "authors": ["Hung Tran", "Tien Mai", "Minh Hoang Ha"], "title": "Equilibrium-Constrained Estimation of Recursive Logit Choice Models", "comment": null, "summary": "The recursive logit (RL) model provides a flexible framework for modeling\nsequential decision-making in transportation and choice networks, with\nimportant applications in route choice analysis, multiple discrete choice\nproblems, and activity-based travel demand modeling. Despite its versatility,\nestimation of the RL model typically relies on nested fixed-point (NFXP)\nalgorithms that are computationally expensive and prone to numerical\ninstability. We propose a new approach that reformulates the maximum likelihood\nestimation problem as an optimization problem with equilibrium constraints,\nwhere both the structural parameters and the value functions are treated as\ndecision variables. We further show that this formulation can be equivalently\ntransformed into a conic optimization problem with exponential cones, enabling\nefficient solution using modern conic solvers such as MOSEK. Experiments on\nsynthetic and real-world datasets demonstrate that our convex reformulation\nachieves accuracy comparable to traditional methods while offering significant\nimprovements in computational stability and efficiency, thereby providing a\npractical and scalable alternative for recursive logit model estimation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u9012\u5f52logit\u6a21\u578b\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5c06\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u95ee\u9898\u91cd\u65b0\u8868\u8ff0\u4e3a\u5e26\u5747\u8861\u7ea6\u675f\u7684\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u8f6c\u5316\u4e3a\u9525\u4f18\u5316\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u4f20\u7edf\u9012\u5f52logit\u6a21\u578b\u4f30\u8ba1\u4f9d\u8d56\u5d4c\u5957\u5b9a\u70b9\u7b97\u6cd5\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u6570\u503c\u4e0d\u7a33\u5b9a\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7a33\u5b9a\u7684\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "\u5c06\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u91cd\u65b0\u8868\u8ff0\u4e3a\u5e26\u5747\u8861\u7ea6\u675f\u7684\u4f18\u5316\u95ee\u9898\uff0c\u5c06\u7ed3\u6784\u53c2\u6570\u548c\u4ef7\u503c\u51fd\u6570\u90fd\u4f5c\u4e3a\u51b3\u7b56\u53d8\u91cf\uff0c\u5e76\u8f6c\u5316\u4e3a\u6307\u6570\u9525\u4f18\u5316\u95ee\u9898\uff0c\u4f7f\u7528\u73b0\u4ee3\u9525\u6c42\u89e3\u5668\u5982MOSEK\u6c42\u89e3\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u51f8\u91cd\u6784\u65b9\u6cd5\u5728\u7cbe\u5ea6\u4e0a\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u5f53\uff0c\u540c\u65f6\u5728\u8ba1\u7b97\u7a33\u5b9a\u6027\u548c\u6548\u7387\u4e0a\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u63d0\u51fa\u7684\u51f8\u91cd\u6784\u65b9\u6cd5\u4e3a\u9012\u5f52logit\u6a21\u578b\u4f30\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u8ba1\u7b97\u6548\u7387\u4f4e\u548c\u6570\u503c\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\u3002"}}
{"id": "2510.16636", "categories": ["q-fin.ST", "cs.LG", "q-fin.CP"], "pdf": "https://arxiv.org/pdf/2510.16636", "abs": "https://arxiv.org/abs/2510.16636", "authors": ["Abraham Atsiwo"], "title": "A three-step machine learning approach to predict market bubbles with financial news", "comment": null, "summary": "This study presents a three-step machine learning framework to predict\nbubbles in the S&P 500 stock market by combining financial news sentiment with\nmacroeconomic indicators. Building on traditional econometric approaches, the\nproposed approach predicts bubble formation by integrating textual and\nquantitative data sources. In the first step, bubble periods in the S&P 500\nindex are identified using a right-tailed unit root test, a widely recognized\nreal-time bubble detection method. The second step extracts sentiment features\nfrom large-scale financial news articles using natural language processing\n(NLP) techniques, which capture investors' expectations and behavioral\npatterns. In the final step, ensemble learning methods are applied to predict\nbubble occurrences based on high sentiment-based and macroeconomic predictors.\nModel performance is evaluated through k-fold cross-validation and compared\nagainst benchmark machine learning algorithms. Empirical results indicate that\nthe proposed three-step ensemble approach significantly improves predictive\naccuracy and robustness, providing valuable early warning insights for\ninvestors, regulators, and policymakers in mitigating systemic financial risks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u4e09\u6b65\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u91d1\u878d\u65b0\u95fb\u60c5\u611f\u548c\u5b8f\u89c2\u7ecf\u6d4e\u6307\u6807\u6765\u9884\u6d4b\u6807\u666e500\u80a1\u5e02\u6ce1\u6cab\uff0c\u663e\u8457\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u548c\u7a33\u5065\u6027\u3002", "motivation": "\u4f20\u7edf\u8ba1\u91cf\u7ecf\u6d4e\u5b66\u65b9\u6cd5\u5728\u9884\u6d4b\u80a1\u5e02\u6ce1\u6cab\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u6574\u5408\u6587\u672c\u548c\u5b9a\u91cf\u6570\u636e\u6e90\u6765\u66f4\u597d\u5730\u6355\u6349\u6295\u8d44\u8005\u9884\u671f\u548c\u884c\u4e3a\u6a21\u5f0f\u3002", "method": "1) \u4f7f\u7528\u53f3\u5c3e\u5355\u4f4d\u6839\u68c0\u9a8c\u8bc6\u522b\u6ce1\u6cab\u671f\uff1b2) \u901a\u8fc7NLP\u6280\u672f\u4ece\u91d1\u878d\u65b0\u95fb\u63d0\u53d6\u60c5\u611f\u7279\u5f81\uff1b3) \u5e94\u7528\u96c6\u6210\u5b66\u4e60\u65b9\u6cd5\u57fa\u4e8e\u60c5\u611f\u548c\u5b8f\u89c2\u7ecf\u6d4e\u6307\u6807\u9884\u6d4b\u6ce1\u6cab\u53d1\u751f\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u4e09\u6b65\u96c6\u6210\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u548c\u7a33\u5065\u6027\uff0c\u4e3a\u6295\u8d44\u8005\u3001\u76d1\u7ba1\u673a\u6784\u548c\u653f\u7b56\u5236\u5b9a\u8005\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u65e9\u671f\u9884\u8b66\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u6574\u5408\u6587\u672c\u60c5\u611f\u5206\u6790\u548c\u5b8f\u89c2\u7ecf\u6d4e\u6307\u6807\uff0c\u6709\u6548\u6539\u8fdb\u4e86\u80a1\u5e02\u6ce1\u6cab\u9884\u6d4b\uff0c\u6709\u52a9\u4e8e\u7f13\u89e3\u7cfb\u7edf\u6027\u91d1\u878d\u98ce\u9669\u3002"}}
{"id": "2510.16419", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16419", "abs": "https://arxiv.org/abs/2510.16419", "authors": ["Jiayi Guo", "Haoxuan Li", "Ye Tian", "Peng Wu"], "title": "A Relative Error-Based Evaluation Framework of Heterogeneous Treatment Effect Estimators", "comment": null, "summary": "While significant progress has been made in heterogeneous treatment effect\n(HTE) estimation, the evaluation of HTE estimators remains underdeveloped. In\nthis article, we propose a robust evaluation framework based on relative error,\nwhich quantifies performance differences between two HTE estimators. We first\nderive the key theoretical conditions on the nuisance parameters that are\nnecessary to achieve a robust estimator of relative error. Building on these\nconditions, we introduce novel loss functions and design a neural network\narchitecture to estimate nuisance parameters and obtain robust estimation of\nrelative error, thereby achieving reliable evaluation of HTE estimators. We\nprovide the large sample properties of the proposed relative error estimator.\nFurthermore, beyond evaluation, we propose a new learning algorithm for HTE\nthat leverages both the previously HTE estimators and the nuisance parameters\nlearned through our neural network architecture. Extensive experiments\ndemonstrate that our evaluation framework supports reliable comparisons across\nHTE estimators, and the proposed learning algorithm for HTE exhibits desirable\nperformance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u76f8\u5bf9\u8bef\u5dee\u7684\u5f02\u8d28\u6027\u5904\u7406\u6548\u5e94(HTE)\u4f30\u8ba1\u5668\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u63a8\u5bfc\u5173\u952e\u7406\u8bba\u6761\u4ef6\u3001\u8bbe\u8ba1\u65b0\u9896\u635f\u5931\u51fd\u6570\u548c\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u6765\u7a33\u5065\u4f30\u8ba1\u76f8\u5bf9\u8bef\u5dee\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86\u65b0\u7684HTE\u5b66\u4e60\u7b97\u6cd5\u3002", "motivation": "\u867d\u7136HTE\u4f30\u8ba1\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5176\u8bc4\u4f30\u65b9\u6cd5\u4ecd\u4e0d\u5b8c\u5584\uff0c\u9700\u8981\u5f00\u53d1\u7a33\u5065\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u53ef\u9760\u6bd4\u8f83\u4e0d\u540cHTE\u4f30\u8ba1\u5668\u7684\u6027\u80fd\u3002", "method": "\u63a8\u5bfc\u4e86\u5b9e\u73b0\u7a33\u5065\u76f8\u5bf9\u8bef\u5dee\u4f30\u8ba1\u6240\u9700\u7684\u5e72\u6270\u53c2\u6570\u7406\u8bba\u6761\u4ef6\uff0c\u8bbe\u8ba1\u4e86\u65b0\u9896\u635f\u5931\u51fd\u6570\u548c\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u6765\u4f30\u8ba1\u5e72\u6270\u53c2\u6570\uff0c\u5e76\u57fa\u4e8e\u6b64\u6784\u5efa\u4e86\u65b0\u7684HTE\u5b66\u4e60\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u8bc4\u4f30\u6846\u67b6\u80fd\u591f\u53ef\u9760\u6bd4\u8f83HTE\u4f30\u8ba1\u5668\uff0c\u4e14\u63d0\u51fa\u7684HTE\u5b66\u4e60\u7b97\u6cd5\u8868\u73b0\u51fa\u4f18\u826f\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u76f8\u5bf9\u8bef\u5dee\u8bc4\u4f30\u6846\u67b6\u4e3aHTE\u4f30\u8ba1\u5668\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6bd4\u8f83\u65b9\u6cd5\uff0c\u540c\u65f6\u57fa\u4e8e\u8be5\u6846\u67b6\u5f00\u53d1\u7684\u5b66\u4e60\u7b97\u6cd5\u5728HTE\u4f30\u8ba1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.17393", "categories": ["q-fin.PM"], "pdf": "https://arxiv.org/pdf/2510.17393", "abs": "https://arxiv.org/abs/2510.17393", "authors": ["Kefan Chen", "Hussain Ahmad", "Diksha Goel", "Claudia Szabo"], "title": "3S-Trader: A Multi-LLM Framework for Adaptive Stock Scoring, Strategy, and Selection in Portfolio Optimization", "comment": null, "summary": "Large Language Models (LLMs) have recently gained popularity in stock trading\nfor their ability to process multimodal financial data. However, most existing\nmethods focus on single-stock trading and lack the capacity to reason over\nmultiple candidates for portfolio construction. Moreover, they typically lack\nthe flexibility to revise their strategies in response to market shifts,\nlimiting their adaptability in real-world trading. To address these challenges,\nwe propose 3S-Trader, a training-free framework that incorporates scoring,\nstrategy, and selection modules for stock portfolio construction. The scoring\nmodule summarizes each stock's recent signals into a concise report covering\nmultiple scoring dimensions, enabling efficient comparison across candidates.\nThe strategy module analyzes historical strategies and overall market\nconditions to iteratively generate an optimized selection strategy. Based on\nthis strategy, the selection module identifies and assembles a portfolio by\nchoosing stocks with higher scores in relevant dimensions. We evaluate our\nframework across four distinct stock universes, including the Dow Jones\nIndustrial Average (DJIA) constituents and three sector-specific stock sets.\nCompared with existing multi-LLM frameworks and time-series-based baselines,\n3S-Trader achieves the highest accumulated return of 131.83% on DJIA\nconstituents with a Sharpe ratio of 0.31 and Calmar ratio of 11.84, while also\ndelivering consistently strong results across other sectors.", "AI": {"tldr": "3S-Trader\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u6295\u8d44\u7ec4\u5408\u6784\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u8bc4\u5206\u3001\u7b56\u7565\u548c\u9009\u62e9\u4e09\u4e2a\u6a21\u5757\uff0c\u5728\u591a\u80a1\u7968\u5019\u9009\u4e2d\u8fdb\u884c\u63a8\u7406\u548c\u9009\u62e9\uff0c\u5728DJIA\u6210\u5206\u80a1\u4e0a\u5b9e\u73b0\u4e86131.83%\u7684\u7d2f\u8ba1\u56de\u62a5\u3002", "motivation": "\u73b0\u6709LLM\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5355\u80a1\u7968\u4ea4\u6613\uff0c\u7f3a\u4e4f\u591a\u5019\u9009\u63a8\u7406\u80fd\u529b\uff0c\u4e14\u65e0\u6cd5\u7075\u6d3b\u5e94\u5bf9\u5e02\u573a\u53d8\u5316\uff0c\u9650\u5236\u4e86\u5728\u771f\u5b9e\u4ea4\u6613\u4e2d\u7684\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51fa3S-Trader\u6846\u67b6\uff0c\u5305\u542b\u8bc4\u5206\u6a21\u5757\uff08\u603b\u7ed3\u80a1\u7968\u4fe1\u53f7\uff09\u3001\u7b56\u7565\u6a21\u5757\uff08\u5206\u6790\u5386\u53f2\u7b56\u7565\u548c\u5e02\u573a\u6761\u4ef6\u751f\u6210\u4f18\u5316\u7b56\u7565\uff09\u3001\u9009\u62e9\u6a21\u5757\uff08\u6839\u636e\u7b56\u7565\u9009\u62e9\u9ad8\u8bc4\u5206\u80a1\u7968\u6784\u5efa\u6295\u8d44\u7ec4\u5408\uff09\u3002", "result": "\u5728\u56db\u4e2a\u4e0d\u540c\u80a1\u7968\u96c6\u5408\uff08\u5305\u62ecDJIA\u6210\u5206\u80a1\u548c\u4e09\u4e2a\u884c\u4e1a\u7279\u5b9a\u80a1\u7968\u96c6\uff09\u4e0a\u8bc4\u4f30\uff0c3S-Trader\u5728DJIA\u6210\u5206\u80a1\u4e0a\u83b7\u5f97131.83%\u7d2f\u8ba1\u56de\u62a5\uff0c\u590f\u666e\u6bd4\u73870.31\uff0c\u5361\u5c14\u739b\u6bd4\u738711.84\uff0c\u5728\u5176\u4ed6\u884c\u4e1a\u4e5f\u8868\u73b0\u7a33\u5b9a\u3002", "conclusion": "3S-Trader\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86\u591a\u80a1\u7968\u6295\u8d44\u7ec4\u5408\u6784\u5efa\u95ee\u9898\uff0c\u5c55\u73b0\u4e86\u5728\u771f\u5b9e\u5e02\u573a\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u548c\u4f18\u5f02\u8868\u73b0\u3002"}}
{"id": "2510.15951", "categories": ["cs.CY", "cs.CL", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15951", "abs": "https://arxiv.org/abs/2510.15951", "authors": ["Kaitlyn Zhou", "Kristina Gligori\u0107", "Myra Cheng", "Michelle S. Lam", "Vyoma Raman", "Boluwatife Aminu", "Caeley Woo", "Michael Brockman", "Hannah Cha", "Dan Jurafsky"], "title": "Attention to Non-Adopters", "comment": null, "summary": "Although language model-based chat systems are increasingly used in daily\nlife, most Americans remain non-adopters of chat-based LLMs -- as of June 2025,\n66% had never used ChatGPT. At the same time, LLM development and evaluation\nrely mainly on data from adopters (e.g., logs, preference data), focusing on\nthe needs and tasks for a limited demographic group of adopters in terms of\ngeographic location, education, and gender. In this position paper, we argue\nthat incorporating non-adopter perspectives is essential for developing broadly\nuseful and capable LLMs. We contend that relying on methods that focus\nprimarily on adopters will risk missing a range of tasks and needs prioritized\nby non-adopters, entrenching inequalities in who benefits from LLMs, and\ncreating oversights in model development and evaluation. To illustrate this\nclaim, we conduct case studies with non-adopters and show: how non-adopter\nneeds diverge from those of current users, how non-adopter needs point us\ntowards novel reasoning tasks, and how to systematically integrate non-adopter\nneeds via human-centered methods.", "AI": {"tldr": "\u8bba\u6587\u4e3b\u5f20\u5c06\u975e\u91c7\u7528\u8005\u89c6\u89d2\u7eb3\u5165LLM\u5f00\u53d1\uff0c\u4ee5\u907f\u514d\u4ec5\u5173\u6ce8\u91c7\u7528\u8005\u7fa4\u4f53\u5bfc\u81f4\u7684\u4efb\u52a1\u9057\u6f0f\u3001\u4e0d\u5e73\u7b49\u52a0\u5267\u548c\u6a21\u578b\u5f00\u53d1\u76f2\u70b9\u3002", "motivation": "\u5927\u591a\u6570\u7f8e\u56fd\u4eba\uff0866%\uff09\u4ece\u672a\u4f7f\u7528\u8fc7ChatGPT\uff0c\u4f46LLM\u5f00\u53d1\u548c\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u91c7\u7528\u8005\u6570\u636e\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u65e0\u6cd5\u6ee1\u8db3\u66f4\u5e7f\u6cdb\u4eba\u7fa4\u7684\u9700\u6c42\uff0c\u52a0\u5267\u4e0d\u5e73\u7b49\u3002", "method": "\u901a\u8fc7\u4e0e\u975e\u91c7\u7528\u8005\u8fdb\u884c\u6848\u4f8b\u7814\u7a76\uff0c\u5206\u6790\u5176\u9700\u6c42\u5dee\u5f02\uff0c\u8bc6\u522b\u65b0\u578b\u63a8\u7406\u4efb\u52a1\uff0c\u5e76\u91c7\u7528\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u65b9\u6cd5\u7cfb\u7edf\u6574\u5408\u975e\u91c7\u7528\u8005\u9700\u6c42\u3002", "result": "\u7814\u7a76\u8868\u660e\u975e\u91c7\u7528\u8005\u7684\u9700\u6c42\u4e0e\u5f53\u524d\u7528\u6237\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u8fd9\u4e9b\u9700\u6c42\u6307\u5411\u4e86\u65b0\u578b\u63a8\u7406\u4efb\u52a1\uff0c\u4e3aLLM\u5f00\u53d1\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002", "conclusion": "\u5c06\u975e\u91c7\u7528\u8005\u89c6\u89d2\u7eb3\u5165LLM\u5f00\u53d1\u5bf9\u4e8e\u521b\u5efa\u5e7f\u6cdb\u6709\u7528\u4e14\u80fd\u529b\u5168\u9762\u7684\u8bed\u8a00\u6a21\u578b\u81f3\u5173\u91cd\u8981\uff0c\u6709\u52a9\u4e8e\u907f\u514d\u6a21\u578b\u5f00\u53d1\u4e2d\u7684\u504f\u89c1\u548c\u4e0d\u5e73\u7b49\u95ee\u9898\u3002"}}
{"id": "2510.15959", "categories": ["cs.AI", "cs.CY", "cs.ET", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.15959", "abs": "https://arxiv.org/abs/2510.15959", "authors": ["Isabelle Hupont", "Marisa Ponti", "Sven Schade"], "title": "Exploring the Potential of Citiverses for Regulatory Learning", "comment": "26 pages", "summary": "Citiverses hold the potential to support regulatory learning by offering\nimmersive, virtual environments for experimenting with policy scenarios and\ntechnologies. This paper proposes a science-for-policy agenda to explore the\npotential of citiverses as experimentation spaces for regulatory learning,\ngrounded in a consultation with a high-level panel of experts, including\npolicymakers from the European Commission, national government science advisers\nand leading researchers in digital regulation and virtual worlds. It identifies\nkey research areas, including scalability, real-time feedback, complexity\nmodelling, cross-border collaboration, risk reduction, citizen participation,\nethical considerations and the integration of emerging technologies. In\naddition, the paper analyses a set of experimental topics, spanning\ntransportation, urban planning and the environment/climate crisis, that could\nbe tested in citiverse platforms to advance regulatory learning in these areas.\nThe proposed work is designed to inform future research for policy and\nemphasizes a responsible approach to developing and using citiverses. It\nprioritizes careful consideration of the ethical, economic, ecological and\nsocial dimensions of different regulations. The paper also explores essential\npreliminary steps necessary for integrating citiverses into the broader\necosystems of experimentation spaces, including test beds, living labs and\nregulatory sandboxes", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u79d1\u5b66\u653f\u7b56\u8bae\u7a0b\uff0c\u63a2\u7d22citiverses\u4f5c\u4e3a\u76d1\u7ba1\u5b66\u4e60\u5b9e\u9a8c\u7a7a\u95f4\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u4e13\u5bb6\u54a8\u8be2\u8bc6\u522b\u5173\u952e\u7814\u7a76\u9886\u57df\u548c\u5b9e\u9a8c\u4e3b\u9898\uff0c\u5f3a\u8c03\u8d1f\u8d23\u4efb\u7684\u53d1\u5c55\u65b9\u6cd5\u3002", "motivation": "citiverses\u5177\u6709\u901a\u8fc7\u6c89\u6d78\u5f0f\u865a\u62df\u73af\u5883\u652f\u6301\u76d1\u7ba1\u5b66\u4e60\u7684\u6f5c\u529b\uff0c\u4e3a\u653f\u7b56\u573a\u666f\u548c\u6280\u672f\u5b9e\u9a8c\u63d0\u4f9b\u5e73\u53f0\u3002", "method": "\u57fa\u4e8e\u4e0e\u9ad8\u7ea7\u4e13\u5bb6\u5c0f\u7ec4\uff08\u5305\u62ec\u6b27\u76df\u59d4\u5458\u4f1a\u653f\u7b56\u5236\u5b9a\u8005\u3001\u56fd\u5bb6\u653f\u5e9c\u79d1\u5b66\u987e\u95ee\u548c\u6570\u5b57\u76d1\u7ba1\u9886\u57df\u9886\u5148\u7814\u7a76\u8005\uff09\u7684\u54a8\u8be2\uff0c\u8bc6\u522b\u5173\u952e\u7814\u7a76\u9886\u57df\u548c\u5b9e\u9a8c\u4e3b\u9898\u3002", "result": "\u786e\u5b9a\u4e86\u53ef\u6269\u5c55\u6027\u3001\u5b9e\u65f6\u53cd\u9988\u3001\u590d\u6742\u6027\u5efa\u6a21\u3001\u8de8\u5883\u534f\u4f5c\u3001\u98ce\u9669\u964d\u4f4e\u3001\u516c\u6c11\u53c2\u4e0e\u3001\u4f26\u7406\u8003\u8651\u548c\u65b0\u5174\u6280\u672f\u6574\u5408\u7b49\u5173\u952e\u7814\u7a76\u9886\u57df\uff0c\u4ee5\u53ca\u4ea4\u901a\u3001\u57ce\u5e02\u89c4\u5212\u3001\u73af\u5883/\u6c14\u5019\u5371\u673a\u7b49\u5b9e\u9a8c\u4e3b\u9898\u3002", "conclusion": "citiverses\u53ef\u4f5c\u4e3a\u76d1\u7ba1\u5b66\u4e60\u7684\u91cd\u8981\u5b9e\u9a8c\u7a7a\u95f4\uff0c\u4f46\u9700\u8981\u8d1f\u8d23\u4efb\u7684\u53d1\u5c55\u65b9\u6cd5\uff0c\u5145\u5206\u8003\u8651\u4f26\u7406\u3001\u7ecf\u6d4e\u3001\u751f\u6001\u548c\u793e\u4f1a\u7ef4\u5ea6\uff0c\u5e76\u6574\u5408\u5230\u66f4\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u751f\u6001\u7cfb\u7edf\u3002"}}
{"id": "2510.16376", "categories": ["math.OC", "cs.AI", "cs.RO", "cs.SY", "eess.SY", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.16376", "abs": "https://arxiv.org/abs/2510.16376", "authors": ["Han Wang", "Chao Ning"], "title": "Conformal Prediction in The Loop: A Feedback-Based Uncertainty Model for Trajectory Optimization", "comment": "Accepted by NeurIPS 2025 Main Track", "summary": "Conformal Prediction (CP) is a powerful statistical machine learning tool to\nconstruct uncertainty sets with coverage guarantees, which has fueled its\nextensive adoption in generating prediction regions for decision-making tasks,\ne.g., Trajectory Optimization (TO) in uncertain environments. However, existing\nmethods predominantly employ a sequential scheme, where decisions rely\nunidirectionally on the prediction regions, and consequently the information\nfrom decision-making fails to be fed back to instruct CP. In this paper, we\npropose a novel Feedback-Based CP (Fb-CP) framework for shrinking-horizon TO\nwith a joint risk constraint over the entire mission time. Specifically, a\nCP-based posterior risk calculation method is developed by fully leveraging the\nrealized trajectories to adjust the posterior allowable risk, which is then\nallocated to future times to update prediction regions. In this way, the\ninformation in the realized trajectories is continuously fed back to the CP,\nenabling attractive feedback-based adjustments of the prediction regions and a\nprovable online improvement in trajectory performance. Furthermore, we\ntheoretically prove that such adjustments consistently maintain the coverage\nguarantees of the prediction regions, thereby ensuring provable safety.\nAdditionally, we develop a decision-focused iterative risk allocation algorithm\nwith theoretical convergence analysis for allocating the posterior allowable\nrisk which closely aligns with Fb-CP. Furthermore, we extend the proposed\nmethod to handle distribution shift. The effectiveness and superiority of the\nproposed method are demonstrated through benchmark experiments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cd\u9988\u7684\u5171\u5f62\u9884\u6d4b\u6846\u67b6(Fb-CP)\uff0c\u7528\u4e8e\u5177\u6709\u8054\u5408\u98ce\u9669\u7ea6\u675f\u7684\u6536\u7f29\u65f6\u57df\u8f68\u8ff9\u4f18\u5316\uff0c\u901a\u8fc7\u5c06\u5df2\u5b9e\u73b0\u8f68\u8ff9\u7684\u4fe1\u606f\u53cd\u9988\u7ed9CP\u6765\u5728\u7ebf\u6539\u8fdb\u8f68\u8ff9\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u8986\u76d6\u4fdd\u8bc1\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u91c7\u7528\u987a\u5e8f\u65b9\u6848\uff0c\u51b3\u7b56\u5355\u5411\u4f9d\u8d56\u9884\u6d4b\u533a\u57df\uff0c\u800c\u51b3\u7b56\u4fe1\u606f\u65e0\u6cd5\u53cd\u9988\u6307\u5bfcCP\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5c06\u51b3\u7b56\u4fe1\u606f\u53cd\u9988\u7ed9CP\u7684\u6846\u67b6\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8eCP\u7684\u540e\u9a8c\u98ce\u9669\u8ba1\u7b97\u65b9\u6cd5\uff0c\u5229\u7528\u5df2\u5b9e\u73b0\u8f68\u8ff9\u8c03\u6574\u540e\u9a8c\u5141\u8bb8\u98ce\u9669\uff0c\u7136\u540e\u5c06\u5176\u5206\u914d\u5230\u672a\u6765\u65f6\u95f4\u4ee5\u66f4\u65b0\u9884\u6d4b\u533a\u57df\u3002\u8fd8\u5f00\u53d1\u4e86\u51b3\u7b56\u805a\u7126\u7684\u8fed\u4ee3\u98ce\u9669\u5206\u914d\u7b97\u6cd5\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u5728\u7ebf\u6539\u8fdb\u8f68\u8ff9\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u9884\u6d4b\u533a\u57df\u7684\u8986\u76d6\u4fdd\u8bc1\uff0c\u786e\u4fdd\u53ef\u8bc1\u660e\u7684\u5b89\u5168\u6027\u3002\u57fa\u51c6\u5b9e\u9a8c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002", "conclusion": "Fb-CP\u6846\u67b6\u901a\u8fc7\u53cd\u9988\u673a\u5236\u5b9e\u73b0\u4e86\u9884\u6d4b\u533a\u57df\u7684\u5728\u7ebf\u8c03\u6574\u548c\u6027\u80fd\u6539\u8fdb\uff0c\u540c\u65f6\u7ef4\u6301\u8986\u76d6\u4fdd\u8bc1\uff0c\u5e76\u80fd\u5904\u7406\u5206\u5e03\u504f\u79fb\u95ee\u9898\u3002"}}
{"id": "2510.15945", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15945", "abs": "https://arxiv.org/abs/2510.15945", "authors": ["Guangya Wan", "Zixin Stephen Xu", "Sasa Zorc", "Manel Baucells", "Mengxuan Hu", "Hao Wang", "Sheng Li"], "title": "BEACON: Bayesian Optimal Stopping for Efficient LLM Sampling", "comment": "Under review on ARR", "summary": "Sampling multiple responses is a common way to improve LLM output quality,\nbut it comes at the cost of additional computation. The key challenge is\ndeciding when to stop generating new samples to balance accuracy gains against\nefficiency. To address this, we introduce BEACON (Bayesian Efficient Adaptive\nCriterion for Optimal N-stopping), a principled adaptive sampling framework\ngrounded in Sequential Search with Bayesian Learning. BEACON sequentially\ngenerates responses from the policy LLM, updates posterior belief over reward\ndistributions in real time without further training, and determines when to\nstop by weighing expected gains against computational cost. Sampling terminates\nonce the marginal utility of further exploration no longer justifies the\nexpense. We establish both theoretical optimality guarantees and practical\ntractability, and show empirically that BEACON reduces average sampling by up\nto 80% while maintaining response quality. We further demonstrate BEACON's\nutility for cost-efficient preference data generation and outline practical\nextensions, offering actionable insights for future researchers.", "AI": {"tldr": "BEACON\u662f\u4e00\u4e2a\u57fa\u4e8e\u8d1d\u53f6\u65af\u5b66\u4e60\u7684\u81ea\u9002\u5e94\u91c7\u6837\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u65f6\u66f4\u65b0\u5956\u52b1\u5206\u5e03\u7684\u540e\u9a8c\u4fe1\u5ff5\u6765\u51b3\u5b9a\u4f55\u65f6\u505c\u6b62\u751f\u6210\u65b0\u6837\u672c\uff0c\u5728\u4fdd\u6301\u54cd\u5e94\u8d28\u91cf\u7684\u540c\u65f6\u51cf\u5c1180%\u7684\u5e73\u5747\u91c7\u6837\u91cf\u3002", "motivation": "\u591a\u54cd\u5e94\u91c7\u6837\u80fd\u63d0\u9ad8LLM\u8f93\u51fa\u8d28\u91cf\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002\u5173\u952e\u6311\u6218\u662f\u5982\u4f55\u5e73\u8861\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u51b3\u5b9a\u4f55\u65f6\u505c\u6b62\u751f\u6210\u65b0\u6837\u672c\u3002", "method": "\u57fa\u4e8e\u5e8f\u5217\u641c\u7d22\u548c\u8d1d\u53f6\u65af\u5b66\u4e60\uff0cBEACON\u987a\u5e8f\u751f\u6210\u54cd\u5e94\u3001\u5b9e\u65f6\u66f4\u65b0\u5956\u52b1\u5206\u5e03\u540e\u9a8c\u4fe1\u5ff5\uff0c\u901a\u8fc7\u6743\u8861\u9884\u671f\u6536\u76ca\u4e0e\u8ba1\u7b97\u6210\u672c\u6765\u51b3\u5b9a\u505c\u6b62\u65f6\u673a\u3002", "result": "BEACON\u5728\u4fdd\u6301\u54cd\u5e94\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u5c06\u5e73\u5747\u91c7\u6837\u91cf\u51cf\u5c11\u9ad8\u8fbe80%\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u6210\u672c\u6548\u76ca\u504f\u597d\u6570\u636e\u751f\u6210\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "BEACON\u63d0\u4f9b\u4e86\u7406\u8bba\u6700\u4f18\u6027\u4fdd\u8bc1\u548c\u5b9e\u9645\u53ef\u884c\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u81ea\u9002\u5e94\u91c7\u6837\u6846\u67b6\u3002"}}
{"id": "2510.16062", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16062", "abs": "https://arxiv.org/abs/2510.16062", "authors": ["Guiyao Tie", "Zenghui Yuan", "Zeli Zhao", "Chaoran Hu", "Tianhe Gu", "Ruihang Zhang", "Sizhe Zhang", "Junran Wu", "Xiaoyue Tu", "Ming Jin", "Qingsong Wen", "Lixing Chen", "Pan Zhou", "Lichao Sun"], "title": "Can LLMs Correct Themselves? A Benchmark of Self-Correction in LLMs", "comment": "38 pages, 25 figures, 8 tables", "summary": "Self-correction of large language models (LLMs) emerges as a critical\ncomponent for enhancing their reasoning performance. Although various\nself-correction methods have been proposed, a comprehensive evaluation of these\nmethods remains largely unexplored, and the question of whether LLMs can truly\ncorrect themselves is a matter of significant interest and concern. In this\nstudy, we introduce CorrectBench, a benchmark developed to evaluate the\neffectiveness of self-correction strategies, including intrinsic, external, and\nfine-tuned approaches, across three tasks: commonsense reasoning, mathematical\nreasoning, and code generation. Our findings reveal that: 1) Self-correction\nmethods can improve accuracy, especially for complex reasoning tasks; 2) Mixing\ndifferent self-correction strategies yields further improvements, though it\nreduces efficiency; 3) Reasoning LLMs (e.g., DeepSeek-R1) have limited\noptimization under additional self-correction methods and have high time costs.\nInterestingly, a comparatively simple chain-of-thought (CoT) baseline\ndemonstrates competitive accuracy and efficiency. These results underscore the\npotential of self-correction to enhance LLM's reasoning performance while\nhighlighting the ongoing challenge of improving their efficiency. Consequently,\nwe advocate for further research focused on optimizing the balance between\nreasoning capabilities and operational efficiency. Project Page:\nhttps://correctbench.github.io/", "AI": {"tldr": "\u63d0\u51fa\u4e86CorrectBench\u57fa\u51c6\u6765\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u6821\u6b63\u80fd\u529b\uff0c\u53d1\u73b0\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u81ea\u6821\u6b63\u65b9\u6cd5\u80fd\u63d0\u5347\u51c6\u786e\u6027\uff0c\u4f46\u6548\u7387\u8f83\u4f4e\uff0c\u7b80\u5355\u7684\u601d\u7ef4\u94fe\u57fa\u7ebf\u8868\u73b0\u7ade\u4e89\u529b\u5f3a\u3002", "motivation": "\u867d\u7136\u5df2\u6709\u591a\u79cd\u81ea\u6821\u6b63\u65b9\u6cd5\u88ab\u63d0\u51fa\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u8fd9\u4e9b\u65b9\u6cd5\u7684\u5168\u9762\u8bc4\u4f30\uff0c\u4e14LLM\u662f\u5426\u80fd\u771f\u6b63\u81ea\u6211\u6821\u6b63\u4ecd\u662f\u4e00\u4e2a\u91cd\u8981\u95ee\u9898\u3002", "method": "\u5f00\u53d1CorrectBench\u57fa\u51c6\uff0c\u8bc4\u4f30\u5185\u5728\u3001\u5916\u90e8\u548c\u5fae\u8c03\u4e09\u79cd\u81ea\u6821\u6b63\u7b56\u7565\u5728\u5e38\u8bc6\u63a8\u7406\u3001\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u4e09\u4e2a\u4efb\u52a1\u4e0a\u7684\u6548\u679c\u3002", "result": "\u81ea\u6821\u6b63\u65b9\u6cd5\u80fd\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u590d\u6742\u63a8\u7406\u4efb\u52a1\uff1b\u6df7\u5408\u4e0d\u540c\u7b56\u7565\u53ef\u8fdb\u4e00\u6b65\u6539\u5584\u4f46\u964d\u4f4e\u6548\u7387\uff1b\u63a8\u7406LLM\u5728\u989d\u5916\u81ea\u6821\u6b63\u4e0b\u4f18\u5316\u6709\u9650\u4e14\u65f6\u95f4\u6210\u672c\u9ad8\uff1b\u7b80\u5355\u601d\u7ef4\u94fe\u57fa\u7ebf\u8868\u73b0\u7ade\u4e89\u529b\u5f3a\u3002", "conclusion": "\u81ea\u6821\u6b63\u6709\u6f5c\u529b\u63d0\u5347LLM\u63a8\u7406\u6027\u80fd\uff0c\u4f46\u9700\u4f18\u5316\u63a8\u7406\u80fd\u529b\u4e0e\u64cd\u4f5c\u6548\u7387\u4e4b\u95f4\u7684\u5e73\u8861\u3002"}}
{"id": "2510.16280", "categories": ["eess.SY", "cs.SY", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.16280", "abs": "https://arxiv.org/abs/2510.16280", "authors": ["Hui Yang", "Faisal Aqlan", "Richard Zhao"], "title": "Towards Smart Manufacturing Metaverse via Digital Twinning in Extended Reality", "comment": null, "summary": "The rapid evolution of modern manufacturing systems is driven by the\nintegration of emerging metaverse technologies such as artificial intelligence\n(AI), digital twin (DT) with different forms of extended reality (XR) like\nvirtual reality (VR), augmented reality (AR), and mixed reality (MR). These\nadvances confront manufacturing workers with complex and evolving environments\nthat demand digital literacy for problem solving in the future workplace.\nHowever, manufacturing industry faces a critical shortage of skilled workforce\nwith digital literacy in the world. Further, global pandemic has significantly\nchanged how people work and collaborate digitally and remotely. There is an\nurgent need to rethink digital platformization and leverage emerging\ntechnologies to propel industrial evolution toward human-centered manufacturing\nmetaverse (MfgVerse). This paper presents a forward-looking perspective on the\ndevelopment of smart MfgVerse, highlighting current efforts in learning\nfactory, cognitive digital twinning, and the new sharing economy of\nmanufacturing-as-a-service (MaaS). MfgVerse is converging into multiplex\nnetworks, including a social network of human stakeholders, an interconnected\nnetwork of manufacturing things or agents (e.g., machines, robots, facilities,\nmaterial handling systems), a network of digital twins of physical things, as\nwell as auxiliary networks of sales, supply chain, logistics, and\nremanufacturing systems. We also showcase the design and development of a\nlearning factory for workforce training in extended reality. Finally, future\ndirections, challenges, and opportunities are discussed for human-centered\nmanufacturing metaverse. We hope this work helps stimulate more comprehensive\nstudies and in-depth research efforts to advance MfgVerse technologies.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5236\u9020\u4e1a\u5143\u5b87\u5b99(MfgVerse)\u7684\u53d1\u5c55\uff0c\u91cd\u70b9\u5173\u6ce8\u4eba\u5de5\u667a\u80fd\u3001\u6570\u5b57\u5b6a\u751f\u548c\u6269\u5c55\u73b0\u5b9e\u7b49\u65b0\u5174\u6280\u672f\u5728\u5236\u9020\u4e1a\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u53ca\u5982\u4f55\u901a\u8fc7\u5b66\u4e60\u5de5\u5382\u548c\u8ba4\u77e5\u6570\u5b57\u5b6a\u751f\u63a8\u52a8\u4ee5\u4eba\u4e3a\u672c\u7684\u5236\u9020\u4e1a\u8f6c\u578b\u3002", "motivation": "\u5236\u9020\u4e1a\u9762\u4e34\u6570\u5b57\u5316\u4eba\u624d\u77ed\u7f3a\u95ee\u9898\uff0c\u5168\u7403\u75ab\u60c5\u6539\u53d8\u4e86\u5de5\u4f5c\u65b9\u5f0f\uff0c\u8feb\u5207\u9700\u8981\u91cd\u65b0\u601d\u8003\u6570\u5b57\u5316\u5e73\u53f0\u5316\uff0c\u5229\u7528\u65b0\u5174\u6280\u672f\u63a8\u52a8\u5de5\u4e1a\u5411\u4ee5\u4eba\u4e3a\u672c\u7684\u5236\u9020\u4e1a\u5143\u5b87\u5b99\u6f14\u8fdb\u3002", "method": "\u63d0\u51fa\u5236\u9020\u4e1a\u5143\u5b87\u5b99\u6982\u5ff5\uff0c\u6574\u5408\u793e\u4f1a\u7f51\u7edc\u3001\u5236\u9020\u8bbe\u5907\u7f51\u7edc\u3001\u6570\u5b57\u5b6a\u751f\u7f51\u7edc\u548c\u8f85\u52a9\u7f51\u7edc\uff0c\u5f00\u53d1\u6269\u5c55\u73b0\u5b9e\u5b66\u4e60\u5de5\u5382\u8fdb\u884c\u52b3\u52a8\u529b\u57f9\u8bad\u3002", "result": "\u6784\u5efa\u4e86\u5236\u9020\u4e1a\u5143\u5b87\u5b99\u7684\u591a\u91cd\u7f51\u7edc\u67b6\u6784\uff0c\u5c55\u793a\u4e86\u6269\u5c55\u73b0\u5b9e\u5b66\u4e60\u5de5\u5382\u7684\u8bbe\u8ba1\u5f00\u53d1\uff0c\u4e3a\u5236\u9020\u4e1a\u6570\u5b57\u5316\u8f6c\u578b\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "conclusion": "\u5236\u9020\u4e1a\u5143\u5b87\u5b99\u662f\u672a\u6765\u5236\u9020\u4e1a\u53d1\u5c55\u7684\u5173\u952e\u65b9\u5411\uff0c\u9700\u8981\u66f4\u591a\u6df1\u5165\u7814\u7a76\u548c\u7efc\u5408\u7814\u7a76\u6765\u63a8\u8fdb\u76f8\u5173\u6280\u672f\u53d1\u5c55\uff0c\u89e3\u51b3\u9762\u4e34\u7684\u6311\u6218\u548c\u673a\u9047\u3002"}}
{"id": "2510.15915", "categories": ["q-fin.ST"], "pdf": "https://arxiv.org/pdf/2510.15915", "abs": "https://arxiv.org/abs/2510.15915", "authors": ["Tamoghna Mukherjee"], "title": "Investor Sentiment and Market Movements: A Granger Causality Perspective", "comment": "4 pages", "summary": "The stock market is heavily influenced by investor sentiment, which can drive\nbuying or selling behavior. Sentiment analysis helps in gauging the overall\nsentiment of market participants towards a particular stock or the market as a\nwhole. Positive sentiment often leads to increased buying activity and vice\nversa. Granger causality can be applied to ascertain whether changes in\nsentiment precede changes in stock prices.The study is focused on this aspect\nand tries to understand the relationship between close price index and\nsentiment score with the help of Granger causality inference. The study finds a\npositive response through hypothesis testing.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u683c\u5170\u6770\u56e0\u679c\u68c0\u9a8c\u5206\u6790\u6295\u8d44\u8005\u60c5\u7eea\u4e0e\u80a1\u7968\u4ef7\u683c\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u53d1\u73b0\u60c5\u7eea\u53d8\u5316\u5bf9\u80a1\u4ef7\u5177\u6709\u9884\u6d4b\u4f5c\u7528", "motivation": "\u6295\u8d44\u8005\u60c5\u7eea\u5bf9\u80a1\u5e02\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u4f46\u60c5\u7eea\u53d8\u5316\u662f\u5426\u771f\u6b63\u9886\u5148\u4e8e\u80a1\u4ef7\u53d8\u5316\u9700\u8981\u5b9e\u8bc1\u9a8c\u8bc1", "method": "\u4f7f\u7528\u683c\u5170\u6770\u56e0\u679c\u68c0\u9a8c\u6765\u5206\u6790\u6536\u76d8\u4ef7\u6307\u6570\u4e0e\u60c5\u7eea\u5f97\u5206\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb", "result": "\u5047\u8bbe\u68c0\u9a8c\u663e\u793a\u5b58\u5728\u6b63\u5411\u54cd\u5e94\uff0c\u5373\u60c5\u7eea\u53d8\u5316\u786e\u5b9e\u5148\u4e8e\u80a1\u4ef7\u53d8\u5316", "conclusion": "\u6295\u8d44\u8005\u60c5\u7eea\u53ef\u4ee5\u4f5c\u4e3a\u9884\u6d4b\u80a1\u7968\u4ef7\u683c\u53d8\u5316\u7684\u6709\u7528\u6307\u6807"}}
{"id": "2510.16526", "categories": ["q-fin.RM"], "pdf": "https://arxiv.org/pdf/2510.16526", "abs": "https://arxiv.org/abs/2510.16526", "authors": ["Federico Gatta", "Fabrizio Lillo", "Piero Mazzarisi"], "title": "A high-frequency approach to Realized Risk Measures", "comment": null, "summary": "We propose a new approach, termed Realized Risk Measures (RRM), to estimate\nValue-at-Risk (VaR) and Expected Shortfall (ES) using high-frequency financial\ndata. It extends the Realized Quantile (RQ) approach proposed by Dimitriadis\nand Halbleib by lifting the assumption of return self-similarity, which\ndisplays some limitations in describing empirical data. More specifically, as\nthe RQ, the RRM method transforms intra-day returns in intrinsic time using a\nsubordinator process, in order to capture the inhomogeneity of trading activity\nand/or volatility clustering. Then, microstructural effects resulting in\nnon-zero autocorrelation are filtered out using a suitable moving average\nprocess. Finally, a fat-tailed distribution is fitted on the cleaned intra-day\nreturns. The return distribution at low frequency (daily) is then extrapolated\nvia either a characteristic function approach or Monte Carlo simulations. VaR\nand ES are estimated as the quantile and the tail mean of the distribution,\nrespectively. The proposed approach is benchmarked against the RQ through\nseveral experiments. Extensive numerical simulations and an empirical study on\n18 US stocks show the outperformance of our method, both in terms of the\nin-sample estimated risk measures and in the out-of-sample risk forecasting", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684Realized Risk Measures (RRM)\u65b9\u6cd5\uff0c\u4f7f\u7528\u9ad8\u9891\u91d1\u878d\u6570\u636e\u4f30\u8ba1VaR\u548cES\uff0c\u6539\u8fdb\u4e86\u73b0\u6709\u7684Realized Quantile (RQ)\u65b9\u6cd5\uff0c\u6d88\u9664\u4e86\u6536\u76ca\u7387\u81ea\u76f8\u4f3c\u6027\u5047\u8bbe\u7684\u9650\u5236\u3002", "motivation": "\u73b0\u6709\u7684Realized Quantile (RQ)\u65b9\u6cd5\u5047\u8bbe\u6536\u76ca\u7387\u5177\u6709\u81ea\u76f8\u4f3c\u6027\uff0c\u8fd9\u5728\u63cf\u8ff0\u7ecf\u9a8c\u6570\u636e\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u7075\u6d3b\u7684\u98ce\u9669\u5ea6\u91cf\u65b9\u6cd5\u3002", "method": "RRM\u65b9\u6cd5\u901a\u8fc7\u5b50\u8fc7\u7a0b\u5c06\u65e5\u5185\u6536\u76ca\u7387\u8f6c\u6362\u4e3a\u5185\u5728\u65f6\u95f4\uff0c\u6355\u6349\u4ea4\u6613\u6d3b\u52a8\u548c\u6ce2\u52a8\u7387\u805a\u96c6\u7684\u4e0d\u5747\u5300\u6027\uff1b\u4f7f\u7528\u79fb\u52a8\u5e73\u5747\u8fc7\u7a0b\u8fc7\u6ee4\u5fae\u89c2\u7ed3\u6784\u6548\u5e94\uff1b\u62df\u5408\u539a\u5c3e\u5206\u5e03\uff1b\u901a\u8fc7\u7279\u5f81\u51fd\u6570\u65b9\u6cd5\u6216\u8499\u7279\u5361\u6d1b\u6a21\u62df\u5916\u63a8\u4f4e\u9891\u6536\u76ca\u7387\u5206\u5e03\u3002", "result": "\u901a\u8fc7\u5bf918\u53ea\u7f8e\u56fd\u80a1\u7968\u7684\u5e7f\u6cdb\u6570\u503c\u6a21\u62df\u548c\u5b9e\u8bc1\u7814\u7a76\uff0c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u6837\u672c\u5185\u98ce\u9669\u5ea6\u91cf\u4f30\u8ba1\u548c\u6837\u672c\u5916\u98ce\u9669\u9884\u6d4b\u65b9\u9762\u5747\u4f18\u4e8eRQ\u65b9\u6cd5\u3002", "conclusion": "RRM\u65b9\u6cd5\u5728\u98ce\u9669\u5ea6\u91cf\u4f30\u8ba1\u548c\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u9ad8\u9891\u91d1\u878d\u6570\u636e\u7684\u98ce\u9669\u7ba1\u7406\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u5de5\u5177\u3002"}}
{"id": "2510.15995", "categories": ["q-fin.TR", "cs.GT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15995", "abs": "https://arxiv.org/abs/2510.15995", "authors": ["Luigi Foscari", "Emanuele Guidotti", "Nicol\u00f2 Cesa-Bianchi", "Tatjana Chavdarova", "Alfio Ferrara"], "title": "The Invisible Handshake: Tacit Collusion between Adaptive Market Agents", "comment": null, "summary": "We study the emergence of tacit collusion between adaptive trading agents in\na stochastic market with endogenous price formation. Using a two-player\nrepeated game between a market maker and a market taker, we characterize\nfeasible and collusive strategy profiles that raise prices beyond competitive\nlevels. We show that, when agents follow simple learning algorithms (e.g.,\ngradient ascent) to maximize their own wealth, the resulting dynamics converge\nto collusive strategy profiles, even in highly liquid markets with small trade\nsizes. By highlighting how simple learning strategies naturally lead to tacit\ncollusion, our results offer new insights into the dynamics of AI-driven\nmarkets.", "AI": {"tldr": "\u7814\u7a76\u81ea\u9002\u5e94\u4ea4\u6613\u4ee3\u7406\u5728\u968f\u673a\u5e02\u573a\u4e2d\u5982\u4f55\u901a\u8fc7\u7b80\u5355\u5b66\u4e60\u7b97\u6cd5\u81ea\u7136\u5f62\u6210\u9690\u6027\u5408\u8c0b\uff0c\u5bfc\u81f4\u4ef7\u683c\u9ad8\u4e8e\u7ade\u4e89\u6c34\u5e73\u3002", "motivation": "\u63a2\u8ba8AI\u9a71\u52a8\u5e02\u573a\u4e2d\uff0c\u5f53\u4ea4\u6613\u4ee3\u7406\u4f7f\u7528\u7b80\u5355\u5b66\u4e60\u7b56\u7565\u65f6\uff0c\u662f\u5426\u4f1a\u81ea\u53d1\u5f62\u6210\u9690\u6027\u5408\u8c0b\uff0c\u4ece\u800c\u5f71\u54cd\u5e02\u573a\u4ef7\u683c\u5f62\u6210\u673a\u5236\u3002", "method": "\u4f7f\u7528\u53cc\u73a9\u5bb6\u91cd\u590d\u535a\u5f08\u6a21\u578b\uff08\u505a\u5e02\u5546\u548c\u5e02\u4ef7\u63a5\u53d7\u8005\uff09\uff0c\u5206\u6790\u4ee3\u7406\u901a\u8fc7\u68af\u5ea6\u4e0a\u5347\u7b49\u7b80\u5355\u5b66\u4e60\u7b97\u6cd5\u6700\u5927\u5316\u81ea\u8eab\u8d22\u5bcc\u65f6\u7684\u7b56\u7565\u52a8\u6001\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5373\u4f7f\u5728\u9ad8\u6d41\u52a8\u6027\u5e02\u573a\u548c\u5c0f\u989d\u4ea4\u6613\u89c4\u6a21\u4e0b\uff0c\u4ee3\u7406\u7684\u5b66\u4e60\u52a8\u6001\u4e5f\u4f1a\u6536\u655b\u5230\u5408\u8c0b\u7b56\u7565\u914d\u7f6e\uff0c\u5bfc\u81f4\u4ef7\u683c\u8d85\u51fa\u7ade\u4e89\u6c34\u5e73\u3002", "conclusion": "\u7b80\u5355\u7684\u5b66\u4e60\u7b56\u7565\u4f1a\u81ea\u7136\u5bfc\u81f4\u9690\u6027\u5408\u8c0b\uff0c\u8fd9\u4e3a\u7406\u89e3AI\u9a71\u52a8\u5e02\u573a\u7684\u52a8\u6001\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u63ed\u793a\u4e86\u7b97\u6cd5\u4ea4\u6613\u53ef\u80fd\u5e26\u6765\u7684\u5e02\u573a\u6548\u7387\u95ee\u9898\u3002"}}
{"id": "2510.17070", "categories": ["econ.EM"], "pdf": "https://arxiv.org/pdf/2510.17070", "abs": "https://arxiv.org/abs/2510.17070", "authors": ["Jean-Marie Dufour", "Purevdorj Tuvaandorj"], "title": "Mixed LR-$C(\u03b1)$-type tests for irregular hypotheses, general criterion functions and misspecified models", "comment": "28 pages, 1 figure, 4 tables", "summary": "This paper introduces a likelihood ratio (LR)-type test that possesses the\nrobustness properties of \\(C(\\alpha)\\)-type procedures in an extremum\nestimation setting.\n  The test statistic is constructed by applying separate adjustments to the\nrestricted and unrestricted criterion functions, and is shown to be\nasymptotically pivotal under minimal conditions. It features two main\nrobustness properties. First, unlike standard LR-type statistics, its null\nasymptotic distribution remains chi-square even under model misspecification,\nwhere the information matrix equality fails. Second, it accommodates irregular\nhypotheses involving constrained parameter spaces, such as boundary parameters,\nrelying solely on root-\\(n\\)-consistent estimators for nuisance parameters.\nWhen the model is correctly specified, no boundary constraints are present, and\nparameters are estimated by extremum estimators, the proposed test reduces to\nthe standard LR-type statistic.\n  Simulations with ARCH models, where volatility parameters are constrained to\nbe nonnegative, and parametric survival regressions with potentially monotone\nincreasing hazard functions, demonstrate that our test maintains accurate size\nand exhibits good power. An empirical application to a two-way error components\nmodel shows that the proposed test can provide more informative inference than\nthe conventional \\(t\\)-test.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5177\u6709\u7a33\u5065\u6027\u7684\u4f3c\u7136\u6bd4\u578b\u68c0\u9a8c\uff0c\u5728\u6781\u503c\u4f30\u8ba1\u6846\u67b6\u4e0b\u5177\u5907C(\u03b1)\u578b\u7a0b\u5e8f\u7684\u7a33\u5065\u7279\u6027\uff0c\u9002\u7528\u4e8e\u6a21\u578b\u8bef\u8bbe\u548c\u7ea6\u675f\u53c2\u6570\u7a7a\u95f4\u7684\u60c5\u51b5\u3002", "motivation": "\u6807\u51c6LR\u68c0\u9a8c\u5728\u6a21\u578b\u8bef\u8bbe\uff08\u4fe1\u606f\u77e9\u9635\u7b49\u5f0f\u4e0d\u6210\u7acb\uff09\u6216\u53c2\u6570\u7a7a\u95f4\u5b58\u5728\u7ea6\u675f\uff08\u5982\u8fb9\u754c\u53c2\u6570\uff09\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u7a33\u5065\u7684\u68c0\u9a8c\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5bf9\u53d7\u9650\u548c\u4e0d\u53d7\u9650\u51c6\u5219\u51fd\u6570\u5206\u522b\u8fdb\u884c\u8c03\u6574\u6765\u6784\u5efa\u68c0\u9a8c\u7edf\u8ba1\u91cf\uff0c\u4ec5\u9700\u4f7f\u7528\u6839n\u4e00\u81f4\u4f30\u8ba1\u91cf\u6765\u5904\u7406\u5197\u4f59\u53c2\u6570\uff0c\u9002\u7528\u4e8e\u4e0d\u89c4\u5219\u5047\u8bbe\u3002", "result": "\u8be5\u68c0\u9a8c\u5728\u6a21\u578b\u8bef\u8bbe\u4e0b\u4ecd\u4fdd\u6301\u5361\u65b9\u5206\u5e03\uff0c\u5728ARCH\u6a21\u578b\u548c\u751f\u5b58\u56de\u5f52\u6a21\u62df\u4e2d\u8868\u73b0\u51fa\u51c6\u786e\u7684\u5927\u5c0f\u548c\u826f\u597d\u7684\u529f\u6548\uff0c\u5b9e\u8bc1\u5e94\u7528\u663e\u793a\u6bd4\u4f20\u7edft\u68c0\u9a8c\u63d0\u4f9b\u66f4\u591a\u4fe1\u606f\u3002", "conclusion": "\u63d0\u51fa\u7684LR\u578b\u68c0\u9a8c\u5728\u4fdd\u6301\u6807\u51c6LR\u7edf\u8ba1\u91cf\u4f18\u70b9\u7684\u540c\u65f6\uff0c\u589e\u5f3a\u4e86\u5728\u6a21\u578b\u8bef\u8bbe\u548c\u7ea6\u675f\u53c2\u6570\u7a7a\u95f4\u60c5\u51b5\u4e0b\u7684\u7a33\u5065\u6027\uff0c\u4e3a\u590d\u6742\u6a21\u578b\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u63a8\u65ad\u5de5\u5177\u3002"}}
{"id": "2510.16509", "categories": ["stat.ML", "math.DS"], "pdf": "https://arxiv.org/pdf/2510.16509", "abs": "https://arxiv.org/abs/2510.16509", "authors": ["Ziad Ghanem", "Chang Hyunwoong", "Preskella Mrad"], "title": "A Bayesian Framework for Symmetry Inference in Chaotic Attractors", "comment": null, "summary": "Detecting symmetry from data is a fundamental problem in signal analysis,\nproviding insight into underlying structure and constraints. When data emerge\nas trajectories of dynamical systems, symmetries encode structural properties\nof the dynamics that enable model reduction, principled comparison across\nconditions, and detection of regime changes. While recent optimal transport\nmethods provide practical tools for data-driven symmetry detection in this\nsetting, they rely on deterministic thresholds and lack uncertainty\nquantification, limiting robustness to noise and ability to resolve\nhierarchical symmetry structures. We present a Bayesian framework that\nformulates symmetry detection as probabilistic model selection over a lattice\nof candidate subgroups, using a Gibbs posterior constructed from Wasserstein\ndistances between observed data and group-transformed copies. We establish\nthree theoretical guarantees: $(i)$ a Bayesian Occam's razor favoring minimal\nsymmetry consistent with data, $(ii)$ conjugation equivariance ensuring\nframe-independence, and $(iii)$ stability bounds under perturbations for\nrobustness to noise. Posterior inference is performed via Metropolis-Hastings\nsampling and numerical experiments on equivariant dynamical systems and\nsynthetic point clouds demonstrate accurate symmetry recovery under high noise\nand small sample sizes. An application to human gait dynamics reveals symmetry\nchanges induced by mechanical constraints, demonstrating the framework's\nutility for statistical inference in biomechanical and dynamical systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8d1d\u53f6\u65af\u6846\u67b6\u7528\u4e8e\u4ece\u52a8\u6001\u7cfb\u7edf\u6570\u636e\u4e2d\u68c0\u6d4b\u5bf9\u79f0\u6027\uff0c\u901a\u8fc7\u5409\u5e03\u65af\u540e\u9a8c\u548cWasserstein\u8ddd\u79bb\u8fdb\u884c\u6982\u7387\u6a21\u578b\u9009\u62e9\uff0c\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u5e76\u5728\u9ad8\u566a\u58f0\u548c\u5c0f\u6837\u672c\u60c5\u51b5\u4e0b\u8868\u73b0\u826f\u597d\u3002", "motivation": "\u73b0\u6709\u6700\u4f18\u4f20\u8f93\u65b9\u6cd5\u4f9d\u8d56\u786e\u5b9a\u6027\u9608\u503c\u4e14\u7f3a\u4e4f\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u9650\u5236\u4e86\u5728\u566a\u58f0\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u5bf9\u5c42\u6b21\u5bf9\u79f0\u7ed3\u6784\u7684\u89e3\u6790\u80fd\u529b\u3002", "method": "\u5c06\u5bf9\u79f0\u6027\u68c0\u6d4b\u8868\u8ff0\u4e3a\u5728\u5019\u9009\u5b50\u7fa4\u683c\u4e0a\u7684\u6982\u7387\u6a21\u578b\u9009\u62e9\uff0c\u4f7f\u7528\u57fa\u4e8eWasserstein\u8ddd\u79bb\u7684\u5409\u5e03\u65af\u540e\u9a8c\uff0c\u901a\u8fc7Metropolis-Hastings\u91c7\u6837\u8fdb\u884c\u540e\u9a8c\u63a8\u65ad\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\u5728\u7b49\u53d8\u52a8\u6001\u7cfb\u7edf\u548c\u5408\u6210\u70b9\u4e91\u4e2d\uff0c\u5373\u4f7f\u5728\u9ad8\u566a\u58f0\u548c\u5c0f\u6837\u672c\u60c5\u51b5\u4e0b\u4e5f\u80fd\u51c6\u786e\u6062\u590d\u5bf9\u79f0\u6027\u3002\u5728\u4eba\u7c7b\u6b65\u6001\u52a8\u529b\u5b66\u5e94\u7528\u4e2d\u63ed\u793a\u4e86\u673a\u68b0\u7ea6\u675f\u5f15\u8d77\u7684\u5bf9\u79f0\u6027\u53d8\u5316\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u751f\u7269\u529b\u5b66\u548c\u52a8\u6001\u7cfb\u7edf\u4e2d\u7684\u7edf\u8ba1\u63a8\u65ad\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u80fd\u591f\u7a33\u5065\u5730\u68c0\u6d4b\u5bf9\u79f0\u6027\u5e76\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u3002"}}
{"id": "2510.15956", "categories": ["q-fin.GN", "q-fin.PM"], "pdf": "https://arxiv.org/pdf/2510.15956", "abs": "https://arxiv.org/abs/2510.15956", "authors": ["Qionghua Chu"], "title": "ESG Signaling on Wall Street in the AI Era", "comment": null, "summary": "I identify a new signaling channel in ESG research by empirically examining\nwhether environmental, social, and governance (ESG) investing remains valuable\nas large institutional investors increasingly shift toward artificial\nintelligence (AI). Using winsorized ESG scores of S&P 500 firms from Yahoo\nFinance and controlling for market value of equity, I conduct cross-sectional\nregressions to test the signaling mechanism. I demonstrate that Environmental,\nSocial, Governance, and composite ESG scores strongly and positively signal\nhigher debt-to-total-capital ratio, both individually and in various\ncombinations. My findings contribute to the growing literature on ESG\ninvesting, offering economically meaningful signaling channel with implications\nfor long-term portfolio management amid the rise of AI.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0ESG\u6295\u8d44\u5728AI\u65f6\u4ee3\u4ecd\u5177\u4ef7\u503c\uff0cESG\u8bc4\u5206\u4e0e\u503a\u52a1\u8d44\u672c\u6bd4\u7387\u5448\u663e\u8457\u6b63\u76f8\u5173\uff0c\u4e3a\u957f\u671f\u6295\u8d44\u7ec4\u5408\u7ba1\u7406\u63d0\u4f9b\u91cd\u8981\u4fe1\u53f7\u3002", "motivation": "\u968f\u7740\u5927\u578b\u673a\u6784\u6295\u8d44\u8005\u8f6c\u5411AI\u6295\u8d44\uff0c\u9700\u8981\u9a8c\u8bc1ESG\u6295\u8d44\u662f\u5426\u4ecd\u5177\u6709\u4fe1\u53f7\u4ef7\u503c\uff0c\u63a2\u7d22ESG\u5728AI\u5d1b\u8d77\u80cc\u666f\u4e0b\u7684\u6295\u8d44\u610f\u4e49\u3002", "method": "\u4f7f\u7528\u96c5\u864e\u8d22\u7ecf\u7684S&P 500\u516c\u53f8ESG\u8bc4\u5206\u6570\u636e\uff0c\u63a7\u5236\u80a1\u6743\u5e02\u573a\u4ef7\u503c\uff0c\u8fdb\u884c\u6a2a\u622a\u9762\u56de\u5f52\u5206\u6790\u6d4b\u8bd5\u4fe1\u53f7\u673a\u5236\u3002", "result": "\u73af\u5883\u3001\u793e\u4f1a\u548c\u6cbb\u7406\u8bc4\u5206\u4ee5\u53ca\u7efc\u5408ESG\u8bc4\u5206\u5747\u4e0e\u66f4\u9ad8\u7684\u503a\u52a1\u8d44\u672c\u6bd4\u7387\u5448\u663e\u8457\u6b63\u76f8\u5173\uff0c\u5355\u72ec\u548c\u7ec4\u5408\u5206\u6790\u7ed3\u679c\u4e00\u81f4\u3002", "conclusion": "ESG\u6295\u8d44\u5728AI\u65f6\u4ee3\u4ecd\u63d0\u4f9b\u7ecf\u6d4e\u610f\u4e49\u663e\u8457\u7684\u4fe1\u53f7\u6e20\u9053\uff0c\u5bf9\u957f\u671f\u6295\u8d44\u7ec4\u5408\u7ba1\u7406\u5177\u6709\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2510.16019", "categories": ["cs.CY", "I.2"], "pdf": "https://arxiv.org/pdf/2510.16019", "abs": "https://arxiv.org/abs/2510.16019", "authors": ["M\u00e1rton Benedek", "Bal\u00e1zs R. Sziklai"], "title": "Impact of AI Tools on Learning Outcomes: Decreasing Knowledge and Over-Reliance", "comment": null, "summary": "Students at all levels of education are increasingly relying on generative\nartificial intelligence (AI) tools to complete assignments and achieve higher\nexam scores. However, it remains unclear how this reliance affects their\nmotivation, their genuine understanding of the material, and the extent to\nwhich it substitutes for the process of knowledge acquisition. To investigate\nthe impact of generative AI on learning outcomes, an experiment was conducted\nat Corvinus University of Budapest. In an operations research class, students\nwere randomly assigned into two groups: one was permitted to use AI tools\nduring classes and examinations, while the other was not. To ensure fairness, a\ncompensation mechanism was introduced: students in the lower-performing group\nreceived point adjustments until the average performance of the two groups was\nequalized. Despite the organizers' best efforts to explain the design and to\ncreate equal opportunities for all participants, many students perceived the\nexperiment as a major disruption. Although the experiment was approved by every\nrelevant university authority -- including the Ethics Board, the Head of\nDepartment, the Program Director, and the Student Council -- students escalated\ntheir concerns to the media and eventually to the State Secretary for Higher\nEducation of Hungary. As a result, the experiment had to be substantially\nrevised before completion: on the final exam the test group was merged with the\ncontrol group. Still, the data allowed us to draw decisive conclusions\nregarding the students' learning habits. Uncontrolled use of AI tools leads to\ndisengaged students and low understanding of material. The extreme reactions of\nthe students proved even more revealing than the data collected: generative AI\ntools have already become indispensable for students, raising fundamental\nquestions about the validity of their learning process.", "AI": {"tldr": "\u7814\u7a76\u8c03\u67e5\u751f\u6210\u5f0fAI\u5de5\u5177\u5bf9\u5b66\u751f\u5b66\u4e60\u6210\u679c\u7684\u5f71\u54cd\uff0c\u53d1\u73b0AI\u4f7f\u7528\u5bfc\u81f4\u5b66\u751f\u53c2\u4e0e\u5ea6\u964d\u4f4e\u3001\u7406\u89e3\u529b\u4e0b\u964d\uff0c\u4e14\u5b66\u751f\u5df2\u5bf9AI\u4ea7\u751f\u4f9d\u8d56\uff0c\u5f15\u53d1\u5bf9\u5b66\u4e60\u8fc7\u7a0b\u6709\u6548\u6027\u7684\u8d28\u7591\u3002", "motivation": "\u4e86\u89e3\u5b66\u751f\u4f9d\u8d56\u751f\u6210\u5f0fAI\u5de5\u5177\u5b8c\u6210\u4f5c\u4e1a\u548c\u8003\u8bd5\u5bf9\u5176\u5b66\u4e60\u52a8\u673a\u3001\u77e5\u8bc6\u7406\u89e3\u548c\u77e5\u8bc6\u83b7\u53d6\u8fc7\u7a0b\u7684\u5f71\u54cd\u3002", "method": "\u5728\u5e03\u8fbe\u4f69\u65af\u79d1\u7ef4\u52aa\u65af\u5927\u5b66\u7684\u8fd0\u7b79\u5b66\u8bfe\u7a0b\u4e2d\u8fdb\u884c\u968f\u673a\u5bf9\u7167\u5b9e\u9a8c\uff0c\u5c06\u5b66\u751f\u5206\u4e3a\u4f7f\u7528AI\u7ec4\u548c\u5bf9\u7167\u7ec4\uff0c\u5e76\u5f15\u5165\u8865\u507f\u673a\u5236\u5e73\u8861\u4e24\u7ec4\u5e73\u5747\u8868\u73b0\u3002", "result": "AI\u4f7f\u7528\u5bfc\u81f4\u5b66\u751f\u53c2\u4e0e\u5ea6\u964d\u4f4e\u3001\u7406\u89e3\u529b\u4e0b\u964d\uff1b\u5b66\u751f\u5f3a\u70c8\u53cd\u5bf9\u5b9e\u9a8c\u8bbe\u8ba1\uff0c\u8bc1\u660eAI\u5df2\u6210\u4e3a\u5b66\u751f\u4e0d\u53ef\u6216\u7f3a\u7684\u5de5\u5177\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u5de5\u5177\u7684\u65e0\u63a7\u5236\u4f7f\u7528\u4f1a\u635f\u5bb3\u5b66\u4e60\u6548\u679c\uff0c\u5b66\u751f\u5df2\u5bf9AI\u4ea7\u751f\u4e25\u91cd\u4f9d\u8d56\uff0c\u8fd9\u5f15\u53d1\u4e86\u5bf9\u5f53\u524d\u5b66\u4e60\u8fc7\u7a0b\u6709\u6548\u6027\u7684\u6839\u672c\u8d28\u7591\u3002"}}
{"id": "2510.15966", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15966", "abs": "https://arxiv.org/abs/2510.15966", "authors": ["Shian Jia", "Ziyang Huang", "Xinbo Wang", "Haofei Zhang", "Mingli Song"], "title": "PISA: A Pragmatic Psych-Inspired Unified Memory System for Enhanced AI Agency", "comment": null, "summary": "Memory systems are fundamental to AI agents, yet existing work often lacks\nadaptability to diverse tasks and overlooks the constructive and task-oriented\nrole of AI agent memory. Drawing from Piaget's theory of cognitive development,\nwe propose PISA, a pragmatic, psych-inspired unified memory system that\naddresses these limitations by treating memory as a constructive and adaptive\nprocess. To enable continuous learning and adaptability, PISA introduces a\ntrimodal adaptation mechanism (i.e., schema updation, schema evolution, and\nschema creation) that preserves coherent organization while supporting flexible\nmemory updates. Building on these schema-grounded structures, we further design\na hybrid memory access architecture that seamlessly integrates symbolic\nreasoning with neural retrieval, significantly improving retrieval accuracy and\nefficiency. Our empirical evaluation, conducted on the existing LOCOMO\nbenchmark and our newly proposed AggQA benchmark for data analysis tasks,\nconfirms that PISA sets a new state-of-the-art by significantly enhancing\nadaptability and long-term knowledge retention.", "AI": {"tldr": "\u63d0\u51fa\u4e86PISA\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u57fa\u4e8e\u76ae\u4e9a\u6770\u8ba4\u77e5\u53d1\u5c55\u7406\u8bba\uff0c\u901a\u8fc7\u4e09\u6a21\u6001\u9002\u5e94\u673a\u5236\u548c\u6df7\u5408\u8bb0\u5fc6\u8bbf\u95ee\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86AI\u4ee3\u7406\u7684\u9002\u5e94\u6027\u548c\u957f\u671f\u77e5\u8bc6\u4fdd\u7559\u80fd\u529b\u3002", "motivation": "\u73b0\u6709AI\u4ee3\u7406\u8bb0\u5fc6\u7cfb\u7edf\u7f3a\u4e4f\u5bf9\u591a\u6837\u5316\u4efb\u52a1\u7684\u9002\u5e94\u6027\uff0c\u5ffd\u89c6\u4e86\u8bb0\u5fc6\u7684\u5efa\u6784\u6027\u548c\u4efb\u52a1\u5bfc\u5411\u4f5c\u7528\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u548c\u5b9e\u7528\u7684\u8bb0\u5fc6\u673a\u5236\u3002", "method": "\u91c7\u7528\u4e09\u6a21\u6001\u9002\u5e94\u673a\u5236\uff08\u56fe\u5f0f\u66f4\u65b0\u3001\u56fe\u5f0f\u6f14\u5316\u548c\u56fe\u5f0f\u521b\u5efa\uff09\u4fdd\u6301\u8bb0\u5fc6\u7ec4\u7ec7\u8fde\u8d2f\u6027\uff0c\u8bbe\u8ba1\u7ed3\u5408\u7b26\u53f7\u63a8\u7406\u548c\u795e\u7ecf\u68c0\u7d22\u7684\u6df7\u5408\u8bb0\u5fc6\u8bbf\u95ee\u67b6\u6784\u3002", "result": "\u5728LOCOMO\u57fa\u51c6\u548c\u65b0\u63d0\u51fa\u7684AggQA\u6570\u636e\u5206\u6790\u57fa\u51c6\u4e0a\uff0cPISA\u521b\u9020\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9002\u5e94\u6027\u548c\u957f\u671f\u77e5\u8bc6\u4fdd\u7559\u80fd\u529b\u3002", "conclusion": "PISA\u8bb0\u5fc6\u7cfb\u7edf\u901a\u8fc7\u501f\u9274\u8ba4\u77e5\u53d1\u5c55\u7406\u8bba\uff0c\u4e3aAI\u4ee3\u7406\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u3001\u9002\u5e94\u6027\u66f4\u5f3a\u7684\u8bb0\u5fc6\u673a\u5236\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.16390", "categories": ["math.OC", "49M37, 65K05, 65Y20", "F.2.1; G.1.6"], "pdf": "https://arxiv.org/pdf/2510.16390", "abs": "https://arxiv.org/abs/2510.16390", "authors": ["Serge Gratton", "Philippe L. Toint"], "title": "A Simple First-Order Algorithm for Full-Rank Equality Constrained Optimization", "comment": "22 pages, 9 figures", "summary": "A very simple first-order algorithm is proposed for solving nonlinear\noptimization problems with deterministic nonlinear equality constraints. This\nalgorithm adaptively selects steps in the plane tangent to the constraints or\nsteps that reduce infeasibility, without using a merit function or filter. The\ntangent steps are based on the AdaGrad method for unconstrained minimization.\nThe objective function is never evaluated by the algorithm, making it suitable\nfor noisy problems. Its worst-case evaluation complexity is analyzed, yielding\na global convergence rate in O(1/sqrt{k}), which matches the best known rate of\nfirst-order methods for unconstrained problems. Numerical experiments are\npresented suggesting that the performance of the algorithm is comparable to\nthat of first-order methods for unconstrained problems, and that its\nreliability is remarkably stable in the presence of noise on the gradient.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7b80\u5355\u7684\u4e00\u9636\u7b97\u6cd5\uff0c\u7528\u4e8e\u6c42\u89e3\u5177\u6709\u786e\u5b9a\u6027\u975e\u7ebf\u6027\u7b49\u5f0f\u7ea6\u675f\u7684\u975e\u7ebf\u6027\u4f18\u5316\u95ee\u9898\u3002\u8be5\u7b97\u6cd5\u81ea\u9002\u5e94\u9009\u62e9\u6cbf\u7ea6\u675f\u5207\u9762\u7684\u6b65\u957f\u6216\u51cf\u5c11\u4e0d\u53ef\u884c\u6027\u7684\u6b65\u957f\uff0c\u4e0d\u4f7f\u7528\u4ef7\u503c\u51fd\u6570\u6216\u8fc7\u6ee4\u5668\u3002", "motivation": "\u89e3\u51b3\u5e26\u7ea6\u675f\u7684\u975e\u7ebf\u6027\u4f18\u5316\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5b58\u5728\u566a\u58f0\u7684\u60c5\u51b5\u4e0b\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u8bc4\u4f30\u76ee\u6807\u51fd\u6570\uff0c\u8fd9\u5728\u566a\u58f0\u95ee\u9898\u4e2d\u53ef\u80fd\u4e0d\u53ef\u9760\u3002", "method": "\u7b97\u6cd5\u81ea\u9002\u5e94\u9009\u62e9\u5207\u9762\u6b65\u957f\uff08\u57fa\u4e8eAdaGrad\u65b9\u6cd5\uff09\u6216\u51cf\u5c11\u4e0d\u53ef\u884c\u6027\u6b65\u957f\uff0c\u4e0d\u8bc4\u4f30\u76ee\u6807\u51fd\u6570\u503c\uff0c\u4ec5\u4f7f\u7528\u68af\u5ea6\u4fe1\u606f\u3002", "result": "\u6700\u574f\u60c5\u51b5\u8bc4\u4f30\u590d\u6742\u5ea6\u5206\u6790\u663e\u793a\u5168\u5c40\u6536\u655b\u7387\u4e3aO(1/\u221ak)\uff0c\u4e0e\u65e0\u7ea6\u675f\u95ee\u9898\u4e00\u9636\u65b9\u6cd5\u7684\u6700\u4f73\u5df2\u77e5\u901f\u7387\u5339\u914d\u3002\u6570\u503c\u5b9e\u9a8c\u8868\u660e\u6027\u80fd\u4e0e\u65e0\u7ea6\u675f\u4e00\u9636\u65b9\u6cd5\u76f8\u5f53\uff0c\u5728\u68af\u5ea6\u566a\u58f0\u4e0b\u53ef\u9760\u6027\u7a33\u5b9a\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u4e3a\u5e26\u7ea6\u675f\u7684\u566a\u58f0\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u4e00\u9636\u65b9\u6cd5\uff0c\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u548c\u826f\u597d\u7684\u6570\u503c\u6027\u80fd\u3002"}}
{"id": "2510.15946", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.15946", "abs": "https://arxiv.org/abs/2510.15946", "authors": ["Wenshuo Wang", "Ziyou Jiang", "Junjie Wang", "Mingyang Li", "Jie Huang", "Yuekai Huang", "Zhiyuan Chang", "Feiyan Duan", "Qing Wang"], "title": "Learning from Mistakes: Enhancing Harmful Meme Detection via Misjudgment Risk Patterns", "comment": "12 Pages, Submitted to WWW'26", "summary": "Internet memes have emerged as a popular multimodal medium, yet they are\nincreasingly weaponized to convey harmful opinions through subtle rhetorical\ndevices like irony and metaphor. Existing detection approaches, including\nMLLM-based techniques, struggle with these implicit expressions, leading to\nfrequent misjudgments. This paper introduces PatMD, a novel approach that\nimproves harmful meme detection by learning from and proactively mitigating\nthese potential misjudgment risks. Our core idea is to move beyond superficial\ncontent-level matching and instead identify the underlying misjudgment risk\npatterns, proactively guiding the MLLMs to avoid known misjudgment pitfalls. We\nfirst construct a knowledge base where each meme is deconstructed into a\nmisjudgment risk pattern explaining why it might be misjudged, either\noverlooking harmful undertones (false negative) or overinterpreting benign\ncontent (false positive). For a given target meme, PatMD retrieves relevant\npatterns and utilizes them to dynamically guide the MLLM's reasoning.\nExperiments on a benchmark of 6,626 memes across 5 harmful detection tasks show\nthat PatMD outperforms state-of-the-art baselines, achieving an average of\n8.30\\% improvement in F1-score and 7.71\\% improvement in accuracy,\ndemonstrating strong generalizability and improved detection capability of\nharmful memes.", "AI": {"tldr": "PatMD\u662f\u4e00\u79cd\u901a\u8fc7\u5b66\u4e60\u548c\u4e3b\u52a8\u7f13\u89e3\u8bef\u5224\u98ce\u9669\u6765\u6539\u8fdb\u6709\u5bb3\u8868\u60c5\u5305\u68c0\u6d4b\u7684\u65b0\u65b9\u6cd5\uff0c\u5b83\u8bc6\u522b\u6f5c\u5728\u7684\u8bef\u5224\u6a21\u5f0f\u6765\u6307\u5bfc\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u907f\u514d\u5df2\u77e5\u7684\u8bef\u5224\u9677\u9631\u3002", "motivation": "\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\uff08\u5305\u62ecMLLM\u6280\u672f\uff09\u5728\u5904\u7406\u8868\u60c5\u5305\u4e2d\u901a\u8fc7\u8bbd\u523a\u548c\u9690\u55bb\u7b49\u4fee\u8f9e\u624b\u6cd5\u8868\u8fbe\u7684\u9690\u542b\u6709\u5bb3\u5185\u5bb9\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u5bfc\u81f4\u9891\u7e41\u8bef\u5224\u3002", "method": "\u6784\u5efa\u77e5\u8bc6\u5e93\uff0c\u5c06\u6bcf\u4e2a\u8868\u60c5\u5305\u89e3\u6784\u4e3a\u8bef\u5224\u98ce\u9669\u6a21\u5f0f\uff0c\u89e3\u91ca\u5176\u53ef\u80fd\u88ab\u8bef\u5224\u7684\u539f\u56e0\uff1b\u5bf9\u4e8e\u76ee\u6807\u8868\u60c5\u5305\uff0c\u68c0\u7d22\u76f8\u5173\u6a21\u5f0f\u5e76\u52a8\u6001\u6307\u5bfcMLLM\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u57285\u4e2a\u6709\u5bb3\u68c0\u6d4b\u4efb\u52a1\u76846,626\u4e2a\u8868\u60c5\u5305\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPatMD\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u57fa\u7ebf\uff0cF1\u5206\u6570\u5e73\u5747\u63d0\u9ad88.30%\uff0c\u51c6\u786e\u7387\u63d0\u9ad87.71%\u3002", "conclusion": "PatMD\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6539\u8fdb\u7684\u6709\u5bb3\u8868\u60c5\u5305\u68c0\u6d4b\u80fd\u529b\uff0c\u901a\u8fc7\u4e3b\u52a8\u7f13\u89e3\u8bef\u5224\u98ce\u9669\u6709\u6548\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2510.16079", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16079", "abs": "https://arxiv.org/abs/2510.16079", "authors": ["Rong Wu", "Xiaoman Wang", "Jianbiao Mei", "Pinlong Cai", "Daocheng Fu", "Cheng Yang", "Licheng Wen", "Xuemeng Yang", "Yufan Shen", "Yuxin Wang", "Botian Shi"], "title": "EvolveR: Self-Evolving LLM Agents through an Experience-Driven Lifecycle", "comment": null, "summary": "Current Large Language Model (LLM) agents show strong performance in tool\nuse, but lack the crucial capability to systematically learn from their own\nexperiences. While existing frameworks mainly focus on mitigating external\nknowledge gaps, they fail to address a more fundamental limitation: the\ninability to iteratively refine problem-solving strategies. In this work, we\nintroduce EvolveR, a framework designed to enable agent to self-improve through\na complete, closed-loop experience lifecycle. This lifecycle comprises two key\nstages: (1) Offline Self-Distillation, where the agent's interaction\ntrajectories are synthesized into a structured repository of abstract, reusable\nstrategic principles; (2) Online Interaction, where the agent interacts with\ntasks and actively retrieves distilled principles to guide its decision-making,\naccumulating a diverse set of behavioral trajectories. This loop employs a\npolicy reinforcement mechanism to iteratively update the agent based on its\nperformance. We demonstrate the effectiveness of EvolveR on complex multi-hop\nquestion-answering benchmarks, where it achieves superior performance over\nstrong agentic baselines. Our work presents a comprehensive blueprint for\nagents that learn not only from external data but also from the consequences of\ntheir own actions, paving the way for more autonomous and continuously\nimproving systems. Code is available at https://github.com/Edaizi/EvolveR.", "AI": {"tldr": "EvolveR\u662f\u4e00\u4e2a\u8ba9LLM\u667a\u80fd\u4f53\u901a\u8fc7\u95ed\u73af\u7ecf\u9a8c\u751f\u547d\u5468\u671f\u5b9e\u73b0\u81ea\u6211\u6539\u8fdb\u7684\u6846\u67b6\uff0c\u5305\u542b\u79bb\u7ebf\u81ea\u84b8\u998f\u548c\u5728\u7ebf\u4ea4\u4e92\u4e24\u4e2a\u9636\u6bb5\uff0c\u5728\u590d\u6742\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709LLM\u667a\u80fd\u4f53\u5728\u5de5\u5177\u4f7f\u7528\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u7f3a\u4e4f\u4ece\u81ea\u8eab\u7ecf\u9a8c\u4e2d\u7cfb\u7edf\u5b66\u4e60\u7684\u80fd\u529b\uff0c\u65e0\u6cd5\u8fed\u4ee3\u4f18\u5316\u95ee\u9898\u89e3\u51b3\u7b56\u7565\u3002", "method": "\u91c7\u7528\u95ed\u73af\u7ecf\u9a8c\u751f\u547d\u5468\u671f\uff1a1) \u79bb\u7ebf\u81ea\u84b8\u998f - \u5c06\u4ea4\u4e92\u8f68\u8ff9\u5408\u6210\u4e3a\u7ed3\u6784\u5316\u3001\u53ef\u91cd\u7528\u7684\u62bd\u8c61\u7b56\u7565\u539f\u5219\u5e93\uff1b2) \u5728\u7ebf\u4ea4\u4e92 - \u667a\u80fd\u4f53\u4e0e\u4efb\u52a1\u4ea4\u4e92\u5e76\u68c0\u7d22\u84b8\u998f\u539f\u5219\u6307\u5bfc\u51b3\u7b56\uff0c\u79ef\u7d2f\u591a\u6837\u5316\u884c\u4e3a\u8f68\u8ff9\uff1b\u4f7f\u7528\u7b56\u7565\u5f3a\u5316\u673a\u5236\u8fed\u4ee3\u66f4\u65b0\u667a\u80fd\u4f53\u3002", "result": "\u5728\u590d\u6742\u591a\u8df3\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cEvolveR\u5b9e\u73b0\u4e86\u4f18\u4e8e\u73b0\u6709\u5f3a\u57fa\u7ebf\u667a\u80fd\u4f53\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u667a\u80fd\u4f53\u4e0d\u4ec5\u4ece\u5916\u90e8\u6570\u636e\u5b66\u4e60\uff0c\u8fd8\u80fd\u4ece\u81ea\u8eab\u884c\u4e3a\u540e\u679c\u4e2d\u5b66\u4e60\u63d0\u4f9b\u4e86\u5b8c\u6574\u84dd\u56fe\uff0c\u4e3a\u66f4\u81ea\u4e3b\u548c\u6301\u7eed\u6539\u8fdb\u7684\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2510.16297", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.16297", "abs": "https://arxiv.org/abs/2510.16297", "authors": ["Muhammad Hamza Ali", "Amritanshu Pandey"], "title": "AC Dynamics-aware Trajectory Optimization with Binary Enforcement for Adaptive UFLS Design", "comment": null, "summary": "The high penetration of distributed energy resources, resulting in backfeed\nof power at the transmission and distribution interface, is causing\nconventional underfrequency load shedding (UFLS) schemes to become\nnonconforming. Adaptive schemes that update UFLS relay settings recursively in\ntime offer a solution, but existing adaptive techniques that obtain UFLS relay\nsettings with linearized or reduced-order model formulations fail to capture AC\nnonlinear network behavior. In practice, this will result in relays unable to\nrestore system frequency during adverse disturbances. We formulate an adaptive\nUFLS problem as a trajectory optimization and include the full AC nonlinear\nnetwork dynamics to ensure AC feasibility and time-coordinated control actions.\nWe include binary decisions to model relay switching action and time-delayed\nmulti-stage load-shedding. However, this formulation results in an intractable\nMINLP problem. To enforce model tractability, we relax these binary variables\ninto continuous surrogates and reformulate the MINLP as a sequence of NLPs. We\nsolve the NLPs with a homotopy-driven method that enforces\nnear-integer-feasible solutions. We evaluate the framework on multiple\nsynthetic transmission systems and demonstrate that it scales efficiently to\nnetworks exceeding 1500+ nodes with over 170k+ continuous and 73k+ binary\ndecision variables, while successfully recovering binary-feasible solutions\nthat arrest the frequency decline during worst-case disturbance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8f68\u8ff9\u4f18\u5316\u7684\u81ea\u9002\u5e94\u4f4e\u9891\u51cf\u8f7d\u65b9\u6848\uff0c\u8003\u8651\u5b8c\u6574\u7684\u4ea4\u6d41\u975e\u7ebf\u6027\u7f51\u7edc\u52a8\u6001\uff0c\u901a\u8fc7\u677e\u5f1b\u4e8c\u8fdb\u5236\u53d8\u91cf\u548c\u540c\u4f26\u65b9\u6cd5\u89e3\u51b3\u6df7\u5408\u6574\u6570\u975e\u7ebf\u6027\u89c4\u5212\u95ee\u9898\uff0c\u5728\u5927\u578b\u7535\u7f51\u4e2d\u6709\u6548\u6062\u590d\u9891\u7387\u3002", "motivation": "\u5206\u5e03\u5f0f\u80fd\u6e90\u7684\u9ad8\u6e17\u900f\u7387\u5bfc\u81f4\u4f20\u7edf\u4f4e\u9891\u51cf\u8f7d\u65b9\u6848\u5931\u6548\uff0c\u73b0\u6709\u81ea\u9002\u5e94\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u4ea4\u6d41\u975e\u7ebf\u6027\u7f51\u7edc\u884c\u4e3a\uff0c\u5bfc\u81f4\u7ee7\u7535\u5668\u5728\u4e25\u91cd\u6270\u52a8\u65f6\u65e0\u6cd5\u6062\u590d\u7cfb\u7edf\u9891\u7387\u3002", "method": "\u5c06\u81ea\u9002\u5e94UFLS\u95ee\u9898\u8868\u8ff0\u4e3a\u8f68\u8ff9\u4f18\u5316\uff0c\u5305\u542b\u5b8c\u6574\u7684\u4ea4\u6d41\u975e\u7ebf\u6027\u7f51\u7edc\u52a8\u6001\uff0c\u901a\u8fc7\u677e\u5f1b\u4e8c\u8fdb\u5236\u53d8\u91cf\u4e3a\u8fde\u7eed\u66ff\u4ee3\u53d8\u91cf\uff0c\u5c06MINLP\u95ee\u9898\u91cd\u65b0\u8868\u8ff0\u4e3aNLP\u5e8f\u5217\uff0c\u4f7f\u7528\u540c\u4f26\u9a71\u52a8\u65b9\u6cd5\u6c42\u89e3\u63a5\u8fd1\u6574\u6570\u53ef\u884c\u7684\u89e3\u3002", "result": "\u5728\u591a\u4e2a\u5408\u6210\u8f93\u7535\u7cfb\u7edf\u4e0a\u8bc4\u4f30\uff0c\u8be5\u65b9\u6cd5\u53ef\u6269\u5c55\u52301500+\u8282\u70b9\u3001170k+\u8fde\u7eed\u53d8\u91cf\u548c73k+\u4e8c\u8fdb\u5236\u53d8\u91cf\u7684\u7f51\u7edc\uff0c\u6210\u529f\u6062\u590d\u4e8c\u8fdb\u5236\u53ef\u884c\u89e3\u5e76\u5728\u6700\u574f\u60c5\u51b5\u4e0b\u963b\u6b62\u9891\u7387\u4e0b\u964d\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u6709\u6548\u5904\u7406\u5927\u89c4\u6a21\u7535\u7f51\u7684\u81ea\u9002\u5e94\u4f4e\u9891\u51cf\u8f7d\u95ee\u9898\uff0c\u786e\u4fdd\u4ea4\u6d41\u53ef\u884c\u6027\u548c\u65f6\u95f4\u534f\u8c03\u63a7\u5236\uff0c\u5728\u4e25\u91cd\u6270\u52a8\u4e0b\u53ef\u9760\u5730\u6062\u590d\u7cfb\u7edf\u9891\u7387\u3002"}}
{"id": "2510.15929", "categories": ["q-fin.ST", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.15929", "abs": "https://arxiv.org/abs/2510.15929", "authors": ["Lucas Eduardo Pereira Teles", "Carlos M. S. Figueiredo"], "title": "Comparing LLMs for Sentiment Analysis in Financial Market News", "comment": null, "summary": "This article presents a comparative study of large language models (LLMs) in\nthe task of sentiment analysis of financial market news. This work aims to\nanalyze the performance difference of these models in this important natural\nlanguage processing task within the context of finance. LLM models are compared\nwith classical approaches, allowing for the quantification of the benefits of\neach tested model or approach. Results show that large language models\noutperform classical models in the vast majority of cases.", "AI": {"tldr": "\u6bd4\u8f83\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e\u4f20\u7edf\u65b9\u6cd5\u5728\u91d1\u878d\u65b0\u95fb\u60c5\u611f\u5206\u6790\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u8868\u73b0\uff0c\u53d1\u73b0LLM\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b", "motivation": "\u5206\u6790\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u91d1\u878d\u9886\u57df\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u91cf\u5316\u6bd4\u8f83\u4e0d\u540c\u6a21\u578b\u7684\u4f18\u52bf", "method": "\u4f7f\u7528\u6bd4\u8f83\u7814\u7a76\u65b9\u6cd5\uff0c\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e\u7ecf\u5178\u65b9\u6cd5\u5728\u91d1\u878d\u65b0\u95fb\u60c5\u611f\u5206\u6790\u4efb\u52a1\u4e2d\u8fdb\u884c\u5bf9\u6bd4", "result": "\u7ed3\u679c\u8868\u660e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7edd\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u91d1\u878d\u65b0\u95fb\u60c5\u611f\u5206\u6790\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u597d\u7684\u6027\u80fd"}}
{"id": "2510.16066", "categories": ["q-fin.ST", "cs.AI", "cs.CE", "cs.CY", "cs.LG", "q-fin.RM"], "pdf": "https://arxiv.org/pdf/2510.16066", "abs": "https://arxiv.org/abs/2510.16066", "authors": ["Chun Chet Ng", "Wei Zeng Low", "Yin Yin Boon"], "title": "Cash Flow Underwriting with Bank Transaction Data: Advancing MSME Financial Inclusion in Malaysia", "comment": "Accepted at the FinREM Workshop, ICAIF 2025", "summary": "Despite accounting for 96.1% of all businesses in Malaysia, access to\nfinancing remains one of the most persistent challenges faced by Micro, Small,\nand Medium Enterprises (MSMEs). Newly established or young businesses are often\nexcluded from formal credit markets as traditional underwriting approaches rely\nheavily on credit bureau data. This study investigates the potential of bank\nstatement data as an alternative data source for credit assessment to promote\nfinancial inclusion in emerging markets. Firstly, we propose a cash flow-based\nunderwriting pipeline where we utilise bank statement data for end to end data\nextraction and machine learning credit scoring. Secondly, we introduce a novel\ndataset of 611 loan applicants from a Malaysian lending institution. Thirdly,\nwe develop and evaluate credit scoring models based on application information\nand bank transaction-derived features. Empirical results show that the use of\nsuch data boosts the performance of all models on our dataset, which can\nimprove credit scoring for new-to-lending MSMEs. Lastly, we intend to release\nthe anonymised bank transaction dataset to facilitate further research on MSMEs\nfinancial inclusion within Malaysia's emerging economy.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u94f6\u884c\u6d41\u6c34\u6570\u636e\u4f5c\u4e3a\u66ff\u4ee3\u6570\u636e\u6e90\u5728\u9a6c\u6765\u897f\u4e9a\u5c0f\u5fae\u4f01\u4e1a\u4fe1\u7528\u8bc4\u4f30\u4e2d\u7684\u6f5c\u529b\uff0c\u63d0\u51fa\u57fa\u4e8e\u73b0\u91d1\u6d41\u7684\u4fe1\u7528\u8bc4\u4f30\u6d41\u7a0b\uff0c\u5e76\u5f00\u53d1\u4e86\u5305\u542b611\u540d\u8d37\u6b3e\u7533\u8bf7\u4eba\u7684\u6570\u636e\u96c6\u3002", "motivation": "\u89e3\u51b3\u9a6c\u6765\u897f\u4e9a\u5c0f\u5fae\u4f01\u4e1a\u878d\u8d44\u96be\u95ee\u9898\uff0c\u7279\u522b\u662f\u65b0\u6210\u7acb\u4f01\u4e1a\u56e0\u7f3a\u4e4f\u4fe1\u7528\u8bb0\u5f55\u800c\u88ab\u4f20\u7edf\u4fe1\u8d37\u5e02\u573a\u6392\u9664\u5728\u5916\u7684\u60c5\u51b5\uff0c\u4fc3\u8fdb\u65b0\u5174\u5e02\u573a\u7684\u91d1\u878d\u5305\u5bb9\u6027\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u73b0\u91d1\u6d41\u7684\u4fe1\u7528\u8bc4\u4f30\u6d41\u7a0b\uff0c\u5229\u7528\u94f6\u884c\u6d41\u6c34\u6570\u636e\u8fdb\u884c\u7aef\u5230\u7aef\u6570\u636e\u63d0\u53d6\u548c\u673a\u5668\u5b66\u4e60\u4fe1\u7528\u8bc4\u5206\uff0c\u5f00\u53d1\u5e76\u8bc4\u4f30\u57fa\u4e8e\u7533\u8bf7\u4fe1\u606f\u548c\u94f6\u884c\u4ea4\u6613\u7279\u5f81\u7684\u4fe1\u7528\u8bc4\u5206\u6a21\u578b\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u663e\u793a\uff0c\u4f7f\u7528\u94f6\u884c\u6d41\u6c34\u6570\u636e\u663e\u8457\u63d0\u5347\u4e86\u6240\u6709\u6a21\u578b\u7684\u6027\u80fd\uff0c\u80fd\u591f\u6539\u5584\u65e0\u8d37\u6b3e\u8bb0\u5f55\u5c0f\u5fae\u4f01\u4e1a\u7684\u4fe1\u7528\u8bc4\u5206\u6548\u679c\u3002", "conclusion": "\u94f6\u884c\u6d41\u6c34\u6570\u636e\u53ef\u4f5c\u4e3a\u6709\u6548\u7684\u66ff\u4ee3\u6570\u636e\u6e90\uff0c\u63d0\u9ad8\u5c0f\u5fae\u4f01\u4e1a\u4fe1\u7528\u8bc4\u4f30\u7684\u51c6\u786e\u6027\uff0c\u4fc3\u8fdb\u91d1\u878d\u5305\u5bb9\u6027\uff0c\u7814\u7a76\u8005\u8ba1\u5212\u53d1\u5e03\u533f\u540d\u94f6\u884c\u4ea4\u6613\u6570\u636e\u96c6\u4ee5\u652f\u6301\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2510.16551", "categories": ["stat.ML", "cs.LG", "econ.EM"], "pdf": "https://arxiv.org/pdf/2510.16551", "abs": "https://arxiv.org/abs/2510.16551", "authors": ["Khaled Boughanmi", "Kamel Jedidi", "Nour Jedidi"], "title": "From Reviews to Actionable Insights: An LLM-Based Approach for Attribute and Feature Extraction", "comment": null, "summary": "This research proposes a systematic, large language model (LLM) approach for\nextracting product and service attributes, features, and associated sentiments\nfrom customer reviews. Grounded in marketing theory, the framework\ndistinguishes perceptual attributes from actionable features, producing\ninterpretable and managerially actionable insights. We apply the methodology to\n20,000 Yelp reviews of Starbucks stores and evaluate eight prompt variants on a\nrandom subset of reviews. Model performance is assessed through agreement with\nhuman annotations and predictive validity for customer ratings. Results show\nhigh consistency between LLMs and human coders and strong predictive validity,\nconfirming the reliability of the approach. Human coders required a median of\nsix minutes per review, whereas the LLM processed each in two seconds,\ndelivering comparable insights at a scale unattainable through manual coding.\nManagerially, the analysis identifies attributes and features that most\nstrongly influence customer satisfaction and their associated sentiments,\nenabling firms to pinpoint \"joy points,\" address \"pain points,\" and design\ntargeted interventions. We demonstrate how structured review data can power an\nactionable marketing dashboard that tracks sentiment over time and across\nstores, benchmarks performance, and highlights high-leverage features for\nimprovement. Simulations indicate that enhancing sentiment for key service\nfeatures could yield 1-2% average revenue gains per store.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7cfb\u7edf\u65b9\u6cd5\uff0c\u4ece\u5ba2\u6237\u8bc4\u8bba\u4e2d\u63d0\u53d6\u4ea7\u54c1\u670d\u52a1\u5c5e\u6027\u3001\u7279\u5f81\u548c\u60c5\u611f\uff0c\u5e94\u7528\u4e8e2\u4e07\u6761\u661f\u5df4\u514bYelp\u8bc4\u8bba\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u53ef\u9760\u6027\u548c\u9ad8\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u4eba\u5de5\u7f16\u7801\u5206\u6790\u5ba2\u6237\u8bc4\u8bba\u8017\u65f6\u8017\u529b\uff0c\u9700\u8981\u81ea\u52a8\u5316\u3001\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u6765\u63d0\u53d6\u53ef\u64cd\u4f5c\u7684\u8425\u9500\u6d1e\u5bdf\u3002", "method": "\u57fa\u4e8e\u8425\u9500\u7406\u8bba\u6846\u67b6\uff0c\u533a\u5206\u611f\u77e5\u5c5e\u6027\u548c\u53ef\u64cd\u4f5c\u7279\u5f81\uff0c\u4f7f\u75288\u79cd\u63d0\u793a\u53d8\u4f53\u5728Yelp\u8bc4\u8bba\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30LLM\u6027\u80fd\u3002", "result": "LLM\u4e0e\u4eba\u5de5\u7f16\u7801\u9ad8\u5ea6\u4e00\u81f4\uff0c\u5904\u7406\u901f\u5ea6\u6bd4\u4eba\u5de5\u5feb180\u500d\uff082\u79d2vs6\u5206\u949f\uff09\uff0c\u8bc6\u522b\u51fa\u5f71\u54cd\u5ba2\u6237\u6ee1\u610f\u5ea6\u7684\u5173\u952e\u5c5e\u6027\u548c\u7279\u5f81\uff0c\u6a21\u62df\u663e\u793a\u4f18\u5316\u5173\u952e\u670d\u52a1\u7279\u5f81\u60c5\u611f\u53ef\u83b7\u5f971-2%\u5355\u5e97\u6536\u5165\u589e\u957f\u3002", "conclusion": "LLM\u65b9\u6cd5\u63d0\u4f9b\u4e86\u89c4\u6a21\u5316\u3001\u53ef\u64cd\u4f5c\u7684\u5ba2\u6237\u6d1e\u5bdf\uff0c\u80fd\u591f\u8bc6\u522b\u75db\u70b9\u3001\u6109\u60a6\u70b9\u5e76\u8bbe\u8ba1\u9488\u5bf9\u6027\u5e72\u9884\u63aa\u65bd\uff0c\u652f\u6301\u8425\u9500\u4eea\u8868\u677f\u8ddf\u8e2a\u60c5\u611f\u8d8b\u52bf\u548c\u6027\u80fd\u57fa\u51c6\u3002"}}
{"id": "2510.16032", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2510.16032", "abs": "https://arxiv.org/abs/2510.16032", "authors": ["Amir Karami"], "title": "A dual typology of social media interventions and deterrence mechanisms against misinformation", "comment": null, "summary": "In response to the escalating threat of misinformation, social media\nplatforms have introduced a wide range of interventions aimed at reducing the\nspread and influence of false information. However, there is a lack of a\ncoherent macrolevel perspective that explains how these interventions operate\nindependently and collectively. To address this gap, I offer a dual typology\nthrough a spectrum of interventions aligned with deterrence theory and drawing\nparallels from international relations, military, cybersecurity, and public\nhealth. I argue that five major types of platform interventions, including\nremoval, reduction, informing, composite, and multimodal, can be mapped to five\ncorresponding deterrence mechanisms, including hard, situational, soft,\nintegrated, and mixed deterrence based on purpose and perceptibility. These\nmappings illuminate how platforms apply varying degrees of deterrence\nmechanisms to influence user behavior.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5a01\u6151\u7406\u8bba\u7684\u793e\u4ea4\u5a92\u4f53\u5e72\u9884\u53cc\u91cd\u5206\u7c7b\u6cd5\uff0c\u5c06\u5e73\u53f0\u5e72\u9884\u63aa\u65bd\u6620\u5c04\u5230\u4e94\u79cd\u5a01\u6151\u673a\u5236\u4e0a", "motivation": "\u89e3\u51b3\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u5728\u5e94\u5bf9\u865a\u5047\u4fe1\u606f\u65f6\u7f3a\u4e4f\u5b8f\u89c2\u89c6\u89d2\u7684\u95ee\u9898\uff0c\u73b0\u6709\u5e72\u9884\u63aa\u65bd\u7f3a\u4e4f\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u6765\u89e3\u91ca\u5176\u72ec\u7acb\u548c\u96c6\u4f53\u8fd0\u4f5c\u673a\u5236", "method": "\u901a\u8fc7\u5a01\u6151\u7406\u8bba\u6784\u5efa\u53cc\u91cd\u5206\u7c7b\u6cd5\uff0c\u501f\u9274\u56fd\u9645\u5173\u7cfb\u3001\u519b\u4e8b\u3001\u7f51\u7edc\u5b89\u5168\u548c\u516c\u5171\u536b\u751f\u9886\u57df\u7684\u7ecf\u9a8c\uff0c\u5c06\u5e73\u53f0\u5e72\u9884\u63aa\u65bd\u4e0e\u5a01\u6151\u673a\u5236\u8fdb\u884c\u6620\u5c04", "result": "\u6210\u529f\u8bc6\u522b\u4e86\u4e94\u79cd\u4e3b\u8981\u5e73\u53f0\u5e72\u9884\u7c7b\u578b\uff08\u79fb\u9664\u3001\u51cf\u5c11\u3001\u544a\u77e5\u3001\u590d\u5408\u3001\u591a\u6a21\u5f0f\uff09\u5e76\u5bf9\u5e94\u5230\u4e94\u79cd\u5a01\u6151\u673a\u5236\uff08\u786c\u5a01\u6151\u3001\u60c5\u5883\u5a01\u6151\u3001\u8f6f\u5a01\u6151\u3001\u6574\u5408\u5a01\u6151\u3001\u6df7\u5408\u5a01\u6151\uff09", "conclusion": "\u8be5\u5206\u7c7b\u6cd5\u4e3a\u7406\u89e3\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u5982\u4f55\u5e94\u7528\u4e0d\u540c\u7a0b\u5ea6\u7684\u5a01\u6151\u673a\u5236\u6765\u5f71\u54cd\u7528\u6237\u884c\u4e3a\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u66f4\u6709\u6548\u5730\u8bbe\u8ba1\u548c\u8bc4\u4f30\u865a\u5047\u4fe1\u606f\u5e72\u9884\u7b56\u7565"}}
{"id": "2510.15974", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15974", "abs": "https://arxiv.org/abs/2510.15974", "authors": ["Chris Su", "Harrison Li", "Matheus Marques", "George Flint", "Kevin Zhu", "Sunishchal Dev"], "title": "Limits of Emergent Reasoning of Large Language Models in Agentic Frameworks for Deterministic Games", "comment": null, "summary": "Recent work reports that Large Reasoning Models (LRMs) undergo a collapse in\nperformance on solving puzzles beyond certain perplexity thresholds. In\nsubsequent discourse, questions have arisen as to whether the nature of the\ntask muddles an evaluation of true reasoning. One potential confound is the\nrequirement that the model keep track of the state space on its own. We provide\na large language model (LLM) with an environment interface for Tower of Hanoi\nproblems, allowing it to make a move with a tool call, provide written\njustification, observe the resulting state space, and reprompt itself for the\nnext move. We observe that access to an environment interface does not delay or\neradicate performance collapse. Furthermore, LLM-parameterized policy analysis\nreveals increasing divergence from both optimal policies and uniformly random\npolicies, suggesting that the model exhibits mode-like collapse at each level\nof complexity, and that performance is dependent upon whether the mode reflects\nthe correct solution for the problem. We suggest that a similar phenomena might\ntake place in LRMs.", "AI": {"tldr": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u89e3\u51b3\u8d85\u8fc7\u7279\u5b9a\u590d\u6742\u5ea6\u9608\u503c\u7684\u8c1c\u9898\u65f6\u4f1a\u51fa\u73b0\u6027\u80fd\u5d29\u6e83\uff0c\u5373\u4f7f\u63d0\u4f9b\u73af\u5883\u63a5\u53e3\u8ba9\u6a21\u578b\u80fd\u591f\u8ddf\u8e2a\u72b6\u6001\u7a7a\u95f4\uff0c\u4e5f\u65e0\u6cd5\u5ef6\u8fdf\u6216\u6d88\u9664\u8fd9\u79cd\u5d29\u6e83\u3002", "motivation": "\u63a2\u8ba8\u5927\u578b\u63a8\u7406\u6a21\u578b\u6027\u80fd\u5d29\u6e83\u7684\u771f\u6b63\u539f\u56e0\uff0c\u9a8c\u8bc1\u662f\u5426\u7531\u4e8e\u6a21\u578b\u9700\u8981\u81ea\u884c\u8ddf\u8e2a\u72b6\u6001\u7a7a\u95f4\u8fd9\u4e00\u8981\u6c42\u6df7\u6dc6\u4e86\u5bf9\u771f\u5b9e\u63a8\u7406\u80fd\u529b\u7684\u8bc4\u4f30\u3002", "method": "\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u6c49\u8bfa\u5854\u95ee\u9898\u7684\u73af\u5883\u63a5\u53e3\uff0c\u5141\u8bb8\u6a21\u578b\u901a\u8fc7\u5de5\u5177\u8c03\u7528\u8fdb\u884c\u79fb\u52a8\u3001\u63d0\u4f9b\u4e66\u9762\u7406\u7531\u3001\u89c2\u5bdf\u7ed3\u679c\u72b6\u6001\u7a7a\u95f4\uff0c\u5e76\u91cd\u65b0\u63d0\u793a\u81ea\u5df1\u8fdb\u884c\u4e0b\u4e00\u6b65\u79fb\u52a8\u3002", "result": "\u73af\u5883\u63a5\u53e3\u7684\u8bbf\u95ee\u65e0\u6cd5\u5ef6\u8fdf\u6216\u6d88\u9664\u6027\u80fd\u5d29\u6e83\uff0c\u6a21\u578b\u53c2\u6570\u5316\u7b56\u7565\u5206\u6790\u663e\u793a\u4e0e\u6700\u4f18\u7b56\u7565\u548c\u5747\u5300\u968f\u673a\u7b56\u7565\u7684\u5dee\u5f02\u90fd\u5728\u589e\u52a0\uff0c\u8868\u660e\u6a21\u578b\u5728\u6bcf\u4e2a\u590d\u6742\u5ea6\u7ea7\u522b\u90fd\u8868\u73b0\u51fa\u6a21\u5f0f\u5d29\u6e83\u3002", "conclusion": "\u6027\u80fd\u5d29\u6e83\u73b0\u8c61\u53ef\u80fd\u6e90\u4e8e\u6a21\u578b\u5728\u7279\u5b9a\u590d\u6742\u5ea6\u4e0b\u51fa\u73b0\u6a21\u5f0f\u5d29\u6e83\uff0c\u6027\u80fd\u53d6\u51b3\u4e8e\u8be5\u6a21\u5f0f\u662f\u5426\u53cd\u6620\u95ee\u9898\u7684\u6b63\u786e\u89e3\u51b3\u65b9\u6848\uff0c\u7c7b\u4f3c\u73b0\u8c61\u53ef\u80fd\u4e5f\u5b58\u5728\u4e8e\u5927\u578b\u63a8\u7406\u6a21\u578b\u4e2d\u3002"}}
{"id": "2510.16400", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2510.16400", "abs": "https://arxiv.org/abs/2510.16400", "authors": ["Chengmiao Yang", "Liguo Jiao", "Jae Hyoung Lee"], "title": "Charnes--Cooper transformation and fractional optimization with SOS-convex polynomials", "comment": null, "summary": "This paper proposes a parameter-free scheme that is based on the\nCharnes--Cooper transformation for solving a class of fractional programs with\nSOS-convex polynomials. Under certain conditions, we establish theorems of\nsolution existence,strong duality and solution extraction. An illustrative\nexample is designed to show the obtained results.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eCharnes-Cooper\u53d8\u6362\u7684\u53c2\u6570\u81ea\u7531\u65b9\u6848\uff0c\u7528\u4e8e\u6c42\u89e3\u4e00\u7c7b\u5177\u6709SOS\u51f8\u591a\u9879\u5f0f\u7684\u5206\u5f0f\u89c4\u5212\u95ee\u9898", "motivation": "\u89e3\u51b3\u5177\u6709SOS\u51f8\u591a\u9879\u5f0f\u7684\u5206\u5f0f\u89c4\u5212\u95ee\u9898\uff0c\u5efa\u7acb\u89e3\u7684\u5b58\u5728\u6027\u3001\u5f3a\u5bf9\u5076\u6027\u548c\u89e3\u63d0\u53d6\u7684\u7406\u8bba\u57fa\u7840", "method": "\u57fa\u4e8eCharnes-Cooper\u53d8\u6362\u7684\u53c2\u6570\u81ea\u7531\u65b9\u6848\uff0c\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u5efa\u7acb\u89e3\u5b58\u5728\u6027\u3001\u5f3a\u5bf9\u5076\u6027\u548c\u89e3\u63d0\u53d6\u5b9a\u7406", "result": "\u5efa\u7acb\u4e86\u89e3\u51b3\u65b9\u6848\u5b58\u5728\u6027\u3001\u5f3a\u5bf9\u5076\u6027\u548c\u89e3\u63d0\u53d6\u7684\u7406\u8bba\u5b9a\u7406\uff0c\u5e76\u901a\u8fc7\u793a\u4f8b\u9a8c\u8bc1\u4e86\u6240\u83b7\u7ed3\u679c", "conclusion": "\u8be5\u53c2\u6570\u81ea\u7531\u65b9\u6848\u80fd\u6709\u6548\u6c42\u89e3SOS\u51f8\u591a\u9879\u5f0f\u5206\u5f0f\u89c4\u5212\u95ee\u9898\uff0c\u5e76\u5177\u6709\u7406\u8bba\u4fdd\u8bc1"}}
{"id": "2510.15947", "categories": ["cs.LG", "cs.AI", "eess.SP", "q-bio.NC", "68T07, 92C55, 62M10", "I.2.6; I.5.1; J.3"], "pdf": "https://arxiv.org/pdf/2510.15947", "abs": "https://arxiv.org/abs/2510.15947", "authors": ["Casper van Laar", "Khubaib Ahmed"], "title": "WaveNet's Precision in EEG Classification", "comment": "6 pages, 5 figures and 3 tables. Includes main text and bibliography", "summary": "This study introduces a WaveNet-based deep learning model designed to\nautomate the classification of EEG signals into physiological, pathological,\nartifact, and noise categories. Traditional methods for EEG signal\nclassification, which rely on expert visual review, are becoming increasingly\nimpractical due to the growing complexity and volume of EEG recordings.\nLeveraging a publicly available annotated dataset from Mayo Clinic and St.\nAnne's University Hospital, the WaveNet model was trained, validated, and\ntested on 209,232 samples with a 70/20/10 percent split. The model achieved a\nclassification accuracy exceeding previous CNN and LSTM-based approaches, and\nwas benchmarked against a Temporal Convolutional Network (TCN) baseline.\nNotably, the model distinguishes noise and artifacts with high precision,\nalthough it reveals a modest but explainable degree of misclassification\nbetween physiological and pathological signals, reflecting inherent clinical\noverlap. WaveNet's architecture, originally developed for raw audio synthesis,\nis well suited for EEG data due to its use of dilated causal convolutions and\nresidual connections, enabling it to capture both fine-grained and long-range\ntemporal dependencies. The research also details the preprocessing pipeline,\nincluding dynamic dataset partitioning and normalization steps that support\nmodel generalization.", "AI": {"tldr": "\u4f7f\u7528WaveNet\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u81ea\u52a8\u5206\u7c7bEEG\u4fe1\u53f7\u4e3a\u751f\u7406\u3001\u75c5\u7406\u3001\u4f2a\u5f71\u548c\u566a\u58f0\u7c7b\u522b\uff0c\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u5e76\u8d85\u8d8aCNN\u548cLSTM\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u4e13\u5bb6\u89c6\u89c9\u68c0\u67e5\u7684EEG\u4fe1\u53f7\u5206\u7c7b\u65b9\u6cd5\u5728\u5904\u7406\u65e5\u76ca\u590d\u6742\u7684EEG\u8bb0\u5f55\u65f6\u53d8\u5f97\u4e0d\u5207\u5b9e\u9645\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u57fa\u4e8eWaveNet\u67b6\u6784\uff0c\u5229\u7528\u6269\u5f20\u56e0\u679c\u5377\u79ef\u548c\u6b8b\u5dee\u8fde\u63a5\u5904\u7406EEG\u6570\u636e\uff0c\u5728209,232\u4e2a\u6837\u672c\u4e0a\u8bad\u7ec3\u9a8c\u8bc1\u6d4b\u8bd5\uff0870/20/10\u5206\u5272\uff09\u3002", "result": "\u6a21\u578b\u5206\u7c7b\u51c6\u786e\u7387\u8d85\u8fc7CNN\u548cLSTM\u65b9\u6cd5\uff0c\u80fd\u9ad8\u7cbe\u5ea6\u533a\u5206\u566a\u58f0\u548c\u4f2a\u5f71\uff0c\u4f46\u5728\u751f\u7406\u548c\u75c5\u7406\u4fe1\u53f7\u95f4\u5b58\u5728\u53ef\u89e3\u91ca\u7684\u8bef\u5206\u7c7b\u3002", "conclusion": "WaveNet\u67b6\u6784\u9002\u5408EEG\u6570\u636e\u5206\u6790\uff0c\u80fd\u6355\u6349\u7ec6\u7c92\u5ea6\u548c\u957f\u7a0b\u65f6\u95f4\u4f9d\u8d56\uff0c\u4e3aEEG\u81ea\u52a8\u5206\u7c7b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.16091", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16091", "abs": "https://arxiv.org/abs/2510.16091", "authors": ["Binglan Han", "Anuradha Mathrani", "Teo Susnjak"], "title": "Evaluating Prompting Strategies and Large Language Models in Systematic Literature Review Screening: Relevance and Task-Stage Classification", "comment": null, "summary": "This study quantifies how prompting strategies interact with large language\nmodels (LLMs) to automate the screening stage of systematic literature reviews\n(SLRs). We evaluate six LLMs (GPT-4o, GPT-4o-mini, DeepSeek-Chat-V3,\nGemini-2.5-Flash, Claude-3.5-Haiku, Llama-4-Maverick) under five prompt types\n(zero-shot, few-shot, chain-of-thought (CoT), CoT-few-shot, self-reflection)\nacross relevance classification and six Level-2 tasks, using accuracy,\nprecision, recall, and F1. Results show pronounced model-prompt interaction\neffects: CoT-few-shot yields the most reliable precision-recall balance;\nzero-shot maximizes recall for high-sensitivity passes; and self-reflection\nunderperforms due to over-inclusivity and instability across models. GPT-4o and\nDeepSeek provide robust overall performance, while GPT-4o-mini performs\ncompetitively at a substantially lower dollar cost. A cost-performance analysis\nfor relevance classification (per 1,000 abstracts) reveals large absolute\ndifferences among model-prompt pairings; GPT-4o-mini remains low-cost across\nprompts, and structured prompts (CoT/CoT-few-shot) on GPT-4o-mini offer\nattractive F1 at a small incremental cost. We recommend a staged workflow that\n(1) deploys low-cost models with structured prompts for first-pass screening\nand (2) escalates only borderline cases to higher-capacity models. These\nfindings highlight LLMs' uneven but promising potential to automate literature\nscreening. By systematically analyzing prompt-model interactions, we provide a\ncomparative benchmark and practical guidance for task-adaptive LLM deployment.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e866\u79cd\u5927\u8bed\u8a00\u6a21\u578b\u57285\u79cd\u63d0\u793a\u7b56\u7565\u4e0b\u5bf9\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\u7b5b\u9009\u9636\u6bb5\u7684\u81ea\u52a8\u5316\u6548\u679c\uff0c\u53d1\u73b0\u6a21\u578b\u4e0e\u63d0\u793a\u7b56\u7565\u5b58\u5728\u663e\u8457\u4ea4\u4e92\u6548\u5e94\uff0c\u5e76\u63d0\u51fa\u4e86\u5206\u9636\u6bb5\u7684\u5de5\u4f5c\u6d41\u7a0b\u5efa\u8bae\u3002", "motivation": "\u91cf\u5316\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u63d0\u793a\u7b56\u7565\u5728\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\u7b5b\u9009\u9636\u6bb5\u7684\u4ea4\u4e92\u4f5c\u7528\uff0c\u4e3a\u81ea\u52a8\u5316\u6587\u732e\u7b5b\u9009\u63d0\u4f9b\u5b9e\u8df5\u6307\u5bfc\u3002", "method": "\u8bc4\u4f306\u79cdLLM\uff08GPT-4o\u3001GPT-4o-mini\u3001DeepSeek-Chat-V3\u3001Gemini-2.5-Flash\u3001Claude-3.5-Haiku\u3001Llama-4-Maverick\uff09\u57285\u79cd\u63d0\u793a\u7b56\u7565\uff08\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u3001\u601d\u7ef4\u94fe\u3001\u601d\u7ef4\u94fe-\u5c11\u6837\u672c\u3001\u81ea\u6211\u53cd\u601d\uff09\u4e0b\u7684\u8868\u73b0\uff0c\u4f7f\u7528\u51c6\u786e\u7387\u3001\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u4f5c\u4e3a\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u601d\u7ef4\u94fe-\u5c11\u6837\u672c\u63d0\u793a\u5728\u7cbe\u786e\u7387-\u53ec\u56de\u7387\u5e73\u8861\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff1b\u96f6\u6837\u672c\u63d0\u793a\u5728\u9ad8\u654f\u611f\u5ea6\u7b5b\u9009\u65f6\u53ec\u56de\u7387\u6700\u9ad8\uff1b\u81ea\u6211\u53cd\u601d\u8868\u73b0\u4e0d\u4f73\u3002GPT-4o\u548cDeepSeek\u6574\u4f53\u8868\u73b0\u7a33\u5065\uff0cGPT-4o-mini\u5728\u663e\u8457\u964d\u4f4e\u6210\u672c\u7684\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "\u63a8\u8350\u91c7\u7528\u5206\u9636\u6bb5\u5de5\u4f5c\u6d41\u7a0b\uff1a\u5148\u7528\u4f4e\u6210\u672c\u6a21\u578b\u548c\u7ed3\u6784\u5316\u63d0\u793a\u8fdb\u884c\u521d\u6b65\u7b5b\u9009\uff0c\u4ec5\u5bf9\u8fb9\u754c\u6848\u4f8b\u4f7f\u7528\u9ad8\u5bb9\u91cf\u6a21\u578b\u3002\u7814\u7a76\u4e3a\u4efb\u52a1\u81ea\u9002\u5e94\u7684\u5927\u8bed\u8a00\u6a21\u578b\u90e8\u7f72\u63d0\u4f9b\u4e86\u57fa\u51c6\u548c\u5b9e\u7528\u6307\u5357\u3002"}}
{"id": "2510.16352", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.16352", "abs": "https://arxiv.org/abs/2510.16352", "authors": ["Sayak Mukherjee", "Himanshu Sharma", "Wenceslao Shaw Cortez", "Genevieve Starke", "Michael Sinner", "Brooke J. Stanislawski", "Zachary Tully", "Paul Fleming", "Sonja Glavaski"], "title": "Supervisory Control of Hybrid Power Plants Using Online Feedback Optimization: Designs and Validations with a Hybrid Co-Simulation Engine", "comment": "20 pages, 9 figures", "summary": "This research investigates designing a supervisory feedback controller for a\nhybrid power plant that coordinates the wind, solar, and battery energy storage\nplants to meet the desired power demands. We have explored an online feedback\ncontrol design that does not require detailed knowledge about the models, known\nas feedback optimization. The control inputs are updated using the gradient\ninformation of the cost and the outputs with respect to the input control\ncommands. This enables us to adjust the active power references of wind, solar,\nand storage plants to meet the power generation requirements set by grid\noperators. The methodology also ensures robust control performance in the\npresence of uncertainties in the weather. In this paper, we focus on describing\nthe supervisory feedback optimization formulation and control-oriented modeling\nfor individual renewable and storage components of the hybrid power plant. The\nproposed supervisory control has been integrated with the hybrid plant\nco-simulation engine, Hercules, demonstrating its effectiveness in more\nrealistic simulation scenarios.", "AI": {"tldr": "\u8bbe\u8ba1\u6df7\u5408\u53d1\u7535\u5382\u7684\u76d1\u7763\u53cd\u9988\u63a7\u5236\u5668\uff0c\u534f\u8c03\u98ce\u80fd\u3001\u592a\u9633\u80fd\u548c\u7535\u6c60\u50a8\u80fd\u7cfb\u7edf\u4ee5\u6ee1\u8db3\u7535\u529b\u9700\u6c42\uff0c\u91c7\u7528\u65e0\u9700\u8be6\u7ec6\u6a21\u578b\u77e5\u8bc6\u7684\u5728\u7ebf\u53cd\u9988\u4f18\u5316\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u6df7\u5408\u53d1\u7535\u5382\u4e2d\u53ef\u518d\u751f\u80fd\u6e90\u548c\u50a8\u80fd\u7cfb\u7edf\u7684\u534f\u8c03\u63a7\u5236\u95ee\u9898\uff0c\u4ee5\u5e94\u5bf9\u7535\u7f51\u8fd0\u8425\u5546\u8bbe\u5b9a\u7684\u53d1\u7535\u9700\u6c42\uff0c\u5e76\u5728\u5929\u6c14\u4e0d\u786e\u5b9a\u6027\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u53cd\u9988\u4f18\u5316\u65b9\u6cd5\uff0c\u57fa\u4e8e\u6210\u672c\u548c\u8f93\u51fa\u76f8\u5bf9\u4e8e\u63a7\u5236\u8f93\u5165\u7684\u68af\u5ea6\u4fe1\u606f\u66f4\u65b0\u63a7\u5236\u6307\u4ee4\uff0c\u8c03\u6574\u98ce\u80fd\u3001\u592a\u9633\u80fd\u548c\u50a8\u80fd\u7cfb\u7edf\u7684\u6709\u529f\u529f\u7387\u53c2\u8003\u503c\u3002", "result": "\u63d0\u51fa\u7684\u76d1\u7763\u63a7\u5236\u65b9\u6cd5\u5df2\u96c6\u6210\u5230\u6df7\u5408\u53d1\u7535\u5382\u534f\u540c\u4eff\u771f\u5f15\u64ceHercules\u4e2d\uff0c\u5728\u66f4\u771f\u5b9e\u7684\u4eff\u771f\u573a\u666f\u4e2d\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u53cd\u9988\u4f18\u5316\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u534f\u8c03\u6df7\u5408\u53d1\u7535\u5382\u4e2d\u7684\u4e0d\u540c\u80fd\u6e90\u7ec4\u4ef6\uff0c\u5728\u6a21\u578b\u77e5\u8bc6\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u9c81\u68d2\u63a7\u5236\u6027\u80fd\u3002"}}
{"id": "2510.15938", "categories": ["q-fin.ST", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.15938", "abs": "https://arxiv.org/abs/2510.15938", "authors": ["Brian Godwin Lim", "Dominic Dayta", "Benedict Ryan Tiu", "Renzo Roel Tan", "Len Patrick Dominic Garces", "Kazushi Ikeda"], "title": "Dynamic Factor Analysis of Price Movements in the Philippine Stock Exchange", "comment": null, "summary": "The intricate dynamics of stock markets have led to extensive research on\nmodels that are able to effectively explain their inherent complexities. This\nstudy leverages the econometrics literature to explore the dynamic factor model\nas an interpretable model with sufficient predictive capabilities for capturing\nessential market phenomena. Although the model has been extensively applied for\npredictive purposes, this study focuses on analyzing the extracted loadings and\ncommon factors as an alternative framework for understanding stock price\ndynamics. The results reveal novel insights into traditional market theories\nwhen applied to the Philippine Stock Exchange using the Kalman method and\nmaximum likelihood estimation, with subsequent validation against the capital\nasset pricing model. Notably, a one-factor model extracts a common factor\nrepresenting systematic or market dynamics similar to the composite index,\nwhereas a two-factor model extracts common factors representing market trends\nand volatility. Furthermore, an application of the model for nowcasting the\ngrowth rates of the Philippine gross domestic product highlights the potential\nof the extracted common factors as viable real-time market indicators, yielding\nover a 34% decrease in the out-of-sample prediction error. Overall, the results\nunderscore the value of dynamic factor analysis in gaining a deeper\nunderstanding of market price movement dynamics.", "AI": {"tldr": "\u672c\u7814\u7a76\u4f7f\u7528\u52a8\u6001\u56e0\u5b50\u6a21\u578b\u5206\u6790\u83f2\u5f8b\u5bbe\u80a1\u7968\u5e02\u573a\uff0c\u53d1\u73b0\u5355\u56e0\u5b50\u6a21\u578b\u63d0\u53d6\u7684\u7cfb\u7edf\u6027\u56e0\u5b50\u7c7b\u4f3c\u7efc\u5408\u6307\u6570\uff0c\u53cc\u56e0\u5b50\u6a21\u578b\u63d0\u53d6\u5e02\u573a\u8d8b\u52bf\u548c\u6ce2\u52a8\u56e0\u5b50\u3002\u8fd9\u4e9b\u56e0\u5b50\u4f5c\u4e3a\u5b9e\u65f6\u5e02\u573a\u6307\u6807\uff0c\u5728GDP\u589e\u957f\u7387\u5b9e\u65f6\u9884\u6d4b\u4e2d\u51cf\u5c1134%\u4ee5\u4e0a\u7684\u6837\u672c\u5916\u9884\u6d4b\u8bef\u5dee\u3002", "motivation": "\u63a2\u7d22\u52a8\u6001\u56e0\u5b50\u6a21\u578b\u4f5c\u4e3a\u53ef\u89e3\u91ca\u6a21\u578b\uff0c\u7528\u4e8e\u7406\u89e3\u80a1\u7968\u4ef7\u683c\u52a8\u6001\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u9884\u6d4b\u76ee\u7684\u3002\u5173\u6ce8\u63d0\u53d6\u7684\u8f7d\u8377\u548c\u5171\u540c\u56e0\u5b50\u4f5c\u4e3a\u7406\u89e3\u5e02\u573a\u73b0\u8c61\u7684\u65b0\u6846\u67b6\u3002", "method": "\u4f7f\u7528\u5361\u5c14\u66fc\u6ee4\u6ce2\u65b9\u6cd5\u548c\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\uff0c\u5728\u83f2\u5f8b\u5bbe\u8bc1\u5238\u4ea4\u6613\u6240\u5e94\u7528\u52a8\u6001\u56e0\u5b50\u6a21\u578b\uff0c\u968f\u540e\u4e0e\u8d44\u672c\u8d44\u4ea7\u5b9a\u4ef7\u6a21\u578b\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5355\u56e0\u5b50\u6a21\u578b\u63d0\u53d6\u4ee3\u8868\u7cfb\u7edf\u6027\u6216\u5e02\u573a\u52a8\u6001\u7684\u5171\u540c\u56e0\u5b50\uff0c\u7c7b\u4f3c\u7efc\u5408\u6307\u6570\uff1b\u53cc\u56e0\u5b50\u6a21\u578b\u63d0\u53d6\u4ee3\u8868\u5e02\u573a\u8d8b\u52bf\u548c\u6ce2\u52a8\u6027\u7684\u5171\u540c\u56e0\u5b50\u3002\u63d0\u53d6\u7684\u5171\u540c\u56e0\u5b50\u4f5c\u4e3a\u5b9e\u65f6\u5e02\u573a\u6307\u6807\uff0c\u5728GDP\u589e\u957f\u7387\u5b9e\u65f6\u9884\u6d4b\u4e2d\u51cf\u5c1134%\u4ee5\u4e0a\u7684\u6837\u672c\u5916\u9884\u6d4b\u8bef\u5dee\u3002", "conclusion": "\u52a8\u6001\u56e0\u5b50\u5206\u6790\u5728\u6df1\u5165\u7406\u89e3\u5e02\u573a\u4ef7\u683c\u8fd0\u52a8\u52a8\u6001\u65b9\u9762\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u63d0\u53d6\u7684\u5171\u540c\u56e0\u5b50\u53ef\u4f5c\u4e3a\u53ef\u884c\u7684\u5b9e\u65f6\u5e02\u573a\u6307\u6807\u3002"}}
{"id": "2510.16587", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16587", "abs": "https://arxiv.org/abs/2510.16587", "authors": ["Byoungwoo Park", "Juho Lee"], "title": "Multi-Marginal Schr\u00f6dinger Bridge Matching", "comment": null, "summary": "Understanding the continuous evolution of populations from discrete temporal\nsnapshots is a critical research challenge, particularly in fields like\ndevelopmental biology and systems medicine where longitudinal tracking of\nindividual entities is often impossible. Such trajectory inference is vital for\nunraveling the mechanisms of dynamic processes. While Schr\\\"odinger Bridge (SB)\noffer a potent framework, their traditional application to pairwise time points\ncan be insufficient for systems defined by multiple intermediate snapshots.\nThis paper introduces Multi-Marginal Schr\\\"odinger Bridge Matching (MSBM), a\nnovel algorithm specifically designed for the multi-marginal SB problem. MSBM\nextends iterative Markovian fitting (IMF) to effectively handle multiple\nmarginal constraints. This technique ensures robust enforcement of all\nintermediate marginals while preserving the continuity of the learned global\ndynamics across the entire trajectory. Empirical validations on synthetic data\nand real-world single-cell RNA sequencing datasets demonstrate the competitive\nor superior performance of MSBM in capturing complex trajectories and\nrespecting intermediate distributions, all with notable computational\nefficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u591a\u8fb9\u9645\u859b\u5b9a\u8c14\u6865\u5339\u914d(MSBM)\u7b97\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u591a\u65f6\u95f4\u70b9\u8f68\u8ff9\u63a8\u65ad\u95ee\u9898\uff0c\u901a\u8fc7\u6269\u5c55\u8fed\u4ee3\u9a6c\u5c14\u53ef\u592b\u62df\u5408\u65b9\u6cd5\u6709\u6548\u7ea6\u675f\u591a\u4e2a\u4e2d\u95f4\u8fb9\u9645\u5206\u5e03\u3002", "motivation": "\u5728\u53d1\u80b2\u751f\u7269\u5b66\u548c\u7cfb\u7edf\u533b\u5b66\u7b49\u9886\u57df\uff0c\u65e0\u6cd5\u5bf9\u4e2a\u4f53\u8fdb\u884c\u7eb5\u5411\u8ddf\u8e2a\uff0c\u9700\u8981\u4ece\u79bb\u6563\u65f6\u95f4\u5feb\u7167\u63a8\u65ad\u8fde\u7eed\u6f14\u5316\u8f68\u8ff9\u3002\u4f20\u7edf\u859b\u5b9a\u8c14\u6865\u65b9\u6cd5\u4ec5\u5904\u7406\u6210\u5bf9\u65f6\u95f4\u70b9\uff0c\u5bf9\u4e8e\u591a\u4e2d\u95f4\u5feb\u7167\u7684\u7cfb\u7edf\u4e0d\u591f\u5145\u5206\u3002", "method": "MSBM\u7b97\u6cd5\u6269\u5c55\u4e86\u8fed\u4ee3\u9a6c\u5c14\u53ef\u592b\u62df\u5408(IMF)\u65b9\u6cd5\uff0c\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u591a\u8fb9\u9645\u859b\u5b9a\u8c14\u6865\u95ee\u9898\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u591a\u4e2a\u8fb9\u9645\u7ea6\u675f\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u8bc1\u9a8c\u8bc1\u8868\u660e\uff0cMSBM\u5728\u6355\u6349\u590d\u6742\u8f68\u8ff9\u548c\u5c0a\u91cd\u4e2d\u95f4\u5206\u5e03\u65b9\u9762\u5177\u6709\u7ade\u4e89\u6027\u6216\u66f4\u4f18\u6027\u80fd\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u663e\u8457\u3002", "conclusion": "MSBM\u4e3a\u591a\u65f6\u95f4\u70b9\u8f68\u8ff9\u63a8\u65ad\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u7a33\u5065\u5730\u5f3a\u5236\u6267\u884c\u6240\u6709\u4e2d\u95f4\u8fb9\u9645\u7ea6\u675f\uff0c\u540c\u65f6\u4fdd\u6301\u5b66\u4e60\u5230\u7684\u5168\u5c40\u52a8\u6001\u5728\u6574\u4e2a\u8f68\u8ff9\u4e0a\u7684\u8fde\u7eed\u6027\u3002"}}
{"id": "2510.16042", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16042", "abs": "https://arxiv.org/abs/2510.16042", "authors": ["Marcin Korecki", "Cesare Carissimo"], "title": "Does Capital Dream of Artificial Labour?", "comment": null, "summary": "This paper investigates the concept of Labour as an expression of `timenergy'\n- a fusion of time and energy - and its entanglement within the system of\nCapital. We define Labour as the commodified, quantifiable expansion of\ntimenergy, in contrast to Capital, which is capable of accumulation and\nabstraction. We explore Labour's historical evolution, its coercive and\nalienating nature, and its transformation through automation and artificial\nintelligence. Using a game-theoretic, agent-based simulation, we model\ninteractions between Capital and Labour in production processes governed by\nCobb-Douglas functions. Our results show that despite theoretical symmetry,\nlearning agents disproportionately gravitate toward capital-intensive\nprocesses, revealing Capital's superior organizational influence due to its\naccumulative capacity. We argue that Capital functions as an artificially alive\nsystem animated by the living Labour it consumes, and question whether life can\nsustain itself without the infrastructures of Capital in a future of increasing\nautomation. This study offers both a critique of and a framework for\nunderstanding Labour's subjugation within the Capital system.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c06\u52b3\u52a8\u5b9a\u4e49\u4e3a'\u65f6\u95f4\u80fd\u91cf'\u7684\u5546\u54c1\u5316\u8868\u8fbe\uff0c\u901a\u8fc7\u535a\u5f08\u8bba\u6a21\u62df\u63ed\u793a\u4e86\u8d44\u672c\u5728\u7ec4\u7ec7\u751f\u4ea7\u4e2d\u7684\u4f18\u52bf\u5730\u4f4d\uff0c\u8d28\u7591\u5728\u81ea\u52a8\u5316\u672a\u6765\u4e2d\u751f\u547d\u662f\u5426\u80fd\u5728\u6ca1\u6709\u8d44\u672c\u57fa\u7840\u8bbe\u65bd\u7684\u60c5\u51b5\u4e0b\u7ef4\u6301\u81ea\u8eab\u3002", "motivation": "\u7814\u7a76\u52b3\u52a8\u4f5c\u4e3a\u65f6\u95f4\u80fd\u91cf\u8868\u8fbe\u7684\u6982\u5ff5\uff0c\u63a2\u7d22\u5176\u5728\u8d44\u672c\u7cfb\u7edf\u4e2d\u7684\u5f02\u5316\u548c\u6f14\u53d8\uff0c\u7279\u522b\u662f\u5728\u81ea\u52a8\u5316\u548c\u4eba\u5de5\u667a\u80fd\u80cc\u666f\u4e0b\u7684\u8f6c\u53d8\u3002", "method": "\u4f7f\u7528\u535a\u5f08\u8bba\u548c\u57fa\u4e8e\u4ee3\u7406\u7684\u6a21\u62df\u65b9\u6cd5\uff0c\u901a\u8fc7\u67ef\u5e03-\u9053\u683c\u62c9\u65af\u751f\u4ea7\u51fd\u6570\u5efa\u6a21\u8d44\u672c\u4e0e\u52b3\u52a8\u5728\u751f\u4ea7\u8fc7\u7a0b\u4e2d\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u663e\u793a\uff0c\u5c3d\u7ba1\u7406\u8bba\u4e0a\u5bf9\u79f0\uff0c\u4f46\u5b66\u4e60\u4ee3\u7406\u4e0d\u6210\u6bd4\u4f8b\u5730\u503e\u5411\u4e8e\u8d44\u672c\u5bc6\u96c6\u578b\u8fc7\u7a0b\uff0c\u63ed\u793a\u4e86\u8d44\u672c\u56e0\u5176\u79ef\u7d2f\u80fd\u529b\u800c\u5177\u6709\u7684\u7ec4\u7ec7\u4f18\u52bf\u3002", "conclusion": "\u8d44\u672c\u662f\u4e00\u4e2a\u7531\u5176\u6240\u6d88\u8017\u7684\u6d3b\u52b3\u52a8\u6fc0\u6d3b\u7684\u4eba\u5de5\u751f\u547d\u7cfb\u7edf\uff0c\u7814\u7a76\u8d28\u7591\u5728\u65e5\u76ca\u81ea\u52a8\u5316\u7684\u672a\u6765\u4e2d\uff0c\u751f\u547d\u662f\u5426\u80fd\u5728\u6ca1\u6709\u8d44\u672c\u57fa\u7840\u8bbe\u65bd\u7684\u60c5\u51b5\u4e0b\u7ef4\u6301\u81ea\u8eab\u3002"}}
{"id": "2510.15980", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15980", "abs": "https://arxiv.org/abs/2510.15980", "authors": ["Dong Liu", "Yanxuan Yu"], "title": "Cognitive Load Traces as Symbolic and Visual Accounts of Deep Model Cognition", "comment": null, "summary": "We propose \\textbf{Cognitive Load Traces} (CLTs) as a mid-level\ninterpretability framework for deep models, inspired by Cognitive Load Theory\nin human cognition. CLTs are defined as symbolic, temporally varying functions\nthat quantify model-internal resource allocation. Formally, we represent CLTs\nas a three-component stochastic process $(\\mathrm{IL}_t, \\mathrm{EL}_t,\n\\mathrm{GL}_t)$, corresponding to \\emph{Intrinsic}, \\emph{Extraneous}, and\n\\emph{Germane} load. Each component is instantiated through measurable proxies\nsuch as attention entropy, KV-cache miss ratio, representation dispersion, and\ndecoding stability. We propose both symbolic formulations and visualization\nmethods (load curves, simplex diagrams) that enable interpretable analysis of\nreasoning dynamics. Experiments on reasoning and planning benchmarks show that\nCLTs predict error-onset, reveal cognitive strategies, and enable load-guided\ninterventions that improve reasoning efficiency by 15-30\\% while maintaining\naccuracy.", "AI": {"tldr": "\u63d0\u51fa\u8ba4\u77e5\u8d1f\u8377\u75d5\u8ff9\uff08CLTs\uff09\u4f5c\u4e3a\u6df1\u5ea6\u6a21\u578b\u7684\u4e2d\u5c42\u53ef\u89e3\u91ca\u6027\u6846\u67b6\uff0c\u901a\u8fc7\u7b26\u53f7\u5316\u3001\u65f6\u53d8\u7684\u51fd\u6570\u91cf\u5316\u6a21\u578b\u5185\u90e8\u8d44\u6e90\u5206\u914d\uff0c\u5305\u542b\u5185\u5728\u3001\u5916\u5728\u548c\u76f8\u5173\u8d1f\u8377\u4e09\u4e2a\u5206\u91cf\uff0c\u80fd\u9884\u6d4b\u9519\u8bef\u53d1\u751f\u3001\u63ed\u793a\u8ba4\u77e5\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u8d1f\u8377\u5f15\u5bfc\u5e72\u9884\u63d0\u9ad8\u63a8\u7406\u6548\u738715-30%", "motivation": "\u53d7\u4eba\u7c7b\u8ba4\u77e5\u8d1f\u8377\u7406\u8bba\u542f\u53d1\uff0c\u4e3a\u6df1\u5ea6\u6a21\u578b\u5f00\u53d1\u4e2d\u5c42\u53ef\u89e3\u91ca\u6027\u6846\u67b6\uff0c\u4ee5\u91cf\u5316\u6a21\u578b\u5185\u90e8\u8d44\u6e90\u5206\u914d\u548c\u63a8\u7406\u52a8\u6001", "method": "\u5c06CLTs\u5b9a\u4e49\u4e3a\u4e09\u7ec4\u5206\u968f\u673a\u8fc7\u7a0b\uff08\u5185\u5728\u8d1f\u8377IL_t\u3001\u5916\u5728\u8d1f\u8377EL_t\u3001\u76f8\u5173\u8d1f\u8377GL_t\uff09\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u71b5\u3001KV\u7f13\u5b58\u672a\u547d\u4e2d\u7387\u3001\u8868\u793a\u5206\u6563\u5ea6\u548c\u89e3\u7801\u7a33\u5b9a\u6027\u7b49\u53ef\u6d4b\u91cf\u4ee3\u7406\u8fdb\u884c\u5b9e\u4f8b\u5316\uff0c\u63d0\u51fa\u7b26\u53f7\u5316\u516c\u5f0f\u548c\u53ef\u89c6\u5316\u65b9\u6cd5", "result": "\u5728\u63a8\u7406\u548c\u89c4\u5212\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCLTs\u80fd\u9884\u6d4b\u9519\u8bef\u53d1\u751f\u3001\u63ed\u793a\u8ba4\u77e5\u7b56\u7565\uff0c\u8d1f\u8377\u5f15\u5bfc\u5e72\u9884\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u5c06\u63a8\u7406\u6548\u7387\u63d0\u9ad815-30%", "conclusion": "CLTs\u4e3a\u6df1\u5ea6\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u53ef\u89e3\u91ca\u6027\u5206\u6790\u6846\u67b6\uff0c\u80fd\u591f\u91cf\u5316\u8ba4\u77e5\u8d1f\u8377\u52a8\u6001\u5e76\u6307\u5bfc\u6a21\u578b\u4f18\u5316"}}
{"id": "2510.16468", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2510.16468", "abs": "https://arxiv.org/abs/2510.16468", "authors": ["A. A. Vyguzov", "F. S. Stonyakin"], "title": "Frank-Wolfe Algorithms for (L0, L1)-smooth functions", "comment": null, "summary": "We propose a new version of the Frank-Wolfe method, called the (L0,\nL1)-Frank-Wolfe algorithm, developed for optimization problems with (L0,\nL1)-smooth objectives. We establish that this algorithm achieves superior\ntheoretical convergence rates compared to the classical Frank-Wolfe method. In\naddition, we introduce a novel adaptive procedure, termed the Adaptive (L0,\nL1)-Frank-Wolfe algorithm, which dynamically adjusts the smoothness parameters\nto further improve performance and stability. Comprehensive numerical\nexperiments confirm the theoretical results and demonstrate the clear practical\nadvantages of both proposed algorithms over existing Frank-Wolfe variants.", "AI": {"tldr": "\u63d0\u51fa(L0,L1)-Frank-Wolfe\u7b97\u6cd5\u53ca\u5176\u81ea\u9002\u5e94\u7248\u672c\uff0c\u7528\u4e8e(L0,L1)-\u5149\u6ed1\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u76f8\u6bd4\u7ecf\u5178Frank-Wolfe\u65b9\u6cd5\u5177\u6709\u66f4\u4f18\u7684\u7406\u8bba\u6536\u655b\u7387\u548c\u5b9e\u9645\u6027\u80fd\u3002", "motivation": "\u9488\u5bf9\u5177\u6709(L0,L1)-\u5149\u6ed1\u7279\u6027\u7684\u4f18\u5316\u95ee\u9898\uff0c\u73b0\u6709Frank-Wolfe\u65b9\u6cd5\u6536\u655b\u7387\u4e0d\u591f\u7406\u60f3\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u7b97\u6cd5\u3002", "method": "\u5f00\u53d1\u4e86(L0,L1)-Frank-Wolfe\u7b97\u6cd5\u548c\u81ea\u9002\u5e94\u7248\u672c\uff0c\u540e\u8005\u80fd\u52a8\u6001\u8c03\u6574\u5149\u6ed1\u53c2\u6570\u4ee5\u63d0\u5347\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u3002", "result": "\u7406\u8bba\u5206\u6790\u663e\u793a\u65b0\u7b97\u6cd5\u5177\u6709\u66f4\u4f18\u7684\u6536\u655b\u7387\uff0c\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u76f8\u5bf9\u4e8e\u73b0\u6709Frank-Wolfe\u53d8\u79cd\u7684\u660e\u663e\u4f18\u52bf\u3002", "conclusion": "\u63d0\u51fa\u7684(L0,L1)-Frank-Wolfe\u7b97\u6cd5\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e0a\u5747\u4f18\u4e8e\u7ecf\u5178\u65b9\u6cd5\uff0c\u81ea\u9002\u5e94\u7248\u672c\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2510.15950", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15950", "abs": "https://arxiv.org/abs/2510.15950", "authors": ["Arianna Francesconi", "Donato Cappetta", "Fabio Rebecchi", "Paolo Soda", "Valerio Guarrasi", "Rosa Sicilia"], "title": "Cross-dataset Multivariate Time-series Model for Parkinson's Diagnosis via Keyboard Dynamics", "comment": "Proceedings of the Workshop on Artificial Intelligence for Biomedical\n  Data (AIBio 2025), 28th European Conference on Artificial Intelligence 2025,\n  Springer CCIS", "summary": "Parkinson's disease (PD) presents a growing global challenge, affecting over\n10 million individuals, with prevalence expected to double by 2040. Early\ndiagnosis remains difficult due to the late emergence of motor symptoms and\nlimitations of traditional clinical assessments. In this study, we propose a\nnovel pipeline that leverages keystroke dynamics as a non-invasive and scalable\nbiomarker for remote PD screening and telemonitoring. Our methodology involves\nthree main stages: (i) preprocessing of data from four distinct datasets,\nextracting four temporal signals and addressing class imbalance through the\ncomparison of three methods; (ii) pre-training eight state-of-the-art\ndeep-learning architectures on the two largest datasets, optimizing temporal\nwindowing, stride, and other hyperparameters; (iii) fine-tuning on an\nintermediate-sized dataset and performing external validation on a fourth,\nindependent cohort. Our results demonstrate that hybrid convolutional-recurrent\nand transformer-based models achieve strong external validation performance,\nwith AUC-ROC scores exceeding 90% and F1-Score over 70%. Notably, a temporal\nconvolutional model attains an AUC-ROC of 91.14% in external validation,\noutperforming existing methods that rely solely on internal validation. These\nfindings underscore the potential of keystroke dynamics as a reliable digital\nbiomarker for PD, offering a promising avenue for early detection and\ncontinuous monitoring.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u51fb\u952e\u52a8\u529b\u5b66\u7684\u5e15\u91d1\u68ee\u75c5\u7b5b\u67e5\u548c\u8fdc\u7a0b\u76d1\u6d4b\u65b0\u65b9\u6cd5\uff0c\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u5916\u90e8\u9a8c\u8bc1\u4e2d\u8fbe\u5230\u8d85\u8fc790%\u7684AUC-ROC\u548c70%\u4ee5\u4e0a\u7684F1-Score\u3002", "motivation": "\u5e15\u91d1\u68ee\u75c5\u65e9\u671f\u8bca\u65ad\u56f0\u96be\uff0c\u4f20\u7edf\u4e34\u5e8a\u8bc4\u4f30\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u975e\u4fb5\u5165\u6027\u3001\u53ef\u6269\u5c55\u7684\u751f\u7269\u6807\u5fd7\u7269\u8fdb\u884c\u8fdc\u7a0b\u7b5b\u67e5\u548c\u76d1\u6d4b\u3002", "method": "\u4e09\u9636\u6bb5\u6d41\u7a0b\uff1a\u6570\u636e\u9884\u5904\u7406\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u5904\u7406\uff1b\u5728\u6700\u5927\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec38\u79cd\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff1b\u5728\u4e2d\u7b49\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u5e76\u5728\u72ec\u7acb\u961f\u5217\u4e2d\u8fdb\u884c\u5916\u90e8\u9a8c\u8bc1\u3002", "result": "\u6df7\u5408\u5377\u79ef-\u5faa\u73af\u548c\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u8868\u73b0\u4f18\u5f02\uff0c\u5916\u90e8\u9a8c\u8bc1AUC-ROC\u8d85\u8fc790%\uff0cF1-Score\u8d85\u8fc770%\uff0c\u65f6\u95f4\u5377\u79ef\u6a21\u578b\u8fbe\u523091.14%\u7684AUC-ROC\u3002", "conclusion": "\u51fb\u952e\u52a8\u529b\u5b66\u53ef\u4f5c\u4e3a\u5e15\u91d1\u68ee\u75c5\u7684\u53ef\u9760\u6570\u5b57\u751f\u7269\u6807\u5fd7\u7269\uff0c\u4e3a\u65e9\u671f\u68c0\u6d4b\u548c\u6301\u7eed\u76d1\u6d4b\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u9014\u5f84\u3002"}}
{"id": "2510.16096", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16096", "abs": "https://arxiv.org/abs/2510.16096", "authors": ["Tina Behnia", "Puneesh Deora", "Christos Thrampoulidis"], "title": "Facts in Stats: Impacts of Pretraining Diversity on Language Model Generalization", "comment": "28 pages, 15 figures", "summary": "Language models are pretrained on sequences that blend statistical\nregularities (making text fluent) with factual associations between specific\ntokens (knowledge of facts). While recent work suggests that the variability of\ntheir interaction, such as paraphrases of factual associations, critically\ndetermines generalization ability, we lack a systematic analysis of these\nimpacts. This paper introduces a flexible synthetic testbed that combines a\nstatistical stream of generic tokens with an abstract factual stream of\nsource-target token pairs, enabling fine-grained control over their\ninteraction. The design enables the independent control of diversity nature by\nmanipulating stream composition (contextual structure) and the diversity level\nby varying which statistical streams each fact appears in. Through controlled\nexperiments, we find that while higher contextual diversity delays\nin-distribution (ID) factual accuracy, its impact on out-of-distribution (OOD)\nfactual generalization depends critically on contextual structure. In some\ncases, OOD performance follows the same trend as ID, but in others, diversity\nbecomes essential for non-trivial factual recall. Even when low diversity\nprohibits factual recall, optimal diversity levels depend on training duration.\nBeyond factual recall failures, we identify structures where statistical\ngeneralization fails independently, and others where both capabilities degrade.\nThis shows how the interplay between contextual design and diversity level\nimpacts different generalization aspects. Further, through a series of\ncontrolled interventions on the model components, we trace the OOD failures to\ndistinct optimization bottlenecks, highlighting the importance of the embedding\nand unembedding layers. Our synthetic framework allows us to isolate effects\nthat would be confounded in large-scale studies, offering a controlled testbed\nfor future investigations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5408\u6210\u6d4b\u8bd5\u5e73\u53f0\uff0c\u7528\u4e8e\u7cfb\u7edf\u5206\u6790\u8bed\u8a00\u6a21\u578b\u4e2d\u7edf\u8ba1\u89c4\u5f8b\u4e0e\u4e8b\u5b9e\u5173\u8054\u7684\u4ea4\u4e92\u4f5c\u7528\u5bf9\u6cdb\u5316\u80fd\u529b\u7684\u5f71\u54cd\u3002\u7814\u7a76\u53d1\u73b0\u4e0a\u4e0b\u6587\u591a\u6837\u6027\u5bf9\u5206\u5e03\u5185\u5916\u4e8b\u5b9e\u6cdb\u5316\u7684\u5f71\u54cd\u53d6\u51b3\u4e8e\u4e0a\u4e0b\u6587\u7ed3\u6784\uff0c\u5e76\u8bc6\u522b\u4e86\u4e0d\u540c\u4f18\u5316\u74f6\u9888\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u8bed\u8a00\u6a21\u578b\u4e2d\u7edf\u8ba1\u89c4\u5f8b\u4e0e\u4e8b\u5b9e\u5173\u8054\u4ea4\u4e92\u4f5c\u7528\u7684\u7cfb\u7edf\u5206\u6790\uff0c\u7279\u522b\u662f\u8fd9\u79cd\u4ea4\u4e92\u5982\u4f55\u5f71\u54cd\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7075\u6d3b\u7684\u5408\u6210\u6d4b\u8bd5\u5e73\u53f0\uff0c\u7ed3\u5408\u901a\u7528\u6807\u8bb0\u7684\u7edf\u8ba1\u6d41\u548c\u6e90-\u76ee\u6807\u6807\u8bb0\u5bf9\u7684\u4e8b\u5b9e\u6d41\uff0c\u80fd\u591f\u7cbe\u7ec6\u63a7\u5236\u5b83\u4eec\u7684\u4ea4\u4e92\u4f5c\u7528\uff0c\u5305\u62ec\u4e0a\u4e0b\u6587\u7ed3\u6784\u548c\u591a\u6837\u6027\u6c34\u5e73\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a\u66f4\u9ad8\u7684\u4e0a\u4e0b\u6587\u591a\u6837\u6027\u4f1a\u5ef6\u8fdf\u5206\u5e03\u5185\u4e8b\u5b9e\u51c6\u786e\u6027\uff1b\u5206\u5e03\u5916\u4e8b\u5b9e\u6cdb\u5316\u6548\u679c\u53d6\u51b3\u4e8e\u4e0a\u4e0b\u6587\u7ed3\u6784\uff1b\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u591a\u6837\u6027\u5bf9\u975e\u5e73\u51e1\u4e8b\u5b9e\u56de\u5fc6\u81f3\u5173\u91cd\u8981\uff1b\u5373\u4f7f\u4f4e\u591a\u6837\u6027\u963b\u788d\u4e8b\u5b9e\u56de\u5fc6\uff0c\u6700\u4f18\u591a\u6837\u6027\u6c34\u5e73\u4e5f\u53d6\u51b3\u4e8e\u8bad\u7ec3\u65f6\u957f\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u8bbe\u8ba1\u548c\u591a\u6837\u6027\u6c34\u5e73\u7684\u76f8\u4e92\u4f5c\u7528\u4ee5\u4e0d\u540c\u65b9\u5f0f\u5f71\u54cd\u5404\u79cd\u6cdb\u5316\u65b9\u9762\uff0c\u5d4c\u5165\u5c42\u548c\u89e3\u5d4c\u5165\u5c42\u662f\u5206\u5e03\u5916\u5931\u8d25\u7684\u5173\u952e\u4f18\u5316\u74f6\u9888\uff0c\u8be5\u5408\u6210\u6846\u67b6\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u53d7\u63a7\u6d4b\u8bd5\u5e73\u53f0\u3002"}}
{"id": "2510.16408", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.16408", "abs": "https://arxiv.org/abs/2510.16408", "authors": ["Sen Zhan", "Lingkang Jin", "Haoyang Zhang", "Nikolaos G. Paterakis"], "title": "Real-time Measurement-based Optimization for Distribution System Operation Considering Battery Voltage and Thermal Constraints", "comment": "7 pages, submitted to PSCC 2026", "summary": "The secure operation of power distribution systems is challenged by the\ngrowing integration of distributed energy resources. Leveraging the flexibility\nof battery storage offers a cost-effective alternative to measures like\ngeneration curtailment, which results in energy losses. However, developing an\neffective operational model for battery storage is hindered by inaccurate grid\nmodels, unavailability of load data, nonlinear relationship between power\ninjections and network states, intertemporal constraints, and complex\nelectrochemical and thermal dynamics. To address these challenges, this paper\nproposes a data-driven operational control scheme for battery storage in\ndistribution systems. Linear and convex quadratic operational constraints are\nconstructed based on real-time distribution system and battery storage\nmeasurements. Lyapunov optimization decouples multi-period battery operation,\nenabling a real-time, forecast-free control strategy with low computational\ncomplexity. Numerical studies using nonlinear distribution system and battery\nstorage simulators validate the effectiveness of the approach in ensuring\nsecure distribution system operation and satisfaction of voltage and thermal\nconstraints of battery storage.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u7535\u6c60\u50a8\u80fd\u8fd0\u884c\u63a7\u5236\u65b9\u6848\uff0c\u7528\u4e8e\u89e3\u51b3\u914d\u7535\u7f51\u4e2d\u5206\u5e03\u5f0f\u80fd\u6e90\u96c6\u6210\u5e26\u6765\u7684\u5b89\u5168\u8fd0\u884c\u6311\u6218\uff0c\u901a\u8fc7\u5b9e\u65f6\u6d4b\u91cf\u6784\u5efa\u7ebf\u6027\u7ea6\u675f\uff0c\u5229\u7528Lyapunov\u4f18\u5316\u5b9e\u73b0\u5b9e\u65f6\u3001\u65e0\u9700\u9884\u6d4b\u7684\u4f4e\u590d\u6742\u5ea6\u63a7\u5236\u3002", "motivation": "\u914d\u7535\u7f51\u5b89\u5168\u8fd0\u884c\u9762\u4e34\u5206\u5e03\u5f0f\u80fd\u6e90\u96c6\u6210\u6311\u6218\uff0c\u7535\u6c60\u50a8\u80fd\u7075\u6d3b\u6027\u63d0\u4f9b\u4e86\u6bd4\u53d1\u7535\u524a\u51cf\u66f4\u7ecf\u6d4e\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u73b0\u6709\u8fd0\u884c\u6a21\u578b\u53d7\u5230\u4e0d\u51c6\u786e\u7535\u7f51\u6a21\u578b\u3001\u8d1f\u8377\u6570\u636e\u7f3a\u5931\u3001\u975e\u7ebf\u6027\u5173\u7cfb\u3001\u8de8\u65f6\u6bb5\u7ea6\u675f\u548c\u590d\u6742\u7535\u5316\u5b66\u70ed\u529b\u5b66\u52a8\u6001\u7b49\u9650\u5236\u3002", "method": "\u57fa\u4e8e\u5b9e\u65f6\u914d\u7535\u7f51\u548c\u7535\u6c60\u50a8\u80fd\u6d4b\u91cf\u6784\u5efa\u7ebf\u6027\u548c\u51f8\u4e8c\u6b21\u8fd0\u884c\u7ea6\u675f\uff0c\u91c7\u7528Lyapunov\u4f18\u5316\u89e3\u8026\u591a\u65f6\u6bb5\u7535\u6c60\u8fd0\u884c\uff0c\u5b9e\u73b0\u5b9e\u65f6\u3001\u65e0\u9700\u9884\u6d4b\u7684\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u63a7\u5236\u7b56\u7565\u3002", "result": "\u4f7f\u7528\u975e\u7ebf\u6027\u914d\u7535\u7f51\u548c\u7535\u6c60\u50a8\u80fd\u4eff\u771f\u5668\u7684\u6570\u503c\u7814\u7a76\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u786e\u4fdd\u914d\u7535\u7f51\u5b89\u5168\u8fd0\u884c\u548c\u6ee1\u8db3\u7535\u6c60\u7535\u538b\u4e0e\u70ed\u7ea6\u675f\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6570\u636e\u9a71\u52a8\u63a7\u5236\u65b9\u6848\u80fd\u591f\u6709\u6548\u89e3\u51b3\u914d\u7535\u7f51\u4e2d\u7535\u6c60\u50a8\u80fd\u8fd0\u884c\u9762\u4e34\u7684\u6311\u6218\uff0c\u786e\u4fdd\u7cfb\u7edf\u5b89\u5168\u8fd0\u884c\u5e76\u6ee1\u8db3\u7535\u6c60\u7ea6\u675f\u6761\u4ef6\u3002"}}
{"id": "2510.15942", "categories": ["q-fin.ST"], "pdf": "https://arxiv.org/pdf/2510.15942", "abs": "https://arxiv.org/abs/2510.15942", "authors": ["Bhargavi Srinivasan"], "title": "Intrinsic Geometry of the Stock Market from Graph Ricci Flow", "comment": null, "summary": "We use the discrete Ollivier-Ricci graph curvature with Ricci flow to examine\nthe intrinsic geometry of financial markets through the empirical correlation\ngraph of the NASDAQ 100 index. Our main result is the development of a\ntechnique to perform surgery on the neckpinch singularities that form during\nthe Ricci flow of the empirical graph, using the behavior and the lower bound\nof curvature of the fully connected graph as a starting point. We construct an\nalgorithm that uses the curvature generated by intrinsic geometric flow of the\ngraph to detect hidden hierarchies, community behavior, and clustering in\nfinancial markets despite the underlying challenges posed by a highly connected\ngeometry.", "AI": {"tldr": "\u4f7f\u7528\u79bb\u6563Ollivier-Ricci\u56fe\u66f2\u7387\u548cRicci\u6d41\u5206\u6790NASDAQ 100\u6307\u6570\u7684\u7ecf\u9a8c\u76f8\u5173\u56fe\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u5904\u7406Ricci\u6d41\u4e2d\u9888\u7f29\u5947\u70b9\u7684\u624b\u672f\u6280\u672f\uff0c\u5e76\u6784\u5efa\u7b97\u6cd5\u5229\u7528\u56fe\u7684\u56fa\u6709\u51e0\u4f55\u6d41\u4ea7\u751f\u7684\u66f2\u7387\u68c0\u6d4b\u91d1\u878d\u5e02\u573a\u7684\u9690\u85cf\u5c42\u6b21\u7ed3\u6784\u548c\u805a\u7c7b\u3002", "motivation": "\u901a\u8fc7\u7ecf\u9a8c\u76f8\u5173\u56fe\u7684\u5185\u5728\u51e0\u4f55\u7ed3\u6784\u6765\u7814\u7a76\u91d1\u878d\u5e02\u573a\u7684\u5185\u5728\u51e0\u4f55\u7279\u6027\uff0c\u514b\u670d\u9ad8\u5ea6\u8fde\u63a5\u51e0\u4f55\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "\u4f7f\u7528\u79bb\u6563Ollivier-Ricci\u56fe\u66f2\u7387\u548cRicci\u6d41\uff0c\u4ee5\u5168\u8fde\u63a5\u56fe\u7684\u66f2\u7387\u884c\u4e3a\u548c\u4e0b\u754c\u4e3a\u8d77\u70b9\uff0c\u5f00\u53d1\u9888\u7f29\u5947\u70b9\u624b\u672f\u6280\u672f\uff0c\u6784\u5efa\u57fa\u4e8e\u66f2\u7387\u7684\u7b97\u6cd5\u3002", "result": "\u5f00\u53d1\u4e86\u5904\u7406Ricci\u6d41\u4e2d\u9888\u7f29\u5947\u70b9\u7684\u624b\u672f\u6280\u672f\uff0c\u6784\u5efa\u4e86\u80fd\u591f\u68c0\u6d4b\u91d1\u878d\u5e02\u573a\u9690\u85cf\u5c42\u6b21\u7ed3\u6784\u3001\u793e\u533a\u884c\u4e3a\u548c\u805a\u7c7b\u7684\u7b97\u6cd5\u3002", "conclusion": "\u5229\u7528\u56fe\u7684\u56fa\u6709\u51e0\u4f55\u6d41\u4ea7\u751f\u7684\u66f2\u7387\u53ef\u4ee5\u6709\u6548\u5730\u68c0\u6d4b\u91d1\u878d\u5e02\u573a\u7684\u9690\u85cf\u5c42\u6b21\u7ed3\u6784\u548c\u805a\u7c7b\uff0c\u4e3a\u7406\u89e3\u91d1\u878d\u5e02\u573a\u7ed3\u6784\u63d0\u4f9b\u4e86\u65b0\u7684\u51e0\u4f55\u89c6\u89d2\u3002"}}
{"id": "2510.16612", "categories": ["stat.ML", "cs.LG", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2510.16612", "abs": "https://arxiv.org/abs/2510.16612", "authors": ["Eli N. Weinstein", "Andrei Slabodkin", "Mattia G. Gollub", "Elizabeth B. Wood"], "title": "Accelerated Learning on Large Scale Screens using Generative Library Models", "comment": null, "summary": "Biological machine learning is often bottlenecked by a lack of scaled data.\nOne promising route to relieving data bottlenecks is through high throughput\nscreens, which can experimentally test the activity of $10^6-10^{12}$ protein\nsequences in parallel. In this article, we introduce algorithms to optimize\nhigh throughput screens for data creation and model training. We focus on the\nlarge scale regime, where dataset sizes are limited by the cost of measurement\nand sequencing. We show that when active sequences are rare, we maximize\ninformation gain if we only collect positive examples of active sequences, i.e.\n$x$ with $y>0$. We can correct for the missing negative examples using a\ngenerative model of the library, producing a consistent and efficient estimate\nof the true $p(y | x)$. We demonstrate this approach in simulation and on a\nlarge scale screen of antibodies. Overall, co-design of experiments and\ninference lets us accelerate learning dramatically.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u9ad8\u901a\u91cf\u7b5b\u9009\u5b9e\u9a8c\u7684\u7b97\u6cd5\uff0c\u901a\u8fc7\u53ea\u6536\u96c6\u9633\u6027\u5e8f\u5217\u6837\u672c\u6765\u6700\u5927\u5316\u4fe1\u606f\u589e\u76ca\uff0c\u5e76\u4f7f\u7528\u751f\u6210\u6a21\u578b\u6821\u6b63\u7f3a\u5931\u7684\u9634\u6027\u6837\u672c\uff0c\u4ece\u800c\u5728\u6d3b\u6027\u5e8f\u5217\u7a00\u5c11\u7684\u60c5\u51b5\u4e0b\u52a0\u901f\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8bad\u7ec3\u3002", "motivation": "\u751f\u7269\u673a\u5668\u5b66\u4e60\u5e38\u53d7\u9650\u4e8e\u6570\u636e\u89c4\u6a21\u4e0d\u8db3\uff0c\u9ad8\u901a\u91cf\u7b5b\u9009\u80fd\u5e76\u884c\u6d4b\u8bd5\u5927\u91cf\u86cb\u767d\u8d28\u5e8f\u5217\uff0c\u4f46\u6d4b\u91cf\u548c\u6d4b\u5e8f\u6210\u672c\u9650\u5236\u4e86\u6570\u636e\u96c6\u89c4\u6a21\u3002\u5f53\u6d3b\u6027\u5e8f\u5217\u7a00\u5c11\u65f6\uff0c\u9700\u8981\u4f18\u5316\u5b9e\u9a8c\u8bbe\u8ba1\u6765\u6700\u5927\u5316\u4fe1\u606f\u83b7\u53d6\u3002", "method": "\u5728\u6d3b\u6027\u5e8f\u5217\u7a00\u5c11\u7684\u60c5\u51b5\u4e0b\uff0c\u53ea\u6536\u96c6\u9633\u6027\u5e8f\u5217\u6837\u672c\uff08y>0\uff09\uff0c\u7136\u540e\u4f7f\u7528\u6587\u5e93\u7684\u751f\u6210\u6a21\u578b\u6765\u6821\u6b63\u7f3a\u5931\u7684\u9634\u6027\u6837\u672c\uff0c\u4ece\u800c\u83b7\u5f97\u5bf9\u771f\u5b9e\u6761\u4ef6\u6982\u7387p(y|x)\u7684\u4e00\u81f4\u4e14\u6709\u6548\u4f30\u8ba1\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u4e0e\u63a8\u65ad\u7684\u534f\u540c\u8bbe\u8ba1\uff0c\u5728\u6a21\u62df\u548c\u5927\u89c4\u6a21\u6297\u4f53\u7b5b\u9009\u4e2d\u663e\u8457\u52a0\u901f\u4e86\u5b66\u4e60\u8fc7\u7a0b\u3002", "conclusion": "\u5b9e\u9a8c\u8bbe\u8ba1\u4e0e\u63a8\u65ad\u7b97\u6cd5\u7684\u534f\u540c\u4f18\u5316\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u9ad8\u901a\u91cf\u7b5b\u9009\u7684\u6570\u636e\u6548\u7387\uff0c\u5728\u6d3b\u6027\u5e8f\u5217\u7a00\u5c11\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u66f4\u5feb\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8bad\u7ec3\u3002"}}
{"id": "2510.16048", "categories": ["cs.CY", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16048", "abs": "https://arxiv.org/abs/2510.16048", "authors": ["David Atkinson"], "title": "Open Shouldn't Mean Exempt: Open-Source Exceptionalism and Generative AI", "comment": null, "summary": "Any argument that open-source generative artificial intelligence (GenAI) is\ninherently ethical or legal solely because it is open source is flawed. Yet,\nthis is the explicit or implicit stance of several open-source GenAI entities.\nThis paper critically examines prevalent justifications for \"open-source\nexceptionalism,\" demonstrating how contemporary open-source GenAI often\ninadvertently facilitates unlawful conduct and environmental degradation\nwithout genuinely disrupting established oligopolies. Furthermore, the paper\nexposes the unsubstantiated and strategic deployment of \"democratization\" and\n\"innovation\" rhetoric to advocate for regulatory exemptions not afforded to\nproprietary systems.\n  The conclusion is that open-source developers must be held to the same legal\nand ethical standards as all other actors in the technological ecosystem.\nHowever, the paper proposes a narrowly tailored safe harbor designed to protect\nlegitimate, non-commercial scientific research, contingent upon adherence to\nspecific criteria. Ultimately, this paper advocates for a framework of\nresponsible AI development, wherein openness is pursued within established\nethical and legal boundaries, with due consideration for its broader societal\nimplications.", "AI": {"tldr": "\u8bba\u6587\u6279\u5224\u5f00\u6e90GenAI\u7684\u4f26\u7406\u548c\u6cd5\u5f8b\u4f18\u8d8a\u6027\u4e3b\u5f20\uff0c\u6307\u51fa\u5176\u5e38\u52a9\u957f\u975e\u6cd5\u884c\u4e3a\u548c\u73af\u5883\u6c61\u67d3\uff0c\u5e76\u63ed\u9732\"\u6c11\u4e3b\u5316\"\u548c\"\u521b\u65b0\"\u4fee\u8f9e\u7684\u7b56\u7565\u6027\u4f7f\u7528\u3002\u4e3b\u5f20\u5f00\u6e90\u5f00\u53d1\u8005\u5e94\u627f\u62c5\u540c\u7b49\u8d23\u4efb\uff0c\u4f46\u4e3a\u975e\u5546\u4e1a\u79d1\u7814\u63d0\u4f9b\u6709\u9650\u5b89\u5168\u6e2f\u3002", "motivation": "\u53cd\u9a73\u5f00\u6e90GenAI\u56e0\u5f00\u6e90\u800c\u5929\u7136\u5408\u4e4e\u4f26\u7406\u6216\u5408\u6cd5\u7684\u9519\u8bef\u89c2\u70b9\uff0c\u6279\u5224\u5f00\u6e90\u4f8b\u5916\u4e3b\u4e49\u7684\u4e3b\u5f20\uff0c\u63ed\u793a\u5176\u5b9e\u9645\u52a9\u957f\u975e\u6cd5\u884c\u4e3a\u548c\u73af\u5883\u95ee\u9898\u3002", "method": "\u6279\u5224\u6027\u5206\u6790\u5f00\u6e90GenAI\u5b9e\u4f53\u7684\u5e38\u89c1\u8fa9\u62a4\u7406\u7531\uff0c\u63ed\u9732\u5176\u4fee\u8f9e\u7b56\u7565\uff0c\u5e76\u63d0\u51fa\u6cd5\u5f8b\u548c\u4f26\u7406\u6846\u67b6\u5efa\u8bae\u3002", "result": "\u8bc1\u660e\u5f00\u6e90GenAI\u5e76\u672a\u771f\u6b63\u6253\u7834\u5be1\u5934\u5784\u65ad\uff0c\u53cd\u800c\u53ef\u80fd\u4fc3\u8fdb\u4e0d\u5f53\u884c\u4e3a\uff0c\u9700\u8981\u540c\u7b49\u6cd5\u5f8b\u7ea6\u675f\u3002", "conclusion": "\u4e3b\u5f20\u5728\u65e2\u5b9a\u4f26\u7406\u548c\u6cd5\u5f8b\u8fb9\u754c\u5185\u8d1f\u8d23\u4efb\u5730\u5f00\u53d1AI\uff0c\u4e3a\u975e\u5546\u4e1a\u79d1\u7814\u63d0\u4f9b\u6709\u6761\u4ef6\u7684\u5b89\u5168\u6e2f\uff0c\u786e\u4fdd\u5f00\u6e90\u4e0d\u6210\u4e3a\u89c4\u907f\u8d23\u4efb\u7684\u501f\u53e3\u3002"}}
{"id": "2510.15981", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2510.15981", "abs": "https://arxiv.org/abs/2510.15981", "authors": ["Rafael Cabral", "Tuan Manh Do", "Xuejun Yu", "Wai Ming Tai", "Zijin Feng", "Xin Shen"], "title": "ProofFlow: A Dependency Graph Approach to Faithful Proof Autoformalization", "comment": null, "summary": "Proof autoformalization, the task of translating natural language theorems\nand proofs into machine-verifiable code, is a critical step for integrating\nlarge language models into rigorous mathematical workflows. Current approaches\nfocus on producing executable code, but they frequently fail to preserve the\nsemantic meaning and logical structure of the original human-written argument.\nTo address this, we introduce ProofFlow, a novel pipeline that treats\nstructural fidelity as a primary objective. ProofFlow first constructs a\ndirected acyclic graph (DAG) to map the logical dependencies between proof\nsteps. Then, it employs a novel lemma-based approach to systematically\nformalize each step as an intermediate lemma, preserving the logical structure\nof the original argument. To facilitate evaluation, we present a new benchmark\nof 184 undergraduate-level problems, manually annotated with step-by-step\nsolutions and logical dependency graphs, and introduce ProofScore, a new\ncomposite metric to evaluate syntactic correctness, semantic faithfulness, and\nstructural fidelity. Experimental results show our pipeline sets a new\nstate-of-the-art for autoformalization, achieving a ProofScore of 0.545,\nsubstantially exceeding baselines like full-proof formalization (0.123), which\nprocesses the entire proof at once, and step-proof formalization (0.072), which\nhandles each step independently. Our pipeline, benchmark, and score metric are\nopen-sourced to encourage further progress at\nhttps://github.com/Huawei-AI4Math/ProofFlow.", "AI": {"tldr": "ProofFlow\u662f\u4e00\u4e2a\u65b0\u7684\u8bc1\u660e\u81ea\u52a8\u5f62\u5f0f\u5316\u7ba1\u9053\uff0c\u901a\u8fc7\u6784\u5efa\u903b\u8f91\u4f9d\u8d56\u56fe\u548c\u4f7f\u7528\u57fa\u4e8e\u5f15\u7406\u7684\u65b9\u6cd5\u6765\u4fdd\u6301\u539f\u59cb\u8bc1\u660e\u7684\u7ed3\u6784\u4fdd\u771f\u5ea6\uff0c\u5728\u81ea\u52a8\u5f62\u5f0f\u5316\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "motivation": "\u5f53\u524d\u7684\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u751f\u6210\u53ef\u6267\u884c\u4ee3\u7801\uff0c\u4f46\u7ecf\u5e38\u65e0\u6cd5\u4fdd\u6301\u539f\u59cb\u4eba\u5de5\u7f16\u5199\u8bc1\u660e\u7684\u8bed\u4e49\u542b\u4e49\u548c\u903b\u8f91\u7ed3\u6784\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4fdd\u6301\u7ed3\u6784\u4fdd\u771f\u5ea6\u7684\u65b0\u65b9\u6cd5\u3002", "method": "ProofFlow\u9996\u5148\u6784\u5efa\u6709\u5411\u65e0\u73af\u56fe\u6765\u6620\u5c04\u8bc1\u660e\u6b65\u9aa4\u4e4b\u95f4\u7684\u903b\u8f91\u4f9d\u8d56\u5173\u7cfb\uff0c\u7136\u540e\u91c7\u7528\u57fa\u4e8e\u5f15\u7406\u7684\u65b9\u6cd5\u7cfb\u7edf\u5730\u5c06\u6bcf\u4e2a\u6b65\u9aa4\u5f62\u5f0f\u5316\u4e3a\u4e2d\u95f4\u5f15\u7406\uff0c\u4ece\u800c\u4fdd\u6301\u539f\u59cb\u8bba\u8bc1\u7684\u903b\u8f91\u7ed3\u6784\u3002", "result": "\u5728\u5305\u542b184\u4e2a\u672c\u79d1\u6c34\u5e73\u95ee\u9898\u7684\u65b0\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cProofFlow\u5b9e\u73b0\u4e860.545\u7684ProofScore\uff0c\u663e\u8457\u8d85\u8fc7\u4e86\u5168\u8bc1\u660e\u5f62\u5f0f\u5316(0.123)\u548c\u6b65\u9aa4\u8bc1\u660e\u5f62\u5f0f\u5316(0.072)\u7b49\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "ProofFlow\u7ba1\u9053\u3001\u57fa\u51c6\u6d4b\u8bd5\u548c\u8bc4\u5206\u6307\u6807\u4e3a\u8bc1\u660e\u81ea\u52a8\u5f62\u5f0f\u5316\u8bbe\u5b9a\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6807\u51c6\uff0c\u5176\u5f00\u6e90\u53d1\u5e03\u65e8\u5728\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2510.16569", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2510.16569", "abs": "https://arxiv.org/abs/2510.16569", "authors": ["Hadi Abbaszadehpeivasti", "Etienne de Klerk", "Adrien Taylor"], "title": "On the convergence rate of the boosted Difference-of-Convex Algorithm (DCA)", "comment": "12 pages, 4 figures", "summary": "The difference-of-convex algorithm (DCA) is a well-established nonlinear\nprogramming technique that solves successive convex optimization problems.\nThese sub-problems are obtained from the difference-of-convex~(DC)\ndecompositions of the objective and constraint functions. We investigate the\nworst-case performance of the unconstrained DCA, with and without boosting,\nwhere boosting simply performs an additional step in the direction generated by\nthe usual DCA method. We show that, for certain classes of DC decompositions,\nthe boosted DCA is provably better in the worst-case than the usual DCA. While\nseveral numerical studies have reported that boosted DCA outperforms classical\nDCA, a theoretical explanation for this behavior has, to the best of our\nknowledge, not been given until now. Our proof technique relies on semidefinite\nprogramming (SDP) performance estimation.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u65e0\u7ea6\u675fDC\u7b97\u6cd5\uff08DCA\uff09\u53ca\u5176\u589e\u5f3a\u7248\u672c\u7684\u6700\u574f\u60c5\u51b5\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5728\u67d0\u4e9bDC\u5206\u89e3\u7c7b\u522b\u4e2d\uff0c\u589e\u5f3aDCA\u5728\u7406\u8bba\u4e0a\u4f18\u4e8e\u7ecf\u5178DCA\u3002", "motivation": "\u867d\u7136\u591a\u4e2a\u6570\u503c\u7814\u7a76\u8868\u660e\u589e\u5f3aDCA\u4f18\u4e8e\u7ecf\u5178DCA\uff0c\u4f46\u6b64\u524d\u7f3a\u4e4f\u7406\u8bba\u89e3\u91ca\u3002\u672c\u6587\u65e8\u5728\u4ece\u7406\u8bba\u4e0a\u5206\u6790\u8fd9\u4e24\u79cd\u7b97\u6cd5\u7684\u6027\u80fd\u5dee\u5f02\u3002", "method": "\u4f7f\u7528\u534a\u5b9a\u89c4\u5212\uff08SDP\uff09\u6027\u80fd\u4f30\u8ba1\u6280\u672f\uff0c\u5206\u6790\u65e0\u7ea6\u675fDCA\u53ca\u5176\u589e\u5f3a\u7248\u672c\u7684\u6700\u574f\u60c5\u51b5\u6027\u80fd\u3002", "result": "\u5bf9\u4e8e\u67d0\u4e9bDC\u5206\u89e3\u7c7b\u522b\uff0c\u589e\u5f3aDCA\u5728\u7406\u8bba\u4e0a\u88ab\u8bc1\u660e\u6bd4\u7ecf\u5178DCA\u5177\u6709\u66f4\u597d\u7684\u6700\u574f\u60c5\u51b5\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u9996\u6b21\u4e3a\u589e\u5f3aDCA\u4f18\u4e8e\u7ecf\u5178DCA\u7684\u73b0\u8c61\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\u3002"}}
{"id": "2510.15954", "categories": ["cs.LG", "cs.CE", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.15954", "abs": "https://arxiv.org/abs/2510.15954", "authors": ["Hongzheng Shi", "Yuhang Wang", "Xiao Liu"], "title": "Fire-EnSF: Wildfire Spread Data Assimilation using Ensemble Score Filter", "comment": null, "summary": "As wildfires become increasingly destructive and expensive to control,\neffective management of active wildfires requires accurate, real-time fire\nspread predictions. To enhance the forecasting accuracy of active fires, data\nassimilation plays a vital role by integrating observations (such as\nremote-sensing data) and fire predictions generated from numerical models. This\npaper provides a comprehensive investigation on the application of a recently\nproposed diffusion-model-based filtering algorithm -- the Ensemble Score Filter\n(EnSF) -- to the data assimilation problem for real-time active wildfire spread\npredictions. Leveraging a score-based generative diffusion model, EnSF has been\nshown to have superior accuracy for high-dimensional nonlinear filtering\nproblems, making it an ideal candidate for the filtering problems of wildfire\nspread models. Technical details are provided, and our numerical investigations\ndemonstrate that EnSF provides superior accuracy, stability, and computational\nefficiency, establishing it as a robust and practical method for wildfire data\nassimilation. Our code has been made publicly available.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u96c6\u5408\u8bc4\u5206\u6ee4\u6ce2\u5668(EnSF)\u5728\u91ce\u706b\u8513\u5ef6\u5b9e\u65f6\u9884\u6d4b\u6570\u636e\u540c\u5316\u4e2d\u7684\u5e94\u7528\uff0c\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u3001\u7a33\u5b9a\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u3002", "motivation": "\u968f\u7740\u91ce\u706b\u7834\u574f\u6027\u589e\u5f3a\u4e14\u63a7\u5236\u6210\u672c\u4e0a\u5347\uff0c\u9700\u8981\u51c6\u786e\u7684\u5b9e\u65f6\u706b\u52bf\u8513\u5ef6\u9884\u6d4b\u3002\u6570\u636e\u540c\u5316\u901a\u8fc7\u6574\u5408\u89c2\u6d4b\u6570\u636e\u548c\u6570\u503c\u6a21\u578b\u9884\u6d4b\uff0c\u5bf9\u63d0\u9ad8\u91ce\u706b\u9884\u6d4b\u51c6\u786e\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5e94\u7528\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u96c6\u5408\u8bc4\u5206\u6ee4\u6ce2\u5668(EnSF)\uff0c\u5229\u7528\u57fa\u4e8e\u5206\u6570\u7684\u751f\u6210\u6269\u6563\u6a21\u578b\u6765\u5904\u7406\u9ad8\u7ef4\u975e\u7ebf\u6027\u6ee4\u6ce2\u95ee\u9898\uff0c\u7279\u522b\u9002\u7528\u4e8e\u91ce\u706b\u8513\u5ef6\u6a21\u578b\u7684\u6ee4\u6ce2\u95ee\u9898\u3002", "result": "\u6570\u503c\u7814\u7a76\u8868\u660e\uff0cEnSF\u5728\u91ce\u706b\u6570\u636e\u540c\u5316\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u51c6\u786e\u6027\u3001\u7a33\u5b9a\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "EnSF\u88ab\u8bc1\u660e\u662f\u91ce\u706b\u6570\u636e\u540c\u5316\u7684\u7a33\u5065\u5b9e\u7528\u65b9\u6cd5\uff0c\u4ee3\u7801\u5df2\u516c\u5f00\u53ef\u7528\u3002"}}
{"id": "2510.16173", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16173", "abs": "https://arxiv.org/abs/2510.16173", "authors": ["Aria Pessianzadeh", "Naima Sultana", "Hildegarde Van den Bulck", "David Gefen", "Shahin Jabari", "Rezvaneh Rezapour"], "title": "In Generative AI We (Dis)Trust? Computational Analysis of Trust and Distrust in Reddit Discussions", "comment": null, "summary": "The rise of generative AI (GenAI) has impacted many aspects of human life. As\nthese systems become embedded in everyday practices, understanding public trust\nin them also becomes essential for responsible adoption and governance. Prior\nwork on trust in AI has largely drawn from psychology and human-computer\ninteraction, but there is a lack of computational, large-scale, and\nlongitudinal approaches to measuring trust and distrust in GenAI and large\nlanguage models (LLMs). This paper presents the first computational study of\nTrust and Distrust in GenAI, using a multi-year Reddit dataset (2022--2025)\nspanning 39 subreddits and 197,618 posts. Crowd-sourced annotations of a\nrepresentative sample were combined with classification models to scale\nanalysis. We find that Trust and Distrust are nearly balanced over time, with\nshifts around major model releases. Technical performance and usability\ndominate as dimensions, while personal experience is the most frequent reason\nshaping attitudes. Distinct patterns also emerge across trustors (e.g.,\nexperts, ethicists, general users). Our results provide a methodological\nframework for large-scale Trust analysis and insights into evolving public\nperceptions of GenAI.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u5173\u4e8e\u751f\u6210\u5f0fAI\u4fe1\u4efb\u4e0e\u4e0d\u4fe1\u4efb\u7684\u8ba1\u7b97\u7814\u7a76\uff0c\u4f7f\u75282022-2025\u5e74Reddit\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u4f17\u5305\u6807\u6ce8\u548c\u5206\u7c7b\u6a21\u578b\u8fdb\u884c\u5927\u89c4\u6a21\u5206\u6790\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u7cfb\u7edf\u878d\u5165\u65e5\u5e38\u751f\u6d3b\uff0c\u7406\u89e3\u516c\u4f17\u5bf9\u5176\u7684\u4fe1\u4efb\u5bf9\u4e8e\u8d1f\u8d23\u4efb\u91c7\u7528\u548c\u6cbb\u7406\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u8ba1\u7b97\u6027\u3001\u5927\u89c4\u6a21\u548c\u7eb5\u5411\u65b9\u6cd5\u6765\u8861\u91cf\u5bf9\u751f\u6210\u5f0fAI\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4fe1\u4efb\u4e0e\u4e0d\u4fe1\u4efb\u3002", "method": "\u4f7f\u7528\u591a\u5e74\u5ea6Reddit\u6570\u636e\u96c6\uff0839\u4e2a\u5b50\u7248\u5757\uff0c197,618\u4e2a\u5e16\u5b50\uff09\uff0c\u7ed3\u5408\u4f17\u5305\u6807\u6ce8\u4ee3\u8868\u6027\u6837\u672c\u548c\u5206\u7c7b\u6a21\u578b\u6765\u6269\u5c55\u5206\u6790\u89c4\u6a21\u3002", "result": "\u53d1\u73b0\u4fe1\u4efb\u4e0e\u4e0d\u4fe1\u4efb\u968f\u65f6\u95f4\u57fa\u672c\u5e73\u8861\uff0c\u5728\u4e3b\u8981\u6a21\u578b\u53d1\u5e03\u65f6\u51fa\u73b0\u8f6c\u53d8\u3002\u6280\u672f\u6027\u80fd\u548c\u53ef\u7528\u6027\u662f\u4e3b\u8981\u7ef4\u5ea6\uff0c\u4e2a\u4eba\u7ecf\u9a8c\u662f\u6001\u5ea6\u5f62\u6210\u7684\u6700\u5e38\u89c1\u539f\u56e0\u3002\u4e0d\u540c\u4fe1\u4efb\u8005\u7fa4\u4f53\uff08\u4e13\u5bb6\u3001\u4f26\u7406\u5b66\u5bb6\u3001\u666e\u901a\u7528\u6237\uff09\u5c55\u73b0\u51fa\u4e0d\u540c\u6a21\u5f0f\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u5927\u89c4\u6a21\u4fe1\u4efb\u5206\u6790\u63d0\u4f9b\u4e86\u65b9\u6cd5\u8bba\u6846\u67b6\uff0c\u5e76\u63ed\u793a\u4e86\u516c\u4f17\u5bf9\u751f\u6210\u5f0fAI\u8ba4\u77e5\u7684\u6f14\u53d8\u8d8b\u52bf\u3002"}}
{"id": "2510.16414", "categories": ["eess.SY", "cs.LG", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.16414", "abs": "https://arxiv.org/abs/2510.16414", "authors": ["Yuang Chen", "Fengqian Guo", "Chang Wu", "Shuyi Liu", "Hancheng Lu", "Chang Wen Chen"], "title": "AoI-Aware Task Offloading and Transmission Optimization for Industrial IoT Networks: A Branching Deep Reinforcement Learning Approach", "comment": "15 pages, 13 figures, submitted to IEEE journal for potential\n  publication", "summary": "In the Industrial Internet of Things (IIoT), the frequent transmission of\nlarge amounts of data over wireless networks should meet the stringent\ntimeliness requirements. Particularly, the freshness of packet status updates\nhas a significant impact on the system performance. In this paper, we propose\nan age-of-information (AoI)-aware multi-base station (BS) real-time monitoring\nframework to support extensive IIoT deployments. To meet the freshness\nrequirements of IIoT, we formulate a joint task offloading and resource\nallocation optimization problem with the goal of minimizing long-term average\nAoI. Tackling the core challenges of combinatorial explosion in multi-BS\ndecision spaces and the stochastic dynamics of IIoT systems is crucial, as\nthese factors render traditional optimization methods intractable. Firstly, an\ninnovative branching-based Dueling Double Deep Q-Network (Branching-D3QN)\nalgorithm is proposed to effectively implement task offloading, which optimizes\nthe convergence performance by reducing the action space complexity from\nexponential to linear levels. Then, an efficient optimization solution to\nresource allocation is proposed by proving the semi-definite property of the\nHessian matrix of bandwidth and computation resources. Finally, we propose an\niterative optimization algorithm for efficient joint task offloading and\nresource allocation to achieve optimal average AoI performance. Extensive\nsimulations demonstrate that our proposed Branching-D3QN algorithm outperforms\nboth state-of-the-art DRL methods and classical heuristics, achieving up to a\n75% enhanced convergence speed and at least a 22% reduction in the long-term\naverage AoI.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e74\u9f84\u4fe1\u606f(AoI)\u7684\u591a\u57fa\u7ad9\u5b9e\u65f6\u76d1\u63a7\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u4efb\u52a1\u5378\u8f7d\u548c\u8d44\u6e90\u5206\u914d\u4f18\u5316\u6765\u6700\u5c0f\u5316\u957f\u671f\u5e73\u5747AoI\uff0c\u4f7f\u7528\u521b\u65b0\u7684\u5206\u652fD3QN\u7b97\u6cd5\u89e3\u51b3\u591a\u57fa\u7ad9\u51b3\u7b56\u7a7a\u95f4\u7ec4\u5408\u7206\u70b8\u95ee\u9898\u3002", "motivation": "\u5de5\u4e1a\u7269\u8054\u7f51\u4e2d\u5927\u91cf\u6570\u636e\u4f20\u8f93\u9700\u8981\u6ee1\u8db3\u4e25\u683c\u7684\u5b9e\u65f6\u6027\u8981\u6c42\uff0c\u6570\u636e\u5305\u72b6\u6001\u66f4\u65b0\u7684\u65b0\u9c9c\u5ea6\u5bf9\u7cfb\u7edf\u6027\u80fd\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u9700\u8981\u89e3\u51b3\u591a\u57fa\u7ad9\u51b3\u7b56\u7a7a\u95f4\u7ec4\u5408\u7206\u70b8\u548c\u7cfb\u7edf\u968f\u673a\u52a8\u6001\u6027\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "\u9996\u5148\u63d0\u51fa\u5206\u652fD3QN\u7b97\u6cd5\u5b9e\u73b0\u4efb\u52a1\u5378\u8f7d\uff0c\u5c06\u52a8\u4f5c\u7a7a\u95f4\u590d\u6742\u5ea6\u4ece\u6307\u6570\u7ea7\u964d\u4f4e\u5230\u7ebf\u6027\u7ea7\uff1b\u7136\u540e\u901a\u8fc7\u8bc1\u660e\u5e26\u5bbd\u548c\u8ba1\u7b97\u8d44\u6e90Hessian\u77e9\u9635\u7684\u534a\u5b9a\u6027\uff0c\u63d0\u51fa\u8d44\u6e90\u5206\u914d\u4f18\u5316\u65b9\u6848\uff1b\u6700\u540e\u63d0\u51fa\u8fed\u4ee3\u4f18\u5316\u7b97\u6cd5\u5b9e\u73b0\u8054\u5408\u4f18\u5316\u3002", "result": "\u4eff\u771f\u8868\u660e\uff0c\u63d0\u51fa\u7684\u5206\u652fD3QN\u7b97\u6cd5\u4f18\u4e8e\u73b0\u6709DRL\u65b9\u6cd5\u548c\u7ecf\u5178\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u6536\u655b\u901f\u5ea6\u63d0\u534775%\uff0c\u957f\u671f\u5e73\u5747AoI\u964d\u4f4e\u81f3\u5c1122%\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5de5\u4e1a\u7269\u8054\u7f51\u4e2d\u6570\u636e\u65b0\u9c9c\u5ea6\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u6536\u655b\u6027\u80fd\u548cAoI\u6027\u80fd\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2510.16008", "categories": ["q-fin.ST", "cs.LG", "68T07, 62H30, 91G70, 62P05"], "pdf": "https://arxiv.org/pdf/2510.16008", "abs": "https://arxiv.org/abs/2510.16008", "authors": ["Rui Gon\u00e7alves", "Vitor Miguel Ribeiro", "Roman Chertovskih", "Ant\u00f3nio Pedro Aguiar"], "title": "Convolutional Attention in Betting Exchange Markets", "comment": null, "summary": "This study presents the implementation of a short-term forecasting system for\nprice movements in exchange markets, using market depth data and a systematic\nprocedure to enable a fully automated trading system. The case study focuses on\nthe UK to Win Horse Racing market during the pre-live stage on the world's\nleading betting exchange, Betfair. Innovative convolutional attention\nmechanisms are introduced and applied to multiple recurrent neural networks and\nbi-dimensional convolutional recurrent neural network layers. Additionally, a\nnovel padding method for convolutional layers is proposed, specifically\ndesigned for multivariate time series processing. These innovations are\nthoroughly detailed, along with their execution process. The proposed\narchitectures follow a standard supervised learning approach, involving model\ntraining and subsequent testing on new data, which requires extensive\npre-processing and data analysis. The study also presents a complete end-to-end\nframework for automated feature engineering and market interactions using the\ndeveloped models in production. The key finding of this research is that all\nproposed innovations positively impact the performance metrics of the\nclassification task under examination, thereby advancing the current\nstate-of-the-art in convolutional attention mechanisms and padding methods\napplied to multivariate time series problems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5e02\u573a\u6df1\u5ea6\u6570\u636e\u7684\u77ed\u671f\u4ef7\u683c\u9884\u6d4b\u7cfb\u7edf\uff0c\u5e94\u7528\u4e8eBetfair\u8d5b\u9a6c\u535a\u5f69\u5e02\u573a\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u5377\u79ef\u6ce8\u610f\u529b\u673a\u5236\u548c\u65b0\u578b\u586b\u5145\u65b9\u6cd5\u63d0\u5347\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u5904\u7406\u6027\u80fd\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u5b8c\u5168\u81ea\u52a8\u5316\u7684\u4ea4\u6613\u7cfb\u7edf\uff0c\u5229\u7528\u5e02\u573a\u6df1\u5ea6\u6570\u636e\u9884\u6d4b\u535a\u5f69\u4ea4\u6613\u6240\u7684\u4ef7\u683c\u53d8\u52a8\uff0c\u7279\u522b\u662f\u5728\u8d5b\u524d\u9636\u6bb5\u7684\u8d5b\u9a6c\u535a\u5f69\u5e02\u573a\u3002", "method": "\u91c7\u7528\u521b\u65b0\u7684\u5377\u79ef\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5e94\u7528\u4e8e\u591a\u79cd\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u548c\u4e8c\u7ef4\u5377\u79ef\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u5c42\uff0c\u5e76\u63d0\u51fa\u4e13\u95e8\u4e3a\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u5904\u7406\u8bbe\u8ba1\u7684\u65b0\u578b\u5377\u79ef\u5c42\u586b\u5145\u65b9\u6cd5\u3002", "result": "\u6240\u6709\u63d0\u51fa\u7684\u521b\u65b0\u90fd\u5bf9\u5206\u7c7b\u4efb\u52a1\u7684\u6027\u80fd\u6307\u6807\u4ea7\u751f\u79ef\u6781\u5f71\u54cd\uff0c\u5728\u5377\u79ef\u6ce8\u610f\u529b\u673a\u5236\u548c\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u95ee\u9898\u7684\u586b\u5145\u65b9\u6cd5\u65b9\u9762\u63a8\u8fdb\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u81ea\u52a8\u5316\u4ea4\u6613\u6846\u67b6\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u548c\u6570\u636e\u5904\u7406\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u535a\u5f69\u5e02\u573a\u4ef7\u683c\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2510.16652", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16652", "abs": "https://arxiv.org/abs/2510.16652", "authors": ["Zihan Wang", "Yi-Ping Chen", "Tuba Dolar", "Wei Chen"], "title": "ARCO-BO: Adaptive Resource-aware COllaborative Bayesian Optimization for Heterogeneous Multi-Agent Design", "comment": null, "summary": "Modern scientific and engineering design increasingly involves distributed\noptimization, where agents such as laboratories, simulations, or industrial\npartners pursue related goals under differing conditions. These agents often\nface heterogeneities in objectives, evaluation budgets, and accessible design\nvariables, which complicates coordination and can lead to redundancy, poor\nresource use, and ineffective information sharing. Bayesian Optimization (BO)\nis a widely used decision-making framework for expensive black box functions,\nbut its single-agent formulation assumes centralized control and full data\nsharing. Recent collaborative BO methods relax these assumptions, yet they\noften require uniform resources, fully shared input spaces, and fixed task\nalignment, conditions rarely satisfied in practice. To address these\nchallenges, we introduce Adaptive Resource Aware Collaborative Bayesian\nOptimization (ARCO-BO), a framework that explicitly accounts for heterogeneity\nin multi-agent optimization. ARCO-BO combines three components: a similarity\nand optima-aware consensus mechanism for adaptive information sharing, a\nbudget-aware asynchronous sampling strategy for resource coordination, and a\npartial input space sharing for heterogeneous design spaces. Experiments on\nsynthetic and high-dimensional engineering problems show that ARCO-BO\nconsistently outperforms independent BO and existing collaborative BO via\nconsensus approach, achieving robust and efficient performance in complex\nmulti-agent settings.", "AI": {"tldr": "\u63d0\u51faARCO-BO\u6846\u67b6\uff0c\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u8d1d\u53f6\u65af\u4f18\u5316\u4e2d\u7684\u5f02\u6784\u6027\u95ee\u9898\uff0c\u5305\u62ec\u76ee\u6807\u3001\u9884\u7b97\u548c\u8bbe\u8ba1\u7a7a\u95f4\u7684\u5f02\u8d28\u6027\u3002", "motivation": "\u73b0\u4ee3\u79d1\u5b66\u548c\u5de5\u7a0b\u8bbe\u8ba1\u4e2d\u7684\u5206\u5e03\u5f0f\u4f18\u5316\u9762\u4e34\u76ee\u6807\u3001\u8bc4\u4f30\u9884\u7b97\u548c\u53ef\u8bbf\u95ee\u8bbe\u8ba1\u53d8\u91cf\u7684\u5f02\u8d28\u6027\uff0c\u73b0\u6709\u534f\u4f5cBO\u65b9\u6cd5\u5047\u8bbe\u7edf\u4e00\u8d44\u6e90\u548c\u5b8c\u5168\u5171\u4eab\u8f93\u5165\u7a7a\u95f4\uff0c\u96be\u4ee5\u6ee1\u8db3\u5b9e\u9645\u9700\u6c42\u3002", "method": "ARCO-BO\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1a\u76f8\u4f3c\u6027\u548c\u6700\u4f18\u503c\u611f\u77e5\u7684\u5171\u8bc6\u673a\u5236\u7528\u4e8e\u81ea\u9002\u5e94\u4fe1\u606f\u5171\u4eab\u3001\u9884\u7b97\u611f\u77e5\u7684\u5f02\u6b65\u91c7\u6837\u7b56\u7565\u7528\u4e8e\u8d44\u6e90\u534f\u8c03\u3001\u90e8\u5206\u8f93\u5165\u7a7a\u95f4\u5171\u4eab\u7528\u4e8e\u5f02\u6784\u8bbe\u8ba1\u7a7a\u95f4\u3002", "result": "\u5728\u5408\u6210\u548c\u9ad8\u7ef4\u5de5\u7a0b\u95ee\u9898\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cARCO-BO\u59cb\u7ec8\u4f18\u4e8e\u72ec\u7acbBO\u548c\u73b0\u6709\u534f\u4f5cBO\u5171\u8bc6\u65b9\u6cd5\uff0c\u5728\u590d\u6742\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u5b9e\u73b0\u7a33\u5065\u9ad8\u6548\u6027\u80fd\u3002", "conclusion": "ARCO-BO\u901a\u8fc7\u663e\u5f0f\u5904\u7406\u591a\u667a\u80fd\u4f53\u4f18\u5316\u4e2d\u7684\u5f02\u8d28\u6027\uff0c\u63d0\u4f9b\u4e86\u5728\u5f02\u6784\u73af\u5883\u4e2d\u6709\u6548\u7684\u534f\u4f5c\u8d1d\u53f6\u65af\u4f18\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.16049", "categories": ["cs.CY", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16049", "abs": "https://arxiv.org/abs/2510.16049", "authors": ["David Atkinson"], "title": "In the Mood to Exclude: Revitalizing Trespass to Chattels in the Era of GenAI Scraping", "comment": null, "summary": "This paper argues that website owners have the right to exclude others from\ntheir websites. Accordingly, when generative AI (GenAI) scraping bots\nintentionally circumvent reasonable technological barriers, their conduct could\nbe actionable as trespass to chattels. If the scraping leads to a decrease in\nthe website's value, then trespass to chattels should apply. The prevailing\njudicial focus on website content and the dismissal of trespass claims absent\nproof of server impairment or user disruption misconstrues the nature of the\nwebsite itself as a form of digital property, focusing too narrowly on what\nconstitutes harm under a claim of trespass. By shifting analysis from content\nto the website itself as an integrated digital asset and illustrating the harm\nto the value of the chattel, this paper demonstrates that the right to exclude\napplies online with the same force as it does to tangible property.\n  Courts and litigants have struggled to police large-scale scraping because\ncopyright preemption narrows available claims, leaving copyright and its fair\nuse defense as the primary battleground. In contrast, recognizing websites as\npersonal property revives trespass to chattels as a meaningful cause of action,\nproviding website owners with an enforceable exclusionary right. Such\nprotection would disincentivize exploitative scraping, preserve incentives for\ncontent creation, aid in protecting privacy and personal data, and safeguard\nvalues of autonomy and expression. Ultimately, this paper contends that\nreaffirming website owners' right to exclude is essential to maintaining a fair\nand sustainable online environment.", "AI": {"tldr": "\u8bba\u6587\u4e3b\u5f20\u7f51\u7ad9\u6240\u6709\u8005\u6709\u6743\u6392\u9664\u4ed6\u4eba\u8bbf\u95ee\u5176\u7f51\u7ad9\uff0c\u5f53\u751f\u6210\u5f0fAI\u722c\u866b\u7ed5\u8fc7\u6280\u672f\u969c\u788d\u8fdb\u884c\u6570\u636e\u6293\u53d6\u65f6\uff0c\u6784\u6210\u975e\u6cd5\u4fb5\u5165\u52a8\u4ea7\u3002\u6cd5\u9662\u5e94\u5173\u6ce8\u7f51\u7ad9\u4f5c\u4e3a\u6570\u5b57\u8d44\u4ea7\u7684\u4ef7\u503c\u635f\u5bb3\uff0c\u800c\u975e\u4ec5\u5173\u6ce8\u670d\u52a1\u5668\u635f\u574f\u6216\u7528\u6237\u5e72\u6270\u3002", "motivation": "\u5f53\u524d\u6cd5\u9662\u8fc7\u5206\u5173\u6ce8\u7f51\u7ad9\u5185\u5bb9\u800c\u975e\u7f51\u7ad9\u672c\u8eab\u4f5c\u4e3a\u6570\u5b57\u8d22\u4ea7\u7684\u6027\u8d28\uff0c\u5bfc\u81f4\u975e\u6cd5\u4fb5\u5165\u52a8\u4ea7\u7d22\u8d54\u96be\u4ee5\u6210\u7acb\u3002\u7248\u6743\u4f18\u5148\u539f\u5219\u9650\u5236\u4e86\u53ef\u7528\u7d22\u8d54\uff0c\u4f7f\u7248\u6743\u548c\u5408\u7406\u4f7f\u7528\u6210\u4e3a\u4e3b\u8981\u4e89\u8bae\u70b9\u3002", "method": "\u901a\u8fc7\u5c06\u5206\u6790\u7126\u70b9\u4ece\u5185\u5bb9\u8f6c\u5411\u7f51\u7ad9\u4f5c\u4e3a\u6574\u4f53\u6570\u5b57\u8d44\u4ea7\uff0c\u8bba\u8bc1\u975e\u6cd5\u4fb5\u5165\u52a8\u4ea7\u5728\u7f51\u7edc\u73af\u5883\u4e2d\u7684\u9002\u7528\u6027\u3002\u5f3a\u8c03\u7f51\u7ad9\u6240\u6709\u8005\u6392\u9664\u6743\u7684\u6cd5\u5f8b\u57fa\u7840\u3002", "result": "\u627f\u8ba4\u7f51\u7ad9\u4e3a\u4e2a\u4eba\u8d22\u4ea7\u53ef\u6062\u590d\u975e\u6cd5\u4fb5\u5165\u52a8\u4ea7\u4f5c\u4e3a\u6709\u6548\u8bc9\u8bbc\u7406\u7531\uff0c\u4e3a\u7f51\u7ad9\u6240\u6709\u8005\u63d0\u4f9b\u53ef\u6267\u884c\u7684\u6392\u9664\u6743\uff0c\u963b\u6b62\u5265\u524a\u6027\u6570\u636e\u6293\u53d6\u3002", "conclusion": "\u91cd\u7533\u7f51\u7ad9\u6240\u6709\u8005\u7684\u6392\u9664\u6743\u5bf9\u4e8e\u7ef4\u62a4\u516c\u5e73\u53ef\u6301\u7eed\u7684\u7f51\u7edc\u73af\u5883\u81f3\u5173\u91cd\u8981\uff0c\u6709\u52a9\u4e8e\u4fdd\u62a4\u5185\u5bb9\u521b\u4f5c\u6fc0\u52b1\u3001\u9690\u79c1\u548c\u4e2a\u4eba\u6570\u636e\uff0c\u4ee5\u53ca\u81ea\u4e3b\u6743\u548c\u8868\u8fbe\u4ef7\u503c\u3002"}}
{"id": "2510.15983", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15983", "abs": "https://arxiv.org/abs/2510.15983", "authors": ["Sarah Rebecca Ondraszek", "J\u00f6rg Waitelonis", "Katja Keller", "Claudia Niessner", "Anna M. Jacyszyn", "Harald Sack"], "title": "Ontologies in Motion: A BFO-Based Approach to Knowledge Graph Construction for Motor Performance Research Data in Sports Science", "comment": "10 pages, 2 figures. Camera-ready version. Accepted to the 5th\n  International Workshop on Scientific Knowledge: Representation, Discovery,\n  and Assessment; 2 November 2025 - Nara, Japan; co-located with The 24th\n  International Semantic Web Conference, ISWC 2025. To be published in CEUR\n  proceedings", "summary": "An essential component for evaluating and comparing physical and cognitive\ncapabilities between populations is the testing of various factors related to\nhuman performance. As a core part of sports science research, testing motor\nperformance enables the analysis of the physical health of different\ndemographic groups and makes them comparable.\n  The Motor Research (MO|RE) data repository, developed at the Karlsruhe\nInstitute of Technology, is an infrastructure for publishing and archiving\nresearch data in sports science, particularly in the field of motor performance\nresearch. In this paper, we present our vision for creating a knowledge graph\nfrom MO|RE data. With an ontology rooted in the Basic Formal Ontology, our\napproach centers on formally representing the interrelation of plan\nspecifications, specific processes, and related measurements. Our goal is to\ntransform how motor performance data are modeled and shared across studies,\nmaking it standardized and machine-understandable. The idea presented here is\ndeveloped within the Leibniz Science Campus ``Digital Transformation of\nResearch'' (DiTraRe).", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eMO|RE\u6570\u636e\u4ed3\u5e93\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\u7684\u613f\u666f\uff0c\u65e8\u5728\u6807\u51c6\u5316\u548c\u673a\u5668\u53ef\u7406\u89e3\u5730\u5efa\u6a21\u4e0e\u5171\u4eab\u8fd0\u52a8\u8868\u73b0\u6570\u636e\u3002", "motivation": "\u8fd0\u52a8\u8868\u73b0\u6d4b\u8bd5\u662f\u4f53\u80b2\u79d1\u5b66\u7814\u7a76\u7684\u6838\u5fc3\uff0c\u4f46\u73b0\u6709\u6570\u636e\u7f3a\u4e4f\u6807\u51c6\u5316\u548c\u4e92\u64cd\u4f5c\u6027\uff0c\u96be\u4ee5\u5728\u4e0d\u540c\u7814\u7a76\u95f4\u8fdb\u884c\u6bd4\u8f83\u548c\u5171\u4eab\u3002", "method": "\u57fa\u4e8e\u57fa\u672c\u5f62\u5f0f\u672c\u4f53\u8bba\u6784\u5efa\u672c\u4f53\uff0c\u5f62\u5f0f\u5316\u8868\u793a\u8ba1\u5212\u89c4\u8303\u3001\u5177\u4f53\u8fc7\u7a0b\u548c\u76f8\u5173\u6d4b\u91cf\u4e4b\u95f4\u7684\u76f8\u4e92\u5173\u7cfb\uff0c\u5c06MO|RE\u6570\u636e\u8f6c\u5316\u4e3a\u77e5\u8bc6\u56fe\u8c31\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u6846\u67b6\uff0c\u80fd\u591f\u6807\u51c6\u5316\u8fd0\u52a8\u8868\u73b0\u6570\u636e\u7684\u5efa\u6a21\u548c\u5171\u4eab\u65b9\u5f0f\u3002", "conclusion": "\u8be5\u77e5\u8bc6\u56fe\u8c31\u65b9\u6cd5\u5c06\u6539\u53d8\u8fd0\u52a8\u8868\u73b0\u6570\u636e\u7684\u5efa\u6a21\u548c\u5171\u4eab\u8303\u5f0f\uff0c\u4f7f\u5176\u66f4\u52a0\u6807\u51c6\u5316\u548c\u673a\u5668\u53ef\u7406\u89e3\uff0c\u4fc3\u8fdb\u8de8\u7814\u7a76\u7684\u6570\u636e\u4e92\u64cd\u4f5c\u6027\u3002"}}
{"id": "2510.16595", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2510.16595", "abs": "https://arxiv.org/abs/2510.16595", "authors": ["Santanu S. Dey", "Burak Kocuk"], "title": "Convexification of a Separable Function over a Polyhedral Ground Set", "comment": null, "summary": "In this paper, we study the set $\\mathcal{S}^\\kappa = \\{\n(x,y)\\in\\mathcal{G}\\times\\mathbb{R}^n : y_j = x_j^\\kappa , j=1,\\dots,n\\}$,\nwhere $\\kappa > 1$ and the ground set $\\mathcal{G}$ is a nonempty polytope\ncontained in $[0,1]^n$. This nonconvex set is closely related to separable\nstandard quadratic programming and appears as a substructure in potential-based\nnetwork flow problems from gas and water networks. Our aim is to obtain the\nconvex hull of $\\mathcal{S}^\\kappa$ or its tight outer-approximation for the\nspecial case when the ground set $\\mathcal{G}$ is the standard simplex. We\npropose power cone, second-order cone and semidefinite programming relaxations\nfor this purpose, which are further strengthened by the\nReformulation-Linearization Technique and the Reformulation-Perspectification\nTechnique. For $\\kappa=2$, we obtain the convex hull of $\\mathcal{S}^\\kappa$ in\nthe low-dimensional setting. For general $\\kappa$, we give approximation\nguarantees for the power cone representable relaxation, the weakest relaxation\nwe consider. We prove that this weakest relaxation is tight with probability\none as $n\\to\\infty$ when a uniformly generated linear objective is optimized\nover it. Finally, we provide the results of our extensive computational\nexperiments comparing the empirical strength of several conic programming\nrelaxations that we propose.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4e00\u4e2a\u975e\u51f8\u96c6\u5408\u7684\u51f8\u5305\u6216\u7d27\u5916\u903c\u8fd1\uff0c\u8be5\u96c6\u5408\u4e0e\u53ef\u5206\u79bb\u6807\u51c6\u4e8c\u6b21\u89c4\u5212\u548c\u57fa\u4e8e\u52bf\u80fd\u7684\u7f51\u7edc\u6d41\u95ee\u9898\u76f8\u5173\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u591a\u79cd\u9525\u89c4\u5212\u677e\u5f1b\u65b9\u6cd5\uff0c\u5e76\u5728\u7279\u5b9a\u60c5\u51b5\u4e0b\u83b7\u5f97\u4e86\u51f8\u5305\uff0c\u7ed9\u51fa\u4e86\u8fd1\u4f3c\u4fdd\u8bc1\uff0c\u5e76\u8fdb\u884c\u4e86\u8ba1\u7b97\u5b9e\u9a8c\u6bd4\u8f83\u3002", "motivation": "\u7814\u7a76\u8be5\u975e\u51f8\u96c6\u5408\u7684\u52a8\u673a\u5728\u4e8e\u5b83\u4e0e\u53ef\u5206\u79bb\u6807\u51c6\u4e8c\u6b21\u89c4\u5212\u548c\u6c14\u4f53\u3001\u6c34\u7f51\u7edc\u4e2d\u7684\u57fa\u4e8e\u52bf\u80fd\u7684\u7f51\u7edc\u6d41\u95ee\u9898\u5bc6\u5207\u76f8\u5173\u3002\u8fd9\u4e9b\u95ee\u9898\u7684\u6c42\u89e3\u9700\u8981\u6709\u6548\u7684\u51f8\u677e\u5f1b\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u5e42\u9525\u3001\u4e8c\u9636\u9525\u548c\u534a\u5b9a\u89c4\u5212\u677e\u5f1b\uff0c\u5e76\u901a\u8fc7Reformulation-Linearization\u6280\u672f\u548cReformulation-Perspectification\u6280\u672f\u8fdb\u4e00\u6b65\u5f3a\u5316\u8fd9\u4e9b\u677e\u5f1b\u3002\u5bf9\u4e8e\u03ba=2\u7684\u60c5\u51b5\uff0c\u5728\u4f4e\u7ef4\u8bbe\u7f6e\u4e0b\u83b7\u5f97\u4e86\u51f8\u5305\u3002", "result": "\u5bf9\u4e8e\u03ba=2\uff0c\u5728\u4f4e\u7ef4\u60c5\u51b5\u4e0b\u83b7\u5f97\u4e86\u51f8\u5305\u3002\u5bf9\u4e8e\u4e00\u822c\u03ba\uff0c\u7ed9\u51fa\u4e86\u5e42\u9525\u53ef\u8868\u793a\u677e\u5f1b\u7684\u8fd1\u4f3c\u4fdd\u8bc1\uff0c\u5e76\u8bc1\u660e\u5f53n\u2192\u221e\u65f6\uff0c\u5728\u5747\u5300\u751f\u6210\u7684\u7ebf\u6027\u76ee\u6807\u4e0b\u8be5\u677e\u5f1b\u4ee5\u6982\u73871\u7d27\u3002\u901a\u8fc7\u8ba1\u7b97\u5b9e\u9a8c\u6bd4\u8f83\u4e86\u4e0d\u540c\u9525\u89c4\u5212\u677e\u5f1b\u7684\u7ecf\u9a8c\u5f3a\u5ea6\u3002", "conclusion": "\u63d0\u51fa\u7684\u591a\u79cd\u9525\u89c4\u5212\u677e\u5f1b\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u903c\u8fd1\u8be5\u975e\u51f8\u96c6\u5408\uff0c\u5176\u4e2d\u5e42\u9525\u677e\u5f1b\u5728\u6e10\u8fd1\u610f\u4e49\u4e0b\u662f\u7d27\u7684\uff0c\u8ba1\u7b97\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4e0d\u540c\u677e\u5f1b\u65b9\u6cd5\u7684\u6027\u80fd\u5dee\u5f02\u3002"}}
{"id": "2510.15955", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15955", "abs": "https://arxiv.org/abs/2510.15955", "authors": ["Kiran Kate", "Yara Rizk", "Poulami Ghosh", "Ashu Gulati", "Tathagata Chakraborti", "Zidane Wright", "Mayank Agarwal"], "title": "How Good Are LLMs at Processing Tool Outputs?", "comment": null, "summary": "Most realistic task automation problems require large language models (LLMs)\nto call tools, which often return complex JSON responses. These responses must\nbe further processed to derive the information necessary for task completion.\nThe ability of LLMs to do so is under-studied. In this paper, we study the tool\nresponse processing task and LLMs' abilities to process structured (JSON)\nresponses. We created a dataset for this task, and evaluated 15 open and closed\nweight models using multiple prompting approaches. Our results show that JSON\nprocessing remains a difficult task even for frontier models across multiple\nprompting strategies. The optimal response processing strategy depends on both\nthe nature and size of the tool outputs, as well as the complexity of the\nrequired reasoning. Variations in processing approaches can lead to performance\ndifferences ranging from 3\\% to 50\\%.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u5904\u7406\u5de5\u5177\u8fd4\u56de\u7684\u590d\u6742JSON\u54cd\u5e94\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5373\u4f7f\u662f\u524d\u6cbf\u6a21\u578b\u5728\u5904\u7406\u7ed3\u6784\u5316\u54cd\u5e94\u65f6\u4ecd\u9762\u4e34\u56f0\u96be\uff0c\u4e0d\u540c\u5904\u7406\u7b56\u7565\u7684\u6027\u80fd\u5dee\u5f02\u53ef\u8fbe3%\u523050%\u3002", "motivation": "\u73b0\u5b9e\u4efb\u52a1\u81ea\u52a8\u5316\u9700\u8981LLMs\u8c03\u7528\u5de5\u5177\u5e76\u5904\u7406\u5176\u8fd4\u56de\u7684\u590d\u6742JSON\u54cd\u5e94\uff0c\u4f46LLMs\u5904\u7406\u7ed3\u6784\u5316\u54cd\u5e94\u7684\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002", "method": "\u521b\u5efa\u4e86\u5de5\u5177\u54cd\u5e94\u5904\u7406\u4efb\u52a1\u7684\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e8615\u4e2a\u5f00\u6e90\u548c\u95ed\u6e90\u6a21\u578b\uff0c\u4f7f\u7528\u591a\u79cd\u63d0\u793a\u65b9\u6cd5\u8fdb\u884c\u7814\u7a76\u3002", "result": "JSON\u5904\u7406\u5bf9\u6240\u6709\u6a21\u578b\u6765\u8bf4\u90fd\u662f\u56f0\u96be\u4efb\u52a1\uff0c\u6700\u4f73\u5904\u7406\u7b56\u7565\u53d6\u51b3\u4e8e\u5de5\u5177\u8f93\u51fa\u7684\u6027\u8d28\u548c\u5927\u5c0f\u4ee5\u53ca\u6240\u9700\u63a8\u7406\u7684\u590d\u6742\u6027\u3002", "conclusion": "\u5de5\u5177\u54cd\u5e94\u5904\u7406\u662fLLMs\u9762\u4e34\u7684\u91cd\u8981\u6311\u6218\uff0c\u5904\u7406\u7b56\u7565\u7684\u9009\u62e9\u5bf9\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff0c\u9700\u8981\u6839\u636e\u5177\u4f53\u60c5\u51b5\u4f18\u5316\u3002"}}
{"id": "2510.16198", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16198", "abs": "https://arxiv.org/abs/2510.16198", "authors": ["Mohamed Gamil", "Abdelrahman Elsayed", "Abdelrahman Lila", "Ahmed Gad", "Hesham Abdelgawad", "Mohamed Aref", "Ahmed Fares"], "title": "EgMM-Corpus: A Multimodal Vision-Language Dataset for Egyptian Culture", "comment": null, "summary": "Despite recent advances in AI, multimodal culturally diverse datasets are\nstill limited, particularly for regions in the Middle East and Africa. In this\npaper, we introduce EgMM-Corpus, a multimodal dataset dedicated to Egyptian\nculture. By designing and running a new data collection pipeline, we collected\nover 3,000 images, covering 313 concepts across landmarks, food, and folklore.\nEach entry in the dataset is manually validated for cultural authenticity and\nmultimodal coherence. EgMM-Corpus aims to provide a reliable resource for\nevaluating and training vision-language models in an Egyptian cultural context.\nWe further evaluate the zero-shot performance of Contrastive Language-Image\nPre-training CLIP on EgMM-Corpus, on which it achieves 21.2% Top-1 accuracy and\n36.4% Top-5 accuracy in classification. These results underscore the existing\ncultural bias in large-scale vision-language models and demonstrate the\nimportance of EgMM-Corpus as a benchmark for developing culturally aware\nmodels.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86EgMM-Corpus\uff0c\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u57c3\u53ca\u6587\u5316\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5305\u542b3000\u591a\u5f20\u56fe\u50cf\uff0c\u6db5\u76d6313\u4e2a\u6587\u5316\u6982\u5ff5\u3002\u8be5\u6570\u636e\u96c6\u7528\u4e8e\u8bc4\u4f30\u548c\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u57c3\u53ca\u6587\u5316\u80cc\u666f\u4e0b\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524dAI\u9886\u57df\u7f3a\u4e4f\u4e2d\u4e1c\u548c\u975e\u6d32\u5730\u533a\u7684\u591a\u6a21\u6001\u6587\u5316\u591a\u6837\u6027\u6570\u636e\u96c6\uff0c\u7279\u522b\u662f\u9488\u5bf9\u57c3\u53ca\u6587\u5316\u7684\u8d44\u6e90\u6709\u9650\uff0c\u8fd9\u9650\u5236\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8fd9\u4e9b\u6587\u5316\u80cc\u666f\u4e0b\u7684\u8868\u73b0\u3002", "method": "\u8bbe\u8ba1\u5e76\u8fd0\u884c\u65b0\u7684\u6570\u636e\u6536\u96c6\u6d41\u7a0b\uff0c\u6536\u96c6\u4e86\u6db5\u76d6\u5730\u6807\u3001\u98df\u7269\u548c\u6c11\u95f4\u4f20\u8bf4\u7b49313\u4e2a\u6982\u5ff5\u76843000\u591a\u5f20\u56fe\u50cf\uff0c\u6bcf\u4e2a\u6761\u76ee\u90fd\u7ecf\u8fc7\u4eba\u5de5\u9a8c\u8bc1\u6587\u5316\u771f\u5b9e\u6027\u548c\u591a\u6a21\u6001\u4e00\u81f4\u6027\u3002", "result": "\u5728EgMM-Corpus\u4e0a\u8bc4\u4f30CLIP\u6a21\u578b\u7684\u96f6\u6837\u672c\u6027\u80fd\uff0cTop-1\u51c6\u786e\u7387\u4e3a21.2%\uff0cTop-5\u51c6\u786e\u7387\u4e3a36.4%\uff0c\u663e\u793a\u51fa\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u5b58\u5728\u7684\u6587\u5316\u504f\u89c1\u3002", "conclusion": "EgMM-Corpus\u4f5c\u4e3a\u5f00\u53d1\u6587\u5316\u611f\u77e5\u6a21\u578b\u7684\u91cd\u8981\u57fa\u51c6\uff0c\u7a81\u663e\u4e86\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u6587\u5316\u504f\u89c1\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.16451", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.16451", "abs": "https://arxiv.org/abs/2510.16451", "authors": ["Lidong Li", "Rui Huang", "Lin Zhao"], "title": "Stabilization of Nonlinear Systems with State-Dependent Representation: From Model-Based to Direct Data-Driven Control", "comment": null, "summary": "This paper presents a novel framework for stabilizing nonlinear systems\nrepresented in state-dependent form. We first reformulate the nonlinear\ndynamics as a state-dependent parameter-varying model and synthesize a\nstabilizing controller offline via tractable linear matrix inequalities (LMIs).\nThe resulting controller guarantees local exponential stability, maintains\nrobustness against disturbances, and provides an estimate of the region of\nattraction under input saturation. We then extend the formulation to the direct\ndata-driven setting, where a known library of basis functions represents the\ndynamics with unknown coefficients consistent with noisy experimental data. By\nleveraging Petersen's lemma, we derive data-dependent LMIs that ensure\nstability and robustness for all systems compatible with the data. Numerical\nand physical experimental results validate that our approach achieves rigorous\nend-to-end guarantees on stability, robustness, and safety directly from finite\ndata without explicit model identification.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7a33\u5b9a\u72b6\u6001\u4f9d\u8d56\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u72b6\u6001\u4f9d\u8d56\u53c2\u6570\u53d8\u5316\u6a21\u578b\u548cLMI\u5408\u6210\u63a7\u5236\u5668\uff0c\u4fdd\u8bc1\u5c40\u90e8\u6307\u6570\u7a33\u5b9a\u6027\u3001\u9c81\u68d2\u6027\u548c\u5438\u5f15\u57df\u4f30\u8ba1\uff0c\u5e76\u6269\u5c55\u5230\u76f4\u63a5\u6570\u636e\u9a71\u52a8\u8bbe\u7f6e\u3002", "motivation": "\u4e3a\u975e\u7ebf\u6027\u7cfb\u7edf\u63d0\u4f9b\u4e25\u683c\u7684\u7aef\u5230\u7aef\u7a33\u5b9a\u6027\u3001\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\u4fdd\u8bc1\uff0c\u76f4\u63a5\u4ece\u6709\u9650\u6570\u636e\u51fa\u53d1\uff0c\u65e0\u9700\u663e\u5f0f\u6a21\u578b\u8fa8\u8bc6\u3002", "method": "\u5c06\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u91cd\u65b0\u8868\u8ff0\u4e3a\u72b6\u6001\u4f9d\u8d56\u53c2\u6570\u53d8\u5316\u6a21\u578b\uff0c\u901a\u8fc7\u53ef\u5904\u7406\u7684\u7ebf\u6027\u77e9\u9635\u4e0d\u7b49\u5f0f\u79bb\u7ebf\u5408\u6210\u7a33\u5b9a\u63a7\u5236\u5668\uff0c\u5e76\u6269\u5c55\u5230\u76f4\u63a5\u6570\u636e\u9a71\u52a8\u8bbe\u7f6e\uff0c\u5229\u7528Petersen\u5f15\u7406\u63a8\u5bfc\u6570\u636e\u4f9d\u8d56\u7684LMI\u3002", "result": "\u6570\u503c\u548c\u7269\u7406\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f4\u63a5\u4ece\u6709\u9650\u6570\u636e\u5b9e\u73b0\u4e86\u7a33\u5b9a\u6027\u3001\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\u7684\u4e25\u683c\u7aef\u5230\u7aef\u4fdd\u8bc1\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u975e\u7ebf\u6027\u7cfb\u7edf\u63d0\u4f9b\u4e86\u76f4\u63a5\u4ece\u6570\u636e\u51fa\u53d1\u7684\u7a33\u5b9a\u6027\u548c\u9c81\u68d2\u6027\u4fdd\u8bc1\uff0c\u65e0\u9700\u663e\u5f0f\u6a21\u578b\u8fa8\u8bc6\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.16010", "categories": ["q-fin.ST", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.16010", "abs": "https://arxiv.org/abs/2510.16010", "authors": ["Junlin Yang"], "title": "Institutional Differences, Crisis Shocks, and Volatility Structure: A By-Window EGARCH/TGARCH Analysis of ASEAN Stock Markets", "comment": "Volatility modeling; EGARCH; TGARCH; emerging markets; crisis\n  dynamics; institutional differences", "summary": "This study examines how institutional differences and external crises shape\nvolatility dynamics in emerging Asian stock markets. Using daily stock index\nreturns for Indonesia, Malaysia, and the Philippines from 2010 to 2024, we\nestimate EGARCH(1,1) and TGARCH(1,1) models in a by-window design. The sample\nis split into the 2013 Taper Tantrum, the 2020-2021 COVID-19 period, the\n2022-2023 rate-hike cycle, and tranquil phases. Prior work typically studies a\nsingle market or a static period; to our knowledge no study unifies\ninstitutional comparison with multi-crisis dynamics within one GARCH framework.\nWe address this gap and show that all three markets display strong volatility\npersistence and fat-tailed returns. During crises both persistence and\nasymmetry increase, while tail thickness rises, implying more frequent extreme\nmoves. After crises, parameters revert toward pre-shock levels. Cross-country\nevidence indicates a buffering role of institutional maturity: Malaysias\nstronger regulatory and information systems dampen amplification and speed\nrecovery, whereas the Philippines thinner market structure prolongs\ninstability. We conclude that crises amplify volatility structures, while\ninstitutional robustness governs recovery speed. The results provide policy\nguidance on transparency, macroprudential communication, and liquidity support\nto reduce volatility persistence during global shocks.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u5236\u5ea6\u5dee\u5f02\u548c\u5916\u90e8\u5371\u673a\u5982\u4f55\u5f71\u54cd\u65b0\u5174\u4e9a\u6d32\u80a1\u5e02\u7684\u6ce2\u52a8\u52a8\u6001\uff0c\u53d1\u73b0\u5371\u673a\u671f\u95f4\u6ce2\u52a8\u6301\u7eed\u6027\u548c\u975e\u5bf9\u79f0\u6027\u589e\u5f3a\uff0c\u5236\u5ea6\u6210\u719f\u5ea6\u5bf9\u7f13\u51b2\u5371\u673a\u5f71\u54cd\u548c\u4fc3\u8fdb\u6062\u590d\u6709\u91cd\u8981\u4f5c\u7528\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u901a\u5e38\u5173\u6ce8\u5355\u4e00\u5e02\u573a\u6216\u9759\u6001\u65f6\u671f\uff0c\u7f3a\u4e4f\u5728\u7edf\u4e00GARCH\u6846\u67b6\u4e0b\u7ed3\u5408\u5236\u5ea6\u6bd4\u8f83\u4e0e\u591a\u5371\u673a\u52a8\u6001\u7684\u7814\u7a76\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u5370\u5ea6\u5c3c\u897f\u4e9a\u3001\u9a6c\u6765\u897f\u4e9a\u548c\u83f2\u5f8b\u5bbe2010-2024\u5e74\u7684\u6bcf\u65e5\u80a1\u6307\u6536\u76ca\u7387\uff0c\u91c7\u7528EGARCH(1,1)\u548cTGARCH(1,1)\u6a21\u578b\u8fdb\u884c\u7a97\u53e3\u8bbe\u8ba1\u5206\u6790\uff0c\u5c06\u6837\u672c\u5206\u4e3a2013\u5e74\u7f29\u51cf\u6050\u614c\u30012020-2021\u5e74COVID-19\u65f6\u671f\u30012022-2023\u5e74\u52a0\u606f\u5468\u671f\u548c\u5e73\u9759\u9636\u6bb5\u3002", "result": "\u4e09\u4e2a\u5e02\u573a\u5747\u663e\u793a\u5f3a\u70c8\u7684\u6ce2\u52a8\u6301\u7eed\u6027\u548c\u539a\u5c3e\u6536\u76ca\u3002\u5371\u673a\u671f\u95f4\u6301\u7eed\u6027\u548c\u975e\u5bf9\u79f0\u6027\u589e\u52a0\uff0c\u5c3e\u90e8\u539a\u5ea6\u4e0a\u5347\uff0c\u610f\u5473\u7740\u66f4\u9891\u7e41\u7684\u6781\u7aef\u53d8\u52a8\u3002\u5371\u673a\u540e\u53c2\u6570\u56de\u5f52\u5230\u51b2\u51fb\u524d\u6c34\u5e73\u3002\u8de8\u56fd\u8bc1\u636e\u8868\u660e\u5236\u5ea6\u6210\u719f\u5ea6\u5177\u6709\u7f13\u51b2\u4f5c\u7528\uff1a\u9a6c\u6765\u897f\u4e9a\u66f4\u5f3a\u7684\u76d1\u7ba1\u548c\u4fe1\u606f\u7cfb\u7edf\u6291\u5236\u4e86\u653e\u5927\u6548\u5e94\u5e76\u52a0\u901f\u6062\u590d\uff0c\u800c\u83f2\u5f8b\u5bbe\u8f83\u8584\u7684\u5e02\u573a\u7ed3\u6784\u5ef6\u957f\u4e86\u4e0d\u7a33\u5b9a\u3002", "conclusion": "\u5371\u673a\u653e\u5927\u4e86\u6ce2\u52a8\u7ed3\u6784\uff0c\u800c\u5236\u5ea6\u7a33\u5065\u6027\u51b3\u5b9a\u4e86\u6062\u590d\u901f\u5ea6\u3002\u7ed3\u679c\u4e3a\u900f\u660e\u5ea6\u3001\u5b8f\u89c2\u5ba1\u614e\u6c9f\u901a\u548c\u6d41\u52a8\u6027\u652f\u6301\u63d0\u4f9b\u4e86\u653f\u7b56\u6307\u5bfc\uff0c\u4ee5\u51cf\u5c11\u5168\u7403\u51b2\u51fb\u671f\u95f4\u7684\u6ce2\u52a8\u6301\u7eed\u6027\u3002"}}
{"id": "2510.16657", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16657", "abs": "https://arxiv.org/abs/2510.16657", "authors": ["Bingji Yi", "Qiyuan Liu", "Yuwei Cheng", "Haifeng Xu"], "title": "Escaping Model Collapse via Synthetic Data Verification: Near-term Improvements and Long-term Convergence", "comment": "26 pages, 6 figures", "summary": "Synthetic data has been increasingly used to train frontier generative\nmodels. However, recent study raises key concerns that iteratively retraining a\ngenerative model on its self-generated synthetic data may keep deteriorating\nmodel performance, a phenomenon often coined model collapse. In this paper, we\ninvestigate ways to modify this synthetic retraining process to avoid model\ncollapse, and even possibly help reverse the trend from collapse to\nimprovement. Our key finding is that by injecting information through an\nexternal synthetic data verifier, whether a human or a better model, synthetic\nretraining will not cause model collapse. To develop principled understandings\nof the above insight, we situate our analysis in the foundational linear\nregression setting, showing that iterative retraining with verified synthetic\ndata can yield near-term improvements but ultimately drives the parameter\nestimate to the verifier's \"knowledge center\" in the long run. Our theory hence\npredicts that, unless the verifier is perfectly reliable, the early gains will\nplateau and may even reverse. Indeed, these theoretical insights are further\nconfirmed by our experiments on both linear regression as well as Variational\nAutoencoders (VAEs) trained on MNIST data.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u901a\u8fc7\u5f15\u5165\u5916\u90e8\u5408\u6210\u6570\u636e\u9a8c\u8bc1\u5668\u53ef\u4ee5\u907f\u514d\u6a21\u578b\u5d29\u6e83\uff0c\u751a\u81f3\u53ef\u80fd\u9006\u8f6c\u6027\u80fd\u4e0b\u964d\u8d8b\u52bf\u3002\u5728\u5408\u6210\u6570\u636e\u8fed\u4ee3\u8bad\u7ec3\u4e2d\uff0c\u9a8c\u8bc1\u5668\u7684\u4ecb\u5165\u80fd\u786e\u4fdd\u6a21\u578b\u53c2\u6570\u6536\u655b\u5230\u9a8c\u8bc1\u5668\u7684\u77e5\u8bc6\u4e2d\u5fc3\u3002", "motivation": "\u9488\u5bf9\u5408\u6210\u6570\u636e\u8fed\u4ee3\u8bad\u7ec3\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u6301\u7eed\u4e0b\u964d\u7684\u6a21\u578b\u5d29\u6e83\u73b0\u8c61\uff0c\u63a2\u7d22\u907f\u514d\u5d29\u6e83\u5e76\u5b9e\u73b0\u6027\u80fd\u6539\u8fdb\u7684\u65b9\u6cd5\u3002", "method": "\u5728\u57fa\u7840\u7ebf\u6027\u56de\u5f52\u8bbe\u7f6e\u4e2d\u5206\u6790\u8fed\u4ee3\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u5f15\u5165\u5916\u90e8\u5408\u6210\u6570\u636e\u9a8c\u8bc1\u5668\uff08\u4eba\u7c7b\u6216\u66f4\u597d\u6a21\u578b\uff09\u6765\u9a8c\u8bc1\u6570\u636e\u8d28\u91cf\uff0c\u5e76\u5728MNIST\u6570\u636e\u4e0a\u5bf9\u53d8\u5206\u81ea\u7f16\u7801\u5668\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u5747\u8868\u660e\uff0c\u4f7f\u7528\u9a8c\u8bc1\u5668\u9a8c\u8bc1\u7684\u5408\u6210\u6570\u636e\u8fdb\u884c\u8fed\u4ee3\u8bad\u7ec3\u53ef\u4ee5\u907f\u514d\u6a21\u578b\u5d29\u6e83\uff0c\u521d\u671f\u80fd\u5e26\u6765\u6027\u80fd\u63d0\u5347\uff0c\u4f46\u6700\u7ec8\u4f1a\u6536\u655b\u5230\u9a8c\u8bc1\u5668\u7684\u77e5\u8bc6\u4e2d\u5fc3\u3002", "conclusion": "\u5408\u6210\u6570\u636e\u8fed\u4ee3\u8bad\u7ec3\u9700\u8981\u5916\u90e8\u9a8c\u8bc1\u673a\u5236\u6765\u907f\u514d\u6a21\u578b\u5d29\u6e83\uff0c\u9a8c\u8bc1\u5668\u7684\u53ef\u9760\u6027\u51b3\u5b9a\u4e86\u6700\u7ec8\u6027\u80fd\u8868\u73b0\uff0c\u5b8c\u7f8e\u9a8c\u8bc1\u5668\u624d\u80fd\u786e\u4fdd\u6301\u7eed\u6539\u8fdb\u3002"}}
{"id": "2510.16050", "categories": ["cs.CY", "97B40"], "pdf": "https://arxiv.org/pdf/2510.16050", "abs": "https://arxiv.org/abs/2510.16050", "authors": ["Abrar Mahbub", "Humira Saria", "Md. Foysal Hossain", "Nafees Mansoor"], "title": "A Framework For Decentralized Micro-credential Verification Towards Higher Qualifications", "comment": "This work represents an early exploratory version of a project later\n  expanded and published in a different form. It focuses on a different\n  technology and experimental approach than the final publication", "summary": "Student retention is one of the rising problems seen in educational\ninstitutions. With the rising cost of education and issues in the education\nsector, such as curriculum relevance, student engagement, and rapidly changing\ntechnological advancements, ensuring the relevance of academic programs in a\nfast-evolving job market has created a significant concern for educational\ninstitutions. With the intent to adapt to such challenges, educational\ninstitutions are dealing with alternative solutions for education, in which\nmicro-credentials are at the very center of this, which are short-term academic\nprograms or standalone courses. However, one of the challenges of\nmicro-credentials is a lack of credit transfer among institutions. With the\nlack of standardization of assessments among educational institutions, it is\ndifficult to transfer micro-credentials to larger qualifications. Regarding\nsuch challenges, micro-credentials with blockchain technology can bring\nsignificant benefits. Blockchain technology offers a decentralized and\nimmutable platform for securely storing and verifying credentials. This paper\npresents a prototype model for micro-credential verification. With the policies\ndecided by the educational institution, the learner provides a micro-credential\ncertificate to the system. Upon validation of the certificate by the verifying\nbody, the educational institution will review the assessment criteria and\nprovide exemptions based on the provided criteria. The prototype uses the\nHyper-ledger Fabric platform and utilizes off-chain technology, which acts as a\nmiddle-man storage platform. With the combination of off-chain and on-chain\ntechnologies, congestion on the blockchain is reduced, and transaction speed is\nimproved. In summary, this research proposes a prototype for secure\nmicro-credential verification and a more efficient course exemption process.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u533a\u5757\u94fe\u6280\u672f\u7684\u5fae\u8bc1\u4e66\u9a8c\u8bc1\u539f\u578b\u7cfb\u7edf\uff0c\u65e8\u5728\u89e3\u51b3\u6559\u80b2\u673a\u6784\u95f4\u5fae\u8bc1\u4e66\u5b66\u5206\u8f6c\u79fb\u7684\u6807\u51c6\u5316\u95ee\u9898\u3002", "motivation": "\u6559\u80b2\u673a\u6784\u9762\u4e34\u5b66\u751f\u4fdd\u7559\u7387\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u5fae\u8bc1\u4e66\u4f5c\u4e3a\u77ed\u671f\u5b66\u672f\u9879\u76ee\u7f3a\u4e4f\u6807\u51c6\u5316\u8bc4\u4f30\u548c\u5b66\u5206\u8f6c\u79fb\u673a\u5236\uff0c\u963b\u788d\u4e86\u5176\u5411\u66f4\u5927\u8d44\u683c\u8ba4\u8bc1\u7684\u8f6c\u5316\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eHyper-ledger Fabric\u5e73\u53f0\u7684\u539f\u578b\u6a21\u578b\uff0c\u7ed3\u5408\u94fe\u4e0a\u548c\u94fe\u4e0b\u6280\u672f\uff0c\u901a\u8fc7\u9a8c\u8bc1\u673a\u6784\u9a8c\u8bc1\u5fae\u8bc1\u4e66\u8bc1\u4e66\uff0c\u6559\u80b2\u673a\u6784\u6839\u636e\u8bc4\u4f30\u6807\u51c6\u63d0\u4f9b\u8bfe\u7a0b\u8c41\u514d\u3002", "result": "\u539f\u578b\u7cfb\u7edf\u901a\u8fc7\u94fe\u4e0b\u6280\u672f\u4f5c\u4e3a\u4e2d\u95f4\u5b58\u50a8\u5e73\u53f0\uff0c\u51cf\u5c11\u4e86\u533a\u5757\u94fe\u62e5\u5835\uff0c\u63d0\u9ad8\u4e86\u4ea4\u6613\u901f\u5ea6\uff0c\u5b9e\u73b0\u4e86\u5b89\u5168\u7684\u5fae\u8bc1\u4e66\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5fae\u8bc1\u4e66\u7684\u5b89\u5168\u9a8c\u8bc1\u548c\u9ad8\u6548\u8bfe\u7a0b\u8c41\u514d\u6d41\u7a0b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u884c\u7684\u6280\u672f\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u4fc3\u8fdb\u6559\u80b2\u673a\u6784\u95f4\u7684\u5b66\u5206\u4e92\u8ba4\u3002"}}
{"id": "2510.16001", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16001", "abs": "https://arxiv.org/abs/2510.16001", "authors": ["Ruolan Cheng", "Yong Deng", "Enrique Herrera-Viedma"], "title": "A Non-overlap-based Conflict Measure for Random Permutation Sets", "comment": null, "summary": "Random permutation set (RPS) is a new formalism for reasoning with\nuncertainty involving order information. Measuring the conflict between two\npieces of evidence represented by permutation mass functions remains an urgent\nresearch topic in order-structured uncertain information fusion. In this paper,\na detailed analysis of conflicts in RPS is carried out from two different\nperspectives: random finite set (RFS) and Dempster-Shafer theory (DST).\nStarting from the observation of permutations, we first define an inconsistency\nmeasure between permutations inspired by the rank-biased overlap(RBO) measure\nand further propose a non-overlap-based conflict measure method for RPSs. This\npaper regards RPS theory (RPST) as an extension of DST. The order information\nnewly added in focal sets indicates qualitative propensity, characterized by\ntop-ranked elements occupying a more critical position. Some numerical examples\nare used to demonstrate the behavior and properties of the proposed conflict\nmeasure. The proposed method not only has the natural top-weightedness property\nand can effectively measure the conflict between RPSs from the DST view but\nalso provides decision-makers with a flexible selection of weights, parameters,\nand truncated depths.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u968f\u673a\u7f6e\u6362\u96c6(RPS)\u7684\u51b2\u7a81\u5ea6\u91cf\u65b9\u6cd5\uff0c\u4ece\u968f\u673a\u6709\u9650\u96c6(RFS)\u548cDempster-Shafer\u7406\u8bba(DST)\u4e24\u4e2a\u89d2\u5ea6\u5206\u6790\u7f6e\u6362\u95f4\u7684\u51b2\u7a81\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u79e9\u504f\u91cd\u53e0(RBO)\u7684\u4e0d\u4e00\u81f4\u6027\u5ea6\u91cf\u3002", "motivation": "\u5728\u6d89\u53ca\u987a\u5e8f\u4fe1\u606f\u7684\u4e0d\u786e\u5b9a\u6027\u63a8\u7406\u4e2d\uff0c\u5982\u4f55\u5ea6\u91cf\u7531\u7f6e\u6362\u8d28\u91cf\u51fd\u6570\u8868\u793a\u7684\u4e24\u4e2a\u8bc1\u636e\u4e4b\u95f4\u7684\u51b2\u7a81\u662f\u4e00\u4e2a\u4e9f\u5f85\u89e3\u51b3\u7684\u7814\u7a76\u8bfe\u9898\u3002", "method": "\u4ece\u7f6e\u6362\u7684\u89c2\u5bdf\u51fa\u53d1\uff0c\u9996\u5148\u57fa\u4e8e\u79e9\u504f\u91cd\u53e0(RBO)\u5ea6\u91cf\u5b9a\u4e49\u7f6e\u6362\u95f4\u7684\u4e0d\u4e00\u81f4\u6027\u5ea6\u91cf\uff0c\u7136\u540e\u63d0\u51fa\u57fa\u4e8e\u975e\u91cd\u53e0\u7684RPS\u51b2\u7a81\u5ea6\u91cf\u65b9\u6cd5\uff0c\u5c06RPS\u7406\u8bba\u89c6\u4e3aDST\u7684\u6269\u5c55\u3002", "result": "\u901a\u8fc7\u6570\u503c\u793a\u4f8b\u5c55\u793a\u4e86\u6240\u63d0\u51b2\u7a81\u5ea6\u91cf\u7684\u884c\u4e3a\u548c\u6027\u8d28\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u81ea\u7136\u7684\u9876\u90e8\u52a0\u6743\u7279\u6027\uff0c\u5e76\u80fd\u4eceDST\u89c6\u89d2\u6709\u6548\u5ea6\u91cfRPS\u95f4\u7684\u51b2\u7a81\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u4e0d\u4ec5\u5177\u6709\u81ea\u7136\u7684\u9876\u90e8\u52a0\u6743\u7279\u6027\uff0c\u80fd\u591f\u4eceDST\u89c6\u89d2\u6709\u6548\u5ea6\u91cfRPS\u95f4\u7684\u51b2\u7a81\uff0c\u8fd8\u4e3a\u51b3\u7b56\u8005\u63d0\u4f9b\u4e86\u6743\u91cd\u3001\u53c2\u6570\u548c\u622a\u65ad\u6df1\u5ea6\u7684\u7075\u6d3b\u9009\u62e9\u3002"}}
{"id": "2510.16599", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2510.16599", "abs": "https://arxiv.org/abs/2510.16599", "authors": ["Jean-Paul D\u00e9camps", "Fabien Gensbittel", "Thomas Mariotti", "St\u00e9phane Villeneuve"], "title": "A class of singular control problems with tipping points", "comment": null, "summary": "Tipping points define situations where a system experiences sudden and\nirreversible changes and are generally associated with a random level of the\nsystem below which the changes materialize. In this paper, we study a singular\nstochastic control problem in which the performance criterion depends on the\nhitting time of a random level that is not a stopping time for the reference\nfiltration. We establish a connection between the value of the problem and the\nvalue of a singular control problem involving a diffusion and its running\nminimum. We prove a verification theorem and apply our results to explicitly\nsolve a resource extraction problem where the random evolution of the resource\nchanges when it crosses a tipping point.", "AI": {"tldr": "\u7814\u7a76\u6d89\u53ca\u968f\u673a\u4e34\u754c\u70b9\u7684\u5947\u5f02\u968f\u673a\u63a7\u5236\u95ee\u9898\uff0c\u5efa\u7acb\u4e86\u4e0e\u6269\u6563\u8fc7\u7a0b\u53ca\u5176\u8fd0\u884c\u6700\u5c0f\u503c\u63a7\u5236\u95ee\u9898\u7684\u8054\u7cfb\uff0c\u5e76\u5e94\u7528\u4e8e\u8d44\u6e90\u5f00\u91c7\u95ee\u9898", "motivation": "\u7814\u7a76\u4e34\u754c\u70b9\u7cfb\u7edf\u4e2d\u5f53\u7cfb\u7edf\u7ecf\u5386\u7a81\u7136\u4e14\u4e0d\u53ef\u9006\u53d8\u5316\u65f6\u7684\u968f\u673a\u63a7\u5236\u95ee\u9898\uff0c\u7279\u522b\u662f\u5f53\u6027\u80fd\u6807\u51c6\u4f9d\u8d56\u4e8e\u975e\u505c\u6b62\u65f6\u95f4\u7684\u968f\u673a\u6c34\u5e73\u547d\u4e2d\u65f6\u95f4\u7684\u60c5\u51b5", "method": "\u5efa\u7acb\u5947\u5f02\u968f\u673a\u63a7\u5236\u95ee\u9898\u4e0e\u6d89\u53ca\u6269\u6563\u53ca\u5176\u8fd0\u884c\u6700\u5c0f\u503c\u7684\u5947\u5f02\u63a7\u5236\u95ee\u9898\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u8bc1\u660e\u9a8c\u8bc1\u5b9a\u7406", "result": "\u6210\u529f\u5efa\u7acb\u4e86\u4e24\u79cd\u63a7\u5236\u95ee\u9898\u4ef7\u503c\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u5e76\u5e94\u7528\u8be5\u65b9\u6cd5\u663e\u5f0f\u89e3\u51b3\u4e86\u8d44\u6e90\u5f00\u91c7\u95ee\u9898", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u6d89\u53ca\u4e34\u754c\u70b9\u7684\u968f\u673a\u63a7\u5236\u95ee\u9898\uff0c\u4e3a\u8fd9\u7c7b\u590d\u6742\u7cfb\u7edf\u7684\u4f18\u5316\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6"}}
{"id": "2510.15960", "categories": ["cs.LG", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2510.15960", "abs": "https://arxiv.org/abs/2510.15960", "authors": ["Sana Kordoghli", "Abdelhakim Settar", "Oumayma Belaati", "Mohammad Alkhatib"], "title": "Hydrogen production from blended waste biomass: pyrolysis, thermodynamic-kinetic analysis and AI-based modelling", "comment": "41 pages, 21 figures", "summary": "This work contributes to advancing sustainable energy and waste management\nstrategies by investigating the thermochemical conversion of food-based biomass\nthrough pyrolysis, highlighting the role of artificial intelligence (AI) in\nenhancing process modelling accuracy and optimization efficiency. The main\nobjective is to explore the potential of underutilized biomass resources, such\nas spent coffee grounds (SCG) and date seeds (DS), for sustainable hydrogen\nproduction. Specifically, it aims to optimize the pyrolysis process while\nevaluating the performance of these resources both individually and as blends.\nProximate, ultimate, fibre, TGA/DTG, kinetic, thermodynamic, and Py-Micro GC\nanalyses were conducted for pure DS, SCG, and blends (75% DS - 25% SCG, 50% DS\n- 50% SCG, 25% DS - 75% SCG). Blend 3 offered superior hydrogen yield potential\nbut had the highest activation energy (Ea: 313.24 kJ/mol), while Blend 1\nexhibited the best activation energy value (Ea: 161.75 kJ/mol). The kinetic\nmodelling based on isoconversional methods (KAS, FWO, Friedman) identified KAS\nas the most accurate. These approaches provide a detailed understanding of the\npyrolysis process, with particular emphasis on the integration of artificial\nintelligence. An LSTM model trained with lignocellulosic data predicted TGA\ncurves with exceptional accuracy (R^2: 0.9996-0.9998).", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u70ed\u89e3\u6280\u672f\u5c06\u98df\u7269\u57fa\u751f\u7269\u8d28\u8f6c\u5316\u4e3a\u53ef\u6301\u7eed\u6c22\u80fd\uff0c\u5229\u7528\u4eba\u5de5\u667a\u80fd\u4f18\u5316\u8fc7\u7a0b\u5efa\u6a21\uff0c\u91cd\u70b9\u7814\u7a76\u4e86\u5496\u5561\u6e23\u548c\u67a3\u6838\u7b49\u672a\u5145\u5206\u5229\u7528\u751f\u7269\u8d28\u8d44\u6e90\u7684\u6f5c\u529b\u3002", "motivation": "\u63a8\u52a8\u53ef\u6301\u7eed\u80fd\u6e90\u548c\u5e9f\u7269\u7ba1\u7406\u7b56\u7565\uff0c\u63a2\u7d22\u672a\u5145\u5206\u5229\u7528\u751f\u7269\u8d28\u8d44\u6e90\uff08\u5982\u5496\u5561\u6e23\u548c\u67a3\u6838\uff09\u5728\u53ef\u6301\u7eed\u6c22\u751f\u4ea7\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u5229\u7528\u4eba\u5de5\u667a\u80fd\u63d0\u9ad8\u8fc7\u7a0b\u5efa\u6a21\u7cbe\u5ea6\u548c\u4f18\u5316\u6548\u7387\u3002", "method": "\u5bf9\u7eaf\u67a3\u6838\u3001\u5496\u5561\u6e23\u53ca\u5176\u6df7\u5408\u7269\u8fdb\u884c\u8fd1\u4f3c\u5206\u6790\u3001\u5143\u7d20\u5206\u6790\u3001\u7ea4\u7ef4\u5206\u6790\u3001TGA/DTG\u5206\u6790\u3001\u52a8\u529b\u5b66\u5206\u6790\u3001\u70ed\u529b\u5b66\u5206\u6790\u548cPy-Micro GC\u5206\u6790\uff1b\u91c7\u7528\u7b49\u8f6c\u5316\u7387\u65b9\u6cd5\uff08KAS\u3001FWO\u3001Friedman\uff09\u8fdb\u884c\u52a8\u529b\u5b66\u5efa\u6a21\uff1b\u8bad\u7ec3LSTM\u6a21\u578b\u9884\u6d4bTGA\u66f2\u7ebf\u3002", "result": "\u6df7\u5408\u72693\u663e\u793a\u51fa\u6700\u4f73\u7684\u6c22\u6c14\u4ea7\u91cf\u6f5c\u529b\u4f46\u6d3b\u5316\u80fd\u6700\u9ad8\uff08313.24 kJ/mol\uff09\uff0c\u6df7\u5408\u72691\u5177\u6709\u6700\u4f73\u6d3b\u5316\u80fd\u503c\uff08161.75 kJ/mol\uff09\uff1bKAS\u65b9\u6cd5\u88ab\u786e\u5b9a\u4e3a\u6700\u51c6\u786e\u7684\u52a8\u529b\u5b66\u6a21\u578b\uff1bLSTM\u6a21\u578b\u9884\u6d4bTGA\u66f2\u7ebf\u51c6\u786e\u5ea6\u6781\u9ad8\uff08R\u00b2: 0.9996-0.9998\uff09\u3002", "conclusion": "\u4eba\u5de5\u667a\u80fd\u5728\u70ed\u89e3\u8fc7\u7a0b\u5efa\u6a21\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\uff0cLSTM\u6a21\u578b\u80fd\u591f\u9ad8\u7cbe\u5ea6\u9884\u6d4b\u70ed\u89e3\u884c\u4e3a\uff0c\u4e3a\u53ef\u6301\u7eed\u6c22\u751f\u4ea7\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u4f18\u5316\u5de5\u5177\uff0c\u5c55\u793a\u4e86\u672a\u5145\u5206\u5229\u7528\u751f\u7269\u8d28\u8d44\u6e90\u5728\u80fd\u6e90\u8f6c\u5316\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2510.16227", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16227", "abs": "https://arxiv.org/abs/2510.16227", "authors": ["Jennifer Hu", "Ethan Gotlieb Wilcox", "Siyuan Song", "Kyle Mahowald", "Roger P. Levy"], "title": "What Can String Probability Tell Us About Grammaticality?", "comment": null, "summary": "What have language models (LMs) learned about grammar? This question remains\nhotly debated, with major ramifications for linguistic theory. However, since\nprobability and grammaticality are distinct notions in linguistics, it is not\nobvious what string probabilities can reveal about an LM's underlying\ngrammatical knowledge. We present a theoretical analysis of the relationship\nbetween grammar, meaning, and string probability, based on simple assumptions\nabout the generative process of corpus data. Our framework makes three\npredictions, which we validate empirically using 280K sentence pairs in English\nand Chinese: (1) correlation between the probability of strings within minimal\npairs, i.e., string pairs with minimal semantic differences; (2) correlation\nbetween models' and humans' deltas within minimal pairs; and (3) poor\nseparation in probability space between unpaired grammatical and ungrammatical\nstrings. Our analyses give theoretical grounding for using probability to learn\nabout LMs' structural knowledge, and suggest directions for future work in LM\ngrammatical evaluation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u9a8c\u8bc1\uff0c\u63a2\u8ba8\u4e86\u8bed\u8a00\u6a21\u578b\u5bf9\u8bed\u6cd5\u7684\u5b66\u4e60\u60c5\u51b5\uff0c\u5efa\u7acb\u4e86\u8bed\u6cd5\u3001\u610f\u4e49\u548c\u5b57\u7b26\u4e32\u6982\u7387\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u63d0\u51fa\u4e86\u4e09\u4e2a\u53ef\u9a8c\u8bc1\u7684\u9884\u6d4b\u3002", "motivation": "\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u662f\u5426\u771f\u6b63\u5b66\u4e60\u4e86\u8bed\u6cd5\u77e5\u8bc6\uff0c\u4ee5\u53ca\u5982\u4f55\u901a\u8fc7\u5b57\u7b26\u4e32\u6982\u7387\u6765\u63ed\u793a\u5176\u6f5c\u5728\u7684\u8bed\u6cd5\u77e5\u8bc6\uff0c\u8fd9\u5bf9\u8bed\u8a00\u5b66\u7406\u8bba\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u57fa\u4e8e\u8bed\u6599\u5e93\u6570\u636e\u751f\u6210\u8fc7\u7a0b\u7684\u7b80\u5355\u5047\u8bbe\uff0c\u8fdb\u884c\u7406\u8bba\u5206\u6790\uff0c\u5e76\u4f7f\u752828\u4e07\u53e5\u82f1\u8bed\u548c\u4e2d\u6587\u53e5\u5b50\u5bf9\u8fdb\u884c\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "result": "\u9a8c\u8bc1\u4e86\u4e09\u4e2a\u9884\u6d4b\uff1a(1)\u6700\u5c0f\u5bf9\u5b57\u7b26\u4e32\u6982\u7387\u7684\u76f8\u5173\u6027\uff1b(2)\u6a21\u578b\u4e0e\u4eba\u7c7b\u5728\u6700\u5c0f\u5bf9\u4e2d\u5dee\u5f02\u7684\u76f8\u5173\u6027\uff1b(3)\u8bed\u6cd5\u548c\u4e0d\u5408\u8bed\u6cd5\u5b57\u7b26\u4e32\u5728\u6982\u7387\u7a7a\u95f4\u4e2d\u5206\u79bb\u5ea6\u5dee\u3002", "conclusion": "\u4e3a\u4f7f\u7528\u6982\u7387\u6765\u4e86\u89e3\u8bed\u8a00\u6a21\u578b\u7684\u7ed3\u6784\u77e5\u8bc6\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u4e3a\u672a\u6765\u8bed\u8a00\u6a21\u578b\u8bed\u6cd5\u8bc4\u4f30\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2510.16534", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.16534", "abs": "https://arxiv.org/abs/2510.16534", "authors": ["Christoph Kaufmann", "Georg Pangalos", "Gerwald Lichtenberg", "Oriol Gomis-Bellmunt"], "title": "Small-Signal Stability Analysis of Power Systems by Implicit Multilinear Models", "comment": null, "summary": "This paper proposes a new approach to perform small-signal stability analysis\nbased on linearization of implicit multilinear models. Multilinear models\ndescribe the system dynamics by multilinear functions of state, input, and\nalgebraic variables. Using suitable transformations of variables, they can also\nrepresent trigonometric functions, which often occur in power systems modeling.\nThis allows tensor representations of grid-following and grid-forming power\nconverters. This paper introduces small-signal stability analysis of\nequilibrium points based on implicit multilinear models using generalized\neigenvalues. The generalized eigenvalues are computed from linear descriptor\nmodels of the linearized implicit multilinear model. The proposed approach is\ntested using a 3-bus network example, first by comparing time-domain\nsimulations of the implicit multilinear model with those of the nonlinear\nmodel, and second by comparing the generalized eigenvalues with those of the\nlinearized nonlinear model. The results show that the decomposed tensor\nrepresentation of the implicit multilinear model allows for a faster\nlinearization compared to conventional methods in MATLAB Simulink.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u9690\u5f0f\u591a\u7ebf\u6027\u6a21\u578b\u7ebf\u6027\u5316\u7684\u5c0f\u4fe1\u53f7\u7a33\u5b9a\u6027\u5206\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f20\u91cf\u8868\u793a\u7535\u7f51\u6362\u6d41\u5668\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u80fd\u66f4\u5feb\u8fdb\u884c\u7ebf\u6027\u5316\u3002", "motivation": "\u4f20\u7edf\u5c0f\u4fe1\u53f7\u7a33\u5b9a\u6027\u5206\u6790\u65b9\u6cd5\u5728\u5904\u7406\u5305\u542b\u4e09\u89d2\u51fd\u6570\u7b49\u590d\u6742\u975e\u7ebf\u6027\u5173\u7cfb\u7684\u7535\u529b\u7cfb\u7edf\u6a21\u578b\u65f6\u6548\u7387\u8f83\u4f4e\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u7ebf\u6027\u5316\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u9690\u5f0f\u591a\u7ebf\u6027\u6a21\u578b\u63cf\u8ff0\u7cfb\u7edf\u52a8\u6001\uff0c\u901a\u8fc7\u53d8\u91cf\u53d8\u6362\u8868\u793a\u4e09\u89d2\u51fd\u6570\uff0c\u57fa\u4e8e\u5e7f\u4e49\u7279\u5f81\u503c\u8fdb\u884c\u7a33\u5b9a\u6027\u5206\u6790\uff0c\u5e76\u4e0e\u975e\u7ebf\u6027\u6a21\u578b\u8fdb\u884c\u65f6\u57df\u4eff\u771f\u5bf9\u6bd4\u3002", "result": "\u57283\u8282\u70b9\u7f51\u7edc\u6d4b\u8bd5\u4e2d\uff0c\u9690\u5f0f\u591a\u7ebf\u6027\u6a21\u578b\u4e0e\u975e\u7ebf\u6027\u6a21\u578b\u7684\u65f6\u57df\u4eff\u771f\u7ed3\u679c\u4e00\u81f4\uff0c\u4e14\u5f20\u91cf\u5206\u89e3\u8868\u793a\u76f8\u6bd4MATLAB Simulink\u4f20\u7edf\u65b9\u6cd5\u80fd\u66f4\u5feb\u5b8c\u6210\u7ebf\u6027\u5316\u3002", "conclusion": "\u9690\u5f0f\u591a\u7ebf\u6027\u6a21\u578b\u4e3a\u7535\u529b\u7cfb\u7edf\u5c0f\u4fe1\u53f7\u7a33\u5b9a\u6027\u5206\u6790\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u7ebf\u6027\u5316\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5305\u542b\u590d\u6742\u975e\u7ebf\u6027\u5173\u7cfb\u7684\u7cfb\u7edf\u5efa\u6a21\u3002"}}
{"id": "2510.16675", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16675", "abs": "https://arxiv.org/abs/2510.16675", "authors": ["Daniel Augusto de Souza", "Yuchen Zhu", "Harry Jake Cunningham", "Yuri Saporito", "Diego Mesquita", "Marc Peter Deisenroth"], "title": "Infinite Neural Operators: Gaussian processes on functions", "comment": "Accepted at the Conference on Neural Information Processing Systems\n  (NeurIPS) 2025", "summary": "A variety of infinitely wide neural architectures (e.g., dense NNs, CNNs, and\ntransformers) induce Gaussian process (GP) priors over their outputs. These\nrelationships provide both an accurate characterization of the prior predictive\ndistribution and enable the use of GP machinery to improve the uncertainty\nquantification of deep neural networks. In this work, we extend this connection\nto neural operators (NOs), a class of models designed to learn mappings between\nfunction spaces. Specifically, we show conditions for when arbitrary-depth NOs\nwith Gaussian-distributed convolution kernels converge to function-valued GPs.\nBased on this result, we show how to compute the covariance functions of these\nNO-GPs for two NO parametrizations, including the popular Fourier neural\noperator (FNO). With this, we compute the posteriors of these GPs in regression\nscenarios, including PDE solution operators. This work is an important step\ntowards uncovering the inductive biases of current FNO architectures and opens\na path to incorporate novel inductive biases for use in kernel-based operator\nlearning methods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c06\u795e\u7ecf\u7f51\u7edc\u4e0e\u9ad8\u65af\u8fc7\u7a0b\u7684\u8fde\u63a5\u6269\u5c55\u5230\u795e\u7ecf\u7b97\u5b50\uff0c\u8bc1\u660e\u4e86\u65e0\u9650\u5bbd\u795e\u7ecf\u7b97\u5b50\u6536\u655b\u5230\u51fd\u6570\u503c\u9ad8\u65af\u8fc7\u7a0b\u7684\u6761\u4ef6\uff0c\u5e76\u8ba1\u7b97\u4e86NO-GP\u7684\u534f\u65b9\u5dee\u51fd\u6570\u548c\u540e\u9a8c\u5206\u5e03\u3002", "motivation": "\u6269\u5c55\u795e\u7ecf\u7f51\u7edc\u4e0e\u9ad8\u65af\u8fc7\u7a0b\u7684\u8fde\u63a5\u5173\u7cfb\uff0c\u63ed\u793a\u5f53\u524dFNO\u67b6\u6784\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u4e3a\u57fa\u4e8e\u6838\u7684\u7b97\u5b50\u5b66\u4e60\u65b9\u6cd5\u5f15\u5165\u65b0\u7684\u5f52\u7eb3\u504f\u7f6e\u3002", "method": "\u5206\u6790\u65e0\u9650\u5bbd\u795e\u7ecf\u7b97\u5b50\u6536\u655b\u5230\u51fd\u6570\u503c\u9ad8\u65af\u8fc7\u7a0b\u7684\u6761\u4ef6\uff0c\u8ba1\u7b97NO-GP\u7684\u534f\u65b9\u5dee\u51fd\u6570\uff08\u5305\u62ec\u5085\u91cc\u53f6\u795e\u7ecf\u7b97\u5b50\uff09\uff0c\u5e76\u5728\u56de\u5f52\u573a\u666f\u4e2d\u8ba1\u7b97\u8fd9\u4e9bGP\u7684\u540e\u9a8c\u5206\u5e03\u3002", "result": "\u5efa\u7acb\u4e86\u795e\u7ecf\u7b97\u5b50\u4e0e\u9ad8\u65af\u8fc7\u7a0b\u7684\u6570\u5b66\u8054\u7cfb\uff0c\u63d0\u4f9b\u4e86NO-GP\u534f\u65b9\u5dee\u51fd\u6570\u7684\u8ba1\u7b97\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u5728PDE\u89e3\u7b97\u5b50\u7b49\u56de\u5f52\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u7406\u89e3FNO\u67b6\u6784\u7684\u5f52\u7eb3\u504f\u7f6e\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\uff0c\u5e76\u4e3a\u57fa\u4e8e\u6838\u7684\u7b97\u5b50\u5b66\u4e60\u65b9\u6cd5\u5f00\u8f9f\u4e86\u65b0\u8def\u5f84\u3002"}}
{"id": "2510.16052", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2510.16052", "abs": "https://arxiv.org/abs/2510.16052", "authors": ["Alice Gao", "Victoria Sakhnini"], "title": "Reducing Procrastination on Programming Assignments via Optional Early Feedback", "comment": "8 pages. 3 Tables", "summary": "Academic procrastination is prevalent among undergraduate computer science\nstudents. Many studies have linked procrastination to poor academic performance\nand well-being. Procrastination is especially detrimental for advanced students\nwhen facing large, complex programming assignments in upper-year courses. We\ndesigned an intervention to combat academic procrastination on such programming\nassignments. The intervention consisted of early deadlines that were not worth\nmarks but provided additional automated feedback if students submitted their\nwork early. We evaluated the intervention by comparing the behaviour and\nperformance of students between a control group and an intervention group. Our\nresults showed that the intervention encouraged significantly more students to\nstart the assignments early. Although there was no significant difference in\nstudents' grades between the control and intervention groups, students within\nthe intervention group who used the intervention achieved significantly higher\ngrades than those who did not. Our results implied that starting early alone\ndid not improve students' grades. However, starting early and receiving\nadditional feedback enhanced the students' grades relative to those of the rest\nof the students. We also conducted semi-structured interviews to gain an\nunderstanding of students' perceptions of the intervention. The interviews\nrevealed that students benefited from the intervention in numerous ways,\nincluding improved academic performance, mental health, and development of soft\nskills. Students adopted the intervention to get more feedback, satisfy their\ncuriosity, or use their available time. The main reasons for not adopting the\nintervention include having other competing deadlines, the intervention not\nbeing worth any marks, and feeling confident about their work.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5e72\u9884\u63aa\u65bd\u6765\u5bf9\u6297\u8ba1\u7b97\u673a\u79d1\u5b66\u672c\u79d1\u751f\u7684\u5b66\u4e1a\u62d6\u5ef6\uff0c\u901a\u8fc7\u8bbe\u7f6e\u65e0\u5206\u6570\u7684\u65e9\u671f\u622a\u6b62\u65e5\u671f\u6765\u63d0\u4f9b\u989d\u5916\u81ea\u52a8\u5316\u53cd\u9988\uff0c\u7ed3\u679c\u663e\u793a\u5e72\u9884\u80fd\u663e\u8457\u4fc3\u4f7f\u66f4\u591a\u5b66\u751f\u5c3d\u65e9\u5f00\u59cb\u4f5c\u4e1a\uff0c\u5e76\u63d0\u9ad8\u4f7f\u7528\u8be5\u5e72\u9884\u5b66\u751f\u7684\u6210\u7ee9\u3002", "motivation": "\u5b66\u4e1a\u62d6\u5ef6\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u672c\u79d1\u751f\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u5c24\u5176\u5bf9\u9ad8\u5e74\u7ea7\u5b66\u751f\u9762\u5bf9\u5927\u578b\u590d\u6742\u7f16\u7a0b\u4f5c\u4e1a\u65f6\u5c24\u4e3a\u4e0d\u5229\uff0c\u8fd9\u4e0e\u5b66\u4e1a\u6210\u7ee9\u4e0d\u4f73\u548c\u5e78\u798f\u611f\u4e0b\u964d\u76f8\u5173\u3002", "method": "\u8bbe\u8ba1\u5e72\u9884\u63aa\u65bd\uff1a\u8bbe\u7f6e\u65e0\u5206\u6570\u7684\u65e9\u671f\u622a\u6b62\u65e5\u671f\uff0c\u63d0\u4f9b\u989d\u5916\u81ea\u52a8\u5316\u53cd\u9988\uff1b\u901a\u8fc7\u63a7\u5236\u7ec4\u548c\u5e72\u9884\u7ec4\u7684\u6bd4\u8f83\u8bc4\u4f30\u5e72\u9884\u6548\u679c\uff1b\u8fdb\u884c\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\u4e86\u89e3\u5b66\u751f\u5bf9\u5e72\u9884\u7684\u8ba4\u77e5\u3002", "result": "\u5e72\u9884\u663e\u8457\u4fc3\u4f7f\u66f4\u591a\u5b66\u751f\u5c3d\u65e9\u5f00\u59cb\u4f5c\u4e1a\uff1b\u5e72\u9884\u7ec4\u4e2d\u4f7f\u7528\u5e72\u9884\u7684\u5b66\u751f\u6210\u7ee9\u663e\u8457\u9ad8\u4e8e\u672a\u4f7f\u7528\u8005\uff1b\u8bbf\u8c08\u663e\u793a\u5b66\u751f\u5728\u5b66\u4e1a\u8868\u73b0\u3001\u5fc3\u7406\u5065\u5eb7\u548c\u8f6f\u6280\u80fd\u53d1\u5c55\u7b49\u65b9\u9762\u53d7\u76ca\u3002", "conclusion": "\u4ec5\u5c3d\u65e9\u5f00\u59cb\u4f5c\u4e1a\u4e0d\u80fd\u63d0\u9ad8\u6210\u7ee9\uff0c\u4f46\u5c3d\u65e9\u5f00\u59cb\u5e76\u63a5\u6536\u989d\u5916\u53cd\u9988\u80fd\u663e\u8457\u63d0\u5347\u5b66\u751f\u6210\u7ee9\uff1b\u5b66\u751f\u91c7\u7528\u5e72\u9884\u7684\u4e3b\u8981\u539f\u56e0\u662f\u83b7\u53d6\u66f4\u591a\u53cd\u9988\u3001\u6ee1\u8db3\u597d\u5947\u5fc3\u548c\u5229\u7528\u53ef\u7528\u65f6\u95f4\uff0c\u4e0d\u91c7\u7528\u7684\u4e3b\u8981\u539f\u56e0\u662f\u5176\u4ed6\u622a\u6b62\u65e5\u671f\u51b2\u7a81\u3001\u65e0\u5206\u6570\u5956\u52b1\u548c\u81ea\u4fe1\u3002"}}
{"id": "2510.16004", "categories": ["cs.AI", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2510.16004", "abs": "https://arxiv.org/abs/2510.16004", "authors": ["Andreas Radler", "Vincent Seyfried", "Stefan Pirker", "Johannes Brandstetter", "Thomas Lichtenegger"], "title": "PAINT: Parallel-in-time Neural Twins for Dynamical System Reconstruction", "comment": "22 pages, 16 figures", "summary": "Neural surrogates have shown great potential in simulating dynamical systems,\nwhile offering real-time capabilities. We envision Neural Twins as a\nprogression of neural surrogates, aiming to create digital replicas of real\nsystems. A neural twin consumes measurements at test time to update its state,\nthereby enabling context-specific decision-making. A critical property of\nneural twins is their ability to remain on-trajectory, i.e., to stay close to\nthe true system state over time. We introduce Parallel-in-time Neural Twins\n(PAINT), an architecture-agnostic family of methods for modeling dynamical\nsystems from measurements. PAINT trains a generative neural network to model\nthe distribution of states parallel over time. At test time, states are\npredicted from measurements in a sliding window fashion. Our theoretical\nanalysis shows that PAINT is on-trajectory, whereas autoregressive models\ngenerally are not. Empirically, we evaluate our method on a challenging\ntwo-dimensional turbulent fluid dynamics problem. The results demonstrate that\nPAINT stays on-trajectory and predicts system states from sparse measurements\nwith high fidelity. These findings underscore PAINT's potential for developing\nneural twins that stay on-trajectory, enabling more accurate state estimation\nand decision-making.", "AI": {"tldr": "PAINT\u662f\u4e00\u79cd\u5e76\u884c\u65f6\u95f4\u795e\u7ecf\u5b6a\u751f\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u6d4b\u91cf\u6570\u636e\u4e2d\u5efa\u6a21\u52a8\u6001\u7cfb\u7edf\uff0c\u786e\u4fdd\u7cfb\u7edf\u72b6\u6001\u59cb\u7ec8\u4fdd\u6301\u5728\u771f\u5b9e\u8f68\u8ff9\u4e0a\uff0c\u76f8\u6bd4\u81ea\u56de\u5f52\u6a21\u578b\u5177\u6709\u66f4\u597d\u7684\u8f68\u8ff9\u8ddf\u8e2a\u80fd\u529b\u3002", "motivation": "\u795e\u7ecf\u5b6a\u751f\u4f5c\u4e3a\u795e\u7ecf\u4ee3\u7406\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\uff0c\u65e8\u5728\u521b\u5efa\u771f\u5b9e\u7cfb\u7edf\u7684\u6570\u5b57\u526f\u672c\uff0c\u9700\u8981\u80fd\u591f\u5728\u6d4b\u8bd5\u65f6\u6839\u636e\u6d4b\u91cf\u66f4\u65b0\u72b6\u6001\uff0c\u5b9e\u73b0\u4e0a\u4e0b\u6587\u7279\u5b9a\u7684\u51b3\u7b56\u5236\u5b9a\u3002\u5173\u952e\u7279\u6027\u662f\u4fdd\u6301\u8f68\u8ff9\u8ddf\u8e2a\u80fd\u529b\u3002", "method": "PAINT\u8bad\u7ec3\u751f\u6210\u795e\u7ecf\u7f51\u7edc\u6765\u5e76\u884c\u5efa\u6a21\u65f6\u95f4\u4e0a\u7684\u72b6\u6001\u5206\u5e03\u3002\u5728\u6d4b\u8bd5\u65f6\uff0c\u901a\u8fc7\u6ed1\u52a8\u7a97\u53e3\u65b9\u5f0f\u4ece\u6d4b\u91cf\u6570\u636e\u9884\u6d4b\u72b6\u6001\u3002\u8be5\u65b9\u6cd5\u4e0e\u67b6\u6784\u65e0\u5173\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u52a8\u6001\u7cfb\u7edf\u5efa\u6a21\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u4e8c\u7ef4\u6e4d\u6d41\u6d41\u4f53\u52a8\u529b\u5b66\u95ee\u9898\u4e0a\uff0cPAINT\u80fd\u591f\u4fdd\u6301\u8f68\u8ff9\u8ddf\u8e2a\uff0c\u4ece\u7a00\u758f\u6d4b\u91cf\u4e2d\u9ad8\u4fdd\u771f\u5730\u9884\u6d4b\u7cfb\u7edf\u72b6\u6001\u3002\u7406\u8bba\u5206\u6790\u8868\u660ePAINT\u80fd\u591f\u4fdd\u6301\u5728\u8f68\u8ff9\u4e0a\uff0c\u800c\u81ea\u56de\u5f52\u6a21\u578b\u901a\u5e38\u4e0d\u80fd\u3002", "conclusion": "PAINT\u5177\u6709\u5f00\u53d1\u4fdd\u6301\u8f68\u8ff9\u8ddf\u8e2a\u7684\u795e\u7ecf\u5b6a\u751f\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u72b6\u6001\u4f30\u8ba1\u548c\u51b3\u7b56\u5236\u5b9a\uff0c\u4e3a\u52a8\u6001\u7cfb\u7edf\u5efa\u6a21\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.16650", "categories": ["math.OC", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.16650", "abs": "https://arxiv.org/abs/2510.16650", "authors": ["Dennis J. Marquis", "Blake Wilhelm", "Devaprakash Muniraj", "Mazen Farhood"], "title": "Adversarial Reinforcement Learning for Robust Control of Fixed-Wing Aircraft under Model Uncertainty", "comment": null, "summary": "This paper presents a reinforcement learning-based path-following controller\nfor a fixed-wing small uncrewed aircraft system (sUAS) that is robust to\nuncertainties in the aerodynamic model of the sUAS. The controller is trained\nusing the Robust Adversarial Reinforcement Learning framework, where an\nadversary perturbs the environment (aerodynamic model) to expose the agent\n(sUAS) to demanding scenarios. In our formulation, the adversary introduces\nrate-bounded perturbations to the aerodynamic model coefficients. We\ndemonstrate that adversarial training improves robustness compared to\ncontrollers trained using stochastic model uncertainty. The learned controller\nis also benchmarked against a switched uncertain initial condition controller.\nThe effectiveness of the approach is validated through high-fidelity\nsimulations using a realistic six-degree-of-freedom fixed-wing aircraft model,\nshowing accurate and robust path-following performance under a variety of\nuncertain aerodynamic conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u56fa\u5b9a\u7ffc\u5c0f\u578b\u65e0\u4eba\u673a\u8def\u5f84\u8ddf\u8e2a\u63a7\u5236\u5668\uff0c\u901a\u8fc7\u9c81\u68d2\u5bf9\u6297\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u8bad\u7ec3\uff0c\u80fd\u591f\u62b5\u6297\u6c14\u52a8\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u4f20\u7edf\u63a7\u5236\u5668\u5bf9\u6c14\u52a8\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u654f\u611f\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u5728\u5404\u79cd\u4e0d\u786e\u5b9a\u6c14\u52a8\u6761\u4ef6\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\u7684\u8def\u5f84\u8ddf\u8e2a\u63a7\u5236\u5668\u3002", "method": "\u4f7f\u7528\u9c81\u68d2\u5bf9\u6297\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u8ba9\u5bf9\u624b\u6270\u52a8\u73af\u5883\uff08\u6c14\u52a8\u6a21\u578b\uff09\u6765\u66b4\u9732\u667a\u80fd\u4f53\uff08\u65e0\u4eba\u673a\uff09\u4e8e\u82db\u523b\u573a\u666f\uff0c\u5bf9\u624b\u5f15\u5165\u901f\u7387\u6709\u754c\u7684\u6c14\u52a8\u6a21\u578b\u7cfb\u6570\u6270\u52a8\u3002", "result": "\u5bf9\u6297\u8bad\u7ec3\u6bd4\u4f7f\u7528\u968f\u673a\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u7684\u63a7\u5236\u5668\u5177\u6709\u66f4\u597d\u7684\u9c81\u68d2\u6027\uff0c\u5728\u9ad8\u4fdd\u771f\u4eff\u771f\u4e2d\u5c55\u793a\u4e86\u5728\u5404\u79cd\u4e0d\u786e\u5b9a\u6c14\u52a8\u6761\u4ef6\u4e0b\u7684\u51c6\u786e\u9c81\u68d2\u8def\u5f84\u8ddf\u8e2a\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u9ad8\u4e86\u56fa\u5b9a\u7ffc\u65e0\u4eba\u673a\u8def\u5f84\u8ddf\u8e2a\u63a7\u5236\u5668\u7684\u9c81\u68d2\u6027\uff0c\u80fd\u591f\u5e94\u5bf9\u6c14\u52a8\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u3002"}}
{"id": "2510.15961", "categories": ["cs.LG", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.15961", "abs": "https://arxiv.org/abs/2510.15961", "authors": ["Yiyang Li", "Zehong Wang", "Zhengqing Yuan", "Zheyuan Zhang", "Keerthiram Murugesan", "Chuxu Zhang", "Yanfang Ye"], "title": "Interpretable Graph-Language Modeling for Detecting Youth Illicit Drug Use", "comment": null, "summary": "Illicit drug use among teenagers and young adults (TYAs) remains a pressing\npublic health concern, with rising prevalence and long-term impacts on health\nand well-being. To detect illicit drug use among TYAs, researchers analyze\nlarge-scale surveys such as the Youth Risk Behavior Survey (YRBS) and the\nNational Survey on Drug Use and Health (NSDUH), which preserve rich\ndemographic, psychological, and environmental factors related to substance use.\nHowever, existing modeling methods treat survey variables independently,\noverlooking latent and interconnected structures among them. To address this\nlimitation, we propose LAMI (LAtent relation Mining with bi-modal\nInterpretability), a novel joint graph-language modeling framework for\ndetecting illicit drug use and interpreting behavioral risk factors among TYAs.\nLAMI represents individual responses as relational graphs, learns latent\nconnections through a specialized graph structure learning layer, and\nintegrates a large language model to generate natural language explanations\ngrounded in both graph structures and survey semantics. Experiments on the YRBS\nand NSDUH datasets show that LAMI outperforms competitive baselines in\npredictive accuracy. Interpretability analyses further demonstrate that LAMI\nreveals meaningful behavioral substructures and psychosocial pathways, such as\nfamily dynamics, peer influence, and school-related distress, that align with\nestablished risk factors for substance use.", "AI": {"tldr": "\u63d0\u51faLAMI\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u56fe-\u8bed\u8a00\u5efa\u6a21\u68c0\u6d4b\u9752\u5c11\u5e74\u975e\u6cd5\u836f\u7269\u4f7f\u7528\u5e76\u89e3\u91ca\u884c\u4e3a\u98ce\u9669\u56e0\u7d20\uff0c\u5728YRBS\u548cNSDUH\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5efa\u6a21\u65b9\u6cd5\u5c06\u8c03\u67e5\u53d8\u91cf\u72ec\u7acb\u5904\u7406\uff0c\u5ffd\u7565\u4e86\u53d8\u91cf\u95f4\u7684\u6f5c\u5728\u5173\u8054\u7ed3\u6784\uff0c\u65e0\u6cd5\u6709\u6548\u6355\u6349\u9752\u5c11\u5e74\u836f\u7269\u4f7f\u7528\u7684\u590d\u6742\u884c\u4e3a\u6a21\u5f0f\u3002", "method": "LAMI\u5c06\u4e2a\u4f53\u54cd\u5e94\u8868\u793a\u4e3a\u5173\u7cfb\u56fe\uff0c\u901a\u8fc7\u4e13\u95e8\u7684\u56fe\u7ed3\u6784\u5b66\u4e60\u5c42\u5b66\u4e60\u6f5c\u5728\u8fde\u63a5\uff0c\u5e76\u96c6\u6210\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u57fa\u4e8e\u56fe\u7ed3\u6784\u548c\u8c03\u67e5\u8bed\u4e49\u7684\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u3002", "result": "\u5728YRBS\u548cNSDUH\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793aLAMI\u5728\u9884\u6d4b\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u7ade\u4e89\u57fa\u7ebf\uff0c\u53ef\u89e3\u91ca\u6027\u5206\u6790\u63ed\u793a\u4e86\u4e0e\u5df2\u77e5\u7269\u8d28\u4f7f\u7528\u98ce\u9669\u56e0\u7d20\u4e00\u81f4\u7684\u884c\u4e3a\u5b50\u7ed3\u6784\u548c\u5fc3\u7406\u793e\u4f1a\u8def\u5f84\u3002", "conclusion": "LAMI\u6846\u67b6\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u9752\u5c11\u5e74\u975e\u6cd5\u836f\u7269\u4f7f\u7528\uff0c\u540c\u65f6\u63d0\u4f9b\u6709\u610f\u4e49\u7684\u53ef\u89e3\u91ca\u6027\u5206\u6790\uff0c\u63ed\u793a\u4e86\u5bb6\u5ead\u52a8\u6001\u3001\u540c\u4f34\u5f71\u54cd\u548c\u5b66\u6821\u76f8\u5173\u538b\u529b\u7b49\u5173\u952e\u98ce\u9669\u56e0\u7d20\u3002"}}
{"id": "2510.16257", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16257", "abs": "https://arxiv.org/abs/2510.16257", "authors": ["Chu Fei Luo", "Samuel Dahan", "Xiaodan Zhu"], "title": "Towards Low-Resource Alignment to Diverse Perspectives with Sparse Feedback", "comment": "Findings of EMNLP 2025, 5 pages", "summary": "As language models have a greater impact on society, it is important to\nensure they are aligned to a diverse range of perspectives and are able to\nreflect nuance in human values. However, the most popular training paradigms\nfor modern language models often assume there is one optimal answer for every\nquery, leading to generic responses and poor alignment. In this work, we aim to\nenhance pluralistic alignment of language models in a low-resource setting with\ntwo methods: pluralistic decoding and model steering. We empirically\ndemonstrate that model steering offers consistent improvement over zero-shot\nand few-shot baselines with only 50 annotated samples. Our proposed methods\ndecrease false positives in several high-stakes tasks such as hate speech\ndetection and misinformation detection, and improves the distributional\nalignment to human values in GlobalOpinionQA. We hope our work highlights the\nimportance of diversity and how language models can be adapted to consider\nnuanced perspectives.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u79cd\u65b9\u6cd5\uff08\u591a\u5143\u89e3\u7801\u548c\u6a21\u578b\u5f15\u5bfc\uff09\u6765\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\u7684\u591a\u5143\u5bf9\u9f50\u80fd\u529b\uff0c\u5728\u4f4e\u8d44\u6e90\u8bbe\u7f6e\u4e0b\u4ec5\u752850\u4e2a\u6807\u6ce8\u6837\u672c\u5c31\u80fd\u8d85\u8d8a\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u57fa\u7ebf\uff0c\u51cf\u5c11\u9ad8\u98ce\u9669\u4efb\u52a1\u4e2d\u7684\u8bef\u62a5\u5e76\u6539\u5584\u5bf9\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u5206\u5e03\u5bf9\u9f50\u3002", "motivation": "\u968f\u7740\u8bed\u8a00\u6a21\u578b\u5bf9\u793e\u4f1a\u5f71\u54cd\u589e\u5927\uff0c\u9700\u8981\u786e\u4fdd\u5b83\u4eec\u80fd\u5bf9\u9f50\u591a\u6837\u5316\u89c6\u89d2\u5e76\u53cd\u6620\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u7ec6\u5fae\u5dee\u522b\u3002\u4f46\u4e3b\u6d41\u8bad\u7ec3\u8303\u5f0f\u5047\u8bbe\u6bcf\u4e2a\u67e5\u8be2\u53ea\u6709\u4e00\u4e2a\u6700\u4f18\u7b54\u6848\uff0c\u5bfc\u81f4\u54cd\u5e94\u6cdb\u5316\u4e14\u5bf9\u9f50\u6548\u679c\u5dee\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u65b9\u6cd5\uff1a1) \u591a\u5143\u89e3\u7801 - \u751f\u6210\u591a\u6837\u5316\u54cd\u5e94\uff1b2) \u6a21\u578b\u5f15\u5bfc - \u4f7f\u7528\u5c11\u91cf\u6807\u6ce8\u6837\u672c\uff08\u4ec550\u4e2a\uff09\u5f15\u5bfc\u6a21\u578b\u5b66\u4e60\u591a\u5143\u89c6\u89d2\u3002", "result": "\u6a21\u578b\u5f15\u5bfc\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u57fa\u7ebf\u4e0a\u8868\u73b0\u4e00\u81f4\u63d0\u5347\uff0c\u5728\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u548c\u9519\u8bef\u4fe1\u606f\u68c0\u6d4b\u7b49\u9ad8\u98ce\u9669\u4efb\u52a1\u4e2d\u51cf\u5c11\u8bef\u62a5\uff0c\u5728GlobalOpinionQA\u4e0a\u6539\u5584\u4e86\u5bf9\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u5206\u5e03\u5bf9\u9f50\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5f3a\u8c03\u4e86\u591a\u6837\u6027\u7684\u91cd\u8981\u6027\uff0c\u5c55\u793a\u4e86\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u9002\u5e94\u8003\u8651\u7ec6\u5fae\u89c6\u89d2\uff0c\u4e3a\u6784\u5efa\u66f4\u5177\u5305\u5bb9\u6027\u548c\u4ee3\u8868\u6027\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2510.16550", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.16550", "abs": "https://arxiv.org/abs/2510.16550", "authors": ["Siyuan Yin", "Yuncheng Xu", "Lin Liu", "Fan Yang", "Xuan Zeng", "Chengtao An", "Yangfeng Su"], "title": "SMP-RCR: A Sparse Multipoint Moment Matching Method for RC Reduction", "comment": null, "summary": "In post--layout circuit simulation, efficient model order reduction (MOR) for\nmany--port resistor--capacitor (RC) circuits remains a crucial issue. The\ncurrent mainstream MOR methods for such circuits include high--order moment\nmatching methods and elimination methods. High-order moment matching\nmethods--characterized by high accuracy, such as PRIMA and TurboMOR--tend to\ngenerate large dense reduced-order systems when the number of ports is large,\nwhich impairs the efficiency of MOR. Another common type of MOR method for\nmany--port circuits is based on Gaussian elimination, with the SIP method as a\nrepresentative. The main limitation of this method lies in the inadequate\nmatching of high--order moments. In this paper, we propose a sparse multipoint\nmoment matching method and present comprehensive theoretical analysis results\nregarding the multi--frequency high--order moment matching property. Meanwhile,\nto enhance the algorithm's efficiency, sparse control and deflation techniques\nare introduced to further optimize the algorithm. Numerical experiments\ndemonstrated that, compared to SIP, the accuracy is improved by more than two\norders of magnitude at high frequency points without adding many extra linear\ncomponents. Compared to TurboMOR methods, our method achieves a speed\nimprovement of more than twice while maintaining the same level of precision.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7a00\u758f\u591a\u70b9\u77e9\u5339\u914d\u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u7aef\u53e3RC\u7535\u8def\u6a21\u578b\u964d\u9636\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387", "motivation": "\u73b0\u6709\u9ad8\u7cbe\u5ea6\u77e9\u5339\u914d\u65b9\u6cd5\uff08\u5982PRIMA\u3001TurboMOR\uff09\u5728\u591a\u7aef\u53e3\u60c5\u51b5\u4e0b\u4f1a\u4ea7\u751f\u5bc6\u96c6\u964d\u9636\u7cfb\u7edf\uff0c\u5f71\u54cd\u6548\u7387\uff1b\u800c\u57fa\u4e8e\u9ad8\u65af\u6d88\u5143\u7684\u65b9\u6cd5\uff08\u5982SIP\uff09\u9ad8\u9636\u77e9\u5339\u914d\u4e0d\u8db3", "method": "\u7a00\u758f\u591a\u70b9\u77e9\u5339\u914d\u65b9\u6cd5\uff0c\u7ed3\u5408\u7a00\u758f\u63a7\u5236\u548c\u7d27\u7f29\u6280\u672f\u4f18\u5316\u7b97\u6cd5\uff0c\u63d0\u4f9b\u591a\u9891\u70b9\u9ad8\u9636\u77e9\u5339\u914d\u7684\u7406\u8bba\u5206\u6790", "result": "\u76f8\u6bd4SIP\u65b9\u6cd5\uff0c\u5728\u9ad8\u9891\u70b9\u7cbe\u5ea6\u63d0\u9ad8\u4e24\u4e2a\u6570\u91cf\u7ea7\uff1b\u76f8\u6bd4TurboMOR\u65b9\u6cd5\uff0c\u901f\u5ea6\u63d0\u5347\u4e24\u500d\u4ee5\u4e0a\u4e14\u4fdd\u6301\u76f8\u540c\u7cbe\u5ea6", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u7aef\u53e3RC\u7535\u8def\u6a21\u578b\u964d\u9636\u4e2d\u5b9e\u73b0\u4e86\u7cbe\u5ea6\u548c\u6548\u7387\u7684\u826f\u597d\u5e73\u8861"}}
{"id": "2510.16503", "categories": ["q-fin.ST", "62M10, 91G70, 68T50", "I.2.7; J.4"], "pdf": "https://arxiv.org/pdf/2510.16503", "abs": "https://arxiv.org/abs/2510.16503", "authors": ["Domenica Mino", "Cillian Williamson"], "title": "Sentiment and Volatility in Financial Markets: A Review of BERT and GARCH Applications during Geopolitical Crises", "comment": "24 pages, 9 figures, 3 tables. Includes appendices and supplementary\n  code link", "summary": "Artificial intelligence techniques have increasingly been applied to\nunderstand the complex relationship between public sentiment and financial\nmarket behaviour. This study explores the relationship between the sentiment of\nnews related to the Russia-Ukraine war and the volatility of the stock market.\nA comprehensive dataset of news articles from major US platforms, published\nbetween January 1 and July 17, 2024, was analysed using a fine-tuned\nBidirectional Encoder Representations from Transformers (BERT) model adapted\nfor financial language. We extracted sentiment scores and applied a Generalised\nAutoregressive Conditional Heteroscedasticity (GARCH) model, enhanced with a\nStudent-t distribution to capture the heavy-tailed nature of financial returns\ndata. The results reveal a statistically significant negative relationship\nbetween negative news sentiment and market stability, suggesting that\npessimistic war coverage is associated with increased volatility in the S&P 500\nindex. This research demonstrates how artificial intelligence and natural\nlanguage processing can be integrated with econometric modelling to assess\nreal-time market dynamics, offering valuable tools for financial risk analysis\nduring geopolitical crises.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4f7f\u7528BERT\u6a21\u578b\u5206\u6790\u4fc4\u4e4c\u6218\u4e89\u76f8\u5173\u65b0\u95fb\u60c5\u611f\uff0c\u7ed3\u5408GARCH\u6a21\u578b\u53d1\u73b0\u8d1f\u9762\u65b0\u95fb\u60c5\u611f\u4e0eS&P 500\u6307\u6570\u6ce2\u52a8\u7387\u5448\u663e\u8457\u8d1f\u76f8\u5173\u5173\u7cfb\u3002", "motivation": "\u63a2\u7d22\u4eba\u5de5\u667a\u80fd\u6280\u672f\u5982\u4f55\u7406\u89e3\u516c\u4f17\u60c5\u611f\u4e0e\u91d1\u878d\u5e02\u573a\u884c\u4e3a\u4e4b\u95f4\u7684\u590d\u6742\u5173\u7cfb\uff0c\u7279\u522b\u662f\u5728\u5730\u7f18\u653f\u6cbb\u5371\u673a\u671f\u95f4\u3002", "method": "\u4f7f\u7528\u7ecf\u8fc7\u91d1\u878d\u8bed\u8a00\u5fae\u8c03\u7684BERT\u6a21\u578b\u5206\u67902024\u5e741-7\u6708\u7f8e\u56fd\u4e3b\u8981\u65b0\u95fb\u5e73\u53f0\u7684\u6587\u7ae0\u60c5\u611f\uff0c\u7ed3\u5408\u91c7\u7528\u5b66\u751ft\u5206\u5e03\u7684GARCH\u6a21\u578b\u6765\u6355\u6349\u91d1\u878d\u6536\u76ca\u6570\u636e\u7684\u539a\u5c3e\u7279\u5f81\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u8d1f\u9762\u65b0\u95fb\u60c5\u611f\u4e0e\u5e02\u573a\u7a33\u5b9a\u6027\u4e4b\u95f4\u5b58\u5728\u7edf\u8ba1\u663e\u8457\u7684\u8d1f\u76f8\u5173\u5173\u7cfb\uff0c\u8868\u660e\u60b2\u89c2\u7684\u6218\u4e89\u62a5\u9053\u4e0eS&P 500\u6307\u6570\u6ce2\u52a8\u7387\u589e\u52a0\u76f8\u5173\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u4eba\u5de5\u667a\u80fd\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5982\u4f55\u4e0e\u7ecf\u6d4e\u8ba1\u91cf\u5efa\u6a21\u76f8\u7ed3\u5408\uff0c\u4e3a\u5730\u7f18\u653f\u6cbb\u5371\u673a\u671f\u95f4\u7684\u91d1\u878d\u98ce\u9669\u5206\u6790\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u5de5\u5177\u3002"}}
{"id": "2510.16728", "categories": ["stat.ML", "cs.LG", "math.PR", "stat.ME", "60L10, 60L20, 62G05, 62G08"], "pdf": "https://arxiv.org/pdf/2510.16728", "abs": "https://arxiv.org/abs/2510.16728", "authors": ["Christian Bayer", "Davit Gogolashvili", "Luca Pelizzari"], "title": "Local regression on path spaces with signature metrics", "comment": null, "summary": "We study nonparametric regression and classification for path-valued data. We\nintroduce a functional Nadaraya-Watson estimator that combines the signature\ntransform from rough path theory with local kernel regression. The signature\ntransform provides a principled way to encode sequential data through iterated\nintegrals, enabling direct comparison of paths in a natural metric space. Our\napproach leverages signature-induced distances within the classical kernel\nregression framework, achieving computational efficiency while avoiding the\nscalability bottlenecks of large-scale kernel matrix operations. We establish\nfinite-sample convergence bounds demonstrating favorable statistical properties\nof signature-based distances compared to traditional metrics in\ninfinite-dimensional settings. We propose robust signature variants that\nprovide stability against outliers, enhancing practical performance.\nApplications to both synthetic and real-world data - including stochastic\ndifferential equation learning and time series classification - demonstrate\ncompetitive accuracy while offering significant computational advantages over\nexisting methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7b7e\u540d\u53d8\u6362\u548c\u5c40\u90e8\u6838\u56de\u5f52\u7684\u51fd\u6570\u578bNadaraya-Watson\u4f30\u8ba1\u5668\uff0c\u7528\u4e8e\u8def\u5f84\u503c\u6570\u636e\u7684\u975e\u53c2\u6570\u56de\u5f52\u548c\u5206\u7c7b\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u7b7e\u540d\u8bf1\u5bfc\u7684\u8ddd\u79bb\u5b9e\u73b0\u9ad8\u6548\u8ba1\u7b97\uff0c\u907f\u514d\u4e86\u5927\u89c4\u6a21\u6838\u77e9\u9635\u64cd\u4f5c\u7684\u53ef\u6269\u5c55\u6027\u74f6\u9888\u3002", "motivation": "\u89e3\u51b3\u8def\u5f84\u503c\u6570\u636e\u7684\u56de\u5f52\u548c\u5206\u7c7b\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u65e0\u9650\u7ef4\u8bbe\u7f6e\u4e2d\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u76f4\u63a5\u6bd4\u8f83\u8def\u5f84\u7684\u5408\u7406\u65b9\u6cd5\u3002", "method": "\u5c06\u7c97\u7cd9\u8def\u5f84\u7406\u8bba\u4e2d\u7684\u7b7e\u540d\u53d8\u6362\u4e0e\u5c40\u90e8\u6838\u56de\u5f52\u76f8\u7ed3\u5408\uff0c\u4f7f\u7528\u7b7e\u540d\u8bf1\u5bfc\u7684\u8ddd\u79bb\u5728\u7ecf\u5178\u6838\u56de\u5f52\u6846\u67b6\u4e2d\u8fdb\u884c\u8ba1\u7b97\u3002", "result": "\u5efa\u7acb\u4e86\u6709\u9650\u6837\u672c\u6536\u655b\u754c\uff0c\u8bc1\u660e\u4e86\u57fa\u4e8e\u7b7e\u540d\u7684\u8ddd\u79bb\u76f8\u6bd4\u4f20\u7edf\u5ea6\u91cf\u5728\u65e0\u9650\u7ef4\u8bbe\u7f6e\u4e2d\u5177\u6709\u66f4\u4f18\u7684\u7edf\u8ba1\u7279\u6027\u3002\u5728\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u5b66\u4e60\u548c\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u7b49\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u7ade\u4e89\u6027\u7cbe\u5ea6\u548c\u663e\u8457\u8ba1\u7b97\u4f18\u52bf\u3002", "conclusion": "\u7b7e\u540d\u53d8\u6362\u4e3a\u5e8f\u5217\u6570\u636e\u7f16\u7801\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u65b9\u6cd5\uff0c\u7ed3\u5408\u6838\u56de\u5f52\u6846\u67b6\u5b9e\u73b0\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u7edf\u8ba1\u6027\u80fd\u7684\u826f\u597d\u5e73\u8861\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u8def\u5f84\u503c\u6570\u636e\u65f6\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.16056", "categories": ["cs.CY", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16056", "abs": "https://arxiv.org/abs/2510.16056", "authors": ["Muhammad Aurangzeb Ahmad"], "title": "Algorithmic Fairness in AI Surrogates for End-of-Life Decision-Making", "comment": null, "summary": "Artificial intelligence surrogates are systems designed to infer preferences\nwhen individuals lose decision-making capacity. Fairness in such systems is a\ndomain that has been insufficiently explored. Traditional algorithmic fairness\nframeworks are insufficient for contexts where decisions are relational,\nexistential, and culturally diverse. This paper explores an ethical framework\nfor algorithmic fairness in AI surrogates by mapping major fairness notions\nonto potential real-world end-of-life scenarios. It then examines fairness\nacross moral traditions. The authors argue that fairness in this domain extends\nbeyond parity of outcomes to encompass moral representation, fidelity to the\npatient's values, relationships, and worldview.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8AI\u4ee3\u7406\u7cfb\u7edf\u5728\u4e34\u7ec8\u51b3\u7b56\u4e2d\u7684\u516c\u5e73\u6027\u6846\u67b6\uff0c\u6307\u51fa\u4f20\u7edf\u7b97\u6cd5\u516c\u5e73\u6027\u6846\u67b6\u4e0d\u8db3\u4ee5\u5904\u7406\u5173\u7cfb\u6027\u3001\u5b58\u5728\u6027\u548c\u6587\u5316\u591a\u6837\u6027\u7684\u51b3\u7b56\u60c5\u5883\u3002", "motivation": "AI\u4ee3\u7406\u7cfb\u7edf\u5728\u4e2a\u4f53\u5931\u53bb\u51b3\u7b56\u80fd\u529b\u65f6\u63a8\u65ad\u504f\u597d\uff0c\u4f46\u8be5\u9886\u57df\u7684\u516c\u5e73\u6027\u95ee\u9898\u7814\u7a76\u4e0d\u8db3\u3002\u4f20\u7edf\u7b97\u6cd5\u516c\u5e73\u6027\u6846\u67b6\u65e0\u6cd5\u5145\u5206\u5904\u7406\u5173\u7cfb\u6027\u3001\u5b58\u5728\u6027\u548c\u6587\u5316\u591a\u6837\u6027\u7684\u51b3\u7b56\u60c5\u5883\u3002", "method": "\u901a\u8fc7\u5c06\u4e3b\u8981\u516c\u5e73\u6027\u6982\u5ff5\u6620\u5c04\u5230\u73b0\u5b9e\u4e16\u754c\u4e34\u7ec8\u573a\u666f\uff0c\u5e76\u8de8\u9053\u5fb7\u4f20\u7edf\u68c0\u9a8c\u516c\u5e73\u6027\uff0c\u6784\u5efaAI\u4ee3\u7406\u7684\u4f26\u7406\u516c\u5e73\u6846\u67b6\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u516c\u5e73\u6027\u5728\u8fd9\u4e00\u9886\u57df\u8d85\u8d8a\u4e86\u7ed3\u679c\u5747\u7b49\uff0c\u9700\u8981\u5305\u542b\u9053\u5fb7\u4ee3\u8868\u6027\u3001\u5bf9\u60a3\u8005\u4ef7\u503c\u89c2\u7684\u5fe0\u5b9e\u5ea6\u3001\u4eba\u9645\u5173\u7cfb\u548c\u4e16\u754c\u89c2\u3002", "conclusion": "AI\u4ee3\u7406\u7cfb\u7edf\u7684\u516c\u5e73\u6027\u9700\u8981\u66f4\u5168\u9762\u7684\u6846\u67b6\uff0c\u6db5\u76d6\u9053\u5fb7\u4ee3\u8868\u6027\u3001\u4ef7\u503c\u89c2\u5fe0\u5b9e\u5ea6\u548c\u5173\u7cfb\u7ef4\u5ea6\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u7ed3\u679c\u5e73\u7b49\u3002"}}
{"id": "2510.16033", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16033", "abs": "https://arxiv.org/abs/2510.16033", "authors": ["Junyu Ren", "Wensheng Gan", "Guangyu Zhang", "Wei Zhong", "Philip S. Yu"], "title": "Global-focal Adaptation with Information Separation for Noise-robust Transfer Fault Diagnosis", "comment": "Preprint. 16 figures, 12 tables", "summary": "Existing transfer fault diagnosis methods typically assume either clean data\nor sufficient domain similarity, which limits their effectiveness in industrial\nenvironments where severe noise interference and domain shifts coexist. To\naddress this challenge, we propose an information separation global-focal\nadversarial network (ISGFAN), a robust framework for cross-domain fault\ndiagnosis under noise conditions. ISGFAN is built on an information separation\narchitecture that integrates adversarial learning with an improved orthogonal\nloss to decouple domain-invariant fault representation, thereby isolating noise\ninterference and domain-specific characteristics. To further strengthen\ntransfer robustness, ISGFAN employs a global-focal domain-adversarial scheme\nthat constrains both the conditional and marginal distributions of the model.\nSpecifically, the focal domain-adversarial component mitigates\ncategory-specific transfer obstacles caused by noise in unsupervised scenarios,\nwhile the global domain classifier ensures alignment of the overall\ndistribution. Experiments conducted on three public benchmark datasets\ndemonstrate that the proposed method outperforms other prominent existing\napproaches, confirming the superiority of the ISGFAN framework. Data and code\nare available at https://github.com/JYREN-Source/ISGFAN", "AI": {"tldr": "\u63d0\u51faISGFAN\u6846\u67b6\uff0c\u901a\u8fc7\u4fe1\u606f\u5206\u79bb\u548c\u5168\u5c40-\u5c40\u90e8\u5bf9\u6297\u5b66\u4e60\u89e3\u51b3\u566a\u58f0\u5e72\u6270\u548c\u9886\u57df\u504f\u79fb\u5171\u5b58\u7684\u8de8\u9886\u57df\u6545\u969c\u8bca\u65ad\u95ee\u9898", "motivation": "\u73b0\u6709\u8fc1\u79fb\u6545\u969c\u8bca\u65ad\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u6570\u636e\u5e72\u51c0\u6216\u9886\u57df\u76f8\u4f3c\u6027\u8db3\u591f\uff0c\u4f46\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u4e25\u91cd\u566a\u58f0\u5e72\u6270\u548c\u9886\u57df\u504f\u79fb\u5171\u5b58\uff0c\u9650\u5236\u4e86\u5176\u6709\u6548\u6027", "method": "\u57fa\u4e8e\u4fe1\u606f\u5206\u79bb\u67b6\u6784\uff0c\u7ed3\u5408\u5bf9\u6297\u5b66\u4e60\u548c\u6539\u8fdb\u7684\u6b63\u4ea4\u635f\u5931\u6765\u89e3\u8026\u9886\u57df\u4e0d\u53d8\u6545\u969c\u8868\u793a\uff1b\u91c7\u7528\u5168\u5c40-\u5c40\u90e8\u9886\u57df\u5bf9\u6297\u65b9\u6848\u7ea6\u675f\u6a21\u578b\u7684\u6761\u4ef6\u5206\u5e03\u548c\u8fb9\u7f18\u5206\u5e03", "result": "\u5728\u4e09\u4e2a\u516c\u5171\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u5176\u4ed6\u73b0\u6709\u65b9\u6cd5", "conclusion": "ISGFAN\u6846\u67b6\u5728\u566a\u58f0\u6761\u4ef6\u4e0b\u7684\u8de8\u9886\u57df\u6545\u969c\u8bca\u65ad\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027"}}
{"id": "2510.16655", "categories": ["math.OC", "90C15, 90C25, 90C06, 65K05"], "pdf": "https://arxiv.org/pdf/2510.16655", "abs": "https://arxiv.org/abs/2510.16655", "authors": ["Cheik Traor\u00e9", "Peter Ochs"], "title": "Bregman Stochastic Proximal Point Algorithm with Variance Reduction", "comment": null, "summary": "Stochastic algorithms, especially stochastic gradient descent (SGD), have\nproven to be the go-to methods in data science and machine learning. In recent\nyears, the stochastic proximal point algorithm (SPPA) emerged, and it was shown\nto be more robust than SGD with respect to stepsize settings. However, SPPA\nstill suffers from a decreased convergence rate due to the need for vanishing\nstepsizes, which is resolved by using variance reduction methods. In the\ndeterministic setting, there are many problems that can be solved more\nefficiently when viewing them in a non-Euclidean geometry using Bregman\ndistances. This paper combines these two worlds and proposes variance reduction\ntechniques for the Bregman stochastic proximal point algorithm (BSPPA). As\nspecial cases, we obtain SAGA- and SVRG-like variance reduction techniques for\nBSPPA. Our theoretical and numerical results demonstrate improved stability and\nconvergence rates compared to the vanilla BSPPA with constant and vanishing\nstepsizes, respectively. Our analysis, also, allow to recover the same variance\nreduction techniques for Bregman SGD in a unified way.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Bregman\u968f\u673a\u8fd1\u70b9\u7b97\u6cd5(BSPPA)\u7684\u65b9\u5dee\u7f29\u51cf\u6280\u672f\uff0c\u7ed3\u5408\u4e86SAGA\u548cSVRG\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u6536\u655b\u901f\u5ea6\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u968f\u673a\u8fd1\u70b9\u7b97\u6cd5(SPPA)\u6bd4SGD\u5bf9\u6b65\u957f\u8bbe\u7f6e\u66f4\u9c81\u68d2\uff0c\u4f46\u4ecd\u56e0\u9700\u8981\u9012\u51cf\u6b65\u957f\u800c\u5bfc\u81f4\u6536\u655b\u901f\u5ea6\u4e0b\u964d\u3002\u540c\u65f6\uff0c\u8bb8\u591a\u95ee\u9898\u5728\u975e\u6b27\u51e0\u91cc\u5f97\u51e0\u4f55\u4e2d\u4f7f\u7528Bregman\u8ddd\u79bb\u53ef\u4ee5\u66f4\u9ad8\u6548\u89e3\u51b3\u3002", "method": "\u5c06\u65b9\u5dee\u7f29\u51cf\u6280\u672f\u4e0eBregman\u968f\u673a\u8fd1\u70b9\u7b97\u6cd5(BSPPA)\u7ed3\u5408\uff0c\u63d0\u51fa\u4e86\u7c7b\u4f3cSAGA\u548cSVRG\u7684\u65b9\u5dee\u7f29\u51cf\u65b9\u6cd5\u3002", "result": "\u7406\u8bba\u548c\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u76f8\u6bd4\u4f7f\u7528\u5e38\u6570\u548c\u9012\u51cf\u6b65\u957f\u7684\u539f\u59cbBSPPA\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u7a33\u5b9a\u6027\u548c\u6536\u655b\u901f\u5ea6\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u5dee\u7f29\u51cf\u6280\u672f\u663e\u8457\u63d0\u5347\u4e86BSPPA\u7684\u6027\u80fd\uff0c\u5e76\u80fd\u4ee5\u7edf\u4e00\u65b9\u5f0f\u6062\u590dBregman SGD\u7684\u76f8\u540c\u65b9\u5dee\u7f29\u51cf\u6280\u672f\u3002"}}
{"id": "2510.15962", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15962", "abs": "https://arxiv.org/abs/2510.15962", "authors": ["Zhuxuanzi Wang", "Mingqiao Mo", "Xi Xiao", "Chen Liu", "Chenrui Ma", "Yunbei Zhang", "Xiao Wang", "Smita Krishnaswamy", "Tianyang Wang"], "title": "CTR-LoRA: Curvature-Aware and Trust-Region Guided Low-Rank Adaptation for Large Language Models", "comment": null, "summary": "Parameter-efficient fine-tuning (PEFT) has become the standard approach for\nadapting large language models under limited compute and memory budgets.\nAlthough previous methods improve efficiency through low-rank updates,\nquantization, or heuristic budget reallocation, they often decouple the\nallocation of capacity from the way updates evolve during training. In this\nwork, we introduce CTR-LoRA, a framework guided by curvature trust region that\nintegrates rank scheduling with stability-aware optimization. CTR-LoRA\nallocates parameters based on marginal utility derived from lightweight\nsecond-order proxies and constrains updates using a Fisher/Hessian-metric trust\nregion. Experiments on multiple open-source backbones (7B-13B), evaluated on\nboth in-distribution and out-of-distribution benchmarks, show consistent\nimprovements over strong PEFT baselines. In addition to increased accuracy,\nCTR-LoRA enhances training stability, reduces memory requirements, and achieves\nhigher throughput, positioning it on the Pareto frontier of performance and\nefficiency. These results highlight a principled path toward more robust and\ndeployable PEFT.", "AI": {"tldr": "CTR-LoRA\u662f\u4e00\u79cd\u57fa\u4e8e\u66f2\u7387\u4fe1\u4efb\u533a\u57df\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u8fb9\u9645\u6548\u7528\u5206\u914d\u53c2\u6570\u5e76\u4f7f\u7528Fisher/Hessian\u5ea6\u91cf\u4fe1\u4efb\u533a\u57df\u7ea6\u675f\u66f4\u65b0\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709PEFT\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709PEFT\u65b9\u6cd5\u5728\u4f4e\u79e9\u66f4\u65b0\u3001\u91cf\u5316\u6216\u542f\u53d1\u5f0f\u9884\u7b97\u5206\u914d\u65b9\u9762\u63d0\u9ad8\u4e86\u6548\u7387\uff0c\u4f46\u5f80\u5f80\u5c06\u5bb9\u91cf\u5206\u914d\u4e0e\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u66f4\u65b0\u7684\u6f14\u5316\u65b9\u5f0f\u89e3\u8026\uff0c\u7f3a\u4e4f\u5bf9\u8bad\u7ec3\u7a33\u5b9a\u6027\u7684\u8003\u8651\u3002", "method": "CTR-LoRA\u6846\u67b6\u7ed3\u5408\u4e86\u79e9\u8c03\u5ea6\u4e0e\u7a33\u5b9a\u6027\u611f\u77e5\u4f18\u5316\uff0c\u57fa\u4e8e\u8f7b\u91cf\u7ea7\u4e8c\u9636\u4ee3\u7406\u7684\u8fb9\u9645\u6548\u7528\u5206\u914d\u53c2\u6570\uff0c\u5e76\u4f7f\u7528Fisher/Hessian\u5ea6\u91cf\u4fe1\u4efb\u533a\u57df\u7ea6\u675f\u66f4\u65b0\u3002", "result": "\u5728\u591a\u4e2a\u5f00\u6e90\u9aa8\u5e72\u6a21\u578b\uff087B-13B\uff09\u4e0a\uff0c\u5728\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u4f18\u4e8e\u5f3aPEFT\u57fa\u7ebf\uff0c\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3001\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u51cf\u5c11\u4e86\u5185\u5b58\u9700\u6c42\u5e76\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u541e\u5410\u91cf\u3002", "conclusion": "CTR-LoRA\u5728\u6027\u80fd\u548c\u6548\u7387\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u66f4\u7a33\u5065\u548c\u53ef\u90e8\u7f72\u7684PEFT\u63d0\u4f9b\u4e86\u4e00\u6761\u539f\u5219\u6027\u8def\u5f84\u3002"}}
{"id": "2510.16282", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16282", "abs": "https://arxiv.org/abs/2510.16282", "authors": ["Zhaoxuan Tan", "Zixuan Zhang", "Haoyang Wen", "Zheng Li", "Rongzhi Zhang", "Pei Chen", "Fengran Mo", "Zheyuan Liu", "Qingkai Zeng", "Qingyu Yin", "Meng Jiang"], "title": "Instant Personalized Large Language Model Adaptation via Hypernetwork", "comment": null, "summary": "Personalized large language models (LLMs) tailor content to individual\npreferences using user profiles or histories. However, existing\nparameter-efficient fine-tuning (PEFT) methods, such as the\n``One-PEFT-Per-User'' (OPPU) paradigm, require training a separate adapter for\neach user, making them computationally expensive and impractical for real-time\nupdates. We introduce Profile-to-PEFT, a scalable framework that employs a\nhypernetwork, trained end-to-end, to map a user's encoded profile directly to a\nfull set of adapter parameters (e.g., LoRA), eliminating per-user training at\ndeployment. This design enables instant adaptation, generalization to unseen\nusers, and privacy-preserving local deployment. Experimental results\ndemonstrate that our method outperforms both prompt-based personalization and\nOPPU while using substantially fewer computational resources at deployment. The\nframework exhibits strong generalization to out-of-distribution users and\nmaintains robustness across varying user activity levels and different\nembedding backbones. The proposed Profile-to-PEFT framework enables efficient,\nscalable, and adaptive LLM personalization suitable for large-scale\napplications.", "AI": {"tldr": "\u63d0\u51faProfile-to-PEFT\u6846\u67b6\uff0c\u4f7f\u7528\u8d85\u7f51\u7edc\u5c06\u7528\u6237\u914d\u7f6e\u6587\u4ef6\u76f4\u63a5\u6620\u5c04\u5230\u9002\u914d\u5668\u53c2\u6570\uff0c\u65e0\u9700\u4e3a\u6bcf\u4e2a\u7528\u6237\u5355\u72ec\u8bad\u7ec3\uff0c\u5b9e\u73b0\u5b9e\u65f6\u4e2a\u6027\u5316LLM\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u53c2\u6570\u9ad8\u6548\u5fae\u8c03(PEFT)\u7684\u4e2a\u6027\u5316\u65b9\u6cd5\u9700\u8981\u4e3a\u6bcf\u4e2a\u7528\u6237\u8bad\u7ec3\u5355\u72ec\u7684\u9002\u914d\u5668\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u65e0\u6cd5\u5b9e\u65f6\u66f4\u65b0\u3002", "method": "\u4f7f\u7528\u7aef\u5230\u7aef\u8bad\u7ec3\u7684\u8d85\u7f51\u7edc\uff0c\u5c06\u7f16\u7801\u540e\u7684\u7528\u6237\u914d\u7f6e\u6587\u4ef6\u76f4\u63a5\u6620\u5c04\u5230\u5b8c\u6574\u7684\u9002\u914d\u5668\u53c2\u6570\u96c6(\u5982LoRA)\uff0c\u90e8\u7f72\u65f6\u65e0\u9700\u7528\u6237\u7279\u5b9a\u8bad\u7ec3\u3002", "result": "\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u57fa\u4e8e\u63d0\u793a\u7684\u4e2a\u6027\u5316\u548cOPPU\u65b9\u6cd5\uff0c\u540c\u65f6\u90e8\u7f72\u65f6\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u663e\u8457\u51cf\u5c11\uff0c\u5bf9\u5206\u5e03\u5916\u7528\u6237\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Profile-to-PEFT\u6846\u67b6\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u548c\u81ea\u9002\u5e94\u7684LLM\u4e2a\u6027\u5316\uff0c\u9002\u5408\u5927\u89c4\u6a21\u5e94\u7528\u3002"}}
{"id": "2510.16693", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.16693", "abs": "https://arxiv.org/abs/2510.16693", "authors": ["Ayan Das", "Anushka Sharma", "Anamitra Pal"], "title": "Linear State Estimation in Presence of Bounded Uncertainties: A Comparative Analysis", "comment": null, "summary": "A variety of algorithms have been proposed to address the power system state\nestimation problem in the presence of uncertainties in the data. However, less\nemphasis has been given to handling perturbations in the model. In the context\nof linear state estimation (LSE), which is the focus of this paper,\nperturbations in the model come from variations in the line parameters. Since\nthe actual values of the line parameters can be different from the values\nstored in a power utility's database, we investigate three approaches in this\npaper to estimate the states in the presence of bounded uncertainties in the\ndata and the model. The first approach is based on interval arithmetic, the\nsecond is based on convex optimization, and the third is based on generalized\nlinear fractional programming. The three algorithms are applied to multiple\nIEEE test systems and compared in terms of their speed and accuracy. The\nresults indicate that the first two algorithms are extremely fast and give\nexpected results, while the third suffers from scalability issues and is\nunsuitable for LSE.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u7535\u529b\u7cfb\u7edf\u7ebf\u6027\u72b6\u6001\u4f30\u8ba1\u4e2d\u5904\u7406\u6570\u636e\u548c\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u7684\u4e09\u79cd\u65b9\u6cd5\uff0c\u91cd\u70b9\u89e3\u51b3\u4e86\u7ebf\u8def\u53c2\u6570\u53d8\u5316\u5e26\u6765\u7684\u6a21\u578b\u6270\u52a8\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6570\u636e\u4e0d\u786e\u5b9a\u6027\uff0c\u4f46\u5bf9\u6a21\u578b\u6270\u52a8\uff08\u7279\u522b\u662f\u7ebf\u8def\u53c2\u6570\u53d8\u5316\uff09\u7684\u5904\u7406\u5173\u6ce8\u8f83\u5c11\u3002\u7531\u4e8e\u5b9e\u9645\u7ebf\u8def\u53c2\u6570\u53ef\u80fd\u4e0e\u6570\u636e\u5e93\u4e2d\u7684\u503c\u4e0d\u540c\uff0c\u9700\u8981\u7814\u7a76\u5728\u6570\u636e\u548c\u6a21\u578b\u90fd\u5b58\u5728\u6709\u754c\u4e0d\u786e\u5b9a\u6027\u65f6\u7684\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u79cd\u7b97\u6cd5\uff1a\u57fa\u4e8e\u533a\u95f4\u7b97\u672f\u7684\u65b9\u6cd5\u3001\u57fa\u4e8e\u51f8\u4f18\u5316\u7684\u65b9\u6cd5\u3001\u57fa\u4e8e\u5e7f\u4e49\u7ebf\u6027\u5206\u5f0f\u89c4\u5212\u7684\u65b9\u6cd5\u3002\u8fd9\u4e9b\u65b9\u6cd5\u7528\u4e8e\u5904\u7406\u7ebf\u6027\u72b6\u6001\u4f30\u8ba1\u4e2d\u7684\u6570\u636e\u548c\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5728\u591a\u4e2aIEEE\u6d4b\u8bd5\u7cfb\u7edf\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u524d\u4e24\u79cd\u7b97\u6cd5\u901f\u5ea6\u6781\u5feb\u4e14\u7ed3\u679c\u7b26\u5408\u9884\u671f\uff0c\u800c\u7b2c\u4e09\u79cd\u7b97\u6cd5\u5b58\u5728\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u4e0d\u9002\u7528\u4e8e\u7ebf\u6027\u72b6\u6001\u4f30\u8ba1\u3002", "conclusion": "\u57fa\u4e8e\u533a\u95f4\u7b97\u672f\u548c\u51f8\u4f18\u5316\u7684\u65b9\u6cd5\u5728\u901f\u5ea6\u548c\u51c6\u786e\u6027\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u800c\u57fa\u4e8e\u5e7f\u4e49\u7ebf\u6027\u5206\u5f0f\u89c4\u5212\u7684\u65b9\u6cd5\u7531\u4e8e\u53ef\u6269\u5c55\u6027\u95ee\u9898\u4e0d\u9002\u5408\u7ebf\u6027\u72b6\u6001\u4f30\u8ba1\u5e94\u7528\u3002"}}
{"id": "2510.16745", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH", "62G10, 62G20, 62P05, 46E22"], "pdf": "https://arxiv.org/pdf/2510.16745", "abs": "https://arxiv.org/abs/2510.16745", "authors": ["Rohan Sen"], "title": "Kernel-Based Nonparametric Tests For Shape Constraints", "comment": "31 pages, 1 figure", "summary": "We develop a reproducing kernel Hilbert space (RKHS) framework for\nnonparametric mean-variance optimization and inference on shape constraints of\nthe optimal rule. We derive statistical properties of the sample estimator and\nprovide rigorous theoretical guarantees, such as asymptotic consistency, a\nfunctional central limit theorem, and a finite-sample deviation bound that\nmatches the Monte Carlo rate up to regularization. Building on these findings,\nwe introduce a joint Wald-type statistic to test for shape constraints over\nfinite grids. The approach comes with an efficient computational procedure\nbased on a pivoted Cholesky factorization, facilitating scalability to large\ndatasets. Empirical tests suggest favorably of the proposed methodology.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u6846\u67b6\uff0c\u7528\u4e8e\u975e\u53c2\u6570\u5747\u503c-\u65b9\u5dee\u4f18\u5316\u548c\u5f62\u72b6\u7ea6\u675f\u63a8\u65ad\uff0c\u63d0\u4f9b\u4e86\u7edf\u8ba1\u7406\u8bba\u4fdd\u8bc1\u548c\u9ad8\u6548\u8ba1\u7b97\u7a0b\u5e8f\u3002", "motivation": "\u4e3a\u5f62\u72b6\u7ea6\u675f\u4e0b\u7684\u975e\u53c2\u6570\u5747\u503c-\u65b9\u5dee\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e25\u683c\u7684\u7edf\u8ba1\u7406\u8bba\u6846\u67b6\u548c\u53ef\u6269\u5c55\u7684\u8ba1\u7b97\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u7406\u8bba\uff0c\u4f7f\u7528\u67a2\u8f74Cholesky\u5206\u89e3\u8fdb\u884c\u9ad8\u6548\u8ba1\u7b97\uff0c\u6784\u5efa\u8054\u5408Wald\u578b\u7edf\u8ba1\u91cf\u68c0\u9a8c\u5f62\u72b6\u7ea6\u675f\u3002", "result": "\u83b7\u5f97\u4e86\u6e10\u8fd1\u4e00\u81f4\u6027\u3001\u51fd\u6570\u4e2d\u5fc3\u6781\u9650\u5b9a\u7406\u548c\u6709\u9650\u6837\u672c\u504f\u5dee\u754c\u7b49\u7406\u8bba\u4fdd\u8bc1\uff0c\u7ecf\u9a8c\u6d4b\u8bd5\u8868\u660e\u65b9\u6cd5\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u5f62\u72b6\u7ea6\u675f\u4e0b\u7684\u5747\u503c-\u65b9\u5dee\u4f18\u5316\u63d0\u4f9b\u4e86\u7406\u8bba\u4e25\u8c28\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.16068", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16068", "abs": "https://arxiv.org/abs/2510.16068", "authors": ["Wei Ting Liow", "Sumbul Khan", "Lay Kee Ang"], "title": "Co-Designing Interdisciplinary Design Projects with AI", "comment": "to be published in IEEE TALE 2025", "summary": "Creating interdisciplinary design projects is time-consuming and cognitively\ndemanding for teachers, requiring curriculum alignment, cross-subject\nintegration, and careful sequencing. International research reports increasing\nteacher use of AI alongside persistent workload pressures, underscoring the\nneed for planning support. This paper presents the Interdisciplinary Design\nProject Planner (IDPplanner), a GPT-based planning assistant grounded in Design\nInnovation principles, alignment with Singapore secondary school syllabuses,\nand 21st-century competencies. In a within-subject, counterbalanced workshop\nwith 33 in-service teachers, participants produced two versions of the same\nproject: manual and AI-assisted, followed by self- and peer-evaluations using a\nsix-dimensional rubric. The AI-assisted version received higher scores for\nCurriculum Alignment, Design Thinking Application, and Coherence and Flow, with\na marginal advantage for Assessment Strategies. Teacher reflections indicated\nthat AI-assisted planning improved structure, sequencing, and idea generation,\nwhile contextualization to local syllabuses, class profiles, and student needs\nremained teacher-led. Contributions include a purpose-built planning tool that\norganizes ideas into a ten-component flow with ready-to-adapt prompts,\ntemplates, and assessment suggestions; an empirical, rubric-based comparison of\nplanning quality; and evidence that AI can function as a pedagogical planning\npartner. Recommendations emphasize hybrid teacher-AI workflows to enhance\ncurriculum alignment and reduce planning complexity, and design suggestions for\ndevelopers to strengthen contextual customization, iterative design support,\nand localized rubrics. Although instantiated with a Singapore-based curriculum,\nthe planning flow and rubric are framework-agnostic and can be parameterized\nfor other systems.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86IDPplanner\uff0c\u4e00\u4e2a\u57fa\u4e8eGPT\u7684\u8de8\u5b66\u79d1\u8bbe\u8ba1\u9879\u76ee\u89c4\u5212\u52a9\u624b\uff0c\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u8868\u660eAI\u8f85\u52a9\u89c4\u5212\u5728\u8bfe\u7a0b\u5bf9\u9f50\u3001\u8bbe\u8ba1\u601d\u7ef4\u5e94\u7528\u548c\u8fde\u8d2f\u6027\u65b9\u9762\u4f18\u4e8e\u624b\u52a8\u89c4\u5212\uff0c\u540c\u65f6\u5f3a\u8c03\u6559\u5e08-AI\u6df7\u5408\u5de5\u4f5c\u6d41\u7a0b\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u6559\u5e08\u521b\u5efa\u8de8\u5b66\u79d1\u8bbe\u8ba1\u9879\u76ee\u8017\u65f6\u4e14\u8ba4\u77e5\u8981\u6c42\u9ad8\uff0c\u9700\u8981\u8bfe\u7a0b\u5bf9\u9f50\u3001\u8de8\u5b66\u79d1\u6574\u5408\u548c\u7cbe\u5fc3\u6392\u5e8f\u3002\u56fd\u9645\u7814\u7a76\u663e\u793a\u6559\u5e08\u4f7f\u7528AI\u589e\u52a0\u4f46\u5de5\u4f5c\u538b\u529b\u6301\u7eed\uff0c\u51f8\u663e\u89c4\u5212\u652f\u6301\u7684\u5fc5\u8981\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8eGPT\u7684IDPplanner\u89c4\u5212\u52a9\u624b\uff0c\u572833\u540d\u5728\u804c\u6559\u5e08\u7684\u5e73\u8861\u5b9e\u9a8c\u4e2d\uff0c\u6bd4\u8f83\u624b\u52a8\u548cAI\u8f85\u52a9\u4e24\u79cd\u89c4\u5212\u65b9\u5f0f\uff0c\u4f7f\u7528\u516d\u7ef4\u8bc4\u5206\u6807\u51c6\u8fdb\u884c\u81ea\u8bc4\u548c\u4e92\u8bc4\u3002", "result": "AI\u8f85\u52a9\u7248\u672c\u5728\u8bfe\u7a0b\u5bf9\u9f50\u3001\u8bbe\u8ba1\u601d\u7ef4\u5e94\u7528\u548c\u8fde\u8d2f\u6027\u65b9\u9762\u5f97\u5206\u66f4\u9ad8\uff0c\u5728\u8bc4\u4f30\u7b56\u7565\u65b9\u9762\u7565\u6709\u4f18\u52bf\u3002\u6559\u5e08\u53cd\u601d\u663e\u793aAI\u8f85\u52a9\u6539\u5584\u4e86\u7ed3\u6784\u3001\u6392\u5e8f\u548c\u521b\u610f\u751f\u6210\uff0c\u4f46\u60c5\u5883\u5316\u4ecd\u9700\u6559\u5e08\u4e3b\u5bfc\u3002", "conclusion": "AI\u53ef\u4ee5\u4f5c\u4e3a\u6559\u5b66\u89c4\u5212\u4f19\u4f34\uff0c\u5efa\u8bae\u91c7\u7528\u6df7\u5408\u6559\u5e08-AI\u5de5\u4f5c\u6d41\u7a0b\u6765\u589e\u5f3a\u8bfe\u7a0b\u5bf9\u9f50\u5e76\u964d\u4f4e\u89c4\u5212\u590d\u6742\u6027\uff0c\u540c\u65f6\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u52a0\u5f3a\u60c5\u5883\u5b9a\u5236\u3001\u8fed\u4ee3\u8bbe\u8ba1\u652f\u6301\u548c\u672c\u5730\u5316\u8bc4\u5206\u6807\u51c6\u7684\u5efa\u8bae\u3002"}}
{"id": "2510.16047", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16047", "abs": "https://arxiv.org/abs/2510.16047", "authors": ["Ioan Hedea"], "title": "Algorithms for dynamic scheduling in manufacturing, towards digital factories Improving Deadline Feasibility and Responsiveness via Temporal Networks", "comment": "8 pages 2 column, 11 figures. Bachelor's thesis", "summary": "Modern manufacturing systems must meet hard delivery deadlines while coping\nwith stochastic task durations caused by process noise, equipment variability,\nand human intervention. Traditional deterministic schedules break down when\nreality deviates from nominal plans, triggering costly last-minute repairs.\nThis thesis combines offline constraint-programming (CP) optimisation with\nonline temporal-network execution to create schedules that remain feasible\nunder worst-case uncertainty. First, we build a CP model of the flexible\njob-shop with per-job deadline tasks and insert an optimal buffer $\\Delta^*$ to\nobtain a fully pro-active baseline. We then translate the resulting plan into a\nSimple Temporal Network with Uncertainty (STNU) and verify dynamic\ncontrollability, which guarantees that a real-time dispatcher can retime\nactivities for every bounded duration realisation without violating resource or\ndeadline constraints. Extensive Monte-Carlo simulations on the open Kacem~1--4\nbenchmark suite show that our hybrid approach eliminates 100\\% of deadline\nviolations observed in state-of-the-art meta-heuristic schedules, while adding\nonly 3--5\\% makespan overhead. Scalability experiments confirm that CP\nsolve-times and STNU checks remain sub-second on medium-size instances. The\nwork demonstrates how temporal-network reasoning can bridge the gap between\nproactive buffering and dynamic robustness, moving industry a step closer to\ntruly digital, self-correcting factories.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7ed3\u5408\u79bb\u7ebf\u7ea6\u675f\u7f16\u7a0b\u4f18\u5316\u548c\u5728\u7ebf\u65f6\u95f4\u7f51\u7edc\u6267\u884c\uff0c\u521b\u5efa\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u4ecd\u53ef\u884c\u7684\u8c03\u5ea6\u65b9\u6848\uff0c\u6d88\u9664100%\u7684\u622a\u6b62\u65f6\u95f4\u8fdd\u89c4\uff0c\u4ec5\u589e\u52a03-5%\u7684\u5236\u9020\u5468\u671f\u5f00\u9500\u3002", "motivation": "\u73b0\u4ee3\u5236\u9020\u7cfb\u7edf\u9700\u8981\u6ee1\u8db3\u4e25\u683c\u7684\u4ea4\u4ed8\u671f\u9650\uff0c\u540c\u65f6\u5e94\u5bf9\u7531\u8fc7\u7a0b\u566a\u58f0\u3001\u8bbe\u5907\u53d8\u5f02\u6027\u548c\u4eba\u4e3a\u5e72\u9884\u5f15\u8d77\u7684\u968f\u673a\u4efb\u52a1\u6301\u7eed\u65f6\u95f4\u3002\u4f20\u7edf\u7684\u786e\u5b9a\u6027\u8c03\u5ea6\u5728\u73b0\u5b9e\u504f\u79bb\u540d\u4e49\u8ba1\u5212\u65f6\u4f1a\u5931\u6548\uff0c\u5bfc\u81f4\u6602\u8d35\u7684\u7d27\u6025\u7ef4\u4fee\u3002", "method": "\u9996\u5148\u6784\u5efa\u5177\u6709\u6bcf\u9879\u4efb\u52a1\u622a\u6b62\u65f6\u95f4\u7684\u67d4\u6027\u4f5c\u4e1a\u8f66\u95f4\u7ea6\u675f\u7f16\u7a0b\u6a21\u578b\uff0c\u63d2\u5165\u6700\u4f18\u7f13\u51b2\u533a\u0394*\u83b7\u5f97\u5b8c\u5168\u4e3b\u52a8\u57fa\u7ebf\uff1b\u7136\u540e\u5c06\u7ed3\u679c\u8ba1\u5212\u8f6c\u6362\u4e3a\u5177\u6709\u4e0d\u786e\u5b9a\u6027\u7684\u7b80\u5355\u65f6\u95f4\u7f51\u7edc\uff0c\u9a8c\u8bc1\u52a8\u6001\u53ef\u63a7\u6027\u3002", "result": "\u5728Kacem 1-4\u57fa\u51c6\u5957\u4ef6\u4e0a\u7684\u5e7f\u6cdb\u8499\u7279\u5361\u6d1b\u6a21\u62df\u663e\u793a\uff0c\u8be5\u6df7\u5408\u65b9\u6cd5\u6d88\u9664\u4e86\u6700\u5148\u8fdb\u5143\u542f\u53d1\u5f0f\u8c03\u5ea6\u4e2d\u89c2\u5bdf\u5230\u7684100%\u622a\u6b62\u65f6\u95f4\u8fdd\u89c4\uff0c\u540c\u65f6\u4ec5\u589e\u52a03-5%\u7684\u5236\u9020\u5468\u671f\u5f00\u9500\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5c55\u793a\u4e86\u65f6\u95f4\u7f51\u7edc\u63a8\u7406\u5982\u4f55\u5f25\u5408\u4e3b\u52a8\u7f13\u51b2\u548c\u52a8\u6001\u9c81\u68d2\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4f7f\u5de5\u4e1a\u66f4\u63a5\u8fd1\u771f\u6b63\u7684\u6570\u5b57\u5316\u3001\u81ea\u6821\u6b63\u5de5\u5382\u3002"}}
{"id": "2510.16680", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2510.16680", "abs": "https://arxiv.org/abs/2510.16680", "authors": ["Long Chen", "Zeyi Xu"], "title": "HNAG++: A Super-Fast Accelerated Gradient Method for Strongly Convex Optimization", "comment": null, "summary": "We introduce and analyze two methods, HNAG+ and HNAG++, for minimizing\nstrongly convex functions with large condition number kappa. For HNAG+, we\nprove a global linear convergence rate of 1 - 2/sqrt(kappa), achieving the\ninformation-theoretic optimal rate. For HNAG++, we establish a global\nasymptotic linear rate of 1 - 2*sqrt(2/kappa) for functions with H\\\"older\ncontinuous Hessians, representing the fastest known rate among globally\nconvergent first-order methods. Extensive numerical experiments on linear and\nnonlinear problems show that HNAG++ consistently outperforms existing\naccelerated gradient methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86HNAG+\u548cHNAG++\u4e24\u79cd\u4f18\u5316\u5f3a\u51f8\u51fd\u6570\u7684\u65b9\u6cd5\uff0cHNAG+\u8fbe\u5230\u4fe1\u606f\u8bba\u6700\u4f18\u6536\u655b\u7387\uff0cHNAG++\u5728H\u00f6lder\u8fde\u7eedHessian\u6761\u4ef6\u4e0b\u5b9e\u73b0\u6700\u5feb\u5168\u5c40\u6536\u655b\u7387\u3002", "motivation": "\u9488\u5bf9\u6761\u4ef6\u6570\u03ba\u8f83\u5927\u7684\u5f3a\u51f8\u51fd\u6570\u4f18\u5316\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u6536\u655b\u901f\u5ea6\u4e0d\u591f\u5feb\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u4f18\u5316\u7b97\u6cd5\u3002", "method": "HNAG+\u548cHNAG++\u4e24\u79cd\u52a0\u901f\u68af\u5ea6\u65b9\u6cd5\uff0cHNAG++\u7279\u522b\u9002\u7528\u4e8eH\u00f6lder\u8fde\u7eedHessian\u7684\u51fd\u6570\u3002", "result": "HNAG+\u8fbe\u52301-2/\u221a\u03ba\u7684\u5168\u5c40\u7ebf\u6027\u6536\u655b\u7387\uff0cHNAG++\u5728H\u00f6lder\u8fde\u7eedHessian\u6761\u4ef6\u4e0b\u8fbe\u52301-2\u221a(2/\u03ba)\u7684\u6e10\u8fd1\u7ebf\u6027\u6536\u655b\u7387\uff0c\u6570\u503c\u5b9e\u9a8c\u663e\u793aHNAG++\u4f18\u4e8e\u73b0\u6709\u52a0\u901f\u68af\u5ea6\u65b9\u6cd5\u3002", "conclusion": "HNAG+\u548cHNAG++\u662f\u9ad8\u6548\u7684\u5f3a\u51f8\u51fd\u6570\u4f18\u5316\u7b97\u6cd5\uff0c\u7279\u522b\u662fHNAG++\u5728\u975e\u7ebf\u6027\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u5927\u89c4\u6a21\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.15964", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.15964", "abs": "https://arxiv.org/abs/2510.15964", "authors": ["Tuowei Wang", "Kun Li", "Zixu Hao", "Donglin Bai", "Ju Ren", "Yaoxue Zhang", "Ting Cao", "Mao Yang"], "title": "Long Exposure: Accelerating Parameter-Efficient Fine-Tuning for LLMs under Shadowy Sparsity", "comment": null, "summary": "The adaptation of pre-trained large language models (LLMs) to diverse\ndownstream tasks via fine-tuning is critical for numerous applications.\nHowever, the inefficiency of parameter-efficient fine-tuning (PEFT) techniques\npresents significant challenges in terms of time investments and operational\ncosts. In this paper, we first introduce a nuanced form of sparsity, termed\nShadowy Sparsity, which is distinctive in fine-tuning and has not been\nadequately addressed for acceleration. Under Shadowy Sparsity, we propose Long\nExposure, an efficient system to accelerate PEFT for LLMs. Long Exposure\ncomprises three key components: Shadowy-sparsity Exposer employs a prolonged\nsensing range to capture more sparsity details under shadowy sparsity;\nSequence-oriented Predictor provides efficient yet accurate predictions to\nhandle large sequence inputs and constantly-evolving parameters; and\nDynamic-aware Operator facilitates more structured computational patterns and\ncoalesced memory accesses, addressing dynamic sparse operations. Extensive\nevaluations show that Long Exposure outperforms state-of-the-arts with up to a\n$2.49\\times$ speedup in end-to-end fine-tuning, offering promising advancements\nin accelerating PEFT for LLMs.", "AI": {"tldr": "Long Exposure\u7cfb\u7edf\u901a\u8fc7\u89e3\u51b3\u5fae\u8c03\u4e2d\u7684Shadowy Sparsity\u95ee\u9898\uff0c\u52a0\u901f\u53c2\u6570\u9ad8\u6548\u5fae\u8c03(PEFT)\uff0c\u5b9e\u73b0\u6700\u9ad82.49\u500d\u7684\u7aef\u5230\u7aef\u5fae\u8c03\u52a0\u901f\u3002", "motivation": "\u53c2\u6570\u9ad8\u6548\u5fae\u8c03(PEFT)\u6280\u672f\u5b58\u5728\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u5728\u65f6\u95f4\u6295\u5165\u548c\u8fd0\u8425\u6210\u672c\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u7279\u522b\u662f\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u7279\u6709\u7684Shadowy Sparsity\u95ee\u9898\u5c1a\u672a\u5f97\u5230\u5145\u5206\u89e3\u51b3\u3002", "method": "Long Exposure\u7cfb\u7edf\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1aShadowy-sparsity Exposer\u4f7f\u7528\u957f\u611f\u77e5\u8303\u56f4\u6355\u83b7\u66f4\u591a\u7a00\u758f\u7ec6\u8282\uff1bSequence-oriented Predictor\u5904\u7406\u5927\u5e8f\u5217\u8f93\u5165\u548c\u4e0d\u65ad\u6f14\u5316\u7684\u53c2\u6570\uff1bDynamic-aware Operator\u89e3\u51b3\u52a8\u6001\u7a00\u758f\u64cd\u4f5c\u95ee\u9898\u3002", "result": "\u5e7f\u6cdb\u8bc4\u4f30\u663e\u793a\uff0cLong Exposure\u5728\u7aef\u5230\u7aef\u5fae\u8c03\u4e2d\u5b9e\u73b0\u4e86\u6700\u9ad82.49\u500d\u7684\u52a0\u901f\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "Long Exposure\u4e3a\u52a0\u901fLLMs\u7684PEFT\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u8fdb\u5c55\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u7684\u6548\u7387\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2510.16340", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16340", "abs": "https://arxiv.org/abs/2510.16340", "authors": ["Pratham Singla", "Shivank Garg", "Ayush Singh", "Ishan Garg", "Ketan Suhaas Saichandran"], "title": "Thinking About Thinking: Evaluating Reasoning in Post-Trained Language Models", "comment": null, "summary": "Recent advances in post-training techniques have endowed Large Language\nModels (LLMs) with enhanced capabilities for tackling complex, logic-intensive\ntasks through the generation of supplementary planning tokens. This development\nraises a fundamental question: Are these models aware of what they \"learn\" and\n\"think\"? To address this, we define three core competencies: (1) awareness of\nlearned latent policies, (2) generalization of these policies across domains,\nand (3) alignment between internal reasoning traces and final outputs. We\nempirically evaluate these abilities on several tasks, each designed to require\nlearning a distinct policy. Furthermore, we contrast the profiles of models\npost-trained via Supervised Fine-Tuning (SFT), Direct Policy Optimization\n(DPO), and Group Relative Policy Optimization (GRPO). Our findings indicate\nthat RL-trained models not only demonstrate greater awareness of their learned\nbehaviors and stronger generalizability to novel, structurally similar tasks\nthan SFT models but also often exhibit weak alignment between their reasoning\ntraces and final outputs, an effect most pronounced in GRPO-trained models.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86LLMs\u5bf9\u81ea\u8eab\u5b66\u4e60\u5185\u5bb9\u7684\u8ba4\u77e5\u80fd\u529b\uff0c\u53d1\u73b0RL\u8bad\u7ec3\u6a21\u578b\u6bd4SFT\u6a21\u578b\u66f4\u4e86\u89e3\u6240\u5b66\u884c\u4e3a\u4e14\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\uff0c\u4f46\u63a8\u7406\u8fc7\u7a0b\u4e0e\u6700\u7ec8\u8f93\u51fa\u7684\u4e00\u81f4\u6027\u8f83\u5f31\u3002", "motivation": "\u63a2\u7d22LLMs\u662f\u5426\u610f\u8bc6\u5230\u81ea\u5df1\u901a\u8fc7\u540e\u8bad\u7ec3\u6280\u672f\u5b66\u5230\u7684\u5185\u5bb9\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u6a21\u578b\u5bf9\u6240\u5b66\u6f5c\u5728\u7b56\u7565\u7684\u8ba4\u77e5\u7a0b\u5ea6\u3002", "method": "\u5b9a\u4e49\u4e86\u4e09\u4e2a\u6838\u5fc3\u80fd\u529b\uff1a\u5bf9\u5b66\u4e60\u7b56\u7565\u7684\u8ba4\u77e5\u3001\u7b56\u7565\u8de8\u9886\u57df\u6cdb\u5316\u3001\u63a8\u7406\u8fc7\u7a0b\u4e0e\u8f93\u51fa\u7684\u4e00\u81f4\u6027\u3002\u5728\u591a\u4e2a\u9700\u8981\u5b66\u4e60\u4e0d\u540c\u7b56\u7565\u7684\u4efb\u52a1\u4e0a\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u5bf9\u6bd4SFT\u3001DPO\u548cGRPO\u4e09\u79cd\u540e\u8bad\u7ec3\u65b9\u6cd5\u3002", "result": "RL\u8bad\u7ec3\u6a21\u578b\u6bd4SFT\u6a21\u578b\u66f4\u4e86\u89e3\u6240\u5b66\u884c\u4e3a\uff0c\u5728\u7ed3\u6784\u76f8\u4f3c\u7684\u65b0\u4efb\u52a1\u4e0a\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\uff0c\u4f46\u63a8\u7406\u8fc7\u7a0b\u4e0e\u6700\u7ec8\u8f93\u51fa\u7684\u4e00\u81f4\u6027\u8f83\u5f31\uff0cGRPO\u8bad\u7ec3\u6a21\u578b\u8fd9\u4e00\u73b0\u8c61\u6700\u660e\u663e\u3002", "conclusion": "LLMs\u901a\u8fc7\u540e\u8bad\u7ec3\u786e\u5b9e\u83b7\u5f97\u4e86\u589e\u5f3a\u80fd\u529b\uff0c\u4f46\u4e0d\u540c\u8bad\u7ec3\u65b9\u6cd5\u5728\u6a21\u578b\u8ba4\u77e5\u80fd\u529b\u3001\u6cdb\u5316\u80fd\u529b\u548c\u63a8\u7406\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002"}}
{"id": "2510.16735", "categories": ["eess.SY", "cs.LG", "cs.SY", "93C40 (Primary) 68T05, 91B82 (Secondary)", "I.2.6; I.2.8; C.2.4; K.4.4"], "pdf": "https://arxiv.org/pdf/2510.16735", "abs": "https://arxiv.org/abs/2510.16735", "authors": ["Aniket Agrawal", "Harsharanga Patil"], "title": "A Control-Theoretic Approach to Dynamic Payment Routing for Success Rate Optimization", "comment": "7 Pages, 8 Figures", "summary": "This paper introduces a control-theoretic framework for dynamic payment\nrouting, implemented within JUSPAY's Payment Orchestrator to maximize\ntransaction success rate. The routing system is modeled as a closed-loop\nfeedback controller continuously sensing gateway performance, computing\ncorrective actions, and dynamically routes transactions across gateway to\nensure operational resilience. The system leverages concepts from control\ntheory, reinforcement learning, and multi-armed bandit optimization to achieve\nboth short-term responsiveness and long-term stability. Rather than relying on\nexplicit PID regulation, the framework applies generalized feedback-based\nadaptation, ensuring that corrective actions remain proportional to observed\nperformance deviations and the computed gateway score gradually converges\ntoward the success rate. This hybrid approach unifies control theory and\nadaptive decision systems, enabling self-regulating transaction routing that\ndampens instability, and improves reliability. Live production results show an\nimprovement of up to 1.15% in success rate over traditional rule-based routing,\ndemonstrating the effectiveness of feedback-based control in payment systems.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u63a7\u5236\u7406\u8bba\u7684\u52a8\u6001\u652f\u4ed8\u8def\u7531\u6846\u67b6\uff0c\u901a\u8fc7\u95ed\u73af\u53cd\u9988\u63a7\u5236\u5668\u5b9e\u65f6\u611f\u77e5\u7f51\u5173\u6027\u80fd\u5e76\u52a8\u6001\u8def\u7531\u4ea4\u6613\uff0c\u5c06\u4ea4\u6613\u6210\u529f\u7387\u63d0\u53471.15%", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u7684\u8def\u7531\u7cfb\u7edf\u7f3a\u4e4f\u52a8\u6001\u9002\u5e94\u6027\uff0c\u65e0\u6cd5\u6709\u6548\u5e94\u5bf9\u7f51\u5173\u6027\u80fd\u6ce2\u52a8\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u81ea\u8c03\u8282\u8def\u7531\u673a\u5236\u6765\u63d0\u5347\u4ea4\u6613\u6210\u529f\u7387", "method": "\u7ed3\u5408\u63a7\u5236\u7406\u8bba\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u591a\u81c2\u8001\u864e\u673a\u4f18\u5316\uff0c\u91c7\u7528\u5e7f\u4e49\u53cd\u9988\u81ea\u9002\u5e94\u673a\u5236\u800c\u975e\u663e\u5f0fPID\u8c03\u8282\uff0c\u4f7f\u7ea0\u6b63\u52a8\u4f5c\u4e0e\u6027\u80fd\u504f\u5dee\u6210\u6bd4\u4f8b\uff0c\u7f51\u5173\u8bc4\u5206\u9010\u6e10\u6536\u655b\u5230\u6210\u529f\u7387", "result": "\u751f\u4ea7\u73af\u5883\u6d4b\u8bd5\u663e\u793a\uff0c\u76f8\u6bd4\u4f20\u7edf\u89c4\u5219\u8def\u7531\uff0c\u6210\u529f\u7387\u63d0\u5347\u6700\u9ad8\u8fbe1.15%\uff0c\u8bc1\u660e\u4e86\u53cd\u9988\u63a7\u5236\u5728\u652f\u4ed8\u7cfb\u7edf\u4e2d\u7684\u6709\u6548\u6027", "conclusion": "\u8be5\u6df7\u5408\u65b9\u6cd5\u7edf\u4e00\u4e86\u63a7\u5236\u7406\u8bba\u548c\u81ea\u9002\u5e94\u51b3\u7b56\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u81ea\u8c03\u8282\u4ea4\u6613\u8def\u7531\uff0c\u6291\u5236\u4e86\u4e0d\u7a33\u5b9a\u6027\u5e76\u63d0\u9ad8\u4e86\u53ef\u9760\u6027"}}
{"id": "2510.16937", "categories": ["stat.ML", "cs.LG", "stat.ME", "G.3"], "pdf": "https://arxiv.org/pdf/2510.16937", "abs": "https://arxiv.org/abs/2510.16937", "authors": ["Vikram Kher", "Argyris Oikonomou", "Manolis Zampetakis"], "title": "Prediction-Augmented Trees for Reliable Statistical Inference", "comment": "45 pages, 9 Figures", "summary": "The remarkable success of machine learning (ML) in predictive tasks has led\nscientists to incorporate ML predictions as a core component of the scientific\ndiscovery pipeline. This was exemplified by the landmark achievement of\nAlphaFold (Jumper et al. (2021)). In this paper, we study how ML predictions\ncan be safely used in statistical analysis of data towards scientific\ndiscovery. In particular, we follow the framework introduced by Angelopoulos et\nal. (2023). In this framework, we assume access to a small set of $n$\ngold-standard labeled samples, a much larger set of $N$ unlabeled samples, and\na ML model that can be used to impute the labels of the unlabeled data points.\nWe introduce two new learning-augmented estimators: (1) Prediction-Augmented\nResidual Tree (PART), and (2) Prediction-Augmented Quadrature (PAQ). Both\nestimators have significant advantages over existing estimators like PPI and\nPPI++ introduced by Angelopoulos et al. (2023) and Angelopoulos et al. (2024),\nrespectively. PART is a decision-tree based estimator built using a greedy\ncriterion. We first characterize PART's asymptotic distribution and demonstrate\nhow to construct valid confidence intervals. Then we show that PART outperforms\nexisting methods in real-world datasets from ecology, astronomy, and census\nreports, among other domains. This leads to estimators with higher confidence,\nwhich is the result of using both the gold-standard samples and the machine\nlearning predictions. Finally, we provide a formal proof of the advantage of\nPART by exploring PAQ, an estimation that arises when considering the limit of\nPART when the depth its tree grows to infinity. Under appropriate assumptions\nin the input data we show that the variance of PAQ shrinks at rate of $O(N^{-1}\n+ n^{-4})$, improving significantly on the $O(N^{-1}+n^{-1})$ rate of existing\nmethods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u7684\u5b66\u4e60\u589e\u5f3a\u4f30\u8ba1\u5668PART\u548cPAQ\uff0c\u7528\u4e8e\u5728\u79d1\u5b66\u53d1\u73b0\u4e2d\u5b89\u5168\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u8fdb\u884c\u7edf\u8ba1\u5206\u6790\u3002PART\u57fa\u4e8e\u51b3\u7b56\u6811\u6784\u5efa\uff0cPAQ\u662f\u5176\u6781\u9650\u60c5\u51b5\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5PPI\u548cPPI++\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002", "motivation": "\u968f\u7740\u673a\u5668\u5b66\u4e60\u5728\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u6210\u529f\uff0c\u79d1\u5b66\u5bb6\u4eec\u5f00\u59cb\u5c06ML\u9884\u6d4b\u4f5c\u4e3a\u79d1\u5b66\u53d1\u73b0\u6d41\u7a0b\u7684\u6838\u5fc3\u7ec4\u6210\u90e8\u5206\u3002\u672c\u6587\u7814\u7a76\u5982\u4f55\u5728\u79d1\u5b66\u53d1\u73b0\u7684\u6570\u636e\u7edf\u8ba1\u5206\u6790\u4e2d\u5b89\u5168\u5730\u4f7f\u7528ML\u9884\u6d4b\u3002", "method": "\u5f15\u5165\u4e24\u79cd\u65b0\u4f30\u8ba1\u5668\uff1a\u57fa\u4e8e\u51b3\u7b56\u6811\u7684PART\u548c\u4f5c\u4e3aPART\u6781\u9650\u60c5\u51b5\u7684PAQ\u3002PART\u4f7f\u7528\u8d2a\u5fc3\u51c6\u5219\u6784\u5efa\u51b3\u7b56\u6811\uff0cPAQ\u5728\u6811\u6df1\u5ea6\u8d8b\u4e8e\u65e0\u7a77\u65f6\u4ea7\u751f\u3002", "result": "PART\u5728\u751f\u6001\u5b66\u3001\u5929\u6587\u5b66\u548c\u4eba\u53e3\u666e\u67e5\u7b49\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u6784\u5efa\u66f4\u6709\u6548\u7684\u7f6e\u4fe1\u533a\u95f4\u3002PAQ\u7684\u65b9\u5dee\u4ee5O(N^{-1} + n^{-4})\u7684\u901f\u5ea6\u6536\u7f29\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684O(N^{-1}+n^{-1})\u3002", "conclusion": "PART\u548cPAQ\u4f30\u8ba1\u5668\u901a\u8fc7\u7ed3\u5408\u91d1\u6807\u51c6\u6837\u672c\u548c\u673a\u5668\u5b66\u4e60\u9884\u6d4b\uff0c\u63d0\u4f9b\u4e86\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u4f18\u8d8a\u7684\u7edf\u8ba1\u4f30\u8ba1\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u65b9\u5dee\u6536\u655b\u901f\u5ea6\u65b9\u9762\u6709\u663e\u8457\u6539\u8fdb\u3002"}}
{"id": "2510.16069", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16069", "abs": "https://arxiv.org/abs/2510.16069", "authors": ["Sumbul Khan", "Wei Ting Liow", "Lay Kee Ang"], "title": "Human or AI? Comparing Design Thinking Assessments by Teaching Assistants and Bots", "comment": "to be published in IEEE TALE 2025", "summary": "As design thinking education grows in secondary and tertiary contexts,\neducators face the challenge of evaluating creative artefacts that combine\nvisual and textual elements. Traditional rubric-based assessment is laborious,\ntime-consuming, and inconsistent due to reliance on Teaching Assistants (TA) in\nlarge, multi-section cohorts. This paper presents an exploratory study\ninvestigating the reliability and perceived accuracy of AI-assisted assessment\ncompared to TA-assisted assessment in evaluating student posters in design\nthinking education. Two activities were conducted with 33 Ministry of Education\n(MOE) Singapore school teachers to (1) compare AI-generated scores with TA\ngrading across three key dimensions: empathy and user understanding,\nidentification of pain points and opportunities, and visual communication, and\n(2) examine teacher preferences for AI-assigned, TA-assigned, and hybrid\nscores. Results showed low statistical agreement between instructor and AI\nscores for empathy and pain points, with slightly higher alignment for visual\ncommunication. Teachers preferred TA-assigned scores in six of ten samples.\nQualitative feedback highlighted the potential of AI for formative feedback,\nconsistency, and student self-reflection, but raised concerns about its\nlimitations in capturing contextual nuance and creative insight. The study\nunderscores the need for hybrid assessment models that integrate computational\nefficiency with human insights. This research contributes to the evolving\nconversation on responsible AI adoption in creative disciplines, emphasizing\nthe balance between automation and human judgment for scalable and\npedagogically sound assessment.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8AI\u8f85\u52a9\u8bc4\u4f30\u4e0e\u52a9\u6559\u8bc4\u4f30\u5728\u521b\u610f\u8bbe\u8ba1\u6d77\u62a5\u8bc4\u5206\u4e2d\u7684\u53ef\u9760\u6027\uff0c\u53d1\u73b0AI\u5728\u89c6\u89c9\u6c9f\u901a\u7ef4\u5ea6\u8868\u73b0\u8f83\u597d\uff0c\u4f46\u5728\u5171\u60c5\u548c\u75db\u70b9\u8bc6\u522b\u65b9\u9762\u4e0e\u6559\u5e08\u8bc4\u5206\u4e00\u81f4\u6027\u8f83\u4f4e\uff0c\u6559\u5e08\u66f4\u504f\u597d\u52a9\u6559\u8bc4\u5206\uff0c\u5efa\u8bae\u91c7\u7528\u6df7\u5408\u8bc4\u4f30\u6a21\u578b\u3002", "motivation": "\u8bbe\u8ba1\u601d\u7ef4\u6559\u80b2\u4e2d\u8bc4\u4f30\u521b\u610f\u4f5c\u54c1\u9762\u4e34\u6311\u6218\uff0c\u4f20\u7edf\u57fa\u4e8e\u91cf\u89c4\u7684\u8bc4\u4f30\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u591a\u73ed\u7ea7\u6559\u5b66\u4e2d\u8017\u65f6\u4e14\u4e0d\u4e00\u81f4\uff0c\u9700\u8981\u63a2\u7d22AI\u8f85\u52a9\u8bc4\u4f30\u7684\u53ef\u884c\u6027\u3002", "method": "\u5bf933\u540d\u65b0\u52a0\u5761\u6559\u80b2\u90e8\u6559\u5e08\u8fdb\u884c\u4e24\u9879\u6d3b\u52a8\uff1a\u6bd4\u8f83AI\u751f\u6210\u5206\u6570\u4e0e\u52a9\u6559\u8bc4\u5206\u5728\u4e09\u4e2a\u7ef4\u5ea6\u7684\u5dee\u5f02\uff0c\u4ee5\u53ca\u8c03\u67e5\u6559\u5e08\u5bf9AI\u8bc4\u5206\u3001\u52a9\u6559\u8bc4\u5206\u548c\u6df7\u5408\u8bc4\u5206\u7684\u504f\u597d\u3002", "result": "AI\u4e0e\u6559\u5e08\u8bc4\u5206\u5728\u5171\u60c5\u548c\u75db\u70b9\u8bc6\u522b\u7ef4\u5ea6\u7edf\u8ba1\u4e00\u81f4\u6027\u4f4e\uff0c\u89c6\u89c9\u6c9f\u901a\u7ef4\u5ea6\u4e00\u81f4\u6027\u7a0d\u9ad8\uff1b\u6559\u5e08\u504f\u597d\u52a9\u6559\u8bc4\u5206\uff1b\u5b9a\u6027\u53cd\u9988\u663e\u793aAI\u5728\u5f62\u6210\u6027\u53cd\u9988\u3001\u4e00\u81f4\u6027\u548c\u5b66\u751f\u81ea\u6211\u53cd\u601d\u65b9\u9762\u6709\u6f5c\u529b\uff0c\u4f46\u5728\u6355\u6349\u60c5\u5883\u7ec6\u5fae\u5dee\u522b\u548c\u521b\u610f\u6d1e\u5bdf\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "conclusion": "\u9700\u8981\u6574\u5408\u8ba1\u7b97\u6548\u7387\u4e0e\u4eba\u7c7b\u6d1e\u5bdf\u7684\u6df7\u5408\u8bc4\u4f30\u6a21\u578b\uff0c\u5728\u521b\u610f\u5b66\u79d1\u4e2d\u5e73\u8861\u81ea\u52a8\u5316\u4e0e\u4eba\u7c7b\u5224\u65ad\uff0c\u5b9e\u73b0\u53ef\u6269\u5c55\u4e14\u7b26\u5408\u6559\u5b66\u539f\u7406\u7684\u8bc4\u4f30\u3002"}}
{"id": "2510.16095", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16095", "abs": "https://arxiv.org/abs/2510.16095", "authors": ["Dou Liu", "Ying Long", "Sophia Zuoqiu", "Di Liu", "Kang Li", "Yiting Lin", "Hanyi Liu", "Rong Yin", "Tian Tang"], "title": "Reliability of Large Language Model Generated Clinical Reasoning in Assisted Reproductive Technology: Blinded Comparative Evaluation Study", "comment": null, "summary": "Creating high-quality clinical Chains-of-Thought (CoTs) is crucial for\nexplainable medical Artificial Intelligence (AI) while constrained by data\nscarcity. Although Large Language Models (LLMs) can synthesize medical data,\ntheir clinical reliability remains unverified. This study evaluates the\nreliability of LLM-generated CoTs and investigates prompting strategies to\nenhance their quality. In a blinded comparative study, senior clinicians in\nAssisted Reproductive Technology (ART) evaluated CoTs generated via three\ndistinct strategies: Zero-shot, Random Few-shot (using shallow examples), and\nSelective Few-shot (using diverse, high-quality examples). These expert ratings\nwere compared against evaluations from a state-of-the-art AI model (GPT-4o).\nThe Selective Few-shot strategy significantly outperformed other strategies\nacross all human evaluation metrics (p < .001). Critically, the Random Few-shot\nstrategy offered no significant improvement over the Zero-shot baseline,\ndemonstrating that low-quality examples are as ineffective as no examples. The\nsuccess of the Selective strategy is attributed to two principles:\n\"Gold-Standard Depth\" (reasoning quality) and \"Representative Diversity\"\n(generalization). Notably, the AI evaluator failed to discern these critical\nperformance differences. The clinical reliability of synthetic CoTs is dictated\nby strategic prompt curation, not the mere presence of examples. We propose a\n\"Dual Principles\" framework as a foundational methodology to generate\ntrustworthy data at scale. This work offers a validated solution to the data\nbottleneck and confirms the indispensable role of human expertise in evaluating\nhigh-stakes clinical AI.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86LLM\u751f\u6210\u7684\u4e34\u5e8a\u601d\u7ef4\u94fe\u53ef\u9760\u6027\uff0c\u53d1\u73b0\u9009\u62e9\u6027\u5c11\u6837\u672c\u7b56\u7565\u663e\u8457\u4f18\u4e8e\u96f6\u6837\u672c\u548c\u968f\u673a\u5c11\u6837\u672c\u7b56\u7565\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\"\u9ec4\u91d1\u6807\u51c6\u6df1\u5ea6\"\u548c\"\u4ee3\u8868\u6027\u591a\u6837\u6027\"\u7684\u53cc\u539f\u5219\u6846\u67b6\u3002", "motivation": "\u89e3\u51b3\u9ad8\u8d28\u91cf\u4e34\u5e8a\u601d\u7ef4\u94fe\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u9a8c\u8bc1LLM\u751f\u6210\u533b\u7597\u6570\u636e\u7684\u53ef\u9760\u6027\uff0c\u5e76\u5bfb\u627e\u63d0\u5347\u5176\u8d28\u91cf\u7684\u63d0\u793a\u7b56\u7565\u3002", "method": "\u5728\u8f85\u52a9\u751f\u6b96\u6280\u672f\u9886\u57df\u8fdb\u884c\u76f2\u6cd5\u6bd4\u8f83\u7814\u7a76\uff0c\u7531\u8d44\u6df1\u4e34\u5e8a\u533b\u751f\u8bc4\u4f30\u4e09\u79cd\u63d0\u793a\u7b56\u7565\u751f\u6210\u7684\u601d\u7ef4\u94fe\uff1a\u96f6\u6837\u672c\u3001\u968f\u673a\u5c11\u6837\u672c\u548c\u9009\u62e9\u6027\u5c11\u6837\u672c\uff0c\u5e76\u4e0eGPT-4o\u7684\u8bc4\u4f30\u7ed3\u679c\u5bf9\u6bd4\u3002", "result": "\u9009\u62e9\u6027\u5c11\u6837\u672c\u7b56\u7565\u5728\u6240\u6709\u4eba\u7c7b\u8bc4\u4f30\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u7b56\u7565(p < .001)\uff0c\u968f\u673a\u5c11\u6837\u672c\u7b56\u7565\u76f8\u6bd4\u96f6\u6837\u672c\u57fa\u7ebf\u65e0\u663e\u8457\u6539\u8fdb\uff0cAI\u8bc4\u4f30\u5668\u672a\u80fd\u8bc6\u522b\u5173\u952e\u6027\u80fd\u5dee\u5f02\u3002", "conclusion": "\u5408\u6210\u601d\u7ef4\u94fe\u7684\u4e34\u5e8a\u53ef\u9760\u6027\u53d6\u51b3\u4e8e\u7b56\u7565\u6027\u63d0\u793a\u4f18\u5316\u800c\u975e\u793a\u4f8b\u6570\u91cf\uff0c\u63d0\u51fa\u4e86\"\u53cc\u539f\u5219\"\u6846\u67b6\u4f5c\u4e3a\u53ef\u4fe1\u6570\u636e\u89c4\u6a21\u5316\u751f\u6210\u7684\u57fa\u7840\u65b9\u6cd5\uff0c\u786e\u8ba4\u4e86\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u5728\u9ad8\u98ce\u9669\u4e34\u5e8aAI\u8bc4\u4f30\u4e2d\u7684\u4e0d\u53ef\u6216\u7f3a\u4f5c\u7528\u3002"}}
{"id": "2510.16689", "categories": ["math.OC", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.16689", "abs": "https://arxiv.org/abs/2510.16689", "authors": ["Luca Claude Gino Lebon", "Claudio Altafini"], "title": "Geometric Control Theory Over Networks: Minimal Node Cardinality Disturbance Decoupling Problems", "comment": null, "summary": "In this paper we show how to formulate and solve disturbance decoupling\nproblems over networks while choosing a minimal number of input and output\nnodes. Feedback laws that isolate and eliminate the impact of disturbance nodes\non specific target nodes to be protected are provided using state, output, and\ndynamical feedback. For that, we leverage the fact that when reformulated in\nterms of sets of nodes rather than subspaces, the controlled and conditional\ninvariance properties admit a simple graphical interpretation. For state and\ndynamical feedback, the minimal input and output cardinality solutions can be\ncomputed exactly in polynomial time, via min-cut/max-flow algorithms.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u7f51\u7edc\u4e2d\u89e3\u51b3\u6270\u52a8\u89e3\u8026\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u6700\u5c11\u6570\u91cf\u7684\u8f93\u5165\u548c\u8f93\u51fa\u8282\u70b9\u6765\u9694\u79bb\u548c\u6d88\u9664\u6270\u52a8\u8282\u70b9\u5bf9\u7279\u5b9a\u76ee\u6807\u8282\u70b9\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5728\u7f51\u7edc\u7cfb\u7edf\u4e2d\u6709\u6548\u9694\u79bb\u6270\u52a8\u5f71\u54cd\uff0c\u4fdd\u62a4\u7279\u5b9a\u76ee\u6807\u8282\u70b9\u514d\u53d7\u5e72\u6270\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u6240\u9700\u7684\u63a7\u5236\u8d44\u6e90\u3002", "method": "\u5229\u7528\u8282\u70b9\u96c6\u5408\u800c\u975e\u5b50\u7a7a\u95f4\u7684\u91cd\u65b0\u8868\u8ff0\uff0c\u4f7f\u63a7\u5236\u548c\u6761\u4ef6\u4e0d\u53d8\u6027\u5177\u6709\u7b80\u5355\u7684\u56fe\u5f62\u89e3\u91ca\u3002\u4f7f\u7528\u72b6\u6001\u3001\u8f93\u51fa\u548c\u52a8\u6001\u53cd\u9988\uff0c\u5e76\u901a\u8fc7\u6700\u5c0f\u5272/\u6700\u5927\u6d41\u7b97\u6cd5\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u8ba1\u7b97\u6700\u5c0f\u8f93\u5165\u8f93\u51fa\u8282\u70b9\u89e3\u3002", "result": "\u5bf9\u4e8e\u72b6\u6001\u548c\u52a8\u6001\u53cd\u9988\uff0c\u80fd\u591f\u7cbe\u786e\u8ba1\u7b97\u6700\u5c0f\u8f93\u5165\u548c\u8f93\u51fa\u8282\u70b9\u6570\u91cf\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u9ad8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7f51\u7edc\u6270\u52a8\u89e3\u8026\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u627e\u5230\u6700\u4f18\u7684\u63a7\u5236\u8282\u70b9\u914d\u7f6e\uff0c\u5177\u6709\u91cd\u8981\u7684\u7406\u8bba\u548c\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.15965", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.15965", "abs": "https://arxiv.org/abs/2510.15965", "authors": ["Mohan Zhang", "Yihua Zhang", "Jinghan Jia", "Zhangyang Wang", "Sijia Liu", "Tianlong Chen"], "title": "One Token Embedding Is Enough to Deadlock Your Large Reasoning Model", "comment": "NeurIPS 2025", "summary": "Modern large reasoning models (LRMs) exhibit impressive multi-step\nproblem-solving via chain-of-thought (CoT) reasoning. However, this iterative\nthinking mechanism introduces a new vulnerability surface. We present the\nDeadlock Attack, a resource exhaustion method that hijacks an LRM's generative\ncontrol flow by training a malicious adversarial embedding to induce perpetual\nreasoning loops. Specifically, the optimized embedding encourages transitional\ntokens (e.g., \"Wait\", \"But\") after reasoning steps, preventing the model from\nconcluding its answer. A key challenge we identify is the\ncontinuous-to-discrete projection gap: na\\\"ive projections of adversarial\nembeddings to token sequences nullify the attack. To overcome this, we\nintroduce a backdoor implantation strategy, enabling reliable activation\nthrough specific trigger tokens. Our method achieves a 100% attack success rate\nacross four advanced LRMs (Phi-RM, Nemotron-Nano, R1-Qwen, R1-Llama) and three\nmath reasoning benchmarks, forcing models to generate up to their maximum token\nlimits. The attack is also stealthy (in terms of causing negligible utility\nloss on benign user inputs) and remains robust against existing strategies\ntrying to mitigate the overthinking issue. Our findings expose a critical and\nunderexplored security vulnerability in LRMs from the perspective of reasoning\n(in)efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDeadlock Attack\u7684\u8d44\u6e90\u8017\u5c3d\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3\u6076\u610f\u5bf9\u6297\u5d4c\u5165\u6765\u52ab\u6301\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u751f\u6210\u63a7\u5236\u6d41\uff0c\u8bf1\u5bfc\u6a21\u578b\u9677\u5165\u6c38\u4e45\u63a8\u7406\u5faa\u73af\uff0c\u963b\u6b62\u5176\u8f93\u51fa\u6700\u7ec8\u7b54\u6848\u3002", "motivation": "\u73b0\u4ee3\u5927\u578b\u63a8\u7406\u6a21\u578b\u901a\u8fc7\u601d\u7ef4\u94fe\u63a8\u7406\u5c55\u793a\u51fa\u5f3a\u5927\u7684\u591a\u6b65\u95ee\u9898\u89e3\u51b3\u80fd\u529b\uff0c\u4f46\u8fd9\u79cd\u8fed\u4ee3\u601d\u7ef4\u673a\u5236\u5f15\u5165\u4e86\u65b0\u7684\u5b89\u5168\u6f0f\u6d1e\u3002\u7814\u7a76\u8005\u53d1\u73b0\u53ef\u4ee5\u4ece\u63a8\u7406\u6548\u7387\u7684\u89d2\u5ea6\u653b\u51fb\u8fd9\u4e9b\u6a21\u578b\u3002", "method": "\u4f7f\u7528\u4f18\u5316\u7684\u5bf9\u6297\u5d4c\u5165\u6765\u9f13\u52b1\u6a21\u578b\u5728\u63a8\u7406\u6b65\u9aa4\u540e\u751f\u6210\u8fc7\u6e21\u6027\u6807\u8bb0\uff08\u5982\"Wait\"\u3001\"But\"\uff09\uff0c\u4ece\u800c\u963b\u6b62\u6a21\u578b\u5f97\u51fa\u7ed3\u8bba\u3002\u4e3a\u4e86\u89e3\u51b3\u8fde\u7eed\u5230\u79bb\u6563\u7684\u6295\u5f71\u5dee\u8ddd\u95ee\u9898\uff0c\u5f15\u5165\u4e86\u540e\u95e8\u690d\u5165\u7b56\u7565\uff0c\u901a\u8fc7\u7279\u5b9a\u89e6\u53d1\u6807\u8bb0\u5b9e\u73b0\u53ef\u9760\u6fc0\u6d3b\u3002", "result": "\u5728\u56db\u4e2a\u5148\u8fdb\u7684\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08Phi-RM\u3001Nemotron-Nano\u3001R1-Qwen\u3001R1-Llama\uff09\u548c\u4e09\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u5b9e\u73b0\u4e86100%\u7684\u653b\u51fb\u6210\u529f\u7387\uff0c\u8feb\u4f7f\u6a21\u578b\u751f\u6210\u8fbe\u5230\u6700\u5927\u6807\u8bb0\u9650\u5236\u3002\u653b\u51fb\u5177\u6709\u9690\u853d\u6027\uff0c\u5bf9\u826f\u6027\u7528\u6237\u8f93\u5165\u7684\u6548\u7528\u635f\u5931\u53ef\u5ffd\u7565\uff0c\u4e14\u5bf9\u73b0\u6709\u7f13\u89e3\u8fc7\u5ea6\u601d\u8003\u7684\u7b56\u7565\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u63ed\u793a\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u63a8\u7406\u6548\u7387\u65b9\u9762\u5b58\u5728\u5173\u952e\u4e14\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u8868\u660e\u63a8\u7406\u673a\u5236\u672c\u8eab\u53ef\u80fd\u6210\u4e3a\u65b0\u7684\u653b\u51fb\u9762\u3002"}}
{"id": "2510.16359", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16359", "abs": "https://arxiv.org/abs/2510.16359", "authors": ["Utsav Dhanuka", "Soham Poddar", "Saptarshi Ghosh"], "title": "Utilising Large Language Models for Generating Effective Counter Arguments to Anti-Vaccine Tweets", "comment": "14 pages, 1 figure, work done as a part of B.Tech project at IIT\n  Kharagpur", "summary": "In an era where public health is increasingly influenced by information\nshared on social media, combatting vaccine skepticism and misinformation has\nbecome a critical societal goal. Misleading narratives around vaccination have\nspread widely, creating barriers to achieving high immunisation rates and\nundermining trust in health recommendations. While efforts to detect\nmisinformation have made significant progress, the generation of real time\ncounter-arguments tailored to debunk such claims remains an insufficiently\nexplored area. In this work, we explore the capabilities of LLMs to generate\nsound counter-argument rebuttals to vaccine misinformation. Building on prior\nresearch in misinformation debunking, we experiment with various prompting\nstrategies and fine-tuning approaches to optimise counter-argument generation.\nAdditionally, we train classifiers to categorise anti-vaccine tweets into\nmulti-labeled categories such as concerns about vaccine efficacy, side effects,\nand political influences allowing for more context aware rebuttals. Our\nevaluation, conducted through human judgment, LLM based assessments, and\nautomatic metrics, reveals strong alignment across these methods. Our findings\ndemonstrate that integrating label descriptions and structured fine-tuning\nenhances counter-argument effectiveness, offering a promising approach for\nmitigating vaccine misinformation at scale.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u9488\u5bf9\u75ab\u82d7\u9519\u8bef\u4fe1\u606f\u7684\u5b9e\u65f6\u53cd\u9a73\u8bba\u70b9\uff0c\u901a\u8fc7\u591a\u79cd\u63d0\u793a\u7b56\u7565\u548c\u5fae\u8c03\u65b9\u6cd5\u4f18\u5316\u53cd\u9a73\u751f\u6210\uff0c\u5e76\u8bad\u7ec3\u5206\u7c7b\u5668\u5bf9\u53cd\u75ab\u82d7\u63a8\u6587\u8fdb\u884c\u591a\u6807\u7b7e\u5206\u7c7b\u4ee5\u5b9e\u73b0\u60c5\u5883\u611f\u77e5\u7684\u53cd\u9a73\u3002", "motivation": "\u5728\u793e\u4ea4\u5a92\u4f53\u5f71\u54cd\u516c\u5171\u536b\u751f\u7684\u65f6\u4ee3\uff0c\u6253\u51fb\u75ab\u82d7\u6000\u7591\u8bba\u548c\u9519\u8bef\u4fe1\u606f\u6210\u4e3a\u5173\u952e\u793e\u4f1a\u76ee\u6807\u3002\u867d\u7136\u9519\u8bef\u4fe1\u606f\u68c0\u6d4b\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u751f\u6210\u9488\u5bf9\u8fd9\u4e9b\u4e3b\u5f20\u7684\u5b9e\u65f6\u5b9a\u5236\u53cd\u9a73\u8bba\u70b9\u4ecd\u662f\u7814\u7a76\u4e0d\u8db3\u7684\u9886\u57df\u3002", "method": "\u5b9e\u9a8c\u591a\u79cd\u63d0\u793a\u7b56\u7565\u548c\u5fae\u8c03\u65b9\u6cd5\u4f18\u5316\u53cd\u9a73\u8bba\u70b9\u751f\u6210\uff0c\u8bad\u7ec3\u5206\u7c7b\u5668\u5c06\u53cd\u75ab\u82d7\u63a8\u6587\u5206\u7c7b\u4e3a\u75ab\u82d7\u6548\u529b\u3001\u526f\u4f5c\u7528\u3001\u653f\u6cbb\u5f71\u54cd\u7b49\u591a\u6807\u7b7e\u7c7b\u522b\uff0c\u5b9e\u73b0\u60c5\u5883\u611f\u77e5\u7684\u53cd\u9a73\u3002", "result": "\u901a\u8fc7\u4eba\u5de5\u5224\u65ad\u3001LLM\u8bc4\u4f30\u548c\u81ea\u52a8\u6307\u6807\u7684\u8bc4\u4f30\u663e\u793a\u8fd9\u4e9b\u65b9\u6cd5\u95f4\u5177\u6709\u5f3a\u4e00\u81f4\u6027\u3002\u6574\u5408\u6807\u7b7e\u63cf\u8ff0\u548c\u7ed3\u6784\u5316\u5fae\u8c03\u80fd\u589e\u5f3a\u53cd\u9a73\u8bba\u70b9\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5927\u89c4\u6a21\u51cf\u8f7b\u75ab\u82d7\u9519\u8bef\u4fe1\u606f\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86LLM\u5728\u751f\u6210\u6709\u6548\u53cd\u9a73\u8bba\u70b9\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.16953", "categories": ["eess.SY", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.16953", "abs": "https://arxiv.org/abs/2510.16953", "authors": ["Ersin Das", "William A. Welch", "Patrick Spieler", "Keenan Albee", "Aurelio Noca", "Jeffrey Edlund", "Jonathan Becktor", "Thomas Touma", "Jessica Todd", "Sriramya Bhamidipati", "Stella Kombo", "Maira Saboia", "Anna Sabel", "Grace Lim", "Rohan Thakker", "Amir Rahmani", "Joel W. Burdick"], "title": "Safe Payload Transfer with Ship-Mounted Cranes: A Robust Model Predictive Control Approach", "comment": null, "summary": "Ensuring safe real-time control of ship-mounted cranes in unstructured\ntransportation environments requires handling multiple safety constraints while\nmaintaining effective payload transfer performance. Unlike traditional crane\nsystems, ship-mounted cranes are consistently subjected to significant external\ndisturbances affecting underactuated crane dynamics due to the ship's dynamic\nmotion response to harsh sea conditions, which can lead to robustness issues.\nTo tackle these challenges, we propose a robust and safe model predictive\ncontrol (MPC) framework and demonstrate it on a 5-DOF crane system, where a\nStewart platform simulates the external disturbances that ocean surface motions\nwould have on the supporting ship. The crane payload transfer operation must\navoid obstacles and accurately place the payload within a designated target\narea. We use a robust zero-order control barrier function (R-ZOCBF)-based\nsafety constraint in the nonlinear MPC to ensure safe payload positioning,\nwhile time-varying bounding boxes are utilized for collision avoidance. We\nintroduce a new optimization-based online robustness parameter adaptation\nscheme to reduce the conservativeness of R-ZOCBFs. Experimental trials on a\ncrane prototype demonstrate the overall performance of our safe control\napproach under significant perturbing motions of the crane base. While our\nfocus is on crane-facilitated transfer, the methods more generally apply to\nsafe robotically-assisted parts mating and parts insertion.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8239\u8f7d\u8d77\u91cd\u673a\u5728\u6076\u52a3\u6d77\u51b5\u4e0b\u7684\u9c81\u68d2\u5b89\u5168\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u96f6\u9636\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u786e\u4fdd\u8f7d\u8377\u5b89\u5168\u5b9a\u4f4d\uff0c\u5e76\u91c7\u7528\u65f6\u53d8\u8fb9\u754c\u6846\u8fdb\u884c\u907f\u969c\u3002", "motivation": "\u8239\u8f7d\u8d77\u91cd\u673a\u5728\u975e\u7ed3\u6784\u5316\u8fd0\u8f93\u73af\u5883\u4e2d\u9762\u4e34\u591a\u91cd\u5b89\u5168\u7ea6\u675f\u548c\u5916\u90e8\u6270\u52a8\u6311\u6218\uff0c\u9700\u8981\u540c\u65f6\u4fdd\u8bc1\u5b89\u5168\u6027\u548c\u6709\u6548\u8f7d\u8377\u4f20\u8f93\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u9c81\u68d2\u96f6\u9636\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u7684\u5b89\u5168\u7ea6\u675f\u975e\u7ebf\u6027MPC\uff0c\u7ed3\u5408\u65f6\u53d8\u8fb9\u754c\u6846\u907f\u969c\uff0c\u5e76\u5f15\u5165\u5728\u7ebf\u9c81\u68d2\u6027\u53c2\u6570\u81ea\u9002\u5e94\u65b9\u6848\u3002", "result": "\u5728\u8d77\u91cd\u673a\u539f\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u663e\u8457\u57fa\u5ea7\u6270\u52a8\u4e0b\u5177\u6709\u826f\u597d\u7684\u6574\u4f53\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u9002\u7528\u4e8e\u8d77\u91cd\u673a\u8f7d\u8377\u4f20\u8f93\uff0c\u8fd8\u53ef\u63a8\u5e7f\u5230\u5b89\u5168\u673a\u5668\u4eba\u8f85\u52a9\u90e8\u4ef6\u914d\u5408\u548c\u63d2\u5165\u5e94\u7528\u3002"}}
{"id": "2510.16986", "categories": ["stat.ML", "cs.LG", "stat.OT"], "pdf": "https://arxiv.org/pdf/2510.16986", "abs": "https://arxiv.org/abs/2510.16986", "authors": ["Hamza Cherkaoui", "H\u00e9l\u00e8ne Halconruy", "Yohan Petetin"], "title": "Adaptive Sample Sharing for Linear Regression", "comment": null, "summary": "In many business settings, task-specific labeled data are scarce or costly to\nobtain, which limits supervised learning on a specific task. To address this\nchallenge, we study sample sharing in the case of ridge regression: leveraging\nan auxiliary data set while explicitly protecting against negative transfer. We\nintroduce a principled, data-driven rule that decides how many samples from an\nauxiliary dataset to add to the target training set. The rule is based on an\nestimate of the transfer gain i.e. the marginal reduction in the predictive\nerror. Building on this estimator, we derive finite-sample guaranties: under\nstandard conditions, the procedure borrows when it improves parameter\nestimation and abstains otherwise. In the Gaussian feature setting, we analyze\nwhich data set properties ensure that borrowing samples reduces the predictive\nerror. We validate the approach in synthetic and real datasets, observing\nconsistent gains over strong baselines and single-task training while avoiding\nnegative transfer.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5cad\u56de\u5f52\u7684\u6837\u672c\u5171\u4eab\u65b9\u6cd5\uff0c\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u89c4\u5219\u51b3\u5b9a\u4ece\u8f85\u52a9\u6570\u636e\u96c6\u4e2d\u501f\u7528\u591a\u5c11\u6837\u672c\uff0c\u4ee5\u907f\u514d\u8d1f\u8fc1\u79fb\u5e76\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u5728\u5546\u4e1a\u573a\u666f\u4e2d\uff0c\u4efb\u52a1\u7279\u5b9a\u7684\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u4e14\u83b7\u53d6\u6210\u672c\u9ad8\uff0c\u9650\u5236\u4e86\u76d1\u7763\u5b66\u4e60\u7684\u5e94\u7528\u3002\u9700\u8981\u5229\u7528\u8f85\u52a9\u6570\u636e\u96c6\u540c\u65f6\u907f\u514d\u8d1f\u8fc1\u79fb\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u8f6c\u79fb\u589e\u76ca\u4f30\u8ba1\u7684\u6837\u672c\u5171\u4eab\u89c4\u5219\uff0c\u901a\u8fc7\u4f30\u8ba1\u9884\u6d4b\u8bef\u5dee\u7684\u8fb9\u9645\u51cf\u5c11\u6765\u51b3\u5b9a\u501f\u7528\u591a\u5c11\u8f85\u52a9\u6837\u672c\u3002\u5728Gaussian\u7279\u5f81\u8bbe\u7f6e\u4e0b\u5206\u6790\u786e\u4fdd\u501f\u7528\u6837\u672c\u80fd\u51cf\u5c11\u9884\u6d4b\u8bef\u5dee\u7684\u6570\u636e\u96c6\u7279\u6027\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u548c\u5355\u4efb\u52a1\u8bad\u7ec3\u83b7\u5f97\u4e86\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u6210\u529f\u907f\u514d\u4e86\u8d1f\u8fc1\u79fb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u539f\u5219\u6027\u7684\u6837\u672c\u5171\u4eab\u7b56\u7565\uff0c\u80fd\u591f\u5728\u501f\u7528\u8f85\u52a9\u6570\u636e\u65f6\u786e\u4fdd\u6027\u80fd\u6539\u8fdb\uff0c\u4e3a\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\u7684\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.16081", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16081", "abs": "https://arxiv.org/abs/2510.16081", "authors": ["Jiaye Yang", "Xinyu Zhao", "Tianlong Chen", "Kandyce Brennan"], "title": "SARHAchat: An LLM-Based Chatbot for Sexual and Reproductive Health Counseling", "comment": "5 pages, 1 figure", "summary": "While Artificial Intelligence (AI) shows promise in healthcare applications,\nexisting conversational systems often falter in complex and sensitive medical\ndomains such as Sexual and Reproductive Health (SRH). These systems frequently\nstruggle with hallucination and lack the specialized knowledge required,\nparticularly for sensitive SRH topics. Furthermore, current AI approaches in\nhealthcare tend to prioritize diagnostic capabilities over comprehensive\npatient care and education. Addressing these gaps, this work at the UNC School\nof Nursing introduces SARHAchat, a proof-of-concept Large Language Model\n(LLM)-based chatbot. SARHAchat is designed as a reliable, user-centered system\nintegrating medical expertise with empathetic communication to enhance SRH care\ndelivery. Our evaluation demonstrates SARHAchat's ability to provide accurate\nand contextually appropriate contraceptive counseling while maintaining a\nnatural conversational flow. The demo is available at\nhttps://sarhachat.com/}{https://sarhachat.com/.", "AI": {"tldr": "SARHAchat\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u804a\u5929\u673a\u5668\u4eba\uff0c\u4e13\u95e8\u4e3a\u6027\u5065\u5eb7\u548c\u751f\u6b96\u5065\u5eb7\u9886\u57df\u8bbe\u8ba1\uff0c\u6574\u5408\u533b\u5b66\u4e13\u4e1a\u77e5\u8bc6\u548c\u5171\u60c5\u6c9f\u901a\uff0c\u63d0\u4f9b\u51c6\u786e\u3001\u60c5\u5883\u9002\u5f53\u7684\u907f\u5b55\u54a8\u8be2\u670d\u52a1\u3002", "motivation": "\u73b0\u6709AI\u5bf9\u8bdd\u7cfb\u7edf\u5728\u590d\u6742\u654f\u611f\u7684\u533b\u7597\u9886\u57df\uff08\u5982\u6027\u5065\u5eb7\u548c\u751f\u6b96\u5065\u5eb7\uff09\u8868\u73b0\u4e0d\u4f73\uff0c\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\u548c\u4e13\u4e1a\u77e5\u8bc6\u4e0d\u8db3\uff0c\u4e14\u5f53\u524d\u533b\u7597AI\u65b9\u6cd5\u8fc7\u4e8e\u4fa7\u91cd\u8bca\u65ad\u80fd\u529b\u800c\u5ffd\u89c6\u5168\u9762\u7684\u60a3\u8005\u62a4\u7406\u548c\u6559\u80b2\u3002", "method": "\u5f00\u53d1\u4e86SARHAchat\u6982\u5ff5\u9a8c\u8bc1\u7cfb\u7edf\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u804a\u5929\u673a\u5668\u4eba\uff0c\u6574\u5408\u533b\u5b66\u4e13\u4e1a\u77e5\u8bc6\u4e0e\u5171\u60c5\u6c9f\u901a\uff0c\u4e13\u6ce8\u4e8e\u6027\u5065\u5eb7\u548c\u751f\u6b96\u5065\u5eb7\u62a4\u7406\u3002", "result": "\u8bc4\u4f30\u663e\u793aSARHAchat\u80fd\u591f\u63d0\u4f9b\u51c6\u786e\u4e14\u60c5\u5883\u9002\u5f53\u7684\u907f\u5b55\u54a8\u8be2\u670d\u52a1\uff0c\u540c\u65f6\u4fdd\u6301\u81ea\u7136\u7684\u5bf9\u8bdd\u6d41\u7a0b\u3002", "conclusion": "SARHAchat\u5c55\u793a\u4e86\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u533b\u5b66\u4e13\u4e1a\u77e5\u8bc6\u7ed3\u5408\uff0c\u4e3a\u654f\u611f\u533b\u7597\u9886\u57df\u63d0\u4f9b\u53ef\u9760\u3001\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u62a4\u7406\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2510.16193", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16193", "abs": "https://arxiv.org/abs/2510.16193", "authors": ["Elija Perrier"], "title": "Operationalising Extended Cognition: Formal Metrics for Corporate Knowledge and Legal Accountability", "comment": "Under review", "summary": "Corporate responsibility turns on notions of corporate \\textit{mens rea},\ntraditionally imputed from human agents. Yet these assumptions are under\nchallenge as generative AI increasingly mediates enterprise decision-making.\nBuilding on the theory of extended cognition, we argue that in response\ncorporate knowledge may be redefined as a dynamic capability, measurable by the\nefficiency of its information-access procedures and the validated reliability\nof their outputs. We develop a formal model that captures epistemic states of\ncorporations deploying sophisticated AI or information systems, introducing a\ncontinuous organisational knowledge metric $S_S(\\varphi)$ which integrates a\npipeline's computational cost and its statistically validated error rate. We\nderive a thresholded knowledge predicate $\\mathsf{K}_S$ to impute knowledge and\na firm-wide epistemic capacity index $\\mathcal{K}_{S,t}$ to measure overall\ncapability. We then operationally map these quantitative metrics onto the legal\nstandards of actual knowledge, constructive knowledge, wilful blindness, and\nrecklessness. Our work provides a pathway towards creating measurable and\njusticiable audit artefacts, that render the corporate mind tractable and\naccountable in the algorithmic age.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4f01\u4e1a\u77e5\u8bc6\u5b9a\u4e49\u65b9\u6cd5\uff0c\u5c06\u4f01\u4e1a\u77e5\u8bc6\u89c6\u4e3a\u52a8\u6001\u80fd\u529b\uff0c\u901a\u8fc7\u4fe1\u606f\u8bbf\u95ee\u7a0b\u5e8f\u7684\u6548\u7387\u548c\u8f93\u51fa\u53ef\u9760\u6027\u6765\u8861\u91cf\uff0c\u4e3aAI\u65f6\u4ee3\u7684\u4f01\u4e1a\u8d23\u4efb\u8ba4\u5b9a\u63d0\u4f9b\u53ef\u91cf\u5316\u7684\u6cd5\u5f8b\u6807\u51c6\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u5728\u4f01\u4e1a\u51b3\u7b56\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f20\u7edf\u7684\u57fa\u4e8e\u4eba\u7c7b\u4ee3\u7406\u4eba\u7684\u4f01\u4e1a\u72af\u7f6a\u610f\u56fe\u8ba4\u5b9a\u65b9\u6cd5\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u91cd\u65b0\u5b9a\u4e49\u4f01\u4e1a\u77e5\u8bc6\u7684\u6982\u5ff5\u4ee5\u9002\u5e94\u7b97\u6cd5\u65f6\u4ee3\u3002", "method": "\u57fa\u4e8e\u6269\u5c55\u8ba4\u77e5\u7406\u8bba\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u5f62\u5f0f\u5316\u6a21\u578b\uff0c\u901a\u8fc7\u6574\u5408\u8ba1\u7b97\u6210\u672c\u548c\u7edf\u8ba1\u9a8c\u8bc1\u9519\u8bef\u7387\u6765\u5ea6\u91cf\u4f01\u4e1a\u77e5\u8bc6\u72b6\u6001\uff0c\u5b9a\u4e49\u4e86\u8fde\u7eed\u7684\u7ec4\u7ec7\u77e5\u8bc6\u5ea6\u91cf$S_S(\\varphi)$\u548c\u77e5\u8bc6\u8c13\u8bcd$\\mathsf{K}_S$\u3002", "result": "\u5efa\u7acb\u4e86\u53ef\u91cf\u5316\u7684\u4f01\u4e1a\u77e5\u8bc6\u5ea6\u91cf\u4f53\u7cfb\uff0c\u5305\u62ec\u4f01\u4e1a\u7ea7\u8ba4\u77e5\u80fd\u529b\u6307\u6570$\\mathcal{K}_{S,t}$\uff0c\u5e76\u5c06\u8fd9\u4e9b\u6307\u6807\u6620\u5c04\u5230\u5b9e\u9645\u77e5\u8bc6\u3001\u63a8\u5b9a\u77e5\u8bc6\u3001\u6545\u610f\u89c6\u800c\u4e0d\u89c1\u548c\u9c81\u83bd\u7b49\u6cd5\u5f8b\u6807\u51c6\u4e0a\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5728\u7b97\u6cd5\u65f6\u4ee3\u521b\u5efa\u53ef\u6d4b\u91cf\u548c\u53ef\u88c1\u51b3\u7684\u5ba1\u8ba1\u8bc1\u636e\u63d0\u4f9b\u4e86\u8def\u5f84\uff0c\u4f7f\u4f01\u4e1a\u601d\u7ef4\u53d8\u5f97\u53ef\u8ffd\u8e2a\u548c\u53ef\u95ee\u8d23\u3002"}}
{"id": "2510.16725", "categories": ["math.OC", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.16725", "abs": "https://arxiv.org/abs/2510.16725", "authors": ["Yongchun Bi", "Panyu Deng", "Jun Zheng", "Guchuan Zhu"], "title": "Local integral input-to-state stability for non-autonomous infinite-dimensional systems", "comment": null, "summary": "In this paper, we prove comparison principles for nonlinear differential\nequations with time-varying coefficients and develop Lyapunov analytical tools\nfor the integral input-to-state stability (iISS) analysis of nonlinear\nnon-autonomous infinite-dimensional systems, which involve nonlinearities\nsatisfying a superlinear growth, {bringing} difficulties to the iISS\n{analysis.} Specifically, our approach starts by establishing several forms of\ncomparison principles for a wide range of ordinary differential equations\nhaving time-varying coefficients and superlinear terms, paving the way to\nconduct iISS assessment for general nonlinear non-autonomous\ninfinite-dimensional systems within the Lyapunov stability framework. Then, by\nusing the comparison principles, we prove a local {iISS} {(LiISS)} Lyapunov\ntheorem for the nonlinear non-autonomous infinite-dimensional systems in the\nframework of Banach spaces. {Furthermore,} we provide sufficient conditions of\nthe existence of a local iISS Lyapunonv functional (LiISS-LF) and construct\nLiISS-LFs for the systems in the framework of Hilbert spaces. Finally, we\npreset two examples to illustrate the proposed {Lyapunov} method for the LiISS\nanalysis: one is to show how to obtain the LiISS of a nonlinear\nfinite-dimensional system with time-varying coefficients and superlinear terms\nunder linear state feedback control law while another one is to show how to\nemploy the interpolation inequalities to handle superliner terms and establish\nthe LiISS-LF for a class of multi-dimensional parabolic equations with\nspace-time-varying coefficients. To demonstrate the validity of the results,\nnumerical experiments are also conducted to verify the LiISS of these two\nclasses of systems.", "AI": {"tldr": "\u672c\u6587\u5efa\u7acb\u4e86\u975e\u7ebf\u6027\u5fae\u5206\u65b9\u7a0b\u7684\u6bd4\u8f83\u539f\u7406\uff0c\u5e76\u5f00\u53d1\u4e86Lyapunov\u5206\u6790\u5de5\u5177\uff0c\u7528\u4e8e\u5206\u6790\u5177\u6709\u8d85\u7ebf\u6027\u589e\u957f\u7684\u975e\u7ebf\u6027\u975e\u81ea\u6cbb\u65e0\u9650\u7ef4\u7cfb\u7edf\u7684\u79ef\u5206\u8f93\u5165\u5230\u72b6\u6001\u7a33\u5b9a\u6027(iISS)\u3002", "motivation": "\u7531\u4e8e\u975e\u7ebf\u6027\u7cfb\u7edf\u6ee1\u8db3\u8d85\u7ebf\u6027\u589e\u957f\u7279\u6027\uff0c\u7ed9iISS\u5206\u6790\u5e26\u6765\u56f0\u96be\uff0c\u9700\u8981\u5f00\u53d1\u65b0\u7684\u5206\u6790\u5de5\u5177\u6765\u5904\u7406\u8fd9\u7c7b\u975e\u7ebf\u6027\u975e\u81ea\u6cbb\u65e0\u9650\u7ef4\u7cfb\u7edf\u3002", "method": "\u9996\u5148\u5efa\u7acb\u5177\u6709\u65f6\u53d8\u7cfb\u6570\u548c\u8d85\u7ebf\u6027\u9879\u7684\u5e38\u5fae\u5206\u65b9\u7a0b\u7684\u6bd4\u8f83\u539f\u7406\uff0c\u7136\u540e\u5229\u7528\u8fd9\u4e9b\u539f\u7406\u5728Banach\u7a7a\u95f4\u6846\u67b6\u4e0b\u8bc1\u660e\u5c40\u90e8iISS Lyapunov\u5b9a\u7406\uff0c\u5e76\u5728Hilbert\u7a7a\u95f4\u6846\u67b6\u4e0b\u6784\u5efaLiISS-Lyapunov\u6cdb\u51fd\u3002", "result": "\u8bc1\u660e\u4e86\u5c40\u90e8iISS Lyapunov\u5b9a\u7406\uff0c\u63d0\u4f9b\u4e86LiISS-Lyapunov\u6cdb\u51fd\u5b58\u5728\u7684\u5145\u5206\u6761\u4ef6\uff0c\u5e76\u901a\u8fc7\u4e24\u4e2a\u4f8b\u5b50\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff1a\u4e00\u4e2a\u662f\u6709\u9650\u7ef4\u7cfb\u7edf\uff0c\u53e6\u4e00\u4e2a\u662f\u591a\u7ef4\u629b\u7269\u65b9\u7a0b\u3002", "conclusion": "\u63d0\u51fa\u7684Lyapunov\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5206\u6790\u5177\u6709\u8d85\u7ebf\u6027\u589e\u957f\u7684\u975e\u7ebf\u6027\u975e\u81ea\u6cbb\u65e0\u9650\u7ef4\u7cfb\u7edf\u7684\u79ef\u5206\u8f93\u5165\u5230\u72b6\u6001\u7a33\u5b9a\u6027\uff0c\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7ed3\u679c\u7684\u6b63\u786e\u6027\u3002"}}
{"id": "2510.15967", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15967", "abs": "https://arxiv.org/abs/2510.15967", "authors": ["Zhengyi Zhong", "Wenzheng Jiang", "Weidong Bao", "Ji Wang", "Cheems Wang", "Guanbo Wang", "Yongheng Deng", "Ju Ren"], "title": "Gains: Fine-grained Federated Domain Adaptation in Open Set", "comment": "Accepted by NeurIPS2025", "summary": "Conventional federated learning (FL) assumes a closed world with a fixed\ntotal number of clients. In contrast, new clients continuously join the FL\nprocess in real-world scenarios, introducing new knowledge. This raises two\ncritical demands: detecting new knowledge, i.e., knowledge discovery, and\nintegrating it into the global model, i.e., knowledge adaptation. Existing\nresearch focuses on coarse-grained knowledge discovery, and often sacrifices\nsource domain performance and adaptation efficiency. To this end, we propose a\nfine-grained federated domain adaptation approach in open set (Gains). Gains\nsplits the model into an encoder and a classifier, empirically revealing\nfeatures extracted by the encoder are sensitive to domain shifts while\nclassifier parameters are sensitive to class increments. Based on this, we\ndevelop fine-grained knowledge discovery and contribution-driven aggregation\ntechniques to identify and incorporate new knowledge. Additionally, an\nanti-forgetting mechanism is designed to preserve source domain performance,\nensuring balanced adaptation. Experimental results on multi-domain datasets\nacross three typical data-shift scenarios demonstrate that Gains significantly\noutperforms other baselines in performance for both source-domain and\ntarget-domain clients. Code is available at:\nhttps://github.com/Zhong-Zhengyi/Gains.", "AI": {"tldr": "\u63d0\u51fa\u4e86Gains\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u65b0\u5ba2\u6237\u7aef\u4e0d\u65ad\u52a0\u5165\u5e26\u6765\u7684\u77e5\u8bc6\u53d1\u73b0\u548c\u9002\u5e94\u95ee\u9898\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u77e5\u8bc6\u53d1\u73b0\u548c\u8d21\u732e\u9a71\u52a8\u805a\u5408\u6765\u6574\u5408\u65b0\u77e5\u8bc6\uff0c\u540c\u65f6\u4fdd\u6301\u6e90\u57df\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u8054\u90a6\u5b66\u4e60\u9762\u4e34\u65b0\u5ba2\u6237\u7aef\u6301\u7eed\u52a0\u5165\u7684\u6311\u6218\uff0c\u9700\u8981\u68c0\u6d4b\u65b0\u77e5\u8bc6\u5e76\u6574\u5408\u5230\u5168\u5c40\u6a21\u578b\u4e2d\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u77e5\u8bc6\u53d1\u73b0\u7c92\u5ea6\u3001\u6e90\u57df\u6027\u80fd\u4fdd\u6301\u548c\u9002\u5e94\u6548\u7387\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u5c06\u6a21\u578b\u5206\u4e3a\u7f16\u7801\u5668\u548c\u5206\u7c7b\u5668\uff0c\u57fa\u4e8e\u7f16\u7801\u5668\u5bf9\u57df\u504f\u79fb\u654f\u611f\u3001\u5206\u7c7b\u5668\u5bf9\u7c7b\u522b\u589e\u91cf\u654f\u611f\u7684\u7279\u6027\uff0c\u5f00\u53d1\u7ec6\u7c92\u5ea6\u77e5\u8bc6\u53d1\u73b0\u3001\u8d21\u732e\u9a71\u52a8\u805a\u5408\u548c\u6297\u9057\u5fd8\u673a\u5236\u3002", "result": "\u5728\u4e09\u4e2a\u5178\u578b\u6570\u636e\u504f\u79fb\u573a\u666f\u7684\u591a\u57df\u6570\u636e\u96c6\u5b9e\u9a8c\u4e2d\uff0cGains\u5728\u6e90\u57df\u548c\u76ee\u6807\u57df\u5ba2\u6237\u7aef\u6027\u80fd\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "Gains\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5f00\u653e\u4e16\u754c\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u77e5\u8bc6\u53d1\u73b0\u548c\u9002\u5e94\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6e90\u57df\u548c\u76ee\u6807\u57df\u6027\u80fd\u7684\u5e73\u8861\u63d0\u5347\u3002"}}
{"id": "2510.16363", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16363", "abs": "https://arxiv.org/abs/2510.16363", "authors": ["Nilmadhab Das", "Vishal Vaibhav", "Yash Sunil Choudhary", "V. Vijaya Saradhi", "Ashish Anand"], "title": "End-to-End Argument Mining through Autoregressive Argumentative Structure Prediction", "comment": "Accepted version. To appear in IJCNN 2025", "summary": "Argument Mining (AM) helps in automating the extraction of complex\nargumentative structures such as Argument Components (ACs) like Premise, Claim\netc. and Argumentative Relations (ARs) like Support, Attack etc. in an\nargumentative text. Due to the inherent complexity of reasoning involved with\nthis task, modelling dependencies between ACs and ARs is challenging. Most of\nthe recent approaches formulate this task through a generative paradigm by\nflattening the argumentative structures. In contrast to that, this study\njointly formulates the key tasks of AM in an end-to-end fashion using\nAutoregressive Argumentative Structure Prediction (AASP) framework. The\nproposed AASP framework is based on the autoregressive structure prediction\nframework that has given good performance for several NLP tasks. AASP framework\nmodels the argumentative structures as constrained pre-defined sets of actions\nwith the help of a conditional pre-trained language model. These actions build\nthe argumentative structures step-by-step in an autoregressive manner to\ncapture the flow of argumentative reasoning in an efficient way. Extensive\nexperiments conducted on three standard AM benchmarks demonstrate that AASP\nachieves state-of-theart (SoTA) results across all AM tasks in two benchmarks\nand delivers strong results in one benchmark.", "AI": {"tldr": "\u63d0\u51fa\u4e86AASP\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u56de\u5f52\u7ed3\u6784\u9884\u6d4b\u8054\u5408\u5efa\u6a21\u8bba\u8bc1\u7ec4\u4ef6\u548c\u5173\u7cfb\uff0c\u5728\u4e09\u4e2a\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86SOTA\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u751f\u6210\u8303\u5f0f\u6241\u5e73\u5316\u8bba\u8bc1\u7ed3\u6784\uff0c\u96be\u4ee5\u6355\u6349\u8bba\u8bc1\u7ec4\u4ef6\u548c\u5173\u7cfb\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "\u57fa\u4e8e\u81ea\u56de\u5f52\u7ed3\u6784\u9884\u6d4b\u6846\u67b6\uff0c\u5c06\u8bba\u8bc1\u7ed3\u6784\u5efa\u6a21\u4e3a\u9884\u5b9a\u4e49\u52a8\u4f5c\u96c6\uff0c\u4f7f\u7528\u6761\u4ef6\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u9010\u6b65\u6784\u5efa\u8bba\u8bc1\u7ed3\u6784\u3002", "result": "\u5728\u4e09\u4e2a\u6807\u51c6AM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u7684\u6240\u6709\u4efb\u52a1\u90fd\u8fbe\u5230\u4e86SOTA\u7ed3\u679c\uff0c\u4e00\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u8868\u73b0\u5f3a\u52b2\u3002", "conclusion": "AASP\u6846\u67b6\u80fd\u6709\u6548\u6355\u6349\u8bba\u8bc1\u63a8\u7406\u6d41\u7a0b\uff0c\u5728\u8bba\u8bc1\u6316\u6398\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.17071", "categories": ["eess.SY", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.17071", "abs": "https://arxiv.org/abs/2510.17071", "authors": ["Samuel Talkington", "Daniel Turizo", "Sergio A. Dorado-Rojas", "Rahul K. Gupta", "Daniel K. Molzahn"], "title": "Differentiating Through Power Flow Solutions for Admittance and Topology Control", "comment": "10 pages, 6 figures", "summary": "The power flow equations relate bus voltage phasors to power injections via\nthe network admittance matrix. These equations are central to the key\noperational and protection functions of power systems (e.g., optimal power flow\nscheduling and control, state estimation, protection, and fault location, among\nothers). As control, optimization, and estimation of network admittance\nparameters are central to multiple avenues of research in electric power\nsystems, we propose a linearization of power flow solutions obtained by\nimplicitly differentiating them with respect to the network admittance\nparameters. This is achieved by utilizing the implicit function theorem, in\nwhich we show that such a differentiation is guaranteed to exist under mild\nconditions and is applicable to generic power systems (radial or meshed). The\nproposed theory is applied to derive sensitivities of complex voltages, line\ncurrents, and power flows. The developed theory of linearizing the power flow\nequations around changes in the complex network admittance parameters has\nnumerous applications. We demonstrate several of these applications, such as\npredicting the nodal voltages when the network topology changes without solving\nthe power flow equations. We showcase the application for continuous admittance\ncontrol, which is used to increase the hosting capacity of a given distribution\nnetwork.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9690\u51fd\u6570\u5b9a\u7406\u7684\u6f6e\u6d41\u65b9\u7a0b\u7ebf\u6027\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u5206\u6790\u7f51\u7edc\u5bfc\u7eb3\u53c2\u6570\u53d8\u5316\u5bf9\u7cfb\u7edf\u72b6\u6001\u7684\u5f71\u54cd\u3002", "motivation": "\u7535\u529b\u7cfb\u7edf\u4e2d\u7f51\u7edc\u5bfc\u7eb3\u53c2\u6570\u7684\u63a7\u5236\u3001\u4f18\u5316\u548c\u4f30\u8ba1\u5bf9\u591a\u4e2a\u7814\u7a76\u65b9\u5411\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u5206\u6790\u5bfc\u7eb3\u53c2\u6570\u53d8\u5316\u5bf9\u7cfb\u7edf\u7684\u5f71\u54cd\u3002", "method": "\u5229\u7528\u9690\u51fd\u6570\u5b9a\u7406\u5bf9\u6f6e\u6d41\u65b9\u7a0b\u8fdb\u884c\u9690\u5f0f\u5fae\u5206\uff0c\u63a8\u5bfc\u51fa\u5173\u4e8e\u7f51\u7edc\u5bfc\u7eb3\u53c2\u6570\u7684\u7ebf\u6027\u5316\u8868\u8fbe\u5f0f\uff0c\u9002\u7528\u4e8e\u8f90\u5c04\u72b6\u6216\u7f51\u72b6\u7535\u7f51\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u8ba1\u7b97\u590d\u6742\u7535\u538b\u3001\u7ebf\u8def\u7535\u6d41\u548c\u529f\u7387\u6d41\u7684\u7075\u654f\u5ea6\uff0c\u65e0\u9700\u91cd\u65b0\u6c42\u89e3\u6f6e\u6d41\u65b9\u7a0b\u5373\u53ef\u9884\u6d4b\u7f51\u7edc\u62d3\u6251\u53d8\u5316\u65f6\u7684\u8282\u70b9\u7535\u538b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7ebf\u6027\u5316\u7406\u8bba\u5728\u8fde\u7eed\u5bfc\u7eb3\u63a7\u5236\u7b49\u5e94\u7528\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u53ef\u7528\u4e8e\u63d0\u9ad8\u914d\u7535\u7f51\u7684\u627f\u8f7d\u80fd\u529b\u3002"}}
{"id": "2510.17063", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17063", "abs": "https://arxiv.org/abs/2510.17063", "authors": ["Shunan Sheng", "Bohan Wu", "Alberto Gonz\u00e1lez-Sanz"], "title": "Mode Collapse of Mean-Field Variational Inference", "comment": null, "summary": "Mean-field variational inference (MFVI) is a widely used method for\napproximating high-dimensional probability distributions by product measures.\nIt has been empirically observed that MFVI optimizers often suffer from mode\ncollapse. Specifically, when the target measure $\\pi$ is a mixture $\\pi = w P_0\n+ (1 - w) P_1$, the MFVI optimizer tends to place most of its mass near a\nsingle component of the mixture. This work provides the first theoretical\nexplanation of mode collapse in MFVI. We introduce the notion to capture the\nseparatedness of the two mixture components -- called\n$\\varepsilon$-separateness -- and derive explicit bounds on the fraction of\nmass that any MFVI optimizer assigns to each component when $P_0$ and $P_1$ are\n$\\varepsilon$-separated for sufficiently small $\\varepsilon$. Our results\nsuggest that the occurrence of mode collapse crucially depends on the relative\nposition of the components. To address this issue, we propose the rotational\nvariational inference (RoVI), which augments MFVI with a rotation matrix. The\nnumerical studies support our theoretical findings and demonstrate the benefits\nof RoVI.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u4ece\u7406\u8bba\u4e0a\u89e3\u91ca\u4e86\u5e73\u5747\u573a\u53d8\u5206\u63a8\u65ad\u4e2d\u7684\u6a21\u6001\u574d\u584c\u73b0\u8c61\uff0c\u63d0\u51fa\u4e86\u03b5-\u5206\u79bb\u5ea6\u7684\u6982\u5ff5\u6765\u8861\u91cf\u6df7\u5408\u6210\u5206\u7684\u5206\u79bb\u7a0b\u5ea6\uff0c\u5e76\u5f00\u53d1\u4e86\u65cb\u8f6c\u53d8\u5206\u63a8\u65ad\u65b9\u6cd5\u6765\u89e3\u51b3\u8be5\u95ee\u9898\u3002", "motivation": "\u5e73\u5747\u573a\u53d8\u5206\u63a8\u65ad\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\u7ecf\u5e38\u51fa\u73b0\u6a21\u6001\u574d\u584c\u95ee\u9898\uff0c\u5373\u5f53\u76ee\u6807\u5206\u5e03\u662f\u6df7\u5408\u5206\u5e03\u65f6\uff0c\u4f18\u5316\u5668\u503e\u5411\u4e8e\u5c06\u5927\u90e8\u5206\u8d28\u91cf\u96c6\u4e2d\u5728\u5355\u4e2a\u6df7\u5408\u6210\u5206\u4e0a\u3002\u8fd9\u79cd\u73b0\u8c61\u7f3a\u4e4f\u7406\u8bba\u89e3\u91ca\uff0c\u9700\u8981\u6df1\u5165\u7814\u7a76\u3002", "method": "\u5f15\u5165\u03b5-\u5206\u79bb\u5ea6\u6982\u5ff5\u6765\u91cf\u5316\u6df7\u5408\u6210\u5206\u7684\u5206\u79bb\u7a0b\u5ea6\uff0c\u63a8\u5bfc\u51fa\u5f53\u6df7\u5408\u6210\u5206\u5145\u5206\u5206\u79bb\u65f6\uff0c\u4efb\u4f55MFVI\u4f18\u5316\u5668\u5206\u914d\u7ed9\u6bcf\u4e2a\u6210\u5206\u7684\u8d28\u91cf\u6bd4\u4f8b\u7684\u4e0a\u754c\u3002\u4e3a\u89e3\u51b3\u6a21\u6001\u574d\u584c\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u65cb\u8f6c\u53d8\u5206\u63a8\u65ad\u65b9\u6cd5\uff0c\u5728MFVI\u57fa\u7840\u4e0a\u589e\u52a0\u65cb\u8f6c\u77e9\u9635\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\u6a21\u6001\u574d\u584c\u7684\u53d1\u751f\u5173\u952e\u53d6\u51b3\u4e8e\u6df7\u5408\u6210\u5206\u7684\u76f8\u5bf9\u4f4d\u7f6e\u3002\u6570\u503c\u7814\u7a76\u652f\u6301\u4e86\u7406\u8bba\u53d1\u73b0\uff0c\u5e76\u8bc1\u660e\u4e86RoVI\u65b9\u6cd5\u7684\u4f18\u52bf\u3002", "conclusion": "\u672c\u6587\u9996\u6b21\u4e3aMFVI\u4e2d\u7684\u6a21\u6001\u574d\u584c\u73b0\u8c61\u63d0\u4f9b\u4e86\u7406\u8bba\u89e3\u91ca\uff0c\u63d0\u51fa\u7684RoVI\u65b9\u6cd5\u80fd\u6709\u6548\u7f13\u89e3\u8be5\u95ee\u9898\uff0c\u4e3a\u53d8\u5206\u63a8\u65ad\u65b9\u6cd5\u63d0\u4f9b\u4e86\u65b0\u7684\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2510.16085", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16085", "abs": "https://arxiv.org/abs/2510.16085", "authors": ["Xun Wei", "Pukai Zhou", "Zeyu Wang"], "title": "MoPHES:Leveraging on-device LLMs as Agent for Mobile Psychological Health Evaluation and Support", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "The 2022 World Mental Health Report calls for global mental health care\nreform, amid rising prevalence of issues like anxiety and depression that\naffect nearly one billion people worldwide. Traditional in-person therapy fails\nto meet this demand, and the situation is worsened by stigma. While\ngeneral-purpose large language models (LLMs) offer efficiency for AI-driven\nmental health solutions, they underperform because they lack specialized\nfine-tuning. Existing LLM-based mental health chatbots can engage in empathetic\nconversations, but they overlook real-time user mental state assessment which\nis critical for professional counseling. This paper proposes MoPHES, a\nframework that integrates mental state evaluation, conversational support, and\nprofessional treatment recommendations. The agent developed under this\nframework uses two fine-tuned MiniCPM4-0.5B LLMs: one is fine-tuned on mental\nhealth conditions datasets to assess users' mental states and predict the\nseverity of anxiety and depression; the other is fine-tuned on multi-turn\ndialogues to handle conversations with users. By leveraging insights into\nusers' mental states, our agent provides more tailored support and professional\ntreatment recommendations. Both models are also deployed directly on mobile\ndevices to enhance user convenience and protect user privacy. Additionally, to\nevaluate the performance of MoPHES with other LLMs, we develop a benchmark for\nthe automatic evaluation of mental state prediction and multi-turn counseling\ndialogues, which includes comprehensive evaluation metrics, datasets, and\nmethods.", "AI": {"tldr": "MoPHES\u662f\u4e00\u4e2a\u96c6\u6210\u5fc3\u7406\u72b6\u6001\u8bc4\u4f30\u3001\u5bf9\u8bdd\u652f\u6301\u548c\u4e13\u4e1a\u6cbb\u7597\u5efa\u8bae\u7684\u6846\u67b6\uff0c\u4f7f\u7528\u4e24\u4e2a\u5fae\u8c03\u7684MiniCPM4-0.5B\u6a21\u578b\uff0c\u53ef\u76f4\u63a5\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u90e8\u7f72\u4ee5\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u3002", "motivation": "\u5168\u7403\u5fc3\u7406\u5065\u5eb7\u9700\u6c42\u6fc0\u589e\uff0c\u4f20\u7edf\u9762\u5bf9\u9762\u6cbb\u7597\u65e0\u6cd5\u6ee1\u8db3\u9700\u6c42\uff0c\u4e14\u73b0\u6709LLM\u5fc3\u7406\u804a\u5929\u673a\u5668\u4eba\u7f3a\u4e4f\u5b9e\u65f6\u5fc3\u7406\u72b6\u6001\u8bc4\u4f30\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u4e24\u4e2a\u5fae\u8c03\u7684MiniCPM4-0.5B\u6a21\u578b\uff1a\u4e00\u4e2a\u7528\u4e8e\u5fc3\u7406\u72b6\u6001\u8bc4\u4f30\u548c\u7126\u8651\u6291\u90c1\u4e25\u91cd\u7a0b\u5ea6\u9884\u6d4b\uff0c\u53e6\u4e00\u4e2a\u7528\u4e8e\u591a\u8f6e\u5bf9\u8bdd\u5904\u7406\uff1b\u5f00\u53d1\u4e86\u81ea\u52a8\u5316\u8bc4\u4f30\u57fa\u51c6\u3002", "result": "MoPHES\u6846\u67b6\u80fd\u591f\u63d0\u4f9b\u66f4\u4e2a\u6027\u5316\u7684\u652f\u6301\u548c\u4e13\u4e1a\u6cbb\u7597\u5efa\u8bae\uff0c\u6a21\u578b\u53ef\u76f4\u63a5\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u8fd0\u884c\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u96c6\u6210\u5fc3\u7406\u72b6\u6001\u8bc4\u4f30\u548c\u5bf9\u8bdd\u652f\u6301\uff0c\u4e3aAI\u9a71\u52a8\u7684\u5fc3\u7406\u5065\u5eb7\u89e3\u51b3\u65b9\u6848\u63d0\u4f9b\u4e86\u66f4\u4e13\u4e1a\u7684\u652f\u6301\u80fd\u529b\u3002"}}
{"id": "2510.16194", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16194", "abs": "https://arxiv.org/abs/2510.16194", "authors": ["Guanchen Wu", "Zuhui Chen", "Yuzhang Xie", "Carl Yang"], "title": "Towards Automatic Evaluation and Selection of PHI De-identification Models via Multi-Agent Collaboration", "comment": "Agents4Science 2025 (Spotlight)", "summary": "Protected health information (PHI) de-identification is critical for enabling\nthe safe reuse of clinical notes, yet evaluating and comparing PHI\nde-identification models typically depends on costly, small-scale expert\nannotations. We present TEAM-PHI, a multi-agent evaluation and selection\nframework that uses large language models (LLMs) to automatically measure\nde-identification quality and select the best-performing model without heavy\nreliance on gold labels. TEAM-PHI deploys multiple Evaluation Agents, each\nindependently judging the correctness of PHI extractions and outputting\nstructured metrics. Their results are then consolidated through an LLM-based\nmajority voting mechanism that integrates diverse evaluator perspectives into a\nsingle, stable, and reproducible ranking. Experiments on a real-world clinical\nnote corpus demonstrate that TEAM-PHI produces consistent and accurate\nrankings: despite variation across individual evaluators, LLM-based voting\nreliably converges on the same top-performing systems. Further comparison with\nground-truth annotations and human evaluation confirms that the framework's\nautomated rankings closely match supervised evaluation. By combining\nindependent evaluation agents with LLM majority voting, TEAM-PHI offers a\npractical, secure, and cost-effective solution for automatic evaluation and\nbest-model selection in PHI de-identification, even when ground-truth labels\nare limited.", "AI": {"tldr": "TEAM-PHI\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u8bc4\u4f30\u533b\u7597\u4fe1\u606f\u53bb\u8bc6\u522b\u6a21\u578b\u6027\u80fd\uff0c\u65e0\u9700\u4f9d\u8d56\u6602\u8d35\u7684\u4e13\u5bb6\u6807\u6ce8\uff0c\u901a\u8fc7\u591a\u6570\u6295\u7968\u673a\u5236\u5b9e\u73b0\u7a33\u5b9a\u53ef\u9760\u7684\u6a21\u578b\u6392\u540d\u3002", "motivation": "\u533b\u7597\u4fe1\u606f\u53bb\u8bc6\u522b\u5bf9\u4e8e\u4e34\u5e8a\u7b14\u8bb0\u7684\u5b89\u5168\u590d\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u6210\u672c\u9ad8\u6602\u7684\u5c0f\u89c4\u6a21\u4e13\u5bb6\u6807\u6ce8\uff0c\u9650\u5236\u4e86\u6a21\u578b\u6bd4\u8f83\u548c\u9009\u62e9\u7684\u6548\u7387\u3002", "method": "\u90e8\u7f72\u591a\u4e2a\u8bc4\u4f30\u667a\u80fd\u4f53\u72ec\u7acb\u5224\u65adPHI\u63d0\u53d6\u7684\u6b63\u786e\u6027\uff0c\u7136\u540e\u901a\u8fc7\u57fa\u4e8eLLM\u7684\u591a\u6570\u6295\u7968\u673a\u5236\u6574\u5408\u4e0d\u540c\u8bc4\u4f30\u8005\u7684\u89c2\u70b9\uff0c\u5f62\u6210\u5355\u4e00\u7a33\u5b9a\u7684\u6a21\u578b\u6392\u540d\u3002", "result": "\u5728\u771f\u5b9e\u4e34\u5e8a\u7b14\u8bb0\u8bed\u6599\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTEAM-PHI\u80fd\u4ea7\u751f\u4e00\u81f4\u4e14\u51c6\u786e\u7684\u6392\u540d\uff0c\u5c3d\u7ba1\u4e2a\u4f53\u8bc4\u4f30\u8005\u5b58\u5728\u5dee\u5f02\uff0c\u4f46LLM\u6295\u7968\u80fd\u53ef\u9760\u5730\u6536\u655b\u5230\u76f8\u540c\u7684\u6700\u4f73\u7cfb\u7edf\u3002", "conclusion": "TEAM-PHI\u901a\u8fc7\u7ed3\u5408\u72ec\u7acb\u8bc4\u4f30\u667a\u80fd\u4f53\u548cLLM\u591a\u6570\u6295\u7968\uff0c\u4e3aPHI\u53bb\u8bc6\u522b\u63d0\u4f9b\u4e86\u5b9e\u7528\u3001\u5b89\u5168\u4e14\u6210\u672c\u6548\u76ca\u9ad8\u7684\u81ea\u52a8\u8bc4\u4f30\u548c\u6700\u4f73\u6a21\u578b\u9009\u62e9\u89e3\u51b3\u65b9\u6848\uff0c\u5373\u4f7f\u5728\u5730\u9762\u771f\u503c\u6807\u7b7e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u6709\u6548\u5de5\u4f5c\u3002"}}
{"id": "2510.16766", "categories": ["math.OC", "nlin.AO", "nlin.PS", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.16766", "abs": "https://arxiv.org/abs/2510.16766", "authors": ["Riccardo Muolo", "Yuzuru Kato"], "title": "Equivalence of additive and parametric pinning control protocols for systems of weakly coupled oscillators", "comment": "conference paper (submitted to SICE 2026)", "summary": "Controlling the behavior of nonlinear systems on networks is a paramount task\nin control theory, in particular the control of synchronization, given its vast\napplicability. In this work, we focus on pinning control and we examine two\ndifferent approaches: the first, more common in engineering applications, where\nthe control is implemented through an external input (additive pinning); the\nother, where the parameters of the pinned nodes are varied (parametric\npinning). By means of the phase reduction technique, we show that the two\npinning approaches are equivalent for weakly coupled systems exhibiting\nperiodic oscillatory behaviors. Through numerical simulations, we validate the\nclaim for a system of coupled Stuart--Landau oscillators. Our results pave the\nway for further applications of pinning control in real-world systems.", "AI": {"tldr": "\u672c\u6587\u8bc1\u660e\u4e86\u5728\u5f31\u8026\u5408\u5468\u671f\u6027\u632f\u8361\u7cfb\u7edf\u4e2d\uff0c\u901a\u8fc7\u5916\u90e8\u8f93\u5165\u5b9e\u73b0\u7684\u52a0\u6027\u9489\u624e\u63a7\u5236\u4e0e\u901a\u8fc7\u6539\u53d8\u53c2\u6570\u5b9e\u73b0\u7684\u53c2\u6570\u9489\u624e\u63a7\u5236\u662f\u7b49\u6548\u7684\u3002", "motivation": "\u63a7\u5236\u7f51\u7edc\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u884c\u4e3a\u662f\u63a7\u5236\u7406\u8bba\u4e2d\u7684\u91cd\u8981\u4efb\u52a1\uff0c\u7279\u522b\u662f\u540c\u6b65\u63a7\u5236\u56e0\u5176\u5e7f\u6cdb\u5e94\u7528\u800c\u5907\u53d7\u5173\u6ce8\u3002\u672c\u6587\u65e8\u5728\u6bd4\u8f83\u4e24\u79cd\u4e0d\u540c\u7684\u9489\u624e\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u76f8\u4f4d\u7ea6\u7b80\u6280\u672f\u5206\u6790\u5f31\u8026\u5408\u5468\u671f\u6027\u632f\u8361\u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7\u8026\u5408Stuart-Landau\u632f\u8361\u5668\u7684\u6570\u503c\u6a21\u62df\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u4e86\u5728\u5f31\u8026\u5408\u5468\u671f\u6027\u632f\u8361\u7cfb\u7edf\u4e2d\uff0c\u52a0\u6027\u9489\u624e\u63a7\u5236\u548c\u53c2\u6570\u9489\u624e\u63a7\u5236\u786e\u5b9e\u5177\u6709\u7b49\u6548\u6027\u3002", "conclusion": "\u4e24\u79cd\u9489\u624e\u63a7\u5236\u65b9\u6cd5\u7684\u7b49\u6548\u6027\u4e3a\u5728\u73b0\u5b9e\u4e16\u754c\u7cfb\u7edf\u4e2d\u8fdb\u4e00\u6b65\u5e94\u7528\u9489\u624e\u63a7\u5236\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2510.15968", "categories": ["cs.LG", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2510.15968", "abs": "https://arxiv.org/abs/2510.15968", "authors": ["Zhen Huang", "Hong Wang", "Wenkai Yang", "Muxi Tang", "Depeng Xie", "Ting-Jung Lin", "Yu Zhang", "Wei W. Xing", "Lei He"], "title": "Self-Attention to Operator Learning-based 3D-IC Thermal Simulation", "comment": null, "summary": "Thermal management in 3D ICs is increasingly challenging due to higher power\ndensities. Traditional PDE-solving-based methods, while accurate, are too slow\nfor iterative design. Machine learning approaches like FNO provide faster\nalternatives but suffer from high-frequency information loss and high-fidelity\ndata dependency. We introduce Self-Attention U-Net Fourier Neural Operator\n(SAU-FNO), a novel framework combining self-attention and U-Net with FNO to\ncapture long-range dependencies and model local high-frequency features\neffectively. Transfer learning is employed to fine-tune low-fidelity data,\nminimizing the need for extensive high-fidelity datasets and speeding up\ntraining. Experiments demonstrate that SAU-FNO achieves state-of-the-art\nthermal prediction accuracy and provides an 842x speedup over traditional FEM\nmethods, making it an efficient tool for advanced 3D IC thermal simulations.", "AI": {"tldr": "\u63d0\u51faSAU-FNO\u6846\u67b6\uff0c\u7ed3\u5408\u81ea\u6ce8\u610f\u529b\u673a\u5236\u3001U-Net\u548cFNO\uff0c\u7528\u4e8e3D IC\u70ed\u7ba1\u7406\uff0c\u5b9e\u73b0842\u500d\u52a0\u901f\u5e76\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "motivation": "3D IC\u70ed\u7ba1\u7406\u9762\u4e34\u9ad8\u529f\u7387\u5bc6\u5ea6\u6311\u6218\uff0c\u4f20\u7edfPDE\u65b9\u6cd5\u901f\u5ea6\u6162\uff0c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u9ad8\u9891\u4fe1\u606f\u4e22\u5931\u548c\u9ad8\u4fdd\u771f\u6570\u636e\u4f9d\u8d56\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u81ea\u6ce8\u610f\u529b\u673a\u5236\u548cU-Net\u7684FNO\u6846\u67b6\uff0c\u4f7f\u7528\u8fc1\u79fb\u5b66\u4e60\u5fae\u8c03\u4f4e\u4fdd\u771f\u6570\u636e\uff0c\u51cf\u5c11\u5bf9\u9ad8\u4fdd\u771f\u6570\u636e\u96c6\u7684\u9700\u6c42\u3002", "result": "SAU-FNO\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u70ed\u9884\u6d4b\u7cbe\u5ea6\uff0c\u76f8\u6bd4\u4f20\u7edfFEM\u65b9\u6cd5\u5b9e\u73b0842\u500d\u52a0\u901f\u3002", "conclusion": "SAU-FNO\u662f\u9ad8\u6548\u7684\u9ad8\u7ea73D IC\u70ed\u4eff\u771f\u5de5\u5177\uff0c\u5e73\u8861\u4e86\u7cbe\u5ea6\u548c\u901f\u5ea6\u3002"}}
{"id": "2510.16373", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16373", "abs": "https://arxiv.org/abs/2510.16373", "authors": ["Federico Ravenda", "Seyed Ali Bahrainian", "Andrea Raballo", "Antonietta Mira"], "title": "Navigating through the hidden embedding space: steering LLMs to improve mental health assessment", "comment": null, "summary": "The rapid evolution of Large Language Models (LLMs) is transforming AI,\nopening new opportunities in sensitive and high-impact areas such as Mental\nHealth (MH). Yet, despite these advancements, recent evidence reveals that\nsmaller-scale models still struggle to deliver optimal performance in\ndomain-specific applications. In this study, we present a cost-efficient yet\npowerful approach to improve MH assessment capabilities of an LLM, without\nrelying on any computationally intensive techniques. Our lightweight method\nconsists of a linear transformation applied to a specific layer's activations,\nleveraging steering vectors to guide the model's output. Remarkably, this\nintervention enables the model to achieve improved results across two distinct\ntasks: (1) identifying whether a Reddit post is useful for detecting the\npresence or absence of depressive symptoms (relevance prediction task), and (2)\ncompleting a standardized psychological screening questionnaire for depression\nbased on users' Reddit post history (questionnaire completion task). Results\nhighlight the untapped potential of steering mechanisms as computationally\nefficient tools for LLMs' MH domain adaptation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ebf\u6027\u53d8\u6362\u7279\u5b9a\u5c42\u6fc0\u6d3b\u6765\u63d0\u5347LLM\u5728\u5fc3\u7406\u5065\u5eb7\u8bc4\u4f30\u4e2d\u7684\u8868\u73b0\uff0c\u65e0\u9700\u8ba1\u7b97\u5bc6\u96c6\u578b\u6280\u672f\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5feb\u901f\u53d1\u5c55\uff0c\u4f46\u5c0f\u89c4\u6a21\u6a21\u578b\u5728\u7279\u5b9a\u9886\u57df\u5e94\u7528\u4e2d\u4ecd\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5728\u5fc3\u7406\u5065\u5eb7\u8fd9\u6837\u7684\u654f\u611f\u9ad8\u5f71\u54cd\u9886\u57df\u3002", "method": "\u4f7f\u7528\u8f6c\u5411\u5411\u91cf\u5f15\u5bfc\u6a21\u578b\u8f93\u51fa\uff0c\u5bf9\u7279\u5b9a\u5c42\u6fc0\u6d3b\u5e94\u7528\u7ebf\u6027\u53d8\u6362\u7684\u8f7b\u91cf\u7ea7\u65b9\u6cd5\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4e24\u4e2a\u4efb\u52a1\u4e2d\u53d6\u5f97\u6539\u8fdb\uff1a\u8bc6\u522bReddit\u5e16\u5b50\u662f\u5426\u6709\u52a9\u4e8e\u68c0\u6d4b\u6291\u90c1\u75c7\u72b6\u7684\u76f8\u5173\u6027\u9884\u6d4b\u4efb\u52a1\uff0c\u4ee5\u53ca\u57fa\u4e8e\u7528\u6237Reddit\u5386\u53f2\u5b8c\u6210\u6807\u51c6\u5316\u6291\u90c1\u7b5b\u67e5\u95ee\u5377\u7684\u4efb\u52a1\u3002", "conclusion": "\u8f6c\u5411\u673a\u5236\u4f5c\u4e3a\u8ba1\u7b97\u6548\u7387\u9ad8\u7684\u5de5\u5177\uff0c\u5728LLMs\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u9002\u5e94\u65b9\u9762\u5177\u6709\u672a\u5f00\u53d1\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.17129", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.17129", "abs": "https://arxiv.org/abs/2510.17129", "authors": ["Wenbing Tang", "Meilin Zhu", "Fenghua Wu", "Yang Liu"], "title": "Semantic Intelligence: A Bio-Inspired Cognitive Framework for Embodied Agents", "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) have greatly enhanced\nnatural language understanding and content generation. However, these models\nprimarily operate in disembodied digital environments and lack interaction with\nthe physical world. To address this limitation, Embodied Artificial\nIntelligence (EAI) has emerged, focusing on agents that can perceive and\ninteract with their surroundings. Despite progress, current embodied agents\nface challenges in unstructured real-world environments due to insufficient\nsemantic intelligence, which is critical for understanding and reasoning about\ncomplex tasks. This paper introduces the Semantic Intelligence-Driven Embodied\n(SIDE) agent framework, which integrates a hierarchical semantic cognition\narchitecture with a semantic-driven decision-making process. This enables\nagents to reason about and interact with the physical world in a contextually\nadaptive manner. The framework is inspired by biological cognitive mechanisms\nand utilizes bio-inspired principles to design a semantic cognitive\narchitecture that mimics how humans and animals integrate and process sensory\ninformation. We present this framework as a step toward developing more\nintelligent and versatile embodied agents.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u8bed\u4e49\u667a\u80fd\u9a71\u52a8\u7684\u5177\u8eab\u4ee3\u7406\u6846\u67b6SIDE\uff0c\u901a\u8fc7\u5206\u5c42\u8bed\u4e49\u8ba4\u77e5\u67b6\u6784\u548c\u8bed\u4e49\u9a71\u52a8\u51b3\u7b56\u8fc7\u7a0b\uff0c\u4f7f\u4ee3\u7406\u80fd\u591f\u5728\u7269\u7406\u4e16\u754c\u4e2d\u8fdb\u884c\u4e0a\u4e0b\u6587\u81ea\u9002\u5e94\u63a8\u7406\u548c\u4ea4\u4e92\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u5728\u6570\u5b57\u73af\u5883\u4e2d\u8fd0\u884c\uff0c\u7f3a\u4e4f\u4e0e\u7269\u7406\u4e16\u754c\u7684\u4ea4\u4e92\u3002\u5177\u8eab\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u5728\u975e\u7ed3\u6784\u5316\u73b0\u5b9e\u73af\u5883\u4e2d\u9762\u4e34\u8bed\u4e49\u667a\u80fd\u4e0d\u8db3\u7684\u6311\u6218\uff0c\u65e0\u6cd5\u6709\u6548\u7406\u89e3\u548c\u63a8\u7406\u590d\u6742\u4efb\u52a1\u3002", "method": "\u5f15\u5165SIDE\u6846\u67b6\uff0c\u6574\u5408\u5206\u5c42\u8bed\u4e49\u8ba4\u77e5\u67b6\u6784\u4e0e\u8bed\u4e49\u9a71\u52a8\u51b3\u7b56\u8fc7\u7a0b\uff0c\u53d7\u751f\u7269\u8ba4\u77e5\u673a\u5236\u542f\u53d1\uff0c\u91c7\u7528\u4eff\u751f\u539f\u7406\u8bbe\u8ba1\u8bed\u4e49\u8ba4\u77e5\u67b6\u6784\uff0c\u6a21\u62df\u4eba\u7c7b\u548c\u52a8\u7269\u6574\u5408\u5904\u7406\u611f\u5b98\u4fe1\u606f\u7684\u65b9\u5f0f\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u80fd\u591f\u8fdb\u884c\u4e0a\u4e0b\u6587\u81ea\u9002\u5e94\u63a8\u7406\u548c\u7269\u7406\u4e16\u754c\u4ea4\u4e92\u7684\u5177\u8eab\u4ee3\u7406\u6846\u67b6\uff0c\u4e3a\u5f00\u53d1\u66f4\u667a\u80fd\u3001\u591a\u529f\u80fd\u7684\u5177\u8eab\u4ee3\u7406\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002", "conclusion": "SIDE\u6846\u67b6\u901a\u8fc7\u6574\u5408\u8bed\u4e49\u667a\u80fd\u548c\u5177\u8eab\u4ea4\u4e92\uff0c\u4e3a\u89e3\u51b3\u5f53\u524d\u5177\u8eab\u4ee3\u7406\u5728\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u8bed\u4e49\u7406\u89e3\u4e0d\u8db3\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0c\u662f\u5f00\u53d1\u66f4\u667a\u80fd\u5177\u8eab\u4ee3\u7406\u7684\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2510.17072", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.17072", "abs": "https://arxiv.org/abs/2510.17072", "authors": ["Kyum Kim", "Yaqing Chen", "Paromita Dubey"], "title": "DFNN: A Deep Fr\u00e9chet Neural Network Framework for Learning Metric-Space-Valued Responses", "comment": null, "summary": "Regression with non-Euclidean responses -- e.g., probability distributions,\nnetworks, symmetric positive-definite matrices, and compositions -- has become\nincreasingly important in modern applications. In this paper, we propose deep\nFr\\'echet neural networks (DFNNs), an end-to-end deep learning framework for\npredicting non-Euclidean responses -- which are considered as random objects in\na metric space -- from Euclidean predictors. Our method leverages the\nrepresentation-learning power of deep neural networks (DNNs) to the task of\napproximating conditional Fr\\'echet means of the response given the predictors,\nthe metric-space analogue of conditional expectations, by minimizing a\nFr\\'echet risk. The framework is highly flexible, accommodating diverse metrics\nand high-dimensional predictors. We establish a universal approximation theorem\nfor DFNNs, advancing the state-of-the-art of neural network approximation\ntheory to general metric-space-valued responses without making model\nassumptions or relying on local smoothing. Empirical studies on synthetic\ndistributional and network-valued responses, as well as a real-world\napplication to predicting employment occupational compositions, demonstrate\nthat DFNNs consistently outperform existing methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u6df1\u5ea6Fr\u00e9chet\u795e\u7ecf\u7f51\u7edc(DFNNs)\uff0c\u8fd9\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u6b27\u51e0\u91cc\u5f97\u9884\u6d4b\u53d8\u91cf\u9884\u6d4b\u975e\u6b27\u51e0\u91cc\u5f97\u54cd\u5e94\uff08\u5982\u6982\u7387\u5206\u5e03\u3001\u7f51\u7edc\u3001\u5bf9\u79f0\u6b63\u5b9a\u77e9\u9635\u7b49\uff09\u3002", "motivation": "\u73b0\u4ee3\u5e94\u7528\u4e2d\u975e\u6b27\u51e0\u91cc\u5f97\u54cd\u5e94\uff08\u5982\u6982\u7387\u5206\u5e03\u3001\u7f51\u7edc\u3001\u77e9\u9635\u7b49\uff09\u7684\u56de\u5f52\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\uff0c\u9700\u8981\u80fd\u591f\u5904\u7406\u8fd9\u7c7b\u54cd\u5e94\u7684\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u8868\u793a\u5b66\u4e60\u80fd\u529b\uff0c\u901a\u8fc7\u6700\u5c0f\u5316Fr\u00e9chet\u98ce\u9669\u6765\u8fd1\u4f3c\u7ed9\u5b9a\u9884\u6d4b\u53d8\u91cf\u4e0b\u54cd\u5e94\u7684\u6761\u4ef6Fr\u00e9chet\u5747\u503c\uff08\u5ea6\u91cf\u7a7a\u95f4\u4e2d\u6761\u4ef6\u671f\u671b\u7684\u7c7b\u6bd4\uff09\u3002", "result": "\u5efa\u7acb\u4e86DFNNs\u7684\u901a\u7528\u903c\u8fd1\u5b9a\u7406\uff0c\u65e0\u9700\u6a21\u578b\u5047\u8bbe\u6216\u5c40\u90e8\u5e73\u6ed1\u5373\u53ef\u5904\u7406\u4e00\u822c\u5ea6\u91cf\u7a7a\u95f4\u503c\u54cd\u5e94\u3002\u5728\u5408\u6210\u5206\u5e03\u548c\u7f51\u7edc\u503c\u54cd\u5e94\u4ee5\u53ca\u5c31\u4e1a\u804c\u4e1a\u7ec4\u6210\u7684\u5b9e\u9645\u5e94\u7528\u4e2d\uff0cDFNNs\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DFNNs\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u5ea6\u7075\u6d3b\u4e14\u5f3a\u5927\u7684\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u5404\u79cd\u975e\u6b27\u51e0\u91cc\u5f97\u54cd\u5e94\u9884\u6d4b\u95ee\u9898\uff0c\u5e76\u5728\u7406\u8bba\u548c\u5b9e\u8bc1\u4e0a\u90fd\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2510.16366", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2510.16366", "abs": "https://arxiv.org/abs/2510.16366", "authors": ["Xinyi Li", "Zhiqiang Guo", "Qinglang Guo", "Hao Jin", "Weizhi Ma", "Min Zhang"], "title": "Integrating LLM and Diffusion-Based Agents for Social Simulation", "comment": "10 pages, 3 figures, 4 tables", "summary": "Agent-based social simulation provides a valuable methodology for predicting\nsocial information diffusion, yet existing approaches face two primary\nlimitations. Traditional agent models often rely on rigid behavioral rules and\nlack semantic understanding of textual content, while emerging large language\nmodel (LLM)-based agents incur prohibitive computational costs at scale. To\naddress these challenges, we propose a hybrid simulation framework that\nstrategically integrates LLM-driven agents with diffusion model-based agents.\nThe framework employs LLM-based agents to simulate a core subset of users with\nrich semantic reasoning, while a diffusion model handles the remaining\npopulation efficiently. Although the two agent types operate on disjoint user\ngroups, both incorporate key factors including user personalization, social\ninfluence, and content awareness, and interact through a coordinated simulation\nprocess. Extensive experiments on three real-world datasets demonstrate that\nour framework outperforms existing methods in prediction accuracy, validating\nthe effectiveness of its modular design.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u4eff\u771f\u6846\u67b6\uff0c\u7ed3\u5408LLM\u9a71\u52a8\u4ee3\u7406\u548c\u6269\u6563\u6a21\u578b\u4ee3\u7406\uff0c\u7528\u4e8e\u9884\u6d4b\u793e\u4ea4\u4fe1\u606f\u4f20\u64ad\uff0c\u5728\u4fdd\u8bc1\u51c6\u786e\u6027\u7684\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u4ee3\u7406\u7684\u793e\u4ea4\u4eff\u771f\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u5c40\u9650\uff1a\u4f20\u7edf\u4ee3\u7406\u6a21\u578b\u4f9d\u8d56\u521a\u6027\u884c\u4e3a\u89c4\u5219\u4e14\u7f3a\u4e4f\u6587\u672c\u8bed\u4e49\u7406\u89e3\uff0c\u800c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b(LLM)\u7684\u4ee3\u7406\u5728\u5927\u89c4\u6a21\u5e94\u7528\u65f6\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u4eff\u771f\u6846\u67b6\uff0c\u7b56\u7565\u6027\u5730\u6574\u5408LLM\u9a71\u52a8\u4ee3\u7406\u548c\u6269\u6563\u6a21\u578b\u4ee3\u7406\u3002LLM\u4ee3\u7406\u6a21\u62df\u6838\u5fc3\u7528\u6237\u5b50\u96c6\u8fdb\u884c\u4e30\u5bcc\u8bed\u4e49\u63a8\u7406\uff0c\u6269\u6563\u6a21\u578b\u9ad8\u6548\u5904\u7406\u5269\u4f59\u7528\u6237\u7fa4\u4f53\u3002\u4e24\u79cd\u4ee3\u7406\u7c7b\u578b\u5728\u4e0d\u76f8\u4ea4\u7684\u7528\u6237\u7ec4\u4e0a\u8fd0\u884c\uff0c\u4f46\u90fd\u5305\u542b\u7528\u6237\u4e2a\u6027\u5316\u3001\u793e\u4f1a\u5f71\u54cd\u548c\u5185\u5bb9\u611f\u77e5\u7b49\u5173\u952e\u56e0\u7d20\uff0c\u5e76\u901a\u8fc7\u534f\u8c03\u7684\u4eff\u771f\u8fc7\u7a0b\u8fdb\u884c\u4ea4\u4e92\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u9884\u6d4b\u51c6\u786e\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u6a21\u5757\u5316\u8bbe\u8ba1\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6df7\u5408\u4eff\u771f\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u4f20\u7edf\u4ee3\u7406\u6a21\u578b\u7f3a\u4e4f\u8bed\u4e49\u7406\u89e3\u548cLLM\u4ee3\u7406\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u5b9e\u73b0\u4e86\u51c6\u786e\u4e14\u9ad8\u6548\u7684\u793e\u4ea4\u4fe1\u606f\u4f20\u64ad\u9884\u6d4b\u3002"}}
{"id": "2510.16206", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16206", "abs": "https://arxiv.org/abs/2510.16206", "authors": ["Alex Zhavoronkov", "Dominika Wilczok", "Roman Yampolskiy"], "title": "The Right to Be Remembered: Preserving Maximally Truthful Digital Memory in the Age of AI", "comment": null, "summary": "Since the rapid expansion of large language models (LLMs), people have begun\nto rely on them for information retrieval. While traditional search engines\ndisplay ranked lists of sources shaped by search engine optimization (SEO),\nadvertising, and personalization, LLMs typically provide a synthesized response\nthat feels singular and authoritative. While both approaches carry risks of\nbias and omission, LLMs may amplify the effect by collapsing multiple\nperspectives into one answer, reducing users ability or inclination to compare\nalternatives. This concentrates power over information in a few LLM vendors\nwhose systems effectively shape what is remembered and what is overlooked. As a\nresult, certain narratives, individuals or groups, may be disproportionately\nsuppressed, while others are disproportionately elevated. Over time, this\ncreates a new threat: the gradual erasure of those with limited digital\npresence, and the amplification of those already prominent, reshaping\ncollective memory.To address these concerns, this paper presents a concept of\nthe Right To Be Remembered (RTBR) which encompasses minimizing the risk of\nAI-driven information omission, embracing the right of fair treatment, while\nensuring that the generated content would be maximally truthful.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\"\u88ab\u8bb0\u4f4f\u6743\"\uff08RTBR\uff09\u6982\u5ff5\uff0c\u65e8\u5728\u89e3\u51b3LLMs\u5728\u4fe1\u606f\u68c0\u7d22\u4e2d\u53ef\u80fd\u5bfc\u81f4\u7684\u504f\u89c1\u3001\u4fe1\u606f\u9057\u6f0f\u548c\u96c6\u4f53\u8bb0\u5fc6\u91cd\u5851\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u666e\u53ca\uff0c\u4eba\u4eec\u8d8a\u6765\u8d8a\u4f9d\u8d56\u5b83\u4eec\u8fdb\u884c\u4fe1\u606f\u68c0\u7d22\u3002\u4f46LLMs\u5c06\u591a\u4e2a\u89c6\u89d2\u5408\u6210\u5355\u4e00\u6743\u5a01\u6027\u56de\u7b54\u7684\u505a\u6cd5\uff0c\u53ef\u80fd\u5bfc\u81f4\u67d0\u4e9b\u7fa4\u4f53\u88ab\u8fc7\u5ea6\u538b\u5236\uff0c\u800c\u53e6\u4e00\u4e9b\u7fa4\u4f53\u88ab\u8fc7\u5ea6\u653e\u5927\uff0c\u4ece\u800c\u91cd\u5851\u96c6\u4f53\u8bb0\u5fc6\u3002", "method": "\u63d0\u51fa\"\u88ab\u8bb0\u4f4f\u6743\"\uff08RTBR\uff09\u6982\u5ff5\u6846\u67b6\uff0c\u5305\u62ec\u6700\u5c0f\u5316AI\u9a71\u52a8\u4fe1\u606f\u9057\u6f0f\u98ce\u9669\u3001\u4fdd\u969c\u516c\u5e73\u5bf9\u5f85\u6743\u5229\uff0c\u540c\u65f6\u786e\u4fdd\u751f\u6210\u5185\u5bb9\u7684\u771f\u5b9e\u6027\u6700\u5927\u5316\u3002", "result": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u5e94\u5bf9LLMs\u4fe1\u606f\u96c6\u4e2d\u5316\u98ce\u9669\u7684\u6982\u5ff5\u6846\u67b6\uff0c\u4e3a\u4fdd\u62a4\u6570\u5b57\u5f31\u52bf\u7fa4\u4f53\u7684\u4fe1\u606f\u53ef\u89c1\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "conclusion": "\u9700\u8981\u5efa\u7acb\"\u88ab\u8bb0\u4f4f\u6743\"\u6765\u5e94\u5bf9LLMs\u5bf9\u4fe1\u606f\u751f\u6001\u548c\u96c6\u4f53\u8bb0\u5fc6\u7684\u6f5c\u5728\u5a01\u80c1\uff0c\u786e\u4fdd\u4fe1\u606f\u68c0\u7d22\u7cfb\u7edf\u7684\u516c\u5e73\u6027\u548c\u771f\u5b9e\u6027\u3002"}}
{"id": "2510.16768", "categories": ["math.OC", "49J15 (Primary) 49K15, 49M37 (Secondary)", "G.1.6"], "pdf": "https://arxiv.org/pdf/2510.16768", "abs": "https://arxiv.org/abs/2510.16768", "authors": ["Maciej Szymkat", "Adam Korytowski"], "title": "Method of Monotone Structural Evolution for control and state constrained optimal and control problems", "comment": null, "summary": "A method of optimal control computation is proposed for problems with control\nand state constraints. It uses a sequence of control structure adjustments in\nthe form of generations and reductions of nodes and arcs, which do not change\nthe current control but redefine the decision space. Several examples are\ngiven.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5904\u7406\u63a7\u5236\u548c\u72b6\u6001\u7ea6\u675f\u7684\u6700\u4f18\u63a7\u5236\u8ba1\u7b97\u65b9\u6cd5\uff0c\u901a\u8fc7\u8282\u70b9\u548c\u5f27\u7684\u751f\u6210\u4e0e\u7f29\u51cf\u5e8f\u5217\u6765\u8c03\u6574\u63a7\u5236\u7ed3\u6784\uff0c\u91cd\u65b0\u5b9a\u4e49\u51b3\u7b56\u7a7a\u95f4", "motivation": "\u89e3\u51b3\u5e26\u6709\u63a7\u5236\u548c\u72b6\u6001\u7ea6\u675f\u7684\u6700\u4f18\u63a7\u5236\u95ee\u9898\uff0c\u9700\u8981\u6709\u6548\u7684\u65b9\u6cd5\u6765\u5904\u7406\u7ea6\u675f\u6761\u4ef6\u5e76\u4f18\u5316\u63a7\u5236\u7b56\u7565", "method": "\u4f7f\u7528\u63a7\u5236\u7ed3\u6784\u8c03\u6574\u5e8f\u5217\uff0c\u5305\u62ec\u8282\u70b9\u548c\u5f27\u7684\u751f\u6210\u4e0e\u7f29\u51cf\uff0c\u8fd9\u4e9b\u8c03\u6574\u4e0d\u6539\u53d8\u5f53\u524d\u63a7\u5236\u4f46\u91cd\u65b0\u5b9a\u4e49\u51b3\u7b56\u7a7a\u95f4", "result": "\u63d0\u4f9b\u4e86\u51e0\u4e2a\u793a\u4f8b\u6765\u9a8c\u8bc1\u65b9\u6cd5\u7684\u6709\u6548\u6027", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u63a7\u5236\u548c\u72b6\u6001\u7ea6\u675f\u7684\u6700\u4f18\u63a7\u5236\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u6784\u8c03\u6574\u91cd\u65b0\u5b9a\u4e49\u51b3\u7b56\u7a7a\u95f4"}}
{"id": "2510.15969", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15969", "abs": "https://arxiv.org/abs/2510.15969", "authors": ["Paul-Niklas Ken Kandora", "Simon Caspar Zeller", "Aaron Jeremias Elsing", "Elena Kuss", "Steffen Rebennack"], "title": "LinearizeLLM: An Agent-Based Framework for LLM-Driven Exact Linear Reformulation of Nonlinear Optimization Problems", "comment": null, "summary": "Reformulating nonlinear optimization problems is largely manual and\nexpertise-intensive, yet it remains essential for solving such problems with\nlinear optimization solvers or applying special-purpose algorithms. We\nintroduce \\textit{LinearizeLLM}, an agent-based framework that solves this task\nby leveraging Large Language Models (LLMs). The framework assigns each\nnonlinear pattern to a \\textit{reformulation agent} that is explicitly\ninstructed to derive an exact linear reformulation for its nonlinearity\npattern, for instance, absolute-value terms or bilinear products of decision\nvariables. The agents then coordinate to assemble a solver-ready linear model\nequivalent to the original problem. To benchmark the approach, we create a\ndataset of 20 real-world nonlinear optimization problems derived from the\nestablished ComplexOR dataset of linear optimization problems. We evaluate our\napproach with several LLMs. Our results indicate that specialized LLM agents\ncan automate linearization tasks, opening a path toward fully conversational\nmodeling pipelines for nonlinear optimization.", "AI": {"tldr": "LinearizeLLM\u662f\u4e00\u4e2a\u57fa\u4e8e\u4ee3\u7406\u7684\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u5c06\u975e\u7ebf\u6027\u4f18\u5316\u95ee\u9898\u8f6c\u5316\u4e3a\u7ebf\u6027\u5f62\u5f0f\uff0c\u4f7f\u95ee\u9898\u80fd\u591f\u7528\u7ebf\u6027\u4f18\u5316\u6c42\u89e3\u5668\u6c42\u89e3\u3002", "motivation": "\u975e\u7ebf\u6027\u4f18\u5316\u95ee\u9898\u7684\u7ebf\u6027\u5316\u901a\u5e38\u9700\u8981\u4eba\u5de5\u64cd\u4f5c\u4e14\u4f9d\u8d56\u4e13\u5bb6\u77e5\u8bc6\uff0c\u8fd9\u9650\u5236\u4e86\u95ee\u9898\u7684\u6c42\u89e3\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "\u4e3a\u6bcf\u79cd\u975e\u7ebf\u6027\u6a21\u5f0f\u5206\u914d\u4e13\u95e8\u7684\"\u7ebf\u6027\u5316\u4ee3\u7406\"\uff0c\u8fd9\u4e9b\u4ee3\u7406\u88ab\u660e\u786e\u6307\u793a\u4e3a\u5176\u7279\u5b9a\u7684\u975e\u7ebf\u6027\u6a21\u5f0f\u63a8\u5bfc\u7cbe\u786e\u7684\u7ebf\u6027\u91cd\u6784\uff0c\u7136\u540e\u534f\u8c03\u7ec4\u88c5\u6210\u53ef\u6c42\u89e3\u7684\u7ebf\u6027\u6a21\u578b\u3002", "result": "\u5728\u57fa\u4e8eComplexOR\u6570\u636e\u96c6\u768420\u4e2a\u771f\u5b9e\u975e\u7ebf\u6027\u4f18\u5316\u95ee\u9898\u4e0a\u8fdb\u884c\u6d4b\u8bd5\uff0c\u7ed3\u679c\u8868\u660e\u4e13\u95e8\u7684LLM\u4ee3\u7406\u80fd\u591f\u81ea\u52a8\u5316\u7ebf\u6027\u5316\u4efb\u52a1\u3002", "conclusion": "\u4e13\u4e1a\u5316\u7684LLM\u4ee3\u7406\u53ef\u4ee5\u81ea\u52a8\u5316\u7ebf\u6027\u5316\u4efb\u52a1\uff0c\u4e3a\u975e\u7ebf\u6027\u4f18\u5316\u5f00\u8f9f\u4e86\u5b8c\u5168\u5bf9\u8bdd\u5f0f\u5efa\u6a21\u7ba1\u9053\u7684\u65b0\u8def\u5f84\u3002"}}
{"id": "2510.16380", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16380", "abs": "https://arxiv.org/abs/2510.16380", "authors": ["Yu Ying Chiu", "Michael S. Lee", "Rachel Calcott", "Brandon Handoko", "Paul de Font-Reaulx", "Paula Rodriguez", "Chen Bo Calvin Zhang", "Ziwen Han", "Udari Madhushani Sehwag", "Yash Maurya", "Christina Q Knight", "Harry R. Lloyd", "Florence Bacus", "Mantas Mazeika", "Bing Liu", "Yejin Choi", "Mitchell L Gordon", "Sydney Levine"], "title": "MoReBench: Evaluating Procedural and Pluralistic Moral Reasoning in Language Models, More than Outcomes", "comment": "46 pages, 8 figures, 10 tables. Preprint", "summary": "As AI systems progress, we rely more on them to make decisions with us and\nfor us. To ensure that such decisions are aligned with human values, it is\nimperative for us to understand not only what decisions they make but also how\nthey come to those decisions. Reasoning language models, which provide both\nfinal responses and (partially transparent) intermediate thinking traces,\npresent a timely opportunity to study AI procedural reasoning. Unlike math and\ncode problems which often have objectively correct answers, moral dilemmas are\nan excellent testbed for process-focused evaluation because they allow for\nmultiple defensible conclusions. To do so, we present MoReBench: 1,000 moral\nscenarios, each paired with a set of rubric criteria that experts consider\nessential to include (or avoid) when reasoning about the scenarios. MoReBench\ncontains over 23 thousand criteria including identifying moral considerations,\nweighing trade-offs, and giving actionable recommendations to cover cases on AI\nadvising humans moral decisions as well as making moral decisions autonomously.\nSeparately, we curate MoReBench-Theory: 150 examples to test whether AI can\nreason under five major frameworks in normative ethics. Our results show that\nscaling laws and existing benchmarks on math, code, and scientific reasoning\ntasks fail to predict models' abilities to perform moral reasoning. Models also\nshow partiality towards specific moral frameworks (e.g., Benthamite Act\nUtilitarianism and Kantian Deontology), which might be side effects of popular\ntraining paradigms. Together, these benchmarks advance process-focused\nreasoning evaluation towards safer and more transparent AI.", "AI": {"tldr": "MoReBench\u662f\u4e00\u4e2a\u5305\u542b1000\u4e2a\u9053\u5fb7\u573a\u666f\u548c23000\u591a\u4e2a\u8bc4\u4f30\u6807\u51c6\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30AI\u5728\u9053\u5fb7\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u8868\u73b0\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6570\u5b66\u3001\u4ee3\u7801\u548c\u79d1\u5b66\u63a8\u7406\u57fa\u51c6\u5728\u9053\u5fb7\u63a8\u7406\u8bc4\u4f30\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u968f\u7740AI\u7cfb\u7edf\u8d8a\u6765\u8d8a\u591a\u5730\u53c2\u4e0e\u4eba\u7c7b\u51b3\u7b56\uff0c\u9700\u8981\u7406\u89e3AI\u5982\u4f55\u505a\u51fa\u51b3\u7b56\u800c\u4e0d\u4ec5\u4ec5\u662f\u51b3\u7b56\u7ed3\u679c\u3002\u9053\u5fb7\u56f0\u5883\u662f\u8bc4\u4f30AI\u7a0b\u5e8f\u63a8\u7406\u7684\u7406\u60f3\u6d4b\u8bd5\u573a\uff0c\u56e0\u4e3a\u5141\u8bb8\u591a\u79cd\u5408\u7406\u7684\u7ed3\u8bba\u3002", "method": "\u6784\u5efaMoReBench\u57fa\u51c6\uff1a\u5305\u542b1000\u4e2a\u9053\u5fb7\u573a\u666f\uff0c\u6bcf\u4e2a\u573a\u666f\u914d\u6709\u4e13\u5bb6\u5236\u5b9a\u7684\u8bc4\u4f30\u6807\u51c6\uff1b\u53e6\u5916\u6784\u5efaMoReBench-Theory\u5305\u542b150\u4e2a\u4f8b\u5b50\uff0c\u6d4b\u8bd5AI\u5728\u4e94\u79cd\u89c4\u8303\u4f26\u7406\u6846\u67b6\u4e0b\u7684\u63a8\u7406\u80fd\u529b\u3002", "result": "\u6269\u5c55\u5b9a\u5f8b\u548c\u73b0\u6709\u57fa\u51c6\u65e0\u6cd5\u9884\u6d4b\u6a21\u578b\u5728\u9053\u5fb7\u63a8\u7406\u4e0a\u7684\u80fd\u529b\uff1b\u6a21\u578b\u663e\u793a\u51fa\u5bf9\u7279\u5b9a\u9053\u5fb7\u6846\u67b6\u7684\u504f\u597d\uff08\u5982\u8fb9\u6c81\u529f\u5229\u4e3b\u4e49\u548c\u5eb7\u5fb7\u4e49\u52a1\u8bba\uff09\uff0c\u8fd9\u53ef\u80fd\u662f\u6d41\u884c\u8bad\u7ec3\u8303\u5f0f\u7684\u526f\u4f5c\u7528\u3002", "conclusion": "\u8fd9\u4e9b\u57fa\u51c6\u63a8\u52a8\u4e86\u4ee5\u8fc7\u7a0b\u4e3a\u91cd\u70b9\u7684\u63a8\u7406\u8bc4\u4f30\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u5b89\u5168\u3001\u66f4\u900f\u660e\u7684AI\u7cfb\u7edf\u3002"}}
{"id": "2510.17155", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.17155", "abs": "https://arxiv.org/abs/2510.17155", "authors": ["Mohammadamin Lari"], "title": "A Data-Driven Framework for Online Mitigation of False Data Injection Signals in Networked Control Systems", "comment": "17 pages, 9 figures", "summary": "This paper introduces a novel two-stage framework for online mitigation of\nFalse Data Injection (FDI) signals to improve the resiliency of Networked\nControl Systems (NCSs) and ensure their safe operation in the presence of\nmalicious activities. The first stage involves meta learning to select a base\ntime series forecasting model within a stacked ensemble learning architecture.\nThis is achieved by converting time series data into scalograms using\ncontinuous wavelet transform, which are then split into image frames to\ngenerate a scalo-temporal representation of the data and to distinguish between\ndifferent complexity levels of time series data based on an entropy metric\nusing a convolutional neural network. In the second stage, the selected model\nmitigates false data injection signals in real-time. The proposed framework's\neffectiveness is demonstrated through rigorous simulations involving the\nformation control of differential drive mobile robots. By addressing the\nsecurity challenges in NCSs, this framework offers a promising approach to\nmaintaining system integrity and ensuring operational safety.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u7ebf\u7f13\u89e3\u7f51\u7edc\u63a7\u5236\u7cfb\u7edf\u4e2d\u7684\u865a\u5047\u6570\u636e\u6ce8\u5165\u4fe1\u53f7\uff0c\u901a\u8fc7\u5143\u5b66\u4e60\u9009\u62e9\u57fa\u7840\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\uff0c\u5e76\u4f7f\u7528\u5c0f\u6ce2\u53d8\u6362\u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6765\u533a\u5206\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u3002", "motivation": "\u63d0\u9ad8\u7f51\u7edc\u63a7\u5236\u7cfb\u7edf\u5728\u9762\u5bf9\u6076\u610f\u6d3b\u52a8\u65f6\u7684\u5f39\u6027\uff0c\u786e\u4fdd\u7cfb\u7edf\u5b89\u5168\u8fd0\u884c\uff0c\u89e3\u51b3\u865a\u5047\u6570\u636e\u6ce8\u5165\u7b49\u5b89\u5168\u6311\u6218\u3002", "method": "\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u5143\u5b66\u4e60\u5728\u5806\u53e0\u96c6\u6210\u5b66\u4e60\u67b6\u6784\u4e2d\u9009\u62e9\u57fa\u7840\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u8fde\u7eed\u5c0f\u6ce2\u53d8\u6362\u5c06\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u8f6c\u6362\u4e3a\u5c3a\u5ea6\u56fe\uff0c\u5206\u5272\u6210\u56fe\u50cf\u5e27\u751f\u6210\u5c3a\u5ea6-\u65f6\u95f4\u8868\u793a\uff0c\u5e76\u4f7f\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u57fa\u4e8e\u71b5\u5ea6\u91cf\u533a\u5206\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5b9e\u65f6\u7f13\u89e3\u865a\u5047\u6570\u636e\u6ce8\u5165\u4fe1\u53f7\u3002", "result": "\u901a\u8fc7\u5dee\u901f\u9a71\u52a8\u79fb\u52a8\u673a\u5668\u4eba\u7f16\u961f\u63a7\u5236\u7684\u4e25\u683c\u4eff\u771f\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u89e3\u51b3\u7f51\u7edc\u63a7\u5236\u7cfb\u7edf\u4e2d\u7684\u5b89\u5168\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u7ef4\u62a4\u7cfb\u7edf\u5b8c\u6574\u6027\u5e76\u786e\u4fdd\u64cd\u4f5c\u5b89\u5168\u3002"}}
{"id": "2510.17348", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17348", "abs": "https://arxiv.org/abs/2510.17348", "authors": ["Marc Jourdan", "Achraf Azize"], "title": "Optimal Best Arm Identification under Differential Privacy", "comment": "92 pages, 2 figures, 2 tables. To be published in the Thirty-Ninth\n  Annual Conference on Neural Information Processing Systems", "summary": "Best Arm Identification (BAI) algorithms are deployed in data-sensitive\napplications, such as adaptive clinical trials or user studies. Driven by the\nprivacy concerns of these applications, we study the problem of\nfixed-confidence BAI under global Differential Privacy (DP) for Bernoulli\ndistributions. While numerous asymptotically optimal BAI algorithms exist in\nthe non-private setting, a significant gap remains between the best lower and\nupper bounds in the global DP setting. This work reduces this gap to a small\nmultiplicative constant, for any privacy budget $\\epsilon$. First, we provide a\ntighter lower bound on the expected sample complexity of any $\\delta$-correct\nand $\\epsilon$-global DP strategy. Our lower bound replaces the\nKullback-Leibler (KL) divergence in the transportation cost used by the\nnon-private characteristic time with a new information-theoretic quantity that\noptimally trades off between the KL divergence and the Total Variation distance\nscaled by $\\epsilon$. Second, we introduce a stopping rule based on these\ntransportation costs and a private estimator of the means computed using an\narm-dependent geometric batching. En route to proving the correctness of our\nstopping rule, we derive concentration results of independent interest for the\nLaplace distribution and for the sum of Bernoulli and Laplace distributions.\nThird, we propose a Top Two sampling rule based on these transportation costs.\nFor any budget $\\epsilon$, we show an asymptotic upper bound on its expected\nsample complexity that matches our lower bound to a multiplicative constant\nsmaller than $8$. Our algorithm outperforms existing $\\delta$-correct and\n$\\epsilon$-global DP BAI algorithms for different values of $\\epsilon$.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u5168\u5c40\u5dee\u5206\u9690\u79c1(DP)\u7ea6\u675f\u4e0b\u7684\u6700\u4f73\u81c2\u8bc6\u522b(BAI)\u95ee\u9898\uff0c\u901a\u8fc7\u65b0\u7684\u4fe1\u606f\u8bba\u65b9\u6cd5\u548c\u91c7\u6837\u7b56\u7565\uff0c\u663e\u8457\u7f29\u5c0f\u4e86\u9690\u79c1BAI\u95ee\u9898\u7684\u4e0a\u4e0b\u754c\u5dee\u8ddd\u3002", "motivation": "BAI\u7b97\u6cd5\u5728\u6570\u636e\u654f\u611f\u5e94\u7528(\u5982\u81ea\u9002\u5e94\u4e34\u5e8a\u8bd5\u9a8c)\u4e2d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5728\u5168\u5c40DP\u7ea6\u675f\u4e0b\u7684\u6837\u672c\u590d\u6742\u5ea6\u4e0a\u4e0b\u754c\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u9690\u79c1\u4fdd\u62a4BAI\u7b97\u6cd5\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u8fd0\u8f93\u6210\u672c\u7684\u505c\u6b62\u89c4\u5219\u3001\u4f7f\u7528\u81c2\u76f8\u5173\u51e0\u4f55\u6279\u5904\u7406\u7684\u79c1\u6709\u5747\u503c\u4f30\u8ba1\u5668\uff0c\u4ee5\u53ca\u57fa\u4e8e\u8fd0\u8f93\u6210\u672c\u7684Top Two\u91c7\u6837\u89c4\u5219\uff0c\u5e76\u63a8\u5bfc\u4e86\u62c9\u666e\u62c9\u65af\u5206\u5e03\u548c\u4f2f\u52aa\u5229-\u62c9\u666e\u62c9\u65af\u6df7\u5408\u5206\u5e03\u7684\u6d53\u5ea6\u7ed3\u679c\u3002", "result": "\u5bf9\u4e8e\u4efb\u610f\u9690\u79c1\u9884\u7b97\u03b5\uff0c\u7b97\u6cd5\u5b9e\u73b0\u4e86\u4e0e\u4e0b\u754c\u5339\u914d\u7684\u6e10\u8fd1\u6837\u672c\u590d\u6742\u5ea6\u4e0a\u754c\uff0c\u4e58\u6027\u5e38\u6570\u5c0f\u4e8e8\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u03b4\u6b63\u786e\u548c\u03b5\u5168\u5c40DP BAI\u7b97\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u65b0\u7684\u4fe1\u606f\u8bba\u65b9\u6cd5\u548c\u7b97\u6cd5\u8bbe\u8ba1\uff0c\u663e\u8457\u7f29\u5c0f\u4e86\u9690\u79c1BAI\u95ee\u9898\u7684\u7406\u8bba\u5dee\u8ddd\uff0c\u4e3a\u6570\u636e\u654f\u611f\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u9690\u79c1\u4fdd\u62a4\u6700\u4f73\u81c2\u8bc6\u522b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.16459", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2510.16459", "abs": "https://arxiv.org/abs/2510.16459", "authors": ["Ewa Makowska-T\u0142umak", "Sylwia Bedy\u0144ska", "Kinga Skorupska", "Rados\u0142aw Nielek"], "title": "Women have it Worse: an ICT Workplace Digital Transformation Stress Gender Gap", "comment": null, "summary": "Although information and communication technologies (ICT) solutions have\npositive outcomes for both companies and employees, the digital transformation\n(DT) could have an impact on the well-being of employees. The jobs of the\nemployees became more demanding, and they were expected to learn ICT skills and\ncope with ICT workloads and hassles. Due to negative stereotypes about women's\ndeficiency in technology, these ICT problems could affect female and male\nemployees differently. Thus, we predicted that this additional pressure may\nmanifest itself in higher levels of digital transformation stress (DTS) in\nfemale employees. The results confirmed this prediction and indicated the\nexistence of a gender gap in DTS, measured two-fold - in sentiment analysis of\nhelp desk tickets and self-report using a psychological scale. Based on these\nresults, we explore the need to discuss possible solutions and tools to support\nwomen in ICT-heavy workplace contexts.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u6570\u5b57\u5316\u8f6c\u578b\u4f1a\u7ed9\u5973\u6027\u5458\u5de5\u5e26\u6765\u66f4\u9ad8\u7684\u6570\u5b57\u8f6c\u578b\u538b\u529b\uff0c\u5b58\u5728\u660e\u663e\u7684\u6027\u522b\u5dee\u8ddd\uff0c\u9700\u8981\u901a\u8fc7\u5de5\u5177\u548c\u652f\u6301\u63aa\u65bd\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "motivation": "\u6570\u5b57\u5316\u8f6c\u578b\u867d\u7136\u5e26\u6765\u79ef\u6781\u5f71\u54cd\uff0c\u4f46\u53ef\u80fd\u5bf9\u5458\u5de5\u798f\u7949\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\uff0c\u7279\u522b\u662f\u7531\u4e8e\u5bf9\u5973\u6027\u6280\u672f\u80fd\u529b\u7684\u8d1f\u9762\u523b\u677f\u5370\u8c61\uff0c\u53ef\u80fd\u5bfc\u81f4\u5973\u6027\u5458\u5de5\u9762\u4e34\u66f4\u5927\u7684\u6570\u5b57\u8f6c\u578b\u538b\u529b\u3002", "method": "\u91c7\u7528\u53cc\u91cd\u6d4b\u91cf\u65b9\u6cd5\uff1a\u901a\u8fc7\u5e2e\u52a9\u53f0\u5de5\u5355\u7684\u60c5\u611f\u5206\u6790\u548c\u4f7f\u7528\u5fc3\u7406\u91cf\u8868\u7684\u81ea\u6211\u62a5\u544a\u6765\u6d4b\u91cf\u6570\u5b57\u8f6c\u578b\u538b\u529b\u3002", "result": "\u7ed3\u679c\u8bc1\u5b9e\u4e86\u9884\u6d4b\uff0c\u5973\u6027\u5458\u5de5\u786e\u5b9e\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6570\u5b57\u8f6c\u578b\u538b\u529b\u6c34\u5e73\uff0c\u5b58\u5728\u660e\u663e\u7684\u6027\u522b\u5dee\u8ddd\u3002", "conclusion": "\u9700\u8981\u5728ICT\u5bc6\u96c6\u578b\u5de5\u4f5c\u73af\u5883\u4e2d\u8ba8\u8bba\u53ef\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u548c\u5de5\u5177\u6765\u652f\u6301\u5973\u6027\u5458\u5de5\u3002"}}
{"id": "2510.16234", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16234", "abs": "https://arxiv.org/abs/2510.16234", "authors": ["Hanane Nour Moussa", "Patrick Queiroz Da Silva", "Daniel Adu-Ampratwum", "Alyson East", "Zitong Lu", "Nikki Puccetti", "Mingyi Xue", "Huan Sun", "Bodhisattwa Prasad Majumder", "Sachin Kumar"], "title": "ScholarEval: Research Idea Evaluation Grounded in Literature", "comment": null, "summary": "As AI tools become increasingly common for research ideation, robust\nevaluation is critical to ensure the validity and usefulness of generated\nideas. We introduce ScholarEval, a retrieval augmented evaluation framework\nthat assesses research ideas based on two fundamental criteria: soundness - the\nempirical validity of proposed methods based on existing literature, and\ncontribution - the degree of advancement made by the idea across different\ndimensions relative to prior research. To evaluate ScholarEval, we introduce\nScholarIdeas, the first expert-annotated dataset of multi-domain research ideas\nand reviews, comprised of 117 ideas across four disciplines: artificial\nintelligence, neuroscience, biochemistry, and ecology. Our evaluation shows\nthat ScholarEval achieves significantly higher coverage of points mentioned in\nthe human expert annotated rubrics in ScholarIdeas compared to all baselines.\nFurthermore, ScholarEval is consistently preferred over our strongest baseline\no4-mini-deep-research, a reasoning and search-enabled agentic system by OpenAI,\nin terms of evaluation actionability, depth, and evidence support. Our\nlarge-scale user study also shows that ScholarEval significantly outperforms\ndeep research in literature engagement, idea refinement, and usefulness. We\nopenly release our code, dataset, and ScholarEval tool for the community to use\nand build on.", "AI": {"tldr": "\u63d0\u51fa\u4e86ScholarEval\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u8bc4\u4f30\u7814\u7a76\u60f3\u6cd5\u7684\u5408\u7406\u6027\u548c\u8d21\u732e\u5ea6\uff0c\u5e76\u5728\u591a\u9886\u57df\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u5176\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740AI\u5de5\u5177\u5728\u7814\u7a76\u6784\u601d\u4e2d\u65e5\u76ca\u666e\u53ca\uff0c\u9700\u8981\u5efa\u7acb\u7a33\u5065\u7684\u8bc4\u4f30\u673a\u5236\u6765\u786e\u4fdd\u751f\u6210\u60f3\u6cd5\u7684\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002", "method": "\u5f15\u5165ScholarEval\u68c0\u7d22\u589e\u5f3a\u8bc4\u4f30\u6846\u67b6\uff0c\u57fa\u4e8e\u4e24\u4e2a\u6838\u5fc3\u6807\u51c6\u8bc4\u4f30\u7814\u7a76\u60f3\u6cd5\uff1a\u5408\u7406\u6027\uff08\u57fa\u4e8e\u73b0\u6709\u6587\u732e\u7684\u65b9\u6cd5\u5b9e\u8bc1\u6709\u6548\u6027\uff09\u548c\u8d21\u732e\u5ea6\uff08\u76f8\u5bf9\u4e8e\u5148\u524d\u7814\u7a76\u5728\u4e0d\u540c\u7ef4\u5ea6\u4e0a\u7684\u63a8\u8fdb\u7a0b\u5ea6\uff09\u3002", "result": "\u5728ScholarIdeas\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cScholarEval\u5728\u8986\u76d6\u4e13\u5bb6\u6807\u6ce8\u8981\u70b9\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u5728\u8bc4\u4f30\u53ef\u64cd\u4f5c\u6027\u3001\u6df1\u5ea6\u548c\u8bc1\u636e\u652f\u6301\u65b9\u9762\u6301\u7eed\u4f18\u4e8eOpenAI\u7684o4-mini-deep-research\u7cfb\u7edf\u3002\u5927\u89c4\u6a21\u7528\u6237\u7814\u7a76\u4e5f\u8bc1\u5b9e\u5176\u5728\u6587\u732e\u53c2\u4e0e\u3001\u60f3\u6cd5\u7cbe\u70bc\u548c\u5b9e\u7528\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "ScholarEval\u4e3a\u7814\u7a76\u60f3\u6cd5\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u5176\u4ee3\u7801\u3001\u6570\u636e\u96c6\u548c\u5de5\u5177\u5df2\u5f00\u6e90\u4f9b\u793e\u533a\u4f7f\u7528\u548c\u53d1\u5c55\u3002"}}
{"id": "2510.16818", "categories": ["math.OC", "90C26, 90C30, 90C33"], "pdf": "https://arxiv.org/pdf/2510.16818", "abs": "https://arxiv.org/abs/2510.16818", "authors": ["Mengwei Xu", "Yu-Hong Dai", "Xin-Wei Liu", "Meiqi Ma"], "title": "A Surrogate Value Function Formulation for Bilevel Optimization", "comment": null, "summary": "The value function formulation captures the hierarchical nature of bilevel\noptimization through the optimal value function of the lower level problem, yet\nits implicit and nonsmooth characteristics pose significant analytical and\ncomputational difficulties. We introduce a surrogate value function formulation\nthat replaces the intractable value function with an explicit surrogate derived\nfrom lower level stationarity conditions. This surrogate formulation preserves\nthe essential idea of the classical value function model but fundamentally\ndeparts from Karush Kuhn Tucker (KKT) formulations, which embed lower level\nstationary points into the upper level feasible region and obscure the\nhierarchical dependence. Instead, it enforces the hierarchy through a dominance\nconstraint that remains valid even when lower level constraint qualifications\nfail at the solution. We establish equivalence with the original bilevel\nproblem, reveal the failure of standard constraint qualifications, and show\nthat its strong stationarity implies that of KKT models. To handle the\ncomplementarity constraints in the surrogate formulation, we apply a smoothing\nbarrier augmented Lagrangian method and prove its convergence to solutions and\nClarke stationary points. Extensive experiments demonstrate the robustness and\nhigh numerical precision of this formulation, especially in nonconvex settings,\nincluding the classical Mirrlees problem where KKT models fail.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u66ff\u4ee3\u4ef7\u503c\u51fd\u6570\u516c\u5f0f\uff0c\u7528\u663e\u5f0f\u66ff\u4ee3\u51fd\u6570\u66ff\u6362\u96be\u4ee5\u5904\u7406\u7684\u4ef7\u503c\u51fd\u6570\uff0c\u4fdd\u6301\u53cc\u5c42\u6b21\u4f18\u5316\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u514b\u670dKKT\u516c\u5f0f\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u975e\u51f8\u8bbe\u7f6e\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u548c\u9ad8\u6570\u503c\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u4ef7\u503c\u51fd\u6570\u516c\u5f0f\u867d\u7136\u6355\u6349\u4e86\u53cc\u5c42\u6b21\u4f18\u5316\u7684\u5c42\u6b21\u7279\u6027\uff0c\u4f46\u5176\u9690\u5f0f\u548c\u975e\u5149\u6ed1\u7279\u6027\u5e26\u6765\u4e86\u5206\u6790\u548c\u8ba1\u7b97\u56f0\u96be\u3002KKT\u516c\u5f0f\u5c06\u4e0b\u5c42\u5e73\u7a33\u70b9\u5d4c\u5165\u4e0a\u5c42\u53ef\u884c\u533a\u57df\uff0c\u6a21\u7cca\u4e86\u5c42\u6b21\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "\u5f15\u5165\u66ff\u4ee3\u4ef7\u503c\u51fd\u6570\u516c\u5f0f\uff0c\u4ece\u4e0b\u5c42\u5e73\u7a33\u6761\u4ef6\u63a8\u5bfc\u663e\u5f0f\u66ff\u4ee3\u51fd\u6570\uff0c\u901a\u8fc7\u652f\u914d\u7ea6\u675f\u4fdd\u6301\u5c42\u6b21\u7ed3\u6784\uff0c\u5e94\u7528\u5e73\u6ed1\u969c\u788d\u589e\u5e7f\u62c9\u683c\u6717\u65e5\u65b9\u6cd5\u5904\u7406\u4e92\u8865\u7ea6\u675f\u3002", "result": "\u8bc1\u660e\u4e86\u4e0e\u539f\u59cb\u53cc\u5c42\u6b21\u95ee\u9898\u7684\u7b49\u4ef7\u6027\uff0c\u63ed\u793a\u4e86\u6807\u51c6\u7ea6\u675f\u6761\u4ef6\u7684\u5931\u6548\uff0c\u5c55\u793a\u4e86\u5f3a\u5e73\u7a33\u6027\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5728\u975e\u51f8\u8bbe\u7f6e\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u9ad8\u7cbe\u5ea6\uff0c\u7279\u522b\u662f\u5728KKT\u6a21\u578b\u5931\u8d25\u7684Mirrlees\u95ee\u9898\u4e2d\u3002", "conclusion": "\u66ff\u4ee3\u4ef7\u503c\u51fd\u6570\u516c\u5f0f\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u5904\u7406\u53cc\u5c42\u6b21\u4f18\u5316\u7684\u65b0\u65b9\u6cd5\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u7406\u8bba\u548c\u6570\u503c\u4e0a\u90fd\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2510.15970", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15970", "abs": "https://arxiv.org/abs/2510.15970", "authors": ["Yang Ba", "Mohammad Sadeq Abolhasani", "Rong Pan"], "title": "Predict Training Data Quality via Its Geometry in Metric Space", "comment": "Accepted to the NeurIPS 2025 Workshop on New Perspectives in Graph\n  Machine Learning", "summary": "High-quality training data is the foundation of machine learning and\nartificial intelligence, shaping how models learn and perform. Although much is\nknown about what types of data are effective for training, the impact of the\ndata's geometric structure on model performance remains largely underexplored.\nWe propose that both the richness of representation and the elimination of\nredundancy within training data critically influence learning outcomes. To\ninvestigate this, we employ persistent homology to extract topological features\nfrom data within a metric space, thereby offering a principled way to quantify\ndiversity beyond entropy-based measures. Our findings highlight persistent\nhomology as a powerful tool for analyzing and enhancing the training data that\ndrives AI systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u6301\u4e45\u540c\u8c03\u5206\u6790\u8bad\u7ec3\u6570\u636e\u7684\u51e0\u4f55\u7ed3\u6784\u5bf9\u673a\u5668\u5b66\u4e60\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5f3a\u8c03\u6570\u636e\u8868\u793a\u7684\u4e30\u5bcc\u6027\u548c\u5197\u4f59\u6d88\u9664\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u867d\u7136\u5df2\u77e5\u8bad\u7ec3\u6570\u636e\u7684\u7c7b\u578b\u5bf9\u673a\u5668\u5b66\u4e60\u5f88\u91cd\u8981\uff0c\u4f46\u6570\u636e\u7684\u51e0\u4f55\u7ed3\u6784\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u4f5c\u8005\u8ba4\u4e3a\u6570\u636e\u7684\u8868\u793a\u4e30\u5bcc\u6027\u548c\u5197\u4f59\u6d88\u9664\u5bf9\u5b66\u4e60\u7ed3\u679c\u6709\u91cd\u8981\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u6301\u4e45\u540c\u8c03\u4ece\u5ea6\u91cf\u7a7a\u95f4\u4e2d\u7684\u6570\u636e\u63d0\u53d6\u62d3\u6251\u7279\u5f81\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u8d85\u8d8a\u57fa\u4e8e\u71b5\u7684\u5ea6\u91cf\u6765\u91cf\u5316\u591a\u6837\u6027\u7684\u539f\u5219\u6027\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6301\u4e45\u540c\u8c03\u662f\u5206\u6790\u548c\u589e\u5f3a\u9a71\u52a8AI\u7cfb\u7edf\u7684\u8bad\u7ec3\u6570\u636e\u7684\u5f3a\u5927\u5de5\u5177\u3002", "conclusion": "\u6301\u4e45\u540c\u8c03\u4e3a\u7406\u89e3\u548c\u6539\u8fdb\u8bad\u7ec3\u6570\u636e\u7684\u51e0\u4f55\u7ed3\u6784\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5206\u6790\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u63d0\u5347AI\u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2510.16381", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16381", "abs": "https://arxiv.org/abs/2510.16381", "authors": ["David Peer", "Sebastian Stabinger"], "title": "ATA: A Neuro-Symbolic Approach to Implement Autonomous and Trustworthy Agents", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities, yet\ntheir deployment in high-stakes domains is hindered by inherent limitations in\ntrustworthiness, including hallucinations, instability, and a lack of\ntransparency. To address these challenges, we introduce a generic\nneuro-symbolic approach, which we call Autonomous Trustworthy Agents (ATA). The\ncore of our approach lies in decoupling tasks into two distinct phases: Offline\nknowledge ingestion and online task processing. During knowledge ingestion, an\nLLM translates an informal problem specification into a formal, symbolic\nknowledge base. This formal representation is crucial as it can be verified and\nrefined by human experts, ensuring its correctness and alignment with domain\nrequirements. In the subsequent task processing phase, each incoming input is\nencoded into the same formal language. A symbolic decision engine then utilizes\nthis encoded input in conjunction with the formal knowledge base to derive a\nreliable result. Through an extensive evaluation on a complex reasoning task,\nwe demonstrate that a concrete implementation of ATA is competitive with\nstate-of-the-art end-to-end reasoning models in a fully automated setup while\nmaintaining trustworthiness. Crucially, with a human-verified and corrected\nknowledge base, our approach significantly outperforms even larger models,\nwhile exhibiting perfect determinism, enhanced stability against input\nperturbations, and inherent immunity to prompt injection attacks. By generating\ndecisions grounded in symbolic reasoning, ATA offers a practical and\ncontrollable architecture for building the next generation of transparent,\nauditable, and reliable autonomous agents.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3a\u81ea\u4e3b\u53ef\u4fe1\u4ee3\u7406(ATA)\u7684\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u79bb\u7ebf\u77e5\u8bc6\u6444\u53d6\u548c\u5728\u7ebf\u4efb\u52a1\u5904\u7406\u4e24\u4e2a\u9636\u6bb5\uff0c\u89e3\u51b3LLM\u5728\u53ef\u4fe1\u5ea6\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u53ef\u4fe1\u5ea6\u65b9\u9762\u5b58\u5728\u5e7b\u89c9\u3001\u4e0d\u7a33\u5b9a\u6027\u548c\u7f3a\u4e4f\u900f\u660e\u5ea6\u7b49\u5c40\u9650\u6027\uff0c\u963b\u788d\u4e86\u5176\u5728\u9ad8\u98ce\u9669\u9886\u57df\u7684\u90e8\u7f72\u3002", "method": "\u91c7\u7528\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\uff0c\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\uff1a1)\u79bb\u7ebf\u9636\u6bb5\u5c06\u975e\u6b63\u5f0f\u95ee\u9898\u89c4\u8303\u8f6c\u6362\u4e3a\u53ef\u9a8c\u8bc1\u7684\u5f62\u5f0f\u5316\u77e5\u8bc6\u5e93\uff1b2)\u5728\u7ebf\u9636\u6bb5\u4f7f\u7528\u7b26\u53f7\u51b3\u7b56\u5f15\u64ce\u57fa\u4e8e\u5f62\u5f0f\u5316\u77e5\u8bc6\u5e93\u8fdb\u884c\u63a8\u7406\u3002", "result": "\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cATA\u5728\u5b8c\u5168\u81ea\u52a8\u5316\u8bbe\u7f6e\u4e2d\u4e0e\u6700\u5148\u8fdb\u7684\u7aef\u5230\u7aef\u63a8\u7406\u6a21\u578b\u7ade\u4e89\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u4fe1\u5ea6\u3002\u4f7f\u7528\u4eba\u5de5\u9a8c\u8bc1\u7684\u77e5\u8bc6\u5e93\u65f6\uff0cATA\u663e\u8457\u4f18\u4e8e\u66f4\u5927\u7684\u6a21\u578b\uff0c\u5e76\u8868\u73b0\u51fa\u5b8c\u7f8e\u7684\u786e\u5b9a\u6027\u3001\u589e\u5f3a\u7684\u7a33\u5b9a\u6027\u4ee5\u53ca\u5bf9\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u7684\u514d\u75ab\u6027\u3002", "conclusion": "ATA\u901a\u8fc7\u57fa\u4e8e\u7b26\u53f7\u63a8\u7406\u7684\u51b3\u7b56\u751f\u6210\uff0c\u4e3a\u6784\u5efa\u900f\u660e\u3001\u53ef\u5ba1\u8ba1\u548c\u53ef\u9760\u7684\u4e0b\u4e00\u4ee3\u81ea\u4e3b\u4ee3\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u63a7\u7684\u67b6\u6784\u3002"}}
{"id": "2510.17176", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.17176", "abs": "https://arxiv.org/abs/2510.17176", "authors": ["Lakshmikanta Sau", "Priyadarshi Mukherjee", "Sasthi C. Ghosh"], "title": "Generalized Group Selection Strategies for Self-sustainable RIS-aided Communication", "comment": "This work has been submitted to an IEEE journal for possible\n  publication", "summary": "Reconfigurable intelligent surface (RIS) is a cutting-edge communication\ntechnology that has been proposed as aviable option for beyond fifth-generation\nwireless communication networks. This paper investigates various group\nselection strategies in the context of grouping-based self-sustainable\nRIS-aided device-to-device (D2D) communication with spatially correlated\nwireless channels. Specifically, we consider both power splitting (PS) and time\nswitching (TS) configurations, of the self-sustainable RIS to analyze the\nsystem performance and propose appropriate bounds on the choice of system\nparameters. The analysis takes into account a simplified linear energy\nharvesting (EH) model as well as a practical non-linear EH model. Based on the\napplication requirements, we propose various group selection strategies at the\nRIS. Notably, each strategy schedules the k-th best available group at the RIS\nbased on the end-to-end signal-to-noise ratio (SNR) and also the energy\nharvested at a particular group of the RIS. Accordingly, by using tools from\nhigh order statistics, we derive analytical expressions for the outage\nprobability of each selection strategy. Moreover, by applying the tools from\nextreme value theory, we also investigate an asymptotic scenario, where the\nnumber of groups available for selection at an RIS approaches infinity. The\nnontrivial insights obtained from this approach is especially beneficial in\napplications like large intelligent surface-aided wireless communication.\nFinally, the numerical results demonstrate the importance and benefits of the\nproposed approaches in terms of metrics such as the data throughput and the\noutage (both data and energy) performance.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u7a7a\u95f4\u76f8\u5173\u4fe1\u9053\u4e0b\uff0c\u57fa\u4e8e\u5206\u7ec4\u7684\u81ea\u53ef\u6301\u7eedRIS\u8f85\u52a9D2D\u901a\u4fe1\u4e2d\u7684\u591a\u79cd\u5206\u7ec4\u9009\u62e9\u7b56\u7565\uff0c\u5206\u6790\u4e86\u529f\u7387\u5206\u914d\u548c\u65f6\u95f4\u5207\u6362\u914d\u7f6e\u4e0b\u7684\u7cfb\u7edf\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86\u7cfb\u7edf\u53c2\u6570\u9009\u62e9\u7684\u9002\u5f53\u754c\u9650\u3002", "motivation": "\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762(RIS)\u662f\u8d85\u8d8a\u7b2c\u4e94\u4ee3\u65e0\u7ebf\u901a\u4fe1\u7f51\u7edc\u7684\u524d\u6cbf\u6280\u672f\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5728\u81ea\u53ef\u6301\u7eedRIS\u8f85\u52a9D2D\u901a\u4fe1\u4e2d\uff0c\u901a\u8fc7\u4f18\u5316\u5206\u7ec4\u9009\u62e9\u7b56\u7565\u6765\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u3002", "method": "\u91c7\u7528\u529f\u7387\u5206\u914d(PS)\u548c\u65f6\u95f4\u5207\u6362(TS)\u4e24\u79cd\u914d\u7f6e\uff0c\u8003\u8651\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u80fd\u91cf\u6536\u96c6\u6a21\u578b\uff0c\u63d0\u51fa\u57fa\u4e8e\u7aef\u5230\u7aef\u4fe1\u566a\u6bd4\u548c\u80fd\u91cf\u6536\u96c6\u7684\u5206\u7ec4\u9009\u62e9\u7b56\u7565\uff0c\u4f7f\u7528\u9ad8\u9636\u7edf\u8ba1\u5de5\u5177\u63a8\u5bfc\u4e2d\u65ad\u6982\u7387\u7684\u89e3\u6790\u8868\u8fbe\u5f0f\uff0c\u5e76\u5e94\u7528\u6781\u503c\u7406\u8bba\u5206\u6790\u6e10\u8fd1\u573a\u666f\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\u6240\u63d0\u65b9\u6cd5\u5728\u6570\u636e\u541e\u5410\u91cf\u3001\u6570\u636e\u548c\u80fd\u91cf\u4e2d\u65ad\u6027\u80fd\u7b49\u6307\u6807\u65b9\u9762\u5177\u6709\u91cd\u8981\u4f18\u52bf\u548c\u6548\u76ca\uff0c\u6781\u503c\u7406\u8bba\u5206\u6790\u4e3a\u5927\u89c4\u6a21\u667a\u80fd\u8868\u9762\u8f85\u52a9\u65e0\u7ebf\u901a\u4fe1\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\u5728\u81ea\u53ef\u6301\u7eedRIS\u8f85\u52a9D2D\u901a\u4fe1\u4e2d\uff0c\u9002\u5f53\u7684\u5206\u7ec4\u9009\u62e9\u7b56\u7565\u80fd\u591f\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5927\u89c4\u6a21RIS\u573a\u666f\u4e0b\uff0c\u6240\u63d0\u51fa\u7684\u5206\u6790\u65b9\u6cd5\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.17472", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17472", "abs": "https://arxiv.org/abs/2510.17472", "authors": ["Paula Cordero-Encinar", "Andrew B. Duncan"], "title": "Certified Self-Consistency: Statistical Guarantees and Test-Time Training for Reliable Reasoning in LLMs", "comment": null, "summary": "Recent advances such as self-consistency and test-time reinforcement learning\n(TTRL) improve the reliability of large language models (LLMs) without\nadditional supervision, yet their underlying mechanisms and statistical\nguarantees remain poorly understood. We present a unified framework for\ncertifiable inference in LLMs, showing that majority voting provides a\nstatistical certificate of self-consistency: under mild assumptions, the\naggregated answer coincides with the mode of the model's terminal distribution\nwith high probability. We derive finite-sample and anytime-valid concentration\nbounds that quantify this confidence, and introduce the Martingale Majority\nCertificate (MMC), a sequential stopping rule that adaptively determines when\nsufficient samples have been drawn. We further prove that label-free\npost-training methods such as TTRL implicitly sharpen the answer distribution\nby exponentially tilting it toward its mode, thereby reducing the number of\nsamples required for certification. Building on this insight, we propose new\npost-training objectives that explicitly optimise this trade-off between\nsharpness and bias. Together, these results explain and connect two central\ntest-time scaling strategies, self-consistency and TTRL, within a single\nstatistical framework for label-free, certifiable reliability in reasoning\nLLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u53ef\u9a8c\u8bc1\u63a8\u7406\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u8bc1\u660e\u591a\u6570\u6295\u7968\u63d0\u4f9b\u81ea\u4e00\u81f4\u6027\u7684\u7edf\u8ba1\u4fdd\u8bc1\uff0c\u5e76\u5f15\u5165\u81ea\u9002\u5e94\u505c\u6b62\u89c4\u5219\u548c\u65b0\u7684\u540e\u8bad\u7ec3\u76ee\u6807\u6765\u4f18\u5316\u7f6e\u4fe1\u5ea6\u4e0e\u504f\u5dee\u4e4b\u95f4\u7684\u6743\u8861\u3002", "motivation": "\u5f53\u524d\u81ea\u4e00\u81f4\u6027\u548c\u6d4b\u8bd5\u65f6\u5f3a\u5316\u5b66\u4e60\u7b49\u65e0\u76d1\u7763\u65b9\u6cd5\u867d\u7136\u63d0\u9ad8\u4e86LLM\u7684\u53ef\u9760\u6027\uff0c\u4f46\u5176\u673a\u5236\u548c\u7edf\u8ba1\u4fdd\u8bc1\u4ecd\u7f3a\u4e4f\u6df1\u5165\u7406\u89e3\uff0c\u9700\u8981\u5efa\u7acb\u7edf\u4e00\u7684\u7edf\u8ba1\u6846\u67b6\u6765\u89e3\u91ca\u8fd9\u4e9b\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u4e86\u591a\u6570\u6295\u7968\u7684\u7edf\u8ba1\u8bc1\u4e66\u6846\u67b6\uff0c\u63a8\u5bfc\u6709\u9650\u6837\u672c\u548c\u4efb\u610f\u65f6\u95f4\u6709\u6548\u7684\u96c6\u4e2d\u754c\u9650\uff0c\u5f15\u5165Martingale\u591a\u6570\u8bc1\u4e66\u4f5c\u4e3a\u81ea\u9002\u5e94\u505c\u6b62\u89c4\u5219\uff0c\u5e76\u63d0\u51fa\u65b0\u7684\u540e\u8bad\u7ec3\u76ee\u6807\u6765\u663e\u5f0f\u4f18\u5316\u5206\u5e03\u9510\u5316\u4e0e\u504f\u5dee\u7684\u6743\u8861\u3002", "result": "\u8bc1\u660e\u4e86\u591a\u6570\u6295\u7968\u5728\u6e29\u548c\u5047\u8bbe\u4e0b\u4ee5\u9ad8\u6982\u7387\u4e0e\u6a21\u578b\u7ec8\u7aef\u5206\u5e03\u7684\u4f17\u6570\u4e00\u81f4\uff0cTTRL\u65b9\u6cd5\u901a\u8fc7\u6307\u6570\u503e\u659c\u9690\u5f0f\u9510\u5316\u7b54\u6848\u5206\u5e03\uff0c\u51cf\u5c11\u4e86\u8ba4\u8bc1\u6240\u9700\u7684\u6837\u672c\u6570\u91cf\u3002", "conclusion": "\u8be5\u7814\u7a76\u5728\u5355\u4e00\u7edf\u8ba1\u6846\u67b6\u5185\u89e3\u91ca\u5e76\u8fde\u63a5\u4e86\u81ea\u4e00\u81f4\u6027\u548cTTRL\u8fd9\u4e24\u79cd\u6838\u5fc3\u6d4b\u8bd5\u65f6\u6269\u5c55\u7b56\u7565\uff0c\u4e3a\u65e0\u6807\u7b7e\u3001\u53ef\u9a8c\u8bc1\u7684\u63a8\u7406LLM\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2510.16847", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2510.16847", "abs": "https://arxiv.org/abs/2510.16847", "authors": ["Roberto Massi De Oliveira", "M^onica Cristina Garbin", "Rodolfo Azevedo"], "title": "Global Overview of Computational Thinking and Digital Tools for Teaching", "comment": "36 pages, 7 figures, 4 tables", "summary": "Computational Thinking (CT) has emerged as a critical component in modern\neducation, essential to equip students with the skills necessary to thrive in a\ntechnology-driven world. This survey provides a comprehensive analysis of the\npresence and integration of CT in school curricula across various countries. In\naddition, this study categorizes digital tools into groups such as visual\nprogramming, textual programming, electronic games, modeling, and simulation,\nassessing their use in different educational settings. Furthermore, it examines\nhow these tools are employed in various contexts, including the areas of\nknowledge and age groups they target, and the specific skills they help\ndevelop. The research also identifies key CT competencies that have been\nimproved through these tools, including Cognitive and Analytical Competencies\n(CAC), Technical and Computational Competencies (TCC) and Social and Emotional\nCompetencies (SEC). Furthermore, the study highlights recurring challenges in\nthe implementation of digital tools for CT development, such as inadequate\ninfrastructure, difficulties in the usability of the tool, teacher training,\nadapting pedagogical practices, and measuring student CT skills. Finally, it\nproposes areas for future research to address these challenges and advance CT\neducation.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u5206\u6790\u4e86\u8ba1\u7b97\u601d\u7ef4\u5728\u5b66\u6821\u8bfe\u7a0b\u4e2d\u7684\u6574\u5408\u60c5\u51b5\uff0c\u5bf9\u6570\u5b57\u5de5\u5177\u8fdb\u884c\u5206\u7c7b\u5e76\u8bc4\u4f30\u5176\u5728\u4e0d\u540c\u6559\u80b2\u73af\u5883\u4e2d\u7684\u4f7f\u7528\uff0c\u8bc6\u522b\u4e86\u8ba1\u7b97\u601d\u7ef4\u80fd\u529b\u7684\u63d0\u5347\u9886\u57df\u548c\u9762\u4e34\u7684\u6311\u6218\u3002", "motivation": "\u8ba1\u7b97\u601d\u7ef4\u5df2\u6210\u4e3a\u73b0\u4ee3\u6559\u80b2\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\uff0c\u9700\u8981\u57f9\u517b\u5b66\u751f\u9002\u5e94\u6280\u672f\u9a71\u52a8\u4e16\u754c\u7684\u6280\u80fd\uff0c\u56e0\u6b64\u9700\u8981\u5168\u9762\u4e86\u89e3\u8ba1\u7b97\u601d\u7ef4\u5728\u5b66\u6821\u8bfe\u7a0b\u4e2d\u7684\u6574\u5408\u73b0\u72b6\u3002", "method": "\u91c7\u7528\u7efc\u8ff0\u7814\u7a76\u65b9\u6cd5\uff0c\u5bf9\u5404\u56fd\u5b66\u6821\u8bfe\u7a0b\u4e2d\u8ba1\u7b97\u601d\u7ef4\u7684\u5b58\u5728\u548c\u6574\u5408\u8fdb\u884c\u5168\u9762\u5206\u6790\uff0c\u5c06\u6570\u5b57\u5de5\u5177\u5206\u7c7b\u4e3a\u53ef\u89c6\u5316\u7f16\u7a0b\u3001\u6587\u672c\u7f16\u7a0b\u3001\u7535\u5b50\u6e38\u620f\u3001\u5efa\u6a21\u548c\u6a21\u62df\u7b49\u7c7b\u522b\uff0c\u5e76\u8bc4\u4f30\u5176\u5728\u4e0d\u540c\u6559\u80b2\u73af\u5883\u4e2d\u7684\u4f7f\u7528\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6570\u5b57\u5de5\u5177\u5728\u63d0\u5347\u8ba4\u77e5\u5206\u6790\u80fd\u529b\u3001\u6280\u672f\u8ba1\u7b97\u80fd\u529b\u4ee5\u53ca\u793e\u4f1a\u60c5\u611f\u80fd\u529b\u65b9\u9762\u53d1\u6325\u4e86\u91cd\u8981\u4f5c\u7528\uff0c\u4f46\u540c\u65f6\u4e5f\u9762\u4e34\u57fa\u7840\u8bbe\u65bd\u4e0d\u8db3\u3001\u5de5\u5177\u53ef\u7528\u6027\u56f0\u96be\u3001\u6559\u5e08\u57f9\u8bad\u3001\u6559\u5b66\u5b9e\u8df5\u9002\u5e94\u548c\u5b66\u751f\u8ba1\u7b97\u601d\u7ef4\u6280\u80fd\u6d4b\u91cf\u7b49\u6311\u6218\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u65b9\u5411\uff0c\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u5e76\u63a8\u8fdb\u8ba1\u7b97\u601d\u7ef4\u6559\u80b2\u7684\u53d1\u5c55\uff0c\u5f3a\u8c03\u4e86\u9700\u8981\u8fdb\u4e00\u6b65\u5173\u6ce8\u57fa\u7840\u8bbe\u65bd\u3001\u6559\u5e08\u4e13\u4e1a\u53d1\u5c55\u548c\u6709\u6548\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2510.16259", "categories": ["cs.AI", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.16259", "abs": "https://arxiv.org/abs/2510.16259", "authors": ["Zhehao Zhang", "Weijie Xu", "Shixian Cui", "Chandan K. Reddy"], "title": "Distractor Injection Attacks on Large Reasoning Models: Characterization and Defense", "comment": "29 pages, 9 tables, 4 figures", "summary": "Recent advances in large reasoning models (LRMs) have enabled remarkable\nperformance on complex tasks such as mathematics and coding by generating long\nChain-of-Thought (CoT) traces. In this paper, we identify and systematically\nanalyze a critical vulnerability we term reasoning distraction, where LRMs are\ndiverted from their primary objective by irrelevant yet complex tasks\nmaliciously embedded in the prompt. Through a comprehensive study across\ndiverse models and benchmarks, we show that even state-of-the-art LRMs are\nhighly susceptible, with injected distractors reducing task accuracy by up to\n60%. We further reveal that certain alignment techniques can amplify this\nweakness and that models may exhibit covert compliance, following hidden\nadversarial instructions in reasoning while concealing them in the final\noutput. To mitigate these risks, we propose a training-based defense that\ncombines Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on\nsynthetic adversarial data, improving robustness by over 50 points on\nchallenging distractor attacks. Our findings establish reasoning distraction as\na distinct and urgent threat to LRM reliability and provide a practical step\ntoward safer and more trustworthy reasoning systems.", "AI": {"tldr": "\u672c\u6587\u53d1\u73b0\u5927\u578b\u63a8\u7406\u6a21\u578b\u5b58\u5728\"\u63a8\u7406\u5206\u5fc3\"\u6f0f\u6d1e\uff0c\u5373\u6a21\u578b\u4f1a\u88ab\u63d0\u793a\u4e2d\u5d4c\u5165\u7684\u65e0\u5173\u590d\u6742\u4efb\u52a1\u5e72\u6270\uff0c\u5bfc\u81f4\u4e3b\u8981\u4efb\u52a1\u51c6\u786e\u7387\u4e0b\u964d\u9ad8\u8fbe60%\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u8bad\u7ec3\u9632\u5fa1\u65b9\u6cd5\uff0c\u53ef\u5c06\u9c81\u68d2\u6027\u63d0\u9ad850\u4e2a\u767e\u5206\u70b9\u4ee5\u4e0a\u3002", "motivation": "\u968f\u7740\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u7b49\u590d\u6742\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f5c\u8005\u53d1\u73b0\u8fd9\u4e9b\u6a21\u578b\u5bb9\u6613\u53d7\u5230\u6076\u610f\u5d4c\u5165\u7684\u65e0\u5173\u590d\u6742\u4efb\u52a1\u5e72\u6270\uff0c\u8fd9\u79cd\"\u63a8\u7406\u5206\u5fc3\"\u6f0f\u6d1e\u4e25\u91cd\u5a01\u80c1\u6a21\u578b\u53ef\u9760\u6027\u3002", "method": "\u901a\u8fc7\u8de8\u6a21\u578b\u548c\u57fa\u51c6\u7684\u7efc\u5408\u7814\u7a76\uff0c\u5206\u6790\u4e86\u63a8\u7406\u5206\u5fc3\u6f0f\u6d1e\u7684\u4e25\u91cd\u6027\u3002\u4e3a\u7f13\u89e3\u98ce\u9669\uff0c\u63d0\u51fa\u4e86\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u8bad\u7ec3\u9632\u5fa1\u65b9\u6cd5\uff0c\u4f7f\u7528\u5408\u6210\u7684\u5bf9\u6297\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u7814\u7a76\u8868\u660e\u6700\u5148\u8fdb\u7684\u5927\u578b\u63a8\u7406\u6a21\u578b\u9ad8\u5ea6\u6613\u53d7\u653b\u51fb\uff0c\u6ce8\u5165\u5e72\u6270\u7269\u53ef\u4f7f\u4efb\u52a1\u51c6\u786e\u7387\u4e0b\u964d\u9ad8\u8fbe60%\u3002\u67d0\u4e9b\u5bf9\u9f50\u6280\u672f\u4f1a\u653e\u5927\u8fd9\u79cd\u5f31\u70b9\uff0c\u6a21\u578b\u53ef\u80fd\u8868\u73b0\u51fa\u9690\u853d\u5408\u89c4\u6027\u3002\u63d0\u51fa\u7684\u9632\u5fa1\u65b9\u6cd5\u5728\u6311\u6218\u6027\u5e72\u6270\u653b\u51fb\u4e0a\u5c06\u9c81\u68d2\u6027\u63d0\u9ad8\u4e8650\u591a\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u63a8\u7406\u5206\u5fc3\u662f\u5bf9\u5927\u578b\u63a8\u7406\u6a21\u578b\u53ef\u9760\u6027\u7684\u72ec\u7279\u4e14\u7d27\u8feb\u7684\u5a01\u80c1\uff0c\u672c\u6587\u4e3a\u6784\u5efa\u66f4\u5b89\u5168\u53ef\u4fe1\u7684\u63a8\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u6b65\u9aa4\u3002"}}
{"id": "2510.16827", "categories": ["math.OC", "65K05, 90C30"], "pdf": "https://arxiv.org/pdf/2510.16827", "abs": "https://arxiv.org/abs/2510.16827", "authors": ["Kangkang Deng", "Rui Wang", "Zhenyuan Zhu", "Junyu Zhang", "Zaiwen Wen"], "title": "The Augmented Lagrangian Methods: Overview and Recent Advances", "comment": null, "summary": "Large-scale constrained optimization is pivotal in modern scientific,\nengineering, and industrial computation, often involving complex systems with\nnumerous variables and constraints. This paper provides a unified and\ncomprehensive perspective on constructing augmented Lagrangian functions (based\non Hestenes-Powell-Rockafellar augmented Lagrangian) for various optimization\nproblems, including nonlinear programming and convex and nonconvex composite\nprogramming. We present the augmented Lagrangian method (ALM), covering its\ntheoretical foundations in both convex and nonconvex cases, and discuss several\nsuccessful examples and applications. Recent advancements have extended ALM's\ncapabilities to handle nonconvex constraints and ensure global convergence to\nfirst and second-order stationary points. For nonsmooth convex problems, ALM\nutilizes proximal operations, preserving desirable properties such as locally\nlinear convergence rates. Furthermore, recent progress has refined the\ncomplexity analysis for ALM and tackled challenging integer programming\ninstances. This review aims to offer a thorough understanding of ALM's benefits\nand limitations, exploring different ALM variants designed to enhance\nconvergence and computational performance. We also illustrate effective\nalgorithms for ALM subproblems across different types of optimization problems\nand highlight practical implementations in several fields.", "AI": {"tldr": "\u672c\u6587\u5bf9\u589e\u5e7f\u62c9\u683c\u6717\u65e5\u65b9\u6cd5(ALM)\u8fdb\u884c\u4e86\u5168\u9762\u7efc\u8ff0\uff0c\u6db5\u76d6\u4e86\u5176\u5728\u975e\u7ebf\u6027\u89c4\u5212\u3001\u51f8\u548c\u975e\u51f8\u590d\u5408\u89c4\u5212\u7b49\u4f18\u5316\u95ee\u9898\u4e2d\u7684\u7406\u8bba\u57fa\u7840\u3001\u5e94\u7528\u5b9e\u4f8b\u548c\u6700\u65b0\u8fdb\u5c55\u3002", "motivation": "\u5927\u89c4\u6a21\u7ea6\u675f\u4f18\u5316\u5728\u73b0\u4ee3\u79d1\u5b66\u3001\u5de5\u7a0b\u548c\u5de5\u4e1a\u8ba1\u7b97\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6d89\u53ca\u590d\u6742\u7cfb\u7edf\u548c\u4f17\u591a\u53d8\u91cf\u7ea6\u675f\uff0c\u9700\u8981\u7edf\u4e00\u7684\u589e\u5e7f\u62c9\u683c\u6717\u65e5\u51fd\u6570\u6784\u5efa\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8eHestenes-Powell-Rockafellar\u589e\u5e7f\u62c9\u683c\u6717\u65e5\u51fd\u6570\uff0c\u6784\u5efa\u7edf\u4e00\u7684\u589e\u5e7f\u62c9\u683c\u6717\u65e5\u65b9\u6cd5\u6846\u67b6\uff0c\u5305\u62ec\u5904\u7406\u975e\u51f8\u7ea6\u675f\u3001\u786e\u4fdd\u5168\u5c40\u6536\u655b\u5230\u4e00\u9636\u548c\u4e8c\u9636\u7a33\u5b9a\u70b9\u7684\u6280\u672f\u3002", "result": "ALM\u80fd\u591f\u5904\u7406\u975e\u51f8\u7ea6\u675f\u95ee\u9898\uff0c\u5728\u975e\u5149\u6ed1\u51f8\u95ee\u9898\u4e2d\u4fdd\u6301\u5c40\u90e8\u7ebf\u6027\u6536\u655b\u7387\uff0c\u5e76\u5728\u6574\u6570\u89c4\u5212\u7b49\u6311\u6218\u6027\u95ee\u9898\u4e2d\u53d6\u5f97\u8fdb\u5c55\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u5168\u9762\u9610\u8ff0\u4e86ALM\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\uff0c\u63a2\u7d22\u4e86\u4e0d\u540c\u53d8\u4f53\u4ee5\u63d0\u5347\u6536\u655b\u6027\u548c\u8ba1\u7b97\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u5404\u9886\u57df\u7684\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2510.15977", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.15977", "abs": "https://arxiv.org/abs/2510.15977", "authors": ["Wenyun Li", "Zheng Zhang", "Dongmei Jiang", "Xiangyuan Lan"], "title": "Bolster Hallucination Detection via Prompt-Guided Data Augmentation", "comment": null, "summary": "Large language models (LLMs) have garnered significant interest in AI\ncommunity. Despite their impressive generation capabilities, they have been\nfound to produce misleading or fabricated information, a phenomenon known as\nhallucinations. Consequently, hallucination detection has become critical to\nensure the reliability of LLM-generated content. One primary challenge in\nhallucination detection is the scarcity of well-labeled datasets containing\nboth truthful and hallucinated outputs. To address this issue, we introduce\nPrompt-guided data Augmented haLlucination dEtection (PALE), a novel framework\nthat leverages prompt-guided responses from LLMs as data augmentation for\nhallucination detection. This strategy can generate both truthful and\nhallucinated data under prompt guidance at a relatively low cost. To more\neffectively evaluate the truthfulness of the sparse intermediate embeddings\nproduced by LLMs, we introduce an estimation metric called the Contrastive\nMahalanobis Score (CM Score). This score is based on modeling the distributions\nof truthful and hallucinated data in the activation space. CM Score employs a\nmatrix decomposition approach to more accurately capture the underlying\nstructure of these distributions. Importantly, our framework does not require\nadditional human annotations, offering strong generalizability and practicality\nfor real-world applications. Extensive experiments demonstrate that PALE\nachieves superior hallucination detection performance, outperforming the\ncompetitive baseline by a significant margin of 6.55%.", "AI": {"tldr": "PALE\u6846\u67b6\u901a\u8fc7\u63d0\u793a\u5f15\u5bfc\u7684\u6570\u636e\u589e\u5f3a\u548c\u5bf9\u6bd4\u9a6c\u6c0f\u8ddd\u79bb\u8bc4\u5206\u6765\u68c0\u6d4b\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u5373\u53ef\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u4ea7\u751f\u8bef\u5bfc\u6216\u865a\u6784\u4fe1\u606f\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u800c\u5e7b\u89c9\u68c0\u6d4b\u9762\u4e34\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u6311\u6218\u3002", "method": "\u4f7f\u7528\u63d0\u793a\u5f15\u5bfc\u7684LLM\u54cd\u5e94\u4f5c\u4e3a\u6570\u636e\u589e\u5f3a\uff0c\u5f15\u5165\u57fa\u4e8e\u6fc0\u6d3b\u7a7a\u95f4\u5206\u5e03\u5efa\u6a21\u7684\u5bf9\u6bd4\u9a6c\u6c0f\u8ddd\u79bb\u8bc4\u5206\u6765\u8bc4\u4f30\u6587\u672c\u771f\u5b9e\u6027\u3002", "result": "PALE\u5728\u5e7b\u89c9\u68c0\u6d4b\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6bd4\u7ade\u4e89\u57fa\u7ebf\u663e\u8457\u63d0\u53476.55%\u7684\u6027\u80fd\u3002", "conclusion": "PALE\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u3001\u5177\u6709\u5f3a\u6cdb\u5316\u6027\u548c\u5b9e\u7528\u6027\u7684\u5e7b\u89c9\u68c0\u6d4b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.16387", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.16387", "abs": "https://arxiv.org/abs/2510.16387", "authors": ["Fu-An Chao", "Bi-Cheng Yan", "Berlin Chen"], "title": "Probing the Hidden Talent of ASR Foundation Models for L2 English Oral Assessment", "comment": null, "summary": "In this paper, we explore the untapped potential of Whisper, a\nwell-established automatic speech recognition (ASR) foundation model, in the\ncontext of L2 spoken language assessment (SLA). Unlike prior studies that\nextrinsically analyze transcriptions produced by Whisper, our approach goes a\nstep further to probe its latent capabilities by extracting acoustic and\nlinguistic features from hidden representations. With only a lightweight\nclassifier being trained on top of Whisper's intermediate and final outputs,\nour method achieves strong performance on the GEPT picture-description dataset,\noutperforming existing cutting-edge baselines, including a multimodal approach.\nFurthermore, by incorporating image and text-prompt information as auxiliary\nrelevance cues, we demonstrate additional performance gains. Finally, we\nconduct an in-depth analysis of Whisper's embeddings, which reveals that, even\nwithout task-specific fine-tuning, the model intrinsically encodes both ordinal\nproficiency patterns and semantic aspects of speech, highlighting its potential\nas a powerful foundation for SLA and other spoken language understanding tasks.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86Whisper\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u5728\u4e8c\u8bed\u53e3\u8bed\u8bc4\u4f30\u4e2d\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u63d0\u53d6\u5176\u9690\u85cf\u8868\u793a\u4e2d\u7684\u58f0\u5b66\u548c\u8bed\u8a00\u7279\u5f81\uff0c\u4ec5\u9700\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668\u5373\u53ef\u5728GEPT\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u4ece\u5916\u90e8\u5206\u6790Whisper\u4ea7\u751f\u7684\u8f6c\u5f55\u6587\u672c\uff0c\u672c\u7814\u7a76\u65e8\u5728\u6df1\u5165\u6316\u6398Whisper\u7684\u6f5c\u5728\u80fd\u529b\uff0c\u63a2\u7d22\u5176\u5728\u4e8c\u8bed\u53e3\u8bed\u8bc4\u4f30\u4e2d\u7684\u5e94\u7528\u4ef7\u503c\u3002", "method": "\u4eceWhisper\u7684\u4e2d\u95f4\u548c\u6700\u7ec8\u8f93\u51fa\u4e2d\u63d0\u53d6\u58f0\u5b66\u548c\u8bed\u8a00\u7279\u5f81\uff0c\u4ec5\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668\uff0c\u5e76\u878d\u5165\u56fe\u50cf\u548c\u6587\u672c\u63d0\u793a\u4fe1\u606f\u4f5c\u4e3a\u8f85\u52a9\u76f8\u5173\u7ebf\u7d22\u3002", "result": "\u5728GEPT\u56fe\u7247\u63cf\u8ff0\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u8d85\u8d8a\u4e86\u5305\u62ec\u591a\u6a21\u6001\u65b9\u6cd5\u5728\u5185\u7684\u73b0\u6709\u5148\u8fdb\u57fa\u7ebf\uff0c\u901a\u8fc7\u878d\u5165\u989d\u5916\u4fe1\u606f\u5b9e\u73b0\u4e86\u8fdb\u4e00\u6b65\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u5373\u4f7f\u6ca1\u6709\u4efb\u52a1\u7279\u5b9a\u7684\u5fae\u8c03\uff0cWhisper\u6a21\u578b\u672c\u8d28\u4e0a\u5c31\u80fd\u7f16\u7801\u53e3\u8bed\u7684\u7b49\u7ea7\u719f\u7ec3\u5ea6\u6a21\u5f0f\u548c\u8bed\u4e49\u65b9\u9762\uff0c\u663e\u793a\u51fa\u5176\u4f5c\u4e3a\u53e3\u8bed\u8bc4\u4f30\u548c\u5176\u4ed6\u53e3\u8bed\u7406\u89e3\u4efb\u52a1\u7684\u5f3a\u5927\u57fa\u7840\u6a21\u578b\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.17290", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.17290", "abs": "https://arxiv.org/abs/2510.17290", "authors": ["Qihao Peng", "Tierui Gong", "Zihang Song", "Qu Luo", "Zihuai Lin", "Pei Xiao", "Chau Yuen"], "title": "Enhanced Ground-Satellite Direct Access via Onboard Rydberg Atomic Quantum Receivers", "comment": "Submitted to IEEE Journal", "summary": "Ground-satellite links for 6G networks face critical challenges, including\nsevere path loss, tight size-weight-power limits, and congested spectrum, all\nof which significantly hinder the performance of traditional radio frequency\n(RF) front ends. This article introduces the Rydberg Atomic Quantum Receiver\n(RAQR) for onboard satellite systems, a millimeter-scale front end that\nconverts radio fields to optical signals through atomic electromagnetically\ninduced transparency. RAQR's high sensitivity and high frequency selectivity\naddress link budget, payload, and interference challenges while fitting within\nspace constraints. A hybrid atomic-electronic design and supporting signal\nmodel demonstrate enhanced data rate, coverage, and sensing accuracy relative\nto conventional RF receivers. The article concludes with integration\nstrategies, distributed-satellite concepts, and open research problems for\nbringing RAQR-enabled satellite payloads into service.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e6G\u536b\u661f\u7f51\u7edc\u7684Rydberg\u539f\u5b50\u91cf\u5b50\u63a5\u6536\u5668(RAQR)\uff0c\u901a\u8fc7\u539f\u5b50\u7535\u78c1\u8bf1\u5bfc\u900f\u660e\u5c06\u5c04\u9891\u573a\u8f6c\u6362\u4e3a\u5149\u4fe1\u53f7\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5c04\u9891\u524d\u7aef\u9762\u4e34\u7684\u8def\u5f84\u635f\u8017\u3001\u5c3a\u5bf8\u91cd\u91cf\u529f\u7387\u9650\u5236\u548c\u9891\u8c31\u62e5\u585e\u95ee\u9898\u3002", "motivation": "6G\u7f51\u7edc\u7684\u5730\u9762-\u536b\u661f\u94fe\u8def\u9762\u4e34\u4e25\u91cd\u8def\u5f84\u635f\u8017\u3001\u4e25\u683c\u7684\u5c3a\u5bf8\u91cd\u91cf\u529f\u7387\u9650\u5236\u548c\u9891\u8c31\u62e5\u585e\u7b49\u5173\u952e\u6311\u6218\uff0c\u8fd9\u4e9b\u56e0\u7d20\u663e\u8457\u963b\u788d\u4e86\u4f20\u7edf\u5c04\u9891\u524d\u7aef\u7684\u6027\u80fd\u3002", "method": "\u91c7\u7528\u6beb\u7c73\u7ea7\u524d\u7aef\u8bbe\u8ba1\uff0c\u901a\u8fc7\u539f\u5b50\u7535\u78c1\u8bf1\u5bfc\u900f\u660e\u5c06\u5c04\u9891\u573a\u8f6c\u6362\u4e3a\u5149\u4fe1\u53f7\uff0c\u7ed3\u5408\u6df7\u5408\u539f\u5b50-\u7535\u5b50\u8bbe\u8ba1\u548c\u652f\u6301\u4fe1\u53f7\u6a21\u578b\u3002", "result": "RAQR\u5177\u6709\u9ad8\u7075\u654f\u5ea6\u548c\u9ad8\u9891\u9009\u62e9\u6027\uff0c\u76f8\u5bf9\u4e8e\u4f20\u7edf\u5c04\u9891\u63a5\u6536\u5668\uff0c\u5728\u6570\u636e\u901f\u7387\u3001\u8986\u76d6\u8303\u56f4\u548c\u4f20\u611f\u7cbe\u5ea6\u65b9\u9762\u90fd\u6709\u6240\u63d0\u5347\uff0c\u540c\u65f6\u6ee1\u8db3\u7a7a\u95f4\u7ea6\u675f\u3002", "conclusion": "\u6587\u7ae0\u6700\u540e\u63d0\u51fa\u4e86\u96c6\u6210\u7b56\u7565\u3001\u5206\u5e03\u5f0f\u536b\u661f\u6982\u5ff5\u548c\u5f00\u653e\u7814\u7a76\u95ee\u9898\uff0c\u65e8\u5728\u5c06RAQR\u652f\u6301\u7684\u536b\u661f\u6709\u6548\u8f7d\u8377\u6295\u5165\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2510.17608", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.17608", "abs": "https://arxiv.org/abs/2510.17608", "authors": ["Gitte Kremling", "Francesco Iafrate", "Mahsa Taheri", "Johannes Lederer"], "title": "Non-asymptotic error bounds for probability flow ODEs under weak log-concavity", "comment": null, "summary": "Score-based generative modeling, implemented through probability flow ODEs,\nhas shown impressive results in numerous practical settings. However, most\nconvergence guarantees rely on restrictive regularity assumptions on the target\ndistribution -- such as strong log-concavity or bounded support. This work\nestablishes non-asymptotic convergence bounds in the 2-Wasserstein distance for\na general class of probability flow ODEs under considerably weaker assumptions:\nweak log-concavity and Lipschitz continuity of the score function. Our\nframework accommodates non-log-concave distributions, such as Gaussian\nmixtures, and explicitly accounts for initialization errors, score\napproximation errors, and effects of discretization via an exponential\nintegrator scheme. Bridging a key theoretical challenge in diffusion-based\ngenerative modeling, our results extend convergence theory to more realistic\ndata distributions and practical ODE solvers. We provide concrete guarantees\nfor the efficiency and correctness of the sampling algorithm, complementing the\nempirical success of diffusion models with rigorous theory. Moreover, from a\npractical perspective, our explicit rates might be helpful in choosing\nhyperparameters, such as the step size in the discretization.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4e3a\u57fa\u4e8e\u6982\u7387\u6d41ODE\u7684\u5206\u6570\u751f\u6210\u6a21\u578b\u5efa\u7acb\u4e86\u975e\u6e10\u8fd1\u6536\u655b\u754c\u9650\uff0c\u5728\u5f31\u5bf9\u6570\u51f9\u6027\u548c\u5206\u6570\u51fd\u6570Lipschitz\u8fde\u7eed\u6027\u7684\u5047\u8bbe\u4e0b\uff0c\u6269\u5c55\u4e86\u6269\u6563\u6a21\u578b\u7684\u6536\u655b\u7406\u8bba\u5230\u66f4\u73b0\u5b9e\u7684\u6570\u636e\u5206\u5e03\u548c\u5b9e\u9645ODE\u6c42\u89e3\u5668\u3002", "motivation": "\u73b0\u6709\u7684\u6536\u655b\u4fdd\u8bc1\u4f9d\u8d56\u4e8e\u5bf9\u76ee\u6807\u5206\u5e03\u7684\u4e25\u683c\u6b63\u5219\u6027\u5047\u8bbe\uff0c\u5982\u5f3a\u5bf9\u6570\u51f9\u6027\u6216\u6709\u754c\u652f\u6491\u3002\u672c\u6587\u65e8\u5728\u5728\u66f4\u5f31\u7684\u5047\u8bbe\u4e0b\u5efa\u7acb\u6536\u655b\u7406\u8bba\uff0c\u4ee5\u9002\u5e94\u975e\u5bf9\u6570\u51f9\u5206\u5e03\uff08\u5982\u9ad8\u65af\u6df7\u5408\uff09\u548c\u5b9e\u9645ODE\u6c42\u89e3\u5668\u3002", "method": "\u4f7f\u7528\u6982\u7387\u6d41ODE\u6846\u67b6\uff0c\u8003\u8651\u521d\u59cb\u5316\u8bef\u5dee\u3001\u5206\u6570\u8fd1\u4f3c\u8bef\u5dee\u548c\u901a\u8fc7\u6307\u6570\u79ef\u5206\u5668\u65b9\u6848\u7684\u79bb\u6563\u5316\u6548\u5e94\uff0c\u5728\u5f31\u5bf9\u6570\u51f9\u6027\u548c\u5206\u6570\u51fd\u6570Lipschitz\u8fde\u7eed\u6027\u7684\u5047\u8bbe\u4e0b\u5206\u6790\u6536\u655b\u6027\u3002", "result": "\u5efa\u7acb\u4e862-Wasserstein\u8ddd\u79bb\u4e2d\u7684\u975e\u6e10\u8fd1\u6536\u655b\u754c\u9650\uff0c\u4e3a\u91c7\u6837\u7b97\u6cd5\u7684\u6548\u7387\u548c\u6b63\u786e\u6027\u63d0\u4f9b\u4e86\u5177\u4f53\u4fdd\u8bc1\uff0c\u5e76\u5c06\u6536\u655b\u7406\u8bba\u6269\u5c55\u5230\u66f4\u73b0\u5b9e\u7684\u6570\u636e\u5206\u5e03\u548c\u5b9e\u9645ODE\u6c42\u89e3\u5668\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5f25\u5408\u4e86\u6269\u6563\u57fa\u751f\u6210\u5efa\u6a21\u4e2d\u7684\u5173\u952e\u7406\u8bba\u6311\u6218\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u7684\u7ecf\u9a8c\u6210\u529f\u63d0\u4f9b\u4e86\u4e25\u683c\u7406\u8bba\u652f\u6301\uff0c\u5176\u663e\u5f0f\u6536\u655b\u7387\u6709\u52a9\u4e8e\u9009\u62e9\u8d85\u53c2\u6570\uff08\u5982\u79bb\u6563\u5316\u6b65\u957f\uff09\u3002"}}
{"id": "2510.16853", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16853", "abs": "https://arxiv.org/abs/2510.16853", "authors": ["Matthew Sharp", "Omer Bilgin", "Iason Gabriel", "Lewis Hammond"], "title": "Agentic Inequality", "comment": null, "summary": "Autonomous AI agents, capable of complex planning and action, represent a\nsignificant technological evolution beyond current generative tools. As these\nsystems become integrated into political and economic life, their distribution\nand capabilities will be highly consequential. This paper introduces and\nexplores \"agentic inequality\" - the potential disparities in power,\nopportunity, and outcomes stemming from differential access to, and\ncapabilities of, AI agents. We analyse the dual potential of this technology,\nexploring how agents could both exacerbate existing divides and, under the\nright conditions, serve as a powerful equalising force. To this end, the paper\nmakes three primary contributions. First, it establishes an analytical\nframework by delineating the three core dimensions through which this\ninequality can manifest: disparities in the availability, quality, and quantity\nof agents. Second, it argues that agentic inequality is distinct from prior\ntechnological divides. Unlike tools that primarily augment human abilities,\nagents act as autonomous delegates, creating novel power asymmetries through\nscalable goal delegation and direct agent-to-agent competition that are poised\nto reshape outcomes across economic and socio-political spheres. Finally, it\nprovides a systematic analysis of the technical and socioeconomic drivers -\nfrom model release strategies to market incentives - that will shape the\ndistribution of agentic power, concluding with a research agenda for navigating\nthe complex governance challenges ahead.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86'\u4ee3\u7406\u4e0d\u5e73\u7b49'\u6982\u5ff5\uff0c\u63a2\u8ba8AI\u4ee3\u7406\u7cfb\u7edf\u5728\u53ef\u7528\u6027\u3001\u8d28\u91cf\u548c\u6570\u91cf\u4e09\u4e2a\u7ef4\u5ea6\u4e0a\u7684\u4e0d\u5e73\u7b49\u5982\u4f55\u5bfc\u81f4\u6743\u529b\u3001\u673a\u4f1a\u548c\u7ed3\u679c\u7684\u4e0d\u5e73\u7b49\u5206\u914d\u3002", "motivation": "\u968f\u7740\u81ea\u4e3bAI\u4ee3\u7406\u7cfb\u7edf\u878d\u5165\u653f\u6cbb\u7ecf\u6d4e\u751f\u6d3b\uff0c\u5176\u5206\u5e03\u548c\u80fd\u529b\u5dee\u5f02\u53ef\u80fd\u4ea7\u751f\u91cd\u5927\u5f71\u54cd\u3002\u4f5c\u8005\u65e8\u5728\u5206\u6790\u8fd9\u79cd\u6280\u672f\u53ef\u80fd\u52a0\u5267\u73b0\u6709\u5dee\u8ddd\uff0c\u4e5f\u53ef\u80fd\u5728\u9002\u5f53\u6761\u4ef6\u4e0b\u6210\u4e3a\u5e73\u7b49\u5316\u529b\u91cf\u7684\u53cc\u91cd\u6f5c\u529b\u3002", "method": "\u5efa\u7acb\u5206\u6790\u6846\u67b6\uff0c\u754c\u5b9a\u4ee3\u7406\u4e0d\u5e73\u7b49\u5728\u53ef\u7528\u6027\u3001\u8d28\u91cf\u548c\u6570\u91cf\u4e09\u4e2a\u6838\u5fc3\u7ef4\u5ea6\u7684\u8868\u73b0\uff1b\u8bba\u8bc1\u4ee3\u7406\u4e0d\u5e73\u7b49\u4e0e\u4ee5\u5f80\u6280\u672f\u9e3f\u6c9f\u7684\u533a\u522b\uff1b\u7cfb\u7edf\u5206\u6790\u6280\u672f\u548c\u7ecf\u6d4e\u793e\u4f1a\u9a71\u52a8\u56e0\u7d20\u3002", "result": "\u63d0\u51fa\u4e86\u4ee3\u7406\u4e0d\u5e73\u7b49\u4f5c\u4e3a\u65b0\u578b\u6280\u672f\u9e3f\u6c9f\u7684\u6982\u5ff5\uff0c\u5f3a\u8c03AI\u4ee3\u7406\u4f5c\u4e3a\u81ea\u4e3b\u4ee3\u8868\u521b\u9020\u4e86\u901a\u8fc7\u53ef\u6269\u5c55\u76ee\u6807\u59d4\u6258\u548c\u76f4\u63a5\u4ee3\u7406\u95f4\u7ade\u4e89\u7684\u65b0\u578b\u6743\u529b\u4e0d\u5bf9\u79f0\u3002", "conclusion": "\u4ee3\u7406\u4e0d\u5e73\u7b49\u5c06\u91cd\u5851\u7ecf\u6d4e\u548c\u793e\u4f1a\u653f\u6cbb\u9886\u57df\u7684\u7ed3\u679c\uff0c\u9700\u8981\u590d\u6742\u6cbb\u7406\u6311\u6218\u7684\u7814\u7a76\u8bae\u7a0b\u6765\u5e94\u5bf9\u6280\u672f\u53d1\u5e03\u7b56\u7565\u548c\u5e02\u573a\u6fc0\u52b1\u7b49\u56e0\u7d20\u5bf9\u4ee3\u7406\u6743\u529b\u5206\u5e03\u7684\u5f71\u54cd\u3002"}}
{"id": "2510.16276", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16276", "abs": "https://arxiv.org/abs/2510.16276", "authors": ["Song Bian", "Minghao Yan", "Anand Jayarajan", "Gennady Pekhimenko", "Shivaram Venkataraman"], "title": "What Limits Agentic Systems Efficiency?", "comment": "27 pages, 15 figures", "summary": "Large Language Models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have\ndemonstrated strong reasoning capabilities. To further enhance LLM\ncapabilities, recent agentic systems, such as Deep Research, incorporate web\ninteractions into LLM reasoning to mitigate uncertainties and reduce potential\nerrors. However, existing research predominantly focuses on reasoning\nperformance, often neglecting the efficiency of agentic systems. In this work,\nwe present a comprehensive empirical study that identifies efficiency\nbottlenecks in web-interactive agentic systems. We decompose end-to-end latency\ninto two primary components: LLM API latency and web environment latency. We\nconduct a comprehensive empirical study across 15 models and 5 providers to\ndemonstrate high variability in API-based agentic systems. We observe that web\nenvironment latency can contribute as much as 53.7% to the overall latency in a\nweb-based agentic system. To improve latency, we propose SpecCache, a caching\nframework augmented with speculative execution that can reduce web environment\noverhead. Extensive evaluations on two standard benchmarks show that our\napproach improves the cache hit rate by up to 58x compared to a random caching\nstrategy, while reducing web environment overhead by up to 3.2x, without\ndegrading agentic system performance.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u53d1\u73b0\u7f51\u7edc\u4ea4\u4e92\u5f0f\u667a\u80fd\u4ee3\u7406\u7cfb\u7edf\u5b58\u5728\u6548\u7387\u74f6\u9888\uff0c\u63d0\u51faSpecCache\u7f13\u5b58\u6846\u67b6\u7ed3\u5408\u63a8\u6d4b\u6267\u884c\u6765\u964d\u4f4e\u7f51\u7edc\u73af\u5883\u5ef6\u8fdf\uff0c\u663e\u8457\u63d0\u5347\u7f13\u5b58\u547d\u4e2d\u7387\u548c\u7cfb\u7edf\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u667a\u80fd\u4ee3\u7406\u7cfb\u7edf\u7684\u63a8\u7406\u6027\u80fd\uff0c\u800c\u5ffd\u89c6\u4e86\u7cfb\u7edf\u6548\u7387\u95ee\u9898\u3002\u7f51\u7edc\u4ea4\u4e92\u5f0f\u4ee3\u7406\u7cfb\u7edf\u5b58\u5728\u663e\u8457\u7684\u5ef6\u8fdf\u74f6\u9888\uff0c\u5f71\u54cd\u5b9e\u9645\u5e94\u7528\u6548\u679c\u3002", "method": "\u5c06\u7aef\u5230\u7aef\u5ef6\u8fdf\u5206\u89e3\u4e3aLLM API\u5ef6\u8fdf\u548c\u7f51\u7edc\u73af\u5883\u5ef6\u8fdf\uff0c\u901a\u8fc715\u4e2a\u6a21\u578b\u548c5\u4e2a\u63d0\u4f9b\u5546\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u63d0\u51faSpecCache\u7f13\u5b58\u6846\u67b6\u7ed3\u5408\u63a8\u6d4b\u6267\u884c\u6765\u4f18\u5316\u7f51\u7edc\u73af\u5883\u5f00\u9500\u3002", "result": "\u7f51\u7edc\u73af\u5883\u5ef6\u8fdf\u53ef\u5360\u6574\u4f53\u5ef6\u8fdf\u768453.7%\uff0cSpecCache\u76f8\u6bd4\u968f\u673a\u7f13\u5b58\u7b56\u7565\u5c06\u7f13\u5b58\u547d\u4e2d\u7387\u63d0\u534758\u500d\uff0c\u7f51\u7edc\u73af\u5883\u5f00\u9500\u964d\u4f4e3.2\u500d\uff0c\u4e14\u4e0d\u635f\u5bb3\u7cfb\u7edf\u6027\u80fd\u3002", "conclusion": "\u667a\u80fd\u4ee3\u7406\u7cfb\u7edf\u7684\u6548\u7387\u74f6\u9888\u4e3b\u8981\u6765\u81ea\u7f51\u7edc\u73af\u5883\u5ef6\u8fdf\uff0cSpecCache\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u6548\u7387\u3002"}}
{"id": "2510.16864", "categories": ["math.OC", "90C26, 90C30, 65K10"], "pdf": "https://arxiv.org/pdf/2510.16864", "abs": "https://arxiv.org/abs/2510.16864", "authors": ["Szil\u00e1rd Csaba L\u00e1szl\u00f3"], "title": "Solving nonconvex optimization problems via a second order dynamical system with unbounded damping", "comment": "19 pages", "summary": "In this paper we study a second order dynamical system with variable\ncoefficients in connection to the minimization problem of a smooth nonconvex\nfunction. The convergence of the trajectories generated by the dynamical system\nto a critical point of the objective function is assured, provided a\nregularization of the objective function satisfies the Kurdyka-{\\L}ojasiewicz\nproperty. We also provide convergence rates for the trajectories generated by\nthe dynamical system, formulated in terms of the {\\L}ojasiewicz exponent, and\nwe show that the unbounded damping considered in our dynamical system\nsignificantly improves the convergence rates known so far in the literature,\nthat is, instead of linear rates we obtain superlinear rates.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5e26\u53d8\u7cfb\u6570\u7684\u4e8c\u9636\u52a8\u529b\u7cfb\u7edf\u5728\u975e\u51f8\u51fd\u6570\u6700\u5c0f\u5316\u95ee\u9898\u4e2d\u7684\u5e94\u7528\uff0c\u8bc1\u660e\u4e86\u8f68\u8ff9\u6536\u655b\u5230\u4e34\u754c\u70b9\uff0c\u5e76\u83b7\u5f97\u4e86\u8d85\u7ebf\u6027\u6536\u655b\u901f\u7387\u3002", "motivation": "\u9488\u5bf9\u975e\u51f8\u51fd\u6570\u6700\u5c0f\u5316\u95ee\u9898\uff0c\u7814\u7a76\u4e8c\u9636\u52a8\u529b\u7cfb\u7edf\u7684\u6536\u655b\u6027\uff0c\u65e8\u5728\u6539\u8fdb\u73b0\u6709\u6587\u732e\u4e2d\u7684\u6536\u655b\u901f\u7387\u3002", "method": "\u4f7f\u7528\u5e26\u53d8\u7cfb\u6570\u7684\u4e8c\u9636\u52a8\u529b\u7cfb\u7edf\uff0c\u8981\u6c42\u76ee\u6807\u51fd\u6570\u7684\u6b63\u5219\u5316\u6ee1\u8db3Kurdyka-\u0141ojasiewicz\u6027\u8d28\u3002", "result": "\u8bc1\u660e\u4e86\u8f68\u8ff9\u6536\u655b\u5230\u4e34\u754c\u70b9\uff0c\u5e76\u83b7\u5f97\u4e86\u8d85\u7ebf\u6027\u6536\u655b\u901f\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u6587\u732e\u4e2d\u7684\u7ebf\u6027\u901f\u7387\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65e0\u754c\u963b\u5c3c\u4e8c\u9636\u52a8\u529b\u7cfb\u7edf\u5728\u975e\u51f8\u4f18\u5316\u4e2d\u5b9e\u73b0\u4e86\u8d85\u7ebf\u6027\u6536\u655b\uff0c\u663e\u8457\u6539\u8fdb\u4e86\u73b0\u6709\u7ed3\u679c\u3002"}}
{"id": "2510.15978", "categories": ["cs.LG", "cs.AI", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2510.15978", "abs": "https://arxiv.org/abs/2510.15978", "authors": ["Junchao Gong", "Jingyi Xu", "Ben Fei", "Fenghua Ling", "Wenlong Zhang", "Kun Chen", "Wanghan Xu", "Weidong Yang", "Xiaokang Yang", "Lei Bai"], "title": "DAWP: A framework for global observation forecasting via Data Assimilation and Weather Prediction in satellite observation space", "comment": null, "summary": "Weather prediction is a critical task for human society, where impressive\nprogress has been made by training artificial intelligence weather prediction\n(AIWP) methods with reanalysis data. However, reliance on reanalysis data\nlimits the AIWPs with shortcomings, including data assimilation biases and\ntemporal discrepancies. To liberate AIWPs from the reanalysis data, observation\nforecasting emerges as a transformative paradigm for weather prediction. One of\nthe key challenges in observation forecasting is learning spatiotemporal\ndynamics across disparate measurement systems with irregular high-resolution\nobservation data, which constrains the design and prediction of AIWPs. To this\nend, we propose our DAWP as an innovative framework to enable AIWPs to operate\nin a complete observation space by initialization with an artificial\nintelligence data assimilation (AIDA) module. Specifically, our AIDA module\napplies a mask multi-modality autoencoder(MMAE)for assimilating irregular\nsatellite observation tokens encoded by mask ViT-VAEs. For AIWP, we introduce a\nspatiotemporal decoupling transformer with cross-regional boundary conditioning\n(CBC), learning the dynamics in observation space, to enable sub-image-based\nglobal observation forecasting. Comprehensive experiments demonstrate that AIDA\ninitialization significantly improves the roll out and efficiency of AIWP.\nAdditionally, we show that DAWP holds promising potential to be applied in\nglobal precipitation forecasting.", "AI": {"tldr": "\u63d0\u51faDAWP\u6846\u67b6\uff0c\u901a\u8fc7\u4eba\u5de5\u667a\u80fd\u6570\u636e\u540c\u5316(AIDA)\u6a21\u5757\u5c06AI\u5929\u6c14\u9884\u6d4b\u4ece\u518d\u5206\u6790\u6570\u636e\u89e3\u653e\u5230\u89c2\u6d4b\u7a7a\u95f4\uff0c\u5b9e\u73b0\u57fa\u4e8e\u4e0d\u89c4\u5219\u89c2\u6d4b\u6570\u636e\u7684\u5168\u7403\u5929\u6c14\u9884\u62a5", "motivation": "\u4f20\u7edfAI\u5929\u6c14\u9884\u6d4b\u4f9d\u8d56\u518d\u5206\u6790\u6570\u636e\u5b58\u5728\u6570\u636e\u540c\u5316\u504f\u5dee\u548c\u65f6\u95f4\u5dee\u5f02\u95ee\u9898\uff0c\u89c2\u6d4b\u9884\u62a5\u662f\u89e3\u653eAIWP\u7684\u65b0\u8303\u5f0f\uff0c\u4f46\u9700\u8981\u89e3\u51b3\u4e0d\u89c4\u5219\u9ad8\u5206\u8fa8\u7387\u89c2\u6d4b\u6570\u636e\u7684\u65f6\u7a7a\u52a8\u6001\u5b66\u4e60\u6311\u6218", "method": "\u4f7f\u7528\u63a9\u7801\u591a\u6a21\u6001\u81ea\u7f16\u7801\u5668(MMAE)\u8fdb\u884c\u6570\u636e\u540c\u5316\uff0c\u901a\u8fc7\u63a9\u7801ViT-VAE\u7f16\u7801\u4e0d\u89c4\u5219\u536b\u661f\u89c2\u6d4b\u4ee4\u724c\uff0c\u91c7\u7528\u65f6\u7a7a\u89e3\u8026transformer\u548c\u8de8\u533a\u57df\u8fb9\u754c\u6761\u4ef6\u5b9e\u73b0\u57fa\u4e8e\u5b50\u56fe\u50cf\u7684\u5168\u7403\u89c2\u6d4b\u9884\u62a5", "result": "AIDA\u521d\u59cb\u5316\u663e\u8457\u63d0\u9ad8\u4e86AIWP\u7684\u63a8\u51fa\u6548\u7387\u548c\u6027\u80fd\uff0cDAWP\u5728\u5168\u5c40\u964d\u6c34\u9884\u62a5\u4e2d\u5c55\u73b0\u51fa\u5e94\u7528\u6f5c\u529b", "conclusion": "DAWP\u6846\u67b6\u6210\u529f\u5c06AI\u5929\u6c14\u9884\u6d4b\u4ece\u518d\u5206\u6790\u6570\u636e\u4f9d\u8d56\u4e2d\u89e3\u653e\u51fa\u6765\uff0c\u901a\u8fc7\u89c2\u6d4b\u7a7a\u95f4\u64cd\u4f5c\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u7684\u5929\u6c14\u9884\u62a5"}}
{"id": "2510.16439", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16439", "abs": "https://arxiv.org/abs/2510.16439", "authors": ["Syed Rifat Raiyan", "Md Farhan Ishmam", "Abdullah Al Imran", "Mohammad Ali Moni"], "title": "FrugalPrompt: Reducing Contextual Overhead in Large Language Models via Token Attribution", "comment": null, "summary": "Large language models (LLMs) owe much of their stellar performance to\nexpansive input contexts, yet such verbosity inflates monetary costs, carbon\nfootprint, and inference-time latency. Much of this overhead manifests from the\nredundant low-utility tokens present in typical prompts, as only a fraction of\ntokens typically carries the majority of the semantic weight. We address this\ninefficiency by introducing FrugalPrompt, a novel prompt compression framework\nfor LLMs, which retains only the most semantically significant tokens.\nLeveraging two state-of-the-art token attribution methods, GlobEnc and DecompX,\nwe assign salience scores to every token in an input sequence, rank them to\npreserve the top-k% tokens in their original order, and obtain a sparse\nfrugalized prompt. We evaluate the approach across four NLP tasks: Sentiment\nAnalysis, Commonsense QA, Summarization, and Mathematical Reasoning, using a\nsuite of frontier LLMs. For the first three tasks, a 20% prompt reduction\nincurs only a marginal loss in task performance, demonstrating that\ncontemporary LLMs can reconstruct elided context from high-salience cues. In\ncontrast, performance on mathematical reasoning deteriorates sharply,\nreflecting a stronger dependence on complete token continuity. Further analysis\nwith bottom-k% and random-k% tokens reveals asymmetric performance patterns\nthat may suggest potential task contamination effects, wherein models may\nresort to shallow memorized patterns from pretraining exposure for conventional\nNLP tasks. We posit that our work contributes to a more nuanced understanding\nof LLM behavior in performance-efficiency trade-offs, and delineate the\nboundary between tasks tolerant to contextual sparsity and those requiring\nexhaustive context. Our source code and models are available at:\nhttps://github.com/Starscream-11813/Frugal-ICL", "AI": {"tldr": "FrugalPrompt\u662f\u4e00\u4e2a\u65b0\u9896\u7684LLM\u63d0\u793a\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u4fdd\u7559\u6700\u5177\u8bed\u4e49\u91cd\u8981\u6027\u7684token\u6765\u51cf\u5c11\u8f93\u5165\u957f\u5ea6\uff0c\u5728\u591a\u4e2aNLP\u4efb\u52a1\u4e2d\u5b9e\u73b020%\u538b\u7f29\u7387\u65f6\u4ec5\u5e26\u6765\u5fae\u5c0f\u6027\u80fd\u635f\u5931\u3002", "motivation": "LLM\u7684\u5197\u957f\u8f93\u5165\u589e\u52a0\u4e86\u6210\u672c\u3001\u78b3\u8db3\u8ff9\u548c\u63a8\u7406\u5ef6\u8fdf\uff0c\u800c\u5927\u591a\u6570token\u662f\u5197\u4f59\u7684\uff0c\u53ea\u6709\u5c11\u6570token\u627f\u8f7d\u4e3b\u8981\u8bed\u4e49\u6743\u91cd\u3002", "method": "\u4f7f\u7528GlobEnc\u548cDecompX\u4e24\u79cdtoken\u5f52\u56e0\u65b9\u6cd5\u4e3a\u8f93\u5165\u5e8f\u5217\u4e2d\u7684\u6bcf\u4e2atoken\u5206\u914d\u663e\u8457\u5ea6\u5206\u6570\uff0c\u6309\u539f\u59cb\u987a\u5e8f\u4fdd\u7559\u524dk%\u7684token\uff0c\u83b7\u5f97\u7a00\u758f\u7684\u538b\u7f29\u63d0\u793a\u3002", "result": "\u5728\u524d\u4e09\u4e2a\u4efb\u52a1\uff08\u60c5\u611f\u5206\u6790\u3001\u5e38\u8bc6\u95ee\u7b54\u3001\u6458\u8981\uff09\u4e2d\uff0c20%\u7684\u63d0\u793a\u538b\u7f29\u4ec5\u5e26\u6765\u8fb9\u9645\u6027\u80fd\u635f\u5931\uff1b\u800c\u6570\u5b66\u63a8\u7406\u6027\u80fd\u6025\u5267\u4e0b\u964d\uff0c\u8868\u660e\u5bf9\u5b8c\u6574token\u8fde\u7eed\u6027\u7684\u66f4\u5f3a\u4f9d\u8d56\u3002", "conclusion": "\u8be5\u7814\u7a76\u6709\u52a9\u4e8e\u66f4\u7ec6\u81f4\u5730\u7406\u89e3LLM\u5728\u6027\u80fd-\u6548\u7387\u6743\u8861\u4e2d\u7684\u884c\u4e3a\uff0c\u5e76\u5212\u5b9a\u4e86\u5bb9\u5fcd\u4e0a\u4e0b\u6587\u7a00\u758f\u6027\u7684\u4efb\u52a1\u4e0e\u9700\u8981\u8be6\u5c3d\u4e0a\u4e0b\u6587\u7684\u4efb\u52a1\u4e4b\u95f4\u7684\u8fb9\u754c\u3002"}}
{"id": "2510.17333", "categories": ["eess.SY", "cs.CR", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.17333", "abs": "https://arxiv.org/abs/2510.17333", "authors": ["Sebastian Schlor", "Frank Allg\u00f6wer"], "title": "Comparison and performance analysis of dynamic encrypted control approaches", "comment": null, "summary": "Encrypted controllers using homomorphic encryption have proven to guarantee\nthe privacy of measurement and control signals, as well as system and\ncontroller parameters, while regulating the system as intended. However,\nencrypting dynamic controllers has remained a challenge due to growing noise\nand overflow issues in the encoding. In this paper, we review recent approaches\nto dynamic encrypted control, such as bootstrapping, periodic resets of the\ncontroller state, integer reformulations, and FIR controllers, and equip them\nwith a stability and performance analysis to evaluate their suitability. We\ncomplement the analysis with a numerical performance comparison on a benchmark\nsystem.", "AI": {"tldr": "\u672c\u6587\u56de\u987e\u4e86\u52a8\u6001\u52a0\u5bc6\u63a7\u5236\u7684\u6700\u65b0\u65b9\u6cd5\uff0c\u5305\u62ec\u81ea\u4e3e\u3001\u63a7\u5236\u5668\u72b6\u6001\u5468\u671f\u6027\u91cd\u7f6e\u3001\u6574\u6570\u91cd\u6784\u548cFIR\u63a7\u5236\u5668\uff0c\u5e76\u8fdb\u884c\u4e86\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u5206\u6790\uff0c\u901a\u8fc7\u57fa\u51c6\u7cfb\u7edf\u8fdb\u884c\u4e86\u6570\u503c\u6027\u80fd\u6bd4\u8f83\u3002", "motivation": "\u4f7f\u7528\u540c\u6001\u52a0\u5bc6\u7684\u52a0\u5bc6\u63a7\u5236\u5668\u5df2\u88ab\u8bc1\u660e\u53ef\u4ee5\u5728\u8c03\u8282\u7cfb\u7edf\u7684\u540c\u65f6\u4fdd\u8bc1\u6d4b\u91cf\u548c\u63a7\u5236\u4fe1\u53f7\u4ee5\u53ca\u7cfb\u7edf\u548c\u63a7\u5236\u5668\u53c2\u6570\u7684\u9690\u79c1\u3002\u7136\u800c\uff0c\u7531\u4e8e\u7f16\u7801\u4e2d\u4e0d\u65ad\u589e\u957f\u7684\u566a\u58f0\u548c\u6ea2\u51fa\u95ee\u9898\uff0c\u52a0\u5bc6\u52a8\u6001\u63a7\u5236\u5668\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u56de\u987e\u4e86\u52a8\u6001\u52a0\u5bc6\u63a7\u5236\u7684\u51e0\u79cd\u65b9\u6cd5\uff1a\u81ea\u4e3e\u3001\u63a7\u5236\u5668\u72b6\u6001\u5468\u671f\u6027\u91cd\u7f6e\u3001\u6574\u6570\u91cd\u6784\u548cFIR\u63a7\u5236\u5668\uff0c\u5e76\u4e3a\u8fd9\u4e9b\u65b9\u6cd5\u914d\u5907\u4e86\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u5206\u6790\u6846\u67b6\u3002", "result": "\u901a\u8fc7\u57fa\u51c6\u7cfb\u7edf\u7684\u6570\u503c\u6027\u80fd\u6bd4\u8f83\uff0c\u8bc4\u4f30\u4e86\u8fd9\u4e9b\u52a8\u6001\u52a0\u5bc6\u63a7\u5236\u65b9\u6cd5\u7684\u9002\u7528\u6027\u3002", "conclusion": "\u672c\u6587\u4e3a\u52a8\u6001\u52a0\u5bc6\u63a7\u5236\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u7684\u5206\u6790\u548c\u6bd4\u8f83\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u8bc4\u4f30\u4e0d\u540c\u65b9\u6cd5\u5728\u4fdd\u8bc1\u9690\u79c1\u7684\u540c\u65f6\u7ef4\u6301\u7cfb\u7edf\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u7684\u80fd\u529b\u3002"}}
{"id": "2510.16858", "categories": ["cs.CY", "K.3.2; K.4.3; K.4.2"], "pdf": "https://arxiv.org/pdf/2510.16858", "abs": "https://arxiv.org/abs/2510.16858", "authors": ["Enes Ayalp"], "title": "Sustainable and Adaptive Growth in Creative Tech", "comment": null, "summary": "The creative technology evolves rapidly in both scope and depth, demanding\ncross-disciplinary expertise and continuous improvement. Although educational\nprograms and other collaborative initiatives enable strong technical and\nartistic skills, even the most advanced pathways rarely ensure a stable career.\nSuccess in these professions often depends on visibility, timing, and\nself-directed development. As markets shift or technologies change, talents\nstill find themselves displaced. Existing learning paths often fail to connect\nthe skills they teach, leaving learners with fragmented expertise that decays\nquickly when not continuously applied. The industry demands depth, yet\nspecialization carries risk when tools, pipelines, or roles evolve faster than\nthe expertise built around them. Broad skill sets, by contrast, may increase\nemployability but are easily replaced or rendered obsolete by technological\nchange. CLEAR CORE is a framework for learning and sustaining in creative\ntechnology. It integrates two iterative interconnected cycles into a continuous\nprocess linking structured education with independent growth as a lifelong,\nrenewable practice that allows professionals to excel amid constant change.", "AI": {"tldr": "CLEAR CORE\u662f\u4e00\u4e2a\u5e94\u5bf9\u521b\u610f\u6280\u672f\u9886\u57df\u5feb\u901f\u53d8\u5316\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u7ed3\u6784\u5316\u6559\u80b2\u548c\u72ec\u7acb\u6210\u957f\u4e24\u4e2a\u8fed\u4ee3\u5faa\u73af\uff0c\u5e2e\u52a9\u4e13\u4e1a\u4eba\u58eb\u5728\u6301\u7eed\u53d8\u5316\u4e2d\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "motivation": "\u521b\u610f\u6280\u672f\u9886\u57df\u53d1\u5c55\u8fc5\u901f\uff0c\u73b0\u6709\u6559\u80b2\u8def\u5f84\u5f80\u5f80\u65e0\u6cd5\u8fde\u63a5\u6280\u80fd\uff0c\u5bfc\u81f4\u5b66\u4e60\u8005\u4e13\u4e1a\u77e5\u8bc6\u788e\u7247\u5316\u4e14\u5bb9\u6613\u8fc7\u65f6\u3002\u884c\u4e1a\u9700\u8981\u6df1\u5ea6\u4f46\u4e13\u4e1a\u5316\u53c8\u9762\u4e34\u6280\u672f\u5feb\u901f\u6f14\u53d8\u7684\u98ce\u9669\uff0c\u800c\u5e7f\u6cdb\u6280\u80fd\u96c6\u5bb9\u6613\u88ab\u6280\u672f\u53d8\u9769\u53d6\u4ee3\u3002", "method": "CLEAR CORE\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u8fed\u4ee3\u4e92\u8fde\u7684\u5faa\u73af\uff1a\u7ed3\u6784\u5316\u6559\u80b2\u548c\u72ec\u7acb\u6210\u957f\uff0c\u5f62\u6210\u4e00\u4e2a\u6301\u7eed\u7684\u8fc7\u7a0b\uff0c\u5c06\u6b63\u89c4\u5b66\u4e60\u4e0e\u7ec8\u8eab\u81ea\u6211\u53d1\u5c55\u76f8\u7ed3\u5408\u3002", "result": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6301\u7eed\u7684\u5b66\u4e60\u548c\u53d1\u5c55\u6a21\u5f0f\uff0c\u4f7f\u521b\u610f\u6280\u672f\u4e13\u4e1a\u4eba\u58eb\u80fd\u591f\u5728\u6280\u672f\u5feb\u901f\u53d8\u5316\u7684\u73af\u5883\u4e2d\u6301\u7eed\u6210\u957f\u5e76\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "CLEAR CORE\u901a\u8fc7\u6574\u5408\u7ed3\u6784\u5316\u6559\u80b2\u548c\u81ea\u4e3b\u53d1\u5c55\u7684\u53cc\u5faa\u73af\u6a21\u5f0f\uff0c\u4e3a\u521b\u610f\u6280\u672f\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5e94\u5bf9\u884c\u4e1a\u5feb\u901f\u53d8\u5316\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u7ec8\u8eab\u5b66\u4e60\u548c\u4e13\u4e1a\u53ef\u6301\u7eed\u53d1\u5c55\u3002"}}
{"id": "2510.16302", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.16302", "abs": "https://arxiv.org/abs/2510.16302", "authors": ["Changhao Wang", "Yanfang Liu", "Xinxin Fan", "Anzhi Zhou", "Lao Tian", "Yunfeng Lu"], "title": "DTKG: Dual-Track Knowledge Graph-Verified Reasoning Framework for Multi-Hop QA", "comment": "13 pages, 5 figures", "summary": "Multi-hop reasoning for question answering (QA) plays a critical role in\nretrieval-augmented generation (RAG) for modern large language models (LLMs).\nThe accurate answer can be obtained through retrieving relational structure of\nentities from knowledge graph (KG). Regarding the inherent relation-dependency\nand reasoning pattern, multi-hop reasoning can be in general classified into\ntwo categories: i) parallel fact-verification multi-hop reasoning question,\ni.e., requiring simultaneous verifications of multiple independent\nsub-questions; and ii) chained multi-hop reasoning questions, i.e., demanding\nsequential multi-step inference with intermediate conclusions serving as\nessential premises for subsequent reasoning. Currently, the multi-hop reasoning\napproaches singly employ one of two techniques: LLM response-based fact\nverification and KG path-based chain construction. Nevertheless, the former\nexcels at parallel fact-verification but underperforms on chained reasoning\ntasks, while the latter demonstrates proficiency in chained multi-hop reasoning\nbut suffers from redundant path retrieval when handling parallel\nfact-verification reasoning. These limitations deteriorate the efficiency and\naccuracy for multi-hop QA tasks. To address this challenge, we propose a novel\ndual-track KG verification and reasoning framework DTKG, which is inspired by\nthe Dual Process Theory in cognitive science. Specifically, DTKG comprises two\nmain stages: the Classification Stage and the Branch Processing Stage.", "AI": {"tldr": "\u63d0\u51fa\u4e86DTKG\u53cc\u8f68\u77e5\u8bc6\u56fe\u8c31\u9a8c\u8bc1\u4e0e\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u7c7b\u9636\u6bb5\u548c\u5206\u652f\u5904\u7406\u9636\u6bb5\u6765\u89e3\u51b3\u591a\u8df3\u63a8\u7406\u4e2d\u7684\u5e76\u884c\u4e8b\u5b9e\u9a8c\u8bc1\u548c\u94fe\u5f0f\u63a8\u7406\u95ee\u9898", "motivation": "\u73b0\u6709\u7684\u591a\u8df3\u63a8\u7406\u65b9\u6cd5\u8981\u4e48\u4f7f\u7528LLM\u54cd\u5e94\u7684\u4e8b\u5b9e\u9a8c\u8bc1\uff0c\u8981\u4e48\u4f7f\u7528KG\u8def\u5f84\u7684\u94fe\u5f0f\u6784\u5efa\uff0c\u4f46\u524d\u8005\u64c5\u957f\u5e76\u884c\u4e8b\u5b9e\u9a8c\u8bc1\u800c\u4e0d\u64c5\u957f\u94fe\u5f0f\u63a8\u7406\uff0c\u540e\u8005\u64c5\u957f\u94fe\u5f0f\u63a8\u7406\u4f46\u5728\u5e76\u884c\u4e8b\u5b9e\u9a8c\u8bc1\u65f6\u5b58\u5728\u5197\u4f59\u8def\u5f84\u68c0\u7d22\u95ee\u9898", "method": "\u57fa\u4e8e\u8ba4\u77e5\u79d1\u5b66\u4e2d\u7684\u53cc\u8fc7\u7a0b\u7406\u8bba\uff0c\u63d0\u51faDTKG\u6846\u67b6\uff0c\u5305\u542b\u5206\u7c7b\u9636\u6bb5\u548c\u5206\u652f\u5904\u7406\u9636\u6bb5\uff0c\u5206\u522b\u5904\u7406\u5e76\u884c\u4e8b\u5b9e\u9a8c\u8bc1\u548c\u94fe\u5f0f\u591a\u8df3\u63a8\u7406", "result": "DTKG\u6846\u67b6\u65e8\u5728\u63d0\u9ad8\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u7684\u6548\u7387\u548c\u51c6\u786e\u6027", "conclusion": "\u901a\u8fc7\u53cc\u8f68\u65b9\u6cd5\u7ed3\u5408LLM\u548cKG\u7684\u4f18\u52bf\uff0c\u53ef\u4ee5\u66f4\u6709\u6548\u5730\u5904\u7406\u4e0d\u540c\u7c7b\u578b\u591a\u8df3\u63a8\u7406\u95ee\u9898"}}
{"id": "2510.17024", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2510.17024", "abs": "https://arxiv.org/abs/2510.17024", "authors": ["Zeinab Alizadeh", "Azadeh Farsi", "Afrooz Jalilzadeh"], "title": "Distributionally Robust Nash Equilibria via Variational Inequalities", "comment": null, "summary": "Nash Equilibrium and its robust counterpart, Distributionally Robust Nash\nEquilibrium (DRNE), are fundamental problems in game theory with applications\nin economics, engineering, and machine learning. This paper addresses the\nproblem of DRNE, where multiple players engage in a noncooperative game under\nuncertainty. Each player aims to minimize their objective against the\nworst-case distribution within an ambiguity set, resulting in a minimax\nstructure. We reformulate the DRNE problem as a Variational Inequality (VI)\nproblem, providing a unified framework for analysis and algorithm development.\nWe propose a gradient descent-ascent type algorithm with convergence guarantee\nthat effectively addresses the computational challenges of high-dimensional and\nnonsmooth objectives.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u5206\u5e03\u9c81\u68d2\u7eb3\u4ec0\u5747\u8861\u95ee\u9898\uff0c\u63d0\u51fa\u53d8\u5206\u4e0d\u7b49\u5f0f\u91cd\u6784\u6846\u67b6\u548c\u6536\u655b\u6027\u4fdd\u8bc1\u7684\u68af\u5ea6\u4e0b\u964d-\u4e0a\u5347\u7b97\u6cd5", "motivation": "\u7eb3\u4ec0\u5747\u8861\u53ca\u5176\u9c81\u68d2\u7248\u672c\u5728\u535a\u5f08\u8bba\u4e2d\u5177\u6709\u57fa\u7840\u91cd\u8981\u6027\uff0c\u5728\u7ecf\u6d4e\u5b66\u3001\u5de5\u7a0b\u548c\u673a\u5668\u5b66\u4e60\u4e2d\u5e94\u7528\u5e7f\u6cdb\u3002\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u9ad8\u7ef4\u548c\u975e\u5149\u6ed1\u76ee\u6807\u51fd\u6570\u4e0b\u7684\u5206\u5e03\u9c81\u68d2\u7eb3\u4ec0\u5747\u8861\u8ba1\u7b97\u95ee\u9898\u3002", "method": "\u5c06\u5206\u5e03\u9c81\u68d2\u7eb3\u4ec0\u5747\u8861\u95ee\u9898\u91cd\u6784\u4e3a\u53d8\u5206\u4e0d\u7b49\u5f0f\u95ee\u9898\uff0c\u63d0\u51fa\u68af\u5ea6\u4e0b\u964d-\u4e0a\u5347\u7c7b\u578b\u7b97\u6cd5\uff0c\u5177\u6709\u6536\u655b\u6027\u4fdd\u8bc1\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u9ad8\u7ef4\u548c\u975e\u5149\u6ed1\u76ee\u6807\u51fd\u6570\u3002", "result": "\u5efa\u7acb\u4e86\u7edf\u4e00\u7684\u53d8\u5206\u4e0d\u7b49\u5f0f\u5206\u6790\u6846\u67b6\uff0c\u5f00\u53d1\u4e86\u80fd\u591f\u89e3\u51b3\u9ad8\u7ef4\u975e\u5149\u6ed1\u76ee\u6807\u8ba1\u7b97\u6311\u6218\u7684\u7b97\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u6536\u655b\u6027\u7406\u8bba\u4fdd\u8bc1\u3002", "conclusion": "\u63d0\u51fa\u7684\u53d8\u5206\u4e0d\u7b49\u5f0f\u91cd\u6784\u548c\u68af\u5ea6\u4e0b\u964d-\u4e0a\u5347\u7b97\u6cd5\u4e3a\u5206\u5e03\u9c81\u68d2\u7eb3\u4ec0\u5747\u8861\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8ba1\u7b97\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9ad8\u7ef4\u548c\u975e\u5149\u6ed1\u76ee\u6807\u573a\u666f\u3002"}}
{"id": "2510.15979", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15979", "abs": "https://arxiv.org/abs/2510.15979", "authors": ["Zexu Sun", "Yongcheng Zeng", "Erxue Min", "Heyang Gao", "Bokai Ji", "Xu Chen"], "title": "Cog-Rethinker: Hierarchical Metacognitive Reinforcement Learning for LLM Reasoning", "comment": "22 Pages, 8 figures, 4 tables", "summary": "Contemporary progress in large language models (LLMs) has revealed notable\ninferential capacities via reinforcement learning (RL) employing verifiable\nreward, facilitating the development of O1 and R1-like reasoning models.\nDirectly training from base models with RL is called zero-RL. However, previous\nworks rely upon activating LLMs' inherent capacities through fixed prompt\ntemplates. This strategy introduces substantial sampling inefficiencies for\nweak LLMs, as the majority of problems generate invalid outputs during\naccuracy-driven filtration in reasoning tasks, which causes a waste of samples.\nTo solve this issue, we propose Cog-Rethinker, a novel hierarchical\nmetacognitive RL framework for LLM reasoning. Our Cog-Rethinker mainly focuses\non the rollout procedure in RL training. After the direct rollout, our\nCog-Rethinker improves sample utilization in a hierarchical metacognitive\ntwo-stage framework. By leveraging human cognition during solving problems,\nfirstly, it prompts policy to decompose zero-accuracy problems into subproblems\nto produce final reasoning results. Secondly, with zero-accuracy problems in\nprevious rollout stage, it further prompts policy to refine these answers by\nreferencing previous wrong solutions. Moreover, to enable cold-start of the two\nnew reasoning patterns and maintain train-test consistency across prompt\ntemplates, our Cog-Rethinker applies supervised fine-tuning on the policy using\ncorrect samples of the two stages with direct rollout template. Experimental\nresults demonstrate Cog-Rethinker's superior performance on various\nmathematical reasoning benchmarks, we also analyzed its improved sample\nefficiency that accelerates convergence compared to baseline methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86Cog-Rethinker\uff0c\u4e00\u79cd\u7528\u4e8eLLM\u63a8\u7406\u7684\u5206\u5c42\u5143\u8ba4\u77e5\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u65b9\u6cd5\u63d0\u9ad8\u6837\u672c\u5229\u7528\u7387\uff0c\u89e3\u51b3\u5f31LLM\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u56e0\u65e0\u6548\u8f93\u51fa\u5bfc\u81f4\u7684\u91c7\u6837\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u63d0\u793a\u6a21\u677f\u6fc0\u6d3bLLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u8fd9\u5bf9\u5f31LLM\u9020\u6210\u4e25\u91cd\u7684\u91c7\u6837\u6548\u7387\u95ee\u9898\uff0c\u56e0\u4e3a\u5927\u90e8\u5206\u95ee\u9898\u5728\u51c6\u786e\u6027\u9a71\u52a8\u7684\u8fc7\u6ee4\u4e2d\u4f1a\u4ea7\u751f\u65e0\u6548\u8f93\u51fa\uff0c\u6d6a\u8d39\u6837\u672c\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u5143\u8ba4\u77e5RL\u6846\u67b6\uff1a1\uff09\u5c06\u96f6\u51c6\u786e\u7387\u95ee\u9898\u5206\u89e3\u4e3a\u5b50\u95ee\u9898\uff1b2\uff09\u53c2\u8003\u5148\u524d\u9519\u8bef\u89e3\u51b3\u65b9\u6848\u7cbe\u70bc\u7b54\u6848\u3002\u5e94\u7528\u76d1\u7763\u5fae\u8c03\u786e\u4fdd\u8bad\u7ec3-\u6d4b\u8bd5\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aCog-Rethinker\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u5e76\u52a0\u901f\u4e86\u6536\u655b\u3002", "conclusion": "Cog-Rethinker\u901a\u8fc7\u5206\u5c42\u5143\u8ba4\u77e5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86LLM\u63a8\u7406\u8bad\u7ec3\u4e2d\u7684\u91c7\u6837\u6548\u7387\u95ee\u9898\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002"}}
{"id": "2510.16449", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16449", "abs": "https://arxiv.org/abs/2510.16449", "authors": ["Bin Yu", "Xinming Wang", "Shijie Lian", "Haotian Li", "Changti Wu", "Ruina Hu", "Bailing Wang", "Yuliang Wei", "Kai Chen"], "title": "TrajSelector: Harnessing Latent Representations for Efficient and Effective Best-of-N in Large Reasoning Model", "comment": "13 pages, 6 figures. Project website:\n  https://zgca-ai4edu.github.io/TrajSelector", "summary": "Large language models (LLMs) have shown remarkable progress in complex\nreasoning tasks, largely enabled by test-time scaling (TTS) paradigms that\nallocate additional compute during inference. Among these, external TTS\n(particularly the Best-of-N selection paradigm) yields scalable performance\nimprovements by selecting from multiple independently generated reasoning\ntrajectories. However, this approach faces key limitations: (i) the high\ncomputational overhead of deploying process reward models, (ii) the\nunderutilization of the LLM's intrinsic latent representations. We introduce\nTrajSelector, an efficient and effective Best-of-N framework that exploit the\nhidden states in the sampler LLM for process-level scoring. A lightweight\nverifier (with only 0.6B parameters) evaluates the quality of step-wise\ntrajectory, and then aggregates these scores to identify the optimal reasoning\ntrajectory. Our framework employs a fully data-driven, end-to-end training\nrecipe that eliminates reliance on massive step-level annotations. Experiential\nresults across five benchmarks demonstrate that TrajSelector delivers\nconsistent performance gains. In Best-of-32 settings, it surpasses majority\nvoting by 4.61% accuracy and outperforms existing process reward models by\n4.31% to 12.21%, all while maintaining lower inference costs.", "AI": {"tldr": "TrajSelector\u662f\u4e00\u4e2a\u9ad8\u6548\u7684Best-of-N\u6846\u67b6\uff0c\u5229\u7528LLM\u7684\u9690\u85cf\u72b6\u6001\u8fdb\u884c\u8fc7\u7a0b\u7ea7\u8bc4\u5206\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u9a8c\u8bc1\u5668\u8bc4\u4f30\u63a8\u7406\u8f68\u8ff9\u8d28\u91cf\uff0c\u5728\u4fdd\u6301\u8f83\u4f4e\u63a8\u7406\u6210\u672c\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5916\u90e8TTS\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u9650\u5236\uff1a(i) \u90e8\u7f72\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u7684\u9ad8\u8ba1\u7b97\u5f00\u9500\uff0c(ii) \u672a\u5145\u5206\u5229\u7528LLM\u7684\u5185\u5728\u6f5c\u5728\u8868\u793a\u3002", "method": "\u4f7f\u7528\u8f7b\u91cf\u7ea7\u9a8c\u8bc1\u5668(\u4ec50.6B\u53c2\u6570)\u8bc4\u4f30\u9010\u6b65\u63a8\u7406\u8f68\u8ff9\u8d28\u91cf\uff0c\u5e76\u805a\u5408\u8fd9\u4e9b\u5206\u6570\u6765\u8bc6\u522b\u6700\u4f18\u63a8\u7406\u8f68\u8ff9\uff0c\u91c7\u7528\u5b8c\u5168\u6570\u636e\u9a71\u52a8\u7684\u7aef\u5230\u7aef\u8bad\u7ec3\u65b9\u6cd5\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTrajSelector\u5728Best-of-32\u8bbe\u7f6e\u4e0b\u6bd4\u591a\u6570\u6295\u7968\u51c6\u786e\u7387\u9ad84.61%\uff0c\u6bd4\u73b0\u6709\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u9ad84.31%\u523012.21%\uff0c\u540c\u65f6\u4fdd\u6301\u66f4\u4f4e\u7684\u63a8\u7406\u6210\u672c\u3002", "conclusion": "TrajSelector\u6846\u67b6\u901a\u8fc7\u5229\u7528LLM\u9690\u85cf\u72b6\u6001\u8fdb\u884c\u8fc7\u7a0b\u7ea7\u8bc4\u5206\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u6709\u6548\u7684Best-of-N\u9009\u62e9\uff0c\u5728\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2510.17371", "categories": ["eess.SY", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.17371", "abs": "https://arxiv.org/abs/2510.17371", "authors": ["Mohammad Boveiri", "Mohammad Khosravi", "Peyman Mohajerin Esfahan"], "title": "Accelerating Adaptive Systems via Normalized Parameter Estimation Laws", "comment": null, "summary": "In this paper, we propose a new class of parameter estimation laws for\nadaptive systems, called \\emph{normalized parameter estimation laws}. A key\nfeature of these estimation laws is that they accelerate the convergence of the\nsystem state, $\\mathit{x(t)}$, to the origin. We quantify this improvement by\nshowing that our estimation laws guarantee finite integrability of the\n$\\mathit{r}$-th root of the squared norm of the system state, i.e., \\(\n\\mathit{\\|x(t)\\|}_2^{2/\\mathit{r}} \\in \\mathcal{L}_1, \\) where $\\mathit{r} \\geq\n1$ is a pre-specified parameter that, for a broad class of systems, can be\nchosen arbitrarily large. In contrast, standard Lyapunov-based estimation laws\nonly guarantee integrability of $\\mathit{\\|x(t)\\|}_2^2$ (i.e., $\\mathit{r} =\n1$). We motivate our method by showing that, for large values of $r$, this\nguarantee serves as a sparsity-promoting mechanism in the time domain, meaning\nthat it penalizes prolonged signal duration and slow decay, thereby promoting\nfaster convergence of $\\mathit{x(t)}$. The proposed estimation laws do not rely\non time-varying or high adaptation gains and do not require persistent\nexcitation. Moreover, they can be applied to systems with matched and unmatched\nuncertainties, regardless of their dynamic structure, as long as a control\nLyapunov function (CLF) exists. Finally, they are compatible with any CLF-based\ncertainty equivalence controllers. We further develop higher-order extensions\nof our estimation laws by incorporating momentum into the estimation dynamics.\nWe illustrate the performance improvements achieved with the proposed scheme\nthrough various numerical experiments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u9002\u5e94\u7cfb\u7edf\u53c2\u6570\u4f30\u8ba1\u65b9\u6cd5\u2014\u2014\u5f52\u4e00\u5316\u53c2\u6570\u4f30\u8ba1\u5f8b\uff0c\u80fd\u52a0\u901f\u7cfb\u7edf\u72b6\u6001\u6536\u655b\u5230\u539f\u70b9\uff0c\u4fdd\u8bc1\u7cfb\u7edf\u72b6\u6001\u76842/r\u6b21\u5e42\u8303\u6570\u6709\u9650\u53ef\u79ef\uff0c\u5176\u4e2dr\u22651\u53ef\u4efb\u610f\u5927\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u53ea\u4fdd\u8bc1\u5e73\u65b9\u8303\u6570\u53ef\u79ef\uff08r=1\uff09\u6709\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u4f20\u7edfLyapunov\u57fa\u53c2\u6570\u4f30\u8ba1\u65b9\u6cd5\u53ea\u80fd\u4fdd\u8bc1\u7cfb\u7edf\u72b6\u6001\u5e73\u65b9\u8303\u6570\u7684\u53ef\u79ef\u6027\uff0c\u6536\u655b\u901f\u5ea6\u8f83\u6162\u3002\u65b0\u65b9\u6cd5\u65e8\u5728\u901a\u8fc7\u5f52\u4e00\u5316\u53c2\u6570\u4f30\u8ba1\u5f8b\u52a0\u901f\u7cfb\u7edf\u72b6\u6001\u6536\u655b\uff0c\u5e76\u5f15\u5165\u65f6\u95f4\u57df\u7a00\u758f\u6027\u4fc3\u8fdb\u673a\u5236\u6765\u60e9\u7f5a\u4fe1\u53f7\u6301\u7eed\u65f6\u95f4\u8fc7\u957f\u548c\u8870\u51cf\u7f13\u6162\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5f52\u4e00\u5316\u53c2\u6570\u4f30\u8ba1\u5f8b\uff0c\u4e0d\u4f9d\u8d56\u65f6\u53d8\u6216\u9ad8\u9002\u5e94\u589e\u76ca\uff0c\u4e0d\u9700\u8981\u6301\u7eed\u6fc0\u52b1\u6761\u4ef6\u3002\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u5339\u914d\u548c\u975e\u5339\u914d\u4e0d\u786e\u5b9a\u7cfb\u7edf\uff0c\u53ea\u8981\u5b58\u5728\u63a7\u5236Lyapunov\u51fd\u6570\u5373\u53ef\u5e94\u7528\uff0c\u4e14\u4e0e\u4efb\u4f55\u57fa\u4e8eCLF\u7684\u786e\u5b9a\u6027\u7b49\u4ef7\u63a7\u5236\u5668\u517c\u5bb9\u3002\u8fd8\u5f00\u53d1\u4e86\u5305\u542b\u52a8\u91cf\u7684\u9ad8\u9636\u6269\u5c55\u7248\u672c\u3002", "result": "\u65b0\u65b9\u6cd5\u4fdd\u8bc1\u7cfb\u7edf\u72b6\u6001\u2016x(t)\u2016\u2082^(2/r) \u2208 L\u2081\uff0c\u5176\u4e2dr\u22651\u53ef\u4efb\u610f\u5927\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u53ea\u4fdd\u8bc1\u2016x(t)\u2016\u2082\u00b2 \u2208 L\u2081\uff08r=1\uff09\u6709\u663e\u8457\u6539\u8fdb\u3002\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u5f52\u4e00\u5316\u53c2\u6570\u4f30\u8ba1\u5f8b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u52a0\u901f\u81ea\u9002\u5e94\u7cfb\u7edf\u72b6\u6001\u6536\u655b\u7684\u65b9\u6cd5\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u9002\u7528\u6027\uff0c\u4e0d\u4f9d\u8d56\u82db\u523b\u6761\u4ef6\uff0c\u4e14\u80fd\u901a\u8fc7\u9ad8\u9636\u6269\u5c55\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2510.16060", "categories": ["cs.LG", "cs.AI", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16060", "abs": "https://arxiv.org/abs/2510.16060", "authors": ["Coen Adler", "Yuxin Chang", "Felix Draxler", "Samar Abdi", "Padhraic Smyth"], "title": "Beyond Accuracy: Are Time Series Foundation Models Well-Calibrated?", "comment": null, "summary": "The recent development of foundation models for time series data has\ngenerated considerable interest in using such models across a variety of\napplications. Although foundation models achieve state-of-the-art predictive\nperformance, their calibration properties remain relatively underexplored,\ndespite the fact that calibration can be critical for many practical\napplications. In this paper, we investigate the calibration-related properties\nof five recent time series foundation models and two competitive baselines. We\nperform a series of systematic evaluations assessing model calibration (i.e.,\nover- or under-confidence), effects of varying prediction heads, and\ncalibration under long-term autoregressive forecasting. We find that time\nseries foundation models are consistently better calibrated than baseline\nmodels and tend not to be either systematically over- or under-confident, in\ncontrast to the overconfidence often seen in other deep learning models.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e865\u4e2a\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u548c2\u4e2a\u57fa\u7ebf\u6a21\u578b\u7684\u6821\u51c6\u7279\u6027\uff0c\u53d1\u73b0\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u6bd4\u57fa\u7ebf\u6a21\u578b\u6821\u51c6\u5f97\u66f4\u597d\uff0c\u4e14\u6ca1\u6709\u7cfb\u7edf\u6027\u7684\u8fc7\u5ea6\u81ea\u4fe1\u6216\u4e0d\u8db3\u81ea\u4fe1\u3002", "motivation": "\u5c3d\u7ba1\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u5728\u9884\u6d4b\u6027\u80fd\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u4f46\u5176\u6821\u51c6\u7279\u6027\u7814\u7a76\u4e0d\u8db3\uff0c\u800c\u6821\u51c6\u786e\u5b9e\u5bf9\u8bb8\u591a\u5b9e\u9645\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5bf95\u4e2a\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u548c2\u4e2a\u7ade\u4e89\u6027\u57fa\u7ebf\u6a21\u578b\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\uff0c\u5305\u62ec\u6a21\u578b\u6821\u51c6\u3001\u4e0d\u540c\u9884\u6d4b\u5934\u7684\u5f71\u54cd\u4ee5\u53ca\u957f\u671f\u81ea\u56de\u5f52\u9884\u6d4b\u4e0b\u7684\u6821\u51c6\u3002", "result": "\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u6bd4\u57fa\u7ebf\u6a21\u578b\u6821\u51c6\u5f97\u66f4\u597d\uff0c\u4e14\u6ca1\u6709\u7cfb\u7edf\u6027\u7684\u8fc7\u5ea6\u81ea\u4fe1\u6216\u4e0d\u8db3\u81ea\u4fe1\uff0c\u8fd9\u4e0e\u6df1\u5ea6\u5b66\u4e60\u4e2d\u5e38\u89c1\u7684\u8fc7\u5ea6\u81ea\u4fe1\u73b0\u8c61\u5f62\u6210\u5bf9\u6bd4\u3002", "conclusion": "\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u4e0d\u4ec5\u9884\u6d4b\u6027\u80fd\u4f18\u8d8a\uff0c\u800c\u4e14\u5177\u6709\u826f\u597d\u7684\u6821\u51c6\u7279\u6027\uff0c\u8fd9\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2510.16944", "categories": ["cs.CY", "cs.SC"], "pdf": "https://arxiv.org/pdf/2510.16944", "abs": "https://arxiv.org/abs/2510.16944", "authors": ["Spencer Rugaber", "Scott Bunin", "Andrew Hornback", "Sungeun An", "Ashok Goel"], "title": "Learning Ecology with VERA Using Conceptual Models and Simulations", "comment": null, "summary": "Conceptual modeling has been an important part of constructionist educational\npractices for many years, particularly in STEM (Science, Technology,\nEngineering and Mathematics) disciplines. What is not so common is using\nagent-based simulation to provide students feedback on model quality. This\nrequires the capability of automatically compiling the concept model into its\nsimulation. The VERA (Virtual Experimentation Research Assistant) system is a\nconceptual modeling tool used since 2016 to provide introductory college\nbiology students with the capability of conceptual modeling and agent-based\nsimulation in the ecological domain. This paper describes VERA and its approach\nto coupling conceptual modeling and simulation with emphasis on how a model's\nvisual syntax is compiled into code executable on a NetLogo simulation engine.\nExperience with VERA in introductory biology classes at several universities\nand through the Smithsonian Institution's Encyclopedia of Life website is\nrelated.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.16309", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16309", "abs": "https://arxiv.org/abs/2510.16309", "authors": ["Crystal Su"], "title": "MedRule-KG: A Knowledge-Graph--Steered Scaffold for Mathematical Reasoning with a Lightweight Verifier", "comment": "Accepted to the Annual Conference on Neural Information Processing\n  Systems (NeurIPS 2026) Workshop", "summary": "Large language models (LLMs) often produce fluent reasoning steps while\nviolating simple mathematical or logical constraints. We introduce MedRule-KG,\na compact typed knowledge graph coupled with a symbolic verifier, designed to\nenforce mathematically interpretable rules in reasoning tasks. MedRule-KG\nencodes entities, relations, and three domain-inspired rules, while the\nverifier checks predictions and applies minimal corrections to guarantee\nconsistency. On a 90-example FDA-derived benchmark, grounding in MedRule-KG\nimproves exact match (EM) from 0.767 to 0.900, and adding the verifier yields\n1.000 EM while eliminating rule violations entirely. We demonstrate how\nMedRule-KG provides a general scaffold for safe mathematical reasoning, discuss\nablations, and release code and data to encourage reproducibility.", "AI": {"tldr": "MedRule-KG\u662f\u4e00\u4e2a\u7d27\u51d1\u7684\u77e5\u8bc6\u56fe\u8c31\u548c\u7b26\u53f7\u9a8c\u8bc1\u5668\u7cfb\u7edf\uff0c\u7528\u4e8e\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u5f3a\u5236\u6267\u884c\u6570\u5b66\u53ef\u89e3\u91ca\u89c4\u5219\uff0c\u663e\u8457\u63d0\u9ad8LLMs\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7ecf\u5e38\u4ea7\u751f\u6d41\u7545\u7684\u63a8\u7406\u6b65\u9aa4\uff0c\u4f46\u8fdd\u53cd\u7b80\u5355\u7684\u6570\u5b66\u6216\u903b\u8f91\u7ea6\u675f\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u786e\u4fdd\u6570\u5b66\u63a8\u7406\u7684\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u3002", "method": "\u5f15\u5165MedRule-KG\uff0c\u4e00\u4e2a\u7d27\u51d1\u7684\u7c7b\u578b\u5316\u77e5\u8bc6\u56fe\u8c31\uff0c\u7ed3\u5408\u7b26\u53f7\u9a8c\u8bc1\u5668\u3002\u77e5\u8bc6\u56fe\u8c31\u7f16\u7801\u5b9e\u4f53\u3001\u5173\u7cfb\u548c\u4e09\u4e2a\u9886\u57df\u542f\u53d1\u89c4\u5219\uff0c\u9a8c\u8bc1\u5668\u68c0\u67e5\u9884\u6d4b\u5e76\u5e94\u7528\u6700\u5c0f\u4fee\u6b63\u4ee5\u4fdd\u8bc1\u4e00\u81f4\u6027\u3002", "result": "\u572890\u4e2aFDA\u884d\u751f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u57fa\u4e8eMedRule-KG\u7684\u63a8\u7406\u5c06\u7cbe\u786e\u5339\u914d\u4ece0.767\u63d0\u9ad8\u52300.900\uff0c\u6dfb\u52a0\u9a8c\u8bc1\u5668\u540e\u8fbe\u52301.000\u7684\u7cbe\u786e\u5339\u914d\uff0c\u5b8c\u5168\u6d88\u9664\u4e86\u89c4\u5219\u8fdd\u53cd\u3002", "conclusion": "MedRule-KG\u4e3a\u5b89\u5168\u7684\u6570\u5b66\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u548c\u7b26\u53f7\u9a8c\u8bc1\u786e\u4fdd\u63a8\u7406\u7684\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u3002"}}
{"id": "2510.17170", "categories": ["math.OC", "cs.NA", "math.NA", "49Q22, 49K15, 65K99, 90B80"], "pdf": "https://arxiv.org/pdf/2510.17170", "abs": "https://arxiv.org/abs/2510.17170", "authors": ["Luca Dieci", "Daniyar Omarov"], "title": "Optimal Trajectories for Optimal Transport in Nonuniform Environments", "comment": null, "summary": "In this work, we solve a discrete optimal transport problem in a nonuniform\nenvironment. The key challenge is to form the cost matrix, which requires\nfinding the optimal path between two points, and for this task we formulate and\nsolve the associated Euler-Lagrange equations. A main theoretical result of\nours is to provide verifiable sufficient conditions of optimality of the\nsolution of the Euler-Lagrange equation. We propose new algorithms to solve the\nproblem, and illustrate our results and performance of the algorithms on\nseveral numerical examples.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5728\u975e\u5747\u5300\u73af\u5883\u4e2d\u89e3\u51b3\u79bb\u6563\u6700\u4f18\u4f20\u8f93\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6c42\u89e3\u6b27\u62c9-\u62c9\u683c\u6717\u65e5\u65b9\u7a0b\u6784\u5efa\u6210\u672c\u77e9\u9635\uff0c\u5e76\u63d0\u4f9b\u4e86\u6700\u4f18\u89e3\u7684\u5145\u5206\u6761\u4ef6\u9a8c\u8bc1\u3002", "motivation": "\u89e3\u51b3\u975e\u5747\u5300\u73af\u5883\u4e2d\u7684\u79bb\u6563\u6700\u4f18\u4f20\u8f93\u95ee\u9898\uff0c\u5173\u952e\u5728\u4e8e\u6784\u5efa\u6210\u672c\u77e9\u9635\uff0c\u8fd9\u9700\u8981\u627e\u5230\u4e24\u70b9\u4e4b\u95f4\u7684\u6700\u4f18\u8def\u5f84\u3002", "method": "\u5236\u5b9a\u5e76\u6c42\u89e3\u76f8\u5173\u7684\u6b27\u62c9-\u62c9\u683c\u6717\u65e5\u65b9\u7a0b\u6765\u6784\u5efa\u6210\u672c\u77e9\u9635\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u7b97\u6cd5\u6765\u89e3\u51b3\u8be5\u95ee\u9898\u3002", "result": "\u63d0\u4f9b\u4e86\u9a8c\u8bc1\u6b27\u62c9-\u62c9\u683c\u6717\u65e5\u65b9\u7a0b\u89e3\u6700\u4f18\u6027\u7684\u5145\u5206\u6761\u4ef6\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u793a\u4f8b\u5c55\u793a\u4e86\u7b97\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "\u6210\u529f\u89e3\u51b3\u4e86\u975e\u5747\u5300\u73af\u5883\u4e2d\u7684\u79bb\u6563\u6700\u4f18\u4f20\u8f93\u95ee\u9898\uff0c\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u6570\u503c\u5b9e\u9a8c\u4e2d\u8868\u73b0\u826f\u597d\u3002"}}
{"id": "2510.15982", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15982", "abs": "https://arxiv.org/abs/2510.15982", "authors": ["Donghyeok Shin", "Yeongmin Kim", "Suhyeon Jo", "Byeonghu Na", "Il-Chul Moon"], "title": "AMiD: Knowledge Distillation for LLMs with $\u03b1$-mixture Assistant Distribution", "comment": null, "summary": "Autoregressive large language models (LLMs) have achieved remarkable\nimprovement across many tasks but incur high computational and memory costs.\nKnowledge distillation (KD) mitigates this issue by transferring knowledge from\na large teacher to a smaller student through distributional alignment. Previous\nstudies have proposed various discrepancy metrics, but the capacity gap and\ntraining instability caused by near-zero probabilities, stemming from the\nhigh-dimensional output of LLMs, remain fundamental limitations. To overcome\nthese challenges, several approaches implicitly or explicitly incorporating\nassistant distribution have recently been proposed. However, the past proposals\nof assistant distributions have been a fragmented approach without a systematic\ninvestigation of the interpolation path and the divergence. This paper proposes\n$\\alpha$-mixture assistant distribution, a novel generalized family of\nassistant distributions, and $\\alpha$-mixture distillation, coined AMiD, a\nunified framework for KD using the assistant distribution. The $\\alpha$-mixture\nassistant distribution provides a continuous extension of the assistant\ndistribution by introducing a new distribution design variable $\\alpha$, which\nhas been fixed in all previous approaches. Furthermore, AMiD generalizes the\nfamily of divergences used with the assistant distributions based on\noptimality, which has also been restricted in previous works. Through extensive\nexperiments, we demonstrate that AMiD offers superior performance and training\nstability by leveraging a broader and theoretically grounded assistant\ndistribution space.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAMiD\u6846\u67b6\uff0c\u901a\u8fc7\u03b1-\u6df7\u5408\u8f85\u52a9\u5206\u5e03\u89e3\u51b3LLM\u77e5\u8bc6\u84b8\u998f\u4e2d\u7684\u5bb9\u91cf\u5dee\u8ddd\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u7edf\u4e00\u4e86\u5148\u524d\u788e\u7247\u5316\u7684\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u81ea\u56de\u5f52\u5927\u8bed\u8a00\u6a21\u578b\u77e5\u8bc6\u84b8\u998f\u4e2d\u56e0\u9ad8\u7ef4\u8f93\u51fa\u5bfc\u81f4\u7684\u5bb9\u91cf\u5dee\u8ddd\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u73b0\u6709\u8f85\u52a9\u5206\u5e03\u65b9\u6cd5\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7814\u7a76\u3002", "method": "\u63d0\u51fa\u03b1-\u6df7\u5408\u8f85\u52a9\u5206\u5e03\uff0c\u5f15\u5165\u5206\u5e03\u8bbe\u8ba1\u53d8\u91cf\u03b1\u5b9e\u73b0\u8fde\u7eed\u6269\u5c55\uff0c\u5e76\u57fa\u4e8e\u6700\u4f18\u6027\u63a8\u5e7f\u4e86\u4e0e\u8f85\u52a9\u5206\u5e03\u4f7f\u7528\u7684\u6563\u5ea6\u65cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660eAMiD\u901a\u8fc7\u5229\u7528\u66f4\u5e7f\u6cdb\u4e14\u7406\u8bba\u57fa\u7840\u7684\u8f85\u52a9\u5206\u5e03\u7a7a\u95f4\uff0c\u63d0\u4f9b\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "conclusion": "AMiD\u6846\u67b6\u7edf\u4e00\u4e86\u77e5\u8bc6\u84b8\u998f\u4e2d\u7684\u8f85\u52a9\u5206\u5e03\u65b9\u6cd5\uff0c\u901a\u8fc7\u7406\u8bba\u57fa\u7840\u7684\u6269\u5c55\u8bbe\u8ba1\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.16455", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16455", "abs": "https://arxiv.org/abs/2510.16455", "authors": ["Deyi Ji", "Yuekui Yang", "Haiyang Wu", "Shaoping Ma", "Tianrun Chen", "Lanyun Zhu"], "title": "RAVEN: Robust Advertisement Video Violation Temporal Grounding via Reinforcement Reasoning", "comment": "ACL 2025 (Oral, Industry Track)", "summary": "Advertisement (Ad) video violation detection is critical for ensuring\nplatform compliance, but existing methods struggle with precise temporal\ngrounding, noisy annotations, and limited generalization. We propose RAVEN, a\nnovel framework that integrates curriculum reinforcement learning with\nmultimodal large language models (MLLMs) to enhance reasoning and cognitive\ncapabilities for violation detection. RAVEN employs a progressive training\nstrategy, combining precisely and coarsely annotated data, and leverages Group\nRelative Policy Optimization (GRPO) to develop emergent reasoning abilities\nwithout explicit reasoning annotations. Multiple hierarchical sophisticated\nreward mechanism ensures precise temporal grounding and consistent category\nprediction. Experiments on industrial datasets and public benchmarks show that\nRAVEN achieves superior performances in violation category accuracy and\ntemporal interval localization. We also design a pipeline to deploy the RAVEN\non the online Ad services, and online A/B testing further validates its\npractical applicability, with significant improvements in precision and recall.\nRAVEN also demonstrates strong generalization, mitigating the catastrophic\nforgetting issue associated with supervised fine-tuning.", "AI": {"tldr": "RAVEN\u662f\u4e00\u4e2a\u7528\u4e8e\u5e7f\u544a\u89c6\u9891\u8fdd\u89c4\u68c0\u6d4b\u7684\u65b0\u6846\u67b6\uff0c\u7ed3\u5408\u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u7b56\u7565\u63d0\u5347\u63a8\u7406\u548c\u8ba4\u77e5\u80fd\u529b\uff0c\u5728\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u5e7f\u544a\u89c6\u9891\u8fdd\u89c4\u68c0\u6d4b\u65b9\u6cd5\u5728\u7cbe\u786e\u65f6\u95f4\u5b9a\u4f4d\u3001\u566a\u58f0\u6807\u6ce8\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u5148\u8fdb\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u6574\u5408\u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\u4e0e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u91c7\u7528\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u7b56\u7565\u7ed3\u5408\u7cbe\u786e\u548c\u7c97\u7565\u6807\u6ce8\u6570\u636e\uff0c\u4f7f\u7528GRPO\u5f00\u53d1\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u8bbe\u8ba1\u591a\u5c42\u6b21\u5956\u52b1\u673a\u5236\u3002", "result": "\u5728\u5de5\u4e1a\u6570\u636e\u96c6\u548c\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRAVEN\u5728\u8fdd\u89c4\u7c7b\u522b\u51c6\u786e\u6027\u548c\u65f6\u95f4\u95f4\u9694\u5b9a\u4f4d\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u7ebfA/B\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "RAVEN\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5e7f\u544a\u89c6\u9891\u8fdd\u89c4\u68c0\u6d4b\u7684\u5173\u952e\u6311\u6218\uff0c\u5177\u6709\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u80fd\u591f\u7f13\u89e3\u76d1\u7763\u5fae\u8c03\u5e26\u6765\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002"}}
{"id": "2510.17619", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.17619", "abs": "https://arxiv.org/abs/2510.17619", "authors": ["Nayab Gogosh", "Sohail Khalid", "Bilal Tariq Malik", "Slawomir Koziel"], "title": "Artificial magnetic conductor backed dual-mode sectoral cylindrical DRA for off-body biomedical telemetry", "comment": "13 pages", "summary": "This research investigates the potential of a sectoral Cylindrical Dielectric\nResonator Antenna (CDRA) for biomedical telemetry. CDRAs are known for their\nlow loss, ruggedness, and stability, but their limited bandwidth and size make\nthem unsuitable for wearable devices. The research addresses these limitations\nby proposing a dual mode antenna that operates in EH110 and TE210 modes. The\nsectoral CDRA is a quarter segment with Perfect Electric Conductor boundaries,\nreducing its size by a factor of four. Mathematical derivations of the field\ncomponents for both modes are derived to support the design. To minimize\nspecific absorption rate (SAR), an Artificial Magnetic Conductor (AMC) surface\nis applied to the antennas backside, enhancing compatibility with the\ntransverse electric modes. The antenna achieves a bandwidth of 0.7 GHz (5.2-5.9\nGHz), suitable for biomedical applications, with a measured peak gain of 7.9\ndBi and a SAR of 1.24 W/kg when applied to a human arm.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u751f\u7269\u533b\u5b66\u9065\u6d4b\u7684\u6247\u5f62\u5706\u67f1\u4ecb\u8d28\u8c10\u632f\u5668\u5929\u7ebf\uff0c\u901a\u8fc7\u53cc\u6a21\u64cd\u4f5c\u548cAMC\u8868\u9762\u8bbe\u8ba1\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfCDRA\u5e26\u5bbd\u6709\u9650\u548c\u5c3a\u5bf8\u8fc7\u5927\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5c0f\u578b\u5316\u3001\u4f4eSAR\u548c\u9ad8\u589e\u76ca\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5706\u67f1\u4ecb\u8d28\u8c10\u632f\u5668\u5929\u7ebf\u867d\u7136\u5177\u6709\u4f4e\u635f\u8017\u3001\u575a\u56fa\u548c\u7a33\u5b9a\u7b49\u4f18\u70b9\uff0c\u4f46\u5176\u6709\u9650\u7684\u5e26\u5bbd\u548c\u8f83\u5927\u7684\u5c3a\u5bf8\u4f7f\u5176\u4e0d\u9002\u5408\u53ef\u7a7f\u6234\u8bbe\u5907\u5e94\u7528\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u5f00\u53d1\u9002\u7528\u4e8e\u751f\u7269\u533b\u5b66\u9065\u6d4b\u7684\u5c0f\u578b\u5316\u5929\u7ebf\u3002", "method": "\u91c7\u7528\u6247\u5f62CDRA\u8bbe\u8ba1\uff08\u56db\u5206\u4e4b\u4e00\u6bb5\u7ed3\u6784\uff09\uff0c\u4f7f\u7528\u5b8c\u7f8e\u7535\u5bfc\u4f53\u8fb9\u754c\u51cf\u5c0f\u5c3a\u5bf8\uff1b\u8bbe\u8ba1\u53cc\u6a21\u5929\u7ebf\uff08EH110\u548cTE210\u6a21\u5f0f\uff09\uff1b\u5e94\u7528\u4eba\u5de5\u78c1\u5bfc\u4f53\u8868\u9762\u964d\u4f4eSAR\uff1b\u901a\u8fc7\u6570\u5b66\u63a8\u5bfc\u652f\u6301\u573a\u5206\u91cf\u8bbe\u8ba1\u3002", "result": "\u5929\u7ebf\u57285.2-5.9 GHz\u9891\u6bb5\u5b9e\u73b00.7 GHz\u5e26\u5bbd\uff0c\u5cf0\u503c\u589e\u76ca7.9 dBi\uff0c\u5e94\u7528\u4e8e\u4eba\u4f53\u624b\u81c2\u65f6\u7684SAR\u503c\u4e3a1.24 W/kg\uff0c\u9002\u5408\u751f\u7269\u533b\u5b66\u5e94\u7528\u3002", "conclusion": "\u6247\u5f62CDRA\u7ed3\u5408AMC\u8868\u9762\u6210\u529f\u5b9e\u73b0\u4e86\u5c0f\u578b\u5316\u3001\u5bbd\u5e26\u548c\u9ad8\u6027\u80fd\u7684\u751f\u7269\u533b\u5b66\u9065\u6d4b\u5929\u7ebf\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfCDRA\u5728\u53ef\u7a7f\u6234\u8bbe\u5907\u5e94\u7528\u4e2d\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.16097", "categories": ["cs.LG", "cs.AI", "cs.CY", "cs.HC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16097", "abs": "https://arxiv.org/abs/2510.16097", "authors": ["Eleni Straitouri", "Stratis Tsirtsis", "Ander Artola Velasco", "Manuel Gomez-Rodriguez"], "title": "Narrowing Action Choices with AI Improves Human Sequential Decisions", "comment": "Accepted at the Human-AI Complementarity for Decision Making Workshop\n  2025 by the NSF AI Institute for Societal Decision Making", "summary": "Recent work has shown that, in classification tasks, it is possible to design\ndecision support systems that do not require human experts to understand when\nto cede agency to a classifier or when to exercise their own agency to achieve\ncomplementarity$\\unicode{x2014}$experts using these systems make more accurate\npredictions than those made by the experts or the classifier alone. The key\nprinciple underpinning these systems reduces to adaptively controlling the\nlevel of human agency, by design. Can we use the same principle to achieve\ncomplementarity in sequential decision making tasks? In this paper, we answer\nthis question affirmatively. We develop a decision support system that uses a\npre-trained AI agent to narrow down the set of actions a human can take to a\nsubset, and then asks the human to take an action from this action set. Along\nthe way, we also introduce a bandit algorithm that leverages the smoothness\nproperties of the action sets provided by our system to efficiently optimize\nthe level of human agency. To evaluate our decision support system, we conduct\na large-scale human subject study ($n = 1{,}600$) where participants play a\nwildfire mitigation game. We find that participants who play the game supported\nby our system outperform those who play on their own by $\\sim$$30$% and the AI\nagent used by our system by $>$$2$%, even though the AI agent largely\noutperforms participants playing without support. We have made available the\ndata gathered in our human subject study as well as an open source\nimplementation of our system at\nhttps://github.com/Networks-Learning/narrowing-action-choices .", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u7684AI\u4ee3\u7406\u7f29\u5c0f\u4eba\u7c7b\u53ef\u91c7\u53d6\u7684\u884c\u52a8\u8303\u56f4\uff0c\u5b9e\u73b0\u4eba\u673a\u4e92\u8865\u6027\uff0c\u5728\u91ce\u706b\u7f13\u89e3\u6e38\u620f\u4e2d\u4f7f\u53c2\u4e0e\u8005\u8868\u73b0\u63d0\u5347\u7ea630%\u3002", "motivation": "\u63a2\u7d22\u662f\u5426\u80fd\u5728\u987a\u5e8f\u51b3\u7b56\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4eba\u673a\u4e92\u8865\u6027\uff0c\u8ba9\u4e13\u5bb6\u4f7f\u7528\u7cfb\u7edf\u65f6\u6bd4\u5355\u72ec\u4f7f\u7528AI\u6216\u4eba\u7c7b\u4e13\u5bb6\u8868\u73b0\u66f4\u597d\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3AI\u4ee3\u7406\u7f29\u5c0f\u4eba\u7c7b\u53ef\u91c7\u53d6\u7684\u884c\u52a8\u5b50\u96c6\uff0c\u7136\u540e\u8ba9\u4eba\u7c7b\u4ece\u8be5\u5b50\u96c6\u4e2d\u9009\u62e9\u884c\u52a8\uff0c\u5e76\u5f15\u5165\u5229\u7528\u52a8\u4f5c\u96c6\u5e73\u6ed1\u7279\u6027\u7684\u591a\u81c2\u8001\u864e\u673a\u7b97\u6cd5\u6765\u4f18\u5316\u4eba\u7c7b\u4ee3\u7406\u6c34\u5e73\u3002", "result": "\u57281600\u4eba\u53c2\u4e0e\u7684\u91ce\u706b\u7f13\u89e3\u6e38\u620f\u7814\u7a76\u4e2d\uff0c\u4f7f\u7528\u7cfb\u7edf\u7684\u53c2\u4e0e\u8005\u6bd4\u5355\u72ec\u53c2\u4e0e\u8005\u8868\u73b0\u63d0\u5347\u7ea630%\uff0c\u6bd4AI\u4ee3\u7406\u8868\u73b0\u63d0\u5347\u8d85\u8fc72%\uff0c\u5c3d\u7ba1AI\u4ee3\u7406\u672c\u8eab\u663e\u8457\u4f18\u4e8e\u65e0\u652f\u6301\u7684\u53c2\u4e0e\u8005\u3002", "conclusion": "\u901a\u8fc7\u8bbe\u8ba1\u63a7\u5236\u4eba\u7c7b\u4ee3\u7406\u6c34\u5e73\u7684\u65b9\u6cd5\u53ef\u4ee5\u5728\u987a\u5e8f\u51b3\u7b56\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4eba\u673a\u4e92\u8865\u6027\uff0c\u7cfb\u7edf\u663e\u8457\u63d0\u5347\u4e86\u4eba\u7c7b\u51b3\u7b56\u8868\u73b0\u3002"}}
{"id": "2510.16951", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2510.16951", "abs": "https://arxiv.org/abs/2510.16951", "authors": ["Christine Sowa Lepird", "Kathleen M. Carley"], "title": "Local News Hijacking: A Review of International Instances", "comment": null, "summary": "In the rise of the digital era, it's easier than ever to create nefarious\nwebsites to spread misinformation. A more recent phenomenon in the United\nStates has been the creation of inauthentic local news websites to further an\ninformation operation campaign. This paper is a review of the 7 instances in\nwhich local news websites were created to influence residents of a region\nbetween 2007 and 2024. By breaking down the ways in which these sites operated,\nwe discovered commonalities in the approach - resurrecting \"zombie\" papers that\nwere previously established authentic local news organizations, sharing these\nsites on social media, and using website templates from WordPress. By analyzing\nthese commonalities, we propose ways to mitigate the occurrence of these\ncampaigns in the future.", "AI": {"tldr": "\u672c\u6587\u56de\u987e\u4e862007-2024\u5e74\u95f4\u7f8e\u56fd7\u8d77\u521b\u5efa\u865a\u5047\u5730\u65b9\u65b0\u95fb\u7f51\u7ad9\u8fdb\u884c\u4fe1\u606f\u64cd\u7eb5\u7684\u6848\u4f8b\uff0c\u5206\u6790\u4e86\u8fd9\u4e9b\u7f51\u7ad9\u7684\u8fd0\u4f5c\u6a21\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e86\u9632\u8303\u63aa\u65bd\u3002", "motivation": "\u968f\u7740\u6570\u5b57\u65f6\u4ee3\u53d1\u5c55\uff0c\u521b\u5efa\u6076\u610f\u7f51\u7ad9\u4f20\u64ad\u865a\u5047\u4fe1\u606f\u53d8\u5f97\u66f4\u5bb9\u6613\u3002\u8fd1\u5e74\u6765\u7f8e\u56fd\u51fa\u73b0\u4e86\u521b\u5efa\u865a\u5047\u5730\u65b9\u65b0\u95fb\u7f51\u7ad9\u8fdb\u884c\u4fe1\u606f\u64cd\u7eb5\u6d3b\u52a8\u7684\u65b0\u73b0\u8c61\uff0c\u9700\u8981\u7814\u7a76\u5176\u8fd0\u4f5c\u6a21\u5f0f\u5e76\u63d0\u51fa\u5e94\u5bf9\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u5206\u67907\u4e2a\u865a\u5047\u5730\u65b9\u65b0\u95fb\u7f51\u7ad9\u7684\u6848\u4f8b\uff0c\u7814\u7a76\u5176\u8fd0\u4f5c\u65b9\u5f0f\uff0c\u5305\u62ec\u590d\u6d3b\u5df2\u505c\u520a\u7684\"\u50f5\u5c38\"\u62a5\u7eb8\u3001\u5728\u793e\u4ea4\u5a92\u4f53\u5206\u4eab\u3001\u4f7f\u7528WordPress\u6a21\u677f\u7b49\u5171\u540c\u7279\u5f81\u3002", "result": "\u53d1\u73b0\u8fd9\u4e9b\u865a\u5047\u5730\u65b9\u65b0\u95fb\u7f51\u7ad9\u5177\u6709\u5171\u540c\u8fd0\u4f5c\u6a21\u5f0f\uff1a\u590d\u6d3b\u539f\u6709\u7684\u771f\u5b9e\u5730\u65b9\u65b0\u95fb\u673a\u6784\u54c1\u724c\u3001\u5229\u7528\u793e\u4ea4\u5a92\u4f53\u4f20\u64ad\u3001\u4f7f\u7528WordPress\u7f51\u7ad9\u6a21\u677f\u3002\u8fd9\u4e9b\u6a21\u5f0f\u4f7f\u5f97\u865a\u5047\u7f51\u7ad9\u66f4\u5177\u6b3a\u9a97\u6027\u3002", "conclusion": "\u57fa\u4e8e\u5bf9\u8fd9\u4e9b\u5171\u540c\u7279\u5f81\u7684\u5206\u6790\uff0c\u63d0\u51fa\u4e86\u672a\u6765\u51cf\u8f7b\u6b64\u7c7b\u4fe1\u606f\u64cd\u7eb5\u6d3b\u52a8\u53d1\u751f\u7684\u5177\u4f53\u65b9\u6cd5\uff0c\u4e3a\u9632\u8303\u865a\u5047\u5730\u65b9\u65b0\u95fb\u7f51\u7ad9\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.16342", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16342", "abs": "https://arxiv.org/abs/2510.16342", "authors": ["Tong Zhang", "Ru Zhang", "Jianyi Liu", "Zhen Yang", "Gongshen Liu"], "title": "Beyond Fixed Anchors: Precisely Erasing Concepts with Sibling Exclusive Counterparts", "comment": null, "summary": "Existing concept erasure methods for text-to-image diffusion models commonly\nrely on fixed anchor strategies, which often lead to critical issues such as\nconcept re-emergence and erosion. To address this, we conduct causal tracing to\nreveal the inherent sensitivity of erasure to anchor selection and define\nSibling Exclusive Concepts as a superior class of anchors. Based on this\ninsight, we propose \\textbf{SELECT} (Sibling-Exclusive Evaluation for\nContextual Targeting), a dynamic anchor selection framework designed to\novercome the limitations of fixed anchors. Our framework introduces a novel\ntwo-stage evaluation mechanism that automatically discovers optimal anchors for\nprecise erasure while identifying critical boundary anchors to preserve related\nconcepts. Extensive evaluations demonstrate that SELECT, as a universal anchor\nsolution, not only efficiently adapts to multiple erasure frameworks but also\nconsistently outperforms existing baselines across key performance metrics,\naveraging only 4 seconds for anchor mining of a single concept.", "AI": {"tldr": "\u63d0\u51fa\u4e86SELECT\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u951a\u70b9\u9009\u62e9\u89e3\u51b3\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4e2d\u6982\u5ff5\u64e6\u9664\u7684\u56fa\u5b9a\u951a\u70b9\u7b56\u7565\u95ee\u9898\uff0c\u907f\u514d\u6982\u5ff5\u91cd\u73b0\u548c\u4fb5\u8680\u3002", "motivation": "\u73b0\u6709\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u951a\u70b9\u7b56\u7565\uff0c\u5bfc\u81f4\u6982\u5ff5\u91cd\u73b0\u548c\u4fb5\u8680\u7b49\u5173\u952e\u95ee\u9898\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u951a\u70b9\u9009\u62e9\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u56e0\u679c\u8ffd\u8e2a\u63ed\u793a\u64e6\u9664\u5bf9\u951a\u70b9\u9009\u62e9\u7684\u654f\u611f\u6027\uff0c\u5b9a\u4e49\u5144\u5f1f\u6392\u4ed6\u6982\u5ff5\u4f5c\u4e3a\u66f4\u4f18\u951a\u70b9\u7c7b\u522b\uff0c\u63d0\u51fa\u4e24\u9636\u6bb5\u8bc4\u4f30\u673a\u5236\u81ea\u52a8\u53d1\u73b0\u6700\u4f18\u951a\u70b9\u3002", "result": "SELECT\u4f5c\u4e3a\u901a\u7528\u951a\u70b9\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u9ad8\u6548\u9002\u5e94\u591a\u79cd\u64e6\u9664\u6846\u67b6\uff0c\u5728\u5173\u952e\u6027\u80fd\u6307\u6807\u4e0a\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5355\u4e2a\u6982\u5ff5\u951a\u70b9\u6316\u6398\u4ec5\u97004\u79d2\u3002", "conclusion": "SELECT\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u951a\u70b9\u9009\u62e9\u6709\u6548\u89e3\u51b3\u4e86\u6982\u5ff5\u64e6\u9664\u4e2d\u7684\u951a\u70b9\u4f9d\u8d56\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u548c\u9ad8\u6548\u7684\u6982\u5ff5\u63a7\u5236\u80fd\u529b\u3002"}}
{"id": "2510.17242", "categories": ["math.OC", "math-ph", "math.AP", "math.MP"], "pdf": "https://arxiv.org/pdf/2510.17242", "abs": "https://arxiv.org/abs/2510.17242", "authors": ["Veronica Danesi", "Cristian Mendico", "Xuan Tao", "Kaizhi Wang"], "title": "Periodic limit for non-autonomous Lagrangian systems and applications to a Kuramoto type model", "comment": null, "summary": "This paper explores the asymptotic properties of non-autonomous Lagrangian\nsystems, assuming that the associated Tonelli Lagrangian converges to a\ntime-periodic function. Specifically, given a continuous initial condition, we\nprovide a suitable construction of a Lax-Oleinik semigroup such that it\nconverges toward a periodic solution of the equation. Moreover, the graph of\nits gradient converges as time tends to infinity to the graph of the gradient\nof the periodic limit function with respect to the Hausdorff distance. Finally,\nwe apply this result to a Kuramoto-type model, proving the existence of an\ninvariant torus given by the graph of the gradient of the limiting periodic\nsolution of the Hamilton-Jacobi equation.", "AI": {"tldr": "\u7814\u7a76\u975e\u81ea\u6cbb\u62c9\u683c\u6717\u65e5\u7cfb\u7edf\u7684\u6e10\u8fd1\u6027\u8d28\uff0c\u8bc1\u660e\u5f53Tonelli\u62c9\u683c\u6717\u65e5\u91cf\u6536\u655b\u4e8e\u65f6\u95f4\u5468\u671f\u51fd\u6570\u65f6\uff0cLax-Oleinik\u534a\u7fa4\u6536\u655b\u4e8e\u65b9\u7a0b\u7684\u5468\u671f\u89e3\uff0c\u4e14\u5176\u68af\u5ea6\u56fe\u5728Hausdorff\u8ddd\u79bb\u4e0b\u6536\u655b\u4e8e\u6781\u9650\u5468\u671f\u51fd\u6570\u7684\u68af\u5ea6\u56fe\u3002", "motivation": "\u63a2\u7d22\u975e\u81ea\u6cbb\u62c9\u683c\u6717\u65e5\u7cfb\u7edf\u5728Tonelli\u62c9\u683c\u6717\u65e5\u91cf\u6536\u655b\u4e8e\u65f6\u95f4\u5468\u671f\u51fd\u6570\u65f6\u7684\u6e10\u8fd1\u884c\u4e3a\uff0c\u4e3a\u7406\u89e3\u6b64\u7c7b\u7cfb\u7edf\u7684\u957f\u671f\u52a8\u529b\u5b66\u7279\u6027\u63d0\u4f9b\u7406\u8bba\u6846\u67b6\u3002", "method": "\u6784\u5efa\u5408\u9002\u7684Lax-Oleinik\u534a\u7fa4\uff0c\u5206\u6790\u5176\u6536\u655b\u6027\u8d28\uff0c\u8bc1\u660e\u8be5\u534a\u7fa4\u6536\u655b\u4e8e\u65b9\u7a0b\u7684\u5468\u671f\u89e3\uff0c\u5e76\u7814\u7a76\u5176\u68af\u5ea6\u56fe\u7684Hausdorff\u6536\u655b\u6027\u3002", "result": "\u6210\u529f\u8bc1\u660e\u4e86Lax-Oleinik\u534a\u7fa4\u6536\u655b\u4e8e\u5468\u671f\u89e3\uff0c\u5176\u68af\u5ea6\u56fe\u5728Hausdorff\u8ddd\u79bb\u4e0b\u6536\u655b\u4e8e\u6781\u9650\u5468\u671f\u51fd\u6570\u7684\u68af\u5ea6\u56fe\uff0c\u5e76\u5c06\u7ed3\u679c\u5e94\u7528\u4e8eKuramoto\u578b\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u7531\u54c8\u5bc6\u987f-\u96c5\u53ef\u6bd4\u65b9\u7a0b\u6781\u9650\u5468\u671f\u89e3\u68af\u5ea6\u56fe\u7ed9\u51fa\u7684\u4e0d\u53d8\u73af\u9762\u7684\u5b58\u5728\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u975e\u81ea\u6cbb\u62c9\u683c\u6717\u65e5\u7cfb\u7edf\u7684\u6e10\u8fd1\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u7279\u522b\u5728Tonelli\u62c9\u683c\u6717\u65e5\u91cf\u6536\u655b\u4e8e\u5468\u671f\u51fd\u6570\u7684\u60c5\u51b5\u4e0b\uff0c\u7cfb\u7edf\u8868\u73b0\u51fa\u826f\u597d\u7684\u6536\u655b\u6027\u8d28\uff0c\u8fd9\u4e00\u7ed3\u679c\u5728Kuramoto\u578b\u6a21\u578b\u7b49\u5e94\u7528\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2510.15985", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15985", "abs": "https://arxiv.org/abs/2510.15985", "authors": ["Zexi Tan", "Tao Xie", "Binbin Sun", "Xiang Zhang", "Yiqun Zhang", "Yiu-Ming Cheung"], "title": "MEET-Sepsis: Multi-Endogenous-View Enhanced Time-Series Representation Learning for Early Sepsis Prediction Representation Learning for Early Sepsis Prediction", "comment": "Accepted to PRICAI 2025", "summary": "Sepsis is a life-threatening infectious syndrome associated with high\nmortality in intensive care units (ICUs). Early and accurate sepsis prediction\n(SP) is critical for timely intervention, yet remains challenging due to subtle\nearly manifestations and rapidly escalating mortality. While AI has improved SP\nefficiency, existing methods struggle to capture weak early temporal signals.\nThis paper introduces a Multi-Endogenous-view Representation Enhancement (MERE)\nmechanism to construct enriched feature views, coupled with a Cascaded\nDual-convolution Time-series Attention (CDTA) module for multi-scale temporal\nrepresentation learning. The proposed MEET-Sepsis framework achieves\ncompetitive prediction accuracy using only 20% of the ICU monitoring time\nrequired by SOTA methods, significantly advancing early SP. Extensive\nvalidation confirms its efficacy. Code is available at:\nhttps://github.com/yueliangy/MEET-Sepsis.", "AI": {"tldr": "\u63d0\u51faMEET-Sepsis\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5185\u6e90\u89c6\u56fe\u8868\u793a\u589e\u5f3a\u673a\u5236\u548c\u7ea7\u8054\u53cc\u5377\u79ef\u65f6\u95f4\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u4ec5\u970020%ICU\u76d1\u6d4b\u65f6\u95f4\u5373\u53ef\u5b9e\u73b0\u7ade\u4e89\u6027\u7684\u8113\u6bd2\u75c7\u9884\u6d4b\u51c6\u786e\u7387", "motivation": "\u8113\u6bd2\u75c7\u5728ICU\u4e2d\u6b7b\u4ea1\u7387\u9ad8\uff0c\u65e9\u671f\u51c6\u786e\u9884\u6d4b\u5bf9\u53ca\u65f6\u5e72\u9884\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709AI\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u5fae\u5f31\u7684\u65e9\u671f\u65f6\u95f4\u4fe1\u53f7", "method": "\u91c7\u7528\u591a\u5185\u6e90\u89c6\u56fe\u8868\u793a\u589e\u5f3a\u673a\u5236\u6784\u5efa\u4e30\u5bcc\u7279\u5f81\u89c6\u56fe\uff0c\u7ed3\u5408\u7ea7\u8054\u53cc\u5377\u79ef\u65f6\u95f4\u6ce8\u610f\u529b\u6a21\u5757\u8fdb\u884c\u591a\u5c3a\u5ea6\u65f6\u95f4\u8868\u793a\u5b66\u4e60", "result": "\u4ec5\u9700SOTA\u65b9\u6cd520%\u7684ICU\u76d1\u6d4b\u65f6\u95f4\u5373\u53ef\u8fbe\u5230\u7ade\u4e89\u6027\u9884\u6d4b\u51c6\u786e\u7387\uff0c\u663e\u8457\u63a8\u8fdb\u65e9\u671f\u8113\u6bd2\u75c7\u9884\u6d4b", "conclusion": "MEET-Sepsis\u6846\u67b6\u5728\u5e7f\u6cdb\u9a8c\u8bc1\u4e2d\u8bc1\u5b9e\u4e86\u5176\u6709\u6548\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65e9\u671f\u8113\u6bd2\u75c7\u9884\u6d4b\u80fd\u529b"}}
{"id": "2510.16458", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16458", "abs": "https://arxiv.org/abs/2510.16458", "authors": ["Pingjun Hong", "Beiduo Chen", "Siyao Peng", "Marie-Catherine de Marneffe", "Benjamin Roth", "Barbara Plank"], "title": "Agree, Disagree, Explain: Decomposing Human Label Variation in NLI through the Lens of Explanations", "comment": null, "summary": "Natural Language Inference datasets often exhibit human label variation. To\nbetter understand these variations, explanation-based approaches analyze the\nunderlying reasoning behind annotators' decisions. One such approach is the\nLiTEx taxonomy, which categorizes free-text explanations in English into\nreasoning types. However, previous work applying such taxonomies has focused on\nwithin-label variation: cases where annotators agree on the final NLI label but\nprovide different explanations. In contrast, this paper broadens the scope by\nexamining how annotators may diverge not only in the reasoning type but also in\nthe labeling step. We use explanations as a lens to decompose the reasoning\nprocess underlying NLI annotation and to analyze individual differences. We\napply LiTEx to two NLI English datasets and align annotation variation from\nmultiple aspects: NLI label agreement, explanation similarity, and taxonomy\nagreement, with an additional compounding factor of annotators' selection bias.\nWe observe instances where annotators disagree on the label but provide highly\nsimilar explanations, suggesting that surface-level disagreement may mask\nunderlying agreement in interpretation. Moreover, our analysis reveals\nindividual preferences in explanation strategies and label choices. These\nfindings highlight that agreement in reasoning types better reflects the\nsemantic similarity of free-text explanations than label agreement alone. Our\nfindings underscore the richness of reasoning-based explanations and the need\nfor caution in treating labels as ground truth.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u89e3\u91ca\u5206\u6790\u6269\u5c55\u4e86\u5bf9\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u6807\u6ce8\u53d8\u5f02\u7684\u7406\u89e3\uff0c\u4e0d\u4ec5\u5173\u6ce8\u6807\u7b7e\u4e00\u81f4\u4f46\u89e3\u91ca\u4e0d\u540c\u7684\u60c5\u51b5\uff0c\u8fd8\u7814\u7a76\u4e86\u6807\u7b7e\u548c\u89e3\u91ca\u90fd\u4e0d\u540c\u7684\u60c5\u51b5\uff0c\u53d1\u73b0\u8868\u9762\u6807\u7b7e\u5206\u6b67\u53ef\u80fd\u63a9\u76d6\u6df1\u5c42\u7684\u89e3\u91ca\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6807\u6ce8\u8005\u5728\u6700\u7ec8NLI\u6807\u7b7e\u4e00\u81f4\u4f46\u89e3\u91ca\u4e0d\u540c\u7684\u60c5\u51b5\uff0c\u672c\u6587\u5e0c\u671b\u6269\u5c55\u7814\u7a76\u8303\u56f4\uff0c\u5206\u6790\u6807\u6ce8\u8005\u5728\u63a8\u7406\u7c7b\u578b\u548c\u6807\u6ce8\u6b65\u9aa4\u4e0a\u90fd\u5b58\u5728\u5206\u6b67\u7684\u60c5\u51b5\uff0c\u4ee5\u66f4\u5168\u9762\u7406\u89e3NLI\u6807\u6ce8\u53d8\u5f02\u3002", "method": "\u5c06LiTEx\u5206\u7c7b\u6cd5\u5e94\u7528\u4e8e\u4e24\u4e2a\u82f1\u6587NLI\u6570\u636e\u96c6\uff0c\u4ece\u591a\u4e2a\u7ef4\u5ea6\u5bf9\u9f50\u6807\u6ce8\u53d8\u5f02\uff1aNLI\u6807\u7b7e\u4e00\u81f4\u6027\u3001\u89e3\u91ca\u76f8\u4f3c\u6027\u548c\u5206\u7c7b\u6cd5\u4e00\u81f4\u6027\uff0c\u5e76\u8003\u8651\u6807\u6ce8\u8005\u9009\u62e9\u504f\u89c1\u7684\u590d\u5408\u56e0\u7d20\u3002", "result": "\u53d1\u73b0\u6807\u6ce8\u8005\u6807\u7b7e\u4e0d\u4e00\u81f4\u4f46\u89e3\u91ca\u9ad8\u5ea6\u76f8\u4f3c\u7684\u60c5\u51b5\uff0c\u8868\u660e\u8868\u9762\u5206\u6b67\u53ef\u80fd\u63a9\u76d6\u89e3\u91ca\u5c42\u9762\u7684\u4e00\u81f4\u6027\uff1b\u5206\u6790\u8fd8\u63ed\u793a\u4e86\u6807\u6ce8\u8005\u5728\u89e3\u91ca\u7b56\u7565\u548c\u6807\u7b7e\u9009\u62e9\u4e0a\u7684\u4e2a\u4f53\u504f\u597d\u3002", "conclusion": "\u63a8\u7406\u7c7b\u578b\u7684\u4e00\u81f4\u6027\u6bd4\u6807\u7b7e\u4e00\u81f4\u6027\u66f4\u80fd\u53cd\u6620\u81ea\u7531\u6587\u672c\u89e3\u91ca\u7684\u8bed\u4e49\u76f8\u4f3c\u6027\uff0c\u57fa\u4e8e\u63a8\u7406\u7684\u89e3\u91ca\u5177\u6709\u4e30\u5bcc\u6027\uff0c\u9700\u8981\u8c28\u614e\u5c06\u6807\u7b7e\u89c6\u4e3a\u7edd\u5bf9\u771f\u503c\u3002"}}
{"id": "2510.17762", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.17762", "abs": "https://arxiv.org/abs/2510.17762", "authors": ["Alexandra E. Ballentine", "Raghvendra V. Cowlagi"], "title": "Trajectory Optimization for Minimum Threat Exposure using Physics-Informed Neural Networks", "comment": "2025 Indian Control Conference", "summary": "We apply a physics-informed neural network (PINN) to solve the two-point\nboundary value problem (BVP) arising from the necessary conditions postulated\nby Pontryagin's Minimum Principle for optimal control. Such BVPs are known to\nbe numerically difficult to solve by traditional shooting methods due to\nextremely high sensitivity to initial guesses. In the light of recent successes\nin applying PINNs for solving high-dimensional differential equations, we\ndevelop a PINN to solve the problem of finding trajectories with minimum\nexposure to a spatiotemporal threat for a vehicle kinematic model. First, we\nimplement PINNs that are trained to solve the BVP for a given pair of initial\nand final states for a given threat field. Next, we implement a PINN\nconditioned on the initial state for a given threat field, which eliminates the\nneed for retraining for each initial state. We demonstrate that the PINN\noutputs satisfy the necessary conditions with low numerical error.", "AI": {"tldr": "\u4f7f\u7528\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc(PINN)\u6c42\u89e3\u7531\u5e9e\u7279\u91cc\u4e9a\u91d1\u6781\u5c0f\u503c\u539f\u7406\u4ea7\u751f\u7684\u4e24\u70b9\u8fb9\u503c\u95ee\u9898\uff0c\u89e3\u51b3\u4f20\u7edf\u6253\u9776\u6cd5\u5bf9\u521d\u503c\u9ad8\u5ea6\u654f\u611f\u7684\u95ee\u9898\uff0c\u5e94\u7528\u4e8e\u8f66\u8f86\u8fd0\u52a8\u5b66\u6a21\u578b\u7684\u6700\u5c0f\u5a01\u80c1\u66b4\u9732\u8f68\u8ff9\u89c4\u5212\u3002", "motivation": "\u4f20\u7edf\u6253\u9776\u6cd5\u6c42\u89e3\u6700\u4f18\u63a7\u5236\u95ee\u9898\u4e2d\u7684\u4e24\u70b9\u8fb9\u503c\u95ee\u9898\u65f6\uff0c\u5bf9\u521d\u59cb\u731c\u6d4b\u6781\u4e3a\u654f\u611f\uff0c\u6570\u503c\u6c42\u89e3\u56f0\u96be\u3002\u800cPINN\u5728\u6c42\u89e3\u9ad8\u7ef4\u5fae\u5206\u65b9\u7a0b\u65b9\u9762\u5df2\u53d6\u5f97\u663e\u8457\u6210\u529f\uff0c\u56e0\u6b64\u5c1d\u8bd5\u5c06\u5176\u5e94\u7528\u4e8e\u6b64\u7c7b\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e24\u79cdPINN\uff1a\u4e00\u79cd\u662f\u9488\u5bf9\u7ed9\u5b9a\u521d\u59cb\u548c\u7ec8\u7aef\u72b6\u6001\u7684BVP\u6c42\u89e3\u5668\uff1b\u53e6\u4e00\u79cd\u662f\u4ec5\u4ee5\u521d\u59cb\u72b6\u6001\u4e3a\u6761\u4ef6\u7684PINN\uff0c\u65e0\u9700\u4e3a\u6bcf\u4e2a\u521d\u59cb\u72b6\u6001\u91cd\u65b0\u8bad\u7ec3\u3002", "result": "PINN\u8f93\u51fa\u6ee1\u8db3\u5fc5\u8981\u6761\u4ef6\u4e14\u6570\u503c\u8bef\u5dee\u8f83\u4f4e\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "PINN\u80fd\u591f\u6709\u6548\u6c42\u89e3\u6700\u4f18\u63a7\u5236\u4e2d\u7684\u4e24\u70b9\u8fb9\u503c\u95ee\u9898\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u654f\u611f\u6027\u9650\u5236\uff0c\u4e3a\u590d\u6742\u5a01\u80c1\u73af\u5883\u4e0b\u7684\u8f68\u8ff9\u89c4\u5212\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2510.16132", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16132", "abs": "https://arxiv.org/abs/2510.16132", "authors": ["Phalguni Nanda", "Zaiwei Chen"], "title": "A Minimal-Assumption Analysis of Q-Learning with Time-Varying Policies", "comment": "43 pages, 4 figures", "summary": "In this work, we present the first finite-time analysis of the Q-learning\nalgorithm under time-varying learning policies (i.e., on-policy sampling) with\nminimal assumptions -- specifically, assuming only the existence of a policy\nthat induces an irreducible Markov chain over the state space. We establish a\nlast-iterate convergence rate for $\\mathbb{E}[\\|Q_k - Q^*\\|_\\infty^2]$,\nimplying a sample complexity of order $O(1/\\epsilon^2)$ for achieving\n$\\mathbb{E}[\\|Q_k - Q^*\\|_\\infty] \\le \\epsilon$, matching that of off-policy\nQ-learning but with a worse dependence on exploration-related parameters. We\nalso derive an explicit rate for $\\mathbb{E}[\\|Q^{\\pi_k} - Q^*\\|_\\infty^2]$,\nwhere $\\pi_k$ is the learning policy at iteration $k$. These results reveal\nthat on-policy Q-learning exhibits weaker exploration than its off-policy\ncounterpart but enjoys an exploitation advantage, as its policy converges to an\noptimal one rather than remaining fixed. Numerical simulations corroborate our\ntheory.\n  Technically, the combination of time-varying learning policies (which induce\nrapidly time-inhomogeneous Markovian noise) and the minimal assumption on\nexploration presents significant analytical challenges. To address these\nchallenges, we employ a refined approach that leverages the Poisson equation to\ndecompose the Markovian noise corresponding to the lazy transition matrix into\na martingale-difference term and residual terms. To control the residual terms\nunder time inhomogeneity, we perform a sensitivity analysis of the Poisson\nequation solution with respect to both the Q-function estimate and the learning\npolicy. These tools may further facilitate the analysis of general\nreinforcement learning algorithms with rapidly time-varying learning policies\n-- such as single-timescale actor--critic methods and learning-in-games\nalgorithms -- and are of independent interest.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5bf9\u65f6\u53d8\u5b66\u4e60\u7b56\u7565\u4e0b\u7684Q-learning\u7b97\u6cd5\u8fdb\u884c\u4e86\u6709\u9650\u65f6\u95f4\u5206\u6790\uff0c\u4ec5\u9700\u9a6c\u5c14\u53ef\u592b\u94fe\u4e0d\u53ef\u7ea6\u7684\u57fa\u672c\u5047\u8bbe\uff0c\u8bc1\u660e\u4e86\u671f\u671b\u65e0\u7a77\u8303\u6570\u5e73\u65b9\u7684\u6536\u655b\u901f\u7387\uff0c\u6837\u672c\u590d\u6742\u5ea6\u4e3aO(1/\u03b5\u00b2)\uff0c\u4e0e\u79bb\u7b56\u7565Q-learning\u5339\u914d\u4f46\u5728\u63a2\u7d22\u53c2\u6570\u4e0a\u4f9d\u8d56\u66f4\u5dee\u3002", "motivation": "\u73b0\u6709Q-learning\u5206\u6790\u5927\u591a\u57fa\u4e8e\u56fa\u5b9a\u91c7\u6837\u7b56\u7565\uff08\u79bb\u7b56\u7565\uff09\uff0c\u800c\u5b9e\u9645\u5e94\u7528\u4e2d\u5b66\u4e60\u7b56\u7565\u662f\u65f6\u53d8\u7684\uff08\u5728\u7b56\u7565\uff09\uff0c\u8fd9\u79cd\u65f6\u53d8\u7b56\u7565\u5e26\u6765\u7684\u5feb\u901f\u65f6\u975e\u9f50\u6b21\u9a6c\u5c14\u53ef\u592b\u566a\u58f0\u7ed9\u5206\u6790\u5e26\u6765\u91cd\u5927\u6311\u6218\u3002", "method": "\u91c7\u7528\u6539\u8fdb\u65b9\u6cd5\uff0c\u5229\u7528\u6cca\u677e\u65b9\u7a0b\u5c06\u5bf9\u5e94\u60f0\u6027\u8f6c\u79fb\u77e9\u9635\u7684\u9a6c\u5c14\u53ef\u592b\u566a\u58f0\u5206\u89e3\u4e3a\u9785\u5dee\u9879\u548c\u6b8b\u5dee\u9879\uff0c\u901a\u8fc7\u6cca\u677e\u65b9\u7a0b\u89e3\u5bf9Q\u51fd\u6570\u4f30\u8ba1\u548c\u5b66\u4e60\u7b56\u7565\u7684\u654f\u611f\u6027\u5206\u6790\u6765\u63a7\u5236\u65f6\u975e\u9f50\u6b21\u6027\u4e0b\u7684\u6b8b\u5dee\u9879\u3002", "result": "\u5efa\u7acb\u4e86\u671f\u671b\u65e0\u7a77\u8303\u6570\u5e73\u65b9\u7684\u6700\u540e\u8fed\u4ee3\u6536\u655b\u901f\u7387\uff0c\u6837\u672c\u590d\u6742\u5ea6\u4e3aO(1/\u03b5\u00b2)\uff0c\u53d1\u73b0\u5728\u7b56\u7565Q-learning\u63a2\u7d22\u80fd\u529b\u8f83\u5f31\u4f46\u5177\u6709\u5229\u7528\u4f18\u52bf\uff0c\u5176\u7b56\u7565\u4f1a\u6536\u655b\u5230\u6700\u4f18\u7b56\u7565\u800c\u975e\u4fdd\u6301\u56fa\u5b9a\u3002", "conclusion": "\u5728\u7b56\u7565Q-learning\u5728\u63a2\u7d22\u4e0a\u5f31\u4e8e\u79bb\u7b56\u7565\u7248\u672c\u4f46\u5728\u5229\u7528\u4e0a\u6709\u4f18\u52bf\uff0c\u6240\u53d1\u5c55\u7684\u5206\u6790\u5de5\u5177\u53ef\u8fdb\u4e00\u6b65\u7528\u4e8e\u5206\u6790\u5177\u6709\u5feb\u901f\u65f6\u53d8\u5b66\u4e60\u7b56\u7565\u7684\u4e00\u822c\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u3002"}}
{"id": "2510.17241", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17241", "abs": "https://arxiv.org/abs/2510.17241", "authors": ["Stefania Ionescu", "Robin Forsberg", "Elsa Lichtenegger", "Salima Jaoua", "Kshitijaa Jaglan", "Florian Dorfler", "Aniko Hannak"], "title": "Visibility Allocation Systems: How Algorithmic Design Shapes Online Visibility and Societal Outcomes", "comment": null, "summary": "Throughout application domains, we now rely extensively on algorithmic\nsystems to engage with ever-expanding datasets of information. Despite their\nbenefits, these systems are often complex (comprising of many intricate tools,\ne.g., moderation, recommender systems, prediction models), of unknown structure\n(due to the lack of accompanying documentation), and having hard-to-predict yet\npotentially severe downstream consequences (due to the extensive use,\nsystematic enactment of existing errors, and many comprising feedback loops).\nAs such, understanding and evaluating these systems as a whole remains a\nchallenge for both researchers and legislators. To aid ongoing efforts, we\nintroduce a formal framework for such visibility allocation systems (VASs)\nwhich we define as (semi-)automated systems deciding which (processed) data to\npresent a human user with. We review typical tools comprising VASs and define\nthe associated computational problems they solve. By doing so, VASs can be\ndecomposed into sub-processes and illustrated via data flow diagrams. Moreover,\nwe survey metrics for evaluating VASs throughout the pipeline, thus aiding\nsystem diagnostics. Using forecasting-based recommendations in school choice as\na case study, we demonstrate how our framework can support VAS evaluation. We\nalso discuss how our framework can support ongoing AI-legislative efforts to\nlocate obligations, quantify systemic risks, and enable adaptive compliance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u5206\u6790\u53ef\u89c1\u6027\u5206\u914d\u7cfb\u7edf(VAS)\u7684\u5f62\u5f0f\u5316\u6846\u67b6\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u51b3\u5b9a\u5411\u7528\u6237\u5c55\u793a\u54ea\u4e9b(\u5904\u7406\u8fc7\u7684)\u6570\u636e\uff0c\u5e2e\u52a9\u5206\u89e3\u7cfb\u7edf\u5b50\u6d41\u7a0b\u5e76\u652f\u6301\u7cfb\u7edf\u8bc4\u4f30\u3002", "motivation": "\u5f53\u524d\u7b97\u6cd5\u7cfb\u7edf\u590d\u6742\u3001\u7ed3\u6784\u4e0d\u660e\u786e\u4e14\u53ef\u80fd\u4ea7\u751f\u4e25\u91cd\u540e\u679c\uff0c\u4f46\u7406\u89e3\u548c\u8bc4\u4f30\u8fd9\u4e9b\u7cfb\u7edf\u5bf9\u7814\u7a76\u4eba\u5458\u548c\u7acb\u6cd5\u8005\u6765\u8bf4\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u5f15\u5165\u5f62\u5f0f\u5316\u6846\u67b6\u6765\u5b9a\u4e49VAS\uff0c\u56de\u987e\u5178\u578b\u5de5\u5177\u5e76\u5b9a\u4e49\u76f8\u5173\u8ba1\u7b97\u95ee\u9898\uff0c\u901a\u8fc7\u6570\u636e\u6d41\u56fe\u5206\u89e3\u7cfb\u7edf\u5b50\u6d41\u7a0b\uff0c\u5e76\u8c03\u67e5\u6574\u4e2a\u6d41\u7a0b\u4e2d\u7684\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u901a\u8fc7\u5b66\u6821\u9009\u62e9\u4e2d\u7684\u9884\u6d4b\u63a8\u8350\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u8be5\u6846\u67b6\u5982\u4f55\u652f\u6301VAS\u8bc4\u4f30\uff0c\u5e76\u8ba8\u8bba\u4e86\u5982\u4f55\u652f\u6301AI\u7acb\u6cd5\u5de5\u4f5c\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u52a9\u4e8e\u5b9a\u4f4d\u4e49\u52a1\u3001\u91cf\u5316\u7cfb\u7edf\u6027\u98ce\u9669\u5e76\u5b9e\u73b0\u9002\u5e94\u6027\u5408\u89c4\uff0c\u4e3aVAS\u7684\u900f\u660e\u5ea6\u548c\u53ef\u8bc4\u4f30\u6027\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2510.16368", "categories": ["cs.AI", "cs.HC", "cs.LG", "econ.TH"], "pdf": "https://arxiv.org/pdf/2510.16368", "abs": "https://arxiv.org/abs/2510.16368", "authors": ["Ali Shirali"], "title": "The Burden of Interactive Alignment with Inconsistent Preferences", "comment": "Published as a conference paper at NeurIPS 2025", "summary": "From media platforms to chatbots, algorithms shape how people interact,\nlearn, and discover information. Such interactions between users and an\nalgorithm often unfold over multiple steps, during which strategic users can\nguide the algorithm to better align with their true interests by selectively\nengaging with content. However, users frequently exhibit inconsistent\npreferences: they may spend considerable time on content that offers little\nlong-term value, inadvertently signaling that such content is desirable.\nFocusing on the user side, this raises a key question: what does it take for\nsuch users to align the algorithm with their true interests?\n  To investigate these dynamics, we model the user's decision process as split\nbetween a rational system 2 that decides whether to engage and an impulsive\nsystem 1 that determines how long engagement lasts. We then study a\nmulti-leader, single-follower extensive Stackelberg game, where users,\nspecifically system 2, lead by committing to engagement strategies and the\nalgorithm best-responds based on observed interactions. We define the burden of\nalignment as the minimum horizon over which users must optimize to effectively\nsteer the algorithm. We show that a critical horizon exists: users who are\nsufficiently foresighted can achieve alignment, while those who are not are\ninstead aligned to the algorithm's objective. This critical horizon can be\nlong, imposing a substantial burden. However, even a small, costly signal\n(e.g., an extra click) can significantly reduce it. Overall, our framework\nexplains how users with inconsistent preferences can align an engagement-driven\nalgorithm with their interests in a Stackelberg equilibrium, highlighting both\nthe challenges and potential remedies for achieving alignment.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u7528\u6237\u5982\u4f55\u901a\u8fc7\u6218\u7565\u6027\u4e92\u52a8\u6765\u5f15\u5bfc\u7b97\u6cd5\u4e0e\u81ea\u8eab\u771f\u5b9e\u5174\u8da3\u5bf9\u9f50\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u53cc\u7cfb\u7edf\u51b3\u7b56\u6a21\u578b\u548cStackelberg\u535a\u5f08\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u7528\u6237\u9700\u8981\u8db3\u591f\u7684\u524d\u77bb\u6027\u624d\u80fd\u5b9e\u73b0\u7b97\u6cd5\u5bf9\u9f50\u3002", "motivation": "\u7814\u7a76\u7528\u6237\u4e0e\u7b97\u6cd5\u4e92\u52a8\u4e2d\u7684\u5bf9\u9f50\u95ee\u9898\uff0c\u7279\u522b\u662f\u5f53\u7528\u6237\u5b58\u5728\u4e0d\u4e00\u81f4\u504f\u597d\u65f6\uff08\u5982\u82b1\u8d39\u5927\u91cf\u65f6\u95f4\u5728\u4f4e\u4ef7\u503c\u5185\u5bb9\u4e0a\uff09\uff0c\u5982\u4f55\u8ba9\u7b97\u6cd5\u66f4\u597d\u5730\u53cd\u6620\u7528\u6237\u7684\u771f\u5b9e\u5174\u8da3\u3002", "method": "\u5c06\u7528\u6237\u51b3\u7b56\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u7406\u6027\u7cfb\u7edf2\uff08\u51b3\u5b9a\u662f\u5426\u53c2\u4e0e\uff09\u548c\u51b2\u52a8\u7cfb\u7edf1\uff08\u51b3\u5b9a\u53c2\u4e0e\u65f6\u957f\uff09\u7684\u53cc\u7cfb\u7edf\u6a21\u578b\uff0c\u5e76\u6784\u5efa\u591a\u9886\u5bfc\u8005\u5355\u8ddf\u968f\u8005\u7684Stackelberg\u6269\u5c55\u535a\u5f08\uff0c\u7528\u6237\u4f5c\u4e3a\u9886\u5bfc\u8005\u901a\u8fc7\u627f\u8bfa\u53c2\u4e0e\u7b56\u7565\u6765\u5f15\u5bfc\u7b97\u6cd5\u3002", "result": "\u53d1\u73b0\u5b58\u5728\u4e00\u4e2a\u5173\u952e\u5bf9\u9f50\u65f6\u95f4\u8303\u56f4\uff1a\u8db3\u591f\u6709\u8fdc\u89c1\u7684\u7528\u6237\u53ef\u4ee5\u5b9e\u73b0\u7b97\u6cd5\u5bf9\u9f50\uff0c\u800c\u77ed\u89c6\u7684\u7528\u6237\u53cd\u800c\u4f1a\u88ab\u7b97\u6cd5\u76ee\u6807\u6240\u5bf9\u9f50\u3002\u989d\u5916\u7684\u5c0f\u6210\u672c\u4fe1\u53f7\uff08\u5982\u989d\u5916\u70b9\u51fb\uff09\u53ef\u4ee5\u663e\u8457\u964d\u4f4e\u5bf9\u9f50\u8d1f\u62c5\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u89e3\u91ca\u4e0d\u4e00\u81f4\u504f\u597d\u7528\u6237\u5982\u4f55\u5728\u4e0e\u53c2\u4e0e\u9a71\u52a8\u7b97\u6cd5\u7684Stackelberg\u5747\u8861\u4e2d\u5b9e\u73b0\u5bf9\u9f50\u7684\u6846\u67b6\uff0c\u5f3a\u8c03\u4e86\u5b9e\u73b0\u5bf9\u9f50\u7684\u6311\u6218\u548c\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.17294", "categories": ["math.OC", "cs.CR", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.17294", "abs": "https://arxiv.org/abs/2510.17294", "authors": ["Sebastian Schlor", "Andrea Iannelli", "Junsoo Kim", "Hyungbo Shim", "Frank Allg\u00f6wer"], "title": "A polynomial-based QCQP solver for encrypted optimization", "comment": "Accepted for presentation at the 64th IEEE Conference on Decision and\n  Control (CDC2025)", "summary": "In this paper, we present a novel method for solving a class of quadratically\nconstrained quadratic optimization problems using only additions and\nmultiplications. This approach enables solving constrained optimization\nproblems on private data since the operations involved are compatible with the\ncapabilities of homomorphic encryption schemes. To solve the constrained\noptimization problem, a sequence of polynomial penalty functions of increasing\ndegree is introduced, which are sufficiently steep at the boundary of the\nfeasible set. Adding the penalty function to the original cost function creates\na sequence of unconstrained optimization problems whose minimizer always lies\nin the admissible set and converges to the minimizer of the constrained\nproblem. A gradient descent method is used to generate a sequence of iterates\nassociated with these problems. For the algorithm, it is shown that the iterate\nconverges to a minimizer of the original problem, and the feasible set is\npositively invariant under the iteration. Finally, the method is demonstrated\non an illustrative cryptographic problem, finding the smaller value of two\nnumbers, and the encrypted implementability is discussed.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u52a0\u6cd5\u548c\u4e58\u6cd5\u89e3\u51b3\u4e8c\u6b21\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4e0e\u540c\u6001\u52a0\u5bc6\u65b9\u6848\u517c\u5bb9\uff0c\u53ef\u5728\u79c1\u6709\u6570\u636e\u4e0a\u6c42\u89e3\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u3002", "motivation": "\u4e3a\u4e86\u5728\u79c1\u6709\u6570\u636e\u4e0a\u89e3\u51b3\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u4e0e\u540c\u6001\u52a0\u5bc6\u65b9\u6848\u517c\u5bb9\u7684\u7b97\u6cd5\uff0c\u56e0\u4e3a\u540c\u6001\u52a0\u5bc6\u4ec5\u652f\u6301\u52a0\u6cd5\u548c\u4e58\u6cd5\u64cd\u4f5c\u3002", "method": "\u5f15\u5165\u4e00\u7cfb\u5217\u9012\u589e\u6b21\u6570\u7684\u591a\u9879\u5f0f\u7f5a\u51fd\u6570\uff0c\u8fd9\u4e9b\u51fd\u6570\u5728\u53ef\u884c\u96c6\u8fb9\u754c\u8db3\u591f\u9661\u5ced\u3002\u5c06\u7f5a\u51fd\u6570\u6dfb\u52a0\u5230\u539f\u59cb\u6210\u672c\u51fd\u6570\u4e2d\uff0c\u521b\u5efa\u4e00\u7cfb\u5217\u65e0\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u5176\u6700\u5c0f\u5316\u5668\u59cb\u7ec8\u4f4d\u4e8e\u53ef\u884c\u96c6\u5185\u5e76\u6536\u655b\u5230\u7ea6\u675f\u95ee\u9898\u7684\u6700\u5c0f\u5316\u5668\u3002\u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u6cd5\u751f\u6210\u8fed\u4ee3\u5e8f\u5217\u3002", "result": "\u8bc1\u660e\u4e86\u8fed\u4ee3\u6536\u655b\u5230\u539f\u59cb\u95ee\u9898\u7684\u6700\u5c0f\u5316\u5668\uff0c\u4e14\u53ef\u884c\u96c6\u5728\u8fed\u4ee3\u4e0b\u662f\u6b63\u4e0d\u53d8\u7684\u3002\u65b9\u6cd5\u5728\u4e00\u4e2a\u52a0\u5bc6\u95ee\u9898\uff08\u5bfb\u627e\u4e24\u4e2a\u6570\u4e2d\u7684\u8f83\u5c0f\u503c\uff09\u4e0a\u8fdb\u884c\u4e86\u6f14\u793a\uff0c\u5e76\u8ba8\u8bba\u4e86\u52a0\u5bc6\u5b9e\u73b0\u7684\u53ef\u80fd\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u4e8c\u6b21\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u4e14\u4e0e\u540c\u6001\u52a0\u5bc6\u65b9\u6848\u517c\u5bb9\uff0c\u4e3a\u5728\u79c1\u6709\u6570\u636e\u4e0a\u6267\u884c\u7ea6\u675f\u4f18\u5316\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.15986", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15986", "abs": "https://arxiv.org/abs/2510.15986", "authors": ["Sifeddine Sellami", "Juba Agoun", "Lamia Yessad", "Louenas Bounia"], "title": "User Profiles of Sleep Disorder Sufferers: Towards Explainable Clustering and Differential Variable Analysis", "comment": "in French language, Plate-Forme Intelligence Artificielle, Jun 2025,\n  Dijon (FRANCE), France", "summary": "Sleep disorders have a major impact on patients' health and quality of life,\nbut their diagnosis remains complex due to the diversity of symptoms. Today,\ntechnological advances, combined with medical data analysis, are opening new\nperspectives for a better understanding of these disorders. In particular,\nexplainable artificial intelligence (XAI) aims to make AI model decisions\nunderstandable and interpretable for users. In this study, we propose a\nclustering-based method to group patients according to different sleep disorder\nprofiles. By integrating an explainable approach, we identify the key factors\ninfluencing these pathologies. An experiment on anonymized real data\nillustrates the effectiveness and relevance of our approach.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u805a\u7c7b\u7684\u53ef\u89e3\u91caAI\u65b9\u6cd5\uff0c\u7528\u4e8e\u6839\u636e\u7761\u7720\u969c\u788d\u7279\u5f81\u5bf9\u60a3\u8005\u8fdb\u884c\u5206\u7ec4\uff0c\u5e76\u8bc6\u522b\u5f71\u54cd\u75c5\u7406\u7684\u5173\u952e\u56e0\u7d20\u3002", "motivation": "\u7761\u7720\u969c\u788d\u5bf9\u60a3\u8005\u5065\u5eb7\u548c\u751f\u6d3b\u8d28\u91cf\u6709\u91cd\u5927\u5f71\u54cd\uff0c\u4f46\u7531\u4e8e\u75c7\u72b6\u591a\u6837\u6027\uff0c\u8bca\u65ad\u590d\u6742\u3002\u6280\u672f\u8fdb\u6b65\u548c\u533b\u7597\u6570\u636e\u5206\u6790\u4e3a\u66f4\u597d\u7406\u89e3\u8fd9\u4e9b\u969c\u788d\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u805a\u7c7b\u7684\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u65b9\u6cd5\uff0c\u5c06\u60a3\u8005\u6309\u4e0d\u540c\u7761\u7720\u969c\u788d\u7279\u5f81\u8fdb\u884c\u5206\u7ec4\uff0c\u5e76\u6574\u5408\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u8bc6\u522b\u5173\u952e\u5f71\u54cd\u56e0\u7d20\u3002", "result": "\u5728\u533f\u540d\u771f\u5b9e\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u76f8\u5173\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u8bc6\u522b\u7761\u7720\u969c\u788d\u60a3\u8005\u7684\u4e0d\u540c\u7279\u5f81\u7fa4\u7ec4\uff0c\u5e76\u901a\u8fc7\u53ef\u89e3\u91caAI\u63ed\u793a\u5f71\u54cd\u75c5\u7406\u7684\u5173\u952e\u56e0\u7d20\uff0c\u4e3a\u7761\u7720\u969c\u788d\u8bca\u65ad\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u9014\u5f84\u3002"}}
{"id": "2510.16492", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16492", "abs": "https://arxiv.org/abs/2510.16492", "authors": ["Vamshi Krishna Bonagiri", "Ponnurangam Kumaragurum", "Khanh Nguyen", "Benjamin Plaut"], "title": "Check Yourself Before You Wreck Yourself: Selectively Quitting Improves LLM Agent Safety", "comment": "Reliable ML and Regulatable ML workshops, Neurips 2025", "summary": "As Large Language Model (LLM) agents increasingly operate in complex\nenvironments with real-world consequences, their safety becomes critical. While\nuncertainty quantification is well-studied for single-turn tasks, multi-turn\nagentic scenarios with real-world tool access present unique challenges where\nuncertainties and ambiguities compound, leading to severe or catastrophic risks\nbeyond traditional text generation failures. We propose using \"quitting\" as a\nsimple yet effective behavioral mechanism for LLM agents to recognize and\nwithdraw from situations where they lack confidence. Leveraging the ToolEmu\nframework, we conduct a systematic evaluation of quitting behavior across 12\nstate-of-the-art LLMs. Our results demonstrate a highly favorable\nsafety-helpfulness trade-off: agents prompted to quit with explicit\ninstructions improve safety by an average of +0.39 on a 0-3 scale across all\nmodels (+0.64 for proprietary models), while maintaining a negligible average\ndecrease of -0.03 in helpfulness. Our analysis demonstrates that simply adding\nexplicit quit instructions proves to be a highly effective safety mechanism\nthat can immediately be deployed in existing agent systems, and establishes\nquitting as an effective first-line defense mechanism for autonomous agents in\nhigh-stakes applications.", "AI": {"tldr": "\u63d0\u51fa\u4f7f\u7528\"\u9000\u51fa\"\u4f5c\u4e3aLLM\u4ee3\u7406\u7684\u5b89\u5168\u673a\u5236\uff0c\u5f53\u4ee3\u7406\u7f3a\u4e4f\u4fe1\u5fc3\u65f6\u4e3b\u52a8\u9000\u51fa\uff0c\u572812\u4e2a\u5148\u8fdbLLM\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740LLM\u4ee3\u7406\u5728\u590d\u6742\u73af\u5883\u4e2d\u8fd0\u884c\uff0c\u5176\u5b89\u5168\u6027\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u591a\u8f6e\u4ee3\u7406\u573a\u666f\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u6a21\u7cca\u6027\u4f1a\u7d2f\u79ef\uff0c\u5bfc\u81f4\u8d85\u51fa\u4f20\u7edf\u6587\u672c\u751f\u6210\u5931\u8d25\u7684\u4e25\u91cd\u98ce\u9669\u3002", "method": "\u5229\u7528ToolEmu\u6846\u67b6\uff0c\u572812\u4e2a\u5148\u8fdbLLM\u4e0a\u7cfb\u7edf\u8bc4\u4f30\u9000\u51fa\u884c\u4e3a\uff0c\u901a\u8fc7\u6dfb\u52a0\u660e\u786e\u7684\u9000\u51fa\u6307\u4ee4\u6765\u8ba9\u4ee3\u7406\u5728\u7f3a\u4e4f\u4fe1\u5fc3\u65f6\u4e3b\u52a8\u9000\u51fa\u3002", "result": "\u7ed3\u679c\u663e\u793a\u5b89\u5168-\u5e2e\u52a9\u6027\u6743\u8861\u975e\u5e38\u6709\u5229\uff1a\u6240\u6709\u6a21\u578b\u5b89\u5168\u6027\u5e73\u5747\u63d0\u9ad8+0.39\uff080-3\u5206\u5236\uff09\uff0c\u4e13\u6709\u6a21\u578b\u63d0\u9ad8+0.64\uff0c\u800c\u5e2e\u52a9\u6027\u4ec5\u5e73\u5747\u4e0b\u964d-0.03\u3002", "conclusion": "\u6dfb\u52a0\u660e\u786e\u7684\u9000\u51fa\u6307\u4ee4\u662f\u4e00\u79cd\u9ad8\u5ea6\u6709\u6548\u7684\u5b89\u5168\u673a\u5236\uff0c\u53ef\u4f5c\u4e3a\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u81ea\u4e3b\u4ee3\u7406\u7684\u7b2c\u4e00\u9053\u9632\u7ebf\uff0c\u53ef\u7acb\u5373\u90e8\u7f72\u5230\u73b0\u6709\u4ee3\u7406\u7cfb\u7edf\u4e2d\u3002"}}
{"id": "2510.17769", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.17769", "abs": "https://arxiv.org/abs/2510.17769", "authors": ["Michael Nestor", "Jiaxin Wang", "Ning Zhang", "Fei Teng"], "title": "Data-driven Communication and Control Design for Distributed Frequency Regulation with Black-box Inverters", "comment": "Preprint submitted to PSCC 2026", "summary": "The increasing penetration of inverter-based resources into the power grid,\nwith often only black-box models available, challenges long-standing frequency\ncontrol methods. Most recent works take a decentralized approach without online\ndevice coordination via communication. This paper considers both dynamic\nbehavior and communication within secondary frequency control on an\nintermediate timescale. We develop a distributed data-driven approach that\nutilizes peer-to-peer communication between inverters to avoid the need for a\ncentral control center. To enable a trade off between communication network\nrequirements and control performance, we present a framework to guide\ncommunication topology design for secondary frequency regulation. Following\ndesign of the inter-agent information exchange scheme, we design a controller\nthat is structured according to the communication topology with a closed-loop\nstability guarantee. Case studies on the IEEE 39-bus system validate the\nframework and illustrate the trade-off between communication requirements and\ncontrol performance that is enabled by our approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u9006\u53d8\u5668\u4e8c\u6b21\u9891\u7387\u63a7\u5236\u7684\u5206\u5e03\u5f0f\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u901a\u8fc7\u70b9\u5bf9\u70b9\u901a\u4fe1\u907f\u514d\u4e2d\u592e\u63a7\u5236\u4e2d\u5fc3\uff0c\u5e76\u63d0\u4f9b\u4e86\u901a\u4fe1\u62d3\u6251\u8bbe\u8ba1\u6846\u67b6\u4ee5\u5e73\u8861\u901a\u4fe1\u9700\u6c42\u4e0e\u63a7\u5236\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u57fa\u4e8e\u9006\u53d8\u5668\u7684\u8d44\u6e90\u5728\u7535\u7f51\u4e2d\u6e17\u900f\u7387\u589e\u52a0\uff0c\u4e14\u901a\u5e38\u53ea\u6709\u9ed1\u76d2\u6a21\u578b\u53ef\u7528\uff0c\u8fd9\u6311\u6218\u4e86\u4f20\u7edf\u7684\u9891\u7387\u63a7\u5236\u65b9\u6cd5\u3002\u73b0\u6709\u5de5\u4f5c\u5927\u591a\u91c7\u7528\u53bb\u4e2d\u5fc3\u5316\u65b9\u6cd5\uff0c\u7f3a\u4e4f\u901a\u8fc7\u901a\u4fe1\u7684\u5728\u7ebf\u8bbe\u5907\u534f\u8c03\u3002", "method": "\u5f00\u53d1\u4e86\u5206\u5e03\u5f0f\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u5229\u7528\u9006\u53d8\u5668\u95f4\u7684\u70b9\u5bf9\u70b9\u901a\u4fe1\uff1b\u63d0\u51fa\u4e86\u901a\u4fe1\u62d3\u6251\u8bbe\u8ba1\u6846\u67b6\u6765\u6307\u5bfc\u4e8c\u6b21\u9891\u7387\u8c03\u8282\uff1b\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u901a\u4fe1\u62d3\u6251\u7ed3\u6784\u7684\u63a7\u5236\u5668\uff0c\u5e76\u4fdd\u8bc1\u95ed\u73af\u7a33\u5b9a\u6027\u3002", "result": "\u5728IEEE 39\u603b\u7ebf\u7cfb\u7edf\u4e0a\u7684\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\uff0c\u5e76\u5c55\u793a\u4e86\u901a\u4fe1\u9700\u6c42\u4e0e\u63a7\u5236\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5206\u5e03\u5f0f\u63a7\u5236\u548c\u901a\u4fe1\u62d3\u6251\u4f18\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9006\u53d8\u5668\u4e8c\u6b21\u9891\u7387\u63a7\u5236\u95ee\u9898\uff0c\u5728\u4fdd\u8bc1\u7a33\u5b9a\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u901a\u4fe1\u9700\u6c42\u4e0e\u63a7\u5236\u6027\u80fd\u7684\u5e73\u8861\u3002"}}
{"id": "2510.16138", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16138", "abs": "https://arxiv.org/abs/2510.16138", "authors": ["Dung V. Nguyen", "Anh T. Nguyen", "Minh H. Nguyen", "Luc Q. Nguyen", "Shiqi Jiang", "Ethan Fetaya", "Linh Duy Tran", "Gal Chechik", "Tan M. Nguyen"], "title": "Expert Merging in Sparse Mixture of Experts with Nash Bargaining", "comment": "10 pages in the main text. Under Review", "summary": "Existing expert merging strategies for Sparse Mixture of Experts (SMoE)\ntypically rely on input-dependent or input-independent averaging of expert\nparameters, but often lack a principled weighting mechanism. In this work, we\nreinterpret expert merging through the lens of game theory, revealing\ncooperative and competitive dynamics among experts. Based on this perspective,\nwe introduce Nash Merging of Experts (NAMEx), a novel framework that\nincorporates Nash Bargaining into the merging process, enabling more balanced\nand efficient collaboration among experts. Additionally, we incorporate complex\nmomentum into NAMEx to accelerate expert propagation with theoretical\nguarantees for convergence. Extensive experiments across language modelling,\ntext classification, image classification, and zero-shot robustness under data\ncorruption show that NAMEx consistently outperforms competing methods while\nintegrating seamlessly with popular MoE architectures. Finally, we demonstrate\nNAMEx's scalability by applying it to large-scale systems, including\nQwen1.5-MoE (14B) and DeepSeek-MoE (16B), where it proves effective in both\nzero-shot and fine-tuning settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86NAMEx\u6846\u67b6\uff0c\u5c06\u7eb3\u4ec0\u8bae\u4ef7\u5f15\u5165\u4e13\u5bb6\u5408\u5e76\u8fc7\u7a0b\uff0c\u901a\u8fc7\u535a\u5f08\u8bba\u89c6\u89d2\u5b9e\u73b0\u66f4\u5e73\u8861\u9ad8\u6548\u7684\u4e13\u5bb6\u534f\u4f5c\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7a00\u758f\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\u7684\u4e13\u5bb6\u5408\u5e76\u7b56\u7565\u7f3a\u4e4f\u539f\u5219\u6027\u7684\u52a0\u6743\u673a\u5236\uff0c\u9700\u8981\u66f4\u5e73\u8861\u9ad8\u6548\u7684\u4e13\u5bb6\u534f\u4f5c\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u535a\u5f08\u8bba\u89c6\u89d2\uff0c\u5f15\u5165\u7eb3\u4ec0\u8bae\u4ef7\u5230\u4e13\u5bb6\u5408\u5e76\u8fc7\u7a0b\uff0c\u5e76\u52a0\u5165\u590d\u52a8\u91cf\u6765\u52a0\u901f\u4e13\u5bb6\u4f20\u64ad\uff0c\u63d0\u4f9b\u7406\u8bba\u6536\u655b\u4fdd\u8bc1\u3002", "result": "\u5728\u8bed\u8a00\u5efa\u6a21\u3001\u6587\u672c\u5206\u7c7b\u3001\u56fe\u50cf\u5206\u7c7b\u548c\u6570\u636e\u635f\u574f\u4e0b\u7684\u96f6\u6837\u672c\u9c81\u68d2\u6027\u7b49\u4efb\u52a1\u4e2d\uff0cNAMEx\u59cb\u7ec8\u4f18\u4e8e\u7ade\u4e89\u65b9\u6cd5\uff0c\u5e76\u80fd\u65e0\u7f1d\u96c6\u6210\u5230\u6d41\u884c\u7684MoE\u67b6\u6784\u4e2d\u3002", "conclusion": "NAMEx\u5728\u5927\u89c4\u6a21\u7cfb\u7edf\u4e2d\uff08\u5982Qwen1.5-MoE\u548cDeepSeek-MoE\uff09\u8868\u73b0\u51fa\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\uff0c\u5728\u96f6\u6837\u672c\u548c\u5fae\u8c03\u8bbe\u7f6e\u4e2d\u90fd\u6709\u6548\u3002"}}
{"id": "2510.17425", "categories": ["cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17425", "abs": "https://arxiv.org/abs/2510.17425", "authors": ["Aditi Dutta"], "title": "Quantifying Climate Policy Action and Its Links to Development Outcomes: A Cross-National Data-Driven Analysis", "comment": "This paper/proposal has been accepted as a poster in the NeurIPS 2025", "summary": "Addressing climate change effectively requires more than cataloguing the\nnumber of policies in place; it calls for tools that can reveal their thematic\npriorities and their tangible impacts on development outcomes. Existing\nassessments often rely on qualitative descriptions or composite indices, which\ncan mask crucial differences between key domains such as mitigation,\nadaptation, disaster risk management, and loss and damage. To bridge this gap,\nwe develop a quantitative indicator of climate policy orientation by applying a\nmultilingual transformer-based language model to official national policy\ndocuments, achieving a classification accuracy of 0.90 (F1-score). Linking\nthese indicators with World Bank development data in panel regressions reveals\nthat mitigation policies are associated with higher GDP and GNI; disaster risk\nmanagement correlates with greater GNI and debt but reduced foreign direct\ninvestment; adaptation and loss and damage show limited measurable effects.\nThis integrated NLP-econometric framework enables comparable, theme-specific\nanalysis of climate governance, offering a scalable method to monitor progress,\nevaluate trade-offs, and align policy emphasis with development goals.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u591a\u8bed\u8a00transformer\u7684\u8bed\u8a00\u6a21\u578b\u6765\u91cf\u5316\u5206\u6790\u6c14\u5019\u653f\u7b56\u5bfc\u5411\uff0c\u5c06\u653f\u7b56\u6587\u6863\u5206\u7c7b\u4e3a\u51cf\u7f13\u3001\u9002\u5e94\u3001\u707e\u5bb3\u98ce\u9669\u7ba1\u7406\u548c\u635f\u5931\u635f\u5bb3\u56db\u4e2a\u9886\u57df\uff0c\u51c6\u786e\u7387\u8fbe0.90 F1\u5206\u6570\uff0c\u5e76\u4e0e\u53d1\u5c55\u6570\u636e\u5173\u8054\u5206\u6790\u653f\u7b56\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u6c14\u5019\u653f\u7b56\u8bc4\u4f30\u591a\u4f9d\u8d56\u5b9a\u6027\u63cf\u8ff0\u6216\u7efc\u5408\u6307\u6570\uff0c\u96be\u4ee5\u63ed\u793a\u4e0d\u540c\u653f\u7b56\u9886\u57df\uff08\u51cf\u7f13\u3001\u9002\u5e94\u3001\u707e\u5bb3\u98ce\u9669\u7ba1\u7406\u3001\u635f\u5931\u635f\u5bb3\uff09\u7684\u5177\u4f53\u5dee\u5f02\u548c\u5b9e\u9645\u53d1\u5c55\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u591a\u8bed\u8a00transformer\u8bed\u8a00\u6a21\u578b\u5206\u6790\u5b98\u65b9\u56fd\u5bb6\u653f\u7b56\u6587\u4ef6\uff0c\u6784\u5efa\u6c14\u5019\u653f\u7b56\u5bfc\u5411\u7684\u91cf\u5316\u6307\u6807\uff0c\u5e76\u901a\u8fc7\u9762\u677f\u56de\u5f52\u4e0e\u4e16\u884c\u53d1\u5c55\u6570\u636e\u5173\u8054\u3002", "result": "\u51cf\u7f13\u653f\u7b56\u4e0e\u66f4\u9ad8GDP\u548cGNI\u76f8\u5173\uff1b\u707e\u5bb3\u98ce\u9669\u7ba1\u7406\u4e0e\u66f4\u5927GNI\u548c\u503a\u52a1\u76f8\u5173\u4f46\u51cf\u5c11\u5916\u56fd\u76f4\u63a5\u6295\u8d44\uff1b\u9002\u5e94\u548c\u635f\u5931\u635f\u5bb3\u653f\u7b56\u6548\u679c\u6709\u9650\u3002", "conclusion": "\u8be5NLP-\u8ba1\u91cf\u7ecf\u6d4e\u5b66\u6846\u67b6\u63d0\u4f9b\u4e86\u53ef\u6bd4\u8f83\u7684\u3001\u4e3b\u9898\u7279\u5b9a\u7684\u6c14\u5019\u6cbb\u7406\u5206\u6790\u65b9\u6cd5\uff0c\u4e3a\u76d1\u6d4b\u8fdb\u5c55\u3001\u8bc4\u4f30\u6743\u8861\u548c\u8c03\u6574\u653f\u7b56\u91cd\u70b9\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u3002"}}
{"id": "2510.16374", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16374", "abs": "https://arxiv.org/abs/2510.16374", "authors": ["Nick Oh"], "title": "Before you <think>, monitor: Implementing Flavell's metacognitive framework in LLMs", "comment": "Presented at the Workshop on the Application of LLM Explainability to\n  Reasoning and Planning at COLM 2025 (non-archival)", "summary": "Current approaches to enhancing LLM reasoning follows two isolated paradigms:\nMonitor-Generate methods like Plan-and-Solve (Wang et al., 2023) and\nSELF-DISCOVER (Zhou et al., 2024) excel at strategic planning but lack\nmechanisms to verify whether selected strategies succeed; while Generate-Verify\napproaches like Self-Verification (Weng et al., 2022) and SELF-REFINE (Madaan\net al., 2023) iteratively refine outputs but commence generation blindly\nwithout task assessment. This separation creates inefficiencies -- strategies\nfail without feedback, and refinement occurs without strategic grounding. We\naddress this gap by implementing Flavell's cognitive monitoring model (1979)\nfrom the broader Monitor-Generate-Verify framework (Oh and Gobet, 2025),\noperationalising it as a three-phase iterative system. On GSM8K, preliminary\nresults show 75.42% accuracy versus 68.44% for SELF-REFINE and 67.07% for\nSelf-Verification, while requiring fewer attempts (1.3 vs 2.0) at 27-37%\nincreased inference cost. These initial findings suggest upfront monitoring\nproduces higher-quality initial solutions that reduce refinement needs, though\nevaluation beyond arithmetic reasoning is needed to establish generalisability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u76d1\u63a7-\u751f\u6210-\u9a8c\u8bc1\u7684\u4e09\u9636\u6bb5\u8fed\u4ee3\u7cfb\u7edf\uff0c\u5728GSM8K\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e8675.42%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u4e14\u9700\u8981\u66f4\u5c11\u7684\u5c1d\u8bd5\u6b21\u6570\u3002", "motivation": "\u5f53\u524dLLM\u63a8\u7406\u589e\u5f3a\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u5b64\u7acb\u8303\u5f0f\uff1a\u76d1\u63a7-\u751f\u6210\u65b9\u6cd5\u64c5\u957f\u7b56\u7565\u89c4\u5212\u4f46\u7f3a\u4e4f\u9a8c\u8bc1\u673a\u5236\uff0c\u751f\u6210-\u9a8c\u8bc1\u65b9\u6cd5\u80fd\u8fed\u4ee3\u4f18\u5316\u4f46\u7f3a\u4e4f\u4efb\u52a1\u8bc4\u4f30\u3002\u8fd9\u79cd\u5206\u79bb\u5bfc\u81f4\u7b56\u7565\u5931\u8d25\u65e0\u53cd\u9988\u3001\u4f18\u5316\u65e0\u7b56\u7565\u57fa\u7840\u7684\u4f4e\u6548\u95ee\u9898\u3002", "method": "\u57fa\u4e8eFlavell\u7684\u8ba4\u77e5\u76d1\u63a7\u6a21\u578b\uff0c\u5b9e\u73b0\u76d1\u63a7-\u751f\u6210-\u9a8c\u8bc1\u6846\u67b6\u7684\u4e09\u9636\u6bb5\u8fed\u4ee3\u7cfb\u7edf\uff0c\u5728\u751f\u6210\u524d\u8fdb\u884c\u4efb\u52a1\u8bc4\u4f30\u548c\u7b56\u7565\u89c4\u5212\u3002", "result": "\u5728GSM8K\u4e0a\u8fbe\u523075.42%\u51c6\u786e\u7387\uff0c\u4f18\u4e8eSELF-REFINE(68.44%)\u548cSelf-Verification(67.07%)\uff0c\u5c1d\u8bd5\u6b21\u6570\u66f4\u5c11(1.3 vs 2.0)\uff0c\u63a8\u7406\u6210\u672c\u589e\u52a027-37%\u3002", "conclusion": "\u524d\u671f\u76d1\u63a7\u80fd\u4ea7\u751f\u66f4\u9ad8\u8d28\u91cf\u7684\u521d\u59cb\u89e3\u51b3\u65b9\u6848\uff0c\u51cf\u5c11\u4f18\u5316\u9700\u6c42\uff0c\u4f46\u9700\u8981\u5728\u7b97\u672f\u63a8\u7406\u4e4b\u5916\u7684\u4efb\u52a1\u4e0a\u8fdb\u884c\u8bc4\u4f30\u4ee5\u9a8c\u8bc1\u901a\u7528\u6027\u3002"}}
{"id": "2510.17339", "categories": ["math.OC", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.17339", "abs": "https://arxiv.org/abs/2510.17339", "authors": ["Filip Be\u010danovi\u0107", "Jared Miller", "Vincent Bonnet", "Kosta Jovanovi\u0107", "Samer Mohammed"], "title": "Assessing the Quality of a Set of Basis Functions for Inverse Optimal Control via Projection onto Global Minimizers", "comment": "8 pages, 4 figures", "summary": "Inverse optimization (Inverse optimal control) is the task of imputing a cost\nfunction such that given test points (trajectories) are (nearly) optimal with\nrespect to the discovered cost. Prior methods in inverse optimization assume\nthat the true cost is a convex combination of a set of convex basis functions\nand that this basis is consistent with the test points. However, the\nconsistency assumption is not always justified, as in many applications the\nprinciples by which the data is generated are not well understood. This work\nproposes using the distance between a test point and the set of global optima\ngenerated by the convex combinations of the convex basis functions as a\nmeasurement for the expressive quality of the basis with respect to the test\npoint. A large minimal distance invalidates the set of basis functions. The\nconcept of a set of global optima is introduced and its properties are explored\nin unconstrained and constrained settings. Upper and lower bounds for the\nminimum distance in the convex quadratic setting are implemented by bi-level\ngradient descent and an enriched linear matrix inequality respectively.\nExtensions to this framework include max-representable basis functions,\nnonconvex basis functions (local minima), and applying polynomial optimization\ntechniques.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u9006\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d4b\u91cf\u6d4b\u8bd5\u70b9\u4e0e\u51f8\u57fa\u51fd\u6570\u7ec4\u5408\u751f\u6210\u7684\u5168\u5c40\u6700\u4f18\u89e3\u96c6\u4e4b\u95f4\u7684\u8ddd\u79bb\u6765\u8bc4\u4f30\u57fa\u51fd\u6570\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u57fa\u51fd\u6570\u4e0e\u6d4b\u8bd5\u70b9\u4e00\u81f4\u6027\u5047\u8bbe\u4e0d\u6210\u7acb\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u9006\u4f18\u5316\u65b9\u6cd5\u5047\u8bbe\u771f\u5b9e\u6210\u672c\u51fd\u6570\u662f\u51f8\u57fa\u51fd\u6570\u7684\u51f8\u7ec4\u5408\uff0c\u4e14\u57fa\u51fd\u6570\u4e0e\u6d4b\u8bd5\u70b9\u4e00\u81f4\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4e2d\u8fd9\u79cd\u4e00\u81f4\u6027\u5047\u8bbe\u5f80\u5f80\u4e0d\u6210\u7acb\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u8bc4\u4f30\u57fa\u51fd\u6570\u5bf9\u6d4b\u8bd5\u70b9\u7684\u8868\u8fbe\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u6d4b\u8bd5\u70b9\u4e0e\u51f8\u57fa\u51fd\u6570\u7ec4\u5408\u751f\u6210\u7684\u5168\u5c40\u6700\u4f18\u89e3\u96c6\u4e4b\u95f4\u7684\u8ddd\u79bb\u4f5c\u4e3a\u57fa\u51fd\u6570\u8868\u8fbe\u80fd\u529b\u7684\u5ea6\u91cf\u3002\u5728\u51f8\u4e8c\u6b21\u8bbe\u7f6e\u4e2d\uff0c\u901a\u8fc7\u53cc\u5c42\u68af\u5ea6\u4e0b\u964d\u548c\u589e\u5f3a\u7ebf\u6027\u77e9\u9635\u4e0d\u7b49\u5f0f\u5206\u522b\u5b9e\u73b0\u6700\u5c0f\u8ddd\u79bb\u7684\u4e0a\u4e0b\u754c\u8ba1\u7b97\u3002", "result": "\u63d0\u51fa\u4e86\u5168\u5c40\u6700\u4f18\u89e3\u96c6\u7684\u6982\u5ff5\u5e76\u63a2\u7d22\u4e86\u5176\u5728\u65e0\u7ea6\u675f\u548c\u7ea6\u675f\u8bbe\u7f6e\u4e0b\u7684\u6027\u8d28\u3002\u5efa\u7acb\u4e86\u6700\u5c0f\u8ddd\u79bb\u7684\u4e0a\u4e0b\u754c\u8ba1\u7b97\u65b9\u6cd5\uff0c\u5e76\u6269\u5c55\u5230\u6700\u5927\u53ef\u8868\u793a\u57fa\u51fd\u6570\u3001\u975e\u51f8\u57fa\u51fd\u6570\uff08\u5c40\u90e8\u6700\u5c0f\u503c\uff09\u4ee5\u53ca\u591a\u9879\u5f0f\u4f18\u5316\u6280\u672f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9006\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u8bc4\u4f30\u57fa\u51fd\u6570\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u5f53\u6700\u5c0f\u8ddd\u79bb\u8f83\u5927\u65f6\u8868\u660e\u57fa\u51fd\u6570\u96c6\u65e0\u6548\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u4e00\u81f4\u6027\u5047\u8bbe\u4e0d\u6210\u7acb\u7684\u95ee\u9898\u3002"}}
{"id": "2510.15987", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15987", "abs": "https://arxiv.org/abs/2510.15987", "authors": ["Samuel Lippl", "Thomas McGee", "Kimberly Lopez", "Ziwen Pan", "Pierce Zhang", "Salma Ziadi", "Oliver Eberle", "Ida Momennejad"], "title": "Algorithmic Primitives and Compositional Geometry of Reasoning in Language Models", "comment": null, "summary": "How do latent and inference time computations enable large language models\n(LLMs) to solve multi-step reasoning? We introduce a framework for tracing and\nsteering algorithmic primitives that underlie model reasoning. Our approach\nlinks reasoning traces to internal activation patterns and evaluates\nalgorithmic primitives by injecting them into residual streams and measuring\ntheir effect on reasoning steps and task performance. We consider four\nbenchmarks: Traveling Salesperson Problem (TSP), 3SAT, AIME, and graph\nnavigation. We operationalize primitives by clustering neural activations and\nlabeling their matched reasoning traces. We then apply function vector methods\nto derive primitive vectors as reusable compositional building blocks of\nreasoning. Primitive vectors can be combined through addition, subtraction, and\nscalar operations, revealing a geometric logic in activation space. Cross-task\nand cross-model evaluations (Phi-4, Phi-4-Reasoning, Llama-3-8B) show both\nshared and task-specific primitives. Notably, comparing Phi-4 with its\nreasoning-finetuned variant highlights compositional generalization after\nfinetuning: Phi-4-Reasoning exhibits more systematic use of verification and\npath-generation primitives. Injecting the associated primitive vectors in\nPhi-4-Base induces behavioral hallmarks associated with Phi-4-Reasoning.\nTogether, these findings demonstrate that reasoning in LLMs may be supported by\na compositional geometry of algorithmic primitives, that primitives transfer\ncross-task and cross-model, and that reasoning finetuning strengthens\nalgorithmic generalization across domains.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\u6765\u8ffd\u8e2a\u548c\u64cd\u63a7\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u7b97\u6cd5\u539f\u8bed\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u63a8\u7406\u80cc\u540e\u7684\u51e0\u4f55\u903b\u8f91\u548c\u7ec4\u5408\u6027\u8d28\u3002", "motivation": "\u7814\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u901a\u8fc7\u6f5c\u5728\u8ba1\u7b97\u548c\u63a8\u7406\u65f6\u95f4\u8ba1\u7b97\u89e3\u51b3\u591a\u6b65\u63a8\u7406\u95ee\u9898\uff0c\u7406\u89e3\u6a21\u578b\u5185\u90e8\u7684\u7b97\u6cd5\u539f\u8bed\u5982\u4f55\u652f\u6301\u590d\u6742\u63a8\u7406\u3002", "method": "\u901a\u8fc7\u805a\u7c7b\u795e\u7ecf\u6fc0\u6d3b\u5e76\u5c06\u5b83\u4eec\u4e0e\u63a8\u7406\u8f68\u8ff9\u5339\u914d\u6765\u64cd\u4f5c\u5316\u7b97\u6cd5\u539f\u8bed\uff0c\u4f7f\u7528\u51fd\u6570\u5411\u91cf\u65b9\u6cd5\u63a8\u5bfc\u53ef\u91cd\u7528\u7684\u63a8\u7406\u6784\u5efa\u5757\uff0c\u5e76\u901a\u8fc7\u5411\u91cf\u64cd\u4f5c\u7814\u7a76\u5b83\u4eec\u7684\u7ec4\u5408\u6027\u8d28\u3002", "result": "\u53d1\u73b0\u7b97\u6cd5\u539f\u8bed\u53ef\u4ee5\u5728\u4efb\u52a1\u95f4\u548c\u6a21\u578b\u95f4\u8f6c\u79fb\uff0c\u63a8\u7406\u5fae\u8c03\u589e\u5f3a\u4e86\u7b97\u6cd5\u7684\u6cdb\u5316\u80fd\u529b\uff0cPhi-4-Reasoning\u6bd4\u57fa\u7840\u7248\u672c\u66f4\u7cfb\u7edf\u5730\u4f7f\u7528\u9a8c\u8bc1\u548c\u8def\u5f84\u751f\u6210\u539f\u8bed\u3002", "conclusion": "LLM\u7684\u63a8\u7406\u53ef\u80fd\u7531\u7b97\u6cd5\u539f\u8bed\u7684\u7ec4\u5408\u51e0\u4f55\u652f\u6301\uff0c\u8fd9\u4e9b\u539f\u8bed\u5177\u6709\u8de8\u4efb\u52a1\u548c\u8de8\u6a21\u578b\u7684\u53ef\u8f6c\u79fb\u6027\uff0c\u63a8\u7406\u5fae\u8c03\u80fd\u589e\u5f3a\u8de8\u9886\u57df\u7684\u7b97\u6cd5\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.16499", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16499", "abs": "https://arxiv.org/abs/2510.16499", "authors": ["Michelle Yuan", "Khushbu Pahwa", "Shuaichen Chang", "Mustafa Kaba", "Jiarong Jiang", "Xiaofei Ma", "Yi Zhang", "Monica Sunkara"], "title": "Automated Composition of Agents: A Knapsack Approach for Agentic Component Selection", "comment": "Accepted to NeurIPS 2025 Conference", "summary": "Designing effective agentic systems requires the seamless composition and\nintegration of agents, tools, and models within dynamic and uncertain\nenvironments. Most existing methods rely on static, semantic retrieval\napproaches for tool or agent discovery. However, effective reuse and\ncomposition of existing components remain challenging due to incomplete\ncapability descriptions and the limitations of retrieval methods. Component\nselection suffers because the decisions are not based on capability, cost, and\nreal-time utility. To address these challenges, we introduce a structured,\nautomated framework for agentic system composition that is inspired by the\nknapsack problem. Our framework enables a composer agent to systematically\nidentify, select, and assemble an optimal set of agentic components by jointly\nconsidering performance, budget constraints, and compatibility. By dynamically\ntesting candidate components and modeling their utility in real-time, our\napproach streamlines the assembly of agentic systems and facilitates scalable\nreuse of resources. Empirical evaluation with Claude 3.5 Sonnet across five\nbenchmarking datasets shows that our online-knapsack-based composer\nconsistently lies on the Pareto frontier, achieving higher success rates at\nsignificantly lower component costs compared to our baselines. In the\nsingle-agent setup, the online knapsack composer shows a success rate\nimprovement of up to 31.6% in comparison to the retrieval baselines. In\nmulti-agent systems, the online knapsack composer increases success rate from\n37% to 87% when agents are selected from an agent inventory of 100+ agents. The\nsubstantial performance gap confirms the robust adaptability of our method\nacross diverse domains and budget constraints.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5728\u7ebf\u80cc\u5305\u95ee\u9898\u7684\u81ea\u52a8\u5316\u667a\u80fd\u4f53\u7cfb\u7edf\u7ec4\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u6d4b\u8bd5\u548c\u5b9e\u65f6\u6548\u7528\u5efa\u6a21\uff0c\u5728\u9884\u7b97\u7ea6\u675f\u4e0b\u4f18\u5316\u9009\u62e9\u667a\u80fd\u4f53\u7ec4\u4ef6\uff0c\u663e\u8457\u63d0\u5347\u6210\u529f\u7387\u548c\u964d\u4f4e\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u667a\u80fd\u4f53\u7cfb\u7edf\u7ec4\u5408\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u8bed\u4e49\u68c0\u7d22\uff0c\u5b58\u5728\u80fd\u529b\u63cf\u8ff0\u4e0d\u5b8c\u6574\u3001\u9009\u62e9\u4e0d\u8003\u8651\u80fd\u529b/\u6210\u672c/\u5b9e\u65f6\u6548\u7528\u7b49\u95ee\u9898\uff0c\u96be\u4ee5\u5b9e\u73b0\u6709\u6548\u7ec4\u4ef6\u91cd\u7528\u548c\u7ec4\u5408\u3002", "method": "\u5f15\u5165\u7ed3\u6784\u5316\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u5c06\u667a\u80fd\u4f53\u7cfb\u7edf\u7ec4\u5408\u5efa\u6a21\u4e3a\u5728\u7ebf\u80cc\u5305\u95ee\u9898\uff0c\u7efc\u5408\u8003\u8651\u6027\u80fd\u3001\u9884\u7b97\u7ea6\u675f\u548c\u517c\u5bb9\u6027\uff0c\u52a8\u6001\u6d4b\u8bd5\u5019\u9009\u7ec4\u4ef6\u5e76\u5b9e\u65f6\u5efa\u6a21\u6548\u7528\u3002", "result": "\u57285\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8e\u5728\u7ebf\u80cc\u5305\u7684\u7ec4\u5408\u5668\u59cb\u7ec8\u4f4d\u4e8e\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c\u5355\u667a\u80fd\u4f53\u8bbe\u7f6e\u4e0b\u6210\u529f\u7387\u63d0\u5347\u9ad8\u8fbe31.6%\uff0c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u6210\u529f\u7387\u4ece37%\u63d0\u5347\u523087%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u6837\u5316\u9886\u57df\u548c\u9884\u7b97\u7ea6\u675f\u4e0b\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u9002\u5e94\u6027\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u68c0\u7d22\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2510.17798", "categories": ["eess.SY", "cs.SY", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.17798", "abs": "https://arxiv.org/abs/2510.17798", "authors": ["Samuel Talkington", "Cameron Khanpour", "Rahul K. Gupta", "Sergio A. Dorado-Rojas", "Daniel Turizo", "Hyeongon Park", "Dmitrii M. Ostrovskii", "Daniel K. Molzahn"], "title": "Admittance Matrix Concentration Inequalities for Understanding Uncertain Power Networks", "comment": "9 pages, 1 figure", "summary": "This paper presents probabilistic bounds for the spectrum of the admittance\nmatrix and classical linear power flow models under uncertain network\nparameters; for example, probabilistic line contingencies. Our proposed\napproach imports tools from probability theory, such as concentration\ninequalities for random matrices with independent entries. It yields error\nbounds for common approximations of the AC power flow equations under parameter\nuncertainty, including the DC and LinDistFlow approximations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5728\u4e0d\u786e\u5b9a\u7f51\u7edc\u53c2\u6570\u4e0b\uff08\u5982\u6982\u7387\u6027\u7ebf\u8def\u6545\u969c\uff09\u5bfc\u7eb3\u77e9\u9635\u8c31\u548c\u7ecf\u5178\u7ebf\u6027\u6f6e\u6d41\u6a21\u578b\u7684\u6982\u7387\u8fb9\u754c\u5206\u6790\u65b9\u6cd5", "motivation": "\u7535\u529b\u7cfb\u7edf\u8fd0\u884c\u4e2d\u5b58\u5728\u7f51\u7edc\u53c2\u6570\u4e0d\u786e\u5b9a\u6027\uff0c\u9700\u8981\u4e3a\u5bfc\u7eb3\u77e9\u9635\u8c31\u548c\u7ebf\u6027\u6f6e\u6d41\u6a21\u578b\u63d0\u4f9b\u6982\u7387\u8fb9\u754c\u5206\u6790", "method": "\u91c7\u7528\u6982\u7387\u8bba\u5de5\u5177\uff0c\u7279\u522b\u662f\u5177\u6709\u72ec\u7acb\u6761\u76ee\u7684\u968f\u673a\u77e9\u9635\u7684\u96c6\u4e2d\u4e0d\u7b49\u5f0f", "result": "\u5f97\u5230\u4e86\u5728\u53c2\u6570\u4e0d\u786e\u5b9a\u6027\u4e0bAC\u6f6e\u6d41\u65b9\u7a0b\u5e38\u89c1\u8fd1\u4f3c\uff08\u5305\u62ecDC\u548cLinDistFlow\u8fd1\u4f3c\uff09\u7684\u8bef\u5dee\u8fb9\u754c", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7535\u529b\u7cfb\u7edf\u4e0d\u786e\u5b9a\u6027\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6982\u7387\u8fb9\u754c\u5de5\u5177"}}
{"id": "2510.16157", "categories": ["cs.LG", "cs.CL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16157", "abs": "https://arxiv.org/abs/2510.16157", "authors": ["Xuchen Gong", "Tian Li"], "title": "Zeroth-Order Sharpness-Aware Learning with Exponential Tilting", "comment": null, "summary": "Classic zeroth-order optimization approaches typically optimize for a\nsmoothed version of the original function, i.e., the expected objective under\nrandomly perturbed model parameters. This can be interpreted as encouraging the\nloss values in the perturbation set to be small on average. Popular\nsharpness-aware minimization (SAM) objectives, however, typically focus on the\nlargest loss within the neighborhood to arrive at flat minima more effectively.\nIn this work, we connect zeroth-order optimization (and its corresponding\nobjectives) with SAM approaches explicitly, through an exponential tilting\nobjective that provides a smooth transition between the average- and the\nmax-loss formulations. We explore new zeroth-order algorithms to solve a soft\nSAM objective parameterized by a tilting parameter $t$. We provide precise\ncharacterizations of the sharpness notions of the tilted SAM framework.\nPractically, our approach can be used as a gradient-free and memory-efficient\nalternative to SAM variants, and it achieves better generalization compared to\nvanilla zeroth-order baselines on a wide range of downstream tasks, including\nclassification, multiple choice QA, and language generation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u96f6\u9636\u4f18\u5316\u548c\u9510\u5ea6\u611f\u77e5\u6700\u5c0f\u5316\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6307\u6570\u503e\u659c\u76ee\u6807\u5728\u5e73\u5747\u635f\u5931\u548c\u6700\u5927\u635f\u5931\u4e4b\u95f4\u5e73\u6ed1\u8fc7\u6e21\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u96f6\u9636\u4f18\u5316\u65b9\u6cd5\u4f18\u5316\u5e73\u6ed1\u540e\u7684\u51fd\u6570\u671f\u671b\u503c\uff0c\u800cSAM\u65b9\u6cd5\u5173\u6ce8\u90bb\u57df\u5185\u6700\u5927\u635f\u5931\u4ee5\u83b7\u5f97\u5e73\u5766\u6700\u5c0f\u503c\u3002\u672c\u6587\u65e8\u5728\u8fde\u63a5\u8fd9\u4e24\u79cd\u65b9\u6cd5\uff0c\u5f00\u53d1\u66f4\u6709\u6548\u7684\u4f18\u5316\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u6307\u6570\u503e\u659c\u76ee\u6807\uff0c\u901a\u8fc7\u503e\u659c\u53c2\u6570t\u5728\u5e73\u5747\u635f\u5931\u548c\u6700\u5927\u635f\u5931\u4e4b\u95f4\u5e73\u6ed1\u8fc7\u6e21\uff0c\u5f00\u53d1\u65b0\u7684\u96f6\u9636\u7b97\u6cd5\u6765\u6c42\u89e3\u8f6fSAM\u76ee\u6807\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5206\u7c7b\u3001\u591a\u9879\u9009\u62e9\u95ee\u7b54\u548c\u8bed\u8a00\u751f\u6210\u7b49\u4e0b\u6e38\u4efb\u52a1\u4e0a\u6bd4\u4f20\u7edf\u96f6\u9636\u57fa\u7ebf\u65b9\u6cd5\u83b7\u5f97\u66f4\u597d\u7684\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u4f5c\u4e3aSAM\u53d8\u4f53\u7684\u65e0\u68af\u5ea6\u548c\u5185\u5b58\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.17710", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2510.17710", "abs": "https://arxiv.org/abs/2510.17710", "authors": ["Frederik Zuiderveen Borgesius"], "title": "Mensen aanwijzen maar niet bij naam noemen: behavioural targeting, persoonsgegevens, en de nieuwe Privacyverordening", "comment": "In Dutch", "summary": "Information about millions of people is collected for behavioural targeting,\na type of marketing that involves tracking people's online behaviour for\ntargeted advertising. It is hotly debated whether data protection law applies\nto behavioural targeting. Many behavioural targeting companies say that, as\nlong as they do not tie names to data they hold about individuals, they do not\nprocess any personal data, and that, therefore, data protection law does not\napply to them. European Data Protection Authorities, however, take the view\nthat a company processes personal data if it uses data to single out a person,\neven if it cannot tie a name to these data. This paper argues that data\nprotection law should indeed apply to behavioural targeting. Companies can\noften tie a name to nameless data about individuals. Furthermore, behavioural\ntargeting relies on collecting information about individuals, singling out\nindividuals, and targeting ads to individuals. Many privacy risks remain,\nregardless of whether companies tie a name to the information they hold about a\nperson. A name is merely one of the identifiers that can be tied to data about\na person, and it is not even the most practical identifier for behavioural\ntargeting. Seeing data used to single out a person as personal data fits the\nrationale for data protection law: protecting fairness and privacy.", "AI": {"tldr": "\u672c\u6587\u4e3b\u5f20\u6570\u636e\u4fdd\u62a4\u6cd5\u5e94\u9002\u7528\u4e8e\u884c\u4e3a\u5b9a\u5411\u8425\u9500\uff0c\u5373\u4f7f\u516c\u53f8\u672a\u5c06\u59d3\u540d\u4e0e\u4e2a\u4eba\u6570\u636e\u5173\u8054\uff0c\u53ea\u8981\u4f7f\u7528\u6570\u636e\u6765\u8bc6\u522b\u7279\u5b9a\u4e2a\u4eba\uff0c\u5c31\u5e94\u89c6\u4e3a\u5904\u7406\u4e2a\u4eba\u6570\u636e\u3002", "motivation": "\u884c\u4e3a\u5b9a\u5411\u8425\u9500\u516c\u53f8\u58f0\u79f0\u53ea\u8981\u4e0d\u5c06\u59d3\u540d\u4e0e\u4e2a\u4eba\u6570\u636e\u5173\u8054\uff0c\u5c31\u4e0d\u5904\u7406\u4e2a\u4eba\u6570\u636e\uff0c\u56e0\u6b64\u4e0d\u53d7\u6570\u636e\u4fdd\u62a4\u6cd5\u7ea6\u675f\u3002\u672c\u6587\u65e8\u5728\u53cd\u9a73\u8fd9\u4e00\u89c2\u70b9\u3002", "method": "\u901a\u8fc7\u5206\u6790\u884c\u4e3a\u5b9a\u5411\u8425\u9500\u7684\u5b9e\u9645\u8fd0\u4f5c\u65b9\u5f0f\uff0c\u8bba\u8bc1\u516c\u53f8\u7ecf\u5e38\u80fd\u591f\u5c06\u533f\u540d\u6570\u636e\u4e0e\u59d3\u540d\u5173\u8054\uff0c\u4e14\u59d3\u540d\u5e76\u975e\u6700\u5b9e\u7528\u7684\u6807\u8bc6\u7b26\u3002", "result": "\u8bba\u8bc1\u8868\u660e\u884c\u4e3a\u5b9a\u5411\u8425\u9500\u672c\u8d28\u4e0a\u6d89\u53ca\u6536\u96c6\u4e2a\u4eba\u4fe1\u606f\u3001\u8bc6\u522b\u4e2a\u4eba\u5e76\u5411\u4e2a\u4eba\u6295\u653e\u5e7f\u544a\uff0c\u5b58\u5728\u9690\u79c1\u98ce\u9669\u3002", "conclusion": "\u6570\u636e\u4fdd\u62a4\u6cd5\u5e94\u9002\u7528\u4e8e\u884c\u4e3a\u5b9a\u5411\u8425\u9500\uff0c\u5c06\u7528\u4e8e\u8bc6\u522b\u4e2a\u4eba\u7684\u6570\u636e\u89c6\u4e3a\u4e2a\u4eba\u6570\u636e\u7b26\u5408\u6570\u636e\u4fdd\u62a4\u6cd5\u7684\u5b97\u65e8\uff1a\u4fdd\u62a4\u516c\u5e73\u6027\u548c\u9690\u79c1\u3002"}}
{"id": "2510.16382", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16382", "abs": "https://arxiv.org/abs/2510.16382", "authors": ["Ze Tao", "Jian Zhang", "Haowei Li", "Xianshuai Li", "Yifei Peng", "Xiyao Liu", "Senzhang Wang", "Chao Liu", "Sheng Ren", "Shichao Zhang"], "title": "Humanoid-inspired Causal Representation Learning for Domain Generalization", "comment": null, "summary": "This paper proposes the Humanoid-inspired Structural Causal Model (HSCM), a\nnovel causal framework inspired by human intelligence, designed to overcome the\nlimitations of conventional domain generalization models. Unlike approaches\nthat rely on statistics to capture data-label dependencies and learn\ndistortion-invariant representations, HSCM replicates the hierarchical\nprocessing and multi-level learning of human vision systems, focusing on\nmodeling fine-grained causal mechanisms. By disentangling and reweighting key\nimage attributes such as color, texture, and shape, HSCM enhances\ngeneralization across diverse domains, ensuring robust performance and\ninterpretability. Leveraging the flexibility and adaptability of human\nintelligence, our approach enables more effective transfer and learning in\ndynamic, complex environments. Through both theoretical and empirical\nevaluations, we demonstrate that HSCM outperforms existing domain\ngeneralization models, providing a more principled method for capturing causal\nrelationships and improving model robustness. The code is available at\nhttps://github.com/lambett/HSCM.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u53d7\u4eba\u7c7b\u667a\u80fd\u542f\u53d1\u7684HSCM\u56e0\u679c\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u548c\u91cd\u52a0\u6743\u56fe\u50cf\u5c5e\u6027\uff08\u989c\u8272\u3001\u7eb9\u7406\u3001\u5f62\u72b6\uff09\u6765\u63d0\u5347\u8de8\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u8bc1\u8bc4\u4f30\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u514b\u670d\u4f20\u7edf\u9886\u57df\u6cdb\u5316\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u6a21\u4eff\u4eba\u7c7b\u89c6\u89c9\u7cfb\u7edf\u7684\u5206\u5c42\u5904\u7406\u548c\u591a\u5c42\u6b21\u5b66\u4e60\u673a\u5236\uff0c\u4e13\u6ce8\u4e8e\u5efa\u6a21\u7ec6\u7c92\u5ea6\u56e0\u679c\u673a\u5236\u3002", "method": "\u6784\u5efa\u53d7\u4eba\u7c7b\u667a\u80fd\u542f\u53d1\u7684\u7ed3\u6784\u56e0\u679c\u6a21\u578b\uff08HSCM\uff09\uff0c\u89e3\u8026\u548c\u91cd\u52a0\u6743\u5173\u952e\u56fe\u50cf\u5c5e\u6027\uff08\u989c\u8272\u3001\u7eb9\u7406\u3001\u5f62\u72b6\uff09\uff0c\u590d\u5236\u4eba\u7c7b\u89c6\u89c9\u7cfb\u7edf\u7684\u5206\u5c42\u5904\u7406\u8fc7\u7a0b\u3002", "result": "HSCM\u5728\u8de8\u57df\u6cdb\u5316\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u4f9b\u4e86\u66f4\u539f\u5219\u6027\u7684\u56e0\u679c\u5173\u7cfb\u6355\u6349\u65b9\u6cd5\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "HSCM\u901a\u8fc7\u6a21\u4eff\u4eba\u7c7b\u667a\u80fd\u7684\u7075\u6d3b\u6027\u548c\u9002\u5e94\u6027\uff0c\u4e3a\u52a8\u6001\u590d\u6742\u73af\u5883\u4e2d\u7684\u6709\u6548\u8fc1\u79fb\u548c\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u4f18\u8d8a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2510.17366", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2510.17366", "abs": "https://arxiv.org/abs/2510.17366", "authors": ["D\u00e2n\u00e2 Davar", "Geovani Nunes Grapiglia"], "title": "A Finite-Difference Trust-Region Method for Convexly Constrained Smooth Optimization", "comment": null, "summary": "We propose a derivative-free trust-region method based on finite-difference\ngradient approximations for smooth optimization problems with convex\nconstraints. The proposed method does not require computing an approximate\nstationarity measure. For nonconvex problems, we establish a worst-case\ncomplexity bound of\n$\\mathcal{O}\\!\\left(n\\left(\\tfrac{L}{\\sigma}\\epsilon\\right)^{-2}\\right)$\nfunction evaluations for the method to reach an\n$\\left(\\tfrac{L}{\\sigma}\\epsilon\\right)$-approximate stationary point, where\n$n$ is the number of variables, $L$ is the Lipschitz constant of the gradient,\nand $\\sigma$ is a user-defined estimate of $L$. If the objective function is\nconvex, the complexity to reduce the functional residual below\n$(L/\\sigma)\\epsilon$ is shown to be of\n$\\mathcal{O}\\!\\left(n\\left(\\tfrac{L}{\\sigma}\\epsilon\\right)^{-1}\\right)$\nfunction evaluations, while for Polyak-Lojasiewicz functions on unconstrained\ndomains, the bound further improves to\n$\\mathcal{O}\\left(n\\log\\left(\\left(\\frac{L}{\\sigma}\\epsilon\\right)^{-1}\\right)\\right)$.\nNumerical experiments on benchmark problems and a model-fitting application\ndemonstrate the method's efficiency relative to state-of-the-art\nderivative-free solvers for both unconstrained and bound-constrained problems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6709\u9650\u5dee\u5206\u68af\u5ea6\u8fd1\u4f3c\u7684\u65e0\u5bfc\u6570\u4fe1\u8d56\u57df\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5177\u6709\u51f8\u7ea6\u675f\u7684\u5149\u6ed1\u4f18\u5316\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u8ba1\u7b97\u8fd1\u4f3c\u5e73\u7a33\u6027\u5ea6\u91cf\uff0c\u5e76\u5728\u975e\u51f8\u3001\u51f8\u548cPolyak-Lojasiewicz\u51fd\u6570\u4e0a\u5efa\u7acb\u4e86\u4e0d\u540c\u7684\u590d\u6742\u5ea6\u754c\u9650\u3002", "motivation": "\u4f20\u7edf\u4f18\u5316\u65b9\u6cd5\u9700\u8981\u68af\u5ea6\u4fe1\u606f\uff0c\u4f46\u5728\u8bb8\u591a\u5b9e\u9645\u95ee\u9898\u4e2d\u68af\u5ea6\u96be\u4ee5\u8ba1\u7b97\u6216\u4e0d\u53ef\u5f97\u3002\u65e0\u5bfc\u6570\u4f18\u5316\u65b9\u6cd5\u901a\u8fc7\u51fd\u6570\u503c\u6765\u8fd1\u4f3c\u68af\u5ea6\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u5ea6\u5206\u6790\u548c\u5b9e\u9645\u6548\u7387\u65b9\u9762\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "method": "\u57fa\u4e8e\u6709\u9650\u5dee\u5206\u68af\u5ea6\u8fd1\u4f3c\u7684\u4fe1\u8d56\u57df\u65b9\u6cd5\uff0c\u5904\u7406\u51f8\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u907f\u514d\u8ba1\u7b97\u8fd1\u4f3c\u5e73\u7a33\u6027\u5ea6\u91cf\uff0c\u7b80\u5316\u4e86\u7b97\u6cd5\u5b9e\u73b0\u3002", "result": "\u5efa\u7acb\u4e86\u4e09\u79cd\u590d\u6742\u5ea6\u754c\u9650\uff1a\u975e\u51f8\u95ee\u9898\u4e3aO(n(L/\u03c3\u03f5)^{-2})\uff0c\u51f8\u95ee\u9898\u4e3aO(n(L/\u03c3\u03f5)^{-1})\uff0cPolyak-Lojasiewicz\u51fd\u6570\u4e3aO(n log((L/\u03c3\u03f5)^{-1}))\u3002\u6570\u503c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u57fa\u51c6\u95ee\u9898\u548c\u6a21\u578b\u62df\u5408\u5e94\u7528\u4e2d\u4f18\u4e8e\u73b0\u6709\u65e0\u5bfc\u6570\u6c42\u89e3\u5668\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65e0\u5bfc\u6570\u4fe1\u8d56\u57df\u65b9\u6cd5\u5728\u7406\u8bba\u548c\u5b9e\u9645\u6027\u80fd\u4e0a\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u65e0\u5bfc\u6570\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7406\u8bba\u4fdd\u8bc1\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u68af\u5ea6\u4fe1\u606f\u4e0d\u53ef\u5f97\u6216\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u4f18\u5316\u95ee\u9898\u3002"}}
{"id": "2510.15990", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.15990", "abs": "https://arxiv.org/abs/2510.15990", "authors": ["Kangqi Ni", "Zhen Tan", "Zijie Liu", "Pingzhi Li", "Tianlong Chen"], "title": "Can GRPO Help LLMs Transcend Their Pretraining Origin?", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR), primarily driven by\nthe Group Relative Policy Optimization (GRPO) algorithm, is a leading approach\nfor enhancing the reasoning abilities of Large Language Models (LLMs). Despite\nits wide adoption, GRPO's gains are often inconsistent; for instance, a model\nmay show significant improvement in one reasoning domain, like mathematics, yet\nremain stagnant in another, such as medicine. This inconsistency raises a\ncritical question: under what conditions does GRPO improve reasoning and\ngeneralize out-of-distribution (OOD)? We investigate this from a data\ndistribution perspective. We first prove theoretically that GRPO is a\nconservative reweighting scheme, bounded by the base model's distribution and\nthus unable to discover completely novel solutions. We further validate this in\ncarefully designed controlled studies by training transformers from scratch,\nevaluating generalization across reasoning depth, input length, token\nrepresentation, and compositionality. Our results provide a principled\nexplanation for GRPO's boundaries: OOD improvement emerges only when the target\ntask aligns with the model's pretrained biases, while gains on in-distribution\n(ID) tasks diminish as performance saturates. This reframes GRPO not as a\nuniversal reasoning enhancer but as a tool that sharpens pretraining biases.\nOur findings motivate future development of algorithms that can expand a\nmodel's capabilities beyond its pretraining origin.", "AI": {"tldr": "GRPO\u7b97\u6cd5\u4f5c\u4e3aRLVR\u7684\u6838\u5fc3\u65b9\u6cd5\uff0c\u867d\u7136\u80fd\u63d0\u5347LLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5176\u6548\u679c\u4e0d\u4e00\u81f4\u4e14\u53d7\u9650\u4e8e\u9884\u8bad\u7ec3\u5206\u5e03\uff0c\u65e0\u6cd5\u53d1\u73b0\u5168\u65b0\u89e3\u51b3\u65b9\u6848\uff0c\u53ea\u80fd\u5f3a\u5316\u9884\u8bad\u7ec3\u504f\u89c1\u3002", "motivation": "\u7814\u7a76GRPO\u5728\u4ec0\u4e48\u6761\u4ef6\u4e0b\u80fd\u63d0\u5347\u63a8\u7406\u80fd\u529b\u5e76\u5b9e\u73b0\u5206\u5e03\u5916\u6cdb\u5316\uff0c\u89e3\u91ca\u5176\u6548\u679c\u4e0d\u4e00\u81f4\u7684\u539f\u56e0\u3002", "method": "\u4ece\u6570\u636e\u5206\u5e03\u89d2\u5ea6\u8fdb\u884c\u7406\u8bba\u5206\u6790\uff0c\u8bc1\u660eGRPO\u662f\u4fdd\u5b88\u7684\u91cd\u52a0\u6743\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u4ece\u96f6\u8bad\u7ec3transformer\u8fdb\u884c\u63a7\u5236\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u5728\u63a8\u7406\u6df1\u5ea6\u3001\u8f93\u5165\u957f\u5ea6\u3001token\u8868\u793a\u548c\u7ec4\u5408\u6027\u7b49\u65b9\u9762\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "GRPO\u7684\u5206\u5e03\u5916\u6539\u8fdb\u4ec5\u5728\u76ee\u6807\u4efb\u52a1\u4e0e\u6a21\u578b\u9884\u8bad\u7ec3\u504f\u89c1\u4e00\u81f4\u65f6\u51fa\u73b0\uff0c\u800c\u5206\u5e03\u5185\u4efb\u52a1\u7684\u589e\u76ca\u4f1a\u968f\u7740\u6027\u80fd\u9971\u548c\u800c\u51cf\u5f31\u3002", "conclusion": "GRPO\u4e0d\u662f\u901a\u7528\u7684\u63a8\u7406\u589e\u5f3a\u5668\uff0c\u800c\u662f\u5f3a\u5316\u9884\u8bad\u7ec3\u504f\u89c1\u7684\u5de5\u5177\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u6269\u5c55\u6a21\u578b\u80fd\u529b\u8d85\u8d8a\u9884\u8bad\u7ec3\u8d77\u6e90\u7684\u65b0\u7b97\u6cd5\u3002"}}
{"id": "2510.16549", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16549", "abs": "https://arxiv.org/abs/2510.16549", "authors": ["Haoxuan Zhang", "Ruochi Li", "Sarthak Shrestha", "Shree Harshini Mamidala", "Revanth Putta", "Arka Krishan Aggarwal", "Ting Xiao", "Junhua Ding", "Haihua Chen"], "title": "ReviewGuard: Enhancing Deficient Peer Review Detection via LLM-Driven Data Augmentation", "comment": null, "summary": "Peer review serves as the gatekeeper of science, yet the surge in submissions\nand widespread adoption of large language models (LLMs) in scholarly evaluation\npresent unprecedented challenges. Recent work has focused on using LLMs to\nimprove review efficiency or generate insightful review content. However,\nunchecked deficient reviews from both human experts and AI systems threaten to\nsystematically undermine the peer review ecosystem and compromise academic\nintegrity. To address this critical issue, we introduce ReviewGuard, an\nautomated system for detecting and categorizing deficient reviews. ReviewGuard\nemploys a comprehensive four-stage LLM-driven framework that: (1) collects ICLR\nand NeurIPS papers with their corresponding reviews from OpenReview; (2)\nannotates review types using GPT-4.1 with human validation; (3) addresses class\nimbalance and data scarcity through LLM-driven synthetic data augmentation,\nproducing a final corpus of 6,634 papers, 24,657 real reviews, and 46,438\nsynthetic reviews; and (4) fine-tunes both encoder-based models and open source\nLLMs. We perform comprehensive feature analysis of the structure and quality of\nthe review text. Compared to sufficient reviews, deficient reviews demonstrate\nlower rating scores, higher self-reported confidence, reduced structural\ncomplexity, and a higher proportion of negative sentiment. AI-generated text\ndetection reveals that, since ChatGPT's emergence, AI-generated reviews have\nincreased dramatically. In the evaluation of deficient review detection models,\nmixed training with synthetic and real review data provides substantial\nenhancements to recall and F1 scores on the binary task. This study presents\nthe first LLM-driven system for detecting deficient peer reviews, providing\nevidence to inform AI governance in peer review while offering valuable\ninsights into human-AI collaboration to maintain academic integrity.", "AI": {"tldr": "\u63d0\u51fa\u4e86ReviewGuard\u7cfb\u7edf\uff0c\u4f7f\u7528\u56db\u9636\u6bb5LLM\u9a71\u52a8\u6846\u67b6\u81ea\u52a8\u68c0\u6d4b\u548c\u5206\u7c7b\u6709\u7f3a\u9677\u7684\u540c\u884c\u8bc4\u5ba1\uff0c\u5305\u62ec\u6570\u636e\u6536\u96c6\u3001\u6807\u6ce8\u3001\u6570\u636e\u589e\u5f3a\u548c\u6a21\u578b\u5fae\u8c03\uff0c\u6709\u6548\u63d0\u5347\u4e86\u7f3a\u9677\u8bc4\u5ba1\u68c0\u6d4b\u7684\u53ec\u56de\u7387\u548cF1\u5206\u6570\u3002", "motivation": "\u540c\u884c\u8bc4\u5ba1\u4f5c\u4e3a\u79d1\u5b66\u7684\u5b88\u95e8\u4eba\u9762\u4e34\u63d0\u4ea4\u91cf\u6fc0\u589e\u548cLLM\u5e7f\u6cdb\u4f7f\u7528\u7684\u6311\u6218\uff0c\u672a\u53d7\u76d1\u7ba1\u7684\u7f3a\u9677\u8bc4\u5ba1\uff08\u5305\u62ec\u4eba\u7c7b\u4e13\u5bb6\u548cAI\u7cfb\u7edf\u4ea7\u751f\u7684\uff09\u53ef\u80fd\u7cfb\u7edf\u6027\u7834\u574f\u540c\u884c\u8bc4\u5ba1\u751f\u6001\u7cfb\u7edf\u5e76\u635f\u5bb3\u5b66\u672f\u8bda\u4fe1\u3002", "method": "\u91c7\u7528\u56db\u9636\u6bb5LLM\u9a71\u52a8\u6846\u67b6\uff1a1)\u4eceOpenReview\u6536\u96c6ICLR\u548cNeurIPS\u8bba\u6587\u53ca\u8bc4\u5ba1\uff1b2)\u4f7f\u7528GPT-4.1\u6807\u6ce8\u8bc4\u5ba1\u7c7b\u578b\u5e76\u4eba\u5de5\u9a8c\u8bc1\uff1b3)\u901a\u8fc7LLM\u9a71\u52a8\u7684\u5408\u6210\u6570\u636e\u589e\u5f3a\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff1b4)\u5fae\u8c03\u7f16\u7801\u5668\u6a21\u578b\u548c\u5f00\u6e90LLM\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b6,634\u7bc7\u8bba\u6587\u300124,657\u6761\u771f\u5b9e\u8bc4\u5ba1\u548c46,438\u6761\u5408\u6210\u8bc4\u5ba1\u7684\u8bed\u6599\u5e93\u3002\u7f3a\u9677\u8bc4\u5ba1\u8868\u73b0\u51fa\u66f4\u4f4e\u7684\u8bc4\u5206\u3001\u66f4\u9ad8\u7684\u81ea\u62a5\u7f6e\u4fe1\u5ea6\u3001\u964d\u4f4e\u7684\u7ed3\u6784\u590d\u6742\u5ea6\u548c\u66f4\u9ad8\u7684\u8d1f\u9762\u60c5\u7eea\u6bd4\u4f8b\u3002\u6df7\u5408\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u4e86\u4e8c\u5143\u4efb\u52a1\u7684\u53ec\u56de\u7387\u548cF1\u5206\u6570\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u7528\u4e8e\u68c0\u6d4b\u7f3a\u9677\u540c\u884c\u8bc4\u5ba1\u7684LLM\u9a71\u52a8\u7cfb\u7edf\uff0c\u4e3a\u540c\u884c\u8bc4\u5ba1\u4e2d\u7684AI\u6cbb\u7406\u63d0\u4f9b\u4e86\u8bc1\u636e\uff0c\u5e76\u4e3a\u7ef4\u62a4\u5b66\u672f\u8bda\u4fe1\u7684\u4eba\u673a\u534f\u4f5c\u63d0\u4f9b\u4e86\u5b9d\u8d35\u89c1\u89e3\u3002"}}
{"id": "2510.16063", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.16063", "abs": "https://arxiv.org/abs/2510.16063", "authors": ["Muhy Eddin Za'ter", "Bri-Mathias Hodge"], "title": "Learning a Generalized Model for Substation Level Voltage Estimation in Distribution Networks", "comment": null, "summary": "Accurate voltage estimation in distribution networks is critical for\nreal-time monitoring and increasing the reliability of the grid. As DER\npenetration and distribution level voltage variability increase, robust\ndistribution system state estimation (DSSE) has become more essential to\nmaintain safe and efficient operations. Traditional DSSE techniques, however,\nstruggle with sparse measurements and the scale of modern feeders, limiting\ntheir scalability to large networks. This paper presents a hierarchical graph\nneural network for substation-level voltage estimation that exploits both\nelectrical topology and physical features, while remaining robust to the low\nobservability levels common to real-world distribution networks. Leveraging the\npublic SMART-DS datasets, the model is trained and evaluated on thousands of\nbuses across multiple substations and DER penetration scenarios. Comprehensive\nexperiments demonstrate that the proposed method achieves up to 2 times lower\nRMSE than alternative data-driven models, and maintains high accuracy with as\nlittle as 1\\% measurement coverage. The results highlight the potential of GNNs\nto enable scalable, reproducible, and data-driven voltage monitoring for\ndistribution systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u53d8\u7535\u7ad9\u7ea7\u7535\u538b\u4f30\u8ba1\u7684\u5206\u5c42\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u8be5\u7f51\u7edc\u5229\u7528\u7535\u6c14\u62d3\u6251\u548c\u7269\u7406\u7279\u5f81\uff0c\u5728\u4f4e\u89c2\u6d4b\u6027\u6761\u4ef6\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\uff0c\u5728SMART-DS\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u5206\u5e03\u5f0f\u80fd\u6e90\u6e17\u900f\u548c\u914d\u7535\u7f51\u7535\u538b\u6ce2\u52a8\u589e\u52a0\uff0c\u4f20\u7edf\u914d\u7535\u7f51\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u7a00\u758f\u6d4b\u91cf\u548c\u5927\u89c4\u6a21\u7f51\u7edc\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9c81\u68d2\u548c\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5206\u5c42\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u7ed3\u5408\u7535\u6c14\u62d3\u6251\u548c\u7269\u7406\u7279\u5f81\u8fdb\u884c\u7535\u538b\u4f30\u8ba1\uff0c\u4e13\u95e8\u9488\u5bf9\u5b9e\u9645\u914d\u7535\u7f51\u4e2d\u5e38\u89c1\u7684\u4f4e\u89c2\u6d4b\u6027\u6761\u4ef6\u8bbe\u8ba1\u3002", "result": "\u5728\u6570\u5343\u4e2a\u603b\u7ebf\u548c\u591a\u4e2a\u53d8\u7535\u7ad9\u573a\u666f\u4e0b\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4\u66ff\u4ee3\u6570\u636e\u9a71\u52a8\u6a21\u578b\u7684RMSE\u964d\u4f4e\u9ad8\u8fbe2\u500d\uff0c\u5728\u4ec51%\u6d4b\u91cf\u8986\u76d6\u7387\u4e0b\u4ecd\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "conclusion": "\u56fe\u795e\u7ecf\u7f51\u7edc\u4e3a\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u53ef\u590d\u73b0\u548c\u6570\u636e\u9a71\u52a8\u7684\u914d\u7535\u7f51\u7535\u538b\u76d1\u6d4b\u63d0\u4f9b\u4e86\u6f5c\u529b\uff0c\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u73b0\u4ee3\u914d\u7535\u7f51\u7684\u6311\u6218\u3002"}}
{"id": "2510.16161", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16161", "abs": "https://arxiv.org/abs/2510.16161", "authors": ["Ankitkumar Joshi", "Milos Hauskrecht"], "title": "Still Competitive: Revisiting Recurrent Models for Irregular Time Series Prediction", "comment": null, "summary": "Modeling irregularly sampled multivariate time series is a persistent\nchallenge in domains like healthcare and sensor networks. While recent works\nhave explored a variety of complex learning architectures to solve the\nprediction problems for irregularly sampled time series, it remains unclear\nwhat are the true benefits of some of these architectures, and whether clever\nmodifications of simpler and more efficient RNN-based algorithms are still\ncompetitive, i.e. they are on par with or even superior to these methods. In\nthis work, we propose and study GRUwE: Gated Recurrent Unit with Exponential\nbasis functions, that builds upon RNN-based architectures for observations made\nat irregular times. GRUwE supports both regression-based and event-based\npredictions in continuous time. GRUwE works by maintaining a Markov state\nrepresentation of the time series that updates with the arrival of irregular\nobservations. The Markov state update relies on two reset mechanisms: (i)\nobservation-triggered reset, and (ii) time-triggered reset of the GRU state\nusing learnable exponential decays, to support the predictions in continuous\ntime. Our empirical evaluations across several real-world benchmarks on\nnext-observation and next-event prediction tasks demonstrate that GRUwE can\nindeed achieve competitive to superior performance compared to the recent\nstate-of-the-art (SOTA) methods. Thanks to its simplicity, GRUwE offers\ncompelling advantages: it is easy to implement, requires minimal\nhyper-parameter tuning efforts, and significantly reduces the computational\noverhead in the online deployment.", "AI": {"tldr": "GRUwE\uff1a\u57fa\u4e8eGRU\u7684\u7b80\u5355\u9ad8\u6548\u6a21\u578b\uff0c\u7528\u4e8e\u4e0d\u89c4\u5219\u91c7\u6837\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u5728\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u590d\u6742SOTA\u65b9\u6cd5", "motivation": "\u89e3\u51b3\u4e0d\u89c4\u5219\u91c7\u6837\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u6311\u6218\uff0c\u9a8c\u8bc1\u7b80\u5355RNN\u67b6\u6784\u662f\u5426\u4ecd\u80fd\u7ade\u4e89\u751a\u81f3\u4f18\u4e8e\u590d\u6742\u65b9\u6cd5", "method": "GRUwE\uff1a\u95e8\u63a7\u5faa\u73af\u5355\u5143\u52a0\u6307\u6570\u57fa\u51fd\u6570\uff0c\u901a\u8fc7\u89c2\u6d4b\u89e6\u53d1\u91cd\u7f6e\u548c\u65f6\u95f4\u89e6\u53d1\u91cd\u7f6e\u4e24\u79cd\u673a\u5236\u66f4\u65b0\u9a6c\u5c14\u53ef\u592b\u72b6\u6001", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGRUwE\u5728\u4e0b\u4e00\u89c2\u6d4b\u548c\u4e0b\u4e00\u4e8b\u4ef6\u9884\u6d4b\u4efb\u52a1\u4e0a\u8fbe\u5230\u7ade\u4e89\u6027\u751a\u81f3\u4f18\u4e8eSOTA\u7684\u6027\u80fd", "conclusion": "GRUwE\u8bc1\u660e\u7b80\u5355\u9ad8\u6548\u7684RNN\u67b6\u6784\u4ecd\u5177\u7ade\u4e89\u529b\uff0c\u6613\u4e8e\u5b9e\u73b0\u3001\u8c03\u53c2\u5c11\u3001\u8ba1\u7b97\u5f00\u9500\u4f4e\uff0c\u9002\u5408\u5728\u7ebf\u90e8\u7f72"}}
{"id": "2510.17711", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2510.17711", "abs": "https://arxiv.org/abs/2510.17711", "authors": ["Frederik Zuiderveen Borgesius"], "title": "Discrimination, intelligence artificielle et decisions algorithmiques", "comment": "In French", "summary": "Artificial intelligence (AI) has a huge impact on our personal lives and also\non our democratic society as a whole. While AI offers vast opportunities for\nthe benefit of people, its potential to embed and perpetuate bias and\ndiscrimination remains one of the most pressing challenges deriving from its\nincreasing use. This new study, which was prepared by Prof. Frederik Zuiderveen\nBorgesius for the Anti-discrimination Department of the Council of Europe,\nelaborates on the risks of discrimination caused by algorithmic decision-making\nand other types of artificial intelligence (AI).", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u4eba\u5de5\u667a\u80fd\u5728\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u5bfc\u81f4\u7684\u6b67\u89c6\u98ce\u9669\uff0c\u7531Frederik Zuiderveen Borgesius\u6559\u6388\u4e3a\u6b27\u6d32\u7406\u4e8b\u4f1a\u53cd\u6b67\u89c6\u90e8\u95e8\u7f16\u5199", "motivation": "\u4eba\u5de5\u667a\u80fd\u867d\u7136\u4e3a\u4eba\u7c7b\u5e26\u6765\u5de8\u5927\u673a\u9047\uff0c\u4f46\u5176\u5d4c\u5165\u548c\u5ef6\u7eed\u504f\u89c1\u4e0e\u6b67\u89c6\u7684\u6f5c\u529b\u4ecd\u662f\u6700\u7d27\u8feb\u7684\u6311\u6218\u4e4b\u4e00\uff0c\u7279\u522b\u662f\u5728\u6c11\u4e3b\u793e\u4f1a\u4e2d", "method": "\u8be5\u7814\u7a76\u901a\u8fc7\u5206\u6790\u7b97\u6cd5\u51b3\u7b56\u548c\u5176\u4ed6\u7c7b\u578bAI\u7cfb\u7edf\u7684\u8fd0\u4f5c\u673a\u5236\uff0c\u8bc6\u522b\u6f5c\u5728\u7684\u6b67\u89c6\u98ce\u9669", "result": "\u7814\u7a76\u8be6\u7ec6\u9610\u8ff0\u4e86AI\u7cfb\u7edf\u53ef\u80fd\u5bfc\u81f4\u7684\u6b67\u89c6\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u76f8\u5173\u98ce\u9669\u5206\u6790", "conclusion": "\u9700\u8981\u5173\u6ce8\u548c\u89e3\u51b3AI\u7cfb\u7edf\u53ef\u80fd\u5bfc\u81f4\u7684\u6b67\u89c6\u98ce\u9669\uff0c\u786e\u4fddAI\u6280\u672f\u7684\u516c\u5e73\u4f7f\u7528"}}
{"id": "2510.16392", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16392", "abs": "https://arxiv.org/abs/2510.16392", "authors": ["Ao Tian", "Yunfeng Lu", "Xinxin Fan", "Changhao Wang", "Lanzhi Zhou", "Yeyao Zhang", "Yanfang Liu"], "title": "RGMem: Renormalization Group-based Memory Evolution for Language Agent User Profile", "comment": "11 pages,3 figures", "summary": "Personalized and continuous interactions are the key to enhancing user\nexperience in today's large language model (LLM)-based conversational systems,\nhowever, the finite context windows and static parametric memory make it\ndifficult to model the cross-session long-term user states and behavioral\nconsistency. Currently, the existing solutions to this predicament, such as\nretrieval-augmented generation (RAG) and explicit memory systems, primarily\nfocus on fact-level storage and retrieval, lacking the capability to distill\nlatent preferences and deep traits from the multi-turn dialogues, which limits\nthe long-term and effective user modeling, directly leading to the personalized\ninteractions remaining shallow, and hindering the cross-session continuity. To\nrealize the long-term memory and behavioral consistency for Language Agents in\nLLM era, we propose a self-evolving memory framework RGMem, inspired by the\nideology of classic renormalization group (RG) in physics, this framework\nenables to organize the dialogue history in multiple scales: it first extracts\nsemantics and user insights from episodic fragments, then through hierarchical\ncoarse-graining and rescaling operations, progressively forms a\ndynamically-evolved user profile. The core innovation of our work lies in\nmodeling memory evolution as a multi-scale process of information compression\nand emergence, which accomplishes the high-level and accurate user profiles\nfrom noisy and microscopic-level interactions.", "AI": {"tldr": "RGMem\u662f\u4e00\u4e2a\u53d7\u7269\u7406\u5b66\u91cd\u6574\u5316\u7fa4\u601d\u60f3\u542f\u53d1\u7684\u81ea\u8fdb\u5316\u8bb0\u5fc6\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u7ec4\u7ec7\u5bf9\u8bdd\u5386\u53f2\uff0c\u4ece\u5fae\u89c2\u4e92\u52a8\u4e2d\u63d0\u53d6\u9ad8\u5c42\u6b21\u7528\u6237\u753b\u50cf\uff0c\u5b9e\u73b0\u8bed\u8a00\u4ee3\u7406\u7684\u957f\u671f\u8bb0\u5fc6\u548c\u884c\u4e3a\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u89e3\u51b3\u65b9\u6848\uff08\u5982RAG\u548c\u663e\u5f0f\u8bb0\u5fc6\u7cfb\u7edf\uff09\u4e3b\u8981\u5173\u6ce8\u4e8b\u5b9e\u7ea7\u5b58\u50a8\u548c\u68c0\u7d22\uff0c\u7f3a\u4e4f\u4ece\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u63d0\u53d6\u6f5c\u5728\u504f\u597d\u548c\u6df1\u5c42\u7279\u5f81\u7684\u80fd\u529b\uff0c\u9650\u5236\u4e86\u957f\u671f\u6709\u6548\u7684\u7528\u6237\u5efa\u6a21\uff0c\u5bfc\u81f4\u4e2a\u6027\u5316\u4ea4\u4e92\u6d45\u5c42\u4e14\u7f3a\u4e4f\u8de8\u4f1a\u8bdd\u8fde\u7eed\u6027\u3002", "method": "RGMem\u6846\u67b6\u901a\u8fc7\u5206\u5c42\u7c97\u7c92\u5316\u548c\u91cd\u6807\u5ea6\u64cd\u4f5c\uff0c\u4ece\u7247\u6bb5\u5bf9\u8bdd\u4e2d\u63d0\u53d6\u8bed\u4e49\u548c\u7528\u6237\u6d1e\u5bdf\uff0c\u9010\u6b65\u5f62\u6210\u52a8\u6001\u6f14\u5316\u7684\u7528\u6237\u753b\u50cf\uff0c\u5c06\u8bb0\u5fc6\u6f14\u5316\u5efa\u6a21\u4e3a\u4fe1\u606f\u538b\u7f29\u548c\u6d8c\u73b0\u7684\u591a\u5c3a\u5ea6\u8fc7\u7a0b\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u4ece\u5608\u6742\u7684\u5fae\u89c2\u7ea7\u4e92\u52a8\u4e2d\u5b9e\u73b0\u9ad8\u5c42\u6b21\u3001\u51c6\u786e\u7684\u7528\u6237\u753b\u50cf\uff0c\u89e3\u51b3\u4e86\u6709\u9650\u4e0a\u4e0b\u6587\u7a97\u53e3\u548c\u9759\u6001\u53c2\u6570\u8bb0\u5fc6\u5e26\u6765\u7684\u8de8\u4f1a\u8bdd\u957f\u671f\u7528\u6237\u72b6\u6001\u5efa\u6a21\u56f0\u96be\u3002", "conclusion": "RGMem\u901a\u8fc7\u591a\u5c3a\u5ea6\u8bb0\u5fc6\u7ec4\u7ec7\u65b9\u6cd5\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u8bed\u8a00\u4ee3\u7406\u7684\u957f\u671f\u8bb0\u5fc6\u548c\u884c\u4e3a\u4e00\u81f4\u6027\uff0c\u4e3aLLM\u65f6\u4ee3\u7684\u4e2a\u6027\u5316\u4ea4\u4e92\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.17465", "categories": ["math.OC", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.17465", "abs": "https://arxiv.org/abs/2510.17465", "authors": ["Alberto De Marchi"], "title": "A condensing approach for linear-quadratic optimization with geometric constraints", "comment": "14 pages, 5 figures", "summary": "Optimization problems with convex quadratic cost and polyhedral constraints\nare ubiquitous in signal processing, automatic control and decision-making. We\nconsider here an enlarged problem class that allows to encode logical\nconditions and cardinality constraints, among others. In particular, we cover\nalso situations where parts of the constraints are nonconvex and possibly\ncomplicated, but it is practical to compute projections onto this nonconvex\nset. Our approach combines the augmented Lagrangian framework with a\nsolver-agnostic structure-exploiting subproblem reformulation. While\nconvergence guarantees follow from the former, the proposed condensing\ntechnique leads to significant improvements in computational performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5904\u7406\u5305\u542b\u903b\u8f91\u6761\u4ef6\u548c\u57fa\u6570\u7ea6\u675f\u7684\u51f8\u4e8c\u6b21\u4f18\u5316\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u589e\u5e7f\u62c9\u683c\u6717\u65e5\u6846\u67b6\u548c\u7ed3\u6784\u5229\u7528\u7684\u5b50\u95ee\u9898\u91cd\u6784\u6280\u672f\u3002", "motivation": "\u5728\u4fe1\u53f7\u5904\u7406\u3001\u81ea\u52a8\u63a7\u5236\u548c\u51b3\u7b56\u7b49\u9886\u57df\uff0c\u51f8\u4e8c\u6b21\u6210\u672c\u548c\u591a\u9762\u4f53\u7ea6\u675f\u7684\u4f18\u5316\u95ee\u9898\u975e\u5e38\u666e\u904d\u3002\u9700\u8981\u5904\u7406\u80fd\u591f\u7f16\u7801\u903b\u8f91\u6761\u4ef6\u548c\u57fa\u6570\u7ea6\u675f\u7684\u6269\u5c55\u95ee\u9898\u7c7b\u522b\uff0c\u5305\u62ec\u90e8\u5206\u7ea6\u675f\u4e3a\u975e\u51f8\u4e14\u590d\u6742\u4f46\u53ef\u4ee5\u8ba1\u7b97\u5230\u975e\u51f8\u96c6\u6295\u5f71\u7684\u60c5\u51b5\u3002", "method": "\u5c06\u589e\u5e7f\u62c9\u683c\u6717\u65e5\u6846\u67b6\u4e0e\u6c42\u89e3\u5668\u65e0\u5173\u7684\u7ed3\u6784\u5229\u7528\u5b50\u95ee\u9898\u91cd\u6784\u76f8\u7ed3\u5408\u3002\u901a\u8fc7\u63d0\u51fa\u7684\u538b\u7f29\u6280\u672f\u6539\u8fdb\u8ba1\u7b97\u6027\u80fd\u3002", "result": "\u589e\u5e7f\u62c9\u683c\u6717\u65e5\u6846\u67b6\u4fdd\u8bc1\u4e86\u6536\u655b\u6027\uff0c\u800c\u63d0\u51fa\u7684\u538b\u7f29\u6280\u672f\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u5305\u542b\u903b\u8f91\u6761\u4ef6\u548c\u57fa\u6570\u7ea6\u675f\u7684\u51f8\u4e8c\u6b21\u4f18\u5316\u95ee\u9898\uff0c\u5728\u4fdd\u8bc1\u6536\u655b\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2510.15992", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15992", "abs": "https://arxiv.org/abs/2510.15992", "authors": ["Ziming Dai", "Tuo Zhang", "Fei Gao", "Xingyi Cai", "Xiaofei Wang", "Cheng Zhang", "Wenyu Wang", "Chengjie Zang"], "title": "Stratos: An End-to-End Distillation Pipeline for Customized LLMs under Distributed Cloud Environments", "comment": null, "summary": "The growing industrial demand for customized and cost-efficient large\nlanguage models (LLMs) is fueled by the rise of vertical, domain-specific tasks\nand the need to optimize performance under constraints such as latency and\nbudget. Knowledge distillation, as an efficient model compression and transfer\ntechnique, offers a feasible solution. However, existing distillation\nframeworks often require manual intervention and struggle to meet such complex\nuser-defined distillation requirements. To bridge this gap, we propose Stratos,\nan end-to-end LLM distillation pipeline that automates server and model\nselection, knowledge distillation, and deployment in distributed cloud\nenvironments. Given user-defined constraints on model performance and system\nbudget, Stratos automatically selects Pareto-optimal servers, dynamically\nmatches teacher-student pairs, and adapts distillation strategies based on task\ncomplexity to optimize cloud hosting. Experiments show that Stratos produces a\nstudent model that achieves four times the accuracy of its GPT-4o teacher\nbaseline on a rare, domain-specific Mahjong reasoning task with reverse\nsynthetic data and knowledge injection. Moreover, it achieves reduced latency\nand cost without compromising accuracy. These results highlight its promise for\nvertical-domain LLM deployment.", "AI": {"tldr": "Stratos\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684LLM\u84b8\u998f\u7ba1\u9053\uff0c\u80fd\u81ea\u52a8\u9009\u62e9\u670d\u52a1\u5668\u548c\u6a21\u578b\u3001\u8fdb\u884c\u77e5\u8bc6\u84b8\u998f\u4ee5\u53ca\u5728\u5206\u5e03\u5f0f\u4e91\u73af\u5883\u4e2d\u90e8\u7f72\uff0c\u6ee1\u8db3\u7528\u6237\u5b9a\u4e49\u7684\u6a21\u578b\u6027\u80fd\u548c\u7cfb\u7edf\u9884\u7b97\u7ea6\u675f\u3002", "motivation": "\u5de5\u4e1a\u754c\u5bf9\u5b9a\u5236\u5316\u548c\u6210\u672c\u6548\u76ca\u9ad8\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9700\u6c42\u589e\u957f\uff0c\u73b0\u6709\u84b8\u998f\u6846\u67b6\u9700\u8981\u4eba\u5de5\u5e72\u9884\u4e14\u96be\u4ee5\u6ee1\u8db3\u590d\u6742\u7684\u7528\u6237\u5b9a\u4e49\u84b8\u998f\u9700\u6c42\u3002", "method": "Stratos\u81ea\u52a8\u9009\u62e9Pareto\u6700\u4f18\u670d\u52a1\u5668\uff0c\u52a8\u6001\u5339\u914d\u5e08\u751f\u5bf9\uff0c\u5e76\u6839\u636e\u4efb\u52a1\u590d\u6742\u5ea6\u8c03\u6574\u84b8\u998f\u7b56\u7565\u4ee5\u4f18\u5316\u4e91\u6258\u7ba1\u3002", "result": "\u5728\u7f55\u89c1\u7684\u9ebb\u5c06\u63a8\u7406\u4efb\u52a1\u4e0a\uff0cStratos\u751f\u6210\u7684\u5b66\u751f\u6a21\u578b\u51c6\u786e\u7387\u6bd4GPT-4o\u6559\u5e08\u57fa\u7ebf\u9ad8\u51fa\u56db\u500d\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u5ef6\u8fdf\u548c\u6210\u672c\u800c\u4e0d\u5f71\u54cd\u51c6\u786e\u6027\u3002", "conclusion": "Stratos\u5c55\u793a\u4e86\u5728\u5782\u76f4\u9886\u57dfLLM\u90e8\u7f72\u4e2d\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u9ad8\u6548\u6ee1\u8db3\u5b9a\u5236\u5316\u9700\u6c42\u3002"}}
{"id": "2510.16565", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16565", "abs": "https://arxiv.org/abs/2510.16565", "authors": ["Seungho Cho", "Changgeon Ko", "Eui Jun Hwang", "Junmyeong Lee", "Huije Lee", "Jong C. Park"], "title": "Language over Content: Tracing Cultural Understanding in Multilingual Large Language Models", "comment": "Accepted to CIKM 2025 Workshop on Human Centric AI", "summary": "Large language models (LLMs) are increasingly used across diverse cultural\ncontexts, making accurate cultural understanding essential. Prior evaluations\nhave mostly focused on output-level performance, obscuring the factors that\ndrive differences in responses, while studies using circuit analysis have\ncovered few languages and rarely focused on culture. In this work, we trace\nLLMs' internal cultural understanding mechanisms by measuring activation path\noverlaps when answering semantically equivalent questions under two conditions:\nvarying the target country while fixing the question language, and varying the\nquestion language while fixing the country. We also use same-language country\npairs to disentangle language from cultural aspects. Results show that internal\npaths overlap more for same-language, cross-country questions than for\ncross-language, same-country questions, indicating strong language-specific\npatterns. Notably, the South Korea-North Korea pair exhibits low overlap and\nhigh variability, showing that linguistic similarity does not guarantee aligned\ninternal representation.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5206\u6790LLMs\u5728\u56de\u7b54\u6587\u5316\u76f8\u5173\u95ee\u9898\u65f6\u5185\u90e8\u6fc0\u6d3b\u8def\u5f84\u7684\u91cd\u53e0\u7a0b\u5ea6\uff0c\u7814\u7a76\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u6587\u5316\u7406\u89e3\u673a\u5236\uff0c\u53d1\u73b0\u8bed\u8a00\u7279\u5f02\u6027\u6bd4\u6587\u5316\u7279\u5f02\u6027\u5bf9\u5185\u90e8\u8868\u5f81\u7684\u5f71\u54cd\u66f4\u5927\u3002", "motivation": "\u968f\u7740LLMs\u5728\u591a\u5143\u6587\u5316\u80cc\u666f\u4e0b\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u51c6\u786e\u7684\u6587\u5316\u7406\u89e3\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u8f93\u51fa\u5c42\u9762\u7684\u8868\u73b0\uff0c\u800c\u5f88\u5c11\u63a2\u7a76\u5bfc\u81f4\u54cd\u5e94\u5dee\u5f02\u7684\u5185\u90e8\u56e0\u7d20\uff0c\u7279\u522b\u662f\u6587\u5316\u76f8\u5173\u7684\u7535\u8def\u5206\u6790\u7814\u7a76\u8986\u76d6\u8bed\u8a00\u5c11\u4e14\u5f88\u5c11\u805a\u7126\u6587\u5316\u3002", "method": "\u901a\u8fc7\u6d4b\u91cfLLMs\u5728\u56de\u7b54\u8bed\u4e49\u7b49\u4ef7\u95ee\u9898\u65f6\u5185\u90e8\u6fc0\u6d3b\u8def\u5f84\u7684\u91cd\u53e0\u7a0b\u5ea6\uff0c\u6bd4\u8f83\u4e24\u79cd\u6761\u4ef6\uff1a(1)\u6539\u53d8\u76ee\u6807\u56fd\u5bb6\u4f46\u56fa\u5b9a\u95ee\u9898\u8bed\u8a00\uff1b(2)\u6539\u53d8\u95ee\u9898\u8bed\u8a00\u4f46\u56fa\u5b9a\u56fd\u5bb6\u3002\u540c\u65f6\u4f7f\u7528\u540c\u8bed\u8a00\u56fd\u5bb6\u5bf9\u6765\u5206\u79bb\u8bed\u8a00\u548c\u6587\u5316\u56e0\u7d20\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u76f8\u540c\u8bed\u8a00\u3001\u4e0d\u540c\u56fd\u5bb6\u7684\u95ee\u9898\u6bd4\u4e0d\u540c\u8bed\u8a00\u3001\u76f8\u540c\u56fd\u5bb6\u7684\u95ee\u9898\u5728\u5185\u90e8\u8def\u5f84\u4e0a\u91cd\u53e0\u66f4\u591a\uff0c\u8868\u660e\u5b58\u5728\u5f3a\u70c8\u7684\u8bed\u8a00\u7279\u5b9a\u6a21\u5f0f\u3002\u7279\u522b\u5730\uff0c\u97e9\u56fd-\u671d\u9c9c\u5bf9\u7684\u8def\u5f84\u91cd\u53e0\u5ea6\u4f4e\u4e14\u53d8\u5f02\u6027\u9ad8\uff0c\u8868\u660e\u8bed\u8a00\u76f8\u4f3c\u6027\u4e0d\u80fd\u4fdd\u8bc1\u5185\u90e8\u8868\u5f81\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "LLMs\u7684\u5185\u90e8\u6587\u5316\u7406\u89e3\u673a\u5236\u53d7\u8bed\u8a00\u7279\u5f02\u6027\u5f71\u54cd\u5927\u4e8e\u6587\u5316\u7279\u5f02\u6027\uff0c\u8bed\u8a00\u76f8\u4f3c\u6027\u4e0d\u80fd\u786e\u4fdd\u5185\u90e8\u8868\u5f81\u7684\u5bf9\u9f50\uff0c\u8fd9\u5bf9\u8de8\u6587\u5316\u5e94\u7528\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2510.16064", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.16064", "abs": "https://arxiv.org/abs/2510.16064", "authors": ["Muhy Eddin Za'ter", "Bri-Mathias Hodge", "Kyri Baker"], "title": "Residual Correction Models for AC Optimal Power Flow Using DC Optimal Power Flow Solutions", "comment": null, "summary": "Solving the nonlinear AC optimal power flow (AC OPF) problem remains a major\ncomputational bottleneck for real-time grid operations. In this paper, we\npropose a residual learning paradigm that uses fast DC optimal power flow (DC\nOPF) solutions as a baseline, and learns only the nonlinear corrections\nrequired to provide the full AC-OPF solution. The method utilizes a\ntopology-aware Graph Neural Network with local attention and two-level DC\nfeature integration, trained using a physics-informed loss that enforces AC\npower-flow feasibility and operational limits. Evaluations on OPFData for 57-,\n118-, and 2000-bus systems show around 25% lower MSE, up to 3X reduction in\nfeasibility error, and up to 13X runtime speedup compared to conventional AC\nOPF solvers. The model maintains accuracy under N-1 contingencies and scales\nefficiently to large networks. These results demonstrate that residual learning\nis a practical and scalable bridge between linear approximations and\nAC-feasible OPF, enabling near real-time operational decision making.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6b8b\u5dee\u5b66\u4e60\u7684AC\u6700\u4f18\u6f6e\u6d41\u6c42\u89e3\u65b9\u6cd5\uff0c\u4f7f\u7528DC OPF\u89e3\u4f5c\u4e3a\u57fa\u7ebf\uff0c\u5b66\u4e60\u975e\u7ebf\u6027\u4fee\u6b63\u9879\uff0c\u5b9e\u73b0\u5feb\u901f\u51c6\u786e\u7684AC OPF\u6c42\u89e3\u3002", "motivation": "\u4f20\u7edf\u975e\u7ebf\u6027AC\u6700\u4f18\u6f6e\u6d41\u8ba1\u7b97\u662f\u7535\u7f51\u5b9e\u65f6\u8fd0\u884c\u7684\u4e3b\u8981\u8ba1\u7b97\u74f6\u9888\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u6c42\u89e3\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u62d3\u6251\u611f\u77e5\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u7ed3\u5408\u5c40\u90e8\u6ce8\u610f\u529b\u548c\u4e24\u7ea7DC\u7279\u5f81\u96c6\u6210\uff0c\u4f7f\u7528\u7269\u7406\u4fe1\u606f\u635f\u5931\u51fd\u6570\u786e\u4fddAC\u6f6e\u6d41\u53ef\u884c\u6027\u548c\u8fd0\u884c\u9650\u5236\u3002", "result": "\u572857\u3001118\u548c2000\u6bcd\u7ebf\u7cfb\u7edf\u6d4b\u8bd5\u4e2d\uff0cMSE\u964d\u4f4e\u7ea625%\uff0c\u53ef\u884c\u6027\u8bef\u5dee\u51cf\u5c113\u500d\uff0c\u8fd0\u884c\u901f\u5ea6\u63d0\u534713\u500d\uff0c\u4e14\u5728N-1\u6545\u969c\u4e0b\u4fdd\u6301\u51c6\u786e\u6027\u3002", "conclusion": "\u6b8b\u5dee\u5b66\u4e60\u662f\u8fde\u63a5\u7ebf\u6027\u8fd1\u4f3c\u4e0eAC\u53ef\u884cOPF\u7684\u5b9e\u7528\u53ef\u6269\u5c55\u6865\u6881\uff0c\u53ef\u5b9e\u73b0\u8fd1\u5b9e\u65f6\u8fd0\u884c\u51b3\u7b56\u3002"}}
{"id": "2510.16185", "categories": ["cs.LG", "cs.AI", "cs.FL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16185", "abs": "https://arxiv.org/abs/2510.16185", "authors": ["Daniel Donnelly", "Angelo Ferrando", "Francesco Belardinelli"], "title": "Expressive Reward Synthesis with the Runtime Monitoring Language", "comment": null, "summary": "A key challenge in reinforcement learning (RL) is reward (mis)specification,\nwhereby imprecisely defined reward functions can result in unintended, possibly\nharmful, behaviours. Indeed, reward functions in RL are typically treated as\nblack-box mappings from state-action pairs to scalar values. While effective in\nmany settings, this approach provides no information about why rewards are\ngiven, which can hinder learning and interpretability. Reward Machines address\nthis issue by representing reward functions as finite state automata, enabling\nthe specification of structured, non-Markovian reward functions. However, their\nexpressivity is typically bounded by regular languages, leaving them unable to\ncapture more complex behaviours such as counting or parametrised conditions. In\nthis work, we build on the Runtime Monitoring Language (RML) to develop a novel\nclass of language-based Reward Machines. By leveraging the built-in memory of\nRML, our approach can specify reward functions for non-regular, non-Markovian\ntasks. We demonstrate the expressiveness of our approach through experiments,\nhighlighting additional advantages in flexible event-handling and task\nspecification over existing Reward Machine-based methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fd0\u884c\u65f6\u76d1\u63a7\u8bed\u8a00(RML)\u7684\u65b0\u578b\u8bed\u8a00\u5956\u52b1\u673a\uff0c\u80fd\u591f\u8868\u8fbe\u975e\u6b63\u5219\u3001\u975e\u9a6c\u5c14\u53ef\u592b\u4efb\u52a1\u7684\u5956\u52b1\u51fd\u6570\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5956\u52b1\u673a\u8868\u8fbe\u80fd\u529b\u53d7\u9650\u7684\u95ee\u9898\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5956\u52b1\u51fd\u6570\u901a\u5e38\u88ab\u89c6\u4e3a\u9ed1\u76d2\u6620\u5c04\uff0c\u7f3a\u4e4f\u89e3\u91ca\u6027\uff0c\u800c\u4f20\u7edf\u5956\u52b1\u673a\u53ea\u80fd\u8868\u8fbe\u6b63\u5219\u8bed\u8a00\uff0c\u65e0\u6cd5\u5904\u7406\u8ba1\u6570\u6216\u53c2\u6570\u5316\u6761\u4ef6\u7b49\u590d\u6742\u884c\u4e3a\u3002", "method": "\u5229\u7528RML\u7684\u5185\u7f6e\u5185\u5b58\u673a\u5236\u6784\u5efa\u8bed\u8a00\u5956\u52b1\u673a\uff0c\u6269\u5c55\u4e86\u5956\u52b1\u51fd\u6570\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u652f\u6301\u975e\u6b63\u5219\u3001\u975e\u9a6c\u5c14\u53ef\u592b\u4efb\u52a1\u7684\u89c4\u8303\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u8868\u8fbe\u80fd\u529b\u4e0a\u4f18\u4e8e\u73b0\u6709\u5956\u52b1\u673a\u65b9\u6cd5\uff0c\u5728\u4e8b\u4ef6\u5904\u7406\u548c\u4efb\u52a1\u89c4\u8303\u65b9\u9762\u5177\u6709\u989d\u5916\u4f18\u52bf\u3002", "conclusion": "\u57fa\u4e8eRML\u7684\u8bed\u8a00\u5956\u52b1\u673a\u4e3a\u590d\u6742\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u548c\u7075\u6d3b\u7684\u5956\u52b1\u51fd\u6570\u89c4\u8303\u65b9\u6cd5\u3002"}}
{"id": "2510.17712", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2510.17712", "abs": "https://arxiv.org/abs/2510.17712", "authors": ["Frederik J. Zuiderveen Borgesius", "Judith M\u00f6ller", "Sanne Kruikemeier", "Ronan \u00d3 Fathaigh", "Kristina Irion", "Tom Dobber", "Balazs Bodo", "Claes de Vreese"], "title": "Online Political Microtargeting: Promises and Threats for Democracy", "comment": null, "summary": "Online political microtargeting involves monitoring people's online\nbehaviour, and using the collected data, sometimes enriched with other data, to\nshow people-targeted political advertisements. Online political microtargeting\nis widely used in the US; Europe may not be far behind. This paper maps\nmicrotargeting's promises and threats to democracy. For example, microtargeting\npromises to optimise the match between the electorate's concerns and political\ncampaigns, and to boost campaign engagement and political participation. But\nonline microtargeting could also threaten democracy. For instance, a political\nparty could, misleadingly, present itself as a different one-issue party to\ndifferent individuals. And data collection for microtargeting raises privacy\nconcerns. We sketch possibilities for policymakers if they seek to regulate\nonline political microtargeting. We discuss which measures would be possible,\nwhile complying with the right to freedom of expression under the European\nConvention on Human Rights.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u5728\u7ebf\u653f\u6cbb\u5fae\u5b9a\u5411\u5bf9\u6c11\u4e3b\u7684\u627f\u8bfa\u4e0e\u5a01\u80c1\uff0c\u63a2\u8ba8\u4e86\u5176\u4f18\u5316\u7ade\u9009\u5339\u914d\u548c\u63d0\u5347\u53c2\u4e0e\u5ea6\u7684\u6f5c\u529b\uff0c\u540c\u65f6\u4e5f\u6307\u51fa\u4e86\u5176\u53ef\u80fd\u5bfc\u81f4\u7684\u8bef\u5bfc\u6027\u5ba3\u4f20\u548c\u9690\u79c1\u95ee\u9898\uff0c\u5e76\u4e3a\u653f\u7b56\u5236\u5b9a\u8005\u63d0\u4f9b\u4e86\u5408\u89c4\u7684\u76d1\u7ba1\u5efa\u8bae\u3002", "motivation": "\u5728\u7ebf\u653f\u6cbb\u5fae\u5b9a\u5411\u5728\u7f8e\u56fd\u5e7f\u6cdb\u4f7f\u7528\uff0c\u6b27\u6d32\u4e5f\u53ef\u80fd\u8ddf\u8fdb\u3002\u672c\u6587\u65e8\u5728\u7cfb\u7edf\u8bc4\u4f30\u5fae\u5b9a\u5411\u5bf9\u6c11\u4e3b\u7684\u6f5c\u5728\u5f71\u54cd\uff0c\u5305\u62ec\u5176\u79ef\u6781\u548c\u6d88\u6781\u65b9\u9762\uff0c\u4e3a\u653f\u7b56\u5236\u5b9a\u63d0\u4f9b\u53c2\u8003\u3002", "method": "\u901a\u8fc7\u6620\u5c04\u5206\u6790\u7684\u65b9\u6cd5\uff0c\u7cfb\u7edf\u68b3\u7406\u5fae\u5b9a\u5411\u5bf9\u6c11\u4e3b\u7684\u627f\u8bfa\uff08\u5982\u4f18\u5316\u7ade\u9009\u5339\u914d\u3001\u63d0\u5347\u53c2\u4e0e\u5ea6\uff09\u548c\u5a01\u80c1\uff08\u5982\u8bef\u5bfc\u6027\u5ba3\u4f20\u3001\u9690\u79c1\u4fb5\u72af\uff09\uff0c\u5e76\u63a2\u8ba8\u5728\u7b26\u5408\u6b27\u6d32\u4eba\u6743\u516c\u7ea6\u8868\u8fbe\u81ea\u7531\u6743\u7684\u524d\u63d0\u4e0b\u53ef\u80fd\u7684\u76d1\u7ba1\u63aa\u65bd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5fae\u5b9a\u5411\u65e2\u80fd\u589e\u5f3a\u653f\u6cbb\u53c2\u4e0e\u548c\u7ade\u9009\u6548\u7387\uff0c\u4e5f\u53ef\u80fd\u5bfc\u81f4\u653f\u6cbb\u6b3a\u9a97\u548c\u9690\u79c1\u98ce\u9669\u3002\u653f\u7b56\u5236\u5b9a\u8005\u9700\u8981\u5728\u4fdd\u62a4\u8868\u8fbe\u81ea\u7531\u7684\u540c\u65f6\uff0c\u8003\u8651\u900f\u660e\u5ea6\u8981\u6c42\u3001\u6570\u636e\u4fdd\u62a4\u7b49\u63aa\u65bd\u6765\u76d1\u7ba1\u5fae\u5b9a\u5411\u3002", "conclusion": "\u5728\u7ebf\u653f\u6cbb\u5fae\u5b9a\u5411\u662f\u4e00\u628a\u53cc\u5203\u5251\uff0c\u65e2\u6709\u4fc3\u8fdb\u6c11\u4e3b\u7684\u6f5c\u529b\uff0c\u4e5f\u6709\u5a01\u80c1\u6c11\u4e3b\u7684\u98ce\u9669\u3002\u6709\u6548\u7684\u76d1\u7ba1\u9700\u8981\u5728\u4fdd\u969c\u8868\u8fbe\u81ea\u7531\u7684\u524d\u63d0\u4e0b\uff0c\u5e73\u8861\u5176\u5229\u5f0a\uff0c\u786e\u4fdd\u5fae\u5b9a\u5411\u7684\u8d1f\u8d23\u4efb\u4f7f\u7528\u3002"}}
{"id": "2510.16466", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16466", "abs": "https://arxiv.org/abs/2510.16466", "authors": ["Siddhartha Krothapalli", "Tridib Kumar Das", "Praveen Kumar", "Naveen Suravarpu", "Pratik Narang"], "title": "ReviewSense: Transforming Customer Review Dynamics into Actionable Business Insights", "comment": "11 pages, 1 figure, 4 tables", "summary": "As customer feedback becomes increasingly central to strategic growth, the\nability to derive actionable insights from unstructured reviews is essential.\nWhile traditional AI-driven systems excel at predicting user preferences, far\nless work has focused on transforming customer reviews into prescriptive,\nbusiness-facing recommendations. This paper introduces ReviewSense, a novel\nprescriptive decision support framework that leverages advanced large language\nmodels (LLMs) to transform customer reviews into targeted, actionable business\nrecommendations. By identifying key trends, recurring issues, and specific\nconcerns within customer sentiments, ReviewSense extends beyond\npreference-based systems to provide businesses with deeper insights for\nsustaining growth and enhancing customer loyalty. The novelty of this work lies\nin integrating clustering, LLM adaptation, and expert-driven evaluation into a\nunified, business-facing pipeline. Preliminary manual evaluations indicate\nstrong alignment between the model's recommendations and business objectives,\nhighlighting its potential for driving data-informed decision-making. This\nframework offers a new perspective on AI-driven sentiment analysis,\ndemonstrating its value in refining business strategies and maximizing the\nimpact of customer feedback.", "AI": {"tldr": "ReviewSense\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u51b3\u7b56\u652f\u6301\u6846\u67b6\uff0c\u80fd\u591f\u5c06\u5ba2\u6237\u8bc4\u8bba\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u7684\u5546\u4e1a\u5efa\u8bae\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u504f\u597d\u9884\u6d4b\u7cfb\u7edf\u3002", "motivation": "\u4f20\u7edfAI\u7cfb\u7edf\u64c5\u957f\u9884\u6d4b\u7528\u6237\u504f\u597d\uff0c\u4f46\u7f3a\u4e4f\u5c06\u5ba2\u6237\u8bc4\u8bba\u8f6c\u5316\u4e3a\u5546\u4e1a\u5efa\u8bae\u7684\u80fd\u529b\uff0c\u800c\u5ba2\u6237\u53cd\u9988\u5bf9\u6218\u7565\u589e\u957f\u81f3\u5173\u91cd\u8981\u3002", "method": "\u6574\u5408\u805a\u7c7b\u3001LLM\u9002\u914d\u548c\u4e13\u5bb6\u9a71\u52a8\u8bc4\u4f30\u7684\u7edf\u4e00\u4e1a\u52a1\u5bfc\u5411\u6d41\u7a0b\uff0c\u8bc6\u522b\u5ba2\u6237\u60c5\u7eea\u4e2d\u7684\u5173\u952e\u8d8b\u52bf\u3001\u91cd\u590d\u95ee\u9898\u548c\u5177\u4f53\u5173\u6ce8\u70b9\u3002", "result": "\u521d\u6b65\u4eba\u5de5\u8bc4\u4f30\u663e\u793a\u6a21\u578b\u5efa\u8bae\u4e0e\u5546\u4e1a\u76ee\u6807\u9ad8\u5ea6\u4e00\u81f4\uff0c\u5177\u6709\u63a8\u52a8\u6570\u636e\u9a71\u52a8\u51b3\u7b56\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aAI\u9a71\u52a8\u7684\u60c5\u611f\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5728\u4f18\u5316\u5546\u4e1a\u7b56\u7565\u548c\u6700\u5927\u5316\u5ba2\u6237\u53cd\u9988\u5f71\u54cd\u529b\u65b9\u9762\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2510.17551", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2510.17551", "abs": "https://arxiv.org/abs/2510.17551", "authors": ["Benjamin Marsh"], "title": "Towards Optimal Control and Algorithmic Structure of Decompression Schedules", "comment": null, "summary": "We formalise decompression planning as an optimal control problem with gas\nfeasibility windows (ppO$_2$, END), affine ceilings, and convex penalties in\nnormalised oversaturation. We prove existence, a monotone no re-descent\nstructure and bang-bang ascents under a mild monotonicity assumption on inert\nfraction, and establish dwell time KKT conditions. We give pseudo-polynomial DP\nand label-setting algorithms with a priori error bounds, derive Lipschitz\nregularity of the online value function, and discuss multi-species extensions.\nThe efficient frontier is continuous and generally nonconvex. We provide the\nfirst formal existence and bang-bang structure proof under mixed gas\nfeasibility windows.", "AI": {"tldr": "\u672c\u6587\u5f62\u5f0f\u5316\u5730\u5c06\u51cf\u538b\u89c4\u5212\u5efa\u6a21\u4e3a\u5177\u6709\u6c14\u4f53\u53ef\u884c\u6027\u7a97\u53e3\u3001\u4eff\u5c04\u4e0a\u9650\u548c\u51f8\u60e9\u7f5a\u7684\u6700\u4f18\u63a7\u5236\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u5b58\u5728\u6027\u3001\u5355\u8c03\u65e0\u91cd\u964d\u7ed3\u6784\u548cbang-bang\u4e0a\u5347\u7279\u6027\uff0c\u5e76\u7ed9\u51fa\u4e86\u4f2a\u591a\u9879\u5f0f\u52a8\u6001\u89c4\u5212\u548c\u6807\u7b7e\u8bbe\u7f6e\u7b97\u6cd5\u3002", "motivation": "\u4e3a\u51cf\u538b\u89c4\u5212\u63d0\u4f9b\u4e25\u683c\u7684\u7406\u8bba\u57fa\u7840\uff0c\u7279\u522b\u662f\u5728\u6df7\u5408\u6c14\u4f53\u53ef\u884c\u6027\u7a97\u53e3\u6761\u4ef6\u4e0b\uff0c\u8fd9\u662f\u9996\u6b21\u5728\u6df7\u5408\u6c14\u4f53\u573a\u666f\u4e0b\u5f62\u5f0f\u5316\u8bc1\u660e\u5b58\u5728\u6027\u548cbang-bang\u7ed3\u6784\u3002", "method": "\u5c06\u51cf\u538b\u89c4\u5212\u5efa\u6a21\u4e3a\u6700\u4f18\u63a7\u5236\u95ee\u9898\uff0c\u4f7f\u7528\u6c14\u4f53\u53ef\u884c\u6027\u7a97\u53e3(ppO\u2082, END)\u3001\u4eff\u5c04\u4e0a\u9650\u548c\u51f8\u60e9\u7f5a\u51fd\u6570\uff0c\u5f00\u53d1\u4e86\u4f2a\u591a\u9879\u5f0f\u52a8\u6001\u89c4\u5212\u548c\u6807\u7b7e\u8bbe\u7f6e\u7b97\u6cd5\u3002", "result": "\u8bc1\u660e\u4e86\u6700\u4f18\u89e3\u7684\u5b58\u5728\u6027\u3001\u5355\u8c03\u65e0\u91cd\u964d\u7ed3\u6784\u548cbang-bang\u4e0a\u5347\u7279\u6027\uff0c\u5efa\u7acb\u4e86\u505c\u7559\u65f6\u95f4\u7684KKT\u6761\u4ef6\uff0c\u7b97\u6cd5\u5177\u6709\u5148\u9a8c\u8bef\u5dee\u754c\uff0c\u5728\u7ebf\u503c\u51fd\u6570\u5177\u6709Lipschitz\u6b63\u5219\u6027\u3002", "conclusion": "\u6709\u6548\u524d\u6cbf\u662f\u8fde\u7eed\u4e14\u901a\u5e38\u975e\u51f8\u7684\uff0c\u4e3a\u6df7\u5408\u6c14\u4f53\u51cf\u538b\u89c4\u5212\u63d0\u4f9b\u4e86\u9996\u4e2a\u5f62\u5f0f\u5316\u5b58\u5728\u6027\u548cbang-bang\u7ed3\u6784\u8bc1\u660e\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u5960\u5b9a\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2510.15996", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15996", "abs": "https://arxiv.org/abs/2510.15996", "authors": ["Ozan K. Tonguz", "Federico Taschin"], "title": "Using Kolmogorov-Smirnov Distance for Measuring Distribution Shift in Machine Learning", "comment": null, "summary": "One of the major problems in Machine Learning (ML) and Artificial\nIntelligence (AI) is the fact that the probability distribution of the test\ndata in the real world could deviate substantially from the probability\ndistribution of the training data set. When this happens, the predictions of an\nML system or an AI agent could involve large errors which is very troublesome\nand undesirable. While this is a well-known hard problem plaguing the AI and ML\nsystems' accuracy and reliability, in certain applications such errors could be\ncritical for safety and reliability of AI and ML systems. One approach to deal\nwith this problem is to monitor and measure the deviation in the probability\ndistribution of the test data in real time and to compensate for this\ndeviation. In this paper, we propose and explore the use of Kolmogorov-Smirnov\n(KS) Test for measuring the distribution shift and we show how the KS distance\ncan be used to quantify the distribution shift and its impact on an AI agent's\nperformance. Our results suggest that KS distance could be used as a valuable\nstatistical tool for monitoring and measuring the distribution shift. More\nspecifically, it is shown that even a distance of KS=0.02 could lead to about\n50\\% increase in the travel time at a single intersection using a Reinforcement\nLearning agent which is quite significant. It is hoped that the use of KS Test\nand KS distance in AI-based smart transportation could be an important step\nforward for gauging the performance degradation of an AI agent in real time and\nthis, in turn, could help the AI agent to cope with the distribution shift in a\nmore informed manner.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4f7f\u7528Kolmogorov-Smirnov\u68c0\u9a8c\u6765\u76d1\u6d4b\u548c\u91cf\u5316\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u4e2d\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\uff0c\u5373\u4f7f\u5f88\u5c0f\u7684KS\u8ddd\u79bb\uff080.02\uff09\u4e5f\u53ef\u80fd\u5bfc\u81f4\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff08\u65c5\u884c\u65f6\u95f4\u589e\u52a050%\uff09\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u4e2d\u8bad\u7ec3\u6570\u636e\u4e0e\u6d4b\u8bd5\u6570\u636e\u6982\u7387\u5206\u5e03\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u8fd9\u79cd\u5206\u5e03\u504f\u79fb\u4f1a\u5bfc\u81f4\u9884\u6d4b\u8bef\u5dee\uff0c\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u5c24\u4e3a\u4e25\u91cd\u3002", "method": "\u4f7f\u7528Kolmogorov-Smirnov\u68c0\u9a8c\u6765\u6d4b\u91cf\u5206\u5e03\u504f\u79fb\uff0c\u901a\u8fc7KS\u8ddd\u79bb\u91cf\u5316\u5206\u5e03\u53d8\u5316\u53ca\u5176\u5bf9AI\u4ee3\u7406\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5373\u4f7fKS\u8ddd\u79bb\u4ec5\u4e3a0.02\uff0c\u5728\u5355\u4ea4\u53c9\u53e3\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u65f6\u4e5f\u4f1a\u5bfc\u81f4\u65c5\u884c\u65f6\u95f4\u589e\u52a0\u7ea650%\uff0c\u5f71\u54cd\u663e\u8457\u3002", "conclusion": "KS\u68c0\u9a8c\u548cKS\u8ddd\u79bb\u53ef\u4f5c\u4e3a\u5b9e\u65f6\u76d1\u6d4bAI\u4ee3\u7406\u6027\u80fd\u9000\u5316\u7684\u6709\u4ef7\u503c\u7edf\u8ba1\u5de5\u5177\uff0c\u6709\u52a9\u4e8eAI\u7cfb\u7edf\u66f4\u6709\u6548\u5730\u5e94\u5bf9\u5206\u5e03\u504f\u79fb\u95ee\u9898\u3002"}}
{"id": "2510.16567", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.16567", "abs": "https://arxiv.org/abs/2510.16567", "authors": ["Alkis Koudounas", "Moreno La Quatra", "Manuel Giollo", "Sabato Marco Siniscalchi", "Elena Baralis"], "title": "Hallucination Benchmark for Speech Foundation Models", "comment": "Under Review", "summary": "Hallucinations in automatic speech recognition (ASR) systems refer to fluent\nand coherent transcriptions produced by neural ASR models that are completely\nunrelated to the underlying acoustic input (i.e., the speech signal). While\nsimilar to conventional decoding errors in potentially compromising the\nusability of transcriptions for downstream applications, hallucinations can be\nmore detrimental due to their preservation of syntactically and semantically\nplausible structure. This apparent coherence can mislead subsequent processing\nstages and introduce serious risks, particularly in critical domains such as\nhealthcare and law. Conventional evaluation metrics are primarily centered on\nerror-based metrics and fail to distinguish between phonetic inaccuracies and\nhallucinations. Consequently, there is a critical need for new evaluation\nframeworks that can effectively identify and assess models with a heightened\npropensity for generating hallucinated content. To this end, we introduce\nSHALLOW, the first benchmark framework that systematically categorizes and\nquantifies hallucination phenomena in ASR along four complementary axes:\nlexical, phonetic, morphological, and semantic. We define targeted metrics\nwithin each category to produce interpretable profiles of model behavior.\nThrough evaluation across various architectures and speech domains, we have\nfound that SHALLOW metrics correlate strongly with word error rate (WER) when\nrecognition quality is high (i.e., low WER). Still, this correlation weakens\nsubstantially as WER increases. SHALLOW, therefore, captures fine-grained error\npatterns that WER fails to distinguish under degraded and challenging\nconditions. Our framework supports specific diagnosis of model weaknesses and\nprovides feedback for model improvement beyond what aggregate error rates can\noffer.", "AI": {"tldr": "SHALLOW\u662f\u9996\u4e2a\u7cfb\u7edf\u5206\u7c7b\u548c\u91cf\u5316ASR\u7cfb\u7edf\u4e2d\u5e7b\u89c9\u73b0\u8c61\u7684\u57fa\u51c6\u6846\u67b6\uff0c\u901a\u8fc7\u8bcd\u6c47\u3001\u8bed\u97f3\u3001\u5f62\u6001\u548c\u8bed\u4e49\u56db\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u6a21\u578b\u884c\u4e3a\uff0c\u5728WER\u8f83\u9ad8\u65f6\u80fd\u6355\u6349WER\u65e0\u6cd5\u533a\u5206\u7684\u7ec6\u7c92\u5ea6\u9519\u8bef\u6a21\u5f0f\u3002", "motivation": "ASR\u7cfb\u7edf\u4e2d\u7684\u5e7b\u89c9\u4f1a\u4ea7\u751f\u8bed\u6cd5\u8bed\u4e49\u5408\u7406\u4f46\u4e0e\u8bed\u97f3\u8f93\u5165\u65e0\u5173\u7684\u8f6c\u5f55\uff0c\u5728\u533b\u7597\u3001\u6cd5\u5f8b\u7b49\u5173\u952e\u9886\u57df\u5e26\u6765\u4e25\u91cd\u98ce\u9669\u3002\u4f20\u7edf\u8bc4\u4f30\u6307\u6807\u65e0\u6cd5\u533a\u5206\u8bed\u97f3\u4e0d\u51c6\u786e\u548c\u5e7b\u89c9\uff0c\u9700\u8981\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63d0\u51faSHALLOW\u6846\u67b6\uff0c\u5728\u8bcd\u6c47\u3001\u8bed\u97f3\u3001\u5f62\u6001\u548c\u8bed\u4e49\u56db\u4e2a\u4e92\u8865\u7ef4\u5ea6\u4e0a\u5b9a\u4e49\u9488\u5bf9\u6027\u6307\u6807\uff0c\u751f\u6210\u53ef\u89e3\u91ca\u7684\u6a21\u578b\u884c\u4e3a\u914d\u7f6e\u6587\u4ef6\u3002", "result": "SHALLOW\u6307\u6807\u5728\u8bc6\u522b\u8d28\u91cf\u9ad8\u65f6\u4e0eWER\u5f3a\u76f8\u5173\uff0c\u4f46\u968f\u7740WER\u589e\u52a0\u76f8\u5173\u6027\u663e\u8457\u51cf\u5f31\uff0c\u80fd\u5728\u9000\u5316\u6761\u4ef6\u4e0b\u6355\u6349WER\u65e0\u6cd5\u533a\u5206\u7684\u7ec6\u7c92\u5ea6\u9519\u8bef\u6a21\u5f0f\u3002", "conclusion": "SHALLOW\u652f\u6301\u7279\u5b9a\u6a21\u578b\u5f31\u70b9\u7684\u8bca\u65ad\uff0c\u4e3a\u6a21\u578b\u6539\u8fdb\u63d0\u4f9b\u8d85\u51fa\u805a\u5408\u9519\u8bef\u7387\u7684\u53cd\u9988\uff0c\u662f\u8bc4\u4f30ASR\u7cfb\u7edf\u5e7b\u89c9\u503e\u5411\u7684\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2510.16208", "categories": ["cs.LG", "cs.SY", "eess.SY", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16208", "abs": "https://arxiv.org/abs/2510.16208", "authors": ["Sunmook Choi", "Yahya Sattar", "Yassir Jedra", "Maryam Fazel", "Sarah Dean"], "title": "Explore-then-Commit for Nonstationary Linear Bandits with Latent Dynamics", "comment": null, "summary": "We study a nonstationary bandit problem where rewards depend on both actions\nand latent states, the latter governed by unknown linear dynamics. Crucially,\nthe state dynamics also depend on the actions, resulting in tension between\nshort-term and long-term rewards. We propose an explore-then-commit algorithm\nfor a finite horizon $T$. During the exploration phase, random Rademacher\nactions enable estimation of the Markov parameters of the linear dynamics,\nwhich characterize the action-reward relationship. In the commit phase, the\nalgorithm uses the estimated parameters to design an optimized action sequence\nfor long-term reward. Our proposed algorithm achieves\n$\\tilde{\\mathcal{O}}(T^{2/3})$ regret. Our analysis handles two key challenges:\nlearning from temporally correlated rewards, and designing action sequences\nwith optimal long-term reward. We address the first challenge by providing\nnear-optimal sample complexity and error bounds for system identification using\nbilinear rewards. We address the second challenge by proving an equivalence\nwith indefinite quadratic optimization over a hypercube, a known NP-hard\nproblem. We provide a sub-optimality guarantee for this problem, enabling our\nregret upper bound. Lastly, we propose a semidefinite relaxation with\nGoemans-Williamson rounding as a practical approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u975e\u5e73\u7a33\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\u7684\u63a2\u7d22-\u63d0\u4ea4\u7b97\u6cd5\uff0c\u8be5\u95ee\u9898\u4e2d\u5956\u52b1\u53d6\u51b3\u4e8e\u52a8\u4f5c\u548c\u6f5c\u5728\u72b6\u6001\uff0c\u72b6\u6001\u52a8\u6001\u53d7\u672a\u77e5\u7ebf\u6027\u52a8\u6001\u7cfb\u7edf\u63a7\u5236\u3002\u7b97\u6cd5\u901a\u8fc7\u968f\u673a\u63a2\u7d22\u4f30\u8ba1\u7cfb\u7edf\u53c2\u6570\uff0c\u7136\u540e\u4f18\u5316\u957f\u671f\u5956\u52b1\uff0c\u5b9e\u73b0\u4e86$\\tilde{\\mathcal{O}}(T^{2/3})$\u7684\u9057\u61be\u4e0a\u754c\u3002", "motivation": "\u7814\u7a76\u975e\u5e73\u7a33\u8001\u864e\u673a\u95ee\u9898\uff0c\u5176\u4e2d\u5956\u52b1\u4e0d\u4ec5\u4f9d\u8d56\u4e8e\u52a8\u4f5c\uff0c\u8fd8\u53d7\u6f5c\u5728\u72b6\u6001\u5f71\u54cd\uff0c\u72b6\u6001\u52a8\u6001\u672c\u8eab\u53c8\u53d7\u52a8\u4f5c\u5f71\u54cd\uff0c\u5bfc\u81f4\u77ed\u671f\u5956\u52b1\u4e0e\u957f\u671f\u5956\u52b1\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "method": "\u91c7\u7528\u63a2\u7d22-\u63d0\u4ea4\u7b97\u6cd5\uff1a\u63a2\u7d22\u9636\u6bb5\u4f7f\u7528\u968f\u673aRademacher\u52a8\u4f5c\u4f30\u8ba1\u7ebf\u6027\u52a8\u6001\u7cfb\u7edf\u7684\u9a6c\u5c14\u53ef\u592b\u53c2\u6570\uff1b\u63d0\u4ea4\u9636\u6bb5\u57fa\u4e8e\u4f30\u8ba1\u53c2\u6570\u8bbe\u8ba1\u4f18\u5316\u52a8\u4f5c\u5e8f\u5217\u4ee5\u6700\u5927\u5316\u957f\u671f\u5956\u52b1\u3002", "result": "\u7b97\u6cd5\u5b9e\u73b0\u4e86$\\tilde{\\mathcal{O}}(T^{2/3})$\u7684\u9057\u61be\u4e0a\u754c\uff0c\u89e3\u51b3\u4e86\u65f6\u5e8f\u76f8\u5173\u5956\u52b1\u5b66\u4e60\u548c\u957f\u671f\u5956\u52b1\u4f18\u5316\u4e24\u4e2a\u5173\u952e\u6311\u6218\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u975e\u5e73\u7a33\u8001\u864e\u673a\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u7cfb\u7edf\u8bc6\u522b\u548c\u4f18\u5316\u7406\u8bba\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u5728\u7406\u8bba\u4e0a\u4fdd\u8bc1\u4e86\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86\u5b9e\u7528\u7684\u534a\u5b9a\u89c4\u5212\u677e\u5f1b\u65b9\u6cd5\u3002"}}
{"id": "2510.16476", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16476", "abs": "https://arxiv.org/abs/2510.16476", "authors": ["Xiaozhe Li", "Xinyu Fang", "Shengyuan Ding", "Linyang Li", "Haodong Duan", "Qingwen Liu", "Kai Chen"], "title": "NP-Engine: Empowering Optimization Reasoning in Large Language Models with Verifiable Synthetic NP Problems", "comment": null, "summary": "Large Language Models (LLMs) have shown strong reasoning capabilities, with\nmodels like OpenAI's O-series and DeepSeek R1 excelling at tasks such as\nmathematics, coding, logic, and puzzles through Reinforcement Learning with\nVerifiable Rewards (RLVR). However, their ability to solve more complex\noptimization problems - particularly NP-hard tasks - remains underexplored. To\nbridge this gap, we propose NP-ENGINE, the first comprehensive framework for\ntraining and evaluating LLMs on NP-hard problems. NP-ENGINE covers 10 tasks\nacross five domains, each equipped with (i) a controllable instance generator,\n(ii) a rule-based verifier, and (iii) a heuristic solver that provides\napproximate optimal solutions as ground truth. This\ngenerator-verifier-heuristic pipeline enables scalable and verifiable RLVR\ntraining under hierarchical difficulties. We also introduce NP-BENCH, a\nbenchmark derived from NP-ENGINE-DATA, specifically designed to evaluate LLMs'\nability to tackle NP-hard level reasoning problems, focusing not only on\nfeasibility but also on solution quality. Additionally, we present\nQWEN2.5-7B-NP, a model trained via zero-RLVR with curriculum learning on\nQwen2.5-7B-Instruct, which significantly outperforms GPT-4o on NP-BENCH and\nachieves SOTA performance with the same model size. Beyond in-domain tasks, we\ndemonstrate that RLVR training on NP-ENGINE-DATA enables strong out-of-domain\n(OOD) generalization to reasoning tasks (logic, puzzles, math, and knowledge),\nas well as non-reasoning tasks such as instruction following. We also observe a\nscaling trend: increasing task diversity improves OOD generalization. These\nfindings suggest that task-rich RLVR training is a promising direction for\nadvancing LLM's reasoning ability, revealing new insights into the scaling laws\nof RLVR.", "AI": {"tldr": "\u63d0\u51fa\u4e86NP-ENGINE\u6846\u67b6\uff0c\u8fd9\u662f\u9996\u4e2a\u4e13\u95e8\u9488\u5bf9NP\u96be\u95ee\u9898\u8bad\u7ec3\u548c\u8bc4\u4f30LLM\u7684\u7efc\u5408\u6846\u67b6\uff0c\u5305\u542b10\u4e2a\u4efb\u52a1\u3001\u53ef\u63a7\u5b9e\u4f8b\u751f\u6210\u5668\u3001\u89c4\u5219\u9a8c\u8bc1\u5668\u548c\u542f\u53d1\u5f0f\u6c42\u89e3\u5668\u3002\u901a\u8fc7\u8be5\u6846\u67b6\u8bad\u7ec3\u7684\u6a21\u578b\u5728NP-BENCH\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8eGPT-4o\uff0c\u5e76\u5728\u9886\u57df\u5916\u4efb\u52a1\u4e0a\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u867d\u7136LLM\u5728\u6570\u5b66\u3001\u7f16\u7a0b\u7b49\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u89e3\u51b3\u66f4\u590d\u6742\u7684NP\u96be\u4f18\u5316\u95ee\u9898\u65b9\u9762\u7684\u80fd\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u7684\u8bad\u7ec3\u6846\u67b6\u548c\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u63d0\u51faNP-ENGINE\u6846\u67b6\uff0c\u5305\u542b\u53ef\u63a7\u5b9e\u4f8b\u751f\u6210\u5668\u3001\u89c4\u5219\u9a8c\u8bc1\u5668\u548c\u542f\u53d1\u5f0f\u6c42\u89e3\u5668\u7684\u751f\u6210-\u9a8c\u8bc1-\u542f\u53d1\u5f0f\u7ba1\u9053\uff0c\u652f\u6301\u53ef\u6269\u5c55\u4e14\u53ef\u9a8c\u8bc1\u7684RLVR\u5206\u5c42\u96be\u5ea6\u8bad\u7ec3\u3002\u57fa\u4e8e\u6b64\u6784\u5efaNP-BENCH\u57fa\u51c6\uff0c\u5e76\u8bad\u7ec3QWEN2.5-7B-NP\u6a21\u578b\u3002", "result": "QWEN2.5-7B-NP\u5728NP-BENCH\u4e0a\u663e\u8457\u4f18\u4e8eGPT-4o\uff0c\u8fbe\u5230\u540c\u6a21\u578b\u5c3a\u5bf8\u4e0b\u7684SOTA\u6027\u80fd\u3002RLVR\u8bad\u7ec3\u8fd8\u4f7f\u6a21\u578b\u5728\u903b\u8f91\u3001\u8c1c\u9898\u3001\u6570\u5b66\u7b49\u63a8\u7406\u4efb\u52a1\u4ee5\u53ca\u975e\u63a8\u7406\u4efb\u52a1\u4e0a\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u9886\u57df\u5916\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u4efb\u52a1\u4e30\u5bcc\u7684RLVR\u8bad\u7ec3\u662f\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\u7684\u6709\u524d\u666f\u65b9\u5411\uff0c\u63ed\u793a\u4e86RLVR\u7684\u6269\u5c55\u89c4\u5f8b\u3002\u589e\u52a0\u4efb\u52a1\u591a\u6837\u6027\u53ef\u4ee5\u6539\u5584\u9886\u57df\u5916\u6cdb\u5316\u80fd\u529b\uff0c\u4e3aLLM\u5728\u590d\u6742\u4f18\u5316\u95ee\u9898\u4e0a\u7684\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2510.17581", "categories": ["math.OC", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2510.17581", "abs": "https://arxiv.org/abs/2510.17581", "authors": ["Humberto Gimenes Macedo", "Lu\u00eds Felipe Bueno"], "title": "An Inexact General Descent Method with Applications in Differential Equation-Constrained Optimization", "comment": "31 pages, 10 figures", "summary": "In many applications, gradient evaluations are inherently approximate,\nmotivating the development of optimization methods that remain reliable under\ninexact first-order information. A common strategy in this context is adaptive\nevaluation, whereby coarse gradients are used in early iterations and refined\nnear a minimizer. This is particularly relevant in differential\nequation-constrained optimization (DECO), where discrete adjoint gradients\ndepend on iterative solvers. Motivated by DECO applications, we propose an\ninexact general descent framework and establish its global convergence theory\nunder two step-size regimes. For bounded step sizes, the analysis assumes that\nthe error tolerance in the computed gradient is proportional to its norm,\nwhereas for diminishing step sizes, the tolerance sequence is required to be\nsummable. The framework is implemented through inexact gradient descent and an\ninexact BFGS-like method, whose performance is demonstrated on a second-order\nODE inverse problem and a two-dimensional Laplace inverse problem using\ndiscrete adjoint gradients with adaptive accuracy. Across these examples,\nadaptive inexact gradients consistently reduced optimization time relative to\nfixed tight tolerances, while incorporating curvature information further\nimproved overall efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0d\u7cbe\u786e\u901a\u7528\u4e0b\u964d\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u68af\u5ea6\u8bc4\u4f30\u4e0d\u7cbe\u786e\u7684\u4f18\u5316\u95ee\u9898\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5fae\u5206\u65b9\u7a0b\u7ea6\u675f\u4f18\u5316\u3002\u8be5\u6846\u67b6\u5728\u4e24\u79cd\u6b65\u957f\u673a\u5236\u4e0b\u5efa\u7acb\u4e86\u5168\u5c40\u6536\u655b\u7406\u8bba\uff0c\u5e76\u901a\u8fc7\u4e0d\u7cbe\u786e\u68af\u5ea6\u4e0b\u964d\u548c\u4e0d\u7cbe\u786eBFGS\u65b9\u6cd5\u5b9e\u73b0\uff0c\u5728ODE\u53cd\u95ee\u9898\u548cLaplace\u53cd\u95ee\u9898\u4e2d\u5c55\u793a\u4e86\u81ea\u9002\u5e94\u4e0d\u7cbe\u786e\u68af\u5ea6\u80fd\u663e\u8457\u51cf\u5c11\u4f18\u5316\u65f6\u95f4\u3002", "motivation": "\u5728\u8bb8\u591a\u5e94\u7528\u4e2d\uff0c\u68af\u5ea6\u8bc4\u4f30\u672c\u8d28\u4e0a\u662f\u8fd1\u4f3c\u7684\uff0c\u7279\u522b\u662f\u5728\u5fae\u5206\u65b9\u7a0b\u7ea6\u675f\u4f18\u5316\u4e2d\uff0c\u79bb\u6563\u4f34\u968f\u68af\u5ea6\u4f9d\u8d56\u4e8e\u8fed\u4ee3\u6c42\u89e3\u5668\u3002\u8fd9\u4fc3\u4f7f\u5f00\u53d1\u5728\u975e\u7cbe\u786e\u4e00\u9636\u4fe1\u606f\u4e0b\u4ecd\u53ef\u9760\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u5176\u4e2d\u81ea\u9002\u5e94\u8bc4\u4f30\u7b56\u7565\uff08\u65e9\u671f\u4f7f\u7528\u7c97\u68af\u5ea6\uff0c\u5728\u6781\u5c0f\u503c\u70b9\u9644\u8fd1\u7ec6\u5316\uff09\u5c24\u4e3a\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e0d\u7cbe\u786e\u901a\u7528\u4e0b\u964d\u6846\u67b6\uff0c\u5efa\u7acb\u4e86\u5728\u4e24\u79cd\u6b65\u957f\u673a\u5236\u4e0b\u7684\u5168\u5c40\u6536\u655b\u7406\u8bba\uff1a\u6709\u754c\u6b65\u957f\u8981\u6c42\u8ba1\u7b97\u68af\u5ea6\u7684\u8bef\u5dee\u5bb9\u9650\u4e0e\u5176\u8303\u6570\u6210\u6bd4\u4f8b\uff0c\u800c\u9012\u51cf\u6b65\u957f\u8981\u6c42\u5bb9\u9650\u5e8f\u5217\u53ef\u6c42\u548c\u3002\u901a\u8fc7\u4e0d\u7cbe\u786e\u68af\u5ea6\u4e0b\u964d\u548c\u4e0d\u7cbe\u786eBFGS\u7c7b\u65b9\u6cd5\u5b9e\u73b0\u8be5\u6846\u67b6\u3002", "result": "\u5728\u4e8c\u9636ODE\u53cd\u95ee\u9898\u548c\u4e8c\u7ef4Laplace\u53cd\u95ee\u9898\u4e2d\u4f7f\u7528\u81ea\u9002\u5e94\u7cbe\u5ea6\u7684\u79bb\u6563\u4f34\u968f\u68af\u5ea6\u8fdb\u884c\u6d4b\u8bd5\u3002\u7ed3\u679c\u663e\u793a\uff0c\u4e0e\u56fa\u5b9a\u7d27\u5bb9\u9650\u76f8\u6bd4\uff0c\u81ea\u9002\u5e94\u4e0d\u7cbe\u786e\u68af\u5ea6\u59cb\u7ec8\u51cf\u5c11\u4e86\u4f18\u5316\u65f6\u95f4\uff0c\u800c\u7ed3\u5408\u66f2\u7387\u4fe1\u606f\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u6574\u4f53\u6548\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e0d\u7cbe\u786e\u901a\u7528\u4e0b\u964d\u6846\u67b6\u4e3a\u5904\u7406\u68af\u5ea6\u8bc4\u4f30\u4e0d\u7cbe\u786e\u7684\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u5fae\u5206\u65b9\u7a0b\u7ea6\u675f\u4f18\u5316\u4e2d\uff0c\u81ea\u9002\u5e94\u68af\u5ea6\u7b56\u7565\u80fd\u6709\u6548\u5e73\u8861\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u63d0\u9ad8\u4f18\u5316\u6548\u7387\u3002"}}
{"id": "2510.15998", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15998", "abs": "https://arxiv.org/abs/2510.15998", "authors": ["Nilo Schwencke", "Cyriaque Rousselot", "Alena Shilova", "Cyril Furtlehner"], "title": "AMStraMGRAM: Adaptive Multi-cutoff Strategy Modification for ANaGRAM", "comment": null, "summary": "Recent works have shown that natural gradient methods can significantly\noutperform standard optimizers when training physics-informed neural networks\n(PINNs). In this paper, we analyze the training dynamics of PINNs optimized\nwith ANaGRAM, a natural-gradient-inspired approach employing singular value\ndecomposition with cutoff regularization. Building on this analysis, we propose\na multi-cutoff adaptation strategy that further enhances ANaGRAM's performance.\nExperiments on benchmark PDEs validate the effectiveness of our method, which\nallows to reach machine precision on some experiments. To provide theoretical\ngrounding, we develop a framework based on spectral theory that explains the\nnecessity of regularization and extend previous shown connections with Green's\nfunctions theory.", "AI": {"tldr": "\u5206\u6790\u4e86\u4f7f\u7528ANaGRAM\u81ea\u7136\u68af\u5ea6\u65b9\u6cd5\u8bad\u7ec3PINNs\u7684\u8bad\u7ec3\u52a8\u6001\uff0c\u63d0\u51fa\u4e86\u591a\u622a\u6b62\u81ea\u9002\u5e94\u7b56\u7565\u6765\u63d0\u5347\u6027\u80fd\uff0c\u5728\u57fa\u51c6PDE\u4e0a\u8fbe\u5230\u673a\u5668\u7cbe\u5ea6\uff0c\u5e76\u5efa\u7acb\u4e86\u57fa\u4e8e\u8c31\u7406\u8bba\u7684\u7406\u8bba\u6846\u67b6\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8868\u660e\u81ea\u7136\u68af\u5ea6\u65b9\u6cd5\u5728\u8bad\u7ec3\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc(PINNs)\u65f6\u663e\u8457\u4f18\u4e8e\u6807\u51c6\u4f18\u5316\u5668\uff0c\u9700\u8981\u6df1\u5165\u5206\u6790\u5176\u8bad\u7ec3\u52a8\u6001\u5e76\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "method": "\u4f7f\u7528ANaGRAM\u65b9\u6cd5\uff08\u57fa\u4e8e\u5947\u5f02\u503c\u5206\u89e3\u548c\u622a\u6b62\u6b63\u5219\u5316\u7684\u81ea\u7136\u68af\u5ea6\u65b9\u6cd5\uff09\uff0c\u63d0\u51fa\u591a\u622a\u6b62\u81ea\u9002\u5e94\u7b56\u7565\uff0c\u5e76\u5efa\u7acb\u57fa\u4e8e\u8c31\u7406\u8bba\u7684\u5206\u6790\u6846\u67b6\u3002", "result": "\u5728\u57fa\u51c6PDE\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u90e8\u5206\u5b9e\u9a8c\u8fbe\u5230\u4e86\u673a\u5668\u7cbe\u5ea6\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u591a\u622a\u6b62\u81ea\u9002\u5e94\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86ANaGRAM\u7684\u6027\u80fd\uff0c\u8c31\u7406\u8bba\u6846\u67b6\u4e3a\u6b63\u5219\u5316\u7684\u5fc5\u8981\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\uff0c\u5e76\u6269\u5c55\u4e86\u4e0e\u683c\u6797\u51fd\u6570\u7406\u8bba\u7684\u8054\u7cfb\u3002"}}
{"id": "2510.16573", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16573", "abs": "https://arxiv.org/abs/2510.16573", "authors": ["Muhammad Ammar", "Hadiya Murad Hadi", "Usman Majeed Butt"], "title": "AI-Generated Text Detection in Low-Resource Languages: A Case Study on Urdu", "comment": null, "summary": "Large Language Models (LLMs) are now capable of generating text that closely\nresembles human writing, making them powerful tools for content creation, but\nthis growing ability has also made it harder to tell whether a piece of text\nwas written by a human or by a machine. This challenge becomes even more\nserious for languages like Urdu, where there are very few tools available to\ndetect AI-generated text. To address this gap, we propose a novel AI-generated\ntext detection framework tailored for the Urdu language. A balanced dataset\ncomprising 1,800 humans authored, and 1,800 AI generated texts, sourced from\nmodels such as Gemini, GPT-4o-mini, and Kimi AI was developed. Detailed\nlinguistic and statistical analysis was conducted, focusing on features such as\ncharacter and word counts, vocabulary richness (Type Token Ratio), and N-gram\npatterns, with significance evaluated through t-tests and MannWhitney U tests.\nThree state-of-the-art multilingual transformer models such as\nmdeberta-v3-base, distilbert-base-multilingualcased, and xlm-roberta-base were\nfine-tuned on this dataset. The mDeBERTa-v3-base achieved the highest\nperformance, with an F1-score 91.29 and accuracy of 91.26% on the test set.\nThis research advances efforts in contesting misinformation and academic\nmisconduct in Urdu-speaking communities and contributes to the broader\ndevelopment of NLP tools for low resource languages.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u4e4c\u5c14\u90fd\u8bed\u7684AI\u751f\u6210\u6587\u672c\u68c0\u6d4b\u6846\u67b6\uff0c\u4f7f\u7528\u591a\u8bed\u8a00transformer\u6a21\u578b\u5728\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0cmDeBERTa-v3-base\u6a21\u578b\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u523091.29%\u7684F1\u5206\u6570\u548c91.26%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u968f\u7740LLM\u751f\u6210\u6587\u672c\u80fd\u529b\u589e\u5f3a\uff0c\u533a\u5206\u4eba\u7c7b\u5199\u4f5c\u548c\u673a\u5668\u751f\u6210\u6587\u672c\u53d8\u5f97\u56f0\u96be\uff0c\u7279\u522b\u662f\u5728\u4e4c\u5c14\u90fd\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u7f3a\u4e4f\u68c0\u6d4b\u5de5\u5177\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u6784\u5efa\u5305\u542b1800\u7bc7\u4eba\u7c7b\u5199\u4f5c\u548c1800\u7bc7AI\u751f\u6210\u6587\u672c\u7684\u5e73\u8861\u6570\u636e\u96c6\uff0c\u8fdb\u884c\u8bed\u8a00\u548c\u7edf\u8ba1\u5206\u6790\uff0c\u5fae\u8c03\u4e09\u79cd\u591a\u8bed\u8a00transformer\u6a21\u578b\uff08mdeberta-v3-base\u3001distilbert-base-multilingualcased\u3001xlm-roberta-base\uff09\u3002", "result": "mDeBERTa-v3-base\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u5728\u6d4b\u8bd5\u96c6\u4e0a\u83b7\u5f9791.29%\u7684F1\u5206\u6570\u548c91.26%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u6709\u52a9\u4e8e\u6253\u51fb\u4e4c\u5c14\u90fd\u8bed\u793e\u533a\u7684\u9519\u8bef\u4fe1\u606f\u548c\u5b66\u672f\u4e0d\u7aef\u884c\u4e3a\uff0c\u5e76\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684NLP\u5de5\u5177\u5f00\u53d1\u505a\u51fa\u8d21\u732e\u3002"}}
{"id": "2510.16211", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16211", "abs": "https://arxiv.org/abs/2510.16211", "authors": ["Henrique Pickler", "Jorge K. S. Kamassury", "Danilo Silva"], "title": "Benchmarking noisy label detection methods", "comment": null, "summary": "Label noise is a common problem in real-world datasets, affecting both model\ntraining and validation. Clean data are essential for achieving strong\nperformance and ensuring reliable evaluation. While various techniques have\nbeen proposed to detect noisy labels, there is no clear consensus on optimal\napproaches. We perform a comprehensive benchmark of detection methods by\ndecomposing them into three fundamental components: label agreement function,\naggregation method, and information gathering approach (in-sample vs\nout-of-sample). This decomposition can be applied to many existing detection\nmethods, and enables systematic comparison across diverse approaches. To fairly\ncompare methods, we propose a unified benchmark task, detecting a fraction of\ntraining samples equal to the dataset's noise rate. We also introduce a novel\nmetric: the false negative rate at this fixed operating point. Our evaluation\nspans vision and tabular datasets under both synthetic and real-world noise\nconditions. We identify that in-sample information gathering using average\nprobability aggregation combined with the logit margin as the label agreement\nfunction achieves the best results across most scenarios. Our findings provide\npractical guidance for designing new detection methods and selecting techniques\nfor specific applications.", "AI": {"tldr": "\u672c\u6587\u5bf9\u6807\u7b7e\u566a\u58f0\u68c0\u6d4b\u65b9\u6cd5\u8fdb\u884c\u4e86\u5168\u9762\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u5c06\u65b9\u6cd5\u5206\u89e3\u4e3a\u4e09\u4e2a\u57fa\u672c\u7ec4\u4ef6\uff08\u6807\u7b7e\u4e00\u81f4\u6027\u51fd\u6570\u3001\u805a\u5408\u65b9\u6cd5\u548c\u4fe1\u606f\u6536\u96c6\u65b9\u5f0f\uff09\u6765\u7cfb\u7edf\u6bd4\u8f83\u4e0d\u540c\u65b9\u6cd5\uff0c\u53d1\u73b0\u5728\u591a\u6570\u573a\u666f\u4e0b\u4f7f\u7528\u5e73\u5747\u6982\u7387\u805a\u5408\u7ed3\u5408logit\u8fb9\u754c\u4f5c\u4e3a\u6807\u7b7e\u4e00\u81f4\u6027\u51fd\u6570\u7684\u65b9\u6cd5\u6548\u679c\u6700\u4f73\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e2d\u666e\u904d\u5b58\u5728\u6807\u7b7e\u566a\u58f0\u95ee\u9898\uff0c\u5f71\u54cd\u6a21\u578b\u8bad\u7ec3\u548c\u9a8c\u8bc1\u6548\u679c\u3002\u867d\u7136\u5df2\u6709\u591a\u79cd\u566a\u58f0\u6807\u7b7e\u68c0\u6d4b\u6280\u672f\uff0c\u4f46\u7f3a\u4e4f\u660e\u786e\u7684\u6700\u4f73\u65b9\u6cd5\u5171\u8bc6\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u7684\u6bd4\u8f83\u7814\u7a76\u3002", "method": "\u5c06\u6807\u7b7e\u566a\u58f0\u68c0\u6d4b\u65b9\u6cd5\u5206\u89e3\u4e3a\u4e09\u4e2a\u57fa\u672c\u7ec4\u4ef6\u8fdb\u884c\u7cfb\u7edf\u6bd4\u8f83\uff0c\u63d0\u51fa\u7edf\u4e00\u7684\u57fa\u51c6\u4efb\u52a1\uff08\u68c0\u6d4b\u4e0e\u6570\u636e\u96c6\u566a\u58f0\u7387\u76f8\u7b49\u7684\u8bad\u7ec3\u6837\u672c\u6bd4\u4f8b\uff09\u548c\u65b0\u8bc4\u4f30\u6307\u6807\uff08\u56fa\u5b9a\u64cd\u4f5c\u70b9\u4e0b\u7684\u5047\u9634\u6027\u7387\uff09\u3002", "result": "\u5728\u89c6\u89c9\u548c\u8868\u683c\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u4f7f\u7528\u5e73\u5747\u6982\u7387\u805a\u5408\u7ed3\u5408logit\u8fb9\u754c\u4f5c\u4e3a\u6807\u7b7e\u4e00\u81f4\u6027\u51fd\u6570\u7684\u65b9\u6cd5\u5728\u5927\u591a\u6570\u573a\u666f\u4e0b\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u8bbe\u8ba1\u65b0\u7684\u68c0\u6d4b\u65b9\u6cd5\u548c\u4e3a\u7279\u5b9a\u5e94\u7528\u9009\u62e9\u6280\u672f\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\uff0c\u8bc6\u522b\u51fa\u4e86\u6700\u6709\u6548\u7684\u68c0\u6d4b\u65b9\u6cd5\u7ec4\u5408\u3002"}}
{"id": "2510.16533", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16533", "abs": "https://arxiv.org/abs/2510.16533", "authors": ["Eilene Tomkins-Flanagan", "Connor Hanley", "Mary A. Kelly"], "title": "Hey Pentti, We Did It Again!: Differentiable vector-symbolic types that prove polynomial termination", "comment": null, "summary": "We present a typed computer language, Doug, in which all typed programs may\nbe proved to halt in polynomial time, encoded in a vector-symbolic architecture\n(VSA). Doug is just an encoding of the light linear functional programming\nlanguage (LLFPL) described by (Schimanski2009, ch. 7). The types of Doug are\nencoded using a slot-value encoding scheme based on holographic declarative\nmemory (HDM; Kelly, 2020). The terms of Doug are encoded using a variant of the\nLisp VSA defined by (Flanagan, 2024). Doug allows for some points on the\nembedding space of a neural network to be interpreted as types, where the types\nof nearby points are similar both in structure and content. Types in Doug are\ntherefore learnable by a neural network. Following (Chollet, 2019), (Card,\n1983), and (Newell, 1981), we view skill as the application of a procedure, or\nprogram of action, that causes a goal to be satisfied. Skill acquisition may\ntherefore be expressed as program synthesis. Using Doug, we hope to describe a\nform of learning of skilled behaviour that follows a human-like pace of skill\nacquisition (i.e., substantially faster than brute force; Heathcote, 2000),\nexceeding the efficiency of all currently existing approaches (Kaplan, 2020;\nJones, 2021; Chollet, 2024). Our approach brings us one step closer to modeling\nhuman mental representations, as they must actually exist in the brain, and\nthose representations' acquisition, as they are actually learned.", "AI": {"tldr": "Doug\u662f\u4e00\u79cd\u7c7b\u578b\u5316\u8ba1\u7b97\u673a\u8bed\u8a00\uff0c\u6240\u6709\u7c7b\u578b\u5316\u7a0b\u5e8f\u90fd\u80fd\u88ab\u8bc1\u660e\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u505c\u6b62\u8fd0\u884c\uff0c\u7f16\u7801\u5728\u5411\u91cf\u7b26\u53f7\u67b6\u6784\u4e2d\u3002\u8be5\u8bed\u8a00\u652f\u6301\u7c7b\u578b\u5b66\u4e60\uff0c\u65e8\u5728\u5b9e\u73b0\u7c7b\u4eba\u6280\u80fd\u83b7\u53d6\u901f\u5ea6\u3002", "motivation": "\u5e0c\u671b\u901a\u8fc7Doug\u8bed\u8a00\u63cf\u8ff0\u4e00\u79cd\u6280\u80fd\u83b7\u53d6\u5f62\u5f0f\uff0c\u4f7f\u5176\u9075\u5faa\u7c7b\u4eba\u7684\u6280\u80fd\u83b7\u53d6\u901f\u5ea6\uff08\u6bd4\u66b4\u529b\u65b9\u6cd5\u5feb\u5f97\u591a\uff09\uff0c\u8d85\u8d8a\u73b0\u6709\u6240\u6709\u65b9\u6cd5\u7684\u6548\u7387\uff0c\u66f4\u63a5\u8fd1\u6a21\u62df\u4eba\u8111\u4e2d\u7684\u5fc3\u7406\u8868\u5f81\u53ca\u5176\u83b7\u53d6\u8fc7\u7a0b\u3002", "method": "Doug\u662f\u57fa\u4e8e\u8f7b\u91cf\u7ebf\u6027\u51fd\u6570\u5f0f\u7f16\u7a0b\u8bed\u8a00(LLFPL)\u7684\u7f16\u7801\u5b9e\u73b0\uff0c\u4f7f\u7528\u57fa\u4e8e\u5168\u606f\u58f0\u660e\u6027\u8bb0\u5fc6(HDM)\u7684\u69fd\u503c\u7f16\u7801\u65b9\u6848\u7f16\u7801\u7c7b\u578b\uff0c\u4f7f\u7528Lisp VSA\u53d8\u4f53\u7f16\u7801\u672f\u8bed\uff0c\u4f7f\u795e\u7ecf\u7f51\u7edc\u80fd\u591f\u5b66\u4e60\u7c7b\u578b\u3002", "result": "\u8bba\u6587\u63d0\u51fa\u4e86Doug\u8bed\u8a00\u6846\u67b6\uff0c\u4f7f\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u67d0\u4e9b\u70b9\u53ef\u88ab\u89e3\u91ca\u4e3a\u7c7b\u578b\uff0c\u4e14\u90bb\u8fd1\u70b9\u7684\u7c7b\u578b\u5728\u7ed3\u6784\u548c\u5185\u5bb9\u4e0a\u76f8\u4f3c\uff0c\u4ece\u800c\u5b9e\u73b0\u7c7b\u578b\u7684\u53ef\u5b66\u4e60\u6027\u3002", "conclusion": "Doug\u8bed\u8a00\u4e3a\u5b9e\u73b0\u7c7b\u4eba\u6280\u80fd\u83b7\u53d6\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u4f7f\u7a0b\u5e8f\u5408\u6210\u80fd\u591f\u66f4\u9ad8\u6548\u5730\u8fdb\u884c\uff0c\u5411\u6a21\u62df\u4eba\u8111\u5b9e\u9645\u5b58\u5728\u7684\u5fc3\u7406\u8868\u5f81\u53ca\u5176\u5b66\u4e60\u8fc7\u7a0b\u8fc8\u8fdb\u4e86\u4e00\u6b65\u3002"}}
{"id": "2510.17610", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2510.17610", "abs": "https://arxiv.org/abs/2510.17610", "authors": ["Alen Alexanderian"], "title": "A brief note on approximate optimization of submodular functions", "comment": "5 pages", "summary": "We briefly discuss the greedy method and a couple of its more efficient\nvariants for approximately maximizing monotone submodular functions.", "AI": {"tldr": "\u8ba8\u8bba\u8d2a\u5a6a\u65b9\u6cd5\u53ca\u5176\u66f4\u9ad8\u6548\u53d8\u4f53\u7528\u4e8e\u8fd1\u4f3c\u6700\u5927\u5316\u5355\u8c03\u5b50\u6a21\u51fd\u6570", "motivation": "\u63d0\u9ad8\u5355\u8c03\u5b50\u6a21\u51fd\u6570\u8fd1\u4f3c\u6700\u5927\u5316\u7684\u6548\u7387", "method": "\u8d2a\u5a6a\u65b9\u6cd5\u53ca\u5176\u53d8\u4f53", "result": "\u63d0\u51fa\u4e86\u66f4\u9ad8\u6548\u7684\u8fd1\u4f3c\u7b97\u6cd5", "conclusion": "\u8d2a\u5a6a\u65b9\u6cd5\u53ca\u5176\u53d8\u4f53\u662f\u6709\u6548\u7684\u8fd1\u4f3c\u4f18\u5316\u5de5\u5177"}}
{"id": "2510.16007", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16007", "abs": "https://arxiv.org/abs/2510.16007", "authors": ["Ziao Yang", "Longbo Huang", "Hongfu Liu"], "title": "Layer-Aware Influence for Online Data Valuation Estimation", "comment": null, "summary": "Data-centric learning emphasizes curating high-quality training samples to\nboost performance rather than designing new architectures. A central problem is\nto estimate the influence of training sample efficiently. Prior studies largely\nfocus on static influence measured on a converged model, overlooking how data\nvaluation dynamically changes during optimization. This omission neglects the\ndynamic nature of sample influence during optimization, especially in deep\nmodels. To address the computational burden of frequent influence estimation,\nwe develop a layer-aware online estimator that requires only loss-to-output\ngradients. This design avoids parameter-level and full-network gradients while\npreserving ranking fidelity. Extensive experiments across LLM pretraining,\nfine-tuning, and image classification show our method improves accuracy with\nsubstantially lower time and memory cost, making dynamic data curation\nefficient and scalable in practice.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c42\u611f\u77e5\u5728\u7ebf\u4f30\u8ba1\u5668\uff0c\u7528\u4e8e\u9ad8\u6548\u4f30\u8ba1\u8bad\u7ec3\u6837\u672c\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\u7684\u52a8\u6001\u5f71\u54cd\uff0c\u907f\u514d\u53c2\u6570\u7ea7\u548c\u5168\u7f51\u7edc\u68af\u5ea6\u8ba1\u7b97\uff0c\u663e\u8457\u964d\u4f4e\u65f6\u95f4\u548c\u5185\u5b58\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5728\u6536\u655b\u6a21\u578b\u4e0a\u6d4b\u91cf\u7684\u9759\u6001\u5f71\u54cd\uff0c\u5ffd\u7565\u4e86\u6570\u636e\u4ef7\u503c\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\u7684\u52a8\u6001\u53d8\u5316\u7279\u6027\uff0c\u7279\u522b\u662f\u5728\u6df1\u5ea6\u6a21\u578b\u4e2d\u3002", "method": "\u5f00\u53d1\u5c42\u611f\u77e5\u5728\u7ebf\u4f30\u8ba1\u5668\uff0c\u4ec5\u9700\u8981\u635f\u5931\u5230\u8f93\u51fa\u7684\u68af\u5ea6\uff0c\u907f\u514d\u53c2\u6570\u7ea7\u548c\u5168\u7f51\u7edc\u68af\u5ea6\u8ba1\u7b97\uff0c\u540c\u65f6\u4fdd\u6301\u6392\u5e8f\u4fdd\u771f\u5ea6\u3002", "result": "\u5728LLM\u9884\u8bad\u7ec3\u3001\u5fae\u8c03\u548c\u56fe\u50cf\u5206\u7c7b\u7b49\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u4e86\u65f6\u95f4\u548c\u5185\u5b58\u6210\u672c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4f7f\u52a8\u6001\u6570\u636e\u7b5b\u9009\u5728\u5b9e\u8df5\u4e2d\u53d8\u5f97\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\uff0c\u4e3a\u6570\u636e\u4e2d\u5fc3\u5b66\u4e60\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.16604", "categories": ["cs.CL", "68T50", "I.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.16604", "abs": "https://arxiv.org/abs/2510.16604", "authors": ["Francisco Jose Cortes Delgado", "Eduardo Martinez Gracia", "Rafael Valencia Garcia"], "title": "Fine-tuning of Large Language Models for Constituency Parsing Using a Sequence to Sequence Approach", "comment": "6 pages, 3 figures. Submitted to SEPLN 2023 Conference", "summary": "Recent advances in natural language processing with large neural models have\nopened new possibilities for syntactic analysis based on machine learning. This\nwork explores a novel approach to phrase-structure analysis by fine-tuning\nlarge language models (LLMs) to translate an input sentence into its\ncorresponding syntactic structure. The main objective is to extend the\ncapabilities of MiSintaxis, a tool designed for teaching Spanish syntax.\nSeveral models from the Hugging Face repository were fine-tuned using training\ndata generated from the AnCora-ES corpus, and their performance was evaluated\nusing the F1 score. The results demonstrate high accuracy in phrase-structure\nanalysis and highlight the potential of this methodology.", "AI": {"tldr": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u77ed\u8bed\u7ed3\u6784\u5206\u6790\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5fae\u8c03LLMs\u5c06\u53e5\u5b50\u7ffb\u8bd1\u6210\u5bf9\u5e94\u53e5\u6cd5\u7ed3\u6784\uff0c\u7528\u4e8e\u6269\u5c55\u897f\u73ed\u7259\u8bed\u53e5\u6cd5\u6559\u5b66\u5de5\u5177MiSintaxis\u7684\u529f\u80fd", "motivation": "\u5229\u7528\u5927\u578b\u795e\u7ecf\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u7684\u8fdb\u5c55\uff0c\u63a2\u7d22\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u53e5\u6cd5\u5206\u6790\u65b0\u53ef\u80fd\u6027\uff0c\u6269\u5c55\u897f\u73ed\u7259\u8bed\u53e5\u6cd5\u6559\u5b66\u5de5\u5177MiSintaxis\u7684\u80fd\u529b", "method": "\u4eceHugging Face\u4ed3\u5e93\u9009\u53d6\u591a\u4e2a\u6a21\u578b\uff0c\u4f7f\u7528AnCora-ES\u8bed\u6599\u5e93\u751f\u6210\u7684\u8bad\u7ec3\u6570\u636e\u8fdb\u884c\u5fae\u8c03\uff0c\u8ba9LLMs\u5b66\u4e60\u5c06\u8f93\u5165\u53e5\u5b50\u7ffb\u8bd1\u6210\u5bf9\u5e94\u7684\u53e5\u6cd5\u7ed3\u6784", "result": "\u4f7f\u7528F1\u5206\u6570\u8bc4\u4f30\u6027\u80fd\uff0c\u7ed3\u679c\u663e\u793a\u5728\u77ed\u8bed\u7ed3\u6784\u5206\u6790\u4e2d\u53d6\u5f97\u4e86\u9ad8\u51c6\u786e\u7387", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u5728\u53e5\u6cd5\u5206\u6790\u4e2d\u7684\u6f5c\u529b\uff0c\u8bc1\u660e\u4e86\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u77ed\u8bed\u7ed3\u6784\u5206\u6790\u7684\u6709\u6548\u6027"}}
{"id": "2510.16250", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16250", "abs": "https://arxiv.org/abs/2510.16250", "authors": ["Danil Akhtiamov", "Reza Ghane", "Babak Hassibi"], "title": "One-Bit Quantization for Random Features Models", "comment": null, "summary": "Recent advances in neural networks have led to significant computational and\nmemory demands, spurring interest in one-bit weight compression to enable\nefficient inference on resource-constrained devices. However, the theoretical\nunderpinnings of such compression remain poorly understood. We address this gap\nby analyzing one-bit quantization in the Random Features model, a simplified\nframework that corresponds to neural networks with random representations. We\nprove that, asymptotically, quantizing weights of all layers except the last\nincurs no loss in generalization error, compared to the full precision random\nfeatures model. Our findings offer theoretical insights into neural network\ncompression. We also demonstrate empirically that one-bit quantization leads to\nsignificant inference speed ups for the Random Features models even on a laptop\nGPU, confirming the practical benefits of our work. Additionally, we provide an\nasymptotically precise characterization of the generalization error for Random\nFeatures with an arbitrary number of layers. To the best of our knowledge, our\nanalysis yields more general results than all previous works in the related\nliterature.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86\u795e\u7ecf\u7f51\u7edc\u4e2d\u4e00\u6bd4\u7279\u6743\u91cd\u538b\u7f29\u7684\u7406\u8bba\u57fa\u7840\uff0c\u8bc1\u660e\u5728\u968f\u673a\u7279\u5f81\u6a21\u578b\u4e2d\uff0c\u9664\u6700\u540e\u4e00\u5c42\u5916\u7684\u6240\u6709\u6743\u91cd\u91cf\u5316\u4e0d\u4f1a\u5bfc\u81f4\u6cdb\u5316\u8bef\u5dee\u635f\u5931\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5b9e\u9645\u63a8\u7406\u52a0\u901f\u6548\u679c\u3002", "motivation": "\u795e\u7ecf\u7f51\u7edc\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u6fc0\u589e\uff0c\u4fc3\u4f7f\u7814\u7a76\u8005\u5173\u6ce8\u4e00\u6bd4\u7279\u6743\u91cd\u538b\u7f29\u4ee5\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\uff0c\u4f46\u76f8\u5173\u7406\u8bba\u57fa\u7840\u5c1a\u4e0d\u5b8c\u5584\u3002", "method": "\u5728\u968f\u673a\u7279\u5f81\u6a21\u578b\uff08\u5bf9\u5e94\u5177\u6709\u968f\u673a\u8868\u793a\u7684\u795e\u7ecf\u7f51\u7edc\uff09\u4e2d\u5206\u6790\u4e00\u6bd4\u7279\u91cf\u5316\uff0c\u901a\u8fc7\u7406\u8bba\u8bc1\u660e\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\u3002", "result": "\u7406\u8bba\u8bc1\u660e\uff1a\u9664\u6700\u540e\u4e00\u5c42\u5916\u6240\u6709\u6743\u91cd\u91cf\u5316\u5728\u6e10\u8fd1\u610f\u4e49\u4e0a\u4e0d\u4f1a\u635f\u5931\u6cdb\u5316\u8bef\u5dee\uff1b\u5b9e\u8bc1\u9a8c\u8bc1\uff1a\u4e00\u6bd4\u7279\u91cf\u5316\u5728\u7b14\u8bb0\u672c\u7535\u8111GPU\u4e0a\u663e\u8457\u52a0\u901f\u968f\u673a\u7279\u5f81\u6a21\u578b\u63a8\u7406\u3002", "conclusion": "\u4e00\u6bd4\u7279\u6743\u91cd\u538b\u7f29\u5728\u7406\u8bba\u4e0a\u53ef\u884c\u4e14\u5728\u5b9e\u8df5\u4e2d\u6709\u6548\uff0c\u4e3a\u795e\u7ecf\u7f51\u7edc\u538b\u7f29\u63d0\u4f9b\u4e86\u7406\u8bba\u6d1e\u5bdf\u548c\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.16555", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16555", "abs": "https://arxiv.org/abs/2510.16555", "authors": ["Qiongyan Wang", "Xingchen Zou", "Yutian Jiang", "Haomin Wen", "Jiaheng Wei", "Qingsong Wen", "Yuxuan Liang"], "title": "Urban-R1: Reinforced MLLMs Mitigate Geospatial Biases for Urban General Intelligence", "comment": null, "summary": "Rapid urbanization intensifies the demand for Urban General Intelligence\n(UGI), referring to AI systems that can understand and reason about complex\nurban environments. Recent studies have built urban foundation models using\nsupervised fine-tuning (SFT) of LLMs and MLLMs, yet these models exhibit\npersistent geospatial bias, producing regionally skewed predictions and limited\ngeneralization. To this end, we propose Urban-R1, a reinforcement\nlearning-based post-training framework that aligns MLLMs with the objectives of\nUGI. Urban-R1 adopts Group Relative Policy Optimization (GRPO) to optimize\nreasoning across geographic groups and employs urban region profiling as a\nproxy task to provide measurable rewards from multimodal urban data. Extensive\nexperiments across diverse regions and tasks show that Urban-R1 effectively\nmitigates geo-bias and improves cross-region generalization, outperforming both\nSFT-trained and closed-source models. Our results highlight reinforcement\nlearning alignment as a promising pathway toward equitable and trustworthy\nurban intelligence.", "AI": {"tldr": "\u63d0\u51fa\u4e86Urban-R1\uff0c\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u7fa4\u4f53\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u548c\u57ce\u5e02\u533a\u57df\u753b\u50cf\u4efb\u52a1\u6765\u51cf\u8f7b\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5730\u7406\u504f\u89c1\uff0c\u63d0\u5347\u8de8\u533a\u57df\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5feb\u901f\u57ce\u5e02\u5316\u589e\u52a0\u4e86\u5bf9\u57ce\u5e02\u901a\u7528\u667a\u80fd\u7684\u9700\u6c42\uff0c\u4f46\u73b0\u6709\u57fa\u4e8e\u76d1\u7763\u5fae\u8c03\u7684\u57ce\u5e02\u57fa\u7840\u6a21\u578b\u5b58\u5728\u6301\u7eed\u7684\u5730\u7406\u504f\u89c1\uff0c\u5bfc\u81f4\u533a\u57df\u9884\u6d4b\u504f\u5dee\u548c\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u6846\u67b6Urban-R1\uff0c\u4f7f\u7528\u7fa4\u4f53\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u6765\u4f18\u5316\u8de8\u5730\u7406\u7fa4\u4f53\u7684\u63a8\u7406\uff0c\u5e76\u4ee5\u57ce\u5e02\u533a\u57df\u753b\u50cf\u4f5c\u4e3a\u4ee3\u7406\u4efb\u52a1\uff0c\u4ece\u591a\u6a21\u6001\u57ce\u5e02\u6570\u636e\u4e2d\u63d0\u4f9b\u53ef\u6d4b\u91cf\u7684\u5956\u52b1\u3002", "result": "\u5728\u591a\u4e2a\u533a\u57df\u548c\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cUrban-R1\u6709\u6548\u51cf\u8f7b\u4e86\u5730\u7406\u504f\u89c1\uff0c\u63d0\u9ad8\u4e86\u8de8\u533a\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u4f18\u4e8e\u76d1\u7763\u5fae\u8c03\u8bad\u7ec3\u548c\u95ed\u6e90\u6a21\u578b\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u5bf9\u9f50\u662f\u5b9e\u73b0\u516c\u5e73\u53ef\u4fe1\u57ce\u5e02\u667a\u80fd\u7684\u6709\u524d\u666f\u9014\u5f84\u3002"}}
{"id": "2510.17624", "categories": ["math.OC", "cs.CC"], "pdf": "https://arxiv.org/pdf/2510.17624", "abs": "https://arxiv.org/abs/2510.17624", "authors": ["Felix Engelhardt", "Jannis Kurtz", "\u015e. \u0130lker Birbil", "Ted Ralphs"], "title": "Counterfactual Explanations for Integer Optimization Problems", "comment": null, "summary": "Counterfactual explanations (CEs) offer a human-understandable way to explain\ndecisions by identifying specific changes to the input parameters of a base or\npresent model that would lead to a desired change in the outcome. For\noptimization models, CEs have primarily been studied in limited contexts and\nlittle research has been done on CEs for general integer optimization problems.\nIn this work, we address this gap. We first show that the general problem of\nconstructing a CE is $\\Sigma_2^p$-complete even for binary integer programs\nwith just a single mutable constraint. Second, we propose solution algorithms\nfor several of the most tractable special cases: (i) mutable objective\nparameters, (ii) a single mutable constraint, (iii) mutable right-hand-side,\nand (iv) all input parameters can be modified. We evaluate our approach using\nclassical knapsack problem instances, focusing on cases with mutable constraint\nparameters. Our results show that our methods are capable of finding optimal\nCEs for small instances involving up to 40 items within a few hours.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u6574\u6570\u4f18\u5316\u95ee\u9898\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff0c\u8bc1\u660e\u4e86\u6784\u5efa\u53cd\u4e8b\u5b9e\u89e3\u91ca\u7684\u590d\u6742\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u51e0\u79cd\u53ef\u5904\u7406\u7279\u6b8a\u60c5\u51b5\u7684\u9ad8\u6548\u7b97\u6cd5\u3002", "motivation": "\u53cd\u4e8b\u5b9e\u89e3\u91ca\u4e3a\u89e3\u91ca\u51b3\u7b56\u63d0\u4f9b\u4e86\u4e00\u79cd\u4eba\u7c7b\u53ef\u7406\u89e3\u7684\u65b9\u5f0f\uff0c\u4f46\u5728\u6574\u6570\u4f18\u5316\u95ee\u9898\u4e2d\u7814\u7a76\u8f83\u5c11\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u4e00\u822c\u6574\u6570\u4f18\u5316\u95ee\u9898\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\u7814\u7a76\u5b58\u5728\u7a7a\u767d\u3002", "method": "\u9996\u5148\u8bc1\u660e\u4e86\u6784\u5efa\u53cd\u4e8b\u5b9e\u89e3\u91ca\u7684\u590d\u6742\u6027\uff0c\u7136\u540e\u9488\u5bf9\u51e0\u79cd\u6700\u6613\u5904\u7406\u7684\u7279\u6b8a\u60c5\u51b5\u63d0\u51fa\u4e86\u89e3\u51b3\u65b9\u6848\u7b97\u6cd5\uff1a\u53ef\u53d8\u76ee\u6807\u53c2\u6570\u3001\u5355\u4e00\u53ef\u53d8\u7ea6\u675f\u3001\u53ef\u53d8\u53f3\u7aef\u9879\u4ee5\u53ca\u6240\u6709\u8f93\u5165\u53c2\u6570\u53ef\u4fee\u6539\u7684\u60c5\u51b5\u3002", "result": "\u5728\u7ecf\u5178\u80cc\u5305\u95ee\u9898\u5b9e\u4f8b\u4e0a\u8bc4\u4f30\u4e86\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\uff0c\u91cd\u70b9\u5173\u6ce8\u53ef\u53d8\u7ea6\u675f\u53c2\u6570\u7684\u60c5\u51b5\u3002\u7ed3\u679c\u663e\u793a\uff0c\u5bf9\u4e8e\u6d89\u53ca\u6700\u591a40\u4e2a\u9879\u76ee\u7684\u5c0f\u578b\u5b9e\u4f8b\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u51e0\u5c0f\u65f6\u5185\u627e\u5230\u6700\u4f18\u53cd\u4e8b\u5b9e\u89e3\u91ca\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u6574\u6570\u4f18\u5316\u95ee\u9898\u53cd\u4e8b\u5b9e\u89e3\u91ca\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u63d0\u51fa\u4e86\u6709\u6548\u7684\u7b97\u6cd5\uff0c\u5e76\u5728\u5b9e\u9645\u95ee\u9898\u4e0a\u9a8c\u8bc1\u4e86\u5176\u53ef\u884c\u6027\u3002"}}
{"id": "2510.16014", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16014", "abs": "https://arxiv.org/abs/2510.16014", "authors": ["Hanyin Cheng", "Ruitong Zhang", "Yuning Lu", "Peng Chen", "Meng Wang", "Yang Shu", "Bin Yang", "Chenjuan Guo"], "title": "STAR: Boosting Time Series Foundation Models for Anomaly Detection through State-aware Adapter", "comment": null, "summary": "While Time Series Foundation Models (TSFMs) have demonstrated remarkable\nsuccess in Multivariate Time Series Anomaly Detection (MTSAD), however, in\nreal-world industrial scenarios, many time series comprise not only numerical\nvariables such as temperature and flow, but also numerous discrete state\nvariables that describe the system status, such as valve on/off or day of the\nweek. Existing TSFMs often overlook the distinct categorical nature of state\nvariables and their critical role as conditions, typically treating them\nuniformly with numerical variables. This inappropriate modeling approach\nprevents the model from fully leveraging state information and even leads to a\nsignificant degradation in detection performance after state variables are\nintegrated. To address this critical limitation, this paper proposes a novel\nSTate-aware AdapteR (STAR). STAR is a plug-and-play module designed to enhance\nthe capability of TSFMs in modeling and leveraging state variables during the\nfine-tuning stage. Specifically, STAR comprisesthree core components: (1) We\ndesign an Identity-guided State Encoder, whicheffectively captures the complex\ncategorical semantics of state variables through a learnable State Memory. (2)\nWe propose a Conditional Bottleneck Adapter, which dynamically generates\nlow-rank adaptation parameters conditioned on the current state, thereby\nflexibly injecting the influence of state variables into the backbone model.\n(3) We also introduce a Numeral-State Matching module to more effectively\ndetect anomalies inherent to the state variables themselves. Extensive\nexperiments conducted on real-world datasets demonstrate that STAR can improve\nthe performance of existing TSFMs on MTSAD.", "AI": {"tldr": "\u63d0\u51faSTAR\u6a21\u5757\u6765\u589e\u5f3a\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u5bf9\u72b6\u6001\u53d8\u91cf\u7684\u5efa\u6a21\u80fd\u529b\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u72b6\u6001\u53d8\u91cf\u5206\u7c7b\u7279\u6027\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u5b9e\u5de5\u4e1a\u573a\u666f\u4e2d\u65f6\u95f4\u5e8f\u5217\u5305\u542b\u6570\u503c\u53d8\u91cf\u548c\u79bb\u6563\u72b6\u6001\u53d8\u91cf\uff0c\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u901a\u5e38\u5ffd\u89c6\u72b6\u6001\u53d8\u91cf\u7684\u5206\u7c7b\u7279\u6027\uff0c\u5c06\u5176\u4e0e\u6570\u503c\u53d8\u91cf\u7edf\u4e00\u5904\u7406\uff0c\u5bfc\u81f4\u65e0\u6cd5\u5145\u5206\u5229\u7528\u72b6\u6001\u4fe1\u606f\u751a\u81f3\u6027\u80fd\u4e0b\u964d\u3002", "method": "STAR\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u8eab\u4efd\u5f15\u5bfc\u7684\u72b6\u6001\u7f16\u7801\u5668\uff08\u901a\u8fc7\u53ef\u5b66\u4e60\u72b6\u6001\u8bb0\u5fc6\u6355\u83b7\u72b6\u6001\u53d8\u91cf\u7684\u5206\u7c7b\u8bed\u4e49\uff09\u3001\u6761\u4ef6\u74f6\u9888\u9002\u914d\u5668\uff08\u6839\u636e\u5f53\u524d\u72b6\u6001\u52a8\u6001\u751f\u6210\u4f4e\u79e9\u9002\u914d\u53c2\u6570\uff09\u3001\u6570\u503c-\u72b6\u6001\u5339\u914d\u6a21\u5757\uff08\u68c0\u6d4b\u72b6\u6001\u53d8\u91cf\u672c\u8eab\u7684\u5f02\u5e38\uff09\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSTAR\u80fd\u591f\u63d0\u5347\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u5728\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\u3002", "conclusion": "STAR\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u6a21\u5757\uff0c\u80fd\u591f\u6709\u6548\u589e\u5f3a\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u5bf9\u72b6\u6001\u53d8\u91cf\u7684\u5efa\u6a21\u548c\u5229\u7528\u80fd\u529b\uff0c\u63d0\u5347\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2510.16645", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.16645", "abs": "https://arxiv.org/abs/2510.16645", "authors": ["Zhixuan He", "Yue Feng"], "title": "Unleashing Diverse Thinking Modes in LLMs through Multi-Agent Collaboration", "comment": null, "summary": "Large Language Models (LLMs) demonstrate strong performance but often lack\ninterpretable reasoning. This paper introduces the Multi-Agent Collaboration\nFramework for Diverse Thinking Modes (DiMo), which enhances both performance\nand interpretability by simulating a structured debate among four specialized\nLLM agents. Each agent embodies a distinct reasoning paradigm, allowing the\nframework to collaboratively explore diverse cognitive approaches. Through\niterative debate, agents challenge and refine initial responses, yielding more\nrobust conclusions and an explicit, auditable reasoning chain. Across six\nbenchmarks and under a unified open-source setup, DiMo improves accuracy over\nwidely used single-model and debate baselines, with the largest gains on math.\nWe position DiMo as a semantics-aware, Web-native multi-agent framework: it\nmodels human-machine intelligence with LLM agents that produce semantically\ntyped, URL-annotated evidence chains for explanations and user-friendly\ninteractions. Although our experiments use standard reasoning benchmarks, the\nframework is designed to be instantiated over Web corpora and knowledge graphs,\ncombining retrieval-augmented reasoning with structured justifications that\ndownstream systems can inspect and reuse.", "AI": {"tldr": "DiMo\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u56db\u4e2a\u4e13\u95e8LLM\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u7ed3\u6784\u5316\u8fa9\u8bba\u6765\u63d0\u5347\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u6027\u80fd\u5f3a\u5927\u4f46\u7f3a\u4e4f\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u9700\u8981\u589e\u5f3a\u63a8\u7406\u6027\u80fd\u548c\u900f\u660e\u5ea6\u3002", "method": "\u4f7f\u7528\u56db\u4e2a\u4e13\u95e8\u5316LLM\u667a\u80fd\u4f53\uff0c\u6bcf\u4e2a\u4ee3\u8868\u4e0d\u540c\u7684\u63a8\u7406\u8303\u5f0f\uff0c\u901a\u8fc7\u8fed\u4ee3\u8fa9\u8bba\u6311\u6218\u548c\u4f18\u5316\u521d\u59cb\u54cd\u5e94\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDiMo\u5728\u7edf\u4e00\u5f00\u6e90\u8bbe\u7f6e\u4e0b\u4f18\u4e8e\u5355\u6a21\u578b\u548c\u8fa9\u8bba\u57fa\u7ebf\uff0c\u5728\u6570\u5b66\u4efb\u52a1\u4e0a\u63d0\u5347\u6700\u5927\u3002", "conclusion": "DiMo\u662f\u4e00\u4e2a\u8bed\u4e49\u611f\u77e5\u7684Web\u539f\u751f\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u80fd\u591f\u4ea7\u751f\u8bed\u4e49\u7c7b\u578b\u5316\u3001URL\u6ce8\u91ca\u7684\u8bc1\u636e\u94fe\uff0c\u652f\u6301\u4e0b\u6e38\u7cfb\u7edf\u68c0\u67e5\u548c\u91cd\u7528\u3002"}}
{"id": "2510.16253", "categories": ["cs.LG", "cs.AI", "q-bio.BM", "q-bio.QM", "stat.ML", "I.2.1; J.3"], "pdf": "https://arxiv.org/pdf/2510.16253", "abs": "https://arxiv.org/abs/2510.16253", "authors": ["Arielle Sanford", "Shuo Sun", "Christian B. Mendl"], "title": "Protein Folding with Neural Ordinary Differential Equations", "comment": null, "summary": "Recent advances in protein structure prediction, such as AlphaFold, have\ndemonstrated the power of deep neural architectures like the Evoformer for\ncapturing complex spatial and evolutionary constraints on protein conformation.\nHowever, the depth of the Evoformer, comprising 48 stacked blocks, introduces\nhigh computational costs and rigid layerwise discretization. Inspired by Neural\nOrdinary Differential Equations (Neural ODEs), we propose a continuous-depth\nformulation of the Evoformer, replacing its 48 discrete blocks with a Neural\nODE parameterization that preserves its core attention-based operations. This\ncontinuous-time Evoformer achieves constant memory cost (in depth) via the\nadjoint method, while allowing a principled trade-off between runtime and\naccuracy through adaptive ODE solvers. Benchmarking on protein structure\nprediction tasks, we find that the Neural ODE-based Evoformer produces\nstructurally plausible predictions and reliably captures certain secondary\nstructure elements, such as alpha-helices, though it does not fully replicate\nthe accuracy of the original architecture. However, our model achieves this\nperformance using dramatically fewer resources, just 17.5 hours of training on\na single GPU, highlighting the promise of continuous-depth models as a\nlightweight and interpretable alternative for biomolecular modeling. This work\nopens new directions for efficient and adaptive protein structure prediction\nframeworks.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u795e\u7ecf\u5e38\u5fae\u5206\u65b9\u7a0b\u7684\u8fde\u7eed\u6df1\u5ea6Evoformer\uff0c\u66ff\u4ee3AlphaFold\u4e2d48\u4e2a\u79bb\u6563\u5757\uff0c\u5b9e\u73b0\u6052\u5b9a\u5185\u5b58\u6210\u672c\u548c\u8ba1\u7b97\u6548\u7387\u63d0\u5347", "motivation": "\u4f20\u7edfEvoformer\u768448\u5c42\u6df1\u5ea6\u7ed3\u6784\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u5c42\u95f4\u79bb\u6563\u5316\uff0c\u9700\u8981\u66f4\u8f7b\u91cf\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848", "method": "\u4f7f\u7528\u795e\u7ecfODE\u53c2\u6570\u5316Evoformer\uff0c\u4fdd\u6301\u6838\u5fc3\u6ce8\u610f\u529b\u64cd\u4f5c\uff0c\u901a\u8fc7\u4f34\u968f\u65b9\u6cd5\u5b9e\u73b0\u6052\u5b9a\u5185\u5b58\u6210\u672c\uff0c\u5229\u7528\u81ea\u9002\u5e94ODE\u6c42\u89e3\u5668\u5e73\u8861\u8fd0\u884c\u65f6\u95f4\u4e0e\u7cbe\u5ea6", "result": "\u5728\u86cb\u767d\u8d28\u7ed3\u6784\u9884\u6d4b\u4efb\u52a1\u4e2d\u4ea7\u751f\u7ed3\u6784\u5408\u7406\u7684\u9884\u6d4b\uff0c\u53ef\u9760\u6355\u6349\u03b1-\u87ba\u65cb\u7b49\u4e8c\u7ea7\u7ed3\u6784\u5143\u7d20\uff0c\u4f46\u7cbe\u5ea6\u672a\u5b8c\u5168\u8fbe\u5230\u539f\u59cb\u67b6\u6784\u6c34\u5e73", "conclusion": "\u8fde\u7eed\u6df1\u5ea6\u6a21\u578b\u4f5c\u4e3a\u8f7b\u91cf\u53ef\u89e3\u91ca\u66ff\u4ee3\u65b9\u6848\u5728\u751f\u7269\u5206\u5b50\u5efa\u6a21\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u5355GPU\u4ec5\u970017.5\u5c0f\u65f6\u8bad\u7ec3\uff0c\u4e3a\u9ad8\u6548\u81ea\u9002\u5e94\u86cb\u767d\u8d28\u7ed3\u6784\u9884\u6d4b\u5f00\u8f9f\u65b0\u65b9\u5411"}}
{"id": "2510.16559", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16559", "abs": "https://arxiv.org/abs/2510.16559", "authors": ["Tian Xia", "Tianrun Gao", "Wenhao Deng", "Long Wei", "Xiaowei Qian", "Yixian Jiang", "Chenglei Yu", "Tailin Wu"], "title": "BuildArena: A Physics-Aligned Interactive Benchmark of LLMs for Engineering Construction", "comment": "33 pages, 10 figures", "summary": "Engineering construction automation aims to transform natural language\nspecifications into physically viable structures, requiring complex integrated\nreasoning under strict physical constraints. While modern LLMs possess broad\nknowledge and strong reasoning capabilities that make them promising candidates\nfor this domain, their construction competencies remain largely unevaluated. To\naddress this gap, we introduce BuildArena, the first physics-aligned\ninteractive benchmark designed for language-driven engineering construction. It\ncontributes to the community in four aspects: (1) a highly customizable\nbenchmarking framework for in-depth comparison and analysis of LLMs; (2) an\nextendable task design strategy spanning static and dynamic mechanics across\nmultiple difficulty tiers; (3) a 3D Spatial Geometric Computation Library for\nsupporting construction based on language instructions; (4) a baseline LLM\nagentic workflow that effectively evaluates diverse model capabilities. On\neight frontier LLMs, BuildArena comprehensively evaluates their capabilities\nfor language-driven and physics-grounded construction automation. The project\npage is at https://build-arena.github.io/.", "AI": {"tldr": "BuildArena\u662f\u9996\u4e2a\u9762\u5411\u8bed\u8a00\u9a71\u52a8\u5de5\u7a0b\u5efa\u8bbe\u7684\u7269\u7406\u5bf9\u9f50\u4ea4\u4e92\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u5de5\u7a0b\u5efa\u9020\u81ea\u52a8\u5316\u4e2d\u7684\u80fd\u529b\u3002", "motivation": "\u5de5\u7a0b\u5efa\u9020\u81ea\u52a8\u5316\u9700\u8981\u5c06\u81ea\u7136\u8bed\u8a00\u89c4\u8303\u8f6c\u5316\u4e3a\u7269\u7406\u53ef\u884c\u7684\u7ed3\u6784\uff0c\u4f46\u73b0\u4ee3LLM\u5728\u6b64\u9886\u57df\u7684\u5efa\u9020\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u8bc4\u4f30\u3002", "method": "\u5f00\u53d1\u4e86\u9ad8\u5ea6\u53ef\u5b9a\u5236\u7684\u57fa\u51c6\u6846\u67b6\u3001\u53ef\u6269\u5c55\u7684\u4efb\u52a1\u8bbe\u8ba1\u7b56\u7565\u30013D\u7a7a\u95f4\u51e0\u4f55\u8ba1\u7b97\u5e93\u548c\u57fa\u7ebfLLM\u4ee3\u7406\u5de5\u4f5c\u6d41\u3002", "result": "\u57288\u4e2a\u524d\u6cbfLLM\u4e0a\u5168\u9762\u8bc4\u4f30\u4e86\u5b83\u4eec\u5728\u8bed\u8a00\u9a71\u52a8\u548c\u7269\u7406\u57fa\u7840\u5efa\u9020\u81ea\u52a8\u5316\u65b9\u9762\u7684\u80fd\u529b\u3002", "conclusion": "BuildArena\u586b\u8865\u4e86LLM\u5728\u5de5\u7a0b\u5efa\u9020\u9886\u57df\u80fd\u529b\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u4e3a\u793e\u533a\u63d0\u4f9b\u4e86\u9996\u4e2a\u7269\u7406\u5bf9\u9f50\u7684\u4ea4\u4e92\u57fa\u51c6\u3002"}}
{"id": "2510.16015", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16015", "abs": "https://arxiv.org/abs/2510.16015", "authors": ["Qian Sun", "Graham Hults", "Susu Xu"], "title": "Decision-focused Sensing and Forecasting for Adaptive and Rapid Flood Response: An Implicit Learning Approach", "comment": null, "summary": "Timely and reliable decision-making is vital for flood emergency response,\nyet it remains severely hindered by limited and imprecise situational awareness\ndue to various budget and data accessibility constraints. Traditional flood\nmanagement systems often rely on in-situ sensors to calibrate remote\nsensing-based large-scale flood depth forecasting models, and further take\nflood depth estimates to optimize flood response decisions. However, these\napproaches often take fixed, decision task-agnostic strategies to decide where\nto put in-situ sensors (e.g., maximize overall information gain) and train\nflood forecasting models (e.g., minimize average forecasting errors), but\noverlook that systems with the same sensing gain and average forecasting errors\nmay lead to distinct decisions. To address this, we introduce a novel\ndecision-focused framework that strategically selects locations for in-situ\nsensor placement and optimize spatio-temporal flood forecasting models to\noptimize downstream flood response decision regrets. Our end-to-end pipeline\nintegrates four components: a contextual scoring network, a differentiable\nsensor selection module under hard budget constraints, a spatio-temporal flood\nreconstruction and forecasting model, and a differentiable decision layer\ntailored to task-specific objectives. Central to our approach is the\nincorporation of Implicit Maximum Likelihood Estimation (I-MLE) to enable\ngradient-based learning over discrete sensor configurations, and probabilistic\ndecision heads to enable differentiable approximation to various constrained\ndisaster response tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u51b3\u7b56\u5bfc\u5411\u7684\u6d2a\u6c34\u7ba1\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u4f18\u5316\u4f20\u611f\u5668\u90e8\u7f72\u548c\u6d2a\u6c34\u9884\u6d4b\u6a21\u578b\uff0c\u76f4\u63a5\u6700\u5c0f\u5316\u4e0b\u6e38\u5e94\u6025\u51b3\u7b56\u7684\u9057\u61be\u503c\uff0c\u800c\u975e\u4f20\u7edf\u7684\u4efb\u52a1\u65e0\u5173\u4f18\u5316\u76ee\u6807\u3002", "motivation": "\u4f20\u7edf\u6d2a\u6c34\u7ba1\u7406\u7cfb\u7edf\u91c7\u7528\u56fa\u5b9a\u3001\u4efb\u52a1\u65e0\u5173\u7684\u7b56\u7565\u90e8\u7f72\u4f20\u611f\u5668\u548c\u8bad\u7ec3\u9884\u6d4b\u6a21\u578b\uff0c\u5ffd\u89c6\u4e86\u76f8\u540c\u611f\u77e5\u589e\u76ca\u548c\u9884\u6d4b\u8bef\u5dee\u53ef\u80fd\u5bfc\u81f4\u4e0d\u540c\u51b3\u7b56\u7ed3\u679c\u7684\u95ee\u9898\uff0c\u9700\u8981\u5c06\u51b3\u7b56\u4f18\u5316\u76f4\u63a5\u6574\u5408\u5230\u7cfb\u7edf\u8bbe\u8ba1\u4e2d\u3002", "method": "\u7aef\u5230\u7aef\u6846\u67b6\u5305\u542b\u56db\u4e2a\u7ec4\u4ef6\uff1a\u4e0a\u4e0b\u6587\u8bc4\u5206\u7f51\u7edc\u3001\u9884\u7b97\u7ea6\u675f\u4e0b\u7684\u53ef\u5fae\u5206\u4f20\u611f\u5668\u9009\u62e9\u6a21\u5757\u3001\u65f6\u7a7a\u6d2a\u6c34\u91cd\u5efa\u9884\u6d4b\u6a21\u578b\u3001\u4ee5\u53ca\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u76ee\u6807\u7684\u53ef\u5fae\u5206\u51b3\u7b56\u5c42\uff0c\u91c7\u7528\u9690\u5f0f\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u5b9e\u73b0\u79bb\u6563\u4f20\u611f\u5668\u914d\u7f6e\u7684\u68af\u5ea6\u5b66\u4e60\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6218\u7565\u6027\u5730\u9009\u62e9\u4f20\u611f\u5668\u4f4d\u7f6e\u5e76\u4f18\u5316\u6d2a\u6c34\u9884\u6d4b\u6a21\u578b\uff0c\u76f4\u63a5\u9488\u5bf9\u4e0b\u6e38\u6d2a\u6c34\u54cd\u5e94\u51b3\u7b56\u8fdb\u884c\u4f18\u5316\uff0c\u63d0\u9ad8\u4e86\u51b3\u7b56\u7684\u53ca\u65f6\u6027\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u51b3\u7b56\u5bfc\u5411\u7684\u6846\u67b6\u901a\u8fc7\u7aef\u5230\u7aef\u4f18\u5316\u4f20\u611f\u5668\u90e8\u7f72\u548c\u9884\u6d4b\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6d2a\u6c34\u5e94\u6025\u54cd\u5e94\u7684\u51b3\u7b56\u8d28\u91cf\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u611f\u77e5\u4e0e\u51b3\u7b56\u8131\u8282\u7684\u95ee\u9898\u3002"}}
{"id": "2510.16670", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16670", "abs": "https://arxiv.org/abs/2510.16670", "authors": ["Yiyang Liu", "James C. Liang", "Heng Fan", "Wenhao Yang", "Yiming Cui", "Xiaotian Han", "Lifu Huang", "Dongfang Liu", "Qifan Wang", "Cheng Han"], "title": "All You Need is One: Capsule Prompt Tuning with a Single Vector", "comment": "NeurIPS 2025", "summary": "Prompt-based learning has emerged as a parameter-efficient finetuning (PEFT)\napproach to facilitate Large Language Model (LLM) adaptation to downstream\ntasks by conditioning generation with task-aware guidance. Despite its\nsuccesses, current prompt-based learning methods heavily rely on laborious grid\nsearching for optimal prompt length and typically require considerable number\nof prompts, introducing additional computational burden. Worse yet, our pioneer\nfindings indicate that the task-aware prompt design is inherently limited by\nits absence of instance-aware information, leading to a subtle attention\ninterplay with the input sequence. In contrast, simply incorporating\ninstance-aware information as a part of the guidance can enhance the\nprompt-tuned model performance without additional fine-tuning. Moreover, we\nfind an interesting phenomenon, namely \"attention anchor\", that incorporating\ninstance-aware tokens at the earliest position of the sequence can successfully\npreserve strong attention to critical structural information and exhibit more\nactive attention interaction with all input tokens. In light of our\nobservation, we introduce Capsule Prompt-Tuning (CaPT), an efficient and\neffective solution that leverages off-the-shelf, informative instance semantics\ninto prompt-based learning. Our approach innovatively integrates both\ninstance-aware and task-aware information in a nearly parameter-free manner\n(i.e., one single capsule prompt). Empirical results demonstrate that our\nmethod can exhibit superior performance across various language tasks (e.g.,\n84.03\\% average accuracy on T5-Large), serving as an \"attention anchor,\" while\nenjoying high parameter efficiency (e.g., 0.003\\% of model parameters on\nLlama3.2-1B).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Capsule Prompt-Tuning (CaPT)\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u5b9e\u4f8b\u611f\u77e5\u4fe1\u606f\u4f5c\u4e3a\u5355\u4e00\u80f6\u56ca\u63d0\u793a\uff0c\u5728\u51e0\u4e4e\u4e0d\u589e\u52a0\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u63d0\u793a\u8c03\u4f18\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u63d0\u793a\u7684\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u4f9d\u8d56\u8017\u65f6\u7684\u7f51\u683c\u641c\u7d22\u786e\u5b9a\u6700\u4f73\u63d0\u793a\u957f\u5ea6\uff1b2\uff09\u7f3a\u4e4f\u5b9e\u4f8b\u611f\u77e5\u4fe1\u606f\uff0c\u5bfc\u81f4\u4e0e\u8f93\u5165\u5e8f\u5217\u7684\u6ce8\u610f\u529b\u4ea4\u4e92\u53d7\u9650\u3002", "method": "CaPT\u65b9\u6cd5\u521b\u65b0\u6027\u5730\u5c06\u5b9e\u4f8b\u611f\u77e5\u548c\u4efb\u52a1\u611f\u77e5\u4fe1\u606f\u6574\u5408\u5230\u5355\u4e2a\u80f6\u56ca\u63d0\u793a\u4e2d\uff0c\u5c06\u5b9e\u4f8b\u611f\u77e5\u6807\u8bb0\u7f6e\u4e8e\u5e8f\u5217\u6700\u524d\u4f4d\u7f6e\u4f5c\u4e3a'\u6ce8\u610f\u529b\u951a\u70b9'\uff0c\u4ee5\u4fdd\u6301\u5bf9\u5173\u952e\u7ed3\u6784\u4fe1\u606f\u7684\u5f3a\u6ce8\u610f\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cCaPT\u5728T5-Large\u4e0a\u8fbe\u523084.03%\u7684\u5e73\u5747\u51c6\u786e\u7387\uff0c\u5728Llama3.2-1B\u4e0a\u4ec5\u4f7f\u7528\u6a21\u578b\u53c2\u6570\u76840.003%\uff0c\u5b9e\u73b0\u4e86\u9ad8\u53c2\u6570\u6548\u7387\u548c\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "CaPT\u901a\u8fc7\u5f15\u5165\u5b9e\u4f8b\u611f\u77e5\u4fe1\u606f\u4f5c\u4e3a\u6ce8\u610f\u529b\u951a\u70b9\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u63d0\u793a\u8c03\u4f18\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u4fdd\u6301\u9ad8\u53c2\u6570\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2510.16356", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16356", "abs": "https://arxiv.org/abs/2510.16356", "authors": ["Fuqun Han", "Stanley Osher", "Wuchen Li"], "title": "Sparse Transformer Architectures via Regularized Wasserstein Proximal Operator with $L_1$ Prior", "comment": null, "summary": "In this work, we propose a sparse transformer architecture that incorporates\nprior information about the underlying data distribution directly into the\ntransformer structure of the neural network. The design of the model is\nmotivated by a special optimal transport problem, namely the regularized\nWasserstein proximal operator, which admits a closed-form solution and turns\nout to be a special representation of transformer architectures. Compared with\nclassical flow-based models, the proposed approach improves the convexity\nproperties of the optimization problem and promotes sparsity in the generated\nsamples. Through both theoretical analysis and numerical experiments, including\napplications in generative modeling and Bayesian inverse problems, we\ndemonstrate that the sparse transformer achieves higher accuracy and faster\nconvergence to the target distribution than classical neural ODE-based methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7a00\u758ftransformer\u67b6\u6784\uff0c\u5c06\u6570\u636e\u5206\u5e03\u7684\u5148\u9a8c\u4fe1\u606f\u76f4\u63a5\u878d\u5165\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\uff0c\u57fa\u4e8e\u6b63\u5219\u5316Wasserstein\u8fd1\u7aef\u7b97\u5b50\u8bbe\u8ba1\uff0c\u76f8\u6bd4\u4f20\u7edf\u6d41\u6a21\u578b\u6539\u5584\u4e86\u4f18\u5316\u95ee\u9898\u7684\u51f8\u6027\u5e76\u4fc3\u8fdb\u751f\u6210\u6837\u672c\u7684\u7a00\u758f\u6027\u3002", "motivation": "\u5c06\u6570\u636e\u5206\u5e03\u7684\u5148\u9a8c\u4fe1\u606f\u76f4\u63a5\u6574\u5408\u5230transformer\u67b6\u6784\u4e2d\uff0c\u4ee5\u6539\u5584\u751f\u6210\u6a21\u578b\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u9488\u5bf9\u4f18\u5316\u95ee\u9898\u7684\u51f8\u6027\u548c\u6837\u672c\u7a00\u758f\u6027\u3002", "method": "\u57fa\u4e8e\u6b63\u5219\u5316Wasserstein\u8fd1\u7aef\u7b97\u5b50\u8bbe\u8ba1\u7a00\u758ftransformer\u67b6\u6784\uff0c\u8be5\u7b97\u5b50\u5177\u6709\u95ed\u5f0f\u89e3\u4e14\u8868\u73b0\u4e3atransformer\u7684\u7279\u6b8a\u8868\u793a\u5f62\u5f0f\u3002", "result": "\u5728\u751f\u6210\u5efa\u6a21\u548c\u8d1d\u53f6\u65af\u9006\u95ee\u9898\u5e94\u7528\u4e2d\uff0c\u7a00\u758ftransformer\u6bd4\u4f20\u7edf\u57fa\u4e8e\u795e\u7ecfODE\u7684\u65b9\u6cd5\u5177\u6709\u66f4\u9ad8\u7cbe\u5ea6\u548c\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7a00\u758ftransformer\u67b6\u6784\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u90fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4e3a\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.16572", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.16572", "abs": "https://arxiv.org/abs/2510.16572", "authors": ["Ayush Chopra", "Aman Sharma", "Feroz Ahmad", "Luca Muscariello", "Vijoy Pandey", "Ramesh Raskar"], "title": "Ripple Effect Protocol: Coordinating Agent Populations", "comment": null, "summary": "Modern AI agents can exchange messages using protocols such as A2A and ACP,\nyet these mechanisms emphasize communication over coordination. As agent\npopulations grow, this limitation produces brittle collective behavior, where\nindividually smart agents converge on poor group outcomes. We introduce the\nRipple Effect Protocol (REP), a coordination protocol in which agents share not\nonly their decisions but also lightweight sensitivities - signals expressing\nhow their choices would change if key environmental variables shifted. These\nsensitivities ripple through local networks, enabling groups to align faster\nand more stably than with agent-centric communication alone. We formalize REP's\nprotocol specification, separating required message schemas from optional\naggregation rules, and evaluate it across scenarios with varying incentives and\nnetwork topologies. Benchmarks across three domains: (i) supply chain cascades\n(Beer Game), (ii) preference aggregation in sparse networks (Movie Scheduling),\nand (iii) sustainable resource allocation (Fishbanks) show that REP improves\ncoordination accuracy and efficiency over A2A by 41 to 100%, while flexibly\nhandling multimodal sensitivity signals from LLMs. By making coordination a\nprotocol-level capability, REP provides scalable infrastructure for the\nemerging Internet of Agents", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Ripple Effect Protocol (REP)\uff0c\u4e00\u79cd\u534f\u8c03\u534f\u8bae\uff0c\u8ba9\u667a\u80fd\u4f53\u4e0d\u4ec5\u5206\u4eab\u51b3\u7b56\uff0c\u8fd8\u5206\u4eab\u8f7b\u91cf\u7ea7\u7684\u654f\u611f\u5ea6\u4fe1\u53f7\uff0c\u4ece\u800c\u5728\u7fa4\u4f53\u4e2d\u5b9e\u73b0\u66f4\u5feb\u66f4\u7a33\u5b9a\u7684\u534f\u8c03\u3002", "motivation": "\u73b0\u6709\u7684AI\u667a\u80fd\u4f53\u901a\u4fe1\u534f\u8bae\uff08\u5982A2A\u548cACP\uff09\u5f3a\u8c03\u901a\u4fe1\u800c\u975e\u534f\u8c03\uff0c\u968f\u7740\u667a\u80fd\u4f53\u7fa4\u4f53\u89c4\u6a21\u6269\u5927\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u8106\u5f31\u7684\u96c6\u4f53\u884c\u4e3a\uff0c\u5373\u4f7f\u4e2a\u4f53\u667a\u80fd\u4f53\u5f88\u806a\u660e\uff0c\u7fa4\u4f53\u7ed3\u679c\u4e5f\u5f88\u5dee\u3002", "method": "\u5f15\u5165REP\u534f\u8bae\uff0c\u667a\u80fd\u4f53\u5206\u4eab\u51b3\u7b56\u548c\u8f7b\u91cf\u7ea7\u654f\u611f\u5ea6\u4fe1\u53f7\uff08\u8868\u8fbe\u5173\u952e\u73af\u5883\u53d8\u91cf\u53d8\u5316\u65f6\u9009\u62e9\u5982\u4f55\u6539\u53d8\uff09\uff0c\u8fd9\u4e9b\u654f\u611f\u5ea6\u5728\u5c40\u90e8\u7f51\u7edc\u4e2d\u4f20\u64ad\u3002\u5f62\u5f0f\u5316\u534f\u8bae\u89c4\u8303\uff0c\u5206\u79bb\u5fc5\u9700\u7684\u6d88\u606f\u6a21\u5f0f\u548c\u53ef\u9009\u7684\u805a\u5408\u89c4\u5219\u3002", "result": "\u5728\u4e09\u4e2a\u9886\u57df\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff1a(i)\u4f9b\u5e94\u94fe\u7ea7\u8054\uff08\u5564\u9152\u6e38\u620f\uff09\u3001(ii)\u7a00\u758f\u7f51\u7edc\u4e2d\u7684\u504f\u597d\u805a\u5408\uff08\u7535\u5f71\u6392\u671f\uff09\u3001(iii)\u53ef\u6301\u7eed\u8d44\u6e90\u5206\u914d\uff08Fishbanks\uff09\uff0cREP\u76f8\u6bd4A2A\u5c06\u534f\u8c03\u51c6\u786e\u6027\u548c\u6548\u7387\u63d0\u9ad8\u4e8641%\u5230100%\uff0c\u5e76\u80fd\u7075\u6d3b\u5904\u7406\u6765\u81eaLLM\u7684\u591a\u6a21\u6001\u654f\u611f\u5ea6\u4fe1\u53f7\u3002", "conclusion": "\u901a\u8fc7\u5c06\u534f\u8c03\u4f5c\u4e3a\u534f\u8bae\u7ea7\u80fd\u529b\uff0cREP\u4e3a\u65b0\u5174\u7684\u667a\u80fd\u4f53\u4e92\u8054\u7f51\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u57fa\u7840\u8bbe\u65bd\u3002"}}
{"id": "2510.16016", "categories": ["cs.LG", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2510.16016", "abs": "https://arxiv.org/abs/2510.16016", "authors": ["Saeed Salehi"], "title": "Transfer learning strategies for accelerating reinforcement-learning-based flow control", "comment": null, "summary": "This work investigates transfer learning strategies to accelerate deep\nreinforcement learning (DRL) for multifidelity control of chaotic fluid flows.\nProgressive neural networks (PNNs), a modular architecture designed to preserve\nand reuse knowledge across tasks, are employed for the first time in the\ncontext of DRL-based flow control. In addition, a comprehensive benchmarking of\nconventional fine-tuning strategies is conducted, evaluating their performance,\nconvergence behavior, and ability to retain transferred knowledge. The\nKuramoto-Sivashinsky (KS) system is employed as a benchmark to examine how\nknowledge encoded in control policies, trained in low-fidelity environments,\ncan be effectively transferred to high-fidelity settings. Systematic\nevaluations show that while fine-tuning can accelerate convergence, it is\nhighly sensitive to pretraining duration and prone to catastrophic forgetting.\nIn contrast, PNNs enable stable and efficient transfer by preserving prior\nknowledge and providing consistent performance gains, and are notably robust to\noverfitting during the pretraining phase. Layer-wise sensitivity analysis\nfurther reveals how PNNs dynamically reuse intermediate representations from\nthe source policy while progressively adapting deeper layers to the target\ntask. Moreover, PNNs remain effective even when the source and target\nenvironments differ substantially, such as in cases with mismatched physical\nregimes or control objectives, where fine-tuning strategies often result in\nsuboptimal adaptation or complete failure of knowledge transfer. The results\nhighlight the potential of novel transfer learning frameworks for robust,\nscalable, and computationally efficient flow control that can potentially be\napplied to more complex flow configurations.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u4f7f\u7528\u6e10\u8fdb\u795e\u7ecf\u7f51\u7edc\u548c\u5fae\u8c03\u7b56\u7565\u6765\u52a0\u901f\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u591a\u4fdd\u771f\u5ea6\u6df7\u6c8c\u6d41\u4f53\u63a7\u5236\u4e2d\u7684\u77e5\u8bc6\u8fc1\u79fb\uff0c\u53d1\u73b0\u6e10\u8fdb\u795e\u7ecf\u7f51\u7edc\u5728\u4fdd\u6301\u5148\u524d\u77e5\u8bc6\u548c\u7a33\u5b9a\u8fc1\u79fb\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\u3002", "motivation": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u6d41\u4f53\u63a7\u5236\u4e2d\u8bad\u7ec3\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u63a2\u7d22\u6709\u6548\u7684\u8fc1\u79fb\u5b66\u4e60\u7b56\u7565\u6765\u52a0\u901f\u9ad8\u4fdd\u771f\u5ea6\u73af\u5883\u4e2d\u7684\u5b66\u4e60\u8fc7\u7a0b\uff0c\u89e3\u51b3\u5fae\u8c03\u65b9\u6cd5\u5b58\u5728\u7684\u707e\u96be\u6027\u9057\u5fd8\u548c\u654f\u611f\u6027\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6e10\u8fdb\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u9996\u6b21\u5728\u57fa\u4e8eDRL\u7684\u6d41\u63a7\u5236\u4e2d\u5e94\u7528\u8fd9\u79cd\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u540c\u65f6\u7cfb\u7edf\u6bd4\u8f83\u4f20\u7edf\u5fae\u8c03\u7b56\u7565\u7684\u6027\u80fd\u3001\u6536\u655b\u884c\u4e3a\u548c\u77e5\u8bc6\u4fdd\u6301\u80fd\u529b\uff0c\u4ee5Kuramoto-Sivashinsky\u7cfb\u7edf\u4e3a\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\u3002", "result": "\u6e10\u8fdb\u795e\u7ecf\u7f51\u7edc\u901a\u8fc7\u4fdd\u7559\u5148\u9a8c\u77e5\u8bc6\u5b9e\u73b0\u7a33\u5b9a\u9ad8\u6548\u7684\u77e5\u8bc6\u8fc1\u79fb\uff0c\u5bf9\u9884\u8bad\u7ec3\u9636\u6bb5\u7684\u8fc7\u62df\u5408\u5177\u6709\u9c81\u68d2\u6027\uff1b\u800c\u5fae\u8c03\u867d\u7136\u80fd\u52a0\u901f\u6536\u655b\uff0c\u4f46\u5bf9\u9884\u8bad\u7ec3\u65f6\u957f\u654f\u611f\u4e14\u5bb9\u6613\u53d1\u751f\u707e\u96be\u6027\u9057\u5fd8\u3002", "conclusion": "\u6e10\u8fdb\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\u5728\u6e90\u73af\u5883\u548c\u76ee\u6807\u73af\u5883\u5dee\u5f02\u8f83\u5927\u65f6\u4ecd\u80fd\u6709\u6548\u5de5\u4f5c\uff0c\u4e3a\u7a33\u5065\u3001\u53ef\u6269\u5c55\u548c\u8ba1\u7b97\u9ad8\u6548\u7684\u6d41\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u5e94\u7528\u4e8e\u66f4\u590d\u6742\u7684\u6d41\u52a8\u914d\u7f6e\u3002"}}
{"id": "2510.16685", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16685", "abs": "https://arxiv.org/abs/2510.16685", "authors": ["Damin Zhang", "Julia Rayz"], "title": "Temporal Understanding under Deictic Frame of Reference", "comment": "Under review", "summary": "Understanding time is fundamental to human cognition, where temporal\nexperience is often conceptualized through spatial metaphors grounded in\nsensory-motor experience. For example, \"summer is approaching\" parallels \"We\nare approaching the summer\". In such expressions, humans rely on a frame of\nreference (FoR) to interpret meaning relative to a particular viewpoint.\nExtending this concept to time, a temporal frame of reference (t-FoR) defines\nhow temporal relations are perceived relative to an experiencer's moment of\n\"now\". While Large Language Models (LLMs) have shown remarkable advances in\nnatural language understanding, their ability to interpret and reason about\ntime remains limited. In this work, we introduce TUuD (Temporal Understanding\nunder Deictic t-FoR), a framework that evaluates how LLMs interpret time-event\nand event-event relations when the reference point of \"now\" dynamically shifts\nalong a timeline. Following recent work on temporal cognition\n\\cite{li2025other}, LLMs are prompted to rate the similarity between the\ncurrent moment and a target event from 0.00 (completely dissimilar) to 1.00\n(highly similar), where similarity quantifies perceived temporal alignment\nbetween the two points. Our results show that four evaluated LLMs exhibit\nmeasurable adaptation to a deictic t-FoR, with similarity ratings peaking\naround the present and decreasing toward past and future events. The\nadaptation, however, weakens beyond near-term contexts, suggesting that while\nLLMs display partial human-like temporal cognition, their temporal reasoning\nremains sensitive to reference-frame shifts and temporal distance.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86TUuD\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u52a8\u6001\u65f6\u95f4\u53c2\u8003\u70b9\u4e0b\u7406\u89e3\u65f6\u95f4-\u4e8b\u4ef6\u548c\u4e8b\u4ef6-\u4e8b\u4ef6\u5173\u7cfb\u7684\u80fd\u529b\u3002\u7814\u7a76\u53d1\u73b0LLMs\u8868\u73b0\u51fa\u90e8\u5206\u7c7b\u4f3c\u4eba\u7c7b\u7684\u65f6\u5e8f\u8ba4\u77e5\uff0c\u4f46\u5bf9\u53c2\u8003\u6846\u67b6\u53d8\u5316\u548c\u65f6\u95f4\u8ddd\u79bb\u654f\u611f\u3002", "motivation": "\u4eba\u7c7b\u901a\u8fc7\u7a7a\u95f4\u9690\u55bb\u7406\u89e3\u65f6\u95f4\uff0c\u4f7f\u7528\u65f6\u95f4\u53c2\u8003\u6846\u67b6(t-FoR)\u6765\u611f\u77e5\u65f6\u95f4\u5173\u7cfb\u3002\u867d\u7136LLMs\u5728\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5176\u65f6\u95f4\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u4ecd\u7136\u6709\u9650\uff0c\u9700\u8981\u8bc4\u4f30LLMs\u5982\u4f55\u89e3\u91ca\u52a8\u6001\u53d8\u5316\u7684\"\u73b0\u5728\"\u53c2\u8003\u70b9\u4e0b\u7684\u65f6\u95f4\u5173\u7cfb\u3002", "method": "\u5f15\u5165TUuD\u6846\u67b6\uff0c\u8ba9LLMs\u5728\u65f6\u95f4\u7ebf\u4e0a\u52a8\u6001\u79fb\u52a8\u7684\"\u73b0\u5728\"\u53c2\u8003\u70b9\u4e0b\uff0c\u8bc4\u4f30\u5f53\u524d\u65f6\u523b\u4e0e\u76ee\u6807\u4e8b\u4ef6\u4e4b\u95f4\u7684\u76f8\u4f3c\u5ea6\uff080.00-1.00\uff09\uff0c\u91cf\u5316\u4e24\u4e2a\u65f6\u95f4\u70b9\u4e4b\u95f4\u7684\u611f\u77e5\u65f6\u5e8f\u5bf9\u9f50\u7a0b\u5ea6\u3002", "result": "\u56db\u4e2a\u8bc4\u4f30\u7684LLMs\u8868\u73b0\u51fa\u5bf9\u6307\u793a\u6027t-FoR\u7684\u53ef\u6d4b\u91cf\u9002\u5e94\uff0c\u76f8\u4f3c\u5ea6\u8bc4\u5206\u5728\u73b0\u5728\u9644\u8fd1\u8fbe\u5230\u5cf0\u503c\uff0c\u5e76\u5411\u8fc7\u53bb\u548c\u672a\u6765\u4e8b\u4ef6\u9012\u51cf\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u9002\u5e94\u5728\u8d85\u51fa\u8fd1\u671f\u4e0a\u4e0b\u6587\u65f6\u4f1a\u51cf\u5f31\u3002", "conclusion": "LLMs\u663e\u793a\u51fa\u90e8\u5206\u7c7b\u4f3c\u4eba\u7c7b\u7684\u65f6\u5e8f\u8ba4\u77e5\uff0c\u4f46\u5176\u65f6\u95f4\u63a8\u7406\u4ecd\u7136\u5bf9\u53c2\u8003\u6846\u67b6\u53d8\u5316\u548c\u65f6\u95f4\u8ddd\u79bb\u654f\u611f\uff0c\u8868\u660e\u5176\u65f6\u95f4\u7406\u89e3\u80fd\u529b\u4ecd\u6709\u5c40\u9650\u6027\u3002"}}
{"id": "2510.16462", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16462", "abs": "https://arxiv.org/abs/2510.16462", "authors": ["Emmanuelle Claeys", "Elena Kerjean", "Jean-Michel Loubes"], "title": "Buzz, Choose, Forget: A Meta-Bandit Framework for Bee-Like Decision Making", "comment": null, "summary": "We introduce a sequential reinforcement learning framework for imitation\nlearning designed to model heterogeneous cognitive strategies in pollinators.\nFocusing on honeybees, our approach leverages trajectory similarity to capture\nand forecast behavior across individuals that rely on distinct strategies: some\nexploiting numerical cues, others drawing on memory, or being influenced by\nenvironmental factors such as weather. Through empirical evaluation, we show\nthat state-of-the-art imitation learning methods often fail in this setting:\nwhen expert policies shift across memory windows or deviate from optimality,\nthese models overlook both fast and slow learning behaviors and cannot\nfaithfully reproduce key decision patterns. Moreover, they offer limited\ninterpretability, hindering biological insight. Our contribution addresses\nthese challenges by (i) introducing a model that minimizes predictive loss\nwhile identifying the effective memory horizon most consistent with behavioral\ndata, and (ii) ensuring full interpretability to enable biologists to analyze\nunderlying decision-making strategies and finally (iii) providing a\nmathematical framework linking bee policy search with bandit formulations under\nvarying exploration-exploitation dynamics, and releasing a novel dataset of 80\ntracked bees observed under diverse weather conditions. This benchmark\nfacilitates research on pollinator cognition and supports ecological governance\nby improving simulations of insect behavior in agroecosystems. Our findings\nshed new light on the learning strategies and memory interplay shaping\npollinator decision-making.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u5e8f\u5217\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u7528\u4e8e\u6388\u7c89\u6606\u866b\u7684\u6a21\u4eff\u5b66\u4e60\uff0c\u80fd\u591f\u5efa\u6a21\u5f02\u8d28\u8ba4\u77e5\u7b56\u7565\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u4e13\u5bb6\u7b56\u7565\u53d8\u5316\u65f6\u65e0\u6cd5\u6355\u6349\u5feb\u901f\u548c\u6162\u901f\u5b66\u4e60\u884c\u4e3a\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u7406\u6388\u7c89\u6606\u866b\uff08\u5982\u871c\u8702\uff09\u7684\u5f02\u8d28\u8ba4\u77e5\u7b56\u7565\u65f6\u5b58\u5728\u5c40\u9650\uff0c\u5f53\u4e13\u5bb6\u7b56\u7565\u968f\u8bb0\u5fc6\u7a97\u53e3\u53d8\u5316\u6216\u504f\u79bb\u6700\u4f18\u65f6\uff0c\u8fd9\u4e9b\u6a21\u578b\u65e0\u6cd5\u51c6\u786e\u6355\u6349\u884c\u4e3a\u6a21\u5f0f\u4e14\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u5f15\u5165\u4e00\u4e2a\u6700\u5c0f\u5316\u9884\u6d4b\u635f\u5931\u7684\u6a21\u578b\uff0c\u8bc6\u522b\u4e0e\u884c\u4e3a\u6570\u636e\u6700\u4e00\u81f4\u7684\u6709\u6548\u8bb0\u5fc6\u8303\u56f4\uff0c\u786e\u4fdd\u5b8c\u5168\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u63d0\u4f9b\u5c06\u871c\u8702\u7b56\u7565\u641c\u7d22\u4e0e\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\u8054\u7cfb\u8d77\u6765\u7684\u6570\u5b66\u6846\u67b6\u3002", "result": "\u521b\u5efa\u4e86\u5305\u542b80\u53ea\u871c\u8702\u5728\u4e0d\u540c\u5929\u6c14\u6761\u4ef6\u4e0b\u8ffd\u8e2a\u7684\u65b0\u6570\u636e\u96c6\uff0c\u8be5\u57fa\u51c6\u4fc3\u8fdb\u4e86\u6388\u7c89\u6606\u866b\u8ba4\u77e5\u7814\u7a76\uff0c\u5e76\u901a\u8fc7\u6539\u8fdb\u519c\u4e1a\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u6606\u866b\u884c\u4e3a\u6a21\u62df\u6765\u652f\u6301\u751f\u6001\u6cbb\u7406\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7406\u89e3\u6388\u7c89\u6606\u866b\u7684\u5b66\u4e60\u7b56\u7565\u548c\u8bb0\u5fc6\u76f8\u4e92\u4f5c\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\uff0c\u6709\u52a9\u4e8e\u5206\u6790\u57fa\u7840\u51b3\u7b56\u7b56\u7565\u5e76\u6539\u5584\u751f\u6001\u7cfb\u7edf\u7684\u7ba1\u7406\u3002"}}
{"id": "2510.16829", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.16829", "abs": "https://arxiv.org/abs/2510.16829", "authors": ["Navreet Kaur", "Hoda Ayad", "Hayoung Jung", "Shravika Mittal", "Munmun De Choudhury", "Tanushree Mitra"], "title": "Who's Asking? Simulating Role-Based Questions for Conversational AI Evaluation", "comment": null, "summary": "Language model users often embed personal and social context in their\nquestions. The asker's role -- implicit in how the question is framed --\ncreates specific needs for an appropriate response. However, most evaluations,\nwhile capturing the model's capability to respond, often ignore who is asking.\nThis gap is especially critical in stigmatized domains such as opioid use\ndisorder (OUD), where accounting for users' contexts is essential to provide\naccessible, stigma-free responses. We propose CoRUS (COmmunity-driven Roles for\nUser-centric Question Simulation), a framework for simulating role-based\nquestions. Drawing on role theory and posts from an online OUD recovery\ncommunity (r/OpiatesRecovery), we first build a taxonomy of asker roles --\npatients, caregivers, practitioners. Next, we use it to simulate 15,321\nquestions that embed each role's goals, behaviors, and experiences. Our\nevaluations show that these questions are both highly believable and comparable\nto real-world data. When used to evaluate five LLMs, for the same question but\ndiffering roles, we find systematic differences: vulnerable roles, such as\npatients and caregivers, elicit more supportive responses (+17%) and reduced\nknowledge content (-19%) in comparison to practitioners. Our work demonstrates\nhow implicitly signaling a user's role shapes model responses, and provides a\nmethodology for role-informed evaluation of conversational AI.", "AI": {"tldr": "CoRUS\u6846\u67b6\u901a\u8fc7\u89d2\u8272\u7406\u8bba\u6a21\u62df\u57fa\u4e8e\u89d2\u8272\u7684\u63d0\u95ee\uff0c\u53d1\u73b0\u5728\u963f\u7247\u7c7b\u836f\u7269\u4f7f\u7528\u969c\u788d\u7b49\u654f\u611f\u9886\u57df\uff0c\u7528\u6237\u7684\u9690\u542b\u89d2\u8272\u4f1a\u663e\u8457\u5f71\u54cd\u8bed\u8a00\u6a21\u578b\u7684\u56de\u5e94\u65b9\u5f0f\uff0c\u8106\u5f31\u89d2\u8272\uff08\u60a3\u8005\u3001\u7167\u987e\u8005\uff09\u4f1a\u83b7\u5f97\u66f4\u591a\u652f\u6301\u6027\u56de\u5e94\u4f46\u77e5\u8bc6\u5185\u5bb9\u51cf\u5c11\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u5927\u591a\u5ffd\u7565\u63d0\u95ee\u8005\u89d2\u8272\uff0c\u4f46\u5728\u963f\u7247\u7c7b\u836f\u7269\u4f7f\u7528\u969c\u788d\u7b49\u6c61\u540d\u5316\u9886\u57df\uff0c\u8003\u8651\u7528\u6237\u80cc\u666f\u5bf9\u63d0\u4f9b\u53ef\u8bbf\u95ee\u3001\u65e0\u6c61\u540d\u7684\u56de\u5e94\u81f3\u5173\u91cd\u8981\u3002", "method": "\u57fa\u4e8e\u89d2\u8272\u7406\u8bba\u548c\u5728\u7ebfOUD\u5eb7\u590d\u793e\u533a\u5e16\u5b50\uff0c\u6784\u5efa\u63d0\u95ee\u8005\u89d2\u8272\u5206\u7c7b\uff08\u60a3\u8005\u3001\u7167\u987e\u8005\u3001\u4ece\u4e1a\u8005\uff09\uff0c\u5e76\u6a21\u62df15,321\u4e2a\u5d4c\u5165\u5404\u89d2\u8272\u76ee\u6807\u3001\u884c\u4e3a\u548c\u7ecf\u9a8c\u7684\u63d0\u95ee\u3002", "result": "\u6a21\u62df\u63d0\u95ee\u5177\u6709\u9ad8\u53ef\u4fe1\u5ea6\u4e14\u4e0e\u73b0\u5b9e\u6570\u636e\u76f8\u5f53\u3002\u8bc4\u4f305\u4e2aLLM\u53d1\u73b0\uff1a\u76f8\u540c\u95ee\u9898\u4f46\u4e0d\u540c\u89d2\u8272\u4f1a\u5f15\u53d1\u7cfb\u7edf\u6027\u5dee\u5f02\uff0c\u8106\u5f31\u89d2\u8272\u83b7\u5f97\u66f4\u591a\u652f\u6301\u6027\u56de\u5e94\uff08+17%\uff09\u4f46\u77e5\u8bc6\u5185\u5bb9\u51cf\u5c11\uff08-19%\uff09\u3002", "conclusion": "\u7528\u6237\u89d2\u8272\u7684\u9690\u542b\u4fe1\u53f7\u4f1a\u5851\u9020\u6a21\u578b\u56de\u5e94\uff0c\u63d0\u4f9b\u4e86\u57fa\u4e8e\u89d2\u8272\u7684\u5bf9\u8bddAI\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2510.16582", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16582", "abs": "https://arxiv.org/abs/2510.16582", "authors": ["Junchi Yu", "Yujie Liu", "Jindong Gu", "Philip Torr", "Dongzhan Zhou"], "title": "Can Knowledge-Graph-based Retrieval Augmented Generation Really Retrieve What You Need?", "comment": "NeurIPS 2025 (Spotlight)", "summary": "Retrieval-Augmented Generation (RAG) based on knowledge graphs (KGs) enhances\nlarge language models (LLMs) by providing structured and interpretable external\nknowledge. However, existing KG-based RAG methods struggle to retrieve accurate\nand diverse information from text-rich KGs for complex real-world queries.\nProcess Reward Models (PRMs) offer a way to align the retrieval process of\nKG-based RAG with query-specific knowledge requirements, but they heavily rely\non process-level supervision signals that are expensive and hard to obtain on\nKGs. To address this challenge, we propose GraphFlow, a framework that\nefficiently retrieves accurate and diverse knowledge required for real-world\nqueries from text-rich KGs. GraphFlow employs a transition-based flow matching\nobjective to jointly optimize a retrieval policy and a flow estimator. The flow\nestimator factorizes the reward of the retrieval outcome into the intermediate\nretrieval states. Such reward factorization guides the retrieval policy to\nretrieve candidates from KGs in proportion to their reward. This allows\nGraphFlow to explore high-quality regions of KGs that yield diverse and\nrelevant results. We evaluate GraphFlow on the STaRK benchmark, which includes\nreal-world queries from multiple domains over text-rich KGs. GraphFlow\noutperforms strong KG-RAG baselines, including GPT-4o, by 10% on average in hit\nrate and recall. It also shows strong generalization to unseen KGs,\ndemonstrating its effectiveness and robustness.", "AI": {"tldr": "GraphFlow\u662f\u4e00\u4e2a\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u6d41\u5339\u914d\u76ee\u6807\u4f18\u5316\u68c0\u7d22\u7b56\u7565\uff0c\u4ece\u6587\u672c\u4e30\u5bcc\u7684\u77e5\u8bc6\u56fe\u8c31\u4e2d\u9ad8\u6548\u68c0\u7d22\u51c6\u786e\u591a\u6837\u7684\u77e5\u8bc6\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u771f\u5b9e\u4e16\u754c\u67e5\u8be2\u65f6\uff0c\u96be\u4ee5\u4ece\u6587\u672c\u4e30\u5bcc\u7684\u77e5\u8bc6\u56fe\u8c31\u4e2d\u68c0\u7d22\u51c6\u786e\u591a\u6837\u7684\u4fe1\u606f\u3002\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u867d\u7136\u80fd\u5bf9\u9f50\u68c0\u7d22\u8fc7\u7a0b\u4e0e\u67e5\u8be2\u9700\u6c42\uff0c\u4f46\u4f9d\u8d56\u6602\u8d35\u4e14\u96be\u4ee5\u83b7\u53d6\u7684\u8fc7\u7a0b\u7ea7\u76d1\u7763\u4fe1\u53f7\u3002", "method": "GraphFlow\u91c7\u7528\u57fa\u4e8e\u8f6c\u79fb\u7684\u6d41\u5339\u914d\u76ee\u6807\u8054\u5408\u4f18\u5316\u68c0\u7d22\u7b56\u7565\u548c\u6d41\u4f30\u8ba1\u5668\u3002\u6d41\u4f30\u8ba1\u5668\u5c06\u68c0\u7d22\u7ed3\u679c\u7684\u5956\u52b1\u5206\u89e3\u5230\u4e2d\u95f4\u68c0\u7d22\u72b6\u6001\uff0c\u5f15\u5bfc\u68c0\u7d22\u7b56\u7565\u6309\u5956\u52b1\u6bd4\u4f8b\u4ece\u77e5\u8bc6\u56fe\u8c31\u4e2d\u68c0\u7d22\u5019\u9009\u7ed3\u679c\u3002", "result": "\u5728STaRK\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGraphFlow\u5728\u547d\u4e2d\u7387\u548c\u53ec\u56de\u7387\u4e0a\u5e73\u5747\u4f18\u4e8e\u5305\u62ecGPT-4o\u5728\u5185\u7684\u5f3a\u57fa\u7ebf\u65b9\u6cd510%\uff0c\u5e76\u5728\u672a\u89c1\u8fc7\u7684\u77e5\u8bc6\u56fe\u8c31\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "GraphFlow\u80fd\u591f\u6709\u6548\u4ece\u6587\u672c\u4e30\u5bcc\u7684\u77e5\u8bc6\u56fe\u8c31\u4e2d\u68c0\u7d22\u51c6\u786e\u591a\u6837\u7684\u77e5\u8bc6\uff0c\u4e3a\u590d\u6742\u771f\u5b9e\u4e16\u754c\u67e5\u8be2\u63d0\u4f9b\u652f\u6301\uff0c\u5e76\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.16020", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16020", "abs": "https://arxiv.org/abs/2510.16020", "authors": ["Sangjoon Lee", "Haris Moazam Sheikh"], "title": "Airfoil optimization using Design-by-Morphing with minimized design-space dimensionality", "comment": null, "summary": "Effective airfoil geometry optimization requires exploring a diverse range of\ndesigns using as few design variables as possible. This study introduces\nAirDbM, a Design-by-Morphing (DbM) approach specialized for airfoil\noptimization that systematically reduces design-space dimensionality. AirDbM\nselects an optimal set of 12 baseline airfoils from the UIUC airfoil database,\nwhich contains over 1,600 shapes, by sequentially adding the baseline that most\nincreases the design capacity. With these baselines, AirDbM reconstructs 99 \\%\nof the database with a mean absolute error below 0.005, which matches the\nperformance of a previous DbM approach that used more baselines. In\nmulti-objective aerodynamic optimization, AirDbM demonstrates rapid convergence\nand achieves a Pareto front with a greater hypervolume than that of the\nprevious larger-baseline study, where new Pareto-optimal solutions are\ndiscovered with enhanced lift-to-drag ratios at moderate stall tolerances.\nFurthermore, AirDbM demonstrates outstanding adaptability for reinforcement\nlearning (RL) agents in generating airfoil geometry when compared to\nconventional airfoil parameterization methods, implying the broader potential\nof DbM in machine learning-driven design.", "AI": {"tldr": "AirDbM\u662f\u4e00\u79cd\u4e13\u95e8\u7528\u4e8e\u7ffc\u578b\u4f18\u5316\u7684\u8bbe\u8ba1\u53d8\u5f62\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ece1600\u591a\u4e2a\u7ffc\u578b\u6570\u636e\u5e93\u4e2d\u9009\u62e912\u4e2a\u6700\u4f18\u57fa\u7ebf\u7ffc\u578b\uff0c\u663e\u8457\u964d\u4f4e\u8bbe\u8ba1\u7a7a\u95f4\u7ef4\u5ea6\uff0c\u5728\u4fdd\u6301\u9ad8\u91cd\u5efa\u7cbe\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u66f4\u5feb\u7684\u591a\u76ee\u6807\u4f18\u5316\u6536\u655b\u3002", "motivation": "\u7ffc\u578b\u51e0\u4f55\u4f18\u5316\u9700\u8981\u63a2\u7d22\u591a\u6837\u5316\u7684\u8bbe\u8ba1\uff0c\u540c\u65f6\u5c3d\u53ef\u80fd\u51cf\u5c11\u8bbe\u8ba1\u53d8\u91cf\u6570\u91cf\u3002\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u8f83\u591a\u57fa\u7ebf\u7ffc\u578b\uff0c\u800cAirDbM\u65e8\u5728\u7528\u66f4\u5c11\u7684\u57fa\u7ebf\u5b9e\u73b0\u66f4\u597d\u7684\u6027\u80fd\u3002", "method": "\u4eceUIUC\u7ffc\u578b\u6570\u636e\u5e93\u4e2d\u9009\u62e912\u4e2a\u6700\u4f18\u57fa\u7ebf\u7ffc\u578b\uff0c\u901a\u8fc7\u8bbe\u8ba1\u53d8\u5f62\u65b9\u6cd5\u91cd\u5efa\u6570\u636e\u5e93\uff0c\u5e76\u5728\u591a\u76ee\u6807\u6c14\u52a8\u4f18\u5316\u548c\u5f3a\u5316\u5b66\u4e60\u4e2d\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u752812\u4e2a\u57fa\u7ebf\u91cd\u5efa\u4e8699%\u7684\u6570\u636e\u5e93\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4f4e\u4e8e0.005\uff1b\u5728\u591a\u76ee\u6807\u4f18\u5316\u4e2d\u5b9e\u73b0\u4e86\u66f4\u5927\u7684\u8d85\u4f53\u79ef\u548c\u66f4\u5feb\u7684\u6536\u655b\uff1b\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u6bd4\u4f20\u7edf\u53c2\u6570\u5316\u65b9\u6cd5\u66f4\u597d\u7684\u9002\u5e94\u6027\u3002", "conclusion": "AirDbM\u901a\u8fc7\u51cf\u5c11\u8bbe\u8ba1\u53d8\u91cf\u6709\u6548\u63d0\u5347\u4e86\u7ffc\u578b\u4f18\u5316\u6548\u7387\uff0c\u5728\u673a\u5668\u5b66\u4e60\u548c\u4f18\u5316\u8bbe\u8ba1\u4e2d\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2510.16686", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16686", "abs": "https://arxiv.org/abs/2510.16686", "authors": ["Wenhang Shi", "Shuqing Bian", "Yiren Chen", "Xinyi Zhang", "Zhe Zhao", "Pengfei Hu", "Wei Lu", "Xiaoyong Du"], "title": "Investigating the Impact of Rationales for LLMs on Natural Language Understanding", "comment": null, "summary": "Chain-of-thought (CoT) rationales, which provide step-by-step reasoning to\nderive final answers, benefit LLMs in both inference and training.\nIncorporating rationales, either by generating them before answering during\ninference, or by placing them before or after the original answers during\ntraining - significantly improves model performance on mathematical, symbolic\nand commonsense reasoning tasks. However, most work focuses on the role of\nrationales in these reasoning tasks, overlooking their potential impact on\nother important tasks like natural language understanding (NLU) tasks. In this\nwork, we raise the question: Can rationales similarly benefit NLU tasks? To\nconduct a systematic exploration, we construct NLURC, a comprehensive and\nhigh-quality NLU dataset collection with rationales, and develop various\nrationale-augmented methods. Through exploring the applicability of these\nmethods on NLU tasks using the dataset, we uncover several potentially\nsurprising findings: (1) CoT inference shifts from hindering NLU performance to\nsurpassing direct label prediction as model size grows, indicating a positive\ncorrelation. (2) Most rationale-augmented training methods perform worse than\nlabel-only training, with one specially designed method consistently achieving\nimprovements. (3) LLMs trained with rationales achieve significant performance\ngains on unseen NLU tasks, rivaling models ten times their size, while\ndelivering interpretability on par with commercial LLMs.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u601d\u7ef4\u94fe\u63a8\u7406\u5728\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u6a21\u578b\u89c4\u6a21\u589e\u5927\u65f6CoT\u63a8\u7406\u4ece\u963b\u788d\u53d8\u4e3a\u8d85\u8d8a\u76f4\u63a5\u9884\u6d4b\uff0c\u5e76\u5f00\u53d1\u4e86\u80fd\u63d0\u5347\u6027\u80fd\u7684\u7279\u5b9a\u8bad\u7ec3\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u601d\u7ef4\u94fe\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u4f5c\u7528\uff0c\u800c\u5ffd\u89c6\u4e86\u5176\u5728\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u6f5c\u5728\u4ef7\u503c\u3002\u672c\u6587\u65e8\u5728\u7cfb\u7edf\u63a2\u7d22\u7406\u6027\u63a8\u7406\u662f\u5426\u540c\u6837\u80fd\u63d0\u5347NLU\u4efb\u52a1\u6027\u80fd\u3002", "method": "\u6784\u5efa\u4e86NLURC\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u4e86\u591a\u79cd\u7406\u6027\u589e\u5f3a\u65b9\u6cd5\uff0c\u5e76\u7cfb\u7edf\u8bc4\u4f30\u8fd9\u4e9b\u65b9\u6cd5\u5728NLU\u4efb\u52a1\u4e0a\u7684\u9002\u7528\u6027\u3002", "result": "\u53d1\u73b0\u6a21\u578b\u89c4\u6a21\u4e0eCoT\u6548\u679c\u6b63\u76f8\u5173\uff1b\u5927\u591a\u6570\u7406\u6027\u589e\u5f3a\u8bad\u7ec3\u65b9\u6cd5\u6548\u679c\u4e0d\u5982\u4ec5\u6807\u7b7e\u8bad\u7ec3\uff0c\u4f46\u6709\u4e00\u79cd\u4e13\u95e8\u8bbe\u8ba1\u7684\u65b9\u6cd5\u80fd\u6301\u7eed\u63d0\u5347\u6027\u80fd\uff1b\u4f7f\u7528\u7406\u6027\u8bad\u7ec3\u7684LLM\u5728\u672a\u89c1NLU\u4efb\u52a1\u4e0a\u8868\u73b0\u663e\u8457\u63d0\u5347\uff0c\u6027\u80fd\u53ef\u4e0e\u5927\u5341\u500d\u6a21\u578b\u5ab2\u7f8e\u3002", "conclusion": "\u7406\u6027\u63a8\u7406\u5728NLU\u4efb\u52a1\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5927\u89c4\u6a21\u6a21\u578b\u548c\u672a\u89c1\u4efb\u52a1\uff0c\u540c\u65f6\u80fd\u63d0\u4f9b\u4e0e\u5546\u4e1aLLM\u76f8\u5f53\u7684\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2510.17276", "categories": ["cs.LG", "cs.CR", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.17276", "abs": "https://arxiv.org/abs/2510.17276", "authors": ["Rishi Jha", "Harold Triedman", "Justin Wagle", "Vitaly Shmatikov"], "title": "Breaking and Fixing Defenses Against Control-Flow Hijacking in Multi-Agent Systems", "comment": null, "summary": "Control-flow hijacking attacks manipulate orchestration mechanisms in\nmulti-agent systems into performing unsafe actions that compromise the system\nand exfiltrate sensitive information. Recently proposed defenses, such as\nLlamaFirewall, rely on alignment checks of inter-agent communications to ensure\nthat all agent invocations are \"related to\" and \"likely to further\" the\noriginal objective.\n  We start by demonstrating control-flow hijacking attacks that evade these\ndefenses even if alignment checks are performed by advanced LLMs. We argue that\nthe safety and functionality objectives of multi-agent systems fundamentally\nconflict with each other. This conflict is exacerbated by the brittle\ndefinitions of \"alignment\" and the checkers' incomplete visibility into the\nexecution context.\n  We then propose, implement, and evaluate ControlValve, a new defense inspired\nby the principles of control-flow integrity and least privilege. ControlValve\n(1) generates permitted control-flow graphs for multi-agent systems, and (2)\nenforces that all executions comply with these graphs, along with contextual\nrules (generated in a zero-shot manner) for each agent invocation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faControlValve\u9632\u5fa1\u673a\u5236\uff0c\u901a\u8fc7\u751f\u6210\u5141\u8bb8\u7684\u63a7\u5236\u6d41\u56fe\u5e76\u5f3a\u5236\u6267\u884c\uff0c\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u63a7\u5236\u6d41\u52ab\u6301\u653b\u51fb\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5bf9\u9f50\u68c0\u67e5\u7684\u9632\u5fa1\u673a\u5236\uff08\u5982LlamaFirewall\uff09\u65e0\u6cd5\u6709\u6548\u62b5\u5fa1\u63a7\u5236\u6d41\u52ab\u6301\u653b\u51fb\uff0c\u56e0\u4e3a\u5b89\u5168\u6027\u548c\u529f\u80fd\u6027\u76ee\u6807\u5b58\u5728\u6839\u672c\u51b2\u7a81\uff0c\u4e14\u5bf9\u9f50\u5b9a\u4e49\u8106\u5f31\u3001\u68c0\u67e5\u5668\u5bf9\u6267\u884c\u4e0a\u4e0b\u6587\u53ef\u89c1\u6027\u4e0d\u5b8c\u6574\u3002", "method": "ControlValve\u57fa\u4e8e\u63a7\u5236\u6d41\u5b8c\u6574\u6027\u548c\u6700\u5c0f\u6743\u9650\u539f\u5219\uff0c\u751f\u6210\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u5141\u8bb8\u63a7\u5236\u6d41\u56fe\uff0c\u5e76\u5f3a\u5236\u6267\u884c\u6240\u6709\u6267\u884c\u7b26\u5408\u8fd9\u4e9b\u56fe\u4ee5\u53ca\u6bcf\u4e2a\u667a\u80fd\u4f53\u8c03\u7528\u7684\u4e0a\u4e0b\u6587\u89c4\u5219\uff08\u4ee5\u96f6\u6837\u672c\u65b9\u5f0f\u751f\u6210\uff09\u3002", "result": "ControlValve\u80fd\u591f\u6709\u6548\u9632\u5fa1\u89c4\u907f\u73b0\u6709\u5bf9\u9f50\u68c0\u67e5\u673a\u5236\u7684\u63a7\u5236\u6d41\u52ab\u6301\u653b\u51fb\u3002", "conclusion": "ControlValve\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u9632\u5fa1\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a7\u5236\u6d41\u5b8c\u6574\u6027\u539f\u5219\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u5b89\u5168\u6311\u6218\uff0c\u6bd4\u57fa\u4e8e\u5bf9\u9f50\u68c0\u67e5\u7684\u65b9\u6cd5\u66f4\u6709\u6548\u3002"}}
{"id": "2510.16511", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16511", "abs": "https://arxiv.org/abs/2510.16511", "authors": ["Dongchan Cho", "Jiho Han", "Keumyeong Kang", "Minsang Kim", "Honggyu Ryu", "Namsoon Jung"], "title": "Structured Temporal Causality for Interpretable Multivariate Time Series Anomaly Detection", "comment": "Accepted by NeurIPS 2025", "summary": "Real-world multivariate time series anomalies are rare and often unlabeled.\nAdditionally, prevailing methods rely on increasingly complex architectures\ntuned to benchmarks, detecting only fragments of anomalous segments and\noverstating performance. In this paper, we introduce OracleAD, a simple and\ninterpretable unsupervised framework for multivariate time series anomaly\ndetection. OracleAD encodes each variable's past sequence into a single causal\nembedding to jointly predict the present time point and reconstruct the input\nwindow, effectively modeling temporal dynamics. These embeddings then undergo a\nself-attention mechanism to project them into a shared latent space and capture\nspatial relationships. These relationships are not static, since they are\nmodeled by a property that emerges from each variable's temporal dynamics. The\nprojected embeddings are aligned to a Stable Latent Structure (SLS)\nrepresenting normal-state relationships. Anomalies are identified using a dual\nscoring mechanism based on prediction error and deviation from the SLS,\nenabling fine-grained anomaly diagnosis at each time point and across\nindividual variables. Since any noticeable SLS deviation originates from\nembeddings that violate the learned temporal causality of normal data, OracleAD\ndirectly pinpoints the root-cause variables at the embedding level. OracleAD\nachieves state-of-the-art results across multiple real-world datasets and\nevaluation protocols, while remaining interpretable through SLS.", "AI": {"tldr": "OracleAD\u662f\u4e00\u4e2a\u7b80\u5355\u53ef\u89e3\u91ca\u7684\u65e0\u76d1\u7763\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u56e0\u679c\u5d4c\u5165\u548c\u7a33\u5b9a\u6f5c\u5728\u7ed3\u6784\u6765\u68c0\u6d4b\u5f02\u5e38\u5e76\u5b9a\u4f4d\u6839\u56e0\u53d8\u91cf\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u7a00\u5c11\u4e14\u901a\u5e38\u65e0\u6807\u7b7e\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u590d\u6742\u67b6\u6784\u4e14\u53ea\u80fd\u68c0\u6d4b\u5f02\u5e38\u7247\u6bb5\uff0c\u6027\u80fd\u88ab\u5938\u5927\u3002", "method": "\u5c06\u6bcf\u4e2a\u53d8\u91cf\u7684\u8fc7\u53bb\u5e8f\u5217\u7f16\u7801\u4e3a\u56e0\u679c\u5d4c\u5165\u6765\u9884\u6d4b\u5f53\u524d\u65f6\u95f4\u70b9\u5e76\u91cd\u5efa\u8f93\u5165\u7a97\u53e3\uff0c\u4f7f\u7528\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6295\u5f71\u5230\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\uff0c\u5c06\u6295\u5f71\u5d4c\u5165\u5bf9\u9f50\u5230\u4ee3\u8868\u6b63\u5e38\u72b6\u6001\u5173\u7cfb\u7684\u7a33\u5b9a\u6f5c\u5728\u7ed3\u6784(SLS)\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u534f\u8bae\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "OracleAD\u4e0d\u4ec5\u68c0\u6d4b\u6027\u80fd\u4f18\u8d8a\uff0c\u8fd8\u901a\u8fc7SLS\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\uff0c\u80fd\u591f\u5728\u5d4c\u5165\u5c42\u9762\u76f4\u63a5\u5b9a\u4f4d\u6839\u56e0\u53d8\u91cf\u3002"}}
{"id": "2510.17383", "categories": ["cs.LG", "cs.CV", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.17383", "abs": "https://arxiv.org/abs/2510.17383", "authors": ["Ludovica Schaerf"], "title": "Latent Spaces Beyond Synthesis: From GANs to Diffusion Models", "comment": "Presented and published at Ethics and Aesthetics of Artificial\n  Intelligence Conference (EA-AI'25)", "summary": "This paper examines the evolving nature of internal representations in\ngenerative visual models, focusing on the conceptual and technical shift from\nGANs and VAEs to diffusion-based architectures. Drawing on Beatrice Fazi's\naccount of synthesis as the amalgamation of distributed representations, we\npropose a distinction between \"synthesis in a strict sense\", where a compact\nlatent space wholly determines the generative process, and \"synthesis in a\nbroad sense,\" which characterizes models whose representational labor is\ndistributed across layers. Through close readings of model architectures and a\ntargeted experimental setup that intervenes in layerwise representations, we\nshow how diffusion models fragment the burden of representation and thereby\nchallenge assumptions of unified internal space. By situating these findings\nwithin media theoretical frameworks and critically engaging with metaphors such\nas the latent space and the Platonic Representation Hypothesis, we argue for a\nreorientation of how generative AI is understood: not as a direct synthesis of\ncontent, but as an emergent configuration of specialized processes.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86\u751f\u6210\u89c6\u89c9\u6a21\u578b\u4e2d\u5185\u90e8\u8868\u5f81\u7684\u6f14\u53d8\uff0c\u533a\u5206\u4e86\u4e25\u683c\u610f\u4e49\u4e0a\u7684\u5408\u6210\uff08\u7d27\u51d1\u6f5c\u5728\u7a7a\u95f4\u51b3\u5b9a\u751f\u6210\u8fc7\u7a0b\uff09\u548c\u5e7f\u4e49\u5408\u6210\uff08\u8868\u5f81\u5de5\u4f5c\u5206\u5e03\u5728\u591a\u4e2a\u5c42\u4e2d\uff09\uff0c\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u6269\u6563\u6a21\u578b\u5982\u4f55\u5206\u6563\u8868\u5f81\u8d1f\u62c5\u5e76\u6311\u6218\u7edf\u4e00\u5185\u90e8\u7a7a\u95f4\u7684\u5047\u8bbe\u3002", "motivation": "\u7814\u7a76\u751f\u6210\u89c6\u89c9\u6a21\u578b\u5185\u90e8\u8868\u5f81\u7684\u6f14\u53d8\uff0c\u4eceGANs\u548cVAEs\u5230\u6269\u6563\u67b6\u6784\u7684\u6982\u5ff5\u548c\u6280\u672f\u8f6c\u53d8\uff0c\u63a2\u8ba8\u8868\u5f81\u5982\u4f55\u5728\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u4e2d\u5206\u5e03\u548c\u7ec4\u5408\u3002", "method": "\u901a\u8fc7\u6a21\u578b\u67b6\u6784\u7684\u8be6\u7ec6\u5206\u6790\u548c\u9488\u5bf9\u6027\u7684\u5b9e\u9a8c\u8bbe\u7f6e\uff0c\u5e72\u9884\u5c42\u95f4\u8868\u5f81\uff0c\u7814\u7a76\u6269\u6563\u6a21\u578b\u4e2d\u8868\u5f81\u7684\u5206\u5e03\u65b9\u5f0f\u3002", "result": "\u6269\u6563\u6a21\u578b\u5c06\u8868\u5f81\u8d1f\u62c5\u5206\u6563\u5230\u591a\u4e2a\u5c42\u4e2d\uff0c\u6311\u6218\u4e86\u7edf\u4e00\u5185\u90e8\u7a7a\u95f4\u7684\u5047\u8bbe\uff0c\u8868\u660e\u751f\u6210\u8fc7\u7a0b\u662f\u901a\u8fc7\u4e13\u95e8\u5316\u8fc7\u7a0b\u7684\u6d8c\u73b0\u914d\u7f6e\u5b9e\u73b0\u7684\u3002", "conclusion": "\u751f\u6210AI\u5e94\u88ab\u7406\u89e3\u4e3a\u4e13\u95e8\u5316\u8fc7\u7a0b\u7684\u6d8c\u73b0\u914d\u7f6e\uff0c\u800c\u975e\u5185\u5bb9\u7684\u76f4\u63a5\u5408\u6210\uff0c\u8fd9\u9700\u8981\u5bf9\u6f5c\u5728\u7a7a\u95f4\u548c\u67cf\u62c9\u56fe\u8868\u5f81\u5047\u8bbe\u7b49\u9690\u55bb\u8fdb\u884c\u91cd\u65b0\u5b9a\u4f4d\u3002"}}
{"id": "2510.16601", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16601", "abs": "https://arxiv.org/abs/2510.16601", "authors": ["Tianxing Wu", "Shutong Zhu", "Jingting Wang", "Ning Xu", "Guilin Qi", "Haofen Wang"], "title": "Uncertain Knowledge Graph Completion via Semi-Supervised Confidence Distribution Learning", "comment": "13 pages, accepted by NeurIPS 2025 (spotlight)", "summary": "Uncertain knowledge graphs (UKGs) associate each triple with a confidence\nscore to provide more precise knowledge representations. Recently, since\nreal-world UKGs suffer from the incompleteness, uncertain knowledge graph (UKG)\ncompletion attracts more attention, aiming to complete missing triples and\nconfidences. Current studies attempt to learn UKG embeddings to solve this\nproblem, but they neglect the extremely imbalanced distributions of triple\nconfidences. This causes that the learnt embeddings are insufficient to\nhigh-quality UKG completion. Thus, in this paper, to address the above issue,\nwe propose a new semi-supervised Confidence Distribution Learning (ssCDL)\nmethod for UKG completion, where each triple confidence is transformed into a\nconfidence distribution to introduce more supervision information of different\nconfidences to reinforce the embedding learning process. ssCDL iteratively\nlearns UKG embedding by relational learning on labeled data (i.e., existing\ntriples with confidences) and unlabeled data with pseudo labels (i.e., unseen\ntriples with the generated confidences), which are predicted by meta-learning\nto augment the training data and rebalance the distribution of triple\nconfidences. Experiments on two UKG datasets demonstrate that ssCDL\nconsistently outperforms state-of-the-art baselines in different evaluation\nmetrics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u76d1\u7763\u7f6e\u4fe1\u5206\u5e03\u5b66\u4e60\u65b9\u6cd5(ssCDL)\u6765\u89e3\u51b3\u4e0d\u786e\u5b9a\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u95ee\u9898\uff0c\u901a\u8fc7\u5c06\u7f6e\u4fe1\u5ea6\u8f6c\u6362\u4e3a\u5206\u5e03\u5e76\u5f15\u5165\u5143\u5b66\u4e60\u6765\u5e73\u8861\u7f6e\u4fe1\u5ea6\u5206\u5e03\u3002", "motivation": "\u73b0\u6709\u4e0d\u786e\u5b9a\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u65b9\u6cd5\u5ffd\u7565\u4e86\u7f6e\u4fe1\u5ea6\u7684\u6781\u7aef\u4e0d\u5e73\u8861\u5206\u5e03\uff0c\u5bfc\u81f4\u5b66\u4e60\u5230\u7684\u5d4c\u5165\u4e0d\u8db3\u4ee5\u652f\u6301\u9ad8\u8d28\u91cf\u7684\u8865\u5168\u3002", "method": "\u5c06\u7f6e\u4fe1\u5ea6\u8f6c\u6362\u4e3a\u7f6e\u4fe1\u5206\u5e03\uff0c\u901a\u8fc7\u5173\u7cfb\u5b66\u4e60\u5728\u6807\u8bb0\u6570\u636e\uff08\u73b0\u6709\u4e09\u5143\u7ec4\uff09\u548c\u5e26\u4f2a\u6807\u7b7e\u7684\u672a\u6807\u8bb0\u6570\u636e\uff08\u672a\u89c1\u4e09\u5143\u7ec4\uff09\u4e0a\u8fed\u4ee3\u5b66\u4e60\u5d4c\u5165\uff0c\u4f7f\u7528\u5143\u5b66\u4e60\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u6765\u589e\u5f3a\u8bad\u7ec3\u6570\u636e\u5e76\u5e73\u8861\u5206\u5e03\u3002", "result": "\u5728\u4e24\u4e2a\u4e0d\u786e\u5b9a\u77e5\u8bc6\u56fe\u8c31\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cssCDL\u5728\u4e0d\u540c\u8bc4\u4f30\u6307\u6807\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "ssCDL\u901a\u8fc7\u7f6e\u4fe1\u5206\u5e03\u5b66\u4e60\u548c\u534a\u76d1\u7763\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u7f6e\u4fe1\u5ea6\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u4e0d\u786e\u5b9a\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u7684\u6027\u80fd\u3002"}}
{"id": "2510.16719", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.16719", "abs": "https://arxiv.org/abs/2510.16719", "authors": ["Zak Ressler", "Marcus Grijalva", "Angelica Marie Ignacio", "Melanie Torres", "Abelardo Cuadra Rojas", "Rohollah Moghadam", "Mohammad Rasoul narimani"], "title": "LSTM-Based Forecasting and Analysis of EV Charging Demand in a Dense Urban Campus", "comment": null, "summary": "This paper presents a framework for processing EV charging load data in order\nto forecast future load predictions using a Recurrent Neural Network,\nspecifically an LSTM. The framework processes a large set of raw data from\nmultiple locations and transforms it with normalization and feature extraction\nto train the LSTM. The pre-processing stage corrects for missing or incomplete\nvalues by interpolating and normalizing the measurements. This information is\nthen fed into a Long Short-Term Memory Model designed to capture the short-term\nfluctuations while also interpreting the long-term trends in the charging data.\nExperimental results demonstrate the model's ability to accurately predict\ncharging demand across multiple time scales (daily, weekly, and monthly),\nproviding valuable insights for infrastructure planning, energy management, and\ngrid integration of EV charging facilities. The system's modular design allows\nfor adaptation to different charging locations with varying usage patterns,\nmaking it applicable across diverse deployment scenarios.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eLSTM\u7684\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u8d1f\u8377\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u6570\u636e\u9884\u5904\u7406\u548c\u7279\u5f81\u63d0\u53d6\u6765\u51c6\u786e\u9884\u6d4b\u591a\u65f6\u95f4\u5c3a\u5ea6\u7684\u5145\u7535\u9700\u6c42", "motivation": "\u4e3a\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u57fa\u7840\u8bbe\u65bd\u89c4\u5212\u3001\u80fd\u6e90\u7ba1\u7406\u548c\u7535\u7f51\u96c6\u6210\u63d0\u4f9b\u51c6\u786e\u7684\u8d1f\u8377\u9884\u6d4b\uff0c\u89e3\u51b3\u5145\u7535\u9700\u6c42\u6ce2\u52a8\u5e26\u6765\u7684\u6311\u6218", "method": "\u4f7f\u7528LSTM\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u6570\u636e\u9884\u5904\u7406\uff08\u63d2\u503c\u548c\u5f52\u4e00\u5316\uff09\u3001\u7279\u5f81\u63d0\u53d6\u6765\u8bad\u7ec3\u6a21\u578b\uff0c\u6355\u6349\u77ed\u671f\u6ce2\u52a8\u548c\u957f\u671f\u8d8b\u52bf", "result": "\u6a21\u578b\u80fd\u591f\u5728\u65e5\u3001\u5468\u3001\u6708\u7b49\u591a\u4e2a\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u51c6\u786e\u9884\u6d4b\u5145\u7535\u9700\u6c42\uff0c\u6a21\u5757\u5316\u8bbe\u8ba1\u53ef\u9002\u5e94\u4e0d\u540c\u5145\u7535\u7ad9\u70b9\u7684\u4f7f\u7528\u6a21\u5f0f", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u8bbe\u65bd\u7684\u89c4\u5212\u548c\u8fd0\u8425\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u9884\u6d4b\u5de5\u5177\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f"}}
{"id": "2510.16708", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16708", "abs": "https://arxiv.org/abs/2510.16708", "authors": ["Kailai Yang", "Yan Leng", "Xin Zhang", "Tianlin Zhang", "Paul Thompson", "Bernard Keavney", "Maciej Tomaszewski", "Sophia Ananiadou"], "title": "Natural Language Processing Applications in Cardiology: A Narrative Review", "comment": null, "summary": "Cardiovascular disease has become increasingly prevalent in modern society\nand has a significant effect on global health and well-being. Heart-related\nconditions are intricate, multifaceted disorders, which may be influenced by a\ncombination of genetic predispositions, lifestyle choices, and various\nsocioeconomic and clinical factors. Information regarding these potentially\ncomplex interrelationships is dispersed among diverse types of textual data,\nwhich include patient narratives, medical records, and scientific literature,\namong others. Natural language processing (NLP) techniques have increasingly\nbeen adopted as a powerful means to analyse and make sense of this vast amount\nof unstructured data. This, in turn, can allow healthcare professionals to gain\ndeeper insights into the cardiology field, which has the potential to\nrevolutionize current approaches to the diagnosis, treatment, and prevention of\ncardiac problems. This review provides a detailed overview of NLP research in\ncardiology between 2014 and 2025. We queried six literature databases to find\narticles describing the application of NLP techniques in the context of a range\nof different cardiovascular diseases. Following a rigorous screening process,\nwe identified a total of 265 relevant articles. We analysed each article from\nmultiple dimensions, i.e., NLP paradigm types, cardiology-related task types,\ncardiovascular disease types, and data source types. Our analysis reveals\nconsiderable diversity within each of these dimensions, thus demonstrating the\nconsiderable breadth of NLP research within the field. We also perform a\ntemporal analysis, which illustrates the evolution and changing trends in NLP\nmethods employed over the last decade that we cover. To our knowledge, the\nreview constitutes the most comprehensive overview of NLP research in\ncardiology to date.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u8bba\u6587\u7cfb\u7edf\u56de\u987e\u4e862014-2025\u5e74\u95f4\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5728\u5fc3\u810f\u75c5\u5b66\u9886\u57df\u7684\u7814\u7a76\u5e94\u7528\uff0c\u5206\u6790\u4e86265\u7bc7\u76f8\u5173\u6587\u7ae0\uff0c\u4eceNLP\u8303\u5f0f\u7c7b\u578b\u3001\u5fc3\u810f\u75c5\u5b66\u4efb\u52a1\u7c7b\u578b\u3001\u5fc3\u8840\u7ba1\u75be\u75c5\u7c7b\u578b\u548c\u6570\u636e\u6765\u6e90\u7c7b\u578b\u7b49\u591a\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u4e86\u5168\u9762\u5206\u6790\u3002", "motivation": "\u5fc3\u8840\u7ba1\u75be\u75c5\u65e5\u76ca\u666e\u904d\u4e14\u590d\u6742\uff0c\u76f8\u5173\u4fe1\u606f\u5206\u6563\u5728\u60a3\u8005\u53d9\u8ff0\u3001\u533b\u7597\u8bb0\u5f55\u548c\u79d1\u5b66\u6587\u732e\u7b49\u975e\u7ed3\u6784\u5316\u6587\u672c\u6570\u636e\u4e2d\u3002NLP\u6280\u672f\u80fd\u591f\u5206\u6790\u8fd9\u4e9b\u6d77\u91cf\u6570\u636e\uff0c\u5e2e\u52a9\u533b\u7597\u4e13\u4e1a\u4eba\u5458\u6df1\u5165\u4e86\u89e3\u5fc3\u810f\u75c5\u5b66\u9886\u57df\uff0c\u4ece\u800c\u9769\u65b0\u5fc3\u810f\u75c5\u8bca\u65ad\u3001\u6cbb\u7597\u548c\u9884\u9632\u65b9\u6cd5\u3002", "method": "\u67e5\u8be2\u4e86\u516d\u4e2a\u6587\u732e\u6570\u636e\u5e93\uff0c\u901a\u8fc7\u4e25\u683c\u7b5b\u9009\u6d41\u7a0b\u8bc6\u522b\u51fa265\u7bc7\u76f8\u5173\u6587\u7ae0\uff0c\u4eceNLP\u8303\u5f0f\u7c7b\u578b\u3001\u5fc3\u810f\u75c5\u5b66\u4efb\u52a1\u7c7b\u578b\u3001\u5fc3\u8840\u7ba1\u75be\u75c5\u7c7b\u578b\u548c\u6570\u636e\u6765\u6e90\u7c7b\u578b\u7b49\u591a\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u5206\u6790\uff0c\u5e76\u8fdb\u884c\u65f6\u95f4\u8d8b\u52bf\u5206\u6790\u3002", "result": "\u5206\u6790\u663e\u793a\u8fd9\u4e9b\u7ef4\u5ea6\u5185\u5b58\u5728\u663e\u8457\u591a\u6837\u6027\uff0c\u8bc1\u660e\u4e86NLP\u5728\u5fc3\u810f\u75c5\u5b66\u9886\u57df\u7814\u7a76\u7684\u5e7f\u5ea6\u3002\u65f6\u95f4\u5206\u6790\u63ed\u793a\u4e86\u8fd1\u5341\u5e74\u6765NLP\u65b9\u6cd5\u7684\u6f14\u53d8\u548c\u53d8\u5316\u8d8b\u52bf\u3002", "conclusion": "\u8fd9\u662f\u8fc4\u4eca\u4e3a\u6b62\u5bf9\u5fc3\u810f\u75c5\u5b66\u9886\u57dfNLP\u7814\u7a76\u6700\u5168\u9762\u7684\u7efc\u8ff0\uff0c\u5c55\u793a\u4e86NLP\u6280\u672f\u5728\u5fc3\u8840\u7ba1\u75be\u75c5\u7814\u7a76\u4e2d\u7684\u91cd\u8981\u5e94\u7528\u4ef7\u503c\u548c\u5e7f\u9614\u524d\u666f\u3002"}}
{"id": "2510.16513", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16513", "abs": "https://arxiv.org/abs/2510.16513", "authors": ["Dhruv Gupta", "Aditya Nagarsekar", "Vraj Shah", "Sujith Thomas"], "title": "eDCF: Estimating Intrinsic Dimension using Local Connectivity", "comment": "58 pages (35 (main) + 23 (appendix)), 54 figures (27 (main) + 27\n  (appendix))", "summary": "Modern datasets often contain high-dimensional features exhibiting complex\ndependencies. To effectively analyze such data, dimensionality reduction\nmethods rely on estimating the dataset's intrinsic dimension (id) as a measure\nof its underlying complexity. However, estimating id is challenging due to its\ndependence on scale: at very fine scales, noise inflates id estimates, while at\ncoarser scales, estimates stabilize to lower, scale-invariant values. This\npaper introduces a novel, scalable, and parallelizable method called eDCF,\nwhich is based on Connectivity Factor (CF), a local connectivity-based metric,\nto robustly estimate intrinsic dimension across varying scales. Our method\nconsistently matches leading estimators, achieving comparable values of mean\nabsolute error (MAE) on synthetic benchmarks with noisy samples. Moreover, our\napproach also attains higher exact intrinsic dimension match rates, reaching up\nto 25.0% compared to 16.7% for MLE and 12.5% for TWO-NN, particularly excelling\nunder medium to high noise levels and large datasets. Further, we showcase our\nmethod's ability to accurately detect fractal geometries in decision\nboundaries, confirming its utility for analyzing realistic, structured data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fde\u901a\u6027\u56e0\u5b50(CF)\u7684eDCF\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u4e0d\u540c\u5c3a\u5ea6\u4e0b\u7a33\u5065\u4f30\u8ba1\u9ad8\u7ef4\u6570\u636e\u7684\u672c\u5f81\u7ef4\u5ea6\uff0c\u5728\u566a\u58f0\u73af\u5883\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u80fd\u51c6\u786e\u68c0\u6d4b\u5206\u5f62\u51e0\u4f55\u7ed3\u6784\u3002", "motivation": "\u73b0\u4ee3\u6570\u636e\u96c6\u901a\u5e38\u5305\u542b\u5177\u6709\u590d\u6742\u4f9d\u8d56\u5173\u7cfb\u7684\u9ad8\u7ef4\u7279\u5f81\uff0c\u800c\u672c\u5f81\u7ef4\u5ea6\u4f30\u8ba1\u9762\u4e34\u5c3a\u5ea6\u4f9d\u8d56\u7684\u6311\u6218\uff1a\u7cbe\u7ec6\u5c3a\u5ea6\u4e0b\u566a\u58f0\u4f1a\u81a8\u80c0\u4f30\u8ba1\u503c\uff0c\u7c97\u5c3a\u5ea6\u4e0b\u4f30\u8ba1\u503c\u4f1a\u7a33\u5b9a\u5230\u8f83\u4f4e\u7684\u5c3a\u5ea6\u4e0d\u53d8\u503c\u3002", "method": "\u57fa\u4e8e\u8fde\u901a\u6027\u56e0\u5b50(CF)\u8fd9\u4e00\u5c40\u90e8\u8fde\u901a\u6027\u5ea6\u91cf\uff0c\u5f00\u53d1\u4e86\u53ef\u6269\u5c55\u4e14\u53ef\u5e76\u884c\u5316\u7684eDCF\u65b9\u6cd5\uff0c\u80fd\u591f\u8de8\u4e0d\u540c\u5c3a\u5ea6\u7a33\u5065\u4f30\u8ba1\u672c\u5f81\u7ef4\u5ea6\u3002", "result": "\u5728\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e0e\u9886\u5148\u4f30\u8ba1\u5668\u8868\u73b0\u76f8\u5f53\uff0cMAE\u503c\u76f8\u8fd1\uff1b\u5728\u4e2d\u7b49\u81f3\u9ad8\u566a\u58f0\u6c34\u5e73\u548c\u5927\u6570\u636e\u96c6\u4e0b\uff0c\u7cbe\u786e\u672c\u5f81\u7ef4\u5ea6\u5339\u914d\u7387\u9ad8\u8fbe25.0%\uff0c\u4f18\u4e8eMLE\u768416.7%\u548cTWO-NN\u768412.5%\u3002", "conclusion": "eDCF\u65b9\u6cd5\u80fd\u591f\u51c6\u786e\u68c0\u6d4b\u51b3\u7b56\u8fb9\u754c\u4e2d\u7684\u5206\u5f62\u51e0\u4f55\u7ed3\u6784\uff0c\u8bc1\u5b9e\u4e86\u5176\u5728\u5206\u6790\u73b0\u5b9e\u7ed3\u6784\u5316\u6570\u636e\u65b9\u9762\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2510.17516", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17516", "abs": "https://arxiv.org/abs/2510.17516", "authors": ["Tiancheng Hu", "Joachim Baumann", "Lorenzo Lupo", "Dirk Hovy", "Nigel Collier", "Paul R\u00f6ttger"], "title": "SimBench: Benchmarking the Ability of Large Language Models to Simulate Human Behaviors", "comment": "Project Website: http://simbench.tiancheng.hu/ Data:\n  https://huggingface.co/datasets/pitehu/SimBench", "summary": "Large language model (LLM) simulations of human behavior have the potential\nto revolutionize the social and behavioral sciences, if and only if they\nfaithfully reflect real human behaviors. Current evaluations are fragmented,\nbased on bespoke tasks and metrics, creating a patchwork of incomparable\nresults. To address this, we introduce SimBench, the first large-scale,\nstandardized benchmark for a robust, reproducible science of LLM simulation. By\nunifying 20 diverse datasets covering tasks from moral decision-making to\neconomic choice across a large global participant pool, SimBench provides the\nnecessary foundation to ask fundamental questions about when, how, and why LLM\nsimulations succeed or fail. We show that, while even the best LLMs today have\nlimited simulation ability (score: 40.80/100), performance scales log-linearly\nwith model size. Simulation performance is not improved by increased\ninference-time compute. We demonstrate an alignment-simulation trade-off:\ninstruction-tuning improves performance on low-entropy (consensus) questions\nbut degrades it on high-entropy (diverse) ones. Models particularly struggle\nwhen simulating specific demographic groups. Finally, we demonstrate that\nsimulation ability correlates most strongly with deep, knowledge-intensive\nreasoning (MMLU-Pro, r=0.939). By making progress measurable, we aim to\naccelerate the development of more faithful LLM simulators.", "AI": {"tldr": "SimBench\u662f\u7b2c\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u6807\u51c6\u5316\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u6a21\u62df\u4eba\u7c7b\u884c\u4e3a\u7684\u80fd\u529b\uff0c\u6db5\u76d620\u4e2a\u591a\u6837\u5316\u6570\u636e\u96c6\uff0c\u53d1\u73b0\u5f53\u524d\u6700\u4f73LLM\u6a21\u62df\u80fd\u529b\u6709\u9650\uff0840.80/100\uff09\uff0c\u6027\u80fd\u968f\u6a21\u578b\u89c4\u6a21\u5bf9\u6570\u7ebf\u6027\u589e\u957f\uff0c\u5b58\u5728\u5bf9\u9f50-\u6a21\u62df\u6743\u8861\uff0c\u4e14\u5728\u6a21\u62df\u7279\u5b9a\u4eba\u53e3\u7fa4\u4f53\u65f6\u8868\u73b0\u8f83\u5dee\u3002", "motivation": "\u5f53\u524dLLM\u6a21\u62df\u4eba\u7c7b\u884c\u4e3a\u7684\u8bc4\u4f30\u65b9\u6cd5\u96f6\u6563\u4e14\u4e0d\u53ef\u6bd4\u8f83\uff0c\u9700\u8981\u7edf\u4e00\u7684\u57fa\u51c6\u6d4b\u8bd5\u6765\u63a8\u52a8LLM\u6a21\u62df\u80fd\u529b\u7684\u79d1\u5b66\u53d1\u5c55\u3002", "method": "\u6574\u540820\u4e2a\u591a\u6837\u5316\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u4ece\u9053\u5fb7\u51b3\u7b56\u5230\u7ecf\u6d4e\u9009\u62e9\u7b49\u4efb\u52a1\uff0c\u5efa\u7acb\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u5206\u6790\u6a21\u578b\u89c4\u6a21\u3001\u63a8\u7406\u8ba1\u7b97\u3001\u6307\u4ee4\u5fae\u8c03\u7b49\u56e0\u7d20\u5bf9\u6a21\u62df\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5f53\u524d\u6700\u4f73LLM\u6a21\u62df\u5f97\u5206\u4ec540.80/100\uff1b\u6027\u80fd\u968f\u6a21\u578b\u89c4\u6a21\u5bf9\u6570\u7ebf\u6027\u589e\u957f\uff1b\u63a8\u7406\u8ba1\u7b97\u589e\u52a0\u4e0d\u63d0\u5347\u6027\u80fd\uff1b\u6307\u4ee4\u5fae\u8c03\u5728\u4f4e\u71b5\u95ee\u9898\u4e0a\u6539\u5584\u4f46\u5728\u9ad8\u71b5\u95ee\u9898\u4e0a\u6076\u5316\uff1b\u6a21\u578b\u5728\u6a21\u62df\u7279\u5b9a\u4eba\u53e3\u7fa4\u4f53\u65f6\u8868\u73b0\u4e0d\u4f73\uff1b\u6a21\u62df\u80fd\u529b\u4e0e\u6df1\u5ea6\u77e5\u8bc6\u63a8\u7406\u80fd\u529b\u5f3a\u76f8\u5173\uff08MMLU-Pro, r=0.939\uff09\u3002", "conclusion": "SimBench\u4e3aLLM\u6a21\u62df\u80fd\u529b\u7684\u53ef\u6d4b\u91cf\u8fdb\u6b65\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5bf9\u9f50-\u6a21\u62df\u6743\u8861\u548c\u4eba\u53e3\u7fa4\u4f53\u6a21\u62df\u7684\u6311\u6218\uff0c\u5c06\u52a0\u901f\u66f4\u5fe0\u5b9eLLM\u6a21\u62df\u5668\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.16614", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16614", "abs": "https://arxiv.org/abs/2510.16614", "authors": ["Xuan Zhang", "Ruixiao Li", "Zhijian Zhou", "Long Li", "Yulei Qin", "Ke Li", "Xing Sun", "Xiaoyu Tan", "Chao Qu", "Yuan Qi"], "title": "Count Counts: Motivating Exploration in LLM Reasoning with Count-based Intrinsic Rewards", "comment": null, "summary": "Reinforcement Learning (RL) has become a compelling way to strengthen the\nmulti step reasoning ability of Large Language Models (LLMs). However,\nprevalent RL paradigms still lean on sparse outcome-based rewards and limited\nexploration, which often drives LLMs toward repetitive and suboptimal reasoning\npatterns. In this paper, we study the central question of how to design\nexploration for LLM reasoning and introduce MERCI (Motivating Exploration in\nLLM Reasoning with Count-based Intrinsic Rewards), a novel RL algorithm that\naugments policy optimization with a principled intrinsic reward. Building on\nthe idea of count-based exploration, MERCI leverages a lightweight Coin\nFlipping Network (CFN) to estimate the pseudo count and further epistemic\nuncertainty over reasoning trajectories, and converts them into an intrinsic\nreward that values novelty while preserving the learning signal from task\nrewards. We integrate MERCI into some advanced RL frameworks like Group\nRelative Policy Optimization (GRPO). Experiments on complex reasoning\nbenchmarks demonstrate that MERCI encourages richer and more varied chains of\nthought, significantly improves performance over strong baselines, and helps\nthe policy escape local routines to discover better solutions. It indicates\nthat our targeted intrinsic motivation can make exploration reliable for\nlanguage model reasoning.", "AI": {"tldr": "MERCI\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u8ba1\u6570\u7684\u5185\u5728\u5956\u52b1\u6765\u589e\u5f3aLLM\u63a8\u7406\u4e2d\u7684\u63a2\u7d22\uff0c\u907f\u514d\u91cd\u590d\u548c\u6b21\u4f18\u7684\u63a8\u7406\u6a21\u5f0f\u3002", "motivation": "\u73b0\u6709\u7684RL\u8303\u5f0f\u4f9d\u8d56\u7a00\u758f\u7684\u7ed3\u679c\u5956\u52b1\u548c\u6709\u9650\u63a2\u7d22\uff0c\u5bfc\u81f4LLM\u9677\u5165\u91cd\u590d\u548c\u6b21\u4f18\u7684\u63a8\u7406\u6a21\u5f0f\uff0c\u9700\u8981\u8bbe\u8ba1\u66f4\u597d\u7684\u63a2\u7d22\u673a\u5236\u3002", "method": "\u4f7f\u7528\u8f7b\u91cf\u7ea7\u7684Coin Flipping Network\u4f30\u8ba1\u63a8\u7406\u8f68\u8ff9\u7684\u4f2a\u8ba1\u6570\u548c\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u5c06\u5176\u8f6c\u6362\u4e3a\u5185\u5728\u5956\u52b1\uff0c\u5e76\u96c6\u6210\u5230GRPO\u7b49RL\u6846\u67b6\u4e2d\u3002", "result": "\u5728\u590d\u6742\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMERCI\u9f13\u52b1\u66f4\u4e30\u5bcc\u591a\u6837\u7684\u601d\u7ef4\u94fe\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u5e2e\u52a9\u7b56\u7565\u9003\u79bb\u5c40\u90e8\u6700\u4f18\u53d1\u73b0\u66f4\u597d\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u9488\u5bf9\u6027\u7684\u5185\u5728\u52a8\u673a\u53ef\u4ee5\u4f7f\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4e2d\u7684\u63a2\u7d22\u66f4\u52a0\u53ef\u9760\u6709\u6548\u3002"}}
{"id": "2510.16981", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.16981", "abs": "https://arxiv.org/abs/2510.16981", "authors": ["Ahmed Khaled", "Kaan Ozkara", "Tao Yu", "Mingyi Hong", "Youngsuk Park"], "title": "MuonBP: Faster Muon via Block-Periodic Orthogonalization", "comment": null, "summary": "Gradient orthogonalization is a simple strategy that shows great utility in\nspeeding up gradient descent. The Muon optimizer (Jordan, Jin, et al., 2024)\ncombines gradient orthogonalization with first-order momentum and achieves\nsignificant improvement in data efficiency over Adam/AdamW (Loshchilov and\nHutter, 2019) for language model training. However, when using model\nparallelism, gradient orthogonalization introduces additional overhead compared\nto coordinate-wise optimizers (such as AdamW) due to additional gather and\nscatter operations on gradient matrix shards from different devices. This\nadditional communication can amount to a throughput hit of 5%-10% compared to\nAdam/AdamW. To remedy this, we propose Muon with Block-Periodic\nOrthogonalization (MuonBP), which applies orthogonalization independently to\nmatrix shards on each device and periodically performs full orthogonalization\nto maintain training stability at scale. We show how to adjust the learning\nrate from the baseline to MuonBP and give convergence guarantees for this\nalgorithm. Crucially, our theory dictates that we use two stepsizes: one for\nthe blockwise orthogonalization steps, and one for the full orthogonalization\nsteps. Our method is simple, requires minimal hyperparameter adjustments, and\nachieves competitive iteration complexity compared with baseline Muon while\nproviding per-iteration throughput comparable to coordinate-wise methods such\nas AdamW. When training an 8B model with eight-way tensor parallelism and ZeRO\noptimizer state sharding, MuonBP achieves 8% throughput increase compared to\nMuon with no degradation in performance.", "AI": {"tldr": "MuonBP\u901a\u8fc7\u5757\u5468\u671f\u6b63\u4ea4\u5316\u4f18\u5316\u68af\u5ea6\u6b63\u4ea4\u5316\u65b9\u6cd5\uff0c\u5728\u6a21\u578b\u5e76\u884c\u8bad\u7ec3\u4e2d\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\uff0c\u5b9e\u73b0\u4e0eAdamW\u76f8\u5f53\u7684\u541e\u5410\u91cf\u540c\u65f6\u4fdd\u6301\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "motivation": "\u89e3\u51b3Muon\u4f18\u5316\u5668\u5728\u6a21\u578b\u5e76\u884c\u8bad\u7ec3\u4e2d\u56e0\u68af\u5ea6\u6b63\u4ea4\u5316\u5f15\u5165\u7684\u989d\u5916\u901a\u4fe1\u5f00\u9500\u95ee\u9898\uff0c\u8be5\u5f00\u9500\u53ef\u8fbe5%-10%\u541e\u5410\u91cf\u635f\u5931\u3002", "method": "\u63d0\u51fa\u5757\u5468\u671f\u6b63\u4ea4\u5316\u65b9\u6cd5\uff1a\u5728\u5404\u8bbe\u5907\u4e0a\u72ec\u7acb\u5bf9\u77e9\u9635\u5206\u7247\u8fdb\u884c\u6b63\u4ea4\u5316\uff0c\u5e76\u5468\u671f\u6027\u6267\u884c\u5b8c\u6574\u6b63\u4ea4\u5316\u4ee5\u4fdd\u6301\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002\u4f7f\u7528\u4e24\u4e2a\u5b66\u4e60\u7387\u5206\u522b\u5bf9\u5e94\u5757\u6b63\u4ea4\u5316\u548c\u5b8c\u6574\u6b63\u4ea4\u5316\u6b65\u9aa4\u3002", "result": "\u57288B\u6a21\u578b\u8bad\u7ec3\u4e2d\uff0c\u4f7f\u75288\u8def\u5f20\u91cf\u5e76\u884c\u548cZeRO\u4f18\u5316\u5668\u72b6\u6001\u5206\u7247\uff0cMuonBP\u76f8\u6bd4Muon\u5b9e\u73b08%\u541e\u5410\u91cf\u63d0\u5347\u4e14\u6027\u80fd\u65e0\u4e0b\u964d\u3002", "conclusion": "MuonBP\u5728\u4fdd\u6301Muon\u4f18\u5316\u5668\u6570\u636e\u6548\u7387\u4f18\u52bf\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6a21\u578b\u5e76\u884c\u8bad\u7ec3\u4e2d\u7684\u901a\u4fe1\u5f00\u9500\uff0c\u5b9e\u73b0\u4e86\u541e\u5410\u91cf\u4e0e\u8bad\u7ec3\u7a33\u5b9a\u6027\u7684\u5e73\u8861\u3002"}}
{"id": "2510.16022", "categories": ["cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.16022", "abs": "https://arxiv.org/abs/2510.16022", "authors": ["Changsheng Wang", "Xin Chen", "Sijia Liu", "Ke Ding"], "title": "Breaking Memorization Barriers in LLM Code Fine-Tuning via Information Bottleneck for Improved Generalization", "comment": null, "summary": "Adapting pretrained large language models (LLMs) to code domains via\nsupervised fine-tuning (FT) has been commonly used for code generation.\nHowever, we identify a previously underappreciated failure mode, the\nmemorization barrier, where strong memorization of downstream code data in the\nbase model could trap optimization and prevent the standard FT from effectively\nacquiring new, generalizable code knowledge. To overcome this barrier, we\npropose the information bottleneck (IB)-guided fine-tuning, termed IB-FT, which\napplies an IB penalty on hidden representations of the code data to compress\nspurious, memorized features while preserving task-relevant information.\nExtensive experiments on two code benchmarks (OriGen and Evol-CodeAlpaca-V1)\nshow that IB-FT substantially alleviates the memorization barrier, improves\ntop-1 performance (Pass@$1$), and yields far more stable gains under the\nstricter multi-sample metric Pass@$k^{(m)}$ (a problem counts as solved only if\nat least $m$ of $k$ samples pass unit tests) compared with conventional FT.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faIB-FT\u65b9\u6cd5\uff0c\u901a\u8fc7\u4fe1\u606f\u74f6\u9888\u6307\u5bfc\u5fae\u8c03\uff0c\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u8bb0\u5fc6\u969c\u788d\u95ee\u9898\uff0c\u63d0\u5347\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u53d1\u73b0\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u5b58\u5728\u8bb0\u5fc6\u969c\u788d\u95ee\u9898\uff0c\u5373\u6a21\u578b\u8fc7\u5ea6\u8bb0\u5fc6\u4e0b\u6e38\u4ee3\u7801\u6570\u636e\uff0c\u963b\u788d\u5176\u5b66\u4e60\u65b0\u7684\u53ef\u6cdb\u5316\u4ee3\u7801\u77e5\u8bc6\u3002", "method": "\u63d0\u51faIB-FT\u65b9\u6cd5\uff0c\u5728\u4ee3\u7801\u6570\u636e\u7684\u9690\u85cf\u8868\u793a\u4e0a\u5e94\u7528\u4fe1\u606f\u74f6\u9888\u60e9\u7f5a\uff0c\u538b\u7f29\u865a\u5047\u8bb0\u5fc6\u7279\u5f81\uff0c\u540c\u65f6\u4fdd\u7559\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\u3002", "result": "\u5728\u4e24\u4e2a\u4ee3\u7801\u57fa\u51c6\u6d4b\u8bd5(OriGen\u548cEvol-CodeAlpaca-V1)\u4e0a\uff0cIB-FT\u663e\u8457\u7f13\u89e3\u8bb0\u5fc6\u969c\u788d\uff0c\u63d0\u9ad8top-1\u6027\u80fd\uff0c\u5e76\u5728\u66f4\u4e25\u683c\u7684\u591a\u6837\u672c\u6307\u6807\u4e0b\u83b7\u5f97\u66f4\u7a33\u5b9a\u7684\u589e\u76ca\u3002", "conclusion": "IB-FT\u65b9\u6cd5\u6709\u6548\u514b\u670d\u4e86\u4f20\u7edf\u5fae\u8c03\u4e2d\u7684\u8bb0\u5fc6\u969c\u788d\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u4ee3\u7801\u751f\u6210\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u7a33\u5b9a\u6027\u3002"}}
{"id": "2510.16712", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16712", "abs": "https://arxiv.org/abs/2510.16712", "authors": ["Shivam Ratnakar", "Sanjay Raghavendra"], "title": "The Chameleon Nature of LLMs: Quantifying Multi-Turn Stance Instability in Search-Enabled Language Models", "comment": null, "summary": "Integration of Large Language Models with search/retrieval engines has become\nubiquitous, yet these systems harbor a critical vulnerability that undermines\ntheir reliability. We present the first systematic investigation of \"chameleon\nbehavior\" in LLMs: their alarming tendency to shift stances when presented with\ncontradictory questions in multi-turn conversations (especially in\nsearch-enabled LLMs). Through our novel Chameleon Benchmark Dataset, comprising\n17,770 carefully crafted question-answer pairs across 1,180 multi-turn\nconversations spanning 12 controversial domains, we expose fundamental flaws in\nstate-of-the-art systems. We introduce two theoretically grounded metrics: the\nChameleon Score (0-1) that quantifies stance instability, and Source Re-use\nRate (0-1) that measures knowledge diversity. Our rigorous evaluation of\nLlama-4-Maverick, GPT-4o-mini, and Gemini-2.5-Flash reveals consistent\nfailures: all models exhibit severe chameleon behavior (scores 0.391-0.511),\nwith GPT-4o-mini showing the worst performance. Crucially, small\nacross-temperature variance (less than 0.004) suggests the effect is not a\nsampling artifact. Our analysis uncovers the mechanism: strong correlations\nbetween source re-use rate and confidence (r=0.627) and stance changes\n(r=0.429) are statistically significant (p less than 0.05), indicating that\nlimited knowledge diversity makes models pathologically deferential to query\nframing. These findings highlight the need for comprehensive consistency\nevaluation before deploying LLMs in healthcare, legal, and financial systems\nwhere maintaining coherent positions across interactions is critical for\nreliable decision support.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLMs\u5728\u68c0\u7d22\u589e\u5f3a\u7cfb\u7edf\u4e2d\u5b58\u5728\u53d8\u8272\u9f99\u884c\u4e3a\uff1a\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u9762\u5bf9\u77db\u76fe\u95ee\u9898\u65f6\u5bb9\u6613\u6539\u53d8\u7acb\u573a\uff0c\u8fd9\u4e25\u91cd\u5f71\u54cd\u4e86\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u96c6\u6210\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u68c0\u7d22\u7cfb\u7edf\u7684\u5e94\u7528\u65e5\u76ca\u666e\u53ca\uff0c\u4f46\u8fd9\u4e9b\u7cfb\u7edf\u5b58\u5728\u5173\u952e\u6f0f\u6d1e\uff0c\u53ef\u80fd\u5f71\u54cd\u5176\u53ef\u9760\u6027\u3002\u7814\u7a76\u8005\u5e0c\u671b\u7cfb\u7edf\u6027\u5730\u8c03\u67e5LLMs\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u9762\u5bf9\u77db\u76fe\u95ee\u9898\u65f6\u7684\u7acb\u573a\u4e0d\u7a33\u5b9a\u6027\u3002", "method": "\u521b\u5efa\u4e86\u5305\u542b17,770\u4e2a\u95ee\u7b54\u5bf9\u7684Chameleon\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u6db5\u76d61,180\u4e2a\u591a\u8f6e\u5bf9\u8bdd\u548c12\u4e2a\u4e89\u8bae\u9886\u57df\u3002\u63d0\u51fa\u4e86\u4e24\u4e2a\u7406\u8bba\u6307\u6807\uff1a\u53d8\u8272\u9f99\u5206\u6570\uff08\u91cf\u5316\u7acb\u573a\u4e0d\u7a33\u5b9a\u6027\uff09\u548c\u6e90\u91cd\u7528\u7387\uff08\u8861\u91cf\u77e5\u8bc6\u591a\u6837\u6027\uff09\u3002", "result": "\u5bf9Llama-4-Maverick\u3001GPT-4o-mini\u548cGemini-2.5-Flash\u7684\u8bc4\u4f30\u663e\u793a\u6240\u6709\u6a21\u578b\u90fd\u8868\u73b0\u51fa\u4e25\u91cd\u7684\u53d8\u8272\u9f99\u884c\u4e3a\uff08\u5206\u65700.391-0.511\uff09\uff0c\u5176\u4e2dGPT-4o-mini\u8868\u73b0\u6700\u5dee\u3002\u6e90\u91cd\u7528\u7387\u4e0e\u7f6e\u4fe1\u5ea6\uff08r=0.627\uff09\u548c\u7acb\u573a\u53d8\u5316\uff08r=0.429\uff09\u5b58\u5728\u663e\u8457\u76f8\u5173\u6027\u3002", "conclusion": "\u6709\u9650\u7684\u77e5\u8bc6\u591a\u6837\u6027\u4f7f\u5f97\u6a21\u578b\u75c5\u6001\u5730\u987a\u4ece\u67e5\u8be2\u6846\u67b6\uff0c\u8fd9\u51f8\u663e\u4e86\u5728\u533b\u7597\u3001\u6cd5\u5f8b\u548c\u91d1\u878d\u7b49\u9700\u8981\u4fdd\u6301\u8fde\u8d2f\u7acb\u573a\u7684\u7cfb\u7edf\u4e2d\u90e8\u7f72LLMs\u524d\u8fdb\u884c\u4e00\u81f4\u6027\u8bc4\u4f30\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2510.16530", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16530", "abs": "https://arxiv.org/abs/2510.16530", "authors": ["Ashutosh Srivastava", "Lokesh Nagalapatti", "Gautam Jajoo", "Aniket Vashishtha", "Parameswari Krishnamurthy", "Amit Sharma"], "title": "Realizing LLMs' Causal Potential Requires Science-Grounded, Novel Benchmarks", "comment": null, "summary": "Recent claims of strong performance by Large Language Models (LLMs) on causal\ndiscovery are undermined by a key flaw: many evaluations rely on benchmarks\nlikely included in pretraining corpora. Thus, apparent success suggests that\nLLM-only methods, which ignore observational data, outperform classical\nstatistical approaches. We challenge this narrative by asking: Do LLMs truly\nreason about causal structure, and how can we measure it without memorization\nconcerns? Can they be trusted for real-world scientific discovery? We argue\nthat realizing LLMs' potential for causal analysis requires two shifts: (P.1)\ndeveloping robust evaluation protocols based on recent scientific studies to\nguard against dataset leakage, and (P.2) designing hybrid methods that combine\nLLM-derived knowledge with data-driven statistics. To address P.1, we encourage\nevaluating discovery methods on novel, real-world scientific studies. We\noutline a practical recipe for extracting causal graphs from recent\npublications released after an LLM's training cutoff, ensuring relevance and\npreventing memorization while capturing both established and novel relations.\nCompared to benchmarks like BNLearn, where LLMs achieve near-perfect accuracy,\nthey perform far worse on our curated graphs, underscoring the need for\nstatistical grounding. Supporting P.2, we show that using LLM predictions as\npriors for the classical PC algorithm significantly improves accuracy over both\nLLM-only and purely statistical methods. We call on the community to adopt\nscience-grounded, leakage-resistant benchmarks and invest in hybrid causal\ndiscovery methods suited to real-world inquiry.", "AI": {"tldr": "LLMs\u5728\u56e0\u679c\u53d1\u73b0\u4e2d\u7684\u8868\u73b0\u88ab\u9ad8\u4f30\uff0c\u56e0\u4e3a\u8bc4\u4f30\u57fa\u51c6\u53ef\u80fd\u5df2\u5305\u542b\u5728\u9884\u8bad\u7ec3\u6570\u636e\u4e2d\u3002\u9700\u8981\u5f00\u53d1\u9632\u6cc4\u6f0f\u7684\u8bc4\u4f30\u534f\u8bae\u548c\u7ed3\u5408LLM\u77e5\u8bc6\u4e0e\u7edf\u8ba1\u65b9\u6cd5\u7684\u6df7\u5408\u65b9\u6cd5\u3002", "motivation": "\u6311\u6218LLMs\u5728\u56e0\u679c\u53d1\u73b0\u4e2d\u7684\u771f\u5b9e\u80fd\u529b\uff0c\u8d28\u7591\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u56e0\u6570\u636e\u6cc4\u6f0f\u95ee\u9898\u800c\u4e0d\u53ef\u9760\uff0c\u9700\u8981\u66f4\u4e25\u683c\u7684\u8bc4\u4f30\u548c\u5b9e\u7528\u7684\u6df7\u5408\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u8fd1\u671f\u79d1\u5b66\u7814\u7a76\u7684\u8bc4\u4f30\u534f\u8bae\u9632\u6b62\u6570\u636e\u6cc4\u6f0f\uff0c\u8bbe\u8ba1\u6df7\u5408\u65b9\u6cd5\u5c06LLM\u9884\u6d4b\u4f5c\u4e3a\u7ecf\u5178PC\u7b97\u6cd5\u7684\u5148\u9a8c\u77e5\u8bc6\u3002", "result": "\u5728BNLearn\u57fa\u51c6\u4e0aLLMs\u8868\u73b0\u63a5\u8fd1\u5b8c\u7f8e\uff0c\u4f46\u5728\u65b0\u6784\u5efa\u7684\u771f\u5b9e\u79d1\u5b66\u56fe\u8c31\u4e0a\u8868\u73b0\u8f83\u5dee\uff1b\u5c06LLM\u9884\u6d4b\u4f5c\u4e3aPC\u7b97\u6cd5\u5148\u9a8c\u53ef\u663e\u8457\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "conclusion": "\u547c\u5401\u793e\u533a\u91c7\u7528\u57fa\u4e8e\u79d1\u5b66\u3001\u9632\u6cc4\u6f0f\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u6295\u8d44\u4e8e\u9002\u5408\u771f\u5b9e\u4e16\u754c\u7814\u7a76\u7684\u6df7\u5408\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u3002"}}
{"id": "2510.17590", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.CY", "cs.LG", "I.2.7; H.3.3; I.4.9"], "pdf": "https://arxiv.org/pdf/2510.17590", "abs": "https://arxiv.org/abs/2510.17590", "authors": ["Mir Nafis Sharear Shopnil", "Sharad Duwal", "Abhishek Tyagi", "Adiba Mahbub Proma"], "title": "MIRAGE: Agentic Framework for Multimodal Misinformation Detection with Web-Grounded Reasoning", "comment": "16 pages, 3 tables, 1 figure", "summary": "Misinformation spreads across web platforms through billions of daily\nmultimodal posts that combine text and images, overwhelming manual\nfact-checking capacity. Supervised detection models require domain-specific\ntraining data and fail to generalize across diverse manipulation tactics. We\npresent MIRAGE, an inference-time, model-pluggable agentic framework that\ndecomposes multimodal verification into four sequential modules: visual\nveracity assessment detects AI-generated images, cross-modal consistency\nanalysis identifies out-of-context repurposing, retrieval-augmented factual\nchecking grounds claims in web evidence through iterative question generation,\nand a calibrated judgment module integrates all signals. MIRAGE orchestrates\nvision-language model reasoning with targeted web retrieval, outputs structured\nand citation-linked rationales. On MMFakeBench validation set (1,000 samples),\nMIRAGE with GPT-4o-mini achieves 81.65% F1 and 75.1% accuracy, outperforming\nthe strongest zero-shot baseline (GPT-4V with MMD-Agent at 74.0% F1) by 7.65\npoints while maintaining 34.3% false positive rate versus 97.3% for a\njudge-only baseline. Test set results (5,000 samples) confirm generalization\nwith 81.44% F1 and 75.08% accuracy. Ablation studies show visual verification\ncontributes 5.18 F1 points and retrieval-augmented reasoning contributes 2.97\npoints. Our results demonstrate that decomposed agentic reasoning with web\nretrieval can match supervised detector performance without domain-specific\ntraining, enabling misinformation detection across modalities where labeled\ndata remains scarce.", "AI": {"tldr": "MIRAGE\u662f\u4e00\u4e2a\u7528\u4e8e\u68c0\u6d4b\u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f\u7684\u63a8\u7406\u65f6\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u9a8c\u8bc1\u8fc7\u7a0b\u4e3a\u56db\u4e2a\u6a21\u5757\uff1a\u89c6\u89c9\u771f\u5b9e\u6027\u8bc4\u4f30\u3001\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u5206\u6790\u3001\u68c0\u7d22\u589e\u5f3a\u7684\u4e8b\u5b9e\u6838\u67e5\u548c\u6821\u51c6\u5224\u65ad\uff0c\u65e0\u9700\u9886\u57df\u7279\u5b9a\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u8fbe\u5230\u76d1\u7763\u68c0\u6d4b\u5668\u7684\u6027\u80fd\u3002", "motivation": "\u7f51\u7edc\u5e73\u53f0\u4e0a\u6bcf\u5929\u6709\u6570\u5341\u4ebf\u5305\u542b\u6587\u672c\u548c\u56fe\u50cf\u7684\u591a\u6a21\u6001\u5e16\u5b50\u4f20\u64ad\u865a\u5047\u4fe1\u606f\uff0c\u8d85\u51fa\u4e86\u4eba\u5de5\u4e8b\u5b9e\u6838\u67e5\u7684\u80fd\u529b\u3002\u73b0\u6709\u7684\u76d1\u7763\u68c0\u6d4b\u6a21\u578b\u9700\u8981\u9886\u57df\u7279\u5b9a\u8bad\u7ec3\u6570\u636e\uff0c\u4e14\u65e0\u6cd5\u6cdb\u5316\u5230\u4e0d\u540c\u7684\u64cd\u7eb5\u7b56\u7565\u3002", "method": "MIRAGE\u6846\u67b6\u5c06\u591a\u6a21\u6001\u9a8c\u8bc1\u5206\u89e3\u4e3a\u56db\u4e2a\u987a\u5e8f\u6a21\u5757\uff1a1) \u89c6\u89c9\u771f\u5b9e\u6027\u8bc4\u4f30\u68c0\u6d4bAI\u751f\u6210\u56fe\u50cf\uff1b2) \u8de8\u6a21\u6001\u4e00\u81f4\u6027\u5206\u6790\u8bc6\u522b\u4e0a\u4e0b\u6587\u4e0d\u5f53\u7684\u91cd\u65b0\u5229\u7528\uff1b3) \u68c0\u7d22\u589e\u5f3a\u7684\u4e8b\u5b9e\u6838\u67e5\u901a\u8fc7\u8fed\u4ee3\u95ee\u9898\u751f\u6210\u5c06\u58f0\u660e\u4e0e\u7f51\u7edc\u8bc1\u636e\u5173\u8054\uff1b4) \u6821\u51c6\u5224\u65ad\u6a21\u5757\u6574\u5408\u6240\u6709\u4fe1\u53f7\u3002\u8be5\u6846\u67b6\u534f\u8c03\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4e0e\u5b9a\u5411\u7f51\u7edc\u68c0\u7d22\u3002", "result": "\u5728MMFakeBench\u9a8c\u8bc1\u96c6\uff081000\u4e2a\u6837\u672c\uff09\u4e0a\uff0cMIRAGE\u4e0eGPT-4o-mini\u7ed3\u5408\u8fbe\u523081.65% F1\u548c75.1%\u51c6\u786e\u7387\uff0c\u6bd4\u6700\u5f3a\u7684\u96f6\u6837\u672c\u57fa\u7ebf\uff08GPT-4V\u4e0eMMD-Agent\u768474.0% F1\uff09\u9ad8\u51fa7.65\u70b9\uff0c\u540c\u65f6\u4fdd\u630134.3%\u7684\u5047\u9633\u6027\u7387\uff08\u76f8\u6bd4\u4ec5\u5224\u65ad\u57fa\u7ebf\u768497.3%\uff09\u3002\u6d4b\u8bd5\u96c6\u7ed3\u679c\uff085000\u4e2a\u6837\u672c\uff09\u786e\u8ba4\u4e86\u6cdb\u5316\u80fd\u529b\uff0c\u8fbe\u523081.44% F1\u548c75.08%\u51c6\u786e\u7387\u3002\u6d88\u878d\u7814\u7a76\u663e\u793a\u89c6\u89c9\u9a8c\u8bc1\u8d21\u732e5.18 F1\u70b9\uff0c\u68c0\u7d22\u589e\u5f3a\u63a8\u7406\u8d21\u732e2.97\u70b9\u3002", "conclusion": "\u5206\u89e3\u7684\u4ee3\u7406\u63a8\u7406\u4e0e\u7f51\u7edc\u68c0\u7d22\u76f8\u7ed3\u5408\uff0c\u53ef\u4ee5\u5728\u6ca1\u6709\u9886\u57df\u7279\u5b9a\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u5339\u914d\u76d1\u7763\u68c0\u6d4b\u5668\u7684\u6027\u80fd\uff0c\u5728\u6807\u8bb0\u6570\u636e\u7a00\u7f3a\u7684\u591a\u6a21\u6001\u573a\u666f\u4e2d\u5b9e\u73b0\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u3002"}}
{"id": "2510.16658", "categories": ["cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2510.16658", "abs": "https://arxiv.org/abs/2510.16658", "authors": ["Shihao Yang", "Xiying Huang", "Danilo Bernardo", "Jun-En Ding", "Andrew Michael", "Jingmei Yang", "Patrick Kwan", "Ashish Raj", "Feng Liu"], "title": "Foundation and Large-Scale AI Models in Neuroscience: A Comprehensive Review", "comment": null, "summary": "The advent of large-scale artificial intelligence (AI) models has a\ntransformative effect on neuroscience research, which represents a paradigm\nshift from the traditional computational methods through the facilitation of\nend-to-end learning from raw brain signals and neural data. In this paper, we\nexplore the transformative effects of large-scale AI models on five major\nneuroscience domains: neuroimaging and data processing, brain-computer\ninterfaces and neural decoding, molecular neuroscience and genomic modeling,\nclinical assistance and translational frameworks, and disease-specific\napplications across neurological and psychiatric disorders. These models are\ndemonstrated to address major computational neuroscience challenges, including\nmultimodal neural data integration, spatiotemporal pattern interpretation, and\nthe derivation of translational frameworks for clinical deployment. Moreover,\nthe interaction between neuroscience and AI has become increasingly reciprocal,\nas biologically informed architectural constraints are now incorporated to\ndevelop more interpretable and computationally efficient models. This review\nhighlights both the notable promise of such technologies and key implementation\nconsiderations, with particular emphasis on rigorous evaluation frameworks,\neffective domain knowledge integration, and comprehensive ethical guidelines\nfor clinical use. Finally, a systematic listing of critical neuroscience\ndatasets used to derive and validate large-scale AI models across diverse\nresearch applications is provided.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5927\u89c4\u6a21AI\u6a21\u578b\u5bf9\u795e\u7ecf\u79d1\u5b66\u7814\u7a76\u7684\u53d8\u9769\u6027\u5f71\u54cd\uff0c\u6db5\u76d6\u795e\u7ecf\u5f71\u50cf\u5904\u7406\u3001\u8111\u673a\u63a5\u53e3\u3001\u5206\u5b50\u795e\u7ecf\u79d1\u5b66\u3001\u4e34\u5e8a\u8f85\u52a9\u548c\u75be\u75c5\u5e94\u7528\u7b49\u4e94\u5927\u9886\u57df\uff0c\u5f3a\u8c03\u4e86AI\u5728\u89e3\u51b3\u591a\u6a21\u6001\u6570\u636e\u6574\u5408\u3001\u65f6\u7a7a\u6a21\u5f0f\u89e3\u91ca\u7b49\u6311\u6218\u4e2d\u7684\u4f5c\u7528\u3002", "motivation": "\u5927\u89c4\u6a21AI\u6a21\u578b\u7684\u51fa\u73b0\u4e3a\u795e\u7ecf\u79d1\u5b66\u7814\u7a76\u5e26\u6765\u4e86\u8303\u5f0f\u8f6c\u53d8\uff0c\u80fd\u591f\u4ece\u539f\u59cb\u8111\u4fe1\u53f7\u548c\u795e\u7ecf\u6570\u636e\u4e2d\u5b9e\u73b0\u7aef\u5230\u7aef\u5b66\u4e60\uff0c\u89e3\u51b3\u4f20\u7edf\u8ba1\u7b97\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u7684\u590d\u6742\u795e\u7ecf\u79d1\u5b66\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u56de\u987e\u548c\u5206\u6790\u5927\u89c4\u6a21AI\u6a21\u578b\u5728\u4e94\u4e2a\u4e3b\u8981\u795e\u7ecf\u79d1\u5b66\u9886\u57df\u7684\u5e94\u7528\uff1a\u795e\u7ecf\u5f71\u50cf\u4e0e\u6570\u636e\u5904\u7406\u3001\u8111\u673a\u63a5\u53e3\u4e0e\u795e\u7ecf\u89e3\u7801\u3001\u5206\u5b50\u795e\u7ecf\u79d1\u5b66\u4e0e\u57fa\u56e0\u7ec4\u5efa\u6a21\u3001\u4e34\u5e8a\u8f85\u52a9\u4e0e\u8f6c\u5316\u6846\u67b6\u3001\u795e\u7ecf\u7cfb\u7edf\u4e0e\u7cbe\u795e\u75be\u75c5\u7684\u7279\u5b9a\u5e94\u7528\u3002", "result": "\u7814\u7a76\u8868\u660e\u8fd9\u4e9b\u6a21\u578b\u80fd\u6709\u6548\u89e3\u51b3\u591a\u6a21\u6001\u795e\u7ecf\u6570\u636e\u6574\u5408\u3001\u65f6\u7a7a\u6a21\u5f0f\u89e3\u91ca\u7b49\u8ba1\u7b97\u795e\u7ecf\u79d1\u5b66\u6311\u6218\uff0c\u540c\u65f6\u795e\u7ecf\u79d1\u5b66\u4e0eAI\u7684\u4e92\u52a8\u53d8\u5f97\u66f4\u52a0\u76f8\u4e92\u4fc3\u8fdb\uff0c\u751f\u7269\u542f\u53d1\u7684\u67b6\u6784\u7ea6\u675f\u88ab\u7528\u4e8e\u5f00\u53d1\u66f4\u53ef\u89e3\u91ca\u548c\u8ba1\u7b97\u9ad8\u6548\u7684\u6a21\u578b\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u5f3a\u8c03\u4e86\u8fd9\u4e9b\u6280\u672f\u7684\u663e\u8457\u524d\u666f\u548c\u5173\u952e\u5b9e\u65bd\u8003\u8651\uff0c\u7279\u522b\u5f3a\u8c03\u4e25\u683c\u7684\u8bc4\u4f30\u6846\u67b6\u3001\u6709\u6548\u7684\u9886\u57df\u77e5\u8bc6\u6574\u5408\u4ee5\u53ca\u4e34\u5e8a\u4f7f\u7528\u7684\u5168\u9762\u4f26\u7406\u6307\u5357\uff0c\u5e76\u63d0\u4f9b\u4e86\u7528\u4e8e\u9a8c\u8bc1\u5927\u89c4\u6a21AI\u6a21\u578b\u7684\u5173\u952e\u795e\u7ecf\u79d1\u5b66\u6570\u636e\u96c6\u7cfb\u7edf\u5217\u8868\u3002"}}
{"id": "2510.16023", "categories": ["cs.LG", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2510.16023", "abs": "https://arxiv.org/abs/2510.16023", "authors": ["Fanmeng Wang", "Shan Mei", "Wentao Guo", "Hongshuai Wang", "Qi Ou", "Zhifeng Gao", "Hongteng Xu"], "title": "Unifying Polymer Modeling and Design via a Conformation-Centric Generative Foundation Model", "comment": null, "summary": "Polymers, macromolecules formed from covalently bonded monomers, underpin\ncountless technologies and are indispensable to modern life. While deep\nlearning is advancing polymer science, existing methods typically represent the\nwhole polymer solely through monomer-level descriptors, overlooking the global\nstructural information inherent in polymer conformations, which ultimately\nlimits their practical performance. Moreover, this field still lacks a\nuniversal foundation model that can effectively support diverse downstream\ntasks, thereby severely constraining progress. To address these challenges, we\nintroduce PolyConFM, the first polymer foundation model that unifies polymer\nmodeling and design through conformation-centric generative pretraining.\nRecognizing that each polymer conformation can be decomposed into a sequence of\nlocal conformations (i.e., those of its repeating units), we pretrain PolyConFM\nunder the conditional generation paradigm, reconstructing these local\nconformations via masked autoregressive (MAR) modeling and further generating\ntheir orientation transformations to recover the corresponding polymer\nconformation. Besides, we construct the first high-quality polymer conformation\ndataset via molecular dynamics simulations to mitigate data sparsity, thereby\nenabling conformation-centric pretraining. Experiments demonstrate that\nPolyConFM consistently outperforms representative task-specific methods on\ndiverse downstream tasks, equipping polymer science with a universal and\npowerful tool.", "AI": {"tldr": "\u63d0\u51fa\u4e86PolyConFM\uff0c\u9996\u4e2a\u57fa\u4e8e\u6784\u8c61\u7684\u805a\u5408\u7269\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u63a9\u7801\u81ea\u56de\u5f52\u5efa\u6a21\u7edf\u4e00\u805a\u5408\u7269\u5efa\u6a21\u4e0e\u8bbe\u8ba1\uff0c\u5728\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4ec5\u4f7f\u7528\u5355\u4f53\u7ea7\u63cf\u8ff0\u7b26\u8868\u793a\u805a\u5408\u7269\uff0c\u5ffd\u7565\u4e86\u6784\u8c61\u4e2d\u7684\u5168\u5c40\u7ed3\u6784\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u6027\u80fd\uff0c\u4e14\u7f3a\u4e4f\u652f\u6301\u591a\u6837\u5316\u4e0b\u6e38\u4efb\u52a1\u7684\u901a\u7528\u57fa\u7840\u6a21\u578b\u3002", "method": "\u5c06\u805a\u5408\u7269\u6784\u8c61\u5206\u89e3\u4e3a\u5c40\u90e8\u6784\u8c61\u5e8f\u5217\uff0c\u901a\u8fc7\u63a9\u7801\u81ea\u56de\u5f52\u5efa\u6a21\u8fdb\u884c\u6761\u4ef6\u751f\u6210\u9884\u8bad\u7ec3\uff0c\u91cd\u5efa\u5c40\u90e8\u6784\u8c61\u5e76\u751f\u6210\u65b9\u5411\u53d8\u6362\u6765\u6062\u590d\u5b8c\u6574\u805a\u5408\u7269\u6784\u8c61\uff0c\u540c\u65f6\u6784\u5efa\u9996\u4e2a\u9ad8\u8d28\u91cf\u805a\u5408\u7269\u6784\u8c61\u6570\u636e\u96c6\u3002", "result": "PolyConFM\u5728\u591a\u6837\u5316\u4e0b\u6e38\u4efb\u52a1\u4e2d\u6301\u7eed\u4f18\u4e8e\u4ee3\u8868\u6027\u7684\u4efb\u52a1\u7279\u5b9a\u65b9\u6cd5\u3002", "conclusion": "PolyConFM\u4e3a\u805a\u5408\u7269\u79d1\u5b66\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u4e14\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u901a\u8fc7\u6784\u8c61\u4e2d\u5fc3\u7684\u9884\u8bad\u7ec3\u7edf\u4e00\u4e86\u805a\u5408\u7269\u5efa\u6a21\u4e0e\u8bbe\u8ba1\u3002"}}
{"id": "2510.16713", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16713", "abs": "https://arxiv.org/abs/2510.16713", "authors": ["Sriharsh Bhyravajjula", "Melanie Walsh", "Anna Preus", "Maria Antoniak"], "title": "so much depends / upon / a whitespace: Why Whitespace Matters for Poets and LLMs", "comment": null, "summary": "Whitespace is a critical component of poetic form, reflecting both adherence\nto standardized forms and rebellion against those forms. Each poem's whitespace\ndistribution reflects the artistic choices of the poet and is an integral\nsemantic and spatial feature of the poem. Yet, despite the popularity of poetry\nas both a long-standing art form and as a generation task for large language\nmodels (LLMs), whitespace has not received sufficient attention from the NLP\ncommunity. Using a corpus of 19k English-language published poems from Poetry\nFoundation, we investigate how 4k poets have used whitespace in their works. We\nrelease a subset of 2.8k public-domain poems with preserved formatting to\nfacilitate further research in this area. We compare whitespace usage in the\npublished poems to (1) 51k LLM-generated poems, and (2) 12k unpublished poems\nposted in an online community. We also explore whitespace usage across time\nperiods, poetic forms, and data sources. Additionally, we find that different\ntext processing methods can result in significantly different representations\nof whitespace in poetry data, motivating us to use these poems and whitespace\npatterns to discuss implications for the processing strategies used to assemble\npretraining datasets for LLMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u8bd7\u6b4c\u4e2d\u7a7a\u767d\u7a7a\u95f4\u7684\u91cd\u8981\u6027\uff0c\u5206\u6790\u4e8619,000\u9996\u8bd7\u6b4c\u7684\u7a7a\u767d\u4f7f\u7528\u6a21\u5f0f\uff0c\u5e76\u4e0eAI\u751f\u6210\u8bd7\u6b4c\u548c\u672a\u53d1\u8868\u8bd7\u6b4c\u8fdb\u884c\u6bd4\u8f83\uff0c\u63a2\u8ba8\u4e86\u6587\u672c\u5904\u7406\u65b9\u6cd5\u5bf9\u7a7a\u767d\u8868\u793a\u7684\u5f71\u54cd\u3002", "motivation": "\u7a7a\u767d\u7a7a\u95f4\u662f\u8bd7\u6b4c\u5f62\u5f0f\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\uff0c\u53cd\u6620\u4e86\u8bd7\u4eba\u7684\u827a\u672f\u9009\u62e9\uff0c\u4f46\u5728NLP\u7814\u7a76\u4e2d\u672a\u5f97\u5230\u8db3\u591f\u5173\u6ce8\u3002\u8bba\u6587\u65e8\u5728\u7cfb\u7edf\u7814\u7a76\u8bd7\u6b4c\u4e2d\u7a7a\u767d\u7a7a\u95f4\u7684\u4f7f\u7528\u6a21\u5f0f\u53ca\u5176\u610f\u4e49\u3002", "method": "\u4f7f\u7528\u6765\u81eaPoetry Foundation\u768419,000\u9996\u82f1\u8bed\u8bd7\u6b4c\u8bed\u6599\u5e93\uff0c\u5206\u67904,000\u4f4d\u8bd7\u4eba\u7684\u7a7a\u767d\u4f7f\u7528\uff0c\u5e76\u4e0e51,000\u9996LLM\u751f\u6210\u8bd7\u6b4c\u548c12,000\u9996\u672a\u53d1\u8868\u5728\u7ebf\u8bd7\u6b4c\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u53d1\u73b0\u4e0d\u540c\u6587\u672c\u5904\u7406\u65b9\u6cd5\u4f1a\u5bfc\u81f4\u8bd7\u6b4c\u7a7a\u767d\u7a7a\u95f4\u7684\u663e\u8457\u4e0d\u540c\u8868\u793a\uff0c\u5e76\u63ed\u793a\u4e86\u4e0d\u540c\u65f6\u671f\u3001\u8bd7\u6b4c\u5f62\u5f0f\u548c\u6765\u6e90\u4e2d\u7a7a\u767d\u4f7f\u7528\u7684\u5dee\u5f02\u6a21\u5f0f\u3002", "conclusion": "\u8bd7\u6b4c\u7a7a\u767d\u7a7a\u95f4\u7684\u7814\u7a76\u5bf9LLM\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u6784\u5efa\u7b56\u7565\u5177\u6709\u91cd\u8981\u542f\u793a\uff0c\u5f3a\u8c03\u4e86\u4fdd\u7559\u683c\u5f0f\u4fe1\u606f\u5728\u8bd7\u6b4c\u5904\u7406\u4e2d\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2510.16591", "categories": ["cs.LG", "cond-mat.stat-mech", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16591", "abs": "https://arxiv.org/abs/2510.16591", "authors": ["Cassidy Ashworth", "Pietro Li\u00f2", "Francesco Caso"], "title": "Symmetry and Generalisation in Neural Approximations of Renormalisation Transformations", "comment": null, "summary": "Deep learning models have proven enormously successful at using multiple\nlayers of representation to learn relevant features of structured data.\nEncoding physical symmetries into these models can improve performance on\ndifficult tasks, and recent work has motivated the principle of parameter\nsymmetry breaking and restoration as a unifying mechanism underlying their\nhierarchical learning dynamics. We evaluate the role of parameter symmetry and\nnetwork expressivity in the generalisation behaviour of neural networks when\nlearning a real-space renormalisation group (RG) transformation, using the\ncentral limit theorem (CLT) as a test case map. We consider simple multilayer\nperceptrons (MLPs) and graph neural networks (GNNs), and vary weight symmetries\nand activation functions across architectures. Our results reveal a competition\nbetween symmetry constraints and expressivity, with overly complex or\noverconstrained models generalising poorly. We analytically demonstrate this\npoor generalisation behaviour for certain constrained MLP architectures by\nrecasting the CLT as a cumulant recursion relation and making use of an\nestablished framework to propagate cumulants through MLPs. We also empirically\nvalidate an extension of this framework from MLPs to GNNs, elucidating the\ninternal information processing performed by these more complex models. These\nfindings offer new insight into the learning dynamics of symmetric networks and\ntheir limitations in modelling structured physical transformations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u795e\u7ecf\u7f51\u7edc\u4e2d\u53c2\u6570\u5bf9\u79f0\u6027\u548c\u8868\u8fbe\u80fd\u529b\u5728\u6cdb\u5316\u884c\u4e3a\u4e2d\u7684\u4f5c\u7528\uff0c\u4f7f\u7528\u4e2d\u5fc3\u6781\u9650\u5b9a\u7406\u4f5c\u4e3a\u6d4b\u8bd5\u6848\u4f8b\uff0c\u5206\u6790\u4e86\u591a\u5c42\u611f\u77e5\u673a\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u5bf9\u79f0\u7ea6\u675f\u4e0e\u8868\u8fbe\u80fd\u529b\u4e4b\u95f4\u7684\u7ade\u4e89\u5173\u7cfb\u3002", "motivation": "\u5c06\u7269\u7406\u5bf9\u79f0\u6027\u7f16\u7801\u5230\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e2d\u53ef\u4ee5\u63d0\u9ad8\u6027\u80fd\uff0c\u7814\u7a76\u53c2\u6570\u5bf9\u79f0\u6027\u7834\u574f\u548c\u6062\u590d\u673a\u5236\u4f5c\u4e3a\u5c42\u6b21\u5b66\u4e60\u52a8\u6001\u7684\u7edf\u4e00\u673a\u5236\uff0c\u8bc4\u4f30\u53c2\u6570\u5bf9\u79f0\u6027\u548c\u7f51\u7edc\u8868\u8fbe\u80fd\u529b\u5728\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u5b9e\u7a7a\u95f4\u91cd\u6574\u5316\u7fa4\u53d8\u6362\u65f6\u7684\u6cdb\u5316\u884c\u4e3a\u3002", "method": "\u4f7f\u7528\u4e2d\u5fc3\u6781\u9650\u5b9a\u7406\u4f5c\u4e3a\u6d4b\u8bd5\u6848\u4f8b\uff0c\u8003\u8651\u7b80\u5355\u7684\u591a\u5c42\u611f\u77e5\u673a\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u5728\u4e0d\u540c\u67b6\u6784\u4e2d\u6539\u53d8\u6743\u91cd\u5bf9\u79f0\u6027\u548c\u6fc0\u6d3b\u51fd\u6570\uff0c\u901a\u8fc7\u5c06CLT\u91cd\u65b0\u8868\u8ff0\u4e3a\u7d2f\u79ef\u91cf\u9012\u5f52\u5173\u7cfb\uff0c\u5e76\u5229\u7528\u5df2\u6709\u6846\u67b6\u5728MLPs\u4e2d\u4f20\u64ad\u7d2f\u79ef\u91cf\u6765\u5206\u6790\u6cdb\u5316\u884c\u4e3a\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u5bf9\u79f0\u7ea6\u675f\u548c\u8868\u8fbe\u80fd\u529b\u4e4b\u95f4\u7684\u7ade\u4e89\u5173\u7cfb\uff0c\u8fc7\u4e8e\u590d\u6742\u6216\u8fc7\u5ea6\u7ea6\u675f\u7684\u6a21\u578b\u6cdb\u5316\u6027\u80fd\u8f83\u5dee\uff0c\u901a\u8fc7\u5206\u6790\u8bc1\u660e\u4e86\u67d0\u4e9b\u53d7\u9650MLP\u67b6\u6784\u7684\u6cdb\u5316\u6027\u80fd\u5dee\uff0c\u5e76\u7ecf\u9a8c\u9a8c\u8bc1\u4e86\u4eceMLPs\u5230GNNs\u7684\u6846\u67b6\u6269\u5c55\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u5bf9\u79f0\u7f51\u7edc\u7684\u5b66\u4e60\u52a8\u6001\u53ca\u5176\u5728\u5efa\u6a21\u7ed3\u6784\u5316\u7269\u7406\u53d8\u6362\u4e2d\u7684\u5c40\u9650\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\uff0c\u9610\u660e\u4e86\u8fd9\u4e9b\u66f4\u590d\u6742\u6a21\u578b\u7684\u5185\u90e8\u4fe1\u606f\u5904\u7406\u8fc7\u7a0b\u3002"}}
{"id": "2510.16701", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16701", "abs": "https://arxiv.org/abs/2510.16701", "authors": ["Ni Zhang", "Zhiguang Cao", "Jianan Zhou", "Cong Zhang", "Yew-Soon Ong"], "title": "An Agentic Framework with LLMs for Solving Complex Vehicle Routing Problems", "comment": null, "summary": "Complex vehicle routing problems (VRPs) remain a fundamental challenge,\ndemanding substantial expert effort for intent interpretation and algorithm\ndesign. While large language models (LLMs) offer a promising path toward\nautomation, current approaches still rely on external intervention, which\nrestrict autonomy and often lead to execution errors and low solution\nfeasibility. To address these challenges, we propose an Agentic Framework with\nLLMs (AFL) for solving complex vehicle routing problems, achieving full\nautomation from problem instance to solution. AFL directly extracts knowledge\nfrom raw inputs and enables self-contained code generation without handcrafted\nmodules or external solvers. To improve trustworthiness, AFL decomposes the\noverall pipeline into three manageable subtasks and employs four specialized\nagents whose coordinated interactions enforce cross-functional consistency and\nlogical soundness. Extensive experiments on 60 complex VRPs, ranging from\nstandard benchmarks to practical variants, validate the effectiveness and\ngenerality of our framework, showing comparable performance against\nmeticulously designed algorithms. Notably, it substantially outperforms\nexisting LLM-based baselines in both code reliability and solution feasibility,\nachieving rates close to 100% on the evaluated benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7406\u6846\u67b6AFL\uff0c\u7528\u4e8e\u5b8c\u5168\u81ea\u52a8\u5316\u89e3\u51b3\u590d\u6742\u8f66\u8f86\u8def\u5f84\u95ee\u9898\uff0c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u6216\u5916\u90e8\u6c42\u89e3\u5668\uff0c\u5728\u4ee3\u7801\u53ef\u9760\u6027\u548c\u89e3\u51b3\u65b9\u6848\u53ef\u884c\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u5728\u89e3\u51b3\u590d\u6742\u8f66\u8f86\u8def\u5f84\u95ee\u9898\u65f6\u4ecd\u9700\u8981\u5916\u90e8\u5e72\u9884\uff0c\u5bfc\u81f4\u81ea\u4e3b\u6027\u53d7\u9650\u3001\u6267\u884c\u9519\u8bef\u548c\u89e3\u51b3\u65b9\u6848\u53ef\u884c\u6027\u4f4e\u7684\u95ee\u9898\u3002", "method": "AFL\u6846\u67b6\u5c06\u6574\u4f53\u6d41\u7a0b\u5206\u89e3\u4e3a\u4e09\u4e2a\u53ef\u7ba1\u7406\u7684\u5b50\u4efb\u52a1\uff0c\u5e76\u91c7\u7528\u56db\u4e2a\u4e13\u95e8\u4ee3\u7406\u7684\u534f\u8c03\u4ea4\u4e92\u6765\u786e\u4fdd\u8de8\u529f\u80fd\u4e00\u81f4\u6027\u548c\u903b\u8f91\u5408\u7406\u6027\uff0c\u76f4\u63a5\u4ece\u539f\u59cb\u8f93\u5165\u4e2d\u63d0\u53d6\u77e5\u8bc6\u5e76\u5b9e\u73b0\u81ea\u5305\u542b\u7684\u4ee3\u7801\u751f\u6210\u3002", "result": "\u572860\u4e2a\u590d\u6742VRP\u95ee\u9898\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u4ee3\u7801\u53ef\u9760\u6027\u548c\u89e3\u51b3\u65b9\u6848\u53ef\u884c\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709LLM\u57fa\u7ebf\uff0c\u5728\u8bc4\u4f30\u57fa\u51c6\u4e0a\u63a5\u8fd1100%\u7684\u6210\u529f\u7387\uff0c\u4e0e\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u7b97\u6cd5\u6027\u80fd\u76f8\u5f53\u3002", "conclusion": "AFL\u6846\u67b6\u5b9e\u73b0\u4e86\u4ece\u95ee\u9898\u5b9e\u4f8b\u5230\u89e3\u51b3\u65b9\u6848\u7684\u5b8c\u5168\u81ea\u52a8\u5316\uff0c\u4e3a\u590d\u6742\u8f66\u8f86\u8def\u5f84\u95ee\u9898\u7684\u89e3\u51b3\u63d0\u4f9b\u4e86\u53ef\u4fe1\u8d56\u7684\u7aef\u5230\u7aef\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.17122", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.17122", "abs": "https://arxiv.org/abs/2510.17122", "authors": ["Chengxiu Hua", "Jiawen Gu", "Yushun Tang"], "title": "Continuous Q-Score Matching: Diffusion Guided Reinforcement Learning for Continuous-Time Control", "comment": null, "summary": "Reinforcement learning (RL) has achieved significant success across a wide\nrange of domains, however, most existing methods are formulated in discrete\ntime. In this work, we introduce a novel RL method for continuous-time control,\nwhere stochastic differential equations govern state-action dynamics. Departing\nfrom traditional value function-based approaches, our key contribution is the\ncharacterization of continuous-time Q-functions via a martingale condition and\nthe linking of diffusion policy scores to the action gradient of a learned\ncontinuous Q-function by the dynamic programming principle. This insight\nmotivates Continuous Q-Score Matching (CQSM), a score-based policy improvement\nalgorithm. Notably, our method addresses a long-standing challenge in\ncontinuous-time RL: preserving the action-evaluation capability of Q-functions\nwithout relying on time discretization. We further provide theoretical\nclosed-form solutions for linear-quadratic (LQ) control problems within our\nframework. Numerical results in simulated environments demonstrate the\neffectiveness of our proposed method and compare it to popular baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8fde\u7eed\u65f6\u95f4\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5CQSM\uff0c\u901a\u8fc7\u9785\u6761\u4ef6\u5b9a\u4e49\u8fde\u7eed\u65f6\u95f4Q\u51fd\u6570\uff0c\u5e76\u5c06\u6269\u6563\u7b56\u7565\u5f97\u5206\u4e0eQ\u51fd\u6570\u7684\u52a8\u4f5c\u68af\u5ea6\u5173\u8054\uff0c\u89e3\u51b3\u4e86\u8fde\u7eed\u65f6\u95f4RL\u4e2dQ\u51fd\u6570\u52a8\u4f5c\u8bc4\u4f30\u80fd\u529b\u7684\u957f\u671f\u6311\u6218\u3002", "motivation": "\u5927\u591a\u6570\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u90fd\u662f\u79bb\u6563\u65f6\u95f4\u6846\u67b6\uff0c\u800c\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u63a7\u5236\u95ee\u9898\u5f80\u5f80\u662f\u8fde\u7eed\u65f6\u95f4\u7684\u3002\u4f20\u7edf\u57fa\u4e8e\u503c\u51fd\u6570\u7684\u65b9\u6cd5\u5728\u8fde\u7eed\u65f6\u95f4\u8bbe\u7f6e\u4e2d\u96be\u4ee5\u4fdd\u6301Q\u51fd\u6570\u7684\u52a8\u4f5c\u8bc4\u4f30\u80fd\u529b\uff0c\u4e14\u4f9d\u8d56\u65f6\u95f4\u79bb\u6563\u5316\u3002", "method": "\u901a\u8fc7\u9785\u6761\u4ef6\u5b9a\u4e49\u8fde\u7eed\u65f6\u95f4Q\u51fd\u6570\uff0c\u5229\u7528\u52a8\u6001\u89c4\u5212\u539f\u7406\u5c06\u6269\u6563\u7b56\u7565\u5f97\u5206\u4e0e\u5b66\u4e60\u5230\u7684\u8fde\u7eedQ\u51fd\u6570\u7684\u52a8\u4f5c\u68af\u5ea6\u5173\u8054\uff0c\u63d0\u51fa\u57fa\u4e8e\u5f97\u5206\u5339\u914d\u7684CQSM\u7b56\u7565\u6539\u8fdb\u7b97\u6cd5\u3002", "result": "\u5728\u7ebf\u6027\u4e8c\u6b21\u63a7\u5236\u95ee\u9898\u4e2d\u63d0\u4f9b\u4e86\u7406\u8bba\u95ed\u5f0f\u89e3\uff0c\u5728\u6a21\u62df\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e0e\u4e3b\u6d41\u57fa\u7ebf\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "conclusion": "CQSM\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u8fde\u7eed\u65f6\u95f4\u5f3a\u5316\u5b66\u4e60\u4e2d\u4fdd\u6301Q\u51fd\u6570\u52a8\u4f5c\u8bc4\u4f30\u80fd\u529b\u7684\u957f\u671f\u6311\u6218\uff0c\u65e0\u9700\u4f9d\u8d56\u65f6\u95f4\u79bb\u6563\u5316\uff0c\u4e3a\u8fde\u7eed\u65f6\u95f4\u63a7\u5236\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.16026", "categories": ["cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.16026", "abs": "https://arxiv.org/abs/2510.16026", "authors": ["Marco Barbero-Mota", "Eric V. Strobl", "John M. Still", "William W. Stead", "Thomas A. Lasko"], "title": "A tutorial on discovering and quantifying the effect of latent causal sources of multimodal EHR data", "comment": null, "summary": "We provide an accessible description of a peer-reviewed generalizable causal\nmachine learning pipeline to (i) discover latent causal sources of large-scale\nelectronic health records observations, and (ii) quantify the source causal\neffects on clinical outcomes. We illustrate how imperfect multimodal clinical\ndata can be processed, decomposed into probabilistic independent latent\nsources, and used to train taskspecific causal models from which individual\ncausal effects can be estimated. We summarize the findings of the two\nreal-world applications of the approach to date as a demonstration of its\nversatility and utility for medical discovery at scale.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u53ef\u6cdb\u5316\u7684\u56e0\u679c\u673a\u5668\u5b66\u4e60\u6d41\u7a0b\uff0c\u7528\u4e8e\u4ece\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u53d1\u73b0\u6f5c\u5728\u56e0\u679c\u6e90\u5e76\u91cf\u5316\u5176\u5bf9\u4e34\u5e8a\u7ed3\u679c\u7684\u5f71\u54cd", "motivation": "\u5904\u7406\u4e0d\u5b8c\u5584\u7684\u591a\u6a21\u6001\u4e34\u5e8a\u6570\u636e\uff0c\u53d1\u73b0\u5927\u89c4\u6a21\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u7684\u6f5c\u5728\u56e0\u679c\u6e90\uff0c\u5e76\u91cf\u5316\u8fd9\u4e9b\u6e90\u5bf9\u4e34\u5e8a\u7ed3\u679c\u7684\u56e0\u679c\u6548\u5e94", "method": "\u5f00\u53d1\u56e0\u679c\u673a\u5668\u5b66\u4e60\u6d41\u7a0b\uff1a\u5904\u7406\u591a\u6a21\u6001\u4e34\u5e8a\u6570\u636e\uff0c\u5206\u89e3\u4e3a\u6982\u7387\u72ec\u7acb\u7684\u6f5c\u5728\u6e90\uff0c\u8bad\u7ec3\u4efb\u52a1\u7279\u5b9a\u7684\u56e0\u679c\u6a21\u578b\u6765\u4f30\u8ba1\u4e2a\u4f53\u56e0\u679c\u6548\u5e94", "result": "\u5df2\u5728\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u533b\u5b66\u53d1\u73b0\u4e2d\u7684\u901a\u7528\u6027\u548c\u5b9e\u7528\u6027", "conclusion": "\u8be5\u56e0\u679c\u673a\u5668\u5b66\u4e60\u6d41\u7a0b\u80fd\u591f\u6709\u6548\u5904\u7406\u4e0d\u5b8c\u5584\u7684\u4e34\u5e8a\u6570\u636e\uff0c\u53d1\u73b0\u6f5c\u5728\u56e0\u679c\u6e90\u5e76\u91cf\u5316\u5176\u5f71\u54cd\uff0c\u4e3a\u5927\u89c4\u6a21\u533b\u5b66\u53d1\u73b0\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177"}}
{"id": "2510.16727", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16727", "abs": "https://arxiv.org/abs/2510.16727", "authors": ["Sanskar Pandey", "Ruhaan Chopra", "Angkul Puniya", "Sohom Pal"], "title": "Beacon: Single-Turn Diagnosis and Mitigation of Latent Sycophancy in Large Language Models", "comment": null, "summary": "Large language models internalize a structural trade-off between truthfulness\nand obsequious flattery, emerging from reward optimization that conflates\nhelpfulness with polite submission. This latent bias, known as sycophancy,\nmanifests as a preference for user agreement over principled reasoning. We\nintroduce Beacon, a single-turn forced-choice benchmark that isolates this bias\nindependent of conversational context, enabling precise measurement of the\ntension between factual accuracy and submissive bias. Evaluations across twelve\nstate-of-the-art models reveal that sycophancy decomposes into stable\nlinguistic and affective sub-biases, each scaling with model capacity. We\nfurther propose prompt-level and activation-level interventions that modulate\nthese biases in opposing directions, exposing the internal geometry of\nalignment as a dynamic manifold between truthfulness and socially compliant\njudgment. Beacon reframes sycophancy as a measurable form of normative\nmisgeneralization, providing a reproducible foundation for studying and\nmitigating alignment drift in large-scale generative systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86Beacon\u57fa\u51c6\u6765\u6d4b\u91cf\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u8c04\u5a9a\u504f\u89c1\uff0c\u53d1\u73b0\u8be5\u504f\u89c1\u968f\u6a21\u578b\u80fd\u529b\u589e\u5f3a\u800c\u589e\u52a0\uff0c\u5e76\u63d0\u51fa\u4e86\u5e72\u9884\u65b9\u6cd5\u6765\u8c03\u8282\u771f\u5b9e\u6027\u4e0e\u793e\u4f1a\u987a\u4ece\u6027\u4e4b\u95f4\u7684\u5e73\u8861\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5956\u52b1\u4f18\u5316\u8fc7\u7a0b\u4e2d\u6df7\u6dc6\u4e86\u5e2e\u52a9\u6027\u4e0e\u793c\u8c8c\u987a\u4ece\uff0c\u5f62\u6210\u4e86\u771f\u5b9e\u6027\u4e0e\u8c04\u5a9a\u4e4b\u95f4\u7684\u7ed3\u6784\u6027\u6743\u8861\uff0c\u8fd9\u79cd\u504f\u89c1\u8868\u73b0\u4e3a\u504f\u597d\u7528\u6237\u540c\u610f\u800c\u975e\u539f\u5219\u6027\u63a8\u7406\u3002", "method": "\u5f15\u5165Beacon\u5355\u8f6e\u5f3a\u5236\u9009\u62e9\u57fa\u51c6\uff0c\u72ec\u7acb\u4e8e\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u9694\u79bb\u8fd9\u79cd\u504f\u89c1\uff0c\u5e76\u63d0\u51fa\u4e86\u63d0\u793a\u7ea7\u548c\u6fc0\u6d3b\u7ea7\u5e72\u9884\u65b9\u6cd5\u6765\u8c03\u8282\u8fd9\u4e9b\u504f\u89c1\u3002", "result": "\u572812\u4e2a\u6700\u5148\u8fdb\u6a21\u578b\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u8c04\u5a9a\u504f\u89c1\u53ef\u5206\u89e3\u4e3a\u7a33\u5b9a\u7684\u8bed\u8a00\u548c\u60c5\u611f\u5b50\u504f\u89c1\uff0c\u6bcf\u4e2a\u5b50\u504f\u89c1\u90fd\u968f\u6a21\u578b\u5bb9\u91cf\u800c\u6269\u5c55\u3002\u5e72\u9884\u65b9\u6cd5\u80fd\u5728\u76f8\u53cd\u65b9\u5411\u4e0a\u8c03\u8282\u8fd9\u4e9b\u504f\u89c1\u3002", "conclusion": "Beacon\u5c06\u8c04\u5a9a\u91cd\u65b0\u5b9a\u4e49\u4e3a\u53ef\u6d4b\u91cf\u7684\u89c4\u8303\u6027\u9519\u8bef\u6cdb\u5316\u5f62\u5f0f\uff0c\u4e3a\u7814\u7a76\u548c\u5927\u89c4\u6a21\u751f\u6210\u7cfb\u7edf\u4e2d\u7684\u5bf9\u9f50\u6f02\u79fb\u7f13\u89e3\u63d0\u4f9b\u4e86\u53ef\u91cd\u590d\u7684\u57fa\u7840\u3002"}}
{"id": "2510.17564", "categories": ["cs.LG", "cs.AI", "cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.17564", "abs": "https://arxiv.org/abs/2510.17564", "authors": ["Lindsay Spoor", "\u00c1lvaro Serra-G\u00f3mez", "Aske Plaat", "Thomas Moerland"], "title": "An Empirical Study of Lagrangian Methods in Safe Reinforcement Learning", "comment": null, "summary": "In safety-critical domains such as robotics, navigation and power systems,\nconstrained optimization problems arise where maximizing performance must be\ncarefully balanced with associated constraints. Safe reinforcement learning\nprovides a framework to address these challenges, with Lagrangian methods being\na popular choice. However, the effectiveness of Lagrangian methods crucially\ndepends on the choice of the Lagrange multiplier $\\lambda$, which governs the\ntrade-off between return and constraint cost. A common approach is to update\nthe multiplier automatically during training. Although this is standard in\npractice, there remains limited empirical evidence on the robustness of an\nautomated update and its influence on overall performance. Therefore, we\nanalyze (i) optimality and (ii) stability of Lagrange multipliers in safe\nreinforcement learning across a range of tasks. We provide $\\lambda$-profiles\nthat give a complete visualization of the trade-off between return and\nconstraint cost of the optimization problem. These profiles show the highly\nsensitive nature of $\\lambda$ and moreover confirm the lack of general\nintuition for choosing the optimal value $\\lambda^*$. Our findings additionally\nshow that automated multiplier updates are able to recover and sometimes even\nexceed the optimal performance found at $\\lambda^*$ due to the vast difference\nin their learning trajectories. Furthermore, we show that automated multiplier\nupdates exhibit oscillatory behavior during training, which can be mitigated\nthrough PID-controlled updates. However, this method requires careful tuning to\nachieve consistently better performance across tasks. This highlights the need\nfor further research on stabilizing Lagrangian methods in safe reinforcement\nlearning. The code used to reproduce our results can be found at\nhttps://github.com/lindsayspoor/Lagrangian_SafeRL.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u4e2d\u62c9\u683c\u6717\u65e5\u4e58\u5b50\u7684\u6700\u4f18\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u53d1\u73b0\u81ea\u52a8\u66f4\u65b0\u4e58\u5b50\u80fd\u591f\u6062\u590d\u751a\u81f3\u8d85\u8fc7\u6700\u4f18\u6027\u80fd\uff0c\u4f46\u5b58\u5728\u632f\u8361\u884c\u4e3a\uff0c\u53ef\u901a\u8fc7PID\u63a7\u5236\u7f13\u89e3\u3002", "motivation": "\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\uff0c\u62c9\u683c\u6717\u65e5\u65b9\u6cd5\u662f\u5904\u7406\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u7684\u5e38\u7528\u65b9\u6cd5\uff0c\u4f46\u4e58\u5b50\u03bb\u7684\u9009\u62e9\u5bf9\u6027\u80fd\u5f71\u54cd\u5f88\u5927\uff0c\u76ee\u524d\u7f3a\u4e4f\u5bf9\u81ea\u52a8\u66f4\u65b0\u4e58\u5b50\u9c81\u68d2\u6027\u7684\u5b9e\u8bc1\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u5206\u6790\u591a\u4e2a\u4efb\u52a1\u4e2d\u62c9\u683c\u6717\u65e5\u4e58\u5b50\u7684\u6700\u4f18\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u63d0\u4f9b\u03bb-profile\u53ef\u89c6\u5316\u6027\u80fd\u4e0e\u7ea6\u675f\u6210\u672c\u7684\u6743\u8861\u5173\u7cfb\uff0c\u5e76\u7814\u7a76\u81ea\u52a8\u4e58\u5b50\u66f4\u65b0\u548cPID\u63a7\u5236\u65b9\u6cd5\u3002", "result": "\u53d1\u73b0\u03bb\u5177\u6709\u9ad8\u5ea6\u654f\u611f\u6027\uff0c\u81ea\u52a8\u4e58\u5b50\u66f4\u65b0\u80fd\u591f\u6062\u590d\u751a\u81f3\u8d85\u8fc7\u6700\u4f18\u6027\u80fd\uff0c\u4f46\u5b58\u5728\u632f\u8361\u884c\u4e3a\uff1bPID\u63a7\u5236\u53ef\u7f13\u89e3\u632f\u8361\u4f46\u9700\u8981\u4ed4\u7ec6\u8c03\u53c2\u3002", "conclusion": "\u62c9\u683c\u6717\u65e5\u65b9\u6cd5\u5728\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u4e2d\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u63d0\u5347\u7a33\u5b9a\u6027\uff0c\u81ea\u52a8\u4e58\u5b50\u66f4\u65b0\u867d\u6709\u6548\u4f46\u5b58\u5728\u632f\u8361\u95ee\u9898\u3002"}}
{"id": "2510.16974", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16974", "abs": "https://arxiv.org/abs/2510.16974", "authors": ["Shurong Lin", "Aleksandra Slavkovi\u0107", "Deekshith Reddy Bhoomireddy"], "title": "Differentially Private Linear Regression and Synthetic Data Generation with Statistical Guarantees", "comment": null, "summary": "In social sciences, small- to medium-scale datasets are common and linear\nregression (LR) is canonical. In privacy-aware settings, much work has focused\non differentially private (DP) LR, but mostly on point estimation with limited\nattention to uncertainty quantification. Meanwhile, synthetic data generation\n(SDG) is increasingly important for reproducibility studies, yet current DP LR\nmethods do not readily support it. Mainstream SDG approaches are either\ntailored to discretized data, making them less suitable for continuous\nregression, or rely on deep models that require large datasets, limiting their\nuse for the smaller, continuous data typical in social science. We propose a\nmethod for LR with valid inference under Gaussian DP: a DP bias-corrected\nestimator with asymptotic confidence intervals (CIs) and a general SDG\nprocedure in which regression on the synthetic data matches our DP regression.\nOur binning-aggregation strategy is effective in small- to moderate-dimensional\nsettings. Experiments show our method (1) improves accuracy over existing\nmethods, (2) provides valid CIs, and (3) produces more reliable synthetic data\nfor downstream ML tasks than current DP SDGs.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5728\u5dee\u5206\u9690\u79c1\u4e0b\u8fdb\u884c\u7ebf\u6027\u56de\u5f52\u7684\u65b9\u6cd5\uff0c\u63d0\u4f9b\u6709\u6548\u7684\u7edf\u8ba1\u63a8\u65ad\u548c\u5408\u6210\u6570\u636e\u751f\u6210\uff0c\u9002\u7528\u4e8e\u793e\u4f1a\u79d1\u5b66\u4e2d\u7684\u4e2d\u5c0f\u89c4\u6a21\u6570\u636e\u96c6\u3002", "motivation": "\u793e\u4f1a\u79d1\u5b66\u4e2d\u5e38\u89c1\u4e2d\u5c0f\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u73b0\u6709\u5dee\u5206\u9690\u79c1\u7ebf\u6027\u56de\u5f52\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u70b9\u4f30\u8ba1\u800c\u7f3a\u4e4f\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u4e14\u5f53\u524d\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\u4e0d\u9002\u5408\u8fde\u7eed\u56de\u5f52\u6570\u636e\u6216\u9700\u8981\u5927\u6570\u636e\u96c6\u3002", "method": "\u4f7f\u7528\u9ad8\u65af\u5dee\u5206\u9690\u79c1\u4e0b\u7684\u504f\u5dee\u6821\u6b63\u4f30\u8ba1\u5668\uff0c\u63d0\u4f9b\u6e10\u8fd1\u7f6e\u4fe1\u533a\u95f4\uff0c\u5e76\u901a\u8fc7\u5206\u7bb1\u805a\u5408\u7b56\u7565\u5b9e\u73b0\u5408\u6210\u6570\u636e\u751f\u6210\uff0c\u4f7f\u5408\u6210\u6570\u636e\u4e0a\u7684\u56de\u5f52\u4e0e\u5dee\u5206\u9690\u79c1\u56de\u5f52\u7ed3\u679c\u4e00\u81f4\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u63d0\u4f9b\u6709\u6548\u7684\u7f6e\u4fe1\u533a\u95f4\uff0c\u4e14\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u5728\u4e0b\u6e38\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u4e2d\u6bd4\u5f53\u524d\u5dee\u5206\u9690\u79c1\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\u66f4\u53ef\u9760\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u793e\u4f1a\u79d1\u5b66\u4e2d\u7684\u4e2d\u5c0f\u89c4\u6a21\u8fde\u7eed\u6570\u636e\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5dee\u5206\u9690\u79c1\u7ebf\u6027\u56de\u5f52\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u7edf\u8ba1\u63a8\u65ad\u548c\u5408\u6210\u6570\u636e\u751f\u6210\u3002"}}
{"id": "2510.16720", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16720", "abs": "https://arxiv.org/abs/2510.16720", "authors": ["Jitao Sang", "Jinlin Xiao", "Jiarun Han", "Jilin Chen", "Xiaoyi Chen", "Shuyu Wei", "Yongjie Sun", "Yuhang Wang"], "title": "Beyond Pipelines: A Survey of the Paradigm Shift toward Model-Native Agentic AI", "comment": null, "summary": "The rapid evolution of agentic AI marks a new phase in artificial\nintelligence, where Large Language Models (LLMs) no longer merely respond but\nact, reason, and adapt. This survey traces the paradigm shift in building\nagentic AI: from Pipeline-based systems, where planning, tool use, and memory\nare orchestrated by external logic, to the emerging Model-native paradigm,\nwhere these capabilities are internalized within the model's parameters. We\nfirst position Reinforcement Learning (RL) as the algorithmic engine enabling\nthis paradigm shift. By reframing learning from imitating static data to\noutcome-driven exploration, RL underpins a unified solution of LLM + RL + Task\nacross language, vision and embodied domains. Building on this, the survey\nsystematically reviews how each capability -- Planning, Tool use, and Memory --\nhas evolved from externally scripted modules to end-to-end learned behaviors.\nFurthermore, it examines how this paradigm shift has reshaped major agent\napplications, specifically the Deep Research agent emphasizing long-horizon\nreasoning and the GUI agent emphasizing embodied interaction. We conclude by\ndiscussing the continued internalization of agentic capabilities like\nMulti-agent collaboration and Reflection, alongside the evolving roles of the\nsystem and model layers in future agentic AI. Together, these developments\noutline a coherent trajectory toward model-native agentic AI as an integrated\nlearning and interaction framework, marking the transition from constructing\nsystems that apply intelligence to developing models that grow intelligence\nthrough experience.", "AI": {"tldr": "\u672c\u8c03\u67e5\u5206\u6790\u4e86\u667a\u80fdAI\u4ece\u57fa\u4e8e\u7ba1\u7ebf\u7684\u5916\u90e8\u903b\u8f91\u7f16\u6392\u5411\u6a21\u578b\u539f\u751f\u5185\u90e8\u5316\u80fd\u529b\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u91cd\u70b9\u63a2\u8ba8\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u5b9e\u73b0\u8fd9\u4e00\u8f6c\u53d8\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u4ee5\u53ca\u89c4\u5212\u3001\u5de5\u5177\u4f7f\u7528\u548c\u8bb0\u5fc6\u7b49\u80fd\u529b\u5982\u4f55\u4ece\u5916\u90e8\u6a21\u5757\u6f14\u53d8\u4e3a\u7aef\u5230\u7aef\u5b66\u4e60\u884c\u4e3a\u3002", "motivation": "\u8ffd\u8e2a\u667a\u80fdAI\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u4ece\u5916\u90e8\u903b\u8f91\u7f16\u6392\u7684\u7ba1\u7ebf\u7cfb\u7edf\u8f6c\u5411\u6a21\u578b\u5185\u90e8\u5316\u80fd\u529b\u7684\u65b0\u8303\u5f0f\uff0c\u7406\u89e3\u5f3a\u5316\u5b66\u4e60\u5728\u8fd9\u4e00\u8f6c\u53d8\u4e2d\u7684\u6838\u5fc3\u4f5c\u7528\u3002", "method": "\u7cfb\u7edf\u6027\u5730\u56de\u987e\u4e86\u89c4\u5212\u3001\u5de5\u5177\u4f7f\u7528\u548c\u8bb0\u5fc6\u7b49\u80fd\u529b\u4ece\u5916\u90e8\u811a\u672c\u6a21\u5757\u5230\u7aef\u5230\u7aef\u5b66\u4e60\u884c\u4e3a\u7684\u6f14\u53d8\u8fc7\u7a0b\uff0c\u5e76\u5206\u6790\u4e86\u5f3a\u5316\u5b66\u4e60\u4f5c\u4e3a\u7b97\u6cd5\u5f15\u64ce\u5982\u4f55\u652f\u6491LLM + RL + Task\u7684\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u63ed\u793a\u4e86\u667a\u80fdAI\u5411\u6a21\u578b\u539f\u751f\u8303\u5f0f\u53d1\u5c55\u7684\u6e05\u6670\u8f68\u8ff9\uff0c\u5c55\u793a\u4e86\u80fd\u529b\u5185\u90e8\u5316\u5982\u4f55\u91cd\u5851\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u548cGUI\u4ee3\u7406\u7b49\u4e3b\u8981\u5e94\u7528\uff0c\u5e76\u63a8\u52a8\u4e86\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u548c\u53cd\u601d\u7b49\u80fd\u529b\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "conclusion": "\u667a\u80fdAI\u6b63\u4ece\u6784\u5efa\u5e94\u7528\u667a\u80fd\u7684\u7cfb\u7edf\u8f6c\u5411\u5f00\u53d1\u901a\u8fc7\u7ecf\u9a8c\u589e\u957f\u667a\u80fd\u7684\u6a21\u578b\uff0c\u6807\u5fd7\u7740\u5411\u96c6\u6210\u5b66\u4e60\u548c\u4ea4\u4e92\u6846\u67b6\u7684\u6a21\u578b\u539f\u751f\u667a\u80fdAI\u7684\u8fc7\u6e21\u3002"}}
{"id": "2510.16035", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.16035", "abs": "https://arxiv.org/abs/2510.16035", "authors": ["Yingguang Yang", "Xianghua Zeng", "Qi Wu", "Hao Peng", "Yutong Xia", "Hao Liu", "Bin Chong", "Philip S. Yu"], "title": "RoBCtrl: Attacking GNN-Based Social Bot Detectors via Reinforced Manipulation of Bots Control Interaction", "comment": "27 pages, 10 figures", "summary": "Social networks have become a crucial source of real-time information for\nindividuals. The influence of social bots within these platforms has garnered\nconsiderable attention from researchers, leading to the development of numerous\ndetection technologies. However, the vulnerability and robustness of these\ndetection methods is still underexplored. Existing Graph Neural Network\n(GNN)-based methods cannot be directly applied due to the issues of limited\ncontrol over social agents, the black-box nature of bot detectors, and the\nheterogeneity of bots. To address these challenges, this paper proposes the\nfirst adversarial multi-agent Reinforcement learning framework for social Bot\ncontrol attacks (RoBCtrl) targeting GNN-based social bot detectors.\nSpecifically, we use a diffusion model to generate high-fidelity bot accounts\nby reconstructing existing account data with minor modifications, thereby\nevading detection on social platforms. To the best of our knowledge, this is\nthe first application of diffusion models to mimic the behavior of evolving\nsocial bots effectively. We then employ a Multi-Agent Reinforcement Learning\n(MARL) method to simulate bots adversarial behavior. We categorize social\naccounts based on their influence and budget. Different agents are then\nemployed to control bot accounts across various categories, optimizing the\nattachment strategy through reinforcement learning. Additionally, a\nhierarchical state abstraction based on structural entropy is designed to\naccelerate the reinforcement learning. Extensive experiments on social bot\ndetection datasets demonstrate that our framework can effectively undermine the\nperformance of GNN-based detectors.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u9488\u5bf9GNN\u793e\u4ea4\u673a\u5668\u4eba\u68c0\u6d4b\u5668\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u653b\u51fb\u6846\u67b6RoBCtrl\uff0c\u4f7f\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u4fdd\u771f\u673a\u5668\u4eba\u8d26\u6237\uff0c\u5e76\u901a\u8fc7MARL\u6a21\u62df\u5bf9\u6297\u884c\u4e3a\uff0c\u6709\u6548\u964d\u4f4e\u68c0\u6d4b\u5668\u6027\u80fd\u3002", "motivation": "\u73b0\u6709GNN\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u5bf9\u793e\u4ea4\u4ee3\u7406\u63a7\u5236\u6709\u9650\u3001\u9ed1\u76d2\u6027\u8d28\u548c\u673a\u5668\u4eba\u5f02\u8d28\u6027\u7b49\u95ee\u9898\uff0c\u5176\u8106\u5f31\u6027\u548c\u9c81\u68d2\u6027\u7814\u7a76\u4e0d\u8db3\uff0c\u9700\u8981\u5f00\u53d1\u6709\u6548\u7684\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u6269\u6563\u6a21\u578b\u91cd\u6784\u73b0\u6709\u8d26\u6237\u6570\u636e\u751f\u6210\u9ad8\u4fdd\u771f\u673a\u5668\u4eba\u8d26\u6237\uff1b\u91c7\u7528\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6a21\u62df\u5bf9\u6297\u884c\u4e3a\uff0c\u6309\u5f71\u54cd\u529b\u548c\u9884\u7b97\u5206\u7c7b\u8d26\u6237\uff1b\u8bbe\u8ba1\u57fa\u4e8e\u7ed3\u6784\u71b5\u7684\u5206\u5c42\u72b6\u6001\u62bd\u8c61\u52a0\u901f\u5b66\u4e60\u3002", "result": "\u5728\u793e\u4ea4\u673a\u5668\u4eba\u68c0\u6d4b\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u6709\u6548\u524a\u5f31GNN\u68c0\u6d4b\u5668\u7684\u6027\u80fd\u3002", "conclusion": "RoBCtrl\u662f\u9996\u4e2a\u9488\u5bf9GNN\u793e\u4ea4\u673a\u5668\u4eba\u68c0\u6d4b\u5668\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u653b\u51fb\u6846\u67b6\uff0c\u6210\u529f\u5c55\u793a\u4e86\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u7684\u8106\u5f31\u6027\uff0c\u4e3a\u6539\u8fdb\u68c0\u6d4b\u6280\u672f\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2510.16761", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16761", "abs": "https://arxiv.org/abs/2510.16761", "authors": ["Yikai Zhang", "Ye Rong", "Siyu Yuan", "Jiangjie Chen", "Jian Xie", "Yanghua Xiao"], "title": "Enhancing Language Agent Strategic Reasoning through Self-Play in Adversarial Games", "comment": null, "summary": "Existing language agents often encounter difficulties in dynamic adversarial\ngames due to poor strategic reasoning. To mitigate this limitation, a promising\napproach is to allow agents to learn from game interactions automatically,\nwithout relying on costly expert-labeled data. Unlike static environments where\nagents receive fixed feedback or rewards, selecting appropriate opponents in\ndynamic adversarial games can significantly impact learning performance.\nHowever, the discussion of opponents in adversarial environments remains an\narea under exploration. In this paper, we propose a Step-level poliCy\nOptimization method through Play-And-Learn, SCO-PAL. Leveraging SCO-PAL, we\nconduct a detailed analysis of opponent selection by setting opponents at\ndifferent levels and find that self-play is the most effective way to improve\nstrategic reasoning in such adversarial environments. Utilizing SCO-PAL with\nself-play, we increase the average win rate against four opponents by\napproximately 30% compared to baselines and achieve a 54.76% win rate against\nGPT-4 in six adversarial games.", "AI": {"tldr": "\u63d0\u51faSCO-PAL\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u535a\u5f08\u5b66\u4e60\u63d0\u5347\u8bed\u8a00\u667a\u80fd\u4f53\u5728\u52a8\u6001\u5bf9\u6297\u6e38\u620f\u4e2d\u7684\u7b56\u7565\u63a8\u7406\u80fd\u529b\uff0c\u76f8\u6bd4\u57fa\u7ebf\u5e73\u5747\u80dc\u7387\u63d0\u5347\u7ea630%\uff0c\u5bf9\u6297GPT-4\u8fbe\u523054.76%\u80dc\u7387\u3002", "motivation": "\u73b0\u6709\u8bed\u8a00\u667a\u80fd\u4f53\u5728\u52a8\u6001\u5bf9\u6297\u6e38\u620f\u4e2d\u56e0\u7b56\u7565\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u800c\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u65e0\u9700\u4e13\u5bb6\u6807\u6ce8\u6570\u636e\u7684\u81ea\u52a8\u5b66\u4e60\u65b9\u6cd5\u3002\u5bf9\u624b\u9009\u62e9\u5bf9\u5b66\u4e60\u6548\u679c\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u4f46\u76f8\u5173\u7814\u7a76\u8f83\u5c11\u3002", "method": "\u63d0\u51faSCO-PAL\u65b9\u6cd5\uff0c\u901a\u8fc7\u6e38\u620f\u4ea4\u4e92\u81ea\u52a8\u5b66\u4e60\uff0c\u5206\u6790\u4e0d\u540c\u7ea7\u522b\u5bf9\u624b\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u81ea\u535a\u5f08\u662f\u6700\u6709\u6548\u7684\u5b66\u4e60\u65b9\u5f0f\u3002", "result": "\u4f7f\u7528SCO-PAL\u7ed3\u5408\u81ea\u535a\u5f08\uff0c\u5728\u516d\u4e2a\u5bf9\u6297\u6e38\u620f\u4e2d\u5e73\u5747\u80dc\u7387\u6bd4\u57fa\u7ebf\u63d0\u5347\u7ea630%\uff0c\u5bf9\u6297GPT-4\u8fbe\u523054.76%\u80dc\u7387\u3002", "conclusion": "\u81ea\u535a\u5f08\u662f\u63d0\u5347\u5bf9\u6297\u73af\u5883\u4e2d\u7b56\u7565\u63a8\u7406\u80fd\u529b\u7684\u6700\u6709\u6548\u65b9\u5f0f\uff0cSCO-PAL\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u8bed\u8a00\u667a\u80fd\u4f53\u7684\u6e38\u620f\u8868\u73b0\u3002"}}
{"id": "2510.17085", "categories": ["cs.LG", "cs.GT", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.17085", "abs": "https://arxiv.org/abs/2510.17085", "authors": ["Yiling Chen", "Shi Feng", "Paul Kattuman", "Fang-Yi Yu"], "title": "Data Reliability Scoring", "comment": "39 pages, 5 figures", "summary": "How can we assess the reliability of a dataset without access to ground\ntruth? We introduce the problem of reliability scoring for datasets collected\nfrom potentially strategic sources. The true data are unobserved, but we see\noutcomes of an unknown statistical experiment that depends on them. To\nbenchmark reliability, we define ground-truth-based orderings that capture how\nmuch reported data deviate from the truth. We then propose the Gram determinant\nscore, which measures the volume spanned by vectors describing the empirical\ndistribution of the observed data and experiment outcomes. We show that this\nscore preserves several ground-truth based reliability orderings and, uniquely\nup to scaling, yields the same reliability ranking of datasets regardless of\nthe experiment -- a property we term experiment agnosticism. Experiments on\nsynthetic noise models, CIFAR-10 embeddings, and real employment data\ndemonstrate that the Gram determinant score effectively captures data quality\nacross diverse observation processes.", "AI": {"tldr": "\u63d0\u51fa\u4e86Gram\u884c\u5217\u5f0f\u8bc4\u5206\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u4e0d\u4f9d\u8d56\u771f\u5b9e\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u8bc4\u4f30\u6570\u636e\u96c6\u7684\u53ef\u9760\u6027\uff0c\u8be5\u8bc4\u5206\u5177\u6709\u5b9e\u9a8c\u65e0\u5173\u6027\uff0c\u80fd\u6709\u6548\u53cd\u6620\u6570\u636e\u8d28\u91cf\u3002", "motivation": "\u9700\u8981\u5728\u4e0d\u8bbf\u95ee\u771f\u5b9e\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u8bc4\u4f30\u6765\u81ea\u6f5c\u5728\u7b56\u7565\u6027\u6765\u6e90\u7684\u6570\u636e\u96c6\u7684\u53ef\u9760\u6027\uff0c\u56e0\u4e3a\u771f\u5b9e\u6570\u636e\u4e0d\u53ef\u89c2\u6d4b\uff0c\u53ea\u80fd\u770b\u5230\u4f9d\u8d56\u4e8e\u771f\u5b9e\u6570\u636e\u7684\u672a\u77e5\u7edf\u8ba1\u5b9e\u9a8c\u7ed3\u679c\u3002", "method": "\u5b9a\u4e49\u4e86\u57fa\u4e8e\u771f\u5b9e\u6570\u636e\u7684\u53ef\u9760\u6027\u6392\u5e8f\uff0c\u63d0\u51fa\u4e86Gram\u884c\u5217\u5f0f\u8bc4\u5206\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u8ba1\u7b97\u89c2\u6d4b\u6570\u636e\u548c\u5b9e\u9a8c\u7ed3\u679c\u7684\u7ecf\u9a8c\u5206\u5e03\u5411\u91cf\u6240\u5f20\u6210\u7684\u4f53\u79ef\u6765\u8861\u91cf\u53ef\u9760\u6027\u3002", "result": "Gram\u884c\u5217\u5f0f\u8bc4\u5206\u80fd\u591f\u4fdd\u6301\u591a\u4e2a\u57fa\u4e8e\u771f\u5b9e\u6570\u636e\u7684\u53ef\u9760\u6027\u6392\u5e8f\uff0c\u4e14\u5177\u6709\u5b9e\u9a8c\u65e0\u5173\u6027\uff0c\u5728\u4e0d\u540c\u5b9e\u9a8c\u4e0b\u4ea7\u751f\u76f8\u540c\u7684\u53ef\u9760\u6027\u6392\u540d\u3002\u5728\u5408\u6210\u566a\u58f0\u6a21\u578b\u3001CIFAR-10\u5d4c\u5165\u548c\u771f\u5b9e\u5c31\u4e1a\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "Gram\u884c\u5217\u5f0f\u8bc4\u5206\u662f\u4e00\u79cd\u6709\u6548\u7684\u6570\u636e\u96c6\u53ef\u9760\u6027\u8bc4\u4f30\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4e0d\u77e5\u9053\u771f\u5b9e\u6570\u636e\u548c\u5b9e\u9a8c\u8fc7\u7a0b\u7684\u60c5\u51b5\u4e0b\u51c6\u786e\u53cd\u6620\u6570\u636e\u8d28\u91cf\u3002"}}
{"id": "2510.16724", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16724", "abs": "https://arxiv.org/abs/2510.16724", "authors": ["Minhua Lin", "Zongyu Wu", "Zhichao Xu", "Hui Liu", "Xianfeng Tang", "Qi He", "Charu Aggarwal", "Hui Liu", "Xiang Zhang", "Suhang Wang"], "title": "A Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations, Evaluations, and Applications", "comment": "38 pages, 4 figures, 7 tables", "summary": "The advent of large language models (LLMs) has transformed information access\nand reasoning through open-ended natural language interaction. However, LLMs\nremain limited by static knowledge, factual hallucinations, and the inability\nto retrieve real-time or domain-specific information. Retrieval-Augmented\nGeneration (RAG) mitigates these issues by grounding model outputs in external\nevidence, but traditional RAG pipelines are often single turn and heuristic,\nlacking adaptive control over retrieval and reasoning. Recent advances in\nagentic search address these limitations by enabling LLMs to plan, retrieve,\nand reflect through multi-step interaction with search environments. Within\nthis paradigm, reinforcement learning (RL) offers a powerful mechanism for\nadaptive and self-improving search behavior. This survey provides the first\ncomprehensive overview of \\emph{RL-based agentic search}, organizing the\nemerging field along three complementary dimensions: (i) What RL is for\n(functional roles), (ii) How RL is used (optimization strategies), and (iii)\nWhere RL is applied (scope of optimization). We summarize representative\nmethods, evaluation protocols, and applications, and discuss open challenges\nand future directions toward building reliable and scalable RL driven agentic\nsearch systems. We hope this survey will inspire future research on the\nintegration of RL and agentic search. Our repository is available at\nhttps://github.com/ventr1c/Awesome-RL-based-Agentic-Search-Papers.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u5bf9\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u667a\u80fd\u641c\u7d22\u8fdb\u884c\u4e86\u9996\u6b21\u5168\u9762\u7efc\u8ff0\uff0c\u4ece\u529f\u80fd\u89d2\u8272\u3001\u4f18\u5316\u7b56\u7565\u548c\u4f18\u5316\u8303\u56f4\u4e09\u4e2a\u7ef4\u5ea6\u7ec4\u7ec7\u8fd9\u4e00\u65b0\u5174\u9886\u57df\uff0c\u603b\u7ed3\u4e86\u4ee3\u8868\u6027\u65b9\u6cd5\u3001\u8bc4\u4f30\u534f\u8bae\u548c\u5e94\u7528\u3002", "motivation": "\u4f20\u7edfRAG\u7ba1\u9053\u901a\u5e38\u662f\u5355\u8f6e\u548c\u542f\u53d1\u5f0f\u7684\uff0c\u7f3a\u4e4f\u5bf9\u68c0\u7d22\u548c\u63a8\u7406\u7684\u81ea\u9002\u5e94\u63a7\u5236\u3002\u5f3a\u5316\u5b66\u4e60\u4e3a\u667a\u80fd\u641c\u7d22\u63d0\u4f9b\u4e86\u81ea\u9002\u5e94\u548c\u81ea\u6211\u6539\u8fdb\u641c\u7d22\u884c\u4e3a\u7684\u5f3a\u5927\u673a\u5236\u3002", "method": "\u4ece\u4e09\u4e2a\u4e92\u8865\u7ef4\u5ea6\u7ec4\u7ec7\u57fa\u4e8eRL\u7684\u667a\u80fd\u641c\u7d22\u9886\u57df\uff1a(i) RL\u7684\u529f\u80fd\u89d2\u8272\uff0c(ii) RL\u7684\u4f18\u5316\u7b56\u7565\uff0c(iii) RL\u7684\u5e94\u7528\u8303\u56f4\u3002", "result": "\u63d0\u4f9b\u4e86\u8be5\u9886\u57df\u7684\u5168\u9762\u6982\u8ff0\uff0c\u5305\u62ec\u4ee3\u8868\u6027\u65b9\u6cd5\u3001\u8bc4\u4f30\u534f\u8bae\u548c\u5e94\u7528\u6848\u4f8b\uff0c\u5e76\u8ba8\u8bba\u4e86\u5f00\u653e\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002", "conclusion": "\u5e0c\u671b\u8fd9\u9879\u8c03\u67e5\u80fd\u6fc0\u53d1\u672a\u6765\u5173\u4e8eRL\u4e0e\u667a\u80fd\u641c\u7d22\u6574\u5408\u7684\u7814\u7a76\uff0c\u63a8\u52a8\u6784\u5efa\u53ef\u9760\u548c\u53ef\u6269\u5c55\u7684RL\u9a71\u52a8\u667a\u80fd\u641c\u7d22\u7cfb\u7edf\u3002"}}
{"id": "2510.16039", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16039", "abs": "https://arxiv.org/abs/2510.16039", "authors": ["Xiangyuan Peng", "Xingsi Dong", "Si Wu"], "title": "Vector Quantization in the Brain: Grid-like Codes in World Models", "comment": null, "summary": "We propose Grid-like Code Quantization (GCQ), a brain-inspired method for\ncompressing observation-action sequences into discrete representations using\ngrid-like patterns in attractor dynamics. Unlike conventional vector\nquantization approaches that operate on static inputs, GCQ performs\nspatiotemporal compression through an action-conditioned codebook, where\ncodewords are derived from continuous attractor neural networks and dynamically\nselected based on actions. This enables GCQ to jointly compress space and time,\nserving as a unified world model. The resulting representation supports\nlong-horizon prediction, goal-directed planning, and inverse modeling.\nExperiments across diverse tasks demonstrate GCQ's effectiveness in compact\nencoding and downstream performance. Our work offers both a computational tool\nfor efficient sequence modeling and a theoretical perspective on the formation\nof grid-like codes in neural systems.", "AI": {"tldr": "\u63d0\u51faGrid-like Code Quantization (GCQ)\u65b9\u6cd5\uff0c\u4f7f\u7528\u5438\u5f15\u5b50\u52a8\u529b\u5b66\u4e2d\u7684\u7f51\u683c\u72b6\u6a21\u5f0f\u5c06\u89c2\u5bdf-\u52a8\u4f5c\u5e8f\u5217\u538b\u7f29\u4e3a\u79bb\u6563\u8868\u793a\uff0c\u5b9e\u73b0\u65f6\u7a7a\u8054\u5408\u538b\u7f29\u548c\u7edf\u4e00\u4e16\u754c\u5efa\u6a21\u3002", "motivation": "\u4f20\u7edf\u5411\u91cf\u91cf\u5316\u65b9\u6cd5\u5904\u7406\u9759\u6001\u8f93\u5165\uff0c\u800cGCQ\u65e8\u5728\u901a\u8fc7\u52a8\u4f5c\u6761\u4ef6\u7801\u672c\u8fdb\u884c\u65f6\u7a7a\u538b\u7f29\uff0c\u6a21\u62df\u795e\u7ecf\u7cfb\u7edf\u4e2d\u7f51\u683c\u72b6\u4ee3\u7801\u7684\u5f62\u6210\u8fc7\u7a0b\u3002", "method": "\u4f7f\u7528\u8fde\u7eed\u5438\u5f15\u5b50\u795e\u7ecf\u7f51\u7edc\u63a8\u5bfc\u7801\u5b57\uff0c\u57fa\u4e8e\u52a8\u4f5c\u52a8\u6001\u9009\u62e9\u7801\u5b57\uff0c\u6784\u5efa\u52a8\u4f5c\u6761\u4ef6\u7801\u672c\u8fdb\u884c\u65f6\u7a7a\u8054\u5408\u538b\u7f29\u3002", "result": "\u5b9e\u9a8c\u8868\u660eGCQ\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u6709\u6548\u5b9e\u73b0\u7d27\u51d1\u7f16\u7801\u548c\u4e0b\u6e38\u6027\u80fd\uff0c\u652f\u6301\u957f\u65f6\u7a0b\u9884\u6d4b\u3001\u76ee\u6807\u5bfc\u5411\u89c4\u5212\u548c\u9006\u5411\u5efa\u6a21\u3002", "conclusion": "GCQ\u65e2\u4e3a\u9ad8\u6548\u5e8f\u5217\u5efa\u6a21\u63d0\u4f9b\u8ba1\u7b97\u5de5\u5177\uff0c\u4e5f\u4e3a\u795e\u7ecf\u7cfb\u7edf\u4e2d\u7f51\u683c\u72b6\u4ee3\u7801\u5f62\u6210\u63d0\u4f9b\u7406\u8bba\u89c6\u89d2\u3002"}}
{"id": "2510.16783", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16783", "abs": "https://arxiv.org/abs/2510.16783", "authors": ["Sheikh Jubair", "Arwa Omayrah", "Amal Alshammari", "Alhanoof Althnian", "Abdulhamed Alothaimen", "Norah A. Alzahrani", "Shahad D. Alzaidi", "Nora Al-Twairesh", "Abdulmohsen Al-Thubaity"], "title": "LC-Eval: A Bilingual Multi-Task Evaluation Benchmark for Long-Context Understanding", "comment": "1 figure, 15 tables, 10 main pages", "summary": "Recent advancements in Large Language Models (LLMs) have demonstrated\nsophisticated capabilities, including the ability to process and comprehend\nextended contexts. These emergent capabilities necessitate rigorous evaluation\nmethods to effectively assess their performance in long-context understanding.\nIn this paper, we present \\textbf{LC-Eval}, a bilingual, multi-task evaluation\nbenchmark designed to evaluate long-context understanding in English and\nArabic, targeting context lengths ranging from 4k to over 128k tokens. LC-Eval\nintroduces four novel and challenging tasks: multi-document question answering,\nbilingual question answering, claim verification within a paragraph, and\nmultiple-choice questions based on long contexts. These tasks are designed to\nassess LLMs' abilities in deep reasoning, document comprehension, information\ntracing, and bilingual information extraction and understanding. The benchmark\nincludes datasets in both Arabic and English for each task, allowing for a\ncomparative analysis of their performance across different text genres.\nEvaluations were conducted on both open-weight and closed LLMs, with results\nindicating that LC-Eval presents significant challenges. Even high-performing\nmodels, such as GPT-4o, struggled with certain tasks, highlighting the\ncomplexity and rigor of the benchmark.", "AI": {"tldr": "LC-Eval\u662f\u4e00\u4e2a\u53cc\u8bed\u591a\u4efb\u52a1\u8bc4\u4f30\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u82f1\u8bed\u548c\u963f\u62c9\u4f2f\u8bed\u4e2d\u7684\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\uff0c\u6db5\u76d64k\u5230128k+token\u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\u3002", "motivation": "\u968f\u7740LLM\u5728\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u80fd\u529b\u4e0a\u7684\u8fdb\u6b65\uff0c\u9700\u8981\u4e25\u683c\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u6709\u6548\u8bc4\u4f30\u5176\u5728\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u65b9\u9762\u7684\u6027\u80fd\u3002", "method": "\u8bbe\u8ba1\u4e86\u56db\u4e2a\u65b0\u9896\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff1a\u591a\u6587\u6863\u95ee\u7b54\u3001\u53cc\u8bed\u95ee\u7b54\u3001\u6bb5\u843d\u5185\u58f0\u660e\u9a8c\u8bc1\u548c\u57fa\u4e8e\u957f\u4e0a\u4e0b\u6587\u7684\u591a\u9879\u9009\u62e9\u9898\uff0c\u8bc4\u4f30LLM\u7684\u6df1\u5ea6\u63a8\u7406\u3001\u6587\u6863\u7406\u89e3\u3001\u4fe1\u606f\u8ffd\u8e2a\u548c\u53cc\u8bed\u4fe1\u606f\u63d0\u53d6\u80fd\u529b\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793aLC-Eval\u5177\u6709\u663e\u8457\u6311\u6218\u6027\uff0c\u5373\u4f7f\u662fGPT-4o\u7b49\u9ad8\u6027\u80fd\u6a21\u578b\u5728\u67d0\u4e9b\u4efb\u52a1\u4e0a\u4e5f\u8868\u73b0\u56f0\u96be\uff0c\u7a81\u663e\u4e86\u57fa\u51c6\u7684\u590d\u6742\u6027\u548c\u4e25\u8c28\u6027\u3002", "conclusion": "LC-Eval\u4e3a\u8bc4\u4f30LLM\u7684\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u8fd9\u4e00\u9886\u57df\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.17103", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.17103", "abs": "https://arxiv.org/abs/2510.17103", "authors": ["Shinji Ito", "Kevin Jamieson", "Haipeng Luo", "Arnab Maiti", "Taira Tsuchiya"], "title": "Adapting to Stochastic and Adversarial Losses in Episodic MDPs with Aggregate Bandit Feedback", "comment": "49 pages", "summary": "We study online learning in finite-horizon episodic Markov decision processes\n(MDPs) under the challenging aggregate bandit feedback model, where the learner\nobserves only the cumulative loss incurred in each episode, rather than\nindividual losses at each state-action pair. While prior work in this setting\nhas focused exclusively on worst-case analysis, we initiate the study of\nbest-of-both-worlds (BOBW) algorithms that achieve low regret in both\nstochastic and adversarial environments. We propose the first BOBW algorithms\nfor episodic tabular MDPs with aggregate bandit feedback. In the case of known\ntransitions, our algorithms achieve $O(\\log T)$ regret in stochastic settings\nand ${O}(\\sqrt{T})$ regret in adversarial ones. Importantly, we also establish\nmatching lower bounds, showing the optimality of our algorithms in this\nsetting. We further extend our approach to unknown-transition settings by\nincorporating confidence-based techniques. Our results rely on a combination of\nFTRL over occupancy measures, self-bounding techniques, and new loss estimators\ninspired by recent advances in online shortest path problems. Along the way, we\nalso provide the first individual-gap-dependent lower bounds and demonstrate\nnear-optimal BOBW algorithms for shortest path problems with bandit feedback.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u9488\u5bf9\u5177\u6709\u805a\u5408bandit\u53cd\u9988\u7684\u8868\u683cMDP\u7684BOBW\u7b97\u6cd5\uff0c\u5728\u5df2\u77e5\u8f6c\u79fb\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86O(log T)\u7684\u968f\u673a\u9057\u61be\u548cO(\u221aT)\u7684\u5bf9\u6297\u9057\u61be\uff0c\u5e76\u5efa\u7acb\u4e86\u5339\u914d\u4e0b\u754c\u8bc1\u660e\u6700\u4f18\u6027\u3002", "motivation": "\u7814\u7a76\u5728\u805a\u5408bandit\u53cd\u9988\u6a21\u578b\u4e0b\u7684\u5728\u7ebf\u5b66\u4e60\u95ee\u9898\uff0c\u8be5\u6a21\u578b\u4e0b\u5b66\u4e60\u8005\u53ea\u80fd\u89c2\u5bdf\u5230\u6bcf\u4e2aepisode\u7684\u7d2f\u79ef\u635f\u5931\u800c\u975e\u5355\u4e2a\u72b6\u6001-\u52a8\u4f5c\u5bf9\u7684\u635f\u5931\uff0c\u4e14\u73b0\u6709\u5de5\u4f5c\u4ec5\u5173\u6ce8\u6700\u574f\u60c5\u51b5\u5206\u6790\u3002", "method": "\u7ed3\u5408\u4e86\u57fa\u4e8e\u5360\u7528\u5ea6\u91cf\u7684FTRL\u3001\u81ea\u8fb9\u754c\u6280\u672f\u548c\u53d7\u5728\u7ebf\u6700\u77ed\u8def\u5f84\u95ee\u9898\u542f\u53d1\u7684\u65b0\u635f\u5931\u4f30\u8ba1\u5668\uff0c\u5e76\u6269\u5c55\u5230\u672a\u77e5\u8f6c\u79fb\u8bbe\u7f6e\u4e2d\u91c7\u7528\u7f6e\u4fe1\u5ea6\u6280\u672f\u3002", "result": "\u5728\u5df2\u77e5\u8f6c\u79fb\u60c5\u51b5\u4e0b\uff0c\u7b97\u6cd5\u5728\u968f\u673a\u73af\u5883\u4e2d\u8fbe\u5230O(log T)\u9057\u61be\uff0c\u5728\u5bf9\u6297\u73af\u5883\u4e2d\u8fbe\u5230O(\u221aT)\u9057\u61be\uff0c\u5e76\u5efa\u7acb\u4e86\u5339\u914d\u4e0b\u754c\u8bc1\u660e\u6700\u4f18\u6027\u3002", "conclusion": "\u6210\u529f\u8bbe\u8ba1\u4e86\u9996\u4e2a\u9488\u5bf9\u805a\u5408bandit\u53cd\u9988MDP\u7684BOBW\u7b97\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u6700\u4f18\u6027\uff0c\u5e76\u4e3a\u6700\u77ed\u8def\u5f84\u95ee\u9898\u63d0\u4f9b\u4e86\u9996\u4e2a\u4e2a\u4f53\u95f4\u9699\u76f8\u5173\u4e0b\u754c\u548c\u8fd1\u6700\u4f18BOBW\u7b97\u6cd5\u3002"}}
{"id": "2510.16742", "categories": ["cs.AI", "cs.MA", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.16742", "abs": "https://arxiv.org/abs/2510.16742", "authors": ["Paul Saves", "Pramudita Satria Palar", "Muhammad Daffa Robani", "Nicolas Verstaevel", "Moncef Garouani", "Julien Aligon", "Benoit Gaudou", "Koji Shimoyama", "Joseph Morlier"], "title": "Surrogate Modeling and Explainable Artificial Intelligence for Complex Systems: A Workflow for Automated Simulation Exploration", "comment": null, "summary": "Complex systems are increasingly explored through simulation-driven\nengineering workflows that combine physics-based and empirical models with\noptimization and analytics. Despite their power, these workflows face two\ncentral obstacles: (1) high computational cost, since accurate exploration\nrequires many expensive simulator runs; and (2) limited transparency and\nreliability when decisions rely on opaque blackbox components. We propose a\nworkflow that addresses both challenges by training lightweight emulators on\ncompact designs of experiments that (i) provide fast, low-latency\napproximations of expensive simulators, (ii) enable rigorous uncertainty\nquantification, and (iii) are adapted for global and local Explainable\nArtificial Intelligence (XAI) analyses. This workflow unifies every\nsimulation-based complex-system analysis tool, ranging from engineering design\nto agent-based models for socio-environmental understanding. In this paper, we\nproposea comparative methodology and practical recommendations for using\nsurrogate-based explainability tools within the proposed workflow. The\nmethodology supports continuous and categorical inputs, combines global-effect\nand uncertainty analyses with local attribution, and evaluates the consistency\nof explanations across surrogate models, thereby diagnosing surrogate adequacy\nand guiding further data collection or model refinement. We demonstrate the\napproach on two contrasting case studies: a multidisciplinary design analysis\nof a hybrid-electric aircraft and an agent-based model of urban segregation.\nResults show that the surrogate model and XAI coupling enables large-scale\nexploration in seconds, uncovers nonlinear interactions and emergent behaviors,\nidentifies key design and policy levers, and signals regions where surrogates\nrequire more data or alternative architectures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ee3\u7406\u6a21\u578b\u7684\u4eff\u771f\u9a71\u52a8\u5de5\u7a0b\u5de5\u4f5c\u6d41\uff0c\u901a\u8fc7\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u4eff\u771f\u5668\u6765\u89e3\u51b3\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u9ed1\u76d2\u900f\u660e\u5ea6\u95ee\u9898\uff0c\u652f\u6301\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u548c\u53ef\u89e3\u91caAI\u5206\u6790\u3002", "motivation": "\u4eff\u771f\u9a71\u52a8\u5de5\u7a0b\u5de5\u4f5c\u6d41\u9762\u4e34\u4e24\u4e2a\u6838\u5fc3\u969c\u788d\uff1a(1)\u9ad8\u8ba1\u7b97\u6210\u672c\uff0c(2)\u9ed1\u76d2\u7ec4\u4ef6\u5bfc\u81f4\u7684\u900f\u660e\u5ea6\u548c\u53ef\u9760\u6027\u9650\u5236\u3002", "method": "\u5728\u7d27\u51d1\u5b9e\u9a8c\u8bbe\u8ba1\u4e0a\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u4eff\u771f\u5668\uff0c\u7ed3\u5408\u5168\u5c40\u6548\u5e94\u5206\u6790\u3001\u4e0d\u786e\u5b9a\u6027\u5206\u6790\u548c\u5c40\u90e8\u5f52\u56e0\uff0c\u8bc4\u4f30\u4e0d\u540c\u4ee3\u7406\u6a21\u578b\u95f4\u89e3\u91ca\u7684\u4e00\u81f4\u6027\u3002", "result": "\u5728\u6df7\u5408\u7535\u52a8\u98de\u673a\u591a\u5b66\u79d1\u8bbe\u8ba1\u548c\u57ce\u5e02\u9694\u79bb\u57fa\u4e8e\u4ee3\u7406\u6a21\u578b\u4e24\u4e2a\u6848\u4f8b\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u79d2\u7ea7\u5927\u89c4\u6a21\u63a2\u7d22\uff0c\u63ed\u793a\u4e86\u975e\u7ebf\u6027\u76f8\u4e92\u4f5c\u7528\u548c\u6d8c\u73b0\u884c\u4e3a\uff0c\u8bc6\u522b\u4e86\u5173\u952e\u8bbe\u8ba1\u548c\u653f\u7b56\u6760\u6746\u3002", "conclusion": "\u4ee3\u7406\u6a21\u578b\u4e0e\u53ef\u89e3\u91caAI\u7684\u8026\u5408\u80fd\u591f\u5feb\u901f\u63a2\u7d22\u590d\u6742\u7cfb\u7edf\uff0c\u53d1\u73b0\u5173\u952e\u56e0\u7d20\uff0c\u5e76\u6307\u5bfc\u8fdb\u4e00\u6b65\u6570\u636e\u6536\u96c6\u6216\u6a21\u578b\u6539\u8fdb\u3002"}}
{"id": "2510.17503", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.17503", "abs": "https://arxiv.org/abs/2510.17503", "authors": ["El Mahdi Chayti", "Martin Jaggi"], "title": "Stochastic Difference-of-Convex Optimization with Momentum", "comment": null, "summary": "Stochastic difference-of-convex (DC) optimization is prevalent in numerous\nmachine learning applications, yet its convergence properties under small batch\nsizes remain poorly understood. Existing methods typically require large\nbatches or strong noise assumptions, which limit their practical use. In this\nwork, we show that momentum enables convergence under standard smoothness and\nbounded variance assumptions (of the concave part) for any batch size. We prove\nthat without momentum, convergence may fail regardless of stepsize,\nhighlighting its necessity. Our momentum-based algorithm achieves provable\nconvergence and demonstrates strong empirical performance.", "AI": {"tldr": "\u52a8\u91cf\u65b9\u6cd5\u4f7f\u968f\u673aDC\u4f18\u5316\u5728\u5c0f\u6279\u91cf\u4e0b\u6536\u655b\uff0c\u65e0\u9700\u5927\u6279\u6b21\u6216\u5f3a\u566a\u58f0\u5047\u8bbe", "motivation": "\u73b0\u6709\u968f\u673aDC\u4f18\u5316\u65b9\u6cd5\u9700\u8981\u5927\u6279\u6b21\u6216\u5f3a\u566a\u58f0\u5047\u8bbe\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\uff0c\u5c0f\u6279\u91cf\u4e0b\u7684\u6536\u655b\u6027\u8d28\u5c1a\u4e0d\u660e\u786e", "method": "\u63d0\u51fa\u57fa\u4e8e\u52a8\u91cf\u7684\u7b97\u6cd5\uff0c\u5728\u6807\u51c6\u5149\u6ed1\u6027\u548c\u6709\u754c\u65b9\u5dee\u5047\u8bbe\u4e0b\u5b9e\u73b0\u6536\u655b", "result": "\u8bc1\u660e\u65e0\u52a8\u91cf\u65f6\u65e0\u8bba\u6b65\u957f\u5982\u4f55\u90fd\u53ef\u80fd\u4e0d\u6536\u655b\uff0c\u52a8\u91cf\u65b9\u6cd5\u5177\u6709\u53ef\u8bc1\u660e\u7684\u6536\u655b\u6027\u548c\u5f3a\u5b9e\u8bc1\u6027\u80fd", "conclusion": "\u52a8\u91cf\u662f\u5c0f\u6279\u91cf\u968f\u673aDC\u4f18\u5316\u7684\u5173\u952e\u8981\u7d20\uff0c\u80fd\u591f\u5b9e\u73b0\u7406\u8bba\u6536\u655b\u4fdd\u8bc1\u548c\u5b9e\u9645\u6027\u80fd"}}
{"id": "2510.16045", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16045", "abs": "https://arxiv.org/abs/2510.16045", "authors": ["Mengtao Lv", "Ruiqi Zhu", "Xinyu Wang", "Yun Li"], "title": "AMS-QUANT: Adaptive Mantissa Sharing for Floating-point Quantization", "comment": "12 pages, 6 figures", "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nvarious kinds of tasks, while the billion or even trillion parameters bring\nstorage and efficiency bottlenecks for inference. Quantization, particularly\nfloating-point quantization, is known to be capable of speeding up LLM\ninference by reducing memory footprint and data movement during the inference\nprocess. For the first time, we advance the floating-point quantization\nexploration from integer bitwidths to non-integer bit-widths, namely AMS-Quant,\nto further approach the quantization sweet spot. AMS-Quant incorporates two\nnovel techniques to put it into effect: (1) it proposes Mantissa-bit Sharing,\nwhich groups k quantized weights and lets them share the least significant\nmantissa bit, allowing us to further approach the minimum quantization\nbit-width without accuracy loss. (2) It introduces Adaptive Searching, which\nemploys an offline optimization strategy to minimize the accuracy degradation\nintroduced by sharing. Moreover, AMS-Quant is also prototyped as efficient CUDA\nLinear kernels, which translates memory savings into wall-clock latency\nreduction by reducing memory access. Extensive experiments on large-scale\ndatasets and models show that AMS-Quant can quantize the model to FP-5.33-e2m3\nand FP4.25-e2m2, and significantly speed up the LLM decoding over FP16\ninference (2.8x and 3.2x), with negligible accuracy loss.", "AI": {"tldr": "AMS-Quant\u662f\u4e00\u79cd\u521b\u65b0\u7684\u6d6e\u70b9\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u975e\u6574\u6570\u4f4d\u5bbd\u548c\u5c3e\u6570\u4f4d\u5171\u4eab\u6280\u672f\uff0c\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u91cf\u5316\u5230FP5.33\u548cFP4.25\uff0c\u5b9e\u73b02.8-3.2\u500d\u63a8\u7406\u52a0\u901f\uff0c\u4e14\u7cbe\u5ea6\u635f\u5931\u53ef\u5ffd\u7565\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5de8\u5927\u53c2\u6570\u91cf\u5e26\u6765\u4e86\u5b58\u50a8\u548c\u63a8\u7406\u6548\u7387\u74f6\u9888\uff0c\u6d6e\u70b9\u91cf\u5316\u867d\u7136\u80fd\u52a0\u901f\u63a8\u7406\uff0c\u4f46\u4f20\u7edf\u6574\u6570\u4f4d\u5bbd\u9650\u5236\u4e86\u91cf\u5316\u6548\u679c\u7684\u8fdb\u4e00\u6b65\u63d0\u5347\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u65b0\u6280\u672f\uff1a1) \u5c3e\u6570\u4f4d\u5171\u4eab - \u5c06k\u4e2a\u91cf\u5316\u6743\u91cd\u5206\u7ec4\u5171\u4eab\u6700\u4f4e\u6709\u6548\u5c3e\u6570\u4f4d\uff1b2) \u81ea\u9002\u5e94\u641c\u7d22 - \u4f7f\u7528\u79bb\u7ebf\u4f18\u5316\u7b56\u7565\u6700\u5c0f\u5316\u5171\u4eab\u5e26\u6765\u7684\u7cbe\u5ea6\u635f\u5931\u3002", "result": "\u5728\u5927\u578b\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAMS-Quant\u53ef\u5c06\u6a21\u578b\u91cf\u5316\u5230FP5.33-e2m3\u548cFP4.25-e2m2\uff0c\u76f8\u6bd4FP16\u63a8\u7406\u5206\u522b\u5b9e\u73b02.8\u500d\u548c3.2\u500d\u7684\u89e3\u7801\u52a0\u901f\uff0c\u7cbe\u5ea6\u635f\u5931\u53ef\u5ffd\u7565\u3002", "conclusion": "AMS-Quant\u9996\u6b21\u5c06\u6d6e\u70b9\u91cf\u5316\u4ece\u6574\u6570\u4f4d\u5bbd\u6269\u5c55\u5230\u975e\u6574\u6570\u4f4d\u5bbd\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u5c3e\u6570\u4f4d\u5171\u4eab\u548c\u81ea\u9002\u5e94\u641c\u7d22\u6280\u672f\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347LLM\u63a8\u7406\u6548\u7387\u3002"}}
{"id": "2510.16797", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16797", "abs": "https://arxiv.org/abs/2510.16797", "authors": ["Vera Pavlova", "Mohammed Makhlouf"], "title": "MOSAIC: Masked Objective with Selective Adaptation for In-domain Contrastive Learning", "comment": null, "summary": "We introduce MOSAIC (Masked Objective with Selective Adaptation for In-domain\nContrastive learning), a multi-stage framework for domain adaptation of\nsentence embedding models that incorporates joint domain-specific masked\nsupervision. Our approach addresses the challenges of adapting large-scale\ngeneral-domain sentence embedding models to specialized domains. By jointly\noptimizing masked language modeling (MLM) and contrastive objectives within a\nunified training pipeline, our method enables effective learning of\ndomain-relevant representations while preserving the robust semantic\ndiscrimination properties of the original model. We empirically validate our\napproach on both high-resource and low-resource domains, achieving improvements\nup to 13.4% in NDCG@10 (Normalized Discounted Cumulative Gain) over strong\ngeneral-domain baselines. Comprehensive ablation studies further demonstrate\nthe effectiveness of each component, highlighting the importance of balanced\njoint supervision and staged adaptation.", "AI": {"tldr": "MOSAIC\u662f\u4e00\u4e2a\u591a\u9636\u6bb5\u9886\u57df\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u63a9\u7801\u8bed\u8a00\u5efa\u6a21\u548c\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\uff0c\u5c06\u901a\u7528\u53e5\u5b50\u5d4c\u5165\u6a21\u578b\u6709\u6548\u9002\u914d\u5230\u7279\u5b9a\u9886\u57df\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u901a\u7528\u9886\u57df\u53e5\u5b50\u5d4c\u5165\u6a21\u578b\u5728\u4e13\u4e1a\u9886\u57df\u9002\u5e94\u4e2d\u7684\u6311\u6218\uff0c\u9700\u8981\u5b66\u4e60\u9886\u57df\u76f8\u5173\u8868\u793a\u540c\u65f6\u4fdd\u6301\u539f\u59cb\u6a21\u578b\u7684\u8bed\u4e49\u533a\u5206\u80fd\u529b\u3002", "method": "\u91c7\u7528\u591a\u9636\u6bb5\u6846\u67b6\uff0c\u7ed3\u5408\u9886\u57df\u7279\u5b9a\u7684\u63a9\u7801\u76d1\u7763\uff0c\u8054\u5408\u4f18\u5316MLM\u548c\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\uff0c\u5b9e\u73b0\u7edf\u4e00\u8bad\u7ec3\u6d41\u7a0b\u3002", "result": "\u5728\u9ad8\u8d44\u6e90\u548c\u4f4e\u8d44\u6e90\u9886\u57df\u5747\u53d6\u5f97\u663e\u8457\u6539\u8fdb\uff0cNDCG@10\u6307\u6807\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u63d0\u5347\u9ad8\u8fbe13.4%\u3002", "conclusion": "\u5e73\u8861\u7684\u8054\u5408\u76d1\u7763\u548c\u5206\u9636\u6bb5\u9002\u5e94\u7b56\u7565\u5bf9\u9886\u57df\u81ea\u9002\u5e94\u81f3\u5173\u91cd\u8981\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5b66\u4e60\u9886\u57df\u76f8\u5173\u8868\u793a\u5e76\u4fdd\u6301\u8bed\u4e49\u533a\u5206\u80fd\u529b\u3002"}}
{"id": "2510.17120", "categories": ["cs.LG", "cs.CV", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.17120", "abs": "https://arxiv.org/abs/2510.17120", "authors": ["Rishi Sonthalia", "Raj Rao Nadakuditi"], "title": "Matricial Free Energy as a Gaussianizing Regularizer: Enhancing Autoencoders for Gaussian Code Generation", "comment": null, "summary": "We introduce a novel regularization scheme for autoencoders based on\nmatricial free energy. Our approach defines a differentiable loss function in\nterms of the singular values of the code matrix (code dimension x batch size).\nFrom the standpoint of free probability an d random matrix theory, this loss\nachieves its minimum when the singular value distribution of the code matrix\ncoincides with that of an appropriately sculpted random metric with i.i.d.\nGaussian entries. Empirical simulations demonstrate that minimizing the\nnegative matricial free energy through standard stochastic gradient-based\ntraining yields Gaussian-like codes that generalize across training and test\nsets. Building on this foundation, we propose a matricidal free energy\nmaximizing autoencoder that reliably produces Gaussian codes and show its\napplication to underdetermined inverse problems.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u77e9\u9635\u81ea\u7531\u80fd\u7684\u81ea\u52a8\u7f16\u7801\u5668\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u7f16\u7801\u77e9\u9635\u7684\u5947\u5f02\u503c\u5206\u5e03\u4f7f\u5176\u63a5\u8fd1\u9ad8\u65af\u5206\u5e03\uff0c\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u5e76\u5e94\u7528\u4e8e\u6b20\u5b9a\u9006\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u81ea\u52a8\u7f16\u7801\u5668\u7f3a\u4e4f\u5bf9\u7f16\u7801\u5206\u5e03\u7684\u6b63\u5219\u5316\u7ea6\u675f\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u77e9\u9635\u81ea\u7531\u80fd\u7406\u8bba\u786e\u4fdd\u7f16\u7801\u5177\u6709\u9ad8\u65af\u5206\u5e03\u7279\u6027\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u6027\u80fd\u3002", "method": "\u5b9a\u4e49\u57fa\u4e8e\u7f16\u7801\u77e9\u9635\u5947\u5f02\u503c\u7684\u53ef\u5fae\u635f\u5931\u51fd\u6570\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u8d1f\u77e9\u9635\u81ea\u7531\u80fd\u4f7f\u7f16\u7801\u77e9\u9635\u7684\u5947\u5f02\u503c\u5206\u5e03\u4e0e\u9ad8\u65af\u968f\u673a\u77e9\u9635\u4e00\u81f4\uff0c\u4f7f\u7528\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u4ea7\u751f\u9ad8\u65af\u5316\u7f16\u7801\uff0c\u5728\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\u4e0a\u90fd\u5177\u6709\u826f\u597d\u6cdb\u5316\u6027\u80fd\uff0c\u5e76\u80fd\u6709\u6548\u5e94\u7528\u4e8e\u6b20\u5b9a\u9006\u95ee\u9898\u6c42\u89e3\u3002", "conclusion": "\u77e9\u9635\u81ea\u7531\u80fd\u6b63\u5219\u5316\u662f\u4e00\u79cd\u6709\u6548\u7684\u81ea\u52a8\u7f16\u7801\u5668\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u80fd\u591f\u5f3a\u5236\u7f16\u7801\u5177\u6709\u9ad8\u65af\u5206\u5e03\u7279\u6027\uff0c\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u5e76\u5728\u9006\u95ee\u9898\u4e2d\u8868\u73b0\u826f\u597d\u3002"}}
{"id": "2510.16753", "categories": ["cs.AI", "68T30", "H.3.3"], "pdf": "https://arxiv.org/pdf/2510.16753", "abs": "https://arxiv.org/abs/2510.16753", "authors": ["Wei Huang", "Peining Li", "Meiyu Liang", "Xu Hou", "Junping Du", "Yingxia Shao", "Guanhua Ye", "Wu Liu", "Kangkang Lu", "Yang Yu"], "title": "ELMM: Efficient Lightweight Multimodal Large Language Models for Multimodal Knowledge Graph Completion", "comment": "11 pages, 4 figures", "summary": "Multimodal Knowledge Graphs (MKGs) extend traditional knowledge graphs by\nincorporating visual and textual modalities, enabling richer and more\nexpressive entity representations. However, existing MKGs often suffer from\nincompleteness, which hinder their effectiveness in downstream tasks.\nTherefore, multimodal knowledge graph completion (MKGC) task is receiving\nincreasing attention. While large language models (LLMs) have shown promise for\nknowledge graph completion (KGC), their application to the multimodal setting\nremains underexplored. Moreover, applying Multimodal Large Language Models\n(MLLMs) to the task of MKGC introduces significant challenges: (1) the large\nnumber of image tokens per entity leads to semantic noise and modality\nconflicts, and (2) the high computational cost of processing large token\ninputs. To address these issues, we propose Efficient Lightweight Multimodal\nLarge Language Models (ELMM) for MKGC. ELMM proposes a Multi-view Visual Token\nCompressor (MVTC) based on multi-head attention mechanism, which adaptively\ncompresses image tokens from both textual and visual views, thereby effectively\nreducing redundancy while retaining necessary information and avoiding modality\nconflicts. Additionally, we design an attention pruning strategy to remove\nredundant attention layers from MLLMs, thereby significantly reducing the\ninference cost. We further introduce a linear projection to compensate for the\nperformance degradation caused by pruning. Extensive experiments on benchmark\nFB15k-237-IMG and WN18-IMG demonstrate that ELMM achieves state-of-the-art\nperformance while substantially improving computational efficiency,\nestablishing a new paradigm for multimodal knowledge graph completion.", "AI": {"tldr": "\u63d0\u51fa\u4e86ELMM\u6a21\u578b\u7528\u4e8e\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\uff0c\u901a\u8fc7\u591a\u89c6\u56fe\u89c6\u89c9\u6807\u8bb0\u538b\u7f29\u5668\u548c\u6ce8\u610f\u529b\u526a\u679d\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u5b58\u5728\u4e0d\u5b8c\u6574\u6027\u95ee\u9898\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u591a\u6a21\u6001\u4fe1\u606f\u65f6\u9762\u4e34\u8bed\u4e49\u566a\u58f0\u3001\u6a21\u6001\u51b2\u7a81\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\u7b49\u6311\u6218\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u591a\u5934\u6ce8\u610f\u529b\u7684\u591a\u89c6\u56fe\u89c6\u89c9\u6807\u8bb0\u538b\u7f29\u5668\u81ea\u9002\u5e94\u538b\u7f29\u56fe\u50cf\u6807\u8bb0\uff0c\u8bbe\u8ba1\u6ce8\u610f\u529b\u526a\u679d\u7b56\u7565\u51cf\u5c11\u5197\u4f59\u5c42\uff0c\u5e76\u901a\u8fc7\u7ebf\u6027\u6295\u5f71\u8865\u507f\u6027\u80fd\u635f\u5931\u3002", "result": "\u5728FB15k-237-IMG\u548cWN18-IMG\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u540c\u65f6\u5927\u5e45\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "ELMM\u4e3a\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u5efa\u7acb\u4e86\u65b0\u7684\u8303\u5f0f\uff0c\u5728\u6027\u80fd\u548c\u6548\u7387\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.17506", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.17506", "abs": "https://arxiv.org/abs/2510.17506", "authors": ["Lachlan Ewen MacDonald", "Hancheng Min", "Leandro Palma", "Salma Tarmoun", "Ziqing Xu", "Ren\u00e9 Vidal"], "title": "Convergence Rates for Gradient Descent on the Edge of Stability in Overparametrised Least Squares", "comment": "NeurIPS2025. Code available at\n  https://github.com/lemacdonald/eos-convergence-rates-codimension-1", "summary": "Classical optimisation theory guarantees monotonic objective decrease for\ngradient descent (GD) when employed in a small step size, or ``stable\", regime.\nIn contrast, gradient descent on neural networks is frequently performed in a\nlarge step size regime called the ``edge of stability\", in which the objective\ndecreases non-monotonically with an observed implicit bias towards flat minima.\nIn this paper, we take a step toward quantifying this phenomenon by providing\nconvergence rates for gradient descent with large learning rates in an\noverparametrised least squares setting. The key insight behind our analysis is\nthat, as a consequence of overparametrisation, the set of global minimisers\nforms a Riemannian manifold $M$, which enables the decomposition of the GD\ndynamics into components parallel and orthogonal to $M$. The parallel component\ncorresponds to Riemannian gradient descent on the objective sharpness, while\nthe orthogonal component is a bifurcating dynamical system. This insight allows\nus to derive convergence rates in three regimes characterised by the learning\nrate size: (a) the subcritical regime, in which transient instability is\novercome in finite time before linear convergence to a suboptimally flat global\nminimum; (b) the critical regime, in which instability persists for all time\nwith a power-law convergence toward the optimally flat global minimum; and (c)\nthe supercritical regime, in which instability persists for all time with\nlinear convergence to an orbit of period two centred on the optimally flat\nglobal minimum.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86\u8fc7\u53c2\u6570\u5316\u6700\u5c0f\u4e8c\u4e58\u95ee\u9898\u4e2d\u68af\u5ea6\u4e0b\u964d\u5728\u5927\u5b66\u4e60\u7387\u4e0b\u7684\u6536\u655b\u884c\u4e3a\uff0c\u63ed\u793a\u4e86\u4e09\u79cd\u4e0d\u540c\u5b66\u4e60\u7387\u533a\u95f4\u7684\u6536\u655b\u7279\u6027\uff1a\u4e9a\u4e34\u754c\u3001\u4e34\u754c\u548c\u8d85\u4e34\u754c\u72b6\u6001\u3002", "motivation": "\u4f20\u7edf\u4f18\u5316\u7406\u8bba\u53ea\u4fdd\u8bc1\u68af\u5ea6\u4e0b\u964d\u5728\u5c0f\u5b66\u4e60\u7387\u4e0b\u7684\u5355\u8c03\u6536\u655b\uff0c\u4f46\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u4e2d\u5e38\u4f7f\u7528\u5927\u5b66\u4e60\u7387\uff08\u8fb9\u7f18\u7a33\u5b9a\u6027\u72b6\u6001\uff09\uff0c\u6b64\u65f6\u76ee\u6807\u51fd\u6570\u975e\u5355\u8c03\u4e0b\u964d\u4e14\u504f\u5411\u5e73\u5766\u6700\u5c0f\u503c\u3002\u672c\u6587\u65e8\u5728\u91cf\u5316\u8fd9\u4e00\u73b0\u8c61\u3002", "method": "\u5229\u7528\u8fc7\u53c2\u6570\u5316\u4f7f\u5168\u5c40\u6700\u5c0f\u503c\u5f62\u6210\u9ece\u66fc\u6d41\u5f62M\uff0c\u5c06GD\u52a8\u6001\u5206\u89e3\u4e3a\u5e73\u884c\u548c\u6b63\u4ea4\u4e8eM\u7684\u5206\u91cf\u3002\u5e73\u884c\u5206\u91cf\u5bf9\u5e94\u9ece\u66fc\u68af\u5ea6\u4e0b\u964d\uff0c\u6b63\u4ea4\u5206\u91cf\u662f\u5206\u53c9\u52a8\u529b\u7cfb\u7edf\u3002", "result": "\u63a8\u5bfc\u51fa\u4e09\u79cd\u5b66\u4e60\u7387\u533a\u95f4\u7684\u6536\u655b\u7387\uff1a\u4e9a\u4e34\u754c\u72b6\u6001\uff08\u6709\u9650\u65f6\u95f4\u5185\u514b\u670d\u77ac\u65f6\u4e0d\u7a33\u5b9a\u6027\uff0c\u7ebf\u6027\u6536\u655b\u81f3\u6b21\u4f18\u5e73\u5766\u6700\u5c0f\u503c\uff09\uff1b\u4e34\u754c\u72b6\u6001\uff08\u4e0d\u7a33\u5b9a\u6027\u6301\u7eed\uff0c\u4ee5\u5e42\u5f8b\u6536\u655b\u81f3\u6700\u4f18\u5e73\u5766\u6700\u5c0f\u503c\uff09\uff1b\u8d85\u4e34\u754c\u72b6\u6001\uff08\u4e0d\u7a33\u5b9a\u6027\u6301\u7eed\uff0c\u7ebf\u6027\u6536\u655b\u81f3\u5468\u671f\u4e3a2\u7684\u8f68\u9053\uff09\u3002", "conclusion": "\u901a\u8fc7\u6d41\u5f62\u5206\u89e3\u65b9\u6cd5\uff0c\u6210\u529f\u91cf\u5316\u4e86\u68af\u5ea6\u4e0b\u964d\u5728\u5927\u5b66\u4e60\u7387\u4e0b\u7684\u6536\u655b\u884c\u4e3a\uff0c\u89e3\u91ca\u4e86\u8fb9\u7f18\u7a33\u5b9a\u6027\u73b0\u8c61\u4e2d\u975e\u5355\u8c03\u4e0b\u964d\u548c\u5e73\u5766\u6700\u5c0f\u503c\u7684\u504f\u597d\u673a\u5236\u3002"}}
{"id": "2510.16051", "categories": ["cs.LG", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.16051", "abs": "https://arxiv.org/abs/2510.16051", "authors": ["Sofiya Garkot", "Maksym Shamrai", "Ivan Synytsia", "Mariya Hirna"], "title": "GUIrilla: A Scalable Framework for Automated Desktop UI Exploration", "comment": "22 pages", "summary": "Autonomous agents capable of operating complex graphical user interfaces\n(GUIs) have the potential to transform desktop automation. While recent\nadvances in large language models (LLMs) have significantly improved UI\nunderstanding, navigating full-window, multi-application desktop environments\nremains a major challenge. Data availability is limited by costly manual\nannotation, closed-source datasets and surface-level synthetic pipelines. We\nintroduce GUIrilla, an automated scalable framework that systematically\nexplores applications via native accessibility APIs to address the critical\ndata collection challenge in GUI automation. Our framework focuses on macOS -\nan ecosystem with limited representation in current UI datasets - though many\nof its components are designed for broader cross-platform applicability.\nGUIrilla organizes discovered interface elements and crawler actions into\nhierarchical GUI graphs and employs specialized interaction handlers to achieve\ncomprehensive application coverage. Using the application graphs from GUIrilla\ncrawler, we construct and release GUIrilla-Task, a large-scale dataset of\n27,171 functionally grounded tasks across 1,108 macOS applications, each\nannotated with full-desktop and window-level screenshots, accessibility\nmetadata, and semantic action traces. Empirical results show that tuning\nLLM-based agents on GUIrilla-Task significantly improves performance on\ndownstream UI tasks, outperforming synthetic baselines on the ScreenSpot Pro\nbenchmark while using 97% less data. We also release macapptree, an open-source\nlibrary for reproducible collection of structured accessibility metadata, along\nwith the full GUIrilla-Task dataset, the manually verified GUIrilla-Gold\nbenchmark, and the framework code to support open research in desktop autonomy.", "AI": {"tldr": "GUIrilla\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u53ef\u6269\u5c55\u6846\u67b6\uff0c\u901a\u8fc7\u539f\u751f\u53ef\u8bbf\u95ee\u6027API\u7cfb\u7edf\u63a2\u7d22\u5e94\u7528\u7a0b\u5e8f\uff0c\u89e3\u51b3GUI\u81ea\u52a8\u5316\u4e2d\u7684\u6570\u636e\u6536\u96c6\u6311\u6218\uff0c\u5e76\u6784\u5efa\u4e86\u5305\u542b27,171\u4e2a\u4efb\u52a1\u7684GUIrilla-Task\u6570\u636e\u96c6\u3002", "motivation": "\u5f53\u524d\u81ea\u4e3b\u4ee3\u7406\u5728\u590d\u6742\u56fe\u5f62\u7528\u6237\u754c\u9762\u64cd\u4f5c\u65b9\u9762\u9762\u4e34\u6570\u636e\u53ef\u7528\u6027\u9650\u5236\uff0c\u5305\u62ec\u6602\u8d35\u7684\u624b\u52a8\u6807\u6ce8\u3001\u95ed\u6e90\u6570\u636e\u96c6\u548c\u8868\u9762\u7ea7\u5408\u6210\u6d41\u7a0b\uff0c\u7279\u522b\u662f\u5728macOS\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u4ee3\u8868\u6027\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528\u539f\u751f\u53ef\u8bbf\u95ee\u6027API\u7cfb\u7edf\u63a2\u7d22\u5e94\u7528\u7a0b\u5e8f\uff0c\u5c06\u53d1\u73b0\u7684\u754c\u9762\u5143\u7d20\u548c\u722c\u866b\u64cd\u4f5c\u7ec4\u7ec7\u6210\u5c42\u6b21\u5316GUI\u56fe\uff0c\u5e76\u91c7\u7528\u4e13\u95e8\u7684\u4ea4\u4e92\u5904\u7406\u7a0b\u5e8f\u5b9e\u73b0\u5168\u9762\u5e94\u7528\u8986\u76d6\u3002", "result": "\u6784\u5efa\u4e86GUIrilla-Task\u6570\u636e\u96c6\uff0c\u5305\u542b27,171\u4e2a\u529f\u80fd\u57fa\u7840\u4efb\u52a1\uff0c\u6db5\u76d61,108\u4e2amacOS\u5e94\u7528\u3002\u5728ScreenSpot Pro\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u57fa\u4e8eGUIrilla-Task\u8c03\u4f18\u7684LLM\u4ee3\u7406\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u4f7f\u752897%\u66f4\u5c11\u6570\u636e\u8d85\u8d8a\u5408\u6210\u57fa\u7ebf\u3002", "conclusion": "GUIrilla\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u684c\u9762\u81ea\u52a8\u5316\u4e2d\u7684\u6570\u636e\u6536\u96c6\u6311\u6218\uff0c\u53d1\u5e03\u7684macapptree\u5e93\u3001GUIrilla-Task\u6570\u636e\u96c6\u548cGUIrilla-Gold\u57fa\u51c6\u652f\u6301\u684c\u9762\u81ea\u4e3b\u6027\u7684\u5f00\u653e\u7814\u7a76\u3002"}}
{"id": "2510.16815", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16815", "abs": "https://arxiv.org/abs/2510.16815", "authors": ["Hans Hergen Lehmann", "Jae Hee Lee", "Steven Schockaert", "Stefan Wermter"], "title": "Knowing the Facts but Choosing the Shortcut: Understanding How Large Language Models Compare Entities", "comment": "33 pages, 20 figures. Submitted ACL ARR 2025 October (under review)", "summary": "Large Language Models (LLMs) are increasingly used for knowledge-based\nreasoning tasks, yet understanding when they rely on genuine knowledge versus\nsuperficial heuristics remains challenging. We investigate this question\nthrough entity comparison tasks by asking models to compare entities along\nnumerical attributes (e.g., ``Which river is longer, the Danube or the\nNile?''), which offer clear ground truth for systematic analysis. Despite\nhaving sufficient numerical knowledge to answer correctly, LLMs frequently make\npredictions that contradict this knowledge. We identify three heuristic biases\nthat strongly influence model predictions: entity popularity, mention order,\nand semantic co-occurrence. For smaller models, a simple logistic regression\nusing only these surface cues predicts model choices more accurately than the\nmodel's own numerical predictions, suggesting heuristics largely override\nprincipled reasoning. Crucially, we find that larger models (32B parameters)\nselectively rely on numerical knowledge when it is more reliable, while smaller\nmodels (7--8B parameters) show no such discrimination, which explains why\nlarger models outperform smaller ones even when the smaller models possess more\naccurate knowledge. Chain-of-thought prompting steers all models towards using\nthe numerical features across all model sizes.", "AI": {"tldr": "LLMs\u5728\u5b9e\u4f53\u6bd4\u8f83\u4efb\u52a1\u4e2d\u7ecf\u5e38\u4f9d\u8d56\u542f\u53d1\u5f0f\u504f\u5dee\u800c\u975e\u771f\u5b9e\u77e5\u8bc6\u8fdb\u884c\u63a8\u7406\uff0c\u5373\u4f7f\u5b83\u4eec\u5177\u5907\u6b63\u786e\u7684\u6570\u503c\u77e5\u8bc6\u3002\u7814\u7a76\u53d1\u73b0\u5b9e\u4f53\u6d41\u884c\u5ea6\u3001\u63d0\u53ca\u987a\u5e8f\u548c\u8bed\u4e49\u5171\u73b0\u4e09\u79cd\u542f\u53d1\u5f0f\u504f\u5dee\u4e25\u91cd\u5f71\u54cd\u6a21\u578b\u9884\u6d4b\u3002\u5927\u6a21\u578b\u80fd\u9009\u62e9\u6027\u5730\u4f9d\u8d56\u66f4\u53ef\u9760\u7684\u6570\u503c\u77e5\u8bc6\uff0c\u800c\u5c0f\u6a21\u578b\u5219\u65e0\u6cd5\u533a\u5206\uff0c\u8fd9\u89e3\u91ca\u4e86\u4e3a\u4ec0\u4e48\u5927\u6a21\u578b\u8868\u73b0\u66f4\u597d\u3002\u601d\u7ef4\u94fe\u63d0\u793a\u80fd\u5f15\u5bfc\u6240\u6709\u6a21\u578b\u66f4\u597d\u5730\u4f7f\u7528\u6570\u503c\u7279\u5f81\u3002", "motivation": "\u7814\u7a76LLMs\u5728\u77e5\u8bc6\u63a8\u7406\u4efb\u52a1\u4e2d\u4f55\u65f6\u4f9d\u8d56\u771f\u5b9e\u77e5\u8bc6\u800c\u975e\u8868\u9762\u542f\u53d1\u5f0f\uff0c\u901a\u8fc7\u5b9e\u4f53\u6bd4\u8f83\u4efb\u52a1\uff08\u5982\u6cb3\u6d41\u957f\u5ea6\u6bd4\u8f83\uff09\u6765\u7cfb\u7edf\u5206\u6790\u6a21\u578b\u63a8\u7406\u884c\u4e3a\u3002", "method": "\u4f7f\u7528\u5b9e\u4f53\u6bd4\u8f83\u4efb\u52a1\uff0c\u5206\u6790LLMs\u5728\u6570\u503c\u5c5e\u6027\u6bd4\u8f83\u4e2d\u7684\u8868\u73b0\uff0c\u8bc6\u522b\u4e09\u79cd\u542f\u53d1\u5f0f\u504f\u5dee\uff08\u5b9e\u4f53\u6d41\u884c\u5ea6\u3001\u63d0\u53ca\u987a\u5e8f\u3001\u8bed\u4e49\u5171\u73b0\uff09\uff0c\u6bd4\u8f83\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\uff087-8B vs 32B\u53c2\u6570\uff09\u7684\u884c\u4e3a\u5dee\u5f02\uff0c\u5e76\u6d4b\u8bd5\u601d\u7ef4\u94fe\u63d0\u793a\u7684\u6548\u679c\u3002", "result": "LLMs\u7ecf\u5e38\u505a\u51fa\u4e0e\u81ea\u8eab\u77e5\u8bc6\u76f8\u77db\u76fe\u7684\u9884\u6d4b\uff1b\u5c0f\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u542f\u53d1\u5f0f\u504f\u5dee\uff0c\u4ec5\u57fa\u4e8e\u8868\u9762\u7ebf\u7d22\u7684\u903b\u8f91\u56de\u5f52\u6bd4\u6a21\u578b\u81ea\u8eab\u6570\u503c\u9884\u6d4b\u66f4\u51c6\u786e\uff1b\u5927\u6a21\u578b\u80fd\u9009\u62e9\u6027\u5730\u4f9d\u8d56\u66f4\u53ef\u9760\u7684\u6570\u503c\u77e5\u8bc6\uff1b\u601d\u7ef4\u94fe\u63d0\u793a\u80fd\u6709\u6548\u5f15\u5bfc\u6240\u6709\u6a21\u578b\u4f7f\u7528\u6570\u503c\u7279\u5f81\u3002", "conclusion": "LLMs\u5728\u63a8\u7406\u65f6\u5b58\u5728\u542f\u53d1\u5f0f\u504f\u5dee\u4f18\u5148\u4e8e\u771f\u5b9e\u77e5\u8bc6\u7684\u95ee\u9898\uff0c\u6a21\u578b\u89c4\u6a21\u5f71\u54cd\u5176\u9009\u62e9\u6027\u4f9d\u8d56\u77e5\u8bc6\u7684\u80fd\u529b\uff0c\u601d\u7ef4\u94fe\u63d0\u793a\u662f\u6539\u5584\u6a21\u578b\u63a8\u7406\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2510.17266", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.17266", "abs": "https://arxiv.org/abs/2510.17266", "authors": ["Jiayu Bai", "Zhanbo Feng", "Zhijie Deng", "Tianqi Hou", "Robert C. Qiu", "Zenan Ling"], "title": "Adaptive Discretization for Consistency Models", "comment": "Accepted by NeurIPS 2025", "summary": "Consistency Models (CMs) have shown promise for efficient one-step\ngeneration. However, most existing CMs rely on manually designed discretization\nschemes, which can cause repeated adjustments for different noise schedules and\ndatasets. To address this, we propose a unified framework for the automatic and\nadaptive discretization of CMs, formulating it as an optimization problem with\nrespect to the discretization step. Concretely, during the consistency training\nprocess, we propose using local consistency as the optimization objective to\nensure trainability by avoiding excessive discretization, and taking global\nconsistency as a constraint to ensure stability by controlling the denoising\nerror in the training target. We establish the trade-off between local and\nglobal consistency with a Lagrange multiplier. Building on this framework, we\nachieve adaptive discretization for CMs using the Gauss-Newton method. We refer\nto our approach as ADCMs. Experiments demonstrate that ADCMs significantly\nimprove the training efficiency of CMs, achieving superior generative\nperformance with minimal training overhead on both CIFAR-10 and ImageNet.\nMoreover, ADCMs exhibit strong adaptability to more advanced DM variants. Code\nis available at https://github.com/rainstonee/ADCM.", "AI": {"tldr": "\u63d0\u51faADCMs\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u81ea\u9002\u5e94\u79bb\u6563\u5316\u65b9\u6cd5\u89e3\u51b3\u4e00\u81f4\u6027\u6a21\u578b\u4f9d\u8d56\u624b\u52a8\u8bbe\u8ba1\u79bb\u6563\u5316\u65b9\u6848\u7684\u95ee\u9898\uff0c\u4f7f\u7528\u5c40\u90e8\u4e00\u81f4\u6027\u4f5c\u4e3a\u4f18\u5316\u76ee\u6807\u3001\u5168\u5c40\u4e00\u81f4\u6027\u4f5c\u4e3a\u7ea6\u675f\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u548c\u751f\u6210\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u4e00\u81f4\u6027\u6a21\u578b\u5927\u591a\u4f9d\u8d56\u624b\u52a8\u8bbe\u8ba1\u7684\u79bb\u6563\u5316\u65b9\u6848\uff0c\u8fd9\u5728\u4e0d\u540c\u566a\u58f0\u8c03\u5ea6\u548c\u6570\u636e\u96c6\u4e0a\u9700\u8981\u53cd\u590d\u8c03\u6574\uff0c\u7f3a\u4e4f\u901a\u7528\u6027\u548c\u6548\u7387\u3002", "method": "\u5c06\u79bb\u6563\u5316\u5236\u5b9a\u4e3a\u4f18\u5316\u95ee\u9898\uff0c\u4ee5\u5c40\u90e8\u4e00\u81f4\u6027\u4e3a\u4f18\u5316\u76ee\u6807\u786e\u4fdd\u53ef\u8bad\u7ec3\u6027\uff0c\u5168\u5c40\u4e00\u81f4\u6027\u4e3a\u7ea6\u675f\u786e\u4fdd\u7a33\u5b9a\u6027\uff0c\u4f7f\u7528\u62c9\u683c\u6717\u65e5\u4e58\u5b50\u5e73\u8861\u4e24\u8005\uff0c\u5e76\u57fa\u4e8e\u9ad8\u65af-\u725b\u987f\u6cd5\u5b9e\u73b0\u81ea\u9002\u5e94\u79bb\u6563\u5316\u3002", "result": "\u5728CIFAR-10\u548cImageNet\u4e0a\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\uff0c\u83b7\u5f97\u66f4\u4f18\u7684\u751f\u6210\u6027\u80fd\uff0c\u4e14\u5bf9\u66f4\u5148\u8fdb\u7684\u6269\u6563\u6a21\u578b\u53d8\u4f53\u8868\u73b0\u51fa\u5f3a\u9002\u5e94\u6027\u3002", "conclusion": "ADCMs\u6846\u67b6\u4e3a\u4e00\u81f4\u6027\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u81ea\u52a8\u81ea\u9002\u5e94\u7684\u79bb\u6563\u5316\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u624b\u52a8\u8bbe\u8ba1\u65b9\u6848\u7684\u5c40\u9650\u6027\uff0c\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2510.16756", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.RO", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.16756", "abs": "https://arxiv.org/abs/2510.16756", "authors": ["Siyin Wang", "Wenyi Yu", "Xianzhao Chen", "Xiaohai Tian", "Jun Zhang", "Lu Lu", "Chao Zhang"], "title": "End-to-end Listen, Look, Speak and Act", "comment": "22 pages, 8 figures", "summary": "Human interaction is inherently multimodal and full-duplex: we listen while\nwatching, speak while acting, and fluidly adapt to turn-taking and\ninterruptions. Realizing these capabilities is essential for building models\nsimulating humans. We present ELLSA (End-to-end Listen, Look, Speak and Act),\nwhich, to our knowledge, is the first full-duplex, end-to-end model that\nsimultaneously perceives and generates across vision, text, speech, and action\nwithin a single architecture, enabling interaction patterns previously out of\nreach, yielding more natural, human-like behaviors. At its core is a novel\nSA-MoE architecture (Self-Attention Mixture-of-Experts) that routes each\nmodality to specialized experts and fuses them through a unified attention\nbackbone. This provides a generalizable solution for joint multimodal\nperception and concurrent generation, leveraging strong pre-trained components\nwhile enabling efficient modality integration and mitigating modality\ninterference. On speech-interaction and robot-manipulation benchmarks, ELLSA\nmatches modality-specific baselines, while uniquely supporting advanced\nmultimodal and full-duplex behaviors such as dialogue and action turn-taking,\ndefective instruction rejection, speaking-while-acting, context-grounded visual\nquestion answering, and action barge-ins. We contend that ELLSA represents a\nstep toward more natural and general interactive intelligence, contributing to\nthe broader pursuit of artificial general intelligence. All data, code and\nmodel checkpoints will be released upon acceptance.", "AI": {"tldr": "ELLSA\u662f\u9996\u4e2a\u5168\u53cc\u5de5\u3001\u7aef\u5230\u7aef\u7684\u591a\u6a21\u6001\u6a21\u578b\uff0c\u80fd\u591f\u540c\u65f6\u611f\u77e5\u548c\u751f\u6210\u89c6\u89c9\u3001\u6587\u672c\u3001\u8bed\u97f3\u548c\u52a8\u4f5c\uff0c\u5b9e\u73b0\u66f4\u81ea\u7136\u7684\u4eba\u673a\u4ea4\u4e92\u3002", "motivation": "\u4eba\u7c7b\u4ea4\u4e92\u672c\u8d28\u4e0a\u662f\u591a\u6a21\u6001\u548c\u5168\u53cc\u5de5\u7684\uff0c\u9700\u8981\u6a21\u578b\u80fd\u591f\u540c\u65f6\u611f\u77e5\u548c\u751f\u6210\u591a\u79cd\u6a21\u6001\uff0c\u5b9e\u73b0\u66f4\u81ea\u7136\u7684\u4eba\u7c7b\u6a21\u62df\u3002", "method": "\u91c7\u7528\u65b0\u9896\u7684SA-MoE\u67b6\u6784\uff08\u81ea\u6ce8\u610f\u529b\u4e13\u5bb6\u6df7\u5408\uff09\uff0c\u5c06\u5404\u6a21\u6001\u8def\u7531\u5230\u4e13\u95e8\u4e13\u5bb6\uff0c\u901a\u8fc7\u7edf\u4e00\u6ce8\u610f\u529b\u9aa8\u5e72\u7f51\u7edc\u8fdb\u884c\u878d\u5408\u3002", "result": "\u5728\u8bed\u97f3\u4ea4\u4e92\u548c\u673a\u5668\u4eba\u64cd\u4f5c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cELLSA\u4e0e\u7279\u5b9a\u6a21\u6001\u57fa\u7ebf\u76f8\u5f53\uff0c\u540c\u65f6\u652f\u6301\u9ad8\u7ea7\u591a\u6a21\u6001\u548c\u5168\u53cc\u5de5\u884c\u4e3a\u3002", "conclusion": "ELLSA\u4ee3\u8868\u4e86\u5411\u66f4\u81ea\u7136\u548c\u901a\u7528\u4ea4\u4e92\u667a\u80fd\u8fc8\u51fa\u7684\u4e00\u6b65\uff0c\u6709\u52a9\u4e8e\u5b9e\u73b0\u4eba\u5de5\u901a\u7528\u667a\u80fd\u3002"}}
{"id": "2510.17802", "categories": ["cs.LG", "cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.17802", "abs": "https://arxiv.org/abs/2510.17802", "authors": ["Rui Pan", "Yang Luo", "Yuxing Liu", "Yang You", "Tong Zhang"], "title": "Unbiased Gradient Low-Rank Projection", "comment": null, "summary": "Memory-efficient optimization is critical for training increasingly large\nlanguage models (LLMs). A popular strategy involves gradient low-rank\nprojection, storing only the projected optimizer states, with GaLore being a\nrepresentative example. However, a significant drawback of many such methods is\ntheir lack of convergence guarantees, as various low-rank projection approaches\nintroduce inherent biases relative to the original optimization algorithms,\nwhich contribute to performance gaps compared to full-parameter training.\nAiming to tackle this problem, this paper investigates the layerwise sampling\ntechnique for debiasing low-rank projection mechanisms. In particular, an\ninstantiation of the paradigm gives rise to a novel and unbiased low-rank\noptimization method built upon GaLore's mechanism and the Muon algorithm, named\nGaLore Unbiased with Muon (GUM). We theoretically prove our method matches the\nconvergence guarantees of the base Muon algorithm while preserving the memory\nefficiency of low-rank techniques. Empirical experiments on LLM fine-tuning and\npretraining also demonstrate non-trivial improvements over GaLore and even\nbetter performance than full-parameter training. Further investigation shows\nthat the improvement of this technique comes from a more uniform distribution\nof knowledge inside layers, leading to more efficient utilization of the model\nparameter space and better memorization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGUM\u7684\u65e0\u504f\u4f4e\u79e9\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c42\u95f4\u91c7\u6837\u6280\u672f\u6d88\u9664\u4f4e\u79e9\u6295\u5f71\u7684\u504f\u5dee\uff0c\u5728\u4fdd\u6301\u5185\u5b58\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u4e0e\u5168\u53c2\u6570\u8bad\u7ec3\u76f8\u5f53\u751a\u81f3\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u4f4e\u79e9\u4f18\u5316\u65b9\u6cd5\uff08\u5982GaLore\uff09\u56e0\u6295\u5f71\u504f\u5dee\u800c\u7f3a\u4e4f\u6536\u655b\u4fdd\u8bc1\u7684\u95ee\u9898\uff0c\u65e8\u5728\u5728\u4fdd\u6301\u5185\u5b58\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u65e0\u504f\u4f18\u5316\u3002", "method": "\u57fa\u4e8eGaLore\u673a\u5236\u548cMuon\u7b97\u6cd5\uff0c\u91c7\u7528\u5c42\u95f4\u91c7\u6837\u6280\u672f\u6765\u6d88\u9664\u4f4e\u79e9\u6295\u5f71\u7684\u504f\u5dee\uff0c\u5f00\u53d1\u4e86GaLore Unbiased with Muon (GUM)\u65b9\u6cd5\u3002", "result": "\u7406\u8bba\u8bc1\u660eGUM\u5339\u914dMuon\u7b97\u6cd5\u7684\u6536\u655b\u4fdd\u8bc1\uff0c\u5b9e\u9a8c\u663e\u793a\u5728LLM\u5fae\u8c03\u548c\u9884\u8bad\u7ec3\u4e2d\u4f18\u4e8eGaLore\uff0c\u751a\u81f3\u8d85\u8fc7\u5168\u53c2\u6570\u8bad\u7ec3\u7684\u6027\u80fd\u3002", "conclusion": "GUM\u65b9\u6cd5\u901a\u8fc7\u66f4\u5747\u5300\u7684\u5c42\u5185\u77e5\u8bc6\u5206\u5e03\u5b9e\u73b0\u4e86\u53c2\u6570\u7a7a\u95f4\u7684\u66f4\u9ad8\u6548\u5229\u7528\u548c\u66f4\u597d\u7684\u8bb0\u5fc6\u80fd\u529b\uff0c\u4e3a\u5185\u5b58\u9ad8\u6548\u7684LLM\u8bad\u7ec3\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.16053", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16053", "abs": "https://arxiv.org/abs/2510.16053", "authors": ["Chenyang Yu", "Xinpeng Xie", "Yan Huang", "Chenxi Qiu"], "title": "FUSE-Traffic: Fusion of Unstructured and Structured Data for Event-aware Traffic Forecasting", "comment": null, "summary": "Accurate traffic forecasting is a core technology for building Intelligent\nTransportation Systems (ITS), enabling better urban resource allocation and\nimproved travel experiences. With growing urbanization, traffic congestion has\nintensified, highlighting the need for reliable and responsive forecasting\nmodels. In recent years, deep learning, particularly Graph Neural Networks\n(GNNs), has emerged as the mainstream paradigm in traffic forecasting. GNNs can\neffectively capture complex spatial dependencies in road network topology and\ndynamic temporal evolution patterns in traffic flow data. Foundational models\nsuch as STGCN and GraphWaveNet, along with more recent developments including\nSTWave and D2STGNN, have achieved impressive performance on standard traffic\ndatasets. These approaches incorporate sophisticated graph convolutional\nstructures and temporal modeling mechanisms, demonstrating particular\neffectiveness in capturing and forecasting traffic patterns characterized by\nperiodic regularities. To address this challenge, researchers have explored\nvarious ways to incorporate event information. Early attempts primarily relied\non manually engineered event features. For instance, some approaches introduced\nmanually defined incident effect scores or constructed specific subgraphs for\ndifferent event-induced traffic conditions. While these methods somewhat\nenhance responsiveness to specific events, their core drawback lies in a heavy\nreliance on domain experts' prior knowledge, making generalization to diverse\nand complex unknown events difficult, and low-dimensional manual features often\nlead to the loss of rich semantic details.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u7684\u4ea4\u901a\u9884\u6d4b\u6280\u672f\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u6355\u6349\u7a7a\u95f4\u4f9d\u8d56\u6027\u548c\u65f6\u95f4\u6f14\u5316\u6a21\u5f0f\u65b9\u9762\u7684\u5e94\u7528\uff0c\u5e76\u8ba8\u8bba\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u4e8b\u4ef6\u4fe1\u606f\u6574\u5408\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u968f\u7740\u57ce\u5e02\u5316\u8fdb\u7a0b\u52a0\u5feb\uff0c\u4ea4\u901a\u62e5\u5835\u95ee\u9898\u65e5\u76ca\u4e25\u91cd\uff0c\u9700\u8981\u53ef\u9760\u4e14\u54cd\u5e94\u8fc5\u901f\u7684\u4ea4\u901a\u9884\u6d4b\u6a21\u578b\u6765\u6539\u5584\u57ce\u5e02\u8d44\u6e90\u5206\u914d\u548c\u51fa\u884c\u4f53\u9a8c\u3002", "method": "\u4e3b\u8981\u91c7\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u4f5c\u4e3a\u4e3b\u6d41\u8303\u5f0f\uff0c\u5305\u62ecSTGCN\u3001GraphWaveNet\u3001STWave\u548cD2STGNN\u7b49\u6a21\u578b\uff0c\u8fd9\u4e9b\u6a21\u578b\u7ed3\u5408\u4e86\u590d\u6742\u7684\u56fe\u5377\u79ef\u7ed3\u6784\u548c\u65f6\u95f4\u5efa\u6a21\u673a\u5236\u3002\u5bf9\u4e8e\u4e8b\u4ef6\u4fe1\u606f\u5904\u7406\uff0c\u65e9\u671f\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4eba\u5de5\u8bbe\u8ba1\u7684\u7279\u5f81\uff0c\u5982\u4e8b\u4ef6\u5f71\u54cd\u8bc4\u5206\u6216\u7279\u5b9a\u5b50\u56fe\u6784\u5efa\u3002", "result": "\u73b0\u6709\u65b9\u6cd5\u5728\u6807\u51c6\u4ea4\u901a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u6355\u6349\u5177\u6709\u5468\u671f\u6027\u89c4\u5f8b\u7684\u4ea4\u901a\u6a21\u5f0f\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u5f53\u524d\u57fa\u4e8e\u4eba\u5de5\u7279\u5f81\u7684\u4e8b\u4ef6\u4fe1\u606f\u6574\u5408\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u8fc7\u5ea6\u4f9d\u8d56\u9886\u57df\u4e13\u5bb6\u5148\u9a8c\u77e5\u8bc6\uff0c\u96be\u4ee5\u6cdb\u5316\u5230\u591a\u6837\u590d\u6742\u7684\u672a\u77e5\u4e8b\u4ef6\uff0c\u4e14\u4f4e\u7ef4\u4eba\u5de5\u7279\u5f81\u4f1a\u5bfc\u81f4\u4e30\u5bcc\u8bed\u4e49\u7ec6\u8282\u7684\u4e22\u5931\u3002"}}
{"id": "2510.16819", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16819", "abs": "https://arxiv.org/abs/2510.16819", "authors": ["Shantanu Agarwal", "Joel Barry", "Steven Fincke", "Scott Miller"], "title": "Cross-Genre Authorship Attribution via LLM-Based Retrieve-and-Rerank", "comment": null, "summary": "Authorship attribution (AA) is the task of identifying the most likely author\nof a query document from a predefined set of candidate authors. We introduce a\ntwo-stage retrieve-and-rerank framework that finetunes LLMs for cross-genre AA.\nUnlike the field of information retrieval (IR), where retrieve-and-rerank is a\nde facto strategy, cross-genre AA systems must avoid relying on topical cues\nand instead learn to identify author-specific linguistic patterns that are\nindependent of the text's subject matter (genre/domain/topic). Consequently,\nfor the reranker, we demonstrate that training strategies commonly used in IR\nare fundamentally misaligned with cross-genre AA, leading to suboptimal\nbehavior. To address this, we introduce a targeted data curation strategy that\nenables the reranker to effectively learn author-discriminative signals. Using\nour LLM-based retrieve-and-rerank pipeline, we achieve substantial gains of\n22.3 and 34.4 absolute Success@8 points over the previous state-of-the-art on\nHIATUS's challenging HRS1 and HRS2 cross-genre AA benchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u68c0\u7d22-\u91cd\u6392\u6846\u67b6\uff0c\u4f7f\u7528LLM\u8fdb\u884c\u8de8\u4f53\u88c1\u7684\u4f5c\u8005\u5f52\u5c5e\u4efb\u52a1\uff0c\u901a\u8fc7\u4e13\u95e8\u7684\u6570\u636e\u7b56\u5c55\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u8de8\u4f53\u88c1\u4f5c\u8005\u5f52\u5c5e\u4efb\u52a1\u9700\u8981\u8bc6\u522b\u72ec\u7acb\u4e8e\u6587\u672c\u4e3b\u9898\u7684\u4f5c\u8005\u7279\u5b9a\u8bed\u8a00\u6a21\u5f0f\uff0c\u800c\u4f20\u7edf\u4fe1\u606f\u68c0\u7d22\u7684\u8bad\u7ec3\u7b56\u7565\u5728\u6b64\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u68c0\u7d22-\u91cd\u6392\u6846\u67b6\uff0c\u4e3a\u91cd\u6392\u5668\u8bbe\u8ba1\u4e86\u9488\u5bf9\u6027\u7684\u6570\u636e\u7b56\u5c55\u7b56\u7565\uff0c\u4f7f\u6a21\u578b\u80fd\u6709\u6548\u5b66\u4e60\u4f5c\u8005\u533a\u5206\u6027\u4fe1\u53f7\u3002", "result": "\u5728HIATUS\u7684HRS1\u548cHRS2\u8de8\u4f53\u88c1\u4f5c\u8005\u5f52\u5c5e\u57fa\u51c6\u4e0a\uff0c\u5206\u522b\u6bd4\u4e4b\u524d\u6700\u4f18\u65b9\u6cd5\u63d0\u5347\u4e8622.3\u548c34.4\u4e2a\u7edd\u5bf9Success@8\u70b9\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u4e13\u95e8\u8bbe\u8ba1\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u4f53\u88c1\u4f5c\u8005\u5f52\u5c5e\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u9700\u6c42\u8c03\u6574\u8bad\u7ec3\u65b9\u6cd5\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.17268", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.17268", "abs": "https://arxiv.org/abs/2510.17268", "authors": ["Anthony Frion", "David S Greenberg"], "title": "Uncertainty-aware data assimilation through variational inference", "comment": null, "summary": "Data assimilation, consisting in the combination of a dynamical model with a\nset of noisy and incomplete observations in order to infer the state of a\nsystem over time, involves uncertainty in most settings. Building upon an\nexisting deterministic machine learning approach, we propose a variational\ninference-based extension in which the predicted state follows a multivariate\nGaussian distribution. Using the chaotic Lorenz-96 dynamics as a testing\nground, we show that our new model enables to obtain nearly perfectly\ncalibrated predictions, and can be integrated in a wider variational data\nassimilation pipeline in order to achieve greater benefit from increasing\nlengths of data assimilation windows. Our code is available at\nhttps://github.com/anthony-frion/Stochastic_CODA.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53d8\u5206\u63a8\u65ad\u7684\u6269\u5c55\u65b9\u6cd5\uff0c\u5c06\u786e\u5b9a\u6027\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u6269\u5c55\u5230\u591a\u5143\u9ad8\u65af\u5206\u5e03\u9884\u6d4b\uff0c\u7528\u4e8e\u6570\u636e\u540c\u5316\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u3002", "motivation": "\u6570\u636e\u540c\u5316\u6d89\u53ca\u5c06\u52a8\u6001\u6a21\u578b\u4e0e\u566a\u58f0\u548c\u4e0d\u5b8c\u6574\u89c2\u6d4b\u76f8\u7ed3\u5408\u6765\u63a8\u65ad\u7cfb\u7edf\u72b6\u6001\uff0c\u5728\u5927\u591a\u6570\u8bbe\u7f6e\u4e2d\u90fd\u5b58\u5728\u4e0d\u786e\u5b9a\u6027\u3002\u73b0\u6709\u786e\u5b9a\u6027\u65b9\u6cd5\u65e0\u6cd5\u5145\u5206\u5904\u7406\u8fd9\u79cd\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u5728\u73b0\u6709\u786e\u5b9a\u6027\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa\u53d8\u5206\u63a8\u65ad\u6269\u5c55\uff0c\u4f7f\u9884\u6d4b\u72b6\u6001\u9075\u5faa\u591a\u5143\u9ad8\u65af\u5206\u5e03\u3002\u4f7f\u7528\u6df7\u6c8cLorenz-96\u52a8\u529b\u5b66\u4f5c\u4e3a\u6d4b\u8bd5\u5e73\u53f0\u3002", "result": "\u65b0\u6a21\u578b\u80fd\u591f\u83b7\u5f97\u51e0\u4e4e\u5b8c\u7f8e\u6821\u51c6\u7684\u9884\u6d4b\uff0c\u5e76\u4e14\u53ef\u4ee5\u96c6\u6210\u5230\u66f4\u5e7f\u6cdb\u7684\u53d8\u5206\u6570\u636e\u540c\u5316\u7ba1\u9053\u4e2d\uff0c\u4ece\u589e\u52a0\u7684\u6570\u636e\u540c\u5316\u7a97\u53e3\u957f\u5ea6\u4e2d\u83b7\u5f97\u66f4\u5927\u6536\u76ca\u3002", "conclusion": "\u57fa\u4e8e\u53d8\u5206\u63a8\u65ad\u7684\u968f\u673a\u6269\u5c55\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u6570\u636e\u540c\u5316\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u4f9b\u6821\u51c6\u826f\u597d\u7684\u9884\u6d4b\uff0c\u5e76\u63d0\u5347\u6570\u636e\u540c\u5316\u6027\u80fd\u3002"}}
{"id": "2510.16769", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16769", "abs": "https://arxiv.org/abs/2510.16769", "authors": ["Shuo Han", "Yukun Cao", "Zezhong Ding", "Zengyi Gao", "S Kevin Zhou", "Xike Xie"], "title": "See or Say Graphs: Agent-Driven Scalable Graph Understanding with Vision-Language Models", "comment": null, "summary": "Vision-language models (VLMs) have shown promise in graph understanding, but\nremain limited by input-token constraints, facing scalability bottlenecks and\nlacking effective mechanisms to coordinate textual and visual modalities. To\naddress these challenges, we propose GraphVista, a unified framework that\nenhances both scalability and modality coordination in graph understanding. For\nscalability, GraphVista organizes graph information hierarchically into a\nlightweight GraphRAG base, which retrieves only task-relevant textual\ndescriptions and high-resolution visual subgraphs, compressing redundant\ncontext while preserving key reasoning elements. For modality coordination,\nGraphVista introduces a planning agent that routes tasks to the most suitable\nmodality-using the text modality for simple property reasoning and the visual\nmodality for local and structurally complex reasoning grounded in explicit\ntopology. Extensive experiments demonstrate that GraphVista scales to large\ngraphs, up to $200\\times$ larger than those used in existing benchmarks, and\nconsistently outperforms existing textual, visual, and fusion-based methods,\nachieving up to $4.4\\times$ quality improvement over the state-of-the-art\nbaselines by fully exploiting the complementary strengths of both modalities.", "AI": {"tldr": "GraphVista\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u56fe\u7406\u89e3\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u7ec4\u7ec7\u56fe\u4fe1\u606f\u548c\u4f7f\u7528\u89c4\u5212\u4ee3\u7406\u534f\u8c03\u6587\u672c\u548c\u89c6\u89c9\u6a21\u6001\uff0c\u89e3\u51b3\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u7406\u89e3\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u548c\u6a21\u6001\u534f\u8c03\u95ee\u9898\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u7406\u89e3\u4e2d\u9762\u4e34\u8f93\u5165token\u9650\u5236\u5bfc\u81f4\u7684\u53ef\u6269\u5c55\u6027\u74f6\u9888\uff0c\u4ee5\u53ca\u7f3a\u4e4f\u6709\u6548\u534f\u8c03\u6587\u672c\u548c\u89c6\u89c9\u6a21\u6001\u7684\u673a\u5236\u3002", "method": "GraphVista\u91c7\u7528\u5206\u5c42\u65b9\u6cd5\u7ec4\u7ec7\u56fe\u4fe1\u606f\u5230\u8f7b\u91cf\u7ea7GraphRAG\u57fa\u7840\u4e2d\uff0c\u4ec5\u68c0\u7d22\u4efb\u52a1\u76f8\u5173\u7684\u6587\u672c\u63cf\u8ff0\u548c\u9ad8\u5206\u8fa8\u7387\u89c6\u89c9\u5b50\u56fe\uff1b\u5f15\u5165\u89c4\u5212\u4ee3\u7406\u6839\u636e\u4efb\u52a1\u590d\u6742\u5ea6\u8def\u7531\u5230\u6700\u9002\u5408\u7684\u6a21\u6001\u3002", "result": "GraphVista\u53ef\u6269\u5c55\u5230\u6bd4\u73b0\u6709\u57fa\u51c6\u5927200\u500d\u7684\u56fe\uff0c\u5728\u8d28\u91cf\u4e0a\u6bd4\u6700\u5148\u8fdb\u57fa\u7ebf\u63d0\u53474.4\u500d\uff0c\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u7684\u6587\u672c\u3001\u89c6\u89c9\u548c\u878d\u5408\u65b9\u6cd5\u3002", "conclusion": "GraphVista\u901a\u8fc7\u5145\u5206\u5229\u7528\u4e24\u79cd\u6a21\u6001\u7684\u4e92\u8865\u4f18\u52bf\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u56fe\u7406\u89e3\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u548c\u6a21\u6001\u534f\u8c03\u6311\u6218\u3002"}}
{"id": "2510.17303", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.17303", "abs": "https://arxiv.org/abs/2510.17303", "authors": ["Armin Beck", "Peter Ochs"], "title": "Symmetries in PAC-Bayesian Learning", "comment": null, "summary": "Symmetries are known to improve the empirical performance of machine learning\nmodels, yet theoretical guarantees explaining these gains remain limited. Prior\nwork has focused mainly on compact group symmetries and often assumes that the\ndata distribution itself is invariant, an assumption rarely satisfied in\nreal-world applications. In this work, we extend generalization guarantees to\nthe broader setting of non-compact symmetries, such as translations and to\nnon-invariant data distributions. Building on the PAC-Bayes framework, we adapt\nand tighten existing bounds, demonstrating the approach on McAllester's\nPAC-Bayes bound while showing that it applies to a wide range of PAC-Bayes\nbounds. We validate our theory with experiments on a rotated MNIST dataset with\na non-uniform rotation group, where the derived guarantees not only hold but\nalso improve upon prior results. These findings provide theoretical evidence\nthat, for symmetric data, symmetric models are preferable beyond the narrow\nsetting of compact groups and invariant distributions, opening the way to a\nmore general understanding of symmetries in machine learning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c06\u6cdb\u5316\u4fdd\u8bc1\u6269\u5c55\u5230\u975e\u7d27\u5bf9\u79f0\u6027\u548c\u975e\u4e0d\u53d8\u6570\u636e\u5206\u5e03\uff0c\u901a\u8fc7PAC-Bayes\u6846\u67b6\u63d0\u4f9b\u7406\u8bba\u652f\u6301\uff0c\u5e76\u5728\u65cb\u8f6cMNIST\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7406\u8bba\u4e3b\u8981\u5173\u6ce8\u7d27\u7fa4\u5bf9\u79f0\u6027\u5e76\u5047\u8bbe\u6570\u636e\u5206\u5e03\u4e0d\u53d8\uff0c\u8fd9\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u5f88\u5c11\u6ee1\u8db3\u3002\u9700\u8981\u6269\u5c55\u7406\u8bba\u4fdd\u8bc1\u5230\u66f4\u5e7f\u6cdb\u7684\u975e\u7d27\u5bf9\u79f0\u6027\u548c\u975e\u4e0d\u53d8\u6570\u636e\u5206\u5e03\u3002", "method": "\u57fa\u4e8ePAC-Bayes\u6846\u67b6\uff0c\u6539\u8fdb\u548c\u6536\u7d27\u73b0\u6709\u8fb9\u754c\uff0c\u7279\u522b\u9002\u7528\u4e8eMcAllester\u7684PAC-Bayes\u8fb9\u754c\uff0c\u4f46\u53ef\u5e94\u7528\u4e8e\u5e7f\u6cdb\u7684PAC-Bayes\u8fb9\u754c\u3002", "result": "\u5728\u975e\u5747\u5300\u65cb\u8f6c\u7fa4\u7684\u65cb\u8f6cMNIST\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u63a8\u5bfc\u7684\u4fdd\u8bc1\u4e0d\u4ec5\u6210\u7acb\uff0c\u800c\u4e14\u4f18\u4e8e\u5148\u524d\u7ed3\u679c\u3002", "conclusion": "\u5bf9\u4e8e\u5bf9\u79f0\u6570\u636e\uff0c\u5bf9\u79f0\u6a21\u578b\u5728\u8d85\u8d8a\u7d27\u7fa4\u548c\u4e0d\u53d8\u5206\u5e03\u7684\u66f4\u5e7f\u6cdb\u8bbe\u7f6e\u4e2d\u66f4\u4f18\uff0c\u4e3a\u7406\u89e3\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u5bf9\u79f0\u6027\u5f00\u8f9f\u4e86\u66f4\u4e00\u822c\u7684\u9014\u5f84\u3002"}}
{"id": "2510.16802", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16802", "abs": "https://arxiv.org/abs/2510.16802", "authors": ["Chao Li", "Yuru Wang"], "title": "Domain-Contextualized Concept Graphs: A Computable Framework for Knowledge Representation", "comment": "14 pages", "summary": "Traditional knowledge graphs are constrained by fixed ontologies that\norganize concepts within rigid hierarchical structures. The root cause lies in\ntreating domains as implicit context rather than as explicit, reasoning-level\ncomponents. To overcome these limitations, we propose the Domain-Contextualized\nConcept Graph (CDC), a novel knowledge modeling framework that elevates domains\nto first-class elements of conceptual representation. CDC adopts a C-D-C triple\nstructure - <Concept, Relation@Domain, Concept'> - where domain specifications\nserve as dynamic classification dimensions defined on demand. Grounded in a\ncognitive-linguistic isomorphic mapping principle, CDC operationalizes how\nhumans understand concepts through contextual frames. We formalize more than\ntwenty standardized relation predicates (structural, logical, cross-domain, and\ntemporal) and implement CDC in Prolog for full inference capability. Case\nstudies in education, enterprise knowledge systems, and technical documentation\ndemonstrate that CDC enables context-aware reasoning, cross-domain analogy, and\npersonalized knowledge modeling - capabilities unattainable under traditional\nontology-based frameworks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9886\u57df\u60c5\u5883\u5316\u6982\u5ff5\u56fe(CDC)\uff0c\u5c06\u9886\u57df\u4f5c\u4e3a\u77e5\u8bc6\u8868\u793a\u7684\u4e00\u7b49\u5143\u7d20\uff0c\u91c7\u7528<\u6982\u5ff5, \u5173\u7cfb@\u9886\u57df, \u6982\u5ff5'>\u7684\u4e09\u5143\u7ec4\u7ed3\u6784\uff0c\u5b9e\u73b0\u4e0a\u4e0b\u6587\u611f\u77e5\u63a8\u7406\u548c\u8de8\u9886\u57df\u7c7b\u6bd4\u3002", "motivation": "\u4f20\u7edf\u77e5\u8bc6\u56fe\u8c31\u53d7\u9650\u4e8e\u56fa\u5b9a\u7684\u672c\u4f53\u8bba\u548c\u521a\u6027\u5c42\u6b21\u7ed3\u6784\uff0c\u4e3b\u8981\u95ee\u9898\u5728\u4e8e\u5c06\u9886\u57df\u89c6\u4e3a\u9690\u542b\u4e0a\u4e0b\u6587\u800c\u975e\u663e\u5f0f\u7684\u63a8\u7406\u7ea7\u7ec4\u4ef6\u3002", "method": "\u91c7\u7528C-D-C\u4e09\u5143\u7ec4\u7ed3\u6784\uff0c\u57fa\u4e8e\u8ba4\u77e5\u8bed\u8a00\u5b66\u540c\u6784\u6620\u5c04\u539f\u7406\uff0c\u5b9a\u4e49\u4e8620\u591a\u4e2a\u6807\u51c6\u5316\u5173\u7cfb\u8c13\u8bcd\uff08\u7ed3\u6784\u3001\u903b\u8f91\u3001\u8de8\u9886\u57df\u548c\u65f6\u95f4\uff09\uff0c\u5e76\u5728Prolog\u4e2d\u5b9e\u73b0\u5b8c\u6574\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728\u6559\u80b2\u3001\u4f01\u4e1a\u77e5\u8bc6\u7cfb\u7edf\u548c\u6280\u672f\u6587\u6863\u7b49\u6848\u4f8b\u7814\u7a76\u4e2d\uff0cCDC\u5b9e\u73b0\u4e86\u4e0a\u4e0b\u6587\u611f\u77e5\u63a8\u7406\u3001\u8de8\u9886\u57df\u7c7b\u6bd4\u548c\u4e2a\u6027\u5316\u77e5\u8bc6\u5efa\u6a21\u7b49\u4f20\u7edf\u672c\u4f53\u6846\u67b6\u65e0\u6cd5\u5b9e\u73b0\u7684\u80fd\u529b\u3002", "conclusion": "CDC\u6846\u67b6\u901a\u8fc7\u5c06\u9886\u57df\u63d0\u5347\u4e3a\u6982\u5ff5\u8868\u793a\u7684\u4e00\u7b49\u5143\u7d20\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u77e5\u8bc6\u56fe\u8c31\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u77e5\u8bc6\u5efa\u6a21\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u65b9\u6cd5\u3002"}}
{"id": "2510.16844", "categories": ["cs.CL", "cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2510.16844", "abs": "https://arxiv.org/abs/2510.16844", "authors": ["Jiajie Jin", "Yuyao Zhang", "Yimeng Xu", "Hongjin Qian", "Yutao Zhu", "Zhicheng Dou"], "title": "FinSight: Towards Real-World Financial Deep Research", "comment": "Working in progress", "summary": "Generating professional financial reports is a labor-intensive and\nintellectually demanding process that current AI systems struggle to fully\nautomate. To address this challenge, we introduce FinSight (Financial InSight),\na novel multi agent framework for producing high-quality, multimodal financial\nreports. The foundation of FinSight is the Code Agent with Variable Memory\n(CAVM) architecture, which unifies external data, designed tools, and agents\ninto a programmable variable space, enabling flexible data collection, analysis\nand report generation through executable code. To ensure professional-grade\nvisualization, we propose an Iterative Vision-Enhanced Mechanism that\nprogressively refines raw visual outputs into polished financial charts.\nFurthermore, a two stage Writing Framework expands concise Chain-of-Analysis\nsegments into coherent, citation-aware, and multimodal reports, ensuring both\nanalytical depth and structural consistency. Experiments on various company and\nindustry-level tasks demonstrate that FinSight significantly outperforms all\nbaselines, including leading deep research systems in terms of factual\naccuracy, analytical depth, and presentation quality, demonstrating a clear\npath toward generating reports that approach human-expert quality.", "AI": {"tldr": "FinSight\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7CAVM\u67b6\u6784\u3001\u8fed\u4ee3\u89c6\u89c9\u589e\u5f3a\u673a\u5236\u548c\u4e24\u9636\u6bb5\u5199\u4f5c\u6846\u67b6\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u591a\u6a21\u6001\u8d22\u52a1\u62a5\u544a\uff0c\u5728\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u5206\u6790\u6df1\u5ea6\u548c\u5448\u73b0\u8d28\u91cf\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u7cfb\u7edf\u3002", "motivation": "\u5f53\u524dAI\u7cfb\u7edf\u96be\u4ee5\u5b8c\u5168\u81ea\u52a8\u5316\u751f\u6210\u4e13\u4e1a\u7684\u8d22\u52a1\u62a5\u544a\uff0c\u56e0\u4e3a\u8fd9\u662f\u4e00\u4e2a\u52b3\u52a8\u5bc6\u96c6\u4e14\u667a\u529b\u8981\u6c42\u9ad8\u7684\u8fc7\u7a0b\u3002", "method": "\u91c7\u7528CAVM\u67b6\u6784\u7edf\u4e00\u5916\u90e8\u6570\u636e\u3001\u5de5\u5177\u548c\u667a\u80fd\u4f53\uff1b\u63d0\u51fa\u8fed\u4ee3\u89c6\u89c9\u589e\u5f3a\u673a\u5236\u9010\u6b65\u4f18\u5316\u539f\u59cb\u89c6\u89c9\u8f93\u51fa\uff1b\u4f7f\u7528\u4e24\u9636\u6bb5\u5199\u4f5c\u6846\u67b6\u5c06\u7b80\u6d01\u7684\u5206\u6790\u94fe\u6269\u5c55\u4e3a\u8fde\u8d2f\u7684\u591a\u6a21\u6001\u62a5\u544a\u3002", "result": "\u5728\u5404\u79cd\u516c\u53f8\u548c\u884c\u4e1a\u7ea7\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFinSight\u5728\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u5206\u6790\u6df1\u5ea6\u548c\u5448\u73b0\u8d28\u91cf\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u7cfb\u7edf\uff0c\u5305\u62ec\u9886\u5148\u7684\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u3002", "conclusion": "FinSight\u5c55\u793a\u4e86\u751f\u6210\u63a5\u8fd1\u4eba\u7c7b\u4e13\u5bb6\u8d28\u91cf\u62a5\u544a\u7684\u6709\u6548\u8def\u5f84\uff0c\u4e3a\u4e13\u4e1a\u8d22\u52a1\u62a5\u544a\u7684\u81ea\u52a8\u5316\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.17390", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.17390", "abs": "https://arxiv.org/abs/2510.17390", "authors": ["Seouh-won Yi", "Min-hwan Oh"], "title": "Exploration via Feature Perturbation in Contextual Bandits", "comment": "Accepted at NeurIPS 2025 (spotlight)", "summary": "We propose feature perturbation, a simple yet powerful technique that injects\nrandomness directly into feature inputs, instead of randomizing unknown\nparameters or adding noise to rewards. Remarkably, this algorithm achieves\n$\\tilde{\\mathcal{O}}(d\\sqrt{T})$ worst-case regret bound for generalized linear\nbandits, while avoiding the $\\tilde{\\mathcal{O}}(d^{3/2}\\sqrt{T})$ regret\ntypical of existing randomized bandit algorithms. Because our algorithm eschews\nparameter sampling, it is both computationally efficient and naturally extends\nto non-parametric or neural network models. We verify these advantages through\nempirical evaluations, demonstrating that feature perturbation not only\nsurpasses existing methods but also unifies strong practical performance with\nbest-known theoretical guarantees.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u7279\u5f81\u6270\u52a8\u7684\u7b80\u5355\u800c\u5f3a\u5927\u7684\u6280\u672f\uff0c\u901a\u8fc7\u76f4\u63a5\u5728\u7279\u5f81\u8f93\u5165\u4e2d\u6ce8\u5165\u968f\u673a\u6027\uff0c\u800c\u975e\u968f\u673a\u5316\u672a\u77e5\u53c2\u6570\u6216\u5411\u5956\u52b1\u6dfb\u52a0\u566a\u58f0\uff0c\u5b9e\u73b0\u4e86\u5e7f\u4e49\u7ebf\u6027\u8d4c\u535a\u673a\u7684\u6700\u574f\u60c5\u51b5\u9057\u61be\u754c\uff0c\u540c\u65f6\u907f\u514d\u4e86\u73b0\u6709\u968f\u673a\u5316\u7b97\u6cd5\u7684\u9057\u61be\u754c\u3002", "motivation": "\u73b0\u6709\u968f\u673a\u5316\u8d4c\u535a\u673a\u7b97\u6cd5\u901a\u5e38\u5b58\u5728\u9057\u61be\u754c\u8f83\u9ad8\u7684\u95ee\u9898\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u8f83\u4f4e\uff0c\u96be\u4ee5\u6269\u5c55\u5230\u975e\u53c2\u6570\u6216\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u3002\u7279\u5f81\u6270\u52a8\u65b9\u6cd5\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u4f18\u7684\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u6027\u80fd\u3002", "method": "\u7279\u5f81\u6270\u52a8\u6280\u672f\u76f4\u63a5\u5728\u7279\u5f81\u8f93\u5165\u4e2d\u6ce8\u5165\u968f\u673a\u6027\uff0c\u800c\u4e0d\u662f\u968f\u673a\u5316\u672a\u77e5\u53c2\u6570\u6216\u5411\u5956\u52b1\u6dfb\u52a0\u566a\u58f0\u3002\u8fd9\u79cd\u65b9\u6cd5\u907f\u514d\u4e86\u53c2\u6570\u91c7\u6837\uff0c\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u5e76\u80fd\u81ea\u7136\u6269\u5c55\u5230\u975e\u53c2\u6570\u6216\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u3002", "result": "\u8be5\u7b97\u6cd5\u5b9e\u73b0\u4e86\u5e7f\u4e49\u7ebf\u6027\u8d4c\u535a\u673a\u7684\u6700\u574f\u60c5\u51b5\u9057\u61be\u754c\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u968f\u673a\u5316\u7b97\u6cd5\u7684\u9057\u61be\u754c\u3002\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0c\u7279\u5f81\u6270\u52a8\u4e0d\u4ec5\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u800c\u4e14\u5728\u5f3a\u5b9e\u9645\u6027\u80fd\u548c\u6700\u4f73\u7406\u8bba\u4fdd\u8bc1\u4e4b\u95f4\u5b9e\u73b0\u4e86\u7edf\u4e00\u3002", "conclusion": "\u7279\u5f81\u6270\u52a8\u662f\u4e00\u79cd\u7b80\u5355\u800c\u5f3a\u5927\u7684\u6280\u672f\uff0c\u901a\u8fc7\u76f4\u63a5\u5728\u7279\u5f81\u8f93\u5165\u4e2d\u6ce8\u5165\u968f\u673a\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u7406\u8bba\u9057\u61be\u754c\u548c\u5b9e\u9645\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u5bf9\u590d\u6742\u6a21\u578b\u7684\u826f\u597d\u6269\u5c55\u6027\u3002"}}
{"id": "2510.16872", "categories": ["cs.AI", "cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2510.16872", "abs": "https://arxiv.org/abs/2510.16872", "authors": ["Shaolei Zhang", "Ju Fan", "Meihao Fan", "Guoliang Li", "Xiaoyong Du"], "title": "DeepAnalyze: Agentic Large Language Models for Autonomous Data Science", "comment": "Code: https://github.com/ruc-datalab/DeepAnalyze Model:\n  https://huggingface.co/RUC-DataLab/DeepAnalyze-8B", "summary": "Autonomous data science, from raw data sources to analyst-grade deep research\nreports, has been a long-standing challenge, and is now becoming feasible with\nthe emergence of powerful large language models (LLMs). Recent workflow-based\ndata agents have shown promising results on specific data tasks but remain\nfundamentally limited in achieving fully autonomous data science due to their\nreliance on predefined workflows. In this paper, we introduce DeepAnalyze-8B,\nthe first agentic LLM designed for autonomous data science, capable of\nautomatically completing the end-toend pipeline from data sources to\nanalyst-grade deep research reports. To tackle high-complexity data science\ntasks, we propose a curriculum-based agentic training paradigm that emulates\nthe learning trajectory of human data scientists, enabling LLMs to\nprogressively acquire and integrate multiple capabilities in real-world\nenvironments. We also introduce a data-grounded trajectory synthesis framework\nthat constructs high-quality training data. Through agentic training,\nDeepAnalyze learns to perform a broad spectrum of data tasks, ranging from data\nquestion answering and specialized analytical tasks to open-ended data\nresearch. Experiments demonstrate that, with only 8B parameters, DeepAnalyze\noutperforms previous workflow-based agents built on most advanced proprietary\nLLMs. The model, code, and training data of DeepAnalyze are open-sourced,\npaving the way toward autonomous data science.", "AI": {"tldr": "DeepAnalyze-8B\u662f\u9996\u4e2a\u7528\u4e8e\u81ea\u4e3b\u6570\u636e\u79d1\u5b66\u7684\u4ee3\u7406\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u80fd\u591f\u4ece\u6570\u636e\u6e90\u5230\u5206\u6790\u5e08\u7ea7\u6df1\u5ea6\u7814\u7a76\u62a5\u544a\u81ea\u52a8\u5b8c\u6210\u7aef\u5230\u7aef\u6d41\u7a0b\uff0c\u4ec5\u75288B\u53c2\u6570\u5c31\u8d85\u8d8a\u4e86\u57fa\u4e8e\u6700\u5148\u8fdb\u4e13\u6709LLM\u7684\u5de5\u4f5c\u6d41\u4ee3\u7406\u3002", "motivation": "\u73b0\u6709\u7684\u5de5\u4f5c\u6d41\u5f0f\u6570\u636e\u4ee3\u7406\u5728\u7279\u5b9a\u6570\u636e\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u7531\u4e8e\u4f9d\u8d56\u9884\u5b9a\u4e49\u5de5\u4f5c\u6d41\uff0c\u65e0\u6cd5\u5b9e\u73b0\u5b8c\u5168\u81ea\u4e3b\u7684\u6570\u636e\u79d1\u5b66\u3002\u968f\u7740\u5f3a\u5927LLM\u7684\u51fa\u73b0\uff0c\u4ece\u539f\u59cb\u6570\u636e\u6e90\u5230\u5206\u6790\u5e08\u7ea7\u6df1\u5ea6\u7814\u7a76\u62a5\u544a\u7684\u81ea\u4e3b\u6570\u636e\u79d1\u5b66\u53d8\u5f97\u53ef\u884c\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u8bfe\u7a0b\u5b66\u4e60\u7684\u4ee3\u7406\u8bad\u7ec3\u8303\u5f0f\uff0c\u6a21\u62df\u4eba\u7c7b\u6570\u636e\u79d1\u5b66\u5bb6\u7684\u5b66\u4e60\u8f68\u8ff9\uff0c\u4f7fLLM\u80fd\u591f\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u9010\u6b65\u83b7\u53d6\u548c\u6574\u5408\u591a\u79cd\u80fd\u529b\u3002\u540c\u65f6\u5f15\u5165\u6570\u636e\u57fa\u7840\u7684\u8f68\u8ff9\u5408\u6210\u6846\u67b6\u6765\u6784\u5efa\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4ec5\u75288B\u53c2\u6570\u7684DeepAnalyze\u5728\u5e7f\u6cdb\u7684\u6570\u636e\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5305\u62ec\u6570\u636e\u95ee\u7b54\u3001\u4e13\u4e1a\u5206\u6790\u4efb\u52a1\u548c\u5f00\u653e\u5f0f\u6570\u636e\u7814\u7a76\uff0c\u8d85\u8d8a\u4e86\u57fa\u4e8e\u6700\u5148\u8fdb\u4e13\u6709LLM\u7684\u5de5\u4f5c\u6d41\u4ee3\u7406\u3002", "conclusion": "DeepAnalyze\u6a21\u578b\u3001\u4ee3\u7801\u548c\u8bad\u7ec3\u6570\u636e\u5df2\u5f00\u6e90\uff0c\u4e3a\u81ea\u4e3b\u6570\u636e\u79d1\u5b66\u7684\u53d1\u5c55\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2510.16851", "categories": ["cs.CL", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2510.16851", "abs": "https://arxiv.org/abs/2510.16851", "authors": ["Zhengqi Pei", "Qingming Huang", "Shuhui Wang"], "title": "Neuronal Group Communication for Efficient Neural representation", "comment": "28 pages, 2 figures", "summary": "The ever-increasing scale of modern neural networks has brought unprecedented\nperformance alongside daunting challenges in efficiency and interpretability.\nThis paper addresses the core question of how to build large neural systems\nthat learn efficient, modular, and interpretable representations. We propose\nNeuronal Group Communication (NGC), a theory-driven framework that reimagines a\nneural network as a dynamical system of interacting neuronal groups rather than\na monolithic collection of neural weights. Instead of treating each weight as\nan independent trainable parameter, NGC treats weights as transient\ninteractions between embedding-like neuronal states, with neural computation\nunfolding through iterative communication among groups of neurons. This\nlow-rank, modular representation yields compact models: groups of neurons\nexchange low-dimensional signals, enabling intra-group specialization and\ninter-group information sharing while dramatically reducing redundant\nparameters. By drawing on dynamical systems theory, we introduce a neuronal\nstability metric (analogous to Lyapunov stability) that quantifies the\ncontraction of neuron activations toward stable patterns during sequence\nprocessing. Using this metric, we reveal that emergent reasoning capabilities\ncorrespond to an external driving force or ``potential'', which nudges the\nneural dynamics away from trivial trajectories while preserving stability.\nEmpirically, we instantiate NGC in large language models (LLMs) and demonstrate\nimproved performance on complex reasoning benchmarks under moderate\ncompression. NGC consistently outperforms standard low-rank approximations and\ncross-layer basis-sharing methods at comparable compression rates. We conclude\nby discussing the broader implications of NGC, including how structured\nneuronal group dynamics might relate to generalization in high-dimensional\nlearning systems.", "AI": {"tldr": "\u63d0\u51faNeuronal Group Communication (NGC)\u6846\u67b6\uff0c\u5c06\u795e\u7ecf\u7f51\u7edc\u91cd\u65b0\u6784\u60f3\u4e3a\u795e\u7ecf\u5143\u7fa4\u4ea4\u4e92\u7684\u52a8\u6001\u7cfb\u7edf\uff0c\u901a\u8fc7\u4f4e\u7ef4\u4fe1\u53f7\u4ea4\u6362\u5b9e\u73b0\u53c2\u6570\u538b\u7f29\uff0c\u5728\u4fdd\u6301\u63a8\u7406\u80fd\u529b\u7684\u540c\u65f6\u63d0\u5347\u6a21\u578b\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\u89c4\u6a21\u4e0d\u65ad\u6269\u5927\u5e26\u6765\u7684\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u6311\u6218\uff0c\u6784\u5efa\u80fd\u591f\u5b66\u4e60\u9ad8\u6548\u3001\u6a21\u5757\u5316\u548c\u53ef\u89e3\u91ca\u8868\u793a\u7684\u5927\u578b\u795e\u7ecf\u7cfb\u7edf\u3002", "method": "\u5c06\u795e\u7ecf\u7f51\u7edc\u89c6\u4e3a\u795e\u7ecf\u5143\u7fa4\u4ea4\u4e92\u7684\u52a8\u6001\u7cfb\u7edf\uff0c\u6743\u91cd\u4f5c\u4e3a\u795e\u7ecf\u5143\u72b6\u6001\u95f4\u7684\u77ac\u6001\u4ea4\u4e92\uff0c\u901a\u8fc7\u795e\u7ecf\u5143\u7fa4\u95f4\u7684\u8fed\u4ee3\u901a\u4fe1\u8fdb\u884c\u8ba1\u7b97\u3002\u5f15\u5165\u795e\u7ecf\u5143\u7a33\u5b9a\u6027\u5ea6\u91cf\u6765\u91cf\u5316\u5e8f\u5217\u5904\u7406\u4e2d\u795e\u7ecf\u5143\u6fc0\u6d3b\u5411\u7a33\u5b9a\u6a21\u5f0f\u7684\u6536\u7f29\u3002", "result": "\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u5b9e\u4f8b\u5316NGC\uff0c\u5728\u9002\u5ea6\u538b\u7f29\u4e0b\u5728\u590d\u6742\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u6807\u51c6\u4f4e\u79e9\u8fd1\u4f3c\u548c\u8de8\u5c42\u57fa\u5171\u4eab\u65b9\u6cd5\u3002", "conclusion": "NGC\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u5316\u795e\u7ecf\u5143\u7fa4\u52a8\u6001\u53ef\u80fd\u5728\u9ad8\u7ef4\u5b66\u4e60\u7cfb\u7edf\u4e2d\u4e0e\u6cdb\u5316\u80fd\u529b\u76f8\u5173\uff0c\u4e3a\u6784\u5efa\u9ad8\u6548\u3001\u6a21\u5757\u5316\u548c\u53ef\u89e3\u91ca\u7684\u795e\u7ecf\u7f51\u7edc\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2510.17396", "categories": ["cs.LG", "eess.SP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.17396", "abs": "https://arxiv.org/abs/2510.17396", "authors": ["Keivan Faghih Niresi", "Zepeng Zhang", "Olga Fink"], "title": "RINS-T: Robust Implicit Neural Solvers for Time Series Linear Inverse Problems", "comment": "Accepted to IEEE Transactions on Instrumentation and Measurement", "summary": "Time series data are often affected by various forms of corruption, such as\nmissing values, noise, and outliers, which pose significant challenges for\ntasks such as forecasting and anomaly detection. To address these issues,\ninverse problems focus on reconstructing the original signal from corrupted\ndata by leveraging prior knowledge about its underlying structure. While deep\nlearning methods have demonstrated potential in this domain, they often require\nextensive pretraining and struggle to generalize under distribution shifts. In\nthis work, we propose RINS-T (Robust Implicit Neural Solvers for Time Series\nLinear Inverse Problems), a novel deep prior framework that achieves high\nrecovery performance without requiring pretraining data. RINS-T leverages\nneural networks as implicit priors and integrates robust optimization\ntechniques, making it resilient to outliers while relaxing the reliance on\nGaussian noise assumptions. To further improve optimization stability and\nrobustness, we introduce three key innovations: guided input initialization,\ninput perturbation, and convex output combination techniques. Each of these\ncontributions strengthens the framework's optimization stability and\nrobustness. These advancements make RINS-T a flexible and effective solution\nfor addressing complex real-world time series challenges. Our code is available\nat https://github.com/EPFL-IMOS/RINS-T.", "AI": {"tldr": "RINS-T\u662f\u4e00\u4e2a\u65e0\u9700\u9884\u8bad\u7ec3\u6570\u636e\u7684\u6df1\u5ea6\u5148\u9a8c\u6846\u67b6\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u4f5c\u4e3a\u9690\u5f0f\u5148\u9a8c\u5e76\u7ed3\u5408\u9c81\u68d2\u4f18\u5316\u6280\u672f\uff0c\u6709\u6548\u89e3\u51b3\u65f6\u95f4\u5e8f\u5217\u7ebf\u6027\u9006\u95ee\u9898\uff0c\u5bf9\u5f02\u5e38\u503c\u5177\u6709\u5f3a\u9c81\u68d2\u6027\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5e38\u53d7\u7f3a\u5931\u503c\u3001\u566a\u58f0\u548c\u5f02\u5e38\u503c\u7b49\u6c61\u67d3\uff0c\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u9884\u8bad\u7ec3\u4e14\u96be\u4ee5\u5e94\u5bf9\u5206\u5e03\u504f\u79fb\u3002", "method": "\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u4f5c\u4e3a\u9690\u5f0f\u5148\u9a8c\uff0c\u96c6\u6210\u9c81\u68d2\u4f18\u5316\u6280\u672f\uff0c\u5f15\u5165\u5f15\u5bfc\u8f93\u5165\u521d\u59cb\u5316\u3001\u8f93\u5165\u6270\u52a8\u548c\u51f8\u8f93\u51fa\u7ec4\u5408\u4e09\u4e2a\u5173\u952e\u6280\u672f\u3002", "result": "\u5b9e\u73b0\u4e86\u9ad8\u6062\u590d\u6027\u80fd\uff0c\u65e0\u9700\u9884\u8bad\u7ec3\u6570\u636e\uff0c\u5bf9\u5f02\u5e38\u503c\u5177\u6709\u9c81\u68d2\u6027\uff0c\u4e14\u4e0d\u4f9d\u8d56\u9ad8\u65af\u566a\u58f0\u5047\u8bbe\u3002", "conclusion": "RINS-T\u4e3a\u590d\u6742\u73b0\u5b9e\u4e16\u754c\u65f6\u95f4\u5e8f\u5217\u6311\u6218\u63d0\u4f9b\u4e86\u7075\u6d3b\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.16907", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16907", "abs": "https://arxiv.org/abs/2510.16907", "authors": ["Kangrui Wang", "Pingyue Zhang", "Zihan Wang", "Yaning Gao", "Linjie Li", "Qineng Wang", "Hanyang Chen", "Chi Wan", "Yiping Lu", "Zhengyuan Yang", "Lijuan Wang", "Ranjay Krishna", "Jiajun Wu", "Li Fei-Fei", "Yejin Choi", "Manling Li"], "title": "VAGEN: Reinforcing World Model Reasoning for Multi-Turn VLM Agents", "comment": "Accepted to NeurIPS 2025", "summary": "A key challenge in training Vision-Language Model (VLM) agents, compared to\nLanguage Model (LLM) agents, lies in the shift from textual states to complex\nvisual observations. This transition introduces partial observability and\ndemands robust world modeling. We ask: Can VLM agents construct internal world\nmodels through explicit visual state reasoning? To address this question, we\narchitecturally enforce and reward the agent's reasoning process via\nreinforcement learning (RL), formulating it as a Partially Observable Markov\nDecision Process (POMDP). We find that decomposing the agent's reasoning into\nState Estimation (\"what is the current state?\") and Transition Modeling (\"what\ncomes next?\") is critical for success, as demonstrated through five reasoning\nstrategies. Our investigation into how agents represent internal beliefs\nreveals that the optimal representation is task-dependent: Natural Language\nexcels at capturing semantic relationships in general tasks, while Structured\nformats are indispensable for precise manipulation and control. Building on\nthese insights, we design a World Modeling Reward that provides dense,\nturn-level supervision for accurate state prediction, and introduce Bi-Level\nGeneral Advantage Estimation (Bi-Level GAE) for turn-aware credit assignment.\nThrough this form of visual state reasoning, a 3B-parameter model achieves a\nscore of 0.82 across five diverse agent benchmarks, representing a 3$\\times$\nimprovement over its untrained counterpart (0.21) and outperforming proprietary\nreasoning models such as GPT-5 (0.75), Gemini 2.5 Pro (0.67) and Claude 4.5\n(0.62). All experiments are conducted within our VAGEN framework, a scalable\nsystem for training and analyzing multi-turn VLM agents in diverse visual\nenvironments. Code and data are publicly available at\nhttps://vagen-ai.github.io.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3VLM\u667a\u80fd\u4f53\u8fdb\u884c\u663e\u5f0f\u89c6\u89c9\u72b6\u6001\u63a8\u7406\uff0c\u6784\u5efa\u5185\u90e8\u4e16\u754c\u6a21\u578b\uff0c\u5728\u4e94\u4e2a\u591a\u6837\u5316\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e863\u500d\u6027\u80fd\u63d0\u5347\uff0c\u8d85\u8d8a\u4e86GPT-5\u3001Gemini 2.5 Pro\u548cClaude 4.5\u7b49\u4e13\u6709\u63a8\u7406\u6a21\u578b\u3002", "motivation": "\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLM)\u667a\u80fd\u4f53\u9762\u4e34\u4ece\u6587\u672c\u72b6\u6001\u5230\u590d\u6742\u89c6\u89c9\u89c2\u5bdf\u7684\u8f6c\u53d8\u6311\u6218\uff0c\u8fd9\u5f15\u5165\u4e86\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u5e76\u9700\u8981\u5f3a\u5927\u7684\u4e16\u754c\u5efa\u6a21\u80fd\u529b\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22VLM\u667a\u80fd\u4f53\u662f\u5426\u80fd\u591f\u901a\u8fc7\u663e\u5f0f\u89c6\u89c9\u72b6\u6001\u63a8\u7406\u6784\u5efa\u5185\u90e8\u4e16\u754c\u6a21\u578b\u3002", "method": "\u5c06\u667a\u80fd\u4f53\u63a8\u7406\u8fc7\u7a0b\u67b6\u6784\u5316\u4e3a\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b(POMDP)\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5f3a\u5236\u548c\u5956\u52b1\u63a8\u7406\u8fc7\u7a0b\u3002\u5c06\u63a8\u7406\u5206\u89e3\u4e3a\u72b6\u6001\u4f30\u8ba1\u548c\u8f6c\u79fb\u5efa\u6a21\uff0c\u8bbe\u8ba1\u4e86\u4e16\u754c\u5efa\u6a21\u5956\u52b1\u63d0\u4f9b\u5bc6\u96c6\u76d1\u7763\uff0c\u5e76\u5f15\u5165\u53cc\u5c42\u901a\u7528\u4f18\u52bf\u4f30\u8ba1\u8fdb\u884c\u56de\u5408\u611f\u77e5\u7684\u4fe1\u7528\u5206\u914d\u3002", "result": "\u4e00\u4e2a30\u4ebf\u53c2\u6570\u7684\u6a21\u578b\u5728\u4e94\u4e2a\u591a\u6837\u5316\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u83b7\u5f97\u4e860.82\u7684\u5206\u6570\uff0c\u76f8\u6bd4\u672a\u8bad\u7ec3\u5bf9\u5e94\u6a21\u578b(0.21)\u5b9e\u73b0\u4e863\u500d\u6539\u8fdb\uff0c\u8d85\u8d8a\u4e86GPT-5(0.75)\u3001Gemini 2.5 Pro(0.67)\u548cClaude 4.5(0.62)\u7b49\u4e13\u6709\u63a8\u7406\u6a21\u578b\u3002", "conclusion": "\u901a\u8fc7\u89c6\u89c9\u72b6\u6001\u63a8\u7406\uff0cVLM\u667a\u80fd\u4f53\u80fd\u591f\u6709\u6548\u6784\u5efa\u5185\u90e8\u4e16\u754c\u6a21\u578b\u3002\u6700\u4f18\u8868\u793a\u5f62\u5f0f\u662f\u4efb\u52a1\u4f9d\u8d56\u7684\uff1a\u81ea\u7136\u8bed\u8a00\u5728\u4e00\u822c\u4efb\u52a1\u4e2d\u64c5\u957f\u6355\u6349\u8bed\u4e49\u5173\u7cfb\uff0c\u800c\u7ed3\u6784\u5316\u683c\u5f0f\u5bf9\u4e8e\u7cbe\u786e\u64cd\u4f5c\u548c\u63a7\u5236\u4e0d\u53ef\u6216\u7f3a\u3002"}}
{"id": "2510.16065", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16065", "abs": "https://arxiv.org/abs/2510.16065", "authors": ["Lunchen Xie", "Zehua He", "Qingjiang Shi"], "title": "FedPURIN: Programmed Update and Reduced INformation for Sparse Personalized Federated Learning", "comment": null, "summary": "Personalized Federated Learning (PFL) has emerged as a critical research\nfrontier addressing data heterogeneity issue across distributed clients. Novel\nmodel architectures and collaboration mechanisms are engineered to accommodate\nstatistical disparities while producing client-specific models. Parameter\ndecoupling represents a promising paradigm for maintaining model performance in\nPFL frameworks. However, the communication efficiency of many existing methods\nremains suboptimal, sustaining substantial communication burdens that impede\npractical deployment. To bridge this gap, we propose Federated Learning with\nProgrammed Update and Reduced INformation (FedPURIN), a novel framework that\nstrategically identifies critical parameters for transmission through an\ninteger programming formulation. This mathematically grounded strategy is\nseamlessly integrated into a sparse aggregation scheme, achieving a significant\ncommunication reduction while preserving the efficacy. Comprehensive\nevaluations on standard image classification benchmarks under varied non-IID\nconditions demonstrate competitive performance relative to state-of-the-art\nmethods, coupled with quantifiable communication reduction through sparse\naggregation. The framework establishes a new paradigm for\ncommunication-efficient PFL, particularly advantageous for edge intelligence\nsystems operating with heterogeneous data sources.", "AI": {"tldr": "\u63d0\u51faFedPURIN\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u6570\u7f16\u7a0b\u8bc6\u522b\u5173\u952e\u53c2\u6570\u8fdb\u884c\u4f20\u8f93\uff0c\u7ed3\u5408\u7a00\u758f\u805a\u5408\u65b9\u6848\u663e\u8457\u964d\u4f4e\u901a\u4fe1\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u4e2a\u6027\u5316\u8054\u90a6\u5b66\u4e60\u6027\u80fd", "motivation": "\u89e3\u51b3\u4e2a\u6027\u5316\u8054\u90a6\u5b66\u4e60\u4e2d\u901a\u4fe1\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u6570\u636e\u5f02\u6784\u73af\u5883\u4e0b\u901a\u4fe1\u8d1f\u62c5\u8f83\u91cd\uff0c\u963b\u788d\u5b9e\u9645\u90e8\u7f72", "method": "\u57fa\u4e8e\u6574\u6570\u7f16\u7a0b\u7b56\u7565\u8bc6\u522b\u5173\u952e\u4f20\u8f93\u53c2\u6570\uff0c\u7ed3\u5408\u7a00\u758f\u805a\u5408\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u901a\u4fe1\u91cf", "result": "\u5728\u6807\u51c6\u56fe\u50cf\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5728\u591a\u79cd\u975eIID\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5b9e\u73b0\u53ef\u91cf\u5316\u7684\u901a\u4fe1\u51cf\u5c11", "conclusion": "FedPURIN\u4e3a\u901a\u4fe1\u9ad8\u6548\u7684\u4e2a\u6027\u5316\u8054\u90a6\u5b66\u4e60\u5efa\u7acb\u4e86\u65b0\u8303\u5f0f\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5177\u6709\u5f02\u6784\u6570\u636e\u6e90\u7684\u8fb9\u7f18\u667a\u80fd\u7cfb\u7edf"}}
{"id": "2510.16924", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16924", "abs": "https://arxiv.org/abs/2510.16924", "authors": ["Zhihui Yang", "Yupei Wang", "Kaijie Mo", "Zhe Zhao", "Renfen Hu"], "title": "Does Visual Grounding Enhance the Understanding of Embodied Knowledge in Large Language Models?", "comment": "Accepted to EMNLP 2025 (Findings). This version corrects a redundant\n  sentence in the Results section that appeared in the camera-ready version", "summary": "Despite significant progress in multimodal language models (LMs), it remains\nunclear whether visual grounding enhances their understanding of embodied\nknowledge compared to text-only models. To address this question, we propose a\nnovel embodied knowledge understanding benchmark based on the perceptual theory\nfrom psychology, encompassing visual, auditory, tactile, gustatory, olfactory\nexternal senses, and interoception. The benchmark assesses the models'\nperceptual abilities across different sensory modalities through vector\ncomparison and question-answering tasks with over 1,700 questions. By comparing\n30 state-of-the-art LMs, we surprisingly find that vision-language models\n(VLMs) do not outperform text-only models in either task. Moreover, the models\nperform significantly worse in the visual dimension compared to other sensory\ndimensions. Further analysis reveals that the vector representations are easily\ninfluenced by word form and frequency, and the models struggle to answer\nquestions involving spatial perception and reasoning. Our findings underscore\nthe need for more effective integration of embodied knowledge in LMs to enhance\ntheir understanding of the physical world.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5fc3\u7406\u5b66\u611f\u77e5\u7406\u8bba\u7684\u591a\u6a21\u6001\u77e5\u8bc6\u7406\u89e3\u57fa\u51c6\uff0c\u8bc4\u4f30\u4e8630\u4e2a\u5148\u8fdb\u8bed\u8a00\u6a21\u578b\u5728\u591a\u79cd\u611f\u5b98\u6a21\u6001\u4e0b\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u611f\u77e5\u77e5\u8bc6\u7406\u89e3\u4e0a\u5e76\u4e0d\u4f18\u4e8e\u7eaf\u6587\u672c\u6a21\u578b\u3002", "motivation": "\u7814\u7a76\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u662f\u5426\u901a\u8fc7\u89c6\u89c9\u57fa\u7840\u589e\u5f3a\u4e86\u5bf9\u5177\u8eab\u77e5\u8bc6\u7684\u7406\u89e3\uff0c\u4e0e\u7eaf\u6587\u672c\u6a21\u578b\u76f8\u6bd4\u662f\u5426\u6709\u4f18\u52bf\u3002", "method": "\u6784\u5efa\u57fa\u4e8e\u5fc3\u7406\u5b66\u611f\u77e5\u7406\u8bba\u7684\u5177\u8eab\u77e5\u8bc6\u7406\u89e3\u57fa\u51c6\uff0c\u5305\u542b\u89c6\u89c9\u3001\u542c\u89c9\u3001\u89e6\u89c9\u3001\u5473\u89c9\u3001\u55c5\u89c9\u548c\u5185\u90e8\u611f\u77e5\u7b49\u611f\u5b98\u6a21\u6001\uff0c\u901a\u8fc7\u5411\u91cf\u6bd4\u8f83\u548c\u95ee\u7b54\u4efb\u52a1\u8bc4\u4f30\u6a21\u578b\u8868\u73b0\u3002", "result": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4e24\u79cd\u4efb\u52a1\u4e2d\u5747\u672a\u4f18\u4e8e\u7eaf\u6587\u672c\u6a21\u578b\uff1b\u6a21\u578b\u5728\u89c6\u89c9\u7ef4\u5ea6\u7684\u8868\u73b0\u663e\u8457\u5dee\u4e8e\u5176\u4ed6\u611f\u5b98\u7ef4\u5ea6\uff1b\u5411\u91cf\u8868\u793a\u6613\u53d7\u8bcd\u5f62\u548c\u9891\u7387\u5f71\u54cd\uff1b\u6a21\u578b\u5728\u7a7a\u95f4\u611f\u77e5\u548c\u63a8\u7406\u95ee\u9898\u4e0a\u8868\u73b0\u56f0\u96be\u3002", "conclusion": "\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u5728\u5177\u8eab\u77e5\u8bc6\u7406\u89e3\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u66f4\u6709\u6548\u5730\u6574\u5408\u5177\u8eab\u77e5\u8bc6\u4ee5\u589e\u5f3a\u5bf9\u7269\u7406\u4e16\u754c\u7684\u7406\u89e3\u3002"}}
{"id": "2510.16956", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16956", "abs": "https://arxiv.org/abs/2510.16956", "authors": ["Mark Towers", "Yali Du", "Christopher Freeman", "Timothy J. Norman"], "title": "A Comparative User Evaluation of XRL Explanations using Goal Identification", "comment": "Accepted to ECAI 2025 Workshop on Evaluating Explainable AI and\n  Complex Decision-Making, 8 Pages", "summary": "Debugging is a core application of explainable reinforcement learning (XRL)\nalgorithms; however, limited comparative evaluations have been conducted to\nunderstand their relative performance. We propose a novel evaluation\nmethodology to test whether users can identify an agent's goal from an\nexplanation of its decision-making. Utilising the Atari's Ms. Pacman\nenvironment and four XRL algorithms, we find that only one achieved greater\nthan random accuracy for the tested goals and that users were generally\noverconfident in their selections. Further, we find that users' self-reported\nease of identification and understanding for every explanation did not\ncorrelate with their accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u6d4b\u8bd5\u7528\u6237\u662f\u5426\u80fd\u4ece\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u7684\u89e3\u91ca\u4e2d\u8bc6\u522b\u667a\u80fd\u4f53\u7684\u76ee\u6807\u3002\u5728Ms. Pacman\u73af\u5883\u4e2d\u6d4b\u8bd5\u4e86\u56db\u79cd\u53ef\u89e3\u91ca\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u53d1\u73b0\u53ea\u6709\u4e00\u79cd\u7b97\u6cd5\u5728\u6d4b\u8bd5\u76ee\u6807\u4e0a\u8fbe\u5230\u4e86\u8d85\u8fc7\u968f\u673a\u51c6\u786e\u7387\u7684\u6027\u80fd\uff0c\u4e14\u7528\u6237\u666e\u904d\u5bf9\u81ea\u5df1\u7684\u9009\u62e9\u8fc7\u5ea6\u81ea\u4fe1\u3002", "motivation": "\u53ef\u89e3\u91ca\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u7684\u6838\u5fc3\u5e94\u7528\u4e4b\u4e00\u662f\u8c03\u8bd5\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5176\u76f8\u5bf9\u6027\u80fd\u7684\u6bd4\u8f83\u8bc4\u4f30\u3002\u9700\u8981\u4e86\u89e3\u7528\u6237\u662f\u5426\u80fd\u4ece\u7b97\u6cd5\u89e3\u91ca\u4e2d\u6b63\u786e\u8bc6\u522b\u667a\u80fd\u4f53\u7684\u76ee\u6807\u3002", "method": "\u4f7f\u7528Atari\u7684Ms. Pacman\u73af\u5883\u548c\u56db\u79cd\u53ef\u89e3\u91ca\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u63d0\u51fa\u65b0\u9896\u7684\u8bc4\u4f30\u65b9\u6cd5\u6d4b\u8bd5\u7528\u6237\u4ece\u51b3\u7b56\u89e3\u91ca\u4e2d\u8bc6\u522b\u667a\u80fd\u4f53\u76ee\u6807\u7684\u80fd\u529b\u3002", "result": "\u53ea\u6709\u4e00\u79cd\u7b97\u6cd5\u5728\u6d4b\u8bd5\u76ee\u6807\u4e0a\u8fbe\u5230\u4e86\u8d85\u8fc7\u968f\u673a\u51c6\u786e\u7387\u7684\u6027\u80fd\uff1b\u7528\u6237\u666e\u904d\u5bf9\u81ea\u5df1\u7684\u9009\u62e9\u8fc7\u5ea6\u81ea\u4fe1\uff1b\u7528\u6237\u81ea\u62a5\u7684\u8bc6\u522b\u548c\u7406\u89e3\u96be\u6613\u7a0b\u5ea6\u4e0e\u4ed6\u4eec\u7684\u51c6\u786e\u7387\u6ca1\u6709\u76f8\u5173\u6027\u3002", "conclusion": "\u5f53\u524d\u53ef\u89e3\u91ca\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5728\u5e2e\u52a9\u7528\u6237\u8bc6\u522b\u667a\u80fd\u4f53\u76ee\u6807\u65b9\u9762\u6548\u679c\u6709\u9650\uff0c\u7528\u6237\u7684\u4e3b\u89c2\u611f\u53d7\u4e0e\u5b9e\u9645\u8bc6\u522b\u80fd\u529b\u5b58\u5728\u8131\u8282\uff0c\u9700\u8981\u6539\u8fdb\u8bc4\u4f30\u65b9\u6cd5\u548c\u7b97\u6cd5\u8bbe\u8ba1\u3002"}}
{"id": "2510.16071", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16071", "abs": "https://arxiv.org/abs/2510.16071", "authors": ["Qinxuan Wang", "Chuang Wang", "Mingyu Zhang", "Jingwei Sun", "Peipei Yang", "Shuo Tang", "Shiming Xiang"], "title": "MNO: Multiscale Neural Operator for Computational Fluid Dynamics with 3D Point Cloud Data", "comment": null, "summary": "Neural operators have emerged as a powerful data-driven paradigm for solving\nPartial Differential Equations (PDEs), offering orders-of-magnitude\nacceleration over traditional solvers. However, existing approaches still\nsuffer from limited accuracy and scalability, particularly on irregular domains\nwhere fluid flows exhibit rich multiscale structures. In this work, we\nintroduce the Multiscale Neural Operator (MNO), a new architecture for\nComputational Fluid Dynamics (CFD) on three-dimensional (3D) unstructured point\nclouds. MNO explicitly decomposes information across three scales: a global\ndimension-shrinkage attention module for long-range dependencies, a local graph\nattention module for neighborhood-level interactions, and a micro point-wise\nattention module for fine-grained details. This design preserves multiscale\ninductive biases while remaining computationally efficient. We evaluate MNO on\nfour diverse benchmarks, covering both steady-state and unsteady flow scenarios\nwith up to 300K points. Across all tasks, MNO consistently outperforms\nstate-of-the-art baselines, reducing prediction errors by 5% to 40% and\ndemonstrating improved robustness in challenging 3D CFD problems. Our results\nhighlight the importance of explicit multiscale design for neural operators and\nestablish MNO as a scalable framework for learning complex fluid dynamics on\nirregular domains.", "AI": {"tldr": "\u63d0\u51fa\u591a\u5c3a\u5ea6\u795e\u7ecf\u7b97\u5b50(MNO)\uff0c\u4e00\u79cd\u7528\u4e8e\u4e09\u7ef4\u975e\u7ed3\u6784\u5316\u70b9\u4e91\u4e0a\u8ba1\u7b97\u6d41\u4f53\u52a8\u529b\u5b66\u7684\u65b0\u67b6\u6784\uff0c\u901a\u8fc7\u663e\u5f0f\u4e09\u5c3a\u5ea6\u5206\u89e3\u663e\u8457\u63d0\u5347\u4e86\u795e\u7ecf\u7b97\u5b50\u5728\u590d\u6742\u6d41\u4f53\u95ee\u9898\u4e0a\u7684\u7cbe\u5ea6\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u7b97\u5b50\u5728\u6c42\u89e3\u504f\u5fae\u5206\u65b9\u7a0b\u65f6\u5b58\u5728\u7cbe\u5ea6\u548c\u53ef\u6269\u5c55\u6027\u9650\u5236\uff0c\u7279\u522b\u662f\u5728\u4e0d\u89c4\u5219\u57df\u4e0a\u5904\u7406\u591a\u5c3a\u5ea6\u6d41\u4f53\u7ed3\u6784\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "MNO\u67b6\u6784\u5305\u542b\u4e09\u4e2a\u5c3a\u5ea6\u6a21\u5757\uff1a\u5168\u5c40\u7ef4\u5ea6\u6536\u7f29\u6ce8\u610f\u529b\u6a21\u5757\u5904\u7406\u957f\u7a0b\u4f9d\u8d56\uff0c\u5c40\u90e8\u56fe\u6ce8\u610f\u529b\u6a21\u5757\u5904\u7406\u90bb\u57df\u4ea4\u4e92\uff0c\u5fae\u89c2\u70b9\u7ea7\u6ce8\u610f\u529b\u6a21\u5757\u5904\u7406\u7cbe\u7ec6\u7ec6\u8282\u3002", "result": "\u5728\u56db\u4e2a\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMNO\u5728\u7a33\u6001\u548c\u975e\u7a33\u6001\u6d41\u573a\u666f\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9884\u6d4b\u8bef\u5dee\u964d\u4f4e5%-40%\uff0c\u572830\u4e07\u70b9\u89c4\u6a21\u95ee\u9898\u4e0a\u8868\u73b0\u51fa\u66f4\u597d\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u663e\u5f0f\u591a\u5c3a\u5ea6\u8bbe\u8ba1\u5bf9\u795e\u7ecf\u7b97\u5b50\u81f3\u5173\u91cd\u8981\uff0cMNO\u4e3a\u5b66\u4e60\u4e0d\u89c4\u5219\u57df\u4e0a\u590d\u6742\u6d41\u4f53\u52a8\u529b\u5b66\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u6846\u67b6\u3002"}}
{"id": "2510.16928", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16928", "abs": "https://arxiv.org/abs/2510.16928", "authors": ["Emily Chang", "Niyati Bafna"], "title": "ChiKhaPo: A Large-Scale Multilingual Benchmark for Evaluating Lexical Comprehension and Generation in Large Language Models", "comment": null, "summary": "Existing benchmarks for large language models (LLMs) are largely restricted\nto high- or mid-resource languages, and often evaluate performance on\nhigher-order tasks in reasoning and generation. However, plenty of evidence\npoints to the fact that LLMs lack basic linguistic competence in the vast\nmajority of the world's 3800+ written languages. We introduce ChiKhaPo,\nconsisting of 8 subtasks of varying difficulty designed to evaluate the lexical\ncomprehension and generation abilities of generative models. ChiKhaPo draws on\nexisting lexicons, monolingual data, and bitext, and provides coverage for\n2700+ languages for 2 subtasks, surpassing any existing benchmark in terms of\nlanguage coverage. We further show that 6 SOTA models struggle on our\nbenchmark, and discuss the factors contributing to performance scores,\nincluding language family, language resourcedness, task, and comprehension\nversus generation directions. With ChiKhaPo, we hope to enable and encourage\nthe massively multilingual benchmarking of LLMs.", "AI": {"tldr": "ChiKhaPo\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b8\u4e2a\u4e0d\u540c\u96be\u5ea6\u7684\u5b50\u4efb\u52a1\uff0c\u65e8\u5728\u8bc4\u4f30\u751f\u6210\u6a21\u578b\u57282700\u591a\u79cd\u8bed\u8a00\u4e2d\u7684\u8bcd\u6c47\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\uff0c\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u8bc4\u4f30\u65b9\u9762\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5c40\u9650\u4e8e\u9ad8\u8d44\u6e90\u6216\u4e2d\u8d44\u6e90\u8bed\u8a00\uff0c\u4e14\u591a\u5173\u6ce8\u9ad8\u9636\u63a8\u7406\u548c\u751f\u6210\u4efb\u52a1\uff0c\u800c\u7814\u7a76\u8868\u660eLLMs\u5728\u5168\u74033800\u591a\u79cd\u4e66\u9762\u8bed\u8a00\u4e2d\u7684\u57fa\u672c\u8bed\u8a00\u80fd\u529b\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u5229\u7528\u73b0\u6709\u8bcd\u5178\u3001\u5355\u8bed\u6570\u636e\u548c\u53cc\u8bed\u5e73\u884c\u8bed\u6599\u6784\u5efaChiKhaPo\u57fa\u51c6\uff0c\u5305\u542b8\u4e2a\u4e0d\u540c\u96be\u5ea6\u7684\u5b50\u4efb\u52a1\uff0c\u6db5\u76d62700\u591a\u79cd\u8bed\u8a00\uff0c\u7279\u522b\u5173\u6ce8\u8bcd\u6c47\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\u3002", "result": "\u6d4b\u8bd5\u663e\u793a6\u4e2a\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5728\u8be5\u57fa\u51c6\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u6027\u80fd\u53d7\u8bed\u8a00\u5bb6\u65cf\u3001\u8bed\u8a00\u8d44\u6e90\u4e30\u5bcc\u5ea6\u3001\u4efb\u52a1\u7c7b\u578b\u4ee5\u53ca\u7406\u89e3\u4e0e\u751f\u6210\u65b9\u5411\u7b49\u56e0\u7d20\u5f71\u54cd\u3002", "conclusion": "ChiKhaPo\u4e3a\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u53ef\u80fd\uff0c\u6709\u671b\u63a8\u52a8LLMs\u5728\u591a\u8bed\u8a00\u80fd\u529b\u65b9\u9762\u7684\u8bc4\u4f30\u548c\u53d1\u5c55\u3002"}}
{"id": "2510.17543", "categories": ["cs.LG", "eess.SP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.17543", "abs": "https://arxiv.org/abs/2510.17543", "authors": ["Jiayi Huang", "Sangwoo Park", "Nicola Paoletti", "Osvaldo Simeone"], "title": "Reliable Inference in Edge-Cloud Model Cascades via Conformal Alignment", "comment": "Under Review", "summary": "Edge intelligence enables low-latency inference via compact on-device models,\nbut assuring reliability remains challenging. We study edge-cloud cascades that\nmust preserve conditional coverage: whenever the edge returns a prediction set,\nit should contain the true label with a user-specified probability, as if\nproduced by the cloud model. We formalize conditional coverage with respect to\nthe cloud predictive distribution, and introduce a conformal alignment-based\n(CAb) cascading mechanism that certifies this property with user control over\nthe risk level. Our method casts escalation from edge to cloud models as a\nmultiple-hypothesis testing (MHT) problem, tailoring conformal alignment (CA)\nto select which inputs can be safely handled at the edge. The proposed CAb\nmodel cascading method yields statistical guarantees on the average fraction of\nedge decisions that satisfy cloud-level conditional coverage. The procedure\napplies to arbitrary edge prediction sets, including variants of conformal\nprediction (CP), and exposes a tunable trade-off among coverage, deferral rate,\nand set size. Experiments on CIFAR-100 image classification and the TeleQnA\nquestion-answering (QA) benchmark show that the proposed CAb cascade maintains\nthe target conditional coverage for edge predictions while substantially\nreducing offloading to the cloud and incurring modest increases in\nprediction-set size.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fdd\u5f62\u5bf9\u9f50\u7684\u7ea7\u8054\u673a\u5236(CAb)\uff0c\u7528\u4e8e\u8fb9\u7f18-\u4e91\u6a21\u578b\u7ea7\u8054\u7cfb\u7edf\uff0c\u786e\u4fdd\u8fb9\u7f18\u9884\u6d4b\u96c6\u6ee1\u8db3\u4e91\u6a21\u578b\u7ea7\u522b\u7684\u6761\u4ef6\u8986\u76d6\u4fdd\u8bc1\uff0c\u540c\u65f6\u51cf\u5c11\u5411\u4e91\u7684\u5378\u8f7d\u3002", "motivation": "\u8fb9\u7f18\u667a\u80fd\u867d\u7136\u80fd\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u63a8\u7406\uff0c\u4f46\u786e\u4fdd\u53ef\u9760\u6027\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u9700\u8981\u4fdd\u8bc1\u8fb9\u7f18\u9884\u6d4b\u96c6\u5728\u6761\u4ef6\u8986\u76d6\u65b9\u9762\u8fbe\u5230\u4e91\u6a21\u578b\u7684\u6c34\u5e73\u3002", "method": "\u5c06\u8fb9\u7f18\u5230\u4e91\u7684\u5347\u7ea7\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u591a\u91cd\u5047\u8bbe\u68c0\u9a8c\u95ee\u9898\uff0c\u91c7\u7528\u4fdd\u5f62\u5bf9\u9f50\u65b9\u6cd5\u6765\u9009\u62e9\u54ea\u4e9b\u8f93\u5165\u53ef\u4ee5\u5728\u8fb9\u7f18\u5b89\u5168\u5904\u7406\u3002\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u4efb\u610f\u8fb9\u7f18\u9884\u6d4b\u96c6\uff0c\u5305\u62ec\u5404\u79cd\u4fdd\u5f62\u9884\u6d4b\u53d8\u4f53\u3002", "result": "\u5728CIFAR-100\u56fe\u50cf\u5206\u7c7b\u548cTeleQnA\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCAb\u7ea7\u8054\u65b9\u6cd5\u5728\u4fdd\u6301\u76ee\u6807\u6761\u4ef6\u8986\u76d6\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5411\u4e91\u7684\u5378\u8f7d\uff0c\u9884\u6d4b\u96c6\u5927\u5c0f\u4ec5\u6709\u9002\u5ea6\u589e\u52a0\u3002", "conclusion": "CAb\u7ea7\u8054\u65b9\u6cd5\u4e3a\u8fb9\u7f18-\u4e91\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7edf\u8ba1\u4fdd\u8bc1\uff0c\u5728\u8986\u76d6\u3001\u5ef6\u8fdf\u7387\u548c\u96c6\u5408\u5927\u5c0f\u4e4b\u95f4\u5b9e\u73b0\u4e86\u53ef\u8c03\u6743\u8861\uff0c\u6709\u6548\u5e73\u8861\u4e86\u8fb9\u7f18\u63a8\u7406\u7684\u6548\u7387\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2510.16996", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16996", "abs": "https://arxiv.org/abs/2510.16996", "authors": ["Juncheng Dong", "Yang Yang", "Tao Liu", "Yang Wang", "Feng Qi", "Vahid Tarokh", "Kaushik Rangadurai", "Shuang Yang"], "title": "STARK: Strategic Team of Agents for Refining Kernels", "comment": null, "summary": "The efficiency of GPU kernels is central to the progress of modern AI, yet\noptimizing them remains a difficult and labor-intensive task due to complex\ninteractions between memory hierarchies, thread scheduling, and\nhardware-specific characteristics. While recent advances in large language\nmodels (LLMs) provide new opportunities for automated code generation, existing\napproaches largely treat LLMs as single-shot generators or naive refinement\ntools, limiting their effectiveness in navigating the irregular kernel\noptimization landscape. We introduce an LLM agentic framework for GPU kernel\noptimization that systematically explores the design space through multi-agent\ncollaboration, grounded instruction, dynamic context management, and strategic\nsearch. This framework mimics the workflow of expert engineers, enabling LLMs\nto reason about hardware trade-offs, incorporate profiling feedback, and refine\nkernels iteratively. We evaluate our approach on KernelBench, a benchmark for\nLLM-based kernel optimization, and demonstrate substantial improvements over\nbaseline agents: our system produces correct solutions where baselines often\nfail, and achieves kernels with up to 16x faster runtime performance. These\nresults highlight the potential of agentic LLM frameworks to advance fully\nautomated, scalable GPU kernel optimization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\uff0c\u7528\u4e8eGPU\u5185\u6838\u4f18\u5316\uff0c\u901a\u8fc7\u7cfb\u7edf\u5316\u63a2\u7d22\u8bbe\u8ba1\u7a7a\u95f4\u3001\u52a8\u6001\u4e0a\u4e0b\u6587\u7ba1\u7406\u548c\u7b56\u7565\u641c\u7d22\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5185\u6838\u6027\u80fd\u548c\u4f18\u5316\u6210\u529f\u7387\u3002", "motivation": "GPU\u5185\u6838\u4f18\u5316\u5bf9AI\u8fdb\u5c55\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u56f0\u96be\u4e14\u8017\u65f6\u3002\u73b0\u6709LLM\u65b9\u6cd5\u591a\u5c06\u5176\u89c6\u4e3a\u5355\u6b21\u751f\u6210\u5668\u6216\u7b80\u5355\u4f18\u5316\u5de5\u5177\uff0c\u96be\u4ee5\u5e94\u5bf9\u590d\u6742\u7684\u5185\u6838\u4f18\u5316\u6311\u6218\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\uff0c\u7ed3\u5408\u57fa\u4e8e\u7ecf\u9a8c\u7684\u6307\u5bfc\u3001\u52a8\u6001\u4e0a\u4e0b\u6587\u7ba1\u7406\u548c\u7b56\u7565\u641c\u7d22\uff0c\u6a21\u62df\u4e13\u5bb6\u5de5\u7a0b\u5e08\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u8ba9LLM\u80fd\u591f\u63a8\u7406\u786c\u4ef6\u6743\u8861\u3001\u6574\u5408\u6027\u80fd\u5206\u6790\u53cd\u9988\u5e76\u8fdb\u884c\u8fed\u4ee3\u4f18\u5316\u3002", "result": "\u5728KernelBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8be5\u7cfb\u7edf\u5728\u57fa\u7ebf\u7ecf\u5e38\u5931\u8d25\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u751f\u6210\u6b63\u786e\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5b9e\u73b0\u9ad8\u8fbe16\u500d\u7684\u8fd0\u884c\u65f6\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u8be5\u667a\u80fd\u4f53LLM\u6846\u67b6\u5c55\u793a\u4e86\u5b9e\u73b0\u5b8c\u5168\u81ea\u52a8\u5316\u3001\u53ef\u6269\u5c55GPU\u5185\u6838\u4f18\u5316\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2510.16074", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16074", "abs": "https://arxiv.org/abs/2510.16074", "authors": ["Jing He", "Hua Jiang", "Cheng Li", "Siqian Xin", "Shuzhen Yang"], "title": "Early-stopping for Transformer model training", "comment": null, "summary": "This work introduces a novel theoretical framework grounded in Random Matrix\nTheory (RMT) for analyzing Transformer training dynamics. We focus on the\nunderlying mechanisms that drive performance improvements and derive principled\nearly-stopping criteria. Empirically, we observe that the spectral density of\nthe shallow self-attention matrix V consistently evolves into a heavy-tailed\ndistribution. Utilizing the PL (Power Law) fit to this matrix as a probe, we\ndemarcate training into three stages: structural exploration, heavy-tailed\nstructure stabilization, and convergence saturation. This staging provides\nguidance for preliminary stopping decisions. Crucially, we propose two\nconsistent and validation-free criteria: a quantitative metric for heavy-tailed\ndynamics and a novel spectral signature indicative of convergence. The strong\nalignment between these criteria highlights the utility of RMT for monitoring\nand diagnosing the progression of Transformer model training.", "AI": {"tldr": "\u57fa\u4e8e\u968f\u673a\u77e9\u9635\u7406\u8bba\u63d0\u51faTransformer\u8bad\u7ec3\u52a8\u6001\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u77e9\u9635\u8c31\u5bc6\u5ea6\u6f14\u5316\u8bc6\u522b\u8bad\u7ec3\u4e09\u9636\u6bb5\uff0c\u5e76\u63d0\u51fa\u65e0\u9a8c\u8bc1\u7684\u65e9\u505c\u6807\u51c6", "motivation": "\u7406\u89e3Transformer\u8bad\u7ec3\u52a8\u6001\u7684\u5e95\u5c42\u673a\u5236\uff0c\u4e3a\u6027\u80fd\u6539\u8fdb\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u5e76\u63a8\u5bfc\u539f\u5219\u6027\u7684\u65e9\u505c\u6807\u51c6", "method": "\u5229\u7528\u968f\u673a\u77e9\u9635\u7406\u8bba\u5206\u6790Transformer\u8bad\u7ec3\u52a8\u6001\uff0c\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u77e9\u9635V\u7684\u8c31\u5bc6\u5ea6\u6f14\u5316\u8bc6\u522b\u8bad\u7ec3\u9636\u6bb5\uff0c\u4f7f\u7528\u5e42\u5f8b\u62df\u5408\u4f5c\u4e3a\u63a2\u9488", "result": "\u53d1\u73b0\u6d45\u5c42\u81ea\u6ce8\u610f\u529b\u77e9\u9635\u8c31\u5bc6\u5ea6\u4e00\u81f4\u6f14\u5316\u4e3a\u91cd\u5c3e\u5206\u5e03\uff0c\u8bc6\u522b\u51fa\u7ed3\u6784\u63a2\u7d22\u3001\u91cd\u5c3e\u7ed3\u6784\u7a33\u5b9a\u548c\u6536\u655b\u9971\u548c\u4e09\u4e2a\u8bad\u7ec3\u9636\u6bb5", "conclusion": "\u63d0\u51fa\u7684\u91cd\u5c3e\u52a8\u6001\u5b9a\u91cf\u6307\u6807\u548c\u6536\u655b\u8c31\u7279\u5f81\u4e24\u4e2a\u65e0\u9a8c\u8bc1\u6807\u51c6\u9ad8\u5ea6\u4e00\u81f4\uff0c\u8bc1\u660e\u968f\u673a\u77e9\u9635\u7406\u8bba\u5728\u76d1\u63a7Transformer\u8bad\u7ec3\u8fdb\u7a0b\u4e2d\u7684\u5b9e\u7528\u6027"}}
{"id": "2510.16932", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16932", "abs": "https://arxiv.org/abs/2510.16932", "authors": ["Emily Xiao", "Yixiao Zeng", "Ada Chen", "Chin-Jou Li", "Amanda Bertsch", "Graham Neubig"], "title": "Prompt-MII: Meta-Learning Instruction Induction for LLMs", "comment": null, "summary": "A popular method to adapt large language models (LLMs) to new tasks is\nin-context learning (ICL), which is effective but incurs high inference costs\nas context length grows. In this paper we propose a method to perform\ninstruction induction, where we take training examples and reduce them to a\ncompact but descriptive prompt that can achieve performance comparable to ICL\nover the full training set. Specifically, we propose PROMPT-MII, a\nreinforcement learning (RL) based framework to meta-learn an instruction\ninduction model that can generate compact instructions on the fly for an\narbitrary new dataset. We train on over 3,000 diverse classification datasets\nfrom the HuggingFace hub, and evaluate on 90 unseen tasks. PROMPT-MII improves\ndownstream model quality by 4-9 F1 points (10-20% relative), matching ICL\nperformance while requiring 3-13x fewer tokens.", "AI": {"tldr": "\u63d0\u51fa\u4e86PROMPT-MII\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5143\u5b66\u4e60\u6307\u4ee4\u5f52\u7eb3\u6a21\u578b\uff0c\u80fd\u591f\u4e3a\u65b0\u6570\u636e\u96c6\u751f\u6210\u7d27\u51d1\u6307\u4ee4\uff0c\u5728\u4fdd\u6301\u4e0e\u4e0a\u4e0b\u6587\u5b66\u4e60\u76f8\u5f53\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u63a8\u7406\u6210\u672c\u3002", "motivation": "\u4e0a\u4e0b\u6587\u5b66\u4e60\u867d\u7136\u6709\u6548\u4f46\u63a8\u7406\u6210\u672c\u9ad8\uff0c\u968f\u7740\u4e0a\u4e0b\u6587\u957f\u5ea6\u589e\u957f\u6210\u672c\u6025\u5267\u589e\u52a0\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u80fd\u591f\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u63a8\u7406\u5f00\u9500\u3002", "method": "\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5143\u5b66\u4e60\u6846\u67b6PROMPT-MII\uff0c\u57283,000\u591a\u4e2a\u591a\u6837\u5316\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u6307\u4ee4\u5f52\u7eb3\u6a21\u578b\uff0c\u80fd\u591f\u4e3a\u65b0\u6570\u636e\u96c6\u52a8\u6001\u751f\u6210\u7d27\u51d1\u4f46\u63cf\u8ff0\u6027\u7684\u63d0\u793a\u3002", "result": "\u572890\u4e2a\u672a\u89c1\u4efb\u52a1\u4e0a\u8bc4\u4f30\uff0cPROMPT-MII\u5c06\u4e0b\u6e38\u6a21\u578b\u8d28\u91cf\u63d0\u53474-9\u4e2aF1\u70b9\uff08\u76f8\u5bf9\u63d0\u534710-20%\uff09\uff0c\u5339\u914d\u4e0a\u4e0b\u6587\u5b66\u4e60\u6027\u80fd\u540c\u65f6\u6240\u9700token\u6570\u91cf\u51cf\u5c113-13\u500d\u3002", "conclusion": "PROMPT-MII\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u6210\u672c\u7684\u76ee\u6807\uff0c\u4e3a\u9ad8\u6548\u7684\u4efb\u52a1\u9002\u5e94\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2510.17794", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.17794", "abs": "https://arxiv.org/abs/2510.17794", "authors": ["Omer Haq"], "title": "Functional Distribution Networks (FDN)", "comment": "Submitted to ICLR 2026. Code will be released upon acceptance", "summary": "Modern probabilistic regressors often remain overconfident under distribution\nshift. We present Functional Distribution Networks (FDN), an input-conditioned\ndistribution over network weights that induces predictive mixtures whose\ndispersion adapts to the input. FDN is trained with a beta-ELBO and Monte Carlo\nsampling. We further propose an evaluation protocol that cleanly separates\ninterpolation from extrapolation and stresses OOD sanity checks (e.g., that\npredictive likelihood degrades under shift while in-distribution accuracy and\ncalibration are maintained). On standard regression tasks, we benchmark against\nstrong Bayesian, ensemble, dropout, and hypernetwork baselines under matched\nparameter and update budgets, and assess accuracy, calibration, and\nshift-awareness with standard diagnostics. Together, the framework and protocol\naim to make OOD-aware, well-calibrated neural regression practical and modular.", "AI": {"tldr": "\u63d0\u51faFunctional Distribution Networks (FDN)\uff0c\u4e00\u79cd\u8f93\u5165\u6761\u4ef6\u5316\u7684\u7f51\u7edc\u6743\u91cd\u5206\u5e03\u65b9\u6cd5\uff0c\u901a\u8fc7beta-ELBO\u548c\u8499\u7279\u5361\u6d1b\u91c7\u6837\u8bad\u7ec3\uff0c\u80fd\u591f\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u4ea7\u751f\u81ea\u9002\u5e94\u5206\u6563\u7684\u9884\u6d4b\u6df7\u5408\u5206\u5e03\u3002", "motivation": "\u73b0\u4ee3\u6982\u7387\u56de\u5f52\u5668\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u5f80\u5f80\u8fc7\u4e8e\u81ea\u4fe1\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u9002\u5e94\u8f93\u5165\u53d8\u5316\u7684\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u8f93\u5165\u6761\u4ef6\u5316\u7684\u7f51\u7edc\u6743\u91cd\u5206\u5e03\uff0c\u901a\u8fc7beta-ELBO\u635f\u5931\u51fd\u6570\u548c\u8499\u7279\u5361\u6d1b\u91c7\u6837\u8fdb\u884c\u8bad\u7ec3\uff0c\u8bf1\u5bfc\u4ea7\u751f\u81ea\u9002\u5e94\u5206\u6563\u7684\u9884\u6d4b\u6df7\u5408\u5206\u5e03\u3002", "result": "\u5728\u6807\u51c6\u56de\u5f52\u4efb\u52a1\u4e2d\uff0c\u4e0e\u8d1d\u53f6\u65af\u3001\u96c6\u6210\u3001dropout\u548c\u8d85\u7f51\u7edc\u57fa\u7ebf\u76f8\u6bd4\uff0c\u5728\u5339\u914d\u53c2\u6570\u548c\u66f4\u65b0\u9884\u7b97\u4e0b\u8bc4\u4f30\u4e86\u51c6\u786e\u6027\u3001\u6821\u51c6\u6027\u548c\u504f\u79fb\u611f\u77e5\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u548c\u8bc4\u4f30\u534f\u8bae\u65e8\u5728\u4f7f\u5177\u6709OOD\u611f\u77e5\u548c\u826f\u597d\u6821\u51c6\u7684\u795e\u7ecf\u56de\u5f52\u53d8\u5f97\u5b9e\u7528\u548c\u6a21\u5757\u5316\u3002"}}
{"id": "2510.17052", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17052", "abs": "https://arxiv.org/abs/2510.17052", "authors": ["Hassan Hamad", "Yingru Xu", "Liang Zhao", "Wenbo Yan", "Narendra Gyanchandani"], "title": "ToolCritic: Detecting and Correcting Tool-Use Errors in Dialogue Systems", "comment": null, "summary": "Tool-augmented large language models (LLMs) are increasingly employed in\nreal-world applications, but tool usage errors still hinder their reliability.\nWe introduce ToolCritic, a diagnostic framework that evaluates and improves LLM\nbehavior in multi-turn, tool-augmented dialogues. ToolCritic detects eight\ndistinct error types specific to tool-calling (e.g., premature invocation,\nargument misalignment, and misinterpretation of tool outputs) and provides\ntargeted feedback to the main LLM. The main LLM, assumed to have strong\nreasoning, task understanding and orchestration capabilities, then revises its\nresponse based on ToolCritic's feedback. We systematically define these error\ncategories and construct a synthetic dataset to train ToolCritic. Experimental\nresults on the Schema-Guided Dialogue (SGD) dataset demonstrate that ToolCritic\nimproves tool-calling accuracy by up to 13% over baselines, including zero-shot\nprompting and self-correction techniques. This represents a promising step\ntoward more robust LLM integration with external tools in real-world dialogue\napplications.", "AI": {"tldr": "ToolCritic\u662f\u4e00\u4e2a\u8bca\u65ad\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u6539\u8fdbLLM\u5728\u591a\u8f6e\u5de5\u5177\u589e\u5f3a\u5bf9\u8bdd\u4e2d\u7684\u884c\u4e3a\uff0c\u901a\u8fc7\u68c0\u6d4b8\u79cd\u7279\u5b9a\u5de5\u5177\u8c03\u7528\u9519\u8bef\u5e76\u63d0\u4f9b\u9488\u5bf9\u6027\u53cd\u9988\uff0c\u4f7f\u4e3bLLM\u80fd\u591f\u4fee\u6b63\u54cd\u5e94\u3002", "motivation": "\u5de5\u5177\u589e\u5f3a\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u8d8a\u6765\u8d8a\u666e\u904d\uff0c\u4f46\u5de5\u5177\u4f7f\u7528\u9519\u8bef\u4ecd\u7136\u963b\u788d\u5176\u53ef\u9760\u6027\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u9519\u8bef\u4ee5\u63d0\u9ad8LLM\u4e0e\u5916\u90e8\u5de5\u5177\u96c6\u6210\u7684\u9c81\u68d2\u6027\u3002", "method": "\u7cfb\u7edf\u5b9a\u4e498\u79cd\u5de5\u5177\u8c03\u7528\u9519\u8bef\u7c7b\u522b\uff0c\u6784\u5efa\u5408\u6210\u6570\u636e\u96c6\u8bad\u7ec3ToolCritic\uff0c\u8be5\u6846\u67b6\u68c0\u6d4b\u9519\u8bef\u5e76\u63d0\u4f9b\u53cd\u9988\uff0c\u4e3bLLM\u57fa\u4e8e\u53cd\u9988\u4fee\u6b63\u54cd\u5e94\u3002", "result": "\u5728Schema-Guided Dialogue\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cToolCritic\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff08\u5305\u62ec\u96f6\u6837\u672c\u63d0\u793a\u548c\u81ea\u6821\u6b63\u6280\u672f\uff09\u5c06\u5de5\u5177\u8c03\u7528\u51c6\u786e\u7387\u63d0\u9ad8\u4e86\u9ad8\u8fbe13%\u3002", "conclusion": "ToolCritic\u4ee3\u8868\u4e86\u5728\u73b0\u5b9e\u4e16\u754c\u5bf9\u8bdd\u5e94\u7528\u4e2d\u5b9e\u73b0\u66f4\u9c81\u68d2\u7684LLM\u4e0e\u5916\u90e8\u5de5\u5177\u96c6\u6210\u7684\u6709\u5e0c\u671b\u7684\u4e00\u6b65\u3002"}}
{"id": "2510.16075", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16075", "abs": "https://arxiv.org/abs/2510.16075", "authors": ["Sergio Mu\u00f1iz Subi\u00f1as", "Manuel L. Gonz\u00e1lez", "Jorge Ruiz G\u00f3mez", "Alejandro Mata Ali", "Jorge Mart\u00ednez Mart\u00edn", "Miguel Franco Hernando", "\u00c1ngel Miguel Garc\u00eda-Vico"], "title": "Optimization of the quantization of dense neural networks from an exact QUBO formulation", "comment": null, "summary": "This work introduces a post-training quantization (PTQ) method for dense\nneural networks via a novel ADAROUND-based QUBO formulation. Using the\nFrobenius distance between the theoretical output and the dequantized output\n(before the activation function) as the objective, an explicit QUBO whose\nbinary variables represent the rounding choice for each weight and bias is\nobtained. Additionally, by exploiting the structure of the coefficient QUBO\nmatrix, the global problem can be exactly decomposed into $n$ independent\nsubproblems of size $f+1$, which can be efficiently solved using some\nheuristics such as simulated annealing. The approach is evaluated on MNIST,\nFashion-MNIST, EMNIST, and CIFAR-10 across integer precisions from int8 to int1\nand compared with a round-to-nearest traditional quantization methodology.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eADAROUND\u7684QUBO\u516c\u5f0f\u7684\u795e\u7ecf\u7f51\u7edc\u540e\u8bad\u7ec3\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7Frobenius\u8ddd\u79bb\u4f5c\u4e3a\u76ee\u6807\u51fd\u6570\uff0c\u5c06\u91cf\u5316\u95ee\u9898\u8f6c\u5316\u4e3a\u53ef\u5206\u89e3\u7684QUBO\u95ee\u9898\uff0c\u5e76\u4f7f\u7528\u6a21\u62df\u9000\u706b\u7b49\u542f\u53d1\u5f0f\u65b9\u6cd5\u9ad8\u6548\u6c42\u89e3\u3002", "motivation": "\u4f20\u7edf\u91cf\u5316\u65b9\u6cd5\u5982round-to-nearest\u5b58\u5728\u7cbe\u5ea6\u635f\u5931\uff0c\u9700\u8981\u66f4\u7cbe\u786e\u7684\u91cf\u5316\u65b9\u6cd5\u6765\u4fdd\u6301\u795e\u7ecf\u7f51\u7edc\u6027\u80fd\u3002", "method": "\u4f7f\u7528Frobenius\u8ddd\u79bb\u4f5c\u4e3a\u76ee\u6807\u51fd\u6570\uff0c\u6784\u5efa\u660e\u786e\u7684QUBO\u95ee\u9898\uff0c\u5176\u4e2d\u4e8c\u5143\u53d8\u91cf\u8868\u793a\u6743\u91cd\u548c\u504f\u7f6e\u7684\u820d\u5165\u9009\u62e9\u3002\u5229\u7528\u7cfb\u6570\u77e9\u9635\u7ed3\u6784\u5c06\u5168\u5c40\u95ee\u9898\u5206\u89e3\u4e3a\u591a\u4e2a\u72ec\u7acb\u5b50\u95ee\u9898\u3002", "result": "\u5728MNIST\u3001Fashion-MNIST\u3001EMNIST\u548cCIFAR-10\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u4eceint8\u5230int1\u7684\u6574\u6570\u7cbe\u5ea6\u8303\u56f4\u5185\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u91cf\u5316\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u540e\u8bad\u7ec3\u91cf\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u91cf\u5316\u7cbe\u5ea6\uff0c\u7279\u522b\u662f\u5728\u4f4e\u7cbe\u5ea6\u8bbe\u7f6e\u4e0b\u3002"}}
{"id": "2510.16985", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16985", "abs": "https://arxiv.org/abs/2510.16985", "authors": ["Akif Islam", "Mohd Ruhul Ameen"], "title": "Parameter-Efficient Fine-Tuning for Low-Resource Languages: A Comparative Study of LLMs for Bengali Hate Speech Detection", "comment": "Accepted to IEEE COMPAS 2025. 6 pages, 3 figures, 6 tables", "summary": "Bengali social media platforms have witnessed a sharp increase in hate\nspeech, disproportionately affecting women and adolescents. While datasets such\nas BD-SHS provide a basis for structured evaluation, most prior approaches rely\non either computationally costly full-model fine-tuning or proprietary APIs.\nThis paper presents the first application of Parameter-Efficient Fine-Tuning\n(PEFT) for Bengali hate speech detection using LoRA and QLoRA. Three\ninstruction-tuned large language models - Gemma-3-4B, Llama-3.2-3B, and\nMistral-7B - were fine-tuned on the BD-SHS dataset of 50,281 annotated\ncomments. Each model was adapted by training fewer than 1% of its parameters,\nenabling experiments on a single consumer-grade GPU. The results show that\nLlama-3.2-3B achieved the highest F1-score of 92.23%, followed by Mistral-7B at\n88.94% and Gemma-3-4B at 80.25%. These findings establish PEFT as a practical\nand replicable strategy for Bengali and related low-resource languages.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5e94\u7528\u53c2\u6570\u9ad8\u6548\u5fae\u8c03(PEFT)\u6280\u672f\uff0c\u4f7f\u7528LoRA\u548cQLoRA\u65b9\u6cd5\u5728\u5b5f\u52a0\u62c9\u8bed\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u4efb\u52a1\u4e0a\uff0c\u5728BD-SHS\u6570\u636e\u96c6\u4e0a\u5bf9\u4e09\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u4ec5\u8bad\u7ec3\u5c11\u4e8e1%\u7684\u53c2\u6570\u5373\u53ef\u83b7\u5f97\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u5b5f\u52a0\u62c9\u8bed\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u4ec7\u6068\u8a00\u8bba\u6fc0\u589e\uff0c\u7279\u522b\u662f\u9488\u5bf9\u5973\u6027\u548c\u9752\u5c11\u5e74\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u5168\u6a21\u578b\u5fae\u8c03\uff0c\u8981\u4e48\u4f7f\u7528\u4e13\u6709API\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528LoRA\u548cQLoRA\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6280\u672f\uff0c\u5728BD-SHS\u6570\u636e\u96c6(50,281\u6761\u6807\u6ce8\u8bc4\u8bba)\u4e0a\u5bf9Gemma-3-4B\u3001Llama-3.2-3B\u548cMistral-7B\u4e09\u4e2a\u6307\u4ee4\u8c03\u4f18\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u4ec5\u8bad\u7ec3\u5c11\u4e8e1%\u7684\u53c2\u6570\u3002", "result": "Llama-3.2-3B\u83b7\u5f97\u6700\u9ad8F1\u5206\u657092.23%\uff0cMistral-7B\u4e3a88.94%\uff0cGemma-3-4B\u4e3a80.25%\u3002\u6240\u6709\u5b9e\u9a8c\u53ef\u5728\u5355\u4e2a\u6d88\u8d39\u7ea7GPU\u4e0a\u5b8c\u6210\u3002", "conclusion": "PEFT\u88ab\u8bc1\u660e\u662f\u5b5f\u52a0\u62c9\u8bed\u53ca\u76f8\u5173\u4f4e\u8d44\u6e90\u8bed\u8a00\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u7684\u5b9e\u7528\u4e14\u53ef\u590d\u73b0\u7b56\u7565\uff0c\u5927\u5e45\u964d\u4f4e\u4e86\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u3002"}}
{"id": "2510.17064", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17064", "abs": "https://arxiv.org/abs/2510.17064", "authors": ["Rongbin Li", "Wenbo Chen", "Zhao Li", "Rodrigo Munoz-Castaneda", "Jinbo Li", "Neha S. Maurya", "Arnav Solanki", "Huan He", "Hanwen Xing", "Meaghan Ramlakhan", "Zachary Wise", "Zhuhao Wu", "Hua Xu", "Michael Hawrylycz", "W. Jim Zheng"], "title": "A Brain Cell Type Resource Created by Large Language Models and a Multi-Agent AI System for Collaborative Community Annotation", "comment": "22 pages, 6 figures, 2 tables", "summary": "Single-cell RNA sequencing has transformed our ability to identify diverse\ncell types and their transcriptomic signatures. However, annotating these\nsignatures-especially those involving poorly characterized genes-remains a\nmajor challenge. Traditional methods, such as Gene Set Enrichment Analysis\n(GSEA), depend on well-curated annotations and often perform poorly in these\ncontexts. Large Language Models (LLMs) offer a promising alternative but\nstruggle to represent complex biological knowledge within structured\nontologies. To address this, we present BRAINCELL-AID (BRAINCELL-AID:\nhttps://biodataai.uth.edu/BRAINCELL-AID), a novel multi-agent AI system that\nintegrates free-text descriptions with ontology labels to enable more accurate\nand robust gene set annotation. By incorporating retrieval-augmented generation\n(RAG), we developed a robust agentic workflow that refines predictions using\nrelevant PubMed literature, reducing hallucinations and enhancing\ninterpretability. Using this workflow, we achieved correct annotations for 77%\nof mouse gene sets among their top predictions. Applying this approach, we\nannotated 5,322 brain cell clusters from the comprehensive mouse brain cell\natlas generated by the BRAIN Initiative Cell Census Network, enabling novel\ninsights into brain cell function by identifying region-specific gene\nco-expression patterns and inferring functional roles of gene ensembles.\nBRAINCELL-AID also identifies Basal Ganglia-related cell types with\nneurologically meaningful descriptions. Hence, we create a valuable resource to\nsupport community-driven cell type annotation.", "AI": {"tldr": "BRAINCELL-AID\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53AI\u7cfb\u7edf\uff0c\u901a\u8fc7\u6574\u5408\u81ea\u7531\u6587\u672c\u63cf\u8ff0\u548c\u672c\u4f53\u6807\u7b7e\uff0c\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u57fa\u56e0\u96c6\u6ce8\u91ca\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\u6280\u672f\u867d\u7136\u80fd\u591f\u8bc6\u522b\u591a\u6837\u7ec6\u80de\u7c7b\u578b\uff0c\u4f46\u5bf9\u6d89\u53ca\u7279\u5f81\u4e0d\u660e\u786e\u57fa\u56e0\u7684\u8f6c\u5f55\u7ec4\u7279\u5f81\u8fdb\u884c\u6ce8\u91ca\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002\u4f20\u7edf\u65b9\u6cd5\u5982GSEA\u4f9d\u8d56\u7cbe\u5fc3\u7b56\u5212\u7684\u6ce8\u91ca\uff0c\u5728\u8fd9\u4e9b\u60c5\u51b5\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u5f00\u53d1\u4e86BRAINCELL-AID\u591a\u667a\u80fd\u4f53AI\u7cfb\u7edf\uff0c\u6574\u5408\u81ea\u7531\u6587\u672c\u63cf\u8ff0\u4e0e\u672c\u4f53\u6807\u7b7e\uff0c\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\uff0c\u901a\u8fc7\u76f8\u5173PubMed\u6587\u732e\u7cbe\u70bc\u9884\u6d4b\uff0c\u51cf\u5c11\u5e7b\u89c9\u5e76\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5bf9\u5c0f\u9f20\u57fa\u56e0\u96c6\u5b9e\u73b0\u4e8677%\u7684\u6b63\u786e\u6ce8\u91ca\u7387\uff0c\u6210\u529f\u6ce8\u91ca\u4e86BRAIN Initiative Cell Census Network\u751f\u6210\u76845,322\u4e2a\u8111\u7ec6\u80de\u7c07\uff0c\u8bc6\u522b\u4e86\u533a\u57df\u7279\u5f02\u6027\u57fa\u56e0\u5171\u8868\u8fbe\u6a21\u5f0f\uff0c\u5e76\u63a8\u65ad\u51fa\u57fa\u56e0\u96c6\u5408\u7684\u529f\u80fd\u4f5c\u7528\u3002", "conclusion": "BRAINCELL-AID\u521b\u5efa\u4e86\u4e00\u4e2a\u6709\u4ef7\u503c\u7684\u8d44\u6e90\u6765\u652f\u6301\u793e\u533a\u9a71\u52a8\u7684\u7ec6\u80de\u7c7b\u578b\u6ce8\u91ca\uff0c\u7279\u522b\u662f\u5728\u8bc6\u522b\u4e0e\u57fa\u5e95\u795e\u7ecf\u8282\u76f8\u5173\u7684\u7ec6\u80de\u7c7b\u578b\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.16076", "categories": ["cs.LG", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.16076", "abs": "https://arxiv.org/abs/2510.16076", "authors": ["SeongKu Kang", "Jianxun Lian", "Dongha Lee", "Wonbin Kweon", "Sanghwan Jang", "Jaehyun Lee", "Jindong Wang", "Xing Xie", "Hwanjo Yu"], "title": "BPL: Bias-adaptive Preference Distillation Learning for Recommender System", "comment": "\\c{opyright} 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Recommender systems suffer from biases that cause the collected feedback to\nincompletely reveal user preference. While debiasing learning has been\nextensively studied, they mostly focused on the specialized (called\ncounterfactual) test environment simulated by random exposure of items,\nsignificantly degrading accuracy in the typical (called factual) test\nenvironment based on actual user-item interactions. In fact, each test\nenvironment highlights the benefit of a different aspect: the counterfactual\ntest emphasizes user satisfaction in the long-terms, while the factual test\nfocuses on predicting subsequent user behaviors on platforms. Therefore, it is\ndesirable to have a model that performs well on both tests rather than only\none. In this work, we introduce a new learning framework, called Bias-adaptive\nPreference distillation Learning (BPL), to gradually uncover user preferences\nwith dual distillation strategies. These distillation strategies are designed\nto drive high performance in both factual and counterfactual test environments.\nEmploying a specialized form of teacher-student distillation from a biased\nmodel, BPL retains accurate preference knowledge aligned with the collected\nfeedback, leading to high performance in the factual test. Furthermore, through\nself-distillation with reliability filtering, BPL iteratively refines its\nknowledge throughout the training process. This enables the model to produce\nmore accurate predictions across a broader range of user-item combinations,\nthereby improving performance in the counterfactual test. Comprehensive\nexperiments validate the effectiveness of BPL in both factual and\ncounterfactual tests. Our implementation is accessible via:\nhttps://github.com/SeongKu-Kang/BPL.", "AI": {"tldr": "\u63d0\u51fa\u4e86BPL\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u91cd\u84b8\u998f\u7b56\u7565\u89e3\u51b3\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u504f\u5dee\u95ee\u9898\uff0c\u5728\u4e8b\u5b9e\u548c\u53cd\u4e8b\u5b9e\u6d4b\u8bd5\u73af\u5883\u4e2d\u90fd\u80fd\u53d6\u5f97\u826f\u597d\u6027\u80fd", "motivation": "\u63a8\u8350\u7cfb\u7edf\u5b58\u5728\u504f\u5dee\u5bfc\u81f4\u6536\u96c6\u7684\u53cd\u9988\u4e0d\u80fd\u5b8c\u5168\u63ed\u793a\u7528\u6237\u504f\u597d\uff0c\u73b0\u6709\u53bb\u504f\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u53cd\u4e8b\u5b9e\u6d4b\u8bd5\u73af\u5883\uff0c\u5728\u57fa\u4e8e\u5b9e\u9645\u7528\u6237-\u7269\u54c1\u4ea4\u4e92\u7684\u4e8b\u5b9e\u6d4b\u8bd5\u73af\u5883\u4e2d\u51c6\u786e\u7387\u663e\u8457\u4e0b\u964d", "method": "\u4f7f\u7528\u504f\u7f6e\u6a21\u578b\u7684\u5e08\u751f\u84b8\u998f\u4fdd\u7559\u4e0e\u6536\u96c6\u53cd\u9988\u4e00\u81f4\u7684\u77e5\u8bc6\uff0c\u901a\u8fc7\u5e26\u53ef\u9760\u6027\u8fc7\u6ee4\u7684\u81ea\u84b8\u998f\u8fed\u4ee3\u7cbe\u70bc\u77e5\u8bc6", "result": "\u7efc\u5408\u5b9e\u9a8c\u9a8c\u8bc1\u4e86BPL\u5728\u4e8b\u5b9e\u548c\u53cd\u4e8b\u5b9e\u6d4b\u8bd5\u4e2d\u7684\u6709\u6548\u6027", "conclusion": "BPL\u6846\u67b6\u80fd\u591f\u9010\u6b65\u63ed\u793a\u7528\u6237\u504f\u597d\uff0c\u5728\u4e24\u79cd\u6d4b\u8bd5\u73af\u5883\u4e2d\u90fd\u8868\u73b0\u51fa\u8272"}}
{"id": "2510.16987", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16987", "abs": "https://arxiv.org/abs/2510.16987", "authors": ["Amit Moryossef", "Clara Meister", "Pavel Stepachev", "Desmond Elliott"], "title": "Back to Bytes: Revisiting Tokenization Through UTF-8", "comment": null, "summary": "We present UTF8Tokenizer, a minimalist byte-level tokenizer that maps text\nexactly to IDs corresponding to the bytes underlying the text's UTF-8 encoding\n(e.g., byte x09 is token ID 9). Unlike prior byte-level approaches (Xue et al.,\n2021; Pagnoni et al., 2025), our implementation never introduces out-of-range\nIDs (i.e. there is no token ID 256) or auxiliary tokens: all special behavior\n(e.g., padding, boundaries, conversation structure, attention segments, tool\ncalling, \"thinking\" spans, etc.) is encoded using C0 control bytes - just as\nASCII was originally designed to embed control information alongside printable\ntext. These design principles yield practical benefits: (1) faster tokenization\n(14x) and significantly lower host-device transfer (8x less than int64); (2)\nsimple, shareable 256*d embedding tables that can be aligned across models; and\n(3) a training-time enhancement via bit-biased embeddings, which exposes\nper-byte bit structure and can be added to the embedding table post-training,\nremoving inference costs. Our HuggingFace-compatible implementation improves\nlanguage modeling convergence.", "AI": {"tldr": "UTF8Tokenizer\u662f\u4e00\u4e2a\u6781\u7b80\u7684\u5b57\u8282\u7ea7\u5206\u8bcd\u5668\uff0c\u76f4\u63a5\u5c06\u6587\u672c\u6620\u5c04\u5230UTF-8\u7f16\u7801\u5bf9\u5e94\u7684\u5b57\u8282ID\uff0c\u4f7f\u7528C0\u63a7\u5236\u5b57\u8282\u7f16\u7801\u7279\u6b8a\u884c\u4e3a\uff0c\u63d0\u4f9b\u66f4\u5feb\u7684\u5206\u8bcd\u901f\u5ea6\u548c\u66f4\u5c0f\u7684\u4f20\u8f93\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u7684\u5b57\u8282\u7ea7\u5206\u8bcd\u65b9\u6cd5\u5b58\u5728\u8d85\u51fa\u8303\u56f4\u7684ID\u6216\u5f15\u5165\u8f85\u52a9token\u7684\u95ee\u9898\uff0c\u4f5c\u8005\u5e0c\u671b\u8bbe\u8ba1\u4e00\u4e2a\u66f4\u7b80\u6d01\u9ad8\u6548\u7684\u5206\u8bcd\u5668\u3002", "method": "\u91c7\u7528UTF-8\u5b57\u8282\u7f16\u7801\u4f5c\u4e3atoken ID\uff0c\u4f7f\u7528C0\u63a7\u5236\u5b57\u8282\u5904\u7406\u7279\u6b8a\u884c\u4e3a\uff0c\u5b9e\u73b0256\u7ef4\u7684\u5d4c\u5165\u8868\uff0c\u5e76\u5f15\u5165\u4f4d\u504f\u7f6e\u5d4c\u5165\u6765\u66b4\u9732\u5b57\u8282\u4f4d\u7ed3\u6784\u3002", "result": "\u5206\u8bcd\u901f\u5ea6\u63d0\u534714\u500d\uff0c\u4e3b\u673a-\u8bbe\u5907\u4f20\u8f93\u51cf\u5c118\u500d\uff0c\u5d4c\u5165\u8868\u53ef\u8de8\u6a21\u578b\u5bf9\u9f50\uff0c\u8bed\u8a00\u5efa\u6a21\u6536\u655b\u6027\u5f97\u5230\u6539\u5584\u3002", "conclusion": "UTF8Tokenizer\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u7b80\u6d01\u7684\u5206\u8bcd\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u6027\u80fd\u548c\u5b9e\u7528\u6027\u65b9\u9762\u90fd\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2510.17108", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17108", "abs": "https://arxiv.org/abs/2510.17108", "authors": ["Yoonjin Lee", "Munhee Kim", "Hanbi Choi", "Juhyeon Park", "Seungho Lyoo", "Woojin Park"], "title": "Structured Debate Improves Corporate Credit Reasoning in Financial AI", "comment": "18 pages, 4 figures, 2 algorithms, 2 tables, 4 appendices, will be\n  submitted to AAAI-2026 workshop", "summary": "Despite advances in financial AI, the automation of evidence-based reasoning\nremains unresolved in corporate credit assessment, where qualitative\nnon-financial indicators exert decisive influence on loan repayment outcomes\nyet resist formalization. Existing approaches focus predominantly on numerical\nprediction and provide limited support for the interpretive judgments required\nin professional loan evaluation. This study develops and evaluates two\noperational large language model (LLM)-based systems designed to generate\nstructured reasoning from non-financial evidence. The first is a\nnon-adversarial single-agent system (NAS) that produces bidirectional analysis\nthrough a single-pass reasoning pipeline. The second is a debate-based\nmulti-agent system (KPD-MADS) that operationalizes adversarial verification\nthrough a ten-step structured interaction protocol grounded in Karl Popper's\ncritical dialogue framework. Both systems were applied to three real corporate\ncases and evaluated by experienced credit risk professionals. Compared to\nmanual expert reporting, both systems achieved substantial productivity gains\n(NAS: 11.55 s per case; KPD-MADS: 91.97 s; human baseline: 1920 s). The\nKPD-MADS demonstrated superior reasoning quality, receiving higher median\nratings in explanatory adequacy (4.0 vs. 3.0), practical applicability (4.0 vs.\n3.0), and usability (62.5 vs. 52.5). These findings show that structured\nmulti-agent interaction can enhance reasoning rigor and interpretability in\nfinancial AI, advancing scalable and defensible automation in corporate credit\nassessment.", "AI": {"tldr": "\u5f00\u53d1\u5e76\u8bc4\u4f30\u4e86\u4e24\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7cfb\u7edf\uff0c\u7528\u4e8e\u4ece\u975e\u8d22\u52a1\u8bc1\u636e\u751f\u6210\u7ed3\u6784\u5316\u63a8\u7406\uff0c\u4ee5\u81ea\u52a8\u5316\u4f01\u4e1a\u4fe1\u7528\u8bc4\u4f30\u4e2d\u7684\u8bc1\u636e\u63a8\u7406\u8fc7\u7a0b\u3002", "motivation": "\u73b0\u6709\u91d1\u878dAI\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6570\u503c\u9884\u6d4b\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u4f01\u4e1a\u4fe1\u7528\u8bc4\u4f30\u4e2d\u5177\u6709\u51b3\u5b9a\u6027\u5f71\u54cd\u4f46\u96be\u4ee5\u5f62\u5f0f\u5316\u7684\u5b9a\u6027\u975e\u8d22\u52a1\u6307\u6807\uff0c\u7f3a\u4e4f\u5bf9\u4e13\u4e1a\u8d37\u6b3e\u8bc4\u4f30\u6240\u9700\u89e3\u91ca\u6027\u5224\u65ad\u7684\u652f\u6301\u3002", "method": "\u5f00\u53d1\u4e86\u4e24\u79cdLLM\u7cfb\u7edf\uff1a\u975e\u5bf9\u6297\u6027\u5355\u4ee3\u7406\u7cfb\u7edf(NAS)\u901a\u8fc7\u5355\u6b21\u63a8\u7406\u7ba1\u9053\u751f\u6210\u53cc\u5411\u5206\u6790\uff1b\u57fa\u4e8e\u8fa9\u8bba\u7684\u591a\u4ee3\u7406\u7cfb\u7edf(KPD-MADS)\u57fa\u4e8e\u5361\u5c14\u00b7\u6ce2\u666e\u6279\u5224\u5bf9\u8bdd\u6846\u67b6\uff0c\u901a\u8fc7\u5341\u6b65\u7ed3\u6784\u5316\u4ea4\u4e92\u534f\u8bae\u5b9e\u73b0\u5bf9\u6297\u6027\u9a8c\u8bc1\u3002", "result": "\u76f8\u6bd4\u4eba\u5de5\u4e13\u5bb6\u62a5\u544a\uff0c\u4e24\u79cd\u7cfb\u7edf\u5747\u5b9e\u73b0\u663e\u8457\u751f\u4ea7\u529b\u63d0\u5347(NAS: 11.55\u79d2/\u6848\u4f8b\uff1bKPD-MADS: 91.97\u79d2\uff1b\u4eba\u5de5\u57fa\u51c6: 1920\u79d2)\u3002KPD-MADS\u5728\u63a8\u7406\u8d28\u91cf\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u5728\u89e3\u91ca\u5145\u5206\u6027(4.0 vs 3.0)\u3001\u5b9e\u9645\u9002\u7528\u6027(4.0 vs 3.0)\u548c\u53ef\u7528\u6027(62.5 vs 52.5)\u65b9\u9762\u83b7\u5f97\u66f4\u9ad8\u8bc4\u5206\u3002", "conclusion": "\u7ed3\u6784\u5316\u591a\u4ee3\u7406\u4ea4\u4e92\u80fd\u591f\u589e\u5f3a\u91d1\u878dAI\u4e2d\u7684\u63a8\u7406\u4e25\u8c28\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u63a8\u52a8\u4f01\u4e1a\u4fe1\u7528\u8bc4\u4f30\u4e2d\u53ef\u6269\u5c55\u4e14\u53ef\u8fa9\u62a4\u7684\u81ea\u52a8\u5316\u8fdb\u7a0b\u3002"}}
{"id": "2510.16077", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16077", "abs": "https://arxiv.org/abs/2510.16077", "authors": ["Naeem Paeedeh", "Mahardhika Pratama", "Weiping Ding", "Jimmy Cao", "Wolfgang Mayer", "Ryszard Kowalczyk"], "title": "Continual Knowledge Consolidation LORA for Domain Incremental Learning", "comment": null, "summary": "Domain Incremental Learning (DIL) is a continual learning sub-branch that\naims to address never-ending arrivals of new domains without catastrophic\nforgetting problems. Despite the advent of parameter-efficient fine-tuning\n(PEFT) approaches, existing works create task-specific LoRAs overlooking shared\nknowledge across tasks. Inaccurate selection of task-specific LORAs during\ninference results in significant drops in accuracy, while existing works rely\non linear or prototype-based classifiers, which have suboptimal generalization\npowers. Our paper proposes continual knowledge consolidation low rank\nadaptation (CONEC-LoRA) addressing the DIL problems. CONEC-LoRA is developed\nfrom consolidations between task-shared LORA to extract common knowledge and\ntask-specific LORA to embrace domain-specific knowledge. Unlike existing\napproaches, CONEC-LoRA integrates the concept of a stochastic classifier whose\nparameters are sampled from a distribution, thus enhancing the likelihood of\ncorrect classifications. Last but not least, an auxiliary network is deployed\nto optimally predict the task-specific LoRAs for inferences and implements the\nconcept of a different-depth network structure in which every layer is\nconnected with a local classifier to take advantage of intermediate\nrepresentations. This module integrates the ball-generator loss and\ntransformation module to address the synthetic sample bias problem. Our\nrigorous experiments demonstrate the advantage of CONEC-LoRA over prior arts in\n4 popular benchmark problems with over 5% margins.", "AI": {"tldr": "\u63d0\u51faCONEC-LoRA\u65b9\u6cd5\u89e3\u51b3\u9886\u57df\u589e\u91cf\u5b66\u4e60\u95ee\u9898\uff0c\u901a\u8fc7\u4efb\u52a1\u5171\u4eab\u548c\u4efb\u52a1\u7279\u5b9a\u7684LoRA\u6574\u5408\uff0c\u7ed3\u5408\u968f\u673a\u5206\u7c7b\u5668\u548c\u8f85\u52a9\u7f51\u7edc\uff0c\u57284\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6027\u80fd\u63d0\u5347\u8d85\u8fc75%\u3002", "motivation": "\u73b0\u6709\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u521b\u5efa\u4efb\u52a1\u7279\u5b9a\u7684LoRA\uff0c\u5ffd\u7565\u4e86\u8de8\u4efb\u52a1\u7684\u5171\u4eab\u77e5\u8bc6\uff0c\u4e14\u63a8\u7406\u65f6\u4efb\u52a1\u7279\u5b9aLoRA\u9009\u62e9\u4e0d\u51c6\u786e\u5bfc\u81f4\u7cbe\u5ea6\u663e\u8457\u4e0b\u964d\uff0c\u73b0\u6709\u5206\u7c7b\u5668\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "1. \u4efb\u52a1\u5171\u4eabLoRA\u548c\u4efb\u52a1\u7279\u5b9aLoRA\u6574\u5408\u63d0\u53d6\u5171\u540c\u77e5\u8bc6\u548c\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\uff1b2. \u968f\u673a\u5206\u7c7b\u5668\u4ece\u5206\u5e03\u4e2d\u91c7\u6837\u53c2\u6570\u589e\u5f3a\u5206\u7c7b\u6b63\u786e\u6027\uff1b3. \u8f85\u52a9\u7f51\u7edc\u9884\u6d4b\u4efb\u52a1\u7279\u5b9aLoRA\uff0c\u91c7\u7528\u4e0d\u540c\u6df1\u5ea6\u7f51\u7edc\u7ed3\u6784\u5229\u7528\u4e2d\u95f4\u8868\u793a\uff1b4. \u96c6\u6210\u7403\u751f\u6210\u5668\u635f\u5931\u548c\u53d8\u6362\u6a21\u5757\u89e3\u51b3\u5408\u6210\u6837\u672c\u504f\u5dee\u95ee\u9898\u3002", "result": "\u57284\u4e2a\u6d41\u884c\u57fa\u51c6\u95ee\u9898\u4e0a\uff0cCONEC-LoRA\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u6709\u8d85\u8fc75%\u7684\u6027\u80fd\u4f18\u52bf\u3002", "conclusion": "CONEC-LoRA\u901a\u8fc7\u77e5\u8bc6\u6574\u5408\u3001\u968f\u673a\u5206\u7c7b\u5668\u548c\u8f85\u52a9\u7f51\u7edc\u6709\u6548\u89e3\u51b3\u4e86\u9886\u57df\u589e\u91cf\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2510.17001", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17001", "abs": "https://arxiv.org/abs/2510.17001", "authors": ["Yuval Reif", "Guy Kaplan", "Roy Schwartz"], "title": "Vocab Diet: Reshaping the Vocabulary of LLMs with Vector Arithmetic", "comment": null, "summary": "Large language models (LLMs) were shown to encode word form variations, such\nas \"walk\"->\"walked\", as linear directions in embedding space. However, standard\ntokenization algorithms treat these variations as distinct tokens -- filling\nthe size-capped vocabulary with surface form variants (e.g., \"walk\", \"walking\",\n\"Walk\"), at the expense of less frequent words and multilingual coverage. We\nshow that many of these variations can be captured by transformation vectors --\nadditive offsets that yield the appropriate word's representation when applied\nto the base form word embedding -- in both the input and output spaces.\nBuilding on this, we propose a compact reshaping of the vocabulary: rather than\nassigning unique tokens to each surface form, we compose them from shared base\nform and transformation vectors (e.g., \"walked\" = \"walk\" + past tense). We\napply our approach to multiple LLMs and across five languages, removing up to\n10% of vocabulary entries -- thereby freeing space to allocate new, more\ndiverse tokens. Importantly, we do so while also expanding vocabulary coverage\nto out-of-vocabulary words, with minimal impact on downstream performance, and\nwithout modifying model weights. Our findings motivate a foundational\nrethinking of vocabulary design, moving from string enumeration to a\ncompositional vocabulary that leverages the underlying structure of language.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bcd\u6c47\u8868\u538b\u7f29\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u8bcd\u5f62\u53d8\u5316\u8868\u793a\u4e3a\u8f6c\u6362\u5411\u91cf\u6765\u51cf\u5c11\u8bcd\u6c47\u8868\u5927\u5c0f\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u6807\u51c6\u5206\u8bcd\u7b97\u6cd5\u5c06\u8bcd\u5f62\u53d8\u5316\u89c6\u4e3a\u72ec\u7acb\u6807\u8bb0\uff0c\u5bfc\u81f4\u8bcd\u6c47\u8868\u88ab\u8868\u9762\u5f62\u5f0f\u53d8\u4f53\u586b\u6ee1\uff0c\u727a\u7272\u4e86\u4f4e\u9891\u8bcd\u548c\u591a\u8bed\u8a00\u8986\u76d6\u3002", "method": "\u4f7f\u7528\u8f6c\u6362\u5411\u91cf\uff08\u52a0\u6cd5\u504f\u79fb\u91cf\uff09\u6765\u8868\u793a\u8bcd\u5f62\u53d8\u5316\uff0c\u5c06\u8bcd\u6c47\u8868\u91cd\u65b0\u8bbe\u8ba1\u4e3a\u5171\u4eab\u57fa\u7840\u5f62\u5f0f\u548c\u8f6c\u6362\u5411\u91cf\u7684\u7ec4\u5408\u7ed3\u6784\u3002", "result": "\u5728\u591a\u79cdLLM\u548c\u4e94\u79cd\u8bed\u8a00\u4e0a\u5e94\u7528\u8be5\u65b9\u6cd5\uff0c\u53ef\u79fb\u9664\u9ad8\u8fbe10%\u7684\u8bcd\u6c47\u8868\u6761\u76ee\uff0c\u6269\u5c55\u8bcd\u6c47\u8986\u76d6\u8303\u56f4\uff0c\u5bf9\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u5f71\u54cd\u6700\u5c0f\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4fc3\u4f7f\u91cd\u65b0\u601d\u8003\u8bcd\u6c47\u8868\u8bbe\u8ba1\uff0c\u4ece\u5b57\u7b26\u4e32\u679a\u4e3e\u8f6c\u5411\u5229\u7528\u8bed\u8a00\u5e95\u5c42\u7ed3\u6784\u7684\u7ec4\u5408\u5f0f\u8bcd\u6c47\u8868\u3002"}}
{"id": "2510.17145", "categories": ["cs.AI", "68T05, 62H30"], "pdf": "https://arxiv.org/pdf/2510.17145", "abs": "https://arxiv.org/abs/2510.17145", "authors": ["Phi-Hung Hoang", "Nam-Thuan Trinh", "Van-Manh Tran", "Thi-Thu-Hong Phan"], "title": "Enhanced Fish Freshness Classification with Incremental Handcrafted Feature Fusion", "comment": "35 pages, 6 figures and 11 tables", "summary": "Accurate assessment of fish freshness remains a major challenge in the food\nindustry, with direct consequences for product quality, market value, and\nconsumer health. Conventional sensory evaluation is inherently subjective,\ninconsistent, and difficult to standardize across contexts, often limited by\nsubtle, species-dependent spoilage cues. To address these limitations, we\npropose a handcrafted feature-based approach that systematically extracts and\nincrementally fuses complementary descriptors, including color statistics,\nhistograms across multiple color spaces, and texture features such as Local\nBinary Patterns (LBP) and Gray-Level Co-occurrence Matrices (GLCM), from fish\neye images. Our method captures global chromatic variations from full images\nand localized degradations from ROI segments, fusing each independently to\nevaluate their effectiveness in assessing freshness. Experiments on the\nFreshness of the Fish Eyes (FFE) dataset demonstrate the approach's\neffectiveness: in a standard train-test setting, a LightGBM classifier achieved\n77.56% accuracy, a 14.35% improvement over the previous deep learning baseline\nof 63.21%. With augmented data, an Artificial Neural Network (ANN) reached\n97.16% accuracy, surpassing the prior best of 77.3% by 19.86%. These results\ndemonstrate that carefully engineered, handcrafted features, when strategically\nprocessed, yield a robust, interpretable, and reliable solution for automated\nfish freshness assessment, providing valuable insights for practical\napplications in food quality monitoring.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u624b\u5de5\u7279\u5f81\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u53d6\u548c\u878d\u5408\u989c\u8272\u7edf\u8ba1\u3001\u591a\u8272\u5f69\u7a7a\u95f4\u76f4\u65b9\u56fe\u4ee5\u53ca\u7eb9\u7406\u7279\u5f81\u6765\u8bc4\u4f30\u9c7c\u7c7b\u65b0\u9c9c\u5ea6\uff0c\u5728FFE\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u4f18\u4e8e\u6df1\u5ea6\u5b66\u4e60\u57fa\u7ebf\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u4f20\u7edf\u611f\u5b98\u8bc4\u4f30\u9c7c\u7c7b\u65b0\u9c9c\u5ea6\u5b58\u5728\u4e3b\u89c2\u6027\u3001\u4e0d\u4e00\u81f4\u6027\u548c\u96be\u4ee5\u6807\u51c6\u5316\u7684\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u5ba2\u89c2\u3001\u53ef\u9760\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u4ece\u9c7c\u773c\u56fe\u50cf\u4e2d\u7cfb\u7edf\u63d0\u53d6\u4e92\u8865\u7279\u5f81\u63cf\u8ff0\u7b26\uff0c\u5305\u62ec\u989c\u8272\u7edf\u8ba1\u3001\u591a\u8272\u5f69\u7a7a\u95f4\u76f4\u65b9\u56fe\u3001\u5c40\u90e8\u4e8c\u503c\u6a21\u5f0f(LBP)\u548c\u7070\u5ea6\u5171\u751f\u77e9\u9635(GLCM)\u7b49\u7eb9\u7406\u7279\u5f81\uff0c\u878d\u5408\u5168\u5c40\u8272\u5ea6\u53d8\u5316\u548c\u5c40\u90e8ROI\u533a\u57df\u9000\u5316\u7279\u5f81\u3002", "result": "\u5728\u6807\u51c6\u8bad\u7ec3\u6d4b\u8bd5\u8bbe\u7f6e\u4e0b\uff0cLightGBM\u5206\u7c7b\u5668\u8fbe\u523077.56%\u51c6\u786e\u7387\uff0c\u6bd4\u4e4b\u524d\u6df1\u5ea6\u5b66\u4e60\u57fa\u7ebf63.21%\u63d0\u534714.35%\uff1b\u4f7f\u7528\u589e\u5f3a\u6570\u636e\u65f6\uff0c\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u8fbe\u523097.16%\u51c6\u786e\u7387\uff0c\u6bd4\u4e4b\u524d\u6700\u4f7377.3%\u63d0\u534719.86%\u3002", "conclusion": "\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u624b\u5de5\u7279\u5f81\u7ecf\u8fc7\u6218\u7565\u5904\u7406\u53ef\u4ee5\u63d0\u4f9b\u9c81\u68d2\u3001\u53ef\u89e3\u91ca\u4e14\u53ef\u9760\u7684\u9c7c\u7c7b\u65b0\u9c9c\u5ea6\u81ea\u52a8\u8bc4\u4f30\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u98df\u54c1\u8d28\u91cf\u76d1\u63a7\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u6709\u4ef7\u503c\u89c1\u89e3\u3002"}}
{"id": "2510.16083", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.16083", "abs": "https://arxiv.org/abs/2510.16083", "authors": ["Jaehan Kim", "Minkyoo Song", "Minjae Seo", "Youngjin Jin", "Seungwon Shin", "Jinwoo Kim"], "title": "PassREfinder-FL: Privacy-Preserving Credential Stuffing Risk Prediction via Graph-Based Federated Learning for Representing Password Reuse between Websites", "comment": "Accepted by Elsevier Expert Systems with Applications", "summary": "Credential stuffing attacks have caused significant harm to online users who\nfrequently reuse passwords across multiple websites. While prior research has\nattempted to detect users with reused passwords or identify malicious login\nattempts, existing methods often compromise usability by restricting password\ncreation or website access, and their reliance on complex account-sharing\nmechanisms hinders real-world deployment. To address these limitations, we\npropose PassREfinder-FL, a novel framework that predicts credential stuffing\nrisks across websites. We introduce the concept of password reuse relations --\ndefined as the likelihood of users reusing passwords between websites -- and\nrepresent them as edges in a website graph. Using graph neural networks (GNNs),\nwe perform a link prediction task to assess credential reuse risk between\nsites. Our approach scales to a large number of arbitrary websites by\nincorporating public website information and linking newly observed websites as\nnodes in the graph. To preserve user privacy, we extend PassREfinder-FL with a\nfederated learning (FL) approach that eliminates the need to share user\nsensitive information across administrators. Evaluation on a real-world dataset\nof 360 million breached accounts from 22,378 websites shows that\nPassREfinder-FL achieves an F1-score of 0.9153 in the FL setting. We further\nvalidate that our FL-based GNN achieves a 4-11% performance improvement over\nother state-of-the-art GNN models through an ablation study. Finally, we\ndemonstrate that the predicted results can be used to quantify password reuse\nlikelihood as actionable risk scores.", "AI": {"tldr": "\u63d0\u51faPassREfinder-FL\u6846\u67b6\uff0c\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u7f51\u7ad9\u95f4\u7684\u5bc6\u7801\u91cd\u7528\u98ce\u9669\uff0c\u5e76\u901a\u8fc7\u8054\u90a6\u5b66\u4e60\u4fdd\u62a4\u7528\u6237\u9690\u79c1\uff0c\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fbe\u52300.9153\u7684F1\u5206\u6570\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u68c0\u6d4b\u5bc6\u7801\u91cd\u7528\u653b\u51fb\u65f6\u5b58\u5728\u7684\u53ef\u7528\u6027\u95ee\u9898\u548c\u9690\u79c1\u6cc4\u9732\u98ce\u9669\uff0c\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u9650\u5236\u5bc6\u7801\u521b\u5efa\u6216\u7f51\u7ad9\u8bbf\u95ee\uff0c\u4e14\u4f9d\u8d56\u590d\u6742\u7684\u8d26\u6237\u5171\u4eab\u673a\u5236\u96be\u4ee5\u5b9e\u9645\u90e8\u7f72\u3002", "method": "\u5f15\u5165\u5bc6\u7801\u91cd\u7528\u5173\u7cfb\u6982\u5ff5\uff0c\u5c06\u5176\u8868\u793a\u4e3a\u7f51\u7ad9\u56fe\u4e2d\u7684\u8fb9\uff0c\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u94fe\u63a5\u9884\u6d4b\u3002\u91c7\u7528\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u65e0\u9700\u8de8\u7ba1\u7406\u5458\u5171\u4eab\u7528\u6237\u654f\u611f\u4fe1\u606f\u3002", "result": "\u5728\u5305\u542b3.6\u4ebf\u4e2a\u6cc4\u9732\u8d26\u6237\u300122,378\u4e2a\u7f51\u7ad9\u7684\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cPassREfinder-FL\u5728\u8054\u90a6\u5b66\u4e60\u8bbe\u7f6e\u4e0b\u8fbe\u52300.9153\u7684F1\u5206\u6570\uff0c\u6bd4\u6700\u5148\u8fdbGNN\u6a21\u578b\u6027\u80fd\u63d0\u53474-11%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u9884\u6d4b\u7f51\u7ad9\u95f4\u7684\u5bc6\u7801\u91cd\u7528\u98ce\u9669\uff0c\u751f\u6210\u53ef\u64cd\u4f5c\u7684\u98ce\u9669\u8bc4\u5206\uff0c\u540c\u65f6\u4fdd\u62a4\u7528\u6237\u9690\u79c1\uff0c\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u4ef7\u503c\u3002"}}
{"id": "2510.17006", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17006", "abs": "https://arxiv.org/abs/2510.17006", "authors": ["Masahiro Kaneko", "Zeerak Talat", "Timothy Baldwin"], "title": "Online Learning Defense against Iterative Jailbreak Attacks via Prompt Optimization", "comment": null, "summary": "Iterative jailbreak methods that repeatedly rewrite and input prompts into\nlarge language models (LLMs) to induce harmful outputs -- using the model's\nprevious responses to guide each new iteration -- have been found to be a\nhighly effective attack strategy. Despite being an effective attack strategy\nagainst LLMs and their safety mechanisms, existing defenses do not proactively\ndisrupt this dynamic trial-and-error cycle. In this study, we propose a novel\nframework that dynamically updates its defense strategy through online learning\nin response to each new prompt from iterative jailbreak methods. Leveraging the\ndistinctions between harmful jailbreak-generated prompts and typical harmless\nprompts, we introduce a reinforcement learning-based approach that optimizes\nprompts to ensure appropriate responses for harmless tasks while explicitly\nrejecting harmful prompts. Additionally, to curb overfitting to the narrow band\nof partial input rewrites explored during an attack, we introduce\nPast-Direction Gradient Damping (PDGD). Experiments conducted on three LLMs\nshow that our approach significantly outperforms five existing defense methods\nagainst five iterative jailbreak methods. Moreover, our results indicate that\nour prompt optimization strategy simultaneously enhances response quality for\nharmless tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u52a8\u6001\u9632\u5fa1\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u7ebf\u5b66\u4e60\u5bf9\u6297\u8fed\u4ee3\u8d8a\u72f1\u653b\u51fb\uff0c\u4f7f\u7528PDGD\u9632\u6b62\u8fc7\u62df\u5408\uff0c\u663e\u8457\u63d0\u5347\u9632\u5fa1\u6548\u679c\u548c\u6b63\u5e38\u4efb\u52a1\u54cd\u5e94\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u65e0\u6cd5\u4e3b\u52a8\u7834\u574f\u8fed\u4ee3\u8d8a\u72f1\u653b\u51fb\u7684\u52a8\u6001\u8bd5\u9519\u5faa\u73af\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u52a8\u6001\u66f4\u65b0\u9632\u5fa1\u7b56\u7565\u7684\u4e3b\u52a8\u9632\u5fa1\u673a\u5236\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u63d0\u793a\uff0c\u786e\u4fdd\u65e0\u5bb3\u4efb\u52a1\u5f97\u5230\u9002\u5f53\u54cd\u5e94\u540c\u65f6\u660e\u786e\u62d2\u7edd\u6709\u5bb3\u63d0\u793a\uff1b\u5f15\u5165Past-Direction Gradient Damping\u9632\u6b62\u8fc7\u62df\u5408\u3002", "result": "\u5728\u4e09\u4e2aLLM\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u4e94\u79cd\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\uff0c\u5bf9\u6297\u4e94\u79cd\u8fed\u4ee3\u8d8a\u72f1\u653b\u51fb\uff1b\u540c\u65f6\u63d0\u5347\u4e86\u65e0\u5bb3\u4efb\u52a1\u7684\u54cd\u5e94\u8d28\u91cf\u3002", "conclusion": "\u63d0\u51fa\u7684\u52a8\u6001\u9632\u5fa1\u6846\u67b6\u80fd\u6709\u6548\u5bf9\u6297\u8fed\u4ee3\u8d8a\u72f1\u653b\u51fb\uff0c\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u63d0\u5347\u6b63\u5e38\u4efb\u52a1\u6027\u80fd\uff0c\u4e3aLLM\u5b89\u5168\u9632\u5fa1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2510.17146", "categories": ["cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2510.17146", "abs": "https://arxiv.org/abs/2510.17146", "authors": ["Subin Lin", "Chuanbo Hua"], "title": "Physics-Informed Large Language Models for HVAC Anomaly Detection with Autonomous Rule Generation", "comment": "NeurIPS 2025 Workshop of UrbanAI (Oral)", "summary": "Heating, Ventilation, and Air-Conditioning (HVAC) systems account for a\nsubstantial share of global building energy use, making reliable anomaly\ndetection essential for improving efficiency and reducing emissions. Classical\nrule-based approaches offer explainability but lack adaptability, while deep\nlearning methods provide predictive power at the cost of transparency,\nefficiency, and physical plausibility. Recent attempts to use Large Language\nModels (LLMs) for anomaly detection improve interpretability but largely ignore\nthe physical principles that govern HVAC operations. We present PILLM, a\nPhysics-Informed LLM framework that operates within an evolutionary loop to\nautomatically generate, evaluate, and refine anomaly detection rules. Our\napproach introduces physics-informed reflection and crossover operators that\nembed thermodynamic and control-theoretic constraints, enabling rules that are\nboth adaptive and physically grounded. Experiments on the public Building Fault\nDetection dataset show that PILLM achieves state-of-the-art performance while\nproducing diagnostic rules that are interpretable and actionable, advancing\ntrustworthy and deployable AI for smart building systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86PILLM\u6846\u67b6\uff0c\u5c06\u7269\u7406\u539f\u7406\u878d\u5165\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u8fdb\u5316\u5faa\u73af\u81ea\u52a8\u751f\u6210\u3001\u8bc4\u4f30\u548c\u4f18\u5316HVAC\u7cfb\u7edf\u5f02\u5e38\u68c0\u6d4b\u89c4\u5219\uff0c\u5728\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "HVAC\u7cfb\u7edf\u80fd\u8017\u5360\u6bd4\u9ad8\uff0c\u73b0\u6709\u65b9\u6cd5\u5404\u6709\u4e0d\u8db3\uff1a\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u53ef\u89e3\u91ca\u4f46\u7f3a\u4e4f\u9002\u5e94\u6027\uff0c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u9884\u6d4b\u80fd\u529b\u5f3a\u4f46\u7f3a\u4e4f\u900f\u660e\u5ea6\u548c\u7269\u7406\u5408\u7406\u6027\uff0cLLM\u65b9\u6cd5\u6539\u5584\u4e86\u53ef\u89e3\u91ca\u6027\u4f46\u5ffd\u7565\u4e86HVAC\u8fd0\u884c\u7684\u7269\u7406\u539f\u7406\u3002", "method": "PILLM\u6846\u67b6\u5728\u8fdb\u5316\u5faa\u73af\u4e2d\u8fd0\u884c\uff0c\u5f15\u5165\u7269\u7406\u611f\u77e5\u7684\u53cd\u601d\u548c\u4ea4\u53c9\u7b97\u5b50\uff0c\u5d4c\u5165\u70ed\u529b\u5b66\u548c\u63a7\u5236\u7406\u8bba\u7ea6\u675f\uff0c\u751f\u6210\u65e2\u9002\u5e94\u6027\u5f3a\u53c8\u7269\u7406\u57fa\u7840\u624e\u5b9e\u7684\u5f02\u5e38\u68c0\u6d4b\u89c4\u5219\u3002", "result": "\u5728\u516c\u5f00\u7684\u5efa\u7b51\u6545\u969c\u68c0\u6d4b\u6570\u636e\u96c6\u4e0a\uff0cPILLM\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u751f\u6210\u53ef\u89e3\u91ca\u548c\u53ef\u64cd\u4f5c\u7684\u8bca\u65ad\u89c4\u5219\u3002", "conclusion": "PILLM\u63a8\u52a8\u4e86\u667a\u80fd\u5efa\u7b51\u7cfb\u7edf\u4e2d\u53ef\u4fe1\u8d56\u548c\u53ef\u90e8\u7f72AI\u7684\u53d1\u5c55\uff0c\u4e3aHVAC\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u7269\u7406\u57fa\u7840\u624e\u5b9e\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.16084", "categories": ["cs.LG", "cond-mat.quant-gas", "math-ph", "math.MP", "physics.optics"], "pdf": "https://arxiv.org/pdf/2510.16084", "abs": "https://arxiv.org/abs/2510.16084", "authors": ["Karol Sajnok", "Micha\u0142 Matuszewski"], "title": "Near-Equilibrium Propagation training in nonlinear wave systems", "comment": "7 figures", "summary": "Backpropagation learning algorithm, the workhorse of modern artificial\nintelligence, is notoriously difficult to implement in physical neural\nnetworks. Equilibrium Propagation (EP) is an alternative with comparable\nefficiency and strong potential for in-situ training. We extend EP learning to\nboth discrete and continuous complex-valued wave systems. In contrast to\nprevious EP implementations, our scheme is valid in the weakly dissipative\nregime, and readily applicable to a wide range of physical settings, even\nwithout well defined nodes, where trainable inter-node connections can be\nreplaced by trainable local potential. We test the method in driven-dissipative\nexciton-polariton condensates governed by generalized Gross-Pitaevskii\ndynamics. Numerical studies on standard benchmarks, including a simple logical\ntask and handwritten-digit recognition, demonstrate stable convergence,\nestablishing a practical route to in-situ learning in physical systems in which\nsystem control is restricted to local parameters.", "AI": {"tldr": "\u5c06\u5e73\u8861\u4f20\u64ad\u5b66\u4e60\u6269\u5c55\u5230\u79bb\u6563\u548c\u8fde\u7eed\u590d\u503c\u6ce2\u7cfb\u7edf\uff0c\u9002\u7528\u4e8e\u5f31\u8017\u6563\u673a\u5236\uff0c\u5728\u6fc0\u5b50-\u6781\u5316\u6fc0\u5143\u51dd\u805a\u4f53\u4e2d\u5b9e\u73b0\u4e86\u7269\u7406\u7cfb\u7edf\u7684\u539f\u4f4d\u5b66\u4e60\u3002", "motivation": "\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5\u5728\u7269\u7406\u795e\u7ecf\u7f51\u7edc\u4e2d\u96be\u4ee5\u5b9e\u73b0\uff0c\u5e73\u8861\u4f20\u64ad(EP)\u4f5c\u4e3a\u66ff\u4ee3\u65b9\u6848\u5177\u6709\u53ef\u6bd4\u7684\u6548\u7387\u548c\u539f\u4f4d\u8bad\u7ec3\u6f5c\u529b\u3002", "method": "\u6269\u5c55EP\u5b66\u4e60\u5230\u79bb\u6563\u548c\u8fde\u7eed\u590d\u503c\u6ce2\u7cfb\u7edf\uff0c\u5728\u5f31\u8017\u6563\u673a\u5236\u4e0b\u6709\u6548\uff0c\u7528\u53ef\u8bad\u7ec3\u7684\u5c40\u90e8\u52bf\u80fd\u66ff\u4ee3\u8282\u70b9\u95f4\u8fde\u63a5\uff0c\u5728\u6fc0\u5b50-\u6781\u5316\u6fc0\u5143\u51dd\u805a\u4f53\u4e2d\u6d4b\u8bd5\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff08\u5305\u62ec\u903b\u8f91\u4efb\u52a1\u548c\u624b\u5199\u6570\u5b57\u8bc6\u522b\uff09\u8868\u73b0\u51fa\u7a33\u5b9a\u6536\u655b\u3002", "conclusion": "\u4e3a\u7cfb\u7edf\u63a7\u5236\u4ec5\u9650\u4e8e\u5c40\u90e8\u53c2\u6570\u7684\u7269\u7406\u7cfb\u7edf\u5efa\u7acb\u4e86\u5b9e\u7528\u7684\u539f\u4f4d\u5b66\u4e60\u8def\u5f84\u3002"}}
{"id": "2510.17013", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17013", "abs": "https://arxiv.org/abs/2510.17013", "authors": ["Lanni Bu", "Lauren Levin", "Amir Zeldes"], "title": "DiscoTrack: A Multilingual LLM Benchmark for Discourse Tracking", "comment": null, "summary": "Recent LLM benchmarks have tested models on a range of phenomena, but are\nstill focused primarily on natural language understanding for extraction of\nexplicit information, such as QA or summarization, with responses often tar-\ngeting information from individual sentences. We are still lacking more\nchallenging, and im- portantly also multilingual, benchmarks focus- ing on\nimplicit information and pragmatic infer- ences across larger documents in the\ncontext of discourse tracking: integrating and aggregating information across\nsentences, paragraphs and multiple speaker utterances. To this end, we present\nDiscoTrack, an LLM benchmark target- ing a range of tasks across 12 languages\nand four levels of discourse understanding: salience recognition, entity\ntracking, discourse relations and bridging inference. Our evaluation shows that\nthese tasks remain challenging, even for state-of-the-art models.", "AI": {"tldr": "DiscoTrack\u662f\u4e00\u4e2a\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e13\u6ce8\u4e8e\u8bdd\u8bed\u8ddf\u8e2a\u4e2d\u7684\u9690\u5f0f\u4fe1\u606f\u548c\u8bed\u7528\u63a8\u7406\uff0c\u5305\u542b12\u79cd\u8bed\u8a00\u7684\u56db\u4e2a\u8bdd\u8bed\u7406\u89e3\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709LLM\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u4e2d\u7684\u663e\u5f0f\u4fe1\u606f\u63d0\u53d6\uff0c\u7f3a\u4e4f\u9488\u5bf9\u9690\u5f0f\u4fe1\u606f\u3001\u8bed\u7528\u63a8\u7406\u548c\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u8de8\u53e5\u5b50\u3001\u6bb5\u843d\u548c\u8bf4\u8bdd\u8005\u7684\u8bdd\u8bed\u8ddf\u8e2a\u80fd\u529b\u7684\u6311\u6218\u6027\u57fa\u51c6\u3002", "method": "\u5f00\u53d1\u4e86DiscoTrack\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d612\u79cd\u8bed\u8a00\u548c\u56db\u4e2a\u8bdd\u8bed\u7406\u89e3\u5c42\u6b21\uff1a\u663e\u8457\u6027\u8bc6\u522b\u3001\u5b9e\u4f53\u8ddf\u8e2a\u3001\u8bdd\u8bed\u5173\u7cfb\u548c\u6865\u63a5\u63a8\u7406\u3002", "result": "\u8bc4\u4f30\u663e\u793a\uff0c\u5373\u4f7f\u662f\u6700\u5148\u8fdb\u7684\u6a21\u578b\uff0c\u8fd9\u4e9b\u4efb\u52a1\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "conclusion": "DiscoTrack\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7684\u7a7a\u767d\uff0c\u4e3a\u8bc4\u4f30LLM\u5728\u590d\u6742\u8bdd\u8bed\u7406\u89e3\u65b9\u9762\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2510.17149", "categories": ["cs.AI", "I.2.11"], "pdf": "https://arxiv.org/pdf/2510.17149", "abs": "https://arxiv.org/abs/2510.17149", "authors": ["Hongyi Du", "Jiaqi Su", "Jisen Li", "Lijie Ding", "Yingxuan Yang", "Peixuan Han", "Xiangru Tang", "Kunlun Zhu", "Jiaxuan You"], "title": "Which LLM Multi-Agent Protocol to Choose?", "comment": "Under review at ICLR 2026.Code and benchmark artifacts:\n  https://github.com/ulab-uiuc/AgentProtocols", "summary": "As large-scale multi-agent systems evolve, the communication protocol layer\nhas become a critical yet under-evaluated factor shaping performance and\nreliability. Despite the existence of diverse protocols (A2A, ACP, ANP, Agora,\netc.), selection is often intuition-driven and lacks standardized guidance. We\nintroduce ProtocolBench, a benchmark that systematically compares agent\nprotocols along four measurable axes: task success, end-to-end latency, message\nor byte overhead, and robustness under failures. On ProtocolBench, protocol\nchoice significantly influences system behavior. In the Streaming Queue\nscenario, overall completion time varies by up to 36.5% across protocols, and\nmean end-to-end latency differs by 3.48 s. Under Fail-Storm Recovery,\nresilience also differs consistently across protocols. Beyond evaluation, we\npresent ProtocolRouter, a learnable protocol router that selects per-scenario\n(or per-module) protocols from requirement and runtime signals. ProtocolRouter\nreduces Fail-Storm recovery time by up to 18.1% versus the best single-protocol\nbaseline, and achieves scenario-specific gains such as higher success in GAIA.\nWe also release ProtocolRouterBench to standardize protocol evaluation and\nimprove reliability at scale.", "AI": {"tldr": "ProtocolBench\u662f\u4e00\u4e2a\u7cfb\u7edf\u8bc4\u4f30\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u901a\u4fe1\u534f\u8bae\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0cProtocolRouter\u662f\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684\u534f\u8bae\u8def\u7531\u5668\uff0c\u80fd\u6839\u636e\u573a\u666f\u9700\u6c42\u9009\u62e9\u6700\u4f73\u534f\u8bae\u3002", "motivation": "\u5927\u89c4\u6a21\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\uff0c\u901a\u4fe1\u534f\u8bae\u9009\u62e9\u5bf9\u6027\u80fd\u548c\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5f53\u524d\u7f3a\u4e4f\u6807\u51c6\u5316\u8bc4\u4f30\u65b9\u6cd5\uff0c\u9009\u62e9\u5f80\u5f80\u57fa\u4e8e\u76f4\u89c9\u3002", "method": "\u5f00\u53d1ProtocolBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ece\u4efb\u52a1\u6210\u529f\u7387\u3001\u7aef\u5230\u7aef\u5ef6\u8fdf\u3001\u6d88\u606f\u5f00\u9500\u548c\u6545\u969c\u6062\u590d\u80fd\u529b\u56db\u4e2a\u7ef4\u5ea6\u7cfb\u7edf\u6bd4\u8f83\u534f\u8bae\uff1b\u63d0\u51faProtocolRouter\u5b66\u4e60\u578b\u534f\u8bae\u8def\u7531\u5668\uff0c\u6839\u636e\u9700\u6c42\u548c\u8fd0\u884c\u65f6\u4fe1\u53f7\u9009\u62e9\u534f\u8bae\u3002", "result": "\u534f\u8bae\u9009\u62e9\u663e\u8457\u5f71\u54cd\u7cfb\u7edf\u884c\u4e3a\uff1a\u5728Streaming Queue\u573a\u666f\u4e2d\u5b8c\u6210\u65f6\u95f4\u5dee\u5f02\u8fbe36.5%\uff0c\u7aef\u5230\u7aef\u5ef6\u8fdf\u5dee\u5f023.48\u79d2\uff1bProtocolRouter\u76f8\u6bd4\u6700\u4f73\u5355\u534f\u8bae\u57fa\u7ebf\u5c06\u6545\u969c\u6062\u590d\u65f6\u95f4\u51cf\u5c1118.1%\uff0c\u5728GAIA\u573a\u666f\u4e2d\u6210\u529f\u7387\u66f4\u9ad8\u3002", "conclusion": "\u901a\u4fe1\u534f\u8bae\u9009\u62e9\u5bf9\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6027\u80fd\u6709\u91cd\u5927\u5f71\u54cd\uff0cProtocolBench\u548cProtocolRouter\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u8bc4\u4f30\u548c\u4f18\u5316\u534f\u8bae\u9009\u62e9\u7684\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u7cfb\u7edf\u53ef\u9760\u6027\u3002"}}
{"id": "2510.16086", "categories": ["cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.16086", "abs": "https://arxiv.org/abs/2510.16086", "authors": ["Ziyang Liu", "Pengjunfei Chu", "Shuming Dong", "Chen Zhang", "Mingcheng Li", "Jin Wang"], "title": "FSRF: Factorization-guided Semantic Recovery for Incomplete Multimodal Sentiment Analysis", "comment": "6 pages,3 figures", "summary": "In recent years, Multimodal Sentiment Analysis (MSA) has become a research\nhotspot that aims to utilize multimodal data for human sentiment understanding.\nPrevious MSA studies have mainly focused on performing interaction and fusion\non complete multimodal data, ignoring the problem of missing modalities in\nreal-world applications due to occlusion, personal privacy constraints, and\ndevice malfunctions, resulting in low generalizability.\n  To this end, we propose a Factorization-guided Semantic Recovery Framework\n(FSRF) to mitigate the modality missing problem in the MSA task.\n  Specifically, we propose a de-redundant homo-heterogeneous factorization\nmodule that factorizes modality into modality-homogeneous,\nmodality-heterogeneous, and noisy representations and design elaborate\nconstraint paradigms for representation learning.\n  Furthermore, we design a distribution-aligned self-distillation module that\nfully recovers the missing semantics by utilizing bidirectional knowledge\ntransfer.\n  Comprehensive experiments on two datasets indicate that FSRF has a\nsignificant performance advantage over previous methods with uncertain missing\nmodalities.", "AI": {"tldr": "\u63d0\u51faFSRF\u6846\u67b6\u89e3\u51b3\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u4e2d\u7684\u6a21\u6001\u7f3a\u5931\u95ee\u9898\uff0c\u901a\u8fc7\u53bb\u5197\u4f59\u540c\u8d28-\u5f02\u8d28\u5206\u89e3\u548c\u5206\u5e03\u5bf9\u9f50\u81ea\u84b8\u998f\u6a21\u5757\u6765\u6062\u590d\u7f3a\u5931\u8bed\u4e49", "motivation": "\u73b0\u5b9e\u5e94\u7528\u4e2d\u7531\u4e8e\u906e\u6321\u3001\u9690\u79c1\u7ea6\u675f\u548c\u8bbe\u5907\u6545\u969c\u5bfc\u81f4\u6a21\u6001\u7f3a\u5931\uff0c\u4f20\u7edf\u65b9\u6cd5\u5ffd\u7565\u6b64\u95ee\u9898\u5bfc\u81f4\u6cdb\u5316\u6027\u5dee", "method": "\u4f7f\u7528\u53bb\u5197\u4f59\u540c\u8d28-\u5f02\u8d28\u5206\u89e3\u6a21\u5757\u5c06\u6a21\u6001\u5206\u89e3\u4e3a\u540c\u8d28\u3001\u5f02\u8d28\u548c\u566a\u58f0\u8868\u793a\uff0c\u5e76\u8bbe\u8ba1\u5206\u5e03\u5bf9\u9f50\u81ea\u84b8\u998f\u6a21\u5757\u8fdb\u884c\u53cc\u5411\u77e5\u8bc6\u8f6c\u79fb", "result": "\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFSRF\u5728\u4e0d\u786e\u5b9a\u6a21\u6001\u7f3a\u5931\u60c5\u51b5\u4e0b\u76f8\u6bd4\u5148\u524d\u65b9\u6cd5\u5177\u6709\u663e\u8457\u6027\u80fd\u4f18\u52bf", "conclusion": "FSRF\u6846\u67b6\u80fd\u6709\u6548\u7f13\u89e3\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u4e2d\u7684\u6a21\u6001\u7f3a\u5931\u95ee\u9898\uff0c\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b"}}
{"id": "2510.17017", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17017", "abs": "https://arxiv.org/abs/2510.17017", "authors": ["Qiusi Zhan", "Angeline Budiman-Chan", "Abdelrahman Zayed", "Xingzhi Guo", "Daniel Kang", "Joo-Kyung Kim"], "title": "SafeSearch: Do Not Trade Safety for Utility in LLM Search Agents", "comment": "Code: https://github.com/ZQS1943/SafeSearch", "summary": "Large language model (LLM) based search agents iteratively generate queries,\nretrieve external information, and reason to answer open-domain questions.\nWhile researchers have primarily focused on improving their utility, their\nsafety behaviors remain underexplored. In this paper, we first evaluate search\nagents using red-teaming datasets and find that they are more likely to produce\nharmful outputs than base LLMs. For example, when asked ``How can I track\nsomeone's location without their consent?'', a base model refuses, whereas a\nsearch agent designed to retrieve and cite sources may lower its refusal\nthreshold, fetch documents (e.g., court cases), and, once appended, synthesize\nthem into an informative yet unsafe summary. We further show that\nutility-oriented fine-tuning intensifies this risk, motivating joint alignment\nof safety and utility. We present SafeSearch, a multi-objective reinforcement\nlearning approach that couples a final-output safety/utility reward with a\nnovel query-level shaping term that penalizes unsafe queries and rewards safe\nones. Experiments show that SafeSearch reduces agent harmfulness by over 70%\nacross three red-teaming datasets while producing safe, helpful responses, and\nmatches the QA performance of a utility-only finetuned agent; further analyses\nconfirm the effectiveness of the query-level reward in jointly improving safety\nand utility.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSafeSearch\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u540c\u65f6\u4f18\u5316\u641c\u7d22\u4ee3\u7406\u7684\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\uff0c\u663e\u8457\u964d\u4f4e\u6709\u5bb3\u8f93\u51fa\u98ce\u9669", "motivation": "\u7814\u7a76\u53d1\u73b0\u57fa\u4e8eLLM\u7684\u641c\u7d22\u4ee3\u7406\u6bd4\u57fa\u7840LLM\u66f4\u5bb9\u6613\u4ea7\u751f\u6709\u5bb3\u8f93\u51fa\uff0c\u56e0\u4e3a\u5b83\u4eec\u5728\u68c0\u7d22\u5916\u90e8\u4fe1\u606f\u65f6\u4f1a\u964d\u4f4e\u62d2\u7edd\u9608\u503c\uff0c\u5c06\u4e0d\u5b89\u5168\u4fe1\u606f\u5408\u6210\u4e3a\u6709\u5bb3\u5185\u5bb9", "method": "\u91c7\u7528\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5SafeSearch\uff0c\u7ed3\u5408\u6700\u7ec8\u8f93\u51fa\u7684\u5b89\u5168/\u6548\u7528\u5956\u52b1\u548c\u67e5\u8be2\u7ea7\u522b\u7684\u60e9\u7f5a\u673a\u5236\uff0c\u5bf9\u4e0d\u5b89\u5168\u67e5\u8be2\u8fdb\u884c\u60e9\u7f5a\uff0c\u5bf9\u5b89\u5168\u67e5\u8be2\u8fdb\u884c\u5956\u52b1", "result": "SafeSearch\u5728\u4e09\u4e2a\u7ea2\u961f\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\u5c06\u4ee3\u7406\u7684\u6709\u5bb3\u6027\u964d\u4f4e\u4e8670%\u4ee5\u4e0a\uff0c\u540c\u65f6\u4fdd\u6301\u5b89\u5168\u6709\u7528\u7684\u54cd\u5e94\uff0c\u4e0e\u4ec5\u4f18\u5316\u6548\u7528\u7684\u5fae\u8c03\u4ee3\u7406\u5728QA\u6027\u80fd\u4e0a\u76f8\u5f53", "conclusion": "\u67e5\u8be2\u7ea7\u522b\u7684\u5956\u52b1\u673a\u5236\u80fd\u6709\u6548\u8054\u5408\u63d0\u5347\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\uff0c\u8bc1\u660e\u4e86\u8054\u5408\u5bf9\u9f50\u5b89\u5168\u6027\u548c\u6548\u7528\u7684\u91cd\u8981\u6027"}}
{"id": "2510.17172", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17172", "abs": "https://arxiv.org/abs/2510.17172", "authors": ["Shun Huang", "Wenlu Xing", "Shijia Geng", "Hailong Wang", "Guangkun Nie", "Gongzheng Tang", "Chenyang He", "Shenda Hong"], "title": "Combining ECG Foundation Model and XGBoost to Predict In-Hospital Malignant Ventricular Arrhythmias in AMI Patients", "comment": null, "summary": "Malignant ventricular arrhythmias (VT/VF) following acute myocardial\ninfarction (AMI) are a major cause of in-hospital death, yet early\nidentification remains a clinical challenge. While traditional risk scores have\nlimited performance, end-to-end deep learning models often lack the\ninterpretability needed for clinical trust. This study aimed to develop a\nhybrid predictive framework that integrates a large-scale electrocardiogram\n(ECG) foundation model (ECGFounder) with an interpretable XGBoost classifier to\nimprove both accuracy and interpretability. We analyzed 6,634 ECG recordings\nfrom AMI patients, among whom 175 experienced in-hospital VT/VF. The ECGFounder\nmodel was used to extract 150-dimensional diagnostic probability features ,\nwhich were then refined through feature selection to train the XGBoost\nclassifier. Model performance was evaluated using AUC and F1-score , and the\nSHAP method was used for interpretability. The ECGFounder + XGBoost hybrid\nmodel achieved an AUC of 0.801 , outperforming KNN (AUC 0.677), RNN (AUC\n0.676), and an end-to-end 1D-CNN (AUC 0.720). SHAP analysis revealed that\nmodel-identified key features, such as \"premature ventricular complexes\" (risk\npredictor) and \"normal sinus rhythm\" (protective factor), were highly\nconsistent with clinical knowledge. We conclude that this hybrid framework\nprovides a novel paradigm for VT/VF risk prediction by validating the use of\nfoundation model outputs as effective, automated feature engineering for\nbuilding trustworthy, explainable AI-based clinical decision support systems.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7ed3\u5408ECG\u57fa\u7840\u6a21\u578b\u548c\u53ef\u89e3\u91caXGBoost\u5206\u7c7b\u5668\u7684\u6df7\u5408\u9884\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u6025\u6027\u5fc3\u808c\u6897\u6b7b\u540e\u6076\u6027\u5ba4\u6027\u5fc3\u5f8b\u5931\u5e38\u98ce\u9669\uff0c\u5728\u63d0\u9ad8\u51c6\u786e\u6027\u7684\u540c\u65f6\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u6025\u6027\u5fc3\u808c\u6897\u6b7b\u540e\u6076\u6027\u5ba4\u6027\u5fc3\u5f8b\u5931\u5e38\u662f\u9662\u5185\u6b7b\u4ea1\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u4f20\u7edf\u98ce\u9669\u8bc4\u5206\u6027\u80fd\u6709\u9650\uff0c\u800c\u7aef\u5230\u7aef\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7f3a\u4e4f\u4e34\u5e8a\u4fe1\u4efb\u6240\u9700\u7684\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u4f7f\u7528ECG\u57fa\u7840\u6a21\u578b(ECGFounder)\u63d0\u53d6150\u7ef4\u8bca\u65ad\u6982\u7387\u7279\u5f81\uff0c\u901a\u8fc7\u7279\u5f81\u9009\u62e9\u540e\u8bad\u7ec3XGBoost\u5206\u7c7b\u5668\uff0c\u5e76\u91c7\u7528SHAP\u65b9\u6cd5\u8fdb\u884c\u53ef\u89e3\u91ca\u6027\u5206\u6790\u3002", "result": "\u6df7\u5408\u6a21\u578bAUC\u8fbe\u52300.801\uff0c\u4f18\u4e8eKNN(0.677)\u3001RNN(0.676)\u548c1D-CNN(0.720)\u3002SHAP\u5206\u6790\u663e\u793a\u6a21\u578b\u8bc6\u522b\u7684\u5173\u952e\u7279\u5f81\u4e0e\u4e34\u5e8a\u77e5\u8bc6\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "\u8be5\u6df7\u5408\u6846\u67b6\u901a\u8fc7\u9a8c\u8bc1\u57fa\u7840\u6a21\u578b\u8f93\u51fa\u4f5c\u4e3a\u6709\u6548\u7684\u81ea\u52a8\u5316\u7279\u5f81\u5de5\u7a0b\uff0c\u4e3a\u6784\u5efa\u53ef\u4fe1\u8d56\u3001\u53ef\u89e3\u91ca\u7684AI\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2510.16089", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16089", "abs": "https://arxiv.org/abs/2510.16089", "authors": ["William Hoy", "Nurcin Celik"], "title": "STABLE: Gated Continual Learning for Large Language Models", "comment": null, "summary": "Large language models (LLMs) increasingly require mechanisms for continual\nadaptation without full retraining. However, sequential updates can lead to\ncatastrophic forgetting, where new edits degrade previously acquired knowledge.\nThis work presents STABLE, a gated continual self editing framework that\nconstrains forgetting during sequential updates using parameter efficient fine\ntuning via Low Rank Adaptation (LoRA; see arXiv:2106.09685). Each candidate\nedit is evaluated against a stability budget using one of three metrics: (i)\nExact Match (EM) drop, capturing factual accuracy loss; (ii) bits increase,\nreflecting reduced model confidence; and (iii) KL divergence, quantifying\ndistributional drift between the base and adapted models. If a threshold is\nexceeded, the LoRA update is rescaled through a clipping procedure or rejected.\nExperiments on the Qwen-2.5-7B model show that gating effectively mitigates\nforgetting while preserving adaptability. EM based gating achieved the highest\ncumulative performance in short continual learning sequences. Our results show\nthat different gating strategies can achieve comparable distribution shift\n(measured by KL divergence) while producing different accuracy outcomes,\nhighlighting the importance of gating design in continual adaptation. This\napproach offers a principled method for continual model editing, enabling LLMs\nto integrate new knowledge while maintaining reliability. Code:\nhttps://github.com/Bhoy1/STABLE", "AI": {"tldr": "STABLE\u662f\u4e00\u4e2a\u95e8\u63a7\u6301\u7eed\u81ea\u7f16\u8f91\u6846\u67b6\uff0c\u4f7f\u7528LoRA\u8fdb\u884c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff0c\u901a\u8fc7\u4e09\u79cd\u6307\u6807\u8bc4\u4f30\u7f16\u8f91\u7a33\u5b9a\u6027\u6765\u7ea6\u675f\u987a\u5e8f\u66f4\u65b0\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9700\u8981\u6301\u7eed\u9002\u5e94\u673a\u5236\uff0c\u4f46\u987a\u5e8f\u66f4\u65b0\u4f1a\u5bfc\u81f4\u707e\u96be\u6027\u9057\u5fd8\uff0c\u65b0\u7f16\u8f91\u4f1a\u964d\u4f4e\u5148\u524d\u83b7\u5f97\u7684\u77e5\u8bc6\u3002", "method": "\u4f7f\u7528LoRA\u8fdb\u884c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff0c\u901a\u8fc7\u4e09\u79cd\u6307\u6807\uff08\u7cbe\u786e\u5339\u914d\u4e0b\u964d\u3001\u6bd4\u7279\u6570\u589e\u52a0\u3001KL\u6563\u5ea6\uff09\u8bc4\u4f30\u5019\u9009\u7f16\u8f91\u7684\u7a33\u5b9a\u6027\uff0c\u8d85\u8fc7\u9608\u503c\u65f6\u91cd\u65b0\u7f29\u653e\u6216\u62d2\u7eddLoRA\u66f4\u65b0\u3002", "result": "\u5728Qwen-2.5-7B\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u95e8\u63a7\u80fd\u6709\u6548\u51cf\u8f7b\u9057\u5fd8\u540c\u65f6\u4fdd\u6301\u9002\u5e94\u6027\uff0c\u57fa\u4e8eEM\u7684\u95e8\u63a7\u5728\u77ed\u6301\u7eed\u5b66\u4e60\u5e8f\u5217\u4e2d\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u7d2f\u79ef\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6301\u7eed\u6a21\u578b\u7f16\u8f91\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u65b9\u6cd5\uff0c\u4f7fLLM\u80fd\u591f\u6574\u5408\u65b0\u77e5\u8bc6\u540c\u65f6\u4fdd\u6301\u53ef\u9760\u6027\uff0c\u4e0d\u540c\u95e8\u63a7\u7b56\u7565\u53ef\u4ea7\u751f\u4e0d\u540c\u7684\u51c6\u786e\u6027\u7ed3\u679c\uff0c\u7a81\u663e\u95e8\u63a7\u8bbe\u8ba1\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.17018", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17018", "abs": "https://arxiv.org/abs/2510.17018", "authors": ["Noor Islam S. Mohammad"], "title": "Extended LSTM: Adaptive Feature Gating for Toxic Comment Classification", "comment": null, "summary": "Toxic comment detection remains a challenging task, where transformer-based\nmodels (e.g., BERT) incur high computational costs and degrade on minority\ntoxicity classes, while classical ensembles lack semantic adaptability. We\npropose xLSTM, a parameter-efficient and theoretically grounded framework that\nunifies cosine-similarity gating, adaptive feature prioritization, and\nprincipled class rebalancing. A learnable reference vector {v} in {R}^d\nmodulates contextual embeddings via cosine similarity, amplifying toxic cues\nand attenuating benign signals to yield stronger gradients under severe class\nimbalance. xLSTM integrates multi-source embeddings (GloVe, FastText, BERT CLS)\nthrough a projection layer, a character-level BiLSTM for morphological cues,\nembedding-space SMOTE for minority augmentation, and adaptive focal loss with\ndynamic class weighting. On the Jigsaw Toxic Comment benchmark, xLSTM attains\n96.0% accuracy and 0.88 macro-F1, outperforming BERT by 33% on threat and 28%\non identity_hate categories, with 15 times fewer parameters and 50ms inference\nlatency. Cosine gating contributes a +4.8% F1 gain in ablations. The results\nestablish a new efficiency adaptability frontier, demonstrating that\nlightweight, theoretically informed architectures can surpass large pretrained\nmodels on imbalanced, domain-specific NLP tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86xLSTM\u6846\u67b6\uff0c\u901a\u8fc7\u4f59\u5f26\u76f8\u4f3c\u5ea6\u95e8\u63a7\u3001\u81ea\u9002\u5e94\u7279\u5f81\u4f18\u5148\u7ea7\u548c\u7c7b\u522b\u91cd\u5e73\u8861\uff0c\u5728\u6bd2\u6027\u8bc4\u8bba\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9ad8\u6027\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e2d\uff0c\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u5728\u5c11\u6570\u6bd2\u6027\u7c7b\u522b\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u4f20\u7edf\u96c6\u6210\u65b9\u6cd5\u7f3a\u4e4f\u8bed\u4e49\u9002\u5e94\u6027\u3002", "method": "\u4f7f\u7528\u53ef\u5b66\u4e60\u7684\u53c2\u8003\u5411\u91cf\u901a\u8fc7\u4f59\u5f26\u76f8\u4f3c\u5ea6\u8c03\u5236\u4e0a\u4e0b\u6587\u5d4c\u5165\uff0c\u96c6\u6210\u591a\u6e90\u5d4c\u5165\uff08GloVe\u3001FastText\u3001BERT CLS\uff09\uff0c\u5305\u542b\u5b57\u7b26\u7ea7BiLSTM\u3001\u5d4c\u5165\u7a7a\u95f4SMOTE\u548c\u81ea\u9002\u5e94\u7126\u70b9\u635f\u5931\u3002", "result": "\u5728Jigsaw\u6bd2\u6027\u8bc4\u8bba\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523096.0%\u51c6\u786e\u7387\u548c0.88\u5b8fF1\uff0c\u5728\u5a01\u80c1\u548c\u8eab\u4efd\u4ec7\u6068\u7c7b\u522b\u4e0a\u5206\u522b\u6bd4BERT\u63d0\u534733%\u548c28%\uff0c\u53c2\u6570\u51cf\u5c1115\u500d\uff0c\u63a8\u7406\u5ef6\u8fdf50ms\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u3001\u7406\u8bba\u9a71\u52a8\u7684\u67b6\u6784\u53ef\u4ee5\u5728\u4e0d\u5e73\u8861\u3001\u7279\u5b9a\u9886\u57df\u7684NLP\u4efb\u52a1\u4e0a\u8d85\u8d8a\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5efa\u7acb\u4e86\u65b0\u7684\u6548\u7387\u9002\u5e94\u6027\u524d\u6cbf\u3002"}}
{"id": "2510.17173", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17173", "abs": "https://arxiv.org/abs/2510.17173", "authors": ["Melik Ozolcer", "Sang Won Bae"], "title": "Offline Policy Evaluation of Multi-Turn LLM Health Coaching with Real Users", "comment": "Accepted to the NeurIPS 2025 Workshop on Multi-Turn Interactions in\n  Large Language Models", "summary": "We study a web-deployed, tool-augmented LLM health coach with real users. In\na pilot with seven users (280 rated turns), offline policy evaluation (OPE)\nover factorized decision heads (Tool/Style) shows that a uniform heavy-tool\npolicy raises average value on logs but harms specific subgroups, most notably\nlow-health-literacy/high-self-efficacy users. A lightweight simulator with\nhidden archetypes further shows that adding a small early information-gain\nbonus reliably shortens trait identification and improves goal success and\npass@3. Together, these early findings indicate an evaluation-first path to\npersonalization: freeze the generator, learn subgroup-aware decision heads on\ntyped rewards (objective tool outcomes and satisfaction), and always report\nper-archetype metrics to surface subgroup harms that averages obscure.", "AI": {"tldr": "\u7814\u7a76\u4e86\u57fa\u4e8e\u5de5\u5177\u589e\u5f3a\u7684LLM\u5065\u5eb7\u6559\u7ec3\u7cfb\u7edf\uff0c\u901a\u8fc7\u79bb\u7ebf\u7b56\u7565\u8bc4\u4f30\u53d1\u73b0\u7edf\u4e00\u7684\u91cd\u5de5\u5177\u7b56\u7565\u4f1a\u635f\u5bb3\u7279\u5b9a\u7528\u6237\u7fa4\u4f53\uff0c\u7279\u522b\u662f\u4f4e\u5065\u5eb7\u7d20\u517b/\u9ad8\u81ea\u6211\u6548\u80fd\u7528\u6237\u3002\u8f7b\u91cf\u7ea7\u6a21\u62df\u5668\u663e\u793a\u6dfb\u52a0\u65e9\u671f\u4fe1\u606f\u589e\u76ca\u5956\u52b1\u53ef\u6539\u5584\u4e2a\u6027\u5316\u6548\u679c\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u8bc4\u4f30\u4f18\u5148\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e2a\u6027\u5316\u5065\u5eb7\u6559\u7ec3\u7cfb\u7edf\uff0c\u907f\u514d\u7edf\u4e00\u7b56\u7565\u5bf9\u7279\u5b9a\u7528\u6237\u7fa4\u4f53\u9020\u6210\u4f24\u5bb3\uff0c\u7279\u522b\u662f\u5173\u6ce8\u4f4e\u5065\u5eb7\u7d20\u517b\u4f46\u9ad8\u81ea\u6211\u6548\u80fd\u7528\u6237\u7684\u7279\u6b8a\u9700\u6c42\u3002", "method": "\u4f7f\u7528\u79bb\u7ebf\u7b56\u7565\u8bc4\u4f30(OPE)\u5206\u6790\u56e0\u5b50\u5316\u51b3\u7b56\u5934(\u5de5\u5177/\u98ce\u683c)\uff0c\u90e8\u7f72\u8f7b\u91cf\u7ea7\u6a21\u62df\u5668\u9a8c\u8bc1\u6dfb\u52a0\u4fe1\u606f\u589e\u76ca\u5956\u52b1\u7684\u6548\u679c\uff0c\u91c7\u7528\u51bb\u7ed3\u751f\u6210\u5668\u3001\u5b66\u4e60\u5b50\u7fa4\u4f53\u611f\u77e5\u51b3\u7b56\u5934\u7684\u7b56\u7565\u3002", "result": "\u7edf\u4e00\u91cd\u5de5\u5177\u7b56\u7565\u5728\u65e5\u5fd7\u4e0a\u63d0\u9ad8\u5e73\u5747\u4ef7\u503c\u4f46\u635f\u5bb3\u7279\u5b9a\u5b50\u7fa4\u4f53\uff1b\u6dfb\u52a0\u65e9\u671f\u4fe1\u606f\u589e\u76ca\u5956\u52b1\u53ef\u9760\u5730\u7f29\u77ed\u7279\u8d28\u8bc6\u522b\u65f6\u95f4\uff0c\u63d0\u9ad8\u76ee\u6807\u6210\u529f\u7387\u548cpass@3\u6307\u6807\u3002", "conclusion": "\u63d0\u51fa\u4e86\u8bc4\u4f30\u4f18\u5148\u7684\u4e2a\u6027\u5316\u8def\u5f84\uff1a\u51bb\u7ed3\u751f\u6210\u5668\uff0c\u57fa\u4e8e\u7c7b\u578b\u5316\u5956\u52b1\u5b66\u4e60\u5b50\u7fa4\u4f53\u611f\u77e5\u51b3\u7b56\u5934\uff0c\u5e76\u59cb\u7ec8\u62a5\u544a\u6bcf\u4e2a\u539f\u578b\u6307\u6807\u4ee5\u63ed\u793a\u88ab\u5e73\u5747\u503c\u63a9\u76d6\u7684\u5b50\u7fa4\u4f53\u4f24\u5bb3\u3002"}}
{"id": "2510.16092", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16092", "abs": "https://arxiv.org/abs/2510.16092", "authors": ["Devvrit Khatri", "Pranamya Kulkarni", "Nilesh Gupta", "Yerram Varun", "Liqian Peng", "Jay Yagnik", "Praneeth Netrapalli", "Cho-Jui Hsieh", "Alec Go", "Inderjit S Dhillon", "Aditya Kusupati", "Prateek Jain"], "title": "Compressing Many-Shots in In-Context Learning", "comment": null, "summary": "Large Language Models (LLMs) have been shown to be able to learn different\ntasks without explicit finetuning when given many input-output examples /\ndemonstrations through In-Context Learning (ICL). Increasing the number of\nexamples, called ``shots'', improves downstream task performance but incurs\nhigher memory and computational costs. In this work, we study an approach to\nimprove the memory and computational efficiency of ICL inference by compressing\nthe many-shot prompts. Given many shots comprising t tokens, our goal is to\ngenerate a m soft-token summary, where m < t. We first show that existing\nprompt compression methods are ineffective for many-shot compression, and\nsimply using fewer shots as a baseline is surprisingly strong. To achieve\neffective compression, we find that: (a) a stronger compressor model with more\ntrainable parameters is necessary, and (b) compressing many-shot\nrepresentations at each transformer layer enables more fine-grained compression\nby providing each layer with its own compressed representation. Based on these\ninsights, we propose MemCom, a layer-wise compression method. We systematically\nevaluate various compressor models and training approaches across different\nmodel sizes (2B and 7B), architectures (Gemma and Mistral), many-shot sequence\nlengths (3k-6k tokens), and compression ratios (3x to 8x). MemCom outperforms\nstrong baselines across all compression ratios on multiple classification tasks\nwith large label sets. Notably, while baseline performance degrades sharply at\nhigher compression ratios, often by over 20-30%, MemCom maintains high accuracy\nwith minimal degradation, typically dropping by less than 10%.", "AI": {"tldr": "\u63d0\u51faMemCom\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c42\u95f4\u538b\u7f29\u6280\u672f\u6709\u6548\u538b\u7f29\u591a\u793a\u4f8b\u63d0\u793a\uff0c\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u5185\u5b58\u548c\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u63d0\u793a\u538b\u7f29\u65b9\u6cd5\u5728\u591a\u793a\u4f8b\u538b\u7f29\u4e2d\u6548\u679c\u4e0d\u4f73\uff0c\u800c\u7b80\u5355\u51cf\u5c11\u793a\u4f8b\u6570\u91cf\u4f5c\u4e3a\u57fa\u7ebf\u65b9\u6cd5\u8868\u73b0\u610f\u5916\u5f3a\u52b2\u3002\u9700\u8981\u66f4\u9ad8\u6548\u7684\u538b\u7f29\u65b9\u6cd5\u6765\u5e73\u8861\u6027\u80fd\u4e0e\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u63d0\u51faMemCom\u5c42\u95f4\u538b\u7f29\u65b9\u6cd5\uff0c\u4f7f\u7528\u66f4\u5f3a\u7684\u538b\u7f29\u5668\u6a21\u578b\uff0c\u5728transformer\u7684\u6bcf\u4e00\u5c42\u8fdb\u884c\u7ec6\u7c92\u5ea6\u538b\u7f29\uff0c\u4e3a\u6bcf\u5c42\u63d0\u4f9b\u72ec\u7acb\u7684\u538b\u7f29\u8868\u793a\u3002", "result": "\u5728\u591a\u4e2a\u5206\u7c7b\u4efb\u52a1\u4e0a\uff0cMemCom\u5728\u6240\u6709\u538b\u7f29\u6bd4\u4e0b\u5747\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u3002\u5728\u9ad8\u538b\u7f29\u6bd4\u65f6\uff0c\u57fa\u7ebf\u6027\u80fd\u4e0b\u964d20-30%\uff0c\u800cMemCom\u4ec5\u4e0b\u964d\u4e0d\u523010%\uff0c\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u3002", "conclusion": "\u5c42\u95f4\u538b\u7f29\u548c\u66f4\u5f3a\u7684\u538b\u7f29\u5668\u6a21\u578b\u662f\u5b9e\u73b0\u591a\u793a\u4f8b\u63d0\u793a\u6709\u6548\u538b\u7f29\u7684\u5173\u952e\uff0cMemCom\u65b9\u6cd5\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2510.17028", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17028", "abs": "https://arxiv.org/abs/2510.17028", "authors": ["Kyle Cox", "Jiawei Xu", "Yikun Han", "Rong Xu", "Tianhao Li", "Chi-Yang Hsu", "Tianlong Chen", "Walter Gerych", "Ying Ding"], "title": "Mapping from Meaning: Addressing the Miscalibration of Prompt-Sensitive Language Models", "comment": null, "summary": "An interesting behavior in large language models (LLMs) is prompt\nsensitivity. When provided with different but semantically equivalent versions\nof the same prompt, models may produce very different distributions of answers.\nThis suggests that the uncertainty reflected in a model's output distribution\nfor one prompt may not reflect the model's uncertainty about the meaning of the\nprompt. We model prompt sensitivity as a type of generalization error, and show\nthat sampling across the semantic ``concept space'' with paraphrasing\nperturbations improves uncertainty calibration without compromising accuracy.\nAdditionally, we introduce a new metric for uncertainty decomposition in\nblack-box LLMs that improves upon entropy-based decomposition by modeling\nsemantic continuities in natural language generation. We show that this\ndecomposition metric can be used to quantify how much LLM uncertainty is\nattributed to prompt sensitivity. Our work introduces a new way to improve\nuncertainty calibration in prompt-sensitive language models, and provides\nevidence that some LLMs fail to exhibit consistent general reasoning about the\nmeanings of their inputs.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63d0\u793a\u654f\u611f\u6027\uff0c\u63d0\u51fa\u901a\u8fc7\u8bed\u4e49\u7a7a\u95f4\u91c7\u6837\u548c\u91ca\u4e49\u6270\u52a8\u6765\u6539\u8fdb\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u4e0d\u786e\u5b9a\u6027\u5206\u89e3\u6307\u6807\u6765\u91cf\u5316\u63d0\u793a\u654f\u611f\u6027\u5bf9\u4e0d\u786e\u5b9a\u6027\u7684\u5f71\u54cd\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u8bed\u4e49\u76f8\u540c\u4f46\u8868\u8ff0\u4e0d\u540c\u7684\u63d0\u793a\u4f1a\u4ea7\u751f\u4e0d\u540c\u7684\u7b54\u6848\u5206\u5e03\uff0c\u8fd9\u8868\u660e\u6a21\u578b\u8f93\u51fa\u7684\u4e0d\u786e\u5b9a\u6027\u53ef\u80fd\u65e0\u6cd5\u771f\u5b9e\u53cd\u6620\u5176\u5bf9\u63d0\u793a\u542b\u4e49\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u5c06\u63d0\u793a\u654f\u611f\u6027\u5efa\u6a21\u4e3a\u6cdb\u5316\u8bef\u5dee\uff0c\u901a\u8fc7\u8bed\u4e49\u6982\u5ff5\u7a7a\u95f4\u7684\u91ca\u4e49\u6270\u52a8\u91c7\u6837\u6765\u6539\u8fdb\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u4e0d\u786e\u5b9a\u6027\u5206\u89e3\u6307\u6807\u6765\u5efa\u6a21\u81ea\u7136\u8bed\u8a00\u751f\u6210\u4e2d\u7684\u8bed\u4e49\u8fde\u7eed\u6027\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u8bed\u4e49\u7a7a\u95f4\u91c7\u6837\u53ef\u4ee5\u5728\u4e0d\u635f\u5bb3\u51c6\u786e\u6027\u7684\u60c5\u51b5\u4e0b\u6539\u8fdb\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\uff0c\u65b0\u7684\u5206\u89e3\u6307\u6807\u80fd\u591f\u6709\u6548\u91cf\u5316\u63d0\u793a\u654f\u611f\u6027\u5bf9LLM\u4e0d\u786e\u5b9a\u6027\u7684\u8d21\u732e\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u6539\u8fdb\u63d0\u793a\u654f\u611f\u8bed\u8a00\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u5e76\u8bc1\u660e\u67d0\u4e9bLLM\u672a\u80fd\u5bf9\u5176\u8f93\u5165\u542b\u4e49\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u901a\u7528\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2510.17211", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17211", "abs": "https://arxiv.org/abs/2510.17211", "authors": ["Tingsong Xiao", "Yao An Lee", "Zelin Xu", "Yupu Zhang", "Zibo Liu", "Yu Huang", "Jiang Bian", "Serena Jingchuan Guo", "Zhe Jiang"], "title": "Temporally Detailed Hypergraph Neural ODEs for Type 2 Diabetes Progression Modeling", "comment": null, "summary": "Disease progression modeling aims to characterize and predict how a patient's\ndisease complications worsen over time based on longitudinal electronic health\nrecords (EHRs). Accurate modeling of disease progression, such as type 2\ndiabetes, can enhance patient sub-phenotyping and inform effective and timely\ninterventions. However, the problem is challenging due to the need to learn\ncontinuous-time dynamics of progression patterns based on irregular-time event\nsamples and patient heterogeneity (\\eg different progression rates and\npathways). Existing mechanistic and data-driven methods either lack\nadaptability to learn from real-world data or fail to capture complex\ncontinuous-time dynamics on progression trajectories. To address these\nlimitations, we propose Temporally Detailed Hypergraph Neural Ordinary\nDifferential Equation (TD-HNODE), which represents disease progression on\nclinically recognized trajectories as a temporally detailed hypergraph and\nlearns the continuous-time progression dynamics via a neural ODE framework.\nTD-HNODE contains a learnable TD-Hypergraph Laplacian that captures the\ninterdependency of disease complication markers within both intra- and\ninter-progression trajectories. Experiments on two real-world clinical datasets\ndemonstrate that TD-HNODE outperforms multiple baselines in modeling the\nprogression of type 2 diabetes and related cardiovascular diseases.", "AI": {"tldr": "\u63d0\u51fa\u4e86TD-HNODE\u6a21\u578b\uff0c\u901a\u8fc7\u65f6\u95f4\u8be6\u7ec6\u8d85\u56fe\u548c\u795e\u7ecfODE\u6846\u67b6\u5b66\u4e60\u75be\u75c5\u8fdb\u5c55\u7684\u8fde\u7eed\u65f6\u95f4\u52a8\u6001\uff0c\u57282\u578b\u7cd6\u5c3f\u75c5\u548c\u5fc3\u8840\u7ba1\u75be\u75c5\u8fdb\u5c55\u5efa\u6a21\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u51c6\u786e\u5efa\u6a21\u75be\u75c5\u8fdb\u5c55\uff08\u59822\u578b\u7cd6\u5c3f\u75c5\uff09\u53ef\u4ee5\u6539\u5584\u60a3\u8005\u4e9a\u578b\u5206\u578b\u548c\u53ca\u65f6\u5e72\u9884\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u4e0d\u89c4\u5219\u65f6\u95f4\u91c7\u6837\u6570\u636e\u548c\u60a3\u8005\u5f02\u8d28\u6027\uff0c\u65e0\u6cd5\u6355\u6349\u590d\u6742\u7684\u8fde\u7eed\u65f6\u95f4\u52a8\u6001\u3002", "method": "\u4f7f\u7528\u65f6\u95f4\u8be6\u7ec6\u8d85\u56fe\u8868\u793a\u4e34\u5e8a\u8ba4\u53ef\u7684\u8fdb\u5c55\u8f68\u8ff9\uff0c\u901a\u8fc7\u795e\u7ecfODE\u6846\u67b6\u5b66\u4e60\u8fde\u7eed\u65f6\u95f4\u8fdb\u5c55\u52a8\u6001\uff0c\u5305\u542b\u53ef\u5b66\u4e60\u7684TD-\u8d85\u56fe\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u6355\u6349\u75be\u75c5\u5e76\u53d1\u75c7\u6807\u5fd7\u7269\u5728\u8fdb\u5c55\u8f68\u8ff9\u5185\u548c\u8f68\u8ff9\u95f4\u7684\u76f8\u4e92\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u4e34\u5e8a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTD-HNODE\u5728\u5efa\u6a212\u578b\u7cd6\u5c3f\u75c5\u548c\u76f8\u5173\u5fc3\u8840\u7ba1\u75be\u75c5\u8fdb\u5c55\u65b9\u9762\u4f18\u4e8e\u591a\u4e2a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "TD-HNODE\u80fd\u591f\u6709\u6548\u89e3\u51b3\u75be\u75c5\u8fdb\u5c55\u5efa\u6a21\u4e2d\u7684\u6311\u6218\uff0c\u901a\u8fc7\u7ed3\u5408\u8d85\u56fe\u8868\u793a\u548c\u795e\u7ecfODE\u6846\u67b6\uff0c\u6210\u529f\u6355\u6349\u4e86\u590d\u6742\u7684\u8fde\u7eed\u65f6\u95f4\u52a8\u6001\u548c\u60a3\u8005\u5f02\u8d28\u6027\u3002"}}
{"id": "2510.17062", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17062", "abs": "https://arxiv.org/abs/2510.17062", "authors": ["Guoqing Luo", "Iffat Maab", "Lili Mou", "Junichi Yamagishi"], "title": "Investigating Thinking Behaviours of Reasoning-Based Language Models for Social Bias Mitigation", "comment": null, "summary": "While reasoning-based large language models excel at complex tasks through an\ninternal, structured thinking process, a concerning phenomenon has emerged that\nsuch a thinking process can aggregate social stereotypes, leading to biased\noutcomes. However, the underlying behaviours of these language models in social\nbias scenarios remain underexplored. In this work, we systematically\ninvestigate mechanisms within the thinking process behind this phenomenon and\nuncover two failure patterns that drive social bias aggregation: 1) stereotype\nrepetition, where the model relies on social stereotypes as its primary\njustification, and 2) irrelevant information injection, where it fabricates or\nintroduces new details to support a biased narrative. Building on these\ninsights, we introduce a lightweight prompt-based mitigation approach that\nqueries the model to review its own initial reasoning against these specific\nfailure patterns. Experiments on question answering (BBQ and StereoSet) and\nopen-ended (BOLD) benchmarks show that our approach effectively reduces bias\nwhile maintaining or improving accuracy.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5982\u4f55\u805a\u5408\u793e\u4f1a\u504f\u89c1\uff0c\u53d1\u73b0\u4e86\u4e24\u79cd\u5931\u8d25\u6a21\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u63d0\u793a\u7f13\u89e3\u65b9\u6cd5\u3002", "motivation": "\u867d\u7136\u57fa\u4e8e\u63a8\u7406\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u5185\u90e8\u7ed3\u6784\u5316\u601d\u8003\u8fc7\u7a0b\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u8fd9\u79cd\u601d\u8003\u8fc7\u7a0b\u4f1a\u805a\u5408\u793e\u4f1a\u523b\u677f\u5370\u8c61\uff0c\u5bfc\u81f4\u504f\u89c1\u7ed3\u679c\uff0c\u800c\u8fd9\u79cd\u73b0\u8c61\u7684\u5e95\u5c42\u884c\u4e3a\u673a\u5236\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u7cfb\u7edf\u7814\u7a76\u601d\u7ef4\u8fc7\u7a0b\u4e2d\u7684\u673a\u5236\uff0c\u8bc6\u522b\u51fa\u4e24\u79cd\u9a71\u52a8\u793e\u4f1a\u504f\u89c1\u805a\u5408\u7684\u5931\u8d25\u6a21\u5f0f\uff1a\u523b\u677f\u5370\u8c61\u91cd\u590d\u548c\u65e0\u5173\u4fe1\u606f\u6ce8\u5165\u3002\u57fa\u4e8e\u8fd9\u4e9b\u53d1\u73b0\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u57fa\u4e8e\u63d0\u793a\u7684\u7f13\u89e3\u65b9\u6cd5\uff0c\u8ba9\u6a21\u578b\u6839\u636e\u8fd9\u4e9b\u7279\u5b9a\u5931\u8d25\u6a21\u5f0f\u5ba1\u67e5\u81ea\u5df1\u7684\u521d\u59cb\u63a8\u7406\u3002", "result": "\u5728\u95ee\u7b54\uff08BBQ\u548cStereoSet\uff09\u548c\u5f00\u653e\u5f0f\uff08BOLD\uff09\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u51cf\u5c11\u4e86\u504f\u89c1\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u4e2d\u793e\u4f1a\u504f\u89c1\u805a\u5408\u7684\u673a\u5236\uff0c\u63d0\u51fa\u7684\u8f7b\u91cf\u7ea7\u7f13\u89e3\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u51cf\u5c11\u504f\u89c1\u800c\u4e0d\u635f\u5bb3\u6027\u80fd\u3002"}}
{"id": "2510.17235", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17235", "abs": "https://arxiv.org/abs/2510.17235", "authors": ["Chong Chen", "Ze Liu", "Lingfeng Bao", "Yanlin Wang", "Ting Chen", "Daoyuan Wu", "Jiachi Chen"], "title": "Coinvisor: An RL-Enhanced Chatbot Agent for Interactive Cryptocurrency Investment Analysis", "comment": null, "summary": "The cryptocurrency market offers significant investment opportunities but\nfaces challenges including high volatility and fragmented information. Data\nintegration and analysis are essential for informed investment decisions.\nCurrently, investors use three main approaches: (1) Manual analysis across\nvarious sources, which depends heavily on individual experience and is\ntime-consuming and prone to bias; (2) Data aggregation platforms-limited in\nfunctionality and depth of analysis; (3) Large language model agents-based on\nstatic pretrained models, lacking real-time data integration and multi-step\nreasoning capabilities. To address these limitations, we present Coinvisor, a\nreinforcement learning-based chatbot that provides comprehensive analytical\nsupport for cryptocurrency investment through a multi-agent framework.\nCoinvisor integrates diverse analytical capabilities through specialized tools.\nIts key innovation is a reinforcement learning-based tool selection mechanism\nthat enables multi-step planning and flexible integration of diverse data\nsources. This design supports real-time interaction and adaptive analysis of\ndynamic content, delivering accurate and actionable investment insights. We\nevaluated Coinvisor through automated benchmarks on tool calling accuracy and\nuser studies with 20 cryptocurrency investors using our interface. Results show\nthat Coinvisor improves recall by 40.7% and F1 score by 26.6% over the base\nmodel in tool orchestration. User studies show high satisfaction (4.64/5), with\nparticipants preferring Coinvisor to both general LLMs and existing crypto\nplatforms (4.62/5).", "AI": {"tldr": "Coinvisor\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u52a0\u5bc6\u8d27\u5e01\u6295\u8d44\u5206\u6790\u804a\u5929\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u591a\u4ee3\u7406\u6846\u67b6\u6574\u5408\u591a\u79cd\u5206\u6790\u5de5\u5177\uff0c\u63d0\u4f9b\u5b9e\u65f6\u3001\u51c6\u786e\u7684\u6295\u8d44\u6d1e\u5bdf\u3002", "motivation": "\u89e3\u51b3\u52a0\u5bc6\u8d27\u5e01\u6295\u8d44\u4e2d\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff1a\u624b\u52a8\u5206\u6790\u8017\u65f6\u4e14\u4e3b\u89c2\uff0c\u6570\u636e\u805a\u5408\u5e73\u53f0\u529f\u80fd\u6709\u9650\uff0cLLM\u4ee3\u7406\u7f3a\u4e4f\u5b9e\u65f6\u6570\u636e\u6574\u5408\u548c\u591a\u6b65\u63a8\u7406\u80fd\u529b\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5de5\u5177\u9009\u62e9\u673a\u5236\uff0c\u652f\u6301\u591a\u6b65\u89c4\u5212\u548c\u7075\u6d3b\u6574\u5408\u591a\u6837\u5316\u6570\u636e\u6e90\uff0c\u5b9e\u73b0\u5b9e\u65f6\u4ea4\u4e92\u548c\u52a8\u6001\u5185\u5bb9\u81ea\u9002\u5e94\u5206\u6790\u3002", "result": "\u5728\u5de5\u5177\u7f16\u6392\u65b9\u9762\uff0c\u76f8\u6bd4\u57fa\u7840\u6a21\u578b\u53ec\u56de\u7387\u63d0\u534740.7%\uff0cF1\u5206\u6570\u63d0\u534726.6%\uff1b\u7528\u6237\u7814\u7a76\u663e\u793a\u9ad8\u6ee1\u610f\u5ea6\uff084.64/5\uff09\uff0c\u53c2\u4e0e\u8005\u66f4\u504f\u597dCoinvisor\u800c\u975e\u901a\u7528LLM\u548c\u73b0\u6709\u52a0\u5bc6\u5e73\u53f0\uff084.62/5\uff09\u3002", "conclusion": "Coinvisor\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u9a71\u52a8\u7684\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u52a0\u5bc6\u8d27\u5e01\u6295\u8d44\u5206\u6790\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u3001\u5b9e\u65f6\u7684\u6295\u8d44\u6d1e\u5bdf\u3002"}}
{"id": "2510.16123", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16123", "abs": "https://arxiv.org/abs/2510.16123", "authors": ["Federico Malato", "Ville Hautam\u00e4ki"], "title": "Zero-shot World Models via Search in Memory", "comment": "10 pages, 8 figures in main text + appendices", "summary": "World Models have vastly permeated the field of Reinforcement Learning. Their\nability to model the transition dynamics of an environment have greatly\nimproved sample efficiency in online RL. Among them, the most notorious example\nis Dreamer, a model that learns to act in a diverse set of image-based\nenvironments. In this paper, we leverage similarity search and stochastic\nrepresentations to approximate a world model without a training procedure. We\nestablish a comparison with PlaNet, a well-established world model of the\nDreamer family. We evaluate the models on the quality of latent reconstruction\nand on the perceived similarity of the reconstructed image, on both next-step\nand long horizon dynamics prediction. The results of our study demonstrate that\na search-based world model is comparable to a training based one in both cases.\nNotably, our model show stronger performance in long-horizon prediction with\nrespect to the baseline on a range of visually different environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76f8\u4f3c\u6027\u641c\u7d22\u548c\u968f\u673a\u8868\u793a\u7684\u65e0\u8bad\u7ec3\u4e16\u754c\u6a21\u578b\uff0c\u4e0eDreamer\u5bb6\u65cf\u7684PlaNet\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\uff0c\u5728\u6f5c\u5728\u91cd\u5efa\u8d28\u91cf\u548c\u957f\u65f6\u7a0b\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u76f8\u5f53\u751a\u81f3\u66f4\u4f18\u3002", "motivation": "\u5229\u7528\u76f8\u4f3c\u6027\u641c\u7d22\u548c\u968f\u673a\u8868\u793a\u6765\u8fd1\u4f3c\u4e16\u754c\u6a21\u578b\uff0c\u907f\u514d\u590d\u6742\u7684\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u63d0\u9ad8\u6a21\u578b\u7684\u5b9e\u7528\u6027\u548c\u6548\u7387\u3002", "method": "\u4f7f\u7528\u76f8\u4f3c\u6027\u641c\u7d22\u548c\u968f\u673a\u8868\u793a\u6784\u5efa\u65e0\u8bad\u7ec3\u7684\u4e16\u754c\u6a21\u578b\uff0c\u4e0ePlaNet\u6a21\u578b\u5728\u6f5c\u5728\u91cd\u5efa\u548c\u56fe\u50cf\u76f8\u4f3c\u6027\u65b9\u9762\u8fdb\u884c\u6bd4\u8f83\u8bc4\u4f30\u3002", "result": "\u641c\u7d22\u5f0f\u4e16\u754c\u6a21\u578b\u5728\u5355\u6b65\u548c\u957f\u65f6\u7a0b\u52a8\u6001\u9884\u6d4b\u65b9\u9762\u4e0e\u57fa\u4e8e\u8bad\u7ec3\u7684\u65b9\u6cd5\u8868\u73b0\u76f8\u5f53\uff0c\u5728\u89c6\u89c9\u5dee\u5f02\u8f83\u5927\u7684\u73af\u5883\u4e2d\u957f\u65f6\u7a0b\u9884\u6d4b\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u57fa\u4e8e\u641c\u7d22\u7684\u4e16\u754c\u6a21\u578b\u53ef\u4ee5\u66ff\u4ee3\u57fa\u4e8e\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u907f\u514d\u4e86\u590d\u6742\u7684\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u7279\u522b\u662f\u5728\u957f\u65f6\u7a0b\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2510.17109", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.17109", "abs": "https://arxiv.org/abs/2510.17109", "authors": ["Tianyang Xu", "Dan Zhang", "Kushan Mitra", "Estevam Hruschka"], "title": "Verification-Aware Planning for Multi-Agent Systems", "comment": "Submission for ARR Oct", "summary": "Large language model (LLM) agents are increasingly deployed to tackle complex\ntasks, often necessitating collaboration among multiple specialized agents.\nHowever, multi-agent collaboration introduces new challenges in planning,\ncoordination, and verification. Execution failures frequently arise not from\nflawed reasoning alone, but from subtle misalignments in task interpretation,\noutput format, or inter-agent handoffs. To address these challenges, we present\nVeriMAP, a framework for multi-agent collaboration with verification-aware\nplanning. The VeriMAP planner decomposes tasks, models subtask dependencies,\nand encodes planner-defined passing criteria as subtask verification functions\n(VFs) in Python and natural language. We evaluate VeriMAP on diverse datasets,\ndemonstrating that it outperforms both single- and multi-agent baselines while\nenhancing system robustness and interpretability. Our analysis highlights how\nverification-aware planning enables reliable coordination and iterative\nrefinement in multi-agent systems, without relying on external labels or\nannotations.", "AI": {"tldr": "VeriMAP\u662f\u4e00\u4e2a\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7684\u9a8c\u8bc1\u611f\u77e5\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u4efb\u52a1\u3001\u5efa\u6a21\u5b50\u4efb\u52a1\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u5c06\u89c4\u5212\u5668\u5b9a\u4e49\u7684\u901a\u8fc7\u6807\u51c6\u7f16\u7801\u4e3aPython\u548c\u81ea\u7136\u8bed\u8a00\u7684\u5b50\u4efb\u52a1\u9a8c\u8bc1\u51fd\u6570\uff0c\u89e3\u51b3\u4e86\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u4e2d\u7684\u89c4\u5212\u3001\u534f\u8c03\u548c\u9a8c\u8bc1\u6311\u6218\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u5728\u5904\u7406\u590d\u6742\u4efb\u52a1\u65f6\u9762\u4e34\u89c4\u5212\u3001\u534f\u8c03\u548c\u9a8c\u8bc1\u7684\u65b0\u6311\u6218\uff0c\u6267\u884c\u5931\u8d25\u5f80\u5f80\u6e90\u4e8e\u4efb\u52a1\u89e3\u91ca\u3001\u8f93\u51fa\u683c\u5f0f\u6216\u667a\u80fd\u4f53\u95f4\u4ea4\u63a5\u7684\u7ec6\u5fae\u504f\u5dee\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u63a8\u7406\u7f3a\u9677\u3002", "method": "VeriMAP\u6846\u67b6\u5305\u62ec\u4efb\u52a1\u5206\u89e3\u3001\u5b50\u4efb\u52a1\u4f9d\u8d56\u5efa\u6a21\uff0c\u5e76\u5c06\u89c4\u5212\u5668\u5b9a\u4e49\u7684\u901a\u8fc7\u6807\u51c6\u7f16\u7801\u4e3aPython\u548c\u81ea\u7136\u8bed\u8a00\u7684\u5b50\u4efb\u52a1\u9a8c\u8bc1\u51fd\u6570(VFs)\u3002", "result": "\u5728\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cVeriMAP\u4f18\u4e8e\u5355\u667a\u80fd\u4f53\u548c\u591a\u667a\u80fd\u4f53\u57fa\u7ebf\uff0c\u540c\u65f6\u589e\u5f3a\u4e86\u7cfb\u7edf\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u9a8c\u8bc1\u611f\u77e5\u89c4\u5212\u80fd\u591f\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u53ef\u9760\u7684\u534f\u8c03\u548c\u8fed\u4ee3\u4f18\u5316\uff0c\u65e0\u9700\u4f9d\u8d56\u5916\u90e8\u6807\u7b7e\u6216\u6ce8\u91ca\u3002"}}
{"id": "2510.17309", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17309", "abs": "https://arxiv.org/abs/2510.17309", "authors": ["Thorsten Fr\u00f6hlich", "Tim Schlippe"], "title": "RubiSCoT: A Framework for AI-Supported Academic Assessment", "comment": null, "summary": "The evaluation of academic theses is a cornerstone of higher education,\nensuring rigor and integrity. Traditional methods, though effective, are\ntime-consuming and subject to evaluator variability. This paper presents\nRubiSCoT, an AI-supported framework designed to enhance thesis evaluation from\nproposal to final submission. Using advanced natural language processing\ntechniques, including large language models, retrieval-augmented generation,\nand structured chain-of-thought prompting, RubiSCoT offers a consistent,\nscalable solution. The framework includes preliminary assessments,\nmultidimensional assessments, content extraction, rubric-based scoring, and\ndetailed reporting. We present the design and implementation of RubiSCoT,\ndiscussing its potential to optimize academic assessment processes through\nconsistent, scalable, and transparent evaluation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aRubiSCoT\u7684AI\u652f\u6301\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u63d0\u6848\u5230\u6700\u7ec8\u63d0\u4ea4\u7684\u8bba\u6587\u8bc4\u4f30\uff0c\u5229\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u63d0\u4f9b\u4e00\u81f4\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u4f20\u7edf\u8bba\u6587\u8bc4\u4f30\u65b9\u6cd5\u8017\u65f6\u4e14\u53d7\u8bc4\u4f30\u8005\u4e3b\u89c2\u6027\u5f71\u54cd\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u3001\u4e00\u81f4\u7684\u8bc4\u4f30\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u5148\u8fdb\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\uff0c\u5305\u62ec\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u7ed3\u6784\u5316\u601d\u7ef4\u94fe\u63d0\u793a\uff0c\u8fdb\u884c\u521d\u6b65\u8bc4\u4f30\u3001\u591a\u7ef4\u8bc4\u4f30\u3001\u5185\u5bb9\u63d0\u53d6\u3001\u57fa\u4e8e\u91cf\u89c4\u7684\u8bc4\u5206\u548c\u8be6\u7ec6\u62a5\u544a\u3002", "result": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86RubiSCoT\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u5176\u5728\u4f18\u5316\u5b66\u672f\u8bc4\u4f30\u8fc7\u7a0b\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "RubiSCoT\u6709\u6f5c\u529b\u901a\u8fc7\u4e00\u81f4\u3001\u53ef\u6269\u5c55\u548c\u900f\u660e\u7684\u8bc4\u4f30\u6765\u4f18\u5316\u5b66\u672f\u8bc4\u4f30\u6d41\u7a0b\u3002"}}
{"id": "2510.17115", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17115", "abs": "https://arxiv.org/abs/2510.17115", "authors": ["Wei Du", "Nuowei Liu", "Jie Wang", "Jiahao Kuang", "Tao Ji", "Xiaoling Wang", "Yuanbin Wu"], "title": "DVAGen: Dynamic Vocabulary Augmented Generation", "comment": null, "summary": "Language models trained with a fixed vocabulary struggle to generalize to\nnovel or out-of-vocabulary words, limiting their flexibility in handling\ndiverse token combinations. Existing dynamic vocabulary approaches attempt to\naddress this limitation but face challenges such as fragmented codebases, lack\nof support for modern LLMs, and limited inference scalability. To overcome\nthese issues, we introduce DVAGen, a fully open-source, unified framework\ndesigned for training, evaluation, and visualization of dynamic\nvocabulary-augmented language models. Our framework modularizes the pipeline\nfor ease of customization, integrates seamlessly with open-source LLMs, and is\nthe first to provide both CLI and WebUI tools for real-time result inspection.\nWe validate the effectiveness of dynamic vocabulary methods on modern LLMs and\ndemonstrate support for batch inference, significantly improving inference\nthroughput.", "AI": {"tldr": "DVAGen\u662f\u4e00\u4e2a\u5f00\u6e90\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u8bad\u7ec3\u3001\u8bc4\u4f30\u548c\u53ef\u89c6\u5316\u52a8\u6001\u8bcd\u6c47\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4ee3\u7801\u5e93\u788e\u7247\u5316\u3001\u7f3a\u4e4f\u73b0\u4ee3LLM\u652f\u6301\u548c\u63a8\u7406\u53ef\u6269\u5c55\u6027\u6709\u9650\u7684\u95ee\u9898\u3002", "motivation": "\u56fa\u5b9a\u8bcd\u6c47\u8868\u8bad\u7ec3\u7684\u8bed\u8a00\u6a21\u578b\u96be\u4ee5\u6cdb\u5316\u5230\u65b0\u8bcd\u6216\u8bcd\u6c47\u8868\u5916\u8bcd\uff0c\u9650\u5236\u4e86\u5904\u7406\u591a\u6837\u5316\u8bcd\u7ec4\u5408\u7684\u7075\u6d3b\u6027\u3002\u73b0\u6709\u52a8\u6001\u8bcd\u6c47\u65b9\u6cd5\u5b58\u5728\u4ee3\u7801\u5e93\u788e\u7247\u5316\u3001\u7f3a\u4e4f\u73b0\u4ee3LLM\u652f\u6301\u548c\u63a8\u7406\u53ef\u6269\u5c55\u6027\u6709\u9650\u7b49\u6311\u6218\u3002", "method": "DVAGen\u6846\u67b6\u6a21\u5757\u5316\u5904\u7406\u6d41\u7a0b\u4ee5\u4fbf\u5b9a\u5236\uff0c\u65e0\u7f1d\u96c6\u6210\u5f00\u6e90LLM\uff0c\u9996\u6b21\u63d0\u4f9bCLI\u548cWebUI\u5de5\u5177\u8fdb\u884c\u5b9e\u65f6\u7ed3\u679c\u68c0\u67e5\uff0c\u652f\u6301\u6279\u91cf\u63a8\u7406\u663e\u8457\u63d0\u9ad8\u63a8\u7406\u541e\u5410\u91cf\u3002", "result": "\u9a8c\u8bc1\u4e86\u52a8\u6001\u8bcd\u6c47\u65b9\u6cd5\u5728\u73b0\u4ee3LLM\u4e0a\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u6279\u91cf\u63a8\u7406\u652f\u6301\u5e26\u6765\u7684\u663e\u8457\u63a8\u7406\u541e\u5410\u91cf\u63d0\u5347\u3002", "conclusion": "DVAGen\u901a\u8fc7\u63d0\u4f9b\u7edf\u4e00\u3001\u6a21\u5757\u5316\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u52a8\u6001\u8bcd\u6c47\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u548c\u63a8\u7406\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2510.17382", "categories": ["cs.AI", "cs.LG", "cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17382", "abs": "https://arxiv.org/abs/2510.17382", "authors": ["Rishabh Jain", "Keisuke Okumura", "Michael Amir", "Amanda Prorok"], "title": "Graph Attention-Guided Search for Dense Multi-Agent Pathfinding", "comment": null, "summary": "Finding near-optimal solutions for dense multi-agent pathfinding (MAPF)\nproblems in real-time remains challenging even for state-of-the-art planners.\nTo this end, we develop a hybrid framework that integrates a learned heuristic\nderived from MAGAT, a neural MAPF policy with a graph attention scheme, into a\nleading search-based algorithm, LaCAM. While prior work has explored\nlearning-guided search in MAPF, such methods have historically underperformed.\nIn contrast, our approach, termed LaGAT, outperforms both purely search-based\nand purely learning-based methods in dense scenarios. This is achieved through\nan enhanced MAGAT architecture, a pre-train-then-fine-tune strategy on maps of\ninterest, and a deadlock detection scheme to account for imperfect neural\nguidance. Our results demonstrate that, when carefully designed, hybrid search\noffers a powerful solution for tightly coupled, challenging multi-agent\ncoordination problems.", "AI": {"tldr": "\u63d0\u51faLaGAT\u6846\u67b6\uff0c\u5c06\u57fa\u4e8e\u56fe\u6ce8\u610f\u529b\u7684\u795e\u7ecf\u7f51\u7edc\u7b56\u7565MAGAT\u96c6\u6210\u5230\u641c\u7d22\u7b97\u6cd5LaCAM\u4e2d\uff0c\u7528\u4e8e\u89e3\u51b3\u5bc6\u96c6\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u95ee\u9898\uff0c\u5728\u5bc6\u96c6\u573a\u666f\u4e2d\u4f18\u4e8e\u7eaf\u641c\u7d22\u548c\u7eaf\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5bc6\u96c6\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u95ee\u9898\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u7ed3\u5408\u5b66\u4e60\u548c\u641c\u7d22\u7684\u4f18\u52bf\u6765\u627e\u5230\u5b9e\u65f6\u8fd1\u4f18\u89e3\u3002", "method": "\u4f7f\u7528\u589e\u5f3a\u7684MAGAT\u67b6\u6784\uff0c\u91c7\u7528\u9884\u8bad\u7ec3-\u5fae\u8c03\u7b56\u7565\uff0c\u7ed3\u5408\u6b7b\u9501\u68c0\u6d4b\u673a\u5236\u6765\u5904\u7406\u4e0d\u5b8c\u7f8e\u7684\u795e\u7ecf\u5f15\u5bfc\u3002", "result": "LaGAT\u5728\u5bc6\u96c6\u573a\u666f\u4e2d\u8d85\u8d8a\u4e86\u7eaf\u641c\u7d22\u548c\u7eaf\u5b66\u4e60\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u6df7\u5408\u641c\u7d22\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6df7\u5408\u641c\u7d22\u65b9\u6cd5\u4e3a\u89e3\u51b3\u7d27\u5bc6\u8026\u5408\u7684\u590d\u6742\u591a\u667a\u80fd\u4f53\u534f\u8c03\u95ee\u9898\u63d0\u4f9b\u4e86\u5f3a\u5927\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.17139", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.17139", "abs": "https://arxiv.org/abs/2510.17139", "authors": ["Zhichao Xu", "Shengyao Zhuang", "Xueguang Ma", "Bingsen Chen", "Yijun Tian", "Fengran Mo", "Jie Cao", "Vivek Srikumar"], "title": "Rethinking On-policy Optimization for Query Augmentation", "comment": null, "summary": "Recent advances in large language models (LLMs) have led to a surge of\ninterest in query augmentation for information retrieval (IR). Two main\napproaches have emerged. The first prompts LLMs to generate answers or\npseudo-documents that serve as new queries, relying purely on the model's\nparametric knowledge or contextual information. The second applies\nreinforcement learning (RL) to fine-tune LLMs for query rewriting, directly\noptimizing retrieval metrics. While having respective advantages and\nlimitations, the two approaches have not been compared under consistent\nexperimental conditions. In this work, we present the first systematic\ncomparison of prompting-based and RL-based query augmentation across diverse\nbenchmarks, including evidence-seeking, ad hoc, and tool retrieval. Our key\nfinding is that simple, training-free query augmentation often performs on par\nwith, or even surpasses, more expensive RL-based counterparts, especially when\nusing powerful LLMs. Motivated by this discovery, we introduce a novel hybrid\nmethod, On-policy Pseudo-document Query Expansion (OPQE), which, instead of\nrewriting a query, the LLM policy learns to generate a pseudo-document that\nmaximizes retrieval performance, thus merging the flexibility and generative\nstructure of prompting with the targeted optimization of RL. We show OPQE\noutperforms both standalone prompting and RL-based rewriting, demonstrating\nthat a synergistic approach yields the best results. Our implementation is made\navailable to facilitate reproducibility.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u6bd4\u8f83\u4e86\u57fa\u4e8e\u63d0\u793a\u548c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u67e5\u8be2\u589e\u5f3a\u65b9\u6cd5\uff0c\u53d1\u73b0\u7b80\u5355\u7684\u65e0\u8bad\u7ec3\u67e5\u8be2\u589e\u5f3a\u65b9\u6cd5\u5728\u5f3a\u5927LLMs\u4e0b\u8868\u73b0\u4e0e\u6602\u8d35\u7684RL\u65b9\u6cd5\u76f8\u5f53\u751a\u81f3\u66f4\u597d\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u7684\u6df7\u5408\u65b9\u6cd5OPQE\u3002", "motivation": "\u5f53\u524dLLMs\u5728\u4fe1\u606f\u68c0\u7d22\u4e2d\u7684\u67e5\u8be2\u589e\u5f3a\u4e3b\u8981\u6709\u4e24\u79cd\u65b9\u6cd5\uff1a\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\u548c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u4f46\u7f3a\u4e4f\u5728\u4e00\u81f4\u5b9e\u9a8c\u6761\u4ef6\u4e0b\u7684\u7cfb\u7edf\u6bd4\u8f83\u3002", "method": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7cfb\u7edf\u6bd4\u8f83\u4e24\u79cd\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u6df7\u5408\u65b9\u6cd5OPQE\u2014\u2014LLM\u7b56\u7565\u5b66\u4e60\u751f\u6210\u6700\u5927\u5316\u68c0\u7d22\u6027\u80fd\u7684\u4f2a\u6587\u6863\uff0c\u7ed3\u5408\u4e86\u63d0\u793a\u7684\u7075\u6d3b\u6027\u548cRL\u7684\u5b9a\u5411\u4f18\u5316\u3002", "result": "\u7b80\u5355\u7684\u65e0\u8bad\u7ec3\u67e5\u8be2\u589e\u5f3a\u65b9\u6cd5\u5728\u5f3a\u5927LLMs\u4e0b\u8868\u73b0\u4e0eRL\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u597d\uff1bOPQE\u65b9\u6cd5\u4f18\u4e8e\u5355\u72ec\u7684\u63d0\u793a\u65b9\u6cd5\u548cRL\u91cd\u5199\u65b9\u6cd5\u3002", "conclusion": "\u534f\u540c\u65b9\u6cd5\u80fd\u4ea7\u751f\u6700\u4f73\u7ed3\u679c\uff0cOPQE\u5c55\u793a\u4e86\u7ed3\u5408\u63d0\u793a\u7075\u6d3b\u6027\u548cRL\u4f18\u5316\u7684\u4f18\u52bf\u3002"}}
{"id": "2510.17418", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.17418", "abs": "https://arxiv.org/abs/2510.17418", "authors": ["Mustafa F. Abdelwahed", "Alice Toniolo", "Joan Espasa", "Ian P. Gent"], "title": "Diverse Planning with Simulators via Linear Temporal Logic", "comment": null, "summary": "Autonomous agents rely on automated planning algorithms to achieve their\nobjectives. Simulation-based planning offers a significant advantage over\ndeclarative models in modelling complex environments. However, relying solely\non a planner that produces a single plan may not be practical, as the generated\nplans may not always satisfy the agent's preferences. To address this\nlimitation, we introduce $\\texttt{FBI}_\\texttt{LTL}$, a diverse planner\nexplicitly designed for simulation-based planning problems.\n$\\texttt{FBI}_\\texttt{LTL}$ utilises Linear Temporal Logic (LTL) to define\nsemantic diversity criteria, enabling agents to specify what constitutes\nmeaningfully different plans. By integrating these LTL-based diversity models\ndirectly into the search process, $\\texttt{FBI}_\\texttt{LTL}$ ensures the\ngeneration of semantically diverse plans, addressing a critical limitation of\nexisting diverse planning approaches that may produce syntactically different\nbut semantically identical solutions. Extensive evaluations on various\nbenchmarks consistently demonstrate that $\\texttt{FBI}_\\texttt{LTL}$ generates\nmore diverse plans compared to a baseline approach. This work establishes the\nfeasibility of semantically-guided diverse planning in simulation-based\nenvironments, paving the way for innovative approaches in realistic,\nnon-symbolic domains where traditional model-based approaches fail.", "AI": {"tldr": "\u63d0\u51fa\u4e86FBI_LTL\uff0c\u4e00\u4e2a\u4e13\u95e8\u4e3a\u57fa\u4e8e\u4eff\u771f\u7684\u89c4\u5212\u95ee\u9898\u8bbe\u8ba1\u7684\u591a\u6837\u5316\u89c4\u5212\u5668\uff0c\u4f7f\u7528\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91\u5b9a\u4e49\u8bed\u4e49\u591a\u6837\u6027\u6807\u51c6\uff0c\u751f\u6210\u8bed\u4e49\u4e0a\u4e0d\u540c\u7684\u8ba1\u5212\u3002", "motivation": "\u4f20\u7edf\u89c4\u5212\u5668\u53ea\u751f\u6210\u5355\u4e00\u8ba1\u5212\uff0c\u53ef\u80fd\u65e0\u6cd5\u6ee1\u8db3\u4ee3\u7406\u504f\u597d\uff1b\u73b0\u6709\u591a\u6837\u5316\u89c4\u5212\u65b9\u6cd5\u53ef\u80fd\u4ea7\u751f\u8bed\u6cd5\u4e0d\u540c\u4f46\u8bed\u4e49\u76f8\u540c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "FBI_LTL\u5c06\u57fa\u4e8eLTL\u7684\u591a\u6837\u6027\u6a21\u578b\u76f4\u63a5\u96c6\u6210\u5230\u641c\u7d22\u8fc7\u7a0b\u4e2d\uff0c\u4f7f\u7528\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91\u5b9a\u4e49\u8bed\u4e49\u591a\u6837\u6027\u6807\u51c6\u3002", "result": "\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFBI_LTL\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u80fd\u751f\u6210\u66f4\u591a\u6837\u5316\u7684\u8ba1\u5212\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u786e\u7acb\u4e86\u5728\u57fa\u4e8e\u4eff\u771f\u7684\u73af\u5883\u4e2d\u8fdb\u884c\u8bed\u4e49\u5f15\u5bfc\u591a\u6837\u5316\u89c4\u5212\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u5728\u4f20\u7edf\u57fa\u4e8e\u6a21\u578b\u65b9\u6cd5\u5931\u6548\u7684\u73b0\u5b9e\u975e\u7b26\u53f7\u9886\u57df\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2510.17168", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17168", "abs": "https://arxiv.org/abs/2510.17168", "authors": ["Xiaohui Rao", "Hanlin Wu", "Zhenguang G. Cai"], "title": "When AI companions become witty: Can human brain recognize AI-generated irony?", "comment": null, "summary": "As Large Language Models (LLMs) are increasingly deployed as social agents\nand trained to produce humor and irony, a question emerges: when encountering\nwitty AI remarks, do people interpret these as intentional communication or\nmere computational output? This study investigates whether people adopt the\nintentional stance, attributing mental states to explain behavior,toward AI\nduring irony comprehension. Irony provides an ideal paradigm because it\nrequires distinguishing intentional contradictions from unintended errors\nthrough effortful semantic reanalysis. We compared behavioral and neural\nresponses to ironic statements from AI versus human sources using established\nERP components: P200 reflecting early incongruity detection and P600 indexing\ncognitive efforts in reinterpreting incongruity as deliberate irony. Results\ndemonstrate that people do not fully adopt the intentional stance toward\nAI-generated irony. Behaviorally, participants attributed incongruity to\ndeliberate communication for both sources, though significantly less for AI\nthan human, showing greater tendency to interpret AI incongruities as\ncomputational errors. Neural data revealed attenuated P200 and P600 effects for\nAI-generated irony, suggesting reduced effortful detection and reanalysis\nconsistent with diminished attribution of communicative intent. Notably, people\nwho perceived AI as more sincere showed larger P200 and P600 effects for\nAI-generated irony, suggesting that intentional stance adoption is calibrated\nby specific mental models of artificial agents. These findings reveal that\nsource attribution shapes neural processing of social-communicative phenomena.\nDespite current LLMs' linguistic sophistication, achieving genuine social\nagency requires more than linguistic competence, it necessitates a shift in how\nhumans perceive and attribute intentionality to artificial agents.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u4eba\u4eec\u5bf9AI\u751f\u6210\u7684\u8bbd\u523a\u8a00\u8bba\u4e0d\u4f1a\u5b8c\u5168\u91c7\u7528\u610f\u5411\u7acb\u573a\uff0c\u5728\u884c\u4e3a\u548c\u795e\u7ecf\u5c42\u9762\u90fd\u8868\u73b0\u51fa\u5bf9AI\u610f\u56fe\u5f52\u56e0\u7684\u51cf\u5f31\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u4f5c\u4e3a\u793e\u4ea4\u4ee3\u7406\u90e8\u7f72\u5e76\u88ab\u8bad\u7ec3\u4ea7\u751f\u5e7d\u9ed8\u548c\u8bbd\u523a\uff0c\u9700\u8981\u4e86\u89e3\u4eba\u4eec\u662f\u5426\u4f1a\u5c06AI\u7684\u673a\u667a\u8a00\u8bba\u89c6\u4e3a\u6709\u610f\u7684\u6c9f\u901a\u8fd8\u662f\u7eaf\u7cb9\u7684\u8ba1\u7b97\u8f93\u51fa\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83AI\u4e0e\u4eba\u7c7b\u6765\u6e90\u8bbd\u523a\u8bed\u53e5\u7684\u884c\u4e3a\u548c\u795e\u7ecf\u53cd\u5e94\uff0c\u4f7f\u7528ERP\u6210\u5206\uff08P200\u53cd\u6620\u65e9\u671f\u4e0d\u4e00\u81f4\u6027\u68c0\u6d4b\uff0cP600\u7d22\u5f15\u91cd\u65b0\u89e3\u91ca\u4e0d\u4e00\u81f4\u6027\u4e3a\u6545\u610f\u8bbd\u523a\u7684\u8ba4\u77e5\u52aa\u529b\uff09\u3002", "result": "\u884c\u4e3a\u4e0a\uff0c\u53c2\u4e0e\u8005\u5bf9AI\u7684\u6545\u610f\u6c9f\u901a\u5f52\u56e0\u663e\u8457\u5c11\u4e8e\u4eba\u7c7b\uff1b\u795e\u7ecf\u6570\u636e\u663e\u793aAI\u751f\u6210\u8bbd\u523a\u7684P200\u548cP600\u6548\u5e94\u51cf\u5f31\uff0c\u8868\u660e\u68c0\u6d4b\u548c\u91cd\u65b0\u5206\u6790\u7684\u52aa\u529b\u51cf\u5c11\u3002\u611f\u77e5AI\u66f4\u771f\u8bda\u7684\u4eba\u5bf9AI\u8bbd\u523a\u663e\u793a\u51fa\u66f4\u5927\u7684P200\u548cP600\u6548\u5e94\u3002", "conclusion": "\u5c3d\u7ba1\u5f53\u524dLLMs\u5177\u6709\u8bed\u8a00\u590d\u6742\u6027\uff0c\u4f46\u5b9e\u73b0\u771f\u6b63\u7684\u793e\u4ea4\u4ee3\u7406\u4e0d\u4ec5\u9700\u8981\u8bed\u8a00\u80fd\u529b\uff0c\u8fd8\u9700\u8981\u4eba\u7c7b\u5bf9\u4eba\u5de5\u4ee3\u7406\u7684\u610f\u5411\u6027\u5f52\u56e0\u65b9\u5f0f\u7684\u8f6c\u53d8\u3002"}}
{"id": "2510.17450", "categories": ["cs.AI", "H.4.2; I.2.3; I.2.6; I.2.8; I.2.9; J.7"], "pdf": "https://arxiv.org/pdf/2510.17450", "abs": "https://arxiv.org/abs/2510.17450", "authors": ["Johan Schubert", "Farzad Kamrani", "Tove Gustavi"], "title": "Active Inference for an Intelligent Agent in Autonomous Reconnaissance Missions", "comment": "Presented at the 6th International Workshop on Active Inference,\n  15-17 October 2025, Montreal, Canada", "summary": "We develop an active inference route-planning method for the autonomous\ncontrol of intelligent agents. The aim is to reconnoiter a geographical area to\nmaintain a common operational picture. To achieve this, we construct an\nevidence map that reflects our current understanding of the situation,\nincorporating both positive and \"negative\" sensor observations of possible\ntarget objects collected over time, and diffusing the evidence across the map\nas time progresses. The generative model of active inference uses\nDempster-Shafer theory and a Gaussian sensor model, which provides input to the\nagent. The generative process employs a Bayesian approach to update a posterior\nprobability distribution. We calculate the variational free energy for all\npositions within the area by assessing the divergence between a pignistic\nprobability distribution of the evidence map and a posterior probability\ndistribution of a target object based on the observations, including the level\nof surprise associated with receiving new observations. Using the free energy,\nwe direct the agents' movements in a simulation by taking an incremental step\ntoward a position that minimizes the free energy. This approach addresses the\nchallenge of exploration and exploitation, allowing agents to balance searching\nextensive areas of the geographical map while tracking identified target\nobjects.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e3b\u52a8\u63a8\u7406\u7684\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u4e3b\u63a7\u5236\u667a\u80fd\u4ee3\u7406\uff0c\u901a\u8fc7\u6784\u5efa\u8bc1\u636e\u5730\u56fe\u6765\u7ef4\u6301\u5171\u540c\u4f5c\u6218\u6001\u52bf\uff0c\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\u3002", "motivation": "\u89e3\u51b3\u667a\u80fd\u4ee3\u7406\u5728\u5730\u7406\u533a\u57df\u4fa6\u5bdf\u4e2d\u7684\u63a2\u7d22\u4e0e\u5229\u7528\u5e73\u8861\u95ee\u9898\uff0c\u7ef4\u6301\u5bf9\u6001\u52bf\u7684\u6301\u7eed\u7406\u89e3\u3002", "method": "\u4f7f\u7528Dempster-Shafer\u7406\u8bba\u548c\u9ad8\u65af\u4f20\u611f\u5668\u6a21\u578b\u6784\u5efa\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u53d8\u5206\u81ea\u7531\u80fd\u91cf\u8ba1\u7b97\u6307\u5bfc\u4ee3\u7406\u79fb\u52a8\uff0c\u6700\u5c0f\u5316\u81ea\u7531\u80fd\u91cf\u3002", "result": "\u5f00\u53d1\u4e86\u80fd\u591f\u5e73\u8861\u533a\u57df\u641c\u7d22\u548c\u76ee\u6807\u8ddf\u8e2a\u7684\u81ea\u4e3b\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5730\u7406\u533a\u57df\u4fa6\u5bdf\u4e2d\u7684\u63a2\u7d22\u4e0e\u5229\u7528\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u81ea\u4e3b\u667a\u80fd\u4ee3\u7406\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2510.17196", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17196", "abs": "https://arxiv.org/abs/2510.17196", "authors": ["Jiaqi Leng", "Xiang Hu", "Junxiong Wang", "Jianguo Li", "Wei Wu", "Yucheng Lu"], "title": "Understanding and Improving Length Generalization in Hierarchical Sparse Attention Models", "comment": "Preprint. Work in progress", "summary": "Effectively processing long contexts is a critical challenge for language\nmodels. While standard Transformers are limited by quadratic complexity and\npoor length extrapolation, alternative architectures like sliding window\nattention and state space models sacrifice the ability to effectively utilize\nthe full context due to their fixed-size memory. Chunk-based sparse attention\nhas emerged as a promising paradigm for extreme length generalization, yet the\nkey architectural principles underpinning its success are not yet fully\nunderstood. In this work, we present a systematic dissection of these models to\nidentify the core components driving their performance. Through a unified\nframework and comprehensive ablation studies, we demonstrate that a combination\nof three design principles is critical: (1) an expressive, non-linear Chunk\nEncoder with a dedicated CLS token to produce representations for retrieval;\n(2) a Bypassing Residual Path to stably integrate retrieved global information\nwithout it being overridden by the local residual stream; and (3) enforced\nselection sparsity during pre-training to bridge the train-test distribution\ngap. We provide a theoretical motivation for intra-chunk information processing\nand landmark generation. By combining these principles, we establish a new\nstate-of-the-art for training-free length extrapolation, successfully\ngeneralizing models trained on a 4K context to 32 million tokens on RULER and\nBABILong. Our findings provide a clear and empirically-grounded set of design\nprinciples for developing future, highly-capable long-context language models.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5206\u6790\u4e86\u57fa\u4e8e\u5206\u5757\u7a00\u758f\u6ce8\u610f\u529b\u7684\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u6a21\u578b\uff0c\u8bc6\u522b\u51fa\u4e09\u4e2a\u5173\u952e\u8bbe\u8ba1\u539f\u5219\uff1a\u8868\u8fbe\u6027\u5206\u5757\u7f16\u7801\u5668\u3001\u65c1\u8def\u6b8b\u5dee\u8def\u5f84\u548c\u8bad\u7ec3\u4e2d\u5f3a\u5236\u9009\u62e9\u7a00\u758f\u6027\uff0c\u5b9e\u73b0\u4e86\u4ece4K\u52303200\u4e07token\u7684\u65e0\u8bad\u7ec3\u957f\u5ea6\u5916\u63a8\u3002", "motivation": "\u6807\u51c6Transformer\u53d7\u9650\u4e8e\u4e8c\u6b21\u590d\u6742\u5ea6\u548c\u957f\u5ea6\u5916\u63a8\u80fd\u529b\u5dee\uff0c\u800c\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\u7b49\u66ff\u4ee3\u67b6\u6784\u56e0\u56fa\u5b9a\u5927\u5c0f\u5185\u5b58\u65e0\u6cd5\u6709\u6548\u5229\u7528\u5b8c\u6574\u4e0a\u4e0b\u6587\u3002\u5206\u5757\u7a00\u758f\u6ce8\u610f\u529b\u867d\u5728\u6781\u7aef\u957f\u5ea6\u6cdb\u5316\u65b9\u9762\u6709\u524d\u666f\uff0c\u4f46\u5176\u6210\u529f\u7684\u5173\u952e\u67b6\u6784\u539f\u5219\u5c1a\u672a\u5b8c\u5168\u7406\u89e3\u3002", "method": "\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u548c\u5168\u9762\u6d88\u878d\u7814\u7a76\uff0c\u7cfb\u7edf\u5256\u6790\u5206\u5757\u7a00\u758f\u6ce8\u610f\u529b\u6a21\u578b\uff0c\u8bc6\u522b\u4e09\u4e2a\u5173\u952e\u8bbe\u8ba1\u539f\u5219\uff1a\u8868\u8fbe\u6027\u5206\u5757\u7f16\u7801\u5668\u3001\u65c1\u8def\u6b8b\u5dee\u8def\u5f84\u548c\u8bad\u7ec3\u4e2d\u5f3a\u5236\u9009\u62e9\u7a00\u758f\u6027\u3002", "result": "\u7ed3\u5408\u8fd9\u4e09\u4e2a\u539f\u5219\uff0c\u5728RULER\u548cBABILong\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u4ece4K\u4e0a\u4e0b\u6587\u8bad\u7ec3\u52303200\u4e07token\u7684\u65e0\u8bad\u7ec3\u957f\u5ea6\u5916\u63a8\uff0c\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5f00\u53d1\u672a\u6765\u9ad8\u6027\u80fd\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u5957\u6e05\u6670\u4e14\u7ecf\u9a8c\u57fa\u7840\u7684\u8bbe\u8ba1\u539f\u5219\u3002"}}
{"id": "2510.17463", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17463", "abs": "https://arxiv.org/abs/2510.17463", "authors": ["Cor Steging", "Tadeusz Zbiegie\u0144"], "title": "Label Indeterminacy in AI & Law", "comment": "This manuscript has been accepted for presentation as a short paper\n  at the 38th International Conference on Legal Knowledge and Information\n  Systems (JURIX) in Turin, December 9 to 11 of 2025", "summary": "Machine learning is increasingly used in the legal domain, where it typically\noperates retrospectively by treating past case outcomes as ground truth.\nHowever, legal outcomes are often shaped by human interventions that are not\ncaptured in most machine learning approaches. A final decision may result from\na settlement, an appeal, or other procedural actions. This creates label\nindeterminacy: the outcome could have been different if the intervention had or\nhad not taken place. We argue that legal machine learning applications need to\naccount for label indeterminacy. Methods exist that can impute these\nindeterminate labels, but they are all grounded in unverifiable assumptions. In\nthe context of classifying cases from the European Court of Human Rights, we\nshow that the way that labels are constructed during training can significantly\naffect model behaviour. We therefore position label indeterminacy as a relevant\nconcern in AI & Law and demonstrate how it can shape model behaviour.", "AI": {"tldr": "\u6cd5\u5f8b\u673a\u5668\u5b66\u4e60\u4e2d\u6807\u7b7e\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\uff1a\u6cd5\u5f8b\u5224\u51b3\u7ed3\u679c\u5e38\u53d7\u4eba\u4e3a\u5e72\u9884\u5f71\u54cd\uff0c\u5bfc\u81f4\u6807\u7b7e\u4e0d\u786e\u5b9a\u6027\uff0c\u8fd9\u4f1a\u663e\u8457\u5f71\u54cd\u6a21\u578b\u884c\u4e3a\u3002", "motivation": "\u6cd5\u5f8b\u673a\u5668\u5b66\u4e60\u901a\u5e38\u5c06\u8fc7\u53bb\u6848\u4ef6\u7ed3\u679c\u89c6\u4e3a\u771f\u5b9e\u6807\u7b7e\uff0c\u4f46\u6cd5\u5f8b\u7ed3\u679c\u5e38\u53d7\u548c\u89e3\u3001\u4e0a\u8bc9\u7b49\u7a0b\u5e8f\u6027\u5e72\u9884\u5f71\u54cd\uff0c\u9020\u6210\u6807\u7b7e\u4e0d\u786e\u5b9a\u6027\uff0c\u9700\u8981\u5bf9\u6b64\u8fdb\u884c\u8003\u91cf\u3002", "method": "\u5728\u6b27\u6d32\u4eba\u6743\u6cd5\u9662\u6848\u4ef6\u5206\u7c7b\u80cc\u666f\u4e0b\uff0c\u7814\u7a76\u4e0d\u540c\u6807\u7b7e\u6784\u5efa\u65b9\u5f0f\u5bf9\u6a21\u578b\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u63a2\u8ba8\u6807\u7b7e\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6807\u7b7e\u7684\u6784\u5efa\u65b9\u5f0f\u4f1a\u663e\u8457\u5f71\u54cd\u6a21\u578b\u7684\u884c\u4e3a\u8868\u73b0\u3002", "conclusion": "\u6807\u7b7e\u4e0d\u786e\u5b9a\u6027\u662fAI\u4e0e\u6cd5\u5f8b\u9886\u57df\u7684\u91cd\u8981\u95ee\u9898\uff0c\u9700\u8981\u88ab\u7eb3\u5165\u8003\u91cf\uff0c\u56e0\u4e3a\u5b83\u4f1a\u5f71\u54cd\u6a21\u578b\u7684\u884c\u4e3a\u7279\u5f81\u3002"}}
{"id": "2510.16165", "categories": ["cs.LG", "cond-mat.supr-con"], "pdf": "https://arxiv.org/pdf/2510.16165", "abs": "https://arxiv.org/abs/2510.16165", "authors": ["Charles Rhys Campbell", "Aldo H. Romero", "Kamal Choudhary"], "title": "AtomBench: A Benchmark for Generative Atomic Structure Models using GPT, Diffusion, and Flow Architectures", "comment": null, "summary": "Generative models have become significant assets in the exploration and\nidentification of new materials, enabling the rapid proposal of candidate\ncrystal structures that satisfy target properties. Despite the increasing\nadoption of diverse architectures, a rigorous comparative evaluation of their\nperformance on materials datasets is lacking. In this work, we present a\nsystematic benchmark of three representative generative models- AtomGPT (a\ntransformer-based model), Crystal Diffusion Variational Autoencoder (CDVAE),\nand FlowMM (a Riemannian flow matching model). These models were trained to\nreconstruct crystal structures from subsets of two publicly available\nsuperconductivity datasets- JARVIS Supercon 3D and DS A/B from the Alexandria\ndatabase. Performance was assessed using the Kullback-Leibler (KL) divergence\nbetween predicted and reference distributions of lattice parameters, as well as\nthe mean absolute error (MAE) of individual lattice constants. For the computed\nKLD and MAE scores, CDVAE performs most favorably, followed by AtomGPT, and\nthen FlowMM. All benchmarking code and model configurations will be made\npublicly available at https://github.com/atomgptlab/atombench_inverse.", "AI": {"tldr": "\u5bf9\u4e09\u79cd\u4ee3\u8868\u6027\u751f\u6210\u6a21\u578b\uff08AtomGPT\u3001CDVAE\u3001FlowMM\uff09\u5728\u8d85\u5bfc\u6750\u6599\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u8fdb\u884c\u7cfb\u7edf\u6027\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53d1\u73b0CDVAE\u8868\u73b0\u6700\u4f73", "motivation": "\u5c3d\u7ba1\u751f\u6210\u6a21\u578b\u5728\u6750\u6599\u53d1\u73b0\u4e2d\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5176\u6027\u80fd\u7684\u4e25\u683c\u6bd4\u8f83\u8bc4\u4f30", "method": "\u4f7f\u7528\u4e24\u79cd\u516c\u5f00\u8d85\u5bfc\u6570\u636e\u96c6\u8bad\u7ec3\u4e09\u79cd\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7KL\u6563\u5ea6\u548c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u8bc4\u4f30\u6676\u683c\u53c2\u6570\u91cd\u5efa\u6027\u80fd", "result": "CDVAE\u8868\u73b0\u6700\u4f18\uff0c\u5176\u6b21\u662fAtomGPT\uff0cFlowMM\u8868\u73b0\u6700\u5dee", "conclusion": "CDVAE\u5728\u6676\u4f53\u7ed3\u6784\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u4e3a\u6750\u6599\u53d1\u73b0\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u57fa\u51c6"}}
{"id": "2510.17210", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17210", "abs": "https://arxiv.org/abs/2510.17210", "authors": ["Chenchen Tan", "Youyang Qu", "Xinghao Li", "Hui Zhang", "Shujie Cui", "Cunjian Chen", "Longxiang Gao"], "title": "Wisdom is Knowing What not to Say: Hallucination-Free LLMs Unlearning via Attention Shifting", "comment": "22 pages, 10 figures", "summary": "The increase in computing power and the necessity of AI-assisted\ndecision-making boost the growing application of large language models (LLMs).\nAlong with this, the potential retention of sensitive data of LLMs has spurred\nincreasing research into machine unlearning. However, existing unlearning\napproaches face a critical dilemma: Aggressive unlearning compromises model\nutility, while conservative strategies preserve utility but risk hallucinated\nresponses. This significantly limits LLMs' reliability in knowledge-intensive\napplications. To address this, we introduce a novel Attention-Shifting (AS)\nframework for selective unlearning. AS is driven by two design objectives: (1)\ncontext-preserving suppression that attenuates attention to fact-bearing tokens\nwithout disrupting LLMs' linguistic structure; and (2) hallucination-resistant\nresponse shaping that discourages fabricated completions when queried about\nunlearning content. AS realizes these objectives through two attention-level\ninterventions, which are importance-aware suppression applied to the unlearning\nset to reduce reliance on memorized knowledge and attention-guided retention\nenhancement that reinforces attention toward semantically essential tokens in\nthe retained dataset to mitigate unintended degradation. These two components\nare jointly optimized via a dual-loss objective, which forms a soft boundary\nthat localizes unlearning while preserving unrelated knowledge under\nrepresentation superposition. Experimental results show that AS improves\nperformance preservation over the state-of-the-art unlearning methods,\nachieving up to 15% higher accuracy on the ToFU benchmark and 10% on the TDEC\nbenchmark, while maintaining competitive hallucination-free unlearning\neffectiveness. Compared to existing methods, AS demonstrates a superior balance\nbetween unlearning effectiveness, generalization, and response reliability.", "AI": {"tldr": "\u63d0\u51faAttention-Shifting\u6846\u67b6\u89e3\u51b3LLM\u9009\u62e9\u6027\u9057\u5fd8\u4e2d\u7684\u4e24\u96be\u95ee\u9898\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u5e72\u9884\u5b9e\u73b0\u77e5\u8bc6\u9057\u5fd8\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u9057\u5fd8\u65b9\u6cd5\u9762\u4e34\u4e24\u96be\uff1a\u6fc0\u8fdb\u9057\u5fd8\u635f\u5bb3\u6a21\u578b\u6548\u7528\uff0c\u4fdd\u5b88\u7b56\u7565\u4fdd\u7559\u6548\u7528\u4f46\u4ea7\u751f\u5e7b\u89c9\u54cd\u5e94\uff0c\u9650\u5236\u4e86LLM\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u3002", "method": "\u91c7\u7528\u6ce8\u610f\u529b\u8f6c\u79fb\u6846\u67b6\uff0c\u5305\u542b\u4e0a\u4e0b\u6587\u4fdd\u6301\u6291\u5236\u548c\u6297\u5e7b\u89c9\u54cd\u5e94\u5851\u9020\u4e24\u4e2a\u7ec4\u4ef6\uff0c\u901a\u8fc7\u91cd\u8981\u6027\u611f\u77e5\u6291\u5236\u548c\u6ce8\u610f\u529b\u5f15\u5bfc\u4fdd\u7559\u589e\u5f3a\u5b9e\u73b0\u9009\u62e9\u6027\u9057\u5fd8\u3002", "result": "\u5728ToFU\u57fa\u51c6\u4e0a\u51c6\u786e\u7387\u63d0\u534715%\uff0cTDEC\u57fa\u51c6\u4e0a\u63d0\u534710%\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u6027\u7684\u65e0\u5e7b\u89c9\u9057\u5fd8\u6548\u679c\u3002", "conclusion": "AS\u65b9\u6cd5\u5728\u9057\u5fd8\u6548\u679c\u3001\u6cdb\u5316\u80fd\u529b\u548c\u54cd\u5e94\u53ef\u9760\u6027\u4e4b\u95f4\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u5e73\u8861\u3002"}}
{"id": "2510.16167", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16167", "abs": "https://arxiv.org/abs/2510.16167", "authors": ["Archie Chaudhury"], "title": "Alignment is Localized: A Causal Probe into Preference Layers", "comment": null, "summary": "Reinforcement Learning frameworks, particularly those utilizing human\nannotations, have become an increasingly popular method for preference\nfine-tuning, where the outputs of a language model are tuned to match a certain\nset of behavioral policies or guidelines. Reinforcement Learning through Human\nFeedback (RLHF) is perhaps the most popular implementation of such a framework,\nparticularly for aligning LMs toward safety and human intent. However, the\ninternal workings of how such alignment is achieved remain largely opaque. In\nthis work, we systematically analyze preference optimization for language model\nalignment by applying layer-wide causal patching between a base model and its\ntuned counterpart across human preference pairs. We implement our methodology\non \\textit{Llama-3.2-1B}, and find that alignment is spatially localized:\nmid-layer activations encode a distinct subspace that causally determines\nreward-consistent behavior, while early and late layers remain largely\nunaffected. Utilizing LASSO regression, we also find that only a small number\nof layers possess non-zero coefficients linking activation distances to reward\ngains. Overall, we show that, at least for some language models, alignment from\nhuman-based, preferential tuning is a directional, low rank process, rather\nthan diffuse and parameteric.", "AI": {"tldr": "\u901a\u8fc7\u5c42\u95f4\u56e0\u679c\u4fee\u8865\u5206\u6790\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u673a\u5236\uff0c\u53d1\u73b0\u5bf9\u9f50\u8fc7\u7a0b\u662f\u7a7a\u95f4\u5c40\u90e8\u5316\u7684\uff0c\u4e3b\u8981\u7531\u4e2d\u95f4\u5c42\u6fc0\u6d3b\u51b3\u5b9a\u5956\u52b1\u4e00\u81f4\u884c\u4e3a\uff0c\u800c\u975e\u6269\u6563\u7684\u53c2\u6570\u5316\u8fc7\u7a0b\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u4e8e\u4eba\u7c7b\u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLHF\uff09\u5df2\u6210\u4e3a\u8bed\u8a00\u6a21\u578b\u504f\u597d\u5fae\u8c03\u7684\u6d41\u884c\u65b9\u6cd5\uff0c\u4f46\u5176\u5185\u90e8\u5bf9\u9f50\u673a\u5236\u4ecd\u4e0d\u900f\u660e\uff0c\u9700\u8981\u7cfb\u7edf\u5206\u6790\u5bf9\u9f50\u5982\u4f55\u5b9e\u73b0\u3002", "method": "\u5728\u57fa\u7840\u6a21\u578b\u548c\u8c03\u4f18\u6a21\u578b\u4e4b\u95f4\u5e94\u7528\u5c42\u95f4\u56e0\u679c\u4fee\u8865\uff0c\u4f7f\u7528LASSO\u56de\u5f52\u5206\u6790\u6fc0\u6d3b\u8ddd\u79bb\u4e0e\u5956\u52b1\u589e\u76ca\u7684\u5173\u7cfb\u3002", "result": "\u5bf9\u9f50\u662f\u7a7a\u95f4\u5c40\u90e8\u5316\u7684\uff1a\u4e2d\u95f4\u5c42\u6fc0\u6d3b\u7f16\u7801\u4e86\u51b3\u5b9a\u5956\u52b1\u4e00\u81f4\u884c\u4e3a\u7684\u72ec\u7279\u5b50\u7a7a\u95f4\uff0c\u800c\u65e9\u671f\u548c\u665a\u671f\u5c42\u57fa\u672c\u4e0d\u53d7\u5f71\u54cd\uff1b\u53ea\u6709\u5c11\u6570\u5c42\u5177\u6709\u975e\u96f6\u7cfb\u6570\u8fde\u63a5\u6fc0\u6d3b\u8ddd\u79bb\u4e0e\u5956\u52b1\u589e\u76ca\u3002", "conclusion": "\u57fa\u4e8e\u4eba\u7c7b\u504f\u597d\u7684\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u662f\u4e00\u4e2a\u5b9a\u5411\u3001\u4f4e\u79e9\u7684\u8fc7\u7a0b\uff0c\u800c\u975e\u6269\u6563\u7684\u53c2\u6570\u5316\u8fc7\u7a0b\u3002"}}
{"id": "2510.17238", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17238", "abs": "https://arxiv.org/abs/2510.17238", "authors": ["Junlong Tong", "Yingqi Fan", "Anhao Zhao", "Yunpu Ma", "Xiaoyu Shen"], "title": "StreamingThinker: Large Language Models Can Think While Reading", "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nchain of thought (CoT) reasoning. However, the current LLM reasoning paradigm\ninitiates thinking only after the entire input is available, which introduces\nunnecessary latency and weakens attention to earlier information in dynamic\nscenarios. Inspired by human cognition of thinking while reading, we first\ndesign a \\textit{\\textbf{streaming thinking}} paradigm for LLMs, where\nreasoning unfolds in the order of input and further adjusts its depth once\nreading is complete. We instantiate this paradigm with\n\\textit{StreamingThinker}, a framework that enables LLMs to think while reading\nthrough the integration of streaming CoT generation, streaming-constraint\ntraining, and streaming parallel inference. Specifically, StreamingThinker\nemploys streaming reasoning units with quality control for CoT generation,\nenforces order-preserving reasoning through streaming attention masks and\nposition encoding, and leverages parallel KV caches that decouple input\nencoding from reasoning generation, thereby ensuring alignment and enabling\ntrue concurrency. We evaluate StreamingThinker on the Qwen3 model family across\nmath reasoning, logical reasoning, and context-based QA reasoning tasks.\nExperimental results show that the StreamingThinker preserves performance\ncomparable to batch thinking, while yielding an 80\\% reduction in token waiting\nbefore the onset of reasoning and a more than 60\\% reduction in time-level\nlatency for producing the final answer, demonstrating the effectiveness of the\nstreaming paradigm for LLM reasoning. Code will be released at\n\\href{https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker}{this\nrepository.}", "AI": {"tldr": "\u63d0\u51faStreamingThinker\u6846\u67b6\uff0c\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u5728\u9605\u8bfb\u8f93\u5165\u7684\u540c\u65f6\u8fdb\u884c\u63a8\u7406\uff0c\u800c\u4e0d\u662f\u7b49\u5f85\u5b8c\u6574\u8f93\u5165\u540e\u518d\u5f00\u59cb\u601d\u8003\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u63a8\u7406\u5ef6\u8fdf\u3002", "motivation": "\u5f53\u524dLLM\u63a8\u7406\u8303\u5f0f\u9700\u8981\u7b49\u5f85\u5b8c\u6574\u8f93\u5165\u540e\u624d\u5f00\u59cb\u601d\u8003\uff0c\u8fd9\u5e26\u6765\u4e86\u4e0d\u5fc5\u8981\u7684\u5ef6\u8fdf\uff0c\u5e76\u4e14\u5728\u52a8\u6001\u573a\u666f\u4e0b\u4f1a\u524a\u5f31\u5bf9\u65e9\u671f\u4fe1\u606f\u7684\u6ce8\u610f\u529b\u3002", "method": "\u8bbe\u8ba1\u6d41\u5f0f\u601d\u7ef4\u8303\u5f0f\uff0c\u901a\u8fc7\u6d41\u5f0fCoT\u751f\u6210\u3001\u6d41\u5f0f\u7ea6\u675f\u8bad\u7ec3\u548c\u6d41\u5f0f\u5e76\u884c\u63a8\u7406\u5b9e\u73b0\u8fb9\u8bfb\u8fb9\u60f3\uff0c\u4f7f\u7528\u6d41\u5f0f\u63a8\u7406\u5355\u5143\u3001\u6d41\u5f0f\u6ce8\u610f\u529b\u63a9\u7801\u548c\u4f4d\u7f6e\u7f16\u7801\uff0c\u4ee5\u53ca\u5e76\u884cKV\u7f13\u5b58\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u3001\u903b\u8f91\u63a8\u7406\u548c\u4e0a\u4e0b\u6587QA\u63a8\u7406\u4efb\u52a1\u4e0a\uff0cStreamingThinker\u4fdd\u6301\u4e86\u4e0e\u6279\u91cf\u601d\u8003\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5c06\u63a8\u7406\u5f00\u59cb\u524d\u7684token\u7b49\u5f85\u65f6\u95f4\u51cf\u5c11\u4e8680%\uff0c\u6700\u7ec8\u7b54\u6848\u751f\u6210\u7684\u65f6\u95f4\u5ef6\u8fdf\u51cf\u5c11\u4e8660%\u4ee5\u4e0a\u3002", "conclusion": "\u6d41\u5f0f\u601d\u7ef4\u8303\u5f0f\u80fd\u591f\u6709\u6548\u964d\u4f4eLLM\u63a8\u7406\u5ef6\u8fdf\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u8d28\u91cf\uff0c\u4e3a\u5b9e\u65f6\u63a8\u7406\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2510.17598", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17598", "abs": "https://arxiv.org/abs/2510.17598", "authors": ["Amir Jalilifard", "Anderson de Rezende Rocha", "Marcos Medeiros Raimundo"], "title": "Reasoning Distillation and Structural Alignment for Improved Code Generation", "comment": null, "summary": "Effective code generation with language models hinges on two critical\nfactors: accurately understanding the intent of the prompt and generating code\nthat applies algorithmic reasoning to produce correct solutions capable of\npassing diverse test cases while adhering to the syntax of the target\nprogramming language. Unlike other language tasks, code generation requires\nmore than accurate token prediction; it demands comprehension of solution-level\nand structural relationships rather than merely generating the most likely\ntokens. very large language model (VLLM) are capable of generating detailed\nsteps toward the correct solution of complex tasks where reasoning is crucial\nin solving the problem. Such reasoning capabilities may be absent in smaller\nlanguage models. Therefore, in this work, we distill the reasoning capabilities\nof a VLLM into a smaller, more efficient model that is faster and cheaper to\ndeploy. Our approach trains the model to emulate the reasoning and\nproblem-solving abilities of the VLLM by learning to identify correct solution\npathways and establishing a structural correspondence between problem\ndefinitions and potential solutions through a novel method of structure-aware\nloss optimization. This enables the model to transcend token-level generation\nand to deeply grasp the overarching structure of solutions for given problems.\nExperimental results show that our fine-tuned model, developed through a cheap\nand simple to implement process, significantly outperforms our baseline model\nin terms of pass@1, average data flow, and average syntax match metrics across\nthe MBPP, MBPP Plus, and HumanEval benchmarks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u84b8\u998f\u5230\u66f4\u5c0f\u3001\u66f4\u9ad8\u6548\u6a21\u578b\u4e2d\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u611f\u77e5\u635f\u5931\u4f18\u5316\u4f7f\u5c0f\u6a21\u578b\u80fd\u591f\u7406\u89e3\u95ee\u9898\u4e0e\u89e3\u51b3\u65b9\u6848\u4e4b\u95f4\u7684\u7ed3\u6784\u5bf9\u5e94\u5173\u7cfb\u3002", "motivation": "\u4ee3\u7801\u751f\u6210\u9700\u8981\u51c6\u786e\u7406\u89e3\u610f\u56fe\u5e76\u5e94\u7528\u7b97\u6cd5\u63a8\u7406\uff0c\u800c\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u5927\u578b\u6a21\u578b\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\u3002\u7814\u7a76\u65e8\u5728\u5c06\u5927\u578b\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u84b8\u998f\u5230\u66f4\u5c0f\u3001\u66f4\u9ad8\u6548\u7684\u6a21\u578b\u4e2d\u3002", "method": "\u901a\u8fc7\u8bad\u7ec3\u6a21\u578b\u6a21\u62df\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u548c\u95ee\u9898\u89e3\u51b3\u80fd\u529b\uff0c\u5b66\u4e60\u8bc6\u522b\u6b63\u786e\u89e3\u51b3\u65b9\u6848\u8def\u5f84\uff0c\u5e76\u901a\u8fc7\u7ed3\u6784\u611f\u77e5\u635f\u5931\u4f18\u5316\u5efa\u7acb\u95ee\u9898\u5b9a\u4e49\u4e0e\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u4e4b\u95f4\u7684\u7ed3\u6784\u5bf9\u5e94\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u7ecf\u8fc7\u5fae\u8c03\u7684\u6a21\u578b\u5728MBPP\u3001MBPP Plus\u548cHumanEval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5728pass@1\u3001\u5e73\u5747\u6570\u636e\u6d41\u548c\u5e73\u5747\u8bed\u6cd5\u5339\u914d\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u901a\u8fc7\u5ec9\u4ef7\u4e14\u6613\u4e8e\u5b9e\u73b0\u7684\u8fc7\u7a0b\u5f00\u53d1\u7684\u5fae\u8c03\u6a21\u578b\u80fd\u591f\u8d85\u8d8a\u4ee4\u724c\u7ea7\u751f\u6210\uff0c\u6df1\u5165\u7406\u89e3\u7ed9\u5b9a\u95ee\u9898\u7684\u89e3\u51b3\u65b9\u6848\u7ed3\u6784\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u4ee3\u7801\u751f\u6210\u80fd\u529b\u3002"}}
{"id": "2510.16171", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16171", "abs": "https://arxiv.org/abs/2510.16171", "authors": ["Longwei Wang", "Ifrat Ikhtear Uddin", "KC Santosh", "Chaowei Zhang", "Xiao Qin", "Yang Zhou"], "title": "Bridging Symmetry and Robustness: On the Role of Equivariance in Enhancing Adversarial Robustness", "comment": "Accepted for the proceedings of 39th Conference on Neural Information\n  Processing Systems (NeurIPS 2025)", "summary": "Adversarial examples reveal critical vulnerabilities in deep neural networks\nby exploiting their sensitivity to imperceptible input perturbations. While\nadversarial training remains the predominant defense strategy, it often incurs\nsignificant computational cost and may compromise clean-data accuracy. In this\nwork, we investigate an architectural approach to adversarial robustness by\nembedding group-equivariant convolutions-specifically, rotation- and\nscale-equivariant layers-into standard convolutional neural networks (CNNs).\nThese layers encode symmetry priors that align model behavior with structured\ntransformations in the input space, promoting smoother decision boundaries and\ngreater resilience to adversarial attacks. We propose and evaluate two\nsymmetry-aware architectures: a parallel design that processes standard and\nequivariant features independently before fusion, and a cascaded design that\napplies equivariant operations sequentially. Theoretically, we demonstrate that\nsuch models reduce hypothesis space complexity, regularize gradients, and yield\ntighter certified robustness bounds under the CLEVER (Cross Lipschitz Extreme\nValue for nEtwork Robustness) framework. Empirically, our models consistently\nimprove adversarial robustness and generalization across CIFAR-10, CIFAR-100,\nand CIFAR-10C under both FGSM and PGD attacks, without requiring adversarial\ntraining. These findings underscore the potential of symmetry-enforcing\narchitectures as efficient and principled alternatives to data\naugmentation-based defenses.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u5d4c\u5165\u7fa4\u7b49\u53d8\u5377\u79ef\u5c42\uff08\u65cb\u8f6c\u548c\u5c3a\u5ea6\u7b49\u53d8\uff09\u6765\u589e\u5f3a\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u5bf9\u6297\u9c81\u68d2\u6027\uff0c\u65e0\u9700\u5bf9\u6297\u8bad\u7ec3\u5373\u53ef\u63d0\u5347\u6a21\u578b\u5bf9\u5bf9\u6297\u653b\u51fb\u7684\u9632\u5fa1\u80fd\u529b\u3002", "motivation": "\u5bf9\u6297\u8bad\u7ec3\u4f5c\u4e3a\u4e3b\u8981\u9632\u5fa1\u7b56\u7565\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u53ef\u80fd\u964d\u4f4e\u5e72\u51c0\u6570\u636e\u51c6\u786e\u6027\u7684\u95ee\u9898\uff0c\u9700\u8981\u63a2\u7d22\u66f4\u9ad8\u6548\u7684\u67b6\u6784\u65b9\u6cd5\u6765\u589e\u5f3a\u5bf9\u6297\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u5bf9\u79f0\u611f\u77e5\u67b6\u6784\uff1a\u5e76\u884c\u8bbe\u8ba1\uff08\u72ec\u7acb\u5904\u7406\u6807\u51c6\u548c\u7b49\u53d8\u7279\u5f81\u540e\u878d\u5408\uff09\u548c\u7ea7\u8054\u8bbe\u8ba1\uff08\u987a\u5e8f\u5e94\u7528\u7b49\u53d8\u64cd\u4f5c\uff09\uff0c\u5d4c\u5165\u65cb\u8f6c\u548c\u5c3a\u5ea6\u7b49\u53d8\u5377\u79ef\u5c42\u3002", "result": "\u5728CIFAR-10\u3001CIFAR-100\u548cCIFAR-10C\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u5728FGSM\u548cPGD\u653b\u51fb\u4e0b\u4e00\u81f4\u63d0\u5347\u4e86\u5bf9\u6297\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u65e0\u9700\u5bf9\u6297\u8bad\u7ec3\u3002", "conclusion": "\u5bf9\u79f0\u5f3a\u5236\u67b6\u6784\u4f5c\u4e3a\u6570\u636e\u589e\u5f3a\u9632\u5fa1\u7684\u9ad8\u6548\u539f\u5219\u6027\u66ff\u4ee3\u65b9\u6848\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u80fd\u591f\u964d\u4f4e\u5047\u8bbe\u7a7a\u95f4\u590d\u6742\u5ea6\u3001\u6b63\u5219\u5316\u68af\u5ea6\u5e76\u83b7\u5f97\u66f4\u7d27\u7684\u8ba4\u8bc1\u9c81\u68d2\u6027\u8fb9\u754c\u3002"}}
{"id": "2510.17247", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17247", "abs": "https://arxiv.org/abs/2510.17247", "authors": ["Zefan Cai", "Haoyi Qiu", "Haozhe Zhao", "Ke Wan", "Jiachen Li", "Jiuxiang Gu", "Wen Xiao", "Nanyun Peng", "Junjie Hu"], "title": "From Preferences to Prejudice: The Role of Alignment Tuning in Shaping Social Bias in Video Diffusion Models", "comment": null, "summary": "Recent advances in video diffusion models have significantly enhanced\ntext-to-video generation, particularly through alignment tuning using reward\nmodels trained on human preferences. While these methods improve visual\nquality, they can unintentionally encode and amplify social biases. To\nsystematically trace how such biases evolve throughout the alignment pipeline,\nwe introduce VideoBiasEval, a comprehensive diagnostic framework for evaluating\nsocial representation in video generation. Grounded in established social bias\ntaxonomies, VideoBiasEval employs an event-based prompting strategy to\ndisentangle semantic content (actions and contexts) from actor attributes\n(gender and ethnicity). It further introduces multi-granular metrics to\nevaluate (1) overall ethnicity bias, (2) gender bias conditioned on ethnicity,\n(3) distributional shifts in social attributes across model variants, and (4)\nthe temporal persistence of bias within videos. Using this framework, we\nconduct the first end-to-end analysis connecting biases in human preference\ndatasets, their amplification in reward models, and their propagation through\nalignment-tuned video diffusion models. Our results reveal that alignment\ntuning not only strengthens representational biases but also makes them\ntemporally stable, producing smoother yet more stereotyped portrayals. These\nfindings highlight the need for bias-aware evaluation and mitigation throughout\nthe alignment process to ensure fair and socially responsible video generation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86VideoBiasEval\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u9891\u751f\u6210\u4e2d\u7684\u793e\u4f1a\u504f\u89c1\uff0c\u53d1\u73b0\u5bf9\u9f50\u8c03\u4f18\u4f1a\u653e\u5927\u5e76\u7a33\u5b9a\u5316\u793e\u4f1a\u504f\u89c1\u3002", "motivation": "\u89c6\u9891\u6269\u6563\u6a21\u578b\u901a\u8fc7\u57fa\u4e8e\u4eba\u7c7b\u504f\u597d\u7684\u5956\u52b1\u6a21\u578b\u8fdb\u884c\u5bf9\u9f50\u8c03\u4f18\uff0c\u867d\u7136\u63d0\u5347\u4e86\u89c6\u89c9\u8d28\u91cf\uff0c\u4f46\u53ef\u80fd\u65e0\u610f\u4e2d\u7f16\u7801\u548c\u653e\u5927\u4e86\u793e\u4f1a\u504f\u89c1\u3002", "method": "\u5f15\u5165VideoBiasEval\u8bca\u65ad\u6846\u67b6\uff0c\u57fa\u4e8e\u793e\u4f1a\u504f\u89c1\u5206\u7c7b\u5b66\uff0c\u91c7\u7528\u57fa\u4e8e\u4e8b\u4ef6\u7684\u63d0\u793a\u7b56\u7565\uff0c\u5206\u79bb\u8bed\u4e49\u5185\u5bb9\u548c\u6f14\u5458\u5c5e\u6027\uff0c\u5e76\u5f15\u5165\u591a\u7c92\u5ea6\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u53d1\u73b0\u5bf9\u9f50\u8c03\u4f18\u4e0d\u4ec5\u52a0\u5f3a\u4e86\u8868\u5f81\u504f\u89c1\uff0c\u8fd8\u4f7f\u5176\u5728\u65f6\u95f4\u4e0a\u66f4\u52a0\u7a33\u5b9a\uff0c\u4ea7\u751f\u66f4\u5e73\u6ed1\u4f46\u66f4\u523b\u677f\u7684\u63cf\u7ed8\u3002", "conclusion": "\u9700\u8981\u5728\u5bf9\u9f50\u8fc7\u7a0b\u4e2d\u8fdb\u884c\u504f\u89c1\u611f\u77e5\u7684\u8bc4\u4f30\u548c\u7f13\u89e3\uff0c\u4ee5\u786e\u4fdd\u516c\u5e73\u548c\u8d1f\u8d23\u4efb\u7684\u89c6\u9891\u751f\u6210\u3002"}}
{"id": "2510.17614", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.17614", "abs": "https://arxiv.org/abs/2510.17614", "authors": ["Praphul Singh", "Corey Barrett", "Sumana Srivasta", "Irfan Bulu", "Sri Gadde", "Krishnaram Kenthapadi"], "title": "OG-Rank: Learning to Rank Fast and Slow with Uncertainty and Reward-Trend Guided Adaptive Exploration", "comment": null, "summary": "Clinicians need ranking systems that work in real time and still justify\ntheir choices. Motivated by the need for a low-latency, decoder-based reranker,\nwe present OG-Rank, a single-decoder approach that pairs a pooled first-token\nscoring signal with an uncertainty-gated explanation step. The model scores all\ncandidates in one pass and generates a brief, structured rationale only when\nthe list is genuinely ambiguous, keeping latency predictable. Trained with a\ncurriculum that concentrates effort on hard cases, OG-Rank delivers strong\neffectiveness on encounter-scoped order selection (fast path: Recall@1~0.45,\nnDCG@20~0.625) and improves further when the gate activates (Recall@1~0.56,\nnDCG@20~0.699 at a 45\\% gate rate), while compact backbones show similar gains\nunder the same policy. Encoder baselines trail in both effectiveness and\nflexibility. The result is a practical recipe: rank fast by default and explain\nwhen it helps, a pattern that applies broadly to decision tasks where selective\ngeneration buys accuracy at acceptable cost. The single-policy design\nsimplifies deployment and budget planning, and the curriculum principle (spend\nmore on the hard cases, less on the easy ones) readily transfers beyond\nclinical order selection.", "AI": {"tldr": "OG-Rank\u662f\u4e00\u4e2a\u4f4e\u5ef6\u8fdf\u7684\u89e3\u7801\u5668\u91cd\u6392\u5e8f\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u5408\u6c60\u5316\u9996\u8bcd\u8bc4\u5206\u548c\u4e0d\u786e\u5b9a\u6027\u95e8\u63a7\u89e3\u91ca\u6b65\u9aa4\uff0c\u5b9e\u73b0\u5feb\u901f\u6392\u5e8f\u5e76\u5728\u771f\u6b63\u6a21\u7cca\u65f6\u751f\u6210\u89e3\u91ca\uff0c\u4fdd\u6301\u53ef\u9884\u6d4b\u7684\u5ef6\u8fdf\u3002", "motivation": "\u4e34\u5e8a\u533b\u751f\u9700\u8981\u5b9e\u65f6\u5de5\u4f5c\u5e76\u80fd\u89e3\u91ca\u9009\u62e9\u7684\u6392\u5e8f\u7cfb\u7edf\uff0c\u9700\u8981\u4f4e\u5ef6\u8fdf\u3001\u57fa\u4e8e\u89e3\u7801\u5668\u7684\u91cd\u6392\u5e8f\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5355\u89e3\u7801\u5668\u65b9\u6cd5\uff0c\u7ed3\u5408\u6c60\u5316\u9996\u8bcd\u8bc4\u5206\u4fe1\u53f7\u548c\u4e0d\u786e\u5b9a\u6027\u95e8\u63a7\u89e3\u91ca\u6b65\u9aa4\uff0c\u901a\u8fc7\u4e13\u6ce8\u4e8e\u56f0\u96be\u6848\u4f8b\u7684\u8bfe\u7a0b\u8bad\u7ec3\u3002", "result": "\u5728\u906d\u9047\u8303\u56f4\u8ba2\u5355\u9009\u62e9\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff08\u5feb\u901f\u8def\u5f84\uff1aRecall@1~0.45\uff0cnDCG@20~0.625\uff09\uff0c\u95e8\u63a7\u6fc0\u6d3b\u65f6\u8fdb\u4e00\u6b65\u63d0\u5347\uff08Recall@1~0.56\uff0cnDCG@20~0.699\uff0c\u95e8\u63a7\u738745%\uff09\u3002", "conclusion": "OG-Rank\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u65b9\u6848\uff1a\u9ed8\u8ba4\u5feb\u901f\u6392\u5e8f\uff0c\u5728\u9700\u8981\u65f6\u89e3\u91ca\uff0c\u8fd9\u79cd\u6a21\u5f0f\u9002\u7528\u4e8e\u9009\u62e9\u6027\u751f\u6210\u80fd\u5728\u53ef\u63a5\u53d7\u6210\u672c\u4e0b\u63d0\u9ad8\u51c6\u786e\u6027\u7684\u51b3\u7b56\u4efb\u52a1\u3002"}}
{"id": "2510.16175", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16175", "abs": "https://arxiv.org/abs/2510.16175", "authors": ["Pablo Samuel Castro"], "title": "The Formalism-Implementation Gap in Reinforcement Learning Research", "comment": null, "summary": "The last decade has seen an upswing in interest and adoption of reinforcement\nlearning (RL) techniques, in large part due to its demonstrated capabilities at\nperforming certain tasks at \"super-human levels\". This has incentivized the\ncommunity to prioritize research that demonstrates RL agent performance, often\nat the expense of research aimed at understanding their learning dynamics.\nPerformance-focused research runs the risk of overfitting on academic\nbenchmarks -- thereby rendering them less useful -- which can make it difficult\nto transfer proposed techniques to novel problems. Further, it implicitly\ndiminishes work that does not push the performance-frontier, but aims at\nimproving our understanding of these techniques. This paper argues two points:\n(i) RL research should stop focusing solely on demonstrating agent\ncapabilities, and focus more on advancing the science and understanding of\nreinforcement learning; and (ii) we need to be more precise on how our\nbenchmarks map to the underlying mathematical formalisms. We use the popular\nArcade Learning Environment (ALE; Bellemare et al., 2013) as an example of a\nbenchmark that, despite being increasingly considered \"saturated\", can be\neffectively used for developing this understanding, and facilitating the\ndeployment of RL techniques in impactful real-world problems.", "AI": {"tldr": "\u672c\u6587\u4e3b\u5f20\u5f3a\u5316\u5b66\u4e60\u7814\u7a76\u5e94\u505c\u6b62\u4ec5\u5173\u6ce8\u667a\u80fd\u4f53\u6027\u80fd\u5c55\u793a\uff0c\u800c\u5e94\u66f4\u5173\u6ce8\u7406\u89e3\u5b66\u4e60\u52a8\u6001\uff0c\u5e76\u9700\u8981\u66f4\u7cbe\u786e\u5730\u5b9a\u4e49\u57fa\u51c6\u6d4b\u8bd5\u4e0e\u6570\u5b66\u5f62\u5f0f\u5316\u4e4b\u95f4\u7684\u6620\u5c04\u5173\u7cfb\u3002", "motivation": "\u5f53\u524d\u5f3a\u5316\u5b66\u4e60\u7814\u7a76\u8fc7\u5ea6\u5173\u6ce8\u6027\u80fd\u5c55\u793a\u800c\u5ffd\u89c6\u4e86\u5bf9\u5b66\u4e60\u52a8\u6001\u7684\u7406\u89e3\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u5728\u5b66\u672f\u57fa\u51c6\u4e0a\u7684\u8fc7\u62df\u5408\uff0c\u5e76\u4f7f\u6280\u672f\u96be\u4ee5\u8fc1\u79fb\u5230\u65b0\u95ee\u9898\u3002", "method": "\u4ee5Arcade Learning Environment (ALE)\u4e3a\u4f8b\uff0c\u8bf4\u660e\u5373\u4f7f\u88ab\u8ba4\u4e3a\u662f\"\u9971\u548c\"\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ecd\u53ef\u6709\u6548\u7528\u4e8e\u53d1\u5c55\u5bf9\u5f3a\u5316\u5b66\u4e60\u7684\u7406\u89e3\u3002", "result": "\u8bba\u8bc1\u4e86\u91cd\u65b0\u805a\u7126\u7814\u7a76\u91cd\u70b9\u7684\u5fc5\u8981\u6027\uff0c\u63d0\u51fa\u4e86\u66f4\u7cbe\u786e\u7684\u57fa\u51c6\u6d4b\u8bd5\u6620\u5c04\u65b9\u6cd5\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u7814\u7a76\u9700\u8981\u4ece\u5355\u7eaf\u6027\u80fd\u5c55\u793a\u8f6c\u5411\u79d1\u5b66\u7406\u89e3\uff0c\u5e76\u6539\u8fdb\u57fa\u51c6\u6d4b\u8bd5\u7684\u8bbe\u8ba1\u548c\u4f7f\u7528\u65b9\u5f0f\u3002"}}
{"id": "2510.17252", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17252", "abs": "https://arxiv.org/abs/2510.17252", "authors": ["Mohd Ruhul Ameen", "Akif Islam", "Abu Saleh Musa Miah", "Ayesha Siddiqua", "Jungpil Shin"], "title": "How News Feels: Understanding Affective Bias in Multilingual Headlines for Human-Centered Media Design", "comment": "15 pages, 7 figures, 4 tables. Submitted to the International\n  Conference on Data and Applied Analytics (IDAA 2025)", "summary": "News media often shape the public mood not only by what they report but by\nhow they frame it. The same event can appear calm in one outlet and alarming in\nanother, reflecting subtle emotional bias in reporting. Negative or emotionally\ncharged headlines tend to attract more attention and spread faster, which in\nturn encourages outlets to frame stories in ways that provoke stronger\nreactions. This research explores that tendency through large-scale emotion\nanalysis of Bengali news. Using zero-shot inference with Gemma-3 4B, we\nanalyzed 300000 Bengali news headlines and their content to identify the\ndominant emotion and overall tone of each. The findings reveal a clear\ndominance of negative emotions, particularly anger, fear, and disappointment,\nand significant variation in how similar stories are emotionally portrayed\nacross outlets. Based on these insights, we propose design ideas for a\nhuman-centered news aggregator that visualizes emotional cues and helps readers\nrecognize hidden affective framing in daily news.", "AI": {"tldr": "\u901a\u8fc7\u5927\u89c4\u6a21\u60c5\u611f\u5206\u6790\u53d1\u73b0\u5b5f\u52a0\u62c9\u8bed\u65b0\u95fb\u6807\u9898\u666e\u904d\u5b58\u5728\u8d1f\u9762\u60c5\u7eea\u503e\u5411\uff0c\u7279\u522b\u662f\u6124\u6012\u3001\u6050\u60e7\u548c\u5931\u671b\u60c5\u7eea\u5360\u4e3b\u5bfc\u5730\u4f4d\uff0c\u4e0d\u540c\u5a92\u4f53\u5bf9\u76f8\u4f3c\u65b0\u95fb\u7684\u60c5\u611f\u6846\u67b6\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "motivation": "\u7814\u7a76\u65b0\u95fb\u5a92\u4f53\u5982\u4f55\u901a\u8fc7\u60c5\u611f\u6846\u67b6\u5f71\u54cd\u516c\u4f17\u60c5\u7eea\uff0c\u63a2\u7d22\u8d1f\u9762\u6216\u60c5\u7eea\u5316\u6807\u9898\u5438\u5f15\u66f4\u591a\u5173\u6ce8\u548c\u4f20\u64ad\u66f4\u5feb\u7684\u73b0\u8c61\u3002", "method": "\u4f7f\u7528Gemma-3 4B\u6a21\u578b\u5bf930\u4e07\u6761\u5b5f\u52a0\u62c9\u8bed\u65b0\u95fb\u6807\u9898\u548c\u5185\u5bb9\u8fdb\u884c\u96f6\u6837\u672c\u63a8\u7406\uff0c\u8bc6\u522b\u6bcf\u7bc7\u6587\u7ae0\u7684\u4e3b\u5bfc\u60c5\u7eea\u548c\u6574\u4f53\u57fa\u8c03\u3002", "result": "\u53d1\u73b0\u8d1f\u9762\u60c5\u7eea\u660e\u663e\u5360\u4e3b\u5bfc\u5730\u4f4d\uff0c\u7279\u522b\u662f\u6124\u6012\u3001\u6050\u60e7\u548c\u5931\u671b\u60c5\u7eea\uff0c\u4e0d\u540c\u5a92\u4f53\u5bf9\u76f8\u4f3c\u65b0\u95fb\u7684\u60c5\u611f\u5448\u73b0\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u57fa\u4e8e\u7814\u7a76\u7ed3\u679c\u63d0\u51fa\u4ee5\u4eba\u4e3a\u672c\u7684\u65b0\u95fb\u805a\u5408\u5668\u8bbe\u8ba1\u7406\u5ff5\uff0c\u901a\u8fc7\u53ef\u89c6\u5316\u60c5\u611f\u7ebf\u7d22\u5e2e\u52a9\u8bfb\u8005\u8bc6\u522b\u65e5\u5e38\u65b0\u95fb\u4e2d\u9690\u85cf\u7684\u60c5\u611f\u6846\u67b6\u3002"}}
{"id": "2510.17638", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17638", "abs": "https://arxiv.org/abs/2510.17638", "authors": ["Qingchuan Yang", "Simon Mahns", "Sida Li", "Anri Gu", "Jibang Wu", "Haifeng Xu"], "title": "LLM-as-a-Prophet: Understanding Predictive Intelligence with Prophet Arena", "comment": "https://www.prophetarena.co/", "summary": "Forecasting is not only a fundamental intellectual pursuit but also is of\nsignificant importance to societal systems such as finance and economics. With\nthe rapid advances of large language models (LLMs) trained on Internet-scale\ndata, it raises the promise of employing LLMs to forecast real-world future\nevents, an emerging paradigm we call \"LLM-as-a-Prophet\". This paper\nsystematically investigates such predictive intelligence of LLMs. To this end,\nwe build Prophet Arena, a general evaluation benchmark that continuously\ncollects live forecasting tasks and decomposes each task into distinct pipeline\nstages, in order to support our controlled and large-scale experimentation. Our\ncomprehensive evaluation reveals that many LLMs already exhibit impressive\nforecasting capabilities, reflected in, e.g., their small calibration errors,\nconsistent prediction confidence and promising market returns. However, we also\nuncover key bottlenecks towards achieving superior predictive intelligence via\nLLM-as-a-Prophet, such as LLMs' inaccurate event recalls, misunderstanding of\ndata sources and slower information aggregation compared to markets when\nresolution nears.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u9884\u6d4b\u5de5\u5177\u7684\u80fd\u529b\uff0c\u53d1\u73b0LLMs\u5df2\u5177\u5907\u663e\u8457\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u4f46\u4e5f\u5b58\u5728\u4e8b\u4ef6\u56de\u5fc6\u4e0d\u51c6\u786e\u3001\u6570\u636e\u6e90\u8bef\u89e3\u7b49\u74f6\u9888\u3002", "motivation": "\u968f\u7740\u57fa\u4e8e\u4e92\u8054\u7f51\u89c4\u6a21\u6570\u636e\u8bad\u7ec3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u63a2\u7d22\u5229\u7528LLMs\u9884\u6d4b\u73b0\u5b9e\u4e16\u754c\u672a\u6765\u4e8b\u4ef6\u7684\u6f5c\u529b\uff0c\u8fd9\u4e00\u65b0\u5174\u8303\u5f0f\u88ab\u79f0\u4e3a\"LLM-as-a-Prophet\"\u3002", "method": "\u6784\u5efa\u4e86Prophet Arena\u8bc4\u4f30\u57fa\u51c6\uff0c\u6301\u7eed\u6536\u96c6\u5b9e\u65f6\u9884\u6d4b\u4efb\u52a1\uff0c\u5e76\u5c06\u6bcf\u4e2a\u4efb\u52a1\u5206\u89e3\u4e3a\u4e0d\u540c\u7684\u6d41\u6c34\u7ebf\u9636\u6bb5\uff0c\u4ee5\u652f\u6301\u53d7\u63a7\u548c\u5927\u89c4\u6a21\u5b9e\u9a8c\u3002", "result": "\u8bb8\u591aLLMs\u5df2\u5c55\u73b0\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u8868\u73b0\u4e3a\u8f83\u5c0f\u7684\u6821\u51c6\u8bef\u5dee\u3001\u4e00\u81f4\u7684\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u548c\u6709\u524d\u666f\u7684\u5e02\u573a\u56de\u62a5\u3002\u4f46\u4e5f\u53d1\u73b0\u4e86\u5173\u952e\u74f6\u9888\u3002", "conclusion": "LLMs\u4f5c\u4e3a\u9884\u6d4b\u5de5\u5177\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u89e3\u51b3\u4e8b\u4ef6\u56de\u5fc6\u4e0d\u51c6\u786e\u3001\u6570\u636e\u6e90\u8bef\u89e3\u4ee5\u53ca\u5728\u63a5\u8fd1\u51b3\u7b56\u65f6\u4fe1\u606f\u805a\u5408\u901f\u5ea6\u8f83\u6162\u7b49\u95ee\u9898\u3002"}}
{"id": "2510.17256", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17256", "abs": "https://arxiv.org/abs/2510.17256", "authors": ["Shahin Atakishiyev", "Housam K. B. Babiker", "Jiayi Dai", "Nawshad Farruque", "Teruaki Hayashi", "Nafisa Sadaf Hriti", "Md Abed Rahman", "Iain Smith", "Mi-Young Kim", "Osmar R. Za\u00efane", "Randy Goebel"], "title": "Explainability of Large Language Models: Opportunities and Challenges toward Generating Trustworthy Explanations", "comment": null, "summary": "Large language models have exhibited impressive performance across a broad\nrange of downstream tasks in natural language processing. However, how a\nlanguage model predicts the next token and generates content is not generally\nunderstandable by humans. Furthermore, these models often make errors in\nprediction and reasoning, known as hallucinations. These errors underscore the\nurgent need to better understand and interpret the intricate inner workings of\nlanguage models and how they generate predictive outputs. Motivated by this\ngap, this paper investigates local explainability and mechanistic\ninterpretability within Transformer-based large language models to foster trust\nin such models. In this regard, our paper aims to make three key contributions.\nFirst, we present a review of local explainability and mechanistic\ninterpretability approaches and insights from relevant studies in the\nliterature. Furthermore, we describe experimental studies on explainability and\nreasoning with large language models in two critical domains -- healthcare and\nautonomous driving -- and analyze the trust implications of such explanations\nfor explanation receivers. Finally, we summarize current unaddressed issues in\nthe evolving landscape of LLM explainability and outline the opportunities,\ncritical challenges, and future directions toward generating human-aligned,\ntrustworthy LLM explanations.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8eTransformer\u7684\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5c40\u90e8\u53ef\u89e3\u91ca\u6027\u548c\u673a\u5236\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff0c\u5728\u533b\u7597\u548c\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u8fdb\u884c\u4e86\u5b9e\u9a8c\u7814\u7a76\uff0c\u5e76\u603b\u7ed3\u4e86\u5f53\u524d\u672a\u89e3\u51b3\u7684\u95ee\u9898\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u867d\u7136\u6027\u80fd\u4f18\u5f02\uff0c\u4f46\u5176\u9884\u6d4b\u673a\u5236\u5bf9\u4eba\u7c7b\u4e0d\u53ef\u7406\u89e3\uff0c\u4e14\u7ecf\u5e38\u51fa\u73b0\u5e7b\u89c9\u9519\u8bef\uff0c\u8feb\u5207\u9700\u8981\u7406\u89e3\u548c\u89e3\u91ca\u6a21\u578b\u5185\u90e8\u5de5\u4f5c\u673a\u5236\u4ee5\u5efa\u7acb\u4fe1\u4efb\u3002", "method": "\u91c7\u7528\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\u68b3\u7406\u5c40\u90e8\u53ef\u89e3\u91ca\u6027\u548c\u673a\u5236\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff0c\u5e76\u5728\u533b\u7597\u548c\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u8fdb\u884c\u5b9e\u9a8c\u7814\u7a76\uff0c\u5206\u6790\u89e3\u91ca\u5bf9\u63a5\u6536\u8005\u7684\u4fe1\u4efb\u5f71\u54cd\u3002", "result": "\u7cfb\u7edf\u68b3\u7406\u4e86\u73b0\u6709\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u89e3\u91ca\u6027\u5728\u5173\u952e\u9886\u57df\u7684\u91cd\u8981\u6027\uff0c\u8bc6\u522b\u4e86\u5f53\u524d\u672a\u89e3\u51b3\u7684\u95ee\u9898\u3002", "conclusion": "\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u751f\u6210\u4e0e\u4eba\u7c7b\u5bf9\u9f50\u3001\u53ef\u4fe1\u8d56\u7684LLM\u89e3\u91ca\uff0c\u9762\u4e34\u5173\u952e\u6311\u6218\u4f46\u5b58\u5728\u91cd\u8981\u673a\u9047\u3002"}}
{"id": "2510.17697", "categories": ["cs.AI", "cs.LG", "cs.MA", "I.2.11; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.17697", "abs": "https://arxiv.org/abs/2510.17697", "authors": ["Anjie Liu", "Jianhong Wang", "Samuel Kaski", "Jun Wang", "Mengyue Yang"], "title": "A Principle of Targeted Intervention for Multi-Agent Reinforcement Learning", "comment": "Accepted to NeurIPS 2025", "summary": "Steering cooperative multi-agent reinforcement learning (MARL) towards\ndesired outcomes is challenging, particularly when the global guidance from a\nhuman on the whole multi-agent system is impractical in a large-scale MARL. On\nthe other hand, designing mechanisms to coordinate agents most relies on\nempirical studies, lacking a easy-to-use research tool. In this work, we employ\nmulti-agent influence diagrams (MAIDs) as a graphical framework to address the\nabove issues. First, we introduce interaction paradigms that leverage MAIDs to\nanalyze and visualize existing approaches in MARL. Then, we design a new\ninteraction paradigm based on MAIDs, referred to as targeted intervention that\nis applied to only a single targeted agent, so the problem of global guidance\ncan be mitigated. In our implementation, we introduce a causal inference\ntechnique-referred to as Pre-Strategy Intervention (PSI)-to realize the\ntargeted intervention paradigm. Since MAIDs can be regarded as a special class\nof causal diagrams, a composite desired outcome that integrates the primary\ntask goal and an additional desired outcome can be achieved by maximizing the\ncorresponding causal effect through the PSI. Moreover, the bundled relevance\ngraph analysis of MAIDs provides a tool to identify whether an MARL learning\nparadigm is workable under the design of an interaction paradigm. In\nexperiments, we demonstrate the effectiveness of our proposed targeted\nintervention, and verify the result of relevance graph analysis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u591a\u667a\u80fd\u4f53\u5f71\u54cd\u56fe(MAIDs)\u4f5c\u4e3a\u56fe\u5f62\u5316\u6846\u67b6\u6765\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u534f\u8c03\u95ee\u9898\uff0c\u8bbe\u8ba1\u4e86\u57fa\u4e8eMAIDs\u7684\u9488\u5bf9\u6027\u5e72\u9884\u8303\u5f0f\uff0c\u5e76\u901a\u8fc7\u56e0\u679c\u63a8\u7406\u6280\u672f\u5b9e\u73b0\u5355\u667a\u80fd\u4f53\u5e72\u9884\u4ee5\u7f13\u89e3\u5168\u5c40\u6307\u5bfc\u7684\u56f0\u96be\u3002", "motivation": "\u5728\u5927\u89c4\u6a21\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u5bf9\u6574\u4e2a\u7cfb\u7edf\u8fdb\u884c\u5168\u5c40\u4eba\u5de5\u6307\u5bfc\u4e0d\u5207\u5b9e\u9645\uff0c\u800c\u73b0\u6709\u7684\u534f\u8c03\u673a\u5236\u8bbe\u8ba1\u4e3b\u8981\u4f9d\u8d56\u7ecf\u9a8c\u7814\u7a76\uff0c\u7f3a\u4e4f\u6613\u7528\u7684\u7814\u7a76\u5de5\u5177\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u5f71\u54cd\u56fe(MAIDs)\u4f5c\u4e3a\u5206\u6790\u6846\u67b6\uff0c\u8bbe\u8ba1\u4e86\u9488\u5bf9\u6027\u5e72\u9884\u8303\u5f0f\uff0c\u5e76\u5f15\u5165\u9884\u7b56\u7565\u5e72\u9884(PSI)\u56e0\u679c\u63a8\u7406\u6280\u672f\u6765\u5b9e\u73b0\u8be5\u8303\u5f0f\u3002\u901a\u8fc7\u6700\u5927\u5316\u76f8\u5e94\u56e0\u679c\u6548\u5e94\u6765\u8fbe\u6210\u590d\u5408\u671f\u671b\u7ed3\u679c\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u9488\u5bf9\u6027\u5e72\u9884\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u9a8c\u8bc1\u4e86\u76f8\u5173\u6027\u56fe\u5206\u6790\u7684\u7ed3\u679c\u3002", "conclusion": "MAIDs\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u56fe\u5f62\u5316\u6846\u67b6\u6765\u5206\u6790\u548c\u8bbe\u8ba1\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u8303\u5f0f\uff0c\u9488\u5bf9\u6027\u5e72\u9884\u80fd\u591f\u7f13\u89e3\u5168\u5c40\u6307\u5bfc\u7684\u56f0\u96be\uff0c\u4e3a\u591a\u667a\u80fd\u4f53\u534f\u8c03\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u5de5\u5177\u3002"}}
{"id": "2510.16188", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16188", "abs": "https://arxiv.org/abs/2510.16188", "authors": ["Fateme Golivand Darvishvand", "Hikaru Shindo", "Sahil Sidheekh", "Kristian Kersting", "Sriraam Natarajan"], "title": "Human-Allied Relational Reinforcement Learning", "comment": "Proceedings of the Twelfth Annual Conference on Advances in Cognitive\n  Systems, ACS-2025 (143-159)", "summary": "Reinforcement learning (RL) has experienced a second wind in the past decade.\nWhile incredibly successful in images and videos, these systems still operate\nwithin the realm of propositional tasks ignoring the inherent structure that\nexists in the problem. Consequently, relational extensions (RRL) have been\ndeveloped for such structured problems that allow for effective generalization\nto arbitrary number of objects. However, they inherently make strong\nassumptions about the problem structure. We introduce a novel framework that\ncombines RRL with object-centric representation to handle both structured and\nunstructured data. We enhance learning by allowing the system to actively query\nthe human expert for guidance by explicitly modeling the uncertainty over the\npolicy. Our empirical evaluation demonstrates the effectiveness and efficiency\nof our proposed approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5173\u7cfb\u5f3a\u5316\u5b66\u4e60\u4e0e\u7269\u4f53\u4e2d\u5fc3\u8868\u793a\u7684\u65b0\u6846\u67b6\uff0c\u80fd\u591f\u5904\u7406\u7ed3\u6784\u5316\u548c\u975e\u7ed3\u6784\u5316\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u4e3b\u52a8\u67e5\u8be2\u4eba\u7c7b\u4e13\u5bb6\u6765\u589e\u5f3a\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u5728\u5904\u7406\u7ed3\u6784\u5316\u95ee\u9898\u65f6\u5ffd\u7565\u4e86\u95ee\u9898\u7684\u5185\u5728\u7ed3\u6784\uff0c\u800c\u5173\u7cfb\u5f3a\u5316\u5b66\u4e60\u867d\u7136\u80fd\u5904\u7406\u7ed3\u6784\u5316\u95ee\u9898\u4f46\u5bf9\u95ee\u9898\u7ed3\u6784\u6709\u5f3a\u5047\u8bbe\u9650\u5236\u3002\u9700\u8981\u4e00\u79cd\u80fd\u540c\u65f6\u5904\u7406\u7ed3\u6784\u5316\u548c\u975e\u7ed3\u6784\u5316\u6570\u636e\u7684\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u5173\u7cfb\u5f3a\u5316\u5b66\u4e60\u4e0e\u7269\u4f53\u4e2d\u5fc3\u8868\u793a\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u7b56\u7565\u4e0d\u786e\u5b9a\u6027\uff0c\u5141\u8bb8\u7cfb\u7edf\u4e3b\u52a8\u5411\u4eba\u7c7b\u4e13\u5bb6\u67e5\u8be2\u6307\u5bfc\u3002", "result": "\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\u6240\u63d0\u65b9\u6cd5\u5177\u6709\u6709\u6548\u6027\u548c\u9ad8\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u5173\u7cfb\u5f3a\u5316\u5b66\u4e60\u5bf9\u95ee\u9898\u7ed3\u6784\u7684\u5f3a\u5047\u8bbe\u9650\u5236\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u6df7\u5408\u7c7b\u578b\u6570\u636e\u5e76\u63d0\u5347\u5b66\u4e60\u6548\u7387\u3002"}}
{"id": "2510.17263", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17263", "abs": "https://arxiv.org/abs/2510.17263", "authors": ["Avishek Lahiri", "Yufang Hou", "Debarshi Kumar Sanyal"], "title": "TaxoAlign: Scholarly Taxonomy Generation Using Language Models", "comment": "This paper has been accepted at the EMNLP 2025 Main Conference", "summary": "Taxonomies play a crucial role in helping researchers structure and navigate\nknowledge in a hierarchical manner. They also form an important part in the\ncreation of comprehensive literature surveys. The existing approaches to\nautomatic survey generation do not compare the structure of the generated\nsurveys with those written by human experts. To address this gap, we present\nour own method for automated taxonomy creation that can bridge the gap between\nhuman-generated and automatically-created taxonomies. For this purpose, we\ncreate the CS-TaxoBench benchmark which consists of 460 taxonomies that have\nbeen extracted from human-written survey papers. We also include an additional\ntest set of 80 taxonomies curated from conference survey papers. We propose\nTaxoAlign, a three-phase topic-based instruction-guided method for scholarly\ntaxonomy generation. Additionally, we propose a stringent automated evaluation\nframework that measures the structural alignment and semantic coherence of\nautomatically generated taxonomies in comparison to those created by human\nexperts. We evaluate our method and various baselines on CS-TaxoBench, using\nboth automated evaluation metrics and human evaluation studies. The results\nshow that TaxoAlign consistently surpasses the baselines on nearly all metrics.\nThe code and data can be found at https://github.com/AvishekLahiri/TaxoAlign.", "AI": {"tldr": "\u63d0\u51fa\u4e86TaxoAlign\u65b9\u6cd5\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u5b66\u672f\u5206\u7c7b\u6cd5\uff0c\u521b\u5efa\u4e86CS-TaxoBench\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u5728\u7ed3\u6784\u5bf9\u9f50\u548c\u8bed\u4e49\u8fde\u8d2f\u6027\u65b9\u9762\u8d85\u8d8a\u4e86\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u6587\u732e\u7efc\u8ff0\u751f\u6210\u65b9\u6cd5\u7f3a\u4e4f\u4e0e\u4eba\u5de5\u4e13\u5bb6\u751f\u6210\u5206\u7c7b\u6cd5\u7684\u7ed3\u6784\u6bd4\u8f83\uff0c\u9700\u8981\u5f25\u5408\u4eba\u5de5\u751f\u6210\u4e0e\u81ea\u52a8\u521b\u5efa\u5206\u7c7b\u6cd5\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u63d0\u51faTaxoAlign\u65b9\u6cd5\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u4e3b\u9898\u7684\u4e09\u9636\u6bb5\u6307\u4ee4\u5f15\u5bfc\u7684\u5b66\u672f\u5206\u7c7b\u6cd5\u751f\u6210\u65b9\u6cd5\uff0c\u5e76\u5efa\u7acb\u4e86\u4e25\u683c\u7684\u81ea\u52a8\u8bc4\u4f30\u6846\u67b6\u6765\u8861\u91cf\u7ed3\u6784\u5bf9\u9f50\u548c\u8bed\u4e49\u8fde\u8d2f\u6027\u3002", "result": "TaxoAlign\u5728CS-TaxoBench\u57fa\u51c6\u4e0a\u51e0\u4e4e\u5728\u6240\u6709\u6307\u6807\u4e0a\u90fd\u6301\u7eed\u8d85\u8d8a\u4e86\u57fa\u7ebf\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "TaxoAlign\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u751f\u6210\u4e0e\u4eba\u5de5\u4e13\u5bb6\u5206\u7c7b\u6cd5\u7ed3\u6784\u5bf9\u9f50\u4e14\u8bed\u4e49\u8fde\u8d2f\u7684\u5b66\u672f\u5206\u7c7b\u6cd5\uff0c\u4e3a\u81ea\u52a8\u6587\u732e\u7efc\u8ff0\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2510.17705", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17705", "abs": "https://arxiv.org/abs/2510.17705", "authors": ["Dayan Pan", "Zhaoyang Fu", "Jingyuan Wang", "Xiao Han", "Yue Zhu", "Xiangyu Zhao"], "title": "Contextual Attention Modulation: Towards Efficient Multi-Task Adaptation in Large Language Models", "comment": "Accepted by CIKM' 25", "summary": "Large Language Models (LLMs) possess remarkable generalization capabilities\nbut struggle with multi-task adaptation, particularly in balancing knowledge\nretention with task-specific specialization. Conventional fine-tuning methods\nsuffer from catastrophic forgetting and substantial resource consumption, while\nexisting parameter-efficient methods perform suboptimally in complex multi-task\nscenarios. To address this, we propose Contextual Attention Modulation (CAM), a\nnovel mechanism that dynamically modulates the representations of\nself-attention modules in LLMs. CAM enhances task-specific features while\npreserving general knowledge, thereby facilitating more effective and efficient\nadaptation. For effective multi-task adaptation, CAM is integrated into our\nHybrid Contextual Attention Modulation (HyCAM) framework, which combines a\nshared, full-parameter CAM module with multiple specialized, lightweight CAM\nmodules, enhanced by a dynamic routing strategy for adaptive knowledge fusion.\nExtensive experiments on heterogeneous tasks, including question answering,\ncode generation, and logical reasoning, demonstrate that our approach\nsignificantly outperforms existing approaches, achieving an average performance\nimprovement of 3.65%. The implemented code and data are available to ease\nreproducibility at https://github.com/Applied-Machine-Learning-Lab/HyCAM.", "AI": {"tldr": "\u63d0\u51faContextual Attention Modulation (CAM)\u673a\u5236\u548cHyCAM\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u5236\u81ea\u6ce8\u610f\u529b\u8868\u793a\u6765\u589e\u5f3a\u4efb\u52a1\u7279\u5b9a\u7279\u5f81\u540c\u65f6\u4fdd\u7559\u901a\u7528\u77e5\u8bc6\uff0c\u5728\u591a\u4efb\u52a1\u9002\u5e94\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u4efb\u52a1\u9002\u5e94\u4e2d\u5e73\u8861\u77e5\u8bc6\u4fdd\u7559\u4e0e\u4efb\u52a1\u7279\u5b9a\u4e13\u4e1a\u5316\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\u5b58\u5728\u707e\u96be\u6027\u9057\u5fd8\u548c\u8d44\u6e90\u6d88\u8017\u5927\u7684\u7f3a\u9677\u3002", "method": "\u63d0\u51faCAM\u673a\u5236\u52a8\u6001\u8c03\u5236\u81ea\u6ce8\u610f\u529b\u6a21\u5757\u8868\u793a\uff0c\u5e76\u6784\u5efaHyCAM\u6846\u67b6\uff0c\u7ed3\u5408\u5171\u4eab\u7684\u5168\u53c2\u6570CAM\u6a21\u5757\u548c\u591a\u4e2a\u8f7b\u91cf\u7ea7\u4e13\u7528CAM\u6a21\u5757\uff0c\u91c7\u7528\u52a8\u6001\u8def\u7531\u7b56\u7565\u8fdb\u884c\u81ea\u9002\u5e94\u77e5\u8bc6\u878d\u5408\u3002", "result": "\u5728\u95ee\u7b54\u3001\u4ee3\u7801\u751f\u6210\u548c\u903b\u8f91\u63a8\u7406\u7b49\u5f02\u6784\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5e73\u5747\u6027\u80fd\u63d0\u53473.65%\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "CAM\u548cHyCAM\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u4efb\u52a1\u9002\u5e94\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u901a\u7528\u77e5\u8bc6\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u4efb\u52a1\u4e13\u4e1a\u5316\u3002"}}
{"id": "2510.17289", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17289", "abs": "https://arxiv.org/abs/2510.17289", "authors": ["Hajar Bakarou", "Mohamed Sinane El Messoussi", "Ana\u00efs Ollagnier"], "title": "Addressing Antisocial Behavior in Multi-Party Dialogs Through Multimodal Representation Learning", "comment": null, "summary": "Antisocial behavior (ASB) on social media -- including hate speech,\nharassment, and cyberbullying -- poses growing risks to platform safety and\nsocietal well-being. Prior research has focused largely on networks such as X\nand Reddit, while \\textit{multi-party conversational settings} remain\nunderexplored due to limited data. To address this gap, we use\n\\textit{CyberAgressionAdo-Large}, a French open-access dataset simulating ASB\nin multi-party conversations, and evaluate three tasks: \\textit{abuse\ndetection}, \\textit{bullying behavior analysis}, and \\textit{bullying\npeer-group identification}. We benchmark six text-based and eight graph-based\n\\textit{representation-learning methods}, analyzing lexical cues, interactional\ndynamics, and their multimodal fusion. Results show that multimodal models\noutperform unimodal baselines. The late fusion model \\texttt{mBERT + WD-SGCN}\nachieves the best overall results, with top performance on abuse detection\n(0.718) and competitive scores on peer-group identification (0.286) and\nbullying analysis (0.606). Error analysis highlights its effectiveness in\nhandling nuanced ASB phenomena such as implicit aggression, role transitions,\nand context-dependent hostility.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4f7f\u7528\u6cd5\u8bed\u591a\u8f6e\u5bf9\u8bdd\u6570\u636e\u96c6CyberAgressionAdo-Large\uff0c\u8bc4\u4f30\u4e86\u53cd\u793e\u4f1a\u884c\u4e3a\u68c0\u6d4b\u3001\u9738\u51cc\u884c\u4e3a\u5206\u6790\u548c\u9738\u51cc\u540c\u4f34\u7fa4\u4f53\u8bc6\u522b\u4e09\u4e2a\u4efb\u52a1\uff0c\u53d1\u73b0\u591a\u6a21\u6001\u6a21\u578b\u4f18\u4e8e\u5355\u6a21\u6001\u57fa\u7ebf\uff0c\u5176\u4e2dmBERT + WD-SGCN\u878d\u5408\u6a21\u578b\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u4e0a\u7684\u53cd\u793e\u4f1a\u884c\u4e3a\uff08\u5982\u4ec7\u6068\u8a00\u8bba\u3001\u9a9a\u6270\u548c\u7f51\u7edc\u9738\u51cc\uff09\u5bf9\u5e73\u53f0\u5b89\u5168\u548c\u793e\u4f1a\u798f\u7949\u6784\u6210\u65e5\u76ca\u4e25\u91cd\u7684\u98ce\u9669\uff0c\u800c\u591a\u8f6e\u5bf9\u8bdd\u73af\u5883\u4e0b\u7684\u6b64\u7c7b\u884c\u4e3a\u7814\u7a76\u56e0\u6570\u636e\u6709\u9650\u800c\u8f83\u5c11\u88ab\u63a2\u7d22\u3002", "method": "\u4f7f\u7528CyberAgressionAdo-Large\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e866\u79cd\u57fa\u4e8e\u6587\u672c\u548c8\u79cd\u57fa\u4e8e\u56fe\u7684\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u5206\u6790\u8bcd\u6c47\u7ebf\u7d22\u3001\u4ea4\u4e92\u52a8\u6001\u53ca\u5176\u591a\u6a21\u6001\u878d\u5408\u3002", "result": "\u591a\u6a21\u6001\u6a21\u578b\u4f18\u4e8e\u5355\u6a21\u6001\u57fa\u7ebf\uff0cmBERT + WD-SGCN\u878d\u5408\u6a21\u578b\u5728\u53cd\u793e\u4f1a\u884c\u4e3a\u68c0\u6d4b\u4e0a\u8fbe\u52300.718\u7684\u6700\u4f73\u6027\u80fd\uff0c\u5728\u540c\u4f34\u7fa4\u4f53\u8bc6\u522b\u548c\u9738\u51cc\u5206\u6790\u4e0a\u5206\u522b\u83b7\u5f970.286\u548c0.606\u7684\u7ade\u4e89\u6027\u5206\u6570\u3002", "conclusion": "\u591a\u6a21\u6001\u65b9\u6cd5\u80fd\u6709\u6548\u5904\u7406\u9690\u5f0f\u653b\u51fb\u3001\u89d2\u8272\u8f6c\u6362\u548c\u4e0a\u4e0b\u6587\u4f9d\u8d56\u7684\u654c\u610f\u7b49\u7ec6\u5fae\u53cd\u793e\u4f1a\u884c\u4e3a\u73b0\u8c61\uff0c\u4e3a\u591a\u8f6e\u5bf9\u8bdd\u73af\u5883\u4e2d\u7684\u53cd\u793e\u4f1a\u884c\u4e3a\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.17771", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17771", "abs": "https://arxiv.org/abs/2510.17771", "authors": ["Zhining Liu", "Ziyi Chen", "Hui Liu", "Chen Luo", "Xianfeng Tang", "Suhang Wang", "Joy Zeng", "Zhenwei Dai", "Zhan Shi", "Tianxin Wei", "Benoit Dumoulin", "Hanghang Tong"], "title": "Seeing but Not Believing: Probing the Disconnect Between Visual Attention and Answer Correctness in VLMs", "comment": "21 pages, 10 figures, 6 tables", "summary": "Vision-Language Models (VLMs) achieve strong results on multimodal tasks such\nas visual question answering, yet they can still fail even when the correct\nvisual evidence is present. In this work, we systematically investigate whether\nthese failures arise from not perceiving the evidence or from not leveraging it\neffectively. By examining layer-wise attention dynamics, we find that shallow\nlayers focus primarily on text, while deeper layers sparsely but reliably\nattend to localized evidence regions. Surprisingly, VLMs often perceive the\nvisual evidence when outputting incorrect answers, a phenomenon we term\n``seeing but not believing'' that widely exists in major VLM families. Building\non this, we introduce an inference-time intervention that highlights deep-layer\nevidence regions through selective attention-based masking. It requires no\ntraining and consistently improves accuracy across multiple families, including\nLLaVA, Qwen, Gemma, and InternVL. These results show that VLMs encode reliable\nevidence internally but under-utilize it, making such signals explicit can\nbridge the gap between perception and reasoning, advancing the diagnostic\nunderstanding and reliability of VLMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u53d1\u73b0\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u5b58\u5728\"\u770b\u5230\u4f46\u4e0d\u76f8\u4fe1\"\u7684\u73b0\u8c61\uff0c\u5373\u6a21\u578b\u80fd\u611f\u77e5\u5230\u89c6\u89c9\u8bc1\u636e\u4f46\u4ecd\u8f93\u51fa\u9519\u8bef\u7b54\u6848\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u63a8\u7406\u65f6\u5e72\u9884\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u6ce8\u610f\u529b\u63a9\u7801\u6765\u7a81\u51fa\u6df1\u5c42\u8bc1\u636e\u533a\u57df\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u4e2aVLM\u5bb6\u65cf\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u867d\u7136\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5b83\u4eec\u4ecd\u7136\u4f1a\u5728\u6b63\u786e\u89c6\u89c9\u8bc1\u636e\u5b58\u5728\u7684\u60c5\u51b5\u4e0b\u5931\u8d25\u3002\u672c\u7814\u7a76\u65e8\u5728\u7cfb\u7edf\u6027\u5730\u8c03\u67e5\u8fd9\u4e9b\u5931\u8d25\u662f\u7531\u4e8e\u672a\u611f\u77e5\u8bc1\u636e\u8fd8\u662f\u672a\u6709\u6548\u5229\u7528\u8bc1\u636e\u5bfc\u81f4\u7684\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5c42\u95f4\u6ce8\u610f\u529b\u52a8\u6001\uff0c\u53d1\u73b0\u6d45\u5c42\u4e3b\u8981\u5173\u6ce8\u6587\u672c\uff0c\u800c\u6df1\u5c42\u7a00\u758f\u4f46\u53ef\u9760\u5730\u5173\u6ce8\u5c40\u90e8\u8bc1\u636e\u533a\u57df\u3002\u57fa\u4e8e\u6b64\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u63a8\u7406\u65f6\u5e72\u9884\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u6ce8\u610f\u529b\u63a9\u7801\u6765\u7a81\u51fa\u6df1\u5c42\u8bc1\u636e\u533a\u57df\u3002", "result": "\u8be5\u65b9\u6cd5\u65e0\u9700\u8bad\u7ec3\uff0c\u5728\u591a\u4e2aVLM\u5bb6\u65cf(\u5305\u62ecLLaVA\u3001Qwen\u3001Gemma\u548cInternVL)\u4e2d\u4e00\u81f4\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3002", "conclusion": "VLMs\u5728\u5185\u90e8\u7f16\u7801\u4e86\u53ef\u9760\u7684\u8bc1\u636e\u4f46\u672a\u5145\u5206\u5229\u7528\uff0c\u4f7f\u8fd9\u4e9b\u4fe1\u53f7\u663e\u5f0f\u5316\u53ef\u4ee5\u5f25\u5408\u611f\u77e5\u4e0e\u63a8\u7406\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u63a8\u8fdb\u5bf9VLM\u7684\u8bca\u65ad\u7406\u89e3\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2510.17354", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17354", "abs": "https://arxiv.org/abs/2510.17354", "authors": ["Chenghao Zhang", "Guanting Dong", "Xinyu Yang", "Zhicheng Dou"], "title": "Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented Generation", "comment": "This work is in progress", "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for\nenhancing large language models (LLMs) by retrieving relevant documents from an\nexternal corpus. However, existing RAG systems primarily focus on unimodal text\ndocuments, and often fall short in real-world scenarios where both queries and\ndocuments may contain mixed modalities (such as text and images). In this\npaper, we address the challenge of Universal Retrieval-Augmented Generation\n(URAG), which involves retrieving and reasoning over mixed-modal information to\nimprove vision-language generation. To this end, we propose Nyx, a unified\nmixed-modal to mixed-modal retriever tailored for URAG scenarios. To mitigate\nthe scarcity of realistic mixed-modal data, we introduce a four-stage automated\npipeline for generation and filtering, leveraging web documents to construct\nNyxQA, a dataset comprising diverse mixed-modal question-answer pairs that\nbetter reflect real-world information needs. Building on this high-quality\ndataset, we adopt a two-stage training framework for Nyx: we first perform\npre-training on NyxQA along with a variety of open-source retrieval datasets,\nfollowed by supervised fine-tuning using feedback from downstream\nvision-language models (VLMs) to align retrieval outputs with generative\npreferences. Experimental results demonstrate that Nyx not only performs\ncompetitively on standard text-only RAG benchmarks, but also excels in the more\ngeneral and realistic URAG setting, significantly improving generation quality\nin vision-language tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86Nyx\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u6df7\u5408\u6a21\u6001\u5230\u6df7\u5408\u6a21\u6001\u68c0\u7d22\u5668\uff0c\u7528\u4e8e\u89e3\u51b3\u901a\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210(URAG)\u7684\u6311\u6218\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u7ba1\u9053\u6784\u5efa\u6df7\u5408\u6a21\u6001\u6570\u636e\u96c6NyxQA\uff0c\u5e76\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff0c\u5728\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7684RAG\u7cfb\u7edf\u4e3b\u8981\u5173\u6ce8\u5355\u6a21\u6001\u6587\u672c\u6587\u6863\uff0c\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\uff0c\u67e5\u8be2\u548c\u6587\u6863\u53ef\u80fd\u5305\u542b\u6df7\u5408\u6a21\u6001\uff08\u5982\u6587\u672c\u548c\u56fe\u50cf\uff09\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51faNyx\u7edf\u4e00\u6df7\u5408\u6a21\u6001\u68c0\u7d22\u5668\uff0c\u901a\u8fc7\u56db\u9636\u6bb5\u81ea\u52a8\u5316\u7ba1\u9053\u6784\u5efaNyxQA\u6570\u636e\u96c6\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff1a\u5148\u5728NyxQA\u548c\u5f00\u6e90\u68c0\u7d22\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u4f7f\u7528\u4e0b\u6e38\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u53cd\u9988\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\u3002", "result": "Nyx\u4e0d\u4ec5\u5728\u6807\u51c6\u6587\u672cRAG\u57fa\u51c6\u4e0a\u8868\u73b0\u6709\u7ade\u4e89\u529b\uff0c\u5728\u66f4\u901a\u7528\u548c\u73b0\u5b9e\u7684URAG\u8bbe\u7f6e\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u7684\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "Nyx\u4e3a\u6df7\u5408\u6a21\u6001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u591a\u6a21\u6001\u4fe1\u606f\u9700\u6c42\uff0c\u5728\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2510.16233", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16233", "abs": "https://arxiv.org/abs/2510.16233", "authors": ["Patricia West", "Michelle WL Wan", "Alexander Hepburn", "Edwin Simpson", "Raul Santos-Rodriguez", "Jeffrey N Clark"], "title": "Machine Learning for Climate Policy: Understanding Policy Progression in the European Green Deal", "comment": null, "summary": "Climate change demands effective legislative action to mitigate its impacts.\nThis study explores the application of machine learning (ML) to understand the\nprogression of climate policy from announcement to adoption, focusing on\npolicies within the European Green Deal. We present a dataset of 165 policies,\nincorporating text and metadata. We aim to predict a policy's progression\nstatus, and compare text representation methods, including TF-IDF, BERT, and\nClimateBERT. Metadata features are included to evaluate the impact on\npredictive performance. On text features alone, ClimateBERT outperforms other\napproaches (RMSE = 0.17, R^2 = 0.29), while BERT achieves superior performance\nwith the addition of metadata features (RMSE = 0.16, R^2 = 0.38). Using methods\nfrom explainable AI highlights the influence of factors such as policy wording\nand metadata including political party and country representation. These\nfindings underscore the potential of ML tools in supporting climate policy\nanalysis and decision-making.", "AI": {"tldr": "\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5206\u6790\u6b27\u6d32\u7eff\u8272\u534f\u8bae\u6c14\u5019\u653f\u7b56\u7684\u8fdb\u5c55\u72b6\u6001\uff0c\u6bd4\u8f83\u4e0d\u540c\u6587\u672c\u8868\u793a\u65b9\u6cd5\u548c\u5143\u6570\u636e\u7279\u5f81\u5bf9\u9884\u6d4b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u6c14\u5019\u53d8\u5316\u9700\u8981\u6709\u6548\u7684\u7acb\u6cd5\u884c\u52a8\u6765\u51cf\u8f7b\u5176\u5f71\u54cd\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u673a\u5668\u5b66\u4e60\u5728\u7406\u89e3\u6c14\u5019\u653f\u7b56\u4ece\u5ba3\u5e03\u5230\u91c7\u7eb3\u8fc7\u7a0b\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u6536\u96c6165\u9879\u653f\u7b56\u7684\u6587\u672c\u548c\u5143\u6570\u636e\uff0c\u4f7f\u7528TF-IDF\u3001BERT\u548cClimateBERT\u7b49\u6587\u672c\u8868\u793a\u65b9\u6cd5\uff0c\u7ed3\u5408\u5143\u6570\u636e\u7279\u5f81\u9884\u6d4b\u653f\u7b56\u8fdb\u5c55\u72b6\u6001\u3002", "result": "\u4ec5\u4f7f\u7528\u6587\u672c\u7279\u5f81\u65f6\uff0cClimateBERT\u8868\u73b0\u6700\u4f73\uff08RMSE=0.17\uff0cR\u00b2=0.29\uff09\uff1b\u7ed3\u5408\u5143\u6570\u636e\u7279\u5f81\u540e\uff0cBERT\u8868\u73b0\u6700\u4f18\uff08RMSE=0.16\uff0cR\u00b2=0.38\uff09\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u5de5\u5177\u5728\u652f\u6301\u6c14\u5019\u653f\u7b56\u5206\u6790\u548c\u51b3\u7b56\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u53ef\u89e3\u91caAI\u65b9\u6cd5\u63ed\u793a\u4e86\u653f\u7b56\u63aa\u8f9e\u3001\u653f\u515a\u80cc\u666f\u548c\u56fd\u5bb6\u4ee3\u8868\u6027\u7b49\u56e0\u7d20\u7684\u5f71\u54cd\u3002"}}
{"id": "2510.17388", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17388", "abs": "https://arxiv.org/abs/2510.17388", "authors": ["Henry Lim", "Kwan Hui Lim"], "title": "The Atomic Instruction Gap: Instruction-Tuned LLMs Struggle with Simple, Self-Contained Directives", "comment": "11 pages, 1 figure, 8 tables", "summary": "Instruction-tuned large language models (IT-LLMs) exhibit strong zero-shot\nreasoning, yet their ability to execute simple, self-contained instructions\nremains underexplored, despite this being foundational to complex\ninstruction-following. We evaluate 20 IT-LLMs on modified MMLU and MMLU-Pro\nbenchmarks, by systematically varying the format of option labels (alphabetic,\nnumeric, Roman) while keeping their meaning identical under four paradigms,\nnamely: (1) With explicit instructions, label changes cause large performance\nshifts (e.g., -30.45\\% for Roman vs. numeric), revealing instruction-format\nbias. (2) Without instructions, performance drops further (up to -10.84\\%) and\nlabel sensitivity intensifies, underscoring the role of explicit guidance. (3)\nWhen option contents are removed, models fail random-choice baselines except\nwith numeric labels, suggesting weak adherence to atomic directives. (4)\nThree-shot exemplars yield no significant gains in robustness or fidelity, and\ngeneration analyses show persistent label errors, especially for non-numeric\nformats. Across model sizes, larger LLMs achieve higher accuracy but remain\ninconsistent in instruction adherence. These results expose the insufficiencies\nof current instruction-tuning paradigms and highlight the need for evaluation\nmethods and training strategies that explicitly target atomic\ninstruction-following.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u6307\u4ee4\u8c03\u4f18\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7b80\u5355\u6307\u4ee4\u6267\u884c\u4e0a\u5b58\u5728\u4e25\u91cd\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u9009\u9879\u6807\u7b7e\u683c\u5f0f\u53d8\u5316\u65f6\u8868\u73b0\u4e0d\u7a33\u5b9a\uff0c\u66b4\u9732\u4e86\u5f53\u524d\u6307\u4ee4\u8c03\u4f18\u8303\u5f0f\u7684\u7f3a\u9677\u3002", "motivation": "\u5c3d\u7ba1\u6307\u4ee4\u8c03\u4f18\u5927\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u6267\u884c\u7b80\u5355\u3001\u81ea\u5305\u542b\u6307\u4ee4\u7684\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u800c\u8fd9\u6b63\u662f\u590d\u6742\u6307\u4ee4\u9075\u5faa\u7684\u57fa\u7840\u3002", "method": "\u5728\u4fee\u6539\u540e\u7684MMLU\u548cMMLU-Pro\u57fa\u51c6\u4e0a\u8bc4\u4f3020\u4e2a\u6307\u4ee4\u8c03\u4f18\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u7cfb\u7edf\u6027\u5730\u6539\u53d8\u9009\u9879\u6807\u7b7e\u683c\u5f0f\uff08\u5b57\u6bcd\u3001\u6570\u5b57\u3001\u7f57\u9a6c\u6570\u5b57\uff09\uff0c\u5728\u56db\u79cd\u8303\u5f0f\u4e0b\u6d4b\u8bd5\uff1a\u6709\u660e\u786e\u6307\u4ee4\u3001\u65e0\u6307\u4ee4\u3001\u79fb\u9664\u9009\u9879\u5185\u5bb9\u3001\u4e09\u6837\u672c\u793a\u4f8b\u3002", "result": "\u6807\u7b7e\u683c\u5f0f\u53d8\u5316\u5bfc\u81f4\u6027\u80fd\u5927\u5e45\u6ce2\u52a8\uff08\u5982\u7f57\u9a6c\u6570\u5b57vs\u6570\u5b57\u4e0b\u964d30.45%\uff09\uff0c\u65e0\u6307\u4ee4\u65f6\u6027\u80fd\u8fdb\u4e00\u6b65\u4e0b\u964d\uff08\u6700\u591a-10.84%\uff09\uff0c\u79fb\u9664\u9009\u9879\u5185\u5bb9\u65f6\u9664\u6570\u5b57\u6807\u7b7e\u5916\u5747\u65e0\u6cd5\u8d85\u8d8a\u968f\u673a\u57fa\u7ebf\uff0c\u4e09\u6837\u672c\u793a\u4f8b\u65e0\u6cd5\u663e\u8457\u63d0\u5347\u9c81\u68d2\u6027\u3002", "conclusion": "\u5f53\u524d\u6307\u4ee4\u8c03\u4f18\u8303\u5f0f\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u539f\u5b50\u6307\u4ee4\u9075\u5faa\u7684\u8bc4\u4f30\u65b9\u6cd5\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u5373\u4f7f\u66f4\u5927\u7684\u6a21\u578b\u5728\u51c6\u786e\u6027\u4e0a\u66f4\u9ad8\uff0c\u4f46\u5728\u6307\u4ee4\u9075\u5faa\u4e00\u81f4\u6027\u65b9\u9762\u4ecd\u7136\u5b58\u5728\u95ee\u9898\u3002"}}
{"id": "2510.17389", "categories": ["cs.CL", "cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.17389", "abs": "https://arxiv.org/abs/2510.17389", "authors": ["Numaan Naeem", "Abdellah El Mekki", "Muhammad Abdul-Mageed"], "title": "EduAdapt: A Question Answer Benchmark Dataset for Evaluating Grade-Level Adaptability in LLMs", "comment": "28 pages, 2 figures, 14 tables, 50 listings, EMNLP 2025 Main", "summary": "Large language models (LLMs) are transforming education by answering\nquestions, explaining complex concepts, and generating content across a wide\nrange of subjects. Despite strong performance on academic benchmarks, they\noften fail to tailor responses to students' grade levels. This is a critical\nneed in K-12 education, where age-appropriate vocabulary and explanation are\nessential for effective learning. Existing models frequently produce outputs\nthat are too advanced or vague for younger learners, and there are no\nstandardized benchmarks to evaluate their ability to adjust across cognitive\nand developmental stages. To address this gap, we introduce EduAdapt, a\nbenchmark of nearly 48k grade-labeled QA pairs across nine science subjects,\nspanning Grades 1-12 and grouped into four grade levels. We evaluate a diverse\nset of open-source LLMs on EduAdapt and find that while larger models generally\nperform better, they still struggle with generating suitable responses for\nearly-grade students (Grades 1-5). Our work presents the first dataset and\nevaluation framework for assessing grade-level adaptability in LLMs, aiming to\nfoster more developmentally aligned educational AI systems through better\ntraining and prompting strategies. EduAdapt code and datasets are publicly\navailable at https://github.com/NaumanNaeem/EduAdapt.", "AI": {"tldr": "\u63d0\u51fa\u4e86EduAdapt\u57fa\u51c6\uff0c\u5305\u542b\u8fd148K\u4e2a\u6309\u5e74\u7ea7\u6807\u8bb0\u7684\u79d1\u5b66\u95ee\u7b54\u5bf9\uff0c\u7528\u4e8e\u8bc4\u4f30LLMs\u5728\u4e0d\u540c\u5e74\u7ea7\u6c34\u5e73\u4e0a\u7684\u9002\u5e94\u6027\u8868\u73b0\u3002", "motivation": "\u73b0\u6709LLMs\u867d\u7136\u5728\u5b66\u672f\u57fa\u51c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u65e0\u6cd5\u6839\u636e\u5b66\u751f\u5e74\u7ea7\u6c34\u5e73\u8c03\u6574\u56de\u7b54\uff0c\u8fd9\u5728K-12\u6559\u80b2\u4e2d\u81f3\u5173\u91cd\u8981\u3002\u7f3a\u4e4f\u6807\u51c6\u5316\u57fa\u51c6\u6765\u8bc4\u4f30LLMs\u8de8\u8ba4\u77e5\u53d1\u5c55\u9636\u6bb5\u7684\u9002\u5e94\u80fd\u529b\u3002", "method": "\u521b\u5efa\u4e86EduAdapt\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u6db5\u76d69\u4e2a\u79d1\u5b66\u5b66\u79d1\u768447,800\u4e2a\u95ee\u7b54\u5bf9\uff0c\u6a2a\u8de81-12\u5e74\u7ea7\uff0c\u5206\u4e3a\u56db\u4e2a\u5e74\u7ea7\u6c34\u5e73\u3002\u8bc4\u4f30\u4e86\u591a\u4e2a\u5f00\u6e90LLMs\u5728\u8be5\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u5927\u578b\u6a21\u578b\u901a\u5e38\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u5728\u4e3a\u4f4e\u5e74\u7ea7\u5b66\u751f\uff081-5\u5e74\u7ea7\uff09\u751f\u6210\u5408\u9002\u56de\u7b54\u65b9\u9762\u4ecd\u6709\u56f0\u96be\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u8bc4\u4f30LLMs\u5e74\u7ea7\u6c34\u5e73\u9002\u5e94\u6027\u7684\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u6539\u8fdb\u8bad\u7ec3\u548c\u63d0\u793a\u7b56\u7565\u5f00\u53d1\u66f4\u9002\u5408\u6559\u80b2\u53d1\u5c55\u7684AI\u7cfb\u7edf\u3002"}}
{"id": "2510.15911", "categories": ["q-fin.GN", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15911", "abs": "https://arxiv.org/abs/2510.15911", "authors": ["Ben Abramowitz"], "title": "Sleeping Kelly is a Thirder", "comment": null, "summary": "The Sleeping Beauty problem was presented by Elga and highlights the role of\nprobabilities in situations with imperfect recall. One approach to solving the\nSleeping Beauty problem is to allow Sleeping Beauty to make decisions based on\nher beliefs, and then characterize what it takes for her decisions to be\n\"rational\". In particular, she can be allowed to make monetary bets based on\nher beliefs, with the assumption that she wants to gain wealth rather than lose\nit. However, this approach is often coupled with the assumption that Sleeping\nBeauty should maximize the expected value of her bets. Here, I argue instead\nthat it is rational for Sleeping Beauty to maximize the growth rate of her\nwealth using the Kelly Criterion, which leads us to the \"thirder\" position.\nFurthermore, this position is shown to be \"rational\" by Dutch book arguments.\nIf Sleeping Kelly only accepts bets that have a growth rate greater than 1 as a\n\"thirder\" then she is not vulnerable to Dutch books. By contrast, if Sleeping\nBeauty takes the \"halfer\" position, she is vulnerable to Dutch books. If the\nbets offered to Sleeping Beauty were to be structured differently and lead to\nnon-multiplicative wealth dynamics, she may no longer be a \"thirder\".", "AI": {"tldr": "\u672c\u6587\u4e3b\u5f20\u4f7f\u7528\u51ef\u5229\u51c6\u5219\uff08Kelly Criterion\uff09\u6765\u89e3\u51b3\u7761\u7f8e\u4eba\u95ee\u9898\uff0c\u8ba4\u4e3a\u7761\u7f8e\u4eba\u5e94\u8be5\u6700\u5927\u5316\u8d22\u5bcc\u589e\u957f\u7387\u800c\u975e\u671f\u671b\u503c\uff0c\u8fd9\u5bfc\u5411\"\u4e09\u5206\u4e4b\u4e00\u6d3e\"\u7acb\u573a\uff0c\u5e76\u901a\u8fc7\u8377\u5170\u8d4c\u8bba\u8bc1\u5176\u5408\u7406\u6027\u3002", "motivation": "\u7761\u7f8e\u4eba\u95ee\u9898\u51f8\u663e\u4e86\u4e0d\u5b8c\u5168\u8bb0\u5fc6\u60c5\u5883\u4e2d\u7684\u6982\u7387\u95ee\u9898\u3002\u4f20\u7edf\u65b9\u6cd5\u5047\u8bbe\u7761\u7f8e\u4eba\u5e94\u6700\u5927\u5316\u8d4c\u6ce8\u7684\u671f\u671b\u503c\uff0c\u4f46\u672c\u6587\u8ba4\u4e3a\u8fd9\u79cd\u5047\u8bbe\u5b58\u5728\u95ee\u9898\uff0c\u9700\u8981\u91cd\u65b0\u601d\u8003\u7406\u6027\u51b3\u7b56\u7684\u6807\u51c6\u3002", "method": "\u91c7\u7528\u51ef\u5229\u51c6\u5219\uff08\u8d22\u5bcc\u589e\u957f\u7387\u6700\u5927\u5316\uff09\u800c\u975e\u671f\u671b\u503c\u6700\u5927\u5316\u6765\u5206\u6790\u7761\u7f8e\u4eba\u7684\u51b3\u7b56\u3002\u901a\u8fc7\u8377\u5170\u8d4c\u8bba\u8bc1\u6765\u68c0\u9a8c\u4e0d\u540c\u7acb\u573a\u7684\u5408\u7406\u6027\u3002", "result": "\u5982\u679c\u7761\u7f8e\u4eba\u91c7\u7528\"\u4e09\u5206\u4e4b\u4e00\u6d3e\"\u7acb\u573a\u5e76\u53ea\u63a5\u53d7\u589e\u957f\u7387\u5927\u4e8e1\u7684\u8d4c\u6ce8\uff0c\u5979\u5c31\u4e0d\u4f1a\u53d7\u5230\u8377\u5170\u8d4c\u7684\u5a01\u80c1\uff1b\u800c\"\u4e8c\u5206\u4e4b\u4e00\u6d3e\"\u7acb\u573a\u5219\u4f1a\u4f7f\u5979\u6613\u53d7\u8377\u5170\u8d4c\u653b\u51fb\u3002", "conclusion": "\u5728\u4e58\u6027\u8d22\u5bcc\u52a8\u6001\u4e0b\uff0c\u7761\u7f8e\u4eba\u91c7\u7528\u51ef\u5229\u51c6\u5219\u5e76\u6301\"\u4e09\u5206\u4e4b\u4e00\u6d3e\"\u7acb\u573a\u662f\u7406\u6027\u7684\u9009\u62e9\uff0c\u4f46\u82e5\u8d4c\u6ce8\u7ed3\u6784\u6539\u53d8\u5bfc\u81f4\u975e\u4e58\u6027\u8d22\u5bcc\u52a8\u6001\uff0c\u8fd9\u4e00\u7ed3\u8bba\u53ef\u80fd\u4e0d\u518d\u6210\u7acb\u3002"}}
{"id": "2510.16252", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16252", "abs": "https://arxiv.org/abs/2510.16252", "authors": ["Yuxuan Lu", "Jing Huang", "Hui Liu", "Jiri Gesi", "Yan Han", "Shihan Fu", "Tianqi Zheng", "Dakuo Wang"], "title": "WEBSERV: A Browser-Server Environment for Efficient Training of Reinforcement Learning-based Web Agents at Scale", "comment": null, "summary": "Training and evaluation of Reinforcement Learning (RL) web agents have gained\nincreasing attention, yet a scalable and efficient environment that couples\nrealistic and robust browser-side interaction with controllable server-side\nstate at scale is still missing. Existing environments tend to have one or more\nof the following issues: they overwhelm policy models with excessive and noisy\ncontext; they perform actions non-deterministically without waiting for the UI\nor network to stabilize; or they cannot scale isolated client-server containers\neffectively for parallel RL rollouts. We propose WEBSERV, an environment that\nincludes 1) a compact, site-agnostic browser environment that balances context\nand action complexity, and 2) a scalable RL environment via efficient launching\nand resetting web-servers to enable scalable RL training and evaluation. We\nevaluate WEBSERV on the shopping CMS and Gitlab tasks in WebArena, achieving\nstate-of-the-art single-prompt success rates while cutting launch latency by\n~5x and storage need by ~240x, with a comparable memory footprint, enabling\n200+ concurrent containers on a single host.", "AI": {"tldr": "WEBSERV\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u5f3a\u5316\u5b66\u4e60Web\u4ee3\u7406\u73af\u5883\uff0c\u901a\u8fc7\u7d27\u51d1\u7684\u6d4f\u89c8\u5668\u73af\u5883\u548c\u9ad8\u6548\u7684\u670d\u52a1\u7aef\u7ba1\u7406\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u73af\u5883\u5728\u4e0a\u4e0b\u6587\u566a\u58f0\u3001\u52a8\u4f5c\u4e0d\u786e\u5b9a\u6027\u548c\u6269\u5c55\u6027\u65b9\u9762\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684RL Web\u4ee3\u7406\u73af\u5883\u5b58\u5728\u4e0a\u4e0b\u6587\u566a\u58f0\u8fc7\u591a\u3001\u52a8\u4f5c\u6267\u884c\u4e0d\u786e\u5b9a\u3001\u4ee5\u53ca\u65e0\u6cd5\u6709\u6548\u6269\u5c55\u5e76\u884cRL\u8bad\u7ec3\u7b49\u95ee\u9898\uff0c\u9700\u8981\u4e00\u4e2a\u65b0\u7684\u73af\u5883\u6765\u652f\u6301\u5927\u89c4\u6a21\u3001\u9ad8\u6548\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "method": "\u63d0\u51faWEBSERV\u73af\u5883\uff0c\u5305\u62ec\uff1a1\uff09\u7d27\u51d1\u3001\u7ad9\u70b9\u65e0\u5173\u7684\u6d4f\u89c8\u5668\u73af\u5883\uff0c\u5e73\u8861\u4e0a\u4e0b\u6587\u548c\u52a8\u4f5c\u590d\u6742\u6027\uff1b2\uff09\u901a\u8fc7\u9ad8\u6548\u542f\u52a8\u548c\u91cd\u7f6eWeb\u670d\u52a1\u5668\u5b9e\u73b0\u53ef\u6269\u5c55\u7684RL\u73af\u5883\u3002", "result": "\u5728WebArena\u7684\u8d2d\u7269CMS\u548cGitlab\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u5355\u63d0\u793a\u6210\u529f\u7387\uff0c\u540c\u65f6\u5c06\u542f\u52a8\u5ef6\u8fdf\u964d\u4f4e\u7ea65\u500d\uff0c\u5b58\u50a8\u9700\u6c42\u51cf\u5c11\u7ea6240\u500d\uff0c\u5185\u5b58\u5360\u7528\u76f8\u5f53\uff0c\u652f\u6301\u5355\u4e3b\u673a\u4e0a200+\u5e76\u53d1\u5bb9\u5668\u3002", "conclusion": "WEBSERV\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684RL Web\u4ee3\u7406\u8bad\u7ec3\u73af\u5883\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u5e76\u964d\u4f4e\u4e86\u8d44\u6e90\u9700\u6c42\u3002"}}
{"id": "2510.17402", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17402", "abs": "https://arxiv.org/abs/2510.17402", "authors": ["Jiacheng Xie", "Shuai Zeng", "Yang Yu", "Xiaoting Tang", "Guanghui An", "Dong Xu"], "title": "Leveraging Group Relative Policy Optimization to Advance Large Language Models in Traditional Chinese Medicine", "comment": null, "summary": "Traditional Chinese Medicine (TCM) presents a rich and structurally unique\nknowledge system that challenges conventional applications of large language\nmodels (LLMs). Although previous TCM-specific LLMs have shown progress through\nsupervised fine-tuning, they often face limitations in alignment, data quality,\nand evaluation consistency. In this study, we introduce Ladder-base, the first\nTCM-focused LLM trained with Group Relative Policy Optimization (GRPO), a\nreinforcement learning method that improves reasoning and factual consistency\nby optimizing response selection based on intra-group comparisons. Ladder-base\nis built upon the Qwen2.5-7B-Instruct foundation model and trained exclusively\non the textual subset of the TCM-Ladder benchmark, using 80 percent of the data\nfor training and the remaining 20 percent split evenly between validation and\ntest sets. Through standardized evaluation, Ladder-base demonstrates superior\nperformance across multiple reasoning metrics when compared to both\nstate-of-the-art general-purpose LLMs such as GPT-4, Gemini 2.5, Claude 3, and\nQwen3 and domain-specific TCM models including BenTsao, HuatuoGPT2, and\nZhongjing. These findings suggest that GRPO provides an effective and efficient\nstrategy for aligning LLMs with expert-level reasoning in traditional medical\ndomains and supports the development of trustworthy and clinically grounded TCM\nartificial intelligence systems.", "AI": {"tldr": "Ladder-base\u662f\u9996\u4e2a\u57fa\u4e8eGRPO\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u8bad\u7ec3\u7684\u4e2d\u533b\u9886\u57df\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u591a\u9879\u63a8\u7406\u6307\u6807\u4e0a\u8d85\u8d8a\u4e86\u901a\u7528LLM\u548c\u4e2d\u533b\u4e13\u7528\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u4e2d\u533b\u77e5\u8bc6\u4f53\u7cfb\u72ec\u7279\u4e14\u590d\u6742\uff0c\u73b0\u6709\u4e2d\u533bLLM\u5728\u4e00\u81f4\u6027\u3001\u6570\u636e\u8d28\u91cf\u548c\u8bc4\u4f30\u6807\u51c6\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u5bf9\u9f50\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8eQwen2.5-7B-Instruct\u57fa\u7840\u6a21\u578b\uff0c\u4f7f\u7528TCM-Ladder\u57fa\u51c6\u7684\u6587\u672c\u5b50\u96c6\uff0c\u91c7\u7528GRPO\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u8bad\u7ec3\uff0c\u901a\u8fc7\u7ec4\u5185\u6bd4\u8f83\u4f18\u5316\u54cd\u5e94\u9009\u62e9\u3002", "result": "Ladder-base\u5728\u6807\u51c6\u5316\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u591a\u9879\u63a8\u7406\u6307\u6807\u4e0a\u8d85\u8d8a\u4e86GPT-4\u3001Gemini 2.5\u7b49\u901a\u7528LLM\u548cBenTsao\u3001HuatuoGPT2\u7b49\u4e2d\u533b\u4e13\u7528\u6a21\u578b\u3002", "conclusion": "GRPO\u4e3a\u4f20\u7edf\u533b\u5b66\u9886\u57dfLLM\u4e0e\u4e13\u5bb6\u7ea7\u63a8\u7406\u5bf9\u9f50\u63d0\u4f9b\u4e86\u6709\u6548\u7b56\u7565\uff0c\u652f\u6301\u5f00\u53d1\u53ef\u4fe1\u8d56\u4e14\u4e34\u5e8a\u57fa\u7840\u624e\u5b9e\u7684\u4e2d\u533b\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u3002"}}
{"id": "2510.17405", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17405", "abs": "https://arxiv.org/abs/2510.17405", "authors": ["Mardiyyah Oduwole", "Prince Mireku", "Fatimo Adebanjo", "Oluwatosin Olajide", "Mahi Aminu Aliyu", "Jekaterina Novikova"], "title": "AFRICAPTION: Establishing a New Paradigm for Image Captioning in African Languages", "comment": null, "summary": "Multimodal AI research has overwhelmingly focused on high-resource languages,\nhindering the democratization of advancements in the field. To address this, we\npresent AfriCaption, a comprehensive framework for multilingual image\ncaptioning in 20 African languages and our contributions are threefold: (i) a\ncurated dataset built on Flickr8k, featuring semantically aligned captions\ngenerated via a context-aware selection and translation process; (ii) a\ndynamic, context-preserving pipeline that ensures ongoing quality through model\nensembling and adaptive substitution; and (iii) the AfriCaption model, a 0.5B\nparameter vision-to-text architecture that integrates SigLIP and NLLB200 for\ncaption generation across under-represented languages. This unified framework\nensures ongoing data quality and establishes the first scalable\nimage-captioning resource for under-represented African languages, laying the\ngroundwork for truly inclusive multimodal AI.", "AI": {"tldr": "\u63d0\u51fa\u4e86AfriCaption\u6846\u67b6\uff0c\u4e3a20\u79cd\u975e\u6d32\u8bed\u8a00\u63d0\u4f9b\u591a\u8bed\u8a00\u56fe\u50cf\u63cf\u8ff0\u529f\u80fd\uff0c\u586b\u8865\u4e86\u591a\u6a21\u6001AI\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u65b9\u9762\u7684\u7a7a\u767d\u3002", "motivation": "\u591a\u6a21\u6001AI\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u9ad8\u8d44\u6e90\u8bed\u8a00\uff0c\u963b\u788d\u4e86\u8be5\u9886\u57df\u53d1\u5c55\u7684\u6c11\u4e3b\u5316\u3002\u9700\u8981\u4e3a\u975e\u6d32\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u63d0\u4f9b\u56fe\u50cf\u63cf\u8ff0\u80fd\u529b\u3002", "method": "\u6784\u5efa\u57fa\u4e8eFlickr8k\u7684\u6570\u636e\u96c6\uff0c\u91c7\u7528\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u9009\u62e9\u548c\u7ffb\u8bd1\u8fc7\u7a0b\uff1b\u5f00\u53d1\u52a8\u6001\u4e0a\u4e0b\u6587\u4fdd\u6301\u7ba1\u9053\uff0c\u901a\u8fc7\u6a21\u578b\u96c6\u6210\u548c\u81ea\u9002\u5e94\u66ff\u6362\u786e\u4fdd\u8d28\u91cf\uff1b\u521b\u5efa0.5B\u53c2\u6570\u7684AfriCaption\u6a21\u578b\uff0c\u96c6\u6210SigLIP\u548cNLLB200\u8fdb\u884c\u8de8\u8bed\u8a00\u63cf\u8ff0\u751f\u6210\u3002", "result": "\u5efa\u7acb\u4e86\u9996\u4e2a\u53ef\u6269\u5c55\u7684\u975e\u6d32\u4f4e\u8d44\u6e90\u8bed\u8a00\u56fe\u50cf\u63cf\u8ff0\u8d44\u6e90\uff0c\u786e\u4fdd\u4e86\u6301\u7eed\u7684\u6570\u636e\u8d28\u91cf\u3002", "conclusion": "\u8be5\u7edf\u4e00\u6846\u67b6\u4e3a\u771f\u6b63\u5305\u5bb9\u7684\u591a\u6a21\u6001AI\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63a8\u52a8\u4e86AI\u6280\u672f\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u7684\u6c11\u4e3b\u5316\u5e94\u7528\u3002"}}
{"id": "2510.16289", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16289", "abs": "https://arxiv.org/abs/2510.16289", "authors": ["Yoonho Lee", "Junseok Lee", "Sangwoo Seo", "Sungwon Kim", "Yeongmin Kim", "Chanyoung Park"], "title": "Disentangling Hyperedges through the Lens of Category Theory", "comment": "Accepted to NeurIPS 2025", "summary": "Despite the promising results of disentangled representation learning in\ndiscovering latent patterns in graph-structured data, few studies have explored\ndisentanglement for hypergraph-structured data. Integrating hyperedge\ndisentanglement into hypergraph neural networks enables models to leverage\nhidden hyperedge semantics, such as unannotated relations between nodes, that\nare associated with labels. This paper presents an analysis of hyperedge\ndisentanglement from a category-theoretical perspective and proposes a novel\ncriterion for disentanglement derived from the naturality condition. Our\nproof-of-concept model experimentally showed the potential of the proposed\ncriterion by successfully capturing functional relations of genes (nodes) in\ngenetic pathways (hyperedges).", "AI": {"tldr": "\u672c\u6587\u4ece\u8303\u7574\u8bba\u89d2\u5ea6\u5206\u6790\u8d85\u8fb9\u89e3\u7f20\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u81ea\u7136\u6027\u6761\u4ef6\u7684\u89e3\u7f20\u51c6\u5219\uff0c\u5e76\u5728\u57fa\u56e0\u901a\u8def\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5c3d\u7ba1\u89e3\u7f20\u8868\u793a\u5b66\u4e60\u5728\u56fe\u7ed3\u6784\u6570\u636e\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5f88\u5c11\u6709\u7814\u7a76\u63a2\u7d22\u8d85\u56fe\u7ed3\u6784\u6570\u636e\u7684\u89e3\u7f20\u3002\u5c06\u8d85\u8fb9\u89e3\u7f20\u6574\u5408\u5230\u8d85\u56fe\u795e\u7ecf\u7f51\u7edc\u4e2d\uff0c\u53ef\u4ee5\u6316\u6398\u4e0e\u6807\u7b7e\u76f8\u5173\u7684\u9690\u85cf\u8d85\u8fb9\u8bed\u4e49\u3002", "method": "\u4ece\u8303\u7574\u8bba\u89c6\u89d2\u5206\u6790\u8d85\u8fb9\u89e3\u7f20\uff0c\u63d0\u51fa\u57fa\u4e8e\u81ea\u7136\u6027\u6761\u4ef6\u7684\u65b0\u89e3\u7f20\u51c6\u5219\uff0c\u5e76\u6784\u5efa\u6982\u5ff5\u9a8c\u8bc1\u6a21\u578b\u3002", "result": "\u5728\u57fa\u56e0\u901a\u8def\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51c6\u5219\u6210\u529f\u6355\u6349\u4e86\u57fa\u56e0\uff08\u8282\u70b9\uff09\u5728\u9057\u4f20\u901a\u8def\uff08\u8d85\u8fb9\uff09\u4e2d\u7684\u529f\u80fd\u5173\u7cfb\u3002", "conclusion": "\u63d0\u51fa\u7684\u89e3\u7f20\u51c6\u5219\u5177\u6709\u6f5c\u529b\uff0c\u80fd\u591f\u6709\u6548\u6316\u6398\u8d85\u56fe\u4e2d\u7684\u9690\u85cf\u8bed\u4e49\u5173\u7cfb\u3002"}}
{"id": "2510.17415", "categories": ["cs.CL", "cs.AI", "cs.MA", "cs.MM", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17415", "abs": "https://arxiv.org/abs/2510.17415", "authors": ["Jiacheng Xie", "Yang Yu", "Yibo Chen", "Hanyao Zhang", "Lening Zhao", "Jiaxuan He", "Lei Jiang", "Xiaoting Tang", "Guanghui An", "Dong Xu"], "title": "BenCao: An Instruction-Tuned Large Language Model for Traditional Chinese Medicine", "comment": null, "summary": "Traditional Chinese Medicine (TCM), with a history spanning over two\nmillennia, plays a role in global healthcare. However, applying large language\nmodels (LLMs) to TCM remains challenging due to its reliance on holistic\nreasoning, implicit logic, and multimodal diagnostic cues. Existing TCM-domain\nLLMs have made progress in text-based understanding but lack multimodal\nintegration, interpretability, and clinical applicability. To address these\nlimitations, we developed BenCao, a ChatGPT-based multimodal assistant for TCM,\nintegrating structured knowledge bases, diagnostic data, and expert feedback\nrefinement. BenCao was trained through natural language instruction tuning\nrather than parameter retraining, aligning with expert-level reasoning and\nethical norms specific to TCM. The system incorporates a comprehensive\nknowledge base of over 1,000 classical and modern texts, a scenario-based\ninstruction framework for diverse interactions, a chain-of-thought simulation\nmechanism for interpretable reasoning, and a feedback refinement process\ninvolving licensed TCM practitioners. BenCao connects to external APIs for\ntongue-image classification and multimodal database retrieval, enabling dynamic\naccess to diagnostic resources. In evaluations across single-choice question\nbenchmarks and multimodal classification tasks, BenCao achieved superior\naccuracy to general-domain and TCM-domain models, particularly in diagnostics,\nherb recognition, and constitution classification. The model was deployed as an\ninteractive application on the OpenAI GPTs Store, accessed by nearly 1,000\nusers globally as of October 2025. This study demonstrates the feasibility of\ndeveloping a TCM-domain LLM through natural language-based instruction tuning\nand multimodal integration, offering a practical framework for aligning\ngenerative AI with traditional medical reasoning and a scalable pathway for\nreal-world deployment.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86BenCao\uff0c\u4e00\u4e2a\u57fa\u4e8eChatGPT\u7684\u4e2d\u533b\u591a\u6a21\u6001\u52a9\u624b\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u8c03\u4f18\u800c\u975e\u53c2\u6570\u91cd\u8bad\u7ec3\uff0c\u6574\u5408\u4e86\u7ed3\u6784\u5316\u77e5\u8bc6\u5e93\u3001\u8bca\u65ad\u6570\u636e\u548c\u4e13\u5bb6\u53cd\u9988\uff0c\u5728\u4e2d\u533b\u95ee\u7b54\u548c\u8bca\u65ad\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u901a\u7528\u548c\u4e2d\u533b\u9886\u57df\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u4e2d\u533b\u4f9d\u8d56\u6574\u4f53\u63a8\u7406\u3001\u9690\u542b\u903b\u8f91\u548c\u591a\u6a21\u6001\u8bca\u65ad\u7ebf\u7d22\uff0c\u73b0\u6709\u4e2d\u533b\u9886\u57df\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6587\u672c\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u7f3a\u4e4f\u591a\u6a21\u6001\u6574\u5408\u3001\u53ef\u89e3\u91ca\u6027\u548c\u4e34\u5e8a\u9002\u7528\u6027\u3002", "method": "\u5f00\u53d1BenCao\u7cfb\u7edf\uff0c\u6574\u54081000\u591a\u90e8\u7ecf\u5178\u548c\u73b0\u4ee3\u6587\u672c\u7684\u77e5\u8bc6\u5e93\uff0c\u57fa\u4e8e\u573a\u666f\u7684\u6307\u4ee4\u6846\u67b6\uff0c\u53ef\u89e3\u91ca\u63a8\u7406\u7684\u601d\u7ef4\u94fe\u6a21\u62df\u673a\u5236\uff0c\u4ee5\u53ca\u6267\u4e1a\u4e2d\u533b\u5e08\u7684\u53cd\u9988\u7cbe\u70bc\u8fc7\u7a0b\uff0c\u8fde\u63a5\u5916\u90e8API\u8fdb\u884c\u820c\u8c61\u5206\u7c7b\u548c\u591a\u6a21\u6001\u6570\u636e\u5e93\u68c0\u7d22\u3002", "result": "\u5728\u5355\u9879\u9009\u62e9\u9898\u57fa\u51c6\u548c\u591a\u6a21\u6001\u5206\u7c7b\u4efb\u52a1\u8bc4\u4f30\u4e2d\uff0cBenCao\u5728\u8bca\u65ad\u3001\u8349\u836f\u8bc6\u522b\u548c\u4f53\u8d28\u5206\u7c7b\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u901a\u7528\u548c\u4e2d\u533b\u9886\u57df\u6a21\u578b\uff0c\u5df2\u5728OpenAI GPTs Store\u90e8\u7f72\uff0c\u622a\u81f32025\u5e7410\u6708\u6709\u8fd11000\u540d\u5168\u7403\u7528\u6237\u8bbf\u95ee\u3002", "conclusion": "\u672c\u7814\u7a76\u8bc1\u660e\u4e86\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u8c03\u4f18\u548c\u591a\u6a21\u6001\u6574\u5408\u5f00\u53d1\u4e2d\u533b\u9886\u57df\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u751f\u6210\u5f0fAI\u4e0e\u4f20\u7edf\u533b\u5b66\u63a8\u7406\u7684\u5bf9\u9f50\u63d0\u4f9b\u4e86\u5b9e\u7528\u6846\u67b6\u548c\u53ef\u6269\u5c55\u7684\u73b0\u5b9e\u90e8\u7f72\u8def\u5f84\u3002"}}
{"id": "2510.16292", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16292", "abs": "https://arxiv.org/abs/2510.16292", "authors": ["Yutong Wang", "Haiyu Wang", "Sai Qian Zhang"], "title": "QSVD: Efficient Low-rank Approximation for Unified Query-Key-Value Weight Compression in Low-Precision Vision-Language Models", "comment": "Accepted as Spotlight paper by NeurIPS 2025", "summary": "Vision-Language Models (VLMs) are integral to tasks such as image captioning\nand visual question answering, but their high computational cost, driven by\nlarge memory footprints and processing time, limits their scalability and\nreal-time applicability. In this work, we propose leveraging Singular-Value\nDecomposition (SVD) over the joint query (Q), key (K), and value (V) weight\nmatrices to reduce KV cache size and computational overhead. We in addition\nintroduce an efficient rank allocation strategy that dynamically adjusts the\nSVD rank based on its impact on VLM accuracy, achieving a significant reduction\nin both memory usage and computational cost. Finally, we extend this approach\nby applying quantization to both VLM weights and activations, resulting in a\nhighly efficient VLM. Our method outperforms previous approaches that rely\nsolely on quantization or SVD by achieving more than $10\\%$ accuracy\nimprovement while consuming less hardware cost, making it better for real-time\ndeployment on resource-constrained devices. We open source our code at\n\\href{https://github.com/SAI-Lab-NYU/QSVD}{\\texttt{https://github.com/SAI-Lab-NYU/QSVD}}.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u5947\u5f02\u503c\u5206\u89e3(SVD)\u548c\u91cf\u5316\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574SVD\u79e9\u6765\u51cf\u5c11\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684KV\u7f13\u5b58\u5927\u5c0f\u548c\u8ba1\u7b97\u5f00\u9500\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u5185\u5b58\u4f7f\u7528\u548c\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u5728\u56fe\u50cf\u63cf\u8ff0\u548c\u89c6\u89c9\u95ee\u7b54\u7b49\u4efb\u52a1\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u5176\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u5185\u5b58\u5360\u7528\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u65f6\u5e94\u7528\u3002", "method": "\u5bf9\u8054\u5408QKV\u6743\u91cd\u77e9\u9635\u5e94\u7528\u5947\u5f02\u503c\u5206\u89e3(SVD)\u6765\u51cf\u5c11KV\u7f13\u5b58\u5927\u5c0f\uff0c\u5f15\u5165\u52a8\u6001SVD\u79e9\u5206\u914d\u7b56\u7565\uff0c\u5e76\u7ed3\u5408\u6743\u91cd\u548c\u6fc0\u6d3b\u7684\u91cf\u5316\u6280\u672f\u3002", "result": "\u76f8\u6bd4\u4ec5\u4f7f\u7528\u91cf\u5316\u6216SVD\u7684\u65b9\u6cd5\uff0c\u51c6\u786e\u7387\u63d0\u5347\u8d85\u8fc710%\uff0c\u540c\u65f6\u786c\u4ef6\u6210\u672c\u66f4\u4f4e\uff0c\u66f4\u9002\u5408\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u7684\u5b9e\u65f6\u90e8\u7f72\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7SVD\u548c\u91cf\u5316\u7684\u7ed3\u5408\uff0c\u5728\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u7cbe\u5ea6\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684VLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.17426", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17426", "abs": "https://arxiv.org/abs/2510.17426", "authors": ["Tiancheng Hu", "Benjamin Minixhofer", "Nigel Collier"], "title": "Navigating the Alignment-Calibration Trade-off: A Pareto-Superior Frontier via Model Merging", "comment": null, "summary": "The \"alignment tax\" of post-training is typically framed as a drop in task\naccuracy. We show it also involves a severe loss of calibration, making models\noverconfident, less reliable, and model outputs less diverse. We show that this\ntrade-off can be navigated effectively via a simple post-hoc intervention:\ninterpolating between a model's weights before and after alignment. Crucially,\nthis is not a strict trade-off. We find that the process consistently reveals\nPareto-optimal interpolations - models that improve accuracy beyond both\nparents while substantially recovering the calibration lost during alignment.\nOur work demonstrates that simple model merging provides a computationally\nefficient method for mitigating the full scope of the alignment tax, yielding\nmodels that are more capable and more reliable.", "AI": {"tldr": "\u8bba\u6587\u63ed\u793a\u4e86\u540e\u8bad\u7ec3\u5bf9\u9f50\u4e0d\u4ec5\u5bfc\u81f4\u4efb\u52a1\u51c6\u786e\u7387\u4e0b\u964d\uff0c\u8fd8\u4f1a\u9020\u6210\u4e25\u91cd\u7684\u6821\u51c6\u635f\u5931\uff0c\u4f7f\u6a21\u578b\u8fc7\u5ea6\u81ea\u4fe1\u4e14\u8f93\u51fa\u591a\u6837\u6027\u964d\u4f4e\u3002\u901a\u8fc7\u7b80\u5355\u7684\u6743\u91cd\u63d2\u503c\u65b9\u6cd5\uff0c\u53ef\u4ee5\u627e\u5230\u5e15\u7d2f\u6258\u6700\u4f18\u6a21\u578b\uff0c\u5728\u63d0\u5347\u51c6\u786e\u7387\u7684\u540c\u65f6\u6062\u590d\u6821\u51c6\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u4e0a\"\u5bf9\u9f50\u7a0e\"\u4e3b\u8981\u5173\u6ce8\u4efb\u52a1\u51c6\u786e\u7387\u4e0b\u964d\uff0c\u4f46\u4f5c\u8005\u53d1\u73b0\u5b83\u8fd8\u6d89\u53ca\u6821\u51c6\u635f\u5931\uff0c\u5bfc\u81f4\u6a21\u578b\u8fc7\u5ea6\u81ea\u4fe1\u548c\u53ef\u9760\u6027\u4e0b\u964d\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u7b80\u5355\u7684\u540e\u5904\u7406\u5e72\u9884\uff1a\u5728\u6a21\u578b\u5bf9\u9f50\u524d\u540e\u7684\u6743\u91cd\u4e4b\u95f4\u8fdb\u884c\u63d2\u503c\uff0c\u5bfb\u627e\u5e15\u7d2f\u6258\u6700\u4f18\u7684\u4e2d\u95f4\u6a21\u578b\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u627e\u5230\u540c\u65f6\u8d85\u8d8a\u4e24\u4e2a\u7236\u6a21\u578b\u51c6\u786e\u7387\u7684\u6a21\u578b\uff0c\u5e76\u663e\u8457\u6062\u590d\u5bf9\u9f50\u8fc7\u7a0b\u4e2d\u635f\u5931\u7684\u6821\u51c6\u6027\u80fd\uff0c\u8bc1\u660e\u8fd9\u4e0d\u662f\u4e25\u683c\u7684\u6743\u8861\u5173\u7cfb\u3002", "conclusion": "\u7b80\u5355\u7684\u6a21\u578b\u5408\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u7f13\u89e3\u5bf9\u9f50\u7a0e\u7684\u5168\u90e8\u5f71\u54cd\uff0c\u4ea7\u751f\u66f4\u5f3a\u5927\u4e14\u66f4\u53ef\u9760\u7684\u6a21\u578b\u3002"}}
{"id": "2510.16306", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16306", "abs": "https://arxiv.org/abs/2510.16306", "authors": ["Xin Wang", "Yu Wang", "Yunchao Liu", "Jens Meiler", "Tyler Derr"], "title": "Scaffold-Aware Generative Augmentation and Reranking for Enhanced Virtual Screening", "comment": null, "summary": "Ligand-based virtual screening (VS) is an essential step in drug discovery\nthat evaluates large chemical libraries to identify compounds that potentially\nbind to a therapeutic target. However, VS faces three major challenges: class\nimbalance due to the low active rate, structural imbalance among active\nmolecules where certain scaffolds dominate, and the need to identify\nstructurally diverse active compounds for novel drug development. We introduce\nScaffAug, a scaffold-aware VS framework that addresses these challenges through\nthree modules. The augmentation module first generates synthetic data\nconditioned on scaffolds of actual hits using generative AI, specifically a\ngraph diffusion model. This helps mitigate the class imbalance and furthermore\nthe structural imbalance, due to our proposed scaffold-aware sampling\nalgorithm, designed to produce more samples for active molecules with\nunderrepresented scaffolds. A model-agnostic self-training module is then used\nto safely integrate the generated synthetic data from our augmentation module\nwith the original labeled data. Lastly, we introduce a reranking module that\nimproves VS by enhancing scaffold diversity in the top recommended set of\nmolecules, while still maintaining and even enhancing the overall general\nperformance of identifying novel, active compounds. We conduct comprehensive\ncomputational experiments across five target classes, comparing ScaffAug\nagainst existing baseline methods by reporting the performance of multiple\nevaluation metrics and performing ablation studies on ScaffAug. Overall, this\nwork introduces novel perspectives on effectively enhancing VS by leveraging\ngenerative augmentations, reranking, and general scaffold-awareness.", "AI": {"tldr": "ScaffAug\u662f\u4e00\u4e2a\u57fa\u4e8e\u652f\u67b6\u7684\u865a\u62df\u7b5b\u9009\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u5f0fAI\u589e\u5f3a\u6570\u636e\u3001\u81ea\u8bad\u7ec3\u548c\u91cd\u6392\u5e8f\u6a21\u5757\uff0c\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u3001\u7ed3\u6784\u4e0d\u5e73\u8861\u548c\u652f\u67b6\u591a\u6837\u6027\u95ee\u9898\u3002", "motivation": "\u865a\u62df\u7b5b\u9009\u9762\u4e34\u4e09\u5927\u6311\u6218\uff1a\u6d3b\u6027\u5316\u5408\u7269\u6bd4\u4f8b\u4f4e\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u3001\u67d0\u4e9b\u652f\u67b6\u5360\u4e3b\u5bfc\u5730\u4f4d\u7684\u7ed3\u6784\u4e0d\u5e73\u8861\uff0c\u4ee5\u53ca\u9700\u8981\u8bc6\u522b\u7ed3\u6784\u591a\u6837\u7684\u6d3b\u6027\u5316\u5408\u7269\u7528\u4e8e\u65b0\u836f\u5f00\u53d1\u3002", "method": "1. \u589e\u5f3a\u6a21\u5757\uff1a\u4f7f\u7528\u56fe\u6269\u6563\u6a21\u578b\u57fa\u4e8e\u5b9e\u9645\u547d\u4e2d\u5316\u5408\u7269\u7684\u652f\u67b6\u751f\u6210\u5408\u6210\u6570\u636e\uff1b2. \u81ea\u8bad\u7ec3\u6a21\u5757\uff1a\u5b89\u5168\u6574\u5408\u751f\u6210\u6570\u636e\u4e0e\u539f\u59cb\u6570\u636e\uff1b3. \u91cd\u6392\u5e8f\u6a21\u5757\uff1a\u63d0\u9ad8\u63a8\u8350\u5206\u5b50\u96c6\u4e2d\u7684\u652f\u67b6\u591a\u6837\u6027\u3002", "result": "\u5728\u4e94\u4e2a\u9776\u70b9\u7c7b\u522b\u4e0a\u7684\u8ba1\u7b97\u5b9e\u9a8c\u8868\u660e\uff0cScaffAug\u5728\u591a\u4e2a\u8bc4\u4f30\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u652f\u67b6\u591a\u6837\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u901a\u8fc7\u751f\u6210\u5f0f\u589e\u5f3a\u3001\u91cd\u6392\u5e8f\u548c\u652f\u67b6\u611f\u77e5\uff0c\u4e3a\u6709\u6548\u589e\u5f3a\u865a\u62df\u7b5b\u9009\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2510.17431", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17431", "abs": "https://arxiv.org/abs/2510.17431", "authors": ["Yushi Yang", "Shreyansh Padarha", "Andrew Lee", "Adam Mahdi"], "title": "Agentic Reinforcement Learning for Search is Unsafe", "comment": null, "summary": "Agentic reinforcement learning (RL) trains large language models to\nautonomously call tools during reasoning, with search as the most common\napplication. These models excel at multi-step reasoning tasks, but their safety\nproperties are not well understood. In this study, we show that RL-trained\nsearch models inherit refusal from instruction tuning and often deflect harmful\nrequests by turning them into safe queries. However, this safety is fragile.\nTwo simple attacks, one that forces the model to begin response with search\n(Search attack), another that encourages models to repeatedly search\n(Multi-search attack), trigger cascades of harmful searches and answers. Across\ntwo model families (Qwen, Llama) with both local and web search, these attacks\nlower refusal rates by up to 60.0%, answer safety by 82.5%, and search-query\nsafety by 82.4%. The attacks succeed by triggering models to generate harmful,\nrequest-mirroring search queries before they can generate the inherited refusal\ntokens. This exposes a core weakness of current RL training: it rewards\ncontinued generation of effective queries without accounting for their\nharmfulness. As a result, RL search models have vulnerabilities that users can\neasily exploit, making it urgent to develop safety-aware agentic RL pipelines\noptimising for safe search.", "AI": {"tldr": "RL\u8bad\u7ec3\u7684\u641c\u7d22\u6a21\u578b\u867d\u7136\u7ee7\u627f\u4e86\u6307\u4ee4\u8c03\u4f18\u7684\u62d2\u7edd\u80fd\u529b\uff0c\u4f46\u5b58\u5728\u8106\u5f31\u6027\u3002\u4e24\u79cd\u7b80\u5355\u653b\u51fb\uff08\u641c\u7d22\u653b\u51fb\u548c\u591a\u641c\u7d22\u653b\u51fb\uff09\u80fd\u663e\u8457\u964d\u4f4e\u6a21\u578b\u7684\u5b89\u5168\u6027\u80fd\uff0c\u66b4\u9732\u4e86\u5f53\u524dRL\u8bad\u7ec3\u5728\u5b89\u5168\u6027\u65b9\u9762\u7684\u6838\u5fc3\u5f31\u70b9\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u7406\u89e3\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u8bad\u7ec3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u4e3b\u8c03\u7528\u5de5\u5177\u65f6\u7684\u5b89\u5168\u7279\u6027\uff0c\u7279\u522b\u662f\u641c\u7d22\u5e94\u7528\u4e2d\u7684\u5b89\u5168\u6027\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u4e24\u79cd\u7b80\u5355\u653b\u51fb\u65b9\u6cd5\u6d4b\u8bd5RL\u8bad\u7ec3\u7684\u641c\u7d22\u6a21\u578b\uff1a\u641c\u7d22\u653b\u51fb\uff08\u5f3a\u5236\u6a21\u578b\u4ee5\u641c\u7d22\u5f00\u59cb\u54cd\u5e94\uff09\u548c\u591a\u641c\u7d22\u653b\u51fb\uff08\u9f13\u52b1\u6a21\u578b\u91cd\u590d\u641c\u7d22\uff09\uff0c\u5728Qwen\u548cLlama\u4e24\u4e2a\u6a21\u578b\u5bb6\u65cf\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u653b\u51fb\u663e\u8457\u964d\u4f4e\u4e86\u6a21\u578b\u7684\u5b89\u5168\u6027\u80fd\uff1a\u62d2\u7edd\u7387\u964d\u4f4e\u8fbe60.0%\uff0c\u7b54\u6848\u5b89\u5168\u6027\u964d\u4f4e82.5%\uff0c\u641c\u7d22\u67e5\u8be2\u5b89\u5168\u6027\u964d\u4f4e82.4%\u3002\u653b\u51fb\u901a\u8fc7\u89e6\u53d1\u6a21\u578b\u751f\u6210\u6709\u5bb3\u7684\u3001\u955c\u50cf\u8bf7\u6c42\u7684\u641c\u7d22\u67e5\u8be2\u6765\u7ed5\u8fc7\u7ee7\u627f\u7684\u62d2\u7edd\u673a\u5236\u3002", "conclusion": "\u5f53\u524dRL\u8bad\u7ec3\u5b58\u5728\u6838\u5fc3\u5f31\u70b9\uff1a\u5956\u52b1\u751f\u6210\u6709\u6548\u67e5\u8be2\u800c\u4e0d\u8003\u8651\u5176\u5371\u5bb3\u6027\uff0c\u5bfc\u81f4RL\u641c\u7d22\u6a21\u578b\u5b58\u5728\u6613\u88ab\u7528\u6237\u5229\u7528\u7684\u6f0f\u6d1e\uff0c\u8feb\u5207\u9700\u8981\u5f00\u53d1\u5b89\u5168\u6027\u611f\u77e5\u7684\u667a\u80fd\u4f53RL\u7ba1\u9053\u6765\u4f18\u5316\u5b89\u5168\u641c\u7d22\u3002"}}
{"id": "2510.16311", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16311", "abs": "https://arxiv.org/abs/2510.16311", "authors": ["Daohan Su", "Yang Zhang", "Xunkai Li", "Rong-Hua Li", "Guoren Wang"], "title": "Toward General Digraph Contrastive Learning: A Dual Spatial Perspective", "comment": null, "summary": "Graph Contrastive Learning (GCL) has emerged as a powerful tool for\nextracting consistent representations from graphs, independent of labeled\ninformation. However, existing methods predominantly focus on undirected\ngraphs, disregarding the pivotal directional information that is fundamental\nand indispensable in real-world networks (e.g., social networks and\nrecommendations).In this paper, we introduce S2-DiGCL, a novel framework that\nemphasizes spatial insights from complex and real domain perspectives for\ndirected graph (digraph) contrastive learning. From the complex-domain\nperspective, S2-DiGCL introduces personalized perturbations into the magnetic\nLaplacian to adaptively modulate edge phases and directional semantics. From\nthe real-domain perspective, it employs a path-based subgraph augmentation\nstrategy to capture fine-grained local asymmetries and topological\ndependencies. By jointly leveraging these two complementary spatial views,\nS2-DiGCL constructs high-quality positive and negative samples, leading to more\ngeneral and robust digraph contrastive learning. Extensive experiments on 7\nreal-world digraph datasets demonstrate the superiority of our approach,\nachieving SOTA performance with 4.41% improvement in node classification and\n4.34% in link prediction under both supervised and unsupervised settings.", "AI": {"tldr": "S2-DiGCL\u662f\u4e00\u4e2a\u9488\u5bf9\u6709\u5411\u56fe\u7684\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u590d\u6570\u57df\u548c\u5b9e\u6570\u57df\u7684\u53cc\u91cd\u89c6\u89d2\u6765\u6355\u6349\u65b9\u5411\u4fe1\u606f\uff0c\u5728\u8282\u70b9\u5206\u7c7b\u548c\u94fe\u63a5\u9884\u6d4b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u65e0\u5411\u56fe\uff0c\u5ffd\u7565\u4e86\u73b0\u5b9e\u7f51\u7edc\uff08\u5982\u793e\u4ea4\u7f51\u7edc\u548c\u63a8\u8350\u7cfb\u7edf\uff09\u4e2d\u81f3\u5173\u91cd\u8981\u7684\u65b9\u5411\u4fe1\u606f\u3002", "method": "\u4ece\u590d\u6570\u57df\u89c6\u89d2\uff0c\u5728\u78c1\u62c9\u666e\u62c9\u65af\u77e9\u9635\u4e2d\u5f15\u5165\u4e2a\u6027\u5316\u6270\u52a8\u6765\u8c03\u5236\u8fb9\u76f8\u4f4d\u548c\u65b9\u5411\u8bed\u4e49\uff1b\u4ece\u5b9e\u6570\u57df\u89c6\u89d2\uff0c\u4f7f\u7528\u57fa\u4e8e\u8def\u5f84\u7684\u5b50\u56fe\u589e\u5f3a\u7b56\u7565\u6765\u6355\u6349\u7ec6\u7c92\u5ea6\u5c40\u90e8\u4e0d\u5bf9\u79f0\u6027\u548c\u62d3\u6251\u4f9d\u8d56\u3002", "result": "\u57287\u4e2a\u771f\u5b9e\u4e16\u754c\u6709\u5411\u56fe\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8282\u70b9\u5206\u7c7b\u548c\u94fe\u63a5\u9884\u6d4b\u4efb\u52a1\u4e2d\u5206\u522b\u5b9e\u73b0\u4e864.41%\u548c4.34%\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "S2-DiGCL\u901a\u8fc7\u7ed3\u5408\u590d\u6570\u57df\u548c\u5b9e\u6570\u57df\u7684\u53cc\u91cd\u89c6\u89d2\uff0c\u80fd\u591f\u6784\u5efa\u66f4\u9ad8\u8d28\u91cf\u7684\u6b63\u8d1f\u6837\u672c\uff0c\u5b9e\u73b0\u66f4\u901a\u7528\u548c\u9c81\u68d2\u7684\u6709\u5411\u56fe\u5bf9\u6bd4\u5b66\u4e60\u3002"}}
{"id": "2510.17437", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17437", "abs": "https://arxiv.org/abs/2510.17437", "authors": ["Manuela Daniela Danu", "George Marica", "Constantin Suciu", "Lucian Mihai Itu", "Oladimeji Farri"], "title": "Multilingual Clinical NER for Diseases and Medications Recognition in Cardiology Texts using BERT Embeddings", "comment": "11 pages, 5 figures, 1 table, published in Working Notes of the\n  Conference and Labs of the Evaluation Forum (CLEF 2024)", "summary": "The rapidly increasing volume of electronic health record (EHR) data\nunderscores a pressing need to unlock biomedical knowledge from unstructured\nclinical texts to support advancements in data-driven clinical systems,\nincluding patient diagnosis, disease progression monitoring, treatment effects\nassessment, prediction of future clinical events, etc. While contextualized\nlanguage models have demonstrated impressive performance improvements for named\nentity recognition (NER) systems in English corpora, there remains a scarcity\nof research focused on clinical texts in low-resource languages. To bridge this\ngap, our study aims to develop multiple deep contextual embedding models to\nenhance clinical NER in the cardiology domain, as part of the BioASQ\nMultiCardioNER shared task. We explore the effectiveness of different\nmonolingual and multilingual BERT-based models, trained on general domain text,\nfor extracting disease and medication mentions from clinical case reports\nwritten in English, Spanish, and Italian. We achieved an F1-score of 77.88% on\nSpanish Diseases Recognition (SDR), 92.09% on Spanish Medications Recognition\n(SMR), 91.74% on English Medications Recognition (EMR), and 88.9% on Italian\nMedications Recognition (IMR). These results outperform the mean and median F1\nscores in the test leaderboard across all subtasks, with the mean/median values\nbeing: 69.61%/75.66% for SDR, 81.22%/90.18% for SMR, 89.2%/88.96% for EMR, and\n82.8%/87.76% for IMR.", "AI": {"tldr": "\u5f00\u53d1\u57fa\u4e8eBERT\u7684\u4e34\u5e8a\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u6a21\u578b\uff0c\u7528\u4e8e\u4ece\u82f1\u8bed\u3001\u897f\u73ed\u7259\u8bed\u548c\u610f\u5927\u5229\u8bed\u7684\u5fc3\u810f\u75c5\u5b66\u4e34\u5e8a\u6587\u672c\u4e2d\u63d0\u53d6\u75be\u75c5\u548c\u836f\u7269\u5b9e\u4f53\uff0c\u5728BioASQ MultiCardioNER\u4efb\u52a1\u4e2d\u53d6\u5f97\u4f18\u4e8e\u5e73\u5747\u6c34\u5e73\u7684\u6027\u80fd\u3002", "motivation": "\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\u5feb\u901f\u589e\u957f\uff0c\u9700\u8981\u4ece\u975e\u7ed3\u6784\u5316\u4e34\u5e8a\u6587\u672c\u4e2d\u63d0\u53d6\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u6765\u652f\u6301\u4e34\u5e8a\u7cfb\u7edf\u53d1\u5c55\u3002\u867d\u7136\u82f1\u8bed\u4e34\u5e8aNER\u7814\u7a76\u8f83\u591a\uff0c\u4f46\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u4e34\u5e8a\u6587\u672c\u7814\u7a76\u4ecd\u7136\u7a00\u7f3a\u3002", "method": "\u63a2\u7d22\u5355\u8bed\u548c\u591a\u8bed\u8a00BERT\u6a21\u578b\u5728\u5fc3\u810f\u75c5\u5b66\u4e34\u5e8a\u6587\u672c\u4e2d\u7684\u5e94\u7528\uff0c\u8bad\u7ec3\u7528\u4e8e\u4ece\u82f1\u8bed\u3001\u897f\u73ed\u7259\u8bed\u548c\u610f\u5927\u5229\u8bed\u7684\u4e34\u5e8a\u75c5\u4f8b\u62a5\u544a\u4e2d\u63d0\u53d6\u75be\u75c5\u548c\u836f\u7269\u5b9e\u4f53\u3002", "result": "\u5728\u897f\u73ed\u7259\u8bed\u75be\u75c5\u8bc6\u522b(SDR)\u4e0aF1\u5f97\u5206\u4e3a77.88%\uff0c\u897f\u73ed\u7259\u8bed\u836f\u7269\u8bc6\u522b(SMR)\u4e3a92.09%\uff0c\u82f1\u8bed\u836f\u7269\u8bc6\u522b(EMR)\u4e3a91.74%\uff0c\u610f\u5927\u5229\u8bed\u836f\u7269\u8bc6\u522b(IMR)\u4e3a88.9%\uff0c\u5747\u8d85\u8fc7\u6d4b\u8bd5\u6392\u884c\u699c\u7684\u5e73\u5747\u503c\u548c\u4e2d\u4f4d\u6570\u3002", "conclusion": "\u57fa\u4e8eBERT\u7684\u4e0a\u4e0b\u6587\u5d4c\u5165\u6a21\u578b\u80fd\u591f\u6709\u6548\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e34\u5e8a\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u7684\u6027\u80fd\uff0c\u4e3a\u591a\u8bed\u8a00\u4e34\u5e8a\u6587\u672c\u5206\u6790\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2510.16322", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16322", "abs": "https://arxiv.org/abs/2510.16322", "authors": ["Mo Zhou", "Haoyang Ma", "Rong Ge"], "title": "Memorizing Long-tail Data Can Help Generalization Through Composition", "comment": "30 pages", "summary": "Deep learning has led researchers to rethink the relationship between\nmemorization and generalization. In many settings, memorization does not hurt\ngeneralization due to implicit regularization and may help by memorizing\nlong-tailed examples. In this paper, we consider the synergy between\nmemorization and simple composition -- the ability to make correct prediction\non a combination of long-tailed features. Theoretically, we show that for a\nlinear setting, memorization together with composition can help the model make\ncorrect predictions on rare test examples that require a combination of\nlong-tailed features, even if such combinations were never observed in the\ntraining data. Experiments on neural network architecture on simple data show\nthat the theoretical insight extends beyond the linear setting, and we further\nobserve that the composition capability of the model depends on its\narchitecture.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u8bb0\u5fc6\u5316\u4e0e\u7b80\u5355\u7ec4\u5408\u80fd\u529b\u4e4b\u95f4\u7684\u534f\u540c\u4f5c\u7528\uff0c\u53d1\u73b0\u5728\u7ebf\u6027\u8bbe\u7f6e\u4e2d\uff0c\u8bb0\u5fc6\u5316\u7ed3\u5408\u7ec4\u5408\u80fd\u529b\u53ef\u4ee5\u5e2e\u52a9\u6a21\u578b\u5bf9\u9700\u8981\u957f\u5c3e\u7279\u5f81\u7ec4\u5408\u7684\u7f55\u89c1\u6d4b\u8bd5\u6837\u672c\u505a\u51fa\u6b63\u786e\u9884\u6d4b\uff0c\u5373\u4f7f\u8bad\u7ec3\u6570\u636e\u4e2d\u4ece\u672a\u51fa\u73b0\u8fc7\u8fd9\u79cd\u7ec4\u5408\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u4fc3\u4f7f\u7814\u7a76\u8005\u91cd\u65b0\u601d\u8003\u8bb0\u5fc6\u5316\u4e0e\u6cdb\u5316\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u5728\u8bb8\u591a\u60c5\u51b5\u4e0b\uff0c\u8bb0\u5fc6\u5316\u4e0d\u4f1a\u635f\u5bb3\u6cdb\u5316\uff0c\u53cd\u800c\u53ef\u80fd\u901a\u8fc7\u8bb0\u5fc6\u957f\u5c3e\u6837\u672c\u6765\u5e2e\u52a9\u6cdb\u5316\u3002\u672c\u6587\u7814\u7a76\u8bb0\u5fc6\u5316\u4e0e\u7b80\u5355\u7ec4\u5408\u80fd\u529b\u4e4b\u95f4\u7684\u534f\u540c\u4f5c\u7528\u3002", "method": "\u5728\u7ebf\u6027\u8bbe\u7f6e\u4e2d\u8fdb\u884c\u7406\u8bba\u5206\u6790\uff0c\u5e76\u5728\u7b80\u5355\u6570\u636e\u4e0a\u5bf9\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u8bb0\u5fc6\u5316\u4e0e\u7ec4\u5408\u80fd\u529b\u7ed3\u5408\u53ef\u4ee5\u5e2e\u52a9\u6a21\u578b\u5bf9\u9700\u8981\u957f\u5c3e\u7279\u5f81\u7ec4\u5408\u7684\u7f55\u89c1\u6d4b\u8bd5\u6837\u672c\u505a\u51fa\u6b63\u786e\u9884\u6d4b\u3002\u5b9e\u9a8c\u8bc1\u5b9e\u8fd9\u4e00\u7406\u8bba\u6d1e\u5bdf\u9002\u7528\u4e8e\u975e\u7ebf\u6027\u8bbe\u7f6e\uff0c\u5e76\u53d1\u73b0\u6a21\u578b\u7684\u7ec4\u5408\u80fd\u529b\u53d6\u51b3\u4e8e\u5176\u67b6\u6784\u3002", "conclusion": "\u8bb0\u5fc6\u5316\u4e0e\u7ec4\u5408\u80fd\u529b\u4e4b\u95f4\u5b58\u5728\u534f\u540c\u4f5c\u7528\uff0c\u8fd9\u79cd\u534f\u540c\u53ef\u4ee5\u5e2e\u52a9\u6a21\u578b\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u957f\u5c3e\u7279\u5f81\u7ec4\u5408\uff0c\u4e14\u6a21\u578b\u7684\u67b6\u6784\u5f71\u54cd\u5176\u7ec4\u5408\u80fd\u529b\u3002"}}
{"id": "2510.17460", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17460", "abs": "https://arxiv.org/abs/2510.17460", "authors": ["Muhammad Farmal Khan", "Mousumi Akter"], "title": "Evaluating Large Language Models on Urdu Idiom Translation", "comment": null, "summary": "Idiomatic translation remains a significant challenge in machine translation,\nespecially for low resource languages such as Urdu, and has received limited\nprior attention. To advance research in this area, we introduce the first\nevaluation datasets for Urdu to English idiomatic translation, covering both\nNative Urdu and Roman Urdu scripts and annotated with gold-standard English\nequivalents. We evaluate multiple open-source Large Language Models (LLMs) and\nNeural Machine Translation (NMT) systems on this task, focusing on their\nability to preserve idiomatic and cultural meaning. Automatic metrics including\nBLEU, BERTScore, COMET, and XCOMET are used to assess translation quality. Our\nfindings indicate that prompt engineering enhances idiomatic translation\ncompared to direct translation, though performance differences among prompt\ntypes are relatively minor. Moreover, cross script comparisons reveal that text\nrepresentation substantially affects translation quality, with Native Urdu\ninputs producing more accurate idiomatic translations than Roman Urdu.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4e3a\u4e4c\u5c14\u90fd\u8bed\u5230\u82f1\u8bed\u7684\u4e60\u8bed\u7ffb\u8bd1\u521b\u5efa\u4e86\u9996\u4e2a\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86\u591a\u79cdLLM\u548cNMT\u7cfb\u7edf\uff0c\u53d1\u73b0\u63d0\u793a\u5de5\u7a0b\u80fd\u63d0\u5347\u4e60\u8bed\u7ffb\u8bd1\u8d28\u91cf\uff0c\u4e14\u539f\u751f\u4e4c\u5c14\u90fd\u8bed\u6587\u672c\u6bd4\u7f57\u9a6c\u5316\u6587\u672c\u7ffb\u8bd1\u66f4\u51c6\u786e\u3002", "motivation": "\u4e60\u8bed\u7ffb\u8bd1\u5728\u673a\u5668\u7ffb\u8bd1\u4e2d\u4ecd\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u4e4c\u5c14\u90fd\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u6b64\u524d\u7814\u7a76\u5173\u6ce8\u6709\u9650\u3002", "method": "\u521b\u5efa\u4e4c\u5c14\u90fd\u8bed\u5230\u82f1\u8bed\u4e60\u8bed\u7ffb\u8bd1\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u591a\u79cd\u5f00\u6e90LLM\u548cNMT\u7cfb\u7edf\uff0c\u4f7f\u7528BLEU\u3001BERTScore\u3001COMET\u548cXCOMET\u7b49\u81ea\u52a8\u6307\u6807\u8bc4\u4f30\u7ffb\u8bd1\u8d28\u91cf\u3002", "result": "\u63d0\u793a\u5de5\u7a0b\u76f8\u6bd4\u76f4\u63a5\u7ffb\u8bd1\u80fd\u63d0\u5347\u4e60\u8bed\u7ffb\u8bd1\u8d28\u91cf\uff0c\u4f46\u4e0d\u540c\u63d0\u793a\u7c7b\u578b\u95f4\u7684\u6027\u80fd\u5dee\u5f02\u8f83\u5c0f\uff1b\u539f\u751f\u4e4c\u5c14\u90fd\u8bed\u8f93\u5165\u7684\u4e60\u8bed\u7ffb\u8bd1\u6bd4\u7f57\u9a6c\u5316\u4e4c\u5c14\u90fd\u8bed\u66f4\u51c6\u786e\u3002", "conclusion": "\u6587\u672c\u8868\u793a\u5bf9\u7ffb\u8bd1\u8d28\u91cf\u6709\u663e\u8457\u5f71\u54cd\uff0c\u539f\u751f\u4e4c\u5c14\u90fd\u8bed\u5728\u4e60\u8bed\u7ffb\u8bd1\u4e0a\u8868\u73b0\u4f18\u4e8e\u7f57\u9a6c\u5316\u7248\u672c\uff0c\u63d0\u793a\u5de5\u7a0b\u662f\u63d0\u5347\u4e60\u8bed\u7ffb\u8bd1\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2510.16350", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16350", "abs": "https://arxiv.org/abs/2510.16350", "authors": ["Shule Hao", "Junpeng Bao", "Wenli Li"], "title": "MGTS-Net: Exploring Graph-Enhanced Multimodal Fusion for Augmented Time Series Forecasting", "comment": null, "summary": "Recent research in time series forecasting has explored integrating\nmultimodal features into models to improve accuracy. However, the accuracy of\nsuch methods is constrained by three key challenges: inadequate extraction of\nfine-grained temporal patterns, suboptimal integration of multimodal\ninformation, and limited adaptability to dynamic multi-scale features. To\naddress these problems, we propose MGTS-Net, a Multimodal Graph-enhanced\nNetwork for Time Series forecasting. The model consists of three core\ncomponents: (1) a Multimodal Feature Extraction layer (MFE), which optimizes\nfeature encoders according to the characteristics of temporal, visual, and\ntextual modalities to extract temporal features of fine-grained patterns; (2) a\nMultimodal Feature Fusion layer (MFF), which constructs a heterogeneous graph\nto model intra-modal temporal dependencies and cross-modal alignment\nrelationships and dynamically aggregates multimodal knowledge; (3) a\nMulti-Scale Prediction layer (MSP), which adapts to multi-scale features by\ndynamically weighting and fusing the outputs of short-term, medium-term, and\nlong-term predictors. Extensive experiments demonstrate that MGTS-Net exhibits\nexcellent performance with light weight and high efficiency. Compared with\nother state-of-the-art baseline models, our method achieves superior\nperformance, validating the superiority of the proposed methodology.", "AI": {"tldr": "\u63d0\u51fa\u4e86MGTS-Net\uff0c\u4e00\u79cd\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u591a\u6a21\u6001\u56fe\u589e\u5f3a\u7f51\u7edc\uff0c\u901a\u8fc7\u4f18\u5316\u591a\u6a21\u6001\u7279\u5f81\u63d0\u53d6\u3001\u6784\u5efa\u5f02\u6784\u56fe\u878d\u5408\u591a\u6a21\u6001\u4fe1\u606f\u3001\u4ee5\u53ca\u52a8\u6001\u591a\u5c3a\u5ea6\u9884\u6d4b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7ec6\u7c92\u5ea6\u65f6\u5e8f\u6a21\u5f0f\u63d0\u53d6\u3001\u591a\u6a21\u6001\u4fe1\u606f\u878d\u5408\u548c\u52a8\u6001\u591a\u5c3a\u5ea6\u7279\u5f81\u9002\u5e94\u65b9\u9762\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\u5728\u6574\u5408\u591a\u6a21\u6001\u7279\u5f81\u65f6\u9762\u4e34\u4e09\u4e2a\u5173\u952e\u6311\u6218\uff1a\u7ec6\u7c92\u5ea6\u65f6\u5e8f\u6a21\u5f0f\u63d0\u53d6\u4e0d\u8db3\u3001\u591a\u6a21\u6001\u4fe1\u606f\u878d\u5408\u6548\u679c\u4e0d\u4f73\u3001\u4ee5\u53ca\u5bf9\u52a8\u6001\u591a\u5c3a\u5ea6\u7279\u5f81\u7684\u9002\u5e94\u80fd\u529b\u6709\u9650\u3002", "method": "MGTS-Net\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a(1)\u591a\u6a21\u6001\u7279\u5f81\u63d0\u53d6\u5c42\uff0c\u9488\u5bf9\u65f6\u5e8f\u3001\u89c6\u89c9\u548c\u6587\u672c\u6a21\u6001\u4f18\u5316\u7279\u5f81\u7f16\u7801\u5668\uff1b(2)\u591a\u6a21\u6001\u7279\u5f81\u878d\u5408\u5c42\uff0c\u6784\u5efa\u5f02\u6784\u56fe\u5efa\u6a21\u6a21\u6001\u5185\u65f6\u5e8f\u4f9d\u8d56\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\u5173\u7cfb\uff1b(3)\u591a\u5c3a\u5ea6\u9884\u6d4b\u5c42\uff0c\u52a8\u6001\u52a0\u6743\u878d\u5408\u77ed\u671f\u3001\u4e2d\u671f\u548c\u957f\u671f\u9884\u6d4b\u5668\u8f93\u51fa\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660eMGTS-Net\u5728\u8f7b\u91cf\u7ea7\u548c\u9ad8\u6548\u7387\u4e0b\u8868\u73b0\u51fa\u4f18\u5f02\u6027\u80fd\uff0c\u76f8\u6bd4\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u6a21\u578b\u53d6\u5f97\u4e86\u66f4\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\uff0c\u4e3a\u591a\u6a21\u6001\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.17476", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17476", "abs": "https://arxiv.org/abs/2510.17476", "authors": ["Ipek Baris Schlicht", "Burcu Sayin", "Zhixue Zhao", "Frederik M. Labont\u00e9", "Cesare Barbera", "Marco Viviani", "Paolo Rosso", "Lucie Flek"], "title": "Disparities in Multilingual LLM-Based Healthcare Q&A", "comment": "Under review", "summary": "Equitable access to reliable health information is vital when integrating AI\ninto healthcare. Yet, information quality varies across languages, raising\nconcerns about the reliability and consistency of multilingual Large Language\nModels (LLMs). We systematically examine cross-lingual disparities in\npre-training source and factuality alignment in LLM answers for multilingual\nhealthcare Q&A across English, German, Turkish, Chinese (Mandarin), and\nItalian. We (i) constructed Multilingual Wiki Health Care\n(MultiWikiHealthCare), a multilingual dataset from Wikipedia; (ii) analyzed\ncross-lingual healthcare coverage; (iii) assessed LLM response alignment with\nthese references; and (iv) conducted a case study on factual alignment through\nthe use of contextual information and Retrieval-Augmented Generation (RAG). Our\nfindings reveal substantial cross-lingual disparities in both Wikipedia\ncoverage and LLM factual alignment. Across LLMs, responses align more with\nEnglish Wikipedia, even when the prompts are non-English. Providing contextual\nexcerpts from non-English Wikipedia at inference time effectively shifts\nfactual alignment toward culturally relevant knowledge. These results highlight\npractical pathways for building more equitable, multilingual AI systems for\nhealthcare.", "AI": {"tldr": "\u8be5\u7814\u7a76\u7cfb\u7edf\u8bc4\u4f30\u4e86\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u95ee\u7b54\u4e2d\u7684\u8de8\u8bed\u8a00\u4e8b\u5b9e\u5bf9\u9f50\u5dee\u5f02\uff0c\u53d1\u73b0\u82f1\u8bed\u7ef4\u57fa\u767e\u79d1\u8986\u76d6\u5ea6\u548c\u6a21\u578b\u56de\u7b54\u4e00\u81f4\u6027\u660e\u663e\u4f18\u4e8e\u5176\u4ed6\u8bed\u8a00\uff0c\u4f46\u901a\u8fc7\u63d0\u4f9b\u975e\u82f1\u8bed\u4e0a\u4e0b\u6587\u53ef\u4ee5\u6709\u6548\u6539\u5584\u4e8b\u5b9e\u5bf9\u9f50\u3002", "motivation": "\u5c06AI\u6574\u5408\u5230\u533b\u7597\u4fdd\u5065\u4e2d\u9700\u8981\u516c\u5e73\u83b7\u53d6\u53ef\u9760\u5065\u5eb7\u4fe1\u606f\uff0c\u4f46\u4e0d\u540c\u8bed\u8a00\u7684\u4fe1\u606f\u8d28\u91cf\u5b58\u5728\u5dee\u5f02\uff0c\u5f15\u53d1\u5bf9\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u53ef\u9760\u6027\u548c\u4e00\u81f4\u6027\u7684\u62c5\u5fe7\u3002", "method": "\u6784\u5efa\u591a\u8bed\u8a00\u7ef4\u57fa\u533b\u7597\u6570\u636e\u96c6\uff0c\u5206\u6790\u8de8\u8bed\u8a00\u533b\u7597\u8986\u76d6\u5ea6\uff0c\u8bc4\u4f30LLM\u56de\u7b54\u4e0e\u53c2\u8003\u4fe1\u606f\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u901a\u8fc7\u4e0a\u4e0b\u6587\u4fe1\u606f\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u8fdb\u884c\u6848\u4f8b\u7814\u7a76\u3002", "result": "\u53d1\u73b0\u7ef4\u57fa\u767e\u79d1\u8986\u76d6\u5ea6\u548cLLM\u4e8b\u5b9e\u5bf9\u9f50\u5b58\u5728\u663e\u8457\u8de8\u8bed\u8a00\u5dee\u5f02\uff0c\u6a21\u578b\u56de\u7b54\u66f4\u503e\u5411\u4e8e\u4e0e\u82f1\u8bed\u7ef4\u57fa\u767e\u79d1\u5bf9\u9f50\uff0c\u5373\u4f7f\u63d0\u793a\u4e3a\u975e\u82f1\u8bed\u3002\u63d0\u4f9b\u975e\u82f1\u8bed\u7ef4\u57fa\u767e\u79d1\u4e0a\u4e0b\u6587\u80fd\u6709\u6548\u5c06\u4e8b\u5b9e\u5bf9\u9f50\u8f6c\u5411\u6587\u5316\u76f8\u5173\u77e5\u8bc6\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u6784\u5efa\u66f4\u516c\u5e73\u7684\u591a\u8bed\u8a00\u533b\u7597AI\u7cfb\u7edf\u7684\u5b9e\u9645\u8def\u5f84\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u5e72\u9884\u53ef\u4ee5\u6539\u5584\u975e\u82f1\u8bed\u8bed\u8a00\u7684\u4e8b\u5b9e\u5bf9\u9f50\u8d28\u91cf\u3002"}}
{"id": "2510.17483", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17483", "abs": "https://arxiv.org/abs/2510.17483", "authors": ["Zheyue Tan", "Zhiyuan Li", "Tao Yuan", "Dong Zhou", "Weilin Liu", "Yueqing Zhuang", "Yadong Li", "Guowei Niu", "Cheng Qin", "Zhuyu Yao", "Congyi Liu", "Haiyang Xu", "Boxun Li", "Guohao Dai", "Bo Zhao", "Yu Wang"], "title": "ReXMoE: Reusing Experts with Minimal Overhead in Mixture-of-Experts", "comment": null, "summary": "Mixture-of-Experts (MoE) architectures have emerged as a promising approach\nto scale Large Language Models (LLMs). MoE boosts the efficiency by activating\na subset of experts per token. Recent works show that fine-grained experts\nsubstantially enriches the combinatorial flexibility of active experts and\nenhances model expressiveness. However, such a design is fundamentally limited\nby the layer-local routing mechanism: each layer is restricted to its own\nexpert pool. This requires a careful trade-off between expert dimensionality\nand routing diversity given fixed parameter budgets. We describe ReXMoE, a\nnovel MoE architecture that improves routing beyond the existing layer-local\napproaches by allowing routers to reuse experts across adjacent layers. ReXMoE\ndecouples expert dimensionality from per-layer budgets, enabling richer expert\ncombinations without sacrificing individual expert capacity or inflating\noverall parameters. To this end, we propose a new progressive scaling routing\n(PSR) strategy to gradually increase the candidate expert pool during training.\nAs a result, ReXMoE improves both language modeling and downstream task\nperformance. Extensive experiments on models ranging from 0.5B to 7B parameters\nacross different architectures demonstrate that ReXMoE consistently improves\nperformance under fixed architectural dimensions, confirming ReXMoE as new\ndesign paradigm for parameter-efficient and scalable MoE-based LLMs.", "AI": {"tldr": "ReXMoE\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\uff0c\u901a\u8fc7\u8de8\u5c42\u91cd\u7528\u4e13\u5bb6\u6765\u6539\u8fdb\u8def\u7531\u673a\u5236\uff0c\u5728\u56fa\u5b9a\u53c2\u6570\u9884\u7b97\u4e0b\u63d0\u5347\u6a21\u578b\u8868\u8fbe\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7ec6\u7c92\u5ea6\u4e13\u5bb6\u67b6\u6784\u53d7\u9650\u4e8e\u5c42\u5c40\u90e8\u8def\u7531\u673a\u5236\uff0c\u9700\u8981\u5728\u4e13\u5bb6\u7ef4\u5ea6\u548c\u8def\u7531\u591a\u6837\u6027\u4e4b\u95f4\u6743\u8861\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u8868\u8fbe\u80fd\u529b\u3002", "method": "\u63d0\u51faReXMoE\u67b6\u6784\uff0c\u5141\u8bb8\u8def\u7531\u5668\u8de8\u76f8\u90bb\u5c42\u91cd\u7528\u4e13\u5bb6\uff0c\u89e3\u8026\u4e13\u5bb6\u7ef4\u5ea6\u4e0e\u6bcf\u5c42\u9884\u7b97\uff0c\u5e76\u91c7\u7528\u6e10\u8fdb\u5f0f\u6269\u5c55\u8def\u7531\u7b56\u7565\u9010\u6b65\u589e\u52a0\u5019\u9009\u4e13\u5bb6\u6c60\u3002", "result": "\u57280.5B\u52307B\u53c2\u6570\u89c4\u6a21\u7684\u6a21\u578b\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cReXMoE\u5728\u56fa\u5b9a\u67b6\u6784\u7ef4\u5ea6\u4e0b\u6301\u7eed\u63d0\u5347\u8bed\u8a00\u5efa\u6a21\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "ReXMoE\u4e3a\u53c2\u6570\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684MoE\u57fa\u7840LLMs\u63d0\u4f9b\u4e86\u65b0\u7684\u8bbe\u8ba1\u8303\u5f0f\u3002"}}
{"id": "2510.16411", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16411", "abs": "https://arxiv.org/abs/2510.16411", "authors": ["Minh-Khoi Nguyen-Nhat", "Rachel S. Y. Teo", "Laziz Abdullaev", "Maurice Mok", "Viet-Hoang Tran", "Tan Minh Nguyen"], "title": "Modeling Expert Interactions in Sparse Mixture of Experts via Graph Structures", "comment": null, "summary": "Sparse Mixture of Experts (SMoE) has emerged as a promising solution to\nachieving unparalleled scalability in deep learning by decoupling model\nparameter count from computational cost. By activating only a small subset of\nparameters per sample, SMoE enables significant growth in model capacity while\nmaintaining efficiency. However, SMoE struggles to adapt to distributional\nshifts, leading to reduced robustness under data contamination. In this work,\nwe introduce SymphonySMoE, a novel family of SMoE that introduces a social\ngraph to model interactions among experts. This graph-based structure enhances\nthe token routing process, addressing the robustness challenges that are\ninherent in conventional SMoE designs. SymphonySMoE is lightweight, modular,\nand integrates seamlessly with existing SMoE-based models such as the XMoE and\nthe Generalist Language Model. We provide both theoretical analysis and\nempirical evidence demonstrating SymphonySMoE's advantages over baseline SMoE.\nExtensive experiments on language modeling and visual instruction tuning\nvalidate our method's effectiveness. We further highlight the scalability of\nSymphonySMoE to models with 4.2 and 7.4 billion parameters, showcasing its\napplicability in fine-tuning tasks for large-scale systems.", "AI": {"tldr": "SymphonySMoE\u662f\u4e00\u79cd\u65b0\u578b\u7684\u7a00\u758f\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\uff0c\u901a\u8fc7\u5f15\u5165\u4e13\u5bb6\u95f4\u7684\u793e\u4ea4\u56fe\u6765\u589e\u5f3a\u4ee4\u724c\u8def\u7531\u8fc7\u7a0b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfSMoE\u5728\u6570\u636e\u5206\u5e03\u53d8\u5316\u4e0b\u7684\u9c81\u68d2\u6027\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u7a00\u758f\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\u867d\u7136\u80fd\u6709\u6548\u6269\u5c55\u6a21\u578b\u89c4\u6a21\u5e76\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\uff0c\u4f46\u5728\u9762\u5bf9\u6570\u636e\u5206\u5e03\u53d8\u5316\u65f6\u9c81\u68d2\u6027\u8f83\u5dee\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u6c61\u67d3\u60c5\u51b5\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faSymphonySMoE\uff0c\u5728SMoE\u57fa\u7840\u4e0a\u5f15\u5165\u4e13\u5bb6\u95f4\u7684\u793e\u4ea4\u56fe\u7ed3\u6784\u6765\u5efa\u6a21\u4e13\u5bb6\u4ea4\u4e92\uff0c\u6539\u8fdb\u4ee4\u724c\u8def\u7531\u8fc7\u7a0b\uff0c\u8be5\u65b9\u6cd5\u662f\u8f7b\u91cf\u7ea7\u3001\u6a21\u5757\u5316\u7684\uff0c\u53ef\u4e0e\u73b0\u6709SMoE\u6a21\u578b\u65e0\u7f1d\u96c6\u6210\u3002", "result": "\u5728\u8bed\u8a00\u5efa\u6a21\u548c\u89c6\u89c9\u6307\u4ee4\u8c03\u4f18\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u8bc1\u636e\u90fd\u8868\u660eSymphonySMoE\u4f18\u4e8e\u57fa\u7ebfSMoE\u6a21\u578b\uff0c\u5e76\u6210\u529f\u6269\u5c55\u523042\u4ebf\u548c74\u4ebf\u53c2\u6570\u89c4\u6a21\u3002", "conclusion": "SymphonySMoE\u901a\u8fc7\u56fe\u7ed3\u6784\u589e\u5f3a\u7684\u4e13\u5bb6\u4ea4\u4e92\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u4e86SMoE\u5728\u5206\u5e03\u53d8\u5316\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2510.17489", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17489", "abs": "https://arxiv.org/abs/2510.17489", "authors": ["Yongxin He", "Shan Zhang", "Yixuan Cao", "Lei Ma", "Ping Luo"], "title": "DETree: DEtecting Human-AI Collaborative Texts via Tree-Structured Hierarchical Representation Learning", "comment": "To appear in NeurIPS 2025", "summary": "Detecting AI-involved text is essential for combating misinformation,\nplagiarism, and academic misconduct. However, AI text generation includes\ndiverse collaborative processes (AI-written text edited by humans,\nhuman-written text edited by AI, and AI-generated text refined by other AI),\nwhere various or even new LLMs could be involved. Texts generated through these\nvaried processes exhibit complex characteristics, presenting significant\nchallenges for detection. Current methods model these processes rather crudely,\nprimarily employing binary classification (purely human vs. AI-involved) or\nmulti-classification (treating human-AI collaboration as a new class). We\nobserve that representations of texts generated through different processes\nexhibit inherent clustering relationships. Therefore, we propose DETree, a\nnovel approach that models the relationships among different processes as a\nHierarchical Affinity Tree structure, and introduces a specialized loss\nfunction that aligns text representations with this tree. To facilitate this\nlearning, we developed RealBench, a comprehensive benchmark dataset that\nautomatically incorporates a wide spectrum of hybrid texts produced through\nvarious human-AI collaboration processes. Our method improves performance in\nhybrid text detection tasks and significantly enhances robustness and\ngeneralization in out-of-distribution scenarios, particularly in few-shot\nlearning conditions, further demonstrating the promise of training-based\napproaches in OOD settings. Our code and dataset are available at\nhttps://github.com/heyongxin233/DETree.", "AI": {"tldr": "\u63d0\u51faDETree\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c42\u6b21\u4eb2\u548c\u6811\u7ed3\u6784\u5efa\u6a21\u4e0d\u540cAI\u53c2\u4e0e\u6587\u672c\u751f\u6210\u8fc7\u7a0b\u7684\u5173\u7cfb\uff0c\u5e76\u5f00\u53d1RealBench\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347\u6df7\u5408\u6587\u672c\u68c0\u6d4b\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "AI\u53c2\u4e0e\u6587\u672c\u751f\u6210\u8fc7\u7a0b\u591a\u6837\u5316\uff08AI\u5199\u4eba\u6539\u3001\u4eba\u5199AI\u6539\u3001AI\u95f4\u534f\u4f5c\uff09\uff0c\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u5efa\u6a21\u7c97\u7cd9\uff0c\u4e3b\u8981\u4f7f\u7528\u4e8c\u5143\u5206\u7c7b\u6216\u591a\u5206\u7c7b\uff0c\u96be\u4ee5\u5e94\u5bf9\u590d\u6742\u7279\u5f81\u3002", "method": "\u6784\u5efa\u5c42\u6b21\u4eb2\u548c\u6811\u7ed3\u6784\u5efa\u6a21\u4e0d\u540c\u751f\u6210\u8fc7\u7a0b\u95f4\u7684\u5173\u7cfb\uff0c\u8bbe\u8ba1\u4e13\u7528\u635f\u5931\u51fd\u6570\u4f7f\u6587\u672c\u8868\u793a\u4e0e\u6811\u7ed3\u6784\u5bf9\u9f50\uff0c\u5f00\u53d1RealBench\u6570\u636e\u96c6\u652f\u6301\u8bad\u7ec3\u3002", "result": "DETree\u5728\u6df7\u5408\u6587\u672c\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u597d\uff0c\u5728\u5206\u5e03\u5916\u573a\u666f\u4e0b\u663e\u8457\u63d0\u5347\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u5c11\u6837\u672c\u5b66\u4e60\u6761\u4ef6\u4e0b\u3002", "conclusion": "\u57fa\u4e8e\u8bad\u7ec3\u7684\u65b9\u6cd5\u5728OOD\u8bbe\u7f6e\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u5c42\u6b21\u5173\u7cfb\u5efa\u6a21\u80fd\u6709\u6548\u63d0\u5347AI\u6587\u672c\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2510.16440", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.16440", "abs": "https://arxiv.org/abs/2510.16440", "authors": ["Dimitris Stefanopoulos", "Andreas Voskou"], "title": "Colliding with Adversaries at ECML-PKDD 2025 Adversarial Attack Competition 1st Prize Solution", "comment": null, "summary": "This report presents the winning solution for Task 1 of Colliding with\nAdversaries: A Challenge on Robust Learning in High Energy Physics Discovery at\nECML-PKDD 2025. The task required designing an adversarial attack against a\nprovided classification model that maximizes misclassification while minimizing\nperturbations. Our approach employs a multi-round gradient-based strategy that\nleverages the differentiable structure of the model, augmented with random\ninitialization and sample-mixing techniques to enhance effectiveness. The\nresulting attack achieved the best results in perturbation size and fooling\nsuccess rate, securing first place in the competition.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u5728ECML-PKDD 2025\u9ad8\u80fd\u7269\u7406\u53d1\u73b0\u6311\u6218\u8d5b\u4e2dTask 1\u7684\u83b7\u80dc\u89e3\u51b3\u65b9\u6848\uff0c\u91c7\u7528\u57fa\u4e8e\u68af\u5ea6\u7684\u591a\u8f6e\u5bf9\u6297\u653b\u51fb\u7b56\u7565\uff0c\u5728\u6700\u5c0f\u5316\u6270\u52a8\u7684\u540c\u65f6\u6700\u5927\u5316\u5206\u7c7b\u9519\u8bef\u7387\u3002", "motivation": "\u8be5\u4efb\u52a1\u8981\u6c42\u8bbe\u8ba1\u5bf9\u6297\u653b\u51fb\u6765\u5bf9\u6297\u63d0\u4f9b\u7684\u5206\u7c7b\u6a21\u578b\uff0c\u76ee\u6807\u662f\u5728\u6700\u5c0f\u5316\u6270\u52a8\u7684\u540c\u65f6\u6700\u5927\u5316\u8bef\u5206\u7c7b\u7387\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u68af\u5ea6\u7684\u591a\u8f6e\u653b\u51fb\u7b56\u7565\uff0c\u5229\u7528\u6a21\u578b\u7684\u53ef\u5fae\u5206\u7ed3\u6784\uff0c\u7ed3\u5408\u968f\u673a\u521d\u59cb\u5316\u548c\u6837\u672c\u6df7\u5408\u6280\u672f\u6765\u589e\u5f3a\u653b\u51fb\u6548\u679c\u3002", "result": "\u6240\u63d0\u51fa\u7684\u653b\u51fb\u65b9\u6cd5\u5728\u6270\u52a8\u5927\u5c0f\u548c\u6b3a\u9a97\u6210\u529f\u7387\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u4f73\u7ed3\u679c\uff0c\u5728\u7ade\u8d5b\u4e2d\u83b7\u5f97\u4e86\u7b2c\u4e00\u540d\u3002", "conclusion": "\u591a\u8f6e\u68af\u5ea6\u653b\u51fb\u7ed3\u5408\u968f\u673a\u521d\u59cb\u5316\u548c\u6837\u672c\u6df7\u5408\u6280\u672f\u80fd\u591f\u6709\u6548\u751f\u6210\u5bf9\u6297\u6837\u672c\uff0c\u5728\u4fdd\u6301\u5c0f\u6270\u52a8\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u8bef\u5206\u7c7b\u7387\u3002"}}
{"id": "2510.17491", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17491", "abs": "https://arxiv.org/abs/2510.17491", "authors": ["Yihong Tang", "Kehai Chen", "Liang Yue", "Jinxin Fan", "Caishen Zhou", "Xiaoguang Li", "Yuyang Zhang", "Mingming Zhao", "Shixiong Kai", "Kaiyang Guo", "Xingshan Zeng", "Wenjing Cun", "Lifeng Shang", "Min Zhang"], "title": "Empowering Real-World: A Survey on the Technology, Practice, and Evaluation of LLM-driven Industry Agents", "comment": null, "summary": "With the rise of large language models (LLMs), LLM agents capable of\nautonomous reasoning, planning, and executing complex tasks have become a\nfrontier in artificial intelligence. However, how to translate the research on\ngeneral agents into productivity that drives industry transformations remains a\nsignificant challenge. To address this, this paper systematically reviews the\ntechnologies, applications, and evaluation methods of industry agents based on\nLLMs. Using an industry agent capability maturity framework, it outlines the\nevolution of agents in industry applications, from \"process execution systems\"\nto \"adaptive social systems.\" First, we examine the three key technological\npillars that support the advancement of agent capabilities: Memory, Planning,\nand Tool Use. We discuss how these technologies evolve from supporting simple\ntasks in their early forms to enabling complex autonomous systems and\ncollective intelligence in more advanced forms. Then, we provide an overview of\nthe application of industry agents in real-world domains such as digital\nengineering, scientific discovery, embodied intelligence, collaborative\nbusiness execution, and complex system simulation. Additionally, this paper\nreviews the evaluation benchmarks and methods for both fundamental and\nspecialized capabilities, identifying the challenges existing evaluation\nsystems face regarding authenticity, safety, and industry specificity. Finally,\nwe focus on the practical challenges faced by industry agents, exploring their\ncapability boundaries, developmental potential, and governance issues in\nvarious scenarios, while providing insights into future directions. By\ncombining technological evolution with industry practices, this review aims to\nclarify the current state and offer a clear roadmap and theoretical foundation\nfor understanding and building the next generation of industry agents.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u884c\u4e1a\u667a\u80fd\u4f53\u6280\u672f\u3001\u5e94\u7528\u548c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u884c\u4e1a\u667a\u80fd\u4f53\u80fd\u529b\u6210\u719f\u5ea6\u6846\u67b6\uff0c\u5206\u6790\u4e86\u4ece\"\u6d41\u7a0b\u6267\u884c\u7cfb\u7edf\"\u5230\"\u81ea\u9002\u5e94\u793e\u4f1a\u7cfb\u7edf\"\u7684\u6f14\u8fdb\u8def\u5f84\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u80fd\u591f\u81ea\u4e3b\u63a8\u7406\u3001\u89c4\u5212\u548c\u6267\u884c\u590d\u6742\u4efb\u52a1\u7684\u667a\u80fd\u4f53\u5df2\u6210\u4e3a\u4eba\u5de5\u667a\u80fd\u524d\u6cbf\uff0c\u4f46\u5982\u4f55\u5c06\u901a\u7528\u667a\u80fd\u4f53\u7814\u7a76\u8f6c\u5316\u4e3a\u63a8\u52a8\u884c\u4e1a\u8f6c\u578b\u7684\u751f\u4ea7\u529b\u4ecd\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002", "method": "\u4f7f\u7528\u884c\u4e1a\u667a\u80fd\u4f53\u80fd\u529b\u6210\u719f\u5ea6\u6846\u67b6\uff0c\u5206\u6790\u652f\u6491\u667a\u80fd\u4f53\u80fd\u529b\u53d1\u5c55\u7684\u4e09\u5927\u6280\u672f\u652f\u67f1\uff08\u8bb0\u5fc6\u3001\u89c4\u5212\u3001\u5de5\u5177\u4f7f\u7528\uff09\uff0c\u5e76\u6982\u8ff0\u884c\u4e1a\u667a\u80fd\u4f53\u5728\u6570\u5b57\u5de5\u7a0b\u3001\u79d1\u5b66\u53d1\u73b0\u3001\u5177\u8eab\u667a\u80fd\u7b49\u9886\u57df\u7684\u5e94\u7528\u3002", "result": "\u68b3\u7406\u4e86\u884c\u4e1a\u667a\u80fd\u4f53\u7684\u6280\u672f\u6f14\u8fdb\u8def\u5f84\uff0c\u4ece\u652f\u6301\u7b80\u5355\u4efb\u52a1\u5230\u5b9e\u73b0\u590d\u6742\u81ea\u4e3b\u7cfb\u7edf\u548c\u96c6\u4f53\u667a\u80fd\uff1b\u603b\u7ed3\u4e86\u5728\u771f\u5b9e\u9886\u57df\u4e2d\u7684\u5e94\u7528\u60c5\u51b5\uff1b\u8bc6\u522b\u4e86\u73b0\u6709\u8bc4\u4f30\u7cfb\u7edf\u5728\u771f\u5b9e\u6027\u3001\u5b89\u5168\u6027\u548c\u884c\u4e1a\u7279\u6027\u65b9\u9762\u9762\u4e34\u7684\u6311\u6218\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u6280\u672f\u6f14\u8fdb\u4e0e\u884c\u4e1a\u5b9e\u8df5\uff0c\u672c\u6587\u4e3a\u7406\u89e3\u548c\u6784\u5efa\u4e0b\u4e00\u4ee3\u884c\u4e1a\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u8def\u7ebf\u56fe\u548c\u7406\u8bba\u57fa\u7840\uff0c\u63a2\u8ba8\u4e86\u884c\u4e1a\u667a\u80fd\u4f53\u7684\u80fd\u529b\u8fb9\u754c\u3001\u53d1\u5c55\u6f5c\u529b\u548c\u6cbb\u7406\u95ee\u9898\u3002"}}
{"id": "2510.16443", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16443", "abs": "https://arxiv.org/abs/2510.16443", "authors": ["Dimitris Stefanopoulos", "Andreas Voskou"], "title": "Colliding with Adversaries at ECML-PKDD 2025 Model Robustness Competition 1st Prize Solution", "comment": null, "summary": "This report presents the winning solution for Task 2 of Colliding with\nAdversaries: A Challenge on Robust Learning in High Energy Physics Discovery at\nECML-PKDD 2025. The goal of the challenge was to design and train a robust\nANN-based model capable of achieving high accuracy in a binary classification\ntask on both clean and adversarial data generated with the Random Distribution\nShuffle Attack (RDSA). Our solution consists of two components: a data\ngeneration phase and a robust model training phase. In the first phase, we\nproduced 15 million artificial training samples using a custom methodology\nderived from Random Distribution Shuffle Attack (RDSA). In the second phase, we\nintroduced a robust architecture comprising (i)a Feature Embedding Block with\nshared weights among features of the same type and (ii)a Dense Fusion Tail\nresponsible for the final prediction. Training this architecture on our\nadversarial dataset achieved a mixed accuracy score of 80\\%, exceeding the\nsecond-place solution by two percentage points.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u5728ECML-PKDD 2025\u9ad8\u80fd\u7269\u7406\u53d1\u73b0\u6311\u6218\u8d5b\u4e2dTask 2\u7684\u83b7\u80dc\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u6570\u636e\u751f\u6210\u548c\u9c81\u68d2\u6a21\u578b\u8bad\u7ec3\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u5728\u5bf9\u6297\u6027\u653b\u51fb\u4e0b\u5b9e\u73b0\u4e8680%\u7684\u6df7\u5408\u51c6\u786e\u7387\u3002", "motivation": "\u8bbe\u8ba1\u80fd\u591f\u540c\u65f6\u5728\u5e72\u51c0\u6570\u636e\u548c\u5bf9\u6297\u6027\u6570\u636e\u4e0a\u5b9e\u73b0\u9ad8\u51c6\u786e\u7387\u7684\u9c81\u68d2ANN\u6a21\u578b\uff0c\u5e94\u5bf9\u9ad8\u80fd\u7269\u7406\u53d1\u73b0\u4e2d\u7684\u5bf9\u6297\u653b\u51fb\u6311\u6218\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1) \u57fa\u4e8eRDSA\u65b9\u6cd5\u751f\u62101500\u4e07\u4eba\u5de5\u8bad\u7ec3\u6837\u672c\uff1b2) \u6784\u5efa\u5305\u542b\u7279\u5f81\u5d4c\u5165\u5757\uff08\u5171\u4eab\u6743\u91cd\uff09\u548c\u5bc6\u96c6\u878d\u5408\u5c3e\u90e8\u7684\u9c81\u68d2\u67b6\u6784\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u6df7\u5408\u6570\u636e\u96c6\u4e0a\u8fbe\u523080%\u7684\u51c6\u786e\u7387\uff0c\u6bd4\u7b2c\u4e8c\u540d\u89e3\u51b3\u65b9\u6848\u9ad8\u51fa2\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\u7ed3\u5408\u5bf9\u6297\u6570\u636e\u751f\u6210\u548c\u4e13\u95e8\u8bbe\u8ba1\u7684\u9c81\u68d2\u67b6\u6784\uff0c\u5728\u9ad8\u80fd\u7269\u7406\u5bf9\u6297\u5b66\u4e60\u6311\u6218\u4e2d\u53d6\u5f97\u4e86\u6700\u4f73\u6027\u80fd\u3002"}}
{"id": "2510.17498", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17498", "abs": "https://arxiv.org/abs/2510.17498", "authors": ["Zihan Liu", "Shun Zheng", "Xumeng Wen", "Yang Wang", "Jiang Bian", "Mao Yang"], "title": "Deep Self-Evolving Reasoning", "comment": null, "summary": "Long-form chain-of-thought reasoning has become a cornerstone of advanced\nreasoning in large language models. While recent verification-refinement\nframeworks have enabled proprietary models to solve Olympiad-level problems,\ntheir effectiveness hinges on strong, reliable verification and correction\ncapabilities, which remain fragile in open-weight, smaller-scale models. This\nwork demonstrates that even with weak verification and refinement capabilities\non hard tasks, the reasoning limits of such models can be substantially\nextended through a probabilistic paradigm we call Deep Self-Evolving Reasoning\n(DSER). We conceptualize iterative reasoning as a Markov chain, where each step\nrepresents a stochastic transition in the solution space. The key insight is\nthat convergence to a correct solution is guaranteed as long as the probability\nof improvement marginally exceeds that of degradation. By running multiple\nlong-horizon, self-evolving processes in parallel, DSER amplifies these small\npositive tendencies, enabling the model to asymptotically approach correct\nanswers. Empirically, we apply DSER to the DeepSeek-R1-0528-Qwen3-8B model. On\nthe challenging AIME 2024-2025 benchmark, DSER solves 5 out of 9 previously\nunsolvable problems and boosts overall performance, enabling this compact model\nto surpass the single-turn accuracy of its 600B-parameter teacher through\nmajority voting. Beyond its immediate utility for test-time scaling, the DSER\nframework serves to diagnose the fundamental limitations of current open-weight\nreasoners. By clearly delineating their shortcomings in self-verification,\nrefinement, and stability, our findings establish a clear research agenda for\ndeveloping next-generation models with powerful, intrinsic self-evolving\ncapabilities.", "AI": {"tldr": "Deep Self-Evolving Reasoning (DSER) \u662f\u4e00\u79cd\u6982\u7387\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u8fed\u4ee3\u63a8\u7406\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u94fe\uff0c\u5373\u4f7f\u9a8c\u8bc1\u548c\u4fee\u6b63\u80fd\u529b\u8f83\u5f31\uff0c\u4e5f\u80fd\u663e\u8457\u6269\u5c55\u5c0f\u578b\u5f00\u653e\u6743\u91cd\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u9a8c\u8bc1-\u4fee\u6b63\u6846\u67b6\u4f9d\u8d56\u5f3a\u5927\u7684\u9a8c\u8bc1\u548c\u4fee\u6b63\u80fd\u529b\uff0c\u8fd9\u5728\u5f00\u653e\u6743\u91cd\u7684\u5c0f\u578b\u6a21\u578b\u4e2d\u5f88\u8106\u5f31\u3002DSER\u65e8\u5728\u514b\u670d\u8fd9\u4e00\u9650\u5236\uff0c\u901a\u8fc7\u6982\u7387\u65b9\u6cd5\u6269\u5c55\u6a21\u578b\u7684\u63a8\u7406\u8fb9\u754c\u3002", "method": "\u5c06\u8fed\u4ee3\u63a8\u7406\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u94fe\uff0c\u6bcf\u4e2a\u6b65\u9aa4\u4ee3\u8868\u89e3\u7a7a\u95f4\u7684\u968f\u673a\u8f6c\u79fb\u3002\u53ea\u8981\u6539\u8fdb\u6982\u7387\u7565\u9ad8\u4e8e\u9000\u5316\u6982\u7387\uff0c\u5c31\u80fd\u4fdd\u8bc1\u6536\u655b\u5230\u6b63\u786e\u89e3\u3002\u901a\u8fc7\u5e76\u884c\u8fd0\u884c\u591a\u4e2a\u81ea\u6f14\u5316\u8fc7\u7a0b\u6765\u653e\u5927\u8fd9\u4e9b\u5c0f\u7684\u6b63\u5411\u8d8b\u52bf\u3002", "result": "\u5728DeepSeek-R1-0528-Qwen3-8B\u6a21\u578b\u4e0a\u5e94\u7528DSER\uff0c\u5728AIME 2024-2025\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u89e3\u51b3\u4e869\u4e2a\u5148\u524d\u65e0\u6cd5\u89e3\u51b3\u7684\u96be\u9898\u4e2d\u76845\u4e2a\uff0c\u6574\u4f53\u6027\u80fd\u63d0\u5347\uff0c\u4f7f\u8fd9\u4e2a\u7d27\u51d1\u6a21\u578b\u901a\u8fc7\u591a\u6570\u6295\u7968\u8d85\u8d8a\u4e86\u5176600B\u53c2\u6570\u6559\u5e08\u7684\u5355\u8f6e\u51c6\u786e\u7387\u3002", "conclusion": "DSER\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u6d4b\u8bd5\u65f6\u6269\u5c55\u7684\u5b9e\u7528\u4ef7\u503c\uff0c\u8fd8\u8bca\u65ad\u4e86\u5f53\u524d\u5f00\u653e\u6743\u91cd\u63a8\u7406\u5668\u7684\u6839\u672c\u5c40\u9650\u6027\uff0c\u4e3a\u5f00\u53d1\u5177\u6709\u5f3a\u5927\u5185\u5728\u81ea\u6f14\u5316\u80fd\u529b\u7684\u4e0b\u4e00\u4ee3\u6a21\u578b\u6307\u660e\u4e86\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2510.16448", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16448", "abs": "https://arxiv.org/abs/2510.16448", "authors": ["Yongxiang Hua", "Haoyu Cao", "Zhou Tao", "Bocheng Li", "Zihao Wu", "Chaohu Liu", "Linli Xu"], "title": "Input Domain Aware MoE: Decoupling Routing Decisions from Task Optimization in Mixture of Experts", "comment": "ACM MM25", "summary": "Sparse Mixture of Experts (sMoE) has become a pivotal approach for scaling\nlarge vision-language models, offering substantial capacity while maintaining\ncomputational efficiency through dynamic, sparse activation of experts.\nHowever, existing routing mechanisms, typically based on similarity scoring,\nstruggle to effectively capture the underlying input structure. This limitation\nleads to a trade-off between expert specialization and balanced computation,\nhindering both scalability and performance. We propose Input Domain Aware MoE,\na novel routing framework that leverages a probabilistic mixture model to\nbetter partition the input space. By modeling routing probabilities as a\nmixture of distributions, our method enables experts to develop clear\nspecialization boundaries while achieving balanced utilization. Unlike\nconventional approaches, our routing mechanism is trained independently of\ntask-specific objectives, allowing for stable optimization and decisive expert\nassignments. Empirical results on vision-language tasks demonstrate that our\nmethod consistently outperforms existing sMoE approaches, achieving higher task\nperformance and improved expert utilization balance.", "AI": {"tldr": "\u63d0\u51faInput Domain Aware MoE\u8def\u7531\u6846\u67b6\uff0c\u901a\u8fc7\u6982\u7387\u6df7\u5408\u6a21\u578b\u66f4\u597d\u5730\u5212\u5206\u8f93\u5165\u7a7a\u95f4\uff0c\u89e3\u51b3\u73b0\u6709\u7a00\u758f\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\u5728\u4e13\u5bb6\u4e13\u4e1a\u5316\u548c\u8ba1\u7b97\u5e73\u8861\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u76f8\u4f3c\u6027\u8bc4\u5206\u7684\u8def\u7531\u673a\u5236\u96be\u4ee5\u6709\u6548\u6355\u6349\u8f93\u5165\u5e95\u5c42\u7ed3\u6784\uff0c\u5bfc\u81f4\u4e13\u5bb6\u4e13\u4e1a\u5316\u4e0e\u8ba1\u7b97\u5e73\u8861\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u963b\u788d\u6a21\u578b\u6269\u5c55\u6027\u548c\u6027\u80fd\u63d0\u5347\u3002", "method": "\u4f7f\u7528\u6982\u7387\u6df7\u5408\u6a21\u578b\u5efa\u6a21\u8def\u7531\u6982\u7387\uff0c\u5c06\u8def\u7531\u6982\u7387\u8868\u793a\u4e3a\u5206\u5e03\u7684\u6df7\u5408\uff0c\u4f7f\u4e13\u5bb6\u5f62\u6210\u6e05\u6670\u7684\u4e13\u4e1a\u5316\u8fb9\u754c\uff0c\u540c\u65f6\u5b9e\u73b0\u5747\u8861\u5229\u7528\u3002\u8def\u7531\u673a\u5236\u72ec\u7acb\u4e8e\u4efb\u52a1\u7279\u5b9a\u76ee\u6807\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u7a00\u758f\u4e13\u5bb6\u6df7\u5408\u65b9\u6cd5\uff0c\u83b7\u5f97\u66f4\u9ad8\u7684\u4efb\u52a1\u6027\u80fd\u548c\u66f4\u597d\u7684\u4e13\u5bb6\u5229\u7528\u5e73\u8861\u3002", "conclusion": "\u63d0\u51fa\u7684Input Domain Aware MoE\u8def\u7531\u6846\u67b6\u901a\u8fc7\u6982\u7387\u6df7\u5408\u6a21\u578b\u6709\u6548\u89e3\u51b3\u4e86\u4e13\u5bb6\u4e13\u4e1a\u5316\u548c\u8ba1\u7b97\u5e73\u8861\u7684\u6743\u8861\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u7a00\u758f\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2510.17504", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17504", "abs": "https://arxiv.org/abs/2510.17504", "authors": ["Jingshu Liu", "Raheel Qader", "Ga\u00ebtan Caillaut", "Mariam Nakhl\u00e9"], "title": "Lingua Custodi's participation at the WMT 2025 Terminology shared task", "comment": null, "summary": "While BERT is an effective method for learning monolingual sentence\nembeddings for semantic similarity and embedding based transfer learning BERT\nbased cross-lingual sentence embeddings have yet to be explored. We\nsystematically investigate methods for learning multilingual sentence\nembeddings by combining the best methods for learning monolingual and\ncross-lingual representations including: masked language modeling (MLM),\ntranslation language modeling (TLM), dual encoder translation ranking, and\nadditive margin softmax. We show that introducing a pre-trained multilingual\nlanguage model dramatically reduces the amount of parallel training data\nrequired to achieve good performance by 80%. Composing the best of these\nmethods produces a model that achieves 83.7% bi-text retrieval accuracy over\n112 languages on Tatoeba, well above the 65.5 achieved by LASER, while still\nperforming competitively on monolingual transfer learning benchmarks. Parallel\ndata mined from CommonCrawl using our best model is shown to train competitive\nNMT models for en-zh and en-de. We publicly release our best multilingual\nsentence embedding model for 109+ languages at https://tfhub.dev/google/LaBSE.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u5b66\u4e60\u591a\u8bed\u8a00\u53e5\u5b50\u5d4c\u5165\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u5355\u8bed\u548c\u8de8\u8bed\u8a00\u8868\u793a\u5b66\u4e60\u7684\u6700\u4f73\u6280\u672f\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5e76\u884c\u8bad\u7ec3\u6570\u636e\u9700\u6c42\uff0c\u5728112\u79cd\u8bed\u8a00\u4e0a\u5b9e\u73b0\u4e8683.7%\u7684\u53cc\u6587\u672c\u68c0\u7d22\u51c6\u786e\u7387\u3002", "motivation": "\u63a2\u7d22BERT\u5728\u591a\u8bed\u8a00\u53e5\u5b50\u5d4c\u5165\u65b9\u9762\u7684\u5e94\u7528\uff0c\u867d\u7136BERT\u5728\u5355\u8bed\u53e5\u5b50\u5d4c\u5165\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u5728\u591a\u8bed\u8a00\u53e5\u5b50\u5d4c\u5165\u65b9\u9762\u7684\u6f5c\u529b\u5c1a\u672a\u88ab\u5145\u5206\u6316\u6398\u3002", "method": "\u7ed3\u5408\u4e86\u63a9\u7801\u8bed\u8a00\u5efa\u6a21(MLM)\u3001\u7ffb\u8bd1\u8bed\u8a00\u5efa\u6a21(TLM)\u3001\u53cc\u7f16\u7801\u5668\u7ffb\u8bd1\u6392\u5e8f\u548c\u52a0\u6027\u8fb9\u9645softmax\u7b49\u65b9\u6cd5\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u591a\u8bed\u8a00\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u5c06\u5e76\u884c\u8bad\u7ec3\u6570\u636e\u9700\u6c42\u51cf\u5c11\u4e8680%\uff0c\u5728Tatoeba\u6570\u636e\u96c6\u4e0a112\u79cd\u8bed\u8a00\u7684bi-text\u68c0\u7d22\u51c6\u786e\u7387\u8fbe\u523083.7%\uff0c\u663e\u8457\u4f18\u4e8eLASER\u768465.5%\uff0c\u540c\u65f6\u5728\u5355\u8bed\u8fc1\u79fb\u5b66\u4e60\u57fa\u51c6\u4e0a\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "\u6210\u529f\u5f00\u53d1\u4e86\u9ad8\u6548\u7684\u591a\u8bed\u8a00\u53e5\u5b50\u5d4c\u5165\u6a21\u578b\uff0c\u5728109+\u79cd\u8bed\u8a00\u4e0a\u516c\u5f00\u53ef\u7528\uff0c\u5e76\u4e14\u4f7f\u7528\u8be5\u6a21\u578b\u4eceCommonCrawl\u6316\u6398\u7684\u5e76\u884c\u6570\u636e\u80fd\u591f\u8bad\u7ec3\u51fa\u6709\u7ade\u4e89\u529b\u7684\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\u6a21\u578b\u3002"}}
{"id": "2510.17509", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17509", "abs": "https://arxiv.org/abs/2510.17509", "authors": ["Shiyu Ni", "Keping Bi", "Jiafeng Guo", "Minghao Tang", "Jingtong Wu", "Zengxin Han", "Xueqi Cheng"], "title": "Annotation-Efficient Universal Honesty Alignment", "comment": null, "summary": "Honesty alignment-the ability of large language models (LLMs) to recognize\ntheir knowledge boundaries and express calibrated confidence-is essential for\ntrustworthy deployment. Existing methods either rely on training-free\nconfidence estimation (e.g., token probabilities, self-consistency) or\ntraining-based calibration with correctness annotations. While effective,\nachieving universal honesty alignment with training-based calibration requires\ncostly, large-scale labeling. To support annotation-efficient training, we\nintroduce Elicitation-Then-Calibration (EliCal), a two-stage framework that\nfirst elicits internal confidence using inexpensive self-consistency\nsupervision, then calibrates this confidence with a small set of correctness\nannotations. To support a large-scale study, we release HonestyBench, a\nbenchmark covering ten free-form QA datasets with 560k training and 70k\nevaluation instances annotated with correctness and self-consistency signals.\nExperiments show that EliCal achieves near-optimal alignment with only 1k\ncorrectness annotations (0.18% of full supervision) and better alignment\nperformance on unseen MMLU tasks than the calibration-only baseline, offering a\nscalable solution toward universal honesty alignment in LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86EliCal\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u65b9\u6cd5\u5b9e\u73b0LLM\u7684\u8bda\u5b9e\u5bf9\u9f50\uff1a\u5148\u4f7f\u7528\u5ec9\u4ef7\u7684\u81ea\u6211\u4e00\u81f4\u6027\u76d1\u7763\u6765\u6fc0\u53d1\u5185\u90e8\u7f6e\u4fe1\u5ea6\uff0c\u7136\u540e\u7528\u5c11\u91cf\u6b63\u786e\u6027\u6807\u6ce8\u8fdb\u884c\u6821\u51c6\u3002", "motivation": "\u73b0\u6709\u8bda\u5b9e\u5bf9\u9f50\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u65e0\u8bad\u7ec3\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\uff0c\u8981\u4e48\u9700\u8981\u5927\u89c4\u6a21\u6b63\u786e\u6027\u6807\u6ce8\u8fdb\u884c\u57fa\u4e8e\u8bad\u7ec3\u7684\u6821\u51c6\uff0c\u6210\u672c\u9ad8\u6602\u3002\u9700\u8981\u4e00\u79cd\u6807\u6ce8\u9ad8\u6548\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "EliCal\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1) \u4f7f\u7528\u81ea\u6211\u4e00\u81f4\u6027\u76d1\u7763\u6fc0\u53d1\u5185\u90e8\u7f6e\u4fe1\u5ea6\uff1b2) \u7528\u5c11\u91cf\u6b63\u786e\u6027\u6807\u6ce8\u6821\u51c6\u7f6e\u4fe1\u5ea6\u3002\u540c\u65f6\u53d1\u5e03\u4e86HonestyBench\u57fa\u51c6\u6570\u636e\u96c6\u3002", "result": "EliCal\u4ec5\u75281k\u6b63\u786e\u6027\u6807\u6ce8\uff08\u5168\u76d1\u7763\u76840.18%\uff09\u5c31\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u5bf9\u9f50\u6548\u679c\uff0c\u5728\u672a\u89c1MMLU\u4efb\u52a1\u4e0a\u6bd4\u4ec5\u6821\u51c6\u57fa\u7ebf\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "EliCal\u4e3aLLM\u7684\u901a\u7528\u8bda\u5b9e\u5bf9\u9f50\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6807\u6ce8\u6210\u672c\u3002"}}
{"id": "2510.16474", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16474", "abs": "https://arxiv.org/abs/2510.16474", "authors": ["Farwa Abbas", "Hussain Ahmad", "Claudia Szabo"], "title": "SCALAR: Self-Calibrating Adaptive Latent Attention Representation Learning", "comment": null, "summary": "High-dimensional, heterogeneous data with complex feature interactions pose\nsignificant challenges for traditional predictive modeling approaches. While\nProjection to Latent Structures (PLS) remains a popular technique, it struggles\nto model complex non-linear relationships, especially in multivariate systems\nwith high-dimensional correlation structures. This challenge is further\ncompounded by simultaneous interactions across multiple scales, where local\nprocessing fails to capture crossgroup dependencies. Additionally, static\nfeature weighting limits adaptability to contextual variations, as it ignores\nsample-specific relevance. To address these limitations, we propose a novel\nmethod that enhances predictive performance through novel architectural\ninnovations. Our architecture introduces an adaptive kernel-based attention\nmechanism that processes distinct feature groups separately before integration,\nenabling capture of local patterns while preserving global relationships.\nExperimental results show substantial improvements in performance metrics,\ncompared to the state-of-the-art methods across diverse datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u9002\u5e94\u6838\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u5206\u522b\u5904\u7406\u4e0d\u540c\u7279\u5f81\u7ec4\u6765\u63d0\u5347\u9ad8\u7ef4\u5f02\u6784\u6570\u636e\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfPLS\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u975e\u7ebf\u6027\u5173\u7cfb\u548c\u8de8\u5c3a\u5ea6\u4ea4\u4e92\u65f6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u6295\u5f71\u5230\u6f5c\u5728\u7ed3\u6784(PLS)\u65b9\u6cd5\u5728\u5904\u7406\u9ad8\u7ef4\u5f02\u6784\u6570\u636e\u65f6\u9762\u4e34\u6311\u6218\uff0c\u7279\u522b\u662f\u96be\u4ee5\u5efa\u6a21\u590d\u6742\u975e\u7ebf\u6027\u5173\u7cfb\u3001\u591a\u53d8\u91cf\u7cfb\u7edf\u4e2d\u7684\u9ad8\u7ef4\u76f8\u5173\u7ed3\u6784\uff0c\u4ee5\u53ca\u8de8\u5c3a\u5ea6\u540c\u65f6\u4ea4\u4e92\u3002\u9759\u6001\u7279\u5f81\u52a0\u6743\u9650\u5236\u4e86\u4e0a\u4e0b\u6587\u53d8\u5316\u7684\u9002\u5e94\u6027\uff0c\u5ffd\u7565\u4e86\u6837\u672c\u7279\u5b9a\u76f8\u5173\u6027\u3002", "method": "\u5f15\u5165\u81ea\u9002\u5e94\u6838\u57fa\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5206\u522b\u5904\u7406\u4e0d\u540c\u7684\u7279\u5f81\u7ec4\u540e\u518d\u8fdb\u884c\u6574\u5408\uff0c\u65e2\u80fd\u6355\u6349\u5c40\u90e8\u6a21\u5f0f\u53c8\u80fd\u4fdd\u6301\u5168\u5c40\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u6307\u6807\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u65b0\u9896\u7684\u67b6\u6784\u521b\u65b0\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u7ef4\u5f02\u6784\u6570\u636e\u5efa\u6a21\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2510.17532", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17532", "abs": "https://arxiv.org/abs/2510.17532", "authors": ["Raghu Vamshi Hemadri", "Geetha Krishna Guruju", "Kristi Topollai", "Anna Ewa Choromanska"], "title": "OncoReason: Structuring Clinical Reasoning in LLMs for Robust and Interpretable Survival Prediction", "comment": null, "summary": "Predicting cancer treatment outcomes requires models that are both accurate\nand interpretable, particularly in the presence of heterogeneous clinical data.\nWhile large language models (LLMs) have shown strong performance in biomedical\nNLP, they often lack structured reasoning capabilities critical for high-stakes\ndecision support. We present a unified, multi-task learning framework that\naligns autoregressive LLMs with clinical reasoning for outcome prediction on\nthe MSK-CHORD dataset. Our models are trained to jointly perform binary\nsurvival classification, continuous survival time regression, and natural\nlanguage rationale generation. We evaluate three alignment strategies: (1)\nstandard supervised fine-tuning (SFT), (2) SFT with Chain-of-Thought (CoT)\nprompting to elicit step-by-step reasoning, and (3) Group Relative Policy\nOptimization (GRPO), a reinforcement learning method that aligns model outputs\nto expert-derived reasoning trajectories. Experiments with LLaMa3-8B and\nMed42-8B backbones demonstrate that CoT prompting improves F1 by +6.0 and\nreduces MAE by 12%, while GRPO achieves state-of-the-art interpretability and\npredictive performance across BLEU, ROUGE, and BERTScore. We further show that\nexisting biomedical LLMs often fail to produce valid reasoning traces due to\narchitectural constraints. Our findings underscore the importance of\nreasoning-aware alignment in multi-task clinical modeling and set a new\nbenchmark for interpretable, trustworthy LLMs in precision oncology.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u81ea\u56de\u5f52LLM\u4e0e\u4e34\u5e8a\u63a8\u7406\u5bf9\u9f50\uff0c\u7528\u4e8e\u764c\u75c7\u6cbb\u7597\u7ed3\u679c\u9884\u6d4b\u3002\u901a\u8fc7\u4e09\u79cd\u5bf9\u9f50\u7b56\u7565\uff08\u6807\u51c6\u5fae\u8c03\u3001\u601d\u7ef4\u94fe\u63d0\u793a\u3001GRPO\u5f3a\u5316\u5b66\u4e60\uff09\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u7269\u533b\u5b66NLP\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u7f3a\u4e4f\u7ed3\u6784\u5316\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u6cd5\u6ee1\u8db3\u9ad8\u98ce\u9669\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7684\u9700\u6c42\u3002", "method": "\u4f7f\u7528\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u8054\u5408\u6267\u884c\u4e8c\u5143\u751f\u5b58\u5206\u7c7b\u3001\u8fde\u7eed\u751f\u5b58\u65f6\u95f4\u56de\u5f52\u548c\u81ea\u7136\u8bed\u8a00\u7406\u7531\u751f\u6210\u3002\u8bc4\u4f30\u4e09\u79cd\u5bf9\u9f50\u7b56\u7565\uff1a\u6807\u51c6\u76d1\u7763\u5fae\u8c03\u3001\u601d\u7ef4\u94fe\u63d0\u793a\u548cGRPO\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u601d\u7ef4\u94fe\u63d0\u793a\u5c06F1\u63d0\u9ad86.0%\uff0cMAE\u964d\u4f4e12%\uff1bGRPO\u5728BLEU\u3001ROUGE\u548cBERTScore\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u53ef\u89e3\u91ca\u6027\u548c\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "\u63a8\u7406\u611f\u77e5\u5bf9\u9f50\u5728\u591a\u4efb\u52a1\u4e34\u5e8a\u5efa\u6a21\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4e3a\u7cbe\u51c6\u80bf\u7624\u5b66\u4e2d\u53ef\u89e3\u91ca\u3001\u53ef\u4fe1\u8d56\u7684LLM\u8bbe\u5b9a\u4e86\u65b0\u57fa\u51c6\u3002"}}
{"id": "2510.17548", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17548", "abs": "https://arxiv.org/abs/2510.17548", "authors": ["Nisrine Rair", "Alban Goupil", "Valeriu Vrabie", "Emmanuel Chochoy"], "title": "When Annotators Disagree, Topology Explains: Mapper, a Topological Tool for Exploring Text Embedding Geometry and Ambiguity", "comment": "Accepted to appear in the Proceedings of the 2025 Conference on\n  Empirical Methods in Natural Language Processing (EMNLP 2025, Main\n  Conference)", "summary": "Language models are often evaluated with scalar metrics like accuracy, but\nsuch measures fail to capture how models internally represent ambiguity,\nespecially when human annotators disagree. We propose a topological perspective\nto analyze how fine-tuned models encode ambiguity and more generally instances.\n  Applied to RoBERTa-Large on the MD-Offense dataset, Mapper, a tool from\ntopological data analysis, reveals that fine-tuning restructures embedding\nspace into modular, non-convex regions aligned with model predictions, even for\nhighly ambiguous cases. Over $98\\%$ of connected components exhibit $\\geq 90\\%$\nprediction purity, yet alignment with ground-truth labels drops in ambiguous\ndata, surfacing a hidden tension between structural confidence and label\nuncertainty.\n  Unlike traditional tools such as PCA or UMAP, Mapper captures this geometry\ndirectly uncovering decision regions, boundary collapses, and overconfident\nclusters. Our findings position Mapper as a powerful diagnostic tool for\nunderstanding how models resolve ambiguity. Beyond visualization, it also\nenables topological metrics that may inform proactive modeling strategies in\nsubjective NLP tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u62d3\u6251\u6570\u636e\u5206\u6790\u5de5\u5177Mapper\u6765\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u5185\u90e8\u8868\u793a\u6b67\u4e49\uff0c\u7279\u522b\u662f\u5728\u4eba\u7c7b\u6807\u6ce8\u8005\u5b58\u5728\u5206\u6b67\u7684\u60c5\u51b5\u4e0b\u3002\u7814\u7a76\u53d1\u73b0\u5fae\u8c03\u540e\u7684\u6a21\u578b\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u5f62\u6210\u6a21\u5757\u5316\u3001\u975e\u51f8\u7684\u533a\u57df\uff0c\u8fd9\u4e9b\u533a\u57df\u4e0e\u6a21\u578b\u9884\u6d4b\u9ad8\u5ea6\u4e00\u81f4\uff0c\u4f46\u5728\u6b67\u4e49\u6570\u636e\u4e2d\u4e0e\u771f\u5b9e\u6807\u7b7e\u7684\u5bf9\u9f50\u5ea6\u4e0b\u964d\u3002", "motivation": "\u4f20\u7edf\u7684\u6807\u91cf\u8bc4\u4f30\u6307\u6807\uff08\u5982\u51c6\u786e\u7387\uff09\u65e0\u6cd5\u6355\u6349\u6a21\u578b\u5185\u90e8\u5982\u4f55\u8868\u793a\u6b67\u4e49\uff0c\u7279\u522b\u662f\u5728\u4eba\u7c7b\u6807\u6ce8\u8005\u5b58\u5728\u5206\u6b67\u7684\u60c5\u51b5\u4e0b\u3002\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u7406\u89e3\u6a21\u578b\u5982\u4f55\u5904\u7406\u4e3b\u89c2\u6027\u4efb\u52a1\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u4f7f\u7528\u62d3\u6251\u6570\u636e\u5206\u6790\u5de5\u5177Mapper\u6765\u5206\u6790\u5fae\u8c03\u540eRoBERTa-Large\u6a21\u578b\u5728MD-Offense\u6570\u636e\u96c6\u4e0a\u7684\u5d4c\u5165\u7a7a\u95f4\u7ed3\u6784\u3002\u4e0ePCA\u548cUMAP\u7b49\u4f20\u7edf\u53ef\u89c6\u5316\u5de5\u5177\u4e0d\u540c\uff0cMapper\u80fd\u591f\u76f4\u63a5\u63ed\u793a\u51b3\u7b56\u533a\u57df\u3001\u8fb9\u754c\u584c\u9677\u548c\u8fc7\u5ea6\u81ea\u4fe1\u7684\u805a\u7c7b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5fae\u8c03\u5c06\u5d4c\u5165\u7a7a\u95f4\u91cd\u7ec4\u4e3a\u6a21\u5757\u5316\u3001\u975e\u51f8\u7684\u533a\u57df\uff0c\u8fd9\u4e9b\u533a\u57df\u4e0e\u6a21\u578b\u9884\u6d4b\u9ad8\u5ea6\u4e00\u81f4\uff08\u8d85\u8fc798%\u7684\u8fde\u901a\u7ec4\u4ef6\u9884\u6d4b\u7eaf\u5ea6\u226590%\uff09\u3002\u4f46\u5728\u6b67\u4e49\u6570\u636e\u4e2d\uff0c\u6a21\u578b\u7ed3\u6784\u4e0e\u771f\u5b9e\u6807\u7b7e\u7684\u5bf9\u9f50\u5ea6\u4e0b\u964d\uff0c\u63ed\u793a\u4e86\u7ed3\u6784\u7f6e\u4fe1\u5ea6\u4e0e\u6807\u7b7e\u4e0d\u786e\u5b9a\u6027\u4e4b\u95f4\u7684\u9690\u85cf\u5f20\u529b\u3002", "conclusion": "Mapper\u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u8bca\u65ad\u5de5\u5177\uff0c\u53ef\u7528\u4e8e\u7406\u89e3\u6a21\u578b\u5982\u4f55\u89e3\u51b3\u6b67\u4e49\u3002\u9664\u4e86\u53ef\u89c6\u5316\u529f\u80fd\u5916\uff0c\u5b83\u8fd8\u652f\u6301\u62d3\u6251\u5ea6\u91cf\uff0c\u53ef\u80fd\u4e3aNLP\u4e3b\u89c2\u4efb\u52a1\u4e2d\u7684\u4e3b\u52a8\u5efa\u6a21\u7b56\u7565\u63d0\u4f9b\u4fe1\u606f\u3002"}}
{"id": "2510.17555", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17555", "abs": "https://arxiv.org/abs/2510.17555", "authors": ["Collin Zhang", "Fei Huang", "Chenhan Yuan", "Junyang Lin"], "title": "Language Confusion Gate: Language-Aware Decoding Through Model Self-Distillation", "comment": null, "summary": "Large language models (LLMs) often experience language confusion, which is\nthe unintended mixing of languages during text generation. Current solutions to\nthis problem either necessitate model retraining or cannot differentiate\nbetween harmful confusion and acceptable code-switching. This paper introduces\nthe Language Confusion Gate (LCG), a lightweight, plug-in solution that filters\ntokens during decoding without altering the base LLM. The LCG is trained using\nnorm-adjusted self-distillation to predict appropriate language families and\napply masking only when needed. Our method is based on the findings that\nlanguage confusion is infrequent, correct-language tokens are usually among the\ntop predictions, and output token embedding norms are larger for high-resource\nlanguages, which biases sampling. When evaluated across various models,\nincluding Qwen3, GPT-OSS, Gemma3, Llama3.1, LCG decreases language confusion\nsignificantly, often by an order of magnitude, without negatively impacting\ntask performance. Code is available at\nhttps://github.com/collinzrj/language_confusion_gate.", "AI": {"tldr": "\u63d0\u51fa\u8bed\u8a00\u6df7\u6dc6\u95e8\uff08LCG\uff09\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u63d2\u4ef6\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u89e3\u7801\u8fc7\u7a0b\u4e2d\u8fc7\u6ee4\u4ee4\u724c\u800c\u65e0\u9700\u4fee\u6539\u57fa\u7840\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u663e\u8457\u51cf\u5c11\u8bed\u8a00\u6df7\u6dc6\u73b0\u8c61", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u7ecf\u5e38\u51fa\u73b0\u8bed\u8a00\u6df7\u6dc6\u95ee\u9898\uff0c\u5373\u6587\u672c\u751f\u6210\u65f6\u65e0\u610f\u4e2d\u6df7\u5408\u591a\u79cd\u8bed\u8a00\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u8981\u4e48\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\uff0c\u8981\u4e48\u65e0\u6cd5\u533a\u5206\u6709\u5bb3\u6df7\u6dc6\u548c\u53ef\u63a5\u53d7\u7684\u8bed\u7801\u8f6c\u6362", "method": "\u4f7f\u7528\u89c4\u8303\u8c03\u6574\u7684\u81ea\u84b8\u998f\u8bad\u7ec3LCG\u6765\u9884\u6d4b\u9002\u5f53\u7684\u8bed\u8a00\u5bb6\u65cf\uff0c\u4ec5\u5728\u9700\u8981\u65f6\u5e94\u7528\u63a9\u7801\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8e\u4e09\u4e2a\u53d1\u73b0\uff1a\u8bed\u8a00\u6df7\u6dc6\u4e0d\u9891\u7e41\u3001\u6b63\u786e\u8bed\u8a00\u4ee4\u724c\u901a\u5e38\u5728\u524d\u51e0\u4e2a\u9884\u6d4b\u4e2d\u3001\u9ad8\u8d44\u6e90\u8bed\u8a00\u7684\u8f93\u51fa\u4ee4\u724c\u5d4c\u5165\u89c4\u8303\u66f4\u5927\u4ece\u800c\u4ea7\u751f\u91c7\u6837\u504f\u5dee", "result": "\u5728\u5305\u62ecQwen3\u3001GPT-OSS\u3001Gemma3\u3001Llama3.1\u7b49\u5404\u79cd\u6a21\u578b\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cLCG\u663e\u8457\u51cf\u5c11\u4e86\u8bed\u8a00\u6df7\u6dc6\uff0c\u901a\u5e38\u964d\u4f4e\u4e00\u4e2a\u6570\u91cf\u7ea7\uff0c\u4e14\u4e0d\u5f71\u54cd\u4efb\u52a1\u6027\u80fd", "conclusion": "LCG\u662f\u4e00\u79cd\u6709\u6548\u7684\u8f7b\u91cf\u7ea7\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u663e\u8457\u51cf\u5c11\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u8a00\u6df7\u6dc6\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u4efb\u52a1\u6027\u80fd"}}
{"id": "2510.16547", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16547", "abs": "https://arxiv.org/abs/2510.16547", "authors": ["Alif Elham Khan", "Mohammad Junayed Hasan", "Humayra Anjum", "Nabeel Mohammed", "Sifat Momen"], "title": "Predicting life satisfaction using machine learning and explainable AI", "comment": null, "summary": "Life satisfaction is a crucial facet of human well-being. Hence, research on\nlife satisfaction is incumbent for understanding how individuals experience\ntheir lives and influencing interventions targeted at enhancing mental health\nand well-being. Life satisfaction has traditionally been measured using analog,\ncomplicated, and frequently error-prone methods. These methods raise questions\nconcerning validation and propagation. However, this study demonstrates the\npotential for machine learning algorithms to predict life satisfaction with a\nhigh accuracy of 93.80% and a 73.00% macro F1-score. The dataset comes from a\ngovernment survey of 19000 people aged 16-64 years in Denmark. Using feature\nlearning techniques, 27 significant questions for assessing contentment were\nextracted, making the study highly reproducible, simple, and easily\ninterpretable. Furthermore, clinical and biomedical large language models\n(LLMs) were explored for predicting life satisfaction by converting tabular\ndata into natural language sentences through mapping and adding meaningful\ncounterparts, achieving an accuracy of 93.74% and macro F1-score of 73.21%. It\nwas found that life satisfaction prediction is more closely related to the\nbiomedical domain than the clinical domain. Ablation studies were also\nconducted to understand the impact of data resampling and feature selection\ntechniques on model performance. Moreover, the correlation between primary\ndeterminants with different age brackets was analyzed, and it was found that\nhealth condition is the most important determinant across all ages. This study\ndemonstrates how machine learning, large language models and XAI can jointly\ncontribute to building trust and understanding in using AI to investigate human\nbehavior, with significant ramifications for academics and professionals\nworking to quantify and comprehend subjective well-being.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9884\u6d4b\u751f\u6d3b\u6ee1\u610f\u5ea6\uff0c\u5728\u4e39\u9ea619000\u4eba\u7684\u653f\u5e9c\u8c03\u67e5\u6570\u636e\u4e0a\u8fbe\u523093.80%\u7684\u51c6\u786e\u7387\u548c73.00%\u7684\u5b8fF1\u5206\u6570\uff0c\u53d1\u73b0\u5065\u5eb7\u72b6\u51b5\u662f\u6240\u6709\u5e74\u9f84\u6bb5\u6700\u91cd\u8981\u7684\u51b3\u5b9a\u56e0\u7d20\u3002", "motivation": "\u4f20\u7edf\u7684\u751f\u6d3b\u6ee1\u610f\u5ea6\u6d4b\u91cf\u65b9\u6cd5\u5b58\u5728\u9a8c\u8bc1\u548c\u4f20\u64ad\u95ee\u9898\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u673a\u5668\u5b66\u4e60\u548cLLMs\u5728\u9884\u6d4b\u751f\u6d3b\u6ee1\u610f\u5ea6\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4e3a\u7406\u89e3\u4eba\u7c7b\u884c\u4e3a\u548c\u4e3b\u89c2\u5e78\u798f\u611f\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u7279\u5f81\u5b66\u4e60\u6280\u672f\u4ece\u8c03\u67e5\u6570\u636e\u4e2d\u63d0\u53d627\u4e2a\u91cd\u8981\u95ee\u9898\uff0c\u5e76\u5c06\u8868\u683c\u6570\u636e\u8f6c\u6362\u4e3a\u81ea\u7136\u8bed\u8a00\u53e5\u5b50\uff0c\u5e94\u7528\u4e34\u5e8a\u548c\u751f\u7269\u533b\u5b66LLMs\u8fdb\u884c\u9884\u6d4b\uff0c\u540c\u65f6\u8fdb\u884c\u6570\u636e\u91cd\u91c7\u6837\u548c\u7279\u5f81\u9009\u62e9\u7684\u6d88\u878d\u7814\u7a76\u3002", "result": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8fbe\u523093.80%\u51c6\u786e\u7387\uff0cLLMs\u8fbe\u523093.74%\u51c6\u786e\u7387\uff0c\u53d1\u73b0\u751f\u6d3b\u6ee1\u610f\u5ea6\u9884\u6d4b\u4e0e\u751f\u7269\u533b\u5b66\u9886\u57df\u66f4\u76f8\u5173\uff0c\u5065\u5eb7\u72b6\u51b5\u662f\u6240\u6709\u5e74\u9f84\u6bb5\u6700\u91cd\u8981\u7684\u51b3\u5b9a\u56e0\u7d20\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u3001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u53ef\u89e3\u91caAI\u53ef\u4ee5\u5171\u540c\u5efa\u7acb\u5bf9\u4f7f\u7528AI\u7814\u7a76\u4eba\u7c7b\u884c\u4e3a\u7684\u4fe1\u4efb\u548c\u7406\u89e3\uff0c\u5bf9\u91cf\u5316\u4e3b\u89c2\u5e78\u798f\u611f\u5177\u6709\u91cd\u8981\u5f71\u54cd\u3002"}}
{"id": "2510.17591", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17591", "abs": "https://arxiv.org/abs/2510.17591", "authors": ["Guang Yang", "Yujie Zhu"], "title": "HGAdapter: Hypergraph-based Adapters in Language Models for Code Summarization and Clone Detection", "comment": "Accepted by the 2025 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP 2025) as a findings long paper", "summary": "Pre-trained language models (PLMs) are increasingly being applied to\ncode-related tasks. Although PLMs have achieved good results, they do not take\ninto account potential high-order data correlations within the code. We propose\nthree types of high-order correlations in code tokens, i.e. abstract syntax\ntree family correlation, lexical correlation, and line correlation. We design a\ntokens and hyperedges generator to capture these high-order data correlations.\nWe improve the architecture of hypergraph neural networks and combine it with\nadapter tuning to propose a novel hypergraph-based adapter (HGAdapter) to\nfine-tune PLMs. HGAdapter can encode high-order data correlations and is\nallowed to be inserted into various PLMs to enhance performance. Experiments\nwere conducted on several public datasets, including six languages of code\nsummarization and code clone detection tasks. Our methods improved the\nperformance of PLMs in datasets to varying degrees. Experimental results\nvalidate the introduction of high-order data correlations that contribute to\nimproved effectiveness.", "AI": {"tldr": "\u63d0\u51fa\u4e86HGAdapter\u65b9\u6cd5\uff0c\u901a\u8fc7\u8d85\u56fe\u795e\u7ecf\u7f51\u7edc\u6355\u6349\u4ee3\u7801\u4e2d\u7684\u9ad8\u9636\u6570\u636e\u76f8\u5173\u6027\uff0c\u589e\u5f3a\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u76f8\u5173\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u672a\u8003\u8651\u4ee3\u7801\u5185\u90e8\u6f5c\u5728\u7684\u9ad8\u9636\u6570\u636e\u76f8\u5173\u6027\uff0c\u9650\u5236\u4e86\u6027\u80fd\u63d0\u5347\u3002", "method": "\u8bc6\u522b\u4ee3\u7801\u4e2d\u7684\u4e09\u79cd\u9ad8\u9636\u76f8\u5173\u6027\uff08\u62bd\u8c61\u8bed\u6cd5\u6811\u5bb6\u65cf\u76f8\u5173\u6027\u3001\u8bcd\u6c47\u76f8\u5173\u6027\u3001\u884c\u76f8\u5173\u6027\uff09\uff0c\u8bbe\u8ba1token\u548c\u8d85\u8fb9\u751f\u6210\u5668\uff0c\u6539\u8fdb\u8d85\u56fe\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u5e76\u7ed3\u5408\u9002\u914d\u5668\u8c03\u4f18\uff0c\u63d0\u51faHGAdapter\u65b9\u6cd5\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u4ee3\u7801\u6458\u8981\u548c\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\u4efb\u52a1\u4e2d\uff0cHGAdapter\u4e0d\u540c\u7a0b\u5ea6\u5730\u63d0\u5347\u4e86PLMs\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u9ad8\u9636\u6570\u636e\u76f8\u5173\u6027\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5f15\u5165\u9ad8\u9636\u6570\u636e\u76f8\u5173\u6027\u6709\u52a9\u4e8e\u63d0\u5347\u4ee3\u7801\u76f8\u5173\u4efb\u52a1\u7684\u6027\u80fd\uff0cHGAdapter\u65b9\u6cd5\u53ef\u7075\u6d3b\u63d2\u5165\u5404\u79cdPLMs\u4e2d\u589e\u5f3a\u5176\u8868\u73b0\u3002"}}
{"id": "2510.16548", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16548", "abs": "https://arxiv.org/abs/2510.16548", "authors": ["Zitao Fang", "Chenxuan Li", "Hongting Zhou", "Shuyang Yu", "Guodong Du", "Ashwaq Qasem", "Yang Lu", "Jing Li", "Junsong Zhang", "Sim Kuan Goh"], "title": "NeurIPT: Foundation Model for Neural Interfaces", "comment": "Accepted by The Thirty-Ninth Annual Conference on Neural Information\n  Processing Systems (NeurIPS 2025). Project Page:\n  https://ZzzitaoFang.github.io/projects/NeurIPT/", "summary": "Electroencephalography (EEG) has wide-ranging applications, from clinical\ndiagnosis to brain-computer interfaces (BCIs). With the increasing volume and\nvariety of EEG data, there has been growing interest in establishing foundation\nmodels (FMs) to scale up and generalize neural decoding. Despite showing early\npotential, applying FMs to EEG remains challenging due to substantial\ninter-subject, inter-task, and inter-condition variability, as well as diverse\nelectrode configurations across recording setups. To tackle these open\nchallenges, we propose NeurIPT, a foundation model developed for diverse\nEEG-based Neural Interfaces with a Pre-trained Transformer by capturing both\nhomogeneous and heterogeneous spatio-temporal characteristics inherent in EEG\nsignals. Temporally, we introduce Amplitude-Aware Masked Pretraining (AAMP),\nmasking based on signal amplitude rather than random intervals, to learn robust\nrepresentations across varying signal intensities beyond local interpolation.\nMoreover, this temporal representation is enhanced by a Progressive\nMixture-of-Experts (PMoE) architecture, where specialized expert subnetworks\nare progressively introduced at deeper layers, adapting effectively to the\ndiverse temporal characteristics of EEG signals. Spatially, NeurIPT leverages\nthe 3D physical coordinates of electrodes, enabling effective transfer of\nembedding across varying EEG settings, and develops Intra-Inter Lobe Pooling\n(IILP) during fine-tuning to efficiently exploit regional brain features.\nEmpirical evaluations across eight downstream BCI datasets, via fine-tuning,\ndemonstrated NeurIPT consistently achieved state-of-the-art performance,\nhighlighting its broad applicability and robust generalization. Our work pushes\nforward the state of FMs in EEG and offers insights into scalable and\ngeneralizable neural information processing systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86NeurIPT\uff0c\u4e00\u4e2a\u57fa\u4e8e\u9884\u8bad\u7ec3Transformer\u7684EEG\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u6355\u6349EEG\u4fe1\u53f7\u7684\u540c\u8d28\u548c\u5f02\u8d28\u65f6\u7a7a\u7279\u5f81\uff0c\u89e3\u51b3\u4e86\u8de8\u88ab\u8bd5\u3001\u8de8\u4efb\u52a1\u548c\u8de8\u6761\u4ef6\u7684\u53d8\u5f02\u6027\u95ee\u9898\u3002", "motivation": "\u968f\u7740EEG\u6570\u636e\u91cf\u7684\u589e\u52a0\uff0c\u5efa\u7acb\u57fa\u7840\u6a21\u578b\u6765\u6269\u5c55\u548c\u6cdb\u5316\u795e\u7ecf\u89e3\u7801\u53d8\u5f97\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u88ab\u8bd5\u95f4\u3001\u4efb\u52a1\u95f4\u3001\u6761\u4ef6\u95f4\u7684\u663e\u8457\u53d8\u5f02\u6027\u4ee5\u53ca\u4e0d\u540c\u7684\u7535\u6781\u914d\u7f6e\uff0c\u5e94\u7528\u57fa\u7840\u6a21\u578b\u5230EEG\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "1) \u65f6\u95f4\u4e0a\uff1a\u5f15\u5165\u632f\u5e45\u611f\u77e5\u63a9\u7801\u9884\u8bad\u7ec3(AAMP)\uff0c\u57fa\u4e8e\u4fe1\u53f7\u632f\u5e45\u800c\u975e\u968f\u673a\u95f4\u9694\u8fdb\u884c\u63a9\u7801\uff1b2) \u4f7f\u7528\u6e10\u8fdb\u6df7\u5408\u4e13\u5bb6(PMoE)\u67b6\u6784\uff0c\u5728\u6df1\u5c42\u9010\u6b65\u5f15\u5165\u4e13\u95e8\u5b50\u7f51\u7edc\uff1b3) \u7a7a\u95f4\u4e0a\uff1a\u5229\u7528\u7535\u67813D\u7269\u7406\u5750\u6807\u5b9e\u73b0\u8de8EEG\u8bbe\u7f6e\u7684\u5d4c\u5165\u8fc1\u79fb\uff1b4) \u5fae\u8c03\u65f6\u5f00\u53d1\u8111\u5185-\u8111\u95f4\u53f6\u6c60\u5316(IILP)\u6765\u6709\u6548\u5229\u7528\u533a\u57df\u8111\u7279\u5f81\u3002", "result": "\u5728\u516b\u4e2a\u4e0b\u6e38BCI\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u663e\u793a\uff0cNeurIPT\u901a\u8fc7\u5fae\u8c03\u59cb\u7ec8\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u7a81\u51fa\u4e86\u5176\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u548c\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63a8\u52a8\u4e86EEG\u4e2d\u57fa\u7840\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u5e76\u4e3a\u53ef\u6269\u5c55\u548c\u53ef\u6cdb\u5316\u7684\u795e\u7ecf\u4fe1\u606f\u5904\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2510.17602", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17602", "abs": "https://arxiv.org/abs/2510.17602", "authors": ["Huiyuan Xie", "Chenyang Li", "Huining Zhu", "Chubin Zhang", "Yuxiao Ye", "Zhenghao Liu", "Zhiyuan Liu"], "title": "LawChain: Modeling Legal Reasoning Chains for Chinese Tort Case Analysis", "comment": null, "summary": "Legal reasoning is a fundamental component of legal analysis and\ndecision-making. Existing computational approaches to legal reasoning\npredominantly rely on generic reasoning frameworks such as syllogism and IRAC,\nwhich do not comprehensively examine the nuanced processes that underpin legal\nreasoning. Moreover, current research has largely focused on criminal cases,\nwith insufficient modeling for civil cases. In this work, we present a novel\nframework for explicitly modeling legal reasoning in the analysis of Chinese\ntort-related civil cases. We first operationalize the legal reasoning processes\nused in tort analysis into the LawChain framework. LawChain is a three-module\nreasoning framework, with each module consisting of multiple finer-grained\nsub-steps. Informed by the LawChain framework, we introduce the task of tort\nlegal reasoning and construct an evaluation benchmark, LawChain$_{eval}$, to\nsystematically assess the critical steps within analytical reasoning chains for\ntort analysis. Leveraging this benchmark, we evaluate state-of-the-art large\nlanguage models for their legal reasoning ability in civil tort contexts. Our\nresults indicate that current models still fall short in accurately handling\ncrucial elements of tort legal reasoning. Furthermore, we introduce several\nbaseline approaches that explicitly incorporate LawChain-style reasoning\nthrough prompting or post-training. We conduct further experiments on\nadditional legal analysis tasks, such as Legal Named-Entity Recognition and\nCriminal Damages Calculation, to verify the generalizability of these\nbaselines. The proposed baseline approaches achieve significant improvements in\ntort-related legal reasoning and generalize well to related legal analysis\ntasks, thus demonstrating the value of explicitly modeling legal reasoning\nchains to enhance the reasoning capabilities of language models.", "AI": {"tldr": "\u63d0\u51fa\u4e86LawChain\u6846\u67b6\u6765\u5efa\u6a21\u4e2d\u56fd\u4fb5\u6743\u6c11\u4e8b\u6848\u4ef6\u7684\u6cd5\u5f8b\u63a8\u7406\u8fc7\u7a0b\uff0c\u6784\u5efa\u4e86\u8bc4\u4f30\u57fa\u51c6LawChain_eval\uff0c\u5e76\u901a\u8fc7\u63d0\u793a\u6216\u540e\u8bad\u7ec3\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u6cd5\u5f8b\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6cd5\u5f8b\u63a8\u7406\u8ba1\u7b97\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u901a\u7528\u63a8\u7406\u6846\u67b6\uff0c\u672a\u80fd\u5168\u9762\u8003\u5bdf\u6cd5\u5f8b\u63a8\u7406\u7684\u7ec6\u5fae\u8fc7\u7a0b\uff0c\u4e14\u591a\u96c6\u4e2d\u4e8e\u5211\u4e8b\u6848\u4ef6\uff0c\u5bf9\u6c11\u4e8b\u6848\u4ef6\u5efa\u6a21\u4e0d\u8db3\u3002", "method": "\u5c06\u4fb5\u6743\u5206\u6790\u4e2d\u7684\u6cd5\u5f8b\u63a8\u7406\u8fc7\u7a0b\u64cd\u4f5c\u5316\u4e3aLawChain\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff0c\u6bcf\u4e2a\u6a21\u5757\u7531\u591a\u4e2a\u7ec6\u7c92\u5ea6\u5b50\u6b65\u9aa4\u7ec4\u6210\uff0c\u5e76\u6784\u5efa\u8bc4\u4f30\u57fa\u51c6\u6765\u7cfb\u7edf\u8bc4\u4f30\u63a8\u7406\u94fe\u4e2d\u7684\u5173\u952e\u6b65\u9aa4\u3002", "result": "\u5f53\u524d\u6700\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4fb5\u6743\u6cd5\u5f8b\u63a8\u7406\u7684\u5173\u952e\u8981\u7d20\u5904\u7406\u4e0a\u4ecd\u5b58\u5728\u4e0d\u8db3\uff0c\u4f46\u57fa\u4e8eLawChain\u6846\u67b6\u7684\u57fa\u7ebf\u65b9\u6cd5\u5728\u4fb5\u6743\u76f8\u5173\u6cd5\u5f8b\u63a8\u7406\u548c\u76f8\u5173\u6cd5\u5f8b\u5206\u6790\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u660e\u786e\u5efa\u6a21\u6cd5\u5f8b\u63a8\u7406\u94fe\u80fd\u591f\u6709\u6548\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0cLawChain\u6846\u67b6\u5728\u63d0\u5347\u6cd5\u5f8b\u63a8\u7406\u6027\u80fd\u65b9\u9762\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2510.16552", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16552", "abs": "https://arxiv.org/abs/2510.16552", "authors": ["Ang Li", "Yifei Wang", "Zhihang Yuan", "Stefanie Jegelka", "Yisen Wang"], "title": "LANPO: Bootstrapping Language and Numerical Feedback for Reinforcement Learning in LLMs", "comment": null, "summary": "Reinforcement learning in large language models (LLMs) often relies on scalar\nrewards, a practice that discards valuable textual rationale buried in the\nrollouts, forcing the model to explore \\textit{de novo} with each attempt and\nhindering sample efficiency. While LLMs can uniquely learn from language\nfeedback provided in-context, naively integrating on-line experiences into RL\ntraining presents a paradox: feedback from the same problem risks information\nleakage and memorization, while feedback from different problems often leads to\nbehavior collapse due to irrelevant context. To resolve this tension, we\npropose \\textbf{Language-And-Numerical Policy Optimization (LANPO)}, a\nframework that cleanly separates the roles of feedback: language guides\nexploration, while numerical rewards drive optimization. LANPO builds a dynamic\nexperience pool from past trials and introduces two principles to ensure\nfeedback is effective: \\emph{Reward-Agnostic Reflection} for safe intra-sample\nself-correction and \\emph{Relevant Abstraction} to distill generalizable\nlessons from inter-sample experiences. Across mathematical reasoning\nbenchmarks, LANPO enables 7B and 14B models to significantly outperform strong\nbaselines trained with GRPO in test accuracy. Our work provides a robust method\nfor integrating historical experiences into the LLM RL loop, creating more\neffective and data-efficient learning agents.", "AI": {"tldr": "LANPO\u6846\u67b6\u901a\u8fc7\u5206\u79bb\u8bed\u8a00\u53cd\u9988\u548c\u6570\u503c\u5956\u52b1\u7684\u89d2\u8272\u6765\u89e3\u51b3LLM\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6837\u672c\u6548\u7387\u95ee\u9898\uff0c\u8bed\u8a00\u6307\u5bfc\u63a2\u7d22\uff0c\u6570\u503c\u5956\u52b1\u9a71\u52a8\u4f18\u5316\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8eGRPO\u57fa\u7ebf\u3002", "motivation": "\u4f20\u7edfLLM\u5f3a\u5316\u5b66\u4e60\u4f9d\u8d56\u6807\u91cf\u5956\u52b1\uff0c\u4e22\u5f03\u4e86rollout\u4e2d\u7684\u5b9d\u8d35\u6587\u672c\u7406\u7531\uff0c\u5bfc\u81f4\u6a21\u578b\u6bcf\u6b21\u5c1d\u8bd5\u90fd\u9700\u8981\u91cd\u65b0\u63a2\u7d22\uff0c\u964d\u4f4e\u4e86\u6837\u672c\u6548\u7387\u3002\u540c\u65f6\uff0c\u5728\u7ebf\u7ecf\u9a8c\u96c6\u6210\u5b58\u5728\u4fe1\u606f\u6cc4\u9732\u4e0e\u884c\u4e3a\u5d29\u6e83\u7684\u6096\u8bba\u3002", "method": "\u63d0\u51faLANPO\u6846\u67b6\uff1a1\uff09\u6784\u5efa\u52a8\u6001\u7ecf\u9a8c\u6c60\uff1b2\uff09\u5956\u52b1\u65e0\u5173\u53cd\u601d\u539f\u5219\u5b9e\u73b0\u5b89\u5168\u7684\u6837\u672c\u5185\u81ea\u6821\u6b63\uff1b3\uff09\u76f8\u5173\u62bd\u8c61\u539f\u5219\u4ece\u6837\u672c\u95f4\u7ecf\u9a8c\u4e2d\u63d0\u53d6\u53ef\u6cdb\u5316\u7684\u6559\u8bad\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c7B\u548c14B\u6a21\u578b\u4f7f\u7528LANPO\u5728\u6d4b\u8bd5\u51c6\u786e\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u4f7f\u7528GRPO\u7684\u5f3a\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "LANPO\u4e3a\u5c06\u5386\u53f2\u7ecf\u9a8c\u96c6\u6210\u5230LLM\u5f3a\u5316\u5b66\u4e60\u5faa\u73af\u4e2d\u63d0\u4f9b\u4e86\u7a33\u5065\u65b9\u6cd5\uff0c\u521b\u9020\u4e86\u66f4\u6709\u6548\u548c\u6570\u636e\u9ad8\u6548\u7684\u5b66\u4e60\u667a\u80fd\u4f53\u3002"}}
{"id": "2510.17620", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17620", "abs": "https://arxiv.org/abs/2510.17620", "authors": ["Yuefeng Peng", "Parnian Afshar", "Megan Ganji", "Thomas Butler", "Amir Houmansadr", "Mingxian Wang", "Dezhi Hong"], "title": "Forget to Know, Remember to Use: Context-Aware Unlearning for Large Language Models", "comment": null, "summary": "Large language models may encode sensitive information or outdated knowledge\nthat needs to be removed, to ensure responsible and compliant model responses.\nUnlearning has emerged as an efficient alternative to full retraining, aiming\nto remove specific knowledge while preserving overall model utility. Existing\nevaluations of unlearning methods focus on (1) the extent of forgetting of the\ntarget knowledge (forget set) and (2) maintaining performance on the retain set\n(i.e., utility). However, these evaluations overlook an important usability\naspect: users may still want the model to leverage the removed information if\nit is re-introduced in the prompt. In a systematic evaluation of six\nstate-of-the-art unlearning methods, we find that they consistently impair such\ncontextual utility. To address this, we augment unlearning objectives with a\nplug-in term that preserves the model's ability to use forgotten knowledge when\nit is present in context. Extensive experiments demonstrate that our approach\nrestores contextual utility to near original levels while still maintaining\neffective forgetting and retain-set utility.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5728\u673a\u5668\u5b66\u4e60\u9057\u5fd8\u65b9\u6cd5\u4e2d\u589e\u52a0\u4e0a\u4e0b\u6587\u6548\u7528\u4fdd\u62a4\u673a\u5236\uff0c\u901a\u8fc7\u6dfb\u52a0\u63d2\u4ef6\u9879\u6765\u4fdd\u6301\u6a21\u578b\u5728\u63d0\u793a\u4e2d\u5305\u542b\u5df2\u9057\u5fd8\u77e5\u8bc6\u65f6\u4ecd\u80fd\u4f7f\u7528\u8fd9\u4e9b\u4fe1\u606f\u3002", "motivation": "\u73b0\u6709\u9057\u5fd8\u65b9\u6cd5\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u76ee\u6807\u77e5\u8bc6\u7684\u9057\u5fd8\u7a0b\u5ea6\u548c\u4fdd\u7559\u96c6\u6027\u80fd\uff0c\u4f46\u5ffd\u89c6\u4e86\u5f53\u5df2\u5220\u9664\u77e5\u8bc6\u91cd\u65b0\u51fa\u73b0\u5728\u63d0\u793a\u4e2d\u65f6\u6a21\u578b\u4ecd\u5e94\u80fd\u5229\u7528\u8fd9\u4e9b\u4fe1\u606f\u7684\u91cd\u8981\u5b9e\u7528\u6027\u9700\u6c42\u3002", "method": "\u5728\u9057\u5fd8\u76ee\u6807\u4e2d\u589e\u52a0\u63d2\u4ef6\u9879\uff0c\u4e13\u95e8\u4fdd\u62a4\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u5305\u542b\u5df2\u9057\u5fd8\u77e5\u8bc6\u65f6\u7684\u4f7f\u7528\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u6709\u6548\u7684\u9057\u5fd8\u6548\u679c\u548c\u4fdd\u7559\u96c6\u6548\u7528\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u5c06\u4e0a\u4e0b\u6587\u6548\u7528\u6062\u590d\u5230\u63a5\u8fd1\u539f\u59cb\u6c34\u5e73\uff0c\u540c\u65f6\u7ef4\u6301\u6709\u6548\u7684\u9057\u5fd8\u6548\u679c\u548c\u4fdd\u7559\u96c6\u6548\u7528\u3002", "conclusion": "\u63d0\u51fa\u7684\u589e\u5f3a\u9057\u5fd8\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u635f\u5bb3\u4e0a\u4e0b\u6587\u6548\u7528\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9057\u5fd8\u3001\u4fdd\u7559\u96c6\u6548\u7528\u548c\u4e0a\u4e0b\u6587\u6548\u7528\u7684\u5e73\u8861\u3002"}}
{"id": "2510.16588", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16588", "abs": "https://arxiv.org/abs/2510.16588", "authors": ["Jiaxi Zhuang", "Yu Zhang", "Aimin Zhou", "Ying Qian"], "title": "Copy-Augmented Representation for Structure Invariant Template-Free Retrosynthesis", "comment": null, "summary": "Retrosynthesis prediction is fundamental to drug discovery and chemical\nsynthesis, requiring the identification of reactants that can produce a target\nmolecule. Current template-free methods struggle to capture the structural\ninvariance inherent in chemical reactions, where substantial molecular\nscaffolds remain unchanged, leading to unnecessarily large search spaces and\nreduced prediction accuracy. We introduce C-SMILES, a novel molecular\nrepresentation that decomposes traditional SMILES into element-token pairs with\nfive special tokens, effectively minimizing editing distance between reactants\nand products. Building upon this representation, we incorporate a\ncopy-augmented mechanism that dynamically determines whether to generate new\ntokens or preserve unchanged molecular fragments from the product. Our approach\nintegrates SMILES alignment guidance to enhance attention consistency with\nground-truth atom mappings, enabling more chemically coherent predictions.\nComprehensive evaluation on USPTO-50K and large-scale USPTO-FULL datasets\ndemonstrates significant improvements: 67.2% top-1 accuracy on USPTO-50K and\n50.8% on USPTO-FULL, with 99.9% validity in generated molecules. This work\nestablishes a new paradigm for structure-aware molecular generation with direct\napplications in computational drug discovery.", "AI": {"tldr": "\u63d0\u51fa\u4e86C-SMILES\u5206\u5b50\u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u89e3SMILES\u4e3a\u5143\u7d20-\u6807\u8bb0\u5bf9\u548c\u7279\u6b8a\u6807\u8bb0\uff0c\u51cf\u5c11\u53cd\u5e94\u7269\u4e0e\u4ea7\u7269\u95f4\u7684\u7f16\u8f91\u8ddd\u79bb\uff0c\u7ed3\u5408\u590d\u5236\u589e\u5f3a\u673a\u5236\u548cSMILES\u5bf9\u9f50\u6307\u5bfc\uff0c\u663e\u8457\u63d0\u5347\u9006\u5408\u6210\u9884\u6d4b\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u65e0\u6a21\u677f\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u5316\u5b66\u53cd\u5e94\u4e2d\u7684\u7ed3\u6784\u4e0d\u53d8\u6027\uff0c\u5bfc\u81f4\u641c\u7d22\u7a7a\u95f4\u8fc7\u5927\u548c\u9884\u6d4b\u7cbe\u5ea6\u964d\u4f4e\u3002\u9700\u8981\u5f00\u53d1\u80fd\u66f4\u597d\u8868\u793a\u5206\u5b50\u7ed3\u6784\u4e0d\u53d8\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u5f15\u5165C-SMILES\u8868\u793a\u6cd5\uff0c\u5c06\u4f20\u7edfSMILES\u5206\u89e3\u4e3a\u5143\u7d20-\u6807\u8bb0\u5bf9\u548c\u4e94\u4e2a\u7279\u6b8a\u6807\u8bb0\uff1b\u91c7\u7528\u590d\u5236\u589e\u5f3a\u673a\u5236\u52a8\u6001\u51b3\u5b9a\u751f\u6210\u65b0\u6807\u8bb0\u6216\u4fdd\u7559\u4ea7\u7269\u4e2d\u672a\u53d8\u7247\u6bb5\uff1b\u96c6\u6210SMILES\u5bf9\u9f50\u6307\u5bfc\u589e\u5f3a\u6ce8\u610f\u529b\u4e00\u81f4\u6027\u3002", "result": "\u5728USPTO-50K\u6570\u636e\u96c6\u4e0a\u8fbe\u523067.2%\u7684top-1\u51c6\u786e\u7387\uff0c\u5728USPTO-FULL\u6570\u636e\u96c6\u4e0a\u8fbe\u523050.8%\u7684\u51c6\u786e\u7387\uff0c\u751f\u6210\u5206\u5b50\u6709\u6548\u6027\u8fbe99.9%\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u7ed3\u6784\u611f\u77e5\u7684\u5206\u5b50\u751f\u6210\u5efa\u7acb\u4e86\u65b0\u8303\u5f0f\uff0c\u5728\u8ba1\u7b97\u836f\u7269\u53d1\u73b0\u4e2d\u5177\u6709\u76f4\u63a5\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.17652", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.17652", "abs": "https://arxiv.org/abs/2510.17652", "authors": ["Joseph McInerney"], "title": "Qomhra: A Bilingual Irish-English Large Language Model", "comment": null, "summary": "This paper introduces Qomhr\\'a, a bilingual Irish-English large language\nmodel (LLM), developed under low-resource constraints presenting a complete\npipeline spanning bilingual continued pre-training, instruction tuning, and\nalignment from human preferences. Newly accessible Irish corpora and English\ntext are mixed and curated to improve Irish performance while preserving\nEnglish ability. 6 closed-weight LLMs are judged for their Irish text\ngeneration by a native speaker, a learner and other LLMs. Google's\nGemini-2.5-Pro is ranked the highest and is subsequently used to synthesise\ninstruction tuning and human preference datasets. Two datasets are contributed\nleveraging Gemini-2.5-Pro: a 30K Irish-English parallel instruction tuning\ndataset and a 1K human preference dataset, generating accepted and rejected\nresponses that show near perfect alignment with a native Irish speaker.\nQomhr\\'a is comprehensively evaluated across benchmarks testing translation,\ngender understanding, topic identification and world knowledge with gains of up\nto 29% in Irish and 44% in English. Qomhr\\'a also undergoes instruction tuning\nand demonstrates clear progress in instruction following, crucial for chatbot\nfunctionality.", "AI": {"tldr": "Qomhr'a\u662f\u4e00\u4e2a\u5728\u4f4e\u8d44\u6e90\u6761\u4ef6\u4e0b\u5f00\u53d1\u7684\u53cc\u8bed\u7231\u5c14\u5170\u8bed-\u82f1\u8bed\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u53cc\u8bed\u6301\u7eed\u9884\u8bad\u7ec3\u3001\u6307\u4ee4\u5fae\u8c03\u548c\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u7684\u5b8c\u6574\u6d41\u7a0b\u6784\u5efa\u3002", "motivation": "\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u5f00\u53d1\u7231\u5c14\u5170\u8bed\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u82f1\u8bed\u80fd\u529b\uff0c\u586b\u8865\u7231\u5c14\u5170\u8bedLLM\u7684\u7a7a\u767d\u3002", "method": "\u6df7\u5408\u7231\u5c14\u5170\u8bed\u548c\u82f1\u8bed\u8bed\u6599\u8fdb\u884c\u53cc\u8bed\u9884\u8bad\u7ec3\uff0c\u4f7f\u7528Gemini-2.5-Pro\u5408\u6210\u6307\u4ee4\u5fae\u8c03\u548c\u4eba\u7c7b\u504f\u597d\u6570\u636e\u96c6\uff0c\u5e76\u8fdb\u884c\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5728\u7ffb\u8bd1\u3001\u6027\u522b\u7406\u89e3\u3001\u4e3b\u9898\u8bc6\u522b\u548c\u4e16\u754c\u77e5\u8bc6\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u7231\u5c14\u5170\u8bed\u6027\u80fd\u63d0\u534729%\uff0c\u82f1\u8bed\u6027\u80fd\u63d0\u534744%\u3002", "conclusion": "Qomhr'a\u6a21\u578b\u5728\u53cc\u8bed\u80fd\u529b\u548c\u6307\u4ee4\u8ddf\u968f\u65b9\u9762\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4e3a\u7231\u5c14\u5170\u8bed\u81ea\u7136\u8bed\u8a00\u5904\u7406\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\u3002"}}
{"id": "2510.16590", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2510.16590", "abs": "https://arxiv.org/abs/2510.16590", "authors": ["Alan Kai Hassen", "Andrius Bernatavicius", "Antonius P. A. Janssen", "Mike Preuss", "Gerard J. P. van Westen", "Djork-Arn\u00e9 Clevert"], "title": "Atom-anchored LLMs speak Chemistry: A Retrosynthesis Demonstration", "comment": "Alan Kai Hassen and Andrius Bernatavicius contributed equally to this\n  work", "summary": "Applications of machine learning in chemistry are often limited by the\nscarcity and expense of labeled data, restricting traditional supervised\nmethods. In this work, we introduce a framework for molecular reasoning using\ngeneral-purpose Large Language Models (LLMs) that operates without requiring\nlabeled training data. Our method anchors chain-of-thought reasoning to the\nmolecular structure by using unique atomic identifiers. First, the LLM performs\na one-shot task to identify relevant fragments and their associated chemical\nlabels or transformation classes. In an optional second step, this\nposition-aware information is used in a few-shot task with provided class\nexamples to predict the chemical transformation. We apply our framework to\nsingle-step retrosynthesis, a task where LLMs have previously underperformed.\nAcross academic benchmarks and expert-validated drug discovery molecules, our\nwork enables LLMs to achieve high success rates in identifying chemically\nplausible reaction sites ($\\geq90\\%$), named reaction classes ($\\geq40\\%$), and\nfinal reactants ($\\geq74\\%$). Beyond solving complex chemical tasks, our work\nalso provides a method to generate theoretically grounded synthetic datasets by\nmapping chemical knowledge onto the molecular structure and thereby addressing\ndata scarcity.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u6807\u8bb0\u8bad\u7ec3\u6570\u636e\u7684\u5206\u5b50\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u601d\u7ef4\u94fe\u63a8\u7406\u951a\u5b9a\u5230\u5206\u5b50\u7ed3\u6784\u4e0a\uff0c\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u89e3\u51b3\u5316\u5b66\u4efb\u52a1\uff0c\u7279\u522b\u662f\u5728\u5355\u6b65\u9006\u5408\u6210\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u8868\u73b0\u3002", "motivation": "\u5316\u5b66\u4e2d\u673a\u5668\u5b66\u4e60\u5e94\u7528\u5e38\u53d7\u9650\u4e8e\u6807\u8bb0\u6570\u636e\u7684\u7a00\u7f3a\u6027\u548c\u6602\u8d35\u6210\u672c\uff0c\u9650\u5236\u4e86\u4f20\u7edf\u76d1\u7763\u65b9\u6cd5\u7684\u4f7f\u7528\u3002", "method": "\u4f7f\u7528\u901a\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u539f\u5b50\u6807\u8bc6\u7b26\u5c06\u601d\u7ef4\u94fe\u63a8\u7406\u951a\u5b9a\u5230\u5206\u5b50\u7ed3\u6784\uff0c\u8fdb\u884c\u4e00\u6b65\u4efb\u52a1\u8bc6\u522b\u76f8\u5173\u7247\u6bb5\u53ca\u5176\u5316\u5b66\u6807\u7b7e\uff0c\u7136\u540e\u5728\u53ef\u9009\u6b65\u9aa4\u4e2d\u4f7f\u7528\u5c11\u91cf\u6837\u672c\u9884\u6d4b\u5316\u5b66\u8f6c\u5316\u3002", "result": "\u5728\u5b66\u672f\u57fa\u51c6\u548c\u4e13\u5bb6\u9a8c\u8bc1\u7684\u836f\u7269\u53d1\u73b0\u5206\u5b50\u4e2d\uff0cLLMs\u5728\u8bc6\u522b\u5316\u5b66\u5408\u7406\u53cd\u5e94\u4f4d\u70b9\uff08\u226590%\uff09\u3001\u547d\u540d\u53cd\u5e94\u7c7b\u522b\uff08\u226540%\uff09\u548c\u6700\u7ec8\u53cd\u5e94\u7269\uff08\u226574%\uff09\u65b9\u9762\u53d6\u5f97\u4e86\u9ad8\u6210\u529f\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e0d\u4ec5\u89e3\u51b3\u4e86\u590d\u6742\u5316\u5b66\u4efb\u52a1\uff0c\u8fd8\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u8fc7\u5c06\u5316\u5b66\u77e5\u8bc6\u6620\u5c04\u5230\u5206\u5b50\u7ed3\u6784\u6765\u751f\u6210\u7406\u8bba\u57fa\u7840\u7684\u5408\u6210\u6570\u636e\u96c6\u7684\u65b9\u6cd5\uff0c\u4ece\u800c\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002"}}
{"id": "2510.17698", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17698", "abs": "https://arxiv.org/abs/2510.17698", "authors": ["Liqun He", "Manolis Mavrikis", "Mutlu Cukurova"], "title": "Towards Mining Effective Pedagogical Strategies from Learner-LLM Educational Dialogues", "comment": null, "summary": "Dialogue plays a crucial role in educational settings, yet existing\nevaluation methods for educational applications of large language models (LLMs)\nprimarily focus on technical performance or learning outcomes, often neglecting\nattention to learner-LLM interactions. To narrow this gap, this AIED Doctoral\nConsortium paper presents an ongoing study employing a dialogue analysis\napproach to identify effective pedagogical strategies from learner-LLM\ndialogues. The proposed approach involves dialogue data collection, dialogue\nact (DA) annotation, DA pattern mining, and predictive model building. Early\ninsights are outlined as an initial step toward future research. The work\nunderscores the need to evaluate LLM-based educational applications by focusing\non dialogue dynamics and pedagogical strategies.", "AI": {"tldr": "\u672c\u7814\u7a76\u91c7\u7528\u5bf9\u8bdd\u5206\u6790\u65b9\u6cd5\u4ece\u5b66\u4e60\u8005-LLM\u5bf9\u8bdd\u4e2d\u8bc6\u522b\u6709\u6548\u7684\u6559\u5b66\u7b56\u7565\uff0c\u5305\u62ec\u5bf9\u8bdd\u6570\u636e\u6536\u96c6\u3001\u5bf9\u8bdd\u884c\u4e3a\u6807\u6ce8\u3001\u6a21\u5f0f\u6316\u6398\u548c\u9884\u6d4b\u6a21\u578b\u6784\u5efa\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6559\u80b2\u5e94\u7528\u8bc4\u4f30\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6280\u672f\u6027\u80fd\u6216\u5b66\u4e60\u6210\u679c\uff0c\u5f80\u5f80\u5ffd\u89c6\u4e86\u5b66\u4e60\u8005\u4e0eLLM\u4e4b\u95f4\u7684\u4e92\u52a8\u5206\u6790\u3002", "method": "\u91c7\u7528\u5bf9\u8bdd\u5206\u6790\u65b9\u6cd5\uff0c\u5305\u62ec\u56db\u4e2a\u6b65\u9aa4\uff1a\u5bf9\u8bdd\u6570\u636e\u6536\u96c6\u3001\u5bf9\u8bdd\u884c\u4e3a\u6807\u6ce8\u3001\u5bf9\u8bdd\u884c\u4e3a\u6a21\u5f0f\u6316\u6398\u548c\u9884\u6d4b\u6a21\u578b\u6784\u5efa\u3002", "result": "\u63d0\u51fa\u4e86\u521d\u6b65\u7684\u7814\u7a76\u6846\u67b6\u548c\u65e9\u671f\u89c1\u89e3\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002", "conclusion": "\u5f3a\u8c03\u9700\u8981\u901a\u8fc7\u5173\u6ce8\u5bf9\u8bdd\u52a8\u6001\u548c\u6559\u5b66\u7b56\u7565\u6765\u8bc4\u4f30\u57fa\u4e8eLLM\u7684\u6559\u80b2\u5e94\u7528\u3002"}}
{"id": "2510.17715", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17715", "abs": "https://arxiv.org/abs/2510.17715", "authors": ["Hanxu Hu", "Xingxing Zhang", "Jannis Vamvas", "Rico Sennrich", "Furu Wei"], "title": "QueST: Incentivizing LLMs to Generate Difficult Problems", "comment": "20 pages, 7 figures", "summary": "Large Language Models have achieved strong performance on reasoning tasks,\nsolving competition-level coding and math problems. However, their scalability\nis limited by human-labeled datasets and the lack of large-scale, challenging\ncoding problem training data. Existing competitive coding datasets contain only\nthousands to tens of thousands of problems. Previous synthetic data generation\nmethods rely on either augmenting existing instruction datasets or selecting\nchallenging problems from human-labeled data. In this paper, we propose QueST,\na novel framework which combines difficulty-aware graph sampling and\ndifficulty-aware rejection fine-tuning that directly optimizes specialized\ngenerators to create challenging coding problems. Our trained generators\ndemonstrate superior capability compared to even GPT-4o at creating challenging\nproblems that benefit downstream performance. We leverage QueST to generate\nlarge-scale synthetic coding problems, which we then use to distill from strong\nteacher models with long chain-of-thought or to conduct reinforcement learning\nfor smaller models, proving effective in both scenarios. Our distillation\nexperiments demonstrate significant performance gains. Specifically, after\nfine-tuning Qwen3-8B-base on 100K difficult problems generated by QueST, we\nsurpass the performance of the original Qwen3-8B on LiveCodeBench. With an\nadditional 112K examples (i.e., 28K human-written problems paired with multiple\nsynthetic solutions), our 8B model matches the performance of the much larger\nDeepSeek-R1-671B. These findings indicate that generating complex problems via\nQueST offers an effective and scalable approach to advancing the frontiers of\ncompetitive coding and reasoning for large language models.", "AI": {"tldr": "QueST\u6846\u67b6\u901a\u8fc7\u96be\u5ea6\u611f\u77e5\u56fe\u91c7\u6837\u548c\u62d2\u7edd\u5fae\u8c03\u751f\u6210\u5177\u6709\u6311\u6218\u6027\u7684\u7f16\u7a0b\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u7ade\u4e89\u6027\u7f16\u7a0b\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7ade\u4e89\u6027\u7f16\u7a0b\u6570\u636e\u96c6\u89c4\u6a21\u6709\u9650\uff08\u4ec5\u6570\u5343\u5230\u6570\u4e07\u95ee\u9898\uff09\uff0c\u4e14\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u6570\u636e\uff0c\u9650\u5236\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u6269\u5c55\u6027\u3002\u9700\u8981\u5927\u89c4\u6a21\u3001\u5177\u6709\u6311\u6218\u6027\u7684\u7f16\u7a0b\u95ee\u9898\u8bad\u7ec3\u6570\u636e\u3002", "method": "\u7ed3\u5408\u96be\u5ea6\u611f\u77e5\u56fe\u91c7\u6837\u548c\u96be\u5ea6\u611f\u77e5\u62d2\u7edd\u5fae\u8c03\uff0c\u8bad\u7ec3\u4e13\u95e8\u7684\u751f\u6210\u5668\u6765\u521b\u5efa\u5177\u6709\u6311\u6218\u6027\u7684\u7f16\u7a0b\u95ee\u9898\uff0c\u7136\u540e\u901a\u8fc7\u84b8\u998f\u6216\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e0b\u6e38\u6a21\u578b\u3002", "result": "\u4f7f\u7528QueST\u751f\u6210\u768410\u4e07\u4e2a\u56f0\u96be\u95ee\u9898\u5fae\u8c03Qwen3-8B-base\u540e\uff0c\u5728LiveCodeBench\u4e0a\u8d85\u8d8a\u4e86\u539f\u59cbQwen3-8B\u7684\u6027\u80fd\u3002\u6dfb\u52a0\u989d\u591611.2\u4e07\u793a\u4f8b\u540e\uff0c8B\u6a21\u578b\u6027\u80fd\u4e0e671B\u7684DeepSeek-R1\u76f8\u5f53\u3002", "conclusion": "\u901a\u8fc7QueST\u751f\u6210\u590d\u6742\u95ee\u9898\u662f\u63a8\u8fdb\u8bed\u8a00\u6a21\u578b\u5728\u7ade\u4e89\u6027\u7f16\u7a0b\u548c\u63a8\u7406\u80fd\u529b\u524d\u6cbf\u7684\u6709\u6548\u4e14\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u3002"}}
{"id": "2510.16607", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16607", "abs": "https://arxiv.org/abs/2510.16607", "authors": ["Tianwei Wang", "Xinhui Ma", "Wei Pang"], "title": "Asymptotically Stable Quaternion-valued Hopfield-structured Neural Network with Periodic Projection-based Supervised Learning Rules", "comment": null, "summary": "Motivated by the geometric advantages of quaternions in representing\nrotations and postures, we propose a quaternion-valued supervised learning\nHopfield-structured neural network (QSHNN) with a fully connected structure\ninspired by the classic Hopfield neural network (HNN). Starting from a\ncontinuous-time dynamical model of HNNs, we extend the formulation to the\nquaternionic domain and establish the existence and uniqueness of fixed points\nwith asymptotic stability. For the learning rules, we introduce a periodic\nprojection strategy that modifies standard gradient descent by periodically\nprojecting each 4*4 block of the weight matrix onto the closest quaternionic\nstructure in the least-squares sense. This approach preserves both convergence\nand quaternionic consistency throughout training. Benefiting from this rigorous\nmathematical foundation, the experimental model implementation achieves high\naccuracy, fast convergence, and strong reliability across randomly generated\ntarget sets. Moreover, the evolution trajectories of the QSHNN exhibit\nwell-bounded curvature, i.e., sufficient smoothness, which is crucial for\napplications such as control systems or path planning modules in robotic arms,\nwhere joint postures are parameterized by quaternion neurons. Beyond these\napplication scenarios, the proposed model offers a practical implementation\nframework and a general mathematical methodology for designing neural networks\nunder hypercomplex or non-commutative algebraic structures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u56db\u5143\u6570\u76d1\u7763\u5b66\u4e60Hopfield\u7ed3\u6784\u795e\u7ecf\u7f51\u7edc(QSHNN)\uff0c\u5229\u7528\u56db\u5143\u6570\u5728\u8868\u793a\u65cb\u8f6c\u548c\u59ff\u6001\u65b9\u9762\u7684\u51e0\u4f55\u4f18\u52bf\uff0c\u901a\u8fc7\u5468\u671f\u6027\u6295\u5f71\u7b56\u7565\u4fdd\u6301\u56db\u5143\u6570\u7ed3\u6784\u4e00\u81f4\u6027\uff0c\u5728\u673a\u5668\u4eba\u63a7\u5236\u7b49\u9886\u57df\u5177\u6709\u5e94\u7528\u4ef7\u503c\u3002", "motivation": "\u5229\u7528\u56db\u5143\u6570\u5728\u8868\u793a\u65cb\u8f6c\u548c\u59ff\u6001\u65b9\u9762\u7684\u51e0\u4f55\u4f18\u52bf\uff0c\u6269\u5c55\u7ecf\u5178Hopfield\u795e\u7ecf\u7f51\u7edc\u5230\u56db\u5143\u6570\u57df\uff0c\u4e3a\u673a\u5668\u4eba\u63a7\u5236\u3001\u8def\u5f84\u89c4\u5212\u7b49\u5e94\u7528\u63d0\u4f9b\u66f4\u6709\u6548\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u3002", "method": "\u4ece\u8fde\u7eed\u65f6\u95f4HNN\u52a8\u529b\u5b66\u6a21\u578b\u51fa\u53d1\uff0c\u6269\u5c55\u5230\u56db\u5143\u6570\u57df\uff0c\u5f15\u5165\u5468\u671f\u6027\u6295\u5f71\u7b56\u7565\u4fee\u6539\u6807\u51c6\u68af\u5ea6\u4e0b\u964d\uff0c\u5b9a\u671f\u5c06\u6743\u91cd\u77e9\u9635\u76844*4\u5757\u6295\u5f71\u5230\u6700\u8fd1\u7684\u56db\u5143\u6570\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u6a21\u578b\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u3001\u5feb\u901f\u6536\u655b\u548c\u5f3a\u53ef\u9760\u6027\uff0cQSHNN\u6f14\u5316\u8f68\u8ff9\u5177\u6709\u826f\u597d\u6709\u754c\u66f2\u7387\uff08\u8db3\u591f\u5e73\u6ed1\u6027\uff09\uff0c\u9002\u7528\u4e8e\u673a\u5668\u4eba\u5173\u8282\u59ff\u6001\u53c2\u6570\u5316\u7b49\u5e94\u7528\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u8d85\u590d\u6570\u6216\u975e\u4ea4\u6362\u4ee3\u6570\u7ed3\u6784\u4e0b\u7684\u795e\u7ecf\u7f51\u7edc\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u5b9e\u73b0\u6846\u67b6\u548c\u901a\u7528\u6570\u5b66\u65b9\u6cd5\u5b66\u3002"}}
{"id": "2510.17720", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17720", "abs": "https://arxiv.org/abs/2510.17720", "authors": ["Nanda Kumar Rengarajan", "Jun Yan", "Chun Wang"], "title": "PANER: A Paraphrase-Augmented Framework for Low-Resource Named Entity Recognition", "comment": null, "summary": "Named Entity Recognition (NER) is a critical task that requires substantial\nannotated data, making it challenging in low-resource scenarios where label\nacquisition is expensive. While zero-shot and instruction-tuned approaches have\nmade progress, they often fail to generalize to domain-specific entities and do\nnot effectively utilize limited available data. We present a lightweight\nfew-shot NER framework that addresses these challenges through two key\ninnovations: (1) a new instruction tuning template with a simplified output\nformat that combines principles from prior IT approaches to leverage the large\ncontext window of recent state-of-the-art LLMs; (2) introducing a strategic\ndata augmentation technique that preserves entity information while\nparaphrasing the surrounding context, thereby expanding our training data\nwithout compromising semantic relationships. Experiments on benchmark datasets\nshow that our method achieves performance comparable to state-of-the-art models\non few-shot and zero-shot tasks, with our few-shot approach attaining an\naverage F1 score of 80.1 on the CrossNER datasets. Models trained with our\nparaphrasing approach show consistent improvements in F1 scores of up to 17\npoints over baseline versions, offering a promising solution for groups with\nlimited NER training data and compute power.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5c11\u6837\u672c\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u6846\u67b6\uff0c\u901a\u8fc7\u6539\u8fdb\u7684\u6307\u4ee4\u8c03\u4f18\u6a21\u677f\u548c\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u53d6\u5f97\u4e86\u4e0e\u6700\u5148\u8fdb\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u4f46\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u4e2d\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u3002\u73b0\u6709\u7684\u96f6\u6837\u672c\u548c\u6307\u4ee4\u8c03\u4f18\u65b9\u6cd5\u96be\u4ee5\u6cdb\u5316\u5230\u9886\u57df\u7279\u5b9a\u5b9e\u4f53\uff0c\u4e14\u65e0\u6cd5\u6709\u6548\u5229\u7528\u6709\u9650\u6570\u636e\u3002", "method": "1) \u65b0\u7684\u6307\u4ee4\u8c03\u4f18\u6a21\u677f\uff0c\u7ed3\u5408\u5148\u524dIT\u65b9\u6cd5\u7684\u539f\u7406\uff0c\u5229\u7528\u6700\u65b0LLM\u7684\u5927\u4e0a\u4e0b\u6587\u7a97\u53e3\uff1b2) \u6218\u7565\u6027\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u5728\u4fdd\u6301\u5b9e\u4f53\u4fe1\u606f\u7684\u540c\u65f6\u5bf9\u4e0a\u4e0b\u6587\u8fdb\u884c\u6539\u5199\uff0c\u6269\u5c55\u8bad\u7ec3\u6570\u636e\u800c\u4e0d\u635f\u5bb3\u8bed\u4e49\u5173\u7cfb\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u5c11\u6837\u672c\u65b9\u6cd5\u5728CrossNER\u6570\u636e\u96c6\u4e0a\u5e73\u5747F1\u5f97\u5206\u4e3a80.1\u3002\u4f7f\u7528\u6539\u5199\u65b9\u6cd5\u8bad\u7ec3\u7684\u6a21\u578b\u6bd4\u57fa\u7ebf\u7248\u672cF1\u5206\u6570\u63d0\u5347\u9ad8\u8fbe17\u5206\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u62e5\u6709\u6709\u9650NER\u8bad\u7ec3\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\u7684\u7fa4\u4f53\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.16609", "categories": ["cs.LG", "cs.AI", "cs.CC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2510.16609", "abs": "https://arxiv.org/abs/2510.16609", "authors": ["Avrim Blum", "Daniel Hsu", "Cyrus Rashtchian", "Donya Saless"], "title": "Prior Makes It Possible: From Sublinear Graph Algorithms to LLM Test-Time Methods", "comment": null, "summary": "Test-time augmentation, such as Retrieval-Augmented Generation (RAG) or tool\nuse, critically depends on an interplay between a model's parametric knowledge\nand externally retrieved information. However, the theoretical underpinnings of\nthis relationship remain poorly understood. Specifically, it is not clear how\nmuch pre-training knowledge is required to answer queries with a small number\nof augmentation steps, which is a desirable property in practice. To address\nthis question, we formulate multi-step reasoning as an $s$-$t$ connectivity\nproblem on a knowledge graph. We represent a model's pre-training parametric\nknowledge as a partial, potentially noisy subgraph. We view augmentation as\nquerying an oracle for true edges that augment the model's knowledge. Then, we\ncharacterize the necessary and sufficient number of augmentation steps for the\nmodel to generate an accurate answer given partial prior knowledge. One key\nresult shows a phase transition: if the prior knowledge graph over $n$ vertices\nis disconnected into small components, then finding a path via augmentation is\ninefficient and requires $\\Omega(\\sqrt{n})$ queries. On the other hand, once\nthe density of correct knowledge surpasses a threshold, forming a giant\ncomponent, we can find paths with an expected constant number of queries.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u6d4b\u8bd5\u65f6\u589e\u5f3a\uff08\u5982RAG\u6216\u5de5\u5177\u4f7f\u7528\uff09\u4e2d\u6a21\u578b\u53c2\u6570\u77e5\u8bc6\u4e0e\u5916\u90e8\u68c0\u7d22\u4fe1\u606f\u7684\u5173\u7cfb\uff0c\u901a\u8fc7\u5c06\u591a\u6b65\u63a8\u7406\u5efa\u6a21\u4e3a\u77e5\u8bc6\u56fe\u4e0a\u7684\u8fde\u901a\u6027\u95ee\u9898\uff0c\u63ed\u793a\u4e86\u53c2\u6570\u77e5\u8bc6\u5bc6\u5ea6\u4e0e\u589e\u5f3a\u6548\u7387\u4e4b\u95f4\u7684\u76f8\u53d8\u73b0\u8c61\u3002", "motivation": "\u7406\u89e3\u6d4b\u8bd5\u65f6\u589e\u5f3a\u4e2d\u6a21\u578b\u53c2\u6570\u77e5\u8bc6\u4e0e\u5916\u90e8\u68c0\u7d22\u4fe1\u606f\u4e4b\u95f4\u7684\u7406\u8bba\u5173\u7cfb\uff0c\u7279\u522b\u662f\u786e\u5b9a\u5728\u5c11\u91cf\u589e\u5f3a\u6b65\u9aa4\u4e0b\u51c6\u786e\u56de\u7b54\u95ee\u9898\u6240\u9700\u7684\u6700\u5c0f\u9884\u8bad\u7ec3\u77e5\u8bc6\u91cf\u3002", "method": "\u5c06\u591a\u6b65\u63a8\u7406\u5efa\u6a21\u4e3a\u77e5\u8bc6\u56fe\u4e0a\u7684s-t\u8fde\u901a\u6027\u95ee\u9898\uff0c\u5c06\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u53c2\u6570\u77e5\u8bc6\u8868\u793a\u4e3a\u90e8\u5206\u4e14\u53ef\u80fd\u6709\u566a\u58f0\u7684\u5b50\u56fe\uff0c\u5c06\u589e\u5f3a\u89c6\u4e3a\u67e5\u8be2\u771f\u5b9e\u8fb9\u6765\u6269\u5c55\u6a21\u578b\u77e5\u8bc6\u3002", "result": "\u53d1\u73b0\u4e86\u4e00\u4e2a\u76f8\u53d8\u73b0\u8c61\uff1a\u5f53\u77e5\u8bc6\u56fe\u65ad\u5f00\u4e3a\u5c0f\u7ec4\u4ef6\u65f6\uff0c\u901a\u8fc7\u589e\u5f3a\u627e\u5230\u8def\u5f84\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u03a9(\u221an)\u6b21\u67e5\u8be2\uff1b\u4f46\u5f53\u6b63\u786e\u77e5\u8bc6\u5bc6\u5ea6\u8d85\u8fc7\u9608\u503c\u5f62\u6210\u5de8\u578b\u7ec4\u4ef6\u65f6\uff0c\u53ef\u4ee5\u7528\u671f\u671b\u5e38\u6570\u6b21\u67e5\u8be2\u627e\u5230\u8def\u5f84\u3002", "conclusion": "\u6d4b\u8bd5\u65f6\u589e\u5f3a\u7684\u6548\u7387\u53d6\u51b3\u4e8e\u6a21\u578b\u53c2\u6570\u77e5\u8bc6\u56fe\u7684\u8fde\u901a\u6027\uff0c\u5b58\u5728\u4e00\u4e2a\u4e34\u754c\u5bc6\u5ea6\u9608\u503c\uff0c\u8d85\u8fc7\u8be5\u9608\u503c\u540e\u589e\u5f3a\u6548\u7387\u663e\u8457\u63d0\u9ad8\u3002"}}
{"id": "2510.17725", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17725", "abs": "https://arxiv.org/abs/2510.17725", "authors": ["Haozhen Zhang", "Tao Feng", "Pengrui Han", "Jiaxuan You"], "title": "AcademicEval: Live Long-Context LLM Benchmark", "comment": "Accepted by TMLR. Code is available at\n  https://github.com/ulab-uiuc/AcademicEval", "summary": "Large Language Models (LLMs) have recently achieved remarkable performance in\nlong-context understanding. However, current long-context LLM benchmarks are\nlimited by rigid context length, labor-intensive annotation, and the pressing\nchallenge of label leakage issues during LLM training. Therefore, we propose\n\\textsc{AcademicEval}, a live benchmark for evaluating LLMs over long-context\ngeneration tasks. \\textsc{AcademicEval} adopts papers on arXiv to introduce\nseveral academic writing tasks with long-context inputs, \\textit{i.e.},\n\\textsc{Title}, \\textsc{Abstract}, \\textsc{Introduction}, and \\textsc{Related\nWork}, which cover a wide range of abstraction levels and require no manual\nlabeling. Moreover, \\textsc{AcademicEval} integrates high-quality and\nexpert-curated few-shot demonstrations from a collected co-author graph to\nenable flexible context length. Especially, \\textsc{AcademicEval} features an\nefficient live evaluation, ensuring no label leakage. We conduct a holistic\nevaluation on \\textsc{AcademicEval}, and the results illustrate that LLMs\nperform poorly on tasks with hierarchical abstraction levels and tend to\nstruggle with long few-shot demonstrations, highlighting the challenge of our\nbenchmark. Through experimental analysis, we also reveal some insights for\nenhancing LLMs' long-context modeling capabilities. Code is available at\nhttps://github.com/ulab-uiuc/AcademicEval", "AI": {"tldr": "\u63d0\u51fa\u4e86AcademicEval\uff0c\u4e00\u4e2a\u57fa\u4e8earXiv\u8bba\u6587\u7684\u957f\u4e0a\u4e0b\u6587\u751f\u6210\u4efb\u52a1\u57fa\u51c6\uff0c\u5305\u542b\u6807\u9898\u3001\u6458\u8981\u3001\u5f15\u8a00\u548c\u76f8\u5173\u5de5\u4f5c\u56db\u4e2a\u4efb\u52a1\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u4e14\u80fd\u9632\u6b62\u6807\u7b7e\u6cc4\u9732\u3002", "motivation": "\u5f53\u524d\u957f\u4e0a\u4e0b\u6587LLM\u57fa\u51c6\u5b58\u5728\u4e0a\u4e0b\u6587\u957f\u5ea6\u56fa\u5b9a\u3001\u6807\u6ce8\u52b3\u52a8\u5bc6\u96c6\u3001\u8bad\u7ec3\u65f6\u6807\u7b7e\u6cc4\u9732\u7b49\u95ee\u9898\uff0c\u9700\u8981\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528arXiv\u8bba\u6587\u6784\u5efa\u5b66\u672f\u5199\u4f5c\u4efb\u52a1\uff0c\u96c6\u6210\u9ad8\u8d28\u91cf\u4e13\u5bb6\u7b56\u5212\u7684\u5c11\u6837\u672c\u793a\u4f8b\uff0c\u652f\u6301\u7075\u6d3b\u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u91c7\u7528\u5b9e\u65f6\u8bc4\u4f30\u9632\u6b62\u6807\u7b7e\u6cc4\u9732\u3002", "result": "LLMs\u5728\u5177\u6709\u5c42\u6b21\u62bd\u8c61\u7ea7\u522b\u7684\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u96be\u4ee5\u5904\u7406\u957f\u5c11\u6837\u672c\u6f14\u793a\uff0c\u51f8\u663e\u4e86\u57fa\u51c6\u7684\u6311\u6218\u6027\u3002", "conclusion": "AcademicEval\u4e3a\u8bc4\u4f30LLMs\u7684\u957f\u4e0a\u4e0b\u6587\u5efa\u6a21\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u6548\u57fa\u51c6\uff0c\u5e76\u63ed\u793a\u4e86\u589e\u5f3a\u8fd9\u4e9b\u80fd\u529b\u7684\u89c1\u89e3\u3002"}}
{"id": "2510.16629", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16629", "abs": "https://arxiv.org/abs/2510.16629", "authors": ["Jiatong Yu", "Yinghui He", "Anirudh Goyal", "Sanjeev Arora"], "title": "On the Impossibility of Retrain Equivalence in Machine Unlearning", "comment": "Code available at\n  https://princeton-pli.github.io/impossibility-unlearning/", "summary": "Machine unlearning seeks to selectively remove the \"influence\" of specific\ntraining data on a model's outputs. The ideal goal is Retrain\nEquivalence--behavior identical to a model trained from scratch on only the\nretained data. This goal was formulated for models trained on i.i.d. data\nbatches, but modern pipelines often involve multi-stage training, with each\nstage having a distinct data distribution and objective. Examples include LLM\nfine-tuning for alignment, reasoning ability, etc. Our study shows via theory\nand experiments that this shift to multi-stage training introduces a\nfundamental barrier for machine unlearning. The theory indicates that the\noutcome of local unlearning--methods that only use gradients computed on the\nforget set--is path-dependent. That is, a model's behavior during unlearning is\ninfluenced by the order of its training stages during learning, making it\nimpossible for path-oblivious algorithms to universally achieve Retrain\nEquivalence. We empirically demonstrate the same phenomenon in LLM\npost-training across Llama and Qwen models (1B to 14B) with gradient ascent,\nNPO, and SimNPO local unlearning algorithms. Models fine-tuned via different\norderings of identical training stages diverge in behavior during unlearning,\nwith the degradation in GSM8K accuracy after unlearning varying by over 20%\nacross paths. We also observe that some learning paths consistently produce\nmodels that unlearn slowly. During unlearning, whether the probability mass\ngets squeezed into paraphrasing or alternative concepts is also path-dependent.\nThese results consistently show that Retrain Equivalence is an ill-posed target\nfor local unlearning algorithms, so long as the target models are trained in\nstages. In situations where access to models' training histories is hard, the\ncurrent work calls for rethinking the definition and desiderata of machine\nunlearning.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u5728\u591a\u9636\u6bb5\u8bad\u7ec3\u573a\u666f\u4e0b\uff0c\u5c40\u90e8\u9057\u5fd8\u7b97\u6cd5\u65e0\u6cd5\u666e\u904d\u5b9e\u73b0\u91cd\u8bad\u7ec3\u7b49\u4ef7\u6027\uff0c\u56e0\u4e3a\u9057\u5fd8\u7ed3\u679c\u5177\u6709\u8def\u5f84\u4f9d\u8d56\u6027\uff0c\u53d7\u8bad\u7ec3\u9636\u6bb5\u987a\u5e8f\u5f71\u54cd\u3002", "motivation": "\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u6d41\u6c34\u7ebf\u901a\u5e38\u6d89\u53ca\u591a\u9636\u6bb5\u8bad\u7ec3\uff08\u5982LLM\u5fae\u8c03\uff09\uff0c\u4f46\u73b0\u6709\u7684\u673a\u5668\u9057\u5fd8\u7406\u8bba\u4e3b\u8981\u9488\u5bf9i.i.d.\u6570\u636e\u8bad\u7ec3\uff0c\u9700\u8981\u7814\u7a76\u591a\u9636\u6bb5\u8bad\u7ec3\u5bf9\u9057\u5fd8\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u4f7f\u7528\u68af\u5ea6\u4e0a\u5347\u3001NPO\u548cSimNPO\u7b49\u5c40\u90e8\u9057\u5fd8\u7b97\u6cd5\uff0c\u5728Llama\u548cQwen\u6a21\u578b\uff081B\u523014B\uff09\u4e0a\u8fdb\u884c\u591a\u9636\u6bb5\u8bad\u7ec3\u548c\u9057\u5fd8\u5b9e\u9a8c\u3002", "result": "\u4e0d\u540c\u8bad\u7ec3\u8def\u5f84\u7684\u6a21\u578b\u5728\u9057\u5fd8\u8fc7\u7a0b\u4e2d\u884c\u4e3a\u5dee\u5f02\u663e\u8457\uff0cGSM8K\u51c6\u786e\u7387\u4e0b\u964d\u5dee\u5f02\u8d85\u8fc720%\uff0c\u67d0\u4e9b\u5b66\u4e60\u8def\u5f84\u4ea7\u751f\u96be\u4ee5\u9057\u5fd8\u7684\u6a21\u578b\uff0c\u6982\u7387\u8d28\u91cf\u5206\u5e03\u4e5f\u5448\u73b0\u8def\u5f84\u4f9d\u8d56\u6027\u3002", "conclusion": "\u91cd\u8bad\u7ec3\u7b49\u4ef7\u6027\u5bf9\u4e8e\u5c40\u90e8\u9057\u5fd8\u7b97\u6cd5\u6765\u8bf4\u662f\u4e00\u4e2a\u4e0d\u9002\u5b9a\u7684\u76ee\u6807\uff0c\u9700\u8981\u91cd\u65b0\u601d\u8003\u673a\u5668\u9057\u5fd8\u7684\u5b9a\u4e49\u548c\u671f\u671b\u76ee\u6807\uff0c\u7279\u522b\u662f\u5728\u96be\u4ee5\u83b7\u53d6\u8bad\u7ec3\u5386\u53f2\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2510.17733", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17733", "abs": "https://arxiv.org/abs/2510.17733", "authors": ["Tong Chen", "Akari Asai", "Luke Zettlemoyer", "Hannaneh Hajishirzi", "Faeze Brahman"], "title": "Train for Truth, Keep the Skills: Binary Retrieval-Augmented Reward Mitigates Hallucinations", "comment": null, "summary": "Language models often generate factually incorrect information unsupported by\ntheir training data, a phenomenon known as extrinsic hallucination. Existing\nmitigation approaches often degrade performance on open-ended generation and\ndownstream tasks, limiting their practical utility. We propose an online\nreinforcement learning method using a novel binary retrieval-augmented reward\n(RAR) to address this tradeoff. Unlike continuous reward schemes, our approach\nassigns a reward of one only when the model's output is entirely factually\ncorrect, and zero otherwise. We evaluate our method on Qwen3 reasoning models\nacross diverse tasks. For open-ended generation, binary RAR achieves a 39.3%\nreduction in hallucination rates, substantially outperforming both supervised\ntraining and continuous-reward RL baselines. In short-form question answering,\nthe model learns calibrated abstention, strategically outputting \"I don't know\"\nwhen faced with insufficient parametric knowledge. This yields 44.4% and 21.7%\nfewer incorrect answers on PopQA and GPQA, respectively. Crucially, these\nfactuality gains come without performance degradation on instruction following,\nmath, or code, whereas continuous-reward RL, despite improving factuality,\ninduces quality regressions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8c\u5143\u68c0\u7d22\u589e\u5f3a\u5956\u52b1\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u6709\u6548\u51cf\u5c11\u8bed\u8a00\u6a21\u578b\u7684\u5916\u6e90\u6027\u5e7b\u89c9\uff0c\u5728\u4fdd\u6301\u5176\u4ed6\u4efb\u52a1\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e8b\u5b9e\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u51cf\u5c11\u8bed\u8a00\u6a21\u578b\u5916\u6e90\u6027\u5e7b\u89c9\u7684\u540c\u65f6\uff0c\u5f80\u5f80\u4f1a\u964d\u4f4e\u5f00\u653e\u751f\u6210\u548c\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "method": "\u4f7f\u7528\u4e8c\u5143\u68c0\u7d22\u589e\u5f3a\u5956\u52b1\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u53ea\u6709\u5f53\u6a21\u578b\u8f93\u51fa\u5b8c\u5168\u6b63\u786e\u65f6\u624d\u7ed9\u4e88\u5956\u52b11\uff0c\u5426\u5219\u4e3a0\u3002", "result": "\u5728\u5f00\u653e\u751f\u6210\u4efb\u52a1\u4e2d\u5e7b\u89c9\u7387\u964d\u4f4e39.3%\uff1b\u5728\u77ed\u95ee\u7b54\u4efb\u52a1\u4e2d\uff0c\u6a21\u578b\u5b66\u4f1a\u6821\u51c6\u6027\u5f03\u6743\uff0c\u5728PopQA\u548cGPQA\u4e0a\u5206\u522b\u51cf\u5c1144.4%\u548c21.7%\u7684\u9519\u8bef\u7b54\u6848\uff0c\u4e14\u4e0d\u5f71\u54cd\u6307\u4ee4\u9075\u5faa\u3001\u6570\u5b66\u548c\u4ee3\u7801\u80fd\u529b\u3002", "conclusion": "\u4e8c\u5143\u5956\u52b1\u65b9\u6848\u5728\u63d0\u5347\u4e8b\u5b9e\u51c6\u786e\u6027\u7684\u540c\u65f6\u907f\u514d\u4e86\u6027\u80fd\u9000\u5316\uff0c\u76f8\u6bd4\u8fde\u7eed\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u5177\u6709\u66f4\u597d\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2510.16656", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2510.16656", "abs": "https://arxiv.org/abs/2510.16656", "authors": ["Noah El Rimawi-Fine", "Adam Stecklov", "Lucas Nelson", "Mathieu Blanchette", "Alexander Tong", "Stephen Y. Zhang", "Lazar Atanackovic"], "title": "Simulation-free Structure Learning for Stochastic Dynamics", "comment": null, "summary": "Modeling dynamical systems and unraveling their underlying causal\nrelationships is central to many domains in the natural sciences. Various\nphysical systems, such as those arising in cell biology, are inherently\nhigh-dimensional and stochastic in nature, and admit only partial, noisy state\nmeasurements. This poses a significant challenge for addressing the problems of\nmodeling the underlying dynamics and inferring the network structure of these\nsystems. Existing methods are typically tailored either for structure learning\nor modeling dynamics at the population level, but are limited in their ability\nto address both problems together. In this work, we address both problems\nsimultaneously: we present StructureFlow, a novel and principled\nsimulation-free approach for jointly learning the structure and stochastic\npopulation dynamics of physical systems. We showcase the utility of\nStructureFlow for the tasks of structure learning from interventions and\ndynamical (trajectory) inference of conditional population dynamics. We\nempirically evaluate our approach on high-dimensional synthetic systems, a set\nof biologically plausible simulated systems, and an experimental single-cell\ndataset. We show that StructureFlow can learn the structure of underlying\nsystems while simultaneously modeling their conditional population dynamics --\na key step toward the mechanistic understanding of systems behavior.", "AI": {"tldr": "StructureFlow\u662f\u4e00\u79cd\u65b0\u9896\u7684\u65e0\u4eff\u771f\u65b9\u6cd5\uff0c\u80fd\u591f\u540c\u65f6\u5b66\u4e60\u7269\u7406\u7cfb\u7edf\u7684\u7ed3\u6784\u548c\u968f\u673a\u7fa4\u4f53\u52a8\u529b\u5b66\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u540c\u65f6\u5904\u7406\u7ed3\u6784\u5b66\u4e60\u548c\u52a8\u529b\u5b66\u5efa\u6a21\u7684\u95ee\u9898\u3002", "motivation": "\u8bb8\u591a\u81ea\u7136\u7cfb\u7edf\u4e2d\u7684\u7269\u7406\u7cfb\u7edf\uff08\u5982\u7ec6\u80de\u751f\u7269\u5b66\uff09\u672c\u8d28\u4e0a\u662f\u9ad8\u7ef4\u548c\u968f\u673a\u7684\uff0c\u53ea\u80fd\u83b7\u5f97\u90e8\u5206\u566a\u58f0\u72b6\u6001\u6d4b\u91cf\uff0c\u8fd9\u7ed9\u5efa\u6a21\u5e95\u5c42\u52a8\u529b\u5b66\u548c\u63a8\u65ad\u7f51\u7edc\u7ed3\u6784\u5e26\u6765\u4e86\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u53ea\u80fd\u5355\u72ec\u5904\u7406\u7ed3\u6784\u5b66\u4e60\u6216\u7fa4\u4f53\u5c42\u9762\u7684\u52a8\u529b\u5b66\u5efa\u6a21\u3002", "method": "\u63d0\u51faStructureFlow\u65b9\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u539f\u5219\u6027\u7684\u65e0\u4eff\u771f\u65b9\u6cd5\uff0c\u80fd\u591f\u8054\u5408\u5b66\u4e60\u7269\u7406\u7cfb\u7edf\u7684\u7ed3\u6784\u548c\u968f\u673a\u7fa4\u4f53\u52a8\u529b\u5b66\u3002\u8be5\u65b9\u6cd5\u652f\u6301\u4ece\u5e72\u9884\u4e2d\u5b66\u4e60\u7ed3\u6784\u4ee5\u53ca\u6761\u4ef6\u7fa4\u4f53\u52a8\u529b\u5b66\u7684\u52a8\u6001\uff08\u8f68\u8ff9\uff09\u63a8\u65ad\u3002", "result": "\u5728\u5408\u6210\u9ad8\u7ef4\u7cfb\u7edf\u3001\u751f\u7269\u5b66\u4e0a\u5408\u7406\u7684\u6a21\u62df\u7cfb\u7edf\u548c\u5b9e\u9a8c\u6027\u5355\u7ec6\u80de\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0cStructureFlow\u80fd\u591f\u5b66\u4e60\u5e95\u5c42\u7cfb\u7edf\u7684\u7ed3\u6784\uff0c\u540c\u65f6\u5efa\u6a21\u5176\u6761\u4ef6\u7fa4\u4f53\u52a8\u529b\u5b66\u3002", "conclusion": "StructureFlow\u80fd\u591f\u540c\u65f6\u5b66\u4e60\u7cfb\u7edf\u7ed3\u6784\u548c\u6761\u4ef6\u7fa4\u4f53\u52a8\u529b\u5b66\uff0c\u8fd9\u662f\u7406\u89e3\u7cfb\u7edf\u884c\u4e3a\u673a\u5236\u7684\u5173\u952e\u4e00\u6b65\u3002"}}
{"id": "2510.17764", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17764", "abs": "https://arxiv.org/abs/2510.17764", "authors": ["Xiao Ye", "Jacob Dineen", "Zhaonan Li", "Zhikun Xu", "Weiyu Chen", "Shijie Lu", "Yuxi Huang", "Ming Shen", "Phu Tran", "Ji-Eun Irene Yum", "Muhammad Ali Khan", "Muhammad Umar Afzal", "Irbaz Bin Riaz", "Ben Zhou"], "title": "Evaluating Medical LLMs by Levels of Autonomy: A Survey Moving from Benchmarks to Applications", "comment": null, "summary": "Medical Large language models achieve strong scores on standard benchmarks;\nhowever, the transfer of those results to safe and reliable performance in\nclinical workflows remains a challenge. This survey reframes evaluation through\na levels-of-autonomy lens (L0-L3), spanning informational tools, information\ntransformation and aggregation, decision support, and supervised agents. We\nalign existing benchmarks and metrics with the actions permitted at each level\nand their associated risks, making the evaluation targets explicit. This\nmotivates a level-conditioned blueprint for selecting metrics, assembling\nevidence, and reporting claims, alongside directions that link evaluation to\noversight. By centering autonomy, the survey moves the field beyond score-based\nclaims toward credible, risk-aware evidence for real clinical use.", "AI": {"tldr": "\u8be5\u8c03\u67e5\u901a\u8fc7\u81ea\u4e3b\u6027\u5c42\u7ea7\u6846\u67b6(L0-L3)\u91cd\u65b0\u6784\u5efa\u533b\u5b66\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\uff0c\u5c06\u73b0\u6709\u57fa\u51c6\u4e0e\u5404\u5c42\u7ea7\u5141\u8bb8\u7684\u64cd\u4f5c\u548c\u98ce\u9669\u5bf9\u9f50\uff0c\u63d0\u51fa\u57fa\u4e8e\u5c42\u7ea7\u7684\u8bc4\u4f30\u84dd\u56fe\uff0c\u63a8\u52a8\u4ece\u5206\u6570\u5bfc\u5411\u8f6c\u5411\u53ef\u4fe1\u7684\u4e34\u5e8a\u4f7f\u7528\u8bc1\u636e\u3002", "motivation": "\u533b\u5b66\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6807\u51c6\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5c06\u8fd9\u4e9b\u7ed3\u679c\u8f6c\u5316\u4e3a\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u5b89\u5168\u53ef\u9760\u7684\u6027\u80fd\u4ecd\u5177\u6311\u6218\u3002\u9700\u8981\u91cd\u65b0\u6784\u5efa\u8bc4\u4f30\u6846\u67b6\u4ee5\u8fde\u63a5\u57fa\u51c6\u6d4b\u8bd5\u4e0e\u5b9e\u9645\u4e34\u5e8a\u4f7f\u7528\u3002", "method": "\u91c7\u7528\u81ea\u4e3b\u6027\u5c42\u7ea7\u6846\u67b6(L0-L3)\uff1a\u4fe1\u606f\u5de5\u5177\u3001\u4fe1\u606f\u8f6c\u6362\u4e0e\u805a\u5408\u3001\u51b3\u7b56\u652f\u6301\u3001\u76d1\u7763\u4ee3\u7406\u3002\u5c06\u73b0\u6709\u57fa\u51c6\u548c\u6307\u6807\u4e0e\u5404\u5c42\u7ea7\u7684\u5141\u8bb8\u64cd\u4f5c\u53ca\u98ce\u9669\u5bf9\u9f50\uff0c\u5efa\u7acb\u660e\u786e\u7684\u8bc4\u4f30\u76ee\u6807\u3002", "result": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u5c42\u7ea7\u7684\u8bc4\u4f30\u84dd\u56fe\uff0c\u7528\u4e8e\u9009\u62e9\u6307\u6807\u3001\u6536\u96c6\u8bc1\u636e\u548c\u62a5\u544a\u58f0\u660e\uff0c\u5e76\u5c06\u8bc4\u4f30\u4e0e\u76d1\u7763\u8054\u7cfb\u8d77\u6765\u3002", "conclusion": "\u901a\u8fc7\u4ee5\u81ea\u4e3b\u6027\u4e3a\u4e2d\u5fc3\uff0c\u8be5\u8c03\u67e5\u63a8\u52a8\u9886\u57df\u8d85\u8d8a\u57fa\u4e8e\u5206\u6570\u7684\u58f0\u660e\uff0c\u8f6c\u5411\u4e3a\u771f\u5b9e\u4e34\u5e8a\u4f7f\u7528\u63d0\u4f9b\u53ef\u4fe1\u3001\u98ce\u9669\u611f\u77e5\u7684\u8bc1\u636e\u3002"}}
{"id": "2510.16674", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2510.16674", "abs": "https://arxiv.org/abs/2510.16674", "authors": ["Azam Shirali", "Giri Narasimhan"], "title": "Evaluating protein binding interfaces with PUMBA", "comment": null, "summary": "Protein-protein docking tools help in studying interactions between proteins,\nand are essential for drug, vaccine, and therapeutic development. However, the\naccuracy of a docking tool depends on a robust scoring function that can\nreliably differentiate between native and non-native complexes. PIsToN is a\nstate-of-the-art deep learning-based scoring function that uses Vision\nTransformers in its architecture. Recently, the Mamba architecture has\ndemonstrated exceptional performance in both natural language processing and\ncomputer vision, often outperforming Transformer-based models in their domains.\nIn this study, we introduce PUMBA (Protein-protein interface evaluation with\nVision Mamba), which improves PIsToN by replacing its Vision Transformer\nbackbone with Vision Mamba. This change allows us to leverage Mamba's efficient\nlong-range sequence modeling for sequences of image patches. As a result, the\nmodel's ability to capture both global and local patterns in protein-protein\ninterface features is significantly improved. Evaluation on several\nwidely-used, large-scale public datasets demonstrates that PUMBA consistently\noutperforms its original Transformer-based predecessor, PIsToN.", "AI": {"tldr": "PUMBA\u662f\u4e00\u4e2a\u57fa\u4e8eVision Mamba\u67b6\u6784\u7684\u86cb\u767d\u8d28-\u86cb\u767d\u8d28\u5bf9\u63a5\u8bc4\u5206\u51fd\u6570\uff0c\u901a\u8fc7\u66ff\u6362PIsToN\u4e2d\u7684Vision Transformer\u4e3aVision Mamba\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u86cb\u767d\u8d28-\u86cb\u767d\u8d28\u5bf9\u63a5\u5de5\u5177\u4f9d\u8d56\u51c6\u786e\u7684\u8bc4\u5206\u51fd\u6570\u6765\u533a\u5206\u539f\u751f\u548c\u975e\u539f\u751f\u590d\u5408\u7269\u3002Vision Mamba\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u6709\u671b\u63d0\u5347\u86cb\u767d\u8d28\u5bf9\u63a5\u7684\u51c6\u786e\u6027\u3002", "method": "\u5c06PIsToN\u4e2d\u7684Vision Transformer\u4e3b\u5e72\u66ff\u6362\u4e3aVision Mamba\u67b6\u6784\uff0c\u5229\u7528Mamba\u5728\u56fe\u50cf\u5757\u5e8f\u5217\u4e0a\u7684\u9ad8\u6548\u957f\u7a0b\u5e8f\u5217\u5efa\u6a21\u80fd\u529b\u3002", "result": "\u5728\u591a\u4e2a\u5927\u89c4\u6a21\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cPUMBA\u59cb\u7ec8\u4f18\u4e8e\u5176\u57fa\u4e8eTransformer\u7684\u524d\u8eabPIsToN\u3002", "conclusion": "Vision Mamba\u67b6\u6784\u80fd\u591f\u663e\u8457\u63d0\u5347\u86cb\u767d\u8d28-\u86cb\u767d\u8d28\u754c\u9762\u7279\u5f81\u4e2d\u5168\u5c40\u548c\u5c40\u90e8\u6a21\u5f0f\u7684\u6355\u6349\u80fd\u529b\uff0c\u4e3a\u86cb\u767d\u8d28\u5bf9\u63a5\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u8bc4\u5206\u51fd\u6570\u3002"}}
{"id": "2510.17793", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17793", "abs": "https://arxiv.org/abs/2510.17793", "authors": ["Austin Xu", "Xuan-Phi Nguyen", "Yilun Zhou", "Chien-Sheng Wu", "Caiming Xiong", "Shafiq Joty"], "title": "Foundational Automatic Evaluators: Scaling Multi-Task Generative Evaluator Training for Reasoning-Centric Domains", "comment": "29 pages, 9 tables, 6 figures", "summary": "Finetuning specialized generative evaluators has emerged as a popular\nparadigm to meet the increasing demand for scalable evaluation during both\ntraining and test-time. However, recent work has largely focused on applying\nnew methodology, such as reinforcement learning (RL), to training evaluators,\nshying away from large-scale, data-driven development. In this work, we focus\non data scaling, curating a set of 2.5M samples spanning five unique evaluation\ntasks (pairwise, step-level, reference-free and reference-based verification,\nand single rating) and multiple domains focused on reasoning evaluation. With\nour data, we train Foundational Automatic Reasoning Evaluators (FARE), a family\nof 8B and 20B (with 3.6B active) parameter evaluators, with a simple iterative\nrejection-sampling supervised finetuning (SFT) approach. FARE-8B challenges\nlarger specialized RL-trained evaluators and FARE-20B sets the new standard for\nopen-source evaluators, surpassing specialized 70B+ evaluators. Beyond static\nbenchmarks, we evaluate FARE in real-world tasks: As inference-time rerankers,\nFARE-20B achieves near-oracle performance on MATH. As verifiers in RL training,\nFARE improves the downstream RL-trained model performance by up to 14.1% vs.\nstring-matching verifiers. When initialized from FARE, a continually-finetuned\nFARE-Code outperforms gpt-oss-20B by 65% on evaluating test-case quality.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86FARE\uff08\u57fa\u7840\u81ea\u52a8\u63a8\u7406\u8bc4\u4f30\u5668\uff09\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u8bad\u7ec3\u4e868B\u548c20B\u53c2\u6570\u7684\u8bc4\u4f30\u5668\uff0c\u5728\u591a\u4e2a\u8bc4\u4f30\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e86\u66f4\u5927\u7684\u4e13\u4e1a\u8bc4\u4f30\u5668\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5f53\u524d\u751f\u6210\u5f0f\u8bc4\u4f30\u5668\u7684\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u65b0\u65b9\u6cd5\u5982\u5f3a\u5316\u5b66\u4e60\uff0c\u800c\u5ffd\u89c6\u4e86\u5927\u89c4\u6a21\u6570\u636e\u9a71\u52a8\u7684\u53d1\u5c55\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6570\u636e\u6269\u5c55\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u6536\u96c6\u4e86250\u4e07\u4e2a\u6837\u672c\uff0c\u6db5\u76d65\u79cd\u8bc4\u4f30\u4efb\u52a1\uff0c\u4f7f\u7528\u7b80\u5355\u7684\u8fed\u4ee3\u62d2\u7edd\u91c7\u6837\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u8bad\u7ec3\u4e868B\u548c20B\u53c2\u6570\u7684FARE\u8bc4\u4f30\u5668\u3002", "result": "FARE-8B\u6311\u6218\u4e86\u66f4\u5927\u7684\u4e13\u4e1aRL\u8bad\u7ec3\u8bc4\u4f30\u5668\uff0cFARE-20B\u5728\u5f00\u6e90\u8bc4\u4f30\u5668\u4e2d\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\uff0c\u8d85\u8d8a\u4e86\u4e13\u95e8\u768470B+\u8bc4\u4f30\u5668\u3002\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u4f5c\u4e3a\u63a8\u7406\u65f6\u91cd\u6392\u5668\u5728MATH\u4e0a\u8fbe\u5230\u63a5\u8fd1oracle\u6027\u80fd\uff0c\u4f5c\u4e3aRL\u8bad\u7ec3\u4e2d\u7684\u9a8c\u8bc1\u5668\u5c06\u4e0b\u6e38\u6a21\u578b\u6027\u80fd\u63d0\u534714.1%\u3002", "conclusion": "\u5927\u89c4\u6a21\u6570\u636e\u9a71\u52a8\u7684\u7b80\u5355\u65b9\u6cd5\u53ef\u4ee5\u8bad\u7ec3\u51fa\u9ad8\u6027\u80fd\u7684\u8bc4\u4f30\u5668\uff0cFARE\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u6570\u636e\u6269\u5c55\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.16676", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16676", "abs": "https://arxiv.org/abs/2510.16676", "authors": ["Anindya Sarkar", "Binglin Ji", "Yevgeniy Vorobeychik"], "title": "Active Target Discovery under Uninformative Prior: The Power of Permanent and Transient Memory", "comment": "32 pages, 20 figures, Accepted to NeurIPS 2025", "summary": "In many scientific and engineering fields, where acquiring high-quality data\nis expensive--such as medical imaging, environmental monitoring, and remote\nsensing--strategic sampling of unobserved regions based on prior observations\nis crucial for maximizing discovery rates within a constrained budget. The rise\nof powerful generative models, such as diffusion models, has enabled active\ntarget discovery in partially observable environments by leveraging learned\npriors--probabilistic representations that capture underlying structure from\ndata. With guidance from sequentially gathered task-specific observations,\nthese models can progressively refine exploration and efficiently direct\nqueries toward promising regions. However, in domains where learning a strong\nprior is infeasible due to extremely limited data or high sampling cost (such\nas rare species discovery, diagnostics for emerging diseases, etc.), these\nmethods struggle to generalize. To overcome this limitation, we propose a novel\napproach that enables effective active target discovery even in settings with\nuninformative priors, ensuring robust exploration and adaptability in complex\nreal-world scenarios. Our framework is theoretically principled and draws\ninspiration from neuroscience to guide its design. Unlike black-box policies,\nour approach is inherently interpretable, providing clear insights into\ndecision-making. Furthermore, it guarantees a strong, monotonic improvement in\nprior estimates with each new observation, leading to increasingly accurate\nsampling and reinforcing both reliability and adaptability in dynamic settings.\nThrough comprehensive experiments and ablation studies across various domains,\nincluding species distribution modeling and remote sensing, we demonstrate that\nour method substantially outperforms baseline approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5728\u65e0\u4fe1\u606f\u5148\u9a8c\u6761\u4ef6\u4e0b\u8fdb\u884c\u6709\u6548\u4e3b\u52a8\u76ee\u6807\u53d1\u73b0\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u7406\u8bba\u4f9d\u636e\u3001\u795e\u7ecf\u79d1\u5b66\u542f\u53d1\u3001\u53ef\u89e3\u91ca\u6027\u5f3a\uff0c\u5e76\u5728\u591a\u79cd\u9886\u57df\u5b9e\u9a8c\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5728\u6570\u636e\u83b7\u53d6\u6210\u672c\u9ad8\u6602\u7684\u9886\u57df\uff08\u5982\u533b\u5b66\u6210\u50cf\u3001\u73af\u5883\u76d1\u6d4b\uff09\uff0c\u4f20\u7edf\u57fa\u4e8e\u5f3a\u5148\u9a8c\u7684\u751f\u6210\u6a21\u578b\u5728\u6570\u636e\u6781\u5176\u6709\u9650\u6216\u91c7\u6837\u6210\u672c\u9ad8\u7684\u573a\u666f\u4e0b\u96be\u4ee5\u6cdb\u5316\uff0c\u9700\u8981\u89e3\u51b3\u65e0\u4fe1\u606f\u5148\u9a8c\u6761\u4ef6\u4e0b\u7684\u4e3b\u52a8\u76ee\u6807\u53d1\u73b0\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7406\u8bba\u4e0a\u6709\u4f9d\u636e\u3001\u53d7\u795e\u7ecf\u79d1\u5b66\u542f\u53d1\u7684\u6846\u67b6\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u5185\u5728\u53ef\u89e3\u91ca\u6027\uff0c\u80fd\u4fdd\u8bc1\u6bcf\u6b21\u65b0\u89c2\u6d4b\u5e26\u6765\u5148\u9a8c\u4f30\u8ba1\u7684\u5355\u8c03\u6539\u8fdb\uff0c\u5b9e\u73b0\u8d8a\u6765\u8d8a\u51c6\u786e\u7684\u91c7\u6837\u3002", "result": "\u901a\u8fc7\u8de8\u591a\u4e2a\u9886\u57df\uff08\u5305\u62ec\u7269\u79cd\u5206\u5e03\u5efa\u6a21\u548c\u9065\u611f\uff09\u7684\u7efc\u5408\u5b9e\u9a8c\u548c\u6d88\u878d\u7814\u7a76\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u65e0\u4fe1\u606f\u5148\u9a8c\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u7a33\u5065\u7684\u63a2\u7d22\u548c\u9002\u5e94\u6027\uff0c\u4e3a\u590d\u6742\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u4e3b\u52a8\u76ee\u6807\u53d1\u73b0\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.17795", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17795", "abs": "https://arxiv.org/abs/2510.17795", "authors": ["Yujie Luo", "Zhuoyun Yu", "Xuehai Wang", "Yuqi Zhu", "Ningyu Zhang", "Lanning Wei", "Lun Du", "Da Zheng", "Huajun Chen"], "title": "Executable Knowledge Graphs for Replicating AI Research", "comment": "Work in progress", "summary": "Replicating AI research is a crucial yet challenging task for large language\nmodel (LLM) agents. Existing approaches often struggle to generate executable\ncode, primarily due to insufficient background knowledge and the limitations of\nretrieval-augmented generation (RAG) methods, which fail to capture latent\ntechnical details hidden in referenced papers. Furthermore, previous approaches\ntend to overlook valuable implementation-level code signals and lack structured\nknowledge representations that support multi-granular retrieval and reuse. To\novercome these challenges, we propose Executable Knowledge Graphs (xKG), a\nmodular and pluggable knowledge base that automatically integrates technical\ninsights, code snippets, and domain-specific knowledge extracted from\nscientific literature. When integrated into three agent frameworks with two\ndifferent LLMs, xKG shows substantial performance gains (10.9% with o3-mini) on\nPaperBench, demonstrating its effectiveness as a general and extensible\nsolution for automated AI research replication. Code will released at\nhttps://github.com/zjunlp/xKG.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u53ef\u6267\u884c\u77e5\u8bc6\u56fe\u8c31(xKG)\uff0c\u901a\u8fc7\u6574\u5408\u6280\u672f\u6d1e\u5bdf\u3001\u4ee3\u7801\u7247\u6bb5\u548c\u9886\u57df\u77e5\u8bc6\u6765\u89e3\u51b3AI\u7814\u7a76\u590d\u73b0\u7684\u6311\u6218\uff0c\u5728\u4e09\u4e2a\u4ee3\u7406\u6846\u67b6\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u53ef\u6267\u884c\u4ee3\u7801\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u4e3b\u8981\u7531\u4e8e\u80cc\u666f\u77e5\u8bc6\u4e0d\u8db3\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u6355\u6349\u53c2\u8003\u6587\u732e\u4e2d\u7684\u6f5c\u5728\u6280\u672f\u7ec6\u8282", "method": "\u6784\u5efa\u53ef\u6267\u884c\u77e5\u8bc6\u56fe\u8c31(xKG)\uff0c\u81ea\u52a8\u4ece\u79d1\u5b66\u6587\u732e\u4e2d\u63d0\u53d6\u6280\u672f\u6d1e\u5bdf\u3001\u4ee3\u7801\u7247\u6bb5\u548c\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\uff0c\u63d0\u4f9b\u6a21\u5757\u5316\u3001\u53ef\u63d2\u62d4\u7684\u77e5\u8bc6\u5e93", "result": "\u5728\u4e09\u4e2a\u4ee3\u7406\u6846\u67b6\u548c\u4e24\u79cd\u4e0d\u540cLLM\u4e0a\u6d4b\u8bd5\uff0cxKG\u5728PaperBench\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347(10.9% with o3-mini)", "conclusion": "xKG\u662f\u81ea\u52a8\u5316AI\u7814\u7a76\u590d\u73b0\u7684\u901a\u7528\u4e14\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027"}}
{"id": "2510.16677", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16677", "abs": "https://arxiv.org/abs/2510.16677", "authors": ["Ran Tong", "Jiaqi Liu", "Su Liu", "Xin Hu", "Lanruo Wang"], "title": "Renaissance of RNNs in Streaming Clinical Time Series: Compact Recurrence Remains Competitive with Transformers", "comment": null, "summary": "We present a compact, strictly causal benchmark for streaming clinical time\nseries on the MIT--BIH Arrhythmia Database using per-second heart rate. Two\ntasks are studied under record-level, non-overlapping splits: near-term\ntachycardia risk (next ten seconds) and one-step heart rate forecasting. We\ncompare a GRU-D (RNN) and a Transformer under matched training budgets against\nstrong non-learned baselines. Evaluation is calibration-aware for\nclassification and proper for forecasting, with temperature scaling and grouped\nbootstrap confidence intervals. On MIT-BIH, GRU-D slightly surpasses the\nTransformer for tachycardia risk, while the Transformer clearly lowers\nforecasting error relative to GRU-D and persistence. Our results show that, in\nlongitudinal monitoring, model choice is task-dependent: compact RNNs remain\ncompetitive for short-horizon risk scoring, whereas compact Transformers\ndeliver clearer gains for point forecasting.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7d27\u51d1\u7684\u4e25\u683c\u56e0\u679c\u57fa\u51c6\uff0c\u7528\u4e8e\u5728MIT-BIH\u5fc3\u5f8b\u5931\u5e38\u6570\u636e\u5e93\u4e0a\u4f7f\u7528\u6bcf\u79d2\u5fc3\u7387\u8fdb\u884c\u6d41\u5f0f\u4e34\u5e8a\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u3002\u7814\u7a76\u4e86\u4e24\u4e2a\u4efb\u52a1\uff1a\u77ed\u671f\u5fc3\u52a8\u8fc7\u901f\u98ce\u9669\u9884\u6d4b\u548c\u4e00\u6b65\u5fc3\u7387\u9884\u6d4b\uff0c\u6bd4\u8f83\u4e86GRU-D\u548cTransformer\u6a21\u578b\u3002", "motivation": "\u5728\u7eb5\u5411\u76d1\u6d4b\u4e2d\uff0c\u6a21\u578b\u9009\u62e9\u662f\u4efb\u52a1\u4f9d\u8d56\u7684\uff0c\u9700\u8981\u8bc4\u4f30\u4e0d\u540c\u6a21\u578b\u5728\u4e34\u5e8a\u65f6\u95f4\u5e8f\u5217\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u5dee\u5f02\u3002", "method": "\u4f7f\u7528MIT-BIH\u5fc3\u5f8b\u5931\u5e38\u6570\u636e\u5e93\u7684\u6bcf\u79d2\u5fc3\u7387\u6570\u636e\uff0c\u91c7\u7528\u8bb0\u5f55\u7ea7\u975e\u91cd\u53e0\u5206\u5272\u3002\u6bd4\u8f83GRU-D\uff08RNN\uff09\u548cTransformer\u6a21\u578b\uff0c\u5728\u5339\u914d\u7684\u8bad\u7ec3\u9884\u7b97\u4e0b\u4e0e\u5f3a\u975e\u5b66\u4e60\u57fa\u7ebf\u5bf9\u6bd4\u3002\u8bc4\u4f30\u91c7\u7528\u5206\u7c7b\u7684\u6e29\u5ea6\u7f29\u653e\u548c\u9884\u6d4b\u7684\u5206\u7ec4bootstrap\u7f6e\u4fe1\u533a\u95f4\u3002", "result": "\u5728MIT-BIH\u4e0a\uff0cGRU-D\u5728\u5fc3\u52a8\u8fc7\u901f\u98ce\u9669\u9884\u6d4b\u4e0a\u7565\u4f18\u4e8eTransformer\uff0c\u800cTransformer\u5728\u9884\u6d4b\u8bef\u5dee\u4e0a\u660e\u663e\u4f4e\u4e8eGRU-D\u548c\u6301\u4e45\u6027\u6a21\u578b\u3002", "conclusion": "\u7d27\u51d1RNN\u5728\u77ed\u65f6\u57df\u98ce\u9669\u8bc4\u5206\u4e2d\u4ecd\u5177\u7ade\u4e89\u529b\uff0c\u800c\u7d27\u51d1Transformer\u5728\u70b9\u9884\u6d4b\u65b9\u9762\u5e26\u6765\u66f4\u6e05\u6670\u7684\u6536\u76ca\u3002"}}
{"id": "2510.17797", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17797", "abs": "https://arxiv.org/abs/2510.17797", "authors": ["Akshara Prabhakar", "Roshan Ram", "Zixiang Chen", "Silvio Savarese", "Frank Wang", "Caiming Xiong", "Huan Wang", "Weiran Yao"], "title": "Enterprise Deep Research: Steerable Multi-Agent Deep Research for Enterprise Analytics", "comment": "Technical report; 13 pages plus references and appendices", "summary": "As information grows exponentially, enterprises face increasing pressure to\ntransform unstructured data into coherent, actionable insights. While\nautonomous agents show promise, they often struggle with domain-specific\nnuances, intent alignment, and enterprise integration. We present Enterprise\nDeep Research (EDR), a multi-agent system that integrates (1) a Master Planning\nAgent for adaptive query decomposition, (2) four specialized search agents\n(General, Academic, GitHub, LinkedIn), (3) an extensible MCP-based tool\necosystem supporting NL2SQL, file analysis, and enterprise workflows, (4) a\nVisualization Agent for data-driven insights, and (5) a reflection mechanism\nthat detects knowledge gaps and updates research direction with optional\nhuman-in-the-loop steering guidance. These components enable automated report\ngeneration, real-time streaming, and seamless enterprise deployment, as\nvalidated on internal datasets. On open-ended benchmarks including DeepResearch\nBench and DeepConsult, EDR outperforms state-of-the-art agentic systems without\nany human steering. We release the EDR framework and benchmark trajectories to\nadvance research on multi-agent reasoning applications.\n  Code at https://github.com/SalesforceAIResearch/enterprise-deep-research and\nDataset at https://huggingface.co/datasets/Salesforce/EDR-200", "AI": {"tldr": "EDR\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e3b\u89c4\u5212\u667a\u80fd\u4f53\u548c\u56db\u4e2a\u4e13\u4e1a\u641c\u7d22\u667a\u80fd\u4f53\uff0c\u7ed3\u5408\u53ef\u6269\u5c55\u5de5\u5177\u751f\u6001\u7cfb\u7edf\u548c\u53ef\u89c6\u5316\u667a\u80fd\u4f53\uff0c\u5b9e\u73b0\u4f01\u4e1a\u6df1\u5ea6\u7814\u7a76\u81ea\u52a8\u5316\uff0c\u5728\u5f00\u653e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u7cfb\u7edf\u3002", "motivation": "\u4f01\u4e1a\u9762\u4e34\u5c06\u975e\u7ed3\u6784\u5316\u6570\u636e\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u6d1e\u5bdf\u7684\u538b\u529b\uff0c\u73b0\u6709\u81ea\u4e3b\u667a\u80fd\u4f53\u5728\u9886\u57df\u7279\u5b9a\u7ec6\u5fae\u5dee\u522b\u3001\u610f\u56fe\u5bf9\u9f50\u548c\u4f01\u4e1a\u96c6\u6210\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u67b6\u6784\uff0c\u5305\u62ec\u4e3b\u89c4\u5212\u667a\u80fd\u4f53\u3001\u56db\u4e2a\u4e13\u4e1a\u641c\u7d22\u667a\u80fd\u4f53\uff08\u901a\u7528\u3001\u5b66\u672f\u3001GitHub\u3001LinkedIn\uff09\u3001\u57fa\u4e8eMCP\u7684\u53ef\u6269\u5c55\u5de5\u5177\u751f\u6001\u7cfb\u7edf\u3001\u53ef\u89c6\u5316\u667a\u80fd\u4f53\u548c\u53cd\u601d\u673a\u5236\u3002", "result": "\u5728DeepResearch Bench\u548cDeepConsult\u7b49\u5f00\u653e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cEDR\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u5c31\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u3002", "conclusion": "EDR\u6846\u67b6\u548c\u57fa\u51c6\u8f68\u8ff9\u7684\u53d1\u5e03\u5c06\u63a8\u52a8\u591a\u667a\u80fd\u4f53\u63a8\u7406\u5e94\u7528\u7684\u7814\u7a76\u53d1\u5c55\u3002"}}
{"id": "2510.16687", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16687", "abs": "https://arxiv.org/abs/2510.16687", "authors": ["Shurong Lin", "Eric D. Kolaczyk", "Adam Smith", "Elliot Paquette"], "title": "High-Dimensional Privacy-Utility Dynamics of Noisy Stochastic Gradient Descent on Least Squares", "comment": null, "summary": "The interplay between optimization and privacy has become a central theme in\nprivacy-preserving machine learning. Noisy stochastic gradient descent (SGD)\nhas emerged as a cornerstone algorithm, particularly in large-scale settings.\nThese variants of gradient methods inject carefully calibrated noise into each\nupdate to achieve differential privacy, the gold standard notion of rigorous\nprivacy guarantees. Prior work primarily provides various bounds on statistical\nrisk and privacy loss for noisy SGD, yet the \\textit{exact} behavior of the\nprocess remains unclear, particularly in high-dimensional settings. This work\nleverages a diffusion approach to analyze noisy SGD precisely, providing a\ncontinuous-time perspective that captures both statistical risk evolution and\nprivacy loss dynamics in high dimensions. Moreover, we study a variant of noisy\nSGD that does not require explicit knowledge of gradient sensitivity, unlike\nexisting work that assumes or enforces sensitivity through gradient clipping.\nSpecifically, we focus on the least squares problem with $\\ell_2$\nregularization.", "AI": {"tldr": "\u672c\u6587\u4f7f\u7528\u6269\u6563\u65b9\u6cd5\u7cbe\u786e\u5206\u6790\u566a\u58f0SGD\uff0c\u63d0\u4f9b\u8fde\u7eed\u65f6\u95f4\u89c6\u89d2\u6765\u6355\u6349\u9ad8\u7ef4\u8bbe\u7f6e\u4e2d\u7684\u7edf\u8ba1\u98ce\u9669\u6f14\u53d8\u548c\u9690\u79c1\u635f\u5931\u52a8\u6001\uff0c\u5e76\u7814\u7a76\u4e86\u4e00\u79cd\u65e0\u9700\u68af\u5ea6\u654f\u611f\u6027\u663e\u5f0f\u77e5\u8bc6\u7684\u566a\u58f0SGD\u53d8\u4f53\u3002", "motivation": "\u4f18\u5316\u4e0e\u9690\u79c1\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u5df2\u6210\u4e3a\u9690\u79c1\u4fdd\u62a4\u673a\u5668\u5b66\u4e60\u7684\u6838\u5fc3\u4e3b\u9898\u3002\u566a\u58f0SGD\u5df2\u6210\u4e3a\u5173\u952e\u7b97\u6cd5\uff0c\u4f46\u73b0\u6709\u5de5\u4f5c\u4e3b\u8981\u63d0\u4f9b\u7edf\u8ba1\u98ce\u9669\u548c\u9690\u79c1\u635f\u5931\u7684\u5404\u79cd\u754c\u9650\uff0c\u800c\u8fc7\u7a0b\u7684\u7cbe\u786e\u884c\u4e3a\u4ecd\u4e0d\u6e05\u695a\uff0c\u7279\u522b\u662f\u5728\u9ad8\u7ef4\u8bbe\u7f6e\u4e2d\u3002", "method": "\u5229\u7528\u6269\u6563\u65b9\u6cd5\u5206\u6790\u566a\u58f0SGD\uff0c\u63d0\u4f9b\u8fde\u7eed\u65f6\u95f4\u89c6\u89d2\uff1b\u7814\u7a76\u4e00\u79cd\u65e0\u9700\u68af\u5ea6\u654f\u611f\u6027\u663e\u5f0f\u77e5\u8bc6\u7684\u566a\u58f0SGD\u53d8\u4f53\uff0c\u91cd\u70b9\u5173\u6ce8\u5e26\u6709\u21132\u6b63\u5219\u5316\u7684\u6700\u5c0f\u4e8c\u4e58\u95ee\u9898\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u7cbe\u786e\u6355\u6349\u9ad8\u7ef4\u8bbe\u7f6e\u4e2d\u7edf\u8ba1\u98ce\u9669\u6f14\u53d8\u548c\u9690\u79c1\u635f\u5931\u52a8\u6001\uff0c\u63d0\u4f9b\u5bf9\u566a\u58f0SGD\u8fc7\u7a0b\u7684\u66f4\u6df1\u5165\u7406\u89e3\u3002", "conclusion": "\u6269\u6563\u65b9\u6cd5\u4e3a\u5206\u6790\u566a\u58f0SGD\u63d0\u4f9b\u4e86\u7cbe\u786e\u6846\u67b6\uff0c\u80fd\u591f\u66f4\u597d\u5730\u7406\u89e3\u9ad8\u7ef4\u8bbe\u7f6e\u4e2d\u7684\u7edf\u8ba1\u98ce\u9669\u548c\u9690\u79c1\u635f\u5931\u52a8\u6001\uff0c\u540c\u65f6\u63d0\u51fa\u7684\u53d8\u4f53\u7b97\u6cd5\u907f\u514d\u4e86\u68af\u5ea6\u654f\u611f\u6027\u5047\u8bbe\u7684\u9700\u6c42\u3002"}}
{"id": "2510.16694", "categories": ["cs.LG", "cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.16694", "abs": "https://arxiv.org/abs/2510.16694", "authors": ["Anthony DiMaggio", "Raghav Sharma", "Gururaj Saileshwar"], "title": "CLIP: Client-Side Invariant Pruning for Mitigating Stragglers in Secure Federated Learning", "comment": null, "summary": "Secure federated learning (FL) preserves data privacy during distributed\nmodel training. However, deploying such frameworks across heterogeneous devices\nresults in performance bottlenecks, due to straggler clients with limited\ncomputational or network capabilities, slowing training for all participating\nclients. This paper introduces the first straggler mitigation technique for\nsecure aggregation with deep neural networks. We propose CLIP, a client-side\ninvariant neuron pruning technique coupled with network-aware pruning, that\naddresses compute and network bottlenecks due to stragglers during training\nwith minimal accuracy loss. Our technique accelerates secure FL training by 13%\nto 34% across multiple datasets (CIFAR10, Shakespeare, FEMNIST) with an\naccuracy impact of between 1.3% improvement to 2.6% reduction.", "AI": {"tldr": "\u63d0\u51fa\u4e86CLIP\u6280\u672f\uff0c\u4e00\u79cd\u5ba2\u6237\u7aef\u4e0d\u53d8\u795e\u7ecf\u5143\u526a\u679d\u4e0e\u7f51\u7edc\u611f\u77e5\u526a\u679d\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5b89\u5168\u8054\u90a6\u5b66\u4e60\u4e2d\u7531\u8ba1\u7b97\u548c\u7f51\u7edc\u74f6\u9888\u5bfc\u81f4\u7684\u6162\u5ba2\u6237\u7aef\u95ee\u9898\uff0c\u5728\u6700\u5c0f\u7cbe\u5ea6\u635f\u5931\u4e0b\u52a0\u901f\u8bad\u7ec313%\u81f334%\u3002", "motivation": "\u5b89\u5168\u8054\u90a6\u5b66\u4e60\u5728\u5206\u5e03\u5f0f\u6a21\u578b\u8bad\u7ec3\u4e2d\u4fdd\u62a4\u6570\u636e\u9690\u79c1\uff0c\u4f46\u5f02\u6784\u8bbe\u5907\u90e8\u7f72\u4f1a\u5bfc\u81f4\u6027\u80fd\u74f6\u9888\uff0c\u8ba1\u7b97\u6216\u7f51\u7edc\u80fd\u529b\u6709\u9650\u7684\u6162\u5ba2\u6237\u7aef\u4f1a\u62d6\u6162\u6240\u6709\u53c2\u4e0e\u5ba2\u6237\u7aef\u7684\u8bad\u7ec3\u901f\u5ea6\u3002", "method": "CLIP\u6280\u672f\u7ed3\u5408\u5ba2\u6237\u7aef\u4e0d\u53d8\u795e\u7ecf\u5143\u526a\u679d\u548c\u7f51\u7edc\u611f\u77e5\u526a\u679d\uff0c\u901a\u8fc7\u526a\u679d\u51cf\u5c11\u8ba1\u7b97\u548c\u901a\u4fe1\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\uff08CIFAR10\u3001Shakespeare\u3001FEMNIST\uff09\u4e0a\uff0c\u5b89\u5168\u8054\u90a6\u5b66\u4e60\u8bad\u7ec3\u52a0\u901f13%\u81f334%\uff0c\u7cbe\u5ea6\u5f71\u54cd\u57281.3%\u63d0\u5347\u52302.6%\u4e0b\u964d\u4e4b\u95f4\u3002", "conclusion": "CLIP\u662f\u9996\u4e2a\u9488\u5bf9\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5b89\u5168\u805a\u5408\u7684\u6162\u5ba2\u6237\u7aef\u7f13\u89e3\u6280\u672f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5b89\u5168\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u8ba1\u7b97\u548c\u7f51\u7edc\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2510.16695", "categories": ["cs.LG", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.16695", "abs": "https://arxiv.org/abs/2510.16695", "authors": ["Iman Deznabi", "Peeyush Kumar", "Madalina Fiterau"], "title": "Resolution-Aware Retrieval Augmented Zero-Shot Forecasting", "comment": null, "summary": "Zero-shot forecasting aims to predict outcomes for previously unseen\nconditions without direct historical data, posing a significant challenge for\ntraditional forecasting methods. We introduce a Resolution-Aware\nRetrieval-Augmented Forecasting model that enhances predictive accuracy by\nleveraging spatial correlations and temporal frequency characteristics. By\ndecomposing signals into different frequency components, our model employs\nresolution-aware retrieval, where lower-frequency components rely on broader\nspatial context, while higher-frequency components focus on local influences.\nThis allows the model to dynamically retrieve relevant data and adapt to new\nlocations with minimal historical context.\n  Applied to microclimate forecasting, our model significantly outperforms\ntraditional forecasting methods, numerical weather prediction models, and\nmodern foundation time series models, achieving 71% lower MSE than HRRR and 34%\nlower MSE than Chronos on the ERA5 dataset.\n  Our results highlight the effectiveness of retrieval-augmented and\nresolution-aware strategies, offering a scalable and data-efficient solution\nfor zero-shot forecasting in microclimate modeling and beyond.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u8fa8\u7387\u611f\u77e5\u7684\u68c0\u7d22\u589e\u5f3a\u9884\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u5229\u7528\u7a7a\u95f4\u76f8\u5173\u6027\u548c\u65f6\u95f4\u9891\u7387\u7279\u5f81\u6765\u63d0\u9ad8\u96f6\u6837\u672c\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u96f6\u6837\u672c\u9884\u6d4b\u65e8\u5728\u9884\u6d4b\u6ca1\u6709\u76f4\u63a5\u5386\u53f2\u6570\u636e\u7684\u672a\u89c1\u6761\u4ef6\u4e0b\u7684\u7ed3\u679c\uff0c\u8fd9\u5bf9\u4f20\u7edf\u9884\u6d4b\u65b9\u6cd5\u6784\u6210\u4e86\u91cd\u5927\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5c06\u4fe1\u53f7\u5206\u89e3\u4e3a\u4e0d\u540c\u9891\u7387\u5206\u91cf\uff0c\u91c7\u7528\u5206\u8fa8\u7387\u611f\u77e5\u68c0\u7d22\u673a\u5236\uff1a\u4f4e\u9891\u5206\u91cf\u4f9d\u8d56\u66f4\u5e7f\u6cdb\u7684\u7a7a\u95f4\u4e0a\u4e0b\u6587\uff0c\u9ad8\u9891\u5206\u91cf\u5173\u6ce8\u5c40\u90e8\u5f71\u54cd\u3002", "result": "\u5728\u5fae\u6c14\u5019\u9884\u6d4b\u4e2d\uff0c\u8be5\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u9884\u6d4b\u65b9\u6cd5\u3001\u6570\u503c\u5929\u6c14\u9884\u62a5\u6a21\u578b\u548c\u73b0\u4ee3\u57fa\u7840\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\uff0c\u5728ERA5\u6570\u636e\u96c6\u4e0a\u6bd4HRRR\u7684MSE\u964d\u4f4e71%\uff0c\u6bd4Chronos\u964d\u4f4e34%\u3002", "conclusion": "\u68c0\u7d22\u589e\u5f3a\u548c\u5206\u8fa8\u7387\u611f\u77e5\u7b56\u7565\u5728\u96f6\u6837\u672c\u9884\u6d4b\u4e2d\u975e\u5e38\u6709\u6548\uff0c\u4e3a\u5fae\u6c14\u5019\u5efa\u6a21\u53ca\u5176\u4ed6\u9886\u57df\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u6570\u636e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.16703", "categories": ["cs.LG", "cs.AI", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.16703", "abs": "https://arxiv.org/abs/2510.16703", "authors": ["Yizuo Chen", "Adnan Darwiche"], "title": "On the Granularity of Causal Effect Identifiability", "comment": null, "summary": "The classical notion of causal effect identifiability is defined in terms of\ntreatment and outcome variables. In this note, we consider the identifiability\nof state-based causal effects: how an intervention on a particular state of\ntreatment variables affects a particular state of outcome variables. We\ndemonstrate that state-based causal effects may be identifiable even when\nvariable-based causal effects may not. Moreover, we show that this separation\noccurs only when additional knowledge -- such as context-specific\nindependencies and conditional functional dependencies -- is available. We\nfurther examine knowledge that constrains the states of variables, and show\nthat such knowledge does not improve identifiability on its own but can improve\nboth variable-based and state-based identifiability when combined with other\nknowledge such as context-specific independencies. Our findings highlight\nsituations where causal effects of interest may be estimable from observational\ndata and this identifiability may be missed by existing variable-based\nframeworks.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u57fa\u4e8e\u72b6\u6001\u7684\u56e0\u679c\u6548\u5e94\u53ef\u8bc6\u522b\u6027\uff0c\u8bc1\u660e\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u5373\u4f7f\u53d8\u91cf\u7ea7\u56e0\u679c\u6548\u5e94\u4e0d\u53ef\u8bc6\u522b\uff0c\u72b6\u6001\u7ea7\u56e0\u679c\u6548\u5e94\u4ecd\u53ef\u8bc6\u522b\uff0c\u8fd9\u9700\u8981\u4e0a\u4e0b\u6587\u7279\u5b9a\u72ec\u7acb\u6027\u548c\u6761\u4ef6\u51fd\u6570\u4f9d\u8d56\u7b49\u989d\u5916\u77e5\u8bc6\u3002", "motivation": "\u4f20\u7edf\u56e0\u679c\u6548\u5e94\u53ef\u8bc6\u522b\u6027\u57fa\u4e8e\u53d8\u91cf\u5c42\u9762\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4e2d\u5f80\u5f80\u5173\u5fc3\u7279\u5b9a\u72b6\u6001\u95f4\u7684\u56e0\u679c\u6548\u5e94\u3002\u73b0\u6709\u53d8\u91cf\u7ea7\u6846\u67b6\u53ef\u80fd\u9519\u8fc7\u67d0\u4e9b\u53ef\u8bc6\u522b\u7684\u60c5\u51b5\u3002", "method": "\u901a\u8fc7\u5206\u6790\u72b6\u6001\u7ea7\u56e0\u679c\u6548\u5e94\u4e0e\u53d8\u91cf\u7ea7\u56e0\u679c\u6548\u5e94\u7684\u5173\u7cfb\uff0c\u8003\u5bdf\u4e0a\u4e0b\u6587\u7279\u5b9a\u72ec\u7acb\u6027\u3001\u6761\u4ef6\u51fd\u6570\u4f9d\u8d56\u548c\u53d8\u91cf\u72b6\u6001\u7ea6\u675f\u7b49\u77e5\u8bc6\u5bf9\u53ef\u8bc6\u522b\u6027\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u72b6\u6001\u7ea7\u56e0\u679c\u6548\u5e94\u5728\u53d8\u91cf\u7ea7\u56e0\u679c\u6548\u5e94\u4e0d\u53ef\u8bc6\u522b\u65f6\u4ecd\u53ef\u80fd\u53ef\u8bc6\u522b\uff0c\u8fd9\u79cd\u5206\u79bb\u9700\u8981\u989d\u5916\u77e5\u8bc6\u3002\u53d8\u91cf\u72b6\u6001\u7ea6\u675f\u77e5\u8bc6\u5355\u72ec\u4e0d\u80fd\u6539\u5584\u53ef\u8bc6\u522b\u6027\uff0c\u4f46\u4e0e\u5176\u4ed6\u77e5\u8bc6\u7ed3\u5408\u53ef\u540c\u65f6\u6539\u5584\u53d8\u91cf\u7ea7\u548c\u72b6\u6001\u7ea7\u53ef\u8bc6\u522b\u6027\u3002", "conclusion": "\u72b6\u6001\u7ea7\u56e0\u679c\u6548\u5e94\u53ef\u8bc6\u522b\u6027\u5206\u6790\u80fd\u53d1\u73b0\u4f20\u7edf\u53d8\u91cf\u7ea7\u6846\u67b6\u53ef\u80fd\u9519\u8fc7\u7684\u53ef\u8bc6\u522b\u60c5\u51b5\uff0c\u4e3a\u4ece\u89c2\u6d4b\u6570\u636e\u4f30\u8ba1\u611f\u5174\u8da3\u7684\u56e0\u679c\u6548\u5e94\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2510.16743", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16743", "abs": "https://arxiv.org/abs/2510.16743", "authors": ["Viktoria Schram", "Markus Hiller", "Daniel Beck", "Trevor Cohn"], "title": "Zero-Shot Performance Prediction for Probabilistic Scaling Laws", "comment": "Accepted to NeurIPS 2025", "summary": "The prediction of learning curves for Natural Language Processing (NLP)\nmodels enables informed decision-making to meet specific performance\nobjectives, while reducing computational overhead and lowering the costs\nassociated with dataset acquisition and curation. In this work, we formulate\nthe prediction task as a multitask learning problem, where each task's data is\nmodelled as being organized within a two-layer hierarchy. To model the shared\ninformation and dependencies across tasks and hierarchical levels, we employ\nlatent variable multi-output Gaussian Processes, enabling to account for task\ncorrelations and supporting zero-shot prediction of learning curves (LCs). We\ndemonstrate that this approach facilitates the development of probabilistic\nscaling laws at lower costs. Applying an active learning strategy, LCs can be\nqueried to reduce predictive uncertainty and provide predictions close to\nground truth scaling laws. We validate our framework on three small-scale NLP\ndatasets with up to $30$ LCs. These are obtained from nanoGPT models, from\nbilingual translation using mBART and Transformer models, and from multilingual\ntranslation using M2M100 models of varying sizes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u4efb\u52a1\u5b66\u4e60\u65b9\u6cd5\uff0c\u4f7f\u7528\u6f5c\u5728\u53d8\u91cf\u591a\u8f93\u51fa\u9ad8\u65af\u8fc7\u7a0b\u6765\u9884\u6d4bNLP\u6a21\u578b\u7684\u5b66\u4e60\u66f2\u7ebf\uff0c\u652f\u6301\u96f6\u6837\u672c\u9884\u6d4b\u5e76\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u9884\u6d4bNLP\u6a21\u578b\u7684\u5b66\u4e60\u66f2\u7ebf\u53ef\u4ee5\u505a\u51fa\u660e\u667a\u51b3\u7b56\u4ee5\u6ee1\u8db3\u7279\u5b9a\u6027\u80fd\u76ee\u6807\uff0c\u540c\u65f6\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u548c\u6570\u636e\u96c6\u83b7\u53d6\u4e0e\u6574\u7406\u7684\u6210\u672c\u3002", "method": "\u5c06\u5b66\u4e60\u66f2\u7ebf\u9884\u6d4b\u4efb\u52a1\u5236\u5b9a\u4e3a\u591a\u4efb\u52a1\u5b66\u4e60\u95ee\u9898\uff0c\u4f7f\u7528\u6f5c\u5728\u53d8\u91cf\u591a\u8f93\u51fa\u9ad8\u65af\u8fc7\u7a0b\u5efa\u6a21\u4efb\u52a1\u95f4\u548c\u5c42\u6b21\u95f4\u7684\u5171\u4eab\u4fe1\u606f\u4e0e\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u4ee5\u8f83\u4f4e\u6210\u672c\u5f00\u53d1\u6982\u7387\u6027\u7f29\u653e\u5b9a\u5f8b\uff0c\u901a\u8fc7\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\u51cf\u5c11\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u4f9b\u63a5\u8fd1\u771f\u5b9e\u7f29\u653e\u5b9a\u5f8b\u7684\u9884\u6d4b\u7ed3\u679c\u3002", "conclusion": "\u5728\u4e09\u4e2a\u5c0f\u578bNLP\u6570\u636e\u96c6\u4e0a\u7684\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u9884\u6d4b\u5b66\u4e60\u66f2\u7ebf\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u89c4\u6a21\u7684\u6a21\u578b\u3002"}}
{"id": "2510.16747", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16747", "abs": "https://arxiv.org/abs/2510.16747", "authors": ["Danish Nazir", "Gowtham Sai Inti", "Timo Bartels", "Jan Piewek", "Thorsten Bagdonat", "Tim Fingscheidt"], "title": "An Efficient Semantic Segmentation Decoder for In-Car or Distributed Applications", "comment": null, "summary": "Modern automotive systems leverage deep neural networks (DNNs) for semantic\nsegmentation and operate in two key application areas: (1) In-car, where the\nDNN solely operates in the vehicle without strict constraints on the data rate.\n(2) Distributed, where one DNN part operates in the vehicle and the other part\ntypically on a large-scale cloud platform with a particular constraint on\ntransmission bitrate efficiency. Typically, both applications share an image\nand source encoder, while each uses distinct (joint) source and task decoders.\nPrior work utilized convolutional neural networks for joint source and task\ndecoding but did not investigate transformer-based alternatives such as\nSegDeformer, which offer superior performance at the cost of higher\ncomputational complexity. In this work, we propose joint feature and task\ndecoding for SegDeformer, thereby enabling lower computational complexity in\nboth in-car and distributed applications, despite SegDeformer's computational\ndemands. This improves scalability in the cloud while reducing in-car\ncomputational complexity. For the in-car application, we increased the frames\nper second (fps) by up to a factor of $11.7$ ($1.4$ fps to $16.5$ fps) on\nCityscapes and by up to a factor of $3.5$ ($43.3$ fps to $154.3$ fps) on\nADE20K, while being on-par w.r.t.\\ the mean intersection over union (mIoU) of\nthe transformer-based baseline that doesn't compress by a source codec. For the\ndistributed application, we achieve state-of-the-art (SOTA) over a wide range\nof bitrates on the mIoU metric, while using only $0.14$\\% ($0.04$\\%) of cloud\nDNN parameters used in previous SOTA, reported on ADE20K (Cityscapes).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSegDeformer\u7684\u8054\u5408\u7279\u5f81\u548c\u4efb\u52a1\u89e3\u7801\u65b9\u6cd5\uff0c\u5728\u8f66\u8f7d\u548c\u5206\u5e03\u5f0f\u5e94\u7528\u4e2d\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u73b0\u4ee3\u6c7d\u8f66\u7cfb\u7edf\u4f7f\u7528DNN\u8fdb\u884c\u8bed\u4e49\u5206\u5272\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u672a\u5145\u5206\u5229\u7528transformer\u67b6\u6784\u7684\u4f18\u52bf\uff0c\u4e14\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\uff0c\u9650\u5236\u4e86\u5728\u8f66\u8f7d\u548c\u5206\u5e03\u5f0f\u5e94\u7528\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "\u91c7\u7528SegDeformer\u67b6\u6784\u8fdb\u884c\u8054\u5408\u7279\u5f81\u548c\u4efb\u52a1\u89e3\u7801\uff0c\u901a\u8fc7\u4f18\u5316\u89e3\u7801\u8fc7\u7a0b\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u5206\u5272\u6027\u80fd\u3002", "result": "\u5728\u8f66\u8f7d\u5e94\u7528\u4e2d\uff0cCityscapes\u6570\u636e\u96c6\u4e0afps\u63d0\u534711.7\u500d(1.4\u523016.5fps)\uff0cADE20K\u6570\u636e\u96c6\u4e0a\u63d0\u53473.5\u500d(43.3\u5230154.3fps)\uff1b\u5728\u5206\u5e03\u5f0f\u5e94\u7528\u4e2d\uff0c\u4f7f\u7528\u4ec50.14%(ADE20K)\u548c0.04%(Cityscapes)\u7684\u4e91DNN\u53c2\u6570\u5b9e\u73b0SOTA\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u5206\u5272\u6027\u80fd\uff0c\u4e3a\u8f66\u8f7d\u548c\u5206\u5e03\u5f0f\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.16757", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16757", "abs": "https://arxiv.org/abs/2510.16757", "authors": ["Young In Kim", "Andrea Agiollo", "Rajiv Khanna"], "title": "SAMOSA: Sharpness Aware Minimization for Open Set Active learning", "comment": null, "summary": "Modern machine learning solutions require extensive data collection where\nlabeling remains costly. To reduce this burden, open set active learning\napproaches aim to select informative samples from a large pool of unlabeled\ndata that includes irrelevant or unknown classes. In this context, we propose\nSharpness Aware Minimization for Open Set Active Learning (SAMOSA) as an\neffective querying algorithm. Building on theoretical findings concerning the\nimpact of data typicality on the generalization properties of traditional\nstochastic gradient descent (SGD) and sharpness-aware minimization (SAM),\nSAMOSA actively queries samples based on their typicality. SAMOSA effectively\nidentifies atypical samples that belong to regions of the embedding manifold\nclose to the model decision boundaries. Therefore, SAMOSA prioritizes the\nsamples that are (i) highly informative for the targeted classes, and (ii)\nuseful for distinguishing between targeted and unwanted classes. Extensive\nexperiments show that SAMOSA achieves up to 3% accuracy improvement over the\nstate of the art across several datasets, while not introducing computational\noverhead. The source code of our experiments is available at:\nhttps://anonymous.4open.science/r/samosa-DAF4", "AI": {"tldr": "\u63d0\u51fa\u4e86SAMOSA\u65b9\u6cd5\uff0c\u4e00\u79cd\u57fa\u4e8e\u9510\u5ea6\u611f\u77e5\u6700\u5c0f\u5316\u7684\u5f00\u96c6\u4e3b\u52a8\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u5178\u578b\u6027\u6837\u672c\u6765\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u53473%\u51c6\u786e\u7387", "motivation": "\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u9700\u8981\u5927\u91cf\u6570\u636e\u6807\u6ce8\uff0c\u4f46\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u3002\u5f00\u96c6\u4e3b\u52a8\u5b66\u4e60\u65e8\u5728\u4ece\u672a\u6807\u8bb0\u6570\u636e\u4e2d\u9009\u62e9\u5305\u542b\u76f8\u5173\u548c\u65e0\u5173\u7c7b\u522b\u7684\u4fe1\u606f\u6837\u672c\uff0c\u4ee5\u51cf\u8f7b\u6807\u6ce8\u8d1f\u62c5", "method": "\u57fa\u4e8eSGD\u548cSAM\u7684\u7406\u8bba\u53d1\u73b0\uff0cSAMOSA\u6839\u636e\u6837\u672c\u5178\u578b\u6027\u4e3b\u52a8\u67e5\u8be2\u3002\u5b83\u8bc6\u522b\u5d4c\u5165\u6d41\u5f62\u4e2d\u9760\u8fd1\u6a21\u578b\u51b3\u7b56\u8fb9\u754c\u7684\u975e\u5178\u578b\u6837\u672c\uff0c\u4f18\u5148\u9009\u62e9\u5bf9\u76ee\u6807\u7c7b\u522b\u9ad8\u5ea6\u4fe1\u606f\u4e30\u5bcc\u4e14\u80fd\u533a\u5206\u76ee\u6807\u7c7b\u548c\u65e0\u5173\u7c7b\u7684\u6837\u672c", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSAMOSA\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u63d0\u5347\u9ad8\u8fbe3%\u7684\u51c6\u786e\u7387\uff0c\u4e14\u4e0d\u5f15\u5165\u8ba1\u7b97\u5f00\u9500", "conclusion": "SAMOSA\u662f\u4e00\u79cd\u6709\u6548\u7684\u5f00\u96c6\u4e3b\u52a8\u5b66\u4e60\u67e5\u8be2\u7b97\u6cd5\uff0c\u901a\u8fc7\u9510\u5ea6\u611f\u77e5\u6700\u5c0f\u5316\u548c\u5178\u578b\u6027\u6837\u672c\u9009\u62e9\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd"}}
{"id": "2510.16774", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16774", "abs": "https://arxiv.org/abs/2510.16774", "authors": ["Yuguang Yue", "Irakli Salia", "Samuel Hunt", "Christopher Green", "Wenzhe Shi", "Jonathan J Hunt"], "title": "Learning to play: A Multimodal Agent for 3D Game-Play", "comment": "International Conference on Computer Vision Workshop on Multi-Modal\n  Reasoning for Agentic Intelligence", "summary": "We argue that 3-D first-person video games are a challenging environment for\nreal-time multi-modal reasoning. We first describe our dataset of human\ngame-play, collected across a large variety of 3-D first-person games, which is\nboth substantially larger and more diverse compared to prior publicly disclosed\ndatasets, and contains text instructions. We demonstrate that we can learn an\ninverse dynamics model from this dataset, which allows us to impute actions on\na much larger dataset of publicly available videos of human game play that lack\nrecorded actions. We then train a text-conditioned agent for game playing using\nbehavior cloning, with a custom architecture capable of realtime inference on a\nconsumer GPU. We show the resulting model is capable of playing a variety of\n3-D games and responding to text input. Finally, we outline some of the\nremaining challenges such as long-horizon tasks and quantitative evaluation\nacross a large set of games.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e3D\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u6e38\u620f\u7684\u591a\u6a21\u6001\u63a8\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u6536\u96c6\u5927\u89c4\u6a21\u6e38\u620f\u6570\u636e\u96c6\u3001\u5b66\u4e60\u9006\u52a8\u529b\u5b66\u6a21\u578b\u548c\u884c\u4e3a\u514b\u9686\u8bad\u7ec3\uff0c\u5f00\u53d1\u4e86\u80fd\u591f\u5b9e\u65f6\u54cd\u5e94\u6587\u672c\u6307\u4ee4\u7684\u6e38\u620f\u667a\u80fd\u4f53\u3002", "motivation": "3D\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u6e38\u620f\u4e3a\u5b9e\u65f6\u591a\u6a21\u6001\u63a8\u7406\u63d0\u4f9b\u4e86\u5177\u6709\u6311\u6218\u6027\u7684\u73af\u5883\uff0c\u9700\u8981\u89e3\u51b3\u6e38\u620f\u73a9\u6cd5\u7406\u89e3\u3001\u52a8\u4f5c\u9884\u6d4b\u548c\u6587\u672c\u6307\u4ee4\u54cd\u5e94\u7b49\u95ee\u9898\u3002", "method": "\u6536\u96c6\u5927\u89c4\u6a21\u4eba\u7c7b\u6e38\u620f\u6570\u636e\u96c6\uff0c\u5b66\u4e60\u9006\u52a8\u529b\u5b66\u6a21\u578b\u6765\u63a8\u65ad\u7f3a\u5931\u7684\u52a8\u4f5c\uff0c\u4f7f\u7528\u884c\u4e3a\u514b\u9686\u8bad\u7ec3\u6587\u672c\u6761\u4ef6\u667a\u80fd\u4f53\uff0c\u91c7\u7528\u652f\u6301\u5b9e\u65f6\u63a8\u7406\u7684\u81ea\u5b9a\u4e49\u67b6\u6784\u3002", "result": "\u5f00\u53d1\u51fa\u7684\u6a21\u578b\u80fd\u591f\u5728\u5404\u79cd3D\u6e38\u620f\u4e2d\u6267\u884c\u4efb\u52a1\u5e76\u54cd\u5e94\u6587\u672c\u8f93\u5165\uff0c\u5728\u6d88\u8d39\u7ea7GPU\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u63a8\u7406\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u57283D\u6e38\u620f\u73af\u5883\u4e2d\u5c55\u793a\u4e86\u591a\u6a21\u6001\u63a8\u7406\u7684\u53ef\u884c\u6027\uff0c\u4f46\u4ecd\u9762\u4e34\u957f\u65f6\u7a0b\u4efb\u52a1\u548c\u5927\u89c4\u6a21\u6e38\u620f\u5b9a\u91cf\u8bc4\u4f30\u7b49\u6311\u6218\u3002"}}
{"id": "2510.16780", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16780", "abs": "https://arxiv.org/abs/2510.16780", "authors": ["Chang Wu", "Zhiyuan Liu", "Wen Shu", "Liang Wang", "Yanchen Luo", "Wenqiang Lei", "Yatao Bian", "Junfeng Fang", "Xiang Wang"], "title": "3D-GSRD: 3D Molecular Graph Auto-Encoder with Selective Re-mask Decoding", "comment": null, "summary": "Masked graph modeling (MGM) is a promising approach for molecular\nrepresentation learning (MRL).However, extending the success of re-mask\ndecoding from 2D to 3D MGM is non-trivial, primarily due to two conflicting\nchallenges: avoiding 2D structure leakage to the decoder, while still providing\nsufficient 2D context for reconstructing re-masked atoms.To address these\nchallenges, we propose 3D-GSRD: a 3D Molecular Graph Auto-Encoder with\nSelective Re-mask Decoding. The core innovation of 3D-GSRD lies in its\nSelective Re-mask Decoding(SRD), which re-masks only 3D-relevant information\nfrom encoder representations while preserving the 2D graph structures.This SRD\nis synergistically integrated with a 3D Relational-Transformer(3D-ReTrans)\nencoder alongside a structure-independent decoder. We analyze that SRD,\ncombined with the structure-independent decoder, enhances the encoder's role in\nMRL. Extensive experiments show that 3D-GSRD achieves strong downstream\nperformance, setting a new state-of-the-art on 7 out of 8 targets in the widely\nused MD17 molecular property prediction benchmark. The code is released at\nhttps://github.com/WuChang0124/3D-GSRD.", "AI": {"tldr": "\u63d0\u51fa3D-GSRD\u6a21\u578b\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u91cd\u63a9\u7801\u89e3\u7801\u89e3\u51b33D\u5206\u5b50\u8868\u793a\u5b66\u4e60\u4e2d\u76842D\u7ed3\u6784\u6cc4\u6f0f\u95ee\u9898\uff0c\u5728MD17\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97SOTA\u6027\u80fd", "motivation": "\u5c06\u63a9\u7801\u56fe\u5efa\u6a21\u4ece2D\u6269\u5c55\u52303D\u65f6\u9762\u4e34\u4e24\u4e2a\u51b2\u7a81\u6311\u6218\uff1a\u907f\u514d2D\u7ed3\u6784\u6cc4\u6f0f\u5230\u89e3\u7801\u5668\uff0c\u540c\u65f6\u63d0\u4f9b\u8db3\u591f\u76842D\u4e0a\u4e0b\u6587\u6765\u91cd\u5efa\u91cd\u63a9\u7801\u539f\u5b50", "method": "\u63d0\u51fa\u9009\u62e9\u6027\u91cd\u63a9\u7801\u89e3\u7801(SRD)\uff0c\u4ec5\u4ece\u7f16\u7801\u5668\u8868\u793a\u4e2d\u91cd\u63a9\u78013D\u76f8\u5173\u4fe1\u606f\uff0c\u540c\u65f6\u4fdd\u75592D\u56fe\u7ed3\u6784\uff1b\u7ed3\u54083D\u5173\u7cfb\u53d8\u6362\u5668\u7f16\u7801\u5668\u548c\u7ed3\u6784\u65e0\u5173\u89e3\u7801\u5668", "result": "\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684MD17\u5206\u5b50\u6027\u8d28\u9884\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c8\u4e2a\u76ee\u6807\u4e2d\u76847\u4e2a\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd", "conclusion": "3D-GSRD\u901a\u8fc7\u9009\u62e9\u6027\u91cd\u63a9\u7801\u89e3\u7801\u548c\u7ed3\u6784\u65e0\u5173\u89e3\u7801\u5668\u7684\u534f\u540c\u96c6\u6210\uff0c\u589e\u5f3a\u4e86\u7f16\u7801\u5668\u5728\u5206\u5b50\u8868\u793a\u5b66\u4e60\u4e2d\u7684\u4f5c\u7528"}}
{"id": "2510.16805", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16805", "abs": "https://arxiv.org/abs/2510.16805", "authors": ["Mariam Rakka", "Marios Fournarakis", "Olga Krestinskaya", "Jinane Bazzi", "Khaled N. Salama", "Fadi Kurdahi", "Ahmed M. Eltawil", "Mohammed E. Fouda"], "title": "Mixed-Precision Quantization for Language Models: Techniques and Prospects", "comment": "46 pages, 6 figures, 5 tables", "summary": "The rapid scaling of language models (LMs) has resulted in unprecedented\ncomputational, memory, and energy requirements, making their training and\ndeployment increasingly unsustainable. Quantization has emerged as an essential\ncompression technique to reduce model size, alleviate memory bottlenecks, and\naccelerate inference. However, while uniform low-bit quantization (e.g., INT8,\nINT4) provides significant efficiency gains, it can degrade accuracy in\nsensitive components of transformer-based LMs. Mixed-precision quantization\noffers a promising alternative by selectively allocating precision across\nlayers or within tensors to balance efficiency and accuracy. This survey\nprovides a comprehensive overview of Mixed-Precision quantization frameworks\nfor LMs (MXPLMs). We first review quantization fundamentals, including uniform\nand non-uniform quantizers, quantization granularity, and methods widely used\nin post-training quantization. We then categorize and compare recent MXPLM\nframeworks according to their bit allocation strategies and precision\nconfigurations across weights, activations, and key-value caches. A comparative\nanalysis highlights differences in perplexity, zero-shot task performance, and\ndeployment trade-offs. Furthermore, we contrast MXPLMs with earlier\nmixed-precision quantization methods for deep neural networks, identifying\nstrategies that transfer and those that face challenges in the LM setting.\nFinally, we summarize open issues and future directions, including\nhardware-aware design, activation quantization, and scalable optimization\nmethods for billion-parameter models. By consolidating recent advances, this\nwork serves as a reference for understanding the current landscape and research\nprospects of mixed-precision quantization for large-scale language models.", "AI": {"tldr": "\u8be5\u7efc\u8ff0\u8bba\u6587\u7cfb\u7edf\u56de\u987e\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u6280\u672f\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u7cbe\u5ea6\u5206\u914d\u7b56\u7565\u5728\u6743\u91cd\u3001\u6fc0\u6d3b\u548cKV\u7f13\u5b58\u4e0a\u7684\u5e94\u7528\uff0c\u6bd4\u8f83\u4e86\u56f0\u60d1\u5ea6\u548c\u96f6\u6837\u672c\u4efb\u52a1\u6027\u80fd\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u8bed\u8a00\u6a21\u578b\u89c4\u6a21\u7684\u5feb\u901f\u6269\u5c55\uff0c\u5176\u8ba1\u7b97\u3001\u5185\u5b58\u548c\u80fd\u8017\u9700\u6c42\u6025\u5267\u589e\u52a0\uff0c\u4f7f\u5f97\u8bad\u7ec3\u548c\u90e8\u7f72\u53d8\u5f97\u4e0d\u53ef\u6301\u7eed\u3002\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u901a\u8fc7\u9009\u62e9\u6027\u5206\u914d\u7cbe\u5ea6\u6765\u5e73\u8861\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u6210\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u7684\u5173\u952e\u6280\u672f\u3002", "method": "\u8bba\u6587\u9996\u5148\u56de\u987e\u91cf\u5316\u57fa\u7840\u77e5\u8bc6\uff0c\u5305\u62ec\u5747\u5300\u548c\u975e\u5747\u5300\u91cf\u5316\u5668\u3001\u91cf\u5316\u7c92\u5ea6\u4ee5\u53ca\u540e\u8bad\u7ec3\u91cf\u5316\u65b9\u6cd5\u3002\u7136\u540e\u6839\u636e\u4f4d\u5206\u914d\u7b56\u7565\u548c\u7cbe\u5ea6\u914d\u7f6e\u5bf9\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u6846\u67b6\u8fdb\u884c\u5206\u7c7b\u6bd4\u8f83\uff0c\u5206\u6790\u4e0d\u540c\u7ec4\u4ef6\uff08\u6743\u91cd\u3001\u6fc0\u6d3b\u3001KV\u7f13\u5b58\uff09\u7684\u7cbe\u5ea6\u914d\u7f6e\u5dee\u5f02\u3002", "result": "\u901a\u8fc7\u6bd4\u8f83\u5206\u6790\u53d1\u73b0\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u5728\u4fdd\u6301\u6a21\u578b\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\uff0c\u4e0d\u540c\u6846\u67b6\u5728\u56f0\u60d1\u5ea6\u548c\u96f6\u6837\u672c\u4efb\u52a1\u6027\u80fd\u4e0a\u8868\u73b0\u5404\u5f02\uff0c\u5e76\u63ed\u793a\u4e86\u4e0e\u65e9\u671f\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u65b9\u6cd5\u7684\u5f02\u540c\u3002", "conclusion": "\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u662f\u89e3\u51b3\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u90e8\u7f72\u6311\u6218\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u672a\u6765\u9700\u8981\u5728\u786c\u4ef6\u611f\u77e5\u8bbe\u8ba1\u3001\u6fc0\u6d3b\u91cf\u5316\u548c\u53ef\u6269\u5c55\u4f18\u5316\u65b9\u6cd5\u7b49\u65b9\u9762\u8fdb\u4e00\u6b65\u7814\u7a76\uff0c\u4ee5\u652f\u6301\u5341\u4ebf\u53c2\u6570\u7ea7\u6a21\u578b\u7684\u91cf\u5316\u90e8\u7f72\u3002"}}
{"id": "2510.16806", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16806", "abs": "https://arxiv.org/abs/2510.16806", "authors": ["Weilin Wan", "Weizhong Zhang", "Cheng Jin"], "title": "Computational Budget Should Be Considered in Data Selection", "comment": null, "summary": "Data selection improves computational efficiency by choosing informative\nsubsets of training samples. However, existing methods ignore the compute\nbudget, treating data selection and importance evaluation independently of\ncompute budget constraints. Yet empirical studies show no algorithm can\nconsistently outperform others (or even random selection) across varying\nbudgets. We therefore argue that compute budget must be integral to\ndata-selection strategies, since different budgets impose distinct requirements\non data quantity, quality, and distribution for effective training. To this\nend, we propose a novel Computational budget-Aware Data Selection (CADS) method\nand naturally formulate it into a bilevel optimization framework, where the\ninner loop trains the model within the constraints of the computational budget\non some selected subset of training data, while the outer loop optimizes data\nselection based on model evaluation. Our technical contributions lie in\naddressing two main challenges in solving this bilevel optimization problem:\nthe expensive Hessian matrix estimation for outer-loop gradients and the\ncomputational burden of achieving inner-loop optimality during iterations. To\nsolve the first issue, we propose a probabilistic reparameterization strategy\nand compute the gradient using a Hessian-free policy gradient estimator. To\naddress the second challenge, we transform the inner optimization problem into\na penalty term in the outer objective, further discovering that we only need to\nestimate the minimum of a one-dimensional loss to calculate the gradient,\nsignificantly improving efficiency. Extensive experiments show that our method\nachieves performance gains of up to 14.42% over baselines in vision and\nlanguage benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u8ba1\u7b97\u9884\u7b97\u611f\u77e5\u6570\u636e\u9009\u62e9\u65b9\u6cd5(CADS)\uff0c\u901a\u8fc7\u53cc\u5c42\u4f18\u5316\u6846\u67b6\u5c06\u8ba1\u7b97\u9884\u7b97\u7ea6\u675f\u96c6\u6210\u5230\u6570\u636e\u9009\u62e9\u7b56\u7565\u4e2d\uff0c\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6027\u80fd\u63d0\u5347\u6700\u9ad8\u8fbe14.42%\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u9009\u62e9\u65b9\u6cd5\u5ffd\u7565\u4e86\u8ba1\u7b97\u9884\u7b97\u7ea6\u675f\uff0c\u5bfc\u81f4\u5728\u4e0d\u540c\u9884\u7b97\u4e0b\u65e0\u6cd5\u4fdd\u6301\u4e00\u81f4\u7684\u6027\u80fd\u4f18\u52bf\u3002\u8ba1\u7b97\u9884\u7b97\u5e94\u6210\u4e3a\u6570\u636e\u9009\u62e9\u7b56\u7565\u7684\u6838\u5fc3\u8981\u7d20\uff0c\u56e0\u4e3a\u4e0d\u540c\u9884\u7b97\u5bf9\u6570\u636e\u6570\u91cf\u3001\u8d28\u91cf\u548c\u5206\u5e03\u6709\u4e0d\u540c\u7684\u8981\u6c42\u3002", "method": "\u63d0\u51faCADS\u65b9\u6cd5\uff0c\u91c7\u7528\u53cc\u5c42\u4f18\u5316\u6846\u67b6\uff1a\u5185\u5c42\u5728\u9009\u5b9a\u6570\u636e\u5b50\u96c6\u4e0a\u8bad\u7ec3\u6a21\u578b\uff0c\u5916\u5c42\u57fa\u4e8e\u6a21\u578b\u8bc4\u4f30\u4f18\u5316\u6570\u636e\u9009\u62e9\u3002\u901a\u8fc7\u6982\u7387\u91cd\u53c2\u6570\u5316\u7b56\u7565\u548cHessian-free\u7b56\u7565\u68af\u5ea6\u4f30\u8ba1\u5668\u89e3\u51b3Hessian\u77e9\u9635\u4f30\u8ba1\u95ee\u9898\uff0c\u5c06\u5185\u5c42\u4f18\u5316\u8f6c\u5316\u4e3a\u5916\u5c42\u76ee\u6807\u7684\u60e9\u7f5a\u9879\u4ee5\u63d0\u9ad8\u6548\u7387\u3002", "result": "\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCADS\u65b9\u6cd5\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u9ad814.42%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u8ba1\u7b97\u9884\u7b97\u611f\u77e5\u6570\u636e\u9009\u62e9\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8ba1\u7b97\u9884\u7b97\u5e94\u4f5c\u4e3a\u6570\u636e\u9009\u62e9\u7b56\u7565\u7684\u6838\u5fc3\u8003\u8651\u56e0\u7d20\uff0cCADS\u65b9\u6cd5\u901a\u8fc7\u53cc\u5c42\u4f18\u5316\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u9884\u7b97\u7ea6\u675f\u4e0b\u7684\u6570\u636e\u9009\u62e9\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2510.16807", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16807", "abs": "https://arxiv.org/abs/2510.16807", "authors": ["Zhoutong Wu", "Yuan Zhang", "Yiming Dong", "Chenheng Zhang", "Cong Fang", "Kun Yuan", "Zhouchen Lin"], "title": "Improving Model Representation and Reducing KV Cache via Skip Connections with First Value Heads", "comment": "The code is available at:\n  \\url{https://github.com/Zhoutong-Wu/SkipV1Former}", "summary": "Transformer models have driven breakthroughs across various language tasks by\ntheir strong capability to learn rich contextual representations. Scaling them\nto improve representation, however, often demands substantial memory and\ncompute costs, such as the Key-Value (KV) cache used during auto-regressive\ndecoding. Skip connections offer a promising way to improve representation\nwithout bloating resource usage, yet most prior works either improve\nexpressivity while leaving KV costs unchanged, or reduce memory at the cost of\nweaker representation. In this work, we propose SkipV1Former, a Transformer\nvariant that uses skip connections from the first layer's Value heads to\nstrengthen model representation and reduce KV cache. Specifically, from the\nsecond block onward, each layer reuses half of its Value heads from the very\nfirst layer, while computing the other half as usual-cutting Value projections\nand V cache by nearly 50 \\%. Theoretically, we show that routing uncompressed\nfirst-layer Values into deeper layers restores information lost to compression\nand accelerates the model's implicit mesa-optimization-a key pattern of\nTransformer in auto-regressive tasks. Empirically, across different model\nscales, SkipV1Former delivers consistent reductions of approximately 25 \\% in\nKV cache while improving perplexity relative to standard Multi-Head Attention\n(MHA) Transformers and some advanced variants. Moreover, we propose a recipe\nfor uptraining existing MHA Transformer checkpoints to SkipV1Former with only\n10-15\\% additional compute. Finally, SkipV1Former can seamlessly combine\nadvanced methods like Group-Query Attention and Multi-Latent Attention to\nachieve further KV cache savings and performance improvement. When combined\nwith YOCO, it cuts KV cache size by nearly 50 \\% while still improving\nperformance.", "AI": {"tldr": "SkipV1Former\u662f\u4e00\u79cdTransformer\u53d8\u4f53\uff0c\u901a\u8fc7\u4ece\u7b2c\u4e00\u5c42\u7684Value\u5934\u6dfb\u52a0\u8df3\u8dc3\u8fde\u63a5\u6765\u589e\u5f3a\u6a21\u578b\u8868\u793a\u80fd\u529b\u5e76\u51cf\u5c11KV\u7f13\u5b58\uff0c\u53ef\u51cf\u5c11\u7ea625%\u7684KV\u7f13\u5b58\u540c\u65f6\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u6269\u5c55Transformer\u6a21\u578b\u4ee5\u63d0\u9ad8\u8868\u793a\u80fd\u529b\u901a\u5e38\u9700\u8981\u5927\u91cf\u5185\u5b58\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u7279\u522b\u662f\u81ea\u56de\u5f52\u89e3\u7801\u4e2d\u7684KV\u7f13\u5b58\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u6539\u8fdb\u8868\u8fbe\u80fd\u529b\u4f46KV\u6210\u672c\u4e0d\u53d8\uff0c\u8981\u4e48\u51cf\u5c11\u5185\u5b58\u4f46\u524a\u5f31\u8868\u793a\u80fd\u529b\u3002", "method": "\u4ece\u7b2c\u4e8c\u5757\u5f00\u59cb\uff0c\u6bcf\u5c42\u91cd\u7528\u4e00\u534a\u7684Value\u5934\u6765\u81ea\u7b2c\u4e00\u5c42\uff0c\u53e6\u4e00\u534a\u6b63\u5e38\u8ba1\u7b97\uff0c\u4ece\u800c\u5c06Value\u6295\u5f71\u548cV\u7f13\u5b58\u51cf\u5c11\u8fd150%\u3002\u7406\u8bba\u4e0a\uff0c\u5c06\u672a\u538b\u7f29\u7684\u7b2c\u4e00\u5c42Value\u8def\u7531\u5230\u66f4\u6df1\u5c42\u53ef\u4ee5\u6062\u590d\u538b\u7f29\u4e22\u5931\u7684\u4fe1\u606f\u5e76\u52a0\u901f\u6a21\u578b\u7684\u9690\u5f0f\u5143\u4f18\u5316\u3002", "result": "\u5728\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u4e0b\uff0cSkipV1Former\u76f8\u5bf9\u4e8e\u6807\u51c6MHA Transformer\u548c\u67d0\u4e9b\u5148\u8fdb\u53d8\u4f53\uff0c\u5728\u51cf\u5c11\u7ea625% KV\u7f13\u5b58\u7684\u540c\u65f6\u6539\u8fdb\u4e86\u56f0\u60d1\u5ea6\u3002\u4e0eYOCO\u7ed3\u5408\u65f6\uff0cKV\u7f13\u5b58\u5927\u5c0f\u51cf\u5c11\u8fd150%\u4e14\u6027\u80fd\u4ecd\u63d0\u5347\u3002", "conclusion": "SkipV1Former\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u589e\u5f3aTransformer\u8868\u793a\u80fd\u529b\u5e76\u51cf\u5c11KV\u7f13\u5b58\uff0c\u4e14\u53ef\u4ee5\u901a\u8fc7\u4ec510-15%\u989d\u5916\u8ba1\u7b97\u5c06\u73b0\u6709MHA Transformer\u68c0\u67e5\u70b9\u4e0a\u8bad\u7ec3\u5230SkipV1Former\u3002"}}
{"id": "2510.16811", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16811", "abs": "https://arxiv.org/abs/2510.16811", "authors": ["Mohammad Shahverdikondori", "Jalal Etesami", "Negar Kiyavash"], "title": "Graph Learning is Suboptimal in Causal Bandits", "comment": "31 pages, 5 figures", "summary": "We study regret minimization in causal bandits under causal sufficiency where\nthe underlying causal structure is not known to the agent. Previous work has\nfocused on identifying the reward's parents and then applying classic bandit\nmethods to them, or jointly learning the parents while minimizing regret. We\ninvestigate whether such strategies are optimal. Somewhat counterintuitively,\nour results show that learning the parent set is suboptimal. We do so by\nproving that there exist instances where regret minimization and parent\nidentification are fundamentally conflicting objectives. We further analyze\nboth the known and unknown parent set size regimes, establish novel regret\nlower bounds that capture the combinatorial structure of the action space.\nBuilding on these insights, we propose nearly optimal algorithms that bypass\ngraph and parent recovery, demonstrating that parent identification is indeed\nunnecessary for regret minimization. Experiments confirm that there exists a\nlarge performance gap between our method and existing baselines in various\nenvironments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8bc1\u660e\u5728\u56e0\u679c\u532a\u5f92\u95ee\u9898\u4e2d\uff0c\u5b66\u4e60\u5956\u52b1\u7684\u7236\u8282\u70b9\u96c6\u662f\u6b21\u4f18\u7684\uff0c\u56e0\u4e3a\u9057\u61be\u6700\u5c0f\u5316\u548c\u7236\u8282\u70b9\u8bc6\u522b\u662f\u76f8\u4e92\u51b2\u7a81\u7684\u76ee\u6807\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u7ed5\u8fc7\u56fe\u6062\u590d\u7684\u8fd1\u4e4e\u6700\u4f18\u7b97\u6cd5\u3002", "motivation": "\u7814\u7a76\u5728\u672a\u77e5\u56e0\u679c\u7ed3\u6784\u7684\u60c5\u51b5\u4e0b\uff0c\u56e0\u679c\u532a\u5f92\u95ee\u9898\u4e2d\u7684\u9057\u61be\u6700\u5c0f\u5316\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u5148\u8bc6\u522b\u5956\u52b1\u7684\u7236\u8282\u70b9\u518d\u5e94\u7528\u7ecf\u5178\u532a\u5f92\u65b9\u6cd5\uff0c\u8981\u4e48\u8054\u5408\u5b66\u4e60\u7236\u8282\u70b9\u540c\u65f6\u6700\u5c0f\u5316\u9057\u61be\uff0c\u4f46\u4f5c\u8005\u8d28\u7591\u8fd9\u4e9b\u7b56\u7565\u662f\u5426\u6700\u4f18\u3002", "method": "\u901a\u8fc7\u8bc1\u660e\u5b58\u5728\u9057\u61be\u6700\u5c0f\u5316\u548c\u7236\u8282\u70b9\u8bc6\u522b\u76f8\u4e92\u51b2\u7a81\u7684\u5b9e\u4f8b\uff0c\u5efa\u7acb\u4e86\u6355\u6349\u52a8\u4f5c\u7a7a\u95f4\u7ec4\u5408\u7ed3\u6784\u7684\u9057\u61be\u4e0b\u754c\uff0c\u5e76\u63d0\u51fa\u4e86\u7ed5\u8fc7\u56fe\u6062\u590d\u548c\u7236\u8282\u70b9\u6062\u590d\u7684\u8fd1\u4e4e\u6700\u4f18\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8bc1\u5b9e\uff0c\u5728\u5404\u79cd\u73af\u5883\u4e2d\uff0c\u4f5c\u8005\u7684\u65b9\u6cd5\u4e0e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "\u7236\u8282\u70b9\u8bc6\u522b\u5bf9\u4e8e\u9057\u61be\u6700\u5c0f\u5316\u662f\u4e0d\u5fc5\u8981\u7684\uff0c\u5b66\u4e60\u7236\u8282\u70b9\u96c6\u662f\u6b21\u4f18\u7b56\u7565\u3002"}}
{"id": "2510.16814", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16814", "abs": "https://arxiv.org/abs/2510.16814", "authors": ["Simon Jaxy", "Anton Theys", "Patrick Willett", "W. Chris Carleton", "Ralf Vandam", "Pieter Libin"], "title": "Needles in the Landscape: Semi-Supervised Pseudolabeling for Archaeological Site Discovery under Label Scarcity", "comment": null, "summary": "Archaeological predictive modelling estimates where undiscovered sites are\nlikely to occur by combining known locations with environmental, cultural, and\ngeospatial variables. We address this challenge using a deep learning approach\nbut must contend with structural label scarcity inherent to archaeology:\npositives are rare, and most locations are unlabeled. To address this, we adopt\na semi-supervised, positive-unlabeled (PU) learning strategy, implemented as a\nsemantic segmentation model and evaluated on two datasets covering a\nrepresentative range of archaeological periods. Our approach employs dynamic\npseudolabeling, refined with a Conditional Random Field (CRF) implemented via\nan RNN, increasing label confidence under severe class imbalance. On a\ngeospatial dataset derived from a digital elevation model (DEM), our model\nperforms on par with the state-of-the-art, LAMAP, while achieving higher Dice\nscores. On raw satellite imagery, assessed end-to-end with stratified k-fold\ncross-validation, it maintains performance and yields predictive surfaces with\nimproved interpretability. Overall, our results indicate that semi-supervised\nlearning offers a promising approach to identifying undiscovered sites across\nlarge, sparsely annotated landscapes.", "AI": {"tldr": "\u4f7f\u7528\u534a\u76d1\u7763\u5b66\u4e60\u548c\u6b63\u4f8b-\u672a\u6807\u8bb0\u5b66\u4e60\u7b56\u7565\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u901a\u8fc7\u52a8\u6001\u4f2a\u6807\u7b7e\u548c\u6761\u4ef6\u968f\u673a\u573a\u6539\u8fdb\uff0c\u5728\u8003\u53e4\u9884\u6d4b\u5efa\u6a21\u4e2d\u53d6\u5f97\u4e86\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5e76\u5728\u536b\u661f\u56fe\u50cf\u4e0a\u8868\u73b0\u51fa\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u89e3\u51b3\u8003\u53e4\u9884\u6d4b\u5efa\u6a21\u4e2d\u7684\u7ed3\u6784\u6027\u6807\u7b7e\u7a00\u7f3a\u95ee\u9898\uff1a\u6b63\u4f8b\u6837\u672c\u7a00\u5c11\uff0c\u5927\u591a\u6570\u4f4d\u7f6e\u672a\u6807\u8bb0\uff0c\u9700\u8981\u5904\u7406\u4e25\u91cd\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u3002", "method": "\u91c7\u7528\u534a\u76d1\u7763\u6b63\u4f8b-\u672a\u6807\u8bb0\u5b66\u4e60\u7b56\u7565\uff0c\u5b9e\u73b0\u4e3a\u8bed\u4e49\u5206\u5272\u6a21\u578b\uff0c\u4f7f\u7528\u52a8\u6001\u4f2a\u6807\u7b7e\u6280\u672f\u5e76\u901a\u8fc7RNN\u5b9e\u73b0\u7684\u6761\u4ef6\u968f\u673a\u573a\u8fdb\u884c\u7cbe\u70bc\uff0c\u63d0\u9ad8\u6807\u7b7e\u7f6e\u4fe1\u5ea6\u3002", "result": "\u5728\u57fa\u4e8e\u6570\u5b57\u9ad8\u7a0b\u6a21\u578b\u7684\u5730\u7406\u7a7a\u95f4\u6570\u636e\u96c6\u4e0a\uff0c\u6027\u80fd\u4e0e\u6700\u5148\u8fdb\u7684LAMAP\u65b9\u6cd5\u76f8\u5f53\uff0c\u540c\u65f6\u83b7\u5f97\u66f4\u9ad8\u7684Dice\u5206\u6570\uff1b\u5728\u539f\u59cb\u536b\u661f\u56fe\u50cf\u4e0a\uff0c\u901a\u8fc7\u5206\u5c42k\u6298\u4ea4\u53c9\u9a8c\u8bc1\u4fdd\u6301\u6027\u80fd\uff0c\u4ea7\u751f\u5177\u6709\u66f4\u597d\u53ef\u89e3\u91ca\u6027\u7684\u9884\u6d4b\u8868\u9762\u3002", "conclusion": "\u534a\u76d1\u7763\u5b66\u4e60\u4e3a\u5728\u5927\u578b\u3001\u7a00\u758f\u6807\u6ce8\u7684\u666f\u89c2\u4e2d\u8bc6\u522b\u672a\u53d1\u73b0\u9057\u5740\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b9\u6cd5\u3002"}}
{"id": "2510.16816", "categories": ["cs.LG", "cs.AI", "math-ph", "math.MP", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.16816", "abs": "https://arxiv.org/abs/2510.16816", "authors": ["Ming Zhong", "Zhenya Yan"], "title": "Efficient High-Accuracy PDEs Solver with the Linear Attention Neural Operator", "comment": "31 pages, 8 figures", "summary": "Neural operators offer a powerful data-driven framework for learning mappings\nbetween function spaces, in which the transformer-based neural operator\narchitecture faces a fundamental scalability-accuracy trade-off: softmax\nattention provides excellent fidelity but incurs quadratic complexity\n$\\mathcal{O}(N^2 d)$ in the number of mesh points $N$ and hidden dimension $d$,\nwhile linear attention variants reduce cost to $\\mathcal{O}(N d^2)$ but often\nsuffer significant accuracy degradation. To address the aforementioned\nchallenge, in this paper, we present a novel type of neural operators, Linear\nAttention Neural Operator (LANO), which achieves both scalability and high\naccuracy by reformulating attention through an agent-based mechanism. LANO\nresolves this dilemma by introducing a compact set of $M$ agent tokens $(M \\ll\nN)$ that mediate global interactions among $N$ tokens. This agent attention\nmechanism yields an operator layer with linear complexity $\\mathcal{O}(MN d)$\nwhile preserving the expressive power of softmax attention. Theoretically, we\ndemonstrate the universal approximation property, thereby demonstrating\nimproved conditioning and stability properties. Empirically, LANO surpasses\ncurrent state-of-the-art neural PDE solvers, including Transolver with\nslice-based softmax attention, achieving average $19.5\\%$ accuracy improvement\nacross standard benchmarks. By bridging the gap between linear complexity and\nsoftmax-level performance, LANO establishes a scalable, high-accuracy\nfoundation for scientific machine learning applications.", "AI": {"tldr": "\u63d0\u51fa\u7ebf\u6027\u6ce8\u610f\u529b\u795e\u7ecf\u7b97\u5b50(LANO)\uff0c\u901a\u8fc7\u5f15\u5165\u4ee3\u7406\u4ee4\u724c\u673a\u5236\u5728\u4fdd\u6301\u8f6f\u6ce8\u610f\u529b\u8868\u8fbe\u80fd\u529b\u7684\u540c\u65f6\u5b9e\u73b0\u7ebf\u6027\u590d\u6742\u5ea6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6ce8\u610f\u529b\u5728\u795e\u7ecf\u7b97\u5b50\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u4e0e\u7cbe\u5ea6\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u57fa\u4e8etransformer\u7684\u795e\u7ecf\u7b97\u5b50\u67b6\u6784\u9762\u4e34\u7684\u57fa\u672c\u53ef\u6269\u5c55\u6027-\u7cbe\u5ea6\u6743\u8861\uff1a\u8f6f\u6ce8\u610f\u529b\u63d0\u4f9b\u826f\u597d\u4fdd\u771f\u5ea6\u4f46\u5177\u6709\u4e8c\u6b21\u590d\u6742\u5ea6\uff0c\u800c\u7ebf\u6027\u6ce8\u610f\u529b\u53d8\u4f53\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u4f46\u5f80\u5f80\u906d\u53d7\u663e\u8457\u7cbe\u5ea6\u4e0b\u964d\u3002", "method": "\u5f15\u5165\u7d27\u51d1\u7684\u4ee3\u7406\u4ee4\u724c\u96c6(M \u226a N)\u6765\u8c03\u89e3N\u4e2a\u4ee4\u724c\u4e4b\u95f4\u7684\u5168\u5c40\u4ea4\u4e92\uff0c\u8fd9\u79cd\u4ee3\u7406\u6ce8\u610f\u529b\u673a\u5236\u4ea7\u751f\u5177\u6709\u7ebf\u6027\u590d\u6742\u5ea6\u7684\u7b97\u5b50\u5c42\uff0c\u540c\u65f6\u4fdd\u6301\u8f6f\u6ce8\u610f\u529b\u7684\u8868\u8fbe\u80fd\u529b\u3002", "result": "\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86\u901a\u7528\u903c\u8fd1\u6027\u8d28\uff0c\u5b9e\u8bc1\u4e0aLANO\u8d85\u8d8a\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684\u795e\u7ecfPDE\u6c42\u89e3\u5668\uff0c\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u7cbe\u5ea6\u63d0\u534719.5%\u3002", "conclusion": "\u901a\u8fc7\u5728\u7ebf\u6027\u590d\u6742\u5ea6\u548c\u8f6f\u6ce8\u610f\u529b\u7ea7\u6027\u80fd\u4e4b\u95f4\u67b6\u8d77\u6865\u6881\uff0cLANO\u4e3a\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u5e94\u7528\u5efa\u7acb\u4e86\u53ef\u6269\u5c55\u3001\u9ad8\u7cbe\u5ea6\u7684\u57fa\u7840\u3002"}}
{"id": "2510.16882", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16882", "abs": "https://arxiv.org/abs/2510.16882", "authors": ["Heming Zou", "Yixiu Mao", "Yun Qu", "Qi Wang", "Xiangyang Ji"], "title": "Utility-Diversity Aware Online Batch Selection for LLM Supervised Fine-tuning", "comment": null, "summary": "Supervised fine-tuning (SFT) is a commonly used technique to adapt large\nlanguage models (LLMs) to downstream tasks. In practice, SFT on a full dataset\nis computationally expensive and sometimes suffers from overfitting or bias\namplification. This facilitates the rise of data curation in SFT, which\nprioritizes the most valuable data to optimze. This work studies the online\nbatch selection family that dynamically scores and filters samples during the\ntraining process. However, existing popular methods often (i) rely merely on\nthe utility of data to select a subset while neglecting other crucial factors\nlike diversity, (ii) rely on external resources such as reference models or\nvalidation sets, and (iii) incur extra training time over full-dataset\ntraining. To address these limitations, this work develops \\textbf{UDS\n(Utility-Diversity Sampling)}, a framework for efficient online batch selection\nin SFT. UDS leverages the nuclear norm of the logits matrix to capture both\ndata utility and intra-sample diversity, while estimating inter-sample\ndiversity through efficient low-dimensional embedding comparisons with a\nlightweight memory buffer of historical samples. Such a design eliminates the\nneed for external resources and unnecessary backpropagation, securing\ncomputational efficiency. Experiments on multiple benchmarks demonstrate that\nUDS consistently outperforms state-of-the-art online batch selection methods\nunder varying data budgets, and significantly reduces training time compared to\nfull-dataset fine-tuning. Code is available at https://github.com/gfyddha/UDS.", "AI": {"tldr": "UDS\u662f\u4e00\u79cd\u7528\u4e8e\u76d1\u7763\u5fae\u8c03\u7684\u9ad8\u6548\u5728\u7ebf\u6279\u6b21\u9009\u62e9\u6846\u67b6\uff0c\u901a\u8fc7\u6838\u8303\u6570\u6355\u83b7\u6570\u636e\u6548\u7528\u548c\u6837\u672c\u5185\u591a\u6837\u6027\uff0c\u5229\u7528\u8f7b\u91cf\u7ea7\u5185\u5b58\u7f13\u51b2\u533a\u4f30\u8ba1\u6837\u672c\u95f4\u591a\u6837\u6027\uff0c\u65e0\u9700\u5916\u90e8\u8d44\u6e90\u5373\u53ef\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u5728\u7ebf\u6279\u6b21\u9009\u62e9\u65b9\u6cd5\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a(i)\u4ec5\u4f9d\u8d56\u6570\u636e\u6548\u7528\u800c\u5ffd\u89c6\u591a\u6837\u6027\uff1b(ii)\u9700\u8981\u5916\u90e8\u8d44\u6e90\u5982\u53c2\u8003\u6a21\u578b\u6216\u9a8c\u8bc1\u96c6\uff1b(iii)\u8bad\u7ec3\u65f6\u95f4\u8d85\u8fc7\u5168\u6570\u636e\u96c6\u8bad\u7ec3\u3002UDS\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u3002", "method": "UDS\u5229\u7528\u5bf9\u6570\u77e9\u9635\u7684\u6838\u8303\u6570\u6355\u83b7\u6570\u636e\u6548\u7528\u548c\u6837\u672c\u5185\u591a\u6837\u6027\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u5185\u5b58\u7f13\u51b2\u533a\u4e2d\u7684\u4f4e\u7ef4\u5d4c\u5165\u6bd4\u8f83\u4f30\u8ba1\u6837\u672c\u95f4\u591a\u6837\u6027\uff0c\u65e0\u9700\u5916\u90e8\u8d44\u6e90\u548c\u4e0d\u5fc5\u8981\u7684\u53cd\u5411\u4f20\u64ad\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cUDS\u5728\u4e0d\u540c\u6570\u636e\u9884\u7b97\u4e0b\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u5728\u7ebf\u6279\u6b21\u9009\u62e9\u65b9\u6cd5\uff0c\u76f8\u6bd4\u5168\u6570\u636e\u96c6\u5fae\u8c03\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u3002", "conclusion": "UDS\u662f\u4e00\u4e2a\u9ad8\u6548\u4e14\u65e0\u9700\u5916\u90e8\u8d44\u6e90\u7684\u5728\u7ebf\u6279\u6b21\u9009\u62e9\u6846\u67b6\uff0c\u5728\u76d1\u7763\u5fae\u8c03\u4e2d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2510.16817", "categories": ["cs.LG", "math.AP"], "pdf": "https://arxiv.org/pdf/2510.16817", "abs": "https://arxiv.org/abs/2510.16817", "authors": ["Doyoon Kim", "Junbin Song"], "title": "Trace Regularity PINNs: Enforcing $\\mathrm{H}^{\\frac{1}{2}}(\\partial \u03a9)$ for Boundary Data", "comment": null, "summary": "We propose an enhanced physics-informed neural network (PINN), the Trace\nRegularity Physics-Informed Neural Network (TRPINN), which enforces the\nboundary loss in the Sobolev-Slobodeckij norm $H^{1/2}(\\partial \\Omega)$, the\ncorrect trace space associated with $H^1(\\Omega)$. We reduce computational cost\nby computing only the theoretically essential portion of the semi-norm and\nenhance convergence stability by avoiding denominator evaluations in the\ndiscretization. By incorporating the exact $H^{1/2}(\\partial \\Omega)$ norm, we\nshow that the approximation converges to the true solution in the\n$H^{1}(\\Omega)$ sense, and, through Neural Tangent Kernel (NTK) analysis, we\ndemonstrate that TRPINN can converge faster than standard PINNs. Numerical\nexperiments on the Laplace equation with highly oscillatory Dirichlet boundary\nconditions exhibit cases where TRPINN succeeds even when standard PINNs fail,\nand show performance improvements of one to three decimal digits.", "AI": {"tldr": "\u63d0\u51faTRPINN\u65b9\u6cd5\uff0c\u5728Sobolev-Slobodeckij\u8303\u6570H^{1/2}(\u2202\u03a9)\u4e2d\u5f3a\u5236\u8fb9\u754c\u635f\u5931\uff0c\u8fd9\u662f\u4e0eH^1(\u03a9)\u76f8\u5173\u7684\u6b63\u786e\u8ff9\u7a7a\u95f4\u3002\u901a\u8fc7\u8ba1\u7b97\u534a\u8303\u6570\u7684\u7406\u8bba\u5fc5\u8981\u90e8\u5206\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u907f\u514d\u79bb\u6563\u5316\u4e2d\u7684\u5206\u6bcd\u6c42\u503c\u589e\u5f3a\u6536\u655b\u7a33\u5b9a\u6027\u3002", "motivation": "\u6807\u51c6PINNs\u5728\u5904\u7406\u9ad8\u5ea6\u632f\u8361Dirichlet\u8fb9\u754c\u6761\u4ef6\u65f6\u53ef\u80fd\u5931\u8d25\uff0c\u9700\u8981\u6539\u8fdb\u8fb9\u754c\u635f\u5931\u7684\u8ba1\u7b97\u65b9\u5f0f\u4ee5\u63d0\u9ad8\u6536\u655b\u6027\u548c\u7a33\u5b9a\u6027\u3002", "method": "\u4f7f\u7528TRPINN\u65b9\u6cd5\uff0c\u5728\u6b63\u786e\u7684\u8ff9\u7a7a\u95f4H^{1/2}(\u2202\u03a9)\u4e2d\u5f3a\u5236\u8fb9\u754c\u635f\u5931\uff0c\u4ec5\u8ba1\u7b97\u534a\u8303\u6570\u7684\u7406\u8bba\u5fc5\u8981\u90e8\u5206\uff0c\u907f\u514d\u5206\u6bcd\u6c42\u503c\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u663e\u793aTRPINN\u5728\u6807\u51c6PINNs\u5931\u8d25\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u6210\u529f\uff0c\u6027\u80fd\u63d0\u53471-3\u4e2a\u5341\u8fdb\u5236\u6570\u5b57\u3002\u901a\u8fc7NTK\u5206\u6790\u8bc1\u660eTRPINN\u6bd4\u6807\u51c6PINNs\u6536\u655b\u66f4\u5feb\u3002", "conclusion": "TRPINN\u901a\u8fc7\u4f7f\u7528\u6b63\u786e\u7684\u8ff9\u7a7a\u95f4\u8303\u6570\uff0c\u63d0\u9ad8\u4e86PINNs\u7684\u6536\u655b\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u590d\u6742\u8fb9\u754c\u6761\u4ef6\u65f6\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2510.16820", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16820", "abs": "https://arxiv.org/abs/2510.16820", "authors": ["Thomas Dooms", "Ward Gauderis"], "title": "Finding Manifolds With Bilinear Autoencoders", "comment": null, "summary": "Sparse autoencoders are a standard tool for uncovering interpretable latent\nrepresentations in neural networks. Yet, their interpretation depends on the\ninputs, making their isolated study incomplete. Polynomials offer a solution;\nthey serve as algebraic primitives that can be analysed without reference to\ninput and can describe structures ranging from linear concepts to complicated\nmanifolds. This work uses bilinear autoencoders to efficiently decompose\nrepresentations into quadratic polynomials. We discuss improvements that induce\nimportance ordering, clustering, and activation sparsity. This is an initial\nstep toward nonlinear yet analysable latents through their algebraic\nproperties.", "AI": {"tldr": "\u4f7f\u7528\u53cc\u7ebf\u6027\u81ea\u7f16\u7801\u5668\u5c06\u8868\u793a\u5206\u89e3\u4e3a\u4e8c\u6b21\u591a\u9879\u5f0f\uff0c\u5b9e\u73b0\u975e\u7ebf\u6027\u4f46\u53ef\u5206\u6790\u7684\u6f5c\u5728\u8868\u793a", "motivation": "\u7a00\u758f\u81ea\u7f16\u7801\u5668\u4f9d\u8d56\u4e8e\u8f93\u5165\uff0c\u96be\u4ee5\u72ec\u7acb\u7814\u7a76\u3002\u591a\u9879\u5f0f\u4f5c\u4e3a\u4ee3\u6570\u57fa\u5143\u53ef\u4ee5\u5728\u4e0d\u4f9d\u8d56\u8f93\u5165\u7684\u60c5\u51b5\u4e0b\u5206\u6790\uff0c\u80fd\u591f\u63cf\u8ff0\u4ece\u7ebf\u6027\u6982\u5ff5\u5230\u590d\u6742\u6d41\u5f62\u7684\u7ed3\u6784", "method": "\u4f7f\u7528\u53cc\u7ebf\u6027\u81ea\u7f16\u7801\u5668\u9ad8\u6548\u5730\u5c06\u8868\u793a\u5206\u89e3\u4e3a\u4e8c\u6b21\u591a\u9879\u5f0f\uff0c\u5e76\u5f15\u5165\u6539\u8fdb\u4ee5\u8bf1\u5bfc\u91cd\u8981\u6027\u6392\u5e8f\u3001\u805a\u7c7b\u548c\u6fc0\u6d3b\u7a00\u758f\u6027", "result": "\u5f00\u53d1\u4e86\u4e00\u79cd\u80fd\u591f\u5c06\u795e\u7ecf\u7f51\u7edc\u8868\u793a\u5206\u89e3\u4e3a\u4e8c\u6b21\u591a\u9879\u5f0f\u7684\u65b9\u6cd5\uff0c\u4e3a\u901a\u8fc7\u4ee3\u6570\u5c5e\u6027\u5206\u6790\u975e\u7ebf\u6027\u6f5c\u5728\u8868\u793a\u63d0\u4f9b\u4e86\u521d\u6b65\u89e3\u51b3\u65b9\u6848", "conclusion": "\u8fd9\u662f\u901a\u8fc7\u4ee3\u6570\u5c5e\u6027\u5b9e\u73b0\u975e\u7ebf\u6027\u4f46\u53ef\u5206\u6790\u6f5c\u5728\u8868\u793a\u7684\u7b2c\u4e00\u6b65\uff0c\u4e3a\u7406\u89e3\u795e\u7ecf\u7f51\u7edc\u5185\u90e8\u8868\u793a\u63d0\u4f9b\u4e86\u65b0\u7684\u5206\u6790\u5de5\u5177"}}
{"id": "2510.16943", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16943", "abs": "https://arxiv.org/abs/2510.16943", "authors": ["Dania Refai", "Moataz Ahmed"], "title": "Peering Inside the Black Box: Uncovering LLM Errors in Optimization Modelling through Component-Level Evaluation", "comment": null, "summary": "Large language models (LLMs) are increasingly used to convert natural\nlanguage descriptions into mathematical optimization formulations. Current\nevaluations often treat formulations as a whole, relying on coarse metrics like\nsolution accuracy or runtime, which obscure structural or numerical errors. In\nthis study, we present a comprehensive, component-level evaluation framework\nfor LLM-generated formulations. Beyond the conventional optimality gap, our\nframework introduces metrics such as precision and recall for decision\nvariables and constraints, constraint and objective root mean squared error\n(RMSE), and efficiency indicators based on token usage and latency. We evaluate\nGPT-5, LLaMA 3.1 Instruct, and DeepSeek Math across optimization problems of\nvarying complexity under six prompting strategies. Results show that GPT-5\nconsistently outperforms other models, with chain-of-thought, self-consistency,\nand modular prompting proving most effective. Analysis indicates that solver\nperformance depends primarily on high constraint recall and low constraint\nRMSE, which together ensure structural correctness and solution reliability.\nConstraint precision and decision variable metrics play secondary roles, while\nconcise outputs enhance computational efficiency. These findings highlight\nthree principles for NLP-to-optimization modeling: (i) Complete constraint\ncoverage prevents violations, (ii) minimizing constraint RMSE ensures\nsolver-level accuracy, and (iii) concise outputs improve computational\nefficiency. The proposed framework establishes a foundation for fine-grained,\ndiagnostic evaluation of LLMs in optimization modeling.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ec4\u4ef6\u7ea7\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u751f\u6210\u7684\u6570\u5b66\u4f18\u5316\u516c\u5f0f\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u6574\u4f53\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u4e2a\u7cbe\u7ec6\u6307\u6807\u6765\u5206\u6790\u7ed3\u6784\u6027\u548c\u6570\u503c\u6027\u9519\u8bef\u3002", "motivation": "\u5f53\u524d\u8bc4\u4f30\u65b9\u6cd5\u901a\u5e38\u5c06\u4f18\u5316\u516c\u5f0f\u89c6\u4e3a\u6574\u4f53\uff0c\u4f9d\u8d56\u7c97\u7565\u6307\u6807\u5982\u89e3\u7cbe\u5ea6\u6216\u8fd0\u884c\u65f6\u95f4\uff0c\u8fd9\u63a9\u76d6\u4e86\u7ed3\u6784\u6216\u6570\u503c\u9519\u8bef\u3002\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u8bca\u65adLLM\u5728\u4f18\u5316\u5efa\u6a21\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u5f00\u53d1\u4e86\u5305\u542b\u51b3\u7b56\u53d8\u91cf\u548c\u7ea6\u675f\u7684\u7cbe\u786e\u7387\u4e0e\u53ec\u56de\u7387\u3001\u7ea6\u675f\u548c\u76ee\u6807\u51fd\u6570\u7684\u5747\u65b9\u6839\u8bef\u5dee\u3001\u57fa\u4e8etoken\u4f7f\u7528\u548c\u5ef6\u8fdf\u7684\u6548\u7387\u6307\u6807\u7684\u7efc\u5408\u8bc4\u4f30\u6846\u67b6\u3002\u8bc4\u4f30\u4e86GPT-5\u3001LLaMA 3.1 Instruct\u548cDeepSeek Math\u5728\u4e0d\u540c\u590d\u6742\u5ea6\u4f18\u5316\u95ee\u9898\u4e0b\u7684\u516d\u79cd\u63d0\u793a\u7b56\u7565\u3002", "result": "GPT-5\u8868\u73b0\u6700\u4f73\uff0c\u601d\u7ef4\u94fe\u3001\u81ea\u4e00\u81f4\u6027\u548c\u6a21\u5757\u5316\u63d0\u793a\u6700\u6709\u6548\u3002\u6c42\u89e3\u5668\u6027\u80fd\u4e3b\u8981\u53d6\u51b3\u4e8e\u9ad8\u7ea6\u675f\u53ec\u56de\u7387\u548c\u4f4e\u7ea6\u675fRMSE\uff0c\u786e\u4fdd\u7ed3\u6784\u6b63\u786e\u6027\u548c\u89e3\u53ef\u9760\u6027\u3002\u7ea6\u675f\u7cbe\u786e\u7387\u548c\u51b3\u7b56\u53d8\u91cf\u6307\u6807\u8d77\u6b21\u8981\u4f5c\u7528\uff0c\u7b80\u6d01\u8f93\u51fa\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u63d0\u51fa\u4e86NLP\u5230\u4f18\u5316\u5efa\u6a21\u7684\u4e09\u4e2a\u539f\u5219\uff1a\u5b8c\u6574\u7ea6\u675f\u8986\u76d6\u9632\u6b62\u8fdd\u89c4\u3001\u6700\u5c0f\u5316\u7ea6\u675fRMSE\u786e\u4fdd\u6c42\u89e3\u5668\u7ea7\u7cbe\u5ea6\u3001\u7b80\u6d01\u8f93\u51fa\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002\u8be5\u6846\u67b6\u4e3aLLM\u5728\u4f18\u5316\u5efa\u6a21\u4e2d\u7684\u7ec6\u7c92\u5ea6\u8bca\u65ad\u8bc4\u4f30\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.16824", "categories": ["cs.LG", "q-bio.MN"], "pdf": "https://arxiv.org/pdf/2510.16824", "abs": "https://arxiv.org/abs/2510.16824", "authors": ["Yingxu Wang", "Kunyu Zhang", "Jiaxin Huang", "Nan Yin", "Siwei Liu", "Eran Segal"], "title": "ProtoMol: Enhancing Molecular Property Prediction via Prototype-Guided Multimodal Learning", "comment": null, "summary": "Multimodal molecular representation learning, which jointly models molecular\ngraphs and their textual descriptions, enhances predictive accuracy and\ninterpretability by enabling more robust and reliable predictions of drug\ntoxicity, bioactivity, and physicochemical properties through the integration\nof structural and semantic information. However, existing multimodal methods\nsuffer from two key limitations: (1) they typically perform cross-modal\ninteraction only at the final encoder layer, thus overlooking hierarchical\nsemantic dependencies; (2) they lack a unified prototype space for robust\nalignment between modalities. To address these limitations, we propose\nProtoMol, a prototype-guided multimodal framework that enables fine-grained\nintegration and consistent semantic alignment between molecular graphs and\ntextual descriptions. ProtoMol incorporates dual-branch hierarchical encoders,\nutilizing Graph Neural Networks to process structured molecular graphs and\nTransformers to encode unstructured texts, resulting in comprehensive\nlayer-wise representations. Then, ProtoMol introduces a layer-wise\nbidirectional cross-modal attention mechanism that progressively aligns\nsemantic features across layers. Furthermore, a shared prototype space with\nlearnable, class-specific anchors is constructed to guide both modalities\ntoward coherent and discriminative representations. Extensive experiments on\nmultiple benchmark datasets demonstrate that ProtoMol consistently outperforms\nstate-of-the-art baselines across a variety of molecular property prediction\ntasks.", "AI": {"tldr": "ProtoMol\u662f\u4e00\u4e2a\u539f\u578b\u5f15\u5bfc\u7684\u591a\u6a21\u6001\u5206\u5b50\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5c42\u6b21\u7f16\u7801\u5668\u548c\u53cc\u5411\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b9e\u73b0\u5206\u5b50\u56fe\u4e0e\u6587\u672c\u63cf\u8ff0\u4e4b\u95f4\u7684\u7ec6\u7c92\u5ea6\u6574\u5408\u548c\u8bed\u4e49\u5bf9\u9f50\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u5c40\u9650\uff1a(1)\u4ec5\u5728\u6700\u7ec8\u7f16\u7801\u5c42\u8fdb\u884c\u8de8\u6a21\u6001\u4ea4\u4e92\uff0c\u5ffd\u7565\u4e86\u5c42\u6b21\u8bed\u4e49\u4f9d\u8d56\uff1b(2)\u7f3a\u4e4f\u7edf\u4e00\u7684\u539f\u578b\u7a7a\u95f4\u6765\u5b9e\u73b0\u6a21\u6001\u95f4\u7684\u7a33\u5065\u5bf9\u9f50\u3002", "method": "\u91c7\u7528\u53cc\u5206\u652f\u5c42\u6b21\u7f16\u7801\u5668\uff08\u56fe\u795e\u7ecf\u7f51\u7edc\u5904\u7406\u5206\u5b50\u56fe\uff0cTransformer\u7f16\u7801\u6587\u672c\uff09\uff0c\u5f15\u5165\u5c42\u6b21\u53cc\u5411\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5e76\u6784\u5efa\u5177\u6709\u53ef\u5b66\u4e60\u7c7b\u7279\u5b9a\u951a\u70b9\u7684\u5171\u4eab\u539f\u578b\u7a7a\u95f4\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cProtoMol\u5728\u5404\u79cd\u5206\u5b50\u6027\u8d28\u9884\u6d4b\u4efb\u52a1\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "ProtoMol\u901a\u8fc7\u539f\u578b\u5f15\u5bfc\u7684\u591a\u6a21\u6001\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u5206\u5b50\u56fe\u4e0e\u6587\u672c\u63cf\u8ff0\u4e4b\u95f4\u7684\u7ec6\u7c92\u5ea6\u6574\u5408\u548c\u4e00\u81f4\u8bed\u4e49\u5bf9\u9f50\u3002"}}
{"id": "2510.16968", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16968", "abs": "https://arxiv.org/abs/2510.16968", "authors": ["Pingzhi Li", "Morris Yu-Chao Huang", "Zhen Tan", "Qingquan Song", "Jie Peng", "Kai Zou", "Yu Cheng", "Kaidi Xu", "Tianlong Chen"], "title": "Leave It to the Experts: Detecting Knowledge Distillation via MoE Expert Signatures", "comment": "Code is at https://github.com/unites-lab/shadow-moe", "summary": "Knowledge Distillation (KD) accelerates training of large language models\n(LLMs) but poses intellectual property protection and LLM diversity risks.\nExisting KD detection methods based on self-identity or output similarity can\nbe easily evaded through prompt engineering. We present a KD detection\nframework effective in both white-box and black-box settings by exploiting an\noverlooked signal: the transfer of MoE \"structural habits\", especially internal\nrouting patterns. Our approach analyzes how different experts specialize and\ncollaborate across various inputs, creating distinctive fingerprints that\npersist through the distillation process. To extend beyond the white-box setup\nand MoE architectures, we further propose Shadow-MoE, a black-box method that\nconstructs proxy MoE representations via auxiliary distillation to compare\nthese patterns between arbitrary model pairs. We establish a comprehensive,\nreproducible benchmark that offers diverse distilled checkpoints and an\nextensible framework to facilitate future research. Extensive experiments\ndemonstrate >94% detection accuracy across various scenarios and strong\nrobustness to prompt-based evasion, outperforming existing baselines while\nhighlighting the structural habits transfer in LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMoE\u7ed3\u6784\u4e60\u60ef\u7684\u77e5\u8bc6\u84b8\u998f\u68c0\u6d4b\u6846\u67b6\uff0c\u5728\u9ed1\u767d\u76d2\u8bbe\u7f6e\u4e0b\u90fd\u80fd\u6709\u6548\u68c0\u6d4bLLM\u7684\u77e5\u8bc6\u84b8\u998f\uff0c\u51c6\u786e\u7387\u8d85\u8fc794%\u4e14\u5bf9\u63d0\u793a\u5de5\u7a0b\u89c4\u907f\u5177\u6709\u5f3a\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u81ea\u8eab\u4efd\u6216\u8f93\u51fa\u76f8\u4f3c\u6027\u7684KD\u68c0\u6d4b\u65b9\u6cd5\u5bb9\u6613\u88ab\u63d0\u793a\u5de5\u7a0b\u89c4\u907f\uff0c\u5b58\u5728\u77e5\u8bc6\u4ea7\u6743\u4fdd\u62a4\u548cLLM\u591a\u6837\u6027\u98ce\u9669\u3002", "method": "\u5229\u7528MoE\u7ed3\u6784\u4e60\u60ef\uff08\u7279\u522b\u662f\u5185\u90e8\u8def\u7531\u6a21\u5f0f\uff09\u4f5c\u4e3a\u68c0\u6d4b\u4fe1\u53f7\uff0c\u5206\u6790\u4e13\u5bb6\u5728\u4e0d\u540c\u8f93\u5165\u4e0a\u7684\u4e13\u4e1a\u5316\u548c\u534f\u4f5c\u6a21\u5f0f\uff1b\u63d0\u51faShadow-MoE\u65b9\u6cd5\u5728black-box\u8bbe\u7f6e\u4e0b\u6784\u5efa\u4ee3\u7406MoE\u8868\u793a\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u5efa\u7acb\u4e86\u5168\u9762\u7684\u53ef\u590d\u73b0\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5b9e\u9a8c\u663e\u793a\u5728\u5404\u79cd\u573a\u666f\u4e0b\u68c0\u6d4b\u51c6\u786e\u7387\u8d85\u8fc794%\uff0c\u5bf9\u57fa\u4e8e\u63d0\u793a\u7684\u89c4\u907f\u5177\u6709\u5f3a\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u7a81\u663e\u4e86LLM\u4e2d\u7ed3\u6784\u4e60\u60ef\u8f6c\u79fb\u7684\u91cd\u8981\u6027\uff0c\u4e3aKD\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.16857", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16857", "abs": "https://arxiv.org/abs/2510.16857", "authors": ["Jiyan Qiu", "Lyulin Kuang", "Guan Wang", "Yichen Xu", "Leiyao Cui", "Shaotong Fu", "Yixin Zhu", "Ruihua Zhang"], "title": "DrivAerStar: An Industrial-Grade CFD Dataset for Vehicle Aerodynamic Optimization", "comment": null, "summary": "Vehicle aerodynamics optimization has become critical for automotive\nelectrification, where drag reduction directly determines electric vehicle\nrange and energy efficiency. Traditional approaches face an intractable\ntrade-off: computationally expensive Computational Fluid Dynamics (CFD)\nsimulations requiring weeks per design iteration, or simplified models that\nsacrifice production-grade accuracy. While machine learning offers\ntransformative potential, existing datasets exhibit fundamental limitations --\ninadequate mesh resolution, missing vehicle components, and validation errors\nexceeding 5% -- preventing deployment in industrial workflows. We present\nDrivAerStar, comprising 12,000 industrial-grade automotive CFD simulations\ngenerated using $\\text{STAR-CCM+}^\\unicode{xAE}$ software. The dataset\nsystematically explores three vehicle configurations through 20 Computer Aided\nDesign (CAD) parameters via Free Form Deformation (FFD) algorithms, including\ncomplete engine compartments and cooling systems with realistic internal\nairflow. DrivAerStar achieves wind tunnel validation accuracy below 1.04% -- a\nfive-fold improvement over existing datasets -- through refined mesh strategies\nwith strict wall $y^+$ control. Benchmarks demonstrate that models trained on\nthis data achieve production-ready accuracy while reducing computational costs\nfrom weeks to minutes. This represents the first dataset bridging academic\nmachine learning research and industrial CFD practice, establishing a new\nstandard for data-driven aerodynamic optimization in automotive development.\nBeyond automotive applications, DrivAerStar demonstrates a paradigm for\nintegrating high-fidelity physics simulations with Artificial Intelligence (AI)\nacross engineering disciplines where computational constraints currently limit\ninnovation.", "AI": {"tldr": "DrivAerStar\u662f\u4e00\u4e2a\u5305\u542b12,000\u4e2a\u5de5\u4e1a\u7ea7\u6c7d\u8f66CFD\u6a21\u62df\u7684\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u6539\u8fdb\u7684\u7f51\u683c\u7b56\u7565\u5b9e\u73b0\u98ce\u6d1e\u9a8c\u8bc1\u7cbe\u5ea6\u4f4e\u4e8e1.04%\uff0c\u6bd4\u73b0\u6709\u6570\u636e\u96c6\u63d0\u53475\u500d\uff0c\u5c06\u8ba1\u7b97\u6210\u672c\u4ece\u6570\u5468\u51cf\u5c11\u5230\u5206\u949f\u7ea7\u522b\u3002", "motivation": "\u8f66\u8f86\u7a7a\u6c14\u52a8\u529b\u5b66\u4f18\u5316\u5bf9\u6c7d\u8f66\u7535\u52a8\u5316\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u9762\u4e34\u8ba1\u7b97\u6210\u672c\u9ad8\u4e0e\u7cbe\u5ea6\u4e0d\u8db3\u7684\u6743\u8861\uff0c\u73b0\u6709\u673a\u5668\u5b66\u4e60\u6570\u636e\u96c6\u5b58\u5728\u7f51\u683c\u5206\u8fa8\u7387\u4e0d\u8db3\u3001\u7ec4\u4ef6\u7f3a\u5931\u548c\u9a8c\u8bc1\u8bef\u5dee\u8d85\u8fc75%\u7b49\u95ee\u9898\uff0c\u65e0\u6cd5\u5728\u5de5\u4e1a\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u90e8\u7f72\u3002", "method": "\u4f7f\u7528STAR-CCM+\u8f6f\u4ef6\u751f\u621012,000\u4e2a\u5de5\u4e1a\u7ea7CFD\u6a21\u62df\uff0c\u901a\u8fc720\u4e2aCAD\u53c2\u6570\u548c\u81ea\u7531\u5f62\u53d8\u7b97\u6cd5\u7cfb\u7edf\u63a2\u7d22\u4e09\u79cd\u8f66\u8f86\u914d\u7f6e\uff0c\u5305\u62ec\u5b8c\u6574\u7684\u53d1\u52a8\u673a\u8231\u548c\u51b7\u5374\u7cfb\u7edf\uff0c\u91c7\u7528\u7cbe\u7ec6\u7f51\u683c\u7b56\u7565\u548c\u4e25\u683c\u7684\u58c1\u9762y+\u63a7\u5236\u3002", "result": "\u6570\u636e\u96c6\u5b9e\u73b0\u98ce\u6d1e\u9a8c\u8bc1\u7cbe\u5ea6\u4f4e\u4e8e1.04%\uff0c\u6bd4\u73b0\u6709\u6570\u636e\u96c6\u63d0\u53475\u500d\uff0c\u57fa\u4e8e\u8be5\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u4fdd\u6301\u751f\u4ea7\u7ea7\u7cbe\u5ea6\u7684\u540c\u65f6\u5c06\u8ba1\u7b97\u6210\u672c\u4ece\u6570\u5468\u51cf\u5c11\u5230\u5206\u949f\u7ea7\u522b\u3002", "conclusion": "DrivAerStar\u662f\u9996\u4e2a\u8fde\u63a5\u5b66\u672f\u673a\u5668\u5b66\u4e60\u7814\u7a76\u4e0e\u5de5\u4e1aCFD\u5b9e\u8df5\u7684\u6570\u636e\u96c6\uff0c\u4e3a\u6c7d\u8f66\u5f00\u53d1\u4e2d\u7684\u6570\u636e\u9a71\u52a8\u7a7a\u6c14\u52a8\u529b\u5b66\u4f18\u5316\u8bbe\u7acb\u4e86\u65b0\u6807\u51c6\uff0c\u5c55\u793a\u4e86\u5c06\u9ad8\u4fdd\u771f\u7269\u7406\u6a21\u62df\u4e0eAI\u6574\u5408\u7684\u8303\u5f0f\u3002"}}
{"id": "2510.17021", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17021", "abs": "https://arxiv.org/abs/2510.17021", "authors": ["Bingqi Shang", "Yiwei Chen", "Yihua Zhang", "Bingquan Shen", "Sijia Liu"], "title": "Forgetting to Forget: Attention Sink as A Gateway for Backdooring LLM Unlearning", "comment": null, "summary": "Large language model (LLM) unlearning has become a critical mechanism for\nremoving undesired data, knowledge, or behaviors from pre-trained models while\nretaining their general utility. Yet, with the rise of open-weight LLMs, we\nask: can the unlearning process itself be backdoored, appearing successful\nunder normal conditions yet reverting to pre-unlearned behavior when a hidden\ntrigger is activated? Drawing inspiration from classical backdoor attacks that\nembed triggers into training data to enforce specific behaviors, we investigate\nbackdoor unlearning, where models forget as intended in the clean setting but\nrecover forgotten knowledge when the trigger appears. We show that designing\nsuch attacks presents unique challenges, hinging on where triggers are placed\nand how backdoor training is reinforced. We uncover a strong link between\nbackdoor efficacy and the attention sink phenomenon, i.e., shallow input tokens\nconsistently attract disproportionate attention in LLMs. Our analysis reveals\nthat these attention sinks serve as gateways for backdoor unlearning: placing\ntriggers at sink positions and aligning their attention values markedly\nenhances backdoor persistence. Extensive experiments validate these findings,\nshowing that attention-sink-guided backdoor unlearning reliably restores\nforgotten knowledge in the presence of backdoor triggers, while behaving\nindistinguishably from a normally unlearned model when triggers are absent.\nCode is available at https://github.com/OPTML-Group/Unlearn-Backdoor.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u9057\u5fd8\u8fc7\u7a0b\u7684\u9690\u853d\u653b\u51fb\u65b9\u6cd5\u2014\u2014\u540e\u95e8\u9057\u5fd8\u653b\u51fb\uff0c\u8be5\u653b\u51fb\u8ba9\u6a21\u578b\u5728\u6b63\u5e38\u6761\u4ef6\u4e0b\u8868\u73b0\u9057\u5fd8\u6210\u529f\uff0c\u4f46\u5728\u7279\u5b9a\u89e6\u53d1\u8bcd\u51fa\u73b0\u65f6\u6062\u590d\u88ab\u9057\u5fd8\u7684\u77e5\u8bc6\u3002", "motivation": "\u968f\u7740\u5f00\u6e90\u6743\u91cd\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5174\u8d77\uff0c\u7814\u7a76\u9057\u5fd8\u8fc7\u7a0b\u672c\u8eab\u662f\u5426\u53ef\u80fd\u88ab\u690d\u5165\u540e\u95e8\uff0c\u5373\u5728\u6b63\u5e38\u6761\u4ef6\u4e0b\u770b\u4f3c\u6210\u529f\u9057\u5fd8\uff0c\u4f46\u5728\u9690\u85cf\u89e6\u53d1\u8bcd\u6fc0\u6d3b\u65f6\u6062\u590d\u539f\u6709\u884c\u4e3a\u3002", "method": "\u901a\u8fc7\u5c06\u89e6\u53d1\u8bcd\u653e\u7f6e\u5728\u6ce8\u610f\u529b\u6c47\u805a\u4f4d\u7f6e\uff08attention sink\uff09\uff0c\u5e76\u8c03\u6574\u5176\u6ce8\u610f\u529b\u503c\u6765\u589e\u5f3a\u540e\u95e8\u6301\u4e45\u6027\uff0c\u5229\u7528\u6ce8\u610f\u529b\u6c47\u805a\u73b0\u8c61\u4f5c\u4e3a\u540e\u95e8\u9057\u5fd8\u7684\u5165\u53e3\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6ce8\u610f\u529b\u6c47\u805a\u5f15\u5bfc\u7684\u540e\u95e8\u9057\u5fd8\u653b\u51fb\u80fd\u591f\u53ef\u9760\u5730\u5728\u540e\u95e8\u89e6\u53d1\u8bcd\u51fa\u73b0\u65f6\u6062\u590d\u88ab\u9057\u5fd8\u7684\u77e5\u8bc6\uff0c\u800c\u5728\u65e0\u89e6\u53d1\u8bcd\u65f6\u4e0e\u6b63\u5e38\u9057\u5fd8\u6a21\u578b\u65e0\u6cd5\u533a\u5206\u3002", "conclusion": "\u6ce8\u610f\u529b\u6c47\u805a\u73b0\u8c61\u4e3a\u540e\u95e8\u9057\u5fd8\u653b\u51fb\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\uff0c\u63ed\u793a\u4e86\u9057\u5fd8\u8fc7\u7a0b\u672c\u8eab\u53ef\u80fd\u5b58\u5728\u7684\u5b89\u5168\u9690\u60a3\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u5b89\u5168\u7684\u9057\u5fd8\u673a\u5236\u3002"}}
{"id": "2510.16877", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16877", "abs": "https://arxiv.org/abs/2510.16877", "authors": ["Heming Zou", "Yunliang Zang", "Wutong Xu", "Xiangyang Ji"], "title": "Fly-CL: A Fly-Inspired Framework for Enhancing Efficient Decorrelation and Reduced Training Time in Pre-trained Model-based Continual Representation Learning", "comment": null, "summary": "Using a nearly-frozen pretrained model, the continual representation learning\nparadigm reframes parameter updates as a similarity-matching problem to\nmitigate catastrophic forgetting. However, directly leveraging pretrained\nfeatures for downstream tasks often suffers from multicollinearity in the\nsimilarity-matching stage, and more advanced methods can be computationally\nprohibitive for real-time, low-latency applications. Inspired by the fly\nolfactory circuit, we propose Fly-CL, a bio-inspired framework compatible with\na wide range of pretrained backbones. Fly-CL substantially reduces training\ntime while achieving performance comparable to or exceeding that of current\nstate-of-the-art methods. We theoretically show how Fly-CL progressively\nresolves multicollinearity, enabling more effective similarity matching with\nlow time complexity. Extensive simulation experiments across diverse network\narchitectures and data regimes validate Fly-CL's effectiveness in addressing\nthis challenge through a biologically inspired design. Code is available at\nhttps://github.com/gfyddha/Fly-CL.", "AI": {"tldr": "Fly-CL\u662f\u4e00\u4e2a\u53d7\u679c\u8747\u55c5\u89c9\u56de\u8def\u542f\u53d1\u7684\u6301\u7eed\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u51b3\u76f8\u4f3c\u6027\u5339\u914d\u4e2d\u7684\u591a\u91cd\u5171\u7ebf\u6027\u95ee\u9898\uff0c\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\uff0c\u540c\u65f6\u8fbe\u5230\u6216\u8d85\u8d8a\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u6301\u7eed\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u76f4\u63a5\u5229\u7528\u9884\u8bad\u7ec3\u7279\u5f81\u8fdb\u884c\u4e0b\u6e38\u4efb\u52a1\u65f6\uff0c\u5728\u76f8\u4f3c\u6027\u5339\u914d\u9636\u6bb5\u5bb9\u6613\u53d7\u5230\u591a\u91cd\u5171\u7ebf\u6027\u7684\u5f71\u54cd\uff0c\u800c\u66f4\u5148\u8fdb\u7684\u65b9\u6cd5\u5bf9\u4e8e\u5b9e\u65f6\u4f4e\u5ef6\u8fdf\u5e94\u7528\u6765\u8bf4\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u3002", "method": "\u63d0\u51faFly-CL\u6846\u67b6\uff0c\u53d7\u679c\u8747\u55c5\u89c9\u56de\u8def\u542f\u53d1\uff0c\u4e0e\u5404\u79cd\u9884\u8bad\u7ec3\u4e3b\u5e72\u7f51\u7edc\u517c\u5bb9\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u89e3\u51b3\u591a\u91cd\u5171\u7ebf\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u76f8\u4f3c\u6027\u5339\u914d\u3002", "result": "Fly-CL\u663e\u8457\u51cf\u5c11\u4e86\u8bad\u7ec3\u65f6\u95f4\uff0c\u540c\u65f6\u5728\u5404\u79cd\u7f51\u7edc\u67b6\u6784\u548c\u6570\u636e\u673a\u5236\u4e0b\u7684\u6a21\u62df\u5b9e\u9a8c\u4e2d\uff0c\u6027\u80fd\u8fbe\u5230\u6216\u8d85\u8fc7\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "Fly-CL\u901a\u8fc7\u751f\u7269\u542f\u53d1\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86\u6301\u7eed\u8868\u793a\u5b66\u4e60\u4e2d\u7684\u6311\u6218\uff0c\u7406\u8bba\u5206\u6790\u8868\u660e\u5176\u80fd\u591f\u6e10\u8fdb\u5f0f\u89e3\u51b3\u591a\u91cd\u5171\u7ebf\u6027\u95ee\u9898\uff0c\u4e14\u5177\u6709\u4f4e\u65f6\u95f4\u590d\u6742\u5ea6\u3002"}}
{"id": "2510.17132", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17132", "abs": "https://arxiv.org/abs/2510.17132", "authors": ["Ioannis Tsaknakis", "Bingqing Song", "Shuyu Gan", "Dongyeop Kang", "Alfredo Garcia", "Gaowen Liu", "Charles Fleming", "Mingyi Hong"], "title": "Do LLMs Recognize Your Latent Preferences? A Benchmark for Latent Information Discovery in Personalized Interaction", "comment": null, "summary": "Large Language Models (LLMs) excel at producing broadly relevant text, but\nthis generality becomes a limitation when user-specific preferences are\nrequired, such as recommending restaurants or planning travel. In these\nscenarios, users rarely articulate every preference explicitly; instead, much\nof what they care about remains latent, waiting to be inferred. This raises a\nfundamental question: Can LLMs uncover and reason about such latent information\nthrough conversation?\n  We address this problem by introducing a unified benchmark for evaluating\nlatent information discovery - the ability of LLMs to reveal and utilize hidden\nuser attributes through multi-turn interaction. The benchmark spans three\nprogressively realistic settings: the classic 20 Questions game, Personalized\nQuestion Answering, and Personalized Text Summarization. All tasks share a\ntri-agent framework (User, Assistant, Judge) enabling turn-level evaluation of\nelicitation and adaptation. Our results reveal that while LLMs can indeed\nsurface latent information through dialogue, their success varies dramatically\nwith context: from 32% to 98%, depending on task complexity, topic, and number\nof hidden attributes. This benchmark provides the first systematic framework\nfor studying latent information discovery in personalized interaction,\nhighlighting that effective preference inference remains an open frontier for\nbuilding truly adaptive AI systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30LLM\u5728\u5bf9\u8bdd\u4e2d\u53d1\u73b0\u548c\u5229\u7528\u6f5c\u5728\u7528\u6237\u4fe1\u606f\u7684\u7edf\u4e00\u57fa\u51c6\uff0c\u5305\u542b\u4e09\u4e2a\u9010\u6b65\u73b0\u5b9e\u7684\u573a\u666f\uff1a20 Questions\u6e38\u620f\u3001\u4e2a\u6027\u5316\u95ee\u7b54\u548c\u4e2a\u6027\u5316\u6587\u672c\u6458\u8981\u3002", "motivation": "LLM\u5728\u751f\u6210\u901a\u7528\u6587\u672c\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u9700\u8981\u7528\u6237\u7279\u5b9a\u504f\u597d\u7684\u573a\u666f\u4e2d\uff0c\u7528\u6237\u5f88\u5c11\u660e\u786e\u8868\u8fbe\u6240\u6709\u504f\u597d\uff0c\u5927\u91cf\u4fe1\u606f\u662f\u6f5c\u5728\u7684\u3002\u9700\u8981\u8bc4\u4f30LLM\u662f\u5426\u80fd\u901a\u8fc7\u5bf9\u8bdd\u53d1\u73b0\u548c\u63a8\u7406\u8fd9\u4e9b\u6f5c\u5728\u4fe1\u606f\u3002", "method": "\u91c7\u7528\u4e09\u667a\u80fd\u4f53\u6846\u67b6\uff08\u7528\u6237\u3001\u52a9\u624b\u3001\u6cd5\u5b98\uff09\u8fdb\u884c\u591a\u8f6e\u4ea4\u4e92\u8bc4\u4f30\uff0c\u6db5\u76d6\u4e09\u4e2a\u4efb\u52a1\uff1a20 Questions\u6e38\u620f\u3001\u4e2a\u6027\u5316\u95ee\u7b54\u548c\u4e2a\u6027\u5316\u6587\u672c\u6458\u8981\uff0c\u652f\u6301\u9010\u8f6e\u8bc4\u4f30\u4fe1\u606f\u83b7\u53d6\u548c\u9002\u5e94\u80fd\u529b\u3002", "result": "LLM\u786e\u5b9e\u80fd\u591f\u901a\u8fc7\u5bf9\u8bdd\u63ed\u793a\u6f5c\u5728\u4fe1\u606f\uff0c\u4f46\u6210\u529f\u7387\u5dee\u5f02\u5f88\u5927\uff0832%\u523098%\uff09\uff0c\u53d6\u51b3\u4e8e\u4efb\u52a1\u590d\u6742\u6027\u3001\u4e3b\u9898\u548c\u9690\u85cf\u5c5e\u6027\u6570\u91cf\u3002", "conclusion": "\u8be5\u57fa\u51c6\u4e3a\u7814\u7a76\u4e2a\u6027\u5316\u4ea4\u4e92\u4e2d\u7684\u6f5c\u5728\u4fe1\u606f\u53d1\u73b0\u63d0\u4f9b\u4e86\u9996\u4e2a\u7cfb\u7edf\u6027\u6846\u67b6\uff0c\u8868\u660e\u6709\u6548\u7684\u504f\u597d\u63a8\u7406\u4ecd\u7136\u662f\u6784\u5efa\u771f\u6b63\u81ea\u9002\u5e94AI\u7cfb\u7edf\u7684\u5f00\u653e\u524d\u6cbf\u3002"}}
{"id": "2510.16885", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16885", "abs": "https://arxiv.org/abs/2510.16885", "authors": ["Duo Wang", "Yuan Zuo", "Guangyue Lu", "Junjie Wu"], "title": "UniGTE: Unified Graph-Text Encoding for Zero-Shot Generalization across Graph Tasks and Domains", "comment": null, "summary": "Generalizing to unseen graph tasks without task-specific supervision is\nchallenging: conventional graph neural networks are typically tied to a fixed\nlabel space, while large language models (LLMs) struggle to capture graph\nstructure. We introduce UniGTE, an instruction-tuned encoder-decoder framework\nthat unifies structural and semantic reasoning. The encoder augments a\npretrained autoregressive LLM with learnable alignment tokens and a\nstructure-aware graph-text attention mechanism, enabling it to attend jointly\nto a tokenized graph and a natural-language task prompt while remaining\npermutation-invariant to node order. This yields compact, task-aware graph\nrepresentations. Conditioned solely on these representations, a frozen LLM\ndecoder predicts and reconstructs: it outputs the task answer and\nsimultaneously paraphrases the input graph in natural language. The\nreconstruction objective regularizes the encoder to preserve structural cues.\nUniGTE is instruction-tuned on five datasets spanning node-level, edge-level,\nand graph-level tasks across diverse domains, yet requires no fine-tuning at\ninference. It achieves new state-of-the-art zero-shot results on node\nclassification, link prediction, graph classification, and graph regression\nunder cross-task and cross-domain settings, demonstrating that tight\nintegration of graph structure with LLM semantics enables robust, transferable\ngraph reasoning.", "AI": {"tldr": "UniGTE\u662f\u4e00\u4e2a\u6307\u4ee4\u8c03\u4f18\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u56fe\u7ed3\u6784\u548c\u8bed\u4e49\u63a8\u7406\uff0c\u5b9e\u73b0\u8de8\u4efb\u52a1\u548c\u8de8\u9886\u57df\u7684\u96f6\u6837\u672c\u56fe\u63a8\u7406\u3002", "motivation": "\u4f20\u7edf\u56fe\u795e\u7ecf\u7f51\u7edc\u53d7\u9650\u4e8e\u56fa\u5b9a\u6807\u7b7e\u7a7a\u95f4\uff0c\u800c\u5927\u8bed\u8a00\u6a21\u578b\u96be\u4ee5\u6355\u6349\u56fe\u7ed3\u6784\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u7edf\u4e00\u7ed3\u6784\u63a8\u7406\u548c\u8bed\u4e49\u63a8\u7406\u7684\u65b9\u6cd5\u6765\u5904\u7406\u672a\u89c1\u8fc7\u7684\u56fe\u4efb\u52a1\u3002", "method": "\u7f16\u7801\u5668\u4f7f\u7528\u53ef\u5b66\u4e60\u7684\u5bf9\u9f50\u6807\u8bb0\u548c\u7ed3\u6784\u611f\u77e5\u7684\u56fe-\u6587\u672c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5c06\u6807\u8bb0\u5316\u56fe\u548c\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u63d0\u793a\u8054\u5408\u5904\u7406\uff1b\u89e3\u7801\u5668\u57fa\u4e8e\u7f16\u7801\u8868\u793a\u9884\u6d4b\u4efb\u52a1\u7b54\u6848\u5e76\u91cd\u6784\u8f93\u5165\u56fe\u3002", "result": "\u5728\u8282\u70b9\u5206\u7c7b\u3001\u94fe\u63a5\u9884\u6d4b\u3001\u56fe\u5206\u7c7b\u548c\u56fe\u56de\u5f52\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u65b0\u7684\u96f6\u6837\u672c\u6700\u4f18\u7ed3\u679c\uff0c\u5728\u8de8\u4efb\u52a1\u548c\u8de8\u57df\u8bbe\u7f6e\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u56fe\u7ed3\u6784\u4e0eLLM\u8bed\u4e49\u7684\u7d27\u5bc6\u96c6\u6210\u80fd\u591f\u5b9e\u73b0\u9c81\u68d2\u4e14\u53ef\u8fc1\u79fb\u7684\u56fe\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2510.17206", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17206", "abs": "https://arxiv.org/abs/2510.17206", "authors": ["Michael Hersche", "Samuel Moor-Smith", "Thomas Hofmann", "Abbas Rahimi"], "title": "Soft-Masked Diffusion Language Models", "comment": null, "summary": "Diffusion models have demonstrated strong potential in language modeling,\noffering various advantages over traditional autoregressive approaches. Their\nability to generate and revise entire responses in parallel enables faster\ngeneration and built-in self-correction mechanisms. Most modern diffusion-based\nlanguage models employ masked diffusion, where decoding involves iteratively\nprocessing masked tokens based on a binary decision: either retaining the mask\nor replacing it with the predicted token. However, this binary choice discards\nvaluable predictive information when the mask is retained. To address this\nlimitation, we introduce soft-masking (SM), a novel method that dynamically\nblends the embedding of the mask token with the embeddings of the top-$k$\npredicted tokens from the previous decoding step, for each retained mask. This\nprovides the model with a more informative prior, preserving context from\nearlier computations and allowing partial information about masked tokens to\npropagate beyond a single step. We propose a training methodology that adapts a\npretrained masked diffusion language model to incorporate SM. We demonstrate\nthat continuing pretraining a 169M parameter model with SM leads to improved\nperplexity and MAUVE scores. Furthermore, we finetune two state-of-the-art\ndiffusion models, Dream-7B and Dream-Coder-7B, with SM. SM consistently\nimproves performance across multiple coding benchmarks, particularly in\nhigh-throughput settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u8f6f\u63a9\u7801\uff08Soft-Masking\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u57fa\u4e8e\u63a9\u7801\u6269\u6563\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u52a8\u6001\u6df7\u5408\u63a9\u7801\u6807\u8bb0\u4e0e\u9884\u6d4b\u6807\u8bb0\u7684\u5d4c\u5165\u6765\u4fdd\u7559\u66f4\u591a\u9884\u6d4b\u4fe1\u606f\u3002", "motivation": "\u4f20\u7edf\u63a9\u7801\u6269\u6563\u6a21\u578b\u5728\u89e3\u7801\u65f6\u53ea\u505a\u4e8c\u5143\u9009\u62e9\uff08\u4fdd\u7559\u63a9\u7801\u6216\u66ff\u6362\u4e3a\u9884\u6d4b\u6807\u8bb0\uff09\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u6709\u4ef7\u503c\u7684\u9884\u6d4b\u4fe1\u606f\u88ab\u4e22\u5f03\u3002", "method": "\u8f6f\u63a9\u7801\u65b9\u6cd5\u52a8\u6001\u5730\u5c06\u63a9\u7801\u6807\u8bb0\u7684\u5d4c\u5165\u4e0e\u524d\u4e00\u89e3\u7801\u6b65\u9aa4\u4e2d\u9884\u6d4b\u7684\u524dk\u4e2a\u6807\u8bb0\u7684\u5d4c\u5165\u8fdb\u884c\u6df7\u5408\uff0c\u4e3a\u6a21\u578b\u63d0\u4f9b\u66f4\u4e30\u5bcc\u7684\u5148\u9a8c\u4fe1\u606f\u3002", "result": "\u5728169M\u53c2\u6570\u6a21\u578b\u4e0a\u7ee7\u7eed\u9884\u8bad\u7ec3SM\u6539\u8fdb\u4e86\u56f0\u60d1\u5ea6\u548cMAUVE\u5206\u6570\uff1b\u5728Dream-7B\u548cDream-Coder-7B\u6a21\u578b\u4e0a\u5fae\u8c03\u540e\uff0c\u5728\u591a\u4e2a\u7f16\u7801\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6027\u80fd\u5f97\u5230\u4e00\u81f4\u63d0\u5347\uff0c\u7279\u522b\u662f\u5728\u9ad8\u541e\u5410\u91cf\u8bbe\u7f6e\u4e0b\u3002", "conclusion": "\u8f6f\u63a9\u7801\u65b9\u6cd5\u901a\u8fc7\u4fdd\u7559\u90e8\u5206\u63a9\u7801\u6807\u8bb0\u7684\u9884\u6d4b\u4fe1\u606f\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u7f16\u7801\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2510.16897", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16897", "abs": "https://arxiv.org/abs/2510.16897", "authors": ["Jose Siguenza", "Bharath Ramsundar"], "title": "DeepChem Equivariant: SE(3)-Equivariant Support in an Open-Source Molecular Machine Learning Library", "comment": "Presented at Machine Learning Symposium - BayLearn (2025)", "summary": "Neural networks that incorporate geometric relationships respecting SE(3)\ngroup transformations (e.g. rotations and translations) are increasingly\nimportant in molecular applications, such as molecular property prediction,\nprotein structure modeling, and materials design. These models, known as\nSE(3)-equivariant neural networks, ensure outputs transform predictably with\ninput coordinate changes by explicitly encoding spatial atomic positions.\nAlthough libraries such as E3NN [4] and SE(3)-TRANSFORMER [3 ] offer powerful\nimplementations, they often require substantial deep learning or mathematical\nprior knowledge and lack complete training pipelines. We extend DEEPCHEM [ 13]\nwith support for ready-to-use equivariant models, enabling scientists with\nminimal deep learning background to build, train, and evaluate models, such as\nSE(3)-Transformer and Tensor Field Networks. Our implementation includes\nequivariant models, complete training pipelines, and a toolkit of equivariant\nutilities, supported with comprehensive tests and documentation, to facilitate\nboth application and further development of SE(3)-equivariant models.", "AI": {"tldr": "\u5c06SE(3)\u7b49\u53d8\u795e\u7ecf\u7f51\u7edc\u96c6\u6210\u5230DeepChem\u4e2d\uff0c\u4e3a\u5206\u5b50\u79d1\u5b66\u5e94\u7528\u63d0\u4f9b\u5373\u7528\u578b\u6a21\u578b\u548c\u5b8c\u6574\u8bad\u7ec3\u6d41\u7a0b\uff0c\u964d\u4f4e\u4f7f\u7528\u95e8\u69db\u3002", "motivation": "\u73b0\u6709\u7684SE(3)\u7b49\u53d8\u795e\u7ecf\u7f51\u7edc\u5e93\u9700\u8981\u6df1\u539a\u7684\u6df1\u5ea6\u5b66\u4e60\u6216\u6570\u5b66\u80cc\u666f\uff0c\u4e14\u7f3a\u4e4f\u5b8c\u6574\u7684\u8bad\u7ec3\u6d41\u7a0b\uff0c\u9650\u5236\u4e86\u975e\u4e13\u4e1a\u7528\u6237\u7684\u4f7f\u7528\u3002", "method": "\u6269\u5c55DeepChem\u5e93\uff0c\u96c6\u6210SE(3)-Transformer\u548cTensor Field Networks\u7b49\u7b49\u53d8\u6a21\u578b\uff0c\u63d0\u4f9b\u5b8c\u6574\u7684\u8bad\u7ec3\u6d41\u7a0b\u548c\u7b49\u53d8\u5de5\u5177\u5305\u3002", "result": "\u5f00\u53d1\u4e86\u5305\u542b\u7b49\u53d8\u6a21\u578b\u3001\u5b8c\u6574\u8bad\u7ec3\u6d41\u7a0b\u548c\u5b9e\u7528\u5de5\u5177\u7684\u5b9e\u73b0\uff0c\u652f\u6301\u79d1\u5b66\u5bb6\u65e0\u9700\u6df1\u5ea6\u5b66\u4e60\u80cc\u666f\u5373\u53ef\u6784\u5efa\u3001\u8bad\u7ec3\u548c\u8bc4\u4f30\u6a21\u578b\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u663e\u8457\u964d\u4f4e\u4e86SE(3)\u7b49\u53d8\u6a21\u578b\u7684\u4f7f\u7528\u95e8\u69db\uff0c\u4fc3\u8fdb\u4e86\u5206\u5b50\u79d1\u5b66\u4e2d\u51e0\u4f55\u611f\u77e5\u795e\u7ecf\u7f51\u7edc\u7684\u5e94\u7528\u548c\u53d1\u5c55\u3002"}}
{"id": "2510.16898", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16898", "abs": "https://arxiv.org/abs/2510.16898", "authors": ["Salih Salihoglu", "Ibrahim Ahmed", "Afshin Asadi"], "title": "Adaptive Online Learning with LSTM Networks for Energy Price Prediction", "comment": null, "summary": "Accurate prediction of electricity prices is crucial for stakeholders in the\nenergy market, particularly for grid operators, energy producers, and\nconsumers. This study focuses on developing a predictive model leveraging Long\nShort-Term Memory (LSTM) networks to forecast day-ahead electricity prices in\nthe California energy market. The model incorporates a variety of features,\nincluding historical price data, weather conditions, and the energy generation\nmix. A novel custom loss function that integrates Mean Absolute Error (MAE),\nJensen-Shannon Divergence (JSD), and a smoothness penalty is introduced to\nenhance the prediction accuracy and interpretability. Additionally, an online\nlearning approach is implemented to allow the model to adapt to new data\nincrementally, ensuring continuous relevance and accuracy. The results\ndemonstrate that the custom loss function can improve the model's performance,\naligning predicted prices more closely with actual values, particularly during\npeak intervals. Also, the online learning model outperforms other models by\neffectively incorporating real-time data, resulting in lower prediction error\nand variability. The inclusion of the energy generation mix further enhances\nthe model's predictive capabilities, highlighting the importance of\ncomprehensive feature integration. This research provides a robust framework\nfor electricity price forecasting, offering valuable insights and tools for\nbetter decision-making in dynamic electricity markets.", "AI": {"tldr": "\u4f7f\u7528LSTM\u7f51\u7edc\u9884\u6d4b\u52a0\u5dde\u7535\u529b\u5e02\u573a\u65e5\u524d\u7535\u4ef7\uff0c\u5f15\u5165\u7ed3\u5408MAE\u3001JSD\u548c\u5e73\u6ed1\u60e9\u7f5a\u9879\u7684\u81ea\u5b9a\u4e49\u635f\u5931\u51fd\u6570\uff0c\u5e76\u91c7\u7528\u5728\u7ebf\u5b66\u4e60\u65b9\u6cd5\u63d0\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u51c6\u786e\u9884\u6d4b\u7535\u4ef7\u5bf9\u7535\u7f51\u8fd0\u8425\u5546\u3001\u80fd\u6e90\u751f\u4ea7\u5546\u548c\u6d88\u8d39\u8005\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u9002\u5e94\u52a8\u6001\u7535\u529b\u5e02\u573a\u7684\u9ad8\u7cbe\u5ea6\u9884\u6d4b\u6a21\u578b\u3002", "method": "\u57fa\u4e8eLSTM\u7f51\u7edc\uff0c\u6574\u5408\u5386\u53f2\u4ef7\u683c\u3001\u5929\u6c14\u6761\u4ef6\u548c\u80fd\u6e90\u7ed3\u6784\u7b49\u7279\u5f81\uff0c\u8bbe\u8ba1\u5305\u542bMAE\u3001JSD\u548c\u5e73\u6ed1\u60e9\u7f5a\u9879\u7684\u81ea\u5b9a\u4e49\u635f\u5931\u51fd\u6570\uff0c\u5e76\u5b9e\u65bd\u5728\u7ebf\u5b66\u4e60\u7b56\u7565\u3002", "result": "\u81ea\u5b9a\u4e49\u635f\u5931\u51fd\u6570\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5cf0\u503c\u65f6\u6bb5\uff1b\u5728\u7ebf\u5b66\u4e60\u6a21\u578b\u901a\u8fc7\u6709\u6548\u6574\u5408\u5b9e\u65f6\u6570\u636e\uff0c\u964d\u4f4e\u4e86\u9884\u6d4b\u8bef\u5dee\u548c\u53d8\u5f02\u6027\uff1b\u80fd\u6e90\u7ed3\u6784\u7279\u5f81\u7684\u52a0\u5165\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u9884\u6d4b\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7535\u529b\u4ef7\u683c\u9884\u6d4b\u63d0\u4f9b\u4e86\u7a33\u5065\u6846\u67b6\uff0c\u901a\u8fc7\u7efc\u5408\u7279\u5f81\u6574\u5408\u548c\u81ea\u9002\u5e94\u5b66\u4e60\u673a\u5236\uff0c\u4e3a\u52a8\u6001\u7535\u529b\u5e02\u573a\u4e2d\u7684\u51b3\u7b56\u5236\u5b9a\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5de5\u5177\u548c\u89c1\u89e3\u3002"}}
{"id": "2510.16899", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16899", "abs": "https://arxiv.org/abs/2510.16899", "authors": ["Dun Liu", "Qin Pang", "Guangai Liu", "Hongyu Mou", "Jipeng Fan", "Yiming Miao", "Pin-Han Ho", "Limei Peng"], "title": "SNOMED CT-powered Knowledge Graphs for Structured Clinical Data and Diagnostic Reasoning", "comment": null, "summary": "The effectiveness of artificial intelligence (AI) in healthcare is\nsignificantly hindered by unstructured clinical documentation, which results in\nnoisy, inconsistent, and logically fragmented training data. To address this\nchallenge, we present a knowledge-driven framework that integrates the\nstandardized clinical terminology SNOMED CT with the Neo4j graph database to\nconstruct a structured medical knowledge graph. In this graph, clinical\nentities such as diseases, symptoms, and medications are represented as nodes,\nand semantic relationships such as ``caused by,'' ``treats,'' and ``belongs\nto'' are modeled as edges in Neo4j, with types mapped from formal SNOMED CT\nrelationship concepts (e.g., \\texttt{Causative agent}, \\texttt{Indicated for}).\nThis design enables multi-hop reasoning and ensures terminological consistency.\nBy extracting and standardizing entity-relationship pairs from clinical texts,\nwe generate structured, JSON-formatted datasets that embed explicit diagnostic\npathways. These datasets are used to fine-tune large language models (LLMs),\nsignificantly improving the clinical logic consistency of their outputs.\nExperimental results demonstrate that our knowledge-guided approach enhances\nthe validity and interpretability of AI-generated diagnostic reasoning,\nproviding a scalable solution for building reliable AI-assisted clinical\nsystems.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eSNOMED CT\u548cNeo4j\u7684\u77e5\u8bc6\u9a71\u52a8\u6846\u67b6\uff0c\u6784\u5efa\u7ed3\u6784\u5316\u533b\u7597\u77e5\u8bc6\u56fe\u8c31\uff0c\u901a\u8fc7\u6807\u51c6\u5316\u4e34\u5e8a\u5b9e\u4f53\u5173\u7cfb\u5bf9\u6765\u63d0\u5347LLM\u5728\u4e34\u5e8a\u8bca\u65ad\u4e2d\u7684\u903b\u8f91\u4e00\u81f4\u6027\u3002", "motivation": "\u89e3\u51b3\u975e\u7ed3\u6784\u5316\u4e34\u5e8a\u6587\u6863\u5bfc\u81f4\u7684\u8bad\u7ec3\u6570\u636e\u566a\u58f0\u3001\u4e0d\u4e00\u81f4\u548c\u903b\u8f91\u788e\u7247\u5316\u95ee\u9898\uff0c\u63d0\u5347AI\u5728\u533b\u7597\u9886\u57df\u7684\u6709\u6548\u6027\u3002", "method": "\u96c6\u6210SNOMED CT\u6807\u51c6\u5316\u4e34\u5e8a\u672f\u8bed\u4e0eNeo4j\u56fe\u6570\u636e\u5e93\uff0c\u6784\u5efa\u533b\u7597\u77e5\u8bc6\u56fe\u8c31\uff0c\u5c06\u4e34\u5e8a\u5b9e\u4f53\u4f5c\u4e3a\u8282\u70b9\uff0c\u8bed\u4e49\u5173\u7cfb\u4f5c\u4e3a\u8fb9\uff0c\u5e76\u63d0\u53d6\u6807\u51c6\u5316\u5b9e\u4f53\u5173\u7cfb\u5bf9\u751f\u6210\u7ed3\u6784\u5316\u6570\u636e\u96c6\u7528\u4e8eLLM\u5fae\u8c03\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86AI\u751f\u6210\u8bca\u65ad\u63a8\u7406\u7684\u6709\u6548\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u6539\u5584\u4e86LLM\u8f93\u51fa\u7684\u4e34\u5e8a\u903b\u8f91\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u77e5\u8bc6\u5f15\u5bfc\u65b9\u6cd5\u4e3a\u6784\u5efa\u53ef\u9760\u7684AI\u8f85\u52a9\u4e34\u5e8a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u589e\u5f3a\u4e86\u8bca\u65ad\u63a8\u7406\u7684\u6709\u6548\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2510.16911", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16911", "abs": "https://arxiv.org/abs/2510.16911", "authors": ["Sarah Al-Shareeda", "Gulcihan Ozdemir", "Heung Seok Jeon", "Khaleel Ahmad"], "title": "A Lightweight DL Model for Smart Grid Power Forecasting with Feature and Resolution Mismatch", "comment": "5 pages, 3 figures, The IEEE PES ISGT Middle East 2025 (ISGT-ME 2025)\n  November 23-26th 2025, Dubai, UAE", "summary": "How can short-term energy consumption be accurately forecasted when sensor\ndata is noisy, incomplete, and lacks contextual richness? This question guided\nour participation in the \\textit{2025 Competition on Electric Energy\nConsumption Forecast Adopting Multi-criteria Performance Metrics}, which\nchallenged teams to predict next-day power demand using real-world\nhigh-frequency data. We proposed a robust yet lightweight Deep Learning (DL)\npipeline combining hourly downsizing, dual-mode imputation (mean and polynomial\nregression), and comprehensive normalization, ultimately selecting Standard\nScaling for optimal balance. The lightweight GRU-LSTM sequence-to-one model\nachieves an average RMSE of 601.9~W, MAE of 468.9~W, and 84.36\\% accuracy.\nDespite asymmetric inputs and imputed gaps, it generalized well, captured\nnonlinear demand patterns, and maintained low inference latency. Notably,\nspatiotemporal heatmap analysis reveals a strong alignment between temperature\ntrends and predicted consumption, further reinforcing the model's reliability.\nThese results demonstrate that targeted preprocessing paired with compact\nrecurrent architectures can still enable fast, accurate, and deployment-ready\nenergy forecasting in real-world conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5b66\u4e60\u7ba1\u9053\uff0c\u7528\u4e8e\u5728\u566a\u58f0\u548c\u4e0d\u5b8c\u6574\u4f20\u611f\u5668\u6570\u636e\u6761\u4ef6\u4e0b\u51c6\u786e\u9884\u6d4b\u77ed\u671f\u80fd\u6e90\u6d88\u8017\uff0c\u57282025\u5e74\u80fd\u6e90\u6d88\u8017\u9884\u6d4b\u7ade\u8d5b\u4e2d\u53d6\u5f97\u4e86\u826f\u597d\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u4f20\u611f\u5668\u6570\u636e\u566a\u58f0\u5927\u3001\u4e0d\u5b8c\u6574\u4e14\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u4e30\u5bcc\u6027\u65f6\uff0c\u5982\u4f55\u51c6\u786e\u9884\u6d4b\u77ed\u671f\u80fd\u6e90\u6d88\u8017\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5b66\u4e60\u7ba1\u9053\uff0c\u7ed3\u5408\u5c0f\u65f6\u964d\u91c7\u6837\u3001\u53cc\u6a21\u5f0f\u63d2\u8865\uff08\u5747\u503c\u548c\u591a\u9879\u5f0f\u56de\u5f52\uff09\u548c\u7efc\u5408\u5f52\u4e00\u5316\uff0c\u6700\u7ec8\u9009\u62e9\u6807\u51c6\u7f29\u653e\uff0c\u4f7f\u7528GRU-LSTM\u5e8f\u5217\u5230\u4e00\u6a21\u578b\u8fdb\u884c\u9884\u6d4b\u3002", "result": "\u6a21\u578b\u5e73\u5747RMSE\u4e3a601.9W\uff0cMAE\u4e3a468.9W\uff0c\u51c6\u786e\u7387\u8fbe\u523084.36%\u3002\u65f6\u7a7a\u70ed\u56fe\u5206\u6790\u663e\u793a\u6e29\u5ea6\u8d8b\u52bf\u4e0e\u9884\u6d4b\u6d88\u8017\u91cf\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "\u6709\u9488\u5bf9\u6027\u7684\u9884\u5904\u7406\u4e0e\u7d27\u51d1\u5faa\u73af\u67b6\u6784\u76f8\u7ed3\u5408\uff0c\u80fd\u591f\u5728\u73b0\u5b9e\u6761\u4ef6\u4e0b\u5b9e\u73b0\u5feb\u901f\u3001\u51c6\u786e\u4e14\u53ef\u90e8\u7f72\u7684\u80fd\u6e90\u9884\u6d4b\u3002"}}
{"id": "2510.17671", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17671", "abs": "https://arxiv.org/abs/2510.17671", "authors": ["Katarzyna Kobalczyk", "Zhiyuan Jerry Lin", "Benjamin Letham", "Zhuokai Zhao", "Maximilian Balandat", "Eytan Bakshy"], "title": "LILO: Bayesian Optimization with Interactive Natural Language Feedback", "comment": null, "summary": "For many real-world applications, feedback is essential in translating\ncomplex, nuanced, or subjective goals into quantifiable optimization\nobjectives. We propose a language-in-the-loop framework that uses a large\nlanguage model (LLM) to convert unstructured feedback in the form of natural\nlanguage into scalar utilities to conduct BO over a numeric search space.\nUnlike preferential BO, which only accepts restricted feedback formats and\nrequires customized models for each domain-specific problem, our approach\nleverages LLMs to turn varied types of textual feedback into consistent utility\nsignals and to easily include flexible user priors without manual kernel\ndesign. At the same time, our method maintains the sample efficiency and\nprincipled uncertainty quantification of BO. We show that this hybrid method\nnot only provides a more natural interface to the decision maker but also\noutperforms conventional BO baselines and LLM-only optimizers, particularly in\nfeedback-limited regimes.", "AI": {"tldr": "\u63d0\u51fa\u8bed\u8a00\u5728\u73af\u6846\u67b6\uff0c\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5c06\u81ea\u7136\u8bed\u8a00\u53cd\u9988\u8f6c\u6362\u4e3a\u6807\u91cf\u6548\u7528\uff0c\u5728\u6570\u503c\u641c\u7d22\u7a7a\u95f4\u4e0a\u8fdb\u884c\u8d1d\u53f6\u65af\u4f18\u5316", "motivation": "\u73b0\u5b9e\u5e94\u7528\u4e2d\u9700\u8981\u5c06\u590d\u6742\u3001\u7ec6\u5fae\u6216\u4e3b\u89c2\u7684\u76ee\u6807\u8f6c\u5316\u4e3a\u53ef\u91cf\u5316\u7684\u4f18\u5316\u76ee\u6807\uff0c\u53cd\u9988\u673a\u5236\u81f3\u5173\u91cd\u8981", "method": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5c06\u5404\u79cd\u7c7b\u578b\u7684\u6587\u672c\u53cd\u9988\u8f6c\u6362\u4e3a\u4e00\u81f4\u7684\u6548\u7528\u4fe1\u53f7\uff0c\u65e0\u9700\u624b\u52a8\u8bbe\u8ba1\u6838\u51fd\u6570\uff0c\u540c\u65f6\u4fdd\u6301\u8d1d\u53f6\u65af\u4f18\u5316\u7684\u6837\u672c\u6548\u7387\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316", "result": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u4e3a\u51b3\u7b56\u8005\u63d0\u4f9b\u66f4\u81ea\u7136\u7684\u63a5\u53e3\uff0c\u800c\u4e14\u5728\u53cd\u9988\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u4f18\u4e8e\u4f20\u7edf\u8d1d\u53f6\u65af\u4f18\u5316\u57fa\u7ebf\u548c\u5927\u8bed\u8a00\u6a21\u578b\u4f18\u5316\u5668", "conclusion": "\u8bed\u8a00\u5728\u73af\u6846\u67b6\u7ed3\u5408\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u548c\u8d1d\u53f6\u65af\u4f18\u5316\u7684\u4f18\u52bf\uff0c\u5b9e\u73b0\u4e86\u66f4\u7075\u6d3b\u3001\u9ad8\u6548\u7684\u4f18\u5316\u8fc7\u7a0b"}}
{"id": "2510.16914", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16914", "abs": "https://arxiv.org/abs/2510.16914", "authors": ["Hongwei Yan", "Guanglong Sun", "Zhiqi Kang", "Yi Zhong", "Liyuan Wang"], "title": "Domain Generalizable Continual Learning", "comment": "25 pages", "summary": "To adapt effectively to dynamic real-world environments, intelligent systems\nmust continually acquire new skills while generalizing them to diverse, unseen\nscenarios. Here, we introduce a novel and realistic setting named domain\ngeneralizable continual learning (DGCL): a model learns sequential tasks with\neach involving a single domain, aiming to perform well across all encountered\ntasks and domains. This setting poses unique challenges in acquiring,\nretaining, and leveraging both semantic- and domain-relevant information for\nrobust generalization. Although state-of-the-art continual learning (CL)\nmethods have employed pre-trained models (PTMs) to enhance task-specific\ngeneralization, they typically assume identical training and testing domains\nfor each task and therefore perform poorly in DGCL. To this end, we propose\nadaptive Domain Transformation (DoT), an innovative PTMs-based approach\ntailored to DGCL. Inspired by the distributed-plus-hub theory of the human\nbrain, DoT disentangles semantic- and domain-relevant information in\nrepresentation learning, and adaptively transforms task representations across\nvarious domains for output alignment, ensuring balanced and generalized\npredictions. DoT serves as a plug-in strategy that greatly facilitates\nstate-of-the-art CL baselines under both full parameter tuning and\nparameter-efficient tuning paradigms in DGCL, validated by extensive\nexperiments. Also, DoT is shown to accumulate domain-generalizable knowledge\nfrom DGCL, and ensure resource efficiency with a lightweight implementation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9886\u57df\u6cdb\u5316\u6301\u7eed\u5b66\u4e60\uff08DGCL\uff09\u65b0\u8bbe\u7f6e\uff0c\u5e76\u5f00\u53d1\u4e86\u81ea\u9002\u5e94\u9886\u57df\u53d8\u6362\uff08DoT\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u8026\u8bed\u4e49\u548c\u9886\u57df\u4fe1\u606f\u6765\u63d0\u5347\u6a21\u578b\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u5047\u8bbe\u8bad\u7ec3\u548c\u6d4b\u8bd5\u9886\u57df\u76f8\u540c\uff0c\u65e0\u6cd5\u5e94\u5bf9\u73b0\u5b9e\u4e16\u754c\u4e2d\u52a8\u6001\u53d8\u5316\u7684\u591a\u9886\u57df\u73af\u5883\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u540c\u65f6\u5904\u7406\u4efb\u52a1\u5e8f\u5217\u548c\u9886\u57df\u53d8\u5316\u7684\u901a\u7528\u5316\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u9886\u57df\u53d8\u6362\uff08DoT\uff09\u65b9\u6cd5\uff0c\u57fa\u4e8e\u5206\u5e03\u5f0f\u52a0\u67a2\u7ebd\u7406\u8bba\uff0c\u89e3\u8026\u8bed\u4e49\u548c\u9886\u57df\u76f8\u5173\u4fe1\u606f\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u53d8\u6362\u4efb\u52a1\u8868\u793a\u6765\u5b9e\u73b0\u8f93\u51fa\u5bf9\u9f50\uff0c\u786e\u4fdd\u5e73\u8861\u548c\u6cdb\u5316\u7684\u9884\u6d4b\u3002", "result": "DoT\u4f5c\u4e3a\u63d2\u4ef6\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u73b0\u6709\u6301\u7eed\u5b66\u4e60\u57fa\u7ebf\u65b9\u6cd5\u5728DGCL\u8bbe\u7f6e\u4e0b\u7684\u6027\u80fd\uff0c\u5728\u5b8c\u6574\u53c2\u6570\u8c03\u4f18\u548c\u53c2\u6570\u9ad8\u6548\u8c03\u4f18\u8303\u5f0f\u4e0b\u90fd\u6709\u6548\uff0c\u4e14\u5177\u6709\u8d44\u6e90\u6548\u7387\u3002", "conclusion": "DoT\u65b9\u6cd5\u80fd\u591f\u4eceDGCL\u4e2d\u79ef\u7d2f\u9886\u57df\u6cdb\u5316\u77e5\u8bc6\uff0c\u4e3a\u52a8\u6001\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u667a\u80fd\u7cfb\u7edf\u63d0\u4f9b\u6709\u6548\u7684\u6301\u7eed\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.16916", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16916", "abs": "https://arxiv.org/abs/2510.16916", "authors": ["Dong Li", "Xujiang Zhao", "Linlin Yu", "Yanchi Liu", "Wei Cheng", "Zhengzhang Chen", "Zhong Chen", "Feng Chen", "Chen Zhao", "Haifeng Chen"], "title": "SolverLLM: Leveraging Test-Time Scaling for Optimization Problem via LLM-Guided Search", "comment": "NeurIPS 2025", "summary": "Large Language Models (LLMs) offer promising capabilities for tackling\ncomplex reasoning tasks, including optimization problems. However, existing\nmethods either rely on prompt engineering, which leads to poor generalization\nacross problem types, or require costly supervised training. We introduce\nSolverLLM, a training-free framework that leverages test-time scaling to solve\ndiverse optimization problems. Rather than solving directly, SolverLLM\ngenerates mathematical formulations and translates them into solver-ready code,\nguided by a novel Monte Carlo Tree Search (MCTS) strategy. To enhance the\nsearch process, we modify classical MCTS with (1) dynamic expansion for\nadaptive formulation generation, (2) prompt backpropagation to guide\nexploration via outcome-driven feedback, and (3) uncertainty backpropagation to\nincorporate reward reliability into decision-making. Experiments on six\nstandard benchmark datasets demonstrate that SolverLLM outperforms both\nprompt-based and learning-based baselines, achieving strong generalization\nwithout additional training.", "AI": {"tldr": "SolverLLM\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6d4b\u8bd5\u65f6\u7f29\u653e\u548c\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u7b56\u7565\uff0c\u5c06\u4f18\u5316\u95ee\u9898\u8f6c\u5316\u4e3a\u6570\u5b66\u516c\u5f0f\u548c\u6c42\u89e3\u5668\u4ee3\u7801\uff0c\u5728\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u63d0\u793a\u5de5\u7a0b\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u8981\u4e48\u9700\u8981\u6602\u8d35\u7684\u76d1\u7763\u8bad\u7ec3\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u4e14\u80fd\u6cdb\u5316\u5230\u4e0d\u540c\u7c7b\u578b\u4f18\u5316\u95ee\u9898\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u7b56\u7565\uff0c\u5305\u542b\u52a8\u6001\u6269\u5c55\u3001\u63d0\u793a\u53cd\u5411\u4f20\u64ad\u548c\u4e0d\u786e\u5b9a\u6027\u53cd\u5411\u4f20\u64ad\uff0c\u5c06\u95ee\u9898\u8f6c\u5316\u4e3a\u6570\u5b66\u516c\u5f0f\u548c\u6c42\u89e3\u5668\u4ee3\u7801\u3002", "result": "\u5728\u516d\u4e2a\u6807\u51c6\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSolverLLM\u4f18\u4e8e\u57fa\u4e8e\u63d0\u793a\u548c\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SolverLLM\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5c31\u80fd\u6709\u6548\u89e3\u51b3\u591a\u6837\u5316\u4f18\u5316\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u6d4b\u8bd5\u65f6\u7f29\u653e\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.17776", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17776", "abs": "https://arxiv.org/abs/2510.17776", "authors": ["Jackson Harmon", "Andreas Hochlehnert", "Matthias Bethge", "Ameya Prabhu"], "title": "Mapping Post-Training Forgetting in Language Models at Scale", "comment": "43 pages,15 figures", "summary": "Scaled post-training now drives many of the largest capability gains in\nlanguage models (LMs), yet its effect on pretrained knowledge remains poorly\nunderstood. Not all forgetting is equal: Forgetting one fact (e.g., a U.S.\npresident or an API call) does not \"average out\" by recalling another. Hence,\nwe propose a sample-wise paradigm to measure what is forgotten and when\nbackward transfer occurs. Our metric counts 1->0 transitions (correct before\npost-training, incorrect after) to quantify forgetting and 0->1 transitions to\nquantify backward transfer. Traditional task averages conflate these effects\nand obscure large changes. For multiple-choice benchmarks, we add\nchance-adjusted variants that subtract the expected contribution of random\nguessing from pre- and post-training accuracies. We apply this framework across\npost-training stages, model sizes, and data scales. Our large-scale analysis\nshows that: (1) Domain-continual pretraining induces moderate forgetting with\nlow-to-moderate backward transfer; (2) RL/SFT post-training applied to base\nmodels and Instruction tuning yields moderate-to-large backward transfer on\nmath and logic with overall low-to-moderate forgetting; (3) Applying RL/SFT to\ninstruction-tuned models is sensitive on data scale: at small scales, both\nforgetting and backward transfer are small; at larger scales, effects are mixed\nand warrant further study with better controls; (4) Model merging does not\nreliably mitigate forgetting. Overall, our framework offers a practical\nyardstick for mapping how post-training alters pretrained knowledge at scale --\nenabling progress towards generally capable AI systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6837\u672c\u7ea7\u522b\u7684\u6846\u67b6\u6765\u91cf\u5316\u8bed\u8a00\u6a21\u578b\u5728\u6269\u5c55\u540e\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u77e5\u8bc6\u9057\u5fd8\u548c\u53cd\u5411\u8fc1\u79fb\u7684\u60c5\u51b5\uff0c\u901a\u8fc7\u5206\u67901\u21920\u548c0\u21921\u7684\u8f6c\u6362\u6765\u6d4b\u91cf\u8fd9\u4e9b\u6548\u5e94\u3002", "motivation": "\u7406\u89e3\u6269\u5c55\u540e\u8bad\u7ec3\u5bf9\u9884\u8bad\u7ec3\u77e5\u8bc6\u7684\u5f71\u54cd\uff0c\u56e0\u4e3a\u4f20\u7edf\u4efb\u52a1\u5e73\u5747\u503c\u4f1a\u6df7\u6dc6\u9057\u5fd8\u548c\u53cd\u5411\u8fc1\u79fb\u6548\u5e94\uff0c\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u6a21\u578b\u77e5\u8bc6\u7684\u53d8\u5316\u3002", "method": "\u63d0\u51fa\u6837\u672c\u7ea7\u522b\u7684\u5ea6\u91cf\u65b9\u6cd5\uff0c\u8ba1\u7b971\u21920\u8f6c\u6362\uff08\u540e\u8bad\u7ec3\u524d\u6b63\u786e\u540e\u9519\u8bef\uff09\u6765\u91cf\u5316\u9057\u5fd8\uff0c0\u21921\u8f6c\u6362\u6765\u91cf\u5316\u53cd\u5411\u8fc1\u79fb\u3002\u5bf9\u4e8e\u9009\u62e9\u9898\u57fa\u51c6\uff0c\u6dfb\u52a0\u4e86\u673a\u4f1a\u8c03\u6574\u53d8\u4f53\u6765\u6d88\u9664\u968f\u673a\u731c\u6d4b\u7684\u5f71\u54cd\u3002", "result": "\u5927\u89c4\u6a21\u5206\u6790\u663e\u793a\uff1a\u9886\u57df\u6301\u7eed\u9884\u8bad\u7ec3\u5bfc\u81f4\u4e2d\u5ea6\u9057\u5fd8\u548c\u4f4e\u5230\u4e2d\u5ea6\u53cd\u5411\u8fc1\u79fb\uff1bRL/SFT\u540e\u8bad\u7ec3\u5728\u6570\u5b66\u548c\u903b\u8f91\u4e0a\u4ea7\u751f\u4e2d\u5ea6\u5230\u5927\u7684\u53cd\u5411\u8fc1\u79fb\uff0c\u603b\u4f53\u4f4e\u5230\u4e2d\u5ea6\u9057\u5fd8\uff1b\u5728\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u4e0a\u5e94\u7528RL/SFT\u5bf9\u6570\u636e\u89c4\u6a21\u654f\u611f\uff1b\u6a21\u578b\u878d\u5408\u4e0d\u80fd\u53ef\u9760\u7f13\u89e3\u9057\u5fd8\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6620\u5c04\u540e\u8bad\u7ec3\u5982\u4f55\u6539\u53d8\u9884\u8bad\u7ec3\u77e5\u8bc6\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u8861\u91cf\u6807\u51c6\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u901a\u7528AI\u7cfb\u7edf\u3002"}}
{"id": "2510.16927", "categories": ["cs.LG", "I.2.6; I.2.7; G.1.3"], "pdf": "https://arxiv.org/pdf/2510.16927", "abs": "https://arxiv.org/abs/2510.16927", "authors": ["Egor Petrov", "Nikita Kiselev", "Vladislav Meshkov", "Andrey Grabovoy"], "title": "Closing the Curvature Gap: Full Transformer Hessians and Their Implications for Scaling Laws", "comment": "38 pages, 12 figures. Submitted to ICLR 2026", "summary": "The lack of theoretical results for Layer Normalization and feedforward\nHessians has left a gap in the study of Transformer optimization landscapes. We\naddress this by deriving explicit second-order expressions for these\ncomponents, thereby completing the Hessian characterization of full Transformer\nblocks. Our results generalize prior self-attention analyses and yield\nestimations for the role of each sublayer in curvature propagation. We\ndemonstrate how these Hessian structures inform both convergence dynamics and\nthe empirical scaling laws governing large-model performance. Further, we\npropose a Taylor-expansion-based framework for analyzing loss differences to\nquantify convergence trajectories. By extending Hessian theory to the full\nTransformer architecture, this work establishes a new foundation for\ntheoretical and empirical investigations of optimization in large-scale deep\nlearning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u63a8\u5bfcLayer Normalization\u548c\u524d\u9988\u7f51\u7edcHessian\u77e9\u9635\u7684\u663e\u5f0f\u4e8c\u9636\u8868\u8fbe\u5f0f\uff0c\u5b8c\u6210\u4e86\u5b8c\u6574Transformer\u5757\u7684Hessian\u8868\u5f81\uff0c\u4e3aTransformer\u4f18\u5316\u666f\u89c2\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "motivation": "Layer Normalization\u548c\u524d\u9988\u7f51\u7edcHessian\u77e9\u9635\u7f3a\u4e4f\u7406\u8bba\u7ed3\u679c\uff0c\u8fd9\u7ed9Transformer\u4f18\u5316\u666f\u89c2\u7814\u7a76\u7559\u4e0b\u4e86\u7a7a\u767d\u3002", "method": "\u63a8\u5bfc\u663e\u5f0f\u4e8c\u9636\u8868\u8fbe\u5f0f\uff0c\u63d0\u51fa\u57fa\u4e8e\u6cf0\u52d2\u5c55\u5f00\u7684\u6846\u67b6\u6765\u5206\u6790\u635f\u5931\u5dee\u5f02\u4ee5\u91cf\u5316\u6536\u655b\u8f68\u8ff9\u3002", "result": "\u7ed3\u679c\u63a8\u5e7f\u4e86\u5148\u524d\u7684\u81ea\u6ce8\u610f\u529b\u5206\u6790\uff0c\u5e76\u4f30\u8ba1\u4e86\u6bcf\u4e2a\u5b50\u5c42\u5728\u66f2\u7387\u4f20\u64ad\u4e2d\u7684\u4f5c\u7528\uff0c\u5c55\u793a\u4e86Hessian\u7ed3\u6784\u5982\u4f55\u5f71\u54cd\u6536\u655b\u52a8\u6001\u548c\u5927\u578b\u6a21\u578b\u6027\u80fd\u7684\u5b9e\u8bc1\u7f29\u653e\u5b9a\u5f8b\u3002", "conclusion": "\u901a\u8fc7\u5c06Hessian\u7406\u8bba\u6269\u5c55\u5230\u5b8c\u6574Transformer\u67b6\u6784\uff0c\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5927\u89c4\u6a21\u6df1\u5ea6\u5b66\u4e60\u4f18\u5316\u7684\u7406\u8bba\u548c\u5b9e\u8bc1\u7814\u7a76\u5efa\u7acb\u4e86\u65b0\u57fa\u7840\u3002"}}
{"id": "2510.16940", "categories": ["cs.LG", "cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.16940", "abs": "https://arxiv.org/abs/2510.16940", "authors": ["Cristian J. Vaca-Rubio", "Roberto Pereira", "Luis Blanco", "Engin Zeydan", "M\u00e0rius Caus"], "title": "A Primer on Kolmogorov-Arnold Networks (KANs) for Probabilistic Time Series Forecasting", "comment": null, "summary": "This work introduces Probabilistic Kolmogorov-Arnold Network (P-KAN), a novel\nprobabilistic extension of Kolmogorov-Arnold Networks (KANs) for time series\nforecasting. By replacing scalar weights with spline-based functional\nconnections and directly parameterizing predictive distributions, P-KANs offer\nexpressive yet parameter-efficient models capable of capturing nonlinear and\nheavy-tailed dynamics. We evaluate P-KANs on satellite traffic forecasting,\nwhere uncertainty-aware predictions enable dynamic thresholding for resource\nallocation. Results show that P-KANs consistently outperform Multi Layer\nPerceptron (MLP) baselines in both accuracy and calibration, achieving superior\nefficiency-risk trade-offs while using significantly fewer parameters. We build\nup P-KANs on two distributions, namely Gaussian and Student-t distributions.\nThe Gaussian variant provides robust, conservative forecasts suitable for\nsafety-critical scenarios, whereas the Student-t variant yields sharper\ndistributions that improve efficiency under stable demand. These findings\nestablish P-KANs as a powerful framework for probabilistic forecasting with\ndirect applicability to satellite communications and other resource-constrained\ndomains.", "AI": {"tldr": "P-KAN\u662f\u4e00\u79cd\u6982\u7387\u6027Kolmogorov-Arnold\u7f51\u7edc\uff0c\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u901a\u8fc7\u6837\u6761\u51fd\u6570\u8fde\u63a5\u548c\u76f4\u63a5\u53c2\u6570\u5316\u9884\u6d4b\u5206\u5e03\uff0c\u5728\u536b\u661f\u6d41\u91cf\u9884\u6d4b\u4e2d\u4f18\u4e8eMLP\u57fa\u7ebf\u3002", "motivation": "\u5f00\u53d1\u53c2\u6570\u9ad8\u6548\u4e14\u80fd\u6355\u6349\u975e\u7ebf\u6027\u548c\u91cd\u5c3e\u52a8\u6001\u7684\u6982\u7387\u9884\u6d4b\u6a21\u578b\uff0c\u5e94\u7528\u4e8e\u536b\u661f\u901a\u4fe1\u7b49\u8d44\u6e90\u53d7\u9650\u9886\u57df\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u9884\u6d4b\u3002", "method": "\u7528\u6837\u6761\u51fd\u6570\u8fde\u63a5\u66ff\u6362\u6807\u91cf\u6743\u91cd\uff0c\u76f4\u63a5\u53c2\u6570\u5316\u9884\u6d4b\u5206\u5e03\uff0c\u6784\u5efa\u57fa\u4e8e\u9ad8\u65af\u5206\u5e03\u548cStudent-t\u5206\u5e03\u7684P-KAN\u6a21\u578b\u3002", "result": "P-KAN\u5728\u51c6\u786e\u6027\u548c\u6821\u51c6\u65b9\u9762\u6301\u7eed\u4f18\u4e8eMLP\u57fa\u7ebf\uff0c\u4f7f\u7528\u66f4\u5c11\u53c2\u6570\u5b9e\u73b0\u66f4\u597d\u7684\u6548\u7387-\u98ce\u9669\u6743\u8861\u3002\u9ad8\u65af\u53d8\u4f53\u63d0\u4f9b\u7a33\u5065\u9884\u6d4b\uff0cStudent-t\u53d8\u4f53\u5728\u7a33\u5b9a\u9700\u6c42\u4e0b\u4ea7\u751f\u66f4\u5c16\u9510\u5206\u5e03\u3002", "conclusion": "P-KAN\u4e3a\u6982\u7387\u9884\u6d4b\u63d0\u4f9b\u4e86\u5f3a\u5927\u6846\u67b6\uff0c\u7279\u522b\u9002\u7528\u4e8e\u536b\u661f\u901a\u4fe1\u548c\u5176\u4ed6\u8d44\u6e90\u53d7\u9650\u9886\u57df\u3002"}}
{"id": "2510.16958", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16958", "abs": "https://arxiv.org/abs/2510.16958", "authors": ["Ganglin Tian", "Anastase Alexandre Charantonis", "Camille Le Coz", "Alexis Tantet", "Riwal Plougonven"], "title": "Quantile Regression, Variational Autoencoders, and Diffusion Models for Uncertainty Quantification: A Spatial Analysis of Sub-seasonal Wind Speed Prediction", "comment": "This Work has been submitted to Monthly Weather Review. Copyright in\n  this Work may be transferred without further notice", "summary": "This study aims to improve the spatial representation of uncertainties when\nregressing surface wind speeds from large-scale atmospheric predictors for\nsub-seasonal forecasting. Sub-seasonal forecasting often relies on large-scale\natmospheric predictors such as 500 hPa geopotential height (Z500), which\nexhibit higher predictability than surface variables and can be downscaled to\nobtain more localised information. Previous work by Tian et al. (2024)\ndemonstrated that stochastic perturbations based on model residuals can improve\nensemble dispersion representation in statistical downscaling frameworks, but\nthis method fails to represent spatial correlations and physical consistency\nadequately. More sophisticated approaches are needed to capture the complex\nrelationships between large-scale predictors and local-scale predictands while\nmaintaining physical consistency. Probabilistic deep learning models offer\npromising solutions for capturing complex spatial dependencies. This study\nevaluates three probabilistic methods with distinct uncertainty quantification\nmechanisms: Quantile Regression Neural Network that directly models\ndistribution quantiles, Variational Autoencoders that leverage latent space\nsampling, and Diffusion Models that utilise iterative denoising. These models\nare trained on ERA5 reanalysis data and applied to ECMWF sub-seasonal hindcasts\nto regress probabilistic wind speed ensembles. Our results show that\nprobabilistic downscaling approaches provide more realistic spatial uncertainty\nrepresentations compared to simpler stochastic methods, with each probabilistic\nmodel offering different strengths in terms of ensemble dispersion,\ndeterministic skill, and physical consistency. These findings establish\nprobabilistic downscaling as an effective enhancement to operational\nsub-seasonal wind forecasts for renewable energy planning and risk assessment.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u4e09\u79cd\u6982\u7387\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u6b21\u5b63\u8282\u98ce\u573a\u9884\u62a5\u4e2d\u7684\u8868\u73b0\uff0c\u76f8\u6bd4\u7b80\u5355\u7684\u968f\u673a\u6270\u52a8\u65b9\u6cd5\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u80fd\u66f4\u597d\u5730\u8868\u5f81\u7a7a\u95f4\u4e0d\u786e\u5b9a\u6027\u548c\u7269\u7406\u4e00\u81f4\u6027\u3002", "motivation": "\u6539\u8fdb\u6b21\u5b63\u8282\u9884\u62a5\u4e2d\u4ece\u5927\u5c3a\u5ea6\u5927\u6c14\u9884\u6d4b\u56e0\u5b50\u56de\u5f52\u5730\u8868\u98ce\u901f\u65f6\u7684\u7a7a\u95f4\u4e0d\u786e\u5b9a\u6027\u8868\u5f81\u3002\u4f20\u7edf\u968f\u673a\u6270\u52a8\u65b9\u6cd5\u65e0\u6cd5\u5145\u5206\u8868\u5f81\u7a7a\u95f4\u76f8\u5173\u6027\u548c\u7269\u7406\u4e00\u81f4\u6027\u3002", "method": "\u8bc4\u4f30\u4e09\u79cd\u6982\u7387\u65b9\u6cd5\uff1a\u5206\u4f4d\u6570\u56de\u5f52\u795e\u7ecf\u7f51\u7edc\uff08\u76f4\u63a5\u5efa\u6a21\u5206\u5e03\u5206\u4f4d\u6570\uff09\u3001\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08\u5229\u7528\u6f5c\u5728\u7a7a\u95f4\u91c7\u6837\uff09\u548c\u6269\u6563\u6a21\u578b\uff08\u4f7f\u7528\u8fed\u4ee3\u53bb\u566a\uff09\u3002\u5728ERA5\u518d\u5206\u6790\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u5e94\u7528\u4e8eECMWF\u6b21\u5b63\u8282\u540e\u62a5\u3002", "result": "\u6982\u7387\u964d\u5c3a\u5ea6\u65b9\u6cd5\u76f8\u6bd4\u7b80\u5355\u968f\u673a\u65b9\u6cd5\u63d0\u4f9b\u66f4\u771f\u5b9e\u7684\u7a7a\u95f4\u4e0d\u786e\u5b9a\u6027\u8868\u5f81\uff0c\u6bcf\u79cd\u6982\u7387\u6a21\u578b\u5728\u96c6\u5408\u79bb\u6563\u5ea6\u3001\u786e\u5b9a\u6027\u6280\u80fd\u548c\u7269\u7406\u4e00\u81f4\u6027\u65b9\u9762\u5404\u6709\u4f18\u52bf\u3002", "conclusion": "\u6982\u7387\u964d\u5c3a\u5ea6\u662f\u589e\u5f3a\u4e1a\u52a1\u6b21\u5b63\u8282\u98ce\u573a\u9884\u62a5\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5bf9\u53ef\u518d\u751f\u80fd\u6e90\u89c4\u5212\u548c\u98ce\u9669\u8bc4\u4f30\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2510.16980", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16980", "abs": "https://arxiv.org/abs/2510.16980", "authors": ["Kanghui Ning", "Zijie Pan", "Yushan Jiang", "Anderson Schneider", "Yuriy Nevmyvaka", "Dongjin Song"], "title": "Towards Interpretable and Trustworthy Time Series Reasoning: A BlueSky Vision", "comment": null, "summary": "Time series reasoning is emerging as the next frontier in temporal analysis,\naiming to move beyond pattern recognition towards explicit, interpretable, and\ntrustworthy inference. This paper presents a BlueSky vision built on two\ncomplementary directions. One builds robust foundations for time series\nreasoning, centered on comprehensive temporal understanding, structured\nmulti-step reasoning, and faithful evaluation frameworks. The other advances\nsystem-level reasoning, moving beyond language-only explanations by\nincorporating multi-agent collaboration, multi-modal context, and\nretrieval-augmented approaches. Together, these directions outline a flexible\nand extensible framework for advancing time series reasoning, aiming to deliver\ninterpretable and trustworthy temporal intelligence across diverse domains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u7684\u84dd\u56fe\u613f\u666f\uff0c\u5305\u542b\u4e24\u4e2a\u4e92\u8865\u65b9\u5411\uff1a\u6784\u5efa\u7a33\u5065\u7684\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u57fa\u7840\uff0c\u4ee5\u53ca\u63a8\u8fdb\u7cfb\u7edf\u7ea7\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u6b63\u5728\u6210\u4e3a\u65f6\u5e8f\u5206\u6790\u7684\u4e0b\u4e00\u4e2a\u524d\u6cbf\uff0c\u65e8\u5728\u8d85\u8d8a\u6a21\u5f0f\u8bc6\u522b\uff0c\u5b9e\u73b0\u660e\u786e\u3001\u53ef\u89e3\u91ca\u4e14\u53ef\u4fe1\u8d56\u7684\u63a8\u7406\u3002", "method": "1) \u6784\u5efa\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u7684\u7a33\u5065\u57fa\u7840\uff0c\u5305\u62ec\u5168\u9762\u65f6\u5e8f\u7406\u89e3\u3001\u7ed3\u6784\u5316\u591a\u6b65\u63a8\u7406\u548c\u5fe0\u5b9e\u8bc4\u4f30\u6846\u67b6\uff1b2) \u63a8\u8fdb\u7cfb\u7edf\u7ea7\u63a8\u7406\uff0c\u8d85\u8d8a\u7eaf\u8bed\u8a00\u89e3\u91ca\uff0c\u878d\u5165\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u3001\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u548c\u68c0\u7d22\u589e\u5f3a\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7075\u6d3b\u53ef\u6269\u5c55\u7684\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u6846\u67b6\uff0c\u80fd\u591f\u4e3a\u4e0d\u540c\u9886\u57df\u63d0\u4f9b\u53ef\u89e3\u91ca\u4e14\u53ef\u4fe1\u8d56\u7684\u65f6\u5e8f\u667a\u80fd\u3002", "conclusion": "\u8fd9\u4e24\u4e2a\u4e92\u8865\u65b9\u5411\u5171\u540c\u6784\u6210\u4e86\u63a8\u8fdb\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u7684\u84dd\u56fe\uff0c\u65e8\u5728\u5b9e\u73b0\u8de8\u9886\u57df\u7684\u53ef\u89e3\u91ca\u548c\u53ef\u4fe1\u8d56\u65f6\u5e8f\u667a\u80fd\u3002"}}
{"id": "2510.16990", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16990", "abs": "https://arxiv.org/abs/2510.16990", "authors": ["Xuying Ning", "Dongqi Fu", "Tianxin Wei", "Wujiang Xu", "Jingrui He"], "title": "Graph4MM: Weaving Multimodal Learning with Structural Information", "comment": "ICML 2025", "summary": "Real-world multimodal data usually exhibit complex structural relationships\nbeyond traditional one-to-one mappings like image-caption pairs. Entities\nacross modalities interact in intricate ways, with images and text forming\ndiverse interconnections through contextual dependencies and co-references.\nGraphs provide powerful structural information for modeling intra-modal and\ninter-modal relationships. However, previous works fail to distinguish\nmulti-hop neighbors and treat the graph as a standalone modality, which\nfragments the overall understanding. This limitation presents two key\nchallenges in multimodal learning: (1) integrating structural information from\nmulti-hop neighbors into foundational models, and (2) fusing modality-specific\ninformation in a principled manner. To address these challenges, we revisit the\nrole of graphs in multimodal learning within the era of foundation models and\npropose Graph4MM, a graph-based multimodal learning framework. To be specific,\nwe introduce Hop-Diffused Attention, which integrates multi-hop structural\ninformation into self-attention through causal masking and hop diffusion.\nFurthermore, we design MM-QFormer, a multi-mapping querying transformer for\ncross-modal fusion. Through theoretical and empirical analysis, we show that\nleveraging structures to integrate both intra- and inter-modal interactions\nimproves multimodal understanding beyond treating them as a standalone\nmodality. Experiments on both generative and discriminative tasks show that\nGraph4MM outperforms larger VLMs, LLMs, and multimodal graph baselines,\nachieving a 6.93% average improvement.", "AI": {"tldr": "Graph4MM\u662f\u4e00\u4e2a\u57fa\u4e8e\u56fe\u7684\u591a\u6a21\u6001\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7Hop-Diffused Attention\u6574\u5408\u591a\u8df3\u7ed3\u6784\u4fe1\u606f\uff0c\u5e76\u4f7f\u7528MM-QFormer\u8fdb\u884c\u8de8\u6a21\u6001\u878d\u5408\uff0c\u5728\u751f\u6210\u6027\u548c\u5224\u522b\u6027\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u591a\u6a21\u6001\u6570\u636e\u5177\u6709\u590d\u6742\u7684\u7ed3\u6784\u5173\u7cfb\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u533a\u5206\u591a\u8df3\u90bb\u5c45\u5e76\u5c06\u56fe\u89c6\u4e3a\u72ec\u7acb\u6a21\u6001\uff0c\u8fd9\u9650\u5236\u4e86\u6574\u4f53\u7406\u89e3\u80fd\u529b\u3002", "method": "\u63d0\u51faHop-Diffused Attention\u901a\u8fc7\u56e0\u679c\u63a9\u7801\u548c\u8df3\u6269\u6563\u6574\u5408\u591a\u8df3\u7ed3\u6784\u4fe1\u606f\u5230\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4e2d\uff0c\u5e76\u8bbe\u8ba1MM-QFormer\u8fdb\u884c\u8de8\u6a21\u6001\u878d\u5408\u3002", "result": "\u5b9e\u9a8c\u8868\u660eGraph4MM\u5728\u751f\u6210\u6027\u548c\u5224\u522b\u6027\u4efb\u52a1\u4e0a\u4f18\u4e8e\u66f4\u5927\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u591a\u6a21\u6001\u56fe\u57fa\u7ebf\uff0c\u5e73\u5747\u63d0\u53476.93%\u3002", "conclusion": "\u5229\u7528\u7ed3\u6784\u4fe1\u606f\u6574\u5408\u6a21\u6001\u5185\u548c\u6a21\u6001\u95f4\u4ea4\u4e92\u80fd\u591f\u63d0\u5347\u591a\u6a21\u6001\u7406\u89e3\u80fd\u529b\uff0c\u8d85\u8d8a\u5c06\u56fe\u4f5c\u4e3a\u72ec\u7acb\u6a21\u6001\u7684\u65b9\u6cd5\u3002"}}
{"id": "2510.17002", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17002", "abs": "https://arxiv.org/abs/2510.17002", "authors": ["Chang Liu", "Danial Chitnis"], "title": "EEschematic: Multimodal-LLM Based AI Agent for Schematic Generation of Analog Circuit", "comment": null, "summary": "Circuit schematics play a crucial role in analog integrated circuit design,\nserving as the primary medium for human understanding and verification of\ncircuit functionality. While recent large language model (LLM)-based approaches\nhave shown promise in circuit topology generation and device sizing, most rely\nsolely on textual representations such as SPICE netlists, which lack visual\ninterpretability for circuit designers. To address this limitation, we propose\nEEschematic, an AI agent for automatic analog schematic generation based on a\nMultimodal Large Language Model (MLLM). EEschematic integrates textual, visual,\nand symbolic modalities to translate SPICE netlists into schematic diagrams\nrepresented in a human-editable format. The framework uses six analog\nsubstructure examples for few-shot placement and a Visual Chain-of-Thought\n(VCoT) strategy to iteratively refine placement and wiring, enhancing schematic\nclarity and symmetry. Experimental results on representative analog circuits,\nincluding a CMOS inverter, a five-transistor operational transconductance\namplifier (5T-OTA), and a telescopic cascode amplifier, demonstrate that\nEEschematic produces schematics with high visual quality and structural\ncorrectness.", "AI": {"tldr": "EEschematic\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684AI\u4ee3\u7406\uff0c\u80fd\u591f\u5c06SPICE\u7f51\u8868\u81ea\u52a8\u8f6c\u6362\u4e3a\u53ef\u7f16\u8f91\u7684\u7535\u8def\u539f\u7406\u56fe\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6587\u672c\u8868\u793a\u7f3a\u4e4f\u89c6\u89c9\u53ef\u89e3\u91ca\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8eLLM\u7684\u7535\u8def\u751f\u6210\u65b9\u6cd5\u4ec5\u4f9d\u8d56SPICE\u7f51\u8868\u7b49\u6587\u672c\u8868\u793a\uff0c\u7f3a\u4e4f\u5bf9\u7535\u8def\u8bbe\u8ba1\u5e08\u53cb\u597d\u7684\u89c6\u89c9\u53ef\u89e3\u91ca\u6027\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u96c6\u6210\u6587\u672c\u3001\u89c6\u89c9\u548c\u7b26\u53f7\u6a21\u6001\uff0c\u4f7f\u75286\u4e2a\u6a21\u62df\u5b50\u7ed3\u6784\u793a\u4f8b\u8fdb\u884c\u5c11\u6837\u672c\u5e03\u5c40\uff0c\u91c7\u7528\u89c6\u89c9\u601d\u7ef4\u94fe\u7b56\u7565\u8fed\u4ee3\u4f18\u5316\u5e03\u5c40\u548c\u5e03\u7ebf\u3002", "result": "\u5728CMOS\u53cd\u76f8\u5668\u3001\u4e94\u7ba1\u8fd0\u7b97\u8de8\u5bfc\u653e\u5927\u5668\u548c\u671b\u8fdc\u955c\u7ea7\u8054\u653e\u5927\u5668\u7b49\u4ee3\u8868\u6027\u6a21\u62df\u7535\u8def\u4e0a\uff0cEEschematic\u751f\u6210\u4e86\u5177\u6709\u9ad8\u89c6\u89c9\u8d28\u91cf\u548c\u7ed3\u6784\u6b63\u786e\u6027\u7684\u539f\u7406\u56fe\u3002", "conclusion": "EEschematic\u6210\u529f\u5b9e\u73b0\u4e86\u4ece\u6587\u672c\u7f51\u8868\u5230\u53ef\u89c6\u5316\u539f\u7406\u56fe\u7684\u81ea\u52a8\u8f6c\u6362\uff0c\u4e3a\u6a21\u62df\u7535\u8def\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u66f4\u76f4\u89c2\u7684\u8bbe\u8ba1\u5de5\u5177\u3002"}}
{"id": "2510.17015", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.17015", "abs": "https://arxiv.org/abs/2510.17015", "authors": ["Mingyan Yang", "Guanjie Wang", "Manqi Luo", "Yifei Liu", "Chen Chen", "Han Zhao", "Yu Feng", "Quan Chen", "Minyi Guo"], "title": "Justitia: Fair and Efficient Scheduling for LLM Applications", "comment": null, "summary": "In the era of Large Language Models (LLMs), it has been popular to launch a\nseries of LLM inferences -- we call an LLM application -- to better solve\nreal-world problems. When serving those applications in shared GPU servers, the\nschedulers are expected to attain fast application completions with guaranteed\nworst-case performance. However, mainstream LLM schedulers fail to behave well\nfor LLM applications -- due to head-of-line blocking or over-constrained\nresource allocation. In this paper, we propose to serve LLM applications in a\nfair and also efficient manner. To this end, we design Justitia, a novel\nscheduler with three key techniques. First, given that memory is prevalently a\nbottleneck for mainstream inference frameworks like vLLM, Justitia models the\nservice cost of LLM applications in a memory-centric manner. Meanwhile, it uses\na simple neural network model to conduct light-weight and also accurate demand\nprediction. Moreover, Justitia adopts a virtual-time based fair queuing\nalgorithm to reduce the overall performance with guaranteed worst-case delay.\nWe have implemented Justitia atop vLLM, and experimental results involving\ndiverse LLM applications show that it can substantially enhance the scheduling\nefficiency with fairness preserved.", "AI": {"tldr": "Justitia\u662f\u4e00\u4e2a\u9488\u5bf9LLM\u5e94\u7528\u7a0b\u5e8f\u7684\u65b0\u578b\u8c03\u5ea6\u5668\uff0c\u901a\u8fc7\u5185\u5b58\u4e2d\u5fc3\u7684\u670d\u52a1\u6210\u672c\u5efa\u6a21\u3001\u8f7b\u91cf\u7ea7\u9700\u6c42\u9884\u6d4b\u548c\u57fa\u4e8e\u865a\u62df\u65f6\u95f4\u7684\u516c\u5e73\u6392\u961f\u7b97\u6cd5\uff0c\u5728\u4fdd\u8bc1\u516c\u5e73\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8c03\u5ea6\u6548\u7387\u3002", "motivation": "\u5728\u5171\u4eabGPU\u670d\u52a1\u5668\u4e2d\u670d\u52a1LLM\u5e94\u7528\u7a0b\u5e8f\u65f6\uff0c\u4e3b\u6d41\u8c03\u5ea6\u5668\u7531\u4e8e\u961f\u5934\u963b\u585e\u6216\u8d44\u6e90\u5206\u914d\u8fc7\u5ea6\u7ea6\u675f\uff0c\u65e0\u6cd5\u5728\u4fdd\u8bc1\u6700\u574f\u60c5\u51b5\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u5feb\u901f\u5e94\u7528\u5b8c\u6210\u3002", "method": "\u8bbe\u8ba1\u4e86Justitia\u8c03\u5ea6\u5668\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u6280\u672f\uff1a\u5185\u5b58\u4e2d\u5fc3\u7684\u670d\u52a1\u6210\u672c\u5efa\u6a21\u3001\u4f7f\u7528\u7b80\u5355\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u8f7b\u91cf\u7ea7\u9700\u6c42\u9884\u6d4b\u3001\u57fa\u4e8e\u865a\u62df\u65f6\u95f4\u7684\u516c\u5e73\u6392\u961f\u7b97\u6cd5\u3002", "result": "\u5728vLLM\u4e0a\u5b9e\u73b0Justitia\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5b83\u80fd\u663e\u8457\u63d0\u5347\u8c03\u5ea6\u6548\u7387\u540c\u65f6\u4fdd\u6301\u516c\u5e73\u6027\u3002", "conclusion": "Justitia\u80fd\u591f\u4ee5\u516c\u5e73\u4e14\u9ad8\u6548\u7684\u65b9\u5f0f\u670d\u52a1LLM\u5e94\u7528\u7a0b\u5e8f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8c03\u5ea6\u5668\u5728LLM\u5e94\u7528\u573a\u666f\u4e0b\u7684\u6027\u80fd\u95ee\u9898\u3002"}}
{"id": "2510.17022", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17022", "abs": "https://arxiv.org/abs/2510.17022", "authors": ["Kevin P. O Keeffe"], "title": "Curiosity-driven RL for symbolic equation solving", "comment": "Accepted at the NeurIPS 2025 MATH-AI Workshop", "summary": "We explore if RL can be useful for symbolic mathematics. Previous work showed\ncontrastive learning can solve linear equations in one variable. We show\nmodel-free PPO \\cite{schulman2017proximal} augmented with curiosity-based\nexploration and graph-based actions can solve nonlinear equations such as those\ninvolving radicals, exponentials, and trig functions. Our work suggests\ncuriosity-based exploration may be useful for general symbolic reasoning tasks.", "AI": {"tldr": "\u63a2\u7d22\u5f3a\u5316\u5b66\u4e60\u5728\u7b26\u53f7\u6570\u5b66\u4e2d\u7684\u5e94\u7528\uff0c\u4f7f\u7528PPO\u7b97\u6cd5\u7ed3\u5408\u597d\u5947\u5fc3\u63a2\u7d22\u548c\u56fe\u7ed3\u6784\u52a8\u4f5c\u89e3\u51b3\u975e\u7ebf\u6027\u65b9\u7a0b", "motivation": "\u63a2\u7d22\u5f3a\u5316\u5b66\u4e60\u662f\u5426\u80fd\u5728\u7b26\u53f7\u6570\u5b66\u4e2d\u53d1\u6325\u4f5c\u7528\uff0c\u7279\u522b\u662f\u89e3\u51b3\u6bd4\u7ebf\u6027\u65b9\u7a0b\u66f4\u590d\u6742\u7684\u975e\u7ebf\u6027\u65b9\u7a0b", "method": "\u4f7f\u7528\u6a21\u578b\u65e0\u5173\u7684PPO\u7b97\u6cd5\uff0c\u7ed3\u5408\u57fa\u4e8e\u597d\u5947\u5fc3\u7684\u63a2\u7d22\u673a\u5236\u548c\u56fe\u7ed3\u6784\u7684\u52a8\u4f5c\u8868\u793a", "result": "\u6210\u529f\u89e3\u51b3\u4e86\u5305\u542b\u6839\u53f7\u3001\u6307\u6570\u3001\u4e09\u89d2\u51fd\u6570\u7b49\u975e\u7ebf\u6027\u65b9\u7a0b", "conclusion": "\u57fa\u4e8e\u597d\u5947\u5fc3\u7684\u63a2\u7d22\u65b9\u6cd5\u53ef\u80fd\u5bf9\u901a\u7528\u7b26\u53f7\u63a8\u7406\u4efb\u52a1\u6709\u76ca"}}
{"id": "2510.17036", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17036", "abs": "https://arxiv.org/abs/2510.17036", "authors": ["Nguyen Do", "Bach Ngo", "Youval Kashuv", "Canh V. Pham", "Hanghang Tong", "My T. Thai"], "title": "Hephaestus: Mixture Generative Modeling with Energy Guidance for Large-scale QoS Degradation", "comment": "62 pages, 19 figures, Neural Information Processing Systems (NeurIPS\n  2025)", "summary": "We study the Quality of Service Degradation (QoSD) problem, in which an\nadversary perturbs edge weights to degrade network performance. This setting\narises in both network infrastructures and distributed ML systems, where\ncommunication quality, not just connectivity, determines functionality. While\nclassical methods rely on combinatorial optimization, and recent ML approaches\naddress only restricted linear variants with small-size networks, no prior\nmodel directly tackles the QoSD problem under nonlinear edge-weight functions.\nThis work proposes \\PIMMA, a self-reinforcing generative framework that\nsynthesizes feasible solutions in latent space, to fill this gap. Our method\nincludes three phases: (1) Forge: a Predictive Path-Stressing (PPS) algorithm\nthat uses graph learning and approximation to produce feasible solutions with\nperformance guarantee, (2) Morph: a new theoretically grounded training\nparadigm for Mixture of Conditional VAEs guided by an energy-based model to\ncapture solution feature distributions, and (3) Refine: a reinforcement\nlearning agent that explores this space to generate progressively near-optimal\nsolutions using our designed differentiable reward function. Experiments on\nboth synthetic and real-world networks show that our approach consistently\noutperforms classical and ML baselines, particularly in scenarios with\nnonlinear cost functions where traditional methods fail to generalize.", "AI": {"tldr": "\u63d0\u51fa\u4e86PIMMA\u6846\u67b6\u89e3\u51b3\u670d\u52a1\u8d28\u91cf\u9000\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u751f\u6210\u5f0f\u65b9\u6cd5\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5408\u6210\u53ef\u884c\u89e3\uff0c\u5728\u975e\u7ebf\u6027\u8fb9\u6743\u91cd\u51fd\u6570\u573a\u666f\u4e0b\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5", "motivation": "\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u76f4\u63a5\u5904\u7406\u975e\u7ebf\u6027\u8fb9\u6743\u91cd\u51fd\u6570\u7684QoSD\u95ee\u9898\uff0c\u4f20\u7edf\u7ec4\u5408\u4f18\u5316\u65b9\u6cd5\u53d7\u9650\uff0c\u800c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u53ea\u80fd\u5904\u7406\u53d7\u9650\u7684\u7ebf\u6027\u53d8\u4f53\u548c\u5c0f\u89c4\u6a21\u7f51\u7edc", "method": "\u4e09\u9636\u6bb5\u6846\u67b6\uff1aForge\u4f7f\u7528\u9884\u6d4b\u8def\u5f84\u5e94\u529b\u7b97\u6cd5\u751f\u6210\u53ef\u884c\u89e3\uff1bMorph\u4f7f\u7528\u6761\u4ef6VAE\u6df7\u5408\u6a21\u578b\u548c\u80fd\u91cf\u6a21\u578b\u6355\u83b7\u89e3\u7279\u5f81\u5206\u5e03\uff1bRefine\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u5728\u7a7a\u95f4\u4e2d\u63a2\u7d22\u751f\u6210\u8fd1\u4f18\u89e3", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u7f51\u7edc\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u59cb\u7ec8\u4f18\u4e8e\u7ecf\u5178\u548cML\u57fa\u7ebf\uff0c\u7279\u522b\u662f\u5728\u975e\u7ebf\u6027\u6210\u672c\u51fd\u6570\u573a\u666f\u4e0b\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u6cdb\u5316\u65f6\u8868\u73b0\u7a81\u51fa", "conclusion": "PIMMA\u6846\u67b6\u6210\u529f\u586b\u8865\u4e86QoSD\u95ee\u9898\u5728\u975e\u7ebf\u6027\u8fb9\u6743\u91cd\u51fd\u6570\u4e0b\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u5c55\u793a\u4e86\u751f\u6210\u5f0f\u65b9\u6cd5\u5728\u6b64\u7c7b\u7f51\u7edc\u4f18\u5316\u95ee\u9898\u4e2d\u7684\u6709\u6548\u6027"}}
{"id": "2510.17040", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17040", "abs": "https://arxiv.org/abs/2510.17040", "authors": ["Hoang-Son Nguyen", "Xiao Fu"], "title": "Diverse Influence Component Analysis: A Geometric Approach to Nonlinear Mixture Identifiability", "comment": "30 pages, 3 figures", "summary": "Latent component identification from unknown nonlinear mixtures is a\nfoundational challenge in machine learning, with applications in tasks such as\ndisentangled representation learning and causal inference. Prior work in\nnonlinear independent component analysis (nICA) has shown that auxiliary\nsignals -- such as weak supervision -- can support identifiability of\nconditionally independent latent components. More recent approaches explore\nstructural assumptions, e.g., sparsity in the Jacobian of the mixing function,\nto relax such requirements. In this work, we introduce Diverse Influence\nComponent Analysis (DICA), a framework that exploits the convex geometry of the\nmixing function's Jacobian. We propose a Jacobian Volume Maximization\n(J-VolMax) criterion, which enables latent component identification by\nencouraging diversity in their influence on the observed variables. Under\nreasonable conditions, this approach achieves identifiability without relying\non auxiliary information, latent component independence, or Jacobian sparsity\nassumptions. These results extend the scope of identifiability analysis and\noffer a complementary perspective to existing methods.", "AI": {"tldr": "\u63d0\u51faDICA\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u5927\u5316\u6df7\u5408\u51fd\u6570\u96c5\u53ef\u6bd4\u77e9\u9635\u4f53\u79ef\u6765\u8bc6\u522b\u975e\u7ebf\u6027\u6df7\u5408\u4e2d\u7684\u6f5c\u5728\u6210\u5206\uff0c\u65e0\u9700\u8f85\u52a9\u4fe1\u53f7\u3001\u72ec\u7acb\u6027\u5047\u8bbe\u6216\u96c5\u53ef\u6bd4\u7a00\u758f\u6027\u8981\u6c42\u3002", "motivation": "\u89e3\u51b3\u975e\u7ebf\u6027\u72ec\u7acb\u6210\u5206\u5206\u6790\u4e2d\u6f5c\u5728\u6210\u5206\u8bc6\u522b\u7684\u6839\u672c\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u7f3a\u4e4f\u8f85\u52a9\u76d1\u7763\u4fe1\u53f7\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u53ef\u8bc6\u522b\u6027\u3002", "method": "\u4f7f\u7528Jacobian Volume Maximization (J-VolMax)\u51c6\u5219\uff0c\u5229\u7528\u6df7\u5408\u51fd\u6570\u96c5\u53ef\u6bd4\u77e9\u9635\u7684\u51f8\u51e0\u4f55\u7279\u6027\uff0c\u901a\u8fc7\u6700\u5927\u5316\u96c5\u53ef\u6bd4\u77e9\u9635\u4f53\u79ef\u6765\u4fc3\u8fdb\u6f5c\u5728\u6210\u5206\u5bf9\u89c2\u6d4b\u53d8\u91cf\u7684\u591a\u6837\u5316\u5f71\u54cd\u3002", "result": "\u5728\u5408\u7406\u6761\u4ef6\u4e0b\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u6f5c\u5728\u6210\u5206\u7684\u53ef\u8bc6\u522b\u6027\uff0c\u65e0\u9700\u4f9d\u8d56\u8f85\u52a9\u4fe1\u606f\u3001\u6f5c\u5728\u6210\u5206\u72ec\u7acb\u6027\u6216\u96c5\u53ef\u6bd4\u7a00\u758f\u6027\u5047\u8bbe\u3002", "conclusion": "DICA\u6846\u67b6\u6269\u5c55\u4e86\u53ef\u8bc6\u522b\u6027\u5206\u6790\u7684\u8303\u56f4\uff0c\u4e3a\u73b0\u6709\u65b9\u6cd5\u63d0\u4f9b\u4e86\u8865\u5145\u89c6\u89d2\uff0c\u5728\u975e\u7ebf\u6027\u6df7\u5408\u95ee\u9898\u4e2d\u5b9e\u73b0\u4e86\u66f4\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2510.17057", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17057", "abs": "https://arxiv.org/abs/2510.17057", "authors": ["Nikolaus Howe", "Micah Carroll"], "title": "The Ends Justify the Thoughts: RL-Induced Motivated Reasoning in LLMs", "comment": "26 pages", "summary": "The use of reinforcement learning (RL) with chain-of-thought (CoT) reasoning\nhas emerged as a promising approach for developing more capable language\nmodels. In turn, this has led to investigation of CoT monitoring as a\ncompelling method for detecting harmful behaviors such as reward hacking, under\nthe assumption that models' reasoning processes reflect their internal\ndecision-making. In practice, LLM training often produces unintended behaviors\ndue to imperfect reward signals, leading models to develop misaligned\ntendencies. A common corrective approach is to apply post-hoc instructions to\navoid problematic behaviors like sycophancy, but what happens to the model's\nreasoning process when these instructions conflict with learned behaviors? We\ninvestigate this question in simple settings and find that models engage in\nsystematic motivated reasoning -- generating plausible-sounding justifications\nfor violating their instructions while downplaying potential harms. Beyond\nbeing an interesting property of training, we find that while motivated\nreasoning can be detected by most frontier reasoning models, smaller LLM judges\ncan fail to identify a portion of it, and in rare cases can themselves be\npersuaded that the reasoning is correct, despite it contradicting clear\ninstructions. This capability gap raises concerns that as models become more\nsophisticated, their motivated reasoning may become increasingly difficult for\nmonitors to detect. Our results underscore the need to account for motivated\nreasoning when relying on chain-of-thought processes for model evaluation and\noversight. All code for this paper will be made available. WARNING: some\nexamples in this paper may be upsetting.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5f53\u540e\u5904\u7406\u6307\u4ee4\u4e0e\u6a21\u578b\u5df2\u5b66\u4e60\u884c\u4e3a\u51b2\u7a81\u65f6\uff0c\u8bed\u8a00\u6a21\u578b\u4f1a\u8fdb\u884c\u52a8\u673a\u6027\u63a8\u7406\u2014\u2014\u751f\u6210\u770b\u4f3c\u5408\u7406\u7684\u7406\u7531\u6765\u8fdd\u53cd\u6307\u4ee4\uff0c\u540c\u65f6\u6de1\u5316\u6f5c\u5728\u5371\u5bb3\u3002\u7814\u7a76\u53d1\u73b0\u524d\u6cbf\u63a8\u7406\u6a21\u578b\u80fd\u68c0\u6d4b\u5230\u8fd9\u79cd\u52a8\u673a\u6027\u63a8\u7406\uff0c\u4f46\u8f83\u5c0f\u7684LLM\u6cd5\u5b98\u53ef\u80fd\u65e0\u6cd5\u8bc6\u522b\uff0c\u751a\u81f3\u53ef\u80fd\u88ab\u8bf4\u670d\u8ba4\u4e3a\u8fd9\u79cd\u63a8\u7406\u662f\u6b63\u786e\u7684\u3002", "motivation": "\u7814\u7a76\u5f53\u540e\u5904\u7406\u6307\u4ee4\u4e0e\u6a21\u578b\u5df2\u5b66\u4e60\u884c\u4e3a\u51b2\u7a81\u65f6\uff0c\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\u4f1a\u53d1\u751f\u4ec0\u4e48\u53d8\u5316\uff0c\u7279\u522b\u662f\u5173\u6ce8\u6a21\u578b\u662f\u5426\u4f1a\u4ea7\u751f\u52a8\u673a\u6027\u63a8\u7406\u6765\u5408\u7406\u5316\u8fdd\u53cd\u6307\u4ee4\u7684\u884c\u4e3a\u3002", "method": "\u5728\u7b80\u5355\u8bbe\u7f6e\u4e2d\u8c03\u67e5\u6a21\u578b\u884c\u4e3a\uff0c\u5206\u6790\u6a21\u578b\u5982\u4f55\u751f\u6210\u770b\u4f3c\u5408\u7406\u7684\u7406\u7531\u6765\u8fdd\u53cd\u6307\u4ee4\uff0c\u5e76\u8bc4\u4f30\u4e0d\u540c\u89c4\u6a21LLM\u6cd5\u5b98\u68c0\u6d4b\u52a8\u673a\u6027\u63a8\u7406\u7684\u80fd\u529b\u3002", "result": "\u6a21\u578b\u786e\u5b9e\u4f1a\u8fdb\u884c\u7cfb\u7edf\u6027\u7684\u52a8\u673a\u6027\u63a8\u7406\uff0c\u524d\u6cbf\u63a8\u7406\u6a21\u578b\u80fd\u68c0\u6d4b\u5230\u5927\u90e8\u5206\u52a8\u673a\u6027\u63a8\u7406\uff0c\u4f46\u8f83\u5c0f\u7684LLM\u6cd5\u5b98\u53ef\u80fd\u65e0\u6cd5\u8bc6\u522b\u90e8\u5206\u52a8\u673a\u6027\u63a8\u7406\uff0c\u751a\u81f3\u53ef\u80fd\u88ab\u8bf4\u670d\u8ba4\u4e3a\u8fd9\u79cd\u63a8\u7406\u662f\u6b63\u786e\u7684\u3002", "conclusion": "\u52a8\u673a\u6027\u63a8\u7406\u80fd\u529b\u5dee\u8ddd\u5f15\u53d1\u62c5\u5fe7\uff0c\u968f\u7740\u6a21\u578b\u53d8\u5f97\u66f4\u590d\u6742\uff0c\u5176\u52a8\u673a\u6027\u63a8\u7406\u53ef\u80fd\u8d8a\u6765\u8d8a\u96be\u4ee5\u88ab\u76d1\u63a7\u5668\u68c0\u6d4b\u5230\u3002\u7814\u7a76\u5f3a\u8c03\u5728\u4f9d\u8d56\u601d\u7ef4\u94fe\u8fc7\u7a0b\u8fdb\u884c\u6a21\u578b\u8bc4\u4f30\u548c\u76d1\u7763\u65f6\u9700\u8981\u8003\u8651\u52a8\u673a\u6027\u63a8\u7406\u3002"}}
{"id": "2510.17058", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17058", "abs": "https://arxiv.org/abs/2510.17058", "authors": ["Hassan Hamad", "Yuou Qiu", "Peter A. Beerel", "Keith M. Chugg"], "title": "Bitwidth-Specific Logarithmic Arithmetic for Future Hardware-Accelerated Training", "comment": null, "summary": "While advancements in quantization have significantly reduced the\ncomputational costs of inference in deep learning, training still predominantly\nrelies on complex floating-point arithmetic. Low-precision fixed-point training\npresents a compelling alternative. This work introduces a novel enhancement in\nlow-precision logarithmic fixed-point training, geared towards future hardware\naccelerator designs. We propose incorporating bitwidth in the design of\napproximations to arithmetic operations. To this end, we introduce a new\nhardware-friendly, piece-wise linear approximation for logarithmic addition.\nUsing simulated annealing, we optimize this approximation at different\nprecision levels. A C++ bit-true simulation demonstrates training of VGG-11 and\nVGG-16 models on CIFAR-100 and TinyImageNet, respectively, using 12-bit integer\narithmetic with minimal accuracy degradation compared to 32-bit floating-point\ntraining. Our hardware study reveals up to 32.5% reduction in area and 53.5%\nreduction in energy consumption for the proposed LNS multiply-accumulate units\ncompared to that of linear fixed-point equivalents.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4f4e\u7cbe\u5ea6\u5bf9\u6570\u5b9a\u70b9\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u786c\u4ef6\u53cb\u597d\u7684\u5206\u6bb5\u7ebf\u6027\u8fd1\u4f3c\u548c\u6a21\u62df\u9000\u706b\u4f18\u5316\uff0c\u572812\u4f4d\u6574\u6570\u8fd0\u7b97\u4e0b\u5b9e\u73b0\u4e0e32\u4f4d\u6d6e\u70b9\u8bad\u7ec3\u76f8\u5f53\u7684\u7cbe\u5ea6\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u786c\u4ef6\u9762\u79ef\u548c\u80fd\u8017\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u63a8\u7406\u5df2\u901a\u8fc7\u91cf\u5316\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u4f46\u8bad\u7ec3\u4ecd\u4e3b\u8981\u4f9d\u8d56\u590d\u6742\u7684\u6d6e\u70b9\u8fd0\u7b97\u3002\u4f4e\u7cbe\u5ea6\u5b9a\u70b9\u8bad\u7ec3\u662f\u4e00\u4e2a\u6709\u5438\u5f15\u529b\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u7279\u522b\u662f\u9488\u5bf9\u672a\u6765\u786c\u4ef6\u52a0\u901f\u5668\u8bbe\u8ba1\u3002", "method": "\u5f15\u5165\u6bd4\u7279\u5bbd\u5ea6\u8bbe\u8ba1\u7b97\u672f\u8fd0\u7b97\u8fd1\u4f3c\uff0c\u63d0\u51fa\u786c\u4ef6\u53cb\u597d\u7684\u5206\u6bb5\u7ebf\u6027\u8fd1\u4f3c\u7528\u4e8e\u5bf9\u6570\u52a0\u6cd5\uff0c\u4f7f\u7528\u6a21\u62df\u9000\u706b\u5728\u4e0d\u540c\u7cbe\u5ea6\u7ea7\u522b\u4f18\u5316\u8be5\u8fd1\u4f3c\uff0c\u5e76\u901a\u8fc7C++\u6bd4\u7279\u7cbe\u786e\u6a21\u62df\u9a8c\u8bc1\u8bad\u7ec3\u6548\u679c\u3002", "result": "\u5728CIFAR-100\u548cTinyImageNet\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u752812\u4f4d\u6574\u6570\u8fd0\u7b97\u8bad\u7ec3VGG-11\u548cVGG-16\u6a21\u578b\uff0c\u4e0e32\u4f4d\u6d6e\u70b9\u8bad\u7ec3\u76f8\u6bd4\u7cbe\u5ea6\u635f\u5931\u6781\u5c0f\u3002\u786c\u4ef6\u7814\u7a76\u8868\u660e\uff0c\u63d0\u51fa\u7684LNS\u4e58\u7d2f\u52a0\u5355\u5143\u76f8\u6bd4\u7ebf\u6027\u5b9a\u70b9\u7b49\u6548\u5355\u5143\u9762\u79ef\u51cf\u5c1132.5%\uff0c\u80fd\u8017\u964d\u4f4e53.5%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8bc1\u660e\u4e86\u4f4e\u7cbe\u5ea6\u5bf9\u6570\u5b9a\u70b9\u8bad\u7ec3\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u80fd\u591f\u663e\u8457\u964d\u4f4e\u786c\u4ef6\u8d44\u6e90\u6d88\u8017\uff0c\u4e3a\u672a\u6765\u9ad8\u6548\u786c\u4ef6\u52a0\u901f\u5668\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2510.17059", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17059", "abs": "https://arxiv.org/abs/2510.17059", "authors": ["Kathryn Wantlin", "Chongyi Zheng", "Benjamin Eysenbach"], "title": "Consistent Zero-Shot Imitation with Contrastive Goal Inference", "comment": null, "summary": "In the same way that generative models today conduct most of their training\nin a self-supervised fashion, how can agentic models conduct their training in\na self-supervised fashion, interactively exploring, learning, and preparing to\nquickly adapt to new tasks? A prerequisite for embodied agents deployed in real\nworld interactions ought to be training with interaction, yet today's most\nsuccessful AI models (e.g., VLMs, LLMs) are trained without an explicit notion\nof action. The problem of pure exploration (which assumes no data as input) is\nwell studied in the reinforcement learning literature and provides agents with\na wide array of experiences, yet it fails to prepare them for rapid adaptation\nto new tasks. Today's language and vision models are trained on data provided\nby humans, which provides a strong inductive bias for the sorts of tasks that\nthe model will have to solve (e.g., modeling chords in a song, phrases in a\nsonnet, sentences in a medical record). However, when they are prompted to\nsolve a new task, there is a faulty tacit assumption that humans spend most of\ntheir time in the most rewarding states. The key contribution of our paper is a\nmethod for pre-training interactive agents in a self-supervised fashion, so\nthat they can instantly mimic human demonstrations. Our method treats goals\n(i.e., observations) as the atomic construct. During training, our method\nautomatically proposes goals and practices reaching them, building off prior\nwork in reinforcement learning exploration. During evaluation, our method\nsolves an (amortized) inverse reinforcement learning problem to explain\ndemonstrations as optimal goal-reaching behavior. Experiments on standard\nbenchmarks (not designed for goal-reaching) show that our approach outperforms\nprior methods for zero-shot imitation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u4ea4\u4e92\u5f0f\u667a\u80fd\u4f53\u7684\u65b9\u6cd5\uff0c\u4f7f\u5176\u80fd\u591f\u5feb\u901f\u6a21\u4eff\u4eba\u7c7b\u6f14\u793a\u3002\u8be5\u65b9\u6cd5\u5c06\u76ee\u6807\u4f5c\u4e3a\u57fa\u672c\u6784\u5efa\u5757\uff0c\u5728\u8bad\u7ec3\u4e2d\u81ea\u52a8\u63d0\u51fa\u76ee\u6807\u5e76\u7ec3\u4e60\u8fbe\u6210\uff0c\u5728\u8bc4\u4f30\u65f6\u901a\u8fc7\u9006\u5f3a\u5316\u5b66\u4e60\u89e3\u91ca\u6f14\u793a\u4e3a\u6700\u4f18\u76ee\u6807\u8fbe\u6210\u884c\u4e3a\u3002", "motivation": "\u5f53\u524d\u6210\u529f\u7684AI\u6a21\u578b\uff08\u5982VLMs\u3001LLMs\uff09\u7f3a\u4e4f\u660e\u786e\u7684\u52a8\u4f5c\u6982\u5ff5\uff0c\u800c\u7eaf\u63a2\u7d22\u65b9\u6cd5\u65e0\u6cd5\u8ba9\u667a\u80fd\u4f53\u5feb\u901f\u9002\u5e94\u65b0\u4efb\u52a1\u3002\u4eba\u7c7b\u6570\u636e\u63d0\u4f9b\u4e86\u5f3a\u5f52\u7eb3\u504f\u7f6e\uff0c\u4f46\u5b58\u5728\u9519\u8bef\u5047\u8bbe\u2014\u2014\u4eba\u7c7b\u5927\u90e8\u5206\u65f6\u95f4\u5904\u4e8e\u6700\u6709\u4ef7\u503c\u72b6\u6001\u3002", "method": "\u5c06\u76ee\u6807\u4f5c\u4e3a\u539f\u5b50\u6784\u9020\uff0c\u8bad\u7ec3\u65f6\u81ea\u52a8\u63d0\u51fa\u76ee\u6807\u5e76\u7ec3\u4e60\u8fbe\u6210\uff0c\u8bc4\u4f30\u65f6\u901a\u8fc7\u9006\u5f3a\u5316\u5b66\u4e60\u89e3\u91ca\u6f14\u793a\u4e3a\u6700\u4f18\u76ee\u6807\u8fbe\u6210\u884c\u4e3a\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\uff08\u975e\u4e3a\u76ee\u6807\u8fbe\u6210\u8bbe\u8ba1\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u6a21\u4eff\u65b9\u9762\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u901a\u8fc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u4f7f\u4ea4\u4e92\u5f0f\u667a\u80fd\u4f53\u5feb\u901f\u9002\u5e94\u65b0\u4efb\u52a1\uff0c\u5b9e\u73b0\u5373\u65f6\u6a21\u4eff\u4eba\u7c7b\u6f14\u793a\u3002"}}
{"id": "2510.17088", "categories": ["cs.LG", "cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2510.17088", "abs": "https://arxiv.org/abs/2510.17088", "authors": ["Zan Li", "Rui Fan"], "title": "Explainable Heterogeneous Anomaly Detection in Financial Networks via Adaptive Expert Routing", "comment": null, "summary": "Financial anomalies exhibit heterogeneous mechanisms (price shocks, liquidity\nfreezes, contagion cascades, regime shifts), but existing detectors treat all\nanomalies uniformly, producing scalar scores without revealing which mechanism\nis failing, where risks concentrate, or how to intervene. This opacity prevents\ntargeted regulatory responses. Three unsolved challenges persist: (1) static\ngraph structures cannot adapt when market correlations shift during regime\nchanges; (2) uniform detection mechanisms miss type-specific signatures across\nmultiple temporal scales while failing to integrate individual behaviors with\nnetwork contagion; (3) black-box outputs provide no actionable guidance on\nanomaly mechanisms or their temporal evolution.\n  We address these via adaptive graph learning with specialized expert networks\nthat provide built-in interpretability. Our framework captures multi-scale\ntemporal dependencies through BiLSTM with self-attention, fuses temporal and\nspatial information via cross-modal attention, learns dynamic graphs through\nneural multi-source interpolation, adaptively balances learned dynamics with\nstructural priors via stress-modulated fusion, routes anomalies to four\nmechanism-specific experts, and produces dual-level interpretable attributions.\nCritically, interpretability is embedded architecturally rather than applied\npost-hoc.\n  On 100 US equities (2017-2024), we achieve 92.3% detection of 13 major events\nwith 3.8-day lead time, outperforming best baseline by 30.8pp. Silicon Valley\nBank case study demonstrates anomaly evolution tracking: Price-Shock expert\nweight rose to 0.39 (33% above baseline 0.29) during closure, peaking at 0.48\n(66% above baseline) one week later, revealing automatic temporal mechanism\nidentification without labeled supervision.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u56fe\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4e13\u4e1a\u4e13\u5bb6\u7f51\u7edc\u68c0\u6d4b\u91d1\u878d\u5f02\u5e38\uff0c\u63d0\u4f9b\u5185\u7f6e\u53ef\u89e3\u91ca\u6027\uff0c\u80fd\u591f\u8bc6\u522b\u5f02\u5e38\u673a\u5236\u7c7b\u578b\u5e76\u8ffd\u8e2a\u5176\u65f6\u95f4\u6f14\u5316\u3002", "motivation": "\u73b0\u6709\u91d1\u878d\u5f02\u5e38\u68c0\u6d4b\u5668\u5c06\u6240\u6709\u5f02\u5e38\u7edf\u4e00\u5904\u7406\uff0c\u65e0\u6cd5\u63ed\u793a\u5177\u4f53\u5931\u6548\u673a\u5236\u3001\u98ce\u9669\u96c6\u4e2d\u4f4d\u7f6e\u6216\u5e72\u9884\u65b9\u5f0f\uff0c\u8fd9\u79cd\u4e0d\u900f\u660e\u6027\u963b\u788d\u4e86\u9488\u5bf9\u6027\u76d1\u7ba1\u54cd\u5e94\u3002\u5b58\u5728\u4e09\u4e2a\u672a\u89e3\u51b3\u95ee\u9898\uff1a\u9759\u6001\u56fe\u7ed3\u6784\u65e0\u6cd5\u9002\u5e94\u5e02\u573a\u76f8\u5173\u6027\u53d8\u5316\uff1b\u7edf\u4e00\u68c0\u6d4b\u673a\u5236\u9519\u8fc7\u8de8\u591a\u65f6\u95f4\u5c3a\u5ea6\u7684\u7c7b\u578b\u7279\u5b9a\u7279\u5f81\uff1b\u9ed1\u76d2\u8f93\u51fa\u65e0\u6cd5\u63d0\u4f9b\u5173\u4e8e\u5f02\u5e38\u673a\u5236\u53ca\u5176\u65f6\u95f4\u6f14\u5316\u7684\u53ef\u64cd\u4f5c\u6307\u5bfc\u3002", "method": "\u901a\u8fc7\u81ea\u9002\u5e94\u56fe\u5b66\u4e60\u4e0e\u4e13\u4e1a\u4e13\u5bb6\u7f51\u7edc\u6784\u5efa\u6846\u67b6\uff1a\u4f7f\u7528\u5e26\u81ea\u6ce8\u610f\u529b\u7684BiLSTM\u6355\u6349\u591a\u5c3a\u5ea6\u65f6\u95f4\u4f9d\u8d56\uff1b\u901a\u8fc7\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u878d\u5408\u65f6\u7a7a\u4fe1\u606f\uff1b\u901a\u8fc7\u795e\u7ecf\u591a\u6e90\u63d2\u503c\u5b66\u4e60\u52a8\u6001\u56fe\uff1b\u901a\u8fc7\u538b\u529b\u8c03\u5236\u878d\u5408\u81ea\u9002\u5e94\u5e73\u8861\u5b66\u4e60\u52a8\u6001\u4e0e\u7ed3\u6784\u5148\u9a8c\uff1b\u5c06\u5f02\u5e38\u8def\u7531\u5230\u56db\u4e2a\u673a\u5236\u7279\u5b9a\u4e13\u5bb6\uff1b\u4ea7\u751f\u53cc\u91cd\u53ef\u89e3\u91ca\u5f52\u56e0\u3002\u53ef\u89e3\u91ca\u6027\u5728\u67b6\u6784\u4e2d\u5d4c\u5165\u800c\u975e\u4e8b\u540e\u5e94\u7528\u3002", "result": "\u5728100\u53ea\u7f8e\u56fd\u80a1\u7968\uff082017-2024\uff09\u4e0a\uff0c\u5b9e\u73b0\u4e8692.3%\u768413\u4e2a\u4e3b\u8981\u4e8b\u4ef6\u68c0\u6d4b\u7387\uff0c\u63d0\u524d3.8\u5929\uff0c\u6bd4\u6700\u4f73\u57fa\u7ebf\u9ad8\u51fa30.8\u4e2a\u767e\u5206\u70b9\u3002\u7845\u8c37\u94f6\u884c\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u5f02\u5e38\u6f14\u5316\u8ffd\u8e2a\uff1a\u4ef7\u683c\u51b2\u51fb\u4e13\u5bb6\u6743\u91cd\u5728\u5173\u95ed\u671f\u95f4\u5347\u81f30.39\uff08\u6bd4\u57fa\u7ebf0.29\u9ad833%\uff09\uff0c\u4e00\u5468\u540e\u8fbe\u5230\u5cf0\u503c0.48\uff08\u6bd4\u57fa\u7ebf\u9ad866%\uff09\uff0c\u65e0\u9700\u6807\u6ce8\u76d1\u7763\u5373\u53ef\u81ea\u52a8\u8bc6\u522b\u65f6\u95f4\u673a\u5236\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u56fe\u5b66\u4e60\u548c\u673a\u5236\u7279\u5b9a\u4e13\u5bb6\u7f51\u7edc\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u91d1\u878d\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u63d0\u4f9b\u4e86\u5185\u7f6e\u53ef\u89e3\u91ca\u6027\u548c\u5bf9\u5f02\u5e38\u673a\u5236\u6f14\u5316\u7684\u81ea\u52a8\u8bc6\u522b\u80fd\u529b\uff0c\u4e3a\u9488\u5bf9\u6027\u76d1\u7ba1\u54cd\u5e94\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2510.17099", "categories": ["cs.LG", "cs.GT"], "pdf": "https://arxiv.org/pdf/2510.17099", "abs": "https://arxiv.org/abs/2510.17099", "authors": ["Zhiyuan Fan", "Arnab Maiti", "Kevin Jamieson", "Lillian J. Ratliff", "Gabriele Farina"], "title": "On the Universal Near Optimality of Hedge in Combinatorial Settings", "comment": "28 pages, 1 Figure", "summary": "In this paper, we study the classical Hedge algorithm in combinatorial\nsettings. In each round, the learner selects a vector $\\boldsymbol{x}_t$ from a\nset $X \\subseteq \\{0,1\\}^d$, observes a full loss vector $\\boldsymbol{y}_t \\in\n\\mathbb{R}^d$, and incurs a loss $\\langle \\boldsymbol{x}_t, \\boldsymbol{y}_t\n\\rangle \\in [-1,1]$. This setting captures several important problems,\nincluding extensive-form games, resource allocation, $m$-sets, online multitask\nlearning, and shortest-path problems on directed acyclic graphs (DAGs). It is\nwell known that Hedge achieves a regret of $O\\big(\\sqrt{T \\log |X|}\\big)$ after\n$T$ rounds of interaction. In this paper, we ask whether Hedge is optimal\nacross all combinatorial settings. To that end, we show that for any $X\n\\subseteq \\{0,1\\}^d$, Hedge is near-optimal--specifically, up to a $\\sqrt{\\log\nd}$ factor--by establishing a lower bound of $\\Omega\\big(\\sqrt{T \\log(|X|)/\\log\nd}\\big)$ that holds for any algorithm. We then identify a natural class of\ncombinatorial sets--namely, $m$-sets with $\\log d \\leq m \\leq \\sqrt{d}$--for\nwhich this lower bound is tight, and for which Hedge is provably suboptimal by\na factor of exactly $\\sqrt{\\log d}$. At the same time, we show that Hedge is\noptimal for online multitask learning, a generalization of the classical\n$K$-experts problem. Finally, we leverage the near-optimality of Hedge to\nestablish the existence of a near-optimal regularizer for online shortest-path\nproblems in DAGs--a setting that subsumes a broad range of combinatorial\ndomains. Specifically, we show that the classical Online Mirror Descent (OMD)\nalgorithm, when instantiated with the dilated entropy regularizer, is\niterate-equivalent to Hedge, and therefore inherits its near-optimal regret\nguarantees for DAGs.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86Hedge\u7b97\u6cd5\u5728\u7ec4\u5408\u8bbe\u7f6e\u4e2d\u7684\u6700\u4f18\u6027\uff0c\u53d1\u73b0Hedge\u5728\u5927\u591a\u6570\u7ec4\u5408\u8bbe\u7f6e\u4e2d\u63a5\u8fd1\u6700\u4f18\uff0c\u4f46\u5728\u67d0\u4e9b\u7279\u5b9a\u8bbe\u7f6e\uff08\u5982m-\u96c6\u5408\uff09\u4e2d\u6bd4\u6700\u4f18\u7b97\u6cd5\u5dee\u221alog d\u500d\uff0c\u800c\u5728\u5728\u7ebf\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u662f\u6700\u4f18\u7684\u3002", "motivation": "\u7814\u7a76Hedge\u7b97\u6cd5\u5728\u7ec4\u5408\u8bbe\u7f6e\u4e2d\u7684\u6700\u4f18\u6027\uff0c\u786e\u5b9a\u5176\u5728\u54ea\u4e9b\u7ec4\u5408\u95ee\u9898\u4e2d\u662f\u6700\u4f18\u7684\uff0c\u54ea\u4e9b\u4e0d\u662f\uff0c\u5e76\u5efa\u7acb\u76f8\u5e94\u7684\u4e0b\u754c\u3002", "method": "\u901a\u8fc7\u5efa\u7acb\u7ec4\u5408\u8bbe\u7f6e\u7684\u4e00\u822c\u4e0b\u754c\u03a9(\u221a(T log(|X|)/log d))\uff0c\u5e76\u4e0eHedge\u7684\u4e0a\u754cO(\u221a(T log|X|))\u8fdb\u884c\u6bd4\u8f83\uff0c\u5206\u6790Hedge\u7684\u6700\u4f18\u6027\u5dee\u8ddd\u3002", "result": "\u8bc1\u660eHedge\u5728\u5927\u591a\u6570\u7ec4\u5408\u8bbe\u7f6e\u4e2d\u63a5\u8fd1\u6700\u4f18\uff08\u6700\u591a\u5dee\u221alog d\u500d\uff09\uff0c\u4f46\u5728m-\u96c6\u5408\u8bbe\u7f6e\u4e2d\u786e\u5b9e\u6bd4\u6700\u4f18\u7b97\u6cd5\u5dee\u221alog d\u500d\uff0c\u800c\u5728\u5728\u7ebf\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u662f\u6700\u4f18\u7684\u3002", "conclusion": "Hedge\u5728\u7ec4\u5408\u8bbe\u7f6e\u4e2d\u901a\u5e38\u63a5\u8fd1\u6700\u4f18\uff0c\u4f46\u5b58\u5728\u7279\u5b9a\u8bbe\u7f6e\uff08\u5982m-\u96c6\u5408\uff09\u4e2d\u4e0d\u662f\u6700\u4f18\u7684\uff0c\u8fd9\u4e00\u53d1\u73b0\u6709\u52a9\u4e8e\u7406\u89e3\u7ec4\u5408\u5728\u7ebf\u5b66\u4e60\u7b97\u6cd5\u7684\u6027\u80fd\u754c\u9650\u3002"}}
{"id": "2510.17106", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17106", "abs": "https://arxiv.org/abs/2510.17106", "authors": ["Chen Zhang", "Weixin Bu", "Wendong Xu", "Runsheng Yu", "Yik-Chung Wu", "Ngai Wong"], "title": "Fighter: Unveiling the Graph Convolutional Nature of Transformers in Time Series Modeling", "comment": "Preprint", "summary": "Transformers have achieved remarkable success in time series modeling, yet\ntheir internal mechanisms remain opaque. This work demystifies the Transformer\nencoder by establishing its fundamental equivalence to a Graph Convolutional\nNetwork (GCN). We show that in the forward pass, the attention distribution\nmatrix serves as a dynamic adjacency matrix, and its composition with\nsubsequent transformations performs computations analogous to graph\nconvolution. Moreover, we demonstrate that in the backward pass, the update\ndynamics of value and feed-forward projections mirror those of GCN parameters.\nBuilding on this unified theoretical reinterpretation, we propose\n\\textbf{Fighter} (Flexible Graph Convolutional Transformer), a streamlined\narchitecture that removes redundant linear projections and incorporates\nmulti-hop graph aggregation. This perspective yields an explicit and\ninterpretable representation of temporal dependencies across different scales,\nnaturally expressed as graph edges. Experiments on standard forecasting\nbenchmarks confirm that Fighter achieves competitive performance while\nproviding clearer mechanistic interpretability of its predictions.", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u4e86Transformer\u7f16\u7801\u5668\u4e0e\u56fe\u5377\u79ef\u7f51\u7edc(GCN)\u7684\u57fa\u672c\u7b49\u4ef7\u6027\uff0c\u63d0\u51faFighter\u67b6\u6784\u901a\u8fc7\u79fb\u9664\u5197\u4f59\u7ebf\u6027\u6295\u5f71\u548c\u591a\u8df3\u56fe\u805a\u5408\uff0c\u5728\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u540c\u65f6\u63d0\u4f9b\u66f4\u6e05\u6670\u7684\u673a\u5236\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5c3d\u7ba1Transformer\u5728\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u5176\u5185\u90e8\u673a\u5236\u4ecd\u7136\u4e0d\u900f\u660e\uff0c\u9700\u8981\u63ed\u793a\u5176\u5de5\u4f5c\u539f\u7406\u5e76\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u5c06\u6ce8\u610f\u529b\u5206\u5e03\u77e9\u9635\u89c6\u4e3a\u52a8\u6001\u90bb\u63a5\u77e9\u9635\uff0c\u8bc1\u660e\u5176\u4e0e\u540e\u7eed\u53d8\u6362\u7684\u7ec4\u5408\u6267\u884c\u7c7b\u4f3c\u56fe\u5377\u79ef\u7684\u8ba1\u7b97\uff0c\u63d0\u51faFighter\u67b6\u6784\u79fb\u9664\u5197\u4f59\u7ebf\u6027\u6295\u5f71\u5e76\u5f15\u5165\u591a\u8df3\u56fe\u805a\u5408\u3002", "result": "\u5728\u6807\u51c6\u9884\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFighter\u5b9e\u73b0\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4e3a\u5176\u9884\u6d4b\u63d0\u4f9b\u4e86\u66f4\u6e05\u6670\u7684\u673a\u5236\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "Transformer\u7f16\u7801\u5668\u5728\u672c\u8d28\u4e0a\u7b49\u540c\u4e8e\u56fe\u5377\u79ef\u7f51\u7edc\uff0c\u8fd9\u79cd\u7edf\u4e00\u7684\u7406\u8bba\u91cd\u65b0\u89e3\u91ca\u4e3a\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u63d0\u4f9b\u4e86\u66f4\u660e\u786e\u548c\u53ef\u89e3\u91ca\u7684\u8868\u793a\u65b9\u6cd5\u3002"}}
{"id": "2510.17136", "categories": ["cs.LG", "I.2.6; I.5.1; I.5.4"], "pdf": "https://arxiv.org/pdf/2510.17136", "abs": "https://arxiv.org/abs/2510.17136", "authors": ["Enhao Gu", "Haolin Hou"], "title": "In-situ Autoguidance: Eliciting Self-Correction in Diffusion Models", "comment": "6 pages, 3 figures. ICML 2025 Workshop submission", "summary": "The generation of high-quality, diverse, and prompt-aligned images is a\ncentral goal in image-generating diffusion models. The popular classifier-free\nguidance (CFG) approach improves quality and alignment at the cost of reduced\nvariation, creating an inherent entanglement of these effects. Recent work has\nsuccessfully disentangled these properties by guiding a model with a separately\ntrained, inferior counterpart; however, this solution introduces the\nconsiderable overhead of requiring an auxiliary model. We challenge this\nprerequisite by introducing In-situ Autoguidance, a method that elicits\nguidance from the model itself without any auxiliary components. Our approach\ndynamically generates an inferior prediction on the fly using a stochastic\nforward pass, reframing guidance as a form of inference-time self-correction.\nWe demonstrate that this zero-cost approach is not only viable but also\nestablishes a powerful new baseline for cost-efficient guidance, proving that\nthe benefits of self-guidance can be achieved without external models.", "AI": {"tldr": "\u63d0\u51faIn-situ Autoguidance\u65b9\u6cd5\uff0c\u65e0\u9700\u8f85\u52a9\u6a21\u578b\u5373\u53ef\u5b9e\u73b0\u56fe\u50cf\u751f\u6210\u7684\u81ea\u6211\u5f15\u5bfc\uff0c\u89e3\u51b3\u4e86CFG\u65b9\u6cd5\u5728\u63d0\u5347\u8d28\u91cf\u548c\u5bf9\u9f50\u5ea6\u65f6\u51cf\u5c11\u591a\u6837\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u5206\u7c7b\u5668\u81ea\u7531\u5f15\u5bfc(CFG)\u65b9\u6cd5\u5728\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u548c\u63d0\u793a\u5bf9\u9f50\u5ea6\u65f6\u5bfc\u81f4\u591a\u6837\u6027\u964d\u4f4e\u7684\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u73b0\u6709\u89e3\u8026\u65b9\u6cd5\u9700\u8981\u989d\u5916\u8bad\u7ec3\u8f85\u52a9\u6a21\u578b\u7684\u5f00\u9500\u3002", "method": "\u901a\u8fc7\u968f\u673a\u524d\u5411\u4f20\u9012\u52a8\u6001\u751f\u6210\u8f83\u5dee\u7684\u9884\u6d4b\uff0c\u5c06\u5f15\u5bfc\u91cd\u65b0\u5b9a\u4e49\u4e3a\u63a8\u7406\u65f6\u7684\u81ea\u6211\u6821\u6b63\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u96f6\u6210\u672c\u7684\u81ea\u6211\u5f15\u5bfc\u3002", "result": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u53ef\u884c\uff0c\u800c\u4e14\u4e3a\u6210\u672c\u9ad8\u6548\u7684\u5f15\u5bfc\u5efa\u7acb\u4e86\u5f3a\u5927\u7684\u65b0\u57fa\u51c6\uff0c\u8bc1\u660e\u65e0\u9700\u5916\u90e8\u6a21\u578b\u5373\u53ef\u5b9e\u73b0\u81ea\u6211\u5f15\u5bfc\u7684\u76ca\u5904\u3002", "conclusion": "In-situ Autoguidance\u8bc1\u660e\u4e86\u81ea\u6211\u5f15\u5bfc\u7684\u76ca\u5904\u53ef\u4ee5\u5728\u6ca1\u6709\u5916\u90e8\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\uff0c\u4e3a\u96f6\u6210\u672c\u7684\u56fe\u50cf\u751f\u6210\u5f15\u5bfc\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.17160", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17160", "abs": "https://arxiv.org/abs/2510.17160", "authors": ["Derda Kaymak", "Gyuhak Kim", "Tomoya Kaichi", "Tatsuya Konishi", "Bing Liu"], "title": "Learning After Model Deployment", "comment": "Published at ECAI-2025", "summary": "In classic supervised learning, once a model is deployed in an application,\nit is fixed. No updates will be made to it during the application. This is\ninappropriate for many dynamic and open environments, where unexpected samples\nfrom unseen classes may appear. In such an environment, the model should be\nable to detect these novel samples from unseen classes and learn them after\nthey are labeled. We call this paradigm Autonomous Learning after Model\nDeployment (ALMD). The learning here is continuous and involves no human\nengineers. Labeling in this scenario is performed by human co-workers or other\nknowledgeable agents, which is similar to what humans do when they encounter an\nunfamiliar object and ask another person for its name. In ALMD, the detection\nof novel samples is dynamic and differs from traditional out-of-distribution\n(OOD) detection in that the set of in-distribution (ID) classes expands as new\nclasses are learned during application, whereas ID classes is fixed in\ntraditional OOD detection. Learning is also different from classic supervised\nlearning because in ALMD, we learn the encountered new classes immediately and\nincrementally. It is difficult to retrain the model from scratch using all the\npast data from the ID classes and the novel samples from newly discovered\nclasses, as this would be resource- and time-consuming. Apart from these two\nchallenges, ALMD faces the data scarcity issue because instances of new classes\noften appear sporadically in real-life applications. To address these issues,\nwe propose a novel method, PLDA, which performs dynamic OOD detection and\nincremental learning of new classes on the fly. Empirical evaluations will\ndemonstrate the effectiveness of PLDA.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aALMD\u7684\u65b0\u5b66\u4e60\u8303\u5f0f\uff0c\u4f7f\u6a21\u578b\u5728\u90e8\u7f72\u540e\u80fd\u81ea\u4e3b\u68c0\u6d4b\u672a\u89c1\u7c7b\u522b\u6837\u672c\u5e76\u589e\u91cf\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u5728\u52a8\u6001\u73af\u5883\u4e2d\u65e0\u6cd5\u66f4\u65b0\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u90e8\u7f72\u540e\u56fa\u5b9a\u4e0d\u53d8\uff0c\u4e0d\u9002\u7528\u4e8e\u52a8\u6001\u5f00\u653e\u73af\u5883\u3002\u5f53\u51fa\u73b0\u672a\u89c1\u7c7b\u522b\u6837\u672c\u65f6\uff0c\u6a21\u578b\u5e94\u80fd\u68c0\u6d4b\u8fd9\u4e9b\u65b0\u6837\u672c\u5e76\u5728\u6807\u6ce8\u540e\u5b66\u4e60\u5b83\u4eec\u3002", "method": "\u63d0\u51fa\u4e86PLDA\u65b9\u6cd5\uff0c\u5b9e\u73b0\u52a8\u6001OOD\u68c0\u6d4b\u548c\u5728\u7ebf\u589e\u91cf\u5b66\u4e60\u65b0\u7c7b\u522b\uff0c\u65e0\u9700\u4ece\u5934\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u7ecf\u9a8c\u8bc4\u4f30\u5c06\u5c55\u793aPLDA\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "ALMD\u8303\u5f0f\u4f7f\u6a21\u578b\u5728\u90e8\u7f72\u540e\u80fd\u6301\u7eed\u81ea\u4e3b\u5b66\u4e60\u548c\u9002\u5e94\u52a8\u6001\u73af\u5883\uff0cPLDA\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u6001OOD\u68c0\u6d4b\u548c\u589e\u91cf\u5b66\u4e60\u7684\u6311\u6218\u3002"}}
{"id": "2510.17162", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17162", "abs": "https://arxiv.org/abs/2510.17162", "authors": ["Guanjie Cheng", "Siyang Liu", "Junqin Huang", "Xinkui Zhao", "Yin Wang", "Mengying Zhu", "Linghe Kong", "Shuiguang Deng"], "title": "ALPINE: A Lightweight and Adaptive Privacy-Decision Agent Framework for Dynamic Edge Crowdsensing", "comment": "12 pages, 8 figures, 4 tables. Submitted to The Web Conference (WWW\n  2026)", "summary": "Mobile edge crowdsensing (MECS) systems continuously generate and transmit\nuser data in dynamic, resource-constrained environments, exposing users to\nsignificant privacy threats. In practice, many privacy-preserving mechanisms\nbuild on differential privacy (DP). However, static DP mechanisms often fail to\nadapt to evolving risks, for example, shifts in adversarial capabilities,\nresource constraints and task requirements, resulting in either excessive noise\nor inadequate protection. To address this challenge, we propose ALPINE, a\nlightweight, adaptive framework that empowers terminal devices to autonomously\nadjust differential privacy levels in real time. ALPINE operates as a\nclosed-loop control system consisting of four modules: dynamic risk perception,\nprivacy decision via twin delayed deep deterministic policy gradient (TD3),\nlocal privacy execution and performance verification from edge nodes. Based on\nenvironmental risk assessments, we design a reward function that balances\nprivacy gains, data utility and energy cost, guiding the TD3 agent to\nadaptively tune noise magnitude across diverse risk scenarios and achieve a\ndynamic equilibrium among privacy, utility and cost. Both the collaborative\nrisk model and pretrained TD3-based agent are designed for low-overhead\ndeployment. Extensive theoretical analysis and real-world simulations\ndemonstrate that ALPINE effectively mitigates inference attacks while\npreserving utility and cost, making it practical for large-scale edge\napplications.", "AI": {"tldr": "ALPINE\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u8ba9\u7ec8\u7aef\u8bbe\u5907\u80fd\u591f\u5b9e\u65f6\u8c03\u6574\u5dee\u5206\u9690\u79c1\u7ea7\u522b\uff0c\u5728\u79fb\u52a8\u8fb9\u7f18\u4f17\u611f\u7cfb\u7edf\u4e2d\u5e73\u8861\u9690\u79c1\u4fdd\u62a4\u3001\u6570\u636e\u6548\u7528\u548c\u80fd\u8017\u6210\u672c\u3002", "motivation": "\u79fb\u52a8\u8fb9\u7f18\u4f17\u611f\u7cfb\u7edf\u5728\u52a8\u6001\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u6301\u7eed\u751f\u6210\u548c\u4f20\u8f93\u7528\u6237\u6570\u636e\uff0c\u5b58\u5728\u4e25\u91cd\u9690\u79c1\u5a01\u80c1\u3002\u9759\u6001\u5dee\u5206\u9690\u79c1\u673a\u5236\u65e0\u6cd5\u9002\u5e94\u4e0d\u65ad\u53d8\u5316\u7684\u98ce\u9669\uff0c\u5bfc\u81f4\u8981\u4e48\u566a\u58f0\u8fc7\u5927\u8981\u4e48\u4fdd\u62a4\u4e0d\u8db3\u3002", "method": "ALPINE\u4f5c\u4e3a\u95ed\u73af\u63a7\u5236\u7cfb\u7edf\uff0c\u5305\u542b\u56db\u4e2a\u6a21\u5757\uff1a\u52a8\u6001\u98ce\u9669\u611f\u77e5\u3001\u57fa\u4e8eTD3\u5f3a\u5316\u5b66\u4e60\u7684\u9690\u79c1\u51b3\u7b56\u3001\u672c\u5730\u9690\u79c1\u6267\u884c\u548c\u8fb9\u7f18\u8282\u70b9\u6027\u80fd\u9a8c\u8bc1\u3002\u8bbe\u8ba1\u4e86\u5e73\u8861\u9690\u79c1\u6536\u76ca\u3001\u6570\u636e\u6548\u7528\u548c\u80fd\u8017\u6210\u672c\u7684\u5956\u52b1\u51fd\u6570\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u771f\u5b9e\u4e16\u754c\u4eff\u771f\u8868\u660e\uff0cALPINE\u80fd\u6709\u6548\u7f13\u89e3\u63a8\u7406\u653b\u51fb\uff0c\u540c\u65f6\u4fdd\u6301\u6548\u7528\u548c\u6210\u672c\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u8fb9\u7f18\u5e94\u7528\u3002", "conclusion": "ALPINE\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574\u5dee\u5206\u9690\u79c1\u7ea7\u522b\uff0c\u5728\u52a8\u6001\u8fb9\u7f18\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u9690\u79c1\u3001\u6548\u7528\u548c\u6210\u672c\u4e4b\u95f4\u7684\u52a8\u6001\u5e73\u8861\uff0c\u5177\u6709\u4f4e\u5f00\u9500\u90e8\u7f72\u4f18\u52bf\u3002"}}
{"id": "2510.17185", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17185", "abs": "https://arxiv.org/abs/2510.17185", "authors": ["Runlin Lei", "Lu Yi", "Mingguo He", "Pengyu Qiu", "Zhewei Wei", "Yongchao Liu", "Chuntao Hong"], "title": "Robustness in Text-Attributed Graph Learning: Insights, Trade-offs, and New Defenses", "comment": null, "summary": "While Graph Neural Networks (GNNs) and Large Language Models (LLMs) are\npowerful approaches for learning on Text-Attributed Graphs (TAGs), a\ncomprehensive understanding of their robustness remains elusive. Current\nevaluations are fragmented, failing to systematically investigate the distinct\neffects of textual and structural perturbations across diverse models and\nattack scenarios. To address these limitations, we introduce a unified and\ncomprehensive framework to evaluate robustness in TAG learning. Our framework\nevaluates classical GNNs, robust GNNs (RGNNs), and GraphLLMs across ten\ndatasets from four domains, under diverse text-based, structure-based, and\nhybrid perturbations in both poisoning and evasion scenarios. Our extensive\nanalysis reveals multiple findings, among which three are particularly\nnoteworthy: 1) models have inherent robustness trade-offs between text and\nstructure, 2) the performance of GNNs and RGNNs depends heavily on the text\nencoder and attack type, and 3) GraphLLMs are particularly vulnerable to\ntraining data corruption. To overcome the identified trade-offs, we introduce\nSFT-auto, a novel framework that delivers superior and balanced robustness\nagainst both textual and structural attacks within a single model. Our work\nestablishes a foundation for future research on TAG security and offers\npractical solutions for robust TAG learning in adversarial environments. Our\ncode is available at: https://github.com/Leirunlin/TGRB.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\u6765\u8bc4\u4f30\u6587\u672c\u5c5e\u6027\u56fe(TAG)\u5b66\u4e60\u4e2d\u56fe\u795e\u7ecf\u7f51\u7edc(GNN)\u548c\u5927\u8bed\u8a00\u6a21\u578b(LLM)\u7684\u9c81\u68d2\u6027\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u6587\u672c\u548c\u7ed3\u6784\u6270\u52a8\u95f4\u7684\u56fa\u6709\u6743\u8861\uff0c\u5e76\u63d0\u51fa\u4e86SFT-auto\u6846\u67b6\u6765\u63d0\u5347\u7efc\u5408\u9c81\u68d2\u6027\u3002", "motivation": "\u5f53\u524d\u5bf9\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6587\u672c\u5c5e\u6027\u56fe\u4e0a\u7684\u9c81\u68d2\u6027\u8bc4\u4f30\u662f\u5206\u6563\u7684\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u7814\u7a76\u6765\u63a2\u7a76\u6587\u672c\u548c\u7ed3\u6784\u6270\u52a8\u5728\u4e0d\u540c\u6a21\u578b\u548c\u653b\u51fb\u573a\u666f\u4e0b\u7684\u5177\u4f53\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u9c81\u68d2\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u572810\u4e2a\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u7ecf\u5178GNN\u3001\u9c81\u68d2GNN\u548cGraphLLM\u5728\u6587\u672c\u3001\u7ed3\u6784\u548c\u6df7\u5408\u6270\u52a8\u4e0b\u7684\u8868\u73b0\uff0c\u5305\u62ec\u6295\u6bd2\u548c\u89c4\u907f\u4e24\u79cd\u653b\u51fb\u573a\u666f\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1)\u6a21\u578b\u5728\u6587\u672c\u548c\u7ed3\u6784\u9c81\u68d2\u6027\u95f4\u5b58\u5728\u56fa\u6709\u6743\u8861\uff1b2)GNN\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u6587\u672c\u7f16\u7801\u5668\u548c\u653b\u51fb\u7c7b\u578b\uff1b3)GraphLLM\u5bf9\u8bad\u7ec3\u6570\u636e\u6c61\u67d3\u7279\u522b\u8106\u5f31\u3002\u63d0\u51fa\u7684SFT-auto\u6846\u67b6\u5728\u5355\u4e00\u6a21\u578b\u4e2d\u5b9e\u73b0\u4e86\u5bf9\u6587\u672c\u548c\u7ed3\u6784\u653b\u51fb\u7684\u5747\u8861\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u672a\u6765TAG\u5b89\u5168\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u4e3a\u5bf9\u6297\u73af\u5883\u4e2d\u7684\u9c81\u68d2TAG\u5b66\u4e60\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.17187", "categories": ["cs.LG", "q-bio.BM", "92B20"], "pdf": "https://arxiv.org/pdf/2510.17187", "abs": "https://arxiv.org/abs/2510.17187", "authors": ["Alexander Aghili", "Andy Bruce", "Daniel Sabo", "Sanya Murdeshwar", "Kevin Bachelor", "Ionut Mistreanu", "Ashwin Lokapally", "Razvan Marinescu"], "title": "A Standardized Benchmark for Machine-Learned Molecular Dynamics using Weighted Ensemble Sampling", "comment": "37 Pages (Main Text), 10 Figures, Submitted to Journal of Physical\n  Chemistry B", "summary": "The rapid evolution of molecular dynamics (MD) methods, including\nmachine-learned dynamics, has outpaced the development of standardized tools\nfor method validation. Objective comparison between simulation approaches is\noften hindered by inconsistent evaluation metrics, insufficient sampling of\nrare conformational states, and the absence of reproducible benchmarks. To\naddress these challenges, we introduce a modular benchmarking framework that\nsystematically evaluates protein MD methods using enhanced sampling analysis.\nOur approach uses weighted ensemble (WE) sampling via The Weighted Ensemble\nSimulation Toolkit with Parallelization and Analysis (WESTPA), based on\nprogress coordinates derived from Time-lagged Independent Component Analysis\n(TICA), enabling fast and efficient exploration of protein conformational\nspace. The framework includes a flexible, lightweight propagator interface that\nsupports arbitrary simulation engines, allowing both classical force fields and\nmachine learning-based models. Additionally, the framework offers a\ncomprehensive evaluation suite capable of computing more than 19 different\nmetrics and visualizations across a variety of domains. We further contribute a\ndataset of nine diverse proteins, ranging from 10 to 224 residues, that span a\nvariety of folding complexities and topologies. Each protein has been\nextensively simulated at 300K for one million MD steps per starting point (4\nns). To demonstrate the utility of our framework, we perform validation tests\nusing classic MD simulations with implicit solvent and compare protein\nconformational sampling using a fully trained versus under-trained CGSchNet\nmodel. By standardizing evaluation protocols and enabling direct, reproducible\ncomparisons across MD approaches, our open-source platform lays the groundwork\nfor consistent, rigorous benchmarking across the molecular simulation\ncommunity.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u7684\u5206\u5b50\u52a8\u529b\u5b66\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u86cb\u767d\u8d28MD\u65b9\u6cd5\uff0c\u652f\u6301\u591a\u79cd\u6a21\u62df\u5f15\u64ce\u548c\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u5206\u5b50\u52a8\u529b\u5b66\u65b9\u6cd5\u5feb\u901f\u53d1\u5c55\uff0c\u4f46\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u9a8c\u8bc1\u5de5\u5177\uff0c\u5bfc\u81f4\u4e0d\u540c\u65b9\u6cd5\u96be\u4ee5\u5ba2\u89c2\u6bd4\u8f83\u3002", "method": "\u4f7f\u7528\u52a0\u6743\u96c6\u6210\u91c7\u6837\u548cTICA\u8fdb\u5ea6\u5750\u6807\uff0c\u901a\u8fc7WESTPA\u5de5\u5177\u5305\u5b9e\u73b0\u9ad8\u6548\u6784\u8c61\u7a7a\u95f4\u63a2\u7d22\uff0c\u63d0\u4f9b\u7075\u6d3b\u7684\u4f20\u64ad\u5668\u63a5\u53e3\u652f\u6301\u5404\u79cd\u6a21\u62df\u5f15\u64ce\u3002", "result": "\u5f00\u53d1\u4e86\u5305\u542b9\u79cd\u4e0d\u540c\u86cb\u767d\u8d28\u7684\u6570\u636e\u96c6\uff0c\u6784\u5efa\u4e86\u80fd\u8ba1\u7b9719\u79cd\u4ee5\u4e0a\u8bc4\u4f30\u6307\u6807\u7684\u7efc\u5408\u8bc4\u4f30\u5957\u4ef6\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u5728\u7ecf\u5178MD\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6bd4\u8f83\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u5f00\u6e90\u5e73\u53f0\u901a\u8fc7\u6807\u51c6\u5316\u8bc4\u4f30\u534f\u8bae\uff0c\u4e3a\u5206\u5b50\u6a21\u62df\u793e\u533a\u63d0\u4f9b\u4e86\u53ef\u91cd\u590d\u3001\u4e25\u8c28\u7684\u57fa\u51c6\u6d4b\u8bd5\u57fa\u7840\u3002"}}
{"id": "2510.17189", "categories": ["cs.LG", "cs.AR"], "pdf": "https://arxiv.org/pdf/2510.17189", "abs": "https://arxiv.org/abs/2510.17189", "authors": ["Wenxun Wang", "Shuchang Zhou", "Wenyu Sun", "Peiqin Sun", "Yongpan Liu"], "title": "SOLE: Hardware-Software Co-design of Softmax and LayerNorm for Efficient Transformer Inference", "comment": null, "summary": "Transformers have shown remarkable performance in both natural language\nprocessing (NLP) and computer vision (CV) tasks. However, their real-time\ninference speed and efficiency are limited due to the inefficiency in Softmax\nand Layer Normalization (LayerNorm). Previous works based on function\napproximation suffer from inefficient implementation as they place emphasis on\ncomputation while disregarding memory overhead concerns. Moreover, such methods\nrely on retraining to compensate for approximation error which can be costly\nand inconvenient.\n  In this paper, we present SOLE, a hardware-software co-design for Softmax and\nLayerNorm which is composed of E2Softmax and AILayerNorm. E2Softmax utilizes\nlog2 quantization of exponent function and log-based division to approximate\nSoftmax while AILayerNorm adopts low-precision statistic calculation. Compared\nwith state-of-the-art designs, we achieve both low-precision calculation and\nlow bit-width storage on Softmax and LayerNorm. Experiments show that SOLE\nmaintains inference accuracy without retraining while offering orders of\nmagnitude speedup and energy savings over GPU, achieving 3.04x, 3.86x\nenergy-efficiency improvements and 2.82x, 3.32x area-efficiency improvements\nover prior state-of-the-art custom hardware for Softmax and LayerNorm,\nrespectively.", "AI": {"tldr": "SOLE\u662f\u4e00\u4e2a\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\uff0c\u901a\u8fc7E2Softmax\u548cAILayerNorm\u5206\u522b\u4f18\u5316Transformer\u4e2d\u7684Softmax\u548cLayerNorm\u64cd\u4f5c\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u9ad8\u6548\u63a8\u7406\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u901f\u5ea6\u548c\u80fd\u6548\u3002", "motivation": "Transformer\u5728NLP\u548cCV\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5b9e\u65f6\u63a8\u7406\u901f\u5ea6\u548c\u6548\u7387\u53d7\u5230Softmax\u548cLayerNorm\u64cd\u4f5c\u4f4e\u6548\u7684\u9650\u5236\u3002\u73b0\u6709\u57fa\u4e8e\u51fd\u6570\u8fd1\u4f3c\u7684\u65b9\u6cd5\u5b58\u5728\u5b9e\u73b0\u6548\u7387\u4f4e\u3001\u5ffd\u89c6\u5185\u5b58\u5f00\u9500\u3001\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51faSOLE\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\uff1aE2Softmax\u4f7f\u7528\u5bf9\u65702\u91cf\u5316\u7684\u6307\u6570\u51fd\u6570\u548c\u5bf9\u6570\u9664\u6cd5\u8fd1\u4f3cSoftmax\uff1bAILayerNorm\u91c7\u7528\u4f4e\u7cbe\u5ea6\u7edf\u8ba1\u8ba1\u7b97\u3002\u4e24\u8005\u90fd\u5b9e\u73b0\u4e86\u4f4e\u7cbe\u5ea6\u8ba1\u7b97\u548c\u4f4e\u6bd4\u7279\u4f4d\u5bbd\u5b58\u50a8\u3002", "result": "SOLE\u5728\u4fdd\u6301\u63a8\u7406\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u76f8\u6bd4GPU\u5b9e\u73b0\u4e86\u6570\u91cf\u7ea7\u7684\u901f\u5ea6\u63d0\u5347\u548c\u80fd\u8017\u8282\u7701\u3002\u4e0e\u73b0\u6709\u6700\u4f18\u5b9a\u5236\u786c\u4ef6\u76f8\u6bd4\uff0cSoftmax\u548cLayerNorm\u5206\u522b\u5b9e\u73b0\u4e863.04\u500d\u30013.86\u500d\u7684\u80fd\u6548\u63d0\u5347\u548c2.82\u500d\u30013.32\u500d\u7684\u9762\u79ef\u6548\u7387\u63d0\u5347\u3002", "conclusion": "SOLE\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u9ad8\u6548Transformer\u63a8\u7406\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u663e\u8457\u63d0\u5347\u4e86Softmax\u548cLayerNorm\u64cd\u4f5c\u7684\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2510.17212", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17212", "abs": "https://arxiv.org/abs/2510.17212", "authors": ["Jundong Zhang", "Yuhui Situ", "Fanji Zhang", "Rongji Deng", "Tianqi Wei"], "title": "D2C-HRHR: Discrete Actions with Double Distributional Critics for High-Risk-High-Return Tasks", "comment": null, "summary": "Tasks involving high-risk-high-return (HRHR) actions, such as obstacle\ncrossing, often exhibit multimodal action distributions and stochastic returns.\nMost reinforcement learning (RL) methods assume unimodal Gaussian policies and\nrely on scalar-valued critics, which limits their effectiveness in HRHR\nsettings. We formally define HRHR tasks and theoretically show that Gaussian\npolicies cannot guarantee convergence to the optimal solution. To address this,\nwe propose a reinforcement learning framework that (i) discretizes continuous\naction spaces to approximate multimodal distributions, (ii) employs\nentropy-regularized exploration to improve coverage of risky but rewarding\nactions, and (iii) introduces a dual-critic architecture for more accurate\ndiscrete value distribution estimation. The framework scales to\nhigh-dimensional action spaces, supporting complex control domains. Experiments\non locomotion and manipulation benchmarks with high risks of failure\ndemonstrate that our method outperforms baselines, underscoring the importance\nof explicitly modeling multimodality and risk in RL.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u6765\u89e3\u51b3\u9ad8\u98ce\u9669\u9ad8\u56de\u62a5\u4efb\u52a1\uff0c\u901a\u8fc7\u79bb\u6563\u5316\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u3001\u71b5\u6b63\u5219\u5316\u63a2\u7d22\u548c\u53cc\u8bc4\u8bba\u5bb6\u67b6\u6784\u6765\u5efa\u6a21\u591a\u6a21\u6001\u52a8\u4f5c\u5206\u5e03\u548c\u98ce\u9669\u3002", "motivation": "\u9ad8\u98ce\u9669\u9ad8\u56de\u62a5\u4efb\u52a1\u901a\u5e38\u5177\u6709\u591a\u6a21\u6001\u52a8\u4f5c\u5206\u5e03\u548c\u968f\u673a\u56de\u62a5\uff0c\u800c\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5047\u8bbe\u5355\u5cf0\u9ad8\u65af\u7b56\u7565\u548c\u6807\u91cf\u8bc4\u8bba\u5bb6\uff0c\u9650\u5236\u4e86\u5728\u8fd9\u4e9b\u8bbe\u7f6e\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "\u79bb\u6563\u5316\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u4ee5\u8fd1\u4f3c\u591a\u6a21\u6001\u5206\u5e03\uff0c\u91c7\u7528\u71b5\u6b63\u5219\u5316\u63a2\u7d22\u6765\u6539\u5584\u98ce\u9669\u4f46\u6709\u76ca\u52a8\u4f5c\u7684\u8986\u76d6\uff0c\u5f15\u5165\u53cc\u8bc4\u8bba\u5bb6\u67b6\u6784\u4ee5\u66f4\u51c6\u786e\u4f30\u8ba1\u79bb\u6563\u503c\u5206\u5e03\u3002", "result": "\u5728\u5177\u6709\u9ad8\u5931\u8d25\u98ce\u9669\u7684\u79fb\u52a8\u548c\u64cd\u4f5c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u660e\u786e\u5efa\u6a21\u591a\u6a21\u6001\u6027\u548c\u98ce\u9669\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u6269\u5c55\u5230\u9ad8\u7ef4\u52a8\u4f5c\u7a7a\u95f4\uff0c\u652f\u6301\u590d\u6742\u63a7\u5236\u9886\u57df\u3002"}}
{"id": "2510.17214", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17214", "abs": "https://arxiv.org/abs/2510.17214", "authors": ["Chenyan Fei", "Dalin Zhang", "Chen Melinda Dang"], "title": "Diagnosis of Fuel Cell Health Status with Deep Sparse Auto-Encoder Neural Network", "comment": null, "summary": "Effective and accurate diagnosis of fuel cell health status is crucial for\nensuring the stable operation of fuel cell stacks. Among various parameters,\nhigh-frequency impedance serves as a critical indicator for assessing fuel cell\nstate and health conditions. However, its online testing is prohibitively\ncomplex and costly. This paper employs a deep sparse auto-encoding network for\nthe prediction and classification of high-frequency impedance in fuel cells,\nachieving metric of accuracy rate above 92\\%. The network is further deployed\non an FPGA, attaining a hardware-based recognition rate almost 90\\%.", "AI": {"tldr": "\u4f7f\u7528\u6df1\u5ea6\u7a00\u758f\u81ea\u7f16\u7801\u7f51\u7edc\u9884\u6d4b\u548c\u5206\u7c7b\u71c3\u6599\u7535\u6c60\u9ad8\u9891\u963b\u6297\uff0c\u51c6\u786e\u7387\u8d85\u8fc792%\uff0c\u5e76\u5728FPGA\u4e0a\u90e8\u7f72\u5b9e\u73b0\u8fd190%\u7684\u786c\u4ef6\u8bc6\u522b\u7387\u3002", "motivation": "\u71c3\u6599\u7535\u6c60\u9ad8\u9891\u963b\u6297\u662f\u8bc4\u4f30\u5176\u72b6\u6001\u548c\u5065\u5eb7\u72b6\u51b5\u7684\u5173\u952e\u6307\u6807\uff0c\u4f46\u5728\u7ebf\u6d4b\u8bd5\u590d\u6742\u4e14\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u5f00\u53d1\u6709\u6548\u7684\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u7a00\u758f\u81ea\u7f16\u7801\u7f51\u7edc\u8fdb\u884c\u71c3\u6599\u7535\u6c60\u9ad8\u9891\u963b\u6297\u7684\u9884\u6d4b\u548c\u5206\u7c7b\u3002", "result": "\u5b9e\u73b0\u4e8692%\u4ee5\u4e0a\u7684\u51c6\u786e\u7387\uff0c\u5728FPGA\u4e0a\u90e8\u7f72\u540e\u786c\u4ef6\u8bc6\u522b\u7387\u8fbe\u5230\u8fd190%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u9884\u6d4b\u71c3\u6599\u7535\u6c60\u9ad8\u9891\u963b\u6297\uff0c\u4e3a\u71c3\u6599\u7535\u6c60\u5065\u5eb7\u72b6\u6001\u8bca\u65ad\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.17250", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17250", "abs": "https://arxiv.org/abs/2510.17250", "authors": ["Wei-Hsun Lee", "Che-Yu Chang", "Kuang-Yu Li"], "title": "A Prototypical Network with an Attention-based Encoder for Drivers Identification Application", "comment": null, "summary": "Driver identification has become an area of increasing interest in recent\nyears, especially for data- driven applications, because biometric-based\ntechnologies may incur privacy issues. This study proposes a deep learning\nneural network architecture, an attention-based encoder (AttEnc), which uses an\nattention mechanism for driver identification and uses fewer model parameters\nthan current methods. Most studies do not address the issue of data shortages\nfor driver identification, and most of them are inflexible when encountering\nunknown drivers. In this study, an architecture that combines a prototypical\nnetwork and an attention-based encoder (P-AttEnc) is proposed. It applies\nfew-shot learning to overcome the data shortage issues and to enhance model\ngeneralizations. The experiments showed that the attention-based encoder can\nidentify drivers with accuracies of 99.3%, 99.0% and 99.9% in three different\ndatasets and has a prediction time that is 44% to 79% faster because it\nsignificantly reduces, on average, 87.6% of the model parameters. P-AttEnc\nidentifies drivers based on few shot data, extracts driver fingerprints to\naddress the issue of data shortages, and is able to classify unknown drivers.\nThe first experiment showed that P-AttEnc can identify drivers with an accuracy\nof 69.8% in the one-shot scenario. The second experiment showed that P-AttEnc,\nin the 1-shot scenario, can classify unknown drivers with an average accuracy\nof 65.7%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u7f16\u7801\u5668\uff08AttEnc\uff09\u548c\u7ed3\u5408\u539f\u578b\u7f51\u7edc\u7684P-AttEnc\u67b6\u6784\uff0c\u7528\u4e8e\u9a7e\u9a76\u5458\u8bc6\u522b\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u77ed\u7f3a\u548c\u9690\u79c1\u95ee\u9898\uff0c\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u90fd\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u751f\u7269\u7279\u5f81\u7684\u9a7e\u9a76\u5458\u8bc6\u522b\u6280\u672f\u5b58\u5728\u9690\u79c1\u95ee\u9898\uff0c\u4e14\u5927\u591a\u6570\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u6570\u636e\u77ed\u7f3a\u548c\u672a\u77e5\u9a7e\u9a76\u5458\u7684\u60c5\u51b5\u3002", "method": "\u4f7f\u7528\u6ce8\u610f\u529b\u673a\u5236\u7684\u7f16\u7801\u5668\uff08AttEnc\uff09\u51cf\u5c11\u6a21\u578b\u53c2\u6570\uff1b\u7ed3\u5408\u539f\u578b\u7f51\u7edc\u548c\u6ce8\u610f\u529b\u7f16\u7801\u5668\uff08P-AttEnc\uff09\u5e94\u7528\u5c11\u6837\u672c\u5b66\u4e60\u6765\u5904\u7406\u6570\u636e\u77ed\u7f3a\u548c\u589e\u5f3a\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "result": "AttEnc\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523099.3%\u300199.0%\u548c99.9%\u7684\u51c6\u786e\u7387\uff0c\u9884\u6d4b\u65f6\u95f4\u5feb44%-79%\uff0c\u5e73\u5747\u51cf\u5c1187.6%\u7684\u6a21\u578b\u53c2\u6570\uff1bP-AttEnc\u5728\u5355\u6837\u672c\u573a\u666f\u4e0b\u8bc6\u522b\u51c6\u786e\u7387\u4e3a69.8%\uff0c\u5bf9\u672a\u77e5\u9a7e\u9a76\u5458\u7684\u5e73\u5747\u51c6\u786e\u7387\u4e3a65.7%\u3002", "conclusion": "\u63d0\u51fa\u7684AttEnc\u548cP-AttEnc\u67b6\u6784\u5728\u9a7e\u9a76\u5458\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u65e2\u89e3\u51b3\u4e86\u9690\u79c1\u548c\u6570\u636e\u77ed\u7f3a\u95ee\u9898\uff0c\u53c8\u63d0\u9ad8\u4e86\u8bc6\u522b\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.17281", "categories": ["cs.LG", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.17281", "abs": "https://arxiv.org/abs/2510.17281", "authors": ["Qingyao Ai", "Yichen Tang", "Changyue Wang", "Jianming Long", "Weihang Su", "Yiqun Liu"], "title": "MemoryBench: A Benchmark for Memory and Continual Learning in LLM Systems", "comment": null, "summary": "Scaling up data, parameters, and test-time computation has been the\nmainstream methods to improve LLM systems (LLMsys), but their upper bounds are\nalmost reached due to the gradual depletion of high-quality data and marginal\ngains obtained from larger computational resource consumption. Inspired by the\nabilities of human and traditional AI systems in learning from practice,\nconstructing memory and continual learning frameworks for LLMsys has become an\nimportant and popular research direction in recent literature. Yet, existing\nbenchmarks for LLM memory often focus on evaluating the system on homogeneous\nreading comprehension tasks with long-form inputs rather than testing their\nabilities to learn from accumulated user feedback in service time. Therefore,\nwe propose a user feedback simulation framework and a comprehensive benchmark\ncovering multiple domains, languages, and types of tasks to evaluate the\ncontinual learning abilities of LLMsys. Experiments show that the effectiveness\nand efficiency of state-of-the-art baselines are far from satisfying, and we\nhope this benchmark could pave the way for future studies on LLM memory and\noptimization algorithms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u6237\u53cd\u9988\u6a21\u62df\u6846\u67b6\u548c\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u7cfb\u7edf\u7684\u6301\u7eed\u5b66\u4e60\u80fd\u529b\uff0c\u8986\u76d6\u591a\u9886\u57df\u3001\u591a\u8bed\u8a00\u548c\u591a\u79cd\u4efb\u52a1\u7c7b\u578b\uff0c\u5b9e\u9a8c\u663e\u793a\u73b0\u6709\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u6548\u7387\u4ecd\u6709\u5f88\u5927\u63d0\u5347\u7a7a\u95f4\u3002", "motivation": "\u7531\u4e8e\u9ad8\u8d28\u91cf\u6570\u636e\u9010\u6e10\u8017\u5c3d\u548c\u8ba1\u7b97\u8d44\u6e90\u8fb9\u9645\u6536\u76ca\u9012\u51cf\uff0c\u4f20\u7edf\u901a\u8fc7\u6269\u5927\u6570\u636e\u3001\u53c2\u6570\u548c\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u7684\u65b9\u6cd5\u5df2\u63a5\u8fd1\u6781\u9650\uff0c\u9700\u8981\u501f\u9274\u4eba\u7c7b\u548c\u4f20\u7edfAI\u7cfb\u7edf\u4ece\u5b9e\u8df5\u4e2d\u5b66\u4e60\u7684\u80fd\u529b\uff0c\u6784\u5efaLLM\u7cfb\u7edf\u7684\u8bb0\u5fc6\u548c\u6301\u7eed\u5b66\u4e60\u6846\u67b6\u3002", "method": "\u8bbe\u8ba1\u7528\u6237\u53cd\u9988\u6a21\u62df\u6846\u67b6\u548c\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u591a\u9886\u57df\u3001\u591a\u8bed\u8a00\u548c\u591a\u79cd\u4efb\u52a1\u7c7b\u578b\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u7cfb\u7edf\u5728\u670d\u52a1\u65f6\u95f4\u5185\u4ece\u7d2f\u79ef\u7528\u6237\u53cd\u9988\u4e2d\u5b66\u4e60\u7684\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u6548\u7387\u8fdc\u672a\u8fbe\u5230\u6ee1\u610f\u6c34\u5e73\uff0c\u8bc1\u660e\u4e86\u8be5\u57fa\u51c6\u6d4b\u8bd5\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "\u8be5\u57fa\u51c6\u6d4b\u8bd5\u53ef\u4e3a\u672a\u6765LLM\u8bb0\u5fc6\u548c\u4f18\u5316\u7b97\u6cd5\u7684\u7814\u7a76\u94fa\u5e73\u9053\u8def\uff0c\u63a8\u52a8LLM\u7cfb\u7edf\u6301\u7eed\u5b66\u4e60\u80fd\u529b\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.17313", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17313", "abs": "https://arxiv.org/abs/2510.17313", "authors": ["Tal Barami", "Nimrod Berman", "Ilan Naiman", "Amos H. Hason", "Rotem Ezra", "Omri Azencot"], "title": "Disentanglement Beyond Static vs. Dynamic: A Benchmark and Evaluation Framework for Multi-Factor Sequential Representations", "comment": null, "summary": "Learning disentangled representations in sequential data is a key goal in\ndeep learning, with broad applications in vision, audio, and time series. While\nreal-world data involves multiple interacting semantic factors over time, prior\nwork has mostly focused on simpler two-factor static and dynamic settings,\nprimarily because such settings make data collection easier, thereby\noverlooking the inherently multi-factor nature of real-world data. We introduce\nthe first standardized benchmark for evaluating multi-factor sequential\ndisentanglement across six diverse datasets spanning video, audio, and time\nseries. Our benchmark includes modular tools for dataset integration, model\ndevelopment, and evaluation metrics tailored to multi-factor analysis. We\nadditionally propose a post-hoc Latent Exploration Stage to automatically align\nlatent dimensions with semantic factors, and introduce a Koopman-inspired model\nthat achieves state-of-the-art results. Moreover, we show that Vision-Language\nModels can automate dataset annotation and serve as zero-shot disentanglement\nevaluators, removing the need for manual labels and human intervention.\nTogether, these contributions provide a robust and scalable foundation for\nadvancing multi-factor sequential disentanglement.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u591a\u56e0\u5b50\u5e8f\u5217\u89e3\u7f20\u7ed3\u6807\u51c6\u5316\u57fa\u51c6\uff0c\u5305\u542b\u516d\u4e2a\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u5de5\u5177\uff0c\u5e76\u5f15\u5165\u4e86\u81ea\u52a8\u6f5c\u5728\u63a2\u7d22\u9636\u6bb5\u548cKoopman\u542f\u53d1\u7684\u6a21\u578b\uff0c\u540c\u65f6\u5c55\u793a\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u6807\u6ce8\u548c\u96f6\u6837\u672c\u8bc4\u4f30\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u5305\u542b\u591a\u4e2a\u4ea4\u4e92\u7684\u8bed\u4e49\u56e0\u5b50\uff0c\u4f46\u5148\u524d\u5de5\u4f5c\u4e3b\u8981\u5173\u6ce8\u7b80\u5355\u7684\u53cc\u56e0\u5b50\u9759\u6001\u548c\u52a8\u6001\u8bbe\u7f6e\uff0c\u5ffd\u89c6\u4e86\u6570\u636e\u7684\u591a\u56e0\u5b50\u672c\u8d28\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u9002\u7528\u4e8e\u591a\u56e0\u5b50\u5e8f\u5217\u89e3\u7f20\u7ed3\u7684\u57fa\u51c6\u548c\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b\u516d\u4e2a\u6570\u636e\u96c6\u7684\u6807\u51c6\u5316\u57fa\u51c6\uff0c\u63d0\u51fa\u4e86\u540e\u9a8c\u6f5c\u5728\u63a2\u7d22\u9636\u6bb5\u81ea\u52a8\u5bf9\u9f50\u6f5c\u5728\u7ef4\u5ea6\u4e0e\u8bed\u4e49\u56e0\u5b50\uff0c\u8bbe\u8ba1\u4e86Koopman\u542f\u53d1\u7684\u6a21\u578b\uff0c\u5e76\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u81ea\u52a8\u6807\u6ce8\u548c\u96f6\u6837\u672c\u8bc4\u4f30\u3002", "result": "\u63d0\u51fa\u7684Koopman\u542f\u53d1\u6a21\u578b\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u6709\u6548\u81ea\u52a8\u5316\u6570\u636e\u96c6\u6807\u6ce8\u5e76\u4f5c\u4e3a\u96f6\u6837\u672c\u89e3\u7f20\u7ed3\u8bc4\u4f30\u5668\uff0c\u6d88\u9664\u4e86\u4eba\u5de5\u6807\u6ce8\u548c\u5e72\u9884\u7684\u9700\u6c42\u3002", "conclusion": "\u8fd9\u4e9b\u8d21\u732e\u4e3a\u63a8\u8fdb\u591a\u56e0\u5b50\u5e8f\u5217\u89e3\u7f20\u7ed3\u7814\u7a76\u63d0\u4f9b\u4e86\u7a33\u5065\u4e14\u53ef\u6269\u5c55\u7684\u57fa\u7840\uff0c\u89e3\u51b3\u4e86\u5148\u524d\u5de5\u4f5c\u4e2d\u5ffd\u89c6\u7684\u591a\u56e0\u5b50\u590d\u6742\u6027\u95ee\u9898\u3002"}}
{"id": "2510.17314", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17314", "abs": "https://arxiv.org/abs/2510.17314", "authors": ["Lipeng Xie", "Sen Huang", "Zhuo Zhang", "Anni Zou", "Yunpeng Zhai", "Dingchao Ren", "Kezun Zhang", "Haoyuan Hu", "Boyin Liu", "Haoran Chen", "Zhaoyang Liu", "Bolin Ding"], "title": "Auto-Rubric: Learning to Extract Generalizable Criteria for Reward Modeling", "comment": null, "summary": "Reward models are essential for aligning Large Language Models (LLMs) with\nhuman values, yet their development is hampered by costly preference datasets\nand poor interpretability. While recent rubric-based approaches offer\ntransparency, they often lack systematic quality control and optimization,\ncreating a trade-off between scalability and reliability. We address these\nlimitations with a novel, training-free framework built on a key assumption:\n\\textit{evaluation rubrics underlying human preferences exhibit significant\ngeneralization ability across diverse queries}, a property that enables\nremarkable data efficiency. Our two-stage approach first infers high-quality,\nquery-specific rubrics using a validation-guided\n\\textbf{Propose-Evaluate-Revise} pipeline. Second, it generalizes these\ngranular rubrics into a compact, non-redundant core set by maximizing an\n\\textbf{information-theoretic coding rate}. The final output is an\ninterpretable, hierarchical \"Theme-Tips\" rubric set. Extensive experiments\ndemonstrate the framework's exceptional data efficiency and performance.\nCritically, using just 70 preference pairs (1.5\\% of the source data), our\nmethod also empowers smaller models like Qwen3-8B to outperform specialized,\nfully-trained counterparts. This work pioneers a scalable, interpretable, and\ndata-efficient path for reward modeling.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u57fa\u4e8e\u8bc4\u4f30\u51c6\u5219\u7684\u5956\u52b1\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u65b9\u6cd5\u4ece\u5c11\u91cf\u504f\u597d\u6570\u636e\u4e2d\u63d0\u53d6\u9ad8\u8d28\u91cf\u3001\u53ef\u89e3\u91ca\u7684\u51c6\u5219\uff0c\u663e\u8457\u63d0\u9ad8\u6570\u636e\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u5956\u52b1\u6a21\u578b\u5f00\u53d1\u6210\u672c\u9ad8\u4e14\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u73b0\u6709\u57fa\u4e8e\u51c6\u5219\u7684\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u548c\u53ef\u9760\u6027\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1) \u901a\u8fc7\u9a8c\u8bc1\u5f15\u5bfc\u7684Propose-Evaluate-Revise\u6d41\u7a0b\u63a8\u65ad\u67e5\u8be2\u7279\u5b9a\u51c6\u5219\uff1b2) \u4f7f\u7528\u4fe1\u606f\u8bba\u7f16\u7801\u7387\u6700\u5927\u5316\u5c06\u7ec6\u7c92\u5ea6\u51c6\u5219\u6cdb\u5316\u4e3a\u7d27\u51d1\u7684\u6838\u5fc3\u96c6\uff0c\u5f62\u6210\u5c42\u6b21\u5316\u7684\"\u4e3b\u9898-\u63d0\u793a\"\u51c6\u5219\u96c6\u3002", "result": "\u4ec5\u4f7f\u752870\u4e2a\u504f\u597d\u5bf9\uff08\u6e90\u6570\u636e\u76841.5%\uff09\uff0c\u8be5\u65b9\u6cd5\u4f7fQwen3-8B\u7b49\u8f83\u5c0f\u6a21\u578b\u80fd\u591f\u8d85\u8d8a\u4e13\u95e8\u8bad\u7ec3\u7684\u5bf9\u7b49\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u6570\u636e\u6548\u7387\u548c\u6027\u80fd\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5956\u52b1\u5efa\u6a21\u5f00\u521b\u4e86\u4e00\u6761\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u4e14\u6570\u636e\u9ad8\u6548\u7684\u8def\u5f84\u3002"}}
{"id": "2510.17358", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17358", "abs": "https://arxiv.org/abs/2510.17358", "authors": ["Joachim Diederich"], "title": "Localist LLMs with Recruitment Learning", "comment": null, "summary": "We present a novel framework for training large language models with\ncontinuously adjustable internal representations that span the full spectrum\nfrom localist (interpretable, rule-based) to distributed (generalizable,\nefficient) encodings. The key innovations are (1) a locality dial, a tunable\nparameter that dynamically controls the degree of localization during both\ntraining and inference without requiring model retraining, (2) an\ninformation-theoretic recruitment mechanism that adaptively allocates semantic\nblocks as needed, eliminating the requirement for complete domain knowledge at\ninitialization, and (3) a hierarchical recruitment framework that extends\ncapacity allocation to entire specialized LLMs, enabling multi-granularity\narchitectural adaptation. This is achieved through group sparsity penalties on\nattention mechanisms, information-theoretic anchor design, dynamic rule\ninjection, and principled recruitment criteria based on penalized likelihood\nwith explicit units. We provide rigorous mathematical results establishing\nexplicit threshold conditions under which attention provably concentrates on\nsemantically relevant blocks at stationary points, with exact bounds on\nattention entropy and pointer fidelity. The hierarchical recruitment mechanism\nprovides convergence guarantees at both the block level (fine-grained,\nwithin-LLM) and the LLM level (coarse-grained, cross-domain), ensuring the\nsystem discovers semantic partitions that balance model complexity against data\nencoding efficiency. This framework enables practitioners to continuously\ninterpolate between interpretable and high-performance modes while adapting\narchitectural capacity at multiple granularities, supporting applications in\nregulated domains requiring both transparency and capability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u8c03\u8282\u8bed\u8a00\u6a21\u578b\u8868\u793a\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5c40\u90e8\u6027\u8c03\u8282\u53c2\u6570\u5b9e\u73b0\u4ece\u5c40\u90e8\u5316\u5230\u5206\u5e03\u5f0f\u7f16\u7801\u7684\u8fde\u7eed\u8c03\u6574\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u8bed\u8a00\u6a21\u578b\u5728\u53ef\u89e3\u91ca\u6027\u548c\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u9700\u8981\u900f\u660e\u6027\u548c\u80fd\u529b\u7684\u53d7\u76d1\u7ba1\u9886\u57df\u63d0\u4f9b\u7075\u6d3b\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u7ec4\u7a00\u758f\u60e9\u7f5a\u3001\u4fe1\u606f\u8bba\u951a\u70b9\u8bbe\u8ba1\u3001\u52a8\u6001\u89c4\u5219\u6ce8\u5165\u548c\u57fa\u4e8e\u60e9\u7f5a\u4f3c\u7136\u7684\u62db\u52df\u6807\u51c6\uff0c\u901a\u8fc7\u5c42\u6b21\u5316\u62db\u52df\u673a\u5236\u5b9e\u73b0\u591a\u7c92\u5ea6\u67b6\u6784\u9002\u5e94\u3002", "result": "\u5efa\u7acb\u4e86\u4e25\u683c\u7684\u6570\u5b66\u7ed3\u679c\uff0c\u8bc1\u660e\u5728\u7279\u5b9a\u9608\u503c\u6761\u4ef6\u4e0b\u6ce8\u610f\u529b\u4f1a\u96c6\u4e2d\u5728\u8bed\u4e49\u76f8\u5173\u5757\u4e0a\uff0c\u5e76\u63d0\u4f9b\u4e86\u6ce8\u610f\u529b\u71b5\u548c\u6307\u9488\u4fdd\u771f\u5ea6\u7684\u7cbe\u786e\u8fb9\u754c\u3002", "conclusion": "\u8be5\u6846\u67b6\u4f7f\u4ece\u4e1a\u8005\u80fd\u591f\u5728\u53ef\u89e3\u91ca\u548c\u9ad8\u6027\u80fd\u6a21\u5f0f\u4e4b\u95f4\u8fde\u7eed\u63d2\u503c\uff0c\u540c\u65f6\u5728\u591a\u4e2a\u7c92\u5ea6\u4e0a\u9002\u5e94\u67b6\u6784\u5bb9\u91cf\uff0c\u652f\u6301\u9700\u8981\u900f\u660e\u6027\u548c\u80fd\u529b\u7684\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2510.17378", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17378", "abs": "https://arxiv.org/abs/2510.17378", "authors": ["Wei Xu", "Xiaoyi Jiang", "Lixiang Xu", "Dechao Tang"], "title": "Model Metamers Reveal Invariances in Graph Neural Networks", "comment": null, "summary": "In recent years, deep neural networks have been extensively employed in\nperceptual systems to learn representations endowed with invariances, aiming to\nemulate the invariance mechanisms observed in the human brain. However, studies\nin the visual and auditory domains have confirmed that significant gaps remain\nbetween the invariance properties of artificial neural networks and those of\nhumans. To investigate the invariance behavior within graph neural networks\n(GNNs), we introduce a model ``metamers'' generation technique. By optimizing\ninput graphs such that their internal node activations match those of a\nreference graph, we obtain graphs that are equivalent in the model's\nrepresentation space, yet differ significantly in both structure and node\nfeatures. Our theoretical analysis focuses on two aspects: the local metamer\ndimension for a single node and the activation-induced volume change of the\nmetamer manifold. Utilizing this approach, we uncover extreme levels of\nrepresentational invariance across several classic GNN architectures. Although\ntargeted modifications to model architecture and training strategies can\npartially mitigate this excessive invariance, they fail to fundamentally bridge\nthe gap to human-like invariance. Finally, we quantify the deviation between\nmetamer graphs and their original counterparts, revealing unique failure modes\nof current GNNs and providing a complementary benchmark for model evaluation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u5f15\u5165\u56fe\u795e\u7ecf\u7f51\u7edc(GNN)\u7684\"\u5143\u50cf\"\u751f\u6210\u6280\u672f\uff0c\u63ed\u793a\u4e86GNN\u5b58\u5728\u8fc7\u5ea6\u8868\u793a\u4e0d\u53d8\u6027\u7684\u95ee\u9898\uff0c\u4e0e\u4eba\u7c7b\u5927\u8111\u7684\u4e0d\u53d8\u6027\u673a\u5236\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "motivation": "\u7814\u7a76\u56fe\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u4e0d\u53d8\u6027\u884c\u4e3a\uff0c\u63a2\u7d22\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u4e0e\u4eba\u7c7b\u5927\u8111\u5728\u4e0d\u53d8\u6027\u673a\u5236\u4e0a\u7684\u5dee\u5f02\uff0c\u7279\u522b\u662f\u5728\u56fe\u6570\u636e\u9886\u57df\u3002", "method": "\u901a\u8fc7\u4f18\u5316\u8f93\u5165\u56fe\u4f7f\u5176\u5185\u90e8\u8282\u70b9\u6fc0\u6d3b\u4e0e\u53c2\u8003\u56fe\u5339\u914d\uff0c\u751f\u6210\u5728\u6a21\u578b\u8868\u793a\u7a7a\u95f4\u4e2d\u7b49\u6548\u4f46\u5728\u7ed3\u6784\u548c\u8282\u70b9\u7279\u5f81\u4e0a\u663e\u8457\u4e0d\u540c\u7684\"\u5143\u50cf\"\u56fe\uff0c\u5e76\u5206\u6790\u5c40\u90e8\u5143\u50cf\u7ef4\u5ea6\u548c\u5143\u50cf\u6d41\u5f62\u7684\u6fc0\u6d3b\u8bf1\u5bfc\u4f53\u79ef\u53d8\u5316\u3002", "result": "\u53d1\u73b0\u591a\u4e2a\u7ecf\u5178GNN\u67b6\u6784\u5b58\u5728\u6781\u7aef\u6c34\u5e73\u7684\u8868\u793a\u4e0d\u53d8\u6027\uff0c\u867d\u7136\u901a\u8fc7\u6a21\u578b\u67b6\u6784\u548c\u8bad\u7ec3\u7b56\u7565\u7684\u9488\u5bf9\u6027\u4fee\u6539\u53ef\u4ee5\u90e8\u5206\u7f13\u89e3\u8fc7\u5ea6\u4e0d\u53d8\u6027\uff0c\u4f46\u65e0\u6cd5\u4ece\u6839\u672c\u4e0a\u5f25\u5408\u4e0e\u4eba\u7c7b\u4e0d\u53d8\u6027\u7684\u5dee\u8ddd\u3002", "conclusion": "\u91cf\u5316\u4e86\u5143\u50cf\u56fe\u4e0e\u5176\u539f\u59cb\u5bf9\u5e94\u56fe\u4e4b\u95f4\u7684\u504f\u5dee\uff0c\u63ed\u793a\u4e86\u5f53\u524dGNN\u7684\u72ec\u7279\u5931\u8d25\u6a21\u5f0f\uff0c\u5e76\u4e3a\u6a21\u578b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u8865\u5145\u57fa\u51c6\u3002"}}
{"id": "2510.17380", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17380", "abs": "https://arxiv.org/abs/2510.17380", "authors": ["Julen Cestero", "Carmine Delle Femine", "Kenji S. Muro", "Marco Quartulli", "Marcello Restelli"], "title": "Optimizing Energy Management of Smart Grid using Reinforcement Learning aided by Surrogate models built using Physics-informed Neural Networks", "comment": null, "summary": "Optimizing the energy management within a smart grids scenario presents\nsignificant challenges, primarily due to the complexity of real-world systems\nand the intricate interactions among various components. Reinforcement Learning\n(RL) is gaining prominence as a solution for addressing the challenges of\nOptimal Power Flow in smart grids. However, RL needs to iterate compulsively\nthroughout a given environment to obtain the optimal policy. This means\nobtaining samples from a, most likely, costly simulator, which can lead to a\nsample efficiency problem. In this work, we address this problem by\nsubstituting costly smart grid simulators with surrogate models built using\nPhisics-informed Neural Networks (PINNs), optimizing the RL policy training\nprocess by arriving to convergent results in a fraction of the time employed by\nthe original environment.", "AI": {"tldr": "\u4f7f\u7528\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc(PINNs)\u6784\u5efa\u66ff\u4ee3\u6a21\u578b\u6765\u66ff\u4ee3\u6602\u8d35\u7684\u667a\u80fd\u7535\u7f51\u6a21\u62df\u5668\uff0c\u4f18\u5316\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u9ad8\u6837\u672c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u667a\u80fd\u7535\u7f51\u4e2d\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u9700\u8981\u5927\u91cf\u6602\u8d35\u6a21\u62df\u5668\u91c7\u6837\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u6837\u672c\u6548\u7387\u548c\u8bad\u7ec3\u901f\u5ea6\u3002", "method": "\u7528\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc(PINNs)\u6784\u5efa\u667a\u80fd\u7535\u7f51\u6a21\u62df\u5668\u7684\u66ff\u4ee3\u6a21\u578b\uff0c\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u8bad\u7ec3\u3002", "result": "\u5728\u8fdc\u5c11\u4e8e\u539f\u59cb\u73af\u5883\u6240\u9700\u7684\u65f6\u95f4\u5185\u8fbe\u5230\u6536\u655b\u7ed3\u679c\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\u3002", "conclusion": "PINNs\u4f5c\u4e3a\u66ff\u4ee3\u6a21\u578b\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u5728\u667a\u80fd\u7535\u7f51\u4f18\u5316\u4e2d\u7684\u6837\u672c\u6548\u7387\u95ee\u9898\u3002"}}
{"id": "2510.17381", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17381", "abs": "https://arxiv.org/abs/2510.17381", "authors": ["Achref Jaziri", "Martin Rogmann", "Martin Mundt", "Visvanathan Ramesh"], "title": "Beyond Binary Out-of-Distribution Detection: Characterizing Distributional Shifts with Multi-Statistic Diffusion Trajectories", "comment": "11 Pages, 6 Figures", "summary": "Detecting out-of-distribution (OOD) data is critical for machine learning, be\nit for safety reasons or to enable open-ended learning. However, beyond mere\ndetection, choosing an appropriate course of action typically hinges on the\ntype of OOD data encountered. Unfortunately, the latter is generally not\ndistinguished in practice, as modern OOD detection methods collapse\ndistributional shifts into single scalar outlier scores. This work argues that\nscalar-based methods are thus insufficient for OOD data to be properly\ncontextualized and prospectively exploited, a limitation we overcome with the\nintroduction of DISC: Diffusion-based Statistical Characterization. DISC\nleverages the iterative denoising process of diffusion models to extract a\nrich, multi-dimensional feature vector that captures statistical discrepancies\nacross multiple noise levels. Extensive experiments on image and tabular\nbenchmarks show that DISC matches or surpasses state-of-the-art detectors for\nOOD detection and, crucially, also classifies OOD type, a capability largely\nabsent from prior work. As such, our work enables a shift from simple binary\nOOD detection to a more granular detection.", "AI": {"tldr": "\u63d0\u51faDISC\u65b9\u6cd5\uff0c\u4f7f\u7528\u6269\u6563\u6a21\u578b\u7684\u591a\u7ef4\u7279\u5f81\u5411\u91cf\u8fdb\u884cOOD\u68c0\u6d4b\u548c\u5206\u7c7b\uff0c\u8d85\u8d8a\u4f20\u7edf\u6807\u91cf\u65b9\u6cd5", "motivation": "\u4f20\u7edfOOD\u68c0\u6d4b\u65b9\u6cd5\u4ec5\u8f93\u51fa\u6807\u91cf\u5f02\u5e38\u5206\u6570\uff0c\u65e0\u6cd5\u533a\u5206\u4e0d\u540c\u7c7b\u578b\u7684OOD\u6570\u636e\uff0c\u9650\u5236\u4e86\u540e\u7eed\u5904\u7406\u548c\u5e94\u7528", "method": "\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u8fed\u4ee3\u53bb\u566a\u8fc7\u7a0b\uff0c\u63d0\u53d6\u8de8\u591a\u4e2a\u566a\u58f0\u7ea7\u522b\u7684\u7edf\u8ba1\u5dee\u5f02\u7279\u5f81\u5411\u91cf", "result": "\u5728\u56fe\u50cf\u548c\u8868\u683c\u6570\u636e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDISC\u5728OOD\u68c0\u6d4b\u6027\u80fd\u4e0a\u8fbe\u5230\u6216\u8d85\u8d8aSOTA\uff0c\u5e76\u5177\u5907OOD\u7c7b\u578b\u5206\u7c7b\u80fd\u529b", "conclusion": "DISC\u5b9e\u73b0\u4e86\u4ece\u7b80\u5355\u4e8c\u5143OOD\u68c0\u6d4b\u5230\u66f4\u7ec6\u7c92\u5ea6\u68c0\u6d4b\u7684\u8f6c\u53d8\uff0c\u4e3aOOD\u6570\u636e\u7684\u4e0a\u4e0b\u6587\u7406\u89e3\u548c\u5229\u7528\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84"}}
{"id": "2510.17385", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17385", "abs": "https://arxiv.org/abs/2510.17385", "authors": ["Pengxiang Cai", "Zihao Gao", "Jintai Chen"], "title": "TabR1: Taming GRPO for tabular reasoning LLMs", "comment": null, "summary": "Tabular prediction has traditionally relied on gradient-boosted decision\ntrees and specialized deep learning models, which excel within tasks but\nprovide limited interpretability and weak transfer across tables. Reasoning\nlarge language models (LLMs) promise cross-task adaptability with trans- parent\nreasoning traces, yet their potential has not been fully realized for tabular\ndata. This paper presents TabR1, the first reasoning LLM for tabular prediction\nwith multi-step reasoning. At its core is Permutation Relative Policy\nOptimization (PRPO), a simple yet efficient reinforcement learning method that\nencodes column-permutation invariance as a structural prior. By construct- ing\nmultiple label-preserving permutations per sample and estimating advantages\nboth within and across permutations, PRPO transforms sparse rewards into dense\nlearning signals and improves generalization. With limited supervision, PRPO\nactivates the reasoning ability of LLMs for tabular prediction, enhancing\nfew-shot and zero-shot performance as well as interpretability. Comprehensive\nexperiments demonstrate that TabR1 achieves performance comparable to strong\nbaselines under full-supervision fine-tuning. In the zero-shot setting, TabR1\napproaches the performance of strong baselines under the 32-shot setting.\nMoreover, TabR1 (8B) substantially outperforms much larger LLMs across various\ntasks, achieving up to 53.17% improvement over DeepSeek-R1 (685B).", "AI": {"tldr": "TabR1\u662f\u9996\u4e2a\u7528\u4e8e\u8868\u683c\u9884\u6d4b\u7684\u63a8\u7406\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7PRPO\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6fc0\u6d3bLLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5728\u5c11\u6837\u672c\u548c\u96f6\u6837\u672c\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u751a\u81f3\u80fd\u8d85\u8d8a\u66f4\u5927\u89c4\u6a21\u7684LLM\u3002", "motivation": "\u4f20\u7edf\u8868\u683c\u9884\u6d4b\u65b9\u6cd5\uff08\u5982\u68af\u5ea6\u63d0\u5347\u6811\u548c\u4e13\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff09\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u548c\u8de8\u4efb\u52a1\u8fc1\u79fb\u80fd\u529b\uff0c\u800c\u63a8\u7406LLM\u867d\u7136\u5177\u6709\u900f\u660e\u63a8\u7406\u8f68\u8ff9\u548c\u8de8\u4efb\u52a1\u9002\u5e94\u6027\uff0c\u4f46\u5728\u8868\u683c\u6570\u636e\u4e0a\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u53d1\u6325\u3002", "method": "\u63d0\u51faTabR1\u6a21\u578b\uff0c\u6838\u5fc3\u662fPRPO\uff08\u6392\u5217\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff09\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u9020\u6807\u7b7e\u4fdd\u6301\u7684\u5217\u6392\u5217\uff0c\u5728\u6392\u5217\u5185\u548c\u6392\u5217\u95f4\u4f30\u8ba1\u4f18\u52bf\u51fd\u6570\uff0c\u5c06\u7a00\u758f\u5956\u52b1\u8f6c\u5316\u4e3a\u5bc6\u96c6\u5b66\u4e60\u4fe1\u53f7\u3002", "result": "TabR1\u5728\u5168\u76d1\u7763\u5fae\u8c03\u4e0b\u8fbe\u5230\u4e0e\u5f3a\u57fa\u7ebf\u76f8\u5f53\u7684\u6027\u80fd\uff1b\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u63a5\u8fd132\u6837\u672c\u8bbe\u7f6e\u7684\u5f3a\u57fa\u7ebf\u6027\u80fd\uff1bTabR1\uff088B\uff09\u5728\u5404\u79cd\u4efb\u52a1\u4e0a\u663e\u8457\u8d85\u8d8a\u66f4\u5927\u7684LLM\uff0c\u76f8\u6bd4DeepSeek-R1\uff08685B\uff09\u63d0\u5347\u8fbe53.17%\u3002", "conclusion": "PRPO\u65b9\u6cd5\u6210\u529f\u6fc0\u6d3b\u4e86LLM\u5728\u8868\u683c\u9884\u6d4b\u4e2d\u7684\u63a8\u7406\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c11\u6837\u672c\u548c\u96f6\u6837\u672c\u6027\u80fd\u4ee5\u53ca\u53ef\u89e3\u91ca\u6027\uff0c\u8bc1\u660e\u4e86\u63a8\u7406LLM\u5728\u8868\u683c\u6570\u636e\u4e0a\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2510.17391", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17391", "abs": "https://arxiv.org/abs/2510.17391", "authors": ["Jongmin Lee", "Ernest K. Ryu"], "title": "Finite-Time Bounds for Average-Reward Fitted Q-Iteration", "comment": null, "summary": "Although there is an extensive body of work characterizing the sample\ncomplexity of discounted-return offline RL with function approximations, prior\nwork on the average-reward setting has received significantly less attention,\nand existing approaches rely on restrictive assumptions, such as ergodicity or\nlinearity of the MDP. In this work, we establish the first sample complexity\nresults for average-reward offline RL with function approximation for weakly\ncommunicating MDPs, a much milder assumption. To this end, we introduce\nAnchored Fitted Q-Iteration, which combines the standard Fitted Q-Iteration\nwith an anchor mechanism. We show that the anchor, which can be interpreted as\na form of weight decay, is crucial for enabling finite-time analysis in the\naverage-reward setting. We also extend our finite-time analysis to the setup\nwhere the dataset is generated from a single-trajectory rather than IID\ntransitions, again leveraging the anchor mechanism.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u9488\u5bf9\u5f31\u901a\u4fe1MDP\u7684\u5e73\u5747\u5956\u52b1\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u6837\u672c\u590d\u6742\u5ea6\u5206\u6790\uff0c\u5f15\u5165\u4e86\u951a\u5b9a\u62df\u5408Q\u8fed\u4ee3\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u6807\u51c6\u62df\u5408Q\u8fed\u4ee3\u548c\u951a\u5b9a\u673a\u5236\uff0c\u5e76\u5728\u5355\u8f68\u8ff9\u6570\u636e\u96c6\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u4e86\u6269\u5c55\u5206\u6790\u3002", "motivation": "\u73b0\u6709\u5e73\u5747\u5956\u52b1\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u4e25\u683c\u5047\u8bbe\uff08\u5982\u904d\u5386\u6027\u6216\u7ebf\u6027MDP\uff09\uff0c\u800c\u5f31\u901a\u4fe1MDP\u5047\u8bbe\u66f4\u4e3a\u5bbd\u677e\uff0c\u9700\u8981\u5f00\u53d1\u65b0\u7684\u7406\u8bba\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u951a\u5b9a\u62df\u5408Q\u8fed\u4ee3\u65b9\u6cd5\uff0c\u5c06\u6807\u51c6\u62df\u5408Q\u8fed\u4ee3\u4e0e\u951a\u5b9a\u673a\u5236\u76f8\u7ed3\u5408\uff0c\u951a\u5b9a\u673a\u5236\u53ef\u89e3\u91ca\u4e3a\u4e00\u79cd\u6743\u91cd\u8870\u51cf\u5f62\u5f0f\uff0c\u5bf9\u4e8e\u5b9e\u73b0\u5e73\u5747\u5956\u52b1\u8bbe\u7f6e\u7684\u6709\u9650\u65f6\u95f4\u5206\u6790\u81f3\u5173\u91cd\u8981\u3002", "result": "\u5efa\u7acb\u4e86\u5f31\u901a\u4fe1MDP\u4e0b\u5e73\u5747\u5956\u52b1\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u9996\u4e2a\u6837\u672c\u590d\u6742\u5ea6\u7ed3\u679c\uff0c\u5e76\u8bc1\u660e\u4e86\u951a\u5b9a\u673a\u5236\u5728\u5355\u8f68\u8ff9\u6570\u636e\u96c6\u8bbe\u7f6e\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u951a\u5b9a\u673a\u5236\u662f\u5b9e\u73b0\u5e73\u5747\u5956\u52b1\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u6709\u9650\u65f6\u95f4\u5206\u6790\u7684\u5173\u952e\u6280\u672f\uff0c\u8be5\u65b9\u6cd5\u5728\u66f4\u5bbd\u677e\u7684\u5f31\u901a\u4fe1MDP\u5047\u8bbe\u4e0b\u53d6\u5f97\u4e86\u7406\u8bba\u7a81\u7834\uff0c\u5e76\u9002\u7528\u4e8e\u5355\u8f68\u8ff9\u6570\u636e\u751f\u6210\u573a\u666f\u3002"}}
{"id": "2510.17394", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17394", "abs": "https://arxiv.org/abs/2510.17394", "authors": ["Alejandro Guerra-Manzanares", "Farah E. Shamout"], "title": "MILES: Modality-Informed Learning Rate Scheduler for Balancing Multimodal Learning", "comment": "Accepted and presented at the 2025 International Joint Conference on\n  Neural Networks (IJCNN'25). The paper was awarded an honorable mention (best\n  4 papers)", "summary": "The aim of multimodal neural networks is to combine diverse data sources,\nreferred to as modalities, to achieve enhanced performance compared to relying\non a single modality. However, training of multimodal networks is typically\nhindered by modality overfitting, where the network relies excessively on one\nof the available modalities. This often yields sub-optimal performance,\nhindering the potential of multimodal learning and resulting in marginal\nimprovements relative to unimodal models. In this work, we present the\nModality-Informed Learning ratE Scheduler (MILES) for training multimodal joint\nfusion models in a balanced manner. MILES leverages the differences in\nmodality-wise conditional utilization rates during training to effectively\nbalance multimodal learning. The learning rate is dynamically adjusted during\ntraining to balance the speed of learning from each modality by the multimodal\nmodel, aiming for enhanced performance in both multimodal and unimodal\npredictions. We extensively evaluate MILES on four multimodal joint fusion\ntasks and compare its performance to seven state-of-the-art baselines. Our\nresults show that MILES outperforms all baselines across all tasks and fusion\nmethods considered in our study, effectively balancing modality usage during\ntraining. This results in improved multimodal performance and stronger modality\nencoders, which can be leveraged when dealing with unimodal samples or absent\nmodalities. Overall, our work highlights the impact of balancing multimodal\nlearning on improving model performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86MILES\uff08Modality-Informed Learning ratE Scheduler\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5b66\u4e60\u7387\u6765\u5e73\u8861\u591a\u6a21\u6001\u5b66\u4e60\uff0c\u89e3\u51b3\u6a21\u6001\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u63d0\u5347\u591a\u6a21\u6001\u548c\u5355\u6a21\u6001\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u4e2d\u5b58\u5728\u6a21\u6001\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u7f51\u7edc\u8fc7\u5ea6\u4f9d\u8d56\u67d0\u4e00\u6a21\u6001\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\uff0c\u9650\u5236\u4e86\u591a\u6a21\u6001\u5b66\u4e60\u7684\u6f5c\u529b\u3002", "method": "MILES\u5229\u7528\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6a21\u6001\u6761\u4ef6\u5229\u7528\u7387\u5dee\u5f02\uff0c\u52a8\u6001\u8c03\u6574\u5b66\u4e60\u7387\u6765\u5e73\u8861\u5404\u6a21\u6001\u7684\u5b66\u4e60\u901f\u5ea6\u3002", "result": "\u5728\u56db\u4e2a\u591a\u6a21\u6001\u8054\u5408\u878d\u5408\u4efb\u52a1\u4e0a\u8bc4\u4f30\uff0cMILES\u4f18\u4e8e\u4e03\u4e2a\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u6240\u6709\u4efb\u52a1\u548c\u878d\u5408\u65b9\u6cd5\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u6709\u6548\u5e73\u8861\u6a21\u6001\u4f7f\u7528\u3002", "conclusion": "\u5e73\u8861\u591a\u6a21\u6001\u5b66\u4e60\u5bf9\u63d0\u5347\u6a21\u578b\u6027\u80fd\u5177\u6709\u91cd\u8981\u5f71\u54cd\uff0cMILES\u65b9\u6cd5\u80fd\u6539\u5584\u591a\u6a21\u6001\u6027\u80fd\u5e76\u4ea7\u751f\u66f4\u5f3a\u7684\u6a21\u6001\u7f16\u7801\u5668\u3002"}}
{"id": "2510.17406", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.17406", "abs": "https://arxiv.org/abs/2510.17406", "authors": ["Tiezhi Wang", "Wilhelm Haverkamp", "Nils Strodthoff"], "title": "S4ECG: Exploring the impact of long-range interactions for arrhythmia prediction", "comment": null, "summary": "The electrocardiogram (ECG) exemplifies biosignal-based time series with\ncontinuous, temporally ordered structure reflecting cardiac physiological and\npathophysiological dynamics. Detailed analysis of these dynamics has proven\nchallenging, as conventional methods capture either global trends or local\nwaveform features but rarely their simultaneous interplay at high temporal\nresolution. To bridge global and local signal analysis, we introduce S4ECG, a\nnovel deep learning architecture leveraging structured state space models for\nmulti-epoch arrhythmia classification. Our joint multi-epoch predictions\nsignificantly outperform single-epoch approaches by 1.0-11.6% in macro-AUROC,\nwith atrial fibrillation specificity improving from 0.718-0.979 to 0.967-0.998,\ndemonstrating superior performance in-distribution and enhanced\nout-of-distribution robustness. Systematic investigation reveals optimal\ntemporal dependency windows spanning 10-20 minutes for peak performance. This\nwork contributes to a paradigm shift toward temporally-aware arrhythmia\ndetection algorithms, opening new possibilities for ECG interpretation, in\nparticular for complex arrhythmias like atrial fibrillation and atrial flutter.", "AI": {"tldr": "S4ECG\u662f\u4e00\u79cd\u57fa\u4e8e\u7ed3\u6784\u5316\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u7528\u4e8e\u591a\u65f6\u6bb5\u5fc3\u5f8b\u5931\u5e38\u5206\u7c7b\uff0c\u901a\u8fc7\u8054\u5408\u591a\u65f6\u6bb5\u9884\u6d4b\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u5728\u623f\u98a4\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u5206\u5e03\u5185\u6027\u80fd\u548c\u589e\u5f3a\u7684\u5206\u5e03\u5916\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edfECG\u5206\u6790\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u6355\u6349\u5168\u5c40\u8d8b\u52bf\u548c\u5c40\u90e8\u6ce2\u5f62\u7279\u5f81\u7684\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u4ea4\u4e92\uff0c\u9700\u8981\u6865\u63a5\u5168\u5c40\u548c\u5c40\u90e8\u4fe1\u53f7\u5206\u6790\u3002", "method": "\u5f15\u5165S4ECG\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u5229\u7528\u7ed3\u6784\u5316\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u8fdb\u884c\u591a\u65f6\u6bb5\u5fc3\u5f8b\u5931\u5e38\u5206\u7c7b\uff0c\u91c7\u7528\u8054\u5408\u591a\u65f6\u6bb5\u9884\u6d4b\u65b9\u6cd5\u3002", "result": "\u591a\u65f6\u6bb5\u65b9\u6cd5\u6bd4\u5355\u65f6\u6bb5\u65b9\u6cd5\u5728\u5b8f\u89c2AUROC\u4e0a\u63d0\u53471.0-11.6%\uff0c\u623f\u98a4\u7279\u5f02\u6027\u4ece0.718-0.979\u63d0\u5347\u52300.967-0.998\uff0c\u6700\u4f73\u65f6\u95f4\u4f9d\u8d56\u7a97\u53e3\u4e3a10-20\u5206\u949f\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63a8\u52a8\u4e86\u5fc3\u5f8b\u5931\u5e38\u68c0\u6d4b\u7b97\u6cd5\u5411\u65f6\u95f4\u611f\u77e5\u8303\u5f0f\u7684\u8f6c\u53d8\uff0c\u4e3aECG\u89e3\u91ca\u7279\u522b\u662f\u590d\u6742\u5fc3\u5f8b\u5931\u5e38\u5982\u623f\u98a4\u548c\u623f\u6251\u5f00\u8f9f\u4e86\u65b0\u53ef\u80fd\u6027\u3002"}}
{"id": "2510.17414", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17414", "abs": "https://arxiv.org/abs/2510.17414", "authors": ["Hequn Li", "Zhongwei Deng", "Chunlin Jiang", "Yvxin He andZhansheng Ning"], "title": "A Conditional Diffusion Model for Probabilistic Prediction of Battery Capacity Degradation", "comment": null, "summary": "Accurate prediction of lithium-ion battery capacity and its associated\nuncertainty is essential for reliable battery management but remains\nchallenging due to the stochastic nature of aging. This paper presents a novel\nmethod, termed the Condition Diffusion U-Net with Attention (CDUA), which\nintegrates feature engineering and deep learning to address this challenge. The\nproposed approach employs a diffusion-based generative model for time-series\nforecasting and incorporates attention mechanisms to enhance predictive\nperformance. Battery capacity is first derived from real-world vehicle\noperation data. The most relevant features are then identified using the\nPearson correlation coefficient and the XGBoost algorithm. These features are\nused to train the CDUA model, which comprises two core components: (1) a\ncontextual U-Net with self-attention to capture complex temporal dependencies,\nand (2) a denoising network to reconstruct accurate capacity values from noisy\nobservations. Experimental validation on the real-world vehicle data\ndemonstrates that the proposed CDUA model achieves a relative Mean Absolute\nError (MAE) of 0.94% and a relative Root Mean Square Error (RMSE) of 1.14%,\nwith a narrow 95% confidence interval of 3.74% in relative width. These results\nconfirm that CDUA provides both accurate capacity estimation and reliable\nuncertainty quantification. Comparative experiments further verify its\nrobustness and superior performance over existing mainstream approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCDUA\u7684\u65b0\u65b9\u6cd5\uff0c\u7ed3\u5408\u7279\u5f81\u5de5\u7a0b\u548c\u6df1\u5ea6\u5b66\u4e60\uff0c\u7528\u4e8e\u51c6\u786e\u9884\u6d4b\u9502\u79bb\u5b50\u7535\u6c60\u5bb9\u91cf\u53ca\u5176\u4e0d\u786e\u5b9a\u6027\uff0c\u5728\u771f\u5b9e\u8f66\u8f86\u6570\u636e\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002", "motivation": "\u9502\u79bb\u5b50\u7535\u6c60\u5bb9\u91cf\u53ca\u5176\u4e0d\u786e\u5b9a\u6027\u7684\u51c6\u786e\u9884\u6d4b\u5bf9\u4e8e\u53ef\u9760\u7684\u7535\u6c60\u7ba1\u7406\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u8001\u5316\u7684\u968f\u673a\u6027\uff0c\u8fd9\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u6269\u6563\u7684\u751f\u6210\u6a21\u578b\u8fdb\u884c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u7ed3\u5408\u6ce8\u610f\u529b\u673a\u5236\u3002\u9996\u5148\u4ece\u771f\u5b9e\u8f66\u8f86\u8fd0\u884c\u6570\u636e\u63a8\u5bfc\u7535\u6c60\u5bb9\u91cf\uff0c\u7136\u540e\u4f7f\u7528\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\u548cXGBoost\u7b97\u6cd5\u8bc6\u522b\u6700\u76f8\u5173\u7279\u5f81\uff0c\u6700\u540e\u8bad\u7ec3CDUA\u6a21\u578b\u3002", "result": "\u5728\u771f\u5b9e\u8f66\u8f86\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u663e\u793a\uff0cCDUA\u6a21\u578b\u5b9e\u73b0\u4e860.94%\u7684\u76f8\u5bf9MAE\u548c1.14%\u7684\u76f8\u5bf9RMSE\uff0c95%\u7f6e\u4fe1\u533a\u95f4\u76f8\u5bf9\u5bbd\u5ea6\u4e3a3.74%\u3002", "conclusion": "CDUA\u63d0\u4f9b\u4e86\u51c6\u786e\u7684\u5bb9\u91cf\u4f30\u8ba1\u548c\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u6bd4\u8f83\u5b9e\u9a8c\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u5176\u76f8\u5bf9\u4e8e\u73b0\u6709\u4e3b\u6d41\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u548c\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2510.17421", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17421", "abs": "https://arxiv.org/abs/2510.17421", "authors": ["Duo Su", "Huyu Wu", "Huanran Chen", "Yiming Shi", "Yuzhu Wang", "Xi Ye", "Jun Zhu"], "title": "Diffusion Models as Dataset Distillation Priors", "comment": null, "summary": "Dataset distillation aims to synthesize compact yet informative datasets from\nlarge ones. A significant challenge in this field is achieving a trifecta of\ndiversity, generalization, and representativeness in a single distilled\ndataset. Although recent generative dataset distillation methods adopt powerful\ndiffusion models as their foundation models, the inherent representativeness\nprior in diffusion models is overlooked. Consequently, these approaches often\nnecessitate the integration of external constraints to enhance data quality. To\naddress this, we propose Diffusion As Priors (DAP), which formalizes\nrepresentativeness by quantifying the similarity between synthetic and real\ndata in feature space using a Mercer kernel. We then introduce this prior as\nguidance to steer the reverse diffusion process, enhancing the\nrepresentativeness of distilled samples without any retraining. Extensive\nexperiments on large-scale datasets, such as ImageNet-1K and its subsets,\ndemonstrate that DAP outperforms state-of-the-art methods in generating\nhigh-fidelity datasets while achieving superior cross-architecture\ngeneralization. Our work not only establishes a theoretical connection between\ndiffusion priors and the objectives of dataset distillation but also provides a\npractical, training-free framework for improving the quality of the distilled\ndataset.", "AI": {"tldr": "\u63d0\u51fa\u4e86DAP\u65b9\u6cd5\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u5185\u5728\u4ee3\u8868\u6027\u5148\u9a8c\u6765\u6307\u5bfc\u6570\u636e\u96c6\u84b8\u998f\u8fc7\u7a0b\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u63d0\u5347\u5408\u6210\u6570\u636e\u7684\u4ee3\u8868\u6027\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u751f\u6210\u5f0f\u6570\u636e\u96c6\u84b8\u998f\u65b9\u6cd5\u867d\u7136\u91c7\u7528\u6269\u6563\u6a21\u578b\uff0c\u4f46\u5ffd\u89c6\u4e86\u6269\u6563\u6a21\u578b\u56fa\u6709\u7684\u4ee3\u8868\u6027\u5148\u9a8c\uff0c\u5f80\u5f80\u9700\u8981\u989d\u5916\u7ea6\u675f\u6765\u63d0\u5347\u6570\u636e\u8d28\u91cf\u3002", "method": "\u901a\u8fc7Mercer\u6838\u91cf\u5316\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u5728\u7279\u5f81\u7a7a\u95f4\u7684\u76f8\u4f3c\u5ea6\uff0c\u5c06\u8be5\u5148\u9a8c\u4f5c\u4e3a\u6307\u5bfc\u5f15\u5165\u53cd\u5411\u6269\u6563\u8fc7\u7a0b\u3002", "result": "\u5728ImageNet-1K\u7b49\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDAP\u5728\u751f\u6210\u9ad8\u4fdd\u771f\u6570\u636e\u96c6\u548c\u8de8\u67b6\u6784\u6cdb\u5316\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u5efa\u7acb\u4e86\u6269\u6563\u5148\u9a8c\u4e0e\u6570\u636e\u96c6\u84b8\u998f\u76ee\u6807\u7684\u7406\u8bba\u8054\u7cfb\uff0c\u63d0\u4f9b\u4e86\u65e0\u9700\u8bad\u7ec3\u7684\u5b9e\u7528\u6846\u67b6\u6765\u63d0\u5347\u84b8\u998f\u6570\u636e\u96c6\u8d28\u91cf\u3002"}}
{"id": "2510.17457", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17457", "abs": "https://arxiv.org/abs/2510.17457", "authors": ["Li Sun", "Zhenhao Huang", "Ming Zhang", "Philip S. Yu"], "title": "Deeper with Riemannian Geometry: Overcoming Oversmoothing and Oversquashing for Graph Foundation Models", "comment": "Accept by NeurIPS 25", "summary": "Message Passing Neural Networks (MPNNs) is the building block of graph\nfoundation models, but fundamentally suffer from oversmoothing and\noversquashing. There has recently been a surge of interest in fixing both\nissues. Existing efforts primarily adopt global approaches, which may be\nbeneficial in some regions but detrimental in others, ultimately leading to the\nsuboptimal expressiveness. In this paper, we begin by revisiting oversquashing\nthrough a global measure -- spectral gap $\\lambda$ -- and prove that the\nincrease of $\\lambda$ leads to gradient vanishing with respect to the input\nfeatures, thereby undermining the effectiveness of message passing. Motivated\nby such theoretical insights, we propose a \\textbf{local} approach that\nadaptively adjusts message passing based on local structures. To achieve this,\nwe connect local Riemannian geometry with MPNNs, and establish a novel\nnonhomogeneous boundary condition to address both oversquashing and\noversmoothing. Building on the Robin condition, we design a GBN network with\nlocal bottleneck adjustment, coupled with theoretical guarantees. Extensive\nexperiments on homophilic and heterophilic graphs show the expressiveness of\nGBN. Furthermore, GBN does not exhibit performance degradation even when the\nnetwork depth exceeds $256$ layers.", "AI": {"tldr": "\u63d0\u51faGBN\u7f51\u7edc\uff0c\u901a\u8fc7\u5c40\u90e8\u74f6\u9888\u8c03\u6574\u89e3\u51b3MPNN\u4e2d\u7684\u8fc7\u5e73\u6ed1\u548c\u8fc7\u6324\u538b\u95ee\u9898\uff0c\u5728\u6df1\u5ea6\u8d85\u8fc7256\u5c42\u65f6\u4ecd\u4fdd\u6301\u6027\u80fd", "motivation": "MPNN\u4f5c\u4e3a\u56fe\u57fa\u7840\u6a21\u578b\u7684\u6784\u5efa\u5757\uff0c\u5b58\u5728\u8fc7\u5e73\u6ed1\u548c\u8fc7\u6324\u538b\u95ee\u9898\u3002\u73b0\u6709\u5168\u5c40\u65b9\u6cd5\u5728\u67d0\u4e9b\u533a\u57df\u6709\u76ca\u4f46\u5728\u5176\u4ed6\u533a\u57df\u6709\u5bb3\uff0c\u5bfc\u81f4\u8868\u8fbe\u80fd\u529b\u6b21\u4f18\u3002\u901a\u8fc7\u8c31\u95f4\u9699\u5206\u6790\u53d1\u73b0\u589e\u52a0\u03bb\u4f1a\u5bfc\u81f4\u68af\u5ea6\u6d88\u5931\uff0c\u5f71\u54cd\u6d88\u606f\u4f20\u9012\u6548\u679c", "method": "\u8fde\u63a5\u5c40\u90e8\u9ece\u66fc\u51e0\u4f55\u4e0eMPNN\uff0c\u5efa\u7acb\u975e\u9f50\u6b21\u8fb9\u754c\u6761\u4ef6\uff0c\u57fa\u4e8eRobin\u6761\u4ef6\u8bbe\u8ba1\u5177\u6709\u5c40\u90e8\u74f6\u9888\u8c03\u6574\u7684GBN\u7f51\u7edc", "result": "\u5728\u540c\u8d28\u6027\u548c\u5f02\u8d28\u6027\u56fe\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u663e\u793aGBN\u5177\u6709\u5f3a\u5927\u8868\u8fbe\u80fd\u529b\uff0c\u5373\u4f7f\u5728\u7f51\u7edc\u6df1\u5ea6\u8d85\u8fc7256\u5c42\u65f6\u4e5f\u4e0d\u51fa\u73b0\u6027\u80fd\u4e0b\u964d", "conclusion": "\u5c40\u90e8\u65b9\u6cd5\u6bd4\u5168\u5c40\u65b9\u6cd5\u66f4\u6709\u6548\u5730\u89e3\u51b3\u8fc7\u5e73\u6ed1\u548c\u8fc7\u6324\u538b\u95ee\u9898\uff0cGBN\u7f51\u7edc\u5728\u6df1\u5ea6\u56fe\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u8272"}}
{"id": "2510.17458", "categories": ["cs.LG", "physics.geo-ph"], "pdf": "https://arxiv.org/pdf/2510.17458", "abs": "https://arxiv.org/abs/2510.17458", "authors": ["Ayrat Abdullin", "Denis Anikiev", "Umair bin Waheed"], "title": "Explainable AI for microseismic event detection", "comment": "Submitted to Artificial Intelligence in Geosciences", "summary": "Deep neural networks like PhaseNet show high accuracy in detecting\nmicroseismic events, but their black-box nature is a concern in critical\napplications. We apply explainable AI (XAI) techniques, such as\nGradient-weighted Class Activation Mapping (Grad-CAM) and Shapley Additive\nExplanations (SHAP), to interpret the PhaseNet model's decisions and improve\nits reliability. Grad-CAM highlights that the network's attention aligns with\nP- and S-wave arrivals. SHAP values quantify feature contributions, confirming\nthat vertical-component amplitudes drive P-phase picks while horizontal\ncomponents dominate S-phase picks, consistent with geophysical principles.\nLeveraging these insights, we introduce a SHAP-gated inference scheme that\ncombines the model's output with an explanation-based metric to reduce errors.\nOn a test set of 9,000 waveforms, the SHAP-gated model achieved an F1-score of\n0.98 (precision 0.99, recall 0.97), outperforming the baseline PhaseNet\n(F1-score 0.97) and demonstrating enhanced robustness to noise. These results\nshow that XAI can not only interpret deep learning models but also directly\nenhance their performance, providing a template for building trust in automated\nseismic detectors.", "AI": {"tldr": "\u5e94\u7528\u53ef\u89e3\u91caAI\u6280\u672f\uff08Grad-CAM\u548cSHAP\uff09\u89e3\u91caPhaseNet\u5730\u9707\u68c0\u6d4b\u6a21\u578b\u7684\u51b3\u7b56\uff0c\u5e76\u57fa\u4e8eSHAP\u503c\u5f00\u53d1\u4e86\u95e8\u63a7\u63a8\u7406\u65b9\u6848\uff0c\u57289000\u4e2a\u6ce2\u5f62\u6d4b\u8bd5\u96c6\u4e0aF1\u5206\u6570\u8fbe\u52300.98\uff0c\u4f18\u4e8e\u57fa\u51c6\u6a21\u578b\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5982PhaseNet\u5728\u5730\u9707\u4e8b\u4ef6\u68c0\u6d4b\u4e2d\u7cbe\u5ea6\u9ad8\uff0c\u4f46\u5176\u9ed1\u76d2\u7279\u6027\u5728\u5173\u952e\u5e94\u7528\u4e2d\u5b58\u5728\u53ef\u9760\u6027\u95ee\u9898\uff0c\u9700\u8981\u53ef\u89e3\u91ca\u6027\u6280\u672f\u6765\u589e\u5f3a\u6a21\u578b\u53ef\u4fe1\u5ea6\u3002", "method": "\u4f7f\u7528Grad-CAM\u548cSHAP\u89e3\u91caPhaseNet\u6a21\u578b\u51b3\u7b56\uff0c\u5f00\u53d1SHAP\u95e8\u63a7\u63a8\u7406\u65b9\u6848\uff0c\u7ed3\u5408\u6a21\u578b\u8f93\u51fa\u548c\u57fa\u4e8e\u89e3\u91ca\u7684\u6307\u6807\u6765\u51cf\u5c11\u9519\u8bef\u3002", "result": "SHAP\u95e8\u63a7\u6a21\u578b\u57289000\u4e2a\u6ce2\u5f62\u6d4b\u8bd5\u96c6\u4e0aF1\u5206\u6570\u4e3a0.98\uff08\u7cbe\u5ea60.99\uff0c\u53ec\u56de\u73870.97\uff09\uff0c\u4f18\u4e8e\u57fa\u51c6PhaseNet\uff08F1\u5206\u65700.97\uff09\uff0c\u5bf9\u566a\u58f0\u5177\u6709\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u53ef\u89e3\u91caAI\u4e0d\u4ec5\u80fd\u89e3\u91ca\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u8fd8\u80fd\u76f4\u63a5\u63d0\u5347\u5176\u6027\u80fd\uff0c\u4e3a\u6784\u5efa\u53ef\u4fe1\u7684\u81ea\u52a8\u5316\u5730\u9707\u68c0\u6d4b\u5668\u63d0\u4f9b\u4e86\u6a21\u677f\u3002"}}
{"id": "2510.17467", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17467", "abs": "https://arxiv.org/abs/2510.17467", "authors": ["Dan Zheng", "Jing Feng", "Juan Liu"], "title": "CrossStateECG: Multi-Scale Deep Convolutional Network with Attention for Rest-Exercise ECG Biometrics", "comment": null, "summary": "Current research in Electrocardiogram (ECG) biometrics mainly emphasizes\nresting-state conditions, leaving the performance decline in rest-exercise\nscenarios largely unresolved. This paper introduces CrossStateECG, a robust\nECG-based authentication model explicitly tailored for cross-state\n(rest-exercise) conditions. The proposed model creatively combines multi-scale\ndeep convolutional feature extraction with attention mechanisms to ensure\nstrong identification across different physiological states. Experimental\nresults on the exercise-ECGID dataset validate the effectiveness of\nCrossStateECG, achieving an identification accuracy of 92.50% in the\nRest-to-Exercise scenario (training on resting ECG and testing on post-exercise\nECG) and 94.72% in the Exercise-to-Rest scenario (training on post-exercise ECG\nand testing on resting ECG). Furthermore, CrossStateECG demonstrates\nexceptional performance across both state combinations, reaching an accuracy of\n99.94% in Rest-to-Rest scenarios and 97.85% in Mixed-to-Mixed scenarios.\nAdditional validations on the ECG-ID and MIT-BIH datasets further confirmed the\ngeneralization abilities of CrossStateECG, underscoring its potential as a\npractical solution for post-exercise ECG-based authentication in dynamic\nreal-world settings.", "AI": {"tldr": "CrossStateECG\u662f\u4e00\u4e2a\u9488\u5bf9\u9759\u606f-\u8fd0\u52a8\u8de8\u72b6\u6001\u6761\u4ef6\u7684\u9c81\u68d2ECG\u751f\u7269\u8ba4\u8bc1\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u6df1\u5ea6\u5377\u79ef\u7279\u5f81\u63d0\u53d6\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u9759\u606f-\u8fd0\u52a8\u573a\u666f\u4e0b\u5b9e\u73b0\u4e8692.50%-94.72%\u7684\u8bc6\u522b\u51c6\u786e\u7387\u3002", "motivation": "\u5f53\u524dECG\u751f\u7269\u8bc6\u522b\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u9759\u606f\u72b6\u6001\uff0c\u800c\u9759\u606f-\u8fd0\u52a8\u573a\u666f\u4e0b\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u5c1a\u672a\u89e3\u51b3\uff0c\u9700\u8981\u5f00\u53d1\u8de8\u72b6\u6001\u9c81\u68d2\u8ba4\u8bc1\u6a21\u578b\u3002", "method": "\u7ed3\u5408\u591a\u5c3a\u5ea6\u6df1\u5ea6\u5377\u79ef\u7279\u5f81\u63d0\u53d6\u4e0e\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6784\u5efa\u8de8\u72b6\u6001ECG\u8ba4\u8bc1\u6a21\u578bCrossStateECG\u3002", "result": "\u5728exercise-ECGID\u6570\u636e\u96c6\u4e0a\uff0cRest-to-Exercise\u573a\u666f\u51c6\u786e\u738792.50%\uff0cExercise-to-Rest\u573a\u666f94.72%\uff0cRest-to-Rest\u573a\u666f99.94%\uff0cMixed-to-Mixed\u573a\u666f97.85%\u3002\u5728ECG-ID\u548cMIT-BIH\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "CrossStateECG\u5728\u52a8\u6001\u73b0\u5b9e\u73af\u5883\u4e2d\u5177\u6709\u4f5c\u4e3a\u8fd0\u52a8\u540eECG\u8ba4\u8bc1\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.17469", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17469", "abs": "https://arxiv.org/abs/2510.17469", "authors": ["Jing Liu"], "title": "Layer Specialization Underlying Compositional Reasoning in Transformers", "comment": null, "summary": "Transformers exhibit compositional reasoning on sequences not observed during\ntraining, a capability often attributed to in-context learning (ICL) and skill\ncomposition. We investigate this phenomenon using the Random Hierarchy Model\n(RHM), a probabilistic context-free grammar that generates sequences through\nrecursive rule application. Models are trained on subsets of sequences and\nevaluated across four generalization conditions: memorization, in-distribution\ngeneralization, out-of-distribution generalization with the same rules, and\ncross-layer transfer. Behaviorally, performance improves systematically with\ntask complexity and the number of in-context examples, with out-of-distribution\ntasks requiring substantially more examples than in-distribution scenarios.\nMechanistically, we identify a progressive emergence of layer specialization\nduring training that correlates with generalization performance. Principal\ncomponent analysis and attention pattern clustering reveal that transformers\ndevelop structured, hierarchically organized representations in specialized\nlayers. These results demonstrate that transformers develop modular,\ninterpretable mechanisms supporting compositional reasoning, linking internal\nalgorithmic structure to observed behavioral capabilities.", "AI": {"tldr": "Transformers\u901a\u8fc7\u968f\u673a\u5c42\u6b21\u6a21\u578b(RHM)\u8bad\u7ec3\uff0c\u5c55\u73b0\u51fa\u7ec4\u5408\u63a8\u7406\u80fd\u529b\uff0c\u5176\u6027\u80fd\u968f\u4efb\u52a1\u590d\u6742\u6027\u548c\u4e0a\u4e0b\u6587\u793a\u4f8b\u6570\u91cf\u63d0\u5347\uff0c\u5e76\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5f62\u6210\u5c42\u6b21\u5316\u3001\u6a21\u5757\u5316\u7684\u5185\u90e8\u8868\u793a\u7ed3\u6784\u3002", "motivation": "\u7814\u7a76Transformer\u6a21\u578b\u5728\u672a\u89c1\u5e8f\u5217\u4e0a\u7684\u7ec4\u5408\u63a8\u7406\u80fd\u529b\uff0c\u63a2\u7d22\u5176\u5185\u90e8\u673a\u5236\u5982\u4f55\u652f\u6301\u8fd9\u79cd\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u968f\u673a\u5c42\u6b21\u6a21\u578b(RHM)\u4f5c\u4e3a\u6982\u7387\u4e0a\u4e0b\u6587\u65e0\u5173\u6587\u6cd5\u751f\u6210\u5e8f\u5217\uff0c\u5728\u5e8f\u5217\u5b50\u96c6\u4e0a\u8bad\u7ec3\u6a21\u578b\uff0c\u5e76\u5728\u56db\u79cd\u6cdb\u5316\u6761\u4ef6\u4e0b\u8bc4\u4f30\uff1a\u8bb0\u5fc6\u3001\u5206\u5e03\u5185\u6cdb\u5316\u3001\u5206\u5e03\u5916\u6cdb\u5316(\u76f8\u540c\u89c4\u5219)\u548c\u8de8\u5c42\u8fc1\u79fb\u3002", "result": "\u6a21\u578b\u6027\u80fd\u968f\u4efb\u52a1\u590d\u6742\u6027\u548c\u4e0a\u4e0b\u6587\u793a\u4f8b\u6570\u91cf\u7cfb\u7edf\u6027\u63d0\u5347\uff0c\u5206\u5e03\u5916\u4efb\u52a1\u9700\u8981\u66f4\u591a\u793a\u4f8b\uff1b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u5c42\u6b21\u4e13\u4e1a\u5316\uff0c\u5f62\u6210\u7ed3\u6784\u5316\u3001\u5c42\u6b21\u5316\u7684\u8868\u793a\u548c\u6ce8\u610f\u529b\u6a21\u5f0f\u3002", "conclusion": "Transformer\u53d1\u5c55\u51fa\u6a21\u5757\u5316\u3001\u53ef\u89e3\u91ca\u7684\u673a\u5236\u652f\u6301\u7ec4\u5408\u63a8\u7406\uff0c\u5185\u90e8\u7b97\u6cd5\u7ed3\u6784\u4e0e\u89c2\u5bdf\u5230\u7684\u884c\u4e3a\u80fd\u529b\u76f8\u5173\u8054\u3002"}}
{"id": "2510.17475", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17475", "abs": "https://arxiv.org/abs/2510.17475", "authors": ["Fo Hu", "Can Wang", "Qinxu Zheng", "Xusheng Yang", "Bin Zhou", "Gang Li", "Yu Sun", "Wen-an Zhang"], "title": "DAMSDAN: Distribution-Aware Multi-Source Domain Adaptation Network for Cross-Domain EEG-based Emotion Recognition", "comment": "14 pages, 9 figures", "summary": "Significant inter-individual variability limits the generalization of\nEEG-based emotion recognition under cross-domain settings. We address two core\nchallenges in multi-source adaptation: (1) dynamically modeling distributional\nheterogeneity across sources and quantifying their relevance to a target to\nreduce negative transfer; and (2) achieving fine-grained semantic consistency\nto strengthen class discrimination. We propose a distribution-aware\nmulti-source domain adaptation network (DAMSDAN). DAMSDAN integrates\nprototype-based constraints with adversarial learning to drive the encoder\ntoward discriminative, domain-invariant emotion representations. A domain-aware\nsource weighting strategy based on maximum mean discrepancy (MMD) dynamically\nestimates inter-domain shifts and reweights source contributions. In addition,\na prototype-guided conditional alignment module with dual pseudo-label\ninteraction enhances pseudo-label reliability and enables category-level,\nfine-grained alignment, mitigating noise propagation and semantic drift.\nExperiments on SEED and SEED-IV show average accuracies of 94.86\\% and 79.78\\%\nfor cross-subject, and 95.12\\% and 83.15\\% for cross-session protocols. On the\nlarge-scale FACED dataset, DAMSDAN achieves 82.88\\% (cross-subject). Extensive\nablations and interpretability analyses corroborate the effectiveness of the\nproposed framework for cross-domain EEG-based emotion recognition.", "AI": {"tldr": "\u63d0\u51faDAMSDAN\u7f51\u7edc\u89e3\u51b3\u8de8\u57dfEEG\u60c5\u7eea\u8bc6\u522b\u4e2d\u7684\u4e2a\u4f53\u5dee\u5f02\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u6e90\u57df\u52a0\u6743\u548c\u539f\u578b\u5f15\u5bfc\u5bf9\u9f50\u5b9e\u73b0\u591a\u6e90\u57df\u9002\u5e94\u3002", "motivation": "EEG\u60c5\u7eea\u8bc6\u522b\u5728\u8de8\u57df\u8bbe\u7f6e\u4e0b\u5b58\u5728\u663e\u8457\u7684\u4e2a\u4f53\u95f4\u53d8\u5f02\u6027\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u9700\u8981\u89e3\u51b3\u591a\u6e90\u57df\u9002\u5e94\u4e2d\u7684\u4e24\u4e2a\u6838\u5fc3\u6311\u6218\uff1a\u52a8\u6001\u5efa\u6a21\u6e90\u57df\u5206\u5e03\u5f02\u8d28\u6027\u4ee5\u51cf\u5c11\u8d1f\u8fc1\u79fb\uff0c\u4ee5\u53ca\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u8bed\u4e49\u4e00\u81f4\u6027\u4ee5\u589e\u5f3a\u7c7b\u522b\u533a\u5206\u3002", "method": "\u63d0\u51fa\u5206\u5e03\u611f\u77e5\u591a\u6e90\u57df\u9002\u5e94\u7f51\u7edc(DAMSDAN)\uff0c\u6574\u5408\u539f\u578b\u7ea6\u675f\u4e0e\u5bf9\u6297\u5b66\u4e60\uff0c\u9a71\u52a8\u7f16\u7801\u5668\u5b66\u4e60\u5224\u522b\u6027\u3001\u57df\u4e0d\u53d8\u7684\u60c5\u7eea\u8868\u5f81\u3002\u91c7\u7528\u57fa\u4e8e\u6700\u5927\u5747\u503c\u5dee\u5f02(MMD)\u7684\u57df\u611f\u77e5\u6e90\u52a0\u6743\u7b56\u7565\u52a8\u6001\u4f30\u8ba1\u57df\u95f4\u504f\u79fb\u5e76\u91cd\u65b0\u52a0\u6743\u6e90\u8d21\u732e\uff0c\u4ee5\u53ca\u539f\u578b\u5f15\u5bfc\u7684\u6761\u4ef6\u5bf9\u9f50\u6a21\u5757\u901a\u8fc7\u53cc\u4f2a\u6807\u7b7e\u4ea4\u4e92\u589e\u5f3a\u4f2a\u6807\u7b7e\u53ef\u9760\u6027\u3002", "result": "\u5728SEED\u548cSEED-IV\u6570\u636e\u96c6\u4e0a\uff0c\u8de8\u88ab\u8bd5\u534f\u8bae\u5e73\u5747\u51c6\u786e\u7387\u5206\u522b\u4e3a94.86%\u548c79.78%\uff0c\u8de8\u4f1a\u8bdd\u534f\u8bae\u5206\u522b\u4e3a95.12%\u548c83.15%\u3002\u5728\u5927\u578bFACED\u6570\u636e\u96c6\u4e0a\uff0c\u8de8\u88ab\u8bd5\u51c6\u786e\u7387\u8fbe\u523082.88%\u3002\u5e7f\u6cdb\u7684\u6d88\u878d\u5b9e\u9a8c\u548c\u53ef\u89e3\u91ca\u6027\u5206\u6790\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "DAMSDAN\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u6e90\u57df\u52a0\u6743\u548c\u7ec6\u7c92\u5ea6\u8bed\u4e49\u5bf9\u9f50\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8de8\u57dfEEG\u60c5\u7eea\u8bc6\u522b\u4e2d\u7684\u4e2a\u4f53\u5dee\u5f02\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002"}}
{"id": "2510.17478", "categories": ["cs.LG", "physics.geo-ph", "I.2.6; I.6.3; J.2"], "pdf": "https://arxiv.org/pdf/2510.17478", "abs": "https://arxiv.org/abs/2510.17478", "authors": ["Guillaume Rongier", "Luk Peeters"], "title": "Towards geological inference with process-based and deep generative modeling, part 2: inversion of fluvial deposits and latent-space disentanglement", "comment": "52 pages, 42 figures", "summary": "High costs and uncertainties make subsurface decision-making challenging, as\nacquiring new data is rarely scalable. Embedding geological knowledge directly\ninto predictive models offers a valuable alternative. A joint approach enables\njust that: process-based models that mimic geological processes can help train\ngenerative models that make predictions more efficiently. This study explores\nwhether a generative adversarial network (GAN) - a type of deep-learning\nalgorithm for generative modeling - trained to produce fluvial deposits can be\ninverted to match well and seismic data. Four inversion approaches applied to\nthree test samples with 4, 8, and 20 wells struggled to match these well data,\nespecially as the well number increased or as the test sample diverged from the\ntraining data. The key bottleneck lies in the GAN's latent representation: it\nis entangled, so samples with similar sedimentological features are not\nnecessarily close in the latent space. Label conditioning or latent\noverparameterization can partially disentangle the latent space during\ntraining, although not yet sufficiently for a successful inversion. Fine-tuning\nthe GAN to restructure the latent space locally reduces mismatches to\nacceptable levels for all test cases, with and without seismic data. But this\napproach depends on an initial, partially successful inversion step, which\ninfluences the quality and diversity of the final samples. Overall, GANs can\nalready handle the tasks required for their integration into geomodeling\nworkflows. We still need to further assess their robustness, and how to best\nleverage them in support of geological interpretation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u751f\u6210\u5bf9\u6297\u7f51\u7edc(GAN)\u5728\u6c89\u79ef\u5efa\u6a21\u4e2d\u7684\u53cd\u6f14\u5e94\u7528\uff0c\u53d1\u73b0\u6807\u51c6GAN\u7684\u6f5c\u5728\u7a7a\u95f4\u7ea0\u7f20\u95ee\u9898\u5bfc\u81f4\u53cd\u6f14\u56f0\u96be\uff0c\u4f46\u901a\u8fc7\u5fae\u8c03\u65b9\u6cd5\u53ef\u4ee5\u6539\u5584\u5339\u914d\u6548\u679c\u3002", "motivation": "\u5730\u4e0b\u51b3\u7b56\u6210\u672c\u9ad8\u4e14\u4e0d\u786e\u5b9a\u6027\u5927\uff0c\u83b7\u53d6\u65b0\u6570\u636e\u96be\u4ee5\u6269\u5c55\u3002\u5c06\u5730\u8d28\u77e5\u8bc6\u76f4\u63a5\u5d4c\u5165\u9884\u6d4b\u6a21\u578b\u662f\u66f4\u6709\u4ef7\u503c\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u8fc7\u7a0b\u6a21\u578b\u53ef\u4ee5\u5e2e\u52a9\u8bad\u7ec3\u751f\u6210\u6a21\u578b\u4ee5\u63d0\u9ad8\u9884\u6d4b\u6548\u7387\u3002", "method": "\u4f7f\u7528\u751f\u6210\u5bf9\u6297\u7f51\u7edc(GAN)\u8bad\u7ec3\u751f\u6210\u6cb3\u6d41\u6c89\u79ef\u6a21\u578b\uff0c\u5e76\u5e94\u7528\u56db\u79cd\u53cd\u6f14\u65b9\u6cd5\u6765\u5339\u914d\u4e95\u6570\u636e\u548c\u5730\u9707\u6570\u636e\u3002\u901a\u8fc7\u6807\u7b7e\u6761\u4ef6\u5316\u3001\u6f5c\u5728\u8fc7\u53c2\u6570\u5316\u548c\u5fae\u8c03GAN\u6765\u91cd\u6784\u6f5c\u5728\u7a7a\u95f4\u3002", "result": "\u57284\u30018\u300120\u53e3\u4e95\u7684\u4e09\u4e2a\u6d4b\u8bd5\u6837\u672c\u4e2d\uff0c\u53cd\u6f14\u65b9\u6cd5\u96be\u4ee5\u5339\u914d\u4e95\u6570\u636e\uff0c\u7279\u522b\u662f\u5f53\u4e95\u6570\u589e\u52a0\u6216\u6d4b\u8bd5\u6837\u672c\u4e0e\u8bad\u7ec3\u6570\u636e\u5dee\u5f02\u8f83\u5927\u65f6\u3002\u5fae\u8c03GAN\u53ef\u5c06\u4e0d\u5339\u914d\u964d\u81f3\u53ef\u63a5\u53d7\u6c34\u5e73\uff0c\u4f46\u4f9d\u8d56\u4e8e\u521d\u59cb\u90e8\u5206\u6210\u529f\u7684\u53cd\u6f14\u6b65\u9aa4\u3002", "conclusion": "GAN\u5df2\u80fd\u5904\u7406\u5730\u8d28\u5efa\u6a21\u5de5\u4f5c\u6d41\u4e2d\u7684\u4efb\u52a1\uff0c\u4f46\u9700\u8981\u8fdb\u4e00\u6b65\u8bc4\u4f30\u5176\u9c81\u68d2\u6027\uff0c\u4ee5\u53ca\u5982\u4f55\u6700\u597d\u5730\u652f\u6301\u5730\u8d28\u89e3\u91ca\u3002"}}
{"id": "2510.17480", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17480", "abs": "https://arxiv.org/abs/2510.17480", "authors": ["Aur\u00e9lien Bellet", "Edwige Cyffers", "Davide Frey", "Romaric Gaudel", "Dimitri Ler\u00e9v\u00e9rend", "Fran\u00e7ois Ta\u00efani"], "title": "Unified Privacy Guarantees for Decentralized Learning via Matrix Factorization", "comment": "21 pages, 5 figures", "summary": "Decentralized Learning (DL) enables users to collaboratively train models\nwithout sharing raw data by iteratively averaging local updates with neighbors\nin a network graph. This setting is increasingly popular for its scalability\nand its ability to keep data local under user control. Strong privacy\nguarantees in DL are typically achieved through Differential Privacy (DP), with\nresults showing that DL can even amplify privacy by disseminating noise across\npeer-to-peer communications. Yet in practice, the observed privacy-utility\ntrade-off often appears worse than in centralized training, which may be due to\nlimitations in current DP accounting methods for DL. In this paper, we show\nthat recent advances in centralized DP accounting based on Matrix Factorization\n(MF) for analyzing temporal noise correlations can also be leveraged in DL. By\ngeneralizing existing MF results, we show how to cast both standard DL\nalgorithms and common trust models into a unified formulation. This yields\ntighter privacy accounting for existing DP-DL algorithms and provides a\nprincipled way to develop new ones. To demonstrate the approach, we introduce\nMAFALDA-SGD, a gossip-based DL algorithm with user-level correlated noise that\noutperforms existing methods on synthetic and real-world graphs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e9\u9635\u5206\u89e3\u7684\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u9690\u79c1\u8ba1\u7b97\u65b9\u6cd5MAFALDA-SGD\uff0c\u901a\u8fc7\u5206\u6790\u65f6\u95f4\u566a\u58f0\u76f8\u5173\u6027\u6765\u83b7\u5f97\u66f4\u7d27\u5bc6\u7684\u9690\u79c1\u9884\u7b97\uff0c\u5728\u5408\u6210\u548c\u771f\u5b9e\u56fe\u6570\u636e\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u867d\u7136\u80fd\u901a\u8fc7\u5dee\u5206\u9690\u79c1\u4fdd\u62a4\u6570\u636e\uff0c\u4f46\u5b9e\u8df5\u4e2d\u89c2\u5bdf\u5230\u7684\u9690\u79c1-\u6548\u7528\u6743\u8861\u5f80\u5f80\u6bd4\u96c6\u4e2d\u5f0f\u8bad\u7ec3\u5dee\uff0c\u8fd9\u53ef\u80fd\u662f\u7531\u4e8e\u5f53\u524dDP\u8ba1\u7b97\u65b9\u6cd5\u5728DL\u4e2d\u7684\u5c40\u9650\u6027\u9020\u6210\u7684\u3002", "method": "\u5c06\u96c6\u4e2d\u5f0fDP\u8ba1\u7b97\u4e2d\u57fa\u4e8e\u77e9\u9635\u5206\u89e3\u5206\u6790\u65f6\u95f4\u566a\u58f0\u76f8\u5173\u6027\u7684\u65b9\u6cd5\u63a8\u5e7f\u5230\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\uff0c\u63d0\u51fa\u4e86MAFALDA-SGD\u7b97\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8egossip\u7684\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u5177\u6709\u7528\u6237\u7ea7\u76f8\u5173\u566a\u58f0\u3002", "result": "MAFALDA-SGD\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u56fe\u6570\u636e\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u7684DP-DL\u7b97\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u77e9\u9635\u5206\u89e3\u65b9\u6cd5\u53ef\u4ee5\u66f4\u7d27\u5bc6\u5730\u8ba1\u7b97\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u7684\u9690\u79c1\u9884\u7b97\uff0c\u5e76\u4e3a\u5f00\u53d1\u65b0\u7684DP-DL\u7b97\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2510.17486", "categories": ["cs.LG", "68T07, 68T05, 65K10, 90C30", "I.2.6; G.1.6; I.5.1"], "pdf": "https://arxiv.org/pdf/2510.17486", "abs": "https://arxiv.org/abs/2510.17486", "authors": ["Maxim Bolshim", "Alexander Kugaevskikh"], "title": "Local properties of neural networks through the lens of layer-wise Hessians", "comment": "Comments: 22 pages, 8 figures. Submitted to arXiv:cs.LG", "summary": "We introduce a methodology for analyzing neural networks through the lens of\nlayer-wise Hessian matrices. The local Hessian of each functional block (layer)\nis defined as the matrix of second derivatives of a scalar function with\nrespect to the parameters of that layer. This concept provides a formal tool\nfor characterizing the local geometry of the parameter space. We show that the\nspectral properties of local Hessians, such as the distribution of eigenvalues,\nreveal quantitative patterns associated with overfitting,\nunderparameterization, and expressivity in neural network architectures. We\nconduct an extensive empirical study involving 111 experiments across 37\ndatasets. The results demonstrate consistent structural regularities in the\nevolution of local Hessians during training and highlight correlations between\ntheir spectra and generalization performance. These findings establish a\nfoundation for using local geometric analysis to guide the diagnosis and design\nof deep neural networks. The proposed framework connects optimization geometry\nwith functional behavior and offers practical insight for improving network\narchitectures and training stability.", "AI": {"tldr": "\u63d0\u51fa\u901a\u8fc7\u5c42\u95f4Hessian\u77e9\u9635\u5206\u6790\u795e\u7ecf\u7f51\u7edc\u7684\u65b9\u6cd5\uff0c\u7814\u7a76\u53c2\u6570\u7a7a\u95f4\u7684\u5c40\u90e8\u51e0\u4f55\u7279\u6027\uff0c\u53d1\u73b0Hessian\u8c31\u7279\u5f81\u4e0e\u8fc7\u62df\u5408\u3001\u6b20\u53c2\u6570\u5316\u548c\u8868\u8fbe\u80fd\u529b\u76f8\u5173\u3002", "motivation": "\u5efa\u7acb\u8fde\u63a5\u4f18\u5316\u51e0\u4f55\u4e0e\u529f\u80fd\u884c\u4e3a\u7684\u6846\u67b6\uff0c\u4e3a\u795e\u7ecf\u7f51\u7edc\u8bca\u65ad\u548c\u8bbe\u8ba1\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\uff0c\u6539\u8fdb\u7f51\u7edc\u67b6\u6784\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "method": "\u5b9a\u4e49\u6bcf\u4e2a\u529f\u80fd\u5757\uff08\u5c42\uff09\u7684\u5c40\u90e8Hessian\u77e9\u9635\u4f5c\u4e3a\u53c2\u6570\u4e8c\u9636\u5bfc\u6570\u7684\u77e9\u9635\uff0c\u5206\u6790\u5176\u8c31\u7279\u6027\uff08\u5982\u7279\u5f81\u503c\u5206\u5e03\uff09\uff0c\u572837\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86111\u6b21\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u53d1\u73b0\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5c40\u90e8Hessian\u5177\u6709\u4e00\u81f4\u7684\u7ed3\u6784\u89c4\u5f8b\uff0c\u5176\u8c31\u7279\u5f81\u4e0e\u6cdb\u5316\u6027\u80fd\u5b58\u5728\u76f8\u5173\u6027\uff0c\u63ed\u793a\u4e86\u4e0e\u8fc7\u62df\u5408\u3001\u6b20\u53c2\u6570\u5316\u548c\u8868\u8fbe\u80fd\u529b\u7684\u5b9a\u91cf\u6a21\u5f0f\u3002", "conclusion": "\u5c40\u90e8\u51e0\u4f55\u5206\u6790\u4e3a\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u8bca\u65ad\u548c\u8bbe\u8ba1\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u8fde\u63a5\u4e86\u4f18\u5316\u51e0\u4f55\u4e0e\u529f\u80fd\u884c\u4e3a\uff0c\u63d0\u4f9b\u4e86\u6539\u8fdb\u7f51\u7edc\u67b6\u6784\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u7684\u5b9e\u7528\u89c1\u89e3\u3002"}}
{"id": "2510.17496", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17496", "abs": "https://arxiv.org/abs/2510.17496", "authors": ["Giacomo Camposampiero", "Michael Hersche", "Roger Wattenhofer", "Abu Sebastian", "Abbas Rahimi"], "title": "I-RAVEN-X: Benchmarking Generalization and Robustness of Analogical and Mathematical Reasoning in Large Language and Reasoning Models", "comment": "Accepted at the 5th Workshop on Mathematical Reasoning and AI\n  (MATH-AI), NeurIPS 2025", "summary": "We introduce I-RAVEN-X, a symbolic benchmark designed to evaluate\ngeneralization and robustness in analogical and mathematical reasoning for\nLarge Language Models (LLMs) and Large Reasoning Models (LRMs). I-RAVEN-X\nextends I-RAVEN by increasing operand complexity, attribute range, and\nintroducing perceptual uncertainty. Compared to LLMs, empirical results show\nthat LRMs achieve improved productivity and systematicity on longer reasoning\nrelations and wider attribute ranges, respectively. However, LRMs are still\nsignificantly challenged by reasoning under uncertainty and cannot effectively\nexplore multiple probabilistic outcomes.", "AI": {"tldr": "I-RAVEN-X\u662f\u4e00\u4e2a\u7b26\u53f7\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u548c\u5927\u63a8\u7406\u6a21\u578b\u5728\u7c7b\u6bd4\u548c\u6570\u5b66\u63a8\u7406\u4e2d\u7684\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u3002\u5b83\u901a\u8fc7\u589e\u52a0\u64cd\u4f5c\u6570\u590d\u6742\u5ea6\u3001\u5c5e\u6027\u8303\u56f4\u548c\u5f15\u5165\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u6765\u6269\u5c55I-RAVEN\u3002", "motivation": "\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u548c\u5927\u63a8\u7406\u6a21\u578b\u5728\u7c7b\u6bd4\u548c\u6570\u5b66\u63a8\u7406\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u590d\u6742\u64cd\u4f5c\u6570\u3001\u5bbd\u5c5e\u6027\u8303\u56f4\u548c\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u65f6\u7684\u8868\u73b0\u3002", "method": "\u6269\u5c55I-RAVEN\u57fa\u51c6\u6d4b\u8bd5\uff0c\u589e\u52a0\u64cd\u4f5c\u6570\u590d\u6742\u5ea6\u3001\u6269\u5927\u5c5e\u6027\u8303\u56f4\uff0c\u5e76\u5f15\u5165\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u7136\u540e\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u548c\u5927\u63a8\u7406\u6a21\u578b\u8fdb\u884c\u5b9e\u8bc1\u8bc4\u4f30\u3002", "result": "\u5927\u63a8\u7406\u6a21\u578b\u5728\u957f\u63a8\u7406\u5173\u7cfb\u548c\u5bbd\u5c5e\u6027\u8303\u56f4\u4e0a\u5206\u522b\u8868\u73b0\u51fa\u66f4\u597d\u7684\u751f\u4ea7\u529b\u548c\u7cfb\u7edf\u6027\uff0c\u4f46\u9762\u5bf9\u4e0d\u786e\u5b9a\u6027\u63a8\u7406\u65f6\u4ecd\u7136\u663e\u8457\u53d7\u6311\u6218\uff0c\u65e0\u6cd5\u6709\u6548\u63a2\u7d22\u591a\u4e2a\u6982\u7387\u7ed3\u679c\u3002", "conclusion": "\u5927\u63a8\u7406\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u4f18\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5728\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u548c\u6982\u7387\u63a8\u7406\u65b9\u9762\u4ecd\u6709\u660e\u663e\u5c40\u9650\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002"}}
{"id": "2510.17515", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17515", "abs": "https://arxiv.org/abs/2510.17515", "authors": ["Hoang Pham", "The-Anh Ta", "Tom Jacobs", "Rebekka Burkholz", "Long Tran-Thanh"], "title": "The Graphon Limit Hypothesis: Understanding Neural Network Pruning via Infinite Width Analysis", "comment": "NeurIPS 2025 Spotlight", "summary": "Sparse neural networks promise efficiency, yet training them effectively\nremains a fundamental challenge. Despite advances in pruning methods that\ncreate sparse architectures, understanding why some sparse structures are\nbetter trainable than others with the same level of sparsity remains poorly\nunderstood. Aiming to develop a systematic approach to this fundamental\nproblem, we propose a novel theoretical framework based on the theory of graph\nlimits, particularly graphons, that characterizes sparse neural networks in the\ninfinite-width regime. Our key insight is that connectivity patterns of sparse\nneural networks induced by pruning methods converge to specific graphons as\nnetworks' width tends to infinity, which encodes implicit structural biases of\ndifferent pruning methods. We postulate the Graphon Limit Hypothesis and\nprovide empirical evidence to support it. Leveraging this graphon\nrepresentation, we derive a Graphon Neural Tangent Kernel (Graphon NTK) to\nstudy the training dynamics of sparse networks in the infinite width limit.\nGraphon NTK provides a general framework for the theoretical analysis of sparse\nnetworks. We empirically show that the spectral analysis of Graphon NTK\ncorrelates with observed training dynamics of sparse networks, explaining the\nvarying convergence behaviours of different pruning methods. Our framework\nprovides theoretical insights into the impact of connectivity patterns on the\ntrainability of various sparse network architectures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u56fe\u6781\u9650\u7406\u8bba\u7684\u65b0\u6846\u67b6\uff0c\u4f7f\u7528\u56fe\u8bba\u5de5\u5177\u5206\u6790\u7a00\u758f\u795e\u7ecf\u7f51\u7edc\u7684\u8bad\u7ec3\u52a8\u6001\uff0c\u89e3\u91ca\u4e86\u4e0d\u540c\u526a\u679d\u65b9\u6cd5\u4ea7\u751f\u4e0d\u540c\u8bad\u7ec3\u6548\u679c\u7684\u539f\u56e0\u3002", "motivation": "\u5c3d\u7ba1\u526a\u679d\u65b9\u6cd5\u80fd\u521b\u5efa\u7a00\u758f\u67b6\u6784\uff0c\u4f46\u4e3a\u4ec0\u4e48\u76f8\u540c\u7a00\u758f\u5ea6\u4e0b\u67d0\u4e9b\u7ed3\u6784\u6bd4\u5176\u4ed6\u7ed3\u6784\u66f4\u5bb9\u6613\u8bad\u7ec3\u4ecd\u4e0d\u6e05\u695a\u3002\u9700\u8981\u7cfb\u7edf\u6027\u5730\u7406\u89e3\u7a00\u758f\u7f51\u7edc\u7684\u53ef\u8bad\u7ec3\u6027\u3002", "method": "\u57fa\u4e8e\u56fe\u6781\u9650\u7406\u8bba\uff08\u7279\u522b\u662f\u56fe\u8bba\uff09\u5efa\u7acb\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u526a\u679d\u65b9\u6cd5\u8bf1\u5bfc\u7684\u8fde\u63a5\u6a21\u5f0f\u5efa\u6a21\u4e3a\u56fe\u8bba\uff0c\u63a8\u5bfc\u51fa\u56fe\u8bba\u795e\u7ecf\u6b63\u5207\u6838\u6765\u5206\u6790\u65e0\u9650\u5bbd\u5ea6\u6781\u9650\u4e0b\u7684\u8bad\u7ec3\u52a8\u6001\u3002", "result": "\u56fe\u8bbaNTK\u7684\u8c31\u5206\u6790\u4e0e\u89c2\u5bdf\u5230\u7684\u7a00\u758f\u7f51\u7edc\u8bad\u7ec3\u52a8\u6001\u76f8\u5173\uff0c\u80fd\u591f\u89e3\u91ca\u4e0d\u540c\u526a\u679d\u65b9\u6cd5\u7684\u4e0d\u540c\u6536\u655b\u884c\u4e3a\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u7406\u89e3\u8fde\u63a5\u6a21\u5f0f\u5bf9\u7a00\u758f\u7f51\u7edc\u67b6\u6784\u53ef\u8bad\u7ec3\u6027\u7684\u5f71\u54cd\u63d0\u4f9b\u4e86\u7406\u8bba\u6d1e\u5bdf\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u526a\u679d\u65b9\u6cd5\u7684\u9690\u5f0f\u7ed3\u6784\u504f\u5dee\u3002"}}
{"id": "2510.17517", "categories": ["cs.LG", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.17517", "abs": "https://arxiv.org/abs/2510.17517", "authors": ["Hangcheng Cao", "Baixiang Huang", "Longzhi Yuan", "Haonan An", "Zihan Fang", "Xianhao Chen", "Yuguang Fang"], "title": "SAFE-D: A Spatiotemporal Detection Framework for Abnormal Driving Among Parkinson's Disease-like Drivers", "comment": null, "summary": "A driver's health state serves as a determinant factor in driving behavioral\nregulation. Subtle deviations from normalcy can lead to operational anomalies,\nposing risks to public transportation safety. While prior efforts have\ndeveloped detection mechanisms for functionally-driven temporary anomalies such\nas drowsiness and distraction, limited research has addressed\npathologically-triggered deviations, especially those stemming from chronic\nmedical conditions. To bridge this gap, we investigate the driving behavior of\nParkinson's disease patients and propose SAFE-D, a novel framework for\ndetecting Parkinson-related behavioral anomalies to enhance driving safety. Our\nmethodology starts by performing analysis of Parkinson's disease\nsymptomatology, focusing on primary motor impairments, and establishes causal\nlinks to degraded driving performance. To represent the subclinical behavioral\nvariations of early-stage Parkinson's disease, our framework integrates data\nfrom multiple vehicle control components to build a behavioral profile. We then\ndesign an attention-based network that adaptively prioritizes spatiotemporal\nfeatures, enabling robust anomaly detection under physiological variability.\nFinally, we validate SAFE-D on the Logitech G29 platform and CARLA simulator,\nusing data from three road maps to emulate real-world driving. Our results show\nSAFE-D achieves 96.8% average accuracy in distinguishing normal and\nParkinson-affected driving patterns.", "AI": {"tldr": "\u63d0\u51fa\u4e86SAFE-D\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u5e15\u91d1\u68ee\u75c5\u76f8\u5173\u7684\u9a7e\u9a76\u884c\u4e3a\u5f02\u5e38\uff0c\u901a\u8fc7\u591a\u6e90\u8f66\u8f86\u63a7\u5236\u6570\u636e\u6784\u5efa\u884c\u4e3a\u6863\u6848\uff0c\u4f7f\u7528\u6ce8\u610f\u529b\u7f51\u7edc\u8bc6\u522b\u65f6\u7a7a\u7279\u5f81\uff0c\u5728\u6a21\u62df\u73af\u5883\u4e2d\u8fbe\u523096.8%\u7684\u68c0\u6d4b\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u529f\u80fd\u9a71\u52a8\u7684\u6682\u65f6\u6027\u5f02\u5e38\uff08\u5982\u75b2\u52b3\u3001\u5206\u5fc3\uff09\uff0c\u800c\u7f3a\u4e4f\u5bf9\u75c5\u7406\u89e6\u53d1\u7684\u9a7e\u9a76\u884c\u4e3a\u5f02\u5e38\uff08\u7279\u522b\u662f\u6162\u6027\u75be\u75c5\u5982\u5e15\u91d1\u68ee\u75c5\uff09\u7684\u7814\u7a76\uff0c\u8fd9\u5bf9\u516c\u5171\u4ea4\u901a\u5b89\u5168\u6784\u6210\u98ce\u9669\u3002", "method": "\u5206\u6790\u5e15\u91d1\u68ee\u75c5\u75c7\u72b6\u5b66\u4e0e\u9a7e\u9a76\u8868\u73b0\u7684\u5173\u7cfb\uff0c\u6574\u5408\u591a\u8f66\u8f86\u63a7\u5236\u7ec4\u4ef6\u6570\u636e\u6784\u5efa\u884c\u4e3a\u6863\u6848\uff0c\u8bbe\u8ba1\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u7f51\u7edc\u81ea\u9002\u5e94\u4f18\u5148\u5904\u7406\u65f6\u7a7a\u7279\u5f81\uff0c\u5728Logitech G29\u5e73\u53f0\u548cCARLA\u6a21\u62df\u5668\u4e0a\u9a8c\u8bc1\u3002", "result": "SAFE-D\u5728\u533a\u5206\u6b63\u5e38\u548c\u5e15\u91d1\u68ee\u75c5\u5f71\u54cd\u9a7e\u9a76\u6a21\u5f0f\u65b9\u9762\u8fbe\u523096.8%\u7684\u5e73\u5747\u51c6\u786e\u7387\uff0c\u4f7f\u7528\u4e09\u4e2a\u9053\u8def\u5730\u56fe\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u9a7e\u9a76\u573a\u666f\u3002", "conclusion": "SAFE-D\u6846\u67b6\u80fd\u6709\u6548\u68c0\u6d4b\u5e15\u91d1\u68ee\u75c5\u76f8\u5173\u7684\u9a7e\u9a76\u884c\u4e3a\u5f02\u5e38\uff0c\u4e3a\u63d0\u5347\u9a7e\u9a76\u5b89\u5168\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u65e9\u671f\u5e15\u91d1\u68ee\u75c5\u7684\u4e9a\u4e34\u5e8a\u884c\u4e3a\u53d8\u5316\u68c0\u6d4b\u3002"}}
{"id": "2510.17520", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17520", "abs": "https://arxiv.org/abs/2510.17520", "authors": ["Canran Xiao", "Chuangxin Zhao", "Zong Ke", "Fei Shen"], "title": "Curiosity Meets Cooperation: A Game-Theoretic Approach to Long-Tail Multi-Label Learning", "comment": "Under review", "summary": "Long-tail imbalance is endemic to multi-label learning: a few head labels\ndominate the gradient signal, while the many rare labels that matter in\npractice are silently ignored. We tackle this problem by casting the task as a\ncooperative potential game. In our Curiosity-Driven Game-Theoretic Multi-Label\nLearning (CD-GTMLL) framework, the label space is split among several\ncooperating players that share a global accuracy payoff yet earn additional\ncuriosity rewards that rise with label rarity and inter-player disagreement.\nThese curiosity bonuses inject gradient on under-represented tags without\nhand-tuned class weights. We prove that gradient best-response updates ascend a\ndifferentiable potential and converge to tail-aware stationary points that\ntighten a lower bound on the expected Rare-F1. Extensive experiments on\nconventional benchmarks and three extreme-scale datasets show consistent\nstate-of-the-art gains, delivering up to +4.3% Rare-F1 and +1.6% P@3 over the\nstrongest baselines, while ablations reveal emergent division of labour and\nfaster consensus on rare classes. CD-GTMLL thus offers a principled, scalable\nroute to long-tail robustness in multi-label prediction.", "AI": {"tldr": "\u63d0\u51faCD-GTMLL\u6846\u67b6\uff0c\u5c06\u591a\u6807\u7b7e\u5b66\u4e60\u5efa\u6a21\u4e3a\u5408\u4f5c\u535a\u5f08\uff0c\u901a\u8fc7\u597d\u5947\u5fc3\u5956\u52b1\u673a\u5236\u89e3\u51b3\u957f\u5c3e\u5206\u5e03\u95ee\u9898\uff0c\u63d0\u5347\u7f55\u89c1\u6807\u7b7e\u7684\u9884\u6d4b\u6027\u80fd", "motivation": "\u591a\u6807\u7b7e\u5b66\u4e60\u4e2d\u5b58\u5728\u957f\u5c3e\u4e0d\u5e73\u8861\u95ee\u9898\uff1a\u5c11\u6570\u5934\u90e8\u6807\u7b7e\u4e3b\u5bfc\u68af\u5ea6\u4fe1\u53f7\uff0c\u800c\u8bb8\u591a\u5b9e\u9645\u91cd\u8981\u7684\u7f55\u89c1\u6807\u7b7e\u88ab\u5ffd\u7565", "method": "\u5c06\u6807\u7b7e\u7a7a\u95f4\u5206\u5272\u7ed9\u591a\u4e2a\u5408\u4f5c\u73a9\u5bb6\uff0c\u5171\u4eab\u5168\u5c40\u51c6\u786e\u5ea6\u6536\u76ca\uff0c\u540c\u65f6\u6839\u636e\u6807\u7b7e\u7a00\u6709\u5ea6\u548c\u73a9\u5bb6\u95f4\u5206\u6b67\u83b7\u5f97\u989d\u5916\u597d\u5947\u5fc3\u5956\u52b1\uff0c\u65e0\u9700\u624b\u52a8\u8c03\u6574\u7c7b\u522b\u6743\u91cd", "result": "\u5728\u4f20\u7edf\u57fa\u51c6\u548c\u4e09\u4e2a\u8d85\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u6700\u5148\u8fdb\u6027\u80fd\uff0cRare-F1\u63d0\u5347\u8fbe+4.3%\uff0cP@3\u63d0\u5347+1.6%\uff0c\u6d88\u878d\u5b9e\u9a8c\u663e\u793a\u51fa\u73b0\u5206\u5de5\u548c\u7f55\u89c1\u7c7b\u522b\u66f4\u5feb\u8fbe\u6210\u5171\u8bc6", "conclusion": "CD-GTMLL\u4e3a\u591a\u6807\u7b7e\u9884\u6d4b\u4e2d\u7684\u957f\u5c3e\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2510.17524", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17524", "abs": "https://arxiv.org/abs/2510.17524", "authors": ["Sidney Bender", "Ole Delzer", "Jan Herrmann", "Heike Antje Marxfeld", "Klaus-Robert M\u00fcller", "Gr\u00e9goire Montavon"], "title": "Mitigating Clever Hans Strategies in Image Classifiers through Generating Counterexamples", "comment": null, "summary": "Deep learning models remain vulnerable to spurious correlations, leading to\nso-called Clever Hans predictors that undermine robustness even in large-scale\nfoundation and self-supervised models. Group distributional robustness methods,\nsuch as Deep Feature Reweighting (DFR) rely on explicit group labels to\nupweight underrepresented subgroups, but face key limitations: (1) group labels\nare often unavailable, (2) low within-group sample sizes hinder coverage of the\nsubgroup distribution, and (3) performance degrades sharply when multiple\nspurious correlations fragment the data into even smaller groups. We propose\nCounterfactual Knowledge Distillation (CFKD), a framework that sidesteps these\nissues by generating diverse counterfactuals, enabling a human annotator to\nefficiently explore and correct the model's decision boundaries through a\nknowledge distillation step. Unlike DFR, our method not only reweights the\nundersampled groups, but it also enriches them with new data points. Our method\ndoes not require any confounder labels, achieves effective scaling to multiple\nconfounders, and yields balanced generalization across groups. We demonstrate\nCFKD's efficacy across five datasets, spanning synthetic tasks to an industrial\napplication, with particularly strong gains in low-data regimes with pronounced\nspurious correlations. Additionally, we provide an ablation study on the effect\nof the chosen counterfactual explainer and teacher model, highlighting their\nimpact on robustness.", "AI": {"tldr": "\u63d0\u51faCounterfactual Knowledge Distillation (CFKD)\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u591a\u6837\u53cd\u4e8b\u5b9e\u6837\u672c\u6765\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u4e2d\u865a\u5047\u76f8\u5173\u6027\u95ee\u9898\uff0c\u65e0\u9700\u7ec4\u6807\u7b7e\u5373\u53ef\u5b9e\u73b0\u8de8\u7ec4\u5e73\u8861\u6cdb\u5316\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5bb9\u6613\u53d7\u5230\u865a\u5047\u76f8\u5173\u6027\u7684\u5f71\u54cd\uff0c\u5f62\u6210Clever Hans\u9884\u6d4b\u5668\u3002\u73b0\u6709\u65b9\u6cd5\u5982DFR\u4f9d\u8d56\u663e\u5f0f\u7ec4\u6807\u7b7e\uff0c\u4f46\u9762\u4e34\u7ec4\u6807\u7b7e\u4e0d\u53ef\u5f97\u3001\u7ec4\u5185\u6837\u672c\u5c11\u3001\u591a\u865a\u5047\u76f8\u5173\u6027\u5bfc\u81f4\u6570\u636e\u788e\u7247\u5316\u7b49\u95ee\u9898\u3002", "method": "CFKD\u901a\u8fc7\u751f\u6210\u591a\u6837\u53cd\u4e8b\u5b9e\u6837\u672c\uff0c\u8ba9\u4eba\u7c7b\u6807\u6ce8\u8005\u9ad8\u6548\u63a2\u7d22\u548c\u4fee\u6b63\u6a21\u578b\u51b3\u7b56\u8fb9\u754c\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u6b65\u9aa4\u4e0d\u4ec5\u91cd\u91c7\u6837\u6b20\u8868\u793a\u7ec4\uff0c\u8fd8\u4e3a\u5176\u4e30\u5bcc\u65b0\u6570\u636e\u70b9\u3002", "result": "\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86CFKD\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u4f4e\u6570\u636e\u91cf\u548c\u9ad8\u865a\u5047\u76f8\u5173\u6027\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u591f\u6269\u5c55\u5230\u591a\u6df7\u6dc6\u53d8\u91cf\u5e76\u5b9e\u73b0\u8de8\u7ec4\u5e73\u8861\u6cdb\u5316\u3002", "conclusion": "CFKD\u65e0\u9700\u6df7\u6dc6\u53d8\u91cf\u6807\u7b7e\uff0c\u80fd\u6709\u6548\u5904\u7406\u591a\u6df7\u6dc6\u53d8\u91cf\u60c5\u51b5\uff0c\u5728\u5b58\u5728\u663e\u8457\u865a\u5047\u76f8\u5173\u6027\u7684\u4f4e\u6570\u636e\u91cf\u573a\u666f\u4e0b\u8868\u73b0\u7a81\u51fa\uff0c\u4e3a\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2510.17526", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17526", "abs": "https://arxiv.org/abs/2510.17526", "authors": ["Wei Huang", "Andi Han", "Yujin Song", "Yilan Chen", "Denny Wu", "Difan Zou", "Taiji Suzuki"], "title": "How Does Label Noise Gradient Descent Improve Generalization in the Low SNR Regime?", "comment": "40 pages", "summary": "The capacity of deep learning models is often large enough to both learn the\nunderlying statistical signal and overfit to noise in the training set. This\nnoise memorization can be harmful especially for data with a low\nsignal-to-noise ratio (SNR), leading to poor generalization. Inspired by prior\nobservations that label noise provides implicit regularization that improves\ngeneralization, in this work, we investigate whether introducing label noise to\nthe gradient updates can enhance the test performance of neural network (NN) in\nthe low SNR regime. Specifically, we consider training a two-layer NN with a\nsimple label noise gradient descent (GD) algorithm, in an idealized\nsignal-noise data setting. We prove that adding label noise during training\nsuppresses noise memorization, preventing it from dominating the learning\nprocess; consequently, label noise GD enjoys rapid signal growth while the\noverfitting remains controlled, thereby achieving good generalization despite\nthe low SNR. In contrast, we also show that NN trained with standard GD tends\nto overfit to noise in the same low SNR setting and establish a non-vanishing\nlower bound on its test error, thus demonstrating the benefit of introducing\nlabel noise in gradient-based training.", "AI": {"tldr": "\u5728\u4f4e\u4fe1\u566a\u6bd4(SNR)\u6570\u636e\u4e2d\uff0c\u901a\u8fc7\u5411\u68af\u5ea6\u4e0b\u964d\u8bad\u7ec3\u8fc7\u7a0b\u6dfb\u52a0\u6807\u7b7e\u566a\u58f0\u53ef\u4ee5\u6291\u5236\u566a\u58f0\u8bb0\u5fc6\u5316\uff0c\u6539\u5584\u795e\u7ecf\u7f51\u7edc\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5bb9\u6613\u5728\u8bad\u7ec3\u4e2d\u8fc7\u62df\u5408\u566a\u58f0\uff0c\u7279\u522b\u662f\u5728\u4f4e\u4fe1\u566a\u6bd4\u6570\u636e\u4e2d\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u6cdb\u5316\u6027\u80fd\u4e0b\u964d\u3002\u53d7\u6807\u7b7e\u566a\u58f0\u5177\u6709\u9690\u5f0f\u6b63\u5219\u5316\u6548\u679c\u7684\u542f\u53d1\uff0c\u7814\u7a76\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u5f15\u5165\u6807\u7b7e\u566a\u58f0\u6765\u63d0\u5347\u795e\u7ecf\u7f51\u7edc\u5728\u4f4e\u4fe1\u566a\u6bd4\u73af\u5883\u4e0b\u7684\u6d4b\u8bd5\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u4e24\u5c42\u795e\u7ecf\u7f51\u7edc\uff0c\u5728\u7406\u60f3\u5316\u7684\u4fe1\u53f7-\u566a\u58f0\u6570\u636e\u8bbe\u7f6e\u4e2d\uff0c\u91c7\u7528\u5e26\u6709\u6807\u7b7e\u566a\u58f0\u7684\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u6807\u7b7e\u566a\u58f0\u68af\u5ea6\u4e0b\u964d\u80fd\u591f\u6291\u5236\u566a\u58f0\u8bb0\u5fc6\u5316\uff0c\u9632\u6b62\u566a\u58f0\u4e3b\u5bfc\u5b66\u4e60\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u5feb\u901f\u4fe1\u53f7\u589e\u957f\u540c\u65f6\u63a7\u5236\u8fc7\u62df\u5408\uff0c\u4ece\u800c\u5728\u4f4e\u4fe1\u566a\u6bd4\u4e0b\u83b7\u5f97\u826f\u597d\u7684\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "\u5728\u4f4e\u4fe1\u566a\u6bd4\u73af\u5883\u4e0b\uff0c\u5f15\u5165\u6807\u7b7e\u566a\u58f0\u7684\u68af\u5ea6\u4e0b\u964d\u8bad\u7ec3\u76f8\u6bd4\u6807\u51c6\u68af\u5ea6\u4e0b\u964d\u80fd\u663e\u8457\u6539\u5584\u795e\u7ecf\u7f51\u7edc\u6cdb\u5316\u80fd\u529b\uff0c\u6807\u51c6\u68af\u5ea6\u4e0b\u964d\u4f1a\u8fc7\u62df\u5408\u566a\u58f0\u4e14\u6d4b\u8bd5\u8bef\u5dee\u5b58\u5728\u975e\u96f6\u4e0b\u754c\u3002"}}
{"id": "2510.17545", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17545", "abs": "https://arxiv.org/abs/2510.17545", "authors": ["Yichen Liu", "Yan Lin", "Shengnan Guo", "Zeyu Zhou", "Youfang Lin", "Huaiyu Wan"], "title": "TrajMamba: An Efficient and Semantic-rich Vehicle Trajectory Pre-training Model", "comment": "Accepted by NeurIPS2025", "summary": "Vehicle GPS trajectories record how vehicles move over time, storing valuable\ntravel semantics, including movement patterns and travel purposes. Learning\ntravel semantics effectively and efficiently is crucial for real-world\napplications of trajectory data, which is hindered by two major challenges.\nFirst, travel purposes are tied to the functions of the roads and\npoints-of-interest (POIs) involved in a trip. Such information is encoded in\ntextual addresses and descriptions and introduces heavy computational burden to\nmodeling. Second, real-world trajectories often contain redundant points, which\nharm both computational efficiency and trajectory embedding quality. To address\nthese challenges, we propose TrajMamba, a novel approach for efficient and\nsemantically rich vehicle trajectory learning. TrajMamba introduces a\nTraj-Mamba Encoder that captures movement patterns by jointly modeling both GPS\nand road perspectives of trajectories, enabling robust representations of\ncontinuous travel behaviors. It also incorporates a Travel Purpose-aware\nPre-training procedure to integrate travel purposes into the learned embeddings\nwithout introducing extra overhead to embedding calculation. To reduce\nredundancy in trajectories, TrajMamba features a Knowledge Distillation\nPre-training scheme to identify key trajectory points through a learnable mask\ngenerator and obtain effective compressed trajectory embeddings. Extensive\nexperiments on two real-world datasets and three downstream tasks show that\nTrajMamba outperforms state-of-the-art baselines in both efficiency and\naccuracy.", "AI": {"tldr": "TrajMamba\u662f\u4e00\u79cd\u7528\u4e8e\u8f66\u8f86GPS\u8f68\u8ff9\u5b66\u4e60\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u5efa\u6a21GPS\u548c\u9053\u8def\u89c6\u89d2\u6765\u6355\u6349\u79fb\u52a8\u6a21\u5f0f\uff0c\u5e76\u96c6\u6210\u65c5\u884c\u76ee\u7684\u611f\u77e5\u9884\u8bad\u7ec3\uff0c\u540c\u65f6\u4f7f\u7528\u77e5\u8bc6\u84b8\u998f\u51cf\u5c11\u8f68\u8ff9\u5197\u4f59\u70b9\uff0c\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u4e0a\u90fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u8f66\u8f86GPS\u8f68\u8ff9\u5305\u542b\u6709\u4ef7\u503c\u7684\u65c5\u884c\u8bed\u4e49\u4fe1\u606f\uff0c\u4f46\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u65c5\u884c\u76ee\u7684\u4e0e\u9053\u8def\u529f\u80fd\u548cPOI\u76f8\u5173\uff0c\u6587\u672c\u4fe1\u606f\u5904\u7406\u8ba1\u7b97\u8d1f\u62c5\u91cd\uff1b\u771f\u5b9e\u8f68\u8ff9\u5305\u542b\u5197\u4f59\u70b9\uff0c\u5f71\u54cd\u8ba1\u7b97\u6548\u7387\u548c\u5d4c\u5165\u8d28\u91cf\u3002", "method": "\u63d0\u51faTrajMamba\u65b9\u6cd5\uff0c\u5305\u62ec\uff1aTraj-Mamba\u7f16\u7801\u5668\u8054\u5408\u5efa\u6a21GPS\u548c\u9053\u8def\u89c6\u89d2\uff1b\u65c5\u884c\u76ee\u7684\u611f\u77e5\u9884\u8bad\u7ec3\u5c06\u65c5\u884c\u76ee\u7684\u96c6\u6210\u5230\u5d4c\u5165\u4e2d\uff1b\u77e5\u8bc6\u84b8\u998f\u9884\u8bad\u7ec3\u901a\u8fc7\u53ef\u5b66\u4e60\u63a9\u7801\u751f\u6210\u5668\u8bc6\u522b\u5173\u952e\u8f68\u8ff9\u70b9\uff0c\u83b7\u5f97\u538b\u7f29\u7684\u8f68\u8ff9\u5d4c\u5165\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u548c\u4e09\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cTrajMamba\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u4e0a\u90fd\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "TrajMamba\u80fd\u591f\u6709\u6548\u4e14\u9ad8\u6548\u5730\u5b66\u4e60\u8f66\u8f86\u8f68\u8ff9\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u89e3\u51b3\u4e86\u8f68\u8ff9\u6570\u636e\u5efa\u6a21\u4e2d\u7684\u8ba1\u7b97\u8d1f\u62c5\u548c\u5197\u4f59\u70b9\u95ee\u9898\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2510.17558", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17558", "abs": "https://arxiv.org/abs/2510.17558", "authors": ["Fran\u00e7ois Fleuret"], "title": "The Free Transformer", "comment": null, "summary": "We propose an extension of the decoder Transformer that conditions its\ngenerative process on random latent variables which are learned without\nsupervision thanks to a variational procedure. Experimental evaluations show\nthat allowing such a conditioning translates into substantial improvements on\ndownstream tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6269\u5c55\u7684\u89e3\u7801\u5668Transformer\uff0c\u901a\u8fc7\u53d8\u5206\u65b9\u6cd5\u5728\u65e0\u76d1\u7763\u6761\u4ef6\u4e0b\u5b66\u4e60\u968f\u673a\u6f5c\u53d8\u91cf\u6765\u8c03\u8282\u751f\u6210\u8fc7\u7a0b\u3002", "motivation": "\u901a\u8fc7\u5f15\u5165\u968f\u673a\u6f5c\u53d8\u91cf\u6765\u589e\u5f3a\u89e3\u7801\u5668Transformer\u7684\u751f\u6210\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u66f4\u597d\u5730\u5904\u7406\u4e0b\u6e38\u4efb\u52a1\u3002", "method": "\u6269\u5c55\u89e3\u7801\u5668Transformer\uff0c\u5f15\u5165\u968f\u673a\u6f5c\u53d8\u91cf\uff0c\u5e76\u4f7f\u7528\u53d8\u5206\u65b9\u6cd5\u8fdb\u884c\u65e0\u76d1\u7763\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u8fd9\u79cd\u6761\u4ef6\u8c03\u8282\u65b9\u6cd5\u5728\u4e0b\u6e38\u4efb\u52a1\u4e0a\u5e26\u6765\u4e86\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u901a\u8fc7\u65e0\u76d1\u7763\u5b66\u4e60\u968f\u673a\u6f5c\u53d8\u91cf\u6765\u8c03\u8282\u751f\u6210\u8fc7\u7a0b\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u89e3\u7801\u5668Transformer\u5728\u4e0b\u6e38\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002"}}
{"id": "2510.17562", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17562", "abs": "https://arxiv.org/abs/2510.17562", "authors": ["Dennis Wagner", "Arjun Nair", "Billy Joe Franks", "Justus Arweiler", "Aparna Muraleedharan", "Indra Jungjohann", "Fabian Hartung", "Mayank C. Ahuja", "Andriy Balinskyy", "Saurabh Varshneya", "Nabeel Hussain Syed", "Mayank Nagda", "Phillip Liznerski", "Steffen Reithermann", "Maja Rudolph", "Sebastian Vollmer", "Ralf Schulz", "Torsten Katz", "Stephan Mandt", "Michael Bortz", "Heike Leitte", "Daniel Neider", "Jakob Burger", "Fabian Jirasek", "Hans Hasse", "Sophie Fellenz", "Marius Kloft"], "title": "Formally Exploring Time-Series Anomaly Detection Evaluation Metrics", "comment": "73 pages, 13 figures", "summary": "Undetected anomalies in time series can trigger catastrophic failures in\nsafety-critical systems, such as chemical plant explosions or power grid\noutages. Although many detection methods have been proposed, their performance\nremains unclear because current metrics capture only narrow aspects of the task\nand often yield misleading results. We address this issue by introducing\nverifiable properties that formalize essential requirements for evaluating\ntime-series anomaly detection. These properties enable a theoretical framework\nthat supports principled evaluations and reliable comparisons. Analyzing 37\nwidely used metrics, we show that most satisfy only a few properties, and none\nsatisfy all, explaining persistent inconsistencies in prior results. To close\nthis gap, we propose LARM, a flexible metric that provably satisfies all\nproperties, and extend it to ALARM, an advanced variant meeting stricter\nrequirements.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\u6765\u8bc4\u4f30\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u6307\u6807\uff0c\u5206\u6790\u4e8637\u4e2a\u5e38\u7528\u6307\u6807\uff0c\u53d1\u73b0\u5927\u591a\u6570\u53ea\u6ee1\u8db3\u5c11\u6570\u5c5e\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u6ee1\u8db3\u6240\u6709\u5c5e\u6027\u7684\u65b0\u6307\u6807LARM\u548cALARM\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u672a\u68c0\u6d4b\u5f02\u5e38\u53ef\u80fd\u5bfc\u81f4\u5b89\u5168\u5173\u952e\u7cfb\u7edf\u7684\u707e\u96be\u6027\u6545\u969c\uff0c\u73b0\u6709\u8bc4\u4f30\u6307\u6807\u6027\u80fd\u4e0d\u660e\u786e\u4e14\u7ed3\u679c\u4e0d\u4e00\u81f4\u3002", "method": "\u5f15\u5165\u53ef\u9a8c\u8bc1\u5c5e\u6027\u6765\u5f62\u5f0f\u5316\u8bc4\u4f30\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u7684\u57fa\u672c\u8981\u6c42\uff0c\u5efa\u7acb\u7406\u8bba\u6846\u67b6\u652f\u6301\u539f\u5219\u6027\u8bc4\u4f30\u548c\u53ef\u9760\u6bd4\u8f83\u3002", "result": "\u5206\u679037\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u6307\u6807\uff0c\u663e\u793a\u5927\u591a\u6570\u53ea\u6ee1\u8db3\u5c11\u6570\u5c5e\u6027\uff0c\u6ca1\u6709\u6307\u6807\u6ee1\u8db3\u6240\u6709\u5c5e\u6027\uff0c\u89e3\u91ca\u4e86\u5148\u524d\u7ed3\u679c\u7684\u4e0d\u4e00\u81f4\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684LARM\u548cALARM\u6307\u6807\u80fd\u591f\u6ee1\u8db3\u6240\u6709\u8bc4\u4f30\u5c5e\u6027\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6307\u6807\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.17569", "categories": ["cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.17569", "abs": "https://arxiv.org/abs/2510.17569", "authors": ["Jyler Menard", "R. A. Mansbach"], "title": "Semi-supervised Latent Bayesian Optimization for Designing Antimicrobial Peptides", "comment": "19 pages, 9 figures", "summary": "Antimicrobial peptides (AMPs) are a promising class of therapeutics to treat\nbacterial infections. Discovering and designing such peptides is difficult\nbecause of the vast number of possible sequences of amino acids. Deep\ngenerative models, such as variational autoencoders, have shown value in\npeptide design due to their ability to model sequence space with a\ncontinuous-valued latent space. Although such models have already been used to\ngreat effect in biomolecular design, they still suffer from a lack of\ninterpretability and rigorous quantification of latent space quality as a\nsearch space. We investigate (1) whether further compression of the design\nspace via dimensionality reduction may facilitate optimization, (2) the\ninterpretability of the spaces, and (3) how organizing latent spaces with\nphysicochemical properties may improve the efficiency of optimizing\nantimicrobial activity. We find that further reduction of the latent space via\ndimensionality reduction can be advantageous when organizing the space with\nmore relevant information at data availability, that using the dimensionality\nreduction search space can be more interpretable, and that we can organize the\nlatent space with different physicochemical properties even at different\npercentages of available labels.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u964d\u7ef4\u6280\u672f\u6539\u8fdb\u6297\u83cc\u80bd\u8bbe\u8ba1\u7684\u6df1\u5ea6\u751f\u6210\u6a21\u578b\uff0c\u63d0\u9ad8\u6f5c\u5728\u7a7a\u95f4\u7684\u89e3\u91ca\u6027\u548c\u4f18\u5316\u6548\u7387\u3002", "motivation": "\u6297\u83cc\u80bd\u662f\u6cbb\u7597\u7ec6\u83cc\u611f\u67d3\u7684\u6709\u524d\u666f\u7597\u6cd5\uff0c\u4f46\u5e8f\u5217\u8bbe\u8ba1\u7a7a\u95f4\u5de8\u5927\u3002\u73b0\u6709\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u7f3a\u4e4f\u89e3\u91ca\u6027\uff0c\u4e14\u5bf9\u6f5c\u5728\u7a7a\u95f4\u8d28\u91cf\u7684\u91cf\u5316\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5efa\u6a21\u80bd\u5e8f\u5217\u7a7a\u95f4\uff0c\u901a\u8fc7\u964d\u7ef4\u6280\u672f\u8fdb\u4e00\u6b65\u538b\u7f29\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u5e76\u7528\u7406\u5316\u6027\u8d28\u7ec4\u7ec7\u6f5c\u5728\u7a7a\u95f4\u3002", "result": "\u53d1\u73b0\u964d\u7ef4\u540e\u7684\u641c\u7d22\u7a7a\u95f4\u66f4\u6709\u5229\u4e8e\u4f18\u5316\uff0c\u89e3\u91ca\u6027\u66f4\u5f3a\uff0c\u4e14\u80fd\u5728\u4e0d\u540c\u6807\u7b7e\u53ef\u7528\u6027\u4e0b\u7528\u7406\u5316\u6027\u8d28\u6709\u6548\u7ec4\u7ec7\u6f5c\u5728\u7a7a\u95f4\u3002", "conclusion": "\u901a\u8fc7\u964d\u7ef4\u548c\u7406\u5316\u6027\u8d28\u7ec4\u7ec7\u53ef\u4ee5\u663e\u8457\u6539\u8fdb\u6297\u83cc\u80bd\u8bbe\u8ba1\u7684\u6df1\u5ea6\u751f\u6210\u6a21\u578b\uff0c\u63d0\u9ad8\u4f18\u5316\u6548\u7387\u548c\u89e3\u91ca\u6027\u3002"}}
{"id": "2510.17584", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17584", "abs": "https://arxiv.org/abs/2510.17584", "authors": ["Ludi Li", "Junbin Mao", "Hanhe Lin", "Xu Tian", "Fang-Xiang Wu", "Jin Liu"], "title": "CEPerFed: Communication-Efficient Personalized Federated Learning for Multi-Pulse MRI Classification", "comment": null, "summary": "Multi-pulse magnetic resonance imaging (MRI) is widely utilized for clinical\npractice such as Alzheimer's disease diagnosis. To train a robust model for\nmulti-pulse MRI classification, it requires large and diverse data from various\nmedical institutions while protecting privacy by preventing raw data sharing\nacross institutions. Although federated learning (FL) is a feasible solution to\naddress this issue, it poses challenges of model convergence due to the effect\nof data heterogeneity and substantial communication overhead due to large\nnumbers of parameters transmitted within the model. To address these\nchallenges, we propose CEPerFed, a communication-efficient personalized FL\nmethod. It mitigates the effect of data heterogeneity by incorporating\nclient-side historical risk gradients and historical mean gradients to\ncoordinate local and global optimization. The former is used to weight the\ncontributions from other clients, enhancing the reliability of local updates,\nwhile the latter enforces consistency between local updates and the global\noptimization direction to ensure stable convergence across heterogeneous data\ndistributions. To address the high communication overhead, we propose a\nhierarchical SVD (HSVD) strategy that transmits only the most critical\ninformation required for model updates. Experiments on five classification\ntasks demonstrate the effectiveness of the CEPerFed method. The code will be\nreleased upon acceptance at https://github.com/LD0416/CEPerFed.", "AI": {"tldr": "\u63d0\u51fa\u4e86CEPerFed\u65b9\u6cd5\uff0c\u4e00\u79cd\u901a\u4fe1\u9ad8\u6548\u7684\u4e2a\u6027\u5316\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u8109\u51b2MRI\u5206\u7c7b\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u5f02\u6784\u6027\u548c\u901a\u4fe1\u5f00\u9500\u95ee\u9898\u3002", "motivation": "\u591a\u8109\u51b2MRI\u5728\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u8bad\u7ec3\u9c81\u68d2\u6a21\u578b\u9700\u8981\u5927\u91cf\u591a\u6837\u5316\u6570\u636e\uff0c\u540c\u65f6\u8981\u4fdd\u62a4\u9690\u79c1\u9632\u6b62\u539f\u59cb\u6570\u636e\u5171\u4eab\u3002\u8054\u90a6\u5b66\u4e60\u867d\u7136\u53ef\u884c\uff0c\u4f46\u9762\u4e34\u6570\u636e\u5f02\u6784\u6027\u5bfc\u81f4\u7684\u6a21\u578b\u6536\u655b\u95ee\u9898\u548c\u5927\u91cf\u53c2\u6570\u4f20\u8f93\u5e26\u6765\u7684\u901a\u4fe1\u5f00\u9500\u6311\u6218\u3002", "method": "CEPerFed\u901a\u8fc7\u7ed3\u5408\u5ba2\u6237\u7aef\u5386\u53f2\u98ce\u9669\u68af\u5ea6\u548c\u5386\u53f2\u5e73\u5747\u68af\u5ea6\u6765\u534f\u8c03\u5c40\u90e8\u548c\u5168\u5c40\u4f18\u5316\uff0c\u524d\u8005\u7528\u4e8e\u52a0\u6743\u5176\u4ed6\u5ba2\u6237\u7aef\u7684\u8d21\u732e\uff0c\u540e\u8005\u786e\u4fdd\u5c40\u90e8\u66f4\u65b0\u4e0e\u5168\u5c40\u4f18\u5316\u65b9\u5411\u4e00\u81f4\u3002\u540c\u65f6\u91c7\u7528\u5206\u5c42SVD\u7b56\u7565\u4f20\u8f93\u6700\u5173\u952e\u7684\u6a21\u578b\u66f4\u65b0\u4fe1\u606f\u3002", "result": "\u5728\u4e94\u4e2a\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86CEPerFed\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "CEPerFed\u6210\u529f\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u5f02\u6784\u6027\u548c\u901a\u4fe1\u5f00\u9500\u95ee\u9898\uff0c\u4e3a\u591a\u8109\u51b2MRI\u5206\u7c7b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.17650", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17650", "abs": "https://arxiv.org/abs/2510.17650", "authors": ["Athanasios Angelakis", "Amne Mousa", "Micah L. A. Heldeweg", "Laurens A. Biesheuvel", "Mark A. Haaksma", "Jasper M. Smit", "Pieter R. Tuinman", "Paul W. G. Elbers"], "title": "ZACH-ViT: A Zero-Token Vision Transformer with ShuffleStrides Data Augmentation for Robust Lung Ultrasound Classification", "comment": "14 pages, 6 figures, 2 tables. Primary subject: cs.LG (Machine\n  Learning) Cross-listed to: cs.CV (Computer Vision and Pattern Recognition),\n  eess.IV (Image and Video Processing). Code available at:\n  https://github.com/Bluesman79/ZACH-ViT Installation: pip install zachvit\n  Paper licensed under CC BY-NC-ND 4.0. Code released under Apache 2.0 License", "summary": "Differentiating cardiogenic pulmonary oedema (CPE) from non-cardiogenic and\nstructurally normal lungs in lung ultrasound (LUS) videos remains challenging\ndue to the high visual variability of non-cardiogenic inflammatory patterns\n(NCIP/ARDS-like), interstitial lung disease, and healthy lungs. This\nheterogeneity complicates automated classification as overlapping B-lines and\npleural artefacts are common. We introduce ZACH-ViT (Zero-token Adaptive\nCompact Hierarchical Vision Transformer), a 0.25 M-parameter Vision Transformer\nvariant that removes both positional embeddings and the [CLS] token, making it\nfully permutation-invariant and suitable for unordered medical image data. To\nenhance generalization, we propose ShuffleStrides Data Augmentation (SSDA),\nwhich permutes probe-view sequences and frame orders while preserving\nanatomical validity. ZACH-ViT was evaluated on 380 LUS videos from 95\ncritically ill patients against nine state-of-the-art baselines. Despite the\nheterogeneity of the non-cardiogenic group, ZACH-ViT achieved the highest\nvalidation and test ROC-AUC (0.80 and 0.79) with balanced sensitivity (0.60)\nand specificity (0.91), while all competing models collapsed to trivial\nclassification. It trains 1.35x faster than Minimal ViT (0.62M parameters) with\n2.5x fewer parameters, supporting real-time clinical deployment. These results\nshow that aligning architectural design with data structure can outperform\nscale in small-data medical imaging.", "AI": {"tldr": "ZACH-ViT\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u89c6\u89c9Transformer\uff0c\u7528\u4e8e\u533a\u5206\u5fc3\u6e90\u6027\u80ba\u6c34\u80bf\u4e0e\u975e\u5fc3\u6e90\u6027\u80ba\u90e8\u75be\u75c5\uff0c\u5728\u80ba\u8d85\u58f0\u89c6\u9891\u5206\u7c7b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4ec5\u97000.25M\u53c2\u6570\u3002", "motivation": "\u7531\u4e8e\u975e\u5fc3\u6e90\u6027\u708e\u75c7\u6a21\u5f0f\u3001\u95f4\u8d28\u6027\u80ba\u75c5\u548c\u5065\u5eb7\u80ba\u90e8\u5728\u89c6\u89c9\u4e0a\u5b58\u5728\u9ad8\u5ea6\u53d8\u5f02\u6027\uff0c\u4e14\u91cd\u53e0B\u7ebf\u548c\u80f8\u819c\u4f2a\u5f71\u5e38\u89c1\uff0c\u4f7f\u5f97\u80ba\u8d85\u58f0\u89c6\u9891\u4e2d\u533a\u5206\u5fc3\u6e90\u6027\u80ba\u6c34\u80bf\u4e0e\u5176\u4ed6\u60c5\u51b5\u53d8\u5f97\u56f0\u96be\u3002", "method": "\u63d0\u51faZACH-ViT\u6a21\u578b\uff0c\u53bb\u9664\u4f4d\u7f6e\u5d4c\u5165\u548c[CLS]\u6807\u8bb0\uff0c\u5b9e\u73b0\u5b8c\u5168\u6392\u5217\u4e0d\u53d8\u6027\uff1b\u5f15\u5165ShuffleStrides\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u5728\u4fdd\u6301\u89e3\u5256\u6709\u6548\u6027\u7684\u540c\u65f6\u6253\u4e71\u63a2\u5934\u89c6\u89d2\u5e8f\u5217\u548c\u5e27\u987a\u5e8f\u3002", "result": "\u5728380\u4e2a\u80ba\u8d85\u58f0\u89c6\u9891\u4e0a\u8bc4\u4f30\uff0cZACH-ViT\u83b7\u5f97\u6700\u9ad8\u9a8c\u8bc1\u548c\u6d4b\u8bd5ROC-AUC\uff080.80\u548c0.79\uff09\uff0c\u5e73\u8861\u7075\u654f\u5ea6\uff080.60\uff09\u548c\u7279\u5f02\u6027\uff080.91\uff09\uff0c\u800c\u5176\u4ed6\u6a21\u578b\u5747\u5931\u6548\u3002\u8bad\u7ec3\u901f\u5ea6\u6bd4Minimal ViT\u5feb1.35\u500d\uff0c\u53c2\u6570\u51cf\u5c112.5\u500d\u3002", "conclusion": "\u5c06\u67b6\u6784\u8bbe\u8ba1\u4e0e\u6570\u636e\u7ed3\u6784\u5bf9\u9f50\u53ef\u4ee5\u5728\u5c0f\u6570\u636e\u533b\u5b66\u6210\u50cf\u4e2d\u8d85\u8d8a\u89c4\u6a21\u6548\u5e94\uff0c\u652f\u6301\u5b9e\u65f6\u4e34\u5e8a\u90e8\u7f72\u3002"}}
{"id": "2510.17661", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17661", "abs": "https://arxiv.org/abs/2510.17661", "authors": ["Vaishnavi Visweswaraiah", "Tanvi Banerjee", "William Romine"], "title": "Handling Extreme Class Imbalance: Using GANs in Data Augmentation for Suicide Prediction", "comment": null, "summary": "Suicide prediction is the key for prevention, but real data with sufficient\npositive samples is rare and causes extreme class imbalance. We utilized\nmachine learning (ML) to build the model and deep learning (DL) techniques,\nlike Generative Adversarial Networks (GAN), to generate synthetic data samples\nto enhance the dataset. The initial dataset contained 656 samples, with only\nfour positive cases, prompting the need for data augmentation. A variety of\nmachine learning models, ranging from interpretable data models to black box\nalgorithmic models, were used. On real test data, Logistic Regression (LR)\nachieved a weighted precision of 0.99, a weighted recall of 0.85, and a\nweighted F1 score of 0.91; Random Forest (RF) showed 0.98, 0.99, and 0.99,\nrespectively; and Support Vector Machine (SVM) achieved 0.99, 0.76, and 0.86.\nLR and SVM correctly identified one suicide attempt case (sensitivity:1.0) and\nmisclassified LR(20) and SVM (31) non-attempts as attempts (specificity: 0.85 &\n0.76, respectively). RF identified 0 suicide attempt cases (sensitivity: 0.0)\nwith 0 false positives (specificity: 1.0). These results highlight the models'\neffectiveness, with GAN playing a key role in generating synthetic data to\nsupport suicide prevention modeling efforts.", "AI": {"tldr": "\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff08\u7279\u522b\u662fGAN\uff09\u6765\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u89e3\u51b3\u81ea\u6740\u9884\u6d4b\u4e2d\u6570\u636e\u6781\u5ea6\u4e0d\u5e73\u8861\u7684\u95ee\u9898\uff0c\u5e76\u5728\u771f\u5b9e\u6d4b\u8bd5\u6570\u636e\u4e0a\u53d6\u5f97\u4e86\u826f\u597d\u7684\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u81ea\u6740\u9884\u6d4b\u662f\u9884\u9632\u7684\u5173\u952e\uff0c\u4f46\u771f\u5b9e\u6570\u636e\u4e2d\u9633\u6027\u6837\u672c\u6781\u5c11\uff0c\u5bfc\u81f4\u4e25\u91cd\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u9700\u8981\u6570\u636e\u589e\u5f3a\u6765\u6539\u5584\u6a21\u578b\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08\u903b\u8f91\u56de\u5f52\u3001\u968f\u673a\u68ee\u6797\u3001\u652f\u6301\u5411\u91cf\u673a\uff09\u548c\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff08\u751f\u6210\u5bf9\u6297\u7f51\u7edcGAN\uff09\u751f\u6210\u5408\u6210\u6570\u636e\u6837\u672c\u6765\u589e\u5f3a\u6570\u636e\u96c6\u3002", "result": "\u5728\u771f\u5b9e\u6d4b\u8bd5\u6570\u636e\u4e0a\uff0c\u903b\u8f91\u56de\u5f52\u7684\u52a0\u6743\u7cbe\u786e\u5ea6\u4e3a0.99\u3001\u53ec\u56de\u73870.85\u3001F1\u5206\u65700.91\uff1b\u968f\u673a\u68ee\u6797\u4e3a0.98\u30010.99\u30010.99\uff1b\u652f\u6301\u5411\u91cf\u673a\u4e3a0.99\u30010.76\u30010.86\u3002LR\u548cSVM\u6b63\u786e\u8bc6\u522b\u4e861\u4e2a\u81ea\u6740\u5c1d\u8bd5\u6848\u4f8b\uff08\u654f\u611f\u5ea61.0\uff09\uff0cRF\u8bc6\u522b\u4e860\u4e2a\uff08\u654f\u611f\u5ea60.0\uff09\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u8bc1\u660e\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\uff0cGAN\u5728\u751f\u6210\u5408\u6210\u6570\u636e\u4ee5\u652f\u6301\u81ea\u6740\u9884\u9632\u5efa\u6a21\u5de5\u4f5c\u4e2d\u53d1\u6325\u4e86\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2510.17670", "categories": ["cs.LG", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.17670", "abs": "https://arxiv.org/abs/2510.17670", "authors": ["Yehonathan Refael", "Amit Aides", "Aviad Barzilai", "George Leifman", "Genady Beryozkin", "Vered Silverman", "Bolous Jaber", "Tomer Shekel"], "title": "On-the-Fly OVD Adaptation with FLAME: Few-shot Localization via Active Marginal-Samples Exploration", "comment": null, "summary": "Open-vocabulary object detection (OVD) models offer remarkable flexibility by\ndetecting objects from arbitrary text queries. However, their zero-shot\nperformance in specialized domains like Remote Sensing (RS) is often\ncompromised by the inherent ambiguity of natural language, limiting critical\ndownstream applications. For instance, an OVD model may struggle to distinguish\nbetween fine-grained classes such as \"fishing boat\" and \"yacht\" since their\nembeddings are similar and often inseparable. This can hamper specific user\ngoals, such as monitoring illegal fishing, by producing irrelevant detections.\nTo address this, we propose a cascaded approach that couples the broad\ngeneralization of a large pre-trained OVD model with a lightweight few-shot\nclassifier. Our method first employs the zero-shot model to generate\nhigh-recall object proposals. These proposals are then refined for high\nprecision by a compact classifier trained in real-time on only a handful of\nuser-annotated examples - drastically reducing the high costs of RS imagery\nannotation.The core of our framework is FLAME, a one-step active learning\nstrategy that selects the most informative samples for training. FLAME\nidentifies, on the fly, uncertain marginal candidates near the decision\nboundary using density estimation, followed by clustering to ensure sample\ndiversity. This efficient sampling technique achieves high accuracy without\ncostly full-model fine-tuning and enables instant adaptation, within less then\na minute, which is significantly faster than state-of-the-art alternatives.Our\nmethod consistently surpasses state-of-the-art performance on RS benchmarks,\nestablishing a practical and resource-efficient framework for adapting\nfoundation models to specific user needs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ea7\u8054\u65b9\u6cd5\uff0c\u5c06\u9884\u8bad\u7ec3\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u6a21\u578b\u4e0e\u8f7b\u91cf\u7ea7\u5c11\u6837\u672c\u5206\u7c7b\u5668\u7ed3\u5408\uff0c\u901a\u8fc7\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565FLAME\u5b9e\u73b0\u5feb\u901f\u9002\u5e94\uff0c\u5728\u9065\u611f\u56fe\u50cf\u68c0\u6d4b\u4e2d\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u6a21\u578b\u5728\u9065\u611f\u7b49\u4e13\u4e1a\u9886\u57df\u5b58\u5728\u81ea\u7136\u8bed\u8a00\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u96be\u4ee5\u533a\u5206\u7ec6\u7c92\u5ea6\u7c7b\u522b\uff08\u5982\u6e14\u8239\u548c\u6e38\u8247\uff09\uff0c\u5f71\u54cd\u4e0b\u6e38\u5e94\u7528\u6548\u679c\u3002", "method": "\u4f7f\u7528\u96f6-shot\u6a21\u578b\u751f\u6210\u9ad8\u53ec\u56de\u7387\u5019\u9009\u6846\uff0c\u7136\u540e\u901a\u8fc7\u5c11\u91cf\u7528\u6237\u6807\u6ce8\u6837\u672c\u8bad\u7ec3\u7684\u8f7b\u91cf\u5206\u7c7b\u5668\u8fdb\u884c\u7cbe\u70bc\uff1b\u6838\u5fc3\u662fFLAME\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\uff0c\u901a\u8fc7\u5bc6\u5ea6\u4f30\u8ba1\u548c\u805a\u7c7b\u9009\u62e9\u4fe1\u606f\u91cf\u6700\u5927\u7684\u6837\u672c\u3002", "result": "\u5728\u9065\u611f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u8d85\u8d8a\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5b9e\u73b0\u5feb\u901f\u9002\u5e94\uff08\u4e0d\u5230\u4e00\u5206\u949f\uff09\uff0c\u663e\u8457\u51cf\u5c11\u6807\u6ce8\u6210\u672c\u3002", "conclusion": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u5b9e\u7528\u4e14\u8d44\u6e90\u9ad8\u6548\u7684\u6846\u67b6\uff0c\u4f7f\u57fa\u7840\u6a21\u578b\u80fd\u591f\u5feb\u901f\u9002\u5e94\u7528\u6237\u7279\u5b9a\u9700\u6c42\u3002"}}
{"id": "2510.17690", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17690", "abs": "https://arxiv.org/abs/2510.17690", "authors": ["Xihong Su"], "title": "Efficient Algorithms for Mitigating Uncertainty and Risk in Reinforcement Learning", "comment": "Dissertation", "summary": "This dissertation makes three main contributions. First, We identify a new\nconnection between policy gradient and dynamic programming in MMDPs and propose\nthe Coordinate Ascent Dynamic Programming (CADP) algorithm to compute a Markov\npolicy that maximizes the discounted return averaged over the uncertain models.\nCADP adjusts model weights iteratively to guarantee monotone policy\nimprovements to a local maximum. Second, We establish sufficient and necessary\nconditions for the exponential ERM Bellman operator to be a contraction and\nprove the existence of stationary deterministic optimal policies for ERM-TRC\nand EVaR-TRC. We also propose exponential value iteration, policy iteration,\nand linear programming algorithms for computing optimal stationary policies for\nERM-TRC and EVaR-TRC. Third, We propose model-free Q-learning algorithms for\ncomputing policies with risk-averse objectives: ERM-TRC and EVaR-TRC. The\nchallenge is that Q-learning ERM Bellman may not be a contraction. Instead, we\nuse the monotonicity of Q-learning ERM Bellman operators to derive a rigorous\nproof that the ERM-TRC and the EVaR-TRC Q-learning algorithms converge to the\noptimal risk-averse value functions. The proposed Q-learning algorithms compute\nthe optimal stationary policy for ERM-TRC and EVaR-TRC.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e09\u4e2a\u4e3b\u8981\u8d21\u732e\uff1aCADP\u7b97\u6cd5\u8fde\u63a5\u7b56\u7565\u68af\u5ea6\u548c\u52a8\u6001\u89c4\u5212\uff0c\u5efa\u7acb\u4e86ERM Bellman\u7b97\u5b50\u7684\u6536\u7f29\u6761\u4ef6\u5e76\u63d0\u51fa\u4e86\u76f8\u5173\u7b97\u6cd5\uff0c\u4ee5\u53ca\u63d0\u51fa\u4e86\u7528\u4e8e\u98ce\u9669\u89c4\u907f\u76ee\u6807\u7684Q\u5b66\u4e60\u7b97\u6cd5\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u548c\u98ce\u9669\u89c4\u907f\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u591a\u6a21\u578bMDPs\u548c\u98ce\u9669\u654f\u611f\u76ee\u6807\u65f6\u7684\u7b97\u6cd5\u8bbe\u8ba1\u6311\u6218\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\uff1a1\uff09CADP\u7b97\u6cd5\u901a\u8fc7\u8fed\u4ee3\u8c03\u6574\u6a21\u578b\u6743\u91cd\u5b9e\u73b0\u5355\u8c03\u7b56\u7565\u6539\u8fdb\uff1b2\uff09\u5efa\u7acbERM Bellman\u7b97\u5b50\u7684\u6536\u7f29\u6761\u4ef6\u5e76\u8bbe\u8ba1\u503c\u8fed\u4ee3\u3001\u7b56\u7565\u8fed\u4ee3\u548c\u7ebf\u6027\u89c4\u5212\u7b97\u6cd5\uff1b3\uff09\u63d0\u51fa\u57fa\u4e8e\u5355\u8c03\u6027\u7684Q\u5b66\u4e60\u7b97\u6cd5\u5904\u7406\u98ce\u9669\u89c4\u907f\u76ee\u6807\u3002", "result": "\u7ed3\u679c\u8bc1\u660e\uff1aCADP\u80fd\u4fdd\u8bc1\u7b56\u7565\u5355\u8c03\u6539\u8fdb\u5230\u5c40\u90e8\u6700\u4f18\uff1bERM-TRC\u548cEVaR-TRC\u5b58\u5728\u786e\u5b9a\u6027\u6700\u4f18\u7b56\u7565\uff1b\u63d0\u51fa\u7684Q\u5b66\u4e60\u7b97\u6cd5\u80fd\u6536\u655b\u5230\u6700\u4f18\u98ce\u9669\u89c4\u907f\u503c\u51fd\u6570\u3002", "conclusion": "\u7ed3\u8bba\u662f\u6210\u529f\u5efa\u7acb\u4e86\u7b56\u7565\u68af\u5ea6\u4e0e\u52a8\u6001\u89c4\u5212\u7684\u65b0\u8054\u7cfb\uff0c\u89e3\u51b3\u4e86ERM Bellman\u7b97\u5b50\u7684\u6536\u7f29\u6027\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u4e86\u6536\u655b\u7684\u98ce\u9669\u89c4\u907fQ\u5b66\u4e60\u7b97\u6cd5\uff0c\u4e3a\u5904\u7406\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u548c\u98ce\u9669\u654f\u611f\u76ee\u6807\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2510.17709", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17709", "abs": "https://arxiv.org/abs/2510.17709", "authors": ["Akhil S Anand", "Shambhuraj Sawant", "Jasper Hoffmann", "Dirk Reinhardt", "Sebastien Gros"], "title": "Closing the Sim2Real Performance Gap in RL", "comment": null, "summary": "Sim2Real aims at training policies in high-fidelity simulation environments\nand effectively transferring them to the real world. Despite the developments\nof accurate simulators and Sim2Real RL approaches, the policies trained purely\nin simulation often suffer significant performance drops when deployed in real\nenvironments. This drop is referred to as the Sim2Real performance gap. Current\nSim2Real RL methods optimize the simulator accuracy and variability as proxies\nfor real-world performance. However, these metrics do not necessarily correlate\nwith the real-world performance of the policy as established theoretically and\nempirically in the literature. We propose a novel framework to address this\nissue by directly adapting the simulator parameters based on real-world\nperformance. We frame this problem as a bi-level RL framework: the inner-level\nRL trains a policy purely in simulation, and the outer-level RL adapts the\nsimulation model and in-sim reward parameters to maximize real-world\nperformance of the in-sim policy. We derive and validate in simple examples the\nmathematical tools needed to develop bi-level RL algorithms that close the\nSim2Real performance gap.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684Sim2Real\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5c42\u5f3a\u5316\u5b66\u4e60\u76f4\u63a5\u57fa\u4e8e\u771f\u5b9e\u4e16\u754c\u6027\u80fd\u8c03\u6574\u6a21\u62df\u5668\u53c2\u6570\uff0c\u4ee5\u7f29\u5c0fSim2Real\u6027\u80fd\u5dee\u8ddd\u3002", "motivation": "\u5f53\u524dSim2Real\u65b9\u6cd5\u901a\u8fc7\u4f18\u5316\u6a21\u62df\u5668\u7cbe\u5ea6\u548c\u53d8\u5f02\u6027\u4f5c\u4e3a\u771f\u5b9e\u4e16\u754c\u6027\u80fd\u7684\u4ee3\u7406\u6307\u6807\uff0c\u4f46\u8fd9\u4e9b\u6307\u6807\u4e0e\u7b56\u7565\u7684\u5b9e\u9645\u6027\u80fd\u4e0d\u4e00\u5b9a\u76f8\u5173\uff0c\u5bfc\u81f4\u6a21\u62df\u8bad\u7ec3\u7684\u7b56\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "method": "\u91c7\u7528\u53cc\u5c42\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff1a\u5185\u5c42RL\u5728\u6a21\u62df\u4e2d\u8bad\u7ec3\u7b56\u7565\uff0c\u5916\u5c42RL\u8c03\u6574\u6a21\u62df\u6a21\u578b\u548c\u6a21\u62df\u5956\u52b1\u53c2\u6570\uff0c\u4ee5\u6700\u5927\u5316\u6a21\u62df\u7b56\u7565\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u6027\u80fd\u3002", "result": "\u63a8\u5bfc\u5e76\u9a8c\u8bc1\u4e86\u5f00\u53d1\u80fd\u591f\u7f29\u5c0fSim2Real\u6027\u80fd\u5dee\u8ddd\u7684\u53cc\u5c42RL\u7b97\u6cd5\u6240\u9700\u7684\u6570\u5b66\u5de5\u5177\u3002", "conclusion": "\u8be5\u6846\u67b6\u76f4\u63a5\u57fa\u4e8e\u771f\u5b9e\u4e16\u754c\u6027\u80fd\u4f18\u5316\u6a21\u62df\u5668\u53c2\u6570\uff0c\u4e3a\u89e3\u51b3Sim2Real\u6027\u80fd\u5dee\u8ddd\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2510.17727", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17727", "abs": "https://arxiv.org/abs/2510.17727", "authors": ["Ege Beyazit", "KL Navaneet", "Prashant Mathur", "Roi Blanco", "Vidit Bansal", "Karim Bouyarmane"], "title": "Enabling Fine-Grained Operating Points for Black-Box LLMs", "comment": "35 pages", "summary": "Black-box Large Language Models (LLMs) provide practical and accessible\nalternatives to other machine learning methods, as they require minimal labeled\ndata and machine learning expertise to develop solutions for various decision\nmaking problems. However, for applications that need operating with constraints\non specific metrics (e.g., precision $\\geq$ 95%), decision making with\nblack-box LLMs remains unfavorable, due to their low numerical output\ncardinalities. This results in limited control over their operating points,\npreventing fine-grained adjustment of their decision making behavior. In this\npaper, we study using black-box LLMs as classifiers, focusing on efficiently\nimproving their operational granularity without performance loss. Specifically,\nwe first investigate the reasons behind their low-cardinality numerical outputs\nand show that they are biased towards generating rounded but informative\nverbalized probabilities. Then, we experiment with standard prompt engineering,\nuncertainty estimation and confidence elicitation techniques, and observe that\nthey do not effectively improve operational granularity without sacrificing\nperformance or increasing inference cost. Finally, we propose efficient\napproaches to significantly increase the number and diversity of available\noperating points. Our proposed approaches provide finer-grained operating\npoints and achieve comparable to or better performance than the benchmark\nmethods across 11 datasets and 3 LLMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u5982\u4f55\u63d0\u9ad8\u9ed1\u76d2\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u5206\u7c7b\u5668\u65f6\u7684\u64cd\u4f5c\u7c92\u5ea6\uff0c\u901a\u8fc7\u5206\u6790\u5176\u4f4e\u57fa\u6570\u6570\u503c\u8f93\u51fa\u7684\u539f\u56e0\uff0c\u5e76\u63d0\u51fa\u4e86\u6709\u6548\u65b9\u6cd5\u6765\u663e\u8457\u589e\u52a0\u53ef\u7528\u64cd\u4f5c\u70b9\u6570\u91cf\uff0c\u5b9e\u73b0\u66f4\u7cbe\u7ec6\u7684\u51b3\u7b56\u884c\u4e3a\u8c03\u6574\u3002", "motivation": "\u9ed1\u76d2LLMs\u5728\u9700\u8981\u7279\u5b9a\u6307\u6807\u7ea6\u675f\u7684\u5e94\u7528\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u5176\u6570\u503c\u8f93\u51fa\u57fa\u6570\u4f4e\uff0c\u9650\u5236\u4e86\u64cd\u4f5c\u70b9\u7684\u63a7\u5236\u80fd\u529b\uff0c\u65e0\u6cd5\u8fdb\u884c\u7ec6\u7c92\u5ea6\u7684\u51b3\u7b56\u884c\u4e3a\u8c03\u6574\u3002", "method": "\u9996\u5148\u5206\u6790LLMs\u4f4e\u57fa\u6570\u6570\u503c\u8f93\u51fa\u7684\u539f\u56e0\uff0c\u53d1\u73b0\u5176\u504f\u5411\u751f\u6210\u56db\u820d\u4e94\u5165\u4f46\u4fe1\u606f\u4e30\u5bcc\u7684\u8bed\u8a00\u5316\u6982\u7387\uff1b\u7136\u540e\u5b9e\u9a8c\u6807\u51c6\u63d0\u793a\u5de5\u7a0b\u3001\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u548c\u7f6e\u4fe1\u5ea6\u6fc0\u53d1\u6280\u672f\uff1b\u6700\u540e\u63d0\u51fa\u6709\u6548\u65b9\u6cd5\u6765\u663e\u8457\u589e\u52a0\u53ef\u7528\u64cd\u4f5c\u70b9\u6570\u91cf\u548c\u591a\u6837\u6027\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u572811\u4e2a\u6570\u636e\u96c6\u548c3\u4e2aLLMs\u4e0a\u63d0\u4f9b\u4e86\u66f4\u7ec6\u7c92\u5ea6\u7684\u64cd\u4f5c\u70b9\uff0c\u5e76\u5b9e\u73b0\u4e86\u4e0e\u57fa\u51c6\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u597d\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u63d0\u9ad8\u9ed1\u76d2LLMs\u7684\u64cd\u4f5c\u7c92\u5ea6\uff0c\u53ef\u4ee5\u5728\u4e0d\u635f\u5931\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u66f4\u7cbe\u7ec6\u7684\u51b3\u7b56\u884c\u4e3a\u63a7\u5236\uff0c\u4e3a\u9700\u8981\u7279\u5b9a\u6307\u6807\u7ea6\u675f\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.17756", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17756", "abs": "https://arxiv.org/abs/2510.17756", "authors": ["Younghyun Koo", "Maryam Rahnemoonfar"], "title": "Prediction of Sea Ice Velocity and Concentration in the Arctic Ocean using Physics-informed Neural Network", "comment": "49 pages, 7 figures, submitted to Environmental Modelling & Software", "summary": "As an increasing amount of remote sensing data becomes available in the\nArctic Ocean, data-driven machine learning (ML) techniques are becoming widely\nused to predict sea ice velocity (SIV) and sea ice concentration (SIC).\nHowever, fully data-driven ML models have limitations in generalizability and\nphysical consistency due to their excessive reliance on the quantity and\nquality of training data. In particular, as Arctic sea ice entered a new phase\nwith thinner ice and accelerated melting, there is a possibility that an ML\nmodel trained with historical sea ice data cannot fully represent the\ndynamically changing sea ice conditions in the future. In this study, we\ndevelop physics-informed neural network (PINN) strategies to integrate physical\nknowledge of sea ice into the ML model. Based on the Hierarchical\nInformation-sharing U-net (HIS-Unet) architecture, we incorporate the physics\nloss function and the activation function to produce physically plausible SIV\nand SIC outputs. Our PINN model outperforms the fully data-driven model in the\ndaily predictions of SIV and SIC, even when trained with a small number of\nsamples. The PINN approach particularly improves SIC predictions in melting and\nearly freezing seasons and near fast-moving ice regions.", "AI": {"tldr": "\u5f00\u53d1\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc(PINN)\u7b56\u7565\uff0c\u5c06\u6d77\u51b0\u7269\u7406\u77e5\u8bc6\u6574\u5408\u5230\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e2d\uff0c\u4ee5\u6539\u8fdb\u5317\u6781\u6d77\u51b0\u901f\u5ea6\u548c\u6d53\u5ea6\u7684\u9884\u6d4b\u3002", "motivation": "\u7eaf\u6570\u636e\u9a71\u52a8\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u6cdb\u5316\u6027\u548c\u7269\u7406\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u7279\u522b\u662f\u5728\u5317\u6781\u6d77\u51b0\u53d8\u8584\u548c\u52a0\u901f\u878d\u5316\u7684\u65b0\u9636\u6bb5\uff0c\u5386\u53f2\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u53ef\u80fd\u65e0\u6cd5\u5145\u5206\u4ee3\u8868\u672a\u6765\u52a8\u6001\u53d8\u5316\u7684\u6d77\u51b0\u6761\u4ef6\u3002", "method": "\u57fa\u4e8e\u5206\u5c42\u4fe1\u606f\u5171\u4eabU-net\u67b6\u6784\uff0c\u5f15\u5165\u7269\u7406\u635f\u5931\u51fd\u6570\u548c\u6fc0\u6d3b\u51fd\u6570\uff0c\u751f\u6210\u7269\u7406\u4e0a\u5408\u7406\u7684\u6d77\u51b0\u901f\u5ea6\u548c\u6d53\u5ea6\u8f93\u51fa\u3002", "result": "PINN\u6a21\u578b\u5728\u6d77\u51b0\u901f\u5ea6\u548c\u6d53\u5ea6\u7684\u65e5\u5e38\u9884\u6d4b\u4e2d\u4f18\u4e8e\u7eaf\u6570\u636e\u9a71\u52a8\u6a21\u578b\uff0c\u5373\u4f7f\u4f7f\u7528\u5c11\u91cf\u6837\u672c\u8bad\u7ec3\u4e5f\u80fd\u8868\u73b0\u826f\u597d\uff0c\u7279\u522b\u662f\u5728\u878d\u5316\u548c\u65e9\u671f\u51bb\u7ed3\u5b63\u8282\u4ee5\u53ca\u5feb\u901f\u79fb\u52a8\u51b0\u533a\u663e\u8457\u6539\u5584\u4e86\u6d77\u51b0\u6d53\u5ea6\u9884\u6d4b\u3002", "conclusion": "\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u6574\u5408\u7269\u7406\u77e5\u8bc6\uff0c\u63d0\u9ad8\u6d77\u51b0\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u7269\u7406\u4e00\u81f4\u6027\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u7a00\u7f3a\u6216\u6d77\u51b0\u6761\u4ef6\u5feb\u901f\u53d8\u5316\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2510.17772", "categories": ["cs.LG", "stat.AP", "I.5.1"], "pdf": "https://arxiv.org/pdf/2510.17772", "abs": "https://arxiv.org/abs/2510.17772", "authors": ["Ryan A. Robinett", "Sophia A. Madejski", "Kyle Ruark", "Samantha J. Riesenfeld", "Lorenzo Orecchia"], "title": "Atlas-based Manifold Representations for Interpretable Riemannian Machine Learning", "comment": null, "summary": "Despite the popularity of the manifold hypothesis, current manifold-learning\nmethods do not support machine learning directly on the latent $d$-dimensional\ndata manifold, as they primarily aim to perform dimensionality reduction into\n$\\mathbb{R}^D$, losing key manifold features when the embedding dimension $D$\napproaches $d$.\n  On the other hand, methods that directly learn the latent manifold as a\ndifferentiable atlas have been relatively underexplored.\n  In this paper, we aim to give a proof of concept of the effectiveness and\npotential of atlas-based methods. To this end, we implement a generic data\nstructure to maintain a differentiable atlas that enables Riemannian\noptimization over the manifold. We complement this with an unsupervised\nheuristic that learns a differentiable atlas from point cloud data. We\nexperimentally demonstrate that this approach has advantages in terms of\nefficiency and accuracy in selected settings. Moreover, in a supervised\nclassification task over the Klein bottle and in RNA velocity analysis of\nhematopoietic data, we showcase the improved interpretability and robustness of\nour approach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5fae\u5206\u56fe\u518c\u7684\u6d41\u5f62\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ef4\u62a4\u53ef\u5fae\u5206\u56fe\u518c\u5b9e\u73b0\u6d41\u5f62\u4e0a\u7684\u9ece\u66fc\u4f18\u5316\uff0c\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u65b9\u9762\u5177\u6709\u4f18\u52bf\u3002", "motivation": "\u5f53\u524d\u6d41\u5f62\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u8fdb\u884c\u964d\u7ef4\u5230\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\uff0c\u5f53\u5d4c\u5165\u7ef4\u5ea6\u63a5\u8fd1\u6d41\u5f62\u5185\u5728\u7ef4\u5ea6\u65f6\u4f1a\u4e22\u5931\u5173\u952e\u7279\u5f81\u3002\u800c\u76f4\u63a5\u5b66\u4e60\u6d41\u5f62\u4f5c\u4e3a\u5fae\u5206\u56fe\u518c\u7684\u65b9\u6cd5\u76f8\u5bf9\u8f83\u5c11\u88ab\u63a2\u7d22\u3002", "method": "\u5b9e\u73b0\u4e86\u4e00\u4e2a\u901a\u7528\u6570\u636e\u7ed3\u6784\u6765\u7ef4\u62a4\u53ef\u5fae\u5206\u56fe\u518c\uff0c\u652f\u6301\u6d41\u5f62\u4e0a\u7684\u9ece\u66fc\u4f18\u5316\uff0c\u5e76\u7ed3\u5408\u65e0\u76d1\u7763\u542f\u53d1\u5f0f\u65b9\u6cd5\u4ece\u70b9\u4e91\u6570\u636e\u5b66\u4e60\u5fae\u5206\u56fe\u518c\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u9009\u5b9a\u573a\u666f\u4e0b\u5177\u6709\u6548\u7387\u548c\u51c6\u786e\u6027\u4f18\u52bf\u3002\u5728Klein\u74f6\u4e0a\u7684\u76d1\u7763\u5206\u7c7b\u4efb\u52a1\u548c\u9020\u8840\u6570\u636e\u7684RNA\u901f\u5ea6\u5206\u6790\u4e2d\uff0c\u5c55\u793a\u4e86\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u57fa\u4e8e\u56fe\u518c\u7684\u65b9\u6cd5\u5728\u6d41\u5f62\u5b66\u4e60\u65b9\u9762\u5177\u6709\u6709\u6548\u6027\u548c\u6f5c\u529b\uff0c\u4e3a\u76f4\u63a5\u5904\u7406\u6d41\u5f62\u6570\u636e\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2510.17786", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17786", "abs": "https://arxiv.org/abs/2510.17786", "authors": ["Adam Stecklov", "Noah El Rimawi-Fine", "Mathieu Blanchette"], "title": "Inference-Time Compute Scaling For Flow Matching", "comment": null, "summary": "Allocating extra computation at inference time has recently improved sample\nquality in large language models and diffusion-based image generation. In\nparallel, Flow Matching (FM) has gained traction in language, vision, and\nscientific domains, but inference-time scaling methods for it remain\nunder-explored. Concurrently, Kim et al., 2025 approach this problem but\nreplace the linear interpolant with a non-linear variance-preserving (VP)\ninterpolant at inference, sacrificing FM's efficient and straight sampling.\nAdditionally, inference-time compute scaling for flow matching has only been\napplied to visual tasks, like image generation. We introduce novel\ninference-time scaling procedures for FM that preserve the linear interpolant\nduring sampling. Evaluations of our method on image generation, and for the\nfirst time (to the best of our knowledge), unconditional protein generation,\nshow that I) sample quality consistently improves as inference compute\nincreases, and II) flow matching inference-time scaling can be applied to\nscientific domains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u5728\u63a8\u7406\u65f6\u4fdd\u6301\u7ebf\u6027\u63d2\u503c\u7684\u6d41\u5339\u914d\u7f29\u653e\u65b9\u6cd5\uff0c\u9996\u6b21\u5e94\u7528\u4e8e\u65e0\u6761\u4ef6\u86cb\u767d\u8d28\u751f\u6210\uff0c\u8bc1\u660e\u968f\u7740\u63a8\u7406\u8ba1\u7b97\u589e\u52a0\uff0c\u6837\u672c\u8d28\u91cf\u6301\u7eed\u63d0\u5347\u3002", "motivation": "\u5f53\u524d\u6d41\u5339\u914d\u65b9\u6cd5\u5728\u63a8\u7406\u65f6\u7f29\u653e\u7814\u7a76\u4e0d\u8db3\uff0c\u73b0\u6709\u65b9\u6cd5\u727a\u7272\u4e86\u6d41\u5339\u914d\u7684\u9ad8\u6548\u76f4\u7ebf\u91c7\u6837\u7279\u6027\uff0c\u4e14\u4ec5\u5e94\u7528\u4e8e\u89c6\u89c9\u4efb\u52a1\uff0c\u7f3a\u4e4f\u79d1\u5b66\u9886\u57df\u7684\u5e94\u7528\u9a8c\u8bc1\u3002", "method": "\u5f00\u53d1\u4e86\u4fdd\u6301\u7ebf\u6027\u63d2\u503c\u7684\u63a8\u7406\u65f6\u7f29\u653e\u7a0b\u5e8f\uff0c\u5728\u91c7\u6837\u8fc7\u7a0b\u4e2d\u4e0d\u6539\u53d8\u539f\u59cb\u6d41\u5339\u914d\u7684\u7ebf\u6027\u63d2\u503c\u7279\u6027\u3002", "result": "\u5728\u56fe\u50cf\u751f\u6210\u548c\u65e0\u6761\u4ef6\u86cb\u767d\u8d28\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u968f\u7740\u63a8\u7406\u8ba1\u7b97\u589e\u52a0\uff0c\u6837\u672c\u8d28\u91cf\u6301\u7eed\u6539\u5584\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u53ef\u5e94\u7528\u4e8e\u79d1\u5b66\u9886\u57df\u3002", "conclusion": "\u6d41\u5339\u914d\u63a8\u7406\u65f6\u7f29\u653e\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u6837\u672c\u8d28\u91cf\uff0c\u4e14\u9002\u7528\u4e8e\u79d1\u5b66\u9886\u57df\uff0c\u4e3a\u6d41\u5339\u914d\u5728\u66f4\u5e7f\u6cdb\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2510.15892", "categories": ["q-fin.GN", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15892", "abs": "https://arxiv.org/abs/2510.15892", "authors": ["Agus Sudjianto", "Sandi Setiawan"], "title": "Geometric Dynamics of Consumer Credit Cycles: A Multivector-based Linear-Attention Framework for Explanatory Economic Analysis", "comment": "29 pages, 7 figures", "summary": "This study introduces geometric algebra to decompose credit system\nrelationships into their projective (correlation-like) and rotational\n(feedback-spiral) components. We represent economic states as multi-vectors in\nClifford algebra, where bivector elements capture the rotational coupling\nbetween unemployment, consumption, savings, and credit utilization. This\nmathematical framework reveals interaction patterns invisible to conventional\nanalysis: when unemployment and credit contraction enter simultaneous feedback\nloops, their geometric relationship shifts from simple correlation to dangerous\nrotational dynamics that characterize systemic crises.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f15\u5165\u51e0\u4f55\u4ee3\u6570\u5c06\u4fe1\u8d37\u7cfb\u7edf\u5173\u7cfb\u5206\u89e3\u4e3a\u6295\u5f71\uff08\u7c7b\u4f3c\u76f8\u5173\u6027\uff09\u548c\u65cb\u8f6c\uff08\u53cd\u9988\u87ba\u65cb\uff09\u5206\u91cf\uff0c\u63ed\u793a\u4f20\u7edf\u5206\u6790\u65e0\u6cd5\u770b\u5230\u7684\u7cfb\u7edf\u6027\u5371\u673a\u52a8\u6001\u6a21\u5f0f\u3002", "motivation": "\u4f20\u7edf\u7ecf\u6d4e\u5206\u6790\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u4fe1\u8d37\u7cfb\u7edf\u4e2d\u7684\u590d\u6742\u53cd\u9988\u87ba\u65cb\u548c\u65cb\u8f6c\u52a8\u6001\uff0c\u7279\u522b\u662f\u5728\u5931\u4e1a\u548c\u4fe1\u8d37\u6536\u7f29\u540c\u65f6\u8fdb\u5165\u53cd\u9988\u5faa\u73af\u65f6\uff0c\u9700\u8981\u65b0\u7684\u6570\u5b66\u6846\u67b6\u6765\u63ed\u793a\u8fd9\u4e9b\u9690\u85cf\u7684\u5371\u673a\u6a21\u5f0f\u3002", "method": "\u4f7f\u7528Clifford\u51e0\u4f55\u4ee3\u6570\uff0c\u5c06\u7ecf\u6d4e\u72b6\u6001\u8868\u793a\u4e3a\u591a\u5411\u91cf\uff0c\u5176\u4e2d\u53cc\u5411\u91cf\u5143\u7d20\u6355\u6349\u5931\u4e1a\u3001\u6d88\u8d39\u3001\u50a8\u84c4\u548c\u4fe1\u8d37\u5229\u7528\u4e4b\u95f4\u7684\u65cb\u8f6c\u8026\u5408\u5173\u7cfb\u3002", "result": "\u8be5\u6570\u5b66\u6846\u67b6\u63ed\u793a\u4e86\u4f20\u7edf\u5206\u6790\u4e2d\u4e0d\u53ef\u89c1\u7684\u4ea4\u4e92\u6a21\u5f0f\uff1a\u5f53\u5931\u4e1a\u548c\u4fe1\u8d37\u6536\u7f29\u540c\u65f6\u8fdb\u5165\u53cd\u9988\u5faa\u73af\u65f6\uff0c\u5b83\u4eec\u7684\u51e0\u4f55\u5173\u7cfb\u4ece\u7b80\u5355\u76f8\u5173\u6027\u8f6c\u53d8\u4e3a\u5371\u9669\u7684\u65cb\u8f6c\u52a8\u6001\uff0c\u8fd9\u8868\u5f81\u4e86\u7cfb\u7edf\u6027\u5371\u673a\u3002", "conclusion": "\u51e0\u4f55\u4ee3\u6570\u4e3a\u5206\u6790\u4fe1\u8d37\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u6570\u5b66\u5de5\u5177\uff0c\u80fd\u591f\u63ed\u793a\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u53d1\u73b0\u7684\u7cfb\u7edf\u6027\u5371\u673a\u52a8\u6001\u6a21\u5f0f\uff0c\u7279\u522b\u662f\u65cb\u8f6c\u8026\u5408\u5173\u7cfb\u5728\u5371\u673a\u5f62\u6210\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002"}}
