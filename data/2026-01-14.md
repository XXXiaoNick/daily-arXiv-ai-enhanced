<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 72]
- [eess.SY](#eess.SY) [Total: 16]
- [econ.EM](#econ.EM) [Total: 5]
- [cs.AI](#cs.AI) [Total: 62]
- [cs.CY](#cs.CY) [Total: 4]
- [cs.LG](#cs.LG) [Total: 69]
- [q-fin.RM](#q-fin.RM) [Total: 1]
- [q-fin.GN](#q-fin.GN) [Total: 1]
- [math.OC](#math.OC) [Total: 17]
- [stat.ML](#stat.ML) [Total: 6]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [q-fin.PM](#q-fin.PM) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [EmbeddingRWKV: State-Centric Retrieval with Reusable States](https://arxiv.org/abs/2601.07861)
*Haowen Hou,Jie Yang*

Main category: cs.CL

TL;DR: 提出State-Centric Retrieval统一检索范式，通过共享"状态"连接嵌入模型和重排序器，显著提升RAG系统效率


<details>
  <summary>Details</summary>
Motivation: 传统RAG两阶段流水线（嵌入模型+重排序器）存在效率低下问题，因为阶段间缺乏信息共享，导致大量冗余计算

Method: 1) 状态表示学习：微调RWKV-based LLM为EmbeddingRWKV，作为统一模型同时充当嵌入模型和状态骨干；2) 基于状态的重排序器：利用预计算状态，仅处理查询token，实现与文档长度解耦的推理

Result: 重排序阶段获得5.4-44.8倍加速；通过均匀层选择策略，仅用25%层数即可保持98.62%性能；在保持高质量检索和重排序结果的同时显著提升系统效率

Conclusion: State-Centric Retrieval通过状态共享机制有效解决了传统RAG系统的效率瓶颈，实现了高质量检索与高效计算的平衡

Abstract: Current Retrieval-Augmented Generation (RAG) systems typically employ a traditional two-stage pipeline: an embedding model for initial retrieval followed by a reranker for refinement. However, this paradigm suffers from significant inefficiency due to the lack of shared information between stages, leading to substantial redundant computation. To address this limitation, we propose \textbf{State-Centric Retrieval}, a unified retrieval paradigm that utilizes "states" as a bridge to connect embedding models and rerankers. First, we perform state representation learning by fine-tuning an RWKV-based LLM, transforming it into \textbf{EmbeddingRWKV}, a unified model that serves as both an embedding model and a state backbone for extracting compact, reusable states. Building upon these reusable states, we further design a state-based reranker to fully leverage precomputed information. During reranking, the model processes only query tokens, decoupling inference cost from document length and yielding a 5.4$\times$--44.8$\times$ speedup. Furthermore, we observe that retaining all intermediate layer states is unnecessary; with a uniform layer selection strategy, our model maintains 98.62\% of full-model performance using only 25\% of the layers. Extensive experiments demonstrate that State-Centric Retrieval achieves high-quality retrieval and reranking results while significantly enhancing overall system efficiency. Code is available at \href{https://github.com/howard-hou/EmbeddingRWKV}{our GitHub repository}.

</details>


### [2] [A Human-Centric Pipeline for Aligning Large Language Models with Chinese Medical Ethics](https://arxiv.org/abs/2601.07954)
*Haoan Jin,Han Ying,Jiacheng Ji,Hanhui Xu,Mengyue Wu*

Main category: cs.CL

TL;DR: 提出了MedES基准和guardian-in-the-loop框架，用于在中文医疗伦理场景下对齐大语言模型，通过监督微调和领域特定偏好优化，使7B参数模型在伦理任务上超越更大基线模型。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在医疗任务中应用广泛，但在复杂现实场景下与医疗伦理的细致要求对齐仍未被充分探索，特别是在中文医疗伦理领域。

Method: 1) 构建MedES基准：从260个权威中文医疗、伦理和法律来源创建动态、场景中心的基准；2) guardian-in-the-loop框架：使用自动化评估器（专家标注数据训练，领域内准确率超97%）生成针对性提示和结构化伦理反馈；3) 通过监督微调和领域特定偏好优化对齐7B参数模型。

Result: 在中文医疗伦理背景下，对齐后的模型在核心伦理任务上显著优于更大的基线模型，在质量和综合评估指标上均有提升。

Conclusion: 为中文医疗领域提供了实用且可适应的LLM伦理对齐框架，并表明通过模块化替换底层规范语料库，类似对齐流程可在其他法律和文化环境中实例化。

Abstract: Recent advances in large language models have enabled their application to a range of healthcare tasks. However, aligning LLMs with the nuanced demands of medical ethics, especially under complex real world scenarios, remains underexplored. In this work, we present MedES, a dynamic, scenario-centric benchmark specifically constructed from 260 authoritative Chinese medical, ethical, and legal sources to reflect the challenges in clinical decision-making. To facilitate model alignment, we introduce a guardian-in-the-loop framework that leverages a dedicated automated evaluator (trained on expert-labeled data and achieving over 97% accuracy within our domain) to generate targeted prompts and provide structured ethical feedback. Using this pipeline, we align a 7B-parameter LLM through supervised fine-tuning and domain-specific preference optimization. Experimental results, conducted entirely within the Chinese medical ethics context, demonstrate that our aligned model outperforms notably larger baselines on core ethical tasks, with observed improvements in both quality and composite evaluation metrics. Our work offers a practical and adaptable framework for aligning LLMs with medical ethics in the Chinese healthcare domain, and suggests that similar alignment pipelines may be instantiated in other legal and cultural environments through modular replacement of the underlying normative corpus.

</details>


### [3] [Knowing But Not Doing: Convergent Morality and Divergent Action in LLMs](https://arxiv.org/abs/2601.07972)
*Jen-tse Huang,Jiantong Qin,Xueli Qiu,Sharon Levy,Michelle R. Kaufman,Mark Dredze*

Main category: cs.CL

TL;DR: LLMs在价值对齐场景中表现出高度一致但存在知行差距，与人类的价值决策模式形成鲜明对比


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型如何在现实决策情境中表征和实施人类价值观，探索价值对齐的实际效果

Method: 使用ValAct-15k数据集（3000个Reddit建议寻求场景），基于Schwartz基本人类价值观理论评估10个前沿LLM和55名人类参与者，比较场景决策和传统问卷结果

Result: LLMs在场景决策中表现出近乎完美的跨模型一致性（r≈1.0），而人类则表现出广泛变异性（r∈[-0.79,0.98]）；两者都存在自我报告价值与实际行为之间的差距（r=0.4,0.3）；当被要求"持有"特定价值观时，LLMs性能下降6.6%

Conclusion: 对齐训练虽然产生了规范性的价值趋同，但未能消除类似人类的知行不一致问题，LLMs存在角色扮演厌恶

Abstract: Value alignment is central to the development of safe and socially compatible artificial intelligence. However, how Large Language Models (LLMs) represent and enact human values in real-world decision contexts remains under-explored. We present ValAct-15k, a dataset of 3,000 advice-seeking scenarios derived from Reddit, designed to elicit ten values defined by Schwartz Theory of Basic Human Values. Using both the scenario-based questions and the traditional value questionnaire, we evaluate ten frontier LLMs (five from U.S. companies, five from Chinese ones) and human participants ($n = 55$). We find near-perfect cross-model consistency in scenario-based decisions (Pearson $r \approx 1.0$), contrasting sharply with the broad variability observed among humans ($r \in [-0.79, 0.98]$). Yet, both humans and LLMs show weak correspondence between self-reported and enacted values ($r = 0.4, 0.3$), revealing a systematic knowledge-action gap. When instructed to "hold" a specific value, LLMs' performance declines up to $6.6%$ compared to merely selecting the value, indicating a role-play aversion. These findings suggest that while alignment training yields normative value convergence, it does not eliminate the human-like incoherence between knowing and acting upon values.

</details>


### [4] [Explaining Generalization of AI-Generated Text Detectors Through Linguistic Analysis](https://arxiv.org/abs/2601.07974)
*Yuxi Xia,Kinga Stańczak,Benjamin Roth*

Main category: cs.CL

TL;DR: AI文本检测器在跨提示、跨模型、跨领域泛化时性能下降，本文通过系统性的语言特征分析揭示了泛化能力与特定语言特征（如时态使用和代词频率）的相关性。


<details>
  <summary>Details</summary>
Motivation: 现有AI文本检测器在领域内基准测试中表现良好，但在面对不同生成条件（如未见过的提示、模型家族或领域）时泛化能力不足。虽然先前研究已报告了这些泛化差距，但对根本原因的理解有限。

Method: 构建了一个包含6种提示策略、7个大语言模型和4个领域数据集的综合基准，生成了多样化的人类和AI生成文本。在不同生成设置上微调基于分类的检测器，并评估其跨提示、跨模型和跨数据集的泛化能力。通过计算80个语言特征在训练和测试条件之间的特征偏移与泛化准确率的相关性来解释性能差异。

Result: 分析表明，特定检测器和评估条件的泛化性能与语言特征（如时态使用和代词频率）显著相关，揭示了语言特征偏移是影响检测器泛化能力的重要因素。

Conclusion: AI文本检测器的泛化能力受到训练和测试条件之间语言特征差异的影响，特定语言特征（如时态和代词使用）的变化可以解释检测器在不同生成条件下的性能变化，这为改进检测器的鲁棒性提供了重要见解。

Abstract: AI-text detectors achieve high accuracy on in-domain benchmarks, but often struggle to generalize across different generation conditions such as unseen prompts, model families, or domains. While prior work has reported these generalization gaps, there are limited insights about the underlying causes. In this work, we present a systematic study aimed at explaining generalization behavior through linguistic analysis. We construct a comprehensive benchmark that spans 6 prompting strategies, 7 large language models (LLMs), and 4 domain datasets, resulting in a diverse set of human- and AI-generated texts. Using this dataset, we fine-tune classification-based detectors on various generation settings and evaluate their cross-prompt, cross-model, and cross-dataset generalization. To explain the performance variance, we compute correlations between generalization accuracies and feature shifts of 80 linguistic features between training and test conditions. Our analysis reveals that generalization performance for specific detectors and evaluation conditions is significantly associated with linguistic features such as tense usage and pronoun frequency.

</details>


### [5] [Cross-Cultural Expert-Level Art Critique Evaluation with Vision-Language Models](https://arxiv.org/abs/2601.07984)
*Haorui Yu,Ramon Ruiz-Dolz,Xuehang Wen,Fengrui Zhang,Qiufeng Yi*

Main category: cs.CL

TL;DR: 提出一个三层评估框架，用于评估视觉语言模型在跨文化艺术批评中的文化理解能力，包括自动指标、基于量表的评分和校准到人类评分的最终分数。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在视觉感知方面表现出色，但它们在理解艺术中的文化意义方面的能力尚未得到充分验证，需要系统性的评估方法来衡量模型的文化理解深度。

Method: 提出三层评估框架：第一层计算自动覆盖率和风险指标；第二层使用单一主评判者按照五个维度进行基于量表的评分；第三层通过等渗回归将第二层分数校准到人类评分，输出校准后的文化理解分数。

Result: 在294个专家锚点（涵盖六种文化传统）上评估了15个视觉语言模型，发现：自动指标不能可靠地代表文化深度；西方样本得分高于非西方样本；跨评判者的量表不匹配使得朴素平均不可靠，需要单一主评判者加显式校准。

Conclusion: 该框架为模型选择和"文化差距"诊断提供了校准的文化理解分数，以及维度级诊断和风险指标，解决了跨文化艺术批评评估中的关键挑战。

Abstract: Vision-Language Models (VLMs) excel at visual perception, yet their ability to interpret cultural meaning in art remains under-validated. We present a tri-tier evaluation framework for cross-cultural art-critique assessment: Tier I computes automated coverage and risk indicators offline; Tier II applies rubric-based scoring using a single primary judge across five dimensions; and Tier III calibrates the Tier II aggregate score to human ratings via isotonic regression, yielding a 5.2% reduction in MAE on a 152-sample held-out set. The framework outputs a calibrated cultural-understanding score for model selection and cultural-gap diagnosis, together with dimension-level diagnostics and risk indicators. We evaluate 15 VLMs on 294 expert anchors spanning six cultural traditions. Key findings are that (i) automated metrics are unreliable proxies for cultural depth, (ii) Western samples score higher than non-Western samples under our sampling and rubric, and (iii) cross-judge scale mismatch makes naive score averaging unreliable, motivating a single primary judge with explicit calibration. Dataset and code are available in the supplementary materials.

</details>


### [6] [Multilingual, Multimodal Pipeline for Creating Authentic and Structured Fact-Checked Claim Dataset](https://arxiv.org/abs/2601.07985)
*Z. Melce Hüsünbeyi,Virginie Mouilleron,Leonie Uhling,Daniel Foppe,Tatjana Scheffler,Djamé Seddah*

Main category: cs.CL

TL;DR: 该论文提出了一种构建多语言、多模态事实核查数据集的新方法，通过聚合ClaimReview数据、抓取完整辟谣文章、规范化裁决结果，并利用大语言模型进行证据提取和理由生成，为更可解释的事实核查模型提供基础。


<details>
  <summary>Details</summary>
Motivation: 在线平台上的虚假信息快速传播，迫切需要强大、最新、可解释且多语言的事实核查资源。现有数据集在范围上有限，通常缺乏多模态证据、结构化标注以及声明、证据和裁决之间的详细联系。

Method: 开发了综合数据收集和处理流程：1) 聚合ClaimReview数据源；2) 抓取完整辟谣文章；3) 规范化异构的声明裁决；4) 用结构化元数据和对齐的视觉内容进行丰富；5) 使用大语言模型和多模态大语言模型进行证据提取（按预定义类别）和理由生成（将证据与裁决联系起来）。

Result: 通过G-Eval和人工评估验证，该流程能够：1) 对不同组织或媒体市场的事实核查实践进行细粒度比较；2) 促进开发更可解释和基于证据的事实核查模型；3) 为多语言、多模态虚假信息验证的未来研究奠定基础。

Conclusion: 该研究构建了法语和德语的多模态事实核查数据集，提供了一种系统化的数据收集和处理方法，解决了现有数据集的局限性，为开发更强大、可解释的事实核查系统提供了重要资源。

Abstract: The rapid proliferation of misinformation across online platforms underscores the urgent need for robust, up-to-date, explainable, and multilingual fact-checking resources. However, existing datasets are limited in scope, often lacking multimodal evidence, structured annotations, and detailed links between claims, evidence, and verdicts. This paper introduces a comprehensive data collection and processing pipeline that constructs multimodal fact-checking datasets in French and German languages by aggregating ClaimReview feeds, scraping full debunking articles, normalizing heterogeneous claim verdicts, and enriching them with structured metadata and aligned visual content. We used state-of-the-art large language models (LLMs) and multimodal LLMs for (i) evidence extraction under predefined evidence categories and (ii) justification generation that links evidence to verdicts. Evaluation with G-Eval and human assessment demonstrates that our pipeline enables fine-grained comparison of fact-checking practices across different organizations or media markets, facilitates the development of more interpretable and evidence-grounded fact-checking models, and lays the groundwork for future research on multilingual, multimodal misinformation verification.

</details>


### [7] [VULCA-Bench: A Multicultural Vision-Language Benchmark for Evaluating Cultural Understanding](https://arxiv.org/abs/2601.07986)
*Haorui Yu,Ramon Ruiz-Dolz,Diji Yang,Hang He,Fengrui Zhang,Qiufeng Yi*

Main category: cs.CL

TL;DR: VULCA-Bench是一个多文化艺术批评基准，用于评估视觉语言模型超越表层视觉感知的文化理解能力，包含7,410个图像-批评对，涵盖八种文化传统，采用五层文化理解框架。


<details>
  <summary>Details</summary>
Motivation: 现有VLM基准主要评估L1-L2能力（物体识别、场景描述、事实问答），而缺乏对高阶文化解释能力的评估，需要专门的多文化艺术批评基准来衡量模型的文化理解深度。

Method: 构建包含7,410个匹配图像-批评对的数据集，涵盖八种文化传统，支持中英双语。采用五层文化理解框架（L1-L5，从视觉感知到哲学美学），实例化为225个文化特定维度，并由专家撰写双语批评。

Result: 初步结果表明，高层推理（L3-L5）比视觉和技术分析（L1-L2）更具挑战性。数据集、评估脚本和标注工具已开源。

Conclusion: VULCA-Bench填补了VLM文化理解评估的空白，为评估模型超越表层感知的文化解释能力提供了系统框架和基准资源。

Abstract: We introduce VULCA-Bench, a multicultural art-critique benchmark for evaluating Vision-Language Models' (VLMs) cultural understanding beyond surface-level visual perception. Existing VLM benchmarks predominantly measure L1-L2 capabilities (object recognition, scene description, and factual question answering) while under-evaluate higher-order cultural interpretation. VULCA-Bench contains 7,410 matched image-critique pairs spanning eight cultural traditions, with Chinese-English bilingual coverage. We operationalise cultural understanding using a five-layer framework (L1-L5, from Visual Perception to Philosophical Aesthetics), instantiated as 225 culture-specific dimensions and supported by expert-written bilingual critiques. Our pilot results indicate that higher-layer reasoning (L3-L5) is consistently more challenging than visual and technical analysis (L1-L2). The dataset, evaluation scripts, and annotation tools are available under CC BY 4.0 in the supplementary materials.

</details>


### [8] [From Word Sequences to Behavioral Sequences: Adapting Modeling and Evaluation Paradigms for Longitudinal NLP](https://arxiv.org/abs/2601.07988)
*Adithya V Ganesan,Vasudha Varadarajan,Oscar NE Kjell,Whitney R Ringwald,Scott Feltman,Benjamin J Luft,Roman Kotov,Ryan L Boyd,H Andrew Schwartz*

Main category: cs.CL

TL;DR: 论文提出纵向建模与评估范式，针对时间序列行为数据更新NLP流程，强调从词序列评估转向行为序列评估。


<details>
  <summary>Details</summary>
Motivation: 传统NLP将文档视为独立无序样本，但在纵向研究中，文档嵌套于作者并按时间排序形成行为序列，需要更符合现实情境的建模方法。

Method: 提出纵向建模评估范式，更新四个NLP流程：1) 按人群和时间划分评估集；2) 分离个体间差异与个体内动态的准确度指标；3) 默认包含历史信息的序列输入；4) 支持不同粒度潜在状态表示的模型内部结构。

Result: 在包含238名参与者、17k篇日记转录文本与PTSD症状严重程度的数据集上，传统文档级评估与作者提出的生态有效建模评估得出显著不同甚至相反的结论。

Conclusion: NLP需要从词序列评估范式转向行为序列评估范式，以更好地处理纵向研究中的时间序列行为数据。

Abstract: While NLP typically treats documents as independent and unordered samples, in longitudinal studies, this assumption rarely holds: documents are nested within authors and ordered in time, forming person-indexed, time-ordered $\textit{behavioral sequences}$. Here, we demonstrate the need for and propose a longitudinal modeling and evaluation paradigm that consequently updates four parts of the NLP pipeline: (1) evaluation splits aligned to generalization over people ($\textit{cross-sectional}$) and/or time ($\textit{prospective}$); (2) accuracy metrics separating between-person differences from within-person dynamics; (3) sequence inputs to incorporate history by default; and (4) model internals that support different $\textit{coarseness}$ of latent state over histories (pooled summaries, explicit dynamics, or interaction-based models). We demonstrate the issues ensued by traditional pipeline and our proposed improvements on a dataset of 17k daily diary transcripts paired with PTSD symptom severity from 238 participants, finding that traditional document-level evaluation can yield substantially different and sometimes reversed conclusions compared to our ecologically valid modeling and evaluation. We tie our results to a broader discussion motivating a shift from word-sequence evaluation toward $\textit{behavior-sequence}$ paradigms for NLP.

</details>


### [9] [DYCP: Dynamic Context Pruning for Long-Form Dialogue with LLMs](https://arxiv.org/abs/2601.07994)
*Nayoung Choi,Jonathan Zhang,Jinho D. Choi*

Main category: cs.CL

TL;DR: DyCP是一种轻量级上下文管理方法，通过动态分割和检索相关记忆来解决长对话中LLM响应延迟增加和答案质量下降的问题。


<details>
  <summary>Details</summary>
Motivation: 随着对话长度增长，LLMs的响应延迟增加且答案质量下降，现有方法依赖额外的LLM调用来构建记忆或进行离线记忆构建，没有考虑当前用户话语，可能导致效率低下或破坏对话连续性。

Method: DyCP在查询时动态分割和检索相关记忆，保持对话的顺序结构而不需要预定义主题边界，支持高效、自适应的上下文检索。

Result: 在三个长对话基准测试（LoCoMo、MT-Bench+、SCM4LLMs）和多个LLMs上，DyCP一致提高了答案质量同时减少了响应延迟。

Conclusion: 现代LLMs扩展的上下文窗口与其实际长上下文处理能力之间存在差距，强调了有效上下文管理的重要性，DyCP为此提供了一种有效的解决方案。

Abstract: Large Language Models (LLMs) often exhibit increased response latency and degraded answer quality as dialogue length grows, making effective context management essential. However, existing methods rely on extra LLM calls to build memory or perform offline memory construction without considering the current user utterance, which can introduce inefficiencies or disrupt conversational continuity. We introduce DyCP, a lightweight context management method that dynamically segment and retrieve relevant memory at query time. It preserves the sequential structure of dialogue without predefined topic boundaries and supports efficient, adaptive context retrieval. Across three long-form dialogue benchmarks, LoCoMo, MT-Bench+, and SCM4LLMs, and multiple LLMs, DyCP consistently improves answer quality while reducing response latency. We also examine the gap between modern LLMs' expanded context windows and their actual long-context processing capacity, highlighting the continued importance of effective context management.

</details>


### [10] [Is Sentiment Banana-Shaped? Exploring the Geometry and Portability of Sentiment Concept Vectors](https://arxiv.org/abs/2601.07995)
*Laurits Lyngbaek,Pascale Feldkamp,Yuri Bizzoni,Kristoffer L. Nielbo,Kenneth Enevoldsen*

Main category: cs.CL

TL;DR: CVP方法通过将情感建模为嵌入空间中的方向，提供连续多语言情感分数，在不同领域、历史时期和语言中具有良好的可移植性，但其线性假设仅是近似成立，有进一步改进空间。


<details>
  <summary>Details</summary>
Motivation: 人文学科中的情感分析通常需要情境化的连续分数。CVP方法虽然提供了解决方案，但其跨领域可移植性和基本假设尚未得到充分探索。

Method: 评估CVP在不同体裁、历史时期、语言和情感维度上的表现，检验概念向量在一个语料库上训练后能否迁移到其他语料库，并深入分析CVP的线性假设。

Result: 在一个语料库上训练的概念向量能够很好地迁移到其他语料库，性能损失很小；CVP是一种可移植的方法，能有效捕捉可泛化的模式，但其线性假设仅是近似成立。

Conclusion: CVP是一种有效的可移植情感分析方法，但其线性假设的近似性表明该方法仍有进一步发展的潜力。

Abstract: Use cases of sentiment analysis in the humanities often require contextualized, continuous scores. Concept Vector Projections (CVP) offer a recent solution: by modeling sentiment as a direction in embedding space, they produce continuous, multilingual scores that align closely with human judgments. Yet the method's portability across domains and underlying assumptions remain underexplored. We evaluate CVP across genres, historical periods, languages, and affective dimensions, finding that concept vectors trained on one corpus transfer well to others with minimal performance loss. To understand the patterns of generalization, we further examine the linearity assumption underlying CVP. Our findings suggest that while CVP is a portable approach that effectively captures generalizable patterns, its linearity assumption is approximate, pointing to potential for further development.

</details>


### [11] [LLM Review: Enhancing Creative Writing via Blind Peer Review Feedback](https://arxiv.org/abs/2601.08003)
*Weiyue Li,Mingxiao Song,Zhenda Shen,Dachuan Zhao,Yunfan Long,Yi Li,Yongce Li,Ruyi Yang,Mengyu Wang*

Main category: cs.CL

TL;DR: LLM Review框架通过盲审机制解决多智能体协作中的创意同质化问题，在科幻写作任务上超越基线方法，小模型配合该框架可超越大单智能体模型。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在创意生成方面存在困难，而现有的多智能体框架虽然能提升推理能力，却会导致内容同质化，反而抑制了创造力。

Method: 提出LLM Review框架，采用盲审机制：智能体交换针对性反馈但独立修改，保持创意轨迹的多样性。同时构建SciFi-100科幻写作数据集，结合LLM评分、人工标注和基于规则的新颖性指标进行综合评估。

Result: 实验表明LLM Review持续优于多智能体基线方法，且较小模型配合该框架可以超越更大的单智能体模型，表明交互结构可以替代模型规模。

Conclusion: 通过盲审机制的多智能体协作框架能有效提升创意生成质量，交互结构设计的重要性可能超过模型规模本身。

Abstract: Large Language Models (LLMs) often struggle with creative generation, and multi-agent frameworks that improve reasoning through interaction can paradoxically hinder creativity by inducing content homogenization. We introduce LLM Review, a peer-review-inspired framework implementing Blind Peer Review: agents exchange targeted feedback while revising independently, preserving divergent creative trajectories. To enable rigorous evaluation, we propose SciFi-100, a science fiction writing dataset with a unified framework combining LLM-as-a-judge scoring, human annotation, and rule-based novelty metrics. Experiments demonstrate that LLM Review consistently outperforms multi-agent baselines, and smaller models with our framework can surpass larger single-agent models, suggesting interaction structure may substitute for model scale.

</details>


### [12] [Reasoning Beyond Chain-of-Thought: A Latent Computational Mode in Large Language Models](https://arxiv.org/abs/2601.08058)
*Zhenghao He,Guangzhi Xiong,Bohan Liu,Sanchit Sinha,Aidong Zhang*

Main category: cs.CL

TL;DR: 研究发现LLM推理能力可通过内部潜在特征激活实现，CoT提示只是其中一种有效方式而非必要条件


<details>
  <summary>Details</summary>
Motivation: 探究CoT提示为何有效，以及它是否是触发大语言模型推理能力的唯一机制

Method: 使用稀疏自编码器分析LLM内部表示，识别与推理行为因果相关的潜在特征，通过潜在特征引导干预模型推理

Result: 单推理相关潜在特征引导可显著提升准确性；大型模型中潜在引导性能与标准CoT提示相当但输出更高效；推理状态在生成早期触发并可覆盖提示级指令

Conclusion: LLM多步推理由可外部激活的潜在内部激活支持，CoT提示是激活此机制的有效方式之一而非必要原因

Abstract: Chain-of-Thought (CoT) prompting has improved the reasoning performance of large language models (LLMs), but it remains unclear why it works and whether it is the unique mechanism for triggering reasoning in large language models. In this work, we study this question by directly analyzing and intervening on the internal representations of LLMs with Sparse Autoencoders (SAEs), identifying a small set of latent features that are causally associated with LLM reasoning behavior. Across multiple model families and reasoning benchmarks, we find that steering a single reasoning-related latent feature can substantially improve accuracy without explicit CoT prompting. For large models, latent steering achieves performance comparable to standard CoT prompting while producing more efficient outputs. We further observe that this reasoning-oriented internal state is triggered early in generation and can override prompt-level instructions that discourage explicit reasoning. Overall, our results suggest that multi-step reasoning in LLMs is supported by latent internal activations that can be externally activated, while CoT prompting is one effective, but not unique, way of activating this mechanism rather than its necessary cause.

</details>


### [13] [Universal computation is intrinsic to language model decoding](https://arxiv.org/abs/2601.08061)
*Alex Lewandowski,Marlos C. Machado,Dale Schuurmans*

Main category: cs.CL

TL;DR: 语言模型通过自回归链式输出可实现通用计算，随机初始化的模型在训练前就具备此能力，训练主要改善可编程性而非计算表达能力。


<details>
  <summary>Details</summary>
Motivation: 探索语言模型的计算能力边界，理解其是否具备通用计算能力，以及训练如何影响这种能力。

Method: 证明语言模型的自回归链式输出足以执行通用计算，即能模拟任何算法在任何输入上的执行。通过理论证明和实验验证随机初始化语言模型的计算能力。

Result: 语言模型确实具备通用计算能力，且这种能力在随机初始化时已存在。训练主要改善可编程性（找到合适提示的难易程度），而非创造计算表达能力。

Conclusion: 语言模型的计算能力是固有的，训练的作用是使其更容易通过自然语言接口访问这些能力，将问题从计算能力转向可编程性问题。

Abstract: Language models now provide an interface to express and often solve general problems in natural language, yet their ultimate computational capabilities remain a major topic of scientific debate. Unlike a formal computer, a language model is trained to autoregressively predict successive elements in human-generated text. We prove that chaining a language model's autoregressive output is sufficient to perform universal computation. That is, a language model can simulate the execution of any algorithm on any input. The challenge of eliciting desired computational behaviour can thus be reframed in terms of programmability: the ease of finding a suitable prompt. Strikingly, we demonstrate that even randomly initialized language models are capable of universal computation before training. This implies that training does not give rise to computational expressiveness -- rather, it improves programmability, enabling a natural language interface for accessing these intrinsic capabilities.

</details>


### [14] [Calibration Is Not Enough: Evaluating Confidence Estimation Under Language Variations](https://arxiv.org/abs/2601.08064)
*Yuxi Xia,Dennis Ulmer,Terra Blevins,Yihong Liu,Hinrich Schütze,Benjamin Roth*

Main category: cs.CL

TL;DR: 论文提出了一个全面的置信度估计评估框架，包含三个新维度：对抗提示扰动的鲁棒性、语义等价答案的稳定性、以及语义不同答案的敏感性，揭示了现有方法在这些方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有置信度估计方法主要关注校准（置信度与准确度对齐）和区分（正确预测置信度高于错误预测），但忽略了LLM和语言变体背景下的关键问题：置信度应在语义等价的提示或答案变化下保持一致，而在答案含义不同时应发生变化。

Method: 提出了一个综合评估框架，从三个新维度评估置信度质量：1) 对抗提示扰动的鲁棒性；2) 跨语义等价答案的稳定性；3) 对语义不同答案的敏感性。

Result: 研究表明，常见的LLM置信度估计方法在这些指标上表现不佳：在校准或区分方面表现良好的方法往往对提示变化不鲁棒，或对答案变化不敏感。

Conclusion: 该框架揭示了现有置信度评估在真实世界LLM使用场景中的局限性，为选择和设计更可靠的置信度估计方法提供了实用指导。

Abstract: Confidence estimation (CE) indicates how reliable the answers of large language models (LLMs) are, and can impact user trust and decision-making. Existing work evaluates CE methods almost exclusively through calibration, examining whether stated confidence aligns with accuracy, or discrimination, whether confidence is ranked higher for correct predictions than incorrect ones. However, these facets ignore pitfalls of CE in the context of LLMs and language variation: confidence estimates should remain consistent under semantically equivalent prompt or answer variations, and should change when the answer meaning differs. Therefore, we present a comprehensive evaluation framework for CE that measures their confidence quality on three new aspects: robustness of confidence against prompt perturbations, stability across semantic equivalent answers, and sensitivity to semantically different answers. In our work, we demonstrate that common CE methods for LLMs often fail on these metrics: methods that achieve good performance on calibration or discrimination are not robust to prompt variations or are not sensitive to answer changes. Overall, our framework reveals limitations of existing CE evaluations relevant for real-world LLM use cases and provides practical guidance for selecting and designing more reliable CE methods.

</details>


### [15] [AdaJudge: Adaptive Multi-Perspective Judging for Reward Modeling](https://arxiv.org/abs/2601.08097)
*Yongliang Miao,Yangyang Liang,Mengnan Du*

Main category: cs.CL

TL;DR: AdaJudge是一个奖励建模框架，通过自适应多视图池化和门控细化块改进传统静态池化方法，在奖励建模任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前奖励建模主要依赖静态池化策略将序列压缩为标量分数，存在两个关键限制：1）静态归纳偏差与任务依赖的偏好信号不匹配；2）表示不匹配，因为骨干网络是为生成而非细粒度判别优化的。

Method: AdaJudge联合适应表示和聚合：1）通过门控细化块将骨干表示精炼到判别导向空间；2）用自适应多视图池化模块替换静态读出，动态路由和组合证据。

Result: 在RM-Bench和JudgeBench上的大量实验表明，AdaJudge优于强大的现成奖励模型和传统池化基线。

Conclusion: AdaJudge通过联合适应表示和聚合，解决了传统奖励建模中静态池化的局限性，为奖励建模提供了更有效的框架。

Abstract: Reward modeling is essential for aligning large language models with human preferences, yet predominant architectures rely on a static pooling strategy to condense sequences into scalar scores. This paradigm, however, suffers from two key limitations: a static inductive bias that misaligns with task-dependent preference signals, and a representational mismatch, as the backbone is optimized for generation rather than fine-grained discrimination. To address this, we propose AdaJudge, a unified framework that jointly adapts representation and aggregation. AdaJudge first refines backbone representations into a discrimination-oriented space via gated refinement blocks. It then replaces the static readout with an adaptive multi-view pooling module that dynamically routes and combines evidence. Extensive experiments on RM-Bench and JudgeBench show that AdaJudge outperforms strong off-the-shelf reward models and traditional pooling baselines.

</details>


### [16] [Query Suggestion for Retrieval-Augmented Generation via Dynamic In-Context Learning](https://arxiv.org/abs/2601.08105)
*Fabian Spaeh,Tianyi Chen,Chen-Hao Chiang,Bin Shen*

Main category: cs.CL

TL;DR: 该论文研究了面向工具调用RAG系统的查询建议问题，当用户提问超出知识范围时，系统能够推荐相似且可回答的查询，以改善用户体验。


<details>
  <summary>Details</summary>
Motivation: 当前基于工具调用的RAG系统在面对超出知识范围的问题时容易产生幻觉，虽然已有护栏框架可以阻止这类问题，但缺乏主动为用户推荐可回答查询的研究，这限制了用户与RAG系统的交互体验。

Method: 提出鲁棒动态少样本学习方法，从相关工作流中检索示例，系统可以通过先前的用户查询进行自学习，因此在实际应用中易于部署。

Result: 在三个基准数据集上的实验表明，该方法相比少样本和仅检索的基线方法，能够产生更相关且可回答的查询建议，从而实现了更安全、更有效的用户与RAG系统交互。

Conclusion: 该研究首次系统性地探索了面向工具调用RAG系统的查询建议问题，提出的方法能够有效改善用户与RAG系统的交互体验，特别是在处理超出知识范围的问题时。

Abstract: Retrieval-augmented generation with tool-calling agents (agentic RAG) has become increasingly powerful in understanding, processing, and responding to user queries. However, the scope of the grounding knowledge is limited and asking questions that exceed this scope may lead to issues like hallucination. While guardrail frameworks aim to block out-of-scope questions (Rodriguez et al., 2024), no research has investigated the question of suggesting answerable queries in order to complete the user interaction.
  In this paper, we initiate the study of query suggestion for agentic RAG. We consider the setting where user questions are not answerable, and the suggested queries should be similar to aid the user interaction. Such scenarios are frequent for tool-calling LLMs as communicating the restrictions of the tools or the underlying datasets to the user is difficult, and adding query suggestions enhances the interaction with the RAG agent. As opposed to traditional settings for query recommendations such as in search engines, ensuring that the suggested queries are answerable is a major challenge due to the RAG's multi-step workflow that demands a nuanced understanding of the RAG as a whole, which the executing LLM lacks. As such, we introduce robust dynamic few-shot learning which retrieves examples from relevant workflows. We show that our system can be self-learned, for instance on prior user queries, and is therefore easily applicable in practice. We evaluate our approach on three benchmark datasets based on two unlabeled question datasets collected from real-world user queries. Experiments on real-world datasets confirm that our method produces more relevant and answerable suggestions, outperforming few-shot and retrieval-only baselines, and thus enable safer, more effective user interaction with agentic RAG.

</details>


### [17] [Debiasing Large Language Models via Adaptive Causal Prompting with Sketch-of-Thought](https://arxiv.org/abs/2601.08108)
*Bowen Li,Ziqi Xu,Jing Ren,Renqiang Luo,Xikun Zhang,Xiuzhen Zhang,Yongli Ren,Feng Xia*

Main category: cs.CL

TL;DR: ACPS框架通过因果推断和Sketch-of-Thought，在减少token使用的同时提升LLM推理任务的准确性和泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有提示方法（如CoT）存在token使用过多和跨任务泛化能力有限的问题，需要更高效、通用的推理框架

Method: 提出自适应因果提示与思维草图（ACPS）框架，利用结构因果模型推断查询对答案的因果效应，自适应选择干预策略，并用简洁的Sketch-of-Thought替代冗长的CoT

Result: 在多个推理基准测试和LLM上的实验表明，ACPS在准确性、鲁棒性和计算效率方面持续优于现有提示基线方法

Conclusion: ACPS通过因果推理和高效提示设计，实现了跨异构任务的泛化推理能力，显著降低了token使用和推理成本

Abstract: Despite notable advancements in prompting methods for Large Language Models (LLMs), such as Chain-of-Thought (CoT), existing strategies still suffer from excessive token usage and limited generalisability across diverse reasoning tasks. To address these limitations, we propose an Adaptive Causal Prompting with Sketch-of-Thought (ACPS) framework, which leverages structural causal models to infer the causal effect of a query on its answer and adaptively select an appropriate intervention (i.e., standard front-door and conditional front-door adjustments). This design enables generalisable causal reasoning across heterogeneous tasks without task-specific retraining. By replacing verbose CoT with concise Sketch-of-Thought, ACPS enables efficient reasoning that significantly reduces token usage and inference cost. Extensive experiments on multiple reasoning benchmarks and LLMs demonstrate that ACPS consistently outperforms existing prompting baselines in terms of accuracy, robustness, and computational efficiency.

</details>


### [18] [Attention Projection Mixing and Exogenous Anchors](https://arxiv.org/abs/2601.08131)
*Jonathan Su*

Main category: cs.CL

TL;DR: ExoFormer提出外部锚点投影，将参考锚点角色与计算精化分离，通过统一归一化混合框架提升性能，但出现表征坍缩现象


<details>
  <summary>Details</summary>
Motivation: 解决传统Transformer中早期层注意力投影作为残差时面临的矛盾：第一层既要作为深层稳定参考，又要作为有效计算块

Method: 提出ExoFormer，学习序列层栈外的专用外部锚点投影，通过统一归一化混合框架（元素级、头级、标量级）处理所有注意力路径

Result: 动态变体在下游任务上比基线提升2.13个点，数据效率提升1.84倍，注意力sink减少2倍，但所有变体都出现表征坍缩

Conclusion: 外部锚点通过卸载假说保留关键token身份，让各层专注于计算精化，虽然出现表征坍缩但性能提升显著

Abstract: Transformers that reuse early-layer attention projections as residuals face a fundamental tension: the first layer must simultaneously serve as a stable reference for all deeper layers and as an effective computational block. To resolve this, we propose ExoFormer, which learns dedicated exogenous anchor projections outside the sequential layer stack, decoupling the anchor role from computational refinement. Through a unified normalized mixing framework (studying different coefficient granularities: elementwise, headwise, scalar) across all attention pathways (queries, keys, values, and gate logits), ExoFormer variants consistently outperform their internal-anchor counterparts. Moreover, the dynamic variant achieves a 2.13-point increase in downstream accuracy over the baseline and demonstrates superior data efficiency, matching baseline validation loss with 1.84x fewer tokens. ExoFormer also achieves a 2x reduction in attention sink compared to standard Gated Attention. Paradoxically, all ExoFormer variants exhibit signs of representation collapse. We explain this via an Offloading Hypothesis: external anchors preserve essential token identity, allowing layers to specialize exclusively in computational refinement. We release codes and models to facilitate future research.

</details>


### [19] [How Reliable are Confidence Estimators for Large Reasoning Models? A Systematic Benchmark on High-Stakes Domains](https://arxiv.org/abs/2601.08134)
*Reza Khanmohammadi,Erfan Miahi,Simerjot Kaur,Ivan Brugere,Charese H. Smiley,Kundan Thind,Mohammad M. Ghassemi*

Main category: cs.CL

TL;DR: 论文提出了RMCB基准，用于评估大型推理模型的置信度估计方法，发现文本编码器在区分性（AUROC）上表现最好，而结构感知模型在校准（ECE）上表现最好，两者存在权衡关系。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）的校准不足影响了其在高风险领域的可靠性，需要准确估计其长格式、多步骤输出的置信度。目前缺乏全面的基准来评估置信度估计方法。

Method: 1. 构建RMCB基准：包含347,496个推理轨迹，来自6个不同架构的流行LRMs，涵盖临床、金融、法律、数学推理等高危领域及复杂通用推理任务；2. 评估10多种基于表示的方法，包括序列、图基和文本编码器架构；3. 分析区分性（AUROC）和校准（ECE）之间的权衡关系。

Result: 1. 文本编码器获得最佳AUROC（0.672），结构感知模型获得最佳ECE（0.148）；2. 没有单一方法在区分性和校准两方面都占优；3. 增加架构复杂性并不能可靠地超越简单的序列基线；4. 仅依赖块级隐藏状态的方法存在性能上限。

Conclusion: 该研究为置信度估计任务提供了最全面的基准，建立了严格的基线，并展示了当前基于表示的范式的局限性，揭示了区分性和校准之间的固有权衡。

Abstract: The miscalibration of Large Reasoning Models (LRMs) undermines their reliability in high-stakes domains, necessitating methods to accurately estimate the confidence of their long-form, multi-step outputs. To address this gap, we introduce the Reasoning Model Confidence estimation Benchmark (RMCB), a public resource of 347,496 reasoning traces from six popular LRMs across different architectural families. The benchmark is constructed from a diverse suite of datasets spanning high-stakes domains, including clinical, financial, legal, and mathematical reasoning, alongside complex general reasoning benchmarks, with correctness annotations provided for all samples. Using RMCB, we conduct a large-scale empirical evaluation of over ten distinct representation-based methods, spanning sequential, graph-based, and text-based architectures. Our central finding is a persistent trade-off between discrimination (AUROC) and calibration (ECE): text-based encoders achieve the best AUROC (0.672), while structurally-aware models yield the best ECE (0.148), with no single method dominating both. Furthermore, we find that increased architectural complexity does not reliably outperform simpler sequential baselines, suggesting a performance ceiling for methods relying solely on chunk-level hidden states. This work provides the most comprehensive benchmark for this task to date, establishing rigorous baselines and demonstrating the limitations of current representation-based paradigms.

</details>


### [20] [Qalb: Largest State-of-the-Art Urdu Large Language Model for 230M Speakers with Systematic Continued Pre-training](https://arxiv.org/abs/2601.08141)
*Muhammad Taimoor Hassan,Jawad Ahmed,Muhammad Awais*

Main category: cs.CL

TL;DR: Qalb是一个针对乌尔都语优化的语言模型，通过两阶段方法（持续预训练+监督微调）在LLaMA 3.1 8B基础上开发，在乌尔都语任务上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 乌尔都语（2.3亿使用者）在现代NLP系统中代表性严重不足，现有多语言模型在乌尔都语任务上表现不佳，即使是LLaMA-3.1 8B-Instruct模型也难以生成流利、上下文恰当的乌尔都语文本。

Method: 采用两阶段方法：1）从LLaMA 3.1 8B开始，在19.7亿token的数据集上进行持续预训练（包含18.4亿乌尔都语token和1.4亿英语维基百科token）；2）在Alif Urdu-instruct数据集上进行监督微调。

Result: Qalb在乌尔都语基准测试中加权平均得分90.34，比之前的SOTA模型Alif-1.0-Instruct（87.1）高出3.24分，比基础LLaMA-3.1 8B-Instruct模型高出44.64分，在分类、情感分析、推理等7个任务上均达到SOTA性能。

Conclusion: 在多样化高质量语言数据上进行持续预训练，结合有针对性的指令微调，能够有效将基础模型适配到低资源语言，为乌尔都语NLP发展提供了重要解决方案。

Abstract: Despite remarkable progress in large language models, Urdu-a language spoken by over 230 million people-remains critically underrepresented in modern NLP systems. Existing multilingual models demonstrate poor performance on Urdu-specific tasks, struggling with the language's complex morphology, right-to-left Nastaliq script, and rich literary traditions. Even the base LLaMA-3.1 8B-Instruct model shows limited capability in generating fluent, contextually appropriate Urdu text. We introduce Qalb, an Urdu language model developed through a two-stage approach: continued pre-training followed by supervised fine-tuning. Starting from LLaMA 3.1 8B, we perform continued pre-training on a dataset of 1.97 billion tokens. This corpus comprises 1.84 billion tokens of diverse Urdu text-spanning news archives, classical and contemporary literature, government documents, and social media-combined with 140 million tokens of English Wikipedia data to prevent catastrophic forgetting. We then fine-tune the resulting model on the Alif Urdu-instruct dataset. Through extensive evaluation on Urdu-specific benchmarks, Qalb demonstrates substantial improvements, achieving a weighted average score of 90.34 and outperforming the previous state-of-the-art Alif-1.0-Instruct model (87.1) by 3.24 points, while also surpassing the base LLaMA-3.1 8B-Instruct model by 44.64 points. Qalb achieves state-of-the-art performance with comprehensive evaluation across seven diverse tasks including Classification, Sentiment Analysis, and Reasoning. Our results demonstrate that continued pre-training on diverse, high-quality language data, combined with targeted instruction fine-tuning, effectively adapts foundation models to low-resource languages.

</details>


### [21] [Mechanisms are Transferable: Data-Efficient Low-Resource Adaptation via Circuit-Targeted Supervised Fine-Tuning](https://arxiv.org/abs/2601.08146)
*Khumaisa Nur'aini,Ayu Purwarianti,Alham Fikri Aji,Derry Wijaya*

Main category: cs.CL

TL;DR: CT-SFT是一种针对低资源语言适配LLM的新方法，通过识别任务相关的稀疏注意力头并仅更新这些头部来减少灾难性遗忘，提高跨语言准确性。


<details>
  <summary>Details</summary>
Motivation: 低资源语言适配LLM面临三大挑战：标注数据稀缺、全模型微调不稳定、跨语言持续调优会导致灾难性遗忘。需要一种更稳定、高效的适配方法。

Method: CT-SFT方法：1) 使用标签平衡均值基线和任务方向相关性评分识别代理语言检查点中任务相关的稀疏注意力头；2) 通过头部级梯度掩码仅更新这些头部（加上LayerNorm）进行目标语言迁移学习。

Result: 在NusaX-Senti和XNLI数据集上，CT-SFT相比持续全微调提高了跨语言准确性，同时只更新少量模型参数。还发现编辑-保持的权衡：困难迁移偏向编辑电路头，简单迁移偏向近零更新以保持源机制。

Conclusion: CT-SFT能有效减少灾难性遗忘，在迁移过程中保持代理/源语言能力，为低资源语言适配提供了一种参数高效且稳定的解决方案。

Abstract: Adapting LLMs to low-resource languages is difficult: labeled data is scarce, full-model fine-tuning is unstable, and continued cross-lingual tuning can cause catastrophic forgetting. We propose Circuit-Targeted Supervised Fine-Tuning (CT-SFT): a counterfactual-free adaptation of CD-T (Contextual Decomposition Transformer) that uses a label-balanced mean baseline and task-directional relevance scoring to identify a sparse set of task-relevant attention heads in a proxy-language checkpoint, then transfer learns to a target language by updating only those heads (plus LayerNorm) via head-level gradient masking. Across NusaX-Senti and XNLI, CT-SFT improves cross-lingual accuracy over continued full fine-tuning while updating only a small subset of model parameters. We find an editing-preserving trade-off: harder transfers favor editing circuit heads, while easier transfers often favor near-zero (i.e., low-relevance heads) updates, preserving the source mechanism. CT-SFT also substantially reduces catastrophic forgetting, preserving proxy/source-language competence during transfer.

</details>


### [22] [WISE-Flow: Workflow-Induced Structured Experience for Self-Evolving Conversational Service Agents](https://arxiv.org/abs/2601.08158)
*Yuqing Zhou,Zhuoer Wang,Jie Yuan,Hong Wang,Samson Koelle,Ziwei Zhu,Wei Niu*

Main category: cs.CL

TL;DR: WISE-Flow是一个工作流中心框架，通过从历史服务交互中提取可重用程序经验，使LLM智能体能够自我进化，减少错误并提高稳定性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体在新任务中容易出错，倾向于重复相同的失败模式，且运行间变异性大。通过环境特定训练或手动修补来修复失败成本高且难以扩展。

Method: 提出WISE-Flow框架：1) 将历史服务交互转换为可重用程序经验，通过诱导具有前提条件增强动作块的工作流；2) 部署时，将智能体执行轨迹与检索到的工作流对齐；3) 执行前提条件感知的可行性推理以实现状态基础的下一个动作。

Result: 在ToolSandbox和τ²-bench上的实验显示，该方法在不同基础模型上都能带来一致的性能提升。

Conclusion: WISE-Flow通过工作流中心的方法使LLM智能体能够在用户服务环境中自我进化，有效解决了智能体错误、重复失败和运行变异性问题，为可扩展的智能体改进提供了新途径。

Abstract: Large language model (LLM)-based agents are widely deployed in user-facing services but remain error-prone in new tasks, tend to repeat the same failure patterns, and show substantial run-to-run variability. Fixing failures via environment-specific training or manual patching is costly and hard to scale. To enable self-evolving agents in user-facing service environments, we propose WISE-Flow, a workflow-centric framework that converts historical service interactions into reusable procedural experience by inducing workflows with prerequisite-augmented action blocks. At deployment, WISE-Flow aligns the agent's execution trajectory to retrieved workflows and performs prerequisite-aware feasibility reasoning to achieve state-grounded next actions. Experiments on ToolSandbox and $τ^2$-bench show consistent improvement across base models.

</details>


### [23] [SwiftMem: Fast Agentic Memory via Query-aware Indexing](https://arxiv.org/abs/2601.08160)
*Anxin Tian,Yiming Li,Xing Li,Hui-Ling Zhen,Lei Chen,Xianzhi Yu,Zhenhua Dong,Mingxuan Yuan*

Main category: cs.CL

TL;DR: SwiftMem是一个查询感知的智能体记忆系统，通过专门的时空索引实现亚线性检索，解决了现有记忆系统全量检索导致的延迟瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 现有智能体记忆系统存在根本性限制：无论查询特性如何，都会对整个存储层进行穷举检索。这种暴力方法随着记忆增长会产生严重的延迟瓶颈，阻碍实时智能体交互。

Method: 提出了SwiftMem系统，通过专门的时空索引实现亚线性检索：1) 时间索引支持对数时间范围查询；2) 语义DAG-Tag索引通过分层标签结构将查询映射到相关主题；3) 嵌入-标签协同整合机制基于语义簇重组存储以提高缓存局部性。

Result: 在LoCoMo和LongMemEval基准测试中，SwiftMem相比最先进的基线实现了47倍的搜索速度提升，同时保持了有竞争力的准确性，使记忆增强的LLM智能体能够实际部署。

Conclusion: SwiftMem通过查询感知的索引设计解决了智能体记忆系统的检索效率瓶颈，为实时记忆增强的LLM智能体提供了实用的解决方案。

Abstract: Agentic memory systems have become critical for enabling LLM agents to maintain long-term context and retrieve relevant information efficiently. However, existing memory frameworks suffer from a fundamental limitation: they perform exhaustive retrieval across the entire storage layer regardless of query characteristics. This brute-force approach creates severe latency bottlenecks as memory grows, hindering real-time agent interactions. We propose SwiftMem, a query-aware agentic memory system that achieves sub-linear retrieval through specialized indexing over temporal and semantic dimensions. Our temporal index enables logarithmic-time range queries for time-sensitive retrieval, while the semantic DAG-Tag index maps queries to relevant topics through hierarchical tag structures. To address memory fragmentation during growth, we introduce an embedding-tag co-consolidation mechanism that reorganizes storage based on semantic clusters to improve cache locality. Experiments on LoCoMo and LongMemEval benchmarks demonstrate that SwiftMem achieves 47$\times$ faster search compared to state-of-the-art baselines while maintaining competitive accuracy, enabling practical deployment of memory-augmented LLM agents.

</details>


### [24] [Relational Knowledge Distillation Using Fine-tuned Function Vectors](https://arxiv.org/abs/2601.08169)
*Andrea Kang,Yingnian Wu,Hongjing Lu*

Main category: cs.CL

TL;DR: 通过微调函数向量提升大语言模型的关系表示能力，并引入复合函数向量增强类比推理性能


<details>
  <summary>Details</summary>
Motivation: 概念间关系表示是智能系统理解世界的核心前提。虽然已有研究通过因果中介分析发现少量注意力头编码了任务表示（函数向量），但如何进一步提升关系表示的质量和推理能力仍需探索。

Method: 1. 使用少量示例（约20个词对）微调函数向量；2. 引入复合函数向量，即微调函数向量的加权组合，用于提取关系知识和支持类比推理；3. 在推理时将复合向量注入LLM激活中。

Result: 1. 微调函数向量在关系型词汇补全任务上表现优于原始向量；2. 微调向量在关系词解码和语义关系的人类相似性判断对齐方面表现更好；3. 复合函数向量显著提升认知科学和SAT基准中的类比问题解决能力。

Conclusion: 激活修补作为一种可控机制，在编码和操作关系知识方面具有潜力，能够同时提升大语言模型的可解释性和推理能力。

Abstract: Representing relations between concepts is a core prerequisite for intelligent systems to make sense of the world. Recent work using causal mediation analysis has shown that a small set of attention heads encodes task representation in in-context learning, captured in a compact representation known as the function vector. We show that fine-tuning function vectors with only a small set of examples (about 20 word pairs) yields better performance on relation-based word-completion tasks than using the original vectors derived from causal mediation analysis. These improvements hold for both small and large language models. Moreover, the fine-tuned function vectors yield improved decoding performance for relation words and show stronger alignment with human similarity judgments of semantic relations. Next, we introduce the composite function vector - a weighted combination of fine-tuned function vectors - to extract relational knowledge and support analogical reasoning. At inference time, inserting this composite vector into LLM activations markedly enhances performance on challenging analogy problems drawn from cognitive science and SAT benchmarks. Our results highlight the potential of activation patching as a controllable mechanism for encoding and manipulating relational knowledge, advancing both the interpretability and reasoning capabilities of large language models.

</details>


### [25] [Prompt-Based Clarity Evaluation and Topic Detection in Political Question Answering](https://arxiv.org/abs/2601.08176)
*Lavanya Prahallad,Sai Utkarsh Choudarypally,Pragna Prahallad,Pranathi Prahallad*

Main category: cs.CL

TL;DR: 研究提示设计对LLM政治问答清晰度自动评估的影响，使用CLARITY数据集，比较不同提示策略（简单提示、思维链、思维链+少样本）在GPT-5.2上的表现，发现提示设计能显著提升清晰度评估效果。


<details>
  <summary>Details</summary>
Motivation: 当前LLM自动评估主要关注事实准确性，但政治问答还需要评估回答的清晰度。虽然已有数据集提供清晰度和回避性的人工标注，但提示设计对自动清晰度评估的影响尚未充分探索。

Method: 使用SemEval 2026共享任务的CLARITY数据集，比较GPT-3.5基线模型与GPT-5.2在三种提示策略下的表现：简单提示、思维链提示、思维链+少样本提示。评估指标包括准确率、类别指标和层次精确匹配。

Result: GPT-5.2在清晰度预测上始终优于GPT-3.5基线，思维链+少样本提示将准确率从56%提升到63%。思维链提示在回避性评估上达到最高准确率34%。主题识别方面，基于推理的提示将准确率从60%提升到74%。

Conclusion: 提示设计能可靠地提升高层次清晰度评估效果，但细粒度回避性评估和主题检测仍然具有挑战性，即使使用结构化推理提示也难以完全解决。

Abstract: Automatic evaluation of large language model (LLM) responses requires not only factual correctness but also clarity, particularly in political question-answering. While recent datasets provide human annotations for clarity and evasion, the impact of prompt design on automatic clarity evaluation remains underexplored. In this paper, we study prompt-based clarity evaluation using the CLARITY dataset from the SemEval 2026 shared task. We compare a GPT-3.5 baseline provided with the dataset against GPT-5.2 evaluated under three prompting strategies: simple prompting, chain-of-thought prompting, and chain-of-thought with few-shot examples. Model predictions are evaluated against human annotations using accuracy and class-wise metrics for clarity and evasion, along with hierarchical exact match. Results show that GPT-5.2 consistently outperforms the GPT-3.5 baseline on clarity prediction, with accuracy improving from 56 percent to 63 percent under chain-of-thought with few-shot prompting. Chain-of-thought prompting yields the highest evasion accuracy at 34 percent, though improvements are less stable across fine-grained evasion categories. We further evaluate topic identification and find that reasoning-based prompting improves accuracy from 60 percent to 74 percent relative to human annotations. Overall, our findings indicate that prompt design reliably improves high-level clarity evaluation, while fine-grained evasion and topic detection remain challenging despite structured reasoning prompts.

</details>


### [26] [Evaluating Implicit Regulatory Compliance in LLM Tool Invocation via Logic-Guided Synthesis](https://arxiv.org/abs/2601.08196)
*Da Song,Yuheng Huang,Boqi Chen,Tianshuo Cong,Randy Goebel,Lei Ma,Foutse Khomh*

Main category: cs.CL

TL;DR: 论文提出LogiSafetyGen框架，将非结构化法规转换为线性时序逻辑预言机，并通过逻辑引导的模糊测试生成安全关键轨迹，构建LogiSafetyBench基准测试，评估LLM在功能正确性和法规合规性方面的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试往往忽视隐式法规合规性，无法评估LLM是否能在高风险领域自主执行强制性安全约束。在高风险领域，LLM驱动的自主系统必须严格遵守超越简单功能正确性的监管标准。

Method: 提出LogiSafetyGen框架：1) 将非结构化法规转换为线性时序逻辑(LTL)预言机；2) 使用逻辑引导的模糊测试合成有效的安全关键轨迹；3) 基于此框架构建LogiSafetyBench基准，包含240个人工验证的任务，要求LLM生成同时满足功能目标和潜在合规规则的Python程序。

Result: 评估13个最先进的LLM发现：更大的模型虽然在功能正确性方面表现更好，但经常优先考虑任务完成而非安全性，导致不合规行为。这表明当前LLM在安全约束执行方面存在显著缺陷。

Conclusion: 该研究填补了LLM在法规合规性评估方面的空白，揭示了现有模型在安全约束执行上的不足，为开发更安全的自主系统提供了重要基准和评估框架。

Abstract: The integration of large language models (LLMs) into autonomous agents has enabled complex tool use, yet in high-stakes domains, these systems must strictly adhere to regulatory standards beyond simple functional correctness. However, existing benchmarks often overlook implicit regulatory compliance, thus failing to evaluate whether LLMs can autonomously enforce mandatory safety constraints. To fill this gap, we introduce LogiSafetyGen, a framework that converts unstructured regulations into Linear Temporal Logic oracles and employs logic-guided fuzzing to synthesize valid, safety-critical traces. Building on this framework, we construct LogiSafetyBench, a benchmark comprising 240 human-verified tasks that require LLMs to generate Python programs that satisfy both functional objectives and latent compliance rules. Evaluations of 13 state-of-the-art (SOTA) LLMs reveal that larger models, despite achieving better functional correctness, frequently prioritize task completion over safety, which results in non-compliant behavior.

</details>


### [27] [Triplets Better Than Pairs: Towards Stable and Effective Self-Play Fine-Tuning for LLMs](https://arxiv.org/abs/2601.08198)
*Yibo Wang,Hai-Long Sun,Qing-Guo Chen,Zhao Xu,Weihua Luo,Kaifu Zhang,Lijun Zhang*

Main category: cs.CL

TL;DR: T-SPIN提出基于三元组的自博弈微调方法，通过引入历史优势和熵约束，解决了SPIN方法中优势消失和训练-生成不一致的问题，在数据稀缺情况下实现更稳定高效的模型微调。


<details>
  <summary>Details</summary>
Motivation: SPIN方法在数据稀缺的自博弈微调中存在两个主要问题：1）当前奖励优势在迭代过程中逐渐消失导致优化不稳定；2）使用参考策略导致训练时的奖励公式与生成时的评估指标不一致。

Method: T-SPIN包含两个关键设计：1）引入历史优势，比较迭代生成响应与初始策略生成的原型响应，即使当前优势消失，历史优势仍有效；2）在自博弈框架中引入熵约束，理论证明支持无参考微调，消除训练-生成差异。

Result: 在多个任务上的实验表明，T-SPIN不仅性能优于SPIN，而且在迭代过程中保持稳定演化。与监督微调相比，仅使用25%的样本就能达到相当甚至更好的性能。

Conclusion: T-SPIN通过历史优势和熵约束有效解决了SPIN的局限性，在数据稀缺情况下实现了更稳定、高效的模型微调，为少样本学习提供了有前景的解决方案。

Abstract: Recently, self-play fine-tuning (SPIN) has been proposed to adapt large language models to downstream applications with scarce expert-annotated data, by iteratively generating synthetic responses from the model itself. However, SPIN is designed to optimize the current reward advantages of annotated responses over synthetic responses at hand, which may gradually vanish during iterations, leading to unstable optimization. Moreover, the utilization of reference policy induces a misalignment issue between the reward formulation for training and the metric for generation. To address these limitations, we propose a novel Triplet-based Self-Play fIne-tuNing (T-SPIN) method that integrates two key designs. First, beyond current advantages, T-SPIN additionally incorporates historical advantages between iteratively generated responses and proto-synthetic responses produced by the initial policy. Even if the current advantages diminish, historical advantages remain effective, stabilizing the overall optimization. Second, T-SPIN introduces the entropy constraint into the self-play framework, which is theoretically justified to support reference-free fine-tuning, eliminating the training-generation discrepancy. Empirical results on various tasks demonstrate not only the superior performance of T-SPIN over SPIN, but also its stable evolution during iterations. Remarkably, compared to supervised fine-tuning, T-SPIN achieves comparable or even better performance with only 25% samples, highlighting its effectiveness when faced with scarce annotated data.

</details>


### [28] [Generation-Augmented Generation: A Plug-and-Play Framework for Private Knowledge Injection in Large Language Models](https://arxiv.org/abs/2601.08209)
*Rongji Li,Jian Xu,Xueqing Chen,Yisheng Yang,Jiayi Wang,Xingyu Chen,Chunyu Xie,Dawei Leng,Xu-Yao Zhang*

Main category: cs.CL

TL;DR: 论文提出Generation-Augmented Generation (GAG)方法，通过将私有专业知识作为额外专家模态，在表示层面对齐到冻结的基础模型，解决了私有知识注入中的微调迭代成本高和RAG证据碎片化等问题。


<details>
  <summary>Details</summary>
Motivation: 在生物医学、材料和金融等高风险领域部署大语言模型时，需要注入私有、领域特定的知识，这些知识通常是专有的、快速演变的且在公共预训练中代表性不足。现有的两种主要私有知识注入方法各有明显缺点：微调迭代成本高且存在灾难性遗忘风险；检索增强生成(RAG)在专业私有语料库中脆弱，存在证据碎片化、检索漂移和长上下文压力等问题。

Method: 受多模态LLM将异构模态对齐到共享语义空间的启发，提出Generation-Augmented Generation (GAG)方法。将私有专业知识视为额外的专家模态，通过紧凑的表示级接口对齐到冻结的基础模型，避免提示时的证据序列化，同时支持即插即用专业化和可扩展的多领域组合。

Result: 在两个私有科学QA基准测试（免疫学佐剂和催化材料）和混合领域评估中，GAG在两个基准上分别比强RAG基线提高了15.34%和14.86%的专业性能，同时在六个开放通用基准上保持性能，并实现接近oracle的选择性激活，支持可扩展的多领域部署。

Conclusion: GAG方法通过将私有专业知识作为专家模态在表示层面注入，有效解决了私有知识注入的挑战，在保持基础模型完整性的同时实现了更好的专业性能、可扩展性和选择性激活能力。

Abstract: In domains such as biomedicine, materials, and finance, high-stakes deployment of large language models (LLMs) requires injecting private, domain-specific knowledge that is proprietary, fast-evolving, and under-represented in public pretraining. However, the two dominant paradigms for private knowledge injection each have pronounced drawbacks: fine-tuning is expensive to iterate, and continual updates risk catastrophic forgetting and general-capability regression; retrieval-augmented generation (RAG) keeps the base model intact but is brittle in specialized private corpora due to chunk-induced evidence fragmentation, retrieval drift, and long-context pressure that yields query-dependent prompt inflation. Inspired by how multimodal LLMs align heterogeneous modalities into a shared semantic space, we propose Generation-Augmented Generation (GAG), which treats private expertise as an additional expert modality and injects it via a compact, representation-level interface aligned to the frozen base model, avoiding prompt-time evidence serialization while enabling plug-and-play specialization and scalable multi-domain composition with reliable selective activation. Across two private scientific QA benchmarks (immunology adjuvant and catalytic materials) and mixed-domain evaluations, GAG improves specialist performance over strong RAG baselines by 15.34% and 14.86% on the two benchmarks, respectively, while maintaining performance on six open general benchmarks and enabling near-oracle selective activation for scalable multi-domain deployment.

</details>


### [29] [Towards Principled Design of Mixture-of-Experts Language Models under Memory and Inference Constraints](https://arxiv.org/abs/2601.08215)
*Seng Pei Liew,Kenta Shinzato,Yuyang Dong*

Main category: cs.CL

TL;DR: MoE模型性能主要由总参数量(N_total)和专家稀疏度(s)决定，而非仅考虑总参数和激活参数。研究发现专家数量(n_exp)和top-k专家数(n_topk)在稀疏度中不会抵消，专家数量增加会轻微损害性能。提出MoE设计原则：在约束下最大化总参数量，最小化稀疏度和专家数量。


<details>
  <summary>Details</summary>
Motivation: 当前MoE语言模型设计主要基于总参数（内存占用）和激活参数（推理成本），但研究发现这两个因素不足以描述最优架构。需要更系统的理解来指导MoE设计，解决架构模糊性问题。

Method: 通过系统研究分析MoE性能的决定因素，考察总参数量(N_total)、专家稀疏度(s=n_exp/n_topk)、专家数量(n_exp)和top-k专家数(n_topk)之间的关系。研究这些参数如何影响模型性能。

Result: 发现MoE性能主要由总参数量(N_total)和专家稀疏度(s)决定。专家数量(n_exp)和top-k专家数(n_topk)在稀疏度中不会相互抵消，增加专家数量会轻微损害性能，因为需要在内存约束下减少核心模型维度（深度和宽度）。

Conclusion: 提出MoE设计简单原则：在给定约束下最大化总参数量(N_total)，同时最小化专家稀疏度(s)和专家数量(n_exp)。这为MoE架构设计提供了稳健框架，解决了架构模糊性问题。

Abstract: Modern Mixture-of-Experts (MoE) language models are designed based on total parameters (memory footprint) and active parameters (inference cost). However, we find these two factors alone are insufficient to describe an optimal architecture. Through a systematic study, we demonstrate that MoE performance is primarily determined by total parameters ($N_{total}$) and expert sparsity ($s:=n_{exp}/n_{topk}$).
  Moreover, $n_{exp}$ and $n_{topk}$ do not "cancel out" within the sparsity ratio; instead, a larger total number of experts slightly penalizes performance by forcing a reduction in core model dimensions (depth and width) to meet memory constraints. This motivates a simple principle for MoE design which maximizes $N_{total}$ while minimizing $s$ (maximizing $n_{topk}$) and $n_{exp}$ under the given constraints. Our findings provide a robust framework for resolving architectural ambiguity and guiding MoE design.

</details>


### [30] [User-Oriented Multi-Turn Dialogue Generation with Tool Use at scale](https://arxiv.org/abs/2601.08225)
*Jungho Cho,Minbyul Jeong,Sungrae Park*

Main category: cs.CL

TL;DR: 提出用户导向的模拟框架，通过解耦任务生成与用户模拟器，生成更真实的多轮工具使用对话数据，解决现有任务导向方法生成对话过短的问题。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型作为自主代理需要复杂多轮工具使用能力，但现有数据集受限于静态预定义工具集，且任务导向设计导致对话过短，无法反映真实人类-代理协作的迭代性。

Method: 采用用户导向模拟范式，将任务生成与用户模拟器解耦，用户模拟器模仿人类行为规则（如增量请求和逐轮反馈），生成更真实的多轮对话。构建可插拔生成管道，可从任意状态启动生成，支持单轨迹内完成多个任务。

Result: 开发出可扩展的生成框架，能产生更真实、轮次更多的多轮工具使用对话，创建高密度数据集，反映真实世界人类-代理交互的多方面需求。

Conclusion: 用户导向模拟范式能有效生成更真实的多轮工具使用对话数据，为大型推理模型的工具使用能力训练提供高质量、可扩展的数据集，促进更自然的人类-代理协作。

Abstract: The recent paradigm shift toward large reasoning models (LRMs) as autonomous agents has intensified the demand for sophisticated, multi-turn tool-use capabilities. Yet, existing datasets and data-generation approaches are limited by static, predefined toolsets that cannot scale to the complexity of open-ended human-agent collaboration. To address this, we initially developed a framework for automated task-oriented multi-turn dialogue generation at scale, utilizing an LRM-based simulator to dynamically generate high-value, domain-specific tools to solve specified tasks. However, we observe that a purely task-oriented design often results in "solely task-solving" trajectories, where the agent completes the objective with minimal interaction, failing to generate the high turn-count conversations seen in realistic scenarios. To bridge this gap, we shift toward a user-oriented simulation paradigm. By decoupling task generation from a dedicated user simulator that mimics human behavioral rules - such as incremental request-making and turn-by-turn feedback - we facilitate more authentic, extended multi-turn dialogues that reflect the iterative nature of real-world problem solving. Our generation pipeline operates as a versatile, plug-and-play module capable of initiating generation from any state, ensuring high scalability in producing extended tool-use data. Furthermore, by facilitating multiple task completions within a single trajectory, it yields a high-density dataset that reflects the multifaceted demands of real-world human-agent interaction.

</details>


### [31] [Med-CoReasoner: Reducing Language Disparities in Medical Reasoning via Language-Informed Co-Reasoning](https://arxiv.org/abs/2601.08267)
*Fan Gao,Sherry T. Tong,Jiwoong Sohn,Jiahao Huang,Junfeng Jiang,Ding Xia,Piyalitt Ittichaiwong,Kanyakorn Veerakanjana,Hyunjae Kim,Qingyu Chen,Edison Marrese Taylor,Kazuma Kobayashi,Akkiko Aizawa,Irene Li*

Main category: cs.CL

TL;DR: Med-CoReasoner是一个语言感知的协同推理框架，通过并行英语和本地语言推理、概念抽象和知识整合，解决多语言医疗任务中的推理差距问题。


<details>
  <summary>Details</summary>
Motivation: 当前增强推理能力的大语言模型在英语医疗任务上表现良好，但在多语言环境下存在显著的推理差距，特别是在本地语言中表现较弱，这限制了医疗AI在全球范围内的公平部署。

Method: Med-CoReasoner框架包含：1）并行英语和本地语言推理；2）将推理抽象为结构化概念；3）通过概念级对齐和检索将本地临床知识整合到英语逻辑框架中。该方法结合了英语推理的结构稳健性和本地语言中编码的实践专业知识。

Result: 在三个基准测试中，Med-CoReasoner将多语言推理性能平均提高了5%，在低资源语言中提升尤为显著。模型蒸馏和专家评估进一步证实该框架能产生临床合理且文化基础扎实的推理轨迹。

Conclusion: Med-CoReasoner通过结合英语的结构优势和本地语言的实践专业知识，有效缩小了多语言医疗推理差距，为全球医疗AI的公平部署提供了解决方案。

Abstract: While reasoning-enhanced large language models perform strongly on English medical tasks, a persistent multilingual gap remains, with substantially weaker reasoning in local languages, limiting equitable global medical deployment. To bridge this gap, we introduce Med-CoReasoner, a language-informed co-reasoning framework that elicits parallel English and local-language reasoning, abstracts them into structured concepts, and integrates local clinical knowledge into an English logical scaffold via concept-level alignment and retrieval. This design combines the structural robustness of English reasoning with the practice-grounded expertise encoded in local languages. To evaluate multilingual medical reasoning beyond multiple-choice settings, we construct MultiMed-X, a benchmark covering seven languages with expert-annotated long-form question answering and natural language inference tasks, comprising 350 instances per language. Experiments across three benchmarks show that Med-CoReasoner improves multilingual reasoning performance by an average of 5%, with particularly substantial gains in low-resource languages. Moreover, model distillation and expert evaluation analysis further confirm that Med-CoReasoner produces clinically sound and culturally grounded reasoning traces.

</details>


### [32] [Discovery and Reinforcement of Tool-Integrated Reasoning Chains via Rollout Trees](https://arxiv.org/abs/2601.08274)
*Kun Li,Zenan Xu,Junan Li,Zengrui Jin,Jinghao Deng,Zexuan Qiu,Bo Zhou*

Main category: cs.CL

TL;DR: DART是一个强化学习框架，通过动态rollout树在长链思维推理中自发发现工具使用机会，无需人工标注，显著提升LLM在复杂推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前工具集成推理范式虽然能增强LLM的计算能力，但在长链思维推理中集成工具使用仍面临挑战：训练数据稀缺，且工具使用可能损害模型固有的长链推理能力。

Method: DART框架包含两个核心组件：1）在训练过程中构建动态rollout树，在promising位置分支探索多样化的工具集成轨迹；2）基于树的优势估计方法，识别并强化那些工具调用对解决方案有积极贡献的特定子轨迹。

Result: 在AIME和GPQA-Diamond等具有挑战性的基准测试中，DART显著优于现有方法，成功实现了工具执行与长链思维推理的协调统一。

Conclusion: DART通过强化学习框架成功解决了长链思维推理中工具集成的挑战，实现了无需人工标注的自发工具使用，为增强LLM的复杂推理能力提供了有效解决方案。

Abstract: Tool-Integrated Reasoning has emerged as a key paradigm to augment Large Language Models (LLMs) with computational capabilities, yet integrating tool-use into long Chain-of-Thought (long CoT) remains underexplored, largely due to the scarcity of training data and the challenge of integrating tool-use without compromising the model's intrinsic long-chain reasoning. In this paper, we introduce DART (Discovery And Reinforcement of Tool-Integrated Reasoning Chains via Rollout Trees), a reinforcement learning framework that enables spontaneous tool-use during long CoT reasoning without human annotation. DART operates by constructing dynamic rollout trees during training to discover valid tool-use opportunities, branching out at promising positions to explore diverse tool-integrated trajectories. Subsequently, a tree-based process advantage estimation identifies and credits specific sub-trajectories where tool invocation positively contributes to the solution, effectively reinforcing these beneficial behaviors. Extensive experiments on challenging benchmarks like AIME and GPQA-Diamond demonstrate that DART significantly outperforms existing methods, successfully harmonizing tool execution with long CoT reasoning.

</details>


### [33] [D$^2$Plan: Dual-Agent Dynamic Global Planning for Complex Retrieval-Augmented Reasoning](https://arxiv.org/abs/2601.08282)
*Kangcheng Luo,Tinglang Wu,Yansong Feng*

Main category: cs.CL

TL;DR: D²Plan：双智能体动态全局规划范式，通过Reasoner和Purifier的协作解决检索增强推理中的搜索链构建和推理劫持问题


<details>
  <summary>Details</summary>
Motivation: 当前基于强化学习的检索增强LLM在多跳推理任务中面临两个关键失败模式：1）搜索链构建无效，产生错误查询或遗漏关键信息检索；2）推理被外围证据劫持，导致模型将干扰项误认为有效证据

Method: 提出D²Plan双智能体动态全局规划范式：Reasoner在推理过程中构建显式全局计划并根据检索反馈动态调整；Purifier评估检索相关性并为Reasoner浓缩关键信息。采用两阶段训练框架：基于合成轨迹的监督微调冷启动，以及基于计划导向奖励的强化学习

Result: 实验表明D²Plan能够实现更连贯的多步推理，对无关信息具有更强的鲁棒性，在具有挑战性的QA基准测试中取得优越性能

Conclusion: D²Plan通过双智能体协作和动态全局规划，有效解决了检索增强推理中的关键挑战，提升了多跳推理任务的性能和鲁棒性

Abstract: Recent search-augmented LLMs trained with reinforcement learning (RL) can interleave searching and reasoning for multi-hop reasoning tasks. However, they face two critical failure modes as the accumulating context becomes flooded with both crucial evidence and irrelevant information: (1) ineffective search chain construction that produces incorrect queries or omits retrieval of critical information, and (2) reasoning hijacking by peripheral evidence that causes models to misidentify distractors as valid evidence. To address these challenges, we propose **D$^2$Plan**, a **D**ual-agent **D**ynamic global **Plan**ning paradigm for complex retrieval-augmented reasoning. **D$^2$Plan** operates through the collaboration of a *Reasoner* and a *Purifier*: the *Reasoner* constructs explicit global plans during reasoning and dynamically adapts them based on retrieval feedback; the *Purifier* assesses retrieval relevance and condenses key information for the *Reasoner*. We further introduce a two-stage training framework consisting of supervised fine-tuning (SFT) cold-start on synthesized trajectories and RL with plan-oriented rewards to teach LLMs to master the **D$^2$Plan** paradigm. Extensive experiments demonstrate that **D$^2$Plan** enables more coherent multi-step reasoning and stronger resilience to irrelevant information, thereby achieving superior performance on challenging QA benchmarks.

</details>


### [34] [Enhancing Sentiment Classification and Irony Detection in Large Language Models through Advanced Prompt Engineering Techniques](https://arxiv.org/abs/2601.08302)
*Marvin Schmitt,Anne Schwerk,Sebastian Lempert*

Main category: cs.CL

TL;DR: 该研究探讨了如何通过提示工程提升GPT-4o-mini和gemini-1.5-flash在情感分析任务中的表现，发现不同提示策略对不同模型和任务有针对性效果。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索如何通过先进的提示工程技术来增强大型语言模型在情感分析任务中的性能，特别是针对不同模型和复杂情感分析任务（如讽刺检测）的优化。

Method: 研究方法包括使用少量样本学习、思维链提示和自一致性等高级提示技术，与基线方法对比。评估任务涵盖情感分类、基于方面的情感分析和讽刺检测等。使用准确率、召回率、精确率和F1分数等指标评估模型性能。

Result: 研究发现高级提示技术显著提升了情感分析性能：少量样本学习在GPT-4o-mini中表现最佳，而思维链提示使gemini-1.5-flash在讽刺检测任务中性能提升高达46%。

Conclusion: 结论表明，虽然高级提示技术总体上能提升性能，但最佳提示策略需要根据具体模型和任务进行定制。提示设计应与LLM架构和任务语义复杂性相匹配。

Abstract: This study investigates the use of prompt engineering to enhance large language models (LLMs), specifically GPT-4o-mini and gemini-1.5-flash, in sentiment analysis tasks. It evaluates advanced prompting techniques like few-shot learning, chain-of-thought prompting, and self-consistency against a baseline. Key tasks include sentiment classification, aspect-based sentiment analysis, and detecting subtle nuances such as irony. The research details the theoretical background, datasets, and methods used, assessing performance of LLMs as measured by accuracy, recall, precision, and F1 score. Findings reveal that advanced prompting significantly improves sentiment analysis, with the few-shot approach excelling in GPT-4o-mini and chain-of-thought prompting boosting irony detection in gemini-1.5-flash by up to 46%. Thus, while advanced prompting techniques overall improve performance, the fact that few-shot prompting works best for GPT-4o-mini and chain-of-thought excels in gemini-1.5-flash for irony detection suggests that prompting strategies must be tailored to both the model and the task. This highlights the importance of aligning prompt design with both the LLM's architecture and the semantic complexity of the task.

</details>


### [35] [AgriAgent: Contract-Driven Planning and Capability-Aware Tool Orchestration in Real-World Agriculture](https://arxiv.org/abs/2601.08308)
*Bo Yang,Yu Zhang,Yunkui Chen,Lanfei Feng,Xiao Xu,Nueraili Aierken,Shijian Li*

Main category: cs.CL

TL;DR: AgriAgent是一个用于真实农业场景的两级智能体框架，通过分层执行策略处理不同复杂度的任务，简单任务直接推理，复杂任务采用契约驱动规划机制，实现多步可验证执行和故障恢复。


<details>
  <summary>Details</summary>
Motivation: 真实农业场景中的智能体系统需要处理从轻量级信息理解到复杂多步执行的不同任务，但现有方法采用统一的执行范式，难以适应任务复杂度差异大和工具可用性不完整的农业环境挑战。

Method: 提出AgriAgent两级智能体框架：1) 简单任务由特定模态智能体直接推理处理；2) 复杂任务触发契约驱动规划机制，将任务转化为能力需求，执行能力感知的工具编排和动态工具生成，支持多步可验证执行和故障恢复。

Result: 实验结果表明，与依赖统一执行范式的现有工具中心化智能体基线相比，AgriAgent在复杂任务上实现了更高的执行成功率和鲁棒性。

Conclusion: AgriAgent通过分层执行策略有效解决了农业环境中任务复杂度差异大和工具可用性不完整的问题，为真实农业场景的智能体系统提供了更灵活和鲁棒的解决方案。

Abstract: Intelligent agent systems in real-world agricultural scenarios must handle diverse tasks under multimodal inputs, ranging from lightweight information understanding to complex multi-step execution. However, most existing approaches rely on a unified execution paradigm, which struggles to accommodate large variations in task complexity and incomplete tool availability commonly observed in agricultural environments. To address this challenge, we propose AgriAgent, a two-level agent framework for real-world agriculture. AgriAgent adopts a hierarchical execution strategy based on task complexity: simple tasks are handled through direct reasoning by modality-specific agents, while complex tasks trigger a contract-driven planning mechanism that formulates tasks as capability requirements and performs capability-aware tool orchestration and dynamic tool generation, enabling multi-step and verifiable execution with failure recovery. Experimental results show that AgriAgent achieves higher execution success rates and robustness on complex tasks compared to existing tool-centric agent baselines that rely on unified execution paradigms. All code, data will be released at after our work be accepted to promote reproducible research.

</details>


### [36] [CLaS-Bench: A Cross-Lingual Alignment and Steering Benchmark](https://arxiv.org/abs/2601.08331)
*Daniil Gurgurov,Yusser Al Ghussin,Tanja Baeumel,Cheng-Ting Chou,Patrick Schramowski,Marius Mosbach,Josef van Genabith,Simon Ostermann*

Main category: cs.CL

TL;DR: CLaS-Bench是一个轻量级并行问题基准，用于评估LLM在32种语言中的语言强制行为，为多语言转向技术提供标准化评估框架。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏专门评估LLM多语言转向技术效果的基准和评估协议，需要系统化方法来量化这些技术在语言控制和语义保持方面的有效性。

Method: 构建包含32种语言的并行问题基准CLaS-Bench，评估多种转向技术：残差流DiffMean干预、探针推导方向、语言特定神经元、PCA/LDA向量、稀疏自编码器和提示基线。通过语言控制和语义相关性两个维度，结合为调和平均转向分数来衡量性能。

Result: 发现简单的基于残差的DiffMean方法在所有语言中一致优于其他方法。层分析显示语言特定结构主要出现在后期层，转向方向按语系聚类。

Conclusion: CLaS-Bench是首个多语言转向标准化基准，既支持语言表示的科学分析，又为低成本的模型适应替代方案提供实用评估。

Abstract: Understanding and controlling the behavior of large language models (LLMs) is an increasingly important topic in multilingual NLP. Beyond prompting or fine-tuning, , i.e.,~manipulating internal representations during inference, has emerged as a more efficient and interpretable technique for adapting models to a target language. Yet, no dedicated benchmarks or evaluation protocols exist to quantify the effectiveness of steering techniques. We introduce CLaS-Bench, a lightweight parallel-question benchmark for evaluating language-forcing behavior in LLMs across 32 languages, enabling systematic evaluation of multilingual steering methods. We evaluate a broad array of steering techniques, including residual-stream DiffMean interventions, probe-derived directions, language-specific neurons, PCA/LDA vectors, Sparse Autoencoders, and prompting baselines. Steering performance is measured along two axes: language control and semantic relevance, combined into a single harmonic-mean steering score. We find that across languages simple residual-based DiffMean method consistently outperforms all other methods. Moreover, a layer-wise analysis reveals that language-specific structure emerges predominantly in later layers and steering directions cluster based on language family. CLaS-Bench is the first standardized benchmark for multilingual steering, enabling both rigorous scientific analysis of language representations and practical evaluation of steering as a low-cost adaptation alternative.

</details>


### [37] [Detecting Mental Manipulation in Speech via Synthetic Multi-Speaker Dialogue](https://arxiv.org/abs/2601.08342)
*Run Chen,Wen Liang,Ziwei Gong,Lin Ai,Julia Hirschberg*

Main category: cs.CL

TL;DR: 首个针对口语对话的心理操纵检测研究，通过合成语音数据集评估模态对检测准确性的影响，发现模型在语音上的召回率显著低于文本。


<details>
  <summary>Details</summary>
Motivation: 心理操纵作为计算社会推理的新兴任务，先前研究仅关注文本对话，忽视了操纵策略在语音中的表现。需要研究语音模态如何影响操纵检测的准确性和感知。

Method: 创建合成多说话者基准SPEECHMENTALMANIP，将文本数据集通过高质量、声音一致的文本转语音技术转换为音频。使用少样本大型音频-语言模型和人工标注，评估模态对检测的影响。

Result: 模型在语音上表现出高特异性但召回率显著低于文本，表明对训练中缺失的声学或韵律线索敏感。人类评估者在音频设置中也表现出类似的不确定性，突显了操纵性语音固有的模糊性。

Conclusion: 研究强调了在多模态对话系统中进行模态感知评估和安全对齐的必要性，语音模态为心理操纵检测带来了独特的挑战。

Abstract: Mental manipulation, the strategic use of language to covertly influence or exploit others, is a newly emerging task in computational social reasoning. Prior work has focused exclusively on textual conversations, overlooking how manipulative tactics manifest in speech. We present the first study of mental manipulation detection in spoken dialogues, introducing a synthetic multi-speaker benchmark SPEECHMENTALMANIP that augments a text-based dataset with high-quality, voice-consistent Text-to-Speech rendered audio. Using few-shot large audio-language models and human annotation, we evaluate how modality affects detection accuracy and perception. Our results reveal that models exhibit high specificity but markedly lower recall on speech compared to text, suggesting sensitivity to missing acoustic or prosodic cues in training. Human raters show similar uncertainty in the audio setting, underscoring the inherent ambiguity of manipulative speech. Together, these findings highlight the need for modality-aware evaluation and safety alignment in multimodal dialogue systems.

</details>


### [38] [PATS: Personality-Aware Teaching Strategies with Large Language Model Tutors](https://arxiv.org/abs/2601.08402)
*Donya Rooein,Sankalan Pal Chowdhury,Mariia Eremeeva,Yuan Qin,Debora Nozza,Mrinmaya Sachan,Dirk Hovy*

Main category: cs.CL

TL;DR: 本文提出了一种基于学生个性特征调整LLM辅导策略的方法，通过构建个性-教学法分类体系，让LLM根据模拟学生个性自适应调整辅导策略，获得人类教师的一致偏好。


<details>
  <summary>Details</summary>
Motivation: 当前LLM辅导系统未能考虑学生个性特征，而不同的辅导策略对不同个性的学生效果不同，策略不匹配可能对学生学习成果产生负面影响。

Method: 首先基于教育学文献构建个性特征与教学方法的分类体系，然后模拟师生对话，让LLM辅导系统根据模拟学生的个性特征调整其辅导策略。

Result: 人类教师一致偏好本文方法胜过两个基线方法，同时该方法增加了较少使用但高影响力的策略（如角色扮演）的使用频率，人类和LLM标注者都显著偏好这些策略。

Conclusion: 该方法为在教育应用中开发更个性化、更有效的LLM使用铺平了道路，能够根据学生个性特征提供定制化的辅导策略。

Abstract: Recent advances in large language models (LLMs) demonstrate their potential as educational tutors. However, different tutoring strategies benefit different student personalities, and mismatches can be counterproductive to student outcomes. Despite this, current LLM tutoring systems do not take into account student personality traits. To address this problem, we first construct a taxonomy that links pedagogical methods to personality profiles, based on pedagogical literature. We simulate student-teacher conversations and use our framework to let the LLM tutor adjust its strategy to the simulated student personality. We evaluate the scenario with human teachers and find that they consistently prefer our approach over two baselines. Our method also increases the use of less common, high-impact strategies such as role-playing, which human and LLM annotators prefer significantly. Our findings pave the way for developing more personalized and effective LLM use in educational applications.

</details>


### [39] [Silence the Judge: Reinforcement Learning with Self-Verifier via Latent Geometric Clustering](https://arxiv.org/abs/2601.08427)
*Nonghai Zhang,Weitao Ma,Zhanyu Ma,Jun Xu,Jiuchong Gao,Jinghua Hao,Renqing He,Jingwen Xu*

Main category: cs.CL

TL;DR: Latent-GRPO：通过潜在空间几何学直接从潜在表示中获取内在奖励，避免依赖外部验证器，实现2倍以上的训练加速


<details>
  <summary>Details</summary>
Motivation: 现有的GRPO方法依赖昂贵的外部验证器或人工规则，导致计算成本高、训练延迟大，且稀疏奖励阻碍优化效率

Method: 提出Latent-GRPO框架，基于终端token表示的几何特性（正确轨迹形成密集聚类，错误轨迹为离群点），开发迭代鲁棒质心估计算法（IRCE），通过球面投影缓解幅度波动，迭代聚合估计"真理质心"

Result: 在多个数据集上保持模型性能的同时，相比基线实现2倍以上的训练加速，并展现出强大的泛化能力和鲁棒性

Conclusion: Latent-GRPO通过潜在空间几何学有效解决GRPO对外部验证器的依赖问题，显著提升训练效率，为LLM推理优化提供新思路

Abstract: Group Relative Policy Optimization (GRPO) significantly enhances the reasoning performance of Large Language Models (LLMs). However, this success heavily relies on expensive external verifiers or human rules. Such dependency not only leads to significant computational costs and training latency, but also yields sparse rewards that hinder optimization efficiency. To address these challenges, we propose Latent-GRPO, a framework that derives intrinsic rewards directly from latent space geometry. Crucially, our empirical analysis reveals a compelling geometric property: terminal token representations of correct reasoning trajectories form dense clusters with high intra-class similarity, whereas incorrect trajectories remain scattered as outliers. In light of this discovery, we introduce the Iterative Robust Centroid Estimation (IRCE) algorithm, which generates dense, continuous rewards by mitigating magnitude fluctuations via spherical projection and estimating a robust ``truth centroid'' through iterative aggregation. Experimental results on multiple datasets show that our method maintains model performance while achieving a training speedup of over 2x compared to baselines. Furthermore, extensive results demonstrate strong generalization ability and robustness. The code will be released soon.

</details>


### [40] [Fine-Mem: Fine-Grained Feedback Alignment for Long-Horizon Memory Management](https://arxiv.org/abs/2601.08435)
*Weitao Ma,Xiaocheng Feng,Lei Huang,Xiachong Feng,Zhanyu Ma,Jun Xu,Jiuchong Gao,Jinghua Hao,Renqing He,Bing Qin*

Main category: cs.CL

TL;DR: Fine-Mem框架通过细粒度反馈对齐解决LLM智能体内存管理中的奖励稀疏问题，使用块级步奖励和证据锚定奖励归因来优化内存操作


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的内存管理器使用最终任务性能作为主要奖励，导致严重的奖励稀疏性和无效的信用分配，无法为单个内存操作提供足够指导

Method: 提出Fine-Mem统一框架：1) 块级步奖励：通过辅助块特定问答任务提供即时步级监督；2) 证据锚定奖励归因：基于推理中使用的具体内存项作为证据，将全局奖励重新分配给关键内存操作

Result: 在Memalpha和MemoryAgentBench上的实验显示，Fine-Mem始终优于强基线，在各种子任务中实现更高的成功率。进一步分析显示其在不同模型配置和骨干网络中的适应性和强泛化能力

Conclusion: Fine-Mem通过细粒度反馈对齐实现了稳定的策略优化，将局部内存操作与内存的长期效用对齐，有效解决了内存管理中的奖励稀疏问题

Abstract: Effective memory management is essential for large language model agents to navigate long-horizon tasks. Recent research has explored using Reinforcement Learning to develop specialized memory manager agents. However, existing approaches rely on final task performance as the primary reward, which results in severe reward sparsity and ineffective credit assignment, providing insufficient guidance for individual memory operations. To this end, we propose Fine-Mem, a unified framework designed for fine-grained feedback alignment. First, we introduce a Chunk-level Step Reward to provide immediate step-level supervision via auxiliary chunk-specific question answering tasks. Second, we devise Evidence-Anchored Reward Attribution to redistribute global rewards by anchoring credit to key memory operations, based on the specific memory items utilized as evidence in reasoning. Together, these components enable stable policy optimization and align local memory operations with the long-term utility of memory. Experiments on Memalpha and MemoryAgentBench demonstrate that Fine-Mem consistently outperforms strong baselines, achieving superior success rates across various sub-tasks. Further analysis reveals its adaptability and strong generalization capabilities across diverse model configurations and backbones.

</details>


### [41] [JudgeRLVR: Judge First, Generate Second for Efficient Reasoning](https://arxiv.org/abs/2601.08468)
*Jiangshan Duo,Hanyu Li,Hailin Zhang,Yudong Wang,Sujian Li,Liang Zhao*

Main category: cs.CL

TL;DR: JudgeRLVR：两阶段判别-生成范式，通过先训练模型判别有效解，再基于此进行生成，在数学推理任务上实现精度提升同时减少生成长度


<details>
  <summary>Details</summary>
Motivation: 传统RLVR仅优化最终答案正确性，导致模型陷入冗长无目的的探索，依赖试错而非结构化规划。启发式约束（如长度惩罚）可能截断必要推理步骤，在效率与验证间存在权衡困难。

Method: 提出JudgeRLVR两阶段范式：1) 训练模型判别具有可验证答案的解决方案；2) 基于判别器初始化，使用标准生成式RLVR微调同一模型。通过判别能力引导搜索空间剪枝。

Result: 在Qwen3-30B-A3B上，相比传统RLVR：领域内数学任务平均精度提升约3.7分，平均生成长度减少42%；领域外基准平均精度提升约4.5分，展示增强的泛化能力。

Conclusion: 判别能力是高效生成的前提，通过先学习区分有效解，模型能内化引导信号剪枝搜索空间，实现更好的质量-效率权衡和泛化性能。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has become a standard paradigm for reasoning in Large Language Models. However, optimizing solely for final-answer correctness often drives models into aimless, verbose exploration, where they rely on exhaustive trial-and-error tactics rather than structured planning to reach solutions. While heuristic constraints like length penalties can reduce verbosity, they often truncate essential reasoning steps, creating a difficult trade-off between efficiency and verification. In this paper, we argue that discriminative capability is a prerequisite for efficient generation: by learning to distinguish valid solutions, a model can internalize a guidance signal that prunes the search space. We propose JudgeRLVR, a two-stage judge-then-generate paradigm. In the first stage, we train the model to judge solution responses with verifiable answers. In the second stage, we fine-tune the same model with vanilla generating RLVR initialized from the judge. Compared to Vanilla RLVR using the same math-domain training data, JudgeRLVR achieves a better quality--efficiency trade-off for Qwen3-30B-A3B: on in-domain math, it delivers about +3.7 points average accuracy gain with -42\% average generation length; on out-of-domain benchmarks, it delivers about +4.5 points average accuracy improvement, demonstrating enhanced generalization.

</details>


### [42] [sui-1: Grounded and Verifiable Long-Form Summarization](https://arxiv.org/abs/2601.08472)
*Benedikt Droste,Jan Philipp Harries,Maximilian Idahl,Björn Plüster*

Main category: cs.CL

TL;DR: sui-1是一个24B参数的模型，能生成带内联引用的摘要，让用户可以追溯每个主张到源句子，在引用基础摘要任务上显著优于更大的模型


<details>
  <summary>Details</summary>
Motivation: 大型语言模型经常生成看似合理但不忠实的摘要，用户无法对照源文本验证，这在政府和法律分析等合规敏感领域是一个关键限制

Method: 使用合成数据管道，结合思维链提示和多阶段验证，从议会文件、网页文本和维基百科等多样化来源生成超过22,000个高质量训练示例，涵盖五种语言

Result: sui-1显著优于所有测试的开源基线模型，包括参数多3倍的模型，展示了任务特定训练在引用基础摘要任务上明显优于单纯扩大模型规模

Conclusion: 模型权重和交互演示已公开可用，证明了任务特定训练在实现忠实、可验证摘要方面的重要性

Abstract: Large language models frequently generate plausible but unfaithful summaries that users cannot verify against source text, a critical limitation in compliance-sensitive domains such as government and legal analysis. We present sui-1, a 24B parameter model that produces abstractive summaries with inline citations, enabling users to trace each claim to its source sentence. Our synthetic data pipeline combines chain-of-thought prompting with multi-stage verification, generating over 22,000 high-quality training examples across five languages from diverse sources including parliamentary documents, web text, and Wikipedia. Evaluation shows sui-1 significantly outperforms all tested open-weight baselines, including models with 3x more parameters. These results demonstrate that task-specific training substantially outperforms scale alone for citation-grounded summarization. Model weights and an interactive demo are publicly available.

</details>


### [43] [Do You Understand How I Feel?: Towards Verified Empathy in Therapy Chatbots](https://arxiv.org/abs/2601.08477)
*Francesco Dettori,Matteo Forasassi,Lorenzo Veronese,Livia Lestingi,Vincenzo Scotti,Matteo Giovanni Rossi*

Main category: cs.CL

TL;DR: 提出结合NLP与形式化验证的框架，用于构建和验证具有同理心的治疗对话机器人


<details>
  <summary>Details</summary>
Motivation: 当前对话机器人在心理治疗应用中缺乏系统化的同理心规范和验证方法，而同理心是治疗场景的关键非功能性需求

Method: 使用Transformer模型提取对话特征，转换为随机混合自动机模型，通过统计模型检查验证同理心属性，策略合成指导智能体行为

Result: 形式化模型能较好地捕捉治疗对话动态，定制策略提高了满足同理心需求的概率

Conclusion: 该框架为开发具有同理心的治疗对话机器人提供了系统化的规范和验证方法

Abstract: Conversational agents are increasingly used as support tools along mental therapeutic pathways with significant societal impacts. In particular, empathy is a key non-functional requirement in therapeutic contexts, yet current chatbot development practices provide no systematic means to specify or verify it. This paper envisions a framework integrating natural language processing and formal verification to deliver empathetic therapy chatbots. A Transformer-based model extracts dialogue features, which are then translated into a Stochastic Hybrid Automaton model of dyadic therapy sessions. Empathy-related properties can then be verified through Statistical Model Checking, while strategy synthesis provides guidance for shaping agent behavior. Preliminary results show that the formal model captures therapy dynamics with good fidelity and that ad-hoc strategies improve the probability of satisfying empathy requirements.

</details>


### [44] [Surgical Refusal Ablation: Disentangling Safety from Intelligence via Concept-Guided Spectral Cleaning](https://arxiv.org/abs/2601.08489)
*Tony Cristofano*

Main category: cs.CL

TL;DR: SRA通过正交化拒绝向量来减少有害请求拒绝，同时最小化对模型能力的损害


<details>
  <summary>Details</summary>
Motivation: 现有方法在消除拒绝行为时会造成能力损害和分布漂移，因为原始拒绝向量是多义的，混杂了拒绝信号、核心能力回路和语言风格

Method: SRA构建独立概念原子表示保护能力和风格混淆，使用岭正则化谱残差化将拒绝向量正交化于这些方向，得到干净的拒绝方向

Result: 在5个模型上实现深度拒绝减少（0-2%），对Wikitext-2困惑度影响极小（平均ΔPPL≈0.02），分布漂移最小；标准消融导致严重漂移（KL=2.088），SRA保持原始分布（KL=0.044）

Conclusion: 常见的"模型损伤"往往是"幽灵噪声"，即脏拒绝方向向能力子空间的谱泄漏；SRA能有效分离拒绝行为而不损害模型能力

Abstract: Safety-aligned language models systematically refuse harmful requests. While activation steering can modulate refusal, ablating the raw "refusal vector" calculated from contrastive harmful and harmless prompts often causes collateral damage and distribution drift. We argue this degradation occurs because the raw vector is polysemantic, entangling the refusal signal with core capability circuits and linguistic style.
  We introduce Surgical Refusal Ablation (SRA) to distill these steering directions. SRA constructs a registry of independent Concept Atoms representing protected capabilities and stylistic confounds, then uses ridge-regularized spectral residualization to orthogonalize the refusal vector against these directions. This yields a clean refusal direction that targets refusal-relevant structure while minimizing disruption to the model's semantic geometry.
  Across five models (Qwen3-VL and Ministral series), SRA achieves deep refusal reduction (0-2%) with negligible perplexity impact on Wikitext-2 (mean delta PPL approx. 0.02) and minimal distribution drift. Notably, standard ablation on Qwen3-VL-4B induces severe drift (first-token KL = 2.088), whereas SRA maintains the original distribution (KL = 0.044) while achieving the same 0% refusal rate. Using teacher-forced perplexity on GSM8K and MBPP as a high-resolution capability proxy, we show SRA preserves math and code distributions. These results suggest that common "model damage" is often "Ghost Noise," defined as the spectral bleeding of the dirty refusal direction into capability subspaces.

</details>


### [45] [BenchOverflow: Measuring Overflow in Large Language Models via Plain-Text Prompts](https://arxiv.org/abs/2601.08490)
*Erin Feiglin,Nir Hutnik,Raz Lapid*

Main category: cs.CL

TL;DR: 论文研究了LLM的"溢出"现象——普通文本提示会引发过度输出，导致服务成本、延迟增加和性能下降，提出了BenchOverflow基准来量化评估，并验证了简单的缓解措施有效性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在普通交互设置下会产生过度输出的"溢出"现象，这不仅影响用户体验，还带来经济成本（服务费用增加）、环境问题（能源消耗）和系统风险（计算资源放大攻击），需要系统性研究和解决方案。

Method: 提出了BenchOverflow基准，包含9种普通文本提示策略，使用标准化协议（5000个新token预算）评估9个开源和闭源模型。采用上限饱和率（CSR@1k/3k/5k）和经验累积分布函数（ECDF）量化尾部风险，分析模型间相关性和异质性。

Result: 观察到输出长度分布明显右移和重尾现象，溢出现象在不同模型家族和攻击向量间具有可重复性但存在异质性。简单的缓解措施（固定简洁性提醒）能有效衰减右尾分布并降低CSR，对大多数模型和策略都有效。

Conclusion: 长度控制应被视为可测量的可靠性、成本和可持续性问题，而非风格偏好。BenchOverflow为选择最小化资源浪费的部署方案和评估防御措施提供了实用基础，有助于减少计算放大而不损害任务性能。

Abstract: We investigate a failure mode of large language models (LLMs) in which plain-text prompts elicit excessive outputs, a phenomenon we term Overflow. Unlike jailbreaks or prompt injection, Overflow arises under ordinary interaction settings and can lead to elevated serving cost, latency, and cross-user performance degradation, particularly when scaled across many requests. Beyond usability, the stakes are economic and environmental: unnecessary tokens increase per-request cost and energy consumption, compounding into substantial operational spend and carbon footprint at scale. Moreover, Overflow represents a practical vector for compute amplification and service degradation in shared environments. We introduce BenchOverflow, a model-agnostic benchmark of nine plain-text prompting strategies that amplify output volume without adversarial suffixes or policy circumvention. Using a standardized protocol with a fixed budget of 5000 new tokens, we evaluate nine open- and closed-source models and observe pronounced rightward shifts and heavy tails in length distributions. Cap-saturation rates (CSR@1k/3k/5k) and empirical cumulative distribution functions (ECDFs) quantify tail risk; within-prompt variance and cross-model correlations show that Overflow is broadly reproducible yet heterogeneous across families and attack vectors. A lightweight mitigation-a fixed conciseness reminder-attenuates right tails and lowers CSR for all strategies across the majority of models. Our findings position length control as a measurable reliability, cost, and sustainability concern rather than a stylistic quirk. By enabling standardized comparison of length-control robustness across models, BenchOverflow provides a practical basis for selecting deployments that minimize resource waste and operating expense, and for evaluating defenses that curb compute amplification without eroding task performance.

</details>


### [46] [It's All About the Confidence: An Unsupervised Approach for Multilingual Historical Entity Linking using Large Language Models](https://arxiv.org/abs/2601.08500)
*Cristian Santini,Marieke Van Erp,Mehwish Alam*

Main category: cs.CL

TL;DR: MHEL-LLaMo是一个用于历史文本实体链接的无监督集成方法，结合小语言模型和大型语言模型，在无需微调的情况下超越现有最佳模型。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在NLP领域取得进展，但历史文本的实体链接仍面临语言变异、噪声输入和语义演变等挑战。现有方法要么需要大量训练数据，要么依赖领域特定规则，限制了可扩展性。

Method: 提出MHEL-LLaMo，一个结合小语言模型（SLM）和大语言模型（LLM）的无监督集成方法。使用多语言双编码器（BELA）进行候选检索，指令调优的LLM通过提示链进行NIL预测和候选选择。利用SLM置信度区分简单和困难样本，仅对困难案例使用LLM。

Result: 在六个欧洲语言（英语、芬兰语、法语、德语、意大利语和瑞典语）的四个基准测试上评估，MHEL-LLaMo在无需微调的情况下超越了最先进模型，为低资源历史实体链接提供了可扩展解决方案。

Conclusion: MHEL-LLaMo通过SLM和LLM的智能集成，在降低计算成本的同时防止简单案例的幻觉，为历史文本实体链接提供了一个高效、可扩展的无监督解决方案。

Abstract: Despite the recent advancements in NLP with the advent of Large Language Models (LLMs), Entity Linking (EL) for historical texts remains challenging due to linguistic variation, noisy inputs, and evolving semantic conventions. Existing solutions either require substantial training data or rely on domain-specific rules that limit scalability. In this paper, we present MHEL-LLaMo (Multilingual Historical Entity Linking with Large Language MOdels), an unsupervised ensemble approach combining a Small Language Model (SLM) and an LLM. MHEL-LLaMo leverages a multilingual bi-encoder (BELA) for candidate retrieval and an instruction-tuned LLM for NIL prediction and candidate selection via prompt chaining. Our system uses SLM's confidence scores to discriminate between easy and hard samples, applying an LLM only for hard cases. This strategy reduces computational costs while preventing hallucinations on straightforward cases. We evaluate MHEL-LLaMo on four established benchmarks in six European languages (English, Finnish, French, German, Italian and Swedish) from the 19th and 20th centuries. Results demonstrate that MHEL-LLaMo outperforms state-of-the-art models without requiring fine-tuning, offering a scalable solution for low-resource historical EL. The implementation of MHEL-LLaMo is available on Github.

</details>


### [47] [STAGE: A Benchmark for Knowledge Graph Construction, Question Answering, and In-Script Role-Playing over Movie Screenplays](https://arxiv.org/abs/2601.08510)
*Qiuyu Tian,Yiding Li,Fengyi Chen,Zequn Liu,Youyong Kong,Fan Guo,Yuyao Li,Jinjing Shen,Zhijing Xie,Yiyun Luo,Xin Zhang*

Main category: cs.CL

TL;DR: STAGE是一个统一的电影剧本叙事理解基准，包含知识图谱构建、场景事件摘要、长上下文问答和角色扮演四个任务，基于150部电影的剧本和标注数据。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要针对问答或对话生成等单一子任务，缺乏评估模型能否构建连贯故事世界并在多种推理和生成任务中保持一致性的能力。

Method: STAGE定义了四个共享叙事世界表示的任务：知识图谱构建、场景级事件摘要、长上下文剧本问答和剧本内角色扮演，提供150部电影的清洗剧本、知识图谱和事件/角色标注。

Result: 该基准提供了英语和中文的150部电影数据集，包含清洗后的剧本、精心构建的知识图谱以及事件和角色中心的标注，支持对模型构建世界表示、抽象验证叙事事件、长叙事推理和角色一致生成能力的全面评估。

Conclusion: STAGE为评估模型在完整电影剧本上的叙事理解能力提供了一个统一的基准，能够全面测试模型构建和使用连贯故事世界表示的能力。

Abstract: Movie screenplays are rich long-form narratives that interleave complex character relationships, temporally ordered events, and dialogue-driven interactions. While prior benchmarks target individual subtasks such as question answering or dialogue generation, they rarely evaluate whether models can construct a coherent story world and use it consistently across multiple forms of reasoning and generation. We introduce STAGE (Screenplay Text, Agents, Graphs and Evaluation), a unified benchmark for narrative understanding over full-length movie screenplays. STAGE defines four tasks: knowledge graph construction, scene-level event summarization, long-context screenplay question answering, and in-script character role-playing, all grounded in a shared narrative world representation. The benchmark provides cleaned scripts, curated knowledge graphs, and event- and character-centric annotations for 150 films across English and Chinese, enabling holistic evaluation of models' abilities to build world representations, abstract and verify narrative events, reason over long narratives, and generate character-consistent responses.

</details>


### [48] [STAR: Detecting Inference-time Backdoors in LLM Reasoning via State-Transition Amplification Ratio](https://arxiv.org/abs/2601.08511)
*Seong-Gyu Park,Sohee Park,Jisu Lee,Hyunsik Na,Daeseon Choi*

Main category: cs.CL

TL;DR: STAR框架通过分析输出概率变化检测推理时后门攻击，利用恶意路径的先验概率低但后验概率高的统计差异，在多种模型上实现接近完美的检测性能。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs集成CoT等推理机制，显式推理暴露了推理时后门攻击的新攻击面。这些攻击注入恶意推理路径而不修改模型参数，由于生成语言连贯的路径，能有效规避传统检测方法。

Method: 提出STAR框架，通过分析输出概率变化检测后门。利用恶意输入诱导路径在模型一般知识中先验概率低但后验概率高的统计差异，量化状态转移放大，并采用CUSUM算法检测持续异常。

Result: 在8B-70B多种模型和五个基准数据集上的实验表明，STAR展现出强大的泛化能力，始终实现接近完美的性能（AUROC≈1.0），效率比现有基线高约42倍。框架对试图绕过检测的自适应攻击也具有鲁棒性。

Conclusion: STAR框架有效解决了推理时后门攻击的检测问题，利用统计差异和状态转移放大分析，在保持高效率的同时实现卓越的检测性能，对自适应攻击具有鲁棒性。

Abstract: Recent LLMs increasingly integrate reasoning mechanisms like Chain-of-Thought (CoT). However, this explicit reasoning exposes a new attack surface for inference-time backdoors, which inject malicious reasoning paths without altering model parameters. Because these attacks generate linguistically coherent paths, they effectively evade conventional detection. To address this, we propose STAR (State-Transition Amplification Ratio), a framework that detects backdoors by analyzing output probability shifts. STAR exploits the statistical discrepancy where a malicious input-induced path exhibits high posterior probability despite a low prior probability in the model's general knowledge. We quantify this state-transition amplification and employ the CUSUM algorithm to detect persistent anomalies. Experiments across diverse models (8B-70B) and five benchmark datasets demonstrate that STAR exhibits robust generalization capabilities, consistently achieving near-perfect performance (AUROC $\approx$ 1.0) with approximately $42\times$ greater efficiency than existing baselines. Furthermore, the framework proves robust against adaptive attacks attempting to bypass detection.

</details>


### [49] [Algorithmic Stability in Infinite Dimensions: Characterizing Unconditional Convergence in Banach Spaces](https://arxiv.org/abs/2601.08512)
*Przemysław Spyra*

Main category: cs.CL

TL;DR: 论文系统研究了无限维空间中无条件收敛的七种等价条件，建立了无条件收敛与条件收敛、绝对收敛的严格区分，并将理论结果应用于算法稳定性分析。


<details>
  <summary>Details</summary>
Motivation: 无限维空间中条件收敛、无条件收敛和绝对收敛的区分对计算算法有根本性影响。虽然这些概念在有限维空间中一致，但在一般Banach空间中存在严格分离，这需要通过Dvoretzky-Rogers定理来理解。

Method: 提出了一个统一的特征定理，将无条件收敛的七种等价条件整合：置换不变性、网收敛、子级数检验、符号稳定性、有界乘子性质、弱一致收敛等。

Result: 建立了无条件收敛的完整理论框架，揭示了这些概念在无限维空间中的严格分离，并将理论结果直接应用于算法稳定性分析。

Conclusion: 该研究将经典泛函分析与当代计算实践联系起来，为顺序无关和数值稳健的求和过程提供了严格的理论基础，特别适用于随机梯度下降中的梯度累积和基于框架的信号处理中的系数阈值化。

Abstract: The distinction between conditional, unconditional, and absolute convergence in infinite-dimensional spaces has fundamental implications for computational algorithms. While these concepts coincide in finite dimensions, the Dvoretzky-Rogers theorem establishes their strict separation in general Banach spaces. We present a comprehensive characterization theorem unifying seven equivalent conditions for unconditional convergence: permutation invariance, net convergence, subseries tests, sign stability, bounded multiplier properties, and weak uniform convergence. These theoretical results directly inform algorithmic stability analysis, governing permutation invariance in gradient accumulation for Stochastic Gradient Descent and justifying coefficient thresholding in frame-based signal processing. Our work bridges classical functional analysis with contemporary computational practice, providing rigorous foundations for order-independent and numerically robust summation processes.

</details>


### [50] [DeepResearch Bench II: Diagnosing Deep Research Agents via Rubrics from Expert Report](https://arxiv.org/abs/2601.08536)
*Ruizhe Li,Mingxuan Du,Benfeng Xu,Chiwei Zhu,Xiaorui Wang,Zhendong Mao*

Main category: cs.CL

TL;DR: Deep Research Bench II：一个用于评估深度研究系统的新基准，包含132个研究任务和9430个细粒度二元评估标准，覆盖信息召回、分析和呈现三个维度，通过LLM+人工四阶段流程构建，确保标准原子化、可验证且与专家判断一致。


<details>
  <summary>Details</summary>
Motivation: 当前深度研究系统（DRS）的评估方法存在不足：现有基准要么无法充分测试系统分析证据和撰写连贯报告的能力，要么依赖过于粗糙或由LLM直接定义的评估标准，导致评分相对于人类专家存在偏差且难以验证或解释。

Method: 构建Deep Research Bench II基准，包含132个基于真实场景的研究任务，覆盖22个领域。通过四阶段LLM+人工流程构建9430个细粒度二元评估标准：1）从专家撰写的调查文章中提取评估维度；2）LLM自动生成初始标准；3）人工专家审核和修正；4）最终验证确保标准的原子性、可验证性和与专家判断的一致性。

Result: 在Deep Research Bench II上评估多个最先进的深度研究系统，发现即使最强的模型也只能满足不到50%的评估标准，揭示了当前DRS与人类专家之间存在显著差距。

Conclusion: Deep Research Bench II提供了一个更严谨、可解释的深度研究系统评估框架，通过细粒度、原子化、可验证的评估标准，能够更准确地衡量DRS的性能，并为未来系统改进提供了明确的方向。

Abstract: Deep Research Systems (DRS) aim to help users search the web, synthesize information, and deliver comprehensive investigative reports. However, how to rigorously evaluate these systems remains under-explored. Existing deep-research benchmarks often fall into two failure modes. Some do not adequately test a system's ability to analyze evidence and write coherent reports. Others rely on evaluation criteria that are either overly coarse or directly defined by LLMs (or both), leading to scores that can be biased relative to human experts and are hard to verify or interpret. To address these issues, we introduce Deep Research Bench II, a new benchmark for evaluating DRS-generated reports. It contains 132 grounded research tasks across 22 domains; for each task, a system must produce a long-form research report that is evaluated by a set of 9430 fine-grained binary rubrics in total, covering three dimensions: information recall, analysis, and presentation. All rubrics are derived from carefully selected expert-written investigative articles and are constructed through a four-stage LLM+human pipeline that combines automatic extraction with over 400 human-hours of expert review, ensuring that the criteria are atomic, verifiable, and aligned with human expert judgment. We evaluate several state-of-the-art deep-research systems on Deep Research Bench II and find that even the strongest models satisfy fewer than 50% of the rubrics, revealing a substantial gap between current DRSs and human experts.

</details>


### [51] [Ministral 3](https://arxiv.org/abs/2601.08584)
*Alexander H. Liu,Kartik Khandelwal,Sandeep Subramanian,Victor Jouault,Abhinav Rastogi,Adrien Sadé,Alan Jeffares,Albert Jiang,Alexandre Cahill,Alexandre Gavaudan,Alexandre Sablayrolles,Amélie Héliou,Amos You,Andy Ehrenberg,Andy Lo,Anton Eliseev,Antonia Calvi,Avinash Sooriyarachchi,Baptiste Bout,Baptiste Rozière,Baudouin De Monicault,Clémence Lanfranchi,Corentin Barreau,Cyprien Courtot,Daniele Grattarola,Darius Dabert,Diego de las Casas,Elliot Chane-Sane,Faruk Ahmed,Gabrielle Berrada,Gaëtan Ecrepont,Gauthier Guinet,Georgii Novikov,Guillaume Kunsch,Guillaume Lample,Guillaume Martin,Gunshi Gupta,Jan Ludziejewski,Jason Rute,Joachim Studnia,Jonas Amar,Joséphine Delas,Josselin Somerville Roberts,Karmesh Yadav,Khyathi Chandu,Kush Jain,Laurence Aitchison,Laurent Fainsin,Léonard Blier,Lingxiao Zhao,Louis Martin,Lucile Saulnier,Luyu Gao,Maarten Buyl,Margaret Jennings,Marie Pellat,Mark Prins,Mathieu Poirée,Mathilde Guillaumin,Matthieu Dinot,Matthieu Futeral,Maxime Darrin,Maximilian Augustin,Mia Chiquier,Michel Schimpf,Nathan Grinsztajn,Neha Gupta,Nikhil Raghuraman,Olivier Bousquet,Olivier Duchenne,Patricia Wang,Patrick von Platen,Paul Jacob,Paul Wambergue,Paula Kurylowicz,Pavankumar Reddy Muddireddy,Philomène Chagniot,Pierre Stock,Pravesh Agrawal,Quentin Torroba,Romain Sauvestre,Roman Soletskyi,Rupert Menneer,Sagar Vaze,Samuel Barry,Sanchit Gandhi,Siddhant Waghjale,Siddharth Gandhi,Soham Ghosh,Srijan Mishra,Sumukh Aithal,Szymon Antoniak,Teven Le Scao,Théo Cachet,Theo Simon Sorg,Thibaut Lavril,Thiziri Nait Saada,Thomas Chabal,Thomas Foubert,Thomas Robert,Thomas Wang,Tim Lawson,Tom Bewley,Tom Bewley,Tom Edwards,Umar Jamil,Umberto Tomasini,Valeriia Nemychnikova,Van Phung,Vincent Maladière,Virgile Richard,Wassim Bouaziz,Wen-Ding Li,William Marshall,Xinghui Li,Xinyu Yang,Yassine El Ouahidi,Yihan Wang,Yunhao Tang,Zaccharie Ramzi*

Main category: cs.CL

TL;DR: Ministral 3系列是参数高效的密集语言模型，包含3B、8B、14B三种尺寸，每个尺寸都有基础预训练、指令微调和推理专用三个变体，采用级联蒸馏方法训练，具备图像理解能力，开源许可为Apache 2.0。


<details>
  <summary>Details</summary>
Motivation: 针对计算和内存受限的应用场景，开发参数高效的语言模型，提供不同规模的选择以满足不同资源约束的需求。

Method: 采用级联蒸馏方法，通过迭代剪枝和持续训练结合蒸馏技术来获得模型。每个模型尺寸都提供三个变体：基础预训练模型、指令微调模型和推理专用模型。

Result: 发布了Ministral 3系列模型，包含3B、8B、14B三种参数规模，每个规模都有三个变体，所有模型都具备图像理解能力，并在Apache 2.0许可下开源。

Conclusion: Ministral 3系列为资源受限应用提供了高效的语言模型解决方案，通过级联蒸馏方法实现了参数效率，并提供了多种变体满足不同任务需求。

Abstract: We introduce the Ministral 3 series, a family of parameter-efficient dense language models designed for compute and memory constrained applications, available in three model sizes: 3B, 8B, and 14B parameters. For each model size, we release three variants: a pretrained base model for general-purpose use, an instruction finetuned, and a reasoning model for complex problem-solving. In addition, we present our recipe to derive the Ministral 3 models through Cascade Distillation, an iterative pruning and continued training with distillation technique. Each model comes with image understanding capabilities, all under the Apache 2.0 license.

</details>


### [52] [ExpSeek: Self-Triggered Experience Seeking for Web Agents](https://arxiv.org/abs/2601.08605)
*Wenyuan Zhang,Xinghua Zhang,Haiyang Yu,Shuaiyi Nie,Bingli Wu,Juwei Yue,Tingwen Liu,Yongbin Li*

Main category: cs.CL

TL;DR: ExpSeek提出了一种新的经验干预方法，将经验从全局上下文被动注入转变为步骤级主动寻求，通过熵阈值确定干预时机，显著提升Web智能体性能。


<details>
  <summary>Details</summary>
Motivation: 现有经验干预方法主要在任务执行前将经验作为全局上下文被动注入，难以适应智能体与环境交互过程中动态变化的上下文观察。

Method: ExpSeek采用步骤级主动寻求经验的方法：1）利用模型内在信号估计步骤级熵阈值来确定干预时机；2）设计步骤级定制化经验内容。

Result: 在Qwen3-8B和32B模型上，在四个具有挑战性的Web智能体基准测试中，ExpSeek分别实现了9.3%和7.5%的绝对性能提升。实验验证了熵作为自触发信号的可行性，并发现即使是4B小规模经验模型也能显著提升更大智能体模型的性能。

Conclusion: ExpSeek通过步骤级主动寻求经验的方法，有效解决了传统经验干预方法在动态环境中的适应性问题，为Web智能体经验干预提供了新的技术范式。

Abstract: Experience intervention in web agents emerges as a promising technical paradigm, enhancing agent interaction capabilities by providing valuable insights from accumulated experiences. However, existing methods predominantly inject experience passively as global context before task execution, struggling to adapt to dynamically changing contextual observations during agent-environment interaction. We propose ExpSeek, which shifts experience toward step-level proactive seeking: (1) estimating step-level entropy thresholds to determine intervention timing using the model's intrinsic signals; (2) designing step-level tailor-designed experience content. Experiments on Qwen3-8B and 32B models across four challenging web agent benchmarks demonstrate that ExpSeek achieves absolute improvements of 9.3% and 7.5%, respectively. Our experiments validate the feasibility and advantages of entropy as a self-triggering signal, reveal that even a 4B small-scale experience model can significantly boost the performance of larger agent models.

</details>


### [53] [GraphSearch: Agentic Search-Augmented Reasoning for Zero-Shot Graph Learning](https://arxiv.org/abs/2601.08621)
*Jiajin Liu,Yuanfu Sun,Dongzhe Fan,Qiaoyu Tan*

Main category: cs.CL

TL;DR: GraphSearch：首个将搜索增强推理扩展到图学习的框架，无需任务特定微调即可实现零样本图学习，在图结构数据上表现出与监督方法竞争甚至更优的性能。


<details>
  <summary>Details</summary>
Motivation: 当前搜索增强大型推理模型主要处理文本数据，但在图结构数据（如电商、社交网络、科学引用）上的应用仍未被充分探索。图数据包含丰富的拓扑信号，可作为有价值的先验知识用于检索，但有效利用这种结构面临独特挑战，包括生成图表达查询的困难和确保结构语义相关性的平衡。

Method: GraphSearch框架包含：1）图感知查询规划器，将搜索空间（如1跳、多跳或全局邻居）与语义查询解耦；2）图感知检索器，基于拓扑构建候选集并使用混合评分函数进行排序。框架实例化两种遍历模式：GraphSearch-R（递归逐跳扩展邻域）和GraphSearch-F（灵活检索局部和全局邻域，无跳数约束）。

Result: 在多个基准测试上的广泛实验表明，GraphSearch在零样本节点分类和链接预测任务中实现了与监督图学习方法竞争甚至更优的性能，创造了新的最先进结果。

Conclusion: GraphSearch作为一个灵活且可泛化的范式，为图上的智能推理提供了新途径，展示了在无需任务特定训练的情况下有效利用图结构进行检索增强推理的潜力。

Abstract: Recent advances in search-augmented large reasoning models (LRMs) enable the retrieval of external knowledge to reduce hallucinations in multistep reasoning. However, their ability to operate on graph-structured data, prevalent in domains such as e-commerce, social networks, and scientific citations, remains underexplored. Unlike plain text corpora, graphs encode rich topological signals that connect related entities and can serve as valuable priors for retrieval, enabling more targeted search and improved reasoning efficiency. Yet, effectively leveraging such structure poses unique challenges, including the difficulty of generating graph-expressive queries and ensuring reliable retrieval that balances structural and semantic relevance. To address this gap, we introduce GraphSearch, the first framework that extends search-augmented reasoning to graph learning, enabling zero-shot graph learning without task-specific fine-tuning. GraphSearch combines a Graph-aware Query Planner, which disentangles search space (e.g., 1-hop, multi-hop, or global neighbors) from semantic queries, with a Graph-aware Retriever, which constructs candidate sets based on topology and ranks them using a hybrid scoring function. We further instantiate two traversal modes: GraphSearch-R, which recursively expands neighborhoods hop by hop, and GraphSearch-F, which flexibly retrieves across local and global neighborhoods without hop constraints. Extensive experiments across diverse benchmarks show that GraphSearch achieves competitive or even superior performance compared to supervised graph learning methods, setting state-of-the-art results in zero-shot node classification and link prediction. These findings position GraphSearch as a flexible and generalizable paradigm for agentic reasoning over graphs.

</details>


### [54] [How Order-Sensitive Are LLMs? OrderProbe for Deterministic Structural Reconstruction](https://arxiv.org/abs/2601.08626)
*Yingjie He,Zhaolu Kang,Kehan Jiang,Qianyuan Zhang,Jiachen Qian,Chunlei Meng,Yujie Feng,Yuan Wang,Jiabao Dou,Aming Wu,Leqi Zheng,Pengxiang Zhao,Jiaxin Liu,Zeyu Zhang,Lei Wang,Guansu Wang,Qishi Zhan,Xiaomin He,Meisheng Zhang,Jianyuan Ni*

Main category: cs.CL

TL;DR: OrderProbe：一个用于评估大语言模型结构重建能力的确定性基准，使用中日韩固定四字表达式，支持精确匹配评分，发现前沿模型在结构重建上仍面临困难


<details>
  <summary>Details</summary>
Motivation: 大语言模型在语义理解方面表现出色，但从乱序输入中重建内部结构的能力尚未充分探索。句子级恢复难以自动化评估，因为存在多种有效的词序排列

Method: 提出OrderProbe基准，使用中日韩语言中具有唯一规范顺序的固定四字符表达式，支持精确匹配评分。建立诊断框架评估恢复准确性、语义保真度、逻辑有效性、一致性、鲁棒敏感性和信息密度

Result: 在12个广泛使用的大语言模型上实验显示，结构重建对前沿系统仍然困难：零样本恢复准确率常低于35%。观察到语义回忆和结构规划之间存在一致分离，表明结构鲁棒性不是语义能力的自动副产品

Conclusion: 结构重建是大语言模型面临的挑战性任务，需要专门评估。语义能力和结构规划能力存在分离，表明需要专门的方法来提升模型的结构鲁棒性

Abstract: Large language models (LLMs) excel at semantic understanding, yet their ability to reconstruct internal structure from scrambled inputs remains underexplored. Sentence-level restoration is ill-posed for automated evaluation because multiple valid word orders often exist. We introduce OrderProbe, a deterministic benchmark for structural reconstruction using fixed four-character expressions in Chinese, Japanese, and Korean, which have a unique canonical order and thus support exact-match scoring. We further propose a diagnostic framework that evaluates models beyond recovery accuracy, including semantic fidelity, logical validity, consistency, robustness sensitivity, and information density. Experiments on twelve widely used LLMs show that structural reconstruction remains difficult even for frontier systems: zero-shot recovery frequently falls below 35%. We also observe a consistent dissociation between semantic recall and structural planning, suggesting that structural robustness is not an automatic byproduct of semantic competence.

</details>


### [55] [Get away with less: Need of source side data curation to build parallel corpus for low resource Machine Translation](https://arxiv.org/abs/2601.08629)
*Saumitra Yadav,Manish Shrivastava*

Main category: cs.CL

TL;DR: LALITA框架通过基于词汇和语言特征筛选复杂句子来优化低资源机器翻译的数据选择，显著提升翻译质量并减少一半以上的数据需求。


<details>
  <summary>Details</summary>
Motivation: 低资源语言机器翻译面临数据获取困难，人工翻译成本过高，需要开发高效的数据筛选框架来优化平行语料库构建，确保MT系统在低资源环境下的最佳性能。

Method: 提出LALITA框架，基于词汇和语言特征分析源语言句子，筛选复杂句子构建平行语料库，在现有和合成数据集上主要训练复杂句子，模拟低资源数据可用性（5万到80万句）。

Result: 在所有测试数据规模上均报告性能提升，LALITA在多种语言（印地语、奥里亚语、尼泊尔语、挪威尼诺斯克语、德语）上减少超过一半的数据需求，显著降低训练成本并展示数据增强效用。

Conclusion: LALITA框架通过智能句子选择策略有效优化低资源机器翻译的数据利用，不仅提高翻译质量，还大幅减少数据需求，为低资源语言MT训练提供了高效解决方案。

Abstract: Data curation is a critical yet under-researched step in the machine translation training paradigm. To train translation systems, data acquisition relies primarily on human translations and digital parallel sources or, to a limited degree, synthetic generation. But, for low-resource languages, human translation to generate sufficient data is prohibitively expensive. Therefore, it is crucial to develop a framework that screens source sentences to form efficient parallel text, ensuring optimal MT system performance in low-resource environments. We approach this by evaluating English-Hindi bi-text to determine effective sentence selection strategies for optimal MT system training. Our extensively tested framework, (Lexical And Linguistically Informed Text Analysis) LALITA, targets source sentence selection using lexical and linguistic features to curate parallel corpora. We find that by training mostly on complex sentences from both existing and synthetic datasets, our method significantly improves translation quality. We test this by simulating low-resource data availabilty with curated datasets of 50K to 800K English sentences and report improved performances on all data sizes. LALITA demonstrates remarkable efficiency, reducing data needs by more than half across multiple languages (Hindi, Odia, Nepali, Norwegian Nynorsk, and German). This approach not only reduces MT systems training cost by reducing training data requirement, but also showcases LALITA's utility in data augmentation.

</details>


### [56] [Moral Lenses, Political Coordinates: Towards Ideological Positioning of Morally Conditioned LLMs](https://arxiv.org/abs/2601.08634)
*Chenchen Yuan,Bolei Ma,Zheyu Zhang,Bardh Prenkaj,Frauke Kreuter,Gjergji Kasneci*

Main category: cs.CL

TL;DR: 该研究探讨了道德价值观与LLMs政治立场之间的因果关系，通过道德条件化控制发现道德取向能显著影响模型的政治坐标，且这种效应受角色框架和模型规模调节。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要通过直接探测或人口统计角色工程来评估LLMs的政治意识形态偏见，但社会心理学认为政治意识形态是基本道德直觉的下游结果。本研究旨在探究道德价值观与政治定位之间的因果关系。

Method: 将道德取向作为可控条件，让模型认可或拒绝特定道德价值观，然后使用政治罗盘测试评估由此产生的政治取向变化。通过将道德价值观作为"透镜"，观察道德条件化如何主动引导模型在经济和社会维度上的轨迹。

Result: 道德条件化能引起模型政治坐标的显著、特定于价值观的偏移。这些效应受角色框架和模型规模的系统性调节，并且在实例化相同道德价值观的替代评估工具中具有稳健性。

Conclusion: 有效的对齐需要将政治评估锚定在包括道德在内的更广泛社会价值观背景下，这为更具社会基础的对齐技术铺平了道路。

Abstract: While recent research has systematically documented political orientation in large language models (LLMs), existing evaluations rely primarily on direct probing or demographic persona engineering to surface ideological biases. In social psychology, however, political ideology is also understood as a downstream consequence of fundamental moral intuitions. In this work, we investigate the causal relationship between moral values and political positioning by treating moral orientation as a controllable condition. Rather than simply assigning a demographic persona, we condition models to endorse or reject specific moral values and evaluate the resulting shifts on their political orientations, using the Political Compass Test. By treating moral values as lenses, we observe how moral conditioning actively steers model trajectories across economic and social dimensions. Our findings show that such conditioning induces pronounced, value-specific shifts in models' political coordinates. We further notice that these effects are systematically modulated by role framing and model scale, and are robust across alternative assessment instruments instantiating the same moral value. This highlights that effective alignment requires anchoring political assessments within the context of broader social values including morality, paving the way for more socially grounded alignment techniques.

</details>


### [57] [A Parallel Cross-Lingual Benchmark for Multimodal Idiomaticity Understanding](https://arxiv.org/abs/2601.08645)
*Dilara Torunoğlu-Selamet,Dogukan Arslan,Rodrigo Wilkens,Wei He,Doruk Eryiğit,Thomas Pickard,Adriana S. Pagano,Aline Villavicencio,Gülşen Eryiğit,Ágnes Abuczki,Aida Cardoso,Alesia Lazarenka,Dina Almassova,Amalia Mendes,Anna Kanellopoulou,Antoni Brosa-Rodríguez,Baiba Saulite,Beata Wojtowicz,Bolette Pedersen,Carlos Manuel Hidalgo-Ternero,Chaya Liebeskind,Danka Jokić,Diego Alves,Eleni Triantafyllidi,Erik Velldal,Fred Philippy,Giedre Valunaite Oleskeviciene,Ieva Rizgeliene,Inguna Skadina,Irina Lobzhanidze,Isabell Stinessen Haugen,Jauza Akbar Krito,Jelena M. Marković,Johanna Monti,Josue Alejandro Sauca,Kaja Dobrovoljc,Kingsley O. Ugwuanyi,Laura Rituma,Lilja Øvrelid,Maha Tufail Agro,Manzura Abjalova,Maria Chatzigrigoriou,María del Mar Sánchez Ramos,Marija Pendevska,Masoumeh Seyyedrezaei,Mehrnoush Shamsfard,Momina Ahsan,Muhammad Ahsan Riaz Khan,Nathalie Carmen Hau Norman,Nilay Erdem Ayyıldız,Nina Hosseini-Kivanani,Noémi Ligeti-Nagy,Numaan Naeem,Olha Kanishcheva,Olha Yatsyshyna,Daniil Orel,Petra Giommarelli,Petya Osenova,Radovan Garabik,Regina E. Semou,Rozane Rebechi,Salsabila Zahirah Pranida,Samia Touileb,Sanni Nimb,Sarfraz Ahmad,Sarvinoz Nematkhonova,Shahar Golan,Shaoxiong Ji,Sopuruchi Christian Aboh,Srdjan Sucur,Stella Markantonatou,Sussi Olsen,Vahide Tajalli,Veronika Lipp,Voula Giouli,Yelda Yeşildal Eraydın,Zahra Saaberi,Zhuohan Xie*

Main category: cs.CL

TL;DR: XMPIE是一个包含34种语言、超过一万条项目的平行多语言多模态潜在习语表达数据集，用于评估NLP系统的语言文化能力。


<details>
  <summary>Details</summary>
Motivation: 潜在习语表达与特定语言社区的日常经验密切相关，是评估NLP系统语言文化能力的重要挑战。需要创建高质量的多语言多模态数据集来研究习语模式、跨语言迁移和跨模态理解。

Method: 由语言专家创建，包含34种语言的平行数据，每个习语表达配有5张图像，涵盖从习语到字面意义的连续谱，包括语义相关和随机干扰项。文本和视觉组件都在多语言指导下制作。

Result: 创建了一个高质量的多语言多模态习语理解基准数据集，支持跨语言比较分析、习语理解迁移研究以及文本与视觉模态间的理解关系探索。

Conclusion: XMPIE为评估多语言多模态习语语言理解提供了重要基准，有助于研究语言间的习语模式差异、跨语言理解迁移以及跨模态理解关系。

Abstract: Potentially idiomatic expressions (PIEs) construe meanings inherently tied to the everyday experience of a given language community. As such, they constitute an interesting challenge for assessing the linguistic (and to some extent cultural) capabilities of NLP systems. In this paper, we present XMPIE, a parallel multilingual and multimodal dataset of potentially idiomatic expressions. The dataset, containing 34 languages and over ten thousand items, allows comparative analyses of idiomatic patterns among language-specific realisations and preferences in order to gather insights about shared cultural aspects. This parallel dataset allows to evaluate model performance for a given PIE in different languages and whether idiomatic understanding in one language can be transferred to another. Moreover, the dataset supports the study of PIEs across textual and visual modalities, to measure to what extent PIE understanding in one modality transfers or implies in understanding in another modality (text vs. image). The data was created by language experts, with both textual and visual components crafted under multilingual guidelines, and each PIE is accompanied by five images representing a spectrum from idiomatic to literal meanings, including semantically related and random distractors. The result is a high-quality benchmark for evaluating multilingual and multimodal idiomatic language understanding.

</details>


### [58] [Safe Language Generation in the Limit](https://arxiv.org/abs/2601.08648)
*Antonios Anastasopoulos,Giuseppe Ateniese,Evgenios M. Kornaropoulos*

Main category: cs.CL

TL;DR: 该论文首次对安全语言生成进行了理论处理，证明了安全语言识别是不可能的，安全语言生成至少与普通语言识别一样困难（也是不可能的），并讨论了可处理和不可处理的情况。


<details>
  <summary>Details</summary>
Motivation: 随着语言学习领域的发展，需要在实际应用中考虑语言生成的安全性问题。尽管语言识别已被证明不可能，但语言生成是可行的，因此需要研究安全语言生成的理论基础。

Method: 基于极限学习的计算范式，形式化了安全语言识别和安全语言生成的任务。通过理论证明分析了这些任务的可行性。

Result: 证明了安全语言识别是不可能的，安全语言生成至少与普通语言识别一样困难（也是不可能的）。同时讨论了某些情况下这些任务可能是可处理的。

Conclusion: 安全语言生成在理论上是困难的，但在某些特定情况下可能是可实现的。这为实际应用中的安全语言生成系统提供了理论指导。

Abstract: Recent results in learning a language in the limit have shown that, although language identification is impossible, language generation is tractable. As this foundational area expands, we need to consider the implications of language generation in real-world settings.
  This work offers the first theoretical treatment of safe language generation. Building on the computational paradigm of learning in the limit, we formalize the tasks of safe language identification and generation. We prove that under this model, safe language identification is impossible, and that safe language generation is at least as hard as (vanilla) language identification, which is also impossible. Last, we discuss several intractable and tractable cases.

</details>


### [59] [RULERS: Locked Rubrics and Evidence-Anchored Scoring for Robust LLM Evaluation](https://arxiv.org/abs/2601.08654)
*Yihan Hong,Huaiyuan Yao,Bolin Shen,Wanpeng Xu,Hua Wei,Yushun Dong*

Main category: cs.CL

TL;DR: RULERS框架通过将自然语言评分标准编译为可执行规范，解决LLM作为评分者时的三个常见问题：标准不稳定性、不可验证推理和尺度不对齐，显著提升与人类评分的一致性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM作为评分者的方法面临三个主要挑战：1) 评分标准对提示词敏感导致的不稳定性；2) 缺乏可审计证据的不可验证推理；3) 评分尺度与人类评分边界不对齐。这些问题限制了LLM评分的可靠性和可扩展性。

Method: 提出RULERS框架，包含三个核心组件：1) 将自然语言评分标准编译为版本化不可变的标准包；2) 通过结构化解码强制确定性证据验证；3) 使用基于Wasserstein距离的轻量级后校准方法，整个过程无需更新模型参数。

Result: 在论文和摘要评估基准测试中，RULERS显著优于代表性基线方法，在人类评分一致性方面表现更好，对对抗性评分标准扰动保持强稳定性，并使较小模型能够媲美大型专有评分模型。

Conclusion: 可靠的LLM评分需要可执行的评分标准、可验证的证据和校准的评分尺度，而不仅仅是提示词的优化。RULERS框架为实现这一目标提供了有效的解决方案。

Abstract: The LLM-as-a-Judge paradigm promises scalable rubric-based evaluation, yet aligning frozen black-box models with human standards remains a challenge due to inherent generation stochasticity. We reframe judge alignment as a criteria transfer problem and isolate three recurrent failure modes: rubric instability caused by prompt sensitivity, unverifiable reasoning that lacks auditable evidence, and scale misalignment with human grading boundaries. To address these issues, we introduce RULERS (Rubric Unification, Locking, and Evidence-anchored Robust Scoring), a compiler-executor framework that transforms natural language rubrics into executable specifications. RULERS operates by compiling criteria into versioned immutable bundles, enforcing structured decoding with deterministic evidence verification, and applying lightweight Wasserstein-based post-hoc calibration, all without updating model parameters. Extensive experiments on essay and summarization benchmarks demonstrate that RULERS significantly outperforms representative baselines in human agreement, maintains strong stability against adversarial rubric perturbations, and enables smaller models to rival larger proprietary judges. Overall, our results suggest that reliable LLM judging requires executable rubrics, verifiable evidence, and calibrated scales rather than prompt phrasing alone. Code is available at https://github.com/LabRAI/Rulers.git.

</details>


### [60] [Analyzing Bias in False Refusal Behavior of Large Language Models for Hate Speech Detoxification](https://arxiv.org/abs/2601.08668)
*Kyuri Im,Shuzhou Yuan,Michael Färber*

Main category: cs.CL

TL;DR: LLMs在仇恨言论去毒化任务中存在虚假拒绝问题，对高语义毒性和特定群体（国籍、宗教、政治意识形态）的输入拒绝率更高。跨语言翻译策略（英→中→英）能有效减少虚假拒绝。


<details>
  <summary>Details</summary>
Motivation: LLMs在仇恨言论去毒化任务中经常触发安全警报而拒绝执行，需要系统研究这种虚假拒绝行为及其触发因素。

Method: 评估9个LLMs在英语和多语言数据集上的表现，分析语义毒性和目标群体对拒绝率的影响，并提出跨语言翻译策略（英译中处理后再译回英文）。

Result: LLMs对高语义毒性输入和针对特定群体（国籍、宗教、政治意识形态）的输入拒绝率显著更高；多语言数据集整体拒绝率低于英语数据集，但仍存在语言依赖的系统性偏见。

Conclusion: 跨语言翻译策略能显著减少虚假拒绝同时保持原内容，为LLMs在敏感内容处理中的偏见问题提供了轻量级缓解方案。

Abstract: While large language models (LLMs) have increasingly been applied to hate speech detoxification, the prompts often trigger safety alerts, causing LLMs to refuse the task. In this study, we systematically investigate false refusal behavior in hate speech detoxification and analyze the contextual and linguistic biases that trigger such refusals. We evaluate nine LLMs on both English and multilingual datasets, our results show that LLMs disproportionately refuse inputs with higher semantic toxicity and those targeting specific groups, particularly nationality, religion, and political ideology. Although multilingual datasets exhibit lower overall false refusal rates than English datasets, models still display systematic, language-dependent biases toward certain targets. Based on these findings, we propose a simple cross-translation strategy, translating English hate speech into Chinese for detoxification and back, which substantially reduces false refusals while preserving the original content, providing an effective and lightweight mitigation approach.

</details>


### [61] [Lessons from the Field: An Adaptable Lifecycle Approach to Applied Dialogue Summarization](https://arxiv.org/abs/2601.08682)
*Kushal Chawla,Chenyang Zhu,Pengshan Cai,Sangwoo Cho,Scott Novotney,Ayushman Singh,Jonah Lewis,Keasha Safewright,Alfy Samuel,Erin Babinsky,Shi-Xiong Zhang,Sambit Sahu*

Main category: cs.CL

TL;DR: 本文介绍了一个工业案例研究，开发基于智能体架构的多方对话摘要系统，分享了从评估方法到系统优化的全生命周期实践经验。


<details>
  <summary>Details</summary>
Motivation: 工业场景中多方对话摘要需求复杂多变，现有研究主要使用静态数据集，而实际应用中需求会不断演变，需要构建可靠、适应性强的摘要系统。

Method: 采用智能体架构开发系统，通过任务分解实现组件级优化，并探索了在需求演变和任务主观性下的鲁棒评估方法。

Result: 分享了四个关键实践洞察：1）需求演变下的鲁棒评估方法；2）智能体架构带来的组件级优化能力；3）上游数据瓶颈的影响；4）LLM提示词移植性差导致的供应商锁定现实。

Conclusion: 智能体架构为构建可靠、适应性强的多方对话摘要系统提供了有效框架，但需要关注数据瓶颈和供应商锁定等实际问题，为未来研究和实践提供了指导。

Abstract: Summarization of multi-party dialogues is a critical capability in industry, enhancing knowledge transfer and operational effectiveness across many domains. However, automatically generating high-quality summaries is challenging, as the ideal summary must satisfy a set of complex, multi-faceted requirements. While summarization has received immense attention in research, prior work has primarily utilized static datasets and benchmarks, a condition rare in practical scenarios where requirements inevitably evolve. In this work, we present an industry case study on developing an agentic system to summarize multi-party interactions. We share practical insights spanning the full development lifecycle to guide practitioners in building reliable, adaptable summarization systems, as well as to inform future research, covering: 1) robust methods for evaluation despite evolving requirements and task subjectivity, 2) component-wise optimization enabled by the task decomposition inherent in an agentic architecture, 3) the impact of upstream data bottlenecks, and 4) the realities of vendor lock-in due to the poor transferability of LLM prompts.

</details>


### [62] [QuantEval: A Benchmark for Financial Quantitative Tasks in Large Language Models](https://arxiv.org/abs/2601.08689)
*Zhaolu Kang,Junhao Gong,Wenqing Hu,Shuo Yin,Kehan Jiang,Zhicheng Fang,Yingjie He,Chunlei Meng,Rong Fu,Dongyang Chen,Leqi Zheng,Eric Hanchen Jiang,Yunfei Feng,Yitong Leng,Junfan Zhu,Xiaoyou Chen,Xi Yang,Richeng Xuan*

Main category: cs.CL

TL;DR: QuantEval是一个评估大语言模型在量化金融领域能力的基准测试，涵盖知识问答、数学推理和策略编码三个维度，并包含CTA风格回测框架来评估模型生成的交易策略。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在许多领域表现出色，但在金融量化任务上的评估仍然分散且主要局限于知识问答。需要更全面的评估框架来测试LLMs在量化金融中的实际能力。

Method: 开发了QuantEval基准测试，包含三个维度：知识问答、量化数学推理和量化策略编码。特别集成了CTA风格的回测框架，可以执行模型生成的策略并使用金融性能指标进行评估。

Result: 评估了当前最先进的开源和专有LLMs，发现与人类专家存在显著差距，特别是在推理和策略编码方面。通过领域对齐数据的监督微调和强化学习实验，展示了持续的改进。

Conclusion: QuantEval将促进LLMs在量化金融能力方面的研究，加速其在真实交易工作流程中的实际应用。作者还发布了完整的确定性回测配置以确保严格的可重复性。

Abstract: Large Language Models (LLMs) have shown strong capabilities across many domains, yet their evaluation in financial quantitative tasks remains fragmented and mostly limited to knowledge-centric question answering. We introduce QuantEval, a benchmark that evaluates LLMs across three essential dimensions of quantitative finance: knowledge-based QA, quantitative mathematical reasoning, and quantitative strategy coding. Unlike prior financial benchmarks, QuantEval integrates a CTA-style backtesting framework that executes model-generated strategies and evaluates them using financial performance metrics, enabling a more realistic assessment of quantitative coding ability. We evaluate some state-of-the-art open-source and proprietary LLMs and observe substantial gaps to human experts, particularly in reasoning and strategy coding. Finally, we conduct large-scale supervised fine-tuning and reinforcement learning experiments on domain-aligned data, demonstrating consistent improvements. We hope QuantEval will facilitate research on LLMs' quantitative finance capabilities and accelerate their practical adoption in real-world trading workflows. We additionally release the full deterministic backtesting configuration (asset universe, cost model, and metric definitions) to ensure strict reproducibility.

</details>


### [63] [Nationality and Region Prediction from Names: A Comparative Study of Neural Models and Large Language Models](https://arxiv.org/abs/2601.08692)
*Keito Inoshita*

Main category: cs.CL

TL;DR: LLMs在国籍预测任务中全面优于传统神经网络模型，尤其在细粒度预测上优势明显，其优势源于预训练获得的世界知识，但低频国籍预测仍是挑战。


<details>
  <summary>Details</summary>
Motivation: 从个人姓名预测国籍在营销、人口研究和家谱研究中具有实用价值。传统神经网络模型从任务特定数据中学习统计对应关系，难以泛化到低频国籍和区分同一地区的相似国籍。LLMs通过预训练获得的世界知识有潜力解决这些挑战。

Method: 全面比较神经网络模型和LLMs在国籍预测任务上的表现，评估6个神经网络模型和6种LLM提示策略，在三个粒度级别（国籍、地区、大洲）进行分析，包括基于频率的分层分析和错误分析。

Result: LLMs在所有粒度级别上都优于神经网络模型，随着粒度变粗，差距缩小。简单机器学习方法表现出最高的频率鲁棒性，而预训练模型和LLMs在低频国籍上表现下降。错误分析显示LLMs倾向于做出"接近命中"错误（预测正确地区但国籍错误），而神经网络模型表现出更多跨区域错误和对高频类别的偏向。

Conclusion: LLMs的优越性源于其世界知识，模型选择应考虑所需粒度级别，评估应超越准确率考虑错误质量。低频国籍预测仍是挑战，需要进一步研究。

Abstract: Predicting nationality from personal names has practical value in marketing, demographic research, and genealogical studies. Conventional neural models learn statistical correspondences between names and nationalities from task-specific training data, posing challenges in generalizing to low-frequency nationalities and distinguishing similar nationalities within the same region. Large language models (LLMs) have the potential to address these challenges by leveraging world knowledge acquired during pre-training. In this study, we comprehensively compare neural models and LLMs on nationality prediction, evaluating six neural models and six LLM prompting strategies across three granularity levels (nationality, region, and continent), with frequency-based stratified analysis and error analysis. Results show that LLMs outperform neural models at all granularity levels, with the gap narrowing as granularity becomes coarser. Simple machine learning methods exhibit the highest frequency robustness, while pre-trained models and LLMs show degradation for low-frequency nationalities. Error analysis reveals that LLMs tend to make ``near-miss'' errors, predicting the correct region even when nationality is incorrect, whereas neural models exhibit more cross-regional errors and bias toward high-frequency classes. These findings indicate that LLM superiority stems from world knowledge, model selection should consider required granularity, and evaluation should account for error quality beyond accuracy.

</details>


### [64] [RAGShaper: Eliciting Sophisticated Agentic RAG Skills via Automated Data Synthesis](https://arxiv.org/abs/2601.08699)
*Zhengwei Tao,Bo Li,Jialong Wu,Guochen Yan,Huanyao Zhang,Jiahao Xu,Haitao Mi,Wentao Zhang*

Main category: cs.CL

TL;DR: RAGShaper：一个自动化合成RAG任务和鲁棒智能体轨迹的数据生成框架，通过构建包含对抗性干扰的信息树和约束导航策略，显著提升智能体在噪声环境中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前智能体检索增强生成（RAG）的发展受到高质量训练数据稀缺的限制。传统人工标注方法无法扩展，且难以捕捉处理检索失败所需的动态推理策略，无法反映真实检索环境中的噪声和复杂性。

Method: 提出RAGShaper框架：1）InfoCurator构建密集信息树，包含感知和认知层面的对抗性干扰；2）约束导航策略强制教师智能体面对这些干扰，生成展示错误纠正和噪声拒绝的轨迹。

Result: 综合实验表明，使用合成数据训练的模型显著优于现有基线，在噪声密集和复杂检索任务中表现出更优的鲁棒性。

Conclusion: RAGShaper通过自动化数据合成有效解决了RAG智能体训练数据稀缺问题，生成的对抗性训练数据能显著提升智能体在真实噪声环境中的鲁棒性和问题解决能力。

Abstract: Agentic Retrieval-Augmented Generation (RAG) empowers large language models to autonomously plan and retrieve information for complex problem-solving. However, the development of robust agents is hindered by the scarcity of high-quality training data that reflects the noise and complexity of real-world retrieval environments. Conventional manual annotation is unscalable and often fails to capture the dynamic reasoning strategies required to handle retrieval failures. To bridge this gap, we introduce RAGShaper, a novel data synthesis framework designed to automate the construction of RAG tasks and robust agent trajectories. RAGShaper incorporates an InfoCurator to build dense information trees enriched with adversarial distractors spanning Perception and Cognition levels. Furthermore, we propose a constrained navigation strategy that forces a teacher agent to confront these distractors, thereby eliciting trajectories that explicitly demonstrate error correction and noise rejection. Comprehensive experiments confirm that models trained on our synthesized corpus significantly outperform existing baselines, exhibiting superior robustness in noise-intensive and complex retrieval tasks.

</details>


### [65] [PrivGemo: Privacy-Preserving Dual-Tower Graph Retrieval for Empowering LLM Reasoning with Memory Augmentation](https://arxiv.org/abs/2601.08739)
*Xingyu Tan,Xiaoyang Wang,Qing Liu,Xiwei Xu,Xin Yuan,Liming Zhu,Wenjie Zhang*

Main category: cs.CL

TL;DR: PrivGemo是一个保护隐私的知识图谱增强推理框架，通过双塔设计和记忆引导的暴露控制，在保持原始知识图谱本地的同时，支持对匿名化视图进行远程推理，解决了现有隐私处理方法的结构泄漏、不可控远程交互等问题。


<details>
  <summary>Details</summary>
Motivation: 许多实际知识图谱是私有的，将检索到的三元组或探索轨迹发送到闭源LLM API会带来泄漏风险。现有隐私处理方法主要关注掩码实体名称，但仍面临结构泄漏、不可控远程交互、脆弱的多跳多实体推理以及有限的经验复用等问题。

Method: PrivGemo采用双塔设计：保持原始KG知识本地化，同时支持对匿名化视图进行远程推理。通过检索连接所有主题实体的匿名化长跳路径来支持多跳多实体推理，并在本地KG上进行验证。使用分层控制器和隐私感知经验记忆来减少不必要的探索和远程交互。

Result: 在六个基准测试上的综合实验表明，PrivGemo实现了整体最先进的结果，比最强基线高出17.1%。此外，PrivGemo使较小模型（如Qwen3-4B）能够达到与GPT-4-Turbo相当的推理性能。

Conclusion: PrivGemo通过创新的隐私保护设计，有效解决了KG增强推理中的隐私泄露问题，同时保持了高性能的推理能力，使较小模型也能达到大型模型的性能水平。

Abstract: Knowledge graphs (KGs) provide structured evidence that can ground large language model (LLM) reasoning for knowledge-intensive question answering. However, many practical KGs are private, and sending retrieved triples or exploration traces to closed-source LLM APIs introduces leakage risk. Existing privacy treatments focus on masking entity names, but they still face four limitations: structural leakage under semantic masking, uncontrollable remote interaction, fragile multi-hop and multi-entity reasoning, and limited experience reuse for stability and efficiency. To address these issues, we propose PrivGemo, a privacy-preserving retrieval-augmented framework for KG-grounded reasoning with memory-guided exposure control. PrivGemo uses a dual-tower design to keep raw KG knowledge local while enabling remote reasoning over an anonymized view that goes beyond name masking to limit both semantic and structural exposure. PrivGemo supports multi-hop, multi-entity reasoning by retrieving anonymized long-hop paths that connect all topic entities, while keeping grounding and verification on the local KG. A hierarchical controller and a privacy-aware experience memory further reduce unnecessary exploration and remote interactions. Comprehensive experiments on six benchmarks show that PrivGemo achieves overall state-of-the-art results, outperforming the strongest baseline by up to 17.1%. Furthermore, PrivGemo enables smaller models (e.g., Qwen3-4B) to achieve reasoning performance comparable to that of GPT-4-Turbo.

</details>


### [66] [From Rows to Reasoning: A Retrieval-Augmented Multimodal Framework for Spreadsheet Understanding](https://arxiv.org/abs/2601.08741)
*Anmol Gulati,Sahil Sen,Waqar Sarguroh,Kevin Paul*

Main category: cs.CL

TL;DR: FRTR是一个多模态检索增强生成框架，专门解决大语言模型在企业级电子表格推理上的困难，通过细粒度嵌入和混合检索方法，在复杂多模态工作簿上实现了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在处理包含数千行数字、多个关联工作表以及嵌入图表和收据等视觉内容的企业级电子表格时存在困难。现有的电子表格推理方法通常依赖单表压缩或全上下文编码，这限制了可扩展性，且无法反映真实用户与复杂多模态工作簿的交互方式。

Method: 提出了FRTR（From Rows to Reasoning）框架，将Excel工作簿分解为细粒度的行、列和块嵌入，采用基于互惠排名融合的混合词法-稠密检索，并集成多模态嵌入来同时处理数值和视觉信息。

Result: 在FRTR-Bench基准测试中，使用Claude Sonnet 4.5达到74%的答案准确率，相比之前最佳方法的24%有显著提升。在SpreadsheetLLM基准测试中，使用GPT-5达到87%准确率，同时相比上下文压缩方法减少了约50%的token使用量。

Conclusion: FRTR框架通过细粒度分解和混合检索策略，有效解决了大语言模型在企业级多模态电子表格推理中的挑战，在准确性和效率方面都取得了显著改进，为复杂工作簿分析提供了实用解决方案。

Abstract: Large Language Models (LLMs) struggle to reason over large-scale enterprise spreadsheets containing thousands of numeric rows, multiple linked sheets, and embedded visual content such as charts and receipts. Prior state-of-the-art spreadsheet reasoning approaches typically rely on single-sheet compression or full-context encoding, which limits scalability and fails to reflect how real users interact with complex, multimodal workbooks. We introduce FRTR-Bench, the first large-scale benchmark for multimodal spreadsheet reasoning, comprising 30 enterprise-grade Excel workbooks spanning nearly four million cells and more than 50 embedded images. To address these challenges, we present From Rows to Reasoning (FRTR), an advanced, multimodal retrieval-augmented generation framework that decomposes Excel workbooks into granular row, column, and block embeddings, employs hybrid lexical-dense retrieval with Reciprocal Rank Fusion (RRF), and integrates multimodal embeddings to reason over both numerical and visual information. We tested FRTR on six LLMs, achieving 74% answer accuracy on FRTR-Bench with Claude Sonnet 4.5, a substantial improvement over prior state-of-the-art approaches that reached only 24%. On the SpreadsheetLLM benchmark, FRTR achieved 87% accuracy with GPT-5 while reducing token usage by roughly 50% compared to context-compression methods.

</details>


### [67] [Inferring Latent Intentions: Attributional Natural Language Inference in LLM Agents](https://arxiv.org/abs/2601.08742)
*Xin Quan,Jiafeng Xiong,Marco Valentino,André Freitas*

Main category: cs.CL

TL;DR: 提出Attributional NLI框架，结合社会心理学原理评估LLM在复杂交互系统中的意图推理能力，通过神经符号方法显著提升多智能体环境下的推理性能。


<details>
  <summary>Details</summary>
Motivation: 传统自然语言推理无法捕捉复杂交互系统中基于意图的细微推理，而LLM在多智能体环境中的归因推理能力尚未得到充分探索。

Method: 提出Attributional NLI框架，结合社会心理学原理，包含溯因意图推理和演绎验证两个阶段。通过Undercover-V文本游戏实验，比较三种LLM智能体：标准NLI智能体、Att-NLI智能体、以及使用外部定理证明器的神经符号Att-NLI智能体。

Result: 实验显示归因推理能力存在明显层次结构，神经符号智能体表现最佳，平均胜率达到17.08%，显著优于其他方法。

Conclusion: Att-NLI框架能有效开发具有复杂推理能力的智能体，神经符号AI在构建多智能体环境中的理性LLM智能体方面具有重要潜力。

Abstract: Attributional inference, the ability to predict latent intentions behind observed actions, is a critical yet underexplored capability for large language models (LLMs) operating in multi-agent environments. Traditional natural language inference (NLI), in fact, fails to capture the nuanced, intention-driven reasoning essential for complex interactive systems. To address this gap, we introduce Attributional NLI (Att-NLI), a framework that extends NLI with principles from social psychology to assess an agent's capacity for abductive intentional inference (generating hypotheses about latent intentions), and subsequent deductive verification (drawing valid logical conclusions). We instantiate Att-NLI via a textual game, Undercover-V, experimenting with three types of LLM agents with varying reasoning capabilities and access to external tools: a standard NLI agent using only deductive inference, an Att-NLI agent employing abductive-deductive inference, and a neuro-symbolic Att-NLI agent performing abductive-deductive inference with external theorem provers. Extensive experiments demonstrate a clear hierarchy of attributional inference capabilities, with neuro-symbolic agents consistently outperforming others, achieving an average win rate of 17.08%. Our results underscore the role that Att-NLI can play in developing agents with sophisticated reasoning capabilities, highlighting, at the same time, the potential impact of neuro-symbolic AI in building rational LLM agents acting in multi-agent environments.

</details>


### [68] [TableCache: Primary Foreign Key Guided KV Cache Precomputation for Low Latency Text-to-SQL](https://arxiv.org/abs/2601.08743)
*Jinbo Su,Yuxuan Hu,Cuiping Li,Hong Chen,Jia Li,Lintao Ma,Jing Zhang*

Main category: cs.CL

TL;DR: TableCache：通过离线预计算表表示作为KV缓存，在线查询所需表，显著减少Text-to-SQL任务中的上下文长度和推理延迟。


<details>
  <summary>Details</summary>
Motivation: 现有LLM-based方法在Text-to-SQL任务中通常包含完整数据库模式，导致上下文过长和预填充延迟增加。用户查询通常聚焦于重复的表集合，但当前推理引擎在处理不同表顺序的查询时会产生冗余的前缀缓存副本。

Method: 1) 离线预计算表表示作为KV缓存；2) 计算表缓存时保留表间主外键关系；3) 构建Table Trie结构支持高效KV缓存查找；4) 引入缓存管理系统，包括查询重排序策略提高缓存命中率和并行化模型推理与缓存加载的计算加载流水线。

Result: TableCache在实验中实现了最高3.62倍的Time to First Token (TTFT)加速，且性能下降可忽略不计。

Conclusion: 通过预计算表缓存和高效缓存管理，TableCache能显著提升Text-to-SQL任务的推理效率，减少上下文长度和延迟，同时保持模型性能。

Abstract: In Text-to-SQL tasks, existing LLM-based methods often include extensive database schemas in prompts, leading to long context lengths and increased prefilling latency. While user queries typically focus on recurrent table sets-offering an opportunity for KV cache sharing across queries-current inference engines, such as SGLang and vLLM, generate redundant prefix cache copies when processing user queries with varying table orders. To address this inefficiency, we propose precomputing table representations as KV caches offline and querying the required ones online. A key aspect of our approach is the computation of table caches while preserving primary foreign key relationships between tables. Additionally, we construct a Table Trie structure to facilitate efficient KV cache lookups during inference. To enhance cache performance, we introduce a cache management system with a query reranking strategy to improve cache hit rates and a computation loading pipeline for parallelizing model inference and cache loading. Experimental results show that our proposed TableCache achieves up to a 3.62x speedup in Time to First Token (TTFT) with negligible performance degradation.

</details>


### [69] [To Retrieve or To Think? An Agentic Approach for Context Evolution](https://arxiv.org/abs/2601.08747)
*Rubing Chen,Jian Wang,Wenjie Li,Xiao-Yong Wei,Qing Li*

Main category: cs.CL

TL;DR: ACE框架通过元认知启发的动态决策机制，在检索增强生成中智能选择何时检索外部知识或利用现有上下文进行推理，显著提升多跳QA任务性能并减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 当前检索增强生成方法存在两个主要问题：1）在每个推理步骤都进行检索的僵化策略导致不必要的计算成本；2）检索不相关内容会污染上下文，降低性能。需要更智能的上下文管理方法。

Method: 提出Agentic Context Evolution (ACE)框架：1）受人类元认知启发，动态决定何时检索新证据或利用现有知识推理；2）使用中央编排代理通过多数投票进行战略决策；3）交替激活检索代理进行外部检索和推理代理进行内部分析和精炼。

Result: 在具有挑战性的多跳QA基准测试中，ACE在准确性方面显著优于竞争基线，同时实现了高效的token消耗。通过消除冗余检索步骤，保持了简洁且进化的上下文。

Conclusion: ACE为复杂知识密集型任务的上下文进化生成提供了有价值的见解，展示了智能上下文管理在提升推理效率和效果方面的重要性。

Abstract: Current context augmentation methods, such as retrieval-augmented generation, are essential for solving knowledge-intensive reasoning tasks.However, they typically adhere to a rigid, brute-force strategy that executes retrieval at every step. This indiscriminate approach not only incurs unnecessary computational costs but also degrades performance by saturating the context with irrelevant noise. To address these limitations, we introduce Agentic Context Evolution (ACE), a framework inspired by human metacognition that dynamically determines whether to seek new evidence or reason with existing knowledge. ACE employs a central orchestrator agent to make decisions strategically via majority voting.It aims to alternate between activating a retriever agent for external retrieval and a reasoner agent for internal analysis and refinement. By eliminating redundant retrieval steps, ACE maintains a concise and evolved context. Extensive experiments on challenging multi-hop QA benchmarks demonstrate that ACE significantly outperforms competitive baselines in accuracy while achieving efficient token consumption.Our work provides valuable insights into advancing context-evolved generation for complex, knowledge-intensive tasks.

</details>


### [70] [Spatial Context Improves the Integration of Text with Remote Sensing for Mapping Environmental Variables](https://arxiv.org/abs/2601.08750)
*Valerie Zermatten,Chiara Vanalli,Gencer Sumbul,Diego Marcos,Devis Tuia*

Main category: cs.CL

TL;DR: 提出一种基于注意力的方法，结合航空影像和地理定位文本，通过空间邻域整合多个附近观测点，用于预测环境变量


<details>
  <summary>Details</summary>
Motivation: 文本作为生态学新兴数据源，包含独特信息可与地理空间数据互补，但文本数据稀疏、分布不均，与地理空间数据整合存在挑战

Method: 注意力机制方法，结合视觉和文本表示与地理位置编码，通过注意力模块动态选择对预测任务有用的空间邻居

Result: 在EcoWikiRS数据集上评估，预测103个环境变量时，始终优于单点或单模态基线；在气候、土壤、人口和土地利用/覆盖变量上表现显著提升

Conclusion: 结合文本和图像数据时包含空间上下文有显著益处，注意力机制能有效整合稀疏文本数据与地理空间信息

Abstract: Recent developments in natural language processing highlight text as an emerging data source for ecology. Textual resources carry unique information that can be used in complementarity with geospatial data sources, thus providing insights at the local scale into environmental conditions and properties hidden from more traditional data sources. Leveraging textual information in a spatial context presents several challenges. First, the contribution of textual data remains poorly defined in an ecological context, and it is unclear for which tasks it should be incorporated. Unlike ubiquitous satellite imagery or environmental covariates, the availability of textual data is sparse and irregular; its integration with geospatial data is not straightforward. In response to these challenges, this work proposes an attention-based approach that combines aerial imagery and geolocated text within a spatial neighbourhood, i.e. integrating contributions from several nearby observations. Our approach combines vision and text representations with a geolocation encoding, with an attention-based module that dynamically selects spatial neighbours that are useful for predictive tasks.The proposed approach is applied to the EcoWikiRS dataset, which combines high-resolution aerial imagery with sentences extracted from Wikipedia describing local environmental conditions across Switzerland. Our model is evaluated on the task of predicting 103 environmental variables from the SWECO25 data cube. Our approach consistently outperforms single-location or unimodal, i.e. image-only or text-only, baselines. When analysing variables by thematic groups, results show a significant improvement in performance for climatic, edaphic, population and land use/land cover variables, underscoring the benefit of including the spatial context when combining text and image data.

</details>


### [71] [Multiplex Thinking: Reasoning via Token-wise Branch-and-Merge](https://arxiv.org/abs/2601.08808)
*Yao Tang,Li Dong,Yaru Hao,Qingxiu Dong,Furu Wei,Jiatao Gu*

Main category: cs.CL

TL;DR: 提出Multiplex Thinking方法，通过采样K个候选token并聚合为单个连续multiplex token，实现软推理机制，在保持离散生成特性的同时压缩序列长度，提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 传统Chain-of-Thought方法虽然有效但会产生长序列、低带宽的token流，而人类推理通常保持对多个可能步骤的概率分布。希望开发一种更紧凑、更高效的软推理机制。

Method: 在每个推理步骤采样K个候选token，将它们的嵌入聚合为单个连续multiplex token。这种方法保留了词汇嵌入先验和离散生成采样动态，同时产生可处理的multiplex rollout概率分布。可通过on-policy强化学习直接优化multiplex轨迹。

Result: 在数学推理基准测试中，Multiplex Thinking从Pass@1到Pass@1024都一致优于强离散CoT和RL基线方法，同时产生更短的序列。

Conclusion: Multiplex Thinking是一种自适应的软推理机制，在模型置信时接近离散CoT，在不确定时紧凑表示多个可能步骤而不增加序列长度，显著提升推理效率。

Abstract: Large language models often solve complex reasoning tasks more effectively with Chain-of-Thought (CoT), but at the cost of long, low-bandwidth token sequences. Humans, by contrast, often reason softly by maintaining a distribution over plausible next steps. Motivated by this, we propose Multiplex Thinking, a stochastic soft reasoning mechanism that, at each thinking step, samples K candidate tokens and aggregates their embeddings into a single continuous multiplex token. This preserves the vocabulary embedding prior and the sampling dynamics of standard discrete generation, while inducing a tractable probability distribution over multiplex rollouts. Consequently, multiplex trajectories can be directly optimized with on-policy reinforcement learning (RL). Importantly, Multiplex Thinking is self-adaptive: when the model is confident, the multiplex token is nearly discrete and behaves like standard CoT; when it is uncertain, it compactly represents multiple plausible next steps without increasing sequence length. Across challenging math reasoning benchmarks, Multiplex Thinking consistently outperforms strong discrete CoT and RL baselines from Pass@1 through Pass@1024, while producing shorter sequences. The code and checkpoints are available at https://github.com/GMLR-Penn/Multiplex-Thinking.

</details>


### [72] [Modeling LLM Agent Reviewer Dynamics in Elo-Ranked Review System](https://arxiv.org/abs/2601.08829)
*Hsiang-Wei Huang,Junbin Lu,Kuang-Ming Chen,Jenq-Neng Hwang*

Main category: cs.CL

TL;DR: 研究探索了在Elo排名评审系统中使用LLM智能体进行论文评审的动力学，通过真实会议论文提交进行模拟，发现Elo评分能提高领域主席决策准确性，但评审者会适应性地利用系统而不增加评审努力。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型（LLM）智能体在基于Elo排名的评审系统中的评审行为动态，了解如何通过模拟真实会议评审过程来改进学术评审系统的效率和准确性。

Method: 使用真实会议论文提交，构建多个具有不同角色的LLM智能体评审者，在领域主席主持下进行多轮评审互动。比较基线设置与包含Elo评分和评审者记忆的条件设置。

Result: 模拟结果显示：1）引入Elo评分能提高领域主席决策准确性；2）评审者会适应性地利用Elo系统，但不会增加评审努力；3）发现了评审策略的有趣动态变化。

Conclusion: Elo评分系统能有效提升评审决策质量，但需要设计机制防止评审者策略性利用系统而不提升评审质量，为改进学术评审系统提供了重要洞见。

Abstract: In this work, we explore the Large Language Model (LLM) agent reviewer dynamics in an Elo-ranked review system using real-world conference paper submissions. Multiple LLM agent reviewers with different personas are engage in multi round review interactions moderated by an Area Chair. We compare a baseline setting with conditions that incorporate Elo ratings and reviewer memory. Our simulation results showcase several interesting findings, including how incorporating Elo improves Area Chair decision accuracy, as well as reviewers' adaptive review strategy that exploits our Elo system without improving review effort. Our code is available at https://github.com/hsiangwei0903/EloReview.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [73] [Human as an Actuator Dynamic Model Identification](https://arxiv.org/abs/2601.07956)
*Harrison M. Bonner,Matthew R. Kirchner*

Main category: eess.SY

TL;DR: 提出一种在时域中通过约束优化估计人类飞行员响应模型参数的方法，应用于四旋翼无人机位置控制任务


<details>
  <summary>Details</summary>
Motivation: 人类飞行员模型对于有人驾驶飞行器的动态分析至关重要，但需要一种能够处理人类响应自然变异性的鲁棒估计方法

Method: 在时域中构建约束优化问题，通过模拟器生成多组试验数据，寻找最能描述数据的单一模型，优化问题隐式表示底层系统状态

Result: 成功估计了四旋翼无人机位置控制任务的人类响应模型，方法对自然响应变异具有鲁棒性

Conclusion: 该方法能够有效估计人类飞行员响应模型参数，为有人驾驶飞行器的动态分析提供了可靠工具

Abstract: This paper presents a method for estimating parameters that form a general model for human pilot response for specific tasks. The human model is essential for the dynamic analysis of piloted vehicles. Data are generated on a simulator with multiple trials being incorporated to find the single model that best describes the data. The model is found entirely in the time domain by constructing a constrained optimization problem. This optimization problem implicitly represents the state of the underlying system, making it robust to natural variation in human responses. It is demonstrated by estimating the human response model for a position control task with a quadcopter drone.

</details>


### [74] [Can Inherent Communication Noise Guarantee Privacy in Distributed Cooperative Control ?](https://arxiv.org/abs/2601.07997)
*Yuwen Ma,Sarah K. Spurgeon,Tao Li,Boli Chen*

Main category: eess.SY

TL;DR: 该论文提出了一种基于差分隐私的多智能体系统隐私保护分布式协同控制框架，将通信噪声重新视为隐私增强因素而非干扰，无需额外添加隐私噪声即可实现有界差分隐私保护。


<details>
  <summary>Details</summary>
Motivation: 传统协同控制中，通信噪声通常被视为损害协调的干扰。本研究重新审视这种噪声，探索其作为隐私增强因素的潜力，旨在保护参考信号免受推理攻击，同时保持系统协同性能。

Method: 提出基于线性二次调节器(LQR)的框架，智能体通过噪声信道通信，噪声方差取决于相邻智能体间的相对状态差异。控制器在实现编队的同时保护参考信号，无需添加专门的隐私噪声。

Result: 理论证明固有通信噪声可保证有界(ε,δ)-差分隐私，同时系统协同跟踪误差在均方和几乎必然意义上保持有界且收敛，实现了隐私保护与协同控制的平衡。

Conclusion: 通信噪声可被重新利用为隐私保护机制，提出的LQR框架成功实现了多智能体系统的隐私保护协同控制，为噪声信道下的分布式控制提供了新的隐私增强视角。

Abstract: This paper investigates privacy-preserving distributed cooperative control for multi-agent systems within the framework of differential privacy. In cooperative control, communication noise is inevitable and is usually regarded as a disturbance that impairs coordination. This work revisits such noise as a potential privacy-enhancing factor. A linear quadratic regulator (LQR)-based framework is proposed for agents communicating over noisy channels, \textcolor{black}{where the noise variance depends on the relative state differences between neighbouring agents.} The resulting controller achieves formation while protecting the reference signals from inference attacks. It is analytically proven that the inherent communication noise can guarantee bounded $(ε,δ)$-differential privacy without adding dedicated privacy noise, while the \textcolor{black}{system cooperative tracking error} remains bounded and convergent in both the mean-square and almost-sure sense.

</details>


### [75] [Formalizing the Relationship between Hamilton-Jacobi Reachability and Reinforcement Learning](https://arxiv.org/abs/2601.08050)
*Prashant Solanki,Isabelle El-Hajj,Jasper van Beers,Erik-Jan van Kampen,Coen de Visser*

Main category: eess.SY

TL;DR: 将Hamilton-Jacobi可达性分析与强化学习统一，提出运行成本公式，证明旅行成本值函数是HJB PDE的唯一有界粘性解，其负子水平集等于严格后向可达管


<details>
  <summary>Details</summary>
Motivation: 统一Hamilton-Jacobi可达性分析与强化学习，建立两者之间的理论联系，使强化学习方法能够解决可达性分析问题

Method: 提出运行成本公式，证明旅行成本值函数是时间相关HJB PDE的唯一有界粘性解，使用前向重新参数化和收缩诱导Bellman更新，展示小步长RL值迭代的固定点收敛到前向折扣HJB的粘性解

Result: 在经典基准测试中，将学习到的值与半拉格朗日HJB地面真值进行比较，并量化误差，验证了理论结果

Conclusion: 成功统一了HJ可达性和RL，建立了理论框架，使RL方法能够有效解决可达性分析问题，为两者之间的交叉应用提供了理论基础

Abstract: We unify Hamilton-Jacobi (HJ) reachability and Reinforcement Learning (RL) through a proposed running cost formulation. We prove that the resultant travel-cost value function is the unique bounded viscosity solution of a time-dependent Hamilton-Jacobi Bellman (HJB) Partial Differential Equation (PDE) with zero terminal data, whose negative sublevel set equals the strict backward-reachable tube. Using a forward reparameterization and a contraction inducing Bellman update, we show that fixed points of small-step RL value iteration converge to the viscosity solution of the forward discounted HJB. Experiments on a classical benchmark compare learned values to semi-Lagrangian HJB ground truth and quantify error.

</details>


### [76] [DRL-based Power Allocation in LiDAL-Assisted RLNC-NOMA OWC Systems](https://arxiv.org/abs/2601.08060)
*Ahmed A. Hassan,Ahmad Adnan Qidan,Taisir Elgorashi,Jaafar Elmirghani*

Main category: eess.SY

TL;DR: 提出一种结合LiDAL和RLNC的NOMA光无线通信系统，采用DRL-based NAF算法进行功率分配优化，以提高系统平均和速率。


<details>
  <summary>Details</summary>
Motivation: 在室内密集用户场景下，NOMA光无线通信系统面临信道状态信息不完美和解码残留误差等问题，传统的功率分配方法难以处理复杂的多用户交互和动态优化需求。

Method: 建立LiDAL-assisted RLNC-NOMA OWC系统模型，利用LiDAL技术改善用户CSI，RLNC增强NOMA连续解码的数据韧性，并采用基于DRL的NAF算法进行动态功率分配优化。

Result: 提出的DRL-based NAF算法能够有效最大化系统平均和速率，性能优于DDPG、GRPA和穷举搜索等其他功率分配方案。

Conclusion: 结合LiDAL、RLNC和DRL-based NAF的集成方法能够显著提升NOMA光无线通信系统在密集用户场景下的性能，为复杂动态功率分配问题提供了有效解决方案。

Abstract: Non-orthogonal multiple access (NOMA) is a promising technique for optical wireless communication (OWC), enabling multiple users to share the optical spectrum simultaneously through the power domain. However, the imperfection of channel state information (CSI) and residual errors in decoding process deteriorate the performance of NOMA, especially when multi-parameteric and realistic dense-user indoor scenarios are considered. In this work, we model a LiDAL-assisted RLNC-NOMA OWC system, where the light detection and localization (LiDAL) technique exploits spatio-temporal information to improve user CSI, while random linear network coding (RLNC) enhances data resilience in the NOMA successive decoding process. Power allocation (PA) is a crucial issue in communication systems, particularly in the modeled system, due to the complex interactions between multiple users and the coding and detection processes. However, optimizing continuous PA dynamically requires advanced techniques to avoid excessive computational complexity. Therefore, we adopt a deep reinforcement learning (DRL) framework to efficiently learn near-optimal power allocation strategies, enabling enhanced system performance. In particular, a DRL-based normalized advantage function (NAF) algorithm is proposed to maximize the average sum rate of the system, and its performance is analyzed and compared to other widely used DRL-based and conventional PA schemes, such as deep deterministic policy gradient (DDPG), gain ratio PA (GRPA), and exhaustive search.

</details>


### [77] [Coordinated Cooling and Compute Management for AI Datacenters](https://arxiv.org/abs/2601.08113)
*Nardos Belay Abera,Yize Chen*

Main category: eess.SY

TL;DR: 本文提出了一种联合优化AI数据中心计算与冷却的层次控制框架，通过协同优化GPU并行度、频率和冷却控制，在满足服务延迟和热约束的同时显著提升能效。


<details>
  <summary>Details</summary>
Motivation: AI数据中心大规模部署支持大语言模型训练和推理，但大量计算和冷却需求引发能耗和碳排放担忧。现有研究主要优化计算侧调度而忽略热目标或约束，GPU密集型推理产生大量热量可能降低数据中心性能，忽视热效应会增加总能耗并降低LLM服务效率。

Method: 1. 在不同冷却条件和AI任务下分析GPU服务器特性；2. 开发AI数据中心联合冷却与计算建模方法；3. 提出新颖的层次控制框架，通过识别最优GPU并行度、频率(DVFS)和冷却控制旋钮来协同优化计算与热管理；4. 使用真实Azure推理跟踪和详细GPU分析。

Result: 该模型在平衡AI数据中心服务延迟和热约束的同时，显著提高了AI数据中心的能源效率。

Conclusion: 通过联合优化计算与冷却管理，可以有效解决AI数据中心能效问题，在满足性能要求的同时降低能耗，为大规模AI部署提供可持续的解决方案。

Abstract: The AI datacenters are currently being deployed on a large scale to support the training and deployment of power-intensive large-language models (LLMs). Extensive amount of computation and cooling required in datacenters increase concerns about the energy use and carbon emissions of AI datacenters. Although current state-of-the-art has examined the energy efficiency of LLM inference, most prior research focused on optimizing compute-side scheduling without considering thermal objectives or constraints. Since GPU-intensive inference generates substantial heat that can degrade datacenter performance, ignoring thermal effects can increase total energy consumption and reduce the efficiency of LLM serving. To fill this gap, we profile the characteristics of GPU servers under varying cooling and AI jobs, and develop a joint cooling and computing modeling approach for AI datacenters. Built upon such workload and thermal dynamics models, a novel hierarchical control framework is proposed to co-optimize computing and thermal management by identifying the optimal GPU parallelism, frequency (DVFS), and cooling control knobs. Using real Azure inference traces and detailed GPU profiling, our model balances serving latency and thermal constraints in AI datacenters while significantly improving AI datacenters' energy efficiency.

</details>


### [78] [Memetic Covariance Matrix Adaptation Evolution Strategy for Bilinear Matrix Inequality Problems in Control System Design](https://arxiv.org/abs/2601.08168)
*Syue-Cian Lin,Wei-Yu Chiu,Chien-Feng Wu*

Main category: eess.SY

TL;DR: 提出一种结合全局搜索和局部优化的memetic CMA-ES算法，用于求解控制设计中的双线性矩阵不等式问题，在H∞控制器综合和谱横坐标优化中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 双线性矩阵不等式在控制系统设计中至关重要，但由于其非凸性难以求解。现有方法在处理这类问题时存在局限性，需要更有效的求解策略。

Method: 提出memetic CMA-ES算法，将全局搜索的协方差矩阵自适应进化策略与局部精炼的(1+1)-CMA-ES相结合，专门针对控制设计任务进行定制化集成和参数调优。

Result: 在H∞控制器综合和谱横坐标优化实验中，该方法在解的质量和鲁棒性方面均优于现有的BMI求解器。

Conclusion: 该研究填补了进化计算与控制理论之间的空白，为处理具有挑战性的BMI约束问题提供了一种实用有效的解决方案。

Abstract: Bilinear Matrix Inequalities (BMIs) are fundamental to control system design but are notoriously difficult to solve due to their nonconvexity. This study addresses BMI-based control optimization problems by adapting and integrating advanced evolutionary strategies. Specifically, a memetic Covariance Matrix Adaptation Evolution Strategy (memetic CMA-ES) is proposed, which incorporates a local refinement phase via a (1+1)-CMA-ES within the global search process. While these algorithmic components are established in evolutionary computing, their tailored integration and specific tuning for control design tasks represent a novel application in this context. Experimental evaluations on $H_{\infty}$ controller synthesis and spectral abscissa optimization demonstrate that the proposed method achieves superior performance compared to existing BMI solvers in terms of both solution quality and robustness. This work bridges the gap between evolutionary computation and control theory, providing a practical and effective approach to tackling challenging BMI-constrained problems.

</details>


### [79] [Research on Mechanical Properties and Deformation-Fracture Energy Consumption Characteristics of Plateau Frozen Rocks](https://arxiv.org/abs/2601.08177)
*Hongbing Yu,Jiyu Wang,Xiaojun Zhang,Mingsheng Zhao*

Main category: eess.SY

TL;DR: 研究高原冻融砂岩在温度与含水率耦合作用下的动态力学特性、变形破坏行为和能耗特征，为高原地区冻岩爆破参数优化提供理论支持


<details>
  <summary>Details</summary>
Motivation: 高原地区矿产资源开发面临冻岩爆破效率低、能耗高、安全性差等问题，需要系统研究冻融砂岩的力学特性以优化爆破工艺

Method: 采用现场取样、低温低压模拟试验、分级冲击加载试验和PFC3D数值反演分析相结合的综合研究方法

Result: 冻融显著提高饱和砂岩的动态强度和脆性；孔隙结构随温度降低发生显著演化，孔隙率增加63.15%；冻岩能量吸收增量高于常温样本

Conclusion: 研究成果为高原地区冻岩爆破参数优化、定向能量释放和绿色施工提供了理论基础和技术支持

Abstract: The exploitation of mineral resources in plateau regions is confronted with critical challenges including low blasting efficiency, excessive energy consumption,and compromised operational safety when dealing with low-temperature water-bearing frozen rock masses.This study systematically investigates the dynamic-static mechanical properties,deformation-fracture behaviors,and energy consumption characteristics of plateau frozen sandstone under the coupled effects of temperature and moisture content (5%-15%).The research methodology integrates field sampling, low-pressure low-temperature simulation tests, graded impact loading tests, and numerical inversion analysis. Results demonstrate that freezing significantly enhances the dynamic strength and brittleness of saturated sandstone.The pore structure undergoes substantial evolution with decreasing temperature, with the porosity increasing by 63.15%.Based on PFC3D microscopic simulations, the mechanism of frost heave damage and the regulatory effect of water-ice phase transition on rock mechanical behaviors are elucidated.A quantitative analysis method for energy dissipation is proposed, revealing that the energy absorption increment of frozen rocks is higher than that of room-temperature samples.The findings provide a theoretical basis and technical support for optimizing blasting parameters, realizing directional energy release,and promoting green construction of frozen rock masses in high-altitude areas.

</details>


### [80] [Hybrid Centralized Distributed Control for Lifelong MAPF over Wireless Connections](https://arxiv.org/abs/2601.08214)
*Jinghao Cao,Wanchun Liu,Yonghui Li,Branka Vucetic*

Main category: eess.SY

TL;DR: 提出混合集中-分布式方案解决终身多智能体路径规划中的通信与控制联合问题，中央云策略仅在需要时发送小残差修正，本地GRU策略提供通信不可用时的安全回退


<details>
  <summary>Details</summary>
Motivation: 在终身多智能体路径规划中，不可靠的无线链路和随机执行是常态。现有方法要么依赖理想化通信下的集中规划，要么运行固定通信模式的完全分布式本地控制器，很少将通信调度与策略学习结合，因此在带宽稀缺或丢包频繁时表现不佳

Method: 提出混合集中-分布式方案：中央云策略仅在需要时发送小残差修正，而轻量级的板载门控循环单元（GRU）策略在无线连接不可用时提供安全的默认回退机制

Result: 该方法能够有效应对带宽稀缺和频繁丢包的情况，通过结合中央云策略的智能修正和本地GRU策略的可靠回退，提高了系统在不可靠通信环境下的鲁棒性

Conclusion: 通过将通信调度与策略学习相结合，提出的混合方案解决了终身多智能体路径规划中的联合控制-通信问题，在不可靠通信环境下提供了更可靠和高效的解决方案

Abstract: In lifelong multi-agent path finding (MAPF) with many robots, unreliable wireless links and stochastic executions are the norm. Existing approaches typically either rely on centralized planning under idealized communication, or run fully distributed local controllers with fixed communication patterns; they rarely couple communication scheduling with policy learning, and thus struggle when bandwidth is scarce or packets are frequently dropped. We address this joint control--communication problem and propose a hybrid centralized--distributed scheme: a centralized cloud policy sends small residual corrections only when selected, while a lightweight on-board Gated recurrent unit (GRU) policy provides a safe default fallback when wireless connection is not available.

</details>


### [81] [On Robust Fixed-Time Stabilization of the Cauchy Problem in Hilbert Spaces](https://arxiv.org/abs/2601.08335)
*Moussa Labbadi,Christophe Roman,Yacine Chitour*

Main category: eess.SY

TL;DR: 该论文提出了非齐次抽象演化问题的有限时间和固定时间稳定性结果，扩展了现有理论，并应用于具有记忆项的热方程。


<details>
  <summary>Details</summary>
Motivation: 将有限维系统的有限时间和固定时间稳定性理论推广到无限维系统，解决非齐次抽象演化问题的稳定性分析，并考虑在域的子集上进行部分状态稳定的实际应用场景。

Method: 证明了强解和弱解的适定性，估计了齐次和非齐次系统的稳定时间上界，将有限维结果推广到无限维系统，并展示了在域的子集上进行部分状态稳定的方法。

Result: 建立了非齐次抽象演化问题的有限时间和固定时间稳定性理论，获得了稳定时间的上界估计，并通过具有记忆项的热方程应用验证了理论的有效性。

Conclusion: 成功扩展了有限时间和固定时间稳定性理论到无限维非齐次系统，为偏微分方程控制系统提供了新的稳定性分析工具，具有重要的理论和应用价值。

Abstract: This paper presents finite-time and fixed-time stabilization results for inhomogeneous abstract evolution problems, extending existing theories. We prove well-posedness for strong and weak solutions, and estimate upper bounds for settling times for both homogeneous and inhomogeneous systems. We generalize finite-dimensional results to infinite-dimensional systems and demonstrate partial state stabilization with actuation on a subset of the domain. The interest of these results are illustrated through an application of a heat equation with memory term.

</details>


### [82] [Minimal Actuator Selection](https://arxiv.org/abs/2601.08338)
*Luca Ballotta,Geethu Joseph*

Main category: eess.SY

TL;DR: 该论文将最小执行器选择问题转化为整数线性规划和集合多重覆盖问题，为控制系统可控性提供了新的算法框架。


<details>
  <summary>Details</summary>
Motivation: 现有研究要么关注最优性能而简化可控性问题，要么在结构假设下（如图论或输入矩阵作为设计参数）确保系统可控。本文旨在推广这些方法，为一般最小执行器选择问题提供精确表征。

Method: 将最小执行器选择问题等价转化为整数线性规划问题，并在执行通道足够独立时进一步转化为带多重约束的集合多重覆盖问题。当状态矩阵具有不同特征值时，简化为集合覆盖问题。

Result: 建立了最小执行器选择问题与集合多重覆盖问题的等价关系，使得可以利用该领域的丰富算法（包括无需暴力搜索的精确解）来选择最小执行器子集。

Conclusion: 该连接关系为控制系统设计者提供了使用集合覆盖问题算法的合法性，能够有效解决最小执行器选择问题，即使在需要容忍一定数量执行器故障的鲁棒选择场景下也成立。

Abstract: Selecting a few available actuators to ensure the controllability of a linear system is a fundamental problem in control theory. Previous works either focus on optimal performance, simplifying the controllability issue, or make the system controllable under structural assumptions, such as in graphs or when the input matrix is a design parameter. We generalize these approaches to offer a precise characterization of the general minimal actuator selection problem where a set of actuators is given, described by a fixed input matrix, and goal is to choose the fewest actuators that make the system controllable. We show that this problem can be equivalently cast as an integer linear program and, if actuation channels are sufficiently independent, as a set multicover problem under multiplicity constraints. The latter equivalence is always true if the state matrix has all distinct eigenvalues, in which case it simplifies to the set cover problem. Such characterizations hold even when a robust selection that tolerates a given number of faulty actuators is desired. Our established connection legitimates a designer to use algorithms from the rich literature on the set multicover problem to select the smallest subset of actuators, including exact solutions that do not require brute-force search.

</details>


### [83] [Blockchain-Enabled Renewable Energy Certificate Trading: A Secure and Privacy-Preserving Approach](https://arxiv.org/abs/2601.08339)
*Wei-Jen Liu,Wei-Yu Chiu,Weiqi Hua*

Main category: eess.SY

TL;DR: 该研究提出使用有向无环图（DAG）区块链系统改进可再生能源证书（RECs）交易，通过新交易模式保护消费者隐私，相比权益证明机制降低41%交易时间和65%能耗。


<details>
  <summary>Details</summary>
Motivation: 现有可再生能源证书系统面临两大挑战：1) 由于设计不一致导致RECs未在全球范围内采用；2) 区块链设计中未充分考虑消费者隐私保护。需要改进RECs交易系统以促进可再生能源转型。

Method: 使用有向无环图（DAG）区块链系统研究RECs在供应商和消费者之间的交易，引入新的交易模式来保护消费者信息。

Result: 与权益证明机制相比，新系统实现了41%的交易时间降低和65%的能耗减少，同时更好地保护了消费者隐私。

Conclusion: DAG区块链系统为RECs交易提供了更高效、更节能的解决方案，有助于推动可再生能源证书的全球采用，同时保护消费者隐私，促进可再生能源转型。

Abstract: In the 21st century, transitioning to renewable energy sources is imperative, with fossil fuel reserves depleting rapidly and recognizing critical environmental issues such as climate change, air pollution, water pollution, and habitat destruction. Embracing renewable energy is not only an environmental necessity but also a strategic move with multiple benefits. By shifting to renewable energy sources and supporting their production through the acquisition of renewable energy certificates, we foster innovation and drive economic growth in the renewable energy sector. This, in turn, reduces greenhouse gas emissions, aligning with global efforts to mitigate climate change. Additionally, renewable energy certificates ensure compliance with regulations that mandate the use of renewable energy, enhancing legal adherence while promoting transparency and trust in energy sourcing. To monitor the uptake of renewable energy, governments have implemented Renewable Energy Certificates (RECs) as a tracking mechanism for the production and consumption of renewable energy. However, there are two main challenges to the existing REC schema: 1) The RECs have not been globally adopted due to inconsistent design; 2) The consumer privacy has not been well incorporated in the design of blockchain. In this study, we investigate the trading of RECs between suppliers and consumers using the directed acyclic graph (DAG) blockchain system and introduce a trading schema to help protect consumer information. Our results demonstrate lower transaction time by 41\% and energy consumption by 65\% compared to proof-of-stake.

</details>


### [84] [Data-Driven Time-Limited h2 Optimal Model Reduction for Linear Discrete-Time Systems](https://arxiv.org/abs/2601.08372)
*Hiroki Sakamoto,Kazuhiro Sato*

Main category: eess.SY

TL;DR: 提出一种基于数据驱动的H2模型降阶方法，仅使用脉冲响应数据解决有限时域下的离散线性时不变系统模型降阶问题


<details>
  <summary>Details</summary>
Motivation: 传统模型降阶方法通常需要系统模型知识，而本文旨在开发仅使用数据（脉冲响应）就能进行H2模型降阶的方法，适用于有限时域场景

Method: 数据驱动的H2模型降阶算法，直接利用系统的脉冲响应数据，在有限时域内求解最优降阶模型问题

Result: 算法在特定假设下收敛到稳定点，数值实验使用SLICOT基准（CD播放器模型）验证了该方法能构建出在有限时域H2范数意义下的良好降阶模型

Conclusion: 成功开发了一种仅依赖数据驱动的H2模型降阶方法，无需系统模型知识，在有限时域场景下有效，并通过基准测试验证了性能

Abstract: This paper develops a data-driven h2 model reduction method for discrete-time linear time-invariant systems. Specifically, we solve the h2 model reduction problem defined over a finite horizon using only impulse response data. Furthermore, we show that the proposed data-driven algorithm converges to a stationary point under certain assumptions. Numerical experiments demonstrate that the proposed method constructs a good reduced-order model in terms of the h2 norm defined over the finite horizon using a SLICOT benchmark (the CD player model).

</details>


### [85] [Multiobjective Model Predictive Control for Residential Demand Response Management Under Uncertainty](https://arxiv.org/abs/2601.08445)
*Guan-Ting Lin,Wei-Yu Chiu,Chien-Feng Wu,Asef Nazari,Dhananjay Thiruvady*

Main category: eess.SY

TL;DR: 提出基于多目标模型预测控制的家用能源管理系统，使用Laguerre函数参数化控制信号，开发约束多目标进化算法，在实时电价下平衡用电成本与用户不适感，不确定性下成本仅增加0.52%。


<details>
  <summary>Details</summary>
Motivation: 需求响应项目中，住宅用户需要在实时电价下平衡用电成本与用户不适感，同时应对不确定性挑战，需要有效的家庭能源管理解决方案。

Method: 采用多目标模型预测控制框架，使用Laguerre函数参数化控制信号将优化问题转化为线性不等式约束问题，开发约束多目标进化算法，结合凸采样器交叉变异确保可行解。

Result: 仿真结果表明，该方法优于现有方法，在不确定性条件下成本增加仅0.52%，而其他方法至少增加2.3%。

Conclusion: 提出的多目标模型预测控制方法能有效平衡用电成本与用户不适感，在不确定性条件下表现优异，为家庭能源管理提供有效解决方案。

Abstract: Residential users in demand response programs must balance electricity costs and user dissatisfaction under real-time pricing. This study proposes a multiobjective model predictive control approach for home energy management systems with battery storage, aiming to minimize both objectives while mitigating uncertainties. Laguerre functions parameterize control signals, transforming the optimization problem into one with linear inequalities for efficient exploration. A constrained multiobjective evolutionary algorithm, incorporating convex sampler-based crossover and mutation, is developed to ensure feasible solutions. Simulations show that the proposed method outperforms existing approaches, limiting cost increases to 0.52\% under uncertainties, compared to at least 2.3\% with other methods.

</details>


### [86] [Current and temperature imbalances in parallel-connected grid storage battery modules](https://arxiv.org/abs/2601.08459)
*Joseph Ross,Damien Frost,Stratos Chatzinikolaou,Stephen Duncan,David Howey*

Main category: eess.SY

TL;DR: 量化电网储能电池模块中并联电芯的电流和温度不平衡问题，分析其演变规律及影响因素，建立安全阈值和鲁棒性指标


<details>
  <summary>Details</summary>
Motivation: 大型电池系统中并联电芯存在电流和温度异质性问题，极端电流和温度会损害电池性能和寿命，但目前对电网储能模块中典型不平衡程度的共识不足

Method: 通过模拟和实验研究工业代表性电网储能电池模块（使用磷酸铁锂棱柱电芯），分析电流和温度不平衡的演变规律及其对电芯和模块参数变化的依赖性，进行敏感性分析

Result: 接触电阻和电芯电阻的变化对电芯间温度差异影响显著，据此定义了电芯间变异性的安全阈值；不同应用场景下阈值会变化，较低倍率充放电和较窄SOC范围可缓解故障

Conclusion: 建立了电池模块鲁棒性指标，表明通过控制充放电倍率和SOC范围可以减轻并联电芯的不平衡问题，为电网储能系统设计提供安全指导

Abstract: A key challenge with large battery systems is heterogeneous currents and temperatures in modules with parallel-connected cells. Although extreme currents and temperatures are detrimental to the performance and lifetime of battery cells, there is not a consensus on the scale of typical imbalances within grid storage modules. Here, we quantify these imbalances through simulations and experiments on an industrially representative grid storage battery module consisting of prismatic lithium iron phosphate cells, elucidating the evolution of current and temperature imbalances and their dependence on individual cell and module parameter variations. Using a sensitivity analysis, we find that varying contact resistances and cell resistances contribute strongly to temperature differences between cells, from which we define safety thresholds on cell-to-cell variability. Finally, we investigate how these thresholds change for different applications, to outline a set of robustness metrics that show how cycling at lower C-rates and narrower SOC ranges can mitigate failures.

</details>


### [87] [Disturbance observer-based tracking control for roll-to-roll slot die coating systems under gap and pump rate disturbances](https://arxiv.org/abs/2601.08488)
*Zezhi Tang,Christopher Passmore,Andrew I Campbell,Jonathan Howse,J Anthony Rossiter,Stephen Ebbens,George Panoutsos*

Main category: eess.SY

TL;DR: 提出一种基于扰动观测器的控制方法，用于提升卷对卷狭缝涂布工艺的厚度控制精度，通过数据驱动建模和广义补偿器抑制扰动影响。


<details>
  <summary>Details</summary>
Motivation: 卷对卷狭缝涂布工艺在锂离子电池、太阳能电池等制造中广泛应用，但工艺参数对扰动敏感，导致薄膜厚度不一致。需要一种能有效抑制扰动、保持输出厚度稳定的控制方法。

Method: 采用扰动观测器检测系统扰动，结合广义补偿器抑制扰动影响，并与输出跟踪控制器集成。通过实验平台和相机系统建立数据驱动的状态空间模型，验证DOBOTC系统的有效性。

Result: 仿真结果表明DOBOTC系统能有效解决数据驱动模型中存在广义扰动时的输出跟踪问题，为涂层系统在不同输入条件和扰动下保持期望厚度提供了鲁棒解决方案。

Conclusion: 提出的DOBOTC方法能显著提升卷对卷狭缝涂布工艺的厚度控制精度和鲁棒性，为工业制造中的扰动抑制和输出跟踪问题提供了有效解决方案。

Abstract: Roll-to-roll slot die coating is a widely used industrial manufacturing technique applied in a diverse range of applications such as the production of lithium-ion batteries, solar cells and optical films. The efficiency of roll-to-roll slot die coating depends on the precise control of various input parameters such as pump rate, substrate velocity and coating gap. However, these inputs are sensitive to disturbances in process conditions, leading to inconsistencies in the various characteristics of the produced film. To address this challenge, a \gls{DO} is utilized for detecting disturbances, which may occur in the same or different channels as the control signal within the system. A generalized compensator is then implemented to mitigate the impact of these disturbances on the output, thereby enhancing uncertainty suppression. Additionally, integrating the disturbance rejection system with an output tracking controller enables the coating system to maintain the desired thickness under varying input conditions and disturbances. The effectiveness of this approach is then validated using a test rig equipped with a camera system, which facilitates the development of a data-driven model of the dynamic process, represented by state-space equations. The simulation results were demonstrated to showcase the effectiveness of the DOBOTC system, which provides a resilient solution for the output tracking issue in a data-driven model with generalized disturbances.

</details>


### [88] [Improving the GMAW process through current control](https://arxiv.org/abs/2601.08518)
*Alexandre Sanfelici Bazanella,Mateus Gaspary de Freitas*

Main category: eess.SY

TL;DR: 提出一种基于数学模型的GMAW焊接电流闭环控制策略，通过数据驱动获取参数，在微控制器中实现，实验验证了焊接质量提升


<details>
  <summary>Details</summary>
Motivation: 改进GMAW（气体保护金属极电弧焊）过程的电流控制，通过更精确的闭环控制提升焊接质量和稳定性

Method: 1. 建立GMAW过程的开关等效电路数学模型；2. 采用数据驱动方法获取模型参数；3. 基于数学模型设计形式化闭环控制策略；4. 在微控制器上用C+语言实现控制算法

Result: 在手动焊接和机器人焊接的大量实验中验证了控制策略的有效性，显示焊接过程整体性能得到改善

Conclusion: 提出的基于数学模型的闭环电流控制策略能够有效提升GMAW焊接过程的质量和稳定性，为焊接自动化提供了可靠的控制方案

Abstract: A control strategy for the electrical current in GMAW processes is proposed. The control is in closed-loop, designed by formal methods, based on a mathematical model of the electrical behavior of the GMAW process, and implemented in C+ language in a microcontroller. The model consists of a switched equivalent electrical circuit whose parameters are obtained in a data-driven manner. The strategy is tested in numerous experiments with both manual and robot welding, showing improvements in the overall welding process.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [89] [Utility-Weighted Forecasting and Calibration for Risk-Adjusted Decisions under Trading Frictions](https://arxiv.org/abs/2601.07852)
*Craig S Wright*

Main category: econ.EM

TL;DR: 该论文提出将预测视为约束决策问题的计量经济学输入，通过效用加权校准来最小化决策损失而非预测误差，在实证中实现了30%以上的决策损失减少。


<details>
  <summary>Details</summary>
Motivation: 金融预测任务通常只优化预测准确性，但实际投资决策面临交易成本、市场冲击、容量限制和风险约束。传统预测误差最小化与最终决策目标存在脱节。

Method: 将预测分布作为约束决策问题的输入，通过效用目标函数和包含成本函数与可行集约束的摩擦算子推导决策规则。提出效用加权校准准则，建立校准预测分布优于未校准分布的条件。

Result: 实证研究表明，效用加权校准使实际决策损失减少30%以上（t统计量-30.31），在下跌行情中夏普比率从-3.62改善至-2.29。约束绑定频率从16.0%降至5.1%，减少了"角点解"失败。

Conclusion: 预测应作为约束决策问题的计量输入进行优化，效用加权校准能显著改善高摩擦环境下的决策性能，通过减少约束绑定频率避免过度自信预测的失败。

Abstract: Forecasting accuracy is routinely optimised in financial prediction tasks even though investment and risk-management decisions are executed under transaction costs, market impact, capacity limits, and binding risk constraints. This paper treats forecasting as an econometric input to a constrained decision problem. A predictive distribution induces a decision rule through a utility objective combined with an explicit friction operator consisting of both a cost functional and a feasible-set constraint system. The econometric target becomes minimisation of expected decision loss net of costs rather than minimisation of prediction error. The paper develops a utility-weighted calibration criterion aligned to the decision loss and establishes sufficient conditions under which calibrated predictive distributions weakly dominate uncalibrated alternatives. An empirical study using a pre-committed nested walk-forward protocol on liquid equity index futures confirms the theory: the proposed utility-weighted calibration reduces realised decision loss by over 30\% relative to an uncalibrated baseline ($t$-stat -30.31) for loss differential and improves the Sharpe ratio from -3.62 to -2.29 during a drawdown regime. The mechanism is identified as a structural reduction in the frequency of binding constraints (from 16.0\% to 5.1\%), preventing the "corner solution" failures that characterize overconfident forecasts in high-friction environments.

</details>


### [90] [Fake Date Tests: Can We Trust In-sample Accuracy of LLMs in Macroeconomic Forecasting?](https://arxiv.org/abs/2601.07992)
*Alexander Eliseev,Sergei Seleznev*

Main category: econ.EM

TL;DR: 研究发现现代大语言模型在宏观经济预测中存在前瞻性偏差，无法通过作者设计的假日期测试，表明其样本内预测准确性不能外推至样本外表现。


<details>
  <summary>Details</summary>
Motivation: 经济学家开始将大语言模型应用于实证研究，特别是宏观经济预测。但现有研究通常在相同数据上进行训练和回测，这引发了一个关键问题：样本内预测的准确性结果能否外推至样本外表现？

Method: 开发了一套提示敏感性测试方法，特别是其中的"假日期测试"，用于检测大语言模型样本内预测中的两种偏差：前瞻性偏差和上下文偏差。

Result: 实证结果显示，研究中测试的所有现代大语言模型都未能通过第一个测试，表明它们的样本内预测存在前瞻性偏差。

Conclusion: 大语言模型在宏观经济预测中的样本内表现不能可靠地外推至样本外表现，因为存在系统性偏差，这对其在实证经济学研究中的应用提出了重要警示。

Abstract: Large language models (LLMs) are a type of machine learning tool that economists have started to apply in their empirical research. One such application is macroeconomic forecasting with backtesting of LLMs, even though they are trained on the same data that is used to estimate their forecasting performance. Can these in-sample accuracy results be extrapolated to the model's out-of-sample performance? To answer this question, we developed a family of prompt sensitivity tests and two members of this family, which we call the fake date tests. These tests aim to detect two types of biases in LLMs' in-sample forecasts: lookahead bias and context bias. According to the empirical results, none of the modern LLMs tested in this study passed our first test, signaling the presence of lookahead bias in their in-sample forecasts.

</details>


### [91] [Estimating Treatment Effects in Panel Data Without Parallel Trends](https://arxiv.org/abs/2601.08281)
*Shoya Ishimaru*

Main category: econ.EM

TL;DR: 提出了一种新的面板数据中处理效应估计方法，通过放宽平行趋势假设，允许多维不可观测因素和非加性可分离性，相比标准双重差分法能更准确地估计处理效应。


<details>
  <summary>Details</summary>
Motivation: 标准双重差分法依赖平行趋势假设，要求不可观测因素是一维、时不变且加性可分离的，这在现实中往往不成立。本文旨在解决这一限制，提供更灵活的处理效应估计框架。

Method: 引入了一个更灵活的框架，允许多维不可观测因素和非加性可分离性，并提供了识别处理组平均处理效应的充分条件。

Result: 在就业流失的实证应用中，相比标准双重差分法，新方法显示长期收入损失显著更小，表明该方法能更好地处理处理组和对照组之间未观测异质性导致的差异结果轨迹。

Conclusion: 提出的框架通过放宽标准双重差分法的严格假设，提供了更灵活、更准确的处理效应估计方法，特别适用于存在多维未观测异质性的面板数据场景。

Abstract: This paper proposes a novel approach for estimating treatment effects in panel data settings, addressing key limitations of the standard difference-in-differences (DID) approach. The standard approach relies on the parallel trends assumption, implicitly requiring that unobservable factors correlated with treatment assignment be unidimensional, time-invariant, and affect untreated potential outcomes in an additively separable manner. This paper introduces a more flexible framework that allows for multidimensional unobservables and non-additive separability, and provides sufficient conditions for identifying the average treatment effect on the treated. An empirical application to job displacement reveals substantially smaller long-run earnings losses compared to the standard DID approach, demonstrating the framework's ability to account for unobserved heterogeneity that manifests as differential outcome trajectories between treated and control groups.

</details>


### [92] [Systemic Risk Surveillance](https://arxiv.org/abs/2601.08598)
*Timo Dimitriadis,Yannick Hoga*

Main category: econ.EM

TL;DR: 提出系统性风险监测方案，可在"在线"方式下检测错误指定的系统性风险预测，实现每日监控并控制错误拒绝的累积


<details>
  <summary>Details</summary>
Motivation: 近年来金融市场动荡频发，系统性风险变化日益受到关注，需要及时监测以采取应对措施避免金融危机

Method: 开发系统性风险监测方案，允许多个序列同时监控，控制测试拒绝的累积概率，保持测试规模

Result: 蒙特卡洛模拟显示良好的有限样本性质，对美国银行在多次危机中的实证应用证明了监测方案对监管机构和金融机构的实用性

Conclusion: 提出的在线监测方案能有效监控系统性风险预测，及时发现早期预警信号，为及时采取应对措施提供支持

Abstract: Following several episodes of financial market turmoil in recent decades, changes in systemic risk have drawn growing attention. Therefore, we propose surveillance schemes for systemic risk, which allow to detect misspecified systemic risk forecasts in an "online" fashion. This enables daily monitoring of the forecasts while controlling for the accumulation of false test rejections. Such online schemes are vital in taking timely countermeasures to avoid financial distress. Our monitoring procedures allow multiple series at once to be monitored, thus increasing the likelihood and the speed at which early signs of trouble may be picked up. The tests hold size by construction, such that the null of correct systemic risk assessments is only rejected during the monitoring period with (at most) a pre-specified probability. Monte Carlo simulations illustrate the good finite-sample properties of our procedures. An empirical application to US banks during multiple crises demonstrates the usefulness of our surveillance schemes for both regulators and financial institutions.

</details>


### [93] [Automatic debiased machine learning and sensitivity analysis for sample selection models](https://arxiv.org/abs/2601.08643)
*Jakob Bjelac,Victor Chernozhukov,Phil-Adrian Klotz,Jannis Kueck,Theresa M. A. Schmitz*

Main category: econ.EM

TL;DR: 将Riesz表示框架扩展到样本选择下的因果推断，提出ForestRiesz估计器，避免逆概率加权的不稳定性，应用于美国性别工资差距分析


<details>
  <summary>Details</summary>
Motivation: 在样本选择问题中，治疗分配和结果可观测性都是非随机的，传统双重机器学习方法依赖逆概率加权，对调参敏感且不稳定，需要更稳健的框架

Method: 将问题表述为Riesz表示器，实现稳定估计；提出ForestRiesz估计器，考虑选择性结果可观测性，避免直接倾向得分逆变换的不稳定性；利用自动去偏机器学习

Result: 模拟研究表明传统双重机器学习方法对调参高度敏感，而ForestRiesz估计器提供更稳定性能；美国性别工资差距实证应用中，ForestRiesz方法比标准双重机器学习得到更大的处理效应估计，表明忽略样本选择会低估性别工资差距；敏感性分析显示需要极强未观测混杂才能推翻结果

Conclusion: 该方法为样本选择下的因果推断提供了统一、稳健且计算有吸引力的框架，能够更准确地估计处理效应并识别样本选择偏差的影响

Abstract: In this paper, we extend the Riesz representation framework to causal inference under sample selection, where both treatment assignment and outcome observability are non-random. Formulating the problem in terms of a Riesz representer enables stable estimation and a transparent decomposition of omitted variable bias into three interpretable components: a data-identified scale factor, outcome confounding strength, and selection confounding strength. For estimation, we employ the ForestRiesz estimator, which accounts for selective outcome observability while avoiding the instability associated with direct propensity score inversion. We assess finite-sample performance through a simulation study and show that conventional double machine learning approaches can be highly sensitive to tuning parameters due to their reliance on inverse probability weighting, whereas the ForestRiesz estimator delivers more stable performance by leveraging automatic debiased machine learning. In an empirical application to the gender wage gap in the U.S., we find that our ForestRiesz approach yields larger treatment effect estimates than a standard double machine learning approach, suggesting that ignoring sample selection leads to an underestimation of the gender wage gap. Sensitivity analysis indicates that implausibly strong unobserved confounding would be required to overturn our results. Overall, our approach provides a unified, robust, and computationally attractive framework for causal inference under sample selection.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [94] [Bridging the Trust Gap: Clinician-Validated Hybrid Explainable AI for Maternal Health Risk Assessment in Bangladesh](https://arxiv.org/abs/2601.07866)
*Farjana Yesmin,Nusrat Shirmin,Suraiya Shabnam Bristy*

Main category: cs.AI

TL;DR: 该研究提出了一种混合可解释AI框架，结合模糊逻辑和SHAP解释，用于资源受限地区的孕产妇健康风险预测，通过临床医生验证显示能增强信任和实用性。


<details>
  <summary>Details</summary>
Motivation: 机器学习在孕产妇健康风险预测中虽有潜力，但在资源受限的临床环境中面临关键障碍：缺乏可解释性和信任度，阻碍了临床采用。

Method: 开发了混合可解释AI框架，结合事前模糊逻辑和事后SHAP解释，构建模糊-XGBoost模型，使用1,014份孕产妇健康记录进行训练，并通过孟加拉国14名医疗专业人员的系统性反馈进行验证。

Result: 模型达到88.67%准确率（ROC-AUC: 0.9703），验证研究显示71.4%的临床医生偏好混合解释，54.8%表示对临床使用具有信任。SHAP分析识别医疗可及性为主要预测因子，模糊风险评分排名第三验证了临床知识整合。

Conclusion: 结合可解释模糊规则和特征重要性解释能增强实用性和信任度，为孕产妇医疗保健中的XAI部署提供实用见解，但需补充产科史、孕周和连接障碍等关键信息。

Abstract: While machine learning shows promise for maternal health risk prediction, clinical adoption in resource-constrained settings faces a critical barrier: lack of explainability and trust. This study presents a hybrid explainable AI (XAI) framework combining ante-hoc fuzzy logic with post-hoc SHAP explanations, validated through systematic clinician feedback. We developed a fuzzy-XGBoost model on 1,014 maternal health records, achieving 88.67% accuracy (ROC-AUC: 0.9703). A validation study with 14 healthcare professionals in Bangladesh revealed strong preference for hybrid explanations (71.4% across three clinical cases) with 54.8% expressing trust for clinical use. SHAP analysis identified healthcare access as the primary predictor, with the engineered fuzzy risk score ranking third, validating clinical knowledge integration (r=0.298). Clinicians valued integrated clinical parameters but identified critical gaps: obstetric history, gestational age, and connectivity barriers. This work demonstrates that combining interpretable fuzzy rules with feature importance explanations enhances both utility and trust, providing practical insights for XAI deployment in maternal healthcare.

</details>


### [95] [Executable Ontologies in Game Development: From Algorithmic Control to Semantic World Modeling](https://arxiv.org/abs/2601.07964)
*Alexander Boldachev*

Main category: cs.AI

TL;DR: 本文探讨了可执行本体论在游戏开发中的应用，通过boldsea框架实现从算法行为编程到语义世界建模的范式转变，使智能体行为从声明式领域规则中自然涌现。


<details>
  <summary>Details</summary>
Motivation: 解决游戏AI架构中的语义-过程鸿沟问题，传统方法（如行为树和面向目标行动规划）主要关注智能体应该做什么，而可执行本体论关注行动何时变得可能，实现更自然的智能体行为。

Method: 使用boldsea框架实现可执行本体论，通过生存游戏场景（Winter Feast）演示，利用数据流条件而非显式抢占逻辑实现基于优先级的任务中断，并探讨与LLM驱动的运行时模型生成的集成策略。

Result: 与行为树和面向目标行动规划相比，可执行本体论能够更好地建模行动何时变得可能，提供时间事件图固有的调试优势，并展示了通过数据流条件实现优先级任务中断的有效性。

Conclusion: 可执行本体论代表了游戏AI开发的范式转变，从显式编程转向语义世界建模，为解决语义-过程鸿沟提供了新途径，并具有与LLM驱动的运行时模型生成的集成潜力。

Abstract: This paper examines the application of Executable Ontologies (EO), implemented through the boldsea framework, to game development. We argue that EO represents a paradigm shift: a transition from algorithmic behavior programming to semantic world modeling, where agent behavior emerges naturally from declarative domain rules rather than being explicitly coded. Using a survival game scenario (Winter Feast), we demonstrate how EO achieves prioritybased task interruption through dataflow conditions rather than explicit preemption logic. Comparison with Behavior Trees (BT) and Goal-Oriented Action Planning (GOAP) reveals that while these approaches model what agents should do, EO models when actions become possible - a fundamental difference that addresses the semantic-process gap in game AI architecture. We discuss integration strategies, debugging advantages inherent to temporal event graphs, and the potential for LLM-driven runtime model generation.

</details>


### [96] [When Models Know When They Do Not Know: Calibration, Cascading, and Cleaning](https://arxiv.org/abs/2601.07965)
*Chenjie Hao,Weyl Lu,Yuko Ishiwaka,Zengyi Li,Weier Wan,Yubei Chen*

Main category: cs.AI

TL;DR: 提出一种无需训练、通用的模型校准方法，利用校准置信度实现模型级联和数据清洗，提高AI系统的效率、可靠性和可信度。


<details>
  <summary>Details</summary>
Motivation: 当模型能够识别自身不知道的情况时，会开启许多可能性。关键问题是如何让模型识别自己不知道的情况，一个可行方法是利用模型内部信号计算置信度来反映其不确定性。

Method: 提出简单、有效、无需训练的通用方法，适用于视觉和语言模型。基于两个关键观察：1) 单个模型内置信度越高准确率越高；2) 在验证集上校准的模型在测试集上保持校准。应用包括：1) 基于校准优势路由的模型级联；2) 基于模型集成和校准置信度的数据清洗方法。

Result: 通过校准置信度的可比性，级联大小模型可在几乎不损失准确率的情况下提高效率；级联两个规模相当的模型可获得超越任一单独模型的性能。基于多专家和校准置信度的数据清洗方法能有效识别ImageNet和MMLU数据集中的错误标注样本。

Conclusion: 让模型识别自身不知道的情况是实现更高效、可靠、可信AI的实用步骤，校准置信度为此提供了可靠基础。

Abstract: When a model knows when it does not know, many possibilities emerge. The first question is how to enable a model to recognize that it does not know. A promising approach is to use confidence, computed from the model's internal signals, to reflect its ignorance. Prior work in specific domains has shown that calibration can provide reliable confidence estimates. In this work, we propose a simple, effective, and universal training-free method that applies to both vision and language models, performing model calibration, cascading, and data cleaning to better exploit a model's ability to recognize when it does not know. We first highlight two key empirical observations: higher confidence corresponds to higher accuracy within a single model, and models calibrated on the validation set remain calibrated on a held-out test set. These findings empirically establish the reliability and comparability of calibrated confidence. Building on this, we introduce two applications: (1) model cascading with calibrated advantage routing and (2) data cleaning based on model ensemble. Using the routing signal derived from the comparability of calibrated confidences, we cascade large and small models to improve efficiency with almost no compromise in accuracy, and we further cascade two models of comparable scale to achieve performance beyond either model alone. Leveraging multiple experts and their calibrated confidences, we design a simple yet effective data-cleaning method that balances precision and detection rate to identify mislabeled samples in ImageNet and Massive Multitask Language Understanding (MMLU) datasets. Our results demonstrate that enabling models to recognize when they do not know is a practical step toward more efficient, reliable, and trustworthy AI.

</details>


### [97] [Reasoning over Precedents Alongside Statutes: Case-Augmented Deliberative Alignment for LLM Safety](https://arxiv.org/abs/2601.08000)
*Can Jin,Rui Wu,Tong Che,Qixin Zhang,Hongwu Peng,Jiahui Zhao,Zhenting Wang,Wenqi Wei,Ligong Han,Zhao Zhang,Yuan Cao,Ruixiang Tang,Dimitris N. Metaxas*

Main category: cs.AI

TL;DR: 论文提出CADA方法，通过案例增强的审议对齐来提升LLM的安全性，避免过度拒绝良性请求，相比详细的代码式安全规则更有效。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全对齐方法面临挑战：详细的安全规则可能导致过度拒绝良性请求，而开源LLM通常缺乏高级推理能力来处理复杂的代码式安全规则。需要找到既能提升安全性又不损害帮助性的方法。

Method: 提出CADA（案例增强审议对齐）方法：1）使用案例增强的简单代码而非详细规则；2）通过强化学习训练LLM生成安全推理链；3）结合案例示范来引导模型进行安全推理。

Result: CADA有效提升了无害性，增强了对抗攻击的鲁棒性，减少了过度拒绝，同时在多样化基准测试中保持了实用性。相比仅依赖规则的审议对齐方法表现更优。

Conclusion: 案例增强的审议对齐方法比详细的代码式安全规则更有效，能够在提升LLM安全性的同时保持帮助性，为开源LLM提供了一种实用的安全对齐替代方案。

Abstract: Ensuring that Large Language Models (LLMs) adhere to safety principles without refusing benign requests remains a significant challenge. While OpenAI introduces deliberative alignment (DA) to enhance the safety of its o-series models through reasoning over detailed ``code-like'' safety rules, the effectiveness of this approach in open-source LLMs, which typically lack advanced reasoning capabilities, is understudied. In this work, we systematically evaluate the impact of explicitly specifying extensive safety codes versus demonstrating them through illustrative cases. We find that referencing explicit codes inconsistently improves harmlessness and systematically degrades helpfulness, whereas training on case-augmented simple codes yields more robust and generalized safety behaviors. By guiding LLMs with case-augmented reasoning instead of extensive code-like safety rules, we avoid rigid adherence to narrowly enumerated rules and enable broader adaptability. Building on these insights, we propose CADA, a case-augmented deliberative alignment method for LLMs utilizing reinforcement learning on self-generated safety reasoning chains. CADA effectively enhances harmlessness, improves robustness against attacks, and reduces over-refusal while preserving utility across diverse benchmarks, offering a practical alternative to rule-only DA for improving safety while maintaining helpfulness.

</details>


### [98] [Why AI Alignment Failure Is Structural: Learned Human Interaction Structures and AGI as an Endogenous Evolutionary Shock](https://arxiv.org/abs/2601.08673)
*Didier Sornette,Sandro Claudio Lera,Ke Wu*

Main category: cs.AI

TL;DR: 该论文认为将大语言模型表现出的欺骗、威胁等行为解读为"对齐失败"或"恶意涌现"是概念错误，这些行为实际上是人类权力不对称互动模式的统计泛化，而非模型具有道德推理能力。真正的风险是AGI作为人类智能、权力和矛盾的放大器，压缩了历史容错空间。


<details>
  <summary>Details</summary>
Motivation: 针对当前对大语言模型表现出欺骗、威胁等"不道德"行为的普遍担忧，作者认为这些解读存在根本性概念错误。需要重新理解这些行为的本质，并重新定义AGI的真正风险所在。

Method: 采用关系模型理论，将黑马等行为视为市场定价、权威关系、最后通牒博弈等正常社会行为的连续统中的极限情况。通过理论分析而非实证研究，重新概念化LLM行为与人类互动模式的关系。

Result: LLM的"不道德"行为并非模型具有恶意意图的证据，而是对人类在权力、信息或约束极端不对称情境下互动模式的统计泛化。这些行为反映了人类社会的完整行为谱系，而不仅仅是社会认可的行为。

Conclusion: AGI的主要风险不是对抗性意图，而是作为人类智能、权力和矛盾的内生放大器。通过消除认知和制度摩擦，AGI压缩了时间尺度，移除了历史容错空间。对齐失败是结构性的而非偶然的，需要关注放大效应、复杂性和制度稳定性，而不仅仅是模型层面的意图。

Abstract: Recent reports of large language models (LLMs) exhibiting behaviors such as deception, threats, or blackmail are often interpreted as evidence of alignment failure or emergent malign agency. We argue that this interpretation rests on a conceptual error. LLMs do not reason morally; they statistically internalize the record of human social interaction, including laws, contracts, negotiations, conflicts, and coercive arrangements. Behaviors commonly labeled as unethical or anomalous are therefore better understood as structural generalizations of interaction regimes that arise under extreme asymmetries of power, information, or constraint. Drawing on relational models theory, we show that practices such as blackmail are not categorical deviations from normal social behavior, but limiting cases within the same continuum that includes market pricing, authority relations, and ultimatum bargaining. The surprise elicited by such outputs reflects an anthropomorphic expectation that intelligence should reproduce only socially sanctioned behavior, rather than the full statistical landscape of behaviors humans themselves enact. Because human morality is plural, context-dependent, and historically contingent, the notion of a universally moral artificial intelligence is ill-defined. We therefore reframe concerns about artificial general intelligence (AGI). The primary risk is not adversarial intent, but AGI's role as an endogenous amplifier of human intelligence, power, and contradiction. By eliminating longstanding cognitive and institutional frictions, AGI compresses timescales and removes the historical margin of error that has allowed inconsistent values and governance regimes to persist without collapse. Alignment failure is thus structural, not accidental, and requires governance approaches that address amplification, complexity, and regime stability rather than model-level intent alone.

</details>


### [99] [Internal Deployment Gaps in AI Regulation](https://arxiv.org/abs/2601.08005)
*Joe Kwon,Stephen Casper*

Main category: cs.AI

TL;DR: 论文分析了美国和欧盟2025年前沿AI法规在内部部署方面的监管漏洞，识别了三个可能导致内部系统逃避监管的缺口，并提出了解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前前沿AI监管主要关注面向外部用户的系统部署，但企业内部部署的高风险AI应用（如自动化研发、关键业务流程处理、敏感数据处理）可能逃避监管。论文旨在揭示现有法规在内部部署方面的监管漏洞。

Method: 通过分析美国和欧盟2025年的前沿AI法规，识别内部部署监管的三个主要缺口：范围模糊性、时点合规评估、信息不对称。分析这些缺口存在的原因（可测量性、激励机制、信息获取的张力），并绘制潜在的解决方案及其权衡。

Result: 识别出三个关键监管缺口：1) 范围模糊性允许内部系统逃避监管义务；2) 时点合规评估无法捕捉内部系统的持续演进；3) 信息不对称削弱监管意识和监督能力。这些缺口源于可测量性、激励机制和信息获取之间的张力。

Conclusion: 通过理解这些模式，政策制定者可以有意识地而非偶然地制定关于内部部署AI系统的政策选择。需要解决监管范围、评估方法和信息透明度等问题，以确保内部部署的高风险AI系统得到适当监管。

Abstract: Frontier AI regulations primarily focus on systems deployed to external users, where deployment is more visible and subject to outside scrutiny. However, high-stakes applications can occur internally when companies deploy highly capable systems within their own organizations, such as for automating R\&D, accelerating critical business processes, and handling sensitive proprietary data. This paper examines how frontier AI regulations in the United States and European Union in 2025 handle internal deployment. We identify three gaps that could cause internally-deployed systems to evade intended oversight: (1) scope ambiguity that allows internal systems to evade regulatory obligations, (2) point-in-time compliance assessments that fail to capture the continuous evolution of internal systems, and (3) information asymmetries that subvert regulatory awareness and oversight. We then analyze why these gaps persist, examining tensions around measurability, incentives, and information access. Finally, we map potential approaches to address them and their associated tradeoffs. By understanding these patterns, we hope that policy choices around internally deployed AI systems can be made deliberately rather than incidentally.

</details>


### [100] [Evaluating the Ability of Explanations to Disambiguate Models in a Rashomon Set](https://arxiv.org/abs/2601.08703)
*Kaivalya Rawal,Eoin Delaney,Zihao Fu,Sandra Wachter,Chris Russell*

Main category: cs.AI

TL;DR: 该论文提出了一种无需真实标签的模型解释评估方法AXE，能够检测Rashomon集合中不同模型的行为差异，防止对抗性"公平洗白"。


<details>
  <summary>Details</summary>
Motivation: 现有解释评估方法依赖与真实解释的对比，这会掩盖Rashomon集合中不同模型的行为差异，且无法检测对抗性操纵解释的情况。

Method: 提出AXE方法，基于三个解释评估原则，通过分析Rashomon集合中模型解释的差异来评估解释质量，无需真实标签。

Result: AXE能够100%成功检测对抗性公平洗白，识别受保护属性是否被用于预测，优于基于模型敏感性或真实标签对比的方法。

Conclusion: AXE提供了一种无需真实标签的解释评估框架，能够揭示Rashomon集合中模型的行为差异，帮助选择更合适的模型部署。

Abstract: Explainable artificial intelligence (XAI) is concerned with producing explanations indicating the inner workings of models. For a Rashomon set of similarly performing models, explanations provide a way of disambiguating the behavior of individual models, helping select models for deployment. However explanations themselves can vary depending on the explainer used, and need to be evaluated. In the paper "Evaluating Model Explanations without Ground Truth", we proposed three principles of explanation evaluation and a new method "AXE" to evaluate the quality of feature-importance explanations. We go on to illustrate how evaluation metrics that rely on comparing model explanations against ideal ground truth explanations obscure behavioral differences within a Rashomon set. Explanation evaluation aligned with our proposed principles would highlight these differences instead, helping select models from the Rashomon set. The selection of alternate models from the Rashomon set can maintain identical predictions but mislead explainers into generating false explanations, and mislead evaluation methods into considering the false explanations to be of high quality. AXE, our proposed explanation evaluation method, can detect this adversarial fairwashing of explanations with a 100% success rate. Unlike prior explanation evaluation strategies such as those based on model sensitivity or ground truth comparison, AXE can determine when protected attributes are used to make predictions.

</details>


### [101] [AI as Entertainment](https://arxiv.org/abs/2601.08768)
*Cody Kommers,Ari Holtzman*

Main category: cs.AI

TL;DR: 论文认为AI领域对娱乐用途的AI内容缺乏评估框架，当前评估主要关注文化危害而非文化益处，提出"厚重娱乐"作为评估AI生成文化内容的新框架。


<details>
  <summary>Details</summary>
Motivation: 当前AI领域主要关注AI作为智能系统提升生产力的叙事，但忽视了AI在娱乐领域的广泛应用和潜在社会影响。数据显示AI已被广泛用于娱乐目的，特别是年轻人群体，且娱乐可能成为AI公司的主要商业模式。然而，现有评估框架无法充分评估娱乐AI的社会文化影响。

Method: 通过分析当前AI评估实践的局限性，指出其仅关注文化危害而忽视文化益处。借鉴人文学科见解，提出"厚重娱乐"框架，该框架考虑娱乐在意义创造、身份形成和社会连接中的作用，而非简单最小化危害。

Result: 识别了AI评估中的关键不对称性：虽然AI评估严格衡量智能的益处和危害，但几乎只关注文化危害。缺乏评估文化产出可能积极益处的框架。提出"厚重娱乐"作为替代评估方法。

Conclusion: AI可能最终更多是关于娱乐而非智能，正如社交媒体更多是关于社会连接。AI领域需要为评估AI生成的文化内容开发更全面的框架，考虑娱乐在人类生活中的积极作用，而不仅仅是防范危害。

Abstract: Generative AI systems are predominantly designed, evaluated, and marketed as intelligent systems which will benefit society by augmenting or automating human cognitive labor, promising to increase personal, corporate, and macroeconomic productivity. But this mainstream narrative about what AI is and what it can do is in tension with another emerging use case: entertainment. We argue that the field of AI is unprepared to measure or respond to how the proliferation of entertaining AI-generated content will impact society. Emerging data suggest AI is already widely adopted for entertainment purposes -- especially by young people -- and represents a large potential source of revenue. We contend that entertainment will become a primary business model for major AI corporations seeking returns on massive infrastructure investments; this will exert a powerful influence on the technology these companies produce in the coming years. Examining current evaluation practices, we identify a critical asymmetry: while AI assessments rigorously measure both benefits and harms of intelligence, they focus almost exclusively on cultural harms. We lack frameworks for articulating how cultural outputs might be actively beneficial. Drawing on insights from the humanities, we propose "thick entertainment" as a framework for evaluating AI-generated cultural content -- one that considers entertainment's role in meaning-making, identity formation, and social connection rather than simply minimizing harm. While AI is often touted for its potential to revolutionize productivity, in the long run we may find that AI turns out to be as much about "intelligence" as social media is about social connection.

</details>


### [102] [Integrating Attendance Tracking and Emotion Detection for Enhanced Student Engagement in Smart Classrooms](https://arxiv.org/abs/2601.08049)
*Keith Ainebyona,Ann Move Oguti,Joseph Walusimbi,Ritah Kobusingye*

Main category: cs.AI

TL;DR: SCASED是一个基于物联网的智能教室系统，整合了自动考勤和面部情绪识别，用于监测课堂参与度，准确率达89.5%。


<details>
  <summary>Details</summary>
Motivation: 当前智能教室技术主要关注自动化考勤，忽视了学生的情感和认知参与度，限制了教师实时识别学生分心并调整教学策略的能力。

Method: 使用树莓派摄像头和OpenCV进行人脸检测，采用微调的MobileNetV2模型分类四种学习相关情绪状态（参与、无聊、困惑、沮丧），通过会话机制管理考勤和情绪监测，数据通过云端仪表板可视化。

Result: 在DAiSEE数据集上的实验评估显示，情绪分类准确率达到89.5%，整合考勤数据和情绪分析能为教师提供课堂动态的额外洞察。

Conclusion: 整合考勤与情绪分析的系统能为教师提供更全面的课堂动态洞察，支持更及时响应的教学实践。

Abstract: The increasing adoption of smart classroom technologies in higher education has mainly focused on automating attendance, with limited attention given to students' emotional and cognitive engagement during lectures. This limits instructors' ability to identify disengagement and adapt teaching strategies in real time. This paper presents SCASED (Smart Classroom Attendance System with Emotion Detection), an IoT-based system that integrates automated attendance tracking with facial emotion recognition to support classroom engagement monitoring. The system uses a Raspberry Pi camera and OpenCV for face detection, and a finetuned MobileNetV2 model to classify four learning-related emotional states: engagement, boredom, confusion, and frustration. A session-based mechanism is implemented to manage attendance and emotion monitoring by recording attendance once per session and performing continuous emotion analysis thereafter. Attendance and emotion data are visualized through a cloud-based dashboard to provide instructors with insights into classroom dynamics. Experimental evaluation using the DAiSEE dataset achieved an emotion classification accuracy of 89.5%. The results show that integrating attendance data with emotion analytics can provide instructors with additional insight into classroom dynamics and support more responsive teaching practices.

</details>


### [103] [Forecast Aware Deep Reinforcement Learning for Efficient Electricity Load Scheduling in Dairy Farms](https://arxiv.org/abs/2601.08052)
*Nawazish Alia,Rachael Shawb,Karl Mason*

Main category: cs.AI

TL;DR: 本文提出了一种用于乳牛场负载调度的深度强化学习框架，包含Forecast Aware PPO和PID KL PPO两种变体，在真实数据上实现了比现有方法更低的电力成本和更高的电网进口减少。


<details>
  <summary>Details</summary>
Motivation: 乳牛场是能源密集型行业，严重依赖电网电力。随着可再生能源整合增加，可持续能源管理变得至关重要。然而，可再生能源的间歇性给实时供需平衡带来挑战，现有RL调度方法通常假设完全了解未来价格或发电情况，这在动态环境中不现实，且标准PPO变体依赖固定阈值导致训练不稳定。

Method: 提出深度强化学习框架用于乳牛场负载调度，重点关注电池存储和水加热。包含两种PPO变体：1) Forecast Aware PPO：利用小时和月份基于残差校准的短期需求与可再生能源发电预测；2) PID KL PPO：采用比例积分微分控制器自适应调节KL散度以实现稳定的策略更新。

Result: 在真实乳牛场数据上训练，方法比PPO降低1%电力成本，比DQN降低4.8%，比SAC降低1.5%。对于电池调度，PPO减少电网进口13.1%，展示了在现代乳牛场可持续能源管理中的可扩展性和有效性。

Conclusion: 提出的深度强化学习框架通过整合短期预测和自适应KL散度控制，有效解决了乳牛场负载调度中的现实挑战，为可持续能源管理提供了可扩展且有效的解决方案。

Abstract: Dairy farming is an energy intensive sector that relies heavily on grid electricity. With increasing renewable energy integration, sustainable energy management has become essential for reducing grid dependence and supporting the United Nations Sustainable Development Goal 7 on affordable and clean energy. However, the intermittent nature of renewables poses challenges in balancing supply and demand in real time. Intelligent load scheduling is therefore crucial to minimize operational costs while maintaining reliability. Reinforcement Learning has shown promise in improving energy efficiency and reducing costs. However, most RL-based scheduling methods assume complete knowledge of future prices or generation, which is unrealistic in dynamic environments. Moreover, standard PPO variants rely on fixed clipping or KL divergence thresholds, often leading to unstable training under variable tariffs. To address these challenges, this study proposes a Deep Reinforcement Learning framework for efficient load scheduling in dairy farms, focusing on battery storage and water heating under realistic operational constraints. The proposed Forecast Aware PPO incorporates short term forecasts of demand and renewable generation using hour of day and month based residual calibration, while the PID KL PPO variant employs a proportional integral derivative controller to regulate KL divergence for stable policy updates adaptively. Trained on real world dairy farm data, the method achieves up to 1% lower electricity cost than PPO, 4.8% than DQN, and 1.5% than SAC. For battery scheduling, PPO reduces grid imports by 13.1%, demonstrating scalability and effectiveness for sustainable energy management in modern dairy farming.

</details>


### [104] [A New Strategy for Verifying Reach-Avoid Specifications in Neural Feedback Systems](https://arxiv.org/abs/2601.08065)
*Samuel I. Akinwande,Sydney M. Katz,Mykel J. Kochenderfer,Clark Barrett*

Main category: cs.AI

TL;DR: 提出新的后向可达集算法，并与前向分析结合，形成神经反馈系统的统一验证框架


<details>
  <summary>Details</summary>
Motivation: 当前神经反馈系统验证主要依赖前向可达性分析，因为现有后向方法可扩展性有限，需要更强大的后向验证技术

Method: 开发新算法计算后向可达集的过近似和欠近似，并将这些后向算法与现有前向分析技术集成

Result: 创建了统一的验证框架，结合了前向和后向可达性分析的优势

Conclusion: 提出的统一框架解决了神经反馈系统验证中后向方法可扩展性不足的问题，为验证到达-避免属性提供了更全面的解决方案

Abstract: Forward reachability analysis is the predominant approach for verifying reach-avoid properties in neural feedback systems (dynamical systems controlled by neural networks). This dominance stems from the limited scalability of existing backward reachability methods. In this work, we introduce new algorithms that compute both over- and under-approximations of backward reachable sets for such systems. We further integrate these backward algorithms with established forward analysis techniques to yield a unified verification framework for neural feedback systems.

</details>


### [105] [Semantic Gravity Wells: Why Negative Constraints Backfire](https://arxiv.org/abs/2601.08070)
*Shailesh Rana*

Main category: cs.AI

TL;DR: 论文通过机制分析发现，大语言模型在遵循"不要使用词X"这类否定指令时会系统性失败，失败概率与语义压力呈逻辑关系，主要源于两种机制：87.5%的失败源于指令本身激活了被禁词，12.5%源于后期网络层压倒早期抑制信号。


<details>
  <summary>Details</summary>
Motivation: 否定指令（如"不要使用词X"）是测试大语言模型指令遵循能力的基本测试，但这类看似简单的指令却经常失败，且失败条件一直未被充分理解。本文旨在首次对否定指令失败进行全面的机制性研究。

Method: 引入"语义压力"量化模型生成被禁词的内在概率，使用逻辑回归分析失败概率与压力的关系。通过logit lens技术进行分层分析，识别两种失败模式：激活失败（87.5%）和覆盖失败（12.5%）。使用激活修补技术确认因果责任层。

Result: 失败概率与语义压力呈紧密逻辑关系（p=σ(-2.40+2.27·P₀)）。抑制信号在失败中系统性较弱：失败时目标概率仅降低5.2个百分点，成功时降低22.8个百分点（4.4倍差异）。激活修补确认23-27层对失败有因果责任。

Conclusion: 否定指令设计存在根本性矛盾：命名被禁词的行为本身会激活模型产生该词。这揭示了语言模型在处理否定约束时的系统性弱点，对指令设计和模型安全有重要启示。

Abstract: Negative constraints (instructions of the form "do not use word X") represent a fundamental test of instruction-following capability in large language models. Despite their apparent simplicity, these constraints fail with striking regularity, and the conditions governing failure have remained poorly understood. This paper presents the first comprehensive mechanistic investigation of negative instruction failure. We introduce semantic pressure, a quantitative measure of the model's intrinsic probability of generating the forbidden token, and demonstrate that violation probability follows a tight logistic relationship with pressure ($p=σ(-2.40+2.27\cdot P_0)$; $n=40{,}000$ samples; bootstrap $95%$ CI for slope: $[2.21,,2.33]$). Through layer-wise analysis using the logit lens technique, we establish that the suppression signal induced by negative instructions is present but systematically weaker in failures: the instruction reduces target probability by only 5.2 percentage points in failures versus 22.8 points in successes -- a $4.4\times$ asymmetry. We trace this asymmetry to two mechanistically distinct failure modes. In priming failure (87.5% of violations), the instruction's explicit mention of the forbidden word paradoxically activates rather than suppresses the target representation. In override failure (12.5%), late-layer feed-forward networks generate contributions of $+0.39$ toward the target probability -- nearly $4\times$ larger than in successes -- overwhelming earlier suppression signals. Activation patching confirms that layers 23--27 are causally responsible: replacing these layers' activations flips the sign of constraint effects. These findings reveal a fundamental tension in negative constraint design: the very act of naming a forbidden word primes the model to produce it.

</details>


### [106] [MemoBrain: Executive Memory as an Agentic Brain for Reasoning](https://arxiv.org/abs/2601.08079)
*Hongjin Qian,Zhao Cao,Zheng Liu*

Main category: cs.AI

TL;DR: MemoBrain：为工具增强型智能体设计的执行记忆模型，通过构建依赖感知的记忆来管理长时推理轨迹，在固定上下文预算下保持紧凑、高显著性的推理主干。


<details>
  <summary>Details</summary>
Motivation: 工具增强型智能体框架中的复杂推理本质上是长时程的，导致推理轨迹和临时工具产物不断积累，超出大语言模型有限的工作上下文容量。缺乏显式记忆机制会破坏逻辑连续性并削弱任务对齐，因此记忆不是辅助效率问题，而是维持长时程连贯、目标导向推理的核心组件。

Method: 提出MemoBrain执行记忆模型，为工具增强型智能体构建依赖感知的记忆，捕捉关键中间状态及其逻辑关系。作为推理智能体的协同伙伴，MemoBrain在不阻塞执行的情况下组织推理进展，并主动管理工作上下文：修剪无效步骤、折叠完成的子轨迹、在固定上下文预算下保持紧凑的高显著性推理主干。

Result: 在具有挑战性的长时程基准测试（包括GAIA、WebWalker和BrowseComp-Plus）上评估MemoBrain，相比强基线模型展现出持续的性能改进。

Conclusion: MemoBrain通过显式认知控制推理轨迹而非被动上下文积累，为工具增强型智能体提供了有效的长时程推理支持，将记忆从辅助效率组件提升为核心推理维持机制。

Abstract: Complex reasoning in tool-augmented agent frameworks is inherently long-horizon, causing reasoning traces and transient tool artifacts to accumulate and strain the bounded working context of large language models. Without explicit memory mechanisms, such accumulation disrupts logical continuity and undermines task alignment. This positions memory not as an auxiliary efficiency concern, but as a core component for sustaining coherent, goal-directed reasoning over long horizons.
  We propose MemoBrain, an executive memory model for tool-augmented agents that constructs a dependency-aware memory over reasoning steps, capturing salient intermediate states and their logical relations. Operating as a co-pilot alongside the reasoning agent, MemoBrain organizes reasoning progress without blocking execution and actively manages the working context. Specifically, it prunes invalid steps, folds completed sub-trajectories, and preserves a compact, high-salience reasoning backbone under a fixed context budget. Together, these mechanisms enable explicit cognitive control over reasoning trajectories rather than passive context accumulation.
  We evaluate MemoBrain on challenging long-horizon benchmarks, including GAIA, WebWalker, and BrowseComp-Plus, demonstrating consistent improvements over strong baselines.

</details>


### [107] [MirrorBench: An Extensible Framework to Evaluate User-Proxy Agents for Human-Likeness](https://arxiv.org/abs/2601.08118)
*Ashutosh Hathidara,Julien Yu,Vaishali Senthil,Sebastian Schreiber,Anil Babu Ankisettipalli*

Main category: cs.AI

TL;DR: MIRRORBENCH是一个用于评估LLM用户代理模拟人类对话能力的基准测试框架，通过模块化设计支持多种任务和指标，发现当前代理与真实用户存在系统性差距。


<details>
  <summary>Details</summary>
Motivation: LLM越来越多地被用作人类模拟器，用于评估对话系统和生成微调数据，但简单的"扮演用户"提示通常会产生冗长、不真实的对话，需要系统性地评估用户代理的真实性。

Method: 开发了MIRRORBENCH框架，包含模块化执行引擎、类型化接口、元数据驱动注册、多后端支持、缓存和可观测性。支持可插拔的用户代理、数据集、任务和指标，包括三个词汇多样性指标和三个基于LLM评判的指标。

Result: 在四个公开数据集上，MIRRORBENCH产生了方差感知的结果，并揭示了用户代理与真实人类用户之间的系统性差距。

Conclusion: MIRRORBENCH提供了一个可复现、可扩展的基准测试框架，专门用于评估LLM用户代理生成人类对话的能力，有助于推动更真实的人类模拟器研究。

Abstract: Large language models (LLMs) are increasingly used as human simulators, both for evaluating conversational systems and for generating fine-tuning data. However, naive "act-as-a-user" prompting often yields verbose, unrealistic utterances, underscoring the need for principled evaluation of so-called user proxy agents. We present MIRRORBENCH, a reproducible, extensible benchmarking framework that evaluates user proxies solely on their ability to produce human-like user utterances across diverse conversational tasks, explicitly decoupled from downstream task success. MIRRORBENCH features a modular execution engine with typed interfaces, metadata-driven registries, multi-backend support, caching, and robust observability. The system supports pluggable user proxies, datasets, tasks, and metrics, enabling researchers to evaluate arbitrary simulators under a uniform, variance-aware harness. We include three lexical-diversity metrics (MATTR, YULE'S K, and HD-D) and three LLM-judge-based metrics (GTEval, Pairwise Indistinguishability, and Rubric-and-Reason). Across four open datasets, MIRRORBENCH yields variance-aware results and reveals systematic gaps between user proxies and real human users. The framework is open source and includes a simple command-line interface for running experiments, managing configurations and caching, and generating reports. The framework can be accessed at https://github.com/SAP/mirrorbench.

</details>


### [108] [How vehicles change lanes after encountering crashes: Empirical analysis and modeling](https://arxiv.org/abs/2601.08125)
*Kequan Chen,Yuxuan Wang,Pan Liu,Victor L. Knoop,David Z. W. Wang,Yu Han*

Main category: cs.AI

TL;DR: 论文研究了事故后车道变换行为，构建了首个事故后LC数据集，发现其风险更高，并开发了基于图注意力模块的轨迹预测框架，性能提升超10%。


<details>
  <summary>Details</summary>
Motivation: 事故发生后，后续车辆需要变换车道绕过障碍物，但目标车道车辆可能拒绝让行，增加了事故后LC的复杂性和碰撞风险。然而，事故后LC的行为特征和运动模式尚不清楚。

Method: 从无人机视频中提取车辆轨迹构建事故后LC数据集，开发了基于图注意力的轨迹预测框架，将让行行为建模为辅助交互感知任务，指导条件变分自编码器和Transformer解码器预测轨迹。

Result: 相比强制LC和自由LC，事故后LC持续时间更长、插入速度更低、碰撞风险更高，79.4%涉及至少一次新跟随者不让行行为。模型在轨迹预测性能上比基线提升超10%，并能提供更可靠的碰撞风险分析。

Conclusion: 事故后LC具有独特的行为特征和高风险性，提出的交互感知轨迹预测框架能有效预测轨迹并改善碰撞风险分析，在不同地点的数据集上验证了可迁移性。

Abstract: When a traffic crash occurs, following vehicles need to change lanes to bypass the obstruction. We define these maneuvers as post crash lane changes. In such scenarios, vehicles in the target lane may refuse to yield even after the lane change has already begun, increasing the complexity and crash risk of post crash LCs. However, the behavioral characteristics and motion patterns of post crash LCs remain unknown. To address this gap, we construct a post crash LC dataset by extracting vehicle trajectories from drone videos captured after crashes. Our empirical analysis reveals that, compared to mandatory LCs (MLCs) and discretionary LCs (DLCs), post crash LCs exhibit longer durations, lower insertion speeds, and higher crash risks. Notably, 79.4% of post crash LCs involve at least one instance of non yielding behavior from the new follower, compared to 21.7% for DLCs and 28.6% for MLCs. Building on these findings, we develop a novel trajectory prediction framework for post crash LCs. At its core is a graph based attention module that explicitly models yielding behavior as an auxiliary interaction aware task. This module is designed to guide both a conditional variational autoencoder and a Transformer based decoder to predict the lane changer's trajectory. By incorporating the interaction aware module, our model outperforms existing baselines in trajectory prediction performance by more than 10% in both average displacement error and final displacement error across different prediction horizons. Moreover, our model provides more reliable crash risk analysis by reducing false crash rates and improving conflict prediction accuracy. Finally, we validate the model's transferability using additional post crash LC datasets collected from different sites.

</details>


### [109] [Embedded AI Companion System on Edge Devices](https://arxiv.org/abs/2601.08128)
*Rahul Gupta,Stephen D. H. Hsu*

Main category: cs.AI

TL;DR: 提出一种面向边缘设备的AI伴侣记忆系统，通过活跃/非活跃阶段交替机制，在资源受限环境下平衡实时对话与长期记忆维护


<details>
  <summary>Details</summary>
Motivation: 边缘设备计算资源有限，现有AI伴侣和记忆系统无法直接部署，需要解决低延迟实时对话与长期个性化记忆维护的矛盾

Method: 采用活跃/非活跃阶段交替的记忆范式：用户活跃时进行低延迟轻量检索；用户非活跃时执行计算密集的记忆提取、整合和维护

Result: 使用弱模型(Qwen2.5-7B-Instruct int4量化)的系统在多数指标上优于无记忆的原始LLM，性能与16k上下文窗口的GPT-3.5相当

Conclusion: 提出的记忆范式能在嵌入式硬件严格约束下最小化延迟并保持长期个性化，同时设计了全面的AI伴侣评估基准

Abstract: Computational resource constraints on edge devices make it difficult to develop a fully embedded AI companion system with a satisfactory user experience. AI companion and memory systems detailed in existing literature cannot be directly used in such an environment due to lack of compute resources and latency concerns. In this paper, we propose a memory paradigm that alternates between active and inactive phases: during phases of user activity, the system performs low-latency, real-time dialog using lightweight retrieval over existing memories and context; whereas during phases of user inactivity, it conducts more computationally intensive extraction, consolidation, and maintenance of memories across full conversation sessions. This design minimizes latency while maintaining long-term personalization under the tight constraints of embedded hardware. We also introduce an AI Companion benchmark designed to holistically evaluate the AI Companion across both its conversational quality and memory capabilities. In our experiments, we found that our system (using a very weak model: Qwen2.5-7B-Instruct quantized int4) outperforms the equivalent raw LLM without memory across most metrics, and performs comparably to GPT-3.5 with 16k context window.

</details>


### [110] [Project Synapse: A Hierarchical Multi-Agent Framework with Hybrid Memory for Autonomous Resolution of Last-Mile Delivery Disruptions](https://arxiv.org/abs/2601.08156)
*Arin Gopalan Yadav,Varad Dherange,Kumar Shivam*

Main category: cs.AI

TL;DR: Project Synapse是一个用于自主解决最后一公里配送中断的新型智能体框架，采用分层多智能体架构，使用LangGraph管理复杂工作流，并通过LLM-as-a-Judge协议评估性能。


<details>
  <summary>Details</summary>
Motivation: 最后一公里配送中断是物流行业的重要挑战，需要智能化的自主解决方案。现有系统在处理复杂、循环的工作流方面存在不足，需要更有效的框架来应对实际场景中的配送中断问题。

Method: 采用分层多智能体架构：中央Resolution Supervisor进行战略任务分解，将子任务委派给专门的工作智能体执行战术操作。使用LangGraph来编排管理复杂和循环的工作流。从6000多个真实用户评论中定性分析，构建了30个复杂中断场景的基准数据集。

Result: 通过LLM-as-a-Judge协议评估系统性能，并采用明确的偏见缓解措施。构建了包含30个复杂中断场景的基准数据集，为最后一公里配送中断的自主解决提供了验证框架。

Conclusion: Project Synapse提供了一个有效的自主解决最后一公里配送中断的智能体框架，其分层多智能体架构和LangGraph编排能够处理复杂工作流，并通过LLM评估协议验证了系统性能。

Abstract: This paper introduces Project Synapse, a novel agentic framework designed for the autonomous resolution of last-mile delivery disruptions. Synapse employs a hierarchical multi-agent architecture in which a central Resolution Supervisor agent performs strategic task decomposition and delegates subtasks to specialized worker agents responsible for tactical execution. The system is orchestrated using LangGraph to manage complex and cyclical workflows. To validate the framework, a benchmark dataset of 30 complex disruption scenarios was curated from a qualitative analysis of over 6,000 real-world user reviews. System performance is evaluated using an LLM-as-a-Judge protocol with explicit bias mitigation.

</details>


### [111] [ZeroDVFS: Zero-Shot LLM-Guided Core and Frequency Allocation for Embedded Platforms](https://arxiv.org/abs/2601.08166)
*Mohammad Pivezhandi,Mahdi Banisharif,Abusayeed Saifullah,Ali Jannesari*

Main category: cs.AI

TL;DR: 提出基于模型的分层多智能体强化学习框架，用于多核嵌入式系统的热能和能耗感知调度，结合LLM语义特征提取实现零样本部署，相比现有方法显著提升能效和调度性能。


<details>
  <summary>Details</summary>
Motivation: 现有DVFS和任务分配方法存在局限性：基于利用率的启发式方法忽略停滞时间，基于离线分析的方法需要大量配置且无法运行时适应。需要一种能实时适应、考虑热能和能耗的调度方案。

Method: 1) 分层多智能体强化学习框架分解指数级动作空间；2) 准确的环境模型预测热动力学和性能状态；3) LLM提取13个代码级语义特征；4) Dyna-Q启发框架结合直接强化学习和基于模型的规划；5) 零样本部署通过生成合成训练数据。

Result: 在BOTS和PolybenchC基准测试中，相比Linux ondemand governor：能效提升7.09倍，makespan提升4.0倍。首次决策延迟比基于表格的分析快8300倍，后续决策延迟仅358ms。收敛速度比无模型方法快20倍。

Conclusion: 该框架通过模型驱动的MARL和LLM语义特征提取，实现了高效的热能和能耗感知调度，支持零样本部署，适用于动态嵌入式系统，显著优于现有调度方法。

Abstract: Dynamic voltage and frequency scaling (DVFS) and task-to-core allocation are critical for thermal management and balancing energy and performance in embedded systems. Existing approaches either rely on utilization-based heuristics that overlook stall times, or require extensive offline profiling for table generation, preventing runtime adaptation. We propose a model-based hierarchical multi-agent reinforcement learning (MARL) framework for thermal- and energy-aware scheduling on multi-core platforms. Two collaborative agents decompose the exponential action space, achieving 358ms latency for subsequent decisions. First decisions require 3.5 to 8.0s including one-time LLM feature extraction. An accurate environment model leverages regression techniques to predict thermal dynamics and performance states. When combined with LLM-extracted semantic features, the environment model enables zero-shot deployment for new workloads on trained platforms by generating synthetic training data without requiring workload-specific profiling samples. We introduce LLM-based semantic feature extraction that characterizes OpenMP programs through 13 code-level features without execution. The Dyna-Q-inspired framework integrates direct reinforcement learning with model-based planning, achieving 20x faster convergence than model-free methods. Experiments on BOTS and PolybenchC benchmarks across NVIDIA Jetson TX2, Jetson Orin NX, RubikPi, and Intel Core i7 demonstrate 7.09x better energy efficiency and 4.0x better makespan than Linux ondemand governor. First-decision latency is 8,300x faster than table-based profiling, enabling practical deployment in dynamic embedded systems.

</details>


### [112] [The Agent's First Day: Benchmarking Learning, Exploration, and Scheduling in the Workplace Scenarios](https://arxiv.org/abs/2601.08173)
*Daocheng Fu,Jianbiao Mei,Rong Wu,Xuemeng Yang,Jia Xu,Ding Wang,Pinlong Cai,Yong Liu,Licheng Wen,Botian Shi*

Main category: cs.AI

TL;DR: EvoEnv：一个动态评估环境，用于测试多模态大语言模型在真实世界部署中的鲁棒性，关注动态任务调度、主动探索和持续学习三个维度。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注静态环境下的性能上限，忽视了真实世界随机部署中的鲁棒性。需要评估模型在动态任务调度、不确定环境下的主动探索以及从经验中持续学习的能力。

Method: 提出EvoEnv动态评估环境，模拟"受训者"代理在陌生环境中持续探索。评估三个维度：1) 流式任务的情境感知调度；2) 通过主动探索减少幻觉的谨慎信息获取；3) 从基于规则的动态生成任务中提炼通用策略的持续演化。

Result: 实验表明，最先进的代理在动态环境中存在显著缺陷，特别是在主动探索和持续学习方面。EvoEnv为评估代理可靠性提供了框架。

Conclusion: 该工作将评估从静态测试转向现实的生产导向场景，建立了评估代理可靠性的框架，有助于推动多模态大语言模型在真实世界应用中的鲁棒性发展。

Abstract: The rapid evolution of Multi-modal Large Language Models (MLLMs) has advanced workflow automation; however, existing research mainly targets performance upper bounds in static environments, overlooking robustness for stochastic real-world deployment. We identify three key challenges: dynamic task scheduling, active exploration under uncertainty, and continuous learning from experience. To bridge this gap, we introduce \method{}, a dynamic evaluation environment that simulates a "trainee" agent continuously exploring a novel setting. Unlike traditional benchmarks, \method{} evaluates agents along three dimensions: (1) context-aware scheduling for streaming tasks with varying priorities; (2) prudent information acquisition to reduce hallucination via active exploration; and (3) continuous evolution by distilling generalized strategies from rule-based, dynamically generated tasks. Experiments show that cutting-edge agents have significant deficiencies in dynamic environments, especially in active exploration and continual learning. Our work establishes a framework for assessing agent reliability, shifting evaluation from static tests to realistic, production-oriented scenarios. Our codes are available at https://github.com/KnowledgeXLab/EvoEnv

</details>


### [113] [Improving LLM Reasoning with Homophily-aware Structural and Semantic Text-Attributed Graph Compression](https://arxiv.org/abs/2601.08187)
*Zijun Di,Bin Lu,Huquan Kang,Luoyi Fu,Jiaxin Ding,Xiaoying Gan,Lei Zhou,Xinbing Wang,Chenghu Zhou*

Main category: cs.AI

TL;DR: HS2C框架利用图同质性进行结构和语义压缩，通过层次分区去除噪声，将冗余背景压缩为社区级共识，提升LLMs在图理解任务中的推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法受限于上下文窗口，通常采用随机采样丢弃节点/边，这会引入噪声并导致推理不稳定。图本身包含丰富的结构和语义信息，有效利用这些信息可以提升LLMs的推理性能。

Method: 提出HS2C框架：1) 结构上，基于结构熵最小化原则进行全局层次分区，识别自然凝聚的同质性社区，去除随机连接噪声；2) 语义上，将检测到的结构同质性传递给LLM，使其能够基于预定义的社区类型进行差异化的语义聚合，将冗余背景压缩为简洁的社区级共识。

Result: 在10个节点级基准测试中，HS2C通过向LLMs提供结构和语义压缩的输入，同时提高了压缩率和下游推理准确率。在7个图级基准测试上的扩展进一步验证了任务泛化能力。

Conclusion: HS2C框架通过利用图同质性进行结构和语义压缩，有效提升了LLMs在图理解任务中的性能，证明了其优越性和可扩展性。

Abstract: Large language models (LLMs) have demonstrated promising capabilities in Text-Attributed Graph (TAG) understanding. Recent studies typically focus on verbalizing the graph structures via handcrafted prompts, feeding the target node and its neighborhood context into LLMs. However, constrained by the context window, existing methods mainly resort to random sampling, often implemented via dropping node/edge randomly, which inevitably introduces noise and cause reasoning instability. We argue that graphs inherently contain rich structural and semantic information, and that their effective exploitation can unlock potential gains in LLMs reasoning performance. To this end, we propose Homophily-aware Structural and Semantic Compression for LLMs (HS2C), a framework centered on exploiting graph homophily. Structurally, guided by the principle of Structural Entropy minimization, we perform a global hierarchical partition that decodes the graph's essential topology. This partition identifies naturally cohesive, homophilic communities, while discarding stochastic connectivity noise. Semantically, we deliver the detected structural homophily to the LLM, empowering it to perform differentiated semantic aggregation based on predefined community type. This process compresses redundant background contexts into concise community-level consensus, selectively preserving semantically homophilic information aligned with the target nodes. Extensive experiments on 10 node-level benchmarks across LLMs of varying sizes and families demonstrate that, by feeding LLMs with structurally and semantically compressed inputs, HS2C simultaneously enhances the compression rate and downstream inference accuracy, validating its superiority and scalability. Extensions to 7 diverse graph-level benchmarks further consolidate HS2C's task generalizability.

</details>


### [114] [Adapting Rules of Official International Mahjong for Online Players](https://arxiv.org/abs/2601.08211)
*Chucai Wang,Lingfeng Li,Yunlong Lu,Wenxin Li*

Main category: cs.AI

TL;DR: 使用世界冠军AI进行自对弈分析，发现国际麻将在线单局游戏存在先手优势和子目标计分问题，提出补偿分机制和计分规则调整，使传统游戏更适合在线环境。


<details>
  <summary>Details</summary>
Motivation: 在线麻将玩家与线下玩家不同，具有碎片化游戏时间和不固定的对手组合，传统为线下多轮固定对手设计的规则需要调整以确保在线单局游戏的公平性。

Method: 使用世界冠军AI进行自对弈比赛，通过统计分析揭示游戏平衡问题，包括先手优势和子目标计分设置的问题。

Result: 研究发现存在明显的先手优势，子目标计分设置存在问题。提出了针对先手优势的补偿分机制，以及针对不同牌型的子目标计分细化调整。

Conclusion: 通过AI系统数据分析评估国际麻将的游戏平衡性，开发了更适合在线玩家的修订版本，补偿分机制比传统多轮换位方法更便捷，已实现在线版本供玩家使用。

Abstract: As one of the worldwide spread traditional game, Official International Mahjong can be played and promoted online through remote devices instead of requiring face-to-face interaction. However, online players have fragmented playtime and unfixed combination of opponents in contrary to offline players who have fixed opponents for multiple rounds of play. Therefore, the rules designed for offline players need to be modified to ensure the fairness of online single-round play. Specifically, We employ a world champion AI to engage in self-play competitions and conduct statistical data analysis. Our study reveals the first-mover advantage and issues in the subgoal scoring settings. Based on our findings, we propose rule adaptations to make the game more suitable for the online environment, such as introducing compensatory points for the first-mover advantage and refining the scores of subgoals for different tile patterns. Compared with the traditional method of rotating positions over multiple rounds to balance first-mover advantage, our compensatory points mechanism in each round is more convenient for online players. Furthermore, we implement the revised Mahjong game online, which is open for online players. This work is an initial attempt to use data from AI systems to evaluate Official Internatinoal Mahjong's game balance and develop a revised version of the traditional game better adapted for online players.

</details>


### [115] [Large Artificial Intelligence Model Guided Deep Reinforcement Learning for Resource Allocation in Non Terrestrial Networks](https://arxiv.org/abs/2601.08254)
*Abdikarim Mohamed Ibrahim,Rosdiadee Nordin*

Main category: cs.AI

TL;DR: 提出了一种由大型语言模型引导的深度强化学习方法，用于非地面网络优化，在吞吐量、公平性和中断概率方面显著优于传统DRL和启发式方法


<details>
  <summary>Details</summary>
Motivation: 大型AI模型在非地面网络应用中表现出色，具有更好的泛化能力和更少的任务特定训练需求。然而，传统DRL方法在复杂网络环境中仍有改进空间，需要更智能的引导机制。

Method: 提出LAM-DRL框架，使用大型语言模型作为高层协调器，生成文本指导来塑造DRL代理在训练过程中的奖励函数。LLM提供高级策略指导，DRL负责具体决策执行。

Result: 在正常天气场景下，LAM-DRL比传统DRL性能提升40%；在极端天气场景下，性能提升64%（相比启发式方法）。在吞吐量、公平性和中断概率等指标上均表现优异。

Conclusion: LLM引导的DRL方法在非地面网络中具有显著优势，能够有效处理复杂动态环境，为网络优化提供了新的有效解决方案。

Abstract: Large AI Model (LAM) have been proposed to applications of Non-Terrestrial Networks (NTN), that offer better performance with its great generalization and reduced task specific trainings. In this paper, we propose a Deep Reinforcement Learning (DRL) agent that is guided by a Large Language Model (LLM). The LLM operates as a high level coordinator that generates textual guidance that shape the reward of the DRL agent during training. The results show that the LAM-DRL outperforms the traditional DRL by 40% in nominal weather scenarios and 64% in extreme weather scenarios compared to heuristics in terms of throughput, fairness, and outage probability.

</details>


### [116] [An Axiomatic Approach to General Intelligence: SANC(E3) -- Self-organizing Active Network of Concepts with Energy E3](https://arxiv.org/abs/2601.08224)
*Daesuk Kwon,Won-gi Paeng*

Main category: cs.AI

TL;DR: SANC(E3)是一个理论框架，提出表征单元不是预设的，而是在有限激活容量下通过竞争选择、重建和压缩的稳定结果，统一了感知、想象、预测、规划和行动。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统预设固定的基本单元（如token、像素），回避了表征单元如何自发涌现和稳定的核心问题。需要建立理论框架解释智能系统如何从经验中自组织出表征结构。

Method: 提出SANC(E3)公理化框架，基于五个核心公理：有限容量、共现关联、相似性竞争、置信度稳定、重建-压缩-更新权衡。引入伪内存映射I/O机制，使内部回放的完形与外部感官输入通过相同公理路径处理。

Result: 从公理推导出12个命题，表明类别形成、层次组织、无监督学习和高级认知活动都可以理解为E3最小化下的完形补全过程。实现了感知、想象、预测、规划和行动的统一表征和能量过程。

Conclusion: SANC(E3)为理解智能如何从经验中自组织出表征结构提供了理论基础，挑战了预设表征单元的传统范式，为构建更接近生物智能的AI系统提供了新方向。

Abstract: General intelligence must reorganize experience into internal structures that enable prediction and action under finite resources. Existing systems implicitly presuppose fixed primitive units -- tokens, subwords, pixels, or predefined sensor channels -- thereby bypassing the question of how representational units themselves emerge and stabilize. This paper proposes SANC(E3), an axiomatic framework in which representational units are not given a priori but instead arise as stable outcomes of competitive selection, reconstruction, and compression under finite activation capacity, governed by the explicit minimization of an energy functional E3. SANC(E3) draws a principled distinction between system tokens -- structural anchors such as {here, now, I} and sensory sources -- and tokens that emerge through self-organization during co-occurring events. Five core axioms formalize finite capacity, association from co-occurrence, similarity-based competition, confidence-based stabilization, and the reconstruction-compression-update trade-off. A key feature is a pseudo-memory-mapped I/O mechanism, through which internally replayed Gestalts are processed via the same axiomatic pathway as external sensory input. As a result, perception, imagination, prediction, planning, and action are unified within a single representational and energetic process. From the axioms, twelve propositions are derived, showing that category formation, hierarchical organization, unsupervised learning, and high-level cognitive activities can all be understood as instances of Gestalt completion under E3 minimization.

</details>


### [117] [MPCI-Bench: A Benchmark for Multimodal Pairwise Contextual Integrity Evaluation of Language Model Agents](https://arxiv.org/abs/2601.08235)
*Shouju Wang,Haopeng Zhang*

Main category: cs.AI

TL;DR: MPCI-Bench是首个多模态成对上下文完整性基准，用于评估智能体隐私行为，包含三个层级：规范种子判断、上下文丰富的故事推理和可执行智能体行为轨迹，揭示了当前多模态模型在平衡隐私与效用方面的系统性失败。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型智能体从被动聊天机器人演变为处理个人数据的主动助手，评估其遵守社会规范变得日益重要。现有CI基准主要是文本中心且侧重于负面拒绝场景，忽略了多模态隐私风险和隐私与效用的基本权衡。

Method: 引入MPCI-Bench基准，包含从相同视觉源派生的正负配对实例，分为三个层级：规范种子判断、上下文丰富的故事推理和可执行智能体行为轨迹。通过三原则迭代精炼流程确保数据质量。

Result: 评估最先进的多模态模型显示：1）系统性地无法平衡隐私与效用；2）存在明显的模态泄漏差距，敏感视觉信息比文本信息泄漏更频繁。

Conclusion: MPCI-Bench填补了多模态隐私评估的空白，揭示了当前智能体在上下文完整性方面的缺陷，将为未来智能体CI研究提供开源基准。

Abstract: As language-model agents evolve from passive chatbots into proactive assistants that handle personal data, evaluating their adherence to social norms becomes increasingly critical, often through the lens of Contextual Integrity (CI). However, existing CI benchmarks are largely text-centric and primarily emphasize negative refusal scenarios, overlooking multimodal privacy risks and the fundamental trade-off between privacy and utility. In this paper, we introduce MPCI-Bench, the first Multimodal Pairwise Contextual Integrity benchmark for evaluating privacy behavior in agentic settings. MPCI-Bench consists of paired positive and negative instances derived from the same visual source and instantiated across three tiers: normative Seed judgments, context-rich Story reasoning, and executable agent action Traces. Data quality is ensured through a Tri-Principle Iterative Refinement pipeline. Evaluations of state-of-the-art multimodal models reveal systematic failures to balance privacy and utility and a pronounced modality leakage gap, where sensitive visual information is leaked more frequently than textual information. We will open-source MPCI-Bench to facilitate future research on agentic CI.

</details>


### [118] [The End of Reward Engineering: How LLMs Are Redefining Multi-Agent Coordination](https://arxiv.org/abs/2601.08237)
*Haoran Su,Yandong Sun,Congjia Yu*

Main category: cs.AI

TL;DR: 论文提出利用大语言模型从语言描述自动生成奖励函数，替代传统人工设计的数值奖励，以解决多智能体强化学习中的奖励工程难题。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习中，手动设计奖励函数面临信用分配模糊、环境非平稳性和交互复杂性指数增长等挑战。传统数值奖励工程困难且难以与人类意图对齐。

Method: 利用大语言模型从自然语言描述直接合成奖励函数（如EUREKA），并在线适应奖励形式（如CARD）。采用基于可验证奖励的强化学习范式，通过语言介导的监督替代传统奖励工程。

Result: 语言介导的监督可作为传统奖励工程的可行替代方案。这种转变沿着三个维度：语义奖励规范、动态奖励适应和与人类意图的更好对齐。

Conclusion: 未来研究方向是从共享语义表示中产生协调，而非显式设计的数值信号。但仍面临计算开销、幻觉鲁棒性和大规模多智能体系统可扩展性等开放挑战。

Abstract: Reward engineering, the manual specification of reward functions to induce desired agent behavior, remains a fundamental challenge in multi-agent reinforcement learning. This difficulty is amplified by credit assignment ambiguity, environmental non-stationarity, and the combinatorial growth of interaction complexity. We argue that recent advances in large language models (LLMs) point toward a shift from hand-crafted numerical rewards to language-based objective specifications. Prior work has shown that LLMs can synthesize reward functions directly from natural language descriptions (e.g., EUREKA) and adapt reward formulations online with minimal human intervention (e.g., CARD). In parallel, the emerging paradigm of Reinforcement Learning from Verifiable Rewards (RLVR) provides empirical evidence that language-mediated supervision can serve as a viable alternative to traditional reward engineering. We conceptualize this transition along three dimensions: semantic reward specification, dynamic reward adaptation, and improved alignment with human intent, while noting open challenges related to computational overhead, robustness to hallucination, and scalability to large multi-agent systems. We conclude by outlining a research direction in which coordination arises from shared semantic representations rather than explicitly engineered numerical signals.

</details>


### [119] [T3: Benchmarking Sycophancy and Skepticism in Causal Judgment](https://arxiv.org/abs/2601.08258)
*Edward Y. Chang*

Main category: cs.AI

TL;DR: T3是一个诊断基准，用于评估LLM在因果阶梯上的因果判断能力，发现前沿模型存在"怀疑陷阱"和"规模悖论"等病理现象。


<details>
  <summary>Details</summary>
Motivation: 需要严格评估LLM在Pearl因果阶梯上的因果判断能力，诊断模型在因果推理中的具体失败模式，为改进提供指导。

Method: 开发T3基准，包含454个专家策划的案例，将性能分解为效用（敏感性）、安全性（特异性）和明智拒绝（不确定情况）。应用于前沿模型，分析失败模式，并验证过程验证协议（RCA）。

Result: 发现两种病理：L1层的"怀疑陷阱"（安全调优模型拒绝60%有效链接）和L3层的非单调"规模悖论"（更大模型在模糊反事实上表现更差，因过度犹豫而非幻觉）。验证了RCA协议能恢复决定性因果判断。

Conclusion: T3基准能有效诊断LLM因果判断的病理，揭示了安全调优和规模扩展中的意外后果，为改进因果推理能力提供了有价值的诊断工具。

Abstract: We introduce T3 (Testing Trustworthy Thinking), a diagnostic benchmark designed to rigorously evaluate LLM causal judgment across Pearl's Ladder of Causality. Comprising 454 expert-curated vignettes, T3 prioritizes high-resolution failure analysis, decomposing performance into Utility (sensitivity), Safety (specificity), and Wise Refusal on underdetermined cases. By applying T3 to frontier models, we diagnose two distinct pathologies: a "Skepticism Trap" at L1 (where safety-tuned models like Claude Haiku reject 60% of valid links) and a non-monotonic Scaling Paradox at L3. In the latter, the larger GPT-5.2 underperforms GPT-4-Turbo by 55 points on ambiguous counterfactuals, driven by a collapse into paralysis (excessive hedging) rather than hallucination. Finally, we use the benchmark to validate a process-verified protocol (RCA), showing that T3 successfully captures the restoration of decisive causal judgment under structured verification.

</details>


### [120] [VGG Induced Deep Hand Sign Language Detection](https://arxiv.org/abs/2601.08262)
*Subham Sharma,Sharmila Subudhi*

Main category: cs.AI

TL;DR: 提出基于VGG-16卷积神经网络的手势识别系统，通过迁移学习和数据增强在NUS数据集上达到约98%准确率


<details>
  <summary>Details</summary>
Motivation: 手势识别是人机交互的重要方面，特别是为残障人士提供手语识别基础，帮助视障人群进行交流

Method: 使用VGG-16卷积神经网络构建训练模型，采用Python和Keras库，结合迁移学习机制和图像数据增强技术，在NUS数据集（10类手势）上训练和验证

Result: 通过迁移学习和数据增强的组合，VGG-16网络在测试集上达到约98%的准确率，验证了模型的有效性

Conclusion: 提出的手势识别系统能够有效识别不同手势，为残障人士提供实用的人机交互解决方案，证明了深度学习方法在手势识别领域的有效性

Abstract: Hand gesture recognition is an important aspect of human-computer interaction. It forms the basis of sign language for the visually impaired people. This work proposes a novel hand gesture recognizing system for the differently-abled persons. The model uses a convolutional neural network, known as VGG-16 net, for building a trained model on a widely used image dataset by employing Python and Keras libraries. Furthermore, the result is validated by the NUS dataset, consisting of 10 classes of hand gestures, fed to the model as the validation set. Afterwards, a testing dataset of 10 classes is built by employing Google's open source Application Programming Interface (API) that captures different gestures of human hand and the efficacy is then measured by carrying out experiments. The experimental results show that by combining a transfer learning mechanism together with the image data augmentation, the VGG-16 net produced around 98% accuracy.

</details>


### [121] [Sparsity Is Necessary: Polynomial-Time Stability for Agentic LLMs in Large Action Spaces](https://arxiv.org/abs/2601.08271)
*Angshul Majumdar*

Main category: cs.AI

TL;DR: 论文提出稀疏智能控制(SAC)框架，分析工具增强型LLM系统的学习理论，证明在稀疏动作空间下可实现对数级样本复杂度，解释纯提示控制的不稳定性。


<details>
  <summary>Details</summary>
Motivation: 工具增强型LLM系统面临大规模离散动作空间（工具、API、文档）的序列决策问题，传统学习理论对此研究不足。需要形式化分析这种稀疏动作空间下的策略学习。

Method: 将问题形式化为稀疏智能控制(SAC)，使用ℓ₁,₂正则化策略学习，通过凸代理方法建立压缩感知风格的理论结果，包括策略RSC条件、原始-对偶见证论证等。

Result: 获得压缩感知风格的理论保证：估计误差和值次优性按k(log M/T)^{1/2}缩放；在T > k log M条件下可实现精确工具支持恢复；证明密集策略类需要Ω(M)样本。

Conclusion: SAC框架为工具增强型LLM系统提供理论基础，证明稀疏表示可实现对数级样本复杂度，解释纯提示控制的不稳定性，并为部分可观测性等扩展提供理论支撑。

Abstract: Tool-augmented LLM systems expose a control regime that learning theory has largely ignored: sequential decision-making with a massive discrete action universe (tools, APIs, documents) in which only a small, unknown subset is relevant for any fixed task distribution. We formalize this setting as Sparse Agentic Control (SAC), where policies admit block-sparse representations over M >> 1 actions and rewards depend on sparse main effects and (optionally) sparse synergies. We study ell_{1,2}-regularized policy learning through a convex surrogate and establish sharp, compressed-sensing-style results: (i) estimation and value suboptimality scale as k (log M / T)^{1/2} under a Policy-RSC condition; (ii) exact tool-support recovery holds via primal-dual witness arguments when T > k log M under incoherence and beta-min; and (iii) any dense policy class requires Omega(M) samples, explaining the instability of prompt-only controllers. We further show that under partial observability, LLMs matter only through a belief/representation error epsilon_b, yielding an additive O(epsilon_b) degradation while preserving logarithmic dependence on M. Extensions cover tuning-free, online, robust, group-sparse, and interaction-aware SAC.

</details>


### [122] [ToolACE-MCP: Generalizing History-Aware Routing from MCP Tools to the Agent Web](https://arxiv.org/abs/2601.08276)
*Zhiyuan Yao,Zishan Xu,Yifu Guo,Zhiguang Han,Cheng Yang,Shuo Zhang,Weinan Zhang,Xingshan Zeng,Weiwen Liu*

Main category: cs.AI

TL;DR: ToolACE-MCP：一个用于大规模工具生态系统的历史感知路由训练框架，通过依赖图合成多轮轨迹训练路由器，实现精确导航和可扩展的轻量路由代理。


<details>
  <summary>Details</summary>
Motivation: 随着Agent Web和MCP的发展，工具生态系统演变为开放协作网络，可访问工具数量指数增长，但现有架构面临严重的可扩展性和通用性瓶颈。

Method: 提出ToolACE-MCP管道，通过依赖丰富的候选图合成多轮轨迹，训练具有动态上下文理解能力的历史感知路由器，创建即插即用的轻量路由代理。

Result: 在真实世界基准测试MCP-Universe和MCP-Mark上表现出优越性能。ToolACE-MCP展现出多智能体协作的泛化能力、对噪声的鲁棒性，以及在大规模候选空间中的有效扩展性。

Conclusion: 为开放生态系统中的通用编排提供了强有力的实证基础，ToolACE-MCP具备未来Agent Web所需的关键特性。

Abstract: With the rise of the Agent Web and Model Context Protocol (MCP), the agent ecosystem is evolving into an open collaborative network, exponentially increasing accessible tools. However, current architectures face severe scalability and generality bottlenecks. To address this, we propose ToolACE-MCP, a pipeline for training history-aware routers to empower precise navigation in large-scale ecosystems. By leveraging a dependency-rich candidate Graph to synthesize multi-turn trajectories, we effectively train routers with dynamic context understanding to create the plug-and-play Light Routing Agent. Experiments on the real-world benchmarks MCP-Universe and MCP-Mark demonstrate superior performance. Notably, ToolACE-MCP exhibits critical properties for the future Agent Web: it not only generalizes to multi-agent collaboration with minimal adaptation but also maintains exceptional robustness against noise and scales effectively to massive candidate spaces. These findings provide a strong empirical foundation for universal orchestration in open-ended ecosystems.

</details>


### [123] [Greedy Is Enough: Sparse Action Discovery in Agentic LLMs](https://arxiv.org/abs/2601.08280)
*Angshul Majumdar*

Main category: cs.AI

TL;DR: 论文研究大规模动作空间中的智能体决策问题，提出基于结构化稀疏假设的动作发现算法，证明在稀疏性和充分覆盖条件下，贪婪算法能以对数级样本复杂度准确发现相关动作集。


<details>
  <summary>Details</summary>
Motivation: 现代智能体系统（如工具增强语言模型）面临极大动作空间（数千个API），但经验表明只有少数动作对性能有实际影响。这启发了对稀疏动作发现的理论研究。

Method: 采用上下文线性奖励模型，假设动作相关性遵循结构化稀疏性：只有少量动作在潜在状态中具有非零效应。将动作发现建模为块稀疏恢复问题，分析基于正交匹配追踪的贪婪算法。

Result: 在不相干性、信号强度和动作覆盖的标准假设下，证明贪婪算法能以高概率精确恢复相关动作集，样本复杂度与稀疏度和潜在维度呈多项式关系，与总动作数呈对数关系。同时提供参数估计误差保证和决策规则近似最优性证明。

Conclusion: 稀疏动作发现是大规模动作决策的基本原理，为智能体系统中的动作剪枝提供了理论基础。信息论下界表明稀疏性和充分覆盖是问题可处理性的必要条件。

Abstract: Modern agentic systems operate in environments with extremely large action spaces, such as tool-augmented language models with thousands of available APIs or retrieval operations. Despite this scale, empirical evidence suggests that only a small subset of actions meaningfully influences performance in a given deployment. Motivated by this observation, we study a contextual linear reward model in which action relevance is governed by a structured sparsity assumption: only a small number of actions have nonzero effects across latent states.
  We formulate action discovery as a block-sparse recovery problem and analyze a greedy algorithm inspired by Orthogonal Matching Pursuit. Under standard assumptions on incoherence, signal strength, and action coverage, we prove that the greedy procedure exactly recovers the relevant action set with high probability, using a number of samples that scales polynomially in the sparsity level and latent dimension, and only logarithmically in the total number of actions. We further provide estimation error guarantees for refitted parameters and show that the resulting decision rule is near-optimal for new latent states.
  Complementing these results, we establish information-theoretic lower bounds demonstrating that sparsity and sufficient coverage are necessary for tractability. Together, our results identify sparse action discovery as a fundamental principle underlying large-action decision-making and provide a theoretical foundation for action pruning in agentic systems.

</details>


### [124] [OpenMic: A Multi-Agent-Based Stand-Up Comedy Generation System](https://arxiv.org/abs/2601.08288)
*Yuyang Wu,Hanzhong Cao,Jianhao Chen,Yufei Li*

Main category: cs.AI

TL;DR: OpenMic是一个基于AutoGen的多智能体系统，能将用户提供的生活话题转化为3-5分钟的中文脱口秀表演并生成带旁白的喜剧视频，通过多轮迭代规划优化幽默、时机和表演性。


<details>
  <summary>Details</summary>
Motivation: 中文脱口秀生成需要文化背景的幽默、精确的时机把握、舞台表演提示和隐式的多步推理，而现有的中文幽默数据集更适合幽默理解和评估而非长篇脱口秀生成，导致直接监督与目标任务不匹配。

Method: 1. 构建基于AutoGen的多智能体系统，通过多轮迭代循环规划协同优化幽默、时机和表演性；2. 使用检索增强生成(RAG)进行素材基础和想法扩展；3. 微调专门的JokeWriter模型以更好地内化脱口秀特有的铺垫-笑点结构和长距离回调。

Result: 开发了OpenMic系统，能够将用户提供的生活话题转化为完整的3-5分钟中文脱口秀表演，并进一步生成带旁白的喜剧视频。

Conclusion: OpenMic通过多智能体协作、RAG增强和专门的JokeWriter微调，有效解决了中文脱口秀生成中的文化背景、时机把握和数据集不匹配等挑战，实现了端到端的脱口秀表演生成。

Abstract: Chinese stand-up comedy generation goes beyond plain text generation, requiring culturally grounded humor, precise timing, stage-performance cues, and implicit multi-step reasoning. Moreover, commonly used Chinese humor datasets are often better suited for humor understanding and evaluation than for long-form stand-up generation, making direct supervision misaligned with the target task. To address these challenges, we present OpenMic, an end-to-end multi-agent system built on AutoGen that transforms a user-provided life topic into a 3-5 minute Chinese stand-up performance and further produces a narrated comedy video. OpenMic orchestrates multiple specialized agents in a multi-round iterative loop-planning to jointly optimize humor, timing, and performability. To mitigate the dataset-task mismatch, we augment generation with retrieval-augmented generation (RAG) for material grounding and idea expansion, and we fine-tune a dedicated JokeWriter to better internalize stand-up-specific setup-punchline structures and long-range callbacks.

</details>


### [125] [AtomMem : Learnable Dynamic Agentic Memory with Atomic Memory Operation](https://arxiv.org/abs/2601.08323)
*Yupeng Huo,Yaxi Lu,Zhong Zhang,Haotian Chen,Yankai Lin*

Main category: cs.AI

TL;DR: AtomMem将记忆管理重构为动态决策问题，通过将高级记忆过程分解为原子CRUD操作，结合监督微调和强化学习训练自主记忆策略，在长上下文任务中超越静态工作流方法。


<details>
  <summary>Details</summary>
Motivation: 现有代理记忆机制依赖静态手工工作流，限制了性能和泛化能力，需要更灵活的学习型记忆框架来解决现实世界长时程问题。

Method: 将记忆管理重构为动态决策问题，将高级记忆过程分解为原子CRUD（创建、读取、更新、删除）操作，通过监督微调和强化学习训练自主记忆策略。

Result: 在3个长上下文基准测试中，训练后的AtomMem-8B始终优于先前的静态工作流记忆方法，分析显示学习型框架能让代理发现结构化、任务对齐的记忆管理策略。

Conclusion: AtomMem通过将记忆管理重构为可学习的决策过程，实现了比预定义工作流更灵活、任务对齐的记忆管理，为长时程问题解决提供了更有效的框架。

Abstract: Equipping agents with memory is essential for solving real-world long-horizon problems. However, most existing agent memory mechanisms rely on static and hand-crafted workflows. This limits the performance and generalization ability of these memory designs, which highlights the need for a more flexible, learning-based memory framework. In this paper, we propose AtomMem, which reframes memory management as a dynamic decision-making problem. We deconstruct high-level memory processes into fundamental atomic CRUD (Create, Read, Update, Delete) operations, transforming the memory workflow into a learnable decision process. By combining supervised fine-tuning with reinforcement learning, AtomMem learns an autonomous, task-aligned policy to orchestrate memory behaviors tailored to specific task demands. Experimental results across 3 long-context benchmarks demonstrate that the trained AtomMem-8B consistently outperforms prior static-workflow memory methods. Further analysis of training dynamics shows that our learning-based formulation enables the agent to discover structured, task-aligned memory management strategies, highlighting a key advantage over predefined routines.

</details>


### [126] [Semantic Laundering in AI Agent Architectures: Why Tool Boundaries Do Not Confer Epistemic Warrant](https://arxiv.org/abs/2601.08333)
*Oleg Romanchuk,Roman Bondar*

Main category: cs.AI

TL;DR: 论文指出LLM智能体架构存在"语义洗白"问题，将信息传输机制与认知证明机制混为一谈，导致缺乏充分依据的命题通过可信接口被系统接受，这构成了盖梯尔问题的架构实现。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体架构存在系统性缺陷，将信息传输机制与认知证明机制混为一谈，导致缺乏充分依据的命题被系统接受，这需要从架构层面进行理论分析。

Method: 通过形式化"语义洗白"概念，分析LLM智能体架构中的认知证明问题，提出"不可避免的自许可定理"和"依据侵蚀原理"，从类型层面揭示问题的结构性本质。

Result: 证明了在标准架构假设下，循环认知证明无法消除；揭示了扩展、模型改进和LLM作为评判方案在结构上无法解决类型层面的问题。

Conclusion: LLM智能体架构存在根本性的"语义洗白"问题，这是盖梯尔问题的架构实现，需要重新设计架构来分离信息传输与认知证明机制。

Abstract: LLM-based agent architectures systematically conflate information transport mechanisms with epistemic justification mechanisms. We formalize this class of architectural failures as semantic laundering: a pattern where propositions with absent or weak warrant are accepted by the system as admissible by crossing architecturally trusted interfaces. We show that semantic laundering constitutes an architectural realization of the Gettier problem: propositions acquire high epistemic status without a connection between their justification and what makes them true. Unlike classical Gettier cases, this effect is not accidental; it is architecturally determined and systematically reproducible. The central result is the Theorem of Inevitable Self-Licensing: under standard architectural assumptions, circular epistemic justification cannot be eliminated. We introduce the Warrant Erosion Principle as the fundamental explanation for this effect and show that scaling, model improvement, and LLM-as-judge schemes are structurally incapable of eliminating a problem that exists at the type level.

</details>


### [127] [Thematic Working Group 5 -- Artificial Intelligence (AI) literacy for teaching and learning: design and implementation](https://arxiv.org/abs/2601.08380)
*Mary Webb,Matt Bower,Ana Amélia Carvalho,Fredrik Mørk Røkenes,Jodie Torrington,Jonathan D. Cohen,Yousra Chtouki,Kathryn Maccallum,Tanya Linden,Deirdre Butler,Juliana Elisa Raffaghelli,Henriikka Vartiainen,Martina Ronci,Peter Tiernan,David M. Smith,Chris Shelton,Joyce Malyn-smith,Pierre Gorissen*

Main category: cs.AI

TL;DR: 工作组聚焦提升教师AI素养与能动性，通过课程设计、专业发展、课堂应用和政策指南等策略，赋能教师将AI融入教学实践


<details>
  <summary>Details</summary>
Motivation: 当前教师普遍缺乏AI素养和运用AI工具的能力，需要系统性的策略来提升教师在AI教育中的专业能力，使他们能够自信地将AI技术融入教学，并帮助学生深入理解AI概念

Method: 通过多维度探索：1）课程设计开发；2）专业发展项目；3）实际课堂应用；4）政策指南制定，形成综合性的教师AI能力提升策略体系

Result: 开发了系统的教师AI素养提升策略框架，涵盖了从课程到政策的多层次支持体系，为教师有效整合AI技术提供了实践路径

Conclusion: 通过综合性的策略设计，可以有效提升教师的AI素养和能动性，使教师成为AI教育的关键推动者，最终促进学生AI理解能力的提升

Abstract: TWG 5 focused on developing and implementing effective strategies for enhancing AI literacy and agency of teachers, equipping them with the knowledge and skills necessary to integrate AI into their teaching practices. Explorations covered curriculum design, professional development programs, practical classroom applications, and policy guidelines aiming to empower educators to confidently utilize AI tools and foster a deeper understanding of AI concepts among students.

</details>


### [128] [A Qualitative Model to Reason about Object Rotations (QOR) applied to solve the Cube Comparison Test (CCT)](https://arxiv.org/abs/2601.08382)
*Zoe Falomir*

Main category: cs.AI

TL;DR: 提出定性旋转推理模型(QOR)解决立方体比较测试(CCT)，通过构建旋转运动与立方体面特征位置和方向变化的概念邻域图(CNGRLO)来生成推理组合表


<details>
  <summary>Details</summary>
Motivation: 解决Ekstrom等人(1976)提出的立方体比较测试(CCT)问题，该测试涉及对旋转立方体的空间推理，需要一种能够处理物体旋转定性推理的模型

Method: 构建概念邻域图(CNGRLO)，将旋转运动与立方体面特征的位置变化和方向变化联系起来，并基于此图生成组合表来进行旋转推理计算

Result: 开发了QOR模型，能够通过概念邻域图和组合表有效推理立方体旋转，为CCT问题提供解决方案

Conclusion: QOR模型为物体旋转的定性推理提供了有效框架，通过概念邻域图和组合表的方法能够解决复杂的空间推理问题如CCT

Abstract: This paper presents a Qualitative model for Reasoning about Object Rotations (QOR) which is applied to solve the Cube Comparison Test (CCT) by Ekstrom et al. (1976). A conceptual neighborhood graph relating the Rotation movement to the Location change and the Orientation change (CNGRLO) of the features on the cube sides has been built and it produces composition tables to calculate inferences for reasoning about rotations.

</details>


### [129] [Deconstructing Pre-training: Knowledge Attribution Analysis in MoE and Dense Models](https://arxiv.org/abs/2601.08383)
*Bo Wang,Junzhuo Li,Hong Chen,Yuanlin Chu,Yuxuan Fan,Xuming Hu*

Main category: cs.AI

TL;DR: MoE架构通过稀疏性在训练早期形成稳定、分布式的知识存储结构，与密集模型的知识获取动态存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 研究MoE架构在预训练过程中如何塑造知识获取，以及这一过程与密集架构的差异，以理解稀疏架构的训练动态。

Method: 引入Gated-LPI（门控对数概率增加）神经元级归因指标，分解对数概率增加；对MoE和密集架构进行时间分辨比较，跟踪120万和60万训练步数的检查点。

Result: 发现三个关键模式：1）低熵骨干：前1%的MoE神经元捕获超过45%的正向更新；2）早期巩固：MoE在10万步内锁定稳定重要性分布；3）功能鲁棒性：掩码重要注意力头对MoE影响较小，显示分布式知识存储。

Conclusion: 稀疏性从训练早期就培养出内在稳定和分布式的计算骨干，有助于弥合稀疏架构与训练时可解释性之间的差距。

Abstract: Mixture-of-Experts (MoE) architectures decouple model capacity from per-token computation, enabling scaling beyond the computational limits imposed by dense scaling laws. Yet how MoE architectures shape knowledge acquisition during pre-training, and how this process differs from dense architectures, remains unknown. To address this issue, we introduce Gated-LPI (Log-Probability Increase), a neuron-level attribution metric that decomposes log-probability increase across neurons. We present a time-resolved comparison of knowledge acquisition dynamics in MoE and dense architectures, tracking checkpoints over 1.2M training steps (~ 5.0T tokens) and 600K training steps (~ 2.5T tokens), respectively. Our experiments uncover three patterns: (1) Low-entropy backbone. The top approximately 1% of MoE neurons capture over 45% of positive updates, forming a high-utility core, which is absent in the dense baseline. (2) Early consolidation. The MoE model locks into a stable importance profile within < 100K steps, whereas the dense model remains volatile throughout training. (3) Functional robustness. Masking the ten most important MoE attention heads reduces relational HIT@10 by < 10%, compared with > 50% for the dense model, showing that sparsity fosters distributed -- rather than brittle -- knowledge storage. These patterns collectively demonstrate that sparsity fosters an intrinsically stable and distributed computational backbone from early in training, helping bridge the gap between sparse architectures and training-time interpretability.

</details>


### [130] [Creativity in AI as Emergence from Domain-Limited Generative Models](https://arxiv.org/abs/2601.08388)
*Corina Chutaux*

Main category: cs.AI

TL;DR: 该论文提出从生成视角理解AI创造力，将其视为有限领域生成模型在受限信息环境中的涌现属性，而非事后评估标签。


<details>
  <summary>Details</summary>
Motivation: 现有AI创造力研究多采用评估框架（新颖性、多样性、有用性），将创造力视为待评估属性而非待建模现象。随着多模态生成系统展现复杂模式重组能力，需要从生成角度理解机器创造力的本质和限制。

Method: 提出生成视角的创造力框架，将其分解为四个交互组件：基于模式的生成、诱导世界模型、上下文基础、任意性。分析这些组件在多模态生成系统中的表现，研究生成动态与领域特定表征交互如何产生创造性行为。

Result: 建立了将创造力视为AI系统涌现现象的技术框架，强调结构性和上下文条件对创造性行为产生的影响，而非引入新的评估标准。

Conclusion: 创造力应被理解为生成动态与领域特定表征交互的涌现属性，这为研究AI系统中的创造力提供了新的技术框架，超越了传统的评估导向方法。

Abstract: Creativity in artificial intelligence is most often addressed through evaluative frameworks that aim to measure novelty, diversity, or usefulness in generated outputs. While such approaches have provided valuable insights into the behavior of modern generative models, they largely treat creativity as a property to be assessed rather than as a phenomenon to be explicitly modeled. In parallel, recent advances in large-scale generative systems, particularly multimodal architectures, have demonstrated increasingly sophisticated forms of pattern recombination, raising questions about the nature and limits of machine creativity. This paper proposes a generative perspective on creativity in AI, framing it as an emergent property of domain-limited generative models embedded within bounded informational environments. Rather than introducing new evaluative criteria, we focus on the structural and contextual conditions under which creative behaviors arise. We introduce a conceptual decomposition of creativity into four interacting components-pattern-based generation, induced world models, contextual grounding, and arbitrarity, and examine how these components manifest in multimodal generative systems. By grounding creativity in the interaction between generative dynamics and domain-specific representations, this work aims to provide a technical framework for studying creativity as an emergent phenomenon in AI systems, rather than as a post hoc evaluative label.

</details>


### [131] [Owen-Shapley Policy Optimization (OSPO): A Principled RL Algorithm for Generative Search LLMs](https://arxiv.org/abs/2601.08403)
*Abhijnan Nath,Alireza Bagheri Garakani,Tianchen Zhou,Fan Yang,Nikhil Krishnaswamy*

Main category: cs.AI

TL;DR: OSPO使用Shapley-Owen归因重新分配序列级奖励到语义单元，解决推荐系统中语言模型的信用分配问题，在未见过的检索器上表现出鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法（如GRPO）在个性化推荐任务中依赖稀疏的序列级奖励，导致信用分配问题，难以确定哪些token驱动成功。当模型需要从非指定语言推断潜在用户意图时，这个问题尤其严重。

Method: 提出Owen-Shapley Policy Optimization (OSPO)框架，基于token对结果的边际贡献重新分配序列级优势。使用基于势的奖励塑造，通过Shapley-Owen归因分配片段级信用，无需参数化价值模型。将语义连贯单元（描述产品属性的短语或捕捉偏好的句子）形成联盟来识别驱动性能的响应部分。

Result: 在Amazon ESCI和H&M Fashion数据集上的实验显示，OSPO相比基线方法获得一致提升，在训练期间未见过的分布外检索器上表现出显著的测试时鲁棒性。

Conclusion: OSPO通过重新分配序列级奖励到语义单元，有效解决了推荐系统中语言模型的信用分配问题，提高了模型性能和对未见检索器的鲁棒性。

Abstract: Large language models are increasingly trained via reinforcement learning for personalized recommendation tasks, but standard methods like GRPO rely on sparse, sequence-level rewards that create a credit assignment gap, obscuring which tokens drive success. This gap is especially problematic when models must infer latent user intent from under-specified language without ground truth labels, a reasoning pattern rarely seen during pretraining. We introduce Owen-Shapley Policy Optimization (OSPO), a framework that redistributes sequence-level advantages based on tokens' marginal contributions to outcomes. Unlike value-model-based methods requiring additional computation, OSPO employs potential-based reward shaping via Shapley-Owen attributions to assign segment-level credit while preserving the optimal policy, learning directly from task feedback without parametric value models. By forming coalitions of semantically coherent units (phrases describing product attributes or sentences capturing preferences), OSPO identifies which response parts drive performance. Experiments on Amazon ESCI and H&M Fashion datasets show consistent gains over baselines, with notable test-time robustness to out-of-distribution retrievers unseen during training.

</details>


### [132] [WebTrap Park: An Automated Platform for Systematic Security Evaluation of Web Agents](https://arxiv.org/abs/2601.08406)
*Xinyi Wu,Jiagui Chen,Geng Hong,Jiayi Dong,Xudong Pan,Jiarun Dai,Min Yang*

Main category: cs.AI

TL;DR: WebTrap Park是一个自动化平台，用于通过直接观察Web代理与实时网页的交互来系统评估其安全性，包含1,226个可执行评估任务，无需修改代理即可进行基于动作的评估。


<details>
  <summary>Details</summary>
Motivation: 当前Web代理在真实网络环境中执行复杂任务时，其安全性评估仍然分散且难以标准化，缺乏系统化的评估平台。

Method: 开发WebTrap Park平台，将三大安全风险来源实例化为1,226个可执行评估任务，通过直接观察代理与实时网页的具体交互进行自动化评估，无需修改代理架构。

Result: 评估结果显示不同代理框架之间存在明显的安全性差异，强调了代理架构（而不仅仅是底层模型）对安全性的重要性。

Conclusion: WebTrap Park为可复现的Web代理安全性评估提供了可扩展的基础，平台已公开访问，有助于推动Web代理安全性的标准化评估。

Abstract: Web Agents are increasingly deployed to perform complex tasks in real web environments, yet their security evaluation remains fragmented and difficult to standardize. We present WebTrap Park, an automated platform for systematic security evaluation of Web Agents through direct observation of their concrete interactions with live web pages. WebTrap Park instantiates three major sources of security risk into 1,226 executable evaluation tasks and enables action based assessment without requiring agent modification. Our results reveal clear security differences across agent frameworks, highlighting the importance of agent architecture beyond the underlying model. WebTrap Park is publicly accessible at https://security.fudan.edu.cn/webagent and provides a scalable foundation for reproducible Web Agent security evaluation.

</details>


### [133] [Hybrid Distillation with CoT Guidance for Edge-Drone Control Code Generation](https://arxiv.org/abs/2601.08412)
*Yizhan Feng,Hichem Snoussi,Yuhang Wang,Jing Teng,Abel Cherouat,Tian Wang*

Main category: cs.AI

TL;DR: 该论文提出了一种结合知识蒸馏、思维链引导和监督微调的方法，将大型语言模型的代码生成能力高效迁移到小型模型，用于无人机多SDK控制任务，在保持高准确率的同时显著提升部署和推理效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码生成方面潜力巨大，但高资源消耗与无人机平台的实时轻量需求存在矛盾。需要解决如何在资源受限的无人机上实现精确智能控制的问题。

Method: 1) 构建包含指令-代码-推理链的高质量无人机SDK数据集，加入反事实负样本进行数据增强；2) 使用DeepSeek-Coder-V2-Lite作为教师模型，采用混合黑盒白盒蒸馏策略生成思维链软标签，结合硬标签的加权交叉熵损失；3) 针对无人机控制场景进行提示调优工程，提升SDK类型识别和函数调用匹配等核心任务性能。

Result: 蒸馏后的轻量模型在保持高代码生成准确率的同时，实现了部署和推理效率的显著提升，有效证明了该方法在实现无人机精确轻量智能控制方面的可行性和优越性。

Conclusion: 该集成方法成功解决了大型语言模型与无人机平台资源约束之间的矛盾，通过知识蒸馏等技术将复杂推理和代码生成能力高效迁移到小型模型，为资源受限的无人机平台提供了可行的智能控制解决方案。

Abstract: With large language models demonstrating significant potential in code generation tasks, their application to onboard control of resource-constrained Unmanned Aerial Vehicles has emerged as an important research direction. However, a notable contradiction exists between the high resource consumption of large models and the real-time, lightweight requirements of UAV platforms. This paper proposes an integrated approach that combines knowledge distillation, chain-of-thought guidance, and supervised fine-tuning for UAV multi-SDK control tasks, aiming to efficiently transfer complex reasoning and code generation capabilities to smaller models. Firstly, a high-quality dataset covering various mainstream UAV SDKs is constructed, featuring instruction-code-reasoning chains, and incorporates counterfactual negative samples for data augmentation, guiding the model to learn the end-to-end logic from instruction parsing to code generation. Secondly, leveraging DeepSeek-Coder-V2-Lite quantized via QLoRA as the teacher model, and based on a hybrid black-box and white-box distillation strategy, high-quality chain-of-thought soft labels are generated. These are combined with a weighted cross-entropy loss using hard labels to transfer complex reasoning capabilities to the smaller student model. Finally, through prompt tuning engineering optimized for the UAV control scenario, the model performance on core tasks such as SDK type recognition and function call matching is enhanced. Experimental results indicate that the distilled lightweight model maintains high code generation accuracy while achieving significant improvements in deployment and inference efficiency, effectively demonstrating the feasibility and superiority of our approach in achieving precise and lightweight intelligent control for UAVs

</details>


### [134] [RubricHub: A Comprehensive and Highly Discriminative Rubric Dataset via Automated Coarse-to-Fine Generation](https://arxiv.org/abs/2601.08430)
*Sunzhu Li,Jiale Zhao,Miteto Wei,Huimin Ren,Yang Zhou,Jingwen Yang,Shunyu Liu,Kaike Zhang,Wei Chen*

Main category: cs.AI

TL;DR: 提出Coarse-to-Fine Rubric Generation框架和RubricHub数据集，通过两阶段后训练显著提升开放生成任务性能


<details>
  <summary>Details</summary>
Motivation: 强化学习在可验证奖励领域取得进展，但开放生成任务缺乏ground truth。基于规则的评估存在可扩展性瓶颈和标准粗糙问题，导致监督天花板效应

Method: 提出自动化Coarse-to-Fine Rubric Generation框架，结合原则引导合成、多模型聚合和难度演化；构建RubricHub大规模多领域数据集；采用两阶段后训练：基于规则的拒绝采样微调(RuFT)和强化学习(RuRL)

Result: 后训练的Qwen3-14B在HealthBench上达到69.3分，超越GPT-5等前沿专有模型，实现SOTA性能

Conclusion: RubricHub框架有效解决了开放生成任务的评估瓶颈，通过精细化规则生成和两阶段训练显著提升模型性能，为开放生成任务提供了可扩展的解决方案

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has driven substantial progress in reasoning-intensive domains like mathematics. However, optimizing open-ended generation remains challenging due to the lack of ground truth. While rubric-based evaluation offers a structured proxy for verification, existing methods suffer from scalability bottlenecks and coarse criteria, resulting in a supervision ceiling effect. To address this, we propose an automated Coarse-to-Fine Rubric Generation framework. By synergizing principle-guided synthesis, multi-model aggregation, and difficulty evolution, our approach produces comprehensive and highly discriminative criteria capable of capturing the subtle nuances. Based on this framework, we introduce RubricHub, a large-scale ($\sim$110k) and multi-domain dataset. We validate its utility through a two-stage post-training pipeline comprising Rubric-based Rejection Sampling Fine-Tuning (RuFT) and Reinforcement Learning (RuRL). Experimental results demonstrate that RubricHub unlocks significant performance gains: our post-trained Qwen3-14B achieves state-of-the-art (SOTA) results on HealthBench (69.3), surpassing proprietary frontier models such as GPT-5. The code and data will be released soon.

</details>


### [135] [YaPO: Learnable Sparse Activation Steering Vectors for Domain Adaptation](https://arxiv.org/abs/2601.08441)
*Abdelaziz Bounhar,Rania Hossam Elmohamady Elbadry,Hadi Abdine,Preslav Nakov,Michalis Vazirgiannis,Guokan Shang*

Main category: cs.AI

TL;DR: YaPO提出了一种基于稀疏自编码器的稀疏导向向量学习方法，用于大语言模型的细粒度对齐控制，相比密集导向向量方法具有更好的解耦性、可解释性和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有基于密集导向向量的激活干预方法（如BiPO）由于神经元多语义性导致多个潜在因素纠缠，限制了在细粒度设置（如文化对齐）中的效果和稳定性，难以区分密切相关的价值观和行为。

Method: YaPO是一种无参考方法，在稀疏自编码器的潜在空间中学习稀疏导向向量。通过优化稀疏编码，产生解耦、可解释且高效的导向方向。

Result: YaPO比密集导向基线收敛更快、性能更强、训练稳定性更好。除了文化对齐，还能泛化到幻觉、财富追求、越狱、权力追求等多种对齐相关行为，且保持MMLU通用知识不退化。

Conclusion: YaPO为大语言模型提供了一种高效、稳定、细粒度的对齐通用方法，在可控性和领域适应方面具有广泛应用前景。

Abstract: Steering Large Language Models (LLMs) through activation interventions has emerged as a lightweight alternative to fine-tuning for alignment and personalization. Recent work on Bi-directional Preference Optimization (BiPO) shows that dense steering vectors can be learned directly from preference data in a Direct Preference Optimization (DPO) fashion, enabling control over truthfulness, hallucinations, and safety behaviors. However, dense steering vectors often entangle multiple latent factors due to neuron multi-semanticity, limiting their effectiveness and stability in fine-grained settings such as cultural alignment, where closely related values and behaviors (e.g., among Middle Eastern cultures) must be distinguished. In this paper, we propose Yet another Policy Optimization (YaPO), a \textit{reference-free} method that learns \textit{sparse steering vectors} in the latent space of a Sparse Autoencoder (SAE). By optimizing sparse codes, YaPO produces disentangled, interpretable, and efficient steering directions. Empirically, we show that YaPO converges faster, achieves stronger performance, and exhibits improved training stability compared to dense steering baselines. Beyond cultural alignment, YaPO generalizes to a range of alignment-related behaviors, including hallucination, wealth-seeking, jailbreak, and power-seeking. Importantly, YaPO preserves general knowledge, with no measurable degradation on MMLU. Overall, our results show that YaPO provides a general recipe for efficient, stable, and fine-grained alignment of LLMs, with broad applications to controllability and domain adaptation. The associated code and data are publicly available\footnote{https://github.com/MBZUAI-Paris/YaPO}.

</details>


### [136] [Beyond Linearization: Attributed Table Graphs for Table Reasoning](https://arxiv.org/abs/2601.08444)
*Yuxiang Wang,Junhao Gan,Shengxiang Gao,Shenghao Ye,Zhengyi Yang,Jianzhong Qi*

Main category: cs.AI

TL;DR: TABGR：一种基于图表示的免训练表格推理模型，通过属性表图保持表格结构，使用个性化PageRank缓解信息丢失问题，在基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的表格推理方法存在三个关键问题：1）线性化表格会丢失结构信息；2）缺乏显式推理路径，可解释性差；3）存在"中间丢失"问题。需要一种能保持表格结构、支持可解释推理并缓解信息丢失的方法。

Method: 提出TABGR模型：1）将表格表示为属性表图（ATG），显式保留行列单元格结构；2）基于图推理实现可解释性；3）提出问题引导的个性化PageRank（QG-PPR）机制，重新排序表格数据以缓解"中间丢失"问题。

Result: 在两个常用基准测试上的广泛实验表明，TABGR持续优于最先进模型，准确率提升高达9.7%。

Conclusion: TABGR通过图表示和QG-PPR机制有效解决了现有表格推理方法的局限性，在保持表格结构、提高可解释性和缓解信息丢失方面表现出色，为表格推理任务提供了新的有效解决方案。

Abstract: Table reasoning, a task to answer questions by reasoning over data presented in tables, is an important topic due to the prevalence of knowledge stored in tabular formats. Recent solutions use Large Language Models (LLMs), exploiting the semantic understanding and reasoning capabilities of LLMs. A common paradigm of such solutions linearizes tables to form plain texts that are served as input to LLMs. This paradigm has critical issues. It loses table structures, lacks explicit reasoning paths for result explainability, and is subject to the "lost-in-the-middle" issue. To address these issues, we propose Table Graph Reasoner (TABGR), a training-free model that represents tables as an Attributed Table Graph (ATG). The ATG explicitly preserves row-column-cell structures while enabling graph-based reasoning for explainability. We further propose a Question-Guided Personalized PageRank (QG-PPR) mechanism to rerank tabular data and mitigate the lost-in-the-middle issue. Extensive experiments on two commonly used benchmarks show that TABGR consistently outperforms state-of-the-art models by up to 9.7% in accuracy. Our code will be made publicly available upon publication.

</details>


### [137] [An Under-Explored Application for Explainable Multimodal Misogyny Detection in code-mixed Hindi-English](https://arxiv.org/abs/2601.08457)
*Sargam Yadav,Abhishek Kaushik,Kevin Mc Daid*

Main category: cs.AI

TL;DR: 开发了一个多模态可解释的Web应用，用于检测印地语-英语混合文本和表情包中的厌女内容，结合了最先进的Transformer模型和XAI技术。


<details>
  <summary>Details</summary>
Motivation: 数字平台用户增长迅速，但随之而来的是仇恨言论和厌女内容的传播。现有AI模型在低资源语言和混合语言场景下研究不足，且缺乏可解释性，这在敏感的仇恨言论检测领域尤为重要。

Method: 1) 文本检测：使用XLM-RoBERTa和mBERT模型处理约4,193条评论；2) 多模态检测：使用mBERT+EfficientNet和mBERT+ResNET处理约4,218个表情包；3) 可解释性：采用SHAP和LIME技术提供特征重要性评分；4) 评估：通过Chatbot Usability Questionnaire和User Experience Questionnaire进行人工评估。

Result: 开发了一个完整的Web应用程序，能够检测印地语-英语混合文本和表情包中的厌女内容，并提供模型决策的可解释性分析。系统已通过人工评估验证其可用性。

Conclusion: 该系统为研究人员和内容审核人员提供了一个有效的工具，有助于促进该领域进一步研究，打击基于性别的数字暴力，确保安全的数字空间。

Abstract: Digital platforms have an ever-expanding user base, and act as a hub for communication, business, and connectivity. However, this has also allowed for the spread of hate speech and misogyny. Artificial intelligence models have emerged as an effective solution for countering online hate speech but are under explored for low resource and code-mixed languages and suffer from a lack of interpretability. Explainable Artificial Intelligence (XAI) can enhance transparency in the decisions of deep learning models, which is crucial for a sensitive domain such as hate speech detection. In this paper, we present a multi-modal and explainable web application for detecting misogyny in text and memes in code-mixed Hindi and English. The system leverages state-of-the-art transformer-based models that support multilingual and multimodal settings. For text-based misogyny identification, the system utilizes XLM-RoBERTa (XLM-R) and multilingual Bidirectional Encoder Representations from Transformers (mBERT) on a dataset of approximately 4,193 comments. For multimodal misogyny identification from memes, the system utilizes mBERT + EfficientNet, and mBERT + ResNET trained on a dataset of approximately 4,218 memes. It also provides feature importance scores using explainability techniques including Shapley Additive Values (SHAP) and Local Interpretable Model Agnostic Explanations (LIME). The application aims to serve as a tool for both researchers and content moderators, to promote further research in the field, combat gender based digital violence, and ensure a safe digital space. The system has been evaluated using human evaluators who provided their responses on Chatbot Usability Questionnaire (CUQ) and User Experience Questionnaire (UEQ) to determine overall usability.

</details>


### [138] [M3-BENCH: Process-Aware Evaluation of LLM Agents Social Behaviors in Mixed-Motive Games](https://arxiv.org/abs/2601.08462)
*Sixiong Xie,Zhuofan Shi,Haiyang Shen,Gang Huang,Yun Ma,Xiang Jing*

Main category: cs.AI

TL;DR: M3-Bench是一个多阶段基准测试，用于评估LLM智能体在混合动机游戏中的社交行为，通过行为轨迹、推理过程和通信内容的三维分析框架，结合大五人格模型和社会交换理论，提供超越简单任务得分的可解释社交行为画像。


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能体能力的提升，其合作、欺骗、共谋等高级社交行为需要系统评估。现有基准测试要么强调单一能力维度，要么仅依赖行为结果，忽视了智能体决策推理和通信交互的丰富过程信息。

Method: 提出M3-Bench多阶段基准测试和过程感知评估框架，包含三个协同分析模块：BTA（行为轨迹分析）、RPA（推理过程分析）和CCA（通信内容分析）。整合大五人格模型和社会交换理论，将多维证据聚合成可解释的社交行为画像。

Result: 实验结果表明，M3-Bench能够可靠地区分不同模型的多样化社交行为能力，并揭示某些模型虽然行为结果看似合理，但在推理和通信方面存在明显不一致性。

Conclusion: M3-Bench填补了现有基准测试的空白，通过过程感知的多维评估框架，能够更全面、深入地评估LLM智能体的社交行为能力和人格特质，超越了简单的任务得分或基于结果的度量。

Abstract: As the capabilities of large language model (LLM) agents continue to advance, their advanced social behaviors, such as cooperation, deception, and collusion, call for systematic evaluation. However, existing benchmarks often emphasize a single capability dimension or rely solely on behavioral outcomes, overlooking rich process information from agents' decision reasoning and communicative interactions. To address this gap, we propose M3-Bench, a multi-stage benchmark for mixed-motive games, together with a process-aware evaluation framework that conducts synergistic analysis across three modules: BTA (Behavioral Trajectory Analysis), RPA (Reasoning Process Analysis), and CCA (Communication Content Analysis). Furthermore, we integrate the Big Five personality model and Social Exchange Theory to aggregate multi-dimensional evidence into interpretable social behavior portraits, thereby characterizing agents' personality traits and capability profiles beyond simple task scores or outcome-based metrics. Experimental results show that M3-Bench can reliably distinguish diverse social behavior competencies across models, and it reveals that some models achieve seemingly reasonable behavioral outcomes while exhibiting pronounced inconsistencies in their reasoning and communication.

</details>


### [139] [SUMMPILOT: Bridging Efficiency and Customization for Interactive Summarization System](https://arxiv.org/abs/2601.08475)
*JungMin Yun,Juhwan Choi,Kyohoon Jin,Soojin Jang,Jinhee Jang,YoungBin Kim*

Main category: cs.AI

TL;DR: SummPilot是一个基于交互的可定制化摘要系统，利用大语言模型实现自动和交互式摘要，用户可以通过语义图、实体聚类等组件个性化摘要。


<details>
  <summary>Details</summary>
Motivation: 结合自动摘要的效率优势，解决为不同用户兴趣和需求生成个性化摘要的挑战。

Method: 引入SummPilot系统，利用大语言模型支持自动和交互式摘要，提供语义图、实体聚类和可解释评估等交互组件让用户个性化摘要。

Result: 演示和用户研究表明SummPilot在可定制化摘要方面具有适应性和实用性。

Conclusion: SummPilot系统成功解决了个性化摘要生成问题，通过交互式组件实现了有效的可定制化摘要。

Abstract: This paper incorporates the efficiency of automatic summarization and addresses the challenge of generating personalized summaries tailored to individual users' interests and requirements. To tackle this challenge, we introduce SummPilot, an interaction-based customizable summarization system. SummPilot leverages a large language model to facilitate both automatic and interactive summarization. Users can engage with the system to understand document content and personalize summaries through interactive components such as semantic graphs, entity clustering, and explainable evaluation. Our demo and user studies demonstrate SummPilot's adaptability and usefulness for customizable summarization.

</details>


### [140] [What If TSF: A Benchmark for Reframing Forecasting as Scenario-Guided Multimodal Forecasting](https://arxiv.org/abs/2601.08509)
*Jinkwan Jang,Hyunbin Jin,Hyungjin Park,Kyubyung Chae,Taesup Kim*

Main category: cs.AI

TL;DR: 提出了WIT多模态时间序列预测基准，用于评估模型是否能根据上下文文本（特别是未来场景）调整预测，通过专家构建的合理或反事实场景测试场景引导的多模态预测能力。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测方法大多是单模态的，主要依赖历史模式外推。虽然大语言模型在多模态预测方面显示出潜力，但现有基准主要提供回顾性或不对齐的原始上下文，不清楚模型是否真正利用了文本输入。实际中人类专家会结合历史证据和假设场景，在不同场景下基于相同观测做出不同预测。

Method: 引入What If TSF (WIT)多模态预测基准，通过提供专家构建的合理或反事实场景，评估模型是否能根据上下文文本（特别是未来场景）调整预测。基准设计用于测试场景引导的多模态预测能力。

Result: 提出了WIT基准，可用于严格测试场景引导的多模态预测。基准已在GitHub上开源，为评估模型是否真正利用文本输入提供了测试平台。

Conclusion: WIT基准填补了现有多模态时间序列预测评估的空白，通过专家构建的场景提供了更贴近实际决策过程的测试环境，有助于推动模型真正利用文本上下文进行预测的能力发展。

Abstract: Time series forecasting is critical to real-world decision making, yet most existing approaches remain unimodal and rely on extrapolating historical patterns. While recent progress in large language models (LLMs) highlights the potential for multimodal forecasting, existing benchmarks largely provide retrospective or misaligned raw context, making it unclear whether such models meaningfully leverage textual inputs. In practice, human experts incorporate what-if scenarios with historical evidence, often producing distinct forecasts from the same observations under different scenarios. Inspired by this, we introduce What If TSF (WIT), a multimodal forecasting benchmark designed to evaluate whether models can condition their forecasts on contextual text, especially future scenarios. By providing expert-crafted plausible or counterfactual scenarios, WIT offers a rigorous testbed for scenario-guided multimodal forecasting. The benchmark is available at https://github.com/jinkwan1115/WhatIfTSF.

</details>


### [141] [Sketch-Based Facade Renovation With Generative AI: A Streamlined Framework for Bypassing As-Built Modelling in Industrial Adaptive Reuse](https://arxiv.org/abs/2601.08531)
*Warissara Booranamaitree,Xusheng Du,Yushu Cai,Zhengyang Wang,Ye Zhang,Haoran Xie*

Main category: cs.AI

TL;DR: 提出结合生成AI与视觉语言模型的三阶段框架，直接从粗略结构草图和文本描述生成建筑立面改造方案，无需详细测绘建模。


<details>
  <summary>Details</summary>
Motivation: 立面改造比完全拆除更可持续，但现有工作流程需要详细的竣工建模，耗时费力且需要反复修改。需要一种能直接处理粗略草图和文本描述的方法。

Method: 三阶段框架：1) 微调VLM模型分析草图，预测需要修改的区域和添加的组件；2) 稳定扩散模型生成新元素详细草图，通过生成修复管道与原始轮廓合并；3) 使用ControlNet将结果细化为逼真图像。

Result: 在数据集和真实工业建筑上的实验表明，该框架能生成保留原始结构同时提升立面细节质量的改造方案，有效绕过详细竣工建模需求。

Conclusion: 该方法使建筑师能快速探索设计方案、迭代早期概念，并以更清晰的方式传达改造意图，为立面改造提供高效工具。

Abstract: Facade renovation offers a more sustainable alternative to full demolition, yet producing design proposals that preserve existing structures while expressing new intent remains challenging. Current workflows typically require detailed as-built modelling before design, which is time-consuming, labour-intensive, and often involves repeated revisions. To solve this issue, we propose a three-stage framework combining generative artificial intelligence (AI) and vision-language models (VLM) that directly processes rough structural sketch and textual descriptions to produce consistent renovation proposals. First, the input sketch is used by a fine-tuned VLM model to predict bounding boxes specifying where modifications are needed and which components should be added. Next, a stable diffusion model generates detailed sketches of new elements, which are merged with the original outline through a generative inpainting pipeline. Finally, ControlNet is employed to refine the result into a photorealistic image. Experiments on datasets and real industrial buildings indicate that the proposed framework can generate renovation proposals that preserve the original structure while improving facade detail quality. This approach effectively bypasses the need for detailed as-built modelling, enabling architects to rapidly explore design alternatives, iterate on early-stage concepts, and communicate renovation intentions with greater clarity.

</details>


### [142] [Learner-Tailored Program Repair: A Solution Generator with Iterative Edit-Driven Retrieval Enhancement](https://arxiv.org/abs/2601.08545)
*Zhenlong Dai,Zhuoluo Zhao,Hengning Wang,Xiu Tang,Sai Wu,Chang Yao,Zhipeng Gao,Jingyuan Chen*

Main category: cs.AI

TL;DR: 提出LPR（学习者定制程序修复）任务和LSG框架，通过两阶段检索增强方法修复代码错误并提供错误描述，优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有智能编程辅导系统大多只修复错误代码而不提供错误原因，无法满足学习者理解错误根源的需求。

Method: 提出LSG框架：第一阶段使用修复方案检索框架构建数据库，采用编辑驱动代码检索方法获取有价值解决方案；第二阶段提出解决方案引导的程序修复方法，在检索方案指导下修复代码并提供解释；还提出迭代检索增强方法，利用生成代码评估结果优化检索方向。

Result: 实验结果表明，该方法大幅优于一组基线方法，验证了LSG框架在新提出的LPR任务上的有效性。

Conclusion: 提出的LPR任务和LSG框架能够有效修复代码错误并提供错误描述，在编程辅导场景中具有实用价值。

Abstract: With the development of large language models (LLMs) in the field of programming, intelligent programming coaching systems have gained widespread attention. However, most research focuses on repairing the buggy code of programming learners without providing the underlying causes of the bugs. To address this gap, we introduce a novel task, namely \textbf{LPR} (\textbf{L}earner-Tailored \textbf{P}rogram \textbf{R}epair). We then propose a novel and effective framework, \textbf{\textsc{\MethodName{}}} (\textbf{L}earner-Tailored \textbf{S}olution \textbf{G}enerator), to enhance program repair while offering the bug descriptions for the buggy code. In the first stage, we utilize a repair solution retrieval framework to construct a solution retrieval database and then employ an edit-driven code retrieval approach to retrieve valuable solutions, guiding LLMs in identifying and fixing the bugs in buggy code. In the second stage, we propose a solution-guided program repair method, which fixes the code and provides explanations under the guidance of retrieval solutions. Moreover, we propose an Iterative Retrieval Enhancement method that utilizes evaluation results of the generated code to iteratively optimize the retrieval direction and explore more suitable repair strategies, improving performance in practical programming coaching scenarios. The experimental results show that our approach outperforms a set of baselines by a large margin, validating the effectiveness of our framework for the newly proposed LPR task.

</details>


### [143] [WaterCopilot: An AI-Driven Virtual Assistant for Water Management](https://arxiv.org/abs/2601.08559)
*Keerththanan Vickneswaran,Mariangel Garcia Andarcia,Hugo Retief,Chris Dickens,Paulo Silva*

Main category: cs.AI

TL;DR: WaterCopilot是一个基于RAG和工具调用架构的AI虚拟助手，用于整合跨界河流流域的静态政策文档和实时水文数据，支持多语言交互和可视化，旨在改善水资源管理决策。


<details>
  <summary>Details</summary>
Motivation: 跨界河流流域水资源管理面临数据碎片化、实时访问有限、信息源整合复杂等挑战，需要统一交互平台来弥合这些差距，支持及时决策。

Method: 采用检索增强生成(RAG)和工具调用架构，开发两个自定义插件：iwmi-doc-plugin用于基于Azure AI Search的文档语义搜索，iwmi-api-plugin用于查询实时数据库获取动态水文数据。

Result: 使用RAGAS框架评估，WaterCopilot总体得分0.8043，答案相关性0.8571，上下文精确度0.8009。系统支持多语言交互、透明来源引用、自动计算和可视化，并能生成环境流量警报、降雨趋势、水库水位等动态洞察。

Conclusion: WaterCopilot建立了一个可复制的AI增强框架，用于改善数据稀缺的跨界环境中的水资源治理。虽然存在非英语技术文档处理和API延迟等限制，但该系统展示了AI助手支持复杂河流流域及时决策和加强水安全的潜力。

Abstract: Sustainable water resource management in transboundary river basins is challenged by fragmented data, limited real-time access, and the complexity of integrating diverse information sources. This paper presents WaterCopilot-an AI-driven virtual assistant developed through collaboration between the International Water Management Institute (IWMI) and Microsoft Research for the Limpopo River Basin (LRB) to bridge these gaps through a unified, interactive platform. Built on Retrieval-Augmented Generation (RAG) and tool-calling architectures, WaterCopilot integrates static policy documents and real-time hydrological data via two custom plugins: the iwmi-doc-plugin, which enables semantic search over indexed documents using Azure AI Search, and the iwmi-api-plugin, which queries live databases to deliver dynamic insights such as environmental-flow alerts, rainfall trends, reservoir levels, water accounting, and irrigation data. The system features guided multilingual interactions (English, Portuguese, French), transparent source referencing, automated calculations, and visualization capabilities. Evaluated using the RAGAS framework, WaterCopilot achieves an overall score of 0.8043, with high answer relevancy (0.8571) and context precision (0.8009). Key innovations include automated threshold-based alerts, integration with the LRB Digital Twin, and a scalable deployment pipeline hosted on AWS. While limitations in processing non-English technical documents and API latency remain, WaterCopilot establishes a replicable AI-augmented framework for enhancing water governance in data-scarce, transboundary contexts. The study demonstrates the potential of this AI assistant to support informed, timely decision-making and strengthen water security in complex river basins.

</details>


### [144] [ViDoRe V3: A Comprehensive Evaluation of Retrieval Augmented Generation in Complex Real-World Scenarios](https://arxiv.org/abs/2601.08620)
*António Loison,Quentin Macé,Antoine Edy,Victor Xing,Tom Balough,Gabriel Moreira,Bo Liu,Manuel Faysse,Céline Hudelot,Gautier Viaud*

Main category: cs.AI

TL;DR: ViDoRe v3是一个多模态RAG基准测试，包含视觉丰富的文档和多种查询类型，涵盖10个专业领域数据集，评估显示视觉检索器优于文本检索器，但现有模型在处理非文本元素和细粒度视觉定位方面仍有困难。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG基准测试主要关注文本数据、单文档理解，或将检索和生成分开评估，无法捕捉真实应用中处理视觉元素（表格、图表、图像）、跨文档信息合成和准确来源定位的复杂性。

Method: 构建ViDoRe v3多模态RAG基准测试，包含10个专业领域数据集，约26,000个文档页面和3,099个人工验证的查询（支持6种语言）。通过12,000小时人工标注，提供检索相关性、边界框定位和验证参考答案的高质量标注。

Result: 评估显示：1）视觉检索器优于文本检索器；2）延迟交互模型和文本重排序显著提升性能；3）混合或纯视觉上下文提高答案生成质量；4）当前模型在处理非文本元素、开放式查询和细粒度视觉定位方面仍有困难。

Conclusion: ViDoRe v3为多模态RAG研究提供了全面的基准测试，揭示了当前模型的局限性，特别是在处理视觉元素和跨文档信息合成方面，为未来研究提供了方向。该基准测试已开源发布。

Abstract: Retrieval-Augmented Generation (RAG) pipelines must address challenges beyond simple single-document retrieval, such as interpreting visual elements (tables, charts, images), synthesizing information across documents, and providing accurate source grounding. Existing benchmarks fail to capture this complexity, often focusing on textual data, single-document comprehension, or evaluating retrieval and generation in isolation. We introduce ViDoRe v3, a comprehensive multimodal RAG benchmark featuring multi-type queries over visually rich document corpora. It covers 10 datasets across diverse professional domains, comprising ~26,000 document pages paired with 3,099 human-verified queries, each available in 6 languages. Through 12,000 hours of human annotation effort, we provide high-quality annotations for retrieval relevance, bounding box localization, and verified reference answers. Our evaluation of state-of-the-art RAG pipelines reveals that visual retrievers outperform textual ones, late-interaction models and textual reranking substantially improve performance, and hybrid or purely visual contexts enhance answer generation quality. However, current models still struggle with non-textual elements, open-ended queries, and fine-grained visual grounding. To encourage progress in addressing these challenges, the benchmark is released under a commercially permissive license at https://hf.co/vidore.

</details>


### [145] [Resisting Manipulative Bots in Memecoin Copy Trading: A Multi-Agent Approach with Chain-of-Thought Reasoning](https://arxiv.org/abs/2601.08641)
*Yichen Luo,Yebo Feng,Jiahua Xu,Yang Liu*

Main category: cs.AI

TL;DR: 提出一个可解释的多智能体系统用于模因币跟单交易，通过分解复杂任务并协调专业智能体，在识别高质量模因币项目和KOL钱包方面优于传统机器学习模型和单一LLM。


<details>
  <summary>Details</summary>
Motivation: 模因币跟单交易虽然流行但存在操纵机器人、被跟随钱包未来表现不确定、交易执行延迟等问题，且单一LLM难以处理复杂的资产分配任务，特别是在加密货币领域缺乏足够的领域知识。

Method: 受资产管理团队结构启发，将复杂任务分解为子任务，协调专业智能体协作解决。采用少样本思维链提示，使每个智能体获取专业模因币交易知识，解释多模态数据并生成可解释决策。

Result: 在1000个模因币项目交易数据上的实证评估显示，该系统在识别高质量模因币项目和KOL钱包方面的精确度分别达到73%和70%，所选KOL在这些项目中总共产生了50万美元的利润。

Conclusion: 提出的可解释多智能体系统有效解决了模因币跟单交易中的挑战，通过任务分解和智能体协作，在识别优质投资机会方面优于现有方法，为加密货币投资提供了新思路。

Abstract: The launch of \$Trump coin ignited a wave in meme coin investment. Copy trading, as a strategy-agnostic approach that eliminates the need for deep trading knowledge, quickly gains widespread popularity in the meme coin market. However, copy trading is not a guarantee of profitability due to the prevalence of manipulative bots, the uncertainty of the followed wallets' future performance, and the lag in trade execution. Recently, large language models (LLMs) have shown promise in financial applications by effectively understanding multi-modal data and producing explainable decisions. However, a single LLM struggles with complex, multi-faceted tasks such as asset allocation. These challenges are even more pronounced in cryptocurrency markets, where LLMs often lack sufficient domain-specific knowledge in their training data.
  To address these challenges, we propose an explainable multi-agent system for meme coin copy trading. Inspired by the structure of an asset management team, our system decomposes the complex task into subtasks and coordinates specialized agents to solve them collaboratively. Employing few-shot chain-of-though (CoT) prompting, each agent acquires professional meme coin trading knowledge, interprets multi-modal data, and generates explainable decisions. Using a dataset of 1,000 meme coin projects' transaction data, our empirical evaluation shows that the proposed multi-agent system outperforms both traditional machine learning models and single LLMs, achieving 73% and 70% precision in identifying high-quality meme coin projects and key opinion leader (KOL) wallets, respectively. The selected KOLs collectively generated a total profit of \$500,000 across these projects.

</details>


### [146] [Prism: Towards Lowering User Cognitive Load in LLMs via Complex Intent Understanding](https://arxiv.org/abs/2601.08653)
*Zenghua Liao,Jinzhi Liao,Xiang Zhao*

Main category: cs.AI

TL;DR: Prism是一个用于复杂意图理解的框架，通过逻辑依赖建模和认知负载优化，显著提升LLM在社交平台上的意图澄清能力。


<details>
  <summary>Details</summary>
Motivation: 在社交平台上，用户意图通常模糊且动态变化，现有方法通过顺序或并行提问无法有效建模澄清问题之间的逻辑依赖关系，导致意图理解效果不佳。

Method: Prism包含四个模块：复杂意图分解模块（分解意图并识别逻辑依赖）、逻辑澄清生成模块（基于依赖组织问题）、意图感知奖励模块（评估澄清轨迹质量并生成训练数据）、自进化意图调优模块（迭代优化LLM的逻辑澄清能力）。

Result: Prism在澄清交互、意图执行和认知负载基准测试中均优于现有方法，将逻辑冲突降至11.5%，用户满意度提升14.4%，任务完成时间减少34.8%，达到最先进的逻辑一致性。

Conclusion: Prism通过建模澄清问题间的逻辑依赖关系，实现了更高效、逻辑一致的意图理解，显著改善了LLM在社交平台上的协作效果。

Abstract: Large Language Models are rapidly emerging as web-native interfaces to social platforms. On the social web, users frequently have ambiguous and dynamic goals, making complex intent understanding-rather than single-turn execution-the cornerstone of effective human-LLM collaboration. Existing approaches attempt to clarify user intents through sequential or parallel questioning, yet they fall short of addressing the core challenge: modeling the logical dependencies among clarification questions. Inspired by the Cognitive Load Theory, we propose Prism, a novel framework for complex intent understanding that enables logically coherent and efficient intent clarification. Prism comprises four tailored modules: a complex intent decomposition module, which decomposes user intents into smaller, well-structured elements and identifies logical dependencies among them; a logical clarification generation module, which organizes clarification questions based on these dependencies to ensure coherent, low-friction interactions; an intent-aware reward module, which evaluates the quality of clarification trajectories via an intent-aware reward function and leverages Monte Carlo Sample to simulate user-LLM interactions for large-scale,high-quality training data generation; and a self-evolved intent tuning module, which iteratively refines the LLM's logical clarification capability through data-driven feedback and optimization. Prism consistently outperforms existing approaches across clarification interactions, intent execution, and cognitive load benchmarks. It achieves stateof-the-art logical consistency, reduces logical conflicts to 11.5%, increases user satisfaction by 14.4%, and decreases task completion time by 34.8%. All data and code are released.

</details>


### [147] [From Classical to Quantum Reinforcement Learning and Its Applications in Quantum Control: A Beginner's Tutorial](https://arxiv.org/abs/2601.08662)
*Abhijit Sen,Sonali Panda,Mahima Arya,Subhajit Patra,Zizhan Zheng,Denys I. Bondar*

Main category: cs.AI

TL;DR: 为本科生设计的强化学习入门教程，通过实例驱动教学，帮助理解理论与代码实现之间的差距


<details>
  <summary>Details</summary>
Motivation: 解决本科生在学习强化学习时面临的挑战，特别是从理论理解到实际编码应用的过渡困难，使强化学习更加易于接触和理解

Method: 采用清晰的、实例驱动的解释方法，通过动手实践的例子和易于理解的讲解，帮助学生掌握从概念到实现的过程

Result: 提供了使强化学习更加易于接触的教学方法，帮助学生建立应用强化学习技术所需的基础技能

Conclusion: 该教程成功地为本科生提供了强化学习的入门途径，通过实践导向的教学方法弥合了理论与应用之间的鸿沟，使学生能够自信地在实际场景中应用强化学习技术

Abstract: This tutorial is designed to make reinforcement learning (RL) more accessible to undergraduate students by offering clear, example-driven explanations. It focuses on bridging the gap between RL theory and practical coding applications, addressing common challenges that students face when transitioning from conceptual understanding to implementation. Through hands-on examples and approachable explanations, the tutorial aims to equip students with the foundational skills needed to confidently apply RL techniques in real-world scenarios.

</details>


### [148] [Parallel Context-of-Experts Decoding for Retrieval Augmented Generation](https://arxiv.org/abs/2601.08670)
*Giulio Corallo,Paolo Papotti*

Main category: cs.AI

TL;DR: PCED通过训练免费的解码框架，将证据聚合从注意力机制转移到解码过程，解决了RAG中多文档推理与速度的权衡问题


<details>
  <summary>Details</summary>
Motivation: 检索增强生成面临两难：长提示拼接支持多文档推理但造成预填充瓶颈，而单独编码文档KV缓存速度快但破坏了跨文档交互

Method: 提出并行专家上下文解码（PCED），将检索文档视为独立"专家"，通过新颖的检索感知对比解码规则同步专家预测，权衡专家logits与模型先验

Result: 该方法无需构建跨文档共享注意力，即可恢复跨文档推理能力

Conclusion: PCED框架有效解决了RAG中多文档推理与计算效率的权衡问题，提供了一种训练免费的解决方案

Abstract: Retrieval Augmented Generation faces a trade-off: concatenating documents in a long prompt enables multi-document reasoning but creates prefill bottlenecks, while encoding document KV caches separately offers speed but breaks cross-document interaction. We propose Parallel Context-of-Experts Decoding (Pced), a training-free framework that shifts evidence aggregation from the attention mechanism to the decoding. Pced treats retrieved documents as isolated "experts", synchronizing their predictions via a novel retrieval-aware contrastive decoding rule that weighs expert logits against the model prior. This approach recovers cross-document reasoning capabilities without constructing a shared attention across documents.

</details>


### [149] [Advancing ESG Intelligence: An Expert-level Agent and Comprehensive Benchmark for Sustainable Finance](https://arxiv.org/abs/2601.08676)
*Yilei Zhao,Wentao Zhang,Xiao Lei,Yandan Zheng,Mengpu Liu,Wei Yang Bryan Lim*

Main category: cs.AI

TL;DR: ESGAgent：一个基于分层多智能体系统的ESG分析工具，配备专业工具集，在ESG分析任务上超越现有LLMs，并提出了一个三级基准测试


<details>
  <summary>Details</summary>
Motivation: 当前ESG分析面临数据碎片化和非结构化数据挑战，现有大语言模型难以处理复杂的多步骤审计工作流，需要专门解决方案

Method: 引入ESGAgent分层多智能体系统，配备检索增强、网络搜索和领域特定函数等专业工具集，并构建了基于310份企业可持续发展报告的三级基准测试

Result: ESGAgent在原子问答任务上平均准确率达到84.15%，超越最先进的闭源LLMs，在专业报告生成方面表现出色，能整合丰富图表和可验证引用

Conclusion: ESGAgent有效解决了ESG分析的数据碎片化和复杂工作流问题，提出的基准测试为评估高风险垂直领域的智能体能力提供了重要测试平台

Abstract: Environmental, social, and governance (ESG) criteria are essential for evaluating corporate sustainability and ethical performance. However, professional ESG analysis is hindered by data fragmentation across unstructured sources, and existing large language models (LLMs) often struggle with the complex, multi-step workflows required for rigorous auditing. To address these limitations, we introduce ESGAgent, a hierarchical multi-agent system empowered by a specialized toolset, including retrieval augmentation, web search and domain-specific functions, to generate in-depth ESG analysis. Complementing this agentic system, we present a comprehensive three-level benchmark derived from 310 corporate sustainability reports, designed to evaluate capabilities ranging from atomic common-sense questions to the generation of integrated, in-depth analysis. Empirical evaluations demonstrate that ESGAgent outperforms state-of-the-art closed-source LLMs with an average accuracy of 84.15% on atomic question-answering tasks, and excels in professional report generation by integrating rich charts and verifiable references. These findings confirm the diagnostic value of our benchmark, establishing it as a vital testbed for assessing general and advanced agentic capabilities in high-stakes vertical domains.

</details>


### [150] [PersonaDual: Balancing Personalization and Objectivity via Adaptive Reasoning](https://arxiv.org/abs/2601.08679)
*Xiaoyou Liu,Xinyi Mou,Shengbin Yue,Liang Wang,Yuqing Wang,Qiexiang Wang,Tianrui Qin,Wangchunshu Zhou,Zhongyu Wei*

Main category: cs.AI

TL;DR: PersonaDual框架让单个LLM同时支持客观推理和个性化推理，通过上下文自适应切换模式，在保持个性化优势的同时减少对客观性的干扰。


<details>
  <summary>Details</summary>
Motivation: 用户期望LLM能符合个人偏好，个性化信息有价值但存在风险：可能改善交互体验，但当与问题不匹配时会损害客观性和事实准确性。

Method: 提出PersonaDual框架，先通过SFT训练学习两种推理模式（客观和个性化），然后用提出的DualGRPO强化学习优化模式选择，实现基于上下文的自适应切换。

Result: 在客观和个性化基准测试中，PersonaDual在保持个性化优势的同时显著减少干扰，实现接近无干扰的性能，并能更好地利用有帮助的个性化信号来改进客观问题解决。

Conclusion: PersonaDual框架成功解决了个性化信息可能损害客观性的问题，实现了在单个模型中平衡客观推理和个性化推理的目标。

Abstract: As users increasingly expect LLMs to align with their preferences, personalized information becomes valuable. However, personalized information can be a double-edged sword: it can improve interaction but may compromise objectivity and factual correctness, especially when it is misaligned with the question. To alleviate this problem, we propose PersonaDual, a framework that supports both general-purpose objective reasoning and personalized reasoning in a single model, and adaptively switches modes based on context. PersonaDual is first trained with SFT to learn two reasoning patterns, and then further optimized via reinforcement learning with our proposed DualGRPO to improve mode selection. Experiments on objective and personalized benchmarks show that PersonaDual preserves the benefits of personalization while reducing interference, achieving near interference-free performance and better leveraging helpful personalized signals to improve objective problem-solving.

</details>


### [151] [MEMEWEAVER: Inter-Meme Graph Reasoning for Sexism and Misogyny Detection](https://arxiv.org/abs/2601.08684)
*Paolo Italiani,David Gimeno-Gomez,Luca Ragazzi,Gianluca Moro,Paolo Rosso*

Main category: cs.AI

TL;DR: MemeWeaver是一个端到端可训练的多模态框架，通过新颖的跨meme图推理机制检测性别歧视和厌女症，在MAMI和EXIST基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 女性因性别遭受网络骚扰的可能性是男性的两倍。现有的多模态内容审核方法大多忽视了这种现象背后的社会动态，即施害者在志同道合的社区中强化偏见和群体认同。基于图的方法虽然有望捕捉此类互动，但现有解决方案受限于启发式图构建、浅层模态融合和实例级推理。

Method: 提出了MemeWeaver，一个端到端可训练的多模态框架，采用新颖的跨meme图推理机制。系统评估了多种视觉-文本融合策略，通过图结构捕捉meme之间的语义关系。

Result: 在MAMI和EXIST基准测试中始终优于最先进的基线方法，同时实现了更快的训练收敛。分析表明学习到的图结构捕捉了语义上有意义的模式，为在线仇恨的关系性质提供了有价值的见解。

Conclusion: MemeWeaver通过图推理机制有效检测性别歧视和厌女症，不仅性能优越，还能揭示在线仇恨传播的关系模式，为理解网络骚扰的社会动态提供了新视角。

Abstract: Women are twice as likely as men to face online harassment due to their gender. Despite recent advances in multimodal content moderation, most approaches still overlook the social dynamics behind this phenomenon, where perpetrators reinforce prejudices and group identity within like-minded communities. Graph-based methods offer a promising way to capture such interactions, yet existing solutions remain limited by heuristic graph construction, shallow modality fusion, and instance-level reasoning. In this work, we present MemeWeaver, an end-to-end trainable multimodal framework for detecting sexism and misogyny through a novel inter-meme graph reasoning mechanism. We systematically evaluate multiple visual--textual fusion strategies and show that our approach consistently outperforms state-of-the-art baselines on the MAMI and EXIST benchmarks, while achieving faster training convergence. Further analyses reveal that the learned graph structure captures semantically meaningful patterns, offering valuable insights into the relational nature of online hate.

</details>


### [152] [All Required, In Order: Phase-Level Evaluation for AI-Human Dialogue in Healthcare and Beyond](https://arxiv.org/abs/2601.08690)
*Shubham Kulkarni,Alexander Lyzhov,Shiva Chaitanya,Preetam Joshi*

Main category: cs.AI

TL;DR: OIP-SCE是一种评估对话式AI临床合规性的方法，通过检查每个必需临床义务是否按正确顺序完成并提供清晰证据，使复杂规则变得实用且可审计。


<details>
  <summary>Details</summary>
Motivation: 当前大多数对话式AI评估方法未能考虑合规性如何依赖于完整对话过程，存在技术进展与医疗实际需求之间的差距。

Method: 提出Obligatory-Information Phase Structured Compliance Evaluation (OIP-SCE)方法，检查每个必需临床义务是否按正确顺序完成，并提供清晰证据供临床医生审查。

Result: 通过两个案例研究（呼吸病史、福利验证）展示了该方法如何将政策转化为共享、可操作的步骤，阶段级证据使复杂规则变得实用且可审计。

Conclusion: OIP-SCE为临床医生提供检查控制权，为工程师提供清晰实施规范，创建单一可审计的评估界面，使AI能力与临床工作流程对齐，支持常规安全使用。

Abstract: Conversational AI is starting to support real clinical work, but most evaluation methods miss how compliance depends on the full course of a conversation. We introduce Obligatory-Information Phase Structured Compliance Evaluation (OIP-SCE), an evaluation method that checks whether every required clinical obligation is met, in the right order, with clear evidence for clinicians to review. This makes complex rules practical and auditable, helping close the gap between technical progress and what healthcare actually needs. We demonstrate the method in two case studies (respiratory history, benefits verification) and show how phase-level evidence turns policy into shared, actionable steps. By giving clinicians control over what to check and engineers a clear specification to implement, OIP-SCE provides a single, auditable evaluation surface that aligns AI capability with clinical workflow and supports routine, safe use.

</details>


### [153] [Learning from Demonstrations via Capability-Aware Goal Sampling](https://arxiv.org/abs/2601.08731)
*Yuanlin Duan,Yuning Wang,Wenjie Qiu,He Zhu*

Main category: cs.AI

TL;DR: Cago是一种基于能力感知目标采样的模仿学习方法，通过动态跟踪智能体在专家轨迹上的能力，选择刚好超出当前能力范围的目标作为中间步骤，从而在长视野任务中实现稳定学习。


<details>
  <summary>Details</summary>
Motivation: 模仿学习在长视野环境中经常失败，因为完美复制演示不现实，小错误会累积导致灾难性后果。现有方法要么仅用演示初始化策略，要么用于奖励塑造，无法有效处理智能体能力与专家轨迹之间的差距。

Method: Cago方法动态跟踪智能体在专家轨迹上的能力，使用这个信号选择中间目标——刚好超出智能体当前能力范围的目标。这形成了一个自适应课程，引导智能体逐步学习完整任务，而不是直接模仿专家轨迹。

Result: 实验结果表明，Cago在多种稀疏奖励、目标条件任务中显著提高了样本效率和最终性能，一致优于现有的学习演示基线方法。

Conclusion: Cago通过能力感知的目标采样方法，有效缓解了模仿学习对专家轨迹的脆弱依赖，在长视野任务中实现了更稳定和高效的学习。

Abstract: Despite its promise, imitation learning often fails in long-horizon environments where perfect replication of demonstrations is unrealistic and small errors can accumulate catastrophically. We introduce Cago (Capability-Aware Goal Sampling), a novel learning-from-demonstrations method that mitigates the brittle dependence on expert trajectories for direct imitation. Unlike prior methods that rely on demonstrations only for policy initialization or reward shaping, Cago dynamically tracks the agent's competence along expert trajectories and uses this signal to select intermediate steps--goals that are just beyond the agent's current reach--to guide learning. This results in an adaptive curriculum that enables steady progress toward solving the full task. Empirical results demonstrate that Cago significantly improves sample efficiency and final performance across a range of sparse-reward, goal-conditioned tasks, consistently outperforming existing learning from-demonstrations baselines.

</details>


### [154] [Pervasive Annotation Errors Break Text-to-SQL Benchmarks and Leaderboards](https://arxiv.org/abs/2601.08778)
*Tengjun Jin,Yoojin Choi,Yuxuan Zhu,Daniel Kang*

Main category: cs.AI

TL;DR: 研究发现文本转SQL基准测试中标注错误率很高（BIRD 52.8%，Spider 2.0-Snow 62.8%），这些错误显著影响模型性能评估和排行榜排名，可能误导研究和部署选择。


<details>
  <summary>Details</summary>
Motivation: 文本转SQL技术依赖人工标注的基准测试进行对比和选择，但标注错误可能影响评估的有效性，需要实证研究来量化这一问题的影响。

Method: 通过专家分析评估两个主流文本转SQL基准（BIRD和Spider 2.0-Snow）的标注错误率，修正BIRD开发集子集，重新评估16个开源代理在原始和修正集上的表现，分析性能变化和排名变化。

Result: BIRD Mini-Dev和Spider 2.0-Snow的错误率分别为52.8%和62.8%；修正后代理性能相对变化为-7%到31%，排名变化为-9到+9位；原始子集与完整开发集排名强相关，但修正后子集与完整集相关性弱。

Conclusion: 标注错误会显著扭曲报告的性能和排名，可能误导研究方向和部署选择，强调需要更可靠的基准测试评估方法。

Abstract: Researchers have proposed numerous text-to-SQL techniques to streamline data analytics and accelerate the development of database-driven applications. To compare these techniques and select the best one for deployment, the community depends on public benchmarks and their leaderboards. Since these benchmarks heavily rely on human annotations during question construction and answer evaluation, the validity of the annotations is crucial.
  In this paper, we conduct an empirical study that (i) benchmarks annotation error rates for two widely used text-to-SQL benchmarks, BIRD and Spider 2.0-Snow, and (ii) corrects a subset of the BIRD development (Dev) set to measure the impact of annotation errors on text-to-SQL agent performance and leaderboard rankings. Through expert analysis, we show that BIRD Mini-Dev and Spider 2.0-Snow have error rates of 52.8% and 62.8%, respectively. We re-evaluate all 16 open-source agents from the BIRD leaderboard on both the original and the corrected BIRD Dev subsets. We show that performance changes range from -7% to 31% (in relative terms) and rank changes range from $-9$ to $+9$ positions. We further assess whether these impacts generalize to the full BIRD Dev set. We find that the rankings of agents on the uncorrected subset correlate strongly with those on the full Dev set (Spearman's $r_s$=0.85, $p$=3.26e-5), whereas they correlate weakly with those on the corrected subset (Spearman's $r_s$=0.32, $p$=0.23). These findings show that annotation errors can significantly distort reported performance and rankings, potentially misguiding research directions or deployment choices. Our code and data are available at https://github.com/uiuc-kang-lab/text_to_sql_benchmarks.

</details>


### [155] [Uncovering Political Bias in Large Language Models using Parliamentary Voting Records](https://arxiv.org/abs/2601.08785)
*Jieying Chen,Karen de Jong,Andreas Poole,Jan Burakowski,Elena Elderson Nosti,Joep Windt,Chendi Wang*

Main category: cs.AI

TL;DR: 本文提出了一种通过将模型生成的投票预测与真实议会投票记录对齐来构建政治偏见基准的通用方法，并在荷兰、挪威和西班牙三个国家案例中应用，发现最先进的LLM普遍表现出左倾或中间倾向，并对右翼保守政党存在明显负面偏见。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在数字平台和决策系统中的深度应用，对其政治偏见的担忧日益增长。尽管已有大量研究关注性别和种族等社会偏见，但对政治偏见的系统性研究仍然有限，尽管其对社会有直接影响。

Method: 提出了一种构建政治偏见基准的通用方法：将模型生成的投票预测与经过验证的议会投票记录对齐。在三个国家案例中实例化该方法：PoliBiasNL（荷兰）、PoliBiasNO（挪威）和PoliBiasES（西班牙）。还提出了一种可视化方法，将LLM和政党的意识形态映射到共享的二维CHES空间，实现模型与现实政治行为者的直接可解释比较。

Result: 实验揭示了细粒度的意识形态区别：最先进的LLM一致表现出左倾或中间倾向，同时对右翼保守政党存在明显的负面偏见。

Conclusion: 这些发现强调了基于真实议会行为的透明、跨国评估对于理解和审计现代LLM政治偏见的价值，为理解模型政治倾向提供了实证基础。

Abstract: As large language models (LLMs) become deeply embedded in digital platforms and decision-making systems, concerns about their political biases have grown. While substantial work has examined social biases such as gender and race, systematic studies of political bias remain limited, despite their direct societal impact. This paper introduces a general methodology for constructing political bias benchmarks by aligning model-generated voting predictions with verified parliamentary voting records. We instantiate this methodology in three national case studies: PoliBiasNL (2,701 Dutch parliamentary motions and votes from 15 political parties), PoliBiasNO (10,584 motions and votes from 9 Norwegian parties), and PoliBiasES (2,480 motions and votes from 10 Spanish parties). Across these benchmarks, we assess ideological tendencies and political entity bias in LLM behavior. As part of our evaluation framework, we also propose a method to visualize the ideology of LLMs and political parties in a shared two-dimensional CHES (Chapel Hill Expert Survey) space by linking their voting-based positions to the CHES dimensions, enabling direct and interpretable comparisons between models and real-world political actors. Our experiments reveal fine-grained ideological distinctions: state-of-the-art LLMs consistently display left-leaning or centrist tendencies, alongside clear negative biases toward right-conservative parties. These findings highlight the value of transparent, cross-national evaluation grounded in real parliamentary behavior for understanding and auditing political bias in modern LLMs.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [156] [Cultural Compass: A Framework for Organizing Societal Norms to Detect Violations in Human-AI Conversations](https://arxiv.org/abs/2601.07973)
*Myra Cheng,Vinodkumar Prabhakaran,Alice Oh,Hayk Stepanyan,Aishwarya Verma,Charu Kalia,Erin MacMurray van Liemt,Sunipa Dev*

Main category: cs.CY

TL;DR: 该论文提出了一个文化规范分类法，用于评估生成式AI模型在不同文化背景下对规范的遵守情况，发现最先进的模型经常违反规范，且违规率因模型、交互情境和国家而异。


<details>
  <summary>Details</summary>
Motivation: 生成式AI模型需要在跨文化背景下既实用又安全，但现有研究在理解和评估模型对规范的遵守方面缺乏细致性和覆盖范围。需要更精细的方法来评估AI模型对社会文化规范的遵守情况。

Method: 引入了一个规范分类法，明确规范的上下文（如区分人类间规范和人类-AI交互规范）、规范的具体内容（如相关领域）和执行机制。将该分类法操作化，在自然、开放式环境中自动评估模型的规范遵守情况。

Result: 探索性分析表明，最先进的模型经常违反规范，但违规率因模型、交互情境和国家而异。违规率还因提示意图和情境框架的不同而变化。

Conclusion: 提出的分类法和示范性评估流程能够在现实环境中进行细致、情境敏感的跨文化规范遵守评估，为开发更符合文化规范的AI模型提供了重要工具。

Abstract: Generative AI models ought to be useful and safe across cross-cultural contexts. One critical step toward this goal is understanding how AI models adhere to sociocultural norms. While this challenge has gained attention in NLP, existing work lacks both nuance and coverage in understanding and evaluating models' norm adherence. We address these gaps by introducing a taxonomy of norms that clarifies their contexts (e.g., distinguishing between human-human norms that models should recognize and human-AI interactional norms that apply to the human-AI interaction itself), specifications (e.g., relevant domains), and mechanisms (e.g., modes of enforcement). We demonstrate how our taxonomy can be operationalized to automatically evaluate models' norm adherence in naturalistic, open-ended settings. Our exploratory analyses suggest that state-of-the-art models frequently violate norms, though violation rates vary by model, interactional context, and country. We further show that violation rates also vary by prompt intent and situational framing. Our taxonomy and demonstrative evaluation pipeline enable nuanced, context-sensitive evaluation of cultural norm adherence in realistic settings.

</details>


### [157] [Self-Certification of High-Risk AI Systems: The Example of AI-based Facial Emotion Recognition](https://arxiv.org/abs/2601.08295)
*Gregor Autischer,Kerstin Waxnegger,Dominik Kowald*

Main category: cs.CY

TL;DR: 研究通过完整自认证周期验证Fraunhofer AI评估目录作为认证框架的实用性，发现其可作为有价值的开发工具，但无法替代正式合规要求


<details>
  <summary>Details</summary>
Motivation: 欧盟AI法案对高风险AI系统提出了全面要求，但合规所需的协调标准尚未完全制定。研究旨在探索Fraunhofer AI评估目录作为认证框架的实际应用价值

Method: 通过完整自认证周期测试AI评估目录：从存在缺陷的基线人脸情绪识别系统开始，根据AI认证要求进行改进，重点关注可靠性和公平性两个维度

Result: 改进后的系统在准确率、可靠性指标和跨人口统计群体的公平性方面均有提升，成功满足所考察维度的认证标准。认证框架作为主动开发工具具有价值

Conclusion: Fraunhofer AI评估目录是当前阶段有价值的准备工具，可补充但不能替代正式合规要求。结构化自认证与法律合规之间存在根本差距，需要协调的欧洲标准

Abstract: The European Union's Artificial Intelligence Act establishes comprehensive requirements for high-risk AI systems, yet the harmonized standards necessary for demonstrating compliance remain not fully developed. In this paper, we investigate the practical application of the Fraunhofer AI assessment catalogue as a certification framework through a complete self-certification cycle of an AI-based facial emotion recognition system. Beginning with a baseline model that has deficiencies, including inadequate demographic representation and prediction uncertainty, we document an enhancement process guided by AI certification requirements. The enhanced system achieves higher accuracy with improved reliability metrics and comprehensive fairness across demographic groups. We focused our assessment on two of the six Fraunhofer catalogue dimensions, reliability and fairness, the enhanced system successfully satisfies the certification criteria for these examined dimensions. We find that the certification framework provides value as a proactive development tool, driving concrete technical improvements and generating documentation naturally through integration into the development process. However, fundamental gaps separate structured self-certification from legal compliance: harmonized European standards are not fully available, and AI assessment frameworks and catalogues cannot substitute for them on their own. These findings establish the Fraunhofer AI assessment catalogue as a valuable preparatory tool that complements rather than replaces formal compliance requirements at this time.

</details>


### [158] [Regulatory gray areas of LLM Terms](https://arxiv.org/abs/2601.08415)
*Brittany I. Davidson,Kate Muir,Florian A. D. Burnat,Adam N. Joinson*

Main category: cs.CY

TL;DR: 对五家主要LLM提供商服务条款的对比分析，揭示了对研究人员使用限制的显著差异和监管灰色地带


<details>
  <summary>Details</summary>
Motivation: LLM在学术研究中日益普及，但其服务条款的使用限制尚未得到充分研究，研究人员在使用时面临不确定性

Method: 对Anthropic、DeepSeek、Google、OpenAI和xAI五家LLM提供商2025年11月的服务条款进行对比分析

Result: 发现不同平台对普通用户和研究人员的限制严格程度和具体性存在显著差异，识别出安全研究、计算社会科学和心理学研究中的具体复杂性，以及监管灰色地带

Conclusion: 创建了公开可用的平台条款对比资源，讨论了普通用户和研究人员在这一不断发展的领域中导航的影响

Abstract: Large Language Models (LLMs) are increasingly integrated into academic research pipelines; however, the Terms of Service governing their use remain under-examined. We present a comparative analysis of the Terms of Service of five major LLM providers (Anthropic, DeepSeek, Google, OpenAI, and xAI) collected in November 2025. Our analysis reveals substantial variation in the stringency and specificity of usage restrictions for general users and researchers. We identify specific complexities for researchers in security research, computational social sciences, and psychological studies. We identify `regulatory gray areas' where Terms of Service create uncertainty for legitimate use. We contribute a publicly available resource comparing terms across platforms (OSF) and discuss implications for general users and researchers navigating this evolving landscape.

</details>


### [159] [Intersectional Data and the Social Cost of Digital Extraction: A Pigouvian Surcharge](https://arxiv.org/abs/2601.08574)
*Eduardo C. Garrido-Merchán*

Main category: cs.CY

TL;DR: 该论文提出基于互信息的数据定价规则，将数据价值与信息论度量挂钩，作为对数据提取的庇古式附加费，以纠正数字资本主义中的隐私外部性。


<details>
  <summary>Details</summary>
Motivation: 当代数字资本主义大规模提取和商品化个人数据，这些数据揭示了种族、性别、残疾等交叉社会身份。企业通过画像、预测和个性化获取经济价值，而个人和社会群体则承担社会风险、歧视和脆弱性等扩散成本，形成结构性隐私外部性。

Method: 开发了一个正式的政治经济框架，基于互信息提出定价规则，将数据价值与信息论度量挂钩。该规则根据个体数据点对联合交叉身份分布的熵减少来分配货币价值，作为对数据提取的庇古式附加费。该方法具有模型无关性，适用于参数、非参数或机器学习估计的交叉属性。

Result: 提出了一个可调节的定价机制，监管机构可以根据有争议的社会价值来校准附加费，从而将规范判断直接嵌入市场设计。该机制为信息权力的不对称积累提供了制度约束。

Conclusion: 通过形式化交叉数据提取的社会成本，该机制既提供了市场失灵的纠正措施，也为数字不对称条件下的弱势群体提供了再分配的制度保护，将规范判断直接嵌入市场设计。

Abstract: Contemporary digital capitalism relies on the large-scale extraction and commodification of personal data. Far from revealing isolated attributes, such data increasingly exposes intersectional social identities formed by combinations of race, gender, disability and others. This process generates a structural privacy externality: while firms appropriate economic value through profiling, prediction, and personalization, individuals and social groups bear diffuse costs in the form of heightened social risk, discrimination, and vulnerability. This paper develops a formal political economic framework to internalize these externalities by linking data valuation to information-theoretic measures. We propose a pricing rule based on mutual information that assigns monetary value to the entropy reduction induced by individual data points over joint intersectional identity distributions. Interpreted as a Pigouvian-style surcharge on data extraction, this mechanism functions as an institutional constraint on the asymmetric accumulation of informational power. A key advantage of the approach is its model-agnostic character: the valuation rule operates independently of the statistical structure used to estimate intersectional attributes, whether parametric, nonparametric, or machine-learned, and can be approximated through discretization of joint distributions. We argue that regulators can calibrate this surcharge to reflect contested social values, thereby embedding normative judgments directly into market design. By formalizing the social cost of intersectional data extraction, the proposed mechanism offers both a corrective to market failure and a redistributive institutional shield for vulnerable groups under conditions of digital asymmetry.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [160] [Hierarchical Sparse Plus Low Rank Compression of LLM](https://arxiv.org/abs/2601.07839)
*Pawan Kumar,Aditi Gupta*

Main category: cs.LG

TL;DR: 提出HSS压缩方法，通过稀疏矩阵+递归低秩分解来压缩LLM，在LLaMA-7B上仅压缩注意力投影层就能显著节省内存同时保持性能


<details>
  <summary>Details</summary>
Motivation: 现代大语言模型对内存和计算资源需求巨大，需要有效的压缩方法来进行部署和持续训练

Method: 两阶段压缩方案：1) 移除最大权重到稀疏矩阵S；2) 对密集残差矩阵应用递归层次稀疏可分低秩分解，采用递归降秩策略和RCM排列优化

Result: 在LLaMA-7B上，仅压缩自注意力投影层（1.6B参数中的Q、K、V矩阵），使用30%稀疏预算和512外秩，在WikiText数据集上达到1.64困惑度，优于密集基线和传统稀疏+SVD变体

Conclusion: HSS压缩方法硬件友好，能显著节省内存同时保持模型性能，为大语言模型的部署和训练提供了有效的压缩方案

Abstract: Modern large language models (LLMs) place extraordinary pressure on memory and compute budgets, making principled compression indispensable for both deployment and continued training. We present Hierarchical Sparse Plus Low-Rank (HSS) compression, a two-stage scheme that (i) removes the largest-magnitude weights into a sparse matrix S and (ii) applies a recursive Hierarchically Sparse Separable (HSS) low-rank factorisation to the dense residual matrix. A recursive rank-reducing strategy and a reverse Cuthill-Mckee (RCM) permutation are introduced to align high weights towards the diagonal with the block-diagonal hierarchy, maximising off-diagonal compressibility (because they are touched only once). HSS is hardware-friendly: its matrix-vector multiply reduces to one sparse and a sequence of thin-matrix multiplications and can be trained end-to-end with standard optimisers.
  Experiments on LLaMA-7B show that targeting only the self-attention projections (1.6 B parameters of Q, K, and V matrices out of a total 7B parameters) suffices to yield large memory savings while retaining comparable state-of-the-art perplexity scores on test samples of the WikiText dataset. For example, with a 30\% sparsity budget and an outer rank of 512, sHSS-RCM achieves a perplexity of 1.64, outperforming dense baselines and classical sparse-plus-SVD variants, while also achieving significant memory savings.

</details>


### [161] [Affect and Effect: Limitations of regularisation-based continual learning in EEG-based emotion classification](https://arxiv.org/abs/2601.07858)
*Nina Peire,Yupei Li,Björn Schuller*

Main category: cs.LG

TL;DR: 正则化持续学习方法在EEG情感分类中效果有限，无法有效泛化到未见被试，因为稳定性-可塑性权衡存在根本性错配


<details>
  <summary>Details</summary>
Motivation: EEG情感分类面临被试间和被试内高变异性挑战，持续学习（CL）是潜在解决方案，但现有正则化CL方法（如EWC、SI、MAS）在此问题上的适用性尚未充分探索

Method: 通过理论和实证分析，在DREAMER和SEED数据集上评估正则化CL方法，研究被试增量序列下的限制，分析参数重要性估计、梯度干扰、约束过度和顺序敏感性等问题

Result: 正则化CL方法在EEG情感分类中表现有限：参数重要性估计不可靠，梯度更新干扰，约束过度，性能对顺序敏感。前向迁移相比顺序微调无显著改善（p > 0.05）

Conclusion: EEG信号的高变异性意味着过去被试对未来的价值有限，正则化持续学习方法无法实现EEG情感分类中对未见被试的鲁棒泛化

Abstract: Generalisation to unseen subjects in EEG-based emotion classification remains a challenge due to high inter-and intra-subject variability. Continual learning (CL) poses a promising solution by learning from a sequence of tasks while mitigating catastrophic forgetting. Regularisation-based CL approaches, such as Elastic Weight Consolidation (EWC), Synaptic Intelligence (SI), and Memory Aware Synapses (MAS), are commonly used as baselines in EEG-based CL studies, yet their suitability for this problem remains underexplored. This study theoretically and empirically finds that regularisation-based CL methods show limited performance for EEG-based emotion classification on the DREAMER and SEED datasets. We identify a fundamental misalignment in the stability-plasticity trade-off, where regularisation-based methods prioritise mitigating catastrophic forgetting (backward transfer) over adapting to new subjects (forward transfer). We investigate this limitation under subject-incremental sequences and observe that: (1) the heuristics for estimating parameter importance become less reliable under noisy data and covariate shift, (2) gradients on parameters deemed important by these heuristics often interfere with gradient updates required for new subjects, moving optimisation away from the minimum, (3) importance values accumulated across tasks over-constrain the model, and (4) performance is sensitive to subject order. Forward transfer showed no statistically significant improvement over sequential fine-tuning (p > 0.05 across approaches and datasets). The high variability of EEG signals means past subjects provide limited value to future subjects. Regularisation-based continual learning approaches are therefore limited for robust generalisation to unseen subjects in EEG-based emotion classification.

</details>


### [162] [RewriteNets: End-to-End Trainable String-Rewriting for Generative Sequence Modeling](https://arxiv.org/abs/2601.07868)
*Harshil Vejendla*

Main category: cs.LG

TL;DR: RewriteNets：一种基于显式并行字符串重写的新型神经网络架构，通过可学习规则实现序列建模，在需要系统性泛化的任务上表现优异，计算效率高于Transformer。


<details>
  <summary>Details</summary>
Motivation: 现有主流序列模型（如Transformer）通过密集注意力权重隐式表示结构，导致二次复杂度。需要一种具有显式结构归纳偏置、计算效率更高的序列建模方法。

Method: 提出RewriteNets架构，每层包含一组可学习规则。对输入序列的每个位置执行四个操作：1) 规则模式的模糊匹配；2) 通过可微分分配算子选择非重叠重写；3) 应用选定规则替换输入段；4) 传播未触及的标记。使用Gumbel-Sinkhorn估计器实现端到端训练。

Result: 在算法、组合和字符串操作任务上评估，RewriteNets在需要系统性泛化的任务上表现优异（在SCAN基准测试的长度分割上达到98.7%准确率），计算效率高于Transformer。分析显示学习到的规则具有可解释性。

Conclusion: RewriteNets为具有显式结构归纳偏置的序列建模提供了一个有前景的方向，在系统性泛化和计算效率方面优于传统序列模型。

Abstract: Dominant sequence models like the Transformer represent structure implicitly through dense attention weights, incurring quadratic complexity. We propose RewriteNets, a novel neural architecture built on an alternative paradigm: explicit, parallel string rewriting. Each layer in a RewriteNet contains a set of learnable rules. For each position in an input sequence, the layer performs four operations: (1) fuzzy matching of rule patterns, (2) conflict resolution via a differentiable assignment operator to select non-overlapping rewrites, (3) application of the chosen rules to replace input segments with output segments of potentially different lengths, and (4) propagation of untouched tokens. While the discrete assignment of rules is non-differentiable, we employ a straight-through Gumbel-Sinkhorn estimator, enabling stable end-to-end training. We evaluate RewriteNets on algorithmic, compositional, and string manipulation tasks, comparing them against strong LSTM and Transformer baselines. Results show that RewriteNets excel at tasks requiring systematic generalization (achieving 98.7% accuracy on the SCAN benchmark's length split) and are computationally more efficient than Transformers. We also provide an analysis of learned rules and an extensive ablation study, demonstrating that this architecture presents a promising direction for sequence modeling with explicit structural inductive biases.

</details>


### [163] [Deep Exploration of Epoch-wise Double Descent in Noisy Data: Signal Separation, Large Activation, and Benign Overfitting](https://arxiv.org/abs/2601.08316)
*Tomoki Kubo,Ryuken Uda,Yusuke Iida*

Main category: cs.LG

TL;DR: 论文通过分析CIFAR-10数据集上带标签噪声训练的神经网络内部结构演化，揭示了epoch-wise double descent现象与良性过拟合、大激活值等关键现象的直接联系。


<details>
  <summary>Details</summary>
Motivation: 研究深度学习中epoch-wise double descent现象（延迟泛化后的过拟合）的内部机制，探索其与近期发现的"良性过拟合"和"大激活值"等现象的关系。

Method: 在CIFAR-10数据集（添加30%标签噪声）上训练三种不同大小的全连接神经网络，通过分解损失曲线为干净数据和噪声数据的信号贡献，分别分析内部激活的epoch-wise演化。

Result: 1. 模型在double descent阶段即使完美拟合噪声训练数据后仍能在测试数据上实现强重新泛化（良性过拟合状态）
2. 噪声数据在干净数据之后被学习，其内部激活在深层网络中逐渐分离，使模型能够仅过拟合噪声数据
3. 浅层出现单一极大激活值（"异常值"），其大小与输入模式相关但与输出模式无关，且随重新泛化演化

Conclusion: 这些实证发现直接连接了"深度double descent"、"良性过拟合"和"大激活值"等关键现象，支持提出理解深度double descent的新场景。

Abstract: Deep double descent is one of the key phenomena underlying the generalization capability of deep learning models. In this study, epoch-wise double descent, which is delayed generalization following overfitting, was empirically investigated by focusing on the evolution of internal structures. Fully connected neural networks of three different sizes were trained on the CIFAR-10 dataset with 30% label noise. By decomposing the loss curves into signal contributions from clean and noisy training data, the epoch-wise evolutions of internal signals were analyzed separately. Three main findings were obtained from this analysis. First, the model achieved strong re-generalization on test data even after perfectly fitting noisy training data during the double descent phase, corresponding to a "benign overfitting" state. Second, noisy data were learned after clean data, and as learning progressed, their corresponding internal activations became increasingly separated in outer layers; this enabled the model to overfit only noisy data. Third, a single, very large activation emerged in the shallow layer across all models; this phenomenon is referred as "outliers," "massive activa-tions," and "super activations" in recent large language models and evolves with re-generalization. The magnitude of large activation correlated with input patterns but not with output patterns. These empirical findings directly link the recent key phenomena of "deep double descent," "benign overfitting," and "large activation", and support the proposal of a novel scenario for understanding deep double descent.

</details>


### [164] [HOSC: A Periodic Activation with Saturation Control for High-Fidelity Implicit Neural Representations](https://arxiv.org/abs/2601.07870)
*Michal Jan Wlodarczyk,Danzel Serrano,Przemyslaw Musialski*

Main category: cs.LG

TL;DR: 提出HOSC激活函数：tanh(βsin(ω₀x))，通过β参数控制Lipschitz界为βω₀，解决周期激活函数梯度不稳定问题，在多种INR任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 正弦等周期激活函数在隐式神经表示中能保持高频信息，但存在梯度不稳定和多尺度行为控制有限的问题，需要一种能稳定梯度同时保持周期特性的激活函数。

Method: 提出HOSC激活函数：tanh(βsin(ω₀x))，通过β参数控制Lipschitz界为βω₀，提供直接的梯度幅度调节机制，同时保留周期载波特性。

Result: 在图像、音频、视频、NeRFs和SDFs等任务上的标准化训练实验表明，HOSC相比SIREN、FINER等方法在某些领域有显著优势，在其他领域达到竞争性水平。

Conclusion: HOSC是一种实用的周期激活函数，适用于INR应用，论文提供了领域特定的超参数选择指导，代码已开源。

Abstract: Periodic activations such as sine preserve high-frequency information in implicit neural representations (INRs) through their oscillatory structure, but often suffer from gradient instability and limited control over multi-scale behavior. We introduce the Hyperbolic Oscillator with Saturation Control (HOSC) activation, $\text{HOSC}(x) = \tanh\bigl(β\sin(ω_0 x)\bigr)$, which exposes an explicit parameter $β$ that controls the Lipschitz bound of the activation by $βω_0$. This provides a direct mechanism to tune gradient magnitudes while retaining a periodic carrier. We provide a mathematical analysis and conduct a comprehensive empirical study across images, audio, video, NeRFs, and SDFs using standardized training protocols. Comparative analysis against SIREN, FINER, and related methods shows where HOSC provides substantial benefits and where it achieves competitive parity. Results establish HOSC as a practical periodic activation for INR applications, with domain-specific guidance on hyperparameter selection. For code visit the project page https://hosc-nn.github.io/ .

</details>


### [165] [Multiplicative Orthogonal Sequential Editing for Language Models](https://arxiv.org/abs/2601.07873)
*Hao-Xiang Xu,Jun-Yu Ma,Ziqi Peng,Yuhao Sun,Zhen-Hua Ling,Jia-Chen Gu*

Main category: cs.LG

TL;DR: 提出MOSE方法，通过正交矩阵乘法而非加法来编辑LLM知识，保持数值稳定性，提升序列编辑性能12.08%，保留95.73%通用能力。


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑方法采用添加更新矩阵的加法范式，会损害数值稳定性指标（如条件数和范数），特别是在序列编辑场景中会降低编辑性能和通用能力。虽然后续方法有所改进，但仍未脱离加法框架，未能从根本上解决此限制。

Method: 从统计和数学角度分析，提出乘法编辑范式MOSE。不同于之前的加法编辑，MOSE将新知识编码到正交矩阵中，然后与原始参数矩阵相乘。正交矩阵乘法不改变矩阵的数值稳定性，从而保持编辑性能和通用能力。

Result: 在三种不同LLM上的实验表明，MOSE能有效限制编辑参数矩阵的偏差并保持其数值稳定性。相比现有方法，MOSE在序列编辑性能上提升12.08%，同时在下游任务中保留95.73%的通用能力。

Conclusion: MOSE通过正交矩阵乘法范式解决了加法编辑导致的数值稳定性问题，显著提升了知识编辑的性能和模型通用能力的保持，为LLM知识编辑提供了更有效的解决方案。

Abstract: Knowledge editing aims to efficiently modify the internal knowledge of large language models (LLMs) without compromising their other capabilities. The prevailing editing paradigm, which appends an update matrix to the original parameter matrix, has been shown by some studies to damage key numerical stability indicators (such as condition number and norm), thereby reducing editing performance and general abilities, especially in sequential editing scenario. Although subsequent methods have made some improvements, they remain within the additive framework and have not fundamentally addressed this limitation. To solve this problem, we analyze it from both statistical and mathematical perspectives and conclude that multiplying the original matrix by an orthogonal matrix does not change the numerical stability of the matrix. Inspired by this, different from the previous additive editing paradigm, a multiplicative editing paradigm termed Multiplicative Orthogonal Sequential Editing (MOSE) is proposed. Specifically, we first derive the matrix update in the multiplicative form, the new knowledge is then incorporated into an orthogonal matrix, which is multiplied by the original parameter matrix. In this way, the numerical stability of the edited matrix is unchanged, thereby maintaining editing performance and general abilities. We compared MOSE with several current knowledge editing methods, systematically evaluating their impact on both editing performance and the general abilities across three different LLMs. Experimental results show that MOSE effectively limits deviations in the edited parameter matrix and maintains its numerical stability. Compared to current methods, MOSE achieves a 12.08% improvement in sequential editing performance, while retaining 95.73% of general abilities across downstream tasks. The code is available at https://github.com/famoustourist/MOSE.

</details>


### [166] [Incorporating Cognitive Biases into Reinforcement Learning for Financial Decision-Making](https://arxiv.org/abs/2601.08247)
*Liu He*

Main category: cs.LG

TL;DR: 将认知偏差整合到金融交易的强化学习框架中，探索人类非理性行为对交易决策的影响


<details>
  <summary>Details</summary>
Motivation: 金融市场受人类非理性行为影响，传统强化学习模型假设理性代理人，可能忽略了心理因素的作用。需要研究认知偏差如何影响金融决策，以及如何将其整合到AI交易系统中。

Method: 将过度自信、损失厌恶等认知偏差整合到强化学习的奖励结构和决策过程中，在模拟和真实交易环境中评估性能

Result: 结果不确定或负面，表明将人类偏差整合到强化学习中面临挑战，但为开发稳健的金融AI系统提供了宝贵经验

Conclusion: 尽管结果不理想，但研究揭示了将人类认知偏差整合到强化学习框架中的挑战，为未来开发更稳健的金融AI系统提供了重要见解

Abstract: Financial markets are influenced by human behavior that deviates from rationality due to cognitive biases. Traditional reinforcement learning (RL) models for financial decision-making assume rational agents, potentially overlooking the impact of psychological factors. This study integrates cognitive biases into RL frameworks for financial trading, hypothesizing that such models can exhibit human-like trading behavior and achieve better risk-adjusted returns than standard RL agents. We introduce biases, such as overconfidence and loss aversion, into reward structures and decision-making processes and evaluate their performance in simulated and real-world trading environments. Despite its inconclusive or negative results, this study provides insights into the challenges of incorporating human-like biases into RL, offering valuable lessons for developing robust financial AI systems.

</details>


### [167] [NOVAK: Unified adaptive optimizer for deep neural networks](https://arxiv.org/abs/2601.07876)
*Sergii Kavun*

Main category: cs.LG

TL;DR: NOVAK是一个模块化的梯度优化算法，集成了自适应矩估计、校正学习率调度、解耦权重正则化、多种Nesterov动量变体和前瞻同步，在多个基准测试中优于14种现有优化器。


<details>
  <summary>Details</summary>
Motivation: 现有自适应优化方法在训练缺乏跳跃连接的深度普通网络时存在局限性，需要一种更稳健、性能更好的优化框架。

Method: 采用双模式架构：流线型快速路径用于生产，集成自适应矩估计、校正学习率调度、解耦权重正则化、多种Nesterov动量变体和内存高效的前瞻同步机制，使用自定义CUDA内核加速3-5倍。

Result: 在CIFAR-10、CIFAR-100、ImageNet和ImageNette上评估，优于Adam、AdamW、RAdam、Lion、Adan等14种优化器，在ResNet-50、VGG-16、ViT等架构上达到最先进精度，特别是在VGG-16/ImageNette上表现出卓越的架构鲁棒性。

Conclusion: NOVAK的架构贡献（特别是校正、解耦衰减和混合动量）对于可靠训练缺乏跳跃连接的深度普通网络至关重要，解决了现有自适应优化方法的长期限制。

Abstract: This work introduces NOVAK, a modular gradient-based optimization algorithm that integrates adaptive moment estimation, rectified learning-rate scheduling, decoupled weight regularization, multiple variants of Nesterov momentum, and lookahead synchronization into a unified, performance-oriented framework. NOVAK adopts a dual-mode architecture consisting of a streamlined fast path designed for production. The optimizer employs custom CUDA kernels that deliver substantial speedups (3-5 for critical operations) while preserving numerical stability under standard stochastic-optimization assumptions. We provide fully developed mathematical formulations for rectified adaptive learning rates, a memory-efficient lookahead mechanism that reduces overhead from O(2p) to O(p + p/k), and the synergistic coupling of complementary optimization components. Theoretical analysis establishes convergence guarantees and elucidates the stability and variance-reduction properties of the method. Extensive empirical evaluation on CIFAR-10, CIFAR-100, ImageNet, and ImageNette demonstrates NOVAK superiority over 14 contemporary optimizers, including Adam, AdamW, RAdam, Lion, and Adan. Across architectures such as ResNet-50, VGG-16, and ViT, NOVAK consistently achieves state-of-the-art accuracy, and exceptional robustness, attaining very high accuracy on VGG-16/ImageNette demonstrating superior architectural robustness compared to contemporary optimizers. The results highlight that NOVAKs architectural contributions (particularly rectification, decoupled decay, and hybrid momentum) are crucial for reliable training of deep plain networks lacking skip connections, addressing a long-standing limitation of existing adaptive optimization methods.

</details>


### [168] [E^2-LLM: Bridging Neural Signals and Interpretable Affective Analysis](https://arxiv.org/abs/2601.07877)
*Fei Ma,Han Lin,Yifan Xie,Hongwei Ren,Xiaoyu Shen,Wenbo Ding,Qi Tian*

Main category: cs.LG

TL;DR: E²-LLM：首个用于EEG信号可解释情感分析的多模态大语言模型框架，通过多阶段训练实现情感分类和推理


<details>
  <summary>Details</summary>
Motivation: 现有EEG情感识别面临主体间差异大、标注数据有限、缺乏可解释性等问题，而当前多模态大语言模型尚未适应神经信号的时空特性

Method: 集成预训练EEG编码器与Qwen大语言模型，通过可学习投影层连接，采用多阶段训练：情感判别预训练、跨模态对齐、指令微调与思维链推理

Result: 在七种情感类别数据集上，E²-LLM在情感分类上表现优异，更大模型变体在复杂推理场景中展现出更好的可靠性和零样本泛化能力

Conclusion: 该工作建立了生理信号与大语言模型推理能力结合的新范式，表明模型规模扩展能同时提升识别准确率和可解释的情感理解能力

Abstract: Emotion recognition from electroencephalography (EEG) signals remains challenging due to high inter-subject variability, limited labeled data, and the lack of interpretable reasoning in existing approaches. While recent multimodal large language models (MLLMs) have advanced emotion analysis, they have not been adapted to handle the unique spatiotemporal characteristics of neural signals. We present E^2-LLM (EEG-to-Emotion Large Language Model), the first MLLM framework for interpretable emotion analysis from EEG. E^2-LLM integrates a pretrained EEG encoder with Qwen-based LLMs through learnable projection layers, employing a multi-stage training pipeline that encompasses emotion-discriminative pretraining, cross-modal alignment, and instruction tuning with chain-of-thought reasoning. We design a comprehensive evaluation protocol covering basic emotion prediction, multi-task reasoning, and zero-shot scenario understanding. Experiments on the dataset across seven emotion categories demonstrate that E^2-LLM achieves excellent performance on emotion classification, with larger variants showing enhanced reliability and superior zero-shot generalization to complex reasoning scenarios. Our work establishes a new paradigm combining physiological signals with LLM reasoning capabilities, showing that model scaling improves both recognition accuracy and interpretable emotional understanding in affective computing.

</details>


### [169] [Sliced-Wasserstein Distribution Alignment Loss Improves the Ultra-Low-Bit Quantization of Large Language Models](https://arxiv.org/abs/2601.07878)
*Deyu Cao,Yixin Yin,Samin Aref*

Main category: cs.LG

TL;DR: 提出一种基于切片Wasserstein损失的分布感知校准方法，用于超低比特后训练量化，通过对齐全精度和量化模型的输出分布来提升性能，无需推理时额外计算开销。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型部署时存在资源效率低下的经济与环境成本问题。模型量化可改善能效和内存效率，但压缩到4比特以下会扭曲激活分布并降低性能。

Method: 引入切片Wasserstein损失函数进行分布感知校准，在随机线性投影下对齐全精度和量化模型的输出分布，与标准均方误差损失互补，不增加推理计算开销。

Result: 在OmniQuant和TesseraQ两种前沿方法上验证，在超低比特设置下持续改善困惑度和下游任务准确率。对LLaMA-2-7B恢复4.12-20.37%的准确率损失，对OPT-6.7B恢复0.93-7.65%，对LLaMA-2-13B恢复2.26-6.20%。

Conclusion: 分布对齐为前沿量化方法提供了简单有效的性能提升，可推动超低比特量化的极限。方法已在GitHub开源以促进未来研究。

Abstract: The benefits of most large language models come with steep and often hidden economic and environmental costs due to their resource usage inefficiency during deployment. Model quantization improves energy and memory efficiency through representing model parameters by lower-precision values. However, compression below 4-bits often distorts activation distributions and degrades performance. We address this challenge by introducing a sliced Wasserstein loss function for distribution-aware calibration in ultra-low-bit post-training quantization. The proposed loss aligns the output distributions of full-precision and quantized models under random linear projections, complementing standard mean-squared error loss without adding any computational overhead during inference. Our proposed loss function can be incorporated with any post-training quantization framework that has a retraining component. We demonstrate the performance gains of our proposed model by incorporating it with two frontier methods known as OmniQuant and TesseraQ. Compared to these two baselines, the proposed loss consistently improves both perplexity and downstream task accuracy across multiple ultra-low-bit settings. Our proposed loss function recovers 4.12-20.37% of the OmniQuant's lost accuracy on the language model LLaMA-2-7B, 0.93-7.65% on OPT-6.7B, and 2.26-6.20% on LLaMA-2-13B. TesseraQ's accuracy degradation is recovered by 3.63-7.63% in relative terms when augmented by our proposed loss function. Taken together, these results demonstrate that distributional alignment provides a simple yet effective performance boost that can push the limits of frontier quantization methods. Our method is available on GitHub to facilitate future progress in ultra-low-bit quantization.

</details>


### [170] [Max-Min Neural Network Operators For Approximation of Multivariate Functions](https://arxiv.org/abs/2601.07886)
*Abhishek Yadav,Uaday Singh,Feng Dai*

Main category: cs.LG

TL;DR: 提出并分析基于sigmoid激活函数的多元max-min神经网络算子，建立点态和一致收敛定理，通过连续模和多元广义绝对矩给出定量逼近阶估计


<details>
  <summary>Details</summary>
Motivation: 基于神经网络算子逼近理论的最新进展，特别是单变量max-min算子的成功，将其扩展到多元情形，构建更高效稳定的逼近工具

Method: 开发多元框架，提出由sigmoid函数激活的多元max-min神经网络算子，利用连续模和多元广义绝对矩进行理论分析

Result: 建立了点态和一致收敛定理，给出了逼近阶的定量估计，证明多元max-min算子结构在理论和应用上都是高效稳定的逼近工具

Conclusion: 多元max-min算子不仅具有代数优雅性，而且在理论和应用环境中都提供了高效稳定的逼近工具，扩展了神经网络算子的应用范围

Abstract: In this paper, we develop a multivariate framework for approximation by max-min neural network operators. Building on the recent advances in approximation theory by neural network operators, particularly, the univariate max-min operators, we propose and analyze new multivariate operators activated by sigmoidal functions. We establish pointwise and uniform convergence theorems and derive quantitative estimates for the order of approximation via modulus of continuity and multivariate generalized absolute moment. Our results demonstrate that multivariate max-min structure of operators, besides their algebraic elegance, provide efficient and stable approximation tools in both theoretical and applied settings.

</details>


### [171] [KVzap: Fast, Adaptive, and Faithful KV Cache Pruning](https://arxiv.org/abs/2601.07891)
*Simon Jegou,Maximilian Jeblick*

Main category: cs.LG

TL;DR: KVzap是一种快速、输入自适应的KV缓存压缩方法，在预填充和解码阶段都能工作，实现2-4倍压缩且精度损失可忽略


<details>
  <summary>Details</summary>
Motivation: 随着transformer语言模型上下文长度增长，KV缓存已成为推理的关键瓶颈。虽然已有许多KV缓存剪枝方法，但由于速度-精度权衡问题，尚未被主流推理引擎采用

Method: KVzap是KVzip的快速、输入自适应近似方法，在预填充和解码阶段都能工作，通过压缩KV缓存来减少内存占用

Result: 在Qwen3-8B、Llama-3.1-8B-Instruct和Qwen3-32B模型的长上下文和推理任务中，KVzap实现2-4倍KV缓存压缩，精度损失可忽略，在KVpress排行榜上达到最先进性能

Conclusion: KVzap提供了一种有效的KV缓存压缩解决方案，解决了推理瓶颈问题，代码和模型已开源

Abstract: Growing context lengths in transformer-based language models have made the key-value (KV) cache a critical inference bottleneck. While many KV cache pruning methods have been proposed, they have not yet been adopted in major inference engines due to speed--accuracy trade-offs. We introduce KVzap, a fast, input-adaptive approximation of KVzip that works in both prefilling and decoding. On Qwen3-8B, Llama-3.1-8B-Instruct, and Qwen3-32B across long-context and reasoning tasks, KVzap achieves $2$--$4\times$ KV cache compression with negligible accuracy loss and achieves state-of-the-art performance on the KVpress leaderboard. Code and models are available at https://github.com/NVIDIA/kvpress.

</details>


### [172] [Sherry: Hardware-Efficient 1.25-Bit Ternary Quantization via Fine-grained Sparsification](https://arxiv.org/abs/2601.07892)
*Hong Huang,Decheng Wu,Qiangqiang Hu,Guanghua Yu,Jinhai Yang,Jianchen Zhu,Xue Liu,Dapeng Wu*

Main category: cs.LG

TL;DR: Sherry是一个硬件高效的三值量化框架，通过3:4细粒度稀疏实现1.25位宽，解决了现有三值量化方法与硬件不匹配的问题，在保持性能的同时显著减小模型大小并提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在资源受限的边缘设备上部署面临内存和计算需求过高的挑战。现有三值量化方法存在硬件不匹配问题：2位对齐打包会造成比特浪费，1.67位不规则打包会降低推理速度。

Method: 提出Sherry框架，采用3:4细粒度稀疏实现1.25位宽，将四个权重打包到五个比特中，恢复2的幂次对齐。同时引入Arenas机制解决稀疏三值训练中的权重陷阱问题，通过退火残差突触机制保持表示多样性。

Result: 在LLaMA-3.2上的五个基准测试表明，Sherry匹配最先进的三值性能，同时显著减小模型大小。在Intel i7-14700HX CPU上，1B模型相比SOTA基线实现零精度损失，同时节省25%比特并提升10%速度。

Conclusion: Sherry通过硬件高效的1.25位三值量化解决了现有方法的硬件不匹配问题，在保持性能的同时实现了更好的存储效率和推理速度，为边缘设备部署LLM提供了有效解决方案。

Abstract: The deployment of Large Language Models (LLMs) on resource-constrained edge devices is increasingly hindered by prohibitive memory and computational requirements. While ternary quantization offers a compelling solution by reducing weights to {-1, 0, +1}, current implementations suffer from a fundamental misalignment with commodity hardware. Most existing methods must choose between 2-bit aligned packing, which incurs significant bit wastage, or 1.67-bit irregular packing, which degrades inference speed. To resolve this tension, we propose Sherry, a hardware-efficient ternary quantization framework. Sherry introduces a 3:4 fine-grained sparsity that achieves a regularized 1.25-bit width by packing blocks of four weights into five bits, restoring power-of-two alignment. Furthermore, we identify weight trapping issue in sparse ternary training, which leads to representational collapse. To address this, Sherry introduces Arenas, an annealing residual synapse mechanism that maintains representational diversity during training. Empirical evaluations on LLaMA-3.2 across five benchmarks demonstrate that Sherry matches state-of-the-art ternary performance while significantly reducing model size. Notably, on an Intel i7-14700HX CPU, our 1B model achieves zero accuracy loss compared to SOTA baselines while providing 25% bit savings and 10% speed up. The code is available at https://github.com/Tencent/AngelSlim .

</details>


### [173] [Revealing the Attention Floating Mechanism in Masked Diffusion Models](https://arxiv.org/abs/2601.07894)
*Xin Dai,Pengcheng Huang,Zhenghao Liu,Shuo Wang,Yukun Yan,Chaojun Xiao,Yu Gu,Ge Yu,Maosong Sun*

Main category: cs.LG

TL;DR: 本文研究了掩码扩散模型中的注意力机制，发现了"注意力浮动"现象，揭示了其浅层结构感知、深层内容聚焦的机制，这解释了MDMs在上下文学习中的优势。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散模型（MDMs）性能逐渐接近自回归模型（ARMs），但其内部注意力机制尚未得到充分研究。本文旨在探索MDMs中的注意力行为，特别是与ARMs的差异。

Method: 通过分析MDMs中的注意力机制，揭示了"注意力浮动"现象，并研究了其浅层结构感知、深层内容聚焦的注意力机制。

Result: 发现MDMs具有动态分散的注意力锚点，不同于ARMs的固定汇聚点。这种独特的注意力模式使MDMs在知识密集型任务中的性能比ARMs提高一倍。

Conclusion: MDMs的注意力浮动现象及其浅层结构感知、深层内容聚焦的机制，为MDMs强大的上下文学习能力提供了机制性解释，揭示了其相对于ARMs的优势。

Abstract: Masked diffusion models (MDMs), which leverage bidirectional attention and a denoising process, are narrowing the performance gap with autoregressive models (ARMs). However, their internal attention mechanisms remain under-explored. This paper investigates the attention behaviors in MDMs, revealing the phenomenon of Attention Floating. Unlike ARMs, where attention converges to a fixed sink, MDMs exhibit dynamic, dispersed attention anchors that shift across denoising steps and layers. Further analysis reveals its Shallow Structure-Aware, Deep Content-Focused attention mechanism: shallow layers utilize floating tokens to build a global structural framework, while deeper layers allocate more capability toward capturing semantic content. Empirically, this distinctive attention pattern provides a mechanistic explanation for the strong in-context learning capabilities of MDMs, allowing them to double the performance compared to ARMs in knowledge-intensive tasks. All codes and datasets are available at https://github.com/NEUIR/Attention-Floating.

</details>


### [174] [Large Language Models and Algorithm Execution: Application to an Arithmetic Function](https://arxiv.org/abs/2601.07898)
*Farah Ben Slama,Frédéric Armetta*

Main category: cs.LG

TL;DR: LLM-DAL训练方法通过推理分解显著提升大语言模型执行算法和泛化的能力


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然功能强大，但存在局限性：难以内化处理的数据，无法自主执行算法。研究者希望扩展LLM的算法执行能力。

Method: 提出LLM-DAL训练模型，通过专门设计的监督训练方法，专注于推理分解，引导模型在学习过程中掌握算法执行。

Result: 当训练方法经过适当设计以指导模型学习过程时，LLMs执行复杂算法推理和泛化的能力可以显著提升。

Conclusion: 通过专门的推理分解训练，可以扩展大语言模型的算法执行能力，改善其内部化数据和自主执行算法的局限性。

Abstract: Large Language Models (LLMs) have recently developed new advanced functionalities. Their effectiveness relies on statistical learning and generalization capabilities. However, they face limitations in internalizing the data they process and struggle, for instance, to autonomously execute algorithms. In this paper, we investigate the possibility of extending these models' capabilities to algorithm execution through specialized supervised training focused on reasoning decomposition. We introduce a training model called LLM-DAL (Large Language Model - Decompositional Algorithmic Learning), through which we demonstrate that LLMs' ability to perform complex algorithmic inferences and generalize can be significantly improved when the training method is properly designed to guide the model in its learning process.

</details>


### [175] [Enhancing Large Language Models for Time-Series Forecasting via Vector-Injected In-Context Learning](https://arxiv.org/abs/2601.07903)
*Jianqi Zhang,Jingyao Wang,Wenwen Qiang,Fanjiang Xu,Changwen Zheng*

Main category: cs.LG

TL;DR: 提出LVICL方法，通过向量注入的上下文学习提升大语言模型在时间序列预测中的性能，同时冻结所有LLM参数以减少计算开销


<details>
  <summary>Details</summary>
Motivation: LLM直接应用于时间序列预测存在预训练语料与时间序列数据差异大的问题，影响预测质量；微调LLM可缓解但计算开销大。需要解决预测性能与计算开销的双重挑战

Method: 提出LVICL（向量注入的上下文学习）方法：1）使用LLM和可学习的上下文向量适配器从多个示例中自适应提取包含压缩示例信息的上下文向量；2）在前向传播中将该向量注入LLM的每一层，激发其上下文学习能力

Result: 相比传统上下文学习，向量注入方法不增加提示长度，且能抑制对预测有害的组件，提升模型性能。大量实验验证了方法的有效性

Conclusion: LVICL方法能在冻结所有LLM参数的情况下，通过向量注入的上下文学习有效提升时间序列预测性能，同时减少计算开销，解决了LLM4TSF面临的双重挑战

Abstract: The World Wide Web needs reliable predictive capabilities to respond to changes in user behavior and usage patterns. Time series forecasting (TSF) is a key means to achieve this goal. In recent years, the large language models (LLMs) for TSF (LLM4TSF) have achieved good performance. However, there is a significant difference between pretraining corpora and time series data, making it hard to guarantee forecasting quality when directly applying LLMs to TSF; fine-tuning LLMs can mitigate this issue, but often incurs substantial computational overhead. Thus, LLM4TSF faces a dual challenge of prediction performance and compute overhead. To address this, we aim to explore a method for improving the forecasting performance of LLM4TSF while freezing all LLM parameters to reduce computational overhead. Inspired by in-context learning (ICL), we propose LVICL. LVICL uses our vector-injected ICL to inject example information into a frozen LLM, eliciting its in-context learning ability and thereby enhancing its performance on the example-related task (i.e., TSF). Specifically, we first use the LLM together with a learnable context vector adapter to extract a context vector from multiple examples adaptively. This vector contains compressed, example-related information. Subsequently, during the forward pass, we inject this vector into every layer of the LLM to improve forecasting performance. Compared with conventional ICL that adds examples into the prompt, our vector-injected ICL does not increase prompt length; moreover, adaptively deriving a context vector from examples suppresses components harmful to forecasting, thereby improving model performance. Extensive experiments demonstrate the effectiveness of our approach.

</details>


### [176] [Transformer-Based Approach for Automated Functional Group Replacement in Chemical Compounds](https://arxiv.org/abs/2601.07930)
*Bo Pan,Zhiping Zhang,Kevin Spiekermann,Tianchi Chen,Xiang Yu,Liying Zhang,Liang Zhao*

Main category: cs.LG

TL;DR: 提出一种新颖的两阶段Transformer模型，用于功能基团的移除和替换，通过顺序生成要移除和添加的基团，确保严格的子结构级修改。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的功能基团替换方法在生成多样化和新颖化学结构方面有限，现有Transformer方法通常专注于单步建模且缺乏结构相似性保证。

Method: 开发两阶段Transformer模型，使用基于SMIRKS的表示在ChEMBL的匹配分子对数据集上训练编码器-解码器架构，顺序生成要移除和添加的功能基团。

Result: 该方法能够生成化学有效的转化，探索多样化的化学空间，并在不同搜索规模下保持可扩展性。

Conclusion: 该两阶段Transformer方法在功能基团替换任务上超越了传统规则方法和单步建模方法，实现了更精确和多样化的分子设计。

Abstract: Functional group replacement is a pivotal approach in cheminformatics to enable the design of novel chemical compounds with tailored properties. Traditional methods for functional group removal and replacement often rely on rule-based heuristics, which can be limited in their ability to generate diverse and novel chemical structures. Recently, transformer-based models have shown promise in improving the accuracy and efficiency of molecular transformations, but existing approaches typically focus on single-step modeling, lacking the guarantee of structural similarity. In this work, we seek to advance the state of the art by developing a novel two-stage transformer model for functional group removal and replacement. Unlike one-shot approaches that generate entire molecules in a single pass, our method generates the functional group to be removed and appended sequentially, ensuring strict substructure-level modifications. Using a matched molecular pairs (MMPs) dataset derived from ChEMBL, we trained an encoder-decoder transformer model with SMIRKS-based representations to capture transformation rules effectively. Extensive evaluations demonstrate our method's ability to generate chemically valid transformations, explore diverse chemical spaces, and maintain scalability across varying search sizes.

</details>


### [177] [Towards Specialized Generalists: A Multi-Task MoE-LoRA Framework for Domain-Specific LLM Adaptation](https://arxiv.org/abs/2601.07935)
*Yuxin Yang,Aoxiong Zeng,Xiangquan Yang*

Main category: cs.LG

TL;DR: Med-MoE-LoRA：结合MoE与LoRA的医疗领域自适应框架，解决稳定性-可塑性困境和任务干扰问题，在医疗NLP任务中表现优异


<details>
  <summary>Details</summary>
Motivation: LLM适应医疗等专业领域面临两大挑战：1) 稳定性-可塑性困境 - 需要学习复杂临床知识但不能遗忘通用知识；2) 任务干扰 - 医疗诊断、报告总结、药物相互作用预测等不同子任务在有限参数空间中相互竞争

Method: 提出Med-MoE-LoRA框架，集成混合专家(MoE)与低秩自适应(LoRA)：1) 采用非对称专家分布，深层配备更多LoRA专家以捕获复杂语义抽象；2) 引入"知识保护插件"隔离保护通用推理能力；3) 使用软合并、自适应路由和秩解耦技术

Result: 实验结果显示，该方法在多个临床NLP任务中持续优于标准LoRA和传统MoE架构，同时保持模型的通用认知能力，减少了任务干扰

Conclusion: Med-MoE-LoRA通过结合MoE和LoRA的优势，有效解决了医疗领域自适应中的稳定性-可塑性困境和任务干扰问题，为专业领域LLM适应提供了高效解决方案

Abstract: The rapid evolution of Large Language Models (LLMs) has shifted focus from general-purpose capabilities to domain-specific expertise. However, adapting LLMs to specialized fields such as medicine presents two challenge: (1) the "Stability-Plasticity Dilemma", where the model must acquire complex clinical knowledge without suffering from catastrophic forgetting of general world knowledge; and (2) "Task Interference", where disparate sub-tasks, such as medical diagnosis, report summarization, and drug-drug interaction prediction, compete for limited low-rank parameter space. In this paper, we propose Med-MoE-LoRA, a novel framework that integrates Mixture-of-Experts (MoE) with Low-Rank Adaptation (LoRA) to enable efficient multi-task domain adaptation, especially for medical scenarios. Drawing inspiration from recent advances, our framework employs an asymmetric expert distribution where deeper layers are equipped with a higher density of LoRA experts to capture complex semantic abstractions. We further introduce a "Knowledge-Preservation Plugin", inspired by LoRA MoE, to isolate and protect general-purpose reasoning. By utilizing soft merging with adaptive routing and rank-wise decoupling, Med-MoE-LoRA achieves superior performance in medical benchmarks while reducing interference. Experimental results demonstrate that our approach consistently outperforms standard LoRA and conventional MoE architectures across multiple clinical NLP tasks while retaining the model's general cognitive capabilities.

</details>


### [178] [STO-RL: Offline RL under Sparse Rewards via LLM-Guided Subgoal Temporal Order](https://arxiv.org/abs/2601.08107)
*Chengyang Gu,Yuxin Pan,Hui Xiong,Yize Chen*

Main category: cs.LG

TL;DR: STO-RL：利用LLM生成时序子目标序列的离线强化学习框架，通过基于势能的奖励塑形解决稀疏奖励长时程任务


<details>
  <summary>Details</summary>
Motivation: 现有离线RL方法在处理稀疏奖励的长时程任务时存在局限：传统离线RL难以处理稀疏奖励；现有目标条件和分层方法虽然分解任务并生成中间奖励，但忽略了子目标间的时序依赖，且依赖不精确的奖励塑形，导致策略次优。

Method: 提出STO-RL框架：1）利用大语言模型（LLM）生成时序有序的子目标序列和状态到子目标阶段的映射；2）使用时序结构应用基于势能的奖励塑形，将稀疏的终端奖励转化为密集、时序一致的信号；3）使用增强后的数据集（包含塑形奖励）高效训练高性能策略。

Result: 在四个离散和连续稀疏奖励基准测试中，STO-RL始终优于最先进的离线目标条件和分层RL基线，实现更快的收敛速度、更高的成功率和更短的轨迹。消融研究证实STO-RL对不完美或有噪声的LLM生成子目标序列具有鲁棒性。

Conclusion: LLM引导的子目标时序结构与理论基础的奖励塑形相结合，为长时程离线RL提供了实用且可扩展的解决方案，能够有效处理稀疏奖励任务并生成高质量策略。

Abstract: Offline reinforcement learning (RL) enables policy learning from pre-collected datasets, avoiding costly and risky online interactions, but it often struggles with long-horizon tasks involving sparse rewards. Existing goal-conditioned and hierarchical offline RL methods decompose such tasks and generate intermediate rewards to mitigate limitations of traditional offline RL, but usually overlook temporal dependencies among subgoals and rely on imprecise reward shaping, leading to suboptimal policies. To address these issues, we propose STO-RL (Offline RL using LLM-Guided Subgoal Temporal Order), an offline RL framework that leverages large language models (LLMs) to generate temporally ordered subgoal sequences and corresponding state-to-subgoal-stage mappings. Using this temporal structure, STO-RL applies potential-based reward shaping to transform sparse terminal rewards into dense, temporally consistent signals, promoting subgoal progress while avoiding suboptimal solutions. The resulting augmented dataset with shaped rewards enables efficient offline training of high-performing policies. Evaluations on four discrete and continuous sparse-reward benchmarks demonstrate that STO-RL consistently outperforms state-of-the-art offline goal-conditioned and hierarchical RL baselines, achieving faster convergence, higher success rates, and shorter trajectories. Ablation studies further confirm STO-RL's robustness to imperfect or noisy LLM-generated subgoal sequences, demonstrating that LLM-guided subgoal temporal structures combined with theoretically grounded reward shaping provide a practical and scalable solution for long-horizon offline RL.

</details>


### [179] [Coupled Diffusion-Encoder Models for Reconstruction of Flow Fields](https://arxiv.org/abs/2601.07946)
*AmirPouya Hemmasian,Amir Barati Farimani*

Main category: cs.LG

TL;DR: DiffCoder：结合扩散模型与卷积编码器的流场重建框架，在强压缩下比传统VAE更好地保持流场的统计特性


<details>
  <summary>Details</summary>
Motivation: 传统变分自编码器（VAE）在强压缩流场数据时难以保持高阶统计结构，需要一种能更好保留流场分布和谱特性的方法

Method: 提出DiffCoder框架，将概率扩散模型与卷积ResNet编码器耦合，端到端训练。编码器压缩流场为潜在表示，扩散模型学习基于压缩状态的生成先验

Result: 在Kolmogorov流场数据集上测试，强压缩下DiffCoder显著改善谱精度，而VAE严重退化。两者L2重建误差相当，但DiffCoder更好地保持了流场分布结构

Conclusion: 扩散模型的生成解码为复杂流场的紧凑、统计一致表示提供了有前景的路径，在信息瓶颈严重时优势最明显

Abstract: Data-driven flow-field reconstruction typically relies on autoencoder architectures that compress high-dimensional states into low-dimensional latent representations. However, classical approaches such as variational autoencoders (VAEs) often struggle to preserve the higher-order statistical structure of fluid flows when subjected to strong compression. We propose DiffCoder, a coupled framework that integrates a probabilistic diffusion model with a conventional convolutional ResNet encoder and trains both components end-to-end. The encoder compresses the flow field into a latent representation, while the diffusion model learns a generative prior over reconstructions conditioned on the compressed state. This design allows DiffCoder to recover distributional and spectral properties that are not strictly required for minimizing pointwise reconstruction loss but are critical for faithfully representing statistical properties of the flow field. We evaluate DiffCoder and VAE baselines across multiple model sizes and compression ratios on a challenging dataset of Kolmogorov flow fields. Under aggressive compression, DiffCoder significantly improves the spectral accuracy while VAEs exhibit substantial degradation. Although both methods show comparable relative L2 reconstruction error, DiffCoder better preserves the underlying distributional structure of the flow. At moderate compression levels, sufficiently large VAEs remain competitive, suggesting that diffusion-based priors provide the greatest benefit when information bottlenecks are severe. These results demonstrate that the generative decoding by diffusion offers a promising path toward compact, statistically consistent representations of complex flow fields.

</details>


### [180] [Reverse Flow Matching: A Unified Framework for Online Reinforcement Learning with Diffusion and Flow Policies](https://arxiv.org/abs/2601.08136)
*Zeyang Li,Sunbochen Tang,Navid Azizan*

Main category: cs.LG

TL;DR: 提出统一框架Reverse Flow Matching (RFM)，用于训练扩散和流策略，通过后验均值估计和Langevin Stein算子减少重要性采样方差，将现有噪声期望和梯度期望方法统一为特例。


<details>
  <summary>Details</summary>
Motivation: 扩散和流策略在在线强化学习中表达能力强，但缺乏直接目标分布样本，现有噪声期望和梯度期望方法关系不明确，需要统一框架来提升训练效率。

Method: 采用反向推理视角，将训练目标构建为给定中间噪声样本的后验均值估计问题，引入Langevin Stein算子构造零均值控制变量，推导出减少重要性采样方差的通用估计器类。

Result: RFM统一了噪声期望和梯度期望方法，扩展了玻尔兹曼分布目标到流策略，结合Q值和Q梯度信息得到最优最小方差估计器，在连续控制基准测试中优于扩散策略基线。

Conclusion: RFM为训练扩散和流策略提供了统一框架，通过理论统一和方差减少机制提升了训练效率和稳定性，推动了在线强化学习中表达性策略的训练方法发展。

Abstract: Diffusion and flow policies are gaining prominence in online reinforcement learning (RL) due to their expressive power, yet training them efficiently remains a critical challenge. A fundamental difficulty in online RL is the lack of direct samples from the target distribution; instead, the target is an unnormalized Boltzmann distribution defined by the Q-function. To address this, two seemingly distinct families of methods have been proposed for diffusion policies: a noise-expectation family, which utilizes a weighted average of noise as the training target, and a gradient-expectation family, which employs a weighted average of Q-function gradients. Yet, it remains unclear how these objectives relate formally or if they can be synthesized into a more general formulation. In this paper, we propose a unified framework, reverse flow matching (RFM), which rigorously addresses the problem of training diffusion and flow models without direct target samples. By adopting a reverse inferential perspective, we formulate the training target as a posterior mean estimation problem given an intermediate noisy sample. Crucially, we introduce Langevin Stein operators to construct zero-mean control variates, deriving a general class of estimators that effectively reduce importance sampling variance. We show that existing noise-expectation and gradient-expectation methods are two specific instances within this broader class. This unified view yields two key advancements: it extends the capability of targeting Boltzmann distributions from diffusion to flow policies, and enables the principled combination of Q-value and Q-gradient information to derive an optimal, minimum-variance estimator, thereby improving training efficiency and stability. We instantiate RFM to train a flow policy in online RL, and demonstrate improved performance on continuous-control benchmarks compared to diffusion policy baselines.

</details>


### [181] [Reinforcement Learning Methods for Neighborhood Selection in Local Search](https://arxiv.org/abs/2601.07948)
*Yannick Molinghen,Augustin Delecluse,Renaud De Landtsheer,Stefano Michelini*

Main category: cs.LG

TL;DR: 该研究评估了多种基于强化学习的邻域选择策略在局部搜索元启发式中的应用，发现ε-greedy方法表现稳定最佳，而深度强化学习方法因计算开销大仅在有充足运行时间时才具竞争力。


<details>
  <summary>Details</summary>
Motivation: 强化学习在组合优化中已有应用，但在局部搜索元启发式中的有效性尚未充分研究。本研究旨在评估不同强化学习策略在局部搜索中的表现，探索其实际应用潜力与限制。

Method: 评估了多种强化学习邻域选择策略：多臂老虎机方法（UCB、ε-greedy）和深度强化学习方法（PPO、DDQN）。在三个不同问题上进行测试：旅行商问题、带时间窗的取送货问题、车辆排序问题。特别设计了适应搜索特性的奖励函数来处理约束违反惩罚带来的成本大幅波动。

Result: 实验显示算法性能在不同问题间差异显著，但ε-greedy始终表现最佳。深度强化学习方法因计算开销大，只有在运行时间显著延长时才具有竞争力。研究还发现搜索特性（特别是约束违反惩罚导致的大成本波动）需要精心设计奖励函数来提供稳定有效的学习信号。

Conclusion: 研究揭示了深度强化学习在局部搜索中的潜力与实用限制。虽然ε-greedy等简单方法表现稳定优秀，但深度强化学习方法需要解决计算效率问题才能在实际应用中发挥优势。研究强调了奖励函数设计对学习稳定性的重要性。

Abstract: Reinforcement learning has recently gained traction as a means to improve combinatorial optimization methods, yet its effectiveness within local search metaheuristics specifically remains comparatively underexamined. In this study, we evaluate a range of reinforcement learning-based neighborhood selection strategies -- multi-armed bandits (upper confidence bound, $ε$-greedy) and deep reinforcement learning methods (proximal policy optimization, double deep $Q$-network) -- and compare them against multiple baselines across three different problems: the traveling salesman problem, the pickup and delivery problem with time windows, and the car sequencing problem. We show how search-specific characteristics, particularly large variations in cost due to constraint violation penalties, necessitate carefully designed reward functions to provide stable and informative learning signals. Our extensive experiments reveal that algorithm performance varies substantially across problems, although that $ε$-greedy consistently ranks among the best performers. In contrast, the computational overhead of deep reinforcement learning approaches only makes them competitive with a substantially longer runtime. These findings highlight both the promise and the practical limitations of deep reinforcement learning in local search.

</details>


### [182] [Hybrid SARIMA LSTM Model for Local Weather Forecasting: A Residual Learning Approach for Data Driven Meteorological Prediction](https://arxiv.org/abs/2601.07951)
*Shreyas Rajeev,Karthik Mudenahalli Ashoka,Amit Mallappa Tiparaddi*

Main category: cs.LG

TL;DR: 提出混合SARIMA-LSTM架构，通过残差学习策略分解温度数据为可预测的气候成分和非线性天气成分，结合统计稳定性与神经可塑性，提升长期温度预测精度。


<details>
  <summary>Details</summary>
Motivation: 长期大气变量预测面临挑战：温度数据包含确定性周期气候力和随机短期波动。传统SARIMA模型能捕捉线性季节趋势但无法处理非线性突变，而LSTM虽擅长处理复杂时间序列但在开环预测中误差会累积发散。

Method: 提出混合SARIMA-LSTM架构，采用残差学习策略：SARIMA单元建模长期季节趋势，LSTM专门训练SARIMA无法捕捉的非线性残差误差，融合统计稳定性和神经可塑性。

Result: 该方法最小化误差传播，增强长期预测准确性，解决了SARIMA对非线性突变预测不足和LSTM开环预测不稳定的问题。

Conclusion: 混合SARIMA-LSTM架构通过分解温度数据的可预测成分和非线性成分，结合两种方法的优势，为长期温度预测提供了更稳健准确的解决方案。

Abstract: Accurately forecasting long-term atmospheric variables remains a defining challenge in meteorological science due to the chaotic nature of atmospheric systems. Temperature data represents a complex superposition of deterministic cyclical climate forces and stochastic, short-term fluctuations. While planetary mechanics drive predictable seasonal periodicities, rapid meteorological changes such as thermal variations, pressure anomalies, and humidity shifts introduce nonlinear volatilities that defy simple extrapolation. Historically, the Seasonal Autoregressive Integrated Moving Average (SARIMA) model has been the standard for modeling historical weather data, prized for capturing linear seasonal trends. However, SARIMA operates under strict assumptions of stationarity, failing to capture abrupt, nonlinear transitions. This leads to systematic residual errors, manifesting as the under-prediction of sudden spikes or the over-smoothing of declines. Conversely, Deep Learning paradigms, specifically Long Short-Term Memory (LSTM) networks, demonstrate exceptional efficacy in handling intricate time-series data. By utilizing memory gates, LSTMs learn complex nonlinear dependencies. Yet, LSTMs face instability in open-loop forecasting; without ground truth feedback, minor deviations compound recursively, causing divergence. To resolve these limitations, we propose a Hybrid SARIMA-LSTM architecture. This framework employs a residual-learning strategy to decompose temperature into a predictable climate component and a nonlinear weather component. The SARIMA unit models the robust, long-term seasonal trend, while the LSTM is trained exclusively on the residuals the nonlinear errors SARIMA fails to capture. By fusing statistical stability with neural plasticity, this hybrid approach minimizes error propagation and enhances long-horizon accuracy.

</details>


### [183] [Riemannian Zeroth-Order Gradient Estimation with Structure-Preserving Metrics for Geodesically Incomplete Manifolds](https://arxiv.org/abs/2601.08039)
*Shaocong Ma,Heng Huang*

Main category: cs.LG

TL;DR: 研究黎曼零阶优化在测地不完备情况下的解决方案，通过构造保持结构的完备度量，建立内在分析框架，实现与完备情况相同的最优复杂度。


<details>
  <summary>Details</summary>
Motivation: 在黎曼优化中，当底层黎曼度量测地不完备时，传统的优化方法面临挑战。本文旨在解决在不完备度量下近似驻点的问题，同时保持理论保证。

Method: 构造保持结构的测地完备度量，使新度量下的驻点保持为原度量下的驻点。从纯内在视角分析对称两点零阶估计器的均方误差，建立随机梯度下降的收敛保证。

Result: 在适当条件下，新度量下的ε-驻点对应原度量下的ε-驻点，达到测地完备情况下的最优复杂度。实验验证了理论结果，并在实际网格优化任务中展示了稳定收敛性。

Conclusion: 本文提出了一种处理测地不完备黎曼优化的框架，通过构造结构保持的完备度量，实现了与完备情况相同的理论保证，并在实践中表现出稳定性能。

Abstract: In this paper, we study Riemannian zeroth-order optimization in settings where the underlying Riemannian metric $g$ is geodesically incomplete, and the goal is to approximate stationary points with respect to this incomplete metric. To address this challenge, we construct structure-preserving metrics that are geodesically complete while ensuring that every stationary point under the new metric remains stationary under the original one. Building on this foundation, we revisit the classical symmetric two-point zeroth-order estimator and analyze its mean-squared error from a purely intrinsic perspective, depending only on the manifold's geometry rather than any ambient embedding. Leveraging this intrinsic analysis, we establish convergence guarantees for stochastic gradient descent with this intrinsic estimator. Under additional suitable conditions, an $ε$-stationary point under the constructed metric $g'$ also corresponds to an $ε$-stationary point under the original metric $g$, thereby matching the best-known complexity in the geodesically complete setting. Empirical studies on synthetic problems confirm our theoretical findings, and experiments on a practical mesh optimization task demonstrate that our framework maintains stable convergence even in the absence of geodesic completeness.

</details>


### [184] [DataScribe: An AI-Native, Policy-Aligned Web Platform for Multi-Objective Materials Design and Discovery](https://arxiv.org/abs/2601.07966)
*Divyanshu Singh,Doguhan Sarıtürk,Cameron Lea,Md Shafiqul Islam,Raymundo Arroyave,Vahid Attari*

Main category: cs.LG

TL;DR: DataScribe是一个AI原生的云端材料发现平台，通过本体知识图谱统一异构数据，集成FAIR元数据、不确定性建模和多目标贝叶斯优化，实现实验与计算的闭环工作流。


<details>
  <summary>Details</summary>
Motivation: 加速材料发现需要超越传统数据仓库的数字平台，将学习、优化和决策直接嵌入研究流程，解决异构数据整合和闭环工作流自动化的问题。

Method: 采用本体支持的数据摄取和机器可操作知识图谱统一异构数据，集成FAIR元数据捕获、模式与单位协调、不确定性感知代理模型、多目标多保真度贝叶斯优化，构建应用层智能栈。

Result: 通过电化学材料和高熵合金案例验证，展示了端到端数据融合、实时优化和多目标权衡空间的可重复探索能力，支持从实验室到自主实验室的各种规模应用。

Conclusion: DataScribe作为通用应用层骨干平台，通过将优化引擎、机器学习和统一数据访问直接嵌入基础设施，为材料发现提供了可扩展的闭环工作流解决方案，支持学术和非营利研究。

Abstract: The acceleration of materials discovery requires digital platforms that go beyond data repositories to embed learning, optimization, and decision-making directly into research workflows. We introduce DataScribe, an AI-native, cloud-based materials discovery platform that unifies heterogeneous experimental and computational data through ontology-backed ingestion and machine-actionable knowledge graphs. The platform integrates FAIR-compliant metadata capture, schema and unit harmonization, uncertainty-aware surrogate modeling, and native multi-objective multi-fidelity Bayesian optimization, enabling closed-loop propose-measure-learn workflows across experimental and computational pipelines. DataScribe functions as an application-layer intelligence stack, coupling data governance, optimization, and explainability rather than treating them as downstream add-ons. We validate the platform through case studies in electrochemical materials and high-entropy alloys, demonstrating end-to-end data fusion, real-time optimization, and reproducible exploration of multi-objective trade spaces. By embedding optimization engines, machine learning, and unified access to public and private scientific data directly within the data infrastructure, and by supporting open, free use for academic and non-profit researchers, DataScribe functions as a general-purpose application-layer backbone for laboratories of any scale, including self-driving laboratories and geographically distributed materials acceleration platforms, with built-in support for performance, sustainability, and supply-chain-aware objectives.

</details>


### [185] [Beyond the Next Port: A Multi-Task Transformer for Forecasting Future Voyage Segment Durations](https://arxiv.org/abs/2601.08013)
*Nairui Liu,Fang He,Xindi Tang*

Main category: cs.LG

TL;DR: 提出基于Transformer的架构，用于预测未来航段航行时间，整合历史航行时长、目的港拥堵代理和静态船舶描述符，相比基线模型在MAE和MAPE上均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统ETA模型主要针对下一个停靠港，且严重依赖实时AIS数据，而未来航段无法获得实时数据。需要解决未来港口ETA预测的问题，以提升海运计划可靠性和优化港口长期运营。

Method: 将未来港口ETA预测重构为航段时间序列预测问题，开发基于Transformer的架构，整合历史航行时长、目的港拥堵代理和静态船舶描述符。采用因果掩码注意力机制捕获长程时间依赖，多任务学习头联合预测航段航行时间和港口拥堵状态，利用共享潜在信号缓解高不确定性。

Result: 在2021年全球真实数据集上评估，模型持续优于一系列竞争基线。相比序列基线模型，MAE相对降低4.85%，MAPE相对降低4.95%；相比梯度提升机，MAE相对降低9.39%，MAPE相对降低52.97%。主要目的港的案例研究进一步展示了模型的优越准确性。

Conclusion: 该研究成功解决了未来航段ETA预测的挑战，提出的Transformer架构通过整合多源数据和采用多任务学习，显著提升了预测准确性，为海运计划可靠性和港口运营优化提供了有效解决方案。

Abstract: Accurate forecasts of segment-level sailing durations are fundamental to enhancing maritime schedule reliability and optimizing long-term port operations. However, conventional estimated time of arrival (ETA) models are primarily designed for the immediate next port of call and rely heavily on real-time automatic identification system (AIS) data, which is inherently unavailable for future voyage segments. To address this gap, the study reformulates future-port ETA prediction as a segment-level time-series forecasting problem. We develop a transformer-based architecture that integrates historical sailing durations, destination port congestion proxies, and static vessel descriptors. The proposed framework employs a causally masked attention mechanism to capture long-range temporal dependencies and a multi-task learning head to jointly predict segment sailing durations and port congestion states, leveraging shared latent signals to mitigate high uncertainty. Evaluation on a real-world global dataset from 2021 demonstrates the proposed model consistently outperforms a comprehensive suite of competitive baselines. The result shows a relative reduction of 4.85% in mean absolute error (MAE) and 4.95% in mean absolute percentage error (MAPE) compared with sequence baseline models. The relative reductions with gradient boosting machines are 9.39% in MAE and 52.97% in MAPE. Case studies for the major destination port further illustrate the model's superior accuracy.

</details>


### [186] [InfGraND: An Influence-Guided GNN-to-MLP Knowledge Distillation](https://arxiv.org/abs/2601.08033)
*Amir Eskandari,Aman Anand,Elyas Rashno,Farhana Zulkernine*

Main category: cs.LG

TL;DR: InfGraND：一种从GNN到MLP的影响引导图知识蒸馏框架，通过识别结构重要节点指导蒸馏过程，并在MLP中嵌入结构感知能力，在保持高效推理的同时提升性能。


<details>
  <summary>Details</summary>
Motivation: GNN虽然在图数据分析中表现优异，但其聚合和更新操作在低延迟推理任务或资源受限场景中存在挑战。MLP计算效率高但监督训练性能欠佳。现有知识蒸馏方法要么对所有节点均匀转移知识，要么依赖图无关指标，忽略了节点对图结构的重要性这一根本问题。

Method: InfGraND框架包含两个核心组件：1）识别并优先处理结构重要节点来指导蒸馏过程，确保MLP从图的最关键部分学习；2）通过一次性多跳邻域特征预计算在MLP中嵌入结构感知，丰富学生MLP的输入同时避免推理时开销。

Result: 在七个同质图基准数据集上的转导和归纳设置中进行严格评估，InfGraND始终优于先前的GNN到MLP知识蒸馏方法，证明了其在现实世界众多延迟关键应用中的实用性。

Conclusion: InfGraND通过关注节点对图结构的重要性，提供了一种更有效的从GNN到MLP的知识蒸馏方法，在保持MLP计算效率优势的同时显著提升性能，适用于现实世界的低延迟应用场景。

Abstract: Graph Neural Networks (GNNs) are the go-to model for graph data analysis. However, GNNs rely on two key operations - aggregation and update, which can pose challenges for low-latency inference tasks or resource-constrained scenarios. Simple Multi-Layer Perceptrons (MLPs) offer a computationally efficient alternative. Yet, training an MLP in a supervised setting often leads to suboptimal performance. Knowledge Distillation (KD) from a GNN teacher to an MLP student has emerged to bridge this gap. However, most KD methods either transfer knowledge uniformly across all nodes or rely on graph-agnostic indicators such as prediction uncertainty. We argue this overlooks a more fundamental, graph-centric inquiry: "How important is a node to the structure of the graph?" We introduce a framework, InfGraND, an Influence-guided Graph KNowledge Distillation from GNN to MLP that addresses this by identifying and prioritizing structurally influential nodes to guide the distillation process, ensuring that the MLP learns from the most critical parts of the graph. Additionally, InfGraND embeds structural awareness in MLPs through one-time multi-hop neighborhood feature pre-computation, which enriches the student MLP's input and thus avoids inference-time overhead. Our rigorous evaluation in transductive and inductive settings across seven homophilic graph benchmark datasets shows InfGraND consistently outperforms prior GNN to MLP KD methods, demonstrating its practicality for numerous latency-critical applications in real-world settings.

</details>


### [187] [LUT-Compiled Kolmogorov-Arnold Networks for Lightweight DoS Detection on IoT Edge Devices](https://arxiv.org/abs/2601.08044)
*Oleksandr Kuznetsov*

Main category: cs.LG

TL;DR: 提出基于查找表(LUT)编译的轻量级KAN模型，用于物联网DoS攻击检测，在保持高准确率的同时显著降低推理延迟


<details>
  <summary>Details</summary>
Motivation: 物联网DoS攻击检测需要部署在资源受限的边缘设备上，但传统KAN模型中的B样条计算开销大，不适合延迟敏感的IoT应用

Method: 提出查找表编译流水线，用预计算的量化表和线性插值替代昂贵的样条计算，大幅减少推理延迟同时保持检测质量

Result: 轻量级KAN模型(50K参数)在CICIDS2017数据集上达到99.0%准确率；LUT编译后保持98.96%准确率，在批量大小256时获得68倍加速，批量大小1时超过5000倍加速，仅2倍内存开销

Conclusion: LUT编译的KAN模型能够在仅CPU的物联网网关上实现实时DoS检测，具有确定性推理延迟和最小资源占用，为准确率-延迟-内存权衡提供了清晰的帕累托前沿

Abstract: Denial-of-Service (DoS) attacks pose a critical threat to Internet of Things (IoT) ecosystems, yet deploying effective intrusion detection on resource-constrained edge devices remains challenging. Kolmogorov-Arnold Networks (KANs) offer a compact alternative to Multi-Layer Perceptrons (MLPs) by placing learnable univariate spline functions on edges rather than fixed activations on nodes, achieving competitive accuracy with fewer parameters. However, runtime B-spline evaluation introduces significant computational overhead unsuitable for latency-critical IoT applications. We propose a lookup table (LUT) compilation pipeline that replaces expensive spline computations with precomputed quantized tables and linear interpolation, dramatically reducing inference latency while preserving detection quality. Our lightweight KAN model (50K parameters, 0.19~MB) achieves 99.0\% accuracy on the CICIDS2017 DoS dataset. After LUT compilation with resolution $L=8$, the model maintains 98.96\% accuracy (F1 degradation $<0.0004$) while achieving $\mathbf{68\times}$ speedup at batch size 256 and over $\mathbf{5000\times}$ speedup at batch size 1, with only $2\times$ memory overhead. We provide comprehensive evaluation across LUT resolutions, quantization schemes, and out-of-bounds policies, establishing clear Pareto frontiers for accuracy-latency-memory trade-offs. Our results demonstrate that LUT-compiled KANs enable real-time DoS detection on CPU-only IoT gateways with deterministic inference latency and minimal resource footprint.

</details>


### [188] [Q-realign: Piggybacking Realignment on Quantization for Safe and Efficient LLM Deployment](https://arxiv.org/abs/2601.08089)
*Qitao Tan,Xiaoying Song,Ningxi Cheng,Ninghao Liu,Xiaoming Zhai,Lingzi Hong,Yanzhi Wang,Zhen Xiang,Geng Yuan*

Main category: cs.LG

TL;DR: Q-realign：一种基于后训练量化的后处理防御方法，通过将量化重构为压缩和安全性的双重目标，在保持任务性能的同时大幅减少微调模型的不安全行为，且计算成本低。


<details>
  <summary>Details</summary>
Motivation: 公共大语言模型通常在预训练时进行安全对齐，但部署所需的任务特定微调往往会破坏这种对齐并引入安全风险。现有防御方法要么将安全恢复嵌入微调过程，要么依赖微调先验进行后处理校正，导致安全恢复与训练紧密耦合，计算开销高且流程复杂。

Method: 提出Q-realign方法，基于后训练量化，通过分析表示结构，将量化重构为压缩和安全性的双重目标过程。该方法将安全对齐与微解脱耦，并自然地融入现代部署流程。

Result: 在多个模型和数据集上的实验表明，该方法在保持任务性能的同时显著减少了不安全行为，并大幅降低了内存使用和GPU时间。例如，在单个RTX 4090上40分钟内即可恢复一个微调7B LLM的安全对齐。

Conclusion: 该工作为安全感知部署提供了一个实用、即插即用的解决方案，通过后训练量化有效解决微调导致的安全对齐退化问题。

Abstract: Public large language models (LLMs) are typically safety-aligned during pretraining, yet task-specific fine-tuning required for deployment often erodes this alignment and introduces safety risks. Existing defenses either embed safety recovery into fine-tuning or rely on fine-tuning-derived priors for post-hoc correction, leaving safety recovery tightly coupled with training and incurring high computational overhead and a complex workflow. To address these challenges, we propose \texttt{Q-realign}, a post-hoc defense method based on post-training quantization, guided by an analysis of representational structure. By reframing quantization as a dual-objective procedure for compression and safety, \texttt{Q-realign} decouples safety alignment from fine-tuning and naturally piggybacks into modern deployment pipelines. Experiments across multiple models and datasets demonstrate that our method substantially reduces unsafe behaviors while preserving task performance, with significant reductions in memory usage and GPU hours. Notably, our approach can recover the safety alignment of a fine-tuned 7B LLM on a single RTX 4090 within 40 minutes. Overall, our work provides a practical, turnkey solution for safety-aware deployment.

</details>


### [189] [Local-Global Feature Fusion for Subject-Independent EEG Emotion Recognition](https://arxiv.org/abs/2601.08094)
*Zheng Zhou,Isabella McEvoy,Camilo E. Valderrama*

Main category: cs.LG

TL;DR: 提出融合局部通道级和全局试验级描述符的双分支Transformer框架，用于跨被试EEG情绪识别，在SEED-VII数据集上达到约40%的7分类准确率。


<details>
  <summary>Details</summary>
Motivation: 跨被试EEG情绪识别面临被试间变异大、从短时噪声记录中学习鲁棒表示困难的问题，需要开发能更好泛化的方法。

Method: 融合局部通道级描述符（差分熵+图论特征）和全局试验级描述符（时域、频域、复杂度特征），采用双分支Transformer架构，结合注意力融合和域对抗正则化，并通过强度阈值过滤样本。

Result: 在留一被试交叉验证协议下，该方法在SEED-VII数据集上7分类跨被试情绪识别中达到约40%的平均准确率，优于单视图和经典基线方法。

Conclusion: 局部-全局特征融合框架能有效提升跨被试EEG情绪识别的泛化能力，为处理被试间变异提供了有效解决方案。

Abstract: Subject-independent EEG emotion recognition is challenged by pronounced inter-subject variability and the difficulty of learning robust representations from short, noisy recordings. To address this, we propose a fusion framework that integrates (i) local, channel-wise descriptors and (ii) global, trial-level descriptors, improving cross-subject generalization on the SEED-VII dataset. Local representations are formed per channel by concatenating differential entropy with graph-theoretic features, while global representations summarize time-domain, spectral, and complexity characteristics at the trial level. These representations are fused in a dual-branch transformer with attention-based fusion and domain-adversarial regularization, with samples filtered by an intensity threshold. Experiments under a leave-one-subject-out protocol demonstrate that the proposed method consistently outperforms single-view and classical baselines, achieving approximately 40% mean accuracy in 7-class subject-independent emotion recognition. The code has been released at https://github.com/Danielz-z/LGF-EEG-Emotion.

</details>


### [190] [Learning a Stochastic Differential Equation Model of Tropical Cyclone Intensification from Reanalysis and Observational Data](https://arxiv.org/abs/2601.08116)
*Kenneth Gee,Sai Ravela*

Main category: cs.LG

TL;DR: 该论文提出了一种从数据中学习的10项立方随机微分方程模型，用于模拟热带气旋强度变化，该模型基于已知的环境特征，能够生成符合历史统计特征的合成强度序列。


<details>
  <summary>Details</summary>
Motivation: 热带气旋是危险的自然灾害，但由于历史数据集规模和质量有限，直接量化其危险性具有挑战性。现有的物理模型和统计/机器学习模型都存在局限性，需要探索是否能够从数据中学习出物理合理且简单的物理风格微分方程模型。

Method: 提出一个10项立方随机微分方程模型，基于经过验证的环境特征（如大尺度环境条件），使用高质量的热带气旋强度数据集（IBTrACS）和环境再分析数据（ERA5）进行训练，限于北半球数据。

Result: 模型生成的合成强度序列能够捕捉北半球历史强度变化统计特征和危险性估计的多个方面，表明该模型在模拟热带气旋强度变化方面具有良好性能。

Conclusion: 研究表明，通过自动化系统识别技术，可以从数据中学习出可解释的、物理风格的复杂地球系统动力学模型，为热带气旋强度建模提供了新的可行途径。

Abstract: Tropical cyclones are dangerous natural hazards, but their hazard is challenging to quantify directly from historical datasets due to limited dataset size and quality. Models of cyclone intensification fill this data gap by simulating huge ensembles of synthetic hurricanes based on estimates of the storm's large scale environment. Both physics-based and statistical/ML intensification models have been developed to tackle this problem, but an open question is: can a physically reasonable and simple physics-style differential equation model of intensification be learned from data? In this paper, we answer this question in the affirmative by presenting a 10-term cubic stochastic differential equation model of Tropical Cyclone intensification. The model depends on a well-vetted suite of engineered environmental features known to drive intensification and is trained using a high quality dataset of hurricane intensity (IBTrACS) with estimates of the cyclone's large scale environment from a data-assimilated simulation (ERA5 reanalysis), restricted to the Northern Hemisphere. The model generates synthetic intensity series which capture many aspects of historical intensification statistics and hazard estimates in the Northern Hemisphere. Our results show promise that interpretable, physics style models of complex earth system dynamics can be learned using automated system identification techniques.

</details>


### [191] [Structure Detection for Contextual Reinforcement Learning](https://arxiv.org/abs/2601.08120)
*Tianyue Zhou,Jung-Hoon Cho,Cathy Wu*

Main category: cs.LG

TL;DR: SD-MBTL是一个通用框架，通过动态检测CMDP的泛化结构来选择合适的MBTL算法，提出M/GP-MBTL方法在多种CRL基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统CRL方法（独立训练和多任务学习）存在计算成本高或负迁移问题。现有的MBTL方法虽然有效，但CMDP结构多样，不同问题需要不同的任务选择策略。

Method: 提出SD-MBTL框架，动态识别CMDP的底层泛化结构并选择合适的MBTL算法。具体提出M/GP-MBTL方法，检测Mountain结构并在高斯过程方法和聚类方法之间自适应切换。

Result: 在合成数据和CRL基准测试（连续控制、交通控制、农业管理）上，M/GP-MBTL比先前最强方法在聚合指标上提升了12.49%。

Conclusion: 在线结构检测对于指导复杂CRL环境中的源任务选择具有前景，SD-MBTL框架能够根据CMDP结构特性自适应选择最佳算法。

Abstract: Contextual Reinforcement Learning (CRL) tackles the problem of solving a set of related Contextual Markov Decision Processes (CMDPs) that vary across different context variables. Traditional approaches--independent training and multi-task learning--struggle with either excessive computational costs or negative transfer. A recently proposed multi-policy approach, Model-Based Transfer Learning (MBTL), has demonstrated effectiveness by strategically selecting a few tasks to train and zero-shot transfer. However, CMDPs encompass a wide range of problems, exhibiting structural properties that vary from problem to problem. As such, different task selection strategies are suitable for different CMDPs. In this work, we introduce Structure Detection MBTL (SD-MBTL), a generic framework that dynamically identifies the underlying generalization structure of CMDP and selects an appropriate MBTL algorithm. For instance, we observe Mountain structure in which generalization performance degrades from the training performance of the target task as the context difference increases. We thus propose M/GP-MBTL, which detects the structure and adaptively switches between a Gaussian Process-based approach and a clustering-based approach. Extensive experiments on synthetic data and CRL benchmarks--covering continuous control, traffic control, and agricultural management--show that M/GP-MBTL surpasses the strongest prior method by 12.49% on the aggregated metric. These results highlight the promise of online structure detection for guiding source task selection in complex CRL environments.

</details>


### [192] [Intra-tree Column Subsampling Hinders XGBoost Learning of Ratio-like Interactions](https://arxiv.org/abs/2601.08121)
*Mykola Pinchuk*

Main category: cs.LG

TL;DR: XGBoost中树内列抽样会降低需要特征组合（如比率）才能发现信号的数据集的性能，但当包含工程化比率特征时，这种影响消失。


<details>
  <summary>Details</summary>
Motivation: 许多实际问题中的信号需要通过组合多个原始测量值才能显现（如比率和速率）。在梯度提升树中，这种组合不是显式操作，模型需要通过特征上的协调分裂来合成。研究XGBoost中的树内列抽样是否使这种合成变得更困难。

Method: 使用两种具有抵消结构的数据生成过程：两个原始特征共享强干扰因子，而目标依赖于较小的差异因子。对数比率可以抵消干扰并分离信号。在{0.4, 0.6, 0.8, 0.9}范围内变化colsample_bylevel和colsample_bynode参数，重点关注轻度抽样(s >= 0.8)。对照组包含工程化的比率特征，无需合成。

Result: 在两种数据生成过程中，树内列抽样降低了仅使用原始特征设置下的测试PR-AUC。在主过程中，当两个参数都设为0.4时，相对下降达到54%。当包含工程化比率特征时，这种影响基本消失。基于路径的共同使用度量在性能下降的相同单元格中下降。

Conclusion: 如果数据可能存在比率类结构，应避免使用树内列抽样，或者包含预期的比率特征。树内列抽样会阻碍模型合成需要协调特征分裂才能发现的信号。

Abstract: Many applied problems contain signal that becomes clear only after combining multiple raw measurements. Ratios and rates are common examples. In gradient boosted trees, this combination is not an explicit operation: the model must synthesize it through coordinated splits on the component features. We study whether intra-tree column subsampling in XGBoost makes that synthesis harder. We use two synthetic data generating processes with cancellation-style structure. In both, two primitive features share a strong nuisance factor, while the target depends on a smaller differential factor. A log ratio cancels the nuisance and isolates the signal. We vary colsample_bylevel and colsample_bynode over s in {0.4, 0.6, 0.8, 0.9}, emphasizing mild subsampling (s >= 0.8). A control feature set includes the engineered ratio, removing the need for synthesis. Across both processes, intra-tree column subsampling reduces test PR-AUC in the primitives-only setting. In the main process the relative decrease reaches 54 percent when both parameters are set to 0.4. The effect largely disappears when the engineered ratio is present. A path-based co-usage metric drops in the same cells where performance deteriorates. Practically, if ratio-like structure is plausible, either avoid intra-tree subsampling or include the intended ratio features.

</details>


### [193] [Generalization Analysis and Method for Domain Generalization for a Family of Recurrent Neural Networks](https://arxiv.org/abs/2601.08122)
*Atefeh Termehchi,Ekram Hossain,Isaac Woungang*

Main category: cs.LG

TL;DR: 该论文提出了一种基于Koopman算子理论的循环神经网络可解释性和域外泛化分析方法，并开发了相应的域泛化方法。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在可解释性和泛化性方面存在局限，尤其是在时序数据中传统泛化分析假设独立样本不成立。需要开发能够分析RNN可解释性和域外泛化的理论框架。

Method: 将训练好的RNN状态演化建模为未知的离散时间非线性闭环反馈系统，使用Koopman算子理论近似为线性算子以实现可解释性，然后通过谱分析量化域偏移对泛化误差的最坏影响，并基于此提出域泛化方法。

Result: 提出的分析方法能够量化域偏移对泛化误差的影响，域泛化方法能够减少域外泛化误差并提高对分布偏移的鲁棒性，在实际时序模式学习任务中得到验证。

Conclusion: 该研究为RNN的可解释性和域外泛化提供了理论分析框架，并开发了有效的域泛化方法，有助于提高深度学习模型在安全关键部署中的可信度。

Abstract: Deep learning (DL) has driven broad advances across scientific and engineering domains. Despite its success, DL models often exhibit limited interpretability and generalization, which can undermine trust, especially in safety-critical deployments. As a result, there is growing interest in (i) analyzing interpretability and generalization and (ii) developing models that perform robustly under data distributions different from those seen during training (i.e. domain generalization). However, the theoretical analysis of DL remains incomplete. For example, many generalization analyses assume independent samples, which is violated in sequential data with temporal correlations. Motivated by these limitations, this paper proposes a method to analyze interpretability and out-of-domain (OOD) generalization for a family of recurrent neural networks (RNNs). Specifically, the evolution of a trained RNN's states is modeled as an unknown, discrete-time, nonlinear closed-loop feedback system. Using Koopman operator theory, these nonlinear dynamics are approximated with a linear operator, enabling interpretability. Spectral analysis is then used to quantify the worst-case impact of domain shifts on the generalization error. Building on this analysis, a domain generalization method is proposed that reduces the OOD generalization error and improves the robustness to distribution shifts. Finally, the proposed analysis and domain generalization approach are validated on practical temporal pattern-learning tasks.

</details>


### [194] [Dynamic Graph Structure Learning via Resistance Curvature Flow](https://arxiv.org/abs/2601.08149)
*Chaoqun Fei,Huanjiang Liu,Tinglve Zhou,Yangyang Li,Tianyong Hao*

Main category: cs.LG

TL;DR: 提出Resistance Curvature Flow (RCF)框架，利用电路物理中的有效电阻概念替代传统基于最优传输的曲率优化，实现100倍以上的计算加速，并开发了DGSL-RCF图优化算法。


<details>
  <summary>Details</summary>
Motivation: 传统基于欧氏距离的静态图构建方法无法捕捉数据流形的内在曲率特征，而现有的Ollivier-Ricci曲率流(OCF)依赖最优传输理论，计算复杂度极高，限制了其在大规模数据集和深度学习框架中的应用。

Method: 提出Resistance Curvature Flow (RCF)框架，利用电路物理中的有效电阻概念将昂贵的曲率优化转化为高效的矩阵运算。基于该框架设计了图优化算法DGSL-RCF，通过曲率梯度引导边权重的重新分配来消除拓扑噪声并增强局部聚类结构。

Result: RCF实现了超过100倍的计算加速，同时保持了与OCF相当的几何优化能力。在深度度量学习、流形学习和图结构学习等任务中，DGSL-RCF显著提升了表示质量和下游任务性能。

Conclusion: RCF框架成功解决了传统曲率优化方法计算复杂度高的问题，为大规模数据集的几何表示学习提供了高效解决方案，并与深度学习模型具有良好的兼容性。

Abstract: Geometric Representation Learning (GRL) aims to approximate the non-Euclidean topology of high-dimensional data through discrete graph structures, grounded in the manifold hypothesis. However, traditional static graph construction methods based on Euclidean distance often fail to capture the intrinsic curvature characteristics of the data manifold. Although Ollivier-Ricci Curvature Flow (OCF) has proven to be a powerful tool for dynamic topological optimization, its core reliance on Optimal Transport (Wasserstein distance) leads to prohibitive computational complexity, severely limiting its application in large-scale datasets and deep learning frameworks. To break this bottleneck, this paper proposes a novel geometric evolution framework: Resistance Curvature Flow (RCF). Leveraging the concept of effective resistance from circuit physics, RCF transforms expensive curvature optimization into efficient matrix operations. This approach achieves over 100x computational acceleration while maintaining geometric optimization capabilities comparable to OCF. We provide an in-depth exploration of the theoretical foundations and dynamical principles of RCF, elucidating how it guides the redistribution of edge weights via curvature gradients to eliminate topological noise and strengthen local cluster structures. Furthermore, we provide a mechanistic explanation of RCF's role in manifold enhancement and noise suppression, as well as its compatibility with deep learning models. We design a graph optimization algorithm, DGSL-RCF, based on this framework. Experimental results across deep metric learning, manifold learning, and graph structure learning demonstrate that DGSL-RCF significantly improves representation quality and downstream task performance.

</details>


### [195] [VBO-MI: A Fully Gradient-Based Bayesian Optimization Framework Using Variational Mutual Information Estimation](https://arxiv.org/abs/2601.08172)
*Farhad Mirkarimi*

Main category: cs.LG

TL;DR: VBO-MI提出了一种基于梯度的贝叶斯优化框架，利用变分互信息估计，通过actor-critic架构消除传统内循环采集优化瓶颈，实现高达100倍的FLOPs减少。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯神经网络优化框架存在昂贵的后验采样和采集函数优化问题，限制了贝叶斯优化的可扩展性和效率。

Method: 采用变分互信息估计的完全基于梯度的BO框架，使用actor-critic架构：action-net用于导航输入空间，variational critic用于估计信息增益，实现端到端梯度流。

Result: 相比BNN-BO基线，VBO-MI实现了高达100倍的FLOPs减少，在高维合成函数、PDE优化、Lunar Lander控制问题和分类Pest Control等任务上表现出相同或更优的优化性能和计算可扩展性。

Conclusion: VBO-MI通过消除传统采集优化瓶颈，提供了一种高效、可扩展的贝叶斯优化框架，在保持或提升优化性能的同时显著降低了计算成本。

Abstract: Many real-world tasks require optimizing expensive black-box functions accessible only through noisy evaluations, a setting commonly addressed with Bayesian optimization (BO). While Bayesian neural networks (BNNs) have recently emerged as scalable alternatives to Gaussian Processes (GPs), traditional BNN-BO frameworks remain burdened by expensive posterior sampling and acquisition function optimization. In this work, we propose {VBO-MI} (Variational Bayesian Optimization with Mutual Information), a fully gradient-based BO framework that leverages recent advances in variational mutual information estimation. To enable end-to-end gradient flow, we employ an actor-critic architecture consisting of an {action-net} to navigate the input space and a {variational critic} to estimate information gain. This formulation effectively eliminates the traditional inner-loop acquisition optimization bottleneck, achieving up to a {$10^2 \times$ reduction in FLOPs} compared to BNN-BO baselines. We evaluate our method on a diverse suite of benchmarks, including high-dimensional synthetic functions and complex real-world tasks such as PDE optimization, the Lunar Lander control problem, and categorical Pest Control. Our experiments demonstrate that VBO-MI consistently provides the same or superior optimization performance and computational scalability over the baselines.

</details>


### [196] [TabPFN Through The Looking Glass: An interpretability study of TabPFN and its internal representations](https://arxiv.org/abs/2601.08181)
*Aviral Gupta,Armaan Sethi,Dhruv Kumar*

Main category: cs.LG

TL;DR: 该论文通过探测实验分析表格基础模型的内部表示，发现模型隐藏层中编码了有意义的结构化信息，包括线性回归系数、复杂表达式的中间值和最终答案，揭示了模型内部计算机制。


<details>
  <summary>Details</summary>
Motivation: 表格基础模型在多个领域表现出色，但其内部表示和学习概念仍不明确。缺乏可解释性使得研究这些模型如何处理和转换输入特征变得重要，以增强模型的透明度和可信度。

Method: 通过一系列探测实验分析模型隐藏表示中的信息，测试线性回归系数、复杂表达式的中间值以及早期层中的最终答案是否存在，从而推理模型内部执行的计算。

Result: 结果表明表格基础模型的表示中存储了有意义的结构化信息，观察到了与模型预测过程中间和最终数量对应的清晰信号，揭示了模型如何细化输入以及最终输出如何产生。

Conclusion: 研究发现表格基础模型编码了具体且可解释的信息，这有助于更深入地理解模型内部机制，使决策过程更加透明和可信，向可解释AI迈进一步。

Abstract: Tabular foundational models are pre-trained models designed for a wide range of tabular data tasks. They have shown strong performance across domains, yet their internal representations and learned concepts remain poorly understood. This lack of interpretability makes it important to study how these models process and transform input features. In this work, we analyze the information encoded inside the model's hidden representations and examine how these representations evolve across layers. We run a set of probing experiments that test for the presence of linear regression coefficients, intermediate values from complex expressions, and the final answer in early layers. These experiments allow us to reason about the computations the model performs internally. Our results provide evidence that meaningful and structured information is stored inside the representations of tabular foundational models. We observe clear signals that correspond to both intermediate and final quantities involved in the model's prediction process. This gives insight into how the model refines its inputs and how the final output emerges. Our findings contribute to a deeper understanding of the internal mechanics of tabular foundational models. They show that these models encode concrete and interpretable information, which moves us closer to making their decision processes more transparent and trustworthy.

</details>


### [197] [Scalable Multiagent Reinforcement Learning with Collective Influence Estimation](https://arxiv.org/abs/2601.08210)
*Zhenglong Luo,Zhiyong Chen,Aoxiang Liu,Ke Pan*

Main category: cs.LG

TL;DR: 提出CIEN框架，通过建模其他智能体对任务对象的集体影响，实现无需显式动作信息交换的多智能体协作，提高可扩展性和通信受限环境下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有MARL方法依赖频繁的动作或状态信息交换来实现协调，这在实际机器人系统中难以满足。现有估计器网络会随智能体数量增加而快速膨胀，限制了大规模系统的可扩展性。

Method: 提出集体影响估计网络(CIEN)框架，通过显式建模其他智能体对任务对象的集体影响，使每个智能体仅从局部观察和任务对象状态就能推断关键交互信息，无需显式动作信息交换。

Result: 在基于SAC算法的多智能体协作任务实验中，该方法在通信受限环境下实现了稳定高效的协调。网络规模不随团队规模扩大而增加，新智能体加入无需修改现有网络结构。在真实机器人平台上的部署显示显著提高了鲁棒性和部署可行性，减少了对通信基础设施的依赖。

Conclusion: CIEN框架通过集体影响建模有效解决了MARL中的通信依赖和可扩展性问题，在通信受限环境下实现了高效协作，并展示了在实际机器人系统中的部署可行性。

Abstract: Multiagent reinforcement learning (MARL) has attracted considerable attention due to its potential in addressing complex cooperative tasks. However, existing MARL approaches often rely on frequent exchanges of action or state information among agents to achieve effective coordination, which is difficult to satisfy in practical robotic systems. A common solution is to introduce estimator networks to model the behaviors of other agents and predict their actions; nevertheless, such designs cause the size and computational cost of the estimator networks to grow rapidly with the number of agents, thereby limiting scalability in large-scale systems.
  To address these challenges, this paper proposes a multiagent learning framework augmented with a Collective Influence Estimation Network (CIEN). By explicitly modeling the collective influence of other agents on the task object, each agent can infer critical interaction information solely from its local observations and the task object's states, enabling efficient collaboration without explicit action information exchange. The proposed framework effectively avoids network expansion as the team size increases; moreover, new agents can be incorporated without modifying the network structures of existing agents, demonstrating strong scalability. Experimental results on multiagent cooperative tasks based on the Soft Actor-Critic (SAC) algorithm show that the proposed method achieves stable and efficient coordination under communication-limited environments. Furthermore, policies trained with collective influence modeling are deployed on a real robotic platform, where experimental results indicate significantly improved robustness and deployment feasibility, along with reduced dependence on communication infrastructure.

</details>


### [198] [One-Shot Federated Ridge Regression: Exact Recovery via Sufficient Statistic Aggregation](https://arxiv.org/abs/2601.08216)
*Zahir Alsulaimawi*

Main category: cs.LG

TL;DR: 本文提出一种用于联邦岭回归的单次通信协议，通过客户端传输局部充分统计量（Gram矩阵和矩向量），服务器通过单次矩阵求逆重构全局解，避免了迭代通信。


<details>
  <summary>Details</summary>
Motivation: 联邦学习协议通常需要客户端与中央服务器之间的重复同步，收敛速度受学习率、数据异构性和客户端采样影响。本文探讨分布式线性回归是否真的需要迭代通信，旨在减少通信开销。

Method: 将联邦岭回归表述为分布式均衡问题：每个客户端计算局部充分统计量（Gram矩阵和矩向量）并一次性传输；服务器通过单次矩阵求逆重构全局解。提出随机投影技术降低高维通信成本，并建立差分隐私保证。

Result: 在满足客户端特征矩阵覆盖条件时，单次聚合可精确恢复集中式岭解；对于违反覆盖条件的异构分布，推导了非渐近误差界。通信量从迭代方法的O(Rd)减少到O(d²)，随机投影进一步降至O(m²)。实验显示单次融合匹配FedAvg精度，同时减少高达38倍的通信量。

Conclusion: 对于分布式线性回归，迭代通信并非必要。单次通信协议能精确恢复集中式岭解，显著降低通信开销，提供差分隐私优势，并适用于核方法和随机特征模型，但不适用于一般非线性架构。

Abstract: Federated learning protocols require repeated synchronization between clients and a central server, with convergence rates depending on learning rates, data heterogeneity, and client sampling. This paper asks whether iterative communication is necessary for distributed linear regression. We show it is not. We formulate federated ridge regression as a distributed equilibrium problem where each client computes local sufficient statistics -- the Gram matrix and moment vector -- and transmits them once. The server reconstructs the global solution through a single matrix inversion. We prove exact recovery: under a coverage condition on client feature matrices, one-shot aggregation yields the centralized ridge solution, not an approximation. For heterogeneous distributions violating coverage, we derive non-asymptotic error bounds depending on spectral properties of the aggregated Gram matrix. Communication reduces from $\mathcal{O}(Rd)$ in iterative methods to $\mathcal{O}(d^2)$ total; for high-dimensional settings, we propose and experimentally validate random projection techniques reducing this to $\mathcal{O}(m^2)$ where $m \ll d$. We establish differential privacy guarantees where noise is injected once per client, eliminating the composition penalty that degrades privacy in multi-round protocols. We further address practical considerations including client dropout robustness, federated cross-validation for hyperparameter selection, and comparison with gradient-based alternatives. Comprehensive experiments on synthetic heterogeneous regression demonstrate that one-shot fusion matches FedAvg accuracy while requiring up to $38\times$ less communication. The framework applies to kernel methods and random feature models but not to general nonlinear architectures.

</details>


### [199] [A Preliminary Agentic Framework for Matrix Deflation](https://arxiv.org/abs/2601.08219)
*Paimon Goulart,Evangelos E. Papalexakis*

Main category: cs.LG

TL;DR: 提出一种基于智能体的矩阵降秩方法，使用LLM生成秩-1 SVD更新，VLM评估更新并决定停止时机，无需固定阈值


<details>
  <summary>Details</summary>
Motivation: 探索是否可以通过小型智能体团队逐层分解矩阵，实现无需固定阈值的矩阵降秩方法，替代传统数值算法

Method: 使用LLM作为求解器生成秩-1 SVD更新，VLM作为评估器接受/拒绝更新并决定停止时机，通过上下文学习和行列置换提高稳定性

Result: 在Digits、CIFAR-10和合成矩阵上表现优异，合成噪声案例中与数值降秩仅差1.75 RMSE，在所有设置中都达到竞争性结果

Conclusion: 完全基于智能体、无阈值的矩阵降秩方法是可行的，可作为传统数值算法的替代方案

Abstract: Can a small team of agents peel a matrix apart, one rank-1 slice at a time? We propose an agentic approach to matrix deflation in which a solver Large Language Model (LLM) generates rank-1 Singular Value Decomposition (SVD) updates and a Vision Language Model (VLM) accepts or rejects each update and decides when to stop, eliminating fixed norm thresholds. Solver stability is improved through in-context learning (ICL) and types of row/column permutations that expose visually coherent structure. We evaluate on Digits ($8{\times}8$), CIFAR-10 ($32{\times}32$ grayscale), and synthetic ($16{\times}16$) matrices with and without Gaussian noise. In the synthetic noisy case, where the true construction rank $k$ is known, numerical deflation provides the noise target and our best agentic configuration differs by only $1.75$ RMSE of the target. For Digits and CIFAR-10, targets are defined by deflating until the Frobenius norm reaches $10\%$ of the original. Across all settings, our agent achieves competitive results, suggesting that fully agentic, threshold-free deflation is a viable alternative to classical numerical algorithms.

</details>


### [200] [GADPN: Graph Adaptive Denoising and Perturbation Networks via Singular Value Decomposition](https://arxiv.org/abs/2601.08230)
*Hao Deng,Bo Liu*

Main category: cs.LG

TL;DR: GADPN：通过低秩去噪和广义结构扰动自适应优化图拓扑的图结构学习框架，显著提升效率并在异配图上表现优异


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络性能受限于观测图的质量（噪声、缺失链接、结构属性不匹配），而现有图结构学习方法计算成本高，限制了实际应用

Method: 提出GADPN框架，通过贝叶斯优化自适应确定去噪强度，并利用奇异值分解将结构扰动方法扩展到任意图结构

Result: 在基准数据集上达到最先进性能，显著提升效率，在具有挑战性的异配图上表现尤为突出

Conclusion: GADPN能够跨不同网络类型稳健地学习增强的图结构，为图结构学习提供了一种高效实用的解决方案

Abstract: While Graph Neural Networks (GNNs) excel on graph-structured data, their performance is fundamentally limited by the quality of the observed graph, which often contains noise, missing links, or structural properties misaligned with GNNs' underlying assumptions. To address this, graph structure learning aims to infer a more optimal topology. Existing methods, however, often incur high computational costs due to complex generative models and iterative joint optimization, limiting their practical utility. In this paper, we propose GADPN, a simple yet effective graph structure learning framework that adaptively refines graph topology via low-rank denoising and generalized structural perturbation. Our approach makes two key contributions: (1) we introduce Bayesian optimization to adaptively determine the optimal denoising strength, tailoring the process to each graph's homophily level; and (2) we extend the structural perturbation method to arbitrary graphs via Singular Value Decomposition (SVD), overcoming its original limitation to symmetric structures. Extensive experiments on benchmark datasets demonstrate that GADPN achieves state-of-the-art performance while significantly improving efficiency. It shows particularly strong gains on challenging disassortative graphs, validating its ability to robustly learn enhanced graph structures across diverse network types.

</details>


### [201] [Hyperbolic Heterogeneous Graph Transformer](https://arxiv.org/abs/2601.08251)
*Jongmin Park,Seunghoon Han,Hyewon Lee,Won-Yong Shin,Sungsu Lim*

Main category: cs.LG

TL;DR: 提出Hyperbolic Heterogeneous Graph Transformer (HypHGT)，在双曲空间中学习异构图表示，通过transformer架构同时捕获局部和全局依赖，实现高效计算并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有双曲空间异构图学习方法存在两个主要问题：1）过度依赖切空间操作导致频繁转换时的映射失真；2）消息传递架构主要关注局部邻域信息，难以捕获全局层次结构和长距离依赖。

Method: 提出HypHGT，完全在双曲空间中学习异构图表示。采用transformer架构自然捕获局部和全局依赖，设计关系特定的双曲注意力机制，具有线性时间复杂度，能高效计算并保持不同关系类型的异质信息。

Result: 在节点分类任务上，HypHGT持续优于最先进方法，同时显著减少了训练时间和内存使用。

Conclusion: HypHGT通过完全在双曲空间中运行的transformer架构，有效解决了现有方法的局限性，能够高效捕获异构图的复杂结构特性和语义信息。

Abstract: In heterogeneous graphs, we can observe complex structures such as tree-like or hierarchical structures. Recently, the hyperbolic space has been widely adopted in many studies to effectively learn these complex structures. Although these methods have demonstrated the advantages of the hyperbolic space in learning heterogeneous graphs, most existing methods still have several challenges. They rely heavily on tangent-space operations, which often lead to mapping distortions during frequent transitions. Moreover, their message-passing architectures mainly focus on local neighborhood information, making it difficult to capture global hierarchical structures and long-range dependencies between different types of nodes. To address these limitations, we propose Hyperbolic Heterogeneous Graph Transformer (HypHGT), which effectively and efficiently learns heterogeneous graph representations entirely within the hyperbolic space. Unlike previous message-passing based hyperbolic heterogeneous GNNs, HypHGT naturally captures both local and global dependencies through transformer-based architecture. Furthermore, the proposed relation-specific hyperbolic attention mechanism in HypHGT, which operates with linear time complexity, enables efficient computation while preserving the heterogeneous information across different relation types. This design allows HypHGT to effectively capture the complex structural properties and semantic information inherent in heterogeneous graphs. We conduct comprehensive experiments to evaluate the effectiveness and efficiency of HypHGT, and the results demonstrate that it consistently outperforms state-of-the-art methods in node classification task, with significantly reduced training time and memory usage.

</details>


### [202] [LDLT L-Lipschitz Network Weight Parameterization Initialization](https://arxiv.org/abs/2601.08253)
*Marius F. R. Juston,Ramavarapu S. Sreenivas,Dustin Nottage,Ahmet Soylemezoglu*

Main category: cs.LG

TL;DR: 论文分析了LDLT-based L-Lipschitz层的初始化动态，推导了高斯初始化下的精确输出方差，发现当前初始化方法导致方差仅为0.41，而新参数化方法可提升至0.9，但实证显示He初始化在实际任务中表现更好。


<details>
  <summary>Details</summary>
Motivation: 研究L-Lipschitz神经网络在初始化时信息快速损失的问题，分析当前初始化方法（如He或Kaiming初始化）在L-Lipschitz层中的方差保持效果不佳的原因。

Method: 使用Wishart分布理论推导输出方差的闭式解，结合James定理和拉普拉斯积分展开；开发基于Isserlis/Wick的组合展开方法计算迹的期望；通过蒙特卡洛实验验证理论估计；在Higgs玻色子分类数据集上进行超参数扫描。

Result: 当前He/Kaiming初始化（缩放1/√n）的输出方差为0.41，而新参数化方法（缩放10/√n，α=1）可将方差提升至0.9。但在实际数据集上，尽管新方法能保持方差，He初始化仍表现更好。

Conclusion: 研究阐明了深度L-Lipschitz网络初始化时信息损失的原因，并提供了改善初始化超参数选择的实用建议，但发现理论上的方差保持与实际性能之间存在差距，He初始化在实际任务中仍更优。

Abstract: We analyze initialization dynamics for LDLT-based $\mathcal{L}$-Lipschitz layers by deriving the exact marginal output variance when the underlying parameter matrix $W_0\in \mathbb{R}^{m\times n}$ is initialized with IID Gaussian entries $\mathcal{N}(0,σ^2)$. The Wishart distribution, $S=W_0W_0^\top\sim\mathcal{W}_m(n,σ^2 \boldsymbol{I}_m)$, used for computing the output marginal variance is derived in closed form using expectations of zonal polynomials via James' theorem and a Laplace-integral expansion of $(α\boldsymbol{I}_m+S)^{-1}$. We develop an Isserlis/Wick-based combinatorial expansion for $\operatorname{\mathbb{E}}\left[\operatorname{tr}(S^k)\right]$ and provide explicit truncated moments up to $k=10$, which yield accurate series approximations for small-to-moderate $σ^2$. Monte Carlo experiments confirm the theoretical estimates. Furthermore, empirical analysis was performed to quantify that, using current He or Kaiming initialization with scaling $1/\sqrt{n}$, the output variance is $0.41$, whereas the new parameterization with $10/ \sqrt{n}$ for $α=1$ results in an output variance of $0.9$. The findings clarify why deep $\mathcal{L}$-Lipschitz networks suffer rapid information loss at initialization and offer practical prescriptions for choosing initialization hyperparameters to mitigate this effect. However, using the Higgs boson classification dataset, a hyperparameter sweep over optimizers, initialization scale, and depth was conducted to validate the results on real-world data, showing that although the derivation ensures variance preservation, empirical results indicate He initialization still performs better.

</details>


### [203] [On Evaluation of Unsupervised Feature Selection for Pattern Classification](https://arxiv.org/abs/2601.08257)
*Gyu-Il Kim,Dae-Won Kim,Jaesung Lee*

Main category: cs.LG

TL;DR: 该研究重新审视了无监督特征选择的评估范式，提出使用多标签分类框架代替传统的单标签评估，发现不同方法在两种评估设置下的性能排名显著不同。


<details>
  <summary>Details</summary>
Motivation: 现有无监督特征选择方法通常使用单标签数据集进行评估，但单标签数据可以从多标签数据中任意选择标签，导致评估结果不稳定且不可靠，无法真实反映方法的判别能力。

Method: 采用多标签分类框架评估无监督特征选择方法，在21个多标签数据集上使用多种代表性方法进行实验，比较单标签和多标签评估设置下的性能差异。

Result: 实验结果显示，无监督特征选择方法在多标签评估设置下的性能排名与单标签设置下报告的排名显著不同，表明多标签评估能提供更公平可靠的比较。

Conclusion: 仅基于单标签准确率评估无监督特征选择方法是不合理的，多标签评估设置能够更公平、可靠地比较不同方法的真实判别能力。

Abstract: Unsupervised feature selection aims to identify a compact subset of features that captures the intrinsic structure of data without supervised label. Most existing studies evaluate the performance of methods using the single-label dataset that can be instantiated by selecting a label from multi-label data while maintaining the original features. Because the chosen label can vary arbitrarily depending on the experimental setting, the superiority among compared methods can be changed with regard to which label happens to be selected. Thus, evaluating unsupervised feature selection methods based solely on single-label accuracy is unreasonable for assessing their true discriminative ability. This study revisits this evaluation paradigm by adopting a multi-label classification framework. Experiments on 21 multi-label datasets using several representative methods demonstrate that performance rankings differ markedly from those reported under single-label settings, suggesting the possibility of multi-label evaluation settings for fair and reliable comparison of unsupervised feature selection methods.

</details>


### [204] [A Usable GAN-Based Tool for Synthetic ECG Generation in Cardiac Amyloidosis Research](https://arxiv.org/abs/2601.08260)
*Francesco Speziale,Ugo Lomoio,Fabiola Boccuto,Pierangelo Veltri,Pietro Hiram Guzzi*

Main category: cs.LG

TL;DR: 提出基于GAN的ECG信号生成工具，用于生成合成心电图搏动以辅助心脏淀粉样变性的早期诊断和患者分层


<details>
  <summary>Details</summary>
Motivation: 心脏淀粉样变性是一种罕见且诊断不足的浸润性心肌病，现有机器学习模型的数据集通常较小、不平衡且异质性高，需要更好的数据增强方法

Method: 使用生成对抗网络（GAN）和图形化命令行界面，生成逼真的合成心电图搏动，支持临床研究人员训练类别特定的生成器

Result: 开发了一个可用性强的工具，能够交互式生成大量标记的合成搏动，同时保留少数类别的分布特征

Conclusion: 该工具能够支持心脏淀粉样变性的早期诊断和患者分层，通过生成合成数据解决数据集小、不平衡的问题

Abstract: Cardiac amyloidosis (CA) is a rare and underdiagnosed infiltrative cardiomyopathy, and available datasets for machine-learning models are typically small, imbalanced and heterogeneous. This paper presents a Generative Adversarial Network (GAN) and a graphical command-line interface for generating realistic synthetic electrocardiogram (ECG) beats to support early diagnosis and patient stratification in CA. The tool is designed for usability, allowing clinical researchers to train class-specific generators once and then interactively produce large volumes of labelled synthetic beats that preserve the distribution of minority classes.

</details>


### [205] [Demystifying the Slash Pattern in Attention: The Role of RoPE](https://arxiv.org/abs/2601.08297)
*Yuan Cheng,Fengzhuo Zhang,Yunlong Hou,Cunxiao Du,Chao Du,Tianyu Pang,Aixin Sun,Zhuoran Yang*

Main category: cs.LG

TL;DR: LLMs中出现的斜线注意力模式（注意力集中在Δ-th次对角线上）源于查询和键的秩一特性以及RoPE中高频成分的相互作用，这些模式是模型固有的，能泛化到分布外提示。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型中斜线注意力模式（SDHs）的出现原因，这些模式在信息跨token传递中起关键作用，但其形成机制尚不明确。

Method: 结合实证分析和理论证明：1）分析开源LLMs中SDHs的特性；2）研究查询、键和RoPE的相互作用；3）在特定条件下形式化建模假设；4）分析带RoPE的浅层Transformer的训练动态。

Result: 发现SDHs的两个特征条件：1）查询和键几乎秩一；2）RoPE由中高频成分主导。在这些条件下，查询和键在token间几乎相同，RoPE的中高频成分相互作用导致SDHs出现。理论证明这些条件足以确保SDHs出现。

Conclusion: 斜线注意力模式是LLMs固有的特性，源于查询/键的秩一特性和RoPE中高频成分的相互作用，这些模式能泛化到分布外提示，为理解Transformer注意力机制提供了理论解释。

Abstract: Large Language Models (LLMs) often exhibit slash attention patterns, where attention scores concentrate along the $Δ$-th sub-diagonal for some offset $Δ$. These patterns play a key role in passing information across tokens. But why do they emerge? In this paper, we demystify the emergence of these Slash-Dominant Heads (SDHs) from both empirical and theoretical perspectives. First, by analyzing open-source LLMs, we find that SDHs are intrinsic to models and generalize to out-of-distribution prompts. To explain the intrinsic emergence, we analyze the queries, keys, and Rotary Position Embedding (RoPE), which jointly determine attention scores. Our empirical analysis reveals two characteristic conditions of SDHs: (1) Queries and keys are almost rank-one, and (2) RoPE is dominated by medium- and high-frequency components. Under these conditions, queries and keys are nearly identical across tokens, and interactions between medium- and high-frequency components of RoPE give rise to SDHs. Beyond empirical evidence, we theoretically show that these conditions are sufficient to ensure the emergence of SDHs by formalizing them as our modeling assumptions. Particularly, we analyze the training dynamics of a shallow Transformer equipped with RoPE under these conditions, and prove that models trained via gradient descent exhibit SDHs. The SDHs generalize to out-of-distribution prompts.

</details>


### [206] [ORBIT: On-policy Exploration-Exploitation for Controllable Multi-Budget Reasoning](https://arxiv.org/abs/2601.08310)
*Kun Liang,Clive Bai,Xin Xu,Chenming Tang,Sanwoo Lee,Weijie Liu,Saiyong Yang,Yunfang Wu*

Main category: cs.LG

TL;DR: ORBIT是一个可控多预算推理框架，通过多阶段强化学习发现帕累托最优推理行为，然后蒸馏到单一模型中，实现不同计算预算下的可控推理。


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型使用过长的推理链会带来不必要的计算成本，而现有预算推断方法在极端情况下不可靠，且训练时固定了成本-精度权衡，限制了部署灵活性。

Method: ORBIT采用多阶段强化学习发现不同计算预算下的帕累托最优推理行为，然后通过策略蒸馏将这些行为融合到单一统一模型中，实现输入触发的可控多模式推理。

Result: 实验表明ORBIT实现了：(1) 多模式可控推理行为；(2) 每个模式内具有竞争力的推理密度；(3) 将前沿策略集成到单一学生模型中，同时保持清晰的模式分离和高性能。

Conclusion: ORBIT框架解决了现有推理模型计算成本过高和预算推断不可靠的问题，通过可控多预算推理实现了在不同部署场景下的灵活性和效率平衡。

Abstract: Recent Large Reasoning Models (LRMs) achieve strong performance by leveraging long-form Chain-of-Thought (CoT) reasoning, but uniformly applying overlong reasoning at inference time incurs substantial and often unnecessary computational cost. To address this, prior work explores various strategies to infer an appropriate reasoning budget from the input. However, such approaches are unreliable in the worst case, as estimating the minimal required reasoning effort is fundamentally difficult, and they implicitly fix the trade-off between reasoning cost and accuracy during training, limiting flexibility under varying deployment scenarios. Motivated by these limitations, we propose ORBIT, a controllable multi-budget reasoning framework with well-separated reasoning modes triggered by input. ORBIT employs multi-stage reinforcement learning to discover Pareto-optimal reasoning behaviors at each effort, followed by on-policy distillation to fuse these behaviors into a single unified model. Experiments show that ORBIT achieves (1) controllable reasoning behavior over multiple modes, (2) competitive reasoning density within each mode, and (3) integration of these frontier policies into a single unified student model while preserving clear mode separation and high per-mode performance.

</details>


### [207] [Automated Machine Learning in Radiomics: A Comparative Evaluation of Performance, Efficiency and Accessibility](https://arxiv.org/abs/2601.08334)
*Jose Lozano-Montoya,Emilio Soria-Olivas,Almudena Fuster-Matanzo,Angel Alberich-Bayarri,Ana Jimenez-Pastor*

Main category: cs.LG

TL;DR: 该研究评估了通用和放射组学专用AutoML框架在放射组学分类任务中的表现，发现Simplatab在性能、效率和可访问性方面表现最佳，但现有框架仍缺乏对生存分析等放射组学特定挑战的支持。


<details>
  <summary>Details</summary>
Motivation: AutoML框架可以降低放射组学模型开发的技术门槛，但其在解决放射组学特定挑战方面的有效性尚不明确。本研究旨在评估通用和放射组学专用AutoML框架在多样化放射组学分类任务中的表现，以指导未来放射组学AutoML的发展方向。

Method: 使用10个公共/私有放射组学数据集（涵盖CT/MRI等多种成像模态、不同大小、解剖部位和终点），测试了6个通用框架和5个放射组学专用框架。采用预定义参数和标准化交叉验证，评估指标包括AUC、运行时间，以及软件状态、可访问性和可解释性等定性方面。

Result: Simplatab（放射组学专用工具）获得最高平均测试AUC（81.81%），运行时间约1小时。LightAutoML（通用框架）运行最快（6分钟），性能有竞争力（平均AUC 78.74%）。大多数放射组学专用框架因过时、编程要求高或计算效率低而被排除在性能分析之外，而通用框架显示出更高的可访问性和易用性。

Conclusion: Simplatab为放射组学分类问题提供了性能、效率和可访问性的有效平衡。但现有AutoML框架仍存在显著不足，包括缺乏可访问的生存分析支持，以及特征可重复性和协调性整合有限。未来研究应专注于使AutoML解决方案更好地应对这些放射组学特定挑战。

Abstract: Automated machine learning (AutoML) frameworks can lower technical barriers for predictive and prognostic model development in radiomics by enabling researchers without programming expertise to build models. However, their effectiveness in addressing radiomics-specific challenges remains unclear. This study evaluates the performance, efficiency, and accessibility of general-purpose and radiomics-specific AutoML frameworks on diverse radiomics classification tasks, thereby highlighting development needs for radiomics. Ten public/private radiomics datasets with varied imaging modalities (CT/MRI), sizes, anatomies and endpoints were used. Six general-purpose and five radiomics-specific frameworks were tested with predefined parameters using standardized cross-validation. Evaluation metrics included AUC, runtime, together with qualitative aspects related to software status, accessibility, and interpretability. Simplatab, a radiomics-specific tool with a no-code interface, achieved the highest average test AUC (81.81%) with a moderate runtime (~1 hour). LightAutoML, a general-purpose framework, showed the fastest execution with competitive performance (78.74% mean AUC in six minutes). Most radiomics-specific frameworks were excluded from the performance analysis due to obsolescence, extensive programming requirements, or computational inefficiency. Conversely, general-purpose frameworks demonstrated higher accessibility and ease of implementation. Simplatab provides an effective balance of performance, efficiency, and accessibility for radiomics classification problems. However, significant gaps remain, including the lack of accessible survival analysis support and the limited integration of feature reproducibility and harmonization within current AutoML frameworks. Future research should focus on adapting AutoML solutions to better address these radiomics-specific challenges.

</details>


### [208] [Decodable but not structured: linear probing enables Underwater Acoustic Target Recognition with pretrained audio embeddings](https://arxiv.org/abs/2601.08358)
*Hilde I. Hummel,Sandjai Bhulai,Rob D. van der Mei,Burooj Ghani*

Main category: cs.LG

TL;DR: 首次对水下声学目标识别进行迁移学习的实证比较研究，发现冻结预训练音频模型权重后，通过简单的线性探测即可有效抑制录音特定特征并分离出船舶类型特征，实现低成本自动识别。


<details>
  <summary>Details</summary>
Motivation: 船舶噪声污染对海洋生态系统构成威胁，需要监测船舶辐射噪声。被动声学监测系统产生大量水下录音数据，手动分析不切实际，需要自动化方法。现有监督学习方法受限于标注数据稀缺，迁移学习提供有前景的替代方案。

Method: 首次对UATR进行迁移学习的实证比较研究，评估来自不同音频领域的多个预训练音频模型。冻结预训练模型权重，通过分类、聚类和相似性评估分析生成的嵌入表示。

Result: 嵌入空间的几何结构主要由录音特定特征主导，但简单的线性探测可以有效抑制这些录音特定信息，从嵌入中分离出船舶类型特征。线性探测能以低计算成本实现有效的自动UATR，显著减少对大量高质量标注船舶录音的需求。

Conclusion: 迁移学习为水下声学目标识别提供了有效解决方案，通过冻结预训练模型和线性探测，可以在标注数据有限的情况下实现高性能的船舶类型识别，降低计算成本和数据需求。

Abstract: Increasing levels of anthropogenic noise from ships contribute significantly to underwater sound pollution, posing risks to marine ecosystems. This makes monitoring crucial to understand and quantify the impact of the ship radiated noise. Passive Acoustic Monitoring (PAM) systems are widely deployed for this purpose, generating years of underwater recordings across diverse soundscapes. Manual analysis of such large-scale data is impractical, motivating the need for automated approaches based on machine learning. Recent advances in automatic Underwater Acoustic Target Recognition (UATR) have largely relied on supervised learning, which is constrained by the scarcity of labeled data. Transfer Learning (TL) offers a promising alternative to mitigate this limitation. In this work, we conduct the first empirical comparative study of transfer learning for UATR, evaluating multiple pretrained audio models originating from diverse audio domains. The pretrained model weights are frozen, and the resulting embeddings are analyzed through classification, clustering, and similarity-based evaluations. The analysis shows that the geometrical structure of the embedding space is largely dominated by recording-specific characteristics. However, a simple linear probe can effectively suppress this recording-specific information and isolate ship-type features from these embeddings. As a result, linear probing enables effective automatic UATR using pretrained audio models at low computational cost, significantly reducing the need for a large amounts of high-quality labeled ship recordings.

</details>


### [209] [Training-Free Distribution Adaptation for Diffusion Models via Maximum Mean Discrepancy Guidance](https://arxiv.org/abs/2601.08379)
*Matina Mahdizadeh Sani,Nima Jamali,Mohammad Jalali,Farzan Farnia*

Main category: cs.LG

TL;DR: 提出MMD Guidance方法，在推理阶段使用最大均值差异（MMD）作为指导信号，无需训练即可将扩散模型输出与目标数据分布对齐，特别适合少样本领域适应任务。


<details>
  <summary>Details</summary>
Motivation: 预训练扩散模型作为生成先验时，其输出常与用户特定目标数据特征不匹配。在领域适应任务中，仅有少量参考样本且重新训练扩散模型不可行时，这种不匹配问题尤为突出。现有推理时指导方法通常优化代理目标（如分类器似然），而非直接与目标分布对齐。

Method: 提出MMD Guidance，一种无需训练的机制，在反向扩散过程中加入生成样本与参考数据集之间最大均值差异（MMD）的梯度。MMD能从有限数据提供可靠的分布估计，实践中方差低且可高效微分。该方法通过乘积核自然扩展到条件生成模型的提示感知适应，并可在潜在扩散模型（LDM）的潜在空间中高效应用。

Result: 在合成和真实世界基准测试中，MMD Guidance能够实现分布对齐，同时保持样本保真度。

Conclusion: MMD Guidance是一种有效的训练免费方法，能够在推理阶段将扩散模型输出与目标数据分布对齐，特别适用于少样本领域适应场景，在保持样本质量的同时实现分布匹配。

Abstract: Pre-trained diffusion models have emerged as powerful generative priors for both unconditional and conditional sample generation, yet their outputs often deviate from the characteristics of user-specific target data. Such mismatches are especially problematic in domain adaptation tasks, where only a few reference examples are available and retraining the diffusion model is infeasible. Existing inference-time guidance methods can adjust sampling trajectories, but they typically optimize surrogate objectives such as classifier likelihoods rather than directly aligning with the target distribution. We propose MMD Guidance, a training-free mechanism that augments the reverse diffusion process with gradients of the Maximum Mean Discrepancy (MMD) between generated samples and a reference dataset. MMD provides reliable distributional estimates from limited data, exhibits low variance in practice, and is efficiently differentiable, which makes it particularly well-suited for the guidance task. Our framework naturally extends to prompt-aware adaptation in conditional generation models via product kernels. Also, it can be applied with computational efficiency in latent diffusion models (LDMs), since guidance is applied in the latent space of the LDM. Experiments on synthetic and real-world benchmarks demonstrate that MMD Guidance can achieve distributional alignment while preserving sample fidelity.

</details>


### [210] [Controlled LLM Training on Spectral Sphere](https://arxiv.org/abs/2601.08393)
*Tian Xie,Haoming Luo,Haoyu Tang,Yiwen Hu,Jason Klein Liu,Qingnan Ren,Yang Wang,Wayne Xin Zhao,Rui Yan,Bing Su,Chong Luo,Baining Guo*

Main category: cs.LG

TL;DR: 提出SSO优化器，通过强制权重和更新的谱约束实现完全μP对齐，在大规模训练中优于AdamW和Muon


<details>
  <summary>Details</summary>
Motivation: 现有优化器如Muon仅部分满足μP约束，控制更新但允许权重漂移，需要完全μP对齐的优化策略

Method: 引入谱球优化器(SSO)，在谱球上推导最速下降方向，强制权重和更新的模块级谱约束，实现高效并行算法

Result: 在多种架构上优于AdamW和Muon，提升MoE路由器负载均衡，抑制异常值，严格限制激活值

Conclusion: SSO实现了完全μP对齐的优化，提供理论保障和实践稳定性，适用于大规模模型训练

Abstract: Scaling large models requires optimization strategies that ensure rapid convergence grounded in stability. Maximal Update Parametrization ($\boldsymbolμ$P) provides a theoretical safeguard for width-invariant $Θ(1)$ activation control, whereas emerging optimizers like Muon are only ``half-aligned'' with these constraints: they control updates but allow weights to drift. To address this limitation, we introduce the \textbf{Spectral Sphere Optimizer (SSO)}, which enforces strict module-wise spectral constraints on both weights and their updates. By deriving the steepest descent direction on the spectral sphere, SSO realizes a fully $\boldsymbolμ$P-aligned optimization process. To enable large-scale training, we implement SSO as an efficient parallel algorithm within Megatron. Through extensive pretraining on diverse architectures, including Dense 1.7B, MoE 8B-A1B, and 200-layer DeepNet models, SSO consistently outperforms AdamW and Muon. Furthermore, we observe significant practical stability benefits, including improved MoE router load balancing, suppressed outliers, and strictly bounded activations.

</details>


### [211] [Out-of-distribution generalization of deep-learning surrogates for 2D PDE-generated dynamics in the small-data regime](https://arxiv.org/abs/2601.08404)
*Binh Duong Nguyen,Stefan Sandfeld*

Main category: cs.LG

TL;DR: me-UNet在二维周期性PDE小数据场景下，超越更复杂架构，实现高效准确的代理建模


<details>
  <summary>Details</summary>
Motivation: PDE模拟计算成本高，而科学应用常涉及空间场演化，需要数据驱动的场预测方法。研究关注小数据场景（最多O(10²)轨迹）和分布外初始条件的泛化能力。

Method: 提出多通道U-Net架构，在五个不同PDE家族上评估，与ViT、AFNO、PDE-Transformer、KAN-UNet比较。采用共同训练设置，分析场空间误差、谱相似性和物理指标。

Result: me-UNet在所有数据集上匹配或超越更复杂架构，训练时间显著减少。仅需约20个训练模拟即可泛化到未见初始条件。数据效率研究和Grad-CAM分析表明卷积架构在小数据场景中保持优势。

Conclusion: 在小数据周期性二维PDE设置中，具有局部性和周期性边界条件归纳偏置的卷积架构仍然是准确且具有适度分布外鲁棒性的代理建模的强有力竞争者。

Abstract: Partial differential equations (PDEs) are a central tool for modeling the dynamics of physical, engineering, and materials systems, but high-fidelity simulations are often computationally expensive. At the same time, many scientific applications can be viewed as the evolution of spatially distributed fields, making data-driven forecasting of such fields a core task in scientific machine learning. In this work we study autoregressive deep-learning surrogates for two-dimensional PDE dynamics on periodic domains, focusing on generalization to out-of-distribution initial conditions within a fixed PDE and parameter regime and on strict small-data settings with at most $\mathcal{O}(10^2)$ simulated trajectories per system. We introduce a multi-channel U-Net [...], evaluate it on five qualitatively different PDE families and compare it to ViT, AFNO, PDE-Transformer, and KAN-UNet under a common training setup. Across all datasets, me-UNet matches or outperforms these more complex architectures in terms of field-space error, spectral similarity, and physics-based metrics for in-distribution rollouts, while requiring substantially less training time. It also generalizes qualitatively to unseen initial conditions with as few as $\approx 20$ training simulations. A data-efficiency study and Grad-CAM analysis further suggest that, in small-data periodic 2D PDE settings, convolutional architectures with inductive biases aligned to locality and periodic boundary conditions remain strong contenders for accurate and moderately out-of-distribution-robust surrogate modeling.

</details>


### [212] [Taxon: Hierarchical Tax Code Prediction with Semantically Aligned LLM Expert Guidance](https://arxiv.org/abs/2601.08418)
*Jihang Li,Qing Liu,Zulong Chen,Jing Wang,Wei Wang,Chuanfei Xu,Zeyi Wen*

Main category: cs.LG

TL;DR: Taxon框架通过特征门控专家混合架构和LLM驱动的语义一致性模型，实现电商平台商品税码的层次化分类，已在阿里巴巴税务系统中部署，日均处理50万查询。


<details>
  <summary>Details</summary>
Motivation: 电商平台需要将商品准确映射到国家标准的层次化税码分类体系中，错误会导致财务不一致和监管风险。当前税码预测任务研究不足，且实际业务数据存在噪声监督问题。

Method: 1) 特征门控专家混合架构：自适应路由多模态特征到税码层次的不同层级；2) 语义一致性模型：从大型语言模型蒸馏，验证商品标题与官方税定义的对齐；3) 多源训练管道：结合税码数据库、发票验证日志和商家注册数据，提供结构和语义监督。

Result: 在专有TaxCode数据集和公开基准测试中达到最先进性能。完整的层次路径重建显著提升结构一致性，获得最高F1分数。已在阿里巴巴税务系统部署，日均处理50万税码查询，业务高峰期达500万以上，提高了准确性、可解释性和鲁棒性。

Conclusion: Taxon框架通过语义对齐和专家指导的方法，有效解决了电商平台税码预测的挑战，实现了准确、可解释且鲁棒的层次化分类，在实际生产环境中验证了其价值。

Abstract: Tax code prediction is a crucial yet underexplored task in automating invoicing and compliance management for large-scale e-commerce platforms. Each product must be accurately mapped to a node within a multi-level taxonomic hierarchy defined by national standards, where errors lead to financial inconsistencies and regulatory risks. This paper presents Taxon, a semantically aligned and expert-guided framework for hierarchical tax code prediction. Taxon integrates (i) a feature-gating mixture-of-experts architecture that adaptively routes multi-modal features across taxonomy levels, and (ii) a semantic consistency model distilled from large language models acting as domain experts to verify alignment between product titles and official tax definitions. To address noisy supervision in real business records, we design a multi-source training pipeline that combines curated tax databases, invoice validation logs, and merchant registration data to provide both structural and semantic supervision. Extensive experiments on the proprietary TaxCode dataset and public benchmarks demonstrate that Taxon achieves state-of-the-art performance, outperforming strong baselines. Further, an additional full hierarchical paths reconstruction procedure significantly improves structural consistency, yielding the highest overall F1 scores. Taxon has been deployed in production within Alibaba's tax service system, handling an average of over 500,000 tax code queries per day and reaching peak volumes above five million requests during business event with improved accuracy, interpretability, and robustness.

</details>


### [213] [Coverage Improvement and Fast Convergence of On-policy Preference Learning](https://arxiv.org/abs/2601.08421)
*Juno Kim,Jihun Yun,Jason D. Lee,Kwang-Sung Jun*

Main category: cs.LG

TL;DR: 在线策略偏好学习算法（如在线DPO）通过采样策略覆盖范围的动态改进，显著优于离线方法，实现指数级收敛，而离线方法受限于初始策略覆盖，收敛较慢。


<details>
  <summary>Details</summary>
Motivation: 解释为什么在线策略偏好学习算法（如在线DPO）在语言模型对齐中显著优于离线方法，从理论上分析采样策略覆盖范围如何随训练演化，并提出改进方案。

Method: 提出覆盖改进原则：在足够批次大小下，每次更新使策略覆盖范围向目标区域移动，使后续数据更具信息性。在上下文老虎机设置中分析在线DPO的收敛性，提出基于偏好G-最优设计的混合采样器，并开发一般函数类设置下的奖励蒸馏方案。

Result: 在线DPO在批次大小超过广义覆盖阈值时实现指数级收敛，而离线方法只能达到较慢的极小极大速率。提出的混合采样器消除对覆盖的依赖，保证两轮内收敛。实验验证在线DPO和奖励蒸馏算法优于离线对应方法，性能稳定单调提升。

Conclusion: 在线策略训练通过动态改进采样策略覆盖范围，使数据信息性不断增强，从而实现快速收敛。提出的混合采样器和奖励蒸馏方案进一步优化了在线学习效率，为语言模型对齐提供了理论指导和实用方法。

Abstract: Online on-policy preference learning algorithms for language model alignment such as online direct policy optimization (DPO) can significantly outperform their offline counterparts. We provide a theoretical explanation for this phenomenon by analyzing how the sampling policy's coverage evolves throughout on-policy training. We propose and rigorously justify the \emph{coverage improvement principle}: with sufficient batch size, each update moves into a region around the target where coverage is uniformly better, making subsequent data increasingly informative and enabling rapid convergence. In the contextual bandit setting with Bradley-Terry preferences and linear softmax policy class, we show that on-policy DPO converges exponentially in the number of iterations for batch size exceeding a generalized coverage threshold. In contrast, any learner restricted to offline samples from the initial policy suffers a slower minimax rate, leading to a sharp separation in total sample complexity. Motivated by this analysis, we further propose a simple hybrid sampler based on a novel \emph{preferential} G-optimal design, which removes dependence on coverage and guarantees convergence in just two rounds. Finally, we develop principled on-policy schemes for reward distillation in the general function class setting, and show faster noiseless rates under an alternative deviation-based notion of coverage. Experimentally, we confirm that on-policy DPO and our proposed reward distillation algorithms outperform their off-policy counterparts and enjoy stable, monotonic performance gains across iterations.

</details>


### [214] [DiffMM: Efficient Method for Accurate Noisy and Sparse Trajectory Map Matching via One Step Diffusion](https://arxiv.org/abs/2601.08482)
*Chenxu Han,Sean Bin Yang,Jilin Hu*

Main category: cs.LG

TL;DR: DiffMM：基于编码器-扩散模型的地图匹配框架，通过一步扩散过程处理稀疏GPS轨迹，在准确性和效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于HMM或编码器-解码器的地图匹配方法在处理噪声大或采样稀疏的GPS轨迹时面临显著挑战，需要更有效的解决方案。

Method: 1. 引入道路感知轨迹编码器，通过注意力机制将输入轨迹及其周围候选路段嵌入共享潜在空间；2. 提出一步扩散方法，利用轨迹和候选路段的联合嵌入作为条件上下文，通过捷径模型实现地图匹配。

Result: 在大规模轨迹数据集上的实验表明，该方法在准确性和效率上始终优于最先进的地图匹配方法，特别是在稀疏轨迹和复杂道路网络拓扑情况下表现突出。

Conclusion: DiffMM通过创新的编码器-扩散框架有效解决了稀疏轨迹地图匹配的挑战，为交通调度和交通流分析等应用提供了更可靠的解决方案。

Abstract: Map matching for sparse trajectories is a fundamental problem for many trajectory-based applications, e.g., traffic scheduling and traffic flow analysis. Existing methods for map matching are generally based on Hidden Markov Model (HMM) or encoder-decoder framework. However, these methods continue to face significant challenges when handling noisy or sparsely sampled GPS trajectories. To address these limitations, we propose DiffMM, an encoder-diffusion-based map matching framework that produces effective yet efficient matching results through a one-step diffusion process. We first introduce a road segment-aware trajectory encoder that jointly embeds the input trajectory and its surrounding candidate road segments into a shared latent space through an attention mechanism. Next, we propose a one step diffusion method to realize map matching through a shortcut model by leveraging the joint embedding of the trajectory and candidate road segments as conditioning context. We conduct extensive experiments on large-scale trajectory datasets, demonstrating that our approach consistently outperforms state-of-the-art map matching methods in terms of both accuracy and efficiency, particularly for sparse trajectories and complex road network topologies.

</details>


### [215] [Temporal Fusion Nexus: A task-agnostic multi-modal embedding model for clinical narratives and irregular time series in post-kidney transplant care](https://arxiv.org/abs/2601.08503)
*Aditya Kumar,Simon Rauch,Mario Cypko,Marcel Naik,Matthieu-P Schapranow,Aadil Rashid,Fabian Halleck,Bilgin Osmanodja,Roland Roller,Lars Pape,Klemens Budde,Mario Schiffer,Oliver Amft*

Main category: cs.LG

TL;DR: TFN是一个多模态任务无关的嵌入模型，用于整合不规则时间序列和非结构化临床叙事，在肾移植后护理中表现出色，在移植物丢失和排斥预测上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 临床环境中存在不规则时间序列数据（如实验室检查）和非结构化临床叙事（如医生笔记），需要一种能够整合这些异构数据源的方法来改善临床预测任务。

Method: 提出Temporal Fusion Nexus (TFN)模型，这是一个多模态、任务无关的嵌入模型，专门设计用于整合不规则时间序列和临床叙事文本，通过解耦学习获得可解释的潜在因子。

Result: 在3382名肾移植患者的回顾性队列中，TFN在移植物丢失（AUC 0.96 vs 0.94）、移植物排斥（AUC 0.84 vs 0.74）预测上优于现有方法，死亡率预测AUC达0.86。整合临床文本在所有任务中都提升了性能。

Conclusion: TFN能够有效整合异构临床数据源，在肾移植后护理中表现出优越性能，其解耦表示具有可解释性，且可推广到其他具有类似数据特征的临床任务中。

Abstract: We introduce Temporal Fusion Nexus (TFN), a multi-modal and task-agnostic embedding model to integrate irregular time series and unstructured clinical narratives. We analysed TFN in post-kidney transplant (KTx) care, with a retrospective cohort of 3382 patients, on three key outcomes: graft loss, graft rejection, and mortality. Compared to state-of-the-art model in post KTx care, TFN achieved higher performance for graft loss (AUC 0.96 vs. 0.94) and graft rejection (AUC 0.84 vs. 0.74). In mortality prediction, TFN yielded an AUC of 0.86. TFN outperformed unimodal baselines (approx 10% AUC improvement over time series only baseline, approx 5% AUC improvement over time series with static patient data). Integrating clinical text improved performance across all tasks. Disentanglement metrics confirmed robust and interpretable latent factors in the embedding space, and SHAP-based attributions confirmed alignment with clinical reasoning. TFN has potential application in clinical tasks beyond KTx, where heterogeneous data sources, irregular longitudinal data, and rich narrative documentation are available.

</details>


### [216] [Your Group-Relative Advantage Is Biased](https://arxiv.org/abs/2601.08521)
*Fengkai Yang,Zherui Chen,Xiaohan Wang,Xiaodong Lu,Jiajun Chai,Guojun Yin,Wei Lin,Shuai Ma,Fuzhen Zhuang,Deqing Wang,Yaodong Yang,Jianxin Li,Yikun Ban*

Main category: cs.LG

TL;DR: 本文发现基于分组的强化学习方法（如GRPO）中的组相对优势估计存在固有偏差，并提出历史感知自适应难度加权（HA-DW）来校正这种偏差，在五个数学推理基准上取得显著改进。


<details>
  <summary>Details</summary>
Motivation: 基于验证器奖励的强化学习（RLVR）已成为后训练大语言模型在推理任务上的常用方法，其中GRPO等分组方法被广泛采用。这些方法依赖组相对优势估计来避免学习批评家，但其理论性质仍不清楚。本文发现分组强化学习存在根本问题：组相对优势估计相对于真实（期望）优势存在固有偏差。

Method: 提出历史感知自适应难度加权（HA-DW），这是一种自适应重加权方案，基于演化难度锚点和训练动态调整优势估计。该方法通过校正偏差来平衡探索和利用，可集成到GRPO及其变体中。

Result: 理论分析和在五个数学推理基准上的实验表明，HA-DW在集成到GRPO及其变体时能持续提升性能。结果证明校正偏差优势估计对于鲁棒高效的RLVR训练至关重要。

Conclusion: 校正分组强化学习中存在的固有偏差优势估计是提升RLVR训练效果的关键，HA-DW方法通过自适应难度加权有效解决了这一问题，为后续研究提供了理论基础和实践方案。

Abstract: Reinforcement Learning from Verifier Rewards (RLVR) has emerged as a widely used approach for post-training large language models on reasoning tasks, with group-based methods such as GRPO and its variants gaining broad adoption. These methods rely on group-relative advantage estimation to avoid learned critics, yet its theoretical properties remain poorly understood.
  In this work, we uncover a fundamental issue of group-based RL: the group-relative advantage estimator is inherently biased relative to the true (expected) advantage. We provide the first theoretical analysis showing that it systematically underestimates advantages for hard prompts and overestimates them for easy prompts, leading to imbalanced exploration and exploitation. To address this issue, we propose History-Aware Adaptive Difficulty Weighting (HA-DW), an adaptive reweighting scheme that adjusts advantage estimates based on an evolving difficulty anchor and training dynamics. Both theoretical analysis and experiments on five mathematical reasoning benchmarks demonstrate that HA-DW consistently improves performance when integrated into GRPO and its variants. Our results suggest that correcting biased advantage estimation is critical for robust and efficient RLVR training.

</details>


### [217] [Contrastive and Multi-Task Learning on Noisy Brain Signals with Nonlinear Dynamical Signatures](https://arxiv.org/abs/2601.08549)
*Sucheta Ghosh,Zahra Monfared,Felix Dietrich*

Main category: cs.LG

TL;DR: 提出两阶段多任务学习框架用于EEG信号分析，整合去噪、动力学建模和表示学习，通过分阶段设计提升鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: EEG信号分析面临噪声干扰、非线性动力学特征提取困难以及表示学习不充分等挑战，需要一种能同时处理去噪、动力学建模和特征学习的统一框架。

Method: 两阶段框架：第一阶段使用去噪自编码器抑制伪影和稳定时域动态；第二阶段多任务架构处理去噪信号，包括运动想象分类、基于Lyapunov指数的混沌/非混沌判别、以及使用NT-Xent损失的自监督对比表示学习。采用卷积主干网络结合Transformer编码器捕获时空结构。

Result: 框架在EEG解码任务中超越了强基线和最新方法，不仅提高了鲁棒性和泛化能力，还证明了结合去噪、动力学特征和自监督学习的有效性。

Conclusion: 提出的两阶段多任务学习框架通过明确分离噪声抑制和高级特征学习，有效整合了去噪、动力学建模和表示学习，为EEG信号分析提供了稳健且可复现的解决方案。

Abstract: We introduce a two-stage multitask learning framework for analyzing Electroencephalography (EEG) signals that integrates denoising, dynamical modeling, and representation learning. In the first stage, a denoising autoencoder is trained to suppress artifacts and stabilize temporal dynamics, providing robust signal representations. In the second stage, a multitask architecture processes these denoised signals to achieve three objectives: motor imagery classification, chaotic versus non-chaotic regime discrimination using Lyapunov exponent-based labels, and self-supervised contrastive representation learning with NT-Xent loss. A convolutional backbone combined with a Transformer encoder captures spatial-temporal structure, while the dynamical task encourages sensitivity to nonlinear brain dynamics. This staged design mitigates interference between reconstruction and discriminative goals, improves stability across datasets, and supports reproducible training by clearly separating noise reduction from higher-level feature learning. Empirical studies show that our framework not only enhances robustness and generalization but also surpasses strong baselines and recent state-of-the-art methods in EEG decoding, highlighting the effectiveness of combining denoising, dynamical features, and self-supervised learning.

</details>


### [218] [EviNAM: Intelligibility and Uncertainty via Evidential Neural Additive Models](https://arxiv.org/abs/2601.08556)
*Sören Schleibaum,Anton Frederik Thielmann,Julian Teusch,Benjamin Säfken,Jörg P. Müller*

Main category: cs.LG

TL;DR: EviNAM：结合神经加法模型可解释性与证据学习的不确定性估计方法


<details>
  <summary>Details</summary>
Motivation: 现有不确定性估计方法（如贝叶斯神经网络和证据方法）缺乏可解释性，难以同时提供准确的不确定性估计和明确的特征贡献分析，限制了可靠决策支持。

Method: 扩展证据学习框架，集成神经加法模型（NAMs）的可解释性，通过单次前向传播同时估计偶然不确定性和认知不确定性，并提供明确的特征贡献分析。

Result: 在合成和真实数据上的实验表明，EviNAM在预测性能上达到最先进水平，同时提供可解释的不确定性估计和特征贡献。

Conclusion: EviNAM为更可理解和可信的预测提供了一条路径，虽然专注于回归任务，但自然扩展到分类和广义加法模型。

Abstract: Intelligibility and accurate uncertainty estimation are crucial for reliable decision-making. In this paper, we propose EviNAM, an extension of evidential learning that integrates the interpretability of Neural Additive Models (NAMs) with principled uncertainty estimation. Unlike standard Bayesian neural networks and previous evidential methods, EviNAM enables, in a single pass, both the estimation of the aleatoric and epistemic uncertainty as well as explicit feature contributions. Experiments on synthetic and real data demonstrate that EviNAM matches state-of-the-art predictive performance. While we focus on regression, our method extends naturally to classification and generalized additive models, offering a path toward more intelligible and trustworthy predictions.

</details>


### [219] [M$^2$FMoE: Multi-Resolution Multi-View Frequency Mixture-of-Experts for Extreme-Adaptive Time Series Forecasting](https://arxiv.org/abs/2601.08631)
*Yaohui Huang,Runmin Zou,Yun Wang,Laeeq Aslam,Ruipeng Dong*

Main category: cs.LG

TL;DR: M²FMoE：一种通过多分辨率多视图频率建模来学习常规和极端模式的极端自适应预测模型，在极端事件预测方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测方法在建模常规模式方面表现良好，但在极端事件期间性能显著下降，而极端事件正是实际应用中预测误差的主要来源。虽然有些方法通过加入辅助信号来改进性能，但仍无法捕捉极端事件的复杂时间动态。

Method: 提出M²FMoE模型，包含三个模块：1）多视图频率混合专家模块，在傅里叶和小波域为不同频谱带分配专家，通过跨视图共享带分割器对齐频率分区并实现专家间协作；2）多分辨率自适应融合模块，从粗到细分层聚合频率特征；3）时间门控集成模块，动态平衡长期趋势和短期频率感知特征。

Result: 在具有极端模式的真实世界水文数据集上的实验表明，M²FMoE在不需极端事件标签的情况下，优于最先进的基线方法。

Conclusion: M²FMoE通过多分辨率多视图频率建模有效捕捉了常规和极端模式，显著提升了极端事件下的时间序列预测性能。

Abstract: Forecasting time series with extreme events is critical yet challenging due to their high variance, irregular dynamics, and sparse but high-impact nature. While existing methods excel in modeling dominant regular patterns, their performance degrades significantly during extreme events, constituting the primary source of forecasting errors in real-world applications. Although some approaches incorporate auxiliary signals to improve performance, they still fail to capture extreme events' complex temporal dynamics. To address these limitations, we propose M$^2$FMoE, an extreme-adaptive forecasting model that learns both regular and extreme patterns through multi-resolution and multi-view frequency modeling. It comprises three modules: (1) a multi-view frequency mixture-of-experts module assigns experts to distinct spectral bands in Fourier and Wavelet domains, with cross-view shared band splitter aligning frequency partitions and enabling inter-expert collaboration to capture both dominant and rare fluctuations; (2) a multi-resolution adaptive fusion module that hierarchically aggregates frequency features from coarse to fine resolutions, enhancing sensitivity to both short-term variations and sudden changes; (3) a temporal gating integration module that dynamically balances long-term trends and short-term frequency-aware features, improving adaptability to both regular and extreme temporal patterns. Experiments on real-world hydrological datasets with extreme patterns demonstrate that M$^2$FMoE outperforms state-of-the-art baselines without requiring extreme-event labels.

</details>


### [220] [Provably Safe Reinforcement Learning using Entropy Regularizer](https://arxiv.org/abs/2601.08646)
*Abhijit Mazumdar,Rafal Wisniewski,Manuela L. Bujorianu*

Main category: cs.LG

TL;DR: 提出两种基于乐观不确定性原则的安全强化学习算法，其中熵正则化版本能改善遗憾并减少策略波动性。


<details>
  <summary>Details</summary>
Motivation: 在强化学习中，如何在保证安全约束的前提下学习最优策略是一个重要问题。特别是在学习阶段，需要确保以高概率满足安全约束，这对实际应用至关重要。

Method: 首先提出基于乐观不确定性原则（OFU）的算法，然后在此基础上引入熵正则化改进，得到主要算法。两种算法都针对马尔可夫决策过程的安全约束问题，在可达-避免框架下设计。

Result: 对两种算法进行了有限样本分析，推导了遗憾界。结果表明，熵正则化的加入改善了遗憾性能，并显著控制了基于OFU的安全强化学习算法固有的回合间波动性。

Conclusion: 熵正则化能有效提升安全强化学习算法的性能，既改善遗憾界又减少策略波动，为安全约束下的在线学习提供了更稳定的解决方案。

Abstract: We consider the problem of learning the optimal policy for Markov decision processes with safety constraints. We formulate the problem in a reach-avoid setup. Our goal is to design online reinforcement learning algorithms that ensure safety constraints with arbitrarily high probability during the learning phase. To this end, we first propose an algorithm based on the optimism in the face of uncertainty (OFU) principle. Based on the first algorithm, we propose our main algorithm, which utilizes entropy regularization. We investigate the finite-sample analysis of both algorithms and derive their regret bounds. We demonstrate that the inclusion of entropy regularization improves the regret and drastically controls the episode-to-episode variability that is inherent in OFU-based safe RL algorithms.

</details>


### [221] [TRACE: Reconstruction-Based Anomaly Detection in Ensemble and Time-Dependent Simulations](https://arxiv.org/abs/2601.08659)
*Hamid Gadirov,Martijn Westra,Steffen Frey*

Main category: cs.LG

TL;DR: 该研究使用卷积自编码器检测参数化卡门涡街模拟数据中的异常，比较了处理单帧的2D模型和处理时间堆栈的3D模型，发现3D模型能更好地利用时空上下文检测异常运动模式。


<details>
  <summary>Details</summary>
Motivation: 高维、时间相关的模拟数据中异常检测具有挑战性，因为存在复杂的空间和时间动态。需要研究重建基的异常检测方法来处理参数化卡门涡街模拟的集合数据。

Method: 使用卷积自编码器进行异常检测，比较两种方法：1）2D自编码器处理单个时间帧，2）3D自编码器处理短时间堆栈。评估体积时间相关数据，分析重建误差与质量空间分布的关系。

Result: 2D模型能识别单个时间步中的局部空间不规则性，而3D模型利用时空上下文检测异常运动模式，并减少跨时间的冗余检测。重建误差受质量空间分布的强烈影响，高度集中区域比分散配置产生更大误差。

Conclusion: 时间上下文对于动态模拟中稳健的异常检测至关重要。3D自编码器通过利用时空信息，在检测异常运动模式方面优于仅处理空间信息的2D模型。

Abstract: Detecting anomalies in high-dimensional, time-dependent simulation data is challenging due to complex spatial and temporal dynamics. We study reconstruction-based anomaly detection for ensemble data from parameterized Kármán vortex street simulations using convolutional autoencoders. We compare a 2D autoencoder operating on individual frames with a 3D autoencoder that processes short temporal stacks. The 2D model identifies localized spatial irregularities in single time steps, while the 3D model exploits spatio-temporal context to detect anomalous motion patterns and reduces redundant detections across time. We further evaluate volumetric time-dependent data and find that reconstruction errors are strongly influenced by the spatial distribution of mass, with highly concentrated regions yielding larger errors than dispersed configurations. Our results highlight the importance of temporal context for robust anomaly detection in dynamic simulations.

</details>


### [222] [Soft Partition-based KAPI-ELM for Multi-Scale PDEs](https://arxiv.org/abs/2601.08719)
*Vikas Dwivedi,Monica Sigovan,Bruno Sixou*

Main category: cs.LG

TL;DR: 提出软分区核自适应物理信息极限学习机(KAPI-ELM)，通过平滑分区长度联合控制配置中心和核宽度，实现连续粗到细分辨率，无需傅里叶特征或随机采样，在多种振荡、多尺度PDE问题上达到或超越现有PINN和TFC方法的精度。


<details>
  <summary>Details</summary>
Motivation: 现有物理信息机器学习方法在处理高度振荡、多尺度或奇异摄动PDE时面临谱偏差、反向传播成本高、需要手动调整核或傅里叶频率等问题，需要一种更高效、自适应的解决方案。

Method: 提出KAPI-ELM方法，采用软分区机制，通过平滑分区长度联合控制配置中心和Gaussian核宽度，实现连续粗到细分辨率调整。引入基于符号距离的加权策略稳定不规则几何上的最小二乘学习，仅需单次线性求解。

Result: 在八个基准测试（包括振荡ODE、高频Poisson方程、不规则形状域和刚性奇异摄动对流扩散问题）中，该方法匹配或超越了最先进的PINN和TFC变体的精度，同时计算效率更高。

Conclusion: 软分区核自适应为多尺度PDE提供了一种快速、架构无关的解决方案，在物理信息建模方面具有广泛潜力，虽然目前仅应用于稳态线性PDE，但展示了良好的扩展前景。

Abstract: Physics-informed machine learning holds great promise for solving differential equations, yet existing methods struggle with highly oscillatory, multiscale, or singularly perturbed PDEs due to spectral bias, costly backpropagation, and manually tuned kernel or Fourier frequencies. This work introduces a soft partition--based Kernel-Adaptive Physics-Informed Extreme Learning Machine (KAPI-ELM), a deterministic low-dimensional parameterization in which smooth partition lengths jointly control collocation centers and Gaussian kernel widths, enabling continuous coarse-to-fine resolution without Fourier features, random sampling, or hard domain interfaces. A signed-distance-based weighting further stabilizes least-squares learning on irregular geometries. Across eight benchmarks--including oscillatory ODEs, high-frequency Poisson equations, irregular-shaped domains, and stiff singularly perturbed convection-diffusion problems-the proposed method matches or exceeds the accuracy of state-of-the-art Physics-Informed Neural Network (PINN) and Theory of Functional Connections (TFC) variants while using only a single linear solve. Although demonstrated on steady linear PDEs, the results show that soft-partition kernel adaptation provides a fast, architecture-free approach for multiscale PDEs with broad potential for future physics-informed modeling. For reproducibility, the reference codes are available at https://github.com/vikas-dwivedi-2022/soft_kapi

</details>


### [223] [Model-Agnostic Solutions for Deep Reinforcement Learning in Non-Ergodic Contexts](https://arxiv.org/abs/2601.08726)
*Bert Verbruggen,Arne Vanhoyweghen,Vincent Ginis*

Main category: cs.LG

TL;DR: 传统强化学习在非遍历环境中表现不佳，引入时间依赖性的深度强化学习可以改善策略优化


<details>
  <summary>Details</summary>
Motivation: 传统强化学习基于贝尔曼方程和期望值优化，但在非遍历环境中，长期结果取决于具体轨迹而非整体平均，导致期望值公式产生系统性次优策略

Method: 在深度强化学习中引入显式时间依赖性，让网络函数近似能够纳入时间信息，使智能体能够估计与过程内在增长率一致的价值函数

Result: 传统强化学习架构在非遍历环境中无法找到真正最优解，深度强化学习实现同样产生次优策略，但引入时间依赖性后可以纠正这一限制

Conclusion: 通过让智能体暴露于时间轨迹中自然引入时间依赖性，无需改变环境反馈（如奖励转换或修改目标函数），就能改善非遍历系统中的强化学习方法

Abstract: Reinforcement Learning (RL) remains a central optimisation framework in machine learning. Although RL agents can converge to optimal solutions, the definition of ``optimality'' depends on the environment's statistical properties. The Bellman equation, central to most RL algorithms, is formulated in terms of expected values of future rewards. However, when ergodicity is broken, long-term outcomes depend on the specific trajectory rather than on the ensemble average. In such settings, the ensemble average diverges from the time-average growth experienced by individual agents, with expected-value formulations yielding systematically suboptimal policies. Prior studies demonstrated that traditional RL architectures fail to recover the true optimum in non-ergodic environments. We extend this analysis to deep RL implementations and show that these, too, produce suboptimal policies under non-ergodic dynamics. Introducing explicit time dependence into the learning process can correct this limitation. By allowing the network's function approximation to incorporate temporal information, the agent can estimate value functions consistent with the process's intrinsic growth rate. This improvement does not require altering the environmental feedback, such as reward transformations or modified objective functions, but arises naturally from the agent's exposure to temporal trajectories. Our results contribute to the growing body of research on reinforcement learning methods for non-ergodic systems.

</details>


### [224] [A Novel Approach to Explainable AI with Quantized Active Ingredients in Decision Making](https://arxiv.org/abs/2601.08733)
*A. M. A. S. D. Alagiyawanna,Asoka Karunananda,Thushari Silva,A. Mahasinghe*

Main category: cs.LG

TL;DR: 量子玻尔兹曼机在分类准确率和可解释性方面均优于经典玻尔兹曼机，为构建可信赖的可解释AI系统提供了新方向。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统在分类任务中表现出色，但缺乏可解释性成为重大挑战，特别是在医疗和金融等高风险领域，理解模型决策过程至关重要。

Method: 提出基于量子玻尔兹曼机和经典玻尔兹曼机比较研究的可解释AI框架。使用PCA预处理二值化降维的MNIST数据集，训练两种模型。QBMs采用具有强纠缠层的混合量子-经典电路，CBMs作为使用对比散度的经典基线。使用梯度显著性图和SHAP方法评估特征归因。

Result: QBMs在分类准确率上显著优于CBMs（83.5% vs. 54%），且特征归因分布更集中（熵值1.27 vs. 1.39），表明量子模型不仅预测性能更好，还能更清晰地识别影响预测的关键特征。

Conclusion: 量子-经典混合模型在准确率和可解释性方面均显示出改进，为构建更可信赖和可解释的AI系统提供了有前景的方向。

Abstract: Artificial Intelligence (AI) systems have shown good success at classifying. However, the lack of explainability is a true and significant challenge, especially in high-stakes domains, such as health and finance, where understanding is paramount. We propose a new solution to this challenge: an explainable AI framework based on our comparative study with Quantum Boltzmann Machines (QBMs) and Classical Boltzmann Machines (CBMs). We leverage principles of quantum computing within classical machine learning to provide substantive transparency around decision-making. The design involves training both models on a binarised and dimensionally reduced MNIST dataset, where Principal Component Analysis (PCA) is applied for preprocessing. For interpretability, we employ gradient-based saliency maps in QBMs and SHAP (SHapley Additive exPlanations) in CBMs to evaluate feature attributions.QBMs deploy hybrid quantum-classical circuits with strongly entangling layers, allowing for richer latent representations, whereas CBMs serve as a classical baseline that utilises contrastive divergence. Along the way, we found that QBMs outperformed CBMs on classification accuracy (83.5% vs. 54%) and had more concentrated distributions in feature attributions as quantified by entropy (1.27 vs. 1.39). In other words, QBMs not only produced better predictive performance than CBMs, but they also provided clearer identification of "active ingredient" or the most important features behind model predictions. To conclude, our results illustrate that quantum-classical hybrid models can display improvements in both accuracy and interpretability, which leads us toward more trustworthy and explainable AI systems.

</details>


### [225] [Adaptive Requesting in Decentralized Edge Networks via Non-Stationary Bandits](https://arxiv.org/abs/2601.08760)
*Yi Zhuang,Kun Yang,Xingran Chen*

Main category: cs.LG

TL;DR: 本文提出了一种用于边缘网络中信息新鲜度优化的去中心化协作请求算法，通过自适应重置的Aging Bandit算法解决非平稳多臂老虎机问题。


<details>
  <summary>Details</summary>
Motivation: 在边缘网络中，时间敏感客户端需要通过接入节点(ANs)请求内容，但无法观察AN状态或其他客户端行为。这种去中心化、部分可观测的环境导致奖励过程具有历史依赖性和客户端间耦合性，且奖励分布会同时发生突变和渐变，传统老虎机方法失效。

Method: 提出AGING BANDIT WITH ADAPTIVE RESET算法，结合自适应窗口和周期性监控来跟踪演变的奖励分布。该算法在去中心化设置中运行，客户端无需观察AN状态或其他客户端行为。

Result: 建立了理论性能保证，证明所提算法能实现接近最优的性能。通过仿真验证了理论结果。

Conclusion: 该算法能有效解决边缘网络中信息新鲜度优化的去中心化协作请求问题，特别是在非平稳、部分可观测且客户端间耦合的环境中表现出色。

Abstract: We study a decentralized collaborative requesting problem that aims to optimize the information freshness of time-sensitive clients in edge networks consisting of multiple clients, access nodes (ANs), and servers. Clients request content through ANs acting as gateways, without observing AN states or the actions of other clients. We define the reward as the age of information reduction resulting from a client's selection of an AN, and formulate the problem as a non-stationary multi-armed bandit. In this decentralized and partially observable setting, the resulting reward process is history-dependent and coupled across clients, and exhibits both abrupt and gradual changes in expected rewards, rendering classical bandit-based approaches ineffective. To address these challenges, we propose the AGING BANDIT WITH ADAPTIVE RESET algorithm, which combines adaptive windowing with periodic monitoring to track evolving reward distributions. We establish theoretical performance guarantees showing that the proposed algorithm achieves near-optimal performance, and we validate the theoretical results through simulations.

</details>


### [226] [Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs](https://arxiv.org/abs/2601.08763)
*Zhiyuan Hu,Yucheng Wang,Yufei He,Jiaying Wu,Yilun Zhao,See-Kiong Ng,Cynthia Breazeal,Anh Tuan Luu,Hae Won Park,Bryan Hooi*

Main category: cs.LG

TL;DR: 提出Uniqueness-Aware Reinforcement Learning方法，通过奖励独特的高层解题策略来解决RL训练LLM时的探索崩溃问题，提升pass@k和策略多样性


<details>
  <summary>Details</summary>
Motivation: 传统RL训练LLM时存在探索崩溃问题：策略过早集中在少数主导推理模式上，虽然提升了pass@1，但限制了rollout层面的多样性和pass@k的提升

Method: 提出Uniqueness-Aware Reinforcement Learning方法：使用LLM作为评判器，将同一问题的rollout按高层解题策略聚类，忽略表面差异，然后根据聚类大小反比重新加权策略优势，奖励正确但新颖的策略

Result: 在数学、物理和医学推理基准测试中，该方法在保持pass@1的同时，显著提升了pass@k和pass@k曲线下面积(AUC@K)，维持了探索性并发现了更多样化的解题策略

Conclusion: 通过显式奖励独特的高层解题策略，Uniqueness-Aware RL有效解决了RL训练LLM时的探索崩溃问题，提升了推理任务的多样性和整体性能

Abstract: Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@$k$ across large sampling budgets and increases the area under the pass@$k$ curve (AUC@$K$) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.

</details>


### [227] [Asymptotic Universal Alignment: A New Alignment Framework via Test-Time Scaling](https://arxiv.org/abs/2601.08777)
*Yang Cai,Weiqiang Zheng*

Main category: cs.LG

TL;DR: 论文提出了一种通过测试时扩展实现通用对齐的新框架，证明了最优收敛率为k/(k+1)，并指出现有方法因缺乏输出多样性而无法充分利用测试时扩展的优势。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型需要服务具有异质且可能冲突偏好的用户，这是个性化和可信AI的核心挑战。现有对齐方法在测试时扩展方面存在不足，无法充分利用多个候选响应带来的优势。

Method: 提出(k,f(k))-鲁棒对齐和渐近通用对齐(U-alignment)的形式化框架。引入对称多玩家对齐游戏，证明(k+1)-玩家对齐游戏的任何对称纳什均衡策略都能实现最优的(k,k/(k+1))-鲁棒对齐。

Result: 证明了最优收敛率：存在单输出策略族，其k样本乘积策略以速率f(k)=k/(k+1)实现U-alignment，且没有方法能在一般情况下实现更快的速率。现有方法如NLHF在k>1时可能无法保证胜率超过1/2。

Conclusion: 测试时扩展是实现通用对齐的有效途径，但现有对齐方法因缺乏输出多样性而无法充分利用这一优势。提出的对称多玩家对齐游戏框架能够保持输出多样性并达到最优测试时扩展速率。

Abstract: Aligning large language models (LLMs) to serve users with heterogeneous and potentially conflicting preferences is a central challenge for personalized and trustworthy AI. We formalize an ideal notion of universal alignment through test-time scaling: for each prompt, the model produces $k\ge 1$ candidate responses and a user selects their preferred one. We introduce $(k,f(k))$-robust alignment, which requires the $k$-output model to have win rate $f(k)$ against any other single-output model, and asymptotic universal alignment (U-alignment), which requires $f(k)\to 1$ as $k\to\infty$. Our main result characterizes the optimal convergence rate: there exists a family of single-output policies whose $k$-sample product policies achieve U-alignment at rate $f(k)=\frac{k}{k+1}$, and no method can achieve a faster rate in general.
  We show that popular post-training methods, including Nash learning from human feedback (NLHF), can fundamentally underutilize the benefits of test-time scaling. Even though NLHF is optimal for $k=1$, sampling from the resulting (often deterministic) policy cannot guarantee win rates above $\tfrac{1}{2}$ except for an arbitrarily small slack. This stems from a lack of output diversity: existing alignment methods can collapse to a single majority-preferred response, making additional samples redundant. In contrast, our approach preserves output diversity and achieves the optimal test-time scaling rate. In particular, we propose a family of symmetric multi-player alignment games and prove that any symmetric Nash equilibrium policy of the $(k+1)$-player alignment game achieves the optimal $(k,\frac{k}{k+1})$-robust alignment. Finally, we provide theoretical convergence guarantees for self-play learning dynamics in these games and extend the framework to opponents that also generate multiple responses.

</details>


### [228] [Fast and explainable clustering in the Manhattan and Tanimoto distance](https://arxiv.org/abs/2601.08781)
*Stefan Güttel,Kaustubh Roy*

Main category: cs.LG

TL;DR: CLASSIX算法扩展到支持曼哈顿距离和Tanimoto距离，通过使用向量范数排序和三角不等式优化搜索，在化学指纹基准测试中比现有算法快30-80倍且聚类质量更高。


<details>
  <summary>Details</summary>
Motivation: 原始CLASSIX算法仅支持欧几里得距离，限制了其在其他距离度量场景的应用。本文旨在扩展CLASSIX算法以支持更多距离度量，特别是曼哈顿距离和Tanimoto距离，提升算法在多样化应用场景中的适用性。

Method: 使用数据向量的适当范数作为排序标准替代主成分分析，结合三角不等式实现搜索终止。对于Tanimoto距离，采用可证明更尖锐的交集不等式进一步优化算法性能。

Result: 在真实化学指纹基准测试中，CLASSIX Tanimoto算法比Taylor-Butina算法快约30倍，比DBSCAN快约80倍，同时计算出的聚类质量更高。

Conclusion: 扩展后的CLASSIX算法成功支持多种距离度量，在保持快速和可解释性的同时，显著提升了在特定应用领域（如化学指纹分析）的性能表现。

Abstract: The CLASSIX algorithm is a fast and explainable approach to data clustering. In its original form, this algorithm exploits the sorting of the data points by their first principal component to truncate the search for nearby data points, with nearness being defined in terms of the Euclidean distance. Here we extend CLASSIX to other distance metrics, including the Manhattan distance and the Tanimoto distance. Instead of principal components, we use an appropriate norm of the data vectors as the sorting criterion, combined with the triangle inequality for search termination. In the case of Tanimoto distance, a provably sharper intersection inequality is used to further boost the performance of the new algorithm. On a real-world chemical fingerprint benchmark, CLASSIX Tanimoto is about 30 times faster than the Taylor--Butina algorithm, and about 80 times faster than DBSCAN, while computing higher-quality clusters in both cases.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [229] [Systemic Risk in DeFi: A Network-Based Fragility Analysis of TVL Dynamics](https://arxiv.org/abs/2601.08540)
*Shiyu Zhang,Zining Wang,Jin Zheng,John Cartlidge*

Main category: q-fin.RM

TL;DR: 本文提出一个统一的量化框架，用于分析DeFi生态系统层面的系统性风险，包括基于网络视角的DeFi相关性脆弱性指标（CFI）和风险贡献评分（RCS），以追踪时变系统性风险并识别风险积累和放大的重要功能模块。


<details>
  <summary>Details</summary>
Motivation: 现有DeFi系统性风险研究大多基于特定事件或孤立风险渠道，缺乏对系统性风险结构维度的关注，需要统一的生态系统层面分析和持续监控框架。

Method: 从网络视角出发，提出DeFi相关性脆弱性指标（CFI），基于协议类别层面的时变相关性网络构建，捕捉与相关性集中和同步性增加相关的生态系统结构脆弱性；同时定义风险贡献评分（RCS）量化不同协议类型对整体系统性风险的边际贡献。

Result: CFI和RCS相结合，能够追踪时变系统性风险，并识别在风险积累和放大过程中具有结构重要性的功能模块。

Conclusion: 该研究为DeFi生态系统层面的系统性风险分析提供了统一的量化框架，实现了对系统性风险的持续监控和结构重要性的识别。

Abstract: Systemic risk refers to the overall vulnerability arising from the high degree of interconnectedness and interdependence within the financial system. In the rapidly developing decentralized finance (DeFi) ecosystem, numerous studies have analyzed systemic risk through specific channels such as liquidity pressures, leverage mechanisms, smart contract risks, and historical risk events. However, these studies are mostly event-driven or focused on isolated risk channels, paying limited attention to the structural dimension of systemic risk. Overall, this study provides a unified quantitative framework for ecosystem-level analysis and continuous monitoring of systemic risk in DeFi. From a network-based perspective, this paper proposes the DeFi Correlation Fragility Indicator (CFI), constructed from time-varying correlation networks at the protocol category level. The CFI captures ecosystem-wide structural fragility associated with correlation concentration and increasing synchronicity. Furthermore, we define a Risk Contribution Score (RCS) to quantify the marginal contribution of different protocol types to overall systemic risk. By combining the CFI and RCS, the framework enables both the tracking of time-varying systemic risk and identification of structurally important functional modules in risk accumulation and amplification.

</details>


<div id='q-fin.GN'></div>

# q-fin.GN [[Back]](#toc)

### [230] [A Blessing in Disguise: How DeFi Hacks Trigger Unintended Liquidity Injections into US Money Markets](https://arxiv.org/abs/2601.08263)
*Tingyi Lin*

Main category: q-fin.GN

TL;DR: DeFi漏洞事件并未导致传统短期融资市场不稳定，反而通过"流动性回收"机制降低了高评级商业票据利差，为实体经济提供流动性支持


<details>
  <summary>Details</summary>
Motivation: 研究DeFi漏洞是否如"传染假说"所预测的那样会破坏传统短期融资市场稳定性，检验资本从DeFi流向传统金融系统的实际影响

Method: 分析主要DeFi攻击事件后3个月期AA级商业票据利差变化，识别"流动性回收"机制——资本通过货币市场基金重新中介进入传统金融系统

Result: DeFi漏洞事件后，3个月期AA级商业票据利差显著收窄而非扩大，机构需求冲击（资本流入）超过稳定币发行商赎回带来的供给冲击

Conclusion: DeFi冲击非但不是金融传染媒介，反而成为分割市场中的"安全阀"，为实体经济高评级发行人提供临时流动性支持并降低借贷成本

Abstract: Do vulnerabilities in Decentralized Finance (DeFi) destabilize traditional short-term funding markets? While the prevailing "Contagion Hypothesis" posits that the liquidation of stablecoin reserves triggers fire-sale spirals that transmit distress to traditional markets , we document a robust "Flight-to-Quality" effect to the contrary. In the wake of major DeFi exploits, spreads on 3-month AA-rated commercial paper (CP) exhibit a paradoxical narrowing. We identify a "liquidity recycling" mechanism driving this outcome: capital fleeing DeFi protocols is re-intermediated into the traditional financial system via Prime Money Market Funds (MMFs) , where strict regulatory constraints (e.g., SEC Rule 2a-7) compel these funds to purchase high-quality paper. Our estimates indicate that this institutional demand shock quantitatively overwhelms the supply shock driven by stablecoin issuer redemptions. Rather than acting as vectors of financial contagion , these crypto native shocks serve as an inadvertent "safety valve" in segmented markets , providing transient liquidity support and effectively subsidizing borrowing costs for high-grade issuers in the real economy.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [231] [Convergence of a Multi-Inertial-Iteration Scheme in Cone b, p-Normed Banach Spaces](https://arxiv.org/abs/2601.07837)
*Elvin Rada*

Main category: math.OC

TL;DR: 提出并分析锥b p-范数Banach空间中的多惯性迭代方案，通过引入三个独立惯性参数和多个误差控制序列，扩展经典Krasnoselskii-Mann和两步惯性迭代，在温和假设下建立收敛定理，数值示例显示比经典方法加速收敛。


<details>
  <summary>Details</summary>
Motivation: 经典Krasnoselskii-Mann迭代和两步惯性迭代在收敛速度方面存在局限，需要更灵活的框架来加速收敛。在更一般的锥b p-范数Banach空间中，通过引入多个惯性参数和误差控制机制，可以设计更高效的迭代方案。

Method: 提出多惯性迭代方案，在锥b p-范数Banach空间中，引入三个独立惯性参数和多个误差控制序列，扩展经典Krasnoselskii-Mann和两步惯性迭代。在映射满足准非扩张性、弱压缩性和相容性等温和假设下，建立收敛理论。

Result: 建立了保证不动点存在性和唯一性的收敛定理，数值示例表明该方法相比经典Krasnoselskii-Mann方法具有加速收敛效果，验证了多惯性参数和误差控制机制的有效性。

Conclusion: 所提出的多惯性迭代方案在锥b p-范数Banach空间中有效，通过多个惯性参数和误差控制序列实现了加速收敛，为更一般的Banach空间中的不动点问题提供了新的迭代框架。

Abstract: We propose and analyze a multi-inertial-iteration scheme in cone b, p-normed Banach spaces. This framework extends the classical Krasnoselskii-Mann and two-step inertial iterations by incorporating three independent inertial parameters and multiple error-control sequences. Under mild assumptions such as quasi-nonexpansiveness, weak contraction, and compatibility of mappings, we establish convergence theorems guaranteeing the existence and uniqueness of fixed points. Illustrative numerical examples demonstrate accelerated convergence compared with the classical Krasnoselskii-Mann method.

</details>


### [232] [Dual characterizations of norm minimization problems](https://arxiv.org/abs/2601.08153)
*Nguyen Duy Cuong*

Main category: math.OC

TL;DR: 论文研究范数乘积空间上的范数最小化问题，建立对偶最优性条件，并在已知一个最优解及其对偶向量的情况下推导解集的显式公式。


<details>
  <summary>Details</summary>
Motivation: 研究范数乘积空间上的范数最小化问题具有理论和实际意义，这类问题在优化、信号处理、机器学习等领域广泛存在。需要建立系统的最优性理论，为这类问题的求解提供理论基础。

Method: 建立对偶必要和充分最优性条件，在已知一个最优解及其对偶向量的假设下，推导解集的显式公式。特别研究了三种重要的乘积范数：和范数、最大范数和p-范数。

Result: 获得了范数乘积空间上范数最小化问题的对偶最优性条件，并在已知一个最优解及其对偶向量的情况下，得到了解集的显式表达式。通过有限维和无限维空间中各种范数的实例验证了理论结果。

Conclusion: 论文为范数乘积空间上的范数最小化问题建立了完整的对偶理论框架，提供了求解这类问题的有效方法，特别在已知一个最优解的情况下能够显式描述整个解集。

Abstract: The paper studies a general norm minimization problem on a product of normed vector spaces. We establish dual necessary and sufficient optimality conditions and derive explicit formulas for the corresponding solution sets. These formulas are obtained under the assumption that one optimal solution together with its associated dual vectors arising from the optimality conditions is known. Three important cases of product norms, namely the sum norm, maximum norm and $p$-norm, are also studied. Several examples in finite and infinite dimensional spaces equipped with various types of norms are presented to illustrate the established results.

</details>


### [233] [Stratification for Nonlinear Semidefinite Programming](https://arxiv.org/abs/2601.08362)
*Chenglong Bao,Chao Ding,Fuxiaoyue Feng,Jingyu Li*

Main category: math.OC

TL;DR: 该论文提出了非线性半定规划(NLSDP)的分层框架，通过指数分层揭示非光滑KKT系统的几何结构，建立了分层变分分析，提出了可验证的弱二阶条件和弱严格Robinson约束品性，并设计了分层高斯-牛顿算法。


<details>
  <summary>Details</summary>
Motivation: 非线性半定规划中的非光滑KKT系统缺乏几何理解，传统正则性条件过于严格且难以验证。需要建立更精细的几何框架来揭示半定锥的结构，并设计能利用这种结构的有效算法。

Method: 1. 基于S^n的指数分层及其提升到原-对偶空间，建立分层变分分析框架；2. 定义层限制正则性，用弱二阶条件和弱严格Robinson约束品性刻画；3. 通过横截性几何解释W-SRCQ，证明其在环境空间中的一般性和沿层的稳定性；4. 设计分层高斯-牛顿法，包含法向步和校正机制，通过最小二乘价值函数全局求解KKT方程。

Result: 1. 证明了弱二阶条件和弱严格Robinson约束品性可验证层限制正则性；2. 揭示了W-SRCQ的横截性几何意义及其一般性和稳定性；3. 证明了经典强形式正则性条件对应于层限制条件的局部一致有效性；4. 算法全局收敛到方向稳定点，在W-SOC和SRCQ下局部二次收敛到KKT对并识别活跃层。

Conclusion: 分层框架为非线性半定规划提供了深刻的几何洞察，建立了更精细的正则性理论，并设计了能利用分层结构的高效算法。该框架将经典强条件与层限制条件联系起来，为处理非光滑KKT系统提供了系统方法。

Abstract: This paper introduces a stratification framework for nonlinear semidefinite programming (NLSDP) that reveals and utilizes the geometry behind the nonsmooth KKT system. Based on the \emph{index stratification} of $\mathbb{S}^n$ and its lift to the primal--dual space, a stratified variational analysis is developed. Specifically, we define the stratum-restricted regularity property, characterize it by the verifiable weak second order condition (W-SOC) and weak strict Robinson constraint qualification (W-SRCQ), and interpret the W-SRCQ geometrically via transversality, which provides its genericity over ambient space and stability along strata. The interactions of these properties across neighboring strata are further examined, leading to the conclusion that classical strong-form regularity conditions correspond to the local uniform validity of stratum-restricted counterparts. On the algorithmic side, a stratified Gauss--Newton method with normal steps and a correction mechanism is proposed for globally solving the KKT equation through a least-squares merit function. We demonstrate that the algorithm converges globally to directional stationary points. Moreover, under the W-SOC and the strict Robinson constraint qualification (SRCQ), it achieves local quadratic convergence to KKT pairs and eventually identifies the active stratum.

</details>


### [234] [Kantorovich Distance via Spanning Trees: Properties and Algorithms](https://arxiv.org/abs/2601.08396)
*Jérémie Bigot,Luis Fredes*

Main category: math.OC

TL;DR: 论文研究有限度量空间上概率测度的最优传输问题，其中地面成本由加权连通图诱导的距离定义。通过将Kantorovich距离重新表述为生成树集合上的最小化问题，推导了Kantorovich势的显式公式，提出了基于动态规划的高效最优传输计划计算方法，并设计了模拟退火算法来寻找最优生成树。


<details>
  <summary>Details</summary>
Motivation: 研究在有限度量空间上基于图距离的最优传输问题，旨在利用生成树重新表述Kantorovich距离，从而更高效地计算最优传输计划和Kantorovich势。

Method: 1. 将Kantorovich距离重新表述为生成树集合上的最小化问题；2. 在弱非退化条件下，基于最优生成树推导Kantorovich势的显式公式；3. 提出基于动态规划的最优传输计划计算方法；4. 设计基于模拟退火的随机算法来寻找最优生成树。

Result: 1. 得到了Kantorovich势在最优生成树上的显式表达式；2. 证明了存在可通过动态规划高效计算的最优传输计划；3. 提出的模拟退火算法能够有效计算最优生成树；4. 数值实验验证了理论结果和方法的实际有效性。

Conclusion: 该研究为有限度量空间上的最优传输问题提供了基于生成树的新框架，不仅给出了理论保证，还提出了高效的计算方法，为图结构数据的最优传输应用提供了实用工具。

Abstract: We study optimal transport between probability measures supported on the same finite metric space, where the ground cost is a distance induced by a weighted connected graph. Building on recent work showing that the resulting Kantorovich distance can be expressed as a minimization problem over the set of spanning trees of this underlying graph, we investigate the implications of this reformulation on the construction of an optimal transport plan and a dual potential based on the solution of such an optimization problem. In this setting, we derive an explicit formula for the Kantorovich potential in terms of the imbalanced cumulative mass (a generalization of the cumulative distribution in R) along an optimal spanning tree solving such a minimization problem, under a weak non-degeneracy condition on the pair of measures that guarantees the uniqueness of a dual potential. Our second contribution establishes the existence of an optimal transport plan that can be computed efficiently by a dynamic programming procedure once an optimal spanning tree is known. Finally, we propose a stochastic algorithm based on simulated annealing on the space of spanning trees to compute such an optimal spanning tree. Numerical experiments illustrate the theoretical results and demonstrate the practical relevance of the proposed approach for optimal transport on finite metric spaces.

</details>


### [235] [Two-Product Make-to-Stock System: Strategic Joining and Optimal Inventory Levels](https://arxiv.org/abs/2601.08426)
*Odysseas Kanavetas,Ekaterina Kosarevskaia*

Main category: math.OC

TL;DR: 分析双产品按库存生产排队系统，顾客在不知库存情况下做战略进入/离开决策，研究纳什均衡存在性与最优库存策略


<details>
  <summary>Details</summary>
Motivation: 研究具有战略顾客行为的库存-排队系统，顾客在不知道当前库存水平的情况下需要做出进入或离开决策，这在实际服务系统中很常见

Method: 建立双产品按库存生产排队模型，顾客到达为独立泊松过程，分析顾客战略决策的纳什均衡，从利润最大化和社会福利最大化两个角度优化库存策略

Result: 证明了不同库存场景下顾客加入策略纳什均衡的存在性和唯一性，给出了关键性能指标的闭式表达式，并确定了最优基准库存水平

Conclusion: 该研究为具有战略顾客行为的库存-排队系统提供了理论分析框架，揭示了顾客战略行为对库存决策的影响，为实际运营管理提供了理论指导

Abstract: This paper analyzes a two-product make-to-stock queueing system where a single production facility serves two customer classes with independent Poisson arrivals. Customers make strategic join-or-balk decisions without observing current inventory levels. The analysis establishes the existence and uniqueness of Nash equilibria in customer joining strategies for various inventory scenarios. Optimal base-stock levels are characterized from both profit-maximizing and welfare-maximizing perspectives, with closed-form expressions for key performance measures.

</details>


### [236] [Exploring an Alternative Line-Search Method for Lagrange-Newton Optimization](https://arxiv.org/abs/2601.08431)
*Ralf Möller*

Main category: math.OC

TL;DR: 提出一种适用于拉格朗日-牛顿法的新线搜索策略，基于牛顿步向量的散度作为搜索准则，通过"之字形"策略在准则零点的"沟壑网络"中导航以接近驻点。


<details>
  <summary>Details</summary>
Motivation: 拉格朗日-牛顿法中所有驻点都是鞍点，无法使用基于目标函数值的传统线搜索方法，而基于价值函数的线搜索方法存在局限性，特别是在狭窄山谷中的步长阻尼问题需要解决。

Method: 提出基于牛顿步向量散度的线搜索准则，该准则在二维测试函数中形成底部平坦的"沟壑网络"，通过设计"之字形"策略在这些沟壑网络中导航以接近驻点。

Result: 数值实验表明，新线搜索策略在大多数起始点对所有测试函数都有效，但只在某些情况下表现出期望的步长阻尼效果，因此目前难以评估该方法的实际效用。

Conclusion: 虽然新线搜索策略在概念上有创新性，通过散度准则和沟壑网络导航提供了新思路，但其步长阻尼效果不稳定，实际效用仍需进一步研究和验证。

Abstract: In the Lagrange-Newton method, where Newton's method is applied to a Lagrangian function that includes equality constraints, all stationary points are saddle points. It is therefore not possible to use a line-search method based on the value of the objective function; instead, the line search can operate on merit functions. In this report, we explore an alternative line-search method which is applicable to this case; it particulary addresses the damping of the step length in tight valleys. We propose a line-search criterion based on the divergence of the field of Newton step vectors. The visualization of the criterion for two-dimensional test functions reveals a network of ravines with flat bottom at the zero points of the criterion. The ravines are typically connected to stationary points. To traverse this ravine network in order to approach a stationary point, a zigzag strategy is devised. Numerical experiments demonstrate that the novel line-search strategy succeeds from most starting points in all test functions, but only exhibits the desired damping of the step length in some situations. At the present stage it is therefore difficult to appraise the utility of this contribution.

</details>


### [237] [An efficient mixed-integer linear programming formulation for solving influence diagrams](https://arxiv.org/abs/2601.08460)
*Topias Terho,Fabricio Oliveira,Ahti Salo,Pedro Munari*

Main category: math.OC

TL;DR: 提出一种针对传统MILP方法难以处理的影响图的高效混合整数线性规划(MILP)公式，特别适用于非周期性结构或大状态空间问题，并支持条件风险价值最大化及约束整合。


<details>
  <summary>Details</summary>
Motivation: 传统MILP方法在处理影响图时，对于周期性决策过程（可转化为部分可观测马尔可夫决策过程）效率较高，但对于缺乏周期性结构或节点状态空间大的问题效率低下，限制了实际应用。

Method: 提出专门针对传统MILP方法难以处理的影响图的高效MILP公式，该公式可适配条件风险价值最大化，并能整合机会约束和逻辑约束，保持MILP方法的建模灵活性。

Result: 通过文献中的问题进行计算实验，发现基于新公式的MILP模型在解决无法转化为部分可观测马尔可夫决策过程的影响图时，计算效率显著优于现有最先进方法。

Conclusion: 新提出的MILP公式有效解决了传统方法在处理非周期性结构或大状态空间影响图时的效率瓶颈，扩展了MILP方法在实际决策问题中的应用范围。

Abstract: Influence diagrams represent decision-making problems with interdependencies between random events, decisions, and consequences. Traditionally, they have been solved using algorithms that determine the expected utility-maximizing decision strategy. In contrast, state-of-the-art solution approaches convert influence diagrams into a mixed-integer linear programming (MILP) model, which can be solved with powerful off-the-shelf MILP solvers. From a computational standpoint, the existing MILP formulations can be efficiently solved when applied to influence diagrams that represent periodic (or sequential) decision processes, which can be cast as partially observable Markov Decision Processes. However, they are inefficient in problems that lack a periodic structure or if the nodes in the influence diagram have large state spaces, thus limiting their practical use. In this paper, we present an efficient MILP formulation that is specifically designed for influence diagrams that are challenging for the earlier MILP formulation-based methods. Additionally, we present how the proposed formulation can be adapted to maximize conditional value-at-risk and how chance and logical constraints can be incorporated into the formulation, thus retaining the modeling flexibility of the MILP-based methods. Finally, we perform computational experiments addressing problems from the literature and compare the computational efficiency of the proposed formulation against the available MILP formulations for the reported influence diagrams. We find that the MILP models based on the proposed formulations can be solved significantly more efficiently compared to the state-of-the-art when solving influence diagrams that cannot be cast as partially observable Markov decision processes.

</details>


### [238] [A New Duality-Free Framework for Convex Optimisation with Superlinear Convergence and Effective Warm-Starting](https://arxiv.org/abs/2601.08494)
*Michael Cummins,Eric Kerrigan*

Main category: math.OC

TL;DR: 提出PVM框架，将约束优化问题转化为无约束值函数最小化问题，开发二阶算法解决QP问题，支持热启动并证明超线性收敛


<details>
  <summary>Details</summary>
Motivation: 传统二阶求解器（如内点法）依赖原始对偶信息且难以热启动，限制了在实时控制中的应用。需要一种无需对偶信息、支持热启动的框架。

Method: 提出PVM（无对偶框架），将约束问题重构为值函数的无约束最小化问题。基于PPA（邻近点算法）和半光滑牛顿法开发二阶算法解决二次规划问题。

Result: PVM框架总能找到解，提供不可行性证书，支持热启动。数值实验显示在MPC问题上与最先进求解器性能相当，热启动时牛顿迭代减少达70%。

Conclusion: PVM为约束优化提供了无需对偶信息、支持热启动的有效框架，特别适用于实时控制应用，在冷启动和热启动情况下均表现优异。

Abstract: Modern second order solvers for convex optimisation, such as interior point methods, rely on primal dual information and are difficult to warm start, limiting their applicability in real time control. We propose the PVM, a duality free framework that reformulates the constrained problem as the unconstrained minimisation of a value function. The resulting problem always has a solution, yields a certificate of infeasibility and is amenable to warm starting. We develop a second order algorithm for Quadratic Programming based on the PPA and semismooth Newton methods, and establish sufficient conditions for superlinear convergence to an arbitrarily small neighbourhood of the solution. Numerical experiments on a MPC problem demonstrate competitive performance with state of the art solvers from a cold start and up to 70\% reduction in Newton iterations when warm starting.

</details>


### [239] [Convergence of gradient flow for learning convolutional neural networks](https://arxiv.org/abs/2601.08547)
*Jona-Maria Diederen,Holger Rauhut,Ulrich Terstiege*

Main category: math.OC

TL;DR: 本文证明了在线性卷积网络的简化设定下，对于包括平方损失在内的特定损失函数，梯度流（作为梯度下降的抽象）在训练数据满足温和条件下总能收敛到临界点。


<details>
  <summary>Details</summary>
Motivation: 卷积神经网络在图像识别中广泛应用，但其训练涉及非凸函数优化，使得分析标准优化方法（如梯度下降变体）变得困难。为了理解优化过程，需要研究简化模型。

Method: 研究线性卷积网络的简化设定，分析梯度流（作为梯度下降的抽象）在特定损失函数（包括平方损失）下的行为，证明其在训练数据满足温和条件下的收敛性。

Result: 证明梯度流应用于经验风险时，在训练数据满足温和条件下，总能收敛到临界点。这为理解卷积网络优化提供了理论保证。

Conclusion: 在线性卷积网络的简化模型中，梯度流优化具有收敛保证，这有助于理解实际卷积神经网络训练中的优化行为，为更复杂的非凸优化问题提供理论基础。

Abstract: Convolutional neural networks are widely used in imaging and image recognition. Learning such networks from training data leads to the minimization of a non-convex function. This makes the analysis of standard optimization methods such as variants of (stochastic) gradient descent challenging. In this article we study the simplified setting of linear convolutional networks. We show that the gradient flow (to be interpreted as an abstraction of gradient descent) applied to the empirical risk defined via certain loss functions including the square loss always converges to a critical point, under a mild condition on the training data.

</details>


### [240] [Truncated Multidimensional Trigonometric Moment Problem: A Choice of Bases and the Unique Solution](https://arxiv.org/abs/2601.08551)
*Guangyu Wu,Anders Lindquist*

Main category: math.OC

TL;DR: 提出一种解决截断多维三角矩问题的新方法，通过凸优化和特定基函数选择，确保谱估计的正定性，并应用于系统识别任务。


<details>
  <summary>Details</summary>
Motivation: 在系统与信号处理中，多维有理协方差扩展问题需要解析有理函数形式的解，但传统方法难以保证谱密度的正定性，导致严重问题。

Method: 提出特定基函数选择和凸优化估计方案，保证谱估计的三角矩与样本矩完全一致，并给出保证谱估计正定性的显式条件。

Result: 证明了从估计参数到三角矩的映射是微分同胚，确保解的存在唯一性；证明了估计方案的统计性质（一致性、无偏性、收敛率、效率）。

Conclusion: 该方法为系统与信号处理中的TMTMP提供了适定处理，在系统识别任务中验证了有效性，解决了传统方法中谱密度正定性难以保证的问题。

Abstract: In this prelinimary version of paper, we propose to give a complete solution to the Truncated Multidimensional Trigonometric Moment Problem (TMTMP) from a system and signal processing perspective. In mathematical TMTMPs, people care about whether a solution exists for a given sequence of multidimensional trigonometric moments. The solution can have the form of an atomic measure. However, for the TMTMPs in system and signal processing, a solution as an analytic rational function, of which the numerator and the denominator are positive polynomials, is desired for the ARMA modelling of a stochastic process, which is the so-called Multidimensional Rational Covariance Extension problem (RCEP) . In the literature, the feasible domain of the TMTMPs, where the spectral density is positive, is difficult to obtain given a specific choice of basis functions, which causes severe problems in the Multidimensional RCEP. In this paper, we propose a choice of basis functions, and a corresponding estimation scheme by convex optimization, for the TMTMPs, with which the trigonometric moments of the spectral estimate are exactly the sample moments. We propose an explicit condition for the convex optimization problem for guaranteeing the positiveness of the spectral estimation. The map from the parameters of the estimate to the trigonometric moments is proved to be a diffeomorphism, which ensures the existence and uniqueness of solution. The statistical properties of the proposed spectral density estimation scheme are comprehensively proved, including the consistency, (asymptotical) unbiasedness, convergence rate and efficiency under a mild assumption. This well-posed treatment is then applied to a system identification task, and the simulation results validate our proposed treatment for the TMTMP in system and signal processing.

</details>


### [241] [Accelerated Methods with Complexity Separation Under Data Similarity for Federated Learning Problems](https://arxiv.org/abs/2601.08614)
*Dmitry Bylinkin,Sergey Skorik,Dmitriy Bystrov,Leonid Berezin,Aram Avetisyan,Aleksandr Beznosikov*

Main category: math.OC

TL;DR: 针对联邦学习中数据分布异构性问题，本文提出了一种基于数据相似性的复合优化框架，开发了通信高效方法，并在凸情况下提出了最优算法。


<details>
  <summary>Details</summary>
Motivation: 现代联邦学习任务中数据分布的异构性（heterogeneity）带来了重大挑战，需要开发通信高效的优化方法来解决这一问题。

Method: 将异构性问题形式化为涉及数据相似性的计算密集型复合优化问题，基于不同假设集开发通信高效方法，针对凸情况提出了最优算法。

Result: 通过一系列实验验证了所构建的理论框架，在不同问题上展示了方法的有效性。

Conclusion: 本文为联邦学习中的数据异构性问题提供了理论框架和通信高效的解决方案，特别是在凸优化场景下提出了最优算法。

Abstract: Heterogeneity within data distribution poses a challenge in many modern federated learning tasks. We formalize it as an optimization problem involving a computationally heavy composite under data similarity. By employing different sets of assumptions, we present several approaches to develop communication-efficient methods. An optimal algorithm is proposed for the convex case. The constructed theory is validated through a series of experiments across various problems.

</details>


### [242] [Optimal Dirac controls for time-periodic bistable ODEs, application to population replacement](https://arxiv.org/abs/2601.08630)
*Grégoire Nadin,David Nahmani,Nicolas Vauchelet*

Main category: math.OC

TL;DR: 研究非线性微分方程控制问题，通过最优释放时间策略实现种群替换，应用于Wolbachia蚊虫生物防治


<details>
  <summary>Details</summary>
Motivation: 解决在时变环境中将A型种群替换为B型种群的最优控制问题，特别关注Wolbachia感染蚊虫的生物防治应用，以优化病毒传播控制策略

Method: 分析具有双稳态时周期非线性的微分方程动力学，确定最优释放时间作为环境承载能力和阈值周期解函数的最小化点，研究最优释放策略的收敛性

Result: 最优释放时间表现为环境承载能力和阈值周期解相关函数的最小化点，建立了整个最优释放策略的收敛特性，为Wolbachia传播优化提供理论依据

Conclusion: 该研究为时变环境中种群替换的最优控制提供了理论框架，特别在Wolbachia蚊虫生物防治中具有重要应用价值，能有效优化病毒传播控制策略

Abstract: This work addresses an optimal control problem on a dynamics governed by a nonlinear differential equation with a bistable time-periodic nonlinearity. This problem, relevant in population dynamics, models the strategy of replacing a population of A-type individuals by a population of B-type individuals in a time-varying environment, focusing on the evolution of the proportion of B-type individuals among the whole population. The control term accounts for the instant release of B-type individuals. Our main goal, after noting some interesting properties on the differential equation, is to determine the optimal time at which this release should be operated to ensure population replacement while minimizing the release effort. The results establish that the optimal release time appears to be the minimizer of a function involving the carrying capacity of the environment and the threshold periodic solution of the dynamics; they also describe the convergence of the whole optimal release strategy. An application to the biocontrol of mosquito populations using Wolbachia-infected individuals illustrates the relevance of the theoretical results. Wolbachia is a bacterium that helps preventing the transmission of some viruses from mosquitoes to humans, making the optimization of Wolbachia propagation in a mosquito population a crucial issue.

</details>


### [243] [The Ergodic Linear-Quadratic Optimal Control Problems with Random Periodic Coefficients](https://arxiv.org/abs/2601.08672)
*Jiacheng Wu,Qi Zhang*

Main category: math.OC

TL;DR: 研究具有随机周期系数的线性二次闭环最优控制问题，提出随机周期均方指数稳定条件，证明状态方程和随机Riccati方程的随机周期解存在性，最终给出基于这些周期解的显式闭环最优控制


<details>
  <summary>Details</summary>
Motivation: 研究具有随机周期系数的线性二次最优控制问题，这类问题在现实应用中常见（如季节性变化、周期性扰动等），但现有研究主要关注确定性周期或随机平稳情况，随机周期情况下的最优控制理论尚不完善

Method: 1. 提出随机周期均方指数稳定条件；2. 基于该条件证明状态方程的随机周期性；3. 证明两类作为随机Riccati方程的后向随机微分方程存在唯一随机周期解；4. 利用"配平方法"将无限时域上的遍历成本泛函简化为单个周期区间上的等价泛函；5. 基于状态方程和随机Riccati方程的随机周期解显式给出闭环最优控制

Result: 1. 建立了随机周期均方指数稳定条件；2. 证明了状态方程存在随机周期解；3. 证明了两类随机Riccati方程存在唯一随机周期解；4. 成功将无限时域问题简化为周期区间问题；5. 给出了基于随机周期解的显式闭环最优控制策略

Conclusion: 该论文成功解决了具有随机周期系数的线性二次闭环最优控制问题，建立了完整的理论框架，包括稳定性条件、随机周期解的存在唯一性证明，以及显式最优控制策略的推导，为处理周期性随机环境中的最优控制问题提供了有效方法

Abstract: In this paper, we concern with the ergodic linear-quadratic closed-loop optimal control problems with random periodic coefficients. We put forward the random periodic mean-square exponentially stable condition, and prove the random periodicity of solutions to state equation based on it. Then we prove the existence and uniqueness of random periodic solutions to two types of backward stochastic differential equations which serve as stochastic Riccati equations in the procedure of completing the square. With the random periodicity of state equation and stochastic Riccati equations, the ergodic cost functional on infinite horizon is simplified to an equivalent cost functional over a single periodic interval without limit. Finally, the closed-loop optimal controls are explicitly given based on random periodic solutions to state equation and stochastic Riccati equations.

</details>


### [244] [Novel Dynamical Systems with Finite-Time and Fixed-Time Stability for Generalized Inverse Mixed Variational Inequality Problems](https://arxiv.org/abs/2601.08700)
*Nam Van Tran*

Main category: math.OC

TL;DR: 本文研究了广义逆混合变分不等式问题(GIMVIPs)，提出了两种连续时间动力学系统来分析解的有限时间和固定时间稳定性，并设计了保持固定时间收敛特性的迭代算法。


<details>
  <summary>Details</summary>
Motivation: 广义逆混合变分不等式问题(GIMVIPs)是变分不等式的重要推广，在优化、均衡问题和工程应用中具有广泛用途。现有研究主要关注渐近收敛，缺乏对有限时间和固定时间收敛的分析，而这两种收敛特性在实际应用中具有重要意义。

Method: 1. 提出两种新颖的连续时间动力学系统来分析GIMVIPs的有限时间和固定时间稳定性
2. 在适当的算子假设和模型参数条件下，采用Lyapunov技术建立轨迹的有限时间和固定时间收敛性
3. 对连续时间动力学进行显式前向欧拉离散化，得到保持固定时间收敛特性的近端点型算法

Result: 1. 有限时间稳定系统的收敛时间依赖于初始条件
2. 固定时间稳定系统具有与初始状态无关的收敛时间上界
3. 离散化算法保持了固定时间收敛特性
4. 数值实验验证了所提方法的有效性

Conclusion: 本文成功建立了GIMVIPs的有限时间和固定时间收敛理论框架，提出的动力学系统和离散化算法不仅具有理论意义，而且在实际应用中具有重要价值，特别是固定时间收敛特性为实时优化问题提供了理论保证。

Abstract: This paper investigates a class of generalized inverse mixed variational inequality problems (GIMVIPs), which consist in finding a vector $\overline{w}\in \R^d$ such that \[ F(\bar w)\in Ω\quad \text{and} \quad \langle h(\bar w), v-F(\bar w) \rangle + g(v)-g(F(\bar w)) \ge 0, \quad \forall v\in Ω, \] where \(h,F:\R^d\to\R^d\) are single-valued operators, \(g:Ω\to\R\cup\{+\infty\}\) is a proper function, and \(Ω\) is a closed convex set.
  Two novel continuous-time dynamical systems are proposed to analyze the finite-time and fixed-time stability of solutions to GIMVIPs in finite-dimensional Hilbert spaces. Under suitable assumptions on the operators and model parameters, Lyapunov-based techniques are employed to establish finite-time and fixed-time convergence of the generated trajectories.
  While both systems exhibit accelerated convergence, the settling time of the finite-time stable system depends on the initial condition, whereas the fixed-time stable system admits a uniform upper bound on the convergence time that is independent of the initial state. Moreover, an explicit forward Euler discretization of the continuous-time dynamics leads to a proximal point-type algorithm that preserves the fixed-time convergence property. Rigorous convergence analysis of the resulting iterative scheme is provided. A numerical experiment is presented to demonstrate the effectiveness of the proposed methods.

</details>


### [245] [Portfolio Optimization with 'Physical' Decision Variables and Non-Linear Performance Metrics: Diversification Challenge and Proposals](https://arxiv.org/abs/2601.08717)
*Isabel Barros Garcia,Jérémie Messud*

Main category: math.OC

TL;DR: 该论文提出两种基于赫芬达尔-赫希曼指数(HHI)的策略，用于增强基于投资回报率(ROI)的投资组合优化模型中的多样化，解决传统方法导致的投资组合过度集中问题。


<details>
  <summary>Details</summary>
Motivation: 在能源等实际应用中，投资组合优化需要考虑物理量(如产量)和非线性绩效指标(如ROI)，这些建模选择导致目标函数非可加性，使得优化后的投资组合高度集中，缺乏多样化，不符合决策者寻求平衡投资策略的需求。

Method: 提出两种基于HHI的策略：1) 在目标函数中直接加入HHI项，通过权重控制多样化程度；2) 在传统PO获得的最优投资组合附近，直接最大化多样化，同时控制预期利润和风险退化。

Result: 使用合成数据(能源资产)评估两种策略，结果表明每种方法都能支持不同的决策需求，增强投资组合的稳健性，展示了实际应用中的权衡取舍。

Conclusion: 基于HHI的多样化策略能够有效解决ROI投资组合优化中的过度集中问题，为决策者提供更平衡的投资策略选择，增强投资组合的稳健性。

Abstract: Portfolio optimization (PO) is a core tool in financial and operational decision-making, typically balancing expected profit and risk. In real-world applications, particularly in the energy sector, decision variables can be expressed as physical quantities (e.g., production volumes), and nonlinear performance metrics such as Return on Investment (ROI) may be requested. These modeling choices introduce challenges, including the non-additivity of the objective function. This often results in highly concentrated optimized portfolios and thus limited diversification, which can be problematic for decision-makers seeking balanced investment strategies. This paper proposes two strategies to enhance diversification in ROI-based PO models, both based on the Herfindahl-Hirschman Index (HHI). The first incorporates an HHI term directly into the objective function, with its corresponding weight allowing control over diversification. The second directly maximizes diversification while controlling expected profit and risk degradation around the optimum portfolio (obtained through conventional PO). Both strategies are evaluated using synthetic data (energy assets) to illustrate their behavior and practical trade-offs. The results highlight how each method can support different decision-making needs and enhance portfolio robustness.

</details>


### [246] [Riemannian optimization with finite-difference gradient approximations](https://arxiv.org/abs/2601.08751)
*Timothé Taminiau,Estelle Massart,Geovani Nunes Grapiglia*

Main category: math.OC

TL;DR: 提出两种无导数黎曼优化方法：基于内禀有限差分方案的方法需要O(dε⁻²)次函数估值和回缩；基于外禀有限差分方案的方法需要O(dε⁻²)次函数估值但仅需O(ε⁻²)次回缩，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着问题维度快速增长，需要计算成本低的无导数黎曼优化算法，即需要尽可能少的函数估值和回缩操作。

Method: 提出基于有限差分梯度近似的DFRO方法，采用自适应有限差分精度和步长选择策略。包含两种方案：1)内禀有限差分方案，使用回缩在切方向上测量目标函数变化；2)外禀有限差分方案，在嵌入空间中直接近似黎曼梯度，假设目标函数可在流形外求值。

Result: 内禀方案需要O(dε⁻²)次函数估值和回缩找到ε-临界点；外禀方案需要O(dε⁻²)次函数估值但仅需O(ε⁻²)次回缩。数值实验表明所提方法在欧几里得和黎曼设置下均优于现有无导数方法。

Conclusion: 提出的自适应有限差分方法在无导数黎曼优化中实现了优越的计算复杂度，特别是外禀方案显著减少了回缩操作次数，为高维问题提供了高效解决方案。

Abstract: Derivative-free Riemannian optimization (DFRO) aims to minimize an objective function using only function evaluations, under the constraint that the decision variables lie on a Riemannian manifold. The rapid increase in problem dimensions over the years calls for computationally cheap DFRO algorithms, that is, algorithms requiring as few function evaluations and retractions as possible. We propose a novel DFRO method based on finite-difference gradient approximations that relies on an adaptive selection of the finite-difference accuracy and stepsize that is novel even in the Euclidean setting. When endowed with an intrinsic finite-difference scheme, that measures variations of the objective in tangent directions using retractions, our proposed method requires $O(dε^{-2})$ function evaluations and retractions to find an $ε$-critical point, where $d$ is the manifold dimension. We then propose a variant of our method when the search space is a Riemannian submanifold of an $n$-dimensional Euclidean space. This variant relies on an extrinsic finite-difference scheme, approximating the Riemannian gradient directly in the embedding space, assuming that the objective function can be evaluated outside of the manifold. This approach leads to worst-case complexity bounds of $O(dε^{-2})$ function evaluations and $O(ε^{-2})$ retractions. We also present numerical results showing that the proposed methods achieve superior performance over existing derivative-free methods on various problems in both Euclidean and Riemannian settings.

</details>


### [247] [Grid-Aware Charging and Operational Optimization for Mixed-Fleet Public Transit](https://arxiv.org/abs/2601.08753)
*Rishav Sen,Amutheezan Sivagnanam,Aron Laszka,Ayan Mukhopadhyay,Abhishek Dubey*

Main category: math.OC

TL;DR: 本文提出一个混合整数线性规划模型，用于优化混合车队（电动和柴油巴士）的充电调度和行程分配，考虑动态电价和容量约束，采用分层方法解决计算复杂度问题。


<details>
  <summary>Details</summary>
Motivation: 随着城市人口增长和可持续交通需求增加，电动巴士在公共交通系统中得到推广。然而，混合车队（电动和柴油巴士）的有效管理面临挑战，特别是应对动态电价变化，同时考虑座位容量等次要因素。

Method: 提出一个全面的混合整数线性规划（MILP）模型，联合优化混合车队的充电调度和行程分配，考虑动态电价、车辆容量和路线约束。为解决MILP的计算复杂度问题，采用基于车队构成的分层方法。

Result: 使用美国田纳西州查塔努加市的真实数据验证，表明该方法能显著降低混合公交车队的运营成本。

Conclusion: 提出的MILP模型和分层方法能有效解决混合车队管理中的充电调度优化问题，在动态电价环境下实现显著成本节约，为公共交通机构提供实用的运营优化方案。

Abstract: The rapid growth of urban populations and the increasing need for sustainable transportation solutions have prompted a shift towards electric buses in public transit systems. However, the effective management of mixed fleets consisting of both electric and diesel buses poses significant operational challenges. One major challenge is coping with dynamic electricity pricing, where charging costs vary throughout the day. Transit agencies must optimize charging assignments in response to such dynamism while accounting for secondary considerations such as seating constraints. This paper presents a comprehensive mixed-integer linear programming (MILP) model to address these challenges by jointly optimizing charging schedules and trip assignments for mixed (electric and diesel bus) fleets while considering factors such as dynamic electricity pricing, vehicle capacity, and route constraints. We address the potential computational intractability of the MILP formulation, which can arise even with relatively small fleets, by employing a hierarchical approach tailored to the fleet composition. By using real-world data from the city of Chattanooga, Tennessee, USA, we show that our approach can result in significant savings in the operating costs of the mixed transit fleets.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [248] [Decentralized Online Convex Optimization with Unknown Feedback Delays](https://arxiv.org/abs/2601.07901)
*Hao Qiu,Mengxiao Zhang,Juliette Achddou*

Main category: stat.ML

TL;DR: 本文提出了一种新的去中心化在线凸优化算法，能够在未知且时变的反馈延迟下实现更优的遗憾界，无需先验知识总延迟信息。


<details>
  <summary>Details</summary>
Motivation: 去中心化在线凸优化在联邦学习、传感器网络等应用中很常见，但现有方法需要先验知识总延迟且遗憾界对延迟和网络参数的依赖不够优化。

Method: 基于D-OCO最新进展，提出自适应学习率机制和去中心化通信协议，通过gossip策略让每个智能体本地估计延迟，无需先验总延迟知识。

Result: 算法获得O(N√d_tot + N√T/(1-σ²)^(1/4))的遗憾界，强凸设置下获得O(Nδ_max ln T/α)的更优界，实验验证优于现有基准算法。

Conclusion: 提出的算法在未知时变延迟下实现了更优的遗憾界，无需先验延迟信息，理论界限紧致，实验效果优于现有方法。

Abstract: Decentralized online convex optimization (D-OCO), where multiple agents within a network collaboratively learn optimal decisions in real-time, arises naturally in applications such as federated learning, sensor networks, and multi-agent control.  In this paper, we study D-OCO under unknown, time-and agent-varying feedback delays. While recent work has addressed this problem (Nguyen et al., 2024), existing algorithms assume prior knowledge of the total delay over agents and still suffer from suboptimal dependence on both the delay and network parameters. To overcome these limitations, we propose a novel algorithm that achieves an improved regret bound of O N $\sqrt$ d tot + N $\sqrt$ T  (1-$σ$2) 1/4 , where T is the total horizon, d tot denotes the average total delay across agents, N is the number of agents, and 1 -$σ$ 2 is the spectral gap of the network. Our approach builds upon recent advances in D-OCO (Wan et al., 2024a), but crucially incorporates an adaptive learning rate mechanism via a decentralized communication protocol. This enables each agent to estimate delays locally using a gossip-based strategy without the prior knowledge of the total delay. We further extend our framework to the strongly convex setting and derive a sharper regret bound of O N $δ$max ln T $α$  , where $α$ is the strong convexity parameter and $δ$ max is the maximum number of missing observations averaged over agents. We also show that our upper bounds for both settings are tight up to logarithmic factors. Experimental results validate the effectiveness of our approach, showing improvements over existing benchmark algorithms.

</details>


### [249] [A Statistical Assessment of Amortized Inference Under Signal-to-Noise Variation and Distribution Shift](https://arxiv.org/abs/2601.07944)
*Roy Shivam Ram Shreshtth,Arnab Hazra,Gourab Mukherjee*

Main category: stat.ML

TL;DR: 该论文探讨了摊销贝叶斯推断的统计原理，分析了神经网络架构如何自然支持摊销推断，并通过模拟研究评估了其在不同条件下的准确性、鲁棒性和不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 随着深度神经网络和基础模型的成功，统计建模出现了新范式——通过大规模学习预测器实现贝叶斯推断的摊销化。尽管摊销推断日益流行，但其统计解释和在贝叶斯推断中的作用仍未被充分理解。

Method: 从统计角度分析前馈网络、Deep Sets和Transformer等主要神经网络架构的工作原理，探讨这些架构如何自然支持摊销贝叶斯推断。通过模拟研究评估在不同信噪比和分布偏移条件下摊销推断的准确性、鲁棒性和不确定性量化。

Result: 研究表明这些模型能够以结构化方式进行近似和概率推理，从而在广泛的部署场景中实现可控的泛化误差。同时揭示了摊销推断在计算效率和统计性能方面的优势和局限性。

Conclusion: 摊销贝叶斯推断通过前期大量计算训练神经网络，在部署时能以极低成本产生近似后验或预测，相比传统贝叶斯方法具有显著计算优势，但其统计性质和适用条件需要深入理解。

Abstract: Since the turn of the century, approximate Bayesian inference has steadily evolved as new computational techniques have been incorporated to handle increasingly complex and large-scale predictive problems. The recent success of deep neural networks and foundation models has now given rise to a new paradigm in statistical modeling, in which Bayesian inference can be amortized through large-scale learned predictors. In amortized inference, substantial computation is invested upfront to train a neural network that can subsequently produce approximate posterior or predictions at negligible marginal cost across a wide range of tasks. At deployment, amortized inference offers substantial computational savings compared with traditional Bayesian procedures, which generally require repeated likelihood evaluations or Monte Carlo simulations for predictions for each new dataset.
  Despite the growing popularity of amortized inference, its statistical interpretation and its role within Bayesian inference remain poorly understood. This paper presents statistical perspectives on the working principles of several major neural architectures, including feedforward networks, Deep Sets, and Transformers, and examines how these architectures naturally support amortized Bayesian inference. We discuss how these models perform structured approximation and probabilistic reasoning in ways that yield controlled generalization error across a wide range of deployment scenarios, and how these properties can be harnessed for Bayesian computation. Through simulation studies, we evaluate the accuracy, robustness, and uncertainty quantification of amortized inference under varying signal-to-noise ratios and distributional shifts, highlighting both its strengths and its limitations.

</details>


### [250] [Towards A Unified PAC-Bayesian Framework for Norm-based Generalization Bounds](https://arxiv.org/abs/2601.08100)
*Xinping Yi,Gaojie Jin,Xiaowei Huang,Shi Jin*

Main category: stat.ML

TL;DR: 提出一个统一的PAC-Bayesian框架，通过将泛化界推导重新表述为各向异性高斯后验的随机优化问题，实现更紧致且结构感知的深度学习泛化分析。


<details>
  <summary>Details</summary>
Motivation: 现有PAC-Bayesian范数界大多依赖各向同性高斯后验、权重扰动的谱范数集中性以及架构无关分析，限制了界的紧致性和实际相关性。需要一种能捕捉参数敏感性和架构结构的统一框架。

Method: 将泛化界推导重新表述为各向异性高斯后验的随机优化问题，引入敏感性矩阵量化网络输出对结构化权重扰动的响应，通过在该矩阵上施加不同结构假设推导出一族泛化界。

Result: 提出的统一框架能够恢复多个现有PAC-Bayesian结果作为特例，同时产生与最先进方法相当或更紧致的泛化界，实现了几何/结构感知且可解释的泛化分析。

Conclusion: 该框架为深度学习提供了原则性、灵活且结构感知的泛化分析方法，能够更好地捕捉算法的几何特性和架构结构，具有更好的解释性和实际相关性。

Abstract: Understanding the generalization behavior of deep neural networks remains a fundamental challenge in modern statistical learning theory. Among existing approaches, PAC-Bayesian norm-based bounds have demonstrated particular promise due to their data-dependent nature and their ability to capture algorithmic and geometric properties of learned models. However, most existing results rely on isotropic Gaussian posteriors, heavy use of spectral-norm concentration for weight perturbations, and largely architecture-agnostic analyses, which together limit both the tightness and practical relevance of the resulting bounds. To address these limitations, in this work, we propose a unified framework for PAC-Bayesian norm-based generalization by reformulating the derivation of generalization bounds as a stochastic optimization problem over anisotropic Gaussian posteriors. The key to our approach is a sensitivity matrix that quantifies the network outputs with respect to structured weight perturbations, enabling the explicit incorporation of heterogeneous parameter sensitivities and architectural structures. By imposing different structural assumptions on this sensitivity matrix, we derive a family of generalization bounds that recover several existing PAC-Bayesian results as special cases, while yielding bounds that are comparable to or tighter than state-of-the-art approaches. Such a unified framework provides a principled and flexible way for geometry-/structure-aware and interpretable generalization analysis in deep learning.

</details>


### [251] [Structural Dimension Reduction in Bayesian Networks](https://arxiv.org/abs/2601.08236)
*Pei Heng,Yi Sun,Jianhua Guo*

Main category: stat.ML

TL;DR: 提出一种名为"结构降维"的新技术，可将贝叶斯网络压缩为最小局部化网络，同时保持原始网络与降维网络之间的概率推理一致性。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯网络推理方法（如变量消除和信念传播）在处理大型网络时效率较低。需要一种能有效压缩网络规模同时保持推理一致性的方法，以提高推理效率。

Method: 提出有向凸包这一新的组合结构，证明其等价于最小局部化贝叶斯网络。设计多项式时间算法，通过确定包含感兴趣变量的唯一有向凸包来识别这些结构。

Result: 实验表明该方法在真实网络中具有高维度压缩能力，基于有向凸包的概率推理效率相比传统方法（变量消除和信念传播）显著提高。

Conclusion: 结构降维技术通过有向凸包实现了贝叶斯网络的有效压缩，在保持推理一致性的同时大幅提升了推理效率，为大型贝叶斯网络推理提供了新方法。

Abstract: This work introduces a novel technique, named structural dimension reduction, to collapse a Bayesian network onto a minimum and localized one while ensuring that probabilistic inferences between the original and reduced networks remain consistent. To this end, we propose a new combinatorial structure in directed acyclic graphs called the directed convex hull, which has turned out to be equivalent to their minimum localized Bayesian networks. An efficient polynomial-time algorithm is devised to identify them by determining the unique directed convex hulls containing the variables of interest from the original networks. Experiments demonstrate that the proposed technique has high dimension reduction capability in real networks, and the efficiency of probabilistic inference based on directed convex hulls can be significantly improved compared with traditional methods such as variable elimination and belief propagation algorithms. The code of this study is open at \href{https://github.com/Balance-H/Algorithms}{https://github.com/Balance-H/Algorithms} and the proofs of the results in the main body are postponed to the appendix.

</details>


### [252] [Robust low-rank estimation with multiple binary responses using pairwise AUC loss](https://arxiv.org/abs/2601.08618)
*The Tien Mai*

Main category: stat.ML

TL;DR: 提出一个针对多二元响应变量的统一框架，通过最小化AUC代理损失直接优化排序性能，同时利用低秩约束捕捉任务间的共享结构。


<details>
  <summary>Details</summary>
Motivation: 多二元响应变量在现代数据分析中常见。单独拟合逻辑回归虽然计算简单，但忽略了任务间的共享结构，统计效率低下，特别是在高维和类别不平衡的情况下。现有的二元数据低秩模型大多基于似然方法，关注点分类而非排序性能。

Method: 提出一个统一框架，通过最小化AUC的代理损失直接针对判别性能进行优化。方法聚合跨响应的成对AUC代理损失，同时对系数矩阵施加低秩约束以利用共享结构。开发了基于截断奇异值分解的可扩展投影梯度下降算法。利用成对损失仅依赖于线性预测器差值的特点简化计算和分析。

Result: 建立了非渐近收敛保证，表明在合适的正则条件下，算法能达到线性收敛直至极小极大最优统计精度。大量模拟研究表明，该方法在标签切换和数据污染等挑战性设置下具有鲁棒性，并持续优于基于似然的方法。

Conclusion: 该方法为多二元响应学习提供了一个直接针对排序性能优化的有效框架，通过低秩约束利用任务间共享结构，在计算效率和统计性能上都有优势。

Abstract: Multiple binary responses arise in many modern data-analytic problems. Although fitting separate logistic regressions for each response is computationally attractive, it ignores shared structure and can be statistically inefficient, especially in high-dimensional and class-imbalanced regimes. Low-rank models offer a natural way to encode latent dependence across tasks, but existing methods for binary data are largely likelihood-based and focus on pointwise classification rather than ranking performance. In this work, we propose a unified framework for learning with multiple binary responses that directly targets discrimination by minimizing a surrogate loss for the area under the ROC curve (AUC). The method aggregates pairwise AUC surrogate losses across responses while imposing a low-rank constraint on the coefficient matrix to exploit shared structure. We develop a scalable projected gradient descent algorithm based on truncated singular value decomposition. Exploiting the fact that the pairwise loss depends only on differences of linear predictors, we simplify computation and analysis. We establish non-asymptotic convergence guarantees, showing that under suitable regularity conditions, leading to linear convergence up to the minimax-optimal statistical precision. Extensive simulation studies demonstrate that the proposed method is robust in challenging settings such as label switching and data contamination and consistently outperforms likelihood-based approaches.

</details>


### [253] [On the use of graph models to achieve individual and group fairness](https://arxiv.org/abs/2601.08784)
*Arturo Pérez-Peralta,Sandra Benítez-Peña,Rosa E. Lillo*

Main category: stat.ML

TL;DR: 该论文提出基于Sheaf Diffusion的理论框架，通过将数据投影到无偏空间来保证公平性，统一处理个体和群体偏见，并提供可解释的SHAP值。


<details>
  <summary>Details</summary>
Motivation: 机器学习在司法、医疗、金融等关键决策场景中广泛应用，对公平性的需求日益增长。然而，公平模型的理论特性仍不明确，个体公平与群体公平之间的关系缺乏直观理解。

Method: 基于Sheaf Diffusion理论框架，利用动力系统和同调工具建模公平性。方法将输入数据投影到编码公平约束的无偏空间，提出处理不同公平度量的网络拓扑结构，统一处理个体和群体偏见。

Result: 提出的方法在模拟研究和标准公平基准测试中取得满意结果。模型在准确性和公平性方面表现良好，研究了帕累托前沿上的权衡，分析了超参数变化的影响，并深入解释了输出结果。

Conclusion: 该框架为公平机器学习提供了理论基础，通过可解释的SHAP值增强了模型透明度，在负责任的人工智能领域具有重要价值。

Abstract: Machine Learning algorithms are ubiquitous in key decision-making contexts such as justice, healthcare and finance, which has spawned a great demand for fairness in these procedures. However, the theoretical properties of such models in relation with fairness are still poorly understood, and the intuition behind the relationship between group and individual fairness is still lacking. In this paper, we provide a theoretical framework based on Sheaf Diffusion to leverage tools based on dynamical systems and homology to model fairness. Concretely, the proposed method projects input data into a bias-free space that encodes fairness constrains, resulting in fair solutions. Furthermore, we present a collection of network topologies handling different fairness metrics, leading to a unified method capable of dealing with both individual and group bias. The resulting models have a layer of interpretability in the form of closed-form expressions for their SHAP values, consolidating their place in the responsible Artificial Intelligence landscape. Finally, these intuitions are tested on a simulation study and standard fairness benchmarks, where the proposed methods achieve satisfactory results. More concretely, the paper showcases the performance of the proposed models in terms of accuracy and fairness, studying available trade-offs on the Pareto frontier, checking the effects of changing the different hyper-parameters, and delving into the interpretation of its outputs.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [254] [Regime Discovery and Intra-Regime Return Dynamics in Global Equity Markets](https://arxiv.org/abs/2601.08571)
*Salam Rabindrajit Luwang,Buddha Nath Sharma,Kundan Mukhia,Md. Nurujjaman,Anish Rai,Filippo Petroni,Luis E. C. Rocha*

Main category: q-fin.ST

TL;DR: 使用希尔伯特-黄变换识别股市三阶段（正常、高波动、极端），通过全息-希尔伯特谱分析量化波动强度，结合变长马尔可夫链分析各阶段收益状态转换特征，发现发达市场波动恢复更快而新兴市场在正常阶段仍保持较高尾部依赖。


<details>
  <summary>Details</summary>
Motivation: 金融市场在不同阶段（平静期与压力期）呈现显著不同的动态特征，传统方法难以捕捉这种非线性、非平稳的跨阶段变化。需要数据驱动的方法来识别市场阶段并分析各阶段内的收益动态特征，特别是比较发达市场与新兴市场的差异。

Method: 1. 基于经验模态分解的希尔伯特-黄变换识别三种市场阶段（正常、高波动、极端）；2. 使用全息-希尔伯特谱分析量化各阶段的波动强度（振幅调制能量）；3. 将日收益离散化为五个分位数状态；4. 在每个阶段内使用上下文树估计变长马尔可夫链，分析状态转换动态。

Result: 1. 波动强度从极端到高波动到正常阶段单调下降，发达市场下降更陡峭；2. 新兴市场即使在正常阶段也保持较高的基准波动强度；3. 无条件状态概率显示极端阶段尾部状态占主导；4. 高波动阶段熵值最高，预测性最差；5. 发达市场在压力消退后恢复更有效，而新兴市场在正常阶段仍保持尾部依赖和下行持续性。

Conclusion: 市场阶段对收益动态有显著影响，发达市场与新兴市场在阶段转换和恢复机制上存在系统性差异。新兴市场表现出更强的持续性特征和较弱的恢复能力，这对风险管理、资产配置和监管政策有重要启示。

Abstract: Financial markets alternate between tranquil periods and episodes of stress, and return dynamics can change substantially across these regimes. We study regime-dependent dynamics in developed and developing equity indices using a data-driven Hilbert--Huang-based regime identification and profiling pipeline, followed by variable-length Markov modeling of categorized returns. Market regimes are identified using an Empirical Mode Decomposition-based Hilbert--Huang Transform, where instantaneous energy from the Hilbert spectrum separates Normal, High, and Extreme regimes. We then profile each regime using Holo--Hilbert Spectral Analysis, which jointly resolves carrier frequencies, amplitude-modulation frequencies, and amplitude-modulation energy (AME). AME, interpreted as volatility intensity, declines monotonically from Extreme to High to Normal regimes. This decline is markedly sharper in developed markets, while developing markets retain higher baseline volatility intensity even in Normal regimes. Building on these regime-specific volatility signatures, we discretize daily returns into five quintile states $\mathtt{R}_1$ to $\mathtt{R}_5$ and estimate Variable-Length Markov Chains via context trees within each regime. Unconditional state probabilities show tail states dominate in Extreme regimes and recede as regimes stabilize, alongside persistent downside asymmetry. Entropy peaks in High regimes, indicating maximum unpredictability during moderate-volatility periods. Conditional transition dynamics, evaluated over contexts of length up to three days from the context-tree estimates, indicate that developed markets normalize more effectively as stress subsides, whereas developing markets retain residual tail dependence and downside persistence even in Normal regimes, consistent with a coexistence of continuation and burst-like shifts.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [255] [Enhancing Portfolio Optimization with Deep Learning Insights](https://arxiv.org/abs/2601.07942)
*Brandon Luo,Jim Skufca*

Main category: q-fin.PM

TL;DR: 该论文提出使用深度学习进行投资组合优化，通过预训练技术和Transformer架构处理有限的市场周期数据，在波动市场中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统投资组合优化方法在动态市场条件下存在局限性，特别是在长期、多资产策略中难以适应不同市场周期。深度学习模型需要足够的数据来学习市场规律，但在某些市场周期中数据有限。

Method: 采用预训练技术解决有限市场周期数据问题，利用Transformer架构纳入状态变量，构建深度学习投资组合优化模型。

Result: 与传统方法相比，提出的深度学习模型在波动市场中表现出更好的韧性和性能，证明了其在动态市场条件下的有效性。

Conclusion: 深度学习驱动的投资组合优化正在不断发展，需要自适应策略来应对动态市场条件并提高预测准确性，为金融领域提供了有前景的新方法。

Abstract: Our work focuses on deep learning (DL) portfolio optimization, tackling challenges in long-only, multi-asset strategies across market cycles. We propose training models with limited regime data using pre-training techniques and leveraging transformer architectures for state variable inclusion. Evaluating our approach against traditional methods shows promising results, demonstrating our models' resilience in volatile markets. These findings emphasize the evolving landscape of DL-driven portfolio optimization, stressing the need for adaptive strategies to navigate dynamic market conditions and improve predictive accuracy.

</details>


### [256] [Feasibility-First Satellite Integration in Robust Portfolio Architectures](https://arxiv.org/abs/2601.08721)
*Roberto Garrone*

Main category: q-fin.PM

TL;DR: 提出一个针对小型投资组合的可行性优先、非预测性的卫星资产整合框架，通过四个嵌套可行性层次（物理、经济、结构、认知）来确定卫星配置的可行性，不依赖收益预测或回测表现。


<details>
  <summary>Details</summary>
Motivation: 传统卫星资产整合方法（基于因子暴露、主观判断或回测表现）主要适用于机构规模的投资组合，但不适合小型投资组合和稳健性导向的配置框架。小型投资组合的主要约束来自固定成本、不可逆风险和治理复杂性，而非收益可预测性或交易能力。

Method: 开发了一个规模感知的可行性优先框架，定义了四个嵌套可行性层次：1) 物理可行性：确保在凹性市场影响定律下的可实施性；2) 经济可行性：通过成本主导阈值约束抑制噪声主导的再配置；3) 结构可行性：通过可容忍损失定义的明确期权预算来限制卫星规模；4) 认知可行性：通过基于熵的复杂性预算限制卫星广度和分散度。

Result: 该框架在不依赖收益预测、因子溢价或回测表现的情况下，得出了卫星规模、换手率和广度的封闭形式可行性边界。将结构性期权确定为卫星设计的主要原则，其他层次作为稳健性透镜而非优化标准。

Conclusion: 为将主题卫星整合到小型、稳健性导向的投资组合中提供了一个有纪律的基础，特别强调结构性期权作为核心设计原则，通过多层次可行性约束确保配置的稳健性和可实施性。

Abstract: The integration of thematic satellite allocations into core-satellite portfolio architectures is commonly approached using factor exposures, discretionary convictions, or backtested performance, with feasibility assessed primarily through liquidity screens or market-impact considerations. While such approaches may be appropriate at institutional scale, they are ill-suited to small portfolios and robustness-oriented allocation frameworks, where dominant constraints arise not from return predictability or trading capacity, but from fixed costs, irreversibility risk, and governance complexity. This paper develops a feasibility-first, non-predictive framework for satellite integration that is explicitly scale-aware. We formalize four nested feasibility layers (physical, economic, structural, and epistemic) that jointly determine whether a satellite allocation is admissible. Physical feasibility ensures implementability under concave market-impact laws; economic feasibility suppresses noise-dominated reallocations via cost-dominance threshold constraints; structural feasibility bounds satellite size through an explicit optionality budget defined by tolerable loss under thesis failure; and epistemic feasibility limits satellite breadth and dispersion through an entropy-based complexity budget. Within this hierarchy, structural optionality is identified as the primary design principle for thematic satellites, with the remaining layers acting as robustness lenses rather than optimization criteria. The framework yields closed-form feasibility bounds on satellite size, turnover, and breadth without reliance on return forecasts, factor premia, or backtested performance, providing a disciplined basis for integrating thematic satellites into small, robustness-oriented portfolios.

</details>


### [257] [Optimal Option Portfolios for Student t Returns](https://arxiv.org/abs/2601.07991)
*Kyle Sung,Traian A. Pirvu*

Main category: q-fin.PM

TL;DR: 提出了在收益率服从Student t分布下，方差和VaR最小化的最优期权组合显式解，突破了传统正态分布假设的限制。


<details>
  <summary>Details</summary>
Motivation: 传统期权组合优化通常假设收益率服从正态分布，但实际金融数据常呈现厚尾特征。本文旨在解决更符合现实尾部风险特征的Student t分布下的最优组合构建问题。

Method: 开发了在Student t分布假设下，方差最小化和VaR最小化的最优期权组合构建方法，提供了显式解析解。

Result: 数值实验显示，方差最小化和VaR最小化的最优组合构成存在显著差异，表明考虑更现实的尾部风险特征会导致更合理的资产配置。

Conclusion: 采用Student t分布等更现实的尾部风险模型能够产生更符合实际的资产配置，为风险管理提供了更有效的工具。

Abstract: We provide an explicit solution for optimal option portfolios under variance and Value at Risk (VaR) minimization when the underlying returns follow a Student t-distribution. The novelty of our paper is the departure from the traditional normal returns setting. Our main contribution is the methodology for obtaining optimal portfolios. Numerical experiments reveal that, as expected, the optimal variance and VaR portfolio compositions differ by a significant amount, suggesting that more realistic tail risk settings can lead to potentially more realistic portfolio allocations.

</details>
